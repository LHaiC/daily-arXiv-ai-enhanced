{"id": "2508.05963", "categories": ["cs.ET", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.05963", "abs": "https://arxiv.org/abs/2508.05963", "authors": ["Michael Beyeler"], "title": "Bionic Vision as Neuroadaptive XR: Closed-Loop Perceptual Interfaces for Neurotechnology", "comment": null, "summary": "Visual neuroprostheses are commonly framed as technologies to restore natural\nsight to people who are blind. In practice, they create a novel mode of\nperception shaped by sparse, distorted, and unstable input. They resemble early\nextended reality (XR) headsets more than natural vision, streaming video from a\nhead-mounted camera to a neural \"display\" with under 1000 pixels, limited field\nof view, low refresh rates, and nonlinear spatial mappings. No amount of\nresolution alone will make this experience natural. This paper proposes a\nreframing: bionic vision as neuroadaptive XR. Rather than replicating natural\nsight, the goal is to co-adapt brain and device through a bidirectional\ninterface that responds to neural constraints, behavioral goals, and cognitive\nstate. By comparing traditional XR, current implants, and proposed\nneuroadaptive systems, it introduces a new design space for inclusive,\nbrain-aware computing. It concludes with research provocations spanning\nencoding, evaluation, learning, and ethics, and invites the XR community to\nhelp shape the future of sensory augmentation.", "AI": {"tldr": "Visual prosthetics are like bad VR, not real eyes. This paper suggests making them 'neuro-adaptive XR' to work *with* the brain, not just try to copy sight. It's about designing systems that learn and adapt to the user, and it calls on the XR community to help build this future.", "motivation": "Current visual neuroprostheses provide a novel mode of perception with sparse, distorted, and unstable input, resembling early XR headsets rather than natural vision. The motivation is to move beyond merely replicating natural sight and instead focus on co-adapting the brain and device.", "method": "The paper proposes a reframing of visual neuroprostheses as 'neuroadaptive XR'. It compares traditional XR, current implants, and the proposed neuroadaptive systems to introduce a new design space for brain-aware computing. It also covers research provocations in encoding, evaluation, learning, and ethics.", "result": "The paper introduces a new design space for inclusive, brain-aware computing by proposing a 'neuroadaptive XR' framework. It highlights that resolution alone is insufficient for a natural experience and emphasizes the need for a bidirectional interface that adapts to the user's neural and cognitive state.", "conclusion": "Visual neuroprostheses should be reframed as neuroadaptive XR, aiming to co-adapt the brain and device through a bidirectional interface that responds to neural constraints, behavioral goals, and cognitive state, rather than replicating natural sight. This new perspective opens up a design space for inclusive, brain-aware computing and invites the XR community to contribute to the future of sensory augmentation."}}
{"id": "2508.05999", "categories": ["cs.ET"], "pdf": "https://arxiv.org/pdf/2508.05999", "abs": "https://arxiv.org/abs/2508.05999", "authors": ["Sergio Rojas-Galeano", "Julian Tejada", "Fernando Marmolejo-Ramos"], "title": "Between Tool and Trouble: Student Attitudes Toward AI in Programming Education", "comment": "arXiv admin note: substantial text overlap with arXiv:2507.22900", "summary": "This study examines how AI code assistants shape novice programmers\nexperiences during a two-part exam in an introductory programming course. In\nthe first part, students completed a programming task with access to AI\nsupport; in the second, they extended their solutions without AI. We collected\nLikert-scale and open-ended responses from 20 students to evaluate their\nperceptions and challenges. Findings suggest that AI tools were perceived as\nhelpful for understanding code and increasing confidence, particularly during\ninitial development. However, students reported difficulties transferring\nknowledge to unaided tasks, revealing possible overreliance and gaps in\nconceptual understanding. These insights highlight the need for pedagogical\nstrategies that integrate AI meaningfully while reinforcing foundational\nprogramming skills.", "AI": {"tldr": "AI\u52a9\u624b\u6709\u52a9\u4e8e\u521d\u5b66\u8005\u7406\u89e3\u4ee3\u7801\u548c\u5efa\u7acb\u4fe1\u5fc3\uff0c\u4f46\u5728\u72ec\u7acb\u5b8c\u6210\u4efb\u52a1\u65f6\u53ef\u80fd\u5bfc\u81f4\u8fc7\u5ea6\u4f9d\u8d56\u548c\u77e5\u8bc6\u8fc1\u79fb\u56f0\u96be\uff0c\u9700\u8981\u6559\u5b66\u7b56\u7565\u6765\u5e73\u8861AI\u8f85\u52a9\u4e0e\u57fa\u7840\u6280\u80fd\u57f9\u517b\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u63a2\u8ba8AI\u4ee3\u7801\u52a9\u624b\u5728\u521d\u5b66\u7f16\u7a0b\u8005\u5165\u95e8\u7f16\u7a0b\u8bfe\u7a0b\u7684\u8003\u8bd5\u4e2d\u6240\u626e\u6f14\u7684\u89d2\u8272\uff0c\u4ee5\u53ca\u5176\u5bf9\u5b66\u4e60\u4f53\u9a8c\u7684\u5f71\u54cd\u3002", "method": "\u672c\u7814\u7a76\u901a\u8fc7\u4e00\u4e2a\u5305\u542b\u4e24\u4e2a\u90e8\u5206\u7684\u8003\u8bd5\uff0c\u8003\u5bdfAI\u4ee3\u7801\u52a9\u624b\u5982\u4f55\u5f71\u54cd\u521d\u5b66\u7f16\u7a0b\u8005\u7684\u4f53\u9a8c\u3002\u5728\u7b2c\u4e00\u90e8\u5206\uff0c\u5b66\u751f\u5728\u6709AI\u652f\u6301\u7684\u60c5\u51b5\u4e0b\u5b8c\u6210\u7f16\u7a0b\u4efb\u52a1\uff1b\u5728\u7b2c\u4e8c\u90e8\u5206\uff0c\u4ed6\u4eec\u9700\u8981\u5728\u6ca1\u6709AI\u7684\u60c5\u51b5\u4e0b\u6269\u5c55\u89e3\u51b3\u65b9\u6848\u3002\u7814\u7a76\u6536\u96c6\u4e8620\u540d\u5b66\u751f\u7684\u674e\u514b\u7279\u91cf\u8868\u548c\u5f00\u653e\u5f0f\u95ee\u9898\u53cd\u9988\uff0c\u4ee5\u8bc4\u4f30\u4ed6\u4eec\u7684\u770b\u6cd5\u548c\u6311\u6218\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0cAI\u5de5\u5177\u88ab\u8ba4\u4e3a\u6709\u52a9\u4e8e\u7406\u89e3\u4ee3\u7801\u548c\u63d0\u9ad8\u4fe1\u5fc3\uff0c\u7279\u522b\u662f\u5728\u521d\u59cb\u5f00\u53d1\u9636\u6bb5\u3002\u7136\u800c\uff0c\u5b66\u751f\u5728\u5c06\u77e5\u8bc6\u8fc1\u79fb\u5230\u65e0AI\u8f85\u52a9\u7684\u4efb\u52a1\u65f6\u9047\u5230\u56f0\u96be\uff0c\u8fd9\u63ed\u793a\u4e86\u53ef\u80fd\u5b58\u5728\u7684\u8fc7\u5ea6\u4f9d\u8d56\u548c\u6982\u5ff5\u7406\u89e3\u4e0a\u7684\u4e0d\u8db3\u3002", "conclusion": "AI\u5de5\u5177\u88ab\u8ba4\u4e3a\u6709\u52a9\u4e8e\u7406\u89e3\u4ee3\u7801\u548c\u63d0\u9ad8\u4fe1\u5fc3\uff0c\u7279\u522b\u662f\u5728\u521d\u59cb\u5f00\u53d1\u9636\u6bb5\u3002\u7136\u800c\uff0c\u5b66\u751f\u5728\u65e0\u9700AI\u7684\u72ec\u7acb\u4efb\u52a1\u4e2d\u4e5f\u9047\u5230\u4e86\u77e5\u8bc6\u8fc1\u79fb\u7684\u56f0\u96be\uff0c\u8fd9\u53ef\u80fd\u8868\u660e\u5bf9AI\u7684\u8fc7\u5ea6\u4f9d\u8d56\u4ee5\u53ca\u6982\u5ff5\u7406\u89e3\u65b9\u9762\u5b58\u5728\u5dee\u8ddd\u3002\u8fd9\u4e9b\u53d1\u73b0\u5f3a\u8c03\u4e86\u9700\u8981\u5236\u5b9a\u6559\u5b66\u7b56\u7565\uff0c\u4ee5\u6709\u610f\u4e49\u5730\u6574\u5408AI\uff0c\u540c\u65f6\u52a0\u5f3a\u57fa\u7840\u7f16\u7a0b\u6280\u80fd\u3002"}}
{"id": "2508.05876", "categories": ["cs.LG", "astro-ph.EP", "astro-ph.IM", "cs.ET"], "pdf": "https://arxiv.org/pdf/2508.05876", "abs": "https://arxiv.org/abs/2508.05876", "authors": ["Francesca Ferrara", "Lander W. Schillinger Arana", "Florian D\u00f6rfler", "Sarah H. Q. Li"], "title": "A Markov Decision Process Framework for Early Maneuver Decisions in Satellite Collision Avoidance", "comment": "16 pages, 13 figures, submitted to the 2025 Astrodynamics Specialist\n  Conference", "summary": "This work presents a Markov decision process (MDP) framework to model\ndecision-making for collision avoidance maneuver (CAM) and a reinforcement\nlearning policy gradient (RL-PG) algorithm to train an autonomous guidance\npolicy using historic CAM data. In addition to maintaining acceptable collision\nrisks, this approach seeks to minimize the average fuel consumption of CAMs by\nmaking early maneuver decisions. We model CAM as a continuous state, discrete\naction and finite horizon MDP, where the critical decision is determining when\nto initiate the maneuver. The MDP model also incorporates analytical models for\nconjunction risk, propellant consumption, and transit orbit geometry. The\nMarkov policy effectively trades-off maneuver delay-which improves the\nreliability of conjunction risk indicators-with propellant consumption-which\nincreases with decreasing maneuver time. Using historical data of tracked\nconjunction events, we verify this framework and conduct an extensive ablation\nstudy on the hyper-parameters used within the MDP. On synthetic conjunction\nevents, the trained policy significantly minimizes both the overall and average\npropellant consumption per CAM when compared to a conventional cut-off policy\nthat initiates maneuvers 24 hours before the time of closest approach (TCA). On\nhistorical conjunction events, the trained policy consumes more propellant\noverall but reduces the average propellant consumption per CAM. For both\nhistorical and synthetic conjunction events, the trained policy achieves equal\nif not higher overall collision risk guarantees.", "AI": {"tldr": "\u4f7f\u7528MDP\u548cRL-PG\u8fdb\u884cCAM\u51b3\u7b56\uff0c\u4ee5\u6700\u5c0f\u5316\u71c3\u6599\u6d88\u8017\u5e76\u4fdd\u8bc1\u78b0\u649e\u98ce\u9669\u3002", "motivation": "\u5728\u4fdd\u8bc1\u53ef\u63a5\u53d7\u7684\u78b0\u649e\u98ce\u9669\u7684\u540c\u65f6\uff0c\u901a\u8fc7\u65e9\u671f\u673a\u52a8\u51b3\u7b56\u6765\u6700\u5c0f\u5316CAM\u7684\u5e73\u5747\u71c3\u6599\u6d88\u8017\u3002", "method": "\u63d0\u51fa\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08MDP\uff09\u6846\u67b6\u6765\u6a21\u62df\u78b0\u649e\u89c4\u907f\u673a\u52a8\uff08CAM\uff09\u7684\u51b3\u7b56\uff0c\u5e76\u63d0\u51fa\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u68af\u5ea6\uff08RL-PG\uff09\u7b97\u6cd5\u6765\u8bad\u7ec3\u81ea\u4e3b\u5bfc\u822a\u7b56\u7565\u3002", "result": "\u4e0e\u4f20\u7edf\u7684\u622a\u6b62\u7b56\u7565\u76f8\u6bd4\uff0c\u8be5\u8bad\u7ec3\u7b56\u7565\u5728\u5408\u6210\u548c\u5386\u53f2\u5bf9\u51b2\u4e8b\u4ef6\u4e2d\u663e\u8457\u51cf\u5c11\u4e86CAM\u7684\u6574\u4f53\u548c\u5e73\u5747\u71c3\u6599\u6d88\u8017\uff0c\u5e76\u5b9e\u73b0\u4e86\u540c\u7b49\u6216\u66f4\u9ad8\u7684\u78b0\u649e\u98ce\u9669\u4fdd\u8bc1\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u57fa\u4e8eMDP\u548cRL-PG\u7684\u81ea\u4e3b\u5bfc\u822a\u7b56\u7565\u5728\u51cf\u5c11\u5e73\u5747\u71c3\u6599\u6d88\u8017\u548c\u4fdd\u8bc1\u78b0\u649e\u98ce\u9669\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u7684\u622a\u6b62\u7b56\u7565\u3002"}}
{"id": "2508.05883", "categories": ["quant-ph", "cs.ET", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.05883", "abs": "https://arxiv.org/abs/2508.05883", "authors": ["Sean Feeney", "Reuben Tate", "John Golden", "Stephan Eidenbenz"], "title": "MPS-JuliQAOA: User-friendly, Scalable MPS-based Simulation for Quantum Optimization", "comment": "14 pages, 7 figures", "summary": "We present the MPS-JuliQAOA simulator, a user-friendly, open-source tool to\nsimulate the Quantum Approximate Optimization Algorithm (QAOA) of any\noptimization problem that can be expressed as diagonal Hamiltonian. By\nleveraging Julia-language constructs and the ITensor package to implement a\nMatrix Product State (MPS) approach to simulating QAOA, MPS-Juli-QAOA\neffortlessly scales to 512 qubits and 20 simulation rounds on the standard\nde-facto benchmark 3-regular MaxCut QAOA problem. MPS-JuliQAOA also has\nbuilt-in parameter finding capabilities, which is a crucial performance aspect\nof QAOA. We illustrate through examples that the user does not need to know MPS\nprinciples or complex automatic differentiation techniques to use MPS-JuliQAOA.\nWe study the scalability of our tool with respect to runtime, memory usage and\naccuracy tradeoffs. Code available at\nhttps://github.com/lanl/JuliQAOA.jl/tree/mps.", "AI": {"tldr": "A new Julia-based simulator (MPS-JuliQAOA) uses MPS to simulate QAOA for diagonal Hamiltonians, offering scalability and parameter-finding features, and is easy to use.", "motivation": "To provide a user-friendly and scalable tool for simulating the Quantum Approximate Optimization Algorithm (QAOA) for a broader range of optimization problems that can be expressed as diagonal Hamiltonians.", "method": "The simulator uses a Matrix Product State (MPS) approach implemented in Julia with the ITensor package to simulate QAOA. It also has built-in parameter-finding capabilities.", "result": "The simulator scales effectively, handling up to 512 qubits and 20 simulation rounds on the 3-regular MaxCut QAOA benchmark. It also demonstrates user-friendliness and analyzes scalability tradeoffs.", "conclusion": "The MPS-JuliQAOA simulator is a user-friendly, open-source tool for simulating QAOA for problems with diagonal Hamiltonians. It scales efficiently and includes parameter-finding capabilities, making it accessible to users without requiring knowledge of MPS or automatic differentiation."}}
{"id": "2508.06342", "categories": ["cs.CV", "cs.SI"], "pdf": "https://arxiv.org/pdf/2508.06342", "abs": "https://arxiv.org/abs/2508.06342", "authors": ["Kieran Elrod", "Katherine Flanigan", "Mario Berg\u00e9s"], "title": "Street View Sociability: Interpretable Analysis of Urban Social Behavior Across 15 Cities", "comment": null, "summary": "Designing socially active streets has long been a goal of urban planning, yet\nexisting quantitative research largely measures pedestrian volume rather than\nthe quality of social interactions. We hypothesize that street view imagery --\nan inexpensive data source with global coverage -- contains latent social\ninformation that can be extracted and interpreted through established social\nscience theory. As a proof of concept, we analyzed 2,998 street view images\nfrom 15 cities using a multimodal large language model guided by Mehta's\ntaxonomy of passive, fleeting, and enduring sociability -- one illustrative\nexample of a theory grounded in urban design that could be substituted or\ncomplemented by other sociological frameworks. We then used linear regression\nmodels, controlling for factors like weather, time of day, and pedestrian\ncounts, to test whether the inferred sociability measures correlate with\ncity-level place attachment scores from the World Values Survey and with\nenvironmental predictors (e.g., green, sky, and water view indices) derived\nfrom individual street view images. Results aligned with long-standing urban\nplanning theory: the sky view index was associated with all three sociability\ntypes, the green view index predicted enduring sociability, and place\nattachment was positively associated with fleeting sociability. These results\nprovide preliminary evidence that street view images can be used to infer\nrelationships between specific types of social interactions and built\nenvironment variables. Further research could establish street view imagery as\na scalable, privacy-preserving tool for studying urban sociability, enabling\ncross-cultural theory testing and evidence-based design of socially vibrant\ncities.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u8857\u666f\u56fe\u50cf\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u6839\u636e\u57ce\u5e02\u89c4\u5212\u7406\u8bba\u91cf\u5316\u4e86\u8857\u9053\u7684\u793e\u4ea4\u4e92\u52a8\u8d28\u91cf\uff0c\u5e76\u5c06\u793e\u4ea4\u6027\u4e0e\u57ce\u5e02\u5f52\u5c5e\u611f\u3001\u5929\u7a7a\u3001\u7eff\u8272\u548c\u6c34\u666f\u6307\u6570\u7b49\u73af\u5883\u56e0\u7d20\u8054\u7cfb\u8d77\u6765\u3002", "motivation": "\u65e8\u5728\u91cf\u5316\u8861\u91cf\u8857\u9053\u7684\u793e\u4ea4\u4e92\u52a8\u8d28\u91cf\uff0c\u800c\u975e\u4ec5\u4ec5\u662f\u884c\u4eba\u6570\u91cf\uff0c\u5e76\u63a2\u7d22\u5229\u7528\u5ec9\u4ef7\u7684\u8857\u666f\u56fe\u50cf\u6570\u636e\u6e90\u63d0\u53d6\u793e\u4f1a\u4fe1\u606f\u7684\u53ef\u884c\u6027\u3002", "method": "\u672c\u7814\u7a76\u4f7f\u7528\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5206\u6790\u4e86\u6765\u81ea 15 \u4e2a\u57ce\u5e02\u7684 2,998 \u5f20\u8857\u666f\u56fe\u50cf\uff0c\u5e76\u4ee5 Mehta \u7684\u88ab\u52a8\u3001\u77ed\u6682\u548c\u6301\u4e45\u7684\u793e\u4ea4\u6027\u5206\u7c7b\u6cd5\u4f5c\u4e3a\u6307\u5bfc\u3002\u968f\u540e\uff0c\u4f7f\u7528\u7ebf\u6027\u56de\u5f52\u6a21\u578b\uff08\u63a7\u5236\u5929\u6c14\u3001\u65f6\u95f4\u3001\u884c\u4eba\u6570\u91cf\u7b49\u56e0\u7d20\uff09\u6765\u68c0\u9a8c\u63a8\u65ad\u7684\u793e\u4ea4\u6027\u6307\u6807\u662f\u5426\u4e0e\u4e16\u754c\u4ef7\u503c\u89c2\u8c03\u67e5\u4e2d\u7684\u57ce\u5e02\u5f52\u5c5e\u611f\u5f97\u5206\u4ee5\u53ca\u4ece\u8857\u666f\u56fe\u50cf\u4e2d\u63d0\u53d6\u7684\u73af\u5883\u9884\u6d4b\u56e0\u5b50\uff08\u5982\u5929\u7a7a\u3001\u7eff\u8272\u548c\u6c34\u666f\u6307\u6570\uff09\u76f8\u5173\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u4e0e\u957f\u671f\u7684\u57ce\u5e02\u89c4\u5212\u7406\u8bba\u4e00\u81f4\uff1a\u5929\u7a7a\u89c6\u91ce\u6307\u6570\u4e0e\u6240\u6709\u4e09\u79cd\u793e\u4ea4\u7c7b\u578b\u76f8\u5173\uff0c\u7eff\u8272\u89c6\u91ce\u6307\u6570\u9884\u793a\u7740\u6301\u4e45\u7684\u793e\u4ea4\u6027\uff0c\u800c\u57ce\u5e02\u5f52\u5c5e\u611f\u5219\u4e0e\u77ed\u6682\u7684\u793e\u4ea4\u6027\u6b63\u76f8\u5173\u3002", "conclusion": "\u8be5\u7814\u7a76\u521d\u6b65\u8bc1\u660e\u4e86\u8857\u666f\u56fe\u50cf\u53ef\u7528\u4e8e\u63a8\u65ad\u7279\u5b9a\u7c7b\u578b\u7684\u793e\u4f1a\u4e92\u52a8\u4e0e\u5efa\u7b51\u73af\u5883\u53d8\u91cf\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5e76\u53ef\u80fd\u6210\u4e3a\u7814\u7a76\u57ce\u5e02\u793e\u4f1a\u6027\u7684\u53ef\u6269\u5c55\u3001\u6ce8\u91cd\u9690\u79c1\u7684\u5de5\u5177\u3002"}}
{"id": "2508.05920", "categories": ["cs.DS", "cs.NA", "math.NA", "65F99", "G.1.3"], "pdf": "https://arxiv.org/pdf/2508.05920", "abs": "https://arxiv.org/abs/2508.05920", "authors": ["Chris Cama\u00f1o", "Raphael A. Meyer", "Kevin Shu"], "title": "Debiasing Polynomial and Fourier Regression", "comment": null, "summary": "We study the problem of approximating an unknown function\n$f:\\mathbb{R}\\to\\mathbb{R}$ by a degree-$d$ polynomial using as few function\nevaluations as possible, where error is measured with respect to a probability\ndistribution $\\mu$. Existing randomized algorithms achieve near-optimal sample\ncomplexities to recover a $ (1+\\varepsilon) $-optimal polynomial but produce\nbiased estimates of the best polynomial approximation, which is undesirable.\n  We propose a simple debiasing method based on a connection between polynomial\nregression and random matrix theory. Our method involves evaluating\n$f(\\lambda_1),\\ldots,f(\\lambda_{d+1})$ where $\\lambda_1,\\ldots,\\lambda_{d+1}$\nare the eigenvalues of a suitably designed random complex matrix tailored to\nthe distribution $\\mu$. Our estimator is unbiased, has near-optimal sample\ncomplexity, and experimentally outperforms iid leverage score sampling.\n  Additionally, our techniques enable us to debias existing methods for\napproximating a periodic function with a truncated Fourier series with\nnear-optimal sample complexity.", "AI": {"tldr": "\u4e3a\u4e86\u65e0\u504f\u5730\u903c\u8fd1\u672a\u77e5\u51fd\u6570\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u968f\u673a\u77e9\u9635\u7406\u8bba\u548c\u591a\u9879\u5f0f\u56de\u5f52\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc4\u4f30\u7279\u5b9a\u968f\u673a\u77e9\u9635\u7684\u7279\u5f81\u503c\u6765\u6d88\u9664\u504f\u5dee\uff0c\u8be5\u65b9\u6cd5\u6837\u672c\u590d\u6742\u5ea6\u63a5\u8fd1\u6700\u4f18\uff0c\u5e76\u4e14\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u968f\u673a\u7b97\u6cd5\u5728\u6062\u590d\u6700\u4f18\u591a\u9879\u5f0f\u903c\u8fd1\u65b9\u9762\u5177\u6709\u63a5\u8fd1\u6700\u4f18\u7684\u6837\u672c\u590d\u6742\u5ea6\uff0c\u4f46\u4f1a\u4ea7\u751f\u6709\u504f\u4f30\u8ba1\uff0c\u8fd9\u662f\u4e0d\u53ef\u53d6\u7684\u3002", "method": "\u901a\u8fc7\u8bc4\u4f30\u7279\u5b9a\u8bbe\u8ba1\u7684\u968f\u673a\u590d\u6570\u77e9\u9635\u7684\u7279\u5f81\u503c\uff08$\nu_1,\nu_2,\nu_3$\uff09\uff0c\u5229\u7528\u591a\u9879\u5f0f\u56de\u5f52\u548c\u968f\u673a\u77e9\u9635\u7406\u8bba\u4e4b\u95f4\u7684\u8054\u7cfb\u6765\u6d88\u9664\u504f\u5dee\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65e0\u504f\u4f30\u8ba1\u91cf\u5177\u6709\u63a5\u8fd1\u6700\u4f18\u7684\u6837\u672c\u590d\u6742\u5ea6\uff0c\u5e76\u4e14\u5728\u5b9e\u9a8c\u4e2d\u4f18\u4e8e\u72ec\u7acb\u540c\u5206\u5e03\u62bd\u6837\u3002\u6b64\u5916\uff0c\u8be5\u6280\u672f\u8fd8\u53ef\u7528\u4e8e\u6539\u8fdb\u5468\u671f\u51fd\u6570\u5085\u91cc\u53f6\u7ea7\u6570\u903c\u8fd1\u7684\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u53ef\u4ee5\u65e0\u504f\u4f30\u8ba1\u6700\u4f73\u591a\u9879\u5f0f\u903c\u8fd1\uff0c\u6837\u672c\u590d\u6742\u5ea6\u63a5\u8fd1\u6700\u4f18\uff0c\u5e76\u4e14\u5728\u5b9e\u9a8c\u4e2d\u4f18\u4e8e\u72ec\u7acb\u540c\u5206\u5e03\u62bd\u6837\u3002"}}
{"id": "2508.05801", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.05801", "abs": "https://arxiv.org/abs/2508.05801", "authors": ["Yingbo Hua"], "title": "A Remark on the AAA Method for Secret-Key Generation in Mobile Networks", "comment": null, "summary": "A broadly applicable method for secret-key generation is named for its\naccumulative, adaptable and additive (AAA) properties. This paper first shows a\nrobustness of its performance. Namely, even if there is an inter correlation or\na leakage caused intra correlation among the superimposed packets, provided\nthere is a nonzero probability for each packet to be missed in full or in part\nby Eve, then the equivocation of the key generated by the AAA method always\nbecomes perfect as the number of superpositions becomes infinite. Also shown in\nthis paper is a comparison between the AAA method and an ideal method based on\nreciprocal channel estimation, which reveals several advantages of the AAA\nmethod.", "AI": {"tldr": "AAA\u5bc6\u94a5\u751f\u6210\u65b9\u6cd5\u901a\u8fc7\u65e0\u9650\u6b21\u53e0\u52a0\uff0c\u5373\u4f7f\u5728\u6709\u5e72\u6270\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u751f\u6210\u5b8c\u7f8e\u5bc6\u94a5\uff0c\u4e14\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4e3a\u4e86\u63d0\u4f9b\u4e00\u79cd\u5b89\u5168\u4e14\u9ad8\u6548\u7684\u5bc6\u94a5\u751f\u6210\u65b9\u6cd5\uff0c\u5e76\u4e0e\u73b0\u6709\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\u4ee5\u5c55\u73b0\u5176\u4f18\u8d8a\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5177\u6709\u7d2f\u79ef\u3001\u81ea\u9002\u5e94\u548c\u7d2f\u52a0\uff08AAA\uff09\u7279\u6027\u7684\u5bc6\u94a5\u751f\u6210\u65b9\u6cd5\uff0c\u5e76\u5bf9\u5176\u6027\u80fd\u9c81\u68d2\u6027\u8fdb\u884c\u4e86\u7814\u7a76\u3002", "result": "AAA\u5bc6\u94a5\u751f\u6210\u65b9\u6cd5\u5728\u5b58\u5728\u4fe1\u9053\u5e72\u6270\u548c\u4fe1\u606f\u6cc4\u9732\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u65e0\u9650\u6b21\u53e0\u52a0\u53ef\u4ee5\u5b9e\u73b0\u5b8c\u7f8e\u7684\u5bc6\u94a5\u751f\u6210\uff0c\u5e76\u4e14\u5728\u4e0e\u7406\u60f3\u65b9\u6cd5\u5bf9\u6bd4\u4e2d\u663e\u793a\u51fa\u82e5\u5e72\u4f18\u52bf\u3002", "conclusion": "AAA\u5bc6\u94a5\u751f\u6210\u65b9\u6cd5\u5728\u65e0\u9650\u6b21\u53e0\u52a0\u540e\u53ef\u4ee5\u5b9e\u73b0\u5b8c\u7f8e\u5bc6\u94a5\uff0c\u5e76\u4e14\u4e0e\u57fa\u4e8e\u4e92\u6613\u4fe1\u9053\u4f30\u8ba1\u7684\u7406\u60f3\u65b9\u6cd5\u76f8\u6bd4\u5177\u6709\u591a\u9879\u4f18\u52bf\u3002"}}
{"id": "2508.05797", "categories": ["cs.DC", "cs.AR"], "pdf": "https://arxiv.org/pdf/2508.05797", "abs": "https://arxiv.org/abs/2508.05797", "authors": ["Sreeharsha Udayashankar", "Abdelrahman Baba", "Samer Al-Kiswany"], "title": "Accelerating Data Chunking in Deduplication Systems using Vector Instructions", "comment": "Under review. This is the follow-up work to our FAST 2025 paper,\n  \"VectorCDC: Accelerating Data Deduplication with Vector Instructions\". The\n  associated code is available at https://github.com/UWASL/dedup-bench", "summary": "Content-defined Chunking (CDC) algorithms dictate the overall space savings\nthat deduplication systems achieve. However, due to their need to scan each\nfile in its entirety, they are slow and often the main performance bottleneck\nwithin data deduplication. We present VectorCDC, a method to accelerate\nhashless CDC algorithms using vector CPU instructions, such as SSE / AVX. Our\nevaluation shows that VectorCDC is effective on Intel, AMD, ARM, and IBM CPUs,\nachieving 8.35x - 26.2x higher throughput than existing vector-accelerated\ntechniques without affecting the deduplication space savings.", "AI": {"tldr": "VectorCDC\u901a\u8fc7\u5411\u91cf\u5316\u6307\u4ee4\u52a0\u901f\u4e86\u6570\u636e\u5220\u9664\u4e2d\u7684\u5185\u5bb9\u5206\u5757\u8fc7\u7a0b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u5185\u5bb9\u5b9a\u4e49\u5206\u5757\uff08CDC\uff09\u7b97\u6cd5\u662f\u5b9e\u73b0\u6570\u636e\u91cd\u590d\u5220\u9664\u7a7a\u95f4\u8282\u7701\u7684\u5173\u952e\uff0c\u4f46\u5176\u9010\u4e2a\u6587\u4ef6\u626b\u63cf\u7684\u7279\u6027\u5bfc\u81f4\u901f\u5ea6\u7f13\u6162\uff0c\u6210\u4e3a\u6570\u636e\u91cd\u590d\u5220\u9664\u6027\u80fd\u7684\u4e3b\u8981\u74f6\u9888\u3002", "method": "VectorCDC\u5229\u7528\u5411\u91cfCPU\u6307\u4ee4\uff08\u5982SSE/AVX\uff09\u6765\u52a0\u901f\u65e0\u54c8\u5e0c\u7684\u5185\u5bb9\u5b9a\u4e49\u5206\u5757\uff08CDC\uff09\u7b97\u6cd5\u3002", "result": "VectorCDC\u5728Intel\u3001AMD\u3001ARM\u548cIBM CPU\u4e0a\u5747\u5b9e\u73b0\u4e868.35\u500d\u81f326.2\u500d\u7684\u541e\u5410\u91cf\u63d0\u5347\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u5411\u91cf\u52a0\u901f\u6280\u672f\uff0c\u5e76\u4e14\u4e0d\u5f71\u54cd\u91cd\u590d\u6570\u636e\u5220\u9664\u7684\u7a7a\u95f4\u8282\u7701\u3002", "conclusion": "VectorCDC\u901a\u8fc7\u5229\u7528\u5411\u91cfCPU\u6307\u4ee4\uff08\u5982SSE/AVX\uff09\u6709\u6548\u52a0\u901f\u4e86\u65e0\u54c8\u5e0c\u7684\u5185\u5bb9\u5b9a\u4e49\u5206\u5757\uff08CDC\uff09\u7b97\u6cd5\uff0c\u5728Intel\u3001AMD\u3001ARM\u548cIBM CPU\u4e0a\u5747\u8868\u73b0\u51fa\u8272\uff0c\u541e\u5410\u91cf\u76f8\u6bd4\u73b0\u6709\u5411\u91cf\u52a0\u901f\u6280\u672f\u63d0\u9ad8\u4e868.35\u500d\u81f326.2\u500d\uff0c\u4e14\u4e0d\u5f71\u54cd\u91cd\u590d\u6570\u636e\u5220\u9664\u7684\u7a7a\u95f4\u8282\u7701\u6548\u679c\u3002"}}
{"id": "2508.05796", "categories": ["cond-mat.mes-hall", "cond-mat.mtrl-sci", "cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2508.05796", "abs": "https://arxiv.org/abs/2508.05796", "authors": ["Yanhong Gu", "Joseph Barker", "Jiemin Li", "Takashi Kikkawa", "Fernando Camino", "Kim Kisslinger", "John Sinsheimer", "Lukas Lienhard", "Jackson J. Bauer", "Caroline A. Ross", "Dmitri N. Basov", "Eiji Saitoh", "Jonathan Pelliciari", "Gerrit E. W. Bauer", "Valentina Bisogni"], "title": "Observing Differential Spin Currents by Resonant Inelastic X-ray Scattering", "comment": null, "summary": "Controlling spin currents, i.e., the flow of spin angular momentum, in small\nmagnetic devices is the principal objective of spin electronics, a main\ncontender for future energy efficient information technologies. Surprisingly, a\npure spin current has never been measured directly since the associated\nelectric stray fields and/or shifts in the non-equilibrium spin-dependent\ndistribution functions are too small for conventional experimental detection\nmethods optimized for charge transport. Here we report that resonant inelastic\nx-ray scattering (RIXS) can bridge this gap by measuring the spin current\ncarried by magnons -- the quanta of the spin wave excitations of the magnetic\norder -- in the presence of temperature gradients across a magnetic insulator.\nThis is possible due to the sensitivity of the momentum- and energy-resolved\nRIXS intensity to minute changes in the magnon distribution under\nnon-equilibrium conditions. We use the Boltzmann equation in the relaxation\ntime approximation to extract transport parameters, such as the magnon lifetime\nat finite momentum, essential for the realization of magnon spintronics.", "AI": {"tldr": "RIXS \u6280\u672f\u9996\u6b21\u76f4\u63a5\u6d4b\u91cf\u4e86\u78c1\u6027\u6750\u6599\u4e2d\u7684\u81ea\u65cb\u6d41\uff0c\u4e3a\u81ea\u65cb\u7535\u5b50\u5b66\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002", "motivation": "\u4e3a\u4e86\u5b9e\u73b0\u81ea\u65cb\u7535\u5b50\u5b66\u548c\u672a\u6765\u8282\u80fd\u4fe1\u606f\u6280\u672f\uff0c\u9700\u8981\u63a7\u5236\u5c0f\u78c1\u6027\u5668\u4ef6\u4e2d\u7684\u81ea\u65cb\u6d41\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u65e0\u6cd5\u76f4\u63a5\u6d4b\u91cf\u7eaf\u81ea\u65cb\u6d41\u3002", "method": "\u4f7f\u7528\u5171\u632f\u975e\u5f39\u6027 X \u5c04\u7ebf\u6563\u5c04\uff08RIXS\uff09\u6280\u672f\uff0c\u5e76\u7ed3\u5408\u73bb\u5c14\u5179\u66fc\u65b9\u7a0b\u5728\u5f1b\u8c6b\u65f6\u95f4\u8fd1\u4f3c\u4e0b\u63d0\u53d6\u8f93\u8fd0\u53c2\u6570\uff0c\u5982\u6709\u9650\u52a8\u91cf\u4e0b\u7684\u78c1\u632f\u5b50\u5bff\u547d\u3002", "result": "\u9996\u6b21\u4f7f\u7528 RIXS \u76f4\u63a5\u6d4b\u91cf\u4e86\u7531\u78c1\u632f\u5b50\u643a\u5e26\u7684\u81ea\u65cb\u6d41\uff0c\u5e76\u63d0\u53d6\u4e86\u81ea\u65cb\u6d41\u7684\u8f93\u8fd0\u53c2\u6570\uff0c\u4e3a\u78c1\u632f\u5b50\u81ea\u65cb\u7535\u5b50\u5b66\u7684\u53d1\u5c55\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "conclusion": "\u901a\u8fc7\u6d4b\u91cf\u6e29\u5ea6\u68af\u5ea6\u4e0b\u7684\u78c1\u7edd\u7f18\u4f53\u4e2d\u7531\u78c1\u632f\u5b50\u643a\u5e26\u7684\u81ea\u65cb\u6d41\uff0c\u7814\u7a76\u4eba\u5458\u53d1\u73b0 RIXS \u53ef\u4ee5\u76f4\u63a5\u6d4b\u91cf\u81ea\u65cb\u6d41\u3002"}}
{"id": "2508.05697", "categories": ["quant-ph", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.05697", "abs": "https://arxiv.org/abs/2508.05697", "authors": ["Marcos Guillermo Lammers", "Federico Hern\u00e1n Holik", "Alejandro Fern\u00e1ndez"], "title": "Quantum Resource Management in the NISQ Era: Implications and Perspectives from Software Engineering", "comment": "in Spanish language", "summary": "Quantum computers represent a radical technological breakthrough in\ninformation processing by leveraging the principles of quantum mechanics to\nsolve highly complex problems beyond the reach of classical systems. However,\nin the current NISQ era (noisy intermediate-scale quantum devices), the\navailable hardware presents several limitations, such as a limited number of\nqubits, high error rates, and short coherence times. Efficient management of\nquantum resources, both physical and logical, is especially relevant in the\ndesign and deployment of quantum algorithms. In this paper, we analyze the role\nof resources in current uses of NISQ devices, identifying their relevance and\nimplications for quantum software engineering. With this contribution, we aim\nto strengthen the field of Quantum Resource Estimation (QRE) and move toward\nscalable and reliable quantum software development", "AI": {"tldr": "Analyzes quantum resource management in NISQ devices to improve quantum software engineering and resource estimation.", "motivation": "To address the limitations of NISQ hardware (limited qubits, high error rates, short coherence times) and the need for efficient management of quantum resources in quantum algorithm design and deployment.", "method": "The paper analyzes the role of resources in current uses of NISQ devices, identifying their relevance and implications for quantum software engineering.", "result": "Identified the relevance and implications of resources in current NISQ device uses for quantum software engineering.", "conclusion": "The paper analyzes resource roles in NISQ devices for quantum software engineering, aiming to advance Quantum Resource Estimation for scalable and reliable quantum software development."}}
{"id": "2508.05722", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.05722", "abs": "https://arxiv.org/abs/2508.05722", "authors": ["Rania Al-Sabbagh"], "title": "PEACH: A sentence-aligned Parallel English-Arabic Corpus for Healthcare", "comment": null, "summary": "This paper introduces PEACH, a sentence-aligned parallel English-Arabic\ncorpus of healthcare texts encompassing patient information leaflets and\neducational materials. The corpus contains 51,671 parallel sentences, totaling\napproximately 590,517 English and 567,707 Arabic word tokens. Sentence lengths\nvary between 9.52 and 11.83 words on average. As a manually aligned corpus,\nPEACH is a gold-standard corpus, aiding researchers in contrastive linguistics,\ntranslation studies, and natural language processing. It can be used to derive\nbilingual lexicons, adapt large language models for domain-specific machine\ntranslation, evaluate user perceptions of machine translation in healthcare,\nassess patient information leaflets and educational materials' readability and\nlay-friendliness, and as an educational resource in translation studies. PEACH\nis publicly accessible.", "AI": {"tldr": "PEACH\u662f\u4e00\u4e2a\u516c\u5f00\u7684\u82f1\u6587-\u963f\u62c9\u4f2f\u6587\u5e73\u884c\u533b\u7597\u6587\u672c\u8bed\u6599\u5e93\uff0c\u5305\u542b51,671\u4e2a\u53e5\u5b50\u5bf9\uff0c\u53ef\u7528\u4e8e\u591a\u79cd\u7814\u7a76\u548c\u6559\u80b2\u76ee\u7684\u3002", "motivation": "\u4e3a\u4e86\u63d0\u4f9b\u4e00\u4e2a\u7528\u4e8e\u5bf9\u6bd4\u8bed\u8a00\u5b66\u3001\u7ffb\u8bd1\u7814\u7a76\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7684\u82f1\u6587-\u963f\u62c9\u4f2f\u6587\u5e73\u884c\u533b\u7597\u6587\u672c\u8bed\u6599\u5e93\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b51,671\u4e2a\u53e5\u5b50\u5bf9\u7684\u82f1\u6587-\u963f\u62c9\u4f2f\u6587\u5e73\u884c\u533b\u7597\u6587\u672c\u8bed\u6599\u5e93\uff0c\u5305\u542b\u60a3\u8005\u4fe1\u606f\u624b\u518c\u548c\u6559\u80b2\u6750\u6599\u3002\u8be5\u8bed\u6599\u5e93\u662f\u624b\u52a8\u5bf9\u9f50\u7684\uff0c\u5e76\u5728\u82f1\u6587\u548c\u963f\u62c9\u4f2f\u6587\u4e0a\u5206\u522b\u5305\u542b\u7ea6590,517\u548c567,707\u4e2a\u8bcd\u6807\u8bb0\u3002\u53e5\u5b50\u957f\u5ea6\u5e73\u5747\u57289.52\u523011.83\u4e2a\u5355\u8bcd\u4e4b\u95f4\u3002", "result": "\u6784\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aPEACH\u7684\u5e73\u884c\u8bed\u6599\u5e93\uff0c\u5305\u542b51,671\u4e2a\u53e5\u5b50\u5bf9\uff0c\u53ef\u7528\u4e8e\u751f\u6210\u53cc\u8bed\u8bcd\u5178\u3001\u5b9a\u5236\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3001\u8bc4\u4f30\u673a\u5668\u7ffb\u8bd1\u7684\u7528\u6237\u611f\u77e5\u3001\u8bc4\u4f30\u60a3\u8005\u4fe1\u606f\u548c\u6559\u80b2\u6750\u6599\u7684\u53ef\u8bfb\u6027\uff0c\u5e76\u4f5c\u4e3a\u7ffb\u8bd1\u7814\u7a76\u7684\u6559\u5b66\u8d44\u6e90\u3002", "conclusion": "PEACH\u662f\u4e00\u4e2a\u5305\u542b51,671\u4e2a\u53e5\u5b50\u5bf9\u7684\u5df2\u516c\u5f00\u7684\u82f1\u6587-\u963f\u62c9\u4f2f\u6587\u5e73\u884c\u533b\u7597\u6587\u672c\u8bed\u6599\u5e93\uff0c\u53ef\u7528\u4e8e\u5bf9\u6bd4\u8bed\u8a00\u5b66\u3001\u7ffb\u8bd1\u7814\u7a76\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff0c\u4ee5\u751f\u6210\u53cc\u8bed\u8bcd\u5178\u3001\u5b9a\u5236\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3001\u8bc4\u4f30\u673a\u5668\u7ffb\u8bd1\u7684\u7528\u6237\u611f\u77e5\u3001\u8bc4\u4f30\u60a3\u8005\u4fe1\u606f\u548c\u6559\u80b2\u6750\u6599\u7684\u53ef\u8bfb\u6027\uff0c\u5e76\u4f5c\u4e3a\u7ffb\u8bd1\u7814\u7a76\u7684\u6559\u5b66\u8d44\u6e90\u3002"}}
{"id": "2508.05827", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.05827", "abs": "https://arxiv.org/abs/2508.05827", "authors": ["Tony Kinchen", "Panagiotis Typaldos", "Andreas A. Malikopoulos"], "title": "A United Framework for Planning Electric Vehicle Charging Accessibility", "comment": null, "summary": "The shift towards electric vehicles (EVs) is crucial for establishing\nsustainable and low-emission urban transportation systems. However, the success\nof this transition depends on the strategic placement of the charging\ninfrastructure. This paper addresses the challenge of optimizing charging\nstation locations in dense urban environments while balancing efficiency with\nspatial accessibility. We propose an optimization framework that integrates\ntraffic simulation, energy consumption modeling, and a mobility equity measure\nto evaluate the social reach of each potential charging station. Using New York\nCity as a case study, we demonstrate consistent improvements in accessibility\n(15-20% reduction in travel time variability). Our results provide a scalable\nmethodology for incorporating equity considerations into EV infrastructure\nplanning, although economic factors and grid integration remain important areas\nfor future development.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u4f18\u5316\u57ce\u5e02\u7535\u52a8\u6c7d\u8f66\u5145\u7535\u6869\u7684\u5e03\u5c40\uff0c\u8003\u8651\u4e86\u6548\u7387\u548c\u516c\u5e73\u6027\uff0c\u5e76\u5728\u7ebd\u7ea6\u5e02\u7684\u6848\u4f8b\u7814\u7a76\u4e2d\u53d6\u5f97\u4e86\u79ef\u6781\u6210\u679c\u3002", "motivation": "\u6587\u7ae0\u65e8\u5728\u89e3\u51b3\u5728\u57ce\u5e02\u73af\u5883\u4e2d\u4f18\u5316\u7535\u52a8\u6c7d\u8f66\u5145\u7535\u6869\u5e03\u5c40\u7684\u6311\u6218\uff0c\u4ee5\u5e73\u8861\u6548\u7387\u548c\u7a7a\u95f4\u53ef\u53ca\u6027\uff0c\u63a8\u52a8\u53ef\u6301\u7eed\u548c\u4f4e\u6392\u653e\u7684\u57ce\u5e02\u4ea4\u901a\u7cfb\u7edf\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u4f18\u5316\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u6574\u5408\u4e86\u4ea4\u901a\u6a21\u62df\u3001\u80fd\u6e90\u6d88\u8017\u6a21\u578b\u4ee5\u53ca\u4e00\u4e2a\u8861\u91cf\u6f5c\u5728\u5145\u7535\u7ad9\u793e\u4f1a\u8986\u76d6\u8303\u56f4\u7684\u79fb\u52a8\u6027\u516c\u5e73\u6027\u6307\u6807\u3002", "result": "\u4f7f\u7528\u7ebd\u7ea6\u5e02\u4f5c\u4e3a\u6848\u4f8b\u7814\u7a76\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u80fd\u591f\u6301\u7eed\u6539\u5584\u53ef\u8fbe\u6027\uff08\u964d\u4f4e15-20%\u7684\u51fa\u884c\u65f6\u95f4\u53d8\u5f02\u6027\uff09\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u79cd\u5c06\u516c\u5e73\u6027\u8003\u91cf\u7eb3\u5165\u7535\u52a8\u6c7d\u8f66\u57fa\u7840\u8bbe\u65bd\u89c4\u5212\u7684\u53ef\u6269\u5c55\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u5408\u4ea4\u901a\u6a21\u62df\u3001\u80fd\u6e90\u6d88\u8017\u6a21\u578b\u548c\u79fb\u52a8\u6027\u516c\u5e73\u6027\u6307\u6807\u7684\u4f18\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u5316\u57ce\u5e02\u7535\u52a8\u6c7d\u8f66\u5145\u7535\u6869\u7684\u5e03\u5c40\uff0c\u5e76\u4ee5\u7ebd\u7ea6\u5e02\u4e3a\u4f8b\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u7ed3\u679c\u663e\u793a\u53ef\u8fbe\u6027\u5f97\u5230\u663e\u8457\u6539\u5584\uff08\u51fa\u884c\u65f6\u95f4\u53d8\u5f02\u6027\u964d\u4f4e15-20%\uff09\u3002"}}
{"id": "2508.05689", "categories": ["cs.CV", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.05689", "abs": "https://arxiv.org/abs/2508.05689", "authors": ["Jinjia Peng", "Zeze Tao", "Huibing Wang", "Meng Wang", "Yang Wang"], "title": "Boosting Adversarial Transferability via Residual Perturbation Attack", "comment": "Accepted to ieee/cvf international conference on computer vision\n  (ICCV2025)", "summary": "Deep neural networks are susceptible to adversarial examples while suffering\nfrom incorrect predictions via imperceptible perturbations. Transfer-based\nattacks create adversarial examples for surrogate models and transfer these\nexamples to target models under black-box scenarios. Recent studies reveal that\nadversarial examples in flat loss landscapes exhibit superior transferability\nto alleviate overfitting on surrogate models. However, the prior arts overlook\nthe influence of perturbation directions, resulting in limited transferability.\nIn this paper, we propose a novel attack method, named Residual Perturbation\nAttack (ResPA), relying on the residual gradient as the perturbation direction\nto guide the adversarial examples toward the flat regions of the loss function.\nSpecifically, ResPA conducts an exponential moving average on the input\ngradients to obtain the first moment as the reference gradient, which\nencompasses the direction of historical gradients. Instead of heavily relying\non the local flatness that stems from the current gradients as the perturbation\ndirection, ResPA further considers the residual between the current gradient\nand the reference gradient to capture the changes in the global perturbation\ndirection. The experimental results demonstrate the better transferability of\nResPA than the existing typical transfer-based attack methods, while the\ntransferability can be further improved by combining ResPA with the current\ninput transformation methods. The code is available at\nhttps://github.com/ZezeTao/ResPA.", "AI": {"tldr": "ResPA\u662f\u4e00\u79cd\u5229\u7528\u6b8b\u5dee\u68af\u5ea6\u7684\u65b0\u578b\u653b\u51fb\u65b9\u6cd5\uff0c\u53ef\u4ee5\u63d0\u9ad8\u9ed1\u76d2\u5bf9\u6297\u6027\u653b\u51fb\u7684\u8fc1\u79fb\u80fd\u529b\u3002", "motivation": "\u4e3a\u4e86\u63d0\u5347\u57fa\u4e8e\u8fc1\u79fb\u7684\u9ed1\u76d2\u653b\u51fb\u7684\u8fc1\u79fb\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u6270\u52a8\u65b9\u5411\u5f71\u54cd\u7684\u95ee\u9898\u3002", "method": "ResPA\u5229\u7528\u6b8b\u5dee\u68af\u5ea6\u4f5c\u4e3a\u6270\u52a8\u65b9\u5411\uff0c\u901a\u8fc7\u6307\u6570\u79fb\u52a8\u5e73\u5747\u8ba1\u7b97\u8f93\u5165\u68af\u5ea6\u7684\u4e00\u9636\u77e9\u4f5c\u4e3a\u53c2\u8003\u68af\u5ea6\uff0c\u5e76\u7ed3\u5408\u5f53\u524d\u68af\u5ea6\u4e0e\u53c2\u8003\u68af\u5ea6\u7684\u6b8b\u5dee\u6765\u6355\u6349\u5168\u5c40\u6270\u52a8\u65b9\u5411\u7684\u53d8\u5316\u3002", "result": "ResPA\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5176\u6bd4\u73b0\u6709\u7684\u5178\u578b\u57fa\u4e8e\u8fc1\u79fb\u7684\u653b\u51fb\u65b9\u6cd5\u5177\u6709\u66f4\u597d\u7684\u8fc1\u79fb\u80fd\u529b\uff0c\u5e76\u4e14\u53ef\u4ee5\u901a\u8fc7\u7ed3\u5408\u5f53\u524d\u8f93\u5165\u8f6c\u6362\u65b9\u6cd5\u8fdb\u4e00\u6b65\u63d0\u9ad8\u8fc1\u79fb\u80fd\u529b\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u6b8b\u5dee\u6270\u52a8\u653b\u51fb\uff08ResPA\uff09\u7684\u65b0\u578b\u653b\u51fb\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u6b8b\u5dee\u68af\u5ea6\u4f5c\u4e3a\u6270\u52a8\u65b9\u5411\uff0c\u5f15\u5bfc\u5bf9\u6297\u6837\u672c\u671d\u7740\u635f\u5931\u51fd\u6570\u7684\u5e73\u5766\u533a\u57df\uff0c\u5e76\u5728\u5b9e\u9645\u6d4b\u8bd5\u4e2d\u5c55\u73b0\u51fa\u6bd4\u73b0\u6709\u57fa\u4e8e\u8fc1\u79fb\u7684\u653b\u51fb\u65b9\u6cd5\u66f4\u4f18\u8d8a\u7684\u8fc1\u79fb\u80fd\u529b\uff0c\u7ed3\u5408\u5f53\u524d\u8f93\u5165\u8f6c\u6362\u65b9\u6cd5\u53ef\u8fdb\u4e00\u6b65\u63d0\u5347\u8fc1\u79fb\u80fd\u529b\u3002"}}
{"id": "2508.05773", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.05773", "abs": "https://arxiv.org/abs/2508.05773", "authors": ["Keyvan Majd", "Hardik Parwana", "Bardh Hoxha", "Steven Hong", "Hideki Okamoto", "Georgios Fainekos"], "title": "GPU-Accelerated Barrier-Rate Guided MPPI Control for Tractor-Trailer Systems", "comment": "Accepted to IEEE ITSC 2025", "summary": "Articulated vehicles such as tractor-trailers, yard trucks, and similar\nplatforms must often reverse and maneuver in cluttered spaces where pedestrians\nare present. We present how Barrier-Rate guided Model Predictive Path Integral\n(BR-MPPI) control can solve navigation in such challenging environments.\nBR-MPPI embeds Control Barrier Function (CBF) constraints directly into the\npath-integral update. By steering the importance-sampling distribution toward\ncollision-free, dynamically feasible trajectories, BR-MPPI enhances the\nexploration strength of MPPI and improves robustness of resulting trajectories.\nThe method is evaluated in the high-fidelity CarMaker simulator on a 12 [m]\ntractor-trailer tasked with reverse and forward parking in a parking lot.\nBR-MPPI computes control inputs in above 100 [Hz] on a single GPU (for\nscenarios with eight obstacles) and maintains better parking clearance than a\nstandard MPPI baseline and an MPPI with collision cost baseline.", "AI": {"tldr": "BR-MPPI\u662f\u4e00\u79cd\u6539\u8fdb\u7684\u5bfc\u822a\u63a7\u5236\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408CBF\u548cMPPI\uff0c\u63d0\u9ad8\u4e86\u94f0\u63a5\u8f66\u8f86\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u5b89\u5168\u6027\u548c\u64cd\u63a7\u6027\u3002", "motivation": "\u89e3\u51b3\u94f0\u63a5\u8f66\u8f86\u5728\u62e5\u6324\u7a7a\u95f4\uff08\u5982\u505c\u8f66\u573a\uff09\u8fdb\u884c\u5012\u8f66\u548c\u673a\u52a8\u65f6\uff0c\u9700\u8981\u8003\u8651\u884c\u4eba\u548c\u52a8\u6001\u53ef\u884c\u6027\u7684\u5bfc\u822a\u95ee\u9898\u3002", "method": "BR-MPPI\uff08Barrier-Rate guided Model Predictive Path Integral\uff09\u63a7\u5236\uff0c\u5c06CBF\u7ea6\u675f\u76f4\u63a5\u5d4c\u5165\u5230\u8def\u5f84\u79ef\u5206\u66f4\u65b0\u4e2d\uff0c\u4ee5\u5f15\u5bfc\u91cd\u8981\u6027\u91c7\u6837\u5206\u5e03\u8d8b\u5411\u65e0\u78b0\u649e\u3001\u52a8\u529b\u5b66\u53ef\u884c\u7684\u8f68\u8ff9\u3002", "result": "\u5728CarMaker\u6a21\u62df\u5668\u4e2d\uff0cBR-MPPI\u6210\u529f\u5b9e\u73b0\u4e8612\u7c73\u7275\u5f15\u8f66\u62d6\u8f66\u7684\u5012\u8f66\u548c\u524d\u8fdb\u505c\u8f66\uff0c\u8ba1\u7b97\u901f\u5ea6\u8d85\u8fc7100 Hz\uff08\u5e268\u4e2a\u969c\u788d\u7269\uff09\uff0c\u5e76\u4e14\u6bd4\u6807\u51c6MPPI\u548c\u5e26\u78b0\u649e\u6210\u672c\u7684MPPI\u5177\u6709\u66f4\u597d\u7684\u505c\u8f66\u95f4\u9699\u3002", "conclusion": "BR-MPPI\u901a\u8fc7\u5c06CBF\u7ea6\u675f\u5d4c\u5165\u8def\u5f84\u79ef\u5206\u66f4\u65b0\u4e2d\uff0c\u63d0\u9ad8\u4e86MPPI\u7684\u63a2\u7d22\u80fd\u529b\u548c\u8f68\u8ff9\u9c81\u68d2\u6027\uff0c\u5728\u6a21\u62df\u5668\u4e2d\u8868\u73b0\u4f18\u4e8e\u6807\u51c6MPPI\u548c\u5e26\u78b0\u649e\u6210\u672c\u7684MPPI\u3002"}}
{"id": "2508.05731", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.05731", "abs": "https://arxiv.org/abs/2508.05731", "authors": ["Yuhang Liu", "Zeyu Liu", "Shuanghe Zhu", "Pengxiang Li", "Congkai Xie", "Jiasheng Wang", "Xueyu Hu", "Xiaotian Han", "Jianbo Yuan", "Xinyao Wang", "Shengyu Zhang", "Hongxia Yang", "Fei Wu"], "title": "InfiGUI-G1: Advancing GUI Grounding with Adaptive Exploration Policy Optimization", "comment": "11 pages, 3 figures", "summary": "The emergence of Multimodal Large Language Models (MLLMs) has propelled the\ndevelopment of autonomous agents that operate on Graphical User Interfaces\n(GUIs) using pure visual input. A fundamental challenge is robustly grounding\nnatural language instructions. This requires a precise spatial alignment, which\naccurately locates the coordinates of each element, and, more critically, a\ncorrect semantic alignment, which matches the instructions to the functionally\nappropriate UI element. Although Reinforcement Learning with Verifiable Rewards\n(RLVR) has proven to be effective at improving spatial alignment for these\nMLLMs, we find that inefficient exploration bottlenecks semantic alignment,\nwhich prevent models from learning difficult semantic associations. To address\nthis exploration problem, we present Adaptive Exploration Policy Optimization\n(AEPO), a new policy optimization framework. AEPO employs a multi-answer\ngeneration strategy to enforce broader exploration, which is then guided by a\ntheoretically grounded Adaptive Exploration Reward (AER) function derived from\nfirst principles of efficiency eta=U/C. Our AEPO-trained models, InfiGUI-G1-3B\nand InfiGUI-G1-7B, establish new state-of-the-art results across multiple\nchallenging GUI grounding benchmarks, achieving significant relative\nimprovements of up to 9.0% against the naive RLVR baseline on benchmarks\ndesigned to test generalization and semantic understanding. Resources are\navailable at https://github.com/InfiXAI/InfiGUI-G1.", "AI": {"tldr": "A new framework called AEPO improves how autonomous agents understand instructions for controlling GUIs by enhancing exploration and semantic alignment, leading to better performance on benchmarks.", "motivation": "Existing Reinforcement Learning with Verifiable Rewards (RLVR) for MLLMs struggles with semantic alignment in GUI grounding due to inefficient exploration, hindering the learning of difficult semantic associations.", "method": "Proposed Adaptive Exploration Policy Optimization (AEPO) framework with a multi-answer generation strategy for broader exploration, guided by a theoretically grounded Adaptive Exploration Reward (AER) function (eta=U/C).", "result": "AEPO achieves significant relative improvements of up to 9.0% against the naive RLVR baseline on benchmarks testing generalization and semantic understanding.", "conclusion": "AEPO-trained models InfiGUI-G1-3B and InfiGUI-G1-7B set new state-of-the-art results on challenging GUI grounding benchmarks, with up to 9.0% improvement over RLVR baseline in generalization and semantic understanding."}}
{"id": "2508.05798", "categories": ["cs.LO", "cs.CL", "math.LO", "quant-ph"], "pdf": "https://arxiv.org/pdf/2508.05798", "abs": "https://arxiv.org/abs/2508.05798", "authors": ["Yuri Gurevich"], "title": "Basic interactive algorithms: Preview", "comment": null, "summary": "This dialog paper offers a preview and provides a foretaste of an upcoming\nwork on the axiomatization of basic interactive algorithms.\n  The modern notion of algorithm was elucidated in the 1930s--1950s. It was\naxiomatized a quarter of a century ago as the notion of ``sequential\nalgorithm'' or ``classical algorithm''; we prefer to call it ``basic algorithm\"\nnow. The axiomatization was used to show that for every basic algorithm there\nis a behaviorally equivalent abstract state machine. It was also used to prove\nthe Church-Turing thesis as it has been understood by the logicians.\n  Starting from the 1960s, the notion of algorithm has expanded --\nprobabilistic algorithms, quantum algorithms, etc. -- prompting introduction of\na much more ambitious version of the Church-Turing thesis commonly known as the\n``physical thesis.'' We emphasize the difference between the two versions of\nthe Church-Turing thesis and illustrate how nondeterministic and probabilistic\nalgorithms can be viewed as basic algorithms with appropriate oracles. The same\nview applies to quantum circuit algorithms and many other classes of\nalgorithms.", "AI": {"tldr": "This paper previews future work on axiomatizing interactive algorithms. It revisits the axiomatization of 'basic' (classical) algorithms and links it to the Church-Turing thesis. The authors propose viewing probabilistic, quantum, and other advanced algorithms as extensions of basic algorithms with oracles.", "motivation": "The motivation is to provide a preview of an upcoming work on the axiomatization of basic interactive algorithms and to clarify the relationship between the classical notion of algorithms and more modern ones like probabilistic and quantum algorithms, including their connection to the Church-Turing thesis.", "method": "The paper reviews the historical development of the notion of algorithm, starting from its elucidation in the 1930s-1950s and its subsequent axiomatization as 'basic algorithm'. It then discusses the expansion of the notion of algorithm to include probabilistic and quantum algorithms, and how these can be viewed as basic algorithms with oracles.", "result": "The paper illustrates how nondeterministic and probabilistic algorithms, as well as quantum circuit algorithms, can be viewed as basic algorithms augmented with oracles. This perspective helps in understanding the broader landscape of algorithmic computation.", "conclusion": "The paper provides a preview of future work on the axiomatization of basic interactive algorithms, drawing parallels with the axiomatization of sequential algorithms and its implications for the Church-Turing thesis. It also frames various advanced algorithms like probabilistic, quantum, and nondeterministic ones as extensions of basic algorithms with oracles."}}
{"id": "2508.05786", "categories": ["cs.NE"], "pdf": "https://arxiv.org/pdf/2508.05786", "abs": "https://arxiv.org/abs/2508.05786", "authors": ["Yang Li", "Luopeiwen Yi", "Tananun Songdechakraiwut"], "title": "Functional Connectivity Graph Neural Networks", "comment": "26 pages, 5 figures, 24 tables", "summary": "Real-world networks often benefit from capturing both local and global\ninteractions. Inspired by multi-modal analysis in brain imaging, where\nstructural and functional connectivity offer complementary views of network\norganization, we propose a graph neural network framework that generalizes this\napproach to other domains. Our method introduces a functional connectivity\nblock based on persistent graph homology to capture global topological\nfeatures. Combined with structural information, this forms a multi-modal\narchitecture called Functional Connectivity Graph Neural Networks. Experiments\nshow consistent performance gains over existing methods, demonstrating the\nvalue of brain-inspired representations for graph-level classification across\ndiverse networks.", "AI": {"tldr": "\u53d7\u5927\u8111\u6210\u50cf\u591a\u6a21\u6001\u5206\u6790\u7684\u542f\u53d1\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7ed3\u6784\u548c\u529f\u80fd\u8fde\u63a5\uff08\u57fa\u4e8e\u6301\u4e45\u56fe\u540c\u6e90\u6027\uff09\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u5728\u56fe\u7ea7\u5206\u7c7b\u4efb\u52a1\u4e0a\u7684\u6709\u6548\u6027\u3002", "motivation": "\u771f\u5b9e\u4e16\u754c\u7684\u7f51\u7edc\u901a\u5e38\u53d7\u76ca\u4e8e\u6355\u83b7\u5c40\u90e8\u548c\u5168\u5c40\u4ea4\u4e92\u3002\u53d7\u5927\u8111\u6210\u50cf\u4e2d\u591a\u6a21\u6001\u5206\u6790\u7684\u542f\u53d1\uff0c\u5176\u4e2d\u7ed3\u6784\u548c\u529f\u80fd\u8fde\u63a5\u63d0\u4f9b\u4e86\u7f51\u7edc\u7ec4\u7ec7\u7684\u4e92\u8865\u89c6\u56fe\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u6b64\u65b9\u6cd5\u63a8\u5e7f\u5230\u5176\u4ed6\u9886\u57df\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6301\u4e45\u56fe\u540c\u6e90\u6027\u7684\u529f\u80fd\u8fde\u63a5\u5757\uff0c\u4ee5\u6355\u83b7\u5168\u5c40\u62d3\u6251\u7279\u5f81\u3002\u4e0e\u7ed3\u6784\u4fe1\u606f\u76f8\u7ed3\u5408\uff0c\u5f62\u6210\u4e86\u51fd\u6570\u8fde\u63a5\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u591a\u6a21\u6001\u67b6\u6784\u3002", "result": "\u5b9e\u9a8c\u4e00\u81f4\u5730\u63d0\u9ad8\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u53d7\u5927\u8111\u542f\u53d1\u7684\u8868\u793a\u5bf9\u4e8e\u8de8\u4e0d\u540c\u7f51\u7edc\u7684\u56fe\u7ea7\u5206\u7c7b\u7684\u4ef7\u503c\u3002", "conclusion": "\u5b9e\u9a8c\u4e00\u81f4\u5730\u63d0\u9ad8\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u53d7\u5927\u8111\u542f\u53d1\u7684\u8868\u793a\u5bf9\u4e8e\u8de8\u4e0d\u540c\u7f51\u7edc\u7684\u56fe\u7ea7\u5206\u7c7b\u7684\u4ef7\u503c\u3002"}}
{"id": "2508.05659", "categories": ["cs.LG", "stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.05659", "abs": "https://arxiv.org/abs/2508.05659", "authors": ["Jeroen F. Uleman", "Loes Crielaard", "Leonie K. Elsenburg", "Guido A. Veldhuis", "Karien Stronks", "Naja Hulvej Rod", "Rick Quax", "V\u00edtor V. Vasconcelos"], "title": "Diagrams-to-Dynamics (D2D): Exploring Causal Loop Diagram Leverage Points under Uncertainty", "comment": "21 pages, 4 figures, 4 tables", "summary": "Causal loop diagrams (CLDs) are widely used in health and environmental\nresearch to represent hypothesized causal structures underlying complex\nproblems. However, as qualitative and static representations, CLDs are limited\nin their ability to support dynamic analysis and inform intervention\nstrategies. Additionally, quantitative CLD analysis methods like network\ncentrality analysis often lead to false inference. We propose\nDiagrams-to-Dynamics (D2D), a method for converting CLDs into exploratory\nsystem dynamics models (SDMs) in the absence of empirical data. With minimal\nuser input - following a protocol to label variables as stocks,\nflows/auxiliaries, or constants - D2D leverages the structural information\nalready encoded in CLDs, namely, link existence and polarity, to simulate\nhypothetical interventions and explore potential leverage points under\nuncertainty. Results suggest that D2D helps distinguish between high- and\nlow-ranked leverage points. We compare D2D to a data-driven SDM constructed\nfrom the same CLD and variable labeling. D2D showed greater consistency with\nthe data-driven model than network centrality analysis, while providing\nuncertainty estimates and guidance for future data collection. The method is\nimplemented in an open-source Python package and a web-based application to\nsupport further testing and lower the barrier to dynamic modeling for\nresearchers working with CLDs. We expect additional validation will further\nestablish the approach's utility across a broad range of cases and domains.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.05779", "categories": ["cs.AR", "quant-ph"], "pdf": "https://arxiv.org/pdf/2508.05779", "abs": "https://arxiv.org/abs/2508.05779", "authors": ["Pengyu Liu", "Mingkuan Xu", "Hengyun Zhou", "Hanrui Wang", "Umut A. Acar", "Yunong Shi"], "title": "ConiQ: Enabling Concatenated Quantum Error Correction on Neutral Atom Arrays", "comment": null, "summary": "Recent progress on concatenated codes, especially many-hypercube codes,\nachieves unprecedented space efficiency. Yet two critical challenges persist in\npractice. First, these codes lack efficient implementations of addressable\nlogical gates. Second, the required high degree of parallelism and long-range\ninteractions pose significant challenges for current hardware platforms. In\nthis paper, we propose an efficient compilation approach for concatenated\ncodes, specifically many-hypercube codes, targeted at neutral atom arrays,\nwhich provide the necessary parallelism and long-range interactions. Our\napproach builds on two key innovations. First, we introduce\nAutomorphism-assisted Hierarchical Addressing (AHA) logical CNOT gates that\nsignificantly reduce spacetime overhead compared to conventional\ndistillation-based methods. Second, we develop Virtual Atom Intermediate\nRepresentation (VAIR) that enables level-wise optimization and legalization. We\nimplement these innovations in ConiQ, a hardware-aware quantum compiler\ndesigned to compile fault-tolerant quantum circuits for neutral atom arrays\nusing many-hypercube codes. Our evaluation demonstrates that ConiQ achieves up\nto 2000x reduction in spacetime overhead and up to 10^6x reduction in\ncompilation time compared to state-of-the-art compilers, with our AHA gates\nproviding an additional overhead reduction of up to 20x. These results\nestablish concatenated codes as a promising approach for fault-tolerant quantum\ncomputing in the near future.", "AI": {"tldr": "A new quantum compiler, ConiQ, uses innovative techniques (AHA gates and VAIR) to drastically reduce the resources needed for quantum computations using concatenated codes on neutral atom hardware, making fault-tolerant quantum computing more feasible.", "motivation": "Two main challenges hinder the practical application of concatenated codes, especially many-hypercube codes, despite their space efficiency: the lack of efficient implementations for addressable logical gates and the hardware challenges posed by their high degree of parallelism and long-range interactions.", "method": "The paper proposes an efficient compilation approach for concatenated codes, specifically many-hypercube codes, targeting neutral atom arrays. This approach utilizes Automorphism-assisted Hierarchical Addressing (AHA) for logical CNOT gates and a Virtual Atom Intermediate Representation (VAIR) for level-wise optimization and legalization. These innovations are implemented in a hardware-aware quantum compiler named ConiQ.", "result": "The evaluation shows that ConiQ, using AHA gates, achieves up to 2000x reduction in spacetime overhead and up to 10^6x reduction in compilation time compared to state-of-the-art compilers. AHA gates alone provide an additional overhead reduction of up to 20x.", "conclusion": "Concatenated codes, particularly many-hypercube codes, are a promising avenue for near-term fault-tolerant quantum computing, thanks to an efficient compilation approach using AHA logical CNOT gates and VAIR, which significantly reduces spacetime overhead and compilation time for neutral atom arrays."}}
{"id": "2508.05778", "categories": ["cs.LG", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2508.05778", "abs": "https://arxiv.org/abs/2508.05778", "authors": ["Jaemin Oh", "Jinsil Lee", "Youngjoon Hong"], "title": "Machine Learning-Based Nonlinear Nudging for Chaotic Dynamical Systems", "comment": "21 pages, 5 figures, 6 tables", "summary": "Nudging is an empirical data assimilation technique that incorporates an\nobservation-driven control term into the model dynamics. The trajectory of the\nnudged system approaches the true system trajectory over time, even when the\ninitial conditions differ. For linear state space models, such control terms\ncan be derived under mild assumptions. However, designing effective nudging\nterms becomes significantly more challenging in the nonlinear setting. In this\nwork, we propose neural network nudging, a data-driven method for learning\nnudging terms in nonlinear state space models. We establish a theoretical\nexistence result based on the Kazantzis--Kravaris--Luenberger observer theory.\nThe proposed approach is evaluated on three benchmark problems that exhibit\nchaotic behavior: the Lorenz 96 model, the Kuramoto--Sivashinsky equation, and\nthe Kolmogorov flow.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u795e\u7ecf\u7f51\u7edc\u4e2d\u56fd\u7684, \u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\u6765\u5b66\u4e60\u975e\u7ebf\u6027\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u4e2d\u7684\u4e2d\u56fd\u9879\uff0c\u5e76\u4f7f\u7528\u4e09\u79cd\u8868\u73b0\u51fa\u6df7\u6c8c\u884c\u4e3a\u7684\u57fa\u51c6\u95ee\u9898\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "motivation": "\u8bbe\u8ba1\u6709\u6548\u7684\u4e2d\u56fd\u9879\u5728\u975e\u7ebf\u6027\u73af\u5883\u4e2d\u66f4\u5177\u6311\u6218\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u795e\u7ecf\u7f51\u7edc\u4e2d\u56fd\u7684, \u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\u6765\u5b66\u4e60\u975e\u7ebf\u6027\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u4e2d\u7684\u4e2d\u56fd\u9879\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4e09\u79cd\u8868\u73b0\u51fa\u6df7\u6c8c\u884c\u4e3a\u7684\u57fa\u51c6\u95ee\u9898\uff08\u6d1b\u4f26\u5179 96 \u6a21\u578b\u3001\u5e93\u5c14\u83ab\u6208\u7f57\u592b-\u897f\u74e6\u6b23\u65af\u57fa\u65b9\u7a0b\u548c\u79d1\u5c14\u83ab\u6208\u7f57\u592b\u6d41\u52a8\uff09\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u975e\u7ebf\u6027\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u7684\u795e\u7ecf\u7f51\u7edc\u4e2d\u56fd\u7684, \u5efa\u7acb\u4e86\u4e00\u4e2a\u57fa\u4e8e Kazantzis-Kravaris-Luenberger \u89c2\u6d4b\u5668\u7406\u8bba\u7684\u7406\u8bba\u5b58\u5728\u7ed3\u679c\u3002"}}
{"id": "2508.05685", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2508.05685", "abs": "https://arxiv.org/abs/2508.05685", "authors": ["Yara Bahram", "Mohammadhadi Shateri", "Eric Granger"], "title": "DogFit: Domain-guided Fine-tuning for Efficient Transfer Learning of Diffusion Models", "comment": "Currently under review. Code will be released upon acceptance", "summary": "Transfer learning of diffusion models to smaller target domains is\nchallenging, as naively fine-tuning the model often results in poor\ngeneralization. Test-time guidance methods help mitigate this by offering\ncontrollable improvements in image fidelity through a trade-off with sample\ndiversity. However, this benefit comes at a high computational cost, typically\nrequiring dual forward passes during sampling. We propose the Domain-guided\nFine-tuning (DogFit) method, an effective guidance mechanism for diffusion\ntransfer learning that maintains controllability without incurring additional\ncomputational overhead. DogFit injects a domain-aware guidance offset into the\ntraining loss, effectively internalizing the guided behavior during the\nfine-tuning process. The domain-aware design is motivated by our observation\nthat during fine-tuning, the unconditional source model offers a stronger\nmarginal estimate than the target model. To support efficient controllable\nfidelity-diversity trade-offs at inference, we encode the guidance strength\nvalue as an additional model input through a lightweight conditioning\nmechanism. We further investigate the optimal placement and timing of the\nguidance offset during training and propose two simple scheduling strategies,\ni.e., late-start and cut-off, which improve generation quality and training\nstability. Experiments on DiT and SiT backbones across six diverse target\ndomains show that DogFit can outperform prior guidance methods in transfer\nlearning in terms of FID and FDDINOV2 while requiring up to 2x fewer sampling\nTFLOPS.", "AI": {"tldr": "DogFit \u662f\u4e00\u79cd\u65b0\u7684\u6269\u6563\u6a21\u578b\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u4e0d\u589e\u52a0\u8ba1\u7b97\u6210\u672c\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u53ef\u63a7\u6027\u3002\u5b83\u901a\u8fc7\u5c06\u9886\u57df\u611f\u77e5\u5f15\u5bfc\u504f\u79fb\u91cf\u6ce8\u5165\u8bad\u7ec3\u635f\u5931\u6765\u5b9e\u73b0\u8fd9\u4e00\u70b9\uff0c\u5e76\u4f7f\u7528\u8f7b\u91cf\u7ea7\u6761\u4ef6\u673a\u5236\u6765\u7f16\u7801\u5f15\u5bfc\u5f3a\u5ea6\u3002\u5b9e\u9a8c\u8868\u660e\uff0cDogFit \u5728\u56fe\u50cf\u8d28\u91cf\u548c\u6548\u7387\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5c06\u9884\u8bad\u7ec3\u7684\u6269\u6563\u6a21\u578b\u8fc1\u79fb\u5230\u65b0\u7684\u9886\u57df\u65f6\uff0c\u5fae\u8c03\u5f80\u5f80\u4f1a\u5bfc\u81f4\u6cdb\u5316\u80fd\u529b\u4e0b\u964d\u3002\u73b0\u6709\u7684\u6d4b\u8bd5\u65f6\u5f15\u5bfc\u65b9\u6cd5\u867d\u7136\u53ef\u4ee5\u6539\u5584\u56fe\u50cf\u4fdd\u771f\u5ea6\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u3002DogFit \u7684\u52a8\u673a\u662f\u5728\u4e0d\u589e\u52a0\u8ba1\u7b97\u5f00\u9500\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u53ef\u63a7\u7684\u8fc1\u79fb\u5b66\u4e60\u3002", "method": "DogFit \u901a\u8fc7\u5c06\u9886\u57df\u611f\u77e5\u5f15\u5bfc\u504f\u79fb\u91cf\u6ce8\u5165\u8bad\u7ec3\u635f\u5931\uff0c\u5728\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u6709\u6548\u5185\u5316\u5f15\u5bfc\u884c\u4e3a\u3002\u901a\u8fc7\u5c06\u5f15\u5bfc\u5f3a\u5ea6\u503c\u4f5c\u4e3a\u989d\u5916\u6a21\u578b\u8f93\u5165\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u53ef\u63a7\u4fdd\u771f\u5ea6-\u591a\u6837\u6027\u6743\u8861\u3002\u7814\u7a76\u4e86\u5f15\u5bfc\u504f\u79fb\u91cf\u5728\u8bad\u7ec3\u4e2d\u7684\u6700\u4f73\u653e\u7f6e\u548c\u65f6\u673a\uff0c\u5e76\u63d0\u51fa\u4e86 late-start \u548c cut-off \u4e24\u79cd\u8c03\u5ea6\u7b56\u7565\u3002", "result": "DogFit \u5728 DiT \u548c SiT \u4e3b\u5e72\u7f51\u7edc\u4ee5\u53ca\u516d\u4e2a\u4e0d\u540c\u7684\u76ee\u6807\u9886\u57df\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\u3002\u7ed3\u679c\u8868\u660e\uff0cDogFit \u5728 FID \u548c FDDINOV2 \u65b9\u9762\u4f18\u4e8e\u5148\u524d\u7684\u65b9\u6cd5\uff0c\u540c\u65f6\u91c7\u6837 TFLOPS \u6700\u591a\u53ef\u51cf\u5c11 2 \u500d\u3002", "conclusion": "DogFit \u901a\u8fc7\u5c06\u9886\u57df\u611f\u77e5\u5f15\u5bfc\u504f\u79fb\u91cf\u6ce8\u5165\u8bad\u7ec3\u635f\u5931\uff0c\u5728\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u6709\u6548\u5185\u5316\u5f15\u5bfc\u884c\u4e3a\uff0c\u4ece\u800c\u5728\u4e0d\u4ea7\u751f\u989d\u5916\u8ba1\u7b97\u5f00\u9500\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u6269\u6563\u8fc1\u79fb\u5b66\u4e60\u4e2d\u5b9e\u73b0\u4e86\u53ef\u63a7\u6027\u3002\u901a\u8fc7\u5c06\u5f15\u5bfc\u5f3a\u5ea6\u503c\u4f5c\u4e3a\u989d\u5916\u6a21\u578b\u8f93\u5165\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u53ef\u63a7\u4fdd\u771f\u5ea6-\u591a\u6837\u6027\u6743\u8861\u3002\u5b9e\u9a8c\u8868\u660e\uff0cDogFit \u5728\u8fc1\u79fb\u5b66\u4e60\u65b9\u9762\u7684 FID \u548c FDDINOV2 \u65b9\u9762\u4f18\u4e8e\u5148\u524d\u7684\u65b9\u6cd5\uff0c\u540c\u65f6\u91c7\u6837 TFLOPS \u6700\u591a\u53ef\u51cf\u5c11 2 \u500d\u3002"}}
{"id": "2508.05762", "categories": ["cond-mat.mtrl-sci", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.05762", "abs": "https://arxiv.org/abs/2508.05762", "authors": ["Sajid Mannan", "Vaibhav Bihani", "Carmelo Gonzales", "Kin Long Kelvin Lee", "Nitya Nand Gosvami", "Sayan Ranu", "Santiago Miret", "N M Anoop Krishnan"], "title": "Evaluating Universal Machine Learning Force Fields Against Experimental Measurements", "comment": null, "summary": "Universal machine learning force fields (UMLFFs) promise to revolutionize\nmaterials science by enabling rapid atomistic simulations across the periodic\ntable. However, their evaluation has been limited to computational benchmarks\nthat may not reflect real-world performance. Here, we present UniFFBench, a\ncomprehensive framework for evaluating UMLFFs against experimental measurements\nof ~1,500 carefully curated mineral structures spanning diverse chemical\nenvironments, bonding types, structural complexity, and elastic properties. Our\nsystematic evaluation of six state-of-the-art UMLFFs reveals a substantial\nreality gap: models achieving impressive performance on computational\nbenchmarks often fail when confronted with experimental complexity. Even the\nbest-performing models exhibit higher density prediction error than the\nthreshold required for practical applications. Most strikingly, we observe\ndisconnects between simulation stability and mechanical property accuracy, with\nprediction errors correlating with training data representation rather than the\nmodeling method. These findings demonstrate that while current computational\nbenchmarks provide valuable controlled comparisons, they may overestimate model\nreliability when extrapolated to experimentally complex chemical spaces.\nAltogether, UniFFBench establishes essential experimental validation standards\nand reveals systematic limitations that must be addressed to achieve truly\nuniversal force field capabilities.", "AI": {"tldr": "UniFFBench \u6846\u67b6\u8bc4\u4f30\u4e86\u901a\u7528\u673a\u5668\u5b66\u4e60\u529b\u573a (UMLFF) \u7684\u5b9e\u9645\u6027\u80fd\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u8ba1\u7b97\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u5b9e\u9a8c\u6570\u636e\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u9645\u5e94\u7528\u9700\u6c42\u3002\u7814\u7a76\u6307\u51fa\u4e86\u6a21\u578b\u5728\u8bad\u7ec3\u6570\u636e\u8868\u793a\u548c\u6a21\u62df\u7a33\u5b9a\u6027\u65b9\u9762\u5b58\u5728\u7cfb\u7edf\u6027\u5c40\u9650\u6027\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u6539\u8fdb\u4ee5\u5b9e\u73b0\u771f\u6b63\u7684\u901a\u7528\u529b\u573a\u3002", "motivation": "\u901a\u7528\u673a\u5668\u5b66\u4e60\u529b\u573a (UMLFF) \u6709\u6f5c\u529b\u901a\u8fc7\u5b9e\u73b0\u8de8\u5143\u7d20\u5468\u671f\u7684\u5feb\u901f\u539f\u5b50\u6a21\u62df\u6765\u5f7b\u5e95\u6539\u53d8\u6750\u6599\u79d1\u5b66\u3002\u7136\u800c\uff0c\u5b83\u4eec\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u6027\u80fd\u8bc4\u4f30\u6709\u9650\uff0c\u4ec5\u9650\u4e8e\u53ef\u80fd\u65e0\u6cd5\u53cd\u6620\u5b9e\u9645\u6027\u80fd\u7684\u8ba1\u7b97\u57fa\u51c6\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86 UniFFBench \u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u5bf9\u7ea6 1500 \u79cd\u7cbe\u5fc3\u7b56\u5212\u7684\u3001\u6db5\u76d6\u4e0d\u540c\u5316\u5b66\u73af\u5883\u3001\u952e\u5408\u7c7b\u578b\u3001\u7ed3\u6784\u590d\u6742\u6027\u548c\u5f39\u6027\u6027\u8d28\u7684\u77ff\u7269\u7ed3\u6784\u8fdb\u884c\u5b9e\u9a8c\u6d4b\u91cf\uff0c\u6765\u8bc4\u4f30\u901a\u7528\u673a\u5668\u5b66\u4e60\u529b\u573a (UMLFF)\u3002\u7814\u7a76\u7cfb\u7edf\u6027\u5730\u8bc4\u4f30\u4e86\u516d\u79cd\u6700\u5148\u8fdb\u7684 UMLFF\uff0c\u5e76\u5206\u6790\u4e86\u6a21\u62df\u7a33\u5b9a\u6027\u4e0e\u673a\u68b0\u6027\u80fd\u51c6\u786e\u6027\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u8ba1\u7b97\u57fa\u51c6\u4e0a\u8868\u73b0\u51fa\u8272\u7684\u6a21\u578b\u5728\u9762\u5bf9\u5b9e\u9a8c\u590d\u6742\u6027\u65f6\u5e38\u5e38\u5931\u8d25\uff0c\u5373\u4f7f\u662f\u8868\u73b0\u6700\u597d\u7684\u6a21\u578b\uff0c\u5176\u5bc6\u5ea6\u9884\u6d4b\u8bef\u5dee\u4e5f\u9ad8\u4e8e\u5b9e\u9645\u5e94\u7528\u6240\u9700\u7684\u9608\u503c\u3002\u6b64\u5916\uff0c\u6a21\u62df\u7a33\u5b9a\u6027\u548c\u673a\u68b0\u6027\u80fd\u51c6\u786e\u6027\u4e4b\u95f4\u5b58\u5728\u8131\u8282\uff0c\u9884\u6d4b\u8bef\u5dee\u4e0e\u8bad\u7ec3\u6570\u636e\u7684\u8868\u793a\u76f8\u5173\uff0c\u800c\u975e\u6a21\u578b\u672c\u8eab\u3002", "conclusion": "\u76ee\u524d\u7684\u8ba1\u7b97\u57fa\u51c6\u53ef\u80fd\u9ad8\u4f30\u4e86\u6a21\u578b\u5728\u590d\u6742\u5316\u5b66\u7a7a\u95f4\u4e2d\u7684\u53ef\u9760\u6027\u3002UniFFBench \u786e\u7acb\u4e86\u57fa\u672c \u5b9e\u9a8c\u9a8c\u8bc1\u6807\u51c6\uff0c\u5e76\u63ed\u793a\u4e86\u5fc5\u987b\u89e3\u51b3\u7684\u7cfb\u7edf\u6027\u5c40\u9650\u6027\uff0c\u624d\u80fd\u5b9e\u73b0\u771f\u6b63\u7684\u901a\u7528\u529b\u573a\u80fd\u529b\u3002"}}
{"id": "2508.05638", "categories": ["physics.app-ph", "physics.data-an"], "pdf": "https://arxiv.org/pdf/2508.05638", "abs": "https://arxiv.org/abs/2508.05638", "authors": ["Amirhossein Amiri-Hezaveh", "Adrian Buganza Tepole"], "title": "A Physics-Augmented Machine Learning Constitutive Model for Damage in Solids", "comment": null, "summary": "We propose a data-driven constitutive framework for anisotropic damage\nmechanics based on the second-order damage tensor approach for both\ncompressible and incompressible materials. The formulation is thermodynamically\nconsistent and satisfies the Clausius-Duhem inequality. The strain energy\ndensity potentials are expressed as isotropic functions of the right\nCauchy-Green deformation tensor, along with structural tensors that encode\nanisotropy either present in the virgin material or resulting from damage. To\nguarantee the polyconvexity condition, non-decreasing convex neural networks\nwith inputs that ensure polyconvexity are used to parameterize the strain\nenergy density potentials. The model vanishes in the undeformed state,\nfulfilling the normality condition. In contrast to classical [1-d] damage\nmodels, the expressiveness of the new data-driven model is enhanced by\nemploying a family of nonlinear, convex, decreasing functions to capture the\neffect of damage. Damage evolution is governed through a damage potential,\nwhere the corresponding threshold is defined in terms of the damage conjugate\nforces. As a special case of the general formulation, a new anisotropic generic\nformat is introduced to predict constitutive responses under damage-induced\nanisotropy in initially isotropic materials. To reduce the computational burden\nduring training, a decoupled training scheme is introduced, and its accuracy is\ndemonstrated in all numerical examples. These include benchmarks for\nincompressible isotropic, transversely isotropic, and compressible orthotropic\nmaterials. The framework is also validated against experimental data capturing\nanisotropic Mullins-type damage.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u6570\u636e\u9a71\u52a8\u6846\u67b6\uff0c\u7528\u4e8c\u9636\u635f\u4f24\u5f20\u91cf\u5904\u7406\u5404\u5411\u5f02\u6027\u635f\u4f24\u529b\u5b66\uff0c\u5e76\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u4fdd\u8bc1\u4fdd\u51f8\u6027\uff0c\u80fd\u9884\u6d4b\u4e0d\u540c\u6750\u6599\u7684\u635f\u4f24\u884c\u4e3a\uff0c\u5e76\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u4e3a\u4e86\u5728\u5404\u5411\u5f02\u6027\u635f\u4f24\u529b\u5b66\u4e2d\u5f00\u53d1\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7684\u672c\u6784\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u5904\u7406\u53ef\u538b\u7f29\u548c\u4e0d\u53ef\u538b\u7f29\u6750\u6599\uff0c\u6ee1\u8db3\u70ed\u529b\u5b66\u4e00\u81f4\u6027\uff0c\u5e76\u80fd\u51c6\u786e\u6355\u6349\u635f\u4f24\u5f15\u8d77\u7684\u5404\u5411\u5f02\u6027\u6548\u5e94\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u4e8c\u9636\u635f\u4f24\u5f20\u91cf\u7684\u6570\u636e\u9a71\u52a8\u672c\u6784\u6846\u67b6\uff0c\u7528\u4e8e\u53ef\u538b\u7f29\u548c\u4e0d\u53ef\u538b\u7f29\u6750\u6599\u7684\u5404\u5411\u5f02\u6027\u635f\u4f24\u529b\u5b66\u3002\u8be5\u6846\u67b6\u662f\u70ed\u529b\u5b66\u4e00\u81f4\u7684\uff0c\u5e76\u6ee1\u8db3Clausius-Duhem\u4e0d\u7b49\u5f0f\u3002\u5e94\u53d8\u80fd\u5bc6\u5ea6\u52bf\u88ab\u8868\u8fbe\u4e3a\u53f3Cauchy-Green\u53d8\u5f62\u5f20\u91cf\u4ee5\u53ca\u7f16\u7801\u6750\u6599\u6216\u635f\u4f24\u5404\u5411\u5f02\u6027\u7684\u7ed3\u6784\u5f20\u91cf\u7684\u5404\u5411\u540c\u6027\u51fd\u6570\u3002\u4e3a\u4e86\u4fdd\u8bc1\u4fdd\u51f8\u6027\uff0c\u91c7\u7528\u4e86\u6ee1\u8db3\u4fdd\u51f8\u6027\u7ea6\u675f\u7684\u975e\u9012\u51cf\u51f8\u795e\u7ecf\u7f51\u7edc\u6765\u53c2\u6570\u5316\u5e94\u53d8\u80fd\u5bc6\u5ea6\u52bf\u3002\u635f\u4f24\u6f14\u5316\u901a\u8fc7\u635f\u4f24\u52bf\u63a7\u5236\uff0c\u5176\u9608\u503c\u7531\u635f\u4f24\u5171\u8f6d\u529b\u5b9a\u4e49\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u5404\u5411\u5f02\u6027\u901a\u7528\u683c\u5f0f\u4f5c\u4e3a\u4e00\u822c\u516c\u5f0f\u7684\u4e00\u4e2a\u7279\u4f8b\uff0c\u7528\u4e8e\u9884\u6d4b\u521d\u59cb\u5404\u5411\u540c\u6027\u6750\u6599\u4e2d\u7531\u635f\u4f24\u5f15\u8d77\u7684\u5404\u5411\u5f02\u6027\u672c\u6784\u54cd\u5e94\u3002\u4e3a\u4e86\u964d\u4f4e\u8bad\u7ec3\u65f6\u7684\u8ba1\u7b97\u8d1f\u62c5\uff0c\u5f15\u5165\u4e86\u4e00\u79cd\u89e3\u8026\u8bad\u7ec3\u65b9\u6848\u3002", "result": "\u901a\u8fc7\u4f7f\u7528\u6ee1\u8db3\u4fdd\u51f8\u6027\u7ea6\u675f\u7684\u975e\u9012\u51cf\u51f8\u795e\u7ecf\u7f51\u7edc\u53c2\u6570\u5316\u5e94\u53d8\u80fd\u5bc6\u5ea6\u52bf\uff0c\u8be5\u6a21\u578b\u80fd\u591f\u51c6\u786e\u6355\u6349\u635f\u4f24\u6548\u5e94\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u5bf9\u4e0d\u53ef\u538b\u7f29\u5404\u5411\u540c\u6027\u3001\u6a2a\u5411\u5404\u5411\u540c\u6027\u548c\u53ef\u538b\u7f29\u6b63\u4ea4\u5404\u5411\u5f02\u6027\u6750\u6599\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u4ee5\u53ca\u5bf9\u5404\u5411\u5f02\u6027Mullins\u578b\u635f\u4f24\u7684\u5b9e\u9a8c\u6570\u636e\u3002\u89e3\u8026\u8bad\u7ec3\u65b9\u6848\u6709\u6548\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "\u8be5\u6846\u67b6\u91c7\u7528\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u57fa\u4e8e\u4e8c\u9636\u635f\u4f24\u5f20\u91cf\uff0c\u80fd\u591f\u5904\u7406\u5404\u5411\u5f02\u6027\u635f\u4f24\u529b\u5b66\u4e2d\u53ef\u538b\u7f29\u548c\u4e0d\u53ef\u538b\u7f29\u6750\u6599\u7684\u672c\u6784\u5173\u7cfb\u3002\u901a\u8fc7\u4f7f\u7528\u6ee1\u8db3\u4fdd\u51f8\u6027\u7ea6\u675f\u7684\u975e\u9012\u51cf\u51f8\u795e\u7ecf\u7f51\u7edc\u6765\u53c2\u6570\u5316\u5e94\u53d8\u80fd\u5bc6\u5ea6\u52bf\uff0c\u786e\u4fdd\u4e86\u70ed\u529b\u5b66\u4e00\u81f4\u6027\u5e76\u6ee1\u8db3Clausius-Duhem\u4e0d\u7b49\u5f0f\u3002\u635f\u4f24\u6f14\u5316\u7531\u635f\u4f24\u52bf\u63a7\u5236\uff0c\u9608\u503c\u7531\u635f\u4f24\u5171\u8f6d\u529b\u5b9a\u4e49\u3002\u6b64\u5916\uff0c\u8be5\u6a21\u578b\u8fd8\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u5404\u5411\u5f02\u6027\u901a\u7528\u683c\u5f0f\uff0c\u7528\u4e8e\u9884\u6d4b\u521d\u59cb\u5404\u5411\u540c\u6027\u6750\u6599\u4e2d\u7531\u635f\u4f24\u5f15\u8d77\u7684\u5404\u5411\u5f02\u6027\u672c\u6784\u54cd\u5e94\u3002\u91c7\u7528\u89e3\u8026\u8bad\u7ec3\u65b9\u6848\u4ee5\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u5e76\u901a\u8fc7\u5bf9\u4e0d\u53ef\u538b\u7f29\u5404\u5411\u540c\u6027\u3001\u6a2a\u5411\u5404\u5411\u540c\u6027\u548c\u53ef\u538b\u7f29\u6b63\u4ea4\u5404\u5411\u5f02\u6027\u6750\u6599\u7684\u57fa\u51c6\u6d4b\u8bd5\u4ee5\u53ca\u4e0e\u5b9e\u9a8c\u6570\u636e\u7684\u5bf9\u6bd4\u9a8c\u8bc1\u4e86\u5176\u51c6\u786e\u6027\uff0c\u7279\u522b\u662f\u5bf9\u6355\u83b7\u5404\u5411\u5f02\u6027Mullins\u578b\u635f\u4f24\u7684\u5b9e\u9a8c\u6570\u636e\u7684\u9a8c\u8bc1\u3002"}}
{"id": "2508.05844", "categories": ["cs.GT", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.05844", "abs": "https://arxiv.org/abs/2508.05844", "authors": ["Fran\u00e7ois Bachoc", "Nicol\u00f2 Cesa-Bianchi", "Tommaso Cesari", "Roberto Colomboni"], "title": "Stochastic Bandits for Crowdsourcing and Multi-Platform Autobidding", "comment": null, "summary": "Motivated by applications in crowdsourcing, where a fixed sum of money is\nsplit among $K$ workers, and autobidding, where a fixed budget is used to bid\nin $K$ simultaneous auctions, we define a stochastic bandit model where arms\nbelong to the $K$-dimensional probability simplex and represent the fraction of\nbudget allocated to each task/auction. The reward in each round is the sum of\n$K$ stochastic rewards, where each of these rewards is unlocked with a\nprobability that varies with the fraction of the budget allocated to that\ntask/auction. We design an algorithm whose expected regret after $T$ steps is\nof order $K\\sqrt{T}$ (up to log factors) and prove a matching lower bound.\nImproved bounds of order $K (\\log T)^2$ are shown when the function mapping\nbudget to probability of unlocking the reward (i.e., terminating the task or\nwinning the auction) satisfies additional diminishing-returns conditions.", "AI": {"tldr": "This paper introduces a new stochastic bandit model for budget allocation problems in crowdsourcing and autobidding. It offers an algorithm with regret proportional to K*sqrt(T), with potential improvements to K*(log T)^2 under certain conditions.", "motivation": "The motivation stems from applications in crowdsourcing (splitting a fixed sum among K workers) and autobidding (using a fixed budget for K simultaneous auctions).", "method": "The paper designs a stochastic bandit algorithm for a novel problem setting and proves theoretical bounds (both upper and lower) on its expected regret. The method involves analyzing the algorithm's performance in terms of regret over T steps.", "result": "The paper achieves an expected regret of O(K*sqrt(T)) and proves a matching lower bound. It also demonstrates improved bounds of O(K*(log T)^2) under specific diminishing-returns conditions.", "conclusion": "The paper defines a stochastic bandit model with arms on the K-dimensional probability simplex, representing budget allocation. It designs an algorithm with O(K*sqrt(T)) expected regret and proves a matching lower bound. Improved bounds of O(K*(log T)^2) are achieved under diminishing-returns conditions."}}
{"id": "2508.05687", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05687", "abs": "https://arxiv.org/abs/2508.05687", "authors": ["Alistair Reid", "Simon O'Callaghan", "Liam Carroll", "Tiberio Caetano"], "title": "Risk Analysis Techniques for Governed LLM-based Multi-Agent Systems", "comment": null, "summary": "Organisations are starting to adopt LLM-based AI agents, with their\ndeployments naturally evolving from single agents towards interconnected,\nmulti-agent networks. Yet a collection of safe agents does not guarantee a safe\ncollection of agents, as interactions between agents over time create emergent\nbehaviours and induce novel failure modes. This means multi-agent systems\nrequire a fundamentally different risk analysis approach than that used for a\nsingle agent.\n  This report addresses the early stages of risk identification and analysis\nfor multi-agent AI systems operating within governed environments where\norganisations control their agent configurations and deployment. In this\nsetting, we examine six critical failure modes: cascading reliability failures,\ninter-agent communication failures, monoculture collapse, conformity bias,\ndeficient theory of mind, and mixed motive dynamics. For each, we provide a\ntoolkit for practitioners to extend or integrate into their existing frameworks\nto assess these failure modes within their organisational contexts.\n  Given fundamental limitations in current LLM behavioural understanding, our\napproach centres on analysis validity, and advocates for progressively\nincreasing validity through staged testing across stages of abstraction and\ndeployment that gradually increases exposure to potential negative impacts,\nwhile collecting convergent evidence through simulation, observational\nanalysis, benchmarking, and red teaming. This methodology establishes the\ngroundwork for robust organisational risk management as these LLM-based\nmulti-agent systems are deployed and operated.", "AI": {"tldr": "LLM\u591a\u4ee3\u7406\u7cfb\u7edf\u7684\u98ce\u9669\u5206\u6790\u65b9\u6cd5\u9700\u8981\u66f4\u65b0\uff0c\u4ee5\u5e94\u5bf9\u4ea4\u4e92\u4e2d\u7684\u6d8c\u73b0\u884c\u4e3a\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u9636\u6bb5\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u5e76\u8bc6\u522b\u4e86\u516d\u79cd\u5173\u952e\u7684\u6545\u969c\u6a21\u5f0f\uff0c\u4e3a\u7ec4\u7ec7\u98ce\u9669\u7ba1\u7406\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "motivation": "\u968f\u7740\u7ec4\u7ec7\u5f00\u59cb\u91c7\u7528\u57fa\u4e8eLLM\u7684AI\u4ee3\u7406\uff0c\u5e76\u4ece\u5355\u4e00\u4ee3\u7406\u53d1\u5c55\u5230\u4e92\u8054\u7684\u591a\u4ee3\u7406\u7f51\u7edc\uff0c\u73b0\u6709\u7684\u98ce\u9669\u5206\u6790\u65b9\u6cd5\u5df2\u4e0d\u8db3\u4ee5\u5e94\u5bf9\u591a\u4ee3\u7406\u7cfb\u7edf\u4ea4\u4e92\u5e26\u6765\u7684\u65b0\u5174\u884c\u4e3a\u548c\u6545\u969c\u6a21\u5f0f\u3002\u56e0\u6b64\uff0c\u6709\u5fc5\u8981\u4e3a\u591a\u4ee3\u7406AI\u7cfb\u7edf\u5f00\u53d1\u4e00\u79cd\u65b0\u7684\u98ce\u9669\u5206\u6790\u65b9\u6cd5\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4ee5\u5206\u6790\u6709\u6548\u6027\u4e3a\u4e2d\u5fc3\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u4e0d\u540c\u62bd\u8c61\u548c\u90e8\u7f72\u9636\u6bb5\u8fdb\u884c\u5206\u9636\u6bb5\u6d4b\u8bd5\u6765\u9010\u6b65\u63d0\u9ad8\u6709\u6548\u6027\u3002\u6d4b\u8bd5\u65b9\u6cd5\u5305\u62ec\u6a21\u62df\u3001\u89c2\u5bdf\u6027\u5206\u6790\u3001\u57fa\u51c6\u6d4b\u8bd5\u548c\u7ea2\u961f\u6f14\u7ec3\uff0c\u4ee5\u6536\u96c6\u4e00\u81f4\u7684\u8bc1\u636e\u3002", "result": "\u62a5\u544a\u8be6\u7ec6\u4ecb\u7ecd\u4e86\u516d\u79cd\u5173\u952e\u7684\u6545\u969c\u6a21\u5f0f\uff1a\u7ea7\u8054\u53ef\u9760\u6027\u6545\u969c\u3001\u4ee3\u7406\u95f4\u901a\u4fe1\u6545\u969c\u3001\u5355\u4e00\u6587\u5316\u5d29\u6e83\u3001\u4ece\u4f17\u504f\u5dee\u3001\u5fc3\u667a\u7406\u8bba\u7f3a\u9677\u548c\u6df7\u5408\u52a8\u673a\u52a8\u6001\u3002\u9488\u5bf9\u6bcf\u79cd\u6a21\u5f0f\uff0c\u62a5\u544a\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5de5\u5177\u5305\uff0c\u4f9b\u4ece\u4e1a\u8005\u6269\u5c55\u6216\u96c6\u6210\u5230\u73b0\u6709\u6846\u67b6\u4e2d\uff0c\u4ee5\u8bc4\u4f30\u5176\u7ec4\u7ec7\u73af\u5883\u4e2d\u7684\u6545\u969c\u6a21\u5f0f\u3002", "conclusion": "\u591a\u667a\u80fd\u4f53AI\u7cfb\u7edf\u9700\u8981\u4e0d\u540c\u4e8e\u5355\u667a\u80fd\u4f53AI\u7cfb\u7edf\u7684\u98ce\u9669\u5206\u6790\u65b9\u6cd5\uff0c\u4ee5\u5e94\u5bf9\u4ea4\u4e92\u4e2d\u51fa\u73b0\u7684\u6d8c\u73b0\u884c\u4e3a\u548c\u65b0\u7684\u6545\u969c\u6a21\u5f0f\u3002\u672c\u62a5\u544a\u63d0\u4f9b\u4e86\u4e00\u4e2a\u98ce\u9669\u8bc6\u522b\u548c\u5206\u6790\u7684\u6846\u67b6\uff0c\u4ee5\u5e94\u5bf9\u7531\u7ec4\u7ec7\u63a7\u5236\u7684\u3001\u4ee5LLM\u4e3a\u57fa\u7840\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u3002"}}
{"id": "2508.06443", "categories": ["cs.AI", "cs.CY", "cs.ET", "cs.GT"], "pdf": "https://arxiv.org/pdf/2508.06443", "abs": "https://arxiv.org/abs/2508.06443", "authors": ["Debabrota Basu", "Udvas Das"], "title": "The Fair Game: Auditing & Debiasing AI Algorithms Over Time", "comment": null, "summary": "An emerging field of AI, namely Fair Machine Learning (ML), aims to quantify\ndifferent types of bias (also known as unfairness) exhibited in the predictions\nof ML algorithms, and to design new algorithms to mitigate them. Often, the\ndefinitions of bias used in the literature are observational, i.e. they use the\ninput and output of a pre-trained algorithm to quantify a bias under concern.\nIn reality,these definitions are often conflicting in nature and can only be\ndeployed if either the ground truth is known or only in retrospect after\ndeploying the algorithm. Thus,there is a gap between what we want Fair ML to\nachieve and what it does in a dynamic social environment. Hence, we propose an\nalternative dynamic mechanism,\"Fair Game\",to assure fairness in the predictions\nof an ML algorithm and to adapt its predictions as the society interacts with\nthe algorithm over time. \"Fair Game\" puts together an Auditor and a Debiasing\nalgorithm in a loop around an ML algorithm. The \"Fair Game\" puts these two\ncomponents in a loop by leveraging Reinforcement Learning (RL). RL algorithms\ninteract with an environment to take decisions, which yields new observations\n(also known as data/feedback) from the environment and in turn, adapts future\ndecisions. RL is already used in algorithms with pre-fixed long-term fairness\ngoals. \"Fair Game\" provides a unique framework where the fairness goals can be\nadapted over time by only modifying the auditor and the different biases it\nquantifies. Thus,\"Fair Game\" aims to simulate the evolution of ethical and\nlegal frameworks in the society by creating an auditor which sends feedback to\na debiasing algorithm deployed around an ML system. This allows us to develop a\nflexible and adaptive-over-time framework to build Fair ML systems pre- and\npost-deployment.", "AI": {"tldr": "\u201cFair Game\u201d\u662f\u4e00\u79cd\u65b0\u7684\u516c\u5e73\u673a\u5668\u5b66\u4e60\u673a\u5236\uff0c\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u6765\u52a8\u6001\u9002\u5e94\u793e\u4f1a\u4ea4\u4e92\u4e2d\u7684\u504f\u89c1\uff0c\u4ee5\u5b9e\u73b0\u7075\u6d3b\u4e14\u53ef\u968f\u65f6\u95f4\u81ea\u9002\u5e94\u7684\u516c\u5e73\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u516c\u5e73\u673a\u5668\u5b66\u4e60\uff08ML\uff09\u65b9\u6cd5\u5728\u5b9a\u4e49\u504f\u89c1\u65f6\u901a\u5e38\u662f\u89c2\u5bdf\u6027\u7684\uff0c\u5e76\u4e14\u5b58\u5728\u76f8\u4e92\u51b2\u7a81\u7684\u60c5\u51b5\uff0c\u8fd9\u4f7f\u5f97\u5b83\u4eec\u5728\u52a8\u6001\u7684\u793e\u4f1a\u73af\u5883\u4e2d\u96be\u4ee5\u90e8\u7f72\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u673a\u5236\u6765\u786e\u4fddML\u7b97\u6cd5\u7684\u516c\u5e73\u6027\uff0c\u5e76\u80fd\u968f\u7740\u793e\u4f1a\u4e0e\u7b97\u6cd5\u7684\u4ea4\u4e92\u800c\u8fdb\u884c\u8c03\u6574\u3002", "method": "\u201cFair Game\u201d\u901a\u8fc7\u5c06\u5ba1\u8ba1\u5458\u548c\u53bb\u504f\u7b97\u6cd5\u7f6e\u4e8e\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u5468\u56f4\u7684\u5faa\u73af\u4e2d\uff0c\u5e76\u5229\u7528\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u6765\u8fde\u63a5\u8fd9\u4e24\u4e2a\u7ec4\u4ef6\u3002", "result": "\u201cFair Game\u201d\u63d0\u4f9b\u4e86\u4e00\u4e2a\u72ec\u7279\u7684\u6846\u67b6\uff0c\u53ef\u4ee5\u901a\u8fc7\u4fee\u6539\u5ba1\u8ba1\u5458\u53ca\u5176\u91cf\u5316\u7684\u504f\u89c1\u6765\u9002\u5e94\u516c\u5e73\u6027\u76ee\u6807\uff0c\u4ece\u800c\u6a21\u62df\u793e\u4f1a\u4e2d\u4f26\u7406\u548c\u6cd5\u5f8b\u6846\u67b6\u7684\u6f14\u53d8\u3002", "conclusion": "\u201cFair Game\u201d\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7075\u6d3b\u4e14\u53ef\u968f\u65f6\u95f4\u81ea\u9002\u5e94\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u90e8\u7f72\u524d\u548c\u90e8\u7f72\u540e\u6784\u5efa\u516c\u5e73\u7684\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u3002"}}
{"id": "2508.06212", "categories": ["cs.DS", "cs.DM", "math.CO"], "pdf": "https://arxiv.org/pdf/2508.06212", "abs": "https://arxiv.org/abs/2508.06212", "authors": ["Romain Bourneuf", "Tim Planken"], "title": "A Structural Linear-Time Algorithm for Computing the Tutte Decomposition", "comment": "41 pages, 4 figures", "summary": "The block-cut tree decomposes a connected graph along its cutvertices,\ndisplaying its 2-connected components. The Tutte-decomposition extends this\nidea to 2-separators in 2-connected graphs, yielding a canonical\ntree-decomposition that decomposes the graph into its triconnected components.\nIn 1973, Hopcroft and Tarjan introduced a linear-time algorithm to compute the\nTutte-decomposition. Cunningham and Edmonds later established a structural\ncharacterization of the Tutte-decomposition via totally-nested 2-separations.\nWe present a conceptually simple algorithm based on this characterization,\nwhich computes the Tutte-decomposition in linear time. Our algorithm first\ncomputes all totally-nested 2-separations and then builds the\nTutte-decomposition from them.\n  Along the way, we derive new structural results on the structure of\ntotally-nested 2-separations in 2-connected graphs using a novel notion of\nstability, which may be of independent interest.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7b97\u6cd5\uff0c\u5728\u7ebf\u6027\u65f6\u95f4\u5185\u8ba1\u7b97\u56fe\u7684Tutte\u5206\u89e3\uff0c\u5e76\u63d0\u4f9b\u4e86\u65b0\u7684\u7ed3\u6784\u7406\u8bba\u7ed3\u679c\u3002", "motivation": "\u65e8\u5728\u6269\u5c55\u5757-\u5272\u6811\u7684\u6982\u5ff5\uff0c\u5c06\u56fe\u5206\u89e3\u4e3a\u4e09\u8fde\u901a\u5206\u91cf\uff0c\u5e76\u63d0\u4f9b\u4e00\u79cd\u8ba1\u7b97Tutte\u5206\u89e3\u7684\u7ebf\u6027\u65f6\u95f4\u7b97\u6cd5\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7b97\u6cd5\uff0c\u9996\u5148\u8ba1\u7b97\u6240\u6709\u5168\u5d4c\u59572-\u5206\u79bb\uff0c\u7136\u540e\u57fa\u4e8e\u8fd9\u4e9b\u5206\u79bb\u6765\u6784\u5efaTutte\u5206\u89e3\u3002", "result": "\u6210\u529f\u5730\u5728\u7ebf\u6027\u65f6\u95f4\u5185\u8ba1\u7b97\u4e86Tutte\u5206\u89e3\uff0c\u5e76\u5f97\u51fa\u4e86\u4e00\u4e9b\u5173\u4e8e\u5168\u5d4c\u59572-\u5206\u79bb\u7ed3\u6784\u7684\u65b0\u7406\u8bba\u7ed3\u679c\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5168\u5d4c\u59572-\u5206\u79bb\u7684\u56fe\u8bba\u5206\u89e3\u65b0\u65b9\u6cd5\uff0c\u5e76\u63a8\u5bfc\u4e86\u65b0\u7684\u7ed3\u6784\u7ed3\u679c\u3002"}}
{"id": "2508.05882", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.05882", "abs": "https://arxiv.org/abs/2508.05882", "authors": ["Yingbo Hua"], "title": "STEEP -- An Alternative To Quantum Key Distribution", "comment": null, "summary": "Secret-message transmission by echoing encrypted probes (STEEP) is discussed\nas an alternative to quantum key distribution (QKD). The former only needs\nclassic or non-quantum channels while the latter needs both quantum and classic\nchannels for secret-key generation. STEEP is shown to yield a secrecy rate\nsufficient for one-time pads encryption in many practical situations including\nin-air channels or undersea optical cables. Other advantages of STEEP over QKD\ninclude cost, complexity, compatibility, and robustness against constant\neavesdropping.", "AI": {"tldr": "STEEP\u662f\u4e00\u79cd\u4ec5\u9700\u7ecf\u5178\u4fe1\u9053\u7684\u5bc6\u94a5\u751f\u6210\u6280\u672f\uff0c\u53ef\u4f5c\u4e3aQKD\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5177\u6709\u6210\u672c\u3001\u590d\u6742\u6027\u3001\u517c\u5bb9\u6027\u548c\u9c81\u68d2\u6027\u7b49\u4f18\u52bf\u3002", "motivation": "\u5c06STEEP\u4f5c\u4e3a\u4e00\u79cd\u66ff\u4ee3\u91cf\u5b50\u5bc6\u94a5\u5206\u53d1\uff08QKD\uff09\u7684\u65b9\u6848\u8fdb\u884c\u8ba8\u8bba\uff0c\u5f3a\u8c03\u5176\u4ec5\u9700\u7ecf\u5178\u4fe1\u9053\u5373\u53ef\u8fdb\u884c\u5bc6\u94a5\u751f\u6210\uff0c\u800cQKD\u9700\u8981\u91cf\u5b50\u548c\u7ecf\u5178\u4fe1\u9053\u3002", "method": "STEEP\uff08Secret-message transmission by echoing encrypted probes\uff09", "result": "STEEP\u5728\u8bb8\u591a\u5b9e\u9645\u60c5\u51b5\uff08\u5305\u62ec\u7a7a\u6c14\u4fe1\u9053\u6216\u6d77\u5e95\u5149\u7f06\uff09\u4e2d\uff0c\u53ef\u4ee5\u5b9e\u73b0\u8db3\u591f\u9ad8\u7684\u4fdd\u5bc6\u7387\uff0c\u8db3\u4ee5\u652f\u6301\u4e00\u6b21\u6027\u5bc6\u7801\u52a0\u5bc6\u3002", "conclusion": "STEEP\u662f\u4e00\u79cd\u6709\u6f5c\u529b\u7684\u91cf\u5b50\u5bc6\u94a5\u5206\u53d1\uff08QKD\uff09\u66ff\u4ee3\u65b9\u6848\uff0c\u5b83\u4ec5\u9700\u8981\u7ecf\u5178\u4fe1\u9053\uff0c\u5e76\u4e14\u5728\u8bb8\u591a\u5b9e\u9645\u5e94\u7528\u573a\u666f\u4e2d\u53ef\u4ee5\u63d0\u4f9b\u8db3\u591f\u9ad8\u7684\u4fdd\u5bc6\u7387\uff0c\u4ee5\u652f\u6301\u4e00\u6b21\u6027\u5bc6\u7801\u52a0\u5bc6\u3002"}}
{"id": "2508.05821", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.05821", "abs": "https://arxiv.org/abs/2508.05821", "authors": ["Shadman Sakib", "Ajay Katangur", "Rahul Dubey"], "title": "A Dynamic Approach to Load Balancing in Cloud Infrastructure: Enhancing Energy Efficiency and Resource Utilization", "comment": "Accepted for publication in 2025 IEEE Cloud Summit", "summary": "Cloud computing has grown rapidly in recent years, mainly due to the sharp\nincrease in data transferred over the internet. This growth makes load\nbalancing a key part of cloud systems, as it helps distribute user requests\nacross servers to maintain performance, prevent overload, and ensure a smooth\nuser experience. Despite its importance, managing server resources and keeping\nworkloads balanced over time remains a major challenge in cloud environments.\nThis paper introduces a novel Score-Based Dynamic Load Balancer (SBDLB) that\nallocates workloads to virtual machines based on real-time performance metrics.\nThe objective is to enhance resource utilization and overall system efficiency.\nThe method was thoroughly tested using the CloudSim 7G platform, comparing its\nperformance against the throttled load balancing strategy. Evaluations were\nconducted across a variety of workloads and scenarios, demonstrating the\nSBDLB's ability to adapt dynamically to workload fluctuations while optimizing\nresource usage. The proposed method outperformed the throttled strategy,\nimproving average response times by 34% and 37% in different scenarios. It also\nreduced data center processing times by an average of 13%. Over a 24-hour\nsimulation, the method decreased operational costs by 15%, promoting a more\nenergy-efficient and sustainable cloud infrastructure through reduced energy\nconsumption.", "AI": {"tldr": "SBDLB\u901a\u8fc7\u5b9e\u65f6\u6027\u80fd\u6307\u6807\u52a8\u6001\u8c03\u6574\u8d1f\u8f7d\uff0c\u663e\u8457\u6539\u5584\u4e86\u4e91\u670d\u52a1\u7684\u54cd\u5e94\u65f6\u95f4\u3001\u5904\u7406\u6548\u7387\u548c\u6210\u672c\u6548\u76ca\uff0c\u540c\u65f6\u4fc3\u8fdb\u4e86\u7eff\u8272\u8ba1\u7b97\u3002", "motivation": "\u4e3a\u4e86\u5e94\u5bf9\u4e91\u73af\u5883\u65e5\u76ca\u589e\u957f\u7684\u6570\u636e\u4f20\u8f93\u548c\u7528\u6237\u8bf7\u6c42\uff0c\u4ee5\u53ca\u89e3\u51b3\u73b0\u6709\u8d1f\u8f7d\u5747\u8861\u6280\u672f\u5728\u8d44\u6e90\u7ba1\u7406\u548c\u5de5\u4f5c\u8d1f\u8f7d\u5747\u8861\u65b9\u9762\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b9e\u65f6\u6027\u80fd\u6307\u6807\u8fdb\u884c\u865a\u62df\u8d1f\u8f7d\u5206\u914d\u7684\u8bc4\u5206\u5236\u52a8\u6001\u8d1f\u8f7d\u5747\u8861\u5668\uff08SBDLB\uff09\uff0c\u5e76\u4f7f\u7528CloudSim 7G\u5e73\u53f0\u8fdb\u884c\u6d4b\u8bd5\u548c\u8bc4\u4f30\u3002", "result": "SBDLB\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u5e73\u5747\u54cd\u5e94\u65f6\u95f4\u5206\u522b\u63d0\u9ad8\u4e8634%\u548c37%\uff0c\u6570\u636e\u4e2d\u5fc3\u5904\u7406\u65f6\u95f4\u5e73\u5747\u7f29\u77ed\u4e8613%\uff0c\u572824\u5c0f\u65f6\u6a21\u62df\u4e2d\u8fd0\u8425\u6210\u672c\u964d\u4f4e\u4e8615%\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u8d44\u6e90\u5229\u7528\u7387\u548c\u80fd\u6548\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u8bc4\u5206\u5236\u52a8\u6001\u8d1f\u8f7d\u5747\u8861\u5668\uff08SBDLB\uff09\uff0c\u4e0e\u4f20\u7edf\u7684\u8282\u6d41\u7b56\u7565\u76f8\u6bd4\uff0c\u5728\u54cd\u5e94\u65f6\u95f4\u3001\u6570\u636e\u4e2d\u5fc3\u5904\u7406\u65f6\u95f4\u548c\u8fd0\u8425\u6210\u672c\u65b9\u9762\u5747\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u5e76\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u8d44\u6e90\u5229\u7528\u7387\u548c\u80fd\u6548\u3002"}}
{"id": "2508.05912", "categories": ["cond-mat.mes-hall", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2508.05912", "abs": "https://arxiv.org/abs/2508.05912", "authors": ["Pedro Pereyra"], "title": "Quantum Hall Resistance and Quantum Hall Plateaus from Edge State Quantization", "comment": "5 pages, 3 figure", "summary": "Despite the extensive literature on the quantum Hall effect (QHE), a direct\nderivation of the phenomenological formula $\\rho_{xy} = h/e^2\\nu$ from first\nprinciples has remained elusive. In this work, we revisit the Landau and\nLandauer-B\\\"uttiker formalisms and impose hard-wall boundary conditions on the\nwavefunction, an essential but often overlooked constraint. This condition\nquantizes the guiding center position and the longitudinal wave number $k_x$,\nleading naturally to a discrete number of edge states without invoking energy\nbending. We derive the Hall resistance directly and recover the standard result\n$\\rho_{xy} = h/e^2\\nu$, along with an explicit expression for the filling\nfactor $\\nu$ in terms of the Fermi energy and magnetic field. The resulting\nresistance steps reproduce the observed QHE plateaus and match experimental\ndata without fitting parameters.", "AI": {"tldr": "\u901a\u8fc7\u65bd\u52a0\u786c\u58c1\u8fb9\u754c\u6761\u4ef6\uff0c\u6211\u4eec\u6210\u529f\u4ece\u7b2c\u4e00\u6027\u539f\u7406\u63a8\u5bfc\u4e86\u91cf\u5b50\u970d\u5c14\u6548\u5e94\u7684\u970d\u5c14\u7535\u963b\u516c\u5f0f\uff0c\u5e76\u5f97\u5230\u4e86\u4e0e\u5b9e\u9a8c\u6570\u636e\u4e00\u81f4\u7684\u7ed3\u679c\u3002", "motivation": "\u586b\u8865\u4e86\u4ece\u7b2c\u4e00\u6027\u539f\u7406\u76f4\u63a5\u63a8\u5bfc\u91cf\u5b50\u970d\u5c14\u6548\u5e94\uff08QHE\uff09\u73b0\u8c61\u5b66\u516c\u5f0f $\rho_{xy} = h/e^2\nu$ \u7684\u7a7a\u767d\u3002", "method": "\u5728 Landau \u548c Landauer-B\u00fcttiker \u5f62\u5f0f\u4e3b\u4e49\u4e2d\u65bd\u52a0\u786c\u58c1\u8fb9\u754c\u6761\u4ef6\uff0c\u5bf9\u6ce2\u51fd\u6570\u8fdb\u884c\u7ea6\u675f\uff0c\u5b9e\u73b0\u4e86\u8fb9\u7f18\u6001\u7684\u91cf\u5b50\u5316\u3002", "result": "\u6210\u529f\u63a8\u5bfc\u51fa\u970d\u5c14\u7535\u963b\u516c\u5f0f $\rho_{xy} = h/e^2\nu$\uff0c\u5e76\u5f97\u5230\u586b\u5145\u56e0\u5b50 $\nu$ \u76f8\u5bf9\u4e8e\u8d39\u7c73\u80fd\u548c\u78c1\u573a\u7684\u663e\u5f0f\u8868\u8fbe\u5f0f\uff0c\u7535\u963b\u9636\u68af\u91cd\u73b0\u4e86\u5b9e\u9a8c\u4e2d\u89c2\u5bdf\u5230\u7684 QHE \u5e73\u53f0\u3002", "conclusion": "\u4ece\u7b2c\u4e00\u6027\u539f\u7406\u63a8\u5bfc\u4e86\u970d\u5c14\u7535\u963b\u516c\u5f0f\uff0c\u5e76\u7ed9\u51fa\u4e86\u586b\u5145\u56e0\u5b50\u03bd\u7684\u663e\u5f0f\u8868\u8fbe\u5f0f\uff0c\u5176\u7ed3\u679c\u4e0e\u5b9e\u9a8c\u6570\u636e\u5b8c\u7f8e\u5339\u914d\u3002"}}
{"id": "2508.05703", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.05703", "abs": "https://arxiv.org/abs/2508.05703", "authors": ["Zhiyan Ding", "Yongtao Zhan", "John Preskill", "Lin Lin"], "title": "End-to-End Efficient Quantum Thermal and Ground State Preparation Made Simple", "comment": null, "summary": "We propose new quantum algorithms for thermal and ground state preparation\nbased on system-bath interactions. These algorithms require only forward\nevolution under a system-bath Hamiltonian in which the bath is a single\nreusable ancilla qubit, making them especially well-suited for early\nfault-tolerant quantum devices. By carefully designing the bath and interaction\nHamiltonians, we prove that the fixed point of the dynamics accurately\napproximates the desired quantum state. Furthermore, we establish theoretical\nguarantees on the mixing time for several physically relevant models, thereby\nproviding a rigorous justification for the end-to-end efficiency of system-bath\ninteraction models in thermal and ground state preparation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u5355\u91cf\u5b50\u6bd4\u7279\u6d74\u548c\u7cfb\u7edf-\u6d74\u76f8\u4e92\u4f5c\u7528\u7684\u65b0\u91cf\u5b50\u7b97\u6cd5\uff0c\u7528\u4e8e\u70ed\u6001\u548c\u57fa\u6001\u5236\u5907\uff0c\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\u7684\u6548\u7387\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u65e9\u671f\u5bb9\u9519\u91cf\u5b50\u8bbe\u5907\u4e0a\u70ed\u6001\u548c\u57fa\u6001\u5236\u5907\u7684\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u7cfb\u7edf-\u6d74\u76f8\u4e92\u4f5c\u7528\u7684\u65b0\u91cf\u5b50\u7b97\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u91cf\u5b50\u7b97\u6cd5\uff0c\u5229\u7528\u7cfb\u7edf-\u6d74\u76f8\u4e92\u4f5c\u7528\uff0c\u5176\u4e2d\u6d74\u662f\u4e00\u4e2a\u53ef\u91cd\u590d\u4f7f\u7528\u7684\u5355\u91cf\u5b50\u6bd4\u7279\u3002\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u6d74\u548c\u76f8\u4e92\u4f5c\u7528\u54c8\u5bc6\u987f\u91cf\uff0c\u8bc1\u660e\u4e86\u52a8\u529b\u5b66\u7684\u56fa\u5b9a\u70b9\u80fd\u591f\u7cbe\u786e\u5730\u903c\u8fd1\u671f\u671b\u7684\u91cf\u5b50\u6001\uff0c\u5e76\u5bf9\u6df7\u5408\u65f6\u95f4\u8fdb\u884c\u4e86\u7406\u8bba\u5206\u6790\u3002", "result": "\u7b97\u6cd5\u4ec5\u9700\u8981\u7cfb\u7edf-\u6d74\u54c8\u5bc6\u987f\u91cf\u7684\u6b63\u5411\u6f14\u5316\uff0c\u6d74\u662f\u4e00\u4e2a\u53ef\u91cd\u590d\u4f7f\u7528\u7684\u5355\u91cf\u5b50\u6bd4\u7279\uff0c\u5e76\u4e14\u5728\u6df7\u5408\u65f6\u95f4\u65b9\u9762\u6709\u7406\u8bba\u4fdd\u8bc1\uff0c\u8bc1\u660e\u4e86\u7cfb\u7edf-\u6d74\u76f8\u4e92\u4f5c\u7528\u6a21\u578b\u5728\u5236\u5907\u70ed\u6001\u548c\u57fa\u6001\u65b9\u9762\u7684\u7aef\u5230\u7aef\u6548\u7387\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b0\u91cf\u5b50\u7b97\u6cd5\u57fa\u4e8e\u7cfb\u7edf-\u6d74\u76f8\u4e92\u4f5c\u7528\uff0c\u9002\u7528\u4e8e\u65e9\u671f\u5bb9\u9519\u91cf\u5b50\u8bbe\u5907\uff0c\u80fd\u591f\u4e3a\u70ed\u6001\u548c\u57fa\u6001\u5236\u5907\u63d0\u4f9b\u7406\u8bba\u4fdd\u8bc1\u3002"}}
{"id": "2508.05775", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.05775", "abs": "https://arxiv.org/abs/2508.05775", "authors": ["Chi Zhang", "Changjia Zhu", "Junjie Xiong", "Xiaoran Xu", "Lingyao Li", "Yao Liu", "Zhuo Lu"], "title": "Guardians and Offenders: A Survey on Harmful Content Generation and Safety Mitigation", "comment": null, "summary": "Large Language Models (LLMs) have revolutionized content creation across\ndigital platforms, offering unprecedented capabilities in natural language\ngeneration and understanding. These models enable beneficial applications such\nas content generation, question and answering (Q&A), programming, and code\nreasoning. Meanwhile, they also pose serious risks by inadvertently or\nintentionally producing toxic, offensive, or biased content. This dual role of\nLLMs, both as powerful tools for solving real-world problems and as potential\nsources of harmful language, presents a pressing sociotechnical challenge. In\nthis survey, we systematically review recent studies spanning unintentional\ntoxicity, adversarial jailbreaking attacks, and content moderation techniques.\nWe propose a unified taxonomy of LLM-related harms and defenses, analyze\nemerging multimodal and LLM-assisted jailbreak strategies, and assess\nmitigation efforts, including reinforcement learning with human feedback\n(RLHF), prompt engineering, and safety alignment. Our synthesis highlights the\nevolving landscape of LLM safety, identifies limitations in current evaluation\nmethodologies, and outlines future research directions to guide the development\nof robust and ethically aligned language technologies.", "AI": {"tldr": "LLM\u529f\u80fd\u5f3a\u5927\u4f46\u5b58\u5728\u98ce\u9669\uff0c\u672c\u7efc\u8ff0\u68b3\u7406\u4e86\u76f8\u5173\u7814\u7a76\u3001\u63d0\u51fa\u5206\u7c7b\u6cd5\u3001\u8bc4\u4f30\u4e86\u9632\u5fa1\u63aa\u65bd\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "LLM\u5728\u5185\u5bb9\u521b\u4f5c\u3001\u95ee\u7b54\u3001\u7f16\u7a0b\u548c\u4ee3\u7801\u63a8\u7406\u7b49\u65b9\u9762\u5c55\u73b0\u4e86\u524d\u6240\u672a\u6709\u7684\u80fd\u529b\uff0c\u4f46\u5b83\u4eec\u4e5f\u53ef\u80fd\u4ea7\u751f\u6709\u6bd2\u3001\u5192\u72af\u6216\u5e26\u6709\u504f\u89c1\u7684\u5185\u5bb9\uff0c\u5e26\u6765\u4e25\u91cd\u7684\u98ce\u9669\u3002\u56e0\u6b64\uff0c\u5982\u4f55\u89e3\u51b3LLM\u5e26\u6765\u7684\u793e\u4f1a\u6280\u672f\u6311\u6218\u662f\u4e00\u4e2a\u7d27\u8feb\u7684\u95ee\u9898\u3002", "method": "\u672c\u7814\u7a76\u7cfb\u7edf\u6027\u5730\u56de\u987e\u4e86\u8fd1\u671f\u5173\u4e8e\u65e0\u610f\u6bd2\u6027\u3001\u5bf9\u6297\u6027\u8d8a\u72f1\u653b\u51fb\u548c\u5185\u5bb9\u5ba1\u6838\u6280\u672f\u7684\u7814\u7a76\uff0c\u5e76\u63d0\u51fa\u4e86LLM\u76f8\u5173\u5371\u5bb3\u548c\u9632\u5fa1\u7684\u7edf\u4e00\u5206\u7c7b\u6cd5\uff0c\u540c\u65f6\u5206\u6790\u4e86\u65b0\u5174\u7684\u591a\u6a21\u6001\u548cLLM\u8f85\u52a9\u8d8a\u72f1\u7b56\u7565\uff0c\u5e76\u8bc4\u4f30\u4e86\u5305\u62ec\u4eba\u7c7b\u53cd\u9988\u5f3a\u5316\u5b66\u4e60\uff08RLHF\uff09\u3001\u63d0\u793a\u5de5\u7a0b\u548c\u5b89\u5168\u5bf9\u9f50\u5728\u5185\u7684\u7f13\u89e3\u63aa\u65bd\u3002", "result": "\u7814\u7a76\u7efc\u5408\u5206\u6790\u4e86LLM\u5b89\u5168\u6027\u7684\u6f14\u53d8\u60c5\u51b5\uff0c\u660e\u786e\u4e86\u5f53\u524d\u8bc4\u4f30\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u4e3a\u5f00\u53d1\u7a33\u5065\u4e14\u7b26\u5408\u4f26\u7406\u7684\u8bed\u8a00\u6280\u672f\u6307\u660e\u4e86\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "LLM\u5728\u5185\u5bb9\u521b\u4f5c\u3001\u95ee\u7b54\u3001\u7f16\u7a0b\u7b49\u9886\u57df\u5177\u6709\u9769\u547d\u6027\u6f5c\u529b\uff0c\u4f46\u4e5f\u5e26\u6765\u751f\u6210\u6709\u5bb3\u5185\u5bb9\u7684\u98ce\u9669\u3002\u672c\u7814\u7a76\u7cfb\u7edf\u56de\u987e\u4e86LLM\u76f8\u5173\u7684\u6bd2\u6027\u3001\u8d8a\u72f1\u653b\u51fb\u548c\u5185\u5bb9\u5ba1\u6838\u6280\u672f\uff0c\u63d0\u51fa\u4e86\u5371\u5bb3\u548c\u9632\u5fa1\u7684\u7edf\u4e00\u5206\u7c7b\u6cd5\uff0c\u5e76\u8bc4\u4f30\u4e86RLHF\u3001\u63d0\u793a\u5de5\u7a0b\u548c\u5b89\u5168\u5bf9\u9f50\u7b49\u7f13\u89e3\u63aa\u65bd\u3002\u7814\u7a76\u5f3a\u8c03\u4e86LLM\u5b89\u5168\u6027\u7684\u6f14\u53d8\uff0c\u6307\u51fa\u4e86\u5f53\u524d\u8bc4\u4f30\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u4e3a\u5f00\u53d1\u7a33\u5065\u4e14\u7b26\u5408\u4f26\u7406\u7684\u8bed\u8a00\u6280\u672f\u89c4\u5212\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2508.05887", "categories": ["eess.SY", "cs.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2508.05887", "abs": "https://arxiv.org/abs/2508.05887", "authors": ["Apostolos I. Rikos", "Nicola Bastianello", "Themistoklis Charalambous", "Karl H. Johansson"], "title": "Distributed Optimization and Learning for Automated Stepsize Selection with Finite Time Coordination", "comment": null, "summary": "Distributed optimization and learning algorithms are designed to operate over\nlarge scale networks enabling processing of vast amounts of data effectively\nand efficiently. One of the main challenges for ensuring a smooth learning\nprocess in gradient-based methods is the appropriate selection of a learning\nstepsize. Most current distributed approaches let individual nodes adapt their\nstepsizes locally. However, this may introduce stepsize heterogeneity in the\nnetwork, thus disrupting the learning process and potentially leading to\ndivergence. In this paper, we propose a distributed learning algorithm that\nincorporates a novel mechanism for automating stepsize selection among nodes.\nOur main idea relies on implementing a finite time coordination algorithm for\neliminating stepsize heterogeneity among nodes. We analyze the operation of our\nalgorithm and we establish its convergence to the optimal solution. We conclude\nour paper with numerical simulations for a linear regression problem,\nshowcasing that eliminating stepsize heterogeneity enhances convergence speed\nand accuracy against current approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5206\u5e03\u5f0f\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u6d88\u9664\u8282\u70b9\u95f4\u7684\u6b65\u957f\u5f02\u8d28\u6027\u6765\u63d0\u9ad8\u5b66\u4e60\u8fc7\u7a0b\u7684\u7a33\u5b9a\u6027\u548c\u6548\u7387\u3002", "motivation": "\u5206\u5e03\u5f0f\u4f18\u5316\u548c\u5b66\u4e60\u7b97\u6cd5\u5728\u5904\u7406\u6d77\u91cf\u6570\u636e\u65f6\u9762\u4e34\u6b65\u957f\u9009\u62e9\u7684\u6311\u6218\uff0c\u5c40\u90e8\u81ea\u9002\u5e94\u6b65\u957f\u53ef\u80fd\u5bfc\u81f4\u7f51\u7edc\u4e2d\u7684\u6b65\u957f\u5f02\u8d28\u6027\uff0c\u4ece\u800c\u5e72\u6270\u5b66\u4e60\u8fc7\u7a0b\u751a\u81f3\u5bfc\u81f4\u53d1\u6563\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5e03\u5f0f\u5b66\u4e60\u7b97\u6cd5\uff0c\u5305\u542b\u4e00\u79cd\u7528\u4e8e\u81ea\u52a8\u9009\u62e9\u8282\u70b9\u95f4\u6b65\u957f\u7684\u65b0\u673a\u5236\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u9650\u65f6\u95f4\u534f\u8c03\u7b97\u6cd5\u6765\u6d88\u9664\u6b65\u957f\u5f02\u8d28\u6027\u3002", "result": "\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u80fd\u591f\u6d88\u9664\u6b65\u957f\u5f02\u8d28\u6027\uff0c\u5e76\u5df2\u8bc1\u660e\u5176\u6536\u655b\u4e8e\u6700\u4f18\u89e3\u3002", "conclusion": "\u901a\u8fc7\u6570\u503c\u6a21\u62df\uff0c\u8bc1\u660e\u4e86\u6d88\u9664\u6b65\u957f\u5f02\u8d28\u6027\u53ef\u4ee5\u63d0\u9ad8\u6536\u655b\u901f\u5ea6\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2508.05732", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05732", "abs": "https://arxiv.org/abs/2508.05732", "authors": ["Pinxuan Li", "Bing Cao", "Changqing Zhang", "Qinghua Hu"], "title": "Generalized Few-Shot Out-of-Distribution Detection", "comment": null, "summary": "Few-shot Out-of-Distribution (OOD) detection has emerged as a critical\nresearch direction in machine learning for practical deployment. Most existing\nFew-shot OOD detection methods suffer from insufficient generalization\ncapability for the open world. Due to the few-shot learning paradigm, the OOD\ndetection ability is often overfit to the limited training data itself, thus\ndegrading the performance on generalized data and performing inconsistently\nacross different scenarios. To address this challenge, we proposed a\nGeneralized Few-shot OOD Detection (GOOD) framework, which empowers the general\nknowledge of the OOD detection model with an auxiliary General Knowledge Model\n(GKM), instead of directly learning from few-shot data. We proceed to reveal\nthe few-shot OOD detection from a generalization perspective and theoretically\nderive the Generality-Specificity balance (GS-balance) for OOD detection, which\nprovably reduces the upper bound of generalization error with a general\nknowledge model. Accordingly, we propose a Knowledge Dynamic Embedding (KDE)\nmechanism to adaptively modulate the guidance of general knowledge. KDE\ndynamically aligns the output distributions of the OOD detection model to the\ngeneral knowledge model based on the Generalized Belief (G-Belief) of GKM,\nthereby boosting the GS-balance. Experiments on real-world OOD benchmarks\ndemonstrate our superiority. Codes will be available.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51faGOOD\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u901a\u7528\u77e5\u8bc6\u6a21\u578b\uff08GKM\uff09\u548c\u77e5\u8bc6\u52a8\u6001\u5d4c\u5165\uff08KDE\uff09\u673a\u5236\uff0c\u89e3\u51b3\u5c11\u6837\u672cOOD\u68c0\u6d4b\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u5c11\u6837\u672cOOD\u68c0\u6d4b\u65b9\u6cd5\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u5bb9\u6613\u8fc7\u62df\u5408\u6709\u9650\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u5bfc\u81f4\u5728\u6cdb\u5316\u6570\u636e\u4e0a\u7684\u6027\u80fd\u4e0b\u964d\uff0c\u5e76\u4e14\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u8868\u73b0\u4e0d\u4e00\u81f4\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u5e7f\u4e49\u5c11\u6837\u672cOOD\u68c0\u6d4b\u201d\uff08GOOD\uff09\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u5f15\u5165\u4e00\u4e2a\u8f85\u52a9\u7684\u201c\u901a\u7528\u77e5\u8bc6\u6a21\u578b\u201d\uff08GKM\uff09\u6765\u589e\u5f3aOOD\u68c0\u6d4b\u6a21\u578b\u7684\u901a\u7528\u77e5\u8bc6\uff0c\u800c\u4e0d\u662f\u76f4\u63a5\u4ece\u5c11\u6837\u672c\u6570\u636e\u4e2d\u5b66\u4e60\u3002\u4ece\u6cdb\u5316\u89d2\u5ea6\u63ed\u793a\u4e86\u5c11\u6837\u672cOOD\u68c0\u6d4b\u95ee\u9898\uff0c\u5e76\u4ece\u7406\u8bba\u4e0a\u63a8\u5bfc\u4e86\u7528\u4e8eOOD\u68c0\u6d4b\u7684\u201c\u6cdb\u5316-\u7279\u5f02\u6027\u5e73\u8861\u201d\uff08GS\u5e73\u8861\uff09\uff0c\u4ee5\u671f\u901a\u8fc7\u901a\u7528\u77e5\u8bc6\u6a21\u578b\u6765\u964d\u4f4e\u6cdb\u5316\u8bef\u5dee\u7684\u4e0a\u754c\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u201c\u77e5\u8bc6\u52a8\u6001\u5d4c\u5165\u201d\uff08KDE\uff09\u673a\u5236\uff0c\u901a\u8fc7GKM\u7684\u5e7f\u4e49\u4fe1\u5ff5\uff08G-Belief\uff09\u81ea\u9002\u5e94\u5730\u8c03\u8282\u901a\u7528\u77e5\u8bc6\u7684\u6307\u5bfc\uff0c\u52a8\u6001\u5730\u5c06OOD\u68c0\u6d4b\u6a21\u578b\u7684\u8f93\u51fa\u5206\u5e03\u4e0e\u901a\u7528\u77e5\u8bc6\u6a21\u578b\u8fdb\u884c\u5bf9\u9f50\uff0c\u4ece\u800c\u63d0\u5347GS\u5e73\u8861\u3002", "result": "\u6240\u63d0\u51fa\u7684GOOD\u6846\u67b6\u901a\u8fc7\u5f15\u5165GKM\u548cKDE\u673a\u5236\uff0c\u5e76\u5728\u7406\u8bba\u4e0a\u63a8\u5bfcGS\u5e73\u8861\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5e76\u5728\u771f\u5b9e\u4e16\u754c\u7684OOD\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u771f\u5b9e\u4e16\u754cOOD\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u8d8a\u3002"}}
{"id": "2508.05838", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG", "cs.SY", "eess.SY", "68T07, 68T40, 90C40, 93E35", "I.2.6; I.2.9; I.2.10"], "pdf": "https://arxiv.org/pdf/2508.05838", "abs": "https://arxiv.org/abs/2508.05838", "authors": ["Ahmad Farooq", "Kamran Iqbal"], "title": "Integrating Vision Foundation Models with Reinforcement Learning for Enhanced Object Interaction", "comment": "Published in the Proceedings of the 2025 3rd International Conference\n  on Robotics, Control and Vision Engineering (RCVE'25). 6 pages, 3 figures, 1\n  table", "summary": "This paper presents a novel approach that integrates vision foundation models\nwith reinforcement learning to enhance object interaction capabilities in\nsimulated environments. By combining the Segment Anything Model (SAM) and\nYOLOv5 with a Proximal Policy Optimization (PPO) agent operating in the\nAI2-THOR simulation environment, we enable the agent to perceive and interact\nwith objects more effectively. Our comprehensive experiments, conducted across\nfour diverse indoor kitchen settings, demonstrate significant improvements in\nobject interaction success rates and navigation efficiency compared to a\nbaseline agent without advanced perception. The results show a 68% increase in\naverage cumulative reward, a 52.5% improvement in object interaction success\nrate, and a 33% increase in navigation efficiency. These findings highlight the\npotential of integrating foundation models with reinforcement learning for\ncomplex robotic tasks, paving the way for more sophisticated and capable\nautonomous agents.", "AI": {"tldr": "\u672c\u7814\u7a76\u5c06SAM\u548cYOLOv5\u7b49\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u4e0ePPO\u5f3a\u5316\u5b66\u4e60\u76f8\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u5728AI2-THOR\u6a21\u62df\u53a8\u623f\u73af\u5883\u4e2d\u7684\u7269\u4f53\u4ea4\u4e92\u6210\u529f\u7387\u548c\u5bfc\u822a\u6548\u7387\u3002", "motivation": "\u4e3a\u4e86\u589e\u5f3a\u673a\u5668\u4eba\u5728\u6a21\u62df\u73af\u5883\u4e2d\u4e0e\u7269\u4f53\u4ea4\u4e92\u7684\u80fd\u529b\uff0c\u5e76\u63d0\u9ad8\u5176\u5bfc\u822a\u6548\u7387\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u5c06\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08SAM\u548cYOLOv5\uff09\u4e0e\u5f3a\u5316\u5b66\u4e60\uff08PPO\uff09\u76f8\u7ed3\u5408\uff0c\u5e76\u5728AI2-THOR\u6a21\u62df\u73af\u5883\u4e2d\u8fdb\u884c\u8bad\u7ec3\u548c\u6d4b\u8bd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u672a\u4f7f\u7528\u9ad8\u7ea7\u611f\u77e5\u7684\u57fa\u7ebf\u4ee3\u7406\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5728\u5e73\u5747\u7d2f\u79ef\u5956\u52b1\u65b9\u9762\u63d0\u9ad8\u4e8668%\uff0c\u5728\u7269\u4f53\u4ea4\u4e92\u6210\u529f\u7387\u65b9\u9762\u63d0\u9ad8\u4e8652.5%\uff0c\u5728\u5bfc\u822a\u6548\u7387\u65b9\u9762\u63d0\u9ad8\u4e8633%\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u5c06\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u4e0e\u5f3a\u5316\u5b66\u4e60\u76f8\u7ed3\u5408\uff0c\u5728\u63d0\u9ad8\u673a\u5668\u4eba\u4e0e\u7269\u4f53\u4ea4\u4e92\u80fd\u529b\u548c\u5bfc\u822a\u6548\u7387\u65b9\u9762\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4e3a\u66f4\u590d\u6742\u3001\u66f4\u5f3a\u5927\u7684\u81ea\u4e3b\u4ee3\u7406\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2508.05766", "categories": ["cs.AI", "cs.LG", "cs.SY", "eess.SY", "nlin.AO"], "pdf": "https://arxiv.org/pdf/2508.05766", "abs": "https://arxiv.org/abs/2508.05766", "authors": ["Bo Wen"], "title": "A Framework for Inherently Safer AGI through Language-Mediated Active Inference", "comment": null, "summary": "This paper proposes a novel framework for developing safe Artificial General\nIntelligence (AGI) by combining Active Inference principles with Large Language\nModels (LLMs). We argue that traditional approaches to AI safety, focused on\npost-hoc interpretability and reward engineering, have fundamental limitations.\nWe present an architecture where safety guarantees are integrated into the\nsystem's core design through transparent belief representations and\nhierarchical value alignment. Our framework leverages natural language as a\nmedium for representing and manipulating beliefs, enabling direct human\noversight while maintaining computational tractability. The architecture\nimplements a multi-agent system where agents self-organize according to Active\nInference principles, with preferences and safety constraints flowing through\nhierarchical Markov blankets. We outline specific mechanisms for ensuring\nsafety, including: (1) explicit separation of beliefs and preferences in\nnatural language, (2) bounded rationality through resource-aware free energy\nminimization, and (3) compositional safety through modular agent structures.\nThe paper concludes with a research agenda centered on the Abstraction and\nReasoning Corpus (ARC) benchmark, proposing experiments to validate our\nframework's safety properties. Our approach offers a path toward AGI\ndevelopment that is inherently safer, rather than retrofitted with safety\nmeasures.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u5408\u4e3b\u52a8\u63a8\u7406\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u5f00\u53d1\u5b89\u5168\u7684\u4eba\u5de5\u901a\u7528\u667a\u80fd\uff08AGI\uff09\u3002\u8be5\u6846\u67b6\u5c06\u5b89\u5168\u4fdd\u969c\u96c6\u6210\u5230\u6838\u5fc3\u8bbe\u8ba1\u4e2d\uff0c\u5229\u7528\u81ea\u7136\u8bed\u8a00\u8fdb\u884c\u4fe1\u5ff5\u8868\u5f81\u548c\u4eba\u7c7b\u76d1\u7763\uff0c\u5e76\u901a\u8fc7\u5206\u5c42\u9a6c\u5c14\u53ef\u592b\u6bef\u5b9e\u73b0\u4ee3\u7406\u7684\u81ea\u6211\u7ec4\u7ec7\u3002\u7814\u7a76\u63d0\u51fa\u4e86\u5177\u4f53\u7684\u5b89\u5168\u673a\u5236\uff0c\u5e76\u8ba1\u5212\u5728ARC\u57fa\u51c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u65e8\u5728\u5b9e\u73b0\u5185\u5728\u66f4\u5b89\u5168\u3001\u800c\u975e\u4e8b\u540e\u6dfb\u52a0\u5b89\u5168\u63aa\u65bd\u7684AGI\u5f00\u53d1\u3002", "motivation": "\u4f20\u7edf\u7684AI\u5b89\u5168\u65b9\u6cd5\uff0c\u4fa7\u91cd\u4e8e\u4e8b\u540e\u53ef\u89e3\u91ca\u6027\u548c\u5956\u52b1\u5de5\u7a0b\uff0c\u5b58\u5728\u6839\u672c\u6027\u7684\u5c40\u9650\u6027\u3002\u672c\u7814\u7a76\u65e8\u5728\u63d0\u4f9b\u4e00\u79cd\u66f4\u5b89\u5168\u7684\u65b9\u6cd5\u6765\u5f00\u53d1AGI\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u4e3b\u52a8\u63a8\u7406\u539f\u7406\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u76f8\u7ed3\u5408\u7684\u65b0\u9896\u6846\u67b6\uff0c\u7528\u4e8e\u5f00\u53d1\u5b89\u5168\u7684\u4eba\u5de5\u901a\u7528\u667a\u80fd\uff08AGI\uff09\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u900f\u660e\u7684\u4fe1\u5ff5\u8868\u5f81\u548c\u5206\u5c42\u4ef7\u503c\u5bf9\u9f50\uff0c\u5c06\u5b89\u5168\u4fdd\u8bc1\u6574\u5408\u5230\u7cfb\u7edf\u7684\u6838\u5fc3\u8bbe\u8ba1\u4e2d\u3002\u5b83\u5229\u7528\u81ea\u7136\u8bed\u8a00\u4f5c\u4e3a\u8868\u5f81\u548c\u64cd\u7eb5\u4fe1\u5ff5\u7684\u5a92\u4ecb\uff0c\u5b9e\u73b0\u4e86\u76f4\u63a5\u7684\u4eba\u5de5\u76d1\u7763\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8ba1\u7b97\u4e0a\u7684\u53ef\u884c\u6027\u3002\u8be5\u67b6\u6784\u5b9e\u73b0\u4e86\u4e00\u4e2a\u591a\u4e3b\u4f53\u7cfb\u7edf\uff0c\u4ee3\u7406\u6839\u636e\u4e3b\u52a8\u63a8\u7406\u539f\u7406\u8fdb\u884c\u81ea\u6211\u7ec4\u7ec7\uff0c\u504f\u597d\u548c\u5b89\u5168\u7ea6\u675f\u901a\u8fc7\u5206\u5c42\u9a6c\u5c14\u53ef\u592b\u6bef\u6d41\u52a8\u3002\u7814\u7a76\u63d0\u51fa\u4e86\u5177\u4f53\u7684\u5b89\u5168\u4fdd\u969c\u673a\u5236\uff0c\u5305\u62ec\uff1a1\uff09\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u663e\u5f0f\u5206\u79bb\u4fe1\u5ff5\u548c\u504f\u597d\uff1b2\uff09\u901a\u8fc7\u8d44\u6e90\u611f\u77e5\u7684\u81ea\u7531\u80fd\u6700\u5c0f\u5316\u5b9e\u73b0\u6709\u754c\u7406\u6027\uff1b3\uff09\u901a\u8fc7\u6a21\u5757\u5316\u4ee3\u7406\u7ed3\u6784\u5b9e\u73b0\u7ec4\u5408\u5b89\u5168\u6027\u3002", "result": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u6846\u67b6\u901a\u8fc7\u5c06\u4e3b\u52a8\u63a8\u7406\u4e0eLLM\u76f8\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86AGI\u7684\u5b89\u5168\u5f00\u53d1\uff0c\u5176\u6838\u5fc3\u8bbe\u8ba1\u6574\u5408\u4e86\u5b89\u5168\u4fdd\u8bc1\u3002\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u8868\u5f81\u4fe1\u5ff5\uff0c\u5b9e\u73b0\u4e86\u4eba\u5de5\u76d1\u7763\u548c\u8ba1\u7b97\u53ef\u884c\u6027\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u663e\u5f0f\u5206\u79bb\u4fe1\u5ff5\u548c\u504f\u597d\u3001\u6709\u754c\u7406\u6027\u548c\u7ec4\u5408\u5b89\u5168\u6027\u673a\u5236\u6765\u786e\u4fdd\u5b89\u5168\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u5c06\u4e3b\u52a8\u63a8\u7406\u539f\u5219\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u76f8\u7ed3\u5408\uff0c\u4ee5\u5f00\u53d1\u5b89\u5168\u7684\u4eba\u5de5\u901a\u7528\u667a\u80fd\uff08AGI\uff09\u7684\u65b0\u9896\u6846\u67b6\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u900f\u660e\u7684\u4fe1\u5ff5\u8868\u5f81\u548c\u5206\u5c42\u4ef7\u503c\u5bf9\u9f50\uff0c\u5c06\u5b89\u5168\u4fdd\u8bc1\u6574\u5408\u5230\u7cfb\u7edf\u7684\u6838\u5fc3\u8bbe\u8ba1\u4e2d\u3002"}}
{"id": "2508.06088", "categories": ["cs.LO"], "pdf": "https://arxiv.org/pdf/2508.06088", "abs": "https://arxiv.org/abs/2508.06088", "authors": ["Kittiphon Phalakarn", "Yun Chen Tsai", "Ichiro Hasuo"], "title": "Widest Path Games and Maximality Inheritance in Bounded Value Iteration for Stochastic Games", "comment": null, "summary": "For model checking stochastic games (SGs), bounded value iteration (BVI)\nalgorithms have gained attention as efficient approximate methods with rigorous\nprecision guarantees. However, BVI may not terminate or converge when the\ntarget SG contains end components. Most existing approaches address this issue\nby explicitly detecting and processing end components--a process that is often\ncomputationally expensive. An exception is the widest path-based BVI approach\npreviously studied by Phalakarn et al., which we refer to as 1WP-BVI. The\nmethod performs particularly well in the presence of numerous end components.\nNonetheless, its theoretical foundations remain somewhat ad hoc. In this paper,\nwe identify and formalize the core principles underlying the widest path-based\nBVI approach by (i) presenting 2WP-BVI, a clean BVI algorithm based on\n(2-player) widest path games, and (ii) proving its correctness using what we\ncall the maximality inheritance principle--a proof principle previously\nemployed in a well-known result in probabilistic model checking. Our\nexperimental results demonstrate the practical relevance and potential of our\nproposed 2WP-BVI algorithm.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e862WP-BVI\u7b97\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709BVI\u7b97\u6cd5\u5728\u5904\u7406\u542b\u7ec8\u7aef\u7ec4\u4ef6\u7684\u968f\u673a\u535a\u5f08\u65f6\u7684\u5c40\u9650\u6027\uff0c\u5e76\u901a\u8fc7\u6700\u5927\u5316\u7ee7\u627f\u539f\u7406\u8bc1\u660e\u4e86\u5176\u6b63\u786e\u6027\uff0c\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u6709\u754c\u503c\u8fed\u4ee3\uff08BVI\uff09\u7b97\u6cd5\u5728\u5904\u7406\u5305\u542b\u7ec8\u7aef\u7ec4\u4ef6\u7684\u968f\u673a\u535a\u5f08\uff08SG\uff09\u65f6\u53ef\u80fd\u4e0d\u7ec8\u6b62\u6216\u4e0d\u6536\u655b\u7684\u95ee\u9898\uff0c\u5e76\u4e3aPhalakarn\u7b49\u4eba\u5148\u524d\u7814\u7a76\u7684\u57fa\u4e8e\u6700\u5bbd\u8def\u5f84\u7684BVI\u65b9\u6cd5\uff081WP-BVI\uff09\u63d0\u4f9b\u66f4\u575a\u5b9e\u7684\u7406\u8bba\u57fa\u7840\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e862WP-BVI\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u57fa\u4e8e\uff082\u4eba\uff09\u6700\u5bbd\u8def\u5f84\u535a\u5f08\uff0c\u5e76\u4f7f\u7528\u6700\u5927\u5316\u7ee7\u627f\u539f\u7406\u8bc1\u660e\u4e86\u5176\u6b63\u786e\u6027\u3002", "result": "\u672c\u6587\u63d0\u51fa\u76842WP-BVI\u7b97\u6cd5\u5728\u5b9e\u8df5\u4e2d\u5177\u6709\u76f8\u5173\u6027\u548c\u6f5c\u529b\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e862WP-BVI\u7b97\u6cd5\uff0c\u5e76\u4f7f\u7528\u6700\u5927\u5316\u7ee7\u627f\u539f\u7406\u8bc1\u660e\u4e86\u5176\u6b63\u786e\u6027\u3002\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u8be5\u7b97\u6cd5\u7684\u5b9e\u9645\u610f\u4e49\u548c\u6f5c\u529b\u3002"}}
{"id": "2508.06389", "categories": ["cs.NE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06389", "abs": "https://arxiv.org/abs/2508.06389", "authors": ["James Stovold"], "title": "Identity Increases Stability in Neural Cellular Automata", "comment": "Accepted to ALIFE 2025", "summary": "Neural Cellular Automata (NCAs) offer a way to study the growth of\ntwo-dimensional artificial organisms from a single seed cell. From the outset,\nNCA-grown organisms have had issues with stability, their natural boundary\noften breaking down and exhibiting tumour-like growth or failing to maintain\nthe expected shape. In this paper, we present a method for improving the\nstability of NCA-grown organisms by introducing an 'identity' layer with simple\nconstraints during training.\n  Results show that NCAs grown in close proximity are more stable compared with\nthe original NCA model. Moreover, only a single identity value is required to\nachieve this increase in stability. We observe emergent movement from the\nstable organisms, with increasing prevalence for models with multiple identity\nvalues.\n  This work lays the foundation for further study of the interaction between\nNCA-grown organisms, paving the way for studying social interaction at a\ncellular level in artificial organisms.", "AI": {"tldr": "\u901a\u8fc7\u5f15\u5165\u201c\u8eab\u4efd\u201d\u5c42\u548c\u7b80\u5355\u7ea6\u675f\u6765\u63d0\u9ad8NCA\u751f\u7269\u4f53\u7684\u7a33\u5b9a\u6027\uff0c\u5e76\u89c2\u5bdf\u5230\u5176\u79fb\u52a8\u73b0\u8c61\u3002", "motivation": "\u89e3\u51b3NCA\u751f\u957f\u751f\u7269\u4f53\u7a33\u5b9a\u6027\u5dee\u3001\u8fb9\u754c\u6613\u5d29\u6e83\u3001\u51fa\u73b0\u80bf\u7624\u6837\u751f\u957f\u6216\u65e0\u6cd5\u4fdd\u6301\u9884\u671f\u5f62\u72b6\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5728\u8bad\u7ec3\u4e2d\u5f15\u5165\u4e00\u4e2a\u5177\u6709\u7b80\u5355\u7ea6\u675f\u7684'\u8eab\u4efd'\u5c42\u6765\u6539\u8fdbNCA\u7684\u7a33\u5b9a\u6027\u3002", "result": "\u4e0e\u539f\u59cbNCA\u6a21\u578b\u76f8\u6bd4\uff0c\u8fd1\u8ddd\u79bb\u751f\u957f\u7684NCAs\u66f4\u7a33\u5b9a\uff0c\u4ec5\u9700\u4e00\u4e2a\u8eab\u4efd\u503c\u5373\u53ef\u5b9e\u73b0\u7a33\u5b9a\u6027\u63d0\u5347\uff0c\u5e76\u89c2\u5bdf\u5230\u7a33\u5b9a\u7684\u751f\u7269\u4f53\u51fa\u73b0\u79fb\u52a8\u73b0\u8c61\uff0c\u5177\u6709\u591a\u4e2a\u8eab\u4efd\u503c\u7684\u6a21\u578b\u4e2d\u79fb\u52a8\u73b0\u8c61\u66f4\u666e\u904d\u3002", "conclusion": "\u901a\u8fc7\u5728\u8bad\u7ec3\u4e2d\u5f15\u5165\u5177\u6709\u7b80\u5355\u7ea6\u675f\u7684\u201c\u8eab\u4efd\u201d\u5c42\uff0c\u53ef\u4ee5\u63d0\u9ad8NCA\u751f\u957f\u751f\u7269\u4f53\u7684\u7a33\u5b9a\u6027\u3002\u8be5\u65b9\u6cd5\u4ec5\u9700\u4e00\u4e2a\u8eab\u4efd\u503c\u5373\u53ef\u63d0\u9ad8\u7a33\u5b9a\u6027\uff0c\u5e76\u53ef\u80fd\u4fc3\u51fa\u73b0\u8c61\u7ea7\u79fb\u52a8\u3002"}}
{"id": "2508.05724", "categories": ["cs.LG", "physics.data-an", "68T07, 81-08, 05C90", "I.2.6; G.2.2; I.5.1"], "pdf": "https://arxiv.org/pdf/2508.05724", "abs": "https://arxiv.org/abs/2508.05724", "authors": ["Massimiliano Romiti"], "title": "A Graph Neural Network Approach for Mapping the Conceptual Structure and Inter-Branch Connectivity of Physics", "comment": "14 pages, 9 figures", "summary": "This work introduces a novel framework for representing and analyzing\nphysical laws as a weighted knowledge graph. We constructed a database of 659\ndistinct physical equations, subjected to rigorous semantic cleaning to resolve\nnotational ambiguities, resulting in a corpus of 400 advanced physics\nequations. We developed an enhanced graph representation where both physical\nconcepts and equations are nodes, connected by weighted inter-equation bridges.\nThese weights are objectively defined using normalized metrics for variable\noverlap, physics-informed importance scores, and bibliometric data. A Graph\nAttention Network (GAT) was trained for link prediction, achieving a test AUC\nof 0.9742 +/- 0.0018 across five independent runs, significantly outperforming\nboth classical heuristics (best baseline AUC: 0.9487) and established GNN\narchitectures like GraphSAGE (AUC: 0.9504, p = 0.029). Statistical testing\nconfirmed significance of all comparisons (p < 0.05), with 2.7% improvement\nover the best baseline. Our analysis reveals three key findings: (i) The model\nautonomously rediscovers the known macroscopic structure of physics,\nidentifying strong conceptual axes between Electromagnetism and Statistical\nMechanics. (ii) It identifies central hub equations that serve as critical\nbridges between multiple physical domains. (iii) The model generates stable,\ncomputationally-derived hypotheses for cross-domain relationships, identifying\nboth known principles and suggesting novel mathematical analogies for further\ntheoretical investigation. The framework can generate hundreds of such\nhypotheses, enabling the creation of specialized datasets for targeted analysis\nof specific physics subfields. Code and data available at\nhttps://github.com/kingelanci/graphysics", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aGraphysics\u7684\u65b0\u6846\u67b6\uff0c\u5b83\u5c06\u7269\u7406\u5b9a\u5f8b\u8868\u793a\u4e3a\u52a0\u6743\u77e5\u8bc6\u56fe\u8c31\uff0c\u5e76\u4f7f\u7528\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\uff08GAT\uff09\u8fdb\u884c\u94fe\u63a5\u9884\u6d4b\u3002\u8be5\u6846\u67b6\u80fd\u591f\u53d1\u73b0\u7269\u7406\u5b66\u7684\u5b8f\u89c2\u7ed3\u6784\uff0c\u8bc6\u522b\u5173\u952e\u65b9\u7a0b\uff0c\u5e76\u751f\u6210\u65b0\u7684\u8de8\u9886\u57df\u5173\u7cfb\u5047\u8bbe\uff0c\u5728\u94fe\u63a5\u9884\u6d4b\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u63d0\u51fa\u4e00\u79cd\u5c06\u7269\u7406\u5b9a\u5f8b\u8868\u793a\u4e3a\u52a0\u6743\u77e5\u8bc6\u56fe\u8c31\u7684\u65b0\u6846\u67b6\uff0c\u4ee5\u8fdb\u884c\u5206\u6790\u548c\u53d1\u73b0\u6f5c\u5728\u7684\u8de8\u9886\u57df\u8054\u7cfb\u3002", "method": "\u91c7\u7528\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\uff08GAT\uff09\u8fdb\u884c\u94fe\u63a5\u9884\u6d4b\uff0c\u5e76\u4f7f\u7528\u5f52\u4e00\u5316\u7684\u53d8\u91cf\u91cd\u53e0\u3001\u7269\u7406\u4fe1\u606f\u91cd\u8981\u6027\u8bc4\u5206\u548c\u6587\u732e\u8ba1\u91cf\u6570\u636e\u5b9a\u4e49\u7684\u6743\u91cd\u6765\u589e\u5f3a\u56fe\u8868\u793a\u3002", "result": "\u5728\u94fe\u63a5\u9884\u6d4b\u4efb\u52a1\u4e2d\uff0cGAT\u6a21\u578b\u8fbe\u5230\u4e860.9742 +/- 0.0018\u7684\u6d4b\u8bd5AUC\uff0c\u663e\u8457\u4f18\u4e8e\u7ecf\u5178\u542f\u53d1\u5f0f\u65b9\u6cd5\uff08\u6700\u4f73AUC\uff1a0.9487\uff09\u548cGraphSAGE\uff08AUC\uff1a0.9504\uff09\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u81ea\u4e3b\u53d1\u73b0\u7269\u7406\u5b66\u7684\u5b8f\u89c2\u7ed3\u6784\uff0c\u8bc6\u522b\u51fa\u7535\u78c1\u5b66\u548c\u7edf\u8ba1\u529b\u5b66\u4e4b\u95f4\u7684\u5f3a\u6982\u5ff5\u8054\u7cfb\uff0c\u5e76\u627e\u51fa\u8fde\u63a5\u591a\u4e2a\u7269\u7406\u9886\u57df\u7684\u5173\u952e\u4e2d\u5fc3\u65b9\u7a0b\u3002\u6b64\u5916\uff0c\u8be5\u6a21\u578b\u80fd\u591f\u751f\u6210\u7a33\u5b9a\u7684\u3001\u7ecf\u8fc7\u8ba1\u7b97\u5f97\u51fa\u7684\u8de8\u9886\u57df\u5173\u7cfb\u5047\u8bbe\uff0c\u5305\u62ec\u5df2\u77e5\u7684\u539f\u7406\u548c\u65b0\u7684\u6570\u5b66\u7c7b\u6bd4\uff0c\u4e3a\u7406\u8bba\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\u3002\u8be5\u6846\u67b6\u80fd\u591f\u751f\u6210\u6570\u767e\u4e2a\u6b64\u7c7b\u5047\u8bbe\uff0c\u5e76\u4e3a\u7279\u5b9a\u7269\u7406\u5b50\u9886\u57df\u521b\u5efa\u4e13\u95e8\u7684\u6570\u636e\u96c6\u3002"}}
{"id": "2508.06047", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2508.06047", "abs": "https://arxiv.org/abs/2508.06047", "authors": ["Suresh Purini", "Siddhant Garg", "Mudit Gaur", "Sankalp Bhat", "Sohan Mupparapu", "Arun Ravindran"], "title": "ArchXBench: A Complex Digital Systems Benchmark Suite for LLM Driven RTL Synthesis", "comment": "Published in 7th ACM/IEEE International Symposium on Machine Learning\n  for CAD", "summary": "Modern SoC datapaths include deeply pipelined, domain-specific accelerators,\nbut their RTL implementation and verification are still mostly done by hand.\nWhile large language models (LLMs) exhibit advanced code-generation abilities\nfor programming languages like Python, their application to Verilog-like RTL\nremains in its nascent stage. This is reflected in the simple arithmetic and\ncontrol circuits currently used to evaluate generative capabilities in existing\nbenchmarks. In this paper, we introduce ArchXBench, a six-level benchmark suite\nthat encompasses complex arithmetic circuits and other advanced digital\nsubsystems drawn from domains such as cryptography, image processing, machine\nlearning, and signal processing. Architecturally, some of these designs are\npurely combinational, others are multi-cycle or pipelined, and many require\nhierarchical composition of modules. For each benchmark, we provide a problem\ndescription, design specification, and testbench, enabling rapid research in\nthe area of LLM-driven agentic approaches for complex digital systems design.\n  Using zero-shot prompting with Claude Sonnet 4, GPT 4.1, o4-mini-high, and\nDeepSeek R1 under a pass@5 criterion, we observed that o4-mini-high\nsuccessfully solves the largest number of benchmarks, 16 out of 30, spanning\nLevels 1, 2, and 3. From Level 4 onward, however, all models consistently fail,\nhighlighting a clear gap in the capabilities of current state-of-the-art LLMs\nand prompting/agentic approaches.", "AI": {"tldr": "LLM \u5728 RTL \u8bbe\u8ba1\u65b9\u9762\u80fd\u529b\u6709\u9650\uff0c\u5373\u4f7f\u662f\u6700\u5148\u8fdb\u7684\u6a21\u578b\u4e5f\u65e0\u6cd5\u5904\u7406\u590d\u6742\u7684\u6570\u5b57\u7cfb\u7edf\u8bbe\u8ba1\u3002ArchXBench \u57fa\u51c6\u5957\u4ef6\u65e8\u5728\u63a8\u52a8 LLM \u5728\u6b64\u9886\u57df\u7684\u7814\u7a76\u3002", "motivation": "\u73b0\u4ee3 SoC \u6570\u636e\u901a\u8def\u5305\u542b\u6df1\u5ea6\u6d41\u6c34\u7ebf\u3001\u7279\u5b9a\u9886\u57df\u7684\u52a0\u901f\u5668\uff0c\u4f46\u5176 RTL \u5b9e\u73b0\u548c\u9a8c\u8bc1\u4ecd\u4e3b\u8981\u624b\u52a8\u5b8c\u6210\u3002\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728 Python \u7b49\u7f16\u7a0b\u8bed\u8a00\u65b9\u9762\u5c55\u73b0\u51fa\u5148\u8fdb\u7684\u4ee3\u7801\u751f\u6210\u80fd\u529b\uff0c\u4f46\u5176\u5728 Verilog \u7c7b RTL \u65b9\u9762\u7684\u5e94\u7528\u4ecd\u5904\u4e8e\u521d\u7ea7\u9636\u6bb5\u3002\u73b0\u6709\u7684\u57fa\u51c6\u6d4b\u8bd5\u4ec5\u4f7f\u7528\u7b80\u5355\u7684\u7b97\u672f\u548c\u63a7\u5236\u7535\u8def\u6765\u8bc4\u4f30\u751f\u6210\u80fd\u529b\u3002", "method": "\u4ecb\u7ecd\u4e86 ArchXBench\uff0c\u4e00\u4e2a\u5305\u542b\u590d\u6742\u7b97\u672f\u7535\u8def\u548c\u5176\u4ed6\u9ad8\u7ea7\u6570\u5b57\u5b50\u7cfb\u7edf\u7684\u516d\u7ea7\u57fa\u51c6\u5957\u4ef6\uff0c\u6db5\u76d6\u5bc6\u7801\u5b66\u3001\u56fe\u50cf\u5904\u7406\u3001\u673a\u5668\u5b66\u4e60\u548c\u4fe1\u53f7\u5904\u7406\u7b49\u9886\u57df\u3002\u8be5\u57fa\u51c6\u5957\u4ef6\u63d0\u4f9b\u4e86\u95ee\u9898\u63cf\u8ff0\u3001\u8bbe\u8ba1\u89c4\u8303\u548c\u6d4b\u8bd5\u5e73\u53f0\uff0c\u4ee5\u4fc3\u8fdb LLM \u9a71\u52a8\u7684\u590d\u6742\u6570\u5b57\u7cfb\u7edf\u8bbe\u8ba1\u7684\u4ee3\u7406\u65b9\u6cd5\u7684\u7814\u7a76\u3002\u4f7f\u7528 Claude Sonnet 4\u3001GPT 4.1\u3001o4-mini-high \u548c DeepSeek R1 \u6a21\u578b\uff0c\u5728 pass@5 \u6807\u51c6\u4e0b\u8fdb\u884c\u4e86\u96f6\u6837\u672c\u63d0\u793a\u8bc4\u4f30\u3002", "result": "o4-mini-high \u5728 30 \u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684 16 \u4e2a\uff08\u6db5\u76d6 Level 1\u30012 \u548c 3\uff09\u6210\u529f\u89e3\u51b3\uff0c\u6570\u91cf\u6700\u591a\u3002\u7136\u800c\uff0c\u4ece Level 4 \u5f00\u59cb\uff0c\u6240\u6709\u6a21\u578b\u5747\u672a\u80fd\u6210\u529f\u89e3\u51b3\u3002", "conclusion": "\u76ee\u524d\u6700\u5148\u8fdb\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u63d0\u793a/\u4ee3\u7406\u65b9\u6cd5\u5728\u5904\u7406\u66f4\u590d\u6742\u7684\u6570\u5b57\u7cfb\u7edf\u8bbe\u8ba1\u65b9\u9762\u4ecd\u5b58\u5728\u660e\u663e\u5dee\u8ddd\uff0c\u4ece ArchXBench \u7684 Level 4 \u5f00\u59cb\uff0c\u6240\u6709\u6a21\u578b\u90fd\u65e0\u6cd5\u6210\u529f\u89e3\u51b3\u3002"}}
{"id": "2508.05831", "categories": ["cs.LG", "cs.NA", "math.NA", "15A29 Inverse problems in linear algebra 65F22, 68T07, 65F05, 62C12", "G.1.3; F.2.1; I.2.6"], "pdf": "https://arxiv.org/pdf/2508.05831", "abs": "https://arxiv.org/abs/2508.05831", "authors": ["Alexander DeLise", "Kyle Loh", "Krish Patel", "Meredith Teague", "Andrea Arnold", "Matthias Chung"], "title": "Optimal Linear Baseline Models for Scientific Machine Learning", "comment": "40 pages, 10 Figures, 9 Tables", "summary": "Across scientific domains, a fundamental challenge is to characterize and\ncompute the mappings from underlying physical processes to observed signals and\nmeasurements. While nonlinear neural networks have achieved considerable\nsuccess, they remain theoretically opaque, which hinders adoption in contexts\nwhere interpretability is paramount. In contrast, linear neural networks serve\nas a simple yet effective foundation for gaining insight into these complex\nrelationships. In this work, we develop a unified theoretical framework for\nanalyzing linear encoder-decoder architectures through the lens of Bayes risk\nminimization for solving data-driven scientific machine learning problems. We\nderive closed-form, rank-constrained linear and affine linear optimal mappings\nfor forward modeling and inverse recovery tasks. Our results generalize\nexisting formulations by accommodating rank-deficiencies in data, forward\noperators, and measurement processes. We validate our theoretical results by\nconducting numerical experiments on datasets from simple biomedical imaging,\nfinancial factor analysis, and simulations involving nonlinear fluid dynamics\nvia the shallow water equations. This work provides a robust baseline for\nunderstanding and benchmarking learned neural network models for scientific\nmachine learning problems.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u79d1\u5b66\u673a\u5668\u5b66\u4e60\u7684\u7ebf\u6027\u795e\u7ecf\u7f51\u7edc\u7406\u8bba\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u6620\u5c04\u95ee\u9898\uff0c\u5e76\u80fd\u5728\u591a\u79cd\u79d1\u5b66\u9886\u57df\u63d0\u4f9b\u53ef\u89e3\u91ca\u548c\u53ef\u9760\u7684\u57fa\u51c6\u3002", "motivation": "\u89e3\u51b3\u79d1\u5b66\u7814\u7a76\u4e2d\u4e00\u4e2a\u57fa\u672c\u6311\u6218\uff1a\u523b\u753b\u548c\u8ba1\u7b97\u4ece\u7269\u7406\u8fc7\u7a0b\u5230\u89c2\u6d4b\u4fe1\u53f7\u548c\u6d4b\u91cf\u7684\u6620\u5c04\u3002\u73b0\u6709\u975e\u7ebf\u6027\u795e\u7ecf\u7f51\u7edc\u867d\u7136\u6210\u529f\uff0c\u4f46\u7406\u8bba\u4e0a\u4e0d\u900f\u660e\uff0c\u5728\u9700\u8981\u53ef\u89e3\u91ca\u6027\u7684\u573a\u666f\u4e2d\u5e94\u7528\u53d7\u9650\u3002\u7ebf\u6027\u795e\u7ecf\u7f51\u7edc\u4f5c\u4e3a\u7b80\u5355\u6709\u6548\u7684\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u7406\u89e3\u590d\u6742\u5173\u7cfb\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u6790\u7ebf\u6027\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\uff0c\u5e76\u5229\u7528\u8d1d\u53f6\u65af\u98ce\u9669\u6700\u5c0f\u5316\u6765\u89e3\u51b3\u6570\u636e\u9a71\u52a8\u7684\u79d1\u5b66\u673a\u5668\u5b66\u4e60\u95ee\u9898\u3002\u5177\u4f53\u63a8\u5bfc\u4e86\u524d\u5411\u5efa\u6a21\u548c\u9006\u5411\u6062\u590d\u4efb\u52a1\u7684\u95ed\u5f0f\u3001\u79e9\u7ea6\u675f\u7684\u7ebf\u6027\u548c\u4eff\u5c04\u7ebf\u6027\u6700\u4f18\u6620\u5c04\uff0c\u5e76\u8003\u8651\u4e86\u6570\u636e\u3001\u524d\u5411\u7b97\u5b50\u548c\u6d4b\u91cf\u8fc7\u7a0b\u7684\u79e9\u4e8f\u7f3a\u60c5\u51b5\u3002", "result": "\u63a8\u5bfc\u4e86\u524d\u5411\u5efa\u6a21\u548c\u9006\u5411\u6062\u590d\u4efb\u52a1\u7684\u95ed\u5f0f\u3001\u79e9\u4e8f\u7f3a\u7684\u7ebf\u6027\u548c\u4eff\u5c04\u7ebf\u6027\u6700\u4f18\u6620\u5c04\u3002\u901a\u8fc7\u5728\u751f\u7269\u533b\u5b66\u6210\u50cf\u3001\u91d1\u878d\u56e0\u5b50\u5206\u6790\u548c\u975e\u7ebf\u6027\u6d41\u4f53\u52a8\u529b\u5b66\uff08\u6d45\u6c34\u65b9\u7a0b\uff09\u7b49\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u6570\u503c\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86\u7406\u8bba\u7ed3\u679c\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u79d1\u5b66\u673a\u5668\u5b66\u4e60\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u6790\u7ebf\u6027\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\uff0c\u5e76\u901a\u8fc7\u8d1d\u53f6\u65af\u98ce\u9669\u6700\u5c0f\u5316\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002\u7814\u7a76\u63a8\u5bfc\u4e86\u524d\u5411\u5efa\u6a21\u548c\u9006\u5411\u6062\u590d\u4efb\u52a1\u7684\u95ed\u5f0f\u3001\u79e9\u7ea6\u675f\u7684\u7ebf\u6027\u548c\u4eff\u5c04\u7ebf\u6027\u6700\u4f18\u6620\u5c04\uff0c\u5e76\u80fd\u63a8\u5e7f\u5230\u6570\u636e\u3001\u524d\u5411\u7b97\u5b50\u548c\u6d4b\u91cf\u8fc7\u7a0b\u79e9\u4e8f\u7f3a\u7684\u60c5\u51b5\u3002\u901a\u8fc7\u5728\u751f\u7269\u533b\u5b66\u6210\u50cf\u3001\u91d1\u878d\u56e0\u5b50\u5206\u6790\u548c\u975e\u7ebf\u6027\u6d41\u4f53\u52a8\u529b\u5b66\uff08\u6d45\u6c34\u65b9\u7a0b\uff09\u7b49\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u6570\u503c\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86\u7406\u8bba\u7ed3\u679c\u7684\u6709\u6548\u6027\u3002\u8be5\u7814\u7a76\u4e3a\u7406\u89e3\u548c\u8bc4\u4f30\u79d1\u5b66\u673a\u5668\u5b66\u4e60\u4e2d\u5b66\u4e60\u5230\u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u63d0\u4f9b\u4e86\u575a\u5b9e\u7684\u57fa\u7840\u548c\u57fa\u51c6\u3002"}}
{"id": "2508.06086", "categories": ["cs.GR", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.06086", "abs": "https://arxiv.org/abs/2508.06086", "authors": ["Kojiro Tanaka", "Keiichi Sato", "Masahiko Mikawa", "Makoto Fujisawa"], "title": "Exploring Interactive Simulation of Grass Display Color Characteristic Based on Real-World Conditions", "comment": "Accepted to 20th IFIP TC13 International Conference on Human-Computer\n  Interaction (INTERACT '25), 24 pages", "summary": "Recent research has focused on incorporating media into living environments\nvia color-controlled materials and image display. In particular, grass-based\ndisplays have drawn attention as landscape-friendly interactive interfaces. To\ndevelop the grass display, it is important to obtain the grass color change\ncharacteristics that depend on the real environment. However, conventional\nmethods require experiments on actual equipment every time the lighting or\nviewpoint changes, which is time-consuming and costly. Although research has\nbegun on simulating grass colors, this approach still faces significant issues\nas it takes many hours for a single measurement. In this paper, we explore an\ninteractive simulation of a grass display color change characteristic based on\nreal-world conditions in a virtual environment. We evaluated our method's\naccuracy by simulating grass color characteristics across multiple viewpoints\nand environments, and then compared the results against prior work. The results\nindicated that our method tended to simulate the grass color characteristics\nsimilar to the actual characteristics and showed the potential to do so more\nquickly and with comparable accuracy to the previous study.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u865a\u62df\u73af\u5883\u4e2d\u4ea4\u4e92\u5f0f\u6a21\u62df\u8349\u5730\u989c\u8272\u53d8\u5316\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u8017\u65f6\u6602\u8d35\u7684\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u6a21\u62df\u7ed3\u679c\u51c6\u786e\u4e14\u6548\u7387\u9ad8\u3002", "motivation": "\u4e3a\u4e86\u5f00\u53d1\u8349\u5730\u663e\u793a\u5668\uff0c\u9700\u8981\u83b7\u5f97\u4f9d\u8d56\u4e8e\u771f\u5b9e\u73af\u5883\u7684\u8349\u5730\u989c\u8272\u53d8\u5316\u7279\u5f81\u3002\u7136\u800c\uff0c\u4f20\u7edf\u65b9\u6cd5\u5728\u6bcf\u6b21\u66f4\u6539\u7167\u660e\u6216\u89c6\u70b9\u65f6\u90fd\u9700\u8981\u5728\u5b9e\u9645\u8bbe\u5907\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u8fd9\u65e2\u8017\u65f6\u53c8\u6602\u8d35\u3002\u76ee\u524d\u5df2\u6709\u4e00\u4e9b\u6a21\u62df\u8349\u5730\u989c\u8272\u7684\u7814\u7a76\uff0c\u4f46\u4ecd\u5b58\u5728\u6d4b\u91cf\u5355\u4e2a\u6837\u672c\u9700\u8981\u6570\u5c0f\u65f6\u7684\u91cd\u5927\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5728\u865a\u62df\u73af\u5883\u4e2d\uff0c\u57fa\u4e8e\u771f\u5b9e\u4e16\u754c\u7684\u6761\u4ef6\uff0c\u4ea4\u4e92\u5f0f\u5730\u6a21\u62df\u8349\u5730\u663e\u793a\u5668\u7684\u989c\u8272\u53d8\u5316\u7279\u5f81\u3002", "result": "\u5bf9\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u89c6\u70b9\u548c\u73af\u5883\u4e0b\u7684\u6a21\u62df\u8349\u5730\u989c\u8272\u7279\u5f81\u8fdb\u884c\u4e86\u51c6\u786e\u6027\u8bc4\u4f30\uff0c\u5e76\u4e0e\u5148\u524d\u7814\u7a76\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6a21\u62df\u7684\u8349\u5730\u989c\u8272\u7279\u5f81\u4e0e\u5b9e\u9645\u60c5\u51b5\u76f8\u4f3c\uff0c\u5e76\u4e14\u5177\u6709\u66f4\u5feb\u3001\u51c6\u786e\u6027\u76f8\u5f53\u7684\u6f5c\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u503e\u5411\u4e8e\u6a21\u62df\u4e0e\u5b9e\u9645\u60c5\u51b5\u76f8\u4f3c\u7684\u8349\u5730\u989c\u8272\u7279\u5f81\uff0c\u5e76\u4e14\u80fd\u591f\u6bd4\u5148\u524d\u7814\u7a76\u66f4\u5feb\u3001\u540c\u6837\u51c6\u786e\u5730\u8fdb\u884c\u6a21\u62df\u3002"}}
{"id": "2508.05823", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2508.05823", "abs": "https://arxiv.org/abs/2508.05823", "authors": ["Emily M. Garrity", "Theodora Ciobanu", "Andriy Zakutayev", "Vladan Stevanovic"], "title": "Emerging ultra-wide band gap semiconductors for future high-frequency electronics", "comment": "Perspective, 6 pages, 3 figures", "summary": "To meet the growing demands of advanced electronic systems, next-generation\npower and RF semiconductor devices must operate efficiently at higher power\nlevels and switching frequencies while remaining compact. Current\nstate-of-the-art GaN semiconductor devices alone cannot meet all these demands.\nEmerging ultra-wide band gap (UWBG) alternatives like diamond, BN, AlN, and\nGa2O3, face significant challenges including limited wafer availability, doping\ndifficulties, and thermal management constraints. Herein we conduct a\nhigh-throughput computational screening for new semiconductors for\nhigh-frequency electronics. In our analysis we compute the modeled Johnson and\nBaliga high-frequency figures of merit in combination with thermal conductivity\nto assess their potential for RF and power devices. We show that there are\nplenty of alternative materials to explore and conclude by discussing\ndopability and synthesis of select candidate materials. This study lays the\nfoundation for discovering new semiconductors that can push the boundaries of\nperformance in applications ranging from EV chargers and solid-state\ntransformers to sub-THz communications and advanced radar technologies.", "AI": {"tldr": "\u9ad8\u901a\u91cf\u8ba1\u7b97\u7b5b\u9009\u65b0\u534a\u5bfc\u4f53\u6750\u6599\u4ee5\u6ee1\u8db3\u672a\u6765\u7535\u5b50\u8bbe\u5907\u9700\u6c42\u3002", "motivation": "\u4e3a\u4e86\u6ee1\u8db3\u5148\u8fdb\u7535\u5b50\u7cfb\u7edf\u65e5\u76ca\u589e\u957f\u7684\u9700\u6c42\uff0c\u4e0b\u4e00\u4ee3\u7535\u529b\u548c\u5c04\u9891\u534a\u5bfc\u4f53\u5668\u4ef6\u9700\u8981\u5728\u4fdd\u6301\u7d27\u51d1\u7684\u540c\u65f6\uff0c\u5728\u9ad8\u529f\u7387\u548c\u5f00\u5173\u9891\u7387\u4e0b\u9ad8\u6548\u8fd0\u884c\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684GaN\u534a\u5bfc\u4f53\u5668\u4ef6\u65e0\u6cd5\u6ee1\u8db3\u6240\u6709\u8fd9\u4e9b\u9700\u6c42\uff0c\u800c\u65b0\u5174\u7684\u8d85\u5bbd\u5e26\u9699\uff08UWBG\uff09\u66ff\u4ee3\u6750\u6599\uff08\u5982\u91d1\u521a\u77f3\u3001\u6c2e\u5316\u787c\u3001\u6c2e\u5316\u94dd\u548c\u6c27\u5316\u9553\uff09\u4e5f\u9762\u4e34\u6676\u5706\u53ef\u7528\u6027\u3001\u63ba\u6742\u56f0\u96be\u548c\u70ed\u7ba1\u7406\u7b49\u6311\u6218\u3002", "method": "\u672c\u7814\u7a76\u901a\u8fc7\u8ba1\u7b97\u5efa\u6a21\uff0c\u7ed3\u5408\u4e86Johnson\u548cBaliga\u9ad8\u9891\u4f18\u503c\u4ee5\u53ca\u70ed\u5bfc\u7387\uff0c\u4ee5\u8bc4\u4f30\u5176\u5728\u5c04\u9891\u548c\u529f\u7387\u5668\u4ef6\u4e2d\u7684\u6f5c\u529b\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5b58\u5728\u5927\u91cf\u53ef\u4f9b\u63a2\u7d22\u7684\u66ff\u4ee3\u6750\u6599\uff0c\u5e76\u5c55\u793a\u4e86\u8fd9\u4e9b\u6750\u6599\u5728\u7535\u52a8\u6c7d\u8f66\u5145\u7535\u5668\u3001\u56fa\u6001\u53d8\u538b\u5668\u3001\u4e9a\u592a\u8d6b\u5179\u901a\u4fe1\u548c\u5148\u8fdb\u96f7\u8fbe\u6280\u672f\u7b49\u5e94\u7528\u4e2d\u63a8\u52a8\u6027\u80fd\u8fb9\u754c\u7684\u6f5c\u529b\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u9ad8\u901a\u91cf\u8ba1\u7b97\u7b5b\u9009\u4e86\u7528\u4e8e\u9ad8\u9891\u7535\u5b50\u5668\u4ef6\u7684\u65b0\u578b\u534a\u5bfc\u4f53\u6750\u6599\uff0c\u5e76\u8ba8\u8bba\u4e86\u90e8\u5206\u5019\u9009\u6750\u6599\u7684\u63ba\u6742\u548c\u5408\u6210\u95ee\u9898\uff0c\u4e3a\u63a8\u52a8\u7535\u5b50\u5668\u4ef6\u6027\u80fd\u8fb9\u754c\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.05641", "categories": ["physics.app-ph", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2508.05641", "abs": "https://arxiv.org/abs/2508.05641", "authors": ["Stephan Menzel", "Benedikt Kersting", "Rana Walied Ahmad", "Abu Sebastian", "Ghazi Sarwat Syed"], "title": "A device-level compact model for mushroom-type phase change memory", "comment": "15 pages, 6 figures", "summary": "In this work we introduce a compact model for mushroom-type phase-change\nmemory devices that incorporates the shape and size of the amorphous mark under\ndifferent programming conditions, and is applicable to both projecting and\nnon-projecting devices. The model includes analytical equations for the\namorphous and crystalline regions and uniquely features a current leakage path\nthat injects current at the outer edge of the electrodes. The results\ndemonstrate that accurately modeling the size and shape of the phase\nconfigurations is crucial for predicting the full-span of the RESET and SET\nprogramming, including the characteristics of threshold switching.\nAdditionally, the model effectively captures read-out behaviors, including the\ndependence of resistance drift and bipolar current asymmetry behaviours on the\nphase configurations. The compact model is also provided in Verilog-A format,\nso it can be easily used in standard circuit-level simulation tools.", "AI": {"tldr": "\u8be5\u6a21\u578b\u8003\u8651\u4e86PCM\u5668\u4ef6\u7684\u51e0\u4f55\u5f62\u72b6\uff0c\u5e76\u80fd\u51c6\u786e\u9884\u6d4b\u7f16\u7a0b\u7279\u6027\u548c\u8bfb\u51fa\u884c\u4e3a\u3002", "motivation": "\u5728\u6b64\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u4e3a\u8611\u83c7\u578b\u76f8\u53d8\u5b58\u50a8\u5668\u5668\u4ef6\u5f15\u5165\u4e86\u4e00\u4e2a\u7d27\u51d1\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5305\u542b\u4e86\u4e0d\u540c\u7f16\u7a0b\u6761\u4ef6\u4e0b\u7684\u975e\u6676\u6807\u8bb0\u7684\u5f62\u72b6\u548c\u5927\u5c0f\uff0c\u5e76\u9002\u7528\u4e8e\u6295\u5f71\u548c\u975e\u6295\u5f71\u5668\u4ef6\u3002", "method": "\u8be5\u6a21\u578b\u5305\u542b\u975e\u6676\u548c\u6676\u4f53\u533a\u57df\u7684\u89e3\u6790\u65b9\u7a0b\uff0c\u5e76\u72ec\u7279\u5730\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7535\u6d41\u6cc4\u6f0f\u8def\u5f84\uff0c\u8be5\u8def\u5f84\u5728\u7535\u6781\u5916\u8fb9\u7f18\u6ce8\u5165\u7535\u6d41\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u51c6\u786e\u6a21\u62df\u76f8\u914d\u7f6e\u7684\u5927\u5c0f\u548c\u5f62\u72b6\u5bf9\u4e8e\u9884\u6d4b RESET \u548c SET \u7f16\u7a0b\u7684\u5168\u8de8\u5ea6\u81f3\u5173\u91cd\u8981\uff0c\u5305\u62ec\u9608\u503c\u5207\u6362\u7684\u7279\u6027\u3002", "conclusion": "\u8be5\u6a21\u578b\u53ef\u6709\u6548\u6355\u83b7\u8bfb\u51fa\u884c\u4e3a\uff0c\u5305\u62ec\u7535\u963b\u6f02\u79fb\u548c\u53cc\u6781\u7535\u6d41\u4e0d\u5bf9\u79f0\u884c\u4e3a\u5bf9\u76f8\u4f4d\u914d\u7f6e\u7684\u4f9d\u8d56\u6027\u3002\u8be5\u7d27\u51d1\u6a21\u578b\u8fd8\u63d0\u4f9b Verilog-A \u683c\u5f0f\uff0c\u6613\u4e8e\u5728\u6807\u51c6\u7535\u8def\u7ea7\u4eff\u771f\u5de5\u5177\u4e2d\u4f7f\u7528\u3002"}}
{"id": "2508.06031", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2508.06031", "abs": "https://arxiv.org/abs/2508.06031", "authors": ["Licheng Ye", "Zehui Xiong", "Lin Gao", "Dusit Niyato"], "title": "An Overlapping Coalition Game Approach for Collaborative Block Mining and Edge Task Offloading in MEC-assisted Blockchain Networks", "comment": "This work has been accepted for publication in IEEE Transactions on\n  Mobile Computing", "summary": "Mobile edge computing (MEC) is a promising technology that enhances the\nefficiency of mobile blockchain networks, by enabling miners, often acted by\nmobile users (MUs) with limited computing resources, to offload\nresource-intensive mining tasks to nearby edge computing servers. Collaborative\nblock mining can further boost mining efficiency by allowing multiple miners to\nform coalitions, pooling their computing resources and transaction data\ntogether to mine new blocks collaboratively. Therefore, an MEC-assisted\ncollaborative blockchain network can leverage the strengths of both\ntechnologies, offering improved efficiency, security, and scalability for\nblockchain systems. While existing research in this area has mainly focused on\nthe single-coalition collaboration mode, where each miner can only join one\ncoalition, this work explores a more comprehensive multi-coalition\ncollaboration mode, which allows each miner to join multiple coalitions. To\nanalyze the behavior of miners and the edge computing service provider (ECP) in\nthis scenario, we propose a novel two-stage Stackelberg game. In Stage I, the\nECP, as the leader, determines the prices of computing resources for all MUs.\nIn Stage II, each MU decides the coalitions to join, resulting in an\noverlapping coalition formation (OCF) game; Subsequently, each coalition\ndecides how many edge computing resources to purchase from the ECP, leading to\nan edge resource competition (ERC) game. We derive the closed-form Nash\nequilibrium for the ERC game, based on which we further propose an OCF-based\nalternating algorithm to achieve a stable coalition structure for the OCF game\nand develop a near-optimal pricing strategy for the ECP's resource pricing\nproblem.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u4e00\u4e2a\u4e24\u9636\u6bb5Stackelberg\u535a\u5f08\uff0c\u5206\u6790\u4e86\u5728\u591a\u8054\u76df\u534f\u4f5c\u6a21\u5f0f\u4e0b\uff0cMEC\u5982\u4f55\u5e2e\u52a9\u533a\u5757\u94fe\u63d0\u9ad8\u6548\u7387\u3002\u7814\u7a76\u89e3\u51b3\u4e86\u8d44\u6e90\u5b9a\u4ef7\u548c\u8054\u76df\u5f62\u6210\u95ee\u9898\uff0c\u4ee5\u5b9e\u73b0\u66f4\u4f18\u7684\u533a\u5757\u94fe\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5355\u8054\u76df\u534f\u4f5c\u6a21\u5f0f\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u66f4\u590d\u6742\u7684\u3001\u5141\u8bb8\u77ff\u5de5\u52a0\u5165\u591a\u4e2a\u8054\u76df\u7684\u591a\u8054\u76df\u534f\u4f5c\u6a21\u5f0f\uff0c\u4ee5\u63d0\u9ad8MEC\u8f85\u52a9\u7684\u534f\u4f5c\u533a\u5757\u94fe\u7f51\u7edc\u7684\u6548\u7387\u3001\u5b89\u5168\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5Stackelberg\u535a\u5f08\uff0c\u5305\u62ecECP\u5b9a\u4ef7\u548cMU\u8054\u76df\u9009\u62e9\u3002\u901a\u8fc7\u6c42\u89e3ERC\u535a\u5f08\u7684\u7eb3\u4ec0\u5747\u8861\uff0c\u8fdb\u800c\u89e3\u51b3OCF\u535a\u5f08\u548cECP\u5b9a\u4ef7\u95ee\u9898\u3002", "result": "\u63a8\u5bfc\u4e86ERC\u535a\u5f08\u7684\u95ed\u5f0f\u7eb3\u4ec0\u5747\u8861\uff0c\u63d0\u51fa\u4e86\u4e00\u79cdOCF\u4ea4\u66ff\u7b97\u6cd5\u4ee5\u83b7\u5f97\u7a33\u5b9a\u7684\u8054\u76df\u7ed3\u6784\uff0c\u5e76\u4e3aECP\u5f00\u53d1\u4e86\u8fd1\u4f18\u7684\u8d44\u6e90\u5b9a\u4ef7\u7b56\u7565\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4e24\u9636\u6bb5Stackelberg\u535a\u5f08\uff0c\u7528\u4e8e\u5206\u6790\u591a\u8054\u76df\u534f\u4f5c\u6a21\u5f0f\u4e0b\u79fb\u52a8\u8fb9\u7f18\u8ba1\u7b97\uff08MEC\uff09\u8f85\u52a9\u7684\u534f\u4f5c\u533a\u5757\u94fe\u7f51\u7edc\u4e2d\u77ff\u5de5\u548c\u8fb9\u7f18\u8ba1\u7b97\u63d0\u4f9b\u5546\uff08ECP\uff09\u7684\u884c\u4e3a\u3002\u7814\u7a76\u63a8\u5bfc\u4e86\u8fb9\u7f18\u8d44\u6e90\u7ade\u4e89\uff08ERC\uff09\u535a\u5f08\u7684\u7eb3\u4ec0\u5747\u8861\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eOCF\u7684\u4ea4\u66ff\u7b97\u6cd5\u6765\u5b9e\u73b0\u7a33\u5b9a\u7684\u8054\u76df\u7ed3\u6784\uff0c\u4ee5\u53ca\u4e00\u79cd\u8fd1\u4e4e\u6700\u4f18\u7684\u5b9a\u4ef7\u7b56\u7565\u3002"}}
{"id": "2508.05702", "categories": ["cs.MA", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.05702", "abs": "https://arxiv.org/abs/2508.05702", "authors": ["Yan Zhang"], "title": "Semantic Reasoning Meets Numerical Precision: An LLM-Powered Multi-Agent System for Power Grid Control", "comment": null, "summary": "The increasing penetration of Distributed Energy Resources (DERs), widespread\nadoption of Electric Vehicles (EVs), and the growing frequency of extreme\nweather events have significantly increased the complexity of power grid\nplanning, operation, and management. Traditional rule-based systems and\nnumerical optimization approaches often struggle with the scale, dynamics, and\nadaptability required by modern power networks. This paper introduces\nGrid-Agent, an autonomous, AI-driven framework that combines Large Language\nModels (LLMs) with multi-agent reinforcement learning to detect and remediate\ngrid violations in real time. Grid-Agent integrates semantic reasoning with\nnumerical precision through a modular agent architecture: a planning agent\ngenerates coordinated action sequences using numerical power flow solvers,\nwhile a validation agent evaluates system stability and action effectiveness\nvia sandboxed execution with safety rollbacks. To ensure scalability,\nGrid-Agent incorporates an adaptive multiscale network representation that\ndynamically selects optimal encoding schemes based on network size and\ncomplexity. The framework enables coordinated violation resolution through\noptimizing switch configurations, battery deployment, and load curtailment\nstrategies. Experimental results in standard IEEE and CIGRE test systems (IEEE\n69-bus, CIGRE MV, and IEEE 30-bus) demonstrate superior violation mitigation\nperformance. Additionally, the framework's built-in data collection and\nlearning capabilities enable continuous learning and adaptation to diverse\nnetwork topologies. The autonomous nature of the framework makes it\nparticularly suitable for modern smart grid applications requiring rapid\nresponse to dynamic operating conditions.", "AI": {"tldr": "Grid-Agent \u662f\u4e00\u4e2a\u7ed3\u5408\u4e86 LLM \u548c\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7684 AI \u6846\u67b6\uff0c\u7528\u4e8e\u5b9e\u65f6\u68c0\u6d4b\u548c\u4fee\u590d\u7535\u7f51\u8fdd\u89c4\u3002\u5b83\u901a\u8fc7\u89c4\u5212\u548c\u9a8c\u8bc1\u667a\u80fd\u4f53\u5b9e\u73b0\u8bed\u4e49\u63a8\u7406\u548c\u6570\u503c\u7cbe\u5ea6\uff0c\u5e76\u91c7\u7528\u81ea\u9002\u5e94\u591a\u5c3a\u5ea6\u7f51\u7edc\u8868\u793a\u786e\u4fdd\u53ef\u6269\u5c55\u6027\u3002\u5b9e\u9a8c\u8bc1\u660e\u5176\u6027\u80fd\u4f18\u8d8a\uff0c\u9002\u7528\u4e8e\u667a\u80fd\u7535\u7f51\u3002", "motivation": "\u7531\u4e8e\u5206\u5e03\u5f0f\u80fd\u6e90 (DER)\u3001\u7535\u52a8\u6c7d\u8f66 (EV) \u548c\u6781\u7aef\u5929\u6c14\u4e8b\u4ef6\u7684\u65e5\u76ca\u666e\u53ca\uff0c\u7535\u529b\u7cfb\u7edf\u7684\u89c4\u5212\u3001\u8fd0\u884c\u548c\u7ba1\u7406\u53d8\u5f97\u65e5\u76ca\u590d\u6742\u3002\u4f20\u7edf\u7684\u57fa\u4e8e\u89c4\u5219\u7684\u7cfb\u7edf\u548c\u6570\u503c\u4f18\u5316\u65b9\u6cd5\u96be\u4ee5\u6ee1\u8db3\u73b0\u4ee3\u7535\u529b\u7f51\u7edc\u5bf9\u89c4\u6a21\u3001\u52a8\u6001\u548c\u9002\u5e94\u6027\u7684\u8981\u6c42\u3002", "method": "\u672c\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u4e2a\u540d\u4e3a Grid-Agent \u7684\u81ea\u4e3b AI \u9a71\u52a8\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u548c\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff0c\u53ef\u5b9e\u65f6\u68c0\u6d4b\u548c\u4fee\u590d\u7535\u7f51\u8fdd\u89c4\u3002Grid-Agent \u901a\u8fc7\u6a21\u5757\u5316\u667a\u80fd\u4f53\u67b6\u6784\u6574\u5408\u4e86\u8bed\u4e49\u63a8\u7406\u548c\u6570\u503c\u7cbe\u5ea6\uff1a\u4e00\u4e2a\u89c4\u5212\u667a\u80fd\u4f53\u4f7f\u7528\u6570\u503c\u6f6e\u6d41\u6c42\u89e3\u5668\u751f\u6210\u534f\u8c03\u7684\u52a8\u4f5c\u5e8f\u5217\uff0c\u800c\u4e00\u4e2a\u9a8c\u8bc1\u667a\u80fd\u4f53\u901a\u8fc7\u6c99\u76d2\u6267\u884c\u548c\u5b89\u5168\u56de\u6eda\u6765\u8bc4\u4f30\u7cfb\u7edf\u7a33\u5b9a\u6027\u548c\u52a8\u4f5c\u6709\u6548\u6027\u3002\u4e3a\u4e86\u786e\u4fdd\u53ef\u6269\u5c55\u6027\uff0cGrid-Agent \u91c7\u7528\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u591a\u5c3a\u5ea6\u7f51\u7edc\u8868\u793a\uff0c\u6839\u636e\u7f51\u7edc\u7684\u5927\u5c0f\u548c\u590d\u6742\u6027\u52a8\u6001\u9009\u62e9\u6700\u4f18\u7f16\u7801\u65b9\u6848\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cGrid-Agent \u6846\u67b6\u80fd\u591f\u901a\u8fc7\u4f18\u5316\u5f00\u5173\u914d\u7f6e\u3001\u7535\u6c60\u90e8\u7f72\u548c\u8d1f\u8377\u524a\u51cf\u7b56\u7565\uff0c\u5b9e\u73b0\u534f\u8c03\u7684\u8fdd\u89c4\u89e3\u51b3\u3002", "conclusion": "Grid-Agent \u6846\u67b6\u5728 IEEE 69 \u603b\u7ebf\u3001CIGRE \u53d8\u7535\u7ad9\u548c IEEE 30 \u603b\u7ebf\u7b49\u6807\u51c6 IEEE \u548c CIGRE \u6d4b\u8bd5\u7cfb\u7edf\u4e2d\u5c55\u793a\u4e86\u4f18\u8d8a\u7684\u8fdd\u89c4\u7f13\u89e3\u6027\u80fd\uff0c\u5e76\u5177\u5907\u5185\u7f6e\u7684\u6570\u636e\u6536\u96c6\u548c\u5b66\u4e60\u80fd\u529b\uff0c\u53ef\u5b9e\u73b0\u6301\u7eed\u5b66\u4e60\u548c\u9002\u5e94\u4e0d\u540c\u7f51\u7edc\u62d3\u6251\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9700\u8981\u5bf9\u52a8\u6001\u8fd0\u884c\u6761\u4ef6\u505a\u51fa\u5feb\u901f\u54cd\u5e94\u7684\u73b0\u4ee3\u667a\u80fd\u7535\u7f51\u5e94\u7528\u3002"}}
{"id": "2508.06316", "categories": ["cs.DS", "cs.CG", "cs.GR", "cs.IT", "cs.NA", "math.IT", "math.NA", "65D15, 65D18, 68P05, 68P30"], "pdf": "https://arxiv.org/pdf/2508.06316", "abs": "https://arxiv.org/abs/2508.06316", "authors": ["Theresa Pollinger", "Masado Ishii", "Jens Domke"], "title": "The Beauty of Anisotropic Mesh Refinement: Omnitrees for Efficient Dyadic Discretizations", "comment": "contains pdf animations; we recommend Okular or Firefox for viewing", "summary": "Structured adaptive mesh refinement (AMR), commonly implemented via quadtrees\nand octrees, underpins a wide range of applications including databases,\ncomputer graphics, physics simulations, and machine learning. However, octrees\nenforce isotropic refinement in regions of interest, which can be especially\ninefficient for problems that are intrinsically anisotropic--much resolution is\nspent where little information is gained. This paper presents omnitrees as an\nanisotropic generalization of octrees and related data structures. Omnitrees\nallow to refine only the locally most important dimensions, providing tree\nstructures that are less deep than bintrees and less wide than octrees. As a\nresult, the convergence of the AMR schemes can be increased by up to a factor\nof the dimensionality d for very anisotropic problems, quickly offsetting their\nmodest increase in storage overhead. We validate this finding on the problem of\nbinary shape representation across 4,166 three-dimensional objects: Omnitrees\nincrease the mean convergence rate by 1.5x, require less storage to achieve\nequivalent error bounds, and maximize the information density of the stored\nfunction faster than octrees. These advantages are projected to be even\nstronger for higher-dimensional problems. We provide a first validation by\nintroducing a time-dependent rotation to create four-dimensional\nrepresentations, and discuss the properties of their 4-d octree and omnitree\napproximations. Overall, omnitree discretizations can make existing AMR\napproaches more efficient, and open up new possibilities for high-dimensional\napplications.", "AI": {"tldr": "omnitrees\u662f\u4e00\u79cd\u65b0\u7684AMR\u6570\u636e\u7ed3\u6784\uff0c\u53ef\u4ee5\u63d0\u9ad8\u5728\u5404\u5411\u5f02\u6027\u95ee\u9898\u4e0a\u7684\u6548\u7387\uff0c\u5e76\u4e3a\u9ad8\u7ef4\u5e94\u7528\u63d0\u4f9b\u66f4\u597d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u516b\u53c9\u6811AMR\u65b9\u6cd5\u5728\u5904\u7406\u5185\u5728\u5404\u5411\u5f02\u6027\u95ee\u9898\u65f6\u6548\u7387\u4f4e\u4e0b\uff0c\u56e0\u4e3a\u5b83\u4eec\u5f3a\u5236\u8fdb\u884c\u5404\u5411\u540c\u6027\u7ec6\u5206\uff0c\u5bfc\u81f4\u5728\u4fe1\u606f\u589e\u76ca\u5f88\u5c0f\u7684\u533a\u57df\u6d6a\u8d39\u4e86\u5927\u91cf\u7684\u5206\u8fa8\u7387\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aomnitree\u7684\u5404\u5411\u5f02\u6027\u81ea\u9002\u5e94\u7f51\u683c\u7ec6\u5206\uff08AMR\uff09\u6570\u636e\u7ed3\u6784\uff0c\u4f5c\u4e3a\u516b\u53c9\u6811\u7684\u63a8\u5e7f\uff0c\u5141\u8bb8\u4ec5\u7ec6\u5316\u5c40\u90e8\u6700\u91cd\u8981\u7684\u7ef4\u5ea6\u3002", "result": "\u5728\u5904\u74063D\u5bf9\u8c61\u5f62\u72b6\u8868\u793a\u7684\u95ee\u9898\u4e0a\uff0comnitrees\u5c06\u5e73\u5747\u6536\u655b\u7387\u63d0\u9ad8\u4e861.5\u500d\uff0c\u5728\u8fbe\u5230\u540c\u7b49\u8bef\u5dee\u754c\u9650\u65f6\u9700\u8981\u66f4\u5c11\u7684\u5b58\u50a8\u7a7a\u95f4\uff0c\u5e76\u4e14\u6bd4\u516b\u53c9\u6811\u66f4\u5feb\u5730\u6700\u5927\u5316\u5b58\u50a8\u51fd\u6570\u7684\u4fe1\u606f\u5bc6\u5ea6\u3002\u5bf9\u4e8e\u66f4\u9ad8\u7ef4\u5ea6\u7684\u95ee\u9898\uff0c\u8fd9\u4e9b\u4f18\u52bf\u9884\u8ba1\u4f1a\u66f4\u52a0\u660e\u663e\u3002", "conclusion": "Omnitree\u79bb\u6563\u5316\u53ef\u4ee5\u63d0\u9ad8\u73b0\u6709AMR\u65b9\u6cd5\u7684\u6548\u7387\uff0c\u5e76\u4e3a\u9ad8\u7ef4\u5e94\u7528\u5f00\u8f9f\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2508.05959", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.05959", "abs": "https://arxiv.org/abs/2508.05959", "authors": ["Amirhossein Taherpour", "Somayeh Khani", "Abbas Taherpour", "Tamer Khattab"], "title": "IRS-Assisted IoT Activity Detection Under Asynchronous Transmission and Heterogeneous Powers: Detectors and Performance Analysis", "comment": null, "summary": "This paper addresses the problem of activity detection in distributed\nInternet of Things (IoT) networks, where devices employ asynchronous\ntransmissions with heterogeneous power levels to report their local\nobservations. The system leverages an intelligent reflecting surface (IRS) to\nenhance detection reliability, with optional incorporation of a direct\nline-of-sight (LoS) path. We formulate the detection problem as a binary\nhypothesis test and develop four detectors: an optimal detector alongside three\ncomputationally efficient detectors designed for practical scenarios with\ndifferent levels of prior knowledge about noise variance, channel state\ninformation, and device transmit powers. For each detector, we derive\nclosed-form expressions for both detection and false alarm probabilities,\nestablishing theoretical performance benchmarks. Extensive simulations validate\nour analytical results and systematically evaluate the impact of key system\nparameters including the number of antennas, samples, users, and IRS elements\non detection performance. The proposed framework effectively bridges\ntheoretical optimality with implementation practicality, providing a scalable\nsolution for IRS-assisted IoT networks in emerging 6G systems.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5206\u5e03\u5f0f\u7269\u8054\u7f51\u7f51\u7edc\u7684\u6d3b\u52a8\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5229\u7528\u667a\u80fd\u53cd\u5c04\u9762 (IRS) \u63d0\u9ad8\u68c0\u6d4b\u53ef\u9760\u6027\u3002\u901a\u8fc7\u5236\u5b9a\u4e8c\u5143\u5047\u8bbe\u68c0\u9a8c\u95ee\u9898\u5e76\u5f00\u53d1\u8ba1\u7b97\u6548\u7387\u9ad8\u7684\u68c0\u6d4b\u5668\uff0c\u63a8\u5bfc\u4e86\u68c0\u6d4b\u548c\u865a\u8b66\u6982\u7387\u7684\u7406\u8bba\u6027\u80fd\u57fa\u51c6\u3002\u4eff\u771f\u7ed3\u679c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u8bc4\u4f30\u4e86\u5173\u952e\u7cfb\u7edf\u53c2\u6570\u7684\u5f71\u54cd\uff0c\u4e3a 6G \u7cfb\u7edf\u4e2d\u7684 IRS \u8f85\u52a9\u7269\u8054\u7f51\u7f51\u7edc\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u89e3\u51b3\u5206\u5e03\u5f0f\u7269\u8054\u7f51\u7f51\u7edc\u4e2d\u7684\u6d3b\u52a8\u68c0\u6d4b\u95ee\u9898\uff0c\u5176\u4e2d\u8bbe\u5907\u91c7\u7528\u5f02\u6b65\u4f20\u8f93\u548c\u5f02\u6784\u529f\u7387\u7ea7\u522b\u6765\u62a5\u544a\u5176\u672c\u5730\u89c2\u6d4b\u7ed3\u679c\u3002\u8be5\u7cfb\u7edf\u5229\u7528\u667a\u80fd\u53cd\u5c04\u9762 (IRS) \u6765\u63d0\u9ad8\u68c0\u6d4b\u53ef\u9760\u6027\uff0c\u5e76\u53ef\u9009\u62e9\u6027\u5730\u7ed3\u5408\u89c6\u8ddd (LoS) \u8def\u5f84\u3002", "method": "\u5c06\u68c0\u6d4b\u95ee\u9898\u5236\u5b9a\u4e3a\u4e8c\u5143\u5047\u8bbe\u68c0\u9a8c\uff0c\u5e76\u5f00\u53d1\u4e86\u56db\u79cd\u68c0\u6d4b\u5668\uff1a\u4e00\u79cd\u6700\u4f18\u68c0\u6d4b\u5668\u548c\u4e09\u79cd\u8ba1\u7b97\u6548\u7387\u9ad8\u7684\u68c0\u6d4b\u5668\uff0c\u9002\u7528\u4e8e\u5177\u6709\u4e0d\u540c\u566a\u58f0\u65b9\u5dee\u3001\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\u548c\u8bbe\u5907\u53d1\u5c04\u529f\u7387\u5148\u9a8c\u77e5\u8bc6\u6c34\u5e73\u7684\u5b9e\u9645\u573a\u666f\u3002", "result": "\u4e3a\u6bcf\u79cd\u68c0\u6d4b\u5668\u63a8\u5bfc\u4e86\u68c0\u6d4b\u548c\u865a\u8b66\u6982\u7387\u7684\u5c01\u95ed\u5f0f\u8868\u8fbe\u5f0f\uff0c\u5efa\u7acb\u4e86\u7406\u8bba\u6027\u80fd\u57fa\u51c6\u3002\u5e7f\u6cdb\u7684\u6a21\u62df\u9a8c\u8bc1\u4e86\u6211\u4eec\u7684\u5206\u6790\u7ed3\u679c\uff0c\u5e76\u7cfb\u7edf\u5730\u8bc4\u4f30\u4e86\u5929\u7ebf\u6570\u91cf\u3001\u6837\u672c\u6570\u91cf\u3001\u7528\u6237\u6570\u91cf\u548c IRS \u5355\u5143\u6570\u91cf\u7b49\u5173\u952e\u7cfb\u7edf\u53c2\u6570\u5bf9\u68c0\u6d4b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u5730\u5c06\u7406\u8bba\u6700\u4f18\u6027\u4e0e\u5b9e\u9645\u53ef\u884c\u6027\u76f8\u7ed3\u5408\uff0c\u4e3a\u65b0\u5174 6G \u7cfb\u7edf\u4e2d\u7684 IRS \u8f85\u52a9\u7269\u8054\u7f51\u7f51\u7edc\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.05904", "categories": ["cs.DC", "cs.DB"], "pdf": "https://arxiv.org/pdf/2508.05904", "abs": "https://arxiv.org/abs/2508.05904", "authors": ["Brandon Baker", "Elliott Brossard", "Chenwei Xie", "Zihao Ye", "Deen Liu", "Yijun Xie", "Arthur Zwiegincew", "Nitya Kumar Sharma", "Gaurav Jain", "Eugene Retunsky", "Mike Halcrow", "Derek Denny-Brown", "Istvan Cseri", "Tyler Akidau", "Yuxiong He"], "title": "Snowpark: Performant, Secure, User-Friendly Data Engineering and AI/ML Next To Your Data", "comment": "12 pages, 6 figures, accepted in ICDCS 2025", "summary": "Snowflake revolutionized data analytics with an elastic architecture that\ndecouples compute and storage, enabling scalable solutions supporting data\narchitectures like data lake, data warehouse, data lakehouse, and data mesh.\nBuilding on this foundation, Snowflake has advanced its AI Data Cloud vision by\nintroducing Snowpark, a managed turnkey solution that supports data engineering\nand AI and ML workloads using Python and other programming languages.\n  This paper outlines Snowpark's design objectives towards high performance,\nstrong security and governance, and ease of use. We detail the architecture of\nSnowpark, highlighting its elastic scalability and seamless integration with\nSnowflake core compute infrastructure. This includes leveraging Snowflake\ncontrol plane for distributed computing and employing a secure sandbox for\nisolating Snowflake SQL workloads from Snowpark executions. Additionally, we\npresent core innovations in Snowpark that drive further performance\nenhancements, such as query initialization latency reduction through Python\npackage caching, improved workload scheduling for customized workloads, and\ndata skew management via efficient row redistribution. Finally, we showcase\nreal-world case studies that illustrate Snowpark's efficiency and effectiveness\nfor large-scale data engineering and AI and ML tasks.", "AI": {"tldr": "Snowflake\u901a\u8fc7Snowpark\u5b9e\u73b0AI\u6570\u636e\u4e91\uff0c\u652f\u6301Python\u7b49\u8bed\u8a00\u8fdb\u884c\u6570\u636e\u5de5\u7a0b\u548cAI/ML\uff0c\u5e76\u4f18\u5316\u4e86\u6027\u80fd\u3001\u5b89\u5168\u548c\u6613\u7528\u6027\u3002", "motivation": "Snowflake\u65e8\u5728\u901a\u8fc7Snowpark\u6269\u5c55\u5176AI\u6570\u636e\u4e91\u80fd\u529b\uff0c\u4ee5\u652f\u6301\u6570\u636e\u5de5\u7a0b\u548cAI/ML\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u5e76\u63d0\u4f9b\u9ad8\u6027\u80fd\u3001\u5f3a\u5b89\u5168\u548c\u6613\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u672c\u6587\u8be6\u7ec6\u4ecb\u7ecd\u4e86Snowpark\u7684\u67b6\u6784\u8bbe\u8ba1\uff0c\u5305\u62ec\u5229\u7528Snowflake\u63a7\u5236\u5e73\u9762\u5b9e\u73b0\u5206\u5e03\u5f0f\u8ba1\u7b97\uff0c\u901a\u8fc7\u5b89\u5168\u6c99\u7bb1\u9694\u79bbSQL\u548cSnowpark\u6267\u884c\uff0c\u4ee5\u53ca\u901a\u8fc7Python\u5305\u7f13\u5b58\u3001\u5de5\u4f5c\u8d1f\u8f7d\u8c03\u5ea6\u4f18\u5316\u548c\u6570\u636e\u503e\u659c\u7ba1\u7406\u7b49\u6280\u672f\u6765\u63d0\u5347\u6027\u80fd\u3002", "result": "Snowpark\u901a\u8fc7\u4e00\u7cfb\u5217\u521b\u65b0\u8bbe\u8ba1\u548c\u4f18\u5316\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u3001\u5b89\u5168\u548c\u6613\u7528\u7684\u6570\u636e\u5904\u7406\u548cAI/ML\u5de5\u4f5c\u8d1f\u8f7d\u652f\u6301\uff0c\u5e76\u5728\u5b9e\u9645\u6848\u4f8b\u4e2d\u5f97\u5230\u4e86\u9a8c\u8bc1\u3002", "conclusion": "Snowpark\u901a\u8fc7\u5176\u5f39\u6027\u53ef\u6269\u5c55\u67b6\u6784\u3001\u5f3a\u5927\u7684\u5b89\u5168\u548c\u6cbb\u7406\u529f\u80fd\u4ee5\u53ca\u6613\u7528\u6027\uff0c\u6709\u6548\u5730\u652f\u6301\u4e86\u5927\u89c4\u6a21\u6570\u636e\u5de5\u7a0b\u548c\u4eba\u5de5\u667a\u80fd/\u673a\u5668\u5b66\u4e60\u4efb\u52a1\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728AI\u6570\u636e\u4e91\u4e2d\u7684\u9886\u5148\u5730\u4f4d\u3002"}}
{"id": "2508.06027", "categories": ["cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2508.06027", "abs": "https://arxiv.org/abs/2508.06027", "authors": ["Ken Uchino", "Yuuki Ogawa", "Satoru Hayami"], "title": "Analysis of Spin Current Generation by Elastic Waves in $f$-wave Altermagnets", "comment": "8 pages, 7 figures", "summary": "We theoretically investigate the mechanism of spin current generation induced\nby elastic waves in nonrelativistic magnets referred to as altermagnets. By\nanalyzing an $f$-wave altermagnet formed by a three-sublattice noncollinear\nantiferromagnetic structure breaking the spatial inversion symmetry on a\ntwo-dimensional triangular lattice within the linear response theory, we show\nthat the nonrelativistic antisymmetric spin-split band structure can give rise\nto spin current generation when either longitudinal or transverse elastic wave\nis applied. We find that the momentum dependence of the antisymmetric spin\nsplitting leads to a characteristic direction-dependent spin current response.\nWe also compare the present nonrelativistic magnetic-order-driven mechanism\nwith the relativistic one in a nonmagnetic Rashba system. These findings\nhighlight the potential of invesion-symmetry-breaking altermagnets as a spin\ncurrent generator driven by elasticity without relying on the relativistic\nspin-orbit coupling.", "AI": {"tldr": "\u8be5\u7814\u7a76\u53d1\u73b0\uff0c\u53cd\u6f14\u5bf9\u79f0\u6027\u7834\u574f\u7684\u4ea4\u66ff\u78c1\u4f53\u53ef\u4ee5\u4f5c\u4e3a\u5f39\u6027\u9a71\u52a8\u7684\u81ea\u65cb\u6d41\u53d1\u751f\u5668\uff0c\u800c\u65e0\u9700\u4f9d\u8d56\u76f8\u5bf9\u8bba\u81ea\u65cb\u8f68\u9053\u8026\u5408\u3002", "motivation": "\u7406\u8bba\u4e0a\u7814\u7a76\u4e86\u5728\u975e\u76f8\u5bf9\u8bba\u78c1\u4f53\uff08\u5373\u4ea4\u66ff\u78c1\u4f53\uff09\u4e2d\u7531\u5f39\u6027\u6ce2\u5f15\u8d77\u7684\u81ea\u65cb\u6d41\u751f\u6210\u673a\u5236\u3002", "method": "\u901a\u8fc7\u7ebf\u6027\u54cd\u5e94\u7406\u8bba\uff0c\u5206\u6790\u4e86\u4e00\u4e2a\u5728\u4e8c\u7ef4\u4e09\u89d2\u6676\u683c\u4e0a\u7834\u574f\u7a7a\u95f4\u53cd\u6f14\u5bf9\u79f0\u6027\u7684\u4e09\u5b50\u6676\u683c\u975e\u5171\u7ebf\u53cd\u94c1\u78c1\u7ed3\u6784\u5f62\u6210\u7684 f \u7535\u5b50\u6ce2\u4ea4\u66ff\u78c1\u4f53\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u53cd\u5bf9\u79f0\u81ea\u65cb\u5288\u88c2\u7684\u52a8\u91cf\u4f9d\u8d56\u6027\u4f1a\u5bfc\u81f4\u5177\u6709\u7279\u5f81\u6027\u7684\u4f9d\u8d56\u4e8e\u65b9\u5411\u7684\u81ea\u65cb\u6d41\u54cd\u5e94\u3002\u7814\u7a76\u8fd8\u5c06\u5f53\u524d\u7684\u975e\u76f8\u5bf9\u8bba\u78c1\u5e8f\u9a71\u52a8\u673a\u5236\u4e0e\u975e\u78c1\u6027 Rashba \u7cfb\u7edf\u4e2d\u7684\u76f8\u5bf9\u8bba\u673a\u5236\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "conclusion": "\u8be5\u7814\u7a76\u7406\u8bba\u4e0a\u7814\u7a76\u4e86\u5728\u975e\u76f8\u5bf9\u8bba\u78c1\u4f53\uff08\u5373\u4ea4\u66ff\u78c1\u4f53\uff09\u4e2d\u7531\u5f39\u6027\u6ce2\u5f15\u8d77\u7684\u81ea\u65cb\u6d41\u751f\u6210\u673a\u5236\u3002\u7814\u7a76\u5206\u6790\u4e86\u4e00\u4e2a\u5728\u4e8c\u7ef4\u4e09\u89d2\u6676\u683c\u4e0a\u7834\u574f\u7a7a\u95f4\u53cd\u6f14\u5bf9\u79f0\u6027\u7684\u4e09\u5b50\u6676\u683c\u975e\u5171\u7ebf\u53cd\u94c1\u78c1\u7ed3\u6784\u5f62\u6210\u7684 f \u7535\u5b50\u6ce2\u4ea4\u66ff\u78c1\u4f53\uff0c\u5e76\u5728\u7ebf\u6027\u54cd\u5e94\u7406\u8bba\u4e0b\u8fdb\u884c\u4e86\u5206\u6790\uff0c\u7ed3\u679c\u8868\u660e\u975e\u76f8\u5bf9\u8bba\u53cd\u5bf9\u79f0\u81ea\u65cb\u5288\u88c2\u80fd\u5e26\u7ed3\u6784\u5728\u65bd\u52a0\u7eb5\u5411\u6216\u6a2a\u5411\u5f39\u6027\u6ce2\u65f6\u4f1a\u4ea7\u751f\u81ea\u65cb\u6d41\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u53cd\u5bf9\u79f0\u81ea\u65cb\u5288\u88c2\u7684\u52a8\u91cf\u4f9d\u8d56\u6027\u4f1a\u5bfc\u81f4\u5177\u6709\u7279\u5f81\u6027\u7684\u4f9d\u8d56\u4e8e\u65b9\u5411\u7684\u81ea\u65cb\u6d41\u54cd\u5e94\u3002\u7814\u7a76\u8fd8\u5c06\u5f53\u524d\u7684\u975e\u76f8\u5bf9\u8bba\u78c1\u5e8f\u9a71\u52a8\u673a\u5236\u4e0e\u975e\u78c1\u6027 Rashba \u7cfb\u7edf\u4e2d\u7684\u76f8\u5bf9\u8bba\u673a\u5236\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002\u8fd9\u4e9b\u53d1\u73b0\u5f3a\u8c03\u4e86\u53cd\u6f14\u5bf9\u79f0\u6027\u7834\u574f\u7684\u4ea4\u66ff\u78c1\u4f53\u4f5c\u4e3a\u5f39\u6027\u9a71\u52a8\u7684\u81ea\u65cb\u6d41\u53d1\u751f\u5668\u7684\u6f5c\u529b\uff0c\u800c\u65e0\u9700\u4f9d\u8d56\u76f8\u5bf9\u8bba\u81ea\u65cb\u8f68\u9053\u8026\u5408\u3002"}}
{"id": "2508.05712", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.05712", "abs": "https://arxiv.org/abs/2508.05712", "authors": ["Bin Luo", "Yuwen Huang", "Jonathan Allcock", "Xiaojun Lin", "Shengyu Zhang", "John C. S. Lui"], "title": "Quantum Algorithms for Finite-horizon Markov Decision Processes", "comment": "Accepted in 42nd International Conference on Machine Learning (ICML\n  2025)", "summary": "In this work, we design quantum algorithms that are more efficient than\nclassical algorithms to solve time-dependent and finite-horizon Markov Decision\nProcesses (MDPs) in two distinct settings: (1) In the exact dynamics setting,\nwhere the agent has full knowledge of the environment's dynamics (i.e.,\ntransition probabilities), we prove that our $\\textbf{Quantum Value Iteration\n(QVI)}$ algorithm $\\textbf{QVI-1}$ achieves a quadratic speedup in the size of\nthe action space $(A)$ compared with the classical value iteration algorithm\nfor computing the optimal policy ($\\pi^{*}$) and the optimal V-value function\n($V_{0}^{*}$). Furthermore, our algorithm $\\textbf{QVI-2}$ provides an\nadditional speedup in the size of the state space $(S)$ when obtaining\nnear-optimal policies and V-value functions. Both $\\textbf{QVI-1}$ and\n$\\textbf{QVI-2}$ achieve quantum query complexities that provably improve upon\nclassical lower bounds, particularly in their dependences on $S$ and $A$. (2)\nIn the generative model setting, where samples from the environment are\naccessible in quantum superposition, we prove that our algorithms\n$\\textbf{QVI-3}$ and $\\textbf{QVI-4}$ achieve improvements in sample complexity\nover the state-of-the-art (SOTA) classical algorithm in terms of $A$,\nestimation error $(\\epsilon)$, and time horizon $(H)$. More importantly, we\nprove quantum lower bounds to show that $\\textbf{QVI-3}$ and $\\textbf{QVI-4}$\nare asymptotically optimal, up to logarithmic factors, assuming a constant time\nhorizon.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.05782", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.05782", "abs": "https://arxiv.org/abs/2508.05782", "authors": ["Xiangyan Chen", "Yufeng Li", "Yujian Gan", "Arkaitz Zubiaga", "Matthew Purver"], "title": "FineDialFact: A benchmark for Fine-grained Dialogue Fact Verification", "comment": null, "summary": "Large Language Models (LLMs) are known to produce hallucinations - factually\nincorrect or fabricated information - which poses significant challenges for\nmany Natural Language Processing (NLP) applications, such as dialogue systems.\nAs a result, detecting hallucinations has become a critical area of research.\nCurrent approaches to hallucination detection in dialogue systems primarily\nfocus on verifying the factual consistency of generated responses. However,\nthese responses often contain a mix of accurate, inaccurate or unverifiable\nfacts, making one factual label overly simplistic and coarse-grained. In this\npaper, we introduce a benchmark, FineDialFact, for fine-grained dialogue fact\nverification, which involves verifying atomic facts extracted from dialogue\nresponses. To support this, we construct a dataset based on publicly available\ndialogue datasets and evaluate it using various baseline methods. Experimental\nresults demonstrate that methods incorporating Chain-of-Thought (CoT) reasoning\ncan enhance performance in dialogue fact verification. Despite this, the best\nF1-score achieved on the HybriDialogue, an open-domain dialogue dataset, is\nonly 0.75, indicating that the benchmark remains a challenging task for future\nresearch. Our dataset and code will be public on GitHub.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86FineDialFact\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u7ec6\u7c92\u5ea6\u7684\u5bf9\u8bdd\u4e8b\u5b9e\u9a8c\u8bc1\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u539f\u5b50\u4e8b\u5b9e\u7684\u6570\u636e\u96c6\u3002\u5b9e\u9a8c\u8868\u660eCoT\u63a8\u7406\u80fd\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u4ecd\u6709\u6311\u6218\u6027\u3002", "motivation": "LLM\uff08\u5927\u8bed\u8a00\u6a21\u578b\uff09\u4f1a\u4ea7\u751f\u5e7b\u89c9\uff08\u5373\u4e8b\u5b9e\u4e0d\u51c6\u786e\u6216\u865a\u6784\u7684\u4fe1\u606f\uff09\uff0c\u8fd9\u5bf9\u5bf9\u8bdd\u7cfb\u7edf\u7b49NLP\u5e94\u7528\u6784\u6210\u4e86\u91cd\u5927\u6311\u6218\u3002\u56e0\u6b64\uff0c\u68c0\u6d4b\u5e7b\u89c9\u5df2\u6210\u4e3a\u4e00\u4e2a\u5173\u952e\u7684\u7814\u7a76\u9886\u57df\u3002\u5f53\u524d\u5bf9\u8bdd\u7cfb\u7edf\u4e2d\u7684\u5e7b\u89c9\u68c0\u6d4b\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u4e8b\u5b9e\u4e00\u81f4\u6027\u9a8c\u8bc1\uff0c\u4f46\u8fd9\u4e9b\u54cd\u5e94\u901a\u5e38\u5305\u542b\u51c6\u786e\u3001\u4e0d\u51c6\u786e\u6216\u65e0\u6cd5\u9a8c\u8bc1\u7684\u4e8b\u5b9e\u7684\u6df7\u5408\uff0c\u4f7f\u5f97\u5355\u4e00\u7684\u4e8b\u5b9e\u6807\u7b7e\u8fc7\u4e8e\u7b80\u5316\u548c\u7c97\u7c92\u5ea6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFineDialFact\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u7ec6\u7c92\u5ea6\u7684\u5bf9\u8bdd\u4e8b\u5b9e\u9a8c\u8bc1\uff0c\u8be5\u9a8c\u8bc1\u6d89\u53ca\u4ece\u5bf9\u8bdd\u54cd\u5e94\u4e2d\u63d0\u53d6\u7684\u4e8b\u5b9e\u539f\u5b50\u8fdb\u884c\u9a8c\u8bc1\u3002\u6784\u5efa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u516c\u5f00\u5bf9\u8bdd\u6570\u636e\u96c6\u7684\u6570\u636e\u96c6\uff0c\u5e76\u4f7f\u7528\u5404\u79cd\u57fa\u7ebf\u65b9\u6cd5\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5305\u542b\u94fe\u5f0f\u601d\u8003\uff08CoT\uff09\u63a8\u7406\u7684\u65b9\u6cd5\u53ef\u4ee5\u63d0\u9ad8\u5bf9\u8bdd\u4e8b\u5b9e\u9a8c\u8bc1\u7684\u6027\u80fd\u3002", "conclusion": "\u5c3d\u7ba1\u91c7\u7528\u4e86\u94fe\u5f0f\u601d\u8003\uff08CoT\uff09\u63a8\u7406\u7b49\u65b9\u6cd5\uff0c\u5728HybriDialogue\uff08\u4e00\u4e2a\u5f00\u653e\u57df\u5bf9\u8bdd\u6570\u636e\u96c6\uff09\u4e0a\u7684\u6700\u4f73F1\u5206\u6570\u4ec5\u4e3a0.75\uff0c\u8fd9\u8868\u660e\u8be5\u57fa\u51c6\u6d4b\u8bd5\u5bf9\u4e8e\u672a\u6765\u7684\u7814\u7a76\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002"}}
{"id": "2508.05895", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.05895", "abs": "https://arxiv.org/abs/2508.05895", "authors": ["Jiaqi Hu", "Karl H. Johansson", "Apostolos I. Rikos"], "title": "Distributed Quantized Average Consensus in Open Multi-Agent Systems with Dynamic Communication Links", "comment": null, "summary": "In this paper, we focus on the distributed quantized average consensus\nproblem in open multi-agent systems consisting of communication links that\nchange dynamically over time. Open multi-agent systems exhibiting the\naforementioned characteristic are referred to as \\textit{open dynamic\nmulti-agent systems} in this work. We present a distributed algorithm that\nenables active nodes in the open dynamic multi-agent system to calculate the\nquantized average of their initial states. Our algorithm consists of the\nfollowing advantages: (i) ensures efficient communication by enabling nodes to\nexchange quantized valued messages, and (ii) exhibits finite time convergence\nto the desired solution. We establish the correctness of our algorithm and we\npresent necessary and sufficient topological conditions for it to successfully\nsolve the quantized average consensus problem in an open dynamic multi-agent\nsystem. Finally, we illustrate the performance of our algorithm with numerical\nsimulations.", "AI": {"tldr": "This paper proposes a distributed algorithm for average consensus in open multi-agent systems with changing links. The algorithm uses quantized messages for efficiency and converges in finite time, with correctness proven and conditions for success identified.", "motivation": "The paper addresses the distributed quantized average consensus problem in open multi-agent systems with dynamically changing communication links, termed open dynamic multi-agent systems.", "method": "A distributed algorithm is proposed that allows active nodes to compute the quantized average of their initial states by exchanging quantized value messages. The algorithm exhibits finite time convergence.", "result": "The algorithm ensures efficient communication through quantized message exchange and achieves finite time convergence. Its performance is validated through numerical simulations.", "conclusion": "The paper presents a distributed algorithm for the quantized average consensus problem in open dynamic multi-agent systems, establishing its correctness and identifying necessary and sufficient topological conditions for successful problem solving."}}
{"id": "2508.05755", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05755", "abs": "https://arxiv.org/abs/2508.05755", "authors": ["Agnieszka Polowczyk", "Alicja Polowczyk", "Dawid Malarz", "Artur Kasymov", "Marcin Mazur", "Jacek Tabor", "Przemys\u0142aw Spurek"], "title": "UnGuide: Learning to Forget with LoRA-Guided Diffusion Models", "comment": null, "summary": "Recent advances in large-scale text-to-image diffusion models have heightened\nconcerns about their potential misuse, especially in generating harmful or\nmisleading content. This underscores the urgent need for effective machine\nunlearning, i.e., removing specific knowledge or concepts from pretrained\nmodels without compromising overall performance. One possible approach is\nLow-Rank Adaptation (LoRA), which offers an efficient means to fine-tune models\nfor targeted unlearning. However, LoRA often inadvertently alters unrelated\ncontent, leading to diminished image fidelity and realism. To address this\nlimitation, we introduce UnGuide -- a novel approach which incorporates\nUnGuidance, a dynamic inference mechanism that leverages Classifier-Free\nGuidance (CFG) to exert precise control over the unlearning process. UnGuide\nmodulates the guidance scale based on the stability of a few first steps of\ndenoising processes, enabling selective unlearning by LoRA adapter. For prompts\ncontaining the erased concept, the LoRA module predominates and is\ncounterbalanced by the base model; for unrelated prompts, the base model\ngoverns generation, preserving content fidelity. Empirical results demonstrate\nthat UnGuide achieves controlled concept removal and retains the expressive\npower of diffusion models, outperforming existing LoRA-based methods in both\nobject erasure and explicit content removal tasks.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51faUnGuide\uff0c\u4e00\u79cd\u5229\u7528\u52a8\u6001CFG\u5f15\u5bfc\u6765\u7cbe\u786e\u63a7\u5236LoRA\u6a21\u578b\u53cd\u5b66\u4e60\u7684\u65b0\u65b9\u6cd5\uff0c\u80fd\u5728\u79fb\u9664\u7279\u5b9a\u6982\u5ff5\u7684\u540c\u65f6\u4fdd\u6301\u56fe\u50cf\u7684\u4fdd\u771f\u5ea6\u548c\u6a21\u578b\u7684\u6574\u4f53\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u5927\u578b\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7684\u8fdb\u6b65\uff0c\u4eba\u4eec\u5bf9\u5b83\u4eec\u53ef\u80fd\u88ab\u6ee5\u7528\uff08\u4f8b\u5982\u751f\u6210\u6709\u5bb3\u6216\u8bef\u5bfc\u6027\u5185\u5bb9\uff09\u7684\u62c5\u5fe7\u65e5\u76ca\u589e\u52a0\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u6709\u6548\u7684\u673a\u5668\u53cd\u5b66\u4e60\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u4e0d\u5f71\u54cd\u6a21\u578b\u6574\u4f53\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\uff0c\u79fb\u9664\u6a21\u578b\u4e2d\u7279\u5b9a\u7684\u77e5\u8bc6\u6216\u6982\u5ff5\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aUnGuide\u7684\u65b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5f15\u5165\u4e86\u4e00\u79cd\u52a8\u6001\u63a8\u7406\u673a\u5236\u2014\u2014UnGuidance\uff0c\u5229\u7528Classifier-Free Guidance\uff08CFG\uff09\u6765\u7cbe\u786e\u63a7\u5236\u673a\u5668\u53cd\u5b66\u4e60\u8fc7\u7a0b\u3002UnGuide\u901a\u8fc7\u8c03\u8282\u5f15\u5bfc\u5c3a\u5ea6\u6765\u5e73\u8861LoRA\u6a21\u5757\u548c\u57fa\u7840\u6a21\u578b\u7684\u4f5c\u7528\uff0c\u4ee5\u5b9e\u73b0\u9009\u62e9\u6027\u53cd\u5b66\u4e60\uff0c\u5e76\u4fdd\u6301\u5185\u5bb9\u7684\u4fdd\u771f\u5ea6\u548c\u771f\u5b9e\u611f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cUnGuide\u5728\u63a7\u5236\u6982\u5ff5\u64e6\u9664\u65b9\u9762\u53d6\u5f97\u4e86\u6210\u529f\uff0c\u5e76\u80fd\u4fdd\u7559\u6269\u6563\u6a21\u578b\u7684\u8868\u8fbe\u80fd\u529b\u3002\u5728\u7269\u4f53\u64e6\u9664\u548c\u663e\u5f0f\u5185\u5bb9\u79fb\u9664\u4efb\u52a1\u4e2d\uff0c\u5176\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u4e8eLoRA\u7684\u65b9\u6cd5\u3002", "conclusion": "UnGuide\u901a\u8fc7\u52a8\u6001\u8c03\u6574CFG\u7684\u5f15\u5bfc\u5c3a\u5ea6\uff0c\u5b9e\u73b0\u4e86\u53ef\u63a7\u7684\u6982\u5ff5\u64e6\u9664\uff0c\u5e76\u4e14\u5728\u4fdd\u6301\u6a21\u578b\u751f\u6210\u80fd\u529b\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u4e8eLoRA\u7684\u65b9\u6cd5\uff0c\u5728\u64e6\u9664\u7269\u4f53\u548c\u663e\u5f0f\u5185\u5bb9\u65b9\u9762\u5747\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2508.05936", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.05936", "abs": "https://arxiv.org/abs/2508.05936", "authors": ["Haohui Pan", "Takuya Kiyokawa", "Tomoki Ishikura", "Shingo Hamada", "Genichiro Matsuda", "Kensuke Harada"], "title": "Modular Vacuum-Based Fixturing System for Adaptive Disassembly Workspace Integration", "comment": "8 pages, 9 figures", "summary": "The disassembly of small household appliances poses significant challenges\ndue to their complex and curved geometries, which render traditional rigid\nfixtures inadequate. In this paper, we propose a modular vacuum-based fixturing\nsystem that leverages commercially available balloon-type soft grippers to\nconform to arbitrarily shaped surfaces and provide stable support during\nscrew-removal tasks. To enable a reliable deployment of the system, we develop\na stability-aware planning framework that samples the bottom surface of the\ntarget object, filters candidate contact points based on geometric continuity,\nand evaluates support configurations using convex hull-based static stability\ncriteria. We compare the quality of object placement under different numbers\nand configurations of balloon hands. In addition, real-world experiments were\nconducted to compare the success rates of traditional rigid fixtures with our\nproposed system. The results demonstrate that our method consistently achieves\nhigher success rates and superior placement stability during screw removal\ntasks.", "AI": {"tldr": "A new soft gripper system using vacuum and balloons provides better stability and success for dismantling complex appliances compared to old rigid systems.", "motivation": "Traditional rigid fixtures are inadequate for disassembling small household appliances due to their complex and curved geometries. The motivation is to develop a more adaptable and stable fixturing system for such tasks.", "method": "A modular vacuum-based fixturing system utilizing commercially available balloon-type soft grippers was developed. A stability-aware planning framework was designed, involving sampling the object's bottom surface, filtering contact points based on geometric continuity, and evaluating support configurations using convex hull-based static stability criteria.", "result": "The system demonstrates consistent higher success rates and superior placement stability during screw removal tasks compared to traditional rigid fixtures. Comparisons were made based on the number and configuration of soft grippers.", "conclusion": "The proposed modular vacuum-based fixturing system using soft grippers achieves higher success rates and superior placement stability for disassembling small household appliances compared to traditional rigid fixtures."}}
{"id": "2508.05776", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05776", "abs": "https://arxiv.org/abs/2508.05776", "authors": ["Thomas L. Griffiths", "Brenden M. Lake", "R. Thomas McCoy", "Ellie Pavlick", "Taylor W. Webb"], "title": "Whither symbols in the era of advanced neural networks?", "comment": null, "summary": "Some of the strongest evidence that human minds should be thought about in\nterms of symbolic systems has been the way they combine ideas, produce novelty,\nand learn quickly. We argue that modern neural networks -- and the artificial\nintelligence systems built upon them -- exhibit similar abilities. This\nundermines the argument that the cognitive processes and representations used\nby human minds are symbolic, although the fact that these neural networks are\ntypically trained on data generated by symbolic systems illustrates that such\nsystems play an important role in characterizing the abstract problems that\nhuman minds have to solve. This argument leads us to offer a new agenda for\nresearch on the symbolic basis of human thought.", "AI": {"tldr": "\u795e\u7ecf\u7f51\u7edc\u5c55\u73b0\u51fa\u7c7b\u4f3c\u4eba\u7c7b\u7684\u667a\u80fd\uff0c\u6311\u6218\u4e86\u5fc3\u667a\u7b26\u53f7\u5316\u7684\u4f20\u7edf\u89c2\u70b9\uff0c\u4f46\u7b26\u53f7\u7cfb\u7edf\u5728\u5b9a\u4e49\u95ee\u9898\u4e0a\u4ecd\u5f88\u91cd\u8981\uff0c\u4fc3\u4f7f\u7814\u7a76\u65b0\u65b9\u5411\u3002", "motivation": "\u63a2\u8ba8\u73b0\u4ee3\u795e\u7ecf\u7f51\u7edc\u7684\u80fd\u529b\u662f\u5426\u80fd\u63d0\u4f9b\u4eba\u7c7b\u5fc3\u667a\u672c\u8d28\u7684\u65b0\u89c6\u89d2\uff0c\u5e76\u8d28\u7591\u7eaf\u7cb9\u7b26\u53f7\u5316\u5fc3\u667a\u6a21\u578b\u7684\u5145\u5206\u6027\u3002", "method": "\u901a\u8fc7\u6bd4\u8f83\u73b0\u4ee3\u795e\u7ecf\u7f51\u7edc\u7684\u80fd\u529b\u4e0e\u4eba\u7c7b\u5fc3\u667a\u7684\u7279\u70b9\uff08\u7ec4\u5408\u3001\u521b\u65b0\u3001\u5feb\u901f\u5b66\u4e60\uff09\uff0c\u8bba\u8bc1\u4e86\u4eba\u7c7b\u5fc3\u667a\u8fc7\u7a0b\u548c\u8868\u5f81\u4e0d\u4e00\u5b9a\u662f\u7b26\u53f7\u5316\u7684\u3002", "result": "\u73b0\u4ee3\u795e\u7ecf\u7f51\u7edc\u7684\u80fd\u529b\u6311\u6218\u4e86\u4eba\u7c7b\u5fc3\u667a\u5b8c\u5168\u662f\u7b26\u53f7\u5316\u7cfb\u7edf\u7684\u89c2\u70b9\uff0c\u4f46\u540c\u65f6\u5f3a\u8c03\u4e86\u7b26\u53f7\u7cfb\u7edf\u5728\u5b9a\u4e49\u4eba\u7c7b\u5fc3\u667a\u9762\u4e34\u7684\u62bd\u8c61\u95ee\u9898\u4e2d\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u73b0\u4ee3\u795e\u7ecf\u7f51\u7edc\uff08\u53ca\u5176\u4e4b\u4e0a\u6784\u5efa\u7684\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\uff09\u5c55\u73b0\u51fa\u4e86\u4e0e\u4eba\u7c7b\u5fc3\u667a\u76f8\u4f3c\u7684\u7ec4\u5408\u3001\u521b\u65b0\u548c\u5feb\u901f\u5b66\u4e60\u80fd\u529b\uff0c\u8fd9\u524a\u5f31\u4e86\u8ba4\u4e3a\u4eba\u7c7b\u5fc3\u667a\u8fc7\u7a0b\u548c\u8868\u5f81\u672c\u8d28\u4e0a\u662f\u7b26\u53f7\u5316\u7684\u8bba\u70b9\u3002\u5c3d\u7ba1\u5982\u6b64\uff0c\u8fd9\u4e9b\u795e\u7ecf\u7f51\u7edc\u7684\u8bad\u7ec3\u4f9d\u8d56\u4e8e\u7b26\u53f7\u7cfb\u7edf\u751f\u6210\u7684\u6570\u636e\uff0c\u8fd9\u8868\u660e\u7b26\u53f7\u7cfb\u7edf\u5728\u63cf\u8ff0\u4eba\u7c7b\u5fc3\u667a\u9700\u8981\u89e3\u51b3\u7684\u62bd\u8c61\u95ee\u9898\u65b9\u9762\u4ecd\u7136\u626e\u6f14\u7740\u91cd\u8981\u89d2\u8272\u3002\u57fa\u4e8e\u8fd9\u4e9b\u89c2\u5bdf\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u5173\u4e8e\u4eba\u7c7b\u601d\u7ef4\u7b26\u53f7\u57fa\u7840\u7814\u7a76\u7684\u65b0\u8bae\u7a0b\u3002"}}
{"id": "2508.06062", "categories": ["cs.AI", "cs.LG", "cs.LO", "68T27, 68T30"], "pdf": "https://arxiv.org/pdf/2508.06062", "abs": "https://arxiv.org/abs/2508.06062", "authors": ["Evgenii E. Vityaev", "Andrei Mantsivoda"], "title": "Don't Forget Imagination!", "comment": "14 pages, 2 figures", "summary": "Cognitive imagination is a type of imagination that plays a key role in human\nthinking. It is not a ``picture-in-the-head'' imagination. It is a faculty to\nmentally visualize coherent and holistic systems of concepts and causal links\nthat serve as semantic contexts for reasoning, decision making and prediction.\nOur position is that the role of cognitive imagination is still greatly\nunderestimated, and this creates numerous problems and diminishes the current\ncapabilities of AI. For instance, when reasoning, humans rely on imaginary\ncontexts to retrieve background info. They also constantly return to the\ncontext for semantic verification that their reasoning is still reasonable.\nThus, reasoning without imagination is blind. This paper is a call for greater\nattention to cognitive imagination as the next promising breakthrough in\nartificial intelligence. As an instrument for simulating cognitive imagination,\nwe propose semantic models -- a new approach to mathematical models that can\nlearn, like neural networks, and are based on probabilistic causal\nrelationships. Semantic models can simulate cognitive imagination because they\nensure the consistency of imaginary contexts and implement a glass-box approach\nthat allows the context to be manipulated as a holistic and coherent system of\ninterrelated facts glued together with causal relations.", "AI": {"tldr": "\u8ba4\u77e5\u60f3\u8c61\u5bf9\u4eba\u7c7b\u601d\u7ef4\u81f3\u5173\u91cd\u8981\uff0c\u4e5f\u662f\u4eba\u5de5\u667a\u80fd\u7684\u4e0b\u4e00\u4e2a\u6f5c\u5728\u7a81\u7834\u70b9\u3002\u672c\u6587\u63d0\u51fa\u8bed\u4e49\u6a21\u578b\u6765\u6a21\u62df\u8ba4\u77e5\u60f3\u8c61\uff0c\u4ee5\u89e3\u51b3\u5f53\u524d\u4eba\u5de5\u667a\u80fd\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u76ee\u524d\u4eba\u5de5\u667a\u80fd\u7684\u63a8\u7406\u80fd\u529b\u88ab\u4e25\u91cd\u4f4e\u4f30\uff0c\u56e0\u4e3a\u5b83\u7f3a\u4e4f\u8ba4\u77e5\u60f3\u8c61\u80fd\u529b\uff0c\u800c\u8ba4\u77e5\u60f3\u8c61\u5728\u4eba\u7c7b\u601d\u8003\u4e2d\u8d77\u7740\u5173\u952e\u4f5c\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u8bed\u4e49\u6a21\u578b\u201d\u7684\u65b0\u578b\u6570\u5b66\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u80fd\u591f\u5b66\u4e60\uff0c\u5e76\u57fa\u4e8e\u6982\u7387\u56e0\u679c\u5173\u7cfb\uff0c\u4ee5\u6a21\u62df\u8ba4\u77e5\u60f3\u8c61\u3002", "result": "\u8bed\u4e49\u6a21\u578b\u80fd\u591f\u786e\u4fdd\u6a21\u62df\u51fa\u7684\u60f3\u8c61\u60c5\u5883\u7684\u4e00\u81f4\u6027\uff0c\u5e76\u63d0\u4f9b\u4e00\u79cd\u201c\u73bb\u7483\u76d2\u5b50\u201d\u65b9\u6cd5\uff0c\u5141\u8bb8\u5c06\u60c5\u5883\u4f5c\u4e3a\u4e00\u4e2a\u76f8\u4e92\u5173\u8054\u7684\u4e8b\u5b9e\u6574\u4f53\u8fdb\u884c\u64cd\u7eb5\uff0c\u5e76\u901a\u8fc7\u56e0\u679c\u5173\u7cfb\u8fdb\u884c\u8fde\u63a5\u3002", "conclusion": "\u4eba\u5de5\u667a\u80fd\u7684\u7a81\u7834\u6027\u8fdb\u5c55\u53ef\u80fd\u5728\u4e8e\u6a21\u62df\u8ba4\u77e5\u60f3\u8c61\u80fd\u529b\uff0c\u800c\u8bed\u4e49\u6a21\u578b\u53ef\u4ee5\u5b9e\u73b0\u8fd9\u4e00\u70b9\u3002"}}
{"id": "2508.06243", "categories": ["cs.LG", "cs.NE", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.06243", "abs": "https://arxiv.org/abs/2508.06243", "authors": ["Ioan-Sorin Comsa", "Purav Shah", "Karthik Vaidhyanathan", "Deepak Gangadharan", "Christof Imhof", "Per Bergamin", "Aryan Kaushik", "Gabriel-Miro Muntean", "Ramona Trestian"], "title": "SCAR: State-Space Compression for AI-Driven Resource Management in 6G-Enabled Vehicular Infotainment Systems", "comment": null, "summary": "The advent of 6G networks opens new possibilities for connected infotainment\nservices in vehicular environments. However, traditional Radio Resource\nManagement (RRM) techniques struggle with the increasing volume and complexity\nof data such as Channel Quality Indicators (CQI) from autonomous vehicles. To\naddress this, we propose SCAR (State-Space Compression for AI-Driven Resource\nManagement), an Edge AI-assisted framework that optimizes scheduling and\nfairness in vehicular infotainment. SCAR employs ML-based compression\ntechniques (e.g., clustering and RBF networks) to reduce CQI data size while\npreserving essential features. These compressed states are used to train\n6G-enabled Reinforcement Learning policies that maximize throughput while\nmeeting fairness objectives defined by the NGMN. Simulations show that SCAR\nincreases time in feasible scheduling regions by 14\\% and reduces unfair\nscheduling time by 15\\% compared to RL baselines without CQI compression.\nFurthermore, Simulated Annealing with Stochastic Tunneling (SAST)-based\nclustering reduces CQI clustering distortion by 10\\%, confirming its\nefficiency. These results demonstrate SCAR's scalability and fairness benefits\nfor dynamic vehicular networks.", "AI": {"tldr": "SCAR\u6846\u67b6\u901a\u8fc7AI\u9a71\u52a8\u7684\u8d44\u6e90\u7ba1\u7406\uff0c\u5229\u7528\u538b\u7f29\u6280\u672f\u4f18\u53166G\u8f66\u8054\u7f51\u4e2d\u7684\u8c03\u5ea6\u548c\u516c\u5e73\u6027\uff0c\u63d0\u9ad8\u4e86\u7f51\u7edc\u6027\u80fd\u5e76\u51cf\u5c11\u4e86\u6570\u636e\u5904\u7406\u5f00\u9500\u3002", "motivation": "\u4f20\u7edf\u7684\u65e0\u7ebf\u8d44\u6e90\u7ba1\u7406\uff08RRM\uff09\u6280\u672f\u96be\u4ee5\u5e94\u5bf9\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u4ea7\u751f\u7684\u65e5\u76ca\u589e\u957f\u548c\u590d\u6742\u7684\u6570\u636e\u91cf\uff08\u5982CQI\uff09\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u4f18\u53166G\u8f66\u8054\u7f51\u73af\u5883\u4e0b\u7684\u8d44\u6e90\u7ba1\u7406\u3002", "method": "SCAR\u6846\u67b6\u7ed3\u5408\u4e86\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u538b\u7f29\u6280\u672f\uff08\u5982\u805a\u7c7b\u548cRBF\u7f51\u7edc\uff09\u6765\u51cf\u5c0f\u4fe1\u9053\u8d28\u91cf\u6307\u793a\uff08CQI\uff09\u6570\u636e\u7684\u5927\u5c0f\uff0c\u5e76\u5229\u75286G\u7f51\u7edc\u548c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7b56\u7565\u6765\u4f18\u5316\u8c03\u5ea6\u548c\u516c\u5e73\u6027\u3002\u540c\u65f6\uff0c\u5b83\u91c7\u7528\u4e86\u6a21\u62df\u9000\u706b\u968f\u673a\u96a7\u9053\uff08SAST\uff09\u7684\u805a\u7c7b\u65b9\u6cd5\u6765\u4f18\u5316\u538b\u7f29\u8fc7\u7a0b\u3002", "result": "SCAR\u6846\u67b6\u5c06\u53ef\u884c\u8c03\u5ea6\u533a\u57df\u5185\u7684\u8fd0\u884c\u65f6\u95f4\u63d0\u9ad8\u4e8614%\uff0c\u5e76\u5c06\u4e0d\u516c\u5e73\u8c03\u5ea6\u65f6\u95f4\u51cf\u5c11\u4e8615%\u3002\u6b64\u5916\uff0c\u57fa\u4e8eSAST\u7684\u805a\u7c7b\u65b9\u6cd5\u5c06CQI\u805a\u7c7b\u5931\u771f\u964d\u4f4e\u4e8610%\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "SCAR\u6846\u67b6\u901a\u8fc7\u5176\u5728\u52a8\u6001\u8f66\u8054\u7f51\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u548c\u516c\u5e73\u6027\u4f18\u52bf\uff0c\u4e3a6G\u7f51\u7edc\u4e0b\u7684\u8f66\u8054\u7f51\u4fe1\u606f\u5a31\u4e50\u670d\u52a1\u5e26\u6765\u4e86\u663e\u8457\u7684\u6539\u8fdb\u3002"}}
{"id": "2508.06344", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2508.06344", "abs": "https://arxiv.org/abs/2508.06344", "authors": ["Robin Sehm", "Christian Ewert", "Rainer Buchty", "Mladen Berekovic", "Saleh Mulhem"], "title": "Nail: Not Another Fault-Injection Framework for Chisel-generated RTL", "comment": "PREPRINT - accepted In Proceedings of the 28th Euromicro Conference\n  Series on Digital System Design (DSD)", "summary": "Fault simulation and emulation are essential techniques for evaluating the\ndependability of integrated circuits, enabling early-stage vulnerability\nanalysis and supporting the implementation of effective mitigation strategies.\nHigh-level hardware description languages such as Chisel facilitate the rapid\ndevelopment of complex fault scenarios with minimal modification to the design.\nHowever, existing Chisel-based fault injection (FI) frameworks are limited by\ncoarse-grained, instruction-level controllability, restricting the precision of\nfault modeling. This work introduces Nail, a Chisel-based open-source FI\nframework that overcomes these limitations by introducing state-based faults.\nThis approach enables fault scenarios that depend on specific system states,\nrather than solely on instruction-level triggers, thereby removing the need for\nprecise timing of fault activation. For greater controllability, Nail allows\nusers to arbitrarily modify internal trigger states via software at runtime. To\nsupport this, Nail automatically generates a software interface, offering\nstraightforward access to the instrumented design. This enables fine-tuning of\nfault parameters during active FI campaigns - a feature particularly beneficial\nfor FPGA emulation, where synthesis is time-consuming. Utilizing these\nfeatures, Nail narrows the gap between the high speed of emulation-based FI\nframeworks, the usability of software-based approaches, and the controllability\nachieved in simulation. We demonstrate Nail's state-based FI and software\nframework by modeling a faulty general-purpose register in a RISC-V processor.\nAlthough this might appear straightforward, it requires state-dependent FI and\nwas previously impossible without fundamental changes to the design. The\napproach was validated in both simulation and FPGA emulation, where the\naddition of Nail introduced less than 1% resource overhead.", "AI": {"tldr": "Nail\u662f\u4e00\u4e2a\u65b0\u7684Chisel\u6545\u969c\u6ce8\u5165\u6846\u67b6\uff0c\u901a\u8fc7\u57fa\u4e8e\u72b6\u6001\u7684\u6ce8\u5165\u548c\u8fd0\u884c\u65f6\u8f6f\u4ef6\u63a7\u5236\uff0c\u5b9e\u73b0\u4e86\u66f4\u7cbe\u786e\u3001\u66f4\u7075\u6d3b\u7684\u6545\u969c\u5efa\u6a21\uff0c\u5e76\u964d\u4f4e\u4e86FPGA\u5b9e\u73b0\u7684\u8d44\u6e90\u5f00\u9500\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8eChisel\u7684\u6545\u969c\u6ce8\u5165\uff08FI\uff09\u6846\u67b6\u5728\u63a7\u5236\u7c92\u5ea6\u4e0a\u53d7\u9650\u4e8e\u6307\u4ee4\u7ea7\u522b\uff0c\u8fd9\u9650\u5236\u4e86\u6545\u969c\u5efa\u6a21\u7684\u7cbe\u786e\u6027\u3002\u4e3a\u4e86\u66f4\u6709\u6548\u5730\u8bc4\u4f30\u96c6\u6210\u7535\u8def\u7684\u53ef\u9760\u6027\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u7cbe\u7ec6\u3001\u66f4\u7075\u6d3b\u7684\u6545\u969c\u6ce8\u5165\u65b9\u6cd5\u3002", "method": "Nail\u662f\u4e00\u4e2a\u57fa\u4e8eChisel\u7684\u5f00\u6e90\u6545\u969c\u6ce8\u5165\uff08FI\uff09\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u5f15\u5165\u57fa\u4e8e\u72b6\u6001\u7684\u6545\u969c\uff08state-based faults\uff09\u6765\u514b\u670d\u73b0\u6709\u6846\u67b6\u7684\u5c40\u9650\u6027\u3002\u8fd9\u79cd\u65b9\u6cd5\u5141\u8bb8\u6545\u969c\u573a\u666f\u4f9d\u8d56\u4e8e\u7279\u5b9a\u7684\u7cfb\u7edf\u72b6\u6001\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u6307\u4ee4\u7ea7\u522b\u7684\u89e6\u53d1\uff0c\u4ece\u800c\u65e0\u9700\u7cbe\u786e\u63a7\u5236\u6545\u969c\u6fc0\u6d3b\u7684\u65f6\u95f4\u3002Nail\u8fd8\u5141\u8bb8\u7528\u6237\u901a\u8fc7\u8f6f\u4ef6\u5728\u8fd0\u884c\u65f6\u4efb\u610f\u4fee\u6539\u5185\u90e8\u89e6\u53d1\u72b6\u6001\uff0c\u5e76\u81ea\u52a8\u751f\u6210\u8f6f\u4ef6\u63a5\u53e3\uff0c\u65b9\u4fbf\u5730\u8bbf\u95ee\u88ab\u63d2\u6869\u7684\u8bbe\u8ba1\u3002", "result": "Nail\u6846\u67b6\u80fd\u591f\u5b9e\u73b0\u72b6\u6001\u4f9d\u8d56\u7684\u6545\u969c\u6ce8\u5165\uff0c\u5e76\u4e14\u53ef\u4ee5\u901a\u8fc7\u8f6f\u4ef6\u5728\u8fd0\u884c\u65f6\u4fee\u6539\u5185\u90e8\u89e6\u53d1\u72b6\u6001\uff0c\u4ece\u800c\u80fd\u591f\u5bf9\u6545\u969c\u53c2\u6570\u8fdb\u884c\u5fae\u8c03\u3002\u5728RISC-V\u5904\u7406\u5668\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u662f\u53ef\u884c\u7684\uff0c\u5e76\u4e14\u5728FPGA\u5b9e\u73b0\u4e0a\u5f15\u5165\u7684\u8d44\u6e90\u5f00\u9500\u5c0f\u4e8e1%\u3002", "conclusion": "Nail\u6846\u67b6\u901a\u8fc7\u5f15\u5165\u57fa\u4e8e\u72b6\u6001\u7684\u6545\u969c\u6ce8\u5165\uff0c\u5b9e\u73b0\u4e86\u6bd4\u73b0\u6709Chisel\u6846\u67b6\u66f4\u7cbe\u7ec6\u3001\u66f4\u53ef\u63a7\u7684\u6545\u969c\u5efa\u6a21\uff0c\u5e76\u4e14\u80fd\u591f\u901a\u8fc7\u8f6f\u4ef6\u5728\u8fd0\u884c\u65f6\u4fee\u6539\u5185\u90e8\u89e6\u53d1\u72b6\u6001\uff0c\u4ece\u800c\u5728\u4eff\u771f\u901f\u5ea6\u3001\u6613\u7528\u6027\u548c\u53ef\u63a7\u6027\u4e4b\u95f4\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u5e73\u8861\u3002\u8be5\u6846\u67b6\u5728RISC-V\u5904\u7406\u5668\u4e0a\u8fdb\u884c\u4e86\u6f14\u793a\u548c\u9a8c\u8bc1\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u5728FPGA\u5b9e\u73b0\u4e0a\u7684\u4f4e\u8d44\u6e90\u5f00\u9500\u3002"}}
{"id": "2508.05899", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2508.05899", "abs": "https://arxiv.org/abs/2508.05899", "authors": ["Zixuan Bian", "Ruohan Ren", "Yue Yang", "Chris Callison-Burch"], "title": "HOLODECK 2.0: Vision-Language-Guided 3D World Generation with Editing", "comment": null, "summary": "3D scene generation plays a crucial role in gaming, artistic creation,\nvirtual reality and many other domains. However, current 3D scene design still\nrelies heavily on extensive manual effort from creators, and existing automated\nmethods struggle to generate open-domain scenes or support flexible editing. As\na result, generating 3D worlds directly from text has garnered increasing\nattention. In this paper, we introduce HOLODECK 2.0, an advanced\nvision-language-guided framework for 3D world generation with support for\ninteractive scene editing based on human feedback. HOLODECK 2.0 can generate\ndiverse and stylistically rich 3D scenes (e.g., realistic, cartoon, anime, and\ncyberpunk styles) that exhibit high semantic fidelity to fine-grained input\ndescriptions, suitable for both indoor and open-domain environments. HOLODECK\n2.0 leverages vision-language models (VLMs) to identify and parse the objects\nrequired in a scene and generates corresponding high-quality assets via\nstate-of-the-art 3D generative models. It then iteratively applies spatial\nconstraints derived from the VLMs to achieve semantically coherent and\nphysically plausible layouts. Human evaluations and CLIP-based assessments\ndemonstrate that HOLODECK 2.0 effectively generates high-quality scenes closely\naligned with detailed textual descriptions, consistently outperforming\nbaselines across indoor and open-domain scenarios. Additionally, we provide\nediting capabilities that flexibly adapt to human feedback, supporting layout\nrefinement and style-consistent object edits. Finally, we present a practical\napplication of HOLODECK 2.0 in procedural game modeling, generating visually\nrich and immersive environments, potentially boosting efficiency.", "AI": {"tldr": "HOLODECK 2.0 \u662f\u4e00\u4e2a\u5148\u8fdb\u7684\u89c6\u89c9-\u8bed\u8a00\u5f15\u5bfc\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u62103D\u4e16\u754c\u5e76\u652f\u6301\u4ea4\u4e92\u5f0f\u573a\u666f\u7f16\u8f91\u3002\u5b83\u80fd\u6839\u636e\u6587\u672c\u63cf\u8ff0\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u98ce\u683c\u591a\u6837\u4e14\u8bed\u4e49\u7cbe\u786e\u76843D\u573a\u666f\uff0c\u5e76\u80fd\u6839\u636e\u7528\u6237\u53cd\u9988\u8fdb\u884c\u7075\u6d3b\u7f16\u8f91\uff0c\u5728\u6e38\u620f\u5efa\u6a21\u7b49\u9886\u57df\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\u3002", "motivation": "\u5f53\u524d\u76843D\u573a\u666f\u8bbe\u8ba1\u4e3b\u8981\u4f9d\u8d56\u624b\u52a8\u521b\u4f5c\uff0c\u4e14\u81ea\u52a8\u5316\u65b9\u6cd5\u5728\u751f\u6210\u5f00\u653e\u57df\u573a\u666f\u548c\u652f\u6301\u7075\u6d3b\u7f16\u8f91\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002\u56e0\u6b64\uff0c\u76f4\u63a5\u4ece\u6587\u672c\u751f\u62103D\u4e16\u754c\u53d7\u5230\u4e86\u8d8a\u6765\u8d8a\u591a\u7684\u5173\u6ce8\u3002", "method": "HOLODECK 2.0 \u6846\u67b6\u5229\u7528\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u89e3\u6790\u6587\u672c\u63cf\u8ff0\u4ee5\u8bc6\u522b\u573a\u666f\u5bf9\u8c61\uff0c\u5e76\u901a\u8fc7\u6700\u5148\u8fdb\u76843D\u751f\u6210\u6a21\u578b\u751f\u6210\u9ad8\u8d28\u91cf\u8d44\u4ea7\u3002\u968f\u540e\uff0c\u5229\u7528\u4eceVLMs\u83b7\u5f97\u7684\u89c6\u7a7a\u95f4\u7ea6\u675f\uff0c\u8fed\u4ee3\u5730\u751f\u6210\u8bed\u4e49\u4e00\u81f4\u4e14\u7269\u7406\u4e0a\u5408\u7406\u7684\u5e03\u5c40\u3002\u6b64\u5916\uff0c\u8be5\u6846\u67b6\u8fd8\u652f\u6301\u57fa\u4e8e\u7528\u6237\u53cd\u9988\u7684\u4ea4\u4e92\u5f0f\u573a\u666f\u7f16\u8f91\uff0c\u5305\u62ec\u5e03\u5c40\u4f18\u5316\u548c\u98ce\u683c\u4e00\u81f4\u7684\u5bf9\u8c61\u7f16\u8f91\u3002", "result": "HOLODECK 2.0 \u80fd\u591f\u751f\u6210\u591a\u6837\u5316\u3001\u98ce\u683c\u4e30\u5bcc\u76843D\u573a\u666f\uff08\u5982\u5199\u5b9e\u3001\u5361\u901a\u3001\u52a8\u6f2b\u3001\u8d5b\u535a\u670b\u514b\u98ce\u683c\uff09\uff0c\u5728\u8bed\u4e49\u4fdd\u771f\u5ea6\u4e0a\u4e0e\u7cbe\u7ec6\u7684\u6587\u672c\u63cf\u8ff0\u9ad8\u5ea6\u4e00\u81f4\uff0c\u9002\u7528\u4e8e\u5ba4\u5185\u548c\u5f00\u653e\u57df\u73af\u5883\u3002\u901a\u8fc7\u4eba\u5de5\u8bc4\u4f30\u548c\u57fa\u4e8eCLIP\u7684\u8bc4\u4f30\uff0c\u8bc1\u660e\u4e86HOLODECK 2.0 \u5728\u751f\u6210\u9ad8\u8d28\u91cf\u573a\u666f\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "HOLODECK 2.0 \u5728\u6e38\u620f\u5efa\u6a21\u7b49\u9886\u57df\u5177\u6709\u5b9e\u9645\u5e94\u7528\u524d\u666f\uff0c\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u591a\u6837\u5316\u4e14\u7b26\u5408\u6587\u672c\u63cf\u8ff0\u76843D\u573a\u666f\uff0c\u5e76\u652f\u6301\u4ea4\u4e92\u5f0f\u7f16\u8f91\uff0c\u5728\u5ba4\u5185\u548c\u5f00\u653e\u57df\u573a\u666f\u4e2d\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002"}}
{"id": "2508.05874", "categories": ["cond-mat.mtrl-sci", "physics.optics"], "pdf": "https://arxiv.org/pdf/2508.05874", "abs": "https://arxiv.org/abs/2508.05874", "authors": ["Hossein Esfandiar", "Fatemeh Abtahi", "Trevor G. Vrckovnik", "G. Quyet Ngo", "Rene Heller", "Ulrich Kentsch", "Fabian Ganss", "Stefan Facsko", "Uta Lucchesi", "Stephan Winnerl", "Falk Eilenberger", "Dennis Arslan", "Sebastian W. Schmitt"], "title": "Structural and Optical Properties of Crystal Ion Sliced BaTiO$_3$ Thin Films", "comment": null, "summary": "Barium titanate (BaTiO$_3$) is a compelling material for integrated photonics\ndue to its strong electro-optic and second-order nonlinear properties. Crystal\nIon Slicing (CIS) presents a scalable and CMOS-compatible route for fabricating\nthin BaTiO$_3$ films; however, ion implantation during CIS introduces lattice\ndamage that can degrade structural and optical performance. In this study, we\ndemonstrate that post-slicing thermal annealing effectively restores the\nstructural integrity and optical quality of CIS-processed BaTiO$_3$ flakes.\nRaman spectroscopy confirms the recovery of crystallinity, while\nsecond-harmonic generation (SHG) microscopy reveals systematic reorientation of\nferroelectric domains and restoration of the associated second-order nonlinear\nsusceptibility tensor $X^{(2)}$. Notably, SHG signals persist even in regions\nwith weak Raman signatures, indicating that long-range ferroelectric order can\nsurvive despite partial lattice disruption. Optical measurements show that the\nlinear dispersion of annealed CIS flakes closely matches that of bulk\nBaTiO$_3$, validating their suitability for photonic integration. Together,\nthese results qualify CIS - combined with thermal annealing - as a viable and\nscalable manufacturing strategy for high-quality BaTiO$_3$-on-insulator (BTOI)\nplatforms, enabling advanced integrated photonic devices for modulation,\nfrequency conversion, and quantum optics.", "AI": {"tldr": "Thermal annealing effectively repairs lattice damage in Barium Titanate films processed with Crystal Ion Slicing, restoring their structural and optical properties for integrated photonics applications.", "motivation": "Barium titanate (BaTiO3) is a compelling material for integrated photonics due to its strong electro-optic and second-order nonlinear properties, but ion implantation during CIS processing introduces lattice damage that can degrade performance.", "method": "The study demonstrates that post-slicing thermal annealing effectively restores the structural integrity and optical quality of CIS-processed BaTiO3 flakes, using Raman spectroscopy and SHG microscopy to confirm the recovery of crystallinity and reorientation of ferroelectric domains, and optical measurements to validate the linear dispersion.", "result": "Post-slicing thermal annealing restores structural integrity and optical quality of CIS-processed BaTiO3 flakes. Raman spectroscopy confirms crystallinity recovery, SHG microscopy reveals ferroelectric domain reorientation and restoration of second-order nonlinear susceptibility, and optical measurements show linear dispersion matching bulk BaTiO3.", "conclusion": "The study qualifies CIS combined with thermal annealing as a viable and scalable manufacturing strategy for high-quality BTOI platforms, enabling advanced integrated photonic devices."}}
{"id": "2508.06050", "categories": ["physics.app-ph", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2508.06050", "abs": "https://arxiv.org/abs/2508.06050", "authors": ["Nguyen Thi My Duc", "Hariharan Srikanth", "Manh-Huong Phan"], "title": "Low-dimensional magnetocaloric materials for energy-efficient magnetic refrigeration: Does size matter?", "comment": null, "summary": "The magnetocaloric effect (MCE) provides a promising foundation for the\ndevelopment of solid-state refrigeration technologies that could replace\nconventional gas compression-based cooling systems. Current research efforts\nprimarily focus on identifying cost-effective magnetic materials that exhibit\nlarge MCEs under low magnetic fields across broad temperature ranges, thereby\nenhancing cooling efficiency. However, practical implementation of magnetic\nrefrigeration requires more than bulk materials; real-world devices demand\nefficient thermal management and compact, scalable architectures, often\nachieved through laminate designs or miniaturized geometries. Magnetocaloric\nmaterials with reduced dimensionality, such as ribbons, thin films, microwires,\nand nanostructures, offer distinct advantages, including improved heat\nexchange, mechanical flexibility, and integration potential. Despite these\nbenefits, a comprehensive understanding of how size, geometry, interfacial\neffects, strain, and surface phenomena influence the MCE remains limited. This\nreview aims to address these knowledge gaps and provide guidance for the\nrational design and engineering of magnetocaloric materials tailored for\nhigh-performance, energy-efficient magnetic refrigeration systems.", "AI": {"tldr": "\u672c\u7efc\u8ff0\u65e8\u5728\u89e3\u51b3\u4f4e\u7ef4\u78c1\u5236\u51b7\u6750\u6599\u7684\u7814\u7a76\u7a7a\u767d\uff0c\u91cd\u70b9\u5173\u6ce8\u5c3a\u5bf8\u3001\u51e0\u4f55\u5f62\u72b6\u3001\u754c\u9762\u6548\u5e94\u3001\u5e94\u53d8\u548c\u8868\u9762\u73b0\u8c61\u7684\u5f71\u54cd\uff0c\u4ee5\u6307\u5bfc\u9ad8\u6027\u80fd\u3001\u8282\u80fd\u78c1\u5236\u51b7\u7cfb\u7edf\u7684\u8bbe\u8ba1\u3002", "motivation": "\u78c1\u5236\u51b7\u6280\u672f\u6709\u6f5c\u529b\u53d6\u4ee3\u4f20\u7edf\u7684\u538b\u7f29\u5236\u51b7\u7cfb\u7edf\uff0c\u4f46\u76ee\u524d\u5bf9\u4f4e\u7ef4\u78c1\u5236\u51b7\u6750\u6599\u7684\u7814\u7a76\u4ecd\u4e0d\u5145\u5206\u3002", "method": "\u5bf9\u5c3a\u5bf8\u3001\u51e0\u4f55\u5f62\u72b6\u3001\u754c\u9762\u6548\u5e94\u3001\u5e94\u53d8\u548c\u8868\u9762\u73b0\u8c61\u5982\u4f55\u5f71\u54cd MCE \u7684\u73b0\u6709\u7814\u7a76\u8fdb\u884c\u56de\u987e\u548c\u5206\u6790\u3002", "result": "\u4f4e\u7ef4\u78c1\u5236\u51b7\u6750\u6599\uff08\u5982\u8584\u819c\u3001\u5fae\u7ebf\u7b49\uff09\u5177\u6709\u4f20\u70ed\u3001\u67d4\u97e7\u6027\u548c\u96c6\u6210\u6027\u7b49\u65b9\u9762\u7684\u4f18\u52bf\uff0c\u4f46\u5bf9\u5176 MCE \u5f71\u54cd\u56e0\u7d20\u7684\u7406\u89e3\u6709\u9650\u3002", "conclusion": "\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u5c3a\u5bf8\u3001\u51e0\u4f55\u5f62\u72b6\u3001\u754c\u9762\u6548\u5e94\u3001\u5e94\u53d8\u548c\u8868\u9762\u73b0\u8c61\u5982\u4f55\u5f71\u54cd MCE\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u6027\u80fd\u3001\u8282\u80fd\u7684\u78c1\u5236\u51b7\u7cfb\u7edf\u3002"}}
{"id": "2508.06320", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2508.06320", "abs": "https://arxiv.org/abs/2508.06320", "authors": ["Simon Krogmann", "Pascal Lenzner", "Alexander Skopalik", "Tobias Str\u00e4ubig"], "title": "Social Welfare in Battery Charging Games", "comment": "To appear at the International Symposium on Algorithmic Game Theory\n  (SAGT 2025)", "summary": "The recent rise of renewable energy produced by many decentralized sources\nyields interesting market design challenges for electrical grids. Balancing\nsupply and demand in such networks is both a temporal and spatial challenge due\nto capacity constraints. The recent surge in the number of household-owned\nbatteries, especially in regions with rooftop solar adoption, offers mitigation\npotential but often acts misaligned with grid-level objectives. In fact, the\ndecision to charge or discharge a household-owned battery is a strategic choice\nby each battery owner governed by selfish incentives. This calls for an\nanalysis from a game-theoretic point of view.\n  We initiate this timely research direction by considering a game-theoretic\nsetting where selfish agents strategically charge or discharge their batteries\nto increase their profit. In particular, we study a Stackelberg-like market\nmodel where a third party introduces price incentives, aiming to optimize\nrenewable energy utilization while preserving grid feasibility. For this, we\nstudy the existence and the quality of equilibria under various pricing\nstrategies. We find that the existence of equilibria crucially depends on the\nchosen pricing and that the obtained social welfare varies widely. This calls\nfor more sophisticated market models and pricing mechanisms and opens up a rich\nfield for future research in Algorithmic Game Theory on incentives in renewable\nenergy networks.", "AI": {"tldr": "Renewable energy and household batteries create market design challenges. This paper uses game theory to study how pricing strategies affect battery usage and grid stability, finding that pricing is key to achieving good outcomes and suggesting more research is needed.", "motivation": "The increasing number of decentralized renewable energy sources and household-owned batteries presents challenges for electrical grid market design. Household batteries, while offering mitigation potential, often operate misaligned with grid-level objectives due to selfish incentives of their owners. This necessitates a game-theoretic analysis.", "method": "The paper adopts a game-theoretic approach, specifically a Stackelberg-like market model, to analyze the strategic charging and discharging of household-owned batteries. It studies the existence and quality of equilibria under various pricing strategies.", "result": "The study found that the existence of equilibria is contingent upon the pricing strategy employed, and the resulting social welfare exhibits significant variation. This highlights the complexity of aligning individual incentives with grid-level objectives.", "conclusion": "The existence of equilibria depends on the chosen pricing strategy, and the social welfare varies widely, indicating a need for more sophisticated market models and pricing mechanisms. This opens up a rich field for future research in Algorithmic Game Theory on incentives in renewable energy networks."}}
{"id": "2508.05890", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2508.05890", "abs": "https://arxiv.org/abs/2508.05890", "authors": ["Yue Zhang", "Zhe Chen", "Daniel Harabor", "Pierre Le Bodic", "Peter J. Stuckey"], "title": "Flow-Based Task Assignment for Large-Scale Online Multi-Agent Pickup and Delivery", "comment": null, "summary": "We study the problem of online Multi-Agent Pickup and Delivery (MAPD), where\na team of agents must repeatedly serve dynamically appearing tasks on a shared\nmap. Existing online methods either rely on simple heuristics, which result in\npoor decisions, or employ complex reasoning, which suffers from limited\nscalability under real-time constraints. In this work, we focus on the task\nassignment subproblem and formulate it as a minimum-cost flow over the\nenvironment graph. This eliminates the need for pairwise distance computations\nand allows agents to be simultaneously assigned to tasks and routed toward\nthem. The resulting flow network also supports efficient guide path extraction\nto integrate with the planner and accelerates planning under real-time\nconstraints. To improve solution quality, we introduce two congestion-aware\nedge cost models that incorporate real-time traffic estimates. This approach\nsupports real-time execution and scales to over 20000 agents and 30000 tasks\nwithin 1-second planning time, outperforming existing baselines in both\ncomputational efficiency and assignment quality.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06 MAPD \u4e2d\u7684\u4efb\u52a1\u5206\u914d\u89c6\u4e3a\u6700\u5c0f\u6210\u672c\u6d41\u95ee\u9898\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u96c6\u6210\u5b9e\u65f6\u4ea4\u901a\u4f30\u8ba1\u6765\u63d0\u9ad8\u6548\u7387\u548c\u8d28\u91cf\uff0c\u5e76\u80fd\u5927\u89c4\u6a21\u5e94\u7528\u4e8e\u5177\u6709\u6311\u6218\u6027\u7684\u573a\u666f\u3002", "motivation": "\u5728\u7ebf\u591a\u4ee3\u7406\u62fe\u53d6\u548c\u4ea4\u4ed8\uff08MAPD\uff09\u95ee\u9898\uff0c\u5176\u4e2d\u4e00\u7ec4\u4ee3\u7406\u5fc5\u987b\u5728\u5171\u4eab\u5730\u56fe\u4e0a\u53cd\u590d\u670d\u52a1\u52a8\u6001\u51fa\u73b0\u7684\u4efb\u52a1\u3002\u73b0\u6709\u7684\u5728\u7ebf\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u4e8e\u7b80\u5355\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\uff08\u5bfc\u81f4\u51b3\u7b56\u4e0d\u4f73\uff09\uff0c\u8981\u4e48\u91c7\u7528\u590d\u6742\u7684\u63a8\u7406\uff08\u5728\u5b9e\u65f6\u7ea6\u675f\u4e0b\u53ef\u6269\u5c55\u6027\u6709\u9650\uff09\u3002", "method": "\u5c06\u4efb\u52a1\u5206\u914d\u5b50\u95ee\u9898\u5236\u5b9a\u4e3a\u73af\u5883\u56fe\u4e0a\u7684\u6700\u5c0f\u6210\u672c\u6d41\u95ee\u9898\uff0c\u6d88\u9664\u4e86\u5bf9\u6210\u5bf9\u8ddd\u79bb\u8ba1\u7b97\u7684\u9700\u6c42\uff0c\u5e76\u5141\u8bb8\u540c\u65f6\u5c06\u4ee3\u7406\u5206\u914d\u7ed9\u4efb\u52a1\u5e76\u5c06\u5176\u8def\u7531\u5230\u4efb\u52a1\u3002\u6240\u751f\u6210\u7684\u6d41\u7f51\u7edc\u8fd8\u652f\u6301\u9ad8\u6548\u7684\u5f15\u5bfc\u8def\u5f84\u63d0\u53d6\uff0c\u4ee5\u4e0e\u89c4\u5212\u5668\u96c6\u6210\uff0c\u5e76\u52a0\u5feb\u5b9e\u65f6\u7ea6\u675f\u4e0b\u7684\u89c4\u5212\u901f\u5ea6\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u652f\u6301\u5b9e\u65f6\u6267\u884c\uff0c\u53ef\u6269\u5c55\u5230\u8d85\u8fc7 20000 \u4e2a\u4ee3\u7406\u548c 30000 \u4e2a\u4efb\u52a1\uff0c\u540c\u65f6\u5728 1 \u79d2\u7684\u89c4\u5212\u65f6\u95f4\u5185\uff0c\u5728\u8ba1\u7b97\u6548\u7387\u548c\u5206\u914d\u8d28\u91cf\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u8ba1\u7b97\u6548\u7387\u548c\u5206\u914d\u8d28\u91cf\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u5e76\u4e14\u53ef\u4ee5\u652f\u6301\u5b9e\u65f6\u6267\u884c\uff0c\u53ef\u6269\u5c55\u5230\u8d85\u8fc7 20000 \u4e2a\u4ee3\u7406\u548c 30000 \u4e2a\u4efb\u52a1\uff0c\u540c\u65f6\u5728 1 \u79d2\u7684\u89c4\u5212\u65f6\u95f4\u5185\u3002"}}
{"id": "2508.06460", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2508.06460", "abs": "https://arxiv.org/abs/2508.06460", "authors": ["Akash Pareek", "Supratim Shit"], "title": "A Simple PTAS for Weighted $k$-means and Sensor Coverage", "comment": null, "summary": "Clustering is a fundamental technique in data analysis, with the $k$-means\nbeing one of the widely studied objectives due to its simplicity and broad\napplicability. In many practical scenarios, data points come with associated\nweights that reflect their importance, frequency, or confidence. Given a\nweighted point set $P \\subset R^d$, where each point $p \\in P$ has a positive\nweight $w_p$, the goal is to compute a set of $k$ centers $C = \\{ c_1, c_2,\n\\ldots, c_k \\} \\subset R^d$ that minimizes the weighted clustering cost:\n$\\Delta_w(P,C) = \\sum_{p \\in P} w_p \\cdot d(p,C)^2$, where $d(p,C)$ denotes the\nEuclidean distance from $p$ to its nearest center in $C$. Although most\nexisting coreset-based algorithms for $k$-means extend naturally to the\nweighted setting and provide a PTAS, no prior work has offered a simple,\ncoreset-free PTAS designed specifically for the weighted $k$-means problem.\n  In this paper, we present a simple PTAS for weighted $k$-means that does not\nrely on coresets. Building upon the framework of Jaiswal, Kumar, and Sen (2012)\nfor the unweighted case, we extend the result to the weighted setting by using\nthe weighted $D^2$-sampling technique. Our algorithm runs in time $n d \\cdot\n2^{O\\left(\\frac{k^2}{\\epsilon}\\right)}$ and outputs a set of $k$ centers whose\ntotal clustering cost is within a $(1 + \\epsilon)$-factor of the optimal cost.\nAs a key application of the weighted $k$-means, we obtain a PTAS for the sensor\ncoverage problem, which can also be viewed as a continuous locational\noptimization problem. For this problem, the best-known result prior to our work\nwas an $O(\\log k)$-approximation by Deshpande (2014), whereas our algorithm\nguarantees a $(1 + \\epsilon)$-approximation to the optimal coverage cost even\nbefore applying refinement steps like Lloyd desent.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u52a0\u6743k-\u5747\u503c\u95ee\u9898\u7684\u7b80\u5355PTAS\uff0c\u65e0\u9700\u6838\u96c6\uff0c\u5e76\u901a\u8fc7\u52a0\u6743D^2\u91c7\u6837\u6280\u672f\u89e3\u51b3\u3002\u8be5\u7b97\u6cd5\u7684\u8fd0\u884c\u65f6\u95f4\u4e3an*d*2^O(k^2/\u03b5)\uff0c\u53ef\u63d0\u4f9b(1+\u03b5)\u56e0\u5b50\u8fd1\u4f3c\u3002\u8be5\u65b9\u6cd5\u8fd8\u4e3a\u4f20\u611f\u5668\u8986\u76d6\u95ee\u9898\u63d0\u4f9b\u4e86(1+\u03b5)-\u8fd1\u4f3c\uff0c\u4f18\u4e8e\u4e4b\u524d\u7684O(log k)-\u8fd1\u4f3c\u3002", "motivation": "\u805a\u7c7b\u662f\u6570\u636e\u5206\u6790\u4e2d\u7684\u57fa\u672c\u6280\u672f\uff0c\u800ck-\u5747\u503c\u56e0\u5176\u7b80\u5355\u6027\u548c\u5e7f\u6cdb\u9002\u7528\u6027\u800c\u6210\u4e3a\u88ab\u5e7f\u6cdb\u7814\u7a76\u7684\u76ee\u6807\u3002\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u6570\u636e\u70b9\u901a\u5e38\u5177\u6709\u53cd\u6620\u5176\u91cd\u8981\u6027\u3001\u9891\u7387\u6216\u7f6e\u4fe1\u5ea6\u7684\u6743\u91cd\u3002\u56e0\u6b64\uff0c\u4e3a\u52a0\u6743k-\u5747\u503c\u95ee\u9898\u5f00\u53d1\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u7b97\u6cd5\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u57fa\u4e8e\u6838\u96c6\u7684\u7b97\u6cd5\u867d\u7136\u53ef\u4ee5\u6269\u5c55\u5230\u52a0\u6743\u60c5\u51b5\uff0c\u4f46\u6ca1\u6709\u63d0\u4f9b\u4e13\u95e8\u4e3a\u52a0\u6743k-\u5747\u503c\u95ee\u9898\u8bbe\u8ba1\u7684\u7b80\u5355\u3001\u65e0\u6838\u96c6\u7684PTAS\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684PTAS\uff0c\u7528\u4e8e\u52a0\u6743k-\u5747\u503c\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u57fa\u4e8eJaiswal\u3001Kumar\u548cSen\uff082012\uff09\u4e3a\u672a\u52a0\u6743\u60c5\u51b5\u63d0\u51fa\u7684\u6846\u67b6\uff0c\u5e76\u5f15\u5165\u4e86\u52a0\u6743D^2\u91c7\u6837\u6280\u672f\u6765\u5904\u7406\u52a0\u6743\u70b9\u96c6\u3002\u7b97\u6cd5\u7684\u65f6\u95f4\u590d\u6742\u5ea6\u4e3an*d*2^O(k^2/\u03b5)\uff0c\u5e76\u4fdd\u8bc1\u8f93\u51fa\u7684k\u4e2a\u4e2d\u5fc3\u7684\u603b\u805a\u7c7b\u6210\u672c\u5728(1+\u03b5)\u56e0\u5b50\u5185\u63a5\u8fd1\u6700\u4f18\u6210\u672c\u3002", "result": "\u8be5\u7b97\u6cd5\u7684\u65f6\u95f4\u590d\u6742\u5ea6\u4e3an*d*2^O(k^2/\u03b5)\uff0c\u53ef\u4ee5\u4fdd\u8bc1\u8f93\u51fak\u4e2a\u4e2d\u5fc3\u7684\u603b\u805a\u7c7b\u6210\u672c\u5728(1+\u03b5)\u56e0\u5b50\u5185\u63a5\u8fd1\u6700\u4f18\u6210\u672c\u3002\u6b64\u5916\uff0c\u8be5\u7b97\u6cd5\u4e3a\u4f20\u611f\u5668\u8986\u76d6\u95ee\u9898\u63d0\u4f9b\u4e86(1+\u03b5)-\u8fd1\u4f3c\uff0c\u4f18\u4e8e\u5148\u524d trabajos \u7684O(log k)-\u8fd1\u4f3c\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u52a0\u6743k-\u5747\u503c\u95ee\u9898\u7684\u4e00\u79cd\u7b80\u5355PTAS\uff0c\u4e0d\u4f9d\u8d56\u4e8e\u6838\u96c6\uff0c\u5e76\u5c06\u52a0\u6743D^2\u91c7\u6837\u6280\u672f\u5e94\u7528\u4e8eJaiswal\u3001Kumar\u548cSen\uff082012\uff09\u63d0\u51fa\u7684\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u52a0\u6743k-\u5747\u503c\u95ee\u9898\u3002\u6b64\u5916\uff0c\u8be5\u7b97\u6cd5\u8fd8\u4e3a\u4f20\u611f\u5668\u8986\u76d6\u95ee\u9898\u63d0\u4f9b\u4e86PTAS\uff0c\u8fd9\u662f\u52a0\u6743k-\u5747\u503c\u7684\u4e00\u4e2a\u5173\u952e\u5e94\u7528\u3002"}}
{"id": "2508.06022", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.06022", "abs": "https://arxiv.org/abs/2508.06022", "authors": ["Zeping Sui", "Qu Luo", "Zilong Liu", "Murat Temiz", "Leila Musavian", "Christos Masouros", "Yong Liang Guan", "Pei Xiao", "Lajos Hanzo"], "title": "Multi-Functional Chirp Signalling for Next-Generation Multi-Carrier Wireless Networks: Communications, Sensing and ISAC Perspectives", "comment": "8 pages, 5 figures, submitted to IEEE Wireless Communications", "summary": "To meet the increasingly demanding quality-of-service requirements of the\nnext-generation multi-carrier mobile networks, it is essential to design\nmulti-functional signalling schemes facilitating efficient, flexible, and\nreliable communication and sensing in complex wireless environments. As a\ncompelling candidate, we advocate chirp signalling, beneficially amalgamating\nsequences (e.g., Zadoff-Chu sequences) with waveforms (e.g., chirp spread\nspectrum and frequency-modulated continuous wave (FMCW) radar), given their\nresilience against doubly selective channels. Besides chirp sequences, a wide\nrange of chirp waveforms is considered, ranging from FMCW to affine\nfrequency-division multiplexing (AFDM), to create a promising chirp\nmulticarrier waveform. This study also highlights the advantages of such\nwaveforms in supporting reliable high-mobility communications, plus integrated\nsensing and communications (ISAC). Finally, we outline several emerging\nresearch directions for chirp signalling designs.", "AI": {"tldr": "Chirp signalling is beneficial for future mobile networks, offering efficient and reliable communication and sensing capabilities.", "motivation": "To meet the increasingly demanding quality-of-service requirements of next-generation multi-carrier mobile networks by designing multi-functional signalling schemes for efficient, flexible, and reliable communication and sensing in complex wireless environments.", "method": "The paper advocates chirp signalling by considering a wide range of chirp waveforms, including chirp sequences (e.g., Zadoff-Chu sequences) and waveforms (e.g., FMCW and AFDM).", "result": "Chirp signalling offers advantages in supporting reliable high-mobility communications and integrated sensing and communications (ISAC).", "conclusion": "Chirp signalling is a promising candidate for next-generation mobile networks due to its resilience against doubly selective channels and its ability to support high-mobility communications and ISAC."}}
{"id": "2508.06001", "categories": ["cs.DC", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06001", "abs": "https://arxiv.org/abs/2508.06001", "authors": ["Kai Zhang", "Peng Wang", "Sai Bi", "Jianming Zhang", "Yuanjun Xiong"], "title": "KnapFormer: An Online Load Balancer for Efficient Diffusion Transformers Training", "comment": "Code is available at https://github.com/Kai-46/KnapFormer/", "summary": "We present KnapFormer, an efficient and versatile framework to combine\nworkload balancing and sequence parallelism in distributed training of\nDiffusion Transformers (DiT). KnapFormer builds on the insight that strong\nsynergy exists between sequence parallelism and the need to address the\nsignificant token imbalance across ranks. This imbalance arises from\nvariable-length text inputs and varying visual token counts in mixed-resolution\nand image-video joint training. KnapFormer redistributes tokens by first\ngathering sequence length metadata across all ranks in a balancing group and\nsolving a global knapsack problem. The solver aims to minimize the variances of\ntotal workload per-GPU, while accounting for the effect of sequence\nparallelism. By integrating DeepSpeed-Ulysees-based sequence parallelism in the\nload-balancing decision process and utilizing a simple semi-empirical workload\nmodel, KnapFormers achieves minimal communication overhead and less than 1%\nworkload discrepancy in real-world training workloads with sequence length\nvarying from a few hundred to tens of thousands. It eliminates straggler\neffects and achieves 2x to 3x speedup when training state-of-the-art diffusion\nmodels like FLUX on mixed-resolution and image-video joint data corpora. We\nopen-source the KnapFormer implementation at\nhttps://github.com/Kai-46/KnapFormer/", "AI": {"tldr": "KnapFormer\u662f\u4e00\u4e2a\u7528\u4e8e\u5206\u5e03\u5f0f\u8bad\u7ec3\u6269\u6563Transformer\uff08DiT\uff09\u7684\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u7ed3\u5408\u5e8f\u5217\u5e76\u884c\u6027\u548c\u8d1f\u8f7d\u5747\u8861\u6765\u89e3\u51b3token\u4e0d\u5e73\u8861\u95ee\u9898\u3002\u901a\u8fc7\u89e3\u51b3\u4e00\u4e2a\u5168\u5c40\u80cc\u5305\u95ee\u9898\u6765\u4f18\u5316token\u5206\u914d\uff0cKnapFormer\u6700\u5c0f\u5316\u4e86GPU\u95f4\u7684\u5de5\u4f5c\u8d1f\u8f7d\u5dee\u5f02\uff0c\u51cf\u5c11\u4e86\u901a\u4fe1\u5f00\u9500\uff0c\u5e76\u5b9e\u73b0\u4e86\u9ad8\u8fbe2-3\u500d\u7684\u8bad\u7ec3\u52a0\u901f\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u53ef\u53d8\u957f\u5ea6\u8f93\u5165\u548c\u6df7\u5408\u6570\u636e\u7c7b\u578b\u65f6\u3002", "motivation": "\u5728\u5206\u5e03\u5f0f\u8bad\u7ec3DiT\u6a21\u578b\u65f6\uff0c\u7531\u4e8e\u8f93\u5165\u6587\u672c\u957f\u5ea6\u53ef\u53d8\u4ee5\u53ca\u6df7\u5408\u5206\u8fa8\u7387\u548c\u56fe\u50cf-\u89c6\u9891\u8054\u5408\u8bad\u7ec3\u4e2d\u89c6\u89c9token\u6570\u91cf\u7684\u53d8\u5316\uff0c\u5bfc\u81f4\u4e0d\u540c\u8ba1\u7b97\u8282\u70b9\uff08ranks\uff09\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u7684token\u4e0d\u5e73\u8861\u95ee\u9898\u3002\u8fd9\u79cd\u4e0d\u5e73\u8861\u4f1a\u5f15\u5165\u62d6\u5c3e\u6548\u5e94\uff08straggler effects\uff09\uff0c\u964d\u4f4e\u8bad\u7ec3\u6548\u7387\u3002", "method": "KnapFormer\u6846\u67b6\u9996\u5148\u6536\u96c6\u6240\u6709\u8ba1\u7b97\u8282\u70b9\uff08ranks\uff09\u7684\u5e8f\u5217\u957f\u5ea6\u5143\u6570\u636e\uff0c\u7136\u540e\u5728\u8d1f\u8f7d\u5747\u8861\u5206\u7ec4\u5185\u89e3\u51b3\u4e00\u4e2a\u5168\u5c40\u80cc\u5305\u95ee\u9898\u3002\u8be5\u4f18\u5316\u95ee\u9898\u7684\u76ee\u6807\u662f\u6700\u5c0f\u5316\u6bcf\u4e2aGPU\u7684\u603b\u5de5\u4f5c\u8d1f\u8f7d\u65b9\u5dee\uff0c\u5e76\u7eb3\u5165\u5e8f\u5217\u5e76\u884c\u6027\u7684\u5f71\u54cd\u3002\u5b83\u5c06\u57fa\u4e8eDeepSpeed-Ulysees\u7684\u5e8f\u5217\u5e76\u884c\u6027\u96c6\u6210\u5230\u8d1f\u8f7d\u5747\u8861\u51b3\u7b56\u8fc7\u7a0b\u4e2d\uff0c\u5e76\u4f7f\u7528\u4e00\u4e2a\u7b80\u5355\u7684\u534a\u7ecf\u9a8c\u5de5\u4f5c\u8d1f\u8f7d\u6a21\u578b\u3002", "result": "KnapFormer\u6846\u67b6\u901a\u8fc7\u5176\u4f18\u5316\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u6700\u5c0f\u7684\u901a\u4fe1\u5f00\u9500\u548c\u4f4e\u4e8e1%\u7684\u5de5\u4f5c\u8d1f\u8f7d\u5dee\u5f02\uff0c\u5373\u4f7f\u5728\u5e8f\u5217\u957f\u5ea6\u4ece\u51e0\u767e\u5230\u51e0\u4e07\u4e0d\u7b49\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u4fdd\u6301\u9ad8\u6548\u3002\u8be5\u6846\u67b6\u5728\u6df7\u5408\u5206\u8fa8\u7387\u548c\u56fe\u50cf-\u89c6\u9891\u8054\u5408\u6570\u636e\u8bed\u6599\u4e0a\u8bad\u7ec3FLUX\u7b49\u5148\u8fdb\u6269\u6563\u6a21\u578b\u65f6\uff0c\u80fd\u591f\u5b9e\u73b02\u52303\u500d\u7684\u52a0\u901f\uff0c\u5e76\u6d88\u9664\u4e86\u62d6\u5c3e\u6548\u5e94\u3002", "conclusion": "KnapFormer\u6846\u67b6\u901a\u8fc7\u6574\u5408\u5e8f\u5217\u5e76\u884c\u6027\u548c\u8d1f\u8f7d\u5747\u8861\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5206\u5e03\u5f0f\u8bad\u7ec3\u4e2dDiT\u6a21\u578b\u9762\u4e34\u7684\u663e\u8457token\u4e0d\u5e73\u8861\u95ee\u9898\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u89e3\u51b3\u5168\u5c40\u80cc\u5305\u95ee\u9898\u6765\u91cd\u65b0\u5206\u914dtokens\uff0c\u65e8\u5728\u6700\u5c0f\u5316\u5168\u5c40\u603b\u5de5\u4f5c\u8d1f\u8f7d\u65b9\u5dee\uff0c\u540c\u65f6\u8003\u8651\u5e8f\u5217\u5e76\u884c\u6027\u7684\u5f71\u54cd\u3002"}}
{"id": "2508.06173", "categories": ["cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2508.06173", "abs": "https://arxiv.org/abs/2508.06173", "authors": ["Christian E. N. Petersen", "Damon J. Carrad", "Thierry D\u00e9sir\u00e9", "Daria Beznasyuk", "Jung-Hyun Kang", "D\u0101gs Ol\u0161teins", "Gunjan Nagda", "Dennis V. Christensen", "Thomas Sand Jespersen"], "title": "$\u03bc_\\mathrm{2T}(n)$: A Method for Extracting the Density Dependent Mobility in Two-Terminal Nanodevices", "comment": null, "summary": "Measuring carrier mobility as a function of the carrier density in\nsemiconductors using Hall effect is the gold standard for quantifying\nscattering mechanisms. However, for nanostructures, the Hall effect is not\napplicable, and the density dependence of mobility is generally inaccessible,\nrendering Hall effect measurements impractical. Here, we present\n$\\mu_\\mathrm{2T}(n)$, a new procedure allowing us to extract the density\ndependent mobility in two-terminal measured nano scale field effect transistors\nat zero magnetic field from conventional conductance vs gate voltage\nmeasurements. We validate $\\mu_\\mathrm{2T}$ against standard Hall measurements\nand then apply the procedure to 256 individual two-terminal InAs nanowire FETs,\nextracting information about the scattering mechanisms. To illustrate its broad\nutility, we reanalyze published data in which mobility had been treated as\ndensity independent. Our method represents a new powerful tool for optimization\nand development of nanomaterials crucial for a wide range of new technologies.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.05716", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.05716", "abs": "https://arxiv.org/abs/2508.05716", "authors": ["Hikaru Wakaura"], "title": "Quantum Reservoir GAN", "comment": null, "summary": "Quantum machine learning is known as one of the promising applications of\nquantum computers. Many types of quantum machine learning methods have been\nreleased, such as Quantum Annealer, Quantum Neural Network, Variational Quantum\nAlgorithms, and Quantum Reservoir Computers. They can work consuming far less\nenergy for networks of equivalent size. Quantum Reservoir Computers, in\nparticular, have no limit on the size of input data. However, their accuracy is\nnot enough for practical use, and the effort to improve accuracy is mainly\nfocused on hardware improvements. Therefore, we propose the approach from\nsoftware called Quantum Reservoir Generative Adversarial Network ( GAN ), which\nuses Quantum Reservoir Computers as a generator of GAN. We performed the\ngeneration of handwritten single digits and monochrome pictures on the CIFAR10\ndataset. As a result, Quantum Reservoir GAN is confirmed to be more accurate\nthan Quantum GAN, Classical Neural Network, and ordinary Quantum Reservoir\nComputers.", "AI": {"tldr": "Quantum Reservoir GAN improves accuracy by using Quantum Reservoir Computers as a GAN generator, outperforming existing methods.", "motivation": "To improve the accuracy of Quantum Reservoir Computers, which is not enough for practical use, by focusing on software improvements instead of hardware.", "method": "The proposed approach is Quantum Reservoir Generative Adversarial Network (GAN), which uses Quantum Reservoir Computers as a generator of GAN.", "result": "Generation of handwritten single digits and monochrome pictures on the CIFAR10 dataset. Quantum Reservoir GAN showed higher accuracy than Quantum GAN, Classical Neural Network, and ordinary Quantum Reservoir Computers.", "conclusion": "Quantum Reservoir GAN is confirmed to be more accurate than Quantum GAN, Classical Neural Network, and ordinary Quantum Reservoir Computers."}}
{"id": "2508.05803", "categories": ["cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2508.05803", "abs": "https://arxiv.org/abs/2508.05803", "authors": ["Abishek Thamma", "Micha Heilbron"], "title": "Human-like fleeting memory improves language learning but impairs reading time prediction in transformer language models", "comment": null, "summary": "Human memory is fleeting. As words are processed, the exact wordforms that\nmake up incoming sentences are rapidly lost. Cognitive scientists have long\nbelieved that this limitation of memory may, paradoxically, help in learning\nlanguage - an idea supported by classic connectionist modelling work. The rise\nof Transformers appears to challenge this idea, as these models can learn\nlanguage effectively, despite lacking memory limitations or other architectural\nrecency biases. Here, we investigate the hypothesized benefit of fleeting\nmemory for language learning in tightly controlled experiments on transformer\nlanguage models. Training transformers with and without fleeting memory on a\ndevelopmentally realistic training set, we find that fleeting memory\nconsistently improves language learning (as quantified by both overall language\nmodelling performance and targeted syntactic evaluation) but, unexpectedly,\nimpairs surprisal-based prediction of human reading times. Interestingly,\nfollow up analyses revealed that this discrepancy - better language modeling,\nyet worse reading time prediction - could not be accounted for by prior\nexplanations of why better language models sometimes fit human reading time\nworse. Together, these results support a benefit of memory limitations on\nneural network language learning - but not on predicting behavior.", "AI": {"tldr": "\u77ed\u671f\u8bb0\u5fc6\u9650\u5236\u6709\u52a9\u4e8e Transformer \u8bed\u8a00\u6a21\u578b\u5b66\u4e60\uff0c\u4f46\u5bf9\u9884\u6d4b\u4eba\u7c7b\u9605\u8bfb\u65f6\u95f4\u6709\u8d1f\u9762\u5f71\u54cd\u3002", "motivation": "\u63a2\u8ba8\u4e86\u5728 Transformer \u6a21\u578b\u4e2d\u5f15\u5165\u77ed\u671f\u8bb0\u5fc6\u4ee5\u89e3\u91ca\u5176\u8bed\u8a00\u5b66\u4e60\u80fd\u529b\uff0c\u5e76\u5bf9\u6bd4\u4e86\u5176\u4e0e\u4eba\u7c7b\u8bb0\u5fc6\u9650\u5236\u7684\u6f5c\u5728\u8054\u7cfb\u3002", "method": "\u901a\u8fc7\u5728 Transformer \u8bed\u8a00\u6a21\u578b\u4e0a\u8fdb\u884c\u53d7\u63a7\u5b9e\u9a8c\uff0c\u5bf9\u6bd4\u4e86\u6709\u65e0\u77ed\u671f\u8bb0\u5fc6\u5bf9\u5176\u8bed\u8a00\u5b66\u4e60\u80fd\u529b\u7684\u5f71\u54cd\u3002", "result": "\u77ed\u671f\u8bb0\u5fc6\u80fd\u591f\u63d0\u5347 Transformer \u8bed\u8a00\u6a21\u578b\u7684\u5b66\u4e60\u6027\u80fd\uff0c\u4f46\u5728\u9884\u6d4b\u4eba\u7c7b\u9605\u8bfb\u65f6\u95f4\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u73b0\u6709\u7406\u8bba\u65e0\u6cd5\u5b8c\u5168\u89e3\u91ca\u8fd9\u79cd\u73b0\u8c61\u3002", "conclusion": "\u867d\u7136\u8bb0\u5fc6\u9650\u5236\u786e\u5b9e\u6709\u52a9\u4e8e\u63d0\u9ad8 Transformer \u8bed\u8a00\u6a21\u578b\u7684\u5b66\u4e60\u80fd\u529b\uff0c\u4f46\u5bf9\u4eba\u7c7b\u9605\u8bfb\u65f6\u95f4\u7684\u9884\u6d4b\u80fd\u529b\u5374\u6709\u6240\u4e0b\u964d\u3002"}}
{"id": "2508.06079", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.06079", "abs": "https://arxiv.org/abs/2508.06079", "authors": ["Tzu-Chien Hsueh", "Bill Lin", "Zijun Chen", "Yeshaiahu Fainman"], "title": "Panel-Scale Reconfigurable Photonic Interconnects for Scalable AI Computation", "comment": "16 pages, 9 figures, 2 tables", "summary": "Panel-scale reconfigurable photonic interconnects on a glass substrate up to\n500-mm x 500-mm or larger are envisioned by proposing a novel photonic switch\nfabric that enables all directional panel-edge-to-panel-edge reach without the\nneed for active repeaters while offering high communication bandwidth,\nplanar-direction reconfigurability, low energy consumption, and compelling data\nbandwidth density for heterogeneous integration of an in-package AI computing\nsystem on a single glass-substrate photonic interposer exceeding thousands of\ncentimeters square. The proposed approach focuses on reconfigurable photonic\ninterconnects, which are integration-compatible with commercial processor\nchiplets and 3D high-bandwidth memory (HBM) stacks on a large-area glass\nsubstrate, to create a novel panel-scale heterogeneously integrated interposer\nor package enabling low-energy and high-capacity\nwavelength-division-multiplexing (WDM) optical data links using advanced\nhigh-speed optical modulators, broadband photodetectors, novel optical crossbar\nswitches with multi-layer waveguides, and in-package frequency comb sources.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u677f\u7ea7\u53ef\u91cd\u6784\u5149\u4e92\u8fde\u6280\u672f\uff0c\u7528\u4e8eAI\u8ba1\u7b97\u7cfb\u7edf\uff0c\u5b9e\u73b0\u9ad8\u5e26\u5bbd\u3001\u4f4e\u529f\u8017\u548c\u5927\u89c4\u6a21\u96c6\u6210\u3002", "motivation": "\u4e3a\u4e86\u6ee1\u8db3AI\u8ba1\u7b97\u7cfb\u7edf\u5bf9\u9ad8\u5e26\u5bbd\u3001\u4f4e\u529f\u8017\u548c\u5927\u89c4\u6a21\u96c6\u6210\u4e92\u8fde\u7684\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5149\u5f00\u5173\u7ed3\u6784\uff0c\u80fd\u591f\u5728\u9762\u677f\u8fb9\u7f18\u5230\u9762\u677f\u8fb9\u7f18\u5b9e\u73b0\u6240\u6709\u65b9\u5411\u7684\u8fde\u63a5\uff0c\u65e0\u9700\u6709\u6e90\u4e2d\u7ee7\u5668\u3002", "result": "\u5b9e\u73b0\u4e86\u9762\u677f\u7ea7\u53ef\u91cd\u6784\u5149\u4e92\u8fde\uff0c\u652f\u6301\u9ad8\u901a\u4fe1\u5e26\u5bbd\u3001\u5e73\u9762\u65b9\u5411\u53ef\u91cd\u6784\u6027\u3001\u4f4e\u529f\u8017\u548c\u9ad8\u6570\u636e\u5e26\u5bbd\u5bc6\u5ea6\uff0c\u5e76\u4e0e\u5546\u4e1a\u5904\u7406\u5668chiplets\u548c3D\u9ad8\u5e26\u5bbd\u5185\u5b58\uff08HBM\uff09\u5806\u6808\u517c\u5bb9\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u9762\u677f\u7ea7\u5149\u4e92\u8fde\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u5f02\u6784\u96c6\u6210\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728AI\u8ba1\u7b97\u7cfb\u7edf\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.05769", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05769", "abs": "https://arxiv.org/abs/2508.05769", "authors": ["Seyed Hadi Seyed", "Ayberk Cansever", "David Hart"], "title": "Improving Masked Style Transfer using Blended Partial Convolution", "comment": null, "summary": "Artistic style transfer has long been possible with the advancements of\nconvolution- and transformer-based neural networks. Most algorithms apply the\nartistic style transfer to the whole image, but individual users may only need\nto apply a style transfer to a specific region in the image. The standard\npractice is to simply mask the image after the stylization. This work shows\nthat this approach tends to improperly capture the style features in the region\nof interest. We propose a partial-convolution-based style transfer network that\naccurately applies the style features exclusively to the region of interest.\nAdditionally, we present network-internal blending techniques that account for\nimperfections in the region selection. We show that this visually and\nquantitatively improves stylization using examples from the SA-1B dataset. Code\nis publicly available at https://github.com/davidmhart/StyleTransferMasked.", "AI": {"tldr": "Existing style transfer methods apply to whole images, but users often only want specific regions styled. Masking after transfer doesn't work well. This paper presents a new method using partial convolutions and internal blending to accurately style only the chosen region, improving results.", "motivation": "While artistic style transfer is possible with neural networks, existing methods apply it to the whole image. Users often only need to apply style transfer to specific regions, and the common practice of masking after stylization fails to properly capture style features within the region of interest. This work aims to address this limitation by enabling accurate, region-specific style transfer.", "method": "This paper proposes a partial-convolution-based style transfer network that applies style features exclusively to a user-specified region of interest. It also introduces network-internal blending techniques to address potential imperfections in the region selection process.", "result": "The proposed method visually and quantitatively improves artistic style transfer by applying it exclusively to the region of interest. Examples from the SA-1B dataset are used to demonstrate these improvements.", "conclusion": "In conclusion, this paper proposes a novel partial-convolution-based style transfer network that accurately applies style features exclusively to the region of interest, along with network-internal blending techniques to handle imperfections in region selection. The approach visually and quantitatively improves stylization, as demonstrated with examples from the SA-1B dataset."}}
{"id": "2508.05937", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.05937", "abs": "https://arxiv.org/abs/2508.05937", "authors": ["Gen Sako", "Takuya Kiyokawa", "Kensuke Harada", "Tomoki Ishikura", "Naoya Miyaji", "Genichiro Matsuda"], "title": "Affordance-Guided Dual-Armed Disassembly Teleoperation for Mating Parts", "comment": "6 pages, 9 figures", "summary": "Robotic non-destructive disassembly of mating parts remains challenging due\nto the need for flexible manipulation and the limited visibility of internal\nstructures. This study presents an affordance-guided teleoperation system that\nenables intuitive human demonstrations for dual-arm fix-and-disassemble tasks\nfor mating parts. The system visualizes feasible grasp poses and disassembly\ndirections in a virtual environment, both derived from the object's geometry,\nto address occlusions and structural complexity. To prevent excessive position\ntracking under load when following the affordance, we integrate a hybrid\ncontroller that combines position and impedance control into the teleoperated\ndisassembly arm. Real-world experiments validate the effectiveness of the\nproposed system, showing improved task success rates and reduced object pose\ndeviation.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u673a\u5668\u4eba\u53cc\u81c2\u62c6\u5378\u5d4c\u63a5\u4ef6\u7684\u9065\u64cd\u4f5c\u7cfb\u7edf\uff0c\u901a\u8fc7\u865a\u62df\u754c\u9762\u63d0\u4f9b\u6293\u53d6\u59ff\u6001\u548c\u62c6\u5378\u65b9\u5411\uff0c\u5e76\u91c7\u7528\u6df7\u5408\u63a7\u5236\u5668\u63d0\u9ad8\u64cd\u4f5c\u7cbe\u5ea6\u548c\u6210\u529f\u7387\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u673a\u5668\u4eba\u62c6\u5378\u5d4c\u63a5\u4ef6\u65f6\u64cd\u4f5c\u7075\u6d3b\u6027\u548c\u5185\u90e8\u7ed3\u6784\u53ef\u89c1\u6027\u6709\u9650\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5141\u8bb8\u7528\u6237\u901a\u8fc7\u6f14\u793a\u8fdb\u884c\u64cd\u4f5c\u7684\u3001\u8003\u8651\u4e86\u7269\u4f53\u51e0\u4f55\u5f62\u72b6\u7684\u3001\u5177\u6709\u6293\u53d6\u59ff\u6001\u548c\u62c6\u5378\u65b9\u5411\u7684\u865a\u62df\u754c\u9762\u7684\u9065\u64cd\u4f5c\u7cfb\u7edf\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u80fd\u591f\u63d0\u9ad8\u4efb\u52a1\u6210\u529f\u7387\u5e76\u51cf\u5c11\u7269\u4f53\u59ff\u6001\u504f\u5dee\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4f4d\u7f6e\u548c\u963b\u6297\u63a7\u5236\u7684\u6df7\u5408\u63a7\u5236\u5668\uff0c\u4ee5\u89e3\u51b3\u53cc\u81c2\u6293\u53d6\u548c\u62c6\u5378\u4efb\u52a1\u4e2d\u5185\u5d4c\u90e8\u4ef6\u7684\u6311\u6218\u3002"}}
{"id": "2508.05792", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05792", "abs": "https://arxiv.org/abs/2508.05792", "authors": ["Kausik Lakkaraju", "Siva Likitha Valluru", "Biplav Srivastava"], "title": "Holistic Explainable AI (H-XAI): Extending Transparency Beyond Developers in AI-Driven Decision Making", "comment": null, "summary": "Current eXplainable AI (XAI) methods largely serve developers, often focusing\non justifying model outputs rather than supporting diverse stakeholder needs. A\nrecent shift toward Evaluative AI reframes explanation as a tool for hypothesis\ntesting, but still focuses primarily on operational organizations. We introduce\nHolistic-XAI (H-XAI), a unified framework that integrates causal rating methods\nwith traditional XAI methods to support explanation as an interactive,\nmulti-method process. H-XAI allows stakeholders to ask a series of questions,\ntest hypotheses, and compare model behavior against automatically constructed\nrandom and biased baselines. It combines instance-level and global\nexplanations, adapting to each stakeholder's goals, whether understanding\nindividual decisions, assessing group-level bias, or evaluating robustness\nunder perturbations. We demonstrate the generality of our approach through two\ncase studies spanning six scenarios: binary credit risk classification and\nfinancial time-series forecasting. H-XAI fills critical gaps left by existing\nXAI methods by combining causal ratings and post-hoc explanations to answer\nstakeholder-specific questions at both the individual decision level and the\noverall model level.", "AI": {"tldr": "H-XAI \u662f\u4e00\u4e2a\u6539\u8fdb\u7684 XAI \u6846\u67b6\uff0c\u5b83\u6574\u5408\u4e86\u56e0\u679c\u8bc4\u5206\u548c\u4f20\u7edf XAI \u65b9\u6cd5\uff0c\u4ee5\u652f\u6301\u5305\u62ec\u7406\u89e3\u4e2a\u4f53\u51b3\u7b56\u3001\u8bc4\u4f30\u7fa4\u4f53\u504f\u89c1\u548c\u8bc4\u4f30\u6270\u52a8\u4e0b\u7684\u9c81\u68d2\u6027\u5728\u5185\u7684\u5404\u79cd\u5229\u76ca\u76f8\u5173\u8005\u76ee\u6807\u3002", "motivation": "\u73b0\u6709\u7684 XAI \u65b9\u6cd5\u4e3b\u8981\u670d\u52a1\u4e8e\u5f00\u53d1\u8005\uff0c\u4fa7\u91cd\u4e8e\u89e3\u91ca\u6a21\u578b\u8f93\u51fa\u6765\u8bc1\u660e\u5176\u5408\u7406\u6027\uff0c\u800c\u4e0d\u662f\u6ee1\u8db3\u591a\u6837\u5316\u5229\u76ca\u76f8\u5173\u8005\u7684\u9700\u6c42\u3002\u5c3d\u7ba1\u8fd1\u671f\u5411\u8bc4\u4f30\u6027 AI \u7684\u8f6c\u53d8\u5c06\u89e3\u91ca\u91cd\u65b0\u5b9a\u4e49\u4e3a\u5047\u8bbe\u68c0\u9a8c\u7684\u5de5\u5177\uff0c\u4f46\u5b83\u4ecd\u7136\u4e3b\u8981\u5173\u6ce8\u8fd0\u8425\u7ec4\u7ec7\u3002", "method": "H-XAI \u662f\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u6574\u5408\u4e86\u56e0\u679c\u8bc4\u5206\u65b9\u6cd5\u548c\u4f20\u7edf XAI \u65b9\u6cd5\uff0c\u652f\u6301\u5c06\u89e3\u91ca\u4f5c\u4e3a\u4e00\u79cd\u4ea4\u4e92\u5f0f\u7684\u3001\u591a\u65b9\u6cd5\u7684\u8fc7\u7a0b\u3002\u5b83\u5141\u8bb8\u5229\u76ca\u76f8\u5173\u8005\u63d0\u51fa\u4e00\u7cfb\u5217\u95ee\u9898\u3001\u68c0\u9a8c\u5047\u8bbe\uff0c\u5e76\u5c06\u6a21\u578b\u884c\u4e3a\u4e0e\u81ea\u52a8\u6784\u5efa\u7684\u968f\u673a\u548c\u504f\u5dee\u57fa\u7ebf\u8fdb\u884c\u6bd4\u8f83\u3002\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u5b9e\u4f8b\u7ea7\u522b\u548c\u5168\u5c40\u89e3\u91ca\uff0c\u5e76\u80fd\u9002\u5e94\u4e0d\u540c\u5229\u76ca\u76f8\u5173\u8005\u7684\u76ee\u6807\uff0c\u5982\u7406\u89e3\u4e2a\u4f53\u51b3\u7b56\u3001\u8bc4\u4f30\u7fa4\u4f53\u504f\u89c1\u6216\u8bc4\u4f30\u6270\u52a8\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "result": "\u901a\u8fc7\u8de8\u8d8a\u4e8c\u5143\u4fe1\u8d37\u98ce\u9669\u5206\u7c7b\u548c\u91d1\u878d\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u516d\u79cd\u573a\u666f\u7684\u4e24\u4e2a\u6848\u4f8b\u7814\u7a76\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u901a\u7528\u6027\u3002", "conclusion": "H-XAI \u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u56e0\u679c\u8bc4\u5206\u548c\u4e8b\u540e\u89e3\u91ca\uff0c\u586b\u8865\u4e86\u73b0\u6709 XAI \u65b9\u6cd5\u7684\u7a7a\u767d\uff0c\u80fd\u591f\u56de\u5e94\u4ece\u4e2a\u4f53\u51b3\u7b56\u5230\u6574\u4f53\u6a21\u578b\u7684\u7279\u5b9a\u5229\u76ca\u76f8\u5173\u8005\u7684\u95ee\u9898\u3002"}}
{"id": "2508.06347", "categories": ["cs.LG", "cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2508.06347", "abs": "https://arxiv.org/abs/2508.06347", "authors": ["Ruiyu Zhang", "Ce Zhao", "Xin Zhao", "Lin Nie", "Wai-Fung Lam"], "title": "Structural Equation-VAE: Disentangled Latent Representations for Tabular Data", "comment": "10 pages, 2 figures", "summary": "Learning interpretable latent representations from tabular data remains a\nchallenge in deep generative modeling. We introduce SE-VAE (Structural\nEquation-Variational Autoencoder), a novel architecture that embeds measurement\nstructure directly into the design of a variational autoencoder. Inspired by\nstructural equation modeling, SE-VAE aligns latent subspaces with known\nindicator groupings and introduces a global nuisance latent to isolate\nconstruct-specific confounding variation. This modular architecture enables\ndisentanglement through design rather than through statistical regularizers\nalone. We evaluate SE-VAE on a suite of simulated tabular datasets and\nbenchmark its performance against a series of leading baselines using standard\ndisentanglement metrics. SE-VAE consistently outperforms alternatives in factor\nrecovery, interpretability, and robustness to nuisance variation. Ablation\nresults reveal that architectural structure, rather than regularization\nstrength, is the key driver of performance. SE-VAE offers a principled\nframework for white-box generative modeling in scientific and social domains\nwhere latent constructs are theory-driven and measurement validity is\nessential.", "AI": {"tldr": "SE-VAE\u662f\u4e00\u79cd\u65b0\u9896\u7684\u53d8\u5206\u81ea\u7f16\u7801\u5668\u67b6\u6784\uff0c\u901a\u8fc7\u8bbe\u8ba1\u5b9e\u73b0\u6f5c\u5728\u8868\u793a\u7684\u53ef\u89e3\u91ca\u6027\u548c\u89e3\u8026\uff0c\u7279\u522b\u9002\u7528\u4e8e\u79d1\u5b66\u548c\u793e\u4f1a\u9886\u57df\u3002", "motivation": "\u4ece\u8868\u683c\u6570\u636e\u4e2d\u5b66\u4e60\u53ef\u89e3\u91ca\u7684\u6f5c\u5728\u8868\u793a\u662f\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u4e2d\u7684\u4e00\u4e2a\u6311\u6218\u3002", "method": "SE-VAE\u662f\u4e00\u79cd\u65b0\u9896\u7684\u67b6\u6784\uff0c\u5b83\u5c06\u6d4b\u91cf\u7ed3\u6784\u76f4\u63a5\u5d4c\u5165\u53d8\u5206\u81ea\u7f16\u7801\u5668\u7684\u8bbe\u8ba1\u4e2d\u3002\u5b83\u501f\u9274\u4e86\u7ed3\u6784\u65b9\u7a0b\u6a21\u578b\uff0c\u5c06\u6f5c\u5728\u5b50\u7a7a\u95f4\u4e0e\u5df2\u77e5\u7684\u6307\u6807\u5206\u7ec4\u5bf9\u9f50\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u4e2a\u5168\u5c40\u5e72\u6270\u6f5c\u5728\u53d8\u91cf\u6765\u5206\u79bb\u7279\u5b9a\u4e8e\u6784\u9020\u7684\u6df7\u6dc6\u53d8\u5f02\u3002", "result": "SE-VAE\u5728\u56e0\u5b50\u6062\u590d\u3001\u53ef\u89e3\u91ca\u6027\u548c\u5bf9\u5e72\u6270\u53d8\u5f02\u7684\u9c81\u68d2\u6027\u65b9\u9762\u59cb\u7ec8\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002\u6d88\u878d\u7ed3\u679c\u8868\u660e\uff0c\u67b6\u6784\u7ed3\u6784\u662f\u6027\u80fd\u7684\u5173\u952e\u9a71\u52a8\u56e0\u7d20\uff0c\u800c\u4e0d\u662f\u6b63\u5219\u5316\u5f3a\u5ea6\u3002", "conclusion": "SE-VAE\u662f\u4e00\u4e2a\u539f\u5219\u6027\u7684\u767d\u76d2\u751f\u6210\u6a21\u578b\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u6f5c\u5728\u7ed3\u6784\u662f\u7406\u8bba\u9a71\u52a8\u4e14\u6d4b\u91cf\u6709\u6548\u6027\u81f3\u5173\u91cd\u8981\u7684\u79d1\u5b66\u548c\u793e\u4f1a\u9886\u57df\u3002"}}
{"id": "2508.05791", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05791", "abs": "https://arxiv.org/abs/2508.05791", "authors": ["Haoran Li", "Lihao Mai", "Muhao Guo", "Jiaqi Wu", "Yang Weng", "Yannan Sun", "Ce Jimmy Liu"], "title": "From Imperfect Signals to Trustworthy Structure: Confidence-Aware Inference from Heterogeneous and Reliability-Varying Utility Data", "comment": "10 pages", "summary": "Accurate distribution grid topology is essential for reliable modern grid\noperations. However, real-world utility data originates from multiple sources\nwith varying characteristics and levels of quality. In this work, developed in\ncollaboration with Oncor Electric Delivery, we propose a scalable framework\nthat reconstructs a trustworthy grid topology by systematically integrating\nheterogeneous data. We observe that distribution topology is fundamentally\ngoverned by two complementary dimensions: the spatial layout of physical\ninfrastructure (e.g., GIS and asset metadata) and the dynamic behavior of the\nsystem in the signal domain (e.g., voltage time series). When jointly\nleveraged, these dimensions support a complete and physically coherent\nreconstruction of network connectivity. To address the challenge of uneven data\nquality without compromising observability, we introduce a confidence-aware\ninference mechanism that preserves structurally informative yet imperfect\ninputs, while quantifying the reliability of each inferred connection for\noperator interpretation. This soft handling of uncertainty is tightly coupled\nwith hard enforcement of physical feasibility: we embed operational\nconstraints, such as transformer capacity limits and radial topology\nrequirements, directly into the learning process. Together, these components\nensure that inference is both uncertainty-aware and structurally valid,\nenabling rapid convergence to actionable, trustworthy topologies under\nreal-world deployment conditions. The proposed framework is validated using\ndata from over 8000 meters across 3 feeders in Oncor's service territory,\ndemonstrating over 95% accuracy in topology reconstruction and substantial\nimprovements in confidence calibration and computational efficiency relative to\nbaseline methods.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u53ef\u6269\u5c55\u6846\u67b6\uff0c\u6574\u5408GIS\u3001\u8d44\u4ea7\u5143\u6570\u636e\u548c\u7535\u538b\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0c\u901a\u8fc7\u7f6e\u4fe1\u5ea6\u611f\u77e5\u63a8\u7406\u548c\u7269\u7406\u7ea6\u675f\uff0c\u4ee5\u8d85\u8fc795%\u7684\u51c6\u786e\u7387\u91cd\u5efa\u914d\u7535\u7f51\u62d3\u6251\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u5b9e\u4e16\u754c\u4e2d\u516c\u7528\u4e8b\u4e1a\u6570\u636e\u6e90\u591a\u6837\u3001\u8d28\u91cf\u4e0d\u4e00\u7684\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u6846\u67b6\u6765\u91cd\u5efa\u53ef\u4fe1\u7684\u914d\u7535\u7f51\u62d3\u6251\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u7cfb\u7edf\u5730\u6574\u5408\u5f02\u6784\u6570\u636e\u6765\u91cd\u5efa\u53ef\u4fe1\u7684\u7535\u7f51\u62d3\u6251\u3002\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u7269\u7406\u57fa\u7840\u8bbe\u65bd\u7684\u7a7a\u95f4\u5e03\u5c40\uff08\u5982GIS\u548c\u8d44\u4ea7\u5143\u6570\u636e\uff09\u548c\u7cfb\u7edf\u5728\u4fe1\u53f7\u57df\uff08\u5982\u7535\u538b\u65f6\u95f4\u5e8f\u5217\uff09\u7684\u52a8\u6001\u884c\u4e3a\u3002\u5f15\u5165\u4e86\u7f6e\u4fe1\u5ea6\u611f\u77e5\u63a8\u7406\u673a\u5236\u6765\u5904\u7406\u4e0d\u5747\u5300\u7684\u6570\u636e\u8d28\u91cf\uff0c\u5e76\u5d4c\u5165\u4e86\u8bf8\u5982\u53d8\u538b\u5668\u5bb9\u91cf\u9650\u5236\u548c\u5f84\u5411\u62d3\u6251\u8981\u6c42\u7b49\u8fd0\u884c\u7ea6\u675f\u3002", "result": "\u4f7f\u7528Oncor\u670d\u52a1\u533a\u57df\u51858000\u591a\u4e2a\u7535\u8868\u548c3\u4e2a\u9988\u7ebf\u7684\u771f\u5b9e\u6570\u636e\u8fdb\u884c\u9a8c\u8bc1\uff0c\u8bc1\u660e\u4e86\u8be5\u6846\u67b6\u5728\u62d3\u6251\u91cd\u6784\u65b9\u9762\u5177\u6709\u8d85\u8fc795%\u7684\u51c6\u786e\u7387\uff0c\u5e76\u5728\u7f6e\u4fe1\u5ea6\u6821\u51c6\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u7a7a\u95f4\u5e03\u5c40\u548c\u4fe1\u53f7\u57df\u52a8\u6001\u884c\u4e3a\uff0c\u5e76\u5d4c\u5165\u64cd\u4f5c\u7ea6\u675f\uff0c\u5b9e\u73b0\u4e8695%\u4ee5\u4e0a\u51c6\u786e\u7387\u7684\u914d\u7535\u7f51\u62d3\u6251\u91cd\u6784\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u7f6e\u4fe1\u5ea6\u6821\u51c6\u548c\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2508.06055", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2508.06055", "abs": "https://arxiv.org/abs/2508.06055", "authors": ["Wonjung Park", "Suhyun Ahn", "Jinah Park"], "title": "LV-Net: Anatomy-aware lateral ventricle shape modeling with a case study on Alzheimer's disease, the Australian Imaging Biomarkers and Lifestyle flagship study of ageing", "comment": null, "summary": "Lateral ventricle (LV) shape analysis holds promise as a biomarker for\nneurological diseases; however, challenges remain due to substantial shape\nvariability across individuals and segmentation difficulties arising from\nlimited MRI resolution. We introduce LV-Net, a novel framework for producing\nindividualized 3D LV meshes from brain MRI by deforming an anatomy-aware joint\nLV-hippocampus template mesh. By incorporating anatomical relationships\nembedded within the joint template, LV-Net reduces boundary segmentation\nartifacts and improves reconstruction robustness. In addition, by classifying\nthe vertices of the template mesh based on their anatomical adjacency, our\nmethod enhances point correspondence across subjects, leading to more accurate\nLV shape statistics. We demonstrate that LV-Net achieves superior\nreconstruction accuracy, even in the presence of segmentation imperfections,\nand delivers more reliable shape descriptors across diverse datasets. Finally,\nwe apply LV-Net to Alzheimer's disease analysis, identifying LV subregions that\nshow significantly associations with the disease relative to cognitively normal\ncontrols. The codes for LV shape modeling are available at\nhttps://github.com/PWonjung/LV_Shape_Modeling.", "AI": {"tldr": "LV-Net\u662f\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u4e2a\u4f53\u53163D\u5de6\u5fc3\u5ba4\u7f51\u683c\u6765\u5206\u6790\u795e\u7ecf\u7cfb\u7edf\u75be\u75c5\uff0c\u5b83\u901a\u8fc7\u5229\u7528\u89e3\u5256\u5b66\u5173\u7cfb\u548c\u589e\u5f3a\u70b9\u5bf9\u5e94\u6765\u63d0\u9ad8\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\uff0c\u5e76\u5728\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u5206\u6790\u4e2d\u663e\u793a\u51fa\u6709\u524d\u666f\u7684\u7ed3\u679c\u3002", "motivation": "\u89e3\u51b3\u795e\u7ecf\u7cfb\u7edf\u75be\u75c5\u4e2d\u5de6\u5fc3\u5ba4\u5f62\u72b6\u5206\u6790\u7684\u6311\u6218\uff0c\u5305\u62ec\u4e2a\u4f53\u95f4\u7684\u5f62\u72b6\u53d8\u5f02\u6027\u548cMRI\u5206\u8fa8\u7387\u6709\u9650\u5bfc\u81f4\u7684\u5206\u5272\u56f0\u96be\u3002", "method": "\u901a\u8fc7\u4e00\u4e2a\u89e3\u5256\u5b66\u9a71\u52a8\u7684\u8054\u5408\u6a21\u578b\u751f\u6210\u4e2a\u4f53\u53163D\u5de6\u5fc3\u5ba4\u7f51\u683c\uff0c\u5e76\u5bf9\u6a21\u677f\u7f51\u683c\u7684\u9876\u70b9\u8fdb\u884c\u5206\u7c7b\u4ee5\u589e\u5f3a\u70b9\u5bf9\u5e94\uff0c\u4ece\u800c\u63d0\u9ad8\u91cd\u5efa\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "result": "LV-Net\u5728\u5b58\u5728\u5206\u5272\u7f3a\u9677\u7684\u60c5\u51b5\u4e0b\u4ecd\u80fd\u5b9e\u73b0\u4f18\u8d8a\u7684\u91cd\u5efa\u7cbe\u5ea6\uff0c\u5e76\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u5f62\u72b6\u7edf\u8ba1\u6570\u636e\u3002", "conclusion": "LV-Net\u5728\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u5206\u6790\u4e2d\u80fd\u591f\u8bc6\u522b\u51fa\u4e0e\u8be5\u75be\u75c5\u663e\u8457\u76f8\u5173\u7684LV\u4e9a\u533a\u57df\uff0c\u5e76\u80fd\u63d0\u4f9b\u6bd4\u4f20\u7edf\u65b9\u6cd5\u66f4\u53ef\u9760\u7684\u5f62\u72b6\u63cf\u8ff0\u7b26\u3002"}}
{"id": "2508.05968", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2508.05968", "abs": "https://arxiv.org/abs/2508.05968", "authors": ["Takashi U. Ito", "Ryosuke Kadono"], "title": "Revisiting $\u03bc$SR Studies of Ion Dynamics in the Light of Extended Kubo-Toyabe Model", "comment": "6 pages, 3 figures", "summary": "The dynamical Kubo-Toyabe (dKT) function is extended to describe the spin\nrelaxation under the coexisting dynamical and static internal magnetic fields.\nA detailed re-evaluation of the previous $\\mu^\\pm$SR data in Na$_x$CoO$_2$\nusing this function disfavors the conventional interpretation based on\nsodium-ion diffusion and instead supports the $\\mu^+$ self-diffusion scenario.\nThis also resolves the long-standing inconsistencies in the dKT-function-based\n$\\mu$SR studies on ion diffusion from the viewpoint of classical\nover-barrier-jump mechanism.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u6539\u8fdbdKT\u51fd\u6570\u6a21\u578b\uff0c\u4e3aNa_xCoO_2\u7684\u03bc\u00b1SR\u6570\u636e\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u91ca\uff0c\u652f\u6301\u03bc+\u81ea\u6269\u6563\u800c\u975e\u94a0\u79bb\u5b50\u6269\u6563\uff0c\u5e76\u89e3\u51b3\u4e86\u8be5\u9886\u57df\u7814\u7a76\u4e2d\u7684\u4e00\u4e9b\u4e89\u8bae\u3002", "motivation": "\u4e3a\u4e86\u63cf\u8ff0\u5728\u52a8\u6001\u548c\u9759\u6001\u5185\u78c1\u573a\u5171\u5b58\u4e0b\u7684\u81ea\u65cb\u5f1b\u8c6b\uff0c\u5e76\u89e3\u51b3dKT\u51fd\u6570\u5728\u79bb\u5b50\u6269\u6563\u03bcSR\u7814\u7a76\u4e2d\u5b58\u5728\u7684\u957f\u671f\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u6269\u5c55\u52a8\u529b\u5b66Kubo-Toyabe\uff08dKT\uff09\u51fd\u6570\u6765\u63cf\u8ff0\u81ea\u65cb\u5f1b\u8c6b\uff0c\u5e76\u5229\u7528\u8be5\u51fd\u6570\u5bf9Na_xCoO_2\u7684\u03bc\u00b1SR\u6570\u636e\u8fdb\u884c\u4e86\u8be6\u7ec6\u7684\u91cd\u65b0\u8bc4\u4f30\u3002", "result": "\u91cd\u65b0\u8bc4\u4f30Na_xCoO_2\u7684\u03bc\u00b1SR\u6570\u636e\u4e0d\u652f\u6301\u4f20\u7edf\u7684\u94a0\u79bb\u5b50\u6269\u6563\u89e3\u91ca\uff0c\u53cd\u800c\u652f\u6301\u03bc+\u81ea\u6269\u6563\u7406\u8bba\uff0c\u4ece\u7ecf\u5178\u8d8a\u5792\u8df3\u8dc3\u673a\u5236\u7684\u89d2\u5ea6\u89e3\u51b3\u4e86dKT\u51fd\u6570\u5728\u79bb\u5b50\u6269\u6563\u03bcSR\u7814\u7a76\u4e2d\u5b58\u5728\u7684\u957f\u671f\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u6269\u5c55\u52a8\u529b\u5b66Kubo-Toyabe\uff08dKT\uff09\u51fd\u6570\u6765\u63cf\u8ff0\u5728\u52a8\u6001\u548c\u9759\u6001\u5185\u78c1\u573a\u5171\u5b58\u4e0b\u7684\u81ea\u65cb\u5f1b\u8c6b\uff0c\u5e76\u91cd\u65b0\u8bc4\u4f30\u4e86Na_xCoO_2\u7684\u03bc\u00b1SR\u6570\u636e\uff0c\u7ed3\u679c\u4e0d\u652f\u6301\u4f20\u7edf\u7684\u94a0\u79bb\u5b50\u6269\u6563\u89e3\u91ca\uff0c\u53cd\u800c\u652f\u6301\u03bc+\u81ea\u6269\u6563\u7406\u8bba\uff0c\u89e3\u51b3\u4e86dKT\u51fd\u6570\u5728\u79bb\u5b50\u6269\u6563\u03bcSR\u7814\u7a76\u4e2d\u5b58\u5728\u7684\u957f\u671f\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u5e76\u4ece\u7ecf\u5178\u8d8a\u5792\u8df3\u8dc3\u673a\u5236\u7684\u89d2\u5ea6\u8fdb\u884c\u4e86\u9610\u91ca\u3002"}}
{"id": "2508.06286", "categories": ["physics.app-ph", "nlin.CD"], "pdf": "https://arxiv.org/pdf/2508.06286", "abs": "https://arxiv.org/abs/2508.06286", "authors": ["Shuaifeng Li", "Di Zhou", "Feng Li", "Panayotis G. Kevrekidis", "Jinkyu Yang"], "title": "Topological edge states and amplitude-dependent delocalization in quasiperiodic elliptically geared lattices", "comment": "10 Pages, 6 Figures", "summary": "We present a class of mechanical lattices based on elliptical gears with\nquasiperiodic modulation and geometric nonlinearity, capable of exhibiting\ntopologically protected modes and amplitude-driven transitions. Starting from a\none-dimensional chain of modulated elliptical gears, we demonstrate the\nemergence of localized edge states arising from quasiperiodic variation in the\ngears' moments of inertia, analogous to the topological edge modes of the\nAubry-Andre-Harper model. Under increasing excitation amplitude, the system\nundergoes a nonlinear transition, where edge localization breaks down and\nenergy delocalizes into the bulk. By coupling multiple such chains with varying\nmodulation phase, we construct a two-dimensional lattice in which the phase\nacts as a synthetic dimension. This structure supports topological wave\npropagation along the synthetic dimension. Nonlinearity again induces a\nbreakdown of topological states, leading to complex, amplitude-dependent wave\npropagation. We further propose a numerical continuation approach to analyzing\nthe periodic orbits and their linear stability, effectively discovering the\nboundary of the basin of bounded motion and detecting the occurrence of\ndelocalization under certain excitation amplitudes. Our results reveal that\nelliptical geared systems offer a passive, amplitude-dependent platform for\nexploring topological phenomena and synthetic dimensionality in mechanical\nmetamaterials.", "AI": {"tldr": "Mechanical lattices based on modulated elliptical gears exhibit topological edge states and amplitude-driven transitions, with applications in metamaterials and synthetic dimensions.", "motivation": "explore topological phenomena and synthetic dimensionality in mechanical metamaterials using elliptical gears.", "method": "numerical continuation approach to analyzing the periodic orbits and their linear stability, effectively discovering the boundary of the basin of bounded motion and detecting the occurrence of delocalization under certain excitation amplitudes.", "result": "demonstrate the emergence of localized edge states arising from quasiperiodic variation in the gears' moments of inertia, analogous to the topological edge modes of the Aubry-Andre-Harper model. Under increasing excitation amplitude, the system undergoes a nonlinear transition, where edge localization breaks down and energy delocalizes into the bulk. Construct a two-dimensional lattice in which the phase acts as a synthetic dimension. This structure supports topological wave propagation along the synthetic dimension. Nonlinearity again induces a breakdown of topological states, leading to complex, amplitude-dependent wave propagation.", "conclusion": "elliptical geared systems offer a passive, amplitude-dependent platform for exploring topological phenomena and synthetic dimensionality in mechanical metamaterials."}}
{"id": "2508.06469", "categories": ["cs.GT", "econ.TH"], "pdf": "https://arxiv.org/pdf/2508.06469", "abs": "https://arxiv.org/abs/2508.06469", "authors": ["Jason Hartline", "Kangning Wang"], "title": "A Geometric Analysis of Gains from Trade", "comment": null, "summary": "We provide a geometric proof that the random proposer mechanism is a\n$4$-approximation to the first-best gains from trade in bilateral exchange. We\nthen refine this geometric analysis to recover the state-of-the-art\napproximation ratio of $3.15$.", "AI": {"tldr": "\u6587\u7ae0\u901a\u8fc7\u51e0\u4f55\u65b9\u6cd5\u7814\u7a76\u4e86\u968f\u673a\u63d0\u8bae\u8005\u673a\u5236\u5728\u53cc\u8fb9\u4ea4\u6613\u4e2d\u7684\u6548\u7387\uff0c\u5f97\u5230\u4e864\u8fd1\u4f3c\u6700\u4f18\u4ea4\u6613\u6536\u76ca\uff0c\u5e76\u8fdb\u4e00\u6b65\u4f18\u5316\u52303.15\u8fd1\u4f3c\u3002", "motivation": "\u63a2\u7a76\u968f\u673a\u63d0\u8bae\u8005\u673a\u5236\u5728\u53cc\u8fb9\u4ea4\u6613\u4e2d\u7684\u6548\u7387\uff0c\u5e76\u5bfb\u6c42\u6700\u4f18\u7684\u8fd1\u4f3c\u6bd4\u3002", "method": "\u6587\u7ae0\u9996\u5148\u4f7f\u7528\u51e0\u4f55\u65b9\u6cd5\u8bc1\u660e\u4e86\u968f\u673a\u63d0\u8bae\u8005\u673a\u5236\u662f\u53cc\u8fb9\u4ea4\u6613\u4e2d\u201c\u6700\u4f18\u4ea4\u6613\u6536\u76ca\u201d\u76844\u8fd1\u4f3c\uff0c\u7136\u540e\u901a\u8fc7\u7ec6\u5316\u8be5\u51e0\u4f55\u5206\u6790\uff0c\u5f97\u5230\u4e86\u76ee\u524d\u6700\u4f18\u76843.15\u8fd1\u4f3c\u6bd4\u3002", "result": "\u968f\u673a\u63d0\u8bae\u8005\u673a\u5236\u88ab\u8bc1\u660e\u662f4\u8fd1\u4f3c\u6700\u4f18\u4ea4\u6613\u6536\u76ca\uff0c\u5e76\u4e14\u901a\u8fc7\u51e0\u4f55\u5206\u6790\u53ef\u4ee5\u8fbe\u52303.15\u8fd1\u4f3c\u6700\u4f18\u4ea4\u6613\u6536\u76ca\u3002", "conclusion": "\u968f\u673a\u63d0\u8bae\u8005\u673a\u5236\u53ef\u4ee5\u5b9e\u73b04\u8fd1\u4f3c\u6700\u4f18\u4ea4\u6613\u6536\u76ca\uff0c\u5e76\u4e14\u901a\u8fc7\u51e0\u4f55\u5206\u6790\u53ef\u4ee5\u8fdb\u4e00\u6b65\u4f18\u5316\u52303.15\u8fd1\u4f3c\u6700\u4f18\u4ea4\u6613\u6536\u76ca\u3002"}}
{"id": "2508.06061", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2508.06061", "abs": "https://arxiv.org/abs/2508.06061", "authors": ["Ainur Zhaikhan", "Malek Khammassi", "Ali H. Sayed"], "title": "Policy Optimization in Multi-Agent Settings under Partially Observable Environments", "comment": null, "summary": "This work leverages adaptive social learning to estimate partially observable\nglobal states in multi-agent reinforcement learning (MARL) problems. Unlike\nexisting methods, the proposed approach enables the concurrent operation of\nsocial learning and reinforcement learning. Specifically, it alternates between\na single step of social learning and a single step of MARL, eliminating the\nneed for the time- and computation-intensive two-timescale learning frameworks.\nTheoretical guarantees are provided to support the effectiveness of the\nproposed method. Simulation results verify that the performance of the proposed\nmethodology can approach that of reinforcement learning when the true state is\nknown.", "AI": {"tldr": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5229\u7528\u81ea\u9002\u5e94\u793e\u4f1a\u5b66\u4e60\u6765\u4f30\u8ba1MARL\u95ee\u9898\u4e2d\u90e8\u5206\u53ef\u89c2\u5bdf\u7684\u5168\u5c40\u72b6\u6001\uff0c\u901a\u8fc7\u4ea4\u66ff\u8fdb\u884c\u793e\u4f1a\u5b66\u4e60\u548cMARL\u6765\u5b9e\u73b0\uff0c\u907f\u514d\u4e86\u8017\u65f6\u7684\u4e24\u65f6\u95f4\u5c3a\u5ea6\u5b66\u4e60\u6846\u67b6\u3002", "motivation": "\u5229\u7528\u81ea\u9002\u5e94\u793e\u4f1a\u5b66\u4e60\u6765\u4f30\u8ba1MARL\u95ee\u9898\u4e2d\u90e8\u5206\u53ef\u89c2\u5bdf\u7684\u5168\u5c40\u72b6\u6001\u3002", "method": "\u901a\u8fc7\u4ea4\u66ff\u8fdb\u884c\u793e\u4f1a\u5b66\u4e60\u548c\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u6765\u5b9e\u73b0\uff0c\u907f\u514d\u4e86\u8017\u65f6\u7684\u4e24\u65f6\u95f4\u5c3a\u5ea6\u5b66\u4e60\u6846\u67b6\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u6a21\u62df\u7ed3\u679c\u4e2d\u5f97\u5230\u4e86\u9a8c\u8bc1\uff0c\u5176\u6027\u80fd\u63a5\u8fd1\u4e8e\u5df2\u77e5\u771f\u5b9e\u72b6\u6001\u7684\u5f3a\u5316\u5b66\u4e60\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u53ef\u4ee5\u901a\u8fc7\u4e0e\u5df2\u77e5\u771f\u5b9e\u72b6\u6001\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u76f8\u5ab2\u7f8e\u7684\u6027\u80fd\u6765\u9a8c\u8bc1\u3002"}}
{"id": "2508.06478", "categories": ["cs.DS", "cs.CC", "math.GR"], "pdf": "https://arxiv.org/pdf/2508.06478", "abs": "https://arxiv.org/abs/2508.06478", "authors": ["Dan Johnson", "Michael Levet", "Petr Vojt\u011bchovsk\u00fd", "Brett Widholm"], "title": "On the Parallel Complexity of Identifying Groups and Quasigroups via Decompositions", "comment": null, "summary": "In this paper, we investigate the computational complexity of isomorphism\ntesting for finite groups and quasigroups, given by their multiplication\ntables. We crucially take advantage of their various decompositions to show the\nfollowing:\n  - We first consider the class $\\mathcal{C}$ of groups that admit direct\nproduct decompositions, where each indecompsable factor is $O(1)$-generated,\nand either perfect or centerless. We show any group in $\\mathcal{C}$ is\nidentified by the $O(1)$-dimensional count-free Weisfeiler--Leman (WL)\nalgorithm with $O(\\log \\log n)$ rounds, and the $O(1)$-dimensional counting WL\nalgorithm with $O(1)$ rounds. Consequently, the isomorphism problem for\n$\\mathcal{C}$ is in $\\textsf{L}$. The previous upper bound for this class was\n$\\textsf{TC}^{1}$, using $O(\\log n)$ rounds of the $O(1)$-dimensional counting\nWL (Grochow and Levet, FCT 2023).\n  - We next consider more generally, the class of groups where each\nindecomposable factor is $O(1)$-generated. We exhibit an $\\textsf{AC}^{3}$\ncanonical labeling procedure for this class. Here, we accomplish this by\nshowing that in the multiplication table model, the direct product\ndecomposition can be computed in $\\textsf{AC}^{3}$, parallelizing the work of\nKayal and Nezhmetdinov (ICALP 2009).\n  - Isomorphism testing between a central quasigroup $G$ and an arbitrary\nquasigroup $H$ is in $\\textsf{NC}$. Here, we take advantage of the fact that\ncentral quasigroups admit an affine decomposition in terms of an underlying\nAbelian group. Only the trivial bound of $n^{\\log(n)+O(1)}$-time was previously\nknown for isomorphism testing of central quasigroups.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u7fa4\u548c\u62df\u7fa4\u7684\u5206\u89e3\u6027\u8d28\uff0c\u5728\u8ba1\u7b97\u590d\u6742\u6027\u65b9\u9762\u6539\u8fdb\u4e86\u540c\u6784\u6d4b\u8bd5\u7684\u754c\u9650\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u6709\u9650\u7fa4\u548c\u62df\u7fa4\u7684\u540c\u6784\u6d4b\u8bd5\u7684\u8ba1\u7b97\u590d\u6742\u6027\uff0c\u7279\u522b\u662f\u5229\u7528\u5b83\u4eec\u7684\u5206\u89e3\u7ed3\u6784\u6765\u6539\u8fdb\u5df2\u77e5\u7684\u4e0a\u754c\u3002", "method": "\u672c\u7814\u7a76\u5229\u7528\u4e86\u7fa4\u548c\u62df\u7fa4\u7684\u5404\u79cd\u5206\u89e3\u6027\u8d28\u3002\u5bf9\u4e8e\u5177\u6709\u76f4\u63a5\u79ef\u5206\u89e3\u7684\u7fa4\uff0c\u4f7f\u7528\u4e86 O(1) \u7ef4\u5ea6\u7684\u8ba1\u6570 Weisfeiler-Leman (WL) \u7b97\u6cd5\uff0c\u5e76\u5229\u7528\u4e86 O(1) \u7ef4\u5ea6\u7684\u8ba1\u6570 WL \u7b97\u6cd5\u3002\u5bf9\u4e8e\u5206\u89e3\u56e0\u5b50\u4e3a O(1) \u751f\u6210\u7684\u7fa4\uff0c\u901a\u8fc7\u5728\u4e58\u6cd5\u8868\u6a21\u578b\u4e2d\u8ba1\u7b97\u76f4\u63a5\u79ef\u5206\u89e3\uff0c\u5b9e\u73b0\u4e86 AC^3 \u89c4\u8303\u6807\u8bb0\u8fc7\u7a0b\u3002\u5bf9\u4e8e\u4e2d\u5fc3\u62df\u7fa4\uff0c\u5229\u7528\u4e86\u5176\u4eff\u5c04\u5206\u89e3\u6027\u8d28\uff0c\u5c06\u5176\u4e0e\u6f5c\u5728\u7684\u963f\u8d1d\u5c14\u7fa4\u8054\u7cfb\u8d77\u6765\u3002", "result": "\u5bf9\u4e8e\u5177\u6709\u76f4\u63a5\u79ef\u5206\u89e3\u4e14\u5206\u89e3\u56e0\u5b50\u6ee1\u8db3\u7279\u5b9a\u6761\u4ef6\u7684\u7fa4\uff08$\textsf{C}$ \u7c7b\uff09\uff0c\u540c\u6784\u95ee\u9898\u5728 $\textsf{L}$ \u4e2d\uff0c\u6bd4\u4e4b\u524d\u4f7f\u7528 $\textsf{TC}^1$ \u7684\u4e0a\u754c\u6709\u6240\u6539\u8fdb\u3002\u5bf9\u4e8e\u5206\u89e3\u56e0\u5b50\u4e3a $O(1)$-\u751f\u6210\u7684\u7fa4\uff0c\u5b58\u5728\u4e00\u4e2a $\textsf{AC}^3$ \u89c4\u8303\u6807\u8bb0\u8fc7\u7a0b\u3002\u5bf9\u4e8e\u4e2d\u5fc3\u62df\u7fa4\u4e0e\u4efb\u610f\u62df\u7fa4\u7684\u540c\u6784\u6d4b\u8bd5\uff0c\u5728 $\textsf{NC}$ \u4e2d\uff0c\u663e\u8457\u4f18\u4e8e\u4e4b\u524d\u5df2\u77e5\u7684 $n^{\text{log}(n)+O(1)}$ \u65f6\u95f4\u754c\u3002", "conclusion": "\u672c\u7814\u7a76\u8868\u660e\uff0c\u5bf9\u4e8e\u5177\u6709\u7279\u5b9a\u5206\u89e3\u6027\u8d28\u7684\u6709\u9650\u7fa4\u548c\u62df\u7fa4\uff0c\u5b83\u4eec\u7684\u540c\u6784\u95ee\u9898\u53ef\u4ee5\u5728\u8f83\u4f4e\u7684\u8ba1\u7b97\u590d\u6742\u6027\u7c7b\u522b\u4e2d\u89e3\u51b3\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u5bf9\u4e8e\u5177\u6709\u76f4\u63a5\u79ef\u5206\u89e3\u7684\u7fa4\uff0c\u5176\u540c\u6784\u95ee\u9898\u5728 L \u4e2d\uff1b\u5bf9\u4e8e\u5206\u89e3\u56e0\u5b50\u4e3a O(1) \u751f\u6210\u7684\u7fa4\uff0c\u5b58\u5728 AC^3 \u89c4\u8303\u6807\u8bb0\u8fc7\u7a0b\uff1b\u5bf9\u4e8e\u4e2d\u5fc3\u62df\u7fa4\uff0c\u5176\u540c\u6784\u6d4b\u8bd5\u5728 NC \u4e2d\u3002"}}
{"id": "2508.06037", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.06037", "abs": "https://arxiv.org/abs/2508.06037", "authors": ["Tien Ngoc Ha", "Daniel Romero"], "title": "Bayesian Radio Map Estimation: Fundamentals and Implementation via Diffusion Models", "comment": null, "summary": "Radio map estimation (RME) is the problem of inferring the value of a certain\nmetric (e.g. signal power) across an area of interest given a collection of\nmeasurements. While most works tackle this problem from a purely non-Bayesian\nperspective, some Bayesian estimators have been proposed. However, the latter\nfocus on estimating the map itself, the Bayesian standpoint is adopted mainly\nto exploit prior information or to capture uncertainty. This paper pursues a\nmore general formulation, where the goal is to determine the posterior\ndistribution of the map given the measurements. Besides handling uncertainty\nand allowing standard Bayesian estimates, solving this problem is seen to\nenable minimum mean square error estimation of arbitrary map functionals (e.g.\ncapacity, bit error rate, or coverage area to name a few) while training only\nfor power estimation. A general Bayesian estimator is proposed based on\nconditional diffusion models and both the Bayesian and non-Bayesian paradigms\nare compared analytically and numerically to determine when the Bayesian\napproach is preferable.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8d1d\u53f6\u65af\u65b9\u6cd5\u6765\u4f30\u8ba1\u65e0\u7ebf\u7535\u5730\u56fe\uff0c\u8be5\u65b9\u6cd5\u53ef\u4ee5\u5904\u7406\u4e0d\u786e\u5b9a\u6027\u5e76\u5b9e\u73b0\u66f4\u7cbe\u786e\u7684\u4f30\u8ba1\uff0c\u540c\u65f6\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4ec5\u8fdb\u884c\u529f\u7387\u4f30\u8ba1\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u65e0\u7ebf\u7535\u5730\u56fe\u4f30\u8ba1\uff08RME\uff09\u95ee\u9898\uff0c\u5e76\u8ffd\u6c42\u66f4\u901a\u7528\u7684\u95ee\u9898\u8bbe\u5b9a\uff0c\u5373\u5728\u7ed9\u5b9a\u6d4b\u91cf\u503c\u7684\u60c5\u51b5\u4e0b\u786e\u5b9a\u5730\u56fe\u7684\u540e\u9a8c\u5206\u5e03\uff0c\u540c\u65f6\u5904\u7406\u4e0d\u786e\u5b9a\u6027\u548c\u5b9e\u73b0\u4efb\u610f\u5730\u56fe\u6cdb\u51fd\u7684\u6700\u5c0f\u5747\u65b9\u8bef\u5dee\u4f30\u8ba1\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6761\u4ef6\u6269\u6563\u6a21\u578b\u7684\u901a\u7528\u8d1d\u53f6\u65af\u4f30\u8ba1\u5668\uff0c\u7528\u4e8e\u63a8\u65ad\u5730\u56fe\u7684\u540e\u9a8c\u5206\u5e03\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5b9e\u73b0\u6700\u5c0f\u5747\u65b9\u8bef\u5dee\u4f30\u8ba1\uff0c\u5e76\u4e14\u53ef\u4ee5\u5904\u7406\u4e0d\u786e\u5b9a\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6761\u4ef6\u6269\u6563\u6a21\u578b\u7684\u901a\u7528\u8d1d\u53f6\u65af\u4f30\u8ba1\u5668\uff0c\u5e76\u4ece\u8d1d\u53f6\u65af\u548c\u975e\u8d1d\u53f6\u65af\u4e24\u4e2a\u8303\u5f0f\u5bf9\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u8fdb\u884c\u4e86\u89e3\u6790\u548c\u6570\u503c\u6bd4\u8f83\uff0c\u4ee5\u786e\u5b9a\u8d1d\u53f6\u65af\u65b9\u6cd5\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2508.06024", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.06024", "abs": "https://arxiv.org/abs/2508.06024", "authors": ["Zheming Yang", "Yunqing Hu", "Sheng Sun", "Wen Ji"], "title": "EC2MoE: Adaptive End-Cloud Pipeline Collaboration Enabling Scalable Mixture-of-Experts Inference", "comment": "9 pages, 8 figures", "summary": "The Mixture-of-Experts (MoE) paradigm has emerged as a promising solution to\nscale up model capacity while maintaining inference efficiency. However,\ndeploying MoE models across heterogeneous end-cloud environments poses new\nchallenges in expert scheduling, communication overhead, and resource\nheterogeneity. In this paper, we propose EC2MoE, an adaptive framework for\nscalable MoE inference via end-cloud pipeline collaboration. First, we design a\nhardware-aware lightweight group gate network that enhances expert selection\nand computational efficiency. By incorporating a hardware-aware local expert\nselection mechanism, the system adaptively filters candidate experts based on\nreal-time device profiles. A lightweight group gate module then integrates\nlocal and global gating outputs to achieve high-quality expert routing with\nminimal overhead. Second, we develop a pipeline optimization mechanism based on\nendcloud collaboration to accelerate MoE inference. This includes an\nencoder-decoder structure based on low-rank compression, which reduces\ntransmission and computation costs. And a route-aware heuristic pipeline\nscheduling algorithm that dynamically allocates inference stages across devices\naccording to workload and network topology. Extensive experiments show that\nEC2MoE can increase throughput by 2.2x to 5.1x and reduce end-to-end latency by\n53% to 67% while maintaining high accuracy compared to state-of-the-art\nmethods. It also maintains good scalability under dynamic load and network\nenvironments.", "AI": {"tldr": "EC2MoE\u901a\u8fc7\u7aef\u4e91\u534f\u540c\u6d41\u6c34\u7ebf\u5316MoE\u63a8\u7406\uff0c\u63d0\u9ad8\u6548\u7387\u5e76\u964d\u4f4e\u5ef6\u8fdf\u3002", "motivation": "\u65e8\u5728\u89e3\u51b3\u5728\u5f02\u6784\u7aef\u4e91\u73af\u5883\u4e2d\u90e8\u7f72MoE\u6a21\u578b\u65f6\u9047\u5230\u7684\u4e13\u5bb6\u8c03\u5ea6\u3001\u901a\u4fe1\u5f00\u9500\u548c\u8d44\u6e90\u5f02\u6784\u6027\u7b49\u6311\u6218\uff0c\u4ee5\u63d0\u9ad8\u6a21\u578b\u5bb9\u91cf\u548c\u63a8\u7406\u6548\u7387\u3002", "method": "EC2MoE\u6846\u67b6\u63d0\u51fa\u4e86\u4e00\u79cd\u786c\u4ef6\u611f\u77e5\u7684\u8f7b\u91cf\u7ea7\u5206\u7ec4\u95e8\u63a7\u7f51\u7edc\uff0c\u901a\u8fc7\u672c\u5730\u4e13\u5bb6\u9009\u62e9\u548c\u5168\u5c40\u95e8\u63a7\u4f18\u5316\u4e13\u5bb6\u8def\u7531\uff1b\u540c\u65f6\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u7aef\u4e91\u534f\u540c\u7684\u6d41\u6c34\u7ebf\u4f18\u5316\u673a\u5236\uff0c\u5305\u62ec\u4f4e\u79e9\u538b\u7f29\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7ed3\u6784\u548c\u8def\u7531\u611f\u77e5\u7684\u542f\u53d1\u5f0f\u6d41\u6c34\u7ebf\u8c03\u5ea6\u7b97\u6cd5\u3002", "result": "EC2MoE\u6846\u67b6\u53ef\u5c06\u541e\u5410\u91cf\u63d0\u9ad82.2\u500d\u81f35.1\u500d\uff0c\u5e76\u5c06\u7aef\u5230\u7aef\u5ef6\u8fdf\u964d\u4f4e53%\u81f367%\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u7cbe\u5ea6\uff0c\u5e76\u4e14\u5728\u52a8\u6001\u8d1f\u8f7d\u548c\u7f51\u7edc\u73af\u5883\u4e0b\u5177\u6709\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "EC2MoE\u6846\u67b6\u901a\u8fc7\u7aef\u4e91\u534f\u540c\u6d41\u6c34\u7ebf\u5316\u5904\u7406\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5728\u5f02\u6784\u7aef\u4e91\u73af\u5883\u4e2d\u90e8\u7f72MoE\u6a21\u578b\u9762\u4e34\u7684\u4e13\u5bb6\u8c03\u5ea6\u3001\u901a\u4fe1\u5f00\u9500\u548c\u8d44\u6e90\u5f02\u6784\u6027\u6311\u6218\uff0c\u5728\u63d0\u9ad8\u541e\u5410\u91cf\u548c\u964d\u4f4e\u5ef6\u8fdf\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u4fdd\u6301\u4e86\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2508.06180", "categories": ["cond-mat.mes-hall", "cond-mat.supr-con"], "pdf": "https://arxiv.org/pdf/2508.06180", "abs": "https://arxiv.org/abs/2508.06180", "authors": ["S. C. ten Kate", "D. C. Ohnmacht", "M. Coraiola", "T. Antonelli", "S. Paredes", "F. J. Schupp", "M. Hinderling", "S. W. Bedell", "W. Belzig", "J. C. Cuevas", "A. E. Svetogorov", "F. Nichele", "D. Sabonis"], "title": "Finite Length Effects and Coulomb Interaction in Ge Quantum Well-Based Josephson Junctions Probed with Microwave Spectroscopy", "comment": null, "summary": "Proximitized Ge quantum wells have emerged as a novel platform for studying\nAndreev bound states (ABSs), due to their expected strong spin-orbit\ninteraction and high mobility. Here, we used microwave spectroscopy techniques\nto investigate ABSs in Josephson junctions (JJs) realized in proximitized Ge\nquantum wells. Spectroscopic signatures observed in a 350 nm junction indicated\nthe presence of multiple ABSs, and were reproduced with a model including\nfinite-length effects. The ABS spectra measured for a $1.2~\\mu$m junction were\nexplained by a model including three ABSs in two conduction channels and finite\nCoulomb interaction. Our work highlights the importance of interactions in JJs\nand serves as a basis for understanding and manipulating ABSs in Ge-based\nhybrid devices.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.05720", "categories": ["quant-ph", "cs.CC", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.05720", "abs": "https://arxiv.org/abs/2508.05720", "authors": ["Hsin-Yuan Huang", "Soonwon Choi", "Jarrod R. McClean", "John Preskill"], "title": "The vast world of quantum advantage", "comment": "17 page perspective, 1 figure + 16 page appendix", "summary": "The quest to identify quantum advantages lies at the heart of quantum\ntechnology. While quantum devices promise extraordinary capabilities, from\nexponential computational speedups to unprecedented measurement precision,\ndistinguishing genuine advantages from mere illusions remains a formidable\nchallenge. In this endeavor, quantum theorists are like prophets attempting to\nforetell the future, yet the boundary between visionary insight and unfounded\nfantasy is perilously thin. In this perspective, we examine our mathematical\ntools for navigating the vast world of quantum advantages across computation,\nlearning, sensing, and communication. We explore five keystone properties:\npredictability, typicality, robustness, verifiability, and usefulness that\ndefine an ideal quantum advantage, and envision what new quantum advantages\ncould arise in a future with ubiquitous quantum technology. We prove that some\nquantum advantages are inherently unpredictable using classical resources\nalone, suggesting a landscape far richer than what we can currently foresee.\nWhile mathematical rigor remains our indispensable guide, the ultimate power of\nquantum technologies may emerge from advantages we cannot yet conceive.", "AI": {"tldr": "\u533a\u5206\u91cf\u5b50\u4f18\u52bf\u7684\u771f\u4f2a\u81f3\u5173\u91cd\u8981\u3002\u672c\u6587\u63a2\u8ba8\u4e86\u5b9a\u4e49\u91cf\u5b50\u4f18\u52bf\u7684\u4e94\u4e2a\u5173\u952e\u5c5e\u6027\uff08\u53ef\u9884\u6d4b\u6027\u3001\u5178\u578b\u6027\u3001\u9c81\u68d2\u6027\u3001\u53ef\u9a8c\u8bc1\u6027\u548c\u6709\u7528\u6027\uff09\uff0c\u5e76\u9884\u6d4b\u4e86\u672a\u6765\u7684\u91cf\u5b50\u4f18\u52bf\u3002\u7814\u7a76\u8868\u660e\uff0c\u4e00\u4e9b\u91cf\u5b50\u4f18\u52bf\u65e0\u6cd5\u4ec5\u7528\u7ecf\u5178\u65b9\u6cd5\u9884\u6d4b\uff0c\u91cf\u5b50\u6280\u672f\u7684\u771f\u6b63\u6f5c\u529b\u53ef\u80fd\u8d85\u4e4e\u6211\u4eec\u7684\u60f3\u8c61\u3002", "motivation": "\u8bc6\u522b\u91cf\u5b50\u4f18\u52bf\u662f\u91cf\u5b50\u6280\u672f\u7684\u5173\u952e\u3002\u867d\u7136\u91cf\u5b50\u8bbe\u5907\u6709\u671b\u63d0\u4f9b\u975e\u51e1\u7684\u80fd\u529b\uff0c\u4f46\u533a\u5206\u771f\u6b63\u7684\u91cf\u5b50\u4f18\u52bf\u548c\u865a\u5e7b\u7684\u91cf\u5b50\u4f18\u52bf\u4ecd\u7136\u662f\u4e00\u4e2a\u4e25\u5cfb\u7684\u6311\u6218\u3002", "method": "\u672c\u6587\u6211\u4eec\u5ba1\u89c6\u4e86\u6211\u4eec\u5728\u8ba1\u7b97\u3001\u5b66\u4e60\u3001\u4f20\u611f\u548c\u901a\u4fe1\u9886\u57df\u4e2d\u7528\u4e8e\u63a2\u7d22\u91cf\u5b50\u4f18\u52bf\u7684\u6570\u5b66\u5de5\u5177\u3002\u6211\u4eec\u63a2\u7d22\u4e86\u4e94\u4e2a\u5173\u952e\u5c5e\u6027\uff1a\u53ef\u9884\u6d4b\u6027\u3001\u5178\u578b\u6027\u3001\u9c81\u68d2\u6027\u3001\u53ef\u9a8c\u8bc1\u6027\u548c\u6709\u7528\u6027\uff0c\u8fd9\u4e9b\u5c5e\u6027\u5b9a\u4e49\u4e86\u7406\u60f3\u7684\u91cf\u5b50\u4f18\u52bf\uff0c\u5e76\u8bbe\u60f3\u4e86\u5728\u666e\u53ca\u7684\u91cf\u5b50\u6280\u672f\u65f6\u4ee3\u53ef\u80fd\u51fa\u73b0\u7684\u65b0\u578b\u91cf\u5b50\u4f18\u52bf\u3002", "result": "\u6211\u4eec\u8bc1\u660e\u4e86\u4e00\u4e9b\u91cf\u5b50\u4f18\u52bf\u672c\u8d28\u4e0a\u662f\u65e0\u6cd5\u4ec5\u4f7f\u7528\u7ecf\u5178\u8d44\u6e90\u6765\u9884\u6d4b\u7684\uff0c\u8fd9\u8868\u660e\u5176\u524d\u666f\u6bd4\u6211\u4eec\u76ee\u524d\u53ef\u9884\u89c1\u7684\u8981\u4e30\u5bcc\u5f97\u591a\u3002", "conclusion": "\u533a\u5206\u771f\u6b63\u7684\u91cf\u5b50\u4f18\u52bf\u4e0e\u865a\u5e7b\u7684\u91cf\u5b50\u4f18\u52bf\u662f\u4e00\u4e2a\u4e25\u5cfb\u7684\u6311\u6218\u3002\u867d\u7136\u6570\u5b66\u4e25\u8c28\u6027\u662f\u6211\u4eec\u7684\u5fc5\u5907\u6307\u5357\uff0c\u4f46\u91cf\u5b50\u6280\u672f\u7684\u7ec8\u6781\u529b\u91cf\u53ef\u80fd\u6e90\u4e8e\u6211\u4eec\u5c1a\u672a\u80fd\u60f3\u8c61\u5230\u7684\u4f18\u52bf\u3002"}}
{"id": "2508.05830", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.05830", "abs": "https://arxiv.org/abs/2508.05830", "authors": ["Tong Li", "Rasiq Hussain", "Mehak Gupta", "Joshua R. Oltmanns"], "title": "\"Mirror\" Language AI Models of Depression are Criterion-Contaminated", "comment": "39 pages, 9 figures", "summary": "A growing number of studies show near-perfect LLM language-based prediction\nof depression assessment scores (up to R2 of .70). However, many develop these\nmodels directly from language responses to depression assessments. These\n\"Mirror models\" suffer from \"criterion contamination\", which arises when a\npredicted score depends in part on the predictors themselves. This causes\nartificial effect size inflation which reduces model generalizability. The\npresent study compares the performance of Mirror models versus \"Non-Mirror\nmodels\", which are developed from language that does not mirror the assessment\nthey are developed to predict. N = 110 research participants completed two\ndifferent interviews: structured diagnostic and life history interviews. GPT-4,\nGPT-4o and LLaMA3-70B were then prompted to predict structured diagnostic\ninterview depression scores from the two transcripts separately. Mirror models\n(using structured diagnostic data) showed very large effect sizes (e.g., R2 =\n.80). As expected, NonMirror models (using life history data) demonstrated\nsmaller effect sizes, but were relatively large (e.g., R2 = .27). When Mirror\nand Non-Mirror model-predicted structured interview depression scores were\ncorrelated with self-reported depression symptoms, Mirror and NonMirror\nperformed the same (e.g., r = ~.54), indicating that Mirror models contain bias\nperhaps due to criterion contamination. Topic modeling identified clusters\nacross Mirror and Non-Mirror models, as well as between true-positive and\nfalse-positive predictions. In this head-to-head comparison study, Mirror\nlanguage AI models of depression showed artificially inflated effect sizes and\nless generalizability. As language AI models for depression continue to evolve,\nincorporating Non-Mirror models may identify interpretable, and generalizable\nsemantic features that have unique utility in real-world psychological\nassessment.", "AI": {"tldr": "LLM models predicting depression are often flawed due to \"criterion contamination\" from using similar training data. This study shows \"Mirror models\" have inflated results but similar real-world performance to \"Non-Mirror models\", indicating bias. Non-Mirror models are more generalizable.", "motivation": "Many existing LLM-based depression prediction models suffer from \"criterion contamination\" because they are trained on data that mirrors the assessment they are designed to predict. This leads to artificially inflated effect sizes and reduced generalizability. This study aims to address this by comparing \"Mirror models\" with \"Non-Mirror models\".", "method": "This study compared the performance of \"Mirror models\" (trained on data mirroring the assessment) and \"Non-Mirror models\" (trained on dissimilar data). Three LLMs (GPT-4, GPT-4o, LLaMA3-70B) were used to predict depression scores from transcripts of diagnostic interviews and life history interviews. Performance was evaluated using R-squared values and correlations with self-reported depression symptoms. Topic modeling was also employed.", "result": "Mirror models showed significantly larger effect sizes (e.g., R2 = .80) than Non-Mirror models (e.g., R2 = .27). However, when predicting depression scores, the Mirror and Non-Mirror models performed similarly when correlated with self-reported depression symptoms (e.g., r = ~.54), suggesting bias in the Mirror models potentially due to criterion contamination. Topic modeling revealed shared clusters across model types and prediction outcomes.", "conclusion": "Mirror language AI models for depression demonstrate artificially inflated effect sizes and reduced generalizability compared to Non-Mirror models. Future research should consider Non-Mirror models to identify interpretable and generalizable semantic features for real-world psychological assessment."}}
{"id": "2508.05772", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05772", "abs": "https://arxiv.org/abs/2508.05772", "authors": ["Can Zhao", "Pengfei Guo", "Dong Yang", "Yucheng Tang", "Yufan He", "Benjamin Simon", "Mason Belue", "Stephanie Harmon", "Baris Turkbey", "Daguang Xu"], "title": "MAISI-v2: Accelerated 3D High-Resolution Medical Image Synthesis with Rectified Flow and Region-specific Contrastive Loss", "comment": null, "summary": "Medical image synthesis is an important topic for both clinical and research\napplications. Recently, diffusion models have become a leading approach in this\narea. Despite their strengths, many existing methods struggle with (1) limited\ngeneralizability that only work for specific body regions or voxel spacings,\n(2) slow inference, which is a common issue for diffusion models, and (3) weak\nalignment with input conditions, which is a critical issue for medical imaging.\nMAISI, a previously proposed framework, addresses generalizability issues but\nstill suffers from slow inference and limited condition consistency. In this\nwork, we present MAISI-v2, the first accelerated 3D medical image synthesis\nframework that integrates rectified flow to enable fast and high quality\ngeneration. To further enhance condition fidelity, we introduce a novel\nregion-specific contrastive loss to enhance the sensitivity to region of\ninterest. Our experiments show that MAISI-v2 can achieve SOTA image quality\nwith $33 \\times$ acceleration for latent diffusion model. We also conducted a\ndownstream segmentation experiment to show that the synthetic images can be\nused for data augmentation. We release our code, training details, model\nweights, and a GUI demo to facilitate reproducibility and promote further\ndevelopment within the community.", "AI": {"tldr": "MAISI-v2 \u662f\u4e00\u4e2a\u52a0\u901f\u7684 3D \u533b\u5b66\u56fe\u50cf\u5408\u6210\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86 SOTA \u7684\u56fe\u50cf\u8d28\u91cf\u548c\u9ad8\u500d\u52a0\u901f\u3002", "motivation": "\u73b0\u6709\u7684\u533b\u5b66\u56fe\u50cf\u5408\u6210\u65b9\u6cd5\u5b58\u5728\u6cdb\u5316\u6027\u6709\u9650\u3001\u63a8\u7406\u901f\u5ea6\u6162\u548c\u4e0e\u8f93\u5165\u6761\u4ef6\u5bf9\u9f50\u6027\u5f31\u7b49\u95ee\u9898\u3002MAISI-v2 \u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "MAISI-v2 \u662f\u4e00\u4e2a\u52a0\u901f\u7684 3D \u533b\u5b66\u56fe\u50cf\u5408\u6210\u6846\u67b6\uff0c\u5b83\u96c6\u6210\u4e86\u4fee\u6b63\u6d41\u6765\u5b9e\u73b0\u5feb\u901f\u548c\u9ad8\u8d28\u91cf\u7684\u751f\u6210\u3002\u4e3a\u4e86\u8fdb\u4e00\u6b65\u63d0\u9ad8\u6761\u4ef6\u4fdd\u771f\u5ea6\uff0c\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7279\u5b9a\u533a\u57df\u5bf9\u6bd4\u5ea6\u635f\u5931\u3002 ", "result": "MAISI-v2 \u5728\u6f5c\u5728\u6269\u6563\u6a21\u578b\u65b9\u9762\u5b9e\u73b0\u4e86 33 \u500d\u7684\u52a0\u901f\uff0c\u8fbe\u5230\u4e86 SOTA \u7684\u56fe\u50cf\u8d28\u91cf\u3002\u4e0b\u6e38\u5206\u5272\u5b9e\u9a8c\u8868\u660e\uff0c\u5408\u6210\u7684\u56fe\u50cf\u53ef\u7528\u4e8e\u6570\u636e\u589e\u5f3a\u3002", "conclusion": "MAISI-v2 \u5b9e\u73b0\u4e86 33 \u500d\u7684\u52a0\u901f\uff0c\u5b9e\u73b0\u4e86 SOTA \u7684\u56fe\u50cf\u8d28\u91cf\uff0c\u5e76\u4e14\u53ef\u4ee5\u7528\u4e8e\u6570\u636e\u589e\u5f3a\u3002"}}
{"id": "2508.05941", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.05941", "abs": "https://arxiv.org/abs/2508.05941", "authors": ["Zhanyi Sun", "Shuran Song"], "title": "Latent Policy Barrier: Learning Robust Visuomotor Policies by Staying In-Distribution", "comment": null, "summary": "Visuomotor policies trained via behavior cloning are vulnerable to covariate\nshift, where small deviations from expert trajectories can compound into\nfailure. Common strategies to mitigate this issue involve expanding the\ntraining distribution through human-in-the-loop corrections or synthetic data\naugmentation. However, these approaches are often labor-intensive, rely on\nstrong task assumptions, or compromise the quality of imitation. We introduce\nLatent Policy Barrier, a framework for robust visuomotor policy learning.\nInspired by Control Barrier Functions, LPB treats the latent embeddings of\nexpert demonstrations as an implicit barrier separating safe, in-distribution\nstates from unsafe, out-of-distribution (OOD) ones. Our approach decouples the\nrole of precise expert imitation and OOD recovery into two separate modules: a\nbase diffusion policy solely on expert data, and a dynamics model trained on\nboth expert and suboptimal policy rollout data. At inference time, the dynamics\nmodel predicts future latent states and optimizes them to stay within the\nexpert distribution. Both simulated and real-world experiments show that LPB\nimproves both policy robustness and data efficiency, enabling reliable\nmanipulation from limited expert data and without additional human correction\nor annotation.", "AI": {"tldr": "LPB\u6846\u67b6\u901a\u8fc7\u5c06\u4e13\u5bb6\u6f14\u793a\u7684\u6f5c\u5728\u5d4c\u5165\u89c6\u4e3a\u4e00\u79cd\u9690\u5f0f\u969c\u788d\u7269\uff0c\u5e76\u7ed3\u5408\u57fa\u7840\u6269\u6563\u7b56\u7565\u548c\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u63d0\u9ad8\u4e86\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u5728\u9762\u5bf9\u534f\u53d8\u91cf\u504f\u79fb\u65f6\u7684\u9c81\u68d2\u6027\u548c\u6570\u636e\u6548\u7387\u3002", "motivation": "\u884c\u4e3a\u514b\u9686\u8bad\u7ec3\u7684\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u5bb9\u6613\u53d7\u5230\u534f\u53d8\u91cf\u504f\u79fb\u7684\u5f71\u54cd\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u4eba\u5de5\u5e72\u9884\u6216\u6570\u636e\u589e\u5f3a\uff09\u6210\u672c\u9ad8\u6602\u4e14\u6709\u5c40\u9650\u6027\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u6765\u63d0\u9ad8\u7b56\u7565\u7684\u9c81\u68d2\u6027\u3002", "method": "LPB\u5c06\u6f5c\u5728\u5d4c\u5165\u89c6\u4e3a\u9690\u5f0f\u969c\u788d\u7269\uff0c\u5c06\u4e13\u5bb6\u6570\u636e\u548c\u6b21\u4f18\u6570\u636e\u5206\u5f00\uff0c\u4f7f\u7528\u4e24\u4e2a\u6a21\u5757\uff1a\u4e00\u4e2a\u4ec5\u57fa\u4e8e\u4e13\u5bb6\u6570\u636e\u7684\u57fa\u7840\u6269\u6563\u7b56\u7565\uff0c\u4ee5\u53ca\u4e00\u4e2a\u5728\u4e13\u5bb6\u548c\u6b21\u4f18\u6570\u636e\u4e0a\u8bad\u7ec3\u7684\u52a8\u529b\u5b66\u6a21\u578b\u3002", "result": "LPB\u5728\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u7684\u5b9e\u9a8c\u4e2d\uff0c\u5728\u6570\u636e\u6548\u7387\u548c\u7b56\u7565\u9c81\u68d2\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5373\u4f7f\u5728\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u5b9e\u73b0\u53ef\u9760\u7684\u64cd\u4f5c\u3002", "conclusion": "LPB\u901a\u8fc7\u5c06\u4e13\u5bb6\u6f14\u793a\u7684\u6f5c\u5728\u5d4c\u5165\u89c6\u4e3a\u533a\u5206\u5b89\u5168\u548c\u4e0d\u5b89\u5168\u72b6\u6001\u7684\u9690\u5f0f\u5c4f\u969c\uff0c\u5e76\u5728\u63a8\u7406\u65f6\u4f18\u5316\u6f5c\u5728\u72b6\u6001\u4ee5\u4fdd\u6301\u5728\u4e13\u5bb6\u5206\u5e03\u5185\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u6570\u636e\u7684\u6548\u7387\u548c\u7b56\u7565\u7684\u9c81\u68d2\u6027\uff0c\u53ef\u4ee5\u5728\u6ca1\u6709\u989d\u5916\u4eba\u5de5\u5e72\u9884\u7684\u60c5\u51b5\u4e0b\uff0c\u4ece\u6709\u9650\u7684\u4e13\u5bb6\u6570\u636e\u4e2d\u8fdb\u884c\u53ef\u9760\u7684\u64cd\u4f5c\u3002"}}
{"id": "2508.05855", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.05855", "abs": "https://arxiv.org/abs/2508.05855", "authors": ["Zixia Wang", "Jia Hu", "Ronghui Mu"], "title": "Safety of Embodied Navigation: A Survey", "comment": null, "summary": "As large language models (LLMs) continue to advance and gain influence, the\ndevelopment of embodied AI has accelerated, drawing significant attention,\nparticularly in navigation scenarios. Embodied navigation requires an agent to\nperceive, interact with, and adapt to its environment while moving toward a\nspecified target in unfamiliar settings. However, the integration of embodied\nnavigation into critical applications raises substantial safety concerns. Given\ntheir deployment in dynamic, real-world environments, ensuring the safety of\nsuch systems is critical. This survey provides a comprehensive analysis of\nsafety in embodied navigation from multiple perspectives, encompassing attack\nstrategies, defense mechanisms, and evaluation methodologies. Beyond conducting\na comprehensive examination of existing safety challenges, mitigation\ntechnologies, and various datasets and metrics that assess effectiveness and\nrobustness, we explore unresolved issues and future research directions in\nembodied navigation safety. These include potential attack methods, mitigation\nstrategies, more reliable evaluation techniques, and the implementation of\nverification frameworks. By addressing these critical gaps, this survey aims to\nprovide valuable insights that can guide future research toward the development\nof safer and more reliable embodied navigation systems. Furthermore, the\nfindings of this study have broader implications for enhancing societal safety\nand increasing industrial efficiency.", "AI": {"tldr": "\u672c\u8c03\u67e5\u5168\u9762\u5206\u6790\u4e86\u5177\u8eab\u5bfc\u822a\u4e2d\u7684\u5b89\u5168\u95ee\u9898\uff0c\u91cd\u70b9\u5173\u6ce8\u653b\u51fb\u3001\u9632\u5fa1\u548c\u8bc4\u4f30\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u7814\u7a76\u6307\u660e\u4e86\u65b9\u5411\uff0c\u4ee5\u671f\u6784\u5efa\u66f4\u5b89\u5168\u7684\u5177\u8eabAI\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u4e0d\u65ad\u53d1\u5c55\u548c\u5f71\u54cd\u529b\u7684\u589e\u5f3a\uff0c\u5177\u8eabAI\u7684\u53d1\u5c55\u4e5f\u968f\u4e4b\u52a0\u901f\uff0c\u7279\u522b\u662f\u5728\u5bfc\u822a\u9886\u57df\uff0c\u8fd9\u5f15\u53d1\u4e86\u4eba\u4eec\u7684\u5e7f\u6cdb\u5173\u6ce8\u3002\u5177\u8eab\u5bfc\u822a\u8981\u6c42\u667a\u80fd\u4f53\u5728\u5411\u6307\u5b9a\u76ee\u6807\u79fb\u52a8\u65f6\uff0c\u80fd\u591f\u611f\u77e5\u3001\u4e92\u52a8\u548c\u9002\u5e94\u5176\u6240\u5904\u7684\u73af\u5883\uff0c\u5c24\u5176\u662f\u5728\u964c\u751f\u7684\u73af\u5883\u4e2d\u3002\u7136\u800c\uff0c\u5c06\u5177\u8eab\u5bfc\u822a\u5e94\u7528\u4e8e\u5173\u952e\u9886\u57df\u4f1a\u5f15\u53d1\u91cd\u5927\u7684\u5b89\u5168\u62c5\u5fe7\u3002\u9274\u4e8e\u5176\u5728\u52a8\u6001\u3001\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u90e8\u7f72\uff0c\u786e\u4fdd\u8fd9\u4e9b\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5bf9\u73b0\u6709\u5b89\u5168\u6311\u6218\u3001\u7f13\u89e3\u6280\u672f\u3001\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6307\u6807\u8fdb\u884c\u4e86\u5168\u9762\u5ba1\u67e5\uff0c\u5e76\u63a2\u8ba8\u4e86\u6f5c\u5728\u7684\u653b\u51fb\u65b9\u6cd5\u3001\u7f13\u89e3\u7b56\u7565\u3001\u66f4\u53ef\u9760\u7684\u8bc4\u4f30\u6280\u672f\u548c\u9a8c\u8bc1\u6846\u67b6\u7684\u5b9e\u65bd\u3002", "result": "\u672c\u8c03\u67e5\u65e8\u5728\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u89c1\u89e3\uff0c\u4ee5\u6307\u5bfc\u5f00\u53d1\u66f4\u5b89\u5168\u3001\u66f4\u53ef\u9760\u7684\u5177\u8eab\u5bfc\u822a\u7cfb\u7edf\u3002\u6b64\u5916\uff0c\u672c\u7814\u7a76\u7ed3\u679c\u5bf9\u63d0\u9ad8\u793e\u4f1a\u5b89\u5168\u548c\u63d0\u5347\u5de5\u4e1a\u6548\u7387\u5177\u6709\u66f4\u5e7f\u6cdb\u7684\u610f\u4e49\u3002", "conclusion": "\u672c\u8c03\u67e5\u5168\u9762\u5206\u6790\u4e86\u5177\u8eab\u5bfc\u822a\u4e2d\u7684\u5b89\u5168\u95ee\u9898\uff0c\u6db5\u76d6\u4e86\u653b\u51fb\u7b56\u7565\u3001\u9632\u5fa1\u673a\u5236\u548c\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5e76\u63a2\u8ba8\u4e86\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2508.06486", "categories": ["cs.DS", "cs.NA", "math.NA", "65F55 65F15", "G.1.3; F.2.1"], "pdf": "https://arxiv.org/pdf/2508.06486", "abs": "https://arxiv.org/abs/2508.06486", "authors": ["Tyler Chen", "Ethan N. Epperly", "Raphael A. Meyer", "Christopher Musco", "Akash Rao"], "title": "Does block size matter in randomized block Krylov low-rank approximation?", "comment": null, "summary": "We study the problem of computing a rank-$k$ approximation of a matrix using\nrandomized block Krylov iteration. Prior work has shown that, for block size $b\n= 1$ or $b = k$, a $(1 + \\varepsilon)$-factor approximation to the best\nrank-$k$ approximation can be obtained after $\\tilde O(k/\\sqrt{\\varepsilon})$\nmatrix-vector products with the target matrix. On the other hand, when $b$ is\nbetween $1$ and $k$, the best known bound on the number of matrix-vector\nproducts scales with $b(k-b)$, which could be as large as $O(k^2)$.\nNevertheless, in practice, the performance of block Krylov methods is often\noptimized by choosing a block size $1 \\ll b \\ll k$. We resolve this\ntheory-practice gap by proving that randomized block Krylov iteration produces\na $(1 + \\varepsilon)$-factor approximate rank-$k$ approximation using $\\tilde\nO(k/\\sqrt{\\varepsilon})$ matrix-vector products for any block size $1\\le b\\le\nk$. Our analysis relies on new bounds for the minimum singular value of a\nrandom block Krylov matrix, which may be of independent interest. Similar\nbounds are central to recent breakthroughs on faster algorithms for sparse\nlinear systems [Peng & Vempala, SODA 2021; Nie, STOC 2022].", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4f7f\u7528\u968f\u673a\u5757\u514b\u96f7\u6d1b\u592b\u8fed\u4ee3\u8ba1\u7b97\u77e9\u9635\u7684\u79e9-$k$ \u8fd1\u4f3c\u95ee\u9898\u3002\u6211\u4eec\u8bc1\u660e\u4e86\u5bf9\u4e8e\u4efb\u610f\u5757\u5927\u5c0f $1 \\le b \\le k$\uff0c\u8be5\u65b9\u6cd5\u4ec5\u9700 $\\tilde O(k/\\sqrt{\\varepsilon})$ \u6b21\u77e9\u9635\u5411\u91cf\u4e58\u79ef\u5373\u53ef\u83b7\u5f97 $(1 + \\varepsilon)$ \u56e0\u5b50\u8fd1\u4f3c\u7684\u79e9-$k$ \u8fd1\u4f3c\uff0c\u89e3\u51b3\u4e86\u7406\u8bba\u4e0e\u5b9e\u8df5\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "motivation": "\u89e3\u51b3\u968f\u673a\u5757\u514b\u96f7\u6d1b\u592b\u8fed\u4ee3\u5728\u5904\u7406\u79e9-$k$ \u8fd1\u4f3c\u65f6\u7684\u7406\u8bba\u4e0e\u5b9e\u8df5\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5373\u5728\u5757\u5927\u5c0f $b$ \u4ecb\u4e8e $1$ \u548c $k$ \u4e4b\u95f4\u65f6\uff0c\u73b0\u6709\u7406\u8bba\u754c\u9650\uff08\u4e0e $b(k-b)$ \u76f8\u5173\uff0c\u53ef\u80fd\u8fbe\u5230 $O(k^2)$\uff09\u4e0e\u5b9e\u8df5\u4e2d\u4f18\u5316\u7684\u5757\u5927\u5c0f\uff08$1 \",\" b \",\" k$\uff09\u4e4b\u95f4\u7684\u4e0d\u7b26\u3002", "method": "\u672c\u6587\u8bc1\u660e\u4e86\u968f\u673a\u5757\u514b\u96f7\u6d1b\u592b\u8fed\u4ee3\u5728\u4efb\u4f55\u5757\u5927\u5c0f $1 \\le b \\le k$ \u4e0b\uff0c\u90fd\u80fd\u5728 $\\tilde O(k/\\sqrt{\\varepsilon})$ \u6b21\u77e9\u9635\u5411\u91cf\u4e58\u79ef\u5185\u5f97\u5230 $(1+\\varepsilon)$ \u56e0\u5b50\u8fd1\u4f3c\u7684\u79e9-$k$ \u8fd1\u4f3c\u3002\u8be5\u5206\u6790\u4f9d\u8d56\u4e8e\u5bf9\u968f\u673a\u5757\u514b\u96f7\u6d1b\u592b\u77e9\u9635\u6700\u5c0f\u5947\u5f02\u503c\u7684\u65b0\u7684\u754c\u9650\u3002", "result": "\u5b9e\u73b0\u4e86\u9488\u5bf9\u4efb\u610f\u5757\u5927\u5c0f $1 \\le b \\le k$ \u7684\u79e9-$k$ \u8fd1\u4f3c\uff0c\u5c06\u6240\u9700\u7684\u77e9\u9635\u5411\u91cf\u4e58\u79ef\u6570\u91cf\u4ece\u53ef\u80fd\u7684\u6700\u5927 $O(k^2)$ \u663e\u8457\u964d\u4f4e\u5230 $\\tilde O(k/\\sqrt{\\varepsilon})$\uff0c\u8fd9\u4e0e\u5757\u5927\u5c0f $b$ \u65e0\u5173\u3002", "conclusion": "\u968f\u673a\u5757\u514b\u96f7\u6d1b\u592b\u8fed\u4ee3\u53ef\u4ee5\u9488\u5bf9\u4efb\u4f55\u5757\u5927\u5c0f $1 \",\" b \",\" k$\uff0c\u5728 $\tilde O(k / \\sqrt{\\varepsilon})$ \u6b21\u77e9\u9635\u5411\u91cf\u4e58\u79ef\u5185\uff0c\u751f\u6210 $(1 + \\varepsilon)$ \u56e0\u5b50\u8fd1\u4f3c\u7684\u6700\u4f73\u79e9-$k$ \u8fd1\u4f3c\u3002"}}
{"id": "2508.06015", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2508.06015", "abs": "https://arxiv.org/abs/2508.06015", "authors": ["Eric V Woods", "Xinren Chen", "Shaolou Wei", "Alisson Kwiatkowski da Silva", "Ayman A El-Zoka", "J Manoj Prabhakar", "Tim M Schwarz", "Yongqiang Kang", "Leonardo S Aota", "Mahander P Singh", "Katja Angenendt", "Ozge Ozgun", "Matic Jovivcevic-Klug", "Patricia Jovivcevic-Klug", "Christian Bross", "Jian Liu", "Rene de Kloe", "Gerhard Dehm", "Stefan Zaefferer", "Yug Joshi", "Baptiste Gault"], "title": "Vacuum Dealloyed Brass as Li-Metal Battery Current Collector: Effect of Zinc and Porosity", "comment": "58 pages, 6 main figures, 20 SI figures; main text and supplementary\n  information included in a single PDF", "summary": "\"Anode-free\" lithium-metal batteries promise significantly higher energy\ndensity than conventional graphite-based lithium-ion batteries; however,\nlithium dendrite growth can lead to internal short circuits with associated\nsafety risks. While porous current collectors can suppress dendrite growth,\noptimal porosity and composition remain unknown. Here, we show that the\ntemperature during vapor phase dealloying (VPD) of alpha-brass (Cu63Zn37)\ncontrols the surface Zn concentration, decreasing from 8 percent to below 1\npercent from 500 to 800 degrees C. The surface composition is controlled by the\ntemperature-dependent diffusion. A battery cell maintains greater than 90\npercent Coulombic efficiency (CE) over 100 cycles when the Zn content is the\nlowest, whereas the higher-Zn samples degraded to approximately 70 percent CE.\nThe difference in surface composition has hence dramatic effects on battery\nperformance, and our results demonstrate how precise compositional control\nenables stable lithium-metal battery operation, establishing about 1 atomic\npercent surface Zn as optimal for preventing capacity fading and uniform\nlithium plating, while establishing predictive relationships between processing\ntemperature and surface composition. This work provides design rules for\nmultifunctional current collectors and demonstrates scalable VPD production for\nnext-generation batteries.", "AI": {"tldr": "\u901a\u8fc7\u63a7\u5236\u9ec4\u94dc\u96c6\u6d41\u4f53\u7684\u84b8\u6c7d\u76f8\u8131\u5408\u91d1\u6e29\u5ea6\uff0c\u53ef\u4ee5\u7cbe\u786e\u8c03\u63a7\u8868\u9762\u950c\u542b\u91cf\uff0c\u5b9e\u73b0\u7a33\u5b9a\u9502\u91d1\u5c5e\u7535\u6c60\u6027\u80fd\u3002\u7ea61%\u7684\u8868\u9762\u950c\u542b\u91cf\u662f\u6700\u4f73\u9009\u62e9\uff0c\u53ef\u663e\u8457\u63d0\u9ad8\u5e93\u4ed1\u6548\u7387\u548c\u5faa\u73af\u5bff\u547d\u3002", "motivation": "\u9502\u91d1\u5c5e\u7535\u6c60\u5177\u6709\u6bd4\u77f3\u58a8\u9502\u79bb\u5b50\u7535\u6c60\u66f4\u9ad8\u7684\u80fd\u91cf\u5bc6\u5ea6\u6f5c\u529b\uff0c\u4f46\u9502\u679d\u6676\u7684\u751f\u957f\u5e26\u6765\u4e86\u5b89\u5168\u98ce\u9669\u3002\u867d\u7136\u591a\u5b54\u96c6\u6d41\u4f53\u5df2\u88ab\u8bc1\u660e\u53ef\u4ee5\u6291\u5236\u679d\u6676\u751f\u957f\uff0c\u4f46\u5176\u6700\u4f73\u5b54\u9699\u7387\u548c\u6210\u5206\u4ecd\u4e0d\u660e\u786e\u3002", "method": "\u901a\u8fc7\u5728500\u81f3800\u6444\u6c0f\u5ea6\u7684\u6e29\u5ea6\u8303\u56f4\u5185\u8fdb\u884c\u84b8\u6c7d\u76f8\u8131\u5408\u91d1\uff08VPD\uff09\u5904\u7406\u9ec4\u94dc\uff08Cu63Zn37\uff09\uff0c\u63a7\u5236\u96c6\u6d41\u4f53\u8868\u9762\u7684\u950c\u542b\u91cf\u3002\u7814\u7a76\u4e86\u6e29\u5ea6\u5bf9\u8868\u9762\u950c\u6d53\u5ea6\u7684\u5f71\u54cd\uff0c\u5e76\u5efa\u7acb\u4e86\u5904\u7406\u6e29\u5ea6\u4e0e\u8868\u9762\u6210\u5206\u4e4b\u95f4\u7684\u9884\u6d4b\u5173\u7cfb\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u5728VPD\u8fc7\u7a0b\u4e2d\uff0c\u6e29\u5ea6\u662f\u63a7\u5236\u8868\u9762\u950c\u542b\u91cf\u7684\u5173\u952e\u56e0\u7d20\uff0c\u4ece500\u6444\u6c0f\u5ea6\u76848%\u964d\u4f4e\u5230800\u6444\u6c0f\u5ea6\u76841%\u4ee5\u4e0b\u3002\u5f53\u950c\u542b\u91cf\u6700\u4f4e\u65f6\uff08\u7ea61%\uff09\uff0c\u7535\u6c60\u5e93\u4ed1\u6548\u7387\uff08CE\uff09\u5728100\u6b21\u5faa\u73af\u540e\u4ecd\u4fdd\u6301\u572890%\u4ee5\u4e0a\uff1b\u800c\u5f53\u950c\u542b\u91cf\u8f83\u9ad8\u65f6\uff0cCE\u5219\u4e0b\u964d\u81f3\u7ea670%\u3002", "conclusion": "\u901a\u8fc7\u7cbe\u786e\u63a7\u5236\u8868\u9762\u950c\u542b\u91cf\uff08\u7ea61\u4e2a\u539f\u5b50\u767e\u5206\u6bd4\uff09\u53ef\u4ee5\u5b9e\u73b0\u7a33\u5b9a\u7684\u9502\u91d1\u5c5e\u7535\u6c60\u8fd0\u884c\uff0c\u6709\u6548\u6291\u5236\u5bb9\u91cf\u8870\u51cf\u5e76\u5b9e\u73b0\u5747\u5300\u9502\u6c89\u79ef\u3002\u672c\u7814\u7a76\u4e3a\u591a\u529f\u80fd\u96c6\u6d41\u4f53\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u4f9d\u636e\uff0c\u5e76\u5c55\u793a\u4e86\u53ef\u6269\u5c55\u7684\u84b8\u6c7d\u76f8\u8131\u5408\u91d1\uff08VPD\uff09\u751f\u4ea7\u5de5\u827a\uff0c\u4ee5\u652f\u6301\u4e0b\u4e00\u4ee3\u7535\u6c60\u7684\u53d1\u5c55\u3002"}}
{"id": "2508.06413", "categories": ["physics.app-ph", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2508.06413", "abs": "https://arxiv.org/abs/2508.06413", "authors": ["Victor Vanpeene", "Olga Stamati", "Francois Cadiou", "Quentin Jacquet", "Julie Villanova", "Sandrine Lyonnard"], "title": "4D operando X-ray nano-holo-tomography reveals multiscale chemomechanics in Silicon-Graphite anode", "comment": null, "summary": "Linking electrode microstructure to electrochemical performance is essential\nfor optimizing Li-ion batteries. However, this requires mechanistic 4D\nobservations at ultimate spatio-temporal scales, which remains elusive. Here we\ndemonstrate the use of operando synchrotron X-ray nano-holo-tomography combined\nwith Digital Volume Correlation to track chemomechanical dynamics at both\nparticle (local) and electrode (averaged) scales. Quantitative scale-bridging\nimage analysis is applied to a high-capacity silicon-graphite anode during its\nformation cycle. Our findings reveal that local diffusion properties, graphite\nparticle morphology and position in the electrode, distance to silicon\nclusters, surface contact with electrolyte and mechanical deformations, all\nhave a direct impact on the local electrochemical activity and irreversibility\n- but these parameters are not equally important. Particularly, we identify\nfast diffusion channels that play a key role and counterbalance intrinsic\ndepth-dependent reaction heterogeneities due to ionic/electronic diffusion\nlimitations. The various structural factors that determine Gr-Si battery\nperformance beyond ensemble properties are classified using a scale of\ninfluence, providing a practical framework for the optimization of materials\nand electrode manufacturing.", "AI": {"tldr": "\u901a\u8fc7X\u5c04\u7ebf\u7eb3\u7c73\u5168\u606f\u65ad\u5c42\u626b\u63cf\u548c\u6570\u5b57\u4f53\u79ef\u76f8\u5173\u6280\u672f\uff0c\u9996\u6b21\u5b9e\u73b0\u4e86\u5bf9\u9502\u79bb\u5b50\u7535\u6c60\u7845-\u77f3\u58a8\u8d1f\u6781\u7684\u591a\u5c3a\u5ea6\u5316\u5b66\u673a\u68b0\u52a8\u529b\u5b66\u8ffd\u8e2a\uff0c\u63ed\u793a\u4e86\u5f71\u54cd\u7535\u6c60\u6027\u80fd\u7684\u5173\u952e\u7ed3\u6784\u56e0\u7d20\u53ca\u5176\u91cd\u8981\u6027\uff0c\u4e3a\u7535\u6c60\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u6846\u67b6\u3002", "motivation": "\u4e3a\u4e86\u4f18\u5316\u9502\u79bb\u5b50\u7535\u6c60\u6027\u80fd\uff0c\u5fc5\u987b\u5c06\u7535\u6781\u7684\u5fae\u89c2\u7ed3\u6784\u4e0e\u5176\u7535\u5316\u5b66\u6027\u80fd\u8054\u7cfb\u8d77\u6765\uff0c\u8fd9\u9700\u8981\u8fbe\u5230\u7ec8\u6781\u65f6\u7a7a\u5c3a\u5ea6\u7684\u529b\u5b66\u8fc7\u7a0b4D\u89c2\u6d4b\uff0c\u800c\u8fd9\u5728\u76ee\u524d\u4ecd\u7136\u96be\u4ee5\u5b9e\u73b0\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528\u64cd\u4f5c\u578b\u540c\u6b65X\u5c04\u7ebf\u7eb3\u7c73\u5168\u606f\u65ad\u5c42\u626b\u63cf\u6280\u672f\uff0c\u5e76\u7ed3\u5408\u6570\u5b57\u4f53\u79ef\u76f8\u5173\u6280\u672f\uff0c\u5bf9\u9502\u79bb\u5b50\u7535\u6c60\u7845-\u77f3\u58a8\u8d1f\u6781\u5728\u5f62\u6210\u5faa\u73af\u8fc7\u7a0b\u4e2d\u7684\u5316\u5b66\u673a\u68b0\u52a8\u529b\u5b66\u8fdb\u884c\u4e86\u591a\u5c3a\u5ea6\u8ffd\u8e2a\u548c\u5206\u6790\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5c40\u90e8\u6269\u6563\u7279\u6027\u3001\u77f3\u58a8\u9897\u7c92\u7684\u5f62\u8c8c\u548c\u5728\u7535\u6781\u4e2d\u7684\u4f4d\u7f6e\u3001\u4e0e\u7845\u56e2\u7c07\u7684\u8ddd\u79bb\u3001\u4e0e\u7535\u89e3\u6db2\u7684\u8868\u9762\u63a5\u89e6\u4ee5\u53ca\u673a\u68b0\u53d8\u5f62\u90fd\u4f1a\u76f4\u63a5\u5f71\u54cd\u5c40\u90e8\u7535\u5316\u5b66\u6d3b\u6027\u548c\u4e0d\u53ef\u9006\u6027\uff0c\u4f46\u8fd9\u4e9b\u53c2\u6570\u7684\u91cd\u8981\u6027\u4e0d\u540c\u3002\u7814\u7a76\u7279\u522b\u6307\u51fa\u4e86\u5feb\u901f\u6269\u6563\u901a\u9053\u5728\u5e73\u8861\u7531\u79bb\u5b50/\u7535\u5b50\u6269\u6563\u9650\u5236\u5f15\u8d77\u7684\u3001\u4e0e\u6df1\u5ea6\u76f8\u5173\u7684\u53cd\u5e94\u975e\u5747\u4e00\u6027\u65b9\u9762\u8d77\u7740\u5173\u952e\u4f5c\u7528\u3002\u7814\u7a76\u8fd8\u6839\u636e\u5f71\u54cd\u7a0b\u5ea6\u5bf9\u51b3\u5b9a\u77f3\u58a8-\u7845\u7535\u6c60\u6027\u80fd\u7684\u5404\u79cd\u7ed3\u6784\u56e0\u7d20\u8fdb\u884c\u4e86\u5206\u7c7b\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5b9e\u7528\u7684\u6846\u67b6\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u64cd\u4f5c\u578b\u540c\u6b65X\u5c04\u7ebf\u7eb3\u7c73\u5168\u606f\u65ad\u5c42\u626b\u63cf\u7ed3\u5408\u6570\u5b57\u4f53\u79ef\u76f8\u5173\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u5bf9\u9502\u79bb\u5b50\u7535\u6c60\u7845-\u77f3\u58a8\u8d1f\u6781\u5728\u5f62\u6210\u5faa\u73af\u8fc7\u7a0b\u4e2d\u7684\u5316\u5b66\u673a\u68b0\u52a8\u529b\u5b66\u8fdb\u884c\u591a\u5c3a\u5ea6\u8ffd\u8e2a\u3002\u7814\u7a76\u63ed\u793a\u4e86\u5c40\u90e8\u6269\u6563\u7279\u6027\u3001\u77f3\u58a8\u9897\u7c92\u5f62\u8c8c\u4e0e\u4f4d\u7f6e\u3001\u4e0e\u7845\u56e2\u7c07\u7684\u8ddd\u79bb\u3001\u4e0e\u7535\u89e3\u6db2\u7684\u8868\u9762\u63a5\u89e6\u4ee5\u53ca\u673a\u68b0\u53d8\u5f62\u7b49\u56e0\u7d20\u5bf9\u5c40\u90e8\u7535\u5316\u5b66\u6d3b\u6027\u548c\u4e0d\u53ef\u9006\u6027\u7684\u5f71\u54cd\uff0c\u5e76\u533a\u5206\u4e86\u8fd9\u4e9b\u56e0\u7d20\u7684\u91cd\u8981\u6027\u3002\u7814\u7a76\u7279\u522b\u8bc6\u522b\u51fa\u5feb\u901f\u6269\u6563\u901a\u9053\u80fd\u591f\u62b5\u6d88\u7531\u79bb\u5b50/\u7535\u5b50\u6269\u6563\u9650\u5236\u5f15\u8d77\u7684\u3001\u4e0e\u6df1\u5ea6\u76f8\u5173\u7684\u53cd\u5e94\u975e\u5747\u4e00\u6027\u3002\u6700\u540e\uff0c\u7814\u7a76\u5bf9\u51b3\u5b9a\u77f3\u58a8-\u7845\u7535\u6c60\u6027\u80fd\u7684\u5404\u79cd\u7ed3\u6784\u56e0\u7d20\u8fdb\u884c\u4e86\u5206\u7c7b\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5b9e\u7528\u7684\u4f18\u5316\u6750\u6599\u548c\u7535\u6781\u5236\u9020\u7684\u6846\u67b6\u3002"}}
{"id": "2508.06054", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.06054", "abs": "https://arxiv.org/abs/2508.06054", "authors": ["Yiheng Wang", "Shutao Zhang", "Ye Xue", "Tsung-Hui Chang"], "title": "Multi-Modal Neural Radio Radiance Field for Localized Statistical Channel Modelling", "comment": null, "summary": "This paper presents MM-LSCM, a self-supervised multi-modal neural radio\nradiance field framework for localized statistical channel modeling (LSCM) for\nnext-generation network optimization. Traditional LSCM methods rely solely on\nRSRP data, limiting their ability to model environmental structures that affect\nsignal propagation. To address this, we propose a dual-branch neural\narchitecture that integrates RSRP data and LiDAR point cloud information,\nenhancing spatial awareness and predictive accuracy. MM-LSCM leverages\nvolume-rendering-based multi-modal synthesis to align radio propagation with\nenvironmental obstacles and employs a self-supervised training approach,\neliminating the need for costly labeled data. Experimental results demonstrate\nthat MM-LSCM significantly outperforms conventional methods in channel\nreconstruction accuracy and robustness to noise, making it a promising solution\nfor real-world wireless network optimization.", "AI": {"tldr": "MM-LSCM \u662f\u4e00\u79cd\u65b0\u7684\u901a\u4fe1\u6846\u67b6\uff0c\u5b83\u4f7f\u7528 LiDAR \u548c RSRP \u6570\u636e\u6765\u63d0\u9ad8\u65e0\u7ebf\u901a\u4fe1\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u7684 LSCM \u65b9\u6cd5\u4ec5\u4f9d\u8d56 RSRP \u6570\u636e\uff0c\u5728\u6a21\u62df\u5f71\u54cd\u4fe1\u53f7\u4f20\u64ad\u7684\u73af\u5883\u7ed3\u6784\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u878d\u5408\u591a\u6a21\u6001\u4fe1\u606f\uff08\u5982 LiDAR \u6570\u636e\uff09\u6765\u589e\u5f3a\u7a7a\u95f4\u611f\u77e5\u548c\u9884\u6d4b\u7cbe\u5ea6\uff0c\u4ee5\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u5206\u652f\u795e\u7ecf\u7f51\u7edc\u7ed3\u6784\uff0c\u8be5\u7ed3\u6784\u96c6\u6210\u4e86 RSRP \u6570\u636e\u548c LiDAR \u70b9\u4e91\u4fe1\u606f\uff0c\u5e76\u5229\u7528\u57fa\u4e8e\u4f53\u79ef\u6e32\u67d3\u7684\u591a\u6a21\u6001\u5408\u6210\u6280\u672f\uff0c\u91c7\u7528\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMM-LSCM \u5728\u4fe1\u9053\u91cd\u5efa\u7684\u51c6\u786e\u6027\u548c\u5bf9\u566a\u58f0\u7684\u9c81\u68d2\u6027\u65b9\u9762\u660e\u663e\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "MM-LSCM \u901a\u8fc7\u96c6\u6210 LiDAR \u6570\u636e\u548c\u81ea\u76d1\u7763\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u65e0\u7ebf\u4fe1\u9053\u5efa\u6a21\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u7f51\u7edc\u4f18\u5316\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.06297", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.06297", "abs": "https://arxiv.org/abs/2508.06297", "authors": ["Yanyu Liu", "Jingying Fu", "Sixiang Liu", "Yitian Zou", "You Fu", "Jiehan Zhou", "Shouhua Zhang"], "title": "KV Cache Compression for Inference Efficiency in LLMs: A Review", "comment": "12 pages", "summary": "Withtherapid advancement of large language models (LLMs), the context length\nfor inference has been continuously increasing, leading to an exponential\ngrowth in the demand for Key-Value (KV) caching. This has resulted in a\nsignificant memory bottleneck, limiting the inference efficiency and\nscalability of the models. Therefore, optimizing the KV cache during inference\nis crucial for enhancing performance and efficiency. This review systematically\nexamines current KV cache optimization techniques, including compression\nstrategies such as selective token strategies, quantization, and attention\ncompression. We evaluate the effectiveness, trade-offs, and application\nscenarios of these methods, providing a comprehensive analysis of their impact\non memory usage and inference speed. We focus on identifying the limitations\nand challenges of existing methods, such as compatibility issues with different\nmodels and tasks. Additionally, this review highlights future research\ndirections, including hybrid optimization techniques, adaptive dynamic\nstrategies, and software-hardware co-design. These approaches aim to improve\ninference efficiency and promote the practical application of large language\nmodels.", "AI": {"tldr": "\u5927\u6a21\u578b\u63a8\u7406\u53d7 KV \u7f13\u5b58\u5185\u5b58\u74f6\u9888\u9650\u5236\uff0c\u672c\u6587\u7efc\u8ff0\u4e86\u538b\u7f29\u6280\u672f\uff08\u9009\u62e9\u6027 token\u3001\u91cf\u5316\u3001\u6ce8\u610f\u529b\u538b\u7f29\uff09\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u6df7\u5408\u4f18\u5316\u3001\u81ea\u9002\u5e94\u7b56\u7565\u548c\u8f6f\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u7684\u65b9\u5411\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u63a8\u7406\u7684\u4e0a\u4e0b\u6587\u957f\u5ea6\u4e0d\u65ad\u589e\u52a0\uff0c\u5bfc\u81f4 KV \u7f13\u5b58\u9700\u6c42\u5448\u6307\u6570\u7ea7\u589e\u957f\uff0c\u8fdb\u800c\u5f15\u53d1\u4e86\u663e\u8457\u7684\u5185\u5b58\u74f6\u9888\uff0c\u9650\u5236\u4e86\u6a21\u578b\u7684\u63a8\u7406\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002\u56e0\u6b64\uff0c\u4f18\u5316\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684 KV \u7f13\u5b58\u5bf9\u4e8e\u63d0\u9ad8\u6027\u80fd\u548c\u6548\u7387\u81f3\u5173\u91cd\u8981\u3002", "method": "\u672c\u7efc\u8ff0\u7cfb\u7edf\u5730\u5ba1\u89c6\u4e86\u5f53\u524d\u7684 KV \u7f13\u5b58\u4f18\u5316\u6280\u672f\uff0c\u91cd\u70b9\u4ecb\u7ecd\u4e86\u538b\u7f29\u7b56\u7565\uff0c\u5982\u9009\u62e9\u6027 token \u7b56\u7565\u3001\u91cf\u5316\u548c\u6ce8\u610f\u529b\u538b\u7f29\uff0c\u5e76\u8bc4\u4f30\u4e86\u5b83\u4eec\u5728\u5185\u5b58\u5360\u7528\u548c\u63a8\u7406\u901f\u5ea6\u65b9\u9762\u7684\u5f71\u54cd\u3001\u6743\u8861\u548c\u5e94\u7528\u573a\u666f\u3002", "result": "\u672c\u7efc\u8ff0\u5bf9 KV \u7f13\u5b58\u4f18\u5316\u6280\u672f\u8fdb\u884c\u4e86\u5168\u9762\u5206\u6790\uff0c\u91cd\u70b9\u5173\u6ce8\u5176\u6709\u6548\u6027\u3001\u6743\u8861\u548c\u5e94\u7528\u573a\u666f\uff0c\u5e76\u6307\u51fa\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u6a21\u578b\u548c\u4efb\u52a1\u517c\u5bb9\u6027\u65b9\u9762\u5b58\u5728\u7684\u5c40\u9650\u6027\u548c\u6311\u6218\u3002", "conclusion": "KV \u7f13\u5b58\u4f18\u5316\u5bf9\u4e8e\u63d0\u5347\u5927\u6a21\u578b\u63a8\u7406\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u5305\u62ec\u6df7\u5408\u4f18\u5316\u3001\u81ea\u9002\u5e94\u52a8\u6001\u7b56\u7565\u4ee5\u53ca\u8f6f\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u3002"}}
{"id": "2508.06403", "categories": ["cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2508.06403", "abs": "https://arxiv.org/abs/2508.06403", "authors": ["Yining Zhang", "Ivan Kulesh", "Sebastiaan L. D. ten Haaf", "Nick van Loo", "Francesco Zatelli", "Tijl Degroote", "Christian G. Prosko", "Srijit Goswami"], "title": "Gate reflectometry in a minimal Kitaev chain device", "comment": "15 pages, 4 main figures, 5 supplementary figures", "summary": "Hybrid quantum dot (QD)-superconductor system can be used to realize Majorana\nzero modes in artificial Kitaev chains. These chains provide a promising\nplatform for the realization of Majorana qubits. Radio-frequency (RF) gate\nreflectometry is a fast, non-invasive, and sensitive technique that can be used\nto read out such qubits. In this work, we use gate reflectometry to probe two\nQDs coupled via a semiconductor-superconductor hybrid segment. We demonstrate\nthat gate sensing can resolve charge stability diagrams and clearly distinguish\nbetween elastic cotunneling and crossed-Andreev reflection, the two key\nprocesses that allow one to form a Kitaev chain. Furthermore, we show that this\ninformation is accessible, even when the system is completely decoupled from\nthe from the normal leads. In this closed regime, we show that the observed\nquantum capacitance signal is indicative of parity switching between the even\nand odd ground states. Our measurements in both open and closed regimes confirm\nthat gate reflectometry captures the essential features of interdot coupling\nand parity dynamics.", "AI": {"tldr": "RF gate reflectometry successfully probed coupled QDs in a hybrid system, resolving key processes for Majorana qubits and demonstrating parity switching in a closed regime.", "motivation": "To realize Majorana zero modes in artificial Kitaev chains using hybrid quantum dot (QD)-superconductor systems for potential applications in Majorana qubits, and to use RF gate reflectometry for reading out these qubits.", "method": "Radio-frequency (RF) gate reflectometry was used to probe two quantum dots (QDs) coupled via a semiconductor-superconductor hybrid segment.", "result": "The study demonstrated that gate sensing can resolve charge stability diagrams and differentiate between elastic cotunneling and crossed-Andreev reflection. It also showed that this information is accessible in a closed regime, where quantum capacitance signals indicate parity switching between even and odd ground states.", "conclusion": "Gate reflectometry can resolve charge stability diagrams and distinguish between elastic cotunneling and crossed-Andreev reflection, which are key processes for forming a Kitaev chain. This information is accessible even when the system is decoupled from normal leads. In the closed regime, the quantum capacitance signal indicates parity switching between even and odd ground states, confirming that gate reflectometry captures essential features of interdot coupling and parity dynamics."}}
{"id": "2508.05736", "categories": ["quant-ph", "cond-mat.quant-gas", "cond-mat.str-el", "hep-lat", "hep-th"], "pdf": "https://arxiv.org/pdf/2508.05736", "abs": "https://arxiv.org/abs/2508.05736", "authors": ["Yizhuo Tian", "N. S. Srivatsa", "Kaidi Xu", "Jesse J. Osborne", "Umberto Borla", "Jad C. Halimeh"], "title": "Role of Plaquette Term in Genuine $2+1$D String Dynamics on Quantum Simulators", "comment": "$10+9$ pages, $4+12$ figures", "summary": "With the advent of quantum simulators of $2+1$D lattice gauge theories\n(LGTs), a fundamental open question is under what circumstances the observed\nphysics is genuinely $2+1$D rather than effectively $1+1$D. Here, we address\nthis question in the ongoing strong effort to quantum-simulate string dynamics\nin $2+1$D LGTs on state-of-the-art quantum hardware. Through tensor network\nsimulations and analytic derivations, we show that the plaquette term, which\nrepresents a magnetic field and only emerges in $d>1$ spatial dimensions, plays\na crucial role in \\textit{genuine} $2+1$D string dynamics deep in the confined\nregime. In its absence and for minimal-length (Manhattan-distance) strings, we\ndemonstrate how string breaking, although on a lattice in $d=2$ spatial\ndimensions, can be effectively mapped to a $1+1$D dynamical process\nindependently of lattice geometry. Our findings not only answer the question of\nwhat qualifies as genuine $2+1$D string dynamics, but also serve as a clear\nguide for future quantum simulation experiments of $2+1$D LGTs.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u5f20\u91cf\u7f51\u7edc\u6a21\u62df\u548c\u89e3\u6790\u63a8\u5bfc\uff0c\u7814\u7a76\u4e86 $2+1$D \u683c\u5b50\u89c4\u8303\u7406\u8bba (LGT) \u4e2d\u5b57\u7b26\u4e32\u52a8\u529b\u5b66\u7684\u91cf\u5b50\u6a21\u62df\u3002\u7814\u7a76\u8868\u660e\uff0cplaquette \u9879\u5bf9\u4e8e\u771f\u6b63 $2+1$D \u7684\u7ea6\u675f\u533a\u57df\u52a8\u529b\u5b66\u81f3\u5173\u91cd\u8981\u3002\u5728 plaquette \u9879\u7f3a\u5931\u7684\u60c5\u51b5\u4e0b\uff0c\u5b57\u7b26\u4e32\u65ad\u88c2\u53ef\u4ee5\u6709\u6548\u5730\u6620\u5c04\u5230 $1+1$D \u7684\u52a8\u529b\u5b66\u8fc7\u7a0b\u3002", "motivation": "\u968f\u7740 $2+1$D LGT \u7684\u91cf\u5b50\u6a21\u62df\u5668\u7684\u51fa\u73b0\uff0c\u4e00\u4e2a\u57fa\u672c\u9057\u7559\u95ee\u9898\u662f\u5728\u4ec0\u4e48\u60c5\u51b5\u4e0b\u89c2\u5bdf\u5230\u7684\u7269\u7406\u5b66\u786e\u5b9e\u662f $2+1$D \u800c\u4e0d\u662f\u6709\u6548\u5730 $1+1$D\u3002", "method": "\u901a\u8fc7\u5f20\u91cf\u7f51\u7edc\u6a21\u62df\u548c\u89e3\u6790\u63a8\u5bfc\uff0c\u7814\u7a76\u4e86 plaquette \u9879\u5728 $2+1$D LGT \u7684\u771f\u6b63 $2+1$D \u52a8\u529b\u5b66\u4e2d\u7684\u4f5c\u7528\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0cplaquette \u9879\u5728\u771f\u6b63 $2+1$D \u7684\u7ea6\u675f\u533a\u57df\u52a8\u529b\u5b66\u4e2d\u8d77\u7740\u5173\u952e\u4f5c\u7528\u3002\u5728 plaquette \u9879\u7f3a\u5931\u7684\u60c5\u51b5\u4e0b\uff0c\u4ee5\u53ca\u5bf9\u4e8e\u6700\u5c0f\u957f\u5ea6\u7684\u5b57\u7b26\u4e32\uff0c\u7814\u7a76\u5c55\u793a\u4e86\u5b57\u7b26\u4e32\u65ad\u88c2\u53ef\u4ee5\u6709\u6548\u5730\u6620\u5c04\u5230 $1+1$D \u7684\u52a8\u529b\u5b66\u8fc7\u7a0b\u3002", "conclusion": "\u8be5\u7814\u7a76\u56de\u7b54\u4e86\u5728\u4ec0\u4e48\u60c5\u51b5\u4e0b $2+1$D \u683c\u5b50\u89c4\u8303\u7406\u8bba (LGT) \u7684\u6a21\u62df\u7269\u7406\u5b66\u786e\u5b9e\u662f $2+1$D \u800c\u4e0d\u662f\u6709\u6548\u5730 $1+1$D \u7684\u95ee\u9898\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u91cf\u5b50\u6a21\u62df\u5b9e\u9a8c\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2508.05843", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.05843", "abs": "https://arxiv.org/abs/2508.05843", "authors": ["Miles Gilberti", "Shane Storks", "Huteng Dai"], "title": "Discovering Properties of Inflectional Morphology in Neural Emergent Communication", "comment": null, "summary": "Emergent communication (EmCom) with deep neural network-based agents promises\nto yield insights into the nature of human language, but remains focused\nprimarily on a few subfield-specific goals and metrics that prioritize\ncommunication schemes which represent attributes with unique characters\none-to-one and compose them syntactically. We thus reinterpret a common EmCom\nsetting, the attribute-value reconstruction game, by imposing a\nsmall-vocabulary constraint to simulate double articulation, and formulating a\nnovel setting analogous to naturalistic inflectional morphology (enabling\nmeaningful comparison to natural language communication schemes). We develop\nnew metrics and explore variations of this game motivated by real properties of\ninflectional morphology: concatenativity and fusionality. Through our\nexperiments, we discover that simulated phonological constraints encourage\nconcatenative morphology, and emergent languages replicate the tendency of\nnatural languages to fuse grammatical attributes.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u5f15\u5165\u5c0f\u8bcd\u6c47\u91cf\u7ea6\u675f\u548c\u6a21\u62df\u5c48\u6298\u5f62\u6001\u5b66\uff0c\u6539\u8fdb\u4e86 EmCom \u7814\u7a76\u7684\u8bbe\u7f6e\uff0c\u5e76\u53d1\u73b0\u6a21\u62df\u7684\u97f3\u7cfb\u7ea6\u675f\u4f1a\u4fc3\u8fdb\u8fde\u63a5\u5f62\u6001\uff0c\u800c\u65b0\u5174\u8bed\u8a00\u4f1a\u878d\u5408\u8bed\u6cd5\u5c5e\u6027\u3002", "motivation": "\u73b0\u6709 EmCom \u7814\u7a76\u8fc7\u4e8e\u5173\u6ce8\u7279\u5b9a\u5b50\u9886\u57df\u7684\u76ee\u6807\u548c\u6307\u6807\uff0c\u4f8b\u5982\u4e00\u5bf9\u4e00\u5730\u8868\u793a\u5c5e\u6027\u5e76\u8fdb\u884c\u53e5\u6cd5\u7ec4\u5408\uff0c\u8fd9\u672a\u80fd\u5145\u5206\u53cd\u6620\u4eba\u7c7b\u8bed\u8a00\u7684\u590d\u6742\u6027\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u548c\u8bbe\u7f6e\u6765\u66f4\u6df1\u5165\u5730\u7406\u89e3\u8bed\u8a00\u7684\u6d8c\u73b0\u673a\u5236\u3002", "method": "\u6211\u4eec\u91cd\u65b0\u8be0\u91ca\u4e86\u5c5e\u6027-\u503c\u91cd\u5efa\u6e38\u620f\uff0c\u5f15\u5165\u4e86\u5c0f\u8bcd\u6c47\u91cf\u7ea6\u675f\u4ee5\u6a21\u62df\u53cc\u91cd\u6784\u6210\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7c7b\u4f3c\u4e8e\u81ea\u7136\u5c48\u6298\u5f62\u6001\u5b66\u7684\u8bbe\u7f6e\u3002\u6211\u4eec\u8fd8\u5f00\u53d1\u4e86\u65b0\u7684\u6307\u6807\uff0c\u5e76\u63a2\u7d22\u4e86\u53d7\u5c48\u6298\u5f62\u6001\u5b66\u7279\u6027\u7684\u542f\u53d1\u7684\u6e38\u620f\u53d8\u4f53\uff0c\u5982\u8fde\u63a5\u6027\u548c\u878d\u5408\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u6a21\u62df\u7684\u97f3\u7cfb\u7ea6\u675f\u80fd\u591f\u4fc3\u8fdb\u8fde\u63a5\u5f62\u6001\u7684\u51fa\u73b0\u3002\u6b64\u5916\uff0c\u5b9e\u9a8c\u4e2d\u7684\u65b0\u5174\u8bed\u8a00\u4e5f\u8868\u73b0\u51fa\u4e0e\u81ea\u7136\u8bed\u8a00\u76f8\u4f3c\u7684\u8d8b\u52bf\uff0c\u5373\u878d\u5408\u4e86\u8bed\u6cd5\u5c5e\u6027\u3002", "conclusion": "\u901a\u8fc7\u6a21\u62df\u53cc\u91cd\u6784\u6210\u548c\u5f62\u6001\u5b66\u5c48\u6298\uff0c\u6211\u4eec\u4e3a\u65b0\u5174\u4ea4\u6d41\uff08EmCom\uff09\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u66f4\u63a5\u8fd1\u81ea\u7136\u8bed\u8a00\u7684\u89c6\u89d2\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u8bc4\u4f30\u6307\u6807\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u6a21\u62df\u7684\u97f3\u7cfb\u7ea6\u675f\u4f1a\u4fc3\u8fdb\u8fde\u63a5\u5f62\u6001\uff0c\u800c\u65b0\u5174\u8bed\u8a00\u4f1a\u4f53\u73b0\u51fa\u878d\u5408\u8bed\u6cd5\u5c5e\u6027\u7684\u81ea\u7136\u8bed\u8a00\u8d8b\u52bf\u3002"}}
{"id": "2508.05783", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05783", "abs": "https://arxiv.org/abs/2508.05783", "authors": ["Mengyu Li", "Guoyao Shen", "Chad W. Farris", "Xin Zhang"], "title": "Few-Shot Deployment of Pretrained MRI Transformers in Brain Imaging Tasks", "comment": "30 pages, 8 figures, 7 tables", "summary": "Machine learning using transformers has shown great potential in medical\nimaging, but its real-world applicability remains limited due to the scarcity\nof annotated data. In this study, we propose a practical framework for the\nfew-shot deployment of pretrained MRI transformers in diverse brain imaging\ntasks. By utilizing the Masked Autoencoder (MAE) pretraining strategy on a\nlarge-scale, multi-cohort brain MRI dataset comprising over 31 million slices,\nwe obtain highly transferable latent representations that generalize well\nacross tasks and datasets. For high-level tasks such as classification, a\nfrozen MAE encoder combined with a lightweight linear head achieves\nstate-of-the-art accuracy in MRI sequence identification with minimal\nsupervision. For low-level tasks such as segmentation, we propose MAE-FUnet, a\nhybrid architecture that fuses multiscale CNN features with pretrained MAE\nembeddings. This model consistently outperforms other strong baselines in both\nskull stripping and multi-class anatomical segmentation under data-limited\nconditions. With extensive quantitative and qualitative evaluations, our\nframework demonstrates efficiency, stability, and scalability, suggesting its\nsuitability for low-resource clinical environments and broader neuroimaging\napplications.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528MAE\u9884\u8bad\u7ec3Transformer\u7684\u5c11\u6837\u672c\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u533b\u5b66\u6210\u50cf\u4e2d\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\u3002\u8be5\u6846\u67b6\u5728\u5206\u7c7b\u548c\u5206\u5272\u4efb\u52a1\u4e0a\u5747\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u8d44\u6e90\u6709\u9650\u7684\u4e34\u5e8a\u73af\u5883\u3002", "motivation": "\u5c3d\u7ba1\u57fa\u4e8eTransformer\u7684\u673a\u5668\u5b66\u4e60\u5728\u533b\u5b66\u6210\u50cf\u9886\u57df\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u6807\u6ce8\u6570\u636e\u7684\u7a00\u7f3a\u6027\u9650\u5236\u4e86\u5176\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u8bad\u7ec3\u7684MRI Transformer\u5728\u591a\u79cd\u8111\u6210\u50cf\u4efb\u52a1\u4e2d\u7684\u5c11\u6837\u672c\u90e8\u7f72\u3002\u5229\u7528\u63a9\u7801\u81ea\u7f16\u7801\u5668\uff08MAE\uff09\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u5728\u5927\u89c4\u6a21\u3001\u591a\u961f\u5217\u8111\u90e8MRI\u6570\u636e\u96c6\uff08\u5305\u542b\u8d85\u8fc73100\u4e07\u5f20\u5207\u7247\uff09\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u83b7\u5f97\u4e86\u5177\u6709\u826f\u597d\u6cdb\u5316\u6027\u7684\u6f5c\u5728\u8868\u793a\u3002\u5bf9\u4e8e\u5206\u7c7b\u7b49\u9ad8\u5c42\u4efb\u52a1\uff0c\u51bb\u7ed3\u7684MAE\u7f16\u7801\u5668\u7ed3\u5408\u8f7b\u91cf\u7ea7\u7ebf\u6027\u5934\u90e8\uff0c\u5728\u76d1\u7763\u4fe1\u53f7\u6781\u5c11\u7684\u60c5\u51b5\u4e0b\uff0c\u5728MRI\u5e8f\u5217\u8bc6\u522b\u65b9\u9762\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u51c6\u786e\u7387\u3002\u5bf9\u4e8e\u5206\u5272\u7b49\u4f4e\u5c42\u4efb\u52a1\uff0c\u63d0\u51fa\u4e86MAE-FUnet\u6df7\u5408\u67b6\u6784\uff0c\u878d\u5408\u4e86\u591a\u5c3a\u5ea6CNN\u7279\u5f81\u548c\u9884\u8bad\u7ec3\u7684MAE\u5d4c\u5165\u3002", "result": "MAE-FUnet\u6a21\u578b\u5728\u6570\u636e\u53d7\u9650\u6761\u4ef6\u4e0b\uff0c\u5728\u9885\u9aa8\u5265\u79bb\u548c\u591a\u7c7b\u89e3\u5256\u5206\u5272\u4efb\u52a1\u4e2d\uff0c\u6301\u7eed\u4f18\u4e8e\u5176\u4ed6\u5f3a\u5927\u7684\u57fa\u7ebf\u6a21\u578b\u3002\u51bb\u7ed3\u7684MAE\u7f16\u7801\u5668\u7ed3\u5408\u8f7b\u91cf\u7ea7\u7ebf\u6027\u5934\u90e8\u5728MRI\u5e8f\u5217\u8bc6\u522b\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u6570\u636e\u7a00\u758f\u7684\u60c5\u51b5\u4e0b\uff0c\u80fd\u591f\u5b9e\u73b0\u9ad8\u6548\u3001\u7a33\u5b9a\u548c\u53ef\u6269\u5c55\u7684\u5c11\u6837\u672c\u5b66\u4e60\uff0c\u9002\u7528\u4e8e\u4f4e\u8d44\u6e90\u4e34\u5e8a\u73af\u5883\u548c\u795e\u7ecf\u5f71\u50cf\u5b66\u5e94\u7528\u3002"}}
{"id": "2508.05946", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.05946", "abs": "https://arxiv.org/abs/2508.05946", "authors": ["Nello Balossino", "Rossana Damiano", "Cristina Gena", "Alberto Lillo", "Anna Maria Marras", "Claudio Mattutino", "Antonio Pizzo", "Alessia Prin", "Fabiana Vernero"], "title": "Social and Telepresence Robots for Accessibility and Inclusion in Small Museums", "comment": null, "summary": "There are still many museums that present accessibility barriers,\nparticularly regarding perceptual, cultural, and cognitive aspects. This is\nespecially evident in low-density population areas. The aim of the ROBSO-PM\nproject is to improve the accessibility of small museums through the use of\nsocial robots and social telepresence robots, focusing on three museums as case\nstudies: the Museum of the Holy Shroud in Turin, a small but globally known\ninstitution, and two lesser known mountain museums: the Museum of the Champlas\ndu Col Carnival and the Pragelato Museum of Alpine Peoples' Costumes and\nTraditions. The project explores two main applications for robots: as guides\nsupporting inclusive visits for foreign or disabled visitors, and as\ntelepresence tools allowing people with limited mobility to access museums\nremotely. From a research perspective, key topics include storytelling, robot\npersonality, empathy, personalization, and, in the case of telepresence,\ncollaboration between the robot and the person, with clearly defined roles and\nautonomy.", "AI": {"tldr": "ROBSO-PM\u9879\u76ee\u5229\u7528\u793e\u4ea4\u673a\u5668\u4eba\u548c\u8fdc\u7a0b\u5448\u73b0\u673a\u5668\u4eba\u6539\u5584\u5c0f\u578b\u535a\u7269\u9986\uff08\u5c24\u5176\u662f\u4f4e\u4eba\u53e3\u5bc6\u5ea6\u5730\u533a\uff09\u7684\u65e0\u969c\u788d\u6027\uff0c\u5173\u6ce8\u5305\u5bb9\u6027\u53c2\u89c2\u548c\u8fdc\u7a0b\u8bbf\u95ee\uff0c\u5e76\u7814\u7a76\u6545\u4e8b\u53d9\u8ff0\u3001\u673a\u5668\u4eba\u4e2a\u6027\u548c\u5171\u60c5\u7b49\u95ee\u9898\u3002", "motivation": "\u8bb8\u591a\u535a\u7269\u9986\u4ecd\u5b58\u5728\u65e0\u969c\u788d\u969c\u788d\uff0c\u7279\u522b\u662f\u5728\u611f\u77e5\u3001\u6587\u5316\u548c\u8ba4\u77e5\u65b9\u9762\uff0c\u8fd9\u5728\u4f4e\u4eba\u53e3\u5bc6\u5ea6\u5730\u533a\u5c24\u4e3a\u660e\u663e\u3002ROBSO-PM \u9879\u76ee\u65e8\u5728\u901a\u8fc7\u4f7f\u7528\u793e\u4ea4\u673a\u5668\u4eba\u548c\u8fdc\u7a0b\u5448\u73b0\u673a\u5668\u4eba\u6765\u6539\u5584\u5c0f\u578b\u535a\u7269\u9986\u7684\u53ef\u8bbf\u95ee\u6027\u3002", "method": "\u8be5\u9879\u76ee\uff08ROBSO-PM\uff09\u5c06\u5229\u7528\u793e\u4ea4\u673a\u5668\u4eba\u548c\u8fdc\u7a0b\u5448\u73b0\u673a\u5668\u4eba\u6765\u89e3\u51b3\u5c0f\u578b\u535a\u7269\u9986\u5728\u611f\u77e5\u3001\u6587\u5316\u548c\u8ba4\u77e5\u65b9\u9762\u5b58\u5728\u7684\u65e0\u969c\u788d\u969c\u788d\u3002\u5c06\u5bf9\u4e09\u4e2a\u535a\u7269\u9986\u8fdb\u884c\u6848\u4f8b\u7814\u7a76\uff1a\u90fd\u7075\u5723\u5bb9\u535a\u7269\u9986\u3001Champlas du Col Carnival\u535a\u7269\u9986\u548cPragelato\u9ad8\u5c71\u6c11\u65cf\u670d\u9970\u4e0e\u4f20\u7edf\u535a\u7269\u9986\u3002\u9879\u76ee\u5c06\u63a2\u7d22\u673a\u5668\u4eba\u4f5c\u4e3a\u5411\u5bfc\uff08\u652f\u6301\u5916\u56fd\u6216\u6b8b\u75be\u6e38\u5ba2\u7684\u5305\u5bb9\u6027\u53c2\u89c2\uff09\u548c\u8fdc\u7a0b\u5448\u73b0\u5de5\u5177\uff08\u5141\u8bb8\u884c\u52a8\u4e0d\u4fbf\u8005\u8fdc\u7a0b\u8bbf\u95ee\u535a\u7269\u9986\uff09\u7684\u4e24\u79cd\u4e3b\u8981\u5e94\u7528\u3002", "result": "\u8be5\u9879\u76ee\u5c06\u63a2\u7d22\u673a\u5668\u4eba\u4f5c\u4e3a\u5411\u5bfc\uff08\u652f\u6301\u5305\u5bb9\u6027\u8bbf\u95ee\uff09\u548c\u8fdc\u7a0b\u5448\u73b0\u5de5\u5177\uff08\u63d0\u4f9b\u8fdc\u7a0b\u535a\u7269\u9986\u8bbf\u95ee\uff09\u7684\u5e94\u7528\u3002\u7814\u7a76\u7684\u5173\u952e\u4e3b\u9898\u5305\u62ec\u6545\u4e8b\u53d9\u8ff0\u3001\u673a\u5668\u4eba\u4e2a\u6027\u548c\u5171\u60c5\uff0c\u4ee5\u53ca\u5728\u8fdc\u7a0b\u5448\u73b0\u65b9\u9762\u673a\u5668\u4eba\u4e0e\u4eba\u4e4b\u95f4\u7684\u534f\u4f5c\u3002", "conclusion": "\u8be5\u9879\u76ee\u65e8\u5728\u901a\u8fc7\u793e\u4ea4\u673a\u5668\u4eba\u548c\u8fdc\u7a0b\u5448\u73b0\u673a\u5668\u4eba\u6765\u6539\u5584\u5c0f\u578b\u535a\u7269\u9986\u7684\u53ef\u8bbf\u95ee\u6027\uff0c\u7279\u522b\u5173\u6ce8\u611f\u77e5\u3001\u6587\u5316\u548c\u8ba4\u77e5\u65b9\u9762\u3002\u9879\u76ee\u63a2\u7d22\u4e86\u673a\u5668\u4eba\u4f5c\u4e3a\u5411\u5bfc\u548c\u8fdc\u7a0b\u5448\u73b0\u5de5\u5177\u7684\u5e94\u7528\uff0c\u5e76\u7814\u7a76\u4e86\u6545\u4e8b\u53d9\u8ff0\u3001\u673a\u5668\u4eba\u4e2a\u6027\u548c\u5171\u60c5\u7b49\u5173\u952e\u4e3b\u9898\u3002"}}
{"id": "2508.05888", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.05888", "abs": "https://arxiv.org/abs/2508.05888", "authors": ["Sahil Bansal", "Sai Shruthi Sistla", "Aarti Arikatala", "Sebastian Schreiber"], "title": "Planning Agents on an Ego-Trip: Leveraging Hybrid Ego-Graph Ensembles for Improved Tool Retrieval in Enterprise Task Planning", "comment": null, "summary": "Effective tool retrieval is essential for AI agents to select from a vast\narray of tools when identifying and planning actions in the context of complex\nuser queries. Despite its central role in planning, this aspect remains\nunderexplored in the literature. Traditional approaches rely primarily on\nsimilarities between user queries and tool descriptions, which significantly\nlimits retrieval accuracy, specifically when handling multi-step user requests.\nTo address these limitations, we propose a Knowledge Graph (KG)-based tool\nretrieval framework that captures the semantic relationships between tools and\ntheir functional dependencies. Our retrieval algorithm leverages ensembles of\n1-hop ego tool graphs to model direct and indirect connections between tools,\nenabling more comprehensive and contextual tool selection for multi-step tasks.\nWe evaluate our approach on a synthetically generated internal dataset across\nsix defined user classes, extending previous work on coherent dialogue\nsynthesis and too retrieval benchmarks. Results demonstrate that our tool\ngraph-based method achieves 91.85% tool coverage on the micro-average Complete\nRecall metric, compared to 89.26% for re-ranked semantic-lexical hybrid\nretrieval, the strongest non-KG baseline in our experiments. These findings\nsupport our hypothesis that the structural information in the KG provides\ncomplementary signals to pure similarity matching, particularly for queries\nrequiring sequential tool composition.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u77e5\u8bc6\u56fe\uff08KG\uff09\u7684\u5de5\u5177\u68c0\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u5efa\u6a21\u5de5\u5177\u4e4b\u95f4\u7684\u8bed\u4e49\u5173\u7cfb\u548c\u529f\u80fd\u4f9d\u8d56\u6027\uff0c\u63d0\u9ad8\u4e86AI\u4ee3\u7406\u5728\u5904\u7406\u590d\u6742\u3001\u591a\u6b65\u7528\u6237\u8bf7\u6c42\u65f6\u7684\u5de5\u5177\u9009\u62e9\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u7528\u6237\u67e5\u8be2\u548c\u5de5\u5177\u63cf\u8ff0\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\uff0c\u8fd9\u5728\u5904\u7406\u591a\u6b65\u7528\u6237\u8bf7\u6c42\u65f6\uff0c\u4f1a\u4e25\u91cd\u9650\u5236\u68c0\u7d22\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u77e5\u8bc6\u56fe\uff08KG\uff09\u7684\u5de5\u5177\u68c0\u7d22\u6846\u67b6\uff0c\u5229\u75281\u8df3\u5de5\u5177\u56fe\u7684\u96c6\u5408\u6765\u5efa\u6a21\u5de5\u5177\u4e4b\u95f4\u76f4\u63a5\u548c\u95f4\u63a5\u7684\u8054\u7cfb\uff0c\u4ee5\u8fdb\u884c\u591a\u6b65\u4efb\u52a1\u7684\u66f4\u5168\u9762\u3001\u66f4\u5177\u4e0a\u4e0b\u6587\u7684\u5de5\u5177\u9009\u62e9\u3002", "result": "\u57fa\u4e8e\u5de5\u5177\u56fe\u7684\u65b9\u6cd5\u5728\u5fae\u5e73\u5747\u5b8c\u6574\u53ec\u56de\u7387\u6307\u6807\u4e0a\u8fbe\u5230\u4e8691.85%\u7684\u5de5\u5177\u8986\u76d6\u7387\uff0c\u800c\u4f5c\u4e3a\u6700\u5f3a\u7684\u975eKG\u57fa\u7ebf\u65b9\u6cd5\u7684\u91cd\u6392\u8bed\u4e49-\u8bcd\u6c47\u6df7\u5408\u68c0\u7d22\u8fbe\u5230\u4e8689.26%\u3002", "conclusion": "\u7528\u6237\u5bf9\u5de5\u5177\u56fe\u7684\u5047\u8bbe\u5f97\u5230\u4e86\u652f\u6301\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u987a\u5e8f\u5de5\u5177\u7ec4\u5408\u7684\u67e5\u8be2\u4e2d\uff0cKG\u7684\u7ed3\u6784\u4fe1\u606f\u63d0\u4f9b\u4e86\u4e0e\u7eaf\u7cb9\u76f8\u4f3c\u6027\u5339\u914d\u4e92\u8865\u7684\u4fe1\u53f7\u3002"}}
{"id": "2508.05836", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.05836", "abs": "https://arxiv.org/abs/2508.05836", "authors": ["Rituparna Datta", "Nibir Chandra Mandal"], "title": "An Effective Approach for Node Classification in Textual Graphs", "comment": null, "summary": "Textual Attribute Graphs (TAGs) are critical for modeling complex networks\nlike citation networks, but effective node classification remains challenging\ndue to difficulties in integrating rich semantics from text with structural\ngraph information. Existing methods often struggle with capturing nuanced\ndomain-specific terminology, modeling long-range dependencies, adapting to\ntemporal evolution, and scaling to massive datasets. To address these issues,\nwe propose a novel framework that integrates TAPE (Text-Attributed Graph\nRepresentation Enhancement) with Graphormer. Our approach leverages a large\nlanguage model (LLM), specifically ChatGPT, within the TAPE framework to\ngenerate semantically rich explanations from paper content, which are then\nfused into enhanced node representations. These embeddings are combined with\nstructural features using a novel integration layer with learned attention\nweights. Graphormer's path-aware position encoding and multi-head attention\nmechanisms are employed to effectively capture long-range dependencies across\nthe citation network. We demonstrate the efficacy of our framework on the\nchallenging ogbn-arxiv dataset, achieving state-of-the-art performance with a\nclassification accuracy of 0.772, significantly surpassing the best GCN\nbaseline of 0.713. Our method also yields strong results in precision (0.671),\nrecall (0.577), and F1-score (0.610). We validate our approach through\ncomprehensive ablation studies that quantify the contribution of each\ncomponent, demonstrating the synergy between semantic and structural\ninformation. Our framework provides a scalable and robust solution for node\nclassification in dynamic TAGs, offering a promising direction for future\nresearch in knowledge systems and scientific discovery.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408ChatGPT\u548cGraphormer\u7684\u65b0\u9896\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u6587\u672c\u5c5e\u6027\u56fe\u4e2d\u7684\u8282\u70b9\u5206\u7c7b\u95ee\u9898\uff0c\u901a\u8fc7\u878d\u5408\u4e30\u5bcc\u7684\u6587\u672c\u8bed\u4e49\u548c\u56fe\u7ed3\u6784\u4fe1\u606f\uff0c\u5728ogbn-arxiv\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u8282\u70b9\u5206\u7c7b\u65b9\u6cd5\u5728\u6574\u5408\u6587\u672c\u8bed\u4e49\u548c\u56fe\u7ed3\u6784\u4fe1\u606f\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u96be\u4ee5\u5904\u7406\u7ec6\u5fae\u7684\u9886\u57df\u7279\u5b9a\u672f\u8bed\u3001\u5efa\u6a21\u8fdc\u7a0b\u4f9d\u8d56\u5173\u7cfb\u3001\u9002\u5e94\u65f6\u95f4\u6f14\u5316\u4ee5\u53ca\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u6269\u5c55\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6574\u5408TAPE\uff08\u6587\u672c\u5c5e\u6027\u56fe\u8868\u793a\u589e\u5f3a\uff09\u548cGraphormer\u7684\u65b0\u9896\u6846\u67b6\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u4e86ChatGPT\uff0c\u5e76\u5728TAPE\u6846\u67b6\u5185\u751f\u6210\u4e86\u6765\u81ea\u8bba\u6587\u5185\u5bb9\u7684\u8bed\u4e49\u4e30\u5bcc\u7684\u89e3\u91ca\uff0c\u8fd9\u4e9b\u89e3\u91ca\u88ab\u878d\u5408\u5230\u589e\u5f3a\u7684\u8282\u70b9\u8868\u793a\u4e2d\u3002\u8fd9\u4e9b\u5d4c\u5165\u4e0e\u7ed3\u6784\u7279\u5f81\u76f8\u7ed3\u5408\uff0c\u5e76\u901a\u8fc7\u5177\u6709\u5b66\u4e60\u6ce8\u610f\u6743\u91cd\u7684\u65b0\u9896\u96c6\u6210\u5c42\u3002Graphormer\u7684\u8def\u5f84\u611f\u77e5\u4f4d\u7f6e\u7f16\u7801\u548c\u591a\u5934\u6ce8\u610f\u673a\u5236\u88ab\u7528\u4e8e\u6709\u6548\u6355\u83b7\u5f15\u7528\u7f51\u7edc\u4e2d\u7684\u8fdc\u7a0b\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "\u5728ogbn-arxiv\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5206\u7c7b\u51c6\u786e\u7387\u8fbe\u5230\u4e860.772\uff0c\u663e\u8457\u8d85\u8fc7\u4e86\u6700\u4f73GCN\u57fa\u7ebf0.713\u3002\u5728\u7cbe\u786e\u7387\uff080.671\uff09\u3001\u53ec\u56de\u7387\uff080.577\uff09\u548cF1\u5206\u6570\uff080.610\uff09\u65b9\u9762\u4e5f\u53d6\u5f97\u4e86\u5f3a\u52b2\u7684\u7ed3\u679c\u3002\u901a\u8fc7\u5168\u9762\u7684\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\uff0c\u91cf\u5316\u4e86\u6bcf\u4e2a\u7ec4\u4ef6\u7684\u8d21\u732e\uff0c\u8bc1\u660e\u4e86\u8bed\u4e49\u548c\u7ed3\u6784\u4fe1\u606f\u4e4b\u95f4\u7684\u534f\u540c\u4f5c\u7528\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u52a8\u6001\u6587\u672c\u5c5e\u6027\u56fe\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u5f3a\u5927\u7684\u8282\u70b9\u5206\u7c7b\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u77e5\u8bc6\u7cfb\u7edf\u548c\u79d1\u5b66\u53d1\u73b0\u7684\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u5e0c\u671b\u7684\u65b9\u5411\u3002"}}
{"id": "2508.06345", "categories": ["cs.CL", "cs.AI", "cs.GR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06345", "abs": "https://arxiv.org/abs/2508.06345", "authors": ["Yanbin Wei", "Jiangyue Yan", "Chun Kang", "Yang Chen", "Hua Liu", "James T. Kwok", "Yu Zhang"], "title": "Harnessing Adaptive Topology Representations for Zero-Shot Graph Question Answering", "comment": null, "summary": "Large Multimodal Models (LMMs) have shown generalized zero-shot capabilities\nin diverse domain question-answering (QA) tasks, including graph QA that\ninvolves complex graph topologies. However, most current approaches use only a\nsingle type of graph representation, namely Topology Representation Form (TRF),\nsuch as prompt-unified text descriptions or style-fixed visual styles. Those\n\"one-size-fits-all\" approaches fail to consider the specific preferences of\ndifferent models or tasks, often leading to incorrect or overly long responses.\nTo address this, we first analyze the characteristics and weaknesses of\nexisting TRFs, and then design a set of TRFs, denoted by $F_{ZS}$, tailored to\nzero-shot graph QA. We then introduce a new metric, Graph Response Efficiency\n(GRE), which measures the balance between the performance and the brevity in\ngraph QA. Built on these, we develop the DynamicTRF framework, which aims to\nimprove both the accuracy and conciseness of graph QA. To be specific,\nDynamicTRF first creates a TRF Preference (TRFP) dataset that ranks TRFs based\non their GRE scores, to probe the question-specific TRF preferences. Then it\ntrains a TRF router on the TRFP dataset, to adaptively assign the best TRF from\n$F_{ZS}$ for each question during the inference. Extensive experiments across 7\nin-domain algorithmic graph QA tasks and 2 out-of-domain downstream tasks show\nthat DynamicTRF significantly enhances the zero-shot graph QA of LMMs in terms\nof accuracy", "AI": {"tldr": "DynamicTRF\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u6700\u9002\u5408\u7279\u5b9a\u95ee\u9898\u7684\u56fe\u8868\u793a\uff08TRF\uff09\uff0c\u63d0\u9ad8\u4e86\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMM\uff09\u5728\u96f6\u6837\u672c\u56fe\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u51c6\u786e\u6027\u548c\u54cd\u5e94\u7b80\u6d01\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u56feQA\u65b9\u6cd5\u4e3b\u8981\u4f7f\u7528\u5355\u4e00\u7c7b\u578b\u7684\u56fe\u8868\u793a\uff08TRF\uff09\uff0c\u5982\u7edf\u4e00\u6587\u672c\u63cf\u8ff0\u6216\u56fa\u5b9a\u89c6\u89c9\u6837\u5f0f\uff0c\u672a\u80fd\u8003\u8651\u4e0d\u540c\u6a21\u578b\u6216\u4efb\u52a1\u7684\u7279\u5b9a\u504f\u597d\uff0c\u5bfc\u81f4\u54cd\u5e94\u4e0d\u6b63\u786e\u6216\u5197\u957f\u3002\u4e3a\u89e3\u51b3\u6b64\u95ee\u9898\uff0c\u6211\u4eec\u5206\u6790\u4e86\u73b0\u6709TRF\u7684\u7279\u70b9\u548c\u5f31\u70b9\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u5957\u9488\u5bf9\u96f6\u6837\u672c\u56feQA\u7684TRF\uff08$F_{ZS}$\uff09\u3002", "method": "DynamicTRF\u6846\u67b6\u9996\u5148\u521b\u5efa\u4e00\u4e2aTRF\u504f\u597d\uff08TRFP\uff09\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u6839\u636eGRE\u5206\u6570\u5bf9TRF\u8fdb\u884c\u6392\u5e8f\uff0c\u4ee5\u63a2\u7a76\u7279\u5b9a\u4e8e\u95ee\u9898\u7684TRF\u504f\u597d\u3002\u7136\u540e\uff0c\u5b83\u5728TRFP\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u4e00\u4e2aTRF\u8def\u7531\u5668\uff0c\u4ee5\u4fbf\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u4e3a\u6bcf\u4e2a\u95ee\u9898\u81ea\u9002\u5e94\u5730\u5206\u914d$F_{ZS}$\u4e2d\u7684\u6700\u4f73TRF\u3002", "result": "\u57287\u4e2a\u57df\u5185\u7b97\u6cd5\u56feQA\u4efb\u52a1\u548c2\u4e2a\u57df\u5916\u4e0b\u6e38\u4efb\u52a1\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cDynamicTRF\u5728\u51c6\u786e\u6027\u65b9\u9762\u663e\u8457\u589e\u5f3a\u4e86LMM\u7684\u96f6\u6837\u672c\u56feQA\u80fd\u529b\u3002", "conclusion": "DynamicTRF\u6846\u67b6\u901a\u8fc7\u521b\u5efaTRF\u504f\u597d\uff08TRFP\uff09\u6570\u636e\u96c6\u6765\u8bc4\u4f30\u4e0d\u540cTRF\u7684\u4f18\u7f3a\u70b9\uff0c\u5e76\u8bad\u7ec3\u4e00\u4e2aTRF\u8def\u7531\u5668\u6765\u4e3a\u6bcf\u4e2a\u95ee\u9898\u81ea\u9002\u5e94\u5730\u5206\u914d\u6700\u4f73TRF\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u96f6\u6837\u672c\u56feQA\u7684\u51c6\u786e\u6027\u548c\u7b80\u6d01\u6027\u3002"}}
{"id": "2508.06114", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2508.06114", "abs": "https://arxiv.org/abs/2508.06114", "authors": ["Fatih Ilgaz", "Elizaveta Spetzler", "Patrick Wiegand", "Robert Rieger", "Jeffrey McCord", "Benjamin Spetzler"], "title": "Operation Regimes and Design Principles of Delta-E Effect Sensors", "comment": null, "summary": "Delta-E effect-based magnetoelectric sensors have emerged as promising\ntechnology for detecting weak magnetic fields at low frequencies. However, the\nperformance of such sensors remains difficult to predict, as signal and noise\ncharacteristics are dictated by interdependent parameters such as magnetic\nlayer geometry, magnetic microstructure, and loss. In this work, we present a\nsystematic experimental study of sub-mm-sized delta-E effect sensors,\ncomprising 24 device configurations that vary in magnetic layer thickness and\nlateral dimensions. The sensors are statistically analyzed to identify the\ninfluence of magnetic layer geometry on performance through a combination of\nmeasurements and simulations. Our findings reveal three distinct operation\nregimes - dominated by electronic noise, magnetic noise, and nonlinearities -\nwhose boundaries shift systematically with magnetic layer thickness. This\nregime behavior governs the trade-offs between sensitivity and noise,\nultimately determining the sensor's limit of detection. Based on these results,\nthe dependency of the regime boundaries on key device parameters is discussed\nin detail, providing fundamental insights for tailoring sensor performance. As\nsuch, this study establishes a necessary foundation for targeted performance\noptimization and the scalable design of advanced delta-E effect sensor systems.", "AI": {"tldr": "Delta-E \u6548\u5e94\u4f20\u611f\u5668\u7684\u6027\u80fd\u53d7\u51e0\u4f55\u7ed3\u6784\u5f71\u54cd\uff0c\u5b58\u5728\u4e09\u79cd\u5de5\u4f5c\u6a21\u5f0f\uff0c\u8fb9\u754c\u968f\u78c1\u5c42\u539a\u5ea6\u53d8\u5316\uff0c\u8fd9\u5f71\u54cd\u4e86\u7075\u654f\u5ea6\u548c\u566a\u58f0\u7684\u6743\u8861\u3002", "motivation": "Delta-E \u6548\u5e94\u4f20\u611f\u5668\u5728\u63a2\u6d4b\u5f31\u78c1\u573a\u65b9\u9762\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46\u5176\u6027\u80fd\u96be\u4ee5\u9884\u6d4b\uff0c\u56e0\u4e3a\u4fe1\u53f7\u548c\u566a\u58f0\u7279\u6027\u53d7\u76f8\u4e92\u4f9d\u8d56\u7684\u53c2\u6570\uff08\u5982\u78c1\u5c42\u51e0\u4f55\u7ed3\u6784\u3001\u78c1\u5fae\u7ed3\u6784\u548c\u635f\u8017\uff09\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u7ec4\u5408\u6d4b\u91cf\u548c\u6a21\u62df\uff0c\u5bf9\u4e9a\u6beb\u7c73\u7ea7 Delta-E \u6548\u5e94\u4f20\u611f\u5668\uff08\u5305\u62ec 24 \u79cd\u4e0d\u540c\u7684\u5668\u4ef6\u914d\u7f6e\uff09\u8fdb\u884c\u7cfb\u7edf\u6027\u5b9e\u9a8c\u7814\u7a76\uff0c\u4ee5\u8bc6\u522b\u78c1\u5c42\u51e0\u4f55\u7ed3\u6784\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u63ed\u793a\u4e86\u4e09\u79cd\u4e0d\u540c\u7684\u5de5\u4f5c\u6a21\u5f0f\u2014\u2014\u7531\u7535\u5b50\u566a\u58f0\u3001\u78c1\u566a\u58f0\u548c\u975e\u7ebf\u6027\u4e3b\u5bfc\u2014\u2014\u5176\u8fb9\u754c\u968f\u7740\u78c1\u5c42\u539a\u5ea6\u7684\u7cfb\u7edf\u6027\u53d8\u5316\u800c\u79fb\u52a8\u3002\u8fd9\u79cd\u6a21\u5f0f\u884c\u4e3a\u51b3\u5b9a\u4e86\u7075\u654f\u5ea6\u548c\u566a\u58f0\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u6700\u7ec8\u51b3\u5b9a\u4e86\u4f20\u611f\u5668\u7684\u63a2\u6d4b\u6781\u9650\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u4f18\u5316\u4f20\u611f\u5668\u6027\u80fd\u548c\u53ef\u6269\u5c55\u8bbe\u8ba1\u5148\u8fdb\u7684 Delta-E \u6548\u5e94\u4f20\u611f\u5668\u7cfb\u7edf\u5960\u5b9a\u4e86\u5fc5\u8981\u7684\u57fa\u7840\u3002"}}
{"id": "2508.06454", "categories": ["cs.AI", "cs.GT"], "pdf": "https://arxiv.org/pdf/2508.06454", "abs": "https://arxiv.org/abs/2508.06454", "authors": ["Joshua Caiata", "Ben Armstrong", "Kate Larson"], "title": "What Voting Rules Actually Do: A Data-Driven Analysis of Multi-Winner Voting", "comment": "41 pages", "summary": "Committee-selection problems arise in many contexts and applications, and\nthere has been increasing interest within the social choice research community\non identifying which properties are satisfied by different multi-winner voting\nrules. In this work, we propose a data-driven framework to evaluate how\nfrequently voting rules violate axioms across diverse preference distributions\nin practice, shifting away from the binary perspective of axiom satisfaction\ngiven by worst-case analysis. Using this framework, we analyze the relationship\nbetween multi-winner voting rules and their axiomatic performance under several\npreference distributions. We then show that neural networks, acting as voting\nrules, can outperform traditional rules in minimizing axiom violations. Our\nresults suggest that data-driven approaches to social choice can inform the\ndesign of new voting systems and support the continuation of data-driven\nresearch in social choice.", "AI": {"tldr": "\u4e00\u7bc7\u5173\u4e8e\u4f7f\u7528\u6570\u636e\u9a71\u52a8\u6846\u67b6\u8bc4\u4f30\u591a\u9009\u6295\u7968\u89c4\u5219\u53ca\u5176\u516c\u7406\u6027\u80fd\u7684\u8bba\u6587\uff0c\u5e76\u63d0\u51fa\u795e\u7ecf\u7f51\u7edc\u53ef\u4ee5\u4f5c\u4e3a\u4e00\u79cd\u6709\u524d\u666f\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "motivation": "\u4e3a\u4e86\u5728\u793e\u4f1a\u9009\u62e9\u7814\u7a76\u4e2d\u8bc6\u522b\u4e0d\u540c\u591a\u9009\u6295\u7968\u89c4\u5219\u6240\u6ee1\u8db3\u7684\u6027\u8d28\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7684\u6846\u67b6\u6765\u8bc4\u4f30\u6295\u7968\u89c4\u5219\u8fdd\u53cd\u516c\u7406\u7684\u9891\u7387\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u6570\u636e\u9a71\u52a8\u7684\u6846\u67b6\u6765\u8bc4\u4f30\u6295\u7968\u89c4\u5219\u5728\u4e0d\u540c\u504f\u597d\u5206\u5e03\u4e0b\u8fdd\u53cd\u516c\u7406\u7684\u9891\u7387\uff0c\u800c\u4e0d\u662f\u91c7\u7528\u6700\u574f\u60c5\u51b5\u5206\u6790\u3002", "result": "\u8bc1\u660e\u4e86\u4f5c\u4e3a\u6295\u7968\u89c4\u5219\u7684\u795e\u7ecf\u7f51\u7edc\u53ef\u4ee5\u4f18\u4e8e\u4f20\u7edf\u89c4\u5219\uff0c\u5728\u6700\u5c0f\u5316\u516c\u7406\u51b2\u7a81\u65b9\u9762\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\u53ef\u4ee5\u8bbe\u8ba1\u51fa\u6bd4\u4f20\u7edf\u65b9\u6cd5\u66f4\u80fd\u4f18\u5316\u6295\u7968\u89c4\u5219\u3001\u51cf\u5c11\u516c\u7406\u51b2\u7a81\u7684\u65b0\u578b\u6295\u7968\u7cfb\u7edf\u3002"}}
{"id": "2508.06110", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.06110", "abs": "https://arxiv.org/abs/2508.06110", "authors": ["Yiran Rex Ma"], "title": "PanelTR: Zero-Shot Table Reasoning Framework Through Multi-Agent Scientific Discussion", "comment": "Accepted at IJCNN 2025", "summary": "Table reasoning, including tabular QA and fact verification, often depends on\nannotated data or complex data augmentation, limiting flexibility and\ngeneralization. LLMs, despite their versatility, often underperform compared to\nsimple supervised models. To approach these issues, we introduce PanelTR, a\nframework utilizing LLM agent scientists for robust table reasoning through a\nstructured scientific approach. PanelTR's workflow involves agent scientists\nconducting individual investigations, engaging in self-review, and\nparticipating in collaborative peer-review discussions. This process, driven by\nfive scientist personas, enables semantic-level transfer without relying on\ndata augmentation or parametric optimization. Experiments across four\nbenchmarks show that PanelTR outperforms vanilla LLMs and rivals fully\nsupervised models, all while remaining independent of training data. Our\nfindings indicate that structured scientific methodology can effectively handle\ncomplex tasks beyond table reasoning with flexible semantic understanding in a\nzero-shot context.", "AI": {"tldr": "PanelTR\u5229\u7528AI\u79d1\u5b66\u5bb6\u8fdb\u884c\u8868\u683c\u63a8\u7406\uff0c\u65e0\u9700\u6570\u636e\u589e\u5f3a\uff0c\u6548\u679c\u4f18\u4e8e\u57fa\u7840LLM\uff0c\u5ab2\u7f8e\u76d1\u7763\u6a21\u578b\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u8868\u683c\u63a8\u7406\uff08\u5305\u62ec\u8868\u683c\u95ee\u7b54\u548c\u4e8b\u5b9e\u6838\u67e5\uff09\u5bf9\u6807\u6ce8\u6570\u636e\u6216\u590d\u6742\u6570\u636e\u589e\u5f3a\u7684\u4f9d\u8d56\u6027\uff0c\u4ee5\u53caLLM\u5728\u6b64\u7c7b\u4efb\u52a1\u4e0a\u7684\u6f5c\u5728\u4e0d\u8db3\uff0c\u63d0\u51faPanelTR\u6846\u67b6\u3002", "method": "PanelTR\u6846\u67b6\u5229\u7528LLM\u9a71\u52a8\u7684\u4ee3\u7406\u79d1\u5b66\u5bb6\uff0c\u901a\u8fc7\u4e2a\u4f53\u7814\u7a76\u3001\u81ea\u6211\u8bc4\u5ba1\u548c\u540c\u4f34\u8bc4\u5ba1\u6765\u6267\u884c\u8868\u683c\u63a8\u7406\u4efb\u52a1\uff0c\u6a21\u62df\u4e86\u79d1\u5b66\u7814\u7a76\u7684\u6d41\u7a0b\uff0c\u5e76\u4e14\u4e0d\u4f9d\u8d56\u6570\u636e\u589e\u5f3a\u6216\u53c2\u6570\u4f18\u5316\u3002", "result": "PanelTR\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7840LLM\uff0c\u5e76\u80fd\u5ab2\u7f8e\u5b8c\u5168\u76d1\u7763\u6a21\u578b\uff0c\u540c\u65f6\u4e0d\u4f9d\u8d56\u8bad\u7ec3\u6570\u636e\u3002", "conclusion": "PanelTR\u5c55\u793a\u4e86\u7ed3\u6784\u5316\u79d1\u5b66\u65b9\u6cd5\u5728\u96f6\u6837\u672c\u60c5\u5883\u4e0b\u5904\u7406\u590d\u6742\u4efb\u52a1\uff08\u5305\u62ec\u8868\u683c\u63a8\u7406\uff09\u7684\u6709\u6548\u6027\uff0c\u5c55\u73b0\u4e86\u7075\u6d3b\u7684\u8bed\u4e49\u7406\u89e3\u80fd\u529b\uff0c\u5e76\u4e14\u4f18\u4e8e\u7b80\u5355\u76d1\u7763\u6a21\u578b\u3002"}}
{"id": "2508.06247", "categories": ["cs.LG", "cs.DS", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.06247", "abs": "https://arxiv.org/abs/2508.06247", "authors": ["Zichun Ye", "Runqi Wang", "Xutong Liu", "Shuai Li"], "title": "Near-Optimal Regret for Efficient Stochastic Combinatorial Semi-Bandits", "comment": null, "summary": "The combinatorial multi-armed bandit (CMAB) is a cornerstone of sequential\ndecision-making framework, dominated by two algorithmic families: UCB-based and\nadversarial methods such as follow the regularized leader (FTRL) and online\nmirror descent (OMD). However, prominent UCB-based approaches like CUCB suffer\nfrom additional regret factor $\\log T$ that is detrimental over long horizons,\nwhile adversarial methods such as EXP3.M and HYBRID impose significant\ncomputational overhead. To resolve this trade-off, we introduce the\nCombinatorial Minimax Optimal Strategy in the Stochastic setting (CMOSS). CMOSS\nis a computationally efficient algorithm that achieves an instance-independent\nregret of $O\\big( (\\log k)^2\\sqrt{kmT}\\big )$ under semi-bandit feedback, where\n$m$ is the number of arms and $k$ is the maximum cardinality of a feasible\naction. Crucially, this result eliminates the dependency on $\\log T$ and\nmatches the established $\\Omega\\big( \\sqrt{kmT}\\big)$ lower bound up to\n$O\\big((\\log k)^2\\big)$. We then extend our analysis to show that CMOSS is also\napplicable to cascading feedback. Experiments on synthetic and real-world\ndatasets validate that CMOSS consistently outperforms benchmark algorithms in\nboth regret and runtime efficiency.", "AI": {"tldr": "CMOSS\u7b97\u6cd5\u901a\u8fc7\u5728\u968f\u673a\u8bbe\u7f6e\u4e0b\u5b9e\u73b0\u66f4\u4f18\u7684\u9057\u61be\u754c\u9650\u548c\u66f4\u4f4e\u7684\u8ba1\u7b97\u5f00\u9500\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u7ec4\u5408\u591a\u81c2\u8001\u864e\u673a\u7b97\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709UCB\u7c7b\u7b97\u6cd5\uff08\u5982CUCB\uff09\u5728\u957f\u5468\u671f\u4e0b\u5177\u6709$\\\\log T$\u7684\u9057\u61be\u56e0\u5b50\u4ee5\u53ca\u5bf9\u6297\u7c7b\u7b97\u6cd5\uff08\u5982FTRL\u3001OMD\u3001EXP3.M\u3001HYBRID\uff09\u8ba1\u7b97\u5f00\u9500\u5927\u7684\u95ee\u9898\uff0c\u63d0\u51faCMOSS\u7b97\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCMOSS\uff08Combinatorial Minimax Optimal Strategy in the Stochastic setting\uff09\u7684\u7b97\u6cd5\u3002", "result": "CMOSS\u7b97\u6cd5\u5728\u534a\u8001\u864e\u673a\u53cd\u9988\u4e0b\u5b9e\u73b0\u4e86$O\\big( (\\log k)^2\\sqrt{kmT}\\big )$\u7684\u5b9e\u4f8b\u65e0\u5173\u9057\u61be\u754c\u9650\uff0c\u5e76\u4e14\u5728\u7ea7\u8054\u53cd\u9988\u4e0b\u540c\u6837\u9002\u7528\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eCMOSS\u5728\u9057\u61be\u548c\u8fd0\u884c\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u7b97\u6cd5\u3002", "conclusion": "CMOSS\u7b97\u6cd5\u5728\u968f\u673a\u8bbe\u7f6e\u4e0b\u5b9e\u73b0\u4e86\u5b9e\u4f8b\u65e0\u5173\u7684\u9057\u61be\u754c\u9650 $O\big( (\\log k)^2\\sqrt{kmT}\\big )$\uff0c\u6d88\u9664\u4e86\u5bf9 $\\log T$ \u7684\u4f9d\u8d56\uff0c\u5e76\u5339\u914d\u4e86\u5df2\u77e5\u7684 $\\Omega\\big( \\sqrt{kmT}\\big)$ \u4e0b\u754c\uff08\u76f8\u5dee $O\\big((\\log k)^2\\big)$\uff09\u3002\u6b64\u5916\uff0cCMOSS\u4e5f\u9002\u7528\u4e8e\u7ea7\u8054\u53cd\u9988\uff0c\u5e76\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u7684\u5b9e\u9a8c\u4e2d\uff0c\u5728\u9057\u61be\u548c\u8fd0\u884c\u65f6\u6548\u7387\u65b9\u9762\u5747\u4f18\u4e8e\u57fa\u51c6\u7b97\u6cd5\u3002"}}
{"id": "2508.06141", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.06141", "abs": "https://arxiv.org/abs/2508.06141", "authors": ["Marco Bertuletti", "Yichao Zhang", "Mahdi Abdollahpour", "Samuel Riedel", "Alessandro Vanelli-Coralli"], "title": "Fast End-to-End Simulation and Exploration of Many-RISCV-Core Baseband Transceivers for Software-Defined Radio-Access Networks", "comment": "7 pages", "summary": "The fast-rising demand for wireless bandwidth requires rapid evolution of\nhigh-performance baseband processing infrastructure. Programmable many-core\nprocessors for software-defined radio (SDR) have emerged as high-performance\nbaseband processing engines, offering the flexibility required to capture\nevolving wireless standards and technologies. This trend must be supported by a\ndesign framework enabling functional validation and end-to-end performance\nanalysis of SDR hardware within realistic radio environment models. We propose\na static binary translation based simulator augmented with a fast, approximate\ntiming model of the hardware and coupled to wireless channel models to simulate\nthe most performance-critical physical layer functions implemented in software\non a many (1024) RISC-V cores cluster customized for SDR. Our framework\nsimulates the detection of a 5G OFDM-symbol on a server-class processor in\n9.5s-3min, on a single thread, depending on the input MIMO size (three orders\nof magnitude faster than RTL simulation). The simulation is easily parallelized\nto 128 threads with 73-121x speedup compared to a single thread.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cdSDR\u4eff\u771f\u6846\u67b6\uff0c\u53ef\u4ee5\u52a0\u901f\u57fa\u5e26\u5904\u7406\u529f\u80fd\u4eff\u771f\uff0c\u6bd4RTL\u4eff\u771f\u5feb\u4e09\u500d\u3002", "motivation": "\u65e0\u7ebf\u5e26\u5bbd\u9700\u6c42\u7684\u5feb\u901f\u589e\u957f\u9700\u8981\u9ad8\u6027\u80fd\u57fa\u5e26\u5904\u7406\u57fa\u7840\u8bbe\u65bd\u7684\u5feb\u901f\u53d1\u5c55\u3002SDR\u7684\u53ef\u7f16\u7a0b\u591a\u6838\u5904\u7406\u5668\u4f5c\u4e3a\u9ad8\u6027\u80fd\u57fa\u5e26\u5904\u7406\u5f15\u64ce\u51fa\u73b0\uff0c\u63d0\u4f9b\u4e86\u6355\u83b7\u4e0d\u65ad\u53d8\u5316\u7684\u65e0\u7ebf\u6807\u51c6\u548c\u6280\u672f\u7684\u7075\u6d3b\u6027\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u4e2a\u8bbe\u8ba1\u6846\u67b6\u6765\u5b9e\u73b0SDR\u786c\u4ef6\u5728\u5b9e\u9645\u65e0\u7ebf\u73af\u5883\u6a21\u578b\u4e2d\u7684\u529f\u80fd\u9a8c\u8bc1\u548c\u7aef\u5230\u7aef\u6027\u80fd\u5206\u6790\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9759\u6001\u4e8c\u8fdb\u5236\u7ffb\u8bd1\u7684\u6a21\u62df\u5668\uff0c\u5e76\u8f85\u4ee5\u5feb\u901f\u3001\u8fd1\u4f3c\u7684\u786c\u4ef6\u65f6\u5e8f\u6a21\u578b\uff0c\u5e76\u4e0e\u65e0\u7ebf\u4fe1\u9053\u6a21\u578b\u8026\u5408\uff0c\u5bf9\u5728\u53ef\u5b9a\u5236\u4e3aSDR\u76841024\u6838RISC-V\u6838\u96c6\u7fa4\u4e0a\u4ee5\u8f6f\u4ef6\u5b9e\u73b0\u7684\u901a\u4fe1\u7269\u7406\u5c42\u51fd\u6570\u8fdb\u884c\u4eff\u771f\u3002", "result": "\u8be5\u6846\u67b6\u80fd\u591f\u4eff\u771f\u670d\u52a1\u5668\u7ea7\u5904\u7406\u5668\u4e0a5G OFDM\u7b26\u53f7\u7684\u68c0\u6d4b\uff0c\u4eff\u771f\u65f6\u95f4\u57289.5\u79d2\u52303\u5206\u949f\u4e4b\u95f4\uff08\u53d6\u51b3\u4e8e\u8f93\u5165MIMO\u5927\u5c0f\uff09\uff0c\u6bd4RTL\u4eff\u771f\u5feb\u4e09\u4e2a\u6570\u91cf\u7ea7\u3002\u4eff\u771f\u53ef\u4ee5\u8f7b\u677e\u5730\u5e76\u884c\u5316\u5230128\u4e2a\u7ebf\u7a0b\uff0c\u4e0e\u5355\u7ebf\u7a0b\u76f8\u6bd4\u5177\u670973-121\u500d\u7684\u52a0\u901f\u3002", "conclusion": "SDR\u786c\u4ef6\u7684\u4eff\u771f\u6846\u67b6\u53ef\u4ee5\u6781\u5927\u5730\u52a0\u901f\u7269\u7406\u5c42\u529f\u80fd\u7684\u4eff\u771f\uff0c\u901f\u5ea6\u6bd4RTL\u4eff\u771f\u5feb\u4e09\u500d\u3002"}}
{"id": "2508.06339", "categories": ["cs.DC", "cs.MS"], "pdf": "https://arxiv.org/pdf/2508.06339", "abs": "https://arxiv.org/abs/2508.06339", "authors": ["Evelyne Ringoot", "Rabab Alomairy", "Valentin Churavy", "Alan Edelman"], "title": "Performant Unified GPU Kernels for Portable Singular Value Computation Across Hardware and Precision", "comment": "12 pages, 6 figures, 4 tables", "summary": "This paper presents a portable, GPU-accelerated implementation of a QR-based\nsingular value computation algorithm in Julia. The singular value ecomposition\n(SVD) is a fundamental numerical tool in scientific computing and machine\nlearning, providing optimal low-rank matrix approximations. Its importance has\nincreased even more in large-scale machine learning pipelines, including large\nlanguage models (LLMs), where it enables low-rank adaptation (LoRA). The\nimplemented algorithm is based on the classic two-stage QR reduction,\nconsisting of successive matrix reduction to band form and bidiagonal form. Our\nimplementation leverages Julia's multiple dispatch and metaprogramming\ncapabilities, integrating with the GPUArrays and KernelAbstractions frameworks\nto provide a unified type and hardware-agnostic function. It supports diverse\nGPU architectures and data types, and is, to our knowledge, the first\nGPU-accelerated singular value implementation to support Apple Metal GPUs and\nhalf precision. Performance results on multiple GPU backends and data types\ndemonstrate that portability does not require sacrificing performance: the\nunified function outperforms most linear algebra libraries (MAGMA, SLATE,\nrocSOLVER, oneMKL) for matrix sizes larger than 1024x1024, and achieves 80%-90%\nof the performance of cuSOLVER for large matrices.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86Julia\u4e2d\u4e00\u79cd\u53ef\u79fb\u690d\u3001GPU\u52a0\u901f\u7684QR\u5947\u5f02\u503c\u5206\u89e3\u7b97\u6cd5\u5b9e\u73b0\uff0c\u8be5\u5b9e\u73b0\u652f\u6301Apple Metal GPU\u548c\u534a\u7cbe\u5ea6\u8ba1\u7b97\uff0c\u5e76\u5728\u5927\u89c4\u6a21\u77e9\u9635\u8ba1\u7b97\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u5947\u5f02\u503c\u5206\u89e3\uff08SVD\uff09\u5728\u79d1\u5b66\u8ba1\u7b97\u548c\u673a\u5668\u5b66\u4e60\uff08\u7279\u522b\u662f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u4f4e\u79e9\u81ea\u9002\u5e94\uff09\u4e2d\u662f\u57fa\u7840\u6027\u7684\u6570\u503c\u5de5\u5177\uff0c\u56e0\u6b64\u9700\u8981\u9ad8\u6548\u4e14\u53ef\u79fb\u690d\u7684\u5b9e\u73b0\u3002", "method": "\u8be5\u5b9e\u73b0\u57fa\u4e8e\u7ecf\u5178\u7684QR\u5206\u89e3\u4e24\u9636\u6bb5\u7ea6\u7b80\u65b9\u6cd5\uff0c\u9010\u6b65\u5c06\u77e9\u9635\u7ea6\u7b80\u4e3a\u5e26\u72b6\u5f62\u5f0f\u548c\u53cc\u5bf9\u89d2\u5f62\u5f0f\uff0c\u5e76\u5229\u7528Julia\u7684GPUArrays\u548cKernelAbstractions\u6846\u67b6\u8fdb\u884cGPU\u52a0\u901f\u3002", "result": "\u6240\u63d0\u51fa\u7684\u5b9e\u73b0\u5c55\u793a\u4e86\u826f\u597d\u7684\u53ef\u79fb\u690d\u6027\u548c\u6027\u80fd\uff0c\u5728\u591a\u79cdGPU\u540e\u7aef\u548c\u6570\u636e\u7c7b\u578b\u4e0a\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c\u5bf9\u4e8e\u5927\u4e8e1024x1024\u7684\u77e9\u9635\u5c3a\u5bf8\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u5927\u591a\u6570\u7ebf\u6027\u4ee3\u6570\u5e93\uff08MAGMA\u3001SLATE\u3001rocSOLVER\u3001oneMKL\uff09\uff0c\u5e76\u8fbe\u5230\u4e86cuSOLVER\u6027\u80fd\u768480%-90%\u3002", "conclusion": "\u8be5\u5b9e\u73b0\u901a\u8fc7\u5229\u7528Julia\u7684\u591a\u79cd\u6d3e\u53d1\u548c\u5143\u7f16\u7a0b\u529f\u80fd\uff0c\u5e76\u4e0eGPUArrays\u548cKernelAbstractions\u6846\u67b6\u96c6\u6210\uff0c\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u7c7b\u578b\u548c\u786c\u4ef6\u65e0\u5173\u7684\u51fd\u6570\uff0c\u652f\u6301\u5404\u79cdGPU\u67b6\u6784\u548c\u6570\u636e\u7c7b\u578b\uff0c\u5e76\u4e14\u662f\u9996\u4e2a\u652f\u6301Apple Metal GPU\u548c\u534a\u7cbe\u5ea6\u8ba1\u7b97\u7684GPU\u52a0\u901f\u5947\u5f02\u503c\u5206\u89e3\u5b9e\u73b0\u3002"}}
{"id": "2508.06464", "categories": ["cond-mat.mes-hall", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2508.06464", "abs": "https://arxiv.org/abs/2508.06464", "authors": ["Iftakhar Bin Elius", "Nathan Valadez", "Gyanendra Dhakal", "Volodymyr Buturlim", "Sabin Regmi", "Dante James", "Peter Radanovich", "Matthew Yankowitz", "Tetiana Romanova", "Andrzej Ptok", "Krzysztof Gofryk", "Dariusz Kaczorowski", "Madhab Neupane"], "title": "Observation of momentum dependent charge density wave gap in EuTe4", "comment": "9 pages, 4 figures", "summary": "The occurrence of charge density wave (CDW) phenomena, particularly in low\ndimensional rare-earth chalcogenides, has attracted substantial research\ninterest. Among these materials, EuTe4, which features multiple Te layers and a\nsingle Eu-Te layer, serves as a promising platform to study the interplay\nbetween CDW order and 4f electron configurations, including magnetism. In this\nstudy, First principles based density functional theory (DFT) calculations were\ncarried out to investigate the electronic band structure modifications arising\nfrom CDW modulation. Angle resolved photoemission spectroscopy (ARPES) revealed\nthe emergence of a CDW gap at the Fermi level, as well as hybridization induced\ngap features at lower binding energies. The low lying CDW gap reaches its\nmaximum along the Gamma-Y high-symmetry direction and a minimum along GX\nreflecting the anisotropic nature of the electronic structure. We also\nperformed low temperature heat capacity measurements in applied magnetic fields\nnear the Neel temperature (TN ~ 6.9 K) to construct the magnetic phase diagram\nof EuTe4. This study provides valuable insight into the directional dependent\nevolution of the Fermi surface nesting induced CDW ordering, along with other\nobserved gap openings within this system.", "AI": {"tldr": "EuTe4\u7684DFT\u8ba1\u7b97\u548cARPES\u5b9e\u9a8c\u63ed\u793a\u4e86CDW\u80fd\u9699\u548c\u6742\u5316\u8bf1\u5bfc\u80fd\u9699\uff0c\u5e76\u8bc1\u5b9e\u4e86\u5176\u5404\u5411\u5f02\u6027\u3002\u4f4e\u6e29\u70ed\u5bb9\u6d4b\u91cf\u6784\u5efa\u4e86EuTe4\u7684\u78c1\u76f8\u56fe\u3002", "motivation": "EuTe4\u4f5c\u4e3a\u4e00\u79cd\u5177\u6709\u591aTe\u5c42\u548c\u5355Eu-Te\u5c42\u7684\u4f4e\u7ef4\u7a00\u571f\u786b\u5c5e\u5316\u7269\uff0c\u662f\u7814\u7a76CDW\u6709\u5e8f\u4e0e4f\u7535\u5b50\u6784\u578b\uff08\u5305\u62ec\u78c1\u6027\uff09\u4e4b\u95f4\u76f8\u4e92\u4f5c\u7528\u7684\u6709\u5e0c\u671b\u7684\u5e73\u53f0\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u7b2c\u4e00\u6027\u539f\u7406\u7684\u5bc6\u5ea6\u6cdb\u51fd\u7406\u8bba\uff08DFT\uff09\u8ba1\u7b97\u7814\u7a76\u4e86CDW\u8c03\u5236\u5f15\u8d77\u7684\u7535\u5b50\u80fd\u5e26\u7ed3\u6784\u53d8\u5316\uff0c\u5e76\u7ed3\u5408\u89d2\u5ea6\u5206\u8fa8\u5149\u7535\u5b50\u80fd\u8c31\uff08ARPES\uff09\u63ed\u793a\u4e86\u8d39\u7c73\u80fd\u7ea7\u5904\u7684CDW\u80fd\u9699\u4ee5\u53ca\u8f83\u4f4e\u7ed3\u5408\u80fd\u5904\u7684\u6742\u5316\u8bf1\u5bfc\u80fd\u9699\u7279\u5f81\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u5728\u8fd1\u5948\u5c14\u6e29\u5ea6\uff08TN ~ 6.9 K\uff09\u4e0b\u65bd\u52a0\u78c1\u573a\u8fdb\u884c\u4f4e\u6e29\u70ed\u5bb9\u6d4b\u91cf\uff0c\u6784\u5efa\u4e86EuTe4\u7684\u78c1\u76f8\u56fe\u3002", "result": "ARPES\u5b9e\u9a8c\u663e\u793a\u8d39\u7c73\u80fd\u7ea7\u51fa\u73b0\u4e86CDW\u80fd\u9699\uff0c\u5e76\u5728\u8f83\u4f4e\u7ed3\u5408\u80fd\u5904\u51fa\u73b0\u4e86\u6742\u5316\u8bf1\u5bfc\u7684\u80fd\u9699\u7279\u5f81\u3002CDW\u80fd\u9699\u5728\u0393-Y\u9ad8\u5bf9\u79f0\u65b9\u5411\u8fbe\u5230\u6700\u5927\u503c\uff0c\u5728GX\u65b9\u5411\u8fbe\u5230\u6700\u5c0f\u503c\uff0c\u4f53\u73b0\u4e86\u7535\u5b50\u7ed3\u6784\u7684\u5404\u5411\u5f02\u6027\u3002\u4f4e\u6e29\u67d4\u78c1\u6027\u6d4b\u91cf\u7ed3\u679c\u6210\u529f\u6784\u5efa\u4e86EuTe4\u7684\u78c1\u76f8\u56fe\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u4f9b\u4e86\u5bf9EuTe4\u7cfb\u7edf\u4e2d\u5b9a\u5411\u4f9d\u8d56\u7684\u8d39\u7c73\u8868\u9762\u5d4c\u5957\u8bf1\u5bfc\u7684CDW\u6709\u5e8f\u4ee5\u53ca\u5176\u4ed6\u89c2\u6d4b\u5230\u7684\u80fd\u9699\u5f00\u5ea6\u5177\u6709\u4ef7\u503c\u7684\u8ba4\u8bc6\u3002"}}
{"id": "2508.05738", "categories": ["quant-ph", "cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2508.05738", "abs": "https://arxiv.org/abs/2508.05738", "authors": ["Norman Hogan", "Efekan K\u00f6kc\u00fc", "Thomas Steckmann", "Liam P. Doak", "Carlos Mejuto-Zaera", "Daan Camps", "Roel Van Beeumen", "Wibe A. de Jong", "A. F. Kemper"], "title": "A quantum computing approach to efficiently simulating correlated materials using impurity models and dynamical mean field theory", "comment": "24 pages, 28 figures", "summary": "The accurate theoretical description of materials with strongly correlated\nelectrons is a formidable challenge, at the forefront of condensed matter\nphysics and computational chemistry alike, and it is one of the targets for\nquantum computing. Dynamical Mean Field Theory (DMFT) is a successful approach\nthat predicts behaviors of such systems by incorporating some correlated\nbehavior, but it is limited by the need to calculate the Green's function for\nthe impurity model. This work proposes a framework for DMFT calculations on\nquantum computers, focusing on near-term applications. It leverages the\nstructure of the impurity problem, combining a low-rank Gaussian subspace\nrepresentation of the ground state and a compressed, short-depth quantum\ncircuit that joins the Gaussian state preparation with the time evolution to\ncompute the necessary Green's functions. We demonstrate the convergence of the\nDMFT algorithm using the Gaussian subspace in a noise-free setting, and show\nthe hardware viability of the circuit compression by extracting the impurity\nGreen's function on IBM quantum processors for a single impurity coupled to\nthree bath orbitals (8 physical qubits and 1 ancilla). We discuss the potential\npaths forward towards realizing this use case of quantum computing in materials\nscience.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5728\u91cf\u5b50\u8ba1\u7b97\u673a\u4e0a\u8fdb\u884cDMFT\u8ba1\u7b97\u7684\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u4f4e\u79e9\u9ad8\u65af\u5b50\u7a7a\u95f4\u548c\u538b\u7f29\u91cf\u5b50\u7535\u8def\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728IBM\u91cf\u5b50\u5904\u7406\u5668\u4e0a\u7684\u53ef\u884c\u6027\u3002", "motivation": "\u7cbe\u786e\u7684\u5f3a\u5173\u8054\u7535\u5b50\u6750\u6599\u7406\u8bba\u63cf\u8ff0\u662f\u51dd\u805a\u6001\u7269\u7406\u548c\u8ba1\u7b97\u5316\u5b66\u524d\u6cbf\u7684\u6311\u6218\uff0c\u4e5f\u662f\u91cf\u5b50\u8ba1\u7b97\u7684\u76ee\u6807\u4e4b\u4e00\u3002DMFT\u662f\u4e00\u79cd\u901a\u8fc7\u7ed3\u5408\u4e00\u4e9b\u5173\u8054\u884c\u4e3a\u6765\u9884\u6d4b\u6b64\u7c7b\u7cfb\u7edf\u884c\u4e3a\u7684\u6210\u529f\u65b9\u6cd5\uff0c\u4f46\u53d7\u9650\u4e8e\u8ba1\u7b97\u6742\u8d28\u6a21\u578b\u683c\u6797\u51fd\u6570\u7684\u9700\u8981\u3002", "method": "\u8be5\u6846\u67b6\u5229\u7528\u4e86\u6742\u8d28\u95ee\u9898\u7684\u7ed3\u6784\uff0c\u7ed3\u5408\u4e86\u4f4e\u79e9\u9ad8\u65af\u5b50\u7a7a\u95f4\u8868\u793a\u57fa\u6001\u548c\u538b\u7f29\u7684\u3001\u77ed\u6df1\u5ea6\u7684\u91cf\u5b50\u7535\u8def\uff0c\u5c06\u9ad8\u65af\u6001\u5236\u5907\u4e0e\u65f6\u95f4\u6f14\u5316\u76f8\u7ed3\u5408\uff0c\u4ee5\u8ba1\u7b97\u5fc5\u8981\u7684\u683c\u6797\u51fd\u6570\u3002", "result": "\u5728\u65e0\u566a\u58f0\u73af\u5883\u4e0b\uff0c\u672c\u7814\u7a76\u8bc1\u660e\u4e86\u4f7f\u7528\u9ad8\u65af\u5b50\u7a7a\u95f4\u8fdb\u884cDMFT\u7b97\u6cd5\u7684\u6536\u655b\u6027\uff0c\u5e76\u901a\u8fc7\u5728IBM\u91cf\u5b50\u5904\u7406\u5668\u4e0a\u63d0\u53d6\u5355\u6742\u8d28\u8026\u5408\u5230\u4e09\u4e2a\u6d74\u8f68\u9053\u7684\u683c\u6797\u51fd\u6570\uff088\u4e2a\u7269\u7406\u91cf\u5b50\u6bd4\u7279\u548c1\u4e2a\u8f85\u52a9\u91cf\u5b50\u6bd4\u7279\uff09\uff0c\u5c55\u793a\u4e86\u91cf\u5b50\u7535\u8def\u538b\u7f29\u7684\u786c\u4ef6\u53ef\u884c\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5728\u91cf\u5b50\u8ba1\u7b97\u673a\u4e0a\u8fdb\u884c\u52a8\u529b\u5b66\u5e73\u5747\u573a\u7406\u8bba\uff08DMFT\uff09\u8ba1\u7b97\u7684\u6846\u67b6\uff0c\u7279\u522b\u5173\u6ce8\u8fd1\u671f\u5e94\u7528\u3002\u8be5\u6846\u67b6\u5229\u7528\u4e86\u6742\u8d28\u95ee\u9898\u7684\u7ed3\u6784\uff0c\u7ed3\u5408\u4e86\u4f4e\u79e9\u9ad8\u65af\u5b50\u7a7a\u95f4\u8868\u793a\u57fa\u6001\u548c\u538b\u7f29\u7684\u3001\u77ed\u6df1\u5ea6\u7684\u91cf\u5b50\u7535\u8def\uff0c\u5c06\u9ad8\u65af\u6001\u5236\u5907\u4e0e\u65f6\u95f4\u6f14\u5316\u76f8\u7ed3\u5408\uff0c\u4ee5\u8ba1\u7b97\u5fc5\u8981\u7684\u683c\u6797\u51fd\u6570\u3002"}}
{"id": "2508.05880", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05880", "abs": "https://arxiv.org/abs/2508.05880", "authors": ["Sree Bhattacharyya", "Lucas Craig", "Tharun Dilliraj", "Jia Li", "James Z. Wang"], "title": "Do Machines Think Emotionally? Cognitive Appraisal Analysis of Large Language Models", "comment": null, "summary": "Affective Computing has been established as a crucial field of inquiry to\nadvance the holistic development of Artificial Intelligence (AI) systems.\nFoundation models -- especially Large Language Models (LLMs) -- have been\nevaluated, trained, or instruction-tuned in several past works, to become\nbetter predictors or generators of emotion. Most of these studies, however,\napproach emotion-related tasks in a supervised manner, assessing or training\nthe capabilities of LLMs using discrete emotion labels associated with stimuli\n(e.g., text, images, video, audio). Evaluation studies, in particular, have\noften been limited to standard and superficial emotion-related tasks, such as\nthe recognition of evoked or expressed emotions. In this paper, we move beyond\nsurface-level emotion tasks to investigate how LLMs reason about emotions\nthrough cognitive dimensions. Drawing from cognitive appraisal theory, we\nexamine whether LLMs produce coherent and plausible cognitive reasoning when\nreasoning about emotionally charged stimuli. We introduce a large-scale\nbenchmark on Cognitive Reasoning for Emotions - CoRE - to evaluate internal\ncognitive structures implicitly used by LLMs for emotional reasoning. Through a\nplethora of evaluation experiments and analysis, we seek to answer: (a) Are\nmodels more likely to implicitly rely on specific cognitive appraisal\ndimensions?, (b) What cognitive dimensions are important for characterizing\nspecific emotions?, and, (c) Can the internal representations of different\nemotion categories in LLMs be interpreted through cognitive appraisal\ndimensions? Our results and analyses reveal diverse reasoning patterns across\ndifferent LLMs. Our benchmark and code will be made publicly available.", "AI": {"tldr": "This paper explores how Large Language Models (LLMs) reason about emotions using cognitive dimensions, moving beyond traditional supervised methods. It introduces a benchmark (CoRE) to evaluate LLMs' internal cognitive structures for emotional reasoning, finding diverse patterns across models.", "motivation": "Most existing studies on LLMs and emotion use supervised methods with discrete emotion labels and focus on superficial tasks like emotion recognition. This paper aims to go beyond surface-level emotion tasks to explore how LLMs reason about emotions through cognitive dimensions.", "method": "The paper investigates LLMs' reasoning about emotions through cognitive dimensions, drawing from cognitive appraisal theory. It introduces a large-scale benchmark, CoRE, to evaluate internal cognitive structures used by LLMs for emotional reasoning through various evaluation experiments and analysis.", "result": "The paper's results and analyses reveal diverse reasoning patterns across different LLMs concerning their implicit reliance on cognitive appraisal dimensions, the importance of these dimensions for specific emotions, and the interpretability of internal representations of emotion categories.", "conclusion": "The study reveals diverse reasoning patterns across different LLMs regarding emotions through cognitive dimensions. The benchmark and code will be made publicly available."}}
{"id": "2508.05813", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05813", "abs": "https://arxiv.org/abs/2508.05813", "authors": ["Raphael Du Sablon", "David Hart"], "title": "Optimization-Free Style Transfer for 3D Gaussian Splats", "comment": null, "summary": "The task of style transfer for 3D Gaussian splats has been explored in many\nprevious works, but these require reconstructing or fine-tuning the splat while\nincorporating style information or optimizing a feature extraction network on\nthe splat representation. We propose a reconstruction- and optimization-free\napproach to stylizing 3D Gaussian splats. This is done by generating a graph\nstructure across the implicit surface of the splat representation. A\nfeed-forward, surface-based stylization method is then used and interpolated\nback to the individual splats in the scene. This allows for any style image and\n3D Gaussian splat to be used without any additional training or optimization.\nThis also allows for fast stylization of splats, achieving speeds under 2\nminutes even on consumer-grade hardware. We demonstrate the quality results\nthis approach achieves and compare to other 3D Gaussian splat style transfer\nmethods. Code is publicly available at\nhttps://github.com/davidmhart/FastSplatStyler.", "AI": {"tldr": "A new method stylizes 3D Gaussian splats quickly and easily by creating a graph on their surface, avoiding complex training or rebuilding steps.", "motivation": "To address the limitations of previous 3D Gaussian splat style transfer methods that require reconstruction or fine-tuning, and to enable fast stylization without additional training or optimization.", "method": "A reconstruction- and optimization-free approach using a graph structure across the implicit surface of the splat representation, with a feed-forward, surface-based stylization method interpolated back to individual splats.", "result": "The method allows any style image and 3D Gaussian splat to be used, achieving stylization speeds under 2 minutes on consumer-grade hardware, with demonstrated quality results.", "conclusion": "The proposed method achieves fast stylization of 3D Gaussian splats without reconstruction or optimization, using a graph structure on the implicit surface and a feed-forward, surface-based stylization method."}}
{"id": "2508.05972", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.05972", "abs": "https://arxiv.org/abs/2508.05972", "authors": ["Shaoting Liu", "Zhou Liu"], "title": "Dynamical Trajectory Planning of Disturbance Consciousness for Air-Land Bimodal Unmanned Aerial Vehicles", "comment": null, "summary": "Air-land bimodal vehicles provide a promising solution for navigating complex\nenvironments by combining the flexibility of aerial locomotion with the energy\nefficiency of ground mobility. To enhance the robustness of trajectory planning\nunder environmental disturbances, this paper presents a disturbance-aware\nplanning framework that incorporates real-time disturbance estimation into both\npath searching and trajectory optimization. A key component of the framework is\na disturbance-adaptive safety boundary adjustment mechanism, which dynamically\nmodifies the vehicle's feasible dynamic boundaries based on estimated\ndisturbances to ensure trajectory feasibility. Leveraging the dynamics model of\nthe bimodal vehicle, the proposed approach achieves adaptive and reliable\nmotion planning across different terrains and operating conditions. A series of\nreal-world experiments and benchmark comparisons on a custom-built platform\nvalidate the effectiveness and robustness of the method, demonstrating\nimprovements in tracking accuracy, task efficiency, and energy performance\nunder both ground and aerial disturbances.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u7a7a\u5730\u4e24\u6816\u8f66\u8f86\u7684\u5e72\u6270\u611f\u77e5\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u5b9e\u65f6\u4f30\u8ba1\u548c\u9002\u5e94\u5e72\u6270\u6765\u63d0\u9ad8\u8f68\u8ff9\u89c4\u5212\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u8f68\u8ff9\u89c4\u5212\u9c81\u68d2\u6027\uff0c\u5e94\u5bf9\u73af\u5883\u5e72\u6270\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5305\u542b\u5b9e\u65f6\u5e72\u6270\u4f30\u8ba1\u7684\u611f\u77e5\u89c4\u5212\u6846\u67b6\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u5e72\u6270\u81ea\u9002\u5e94\u5b89\u5168\u8fb9\u754c\u8c03\u6574\u673a\u5236\uff0c\u8be5\u673a\u5236\u80fd\u591f\u6839\u636e\u4f30\u8ba1\u7684\u5e72\u6270\u52a8\u6001\u4fee\u6539\u8f66\u8f86\u7684\u53ef\u884c\u52a8\u6001\u8fb9\u754c\uff0c\u4ee5\u4fdd\u8bc1\u8f68\u8ff9\u7684\u53ef\u884c\u6027\u3002", "result": "\u5b9e\u9a8c\u548c\u57fa\u51c6\u6d4b\u8bd5\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u8ddf\u8e2a\u7cbe\u5ea6\u3001\u4efb\u52a1\u6548\u7387\u548c\u80fd\u6e90\u6027\u80fd\u65b9\u9762\u5747\u6709\u6240\u63d0\u9ad8\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u8f66\u8f86\u7684\u53ef\u884c\u52a8\u6001\u8fb9\u754c\uff0c\u5b9e\u73b0\u4e86\u81ea\u9002\u5e94\u548c\u53ef\u9760\u7684\u8fd0\u52a8\u89c4\u5212\uff0c\u5e76\u5728\u5404\u79cd\u5730\u9762\u548c\u7a7a\u4e2d\u5e72\u6270\u4e0b\u5747\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.05996", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05996", "abs": "https://arxiv.org/abs/2508.05996", "authors": ["Kaitao Chen", "Mianxin Liu", "Daoming Zong", "Chaoyue Ding", "Shaohao Rui", "Yankai Jiang", "Mu Zhou", "Xiaosong Wang"], "title": "Mediator-Guided Multi-Agent Collaboration among Open-Source Models for Medical Decision-Making", "comment": "14 pages, 4 figures", "summary": "Complex medical decision-making involves cooperative workflows operated by\ndifferent clinicians. Designing AI multi-agent systems can expedite and augment\nhuman-level clinical decision-making. Existing multi-agent researches primarily\nfocus on language-only tasks, yet their extension to multimodal scenarios\nremains challenging. A blind combination of diverse vision-language models\n(VLMs) can amplify an erroneous outcome interpretation. VLMs in general are\nless capable in instruction following and importantly self-reflection, compared\nto large language models (LLMs) of comparable sizes. This disparity largely\nconstrains VLMs' ability in cooperative workflows. In this study, we propose\nMedOrch, a mediator-guided multi-agent collaboration framework for medical\nmultimodal decision-making. MedOrch employs an LLM-based mediator agent that\nenables multiple VLM-based expert agents to exchange and reflect on their\noutputs towards collaboration. We utilize multiple open-source general-purpose\nand domain-specific VLMs instead of costly GPT-series models, revealing the\nstrength of heterogeneous models. We show that the collaboration within\ndistinct VLM-based agents can surpass the capabilities of any individual agent.\nWe validate our approach on five medical vision question answering benchmarks,\ndemonstrating superior collaboration performance without model training. Our\nfindings underscore the value of mediator-guided multi-agent collaboration in\nadvancing medical multimodal intelligence. Our code will be made publicly\navailable.", "AI": {"tldr": "MedOrch\uff1a\u4e00\u4e2a\u7531LLM\u534f\u8c03\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u533b\u7597\u591a\u6a21\u6001\u51b3\u7b56\u534f\u4f5c\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u667a\u80fd\u4f53\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u8bed\u8a00\u4efb\u52a1\u4e0a\uff0c\u5c06\u5176\u6269\u5c55\u5230\u591a\u6a21\u6001\u573a\u666f\u9762\u4e34\u6311\u6218\uff0c\u5e76\u4e14VLM\u5728\u6307\u4ee4\u9075\u5faa\u548c\u81ea\u6211\u53cd\u601d\u65b9\u9762\u80fd\u529b\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u5176\u5728\u534f\u4f5c\u4e2d\u7684\u5e94\u7528\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u4ee5\u63d0\u5347\u533b\u7597\u591a\u6a21\u6001\u51b3\u7b56\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMedOrch\u7684\u534f\u8c03\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u4f7f\u7528\u57fa\u4e8eLLM\u7684\u534f\u8c03\u667a\u80fd\u4f53\u6765\u6307\u5bfc\u591a\u4e2a\u57fa\u4e8eVLM\u7684\u4e13\u5bb6\u667a\u80fd\u4f53\u8fdb\u884c\u8f93\u51fa\u4ea4\u6362\u548c\u53cd\u601d\uff0c\u4ee5\u5b9e\u73b0\u534f\u4f5c\u3002\u6846\u67b6\u91c7\u7528\u4e86\u591a\u79cd\u901a\u7528\u548c\u7279\u5b9a\u9886\u57df\u7684\u5f00\u6e90VLM\uff0c\u800c\u975e\u6602\u8d35\u7684GPT\u7cfb\u5217\u6a21\u578b\u3002", "result": "\u5728\u4e94\u4e2a\u533b\u7597\u89c6\u89c9\u95ee\u7b54\u57fa\u51c6\u4e0a\u9a8c\u8bc1\u4e86MedOrch\u7684\u6709\u6548\u6027\uff0c\u8bc1\u660e\u4e86\u5176\u534f\u4f5c\u6027\u80fd\u4f18\u4e8e\u4efb\u4f55\u5355\u4e00\u667a\u80fd\u4f53\uff0c\u5e76\u4e14\u65e0\u9700\u6a21\u578b\u8bad\u7ec3\u3002", "conclusion": "MedOrch\u6846\u67b6\u901a\u8fc7LLM\u9a71\u52a8\u7684\u534f\u8c03\uff0c\u5b9e\u73b0\u4e86\u591a\u6a21\u6001\u533b\u7597\u51b3\u7b56\u7684\u6709\u6548\u534f\u4f5c\uff0c\u8d85\u8d8a\u4e86\u5355\u4e00\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u65e0\u9700\u8fdb\u884c\u6a21\u578b\u8bad\u7ec3\uff0c\u5c55\u793a\u4e86\u5728\u63d0\u5347\u533b\u7597\u591a\u6a21\u6001\u667a\u80fd\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.06156", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2508.06156", "abs": "https://arxiv.org/abs/2508.06156", "authors": ["Liqi Wang", "Xuhe Gong", "Zicun Li", "Ruijuan Xiao", "Hong Li"], "title": "Revealing the Staging Structural Evolution and Li (De)Intercalation Kinetics in Graphite Anodes via Machine Learning Potential", "comment": null, "summary": "Revealing the dynamic structural evolution and lithium transport properties\nduring the charge/discharge processes is crucial for optimizing graphite anodes\nin lithium-ion batteries, enabling high stability and fast-charging\nperformance. However, the dynamic coupling mechanisms among carbon layer\nkinetics, lithium (de)intercalation/diffusion, and defects regulation remain\ninsufficiently understood. In this study, we developed a universal automated\nworkflow based on machine learning potentials to simulate the dynamic lithium\n(de)intercalation process. With this approach, the staging structural evolution\nof lithium-graphite intercalation compounds and their lithium transport\nbehavior were resolved through molecular dynamics simulations. By introducing\nstacking faults into the graphite structure, we successfully simulated stage\ntransitions driven by carbon layer sliding and reorganization, accompanied by\nstress release and structural stabilization. The dynamics of carbon layers\nregulate the lithium (de)intercalation positional selectivity, producing\nintermediate states with varying lithium concentrations and distributions\nduring cycling. This facilitates the formation and transformation of stage\nstructures while mitigating residual stress accumulation. A fundamental kinetic\nasymmetry arises between lithium intercalation and deintercalation, driven by\nthe continuous and heterogeneous lithium transport and carbon layer sliding\nduring charge/discharge processes. The carbon defects regulate lithium\ntransport, in which the atomic-scale defects confine intralayer lithium\ntransport and carbon sliding while enabling interlayer transport via dynamic\nlithium trapping/release mechanisms. Accordingly, for the future design, it is\ncritical to construct structural units with controllable carbon layer\nsliding/reorganization, and tunable defects to enhance lithium-ion transport.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u673a\u5668\u5b66\u4e60\u52bf\u80fd\u548c\u5206\u5b50\u52a8\u529b\u5b66\u6a21\u62df\uff0c\u63ed\u793a\u4e86\u9502\u79bb\u5b50\u5728\u77f3\u58a8\u4e2d\u7684\u52a8\u6001\u7ed3\u6784\u6f14\u53d8\u548c\u4f20\u8f93\u673a\u5236\uff0c\u4e3a\u4f18\u5316\u9502\u79bb\u5b50\u7535\u6c60\u8d1f\u6781\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\u3002", "motivation": "\u4e3a\u4e86\u4f18\u5316\u9502\u79bb\u5b50\u7535\u6c60\u77f3\u58a8\u8d1f\u6781\u7684\u7a33\u5b9a\u6027\u548c\u5feb\u5145\u6027\u80fd\uff0c\u9700\u8981\u63ed\u793a\u5176\u5728\u5145\u653e\u7535\u8fc7\u7a0b\u4e2d\u52a8\u6001\u7ed3\u6784\u6f14\u53d8\u548c\u9502\u79bb\u5b50\u4f20\u8f93\u6027\u8d28\uff0c\u4f46\u76ee\u524d\u5bf9\u78b3\u5c42\u52a8\u529b\u5b66\u3001\u9502\u79bb\u5b50\uff08\u8131\uff09\u5d4c\u5165/\u6269\u6563\u548c\u7f3a\u9677\u8c03\u63a7\u4e4b\u95f4\u7684\u52a8\u6001\u8026\u5408\u673a\u5236\u7406\u89e3\u4e0d\u8db3\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u52bf\u80fd\u7684\u901a\u7528\u81ea\u52a8\u5316\u5de5\u4f5c\u6d41\uff0c\u5229\u7528\u5206\u5b50\u52a8\u529b\u5b66\u6a21\u62df\u4e86\u9502\u79bb\u5b50\uff08\u8131\uff09\u5d4c\u5165\u8fc7\u7a0b\uff0c\u5e76\u5f15\u5165\u4e86\u5806\u53e0\u5c42\u9519\u6765\u6a21\u62df the staging \u7ed3\u6784\u8f6c\u53d8\u3002", "result": "\u901a\u8fc7\u6a21\u62df\u53d1\u73b0\uff0c\u5806\u53e0\u5c42\u9519\u9a71\u52a8\u7684 the staging \u7ed3\u6784\u8f6c\u53d8\u4f34\u968f\u7740\u5e94\u529b\u91ca\u653e\u548c\u7ed3\u6784\u7a33\u5b9a\u5316\u3002\u78b3\u5c42\u7684\u52a8\u529b\u5b66\u8c03\u63a7\u4e86\u9502\u79bb\u5b50\uff08\u8131\uff09\u5d4c\u5165\u7684\u4f4d\u7f6e\u9009\u62e9\u6027\uff0c\u4ea7\u751f\u4e86\u4e0d\u540c\u9502\u6d53\u5ea6\u548c\u5206\u5e03\u7684\u4e2d\u95f4\u6001\u3002\u7814\u7a76\u63ed\u793a\u4e86\u9502\u79bb\u5b50\u5d4c\u5165\u548c\u8131\u5d4c\u4e4b\u95f4\u5b58\u5728\u52a8\u529b\u5b66\u4e0d\u5bf9\u79f0\u6027\u3002\u78b3\u7f3a\u9677\u4f1a\u9650\u5236\u5c42\u5185\u9502\u79bb\u5b50\u4f20\u8f93\u548c\u78b3\u5c42\u6ed1\u52a8\uff0c\u4f46\u901a\u8fc7\u52a8\u6001\u7684\u9502\u79bb\u5b50\u4fd8\u83b7/\u91ca\u653e\u673a\u5236\u4fc3\u8fdb\u5c42\u95f4\u4f20\u8f93\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u52bf\u80fd\u6a21\u62df\u4e86\u9502\u79bb\u5b50\u5728\u77f3\u58a8\u4e2d\u7684\u5d4c\u5165/\u8131\u5d4c\u8fc7\u7a0b\uff0c\u63ed\u793a\u4e86\u78b3\u5c42\u52a8\u529b\u5b66\u3001\u9502\u79bb\u5b50\u4f20\u8f93\u548c\u7f3a\u9677\u8c03\u63a7\u4e4b\u95f4\u7684\u8026\u5408\u673a\u5236\u3002\u901a\u8fc7\u5f15\u5165\u5806\u53e0\u5c42\u9519\uff0c\u6a21\u62df\u4e86\u7531\u78b3\u5c42\u6ed1\u52a8\u548c\u91cd\u7ec4\u9a71\u52a8\u7684 the staging \u7ed3\u6784\u8f6c\u53d8\uff0c\u5e76\u9610\u660e\u4e86\u5176\u5bf9\u9502\u79bb\u5b50\u4f20\u8f93\u548c\u5e94\u529b\u79ef\u7d2f\u7684\u5f71\u54cd\u3002\u7814\u7a76\u53d1\u73b0\u9502\u79bb\u5b50\u5d4c\u5165/\u8131\u5d4c\u8fc7\u7a0b\u4e2d\u5b58\u5728\u52a8\u529b\u5b66\u4e0d\u5bf9\u79f0\u6027\uff0c\u78b3\u7f3a\u9677\u4f1a\u9650\u5236\u5c42\u5185\u4f20\u8f93\u4f46\u4fc3\u8fdb\u5c42\u95f4\u4f20\u8f93\u3002\u56e0\u6b64\uff0c\u672a\u6765\u7684\u8bbe\u8ba1\u5e94\u5173\u6ce8\u53ef\u63a7\u7684\u78b3\u5c42\u6ed1\u52a8/\u91cd\u7ec4\u548c\u53ef\u8c03\u7684\u7f3a\u9677\uff0c\u4ee5\u589e\u5f3a\u9502\u79bb\u5b50\u4f20\u8f93\u3002"}}
{"id": "2508.06149", "categories": ["cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.06149", "abs": "https://arxiv.org/abs/2508.06149", "authors": ["Gunhee Cho", "Yun-Gyung Cheong"], "title": "Scaling Personality Control in LLMs with Big Five Scaler Prompts", "comment": null, "summary": "We present Big5-Scaler, a prompt-based framework for conditioning large\nlanguage models (LLMs) with controllable Big Five personality traits. By\nembedding numeric trait values into natural language prompts, our method\nenables fine-grained personality control without additional training. We\nevaluate Big5-Scaler across trait expression, dialogue generation, and human\ntrait imitation tasks. Results show that it induces consistent and\ndistinguishable personality traits across models, with performance varying by\nprompt type and scale. Our analysis highlights the effectiveness of concise\nprompts and lower trait intensities, providing a efficient approach for\nbuilding personality-aware dialogue agents.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.06176", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.06176", "abs": "https://arxiv.org/abs/2508.06176", "authors": ["Marco Bertuletti", "Yichao Zhang", "Alessandro Vanelli-Coralli", "Luca Benini"], "title": "A 66-Gb/s/5.5-W RISC-V Many-Core Cluster for 5G+ Software-Defined Radio Uplinks", "comment": null, "summary": "Following the scale-up of new radio (NR) complexity in 5G and beyond, the\nphysical layer's computing load on base stations is increasing under a strictly\nconstrained latency and power budget; base stations must process > 20-Gb/s\nuplink wireless data rate on the fly, in < 10 W. At the same time, the\nprogrammability and reconfigurability of base station components are the key\nrequirements; it reduces the time and cost of new networks' deployment, it\nlowers the acceptance threshold for industry players to enter the market, and\nit ensures return on investments in a fast-paced evolution of standards. In\nthis article, we present the design of a many-core cluster for 5G and beyond\nbase station processing. Our design features 1024, streamlined RISC-V cores\nwith domain-specific FP extensions, and 4-MiB shared memory. It provides the\nnecessary computational capabilities for software-defined processing of the\nlower physical layer of 5G physical uplink shared channel (PUSCH), satisfying\nhigh-end throughput requirements (66 Gb/s for a transition time interval (TTI),\n9.4-302 Gb/s depending on the processing stage). The throughput metrics for the\nimplemented functions are ten times higher than in state-of-the-art (SoTA)\napplication-specific instruction processors (ASIPs). The energy efficiency on\nkey NR kernels (2-41 Gb/s/W), measured at 800 MHz, 25 {\\deg}C, and 0.8 V, on a\nplaced and routed instance in 12-nm CMOS technology, is competitive with SoTA\narchitectures. The PUSCH processing runs end-to-end on a single cluster in 1.7\nms, at <6-W average power consumption, achieving 12 Gb/s/W.", "AI": {"tldr": "\u8be5\u8bba\u6587\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5305\u542b1024\u4e2aRISC-V\u6838\u5fc3\u7684\u591a\u6838\u96c6\u7fa4\uff0c\u7528\u4e8e5G\u57fa\u7ad9\u5904\u7406\uff0c\u5728\u6ee1\u8db3\u4e25\u683c\u529f\u8017\u548c\u5ef6\u8fdf\u8981\u6c42\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u6bd4\u73b0\u6709\u6280\u672f\u9ad8\u5341\u500d\u7684\u541e\u5410\u91cf\u548c\u5177\u6709\u7ade\u4e89\u529b\u7684\u80fd\u6548\u3002", "motivation": "\u968f\u77405G\u53ca\u66f4\u9ad8\u7248\u672c\u4e2d\u65b0\u65e0\u7ebf\u7535\uff08NR\uff09\u7684\u590d\u6742\u6027\u589e\u52a0\uff0c\u57fa\u7ad9\u7684\u7269\u7406\u5c42\u8ba1\u7b97\u8d1f\u8f7d\u5728\u4e25\u683c\u7684\u5ef6\u8fdf\u548c\u529f\u8017\u9884\u7b97\u4e0b\u4e0d\u65ad\u589e\u52a0\uff0c\u9700\u8981\u5904\u7406\u6bcf\u79d2\u8d85\u8fc720Gb\u7684\u4e0a\u884c\u94fe\u8def\u65e0\u7ebf\u6570\u636e\u901f\u7387\uff0c\u540c\u65f6\u529f\u8017\u4f4e\u4e8e10W\u3002\u6b64\u5916\uff0c\u57fa\u7ad9\u7ec4\u4ef6\u7684\u53ef\u7f16\u7a0b\u6027\u548c\u53ef\u91cd\u6784\u6027\u662f\u5173\u952e\u8981\u6c42\uff0c\u4ee5\u964d\u4f4e\u90e8\u7f72\u6210\u672c\u548c\u65f6\u95f4\uff0c\u5e76\u9002\u5e94\u5feb\u901f\u53d1\u5c55\u7684\u6807\u51c6\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u591a\u6838\u96c6\u7fa4\uff0c\u5305\u542b1024\u4e2a\u6d41\u7ebf\u578bRISC-V\u6838\u5fc3\u548c\u7279\u5b9a\u9886\u57df\u7684FP\u6269\u5c55\uff0c\u4ee5\u53ca4MB\u5171\u4eab\u5185\u5b58\uff0c\u7528\u4e8e5G\u53ca\u66f4\u9ad8\u7248\u672c\u7684\u57fa\u7ad9\u5904\u7406\u3002", "result": "\u8be5\u591a\u6838\u96c6\u7fa4\u8bbe\u8ba1\u80fd\u591f\u6ee1\u8db35G\u7269\u7406\u4e0a\u884c\u5171\u4eab\u4fe1\u9053\uff08PUSCH\uff09\u7684\u8f6f\u4ef6\u5b9a\u4e49\u5904\u7406\u7684\u8ba1\u7b97\u80fd\u529b\u8981\u6c42\uff0c\u5728\u8fc7\u6e21\u65f6\u95f4\u95f4\u9694\uff08TTI\uff09\u4e0b\u63d0\u4f9b66Gb/s\u7684\u541e\u5410\u91cf\uff0c\u5177\u4f53\u541e\u5410\u91cf\u57289.4-302 Gb/s\u4e4b\u95f4\uff0c\u5177\u4f53\u53d6\u51b3\u4e8e\u5904\u7406\u9636\u6bb5\u3002\u4e0e\u5176\u4ed6\u73b0\u6709\u7684\u4e13\u7528ASIP\u76f8\u6bd4\uff0c\u6240\u5b9e\u73b0\u7684\u541e\u5410\u91cf\u6307\u6807\u63d0\u9ad8\u4e86\u5341\u500d\u3002", "conclusion": "\u8be5\u8bbe\u8ba1\u5b9e\u73b0\u4e861024\u4e2aRISC-V\u6838\u5fc3\uff0c\u572812nm CMOS\u6280\u672f\u4e0b\uff0c\u5728800MHz\uff0c25\u00b0C\uff0c0.8V\u4e0b\uff0c\u5bf9\u4e8e\u5173\u952eNR\u5185\u6838\u5177\u67092-41 Gb/s/W\u7684\u80fd\u6548\uff0c\u5e76\u4e14\u57281.7\u6beb\u79d2\u5185\u4ee5\u4f4e\u4e8e6W\u7684\u5e73\u5747\u529f\u8017\u7aef\u5230\u7aef\u8fd0\u884cPUSCH\u5904\u7406\uff0c\u5b9e\u73b0\u4e8612 Gb/s/W\u7684\u80fd\u6548\uff0c\u5176\u541e\u5410\u91cf\u6bd4\u73b0\u6709\u7684ASIPs\u9ad8\u5341\u500d\u3002"}}
{"id": "2508.06406", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06406", "abs": "https://arxiv.org/abs/2508.06406", "authors": ["Murtaza Rangwala", "Venugopal K R", "Rajkumar Buyya"], "title": "Blockchain-Enabled Federated Learning", "comment": "32 pages, 6 figures, chapter for edited book (Federated Learning:\n  Foundations and Applications)", "summary": "Blockchain-enabled federated learning (BCFL) addresses fundamental challenges\nof trust, privacy, and coordination in collaborative AI systems. This chapter\nprovides comprehensive architectural analysis of BCFL systems through a\nsystematic four-dimensional taxonomy examining coordination structures,\nconsensus mechanisms, storage architectures, and trust models. We analyze\ndesign patterns from blockchain-verified centralized coordination to fully\ndecentralized peer-to-peer networks, evaluating trade-offs in scalability,\nsecurity, and performance. Through detailed examination of consensus mechanisms\ndesigned for federated learning contexts, including Proof of Quality and Proof\nof Federated Learning, we demonstrate how computational work can be repurposed\nfrom arbitrary cryptographic puzzles to productive machine learning tasks. The\nchapter addresses critical storage challenges by examining multi-tier\narchitectures that balance blockchain's transaction constraints with neural\nnetworks' large parameter requirements while maintaining cryptographic\nintegrity. A technical case study of the TrustMesh framework illustrates\npractical implementation considerations in BCFL systems through distributed\nimage classification training, demonstrating effective collaborative learning\nacross IoT devices with highly non-IID data distributions while maintaining\ncomplete transparency and fault tolerance. Analysis of real-world deployments\nacross healthcare consortiums, financial services, and IoT security\napplications validates the practical viability of BCFL systems, achieving\nperformance comparable to centralized approaches while providing enhanced\nsecurity guarantees and enabling new models of trustless collaborative\nintelligence.", "AI": {"tldr": "BCFL\u901a\u8fc7\u5176\u72ec\u7279\u7684\u67b6\u6784\u3001\u5171\u8bc6\u673a\u5236\u548c\u5b58\u50a8\u89e3\u51b3\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u534f\u4f5cAI\u4e2d\u7684\u4fe1\u4efb\u3001\u9690\u79c1\u548c\u534f\u8c03\u95ee\u9898\uff0c\u5e76\u5728\u5404\u79cd\u5b9e\u9645\u5e94\u7528\u4e2d\u5f97\u5230\u4e86\u9a8c\u8bc1\uff0c\u8bc1\u660e\u4e86\u5176\u6027\u80fd\u548c\u5b89\u5168\u6027\u3002", "motivation": "BCFL\u89e3\u51b3\u4e86\u534f\u4f5cAI\u7cfb\u7edf\u4e2d\u4fe1\u4efb\u3001\u9690\u79c1\u548c\u534f\u8c03\u7684\u57fa\u672c\u6311\u6218\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u7684\u56db\u7ef4\u5206\u7c7b\uff08\u534f\u8c03\u7ed3\u6784\u3001\u5171\u8bc6\u673a\u5236\u3001\u5b58\u50a8\u67b6\u6784\u548c\u4fe1\u4efb\u6a21\u578b\uff09\u5bf9BCFL\u7cfb\u7edf\u8fdb\u884c\u5168\u9762\u7684\u67b6\u6784\u5206\u6790\uff0c\u68c0\u67e5\u4e86\u4ece\u533a\u5757\u94fe\u9a8c\u8bc1\u7684\u96c6\u4e2d\u534f\u8c03\u5230\u5b8c\u5168\u53bb\u4e2d\u5fc3\u5316\u7684\u70b9\u5bf9\u70b9\u7f51\u7edc\u7684\u8bbe\u8ba1\u6a21\u5f0f\uff0c\u5e76\u8bc4\u4f30\u4e86\u53ef\u6269\u5c55\u6027\u3001\u5b89\u5168\u6027\u548c\u6027\u80fd\u65b9\u9762\u7684\u6743\u8861\u3002\u8be6\u7ec6\u7814\u7a76\u4e86\u5305\u62ec\u8d28\u91cf\u8bc1\u660e\u548c\u8054\u90a6\u5b66\u4e60\u8bc1\u660e\u5728\u5185\u7684\u3001\u7528\u4e8e\u8054\u90a6\u5b66\u4e60\u7684\u5171\u8bc6\u673a\u5236\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5173\u4e8e\u5982\u4f55\u5c06\u8ba1\u7b97\u5de5\u4f5c\u4ece\u4efb\u610f\u5bc6\u7801\u5b66\u96be\u9898\u91cd\u65b0\u7528\u4e8e\u751f\u4ea7\u6027\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u7684\u6f14\u793a\u3002\u6700\u540e\uff0c\u901a\u8fc7\u4e00\u4e2a\u5173\u4e8eTrustMesh\u6846\u67b6\u7684\u6280\u672f\u6848\u4f8b\u7814\u7a76\uff0c\u8bf4\u660e\u4e86BCFL\u7cfb\u7edf\u5728\u5206\u5e03\u5f0f\u56fe\u50cf\u5206\u7c7b\u8bad\u7ec3\u4e2d\u7684\u5b9e\u9645\u5b9e\u65bd\u8003\u8651\u56e0\u7d20\uff0c\u8be5\u6848\u4f8b\u5c55\u793a\u4e86\u5728\u5177\u6709\u9ad8\u5ea6\u975eIID\u6570\u636e\u5206\u5e03\u7684\u7269\u8054\u7f51\u8bbe\u5907\u4e4b\u95f4\u8fdb\u884c\u6709\u6548\u7684\u534f\u4f5c\u5b66\u4e60\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5b8c\u5168\u7684\u900f\u660e\u5ea6\u548c\u5bb9\u9519\u80fd\u529b\u3002", "result": "\u8be5\u5206\u6790\u8be6\u7ec6\u8003\u5bdf\u4e86\u5171\u8bc6\u673a\u5236\uff0c\u6f14\u793a\u4e86\u5982\u4f55\u5c06\u8ba1\u7b97\u5de5\u4f5c\u91cd\u65b0\u7528\u4e8e\u673a\u5668\u5b66\u4e60\u4efb\u52a1\uff1b\u89e3\u51b3\u4e86\u5b58\u50a8\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u5e73\u8861\u533a\u5757\u94fe\u4ea4\u6613\u9650\u5236\u548c\u795e\u7ecf\u7f51\u7edc\u53c2\u6570\u9700\u6c42\u7684\u67b6\u6784\uff1b\u5e76\u901a\u8fc7TrustMesh\u6846\u67b6\u7684\u6848\u4f8b\u7814\u7a76\uff0c\u5c55\u793a\u4e86\u5728\u7269\u8054\u7f51\u8bbe\u5907\u4e0a\u8fdb\u884c\u5206\u5e03\u5f0f\u56fe\u50cf\u5206\u7c7b\u8bad\u7ec3\u7684\u6709\u6548\u6027\u3002", "conclusion": "BCFL\u7cfb\u7edf\u5728\u533b\u7597\u4fdd\u5065\u8054\u76df\u3001\u91d1\u878d\u670d\u52a1\u548c\u7269\u8054\u7f51\u5b89\u5168\u5e94\u7528\u7b49\u5b9e\u9645\u90e8\u7f72\u5206\u6790\uff0c\u9a8c\u8bc1\u4e86\u5176\u53ef\u884c\u6027\uff0c\u5b9e\u73b0\u4e86\u4e0e\u4e2d\u5fc3\u5316\u65b9\u6cd5\u76f8\u5ab2\u7f8e\u7684\u6027\u80fd\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u589e\u5f3a\u7684\u5b89\u5168\u4fdd\u8bc1\uff0c\u5e76\u652f\u6301\u65b0\u7684\u65e0\u4fe1\u4efb\u534f\u4f5c\u667a\u80fd\u6a21\u578b\u3002"}}
{"id": "2508.06466", "categories": ["cond-mat.mes-hall", "cond-mat.quant-gas", "physics.optics", "quant-ph"], "pdf": "https://arxiv.org/pdf/2508.06466", "abs": "https://arxiv.org/abs/2508.06466", "authors": ["Quan Lin", "Tianyu Li", "Haiping Hu", "Wei Yi", "Peng Xue"], "title": "Simulating Floquet non-Abelian topological insulator with photonic quantum walks", "comment": "9 pages, 4 figures", "summary": "Floquet non-Abelian topological phases emerge in periodically driven systems\nand exhibit properties that are absent in their Abelian or static counterparts.\nDubbed the Floquet non-Abelian topological insulators (FNATIs), they are\ncharacterized by non-Abelian topological charges and feature multifold\nbulk-boundary correspondence, making their experimental observation\nchallenging. Here we simulate the FNATI using a higher-dimensional photonic\nquantum walk and develop dynamic measurement schemes to demonstrate key\nsignatures of the FNATI. Importantly, combining a direct bulk-dynamic detection\nfor the underlying quaternion topological charge, and a spatially-resolved\ninjection spectroscopy for the edge states, we experimentally establish the\nmultifold bulk-boundary correspondence, and, in particular, identify the\nanomalous non-Abelian phase where edge states appear in all band gaps, despite\nthe presence of a trivial topological charge. Our experiment marks the first\nexperimental characterization of the FNATI, providing general insight into the\nnon-Abelian topological phases.", "AI": {"tldr": "\u901a\u8fc7\u9ad8\u7ef4\u5149\u91cf\u5b50\u884c\u8d70\u6a21\u62df\u548c\u52a8\u6001\u6d4b\u91cf\u65b9\u6848\uff0c\u9996\u6b21\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6c1f\u975e\u963f\u8d1d\u5c14\u62d3\u6251\u7edd\u7f18\u4f53\uff0c\u5e76\u63ed\u793a\u4e86\u5176\u7279\u6b8a\u7684\u975e\u963f\u8d1d\u5c14\u76f8\u3002", "motivation": "\u4e3a\u5e94\u5bf9\u6c1f\u975e\u963f\u8d1d\u5c14\u62d3\u6251\u7edd\u7f18\u4f53\uff08FNATIs\uff09\u56e0\u5176\u975e\u963f\u8d1d\u5c14\u62d3\u6251\u8377\u548c\u591a\u91cd\u4f53\u5206\u91ce\u8fb9\u754c\u5bf9\u5e94\u6027\u800c\u5bfc\u81f4\u7684\u5b9e\u9a8c\u89c2\u6d4b\u6311\u6218\u3002", "method": "\u5229\u7528\u9ad8\u7ef4\u5149\u91cf\u5b50\u884c\u8d70\u6a21\u62df\u4e86\u6c1f\u975e\u963f\u8d1d\u5c14\u62d3\u6251\u7edd\u7f18\u4f53\uff0c\u5e76\u5f00\u53d1\u4e86\u52a8\u6001\u6d4b\u91cf\u65b9\u6848\u3002", "result": "\u5b9e\u9a8c\u6210\u529f\u5efa\u7acb\u4e86\u591a\u91cd\u4f53\u5206\u91ce\u8fb9\u754c\u5bf9\u5e94\u6027\uff0c\u5e76\u8bc6\u522b\u51fa\u4e00\u79cd\u7279\u6b8a\u7684\u975e\u963f\u8d1d\u5c14\u76f8\uff0c\u5176\u8fb9\u754c\u6001\u51fa\u73b0\u5728\u6240\u6709\u5e26\u9699\u4e2d\uff0c\u5c3d\u7ba1\u5176\u62d3\u6251\u8377\u662f\u5e73\u51e1\u7684\u3002\u5177\u4f53\u800c\u8a00\uff0c\u7ed3\u5408\u4e86\u5bf9\u56db\u5143\u6570\u62d3\u6251\u8377\u7684\u76f4\u63a5\u5206\u91ce\u52a8\u6001\u63a2\u6d4b\u548c\u5bf9\u8fb9\u754c\u6001\u7684\u7a7a\u95f4\u5206\u8fa8\u6ce8\u5165\u5149\u8c31\u3002", "conclusion": "\u8be5\u5b9e\u9a8c\u9996\u6b21\u5b9e\u73b0\u4e86\u6c1f\u975e\u963f\u8d1d\u5c14\u62d3\u6251\u7edd\u7f18\u4f53\u7684\u5b9e\u9a8c\u8868\u5f81\uff0c\u5e76\u63d0\u4f9b\u4e86\u5bf9\u975e\u963f\u8d1d\u5c14\u62d3\u6251\u76f8\u7684\u666e\u9002\u6027\u8ba4\u8bc6\u3002"}}
{"id": "2508.05745", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.05745", "abs": "https://arxiv.org/abs/2508.05745", "authors": ["Simon Cichy", "Paul K. Faehrmann", "Lennart Bittel", "Jens Eisert", "Hakop Pashayan"], "title": "Classical simulation of noisy quantum circuits via locally entanglement-optimal unravelings", "comment": "23 pages + 39 page appendix, 15 figures, 4 tables. Comments welcome", "summary": "Classical simulations of noisy quantum circuits is instrumental to our\nunderstanding of the behavior of real world quantum systems and the\nidentification of regimes where one expects quantum advantage. In this work, we\npresent a highly parallelizable tensor-network-based classical algorithm --\nequipped with rigorous accuracy guarantees -- for simulating $n$-qubit quantum\ncircuits with arbitrary single-qubit noise. Our algorithm represents the state\nof a noisy quantum system by a particular ensemble of matrix product states\nfrom which we stochastically sample. Each single qubit noise process acting on\na pure state is then represented by the ensemble of states that achieve the\nminimal average entanglement (the entanglement of formation) between the noisy\nqubit and the remainder. This approach lets us use a more compact\nrepresentation of the quantum state for a given accuracy requirement and noise\nlevel. For a given maximum bond dimension $\\chi$ and circuit, our algorithm\ncomes with an upper bound on the simulation error, runs in poly$(n,\\chi)$-time\nand improves upon related prior work (1) in scope: by extending from the three\ncommonly considered noise models to general single qubit noise (2) in\nperformance: by employing a state-dependent locally-entanglement-optimal\nunraveling and (3) in conceptual contribution: by showing that the fixed\nunraveling used in prior work becomes equivalent to our choice of unraveling in\nthe special case of depolarizing and dephasing noise acting on a maximally\nentangled state.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u3001\u53ef\u5e76\u884c\u5316\u7684\u5f20\u91cf\u7f51\u7edc\u7b97\u6cd5\uff0c\u7528\u4e8e\u6a21\u62df\u542b\u5355\u6bd4\u7279\u566a\u58f0\u7684\u91cf\u5b50\u7535\u8def\uff0c\u8be5\u7b97\u6cd5\u5728\u7cbe\u5ea6\u3001\u6027\u80fd\u548c\u9002\u7528\u8303\u56f4\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4e3a\u4e86\u7406\u89e3\u771f\u5b9e\u4e16\u754c\u91cf\u5b50\u7cfb\u7edf\u7684\u884c\u4e3a\u4ee5\u53ca\u786e\u5b9a\u91cf\u5b50\u4f18\u52bf\u7684\u9884\u671f\u533a\u57df\uff0c\u5bf9\u542b\u566a\u58f0\u91cf\u5b50\u7535\u8def\u8fdb\u884c\u7ecf\u5178\u6a21\u62df\u662f\u81f3\u5173\u91cd\u8981\u7684\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f20\u91cf\u7f51\u7edc\u7684\u7ecf\u5178\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u53ef\u4ee5\u9ad8\u5ea6\u5e76\u884c\u5316\uff0c\u5e76\u5177\u6709\u4e25\u683c\u7684\u7cbe\u5ea6\u4fdd\u8bc1\uff0c\u7528\u4e8e\u6a21\u62df\u5177\u6709\u4efb\u610f\u5355\u6bd4\u7279\u566a\u58f0\u7684n\u91cf\u5b50\u6bd4\u7279\u91cf\u5b50\u7ebf\u8def\u3002\u8be5\u7b97\u6cd5\u901a\u8fc7\u4e00\u79cd\u7279\u6b8a\u7684\u77e9\u9635\u4e58\u79ef\u6001\u7cfb\u7efc\u6765\u8868\u793a\u566a\u58f0\u91cf\u5b50\u7cfb\u7edf\u7684\u72b6\u6001\uff0c\u5e76\u4ece\u4e2d\u8fdb\u884c\u968f\u673a\u91c7\u6837\u3002\u6bcf\u4e00\u4e2a\u4f5c\u7528\u5728\u7eaf\u6001\u4e0a\u7684\u5355\u6bd4\u7279\u566a\u58f0\u8fc7\u7a0b\uff0c\u90fd\u88ab\u8868\u793a\u4e3a\u8fbe\u5230\u6700\u5c0f\u5e73\u5747\u7ea0\u7f20\uff08\u5f62\u6210\u7ea0\u7f20\uff09\u7684\u7cfb\u7efc\u72b6\u6001\u3002", "result": "\u8be5\u7b97\u6cd5\u53ef\u4ee5\u5728\u7ed9\u5b9a\u6700\u5927\u952e\u7ef4\u5ea6\u03c7\u548c\u7535\u8def\u7684\u60c5\u51b5\u4e0b\uff0c\u5bf9\u6a21\u62df\u8bef\u5dee\u63d0\u4f9b\u4e00\u4e2a\u4e0a\u9650\uff0c\u8fd0\u884c\u65f6\u95f4\u4e3a\u591a\u9879\u5f0f\uff08n,\u03c7\uff09\u65f6\u95f4\u3002\u4e0e\u73b0\u6709\u5de5\u4f5c\u76f8\u6bd4\uff0c\u8be5\u7b97\u6cd5\u5728\u5904\u7406\u8303\u56f4\uff08\u4ece\u4e09\u79cd\u5e38\u89c1\u7684\u566a\u58f0\u6a21\u578b\u6269\u5c55\u5230\u4e00\u822c\u7684\u5355\u6bd4\u7279\u566a\u58f0\uff09\u3001\u6027\u80fd\uff08\u91c7\u7528\u72b6\u6001\u4f9d\u8d56\u7684\u5c40\u90e8\u7ea0\u7f20\u6700\u4f18\u5c55\u5f00\uff09\u548c\u6982\u5ff5\u8d21\u732e\uff08\u8bc1\u660e\u4e86\u56fa\u5b9a\u5c55\u5f00\u5728\u7279\u5b9a\u566a\u58f0\u6a21\u578b\u4e0b\u7b49\u4ef7\u4e8e\u672c\u6587\u63d0\u51fa\u7684\u5c55\u5f00\u65b9\u5f0f\uff09\u65b9\u9762\u90fd\u6709\u6240\u6539\u8fdb\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u901a\u8fc7\u91c7\u7528\u72b6\u6001\u4f9d\u8d56\u7684\u3001\u5c40\u90e8\u7ea0\u7f20\u6700\u4f18\u7684\u5c55\u5f00\u65b9\u5f0f\uff0c\u53ef\u4ee5\u66f4\u6709\u6548\u5730\u6a21\u62df\u91cf\u5b50\u7ebf\u8def\uff0c\u5e76\u4e14\u5728\u7cbe\u5ea6\u8981\u6c42\u548c\u566a\u58f0\u6c34\u5e73\u7684\u6743\u8861\u4e0a\u8868\u73b0\u66f4\u4f18\uff0c\u540c\u65f6\u6269\u5c55\u4e86\u5bf9\u4e00\u822c\u5355\u6bd4\u7279\u566a\u58f0\u7684\u5904\u7406\u80fd\u529b\u3002"}}
{"id": "2508.05909", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.05909", "abs": "https://arxiv.org/abs/2508.05909", "authors": ["Zhanghao Hu", "Qinglin Zhu", "Siya Qi", "Yulan He", "Hanqi Yan", "Lin Gui"], "title": "Spectrum Projection Score: Aligning Retrieved Summaries with Reader Models in Retrieval-Augmented Generation", "comment": null, "summary": "Large Language Models (LLMs) have shown improved generation performance\nthrough retrieval-augmented generation (RAG) following the retriever-reader\nparadigm, which supplements model inputs with externally retrieved knowledge.\nHowever, prior work often evaluates RAG holistically, assessing the retriever\nand reader jointly, making it difficult to isolate the true contribution of\nretrieval, particularly given the prompt sensitivity of LLMs used as readers.\nWe introduce Spectrum Projection Score (SPS), a lightweight, supervision-free\nmetric that allows the reader to gauge the semantic alignment of a retrieved\nsummary with its hidden representation by comparing the area formed by\ngenerated tokens from the summary, and the principal directions of subspace in\nthe reader and to measure the relevance. Building on SPS we present xCompress,\nan inference time controller framework that dynamically samples, ranks, and\ncompresses retrieval summary candidates. Extensive experiments on five QA\nbenchmarks with four open source LLMs show that SPS not only enhances\nperformance across a range of tasks but also provides a principled perspective\non the interaction between retrieval and generation.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a Spectrum Projection Score (SPS) \u7684\u65b0\u5ea6\u91cf\u6807\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u4e2d\u68c0\u7d22\u5230\u7684\u4fe1\u606f\u4e0e\u8bed\u8a00\u6a21\u578b\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u3002\u540c\u65f6\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a xCompress \u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u5316 RAG \u8fc7\u7a0b\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSPS \u548c xCompress \u80fd\u591f\u63d0\u9ad8 RAG \u7cfb\u7edf\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u65b9\u9762\u8868\u73b0\u51fa\u6539\u8fdb\u7684\u751f\u6210\u6027\u80fd\uff0c\u4f46\u901a\u5e38\u5bf9 RAG \u8fdb\u884c\u6574\u4f53\u8bc4\u4f30\uff0c\u96be\u4ee5\u5206\u79bb\u68c0\u7d22\u7684\u5b9e\u9645\u8d21\u732e\uff0c\u56e0\u4e3a LLMs \u5bf9\u63d0\u793a\u5f88\u654f\u611f\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "Spectrum Projection Score (SPS) \u901a\u8fc7\u6bd4\u8f83\u7531\u6458\u8981\u751f\u6210\u7684\u6807\u8bb0\u5f62\u6210\u7684\u533a\u57df\u4e0e\u8bfb\u8005\u5b50\u7a7a\u95f4\u7684\u4e3b\u65b9\u5411\u6765\u8bc4\u4f30\u68c0\u7d22\u6458\u8981\u4e0e\u9690\u85cf\u8868\u793a\u7684\u8bed\u4e49\u4e00\u81f4\u6027\uff0c\u5e76\u8861\u91cf\u76f8\u5173\u6027\u3002xCompress \u662f\u4e00\u4e2a\u63a8\u7406\u65f6\u63a7\u5236\u5668\u6846\u67b6\uff0c\u53ef\u4ee5\u52a8\u6001\u5730\u5bf9\u68c0\u7d22\u6458\u8981\u5019\u9009\u8fdb\u884c\u91c7\u6837\u3001\u6392\u5e8f\u548c\u538b\u7f29\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSPS \u4e0d\u4ec5\u80fd\u63d0\u9ad8\u5404\u79cd\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u8fd8\u80fd\u4e3a\u8861\u91cf\u68c0\u7d22\u548c\u751f\u6210\u4e4b\u95f4\u7684\u4ea4\u4e92\u63d0\u4f9b\u4e00\u4e2a\u6709\u539f\u5219\u7684\u89c6\u89d2\u3002", "conclusion": "Spectrum Projection Score (SPS) \u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u65e0\u76d1\u7763\u7684\u5ea6\u91cf\u6807\u51c6\uff0c\u901a\u8fc7\u6bd4\u8f83\u7531\u6458\u8981\u751f\u6210\u7684\u6807\u8bb0\u5f62\u6210\u7684\u533a\u57df\u4e0e\u8bfb\u8005\u5b50\u7a7a\u95f4\u7684\u4e3b\u65b9\u5411\uff0c\u6765\u8bc4\u4f30\u68c0\u7d22\u6458\u8981\u4e0e\u9690\u85cf\u8868\u793a\u7684\u8bed\u4e49\u4e00\u81f4\u6027\uff0c\u5e76\u8861\u91cf\u76f8\u5173\u6027\u3002\u57fa\u4e8e SPS\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 xCompress\uff0c\u4e00\u4e2a\u63a8\u7406\u65f6\u63a7\u5236\u5668\u6846\u67b6\uff0c\u53ef\u4ee5\u52a8\u6001\u5730\u5bf9\u68c0\u7d22\u6458\u8981\u5019\u9009\u8fdb\u884c\u91c7\u6837\u3001\u6392\u5e8f\u548c\u538b\u7f29\u3002"}}
{"id": "2508.05819", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05819", "abs": "https://arxiv.org/abs/2508.05819", "authors": ["Jong-Ik Park", "Carlee Joe-Wong", "Gary K. Fedder"], "title": "MZEN: Multi-Zoom Enhanced NeRF for 3-D Reconstruction with Unknown Camera Poses", "comment": null, "summary": "Neural Radiance Fields (NeRF) methods excel at 3D reconstruction from\nmultiple 2D images, even those taken with unknown camera poses. However, they\nstill miss the fine-detailed structures that matter in industrial inspection,\ne.g., detecting sub-micron defects on a production line or analyzing chips with\nScanning Electron Microscopy (SEM). In these scenarios, the sensor resolution\nis fixed and compute budgets are tight, so the only way to expose fine\nstructure is to add zoom-in images; yet, this breaks the multi-view consistency\nthat pose-free NeRF training relies on. We propose Multi-Zoom Enhanced NeRF\n(MZEN), the first NeRF framework that natively handles multi-zoom image sets.\nMZEN (i) augments the pin-hole camera model with an explicit, learnable zoom\nscalar that scales the focal length, and (ii) introduces a novel pose strategy:\nwide-field images are solved first to establish a global metric frame, and\nzoom-in images are then pose-primed to the nearest wide-field counterpart via a\nzoom-consistent crop-and-match procedure before joint refinement. Across eight\nforward-facing scenes$\\unicode{x2013}$synthetic TCAD models, real SEM of\nmicro-structures, and BLEFF objects$\\unicode{x2013}$MZEN consistently\noutperforms pose-free baselines and even high-resolution variants, boosting\nPSNR by up to $28 \\%$, SSIM by $10 \\%$, and reducing LPIPS by up to $222 \\%$.\nMZEN, therefore, extends NeRF to real-world factory settings, preserving global\naccuracy while capturing the micron-level details essential for industrial\ninspection.", "AI": {"tldr": "MZEN \u662f\u9996\u4e2a\u80fd\u591f\u539f\u751f\u5904\u7406\u591a\u91cd\u653e\u5927\u56fe\u50cf\u96c6\u7684 NeRF \u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709 NeRF \u5728\u6355\u6349\u5de5\u4e1a\u68c0\u6d4b\u6240\u9700\u7cbe\u7ec6\u7ed3\u6784\u7ec6\u8282\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002\u5b83\u901a\u8fc7\u6539\u8fdb\u76f8\u673a\u6a21\u578b\u548c\u59ff\u6001\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u548c\u7ec6\u8282\u6355\u6349\uff0c\u9002\u7528\u4e8e\u5de5\u5382\u73af\u5883\u3002", "motivation": "\u73b0\u6709\u7684 NeRF \u65b9\u6cd5\u867d\u7136\u5728\u4ece\u5177\u6709\u672a\u77e5\u76f8\u673a\u4f4d\u59ff\u7684\u591a\u5f20 2D \u56fe\u50cf\u8fdb\u884c 3D \u91cd\u5efa\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5de5\u4e1a\u68c0\u6d4b\u7b49\u573a\u666f\u4e0b\uff0c\u4f8b\u5982\u5728\u751f\u4ea7\u7ebf\u4e0a\u68c0\u6d4b\u4e9a\u5fae\u7c73\u7ea7\u7f3a\u9677\u6216\u4f7f\u7528\u626b\u63cf\u7535\u5b50\u663e\u5fae\u955c\u5206\u6790\u82af\u7247\u65f6\uff0c\u4ecd\u7136\u7f3a\u4e4f\u5bf9\u7cbe\u7ec6\u7ed3\u6784\u7ec6\u8282\u7684\u6355\u6349\u80fd\u529b\u3002\u5728\u8fd9\u4e9b\u573a\u666f\u4e2d\uff0c\u4f20\u611f\u5668\u5206\u8fa8\u7387\u56fa\u5b9a\u4e14\u8ba1\u7b97\u9884\u7b97\u6709\u9650\uff0c\u4e3a\u4e86\u5c55\u73b0\u7cbe\u7ec6\u7ed3\u6784\uff0c\u9700\u8981\u52a0\u5165\u653e\u5927\u56fe\u50cf\uff1b\u7136\u800c\uff0c\u8fd9\u4f1a\u7834\u574f\u65e0\u59ff\u6001 NeRF \u8bad\u7ec3\u6240\u4f9d\u8d56\u7684\u591a\u89c6\u56fe\u4e00\u81f4\u6027\u3002", "method": "MZEN \u901a\u8fc7 (i) \u589e\u5f3a\u9488\u5b54\u76f8\u673a\u6a21\u578b\uff0c\u52a0\u5165\u53ef\u5b66\u4e60\u7684\u7f29\u653e\u56e0\u5b50\u6765\u7f29\u653e\u7126\u8ddd\uff0c\u4ee5\u53ca (ii) \u5f15\u5165\u4e00\u79cd\u65b0\u9896\u7684\u59ff\u6001\u7b56\u7565\uff1a\u9996\u5148\u5904\u7406\u5e7f\u89d2\u56fe\u50cf\u4ee5\u5efa\u7acb\u5168\u5c40\u5ea6\u91cf\u6846\u67b6\uff0c\u7136\u540e\u901a\u8fc7\u4e00\u79cd\u7f29\u653e\u4e00\u81f4\u7684\u88c1\u526a\u548c\u5339\u914d\u7a0b\u5e8f\u5c06\u53d8\u7126\u56fe\u50cf\u7684\u59ff\u6001\u521d\u59cb\u5316\u5230\u6700\u8fd1\u7684\u5e7f\u89d2\u5bf9\u5e94\u56fe\u50cf\uff0c\u6700\u540e\u8fdb\u884c\u8054\u5408\u4f18\u5316\u3002", "result": "MZEN \u5728\u516b\u4e2a\u524d\u5411\u573a\u666f\uff08\u5305\u62ec\u5408\u6210 TCAD \u6a21\u578b\u3001\u5fae\u89c2\u7ed3\u6784\u7684\u771f\u5b9e SEM \u56fe\u50cf\u4ee5\u53ca BLEFF \u5bf9\u8c61\uff09\u4e0a\uff0c\u4e00\u81f4\u5730\u4f18\u4e8e\u65e0\u59ff\u6001\u57fa\u7ebf\u548c\u9ad8\u5206\u8fa8\u7387\u53d8\u4f53\uff0c\u5176 PSNR \u63d0\u9ad8\u4e86 28%\uff0cSSIM \u63d0\u9ad8\u4e86 10%\uff0cLPIPS \u964d\u4f4e\u4e86 222%\u3002", "conclusion": "MZEN \u6210\u529f\u5730\u5c06 NeRF \u6269\u5c55\u5230\u4e86\u5b9e\u9645\u7684\u5de5\u5382\u73af\u5883\uff0c\u5728\u4fdd\u6301\u5168\u5c40\u51c6\u786e\u6027\u7684\u540c\u65f6\uff0c\u6355\u6349\u5230\u4e86\u5bf9\u5de5\u4e1a\u68c0\u6d4b\u81f3\u5173\u91cd\u8981\u7684\u5fae\u7c73\u7ea7\u7ec6\u8282\u3002"}}
{"id": "2508.06053", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06053", "abs": "https://arxiv.org/abs/2508.06053", "authors": ["Kaixuan Wu", "Yuanzhuo Xu", "Zejun Zhang", "Weiping Zhu", "Steve Drew", "Xiaoguang Niu"], "title": "ReNiL: Relative Neural Inertial Locator with Any-Scale Bayesian Inference", "comment": null, "summary": "Pedestrian inertial localization is key for mobile and IoT services because\nit provides infrastructure-free positioning. Yet most learning-based methods\ndepend on fixed sliding-window integration, struggle to adapt to diverse motion\nscales and cadences, and yield inconsistent uncertainty, limiting real-world\nuse. We present ReNiL, a Bayesian deep-learning framework for accurate,\nefficient, and uncertainty-aware pedestrian localization. ReNiL introduces\nInertial Positioning Demand Points (IPDPs) to estimate motion at contextually\nmeaningful waypoints instead of dense tracking, and supports inference on IMU\nsequences at any scale so cadence can match application needs. It couples a\nmotion-aware orientation filter with an Any-Scale Laplace Estimator (ASLE), a\ndual-task network that blends patch-based self-supervision with Bayesian\nregression. By modeling displacements with a Laplace distribution, ReNiL\nprovides homogeneous Euclidean uncertainty that integrates cleanly with other\nsensors. A Bayesian inference chain links successive IPDPs into consistent\ntrajectories. On RoNIN-ds and a new WUDataset covering indoor and outdoor\nmotion from 28 participants, ReNiL achieves state-of-the-art displacement\naccuracy and uncertainty consistency, outperforming TLIO, CTIN, iMoT, and RoNIN\nvariants while reducing computation. Application studies further show\nrobustness and practicality for mobile and IoT localization, making ReNiL a\nscalable, uncertainty-aware foundation for next-generation positioning.", "AI": {"tldr": "ReNiL\u662f\u4e00\u4e2a\u521b\u65b0\u7684\u8d1d\u53f6\u65af\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7IPDP\u548cASLE\u6280\u672f\u5b9e\u73b0\u4e86\u51c6\u786e\u3001\u9ad8\u6548\u4e14\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u884c\u4eba\u60ef\u6027\u5b9a\u4f4d\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u9886\u5148\u6210\u679c\u3002", "motivation": "\u73b0\u6709\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u56fa\u5b9a\u7684\u6ed1\u52a8\u7a97\u53e3\u79ef\u5206\uff0c\u96be\u4ee5\u9002\u5e94\u4e0d\u540c\u8fd0\u52a8\u5c3a\u5ea6\u548c\u8282\u594f\uff0c\u5e76\u4e14\u4f1a\u4ea7\u751f\u4e0d\u786e\u786e\u5b9a\u6027\uff0c\u9650\u5236\u4e86\u5176\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u5e94\u7528\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5b9e\u73b0\u51c6\u786e\u3001\u9ad8\u6548\u4e14\u80fd\u611f\u77e5\u4e0d\u786e\u5b9a\u6027\u7684\u884c\u4eba\u5b9a\u4f4d\u65b9\u6cd5\u3002", "method": "ReNiL\u6846\u67b6\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d1d\u53f6\u65af\u6df1\u5ea6\u5b66\u4e60\u7684\u884c\u4eba\u5b9a\u4f4d\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u5f15\u5165\u4e86\u60ef\u6027\u5b9a\u4f4d\u9700\u6c42\u70b9\uff08IPDP\uff09\u6765\u4f30\u8ba1\u6709\u610f\u4e49\u7684\u822a\u70b9\u5904\u7684\u8fd0\u52a8\uff0c\u800c\u4e0d\u662f\u8fdb\u884c\u5bc6\u96c6\u7684\u8ddf\u8e2a\u3002\u5b83\u8fd8\u652f\u6301\u5728\u4efb\u4f55\u5c3a\u5ea6\u4e0a\u5bf9IMU\u5e8f\u5217\u8fdb\u884c\u63a8\u7406\uff0c\u4f7f\u6b65\u6001\u9002\u5e94\u5e94\u7528\u7a0b\u5e8f\u7684\u9700\u6c42\u3002ReNiL\u5c06\u8fd0\u52a8\u611f\u77e5\u65b9\u5411\u6ee4\u6ce2\u5668\u4e0e\u4efb\u4f55\u5c3a\u5ea6\u62c9\u666e\u62c9\u65af\u4f30\u8ba1\u5668\uff08ASLE\uff09\u76f8\u7ed3\u5408\uff0cASLE\u662f\u4e00\u4e2a\u53cc\u4efb\u52a1\u7f51\u7edc\uff0c\u878d\u5408\u4e86\u57fa\u4e8e\u5757\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u548c\u8d1d\u53f6\u65af\u56de\u5f52\u3002\u901a\u8fc7\u4f7f\u7528\u62c9\u666e\u62c9\u65af\u5206\u5e03\u5bf9\u4f4d\u79fb\u8fdb\u884c\u5efa\u6a21\uff0cReNiL\u63d0\u4f9b\u4e86\u540c\u8d28\u6b27\u51e0\u91cc\u5f97\u4e0d\u786e\u5b9a\u6027\uff0c\u53ef\u4ee5\u4e0e\u5176\u4ed6\u4f20\u611f\u5668\u987a\u5229\u96c6\u6210\u3002\u6700\u540e\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u63a8\u7406\u94fe\u5c06\u8fde\u7eed\u7684IPDP\u94fe\u63a5\u6210\u4e00\u81f4\u7684\u8f68\u8ff9\u3002", "result": "ReNiL\u5728RoNIN-ds\u548cWUDataset\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u4f4d\u79fb\u7cbe\u5ea6\u548c\u4e0d\u786e\u5b9a\u6027\u4e00\u81f4\u6027\uff0c\u4f18\u4e8eTLIO\u3001CTIN\u3001iMoT\u548cRoNIN\u7b49\u65b9\u6cd5\uff0c\u5e76\u51cf\u5c11\u4e86\u8ba1\u7b97\u91cf\u3002", "conclusion": "ReNiL\u6846\u67b6\u5728RoNIN-ds\u548cWUDataset\u6570\u636e\u96c6\u4e0a\u5747\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u4f4d\u79fb\u7cbe\u5ea6\u548c\u4e0d\u786e\u5b9a\u6027\u4e00\u81f4\u6027\uff0c\u4f18\u4e8eTLIO\u3001CTIN\u3001iMoT\u548cRoNIN\u7b49\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u8ba1\u7b97\u91cf\u3002\u5e94\u7528\u7814\u7a76\u8868\u660e\u5176\u5728\u79fb\u52a8\u548c\u7269\u8054\u7f51\u5b9a\u4f4d\u65b9\u9762\u5177\u6709\u9c81\u68d2\u6027\u548c\u5b9e\u7528\u6027\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u53ef\u611f\u77e5\u4e0d\u786e\u5b9a\u6027\u7684\u57fa\u7840\u3002"}}
{"id": "2508.06042", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06042", "abs": "https://arxiv.org/abs/2508.06042", "authors": ["Daechul Ahn", "San Kim", "Jonghyun Choi"], "title": "Society of Mind Meets Real-Time Strategy: A Hierarchical Multi-Agent Framework for Strategic Reasoning", "comment": "COLM 2025", "summary": "Large Language Models (LLMs) have recently demonstrated impressive action\nsequence prediction capabilities but often struggle with dynamic, long-horizon\ntasks such as real-time strategic games. In a game such as StarCraftII (SC2),\nagents need to manage resource constraints and adapt to evolving battlefield\nsituations in a partially observable environment. This often overwhelms\nexisiting LLM-based approaches. To address these challenges, we propose a\nhierarchical multi-agent framework that employs specialized imitation learning\nagents under a meta-controller called Strategic Planner (SP). By expert\ndemonstrations, each specialized agent learns a distinctive strategy, such as\naerial support or defensive maneuvers, and produces coherent, structured\nmultistep action sequences. The SP then orchestrates these proposals into a\nsingle, environmentally adaptive plan that ensures local decisions aligning\nwith long-term strategies. We call this HIMA (Hierarchical Imitation\nMulti-Agent). We also present TEXTSCII-ALL, a comprehensive SC2 testbed that\nencompasses all race match combinations in SC2. Our empirical results show that\nHIMA outperforms state of the arts in strategic clarity, adaptability, and\ncomputational efficiency, underscoring the potential of combining specialized\nimitation modules with meta-level orchestration to develop more robust,\ngeneral-purpose AI agents.", "AI": {"tldr": "A new hierarchical multi-agent framework called HIMA, which uses a Strategic Planner (SP) to orchestrate specialized imitation learning agents, excels in StarCraftII by improving strategic clarity, adaptability, and efficiency compared to existing methods. It also introduces TEXTSCII-ALL, a comprehensive SC2 testbed.", "motivation": "LLMs struggle with dynamic, long-horizon tasks such as real-time strategic games like StarCraftII (SC2), which require managing resource constraints and adapting to evolving battlefield situations in a partially observable environment.", "method": "A hierarchical multi-agent framework that employs specialized imitation learning agents under a meta-controller called Strategic Planner (SP). The SP orchestrates proposals from specialized agents into a single, environmentally adaptive plan.", "result": "Empirical results show that HIMA outperforms state of the arts in strategic clarity, adaptability, and computational efficiency.", "conclusion": "HIMA outperforms state of the arts in strategic clarity, adaptability, and computational efficiency, underscoring the potential of combining specialized imitation modules with meta-level orchestration to develop more robust, general-purpose AI agents."}}
{"id": "2508.05905", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.05905", "abs": "https://arxiv.org/abs/2508.05905", "authors": ["Jeffrey Uhlmann"], "title": "The Fourth State: Signed-Zero Ternary for Stable LLM Quantization (and More)", "comment": null, "summary": "Quantization is usually regarded as a means to trade quality of performance\nfor reduced compute requirements, i.e., as a suboptimal approximation. However,\nif examined in terms of a fixed overall resource budget, a very different\nperspective arises. We introduce Signed-Zero Ternary (SZT), a 2-bit\nquantization that deterministically provides gradient information with no\nforward-path penalty. Our analysis provides evidence that it may improve\ninformation density compared to non-quantized alternatives.", "AI": {"tldr": "SZT\u662f\u4e00\u79cd2\u4f4d\u91cf\u5316\u65b9\u6cd5\uff0c\u5728\u56fa\u5b9a\u8d44\u6e90\u9884\u7b97\u4e0b\uff0c\u5b83\u80fd\u786e\u5b9a\u6027\u5730\u63d0\u4f9b\u68af\u5ea6\u4fe1\u606f\uff0c\u4e14\u6ca1\u6709\u524d\u5411\u4f20\u64ad\u5f00\u9500\uff0c\u5e76\u53ef\u80fd\u63d0\u9ad8\u4fe1\u606f\u5bc6\u5ea6\u3002", "motivation": "\u5c06\u91cf\u5316\u89c6\u4e3a\u727a\u7272\u6027\u80fd\u6362\u53d6\u66f4\u4f4e\u8ba1\u7b97\u9700\u6c42\u7684\u6b21\u4f18\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u4f46\u5728\u56fa\u5b9a\u8d44\u6e90\u9884\u7b97\u4e0b\uff0cSZT\u63d0\u4f9b\u4e86\u4e0d\u540c\u7684\u89c6\u89d2\u3002", "method": "SZT\u662f\u4e00\u79cd2\u4f4d\u91cf\u5316\u65b9\u6cd5\uff0c\u53ef\u4ee5\u786e\u5b9a\u6027\u5730\u63d0\u4f9b\u68af\u5ea6\u4fe1\u606f\uff0c\u4e14\u6ca1\u6709\u524d\u5411\u4f20\u64ad\u5f00\u9500\u3002", "result": "SZT\u53ef\u80fd\u6bd4\u975e\u91cf\u5316\u65b9\u6cd5\u63d0\u9ad8\u4fe1\u606f\u5bc6\u5ea6\u3002SZT\u662f\u4e00\u79cd2\u4f4d\u91cf\u5316\uff0c\u786e\u5b9a\u6027\u5730\u63d0\u4f9b\u68af\u5ea6\u4fe1\u606f\uff0c\u6ca1\u6709\u524d\u5411\u4f20\u64ad\u5f00\u9500\u3002   ", "conclusion": "SZT\u662f\u4e00\u79cd2\u4f4d\u91cf\u5316\u65b9\u6cd5\uff0c\u5b83\u786e\u5b9a\u6027\u5730\u63d0\u4f9b\u68af\u5ea6\u4fe1\u606f\uff0c\u4e14\u6ca1\u6709\u524d\u5411\u4f20\u64ad\u5f00\u9500\u3002\u5b83\u53ef\u80fd\u6bd4\u975e\u91cf\u5316\u65b9\u6cd5\u63d0\u9ad8\u4fe1\u606f\u5bc6\u5ea6\u3002"}}
{"id": "2508.06200", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2508.06200", "abs": "https://arxiv.org/abs/2508.06200", "authors": ["Elbruz Murat Baba", "Stefano Deledda", "Smagul Karazhanov"], "title": "Scalable Production of Photochromic Yttrium Oxyhydride Powder via Ball Milling", "comment": "7 pages, 3 Figures", "summary": "Yttrium oxyhydride (YHO) represents one of the most promising photochromic\nmaterials discovered in recent years, yet its practical deployment has been\nseverely constrained by the limitations of thin film deposition methods. Here\nwe demonstrate the first successful synthesis of photochromic YHO powders\nthrough reactive ball milling under hydrogen atmosphere followed by controlled\noxidation a fundamentally scalable approach that overcomes the production\nbarriers facing this technology. High-energy planetary ball milling of yttrium\nmetal under 50 bar hydrogen for 20 hours, followed by controlled oxidation in\nultra-dry technical air, yielded nanostructured YHO powders with less than 500\nnm particle sizes. These powders exhibit robust photochromic response with\nreflectance modulation at 850 nm under 405 nm excitation, reversible cycling\nbehavior, and the characteristic memory effect previously observed only in thin\nfilms. Powder X-ray diffraction confirms the formation of the cubic YHO phase\nwith lattice expansion consistent with oxygen incorporation into the yttrium\nhydride structure. Critically, we demonstrate that YHO powders can be processed\ninto polymer composites enabling spatially-resolved photochromic patterning a\ncapability essential for practical device applications. While optimization of\noptical contrast remains an opportunity for future work, this powder synthesis\nroute fundamentally transforms the manufacturing of YHO-based photochromic\nsystems, enabling mass-scale production using established industrial ball\nmilling infrastructure. These findings establish a viable pathway toward\ncommercial deployment of YHO in smart windows, adaptive optics, and rewritable\ninformation storage applications.", "AI": {"tldr": "A new scalable method using reactive ball milling and controlled oxidation successfully produced Yttrium oxyhydride (YHO) powders, overcoming previous thin-film limitations. These powders exhibit photochromic properties and can be used in polymer composites for applications like smart windows.", "motivation": "The practical deployment of Yttrium oxyhydride (YHO), a promising photochromic material, has been limited by the challenges associated with thin film deposition methods. This work aims to overcome these production barriers by developing a scalable synthesis route for YHO powders.", "method": "The synthesis involved reactive ball milling of yttrium metal under hydrogen atmosphere, followed by controlled oxidation in dry air. High-energy planetary ball milling was employed for 20 hours under 50 bar hydrogen, and the resulting material was then oxidized to yield nanostructured YHO powders with particle sizes under 500 nm. Powder X-ray diffraction was used to confirm the formation of the cubic YHO phase.", "result": "Nanostructured YHO powders with particle sizes less than 500 nm were successfully synthesized. These powders exhibit robust photochromic response, with reflectance modulation at 850 nm under 405 nm excitation. The material also shows reversible cycling behavior and the characteristic memory effect. X-ray diffraction confirmed the formation of the cubic YHO phase. Furthermore, the YHO powders were processed into polymer composites, enabling spatially-resolved photochromic patterning.", "conclusion": "Yttrium oxyhydride (YHO) powders have been successfully synthesized via reactive ball milling and controlled oxidation, overcoming previous thin film deposition limitations. This scalable method enables the production of nanostructured YHO powders with robust photochromic properties, reversible cycling, and memory effects. The powders can be processed into polymer composites for device applications, paving the way for mass production and commercial deployment in smart windows, adaptive optics, and rewritable information storage."}}
{"id": "2508.06336", "categories": ["cs.LG", "cs.AI", "cs.HC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.06336", "abs": "https://arxiv.org/abs/2508.06336", "authors": ["Constantin Ruhdorfer", "Matteo Bortoletto", "Victor Oei", "Anna Penzkofer", "Andreas Bulling"], "title": "Unsupervised Partner Design Enables Robust Ad-hoc Teamwork", "comment": "16 pages", "summary": "We introduce Unsupervised Partner Design (UPD) - a population-free,\nmulti-agent reinforcement learning framework for robust ad-hoc teamwork that\nadaptively generates training partners without requiring pretrained partners or\nmanual parameter tuning. UPD constructs diverse partners by stochastically\nmixing an ego agent's policy with biased random behaviours and scores them\nusing a variance-based learnability metric that prioritises partners near the\nego agent's current learning frontier. We show that UPD can be integrated with\nunsupervised environment design, resulting in the first method enabling fully\nunsupervised curricula over both level and partner distributions in a\ncooperative setting. Through extensive evaluations on Overcooked-AI and the\nOvercooked Generalisation Challenge, we demonstrate that this dynamic partner\ncurriculum is highly effective: UPD consistently outperforms both\npopulation-based and population-free baselines as well as ablations. In a user\nstudy, we further show that UPD achieves higher returns than all baselines and\nwas perceived as significantly more adaptive, more human-like, a better\ncollaborator, and less frustrating.", "AI": {"tldr": "Unsupervised Partner Design (UPD) is a new RL framework that creates its own diverse training partners on the fly, improving ad-hoc teamwork without needing pre-existing partners or manual tuning. It outperforms existing methods and is perceived as more human-like and less frustrating by users.", "motivation": "To develop a population-free, multi-agent reinforcement learning framework for robust ad-hoc teamwork that adaptively generates training partners without requiring pretrained partners or manual parameter tuning.", "method": "UPD constructs diverse partners by stochastically mixing an ego agent's policy with biased random behaviours and scores them using a variance-based learnability metric that prioritises partners near the ego agent's current learning frontier. It can be integrated with unsupervised environment design for fully unsupervised curricula over both level and partner distributions in a cooperative setting.", "result": "UPD enables fully unsupervised curricula over both level and partner distributions in a cooperative setting, achieving higher returns and better collaboration in user studies.", "conclusion": "UPD consistently outperforms both population-based and population-free baselines and ablations in Overcooked-AI and Overcooked Generalisation Challenge, and is perceived as more adaptive, human-like, a better collaborator, and less frustrating in user study."}}
{"id": "2508.06275", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.06275", "abs": "https://arxiv.org/abs/2508.06275", "authors": ["SaiKrishna Saketh Yellapragada", "Esa Ollila", "Mario Costa"], "title": "Efficient Deep Neural Receiver with Post-Training Quantization", "comment": null, "summary": "Deep learning has recently garnered significant interest in wireless\ncommunications due to its superior performance compared to traditional\nmodel-based algorithms. Deep convolutional neural networks (CNNs) have\ndemonstrated notable improvements in block error rate (BLER) under various\nchannel models and mobility scenarios. However, the high computational\ncomplexity and resource demands of deep CNNs pose challenges for deployment in\nresource-constrained edge systems. The 3rd Generation Partnership Project\n(3GPP) Release 20 highlights the pivotal role of artificial intelligence (AI)\nintegration in enabling advanced radio-access networks for 6G systems. The hard\nreal-time processing demands of 5G and 6G require efficient techniques such as\npost-training quantization (PTQ), quantization-aware training (QAT), pruning,\nand hybrid approaches to meet latency requirements. In this paper, we focus on\nPTQ to reduce model complexity by lowering the bit-width of weights, thereby\nenhancing computational efficiency. Our analysis employs symmetric uniform\nquantization, applying both per-tensor and per-channel PTQ to a neural receiver\nachieving performance comparable to full-precision models. Specifically, 8-bit\nper-channel quantization maintains BLER performance with minimal degradation,\nwhile 4-bit quantization shows great promise but requires further optimization\nto achieve target BLER levels. These results highlight the potential of\nultra-low bitwidth PTQ for efficient neural receiver deployment in 6G systems.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4f7f\u7528PTQ\u964d\u4f4e\u6a21\u578b\u590d\u6742\u5ea6\uff0c\u4ee5\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u57286G\u7cfb\u7edf\u4e2d\u90e8\u7f72\u795e\u7ecf\u63a5\u6536\u5668\u7684\u6f5c\u529b\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u6df1\u5ea6CNN\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u7cfb\u7edf\u4e2d\u90e8\u7f72\u9762\u4e34\u7684\u8ba1\u7b97\u590d\u6742\u6027\u548c\u8d44\u6e90\u9700\u6c42\u6311\u6218\uff0c\u5e76\u6ee1\u8db35G\u548c6G\u7684\u786c\u5b9e\u65f6\u5904\u7406\u9700\u6c42\u3002", "method": "\u672c\u6587\u5c06\u5bf9\u79f0\u5747\u5300\u91cf\u5316\u5e94\u7528\u4e8e\u795e\u7ecf\u63a5\u6536\u5668\uff0c\u5e76\u5206\u522b\u4f7f\u7528\u4e86per-tensor\u548cper-channel PTQ\u3002", "result": "8\u4f4dper-channel\u91cf\u5316\u5728\u4fdd\u6301BLER\u6027\u80fd\u65b9\u9762\u5177\u6709\u53ef\u6bd4\u6027\uff0c\u800c4\u4f4d\u91cf\u5316\u663e\u793a\u51fa\u5de8\u5927\u6f5c\u529b\u4f46\u9700\u8981\u8fdb\u4e00\u6b65\u4f18\u5316\u3002", "conclusion": "8\u4f4dper-channel\u91cf\u5316\u5728\u4fdd\u6301BLER\u6027\u80fd\u65b9\u9762\u5177\u6709\u53ef\u6bd4\u6027\uff0c\u800c4\u4f4d\u91cf\u5316\u6709\u5f85\u8fdb\u4e00\u6b65\u4f18\u5316\u3002"}}
{"id": "2508.06301", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.DC"], "pdf": "https://arxiv.org/pdf/2508.06301", "abs": "https://arxiv.org/abs/2508.06301", "authors": ["Junhyeog Yun", "Minui Hong", "Gunhee Kim"], "title": "FedMeNF: Privacy-Preserving Federated Meta-Learning for Neural Fields", "comment": "ICCV 2025", "summary": "Neural fields provide a memory-efficient representation of data, which can\neffectively handle diverse modalities and large-scale data. However, learning\nto map neural fields often requires large amounts of training data and\ncomputations, which can be limited to resource-constrained edge devices. One\napproach to tackle this limitation is to leverage Federated Meta-Learning\n(FML), but traditional FML approaches suffer from privacy leakage. To address\nthese issues, we introduce a novel FML approach called FedMeNF. FedMeNF\nutilizes a new privacy-preserving loss function that regulates privacy leakage\nin the local meta-optimization. This enables the local meta-learner to optimize\nquickly and efficiently without retaining the client's private data. Our\nexperiments demonstrate that FedMeNF achieves fast optimization speed and\nrobust reconstruction performance, even with few-shot or non-IID data across\ndiverse data modalities, while preserving client data privacy.", "AI": {"tldr": "FedMeNF is a new Federated Meta-Learning method for neural fields that improves efficiency and privacy on edge devices using a privacy-preserving loss function.", "motivation": "Neural fields are memory-efficient but require significant data and computation, limiting their use on edge devices. Traditional Federated Meta-Learning (FML) for neural fields faces privacy leakage issues.", "method": "FedMeNF, a Federated Meta-Learning approach utilizing a new privacy-preserving loss function to regulate privacy leakage during local meta-optimization, allowing efficient learning without retaining client's private data.", "result": "Experiments show FedMeNF achieves fast optimization speed and robust reconstruction performance, preserving client data privacy.", "conclusion": "FedMeNF enables fast optimization and robust reconstruction for neural fields on resource-constrained edge devices, even with few-shot or non-IID data across diverse modalities, while preserving client data privacy through a novel privacy-preserving loss function."}}
{"id": "2508.05906", "categories": ["quant-ph", "cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2508.05906", "abs": "https://arxiv.org/abs/2508.05906", "authors": ["Hyunseok Oh", "Viraj Dharod", "Carl Padgett", "Lillian B. Hughes", "Jayameenakshi Venkatraman", "Shreyas Parthasarathy", "Ekaterina Osipova", "Ian Hedgepeth", "Jeffrey V. Cady", "Luca Basso", "Yongqiang Wang", "Michael Titze", "Edward S. Bielejec", "Andrew M. Mounce", "Dirk Bouwmeester", "Ania C. Bleszynski Jayich"], "title": "A spin-embedded diamond optomechanical resonator with mechanical quality factor exceeding one million", "comment": null, "summary": "Diamond optomechanical crystal (OMC) devices with embedded color center spins\nare promising platforms for a broad range of applications in quantum sensing,\nnetworking, and computing applications, offering an interface between a\nGHz-frequency mechanical mode and both optical photons and coherent spins. A\ncrucial but elusive step towards realizing this platform is to engineer a\ndevice with a high-quality factor mechanical mode while preserving the\nbulk-like coherence of embedded spins. Here we demonstrate sideband-resolved\ndiamond OMCs with mechanical quality factors in excess of $10^6$ at cryogenic\ntemperatures, and find coherence times up to $T_2$ = 270 $\\mu$s for embedded\nnitrogen vacancy (NV) centers. Furthermore, we measure these devices across\nfive orders of magnitude in intracavity optical power, demonstrating robust\npower handling and a high optomechanical cooperativity ($C\\gg1$) at cryogenic\ntemperatures that is essential for a broad range of quantum protocols requiring\nstrong, coherent interactions between photons and phonons. These results are\nenabled by a robust, high-throughput method for forming single-crystal diamond\nmembranes in combination with chemical vapor deposition (CVD) diamond\novergrowth with nitrogen $\\delta$-doping. We discuss the prospects of this\nplatform for hybrid spin-mechanical devices in the quantum regime.", "AI": {"tldr": "\u7814\u7a76\u4eba\u5458\u5229\u7528\u91d1\u521a\u77f3\u5149\u529b\u5b66\u6676\u4f53\uff08OMC\uff09\u5668\u4ef6\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u9ad8\u54c1\u8d28\u673a\u68b0\u6a21\u5f0f\u548c\u957f\u76f8\u5e72\u65f6\u95f4\u7684\u91d1\u521a\u77f3NV\u8272\u5fc3\uff0c\u4e3a\u91cf\u5b50\u6280\u672f\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "motivation": "\u4e3a\u4e86\u5b9e\u73b0\u91d1\u521a\u77f3\u5149\u529b\u5b66\u6676\u4f53\uff08OMC\uff09\u5668\u4ef6\u5728\u91cf\u5b50\u4f20\u611f\u3001\u7f51\u7edc\u548c\u8ba1\u7b97\u4e2d\u7684\u5e94\u7528\uff0c\u9700\u8981\u5de5\u7a0b\u5316\u4e00\u79cd\u80fd\u591f\u540c\u65f6\u4fdd\u6301\u9ad8\u54c1\u8d28\u56e0\u6570\u673a\u68b0\u6a21\u5f0f\u548c\u5d4c\u5165\u81ea\u65cb\u4f53\u76f8\u5e72\u6027\u7684\u5668\u4ef6\u3002\u672c\u7814\u7a76\u65e8\u5728\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\u3002", "method": "\u672c\u7814\u7a76\u901a\u8fc7\u7ed3\u5408\u5355\u6676\u91d1\u521a\u77f3\u8584\u819c\u5f62\u6210\u6280\u672f\u548c\u5316\u5b66\u6c14\u76f8\u6c89\u79ef\uff08CVD\uff09\u91d1\u521a\u77f3\u8fc7\u751f\u957f\u6280\u672f\uff0c\u5236\u5907\u4e86\u5177\u6709\u5d4c\u5165\u8272\u5fc3\u81ea\u65cb\u7684\u91d1\u521a\u77f3\u5149\u529b\u5b66\u6676\u4f53\uff08OMC\uff09\u5668\u4ef6\u3002\u5bf9\u8be5\u5668\u4ef6\u8fdb\u884c\u4e86\u4f4e\u6e29\u4e0b\u7684\u673a\u68b0\u54c1\u8d28\u56e0\u6570\u3001NV\u8272\u5fc3\u76f8\u5e72\u65f6\u95f4\u548c\u8154\u5185\u5149\u529f\u7387\u7684\u6d4b\u91cf\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u9ad8\u5149\u673a\u534f\u540c\u4f5c\u7528\u3002", "result": "\u672c\u7814\u7a76\u6210\u529f\u5c55\u793a\u4e86 sideband-resolved \u91d1\u521a\u77f3 OMC \u5668\u4ef6\uff0c\u5176\u5728\u4f4e\u6e29\u4e0b\u7684\u673a\u68b0\u54c1\u8d28\u56e0\u6570\u8d85\u8fc7 $10^6$\uff0c\u5d4c\u5165\u7684 NV \u8272\u5fc3\u76f8\u5e72\u65f6\u95f4\u6700\u957f\u53ef\u8fbe $T_2$ = 270 $\\mu$s\u3002\u6b64\u5916\uff0c\u5728\u4e94\u4e2a\u6570\u91cf\u7ea7\u7684\u8154\u5185\u5149\u529f\u7387\u8303\u56f4\u5185\uff0c\u5668\u4ef6\u8868\u73b0\u51fa\u7a33\u5b9a\u7684\u529f\u7387\u5904\u7406\u80fd\u529b\u548c\u9ad8\u5149\u673a\u534f\u540c\u4f5c\u7528 ($C\\gg1$)\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u5177\u6709\u9ad8\u54c1\u8d28\u56e0\u6570\u673a\u68b0\u6a21\u5f0f\u548c\u957f\u76f8\u5e72\u65f6\u95f4\u91d1\u521a\u77f3NV\u8272\u5fc3\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u5728\u91cf\u5b50\u8ba1\u7b97\u3001\u4f20\u611f\u548c\u7f51\u7edc\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.05749", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.05749", "abs": "https://arxiv.org/abs/2508.05749", "authors": ["Guilherme A. Bridi", "Debbie Lim", "Lirand\u00eb Pira", "Raqueline A. M. Santos", "Franklin de L. Marquezino", "Soumik Adhikary"], "title": "Expressivity Limits and Trainability Guarantees in Quantum Walk-based Optimization", "comment": "21 pages, 1 figure", "summary": "Quantum algorithms have emerged as a promising tool to solve combinatorial\noptimization problems. The quantum walk optimization algorithm (QWOA) is one\nsuch variational approach that has recently gained attention. In the broader\ncontext of variational quantum algorithms (VQAs), understanding the\nexpressivity and trainability of the ansatz has proven critical for evaluating\ntheir performance. A key method to study both these aspects involves analyzing\nthe dimension of the dynamic Lie algebra (DLA). In this work, we derive novel\nupper bounds on the DLA dimension for QWOA applied to arbitrary optimization\nproblems. The consequence of our result is twofold: (a) it allows us to\nidentify complexity-theoretic conditions under which QWOA must be\noverparameterized to obtain optimal or approximate solutions, and (b) it\nimplies the absence of barren plateaus in the loss landscape of QWOA for\n$\\mathsf{NP}$ optimization problems with polynomially bounded cost functions\n($\\mathsf{NPO}\\text{-}\\mathsf{PB}$).", "AI": {"tldr": "\u91cf\u5b50\u7b97\u6cd5\u5728\u89e3\u51b3\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u65b9\u9762\u663e\u793a\u51fa\u6f5c\u529b\u3002\u672c\u7814\u7a76\u5206\u6790\u4e86\u91cf\u5b50\u6e38\u8d70\u4f18\u5316\u7b97\u6cd5\uff08QWOA\uff09\u7684\u8868\u8fbe\u80fd\u529b\u548c\u53ef\u8bad\u7ec3\u6027\uff0c\u5f97\u51fa\u4e86\u5176\u7ef4\u5ea6\u4e0a\u9650\uff0c\u5e76\u8868\u660e\u5728NP\u4f18\u5316\u95ee\u9898\u4e2d\u4e0d\u5b58\u5728\u65e0 \u09aa\u09cd\u09b0\u09a4\u09bf\u09b6\u09cd\u09b0\u09c1\u09a4\u09bf\u5e73\u539f\u3002", "motivation": "\u7406\u89e3\u91cf\u5b50\u7b97\u6cd5\uff08\u7279\u522b\u662fQWOA\uff09\u7684\u8868\u8fbe\u80fd\u529b\u548c\u53ef\u8bad\u7ec3\u6027\u5bf9\u4e8e\u8bc4\u4f30\u5176\u5728\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u4e2d\u7684\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u5206\u6790\u52a8\u6001\u674e\u4ee3\u6570\uff08DLA\uff09\u7684\u7ef4\u5ea6\u6765\u7814\u7a76QWOA\u7684\u8868\u8fbe\u80fd\u529b\u548c\u53ef\u8bad\u7ec3\u6027\u3002", "result": "\u5f97\u51fa\u4e86QWOA\u9002\u7528\u4e8eNP\u4f18\u5316\u95ee\u9898\u7684\u7ef4\u5ea6\u4e0a\u9650\uff0c\u5e76\u6307\u51fa\u4e86\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0bQWOA\u4e3a\u4e86\u83b7\u5f97\u6700\u4f18\u6216\u8fd1\u4f3c\u89e3\u5fc5\u987b\u8fc7\u53c2\u6570\u5316\u3002", "conclusion": "\u8be5\u7814\u7a76\u63a8\u5bfc\u4e86\u91cf\u5b50\u6e38\u8d70\u4f18\u5316\u7b97\u6cd5\uff08QWOA\uff09\u7528\u4e8e\u4efb\u610f\u4f18\u5316\u95ee\u9898\u7684\u7ef4\u5ea6\u4e0a\u9650\uff0c\u63ed\u793a\u4e86QWOA\u5728\u89e3\u51b3NP\u4f18\u5316\u95ee\u9898\u65f6\u907f\u514d\u4e86\u65e0 \u09aa\u09cd\u09b0\u09a4\u09bf\u09b6\u09cd\u09b0\u09c1\u09a4\u09bf\u5e73\u539f\u3002"}}
{"id": "2508.05938", "categories": ["cs.CL", "cs.AI", "cs.CY", "I.2.7; K.4"], "pdf": "https://arxiv.org/pdf/2508.05938", "abs": "https://arxiv.org/abs/2508.05938", "authors": ["Rafal Kocielnik", "Min Kim", "Penphob", "Boonyarungsrit", "Fereshteh Soltani", "Deshawn Sambrano", "Animashree Anandkumar", "R. Michael Alvarez"], "title": "Prosocial Behavior Detection in Player Game Chat: From Aligning Human-AI Definitions to Efficient Annotation at Scale", "comment": "9 pages, 4 figures, 4 tables", "summary": "Detecting prosociality in text--communication intended to affirm, support, or\nimprove others' behavior--is a novel and increasingly important challenge for\ntrust and safety systems. Unlike toxic content detection, prosociality lacks\nwell-established definitions and labeled data, requiring new approaches to both\nannotation and deployment. We present a practical, three-stage pipeline that\nenables scalable, high-precision prosocial content classification while\nminimizing human labeling effort and inference costs. First, we identify the\nbest LLM-based labeling strategy using a small seed set of human-labeled\nexamples. We then introduce a human-AI refinement loop, where annotators review\nhigh-disagreement cases between GPT-4 and humans to iteratively clarify and\nexpand the task definition-a critical step for emerging annotation tasks like\nprosociality. This process results in improved label quality and definition\nalignment. Finally, we synthesize 10k high-quality labels using GPT-4 and train\na two-stage inference system: a lightweight classifier handles high-confidence\npredictions, while only $\\sim$35\\% of ambiguous instances are escalated to\nGPT-4o. This architecture reduces inference costs by $\\sim$70% while achieving\nhigh precision ($\\sim$0.90). Our pipeline demonstrates how targeted human-AI\ninteraction, careful task formulation, and deployment-aware architecture design\ncan unlock scalable solutions for novel responsible AI tasks.", "AI": {"tldr": "\u901a\u8fc7\u7ed3\u5408\u4eba\u7c7b\u667a\u80fd\u548c\u4eba\u5de5\u667a\u80fd\uff0c\u5e76\u4ed4\u7ec6\u8bbe\u8ba1\u7cfb\u7edf\u67b6\u6784\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u68c0\u6d4b\u6587\u672c\u4e2d\u7684\u4eb2\u793e\u4f1a\u6027\uff0c\u540c\u65f6\u964d\u4f4e\u6210\u672c\u3002", "motivation": "\u68c0\u6d4b\u6587\u672c\u4e2d\u7684\u4eb2\u793e\u4f1a\u6027\uff08\u65e8\u5728\u80af\u5b9a\u3001\u652f\u6301\u6216\u6539\u5584\u4ed6\u4eba\u884c\u4e3a\u7684\u6c9f\u901a\uff09\u662f\u4fe1\u4efb\u548c\u5b89\u5168\u7cfb\u7edf\u9762\u4e34\u7684\u65b0\u9896\u4e14\u65e5\u76ca\u91cd\u8981\u7684\u6311\u6218\u3002\u4e0e\u6709\u6bd2\u5185\u5bb9\u68c0\u6d4b\u4e0d\u540c\uff0c\u4eb2\u793e\u4f1a\u6027\u7f3a\u4e4f\u65e2\u5b9a\u7684\u5b9a\u4e49\u548c\u6807\u8bb0\u6570\u636e\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u4e09\u9636\u6bb5\u6d41\u7a0b\uff0c\u5305\u62ec\u57fa\u4e8e LLM \u7684\u6807\u6ce8\u7b56\u7565\u9009\u62e9\u3001\u4eba\u7c7b-AI \u7ec6\u5316\u5faa\u73af\u4ee5\u53ca\u91c7\u7528\u8f7b\u91cf\u7ea7\u5206\u7c7b\u5668\u548c GPT-4o \u7684\u4e24\u9636\u6bb5\u63a8\u7406\u7cfb\u7edf\u3002", "result": "\u8be5\u7cfb\u7edf\u5b9e\u73b0\u4e86\u7ea6 0.90 \u7684\u9ad8\u7cbe\u5ea6\uff0c\u540c\u65f6\u5c06\u63a8\u7406\u6210\u672c\u964d\u4f4e\u4e86\u7ea6 70%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u65b0\u5174\u7684\u8d1f\u8d23\u4efb\u4eba\u5de5\u667a\u80fd\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5b83\u7ed3\u5408\u4e86\u4eba\u7c7b\u4e0e\u4eba\u5de5\u667a\u80fd\u7684\u4e92\u52a8\u3001\u4ed4\u7ec6\u7684\u4efb\u52a1\u6784\u5efa\u548c\u9762\u5411\u90e8\u7f72\u7684\u67b6\u6784\u8bbe\u8ba1\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u7684\u4eb2\u793e\u4f1a\u5185\u5bb9\u5206\u7c7b\u3002"}}
{"id": "2508.05829", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05829", "abs": "https://arxiv.org/abs/2508.05829", "authors": ["Guoping Xu", "Hua-Chieh Shao", "You Zhang"], "title": "TSMS-SAM2: Multi-scale Temporal Sampling Augmentation and Memory-Splitting Pruning for Promptable Video Object Segmentation and Tracking in Surgical Scenarios", "comment": "23 pages, 5 figures", "summary": "Promptable video object segmentation and tracking (VOST) has seen significant\nadvances with the emergence of foundation models like Segment Anything Model 2\n(SAM2); however, their application in surgical video analysis remains\nchallenging due to complex motion dynamics and the redundancy of memory that\nimpedes effective learning. In this work, we propose TSMS-SAM2, a novel\nframework that enhances promptable VOST in surgical videos by addressing\nchallenges of rapid object motion and memory redundancy in SAM2. TSMS-SAM2\nintroduces two key strategies: multi-temporal-scale video sampling augmentation\nto improve robustness against motion variability, and a memory splitting and\npruning mechanism that organizes and filters past frame features for more\nefficient and accurate segmentation. Evaluated on EndoVis2017 and EndoVis2018\ndatasets, TSMS-SAM2 achieved the highest mean Dice scores of 95.24 and 86.73,\nrespectively, outperforming prior SAM-based and task-specific methods.\nExtensive ablation studies confirm the effectiveness of multiscale temporal\naugmentation and memory splitting, highlighting the framework's potential for\nrobust, efficient segmentation in complex surgical scenarios. Our source code\nwill be available at https://github.com/apple1986/TSMS-SAM2.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.06095", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06095", "abs": "https://arxiv.org/abs/2508.06095", "authors": ["Mitchell Abrams", "Thies Oelerich", "Christian Hartl-Nesic", "Andreas Kugi", "Matthias Scheutz"], "title": "Incremental Language Understanding for Online Motion Planning of Robot Manipulators", "comment": "8 pages, 9 figures, accepted at IROS 2025", "summary": "Human-robot interaction requires robots to process language incrementally,\nadapting their actions in real-time based on evolving speech input. Existing\napproaches to language-guided robot motion planning typically assume fully\nspecified instructions, resulting in inefficient stop-and-replan behavior when\ncorrections or clarifications occur. In this paper, we introduce a novel\nreasoning-based incremental parser which integrates an online motion planning\nalgorithm within the cognitive architecture. Our approach enables continuous\nadaptation to dynamic linguistic input, allowing robots to update motion plans\nwithout restarting execution. The incremental parser maintains multiple\ncandidate parses, leveraging reasoning mechanisms to resolve ambiguities and\nrevise interpretations when needed. By combining symbolic reasoning with online\nmotion planning, our system achieves greater flexibility in handling speech\ncorrections and dynamically changing constraints. We evaluate our framework in\nreal-world human-robot interaction scenarios, demonstrating online adaptions of\ngoal poses, constraints, or task objectives. Our results highlight the\nadvantages of integrating incremental language understanding with real-time\nmotion planning for natural and fluid human-robot collaboration. The\nexperiments are demonstrated in the accompanying video at\nwww.acin.tuwien.ac.at/42d5.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4e86\u589e\u91cf\u8bed\u8a00\u7406\u89e3\u548c\u5b9e\u65f6\u8fd0\u52a8\u89c4\u5212\u7684\u65b0\u65b9\u6cd5\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u66f4\u6d41\u7545\u5730\u54cd\u5e94\u548c\u9002\u5e94\u4e0d\u65ad\u53d8\u5316\u7684\u8bed\u97f3\u6307\u4ee4\uff0c\u63d0\u9ad8\u4e86\u4eba\u673a\u534f\u4f5c\u7684\u6548\u7387\u548c\u81ea\u7136\u5ea6\u3002", "motivation": "\u73b0\u6709\u6280\u672f\u5728\u8bed\u8a00\u5f15\u5bfc\u673a\u5668\u4eba\u8fd0\u52a8\u89c4\u5212\u65f6\u901a\u5e38\u5047\u8bbe\u6307\u4ee4\u662f\u5b8c\u5168\u6307\u5b9a\u7684\uff0c\u8fd9\u5728\u53d1\u751f\u66f4\u6b63\u6216\u6f84\u6e05\u65f6\u4f1a\u5bfc\u81f4\u4f4e\u6548\u7684\u505c\u6b62\u548c\u91cd\u65b0\u89c4\u5212\u884c\u4e3a\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5b9e\u65f6\u9002\u5e94\u4e0d\u65ad\u53d8\u5316\u7684\u8bed\u8a00\u8f93\u5165\u5e76\u66f4\u65b0\u8fd0\u52a8\u89c4\u5212\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u63a8\u7406\u7684\u589e\u91cf\u89e3\u6790\u5668\uff0c\u5c06\u5728\u7ebf\u8fd0\u52a8\u89c4\u5212\u7b97\u6cd5\u96c6\u6210\u5230\u8ba4\u77e5\u67b6\u6784\u4e2d\uff0c\u80fd\u591f\u6301\u7eed\u9002\u5e94\u52a8\u6001\u8bed\u8a00\u8f93\u5165\uff0c\u4ece\u800c\u65e0\u9700\u91cd\u65b0\u5f00\u59cb\u6267\u884c\u5373\u53ef\u66f4\u65b0\u8fd0\u52a8\u89c4\u5212\u3002", "result": "\u5728\u73b0\u5b9e\u4e16\u754c\u7684\u4eba\u673a\u4ea4\u4e92\u573a\u666f\u4e2d\u8bc4\u4f30\u4e86\u8be5\u6846\u67b6\uff0c\u5c55\u793a\u4e86\u5bf9\u76ee\u6807\u59ff\u52bf\u3001\u7ea6\u675f\u6216\u4efb\u52a1\u76ee\u6807\u7684\u5728\u7ebf\u9002\u5e94\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5904\u7406\u8bed\u97f3\u66f4\u6b63\u548c\u52a8\u6001\u53d8\u5316\u7ea6\u675f\u65b9\u9762\u7684\u4f18\u52bf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5c06\u589e\u91cf\u8bed\u8a00\u7406\u89e3\u4e0e\u5b9e\u65f6\u8fd0\u52a8\u89c4\u5212\u76f8\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u66f4\u7075\u6d3b\u7684\u8bed\u97f3\u66f4\u6b63\u548c\u52a8\u6001\u53d8\u5316\u7ea6\u675f\u5904\u7406\uff0c\u4ece\u800c\u5728\u73b0\u5b9e\u4e16\u754c\u7684\u4eba\u673a\u4ea4\u4e92\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86\u66f4\u81ea\u7136\u6d41\u7545\u7684\u4eba\u673a\u534f\u4f5c\u3002"}}
{"id": "2508.06060", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06060", "abs": "https://arxiv.org/abs/2508.06060", "authors": ["Sankarshan Damle", "Boi Faltings"], "title": "LLMs for Resource Allocation: A Participatory Budgeting Approach to Inferring Preferences", "comment": "Published in the Proceedings of the 28th European Conference on\n  Artificial Intelligence (ECAI 2025)", "summary": "Large Language Models (LLMs) are increasingly expected to handle complex\ndecision-making tasks, yet their ability to perform structured resource\nallocation remains underexplored. Evaluating their reasoning is also difficult\ndue to data contamination and the static nature of existing benchmarks. We\npresent a dual-purpose framework leveraging Participatory Budgeting (PB) both\nas (i) a practical setting for LLM-based resource allocation and (ii) an\nadaptive benchmark for evaluating their reasoning capabilities. We task LLMs\nwith selecting project subsets under feasibility (e.g., budget) constraints via\nthree prompting strategies: greedy selection, direct optimization, and a\nhill-climbing-inspired refinement. We benchmark LLMs' allocations against a\nutility-maximizing oracle. Interestingly, we also test whether LLMs can infer\nstructured preferences from natural-language voter input or metadata, without\nexplicit votes. By comparing allocations based on inferred preferences to those\nfrom ground-truth votes, we evaluate LLMs' ability to extract preferences from\nopen-ended input. Our results underscore the role of prompt design and show\nthat LLMs hold promise for mechanism design with unstructured inputs.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u53c2\u4e0e\u5f0f\u9884\u7b97\u4e2d\u7684\u8d44\u6e90\u5206\u914d\u548c\u63a8\u7406\u80fd\u529b\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u63d0\u793a\u8bcd\u8bbe\u8ba1\u5bf9LLM\u7684\u8868\u73b0\u81f3\u5173\u91cd\u8981\uff0c\u5e76\u4e14LLMs\u6709\u6f5c\u529b\u5904\u7406\u975e\u7ed3\u6784\u5316\u7684\u7528\u6237\u8f93\u5165\u3002", "motivation": "LLMs\u5728\u5904\u7406\u590d\u6742\u51b3\u7b56\u4efb\u52a1\u65b9\u9762\u7684\u6f5c\u529b\u65e5\u76ca\u589e\u957f\uff0c\u5c24\u5176\u662f\u5728\u7ed3\u6784\u5316\u8d44\u6e90\u5206\u914d\u65b9\u9762\uff0c\u4f46\u73b0\u6709\u57fa\u51c6\u96be\u4ee5\u8bc4\u4f30\u5176\u63a8\u7406\u80fd\u529b\uff0c\u4e14\u6570\u636e\u6c61\u67d3\u95ee\u9898\u666e\u904d\u5b58\u5728\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u53c2\u4e0e\u5f0f\u9884\u7b97\uff08PB\uff09\u7684\u53cc\u91cd\u6846\u67b6\uff0c\u7528\u4e8eLLM\u8d44\u6e90\u5206\u914d\u548c\u4f5c\u4e3a\u81ea\u9002\u5e94\u57fa\u51c6\u3002\u6d4b\u8bd5\u4e86\u4e09\u79cd\u63d0\u793a\u7b56\u7565\uff1a\u8d2a\u5a6a\u9009\u62e9\u3001\u76f4\u63a5\u4f18\u5316\u548c\u7c7b\u4f3c\u722c\u5c71\u6cd5\u7684\u4f18\u5316\u3002\u8bc4\u4f30\u4e86LLM\u4ece\u81ea\u7136\u8bed\u8a00\u9009\u6c11\u8f93\u5165\u6216\u5143\u6570\u636e\u4e2d\u63a8\u65ad\u7ed3\u6784\u5316\u504f\u597d\u4ee5\u53ca\u8fdb\u884c\u8d44\u6e90\u5206\u914d\u7684\u80fd\u529b\u3002", "result": "LLMs\u5728\u8d44\u6e90\u5206\u914d\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u63d0\u793a\u8bcd\u8bbe\u8ba1\u5bf9\u7ed3\u679c\u6709\u663e\u8457\u5f71\u54cd\u3002LLMs\u80fd\u591f\u4ece\u975e\u7ed3\u6784\u5316\u8f93\u5165\u4e2d\u63a8\u65ad\u7528\u6237\u504f\u597d\uff0c\u4f46\u5176\u51c6\u786e\u6027\u6709\u5f85\u63d0\u9ad8\u3002\u4e0e\u6700\u4f18\u5206\u914d\u76f8\u6bd4\uff0cLLMs\u7684\u5206\u914d\u6709\u63d0\u5347\u7a7a\u95f4\u3002", "conclusion": "LLMs\u5728\u673a\u5236\u8bbe\u8ba1\u548c\u5904\u7406\u975e\u7ed3\u6784\u5316\u8f93\u5165\u65b9\u9762\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46\u63d0\u793a\u8bcd\u8bbe\u8ba1\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2508.05915", "categories": ["cs.LG", "62M10"], "pdf": "https://arxiv.org/pdf/2508.05915", "abs": "https://arxiv.org/abs/2508.05915", "authors": ["Alex Glushkovsky"], "title": "Dual Signal Decomposition of Stochastic Time Series", "comment": "21 pages, 9 figures, 1 table", "summary": "The research paper addresses decomposition of a stochastic time series into\nthree time series representing a dual signal i.e., the mean and the dispersion,\nwith noise isolated. Decomposition is done by applying machine learning to fit\na dual signal. Machine learning minimizes the loss function which compromises\nbetween fitting the original time series and penalizing irregularities of the\ndual signal. The latter includes terms based on the first and second order\nderivatives along time. To preserve special patterns, weighting of the\nregularization components of the loss function has been introduced based on\nStatistical Process Control methodology. The proposed decomposition can be\napplied as a smoothing algorithm against the mean and dispersion of the time\nseries. By isolating noise, the proposed decomposition can be seen as a\ndenoising algorithm. Two approaches of the learning process have been\nconsidered: sequential and jointly. The former approach learns the mean signal\nfirst and then dispersion. The latter approach fits the dual signal jointly.\nJointly learning can uncover complex relationships for the time series with\nheteroskedasticity. Learning has been set by solving the direct non-linear\nunconstrained optimization problem or by applying neural networks that have\nsequential or twin output architectures. Tuning of the loss function\nhyperparameters focuses on the isolated noise to be a stationary stochastic\nprocess without autocorrelation properties. Depending on the applications, the\nhyperparameters of the learning can be tuned towards either the discrete states\nby stepped signal or smoothed series. The decomposed dual signal can be\nrepresented on the 2D space and used to learn inherent structures, to forecast\nboth mean and dispersion, or to analyze cross effects in case of multiple time\nseries.", "AI": {"tldr": "\u4e00\u79cd\u65b0\u7684\u65f6\u95f4\u5e8f\u5217\u5206\u89e3\u65b9\u6cd5\uff0c\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u5206\u79bb\u5747\u503c\u3001\u79bb\u6563\u5ea6\u548c\u566a\u58f0\u3002", "motivation": "\u7814\u7a76\u65f6\u95f4\u5e8f\u5217\u7684\u5206\u89e3\uff0c\u5c06\u5176\u5206\u89e3\u4e3a\u4e09\u4e2a\u8868\u793a\u53cc\u4fe1\u53f7\uff08\u5747\u503c\u548c\u79bb\u6563\u5ea6\uff09\u5e76\u5206\u79bb\u566a\u58f0\u7684\u65f6\u95f4\u5e8f\u5217\u3002", "method": "\u901a\u8fc7\u5e94\u7528\u673a\u5668\u5b66\u4e60\u62df\u5408\u53cc\u4fe1\u53f7\uff0c\u6700\u5c0f\u5316\u635f\u5931\u51fd\u6570\uff0c\u8be5\u51fd\u6570\u5728\u62df\u5408\u539f\u59cb\u65f6\u95f4\u5e8f\u5217\u548c\u60e9\u7f5a\u53cc\u4fe1\u53f7\u7684\u6b63\u5219\u5316\u9879\u4e4b\u95f4\u8fdb\u884c\u6743\u8861\uff0c\u6b63\u5219\u5316\u9879\u57fa\u4e8e\u65f6\u95f4\u7684\u4e00\u9636\u548c\u4e8c\u9636\u5bfc\u6570\u3002\u4e3a\u4e86\u4fdd\u7559\u7279\u6b8a\u6a21\u5f0f\uff0c\u57fa\u4e8e\u7edf\u8ba1\u8fc7\u7a0b\u63a7\u5236\u65b9\u6cd5\u5b66\u5bf9\u635f\u5931\u51fd\u6570\u7684\u6b63\u5219\u5316\u5206\u91cf\u8fdb\u884c\u4e86\u52a0\u6743\u3002", "result": "\u7814\u7a76\u4e86\u987a\u5e8f\u5b66\u4e60\u548c\u8054\u5408\u5b66\u4e60\u4e24\u79cd\u5b66\u4e60\u65b9\u6cd5\uff0c\u5e76\u8003\u8651\u4e86\u901a\u8fc7\u6c42\u89e3\u975e\u7ebf\u6027\u4f18\u5316\u95ee\u9898\u6216\u5e94\u7528\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u5b66\u4e60\u3002\u901a\u8fc7\u8c03\u6574\u635f\u5931\u51fd\u6570\u8d85\u53c2\u6570\uff0c\u53ef\u4ee5\u5c06\u566a\u58f0\u89c6\u4e3a\u5e73\u7a33\u968f\u673a\u8fc7\u7a0b\u3002\u5206\u89e3\u540e\u7684\u53cc\u4fe1\u53f7\u53ef\u4ee5\u5728\u4e8c\u7ef4\u7a7a\u95f4\u8868\u793a\uff0c\u7528\u4e8e\u5b66\u4e60\u56fa\u6709\u7ed3\u6784\u3001\u9884\u6d4b\u5747\u503c\u548c\u79bb\u6563\u5ea6\u6216\u5206\u6790\u591a\u65f6\u95f4\u5e8f\u5217\u7684\u4ea4\u53c9\u6548\u5e94\u3002", "conclusion": "\u8be5\u5206\u89e3\u65b9\u6cd5\u53ef\u4ee5\u4f5c\u4e3a\u5e73\u6ed1\u7b97\u6cd5\uff0c\u901a\u8fc7\u5206\u79bb\u566a\u58f0\u8fbe\u5230\u53bb\u566a\u7684\u76ee\u7684\u3002"}}
{"id": "2508.06246", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2508.06246", "abs": "https://arxiv.org/abs/2508.06246", "authors": ["Kang Wang", "Yingchen Peng", "Boying Huang", "Chun Zhou", "Qianlu Sun", "Fujie Tang", "Zhenglu Li", "Weigao Xu", "Kezhao Du", "Xingzhi Wang", "Ye Yang"], "title": "Correlation between Exciton Dynamics and Spin Structure in van der Waals Antiferromagnet NiPS3", "comment": null, "summary": "The emerging magnetic van der Waals (vdW) materials provide a platform for\nexploring novel physics regarding magnetism in low dimensions and developing\nultrathin spintronic applications. Here, we investigate the ultrafast dynamics\nof excitons in a vdW NiPS3 crystal. The temporal evolution of the transient\nreflection spectra indicates that the spin-correlated exciton is formed through\nphotocarrier localization, the rate of which is independent of the magnetic\ndegrees of freedom. However, the recombination rate of these excitons is\nconnected with the long-range magnetic order, and this connection probably\narise from a spin-flip rooted in the underlying antiferromagnetic background\nduring the recombination. Our findings uncover intertwined coupling between\ncarrier, lattice and spin degrees of freedom in NiPS3, which may pave the path\ntoward ultrafast optical manipulation of spin-related quantum states in vdW\nantiferromagnets.", "AI": {"tldr": "Ultrafast study of NiPS3 shows excitons couple to magnetism; recombination rate depends on magnetic order, possibly via spin-flips. This could enable optical control of spin states in vdW antiferromagnets.", "motivation": "Exploring novel physics of magnetism in low dimensions and developing ultrathin spintronic applications using emerging magnetic van der Waals (vdW) materials.", "method": "Ultrafast dynamics of excitons in a vdW NiPS3 crystal were investigated using transient reflection spectra.", "result": "Exciton formation through photocarrier localization is independent of magnetic degrees of freedom, but exciton recombination rate is linked to long-range magnetic order, likely due to spin-flip processes related to the antiferromagnetic background.", "conclusion": "This study reveals a strong coupling between carrier, lattice, and spin degrees of freedom in NiPS3, suggesting potential for ultrafast optical manipulation of spin-related quantum states in vdW antiferromagnets."}}
{"id": "2508.06433", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.06433", "abs": "https://arxiv.org/abs/2508.06433", "authors": ["Runnan Fang", "Yuan Liang", "Xiaobin Wang", "Jialong Wu", "Shuofei Qiao", "Pengjun Xie", "Fei Huang", "Huajun Chen", "Ningyu Zhang"], "title": "Memp: Exploring Agent Procedural Memory", "comment": "Work in progress", "summary": "Large Language Models (LLMs) based agents excel at diverse tasks, yet they\nsuffer from brittle procedural memory that is manually engineered or entangled\nin static parameters. In this work, we investigate strategies to endow agents\nwith a learnable, updatable, and lifelong procedural memory. We propose Memp\nthat distills past agent trajectories into both fine-grained, step-by-step\ninstructions and higher-level, script-like abstractions, and explore the impact\nof different strategies for Build, Retrieval, and Update of procedural memory.\nCoupled with a dynamic regimen that continuously updates, corrects, and\ndeprecates its contents, this repository evolves in lockstep with new\nexperience. Empirical evaluation on TravelPlanner and ALFWorld shows that as\nthe memory repository is refined, agents achieve steadily higher success rates\nand greater efficiency on analogous tasks. Moreover, procedural memory built\nfrom a stronger model retains its value: migrating the procedural memory to a\nweaker model yields substantial performance gains.", "AI": {"tldr": "Memp\u901a\u8fc7\u63d0\u70bc\u548c\u52a8\u6001\u66f4\u65b0\u7a0b\u5e8f\u5316\u8bb0\u5fc6\uff0c\u589e\u5f3a\u4e86LLM\u4ee3\u7406\u5728\u5904\u7406\u65b0\u7ecf\u9a8c\u65b9\u9762\u7684\u80fd\u529b\uff0c\u63d0\u9ad8\u4e86\u4efb\u52a1\u6210\u529f\u7387\u548c\u6548\u7387\uff0c\u5e76\u4e14\u8bb0\u5fc6\u7684\u53ef\u8fc1\u79fb\u6027\u4e5f\u5f97\u5230\u4e86\u9a8c\u8bc1\u3002", "motivation": "LLM\u9a71\u52a8\u7684\u4ee3\u7406\u867d\u7136\u64c5\u957f\u591a\u79cd\u4efb\u52a1\uff0c\u4f46\u5176\u7a0b\u5e8f\u5316\u8bb0\u5fc6\u8106\u5f31\uff0c\u9700\u8981\u624b\u52a8\u5de5\u7a0b\u5316\u6216\u88ab\u56fa\u5b9a\u5728\u9759\u6001\u53c2\u6570\u4e2d\u3002\u672c\u7814\u7a76\u65e8\u5728\u4e3a\u4ee3\u7406\u63d0\u4f9b\u53ef\u5b66\u4e60\u3001\u53ef\u66f4\u65b0\u3001\u7ec8\u8eab\u5236\u7684\u7a0b\u5e8f\u5316\u8bb0\u5fc6\u3002", "method": "\u63d0\u51faMemp\uff0c\u5c06\u8fc7\u53bb\u7684\u4ee3\u7406\u8f68\u8ff9\u63d0\u70bc\u6210\u7ec6\u7c92\u5ea6\u7684\u5206\u6b65\u6307\u4ee4\u548c\u66f4\u9ad8\u7ea7\u7684\u811a\u672c\u5316\u62bd\u8c61\uff0c\u5e76\u63a2\u7d22\u7a0b\u5e8f\u5316\u8bb0\u5fc6\u7684\u6784\u5efa\u3001\u68c0\u7d22\u548c\u66f4\u65b0\u7b56\u7565\u3002\u7ed3\u5408\u52a8\u6001\u4f53\u5236\uff0c\u6301\u7eed\u66f4\u65b0\u3001\u4fee\u6b63\u548c\u5f03\u7528\u5176\u5185\u5bb9\uff0c\u4f7f\u5176\u4e0e\u65b0\u7ecf\u9a8c\u540c\u6b65\u6f14\u8fdb\u3002", "result": "\u5728TravelPlanner\u548cALFWorld\u4e0a\u7684\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660e\uff0c\u968f\u7740\u8bb0\u5fc6\u5e93\u7684\u5b8c\u5584\uff0c\u4ee3\u7406\u5728\u7c7b\u4f3c\u4efb\u52a1\u4e0a\u7684\u6210\u529f\u7387\u548c\u6548\u7387\u7a33\u6b65\u63d0\u9ad8\u3002\u6b64\u5916\uff0c\u66f4\u5f3a\u7684\u6a21\u578b\u6784\u5efa\u7684\u7a0b\u5e8f\u5316\u8bb0\u5fc6\u4ecd\u7136\u6709\u4ef7\u503c\uff1a\u5c06\u7a0b\u5e8f\u5316\u8bb0\u5fc6\u8fc1\u79fb\u5230\u66f4\u5f31\u7684\u6a21\u578b\u53ef\u4ee5\u5e26\u6765\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "LLM\u9a71\u52a8\u7684\u4ee3\u7406\u53ef\u4ee5\u901a\u8fc7Memp\u5b66\u4e60\u3001\u66f4\u65b0\u548c\u7ef4\u62a4\u7a0b\u5e8f\u5316\u8bb0\u5fc6\uff0c\u4ece\u800c\u5728\u7c7b\u4f3c\u4efb\u52a1\u4e0a\u53d6\u5f97\u66f4\u9ad8\u7684\u6210\u529f\u7387\u548c\u6548\u7387\u3002"}}
{"id": "2508.06340", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.06340", "abs": "https://arxiv.org/abs/2508.06340", "authors": ["Danish Mehmood Mughal", "Daniyal Munir", "Qazi Arbab Ahmed", "Hans D. Schotten", "Thorsten Jungeblut", "Sang-Hyo Kim", "Min Young Chung"], "title": "MALRIS: Malicious Hardware in RIS-Assisted Wireless Communications", "comment": "Accepted for presentation at IEEE CSCN 2025", "summary": "Reconfigurable intelligent surfaces (RIS) enhance wireless communication by\ndynamically shaping the propagation environment, but their integration\nintroduces hardware-level security risks. This paper presents the concept of\nMalicious RIS (MALRIS), where compromised components behave adversarially, even\nunder passive operation. The focus of this work is on practical threats such as\nmanufacturing time tampering, malicious firmware, and partial element control.\nTwo representative attacks, power-splitting and element-splitting, are modeled\nto assess their impact. Simulations in a RIS-assisted system reveal that even a\nlimited hardware compromise can significantly degrade performance metrics such\nas bit error rate, throughput, and secrecy metrics. By exposing this overlooked\nthreat surface, this work aims to promote awareness and support secure,\ntrustworthy RIS deployment in future wireless networks.", "AI": {"tldr": "\u91cd\u6784\u667a\u80fd\u8868\u9762\uff08RIS\uff09\u53ef\u80fd\u5b58\u5728\u786c\u4ef6\u5b89\u5168\u98ce\u9669\uff0c\u88ab\u7be1\u6539\u7684RIS\uff08MALRIS\uff09\u5373\u4f7f\u5728\u88ab\u52a8\u64cd\u4f5c\u4e0b\u4e5f\u80fd\u53d1\u8d77\u653b\u51fb\uff0c\u5f71\u54cd\u901a\u4fe1\u6027\u80fd\u3002", "motivation": "\u968f\u7740RIS\u5728\u65e0\u7ebf\u901a\u4fe1\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5176\u786c\u4ef6\u5c42\u9762\u7684\u5b89\u5168\u98ce\u9669\u65e5\u76ca\u51f8\u663e\u3002\u672c\u7814\u7a76\u65e8\u5728\u8bc6\u522b\u5e76\u5206\u6790\u7531\u6076\u610f\u7be1\u6539RIS\u7ec4\u4ef6\uff08MALRIS\uff09\u5f15\u53d1\u7684\u5b9e\u9645\u5b89\u5168\u5a01\u80c1\uff0c\u5982\u5236\u9020\u8fc7\u7a0b\u4e2d\u7684\u7be1\u6539\u3001\u6076\u610f\u56fa\u4ef6\u548c\u90e8\u5206\u5355\u5143\u63a7\u5236\uff0c\u4ee5\u63d0\u9ad8\u4e1a\u754c\u5bf9\u8fd9\u4e00\u65b0\u5174\u5b89\u5168\u5a01\u80c1\u7684\u8ba4\u8bc6\u3002", "method": "\u672c\u6587\u901a\u8fc7\u6a21\u62df\u529f\u7387\u5206\u88c2\u548c\u5355\u5143\u5206\u88c2\u8fd9\u4e24\u79cd\u4ee3\u8868\u6027\u653b\u51fb\uff0c\u6765\u8bc4\u4f30\u786c\u4ef6\u5c42\u9762\u6076\u610f\u7be1\u6539\u5bf9RIS\u6027\u80fd\u7684\u5f71\u54cd\u3002\u7814\u7a76\u5728RIS\u8f85\u52a9\u7cfb\u7edf\u4e2d\u8fdb\u884c\u4e86\u4eff\u771f\uff0c\u5e76\u5206\u6790\u4e86\u8fd9\u4e9b\u653b\u51fb\u5bf9\u6bd4\u7279\u9519\u8bef\u7387\u3001\u541e\u5410\u91cf\u548c\u4fdd\u5bc6\u6027\u7b49\u5173\u952e\u6027\u80fd\u6307\u6807\u7684\u5f71\u54cd\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u5373\u4f7f\u662f\u6709\u9650\u7684\u786c\u4ef6\u7be1\u6539\uff0c\u4e5f\u80fd\u663e\u8457\u964d\u4f4eRIS\u8f85\u52a9\u7cfb\u7edf\u7684\u6bd4\u7279\u9519\u8bef\u7387\u3001\u541e\u5410\u91cf\u548c\u4fdd\u5bc6\u6027\u7b49\u6027\u80fd\u6307\u6807\u3002\u8fd9\u8bc1\u660e\u4e86MALRIS\u653b\u51fb\u7684\u6709\u6548\u6027\u548c\u6f5c\u5728\u5371\u5bb3\u3002", "conclusion": "\u672c\u7bc7\u8bba\u6587\u63ed\u793a\u4e86\u91cd\u6784\u667a\u80fd\u8868\u9762\uff08RIS\uff09\u5728\u786c\u4ef6\u5c42\u9762\u9762\u4e34\u7684\u5b89\u5168\u98ce\u9669\uff0c\u5e76\u63d0\u51fa\u4e86\u6076\u610fRIS\uff08MALRIS\uff09\u7684\u6982\u5ff5\uff0c\u5373\u88ab\u7be1\u6539\u7684\u7ec4\u4ef6\u5373\u4f7f\u5728\u88ab\u52a8\u64cd\u4f5c\u4e0b\u4e5f\u80fd\u4ea7\u751f\u5bf9\u6297\u6027\u884c\u4e3a\u3002\u901a\u8fc7\u6a21\u62df\u529f\u7387\u5206\u88c2\u548c\u5355\u5143\u5206\u88c2\u7b49\u653b\u51fb\uff0c\u8bc1\u660e\u4e86\u5373\u4f7f\u662f\u6709\u9650\u7684\u786c\u4ef6\u7be1\u6539\u4e5f\u80fd\u4e25\u91cd\u5f71\u54cd\u6bd4\u7279\u9519\u8bef\u7387\u3001\u541e\u5410\u91cf\u548c\u4fdd\u5bc6\u6027\u7b49\u6027\u80fd\u6307\u6807\u3002\u8be5\u7814\u7a76\u65e8\u5728\u63d0\u9ad8\u5bf9RIS\u5b89\u5168\u5a01\u80c1\u7684\u8ba4\u8bc6\uff0c\u4ee5\u652f\u6301\u672a\u6765\u65e0\u7ebf\u7f51\u7edc\u4e2d\u5b89\u5168\u53ef\u9760\u7684RIS\u90e8\u7f72\u3002"}}
{"id": "2508.05751", "categories": ["quant-ph", "cond-mat.quant-gas", "cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2508.05751", "abs": "https://arxiv.org/abs/2508.05751", "authors": ["Diego Barberena"], "title": "Generalized Holstein-Primakoff mapping and $1/N$ expansion of collective spin systems undergoing single particle dissipation", "comment": "18 pages + 14 appendix, 9 figures", "summary": "We develop a generalization of the Schwinger boson and Holstein-Primakoff\ntransformations that is applicable to ensembles of $N$ spin $1/2$'s with weak\npermutational symmetry. These generalized mappings are constructed by\nintroducing two independent bosonic variables that describe fluctuations\nparallel and transverse to the collective Bloch vector built out of the\noriginal spin $1/2$'s. Using this representation, we develop a systematic $1/N$\nexpansion and write down explicitly leading and next-to-leading order terms. We\nthen illustrate how to apply these techniques using four example systems: (i)\nan ensemble of atoms undergoing spontaneous emission, incoherent pumping and\nsingle particle dephasing; (ii) a superradiant laser above and in the vicinity\nof the upper lasing transition; (iii) the all-to-all transverse field Ising\nmodel subject to incoherent pumping in the vicinity of its ordering phase\ntransition; and (iv) the Dicke model at finite temperature both away and in the\nvicinity of its thermal phase transition. Thus, these mappings provide a\ncommon, Bloch-sphere based, geometrical description of all-to-all systems\nsubject to single particle dissipation or at finite temperature, including\ntheir phase transitions.", "AI": {"tldr": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u9002\u7528\u4e8eN\u4e2a\u81ea\u65cb1/2\u96c6\u5408\u7684\u5e7f\u4e49\u53d8\u6362\uff0c\u53ef\u4ee5\u63cf\u8ff0\u5168\u5c40\u8026\u5408\u7cfb\u7edf\u53ca\u5176\u76f8\u53d8\u3002", "motivation": "\u9700\u8981\u4e00\u79cd\u9002\u7528\u4e8eN\u4e2a\u81ea\u65cb1/2\u96c6\u5408\u7684\u5e7f\u4e49\u53d8\u6362\uff0c\u8be5\u53d8\u6362\u5177\u6709\u5f31\u7f6e\u6362\u5bf9\u79f0\u6027\uff0c\u80fd\u591f\u63cf\u8ff0\u5355\u7c92\u5b50\u8017\u6563\u6216\u6709\u9650\u6e29\u5ea6\u4e0b\u7684\u5168\u5c40\u8026\u5408\u7cfb\u7edf\u53ca\u5176\u76f8\u53d8\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u4e24\u4e2a\u72ec\u7acb\u7684\u73bb\u8272\u5b50\u53d8\u91cf\u6765\u6784\u5efa\u5e7f\u4e49\u53d8\u6362\uff0c\u8be5\u53d8\u91cf\u63cf\u8ff0\u4e86\u76f8\u5bf9\u4e8e\u96c6\u4f53\u5e03\u6d1b\u8d6b\u77e2\u91cf\uff08\u7531\u539f\u59cb\u81ea\u65cb1/2\u6784\u6210\uff09\u7684\u5e76\u884c\u548c\u6a2a\u5411\u6da8\u843d\u3002\u5229\u7528\u8fd9\u79cd\u8868\u793a\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u7cfb\u7edf\u76841/N\u5c55\u5f00\uff0c\u5e76\u660e\u786e\u5199\u51fa\u4e86\u524d\u5bfc\u548c\u6b21\u524d\u5bfc\u9636\u9879\u3002", "result": "\u6211\u4eec\u5f00\u53d1\u4e86\u5e7f\u4e49\u53d8\u6362\uff0c\u5e76\u6f14\u793a\u4e86\u5176\u5728\u56db\u79cd\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\uff1a(i)\u7ecf\u5386\u81ea\u53d1\u8f90\u5c04\u3001\u4e0d\u76f8\u5e72\u6cf5\u6d66\u548c\u5355\u7c92\u5b50\u9000\u76f8\u5e72\u7684\u539f\u5b50\u7cfb\u7efc\uff1b(ii)\u8d85\u8f90\u5c04\u6fc0\u5149\u5668\uff1b(iii)\u53d7\u4e0d\u76f8\u5e72\u6cf5\u6d66\u5f71\u54cd\u7684\u6a2a\u5411\u573a\u4f0a\u8f9b\u6a21\u578b\uff1b(iv)\u8fea\u514b\u6a21\u578b\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u63d0\u51fa\u4e86\u4e00\u4e2a\u9002\u7528\u4e8e\u5177\u6709\u5f31\u7f6e\u6362\u5bf9\u79f0\u6027\u7684N\u4e2a\u81ea\u65cb1/2\u96c6\u5408\u7684\u5e7f\u4e49\u53d8\u6362\uff0c\u8be5\u53d8\u6362\u53ef\u4ee5\u51e0\u4f55\u5730\u63cf\u8ff0\u53d7\u5355\u7c92\u5b50\u8017\u6563\u6216\u6709\u9650\u6e29\u5ea6\u5f71\u54cd\u7684\u5168\u5c40\u8026\u5408\u7cfb\u7edf\u53ca\u5176\u76f8\u53d8\u3002"}}
{"id": "2508.05987", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.05987", "abs": "https://arxiv.org/abs/2508.05987", "authors": ["Chunyun Zhang", "Hongyan Zhao", "Chaoran Cui", "Qilong Song", "Zhiqing Lu", "Shuai Gong", "Kailin Liu"], "title": "Adversarial Topic-aware Prompt-tuning for Cross-topic Automated Essay Scoring", "comment": null, "summary": "Cross-topic automated essay scoring (AES) aims to develop a transferable\nmodel capable of effectively evaluating essays on a target topic. A significant\nchallenge in this domain arises from the inherent discrepancies between topics.\nWhile existing methods predominantly focus on extracting topic-shared features\nthrough distribution alignment of source and target topics, they often neglect\ntopic-specific features, limiting their ability to assess critical traits such\nas topic adherence. To address this limitation, we propose an Adversarial\nTOpic-aware Prompt-tuning (ATOP), a novel method that jointly learns\ntopic-shared and topic-specific features to improve cross-topic AES. ATOP\nachieves this by optimizing a learnable topic-aware prompt--comprising both\nshared and specific components--to elicit relevant knowledge from pre-trained\nlanguage models (PLMs). To enhance the robustness of topic-shared prompt\nlearning and mitigate feature scale sensitivity introduced by topic alignment,\nwe incorporate adversarial training within a unified regression and\nclassification framework. In addition, we employ a neighbor-based classifier to\nmodel the local structure of essay representations and generate pseudo-labels\nfor target-topic essays. These pseudo-labels are then used to guide the\nsupervised learning of topic-specific prompts tailored to the target topic.\nExtensive experiments on the publicly available ASAP++ dataset demonstrate that\nATOP significantly outperforms existing state-of-the-art methods in both\nholistic and multi-trait essay scoring. The implementation of our method is\npublicly available at: https://anonymous.4open.science/r/ATOP-A271.", "AI": {"tldr": "\u63d0\u51fa ATOP \u65b9\u6cd5\uff0c\u901a\u8fc7\u8054\u5408\u5b66\u4e60\u4e3b\u9898\u5171\u4eab\u548c\u4e3b\u9898\u7279\u5b9a\u7279\u5f81\uff0c\u5e76\u7ed3\u5408\u5bf9\u6297\u8bad\u7ec3\u548c\u4f2a\u6807\u7b7e\u6280\u672f\uff0c\u63d0\u5347\u4e86\u8de8\u4e3b\u9898\u81ea\u52a8\u4f5c\u6587\u8bc4\u5206\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u8de8\u4e3b\u9898\u81ea\u52a8\u4f5c\u6587\u8bc4\u5206 (AES) \u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u901a\u8fc7\u6e90\u57df\u548c\u76ee\u6807\u57df\u7684\u5206\u5e03\u5bf9\u9f50\u6765\u63d0\u53d6\u4e3b\u9898\u5171\u4eab\u7279\u5f81\uff0c\u4f46\u5ffd\u7565\u4e86\u4e3b\u9898\u7279\u5b9a\u7279\u5f81\uff0c\u9650\u5236\u4e86\u5176\u8bc4\u4f30\u4f9d\u9898\u6027\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a ATOP (Adversarial TOpic-aware Prompt-tuning) \u7684\u65b0\u9896\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u4f18\u5316\u53ef\u5b66\u4e60\u7684\u4e3b\u9898\u611f\u77e5\u63d0\u793a\uff08\u5305\u542b\u5171\u4eab\u548c\u7279\u5b9a\u7ec4\u4ef6\uff09\u6765\u5171\u540c\u5b66\u4e60\u4e3b\u9898\u5171\u4eab\u548c\u4e3b\u9898\u7279\u5b9a\u7279\u5f81\uff0c\u4ee5\u5f15\u53d1\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b (PLM) \u7684\u76f8\u5173\u77e5\u8bc6\u3002\u4e3a\u4e86\u589e\u5f3a\u4e3b\u9898\u5171\u4eab\u63d0\u793a\u5b66\u4e60\u7684\u9c81\u68d2\u6027\u5e76\u7f13\u89e3\u4e3b\u9898\u5bf9\u9f50\u5e26\u6765\u7684\u7279\u5f81\u5c3a\u5ea6\u654f\u611f\u6027\uff0c\u8be5\u65b9\u6cd5\u5728\u7edf\u4e00\u7684\u56de\u5f52\u548c\u5206\u7c7b\u6846\u67b6\u4e2d\u5f15\u5165\u4e86\u5bf9\u6297\u8bad\u7ec3\u3002\u6b64\u5916\uff0c\u8fd8\u91c7\u7528\u57fa\u4e8e\u90bb\u5c45\u7684\u5206\u7c7b\u5668\u6765\u6a21\u62df\u8bba\u6587\u8868\u793a\u7684\u5c40\u90e8\u7ed3\u6784\u5e76\u4e3a\u76ee\u6807\u4e3b\u9898\u8bba\u6587\u751f\u6210\u4f2a\u6807\u7b7e\uff0c\u4ee5\u6307\u5bfc\u7279\u5b9a\u4e8e\u4e3b\u9898\u7684\u63d0\u793a\u7684\u76d1\u7763\u5b66\u4e60\u3002", "result": "ATOP \u5728 ASAP++ \u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u6574\u4f53\u548c\u591a\u7ef4\u5ea6\u4f5c\u6587\u8bc4\u5206\u65b9\u9762\u5747\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "ATOP \u65b9\u6cd5\u5728\u6574\u4f53\u548c\u591a\u7ef4\u5ea6\u8bc4\u5206\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002"}}
{"id": "2508.06326", "categories": ["cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.06326", "abs": "https://arxiv.org/abs/2508.06326", "authors": ["Nathaniel Virgo", "Martin Biehl", "Manuel Baltieri", "Matteo Capucci"], "title": "A \"good regulator theorem\" for embodied agents", "comment": "Accepted at the Artificial Life conference 2025 (ALife 2025). 10\n  pages, 1 figure", "summary": "In a classic paper, Conant and Ashby claimed that \"every good regulator of a\nsystem must be a model of that system.\" Artificial Life has produced many\nexamples of systems that perform tasks with apparently no model in sight; these\nsuggest Conant and Ashby's theorem doesn't easily generalise beyond its\nrestricted setup. Nevertheless, here we show that a similar intuition can be\nfleshed out in a different way: whenever an agent is able to perform a\nregulation task, it is possible for an observer to interpret it as having\n\"beliefs\" about its environment, which it \"updates\" in response to sensory\ninput. This notion of belief updating provides a notion of model that is more\nsophisticated than Conant and Ashby's, as well as a theorem that is more\nbroadly applicable. However, it necessitates a change in perspective, in that\nthe observer plays an essential role in the theory: models are not a mere\nproperty of the system but are imposed on it from outside. Our theorem holds\nregardless of whether the system is regulating its environment in a classic\ncontrol theory setup, or whether it's regulating its own internal state; the\nmodel is of its environment either way. The model might be trivial, however,\nand this is how the apparent counterexamples are resolved.", "AI": {"tldr": "\u4e3b\u4f53\u80fd\u591f\u6267\u884c\u8c03\u8282\u4efb\u52a1\uff0c\u53ef\u4ee5\u88ab\u89c6\u4e3a\u62e5\u6709\u4e00\u4e2a\u53ef\u66f4\u65b0\u7684\u5173\u4e8e\u5176\u73af\u5883\u7684\u201c\u4fe1\u5ff5\u201d\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u6bd4Conant\u548cAshby\u7684\u6a21\u578b\u66f4\u5177\u666e\u9002\u6027\u3002", "motivation": "\u63a2\u7d22\u201c\u6bcf\u4e2a\u597d\u7684\u7cfb\u7edf\u8c03\u8282\u5668\u5fc5\u987b\u662f\u8be5\u7cfb\u7edf\u7684\u6a21\u578b\u201d\u8fd9\u4e00\u7ecf\u5178\u89c2\u70b9\u7684\u6cdb\u5316\u53ef\u80fd\u6027\uff0c\u7279\u522b\u662f\u5728\u4eba\u5de5\u667a\u80fd\u751f\u547d\uff08Artificial Life\uff09\u9886\u57df\u4e2d\u51fa\u73b0\u7684\u65e0\u9700\u663e\u5f0f\u6a21\u578b\u7684\u7cfb\u7edf\u3002", "method": "\u901a\u8fc7\u5c06\u201c\u4fe1\u5ff5\u66f4\u65b0\u201d\u89c6\u4e3a\u4e00\u79cd\u66f4\u590d\u6742\u7684\u6a21\u578b\u5f62\u5f0f\uff0c\u5e76\u63d0\u51fa\u4e00\u4e2a\u66f4\u5e7f\u6cdb\u9002\u7528\u7684\u5b9a\u7406\u6765\u9610\u8ff0\u8fd9\u4e00\u89c2\u70b9\u3002", "result": "\u5f53\u4e00\u4e2a\u4e3b\u4f53\u80fd\u591f\u6267\u884c\u8c03\u8282\u4efb\u52a1\u65f6\uff0c\u53ef\u4ee5\u5c06\u5176\u89e3\u91ca\u4e3a\u62e5\u6709\u4e00\u4e2a\u5173\u4e8e\u5176\u73af\u5883\u7684\u201c\u4fe1\u5ff5\u201d\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u80fd\u591f\u6839\u636e\u611f\u77e5\u8f93\u5165\u8fdb\u884c\u201c\u66f4\u65b0\u201d\u3002", "conclusion": "\u8be5\u5b9a\u7406\u9002\u7528\u4e8e\u7ecf\u5178\u63a7\u5236\u7406\u8bba\u8bbe\u7f6e\u6216\u7cfb\u7edf\u81ea\u8eab\u5185\u90e8\u72b6\u6001\u7684\u8c03\u8282\uff0c\u6a21\u578b\u59cb\u7ec8\u662f\u5173\u4e8e\u73af\u5883\u7684\u3002"}}
{"id": "2508.05851", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05851", "abs": "https://arxiv.org/abs/2508.05851", "authors": ["Ka-Wai Yung", "Felix J. S. Bragman", "Jialang Xu", "Imanol Luengo", "Danail Stoyanov", "Evangelos B. Mazomenos"], "title": "Temporal Cluster Assignment for Efficient Real-Time Video Segmentation", "comment": null, "summary": "Vision Transformers have substantially advanced the capabilities of\nsegmentation models across both image and video domains. Among them, the Swin\nTransformer stands out for its ability to capture hierarchical, multi-scale\nrepresentations, making it a popular backbone for segmentation in videos.\nHowever, despite its window-attention scheme, it still incurs a high\ncomputational cost, especially in larger variants commonly used for dense\nprediction in videos. This remains a major bottleneck for real-time,\nresource-constrained applications. Whilst token reduction methods have been\nproposed to alleviate this, the window-based attention mechanism of Swin\nrequires a fixed number of tokens per window, limiting the applicability of\nconventional pruning techniques. Meanwhile, training-free token clustering\napproaches have shown promise in image segmentation while maintaining window\nconsistency. Nevertheless, they fail to exploit temporal redundancy, missing a\nkey opportunity to further optimize video segmentation performance. We\nintroduce Temporal Cluster Assignment (TCA), a lightweight and effective,\nfine-tuning-free strategy that enhances token clustering by leveraging temporal\ncoherence across frames. Instead of indiscriminately dropping redundant tokens,\nTCA refines token clusters using temporal correlations, thereby retaining\nfine-grained details while significantly reducing computation. Extensive\nevaluations on YouTube-VIS 2019, YouTube-VIS 2021, OVIS, and a private surgical\nvideo dataset show that TCA consistently boosts the accuracy-speed trade-off of\nexisting clustering-based methods. Our results demonstrate that TCA generalizes\ncompetently across both natural and domain-specific videos.", "AI": {"tldr": "Swin Transformer \u5728\u89c6\u9891\u5206\u5272\u4e2d\u5b58\u5728\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a TCA \u7684\u7b56\u7565\uff0c\u901a\u8fc7\u5229\u7528\u65f6\u95f4\u4fe1\u606f\u6765\u4f18\u5316 token \u805a\u7c7b\uff0c\u4ece\u800c\u5728\u4e0d\u635f\u5931\u51c6\u786e\u6027\u7684\u60c5\u51b5\u4e0b\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u7684 Swin Transformer \u5728\u89c6\u9891\u5206\u5272\u4e2d\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u5c24\u5176\u662f\u5728\u8f83\u5927\u7684\u6a21\u578b\u4e2d\u3002\u867d\u7136\u4e00\u4e9b token \u7ea6\u51cf\u65b9\u6cd5\u88ab\u63d0\u51fa\uff0c\u4f46\u5b83\u4eec\u65e0\u6cd5\u5f88\u597d\u5730\u5e94\u7528\u4e8e Swin Transformer \u7684\u7a97\u53e3\u6ce8\u610f\u529b\u673a\u5236\u3002\u6b64\u5916\uff0c\u7528\u4e8e\u56fe\u50cf\u5206\u5272\u7684\u8bad\u7ec3\u65e0\u5173 token \u805a\u7c7b\u65b9\u6cd5\u672a\u80fd\u5229\u7528\u65f6\u95f4\u5197\u4f59\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u65f6\u95f4\u805a\u7c7b\u5206\u914d\uff08TCA\uff09\u7684\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u901a\u8fc7\u5229\u7528\u8de8\u5e27\u7684\u65f6\u95f4\u76f8\u5e72\u6027\u6765\u4f18\u5316 token \u805a\u7c7b\uff0c\u4ee5\u5728\u5206\u5272\u4efb\u52a1\u4e2d\u51cf\u5c11\u8ba1\u7b97\u91cf\u5e76\u63d0\u9ad8\u6548\u7387\u3002", "result": "TCA \u7b56\u7565\u5728 YouTube-VIS 2019\u3001YouTube-VIS 2021\u3001OVIS \u548c\u4e00\u4e2a\u79c1\u6709\u7684\u624b\u672f\u89c6\u9891\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u8bc4\u4f30\uff0c\u7ed3\u679c\u8868\u660e TCA \u80fd\u591f\u4e00\u81f4\u5730\u63d0\u9ad8\u73b0\u6709\u57fa\u4e8e\u805a\u7c7b\u7684\u65b9\u6cd5\u7684\u51c6\u786e-\u901f\u5ea6\u6743\u8861\u3002", "conclusion": "TCA \u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u65e0\u9700\u5fae\u8c03\u7684\u7b56\u7565\uff0c\u901a\u8fc7\u5229\u7528\u8de8\u5e27\u7684\u65f6\u95f4\u76f8\u5e72\u6027\u6765\u589e\u5f3a token \u805a\u7c7b\uff0c\u4ece\u800c\u5728\u63d0\u9ad8\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u5e76\u4e14\u5728\u81ea\u7136\u89c6\u9891\u548c\u7279\u5b9a\u9886\u57df\u89c6\u9891\u4e2d\u90fd\u8868\u73b0\u826f\u597d\u3002"}}
{"id": "2508.06096", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06096", "abs": "https://arxiv.org/abs/2508.06096", "authors": ["Eric Jing", "Abdeslam Boularias"], "title": "Bounding Distributional Shifts in World Modeling through Novelty Detection", "comment": "7 pages, 6 figures", "summary": "Recent work on visual world models shows significant promise in latent state\ndynamics obtained from pre-trained image backbones. However, most of the\ncurrent approaches are sensitive to training quality, requiring near-complete\ncoverage of the action and state space during training to prevent divergence\nduring inference. To make a model-based planning algorithm more robust to the\nquality of the learned world model, we propose in this work to use a\nvariational autoencoder as a novelty detector to ensure that proposed action\ntrajectories during planning do not cause the learned model to deviate from the\ntraining data distribution. To evaluate the effectiveness of this approach, a\nseries of experiments in challenging simulated robot environments was carried\nout, with the proposed method incorporated into a model-predictive control\npolicy loop extending the DINO-WM architecture. The results clearly show that\nthe proposed method improves over state-of-the-art solutions in terms of data\nefficiency.", "AI": {"tldr": "\u5728\u4e16\u754c\u6a21\u578b\u4e2d\u52a0\u5165\u65b0\u9896\u6027\u68c0\u6d4b\u5668\uff0c\u4ee5\u63d0\u9ad8\u89c4\u5212\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u4e3a\u4e86\u4f7f\u57fa\u4e8e\u6a21\u578b\u7684\u89c4\u5212\u7b97\u6cd5\u5bf9\u6240\u5b66\u4e60\u7684\u4e16\u754c\u6a21\u578b\u7684\u8d28\u91cf\u66f4\u52a0\u9c81\u68d2\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u9896\u7684\u68c0\u6d4b\u65b9\u6cd5\u6765\u9632\u6b62\u6a21\u578b\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u51fa\u73b0\u504f\u5dee\u3002", "method": "\u63d0\u51fa\u4f7f\u7528\u53d8\u5206\u81ea\u7f16\u7801\u5668\u4f5c\u4e3a\u65b0\u9896\u6027\u68c0\u6d4b\u5668\uff0c\u4ee5\u786e\u4fdd\u89c4\u5212\u671f\u95f4\u63d0\u51fa\u7684\u52a8\u4f5c\u8f68\u8ff9\u4e0d\u4f1a\u5bfc\u81f4\u6240\u5b66\u4e60\u7684\u6a21\u578b\u504f\u79bb\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u6a21\u62df\u673a\u5668\u4eba\u73af\u5883\u4e2d\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5e76\u7ed3\u5408\u5230\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u7b56\u7565\u5faa\u73af\u4e2d\uff0c\u6269\u5c55\u4e86DINO-WM\u67b6\u6784\uff0c\u5728\u6570\u636e\u6548\u7387\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6539\u8fdb\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u901a\u8fc7\u5c06\u53d8\u5206\u81ea\u7f16\u7801\u5668\u7528\u4f5c\u65b0\u9896\u6027\u68c0\u6d4b\u5668\uff0c\u63d0\u9ad8\u4e86\u57fa\u4e8e\u6a21\u578b\u7684\u89c4\u5212\u7b97\u6cd5\u5bf9\u6240\u5b66\u4e60\u7684\u4e16\u754c\u6a21\u578b\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u4e14\u5728\u6570\u636e\u6548\u7387\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002"}}
{"id": "2508.05921", "categories": ["cs.LG", "math.FA", "math.RT", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2508.05921", "abs": "https://arxiv.org/abs/2508.05921", "authors": ["Siddharth Rout"], "title": "Fast, Convex and Conditioned Network for Multi-Fidelity Vectors and Stiff Univariate Differential Equations", "comment": null, "summary": "Accuracy in neural PDE solvers often breaks down not because of limited\nexpressivity, but due to poor optimisation caused by ill-conditioning,\nespecially in multi-fidelity and stiff problems. We study this issue in\nPhysics-Informed Extreme Learning Machines (PIELMs), a convex variant of neural\nPDE solvers, and show that asymptotic components in governing equations can\nproduce highly ill-conditioned activation matrices, severely limiting\nconvergence. We introduce Shifted Gaussian Encoding, a simple yet effective\nactivation filtering step that increases matrix rank and expressivity while\npreserving convexity. Our method extends the solvable range of Peclet numbers\nin steady advection-diffusion equations by over two orders of magnitude,\nachieves up to six orders lower error on multi-frequency function learning, and\nfits high-fidelity image vectors more accurately and faster than deep networks\nwith over a million parameters. This work highlights that conditioning, not\ndepth, is often the bottleneck in scientific neural solvers and that simple\narchitectural changes can unlock substantial gains.", "AI": {"tldr": "\u795e\u7ecf\u504f\u5fae\u5206\u65b9\u7a0b\u6c42\u89e3\u5668\u4e2d\u7684\u51c6\u786e\u6027\u901a\u5e38\u4f1a\u56e0\u4f18\u5316\u4e0d\u5f53\u800c\u5d29\u6e83\uff0c\u8fd9\u4e3b\u8981\u662f\u7531\u4e8e\u591a\u4fdd\u771f\u5ea6\u548c\u75c5\u6001\u95ee\u9898\u4e2d\u7684\u75c5\u6001\u95ee\u9898\u3002\u6211\u4eec\u7814\u7a76\u4e86\u7269\u7406\u4fe1\u606f\u6781\u7aef\u5b66\u4e60\u673a (PIELM) \u4e2d\u7684\u6b64\u95ee\u9898\uff0c\u8fd9\u662f\u4e00\u79cd\u795e\u7ecf\u504f\u5fae\u5206\u65b9\u7a0b\u6c42\u89e3\u5668\u7684\u51f8\u53d8\u4f53\uff0c\u5e76\u8868\u660e\u63a7\u5236\u65b9\u7a0b\u4e2d\u7684\u6e10\u8fd1\u5206\u91cf\u4f1a\u4ea7\u751f\u9ad8\u5ea6\u75c5\u6001\u7684\u6fc0\u6d3b\u77e9\u9635\uff0c\u4e25\u91cd\u9650\u5236\u4e86\u6536\u655b\u6027\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u4f46\u6709\u6548\u7684\u6fc0\u6d3b\u51fd\u6570\u8fc7\u6ee4\u6b65\u9aa4\uff0c\u79f0\u4e3a\u79fb\u4f4d\u9ad8\u65af\u7f16\u7801\uff0c\u5b83\u589e\u52a0\u4e86\u77e9\u9635\u79e9\u548c\u8868\u8fbe\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u51f8\u6027\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5c06\u7a33\u6001\u5bf9\u6d41\u6269\u6563\u65b9\u7a0b\u4e2d\u7684 Peclet \u6570\u7684\u53ef\u89e3\u8303\u56f4\u6269\u5c55\u4e86\u4e24\u4e2a\u6570\u91cf\u7ea7\u4ee5\u4e0a\uff0c\u5728\u591a\u9891\u51fd\u6570\u5b66\u4e60\u4e2d\u5b9e\u73b0\u4e86\u4f4e\u81f3\u516d\u4e2a\u6570\u91cf\u7ea7\u7684\u8bef\u5dee\uff0c\u5e76\u4e14\u6bd4\u5177\u6709\u4e00\u767e\u4e07\u591a\u4e2a\u53c2\u6570\u7684\u6df1\u5ea6\u7f51\u7edc\u66f4\u51c6\u786e\u3001\u66f4\u5feb\u5730\u62df\u5408\u9ad8\u4fdd\u771f\u56fe\u50cf\u77e2\u91cf\u3002\u8fd9\u9879\u5de5\u4f5c\u5f3a\u8c03\uff0c\u6761\u4ef6\u800c\u975e\u6df1\u5ea6\u901a\u5e38\u662f\u79d1\u5b66\u795e\u7ecf\u89e3\u7b97\u5668\u4e2d\u7684\u74f6\u9888\uff0c\u800c\u7b80\u5355\u7684\u4f53\u7cfb\u7ed3\u6784\u66f4\u6539\u53ef\u4ee5\u5e26\u6765\u5b9e\u8d28\u6027\u7684\u597d\u5904\u3002", "motivation": "\u795e\u7ecf\u504f\u5fae\u5206\u65b9\u7a0b\u6c42\u89e3\u5668\u4e2d\u7684\u51c6\u786e\u6027\u901a\u5e38\u4f1a\u56e0\u4f18\u5316\u4e0d\u5f53\u800c\u5d29\u6e83\uff0c\u8fd9\u4e3b\u8981\u662f\u7531\u4e8e\u591a\u4fdd\u771f\u5ea6\u548c\u75c5\u6001\u95ee\u9898\u4e2d\u7684\u75c5\u6001\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u4f46\u6709\u6548\u7684\u6fc0\u6d3b\u51fd\u6570\u8fc7\u6ee4\u6b65\u9aa4\uff0c\u79f0\u4e3a\u79fb\u4f4d\u9ad8\u65af\u7f16\u7801\uff0c\u5b83\u589e\u52a0\u4e86\u77e9\u9635\u79e9\u548c\u8868\u8fbe\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u51f8\u6027\u3002", "result": "\u5c06\u7a33\u6001\u5bf9\u6d41\u6269\u6563\u65b9\u7a0b\u4e2d\u7684 Peclet \u6570\u7684\u53ef\u89e3\u8303\u56f4\u6269\u5c55\u4e86\u4e24\u4e2a\u6570\u91cf\u7ea7\u4ee5\u4e0a\uff0c\u5728\u591a\u9891\u51fd\u6570\u5b66\u4e60\u4e2d\u5b9e\u73b0\u4e86\u4f4e\u81f3\u516d\u4e2a\u6570\u91cf\u7ea7\u7684\u8bef\u5dee\uff0c\u5e76\u4e14\u6bd4\u5177\u6709\u4e00\u767e\u4e07\u591a\u4e2a\u53c2\u6570\u7684\u6df1\u5ea6\u7f51\u7edc\u66f4\u51c6\u786e\u3001\u66f4\u5feb\u5730\u62df\u5408\u9ad8\u4fdd\u771f\u56fe\u50cf\u77e2\u91cf\u3002", "conclusion": "\u5e94\u529b\u3001\u6df1\u5ea6\u800c\u975e\u6761\u4ef6\u662f\u79d1\u5b66\u795e\u7ecf\u89e3\u7b97\u5668\u4e2d\u7684\u74f6\u9888\uff0c\u5e76\u4e14\u7b80\u5355\u7684\u4f53\u7cfb\u7ed3\u6784\u66f4\u6539\u53ef\u4ee5\u5e26\u6765\u5b9e\u8d28\u6027\u7684\u597d\u5904\u3002"}}
{"id": "2508.06311", "categories": ["cond-mat.mtrl-sci", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2508.06311", "abs": "https://arxiv.org/abs/2508.06311", "authors": ["Lukas Volkmer", "Leonardo Medrano Sandonas", "Philip Grimm", "Julia Kristin Hufenbach", "Gianaurelio Cuniberti"], "title": "On-the-Fly Machine Learning of Interatomic Potentials for Elastic Property Modeling in Al-Mg-Zr Solid Solutions", "comment": "17 pages, 6 figures, 1 table", "summary": "The development of resilient and lightweight Aluminum alloys is central to\nadvancing structural materials for energy-efficient engineering applications.\nTo address this challenge, in this study, we explore the elastic properties of\nAl-Mg-Zr solid solutions by integrating advanced machine learning (ML)\ntechniques with quantum-mechanical (QM) atomistic simulations. For this\npurpose, we develop accurate and transferable machine-learned interatomic\npotentials (MLIPs) using two complementary approaches: (i) an on-the-fly\nlearning scheme combined with Bayesian linear regression during ab initio\nmolecular dynamics simulations, and (ii) the equivariant neural network\narchitecture MACE. Both MLIPs facilitate the prediction of\ncomposition-dependent elastic properties while drastically reducing the\ncomputational cost compared to conventional QM methods. Comparison with\nultrasonic measurements shows that the deviation between simulation and\nexperiment remains within a few GPa across all Al-Mg-Zr systems investigated.\nThese potentials also enable the systematic exploration of the Al-Mg-Zr solid\nsolution phase space and provide insights into the elastic behavior as a\nfunction of alloying element concentration. Hence, our findings demonstrate the\nreliability and transferability of the parameterized on-the-fly MLIPs, making\nthem valuable for accelerating the design of Al alloys with tailored\nphysicomechanical properties in complex compositional spaces. While the present\nstudy focuses on homogeneous phases, it establishes a foundation for future\nmultiscale simulations that include microstructural features such as\nprecipitates and grain boundaries.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u673a\u5668\u5b66\u4e60\u548c\u91cf\u5b50\u529b\u5b66\u6a21\u62df\u5f00\u53d1\u4e86Al-Mg-Zr\u56fa\u6eb6\u4f53\u7684\u5f39\u6027\u7279\u6027\u9884\u6d4b\u6a21\u578b\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u6a21\u578b\u51c6\u786e\u4e14\u53ef\u8f6c\u79fb\uff0c\u53ef\u52a0\u901f\u94dd\u5408\u91d1\u8bbe\u8ba1\u3002", "motivation": "\u4e3a\u4e86\u5f00\u53d1\u5177\u6709\u5f39\u6027\u548c\u8f7b\u8d28\u7684\u94dd\u5408\u91d1\u4ee5\u6ee1\u8db3\u5177\u6709\u80fd\u6e90\u6548\u7387\u7684\u5de5\u7a0b\u5e94\u7528\u7684\u9700\u6c42\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22Al-Mg-Zr\u56fa\u6eb6\u4f53\u7684\u5f39\u6027\u7279\u6027\u3002", "method": "\u672c\u7814\u7a76\u7ed3\u5408\u4e86\u5148\u8fdb\u7684\u673a\u5668\u5b66\u4e60\uff08ML\uff09\u6280\u672f\u548c\u91cf\u5b50\u529b\u5b66\uff08QM\uff09\u539f\u5b50\u6a21\u62df\u6765\u63a2\u7d22Al-Mg-Zr\u56fa\u6eb6\u4f53\u7684\u5f39\u6027\u7279\u6027\u3002\u7814\u7a76\u4eba\u5458\u5f00\u53d1\u4e86\u4e24\u79cd\u673a\u5668\u5b66\u4e60\u52bf\uff08MLIPs\uff09\uff1a\u4e00\u79cd\u662f\u4f7f\u7528'\u73b0\u5b66\u73b0\u7528'\uff08on-the-fly\uff09\u5b66\u4e60\u65b9\u6848\u7ed3\u5408\u4ece\u5934\u7b97\u5206\u5b50\u52a8\u529b\u5b66\u6a21\u62df\u4e2d\u7684\u8d1d\u53f6\u65af\u7ebf\u6027\u56de\u5f52\uff0c\u53e6\u4e00\u79cd\u662f\u4f7f\u7528\u7b49\u53d8\u795e\u7ecf\u7f51\u7edc\u67b6\u6784MACE\u3002", "result": "\u7814\u7a76\u4eba\u5458\u6210\u529f\u5f00\u53d1\u4e86\u51c6\u786e\u4e14\u53ef\u8f6c\u79fb\u7684\u673a\u5668\u5b66\u4e60\u52bf\uff08MLIPs\uff09\uff0c\u53ef\u4ee5\u9884\u6d4b\u6210\u5206\u4f9d\u8d56\u7684\u5f39\u6027\u7279\u6027\uff0c\u540c\u65f6\u5927\u5927\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002\u4e0e\u8d85\u58f0\u6ce2\u6d4b\u91cf\u7ed3\u679c\u7684\u6bd4\u8f83\u663e\u793a\uff0c\u6a21\u62df\u4e0e\u5b9e\u9a8c\u4e4b\u95f4\u7684\u504f\u5dee\u5728\u6240\u6709\u7814\u7a76\u7684Al-Mg-Zr\u7cfb\u7edf\u4e2d\u5747\u5728\u51e0\u4e2aGPa\u4e4b\u5185\u3002\u8fd9\u4e9b\u52bf\u8fd8\u80fd\u591f\u7cfb\u7edf\u5730\u63a2\u7d22Al-Mg-Zr\u56fa\u6eb6\u4f53\u7684\u76f8\u7a7a\u95f4\uff0c\u5e76\u63d0\u4f9b\u5173\u4e8e\u5f39\u6027\u884c\u4e3a\u968f\u5408\u91d1\u5143\u7d20\u6d53\u5ea6\u53d8\u5316\u7684\u89c1\u89e3\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u5728\u7ebf\u5b66\u4e60\u673a\u5668\u5b66\u4e60\u52bf\uff08MLIPs\uff09\u7684\u53ef\u9760\u6027\u548c\u53ef\u8f6c\u79fb\u6027\uff0c\u8fd9\u5bf9\u4e8e\u5728\u590d\u6742\u6210\u5206\u7a7a\u95f4\u4e2d\u52a0\u901f\u5177\u6709\u5b9a\u5236\u7684\u7269\u7406\u673a\u68b0\u6027\u80fd\u7684\u94dd\u5408\u91d1\u8bbe\u8ba1\u975e\u5e38\u6709\u7528\u3002\u867d\u7136\u672c\u7814\u7a76\u4fa7\u91cd\u4e8e\u5747\u5300\u76f8\uff0c\u4f46\u5b83\u4e3a\u672a\u6765\u5305\u62ec\u6c89\u6dc0\u7269\u548c\u6676\u754c\u7b49\u5fae\u89c2\u7ed3\u6784\u7279\u5f81\u7684\u591a\u5c3a\u5ea6\u6a21\u62df\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.06428", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.06428", "abs": "https://arxiv.org/abs/2508.06428", "authors": ["Zhiwen Zhou"], "title": "Full-Dimensional Beamforming for Multi-User MIMO-OFDM ISAC for Low-Altitude UAV with Zero Sensing Resource Allocation", "comment": null, "summary": "Low-altitude unmanned aerial vehicles (UAVs) are expected to play an\nimportant role for low-altitude economy with a wide range of applications like\nprecise agriculture, aerial delivery and surveillance. Integrated sensing and\ncommunication (ISAC) is a key technology to enable the large-scale deployment\nand routine usage of UAVs by providing both communication and sensing services\nefficiently. For UAV ISAC systems, as UAV often acts as both a communication\nuser equipment (UE) and a sensing target, traditional ISAC systems that usually\nallocate dedicated TF resources for sensing are inefficient due to the severe\ndegradation of communication spectral efficiency. To address this issue, in\nthis paper, we propose a novel multiple-input multiple-output (MIMO) orthogonal\nfrequency division multiplexing (OFDM)-based ISAC framework for UAVs that\neliminates the need for dedicated sensing TF resources, achieving zero TF\nsensing overhead. By designing the transmit beamforming to meet the\nrequirements for both communication and sensing tasks, our proposed approach\nenables the communication TF resources to be fully reused for sensing, thereby\nenhancing both the communication sum rate and the sensing performance in terms\nof resolution, unambiguous range, and accuracy. Additionally, we introduce a\nlow-complexity target searching beamforming algorithm and a two-stage\nsuper-resolution sensing algorithm, which ensure efficient implementation.\nSimulation results demonstrate that the proposed MIMO-OFDM-ISAC framework not\nonly improves the communication sum rate but also outperforms traditional ISAC\nsystems in sensing performance, making it a promising solution for future ISAC\nsystems to support low-altitude UAVs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65e0\u4eba\u673a\u96c6\u6210\u4f20\u611f\u4e0e\u901a\u4fe1\uff08ISAC\uff09\u6846\u67b6\uff0c\u5229\u7528MIMO-OFDM\u6280\u672f\uff0c\u65e0\u9700\u4e13\u7528\u4f20\u611f\u65f6\u9891\u8d44\u6e90\uff0c\u5b9e\u73b0\u901a\u4fe1\u65f6\u9891\u8d44\u6e90\u7684\u5b8c\u5168\u590d\u7528\uff0c\u540c\u65f6\u63d0\u5347\u901a\u4fe1\u901f\u7387\u548c\u4f20\u611f\u6027\u80fd\uff0c\u5e76\u8f85\u4ee5\u4f4e\u590d\u6742\u5ea6\u7b97\u6cd5\uff0c\u4ee5\u652f\u6301\u4f4e\u7a7a\u65e0\u4eba\u673a\u7684\u5e7f\u6cdb\u5e94\u7528\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u4f20\u7edfISAC\u7cfb\u7edf\u4e3a\u65e0\u4eba\u673aISAC\u7cfb\u7edf\u5206\u914d\u4e13\u7528\u4f20\u611f\u65f6\u9891\u8d44\u6e90\u6548\u7387\u4f4e\u4e0b\uff0c\u4ee5\u53ca\u901a\u4fe1\u9891\u8c31\u6548\u7387\u4e25\u91cd\u4e0b\u964d\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8eMIMO-OFDM\u7684UAVISAC\u6846\u67b6\uff0c\u901a\u8fc7\u8bbe\u8ba1\u4f20\u8f93\u6ce2\u675f\u6210\u5f62\u6765\u6ee1\u8db3\u901a\u4fe1\u548c\u4f20\u611f\u4efb\u52a1\u7684\u9700\u6c42\uff0c\u4ece\u800c\u5b9e\u73b0\u901a\u4fe1\u65f6\u9891\u8d44\u6e90\u7684\u5b8c\u5168\u590d\u7528\u3002\u5f15\u5165\u4e86\u4f4e\u590d\u6742\u5ea6\u76ee\u6807\u641c\u7d22\u6ce2\u675f\u6210\u5f62\u7b97\u6cd5\u548c\u4e24\u9636\u6bb5\u8d85\u5206\u8fa8\u7387\u4f20\u611f\u7b97\u6cd5\u3002", "result": "\u6240\u63d0\u51fa\u7684MIMO-OFDM-ISAC\u6846\u67b6\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u901a\u4fe1\u548c\u4f20\u611f\u6027\u80fd\uff0c\u800c\u4e14\u4f18\u4e8e\u4f20\u7edf\u7684ISAC\u7cfb\u7edf\uff0c\u4e3a\u672a\u6765\u652f\u6301\u4f4e\u7a7a\u65e0\u4eba\u673a\u7684ISAC\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684MIMO-OFDM-ISAC\u6846\u67b6\u901a\u8fc7\u6d88\u9664\u5bf9\u4e13\u7528\u4f20\u611f\u65f6\u9891\u8d44\u6e90\u7684\u9700\u6c42\uff0c\u5b9e\u73b0\u4e86\u96f6\u65f6\u9891\u4f20\u611f\u5f00\u9500\uff0c\u5e76\u80fd\u901a\u8fc7\u8bbe\u8ba1\u534f\u540c\u901a\u4fe1\u548c\u4f20\u611f\u4efb\u52a1\u7684\u4f20\u8f93\u6ce2\u675f\u6210\u5f62\uff0c\u5b9e\u73b0\u901a\u4fe1\u65f6\u9891\u8d44\u6e90\u7684\u5b8c\u5168\u590d\u7528\uff0c\u4ece\u800c\u63d0\u9ad8\u901a\u4fe1\u548c\u4f20\u611f\u6027\u80fd\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u4f4e\u590d\u6742\u5ea6\u76ee\u6807\u641c\u7d22\u6ce2\u675f\u6210\u5f62\u7b97\u6cd5\u548c\u4e24\u9636\u6bb5\u8d85\u5206\u8fa8\u7387\u4f20\u611f\u7b97\u6cd5\u4ee5\u786e\u4fdd\u9ad8\u6548\u5b9e\u73b0\u3002"}}
{"id": "2508.05754", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.05754", "abs": "https://arxiv.org/abs/2508.05754", "authors": ["Stefan K. Seritan", "Aditya Dhumuntarao", "Aidan Q. Wilber-Gauthier", "Kenneth M. Rudinger", "Antonio E. Russo", "Robin Blume-Kohout", "Andrew D. Baczewski", "Timothy Proctor"], "title": "Benchmarking quantum computers with any quantum algorithm", "comment": null, "summary": "Application-based benchmarks are increasingly used to quantify and compare\nquantum computers' performance. However, because contemporary quantum computers\ncannot run utility-scale computations, these benchmarks currently test this\nhardware's performance on ``small'' problem instances that are not necessarily\nrepresentative of utility-scale problems. Furthermore, these benchmarks often\nemploy methods that are unscalable, limiting their ability to track progress\ntowards utility-scale applications. In this work, we present a method for\ncreating scalable and efficient benchmarks from any quantum algorithm or\napplication. Our subcircuit volumetric benchmarking (SVB) method runs\nsubcircuits of varied shape that are ``snipped out'' from some target circuit,\nwhich could implement a utility-scale algorithm. SVB is scalable and it enables\nestimating a capability coefficient that concisely summarizes progress towards\nimplementing the target circuit. We demonstrate SVB with experiments on IBM Q\nsystems using a Hamiltonian block-encoding subroutine from quantum chemistry\nalgorithms.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u5b50\u7535\u8def\u4f53\u79ef\u57fa\u51c6\u6d4b\u8bd5\uff08SVB\uff09\u7684\u53ef\u6269\u5c55\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u8ddf\u8e2a\u91cf\u5b50\u8ba1\u7b97\u673a\u5728\u5b9e\u73b0\u5927\u89c4\u6a21\u5e94\u7528\u65b9\u9762\u7684\u8fdb\u5c55\u3002", "motivation": "\u5f53\u524d\u7684\u57fa\u4e8e\u5e94\u7528\u7a0b\u5e8f\u7684\u57fa\u51c6\u6d4b\u8bd5\u5728\u6d4b\u8bd5\u5c0f\u89c4\u6a21\u95ee\u9898\u5b9e\u4f8b\u65f6\uff0c\u53ef\u80fd\u65e0\u6cd5\u4ee3\u8868\u6548\u7528\u89c4\u6a21\u95ee\u9898\uff0c\u5e76\u4e14\u5e38\u5e38\u4f7f\u7528\u4e0d\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u8ddf\u8e2a\u5411\u6548\u7528\u89c4\u6a21\u5e94\u7528\u7a0b\u5e8f\u8fdb\u5c55\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u5b50\u7535\u8def\u4f53\u79ef\u57fa\u51c6\u6d4b\u8bd5\uff08SVB\uff09\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u8fd0\u884c\u4ece\u76ee\u6807\u7535\u8def\uff08\u53ef\u5b9e\u73b0\u6548\u7528\u89c4\u6a21\u7b97\u6cd5\uff09\u4e2d\u201c\u526a\u5207\u201d\u51fa\u6765\u7684\u5404\u79cd\u5f62\u72b6\u7684\u5b50\u7535\u8def\u6765\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u901a\u8fc7\u5728IBM Q\u7cfb\u7edf\u4e0a\u4f7f\u7528\u91cf\u5b50\u5316\u5b66\u7b97\u6cd5\u7684\u54c8\u5bc6\u987f\u91cf\u5757\u7f16\u7801\u5b50\u7a0b\u5e8f\u8fdb\u884c\u6f14\u793a\uff0c\u8bc1\u660e\u4e86SVB\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u5b50\u7535\u8def\u4f53\u79ef\u57fa\u51c6\u6d4b\u8bd5\uff08SVB\uff09\u65b9\u6cd5\u53ef\u4ee5\u4ece\u4efb\u4f55\u91cf\u5b50\u7b97\u6cd5\u6216\u5e94\u7528\u7a0b\u5e8f\u521b\u5efa\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u80fd\u4f30\u8ba1\u4e00\u4e2a\u80fd\u529b\u7cfb\u6570\u4ee5\u7b80\u6d01\u5730\u603b\u7ed3\u5b9e\u73b0\u76ee\u6807\u7535\u8def\u7684\u8fdb\u5c55\u3002"}}
{"id": "2508.06016", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06016", "abs": "https://arxiv.org/abs/2508.06016", "authors": ["Sagar Gandhi", "Vishal Gandhi"], "title": "Crisp Attention: Regularizing Transformers via Structured Sparsity", "comment": null, "summary": "The quadratic computational cost of the self-attention mechanism is a primary\nchallenge in scaling Transformer models. While attention sparsity is widely\nstudied as a technique to improve computational efficiency, it is almost\nuniversally assumed to come at the cost of model accuracy. In this paper, we\nreport a surprising counter-example to this common wisdom. By introducing\nstructured, post-hoc sparsity to the attention mechanism of a DistilBERT model\nduring fine-tuning on the SST-2 sentiment analysis task, we find that model\naccuracy improves significantly. Our model with 80\\% attention sparsity\nachieves a validation accuracy of 91.59\\%, a 0.97\\% absolute improvement over\nthe dense baseline. We hypothesize that this phenomenon is due to sparsity\nacting as a powerful implicit regularizer, preventing the model from\noverfitting by forcing it to make predictions with a more constrained and\nrobust set of features. Our work recasts attention sparsity not just as a tool\nfor computational efficiency, but as a potential method for improving the\ngeneralization and performance of Transformer models.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u6ce8\u610f\u529b\u7a00\u758f\u6027\u5bf9Transformer\u6a21\u578b\u7cbe\u5ea6\u7684\u5f71\u54cd\u3002\u5b9e\u9a8c\u53d1\u73b0\uff0c\u901a\u8fc7\u5f15\u5165\u7ed3\u6784\u5316\u7a00\u758f\u6027\uff0c\u6a21\u578b\u7cbe\u5ea6\u53cd\u800c\u5f97\u5230\u4e86\u63d0\u5347\uff0c\u8bc1\u660e\u4e86\u7a00\u758f\u6027\u53ef\u4ee5\u4f5c\u4e3a\u4e00\u79cd\u6709\u6548\u7684\u6b63\u5219\u5316\u624b\u6bb5\uff0c\u63d0\u9ad8\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "Transformer\u6a21\u578b\u5728\u6269\u5c55\u65f6\u9762\u4e34\u7684\u4e3b\u8981\u6311\u6218\u662f\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684\u4e8c\u6b21\u8ba1\u7b97\u6210\u672c\u3002\u867d\u7136\u6ce8\u610f\u529b\u7a00\u758f\u6027\u88ab\u5e7f\u6cdb\u7814\u7a76\u4f5c\u4e3a\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u7684\u6280\u672f\uff0c\u4f46\u901a\u5e38\u8ba4\u4e3a\u4f1a\u4ee5\u727a\u7272\u6a21\u578b\u7cbe\u5ea6\u4e3a\u4ee3\u4ef7\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u6ce8\u610f\u529b\u7a00\u758f\u6027\u662f\u5426\u53ef\u4ee5\u4e0d\u4ee5\u727a\u7272\u7cbe\u5ea6\u4e3a\u4ee3\u4ef7\uff0c\u751a\u81f3\u63d0\u9ad8\u6a21\u578b\u7cbe\u5ea6\u3002", "method": "\u901a\u8fc7\u5728\u5fae\u8c03DistilBERT\u6a21\u578b\u65f6\uff0c\u5411\u5176\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5f15\u5165\u7ed3\u6784\u5316\u7684\u3001\u4e8b\u540e\u7684\u7a00\u758f\u6027\uff0c\u5e76\u5728SST-2\u60c5\u611f\u5206\u6790\u4efb\u52a1\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u5728SST-2\u60c5\u611f\u5206\u6790\u4efb\u52a1\u4e0a\uff0c80%\u6ce8\u610f\u529b\u7a00\u758f\u6027\u7684\u6a21\u578b\u8fbe\u5230\u4e8691.59%\u7684\u9a8c\u8bc1\u51c6\u786e\u7387\uff0c\u6bd4\u65e0\u7a00\u758f\u6027\u57fa\u7ebf\u6a21\u578b\u7edd\u5bf9\u63d0\u9ad8\u4e860.97%\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5728Transformer\u6a21\u578b\u4e2d\u5f15\u5165\u7a00\u758f\u6027\u4e0d\u4ec5\u53ef\u4ee5\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\uff0c\u8fd8\u6709\u53ef\u80fd\u63d0\u9ad8\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6027\u80fd\u3002"}}
{"id": "2508.05852", "categories": ["cs.CV", "I.5.4"], "pdf": "https://arxiv.org/pdf/2508.05852", "abs": "https://arxiv.org/abs/2508.05852", "authors": ["Kaiser Hamid", "Khandakar Ashrafi Akbar", "Nade Liang"], "title": "VISTA: Vision-Language Imitation of Situational Thinking and Attention for Human-Like Driver Focus in Dynamic Environments", "comment": null, "summary": "Driver visual attention prediction is a critical task in autonomous driving\nand human-computer interaction (HCI) research. Most prior studies focus on\nestimating attention allocation at a single moment in time, typically using\nstatic RGB images such as driving scene pictures. In this work, we propose a\nvision-language framework that models the changing landscape of drivers' gaze\nthrough natural language, using few-shot and zero-shot learning on single RGB\nimages. We curate and refine high-quality captions from the BDD-A dataset using\nhuman-in-the-loop feedback, then fine-tune LLaVA to align visual perception\nwith attention-centric scene understanding. Our approach integrates both\nlow-level cues and top-down context (e.g., route semantics, risk anticipation),\nenabling language-based descriptions of gaze behavior. We evaluate performance\nacross training regimes (few shot, and one-shot) and introduce domain-specific\nmetrics for semantic alignment and response diversity. Results show that our\nfine-tuned model outperforms general-purpose VLMs in attention shift detection\nand interpretability. To our knowledge, this is among the first attempts to\ngenerate driver visual attention allocation and shifting predictions in natural\nlanguage, offering a new direction for explainable AI in autonomous driving.\nOur approach provides a foundation for downstream tasks such as behavior\nforecasting, human-AI teaming, and multi-agent coordination.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89c6\u89c9-\u8bed\u8a00\u6846\u67b6\uff0c\u5229\u7528\u81ea\u7136\u8bed\u8a00\u548c\u5c11\u6837\u672c\u5b66\u4e60\u6765\u9884\u6d4b\u9a7e\u9a76\u5458\u7684\u6ce8\u610f\u529b\u8f6c\u79fb\uff0c\u5e76\u5728 BDD-A \u6570\u636e\u96c6\u4e0a\u901a\u8fc7\u5fae\u8c03 LLaVA \u6765\u5b9e\u73b0\uff0c\u7ed3\u679c\u663e\u793a\u5176\u5728\u6ce8\u610f\u8f6c\u79fb\u68c0\u6d4b\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u4f18\u4e8e\u901a\u7528\u6a21\u578b\u3002", "motivation": "\u5927\u591a\u6570\u73b0\u6709\u7814\u7a76\u7740\u91cd\u4e8e\u5728\u5355\u4e00\u65f6\u95f4\u70b9\u4f30\u8ba1\u6ce8\u610f\u529b\u5206\u914d\uff0c\u901a\u5e38\u4f7f\u7528\u9759\u6001 RGB \u56fe\u50cf\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u89c6\u89c9-\u8bed\u8a00\u6846\u67b6\uff0c\u5229\u7528\u5c11\u6837\u672c\u548c\u96f6\u6837\u672c\u5b66\u4e60\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u6a21\u62df\u9a7e\u9a76\u5458\u6ce8\u89c6\u7684\u53d8\u5316\uff0c\u5e76\u5bf9 BDD-A \u6570\u636e\u96c6\u7684\u6807\u9898\u8fdb\u884c\u4f18\u5316\uff0c\u7136\u540e\u5bf9 LLaVA \u8fdb\u884c\u5fae\u8c03\uff0c\u4f7f\u89c6\u89c9\u611f\u77e5\u4e0e\u4ee5\u6ce8\u610f\u529b\u4e3a\u4e2d\u5fc3\u7684\u573a\u666f\u7406\u89e3\u4fdd\u6301\u4e00\u81f4\u3002\u8be5\u65b9\u6cd5\u96c6\u6210\u4e86\u4f4e\u7ea7\u7ebf\u7d22\u548c\u9ad8\u7ea7\u4e0a\u4e0b\u6587\uff08\u5982\u8def\u7ebf\u8bed\u4e49\u3001\u98ce\u9669\u9884\u671f\uff09\u3002", "result": "\u5728\u6ce8\u610f\u529b\u8f6c\u79fb\u68c0\u6d4b\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\uff0c\u5fae\u8c03\u540e\u7684\u6a21\u578b\u4f18\u4e8e\u901a\u7528\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9996\u6b21\u5c1d\u8bd5\u4ee5\u81ea\u7136\u8bed\u8a00\u751f\u6210\u9a7e\u9a76\u5458\u7684\u89c6\u89c9\u6ce8\u610f\u529b\u5206\u914d\u548c\u8f6c\u79fb\u9884\u6d4b\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\uff0c\u5e76\u4e3a\u4e0b\u6e38\u4efb\u52a1\uff08\u5982\u884c\u4e3a\u9884\u6d4b\u3001\u4eba\u673a\u534f\u4f5c\u548c\u591a\u667a\u80fd\u4f53\u534f\u8c03\uff09\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.06181", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06181", "abs": "https://arxiv.org/abs/2508.06181", "authors": ["Jan W\u0119grzynowski", "Piotr Kicki", "Grzegorz Czechmanowski", "Maciej Krupka", "Krzysztof Walas"], "title": "Beyond Constant Parameters: Hyper Prediction Models and HyperMPC", "comment": null, "summary": "Model Predictive Control (MPC) is among the most widely adopted and reliable\nmethods for robot control, relying critically on an accurate dynamics model.\nHowever, existing dynamics models used in the gradient-based MPC are limited by\ncomputational complexity and state representation. To address this limitation,\nwe propose the Hyper Prediction Model (HyperPM) - a novel approach in which we\nproject the unmodeled dynamics onto a time-dependent dynamics model. This\ntime-dependency is captured through time-varying model parameters, whose\nevolution over the MPC prediction horizon is learned using a neural network.\nSuch formulation preserves the computational efficiency and robustness of the\nbase model while equipping it with the capacity to anticipate previously\nunmodeled phenomena. We evaluated the proposed approach on several challenging\nsystems, including real-world F1TENTH autonomous racing, and demonstrated that\nit significantly reduces long-horizon prediction errors. Moreover, when\nintegrated within the MPC framework (HyperMPC), our method consistently\noutperforms existing state-of-the-art techniques.", "AI": {"tldr": "\u901a\u8fc7 Hyper Prediction Model (HyperPM) \u6539\u8fdb\u4e86\u673a\u5668\u4eba\u63a7\u5236\u4e2d\u7684\u6a21\u578b\u9884\u6d4b\u63a7\u5236 (MPC)\uff0c\u63d0\u9ad8\u4e86\u9884\u6d4b\u7cbe\u5ea6\u548c\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u81ea\u52a8\u9a7e\u9a76\u8d5b\u8f66\u7b49\u590d\u6742\u573a\u666f\u4e0b\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u57fa\u4e8e\u68af\u5ea6 MPC \u4e2d\u52a8\u529b\u5b66\u6a21\u578b\u5b58\u5728\u7684\u8ba1\u7b97\u590d\u6742\u6027\u548c\u72b6\u6001\u8868\u793a\u7684\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a Hyper Prediction Model (HyperPM) \u7684\u65b0\u65b9\u6cd5\uff0c\u5c06\u672a\u5efa\u6a21\u7684\u52a8\u529b\u5b66\u6620\u5c04\u5230\u4e00\u4e2a\u65f6\u53d8\u52a8\u529b\u5b66\u6a21\u578b\u3002\u901a\u8fc7\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60 MPC \u9884\u6d4b\u8303\u56f4\u5185\u7684\u65f6\u53d8\u6a21\u578b\u53c2\u6570\u6765\u6355\u6349\u8fd9\u79cd\u65f6\u53d8\u6027\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u4fdd\u7559\u4e86\u57fa\u7840\u6a21\u578b\u7684\u8ba1\u7b97\u6548\u7387\u548c\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u80fd\u591f\u9884\u6d4b\u4ee5\u524d\u672a\u5efa\u6a21\u7684\u73b0\u8c61\uff0c\u5e76\u5728\u591a\u4e2a\u6311\u6218\u6027\u7cfb\u7edf\uff08\u5305\u62ec\u771f\u5b9e\u7684 F1TENTH \u81ea\u52a8\u9a7e\u9a76\u8d5b\u8f66\uff09\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u8bc1\u660e\u5176\u663e\u8457\u51cf\u5c11\u4e86\u957f\u65f6\u9884\u6d4b\u8bef\u5dee\u3002", "conclusion": "\u4f7f\u7528 Hyper Prediction Model (HyperPM) \u7ed3\u5408\u6a21\u578b\u9884\u6d4b\u63a7\u5236 (MPC) \u6846\u67b6\uff08\u79f0\u4e3a HyperMPC\uff09\u80fd\u591f\u663e\u8457\u51cf\u5c11\u957f\u65f6\u9884\u6d4b\u8bef\u5dee\uff0c\u5e76\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u6280\u672f\uff0c\u5c24\u5176\u662f\u5728\u771f\u5b9e\u7684 F1TENTH \u81ea\u52a8\u9a7e\u9a76\u8d5b\u8f66\u7b49\u6311\u6218\u6027\u7cfb\u7edf\u4e0a\u3002"}}
{"id": "2508.06064", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06064", "abs": "https://arxiv.org/abs/2508.06064", "authors": ["Harold Silv\u00e8re Kiossou", "Siegfried Nijssen", "Pierre Schaus"], "title": "A Generic Complete Anytime Beam Search for Optimal Decision Tree", "comment": null, "summary": "Finding an optimal decision tree that minimizes classification error is known\nto be NP-hard. While exact algorithms based on MILP, CP, SAT, or dynamic\nprogramming guarantee optimality, they often suffer from poor anytime behavior\n-- meaning they struggle to find high-quality decision trees quickly when the\nsearch is stopped before completion -- due to unbalanced search space\nexploration. To address this, several anytime extensions of exact methods have\nbeen proposed, such as LDS-DL8.5, Top-k-DL8.5, and Blossom, but they have not\nbeen systematically compared, making it difficult to assess their relative\neffectiveness. In this paper, we propose CA-DL8.5, a generic, complete, and\nanytime beam search algorithm that extends the DL8.5 framework and unifies some\nexisting anytime strategies. In particular, CA-DL8.5 generalizes previous\napproaches LDS-DL8.5 and Top-k-DL8.5, by allowing the integration of various\nheuristics and relaxation mechanisms through a modular design. The algorithm\nreuses DL8.5's efficient branch-and-bound pruning and trie-based caching,\ncombined with a restart-based beam search that gradually relaxes pruning\ncriteria to improve solution quality over time. Our contributions are twofold:\n(1) We introduce this new generic framework for exact and anytime decision tree\nlearning, enabling the incorporation of diverse heuristics and search\nstrategies; (2) We conduct a rigorous empirical comparison of several\ninstantiations of CA-DL8.5 -- based on Purity, Gain, Discrepancy, and Top-k\nheuristics -- using an anytime evaluation metric called the primal gap\nintegral. Experimental results on standard classification benchmarks show that\nCA-DL8.5 using LDS (limited discrepancy) consistently provides the best anytime\nperformance, outperforming both other CA-DL8.5 variants and the Blossom\nalgorithm while maintaining completeness and optimality guarantees.", "AI": {"tldr": "\u5728\u5bfb\u627e\u6700\u4f18\u51b3\u7b56\u6811\u7684NP\u96be\u9898\u4e2d\uff0cCA-DL8.5\u901a\u8fc7\u5176\u521b\u65b0\u7684\u526a\u679d\u641c\u7d22\u6846\u67b6\u548c\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4efb\u610f\u65f6\u6bb5\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u4f7f\u7528LDS\u542f\u53d1\u5f0f\u65b9\u6cd5\u65f6\u8868\u73b0\u7a81\u51fa\uff0c\u4e3a\u51b3\u7b56\u6811\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u73b0\u6709\u7684\u7cbe\u786e\u7b97\u6cd5\u5728\u5bfb\u627e\u6700\u4f18\u51b3\u7b56\u6811\u65f6\u5b58\u5728\u4efb\u610f\u884c\u4e3a\u4e0d\u4f73\u7684\u95ee\u9898\uff0c\u5373\u96be\u4ee5\u5728\u641c\u7d22\u505c\u6b62\u524d\u5feb\u901f\u627e\u5230\u9ad8\u8d28\u91cf\u7684\u51b3\u7b56\u6811\u3002\u867d\u7136\u5df2\u7ecf\u63d0\u51fa\u4e86\u4e00\u4e9b\u7cbe\u786e\u65b9\u6cd5\u7684\u4efb\u610f\u6269\u5c55\uff0c\u4f46\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7684\u6bd4\u8f83\u3002", "method": "CA-DL8.5\u662f\u4e00\u79cd\u901a\u7528\u7684\u3001\u5b8c\u6574\u7684\u3001\u4efb\u4f55\u65f6\u6bb5\u7684\u526a\u679d\u641c\u7d22\u7b97\u6cd5\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u5141\u8bb8\u96c6\u6210\u5404\u79cd\u542f\u53d1\u5f0f\u65b9\u6cd5\u548c\u677e\u5f1b\u673a\u5236\uff0c\u91cd\u7528DL8.5\u7684\u6709\u6548\u5206\u652f\u526a\u679d\u548c\u57fa\u4e8etrie\u7684\u7f13\u5b58\uff0c\u7ed3\u5408\u57fa\u4e8e\u91cd\u542f\u7684\u526a\u679d\u641c\u7d22\uff0c\u901a\u8fc7\u9010\u6b65\u653e\u5bbd\u526a\u679d\u6807\u51c6\u6765\u968f\u7740\u65f6\u95f4\u7684\u63a8\u79fb\u63d0\u9ad8\u89e3\u51b3\u65b9\u6848\u7684\u8d28\u91cf\u3002", "result": "CA-DL8.5\u4f7f\u7528LDS\uff08\u6709\u9650\u5dee\u5f02\uff09\u542f\u53d1\u5f0f\u65b9\u6cd5\u5728\u4efb\u4f55\u65f6\u6bb5\u6027\u80fd\u65b9\u9762\u59cb\u7ec8\u8868\u73b0\u6700\u4f73\uff0c\u4f18\u4e8e\u5176\u4ed6CA-DL8.5\u53d8\u4f53\u548cBlossom\u7b97\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u5b8c\u6574\u6027\u548c\u6700\u4f18\u6027\u4fdd\u8bc1\u3002", "conclusion": "CA-DL8.5\u662f\u4e00\u4e2a\u901a\u7528\u7684\u3001\u5b8c\u6574\u7684\u3001\u4efb\u4f55\u65f6\u6bb5\u7684\u526a\u679d\u641c\u7d22\u7b97\u6cd5\uff0c\u5b83\u6269\u5c55\u4e86DL8.5\u6846\u67b6\u5e76\u7edf\u4e00\u4e86\u4e00\u4e9b\u73b0\u6709\u7684\u4efb\u4f55\u65f6\u6bb5\u7b56\u7565\u3002CA-DL8.5\u901a\u8fc7\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u5141\u8bb8\u96c6\u6210\u5404\u79cd\u542f\u53d1\u5f0f\u65b9\u6cd5\u548c\u677e\u5f1b\u673a\u5236\uff0c\u5e76\u4e14\u53ef\u4ee5\u91cd\u7528DL8.5\u7684\u6709\u6548\u5206\u652f\u526a\u679d\u548c\u57fa\u4e8etrie\u7684\u7f13\u5b58\uff0c\u7ed3\u5408\u57fa\u4e8e\u91cd\u542f\u7684\u526a\u679d\u641c\u7d22\uff0c\u901a\u8fc7\u9010\u6b65\u653e\u5bbd\u526a\u679d\u6807\u51c6\u6765\u968f\u7740\u65f6\u95f4\u7684\u63a8\u79fb\u63d0\u9ad8\u89e3\u51b3\u65b9\u6848\u7684\u8d28\u91cf\u3002\u5728\u6807\u51c6\u5206\u7c7b\u57fa\u51c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4f7f\u7528LDS\uff08\u6709\u9650\u5dee\u5f02\uff09\u7684CA-DL8.5\u5728\u4efb\u4f55\u65f6\u6bb5\u6027\u80fd\u65b9\u9762\u59cb\u7ec8\u8868\u73b0\u6700\u4f73\uff0c\u5e76\u4e14\u4f18\u4e8e\u5176\u4ed6CA-DL8.5\u53d8\u4f53\u548cBlossom\u7b97\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u5b8c\u6574\u6027\u548c\u6700\u4f18\u6027\u4fdd\u8bc1\u3002"}}
{"id": "2508.05928", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.05928", "abs": "https://arxiv.org/abs/2508.05928", "authors": ["Si Shen", "Peijun Shen", "Wenhua Zhao", "Danhao Zhu"], "title": "Mitigating Think-Answer Mismatch in LLM Reasoning Through Noise-Aware Advantage Reweighting", "comment": null, "summary": "Group-Relative Policy Optimization (GRPO) is a key technique for training\nlarge reasoning models, yet it suffers from a critical vulnerability: the\n\\emph{Think-Answer Mismatch}, where noisy reward signals corrupt the learning\nprocess. This problem is most severe in unbalanced response groups,\nparadoxically degrading the signal precisely when it should be most\ninformative. To address this challenge, we propose Stable Group-Relative Policy\nOptimization (S-GRPO), a principled enhancement that derives optimal,\nnoise-aware advantage weights to stabilize training. Our comprehensive\nexperiments on mathematical reasoning benchmarks demonstrate S-GRPO's\neffectiveness and robustness. On various models, S-GRPO significantly\noutperforms DR. GRPO, achieving performance gains of +2.5% on\nQwen-Math-7B-Base, +2.2% on Llama-3.2-3B-Base, and +2.4% on\nQwen-Math-1.5B-Instruct. Most critically, while standard GRPO fails to learn\nunder 20% synthetic reward noise, S-GRPO maintains stable learning progress.\nThese results highlight S-GRPO's potential for more robust and effective\ntraining of large-scale reasoning models. \\footnote{Code and data are available\nat: https://github.com/shenpeijun0212/S-GRPO", "AI": {"tldr": "S-GRPO\u901a\u8fc7\u4f18\u5316\u566a\u58f0\u611f\u77e5\u4f18\u52bf\u6743\u91cd\uff0c\u89e3\u51b3\u4e86GRPO\u5728\u5927\u578b\u63a8\u7406\u6a21\u578b\u8bad\u7ec3\u4e2d\u7684\u201c\u601d\u8003-\u56de\u7b54\u4e0d\u5339\u914d\u201d\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u8bad\u7ec3\u7684\u7a33\u5b9a\u6027\u548c\u6027\u80fd\uff0c\u5c24\u5176\u5728\u5b58\u5728\u566a\u58f0\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u66f4\u4f73\u3002", "motivation": "\u65e8\u5728\u89e3\u51b3\u5173\u952e\u6280\u672fGRPO\u5728\u8bad\u7ec3\u5927\u578b\u63a8\u7406\u6a21\u578b\u65f6\u5b58\u5728\u7684\u201c\u601d\u8003-\u56de\u7b54\u4e0d\u5339\u914d\u201d\u95ee\u9898\uff0c\u8be5\u95ee\u9898\u4f1a\u5bfc\u81f4\u566a\u58f0\u5956\u52b1\u4fe1\u53f7\u7834\u574f\u5b66\u4e60\u8fc7\u7a0b\uff0c\u5c24\u5176\u5728\u54cd\u5e94\u7ec4\u4e0d\u5e73\u8861\u65f6\u66f4\u4e3a\u4e25\u91cd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u7a33\u5b9a\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08S-GRPO\uff09\u7684\u6539\u8fdb\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u63a8\u5bfc\u6700\u4f18\u7684\u3001\u8003\u8651\u566a\u58f0\u7684\u4f18\u52bf\u6743\u91cd\u6765\u7a33\u5b9a\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u4ee5\u89e3\u51b3\u201c\u601d\u8003-\u56de\u7b54\u4e0d\u5339\u914d\u201d\u95ee\u9898\u3002", "result": "S-GRPO\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8eDR. GRPO\uff0c\u5728\u4e0d\u540c\u6a21\u578b\u4e0a\u5747\u5b9e\u73b0\u4e86\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u4e14\u5728\u5b58\u5728\u9ad8\u8fbe20%\u7684\u5408\u6210\u5956\u52b1\u566a\u58f0\u65f6\u4ecd\u80fd\u4fdd\u6301\u7a33\u5b9a\u7684\u5b66\u4e60\u3002\u4ee3\u7801\u548c\u6570\u636e\u53ef\u5728https://github.com/shenpeijun0212/S-GRPO\u83b7\u53d6\u3002", "conclusion": "S-GRPO\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5728Qwen-Math-7B-Base\u3001Llama-3.2-3B-Base\u548cQwen-Math-1.5B-Instruct\u6a21\u578b\u4e0a\u5206\u522b\u5b9e\u73b0\u4e86+2.5%\u3001+2.2%\u548c+2.4%\u7684\u6027\u80fd\u63d0\u5347\u3002\u572820%\u7684\u5408\u6210\u5956\u52b1\u566a\u58f0\u4e0b\uff0c\u6807\u51c6GRPO\u5931\u6548\uff0c\u800cS-GRPO\u4ecd\u80fd\u4fdd\u6301\u7a33\u5b9a\u7684\u5b66\u4e60\u8fdb\u5c55\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u66f4\u5927\u89c4\u6a21\u63a8\u7406\u6a21\u578b\u8bad\u7ec3\u4e2d\u66f4\u4f18\u8d8a\u548c\u6709\u6548\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.06399", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2508.06399", "abs": "https://arxiv.org/abs/2508.06399", "authors": ["Mohamad Baker Shoker", "Sitaram Ramakrishnan", "Boris Croes", "Olivier Cregut", "Nicolas Beyer", "Kokou Dorkenoo", "Pierre Rodi\u00e8re", "Bj\u00f6rn Wehinger", "Gaston Garbarino", "Mohamed Mezouar", "Marine Verseils", "Pierre Fertey", "Salia Cherifi-Hertel", "Pierre Bouvier", "Mael Guennou"], "title": "An underdog story: Re-emergence of a polar instability at high pressure in KNbO3", "comment": "7 pages, 4 figures", "summary": "Ferroelectricity in perovskites is known to be suppressed by a moderate\nhydrostatic pressure. The notion that a polar instability should reappear in a\nhigher pressure regime is well accepted theoretically but experiments have\nfailed so far to provide a conclusive evidence for it. Here, we investigate a\nclassical but comparatively underlooked ferroelectric perovskite KNbO3. We use\nsingle crystal X-ray diffraction, infrared and Raman spectroscopy and\nsecond-harmonic generation to explore the phase transition sequence at high\npressures up to 63 GPa. We show that the ferroelectric instability manifests\nitself in the emergence of an incommensurate modulation of the perovskite\nstructure that combines cation displacements and tilts of the oxygen octahedra.\nSoft modes associated to the tilts and the modulation are clearly observed\nalong with persistent order-disorder signatures. This demonstrates the presence\nof the high-pressure polar instability in a lead-free perovskite in spite of\nthe centrosymmetric character of all observed high-pressure phases.", "AI": {"tldr": "\u5728\u9ad8\u538b\u4e0b\uff0cK NbO3 \u91cd\u65b0\u8868\u73b0\u51fa\u94c1\u7535\u6027\uff0c\u8868\u73b0\u4e3a\u5305\u542b\u9633\u79bb\u5b50\u4f4d\u79fb\u548c\u6c27\u516b\u9762\u4f53\u503e\u659c\u7684\u4e0d\u76f8\u79f0\u8c03\u5236\uff0c\u5e76\u4f34\u6709\u8f6f\u6a21\u548c\u65e0\u5e8f-\u6709\u5e8f\u7279\u5f81\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5728\u9ad8\u538b\u6761\u4ef6\u4e0b\uff0c\u5148\u524d\u88ab\u6291\u5236\u7684\u94c1\u7535\u6027\u662f\u5426\u4f1a\u5728\u66f4\u9ad8\u538b\u529b\u533a\u57df\u91cd\u73b0\uff0c\u4ee5\u9a8c\u8bc1\u7406\u8bba\u9884\u6d4b\uff0c\u5e76\u4e3a\u94c1\u7535\u6750\u6599\u5728\u9ad8\u538b\u4e0b\u7684\u884c\u4e3a\u63d0\u4f9b\u5b9e\u9a8c\u8bc1\u636e\u3002", "method": "\u672c\u7814\u7a76\u4f7f\u7528\u5355\u6676X\u5c04\u7ebf\u884d\u5c04\u3001\u7ea2\u5916\u5149\u8c31\u3001\u62c9\u66fc\u5149\u8c31\u548c\u4e8c\u6b21\u8c10\u6ce2\u751f\u6210\u6280\u672f\uff0c\u5728\u9ad8\u538b\uff08\u9ad8\u8fbe63 GPa\uff09\u4e0b\u7814\u7a76\u4e86\u9499\u949b\u77ff\u94c1\u7535\u6750\u6599K NbO3 \u7684\u76f8\u53d8\u5e8f\u5217\u3002", "result": "\u5728\u9ad8\u538b\uff08\u9ad8\u8fbe63 GPa\uff09\u4e0b\uff0cK NbO3 \u7684\u94c1\u7535\u6027\u8868\u73b0\u4e3a\u6676\u4f53\u7ed3\u6784\u7684\u4e0d\u76f8\u79f0\u8c03\u5236\uff0c\u7ed3\u5408\u4e86\u9633\u79bb\u5b50\u4f4d\u79fb\u548c\u6c27\u516b\u9762\u4f53\u503e\u659c\u3002\u89c2\u5bdf\u5230\u4e86\u4e0e\u503e\u659c\u548c\u8c03\u5236\u76f8\u5173\u7684\u8f6f\u6a21\uff0c\u4ee5\u53ca\u6301\u7eed\u5b58\u5728\u7684\u65e0\u5e8f-\u6709\u5e8f\u7279\u5f81\uff0c\u8bc1\u660e\u4e86\u65e0\u94c5\u9499\u949b\u77ff\u5728\u9ad8\u538b\u4e0b\u5b58\u5728\u6781\u6027\u4e0d\u7a33\u5b9a\u6027\uff0c\u5c3d\u7ba1\u5176\u9ad8\u538b\u76f8\u5747\u5177\u6709\u4e2d\u5fc3\u5bf9\u79f0\u6027\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5728\u9ad8\u538b\u4e0b\uff0c\u80af\u5c3c\u4e9a\u94cc\u9178\u94be\uff08KNbO3\uff09\u7684\u94c1\u7535\u6027\u8868\u73b0\u4e3a\u4e00\u79cd\u5305\u542b\u9633\u79bb\u5b50\u4f4d\u79fb\u548c\u6c27\u516b\u9762\u4f53\u503e\u659c\u7684\u6676\u4f53\u7ed3\u6784\u7684\u4e0d\u76f8\u79f0\u8c03\u5236\u3002\u901a\u8fc7X\u5c04\u7ebf\u884d\u5c04\u3001\u5149\u8c31\u5b66\u548c\u4e8c\u6b21\u8c10\u6ce2\u751f\u6210\u7b49\u5b9e\u9a8c\u624b\u6bb5\uff0c\u5728\u9ad8\u8fbe63 GPa\u7684\u538b\u529b\u4e0b\uff0c\u89c2\u5bdf\u5230\u4e86\u4e0e\u503e\u659c\u548c\u8c03\u5236\u76f8\u5173\u7684\u8f6f\u6a21\uff0c\u4ee5\u53ca\u6301\u7eed\u5b58\u5728\u7684\u65e0\u5e8f-\u6709\u5e8f\u7279\u5f81\u3002\u8fd9\u8bc1\u660e\u4e86\u65e0\u94c5\u9499\u949b\u77ff\u5728\u9ad8\u538b\u4e0b\u5b58\u5728\u6781\u6027\u4e0d\u7a33\u5b9a\u6027\uff0c\u5c3d\u7ba1\u5176\u6240\u6709\u9ad8\u538b\u76f8\u5747\u5177\u6709\u4e2d\u5fc3\u5bf9\u79f0\u6027\u3002"}}
{"id": "2508.06338", "categories": ["quant-ph", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.06338", "abs": "https://arxiv.org/abs/2508.06338", "authors": ["Jisheng Dai", "Xue-Qin Jiang", "Tao Wang", "Peng Huang", "Guihua Zeng"], "title": "Arbitrarily-high-dimensional reconciliation via cross-rotation for continuous-variable quantum key distribution", "comment": "12 pages, 7 figures, Accepted 9 July, 2025, Physical Review A", "summary": "Multidimensional rotation serves as a powerful tool for enhancing information\nreconciliation and extending the transmission distance in continuous-variable\nquantum key distribution (CV-QKD). However, the lack of closed-form orthogonal\ntransformations for high-dimensional rotations has limited the maximum\nreconciliation efficiency to channels with 8 dimensions over the past decade.\nThis paper presents a cross-rotation scheme to overcome this limitation and\nenable reconciliation in arbitrarily high dimensions, constrained to even\nmultiples of 8. The key treatment involves reshaping the string vector into\nmatrix form and applying orthogonal transformations to its columns and rows in\na cross manner, thereby increasing the reconciliation dimension by one order\nper cross-rotation while significantly reducing the communication overhead over\nthe classical channel. A rigorous performance analysis is also presented from\nthe perspective of achievable sum-rate. Simulation results demonstrate that\n64-dimensional cross-rotation nearly approaches the upper bound, making it a\nrecommended choice for practical implementations.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u9ad8\u7ef4CV-QKD\u4fe1\u606f\u534f\u8c03\u65b0\u65b9\u6cd5\uff0c\u7a81\u7834\u7ef4\u5ea6\u9650\u5236\uff0c\u63d0\u9ad8\u6548\u7387\uff0c\u964d\u4f4e\u5f00\u9500\u3002", "motivation": "\u4e3a\u4e86\u514b\u670d\u73b0\u6709CV-QKD\u4e2d\u9ad8\u7ef4\u65cb\u8f6c\u95ed\u5408\u5f62\u5f0f\u6b63\u4ea4\u53d8\u6362\u7684\u7f3a\u5931\uff0c\u8be5\u7814\u7a76\u65e8\u5728\u7a81\u7834\u7ef4\u5ea6\u9650\u5236\uff0c\u63d0\u9ad8\u4fe1\u606f\u534f\u8c03\u6548\u7387\u548c\u4f20\u8f93\u8ddd\u79bb\u3002", "method": "\u901a\u8fc7\u5c06\u5b57\u7b26\u4e32\u5411\u91cf\u91cd\u5851\u4e3a\u77e9\u9635\u5f62\u5f0f\uff0c\u5e76\u4ee5\u4ea4\u53c9\u65b9\u5f0f\u5bf9\u5176\u5217\u548c\u884c\u5e94\u7528\u6b63\u4ea4\u53d8\u6362\uff0c\u5b9e\u73b0\u7ef4\u5ea6\u63d0\u5347\u548c\u5f00\u9500\u964d\u4f4e\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c64\u7ef4\u4ea4\u53c9\u65cb\u8f6c\u51e0\u4e4e\u63a5\u8fd1\u7406\u8bba\u4e0a\u9650\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4ea4\u53c9\u65cb\u8f6c\u65b9\u6848\uff0c\u7a81\u7834\u4e86\u9ad8\u7ef4\u65cb\u8f6c\u7684\u95ed\u5408\u5f62\u5f0f\u6b63\u4ea4\u53d8\u6362\u9650\u5236\uff0c\u5b9e\u73b0\u4e86\u4efb\u610f\u9ad8\u7ef4\u5ea6\uff08\u9650\u4e8e8\u7684\u5076\u6570\u500d\uff09\u7684CV-QKD\u4fe1\u606f\u534f\u8c03\uff0c\u5e76\u663e\u8457\u964d\u4f4e\u4e86\u7ecf\u5178\u4fe1\u9053\u901a\u4fe1\u5f00\u9500\u3002"}}
{"id": "2508.05805", "categories": ["quant-ph", "81P68"], "pdf": "https://arxiv.org/pdf/2508.05805", "abs": "https://arxiv.org/abs/2508.05805", "authors": ["Alok Shukla", "Prakash Vedula"], "title": "Modular Quantum Amplitude Estimation: A Scalable and Adaptive Framework", "comment": "16 pages", "summary": "Quantum Amplitude Estimation (QAE) is a key primitive in quantum computing,\nbut its standard implementation using Quantum Phase Estimation is\nresource-intensive, requiring a large number of coherent qubits in a single\ncircuit block to achieve high precision. This presents a significant challenge\nfor near-term Noisy Intermediate-Scale Quantum (NISQ) devices. To address this,\nwe introduce the Adaptive Windowed Quantum Amplitude Estimation (AWQAE)\nframework, a modular, scalable and adaptive approach that decouples estimation\nprecision from the number of physical qubits required in a single circuit.\nAWQAE operates by iteratively estimating the phase bits in small, fixed-size\nchunks, using a number of smaller, independent quantum circuits, which are\namenable to parallel processing. A key technical contribution of this work is\nintroduction of a phase resolution circuit and an ancilla-guided mechanism that\nenables accurate chunk assignment and eigenphase reconstruction in the presence\nof multiple eigenstates. This design is inherently NISQ-friendly, by lowering\ncircuit depth and qubit count per block to reduce decoherence and noise\neffects. A key component of our approach is a robust classical post-processing\nalgorithm that resolves measurement ambiguities that arise during the iterative\nprocess. This post-processing routine uses a least-significant-bit\n(LSB)-to-most-significant-bit (MSB) correction to reconstruct the full,\nhigh-precision phase estimate, ensuring accuracy. By combining a modular\nquantum-classical loop with an ambiguity-aware reconstruction method, AWQAE\noffers a powerful and flexible solution for performing high-precision QAE on\nresource-constrained quantum hardware. Our approach demonstrates enhanced\nscalability, and adaptability, making it a promising candidate for practical\napplications of QAE in the NISQ era.", "AI": {"tldr": "\u91cf\u5b50\u5e45\u5ea6\u4f30\u8ba1\uff08QAE\uff09\u5728NISQ\u8bbe\u5907\u4e0a\u662f\u8d44\u6e90\u5bc6\u96c6\u578b\u7684\u3002\u6211\u4eec\u63d0\u51fa\u4e86AWQAE\uff0c\u4e00\u4e2a\u6a21\u5757\u5316\u3001\u53ef\u6269\u5c55\u548c\u81ea\u9002\u5e94\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u8fed\u4ee3\u5730\u4f30\u8ba1\u76f8\u4f4d\u6bd4\u7279\u5757\u6765\u964d\u4f4e\u5bf9\u91cf\u5b50\u6bd4\u7279\u6570\u91cf\u7684\u8981\u6c42\u3002\u5b83\u4f7f\u7528\u91cf\u5b50\u7535\u8def\u548c\u7ecf\u5178\u540e\u5904\u7406\u6765\u63d0\u9ad8\u7cbe\u5ea6\u548c\u5904\u7406\u6d4b\u91cf\u6b67\u4e49\uff0c\u4f7f\u5176\u6210\u4e3aNISQ\u8bbe\u5907\u7684\u7406\u60f3\u9009\u62e9\u3002", "motivation": "\u6807\u51c6\u7684\u91cf\u5b50\u5e45\u5ea6\u4f30\u8ba1\uff08QAE\uff09\u5b9e\u73b0\u9700\u8981\u5927\u91cf\u7684\u76f8\u5e72\u91cf\u5b50\u6bd4\u7279\u5728\u5355\u4e2a\u7535\u8def\u5757\u4e2d\u4ee5\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\uff0c\u8fd9\u5bf9\u8fd1\u671f\u7684NISQ\u8bbe\u5907\u6765\u8bf4\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002AWQAE\u65e8\u5728\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "method": "AWQAE\u901a\u8fc7\u8fed\u4ee3\u5730\u4ee5\u5c0f\u5757\u3001\u56fa\u5b9a\u5927\u5c0f\u7684\u5757\u6765\u4f30\u8ba1\u76f8\u4f4d\u6bd4\u7279\uff0c\u4f7f\u7528\u591a\u4e2a\u8f83\u5c0f\u3001\u72ec\u7acb\u7684\u91cf\u5b50\u7535\u8def\u6765\u8fd0\u884c\uff0c\u8fd9\u6709\u5229\u4e8e\u5e76\u884c\u5904\u7406\u3002\u8be5\u6846\u67b6\u5f15\u5165\u4e86\u4e00\u4e2a\u76f8\u4f4d\u5206\u8fa8\u7387\u7535\u8def\u548c\u4e00\u4e2a\u8f85\u52a9\u5f15\u5bfc\u673a\u5236\uff0c\u4ee5\u5728\u5b58\u5728\u591a\u4e2a\u672c\u5f81\u6001\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u7cbe\u786e\u7684\u5757\u5206\u914d\u548c\u672c\u5f81\u76f8\u4f4d\u91cd\u5efa\u3002\u5b83\u7ed3\u5408\u4e86\u4e00\u4e2a\u6a21\u5757\u5316\u7684\u91cf\u5b50-\u7ecf\u5178\u5faa\u73af\u548c\u4e00\u4e2a\u5177\u6709\u6a21\u7cca\u610f\u8bc6\u7684\u91cd\u5efa\u65b9\u6cd5\u3002", "result": "AWQAE\u901a\u8fc7\u964d\u4f4e\u5355\u5757\u7535\u8def\u7684\u7535\u8def\u6df1\u5ea6\u548c\u91cf\u5b50\u6bd4\u7279\u6570\u91cf\uff0c\u51cf\u5c11\u4e86\u9000\u76f8\u5e72\u548c\u566a\u58f0\u7684\u5f71\u54cd\uff0c\u4f7f\u5176\u5177\u6709NISQ\u517c\u5bb9\u6027\u3002\u5b83\u901a\u8fc7\u4e00\u79cd\u89e3\u51b3\u6d4b\u91cf\u6b67\u4e49\u7684\u7ecf\u5178\u540e\u5904\u7406\u7b97\u6cd5\u6765\u786e\u4fdd\u51c6\u786e\u6027\u3002", "conclusion": "AWQAE\u662f\u4e00\u4e2a\u5f3a\u5927\u7684\u3001\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7528\u4e8e\u5728\u8d44\u6e90\u53d7\u9650\u7684\u91cf\u5b50\u786c\u4ef6\u4e0a\u6267\u884c\u9ad8\u7cbe\u5ea6\u7684QAE\u3002\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u589e\u5f3a\u7684\u53ef\u6269\u5c55\u6027\u548c\u9002\u5e94\u6027\uff0c\u4f7f\u5176\u6210\u4e3aNISQ\u65f6\u4ee3QAE\u5b9e\u9645\u5e94\u7528\u7684\u6709\u5e0c\u671b\u7684\u5019\u9009\u8005\u3002"}}
{"id": "2508.06026", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06026", "abs": "https://arxiv.org/abs/2508.06026", "authors": ["Yidong Wang", "Xin Wang", "Cunxiang Wang", "Junfeng Fang", "Qiufeng Wang", "Jianing Chu", "Xuran Meng", "Shuxun Yang", "Libo Qin", "Yue Zhang", "Wei Ye", "Shikun Zhang"], "title": "Temporal Self-Rewarding Language Models: Decoupling Chosen-Rejected via Past-Future", "comment": "12 pages, 5 figures", "summary": "Self-Rewarding Language Models propose an architecture in which the Large\nLanguage Models(LLMs) both generates responses and evaluates its own outputs\nvia LLM-as-a-Judge prompting, dynamically improving its generative capabilities\nthrough iterative Direct Preference Optimization (DPO). However, our analysis\nreveals a critical limitation in existing Self-Rewarding paradigms: the\nsynchronized improvement of chosen and rejected responses progressively narrows\nthe representational difference between contrasting samples, undermining\neffective preference learning. We propose \\textbf{Temporal Self-Rewarding\nLanguage Models} that strategically coordinate past, present, and future model\ngenerations to sustain learning signals. Our dual-phase framework introduces:\n(1) \\textit{Anchored Rejection} - fixing rejected responses using the past\ninitial model's outputs and (2) \\textit{Future-Guided Chosen} - dynamically\ncurating chosen samples using next-generation model predictions. Extensive\nexperiments across three model families (Llama, Qwen, Mistral) and different\nmodel sizes (Llama3B/8B/70B) demonstrate significant improvements when trained\nwith our method compared to Self-Rewarding using same computation resources.\nFor example, Llama3.1-8B reaches a 29.44 win rate on AlpacaEval 2.0 with our\nmethod, outperforming the Self-Rewarding baseline (19.69) by 9.75. Notably, our\nmethod also demonstrates superior out-of-distribution generalization across\nmathematical reasoning (GSM8K), knowledge-based QA (ARC, TruthfulQA), and code\ngeneration (HumanEval) tasks, even though we do not specifically collect such\ntraining data.", "AI": {"tldr": "\u81ea\u6211\u5956\u52b1\u8bed\u8a00\u6a21\u578b\u901a\u8fc7 LLM-as-a-Judge \u548c DPO \u8fed\u4ee3\u5730\u63d0\u9ad8\u751f\u6210\u80fd\u529b\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u65b9\u6cd5\u4f1a\u7f29\u5c0f\u5bf9\u6bd4\u6837\u672c\u4e4b\u95f4\u7684\u4ee3\u8868\u6027\u5dee\u5f02\uff0c\u4ece\u800c\u524a\u5f31\u504f\u597d\u5b66\u4e60\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u65f6\u95f4\u81ea\u6211\u5956\u52b1\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u951a\u5b9a\u62d2\u7edd\u548c\u672a\u6765\u6307\u5bfc\u9009\u62e9\u6765\u534f\u8c03\u8fc7\u53bb\u7684\u3001\u73b0\u5728\u7684\u548c\u672a\u6765\u7684\u6a21\u578b\u751f\u6210\uff0c\u4ee5\u7ef4\u6301\u5b66\u4e60\u4fe1\u53f7\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5404\u79cd\u6a21\u578b\u4e0a\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6539\u8fdb\uff0c\u5e76\u5728\u5404\u79cd\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u81ea\u6211\u5956\u52b1\u8303\u5f0f\u5b58\u5728\u4e00\u4e2a\u5173\u952e\u9650\u5236\uff1a\u6240\u9009\u54cd\u5e94\u548c\u88ab\u62d2\u7edd\u54cd\u5e94\u7684\u540c\u6b65\u6539\u8fdb\u4f1a\u9010\u6e10\u7f29\u5c0f\u5bf9\u6bd4\u6837\u672c\u4e4b\u95f4\u7684\u4ee3\u8868\u6027\u5dee\u5f02\uff0c\u4ece\u800c\u7834\u574f\u6709\u6548\u7684\u504f\u597d\u5b66\u4e60\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u65f6\u95f4\u81ea\u6211\u5956\u52b1\u8bed\u8a00\u6a21\u578b\u201d\u7684\u53cc\u9636\u6bb5\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5305\u62ec\u201c\u951a\u5b9a\u62d2\u7edd\u201d\uff08\u4f7f\u7528\u8fc7\u53bb\u521d\u59cb\u6a21\u578b\u7684\u8f93\u51fa\u6765\u56fa\u5b9a\u88ab\u62d2\u7edd\u7684\u54cd\u5e94\uff09\u548c\u201c\u672a\u6765\u6307\u5bfc\u9009\u62e9\u201d\uff08\u4f7f\u7528\u4e0b\u4e00\u4ee3\u6a21\u578b\u9884\u6d4b\u6765\u52a8\u6001\u7b56\u5212\u88ab\u9009\u62e9\u7684\u6837\u672c\uff09\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u81ea\u6211\u5956\u52b1\u8303\u5f0f\u4e2d\u540c\u6b65\u6539\u8fdb\u9009\u5b9a\u548c\u62d2\u7edd\u54cd\u5e94\u800c\u5bfc\u81f4\u7684\u4ee3\u8868\u6027\u5dee\u5f02\u7f29\u5c0f\u7684\u5173\u952e\u9650\u5236\u3002", "result": "\u5728 Llama\u3001Qwen \u548c Mistral \u4e09\u4e2a\u6a21\u578b\u7cfb\u5217\u4ee5\u53ca\u4e0d\u540c\u7684\u6a21\u578b\u5927\u5c0f\uff08Llama3B/8B/70B\uff09\u4e0a\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u4f7f\u7528\u76f8\u540c\u8ba1\u7b97\u8d44\u6e90\u7684\u81ea\u6211\u5956\u52b1\u76f8\u6bd4\uff0c\u4f7f\u7528\u6240\u63d0\u51fa\u65b9\u6cd5\u8bad\u7ec3\u7684\u6a21\u578b\u6709\u4e86\u663e\u8457\u6539\u8fdb\u3002\u4f8b\u5982\uff0cLlama3.1-8B \u5728 AlpacaEval 2.0 \u4e0a\u7684\u80dc\u7387\u8fbe\u5230\u4e86 29.44%\uff0c\u6bd4\u81ea\u6211\u5956\u52b1\u57fa\u7ebf\uff0819.69%\uff09\u63d0\u9ad8\u4e86 9.75%\u3002\u6b64\u5916\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u6570\u5b66\u63a8\u7406\uff08GSM8K\uff09\u3001\u57fa\u4e8e\u77e5\u8bc6\u7684\u95ee\u7b54\uff08ARC\u3001TruthfulQA\uff09\u548c\u4ee3\u7801\u751f\u6210\uff08HumanEval\uff09\u7b49\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u975e\u5206\u5e03\u5916\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65f6\u95f4\u81ea\u6211\u5956\u52b1\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u951a\u5b9a\u62d2\u7edd\u548c\u672a\u6765\u6307\u5bfc\u9009\u62e9\u6765\u89e3\u51b3\u73b0\u6709\u81ea\u6211\u5956\u52b1\u8303\u5f0f\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u534f\u8c03\u8fc7\u53bb\u7684\u3001\u73b0\u5728\u7684\u548c\u672a\u6765\u7684\u6a21\u578b\u751f\u6210\u6765\u7ef4\u6301\u5b66\u4e60\u4fe1\u53f7\uff0c\u5728\u5404\u79cd\u6a21\u578b\u7cfb\u5217\u548c\u5927\u5c0f\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u5e76\u5c55\u793a\u4e86\u4f18\u4e8e\u81ea\u6211\u5956\u52b1\u57fa\u7ebf\u7684\u663e\u8457\u6539\u8fdb\uff0c\u540c\u65f6\u5728\u5404\u79cd\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.05857", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05857", "abs": "https://arxiv.org/abs/2508.05857", "authors": ["Qiaomu Miao", "Vivek Raju Golani", "Jingyi Xu", "Progga Paromita Dutta", "Minh Hoai", "Dimitris Samaras"], "title": "Multi-view Gaze Target Estimation", "comment": "Accepted to ICCV 2025", "summary": "This paper presents a method that utilizes multiple camera views for the gaze\ntarget estimation (GTE) task. The approach integrates information from\ndifferent camera views to improve accuracy and expand applicability, addressing\nlimitations in existing single-view methods that face challenges such as face\nocclusion, target ambiguity, and out-of-view targets. Our method processes a\npair of camera views as input, incorporating a Head Information Aggregation\n(HIA) module for leveraging head information from both views for more accurate\ngaze estimation, an Uncertainty-based Gaze Selection (UGS) for identifying the\nmost reliable gaze output, and an Epipolar-based Scene Attention (ESA) module\nfor cross-view background information sharing. This approach significantly\noutperforms single-view baselines, especially when the second camera provides a\nclear view of the person's face. Additionally, our method can estimate the gaze\ntarget in the first view using the image of the person in the second view only,\na capability not possessed by single-view GTE methods. Furthermore, the paper\nintroduces a multi-view dataset for developing and evaluating multi-view GTE\nmethods. Data and code are available at\nhttps://www3.cs.stonybrook.edu/~cvl/multiview_gte.html", "AI": {"tldr": "\u672c\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u591a\u6444\u50cf\u5934\u89c6\u56fe\u8fdb\u884c\u6ce8\u89c6\u76ee\u6807\u4f30\u8ba1\uff08GTE\uff09\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6574\u5408\u591a\u89c6\u56fe\u4fe1\u606f\u3001\u5934\u90e8\u4fe1\u606f\u805a\u5408\u3001\u4e0d\u786e\u5b9a\u6027\u6ce8\u89c6\u9009\u62e9\u548c\u89c6\u5dee\u56fe\u573a\u666f\u6ce8\u610f\u529b\u7b49\u6a21\u5757\uff0c\u89e3\u51b3\u4e86\u5355\u89c6\u56fe\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u76f8\u5e94\u7684\u6570\u636e\u96c6\u548c\u4ee3\u7801\u3002", "motivation": "\u8be5\u65b9\u6cd5\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u5355\u89c6\u56fe\u65b9\u6cd5\u5728\u5904\u7406\u4eba\u8138\u906e\u6321\u3001\u76ee\u6807\u4e0d\u786e\u5b9a\u6027\u548c\u89c6\u91ce\u5916\u76ee\u6807\u7b49\u95ee\u9898\u65f6\u9762\u4e34\u7684\u6311\u6218\uff0c\u901a\u8fc7\u5229\u7528\u591a\u4e2a\u6444\u50cf\u5934\u89c6\u56fe\u6765\u63d0\u9ad8\u6ce8\u89c6\u76ee\u6807\u4f30\u8ba1\uff08GTE\uff09\u4efb\u52a1\u7684\u51c6\u786e\u6027\u548c\u6269\u5c55\u6027\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u65b9\u6cd5\u6574\u5408\u4e86\u6765\u81ea\u4e0d\u540c\u6444\u50cf\u5934\u89c6\u56fe\u7684\u4fe1\u606f\uff0c\u4ee5\u63d0\u9ad8\u51c6\u786e\u6027\u5e76\u6269\u5927\u9002\u7528\u6027\u3002\u5177\u4f53\u800c\u8a00\uff0c\u8be5\u65b9\u6cd5\u5904\u7406\u4e00\u5bf9\u6444\u50cf\u5934\u89c6\u56fe\u4f5c\u4e3a\u8f93\u5165\uff0c\u5305\u62ec\u4e00\u4e2a\u5934\u90e8\u4fe1\u606f\u805a\u5408\uff08HIA\uff09\u6a21\u5757\uff0c\u7528\u4e8e\u5229\u7528\u6765\u81ea\u4e24\u4e2a\u89c6\u56fe\u7684\u5934\u90e8\u4fe1\u606f\u8fdb\u884c\u66f4\u51c6\u786e\u7684\u6ce8\u89c6\u4f30\u8ba1\uff1b\u4e00\u4e2a\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u7684\u6ce8\u89c6\u9009\u62e9\uff08UGS\uff09\u6a21\u5757\uff0c\u7528\u4e8e\u8bc6\u522b\u6700\u53ef\u9760\u7684\u6ce8\u89c6\u8f93\u51fa\uff1b\u4ee5\u53ca\u4e00\u4e2a\u57fa\u4e8e\u89c6\u5dee\u56fe\u7684\u573a\u666f\u6ce8\u610f\u529b\uff08ESA\uff09\u6a21\u5757\uff0c\u7528\u4e8e\u8de8\u89c6\u56fe\u80cc\u666f\u4fe1\u606f\u5171\u4eab\u3002", "result": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u591a\u89c6\u56fe\u65b9\u6cd5\u5728\u6ce8\u89c6\u76ee\u6807\u4f30\u8ba1\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u5355\u89c6\u56fe\u57fa\u7ebf\uff0c\u5e76\u4e14\u80fd\u591f\u5b9e\u73b0\u5355\u89c6\u56fe\u65b9\u6cd5\u65e0\u6cd5\u5b8c\u6210\u7684\u4ec5\u5229\u7528\u53e6\u4e00\u6444\u50cf\u5934\u56fe\u50cf\u4f30\u8ba1\u6ce8\u89c6\u76ee\u6807\u7684\u80fd\u529b\u3002\u6b64\u5916\uff0c\u8be5\u8bba\u6587\u8fd8\u5f15\u5165\u4e86\u4e00\u4e2a\u7528\u4e8e\u5f00\u53d1\u548c\u8bc4\u4f30\u591a\u89c6\u56feGTE\u65b9\u6cd5\u7684\u591a\u89c6\u56fe\u6570\u636e\u96c6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u5355\u89c6\u56fe\u57fa\u7ebf\uff0c\u5c24\u5176\u662f\u5728\u7b2c\u4e8c\u4e2a\u6444\u50cf\u5934\u80fd\u6e05\u6670\u770b\u5230\u4eba\u8138\u7684\u60c5\u51b5\u4e0b\u3002\u6b64\u5916\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u7684\u65b9\u6cd5\u53ef\u4ee5\u4ec5\u5229\u7528\u7b2c\u4e8c\u4e2a\u6444\u50cf\u5934\u4e2d\u7684\u4eba\u8138\u56fe\u50cf\u6765\u4f30\u8ba1\u7b2c\u4e00\u4e2a\u6444\u50cf\u5934\u4e2d\u7684\u6ce8\u89c6\u76ee\u6807\uff0c\u8fd9\u662f\u5355\u89c6\u56feGTE\u65b9\u6cd5\u6240\u4e0d\u5177\u5907\u7684\u80fd\u529b\u3002"}}
{"id": "2508.06206", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06206", "abs": "https://arxiv.org/abs/2508.06206", "authors": ["Hanqing Wang", "Shaoyang Wang", "Yiming Zhong", "Zemin Yang", "Jiamin Wang", "Zhiqing Cui", "Jiahao Yuan", "Yifan Han", "Mingyu Liu", "Yuexin Ma"], "title": "Affordance-R1: Reinforcement Learning for Generalizable Affordance Reasoning in Multimodal Large Language Model", "comment": null, "summary": "Affordance grounding focuses on predicting the specific regions of objects\nthat are associated with the actions to be performed by robots. It plays a\nvital role in the fields of human-robot interaction, human-object interaction,\nembodied manipulation, and embodied perception. Existing models often neglect\nthe affordance shared among different objects because they lack the\nChain-of-Thought(CoT) reasoning abilities, limiting their out-of-domain (OOD)\ngeneralization and explicit reasoning capabilities. To address these\nchallenges, we propose Affordance-R1, the first unified affordance grounding\nframework that integrates cognitive CoT guided Group Relative Policy\nOptimization (GRPO) within a reinforcement learning paradigm. Specifically, we\ndesigned a sophisticated affordance function, which contains format,\nperception, and cognition rewards to effectively guide optimization directions.\nFurthermore, we constructed a high-quality affordance-centric reasoning\ndataset, ReasonAff, to support training. Trained exclusively via reinforcement\nlearning with GRPO and without explicit reasoning data, Affordance-R1 achieves\nrobust zero-shot generalization and exhibits emergent test-time reasoning\ncapabilities. Comprehensive experiments demonstrate that our model outperforms\nwell-established methods and exhibits open-world generalization. To the best of\nour knowledge, Affordance-R1 is the first to integrate GRPO-based RL with\nreasoning into affordance reasoning. The code of our method and our dataset is\nreleased on https://github.com/hq-King/Affordance-R1.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86 Affordance-R1\uff0c\u4e00\u4e2a\u521b\u65b0\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u8ba4\u77e5\u6027\u601d\u7ef4\u94fe\uff08CoT\uff09\u548c\u7fa4\u4f53\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08GRPO\uff09\u6765\u89e3\u51b3\u673a\u5668\u4eba\u535a\u5f08\u63a8\u7406\u4e2d\u7684\u6311\u6218\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u6cdb\u5316\u80fd\u529b\u548c\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u5e38\u5e38\u5ffd\u7565\u4e0d\u540c\u7269\u4f53\u4e4b\u95f4\u5171\u4eab\u7684\u535a\u5f08\uff0c\u56e0\u4e3a\u5b83\u4eec\u7f3a\u4e4f\u601d\u7ef4\u94fe\uff08CoT\uff09\u63a8\u7406\u80fd\u529b\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u5728\u57df\u5916\uff08OOD\uff09\u7684\u6cdb\u5316\u80fd\u529b\u548c\u663e\u5f0f\u63a8\u7406\u80fd\u529b\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86 Affordance-R1\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u5c06\u8ba4\u77e5\u6027\u601d\u7ef4\u94fe\uff08CoT\uff09\u6307\u5bfc\u7684\u7fa4\u4f53\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08GRPO\uff09\u6574\u5408\u5230\u5f3a\u5316\u5b66\u4e60\u8303\u5f0f\u4e2d\u7684\u7edf\u4e00\u6027\u535a\u5f08\u63a8\u7406\u6846\u67b6\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u590d\u6742\u7684\u535a\u5f08\u51fd\u6570\uff0c\u5176\u4e2d\u5305\u542b\u683c\u5f0f\u3001\u611f\u77e5\u548c\u8ba4\u77e5\u5956\u52b1\uff0c\u4ee5\u6709\u6548\u5730\u6307\u5bfc\u4f18\u5316\u65b9\u5411\u3002", "result": "\u6240\u63d0\u51fa\u7684 Affordance-R1 \u5728\u6ca1\u6709\u663e\u5f0f\u63a8\u7406\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u4ec5\u901a\u8fc7 GRPO \u7684\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u8bad\u7ec3\uff0c\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5c55\u73b0\u51fa\u6d8c\u73b0\u7684\u6d4b\u8bd5\u65f6\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "Affordance-R1 \u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5c55\u73b0\u51fa\u6d8c\u73b0\u7684\u6d4b\u8bd5\u65f6\u63a8\u7406\u80fd\u529b\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0c\u6211\u4eec\u7684\u6a21\u578b\u4f18\u4e8e\u6210\u719f\u7684\u65b9\u6cd5\uff0c\u5e76\u5c55\u73b0\u51fa\u5f00\u653e\u4e16\u754c\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.06074", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06074", "abs": "https://arxiv.org/abs/2508.06074", "authors": ["Siyi Lu", "Run Liu", "Dongsheng Yang", "Lei He"], "title": "ME$^3$-BEV: Mamba-Enhanced Deep Reinforcement Learning for End-to-End Autonomous Driving with BEV-Perception", "comment": null, "summary": "Autonomous driving systems face significant challenges in perceiving complex\nenvironments and making real-time decisions. Traditional modular approaches,\nwhile offering interpretability, suffer from error propagation and coordination\nissues, whereas end-to-end learning systems can simplify the design but face\ncomputational bottlenecks. This paper presents a novel approach to autonomous\ndriving using deep reinforcement learning (DRL) that integrates bird's-eye view\n(BEV) perception for enhanced real-time decision-making. We introduce the\n\\texttt{Mamba-BEV} model, an efficient spatio-temporal feature extraction\nnetwork that combines BEV-based perception with the Mamba framework for\ntemporal feature modeling. This integration allows the system to encode vehicle\nsurroundings and road features in a unified coordinate system and accurately\nmodel long-range dependencies. Building on this, we propose the\n\\texttt{ME$^3$-BEV} framework, which utilizes the \\texttt{Mamba-BEV} model as a\nfeature input for end-to-end DRL, achieving superior performance in dynamic\nurban driving scenarios. We further enhance the interpretability of the model\nby visualizing high-dimensional features through semantic segmentation,\nproviding insight into the learned representations. Extensive experiments on\nthe CARLA simulator demonstrate that \\texttt{ME$^3$-BEV} outperforms existing\nmodels across multiple metrics, including collision rate and trajectory\naccuracy, offering a promising solution for real-time autonomous driving.", "AI": {"tldr": "ME^3-BEV uses Mamba-BEV and DRL for better autonomous driving, showing improved performance and interpretability in simulations.", "motivation": "Traditional modular approaches in autonomous driving suffer from error propagation and coordination issues, while end-to-end learning systems face computational bottlenecks. The paper aims to address these challenges by proposing a novel approach that integrates BEV perception with deep reinforcement learning for enhanced real-time decision-making in complex driving environments.", "method": "The paper introduces the Mamba-BEV model for efficient spatio-temporal feature extraction, combining bird's-eye view (BEV) perception with the Mamba framework. This model is integrated into the ME^3-BEV framework, which uses deep reinforcement learning (DRL) for end-to-end decision-making. Interpretability is addressed by visualizing high-dimensional features through semantic segmentation.", "result": "Extensive experiments on the CARLA simulator show that the ME^3-BEV framework outperforms existing models in metrics such as collision rate and trajectory accuracy, demonstrating its potential as a promising solution for real-time autonomous driving.", "conclusion": "ME^3-BEV, a novel framework integrating Mamba-BEV and DRL, achieves superior performance in autonomous driving by enhancing real-time decision-making through efficient spatio-temporal feature extraction and long-range dependency modeling. The model's interpretability is improved via semantic segmentation visualization, and experiments on the CARLA simulator validate its effectiveness against existing models."}}
{"id": "2508.05957", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05957", "abs": "https://arxiv.org/abs/2508.05957", "authors": ["Hasibul Karim Shanto", "Umme Ayman Koana", "Shadikur Rahman"], "title": "Multi-Armed Bandits-Based Optimization of Decision Trees", "comment": null, "summary": "Decision trees, without appropriate constraints, can easily become overly\ncomplex and prone to overfit, capturing noise rather than generalizable\npatterns. To resolve this problem,pruning operation is a crucial part in\noptimizing decision trees, as it not only reduces the complexity of trees but\nalso decreases the probability of generating overfit models. The conventional\npruning techniques like Cost-Complexity Pruning (CCP) and Reduced Error Pruning\n(REP) are mostly based on greedy approaches that focus on immediate gains in\nperformance while pruning nodes of the decision tree. However, this might\nresult in a lower generalization in the long run, compromising the robust\nability of the tree model when introduced to unseen data samples, particularly\nwhen trained with small and complex datasets. To address this challenge, we are\nproposing a Multi-Armed Bandits (MAB)-based pruning approach, a reinforcement\nlearning (RL)-based technique, that will dynamically prune the tree to generate\nan optimal decision tree with better generalization. Our proposed approach\nassumes the pruning process as an exploration-exploitation problem, where we\nare utilizing the MAB algorithms to find optimal branch nodes to prune based on\nfeedback from each pruning actions. Experimental evaluation on several\nbenchmark datasets, demonstrated that our proposed approach results in better\npredictive performance compared to the traditional ones. This suggests the\npotential of utilizing MAB for a dynamic and probabilistic way of decision tree\npruning, in turn optimizing the decision tree-based model.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e MAB \u7684\u51b3\u7b56\u6811\u526a\u679d\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u526a\u679d\u89c6\u4e3a\u63a2\u7d22-\u5229\u7528\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u52a8\u6001\u526a\u679d\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7684\u526a\u679d\u6280\u672f\uff08\u5982 CCP \u548c REP\uff09\u591a\u57fa\u4e8e\u8d2a\u5a6a\u65b9\u6cd5\uff0c\u53ef\u80fd\u5bfc\u81f4\u6a21\u578b\u5728\u9762\u5bf9\u5c0f\u578b\u590d\u6742\u6570\u636e\u96c6\u65f6\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u52a8\u6001\u526a\u679d\u4ee5\u83b7\u5f97\u66f4\u597d\u6cdb\u5316\u80fd\u529b\u7684\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u81c2\u8001\u864e\u673a\uff08MAB\uff09\u7684\u526a\u679d\u65b9\u6cd5\uff0c\u5c06\u526a\u679d\u8fc7\u7a0b\u5efa\u6a21\u4e3a\u63a2\u7d22-\u5229\u7528\u95ee\u9898\uff0c\u5e76\u5229\u7528 MAB \u7b97\u6cd5\u6839\u636e\u53cd\u9988\u52a8\u6001\u5730\u9009\u62e9\u6700\u4f18\u5206\u652f\u8fdb\u884c\u526a\u679d\u3002", "result": "\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684 MAB \u526a\u679d\u65b9\u6cd5\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u9884\u6d4b\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u81c2\u8001\u864e\u673a\uff08MAB\uff09\u7684\u51b3\u7b56\u6811\u526a\u679d\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u526a\u679d\u8fc7\u7a0b\u89c6\u4e3a\u63a2\u7d22-\u5229\u7528\u95ee\u9898\uff0c\u5229\u7528 MAB \u7b97\u6cd5\u6839\u636e\u6bcf\u6b21\u526a\u679d\u64cd\u4f5c\u7684\u53cd\u9988\u52a8\u6001\u5730\u5bfb\u627e\u6700\u4f18\u526a\u679d\u5206\u652f\uff0c\u4ee5\u751f\u6210\u6cdb\u5316\u80fd\u529b\u66f4\u5f3a\u7684\u51b3\u7b56\u6811\u6a21\u578b\uff0c\u5e76\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u4f20\u7edf\u7684\u526a\u679d\u6280\u672f\u3002"}}
{"id": "2508.06421", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2508.06421", "abs": "https://arxiv.org/abs/2508.06421", "authors": ["Andrzej Dzienia", "Patrycja Taborowska", "Pawel Kubica-Cypek", "Dawid Janas"], "title": "Programing optical properties of single-walled carbon nanotubes with benzoyl peroxide derivatives of tailored chemical characteristics", "comment": "Pages 1-30 (main text), pages 31-51 (supporting information)", "summary": "Semiconducting single-walled carbon nanotubes (SWCNTs) have great potential\nfor optoelectronics and photonics, further enhanced by covalent\nfunctionalization. However, scalable and controlled surface modification is\nchallenging due to complex methodologies and unstable reagents. Benzoyl\nperoxide (BPO) has emerged as a simple alternative for introducing luminescent\ndefects into SWCNTs. Yet, the lack of understanding of its radical chemistry\nlimits precise defect engineering using BPOs. This is a major obstacle to the\neffective application of BPO in chemistry, despite its widespread use as a\nradical initiator. We present a thorough investigation into the radical\nchemistry of self-synthesized BPOs for functionalizing polymer-wrapped (6,5)\nand (7,5) SWCNTs in non-polar solvents, providing critical insights into the\ndecomposition of BPO and its analogs. By varying the electronic and steric\nproperties of typically unavailable BPO derivatives, we demonstrate tunability\nover the photoluminescence characteristics of SWCNTs, allowing control over\ndefect density and light emission wavelength. This toolbox of BPO derivatives,\ncreated with simple radical chemistry and accessible organic precursors,\nalongside clarified structure-property relationships, facilitates effective\nimplementation of BPO in chemical transformations and meticulous engineering of\nluminescent defects in SWCNTs for optoelectronic applications. Notably, this\nresearch offers insights into why SWCNTs modified with electron-deficient\nreactants provide the best optical characteristics.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u7cbe\u7ec6\u8c03\u63a7BPO\u7684\u81ea\u7531\u57fa\u5316\u5b66\uff0c\u5b9e\u73b0\u4e86\u5bf9\u78b3\u7eb3\u7c73\u7ba1\u53d1\u5149\u7279\u6027\u7684\u7cbe\u786e\u63a7\u5236\uff0c\u4e3a\u5149\u7535\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u76ee\u524d\u5728SWCNT\u8868\u9762\u6539\u6027\u4e2d\uff0c\u4f7f\u7528BPO\u8fdb\u884c\u7cbe\u786e\u7f3a\u9677\u5de5\u7a0b\u7684\u81ea\u7531\u57fa\u5316\u5b66\u7406\u89e3\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u6539\u53d8BPO\u884d\u751f\u7269\u7684\u7535\u5b50\u548c\u7a7a\u95f4\u6027\u8d28\uff0c\u7814\u7a76\u4e86\u5176\u5bf9\u805a\u5408\u7269\u5305\u88f9\u7684(6,5)\u548c(7,5)SWCNT\u7684\u529f\u80fd\u5316\uff0c\u5e76\u5206\u6790\u4e86BPO\u53ca\u5176\u7c7b\u4f3c\u7269\u7684\u5206\u89e3\u3002", "result": "\u5236\u5907\u4e86\u4e00\u7cfb\u5217BPO\u884d\u751f\u7269\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u5728\u8c03\u63a7SWCNT\u5149\u81f4\u53d1\u5149\u7279\u6027\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u5b9e\u73b0\u4e86\u5bf9\u7f3a\u9677\u5bc6\u5ea6\u548c\u53d1\u5149\u6ce2\u957f\u7684\u63a7\u5236\uff0c\u5e76\u53d1\u73b0\u4e86\u7f3a\u7535\u5b50\u53cd\u5e94\u7269\u4fee\u9970\u7684SWCNT\u5177\u6709\u6700\u4f73\u5149\u5b66\u7279\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u9610\u660e\u4e86BPO\u884d\u751f\u7269\u7684\u81ea\u7531\u57fa\u5316\u5b66\uff0c\u5e76\u5c55\u793a\u4e86\u5982\u4f55\u901a\u8fc7\u8c03\u63a7SWCNT\u7684\u5149\u81f4\u53d1\u5149\u7279\u6027\u6765\u63a7\u5236\u7f3a\u9677\u5bc6\u5ea6\u548c\u53d1\u5149\u6ce2\u957f\uff0c\u4e3a\u5149\u7535\u5e94\u7528\u63d0\u4f9b\u4e86\u7cbe\u786e\u8c03\u63a7\u53d1\u5149\u7f3a\u9677\u7684\u5de5\u5177\u7bb1\uff0c\u5e76\u63ed\u793a\u4e86\u7f3a\u7535\u5b50\u53cd\u5e94\u7269\u4fee\u9970\u7684SWCNT\u4e3a\u4f55\u5177\u6709\u6700\u4f73\u5149\u5b66\u7279\u6027\u7684\u539f\u56e0\u3002"}}
{"id": "2508.05815", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.05815", "abs": "https://arxiv.org/abs/2508.05815", "authors": ["Cyril Belardinelli"], "title": "Riemann-Zeta-Regularisation of Feynman Path Integrals", "comment": "19 pages", "summary": "The Feynman Propagator of a charged particle confined to an anisotropic\nHarmonic Oscillator potential and moving in a crossed electromagnetic field is\ncalculated in a conceptually new way. The calculation is based on the expansion\nof the path variable into a complex Fourier series. The path integral then\nbecomes an infinite product of Gaussian integrals. This product is divergent.\nIt turns out that we can regularize this product by using the zeta-function. It\nis a remarkable fact that the zeta-function is so well suited as a\nregularizator for divergent path integrals.", "AI": {"tldr": "A new method using zeta-function regularization was used to calculate the Feynman propagator for a charged particle in a specific potential and field, showing the zeta-function's suitability for divergent path integrals.", "motivation": "To calculate the Feynman propagator of a charged particle in a complex system (anisotropic harmonic oscillator potential with a crossed electromagnetic field) using a conceptually new method.", "method": "The Feynman propagator is calculated using a novel approach involving the expansion of the path variable into a complex Fourier series, transforming the path integral into an infinite product of Gaussian integrals. This divergent product is then regularized using the zeta-function.", "result": "A method for regularizing divergent path integrals using the zeta-function has been developed and applied to calculate the Feynman propagator for the specified system. The study highlights the effectiveness of the zeta-function as a regularizer.", "conclusion": "The zeta-function is a well-suited regularizer for divergent path integrals, as demonstrated by its application to the Feynman propagator of a charged particle in an anisotropic harmonic oscillator potential and a crossed electromagnetic field."}}
{"id": "2508.06030", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06030", "abs": "https://arxiv.org/abs/2508.06030", "authors": ["Kartik Sharma", "Yiqiao Jin", "Rakshit Trivedi", "Srijan Kumar"], "title": "Efficient Knowledge Probing of Large Language Models by Adapting Pre-trained Embeddings", "comment": null, "summary": "Large language models (LLMs) acquire knowledge across diverse domains such as\nscience, history, and geography encountered during generative pre-training.\nHowever, due to their stochasticity, it is difficult to predict what LLMs have\nacquired. Prior work has developed different ways to probe this knowledge by\ninvestigating the hidden representations, crafting specific task prompts,\ncurating representative samples, and estimating their uncertainty. However,\nthese methods require making forward passes through the underlying model to\nprobe the LLM's knowledge about a specific fact, making them computationally\nexpensive and time-consuming. To bridge this gap, we propose $\\textbf{PEEK}$ or\n$\\textbf{P}$roxy $\\textbf{E}$mbeddings to $\\textbf{E}$stimate\n$\\textbf{K}$nowledge of LLMs, by leveraging the pre-trained embedding models\nthat effectively encode factual knowledge as text or graphs as proxies for\nLLMs. First, we identify a training set of facts known by LLMs through various\nprobing strategies and then adapt embedding models to predict the LLM outputs\nwith a linear decoder layer. Comprehensive evaluation on $3$ Wikipedia-derived\ndatasets, $4$ LLMs, and $7$ embedding models shows that embeddings can predict\nLLM knowledge on a held-out set with up to 90 % accuracy. Furthermore, we find\nthat sentence embedding models are more suitable than graph embeddings to\npredict LLM knowledge, shedding light on the underlying representation of the\nfactual landscape. Thus, we believe that knowledge-adapted embeddings can be\nused to identify knowledge gaps in LLMs at scale and can provide deeper\ninsights into LLMs' internal inductive bias. The code and data are made\navailable at https://github.com/claws-lab/peek.", "AI": {"tldr": "LLM\u77e5\u8bc6\u96be\u4ee5\u9884\u6d4b\u4e14\u63a2\u6d4b\u6210\u672c\u9ad8\u3002\u63d0\u51faPEEK\u65b9\u6cd5\uff0c\u5229\u7528\u4ee3\u7406\u5d4c\u5165\u6a21\u578b\uff08\u7279\u522b\u662f\u53e5\u5b50\u5d4c\u5165\uff09\u4ee5\u9ad8\u8fbe90%\u7684\u51c6\u786e\u7387\u9884\u6d4bLLM\u77e5\u8bc6\uff0c\u540c\u65f6\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "LLM\u5728\u9884\u8bad\u7ec3\u4e2d\u83b7\u53d6\u4e86\u8de8\u5b66\u79d1\u7684\u77e5\u8bc6\uff0c\u4f46\u5176\u968f\u673a\u6027\u4f7f\u5f97\u9884\u6d4b\u5176\u77e5\u8bc6\u83b7\u53d6\u53d8\u5f97\u56f0\u96be\u3002\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u63a2\u6d4b\u9690\u85cf\u8868\u793a\u3001\u8bbe\u8ba1\u7279\u5b9a\u4efb\u52a1\u63d0\u793a\u3001\u7b56\u5212\u4ee3\u8868\u6027\u6837\u672c\u548c\u4f30\u8ba1\u4e0d\u786e\u5b9a\u6027\uff09\u9700\u8981\u5bf9\u6a21\u578b\u8fdb\u884c\u591a\u6b21\u524d\u5411\u4f20\u64ad\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u4e14\u8017\u65f6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPEEK\uff08Proxy Embeddings to Estimate Knowledge\uff09\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u7684\u5d4c\u5165\u6a21\u578b\u4f5c\u4e3aLLM\u7684\u4ee3\u7406\uff0c\u901a\u8fc7\u7ebf\u6027\u89e3\u7801\u5668\u5c42\u6765\u9884\u6d4bLLM\u7684\u8f93\u51fa\u6765\u4f30\u8ba1LLM\u7684\u77e5\u8bc6\u3002\u9996\u5148\u901a\u8fc7\u5404\u79cd\u63a2\u6d4b\u7b56\u7565\u8bc6\u522bLLM\u5df2\u77e5\u7684\u8bad\u7ec3\u4e8b\u5b9e\u96c6\uff0c\u7136\u540e\u8c03\u6574\u5d4c\u5165\u6a21\u578b\u3002", "result": "\u57283\u4e2a\u57fa\u4e8eWikipedia\u7684\u6570\u636e\u96c6\u30014\u4e2aLLM\u548c7\u4e2a\u5d4c\u5165\u6a21\u578b\u4e0a\u8fdb\u884c\u4e86\u5168\u9762\u8bc4\u4f30\uff0c\u7ed3\u679c\u8868\u660e\u5d4c\u5165\u53ef\u4ee5\u5728\u6837\u672c\u5916\u6570\u636e\u96c6\u4e0a\u4ee5\u9ad8\u8fbe90%\u7684\u51c6\u786e\u7387\u9884\u6d4bLLM\u7684\u77e5\u8bc6\u3002\u6b64\u5916\uff0c\u7814\u7a76\u53d1\u73b0\u53e5\u5b50\u5d4c\u5165\u6a21\u578b\u6bd4\u56fe\u5d4c\u5165\u6a21\u578b\u66f4\u9002\u5408\u9884\u6d4bLLM\u7684\u77e5\u8bc6\u3002", "conclusion": "\u77e5\u8bc6\u9002\u5e94\u578b\u5d4c\u5165\u53ef\u4ee5\u5927\u89c4\u6a21\u8bc6\u522bLLM\u4e2d\u7684\u77e5\u8bc6\u5dee\u8ddd\uff0c\u5e76\u4e3aLLM\u7684\u5185\u90e8\u5f52\u7eb3\u504f\u5dee\u63d0\u4f9b\u66f4\u6df1\u5165\u7684\u89c1\u89e3\u3002\u8bc4\u4f30\u8868\u660e\uff0c\u5d4c\u5165\u53ef\u4ee5\u9884\u6d4bLLM\u5728\u6837\u672c\u5916\u77e5\u8bc6\uff0c\u51c6\u786e\u7387\u9ad8\u8fbe90%\u3002"}}
{"id": "2508.05898", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05898", "abs": "https://arxiv.org/abs/2508.05898", "authors": ["Hamidreza Dastmalchi", "Aijun An", "Ali cheraghian"], "title": "ETTA: Efficient Test-Time Adaptation for Vision-Language Models through Dynamic Embedding Updates", "comment": "BMVC2025", "summary": "Pretrained vision-language models (VLMs) like CLIP show strong zero-shot\nperformance but struggle with generalization under distribution shifts.\nTest-Time Adaptation (TTA) addresses this by adapting VLMs to unlabeled test\ndata in new domains. While some TTA methods rely on prompt-tuning,\ntraining-free cache-based approaches are preferred for efficiency. However,\ncurrent cache-based TTA models store only a limited set of high-confidence\nsamples, restricting the decision boundary to these samples and ignoring the\ninfluence of other incoming test data. To address this, we propose Efficient\nTest-Time Adaptation (ETTA), introducing a Recursive Updating module that\nintegrates all incoming test samples, progressively refining the decision\nboundary. This strategy mimics an unbounded cache, dynamically updating\ncontextual embeddings for improved accuracy with minimal memory and\ncomputational overhead. ETTA also includes an Adaptive Ensemble module to\nreduce prompt dependency in image-to-text scores by dynamically selecting\noptimal prompts for each class. Furthermore, ETTA adaptively combines scores\nfrom both modules based on confidence levels, leveraging their complementary\nstrengths. Extensive experiments on two benchmarks confirm that ETTA surpasses\nthe state-of-the-art TTA models in computational complexity and accuracy,\nsetting a new standard for effective, efficient test-time adaptation. The code\nhas been released at https://github.com/hamidreza-dastmalchi/ETTA.", "AI": {"tldr": "ETTA is a test-time adaptation method that improves VLM generalization under distribution shifts by using a Recursive Updating module to integrate all test samples and an Adaptive Ensemble module to optimize prompt selection, outperforming existing methods in accuracy and efficiency.", "motivation": "Current cache-based TTA models store limited high-confidence samples, restricting the decision boundary and ignoring other incoming test data. ETTA aims to address this by integrating all incoming test samples for improved accuracy with minimal overhead.", "method": "ETTA introduces a Recursive Updating module that integrates all incoming test samples, progressively refining the decision boundary, mimicking an unbounded cache. It also includes an Adaptive Ensemble module to reduce prompt dependency by dynamically selecting optimal prompts for each class. ETTA adaptively combines scores from both modules based on confidence levels.", "result": "ETTA surpasses the state-of-the-art TTA models in computational complexity and accuracy.", "conclusion": "ETTA surpasses the state-of-the-art TTA models in computational complexity and accuracy, setting a new standard for effective, efficient test-time adaptation."}}
{"id": "2508.06207", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06207", "abs": "https://arxiv.org/abs/2508.06207", "authors": ["Andrea Dal Prete", "Seyram Ofori", "Chan Yon Sin", "Ashwin Narayan", "Francesco Braghin", "Marta Gandolla", "Haoyong Yu"], "title": "Computer Vision-based Adaptive Control for Back Exoskeleton Performance Optimization", "comment": null, "summary": "Back exoskeletons can reduce musculoskeletal strain, but their effectiveness\ndepends on support modulation and adaptive control. This study addresses two\nchallenges: defining optimal support strategies and developing adaptive control\nbased on payload estimation. We introduce an optimization space based on muscle\nactivity reduction, perceived discomfort, and user preference, constructing\nfunctions to identify optimal strategies. Experiments with 12 subjects revealed\noptimal operating regions, highlighting the need for dynamic modulation. Based\non these insights, we developed a vision-based adaptive control pipeline that\nestimates payloads in real-time by enhancing exoskeleton contextual\nunderstanding, minimising latency and enabling support adaptation within the\ndefined optimisation space. Validation with 12 more subjects showed over 80%\naccuracy and improvements across all metrics. Compared to static control,\nadaptive modulation reduced peak back muscle activation by up to 23% while\npreserving user preference and minimising discomfort. These findings validate\nthe proposed framework and highlight the potential of intelligent,\ncontext-aware control in industrial exoskeletons.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u4f18\u5316\u652f\u6491\u7b56\u7565\u548c\u5f00\u53d1\u57fa\u4e8e\u89c6\u89c9\u7684\u81ea\u9002\u5e94\u63a7\u5236\uff0c\u63d0\u9ad8\u4e86\u5916\u9aa8\u9abc\u7684\u6709\u6548\u6027\uff0c\u51cf\u5c11\u4e86\u7528\u6237\u7684\u808c\u8089\u8d1f\u62c5\u548c\u4e0d\u9002\u611f\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u80cc\u90e8\u5916\u9aa8\u9abc\u7684\u6709\u6548\u6027\uff0c\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a\u5b9a\u4e49\u6700\u4f18\u652f\u6491\u7b56\u7565\u4ee5\u53ca\u5f00\u53d1\u57fa\u4e8e\u8d1f\u8f7d\u4f30\u8ba1\u7684\u81ea\u9002\u5e94\u63a7\u5236\u3002", "method": "\u672c\u7814\u7a76\u901a\u8fc7\u4f18\u5316\u7a7a\u95f4\u5b9a\u4e49\u6700\u4f18\u652f\u6491\u7b56\u7565\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u7684\u81ea\u9002\u5e94\u63a7\u5236\u6d41\u7a0b\uff0c\u5229\u7528\u673a\u5668\u5b66\u4e60\u5b9e\u65f6\u4f30\u7b97\u8d1f\u8f7d\uff0c\u4ee5\u6700\u5c0f\u5316\u5ef6\u8fdf\u5e76\u5b9e\u73b0\u652f\u6491\u7684\u52a8\u6001\u8c03\u6574\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u7684\u81ea\u9002\u5e94\u5916\u9aa8\u9abc\u63a7\u5236\u6846\u67b6\u80fd\u591f\u51c6\u786e\u4f30\u8ba1\u8d1f\u8f7d\uff08\u51c6\u786e\u7387\u8d85\u8fc780%\uff09\uff0c\u5e76\u5c06\u5cf0\u503c\u80cc\u90e8\u808c\u8089\u6fc0\u6d3b\u964d\u4f4e\u9ad8\u8fbe23%\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u7528\u6237\u7684\u504f\u597d\u548c\u8212\u9002\u5ea6\uff0c\u4e0e\u9759\u6001\u63a7\u5236\u76f8\u6bd4\u6709\u4e86\u663e\u8457\u6539\u5584\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u6846\u67b6\u901a\u8fc7\u5b9e\u73b0\u52a8\u6001\u8c03\u6574\u548c\u667a\u80fd\u4e0a\u4e0b\u6587\u611f\u77e5\u63a7\u5236\uff0c\u9a8c\u8bc1\u4e86\u5728\u5de5\u4e1a\u5916\u9aa8\u9abc\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u80cc\u90e8\u808c\u8089\u6fc0\u6d3b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u7528\u6237\u7684\u504f\u597d\u548c\u8212\u9002\u5ea6\u3002"}}
{"id": "2508.06091", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06091", "abs": "https://arxiv.org/abs/2508.06091", "authors": ["Stan P Hauke", "Przemys\u0142aw Andrzej Wa\u0142\u0119ga"], "title": "Aggregate-Combine-Readout GNNs Are More Expressive Than Logic C2", "comment": "18 pages", "summary": "In recent years, there has been growing interest in understanding the\nexpressive power of graph neural networks (GNNs) by relating them to logical\nlanguages. This research has been been initialised by an influential result of\nBarcel\\'o et al. (2020), who showed that the graded modal logic (or a guarded\nfragment of the logic C2), characterises the logical expressiveness of\naggregate-combine GNNs. As a ``challenging open problem'' they left the\nquestion whether full C2 characterises the logical expressiveness of\naggregate-combine-readout GNNs. This question has remained unresolved despite\nseveral attempts. In this paper, we solve the above open problem by proving\nthat the logical expressiveness of aggregate-combine-readout GNNs strictly\nexceeds that of C2. This result holds over both undirected and directed graphs.\nBeyond its implications for GNNs, our work also leads to purely logical\ninsights on the expressive power of infinitary logics.", "AI": {"tldr": "\u805a\u5408-\u7ec4\u5408-\u8bfb\u51faGNNs\u7684\u903b\u8f91\u8868\u8fbe\u80fd\u529b\u8d85\u8fc7C2\u3002", "motivation": "\u5df2\u6709\u7814\u7a76\u5c06GNNs\u7684\u8868\u8fbe\u80fd\u529b\u4e0e\u903b\u8f91\u8bed\u8a00\u8054\u7cfb\u8d77\u6765\uff0cBarcel\u00f3\u7b49\u4eba\u7684\u7814\u7a76\u8868\u660e\uff0c\u5e26\u6743\u6a21\u6001\u903b\u8f91\uff08\u6216C2\u903b\u8f91\u7684\u4fdd\u62a4\u7247\u6bb5\uff09\u53ef\u4ee5\u8868\u5f81\u805a\u5408-\u7ec4\u5408GNNs\u7684\u903b\u8f91\u8868\u8fbe\u80fd\u529b\u3002\u4f46\u805a\u5408-\u7ec4\u5408-\u8bfb\u51faGNNs\u7684\u8868\u8fbe\u80fd\u529b\u662f\u5426\u4e0eC2\u76f8\u540c\uff0c\u4ecd\u7136\u662f\u4e00\u4e2a\u60ac\u800c\u672a\u51b3\u7684\u6311\u6218\u6027\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u8bc1\u660e\u805a\u5408-\u7ec4\u5408-\u8bfb\u51faGNNs\u7684\u903b\u8f91\u8868\u8fbe\u80fd\u529b\u4e25\u683c\u8d85\u8fc7\u4e86C2\u3002", "result": "\u805a\u5408-\u7ec4\u5408-\u8bfb\u51faGNNs\u7684\u903b\u8f91\u8868\u8fbe\u80fd\u529b\u4e25\u683c\u8d85\u8fc7\u4e86C2\uff0c\u8be5\u7ed3\u679c\u540c\u65f6\u9002\u7528\u4e8e\u65e0\u5411\u56fe\u548c\u6709\u5411\u56fe\u3002\u6b64\u5916\uff0c\u8be5\u7814\u7a76\u8fd8\u4e3a\u65e0\u9650\u903b\u8f91\u7684\u8868\u8fbe\u80fd\u529b\u63d0\u4f9b\u4e86\u7eaf\u7cb9\u7684\u903b\u8f91\u89c1\u89e3\u3002", "conclusion": "\u8be5\u7814\u7a76\u89e3\u51b3\u4e86\u56fe\u795e\u7ecf\u7f51\u7edc(GNNs)\u7684\u8868\u8fbe\u80fd\u529b\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u805a\u5408-\u7ec4\u5408-\u8bfb\u51faGNNs\u7684\u903b\u8f91\u8868\u8fbe\u80fd\u529b\u8d85\u8fc7\u4e86C2\u903b\u8f91\u3002"}}
{"id": "2508.05960", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05960", "abs": "https://arxiv.org/abs/2508.05960", "authors": ["Haohui Chen", "Zhiyong Chen"], "title": "Mildly Conservative Regularized Evaluation for Offline Reinforcement Learning", "comment": null, "summary": "Offline reinforcement learning (RL) seeks to learn optimal policies from\nstatic datasets without further environment interaction. A key challenge is the\ndistribution shift between the learned and behavior policies, leading to\nout-of-distribution (OOD) actions and overestimation. To prevent gross\noverestimation, the value function must remain conservative; however, excessive\nconservatism may hinder performance improvement. To address this, we propose\nthe mildly conservative regularized evaluation (MCRE) framework, which balances\nconservatism and performance by combining temporal difference (TD) error with a\nbehavior cloning term in the Bellman backup. Building on this, we develop the\nmildly conservative regularized Q-learning (MCRQ) algorithm, which integrates\nMCRE into an off-policy actor-critic framework. Experiments show that MCRQ\noutperforms strong baselines and state-of-the-art offline RL algorithms on\nbenchmark datasets.", "AI": {"tldr": "Offline RL methods struggle with distribution shift, leading to OOD actions and overestimation. This paper introduces the MCRE framework and MCRQ algorithm to balance conservatism and performance by incorporating behavior cloning into the Bellman backup, showing improved results over existing methods.", "motivation": "Offline RL faces challenges due to distribution shift between learned and behavior policies, causing out-of-distribution (OOD) actions and overestimation. While conservatism in the value function can prevent overestimation, excessive conservatism may limit performance. MCRE aims to balance conservatism and performance.", "method": "The study proposes the mildly conservative regularized evaluation (MCRE) framework, which integrates temporal difference (TD) error with a behavior cloning term in the Bellman backup. This framework is then used to develop the mildly conservative regularized Q-learning (MCRQ) algorithm, an off-policy actor-critic algorithm.", "result": "Experiments demonstrate that MCRQ outperforms strong baselines and state-of-the-art offline RL algorithms on benchmark datasets.", "conclusion": "MCRQ's performance surpasses strong baselines and state-of-the-art offline RL algorithms on benchmark datasets."}}
{"id": "2508.06436", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2508.06436", "abs": "https://arxiv.org/abs/2508.06436", "authors": ["Reshma Devi", "Keith T. Butler", "Gopalakrishnan Sai Gautam"], "title": "Leveraging transfer learning for accurate estimation of ionic migration barriers in solids", "comment": null, "summary": "Ionic mobility determines the rate performance of several applications, such\nas batteries, fuel cells, and electrochemical sensors and is exponentially\ndependent on the migration barrier ($E_m$), a difficult to measure/calculate\nquantity. Previous approaches to identify materials with high ionic mobility\nhave relied on imprecise descriptors given the lack of generalizable models to\npredict $E_m$. Here, we present a graph neural network based architecture that\nleverages principles of transfer learning to efficiently and accurately predict\n$E_m$ across a diverse set of materials. We use a model pre-trained\nsimultaneously on seven distinct bulk properties (labeled MPT), modify the MPT\nmodel to classify different migration pathways in a structure, and fine-tune\n(FT) on a manually-curated literature-derived dataset of 619 $E_m$ data points\ncalculated with density functional theory. Importantly, our best-performing FT\nmodel (labeled MODEL-3) demonstrates substantial improvements in prediction\naccuracy compared to classical machine learning methods, graph models trained\nfrom scratch, and a universal machine learned interatomic potential, with a\nR$^2$ score of 0.703 and a mean absolute error of 0.261 eV on the test set.\nNotably, MODEL-3 is able to distinguish different migration pathways within a\nstructure and also demonstrates excellent ability to generalize across\nintercalant compositions and chemistries. As a classifier, MODEL-3 exhibits\n80\\% accuracy and 82.8\\% precision in identifying materials that are `good'\nionic conductors (i.e., structures with $E_m <$0.65~eV). Thus, our work\ndemonstrates the effective use of FT strategies and architectural modifications\nnecessary for making swift and accurate $E_m$ predictions, which will be useful\nfor materials discovery in batteries and for predicting other data-scarce\nmaterial properties.", "AI": {"tldr": "\u672c\u6587\u4f7f\u7528\u4e00\u79cd\u6539\u8fdb\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\uff08\u8fc1\u79fb\u5b66\u4e60 + \u5206\u7c7b\uff09\uff0c\u80fd\u591f\u51c6\u786e\u9884\u6d4b\u6750\u6599\u7684\u79bb\u5b50\u8fc1\u79fb\u52bf\u5792 (Em)\uff0c\u5e76\u8bc6\u522b\u51fa\u826f\u597d\u7684\u79bb\u5b50\u5bfc\u4f53\uff0c\u4ece\u800c\u52a0\u901f\u7535\u6c60\u6750\u6599\u7684\u53d1\u73b0\u3002", "motivation": "\u79bb\u5b50\u8fc1\u79fb\u7387\u5bf9\u4e8e\u7535\u6c60\u3001\u71c3\u6599\u7535\u6c60\u548c\u7535\u5316\u5b66\u4f20\u611f\u5668\u7b49\u5e94\u7528\u7684\u500d\u7387\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u5b83\u968f\u8fc1\u79fb\u52bf\u5792 (Em) \u6307\u6570\u7ea7\u53d8\u5316\uff0c\u800c Em \u662f\u4e00\u4e2a\u96be\u4ee5\u6d4b\u91cf/\u8ba1\u7b97\u7684\u91cf\u3002\u5148\u524d\u8bc6\u522b\u9ad8\u79bb\u5b50\u8fc1\u79fb\u7387\u6750\u6599\u7684\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u4e0d\u7cbe\u786e\u7684\u63cf\u8ff0\u7b26\uff0c\u56e0\u4e3a\u7f3a\u4e4f\u53ef\u63a8\u5e7f\u7684 Em \u9884\u6d4b\u6a21\u578b\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u67b6\u6784\uff0c\u8be5\u67b6\u6784\u5229\u7528\u8fc1\u79fb\u5b66\u4e60\u539f\u7406\u6765\u9ad8\u6548\u3001\u51c6\u786e\u5730\u9884\u6d4b\u591a\u79cd\u6750\u6599\u7684 Em\u3002\u8be5\u6a21\u578b\u5728\u4e03\u79cd\u4e0d\u540c\u7684\u4f53\u6027\u8d28\u4e0a\u8fdb\u884c\u4e86\u9884\u8bad\u7ec3\uff08MPT\uff09\uff0c\u7136\u540e\u4fee\u6539\u4ee5\u5bf9\u7ed3\u6784\u4e2d\u7684\u4e0d\u540c\u8fc1\u79fb\u8def\u5f84\u8fdb\u884c\u5206\u7c7b\uff0c\u5e76\u5728\u4e00\u4e2a\u624b\u52a8\u6574\u7406\u7684\u3001\u6765\u81ea\u6587\u732e\u7684 619 \u4e2a Em \u6570\u636e\u70b9\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5fae\u8c03\uff08FT\uff09\u3002", "result": "\u4f5c\u8005\u63d0\u51fa\u7684 FT \u6a21\u578b\uff08MODEL-3\uff09\u5728\u9884\u6d4b\u7cbe\u5ea6\u65b9\u9762\u6bd4\u7ecf\u5178\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u3001\u4ece\u5934\u8bad\u7ec3\u7684\u56fe\u6a21\u578b\u548c\u901a\u7528\u7684\u673a\u5668\u5b66\u4e60\u539f\u5b50\u95f4\u52bf\u80fd\u6709\u4e86\u663e\u8457\u63d0\u9ad8\uff0c\u5728\u6d4b\u8bd5\u96c6\u4e0a\u7684 R\u00b2 \u8bc4\u5206\u4e3a 0.703\uff0c\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u4e3a 0.261 eV\u3002\u8be5\u6a21\u578b\u80fd\u591f\u533a\u5206\u7ed3\u6784\u4e2d\u4e0d\u540c\u7684\u8fc1\u79fb\u8def\u5f84\uff0c\u5e76\u4e14\u80fd\u591f\u8de8\u79bb\u5b50\u7ec4\u6210\u548c\u5316\u5b66\u6027\u8d28\u8fdb\u884c\u6cdb\u5316\u3002\u4f5c\u4e3a\u4e00\u4e2a\u5206\u7c7b\u5668\uff0cMODEL-3 \u5728\u8bc6\u522b\u201c\u826f\u597d\u201d\u79bb\u5b50\u5bfc\u4f53\uff08Em < 0.65 eV\uff09\u7684\u6750\u6599\u65f6\uff0c\u51c6\u786e\u7387\u4e3a 80%\uff0c\u7cbe\u786e\u7387\u4e3a 82.8%\u3002", "conclusion": "\u672c\u6587\u5c55\u793a\u4e86\u5728\u5feb\u901f\u51c6\u786e\u9884\u6d4b\u79bb\u5b50\u8fc1\u79fb\u52bf\u5792 (Em) \u65b9\u9762\uff0c\u8fc1\u79fb\u5b66\u4e60\u7b56\u7565\u548c\u6a21\u578b\u67b6\u6784\u4fee\u6539\u7684\u6709\u6548\u6027\uff0c\u8fd9\u5bf9\u4e8e\u7535\u6c60\u6750\u6599\u53d1\u73b0\u548c\u5176\u4ed6\u6570\u636e\u7a00\u758f\u6750\u6599\u6027\u8d28\u7684\u9884\u6d4b\u975e\u5e38\u6709\u7528\u3002"}}
{"id": "2508.05854", "categories": ["quant-ph", "math.OC"], "pdf": "https://arxiv.org/pdf/2508.05854", "abs": "https://arxiv.org/abs/2508.05854", "authors": ["Javier Pena", "Vikesh Siddhu", "Sridhar Tayur"], "title": "Tailored First-order and Interior-point methods and a new semidefinite programming hierarchy for entanglement detection", "comment": "55 pages, 5 figures", "summary": "Quantum entanglement lies at the heart of quantum information science, yet\nits reliable detection in high-dimensional or noisy systems remains a\nfundamental computational challenge. Semidefinite programming (SDP)\nhierarchies, such as the Doherty-Parrilo-Spedalieri (DPS) and Extension (EXT)\nhierarchies, offer complete methods for entanglement detection, but their\npractical use is limited by exponential growth in problem size. In this paper,\nwe introduce a new SDP hierarchy, PST, that is sandwiched between EXT and\nDPS--offering a tighter approximation to the set of separable states than EXT,\nwhile incurring lower computational overhead than DPS.\n  We develop compact, polynomially-scalable descriptions of EXT and PST using\npartition mappings and operators. These descriptions in turn yield formulations\nthat satisfy desirable properties such as the Slater condition and are\nwell-suited to both first-order methods (FOMs) and interior-point methods\n(IPMs). We design a suite of entanglement detection algorithms: three FOMs\n(Frank-Wolfe, projected gradient, and fast projected gradient) based on a\nleast-squares formulation, and a custom primal-dual IPM based on a conic\nprogramming formulation. These methods are numerically stable and capable of\nproducing entanglement witnesses or proximity measures, even in cases where\nstates lie near the boundary of separability.\n  Numerical experiments on benchmark quantum states demonstrate that our\nalgorithms improve the ability to solve deeper levels of the SDP hierarchy. In\nparticular, the PST hierarchy combined with FOMs enables scalable and effective\nentanglement detection in relatively easy instances, while our IPM approach\noffers robustness and early witness recovery for the more difficult ones. Our\nresults highlight the benefits of tailoring algorithmic formulations to\nhierarchy structure to advance entanglement detection at scale.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684SDP\u5c42\u7ea7PST\uff0c\u5e76\u5f00\u53d1\u4e86\u76f8\u5e94\u7684\u7b97\u6cd5\uff0c\u4ee5\u66f4\u6709\u6548\u3001\u53ef\u6269\u5c55\u7684\u65b9\u5f0f\u68c0\u6d4b\u91cf\u5b50\u7ea0\u7f20\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u8ba1\u7b97\u9650\u5236\u3002", "motivation": "\u91cf\u5b50\u7ea0\u7f20\u5728\u91cf\u5b50\u4fe1\u606f\u79d1\u5b66\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5728\u9ad8\u7ef4\u6216\u566a\u58f0\u7cfb\u7edf\u4e2d\u53ef\u9760\u5730\u68c0\u6d4b\u5b83\u4ecd\u7136\u662f\u4e00\u4e2a\u57fa\u672c\u7684\u8ba1\u7b97\u6311\u6218\u3002\u73b0\u6709\u7684SDP\u5c42\u7ea7\uff08\u5982DPS\u548cEXT\uff09\u867d\u7136\u63d0\u4f9b\u4e86\u5b8c\u6574\u7684\u68c0\u6d4b\u65b9\u6cd5\uff0c\u4f46\u7531\u4e8e\u95ee\u9898\u89c4\u6a21\u7684\u6307\u6570\u589e\u957f\u800c\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u901a\u8fc7\u7d27\u51d1\u7684\u591a\u9879\u5f0f\u53ef\u6269\u5c55\u63cf\u8ff0\uff08\u5229\u7528\u5206\u533a\u6620\u5c04\u548c\u7b97\u5b50\uff09\u6765\u6784\u5efaEXT\u548cPST\u5c42\u7ea7\uff0c\u5e76\u57fa\u4e8e\u8fd9\u4e9b\u63cf\u8ff0\u5f00\u53d1\u4e86\u591a\u79cd\u7b97\u6cd5\uff0c\u5305\u62ec\u57fa\u4e8e\u6700\u5c0f\u4e8c\u4e58\u6cd5\u7684\u4e09\u4e2a\u4e00\u9636\u65b9\u6cd5\uff08Frank-Wolfe\u3001\u6295\u5f71\u68af\u5ea6\u548c\u5feb\u901f\u6295\u5f71\u68af\u5ea6\uff09\u4ee5\u53ca\u4e00\u4e2a\u57fa\u4e8e\u4e8c\u6b21\u9525\u89c4\u5212\u7684\u81ea\u5b9a\u4e49\u539f\u59cb\u5bf9\u5076\u5185\u70b9\u6cd5\u3002", "result": "\u6240\u63d0\u51fa\u7684PST\u5c42\u7ea7\u548c\u7b97\u6cd5\u5728\u6570\u503c\u5b9e\u9a8c\u4e2d\u80fd\u591f\u66f4\u6709\u6548\u5730\u89e3\u51b3\u66f4\u6df1\u5c42\u6b21\u7684SDP\u95ee\u9898\u3002\u5177\u4f53\u6765\u8bf4\uff0cPST\u5c42\u7ea7\u7ed3\u5408\u4e00\u9636\u65b9\u6cd5\u80fd\u6709\u6548\u5904\u7406\u76f8\u5bf9\u5bb9\u6613\u7684\u5b9e\u4f8b\uff0c\u800c\u5185\u70b9\u6cd5\u5728\u5904\u7406\u66f4\u56f0\u96be\u7684\u5b9e\u4f8b\u65f6\u5219\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u5e76\u80fd\u8fdb\u884c\u65e9\u671f\u5bf9\u5076\u6062\u590d\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6839\u636e\u5c42\u7ea7\u7ed3\u6784\u5b9a\u5236\u7b97\u6cd5\u914d\u65b9\u80fd\u591f\u4fc3\u8fdb\u5927\u89c4\u6a21\u7ea0\u7f20\u68c0\u6d4b\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684SDP\u5c42\u7ea7PST\uff0c\u5b83\u5728EXT\u548cDPS\u4e4b\u95f4\uff0c\u4e3a\u53ef\u5206\u79bb\u6001\u63d0\u4f9b\u6bd4EXT\u66f4\u7cbe\u786e\u7684\u903c\u8fd1\uff0c\u540c\u65f6\u6bd4DPS\u5177\u6709\u66f4\u4f4e\u7684\u8ba1\u7b97\u5f00\u9500\u3002\u7814\u7a76\u8fd8\u5f00\u53d1\u4e86\u89e3\u51b3SDP\u5c42\u7ea7\u95ee\u9898\u7684\u7b97\u6cd5\uff0c\u5e76\u5728\u57fa\u51c6\u91cf\u5b50\u6001\u7684\u6570\u503c\u5b9e\u9a8c\u4e2d\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2508.06046", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06046", "abs": "https://arxiv.org/abs/2508.06046", "authors": ["Xinda Wang", "Zhengxu Hou", "Yangshijie Zhang", "Bingren Yan", "Zhibo Yang", "Xingsheng Zhang", "Luxi Xing", "Qiang Zhou", "Chen Zhang"], "title": "EvolvR: Self-Evolving Pairwise Reasoning for Story Evaluation to Enhance Generation", "comment": null, "summary": "Although the effectiveness of Large Language Models (LLMs) as judges\n(LLM-as-a-judge) has been validated, their performance remains limited in\nopen-ended tasks, particularly in story evaluation. Accurate story evaluation\nis crucial not only for assisting human quality judgment but also for providing\nkey signals to guide story generation. However, existing methods face a\ndilemma: prompt engineering for closed-source models suffers from poor\nadaptability, while fine-tuning approaches for open-source models lack the\nrigorous reasoning capabilities essential for story evaluation. To address\nthis, we propose the Self-Evolving Pairwise Reasoning (EvolvR) framework.\nGrounded in pairwise comparison, the framework first self-synthesizes\nscore-aligned Chain-of-Thought (CoT) data via a multi-persona strategy. To\nensure data quality, these raw CoTs undergo a self-filtering process, utilizing\nmulti-agents to guarantee their logical rigor and robustness. Finally, the\nevaluator trained on the refined data is deployed as a reward model to guide\nthe story generation task. Experimental results demonstrate that our framework\nachieves state-of-the-art (SOTA) performance on three evaluation benchmarks\nincluding StoryER, HANNA and OpenMEVA. Furthermore, when served as a reward\nmodel, it significantly enhances the quality of generated stories, thereby\nfully validating the superiority of our self-evolving approach.", "AI": {"tldr": "EvolvR\u6846\u67b6\u901a\u8fc7\u81ea\u5408\u6210\u548c\u81ea\u8fc7\u6ee4CoT\u6570\u636e\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6545\u4e8b\u8bc4\u4f30\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97SOTA\u6027\u80fd\uff0c\u540c\u65f6\u63d0\u5347\u4e86\u6545\u4e8b\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u6545\u4e8b\u8bc4\u4f30\u65b9\u9762\u5b58\u5728\u56f0\u5883\uff1a\u95ed\u6e90\u6a21\u578b\u7684\u63d0\u793a\u5de5\u7a0b\u9002\u5e94\u6027\u5dee\uff0c\u800c\u5f00\u6e90\u6a21\u578b\u7684\u5fae\u8c03\u65b9\u6cd5\u7f3a\u4e4f\u4e25\u683c\u7684\u63a8\u7406\u80fd\u529b\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u9700\u8981\u4e00\u4e2a\u66f4\u4f18\u7684\u65b9\u6cd5\u6765\u8f85\u52a9\u4eba\u7c7b\u8fdb\u884c\u8d28\u91cf\u5224\u65ad\u5e76\u6307\u5bfc\u6545\u4e8b\u751f\u6210\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEvolvR\u7684\u81ea\u8fdb\u5316\u914d\u5bf9\u63a8\u7406\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u591a\u91cd\u8eab\u4efd\u7b56\u7565\u81ea\u5408\u6210\u4e0e\u5206\u6570\u5bf9\u9f50\u7684\u601d\u7ef4\u94fe\uff08CoT\uff09\u6570\u636e\uff0c\u5e76\u5229\u7528\u591a\u667a\u80fd\u4f53\u8fdb\u884c\u81ea\u8fc7\u6ee4\u4ee5\u4fdd\u8bc1CoT\u6570\u636e\u7684\u903b\u8f91\u4e25\u8c28\u6027\u548c\u9c81\u68d2\u6027\uff0c\u6700\u540e\u5c06\u8bad\u7ec3\u597d\u7684\u8bc4\u4f30\u5668\u4f5c\u4e3a\u5956\u52b1\u6a21\u578b\u6765\u6307\u5bfc\u6545\u4e8b\u751f\u6210\u4efb\u52a1\u3002", "result": "EvolvR\u6846\u67b6\u5728StoryER\u3001HANNA\u548cOpenMEVA\u4e09\u4e2a\u8bc4\u4f30\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u6b64\u5916\uff0c\u4f5c\u4e3a\u5956\u52b1\u6a21\u578b\u4f7f\u7528\u65f6\uff0cEvolvR\u663e\u8457\u63d0\u9ad8\u4e86\u751f\u6210\u6545\u4e8b\u7684\u8d28\u91cf\u3002", "conclusion": "EvolvR\u6846\u67b6\u5728\u4e09\u4e2a\u8bc4\u4f30\u57fa\u51c6\uff08StoryER\u3001HANNA\u548cOpenMEVA\uff09\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u4f5c\u4e3a\u5956\u52b1\u6a21\u578b\u663e\u8457\u63d0\u9ad8\u4e86\u751f\u6210\u6545\u4e8b\u7684\u8d28\u91cf\uff0c\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002"}}
{"id": "2508.06229", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06229", "abs": "https://arxiv.org/abs/2508.06229", "authors": ["Zihao Xu", "Ce Hao", "Chunzheng Wang", "Kuankuan Sima", "Fan Shi", "Jin Song Dong"], "title": "REBot: Reflexive Evasion Robot for Instantaneous Dynamic Obstacle Avoidance", "comment": null, "summary": "Dynamic obstacle avoidance (DOA) is critical for quadrupedal robots operating\nin environments with moving obstacles or humans. Existing approaches typically\nrely on navigation-based trajectory replanning, which assumes sufficient\nreaction time and leading to fails when obstacles approach rapidly. In such\nscenarios, quadrupedal robots require reflexive evasion capabilities to perform\ninstantaneous, low-latency maneuvers. This paper introduces Reflexive Evasion\nRobot (REBot), a control framework that enables quadrupedal robots to achieve\nreal-time reflexive obstacle avoidance. REBot integrates an avoidance policy\nand a recovery policy within a finite-state machine. With carefully designed\nlearning curricula and by incorporating regularization and adaptive rewards,\nREBot achieves robust evasion and rapid stabilization in instantaneous DOA\ntasks. We validate REBot through extensive simulations and real-world\nexperiments, demonstrating notable improvements in avoidance success rates,\nenergy efficiency, and robustness to fast-moving obstacles. Videos and appendix\nare available on https://rebot-2025.github.io/.", "AI": {"tldr": "REBot\u662f\u4e00\u4e2a\u7528\u4e8e\u56db\u8db3\u673a\u5668\u4eba\u7684\u63a7\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u89c4\u907f\u548c\u6062\u590d\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u5bf9\u5feb\u901f\u79fb\u52a8\u969c\u788d\u7269\u7684\u5b9e\u65f6\u53cd\u5c04\u5f0f\u89c4\u907f\uff0c\u5e76\u5728\u6a21\u62df\u548c\u73b0\u5b9e\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u57fa\u4e8e\u5bfc\u822a\u7684\u8f68\u8ff9\u91cd\u65b0\u89c4\u5212\u65b9\u6cd5\u5728\u9762\u5bf9\u5feb\u901f\u63a5\u8fd1\u7684\u969c\u788d\u7269\u65f6\u5931\u6548\u7684\u95ee\u9898\uff0c\u9700\u8981\u56db\u8db3\u673a\u5668\u4eba\u5177\u5907\u5373\u65f6\u3001\u4f4e\u5ef6\u8fdf\u7684\u53cd\u5e94\u80fd\u529b\u3002\u56e0\u6b64\uff0c\u8be5\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u5b9e\u73b0\u5b9e\u65f6\u53cd\u5c04\u5f0f\u969c\u788d\u7269\u89c4\u907f\u7684\u63a7\u5236\u6846\u67b6\u3002", "method": "REBot\u6846\u67b6\u6574\u5408\u4e86\u4e00\u4e2a\u89c4\u907f\u7b56\u7565\u548c\u4e00\u4e2a\u6062\u590d\u7b56\u7565\uff0c\u5e76\u901a\u8fc7\u6709\u9650\u72b6\u6001\u673a\u8fdb\u884c\u7ba1\u7406\u3002\u8be5\u6846\u67b6\u91c7\u7528\u4e86\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5b66\u4e60\u8bfe\u7a0b\uff0c\u5e76\u7ed3\u5408\u4e86\u6b63\u5219\u5316\u548c\u81ea\u9002\u5e94\u5956\u52b1\u673a\u5236\uff0c\u4ee5\u5b9e\u73b0\u9c81\u68d2\u7684\u89c4\u907f\u548c\u5feb\u901f\u7684\u7a33\u5b9a\u6027\u3002", "result": "\u901a\u8fc7\u5e7f\u6cdb\u7684\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u9a8c\u8bc1\uff0cREBot\u6846\u67b6\u5728\u89c4\u907f\u6210\u529f\u7387\u3001\u80fd\u91cf\u6548\u7387\u548c\u9c81\u68d2\u6027\u65b9\u9762\u5747\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6539\u8fdb\u3002", "conclusion": "REBot\u6846\u67b6\u5728\u52a8\u6001\u969c\u788d\u7269\u89c4\u907f\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u7684\u4f18\u52bf\uff0c\u5305\u62ec\u66f4\u9ad8\u7684\u89c4\u907f\u6210\u529f\u7387\u3001\u80fd\u91cf\u6548\u7387\u548c\u5bf9\u5feb\u901f\u79fb\u52a8\u969c\u788d\u7269\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.05977", "categories": ["cs.LG", "physics.flu-dyn"], "pdf": "https://arxiv.org/pdf/2508.05977", "abs": "https://arxiv.org/abs/2508.05977", "authors": ["Aoming Liang", "Chi Cheng", "Dashuai Chen", "Boai Sun", "Dixia Fan"], "title": "LinguaFluid: Language Guided Fluid Control via Semantic Rewards in Reinforcement Learning", "comment": null, "summary": "In the domain of scientific machine learning, designing effective reward\nfunctions remains a challenge in reinforcement learning (RL), particularly in\nenvironments where task goals are difficult to specify numerically. Reward\nfunctions in existing work are predominantly based on heuristics, manual\nengineering, or task-specific tuning. In this work, we introduce a semantically\naligned reinforcement learning method where rewards are computed by aligning\nthe current state with a target semantic instruction using a\nSentence-Bidirectional Encoder Representations from Transformers (SBERT).\nInstead of relying on manually defined reward functions, the policy receives\nfeedback based on the reward, which is a cosine similarity between the goal\ntextual description and the statement description in the episode. We evaluated\nour approach in several environments and showed that semantic reward can guide\nlearning to achieve competitive control behavior, even in the absence of\nhand-crafted reward functions. Our study demonstrates a correlation between the\nlanguage embedding space and the conventional Euclidean space. This framework\nopens new horizons for aligning agent behavior with natural language goals and\nlays the groundwork for a more seamless integration of larger language models\n(LLMs) and fluid control applications.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u4f7f\u7528SBERT\u5c06\u72b6\u6001\u4e0e\u6587\u672c\u76ee\u6807\u5bf9\u9f50\u6765\u8ba1\u7b97\u5956\u52b1\uff0c\u65e0\u9700\u624b\u52a8\u8bbe\u8ba1\u5956\u52b1\u51fd\u6570\uff0c\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u6709\u7ade\u4e89\u529b\u7684\u63a7\u5236\u884c\u4e3a\u3002", "motivation": "\u5728\u79d1\u5b66\u673a\u5668\u5b66\u4e60\u9886\u57df\uff0c\u8bbe\u8ba1\u6709\u6548\u7684\u5956\u52b1\u51fd\u6570\u4ecd\u7136\u662f\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4e2d\u7684\u4e00\u4e2a\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u4efb\u52a1\u76ee\u6807\u96be\u4ee5\u7528\u6570\u5b57\u6307\u5b9a\u7684\u73af\u5883\u4e2d\u3002\u73b0\u6709\u5de5\u4f5c\u4e2d\u7684\u5956\u52b1\u51fd\u6570\u4e3b\u8981\u57fa\u4e8e\u542f\u53d1\u5f0f\u65b9\u6cd5\u3001\u624b\u52a8\u5de5\u7a0b\u6216\u7279\u5b9a\u4efb\u52a1\u7684\u8c03\u6574\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bed\u4e49\u5bf9\u9f50\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f7f\u7528\u53e5\u5b50-\u53cc\u5411Encoder\u8868\u793a\u53d8\u6362\u5668(SBERT)\u5c06\u5f53\u524d\u72b6\u6001\u4e0e\u76ee\u6807\u8bed\u4e49\u6307\u4ee4\u5bf9\u9f50\u6765\u8ba1\u7b97\u5956\u52b1\u3002\u7b56\u7565\u63a5\u6536\u57fa\u4e8e\u5956\u52b1\u7684\u53cd\u9988\uff0c\u8be5\u5956\u52b1\u662f\u76ee\u6807\u6587\u672c\u63cf\u8ff0\u4e0e\u56de\u5408\u5185\u8bed\u53e5\u63cf\u8ff0\u4e4b\u95f4\u7684\u4f59\u5f26\u76f8\u4f3c\u5ea6\u3002", "result": "\u5728\u51e0\u4e2a\u73af\u5883\u4e2d\u8bc4\u4f30\u4e86\u8be5\u65b9\u6cd5\uff0c\u7ed3\u679c\u8868\u660e\uff0c\u5373\u4f7f\u6ca1\u6709\u624b\u5de5\u5236\u4f5c\u7684\u5956\u52b1\u51fd\u6570\uff0c\u8bed\u4e49\u5956\u52b1\u4e5f\u80fd\u6307\u5bfc\u5b66\u4e60\u4ee5\u5b9e\u73b0\u6709\u7ade\u4e89\u529b\u7684\u63a7\u5236\u884c\u4e3a\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c06\u8bed\u8a00\u5d4c\u5165\u7a7a\u95f4\u4e0e\u4f20\u7edf\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\u76f8\u5173\u8054\uff0c\u4e3a\u667a\u80fd\u4f53\u884c\u4e3a\u4e0e\u81ea\u7136\u8bed\u8a00\u76ee\u6807\u5bf9\u9f50\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\uff0c\u5e76\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u6d41\u4f53\u63a7\u5236\u5e94\u7528\u7684\u65e0\u7f1d\u96c6\u6210\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.06456", "categories": ["cond-mat.mtrl-sci", "physics.data-an"], "pdf": "https://arxiv.org/pdf/2508.06456", "abs": "https://arxiv.org/abs/2508.06456", "authors": ["Yonatan Kurniawan", "Mingjian Wen", "Ellad B. Tadmor", "Mark K. Transtrum"], "title": "Comparative study of ensemble-based uncertainty quantification methods for neural network interatomic potentials", "comment": null, "summary": "Machine learning interatomic potentials (MLIPs) enable atomistic simulations\nwith near first-principles accuracy at substantially reduced computational\ncost, making them powerful tools for large-scale materials modeling. The\naccuracy of MLIPs is typically validated on a held-out dataset of \\emph{ab\ninitio} energies and atomic forces. However, accuracy on these small-scale\nproperties does not guarantee reliability for emergent, system-level behavior\n-- precisely the regime where atomistic simulations are most needed, but for\nwhich direct validation is often computationally prohibitive. As a practical\nheuristic, predictive precision -- quantified as inverse uncertainty -- is\ncommonly used as a proxy for accuracy, but its reliability remains poorly\nunderstood, particularly for system-level predictions. In this work, we\nsystematically assess the relationship between predictive precision and\naccuracy in both in-distribution (ID) and out-of-distribution (OOD) regimes,\nfocusing on ensemble-based uncertainty quantification methods for neural\nnetwork potentials, including bootstrap, dropout, random initialization, and\nsnapshot ensembles. We use held-out cross-validation for ID assessment and\ncalculate cold curve energies and phonon dispersion relations for OOD testing.\nThese evaluations are performed across various carbon allotropes as\nrepresentative test systems. We find that uncertainty estimates can behave\ncounterintuitively in OOD settings, often plateauing or even decreasing as\npredictive errors grow. These results highlight fundamental limitations of\ncurrent uncertainty quantification approaches and underscore the need for\ncaution when using predictive precision as a stand-in for accuracy in\nlarge-scale, extrapolative applications.", "AI": {"tldr": "MLIPs \u7684\u9884\u6d4b\u7cbe\u5ea6\u4e0d\u4e00\u5b9a\u80fd\u53cd\u6620\u5176\u51c6\u786e\u6027\uff0c\u5c24\u5176\u662f\u5728\u5206\u5e03\u5916\uff08OOD\uff09\u60c5\u51b5\u4e0b\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u5728 OOD \u8bbe\u7f6e\u4e0b\u53ef\u80fd\u4e0e\u9884\u6d4b\u8bef\u5dee\u65e0\u5173\uff0c\u751a\u81f3\u5448\u8d1f\u76f8\u5173\u3002\u8fd9\u8868\u660e\u5728\u4f9d\u8d56 MLIPs \u8fdb\u884c\u5927\u89c4\u6a21\u6a21\u62df\u65f6\uff0c\u9700\u8981\u8c28\u614e\u4f7f\u7528\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u5185\u539f\u5b50\u52bf\uff08MLIPs\uff09\u80fd\u591f\u5728\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\uff0c\u4ee5\u63a5\u8fd1\u7b2c\u4e00\u6027\u539f\u7406\u7684\u7cbe\u5ea6\u8fdb\u884c\u539f\u5b50\u5c3a\u5ea6\u6a21\u62df\uff0c\u56e0\u6b64\u6210\u4e3a\u5927\u89c4\u6a21\u6750\u6599\u5efa\u6a21\u7684\u6709\u529b\u5de5\u5177\u3002\u7136\u800c\uff0cMLIPs \u7684\u51c6\u786e\u6027\u901a\u5e38\u5728\u4ece\u4ece\u5934\u7b97\u80fd\u91cf\u548c\u539f\u5b50\u529b\u4e2d\u4fdd\u7559\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u9a8c\u8bc1\u3002\u4f46\u8fd9\u79cd\u5728\u5c0f\u5c3a\u5ea6\u6027\u8d28\u4e0a\u7684\u51c6\u786e\u6027\u5e76\u4e0d\u80fd\u4fdd\u8bc1\u5728\u6d8c\u73b0\u7684\u3001\u7cfb\u7edf\u5c42\u9762\u7684\u884c\u4e3a\u4e0a\u7684\u53ef\u9760\u6027\u2014\u2014\u8fd9\u6070\u6070\u662f\u539f\u5b50\u5c3a\u5ea6\u6a21\u62df\u6700\u9700\u8981\u7684\uff0c\u4f46\u901a\u5e38\u7531\u4e8e\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\u800c\u65e0\u6cd5\u76f4\u63a5\u9a8c\u8bc1\u7684\u9886\u57df\u3002\u4f5c\u4e3a\u4e00\u79cd\u5b9e\u7528\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u9884\u6d4b\u7cbe\u5ea6\uff08\u91cf\u5316\u4e3a\u4e0d\u786e\u5b9a\u6027\u7684\u5012\u6570\uff09\u901a\u5e38\u88ab\u7528\u4f5c\u51c6\u786e\u6027\u7684\u4ee3\u7406\uff0c\u4f46\u5176\u53ef\u9760\u6027\u4ecd\u7136\u77e5\u4e4b\u751a\u5c11\uff0c\u7279\u522b\u662f\u5728\u7cfb\u7edf\u5c42\u9762\u7684\u9884\u6d4b\u4e2d\u3002", "method": "\u7814\u7a76\u4eba\u5458\u7cfb\u7edf\u5730\u8bc4\u4f30\u4e86\u9884\u6d4b\u7cbe\u5ea6\u4e0e\u51c6\u786e\u6027\u5728\u5206\u5e03\u5185\uff08ID\uff09\u548c\u5206\u5e03\u5916\uff08OOD\uff09\u4e24\u79cd\u60c5\u51b5\u4e0b\u7684\u5173\u7cfb\uff0c\u91cd\u70b9\u5173\u6ce8\u57fa\u4e8e\u96c6\u5408\u7684\u795e\u7ecf\u7f51\u7edc\u52bf\u80fd\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\uff0c\u5305\u62ec bootstrap\u3001dropout\u3001\u968f\u673a\u521d\u59cb\u5316\u548c\u5feb\u7167\u96c6\u5408\u3002\u7814\u7a76\u4eba\u5458\u4f7f\u7528\u7559\u51fa\u4ea4\u53c9\u9a8c\u8bc1\u8fdb\u884c ID \u8bc4\u4f30\uff0c\u5e76\u8ba1\u7b97\u51b7\u66f2\u7ebf\u80fd\u91cf\u548c\u58f0\u5b50\u8272\u6563\u5173\u7cfb\u8fdb\u884c OOD \u6d4b\u8bd5\u3002\u8fd9\u4e9b\u8bc4\u4f30\u5728\u5404\u79cd\u78b3\u540c\u7d20\u5f02\u5f62\u4f53\u4e0a\u8fdb\u884c\uff0c\u4f5c\u4e3a\u4ee3\u8868\u6027\u6d4b\u8bd5\u7cfb\u7edf\u3002", "result": "\u5728\u5206\u5e03\u5916\uff08OOD\uff09\u8bbe\u7f6e\u4e2d\uff0c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u53ef\u80fd\u8868\u73b0\u51fa\u8fdd\u53cd\u76f4\u89c9\u7684\u884c\u4e3a\uff0c\u968f\u7740\u9884\u6d4b\u8bef\u5dee\u7684\u589e\u52a0\uff0c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u5f80\u5f80\u4f1a\u8d8b\u4e8e\u5e73\u7a33\u751a\u81f3\u4e0b\u964d\u3002\u8fd9\u63ed\u793a\u4e86\u5f53\u524d\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\u7684\u6839\u672c\u5c40\u9650\u6027\uff0c\u5e76\u5f3a\u8c03\u4e86\u5728\u5927\u578b\u3001\u5916\u63a8\u5e94\u7528\u4e2d\u4f7f\u7528\u9884\u6d4b\u7cbe\u5ea6\u4f5c\u4e3a\u51c6\u786e\u6027\u66ff\u4ee3\u6307\u6807\u65f6\u7684\u6ce8\u610f\u4e8b\u9879\u3002", "conclusion": "\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u5728 OOD \u8bbe\u7f6e\u4e2d\u53ef\u80fd\u8868\u73b0\u51fa\u8fdd\u53cd\u76f4\u89c9\u7684\u884c\u4e3a\uff0c\u968f\u7740\u9884\u6d4b\u8bef\u5dee\u7684\u589e\u957f\uff0c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u5f80\u5f80\u4f1a\u8d8b\u4e8e\u5e73\u7a33\u751a\u81f3\u4e0b\u964d\u3002\u8fd9\u7a81\u663e\u4e86\u5f53\u524d\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\u7684\u6839\u672c\u5c40\u9650\u6027\uff0c\u5e76\u5f3a\u8c03\u5728\u5927\u578b\u3001\u5916\u63a8\u5e94\u7528\u4e2d\u4f7f\u7528\u9884\u6d4b\u7cbe\u5ea6\u4f5c\u4e3a\u51c6\u786e\u6027\u7684\u66ff\u4ee3\u6307\u6807\u65f6\u9700\u8981\u8c28\u614e\u3002"}}
{"id": "2508.06094", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06094", "abs": "https://arxiv.org/abs/2508.06094", "authors": ["Morris Alper", "Moran Yanuka", "Raja Giryes", "Ga\u0161per Begu\u0161"], "title": "ConlangCrafter: Constructing Languages with a Multi-Hop LLM Pipeline", "comment": "Project page: https://conlangcrafter.github.io", "summary": "Constructed languages (conlangs) such as Esperanto and Quenya have played\ndiverse roles in art, philosophy, and international communication. Meanwhile,\nlarge-scale foundation models have revolutionized creative generation in text,\nimages, and beyond. In this work, we leverage modern LLMs as computational\ncreativity aids for end-to-end conlang creation. We introduce ConlangCrafter, a\nmulti-hop pipeline that decomposes language design into modular stages --\nphonology, morphology, syntax, lexicon generation, and translation. At each\nstage, our method leverages LLMs' meta-linguistic reasoning capabilities,\ninjecting randomness to encourage diversity and leveraging self-refinement\nfeedback to encourage consistency in the emerging language description. We\nevaluate ConlangCrafter on metrics measuring coherence and typological\ndiversity, demonstrating its ability to produce coherent and varied conlangs\nwithout human linguistic expertise.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86ConlangCrafter\uff0c\u4e00\u4e2a\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u6765\u81ea\u52a8\u5316\u4eba\u9020\u8bed\u8a00\uff08conlangs\uff09\u521b\u5efa\u8fc7\u7a0b\u7684\u6846\u67b6\u3002\u8be5\u6846\u67b6\u5c06\u8bed\u8a00\u8bbe\u8ba1\u5206\u89e3\u4e3a\u591a\u4e2a\u9636\u6bb5\uff0c\u5982\u97f3\u7cfb\u3001\u5f62\u6001\u3001\u53e5\u6cd5\u548c\u8bcd\u6c47\u751f\u6210\uff0c\u5e76\u5229\u7528LLMs\u7684\u63a8\u7406\u80fd\u529b\u548c\u81ea\u6211\u5b8c\u5584\u673a\u5236\u6765\u786e\u4fdd\u751f\u6210\u8bed\u8a00\u7684\u4e00\u81f4\u6027\u548c\u591a\u6837\u6027\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u65e0\u9700\u4eba\u7c7b\u8bed\u8a00\u5b66\u4e13\u4e1a\u77e5\u8bc6\u5373\u53ef\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u8bed\u8a00\u3002", "motivation": "\u5728\u4eba\u9020\u8bed\u8a00\uff08conlangs\uff09\u5982\u4e16\u754c\u8bed\u548c\u594e\u5c3c\u4e9a\u8bed\u5728\u827a\u672f\u3001\u54f2\u5b66\u548c\u56fd\u9645\u4ea4\u6d41\u4e2d\u626e\u6f14\u591a\u6837\u5316\u89d2\u8272\u7684\u80cc\u666f\u4e0b\uff0c\u4ee5\u53ca\u5927\u578b\u57fa\u7840\u6a21\u578b\u5728\u6587\u672c\u3001\u56fe\u50cf\u7b49\u9886\u57df\u7684\u521b\u9020\u6027\u751f\u6210\u65b9\u9762\u5e26\u6765\u9769\u547d\u6027\u53d8\u5316\u7684\u80cc\u666f\u4e0b\uff0c\u672c\u5de5\u4f5c\u65e8\u5728\u5229\u7528LLMs\u4f5c\u4e3a\u521b\u9020\u6027\u8f85\u52a9\u5de5\u5177\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u7684\u8bed\u8a00\u521b\u9020\u3002", "method": "\u8be5\u5de5\u4f5c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4f5c\u4e3a\u8ba1\u7b97\u521b\u9020\u529b\u8f85\u52a9\u5de5\u5177\uff0c\u7528\u4e8e\u7aef\u5230\u7aef\u7684\u8bed\u8a00\u521b\u9020\u3002\u5de5\u4f5c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aConlangCrafter\u7684\u591a\u8df3\u7ba1\u7ebf\uff0c\u5c06\u8bed\u8a00\u8bbe\u8ba1\u5206\u89e3\u4e3a\u97f3\u7cfb\u3001\u5f62\u6001\u3001\u53e5\u6cd5\u3001\u8bcd\u6c47\u751f\u6210\u548c\u7ffb\u8bd1\u7b49\u6a21\u5757\u5316\u9636\u6bb5\u3002\u5728\u6bcf\u4e2a\u9636\u6bb5\uff0c\u8be5\u65b9\u6cd5\u90fd\u5229\u7528LLMs\u7684\u5143\u8bed\u8a00\u63a8\u7406\u80fd\u529b\uff0c\u6ce8\u5165\u968f\u673a\u6027\u4ee5\u9f13\u52b1\u591a\u6837\u6027\uff0c\u5e76\u5229\u7528\u81ea\u6211\u5b8c\u5584\u53cd\u9988\u4ee5\u9f13\u52b1\u65b0\u5174\u8bed\u8a00\u63cf\u8ff0\u7684\u4e00\u81f4\u6027\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0cConlangCrafter\u5728\u8861\u91cf\u8fde\u8d2f\u6027\u548c\u7c7b\u578b\u5b66\u591a\u6837\u6027\u7684\u6307\u6807\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u80fd\u591f\u751f\u6210\u8fde\u8d2f\u4e14\u591a\u6837\u5316\u7684\u4eba\u9020\u8bed\u8a00\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u8fde\u8d2f\u4e14\u591a\u6837\u5316\u7684\u8bed\u8a00\uff0c\u800c\u65e0\u9700\u4eba\u7c7b\u7684\u8bed\u8a00\u5b66\u4e13\u4e1a\u77e5\u8bc6\u3002"}}
{"id": "2508.05903", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05903", "abs": "https://arxiv.org/abs/2508.05903", "authors": ["Lang Nie", "Yuan Mei", "Kang Liao", "Yunqiu Xu", "Chunyu Lin", "Bin Xiao"], "title": "Robust Image Stitching with Optimal Plane", "comment": "* Equal contribution", "summary": "We present \\textit{RopStitch}, an unsupervised deep image stitching framework\nwith both robustness and naturalness. To ensure the robustness of\n\\textit{RopStitch}, we propose to incorporate the universal prior of content\nperception into the image stitching model by a dual-branch architecture. It\nseparately captures coarse and fine features and integrates them to achieve\nhighly generalizable performance across diverse unseen real-world scenes.\nConcretely, the dual-branch model consists of a pretrained branch to capture\nsemantically invariant representations and a learnable branch to extract\nfine-grained discriminative features, which are then merged into a whole by a\ncontrollable factor at the correlation level. Besides, considering that content\nalignment and structural preservation are often contradictory to each other, we\npropose a concept of virtual optimal planes to relieve this conflict. To this\nend, we model this problem as a process of estimating homography decomposition\ncoefficients, and design an iterative coefficient predictor and minimal\nsemantic distortion constraint to identify the optimal plane. This scheme is\nfinally incorporated into \\textit{RopStitch} by warping both views onto the\noptimal plane bidirectionally. Extensive experiments across various datasets\ndemonstrate that \\textit{RopStitch} significantly outperforms existing methods,\nparticularly in scene robustness and content naturalness. The code is available\nat {\\color{red}https://github.com/MmelodYy/RopStitch}.", "AI": {"tldr": "RopStitch\u662f\u4e00\u4e2a\u65e0\u76d1\u7763\u7684\u6df1\u5ea6\u56fe\u50cf\u62fc\u63a5\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u5206\u652f\u67b6\u6784\u548c\u865a\u62df\u6700\u4f18\u5e73\u9762\u89e3\u51b3\u4e86\u9c81\u68d2\u6027\u548c\u81ea\u7136\u6027\u7684\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4e3a\u4e86\u5728\u56fe\u50cf\u62fc\u63a5\u4e2d\u5b9e\u73b0\u9c81\u68d2\u6027\u548c\u81ea\u7136\u6027\uff0c\u540c\u65f6\u89e3\u51b3\u5185\u5bb9\u5bf9\u9f50\u548c\u7ed3\u6784\u4fdd\u6301\u4e4b\u95f4\u7684\u51b2\u7a81\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRopStitch\u7684\u65e0\u76d1\u7763\u6df1\u5ea6\u56fe\u50cf\u62fc\u63a5\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u5185\u5bb9\u611f\u77e5\u901a\u7528\u5148\u9a8c\u7684\u53cc\u5206\u652f\u7ed3\u6784\uff08\u9884\u8bad\u7ec3\u5206\u652f\u63d0\u53d6\u8bed\u4e49\u4e0d\u53d8\u8868\u793a\uff0c\u53ef\u5b66\u4e60\u5206\u652f\u63d0\u53d6\u7ec6\u7c92\u5ea6\u5224\u522b\u6027\u7279\u5f81\uff09\uff0c\u5e76\u901a\u8fc7\u53ef\u63a7\u56e0\u5b50\u5728\u76f8\u5173\u5c42\u9762\u8fdb\u884c\u878d\u5408\uff0c\u4ee5\u5b9e\u73b0\u8de8\u4e0d\u540c\u771f\u5b9e\u573a\u666f\u7684\u9ad8\u5ea6\u6cdb\u5316\u6027\u80fd\u3002\u4e3a\u4e86\u89e3\u51b3\u5185\u5bb9\u5bf9\u9f50\u548c\u7ed3\u6784\u4fdd\u6301\u4e4b\u95f4\u7684\u51b2\u7a81\uff0c\u63d0\u51fa\u865a\u62df\u6700\u4f18\u5e73\u9762\u6982\u5ff5\uff0c\u5c06\u95ee\u9898\u5efa\u6a21\u4e3a\u4f30\u8ba1\u5355\u5e94\u6027\u5206\u89e3\u7cfb\u6570\uff0c\u5e76\u8bbe\u8ba1\u4e86\u8fed\u4ee3\u7cfb\u6570\u9884\u6d4b\u5668\u548c\u6700\u5c0f\u8bed\u4e49\u5931\u771f\u7ea6\u675f\u6765\u8bc6\u522b\u6700\u4f18\u5e73\u9762\uff0c\u901a\u8fc7\u53cc\u5411\u5c06\u4e24\u4e2a\u89c6\u56fe\u626d\u66f2\u5230\u6700\u4f18\u5e73\u9762\u6765\u6574\u5408\u8be5\u65b9\u6848\u3002", "result": "\u5728\u5404\u79cd\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cRopStitch\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u573a\u666f\u9c81\u68d2\u6027\u548c\u5185\u5bb9\u81ea\u7136\u5ea6\u65b9\u9762\u3002", "conclusion": "RopStitch\u5728\u573a\u666f\u9c81\u68d2\u6027\u548c\u5185\u5bb9\u81ea\u7136\u5ea6\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2508.06266", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06266", "abs": "https://arxiv.org/abs/2508.06266", "authors": ["Zezeng Li", "Rui Yang", "Ruochen Chen", "ZhongXuan Luo", "Liming Chen"], "title": "ADPro: a Test-time Adaptive Diffusion Policy for Robot Manipulation via Manifold and Initial Noise Constraints", "comment": null, "summary": "Diffusion policies have recently emerged as a powerful class of visuomotor\ncontrollers for robot manipulation, offering stable training and expressive\nmulti-modal action modeling. However, existing approaches typically treat\naction generation as an unconstrained denoising process, ignoring valuable a\npriori knowledge about geometry and control structure. In this work, we propose\nthe Adaptive Diffusion Policy (ADP), a test-time adaptation method that\nintroduces two key inductive biases into the diffusion. First, we embed a\ngeometric manifold constraint that aligns denoising updates with task-relevant\nsubspaces, leveraging the fact that the relative pose between the end-effector\nand target scene provides a natural gradient direction, and guiding denoising\nalong the geodesic path of the manipulation manifold. Then, to reduce\nunnecessary exploration and accelerate convergence, we propose an analytically\nguided initialization: rather than sampling from an uninformative prior, we\ncompute a rough registration between the gripper and target scenes to propose a\nstructured initial noisy action. ADP is compatible with pre-trained diffusion\npolicies and requires no retraining, enabling test-time adaptation that tailors\nthe policy to specific tasks, thereby enhancing generalization across novel\ntasks and environments. Experiments on RLBench, CALVIN, and real-world dataset\nshow that ADPro, an implementation of ADP, improves success rates,\ngeneralization, and sampling efficiency, achieving up to 25% faster execution\nand 9% points over strong diffusion baselines.", "AI": {"tldr": "ADP\u662f\u4e00\u79cd\u7528\u4e8e\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u6269\u6563\u7b56\u7565\uff0c\u901a\u8fc7\u51e0\u4f55\u7ea6\u675f\u548c\u7ed3\u6784\u5316\u521d\u59cb\u5316\u6765\u63d0\u9ad8\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u6269\u6563\u7b56\u7565\u5728\u4f5c\u4e3a\u89c6\u89c9-\u8fd0\u52a8\u63a7\u5236\u5668\u65f6\uff0c\u867d\u7136\u8bad\u7ec3\u7a33\u5b9a\u4e14\u80fd\u8868\u8fbe\u591a\u6a21\u5f0f\u52a8\u4f5c\uff0c\u4f46\u5b83\u4eec\u5c06\u52a8\u4f5c\u751f\u6210\u89c6\u4e3a\u65e0\u7ea6\u675f\u7684\u53bb\u566a\u8fc7\u7a0b\uff0c\u5ffd\u7565\u4e86\u5173\u4e8e\u51e0\u4f55\u548c\u63a7\u5236\u7ed3\u6784\u7684\u5148\u9a8c\u77e5\u8bc6\u3002\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "method": "ADP\u662f\u4e00\u79cd\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u65b9\u6cd5\uff0c\u5b83\u5728\u6269\u6563\u8fc7\u7a0b\u4e2d\u5f15\u5165\u4e86\u4e24\u4e2a\u5173\u952e\u7684\u5f52\u7eb3\u504f\u7f6e\uff1a1. \u51e0\u4f55\u6d41\u5f62\u7ea6\u675f\uff1a\u5c06\u53bb\u566a\u66f4\u65b0\u4e0e\u4efb\u52a1\u76f8\u5173\u7684\u5b50\u7a7a\u95f4\u5bf9\u9f50\uff0c\u5229\u7528\u672b\u7aef\u6267\u884c\u5668\u548c\u76ee\u6807\u573a\u666f\u4e4b\u95f4\u7684\u76f8\u5bf9\u59ff\u6001\u4f5c\u4e3a\u68af\u5ea6\u65b9\u5411\uff0c\u5e76\u6cbf\u7740\u64cd\u4f5c\u6d41\u5f62\u7684\u6d4b\u5730\u7ebf\u5f15\u5bfc\u53bb\u566a\u30022. \u5206\u6790\u5f15\u5bfc\u521d\u59cb\u5316\uff1a\u901a\u8fc7\u8ba1\u7b97\u6293\u624b\u548c\u76ee\u6807\u573a\u666f\u4e4b\u95f4\u7684\u7c97\u7565\u5bf9\u9f50\u6765\u63d0\u51fa\u7ed3\u6784\u5316\u7684\u521d\u59cb\u566a\u58f0\u52a8\u4f5c\uff0c\u4ee5\u51cf\u5c11\u4e0d\u5fc5\u8981\u7684\u63a2\u7d22\u5e76\u52a0\u901f\u6536\u655b\u3002", "result": "ADP\uff08\u901a\u8fc7ADP\u5b9e\u73b0\uff09\u5728RLBench\u3001CALVIN\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cADP\u63d0\u9ad8\u4e86\u6210\u529f\u7387\u3001\u6cdb\u5316\u6027\u548c\u91c7\u6837\u6548\u7387\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe25%\u7684\u6267\u884c\u901f\u5ea6\u63d0\u5347\u548c9%\u7684\u57fa\u7ebf\u63d0\u5347\u3002", "conclusion": "ADP\u901a\u8fc7\u5f15\u5165\u51e0\u4f55\u6d41\u5f62\u7ea6\u675f\u548c\u5206\u6790\u5f15\u5bfc\u521d\u59cb\u5316\uff0c\u80fd\u591f\u9002\u5e94\u9884\u8bad\u7ec3\u7684\u6269\u6563\u7b56\u7565\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\uff0c\u4ece\u800c\u5728\u6d4b\u8bd5\u65f6\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\u8fdb\u884c\u8c03\u6574\uff0c\u63d0\u9ad8\u4e86\u5728\u65b0\u578b\u4efb\u52a1\u548c\u73af\u5883\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cADP\u5728\u6210\u529f\u7387\u3001\u6cdb\u5316\u6027\u548c\u91c7\u6837\u6548\u7387\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u7684\u6269\u6563\u57fa\u7ebf\u3002"}}
{"id": "2508.06111", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06111", "abs": "https://arxiv.org/abs/2508.06111", "authors": ["Dewi S. W. Gould", "Bruno Mlodozeniec", "Samuel F. Brown"], "title": "SKATE, a Scalable Tournament Eval: Weaker LLMs differentiate between stronger ones using verifiable challenges", "comment": "7 pages and appendices", "summary": "Evaluating the capabilities and risks of foundation models is paramount, yet\ncurrent methods demand extensive domain expertise, hindering their scalability\nas these models rapidly evolve. We introduce SKATE: a novel evaluation\nframework in which large language models (LLMs) compete by generating and\nsolving verifiable tasks for one another. Our core insight is to treat\nevaluation as a game: models act as both task-setters and solvers, incentivized\nto create questions which highlight their own strengths while exposing others'\nweaknesses. SKATE offers several key advantages, balancing scalability,\nopen-endedness, and objectivity. It is fully automated, data-free, and\nscalable, requiring no human input or domain expertise. By using verifiable\ntasks rather than LLM judges, scoring is objective. Unlike domain-limited\nprogrammatically-generated benchmarks (e.g. chess-playing or spatial\nreasoning), having LLMs creatively pose challenges enables open-ended and\nscalable evaluation. As a proof of concept, we introduce LLM-set\ncode-output-prediction (COP) challenges as a verifiable and extensible\nframework in which to test our approach. Using a TrueSkill-based ranking\nsystem, we evaluate six frontier LLMs and find that: (1) weaker models can\nreliably differentiate and score stronger ones, (2) LLM-based systems are\ncapable of self-preferencing behavior, generating questions that align with\ntheir own capabilities, and (3) SKATE automatically surfaces fine-grained\ncapability differences between models. Our findings are an important step\ntowards general, scalable evaluation frameworks which can keep pace with LLM\nprogress.", "AI": {"tldr": "SKATE\u662f\u4e00\u4e2a\u521b\u65b0\u7684\u3001\u81ea\u52a8\u5316\u7684LLM\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u8ba9LLM\u76f8\u4e92\u8bbe\u7f6e\u548c\u89e3\u51b3\u4efb\u52a1\u6765\u8bc4\u4f30\u5b83\u4eec\u7684\u80fd\u529b\u548c\u98ce\u9669\uff0c\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u6216\u9886\u57df\u77e5\u8bc6\uff0c\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u3001\u5f00\u653e\u548c\u5ba2\u89c2\u7684\u8bc4\u4f30\u3002", "motivation": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u80fd\u529b\u548c\u98ce\u9669\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u5e7f\u6cdb\u7684\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u53ef\u6269\u5c55\u6027\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u6a21\u578b\u5728\u5feb\u901f\u53d1\u5c55\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u4ee5\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\u3002", "method": "SKATE\u6846\u67b6\u5c06\u8bc4\u4f30\u89c6\u4e3a\u4e00\u573a\u6e38\u620f\uff0c\u7531LLMs\u626e\u6f14\u4efb\u52a1\u8bbe\u5b9a\u8005\u548c\u89e3\u51b3\u8005\u7684\u89d2\u8272\u3002\u5b83\u4eec\u88ab\u6fc0\u52b1\u53bb\u521b\u9020\u7a81\u51fa\u81ea\u8eab\u4f18\u52bf\u5e76\u66b4\u9732\u5176\u4ed6\u6a21\u578b\u5f31\u70b9\u7684\u4efb\u52a1\u3002\u4f5c\u4e3a\u6982\u5ff5\u9a8c\u8bc1\uff0c\u7814\u7a76\u5f15\u5165\u4e86LLM\u8bbe\u5b9a\u7684\u4ee3\u7801\u8f93\u51fa\u9884\u6d4b\uff08COP\uff09\u6311\u6218\uff0c\u8fd9\u662f\u4e00\u4e2a\u53ef\u9a8c\u8bc1\u4e14\u53ef\u6269\u5c55\u7684\u6d4b\u8bd5\u8be5\u65b9\u6cd5\u8bba\u7684\u6846\u67b6\u3002\u8be5\u6846\u67b6\u4f7f\u7528\u57fa\u4e8eTrueSkill\u7684\u6392\u540d\u7cfb\u7edf\u6765\u8bc4\u4f30\u516d\u4e2a\u524d\u6cbfLLM\u3002", "result": "SKATE\u6846\u67b6\u80fd\u591f\u53ef\u9760\u5730\u533a\u5206\u548c\u8bc4\u4f30\u4e0d\u540c\u80fd\u529b\u7684LLMs\uff0c\u63ed\u793a\u4e86LLM\u9a71\u52a8\u7684\u7cfb\u7edf\u4f1a\u8868\u73b0\u51fa\u81ea\u6211\u504f\u597d\u7684\u884c\u4e3a\uff0c\u5e76\u81ea\u52a8\u53d1\u73b0\u4e86\u6a21\u578b\u95f4\u7ec6\u7c92\u5ea6\u7684\u80fd\u529b\u5dee\u5f02\u3002\u7814\u7a76\u7ed3\u679c\u8bc1\u660e\u4e86SKATE\u5728\u5b9e\u73b0\u901a\u7528\u3001\u53ef\u6269\u5c55\u7684\u8bc4\u4f30\u6846\u67b6\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "SKATE\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u8ba9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u76f8\u4e92\u7ade\u4e89\u751f\u6210\u548c\u89e3\u51b3\u53ef\u9a8c\u8bc1\u7684\u4efb\u52a1\uff0c\u5b9e\u73b0\u4e86\u5bf9\u8fd9\u4e9b\u6a21\u578b\u80fd\u529b\u548c\u98ce\u9669\u7684\u53ef\u6269\u5c55\u3001\u5f00\u653e\u548c\u5ba2\u89c2\u7684\u8bc4\u4f30\u3002\u8be5\u6846\u67b6\u81ea\u52a8\u5316\u3001\u65e0\u9700\u6570\u636e\u548c\u4eba\u5de5\u5e72\u9884\uff0c\u5e76\u901a\u8fc7LLM\u751f\u6210\u7684\u3001\u4e0e\u5176\u81ea\u8eab\u80fd\u529b\u5bf9\u9f50\u7684\u4efb\u52a1\u6765\u63ed\u793a\u6a21\u578b\u95f4\u7684\u7ec6\u5fae\u80fd\u529b\u5dee\u5f02\u3002\u7814\u7a76\u8868\u660e\uff0c\u8f83\u5f31\u7684\u6a21\u578b\u53ef\u4ee5\u53ef\u9760\u5730\u533a\u5206\u548c\u8bc4\u4f30\u66f4\u5f3a\u7684\u6a21\u578b\uff0c\u5e76\u4e14LLM\u9a71\u52a8\u7684\u7cfb\u7edf\u4f1a\u8868\u73b0\u51fa\u81ea\u6211\u504f\u597d\u7684\u884c\u4e3a\u3002"}}
{"id": "2508.05984", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.05984", "abs": "https://arxiv.org/abs/2508.05984", "authors": ["Ankur Naskar", "Gugan Thoppe", "Vijay Gupta"], "title": "Parameter-free Optimal Rates for Nonlinear Semi-Norm Contractions with Applications to $Q$-Learning", "comment": null, "summary": "Algorithms for solving \\textit{nonlinear} fixed-point equations -- such as\naverage-reward \\textit{$Q$-learning} and \\textit{TD-learning} -- often involve\nsemi-norm contractions. Achieving parameter-free optimal convergence rates for\nthese methods via Polyak--Ruppert averaging has remained elusive, largely due\nto the non-monotonicity of such semi-norms. We close this gap by (i.) recasting\nthe averaged error as a linear recursion involving a nonlinear perturbation,\nand (ii.) taming the nonlinearity by coupling the semi-norm's contraction with\nthe monotonicity of a suitably induced norm. Our main result yields the first\nparameter-free $\\tilde{O}(1/\\sqrt{t})$ optimal rates for $Q$-learning in both\naverage-reward and exponentially discounted settings, where $t$ denotes the\niteration index. The result applies within a broad framework that accommodates\nsynchronous and asynchronous updates, single-agent and distributed deployments,\nand data streams obtained either from simulators or along Markovian\ntrajectories.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u65b0\u7684\u65b9\u6cd5\u89e3\u51b3\u4e86Q\u5b66\u4e60\u7684\u6536\u655b\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u6700\u4f18\u6536\u655b\u7387\uff0c\u5e76\u9002\u7528\u4e8e\u591a\u79cd\u573a\u666f\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u57fa\u4e8e\u591a\u4e9a\u514b-\u9c81\u73c0\u7279\u5e73\u5747\u7684\u7b97\u6cd5\u5728\u5904\u7406\u534a\u8303\u6570\u6536\u7f29\u65f6\uff0c\u7531\u4e8e\u534a\u8303\u6570\u975e\u5355\u8c03\u6027\u800c\u96be\u4ee5\u5b9e\u73b0\u53c2\u6570\u65e0\u5173\u7684\u6700\u4f18\u6536\u655b\u7387\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5c06\u5e73\u5747\u8bef\u5dee\u91cd\u6784\u4e3a\u6d89\u53ca\u975e\u7ebf\u6027\u6270\u52a8\u7684\u7ebf\u6027\u9012\u5f52\uff0c\u5e76\u7ed3\u5408\u534a\u8303\u6570\u7684\u6536\u7f29\u6027\u548c\u9002\u5f53\u8bf1\u5bfc\u8303\u6570\u7684\u5355\u8c03\u6027\u6765\u63a7\u5236\u975e\u7ebf\u6027\u3002", "result": "\u5b9e\u73b0\u4e86Q\u5b66\u4e60\u5728\u5e73\u5747\u5956\u52b1\u548c\u6307\u6570\u8870\u51cf\u8bbe\u7f6e\u4e2d\u7684\u53c2\u6570\u65e0\u5173\u7684$\tilde{O}(1/\text{t})$\u6700\u4f18\u6536\u655b\u7387\uff0c\u8be5\u7ed3\u679c\u9002\u7528\u4e8e\u5e7f\u6cdb\u7684\u573a\u666f\uff0c\u5305\u62ec\u540c\u6b65/\u5f02\u6b65\u66f4\u65b0\u3001\u5355\u667a\u80fd\u4f53/\u5206\u5e03\u5f0f\u90e8\u7f72\u4ee5\u53ca\u6765\u81ea\u6a21\u62df\u5668\u6216\u9a6c\u5c14\u53ef\u592b\u8f68\u8ff9\u7684\u6570\u636e\u6d41\u3002", "conclusion": "\u8be5\u7814\u7a76\u9996\u6b21\u5b9e\u73b0\u4e86Q\u5b66\u4e60\u5728\u5e73\u5747\u5956\u52b1\u548c\u6307\u6570\u8870\u51cf\u8bbe\u7f6e\u4e2d\u7684\u53c2\u6570\u65e0\u5173\u7684$\tilde{O}(1/\text{t})$\u6700\u4f18\u6536\u655b\u7387\u3002"}}
{"id": "2508.06459", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2508.06459", "abs": "https://arxiv.org/abs/2508.06459", "authors": ["Reshma Devi", "Avaneesh Balasubramanian", "Keith T. Butler", "Gopalakrishnan Sai Gautam"], "title": "A literature-derived dataset of migration barriers for quantifying ionic transport in battery materials", "comment": null, "summary": "The rate performance of any electrode or solid electrolyte material used in a\nbattery is critically dependent on the migration barrier ($E_m$) governing the\nmotion of the intercalant ion, which is a difficult-to-estimate quantity both\nexperimentally and computationally. The foundation for constructing and\nvalidating accurate machine learning (ML) models that are capable of predicting\n$E_m$, and hence accelerating the discovery of novel electrodes and solid\nelectrolytes, lies in the availability of high-quality dataset(s) containing\n$E_m$. Addressing this critical requirement, we present a comprehensive dataset\ncomprising 619 distinct literature-reported $E_m$ values calculated using\ndensity functional theory based nudged elastic band computations, across 443\ncompositions and 27 structural groups consisting of various compounds that have\nbeen explored as electrodes or solid electrolytes in batteries. Our dataset\nincludes compositions that correspond to fully charged and/or discharged states\nof electrode materials, with intermediate compositions incorporated in select\ninstances. Crucially, for each compound, our dataset provides structural\ninformation, including the initial and final positions of the migrating ion,\nalong with its corresponding $E_m$ in easy-to-use .xlsx and JSON formats. We\nenvision our dataset to be a highly useful resource for the scientific\ncommunity, facilitating the development of advanced ML models that can predict\n$E_m$ precisely and accelerate materials discovery.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b619\u4e2a\u6587\u732e\u62a5\u9053\u7684\u7535\u6c60\u6750\u6599\u8fc1\u79fb\u80fd\u5792\uff08Em\uff09\u53ca\u5176\u76f8\u5173\u7ed3\u6784\u4fe1\u606f\u7684\u7efc\u5408\u6570\u636e\u96c6\uff0c\u65e8\u5728\u652f\u6301\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5f00\u53d1\uff0c\u4ee5\u52a0\u901f\u65b0\u578b\u7535\u6c60\u6750\u6599\u7684\u53d1\u73b0\u3002", "motivation": "\u7535\u6c60\u6750\u6599\u7684\u500d\u7387\u6027\u80fd\u4e25\u91cd\u4f9d\u8d56\u4e8e\u63a7\u5236\u5d4c\u5165\u79bb\u5b50\u8fd0\u52a8\u7684\u8fc1\u79fb\u80fd\u5792\uff08Em\uff09\uff0c\u4f46\u8be5\u91cf\u96be\u4ee5\u901a\u8fc7\u5b9e\u9a8c\u548c\u8ba1\u7b97\u8fdb\u884c\u7cbe\u786e\u4f30\u7b97\uff0c\u963b\u788d\u4e86\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u6750\u6599\u53d1\u73b0\u3002", "method": "\u901a\u8fc7\u5bc6\u5ea6\u6cdb\u51fd\u7406\u8bba\uff08DFT\uff09\u548c\u6539\u8fdb\u7684\u6c16 \u0646\u0637\uff08NEB\uff09\u8ba1\u7b97\u65b9\u6cd5\uff0c\u4ece\u6587\u732e\u4e2d\u6536\u96c6\u4e86619\u4e2a\u8fc1\u79fb\u80fd\u5792\uff08Em\uff09\u6570\u636e\uff0c\u6db5\u76d6\u4e86443\u79cd\u5316\u5408\u7269\u548c27\u79cd\u7ed3\u6784\u7ec4\u5206\u3002", "result": "\u521b\u5efa\u4e86\u4e00\u4e2a\u5305\u542b619\u4e2a\u6587\u732e\u62a5\u9053\u7684\u8fc1\u79fb\u80fd\u5792\uff08Em\uff09\u503c\u7684\u7efc\u5408\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5305\u542b\u6bcf\u79cd\u5316\u5408\u7269\u7684\u7ed3\u6784\u4fe1\u606f\u3001\u79bb\u5b50\u8fc1\u79fb\u7684\u521d\u59cb\u548c\u6700\u7ec8\u4f4d\u7f6e\uff0c\u5e76\u4ee5.xlsx\u548cJSON\u683c\u5f0f\u63d0\u4f9b\uff0c\u53ef\u7528\u4e8e\u5f00\u53d1\u7cbe\u786e\u9884\u6d4bEm\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u4e3a\u7535\u6c60\u6750\u6599\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5f00\u53d1\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u8d44\u6e90\uff0c\u6709\u671b\u52a0\u901f\u65b0\u578b\u7535\u6781\u548c\u56fa\u6001\u7535\u89e3\u8d28\u7684\u53d1\u73b0\u3002"}}
{"id": "2508.06103", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.06103", "abs": "https://arxiv.org/abs/2508.06103", "authors": ["Mohamed Basem", "Islam Oshallah", "Ali Hamdi", "Ammar Mohammed"], "title": "Few-Shot Prompting for Extractive Quranic QA with Instruction-Tuned LLMs", "comment": "6 pages , 2 figures , Accepted in IMSA 2025,Egypt ,\n  https://imsa.msa.edu.eg/", "summary": "This paper presents two effective approaches for Extractive Question\nAnswering (QA) on the Quran. It addresses challenges related to complex\nlanguage, unique terminology, and deep meaning in the text. The second uses\nfew-shot prompting with instruction-tuned large language models such as Gemini\nand DeepSeek. A specialized Arabic prompt framework is developed for span\nextraction. A strong post-processing system integrates subword alignment,\noverlap suppression, and semantic filtering. This improves precision and\nreduces hallucinations. Evaluations show that large language models with Arabic\ninstructions outperform traditional fine-tuned models. The best configuration\nachieves a pAP10 score of 0.637. The results confirm that prompt-based\ninstruction tuning is effective for low-resource, semantically rich QA tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u7ecf\u6587\u62bd\u53d6\u5f0f\u95ee\u7b54\u65b9\u6cd5\uff0c\u4e00\u79cd\u662f\u5fae\u8c03\u6a21\u578b\uff0c\u53e6\u4e00\u79cd\u662f\u4f7f\u7528\u6307\u4ee4\u8c03\u4f18\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5c11\u6837\u672c\u63d0\u793a\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u6307\u4ee4\u8c03\u4f18\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u7ecf\u6587\u8fd9\u79cd\u8d44\u6e90\u7a00\u7f3a\u3001\u8bed\u4e49\u4e30\u5bcc\u7684\u95ee\u7b54\u4efb\u52a1\u4e0a\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u89e3\u51b3\u7ecf\u6587\u6587\u672c\u4e2d\u590d\u6742\u7684\u8bed\u8a00\u3001\u72ec\u7279\u7684\u672f\u8bed\u548c\u6df1\u5c42\u542b\u4e49\u7684\u6311\u6218\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e24\u79cd\u6709\u6548\u7684\u7ecf\u6587\u62bd\u53d6\u5f0f\u95ee\u7b54\u65b9\u6cd5\u3002\u9996\u5148\uff0c\u4ed6\u4eec\u5bf9\u4e00\u4e2a\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u4ee5\u751f\u6210\u7b54\u6848\u3002\u5176\u6b21\uff0c\u4ed6\u4eec\u4f7f\u7528\u7c7b\u4f3cGemini\u548cDeepSeek\u7b49\u6307\u4ee4\u8c03\u4f18\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5c11\u6837\u672c\u63d0\u793a\u3002\u5f00\u53d1\u4e86\u4e00\u4e2a\u4e13\u95e8\u7684\u963f\u62c9\u4f2f\u8bed\u63d0\u793a\u6846\u67b6\u6765\u8fdb\u884c\u8de8\u5ea6\u63d0\u53d6\u3002\u4e00\u4e2a\u5f3a\u5927\u7684\u540e\u5904\u7406\u7cfb\u7edf\u6574\u5408\u4e86\u5b50\u8bcd\u5bf9\u9f50\u3001\u91cd\u53e0\u6291\u5236\u548c\u8bed\u4e49\u8fc7\u6ee4\u3002\u8fd9\u63d0\u9ad8\u4e86\u7cbe\u5ea6\u5e76\u51cf\u5c11\u4e86\u5e7b\u89c9\u3002", "result": "\u8bc4\u4f30\u8868\u660e\uff0c\u5177\u6709\u963f\u62c9\u4f2f\u8bed\u6307\u4ee4\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f18\u4e8e\u4f20\u7edf\u7684\u5fae\u8c03\u6a21\u578b\u3002\u6700\u4f73\u914d\u7f6e\u5b9e\u73b0\u4e860.637\u7684pAP10\u5206\u6570\u3002", "conclusion": "\u57fa\u4e8e\u63d0\u793a\u7684\u6307\u4ee4\u8c03\u4f18\u5bf9\u4e8e\u8d44\u6e90\u7a00\u7f3a\u3001\u8bed\u4e49\u4e30\u5bcc\u7684\u95ee\u7b54\u4efb\u52a1\u6709\u6548\u3002"}}
{"id": "2508.05907", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05907", "abs": "https://arxiv.org/abs/2508.05907", "authors": ["Ilya Chugunov"], "title": "Neural Field Representations of Mobile Computational Photography", "comment": "PhD thesis", "summary": "Over the past two decades, mobile imaging has experienced a profound\ntransformation, with cell phones rapidly eclipsing all other forms of digital\nphotography in popularity. Today's cell phones are equipped with a diverse\nrange of imaging technologies - laser depth ranging, multi-focal camera arrays,\nand split-pixel sensors - alongside non-visual sensors such as gyroscopes,\naccelerometers, and magnetometers. This, combined with on-board integrated\nchips for image and signal processing, makes the cell phone a versatile\npocket-sized computational imaging platform. Parallel to this, we have seen in\nrecent years how neural fields - small neural networks trained to map\ncontinuous spatial input coordinates to output signals - enable the\nreconstruction of complex scenes without explicit data representations such as\npixel arrays or point clouds. In this thesis, I demonstrate how carefully\ndesigned neural field models can compactly represent complex geometry and\nlighting effects. Enabling applications such as depth estimation, layer\nseparation, and image stitching directly from collected in-the-wild mobile\nphotography data. These methods outperform state-of-the-art approaches without\nrelying on complex pre-processing steps, labeled ground truth data, or machine\nlearning priors. Instead, they leverage well-constructed, self-regularized\nmodels that tackle challenging inverse problems through stochastic gradient\ndescent, fitting directly to raw measurements from a smartphone.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u573a\u7684\u79fb\u52a8\u6444\u5f71\u6570\u636e\u5904\u7406\u65b9\u6cd5\uff0c\u80fd\u591f\u9ad8\u6548\u5730\u8fdb\u884c\u6df1\u5ea6\u4f30\u8ba1\u3001\u56fe\u5c42\u5206\u79bb\u548c\u56fe\u50cf\u62fc\u63a5\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u4e14\u65e0\u9700\u590d\u6742\u9884\u5904\u7406\u6216\u6807\u8bb0\u6570\u636e\u3002", "motivation": "\u79fb\u52a8\u6210\u50cf\u6280\u672f\u5728\u8fc7\u53bb\u4e8c\u5341\u5e74\u4e2d\u53d1\u751f\u4e86\u6df1\u523b\u7684\u53d8\u9769\uff0c\u624b\u673a\u5df2\u8fc5\u901f\u8d85\u8d8a\u6240\u6709\u5176\u4ed6\u5f62\u5f0f\u7684\u6570\u7801\u6444\u5f71\u3002\u5982\u4eca\u7684\u624b\u673a\u4e0d\u4ec5\u914d\u5907\u4e86\u6fc0\u5149\u6df1\u5ea6\u6d4b\u8ddd\u3001\u591a\u7126\u70b9\u76f8\u673a\u9635\u5217\u548c\u5206\u50cf\u7d20\u4f20\u611f\u5668\u7b49\u591a\u79cd\u6210\u50cf\u6280\u672f\uff0c\u8fd8\u96c6\u6210\u4e86\u9640\u87ba\u4eea\u3001\u52a0\u901f\u8ba1\u548c\u78c1\u529b\u8ba1\u7b49\u975e\u89c6\u89c9\u4f20\u611f\u5668\u3002\u7ed3\u5408\u677f\u8f7d\u7684\u56fe\u50cf\u548c\u4fe1\u53f7\u5904\u7406\u96c6\u6210\u82af\u7247\uff0c\u8fd9\u4f7f\u5f97\u624b\u673a\u6210\u4e3a\u4e00\u4e2a\u591a\u529f\u80fd\u7684\u4fbf\u643a\u5f0f\u8ba1\u7b97\u6210\u50cf\u5e73\u53f0\u3002\u4e0e\u6b64\u540c\u65f6\uff0c\u8fd1\u5e74\u6765\u51fa\u73b0\u7684\u795e\u7ecf\u573a\uff08\u8bad\u7ec3\u7528\u4e8e\u5c06\u8fde\u7eed\u7a7a\u95f4\u8f93\u5165\u5750\u6807\u6620\u5c04\u5230\u8f93\u51fa\u4fe1\u53f7\u7684\u5c0f\u578b\u795e\u7ecf\u7f51\u7edc\uff09\u80fd\u591f\u5728\u6ca1\u6709\u50cf\u7d20\u6570\u7ec4\u6216\u70b9\u4e91\u7b49\u663e\u5f0f\u6570\u636e\u8868\u793a\u7684\u60c5\u51b5\u4e0b\u91cd\u5efa\u590d\u6742\u573a\u666f\u3002\u56e0\u6b64\uff0c\u672c\u6587\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u5229\u7528\u795e\u7ecf\u573a\u6a21\u578b\u6765\u5904\u7406\u79fb\u52a8\u6444\u5f71\u6570\u636e\uff0c\u4ee5\u5b9e\u73b0\u66f4\u9ad8\u6548\u3001\u66f4\u5f3a\u5927\u7684\u6210\u50cf\u5e94\u7528\u3002", "method": "\u6587\u4e2d\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u795e\u7ecf\u573a\u6a21\u578b\u6765\u5904\u7406\u79fb\u52a8\u6444\u5f71\u6570\u636e\u7684\u65b9\u6cd5\uff0c\u5177\u4f53\u5305\u62ec\u6df1\u5ea6\u4f30\u8ba1\u3001\u56fe\u5c42\u5206\u79bb\u548c\u56fe\u50cf\u62fc\u63a5\u7b49\u5e94\u7528\u3002\u8be5\u65b9\u6cd5\u4e0d\u4f9d\u8d56\u590d\u6742\u7684\u9884\u5904\u7406\u6b65\u9aa4\u3001\u6807\u8bb0\u7684\u771f\u5b9e\u6570\u636e\u6216\u673a\u5668\u5b66\u4e60\u5148\u9a8c\u77e5\u8bc6\uff0c\u800c\u662f\u5229\u7528\u7cbe\u5fc3\u6784\u5efa\u7684\u3001\u81ea\u6b63\u5219\u5316\u7684\u6a21\u578b\uff0c\u901a\u8fc7\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u76f4\u63a5\u62df\u5408\u667a\u80fd\u624b\u673a\u7684\u539f\u59cb\u6d4b\u91cf\u6570\u636e\u6765\u89e3\u51b3\u9006\u95ee\u9898\u3002", "result": "\u6240\u63d0\u51fa\u7684\u795e\u7ecf\u573a\u6a21\u578b\u80fd\u591f\u7d27\u51d1\u5730\u8868\u793a\u590d\u6742\u7684\u51e0\u4f55\u548c\u5149\u7167\u6548\u679c\uff0c\u5e76\u53ef\u76f4\u63a5\u5e94\u7528\u4e8e\u6df1\u5ea6\u4f30\u8ba1\u3001\u56fe\u5c42\u5206\u79bb\u548c\u56fe\u50cf\u62fc\u63a5\u7b49\u4efb\u52a1\u3002\u76f8\u8f83\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u8be5\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u6709\u6240\u8d85\u8d8a\uff0c\u5e76\u4e14\u65e0\u9700\u590d\u6742\u7684\u9884\u5904\u7406\u3001\u6807\u8bb0\u6570\u636e\u6216\u673a\u5668\u5b66\u4e60\u5148\u9a8c\u77e5\u8bc6\u3002", "conclusion": "\u8be5\u8bba\u6587\u5c55\u793a\u4e86\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u795e\u7ecf\u573a\u6a21\u578b\u5982\u4f55\u7d27\u51d1\u5730\u8868\u793a\u590d\u6742\u7684\u51e0\u4f55\u548c\u5149\u7167\u6548\u679c\uff0c\u4ece\u800c\u53ef\u4ee5\u76f4\u63a5\u4ece\u91ce\u5916\u6536\u96c6\u7684\u79fb\u52a8\u6444\u5f71\u6570\u636e\u4e2d\u5b9e\u73b0\u6df1\u5ea6\u4f30\u8ba1\u3001\u56fe\u5c42\u5206\u79bb\u548c\u56fe\u50cf\u62fc\u63a5\u7b49\u5e94\u7528\u3002\u8fd9\u4e9b\u65b9\u6cd5\u5728\u4e0d\u4f9d\u8d56\u590d\u6742\u9884\u5904\u7406\u6b65\u9aa4\u3001\u6807\u8bb0\u7684\u771f\u5b9e\u6570\u636e\u6216\u673a\u5668\u5b66\u4e60\u5148\u9a8c\u77e5\u8bc6\u7684\u60c5\u51b5\u4e0b\uff0c\u6027\u80fd\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002\u76f8\u53cd\uff0c\u5b83\u4eec\u5229\u7528\u7cbe\u5fc3\u6784\u5efa\u7684\u3001\u81ea\u6b63\u5219\u5316\u7684\u6a21\u578b\uff0c\u901a\u8fc7\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u76f4\u63a5\u62df\u5408\u667a\u80fd\u624b\u673a\u7684\u539f\u59cb\u6d4b\u91cf\u6570\u636e\u6765\u89e3\u51b3\u5177\u6709\u6311\u6218\u6027\u7684\u9006\u95ee\u9898\u3002"}}
{"id": "2508.06276", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06276", "abs": "https://arxiv.org/abs/2508.06276", "authors": ["Juan Heredia", "Christian Schlette", "Mikkel Baun Kj\u00e6rgaard"], "title": "EcBot: Data-Driven Energy Consumption Open-Source MATLAB Library for Manipulators", "comment": null, "summary": "Existing literature proposes models for estimating the electrical power of\nmanipulators, yet two primary limitations prevail. First, most models are\npredominantly tested using traditional industrial robots. Second, these models\noften lack accuracy. To address these issues, we introduce an open source\nMatlab-based library designed to automatically generate \\ac{ec} models for\nmanipulators. The necessary inputs for the library are Denavit-Hartenberg\nparameters, link masses, and centers of mass. Additionally, our model is\ndata-driven and requires real operational data, including joint positions,\nvelocities, accelerations, electrical power, and corresponding timestamps. We\nvalidated our methodology by testing on four lightweight robots sourced from\nthree distinct manufacturers: Universal Robots, Franka Emika, and Kinova. The\nmodel underwent testing, and the results demonstrated an RMSE ranging from 1.42\nW to 2.80 W for the training dataset and from 1.45 W to 5.25 W for the testing\ndataset.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u5f00\u6e90\u5e93\uff0c\u7528\u4e8e\u4e3a\u901a\u7528\u673a\u68b0\u81c2\u81ea\u52a8\u751f\u6210\u7535\u80fd\u6d88\u8017\u6a21\u578b\uff0c\u5e76\u7528\u5b9e\u9645\u6570\u636e\u9a8c\u8bc1\u4e86\u5176\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u673a\u68b0\u81c2\u7535\u80fd\u6d88\u8017\u6a21\u578b\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1. \u5927\u591a\u6570\u6a21\u578b\u4ec5\u5728\u4f20\u7edf\u5de5\u4e1a\u673a\u5668\u4eba\u4e0a\u8fdb\u884c\u6d4b\u8bd5\uff1b2. \u6a21\u578b\u7684\u51c6\u786e\u6027\u6709\u5f85\u63d0\u9ad8\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63d0\u4f9b\u4e00\u4e2a\u66f4\u901a\u7528\u3001\u66f4\u51c6\u786e\u7684\u7535\u80fd\u6d88\u8017\u5efa\u6a21\u65b9\u6cd5\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5f00\u6e90\u7684Matlab\u5e93\uff0c\u8be5\u5e93\u9700\u8981\u4f7f\u7528\u6307\u5b9a\u673a\u5668\u4eba\u7684\u8fd0\u52a8\u5b66\u53c2\u6570\uff08\u5982Denavit-Hartenberg\u53c2\u6570\uff09\u548c\u52a8\u529b\u5b66\u53c2\u6570\uff08\u5982\u8fde\u6746\u8d28\u91cf\u3001\u8d28\u5fc3\uff09\u4ee5\u53ca\u5b9e\u9645\u8fd0\u884c\u6570\u636e\uff08\u5982\u5173\u8282\u4f4d\u7f6e\u3001\u901f\u5ea6\u3001\u52a0\u901f\u5ea6\u3001\u7535\u80fd\u6d88\u8017\u548c\u65f6\u95f4\u6233\uff09\u6765\u751f\u6210\u7535\u80fd\u6d88\u8017\u6a21\u578b\u3002", "result": "\u6240\u63d0\u51fa\u7684\u6a21\u578b\u5728\u56db\u4e2a\u8f7b\u91cf\u7ea7\u673a\u5668\u4eba\uff08\u6765\u81eaUniversal Robots\u3001Franka Emika\u548cKinova\uff09\u4e0a\u8fdb\u884c\u4e86\u6d4b\u8bd5\u3002\u7ed3\u679c\u663e\u793a\uff0c\u6a21\u578b\u5728\u8bad\u7ec3\u6570\u636e\u96c6\u4e0a\u7684\u5747\u65b9\u6839\u8bef\u5dee\uff08RMSE\uff09\u57281.42 W\u52302.80 W\u4e4b\u95f4\uff0c\u5728\u6d4b\u8bd5\u6570\u636e\u96c6\u4e0a\u7684RMSE\u57281.45 W\u52305.25 W\u4e4b\u95f4\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMatlab\u7684\u5f00\u6e90\u5e93\uff0c\u7528\u4e8e\u4e3a\u673a\u68b0\u81c2\u81ea\u52a8\u751f\u6210\u7535\u80fd\u6d88\u8017\u6a21\u578b\uff0c\u5e76\u4f7f\u7528\u56db\u4e2a\u8f7b\u91cf\u7ea7\u673a\u5668\u4eba\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u5c55\u793a\u4e86\u5176\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2508.06129", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06129", "abs": "https://arxiv.org/abs/2508.06129", "authors": ["Bachtiar Herdianto", "Romain Billot", "Flavien Lucas", "Marc Sevaux"], "title": "Study of Robust Features in Formulating Guidance for Heuristic Algorithms for Solving the Vehicle Routing Problem", "comment": "22 pages, 14 figures", "summary": "The Vehicle Routing Problem (VRP) is a complex optimization problem with\nnumerous real-world applications, mostly solved using metaheuristic algorithms\ndue to its $\\mathcal{NP}$-Hard nature. Traditionally, these metaheuristics rely\non human-crafted designs developed through empirical studies. However, recent\nresearch shows that machine learning methods can be used the structural\ncharacteristics of solutions in combinatorial optimization, thereby aiding in\ndesigning more efficient algorithms, particularly for solving VRP. Building on\nthis advancement, this study extends the previous research by conducting a\nsensitivity analysis using multiple classifier models that are capable of\npredicting the quality of VRP solutions. Hence, by leveraging explainable AI,\nthis research is able to extend the understanding of how these models make\ndecisions. Finally, our findings indicate that while feature importance varies,\ncertain features consistently emerge as strong predictors. Furthermore, we\npropose a unified framework able of ranking feature impact across different\nscenarios to illustrate this finding. These insights highlight the potential of\nfeature importance analysis as a foundation for developing a guidance mechanism\nof metaheuristic algorithms for solving the VRP.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u673a\u5668\u5b66\u4e60\u548c\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\uff0c\u5206\u6790\u4e86\u5f71\u54cd\u8f66\u8f86\u8def\u5f84\u95ee\u9898\uff08VRP\uff09\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u7684\u7279\u5f81\uff0c\u4e3a\u6539\u8fdb\u5143\u542f\u53d1\u5f0f\u7b97\u6cd5\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002", "motivation": "\u9274\u4e8e\u8f66\u8f86\u8def\u5f84\u95ee\u9898\uff08VRP\uff09\u7684NP-Hard\u6027\u8d28\uff0c\u4f20\u7edf\u4e0a\u4e3b\u8981\u4f9d\u9760\u4eba\u5de5\u8bbe\u8ba1\u7684\u5143\u542f\u53d1\u5f0f\u7b97\u6cd5\u89e3\u51b3\uff0c\u800c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5728\u7406\u89e3\u7ec4\u5408\u4f18\u5316\u89e3\u51b3\u65b9\u6848\u7ed3\u6784\u7279\u6027\u65b9\u9762\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u8fdb\u4e00\u6b65\u6539\u8fdbVRP\u7684\u5143\u542f\u53d1\u5f0f\u7b97\u6cd5\u8bbe\u8ba1\u3002", "method": "\u672c\u7814\u7a76\u901a\u8fc7\u4f7f\u7528\u591a\u79cd\u80fd\u591f\u9884\u6d4b\u8f66\u8f86\u8def\u5f84\u95ee\u9898\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u7684\u5206\u7c7b\u5668\u6a21\u578b\u8fdb\u884c\u654f\u611f\u6027\u5206\u6790\uff0c\u5e76\u5229\u7528\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\u6765\u6269\u5c55\u5bf9\u6b64\u7c7b\u6a21\u578b\u51b3\u7b56\u8fc7\u7a0b\u7684\u7406\u89e3\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u867d\u7136\u7279\u5f81\u91cd\u8981\u6027\u56e0\u6a21\u578b\u800c\u5f02\uff0c\u4f46\u67d0\u4e9b\u7279\u5f81\u59cb\u7ec8\u662f\u91cd\u8981\u7684\u9884\u6d4b\u56e0\u5b50\u3002\u6b64\u5916\uff0c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u80fd\u591f\u8de8\u4e0d\u540c\u573a\u666f\u5bf9\u7279\u5f81\u5f71\u54cd\u8fdb\u884c\u6392\u540d\u7684\u7edf\u4e00\u6846\u67b6\u3002", "conclusion": "\u672c\u7814\u7a76\u7684\u7ed3\u679c\u8868\u660e\uff0c\u7279\u5f81\u91cd\u8981\u6027\u5206\u6790\u53ef\u4ee5\u4f5c\u4e3a\u5f00\u53d1\u7528\u4e8e\u89e3\u51b3\u8f66\u8f86\u8def\u5f84\u95ee\u9898\u7684\u5143\u542f\u53d1\u5f0f\u7b97\u6cd5\u7684\u6307\u5bfc\u673a\u5236\u3002"}}
{"id": "2508.05988", "categories": ["cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.05988", "abs": "https://arxiv.org/abs/2508.05988", "authors": ["Wenhao Zeng", "Yaoning Wang", "Chao Hu", "Yuling Shi", "Chengcheng Wan", "Hongyu Zhang", "Xiaodong Gu"], "title": "Pruning the Unsurprising: Efficient Code Reasoning via First-Token Surprisal", "comment": "Code and model available at https://github.com/Zengwh02/ASAP", "summary": "Recently, Large Reasoning Models (LRMs) have demonstrated remarkable\ncapabilities in code reasoning by scaling up the length of Chain-of-Thought\n(CoT). However, excessively long reasoning traces introduce substantial\nchallenges in terms of training cost, inference latency, and deployment\nfeasibility. While various CoT compression approaches have emerged to address\nthis challenge, they face inherent trade-offs: token-level methods often\ndisrupt syntactic and logical coherence, while step-level methods based on\nperplexity fail to reliably capture the logically critical reasoning steps. In\nthis paper, we propose ASAP (Anchor-guided, Surprisal-based Pruning), a novel\ncoarse-to-fine framework for CoT compression. ASAP first performs anchor-guided\npruning to preserve the core reasoning structure, which efficiently reduces the\nsearch space for subsequent processing. It then enables a logic-aware pruning\nby selecting logically essential reasoning steps based on a novel first-token\nsurprisal metric. Finally, ASAP teaches models to autonomously generate and\nleverage these concise CoTs at inference time, enabling efficient reasoning in\ncoding tasks. Experiments show that ASAP achieves state-of-the-art accuracy\nacross multiple code generation benchmarks while substantially reducing\ntraining and inference costs. On the challenging LiveCodeBench v4_v5 benchmark,\nour approach reduces token generation by 23.5% and inference latency by 43.5%\ncompared to the strongest baseline, while achieving a competitive accuracy of\n36.19% in Pass@1. Our results highlight a promising direction for building\npowerful and efficient LRMs.", "AI": {"tldr": "ASAP\u901a\u8fc7\u951a\u70b9\u5f15\u5bfc\u548c\u60ca\u5947\u5ea6\u5ea6\u91cf\u538b\u7f29\u4e86\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u7684\u4ee3\u7801\u63a8\u7406\uff08CoT\uff09\uff0c\u5728\u4fdd\u6301\u9ad8\u51c6\u786e\u6027\u7684\u540c\u65f6\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8bad\u7ec3\u548c\u63a8\u7406\u6210\u672c\u3002", "motivation": "LRMs\u5728\u4ee3\u7801\u63a8\u7406\u65b9\u9762\u5c55\u73b0\u4e86\u5f3a\u5927\u7684\u80fd\u529b\uff0c\u4f46\u8fc7\u957f\u7684\u63a8\u7406\u8fc7\u7a0b\u5e26\u6765\u4e86\u9ad8\u6602\u7684\u8bad\u7ec3\u6210\u672c\u3001\u63a8\u7406\u5ef6\u8fdf\u548c\u90e8\u7f72\u95ee\u9898\u3002\u73b0\u6709\u7684CoT\u538b\u7f29\u65b9\u6cd5\u5728\u4fdd\u6301\u903b\u8f91\u4e00\u81f4\u6027\u548c\u8bc6\u522b\u5173\u952e\u63a8\u7406\u6b65\u9aa4\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "ASAP\uff08Anchor-guided, Surprisal-based Pruning\uff09\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u7c97\u7c92\u5ea6\u5230\u7ec6\u7c92\u5ea6\u6846\u67b6\uff0c\u7528\u4e8e\u4ee3\u7801\u63a8\u7406\uff08CoT\uff09\u538b\u7f29\u3002\u5b83\u9996\u5148\u901a\u8fc7\u951a\u70b9\u5f15\u5bfc\u8fdb\u884c\u4fee\u526a\uff0c\u4ee5\u4fdd\u7559\u6838\u5fc3\u63a8\u7406\u7ed3\u6784\uff0c\u7136\u540e\u901a\u8fc7\u65b0\u9896\u7684\u7b2c\u4e00\u4e2a\u8bcd\u60ca\u5947\u5ea6\u5ea6\u91cf\u6765\u9009\u62e9\u903b\u8f91\u4e0a\u5fc5\u4e0d\u53ef\u5c11\u7684\u63a8\u7406\u6b65\u9aa4\uff0c\u6700\u540e\u6559\u4f1a\u6a21\u578b\u5728\u63a8\u7406\u65f6\u81ea\u4e3b\u751f\u6210\u548c\u5229\u7528\u7b80\u6d01\u7684CoT\u3002", "result": "ASAP\u5728\u591a\u4e2a\u4ee3\u7801\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u51c6\u786e\u6027\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8bad\u7ec3\u548c\u63a8\u7406\u6210\u672c\u3002\u5728LiveCodeBench v4_v5\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4e0e\u6700\u5f3a\u7684\u57fa\u7ebf\u76f8\u6bd4\uff0cASAP\u5c06token\u751f\u6210\u51cf\u5c11\u4e8623.5%\uff0c\u63a8\u7406\u5ef6\u8fdf\u51cf\u5c11\u4e8643.5%\uff0c\u540c\u65f6\u5728Pass@1\u4e0a\u8fbe\u5230\u4e8636.19%\u7684\u7ade\u4e89\u529b\u3002", "conclusion": "ASAP\u901a\u8fc7\u951a\u70b9\u5f15\u5bfc\u548c\u60ca\u5947\u5ea6\u5ea6\u91cf\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u51c6\u786e\u6027\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8bad\u7ec3\u548c\u63a8\u7406\u6210\u672c\uff0c\u5e76\u5728LiveCodeBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u51cf\u5c11\u4e8623.5%\u7684token\u751f\u6210\u548c43.5%\u7684\u63a8\u7406\u5ef6\u8fdf\u3002"}}
{"id": "2508.06011", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.06011", "abs": "https://arxiv.org/abs/2508.06011", "authors": ["D. -S. Wang"], "title": "State-adaptive quantum error correction and fault-tolerant quantum computing", "comment": "Comments are welcome", "summary": "We present a theoretical framework for state-adaptive quantum error\ncorrection (SAQEC) that bridges the gap between quantum computing and error\ncorrection paradigms. By incorporating knowledge of quantum states into the\nerror correction process, we establish a new capacity regime governed by\nquantum mutual information rather than coherent information. This approach\nreveals a fundamental connection to entanglement-assisted protocols. We\ndemonstrate practical applications in fault-tolerant quantum computation,\nshowing how state-adaptivity enables enhanced error correction without\nadditional measurement overhead. The framework provides new insights into\nquantum channel capacities while offering implementation advantages for current\nquantum computing platforms.", "AI": {"tldr": "SAQEC \u6846\u67b6\u901a\u8fc7\u6574\u5408\u91cf\u5b50\u72b6\u6001\u77e5\u8bc6\u6765\u6539\u8fdb\u91cf\u5b50\u7ea0\u9519\uff0c\u4ece\u800c\u5728\u91cf\u5b50\u8ba1\u7b97\u4e2d\u5b9e\u73b0\u66f4\u9ad8\u7684\u6548\u7387\u548c\u66f4\u597d\u7684\u7ea0\u9519\u6027\u80fd\u3002", "motivation": "\u5f25\u5408\u91cf\u5b50\u8ba1\u7b97\u4e0e\u7ea0\u9519\u8303\u5f0f\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5e76\u5c55\u793a\u4e86\u72b6\u6001\u81ea\u9002\u5e94\u5728\u5bb9\u9519\u91cf\u5b50\u8ba1\u7b97\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u72b6\u6001\u81ea\u9002\u5e94\u91cf\u5b50\u7ea0\u9519\uff08SAQEC\uff09\u7684\u7406\u8bba\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5c06\u91cf\u5b50\u72b6\u6001\u7684\u77e5\u8bc6\u7eb3\u5165\u7ea0\u9519\u8fc7\u7a0b\uff0c\u4ece\u800c\u5efa\u7acb\u4e86\u7531\u91cf\u5b50\u4e92\u4fe1\u606f\u800c\u975e\u76f8\u5e72\u4fe1\u606f\u51b3\u5b9a\u7684\u65b0\u5bb9\u91cf\u4f53\u7cfb\u3002", "result": "SAQEC \u80fd\u591f\u5b9e\u73b0\u5728\u4e0d\u589e\u52a0\u989d\u5916\u6d4b\u91cf\u5f00\u9500\u7684\u60c5\u51b5\u4e0b\uff0c\u63d0\u4f9b\u589e\u5f3a\u7684\u7ea0\u9519\u80fd\u529b\u3002", "conclusion": "SAQEC \u5728\u4e0d\u589e\u52a0\u989d\u5916\u6d4b\u91cf\u5f00\u9500\u7684\u60c5\u51b5\u4e0b\uff0c\u4e3a\u5bb9\u9519\u91cf\u5b50\u8ba1\u7b97\u63d0\u4f9b\u4e86\u6539\u8fdb\u7684\u7ea0\u9519\u80fd\u529b\uff0c\u5e76\u4e3a\u91cf\u5b50\u4fe1\u9053\u5bb9\u91cf\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\uff0c\u540c\u65f6\u4e3a\u5f53\u524d\u7684\u91cf\u5b50\u8ba1\u7b97\u5e73\u53f0\u63d0\u4f9b\u4e86\u5b9e\u73b0\u7684\u4f18\u52bf\u3002"}}
{"id": "2508.06105", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06105", "abs": "https://arxiv.org/abs/2508.06105", "authors": ["Shengyuan Chen", "Chuang Zhou", "Zheng Yuan", "Qinggang Zhang", "Zeyang Cui", "Hao Chen", "Yilin Xiao", "Jiannong Cao", "Xiao Huang"], "title": "You Don't Need Pre-built Graphs for RAG: Retrieval Augmented Generation with Adaptive Reasoning Structures", "comment": null, "summary": "Large language models (LLMs) often suffer from hallucination, generating\nfactually incorrect statements when handling questions beyond their knowledge\nand perception. Retrieval-augmented generation (RAG) addresses this by\nretrieving query-relevant contexts from knowledge bases to support LLM\nreasoning. Recent advances leverage pre-constructed graphs to capture the\nrelational connections among distributed documents, showing remarkable\nperformance in complex tasks. However, existing Graph-based RAG (GraphRAG)\nmethods rely on a costly process to transform the corpus into a graph,\nintroducing overwhelming token cost and update latency. Moreover, real-world\nqueries vary in type and complexity, requiring different logic structures for\naccurate reasoning. The pre-built graph may not align with these required\nstructures, resulting in ineffective knowledge retrieval. To this end, we\npropose a \\textbf{\\underline{Logic}}-aware\n\\textbf{\\underline{R}}etrieval-\\textbf{\\underline{A}}ugmented\n\\textbf{\\underline{G}}eneration framework (\\textbf{LogicRAG}) that dynamically\nextracts reasoning structures at inference time to guide adaptive retrieval\nwithout any pre-built graph. LogicRAG begins by decomposing the input query\ninto a set of subproblems and constructing a directed acyclic graph (DAG) to\nmodel the logical dependencies among them. To support coherent multi-step\nreasoning, LogicRAG then linearizes the graph using topological sort, so that\nsubproblems can be addressed in a logically consistent order. Besides, LogicRAG\napplies graph pruning to reduce redundant retrieval and uses context pruning to\nfilter irrelevant context, significantly reducing the overall token cost.\nExtensive experiments demonstrate that LogicRAG achieves both superior\nperformance and efficiency compared to state-of-the-art baselines.", "AI": {"tldr": "LogicRAG\u662f\u4e00\u79cd\u65b0\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u52a8\u6001\u63d0\u53d6\u63a8\u7406\u7ed3\u6784\u6765\u89e3\u51b3\u73b0\u6709GraphRAG\u65b9\u6cd5\u7684\u6210\u672c\u548c\u6548\u7387\u95ee\u9898\uff0c\u65e0\u9700\u9884\u6784\u5efa\u56fe\uff0c\u5e76\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u56fe\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08GraphRAG\uff09\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u4e00\u4e2a\u6602\u8d35\u7684\u8fc7\u7a0b\u6765\u5c06\u8bed\u6599\u5e93\u8f6c\u6362\u4e3a\u56fe\uff0c\u8fd9\u4f1a\u5e26\u6765\u5de8\u5927\u7684\u4ee4\u724c\u6210\u672c\u548c\u66f4\u65b0\u5ef6\u8fdf\u3002\u6b64\u5916\uff0c\u73b0\u5b9e\u4e16\u754c\u7684\u67e5\u8be2\u5728\u7c7b\u578b\u548c\u590d\u6742\u6027\u4e0a\u5404\u4e0d\u76f8\u540c\uff0c\u9700\u8981\u4e0d\u540c\u7684\u903b\u8f91\u7ed3\u6784\u6765\u8fdb\u884c\u51c6\u786e\u63a8\u7406\u3002\u9884\u5148\u6784\u5efa\u7684\u56fe\u53ef\u80fd\u4e0e\u8fd9\u4e9b\u6240\u9700\u7684\u7ed3\u6784\u4e0d\u4e00\u81f4\uff0c\u5bfc\u81f4\u77e5\u8bc6\u68c0\u7d22\u65e0\u6548\u3002", "method": "LogicRAG\u6846\u67b6\u9996\u5148\u5c06\u8f93\u5165\u67e5\u8be2\u5206\u89e3\u4e3a\u4e00\u7ec4\u5b50\u95ee\u9898\uff0c\u5e76\u6784\u5efa\u4e00\u4e2a\u6709\u5411\u65e0\u73af\u56fe\uff08DAG\uff09\u6765\u6a21\u62df\u5b83\u4eec\u4e4b\u95f4\u7684\u903b\u8f91\u4f9d\u8d56\u5173\u7cfb\u3002\u4e3a\u4e86\u652f\u6301\u8fde\u8d2f\u7684\u591a\u6b65\u63a8\u7406\uff0cLogicRAG\u4f7f\u7528\u62d3\u6251\u6392\u5e8f\u5bf9\u56fe\u8fdb\u884c\u7ebf\u6027\u5316\uff0c\u4ee5\u4fbf\u4ee5\u903b\u8f91\u4e00\u81f4\u7684\u987a\u5e8f\u89e3\u51b3\u5b50\u95ee\u9898\u3002\u6b64\u5916\uff0cLogicRAG\u5e94\u7528\u56fe\u526a\u679d\u6765\u51cf\u5c11\u5197\u4f59\u68c0\u7d22\uff0c\u5e76\u4f7f\u7528\u4e0a\u4e0b\u6587\u526a\u679d\u6765\u8fc7\u6ee4\u4e0d\u76f8\u5173\u7684\u4e0a\u4e0b\u6587\uff0c\u4ece\u800c\u663e\u8457\u964d\u4f4e\u4e86\u603b\u7684\u4ee4\u724c\u6210\u672c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLogicRAG\u5728\u6027\u80fd\u548c\u6548\u7387\u65b9\u9762\u5747\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u3002", "conclusion": "LogicRAG\u6846\u67b6\u901a\u8fc7\u5728\u63a8\u7406\u65f6\u52a8\u6001\u63d0\u53d6\u63a8\u7406\u7ed3\u6784\u6765\u6307\u5bfc\u81ea\u9002\u5e94\u68c0\u7d22\uff0c\u65e0\u9700\u9884\u6784\u5efa\u56fe\uff0c\u5e76\u5728\u6027\u80fd\u548c\u6548\u7387\u65b9\u9762\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u3002"}}
{"id": "2508.05922", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.05922", "abs": "https://arxiv.org/abs/2508.05922", "authors": ["Sri Ramana Saketh Vasanthawada", "Pengkun Liu", "Pingbo Tang"], "title": "Enhancing Construction Site Analysis and Understanding with 3D Segmentation", "comment": null, "summary": "Monitoring construction progress is crucial yet resource-intensive, prompting\nthe exploration of computer-vision-based methodologies for enhanced efficiency\nand scalability. Traditional data acquisition methods, primarily focusing on\nindoor environments, falter in construction site's complex, cluttered, and\ndynamically changing conditions. This paper critically evaluates the\napplication of two advanced 3D segmentation methods, Segment Anything Model\n(SAM) and Mask3D, in challenging outdoor and indoor conditions. Trained\ninitially on indoor datasets, both models' adaptability and performance are\nassessed in real-world construction settings, highlighting the gap in current\nsegmentation approaches due to the absence of benchmarks for outdoor scenarios.\nThrough a comparative analysis, this study not only showcases the relative\neffectiveness of SAM and Mask3D but also addresses the critical need for\ntailored segmentation workflows capable of extracting actionable insights from\nconstruction site data, thereby advancing the field towards more automated and\nprecise monitoring techniques.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86SAM\u548cMask3D\u5728\u5efa\u7b51\u5de5\u5730\u73af\u5883\u4e0b\u7684\u4e09\u7ef4\u5206\u5272\u80fd\u529b\uff0c\u53d1\u73b0\u5728\u6237\u5916\u573a\u666f\u5e94\u7528\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u5e76\u547c\u5401\u5f00\u53d1\u66f4\u9002\u5e94\u5efa\u7b51\u5de5\u5730\u7279\u5b9a\u9700\u6c42\u7684\u5206\u5272\u65b9\u6cd5\u3002", "motivation": "\u7531\u4e8e\u4f20\u7edf\u7684\u73b0\u573a\u76d1\u63a7\u65b9\u6cd5\u6548\u7387\u4f4e\u4e0b\u4e14\u6210\u672c\u9ad8\u6602\uff0c\u56e0\u6b64\u9700\u8981\u63a2\u7d22\u8ba1\u7b97\u673a\u89c6\u89c9\u65b9\u6cd5\u6765\u63d0\u9ad8\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002\u73b0\u6709\u7684\u6570\u636e\u91c7\u96c6\u65b9\u6cd5\u4e3b\u8981\u96c6\u4e2d\u5728\u5ba4\u5185\u73af\u5883\uff0c\u5728\u5efa\u7b51\u5de5\u5730\u7684\u590d\u6742\u3001\u6df7\u4e71\u548c\u52a8\u6001\u53d8\u5316\u6761\u4ef6\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u6b64\u6709\u5fc5\u8981\u8bc4\u4f30\u5148\u8fdb\u7684\u4e09\u7ef4\u5206\u5272\u6280\u672f\u5728\u8fd9\u4e9b\u6761\u4ef6\u4e0b\u7684\u8868\u73b0\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528\u4e86\u6bd4\u8f83\u5206\u6790\u7684\u65b9\u6cd5\uff0c\u8bc4\u4f30\u4e86SAM\u548cMask3D\u4e24\u79cd\u4e09\u7ef4\u5206\u5272\u6a21\u578b\u5728\u771f\u5b9e\u5efa\u7b51\u73af\u5883\uff08\u5305\u62ec\u5ba4\u5185\u548c\u5ba4\u5916\uff09\u4e2d\u7684\u9002\u5e94\u6027\u548c\u6027\u80fd\u3002\u901a\u8fc7\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u5b9e\u9645\u573a\u666f\u4e2d\u8fdb\u884c\u6d4b\u8bd5\uff0c\u5e76\u4e0e\u4ec5\u5728\u5ba4\u5185\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\u8fdb\u884c\u5bf9\u6bd4\uff0c\u6765\u5c55\u793a\u5b83\u4eec\u76f8\u5bf9\u7684\u6709\u6548\u6027\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0cSAM\u548cMask3D\u5728\u5e94\u5bf9\u5efa\u7b51\u5de5\u5730\u590d\u6742\u591a\u53d8\u7684\u5ba4\u5185\u5916\u6761\u4ef6\u65f6\uff0c\u5728\u9002\u5e94\u6027\u548c\u6027\u80fd\u65b9\u9762\u5b58\u5728\u5dee\u5f02\u3002\u8be5\u7814\u7a76\u8fd8\u7a81\u663e\u4e86\u5f53\u524d\u5206\u5272\u65b9\u6cd5\u5728\u7f3a\u4e4f\u9488\u5bf9\u6237\u5916\u573a\u666f\u7684\u57fa\u51c6\u6d4b\u8bd5\u65b9\u9762\u5b58\u5728\u7684\u4e0d\u8db3\uff0c\u5e76\u5f3a\u8c03\u4e86\u5f00\u53d1\u5b9a\u5236\u5316\u5206\u5272\u5de5\u4f5c\u6d41\u7a0b\u4ee5\u4ece\u5efa\u7b51\u5de5\u5730\u6570\u636e\u4e2d\u63d0\u53d6\u53ef\u64cd\u4f5c\u89c1\u89e3\u7684\u5fc5\u8981\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u5bf9SAM\u548cMask3D\u8fd9\u4e24\u79cd\u5148\u8fdb\u7684\u4e09\u7ef4\u5206\u5272\u65b9\u6cd5\u5728\u5ba4\u5185\u5916\u590d\u6742\u5efa\u7b51\u73af\u5883\u4e2d\u7684\u5e94\u7528\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5e76\u5f3a\u8c03\u4e86\u5f53\u524d\u5206\u5272\u65b9\u6cd5\u5728\u6237\u5916\u573a\u666f\u57fa\u51c6\u6d4b\u8bd5\u7f3a\u5931\u65b9\u9762\u5b58\u5728\u7684\u4e0d\u8db3\u3002\u7814\u7a76\u7ed3\u679c\u4e3a\u5f00\u53d1\u66f4\u4e13\u4e1a\u7684\u5206\u5272\u5de5\u4f5c\u6d41\u7a0b\u63d0\u4f9b\u4e86\u4f9d\u636e\uff0c\u4ee5\u671f\u4ece\u5efa\u7b51\u5de5\u5730\u6570\u636e\u4e2d\u63d0\u53d6\u6709\u4ef7\u503c\u7684\u89c1\u89e3\uff0c\u4ece\u800c\u63a8\u52a8\u66f4\u81ea\u52a8\u5316\u3001\u66f4\u7cbe\u786e\u7684\u76d1\u63a7\u6280\u672f\u3002"}}
{"id": "2508.06278", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06278", "abs": "https://arxiv.org/abs/2508.06278", "authors": ["Petr Novak", "Stefan Biffl", "Marek Obitko", "Petr Kadera"], "title": "Mitigating Undesired Conditions in Flexible Production with Product-Process-Resource Asset Knowledge Graphs", "comment": "3 pages, 1 figure", "summary": "Contemporary industrial cyber-physical production systems (CPPS) composed of\nrobotic workcells face significant challenges in the analysis of undesired\nconditions due to the flexibility of Industry 4.0 that disrupts traditional\nquality assurance mechanisms. This paper presents a novel industry-oriented\nsemantic model called Product-Process-Resource Asset Knowledge Graph (PPR-AKG),\nwhich is designed to analyze and mitigate undesired conditions in flexible\nCPPS. Built on top of the well-proven Product-Process-Resource (PPR) model\noriginating from ISA-95 and VDI-3682, a comprehensive OWL ontology addresses\nshortcomings of conventional model-driven engineering for CPPS, particularly\ninadequate undesired condition and error handling representation. The\nintegration of semantic technologies with large language models (LLMs) provides\nintuitive interfaces for factory operators, production planners, and engineers\nto interact with the entire model using natural language. Evaluation with the\nuse case addressing electric vehicle battery remanufacturing demonstrates that\nthe PPR-AKG approach efficiently supports resource allocation based on\nexplicitly represented capabilities as well as identification and mitigation of\nundesired conditions in production. The key contributions include (1) a\nholistic PPR-AKG model capturing multi-dimensional production knowledge, and\n(2) the useful combination of the PPR-AKG with LLM-based chatbots for human\ninteraction.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86PPR-AKG\u6a21\u578b\u548c\u57fa\u4e8eLLM\u7684\u804a\u5929\u673a\u5668\u4eba\uff0c\u7528\u4e8e\u5206\u6790\u548c\u89e3\u51b3\u67d4\u6027CPPS\u4e2d\u7684\u4e0d\u671f\u671b\u72b6\u51b5\uff0c\u5e76\u652f\u6301\u8d44\u6e90\u5206\u914d\u3002", "motivation": "\u89e3\u51b3\u5f53\u4eca\u7531\u673a\u5668\u4eba\u5de5\u4f5c\u5355\u5143\u7ec4\u6210\u7684\u5de5\u4e1a\u7f51\u7edc\u5316\u7269\u7406\u751f\u4ea7\u7cfb\u7edf\uff08CPPS\uff09\u5728\u5206\u6790\u4e0d\u671f\u671b\u6761\u4ef6\u65f6\u9762\u4e34\u7684\u6311\u6218\uff0c\u4ee5\u53ca\u884c\u4e1a4.0\u7684\u7075\u6d3b\u6027\u7834\u574f\u4e86\u4f20\u7edf\u8d28\u91cf\u4fdd\u8bc1\u673a\u5236\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u4ea7\u54c1-\u8fc7\u7a0b-\u8d44\u6e90\u8d44\u4ea7\u77e5\u8bc6\u56fe\u8c31\uff08PPR-AKG\uff09\u7684\u5de5\u4e1a\u5bfc\u5411\u8bed\u4e49\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u57fa\u4e8ePPR\u6a21\u578b\u548cOWL\u672c\u4f53\uff0c\u5e76\u96c6\u6210\u4e86\u8bed\u4e49\u6280\u672f\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63d0\u4f9b\u4ea4\u4e92\u754c\u9762\u3002", "result": "PPR-AKG\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5730\u652f\u6301\u57fa\u4e8e\u660e\u786e\u8868\u793a\u7684\u80fd\u529b\u7684\u8d44\u6e90\u5206\u914d\uff0c\u4ee5\u53ca\u8bc6\u522b\u548c\u7f13\u89e3\u751f\u4ea7\u4e2d\u7684\u4e0d\u671f\u671b\u72b6\u51b5\u3002\u8be5\u6a21\u578b\u6355\u83b7\u4e86\u591a\u7ef4\u5ea6\u7684\u751f\u4ea7\u77e5\u8bc6\uff0c\u5e76\u80fd\u4e0e\u57fa\u4e8eLLM\u7684\u804a\u5929\u673a\u5668\u4eba\u7ed3\u5408\u4ee5\u5b9e\u73b0\u4eba\u673a\u4ea4\u4e92\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86PPR-AKG\u6a21\u578b\uff0c\u5e76\u7ed3\u5408\u4e86\u57fa\u4e8eLLM\u7684\u804a\u5929\u673a\u5668\u4eba\uff0c\u4ee5\u5e94\u5bf9\u5de5\u4e1a4.0\u4e2d\u67d4\u6027\u5316\u751f\u4ea7\u7cfb\u7edf\uff08CPPS\uff09\u7684\u8d28\u91cf\u4fdd\u8bc1\u6311\u6218\uff0c\u80fd\u591f\u6709\u6548\u5730\u652f\u6301\u8d44\u6e90\u5206\u914d\u548c\u8bc6\u522b\u4e0e\u7f13\u89e3\u751f\u4ea7\u4e2d\u7684\u4e0d\u671f\u671b\u72b6\u51b5\u3002"}}
{"id": "2508.06145", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06145", "abs": "https://arxiv.org/abs/2508.06145", "authors": ["Byeonghun Bang", "Jongsuk Yoon", "Dong-Jin Chang", "Seho Park", "Yong Oh Lee"], "title": "Retrieval Augmented Large Language Model System for Comprehensive Drug Contraindications", "comment": null, "summary": "The versatility of large language models (LLMs) has been explored across\nvarious sectors, but their application in healthcare poses challenges,\nparticularly in the domain of pharmaceutical contraindications where accurate\nand reliable information is required. This study enhances the capability of\nLLMs to address contraindications effectively by implementing a Retrieval\nAugmented Generation (RAG) pipeline. Utilizing OpenAI's GPT-4o-mini as the base\nmodel, and the text-embedding-3-small model for embeddings, our approach\nintegrates Langchain to orchestrate a hybrid retrieval system with re-ranking.\nThis system leverages Drug Utilization Review (DUR) data from public databases,\nfocusing on contraindications for specific age groups, pregnancy, and\nconcomitant drug use. The dataset includes 300 question-answer pairs across\nthree categories, with baseline model accuracy ranging from 0.49 to 0.57.\nPost-integration of the RAG pipeline, we observed a significant improvement in\nmodel accuracy, achieving rates of 0.94, 0.87, and 0.89 for contraindications\nrelated to age groups, pregnancy, and concomitant drug use, respectively. The\nresults indicate that augmenting LLMs with a RAG framework can substantially\nreduce uncertainty in prescription and drug intake decisions by providing more\nprecise and reliable drug contraindication information.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u6574\u5408RAG\u7ba1\u9053\uff0c\u663e\u8457\u63d0\u9ad8\u4e86LLM\u5728\u836f\u7269\u7981\u5fcc\u75c7\u4fe1\u606f\u65b9\u9762\u7684\u51c6\u786e\u6027\uff0c\u4e3a\u533b\u7597\u9886\u57df\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u51b3\u7b56\u652f\u6301\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u63d0\u9ad8LLM\u5728\u836f\u7269\u7981\u5fcc\u75c7\u9886\u57df\u7684\u5e94\u7528\u80fd\u529b\uff0c\u56e0\u4e3a\u8be5\u9886\u57df\u9700\u8981\u51c6\u786e\u53ef\u9760\u7684\u4fe1\u606f\uff0c\u800c\u73b0\u6709LLM\u7684\u5e94\u7528\u5b58\u5728\u6311\u6218\u3002", "method": "\u4f7f\u7528OpenAI\u7684GPT-4o-mini\u4f5c\u4e3a\u57fa\u7840\u6a21\u578b\uff0ctext-embedding-3-small\u6a21\u578b\u7528\u4e8e\u5d4c\u5165\uff0c\u5e76\u5229\u7528Langchain\u7f16\u6392\u4e86\u4e00\u4e2a\u7ed3\u5408\u4e86\u91cd\u65b0\u6392\u5e8f\u7684\u6df7\u5408\u68c0\u7d22\u7cfb\u7edf\u3002\u8be5\u7cfb\u7edf\u6574\u5408\u4e86\u6765\u81ea\u516c\u5171\u6570\u636e\u5e93\u7684\u836f\u7269\u5229\u7528\u5ba1\u67e5\uff08DUR\uff09\u6570\u636e\uff0c\u91cd\u70b9\u5173\u6ce8\u7279\u5b9a\u5e74\u9f84\u7ec4\u3001\u6000\u5b55\u548c\u4f34\u968f\u7528\u836f\u7684\u7981\u5fcc\u75c7\u3002", "result": "\u5728\u6574\u5408RAG\u7ba1\u9053\u540e\uff0c\u6a21\u578b\u5728\u4e0e\u5e74\u9f84\u7ec4\u3001\u6000\u5b55\u548c\u4f34\u968f\u7528\u836f\u76f8\u5173\u7684\u7981\u5fcc\u75c7\u65b9\u9762\u7684\u51c6\u786e\u7387\u5206\u522b\u8fbe\u5230\u4e860.94\u30010.87\u548c0.89\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff08\u51c6\u786e\u7387\u57280.49\u52300.57\u4e4b\u95f4\uff09\u3002", "conclusion": "LLM\u901a\u8fc7RAG\u6846\u67b6\u7684\u589e\u5f3a\u53ef\u4ee5\u5927\u5927\u51cf\u5c11\u5904\u65b9\u548c\u836f\u7269\u6444\u5165\u51b3\u7b56\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u63d0\u4f9b\u66f4\u7cbe\u786e\u3001\u66f4\u53ef\u9760\u7684\u836f\u7269\u7981\u5fcc\u4fe1\u606f\u3002"}}
{"id": "2508.05995", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.05995", "abs": "https://arxiv.org/abs/2508.05995", "authors": ["Fei Xu Yu", "Gina Adam", "Nathaniel D. Bastian", "Tian Lan"], "title": "Optimizing Prompt Sequences using Monte Carlo Tree Search for LLM-Based Optimization", "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\ncode generation and structured reasoning; however, their performance often\ndegrades on complex tasks that require consistent multi-step planning. Recent\nwork has explored combining LLMs with Monte Carlo Tree Search (MCTS), yet\nexisting approaches primarily focus on generating heuristic-based code for\noptimization or target simpler tasks where correctness alone is sufficient. In\nthis work, we propose MCTS-OPS, a novel neural-symbolic framework that\nformulates prompt selection as a sequential decision process guided by MCTS.\nOur method explores and refines multi-step prompt sequences for the goal of\nimproving code generation quality and enhancing the problem-solving\ncapabilities of LLMs in general optimization. Experiments on network\noptimization show significant improvement over the baselines, both in the\nsuccess rate of executing the generated code and in the optimization results\nwith the specified objective and constraints (2$\\sim$4$\\times$ higher reward\nand 3$\\times$ lower standard deviation). Moreover, it improves the chance of\nattaining the optimal solution by about 10\\% of cases, compared to baseline\nmethods in hard problems. These results highlight the promise of combining\nsymbolic planning with LLMs for robust, high-quality code generation in complex\ndomains.", "AI": {"tldr": "MCTS-OPS\u901a\u8fc7\u5c06\u63d0\u793a\u9009\u62e9\u4f5c\u4e3a\u987a\u5e8f\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5229\u7528MCTS\u6765\u6539\u8fdbLLM\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u4ee3\u7801\u751f\u6210\u548c\u4f18\u5316\u80fd\u529b\u3002", "motivation": "LLMs\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u9700\u8981\u591a\u6b65\u89c4\u5212\u65f6\u6027\u80fd\u4f1a\u4e0b\u964d\uff0c\u800c\u73b0\u6709LLM\u4e0eMCTS\u7684\u7ed3\u5408\u65b9\u6cd5\u4e3b\u8981\u96c6\u4e2d\u5728\u751f\u6210\u7528\u4e8e\u4f18\u5316\u7684\u542f\u53d1\u5f0f\u4ee3\u7801\uff0c\u6216\u9488\u5bf9\u4ec5\u51ed\u6b63\u786e\u6027\u5c31\u8db3\u591f\u5904\u7406\u7684\u7b80\u5355\u4efb\u52a1\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u795e\u7ecf\u7b26\u53f7\u6846\u67b6MCTS-OPS\uff0c\u5c06\u63d0\u793a\u9009\u62e9\u5236\u5b9a\u4e3a\u7531MCTS\u5f15\u5bfc\u7684\u987a\u5e8f\u51b3\u7b56\u8fc7\u7a0b\uff0c\u4ee5\u6539\u8fdb\u4ee3\u7801\u751f\u6210\u8d28\u91cf\u548cLLM\u5728\u901a\u7528\u4f18\u5316\u4e2d\u7684\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u3002", "result": "\u5728\u7f51\u7edc\u4f18\u5316\u5b9e\u9a8c\u4e2d\uff0cMCTS-OPS\u5728\u6267\u884c\u751f\u6210\u4ee3\u7801\u7684\u6210\u529f\u7387\u548c\u4f18\u5316\u7ed3\u679c\uff08\u5956\u52b1\u63d0\u9ad82-4\u500d\uff0c\u6807\u51c6\u5dee\u964d\u4f4e3\u500d\uff09\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u80fd\u5c06\u83b7\u5f97\u6700\u4f18\u89e3\u7684\u673a\u4f1a\u63d0\u9ad8\u7ea610%\u3002", "conclusion": "LLM\u7ed3\u5408\u7b26\u53f7\u89c4\u5212\u5728\u590d\u6742\u9886\u57df\u4e2d\u8fdb\u884c\u9c81\u68d2\u3001\u9ad8\u8d28\u91cf\u4ee3\u7801\u751f\u6210\u65b9\u9762\u663e\u793a\u51fa\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2508.06078", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.06078", "abs": "https://arxiv.org/abs/2508.06078", "authors": ["Yu-Chao Hsu", "Jiun-Cheng Jiang", "Chun-Hua Lin", "Wei-Ting Chen", "Kuo-Chung Peng", "Prayag Tiwari", "Samuel Yen-Chi Chen", "En-Jui Kuo"], "title": "Federated Quantum Kernel-Based Long Short-term Memory for Human Activity Recognition", "comment": null, "summary": "In this work, we introduce the Federated Quantum Kernel-Based Long Short-term\nMemory (Fed-QK-LSTM) framework, integrating the quantum kernel methods and Long\nShort-term Memory into federated learning.Within Fed-QK-LSTM framework, we\nenhance human activity recognition (HAR) in privacy-sensitive environments and\nleverage quantum computing for distributed learning systems.The\nDeepConv-QK-LSTM architecture on each client node employs convolutional layers\nfor efficient local pattern capture, this design enables the use of a shallow\nQK-LSTM to model long-range relationships within the HAR data.The quantum\nkernel method enables the model to capture complex non-linear relationships in\nmultivariate time-series data with fewer trainable parameters.Experimental\nresults on RealWorld HAR dataset demonstrate that Fed-QK-LSTM framework\nachieves competitive accuracy across different client settings and local\ntraining rounds.We showcase the potential of Fed-QK-LSTM framework for robust\nand privacy-preserving human activity recognition in real-world applications,\nespecially in edge computing environments and on scarce quantum devices.", "AI": {"tldr": "\u672c\u5de5\u4f5c\u63d0\u51fa\u4e86Fed-QK-LSTM\u6846\u67b6\uff0c\u5c06\u91cf\u5b50\u6838\u65b9\u6cd5\u548cLSTM\u96c6\u6210\u5230\u8054\u90a6\u5b66\u4e60\u4e2d\uff0c\u7528\u4e8e\u589e\u5f3a\u9690\u79c1\u654f\u611f\u73af\u5883\u4e2d\u7684\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\u3002\u8be5\u6846\u67b6\u5229\u7528\u91cf\u5b50\u8ba1\u7b97\u6355\u83b7\u590d\u6742\u5173\u7cfb\uff0c\u5e76\u5728RealWorld HAR\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u51c6\u786e\u6027\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u8fb9\u7f18\u8ba1\u7b97\u548c\u7a00\u758f\u91cf\u5b50\u8bbe\u5907\u3002", "motivation": "\u5728\u9690\u79c1\u654f\u611f\u7684\u73af\u5883\u4e2d\u589e\u5f3a\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\uff08HAR\uff09\uff0c\u5e76\u4e3a\u5206\u5e03\u5f0f\u5b66\u4e60\u7cfb\u7edf\u5229\u7528\u91cf\u5b50\u8ba1\u7b97\u3002", "method": "\u672c\u5de5\u4f5c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFederated Quantum Kernel-Based Long Short-term Memory (Fed-QK-LSTM)\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5c06\u91cf\u5b50\u6838\u65b9\u6cd5\u548c\u957f\u77ed\u671f\u8bb0\u5fc6\uff08LSTM\uff09\u96c6\u6210\u5230\u8054\u90a6\u5b66\u4e60\u4e2d\u3002\u5728\u8be5\u6846\u67b6\u5185\uff0c\u5229\u7528\u91cf\u5b50\u8ba1\u7b97\u6765\u589e\u5f3a\u9690\u79c1\u654f\u611f\u73af\u5883\u4e2d\u7684\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\uff08HAR\uff09\u548c\u5206\u5e03\u5f0f\u5b66\u4e60\u7cfb\u7edf\u3002\u6bcf\u4e2a\u5ba2\u6237\u7aef\u8282\u70b9\u4e0a\u7684DeepConv-QK-LSTM\u67b6\u6784\u91c7\u7528\u5377\u79ef\u5c42\u6765\u6709\u6548\u6355\u83b7\u5c40\u90e8\u6a21\u5f0f\uff0c\u5e76\u4f7f\u7528\u6d45\u5c42QK-LSTM\u6765\u6a21\u62dfHAR\u6570\u636e\u4e2d\u7684\u957f\u671f\u4f9d\u8d56\u5173\u7cfb\u3002\u91cf\u5b50\u6838\u65b9\u6cd5\u4f7f\u6a21\u578b\u80fd\u591f\u7528\u66f4\u5c11\u7684\u8bad\u7ec3\u53c2\u6570\u6355\u83b7\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e2d\u7684\u590d\u6742\u975e\u7ebf\u6027\u5173\u7cfb\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cFed-QK-LSTM\u6846\u67b6\u5728RealWorld HAR\u6570\u636e\u96c6\u4e0a\uff0c\u5728\u4e0d\u540c\u7684\u5ba2\u6237\u7aef\u8bbe\u7f6e\u548c\u672c\u5730\u8bad\u7ec3\u8f6e\u6b21\u4e2d\uff0c\u5b9e\u73b0\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u51c6\u786e\u6027\u3002", "conclusion": "Fed-QK-LSTM\u6846\u67b6\u5728\u4e0d\u540c\u7684\u5ba2\u6237\u7aef\u8bbe\u7f6e\u548c\u672c\u5730\u8bad\u7ec3\u8f6e\u6b21\u4e2d\u5b9e\u73b0\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u51c6\u786e\u6027\uff0c\u5c55\u793a\u4e86\u5176\u5728\u771f\u5b9e\u5e94\u7528\u4e2d\u8fdb\u884c\u9c81\u68d2\u4e14\u4fdd\u62a4\u9690\u79c1\u7684\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\u7684\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u5728\u8fb9\u7f18\u8ba1\u7b97\u73af\u5883\u548c\u7a00\u758f\u91cf\u5b50\u8bbe\u5907\u4e0a\u3002"}}
{"id": "2508.06124", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06124", "abs": "https://arxiv.org/abs/2508.06124", "authors": ["Sayantan Adak", "Pratyush Chatterjee", "Somnath Banerjee", "Rima Hazra", "Somak Aditya", "Animesh Mukherjee"], "title": "AURA: Affordance-Understanding and Risk-aware Alignment Technique for Large Language Models", "comment": null, "summary": "Present day LLMs face the challenge of managing affordance-based safety\nrisks-situations where outputs inadvertently facilitate harmful actions due to\noverlooked logical implications. Traditional safety solutions, such as scalar\noutcome-based reward models, parameter tuning, or heuristic decoding\nstrategies, lack the granularity and proactive nature needed to reliably detect\nand intervene during subtle yet crucial reasoning steps. Addressing this\nfundamental gap, we introduce AURA, an innovative, multi-layered framework\ncentered around Process Reward Models (PRMs), providing comprehensive, step\nlevel evaluations across logical coherence and safety-awareness. Our framework\nseamlessly combines introspective self-critique, fine-grained PRM assessments,\nand adaptive safety-aware decoding to dynamically and proactively guide models\ntoward safer reasoning trajectories. Empirical evidence clearly demonstrates\nthat this approach significantly surpasses existing methods, significantly\nimproving the logical integrity and affordance-sensitive safety of model\noutputs. This research represents a pivotal step toward safer, more\nresponsible, and contextually aware AI, setting a new benchmark for\nalignment-sensitive applications.", "AI": {"tldr": "LLMs struggle with safety risks where outputs enable harmful actions due to overlooked implications. Existing safety methods are insufficient. We introduce AURA, a framework using Process Reward Models (PRMs) for step-level safety checks, which proactively guides LLMs to safer reasoning. AURA significantly outperforms current methods, enhancing logical integrity and safety awareness.", "motivation": "Traditional safety solutions are inadequate for managing affordance-based safety risks in LLMs due to their lack of granularity and proactive intervention capabilities during subtle reasoning steps.", "method": "AURA, a multi-layered framework utilizing Process Reward Models (PRMs) for step-level evaluations, combines introspective self-critique, fine-grained PRM assessments, and adaptive safety-aware decoding to guide models toward safer reasoning.", "result": "Empirical evidence shows AURA significantly surpasses existing methods in improving the logical integrity and affordance-sensitive safety of LLM outputs.", "conclusion": "AURA enables safer, more responsible, and contextually aware AI by significantly improving logical integrity and affordance-sensitive safety of model outputs, setting a new benchmark for alignment-sensitive applications."}}
{"id": "2508.05950", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05950", "abs": "https://arxiv.org/abs/2508.05950", "authors": ["Yanxing Liang", "Yinghui Wang", "Jinlong Yang", "Wei Li"], "title": "A 3DGS-Diffusion Self-Supervised Framework for Normal Estimation from a Single Image", "comment": null, "summary": "The lack of spatial dimensional information remains a challenge in normal\nestimation from a single image. Recent diffusion-based methods have\ndemonstrated significant potential in 2D-to-3D implicit mapping, they rely on\ndata-driven statistical priors and miss the explicit modeling of light-surface\ninteraction, leading to multi-view normal direction conflicts. Moreover, the\ndiscrete sampling mechanism of diffusion models causes gradient discontinuity\nin differentiable rendering reconstruction modules, preventing 3D geometric\nerrors from being backpropagated to the normal generation network, thereby\nforcing existing methods to depend on dense normal annotations. This paper\nproposes SINGAD, a novel Self-supervised framework from a single Image for\nNormal estimation via 3D GAussian splatting guided Diffusion. By integrating\nphysics-driven light-interaction modeling and a differentiable rendering-based\nreprojection strategy, our framework directly converts 3D geometric errors into\nnormal optimization signals, solving the challenges of multi-view geometric\ninconsistency and data dependency. Specifically, the framework constructs a\nlight-interaction-driven 3DGS reparameterization model to generate multi-scale\ngeometric features consistent with light transport principles, ensuring\nmulti-view normal consistency. A cross-domain feature fusion module is designed\nwithin a conditional diffusion model, embedding geometric priors to constrain\nnormal generation while maintaining accurate geometric error propagation.\nFurthermore, a differentiable 3D reprojection loss strategy is introduced for\nself-supervised optimization that minimizes geometric error between the\nreconstructed and input image, eliminating dependence on annotated normal\ndatasets. Quantitative evaluations on the Google Scanned Objects dataset\ndemonstrate that our method outperforms state-of-the-art approaches across\nmultiple metrics.", "AI": {"tldr": "SINGAD\u662f\u4e00\u4e2a\u521b\u65b0\u7684\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u5229\u75283D\u9ad8\u65af\u6cfc\u6e85\u548c\u6269\u6563\u6a21\u578b\u4ece\u5355\u5f20\u56fe\u50cf\u4f30\u8ba1\u6cd5\u7ebf\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u591a\u89c6\u56fe\u4e00\u81f4\u6027\u548c\u65e0\u9700\u5bc6\u96c6\u6807\u6ce8\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e86\u4f18\u4e8eSOTA\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5355\u5f20\u56fe\u50cf\u4f30\u8ba1\u6cd5\u7ebf\u65f6\u7f3a\u4e4f\u7a7a\u95f4\u7ef4\u5ea6\u4fe1\u606f\u7684\u95ee\u9898\uff0c\u4ee5\u53ca\u73b0\u6709\u6269\u6563\u6a21\u578b\u4f9d\u8d56\u6570\u636e\u9a71\u52a8\u7684\u7edf\u8ba1\u5148\u9a8c\u3001\u5ffd\u7565\u5149-\u8868\u9762\u4ea4\u4e92\u5efa\u6a21\u3001\u5b58\u5728\u591a\u89c6\u56fe\u6cd5\u7ebf\u65b9\u5411\u51b2\u7a81\u3001\u68af\u5ea6\u4e0d\u8fde\u7eed\u5bfc\u81f4\u65e0\u6cd5\u53cd\u5411\u4f20\u64ad3D\u51e0\u4f55\u8bef\u5dee\u4ee5\u53ca\u9700\u8981\u5bc6\u96c6\u6cd5\u7ebf\u6807\u6ce8\u7b49\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSINGAD\u7684\u65b0\u578b\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u5229\u75283D\u9ad8\u65af\u6cfc\u6e85\u5f15\u5bfc\u7684\u6269\u6563\u6a21\u578b\u4ece\u5355\u5f20\u56fe\u50cf\u4f30\u8ba1\u6cd5\u7ebf\u3002\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u5149\u7167\u4ea4\u4e92\u6a21\u578b\u548c\u53ef\u5fae\u5206\u6e32\u67d3\u7684\u91cd\u6295\u5f71\u7b56\u7565\uff0c\u76f4\u63a5\u5c063D\u51e0\u4f55\u8bef\u5dee\u8f6c\u6362\u4e3a\u6cd5\u7ebf\u4f18\u5316\u4fe1\u53f7\u3002\u5177\u4f53\u800c\u8a00\uff0c\u901a\u8fc7\u6784\u5efa\u5149\u7167\u4ea4\u4e92\u9a71\u52a8\u76843DGS\u91cd\u53c2\u6570\u5316\u6a21\u578b\u751f\u6210\u591a\u5c3a\u5ea6\u51e0\u4f55\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u8de8\u57df\u7279\u5f81\u878d\u5408\u6a21\u5757\u5728\u6761\u4ef6\u6269\u6563\u6a21\u578b\u4e2d\u5d4c\u5165\u51e0\u4f55\u5148\u9a8c\u6765\u7ea6\u675f\u6cd5\u7ebf\u751f\u6210\uff0c\u540c\u65f6\u5229\u7528\u53ef\u5fae\u52063D\u91cd\u6295\u5f71\u635f\u5931\u8fdb\u884c\u81ea\u76d1\u7763\u4f18\u5316\u3002", "result": "SINGAD\u6846\u67b6\u80fd\u591f\u89e3\u51b3\u591a\u89c6\u56fe\u51e0\u4f55\u4e0d\u4e00\u81f4\u548c\u6570\u636e\u4f9d\u8d56\u6027\u95ee\u9898\uff0c\u5e76\u901a\u8fc7Google Scanned Objects\u6570\u636e\u96c6\u7684\u91cf\u5316\u8bc4\u4f30\u8bc1\u660e\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "SINGAD\u901a\u8fc7\u7ed3\u5408\u7269\u7406\u9a71\u52a8\u7684\u5149\u7167\u4ea4\u4e92\u6a21\u578b\u548c\u57fa\u4e8e\u53ef\u5fae\u5206\u6e32\u67d3\u7684\u91cd\u6295\u5f71\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u591a\u89c6\u56fe\u51e0\u4f55\u4e0d\u4e00\u81f4\u548c\u6570\u636e\u4f9d\u8d56\u6027\u95ee\u9898\uff0c\u5e76\u5728Google Scanned Objects\u6570\u636e\u96c6\u7684\u91cf\u5316\u8bc4\u4f30\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2508.06283", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06283", "abs": "https://arxiv.org/abs/2508.06283", "authors": ["Saad Ejaz", "Marco Giberna", "Muhammad Shaheer", "Jose Andres Millan-Romera", "Ali Tourani", "Paul Kremer", "Holger Voos", "Jose Luis Sanchez-Lopez"], "title": "Situationally-aware Path Planning Exploiting 3D Scene Graphs", "comment": null, "summary": "3D Scene Graphs integrate both metric and semantic information, yet their\nstructure remains underutilized for improving path planning efficiency and\ninterpretability. In this work, we present S-Path, a situationally-aware path\nplanner that leverages the metric-semantic structure of indoor 3D Scene Graphs\nto significantly enhance planning efficiency. S-Path follows a two-stage\nprocess: it first performs a search over a semantic graph derived from the\nscene graph to yield a human-understandable high-level path. This also\nidentifies relevant regions for planning, which later allows the decomposition\nof the problem into smaller, independent subproblems that can be solved in\nparallel. We also introduce a replanning mechanism that, in the event of an\ninfeasible path, reuses information from previously solved subproblems to\nupdate semantic heuristics and prioritize reuse to further improve the\nefficiency of future planning attempts. Extensive experiments on both\nreal-world and simulated environments show that S-Path achieves average\nreductions of 5.7x in planning time while maintaining comparable path\noptimality to classical sampling-based planners and surpassing them in complex\nscenarios, making it an efficient and interpretable path planner for\nenvironments represented by indoor 3D Scene Graphs.", "AI": {"tldr": "S-Path leverages 3D Scene Graphs for efficient and interpretable path planning in indoor environments, outperforming traditional methods in complex situations.", "motivation": "The paper addresses the underutilization of the metric-semantic structure of 3D Scene Graphs for improving path planning efficiency and interpretability.", "method": "S-Path uses a two-stage process: first, it searches a semantic graph derived from the 3D scene graph for a high-level path and identifies relevant planning regions. Then, it decomposes the problem into smaller subproblems solved in parallel. It also includes a replanning mechanism that reuses information from solved subproblems to update semantic heuristics for future planning attempts.", "result": "Extensive experiments show S-Path achieves average reductions of 5.7x in planning time compared to classical sampling-based planners, maintains comparable path optimality, and surpasses them in complex scenarios.", "conclusion": "S-Path is an efficient and interpretable path planner for indoor 3D Scene Graphs, achieving significant reductions in planning time while maintaining comparable path optimality."}}
{"id": "2508.06225", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06225", "abs": "https://arxiv.org/abs/2508.06225", "authors": ["Zailong Tian", "Zhuoheng Han", "Yanzhe Chen", "Haozhe Xu", "Xi Yang", "richeng xuan", "Hongfeng Wang", "Lizi Liao"], "title": "Overconfidence in LLM-as-a-Judge: Diagnosis and Confidence-Driven Solution", "comment": null, "summary": "Large Language Models (LLMs) are widely used as automated judges, where\npractical value depends on both accuracy and trustworthy, risk-aware judgments.\nExisting approaches predominantly focus on accuracy, overlooking the necessity\nof well-calibrated confidence, which is vital for adaptive and reliable\nevaluation pipelines. In this work, we advocate a shift from accuracy-centric\nevaluation to confidence-driven, risk-aware LLM-as-a-Judge systems, emphasizing\nthe necessity of well-calibrated confidence for trustworthy and adaptive\nevaluation. We systematically identify the **Overconfidence Phenomenon** in\ncurrent LLM-as-a-Judges, where predicted confidence significantly overstates\nactual correctness, undermining reliability in practical deployment. To\nquantify this phenomenon, we introduce **TH-Score**, a novel metric measuring\nconfidence-accuracy alignment. Furthermore, we propose **LLM-as-a-Fuser**, an\nensemble framework that transforms LLMs into reliable, risk-aware evaluators.\nExtensive experiments demonstrate that our approach substantially improves\ncalibration and enables adaptive, confidence-driven evaluation pipelines,\nachieving superior reliability and accuracy compared to existing baselines.", "AI": {"tldr": "\u8be5\u7814\u7a76\u89e3\u51b3\u4e86LLM\u4f5c\u4e3a\u88c1\u5224\u65f6\u5b58\u5728\u7684\u201c\u8fc7\u5ea6\u81ea\u4fe1\u73b0\u8c61\u201d\uff0c\u63d0\u51faTH-Score\u5ea6\u91cf\u6807\u51c6\u548cLLM-as-a-Fuser\u6846\u67b6\uff0c\u901a\u8fc7\u63d0\u9ad8\u7f6e\u4fe1\u5ea6\u6821\u51c6\u6765\u589e\u5f3a\u8bc4\u4f30\u7684\u53ef\u9760\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709LLM\u4f5c\u4e3a\u81ea\u52a8\u5316\u88c1\u5224\u7684\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u51c6\u786e\u6027\uff0c\u5ffd\u89c6\u4e86\u826f\u597d\u6821\u51c6\u7684\u7f6e\u4fe1\u5ea6\u7684\u91cd\u8981\u6027\uff0c\u800c\u7f6e\u4fe1\u5ea6\u5bf9\u4e8e\u81ea\u9002\u5e94\u548c\u53ef\u9760\u7684\u8bc4\u4f30\u6d41\u7a0b\u81f3\u5173\u91cd\u8981\u3002\u672c\u7814\u7a76\u63d0\u5021\u4ece\u4ee5\u51c6\u786e\u6027\u4e3a\u4e2d\u5fc3\u7684\u8bc4\u4f30\u8f6c\u5411\u4ee5\u7f6e\u4fe1\u5ea6\u4e3a\u9a71\u52a8\u3001\u98ce\u9669\u611f\u77e5\u7684LLM\u4f5c\u4e3a\u88c1\u5224\u7684\u7cfb\u7edf\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTH-Score\u7684\u65b0\u5ea6\u91cf\u6807\u51c6\uff0c\u7528\u4e8e\u91cf\u5316\u7f6e\u4fe1\u5ea6-\u51c6\u786e\u6027\u5bf9\u9f50\uff0c\u5e76\u63d0\u51fa\u4e86LLM-as-a-Fuser\u96c6\u6210\u6846\u67b6\u3002", "result": "\u91cf\u5316\u4e86LLM\u201c\u8fc7\u5ea6\u81ea\u4fe1\u73b0\u8c61\u201d\uff08\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u663e\u8457\u9ad8\u4e8e\u5b9e\u9645\u6b63\u786e\u7387\uff09\uff0c\u5e76\u63d0\u51faTH-Score\u5ea6\u91cf\u6807\u51c6\u3002\u5b9e\u9a8c\u8bc1\u660eLLM-as-a-Fuser\u6846\u67b6\u80fd\u63d0\u5347\u6821\u51c6\u5ea6\uff0c\u5b9e\u73b0\u81ea\u9002\u5e94\u3001\u7f6e\u4fe1\u5ea6\u9a71\u52a8\u7684\u8bc4\u4f30\uff0c\u63d0\u9ad8\u53ef\u9760\u6027\u548c\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLLM-as-a-Fuser\u7684\u96c6\u6210\u6846\u67b6\uff0c\u5c06LLM\u8f6c\u53d8\u4e3a\u53ef\u9760\u3001\u98ce\u9669\u611f\u77e5\u7684\u8bc4\u4f30\u8005\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8bc1\u660e\u8be5\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u9ad8\u6821\u51c6\u5ea6\uff0c\u5b9e\u73b0\u81ea\u9002\u5e94\u3001\u7f6e\u4fe1\u5ea6\u9a71\u52a8\u7684\u8bc4\u4f30\u6d41\u7a0b\uff0c\u76f8\u6bd4\u73b0\u6709\u57fa\u7ebf\u5728\u53ef\u9760\u6027\u548c\u51c6\u786e\u6027\u65b9\u9762\u5747\u6709\u63d0\u5347\u3002"}}
{"id": "2508.06023", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06023", "abs": "https://arxiv.org/abs/2508.06023", "authors": ["Xiaobin Shen", "Jonathan Elmer", "George H. Chen"], "title": "Stepwise Fine and Gray: Subject-Specific Variable Selection Shows When Hemodynamic Data Improves Prognostication of Comatose Post-Cardiac Arrest Patients", "comment": null, "summary": "Prognostication for comatose post-cardiac arrest patients is a critical\nchallenge that directly impacts clinical decision-making in the ICU. Clinical\ninformation that informs prognostication is collected serially over time.\nShortly after cardiac arrest, various time-invariant baseline features are\ncollected (e.g., demographics, cardiac arrest characteristics). After ICU\nadmission, additional features are gathered, including time-varying hemodynamic\ndata (e.g., blood pressure, doses of vasopressor medications). We view these as\ntwo phases in which we collect new features. In this study, we propose a novel\nstepwise dynamic competing risks model that improves the prediction of\nneurological outcomes by automatically determining when to take advantage of\ntime-invariant features (first phase) and time-varying features (second phase).\nNotably, our model finds patients for whom this second phase (time-varying\nhemodynamic) information is beneficial for prognostication and also when this\ninformation is beneficial (as we collect more hemodynamic data for a patient\nover time, how important these data are for prognostication varies). Our\napproach extends the standard Fine and Gray model to explicitly model the two\nphases and to incorporate neural networks to flexibly capture complex nonlinear\nfeature relationships. Evaluated on a retrospective cohort of 2,278 comatose\npost-arrest patients, our model demonstrates robust discriminative performance\nfor the competing outcomes of awakening, withdrawal of life-sustaining therapy,\nand death despite maximal support. Our approach generalizes to more than two\nphases in which new features are collected and could be used in other dynamic\nprediction tasks, where it may be helpful to know when and for whom newly\ncollected features significantly improve prediction.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u52a8\u6001\u7ade\u4e89\u98ce\u9669\u6a21\u578b\uff0c\u7528\u4e8e\u6539\u8fdb\u5fc3\u810f\u9aa4\u505c\u540e\u660f\u8ff7\u60a3\u8005\u7684\u9884\u540e\u9884\u6d4b\u3002\u8be5\u6a21\u578b\u80fd\u7ed3\u5408\u65f6\u95f4\u4e0d\u53d8\u548c\u65f6\u95f4\u53d8\u5316\u7684\u7279\u5f81\uff0c\u5e76\u81ea\u52a8\u786e\u5b9a\u4f55\u65f6\u5229\u7528\u8fd9\u4e9b\u4fe1\u606f\u3002\u6a21\u578b\u5728\u771f\u5b9e\u6570\u636e\u4e0a\u8868\u73b0\u51fa\u826f\u597d\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u5e76\u53ef\u63a8\u5e7f\u5230\u5176\u4ed6\u9700\u8981\u52a8\u6001\u7279\u5f81\u5206\u6790\u7684\u573a\u666f\u3002", "motivation": "\u5fc3\u810f\u9aa4\u505c\u540e\u660f\u8ff7\u60a3\u8005\u7684\u9884\u540e\u8bc4\u4f30\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\uff0c\u76f4\u63a5\u5f71\u54cdICU\u7684\u4e34\u5e8a\u51b3\u7b56\u3002\u6b64\u7814\u7a76\u65e8\u5728\u6539\u8fdb\u9884\u540e\u9884\u6d4b\uff0c\u7279\u522b\u662f\u5229\u7528\u968f\u65f6\u95f4\u53d8\u5316\uff08\u5982\u8840\u6d41\u52a8\u529b\u5b66\u6570\u636e\uff09\u548c\u4e0d\u968f\u65f6\u95f4\u53d8\u5316\uff08\u5982\u4eba\u53e3\u7edf\u8ba1\u5b66\u3001\u5fc3\u810f\u9aa4\u505c\u7279\u5f81\uff09\u7684\u7279\u5f81\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u9010\u6b65\u52a8\u6001\u7ade\u4e89\u98ce\u9669\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u901a\u8fc7\u81ea\u52a8\u786e\u5b9a\u4f55\u65f6\u5229\u7528\u4e0d\u968f\u65f6\u95f4\u53d8\u5316\u7684\u7279\u5f81\uff08\u7b2c\u4e00\u9636\u6bb5\uff09\u548c\u968f\u65f6\u95f4\u53d8\u5316\u7684\u7279\u5f81\uff08\u7b2c\u4e8c\u9636\u6bb5\uff09\u6765\u6539\u8fdb\u795e\u7ecf\u7cfb\u7edf\u7ed3\u5c40\u7684\u9884\u6d4b\u3002\u8be5\u65b9\u6cd5\u6269\u5c55\u4e86\u6807\u51c6\u7684Fine and Gray\u6a21\u578b\uff0c\u660e\u786e\u6a21\u62df\u4e86\u4e24\u4e2a\u9636\u6bb5\uff0c\u5e76\u7ed3\u5408\u4e86\u795e\u7ecf\u7f51\u7edc\u6765\u7075\u6d3b\u5730\u6355\u6349\u590d\u6742\u7684\u975e\u7ebf\u6027\u7279\u5f81\u5173\u7cfb\u3002", "result": "\u6211\u4eec\u7684\u6a21\u578b\u80fd\u591f\u8bc6\u522b\u51fa\u54ea\u4e9b\u60a3\u8005\u7684\u7b2c\u4e8c\u9636\u6bb5\uff08\u968f\u65f6\u95f4\u53d8\u5316\u7684\u8840\u6d41\u52a8\u529b\u5b66\uff09\u4fe1\u606f\u5bf9\u9884\u540e\u6709\u76ca\uff0c\u4ee5\u53ca\u4f55\u65f6\u8fd9\u4e9b\u4fe1\u606f\u6709\u76ca\uff08\u968f\u7740\u6211\u4eec\u6536\u96c6\u60a3\u8005\u968f\u65f6\u95f4\u7684\u66f4\u591a\u8840\u6d41\u52a8\u529b\u5b66\u6570\u636e\uff0c\u8fd9\u4e9b\u6570\u636e\u5bf9\u9884\u540e\u7684\u91cd\u8981\u6027\u4e5f\u4f1a\u53d8\u5316\uff09\u3002", "conclusion": "\u6211\u4eec\u7684\u6a21\u578b\u57282,278\u540d\u5fc3\u810f\u9aa4\u505c\u540e\u660f\u8ff7\u60a3\u8005\u7684\u56de\u987e\u6027\u961f\u5217\u4e2d\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5728\u533a\u5206\u89c9\u9192\u3001\u64a4\u9664\u751f\u547d\u652f\u6301\u548c\u6700\u5927\u652f\u6301\u4e0b\u7684\u6b7b\u4ea1\u7b49\u7ade\u4e89\u6027\u7ed3\u5c40\u65b9\u9762\u8868\u73b0\u51fa\u7a33\u5065\u7684\u8fa8\u522b\u6027\u80fd\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u53ef\u4ee5\u63a8\u5e7f\u5230\u6536\u96c6\u65b0\u7279\u5f81\u7684\u591a\u9636\u6bb5\u573a\u666f\uff0c\u5e76\u53ef\u7528\u4e8e\u5176\u4ed6\u52a8\u6001\u9884\u6d4b\u4efb\u52a1\uff0c\u4ee5\u786e\u5b9a\u4f55\u65f6\u4ee5\u53ca\u5bf9\u54ea\u4e9b\u60a3\u8005\u65b0\u6536\u96c6\u7684\u7279\u5f81\u80fd\u663e\u8457\u6539\u5584\u9884\u6d4b\u3002"}}
{"id": "2508.06121", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.06121", "abs": "https://arxiv.org/abs/2508.06121", "authors": ["Kohei Oshio", "Kaito Wada", "Naoki Yamamoto"], "title": "Near-Heisenberg-limited parallel amplitude estimation with logarithmic depth circuit", "comment": "16 pages, 7 figures", "summary": "Quantum amplitude estimation is one of the core subroutines in quantum\nalgorithms. This paper gives a parallelized amplitude estimation (PAE)\nalgorithm, that simultaneously achieves near-Heisenberg scaling in the total\nnumber of queries and sub-linear scaling in the circuit depth, with respect to\nthe estimation precision. The algorithm is composed of a global GHZ state\nfollowed by separated low-depth Grover circuits; the number of qubits in the\nGHZ state and the depth of each circuit is tunable as a trade-off way, which\nparticularly enables even near-Heisenberg-limited and logarithmic-depth\nalgorithm for amplitude estimation. The quantum signal processing technique is\neffectively used to build the algorithm. The proposed algorithm has a form of\ndistributed quantum computing, which may be suitable for device implementation.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u91cf\u5b50\u5e45\u5ea6\u4f30\u8ba1\u7b97\u6cd5\uff08PAE\uff09\uff0c\u5b83\u4f7f\u7528GHZ\u6001\u548cGrover\u7535\u8def\uff0c\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u901f\u5ea6\u548c\u66f4\u4f4e\u7684\u7535\u8def\u6df1\u5ea6\uff0c\u5e76\u9002\u7528\u4e8e\u5206\u5e03\u5f0f\u91cf\u5b50\u8ba1\u7b97\u3002", "motivation": "\u91cf\u5b50\u5e45\u5ea6\u4f30\u8ba1\u662f\u91cf\u5b50\u7b97\u6cd5\u7684\u6838\u5fc3\u5b50\u7a0b\u5e8f\uff0c\u8be5\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u540c\u65f6\u5b9e\u73b0\u8fd1\u4e4e\u6d77\u68ee\u5821\u6781\u9650\u7684\u67e5\u8be2\u6b21\u6570\u548c\u4e9a\u7ebf\u6027\u7535\u8def\u6df1\u5ea6\u7684\u7b97\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u4f30\u8ba1\u7cbe\u5ea6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5e76\u884c\u5316\u5e45\u5ea6\u4f30\u8ba1\uff08PAE\uff09\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u5229\u7528\u5168\u5c40GHZ\u6001\u548c\u5206\u79bb\u7684\u4f4e\u6df1\u5ea6Grover\u7535\u8def\uff0c\u5e76\u901a\u8fc7\u91cf\u5b50\u4fe1\u53f7\u5904\u7406\u6280\u672f\u8fdb\u884c\u6784\u5efa\u3002", "result": "\u8be5\u7b97\u6cd5\u5b9e\u73b0\u4e86\u8fd1\u4e4e\u6d77\u68ee\u5821\u6781\u9650\u7684\u603b\u67e5\u8be2\u6b21\u6570\u548c\u5173\u4e8e\u4f30\u8ba1\u7cbe\u5ea6\u7684\u4e9a\u7ebf\u6027\u7535\u8def\u6df1\u5ea6\uff0c\u5e76\u4e14\u53ef\u4ee5\u9488\u5bf9GHZ\u6001\u4e2d\u7684\u91cf\u5b50\u6bd4\u7279\u6570\u91cf\u548c\u6bcf\u4e2a\u7535\u8def\u7684\u6df1\u5ea6\u8fdb\u884c\u8c03\u6574\uff0c\u4ece\u800c\u5b9e\u73b0\u8fd1\u4e4e\u6d77\u68ee\u5821\u6781\u9650\u548c\u5bf9\u6570\u6df1\u5ea6\u7684\u5e45\u5ea6\u4f30\u8ba1\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u901a\u8fc7\u7ed3\u5408\u5168\u5c40GHZ\u6001\u548c\u5206\u79bb\u7684\u4f4e\u6df1\u5ea6Grover\u7535\u8def\uff0c\u5b9e\u73b0\u4e86\u8fd1\u4e4e\u6d77\u68ee\u5821\u6781\u9650\u7684\u603b\u67e5\u8be2\u6b21\u6570\u548c\u4e9a\u7ebf\u6027\u7535\u8def\u6df1\u5ea6\u7684\u540c\u65f6\u53ef\u6269\u5c55\u6027\uff0c\u5e76\u5229\u7528\u91cf\u5b50\u4fe1\u53f7\u5904\u7406\u6280\u672f\u6784\u5efa\uff0c\u9002\u7528\u4e8e\u5206\u5e03\u5f0f\u91cf\u5b50\u8ba1\u7b97\u3002"}}
{"id": "2508.06135", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06135", "abs": "https://arxiv.org/abs/2508.06135", "authors": ["Lingyuan Liu", "Mengxiang Zhang"], "title": "Less is More: Selective Reflection for Compatible and Efficient Knowledge Distillation in Large Language Models", "comment": null, "summary": "Knowledge Distillation (KD) is a fundamental technique for compressing large\nlanguage models (LLMs) into compact, efficient student models. However,\nexisting white-box KD methods mainly focus on balancing ground truth and\nstudent-generated responses while overlooking two critical factors: training\ndata quality and student-model compatibility. To address these limitations, we\npropose Selective Reflection Distillation (SRD), a novel data curation\nframework that leverages reflections from student models to systematically\nrefine training data. SRD dynamically evaluates and selects prompt-response\npairs by comparing ground truth data with student model outputs, selectively\ncurating high-quality, student-compatible training instances through automated\nranking based on difficulty. Furthermore, after selecting the training data, a\ncurriculum scheduling strategy is employed to incrementally introduce these\ncurated subsets into the distillation process at fixed intervals. As a\nplug-and-play enhancement, SRD consistently improves distillation outcomes\nacross diverse white-box KD approaches and model architectures, as well as\ndecreases computational cost significantly during KD training. Experiments on a\nrange of language model benchmarks demonstrate SRD's consistent improvements in\ndistilled model performance, as well as a reduction in training runtime by up\nto 39%, under diverse KD methods and model families. Notably, SRD operates as a\nplug-and-play module, enhancing sample efficiency without modifying underlying\nKD algorithms. Our findings highlight that data quality and compatibility are\npivotal to effective and efficient distillation of LLMs, and SRD provides a\nprincipled framework to achieve both. This work advances the understanding of\ndata-centric factors in KD and offers practical insights for enhancing the\ncapability and efficiency of compressed LLMs.", "AI": {"tldr": "SRD \u662f\u4e00\u79cd\u65b0\u7684\u6570\u636e\u7cbe\u9009\u6846\u67b6\uff0c\u5229\u7528\u5b66\u751f\u6a21\u578b\u7684\u8f93\u51fa\u6765\u4f18\u5316\u8bad\u7ec3\u6570\u636e\uff0c\u63d0\u5347\u4e86\u77e5\u8bc6\u84b8\u998f\u7684\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u767d\u76d2\u77e5\u8bc6\u84b8\u998f\uff08KD\uff09\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5e73\u8861\u771f\u5b9e\u6807\u7b7e\u548c\u5b66\u751f\u6a21\u578b\u751f\u6210\u7684\u54cd\u5e94\uff0c\u5374\u5ffd\u7565\u4e86\u8bad\u7ec3\u6570\u636e\u8d28\u91cf\u548c\u5b66\u751f\u6a21\u578b\u517c\u5bb9\u6027\u8fd9\u4e24\u4e2a\u5173\u952e\u56e0\u7d20\u3002", "method": "SRD \u901a\u8fc7\u6bd4\u8f83\u771f\u5b9e\u6807\u7b7e\u6570\u636e\u548c\u5b66\u751f\u6a21\u578b\u8f93\u51fa\u6765\u52a8\u6001\u8bc4\u4f30\u548c\u9009\u62e9\u63d0\u793a-\u54cd\u5e94\u5bf9\uff0c\u5e76\u57fa\u4e8e\u96be\u5ea6\u81ea\u52a8\u6392\u5e8f\uff0c\u4ece\u800c\u7cfb\u7edf\u5730\u4f18\u5316\u8bad\u7ec3\u6570\u636e\u3002\u5728\u9009\u5b9a\u8bad\u7ec3\u6570\u636e\u540e\uff0c\u91c7\u7528\u8bfe\u7a0b\u8c03\u5ea6\u7b56\u7565\u5728\u56fa\u5b9a\u95f4\u9694\u5185\u9010\u6b65\u5c06\u8fd9\u4e9b\u7cbe\u9009\u7684\u5b50\u96c6\u5f15\u5165\u84b8\u998f\u8fc7\u7a0b\u3002", "result": "SRD \u80fd\u591f\u8de8\u8d8a\u591a\u79cd\u767d\u76d2 KD \u65b9\u6cd5\u548c\u6a21\u578b\u67b6\u6784\uff0c\u4e00\u81f4\u6027\u5730\u6539\u5584\u84b8\u998f\u7ed3\u679c\uff0c\u5e76\u663e\u8457\u964d\u4f4e KD \u8bad\u7ec3\u671f\u95f4\u7684\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "SRD \u4f5c\u4e3a\u4e00\u4e2a\u5373\u63d2\u5373\u7528\u6a21\u5757\uff0c\u63d0\u9ad8\u4e86\u6837\u672c\u6548\u7387\uff0c\u5e76\u4e14\u4e0d\u9700\u8981\u4fee\u6539\u5e95\u5c42\u77e5\u8bc6\u84b8\u998f\u7b97\u6cd5\u3002\u5b9e\u9a8c\u8bc1\u660e SRD \u5728\u591a\u79cd\u8bed\u8a00\u6a21\u578b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u80fd\u591f\u4e00\u81f4\u6027\u5730\u63d0\u5347\u84b8\u998f\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5e76\u80fd\u5c06\u8bad\u7ec3\u65f6\u95f4\u6700\u591a\u51cf\u5c11 39%\u3002\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u6570\u636e\u8d28\u91cf\u548c\u517c\u5bb9\u6027\u5bf9\u4e8e LLM \u6709\u6548\u548c\u9ad8\u6548\u84b8\u998f\u7684\u5173\u952e\u6027\uff0c\u5e76\u4e3a\u5b9e\u73b0\u8fd9\u4e24\u8005\u63d0\u4f9b\u4e86\u4e00\u4e2a\u539f\u5219\u6027\u7684\u6846\u67b6\u3002"}}
{"id": "2508.05954", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.05954", "abs": "https://arxiv.org/abs/2508.05954", "authors": ["Han Lin", "Jaemin Cho", "Amir Zadeh", "Chuan Li", "Mohit Bansal"], "title": "Bifrost-1: Bridging Multimodal LLMs and Diffusion Models with Patch-level CLIP Latents", "comment": "Project Page: https://bifrost-1.github.io", "summary": "There is growing interest in integrating high-fidelity visual synthesis\ncapabilities into large language models (LLMs) without compromising their\nstrong reasoning capabilities. Existing methods that directly train LLMs or\nbridge LLMs and diffusion models usually suffer from costly training since the\nbackbone LLMs have not seen image representations during pretraining. We\npresent Bifrost-1, a unified framework that bridges pretrained multimodal LLMs\n(MLLMs) and diffusion models using patch-level CLIP image embeddings as latent\nvariables, which are natively aligned with the MLLM's CLIP visual encoder.\nThese patch-level image embeddings are integrated into the diffusion model with\na lightweight adaptation of its ControlNet. To retain the original multimodal\nreasoning capabilities of MLLMs, we equip the MLLM with a visual generation\nbranch initialized from the original MLLM parameters when predicting the\npatch-level image embeddings. By seamlessly integrating pretrained MLLMs and\ndiffusion models with patch-level CLIP latents, our framework enables\nhigh-fidelity controllable image generation with significant training\nefficiency. Our experiments demonstrate that Bifrost-1 achieves comparable or\nbetter performance than previous methods in terms of visual fidelity and\nmultimodal understanding, with substantially lower compute during training. We\nalso provide comprehensive ablation studies showing the effectiveness of our\ndesign choices.", "AI": {"tldr": "Bifrost-1\u662f\u4e00\u4e2a\u521b\u65b0\u7684\u6846\u67b6\uff0c\u5b83\u5c06\u9884\u8bad\u7ec3\u7684\u591a\u6a21\u6001LLM\u4e0e\u6269\u6563\u6a21\u578b\u76f8\u7ed3\u5408\uff0c\u4f7f\u7528\u8865\u4e01\u7ea7CLIP\u56fe\u50cf\u5d4c\u5165\u4f5c\u4e3a\u6865\u6881\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\u751f\u6210\uff0c\u540c\u65f6\u4fdd\u7559\u4e86LLM\u5f3a\u5927\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5c06\u9ad8\u4fdd\u771f\u89c6\u89c9\u5408\u6210\u80fd\u529b\u96c6\u6210\u5230\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e2d\u65f6\uff0c\u901a\u5e38\u9762\u4e34\u6602\u8d35\u7684\u8bad\u7ec3\u6210\u672c\uff0c\u56e0\u4e3aLLM\u5728\u9884\u8bad\u7ec3\u671f\u95f4\u672a\u63a5\u89e6\u8fc7\u56fe\u50cf\u8868\u793a\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "Bifrost-1\u6846\u67b6\u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u591a\u6a21\u6001LLM\u548c\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u8865\u4e01\u7ea7CLIP\u56fe\u50cf\u5d4c\u5165\u4f5c\u4e3a\u6f5c\u5728\u53d8\u91cf\u8fdb\u884c\u8fde\u63a5\u3002\u5b83\u901a\u8fc7\u8f7b\u91cf\u7ea7\u5730\u8c03\u6574ControlNet\u6765\u96c6\u6210\u8fd9\u4e9b\u5d4c\u5165\uff0c\u5e76\u4e3aLLM\u914d\u5907\u4e86\u4e00\u4e2a\u89c6\u89c9\u751f\u6210\u5206\u652f\uff0c\u8be5\u5206\u652f\u4ece\u539f\u59cbLLM\u53c2\u6570\u521d\u59cb\u5316\uff0c\u7528\u4e8e\u9884\u6d4b\u8865\u4e01\u7ea7\u56fe\u50cf\u5d4c\u5165\u3002", "result": "Bifrost-1\u5728\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u591a\u6a21\u6001\u7406\u89e3\u65b9\u9762\u53d6\u5f97\u4e86\u4e0e\u5148\u524d\u65b9\u6cd5\u76f8\u5f53\u6216\u66f4\u4f18\u7684\u6027\u80fd\uff0c\u540c\u65f6\u8bad\u7ec3\u8ba1\u7b97\u91cf\u663e\u8457\u964d\u4f4e\u3002\u6d88\u878d\u7814\u7a76\u4e5f\u8bc1\u660e\u4e86\u5176\u8bbe\u8ba1\u9009\u62e9\u7684\u6709\u6548\u6027\u3002", "conclusion": "Bifrost-1\u6846\u67b6\u901a\u8fc7\u96c6\u6210\u9884\u8bad\u7ec3\u7684\u591a\u6a21\u6001LLM\u548c\u6269\u6563\u6a21\u578b\uff0c\u5e76\u5229\u7528\u56fe\u50cf\u5d4c\u5165\u4f5c\u4e3a\u6f5c\u5728\u53d8\u91cf\uff0c\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u53ef\u63a7\u56fe\u50cf\u751f\u6210\uff0c\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u4e86\u8bad\u7ec3\u6548\u7387\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0cBifrost-1\u5728\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u591a\u6a21\u6001\u7406\u89e3\u65b9\u9762\u8fbe\u5230\u4e86\u4e0e\u5148\u524d\u65b9\u6cd5\u76f8\u5f53\u6216\u66f4\u4f18\u7684\u6027\u80fd\uff0c\u4e14\u8bad\u7ec3\u8ba1\u7b97\u91cf\u5927\u5927\u964d\u4f4e\u3002"}}
{"id": "2508.06291", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06291", "abs": "https://arxiv.org/abs/2508.06291", "authors": ["Christian Rauch", "Bj\u00f6rn Ellensohn", "Linus Nwankwo", "Vedant Dave", "Elmar Rueckert"], "title": "Real-Time 3D Vision-Language Embedding Mapping", "comment": null, "summary": "A metric-accurate semantic 3D representation is essential for many robotic\ntasks. This work proposes a simple, yet powerful, way to integrate the 2D\nembeddings of a Vision-Language Model in a metric-accurate 3D representation at\nreal-time. We combine a local embedding masking strategy, for a more distinct\nembedding distribution, with a confidence-weighted 3D integration for more\nreliable 3D embeddings. The resulting metric-accurate embedding representation\nis task-agnostic and can represent semantic concepts on a global multi-room, as\nwell as on a local object-level. This enables a variety of interactive robotic\napplications that require the localisation of objects-of-interest via natural\nlanguage. We evaluate our approach on a variety of real-world sequences and\ndemonstrate that these strategies achieve a more accurate object-of-interest\nlocalisation while improving the runtime performance in order to meet our\nreal-time constraints. We further demonstrate the versatility of our approach\nin a variety of interactive handheld, mobile robotics and manipulation tasks,\nrequiring only raw image data.", "AI": {"tldr": "This paper presents a real-time method to create accurate 3D semantic representations for robots by integrating 2D vision-language embeddings, improving object localization and performance in various robotic tasks.", "motivation": "To create a metric-accurate semantic 3D representation for robotic tasks by integrating 2D embeddings from a Vision-Language Model in real-time.", "method": "The paper proposes integrating 2D embeddings from a Vision-Language Model into a metric-accurate 3D representation using a local embedding masking strategy and confidence-weighted 3D integration.", "result": "The approach results in a metric-accurate embedding representation that enables accurate object-of-interest localization and improved runtime performance, meeting real-time constraints.", "conclusion": "The proposed method achieves accurate object localization and improves runtime performance for real-time robotic applications, demonstrating versatility across various robotic tasks using only raw image data."}}
{"id": "2508.06226", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06226", "abs": "https://arxiv.org/abs/2508.06226", "authors": ["Yumeng Fu", "Jiayin Zhu", "Lingling Zhang", "Bo Zhao", "Shaoxuan Ma", "Yushun Zhang", "Yanrui Wu", "Wenjun Wu"], "title": "GeoLaux: A Benchmark for Evaluating MLLMs' Geometry Performance on Long-Step Problems Requiring Auxiliary Lines", "comment": null, "summary": "Geometry problem solving (GPS) requires models to master diagram\ncomprehension, logical reasoning, knowledge application, numerical computation,\nand auxiliary line construction. This presents a significant challenge for\nMultimodal Large Language Models (MLLMs). However, existing benchmarks for\nevaluating MLLM geometry skills overlook auxiliary line construction and lack\nfine-grained process evaluation, making them insufficient for assessing MLLMs'\nlong-step reasoning abilities. To bridge these gaps, we present the GeoLaux\nbenchmark, comprising 2,186 geometry problems, incorporating both calculation\nand proving questions. Notably, the problems require an average of 6.51\nreasoning steps, with a maximum of 24 steps, and 41.8% of them need auxiliary\nline construction. Building on the dataset, we design a novel five-dimensional\nevaluation strategy assessing answer correctness, process correctness, process\nquality, auxiliary line impact, and error causes. Extensive experiments on 13\nleading MLLMs (including thinking models and non-thinking models) yield three\npivotal findings: First, models exhibit substantial performance degradation in\nextended reasoning steps (nine models demonstrate over 50% performance drop).\nSecond, compared to calculation problems, MLLMs tend to take shortcuts when\nsolving proving problems. Third, models lack auxiliary line awareness, and\nenhancing this capability proves particularly beneficial for overall geometry\nreasoning improvement. These findings establish GeoLaux as both a benchmark for\nevaluating MLLMs' long-step geometric reasoning with auxiliary lines and a\nguide for capability advancement. Our dataset and code are included in\nsupplementary materials and will be released.", "AI": {"tldr": "GeoLaux \u662f\u4e00\u4e2a\u5305\u542b 2,186 \u9053\u51e0\u4f55\u9898\u7684\u65b0\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30 MLLM \u7684\u957f\u6b65\u9aa4\u63a8\u7406\u548c\u8f85\u52a9\u7ebf\u6784\u5efa\u80fd\u529b\u3002\u5b9e\u9a8c\u53d1\u73b0 MLLM \u5728\u957f\u63a8\u7406\u3001\u8bc1\u660e\u9898\u548c\u8f85\u52a9\u7ebf\u7406\u89e3\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u4f46\u63d0\u5347\u8f85\u52a9\u7ebf\u80fd\u529b\u53ef\u663e\u8457\u6539\u5584\u5176\u51e0\u4f55\u63a8\u7406\u8868\u73b0\u3002", "motivation": "\u73b0\u6709 MLLM \u51e0\u4f55\u80fd\u529b\u8bc4\u4f30\u57fa\u51c6\u5ffd\u7565\u4e86\u8f85\u52a9\u7ebf\u6784\u5efa\uff0c\u5e76\u4e14\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u8fc7\u7a0b\u8bc4\u4f30\uff0c\u4e0d\u8db3\u4ee5\u8861\u91cf MLLM \u7684\u957f\u6b65\u9aa4\u63a8\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51fa GeoLaux \u57fa\u51c6\uff0c\u5305\u542b 2,186 \u9053\u51e0\u4f55\u9898\uff08\u8ba1\u7b97\u9898\u548c\u8bc1\u660e\u9898\uff09\uff0c\u5e73\u5747\u63a8\u7406\u6b65\u9aa4 6.51 \u6b65\uff0c41.8% \u7684\u9898\u76ee\u9700\u8981\u8f85\u52a9\u7ebf\u3002\u8bbe\u8ba1\u4e86\u5305\u62ec\u7b54\u6848\u6b63\u786e\u6027\u3001\u8fc7\u7a0b\u6b63\u786e\u6027\u3001\u8fc7\u7a0b\u8d28\u91cf\u3001\u8f85\u52a9\u7ebf\u5f71\u54cd\u548c\u9519\u8bef\u539f\u56e0\u7684\u4e94\u7ef4\u5ea6\u8bc4\u4f30\u7b56\u7565\u3002", "result": "\u5728 13 \u4e2a\u9886\u5148 MLLM \u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff1a1. \u63a8\u7406\u6b65\u9aa4\u8d8a\u957f\uff0c\u6a21\u578b\u6027\u80fd\u4e0b\u964d\u8d8a\u660e\u663e\uff089 \u4e2a\u6a21\u578b\u6027\u80fd\u4e0b\u964d\u8d85 50%\uff09\uff1b2. MLLM \u5728\u89e3\u51b3\u8bc1\u660e\u9898\u65f6\u503e\u5411\u4e8e\u91c7\u53d6\u6377\u5f84\uff1b3. \u6a21\u578b\u7f3a\u4e4f\u5bf9\u8f85\u52a9\u7ebf\u7684\u7406\u89e3\uff0c\u63d0\u5347\u6b64\u80fd\u529b\u5bf9\u6574\u4f53\u51e0\u4f55\u63a8\u7406\u975e\u5e38\u6709\u76ca\u3002", "conclusion": "GeoLaux \u586b\u8865\u4e86\u73b0\u6709\u51e0\u4f55\u95ee\u9898\u89e3\u51b3\u57fa\u51c6\u5728\u8f85\u52a9\u7ebf\u6784\u5efa\u548c\u7ec6\u7c92\u5ea6\u8fc7\u7a0b\u8bc4\u4f30\u65b9\u9762\u7684\u7a7a\u767d\uff0c\u4e3a\u8bc4\u4f30 MLLM \u7684\u957f\u6b65\u9aa4\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u65b0\u7684\u6807\u51c6\u3002\u5b9e\u9a8c\u7ed3\u679c\u63ed\u793a\u4e86 MLLM \u5728\u957f\u63a8\u7406\u6b65\u9aa4\u3001\u8bc1\u660e\u9898\u548c\u8f85\u52a9\u7ebf\u7406\u89e3\u65b9\u9762\u5b58\u5728\u7684\u6311\u6218\uff0c\u5e76\u5f3a\u8c03\u4e86\u63d0\u5347\u8f85\u52a9\u7ebf\u80fd\u529b\u5bf9\u6539\u8fdb\u51e0\u4f55\u63a8\u7406\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2508.06034", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06034", "abs": "https://arxiv.org/abs/2508.06034", "authors": ["Qin Chen", "Guojie Song"], "title": "Adaptive Heterogeneous Graph Neural Networks: Bridging Heterophily and Heterogeneity", "comment": "Accepted tp CIKM 2025", "summary": "Heterogeneous graphs (HGs) are common in real-world scenarios and often\nexhibit heterophily. However, most existing studies focus on either\nheterogeneity or heterophily in isolation, overlooking the prevalence of\nheterophilic HGs in practical applications. Such ignorance leads to their\nperformance degradation. In this work, we first identify two main challenges in\nmodeling heterophily HGs: (1) varying heterophily distributions across hops and\nmeta-paths; (2) the intricate and often heterophily-driven diversity of\nsemantic information across different meta-paths. Then, we propose the Adaptive\nHeterogeneous Graph Neural Network (AHGNN) to tackle these challenges. AHGNN\nemploys a heterophily-aware convolution that accounts for heterophily\ndistributions specific to both hops and meta-paths. It then integrates messages\nfrom diverse semantic spaces using a coarse-to-fine attention mechanism, which\nfilters out noise and emphasizes informative signals. Experiments on seven\nreal-world graphs and twenty baselines demonstrate the superior performance of\nAHGNN, particularly in high-heterophily situations.", "AI": {"tldr": "AHGNN\u662f\u4e00\u79cd\u65b0\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\uff0c\u53ef\u4ee5\u5904\u7406\u5f02\u8d28\u6027\u5f02\u6784\u56fe\u3002", "motivation": "\u5927\u591a\u6570\u73b0\u6709\u7814\u7a76\u8981\u4e48\u5b64\u7acb\u5730\u5173\u6ce8\u5f02\u6784\u6027\uff0c\u8981\u4e48\u5b64\u7acb\u5730\u5173\u6ce8\u5f02\u8d28\u6027\uff0c\u5374\u5ffd\u7565\u4e86\u5b9e\u9645\u5e94\u7528\u4e2d\u5f02\u8d28\u6027\u5f02\u6784\u56fe\u7684\u666e\u904d\u5b58\u5728\uff0c\u8fd9\u4f1a\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "method": "AHGNN\u91c7\u7528\u4e00\u79cd\u8003\u8651\u4e86\u7279\u5b9a\u4e8e\u8df3\u6570\u548c\u5143\u8def\u5f84\u7684\u5f02\u8d28\u6027\u5206\u5e03\u7684\u5f02\u8d28\u6027\u611f\u77e5\u5377\u79ef\u3002\u7136\u540e\uff0c\u5b83\u4f7f\u7528\u4e00\u79cd\u7531\u7c97\u5230\u7ec6\u7684\u6ce8\u610f\u529b\u673a\u5236\u6765\u6574\u5408\u6765\u81ea\u4e0d\u540c\u8bed\u4e49\u7a7a\u95f4\u7684\u6d88\u606f\uff0c\u4ee5\u8fc7\u6ee4\u566a\u58f0\u5e76\u5f3a\u8c03\u4fe1\u606f\u4fe1\u53f7\u3002", "result": "AHGNN\u5728\u4e03\u4e2a\u73b0\u5b9e\u4e16\u754c\u7684\u56fe\u548c\u4e8c\u5341\u4e2a\u57fa\u7ebf\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u5176\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u9ad8\u5f02\u8d28\u6027\u60c5\u51b5\u4e0b\u3002", "conclusion": "AHGNN\u5728\u73b0\u5b9e\u4e16\u754c\u7684\u4e03\u4e2a\u56fe\u548c\u4e8c\u5341\u4e2a\u57fa\u7ebf\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u5e76\u4e14\u8868\u73b0\u51fa\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u9ad8\u5f02\u8d28\u6027\u60c5\u51b5\u4e0b\u3002"}}
{"id": "2508.06130", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.06130", "abs": "https://arxiv.org/abs/2508.06130", "authors": ["Pierre Cazals", "Amalia Sorondo", "Victor Onofre", "Constantin Dalyac", "Wesley da Silva Coelho", "Vittorio Vitale"], "title": "Quantum Optimization on Rydberg Atom Arrays with Arbitrary Connectivity: Gadgets Limitations and a Heuristic Approach", "comment": "13 pages, 4 figures", "summary": "Programmable quantum systems based on Rydberg atom arrays have recently\nemerged as a promising testbed for combinatorial optimization. Indeed, the\nMaximum Weighted Independent Set problem on unit-disk graphs can be efficiently\nmapped to such systems due to their geometric constraints. However, extending\nthis capability to arbitrary graph instances typically necessitates the use of\nreduction gadgets, which introduce additional experimental overhead and\ncomplexity. Here, we analyze the complexity-theoretic limits of polynomial\nreductions from arbitrary graphs to unit-disk instances. We prove any such\nreduction incurs a quadratic blow-up in vertex count and degrades solution\napproximation guarantees. As a practical alternative, we propose a\ndivide-and-conquer heuristic with only linear overhead which leverages\nprecalibrated atomic layouts. We benchmark it on Erd\\\"os-R\\'enyi graphs, and\ndemonstrate feasibility on the Orion Alpha processor.", "AI": {"tldr": "\u57fa\u4e8eRydberg\u539f\u5b50\u9635\u5217\u7684\u91cf\u5b50\u7cfb\u7edf\u5728\u7ec4\u5408\u4f18\u5316\u65b9\u9762\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46\u5c06\u4efb\u610f\u56fe\u5b9e\u4f8b\u6620\u5c04\u5230\u8be5\u7cfb\u7edf\u5b58\u5728\u6311\u6218\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u5206\u6cbb\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u5e76\u8bc1\u660e\u4e86\u591a\u9879\u5f0f\u5f52\u7ea6\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5c06\u4efb\u610f\u56fe\u5b9e\u4f8b\u6620\u5c04\u5230\u57fa\u4e8eRydberg\u539f\u5b50\u9635\u5217\u7684\u53ef\u7f16\u7a0b\u91cf\u5b50\u7cfb\u7edf\u65f6\uff0c\u4f7f\u7528\u5f52\u7ea6\u5c0f\u5de5\u5177\u5e26\u6765\u7684\u5b9e\u9a8c\u5f00\u9500\u548c\u590d\u6742\u6027\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u6cbb\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u7ebf\u6027\u5f00\u9500\uff0c\u5e76\u5229\u7528\u9884\u6821\u51c6\u7684\u539f\u5b50\u5e03\u5c40\u3002", "result": "\u8bc1\u660e\u4e86\u4ece\u4efb\u610f\u56fe\u5230\u5355\u4f4d\u78c1\u76d8\u5b9e\u4f8b\u7684\u4efb\u4f55\u591a\u9879\u5f0f\u5f52\u7ea6\u90fd\u4f1a\u5bfc\u81f4\u9876\u70b9\u6570\u91cf\u7684\u4e8c\u6b21\u589e\u957f\uff0c\u5e76\u964d\u4f4e\u4e86\u89e3\u51b3\u65b9\u6848\u7684\u8fd1\u4f3c\u4fdd\u8bc1\u3002", "conclusion": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u89e3\u51b3\u4efb\u610f\u56fe\u5b9e\u4f8b\u7684\u6700\u5927\u52a0\u6743\u72ec\u7acb\u96c6\u95ee\u9898\u7684\u5206\u6cbb\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4ec5\u5177\u6709\u7ebf\u6027\u5f00\u9500\uff0c\u5e76\u4e14\u53ef\u4ee5\u5728Orion Alpha\u5904\u7406\u5668\u4e0a\u8fd0\u884c\u3002"}}
{"id": "2508.05976", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.05976", "abs": "https://arxiv.org/abs/2508.05976", "authors": ["Zhihao Zhu", "Yifan Zheng", "Siyu Pan", "Yaohui Jin", "Yao Mu"], "title": "PASG: A Closed-Loop Framework for Automated Geometric Primitive Extraction and Semantic Anchoring in Robotic Manipulation", "comment": "Accepted to ICCV 2025. 8 pages main paper, 8 figures, plus\n  supplementary material", "summary": "The fragmentation between high-level task semantics and low-level geometric\nfeatures remains a persistent challenge in robotic manipulation. While\nvision-language models (VLMs) have shown promise in generating affordance-aware\nvisual representations, the lack of semantic grounding in canonical spaces and\nreliance on manual annotations severely limit their ability to capture dynamic\nsemantic-affordance relationships. To address these, we propose Primitive-Aware\nSemantic Grounding (PASG), a closed-loop framework that introduces: (1)\nAutomatic primitive extraction through geometric feature aggregation, enabling\ncross-category detection of keypoints and axes; (2) VLM-driven semantic\nanchoring that dynamically couples geometric primitives with functional\naffordances and task-relevant description; (3) A spatial-semantic reasoning\nbenchmark and a fine-tuned VLM (Qwen2.5VL-PA). We demonstrate PASG's\neffectiveness in practical robotic manipulation tasks across diverse scenarios,\nachieving performance comparable to manual annotations. PASG achieves a\nfiner-grained semantic-affordance understanding of objects, establishing a\nunified paradigm for bridging geometric primitives with task semantics in\nrobotic manipulation.", "AI": {"tldr": "PASG\u6846\u67b6\u901a\u8fc7\u81ea\u52a8\u63d0\u53d6\u51e0\u4f55\u539f\u59cb\u7279\u5f81\u5e76\u5229\u7528VLM\u8fdb\u884c\u8bed\u4e49\u951a\u5b9a\uff0c\u89e3\u51b3\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u8bed\u4e49\u4e0e\u51e0\u4f55\u7684\u9e3f\u6c9f\uff0c\u63d0\u9ad8\u4e86\u7406\u89e3\u7684\u7cbe\u7ec6\u5ea6\u548c\u4efb\u52a1\u8868\u73b0\u3002", "motivation": "\u89e3\u51b3\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u9ad8\u7ea7\u4efb\u52a1\u8bed\u4e49\u4e0e\u4f4e\u7ea7\u51e0\u4f55\u7279\u5f81\u4e4b\u95f4\u788e\u7247\u5316\u7684\u95ee\u9898\uff0c\u4ee5\u53ca\u73b0\u6709VLM\u5728\u8bed\u4e49\u63a5\u5730\u3001\u52a8\u6001\u8bed\u4e49-\u80fd\u529b\u5173\u7cfb\u6355\u6349\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPASG\uff08Primitive-Aware Semantic Grounding\uff09\u7684\u95ed\u73af\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5305\u542b\u81ea\u52a8\u539f\u59cb\u7279\u5f81\u63d0\u53d6\u3001VLM\u9a71\u52a8\u7684\u8bed\u4e49\u951a\u5b9a\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u7a7a\u95f4-\u8bed\u4e49\u63a8\u7406\u57fa\u51c6\u548c\u5fae\u8c03\u7684VLM\uff08Qwen2.5VL-PA\uff09\u3002", "result": "PASG\u5728\u5b9e\u9645\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u5c55\u73b0\u4e86\u6709\u6548\u6027\uff0c\u8de8\u8d8a\u4e86\u591a\u6837\u5316\u573a\u666f\uff0c\u5728\u7ec6\u7c92\u5ea6\u7684\u8bed\u4e49-\u80fd\u529b\u7406\u89e3\u65b9\u9762\u53d6\u5f97\u4e86\u4e0e\u624b\u52a8\u6807\u6ce8\u76f8\u5ab2\u7f8e\u7684\u6027\u80fd\u3002", "conclusion": "PASG\u6846\u67b6\u5b9e\u73b0\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u51e0\u4f55\u539f\u59cb\u7279\u5f81\u4e0e\u4efb\u52a1\u8bed\u4e49\u7684\u7edf\u4e00\uff0c\u80fd\u591f\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u7684\u8bed\u4e49-\u80fd\u529b\u7406\u89e3\uff0c\u5e76\u4e14\u5728\u5b9e\u9645\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6027\u80fd\u53ef\u5ab2\u7f8e\u624b\u52a8\u6807\u6ce8\u3002"}}
{"id": "2508.06295", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06295", "abs": "https://arxiv.org/abs/2508.06295", "authors": ["Juan Heredia", "Emil Stubbe Kolvig-Raun", "Sune Lundo Sorensen", "Mikkel Baun Kjaergaard"], "title": "Evaluating Robot Program Performance with Power Consumption Driven Metrics in Lightweight Industrial Robots", "comment": null, "summary": "The code performance of industrial robots is typically analyzed through CPU\nmetrics, which overlook the physical impact of code on robot behavior. This\nstudy introduces a novel framework for assessing robot program performance from\nan embodiment perspective by analyzing the robot's electrical power profile.\nOur approach diverges from conventional CPU based evaluations and instead\nleverages a suite of normalized metrics, namely, the energy utilization\ncoefficient, the energy conversion metric, and the reliability coefficient, to\ncapture how efficiently and reliably energy is used during task execution.\nComplementing these metrics, the established robot wear metric provides further\ninsight into long term reliability. Our approach is demonstrated through an\nexperimental case study in machine tending, comparing four programs with\ndiverse strategies using a UR5e robot. The proposed metrics directly compare\nand categorize different robot programs, regardless of the specific task, by\nlinking code performance to its physical manifestation through power\nconsumption patterns. Our results reveal the strengths and weaknesses of each\nstrategy, offering actionable insights for optimizing robot programming\npractices. Enhancing energy efficiency and reliability through this embodiment\ncentric approach not only improves individual robot performance but also\nsupports broader industrial objectives such as sustainable manufacturing and\ncost reduction.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u4ece\u5177\u8eab\u89d2\u5ea6\u901a\u8fc7\u5206\u6790\u673a\u5668\u4eba\u7535\u80fd\u5256\u9762\u6765\u8bc4\u4f30\u673a\u5668\u4eba\u7a0b\u5e8f\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfCPU\u6307\u6807\u5ffd\u89c6\u4ee3\u7801\u7269\u7406\u5f71\u54cd\u7684\u95ee\u9898\u3002\u8be5\u6846\u67b6\u4f7f\u7528\u80fd\u91cf\u5229\u7528\u7cfb\u6570\u3001\u80fd\u91cf\u8f6c\u6362\u6307\u6807\u548c\u53ef\u9760\u6027\u7cfb\u6570\u7b49\u6307\u6807\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u4e3a\u4f18\u5316\u673a\u5668\u4eba\u7f16\u7a0b\u548c\u5b9e\u73b0\u53ef\u6301\u7eed\u5236\u9020\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002", "motivation": "\u4f20\u7edf\u7684\u5de5\u4e1a\u673a\u5668\u4eba\u4ee3\u7801\u6027\u80fd\u5206\u6790\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8CPU\u6307\u6807\uff0c\u800c\u5ffd\u89c6\u4e86\u4ee3\u7801\u5bf9\u673a\u5668\u4eba\u884c\u4e3a\u7684\u7269\u7406\u5f71\u54cd\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f25\u8865\u8fd9\u4e00\u4e0d\u8db3\uff0c\u63d0\u51fa\u4e00\u79cd\u4ece\u5177\u8eab\u89d2\u5ea6\u8bc4\u4f30\u673a\u5668\u4eba\u7a0b\u5e8f\u6027\u80fd\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u4ece\u5177\u8eab\u89d2\u5ea6\u8bc4\u4f30\u673a\u5668\u4eba\u7a0b\u5e8f\u6027\u80fd\uff0c\u901a\u8fc7\u5206\u6790\u673a\u5668\u4eba\u7535\u80fd\u5256\u9762\uff0c\u5e76\u5f15\u5165\u4e86\u80fd\u91cf\u5229\u7528\u7cfb\u6570\u3001\u80fd\u91cf\u8f6c\u6362\u6307\u6807\u548c\u53ef\u9760\u6027\u7cfb\u6570\u8fd9\u5957\u5f52\u4e00\u5316\u6307\u6807\uff0c\u540c\u65f6\u7ed3\u5408\u4e86\u673a\u5668\u4eba\u78e8\u635f\u5ea6\u6307\u6807\uff0c\u4ee5\u6355\u6349\u80fd\u6e90\u4f7f\u7528\u6548\u7387\u548c\u53ef\u9760\u6027\u3002", "result": "\u901a\u8fc7\u5728\u673a\u5668\u4eba\u5de5\u6599\u642c\u8fd0\u4efb\u52a1\u7684\u5b9e\u9a8c\u6848\u4f8b\u7814\u7a76\u4e2d\uff0c\u5bf9UR5e\u673a\u5668\u4eba\u4e0a\u7684\u56db\u79cd\u4e0d\u540c\u7b56\u7565\u7a0b\u5e8f\u8fdb\u884c\u8bc4\u4f30\uff0c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7684\u6307\u6807\u53ef\u4ee5\u76f4\u63a5\u6bd4\u8f83\u548c\u533a\u5206\u4e0d\u540c\u7684\u673a\u5668\u4eba\u7a0b\u5e8f\uff0c\u63ed\u793a\u4e86\u6bcf\u79cd\u7b56\u7565\u7684\u4f18\u7f3a\u70b9\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u4ece\u5177\u8eab\u89d2\u5ea6\u8bc4\u4f30\u673a\u5668\u4eba\u7a0b\u5e8f\u6027\u80fd\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790\u673a\u5668\u4eba\u7684\u7535\u80fd\u5256\u9762\uff0c\u80fd\u591f\u6709\u6548\u6355\u6349\u4ee3\u7801\u6027\u80fd\u7684\u7269\u7406\u4f53\u73b0\uff0c\u4e3a\u4f18\u5316\u673a\u5668\u4eba\u7f16\u7a0b\u5b9e\u8df5\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89c1\u89e3\uff0c\u5e76\u6709\u52a9\u4e8e\u5b9e\u73b0\u53ef\u6301\u7eed\u5236\u9020\u548c\u964d\u4f4e\u6210\u672c\u7684\u5de5\u4e1a\u76ee\u6807\u3002"}}
{"id": "2508.06230", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06230", "abs": "https://arxiv.org/abs/2508.06230", "authors": ["Ruben Sharma", "Sebastijan Duman\u010di\u0107", "Ross D. King", "Andrew Cropper"], "title": "Learning Logical Rules using Minimum Message Length", "comment": null, "summary": "Unifying probabilistic and logical learning is a key challenge in AI. We\nintroduce a Bayesian inductive logic programming approach that learns minimum\nmessage length programs from noisy data. Our approach balances hypothesis\ncomplexity and data fit through priors, which explicitly favour more general\nprograms, and a likelihood that favours accurate programs. Our experiments on\nseveral domains, including game playing and drug design, show that our method\nsignificantly outperforms previous methods, notably those that learn minimum\ndescription length programs. Our results also show that our approach is\ndata-efficient and insensitive to example balance, including the ability to\nlearn from exclusively positive examples.", "AI": {"tldr": "A Bayesian approach to inductive logic programming learns programs from noisy data more effectively than previous methods.", "motivation": "Unifying probabilistic and logical learning is a key challenge in AI.", "method": "Bayesian inductive logic programming, learning minimum message length programs from noisy data, balancing hypothesis complexity and data fit through priors and likelihood.", "result": "Our method significantly outperforms previous methods, notably those that learn minimum description length programs, and is data-efficient and insensitive to example balance, including learning from exclusively positive examples.", "conclusion": "We propose a Bayesian inductive logic programming approach that learns minimum message length programs from noisy data, outperforming previous methods in various domains and demonstrating data-efficiency and insensitivity to example balance."}}
{"id": "2508.06041", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06041", "abs": "https://arxiv.org/abs/2508.06041", "authors": ["Sangwoo Kwon", "Seong Hoon Seo", "Jae W. Lee", "Yeonhong Park"], "title": "DP-LLM: Runtime Model Adaptation with Dynamic Layer-wise Precision Assignment", "comment": null, "summary": "How can we effectively handle queries for on-device large language models\n(LLMs) with varying runtime constraints, such as latency and accuracy?\nMulti-scale quantization addresses this challenge by enabling memory-efficient\nruntime model adaptation of LLMs through the overlaying of multiple model\nvariants quantized to different bitwidths. Meanwhile, an important question\nstill remains open-ended: how can models be properly configured to match a\ntarget precision or latency? While mixed-precision offers a promising solution,\nwe take this further by leveraging the key observation that the sensitivity of\neach layer dynamically changes across decoding iterations. Building on this\ninsight, we introduce DP-LLM, a novel mechanism that dynamically assigns\nprecision to each layer based on input values. DP-LLM augments each linear\nlayer in an LLM with a precision selector that determines the bitwidth at\nruntime using a lightweight error estimator and threshold values learned\nthrough fine-tuning. Experimental results across multiple models and benchmarks\ndemonstrate that DP-LLM achieves a superior performance-latency trade-off,\noutperforming prior approaches.", "AI": {"tldr": "DP-LLM \u662f\u4e00\u79cd\u65b0\u7684\u673a\u5236\uff0c\u53ef\u4ee5\u6839\u636e\u8f93\u5165\u503c\u52a8\u6001\u5730\u4e3a LLM \u4e2d\u7684\u6bcf\u4e2a\u5c42\u5206\u914d\u7cbe\u5ea6\uff0c\u4ee5\u4f18\u5316\u5ef6\u8fdf\u548c\u7cbe\u5ea6\u3002", "motivation": "\u4e3a\u4e86\u6709\u6548\u5904\u7406\u5177\u6709\u4e0d\u540c\u8fd0\u884c\u65f6\u7ea6\u675f\uff08\u4f8b\u5982\u5ef6\u8fdf\u548c\u7cbe\u5ea6\uff09\u7684\u8bbe\u5907\u4e0a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u67e5\u8be2\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u52a8\u6001\u9002\u5e94\u8fd0\u884c\u65f6\u6a21\u578b\u914d\u7f6e\u7684\u65b9\u6cd5\u3002", "method": "DP-LLM \u901a\u8fc7\u4e3a LLM \u4e2d\u7684\u6bcf\u4e2a\u7ebf\u6027\u5c42\u589e\u52a0\u4e00\u4e2a\u7cbe\u5ea6\u9009\u62e9\u5668\uff0c\u8be5\u9009\u62e9\u5668\u5229\u7528\u8f7b\u91cf\u7ea7\u9519\u8bef\u4f30\u8ba1\u5668\u548c\u901a\u8fc7\u5fae\u8c03\u5b66\u4e60\u7684\u9608\u503c\uff0c\u5728\u8fd0\u884c\u65f6\u786e\u5b9a\u6bd4\u7279\u5bbd\u5ea6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDP-LLM \u5728\u591a\u4e2a\u6a21\u578b\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u6027\u80fd-\u5ef6\u8fdf\u6743\u8861\u3002", "conclusion": "DP-LLM \u901a\u8fc7\u52a8\u6001\u5206\u914d\u7cbe\u5ea6\u5230\u6bcf\u4e2a\u5c42\uff0c\u5b9e\u73b0\u4e86\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\u7684\u6027\u80fd-\u5ef6\u8fdf\u6743\u8861\u3002"}}
{"id": "2508.06131", "categories": ["quant-ph", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06131", "abs": "https://arxiv.org/abs/2508.06131", "authors": ["Philip Anton Hernicht", "Alona Sakhnenko", "Corey O'Meara", "Giorgio Cortiana", "Jeanette Miriam Lorenz"], "title": "Enhancing the Scalability of Classical Surrogates for Real-World Quantum Machine Learning Applications", "comment": "9 pages, 8 figures", "summary": "Quantum machine learning (QML) presents potential for early industrial\nadoption, yet limited access to quantum hardware remains a significant\nbottleneck for deployment of QML solutions. This work explores the use of\nclassical surrogates to bypass this restriction, which is a technique that\nallows to build a lightweight classical representation of a (trained) quantum\nmodel, enabling to perform inference on entirely classical devices. We reveal\nprohibiting high computational demand associated with previously proposed\nmethods for generating classical surrogates from quantum models, and propose an\nalternative pipeline enabling generation of classical surrogates at a larger\nscale than was previously possible. Previous methods required at least a\nhigh-performance computing (HPC) system for quantum models of below industrial\nscale (ca. 20 qubits), which raises questions about its practicality. We\ngreatly minimize the redundancies of the previous approach, utilizing only a\nminute fraction of the resources previously needed. We demonstrate the\neffectiveness of our method on a real-world energy demand forecasting problem,\nconducting rigorous testing of performance and computation demand in both\nsimulations and on quantum hardware. Our results indicate that our method\nachieves high accuracy on the testing dataset while its computational resource\nrequirements scale linearly rather than exponentially. This work presents a\nlightweight approach to transform quantum solutions into classically deployable\nversions, facilitating faster integration of quantum technology in industrial\nsettings. Furthermore, it can serve as a powerful research tool in search\npractical quantum advantage in an empirical setup.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u521b\u5efa\u7ecf\u5178\u4ee3\u7406\u6a21\u578b\uff0c\u4ece\u800c\u80fd\u591f\u5728\u7ecf\u5178\u8bbe\u5907\u4e0a\u8fd0\u884c\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u91cf\u5b50\u786c\u4ef6\u53ef\u7528\u6027\u9650\u5236\u7684\u95ee\u9898\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u91cf\u5b50\u673a\u5668\u5b66\u4e60\uff08QML\uff09\u5728\u5de5\u4e1a\u5e94\u7528\u4e2d\u53d7\u9650\u4e8e\u91cf\u5b50\u786c\u4ef6\u53ef\u7528\u6027\u7684\u95ee\u9898\uff0c\u672c\u7814\u7a76\u63a2\u7d22\u4e86\u4f7f\u7528\u7ecf\u5178\u4ee3\u7406\u6a21\u578b\u6765\u7ed5\u8fc7\u8fd9\u4e00\u9650\u5236\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u66ff\u4ee3\u6027\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u5148\u524d\u65b9\u6cd5\u7684\u5197\u4f59\u6765\u751f\u6210\u7ecf\u5178\u4ee3\u7406\u6a21\u578b\uff0c\u4ece\u800c\u80fd\u591f\u4ee5\u524d\u6240\u672a\u6709\u7684\u89c4\u6a21\u751f\u6210\u7ecf\u5178\u4ee3\u7406\u6a21\u578b\uff0c\u5e76\u5927\u5927\u51cf\u5c11\u4e86\u6240\u9700\u7684\u8ba1\u7b97\u8d44\u6e90\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u80fd\u6e90\u9700\u6c42\u9884\u6d4b\u95ee\u9898\u4e0a\u8868\u73b0\u51fa\u6709\u6548\u6027\uff0c\u5e76\u5728\u6a21\u62df\u548c\u91cf\u5b50\u786c\u4ef6\u4e0a\u8fdb\u884c\u4e86\u4e25\u683c\u7684\u6027\u80fd\u548c\u8ba1\u7b97\u9700\u6c42\u6d4b\u8bd5\u3002\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6d4b\u8bd5\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u51c6\u786e\u6027\uff0c\u5e76\u4e14\u5176\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u662f\u7ebf\u6027\u6269\u5c55\u800c\u975e\u6307\u6570\u6269\u5c55\u7684\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u91cf\u5b50\u6a21\u578b\u8f6c\u5316\u4e3a\u7ecf\u5178\u53ef\u90e8\u7f72\u7248\u672c\u7684\u65b0\u65b9\u6cd5\uff0c\u6709\u52a9\u4e8e\u52a0\u5feb\u91cf\u5b50\u6280\u672f\u5728\u5de5\u4e1a\u73af\u5883\u4e2d\u7684\u96c6\u6210\uff0c\u5e76\u53ef\u4f5c\u4e3a\u5728\u5b9e\u9645\u8bbe\u7f6e\u4e2d\u5bfb\u627e\u5b9e\u9645\u91cf\u5b50\u4f18\u52bf\u7684\u6709\u529b\u7814\u7a76\u5de5\u5177\u3002"}}
{"id": "2508.06155", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06155", "abs": "https://arxiv.org/abs/2508.06155", "authors": ["Renhan Zhang", "Lian Lian", "Zhen Qi", "Guiran Liu"], "title": "Semantic and Structural Analysis of Implicit Biases in Large Language Models: An Interpretable Approach", "comment": null, "summary": "This paper addresses the issue of implicit stereotypes that may arise during\nthe generation process of large language models. It proposes an interpretable\nbias detection method aimed at identifying hidden social biases in model\noutputs, especially those semantic tendencies that are not easily captured\nthrough explicit linguistic features. The method combines nested semantic\nrepresentation with a contextual contrast mechanism. It extracts latent bias\nfeatures from the vector space structure of model outputs. Using attention\nweight perturbation, it analyzes the model's sensitivity to specific social\nattribute terms, thereby revealing the semantic pathways through which bias is\nformed. To validate the effectiveness of the method, this study uses the\nStereoSet dataset, which covers multiple stereotype dimensions including\ngender, profession, religion, and race. The evaluation focuses on several key\nmetrics, such as bias detection accuracy, semantic consistency, and contextual\nsensitivity. Experimental results show that the proposed method achieves strong\ndetection performance across various dimensions. It can accurately identify\nbias differences between semantically similar texts while maintaining high\nsemantic alignment and output stability. The method also demonstrates high\ninterpretability in its structural design. It helps uncover the internal bias\nassociation mechanisms within language models. This provides a more transparent\nand reliable technical foundation for bias detection. The approach is suitable\nfor real-world applications where high trustworthiness of generated content is\nrequired.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u5d4c\u5957\u8bed\u4e49\u8868\u5f81\u548c\u4e0a\u4e0b\u6587\u5bf9\u6bd4\u673a\u5236\u7684\u53ef\u89e3\u91ca\u65b9\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4b\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u9690\u5f0f\u793e\u4f1a\u504f\u89c1\uff0c\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u6709\u6548\u4e14\u5177\u6709\u9ad8\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u8fc7\u7a0b\u4e2d\u53ef\u80fd\u51fa\u73b0\u7684\u9690\u5f0f\u523b\u677f\u5370\u8c61\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u504f\u5dee\u68c0\u6d4b\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc6\u522b\u6a21\u578b\u8f93\u51fa\u4e2d\u9690\u85cf\u7684\u3001\u4e0d\u6613\u901a\u8fc7\u663e\u6027\u8bed\u8a00\u7279\u5f81\u6355\u6349\u7684\u8bed\u4e49\u503e\u5411\u3002", "method": "\u7ed3\u5408\u5d4c\u5957\u8bed\u4e49\u8868\u5f81\u548c\u4e0a\u4e0b\u6587\u5bf9\u6bd4\u673a\u5236\uff0c\u63d0\u53d6\u8bcd\u5411\u91cf\u7a7a\u95f4\u7ed3\u6784\u4e2d\u7684\u6f5c\u5728\u504f\u5dee\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u6ce8\u610f\u529b\u6743\u91cd\u6270\u52a8\u5206\u6790\u6a21\u578b\u5bf9\u7279\u5b9a\u793e\u4f1a\u5c5e\u6027\u8bcd\u6c47\u7684\u654f\u611f\u5ea6\uff0c\u4ece\u800c\u63ed\u793a\u504f\u5dee\u5f62\u6210\u7684\u8bed\u4e49\u901a\u8def\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728StereoSet\u6570\u636e\u96c6\u4e0a\uff0c\u5728\u6027\u522b\u3001\u804c\u4e1a\u3001\u5b97\u6559\u548c\u79cd\u65cf\u7b49\u591a\u4e2a\u523b\u677f\u5370\u8c61\u7ef4\u5ea6\u4e0a\u5747\u8868\u73b0\u51fa\u826f\u597d\u7684\u68c0\u6d4b\u6548\u679c\uff0c\u80fd\u591f\u51c6\u786e\u8bc6\u522b\u504f\u5dee\uff0c\u4fdd\u6301\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u8f93\u51fa\u7a33\u5b9a\u6027\uff0c\u5e76\u5177\u6709\u9ad8\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u7ef4\u5ea6\u4e0a\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u68c0\u6d4b\u6027\u80fd\uff0c\u80fd\u591f\u51c6\u786e\u8bc6\u522b\u8bed\u4e49\u76f8\u4f3c\u6587\u672c\u95f4\u7684\u504f\u5dee\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u8f93\u51fa\u7a33\u5b9a\u6027\u3002\u8be5\u65b9\u6cd5\u7ed3\u6784\u8bbe\u8ba1\u5177\u6709\u9ad8\u53ef\u89e3\u91ca\u6027\uff0c\u6709\u52a9\u4e8e\u63ed\u793a\u8bed\u8a00\u6a21\u578b\u5185\u90e8\u7684\u504f\u5dee\u5173\u8054\u673a\u5236\uff0c\u4e3a\u504f\u5dee\u68c0\u6d4b\u63d0\u4f9b\u4e86\u66f4\u900f\u660e\u3001\u66f4\u53ef\u9760\u7684\u6280\u672f\u57fa\u7840\uff0c\u9002\u7528\u4e8e\u751f\u6210\u5185\u5bb9\u53ef\u4fe1\u5ea6\u8981\u6c42\u9ad8\u7684\u5b9e\u9645\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2508.05982", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05982", "abs": "https://arxiv.org/abs/2508.05982", "authors": ["Qingyang Liu", "Bingjie Gao", "Weiheng Huang", "Jun Zhang", "Zhongqian Sun", "Yang Wei", "Zelin Peng", "Qianli Ma", "Shuai Yang", "Zhaohe Liao", "Haonan Zhao", "Li Niu"], "title": "AnimateScene: Camera-controllable Animation in Any Scene", "comment": null, "summary": "3D scene reconstruction and 4D human animation have seen rapid progress and\nbroad adoption in recent years. However, seamlessly integrating reconstructed\nscenes with 4D human animation to produce visually engaging results remains\nchallenging. One key difficulty lies in placing the human at the correct\nlocation and scale within the scene while avoiding unrealistic\ninterpenetration. Another challenge is that the human and the background may\nexhibit different lighting and style, leading to unrealistic composites. In\naddition, appealing character motion videos are often accompanied by camera\nmovements, which means that the viewpoints need to be reconstructed along a\nspecified trajectory. We present AnimateScene, which addresses the above issues\nin a unified framework. First, we design an accurate placement module that\nautomatically determines a plausible 3D position for the human and prevents any\ninterpenetration within the scene during motion. Second, we propose a\ntraining-free style alignment method that adapts the 4D human representation to\nmatch the background's lighting and style, achieving coherent visual\nintegration. Finally, we design a joint post-reconstruction method for both the\n4D human and the 3D scene that allows camera trajectories to be inserted,\nenabling the final rendered video to feature visually appealing camera\nmovements. Extensive experiments show that AnimateScene generates dynamic scene\nvideos with high geometric detail and spatiotemporal coherence across various\ncamera and action combinations.", "AI": {"tldr": "AnimateScene unifies 3D scene reconstruction and 4D human animation by accurately placing humans, aligning their style with the scene, and enabling dynamic camera movements for realistic and engaging videos.", "motivation": "Integrating 3D scenes with 4D human animation is challenging due to difficulties in accurate human placement, avoiding interpenetration, aligning human and background style/lighting, and incorporating camera movements for visually engaging results.", "method": "AnimateScene employs a three-pronged approach: 1. An accurate placement module for correct human positioning and interpenetration avoidance. 2. A training-free style alignment method to match human representation with background lighting and style. 3. A joint post-reconstruction method for humans and scenes that incorporates camera trajectories.", "result": "Extensive experiments demonstrate that AnimateScene generates dynamic scene videos with high geometric detail and spatiotemporal coherence, effectively handling various camera and action combinations.", "conclusion": "AnimateScene provides a unified framework that successfully addresses the challenges of integrating 3D scenes with 4D human animations. It generates dynamic scene videos with high geometric detail and spatiotemporal coherence, adaptable to various camera and action combinations."}}
{"id": "2508.06313", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06313", "abs": "https://arxiv.org/abs/2508.06313", "authors": ["Amir Hossein Barjini", "Mohammad Bahari", "Mahdi Hejrati", "Jouni Mattila"], "title": "Surrogate-Enhanced Modeling and Adaptive Modular Control of All-Electric Heavy-Duty Robotic Manipulators", "comment": "This is submitted to IEEE T-ASE", "summary": "This paper presents a unified system-level modeling and control framework for\nan all-electric heavy-duty robotic manipulator (HDRM) driven by\nelectromechanical linear actuators (EMLAs). A surrogate-enhanced actuator\nmodel, combining integrated electromechanical dynamics with a neural network\ntrained on a dedicated testbed, is integrated into an extended virtual\ndecomposition control (VDC) architecture augmented by a natural adaptation law.\nThe derived analytical HDRM model supports a hierarchical control structure\nthat seamlessly maps high-level force and velocity objectives to real-time\nactuator commands, accompanied by a Lyapunov-based stability proof. In\nmulti-domain simulations of both cubic and a custom planar triangular\ntrajectory, the proposed adaptive modular controller achieves sub-centimeter\nCartesian tracking accuracy. Experimental validation of the same 1-DoF platform\nunder realistic load emulation confirms the efficacy of the proposed control\nstrategy. These findings demonstrate that a surrogate-enhanced EMLA model\nembedded in the VDC approach can enable modular, real-time control of an\nall-electric HDRM, supporting its deployment in next-generation mobile working\nmachines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5168\u7535\u52a8\u91cd\u578b\u673a\u5668\u4eba\u64cd\u4f5c\u5668\uff08HDRM\uff09\u7684\u5efa\u6a21\u4e0e\u63a7\u5236\u6846\u67b6\u3002\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u795e\u7ecf\u7f51\u7edc\u548c\u865a\u62df\u5206\u89e3\u63a7\u5236\uff08VDC\uff09\uff0c\u5e76\u901a\u8fc7\u4e86\u4eff\u771f\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u8ddf\u8e2a\uff0c\u53ef\u7528\u4e8e\u4e0b\u4e00\u4ee3\u79fb\u52a8\u5de5\u4f5c\u673a\u5668\u3002", "motivation": "\u4e3a\u4e86\u7ed9\u5168\u7535\u52a8\u91cd\u578b\u673a\u5668\u4eba\u64cd\u4f5c\u5668\uff08HDRM\uff09\u63d0\u4f9b\u4e00\u4e2a\u7edf\u4e00\u7684\u7cfb\u7edf\u7ea7\u5efa\u6a21\u548c\u63a7\u5236\u6846\u67b6\uff0c\u4ee5\u5b9e\u73b0\u7cbe\u786e\u7684\u5b9e\u65f6\u63a7\u5236\uff0c\u652f\u6301\u5176\u5728\u4e0b\u4e00\u4ee3\u79fb\u52a8\u5de5\u4f5c\u673a\u5668\u4e2d\u7684\u90e8\u7f72\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u7cfb\u7edf\u7ea7\u5efa\u6a21\u548c\u63a7\u5236\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u9488\u5bf9\u7531\u673a\u7535\u7ebf\u6027\u6267\u884c\u5668\uff08EMLA\uff09\u9a71\u52a8\u7684\u5168\u7535\u52a8\u91cd\u578b\u673a\u5668\u4eba\u64cd\u4f5c\u5668\uff08HDRM\uff09\u3002\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u7ed3\u5408\u4e86\u96c6\u6210\u673a\u7535\u52a8\u529b\u5b66\u548c\u5728\u4e13\u7528\u6d4b\u8bd5\u53f0\u4e0a\u8bad\u7ec3\u7684\u795e\u7ecf\u7f51\u7edc\u7684\u66ff\u4ee3\u589e\u5f3a\u6267\u884c\u5668\u6a21\u578b\uff0c\u5e76\u5c06\u5176\u96c6\u6210\u5230\u5177\u6709\u81ea\u7136\u81ea\u9002\u5e94\u5f8b\u7684\u6269\u5c55\u865a\u62df\u5206\u89e3\u63a7\u5236\uff08VDC\uff09\u67b6\u6784\u4e2d\u3002\u6240\u5bfc\u51fa\u7684\u5206\u6790HDRM\u6a21\u578b\u652f\u6301\u4e00\u79cd\u5206\u5c42\u63a7\u5236\u7ed3\u6784\uff0c\u8be5\u7ed3\u6784\u53ef\u5c06\u9ad8\u7ea7\u529b\u548c\u901f\u5ea6\u76ee\u6807\u65e0\u7f1d\u6620\u5c04\u5230\u5b9e\u65f6\u6267\u884c\u5668\u547d\u4ee4\uff0c\u5e76\u9644\u6709\u57fa\u4e8eLyapunov\u7684\u7a33\u5b9a\u6027\u8bc1\u660e\u3002", "result": "\u5728\u7acb\u65b9\u4f53\u548c\u81ea\u5b9a\u4e49\u5e73\u9762\u4e09\u89d2\u5f62\u8f68\u8ff9\u7684\u591a\u57df\u4eff\u771f\u4e2d\uff0c\u6240\u63d0\u51fa\u7684\u81ea\u9002\u5e94\u6a21\u5757\u5316\u63a7\u5236\u5668\u5b9e\u73b0\u4e86\u5398\u7c73\u7ea7\u4ee5\u4e0b\u7684\u7b1b\u5361\u5c14\u8ddf\u8e2a\u7cbe\u5ea6\u3002\u5728\u540c\u4e001\u81ea\u7531\u5ea6\u5e73\u53f0\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8be5\u5e73\u53f0\u5728\u73b0\u5b9e\u8d1f\u8f7d\u6a21\u62df\u4e0b\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u8bc1\u5b9e\u4e86\u6240\u63d0\u51fa\u63a7\u5236\u7b56\u7565\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u81ea\u9002\u5e94\u6a21\u5757\u5316\u63a7\u5236\u5668\u901a\u8fc7\u5d4c\u5165VDC\u65b9\u6cd5\u4e2d\u7684\u66ff\u4ee3\u589e\u5f3aEMLA\u6a21\u578b\uff0c\u80fd\u591f\u5b9e\u73b0\u5168\u7535\u52a8HDRM\u7684\u6a21\u5757\u5316\u3001\u5b9e\u65f6\u63a7\u5236\uff0c\u652f\u6301\u5176\u5728\u4e0b\u4e00\u4ee3\u79fb\u52a8\u5de5\u4f5c\u673a\u5668\u4e2d\u7684\u90e8\u7f72\u3002"}}
{"id": "2508.06263", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06263", "abs": "https://arxiv.org/abs/2508.06263", "authors": ["Andrew Cropper", "David M. Cerna", "Matti J\u00e4rvisalo"], "title": "Symmetry breaking for inductive logic programming", "comment": null, "summary": "The goal of inductive logic programming is to search for a hypothesis that\ngeneralises training data and background knowledge. The challenge is searching\nvast hypothesis spaces, which is exacerbated because many logically equivalent\nhypotheses exist. To address this challenge, we introduce a method to break\nsymmetries in the hypothesis space. We implement our idea in answer set\nprogramming. Our experiments on multiple domains, including visual reasoning\nand game playing, show that our approach can reduce solving times from over an\nhour to just 17 seconds.", "AI": {"tldr": "\u901a\u8fc7\u5728\u5047\u8bbe\u7a7a\u95f4\u4e2d\u6253\u7834\u5bf9\u79f0\u6027\u6765\u52a0\u901f\u5f52\u7eb3\u903b\u8f91\u7f16\u7a0b\u7684\u641c\u7d22\u8fc7\u7a0b\u3002", "motivation": "\u5f52\u7eb3\u903b\u8f91\u7f16\u7a0b\u7684\u76ee\u6807\u662f\u641c\u7d22\u80fd\u591f\u6cdb\u5316\u8bad\u7ec3\u6570\u636e\u548c\u80cc\u666f\u77e5\u8bc6\u7684\u5047\u8bbe\u3002\u7136\u800c\uff0c\u641c\u7d22\u5de8\u5927\u7684\u5047\u8bbe\u7a7a\u95f4\u662f\u4e00\u4e2a\u6311\u6218\uff0c\u56e0\u4e3a\u5b58\u5728\u8bb8\u591a\u903b\u8f91\u4e0a\u7b49\u4ef7\u7684\u5047\u8bbe\u3002", "method": "\u5c06\u6253\u7834\u5bf9\u79f0\u6027\u7684\u60f3\u6cd5\u5b9e\u73b0\u5728\u5e94\u7b54\u96c6\u7f16\u7a0b\u4e2d\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u53ef\u4ee5\u5c06\u89e3\u51b3\u65f6\u95f4\u4ece\u4e00\u5c0f\u65f6\u4ee5\u4e0a\u7f29\u77ed\u523017\u79d2\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5728\u5047\u8bbe\u7a7a\u95f4\u4e2d\u6253\u7834\u5bf9\u79f0\u6027\u6765\u89e3\u51b3\u5f52\u7eb3\u903b\u8f91\u7f16\u7a0b\u4e2d\u7684\u641c\u7d22\u6311\u6218\uff0c\u5e76\u5728\u591a\u4e2a\u9886\u57df\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2508.06066", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06066", "abs": "https://arxiv.org/abs/2508.06066", "authors": ["Barak Gahtan", "Alex M. Bronstein"], "title": "Architecture-Aware Generalization Bounds for Temporal Networks: Theory and Fair Comparison Methodology", "comment": null, "summary": "Deep temporal architectures such as Temporal Convolutional Networks (TCNs)\nachieve strong predictive performance on sequential data, yet theoretical\nunderstanding of their generalization remains limited. We address this gap by\nproviding both the first non-vacuous, architecture-aware generalization bounds\nfor deep temporal models and a principled evaluation methodology.\n  For exponentially $\\beta$-mixing sequences, we derive bounds scaling as $\nO\\!\\Bigl(R\\,\\sqrt{\\tfrac{D\\,p\\,n\\,\\log N}{N}}\\Bigr), $ where $D$ is network\ndepth, $p$ kernel size, $n$ input dimension, and $R$ weight norm. Our\ndelayed-feedback blocking mechanism transforms dependent samples into\neffectively independent ones while discarding only $O(1/\\log N)$ of the data,\nyielding $\\sqrt{D}$ scaling instead of exponential, implying that doubling\ndepth requires approximately quadrupling the training data.\n  We also introduce a fair-comparison methodology that fixes the effective\nsample size to isolate the effect of temporal structure from information\ncontent. Under $N_{\\text{eff}}=2{,}000$, strongly dependent sequences\n($\\rho=0.8$) exhibit $\\approx76\\%$ smaller generalization gaps than weakly\ndependent ones ($\\rho=0.2$), challenging the intuition that dependence is\npurely detrimental. Yet convergence rates diverge from theory: weak\ndependencies follow $N_{\\text{eff}}^{-1.21}$ scaling and strong dependencies\nfollow $N_{\\text{eff}}^{-0.89}$, both steeper than the predicted $N^{-0.5}$.\nThese findings reveal that temporal dependence can enhance learning under fixed\ninformation budgets, while highlighting gaps between theory and practice that\nmotivate future research.", "AI": {"tldr": "\u8be5\u7814\u7a76\u4e3a\u6df1\u5ea6\u65f6\u95f4\u6a21\u578b\uff08\u5982 TCN\uff09\u63d0\u4f9b\u4e86\u7406\u8bba\u5206\u6790\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u5c3d\u7ba1\u6df1\u5ea6\u4f1a\u589e\u52a0\u6cdb\u5316\u8bef\u5dee\uff0c\u4f46\u65f6\u95f4\u4f9d\u8d56\u6027\u5b9e\u9645\u4e0a\u53ef\u4ee5\u63d0\u9ad8\u5b66\u4e60\u6548\u7387\u3002\u7136\u800c\uff0c\u5b9e\u9a8c\u7ed3\u679c\u4e0e\u7406\u8bba\u9884\u6d4b\u5b58\u5728\u5dee\u5f02\uff0c\u8fd9\u8868\u660e\u4e86\u672a\u6765\u7814\u7a76\u7684\u65b9\u5411\u3002", "motivation": "\u6df1\u5ea6\u65f6\u95f4\u6a21\u578b\uff08\u5982 TCN\uff09\u7684\u6cdb\u5316\u7406\u8bba\u7406\u89e3\u4ecd\u7136\u6709\u9650\u3002\u6211\u4eec\u901a\u8fc7\u4e3a\u6df1\u5ea6\u65f6\u95f4\u6a21\u578b\u63d0\u4f9b\u7b2c\u4e00\u4e2a\u975e\u7a7a\u6cdb\u3001\u611f\u77e5\u67b6\u6784\u7684\u6cdb\u5316\u754c\u9650\u4ee5\u53ca\u4e00\u4e2a\u539f\u5219\u6027\u7684\u8bc4\u4f30\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e2a\u5dee\u8ddd\u3002", "method": "\u6211\u4eec\u4f7f\u7528\u5ef6\u8fdf\u53cd\u9988\u963b\u585e\u673a\u5236\u5c06\u76f8\u5173\u7684\u6837\u672c\u8f6c\u6362\u4e3a\u6709\u6548\u7684\u72ec\u7acb\u6837\u672c\uff0c\u540c\u65f6\u53ea\u4e22\u5f03 $O(1/\text{log } N)$ \u7684\u6570\u636e\uff0c\u4ece\u800c\u83b7\u5f97 $\text{log } N$ \u7684\u7f29\u653e\u800c\u4e0d\u662f\u6307\u6570\u7ea7\u7684\u7f29\u653e\u3002\u6211\u4eec\u8fd8\u5f15\u5165\u4e86\u4e00\u79cd\u516c\u5e73\u6bd4\u8f83\u65b9\u6cd5\uff0c\u5c06\u6709\u6548\u6837\u672c\u91cf\u56fa\u5b9a\u4e3a 2,000\uff0c\u4ee5\u9694\u79bb\u65f6\u95f4\u7ed3\u6784\u5bf9\u4fe1\u606f\u5185\u5bb9\u7684\u5f71\u54cd\u3002", "result": "\u6211\u4eec\u63a8\u5bfc\u51fa\u4e86\u89c4\u6a21\u4e3a $O(R \text{sqrt}(\frac{Dn \text{log } N}{N}))$ \u7684\u754c\u9650\u3002\u5f3a\u76f8\u5173\u5e8f\u5217\uff08$\rho=0.8$\uff09\u7684\u6bd4\u5f31\u76f8\u5173\u5e8f\u5217\uff08$\rho=0.2$\uff09\u7684\u6cdb\u5316\u5dee\u8ddd\u5c0f\u7ea6 76%\u3002\u7136\u800c\uff0c\u6536\u655b\u901f\u5ea6\u4e0e\u7406\u8bba\u9884\u6d4b\u7684 $N^{-0.5}$ \u4e0d\u540c\uff0c\u5f31\u76f8\u5173\u5e8f\u5217\u9075\u5faa $N_{\text{eff}}^{-1.21}$ \u7684\u7f29\u653e\uff0c\u5f3a\u76f8\u5173\u5e8f\u5217\u9075\u5faa $N_{\text{eff}}^{-0.89}$ \u7684\u7f29\u653e\u3002", "conclusion": "\u7406\u8bba\u548c\u5b9e\u8df5\u4e4b\u95f4\u7684\u5dee\u8ddd\u8868\u660e\u9700\u8981\u672a\u6765\u7684\u7814\u7a76\uff0c\u65f6\u95f4\u4f9d\u8d56\u6027\u53ef\u4ee5\u5728\u56fa\u5b9a\u7684\u4fe1\u606f\u9884\u7b97\u4e0b\u589e\u5f3a\u5b66\u4e60\u3002"}}
{"id": "2508.06159", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.06159", "abs": "https://arxiv.org/abs/2508.06159", "authors": ["Peng-Fei Zhou", "Shuang Qiao", "An-Chun Ji", "Shi-Ju Ran"], "title": "Diagonalizing large-scale quantum many-body Hamiltonians using variational quantum circuit and tensor network", "comment": null, "summary": "Exact diagonalization (ED) is an essential tool for exploring quantum\nmany-body physics but is fundamentally limited by the exponentially-scaled\ncomputational complexity. Here, we propose tensor network variational\ndiagonalization (TNVD), which encodes the full eigenenergy spectrum of a\nquantum many-body Hamiltonian into a matrix product state, and encodes the\neigenstates as the evolutions of product states using variational quantum\ncircuit (VQC). Thereby, TNVD reduces the computational complexity of\ndiagonalization from exponential to polynomial in system size $N$. Numerical\nbenchmarks up to $N=100$ spins are provided, which far surpass the\ncomputational limit of ED. We further consider quantum Ising model in a random\nfield to reveal the underlying reliance between the efficiency of TNVD and\nentanglement properties of eigenstates. Typical signs, including the\ndistribution of entanglement entropy (EE) versus eigenenergy and the density of\nstate versus EE, are suggested to indicate area law of entanglement entropy or\nits violation, which are essential to the TNVD efficiency. Our work establishes\nTNVD as a powerful and scalable diagonalization approach for large-scale\nquantum many-body Hamiltonians. The incorporation of VQC lays a promising\npathway to applying quantum computation to address the volume-law-EE\nHamiltonians that lack efficient classical approaches.", "AI": {"tldr": "TNVD\u901a\u8fc7\u5c06\u91cf\u5b50\u54c8\u5bc6\u987f\u91cf\u8c31\u7f16\u7801\u5230\u77e9\u9635\u4e58\u79ef\u6001\uff0c\u5e76\u5c06\u7279\u5f81\u6001\u7f16\u7801\u4e3aVQC\u6f14\u5316\uff0c\u5c06\u5bf9\u89d2\u5316\u590d\u6742\u5ea6\u4ece\u6307\u6570\u7ea7\u964d\u81f3\u591a\u9879\u5f0f\u7ea7\uff0c\u5e76\u80fd\u5904\u7406\u5927\u7cfb\u7edf\u548c\u4f53-\u71b5\u54c8\u5bc6\u987f\u91cf\u3002", "motivation": "\u7cbe\u786e\u5bf9\u89d2\u5316\uff08ED\uff09\u662f\u63a2\u7d22\u91cf\u5b50\u591a\u4f53\u7269\u7406\u5b66\u7684\u57fa\u672c\u5de5\u5177\uff0c\u4f46\u5176\u8ba1\u7b97\u590d\u6742\u5ea6\u968f\u7cfb\u7edf\u5927\u5c0f\u5448\u6307\u6570\u589e\u957f\uff0c\u8fd9\u662f\u4e00\u4e2a\u6839\u672c\u6027\u7684\u9650\u5236\u3002", "method": "TNVD\u5c06\u91cf\u5b50\u591a\u4f53\u54c8\u5bc6\u987f\u91cf\u7684\u6574\u4e2a\u7279\u5f81\u80fd\u91cf\u8c31\u7f16\u7801\u5230\u77e9\u9635\u4e58\u79ef\u6001\u4e2d\uff0c\u5e76\u5c06\u7279\u5f81\u6001\u7f16\u7801\u4e3a\u53d8\u5206\u91cf\u5b50\u7ebf\u8def\uff08VQC\uff09\u7684\u4e58\u79ef\u6001\u6f14\u5316\u3002", "result": "TNVD\u5c06\u5bf9\u89d2\u5316\u8ba1\u7b97\u590d\u6742\u5ea6\u4ece\u6307\u6570\u7ea7\u964d\u4f4e\u5230\u7cfb\u7edf\u4e2dN\u7684\u5927\u5c0f\u5448\u591a\u9879\u5f0f\u7ea7\u3002\u901a\u8fc7\u9ad8\u8fbeN=100\u4e2a\u81ea\u65cb\u7684\u6570\u503c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc1\u660e\u4e86TNVD\u7684\u6709\u6548\u6027\uff0c\u8fdc\u8d85ED\u7684\u8ba1\u7b97\u6781\u9650\u3002\u6b64\u5916\uff0c\u8fd8\u63ed\u793a\u4e86TNVD\u6548\u7387\u4e0e\u7279\u5f81\u6001\u7ea0\u7f20\u6027\u8d28\u4e4b\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\uff0c\u5e76\u901a\u8fc7\u7ea0\u7f20\u71b5\uff08EE\uff09\u4e0e\u7279\u5f81\u80fd\u91cf\u4ee5\u53ca\u6001\u5bc6\u5ea6\u4e0eEE\u7684\u5206\u5e03\u7b49\u5178\u578b\u7279\u5f81\uff0c\u4e3a\u5224\u65ad\u662f\u5426\u5b58\u5728\u4f53-\u71b5\u6216\u5176\u8fdd\u53cd\u63d0\u4f9b\u4e86\u6307\u793a\u3002", "conclusion": "TNVD\u662f\u4e00\u79cd\u5f3a\u5927\u4e14\u53ef\u6269\u5c55\u7684\u5bf9\u89d2\u5316\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u91cf\u5b50\u591a\u4f53\u54c8\u5bc6\u987f\u91cf\u3002VQC\u7684\u7ed3\u5408\u4e3a\u5e94\u7528\u91cf\u5b50\u8ba1\u7b97\u89e3\u51b3\u4e86\u7ecf\u5178\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u7684\u4f53-\u71b5\u54c8\u5bc6\u987f\u91cf\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u9014\u5f84\u3002"}}
{"id": "2508.06163", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06163", "abs": "https://arxiv.org/abs/2508.06163", "authors": ["Yingfeng Luo", "Dingyang Lin", "Junxin Wang", "Ziqiang Xu", "Kaiyan Chang", "Tong Zheng", "Bei Li", "Anxiang Ma", "Tong Xiao", "Zhengtao Yu", "Jingbo Zhu"], "title": "One Size Does Not Fit All: A Distribution-Aware Sparsification for More Precise Model Merging", "comment": "Under review", "summary": "Model merging has emerged as a compelling data-free paradigm for multi-task\nlearning, enabling the fusion of multiple fine-tuned models into a single,\npowerful entity. A key technique in merging methods is sparsification, which\nprunes redundant parameters from task vectors to mitigate interference.\nHowever, prevailing approaches employ a ``one-size-fits-all'' strategy,\napplying a uniform sparsity ratio that overlooks the inherent structural and\nstatistical heterogeneity of model parameters. This often leads to a suboptimal\ntrade-off, where critical parameters are inadvertently pruned while less useful\nones are retained. To address this limitation, we introduce \\textbf{TADrop}\n(\\textbf{T}ensor-wise \\textbf{A}daptive \\textbf{Drop}), an adaptive\nsparsification strategy that respects this heterogeneity. Instead of a global\nratio, TADrop assigns a tailored sparsity level to each parameter tensor based\non its distributional properties. The core intuition is that tensors with\ndenser, more redundant distributions can be pruned aggressively, while sparser,\nmore critical ones are preserved. As a simple and plug-and-play module, we\nvalidate TADrop by integrating it with foundational, classic, and SOTA merging\nmethods. Extensive experiments across diverse tasks (vision, language, and\nmultimodal) and models (ViT, BEiT) demonstrate that TADrop consistently and\nsignificantly boosts their performance. For instance, when enhancing a leading\nmerging method, it achieves an average performance gain of 2.0\\% across 8\nViT-B/32 tasks. TADrop provides a more effective way to mitigate parameter\ninterference by tailoring sparsification to the model's structure, offering a\nnew baseline for high-performance model merging.", "AI": {"tldr": "TADrop \u662f\u4e00\u79cd\u65b0\u9896\u7684\u81ea\u9002\u5e94\u7a00\u758f\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e3a\u4e0d\u540c\u53c2\u6570\u5f20\u91cf\u5b9a\u5236\u7a00\u758f\u5ea6\u6765\u6539\u8fdb\u6a21\u578b\u5408\u5e76\uff0c\u5e76\u5728\u591a\u9879\u4efb\u52a1\u548c\u6a21\u578b\u4e0a\u663e\u8457\u63d0\u9ad8\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u6a21\u578b\u5408\u5e76\u65b9\u6cd5\u91c7\u7528\u201c\u4e00\u5200\u5207\u201d\u7684\u7b56\u7565\uff0c\u5e94\u7528\u7edf\u4e00\u7684\u7a00\u758f\u7387\uff0c\u5ffd\u7565\u4e86\u6a21\u578b\u53c2\u6570\u56fa\u6709\u7684\u7ed3\u6784\u548c\u7edf\u8ba1\u5f02\u8d28\u6027\uff0c\u5bfc\u81f4\u5173\u952e\u53c2\u6570\u88ab\u4fee\u526a\uff0c\u800c\u975e\u5173\u952e\u53c2\u6570\u88ab\u4fdd\u7559\uff0c\u4ece\u800c\u9020\u6210\u6b21\u4f18\u7684\u6743\u8861\u3002", "method": "TADrop\uff08Tensor-wise Adaptive Drop\uff09\u662f\u4e00\u79cd\u81ea\u9002\u5e94\u7a00\u758f\u5316\u7b56\u7565\uff0c\u5b83\u4e0d\u91c7\u7528\u5168\u5c40\u7a00\u758f\u7387\uff0c\u800c\u662f\u6839\u636e\u53c2\u6570\u5f20\u91cf\u7684\u5206\u5e03\u7279\u6027\u4e3a\u5176\u5206\u914d\u5b9a\u5236\u5316\u7684\u7a00\u758f\u5ea6\u7ea7\u522b\u3002\u5176\u6838\u5fc3\u601d\u60f3\u662f\uff0c\u5206\u5e03\u66f4\u5bc6\u96c6\u3001\u5197\u4f59\u6027\u66f4\u9ad8\u7684\u5f20\u91cf\u53ef\u4ee5\u88ab\u5927\u529b\u4fee\u526a\uff0c\u800c\u66f4\u7a00\u758f\u3001\u66f4\u5173\u952e\u7684\u5f20\u91cf\u5219\u88ab\u4fdd\u7559\u3002", "result": "TADrop \u88ab\u96c6\u6210\u5230\u57fa\u7840\u3001\u7ecf\u5178\u548c SOTA \u6a21\u578b\u5408\u5e76\u65b9\u6cd5\u4e2d\uff0c\u5e76\u901a\u8fc7\u5728\u89c6\u89c9\u3001\u8bed\u8a00\u548c\u591a\u6a21\u6001\u7684\u5e7f\u6cdb\u4efb\u52a1\u4ee5\u53ca ViT\u3001BEiT \u7b49\u6a21\u578b\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cTADrop \u80fd\u591f\u6301\u7eed\u4e14\u663e\u8457\u5730\u63d0\u5347\u6027\u80fd\u3002\u4f8b\u5982\uff0c\u5728\u589e\u5f3a\u4e00\u79cd\u9886\u5148\u7684\u6a21\u578b\u5408\u5e76\u65b9\u6cd5\u65f6\uff0cTADrop \u5728 8 \u4e2a ViT-B/32 \u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u5e73\u5747 2.0% \u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "TADrop \u901a\u8fc7\u6839\u636e\u53c2\u6570\u5f20\u91cf\u7684\u5206\u5e03\u7279\u6027\u4e3a\u6bcf\u4e2a\u5f20\u91cf\u5206\u914d\u5b9a\u5236\u5316\u7684\u7a00\u758f\u5ea6\u7ea7\u522b\uff0c\u5e76\u76f8\u5e94\u5730\u8c03\u6574\u7a00\u758f\u5316\u7b56\u7565\uff0c\u4ece\u800c\u6709\u6548\u7f13\u89e3\u53c2\u6570\u5e72\u6270\uff0c\u4e3a\u9ad8\u6027\u80fd\u6a21\u578b\u5408\u5e76\u63d0\u4f9b\u4e86\u65b0\u7684\u57fa\u51c6\u3002"}}
{"id": "2508.05989", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.05989", "abs": "https://arxiv.org/abs/2508.05989", "authors": ["Younjoon Chung", "Hyoungseob Park", "Patrick Rim", "Xiaoran Zhang", "Jihe He", "Ziyao Zeng", "Safa Cicek", "Byung-Woo Hong", "James S. Duncan", "Alex Wong"], "title": "ETA: Energy-based Test-time Adaptation for Depth Completion", "comment": null, "summary": "We propose a method for test-time adaptation of pretrained depth completion\nmodels. Depth completion models, trained on some ``source'' data, often predict\nerroneous outputs when transferred to ``target'' data captured in novel\nenvironmental conditions due to a covariate shift. The crux of our method lies\nin quantifying the likelihood of depth predictions belonging to the source data\ndistribution. The challenge is in the lack of access to out-of-distribution\n(target) data prior to deployment. Hence, rather than making assumptions\nregarding the target distribution, we utilize adversarial perturbations as a\nmechanism to explore the data space. This enables us to train an energy model\nthat scores local regions of depth predictions as in- or out-of-distribution.\nWe update the parameters of pretrained depth completion models at test time to\nminimize energy, effectively aligning test-time predictions to those of the\nsource distribution. We call our method ``Energy-based Test-time Adaptation'',\nor ETA for short. We evaluate our method across three indoor and three outdoor\ndatasets, where ETA improve over the previous state-of-the-art method by an\naverage of 6.94% for outdoors and 10.23% for indoors. Project Page:\nhttps://fuzzythecat.github.io/eta.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.06319", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06319", "abs": "https://arxiv.org/abs/2508.06319", "authors": ["Sagar Parekh", "Heramb Nemlekar", "Dylan P. Losey"], "title": "Towards Balanced Behavior Cloning from Imbalanced Datasets", "comment": null, "summary": "Robots should be able to learn complex behaviors from human demonstrations.\nIn practice, these human-provided datasets are inevitably imbalanced: i.e., the\nhuman demonstrates some subtasks more frequently than others. State-of-the-art\nmethods default to treating each element of the human's dataset as equally\nimportant. So if -- for instance -- the majority of the human's data focuses on\nreaching a goal, and only a few state-action pairs move to avoid an obstacle,\nthe learning algorithm will place greater emphasis on goal reaching. More\ngenerally, misalignment between the relative amounts of data and the importance\nof that data causes fundamental problems for imitation learning approaches. In\nthis paper we analyze and develop learning methods that automatically account\nfor mixed datasets. We formally prove that imbalanced data leads to imbalanced\npolicies when each state-action pair is weighted equally; these policies\nemulate the most represented behaviors, and not the human's complex, multi-task\ndemonstrations. We next explore algorithms that rebalance offline datasets\n(i.e., reweight the importance of different state-action pairs) without human\noversight. Reweighting the dataset can enhance the overall policy performance.\nHowever, there is no free lunch: each method for autonomously rebalancing\nbrings its own pros and cons. We formulate these advantages and disadvantages,\nhelping other researchers identify when each type of approach is most\nappropriate. We conclude by introducing a novel meta-gradient rebalancing\nalgorithm that addresses the primary limitations behind existing approaches.\nOur experiments show that dataset rebalancing leads to better downstream\nlearning, improving the performance of general imitation learning algorithms\nwithout requiring additional data collection. See our project website:\nhttps://collab.me.vt.edu/data_curation/.", "AI": {"tldr": "\u6a21\u4eff\u5b66\u4e60\u4e2d\u7684\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\u4f1a\u5f71\u54cd\u7b97\u6cd5\u6027\u80fd\u3002\u672c\u7814\u7a76\u5206\u6790\u4e86\u8be5\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5143\u68af\u5ea6\u518d\u5e73\u8861\u7b97\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u5b66\u4e60\u6027\u80fd\u3002", "motivation": "\u4eba\u7c7b\u6f14\u793a\u6570\u636e\u96c6\u4e2d\u5b58\u5728\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5373\u4eba\u7c7b\u6f14\u793a\u8005\u4f1a\u66f4\u9891\u7e41\u5730\u6f14\u793a\u67d0\u4e9b\u5b50\u4efb\u52a1\u3002\u73b0\u6709\u7684\u5b66\u4e60\u65b9\u6cd5\u5c06\u4eba\u7c7b\u6570\u636e\u96c6\u4e2d\u7684\u6bcf\u4e2a\u5143\u7d20\u540c\u7b49\u5bf9\u5f85\uff0c\u8fd9\u4f1a\u5bfc\u81f4\u5b66\u4e60\u7b97\u6cd5\u66f4\u4fa7\u91cd\u4e8e\u6f14\u793a\u6b21\u6570\u8f83\u591a\u7684\u884c\u4e3a\uff0c\u800c\u4e0d\u662f\u4eba\u7c7b\u590d\u6742\u3001\u591a\u4efb\u52a1\u7684\u6f14\u793a\u3002", "method": "\u5206\u6790\u4e86\u6570\u636e\u4e0d\u5e73\u8861\u5bf9\u6a21\u4eff\u5b66\u4e60\u7684\u5f71\u54cd\uff0c\u5e76\u63a2\u7d22\u4e86\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u5373\u53ef\u91cd\u65b0\u5e73\u8861\u79bb\u7ebf\u6570\u636e\u96c6\uff08\u5373\u91cd\u65b0\u52a0\u6743\u4e0d\u540c\u72b6\u6001-\u52a8\u4f5c\u5bf9\u7684\u91cd\u8981\u6027\uff09\u7684\u7b97\u6cd5\u3002\u6700\u540e\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5143\u68af\u5ea6\u518d\u5e73\u8861\u7b97\u6cd5\u3002", "result": "\u8bc1\u660e\u4e86\u6570\u636e\u4e0d\u5e73\u8861\u4f1a\u5bfc\u81f4\u7b56\u7565\u4e0d\u5e73\u8861\uff0c\u5e76\u63d0\u51fa\u4e86\u80fd\u81ea\u52a8\u5904\u7406\u6df7\u5408\u6570\u636e\u96c6\u7684\u5b66\u4e60\u65b9\u6cd5\u3002\u540c\u65f6\uff0c\u5206\u6790\u4e86\u4e0d\u540c\u81ea\u4e3b\u91cd\u65b0\u5e73\u8861\u65b9\u6cd5\u7684\u4f18\u7f3a\u70b9\uff0c\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u9009\u62e9\u4f9d\u636e\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u5143\u68af\u5ea6\u518d\u5e73\u8861\u7b97\u6cd5\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u4e3b\u8981\u5c40\u9650\u6027\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u6570\u636e\u96c6\u518d\u5e73\u8861\u53ef\u4ee5\u63d0\u9ad8\u901a\u7528\u6a21\u4eff\u5b66\u4e60\u7b97\u6cd5\u7684\u6027\u80fd\uff0c\u800c\u65e0\u9700\u989d\u5916\u7684\u6570\u636e\u6536\u96c6\u3002"}}
{"id": "2508.06296", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06296", "abs": "https://arxiv.org/abs/2508.06296", "authors": ["Pierre Peign\u00e9 - Lefebvre", "Quentin Feuillade-Montixi", "Tom David", "Nicolas Miailhe"], "title": "LLM Robustness Leaderboard v1 --Technical report", "comment": null, "summary": "This technical report accompanies the LLM robustness leaderboard published by\nPRISM Eval for the Paris AI Action Summit. We introduce PRISM Eval Behavior\nElicitation Tool (BET), an AI system performing automated red-teaming through\nDynamic Adversarial Optimization that achieves 100% Attack Success Rate (ASR)\nagainst 37 of 41 state-of-the-art LLMs. Beyond binary success metrics, we\npropose a fine-grained robustness metric estimating the average number of\nattempts required to elicit harmful behaviors, revealing that attack difficulty\nvaries by over 300-fold across models despite universal vulnerability. We\nintroduce primitive-level vulnerability analysis to identify which jailbreaking\ntechniques are most effective for specific hazard categories. Our collaborative\nevaluation with trusted third parties from the AI Safety Network demonstrates\npractical pathways for distributed robustness assessment across the community.", "AI": {"tldr": "PRISM Eval\u7684BET\u5de5\u5177\u901a\u8fc7\u81ea\u52a8\u5316\u7ea2\u961f\u6d4b\u8bd5\uff0c\u5728\u8bc4\u4f30LLM\u9c81\u68d2\u6027\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u80fd\u6210\u529f\u8bc6\u522b\u51fa\u5927\u90e8\u5206\u6a21\u578b\u7684\u6709\u5bb3\u884c\u4e3a\uff0c\u5e76\u63d0\u4f9b\u4e86\u66f4\u7cbe\u7ec6\u7684\u8bc4\u4f30\u6307\u6807\u548c\u6f0f\u6d1e\u5206\u6790\u3002", "motivation": "\u4e3a\u4e86\u8bc4\u4f30\u548c\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u9c81\u68d2\u6027\uff0c\u7279\u522b\u662f\u8bc6\u522b\u548c\u91cf\u5316\u5176\u5728\u9762\u5bf9\u5bf9\u6297\u6027\u653b\u51fb\u65f6\u7684\u8106\u5f31\u6027\u3002", "method": "PRISM Eval\u884c\u4e3a\u8bf1\u5bfc\u5de5\u5177\uff08BET\uff09\u901a\u8fc7\u52a8\u6001\u5bf9\u6297\u6027\u4f18\u5316\u8fdb\u884c\u81ea\u52a8\u5316\u7ea2\u961f\u6d4b\u8bd5\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u7cbe\u7ec6\u5316\u7684\u9c81\u68d2\u6027\u6307\u6807\u6765\u8bc4\u4f30\u8bf1\u5bfc\u6709\u5bb3\u884c\u4e3a\u6240\u9700\u7684\u5e73\u5747\u5c1d\u8bd5\u6b21\u6570\uff0c\u4ee5\u53ca\u8fdb\u884c\u539f\u59cb\u7ea7\u522b\u6f0f\u6d1e\u5206\u6790\u3002", "result": "BET\u572841\u4e2a\u6700\u5148\u8fdb\u7684LLM\u4e2d\uff0c\u670937\u4e2a\u8fbe\u5230\u4e86100%\u7684\u653b\u51fb\u6210\u529f\u7387\u3002\u7814\u7a76\u8fd8\u53d1\u73b0\uff0c\u4e0d\u540c\u6a21\u578b\u5728\u8bf1\u5bfc\u6709\u5bb3\u884c\u4e3a\u7684\u96be\u5ea6\u4e0a\u5dee\u5f02\u663e\u8457\uff0c\u76f8\u5dee\u8d85\u8fc7300\u500d\uff0c\u5e76\u8bc6\u522b\u51fa\u5bf9\u7279\u5b9a\u5371\u5bb3\u7c7b\u522b\u6700\u6709\u6548\u7684\u8d8a\u72f1\u6280\u672f\u3002", "conclusion": "PRISM Eval BET\u5728\u8bc6\u522bLLM\u7684\u9c81\u68d2\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5176\u52a8\u6001\u5bf9\u6297\u6027\u4f18\u5316\u65b9\u6cd5\u80fd\u4ee5100%\u7684\u6210\u529f\u7387\u8bc6\u522b37/41\u4e2a\u6a21\u578b\u7684\u6709\u5bb3\u884c\u4e3a\u3002\u8be5\u5de5\u5177\u8fd8\u5f15\u5165\u4e86\u7cbe\u7ec6\u5316\u7684\u9c81\u68d2\u6027\u6307\u6807\u548c\u539f\u59cb\u7ea7\u522b\u6f0f\u6d1e\u5206\u6790\uff0c\u4e3a\u793e\u533a\u5206\u5e03\u5f0f\u8bc4\u4f30\u63d0\u4f9b\u4e86\u5b9e\u7528\u65b9\u6cd5\u3002"}}
{"id": "2508.06097", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06097", "abs": "https://arxiv.org/abs/2508.06097", "authors": ["Simon B\u00fchrer", "Andreas Plesner", "Till Aczel", "Roger Wattenhofer"], "title": "Recurrent Deep Differentiable Logic Gate Networks", "comment": null, "summary": "While differentiable logic gates have shown promise in feedforward networks,\ntheir application to sequential modeling remains unexplored. This paper\npresents the first implementation of Recurrent Deep Differentiable Logic Gate\nNetworks (RDDLGN), combining Boolean operations with recurrent architectures\nfor sequence-to-sequence learning.\n  Evaluated on WMT'14 English-German translation, RDDLGN achieves 5.00 BLEU and\n30.9\\% accuracy during training, approaching GRU performance (5.41 BLEU) and\ngraceful degradation (4.39 BLEU) during inference. This work establishes\nrecurrent logic-based neural computation as viable, opening research directions\nfor FPGA acceleration in sequential modeling and other recursive network\narchitectures.", "AI": {"tldr": "RDDLGN\u5c06\u53ef\u5fae\u903b\u8f91\u95e8\u4e0e\u5faa\u73af\u7f51\u7edc\u76f8\u7ed3\u5408\uff0c\u5728\u5e8f\u5217\u5230\u5e8f\u5217\u5b66\u4e60\u4e2d\u53d6\u5f97\u4e86\u6709\u524d\u666f\u7684\u7ed3\u679c\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u5f00\u8f9f\u4e86\u9053\u8def\u3002", "motivation": "\u89e3\u51b3\u4e86\u53ef\u5fae\u903b\u8f91\u95e8\u5728\u987a\u5e8f\u5efa\u6a21\u4e2d\u7684\u5e94\u7528\u7a7a\u767d\uff0c\u63a2\u7d22\u5c06\u5e03\u5c14\u8fd0\u7b97\u4e0e\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u7ed3\u5408\u7684\u53ef\u80fd\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u5faa\u73af\u6df1\u5ea6\u53ef\u5fae\u903b\u8f91\u95e8\u7f51\u7edc\uff08RDDLGN\uff09\u7684\u65b0\u578b\u7f51\u7edc\uff0c\u5c06\u5e03\u5c14\u8fd0\u7b97\u4e0e\u5faa\u73af\u67b6\u6784\u76f8\u7ed3\u5408\uff0c\u7528\u4e8e\u5e8f\u5217\u5230\u5e8f\u5217\u5b66\u4e60\u3002", "result": "RDDLGN\u5728WMT'14\u82f1\u5fb7\u7ffb\u8bd1\u4efb\u52a1\u4e0a\u8fbe\u5230\u4e865.00 BLEU\u548c30.9%\u7684\u8bad\u7ec3\u51c6\u786e\u7387\uff0c\u5728\u63a8\u7406\u65f6\u63a5\u8fd1GRU\u7684\u6027\u80fd\uff085.41 BLEU\uff09\u548c\u4f18\u96c5\u964d\u7ea7\uff084.39 BLEU\uff09\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5c06\u53ef\u5fae\u903b\u8f91\u95e8\u5e94\u7528\u4e8e\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\uff0c\u4e3a\u5e8f\u5217\u5efa\u6a21\u5f00\u8f9f\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\uff0c\u5e76\u4e3aFPGA\u52a0\u901f\u7b49\u9012\u5f52\u7f51\u7edc\u67b6\u6784\u63d0\u4f9b\u4e86\u53ef\u80fd\u6027\u3002"}}
{"id": "2508.06175", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.06175", "abs": "https://arxiv.org/abs/2508.06175", "authors": ["Olga Solodovnikova", "Ulrik L. Andersen", "Jonas S. Neergaard-Nielsen"], "title": "Fast simulations of continuous-variable circuits using the coherent state decomposition", "comment": "29 pages, 8 figures", "summary": "We present \\texttt{lcg\\_plus}, an open-source Python library for the\nsimulation of continuous-variable quantum circuits with both generaldyne and\nphoton-number-resolving detector capabilities. Our framework merges the linear\ncombination of Gaussians methodology with the coherent state decomposition of\narbitrary non-Gaussian states, forming a bridge between the Gaussian and Fock\nbasis representations. By tracking the Wigner function, we can simulate the\naction of Gaussian channels and measurements on multi-mode systems in a fast\nand accurate numerical framework. The calculation of the quality measures of\nquantum states is convenient in this formalism, and we derive expressions for\nthe analytical gradients of these measures with respect to parameterized\ncircuit elements. We demonstrate the utility of this methodology by optimizing\nthe heralded preparation of a qunaught state, a crucial component for building\na fault-tolerant photonic quantum computer, with a Gaussian Boson sampling\ncircuit containing inefficient components.", "AI": {"tldr": "lcg_plus\u662f\u4e00\u4e2a\u7528\u4e8e\u6a21\u62df\u91cf\u5b50\u7535\u8def\u7684Python\u5e93\uff0c\u53ef\u4ee5\u5904\u7406\u975e\u9ad8\u65af\u6001\u548c\u4f4e\u6548\u7ec4\u4ef6\uff0c\u5e76\u80fd\u4f18\u5316\u91cf\u5b50\u6001\u7684\u5236\u5907\u3002", "motivation": "\u4e3a\u4e86\u5b9e\u73b0\u5bb9\u9519\u5149\u5b50\u91cf\u5b50\u8ba1\u7b97\uff0c\u9700\u8981\u9ad8\u6548\u5236\u5907\u5173\u952e\u7684\u91cf\u5b50\u6001\uff0c\u4f8b\u5982qunaught\u6001\u3002\u73b0\u6709\u7684\u6a21\u62df\u65b9\u6cd5\u5728\u5904\u7406\u5305\u542b\u4f4e\u6548\u7ec4\u4ef6\u7684\u91cf\u5b50\u7535\u8def\u65f6\u5b58\u5728\u6311\u6218\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u51c6\u786e\u9ad8\u6548\u6a21\u62df\u8fde\u7eed\u53d8\u91cf\u91cf\u5b50\u7535\u8def\uff08\u5305\u62ec\u975e\u9ad8\u65af\u6001\u548c\u4f4e\u6548\u7ec4\u4ef6\uff09\u7684\u6846\u67b6\uff0c\u4ee5\u652f\u6301\u91cf\u5b50\u6001\u7684\u4f18\u5316\u5236\u5907\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86lcg_plus\uff0c\u4e00\u4e2a\u5f00\u6e90Python\u5e93\uff0c\u5b9e\u73b0\u4e86\u8fde\u7eed\u53d8\u91cf\u91cf\u5b50\u7535\u8def\u7684\u6a21\u62df\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u9ad8\u65af\u6001\u7ebf\u6027\u7ec4\u5408\uff08lcg\uff09\u548c\u76f8\u5e72\u6001\u5206\u89e3\u6280\u672f\uff0c\u80fd\u591f\u5904\u7406\u4efb\u610f\u975e\u9ad8\u65af\u6001\u3002\u901a\u8fc7\u8ffd\u8e2aWigner\u51fd\u6570\uff0c\u6a21\u62df\u9ad8\u65af\u901a\u9053\u548c\u6d4b\u91cf\u64cd\u4f5c\uff0c\u5e76\u63a8\u5bfc\u4e86\u91cf\u5b50\u6001\u8d28\u91cf\u5ea6\u91cf\u7684\u89e3\u6790\u68af\u5ea6\uff0c\u7528\u4e8e\u4f18\u5316\u542b\u4f4e\u6548\u7ec4\u4ef6\u7684\u91cf\u5b50\u7535\u8def\u3002", "result": "lcg_plus\u5e93\u80fd\u591f\u5feb\u901f\u51c6\u786e\u5730\u6a21\u62df\u8fde\u7eed\u53d8\u91cf\u91cf\u5b50\u7535\u8def\uff0c\u5305\u62ec\u975e\u9ad8\u65af\u6001\u548c\u4f4e\u6548\u7ec4\u4ef6\u3002\u901a\u8fc7\u8ffd\u8e2aWigner\u51fd\u6570\uff0c\u53ef\u4ee5\u6a21\u62df\u9ad8\u65af\u901a\u9053\u548c\u6d4b\u91cf\u64cd\u4f5c\u3002\u8be5\u5e93\u8fd8\u652f\u6301\u8ba1\u7b97\u91cf\u5b50\u6001\u8d28\u91cf\u5ea6\u91cf\u7684\u89e3\u6790\u68af\u5ea6\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8e\u4f18\u5316\u9ad8\u65af\u73bb\u8272\u5b50\u91c7\u6837\u7535\u8def\u4e2dqunaught\u6001\u7684\u5236\u5907\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86lcg_plus\uff0c\u4e00\u4e2a\u5f00\u6e90Python\u5e93\uff0c\u7528\u4e8e\u6a21\u62df\u5177\u6709\u8fde\u7eed\u53d8\u91cf\u3001\u901a\u7528\u6d4b\u632f\u548c\u5149\u5b50\u6570\u5206\u8fa8\u63a2\u6d4b\u5668\u80fd\u529b\u7684\u91cf\u5b50\u7535\u8def\u3002\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u9ad8\u65af\u6001\u7ebf\u6027\u7ec4\u5408\u65b9\u6cd5\u548c\u4efb\u610f\u975e\u9ad8\u65af\u6001\u76f8\u5e72\u6001\u5206\u89e3\uff0c\u5f25\u5408\u4e86\u9ad8\u65af\u548cfock\u57fa\u8868\u793a\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002\u901a\u8fc7\u8ffd\u8e2aWigner\u51fd\u6570\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5feb\u901f\u51c6\u786e\u5730\u6a21\u62df\u9ad8\u65af\u901a\u9053\u548c\u6d4b\u91cf\u5728\u591a\u6a21\u7cfb\u7edf\u4e0a\u7684\u4f5c\u7528\uff0c\u5e76\u65b9\u4fbf\u5730\u8ba1\u7b97\u91cf\u5b50\u6001\u7684\u8d28\u91cf\u5ea6\u91cf\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u5bfc\u51fa\u4e86\u8fd9\u4e9b\u5ea6\u91cf\u76f8\u5bf9\u4e8e\u53c2\u6570\u5316\u7535\u8def\u5143\u4ef6\u7684\u89e3\u6790\u68af\u5ea6\u8868\u8fbe\u5f0f\u3002\u6700\u540e\uff0c\u901a\u8fc7\u4f18\u5316\u9ad8\u65af\u73bb\u8272\u5b50\u91c7\u6837\u7535\u8def\u5236\u5907qunaught\u6001\u7684\u521d\u6b65\u5236\u5907\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u5305\u542b\u4f4e\u6548\u7ec4\u4ef6\u7684\u60c5\u51b5\u4e0b\uff0c\u5bf9\u4e8e\u6784\u5efa\u5bb9\u9519\u5149\u5b50\u91cf\u5b50\u8ba1\u7b97\u673a\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2508.06165", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06165", "abs": "https://arxiv.org/abs/2508.06165", "authors": ["Weitao Li", "Boran Xiang", "Xiaolong Wang", "Zhinan Gou", "Weizhi Ma", "Yang Liu"], "title": "UR$^2$: Unify RAG and Reasoning through Reinforcement Learning", "comment": null, "summary": "Large Language Models (LLMs) have shown remarkable capabilities through two\ncomplementary paradigms: Retrieval-Augmented Generation (RAG), which enhances\nknowledge grounding, and Reinforcement Learning from Verifiable Rewards (RLVR),\nwhich optimizes complex reasoning abilities. However, these two capabilities\nare often developed in isolation, and existing efforts to unify them remain\nnarrow in scope-typically limited to open-domain QA with fixed retrieval\nsettings and task-specific assumptions. This lack of integration constrains\ngeneralization and limits the applicability of RAG-RL methods to broader\ndomains. To bridge this gap, we propose UR2 (Unified RAG and Reasoning), a\ngeneral framework that unifies retrieval and reasoning through reinforcement\nlearning. UR2 introduces two key contributions: a difficulty-aware curriculum\ntraining that selectively invokes retrieval only for challenging problems, and\na hybrid knowledge access strategy combining domain-specific offline corpora\nwith LLM-generated summaries. These components are designed to enable dynamic\ncoordination between retrieval and reasoning, improving adaptability across a\ndiverse range of tasks. Experiments across open-domain QA, MMLU-Pro, medical,\nand mathematical reasoning tasks demonstrate that UR2 (built on Qwen2.5-3/7B\nand LLaMA-3.1-8B) significantly outperforms existing RAG and RL methods,\nachieving comparable performance to GPT-4o-mini and GPT-4.1-mini on several\nbenchmarks. We have released all code, models, and data at\nhttps://github.com/Tsinghua-dhy/UR2.", "AI": {"tldr": "UR2\u662f\u4e00\u4e2a\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u7edf\u4e00\u4e86\u68c0\u7d22\uff08RAG\uff09\u548c\u63a8\u7406\uff08RLVR\uff09\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5b64\u7acb\u5f00\u53d1\u7684\u9650\u5236\u3002\u5b83\u91c7\u7528\u96be\u5ea6\u611f\u77e5\u8bfe\u7a0b\u8bad\u7ec3\u548c\u6df7\u5408\u77e5\u8bc6\u8bbf\u95ee\u7b56\u7565\uff0c\u5728\u591a\u79cd\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u6027\u80fd\u5ab2\u7f8eGPT-4o-mini\u548cGPT-4.1-mini\u3002", "motivation": "\u73b0\u6709\u7684RAG\u548cRL\u65b9\u6cd5\u901a\u5e38\u662f\u5b64\u7acb\u5f00\u53d1\u7684\uff0c\u5e76\u4e14\u5c06\u5b83\u4eec\u7edf\u4e00\u8d77\u6765\u7684\u73b0\u6709\u52aa\u529b\u8303\u56f4\u6709\u9650\uff0c\u901a\u5e38\u4ec5\u9650\u4e8e\u5177\u6709\u56fa\u5b9a\u68c0\u7d22\u8bbe\u7f6e\u548c\u7279\u5b9a\u4efb\u52a1\u5047\u8bbe\u7684\u5f00\u653e\u57dfQA\u3002\u8fd9\u79cd\u7f3a\u4e4f\u6574\u5408\u9650\u5236\u4e86\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u9650\u5236\u4e86RAG-RL\u65b9\u6cd5\u5728\u66f4\u5e7f\u6cdb\u9886\u57df\u7684\u9002\u7528\u6027\u3002", "method": "\u63d0\u51faUR2\uff08Unified RAG and Reasoning\uff09\u901a\u7528\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u7edf\u4e00\u68c0\u7d22\u548c\u63a8\u7406\u3002\u5f15\u5165\u4e86\u96be\u5ea6\u611f\u77e5\u8bfe\u7a0b\u8bad\u7ec3\uff08\u9009\u62e9\u6027\u5730\u4ec5\u4e3a\u590d\u6742\u95ee\u9898\u8c03\u7528\u68c0\u7d22\uff09\u548c\u6df7\u5408\u77e5\u8bc6\u8bbf\u95ee\u7b56\u7565\uff08\u7ed3\u5408\u7279\u5b9a\u9886\u57df\u7684\u79bb\u7ebf\u8bed\u6599\u5e93\u548cLLM\u751f\u6210\u7684\u6458\u8981\uff09\uff0c\u4ee5\u5b9e\u73b0\u68c0\u7d22\u548c\u63a8\u7406\u4e4b\u95f4\u7684\u52a8\u6001\u534f\u8c03\u3002", "result": "UR2\u6846\u67b6\u5728\u5f00\u653e\u57dfQA\u3001MMLU-Pro\u3001\u533b\u5b66\u548c\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5b83\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684RAG\u548cRL\u65b9\u6cd5\uff0c\u5e76\u5728\u4e00\u4e9b\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fbe\u5230\u4e86\u4e0eGPT-4o-mini\u548cGPT-4.1-mini\u76f8\u5f53\u7684\u6027\u80fd\u3002", "conclusion": "UR2\u6846\u67b6\u6210\u529f\u5730\u7edf\u4e00\u4e86\u68c0\u7d22\u548c\u63a8\u7406\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709RAG\u548cRL\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u4e0eGPT-4o-mini\u548cGPT-4.1-mini\u76f8\u5f53\u3002"}}
{"id": "2508.05990", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05990", "abs": "https://arxiv.org/abs/2508.05990", "authors": ["Haichao Wang", "Xinyue Xi", "Jiangtao Wen", "Yuxing Han"], "title": "Fast Motion Estimation and Context-Aware Refinement for Efficient Bayer-Domain Video Vision", "comment": null, "summary": "The efficiency of video computer vision system remains a challenging task due\nto the high temporal redundancy inside a video. Existing works have been\nproposed for efficient vision computer vision. However, they do not fully\nreduce the temporal redundancy and neglect the front end computation overhead.\nIn this paper, we propose an efficient video computer vision system. First,\nimage signal processor is removed and Bayer-format data is directly fed into\nvideo computer vision models, thus saving the front end computation. Second,\ninstead of optical flow models and video codecs, a fast block matching-based\nmotion estimation algorithm is proposed specifically for efficient video\ncomputer vision, with a MV refinement module. To correct the error,\ncontext-aware block refinement network is introduced to refine regions with\nlarge error. To further balance the accuracy and efficiency, a frame selection\nstrategy is employed. Experiments on multiple video computer vision tasks\ndemonstrate that our method achieves significant acceleration with slight\nperformance loss.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89c6\u9891\u8ba1\u7b97\u673a\u89c6\u89c9\u7cfb\u7edf\uff0c\u901a\u8fc7\u79fb\u9664ISP\u3001\u4f7f\u7528\u5feb\u901f\u5757\u5339\u914d\u8fd0\u52a8\u4f30\u8ba1\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u5757\u7ec6\u5316\u7f51\u7edc\uff0c\u5e76\u7ed3\u5408\u5e27\u9009\u62e9\u7b56\u7565\uff0c\u5728\u4fdd\u8bc1\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u52a0\u901f\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u8ba1\u7b97\u673a\u89c6\u89c9\u7cfb\u7edf\u6548\u7387\u4f4e\u4e0b\uff0c\u4e3b\u8981\u7531\u4e8e\u89c6\u9891\u4e2d\u5b58\u5728\u9ad8\u65f6\u95f4\u5197\u4f59\uff0c\u4e14\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u51cf\u5c11\u65f6\u95f4\u5197\u4f59\u5e76\u5ffd\u7565\u4e86\u524d\u7aef\u8ba1\u7b97\u5f00\u9500\u3002", "method": "1. \u79fb\u9664\u56fe\u50cf\u4fe1\u53f7\u5904\u7406\u5668\uff0c\u76f4\u63a5\u5c06Bayer\u683c\u5f0f\u6570\u636e\u8f93\u5165\u89c6\u9891\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\uff0c\u4ee5\u8282\u7701\u524d\u7aef\u8ba1\u7b97\u5f00\u9500\u3002\n2. \u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5757\u5339\u914d\u7684\u5feb\u901f\u8fd0\u52a8\u4f30\u8ba1\u7b97\u6cd5\uff0c\u5e76\u8f85\u4ee5\u8fd0\u52a8\u77e2\u91cf\uff08MV\uff09\u7ec6\u5316\u6a21\u5757\uff0c\u4ee5\u66ff\u4ee3\u5149\u6d41\u6a21\u578b\u548c\u89c6\u9891\u7f16\u7801\u5668\u3002\n3. \u5f15\u5165\u4e0a\u4e0b\u6587\u611f\u77e5\u5757\u7ec6\u5316\u7f51\u7edc\uff0c\u7528\u4e8e\u4fee\u6b63\u5b58\u5728\u8f83\u5927\u8bef\u5dee\u7684\u533a\u57df\u3002\n4. \u91c7\u7528\u5e27\u9009\u62e9\u7b56\u7565\uff0c\u4ee5\u5e73\u8861\u7cbe\u5ea6\u548c\u6548\u7387\u3002", "result": "\u6240\u63d0\u51fa\u7684\u7cfb\u7edf\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u52a0\u901f\uff0c\u5e76\u4e14\u5728\u591a\u4e2a\u89c6\u9891\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u591a\u4e2a\u89c6\u9891\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u52a0\u901f\uff0c\u540c\u65f6\u53ea\u5e26\u6765\u4e86\u8f7b\u5fae\u7684\u6027\u80fd\u635f\u5931\u3002"}}
{"id": "2508.06330", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06330", "abs": "https://arxiv.org/abs/2508.06330", "authors": ["Baorun Li", "Chengrui Zhu", "Siyi Du", "Bingran Chen", "Jie Ren", "Wenfei Wang", "Yong Liu", "Jiajun Lv"], "title": "L2Calib: $SE(3)$-Manifold Reinforcement Learning for Robust Extrinsic Calibration with Degenerate Motion Resilience", "comment": "IROS2025", "summary": "Extrinsic calibration is essential for multi-sensor fusion, existing methods\nrely on structured targets or fully-excited data, limiting real-world\napplicability. Online calibration further suffers from weak excitation, leading\nto unreliable estimates. To address these limitations, we propose a\nreinforcement learning (RL)-based extrinsic calibration framework that\nformulates extrinsic calibration as a decision-making problem, directly\noptimizes $SE(3)$ extrinsics to enhance odometry accuracy. Our approach\nleverages a probabilistic Bingham distribution to model 3D rotations, ensuring\nstable optimization while inherently retaining quaternion symmetry. A\ntrajectory alignment reward mechanism enables robust calibration without\nstructured targets by quantitatively evaluating estimated tightly-coupled\ntrajectory against a reference trajectory. Additionally, an automated data\nselection module filters uninformative samples, significantly improving\nefficiency and scalability for large-scale datasets. Extensive experiments on\nUAVs, UGVs, and handheld platforms demonstrate that our method outperforms\ntraditional optimization-based approaches, achieving high-precision calibration\neven under weak excitation conditions. Our framework simplifies deployment on\ndiverse robotic platforms by eliminating the need for high-quality initial\nextrinsics and enabling calibration from routine operating data. The code is\navailable at https://github.com/APRIL-ZJU/learn-to-calibrate.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u9c81\u68d2\u5916\u7279\u6027\u6821\u51c6\u6846\u67b6\uff0c\u65e0\u9700\u7ed3\u6784\u5316\u76ee\u6807\u6216\u5145\u5206\u6fc0\u52b1\u7684\u6570\u636e\uff0c\u5373\u53ef\u5728\u591a\u79cd\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u6821\u51c6\u3002", "motivation": "\u73b0\u6709\u5916\u7279\u6027\u6821\u51c6\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u7ed3\u6784\u5316\u76ee\u6807\u6216\u5145\u5206\u6fc0\u52b1\u7684\u6570\u636e\uff0c\u9650\u5236\u4e86\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u5e94\u7528\u3002\u5728\u7ebf\u6821\u51c6\u5728\u6fc0\u52b1\u4e0d\u8db3\u7684\u60c5\u51b5\u4e0b\u8fdb\u4e00\u6b65\u53d7\u5230\u5f71\u54cd\uff0c\u5bfc\u81f4\u4f30\u8ba1\u4e0d\u53ef\u9760\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684\u5916\u7279\u6027\u6821\u51c6\u6846\u67b6\uff0c\u5c06\u5916\u7279\u6027\u6821\u51c6\u6784\u5efa\u4e3a\u51b3\u7b56\u95ee\u9898\uff0c\u76f4\u63a5\u4f18\u5316SE(3)\u5916\u7279\u6027\u4ee5\u63d0\u9ad8\u91cc\u7a0b\u8ba1\u7cbe\u5ea6\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u6982\u7387\u6027\u7684Bingham\u5206\u5e03\u5bf93D\u65cb\u8f6c\u8fdb\u884c\u5efa\u6a21\uff0c\u4ee5\u5b9e\u73b0\u7a33\u5b9a\u7684\u4f18\u5316\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u56db\u5143\u6570\u5bf9\u79f0\u6027\u3002\u8f68\u8ff9\u5bf9\u9f50\u5956\u52b1\u673a\u5236\u901a\u8fc7\u91cf\u5316\u8bc4\u4f30\u4f30\u8ba1\u7684\u7d27\u8026\u5408\u8f68\u8ff9\u4e0e\u53c2\u8003\u8f68\u8ff9\uff0c\u5b9e\u73b0\u4e86\u5728\u6ca1\u6709\u7ed3\u6784\u5316\u76ee\u6807\u60c5\u51b5\u4e0b\u7684\u9c81\u68d2\u6821\u51c6\u3002\u6b64\u5916\uff0c\u81ea\u52a8\u6570\u636e\u9009\u62e9\u6a21\u5757\u8fc7\u6ee4\u4e86\u4fe1\u606f\u91cf\u4e0d\u8db3\u7684\u6837\u672c\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5927\u89c4\u6a21\u6570\u636e\u96c6\u7684\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002", "result": "\u5728UAV\u3001UGV\u548c\u624b\u6301\u5e73\u53f0\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u4f18\u4e8e\u4f20\u7edf\u7684\u57fa\u4e8e\u4f18\u5316\u7684\u65b9\u6cd5\uff0c\u5373\u4f7f\u5728\u6fc0\u52b1\u4e0d\u8db3\u7684\u6761\u4ef6\u4e0b\u4e5f\u80fd\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u6821\u51c6\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u6d88\u9664\u5bf9\u9ad8\u8d28\u91cf\u521d\u59cb\u5916\u7279\u6027\u7684\u9700\u6c42\uff0c\u5e76\u80fd\u591f\u4ece\u5e38\u89c4\u64cd\u4f5c\u6570\u636e\u4e2d\u8fdb\u884c\u6821\u51c6\uff0c\u7b80\u5316\u4e86\u5728\u4e0d\u540c\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u90e8\u7f72\u7684\u6d41\u7a0b\u3002"}}
{"id": "2508.06108", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06108", "abs": "https://arxiv.org/abs/2508.06108", "authors": ["Xing Lei", "Wenyan Yang", "Kaiqiang Ke", "Shentao Yang", "Xuetao Zhang", "Joni Pajarinen", "Donglin Wang"], "title": "GCHR : Goal-Conditioned Hindsight Regularization for Sample-Efficient Reinforcement Learning", "comment": null, "summary": "Goal-conditioned reinforcement learning (GCRL) with sparse rewards remains a\nfundamental challenge in reinforcement learning. While hindsight experience\nreplay (HER) has shown promise by relabeling collected trajectories with\nachieved goals, we argue that trajectory relabeling alone does not fully\nexploit the available experiences in off-policy GCRL methods, resulting in\nlimited sample efficiency. In this paper, we propose Hindsight Goal-conditioned\nRegularization (HGR), a technique that generates action regularization priors\nbased on hindsight goals. When combined with hindsight self-imitation\nregularization (HSR), our approach enables off-policy RL algorithms to maximize\nexperience utilization. Compared to existing GCRL methods that employ HER and\nself-imitation techniques, our hindsight regularizations achieve substantially\nmore efficient sample reuse and the best performances, which we empirically\ndemonstrate on a suite of navigation and manipulation tasks.", "AI": {"tldr": "Hindsight Goal-conditioned Regularization (HGR) and hindsight self-imitation regularization (HSR) improve sample efficiency in goal-conditioned reinforcement learning by generating action regularization priors based on hindsight goals, outperforming existing methods in navigation and manipulation tasks.", "motivation": "Hindsight experience replay (HER) alone does not fully exploit the available experiences in off-policy GCRL methods, resulting in limited sample efficiency.", "method": "Hindsight Goal-conditioned Regularization (HGR) generates action regularization priors based on hindsight goals. When combined with hindsight self-imitation regularization (HSR), our approach enables off-policy RL algorithms to maximize experience utilization.", "result": "Our hindsight regularizations achieve substantially more efficient sample reuse and the best performances on a suite of navigation and manipulation tasks.", "conclusion": "Hindsight regularizations achieve substantially more efficient sample reuse and the best performances, which we empirically demonstrate on a suite of navigation and manipulation tasks."}}
{"id": "2508.06184", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.06184", "abs": "https://arxiv.org/abs/2508.06184", "authors": ["Floyd M. Creevey", "Hitham T. Hassan", "James McCafferty", "Lloyd C. L. Hollenberg", "Sergii Strelchuk"], "title": "Scalable Quantum State Preparation for Encoding Genomic Data with Matrix Product States", "comment": null, "summary": "As quantum computing hardware advances, the need for algorithms that\nfacilitate the loading of classical data into the quantum states of these\ndevices has become increasingly important. This study presents a method for\nproducing scalable quantum circuits to encode genomic data using the Matrix\nProduct State (MPS) formalism. The method is illustrated by encoding the genome\nof the bacteriophage $\\Phi X174$ into a 15-qubit state, and analysing the\ntrade-offs between MPS bond dimension, reconstruction error, and the resulting\ncircuit complexity. This study proposes methods for optimising encoding\ncircuits with standard benchmark datasets for the emerging field of quantum\nbioinformatics. The results for circuit generation and simulation on HPC and on\ncurrent quantum hardware demonstrate the viability and utility of the encoding.", "AI": {"tldr": "This paper presents a method using Matrix Product States to encode genomic data into quantum circuits, demonstrating its effectiveness for quantum bioinformatics applications.", "motivation": "The increasing need for algorithms to load classical data into quantum states as quantum computing hardware advances, particularly for applications in quantum bioinformatics.", "method": "A method for producing scalable quantum circuits to encode genomic data using the Matrix Product State (MPS) formalism is presented and illustrated by encoding the genome of the bacteriophage $\\Phi X174$ into a 15-qubit state.", "result": "The study analyzes trade-offs between MPS bond dimension, reconstruction error, and circuit complexity, and demonstrates the viability and utility of the encoding through circuit generation and simulation on HPC and current quantum hardware.", "conclusion": "The study demonstrates the viability and utility of the proposed MPS-based method for encoding genomic data into quantum states, showing potential for quantum bioinformatics."}}
{"id": "2508.06167", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.06167", "abs": "https://arxiv.org/abs/2508.06167", "authors": ["V\u00edt Gvo\u017ediak"], "title": "Pragmatics beyond humans: meaning, communication, and LLMs", "comment": null, "summary": "The paper reconceptualizes pragmatics not as a subordinate, third dimension\nof meaning, but as a dynamic interface through which language operates as a\nsocially embedded tool for action. With the emergence of large language models\n(LLMs) in communicative contexts, this understanding needs to be further\nrefined and methodologically reconsidered. The first section challenges the\ntraditional semiotic trichotomy, arguing that connectionist LLM architectures\ndestabilize established hierarchies of meaning, and proposes the Human-Machine\nCommunication (HMC) framework as a more suitable alternative. The second\nsection examines the tension between human-centred pragmatic theories and the\nmachine-centred nature of LLMs. While traditional, Gricean-inspired pragmatics\ncontinue to dominate, it relies on human-specific assumptions ill-suited to\npredictive systems like LLMs. Probabilistic pragmatics, particularly the\nRational Speech Act framework, offers a more compatible teleology by focusing\non optimization rather than truth-evaluation. The third section addresses the\nissue of substitutionalism in three forms - generalizing, linguistic, and\ncommunicative - highlighting the anthropomorphic biases that distort LLM\nevaluation and obscure the role of human communicative subjects. Finally, the\npaper introduces the concept of context frustration to describe the paradox of\nincreased contextual input paired with a collapse in contextual understanding,\nemphasizing how users are compelled to co-construct pragmatic conditions both\nfor the model and themselves. These arguments suggest that pragmatic theory may\nneed to be adjusted or expanded to better account for communication involving\ngenerative AI.", "AI": {"tldr": "\u672c\u6587\u8ba4\u4e3a\uff0c\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u51fa\u73b0\uff0c\u9700\u8981\u91cd\u65b0\u5ba1\u89c6\u8bed\u7528\u5b66\u7406\u8bba\u3002\u8bba\u6587\u6311\u6218\u4e86\u4f20\u7edf\u7684\u7b26\u53f7\u4e09\u5206\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4eba\u673a\u901a\u4fe1\uff08HMC\uff09\u6846\u67b6\u3002\u5b83\u8fd8\u63a2\u8ba8\u4e86\u4eba\u7c7b\u4e2d\u5fc3\u548c\u673a\u5668\u4e2d\u5fc3\u8bed\u7528\u5b66\u7406\u8bba\u4e4b\u95f4\u7684\u5f20\u529b\uff0c\u5e76\u8ba4\u4e3a\u6982\u7387\u8bed\u7528\u5b66\u6bd4\u4f20\u7edf\u7684\u683c\u83b1\u65af\u8bed\u7528\u5b66\u66f4\u9002\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002\u6700\u540e\uff0c\u8bba\u6587\u5f15\u5165\u4e86\u201c\u8bed\u5883\u632b\u8d25\u201d\u7684\u6982\u5ff5\uff0c\u4ee5\u63cf\u8ff0\u4e0e\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u4ea4\u4e92\u65f6\u51fa\u73b0\u7684\u8bed\u5883\u7406\u89e3\u6311\u6218\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u4ea4\u9645\u73af\u5883\u4e2d\u7684\u51fa\u73b0\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u6539\u8fdb\u548c\u91cd\u65b0\u8003\u8651\u8bed\u7528\u5b66\u7684\u7406\u89e3\u548c\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u6311\u6218\u4f20\u7edf\u7684\u7b26\u53f7\u4e09\u5206\u6cd5\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u5408\u9002\u7684\u4eba\u673a\u901a\u4fe1\uff08HMC\uff09\u6846\u67b6\u3002\u901a\u8fc7\u8003\u5bdf\u4ee5\u4eba\u7c7b\u4e3a\u4e2d\u5fc3\u7684\u8bed\u7528\u5b66\u7406\u8bba\u4e0e\u4ee5\u673a\u5668\u4e3a\u4e2d\u5fc3\u7684\u8bed\u8a00\u6a21\u578b\u4e4b\u95f4\u7684\u5f20\u529b\uff0c\u5f3a\u8c03\u4e86\u6982\u7387\u8bed\u7528\u5b66\uff08\u7279\u522b\u662f\u7406\u6027\u8a00\u8bed\u884c\u4e3a\u6846\u67b6\uff09\u6bd4\u4f20\u7edf\u7684\u3001\u57fa\u4e8e\u683c\u83b1\u65af\u7684\u8bed\u7528\u5b66\u66f4\u5177\u517c\u5bb9\u6027\uff0c\u56e0\u4e3a\u5b83\u4fa7\u91cd\u4e8e\u4f18\u5316\u800c\u975e\u771f\u503c\u8bc4\u4f30\u3002\u901a\u8fc7\u89e3\u51b3\u4e09\u79cd\u5f62\u5f0f\u7684\u66ff\u4ee3\u4e3b\u4e49\u2014\u2014\u6982\u62ec\u6027\u3001\u8bed\u8a00\u6027\u548c\u4ea4\u6d41\u6027\u2014\u2014\u5e76\u5f3a\u8c03\u4e86\u626d\u66f2\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u548c\u6a21\u7cca\u4eba\u7c7b\u4ea4\u6d41\u4e3b\u4f53\u4f5c\u7528\u7684\u62df\u4eba\u5316\u504f\u89c1\u3002\u6700\u540e\uff0c\u901a\u8fc7\u5f15\u5165\u201c\u8bed\u5883\u632b\u8d25\u201d\u7684\u6982\u5ff5\u6765\u63cf\u8ff0\u8bed\u5883\u8f93\u5165\u589e\u52a0\u4f46\u8bed\u5883\u7406\u89e3\u5d29\u6e83\u7684\u6096\u8bba\uff0c\u5e76\u5f3a\u8c03\u7528\u6237\u5982\u4f55\u88ab\u8feb\u4e3a\u6a21\u578b\u548c\u81ea\u8eab\u5171\u540c\u6784\u5efa\u8bed\u5883\u6761\u4ef6\u3002", "result": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u8fde\u63a5\u4e3b\u4e49\u67b6\u6784\u7834\u574f\u4e86\u65e2\u5b9a\u7684\u610f\u4e49\u5c42\u6b21\u7ed3\u6784\uff0c\u5e76\u63d0\u51fa\u4e86\u4eba\u673a\u901a\u4fe1\uff08HMC\uff09\u6846\u67b6\u3002\u6982\u7387\u8bed\u7528\u5b66\uff0c\u7279\u522b\u662f\u7406\u6027\u8a00\u8bed\u884c\u4e3a\u6846\u67b6\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u517c\u5bb9\u7684\u76ee\u7684\u8bba\uff0c\u901a\u8fc7\u5173\u6ce8\u4f18\u5316\u800c\u975e\u771f\u503c\u8bc4\u4f30\u6765\u89e3\u51b3\u4eba\u7c7b\u4e2d\u5fc3\u8bed\u7528\u5b66\u7406\u8bba\u4e0eLLM\u673a\u5668\u4e2d\u5fc3\u6027\u8d28\u4e4b\u95f4\u7684\u5f20\u529b\u3002\u8bed\u7528\u5b66\u7406\u8bba\u53ef\u80fd\u9700\u8981\u8c03\u6574\u6216\u6269\u5c55\uff0c\u4ee5\u66f4\u597d\u5730\u89e3\u91ca\u6d89\u53ca\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u7684\u4ea4\u6d41\u3002", "conclusion": "\u672c\u7bc7\u8bba\u6587\u8ba4\u4e3a\uff0c\u4e3a\u4e86\u66f4\u597d\u5730\u89e3\u91ca\u6d89\u53ca\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u7684\u4ea4\u6d41\uff0c\u53ef\u80fd\u9700\u8981\u8c03\u6574\u6216\u6269\u5c55\u8bed\u7528\u5b66\u7406\u8bba\u3002"}}
{"id": "2508.05991", "categories": ["cs.CV", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.05991", "abs": "https://arxiv.org/abs/2508.05991", "authors": ["Juewen Hu", "Yexin Li", "Jiulin Li", "Shuo Chen", "Pring Wong"], "title": "ECMF: Enhanced Cross-Modal Fusion for Multimodal Emotion Recognition in MER-SEMI Challenge", "comment": null, "summary": "Emotion recognition plays a vital role in enhancing human-computer\ninteraction. In this study, we tackle the MER-SEMI challenge of the MER2025\ncompetition by proposing a novel multimodal emotion recognition framework. To\naddress the issue of data scarcity, we leverage large-scale pre-trained models\nto extract informative features from visual, audio, and textual modalities.\nSpecifically, for the visual modality, we design a dual-branch visual encoder\nthat captures both global frame-level features and localized facial\nrepresentations. For the textual modality, we introduce a context-enriched\nmethod that employs large language models to enrich emotional cues within the\ninput text. To effectively integrate these multimodal features, we propose a\nfusion strategy comprising two key components, i.e., self-attention mechanisms\nfor dynamic modality weighting, and residual connections to preserve original\nrepresentations. Beyond architectural design, we further refine noisy labels in\nthe training set by a multi-source labeling strategy. Our approach achieves a\nsubstantial performance improvement over the official baseline on the\nMER2025-SEMI dataset, attaining a weighted F-score of 87.49% compared to\n78.63%, thereby validating the effectiveness of the proposed framework.", "AI": {"tldr": "\u4e3a\u4e86\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u60c5\u7eea\u8bc6\u522b\u6846\u67b6\uff0c\u5229\u7528\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b\u63d0\u53d6\u7279\u5f81\uff0c\u5e76\u91c7\u7528\u7279\u6b8a\u7684\u878d\u5408\u7b56\u7565\u548c\u6807\u7b7e\u4f18\u5316\u65b9\u6cd5\uff0c\u5728MER2025-SEMI\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u6570\u636e\u7a00\u758f\u6027\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u589e\u5f3a\u4eba\u673a\u4ea4\u4e92\u6765\u63a8\u52a8\u60c5\u7eea\u8bc6\u522b\u7684\u53d1\u5c55\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6a21\u6001\u60c5\u7eea\u8bc6\u522b\u6846\u67b6\uff0c\u5229\u7528\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b\u63d0\u53d6\u89c6\u89c9\u3001\u542c\u89c9\u548c\u6587\u672c\u6a21\u6001\u7684\u7279\u5f81\u3002\u89c6\u89c9\u6a21\u6001\u91c7\u7528\u53cc\u5206\u652f\u7f16\u7801\u5668\uff0c\u6355\u6349\u5168\u5c40\u548c\u5c40\u90e8\u9762\u90e8\u7279\u5f81\uff1b\u6587\u672c\u6a21\u6001\u91c7\u7528\u4e0a\u4e0b\u6587\u4e30\u5bcc\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u589e\u5f3a\u6587\u672c\u4e2d\u7684\u60c5\u7eea\u7ebf\u7d22\u3002\u878d\u5408\u7b56\u7565\u5305\u62ec\u81ea\u6ce8\u610f\u529b\u673a\u5236\u548c\u6b8b\u5dee\u8fde\u63a5\uff0c\u7528\u4e8e\u52a8\u6001\u52a0\u6743\u548c\u4fdd\u7559\u539f\u59cb\u8868\u793a\u3002\u6b64\u5916\uff0c\u8fd8\u91c7\u7528\u591a\u6e90\u6807\u6ce8\u7b56\u7565\u4f18\u5316\u4e86\u8bad\u7ec3\u96c6\u7684\u6807\u7b7e\u3002", "result": "\u8be5\u6846\u67b6\u5728MER2025-SEMI\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e8687.49%\u7684\u52a0\u6743F\u5206\u6570\uff0c\u663e\u8457\u4f18\u4e8e\u5b98\u65b9\u57fa\u7ebf\uff0878.63%\uff09\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728MER2025-SEMI\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u52a0\u6743F\u5206\u6570\u4ece78.63%\u63d0\u9ad8\u523087.49%\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u6846\u67b6\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.06404", "categories": ["cs.RO", "math.OC"], "pdf": "https://arxiv.org/pdf/2508.06404", "abs": "https://arxiv.org/abs/2508.06404", "authors": ["Abdullah Zareh Andaryan", "Michael G. H. Bell", "Mohsen Ramezani", "Glenn Geers"], "title": "V*: An Efficient Motion Planning Algorithm for Autonomous Vehicles", "comment": null, "summary": "Autonomous vehicle navigation in structured environments requires planners\ncapable of generating time-optimal, collision-free trajectories that satisfy\ndynamic and kinematic constraints. We introduce V*, a graph-based motion\nplanner that represents speed and direction as explicit state variables within\na discretised space-time-velocity lattice. Unlike traditional methods that\ndecouple spatial search from dynamic feasibility or rely on post-hoc smoothing,\nV* integrates both motion dimensions directly into graph construction through\ndynamic graph generation during search expansion. To manage the complexity of\nhigh-dimensional search, we employ a hexagonal discretisation strategy and\nprovide formal mathematical proofs establishing optimal waypoint spacing and\nminimal node redundancy under constrained heading transitions for\nvelocity-aware motion planning. We develop a mathematical formulation for\ntransient steering dynamics in the kinematic bicycle model, modelling steering\nangle convergence with exponential behaviour, and deriving the relationship for\nconvergence rate parameters. This theoretical foundation, combined with\ngeometric pruning strategies that eliminate expansions leading to infeasible\nsteering configurations, enables V* to evaluate dynamically admissible\nmanoeuvres, ensuring each trajectory is physically realisable without further\nrefinement. We further demonstrate V*'s performance in simulation studies with\ncluttered and dynamic environments involving moving obstacles, showing its\nability to avoid conflicts, yield proactively, and generate safe, efficient\ntrajectories with temporal reasoning capabilities for waiting behaviours and\ndynamic coordination.", "AI": {"tldr": "V*\u662f\u4e00\u79cd\u7528\u4e8e\u81ea\u4e3b\u8f66\u8f86\u5bfc\u822a\u7684\u8fd0\u52a8\u89c4\u5212\u5668\uff0c\u5b83\u901a\u8fc7\u65f6\u7a7a\u901f\u5ea6\u683c\u548c\u52a8\u6001\u56fe\u751f\u6210\u6765\u4f18\u5316\u8f68\u8ff9\uff0c\u5e76\u80fd\u5728\u590d\u6742\u73af\u5883\u4e2d\u6709\u6548\u907f\u969c\u3002", "motivation": "\u65e8\u5728\u89e3\u51b3\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u81ea\u4e3b\u8f66\u8f86\u5bfc\u822a\u4e2d\u7684\u65f6\u95f4\u6700\u4f18\u3001\u65e0\u78b0\u649e\u8f68\u8ff9\u89c4\u5212\u95ee\u9898\uff0c\u540c\u65f6\u6ee1\u8db3\u52a8\u6001\u548c\u8fd0\u52a8\u5b66\u7ea6\u675f\u3002", "method": "V*\u662f\u4e00\u79cd\u57fa\u4e8e\u56fe\u7684\u8fd0\u52a8\u89c4\u5212\u5668\uff0c\u4f7f\u7528\u65f6\u7a7a\u901f\u5ea6\u683c\u8868\u793a\u72b6\u6001\uff0c\u5e76\u901a\u8fc7\u52a8\u6001\u56fe\u751f\u6210\u6765\u96c6\u6210\u8fd0\u52a8\u89c4\u5212\u7684\u4e24\u4e2a\u7ef4\u5ea6\u3002\u5b83\u91c7\u7528\u516d\u8fb9\u5f62\u79bb\u6563\u5316\u7b56\u7565\uff0c\u5e76\u7ed3\u5408\u77ac\u6001\u8f6c\u5411\u52a8\u529b\u5b66\u548c\u51e0\u4f55\u4fee\u526a\u7b56\u7565\u6765\u786e\u4fdd\u8f68\u8ff9\u7684\u53ef\u884c\u6027\u3002", "result": "V*\u5728\u6a21\u62df\u7814\u7a76\u4e2d\uff0c\u80fd\u591f\u5728\u6742\u4e71\u548c\u52a8\u6001\u73af\u5883\u4e2d\uff0c\u5305\u62ec\u79fb\u52a8\u969c\u788d\u7269\uff0c\u751f\u6210\u5b89\u5168\u3001\u9ad8\u6548\u4e14\u5177\u6709\u65f6\u95f4\u63a8\u7406\u80fd\u529b\u7684\u8f68\u8ff9\uff0c\u5e76\u80fd\u4e3b\u52a8\u907f\u8ba9\u548c\u8fdb\u884c\u52a8\u6001\u534f\u8c03\u3002", "conclusion": "V*\u901a\u8fc7\u5728\u65f6\u7a7a\u901f\u5ea6\u683c\u4e2d\u663e\u5f0f\u5730\u8868\u793a\u901f\u5ea6\u548c\u65b9\u5411\uff0c\u5e76\u96c6\u6210\u7a7a\u95f4\u641c\u7d22\u548c\u52a8\u6001\u53ef\u884c\u6027\uff0c\u5b9e\u73b0\u4e86\u65f6\u95f4\u6700\u4f18\u3001\u65e0\u78b0\u649e\u7684\u8f68\u8ff9\u89c4\u5212\u3002"}}
{"id": "2508.06348", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06348", "abs": "https://arxiv.org/abs/2508.06348", "authors": ["Mille Mei Zhen Loo", "Gert Luzkov", "Paolo Burelli"], "title": "AntiCheatPT: A Transformer-Based Approach to Cheat Detection in Competitive Computer Games", "comment": null, "summary": "Cheating in online video games compromises the integrity of gaming\nexperiences. Anti-cheat systems, such as VAC (Valve Anti-Cheat), face\nsignificant challenges in keeping pace with evolving cheating methods without\nimposing invasive measures on users' systems. This paper presents\nAntiCheatPT\\_256, a transformer-based machine learning model designed to detect\ncheating behaviour in Counter-Strike 2 using gameplay data. To support this, we\nintroduce and publicly release CS2CD: A labelled dataset of 795 matches. Using\nthis dataset, 90,707 context windows were created and subsequently augmented to\naddress class imbalance. The transformer model, trained on these windows,\nachieved an accuracy of 89.17\\% and an AUC of 93.36\\% on an unaugmented test\nset. This approach emphasizes reproducibility and real-world applicability,\noffering a robust baseline for future research in data-driven cheat detection.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAntiCheatPT_256\u7684Transformer\u6a21\u578b\uff0c\u7528\u4e8e\u68c0\u6d4b\u300a\u53cd\u6050\u7cbe\u82f12\u300b\u4e2d\u7684\u4f5c\u5f0a\u884c\u4e3a\uff0c\u5e76\u53d1\u5e03\u4e86CS2CD\u6570\u636e\u96c6\u3002\u8be5\u6a21\u578b\u51c6\u786e\u7387\u8fbe\u523089.17%\uff0cAUC\u8fbe\u523093.36%\uff0c\u4e3a\u6570\u636e\u9a71\u52a8\u7684\u53cd\u4f5c\u5f0a\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002", "motivation": "\u5728\u7ebf\u89c6\u9891\u6e38\u620f\u4e2d\u7684\u4f5c\u5f0a\u884c\u4e3a\u4f1a\u635f\u5bb3\u6e38\u620f\u7684\u516c\u5e73\u6027\u3002\u73b0\u6709\u7684\u53cd\u4f5c\u5f0a\u7cfb\u7edf\uff08\u5982VAC\uff09\u5728\u5e94\u5bf9\u4e0d\u65ad\u53d8\u5316\u7684\u4f5c\u5f0a\u65b9\u6cd5\u65f6\u9762\u4e34\u6311\u6218\uff0c\u5e76\u4e14\u53ef\u80fd\u9700\u8981\u4fb5\u5165\u6027\u7684\u7528\u6237\u7cfb\u7edf\u63aa\u65bd\u3002\u672c\u7814\u7a76\u65e8\u5728\u63d0\u4f9b\u4e00\u79cd\u65e0\u9700\u4fb5\u5165\u6027\u63aa\u65bd\u5373\u53ef\u68c0\u6d4b\u4f5c\u5f0a\u884c\u4e3a\u7684\u65b9\u6cd5\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAntiCheatPT_256\u7684Transformer\u6a21\u578b\uff0c\u5229\u7528\u300a\u53cd\u6050\u7cbe\u82f12\u300b\u7684\u6e38\u620f\u6570\u636e\u6765\u68c0\u6d4b\u4f5c\u5f0a\u884c\u4e3a\u3002\u7814\u7a76\u4eba\u5458\u521b\u5efa\u5e76\u516c\u5f00\u4e86\u4e00\u4e2a\u5305\u542b795\u573a\u6bd4\u8d5b\u7684CS2CD\u6570\u636e\u96c6\uff0c\u5e76\u5bf990,707\u4e2a\u4e0a\u4e0b\u6587\u7a97\u53e3\u8fdb\u884c\u4e86\u589e\u5f3a\u4ee5\u89e3\u51b3\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "result": "\u5728\u672a\u589e\u5f3a\u7684\u6d4b\u8bd5\u96c6\u4e0a\uff0c\u6240\u63d0\u51fa\u7684Transformer\u6a21\u578b\u8fbe\u5230\u4e8689.17%\u7684\u51c6\u786e\u7387\u548c93.36%\u7684AUC\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86AntiCheatPT_256\u6a21\u578b\uff0c\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u7528\u4e8e\u68c0\u6d4b\u300a\u53cd\u6050\u7cbe\u82f12\u300b\u4e2d\u7684\u4f5c\u5f0a\u884c\u4e3a\uff0c\u5e76\u53d6\u5f97\u4e8689.17%\u7684\u51c6\u786e\u7387\u548c93.36%\u7684AUC\u3002\u8be5\u7814\u7a76\u8fd8\u53d1\u5e03\u4e86CS2CD\u6570\u636e\u96c6\uff0c\u4e3a\u672a\u6765\u53cd\u4f5c\u5f0a\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.06151", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06151", "abs": "https://arxiv.org/abs/2508.06151", "authors": ["Yong Oh Lee", "JeeEun Kim", "Jung Woo Lee"], "title": "Improving Diagnostic Accuracy for Oral Cancer with inpainting Synthesis Lesions Generated Using Diffusion Models", "comment": null, "summary": "In oral cancer diagnostics, the limited availability of annotated datasets\nfrequently constrains the performance of diagnostic models, particularly due to\nthe variability and insufficiency of training data. To address these\nchallenges, this study proposed a novel approach to enhance diagnostic accuracy\nby synthesizing realistic oral cancer lesions using an inpainting technique\nwith a fine-tuned diffusion model. We compiled a comprehensive dataset from\nmultiple sources, featuring a variety of oral cancer images. Our method\ngenerated synthetic lesions that exhibit a high degree of visual fidelity to\nactual lesions, thereby significantly enhancing the performance of diagnostic\nalgorithms. The results show that our classification model achieved a\ndiagnostic accuracy of 0.97 in differentiating between cancerous and\nnon-cancerous tissues, while our detection model accurately identified lesion\nlocations with 0.85 accuracy. This method validates the potential for synthetic\nimage generation in medical diagnostics and paves the way for further research\ninto extending these methods to other types of cancer diagnostics.", "AI": {"tldr": "\u4f7f\u7528\u7ecf\u8fc7\u5fae\u8c03\u7684\u6269\u6563\u6a21\u578b\u7684\u4fee\u590d\u6280\u672f\u5408\u6210\u4e86\u903c\u771f\u7684\u53e3\u8154\u764c\u75c5\u53d8\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8bca\u65ad\u7b97\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u53e3\u8154\u764c\u8bca\u65ad\u4e2d\uff0c\u6807\u6ce8\u6570\u636e\u7684\u6709\u9650\u6027\uff0c\u7279\u522b\u662f\u7531\u4e8e\u8bad\u7ec3\u6570\u636e\u7684\u53ef\u53d8\u6027\u548c\u4e0d\u5145\u5206\u6027\uff0c\u7ecf\u5e38\u9650\u5236\u8bca\u65ad\u6a21\u578b\u7684\u6027\u80fd\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f7f\u7528\u7ecf\u8fc7\u5fae\u8c03\u7684\u6269\u6563\u6a21\u578b\u7684\u4fee\u590d\u6280\u672f\u6765\u5408\u6210\u903c\u771f\u7684\u53e3\u8154\u764c\u75c5\u53d8\uff0c\u4ee5\u63d0\u9ad8\u8bca\u65ad\u51c6\u786e\u6027\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u6211\u4eec\u7684\u5206\u7c7b\u6a21\u578b\u5728\u533a\u5206\u764c\u6027\u548c\u975e\u764c\u6027\u7ec4\u7ec7\u65b9\u9762\u53d6\u5f97\u4e860.97\u7684\u8bca\u65ad\u51c6\u786e\u7387\uff0c\u800c\u6211\u4eec\u7684\u68c0\u6d4b\u6a21\u578b\u5219\u4ee50.85\u7684\u51c6\u786e\u7387\u51c6\u786e\u8bc6\u522b\u4e86\u75c5\u53d8\u4f4d\u7f6e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9a8c\u8bc1\u4e86\u5408\u6210\u56fe\u50cf\u751f\u6210\u5728\u533b\u5b66\u8bca\u65ad\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u4e3a\u5c06\u8fd9\u4e9b\u65b9\u6cd5\u6269\u5c55\u5230\u5176\u4ed6\u764c\u75c7\u8bca\u65ad\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2508.06193", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.06193", "abs": "https://arxiv.org/abs/2508.06193", "authors": ["Olga Solodovnikova", "Ulrik L. Andersen", "Jonas S. Neergaard-Nielsen"], "title": "The loss tolerance of cat breeding for fault-tolerant grid state generation", "comment": "20 pages, 12 figures", "summary": "The development of a continuous-variable photonic quantum computer depends on\nthe reliable preparation of high-quality Gottesman-Kitaev-Preskill states. The\nmost promising GKP preparation scheme is the cat breeding protocol, which can\ngenerate GKP states deterministically given a source of squeezed cat states,\nusing beam splitters, homodyne detectors and a feedforward displacement.\nHowever, analyzing the performance of the protocol under loss is cumbersome due\nto the exponential scaling of the system. By representing the Wigner function\nof the input states as a linear combination of Gaussians, we are able to\nquickly and accurately simulate several rounds of breeding with mixed input\nstates. Using this novel method, we find that optical loss decreases the\noverall success probability of the protocol, and prohibits the preparation of a\nfault-tolerant GKP state when the loss exceeds 4\\%. Our methodology is\navailable as open-source code.", "AI": {"tldr": "A new method to simulate the cat breeding protocol for GKP states shows that loss above 4% prevents fault-tolerant GKP state preparation.", "motivation": "Analyzing the performance of the cat breeding protocol under loss is cumbersome due to exponential scaling.", "method": "Representing the Wigner function of the input states as a linear combination of Gaussians to simulate the breeding protocol.", "result": "Optical loss decreases the overall success probability of the protocol.", "conclusion": "If optical loss exceeds 4%, it prevents the preparation of a fault-tolerant GKP state."}}
{"id": "2508.06178", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06178", "abs": "https://arxiv.org/abs/2508.06178", "authors": ["Hugo Abonizio", "Thales Almeida", "Roberto Lotufo", "Rodrigo Nogueira"], "title": "Comparing Knowledge Injection Methods for LLMs in a Low-Resource Regime", "comment": null, "summary": "Large language models (LLMs) often require vast amounts of text to\neffectively acquire new knowledge. While continuing pre-training on large\ncorpora or employing retrieval-augmented generation (RAG) has proven\nsuccessful, updating an LLM with only a few thousand or million tokens remains\nchallenging. In this work, we investigate the task of injecting small,\nunstructured information into LLMs and its relation to the catastrophic\nforgetting phenomenon. We use a dataset of recent news -- ensuring no overlap\nwith the model's pre-training data -- to evaluate the knowledge acquisition by\nprobing the model with question-answer pairs related the learned information.\nStarting from a continued pre-training baseline, we explored different\naugmentation algorithms to generate synthetic data to improve the knowledge\nacquisition capabilities. Our experiments show that simply continuing\npre-training on limited data yields modest improvements, whereas exposing the\nmodel to diverse textual variations significantly improves the learning of new\nfacts -- particularly with methods that induce greater variability through\ndiverse prompting. Furthermore, we shed light on the forgetting phenomenon in\nsmall-data regimes, illustrating the delicate balance between learning new\ncontent and retaining existing capabilities. We also confirm the sensitivity of\nRAG-based approaches for knowledge injection, which often lead to greater\ndegradation on control datasets compared to parametric methods. Finally, we\ndemonstrate that models can generate effective synthetic training data\nthemselves, suggesting a pathway toward self-improving model updates. All code\nand generated data used in our experiments are publicly available, providing a\nresource for studying efficient knowledge injection in LLMs with limited data\nat https://github.com/hugoabonizio/knowledge-injection-methods.", "AI": {"tldr": "\u201c\u672c\u7814\u7a76\u805a\u7126\u4e8e\u5982\u4f55\u7528\u5c11\u91cf\u6570\u636e\u9ad8\u6548\u5730\u66f4\u65b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u901a\u8fc7\u591a\u6837\u5316\u7684\u6587\u672c\u589e\u5f3a\uff08\u5c24\u5176\u662f\u6a21\u578b\u81ea\u751f\u6210\u6570\u636e\uff09\u6bd4\u7b80\u5355\u7684\u6301\u7eed\u9884\u8bad\u7ec3\u6216RAG\u66f4\u80fd\u6709\u6548\u5730\u6ce8\u5165\u65b0\u77e5\u8bc6\uff0c\u5e76\u80fd\u66f4\u597d\u5730\u9632\u6b62\u6a21\u578b\u9057\u5fd8\u65e7\u77e5\u8bc6\u3002\u6a21\u578b\u81ea\u751f\u6210\u6570\u636e\u7684\u65b9\u6cd5\u4e3a\u672a\u6765LLM\u7684\u81ea\u6211\u6539\u8fdb\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002\u201d", "motivation": "\u201c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u901a\u5e38\u9700\u8981\u5927\u91cf\u6587\u672c\u624d\u80fd\u6709\u6548\u5730\u83b7\u53d6\u65b0\u77e5\u8bc6\u3002\u5c3d\u7ba1\u6301\u7eed\u9884\u8bad\u7ec3\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u5df2\u88ab\u8bc1\u660e\u662f\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u4f46\u4ec5\u7528\u6570\u5343\u6216\u6570\u767e\u4e07\u4e2a\u6587\u672c\u7247\u6bb5\u6765\u66f4\u65b0LLM\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002\u56e0\u6b64\uff0c\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u5982\u4f55\u5c06\u5c11\u91cf\u975e\u7ed3\u6784\u5316\u4fe1\u606f\u6ce8\u5165LLM\u7684\u95ee\u9898\uff0c\u5e76\u63a2\u8ba8\u5176\u4e0e\u707e\u96be\u6027\u9057\u5fd8\u73b0\u8c61\u7684\u5173\u8054\u3002\u201d", "method": "\u201c\u672c\u6587\u7814\u7a76\u4e86\u5982\u4f55\u5c06\u5c11\u91cf\u975e\u7ed3\u6784\u5316\u4fe1\u606f\u6ce8\u5165\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u5e76\u63a2\u7a76\u4e86\u5176\u4e0e\u707e\u96be\u6027\u9057\u5fd8\u7684\u5173\u7cfb\u3002\u7814\u7a76\u4eba\u5458\u4f7f\u7528\u4e86\u5305\u542b\u8fd1\u671f\u65b0\u95fb\u7684\u7279\u5b9a\u6570\u636e\u96c6\uff0c\u4ee5\u786e\u4fdd\u4e0e\u6a21\u578b\u9884\u8bad\u7ec3\u6570\u636e\u65e0\u91cd\u53e0\u3002\u901a\u8fc7\u5bf9\u6a21\u578b\u8fdb\u884c\u95ee\u7b54\u6d4b\u8bd5\u6765\u8bc4\u4f30\u77e5\u8bc6\u83b7\u53d6\u80fd\u529b\u3002\u5728\u57fa\u7ebf\u7684\u57fa\u7840\u4e0a\uff0c\u7814\u7a76\u8005\u63a2\u7d22\u4e86\u4e0d\u540c\u7684\u6570\u636e\u589e\u5f3a\u7b97\u6cd5\uff0c\u4ee5\u751f\u6210\u5408\u6210\u6570\u636e\u6765\u63d0\u5347\u77e5\u8bc6\u83b7\u53d6\u80fd\u529b\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u7b80\u5355\u5730\u5728\u6709\u9650\u6570\u636e\u4e0a\u8fdb\u884c\u6301\u7eed\u9884\u8bad\u7ec3\u6548\u679c\u6709\u9650\uff0c\u800c\u901a\u8fc7\u591a\u6837\u5316\u63d0\u793a\u8bf1\u5bfc\u51fa\u7684\u6587\u672c\u53d8\u4f53\u80fd\u663e\u8457\u63d0\u9ad8\u65b0\u77e5\u8bc6\u7684\u5b66\u4e60\u6548\u679c\u3002\u7814\u7a76\u8fd8\u5173\u6ce8\u4e86\u5c0f\u6570\u636e\u91cf\u4e0b\u7684\u9057\u5fd8\u73b0\u8c61\uff0c\u5e76\u6bd4\u8f83\u4e86RAG\u65b9\u6cd5\u548c\u53c2\u6570\u5316\u65b9\u6cd5\u5728\u77e5\u8bc6\u6ce8\u5165\u4e0a\u7684\u8868\u73b0\uff0c\u6700\u540e\u9a8c\u8bc1\u4e86\u6a21\u578b\u81ea\u8eab\u751f\u6210\u5408\u6210\u8bad\u7ec3\u6570\u636e\u7684\u6f5c\u529b\u3002\u201d", "result": "\u201c\u5b9e\u9a8c\u8868\u660e\uff0c\u4ec5\u5728\u6709\u9650\u6570\u636e\u4e0a\u8fdb\u884c\u6301\u7eed\u9884\u8bad\u7ec3\u53ea\u80fd\u5e26\u6765\u9002\u5ea6\u7684\u6539\u8fdb\uff0c\u800c\u66b4\u9732\u4e8e\u591a\u6837\u5316\u7684\u6587\u672c\u53d8\u4f53\uff08\u5c24\u5176\u662f\u901a\u8fc7\u591a\u6837\u5316\u63d0\u793a\u8bf1\u5bfc\u7684\u53d8\u4f53\uff09\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u6a21\u578b\u5b66\u4e60\u65b0\u4e8b\u5b9e\u7684\u80fd\u529b\u3002\u7814\u7a76\u8fd8\u63ed\u793a\u4e86\u5728\u5c0f\u6570\u636e\u91cf\u4e0b\u7684\u9057\u5fd8\u73b0\u8c61\uff0c\u5e76\u5f3a\u8c03\u4e86\u5b66\u4e60\u65b0\u5185\u5bb9\u548c\u4fdd\u7559\u73b0\u6709\u80fd\u529b\u4e4b\u95f4\u7684\u5fae\u5999\u5e73\u8861\u3002\u4e0e\u53c2\u6570\u5316\u65b9\u6cd5\u76f8\u6bd4\uff0cRAG\u65b9\u6cd5\u5728\u77e5\u8bc6\u6ce8\u5165\u65b9\u9762\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u654f\u611f\u6027\uff0c\u5e76\u4e14\u66f4\u5bb9\u6613\u5bfc\u81f4\u5728\u63a7\u5236\u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\u4e0b\u964d\u3002\u6700\u540e\uff0c\u5b9e\u9a8c\u8bc1\u660e\u6a21\u578b\u80fd\u591f\u81ea\u884c\u751f\u6210\u6709\u6548\u7684\u5408\u6210\u8bad\u7ec3\u6570\u636e\uff0c\u4e3a\u901a\u8fc7\u6a21\u578b\u81ea\u6211\u6539\u8fdb\u6765\u66f4\u65b0\u6a21\u578b\u63d0\u4f9b\u4e86\u9014\u5f84\u3002\u201d", "conclusion": "\u201c\u5728\u6709\u9650\u7684\u6570\u636e\u4e0b\uff0c\u901a\u8fc7\u5f15\u5165\u591a\u6837\u5316\u7684\u6587\u672c\u53d8\u4f53\uff0c\u7279\u522b\u662f\u5229\u7528\u6a21\u578b\u81ea\u8eab\u751f\u6210\u5408\u6210\u6570\u636e\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8LLM\u83b7\u53d6\u65b0\u77e5\u8bc6\u7684\u80fd\u529b\uff0c\u540c\u65f6\u7f13\u89e3\u707e\u96be\u6027\u9057\u5fd8\u7684\u73b0\u8c61\u3002RAG\u65b9\u6cd5\u5728\u5c0f\u6570\u636e\u6ce8\u5165\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u53c2\u6570\u5316\u65b9\u6cd5\u5219\u66f4\u5177\u4f18\u52bf\u3002\u201d"}}
{"id": "2508.05994", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05994", "abs": "https://arxiv.org/abs/2508.05994", "authors": ["Huadong Wu", "Yi Fu", "Yunhao Li", "Yuan Gao", "Kang Du"], "title": "EvoMakeup: High-Fidelity and Controllable Makeup Editing with MakeupQuad", "comment": null, "summary": "Facial makeup editing aims to realistically transfer makeup from a reference\nto a target face. Existing methods often produce low-quality results with\ncoarse makeup details and struggle to preserve both identity and makeup\nfidelity, mainly due to the lack of structured paired data -- where source and\nresult share identity, and reference and result share identical makeup. To\naddress this, we introduce MakeupQuad, a large-scale, high-quality dataset with\nnon-makeup faces, references, edited results, and textual makeup descriptions.\nBuilding on this, we propose EvoMakeup, a unified training framework that\nmitigates image degradation during multi-stage distillation, enabling iterative\nimprovement of both data and model quality. Although trained solely on\nsynthetic data, EvoMakeup generalizes well and outperforms prior methods on\nreal-world benchmarks. It supports high-fidelity, controllable, multi-task\nmakeup editing -- including full-face and partial reference-based editing, as\nwell as text-driven makeup editing -- within a single model. Experimental\nresults demonstrate that our method achieves superior makeup fidelity and\nidentity preservation, effectively balancing both aspects. Code and dataset\nwill be released upon acceptance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a EvoMakeup \u7684\u65b0\u65b9\u6cd5\u548c MakeupQuad \u6570\u636e\u96c6\uff0c\u7528\u4e8e\u89e3\u51b3\u73b0\u6709\u9762\u90e8\u5316\u5986\u7f16\u8f91\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002EvoMakeup \u901a\u8fc7\u591a\u9636\u6bb5\u84b8\u998f\u548c\u8fed\u4ee3\u6539\u8fdb\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u3001\u53ef\u63a7\u7684\u5986\u5bb9\u7f16\u8f91\uff0c\u5e76\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u9762\u90e8\u5316\u5986\u7f16\u8f91\u65b9\u6cd5\u7531\u4e8e\u7f3a\u4e4f\u7ed3\u6784\u5316\u7684\u914d\u5bf9\u6570\u636e\uff0c\u5e38\u5e38\u4ea7\u751f\u5986\u5bb9\u7ec6\u8282\u7c97\u7cd9\u3001\u96be\u4ee5\u540c\u65f6\u4fdd\u7559\u8eab\u4efd\u548c\u5986\u5bb9\u4fdd\u771f\u5ea6\u7684\u4f4e\u8d28\u91cf\u7ed3\u679c\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a EvoMakeup \u7684\u7edf\u4e00\u8bad\u7ec3\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u591a\u9636\u6bb5\u84b8\u998f\u6765\u51cf\u8f7b\u56fe\u50cf\u9000\u5316\uff0c\u4ece\u800c\u80fd\u591f\u8fed\u4ee3\u5730\u6539\u8fdb\u6570\u636e\u548c\u6a21\u578b\u8d28\u91cf\u3002\u5f15\u5165\u4e86\u4e00\u4e2a\u540d\u4e3a MakeupQuad \u7684\u5927\u89c4\u6a21\u3001\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5305\u542b\u65e0\u5986\u9762\u5b54\u3001\u53c2\u8003\u5986\u5bb9\u3001\u7f16\u8f91\u7ed3\u679c\u548c\u6587\u672c\u5986\u5bb9\u63cf\u8ff0\u3002", "result": "EvoMakeup \u5728\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u5ea6\u7684\u5986\u5bb9\u7f16\u8f91\uff0c\u5e76\u80fd\u6709\u6548\u4fdd\u7559\u8eab\u4efd\u4fe1\u606f\u3002", "conclusion": "EvoMakeup \u6846\u67b6\u5728\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u5ea6\u3001\u53ef\u63a7\u7684\u591a\u4efb\u52a1\u5986\u5bb9\u7f16\u8f91\uff0c\u6709\u6548\u5e73\u8861\u4e86\u5986\u5bb9\u4fdd\u771f\u5ea6\u548c\u8eab\u4efd\u4fdd\u7559\u3002"}}
{"id": "2508.06426", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06426", "abs": "https://arxiv.org/abs/2508.06426", "authors": ["Youguang Xing", "Xu Luo", "Junlin Xie", "Lianli Gao", "Hengtao Shen", "Jingkuan Song"], "title": "Shortcut Learning in Generalist Robot Policies: The Role of Dataset Diversity and Fragmentation", "comment": "CoRL 2025", "summary": "Generalist robot policies trained on large-scale datasets such as Open\nX-Embodiment (OXE) demonstrate strong performance across a wide range of tasks.\nHowever, they often struggle to generalize beyond the distribution of their\ntraining data. In this paper, we investigate the underlying cause of this\nlimited generalization capability. We identify shortcut learning -- the\nreliance on task-irrelevant features -- as a key impediment to generalization.\nThrough comprehensive theoretical and empirical analysis, we uncover two\nprimary contributors to shortcut learning: (1) limited diversity within\nindividual sub-datasets, and (2) significant distributional disparities across\nsub-datasets, leading to dataset fragmentation. These issues arise from the\ninherent structure of large-scale datasets like OXE, which are typically\ncomposed of multiple sub-datasets collected independently across varied\nenvironments and embodiments. Our findings provide critical insights into\ndataset collection strategies that can reduce shortcut learning and enhance the\ngeneralization ability of generalist robot policies. Moreover, in scenarios\nwhere acquiring new large-scale data is impractical, we demonstrate that\ncarefully selected robotic data augmentation strategies can effectively reduce\nshortcut learning in existing offline datasets, thereby improving\ngeneralization capabilities of generalist robot policies, e.g., $\\pi_0$, in\nboth simulation and real-world environments. More information at\nhttps://lucky-light-sun.github.io/proj/shortcut-learning-in-grps/.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.06352", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.06352", "abs": "https://arxiv.org/abs/2508.06352", "authors": ["Christian Meske", "Justin Brenne", "Erdi Uenal", "Sabahat Oelcer", "Ayseguel Doganguen"], "title": "From Explainable to Explanatory Artificial Intelligence: Toward a New Paradigm for Human-Centered Explanations through Generative AI", "comment": null, "summary": "Current explainable AI (XAI) approaches prioritize algorithmic transparency\nand present explanations in abstract, non-adaptive formats that often fail to\nsupport meaningful end-user understanding. This paper introduces \"Explanatory\nAI\" as a complementary paradigm that leverages generative AI capabilities to\nserve as explanatory partners for human understanding rather than providers of\nalgorithmic transparency. While XAI reveals algorithmic decision processes for\nmodel validation, Explanatory AI addresses contextual reasoning to support\nhuman decision-making in sociotechnical contexts. We develop a definition and\nsystematic eight-dimensional conceptual model distinguishing Explanatory AI\nthrough narrative communication, adaptive personalization, and progressive\ndisclosure principles. Empirical validation through Rapid Contextual Design\nmethodology with healthcare professionals demonstrates that users consistently\nprefer context-sensitive, multimodal explanations over technical transparency.\nOur findings reveal the practical urgency for AI systems designed for human\ncomprehension rather than algorithmic introspection, establishing a\ncomprehensive research agenda for advancing user-centered AI explanation\napproaches across diverse domains and cultural contexts.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u201c\u89e3\u91ca\u6027AI\u201d\u6982\u5ff5\uff0c\u5f3a\u8c03AI\u5e94\u4f5c\u4e3a\u4eba\u7c7b\u7406\u89e3\u7684\u4f19\u4f34\uff0c\u63d0\u4f9b\u9002\u5e94\u6027\u3001\u60c5\u5883\u5316\u7684\u89e3\u91ca\uff0c\u800c\u975e\u4ec5\u4ec5\u8ffd\u6c42\u7b97\u6cd5\u900f\u660e\u5ea6\u3002\u7814\u7a76\u901a\u8fc7\u7528\u6237\u5b9e\u9a8c\u8bc1\u660e\uff0c\u7528\u6237\u66f4\u559c\u6b22\u8fd9\u79cd\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u89e3\u91ca\u65b9\u5f0f\uff0c\u5e76\u4e3a\u672a\u6765AI\u89e3\u91ca\u7814\u7a76\u6307\u660e\u4e86\u65b9\u5411\u3002", "motivation": "\u5f53\u524d\u7684XAI\u65b9\u6cd5\u8fc7\u4e8e\u5173\u6ce8\u7b97\u6cd5\u900f\u660e\u5ea6\uff0c\u5176\u89e3\u91ca\u683c\u5f0f\u62bd\u8c61\u4e14\u7f3a\u4e4f\u9002\u5e94\u6027\uff0c\u672a\u80fd\u6709\u6548\u652f\u6301\u7528\u6237\u7684\u7406\u89e3\u3002\u56e0\u6b64\uff0c\u63d0\u51fa\u201c\u89e3\u91ca\u6027AI\u201d\u4f5c\u4e3a\u4e00\u79cd\u8865\u5145\u8303\u5f0f\uff0c\u5229\u7528\u751f\u6210\u5f0fAI\u7684\u80fd\u529b\uff0c\u5145\u5f53\u4eba\u7c7b\u7406\u89e3\u7684\u4f19\u4f34\uff0c\u800c\u975e\u4ec5\u4ec5\u63d0\u4f9b\u7b97\u6cd5\u900f\u660e\u5ea6\u3002", "method": "\u901a\u8fc7\u5feb\u901f\u60c5\u5883\u8bbe\u8ba1\u65b9\u6cd5\uff08Rapid Contextual Design\uff09\u8fdb\u884c\u5b9e\u8bc1\u9a8c\u8bc1\uff0c\u5e76\u4e0e\u533b\u7597\u4e13\u4e1a\u4eba\u5458\u5408\u4f5c\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5305\u542b\u516b\u4e2a\u7ef4\u5ea6\u7684\u6982\u5ff5\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u901a\u8fc7\u53d9\u4e8b\u6027\u6c9f\u901a\u3001\u81ea\u9002\u5e94\u4e2a\u6027\u5316\u548c\u6e10\u8fdb\u5f0f\u62ab\u9732\u539f\u5219\u6765\u533a\u5206\u89e3\u91ca\u6027AI\u3002\u5b9e\u8bc1\u7814\u7a76\u8868\u660e\uff0c\u7528\u6237\u4e00\u81f4\u504f\u597d\u4e0a\u4e0b\u6587\u654f\u611f\u3001\u591a\u6a21\u6001\u7684\u89e3\u91ca\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u7528\u6237\u66f4\u503e\u5411\u4e8e\u9009\u62e9\u4e0a\u4e0b\u6587\u611f\u77e5\u3001\u591a\u6a21\u6001\u89e3\u91ca\uff0c\u800c\u975e\u6280\u672f\u900f\u660e\u5ea6\u3002\u8fd9\u5f3a\u8c03\u4e86\u8bbe\u8ba1\u9762\u5411\u4eba\u7c7b\u7406\u89e3\u800c\u975e\u7b97\u6cd5\u81ea\u7701\u7684AI\u7cfb\u7edf\u7684\u5b9e\u9645\u9700\u6c42\uff0c\u5e76\u4e3a\u5728\u4e0d\u540c\u9886\u57df\u548c\u6587\u5316\u80cc\u666f\u4e0b\u63a8\u8fdb\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684AI\u89e3\u91ca\u65b9\u6cd5\u5960\u5b9a\u4e86\u5168\u9762\u7684\u7814\u7a76\u8bae\u7a0b\u3002"}}
{"id": "2508.06183", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06183", "abs": "https://arxiv.org/abs/2508.06183", "authors": ["Xiyuan Yang", "Shengyuan Hu", "Soyeon Kim", "Tian Li"], "title": "Differentially Private Federated Clustering with Random Rebalancing", "comment": "21 pages", "summary": "Federated clustering aims to group similar clients into clusters and produce\none model for each cluster. Such a personalization approach typically improves\nmodel performance compared with training a single model to serve all clients,\nbut can be more vulnerable to privacy leakage. Directly applying client-level\ndifferentially private (DP) mechanisms to federated clustering could degrade\nthe utilities significantly. We identify that such deficiencies are mainly due\nto the difficulties of averaging privacy noise within each cluster (following\nstandard privacy mechanisms), as the number of clients assigned to the same\nclusters is uncontrolled. To this end, we propose a simple and effective\ntechnique, named RR-Cluster, that can be viewed as a light-weight add-on to\nmany federated clustering algorithms. RR-Cluster achieves reduced privacy noise\nvia randomly rebalancing cluster assignments, guaranteeing a minimum number of\nclients assigned to each cluster. We analyze the tradeoffs between decreased\nprivacy noise variance and potentially increased bias from incorrect\nassignments and provide convergence bounds for RR-Clsuter. Empirically, we\ndemonstrate the RR-Cluster plugged into strong federated clustering algorithms\nresults in significantly improved privacy/utility tradeoffs across both\nsynthetic and real-world datasets.", "AI": {"tldr": "RR-Cluster \u662f\u4e00\u79cd\u7528\u4e8e\u8054\u90a6\u805a\u7c7b\u7684\u6280\u672f\uff0c\u901a\u8fc7\u968f\u673a\u91cd\u65b0\u5e73\u8861\u805a\u7c7b\u5206\u914d\u6765\u51cf\u5c11\u9690\u79c1\u566a\u58f0\uff0c\u4ece\u800c\u63d0\u9ad8\u9690\u79c1/\u6548\u7528\u6743\u8861\u3002", "motivation": "\u8054\u90a6\u805a\u7c7b\u65e8\u5728\u5bf9\u76f8\u4f3c\u5ba2\u6237\u8fdb\u884c\u5206\u7ec4\uff0c\u4f46\u53ef\u80fd\u66f4\u5bb9\u6613\u53d7\u5230\u9690\u79c1\u6cc4\u9732\u7684\u5f71\u54cd\u3002\u76f4\u63a5\u5c06\u5ba2\u6237\u7aef\u7ea7\u522b\u7684\u5dee\u5206\u9690\u79c1\uff08DP\uff09\u673a\u5236\u5e94\u7528\u4e8e\u8054\u90a6\u805a\u7c7b\u53ef\u80fd\u4f1a\u663e\u8457\u964d\u4f4e\u6548\u7528\uff0c\u56e0\u4e3a\u5728\u805a\u7c7b\u4e2d\u7684\u5ba2\u6237\u6570\u91cf\u4e0d\u53d7\u63a7\u5236\uff0c\u5bfc\u81f4\u9690\u79c1\u566a\u58f0\u96be\u4ee5\u5e73\u5747\u3002", "method": "RR-Cluster \u901a\u8fc7\u968f\u673a\u91cd\u65b0\u5e73\u8861\u805a\u7c7b\u5206\u914d\u6765\u4fdd\u8bc1\u6bcf\u4e2a\u805a\u7c7b\u7684\u6700\u5c0f\u5ba2\u6237\u6570\u91cf\uff0c\u4ece\u800c\u5b9e\u73b0\u964d\u4f4e\u9690\u79c1\u566a\u58f0\u3002", "result": "RR-Cluster \u80fd\u591f\u663e\u8457\u6539\u5584\u8de8\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u7684\u9690\u79c1/\u6548\u7528\u6743\u8861\u3002", "conclusion": "RR-Cluster \u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u6280\u672f\uff0c\u4f5c\u4e3a\u8bb8\u591a\u8054\u90a6\u805a\u7c7b\u7b97\u6cd5\u7684\u8f7b\u91cf\u7ea7\u9644\u52a0\u7ec4\u4ef6\uff0c\u901a\u8fc7\u968f\u673a\u91cd\u65b0\u5e73\u8861\u805a\u7c7b\u5206\u914d\u6765\u51cf\u5c11\u9690\u79c1\u566a\u58f0\uff0c\u4fdd\u8bc1\u6bcf\u4e2a\u805a\u7c7b\u7684\u6700\u5c0f\u5ba2\u6237\u6570\u91cf\u3002\u8be5\u6280\u672f\u5728\u9690\u79c1/\u6548\u7528\u6743\u8861\u65b9\u9762\u8868\u73b0\u51fa\u663e\u8457\u6539\u8fdb\u3002"}}
{"id": "2508.06210", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.06210", "abs": "https://arxiv.org/abs/2508.06210", "authors": ["Ivan Saychenko", "Robert Weiss", "Rita Veilande", "Scott Parkins", "Mark Sadgrove", "Sandro Wimberger"], "title": "Detecting entanglement between quantum emitters using directional emission", "comment": null, "summary": "Recently, it was shown that quantum interference in a system containing a\npolarized and unpolarized emitter can allow directional emission of photons\ninto a circulating cavity. Here, we ask whether high directionality of photon\nemission in this system implies a high degree of quantum correlation between\nthe two emitters. We show that the answer is a qualified \"yes\", with photon\nemission directionality and emitter-emitter entanglement showing a monotonic\nrelationship over a broad parameter range. The relationship only breaks down in\nthe limit of perfect directionality. Furthermore, under reasonable assumptions\nfor experimental parameters and stability, we show that the statistics of\nmeasured directionality allow a reliable estimate of the concurrence. This\nresult implies that directionality of photon emission in the state preparation\nstage can be used to determine the entanglement between the emitters, with\npotential applications to more generic cases including quantum networks.", "AI": {"tldr": "\u5149\u5b50\u53d1\u5c04\u65b9\u5411\u6027\u4e0e\u91cf\u5b50\u7ea0\u7f20\u76f8\u5173\uff0c\u53ef\u7528\u4e8e\u4f30\u7b97\u7ea0\u7f20\u5ea6\u3002", "motivation": "\u7814\u7a76\u9ad8\u65b9\u5411\u6027\u5149\u5b50\u53d1\u5c04\u662f\u5426\u610f\u5473\u7740\u53d1\u5c04\u4f53\u4e4b\u95f4\u5b58\u5728\u9ad8\u5ea6\u91cf\u5b50\u5173\u8054\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u5c55\u793a\u4e86\u5149\u5b50\u53d1\u5c04\u65b9\u5411\u6027\u548c\u53d1\u5c04\u4f53-\u53d1\u5c04\u4f53\u7ea0\u7f20\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "result": "\u5149\u5b50\u53d1\u5c04\u65b9\u5411\u6027\u548c\u53d1\u5c04\u4f53-\u53d1\u5c04\u4f53\u7ea0\u7f20\u4e4b\u95f4\u5b58\u5728\u5355\u8c03\u5173\u7cfb\uff0c\u4e14\u5728\u5408\u7406\u5b9e\u9a8c\u6761\u4ef6\u4e0b\uff0c\u5149\u5b50\u53d1\u5c04\u65b9\u5411\u6027\u7684\u6d4b\u91cf\u7edf\u8ba1\u53ef\u7528\u4e8e\u4f30\u8ba1\u53d1\u5c04\u4f53\u4e4b\u95f4\u7684\u5e76\u53d1\u5ea6\u3002", "conclusion": "\u53ef\u5b9a\u5411\u5149\u5b50\u53d1\u5c04\u548c\u53d1\u5c04\u4f53-\u53d1\u5c04\u4f53\u7ea0\u7f20\u5728\u5927\u90e8\u5206\u53c2\u6570\u8303\u56f4\u5185\u5b58\u5728\u5355\u8c03\u5173\u7cfb\uff0c\u4f46\u5728\u5b8c\u7f8e\u65b9\u5411\u6027\u6781\u9650\u4e0b\u4f1a\u5931\u6548\u3002"}}
{"id": "2508.06186", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06186", "abs": "https://arxiv.org/abs/2508.06186", "authors": ["Ali Sarabadani", "Maryam Abdollahi Shamami", "Hamidreza Sadeghsalehi", "Borhan Asadi", "Saba Hesaraki"], "title": "DKG-LLM : A Framework for Medical Diagnosis and Personalized Treatment Recommendations via Dynamic Knowledge Graph and Large Language Model Integration", "comment": null, "summary": "Large Language Models (LLMs) have grown exponentially since the release of\nChatGPT. These models have gained attention due to their robust performance on\nvarious tasks, including language processing tasks. These models achieve\nunderstanding and comprehension of tasks by training billions of parameters.\nThe development of these models is a transformative force in enhancing natural\nlanguage understanding and has taken a significant step towards artificial\ngeneral intelligence (AGI). In this study, we aim to present the DKG-LLM\nframework. The DKG-LLM framework introduces a groundbreaking approach to\nmedical diagnosis and personalized treatment recommendations by integrating a\ndynamic knowledge graph (DKG) with the Grok 3 large language model. Using the\nAdaptive Semantic Fusion Algorithm (ASFA), heterogeneous medical data\n(including clinical reports and PubMed articles) and patient records\ndynamically generate a knowledge graph consisting of 15,964 nodes in 13\ndistinct types (e.g., diseases, symptoms, treatments, patient profiles) and\n127,392 edges in 26 relationship types (e.g., causal, therapeutic,\nassociation). ASFA utilizes advanced probabilistic models, Bayesian inference,\nand graph optimization to extract semantic information, dynamically updating\nthe graph with approximately 150 new nodes and edges in each data category\nwhile maintaining scalability with up to 987,654 edges. Real-world datasets,\nincluding MIMIC-III and PubMed, were utilized to evaluate the proposed\narchitecture. The evaluation results show that DKG-LLM achieves a diagnostic\naccuracy of 84.19%. The model also has a treatment recommendation accuracy of\n89.63% and a semantic coverage of 93.48%. DKG-LLM is a reliable and\ntransformative tool that handles noisy data and complex multi-symptom diseases,\nalong with feedback-based learning from physician input.", "AI": {"tldr": "DKG-LLM \u6846\u67b6\u6574\u5408\u4e86\u52a8\u6001\u77e5\u8bc6\u56fe (DKG) \u548c Grok 3 LLM\uff0c\u901a\u8fc7 ASFA \u7b97\u6cd5\u5904\u7406\u533b\u7597\u6570\u636e\uff0c\u5728\u533b\u7597\u8bca\u65ad\u548c\u6cbb\u7597\u63a8\u8350\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u51c6\u786e\u7387\u5206\u522b\u4e3a 84.19% \u548c 89.63%\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7\u6574\u5408\u52a8\u6001\u77e5\u8bc6\u56fe (DKG) \u4e0e Grok 3 \u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u63d0\u51fa DKG-LLM \u6846\u67b6\uff0c\u4ee5\u5b9e\u73b0\u533b\u7597\u8bca\u65ad\u548c\u4e2a\u6027\u5316\u6cbb\u7597\u63a8\u8350\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86 DKG-LLM \u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u52a8\u6001\u77e5\u8bc6\u56fe (DKG) \u4e0e Grok 3 \u5927\u578b\u8bed\u8a00\u6a21\u578b\u76f8\u7ed3\u5408\uff0c\u5e76\u5229\u7528\u81ea\u9002\u5e94\u8bed\u4e49\u878d\u5408\u7b97\u6cd5 (ASFA) \u5904\u7406\u5f02\u6784\u533b\u7597\u6570\u636e\u548c\u60a3\u8005\u8bb0\u5f55\uff0c\u52a8\u6001\u751f\u6210\u77e5\u8bc6\u56fe\u3002", "result": "DKG-LLM \u5728 MIMIC-III \u548c PubMed \u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0c\u5176\u8bca\u65ad\u51c6\u786e\u7387\u4e3a 84.19%\uff0c\u6cbb\u7597\u63a8\u8350\u51c6\u786e\u7387\u4e3a 89.63%\uff0c\u8bed\u4e49\u8986\u76d6\u7387\u4e3a 93.48%\u3002", "conclusion": "DKG-LLM \u662f\u4e00\u4e2a\u53ef\u9760\u4e14\u5177\u6709\u53d8\u9769\u6027\u7684\u5de5\u5177\uff0c\u53ef\u5904\u7406\u5608\u6742\u7684\u6570\u636e\u548c\u590d\u6742\u7684\u591a\u75c7\u72b6\u75be\u75c5\uff0c\u5e76\u80fd\u4ece\u533b\u751f\u90a3\u91cc\u83b7\u5f97\u57fa\u4e8e\u53cd\u9988\u7684\u5b66\u4e60\u3002"}}
{"id": "2508.06009", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06009", "abs": "https://arxiv.org/abs/2508.06009", "authors": ["Jun Feng", "Zixin Wang", "Zhentao Zhang", "Yue Guo", "Zhihan Zhou", "Xiuyi Chen", "Zhenyang Li", "Dawei Yin"], "title": "MathReal: We Keep It Real! A Real Scene Benchmark for Evaluating Math Reasoning in Multimodal Large Language Models", "comment": "29 pages, 16 figures", "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities in visual mathematical reasoning across various existing\nbenchmarks. However, these benchmarks are predominantly based on clean or\nprocessed multimodal inputs, without incorporating the images provided by\nreal-world Kindergarten through 12th grade (K-12) educational users. To address\nthis gap, we introduce MathReal, a meticulously curated dataset comprising\n2,000 mathematical questions with images captured by handheld mobile devices in\nauthentic scenarios. Each question is an image, containing the question text\nand visual element. We systematically classify the real images into three\nprimary categories: image quality degradation, perspective variation, and\nirrelevant content interference, which are further delineated into 14\nsubcategories. Additionally, MathReal spans five core knowledge and ability\ncategories, which encompass three question types and are divided into three\ndifficulty levels. To comprehensively evaluate the multimodal mathematical\nreasoning abilities of state-of-the-art MLLMs in real-world scenarios, we\ndesign six experimental settings that enable a systematic analysis of their\nperformance. Through extensive experimentation, we find that the\nproblem-solving abilities of existing MLLMs are significantly challenged in\nrealistic educational contexts. Based on this, we conduct a thorough analysis\nof their performance and error patterns, providing insights into their\nrecognition, comprehension, and reasoning capabilities, and outlining\ndirections for future improvements. Data and code:\nhttps://github.com/junfeng0288/MathReal.", "AI": {"tldr": "\u7531\u4e8e\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u672a\u80fd\u5145\u5206\u6a21\u62df\u771f\u5b9e\u4e16\u754cK-12\u6559\u80b2\u7528\u6237\u7684\u624b\u6301\u8bbe\u5907\u56fe\u50cf\u8f93\u5165\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u4e86MathReal\u6570\u636e\u96c6\uff0c\u5305\u542b2000\u4e2a\u5305\u542b\u6570\u5b66\u95ee\u9898\u7684\u771f\u5b9e\u573a\u666f\u56fe\u50cf\u3002\u5b9e\u9a8c\u8bc4\u4f30\u4e86\u73b0\u6709MLLM\u5728\u8fd9\u4e9b\u771f\u5b9e\u573a\u666f\u4e0b\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5176\u80fd\u529b\u53d7\u5230\u663e\u8457\u5f71\u54cd\uff0c\u5e76\u5bf9\u6a21\u578b\u7684\u9519\u8bef\u6a21\u5f0f\u8fdb\u884c\u4e86\u5206\u6790\uff0c\u4e3a\u672a\u6765\u7684\u6a21\u578b\u6539\u8fdb\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u4f7f\u7528\u6e05\u6670\u6216\u5904\u7406\u8fc7\u7684\u591a\u6a21\u6001\u8f93\u5165\uff0c\u672a\u80fd\u53cd\u6620\u771f\u5b9eK-12\u6559\u80b2\u7528\u6237\u901a\u8fc7\u624b\u673a\u62cd\u6444\u7684\u56fe\u50cf\u8f93\u5165\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aMathReal\u7684\u6570\u636e\u96c6\uff0c\u5305\u542b2000\u4e2a\u771f\u5b9e\u4e16\u754cK-12\u6559\u80b2\u573a\u666f\u4e0b\u7528\u624b\u673a\u62cd\u6444\u7684\u6570\u5b66\u95ee\u9898\u56fe\u50cf\u3002\u5bf9\u56fe\u50cf\u8fdb\u884c\u4e86\u8be6\u7ec6\u7684\u5206\u7c7b\uff08\u5305\u62ec\u56fe\u50cf\u8d28\u91cf\u3001\u89c6\u89d2\u548c\u65e0\u5173\u5185\u5bb9\u7b4914\u4e2a\u5b50\u7c7b\uff09\uff0c\u5e76\u6db5\u76d6\u4e86\u4e94\u79cd\u6838\u5fc3\u77e5\u8bc6\u80fd\u529b\u3001\u4e09\u79cd\u95ee\u9898\u7c7b\u578b\u548c\u4e09\u4e2a\u96be\u5ea6\u7ea7\u522b\u3002\u8bbe\u8ba1\u4e86\u516d\u79cd\u5b9e\u9a8c\u8bbe\u7f6e\u6765\u8bc4\u4f30\u73b0\u6709MLLM\u7684\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u73b0\u6709MLLM\u5728\u771f\u5b9e\u6559\u80b2\u573a\u666f\u4e0b\u7684\u89e3\u51b3\u95ee\u9898\u80fd\u529b\u9762\u4e34\u4e25\u5cfb\u6311\u6218\uff0c\u8bba\u6587\u6df1\u5165\u5206\u6790\u4e86\u6a21\u578b\u7684\u8868\u73b0\u548c\u9519\u8bef\u6a21\u5f0f\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u5728\u8bc6\u522b\u3001\u7406\u89e3\u548c\u63a8\u7406\u65b9\u9762\u7684\u80fd\u529b\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u6539\u8fdb\u7684\u65b9\u5411\u3002", "conclusion": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u771f\u5b9e\u6559\u80b2\u573a\u666f\u4e0b\u89e3\u51b3\u6570\u5b66\u95ee\u9898\u7684\u80fd\u529b\u53d7\u5230\u663e\u8457\u6311\u6218\u3002\u901a\u8fc7\u5206\u6790\u5176\u5728\u56fe\u50cf\u8d28\u91cf\u4e0b\u964d\u3001\u89c6\u89d2\u53d8\u5316\u548c\u65e0\u5173\u5185\u5bb9\u5e72\u6270\u7b49\u65b9\u9762\u7684\u8868\u73b0\uff0c\u8bba\u6587\u4e3a\u672a\u6765\u6539\u8fdb\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u6570\u5b66\u63a8\u7406\u80fd\u529b\u65b9\u9762\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2508.06368", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06368", "abs": "https://arxiv.org/abs/2508.06368", "authors": ["Claudia dAmato", "Giuseppe Rubini", "Francesco Didio", "Donato Francioso", "Fatima Zahra Amara", "Nicola Fanizzi"], "title": "Automated Creation of the Legal Knowledge Graph Addressing Legislation on Violence Against Women: Resource, Methodology and Lessons Learned", "comment": null, "summary": "Legal decision-making process requires the availability of comprehensive and\ndetailed legislative background knowledge and up-to-date information on legal\ncases and related sentences/decisions. Legal Knowledge Graphs (KGs) would be a\nvaluable tool to facilitate access to legal information, to be queried and\nexploited for the purpose, and to enable advanced reasoning and machine\nlearning applications. Indeed, legal KGs may act as knowledge intensive\ncomponent to be used by pre-dictive machine learning solutions supporting the\ndecision process of the legal expert. Nevertheless, a few KGs can be found in\nthe legal domain. To fill this gap, we developed a legal KG targeting legal\ncases of violence against women, along with clear adopted methodologies.\nSpecifically, the paper introduces two complementary approaches for automated\nlegal KG construction; a systematic bottom-up approach, customized for the\nlegal domain, and a new solution leveraging Large Language Models. Starting\nfrom legal sentences publicly available from the European Court of Justice, the\nsolutions integrate structured data extraction, ontology development, and\nsemantic enrichment to produce KGs tailored for legal cases involving violence\nagainst women. After analyzing and comparing the results of the two approaches,\nthe developed KGs are validated via suitable competency questions. The obtained\nKG may be impactful for multiple purposes: can improve the accessibility to\nlegal information both to humans and machine, can enable complex queries and\nmay constitute an important knowledge component to be possibly exploited by\nmachine learning tools tailored for predictive justice.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e24\u79cd\u65b9\u6cd5\uff08\u81ea\u4e0b\u800c\u4e0a\u548c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff09\u6765\u6784\u5efa\u9488\u5bf9\u66b4\u529b\u4fb5\u5bb3\u5987\u5973\u6848\u4ef6\u7684\u6cd5\u5f8b\u77e5\u8bc6\u56fe\u8c31\uff0c\u4ee5\u63d0\u9ad8\u6cd5\u5f8b\u4fe1\u606f\u7684\u6613\u8bbf\u95ee\u6027\u5e76\u652f\u6301\u673a\u5668\u5b66\u4e60\u5e94\u7528\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u6cd5\u5f8b\u9886\u57df\u4e2d\u6cd5\u5f8b\u77e5\u8bc6\u56fe\u8c31\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u672c\u7814\u7a76\u65e8\u5728\u521b\u5efa\u4e00\u4e2a\u9488\u5bf9\u66b4\u529b\u4fb5\u5bb3\u5987\u5973\u6848\u4ef6\u7684\u6cd5\u5f8b\u77e5\u8bc6\u56fe\u8c31\uff0c\u4ee5\u652f\u6301\u6cd5\u5f8b\u51b3\u7b56\u8fc7\u7a0b\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e24\u79cd\u4e92\u8865\u7684\u65b9\u6cd5\u6765\u81ea\u52a8\u6784\u5efa\u6cd5\u5f8b\u77e5\u8bc6\u56fe\u8c31\uff1a\u4e00\u79cd\u662f\u9488\u5bf9\u6cd5\u5f8b\u9886\u57df\u5b9a\u5236\u7684\u7cfb\u7edf\u5316\u81ea\u4e0b\u800c\u4e0a\u65b9\u6cd5\uff0c\u53e6\u4e00\u79cd\u662f\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u65b0\u89e3\u51b3\u65b9\u6848\u3002\u4ece\u6b27\u6d32\u6cd5\u9662\u516c\u5f00\u7684\u6cd5\u5f8b\u5224\u4f8b\u5165\u624b\uff0c\u7ed3\u5408\u7ed3\u6784\u5316\u6570\u636e\u63d0\u53d6\u3001\u672c\u4f53\u5f00\u53d1\u548c\u8bed\u4e49\u4e30\u5bcc\uff0c\u6784\u5efa\u9488\u5bf9\u6d89\u53ca\u66b4\u529b\u4fb5\u5bb3\u5987\u5973\u6848\u4ef6\u7684\u77e5\u8bc6\u56fe\u8c31\u3002", "result": "\u7814\u7a76\u5f00\u53d1\u4e86\u4e24\u79cd\u65b9\u6cd5\u6765\u6784\u5efa\u6cd5\u5f8b\u77e5\u8bc6\u56fe\u8c31\uff0c\u5e76\u901a\u8fc7\u80fd\u529b\u95ee\u9898\u5bf9\u751f\u6210\u7684\u77e5\u8bc6\u56fe\u8c31\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6cd5\u5f8b\u77e5\u8bc6\u56fe\u8c31\uff08KG\uff09\u53ef\u63d0\u9ad8\u6cd5\u5f8b\u4fe1\u606f\u7684\u6613\u8bbf\u95ee\u6027\uff0c\u652f\u6301\u590d\u6742\u67e5\u8be2\uff0c\u5e76\u53ef\u4f5c\u4e3a\u652f\u6301\u9884\u6d4b\u6027\u53f8\u6cd5\u7684\u673a\u5668\u5b66\u4e60\u5de5\u5177\u7684\u77e5\u8bc6\u7ec4\u6210\u90e8\u5206\u3002"}}
{"id": "2508.06199", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06199", "abs": "https://arxiv.org/abs/2508.06199", "authors": ["Mateusz Praski", "Jakub Adamczyk", "Wojciech Czech"], "title": "Benchmarking Pretrained Molecular Embedding Models For Molecular Representation Learning", "comment": null, "summary": "Pretrained neural networks have attracted significant interest in chemistry\nand small molecule drug design. Embeddings from these models are widely used\nfor molecular property prediction, virtual screening, and small data learning\nin molecular chemistry. This study presents the most extensive comparison of\nsuch models to date, evaluating 25 models across 25 datasets. Under a fair\ncomparison framework, we assess models spanning various modalities,\narchitectures, and pretraining strategies. Using a dedicated hierarchical\nBayesian statistical testing model, we arrive at a surprising result: nearly\nall neural models show negligible or no improvement over the baseline ECFP\nmolecular fingerprint. Only the CLAMP model, which is also based on molecular\nfingerprints, performs statistically significantly better than the\nalternatives. These findings raise concerns about the evaluation rigor in\nexisting studies. We discuss potential causes, propose solutions, and offer\npractical recommendations.", "AI": {"tldr": "\u795e\u7ecf\u7f51\u7edc\u5728\u5206\u5b50\u5316\u5b66\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u666e\u904d\u4e0d\u5982\u9884\u671f\uff0cCLAMP\u6a21\u578b\u662f\u552f\u4e00\u7684\u4f8b\u5916\uff0c\u8fd9\u8868\u660e\u73b0\u6709\u7814\u7a76\u7684\u8bc4\u4f30\u65b9\u6cd5\u53ef\u80fd\u5b58\u5728\u95ee\u9898\u3002", "motivation": "\u4e3a\u4e86\u5e94\u5bf9\u9884\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u5728\u5316\u5b66\u548c\u836f\u7269\u8bbe\u8ba1\u9886\u57df\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63d0\u4f9b\u4e00\u4e2a\u6700\u5168\u9762\u7684\u6a21\u578b\u6bd4\u8f83\uff0c\u4ee5\u8bc4\u4f30\u5b83\u4eec\u5728\u5206\u5b50\u5316\u5b66\u4e2d\u7684\u5b9e\u9645\u6548\u7528\u3002", "method": "\u672c\u7814\u7a76\u5728\u4e00\u4e2a\u516c\u5e73\u7684\u6bd4\u8f83\u6846\u67b6\u4e0b\uff0c\u4f7f\u7528\u5206\u5c42\u8d1d\u53f6\u65af\u7edf\u8ba1\u68c0\u9a8c\u6a21\u578b\uff0c\u5bf925\u4e2a\u4e0d\u540c\u6a21\u6001\u3001\u67b6\u6784\u548c\u9884\u8bad\u7ec3\u7b56\u7565\u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u572825\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u8bc4\u4f30\u3002", "result": "\u5728\u672c\u6b21\u5e7f\u6cdb\u7684\u6bd4\u8f83\u4e2d\uff0c\u9664\u4e86\u57fa\u4e8e\u5206\u5b50\u6307\u7eb9\u7684CLAMP\u6a21\u578b\u5916\uff0c\u51e0\u4e4e\u6240\u6709\u5176\u4ed6\u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u5728\u5206\u5b50\u5c5e\u6027\u9884\u6d4b\u3001\u865a\u62df\u7b5b\u9009\u548c\u5206\u5b50\u5316\u5b66\u5c0f\u6570\u636e\u5b66\u4e60\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u4e0e\u57fa\u7ebfECFP\u5206\u5b50\u6307\u7eb9\u76f8\u6bd4\uff0c\u90fd\u6ca1\u6709\u7edf\u8ba1\u5b66\u4e0a\u7684\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "\u5927\u591a\u6570\u9884\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u5728\u5206\u5b50\u5316\u5b66\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4ec5\u6709\u57fa\u4e8e\u5206\u5b50\u6307\u7eb9\u7684CLAMP\u6a21\u578b\u663e\u793a\u51fa\u7edf\u8ba1\u5b66\u4e0a\u7684\u663e\u8457\u4f18\u52bf\uff0c\u8fd9\u5f15\u53d1\u4e86\u5bf9\u73b0\u6709\u7814\u7a76\u8bc4\u4f30\u4e25\u8c28\u6027\u7684\u62c5\u5fe7\u3002"}}
{"id": "2508.06223", "categories": ["quant-ph", "physics.optics"], "pdf": "https://arxiv.org/pdf/2508.06223", "abs": "https://arxiv.org/abs/2508.06223", "authors": ["Yichen Zhang", "David Dlaka", "James McDougall", "James Y Tai", "Petros Androvitsaneas", "Edmund Harbord", "Ruth Oulton", "Andrew B. Young"], "title": "Aspheric lens design proposal for near-perfect mode-matching of a broadband quantum dot micropillar to a single-mode fibre", "comment": "8 pages, 5 figures, also contains supplementary document with 2\n  pages, 1 figure", "summary": "Quantum dots in micropillars are one of the most promising options for a\nbright, deterministic single photon source. While highly efficient devices\n(>95%) have been designed, there remains a significant bottleneck that impacts\nthe overall system efficiency: the large numerical aperture of the output mode.\nThis leads to inefficient coupling of emitted photons into single-mode fibre,\nthus limiting practical integration into quantum computing and communication\narchitectures. We show that with the addition of a well designed aspheric SiO2\nmicrolens we can decrease the mode-matching losses to a SMF from 83.1% to\n<0.1(0.1)%. This can result in a single photon source design with 96.4(0.1)%\nend-to-end efficiency, paving the way for scalable photonic quantum\ntechnologies.", "AI": {"tldr": "\u901a\u8fc7\u5728\u91cf\u5b50\u70b9\u5fae\u67f1\u4e2d\u6dfb\u52a0SiO2\u5fae\u900f\u955c\uff0c\u89e3\u51b3\u4e86\u5149\u5b50\u8026\u5408\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u7aef\u5230\u7aef\u6548\u7387\uff0c\u4e3a\u91cf\u5b50\u6280\u672f\u7684\u53d1\u5c55\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "motivation": "\u91cf\u5b50\u70b9\u5fae\u67f1\u662f\u5236\u9020\u660e\u4eae\u3001\u786e\u5b9a\u6027\u5355\u5149\u5b50\u6e90\u7684\u6709\u5e0c\u671b\u7684\u9009\u62e9\uff0c\u4f46\u8f93\u51fa\u6a21\u5f0f\u7684\u5927\u6570\u503c\u5b54\u5f84\u9650\u5236\u4e86\u5176\u4e0e\u5355\u6a21\u5149\u7ea4\u7684\u8026\u5408\u6548\u7387\uff0c\u963b\u788d\u4e86\u5176\u5728\u91cf\u5b50\u8ba1\u7b97\u548c\u901a\u4fe1\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u91cf\u5b50\u70b9\u5fae\u67f1\u4e2d\u52a0\u5165\u975e\u7403\u9762SiO2\u5fae\u900f\u955c\u7684\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u8f93\u51fa\u6a21\u5f0f\u5927\u6570\u503c\u5b54\u5f84\u5bfc\u81f4\u7684\u5149\u5b50\u8026\u5408\u6548\u7387\u4f4e\u7684\u95ee\u9898\u3002", "result": "\u901a\u8fc7\u5f15\u5165SiO2\u5fae\u900f\u955c\uff0c\u5c06\u5355\u6a21\u5149\u7ea4\u7684\u6a21\u5f0f\u5339\u914d\u635f\u8017\u4ece83.1%\u964d\u4f4e\u52300.1%\u4ee5\u4e0b\uff0c\u4f7f\u5355\u5149\u5b50\u6e90\u7684\u7aef\u5230\u7aef\u6548\u7387\u8fbe\u523096.4%\u3002", "conclusion": "\u901a\u8fc7\u6dfb\u52a0\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u975e\u7403\u9762SiO2\u5fae\u900f\u955c\uff0c\u53ef\u4ee5\u5c06\u6a21\u5f0f\u5339\u914d\u635f\u8017\u4ece83.1%\u964d\u4f4e\u52300.1%\u4ee5\u4e0b\uff0c\u4ece\u800c\u5b9e\u73b096.4%\u7684\u7aef\u5230\u7aef\u6548\u7387\uff0c\u4e3a\u53ef\u6269\u5c55\u7684\u5149\u5b50\u91cf\u5b50\u6280\u672f\u94fa\u5e73\u9053\u8def\u3002"}}
{"id": "2508.06194", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06194", "abs": "https://arxiv.org/abs/2508.06194", "authors": ["Lai Jiang", "Yuekang Li", "Xiaohan Zhang", "Youtao Ding", "Li Pan"], "title": "Beyond Uniform Criteria: Scenario-Adaptive Multi-Dimensional Jailbreak Evaluation", "comment": null, "summary": "Precise jailbreak evaluation is vital for LLM red teaming and jailbreak\nresearch. Current approaches employ binary classification ( e.g., string\nmatching, toxic text classifiers, LLM-driven methods), yielding only \"yes/no\"\nlabels without quantifying harm intensity. Existing multi-dimensional\nframeworks ( e.g., Security Violation, Relative Truthfulness, Informativeness)\napply uniform evaluation criteria across scenarios, resulting in\nscenario-specific mismatches--for instance, \"Relative Truthfulness\" is\nirrelevant to \"hate speech\"--which compromise evaluation precision. To tackle\nthese limitations, we introduce SceneJailEval, with key contributions: (1) A\ngroundbreaking scenario-adaptive multi-dimensional framework for jailbreak\nevaluation, overcoming the critical \"one-size-fits-all\" constraint of existing\nmulti-dimensional methods, and featuring strong extensibility to flexibly adapt\nto customized or emerging scenarios. (2) A comprehensive 14-scenario dataset\nwith diverse jailbreak variants and regional cases, filling the long-standing\ngap in high-quality, holistic benchmarks for scenario-adaptive evaluation. (3)\nSceneJailEval achieves state-of-the-art results, with an F1 score of 0.917 on\nour full-scenario dataset (+6% over prior SOTA) and 0.995 on JBB (+3% over\nprior SOTA), surpassing accuracy limits of existing evaluation methods in\nheterogeneous scenarios and confirming its advantage.", "AI": {"tldr": "SceneJailEval\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u3001\u573a\u666f\u81ea\u9002\u5e94\u7684\u8d8a\u72f1\u8bc4\u4f30\u6846\u67b6\u548c\u6570\u636e\u96c6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u6027\u80fd\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6539\u8fdb\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8d8a\u72f1\u8bc4\u4f30\u65b9\u6cd5\u8981\u4e48\u53ea\u63d0\u4f9b\u4e8c\u5143\u6807\u7b7e\uff0c\u65e0\u6cd5\u91cf\u5316\u5371\u5bb3\u7a0b\u5ea6\uff0c\u8981\u4e48\u4f7f\u7528\u7edf\u4e00\u7684\u8bc4\u4f30\u6807\u51c6\uff0c\u5bfc\u81f4\u5728\u7279\u5b9a\u573a\u666f\u4e0b\u8bc4\u4f30\u4e0d\u51c6\u786e\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u573a\u666f\u81ea\u9002\u5e94\u591a\u7ef4\u5ea6\u6846\u67b6\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b14\u4e2a\u573a\u666f\u7684\u7efc\u5408\u6570\u636e\u96c6\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "result": "SceneJailEval\u5728\u5305\u542b14\u4e2a\u573a\u666f\u7684\u5168\u9762\u6570\u636e\u96c6\u4e2d\u53d6\u5f97\u4e860.917\u7684F1\u5206\u6570\uff0c\u5728JBB\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e860.995\u7684F1\u5206\u6570\uff0c\u8d85\u8fc7\u4e86\u73b0\u6709\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "conclusion": "SceneJailEval\u5728\u5305\u542b14\u4e2a\u573a\u666f\u7684\u5168\u9762\u6570\u636e\u96c6\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u76840.917\u7684F1\u5206\u6570\uff0c\u5728JBB\u4e0a\u5b9e\u73b0\u4e860.995\u7684F1\u5206\u6570\uff0c\u63d0\u9ad8\u4e866%\u548c3%\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5f02\u6784\u573a\u666f\u4e0b\u7684\u4f18\u8d8a\u6027\u3002"}}
{"id": "2508.06014", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06014", "abs": "https://arxiv.org/abs/2508.06014", "authors": ["Minsu Kim", "Subin Jeon", "In Cho", "Mijin Yoo", "Seon Joo Kim"], "title": "ExploreGS: Explorable 3D Scene Reconstruction with Virtual Camera Samplings and Diffusion Priors", "comment": "10 pages, 6 Figures, ICCV 2025", "summary": "Recent advances in novel view synthesis (NVS) have enabled real-time\nrendering with 3D Gaussian Splatting (3DGS). However, existing methods struggle\nwith artifacts and missing regions when rendering from viewpoints that deviate\nfrom the training trajectory, limiting seamless scene exploration. To address\nthis, we propose a 3DGS-based pipeline that generates additional training views\nto enhance reconstruction. We introduce an information-gain-driven virtual\ncamera placement strategy to maximize scene coverage, followed by video\ndiffusion priors to refine rendered results. Fine-tuning 3D Gaussians with\nthese enhanced views significantly improves reconstruction quality. To evaluate\nour method, we present Wild-Explore, a benchmark designed for challenging scene\nexploration. Experiments demonstrate that our approach outperforms existing\n3DGS-based methods, enabling high-quality, artifact-free rendering from\narbitrary viewpoints.\n  https://exploregs.github.io", "AI": {"tldr": "Enhance 3D Gaussian Splatting by generating more training views with smart camera placement and diffusion models to achieve artifact-free rendering from any viewpoint.", "motivation": "Existing 3DGS methods struggle with artifacts and missing regions when rendering from viewpoints deviating from the training trajectory, limiting seamless scene exploration.", "method": "A 3DGS-based pipeline that generates additional training views using an information-gain-driven virtual camera placement strategy and refines rendered results with video diffusion priors. Fine-tuning 3D Gaussians with these enhanced views improves reconstruction quality.", "result": "Experiments on the Wild-Explore benchmark demonstrate that the proposed approach outperforms existing 3DGS-based methods, enabling high-quality, artifact-free rendering from arbitrary viewpoints.", "conclusion": "The proposed 3DGS-based pipeline with enhanced training views and a novel virtual camera placement strategy significantly improves reconstruction quality and enables high-quality, artifact-free rendering from arbitrary viewpoints, outperforming existing methods."}}
{"id": "2508.06208", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06208", "abs": "https://arxiv.org/abs/2508.06208", "authors": ["Ce Na", "Kai Yang", "Dengzhao Fang", "Yu Li", "Jingtong Gao", "Chengcheng Zhu", "Jiale Zhang", "Xiaobing Sun", "Yi Chang"], "title": "Graph Federated Learning for Personalized Privacy Recommendation", "comment": null, "summary": "Federated recommendation systems (FedRecs) have gained significant attention\nfor providing privacy-preserving recommendation services. However, existing\nFedRecs assume that all users have the same requirements for privacy\nprotection, i.e., they do not upload any data to the server. The approaches\noverlook the potential to enhance the recommendation service by utilizing\npublicly available user data. In real-world applications, users can choose to\nbe private or public. Private users' interaction data is not shared, while\npublic users' interaction data can be shared. Inspired by the issue, this paper\nproposes a novel Graph Federated Learning for Personalized Privacy\nRecommendation (GFed-PP) that adapts to different privacy requirements while\nimproving recommendation performance. GFed-PP incorporates the interaction data\nof public users to build a user-item interaction graph, which is then used to\nform a user relationship graph. A lightweight graph convolutional network (GCN)\nis employed to learn each user's user-specific personalized item embedding. To\nprotect user privacy, each client learns the user embedding and the scoring\nfunction locally. Additionally, GFed-PP achieves optimization of the federated\nrecommendation framework through the initialization of item embedding on\nclients and the aggregation of the user relationship graph on the server.\nExperimental results demonstrate that GFed-PP significantly outperforms\nexisting methods for five datasets, offering superior recommendation accuracy\nwithout compromising privacy. This framework provides a practical solution for\naccommodating varying privacy preferences in federated recommendation systems.", "AI": {"tldr": "GFed-PP \u662f\u4e00\u79cd\u65b0\u9896\u7684\u56fe\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u53ef\u6839\u636e\u7528\u6237\u9690\u79c1\u504f\u597d\u8fdb\u884c\u8c03\u6574\uff0c\u5e76\u5229\u7528\u516c\u5f00\u7684\u7528\u6237\u6570\u636e\u6765\u63d0\u9ad8\u63a8\u8350\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u8054\u90a6\u63a8\u8350\u7cfb\u7edf\uff08FedRecs\uff09\u5047\u8bbe\u6240\u6709\u7528\u6237\u90fd\u6709\u76f8\u540c\u7684\u9690\u79c1\u4fdd\u62a4\u8981\u6c42\uff0c\u8fd9\u5ffd\u7565\u4e86\u5229\u7528\u516c\u5f00\u7684\u7528\u6237\u6570\u636e\u6765\u589e\u5f3a\u63a8\u8350\u670d\u52a1\u7684\u6f5c\u529b\u3002\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\uff0c\u7528\u6237\u53ef\u4ee5\u9009\u62e9\u516c\u5f00\u6216\u79c1\u6709\u3002GFed-PP \u65e8\u5728\u9002\u5e94\u4e0d\u540c\u7684\u9690\u79c1\u8981\u6c42\uff0c\u540c\u65f6\u63d0\u9ad8\u63a8\u8350\u6027\u80fd\u3002", "method": "GFed-PP \u5305\u542b\u7528\u6237\u4ea4\u4e92\u6570\u636e\u4ee5\u6784\u5efa\u7528\u6237-\u7269\u54c1\u4ea4\u4e92\u56fe\uff0c\u7136\u540e\u7528\u4e8e\u6784\u5efa\u7528\u6237\u5173\u7cfb\u56fe\u3002\u91c7\u7528\u8f7b\u91cf\u7ea7\u56fe\u5377\u79ef\u7f51\u7edc\uff08GCN\uff09\u6765\u5b66\u4e60\u6bcf\u4e2a\u7528\u6237\u7684\u7528\u6237\u7279\u5b9a\u4e2a\u6027\u5316\u7269\u54c1\u5d4c\u5165\u3002\u4e3a\u4e86\u4fdd\u62a4\u7528\u6237\u9690\u79c1\uff0c\u6bcf\u4e2a\u5ba2\u6237\u7aef\u5728\u672c\u5730\u5b66\u4e60\u7528\u6237\u5d4c\u5165\u548c\u8bc4\u5206\u51fd\u6570\u3002\u6b64\u5916\uff0cGFed-PP \u901a\u8fc7\u5728\u5ba2\u6237\u7aef\u521d\u59cb\u5316\u7269\u54c1\u5d4c\u5165\u4ee5\u53ca\u5728\u670d\u52a1\u5668\u7aef\u805a\u5408\u7528\u6237\u5173\u7cfb\u56fe\u6765\u5b9e\u73b0\u8054\u90a6\u63a8\u8350\u6846\u67b6\u7684\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cGFed-PP \u5728\u4e94\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u4e0d\u635f\u5bb3\u9690\u79c1\u7684\u60c5\u51b5\u4e0b\u63d0\u4f9b\u4e86\u5353\u8d8a\u7684\u63a8\u8350\u51c6\u786e\u6027\u3002", "conclusion": "GFed-PP \u6846\u67b6\u4e3a\u9002\u5e94\u8054\u90a6\u63a8\u8350\u7cfb\u7edf\u4e2d\u4e0d\u540c\u7684\u9690\u79c1\u504f\u597d\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u4e14\u5728\u4e0d\u635f\u5bb3\u9690\u79c1\u7684\u60c5\u51b5\u4e0b\u63d0\u4f9b\u4e86\u5353\u8d8a\u7684\u63a8\u8350\u51c6\u786e\u6027\u3002"}}
{"id": "2508.06236", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.06236", "abs": "https://arxiv.org/abs/2508.06236", "authors": ["Zhu Sun", "Balint Koczor"], "title": "Space and Time Cost of Continuous Rotations in Surface Codes", "comment": "12 pages, 8 figures", "summary": "While Clifford operations are relatively easy to implement in fault-tolerant\nquantum computers,continuous rotation gates remain a significant bottleneck in\ntypical quantum algorithms. In this work, we ask the question: \"What is the\nmost efficient approach for implementing continuous rotations in a surface code\narchitecture?\" Several techniques have been developed to reduce the T-count or\nT-depth of rotations, such as Hamming weight phasing and catalyst towers.\nHowever, these methods often require additional a number of ancilla qubits, and\nthus the ultimate cost function one needs to optimise against should rather be\nthe total runtime or the total space required for performing a rotation. We\nexplicitly construct surface code layouts for catalyst towers in two practical\napplication examples in the context of option pricing: (a) implementing a phase\noracle circuit, which is a ubiquitous subroutine in many quantum algorithms,\nand (b) state preparation using a variational quantum circuit. Our analysis\nshows that, at small and medium code distances, catalyst towers not only reduce\nthe runtime but can also decrease the total spacetime volume of rotations.\nHowever, at large code distances, conventional Clifford+T synthesis may prove\nmore efficient. Additionally, we note that our conclusions are sensitive to\nspecific application scenarios and the choices of various parameters.\nNevertheless, catalyst towers may be particularly advantageous for early\nfault-tolerant quantum applications, where low and medium code distances are\nassumed and a spacetime tradeoff is needed to reduce the runtime of individual\ncircuit runs, such as in scenarios involving high circuit repetition counts.", "AI": {"tldr": "Continuous rotations are hard in quantum computers. Catalyst towers can make them more efficient in surface codes, especially for early applications needing faster runs, but conventional methods might be better for larger, more complex setups.", "motivation": "To determine the most efficient approach for implementing continuous rotations in a surface code architecture, considering total runtime and space rather than just T-count/T-depth, especially for applications like option pricing.", "method": "Explicit construction of surface code layouts for catalyst towers and analysis of their spacetime volume and runtime compared to conventional Clifford+T synthesis, considering factors like code distance and ancilla qubits.", "result": "At small and medium code distances, catalyst towers reduce runtime and spacetime volume for rotations. However, at large code distances, conventional Clifford+T synthesis may be more efficient. The conclusions are sensitive to specific applications and parameter choices.", "conclusion": "Catalyst towers may be particularly advantageous for early fault-tolerant quantum applications, especially when spacetime tradeoffs are needed to reduce runtime, though conventional Clifford+T synthesis may be more efficient at large code distances."}}
{"id": "2508.06196", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.06196", "abs": "https://arxiv.org/abs/2508.06196", "authors": ["Nizi Nazar", "Ehsaneddin Asgari"], "title": "EICAP: Deep Dive in Assessment and Enhancement of Large Language Models in Emotional Intelligence through Multi-Turn Conversations", "comment": null, "summary": "Emotional Intelligence (EI) is a critical yet underexplored dimension in the\ndevelopment of human-aligned LLMs. To address this gap, we introduce a unified,\npsychologically grounded four-layer taxonomy of EI tailored for large language\nmodels (LLMs), encompassing emotional tracking, cause inference, appraisal, and\nemotionally appropriate response generation. Building on this framework, we\npresent EICAP-Bench, a novel MCQ style multi-turn benchmark designed to\nevaluate EI capabilities in open-source LLMs across diverse linguistic and\ncultural contexts. We evaluate six LLMs: LLaMA3 (8B), LLaMA3-Instruct, Gemma\n(9B), Gemma-Instruct, Qwen2.5 (7B), and Qwen2.5-Instruct on EmoCap-Bench,\nidentifying Qwen2.5-Instruct as the strongest baseline. To assess the potential\nfor enhancing EI capabilities, we fine-tune both Qwen2.5-Base and\nQwen2.5-Instruct using LoRA adapters on UltraChat (UC), a large-scale,\ninstruction-tuned dialogue dataset, in both English and Arabic. Our statistical\nanalysis reveals that among the five EI layers, only the Appraisal layer shows\nsignificant improvement through UC-based fine-tuning. These findings highlight\nthe limitations of existing pretraining and instruction-tuning paradigms in\nequipping LLMs with deeper emotional reasoning and underscore the need for\ntargeted data and modeling strategies for comprehensive EI alignment.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684 EI \u5206\u7c7b\u548c\u57fa\u51c6\uff08EICAP-Bench\uff09\uff0c\u4ee5\u8bc4\u4f30\u548c\u589e\u5f3a LLMs \u7684\u60c5\u611f\u667a\u80fd\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u73b0\u6709\u6a21\u578b\u5728\u60c5\u611f\u63a8\u7406\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5e76\u4e14\u5f53\u524d\u7684\u5fae\u8c03\u65b9\u6cd5\u4ec5\u5728\u7279\u5b9a EI \u5c42\uff08\u8bc4\u4f30\u5c42\uff09\u4e0a\u6709\u6548\uff0c\u8868\u660e\u9700\u8981\u65b0\u7684\u7b56\u7565\u6765\u5168\u9762\u63d0\u5347 LLMs \u7684\u60c5\u611f\u667a\u80fd\u3002", "motivation": "\u89e3\u51b3\u5f53\u524d LLMs \u5728\u60c5\u611f\u667a\u80fd\uff08EI\uff09\u7ef4\u5ea6\u4e0a\u63a2\u7d22\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u7684\u3001\u5fc3\u7406\u5b66\u4e3a\u57fa\u7840\u7684\u3001\u9488\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u60c5\u611f\u667a\u80fd\uff08EI\uff09\u56db\u5c42\u5206\u7c7b\uff0c\u5305\u62ec\u60c5\u611f\u8ffd\u8e2a\u3001\u539f\u56e0\u63a8\u65ad\u3001\u8bc4\u4f30\u548c\u60c5\u611f\u9002\u5b9c\u7684\u54cd\u5e94\u751f\u6210\u3002\u5728\u6b64\u6846\u67b6\u57fa\u7840\u4e0a\uff0c\u63d0\u51fa EICAP-Bench\uff0c\u4e00\u4e2a\u65b0\u9896\u7684 MCQ \u98ce\u683c\u7684\u591a\u8f6e\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5f00\u6e90 LLMs \u5728\u4e0d\u540c\u8bed\u8a00\u548c\u6587\u5316\u80cc\u666f\u4e0b\u7684 EI \u80fd\u529b\u3002\u4f7f\u7528 LoRA \u9002\u914d\u5668\u5728 UltraChat (UC) \u6570\u636e\u96c6\u4e0a\u5bf9 Qwen2.5-Base \u548c Qwen2.5-Instruct \u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u5728 EICAP-Bench \u57fa\u51c6\u4e0a\uff0cQwen2.5-Instruct \u662f\u8868\u73b0\u6700\u597d\u7684\u6a21\u578b\u3002\u901a\u8fc7 UC \u5fae\u8c03\uff0c\u4ec5\u8bc4\u4f30\u5c42\u5728 EI \u5404\u5c42\u4e2d\u663e\u793a\u51fa\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "\u73b0\u6709\u7684\u9884\u8bad\u7ec3\u548c\u6307\u4ee4\u8c03\u4f18\u8303\u5f0f\u5728\u4f7f LLMs \u5177\u5907\u66f4\u6df1\u5c42\u6b21\u7684\u60c5\u611f\u63a8\u7406\u80fd\u529b\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u51f8\u663e\u4e86\u9488\u5bf9\u5168\u9762\u60c5\u611f\u667a\u80fd\u5bf9\u9f50\u6240\u9700\u7684\u5b9a\u5411\u6570\u636e\u548c\u5efa\u6a21\u7b56\u7565\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2508.06021", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06021", "abs": "https://arxiv.org/abs/2508.06021", "authors": ["Utku Ozbulak", "Michaela Cohrs", "Hristo L. Svilenov", "Joris Vankerschaver", "Wesley De Neve"], "title": "Improved Sub-Visible Particle Classification in Flow Imaging Microscopy via Generative AI-Based Image Synthesis", "comment": null, "summary": "Sub-visible particle analysis using flow imaging microscopy combined with\ndeep learning has proven effective in identifying particle types, enabling the\ndistinction of harmless components such as silicone oil from protein particles.\nHowever, the scarcity of available data and severe imbalance between particle\ntypes within datasets remain substantial hurdles when applying multi-class\nclassifiers to such problems, often forcing researchers to rely on less\neffective methods. The aforementioned issue is particularly challenging for\nparticle types that appear unintentionally and in lower numbers, such as\nsilicone oil and air bubbles, as opposed to protein particles, where obtaining\nlarge numbers of images through controlled settings is comparatively\nstraightforward. In this work, we develop a state-of-the-art diffusion model to\naddress data imbalance by generating high-fidelity images that can augment\ntraining datasets, enabling the effective training of multi-class deep neural\nnetworks. We validate this approach by demonstrating that the generated samples\nclosely resemble real particle images in terms of visual quality and structure.\nTo assess the effectiveness of using diffusion-generated images in training\ndatasets, we conduct large-scale experiments on a validation dataset comprising\n500,000 protein particle images and demonstrate that this approach improves\nclassification performance with no negligible downside. Finally, to promote\nopen research and reproducibility, we publicly release both our diffusion\nmodels and the trained multi-class deep neural network classifiers, along with\na straightforward interface for easy integration into future studies, at\nhttps://github.com/utkuozbulak/svp-generative-ai.", "AI": {"tldr": "\u5229\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u56fe\u50cf\u89e3\u51b3\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u63d0\u5347\u9897\u7c92\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u9488\u5bf9\u6d41\u6210\u50cf\u663e\u5fae\u955c\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u5728\u9897\u7c92\u8bc6\u522b\u4e2d\u9047\u5230\u7684\u6570\u636e\u7a00\u758f\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u6570\u91cf\u8f83\u5c11\u7684\u9897\u7c92\u7c7b\u578b\uff08\u5982\u7845\u6cb9\u548c\u6c14\u6ce1\uff09\uff0c\u4f20\u7edf\u7684\u5206\u7c7b\u65b9\u6cd5\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u91c7\u7528\u5148\u8fdb\u7684\u6269\u6563\u6a21\u578b\u751f\u6210\u9ad8\u4fdd\u771f\u5ea6\u56fe\u50cf\uff0c\u4ee5\u6269\u5145\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u89e3\u51b3\u4e9a\u53ef\u89c1\u9897\u7c92\u5206\u6790\u4e2d\u6570\u636e\u4e0d\u5e73\u8861\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u9488\u5bf9\u6570\u91cf\u7a00\u5c11\u7684\u9897\u7c92\u7c7b\u578b\uff08\u5982\u7845\u6cb9\u548c\u6c14\u6ce1\uff09\u3002", "result": "\u901a\u8fc7\u5728\u5305\u542b50\u4e07\u4e2a\u86cb\u767d\u8d28\u9897\u7c92\u56fe\u50cf\u7684\u9a8c\u8bc1\u96c6\u4e0a\u8fdb\u884c\u5927\u89c4\u6a21\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u4f7f\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u56fe\u50cf\u6765\u8bad\u7ec3\u6570\u636e\u96c6\u53ef\u4ee5\u63d0\u9ad8\u5206\u7c7b\u6027\u80fd\uff0c\u4e14\u6ca1\u6709\u660e\u663e\u7684\u8d1f\u9762\u5f71\u54cd\u3002\u751f\u6210\u7684\u6837\u672c\u5728\u89c6\u89c9\u8d28\u91cf\u548c\u7ed3\u6784\u4e0a\u4e0e\u771f\u5b9e\u9897\u7c92\u56fe\u50cf\u76f8\u4f3c\u3002", "conclusion": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u65b9\u6cd5\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u4fdd\u771f\u5ea6\u7684\u4e9a\u53ef\u89c1\u9897\u7c92\u56fe\u50cf\uff0c\u4ee5\u89e3\u51b3\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5e76\u6709\u6548\u63d0\u5347\u4e86\u591a\u7c7b\u522b\u5206\u7c7b\u5668\u7684\u6027\u80fd\u3002\u901a\u8fc7\u5927\u89c4\u6a21\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u751f\u6210\u7684\u6837\u672c\u5728\u89c6\u89c9\u8d28\u91cf\u548c\u7ed3\u6784\u4e0a\u4e0e\u771f\u5b9e\u9897\u7c92\u56fe\u50cf\u9ad8\u5ea6\u76f8\u4f3c\uff0c\u4e14\u672a\u5e26\u6765\u660e\u663e\u7f3a\u70b9\u3002\u7814\u7a76\u56e2\u961f\u5df2\u516c\u5f00\u4e86\u6269\u6563\u6a21\u578b\u3001\u8bad\u7ec3\u597d\u7684\u5206\u7c7b\u5668\u4ee5\u53ca\u63a5\u53e3\uff0c\u4ee5\u4fc3\u8fdb\u672a\u6765\u7814\u7a76\u7684\u5f00\u653e\u6027\u548c\u53ef\u590d\u73b0\u6027\u3002"}}
{"id": "2508.06214", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06214", "abs": "https://arxiv.org/abs/2508.06214", "authors": ["Hai Zhong", "Xun Wang", "Zhuoran Li", "Longbo Huang"], "title": "Reparameterization Proximal Policy Optimization", "comment": null, "summary": "Reparameterization policy gradient (RPG) is promising for improving sample\nefficiency by leveraging differentiable dynamics. However, a critical barrier\nis its training instability, where high-variance gradients can destabilize the\nlearning process. To address this, we draw inspiration from Proximal Policy\nOptimization (PPO), which uses a surrogate objective to enable stable sample\nreuse in the model-free setting. We first establish a connection between this\nsurrogate objective and RPG, which has been largely unexplored and is\nnon-trivial. Then, we bridge this gap by demonstrating that the\nreparameterization gradient of a PPO-like surrogate objective can be computed\nefficiently using backpropagation through time. Based on this key insight, we\npropose Reparameterization Proximal Policy Optimization (RPO), a stable and\nsample-efficient RPG-based method. RPO enables multiple epochs of stable sample\nreuse by optimizing a clipped surrogate objective tailored for RPG, while being\nfurther stabilized by Kullback-Leibler (KL) divergence regularization and\nremaining fully compatible with existing variance reduction methods. We\nevaluate RPO on a suite of challenging locomotion and manipulation tasks, where\nexperiments demonstrate that our method achieves superior sample efficiency and\nstrong performance.", "AI": {"tldr": "RPO, inspired by PPO, stabilizes RPG training by using a clipped surrogate objective and KL divergence regularization, leading to better sample efficiency and performance.", "motivation": "To address the training instability and high-variance gradients in Reparameterization Policy Gradient (RPG) methods.", "method": "Reparameterization Proximal Policy Optimization (RPO), which optimizes a clipped surrogate objective tailored for RPG, stabilized by KL divergence regularization and compatible with variance reduction methods.", "result": "RPO enables multiple epochs of stable sample reuse, achieving superior sample efficiency and strong performance.", "conclusion": "RPO achieves superior sample efficiency and strong performance on locomotion and manipulation tasks."}}
{"id": "2508.06238", "categories": ["quant-ph", "physics.optics"], "pdf": "https://arxiv.org/pdf/2508.06238", "abs": "https://arxiv.org/abs/2508.06238", "authors": ["Alexey Gorlach", "Andrea Pizzi", "Klaus M\u00f8lmer", "Joseph Avron", "Mordechai Segev", "Ido Kaminer"], "title": "Supercoherence: Harnessing Long-Range Interactions to Preserve Collective Coherence in Disordered Systems", "comment": null, "summary": "Artificial quantum systems with synthetic dimensions enable exploring novel\nquantum phenomena difficult to create in conventional materials. These\nsynthetic degrees of freedom increase the system's dimensionality without\naltering its physical structure, accessing higher-dimensional physics in\nlower-dimensional setups. However, synthetic quantum systems often suffer from\nintrinsic disorder, causing rapid decoherence that limits scalability, a major\nobstacle in quantum information science. Here, we show that introducing just a\nfew long-range interactions can mitigate decoherence, creating persistent\ncollective coherence in highly symmetric collective excited states. We term\nthis universal phenomenon \"supercoherence\" and show its exceptional robustness\nagainst disorder up to a dynamical phase transition at critical interaction\nstrength and disorder. Supercoherence stabilizes not only coherence but also\nall other quantum properties of the states, challenging traditional views on\nthe inevitability of decoherence in disordered interacting quantum systems and\nsuggesting new opportunities for quantum memory and information processing.", "AI": {"tldr": "\u5408\u6210\u91cf\u5b50\u7cfb\u7edf\u4e2d\u7684\u65e0\u5e8f\u6027\u4f1a\u5bfc\u81f4\u9000\u76f8\u5e72\uff0c\u9650\u5236\u5176\u53ef\u6269\u5c55\u6027\u3002\u672c\u7814\u7a76\u5f15\u5165\u957f\u7a0b\u76f8\u4e92\u4f5c\u7528\uff0c\u53d1\u73b0\u4e86\u201c\u8d85\u76f8\u5e72\u201d\u73b0\u8c61\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u9000\u76f8\u5e72\uff0c\u7a33\u5b9a\u4e86\u91cf\u5b50\u6001\uff0c\u5e76\u4e3a\u91cf\u5b50\u4fe1\u606f\u5904\u7406\u63d0\u4f9b\u4e86\u65b0\u673a\u9047\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5408\u6210\u91cf\u5b50\u7cfb\u7edf\u56fa\u6709\u7684\u65e0\u5e8f\u6027\u5bfc\u81f4\u7684\u5feb\u901f\u9000\u76f8\u5e72\u95ee\u9898\uff0c\u4ee5\u53ca\u5176\u5728\u91cf\u5b50\u4fe1\u606f\u79d1\u5b66\u4e2d\u5bf9\u53ef\u6269\u5c55\u6027\u7684\u9650\u5236\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u5c11\u91cf\u957f\u7a0b\u76f8\u4e92\u4f5c\u7528\u6765\u7f13\u89e3\u9000\u76f8\u5e72\uff0c\u5728\u9ad8\u5ea6\u5bf9\u79f0\u7684\u96c6\u4f53\u6fc0\u53d1\u6001\u4e2d\u4ea7\u751f\u6301\u4e45\u7684\u96c6\u4f53\u76f8\u5e72\u3002", "result": "\u5f15\u5165\u5c11\u91cf\u957f\u7a0b\u76f8\u4e92\u4f5c\u7528\u53ef\u4ee5\u7f13\u89e3\u9000\u76f8\u5e72\uff0c\u4ea7\u751f\u6301\u4e45\u7684\u96c6\u4f53\u76f8\u5e72\uff0c\u5e76\u8bc1\u660e\u4e86\u201c\u8d85\u76f8\u5e72\u201d\u73b0\u8c61\u7684\u9c81\u68d2\u6027\uff0c\u5373\u4f7f\u5728\u9ad8\u8fbe\u4e34\u754c\u76f8\u4e92\u4f5c\u7528\u5f3a\u5ea6\u548c\u65e0\u5e8f\u5ea6\u7684\u52a8\u529b\u5b66\u76f8\u53d8\u4e0b\u4e5f\u80fd\u4fdd\u6301\u3002", "conclusion": "\u8be5\u7814\u7a76\u53d1\u73b0\u4e86\u201c\u8d85\u76f8\u5e72\u201d\u73b0\u8c61\uff0c\u53ef\u4ee5\u7a33\u5b9a\u9000\u706b\u3001\u76f8\u5e72\u6027\u548c\u5176\u4ed6\u91cf\u5b50\u6001\u5c5e\u6027\uff0c\u5e76\u6311\u6218\u4e86\u9000\u706b\u5728\u65e0\u5e8f\u76f8\u4e92\u4f5c\u7528\u91cf\u5b50\u7cfb\u7edf\u4e2d\u4e0d\u53ef\u907f\u514d\u6027\u7684\u4f20\u7edf\u89c2\u70b9\uff0c\u4e3a\u91cf\u5b50\u5185\u5b58\u548c\u4fe1\u606f\u5904\u7406\u5f00\u8f9f\u4e86\u65b0\u673a\u4f1a\u3002"}}
{"id": "2508.06204", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06204", "abs": "https://arxiv.org/abs/2508.06204", "authors": ["Richard Willats", "Josh Pennington", "Aravind Mohan", "Bertie Vidgen"], "title": "Classification is a RAG problem: A case study on hate speech detection", "comment": null, "summary": "Robust content moderation requires classification systems that can quickly\nadapt to evolving policies without costly retraining. We present classification\nusing Retrieval-Augmented Generation (RAG), which shifts traditional\nclassification tasks from determining the correct category in accordance with\npre-trained parameters to evaluating content in relation to contextual\nknowledge retrieved at inference. In hate speech detection, this transforms the\ntask from \"is this hate speech?\" to \"does this violate the hate speech policy?\"\n  Our Contextual Policy Engine (CPE) - an agentic RAG system - demonstrates\nthis approach and offers three key advantages: (1) robust classification\naccuracy comparable to leading commercial systems, (2) inherent explainability\nvia retrieved policy segments, and (3) dynamic policy updates without model\nretraining. Through three experiments, we demonstrate strong baseline\nperformance and show that the system can apply fine-grained policy control by\ncorrectly adjusting protection for specific identity groups without requiring\nretraining or compromising overall performance. These findings establish that\nRAG can transform classification into a more flexible, transparent, and\nadaptable process for content moderation and wider classification problems.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.06032", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06032", "abs": "https://arxiv.org/abs/2508.06032", "authors": ["Kiran Chhatre", "Christopher Peters", "Srikrishna Karanam"], "title": "Learning 3D Texture-Aware Representations for Parsing Diverse Human Clothing and Body Parts", "comment": "16 pages, 11 figures", "summary": "Existing methods for human parsing into body parts and clothing often use\nfixed mask categories with broad labels that obscure fine-grained clothing\ntypes. Recent open-vocabulary segmentation approaches leverage pretrained\ntext-to-image (T2I) diffusion model features for strong zero-shot transfer, but\ntypically group entire humans into a single person category, failing to\ndistinguish diverse clothing or detailed body parts. To address this, we\npropose Spectrum, a unified network for part-level pixel parsing (body parts\nand clothing) and instance-level grouping. While diffusion-based\nopen-vocabulary models generalize well across tasks, their internal\nrepresentations are not specialized for detailed human parsing. We observe\nthat, unlike diffusion models with broad representations, image-driven 3D\ntexture generators maintain faithful correspondence to input images, enabling\nstronger representations for parsing diverse clothing and body parts. Spectrum\nintroduces a novel repurposing of an Image-to-Texture (I2Tx) diffusion model --\nobtained by fine-tuning a T2I model on 3D human texture maps -- for improved\nalignment with body parts and clothing. From an input image, we extract\nhuman-part internal features via the I2Tx diffusion model and generate\nsemantically valid masks aligned to diverse clothing categories through\nprompt-guided grounding. Once trained, Spectrum produces semantic segmentation\nmaps for every visible body part and clothing category, ignoring standalone\ngarments or irrelevant objects, for any number of humans in the scene. We\nconduct extensive cross-dataset experiments -- separately assessing body parts,\nclothing parts, unseen clothing categories, and full-body masks -- and\ndemonstrate that Spectrum consistently outperforms baseline methods in\nprompt-based segmentation.", "AI": {"tldr": "Spectrum is a new network that uses a special diffusion model to better understand and label body parts and clothes in images, even with detailed or unusual clothing, outperforming previous methods.", "motivation": "Existing human parsing methods suffer from fixed, broad mask categories that obscure fine-grained clothing types. While open-vocabulary segmentation approaches show promise, they often fail to distinguish diverse clothing or detailed body parts by treating entire humans as a single category. This work addresses this gap by proposing a unified network for detailed part-level parsing and instance-level grouping.", "method": "Spectrum utilizes a repurposed Image-to-Texture (I2Tx) diffusion model, fine-tuned from a text-to-image model on 3D human texture maps, to extract internal features for human parsing. It generates semantically valid masks aligned with diverse clothing categories through prompt-guided grounding, enabling detailed segmentation of body parts and clothing for any number of humans in a scene.", "result": "Extensive cross-dataset experiments demonstrate that Spectrum consistently outperforms baseline methods in prompt-based segmentation. The network shows strong performance in segmenting body parts, clothing parts, unseen clothing categories, and full-body masks, highlighting its effectiveness and generalization capabilities.", "conclusion": "Spectrum, a novel unified network, effectively addresses the limitations of existing human parsing methods by integrating part-level pixel parsing (body parts and clothing) and instance-level grouping. It leverages a repurposed Image-to-Texture (I2Tx) diffusion model to capture fine-grained details of diverse clothing and body parts, outperforming baseline methods in prompt-based segmentation across various datasets and tasks."}}
{"id": "2508.06113", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06113", "abs": "https://arxiv.org/abs/2508.06113", "authors": ["Jian Wang", "Chaokang Jiang", "Haitao Xu"], "title": "GMF-Drive: Gated Mamba Fusion with Spatial-Aware BEV Representation for End-to-End Autonomous Driving", "comment": "7 pages, 4 figures", "summary": "Diffusion-based models are redefining the state-of-the-art in end-to-end\nautonomous driving, yet their performance is increasingly hampered by a\nreliance on transformer-based fusion. These architectures face fundamental\nlimitations: quadratic computational complexity restricts the use of\nhigh-resolution features, and a lack of spatial priors prevents them from\neffectively modeling the inherent structure of Bird's Eye View (BEV)\nrepresentations. This paper introduces GMF-Drive (Gated Mamba Fusion for\nDriving), an end-to-end framework that overcomes these challenges through two\nprincipled innovations. First, we supersede the information-limited\nhistogram-based LiDAR representation with a geometrically-augmented pillar\nformat encoding shape descriptors and statistical features, preserving critical\n3D geometric details. Second, we propose a novel hierarchical gated mamba\nfusion (GM-Fusion) architecture that substitutes an expensive transformer with\na highly efficient, spatially-aware state-space model (SSM). Our core BEV-SSM\nleverages directional sequencing and adaptive fusion mechanisms to capture\nlong-range dependencies with linear complexity, while explicitly respecting the\nunique spatial properties of the driving scene. Extensive experiments on the\nchallenging NAVSIM benchmark demonstrate that GMF-Drive achieves a new\nstate-of-the-art performance, significantly outperforming DiffusionDrive.\nComprehensive ablation studies validate the efficacy of each component,\ndemonstrating that task-specific SSMs can surpass a general-purpose transformer\nin both performance and efficiency for autonomous driving.", "AI": {"tldr": "GMF-Drive\u4f7f\u7528\u51e0\u4f55\u589e\u5f3a\u7684\u67f1\u72b6LiDAR\u8868\u793a\u548c\u57fa\u4e8eMamba\u7684GM-Fusion\u67b6\u6784\uff0c\u89e3\u51b3\u4e86Transformer\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u7a7a\u95f4\u5148\u9a8c\u95ee\u9898\uff0c\u5e76\u5728NAVSIM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u4f18\u4e8eDiffusionDrive\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eTransformer\u7684\u6269\u6563\u6a21\u578b\u5728\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u4e2d\u867d\u7136\u6027\u80fd\u4f18\u8d8a\uff0c\u4f46\u9762\u4e34\u4e8c\u6b21\u8ba1\u7b97\u590d\u6742\u5ea6\u9650\u5236\u4e86\u9ad8\u5206\u8fa8\u7387\u7279\u5f81\u7684\u4f7f\u7528\uff0c\u5e76\u4e14\u7f3a\u4e4f\u7a7a\u95f4\u5148\u9a8c\u5bfc\u81f4\u65e0\u6cd5\u6709\u6548\u6a21\u62dfBEV\u8868\u793a\u7684\u56fa\u6709\u7ed3\u6784\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGMF-Drive\u7684\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u4e3b\u8981\u521b\u65b0\uff1a1. \u4f7f\u7528\u51e0\u4f55\u589e\u5f3a\u7684\u67f1\u72b6\u683c\u5f0f\u66ff\u4ee3\u57fa\u4e8e\u76f4\u65b9\u56fe\u7684LiDAR\u8868\u793a\uff0c\u4ee5\u4fdd\u7559\u5173\u952e\u76843D\u51e0\u4f55\u7ec6\u8282\u30022. \u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5c42\u7ea7\u95e8\u63a7Mamba\u878d\u5408\uff08GM-Fusion\uff09\u67b6\u6784\uff0c\u7528\u9ad8\u6548\u3001\u7a7a\u95f4\u611f\u77e5\u7684\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08SSM\uff09\u66ff\u4ee3Transformer\uff0c\u8be5\u67b6\u6784\u5229\u7528\u65b9\u5411\u6392\u5e8f\u548c\u81ea\u9002\u5e94\u878d\u5408\u673a\u5236\uff0c\u4ee5\u7ebf\u6027\u590d\u6742\u5ea6\u6355\u83b7\u957f\u8ddd\u79bb\u4f9d\u8d56\u5173\u7cfb\uff0c\u5e76\u5c0a\u91cd\u9a7e\u9a76\u573a\u666f\u7684\u7a7a\u95f4\u7279\u6027\u3002", "result": "GMF-Drive\u5728NAVSIM\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u53d6\u5f97\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8eDiffusionDrive\u3002\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u6bcf\u4e2a\u7ec4\u4ef6\u7684\u6709\u6548\u6027\uff0c\u8868\u660e\u7279\u5b9a\u4efb\u52a1\u7684SSM\u5728\u81ea\u52a8\u9a7e\u9a76\u4efb\u52a1\u4e0a\u53ef\u4ee5\u8d85\u8d8a\u901a\u7528\u7684Transformer\uff0c\u65e0\u8bba\u662f\u5728\u6027\u80fd\u8fd8\u662f\u6548\u7387\u65b9\u9762\u3002", "conclusion": "GMF-Drive\u901a\u8fc7\u4f7f\u7528\u51e0\u4f55\u589e\u5f3a\u7684\u67f1\u72b6\u8868\u793a\u548c\u65b0\u9896\u7684GM-Fusion\u67b6\u6784\uff08\u57fa\u4e8eMamba\uff09\u5728NAVSIM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8eDiffusionDrive\uff0c\u5e76\u8bc1\u660e\u4e86\u7279\u5b9a\u4efb\u52a1\u7684SSM\u5728\u6027\u80fd\u548c\u6548\u7387\u4e0a\u4f18\u4e8e\u901a\u7528Transformer\u3002"}}
{"id": "2304.04475", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2304.04475", "abs": "https://arxiv.org/abs/2304.04475", "authors": ["Gaurav Deshkar", "Jayanta Kshirsagar", "Harshal Hayatnagarkar", "Janani Venugopalan"], "title": "Epidemic Control on a Large-Scale-Agent-Based Epidemiology Model using Deep Deterministic Policy Gradient", "comment": null, "summary": "To mitigate the impact of the pandemic, several measures include lockdowns,\nrapid vaccination programs, school closures, and economic stimulus. These\ninterventions can have positive or unintended negative consequences. Current\nresearch to model and determine an optimal intervention automatically through\nround-tripping is limited by the simulation objectives, scale (a few thousand\nindividuals), model types that are not suited for intervention studies, and the\nnumber of intervention strategies they can explore (discrete vs continuous). We\naddress these challenges using a Deep Deterministic Policy Gradient (DDPG)\nbased policy optimization framework on a large-scale (100,000 individual)\nepidemiological agent-based simulation where we perform multi-objective\noptimization. We determine the optimal policy for lockdown and vaccination in a\nminimalist age-stratified multi-vaccine scenario with a basic simulation for\neconomic activity. With no lockdown and vaccination (mid-age and elderly),\nresults show optimal economy (individuals below the poverty line) with balanced\nhealth objectives (infection, and hospitalization). An in-depth simulation is\nneeded to further validate our results and open-source our framework.", "AI": {"tldr": "This paper presents a DDPG-based framework for optimizing pandemic interventions like lockdowns and vaccinations in large-scale simulations. It found a balance between economic and health outcomes in a test scenario, but calls for more research.", "motivation": "Current research for modeling and optimizing interventions during pandemics is limited by simulation objectives, scale, model suitability, and the range of strategies that can be explored. The study aims to address these challenges by developing a more robust framework for automated optimal intervention determination.", "method": "The study utilizes a Deep Deterministic Policy Gradient (DDPG) based policy optimization framework integrated with a large-scale (100,000 individuals) agent-based epidemiological simulation. This approach allows for multi-objective optimization to determine optimal intervention strategies, specifically focusing on lockdown and vaccination policies within a minimalist, age-stratified, multi-vaccine scenario that includes basic economic activity simulation.", "result": "The simulation, with no lockdown and vaccination applied to mid-age and elderly populations, indicated that the optimal policy resulted in a balanced economy (measured by individuals below the poverty line) alongside balanced health objectives (infection and hospitalization rates).", "conclusion": "The study proposes a Deep Deterministic Policy Gradient (DDPG) based policy optimization framework for large-scale epidemiological agent-based simulations to determine optimal interventions like lockdowns and vaccinations. The framework addresses limitations of current research by handling large scales, diverse model types, and exploring a wider range of intervention strategies through multi-objective optimization. The results suggest an optimal economy with balanced health objectives in a minimalist scenario, but further in-depth simulation is needed for validation."}}
{"id": "2508.06255", "categories": ["quant-ph", "physics.optics"], "pdf": "https://arxiv.org/pdf/2508.06255", "abs": "https://arxiv.org/abs/2508.06255", "authors": ["Georgia Booton", "Tabijah Wasawo", "William O. C. Davis", "Cameron McGarry", "K. R. Rusimova", "Alex O. C. Davis", "Josh Nunn", "Peter J. Mosley"], "title": "Cavity-based optical switching via phase modulation in warm rubidium vapor", "comment": null, "summary": "Optical switching remains a key outstanding challenge for scalable\nfault-tolerant photonic quantum computing due to the trade-off between speed,\nbandwidth, and loss. Scalable quantum photonics demands all three, to enable\nhigh computational clock rates and resource efficient scaling to large systems.\nWe present a cavity-based optical switch that overcomes this limitation,\ndemonstrating 22 ns rise time, insertion loss of 2.4 dB, and 17.5 dB extinction\nratio. All-optical control is achieved via phase modulation of a signal field\ndetuned from the near-degenerate two-photon absorption ladder in warm rubidium\nvapor. The ultimate performance of our switch, combining both speed and\nefficiency, will find applications in active multiplexing, loop-based quantum\nmemory, and feedforward for quantum error-correction protocols.", "AI": {"tldr": "\u4e00\u79cd\u5229\u7528\u94f7\u84b8\u6c14\u5b9e\u73b0\u5168\u5149\u63a7\u5236\u7684\u5149\u5f00\u5173\uff0c\u901f\u5ea6\u5feb\u3001\u635f\u8017\u4f4e\u3001\u6d88\u5149\u6bd4\u9ad8\uff0c\u53ef\u7528\u4e8e\u91cf\u5b50\u8ba1\u7b97\u3002", "motivation": "\u53ef\u6269\u5c55\u3001\u5bb9\u9519\u7684\u5149\u5b50\u91cf\u5b50\u8ba1\u7b97\u4e2d\u7684\u5149\u5b66\u5f00\u5173\u4ecd\u7136\u662f\u4e00\u4e2a\u5173\u952e\u7684\u6311\u6218\uff0c\u56e0\u4e3a\u901f\u5ea6\u3001\u5e26\u5bbd\u548c\u635f\u8017\u4e4b\u95f4\u5b58\u5728\u6743\u8861\u3002", "method": "\u5168\u5149\u63a7\u5236", "result": "\u5b9e\u73b0\u4e86 22 ns \u7684\u4e0a\u5347\u65f6\u95f4\u30012.4 dB \u7684\u63d2\u5165\u635f\u8017\u548c 17.5 dB \u7684\u6d88\u5149\u6bd4\u3002", "conclusion": "\u53ef\u6269\u5c55\u7684\u91cf\u5b50\u5149\u5b50\u5b66\u9700\u8981\u9ad8\u901f\u3001\u9ad8\u5e26\u5bbd\u548c\u4f4e\u635f\u8017\uff0c\u800c\u6211\u4eec\u63d0\u51fa\u7684\u57fa\u4e8e\u8154\u7684\u5149\u5f00\u5173\u514b\u670d\u4e86\u8fd9\u4e00\u9650\u5236\uff0c\u5b9e\u73b0\u4e86 22 ns \u7684\u4e0a\u5347\u65f6\u95f4\u30012.4 dB \u7684\u63d2\u5165\u635f\u8017\u548c 17.5 dB \u7684\u6d88\u5149\u6bd4\u3002\u8be5\u5f00\u5173\u901a\u8fc7\u5931\u8c10\u4e8e\u6e29\u70ed\u94f7\u84b8\u6c14\u4e2d\u8fd1\u7b80\u5e76\u53cc\u5149\u5b50\u5438\u6536\u68af\u7684\u5149\u4fe1\u53f7\u573a\u76f8\u4f4d\u8c03\u5236\u5b9e\u73b0\u5168\u5149\u63a7\u5236\u3002"}}
{"id": "2508.06220", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06220", "abs": "https://arxiv.org/abs/2508.06220", "authors": ["Keummin Ka", "Junhyeong Park", "Jahyun Jeon", "Youngjae Yu"], "title": "InfoCausalQA:Can Models Perform Non-explicit Causal Reasoning Based on Infographic?", "comment": "14 pages, 9 figures", "summary": "Recent advances in Vision-Language Models (VLMs) have demonstrated impressive\ncapabilities in perception and reasoning. However, the ability to perform\ncausal inference -- a core aspect of human cognition -- remains underexplored,\nparticularly in multimodal settings. In this study, we introduce InfoCausalQA,\na novel benchmark designed to evaluate causal reasoning grounded in\ninfographics that combine structured visual data with textual context. The\nbenchmark comprises two tasks: Task 1 focuses on quantitative causal reasoning\nbased on inferred numerical trends, while Task 2 targets semantic causal\nreasoning involving five types of causal relations: cause, effect,\nintervention, counterfactual, and temporal. We manually collected 494\ninfographic-text pairs from four public sources and used GPT-4o to generate\n1,482 high-quality multiple-choice QA pairs. These questions were then\ncarefully revised by humans to ensure they cannot be answered based on\nsurface-level cues alone but instead require genuine visual grounding. Our\nexperimental results reveal that current VLMs exhibit limited capability in\ncomputational reasoning and even more pronounced limitations in semantic causal\nreasoning. Their significantly lower performance compared to humans indicates a\nsubstantial gap in leveraging infographic-based information for causal\ninference. Through InfoCausalQA, we highlight the need for advancing the causal\nreasoning abilities of multimodal AI systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86InfoCausalQA\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u4fe1\u606f\u56fe\u4e0a\u7684\u56e0\u679c\u63a8\u7406\u80fd\u529b\u3002\u7ed3\u679c\u663e\u793a\u5f53\u524d\u6a21\u578b\u5728\u8fd9\u65b9\u9762\u80fd\u529b\u4e0d\u8db3\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u56e0\u679c\u63a8\u7406\u80fd\u529b\u65b9\u9762\uff0c\u5c24\u5176\u662f\u5728\u6df7\u5408\u6a21\u6001\u573a\u666f\u4e0b\uff0c\u4ecd\u6709\u5f85\u63a2\u7d22\u3002\u9700\u8981\u4e00\u4e2a\u4e13\u95e8\u7684\u57fa\u51c6\u6765\u8bc4\u4f30\u6a21\u578b\u5728\u4fe1\u606f\u56fe\u7b49\u590d\u6742\u6570\u636e\u4e0a\u7684\u56e0\u679c\u63a8\u7406\u80fd\u529b\u3002", "method": "\u521b\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aInfoCausalQA\u7684\u65b0\u57fa\u51c6\uff0c\u5305\u542b\u4e24\u4e2a\u5b50\u4efb\u52a1\uff1a\u57fa\u4e8e\u6570\u503c\u8d8b\u52bf\u7684\u6570\u91cf\u56e0\u679c\u63a8\u7406\u548c\u6d89\u53ca\u4e94\u79cd\u56e0\u679c\u5173\u7cfb\uff08\u539f\u56e0\u3001\u7ed3\u679c\u3001\u5e72\u9884\u3001\u53cd\u4e8b\u5b9e\u3001\u65f6\u95f4\uff09\u7684\u8bed\u4e49\u56e0\u679c\u63a8\u7406\u3002\u8be5\u57fa\u51c6\u5305\u542b494\u4e2a\u4fe1\u606f\u56fe-\u6587\u672c\u5bf9\uff0c\u5e76\u4f7f\u7528GPT-4o\u751f\u6210\u4e861,482\u4e2a\u591a\u9879\u9009\u62e9\u95ee\u7b54\u5bf9\uff0c\u5176\u4e2d\u624b\u52a8\u5ba1\u67e5\u786e\u4fdd\u95ee\u9898\u9700\u8981\u6df1\u5c42\u7406\u89e3\u800c\u975e\u8868\u9762\u7ebf\u7d22\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u8ba1\u7b97\u56e0\u679c\u63a8\u7406\u65b9\u9762\u80fd\u529b\u6709\u9650\uff0c\u5728\u8bed\u4e49\u56e0\u679c\u63a8\u7406\u65b9\u9762\u8868\u73b0\u66f4\u5dee\uff0c\u4e0e\u4eba\u7c7b\u76f8\u6bd4\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0c\u663e\u793a\u51fa\u5728\u5229\u7528\u4fe1\u606f\u56fe\u8fdb\u884c\u56e0\u679c\u63a8\u65ad\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "conclusion": "\u76ee\u524d\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u56e0\u679c\u63a8\u7406\u65b9\u9762\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u4fe1\u606f\u56fe\u7b49\u878d\u5408\u4e86\u7ed3\u6784\u5316\u89c6\u89c9\u6570\u636e\u548c\u6587\u672c\u4fe1\u606f\u7684\u6df7\u5408\u6a21\u6001\u573a\u666f\u65f6\uff0c\u80fd\u529b\u6709\u9650\u3002InfoCausalQA\u57fa\u51c6\u7684\u5efa\u7acb\u7a81\u663e\u4e86\u63d0\u5347\u591a\u6a21\u6001\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u56e0\u679c\u63a8\u7406\u80fd\u529b\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2508.06033", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06033", "abs": "https://arxiv.org/abs/2508.06033", "authors": ["Yiming Gong", "Zhen Zhu", "Minjia Zhang"], "title": "InstantEdit: Text-Guided Few-Step Image Editing with Piecewise Rectified Flow", "comment": "ICCV 2025", "summary": "We propose a fast text-guided image editing method called InstantEdit based\non the RectifiedFlow framework, which is structured as a few-step editing\nprocess that preserves critical content while following closely to textual\ninstructions. Our approach leverages the straight sampling trajectories of\nRectifiedFlow by introducing a specialized inversion strategy called PerRFI. To\nmaintain consistent while editable results for RectifiedFlow model, we further\npropose a novel regeneration method, Inversion Latent Injection, which\neffectively reuses latent information obtained during inversion to facilitate\nmore coherent and detailed regeneration. Additionally, we propose a\nDisentangled Prompt Guidance technique to balance editability with detail\npreservation, and integrate a Canny-conditioned ControlNet to incorporate\nstructural cues and suppress artifacts. Evaluation on the PIE image editing\ndataset demonstrates that InstantEdit is not only fast but also achieves better\nqualitative and quantitative results compared to state-of-the-art few-step\nediting methods.", "AI": {"tldr": "InstantEdit is a fast and effective text-guided image editing method based on RectifiedFlow, improving upon existing techniques by preserving content and adhering to text prompts.", "motivation": "To develop a fast text-guided image editing method that preserves critical content while closely following textual instructions, balancing editability with detail preservation and suppressing artifacts.", "method": "InstantEdit utilizes the RectifiedFlow framework with a specialized inversion strategy (PerRFI), a novel regeneration method (Inversion Latent Injection), a Disentangled Prompt Guidance technique, and a Canny-conditioned ControlNet.", "result": "InstantEdit demonstrates fast performance and achieves superior qualitative and quantitative results compared to existing few-step editing methods.", "conclusion": "InstantEdit is a fast text-guided image editing method that achieves better qualitative and quantitative results compared to state-of-the-art few-step editing methods on the PIE image editing dataset."}}
{"id": "2508.06177", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06177", "abs": "https://arxiv.org/abs/2508.06177", "authors": ["Dominik Br\u00e4mer", "Diana Kleingarn", "Oliver Urbann"], "title": "Graph-based Robot Localization Using a Graph Neural Network with a Floor Camera and a Feature Rich Industrial Floor", "comment": "Accepted at 28th RoboCup International Symposium, Salvador, Brasil", "summary": "Accurate localization represents a fundamental challenge in\n  robotic navigation. Traditional methodologies, such as Lidar or QR-code based\nsystems, suffer from inherent scalability and adaptability con straints,\nparticularly in complex environments. In this work, we propose\n  an innovative localization framework that harnesses flooring characteris tics\nby employing graph-based representations and Graph Convolutional\n  Networks (GCNs). Our method uses graphs to represent floor features,\n  which helps localize the robot more accurately (0.64cm error) and more\n  efficiently than comparing individual image features. Additionally, this\n  approach successfully addresses the kidnapped robot problem in every\n  frame without requiring complex filtering processes. These advancements\n  open up new possibilities for robotic navigation in diverse environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5730\u677f\u7279\u5f81\u7684\u521b\u65b0\u672c\u5730\u5316\u6846\u67b6\uff0c\u4f7f\u7528\u56fe\u5377\u79ef\u7f51\u7edc\uff08GCN\uff09\u548c\u56fe\u8868\u793a\uff0c\u5b9e\u73b0\u4e86\u6bd4\u4f20\u7edf\u65b9\u6cd5\u66f4\u51c6\u786e\u3001\u66f4\u9ad8\u6548\u7684\u673a\u5668\u4eba\u672c\u5730\u5316\uff0c\u5e76\u89e3\u51b3\u4e86\u590d\u6742\u7684\u673a\u5668\u4eba\u672c\u5730\u5316\u95ee\u9898\u3002", "motivation": "\u51c6\u786e\u7684\u5b9a\u4f4d\u662f\u673a\u5668\u4eba\u5bfc\u822a\u7684\u4e00\u4e2a\u57fa\u672c\u6311\u6218\u3002\u4f20\u7edf\u65b9\u6cd5\uff0c\u5982\u57fa\u4e8e Lidar \u6216 QR \u7801\u7684\u7cfb\u7edf\uff0c\u5728\u53ef\u6269\u5c55\u6027\u548c\u9002\u5e94\u6027\u65b9\u9762\u5b58\u5728\u56fa\u6709\u7684\u7ea6\u675f\uff0c\u5c24\u5176\u662f\u5728\u590d\u6742\u73af\u5883\u4e2d\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u56fe\u7684\u8868\u793a\u548c\u56fe\u5377\u79ef\u7f51\u7edc\uff08GCN\uff09\u5229\u7528\u5730\u677f\u7279\u5f81\u3002", "result": "\u8be5\u65b9\u6cd5\u4f7f\u7528\u56fe\u8868\u793a\u5730\u677f\u7279\u5f81\uff0c\u4e0e\u6bd4\u8f83\u5355\u4e2a\u56fe\u50cf\u7279\u5f81\u76f8\u6bd4\uff0c\u53ef\u4ee5\u66f4\u51c6\u786e\uff080.64 \u5398\u7c73\u8bef\u5dee\uff09\u66f4\u6709\u6548\u5730\u8fdb\u884c\u673a\u5668\u4eba\u672c\u5730\u5316\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u5728\u6bcf\u4e2a\u5e27\u4e2d\u6210\u529f\u89e3\u51b3\u4e86\u88ab\u7ed1\u67b6\u7684\u673a\u5668\u4eba\u95ee\u9898\uff0c\u800c\u65e0\u9700\u590d\u6742\u7684\u8fc7\u6ee4\u8fc7\u7a0b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u673a\u5668\u4eba\u5728\u5404\u79cd\u73af\u5883\u4e2d\u5bfc\u822a\u5f00\u8f9f\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2507.12286", "categories": ["cs.LO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12286", "abs": "https://arxiv.org/abs/2507.12286", "authors": ["Anouk Oudshoorn", "Magdalena Ortiz", "Mantas Simkus"], "title": "SHACL Validation in the Presence of Ontologies: Semantics and Rewriting Techniques", "comment": "36 pages, 6 figures, submitted to the journal of Artificial\n  Intelligence (AIJ)", "summary": "SHACL and OWL are two prominent W3C standards for managing RDF data. These\nlanguages share many features, but they have one fundamental difference: OWL,\ndesigned for inferring facts from incomplete data, makes the open-world\nassumption, whereas SHACL is a constraint language that treats the data as\ncomplete and must be validated under the closed-world assumption. The\ncombination of both formalisms is very appealing and has been called for, but\ntheir semantic gap is a major challenge, semantically and computationally. In\nthis paper, we advocate a semantics for SHACL validation in the presence of\nontologies based on core universal models. We provide a technique for\nconstructing these models for ontologies in the rich data-tractable description\nlogic Horn-ALCHIQ. Furthermore, we use a finite representation of this model to\ndevelop a rewriting technique that reduces SHACL validation in the presence of\nontologies to standard validation. Finally, we study the complexity of SHACL\nvalidation in the presence of ontologies, and show that even very simple\nontologies make the problem EXPTIME-complete, and PTIME-complete in data\ncomplexity.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u672c\u4f53\u5b58\u5728\u7684\u60c5\u51b5\u4e0b\u8fdb\u884cSHACL\u9a8c\u8bc1\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u95ee\u9898\u8f6c\u5316\u4e3a\u6807\u51c6\u9a8c\u8bc1\u6765\u89e3\u51b3\u8bed\u4e49\u548c\u8ba1\u7b97\u6311\u6218\uff0c\u5e76\u5206\u6790\u4e86\u5176\u590d\u6742\u6027\u3002", "motivation": "SHACL\u548cOWL\u662f\u4e24\u79cd\u7528\u4e8e\u7ba1\u7406RDF\u6570\u636e\u7684W3C\u6807\u51c6\uff0c\u5b83\u4eec\u6709\u8bb8\u591a\u5171\u540c\u70b9\uff0c\u4f46\u5b58\u5728\u5f00\u653e\u4e16\u754c\u5047\u8bbe\uff08OWL\uff09\u548c\u5c01\u95ed\u4e16\u754c\u5047\u8bbe\uff08SHACL\uff09\u8fd9\u4e00\u6839\u672c\u533a\u522b\u3002\u5c06\u4e24\u8005\u7ed3\u5408\u8d77\u6765\u5177\u6709\u5438\u5f15\u529b\uff0c\u4f46\u5b58\u5728\u8bed\u4e49\u548c\u8ba1\u7b97\u4e0a\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u6784\u5efa\u6838\u5fc3\u901a\u7528\u6a21\u578b\u7684\u6280\u672f\uff0c\u5e76\u5229\u7528\u5176\u6709\u9650\u8868\u793a\u6765\u5f00\u53d1\u4e00\u79cd\u5c06SHACL\u9a8c\u8bc1\uff08\u5728\u672c\u4f53\u5b58\u5728\u7684\u60c5\u51b5\u4e0b\uff09\u7b80\u5316\u4e3a\u6807\u51c6\u9a8c\u8bc1\u7684\u91cd\u5199\u6280\u672f\u3002", "result": "\u7814\u7a76\u4e86SHACL\u9a8c\u8bc1\uff08\u5728\u672c\u4f53\u5b58\u5728\u7684\u60c5\u51b5\u4e0b\uff09\u7684\u590d\u6742\u6027\uff0c\u53d1\u73b0\u5373\u4f7f\u662f\u975e\u5e38\u7b80\u5355\u7684\u672c\u4f53\u4e5f\u4f1a\u4f7f\u95ee\u9898\u6210\u4e3aEXPTIME-complete\uff0c\u800c\u5728\u6570\u636e\u590d\u6742\u5ea6\u65b9\u9762\u4e3aPTIME-complete\u3002", "conclusion": "SHACL\u548cOWL\u7684\u7ed3\u5408\u5177\u6709\u5438\u5f15\u529b\uff0c\u4f46\u5b58\u5728\u8bed\u4e49\u9e3f\u6c9f\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6838\u5fc3\u901a\u7528\u6a21\u578b\u7684SHACL\u9a8c\u8bc1\u8bed\u4e49\uff0c\u5e76\u4e3aHorn-ALCHIQ\u4e2d\u7684\u672c\u4f53\u6784\u5efa\u4e86\u8fd9\u4e9b\u6a21\u578b\u3002"}}
{"id": "2508.06244", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.06244", "abs": "https://arxiv.org/abs/2508.06244", "authors": ["Xurun Wang", "Guangrui Liu", "Xinjie Li", "Haoyu He", "Lin Yao", "Weizhe Zhang"], "title": "Membership Inference Attack with Partial Features", "comment": null, "summary": "Machine learning models have been shown to be susceptible to membership\ninference attack, which can be used to determine whether a given sample appears\nin the training data. Existing membership inference methods commonly assume\nthat the adversary has full access to the features of the target sample. This\nassumption, however, does not hold in many real-world scenarios where only\npartial features information is available, thereby limiting the applicability\nof these methods. In this work, we study an inference scenario where the\nadversary observes only partial features of each sample and aims to infer\nwhether this observed subset was present in the training set of the target\nmodel. We define this problem as Partial Feature Membership Inference (PFMI).\nTo address this problem, we propose MRAD (Memory-guided Reconstruction and\nAnomaly Detection), a two-stage attack framework. In the first stage, MRAD\noptimizes the unknown feature values to minimize the loss of the sample. In the\nsecond stage, it measures the deviation between the reconstructed sample and\nthe training distribution using anomaly detection. Empirical results\ndemonstrate that MRAD is effective across a range of datasets, and maintains\ncompatibility with various off-the-shelf anomaly detection techniques. For\nexample, on STL-10, our attack achieves an AUC of around 0.6 even with 40% of\nthe missing features.", "AI": {"tldr": "MRAD\u662f\u4e00\u79cd\u9488\u5bf9\u90e8\u5206\u7279\u5f81\u4fe1\u606f\u53ef\u7528\u7684\u6210\u5458\u63a8\u65ad\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u5efa\u548c\u5f02\u5e38\u68c0\u6d4b\u6765\u8bc6\u522b\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u6837\u672c\u3002", "motivation": "\u73b0\u6709\u7684\u6210\u5458\u63a8\u65ad\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u5bf9\u624b\u53ef\u4ee5\u5b8c\u5168\u8bbf\u95ee\u76ee\u6807\u6837\u672c\u7684\u7279\u5f81\uff0c\u4f46\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\uff0c\u901a\u5e38\u53ea\u80fd\u83b7\u5f97\u90e8\u5206\u7279\u5f81\u4fe1\u606f\uff0c\u8fd9\u9650\u5236\u4e86\u8fd9\u4e9b\u65b9\u6cd5\u7684\u53ef\u884c\u6027\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u90e8\u5206\u7279\u5f81\u4fe1\u606f\u53ef\u7528\u65f6\u7684\u6210\u5458\u63a8\u65ad\u95ee\u9898\u3002", "method": "MRAD\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u653b\u51fb\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u901a\u8fc7\u4f18\u5316\u672a\u77e5\u7279\u5f81\u503c\u6765\u6700\u5c0f\u5316\u6837\u672c\u635f\u5931\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5229\u7528\u5f02\u5e38\u68c0\u6d4b\u6d4b\u91cf\u91cd\u5efa\u6837\u672c\u4e0e\u8bad\u7ec3\u5206\u5e03\u7684\u504f\u5dee\u3002", "result": "MRAD\u5728\u591a\u79cd\u6570\u636e\u96c6\u4e0a\u5747\u8868\u73b0\u51fa\u6709\u6548\u6027\uff0c\u5373\u4f7f\u5728\u7f3a\u593140%\u7684\u7279\u5f81\u65f6\uff0c\u5728STL-10\u4e0a\u7684\u653b\u51fbAUC\u4e5f\u80fd\u8fbe\u5230\u7ea60.6\u3002\u6b64\u5916\uff0cMRAD\u4e0e\u591a\u79cd\u73b0\u6210\u7684\u5f02\u5e38\u68c0\u6d4b\u6280\u672f\u517c\u5bb9\u3002", "conclusion": "MRAD\u5728STL-10\u7b49\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u6709\u6548\u6027\u9a8c\u8bc1\uff0c\u5373\u4f7f\u5728\u7f3a\u593140%\u7279\u5f81\u7684\u60c5\u51b5\u4e0b\uff0c\u4ecd\u80fd\u8fbe\u5230\u7ea60.6\u7684AUC\uff0c\u5e76\u4e14\u53ef\u4ee5\u517c\u5bb9\u591a\u79cd\u5f02\u5e38\u68c0\u6d4b\u6280\u672f\u3002"}}
{"id": "2508.06294", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.06294", "abs": "https://arxiv.org/abs/2508.06294", "authors": ["Pol Juli\u00e0 Farr\u00e9", "Chris Aaron Schneider", "Christian Deppe"], "title": "Secure Hybrid Key Growing via Coherence Witnessing and Bipartite Encoding", "comment": "6 pages, 2 figures", "summary": "We propose a novel Hybrid Key Growing (HKG) protocol based on quantum\nprinciples and a classical physical-layer assumption. We simultaneously exploit\nthe quantum photon-number and photon-time-bin Degrees of Freedom (DoFs),\neffectively doubling the bit-per-pulse rate compared to conventional Quantum\nKey Growing (QKG) schemes. Our protocol integrates entity authentication, and\nis designed for practical implementation by avoiding reliance on single-photon\nsources or detectors. By incorporating prior knowledge about the quantum\nchannel, the scheme actively mitigates noise effects, making it suitable for\nreal-world conditions. Under certain assumptions on experimental efficiencies,\nour approach also promises an increased key generation rate in bits per second.\nOur simulation results display, first, expected outcomes to gain assurance\nabout the correctness of our implementation and, second, relevant dependencies\nthat showcase desirable properties of our scheme in regimes of low photon loss\nand dephasing. In particular, within such regimes, our encoding scheme reduces\nthe Quantum Bit Error Rate (QBER) while preserving the ability to detect\neavesdropping and identity-forgery attempts.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u91cf\u5b50\u5bc6\u94a5\u5206\u53d1\u534f\u8bae\uff08HKG\uff09\uff0c\u5229\u7528\u5149\u5b50\u6570\u548c\u65f6\u95f4 \u092e\u094d\u0939\u0923\u0942\u0928\u091a\u81ea\u7531\u5ea6\uff0c\u63d0\u9ad8\u4e86\u5bc6\u94a5\u751f\u6210\u901f\u7387\u548c\u5b89\u5168\u6027\uff0c\u5e76\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u91cf\u5b50\u5bc6\u94a5\u5206\u53d1\u7684\u6548\u7387\u548c\u5b9e\u7528\u6027\uff0c\u901a\u8fc7\u540c\u65f6\u5229\u7528\u591a\u4e2a\u81ea\u7531\u5ea6\u5e76\u51cf\u5c11\u5bf9\u7279\u5b9a\u5b9e\u9a8c\u8bbe\u5907\u7684\u8981\u6c42\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u91cf\u5b50\u539f\u7406\u548c\u7ecf\u5178\u7269\u7406\u5c42\u5047\u8bbe\u7684\u6df7\u5408\u5bc6\u94a5\u589e\u957f\uff08HKG\uff09\u534f\u8bae\uff0c\u8be5\u534f\u8bae\u540c\u65f6\u5229\u7528\u4e86\u91cf\u5b50\u5149\u5b50\u6570\u548c\u5149\u5b50\u65f6\u95f4 \u092e\u094d\u0939\u0923\u0942\u0928\u091a\u81ea\u7531\u5ea6\uff08DoFs\uff09\uff0c\u5e76\u5c06\u5b9e\u4f53\u8ba4\u8bc1\u96c6\u6210\u5230\u534f\u8bae\u4e2d\uff0c\u907f\u514d\u4e86\u5bf9\u5355\u5149\u5b50\u6e90\u6216\u63a2\u6d4b\u5668\u7684\u4f9d\u8d56\uff0c\u5e76\u80fd\u4e3b\u52a8\u7f13\u89e3\u566a\u58f0\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0cHKG\u534f\u8bae\u80fd\u51cf\u5c11\u91cf\u5b50\u6bd4\u7279\u8bef\u7387\uff08QBER\uff09\uff0c\u5728\u4f4e\u5149\u5b50\u635f\u8017\u548c\u9000\u76f8\u5e72\u7684\u60c5\u51b5\u4e0b\uff0c\u80fd\u591f\u6709\u6548\u68c0\u6d4b\u7a83\u542c\u548c\u8eab\u4efd\u4f2a\u9020\uff0c\u5e76\u6709\u671b\u63d0\u9ad8\u5bc6\u94a5\u751f\u6210\u901f\u7387\u3002", "conclusion": "\u8be5\u534f\u8bae\u901a\u8fc7\u5229\u7528\u91cf\u5b50\u4fe1\u9053\u4fe1\u606f\u548c\u51cf\u5c11\u91cf\u5b50\u6bd4\u7279\u8bef\u7387\u6765\u63d0\u9ad8\u5bc6\u94a5\u751f\u6210\u901f\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u7a83\u542c\u548c\u8eab\u4efd\u4f2a\u9020\u68c0\u6d4b\u80fd\u529b\u3002"}}
{"id": "2508.06277", "categories": ["cs.CL", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.06277", "abs": "https://arxiv.org/abs/2508.06277", "authors": ["Theresa Pekarek Rosin", "Burak Can Kaplan", "Stefan Wermter"], "title": "Large Language Model Data Generation for Enhanced Intent Recognition in German Speech", "comment": "11 pages, 3 figures, accepted at KONVENS 2025", "summary": "Intent recognition (IR) for speech commands is essential for artificial\nintelligence (AI) assistant systems; however, most existing approaches are\nlimited to short commands and are predominantly developed for English. This\npaper addresses these limitations by focusing on IR from speech by elderly\nGerman speakers. We propose a novel approach that combines an adapted Whisper\nASR model, fine-tuned on elderly German speech (SVC-de), with Transformer-based\nlanguage models trained on synthetic text datasets generated by three\nwell-known large language models (LLMs): LeoLM, Llama3, and ChatGPT. To\nevaluate the robustness of our approach, we generate synthetic speech with a\ntext-to-speech model and conduct extensive cross-dataset testing. Our results\nshow that synthetic LLM-generated data significantly boosts classification\nperformance and robustness to different speaking styles and unseen vocabulary.\nNotably, we find that LeoLM, a smaller, domain-specific 13B LLM, surpasses the\nmuch larger ChatGPT (175B) in dataset quality for German intent recognition.\nOur approach demonstrates that generative AI can effectively bridge data gaps\nin low-resource domains. We provide detailed documentation of our data\ngeneration and training process to ensure transparency and reproducibility.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408\u9002\u914d\u7684Whisper ASR\u6a21\u578b\u548cLLM\u751f\u6210\u7684\u5408\u6210\u6570\u636e\uff0c\u7528\u4e8e\u8bc6\u522b\u8001\u5e74\u5fb7\u8bed\u4f7f\u7528\u8005\u7684\u8bed\u97f3\u610f\u56fe\uff0c\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u610f\u56fe\u8bc6\u522b\uff08IR\uff09\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u77ed\u547d\u4ee4\u4e14\u591a\u4e3a\u82f1\u8bed\uff0c\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\uff0c\u4e13\u6ce8\u4e8e\u8bc6\u522b\u8001\u5e74\u5fb7\u8bed\u4f7f\u7528\u8005\u8bed\u97f3\u4e2d\u7684\u610f\u56fe\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u9002\u914d\u7684Whisper ASR\u6a21\u578b\uff08SVC-de\uff09\u548c\u57fa\u4e8eTransformer\u7684\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u540e\u8005\u5728\u7531LeoLM\u3001Llama3\u548cChatGPT\u751f\u6210\u7684\u4e09\u79cd\u5408\u6210\u6587\u672c\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bad\u7ec3\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u5408\u6210\u7684LLM\u751f\u6210\u6570\u636e\u663e\u8457\u63d0\u9ad8\u4e86\u5206\u7c7b\u6027\u80fd\uff0c\u589e\u5f3a\u4e86\u5bf9\u4e0d\u540c\u8bf4\u8bdd\u98ce\u683c\u548c\u672a\u89c1\u8bcd\u6c47\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u53ef\u4ee5\u6709\u6548\u5730\u5f25\u5408\u4f4e\u8d44\u6e90\u9886\u57df\u7684\u6578\u64da\u5dee\u8ddd\uff0c\u5e76\u4e14LeoLM\u5728\u5fb7\u8bed\u610f\u56fe\u8bc6\u522b\u65b9\u9762\u7684\u6570\u636e\u96c6\u8d28\u91cf\u4f18\u4e8eChatGPT\u3002"}}
{"id": "2508.06036", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06036", "abs": "https://arxiv.org/abs/2508.06036", "authors": ["Jun Xie", "Yingjian Zhu", "Feng Chen", "Zhenghao Zhang", "Xiaohui Fan", "Hongzhu Yi", "Xinming Wang", "Chen Yu", "Yue Bi", "Zhaoran Zhao", "Xiongjun Guan", "Zhepeng Wang"], "title": "More Is Better: A MoE-Based Emotion Recognition Framework with Human Preference Alignment", "comment": null, "summary": "In this paper, we present our solution for the semi-supervised learning track\n(MER-SEMI) in MER2025. We propose a comprehensive framework, grounded in the\nprinciple that \"more is better,\" to construct a robust Mixture of Experts (MoE)\nemotion recognition system. Our approach integrates a diverse range of input\nmodalities as independent experts, including novel signals such as knowledge\nfrom large Vision-Language Models (VLMs) and temporal Action Unit (AU)\ninformation. To effectively utilize unlabeled data, we introduce a\nconsensus-based pseudo-labeling strategy, generating high-quality labels from\nthe agreement between a baseline model and Gemini, which are then used in a\ntwo-stage training paradigm. Finally, we employ a multi-expert voting ensemble\ncombined with a rule-based re-ranking process to correct prediction bias and\nbetter align the outputs with human preferences. Evaluated on the MER2025-SEMI\nchallenge dataset, our method achieves an F1-score of 0.8772 on the test set,\nranking 2nd in the track. Our code is available at\nhttps://github.com/zhuyjan/MER2025-MRAC25.", "AI": {"tldr": "\u901a\u8fc7\u878d\u5408\u591a\u79cd\u6a21\u6001\uff08\u5305\u62ecVLMs\u548cAU\u4fe1\u606f\uff09\u5e76\u5229\u7528\u4f2a\u6807\u7b7e\u7b56\u7565\uff0c\u672c\u7814\u7a76\u6784\u5efa\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u591a\u4e13\u5bb6\u6df7\u5408\u60c5\u611f\u8bc6\u522b\u7cfb\u7edf\uff0c\u5728MER2025\u7ade\u8d5b\u4e2d\u53d6\u5f97\u4f18\u5f02\u6210\u7ee9\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u4e3aMER2025\u60c5\u611f\u8bc6\u522b\u6311\u6218\u8d5b\u7684\u534a\u76d1\u7763\u5b66\u4e60\u63d0\u4f9b\u4e00\u4e2a\u5f3a\u5927\u4e14\u5168\u9762\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u662f\u5229\u7528\u591a\u79cd\u8f93\u5165\u6a21\u6001\u548c\u672a\u6807\u8bb0\u6570\u636e\u6765\u63d0\u5347\u60c5\u611f\u8bc6\u522b\u7684\u51c6\u786e\u6027\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u4e13\u5bb6\u6df7\u5408\uff08MoE\uff09\u6846\u67b6\uff0c\u96c6\u6210\u4e86\u5305\u62ec\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u77e5\u8bc6\u548c\u65f6\u95f4\u52a8\u4f5c\u5355\u5143\uff08AU\uff09\u4fe1\u606f\u5728\u5185\u7684\u591a\u79cd\u8f93\u5165\u6a21\u6001\u3002\u7814\u7a76\u91c7\u7528\u4e86\u57fa\u4e8e\u5171\u8bc6\u7684\u4f2a\u6807\u7b7e\u7b56\u7565\u6765\u5229\u7528\u672a\u6807\u8bb0\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u591a\u4e13\u5bb6\u6295\u7968\u96c6\u6210\u548c\u57fa\u4e8e\u89c4\u5219\u7684\u91cd\u65b0\u6392\u5e8f\u6765\u4f18\u5316\u9884\u6d4b\u3002", "result": "\u5728MER2025-SEMI\u6311\u6218\u8d5b\u7684\u6d4b\u8bd5\u96c6\u4e0a\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5b9e\u73b0\u4e860.8772\u7684F1\u5206\u6570\uff0c\u4f4d\u5217\u7b2c\u4e8c\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u591a\u4e13\u5bb6\u6df7\u5408\uff08MoE\uff09\u60c5\u611f\u8bc6\u522b\u7cfb\u7edf\u5728MER2025-SEMI\u6311\u6218\u8d5b\u7684\u6d4b\u8bd5\u96c6\u4e0a\u8fbe\u5230\u4e860.8772\u7684F1\u5206\u6570\uff0c\u53d6\u5f97\u4e86\u7b2c\u4e8c\u540d\u7684\u6210\u7ee9\u3002"}}
{"id": "2508.06227", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06227", "abs": "https://arxiv.org/abs/2508.06227", "authors": ["Md Sazidur Rahman", "David Cabecinhas", "Ricard Marxer"], "title": "Depth Jitter: Seeing through the Depth", "comment": null, "summary": "Depth information is essential in computer vision, particularly in underwater\nimaging, robotics, and autonomous navigation. However, conventional\naugmentation techniques overlook depth aware transformations, limiting model\nrobustness in real world depth variations. In this paper, we introduce\nDepth-Jitter, a novel depth-based augmentation technique that simulates natural\ndepth variations to improve generalization. Our approach applies adaptive depth\noffsetting, guided by depth variance thresholds, to generate synthetic depth\nperturbations while preserving structural integrity. We evaluate Depth-Jitter\non two benchmark datasets, FathomNet and UTDAC2020 demonstrating its impact on\nmodel stability under diverse depth conditions. Extensive experiments compare\nDepth-Jitter against traditional augmentation strategies such as ColorJitter,\nanalyzing performance across varying learning rates, encoders, and loss\nfunctions. While Depth-Jitter does not always outperform conventional methods\nin absolute performance, it consistently enhances model stability and\ngeneralization in depth-sensitive environments. These findings highlight the\npotential of depth-aware augmentation for real-world applications and provide a\nfoundation for further research into depth-based learning strategies. The\nproposed technique is publicly available to support advancements in depth-aware\naugmentation. The code is publicly available on\n\\href{https://github.com/mim-team/Depth-Jitter}{github}.", "AI": {"tldr": "Depth-Jitter \u662f\u4e00\u79cd\u65b0\u7684\u6df1\u5ea6\u611f\u77e5\u589e\u5f3a\u6280\u672f\uff0c\u901a\u8fc7\u6a21\u62df\u6df1\u5ea6\u53d8\u5316\u6765\u63d0\u9ad8\u6a21\u578b\u5728\u5404\u79cd\u6df1\u5ea6\u6761\u4ef6\u4e0b\u7684\u7a33\u5b9a\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u7684\u589e\u5f3a\u6280\u672f\u5ffd\u7565\u4e86\u6df1\u5ea6\u611f\u77e5\u53d8\u6362\uff0c\u9650\u5236\u4e86\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u6df1\u5ea6\u53d8\u5316\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "method": "Depth-Jitter \u901a\u8fc7\u5e94\u7528\u81ea\u9002\u5e94\u6df1\u5ea6\u504f\u79fb\uff0c\u5e76\u4ee5\u6df1\u5ea6\u65b9\u5dee\u9608\u503c\u4f5c\u4e3a\u6307\u5bfc\uff0c\u5728\u4fdd\u6301\u7ed3\u6784\u5b8c\u6574\u6027\u7684\u540c\u65f6\u751f\u6210\u5408\u6210\u6df1\u5ea6\u6270\u52a8\u3002", "result": "\u5728 FathomNet \u548c UTDAC2020 \u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0cDepth-Jitter \u5728\u5404\u79cd\u6df1\u5ea6\u6761\u4ef6\u4e0b\u80fd\u589e\u5f3a\u6a21\u578b\u7684\u7a33\u5b9a\u6027\u3002\u4e0e ColorJitter \u7b49\u4f20\u7edf\u589e\u5f3a\u7b56\u7565\u76f8\u6bd4\uff0cDepth-Jitter \u5728\u6a21\u578b\u7a33\u5b9a\u6027\u65b9\u9762\u8868\u73b0\u66f4\u4f18\uff0c\u4f46\u7edd\u5bf9\u6027\u80fd\u4e0d\u4e00\u5b9a\u603b\u662f\u9886\u5148\u3002", "conclusion": "Depth-Jitter \u662f\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u6df1\u5ea6\u611f\u77e5\u7684\u589e\u5f3a\u6280\u672f\uff0c\u901a\u8fc7\u6a21\u62df\u81ea\u7136\u6df1\u5ea6\u53d8\u5316\u6765\u63d0\u9ad8\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u7a33\u5b9a\u6027\uff0c\u5c24\u5176\u662f\u5728\u6df1\u5ea6\u654f\u611f\u7684\u73af\u5883\u4e2d\u3002\u867d\u7136\u5b83\u4e0d\u4e00\u5b9a\u603b\u662f\u80fd\u5728\u7edd\u5bf9\u6027\u80fd\u4e0a\u8d85\u8d8a\u4f20\u7edf\u65b9\u6cd5\uff0c\u4f46\u5b83\u80fd\u7a33\u5b9a\u5730\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.06304", "categories": ["quant-ph", "cond-mat.quant-gas"], "pdf": "https://arxiv.org/pdf/2508.06304", "abs": "https://arxiv.org/abs/2508.06304", "authors": ["Emma C. King", "Giovanna Morigi", "Rapha\u00ebl Menu"], "title": "Shortcuts to adiabaticity with a quantum control field", "comment": "5 pages, 4 figures. Comments welcome!", "summary": "Quantum adiabatic dynamics is the crucial element of adiabatic quantum\ncomputing and quantum annealing. Shortcuts to adiabaticity enable acceleration\nof the computational time by suppressing unwanted non-adiabatic processes with\ndesigned classical fields. Here, we consider quantum state transfer in the\nLandau-Zener model, which exemplifies the key elements of quantum adiabatic\ndynamics. We argue that non-adiabatic transitions can be suppressed by\nautonomous quantum dynamics, which involves coupling the Landau-Zener qubit to\na second quantum system. By tuning the coupling strength, the composite quantum\ndynamics can reduce the probability of unwanted processes by more than two\norders of magnitude. This is a prime example of control where the quantum\nproperties of the control fields are key for implementing shortcuts to\nadiabaticity.", "AI": {"tldr": "\u91cf\u5b50\u7edd\u70ed\u8ba1\u7b97\u548c\u91cf\u5b50\u9000\u706b\u7684\u52a0\u901f\u53ef\u4ee5\u901a\u8fc7\u8bbe\u8ba1\u7ecf\u5178\u573a\u6765\u6291\u5236\u4e0d\u60f3\u8981\u7684\u975e\u7edd\u70ed\u8fc7\u7a0b\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u91cf\u5b50\u6bd4\u7279\u4e0e\u7b2c\u4e8c\u4e2a\u91cf\u5b50\u7cfb\u7edf\u8026\u5408\uff0c\u5229\u7528\u81ea\u4e3b\u91cf\u5b50\u52a8\u529b\u5b66\u6765\u6291\u5236\u5170\u9053-\u6cfd\u7eb3\u6a21\u578b\u4e2d\u7684\u975e\u7edd\u70ed\u8dc3\u8fc1\uff0c\u5e76\u5c06\u4e0d\u9700\u8981\u7684\u8fc7\u7a0b\u7684\u6982\u7387\u964d\u4f4e\u4e86\u4e24\u4e2a\u6570\u91cf\u7ea7\u4ee5\u4e0a\u3002", "motivation": "\u91cf\u5b50\u7edd\u70ed\u52a8\u529b\u5b66\u662f\u91cf\u5b50\u7edd\u70ed\u8ba1\u7b97\u548c\u91cf\u5b50\u9000\u706b\u7684\u5173\u952e\u56e0\u7d20\u3002\u7edd\u70ed\u6027\u6377\u5f84\u53ef\u4ee5\u901a\u8fc7\u8bbe\u8ba1\u7ecf\u5178\u573a\u6765\u6291\u5236\u4e0d\u60f3\u8981\u7684\u975e\u7edd\u70ed\u8fc7\u7a0b\uff0c\u4ece\u800c\u52a0\u901f\u8ba1\u7b97\u65f6\u95f4\u3002", "method": "\u672c\u7814\u7a76\u8003\u8651\u4e86\u5170\u9053-\u6cfd\u7eb3\u6a21\u578b\u4e2d\u7684\u91cf\u5b50\u6001\u8f6c\u79fb\uff0c\u8be5\u6a21\u578b\u4f8b\u8bc1\u4e86\u91cf\u5b50\u7edd\u70ed\u52a8\u529b\u5b66\u7684\u5173\u952e\u8981\u7d20\u3002\u901a\u8fc7\u5c06\u5170\u9053-\u6cfd\u7eb3\u91cf\u5b50\u6bd4\u7279\u4e0e\u7b2c\u4e8c\u4e2a\u91cf\u5b50\u7cfb\u7edf\u8026\u5408\uff0c\u5229\u7528\u81ea\u4e3b\u91cf\u5b50\u52a8\u529b\u5b66\u6291\u5236\u4e86\u975e\u7edd\u70ed\u8dc3\u8fc1\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5c06\u4e0d\u9700\u8981\u7684\u8fc7\u7a0b\u7684\u6982\u7387\u964d\u4f4e\u4e24\u4e2a\u6570\u91cf\u7ea7\u4ee5\u4e0a\u3002", "conclusion": "\u901a\u8fc7\u8c03\u6574\u8026\u5408\u5f3a\u5ea6\uff0c\u590d\u5408\u91cf\u5b50\u52a8\u529b\u5b66\u53ef\u4ee5\u5c06\u4e0d\u9700\u8981\u7684\u8fc7\u7a0b\u7684\u6982\u7387\u964d\u4f4e\u4e24\u4e2a\u6570\u91cf\u7ea7\u4ee5\u4e0a\u3002\u8fd9\u662f\u91cf\u5b50\u6027\u8d28\u7684\u63a7\u5236\u573a\u5b9e\u73b0\u7edd\u70ed\u6027\u6377\u5f84\u7684\u4e00\u4e2a\u4e3b\u8981\u4f8b\u5b50\u3002"}}
{"id": "2508.06309", "categories": ["cs.CL", "math.PR"], "pdf": "https://arxiv.org/pdf/2508.06309", "abs": "https://arxiv.org/abs/2508.06309", "authors": ["Ruichong Zhang"], "title": "Matrix-Driven Instant Review: Confident Detection and Reconstruction of LLM Plagiarism on PC", "comment": null, "summary": "In recent years, concerns about intellectual property (IP) in large language\nmodels (LLMs) have grown significantly. Plagiarizing other LLMs (through direct\nweight copying, upcycling, pruning, or continual pretraining) and claiming\nauthorship without properly attributing to the original license, is a serious\nmisconduct that can lead to significant financial and reputational harm to the\noriginal developers. However, existing methods for detecting LLM plagiarism\nfall short in key areas. They fail to accurately reconstruct weight\ncorrespondences, lack the ability to compute statistical significance measures\nsuch as $p$-values, and may mistakenly flag models trained on similar data as\nbeing related. To address these limitations, we propose Matrix-Driven Instant\nReview (MDIR), a novel method that leverages matrix analysis and Large\nDeviation Theory. MDIR achieves accurate reconstruction of weight\nrelationships, provides rigorous $p$-value estimation, and focuses exclusively\non weight similarity without requiring full model inference. Experimental\nresults demonstrate that MDIR reliably detects plagiarism even after extensive\ntransformations, such as random permutations and continual pretraining with\ntrillions of tokens. Moreover, all detections can be performed on a single PC\nwithin an hour, making MDIR both efficient and accessible.", "AI": {"tldr": "MDIR\u901a\u8fc7\u77e9\u9635\u5206\u6790\u548c\u6781\u5927\u504f\u5dee\u7406\u8bba\u89e3\u51b3\u4e86\u73b0\u6709LLM\u6284\u88ad\u68c0\u6d4b\u65b9\u6cd5\u7684\u4e0d\u8db3\uff0c\u80fd\u591f\u51c6\u786e\u9ad8\u6548\u5730\u68c0\u6d4b\u6284\u88ad\uff0c\u5373\u4f7f\u5728\u6a21\u578b\u7ecf\u8fc7\u5927\u91cf\u8f6c\u6362\u540e\u4e5f\u80fd\u594f\u6548\u3002", "motivation": "\u73b0\u6709LLM\u6284\u88ad\u68c0\u6d4b\u65b9\u6cd5\u5728\u91cd\u5efa\u6743\u91cd\u5bf9\u5e94\u5173\u7cfb\u3001\u8ba1\u7b97p\u503c\u7b49\u7edf\u8ba1\u663e\u8457\u6027\u5ea6\u91cf\u4ee5\u53ca\u533a\u5206\u76f8\u4f3c\u6a21\u578b\u548c\u6284\u88ad\u6a21\u578b\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002\u867d\u7136\u5b58\u5728\u5bf9LLM\u7684\u77e5\u8bc6\u4ea7\u6743\u62c5\u5fe7\uff0c\u4f46\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u7684\u73b0\u6709\u65b9\u6cd5\u6548\u679c\u4e0d\u4f73\u3002", "method": "MDIR\u5229\u7528\u77e9\u9635\u5206\u6790\u548c\u6781\u5927\u504f\u5dee\u7406\u8bba\u6765\u51c6\u786e\u91cd\u5efa\u6743\u91cd\u5173\u7cfb\uff0c\u63d0\u4f9b\u4e25\u683c\u7684p\u503c\u4f30\u8ba1\uff0c\u5e76\u4e14\u4ec5\u5173\u6ce8\u6743\u91cd\u76f8\u4f3c\u6027\uff0c\u65e0\u9700\u8fdb\u884c\u5b8c\u6574\u7684\u6a21\u578b\u63a8\u7406\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMDIR\u5373\u4f7f\u5728\u7ecf\u8fc7\u968f\u673a\u7f6e\u6362\u548c\u6301\u7eed\u9884\u8bad\u7ec3\uff08\u6d89\u53ca\u6570\u4e07\u4ebf\u4e2a\u6807\u8bb0\uff09\u7b49\u5e7f\u6cdb\u8f6c\u6362\u540e\uff0c\u4ecd\u80fd\u53ef\u9760\u5730\u68c0\u6d4b\u5230\u6284\u88ad\u3002\u6b64\u5916\uff0c\u6240\u6709\u68c0\u6d4b\u90fd\u53ef\u4ee5\u5728\u4e00\u5c0f\u65f6\u5185\u5728\u5355\u53f0PC\u4e0a\u5b8c\u6210\u3002", "conclusion": "MDIR\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u8bbf\u95ee\u7684\u65b0\u9896\u65b9\u6cd5\uff0c\u53ef\u4ee5\u51c6\u786e\u68c0\u6d4bLLM\u6284\u88ad\uff0c\u5373\u4f7f\u5728\u7ecf\u8fc7\u5927\u91cf\u8f6c\u6362\u540e\u4e5f\u80fd\u53ef\u9760\u5730\u68c0\u6d4b\u5230\u6284\u88ad\u3002"}}
{"id": "2508.06038", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06038", "abs": "https://arxiv.org/abs/2508.06038", "authors": ["Huanyu Wang", "Jushi Kai", "Haoli Bai", "Lu Hou", "Bo Jiang", "Ziwei He", "Zhouhan Lin"], "title": "Fourier-VLM: Compressing Vision Tokens in the Frequency Domain for Large Vision-Language Models", "comment": "12 pages, 4 figures", "summary": "Vision-Language Models (VLMs) typically replace the predefined image\nplaceholder token (<image>) in textual instructions with visual features from\nan image encoder, forming the input to a backbone Large Language Model (LLM).\nHowever, the large number of vision tokens significantly increases the context\nlength, leading to high computational overhead and inference latency. While\nprevious efforts mitigate this by selecting only important visual features or\nleveraging learnable queries to reduce token count, they often compromise\nperformance or introduce substantial extra costs. In response, we propose\nFourier-VLM, a simple yet efficient method that compresses visual\nrepresentations in the frequency domain. Our approach is motivated by the\nobservation that vision features output from the vision encoder exhibit\nconcentrated energy in low-frequency components. Leveraging this, we apply a\nlow-pass filter to the vision features using a two-dimentional Discrete Cosine\nTransform (DCT). Notably, the DCT is efficiently computed via the Fast Fourier\nTransform (FFT) operator with a time complexity of $\\mathcal{O}(n\\log n)$,\nminimizing the extra computational cost while introducing no additional\nparameters. Extensive experiments across various image-based benchmarks\ndemonstrate that Fourier-VLM achieves competitive performance with strong\ngeneralizability across both LLaVA and Qwen-VL architectures. Crucially, it\nreduce inference FLOPs by up to 83.8% and boots generation speed by 31.2%\ncompared to LLaVA-v1.5, highlighting the superior efficiency and practicality.", "AI": {"tldr": "Fourier-VLM\u662f\u4e00\u79cd\u521b\u65b0\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u65b9\u6cd5\uff0c\u901a\u8fc7\u9891\u57df\u538b\u7f29\u6280\u672f\u663e\u8457\u63d0\u9ad8\u4e86\u6548\u7387\uff0c\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u548c\u5ef6\u8fdf\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5f3a\u5927\u7684\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u7531\u4e8e\u5927\u91cf\u89c6\u89c9\u6807\u8bb0\u5bfc\u81f4\u7684\u957f\u4e0a\u4e0b\u6587\u957f\u5ea6\u3001\u9ad8\u8ba1\u7b97\u5f00\u9500\u548c\u63a8\u7406\u5ef6\u8fdf\u95ee\u9898\uff0c\u540c\u65f6\u907f\u514d\u4ee5\u5f80\u65b9\u6cd5\u5728\u6027\u80fd\u6216\u6210\u672c\u4e0a\u7684\u59a5\u534f\u3002", "method": "Fourier-VLM\u5229\u7528\u79bb\u6563\u4f59\u5f26\u53d8\u6362\uff08DCT\uff09\u4f5c\u4e3a\u4f4e\u901a\u6ee4\u6ce2\u5668\uff0c\u5728\u9891\u57df\u4e2d\u538b\u7f29\u89c6\u89c9\u7279\u5f81\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u4e86\u89c6\u89c9\u7279\u5f81\u5728\u4f4e\u9891\u5206\u91cf\u4e2d\u80fd\u91cf\u96c6\u4e2d\u7684\u7279\u6027\uff0c\u5e76\u901a\u8fc7\u5feb\u901f\u5085\u91cc\u53f6\u53d8\u6362\uff08FFT\uff09\u9ad8\u6548\u8ba1\u7b97DCT\uff0c\u4ece\u800c\u5728\u4e0d\u5f15\u5165\u989d\u5916\u53c2\u6570\u7684\u60c5\u51b5\u4e0b\uff0c\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "result": "Fourier-VLM\u5728LLaVA\u548cQwen-VL\u7b49\u591a\u79cd\u67b6\u6784\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u5728\u591a\u4e2a\u56fe\u50cf\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u548c\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002\u4e0eLLaVA-v1.5\u76f8\u6bd4\uff0c\u5176\u63a8\u7406FLOPs\u964d\u4f4e\u4e86\u9ad8\u8fbe83.8%\uff0c\u751f\u6210\u901f\u5ea6\u63d0\u5347\u4e8631.2%\uff0c\u8bc1\u660e\u4e86\u5176\u4f18\u8d8a\u7684\u6548\u7387\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "Fourier-VLM\u901a\u8fc7\u5728\u9891\u57df\u4e2d\u538b\u7f29\u89c6\u89c9\u8868\u793a\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u540c\u65f6\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u5f00\u9500\u548c\u63a8\u7406\u5ef6\u8fdf\u3002"}}
{"id": "2508.06249", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06249", "abs": "https://arxiv.org/abs/2508.06249", "authors": ["David Kacz\u00e9r", "Magnus J\u00f8rgenv\u00e5g", "Clemens Vetter", "Lucie Flek", "Florian Mai"], "title": "In-Training Defenses against Emergent Misalignment in Language Models", "comment": "Under review", "summary": "Fine-tuning lets practitioners repurpose aligned large language models (LLMs)\nfor new domains, yet recent work reveals emergent misalignment (EMA): Even a\nsmall, domain-specific fine-tune can induce harmful behaviors far outside the\ntarget domain. Even in the case where model weights are hidden behind a\nfine-tuning API, this gives attackers inadvertent access to a broadly\nmisaligned model in a way that can be hard to detect from the fine-tuning data\nalone. We present the first systematic study of in-training safeguards against\nEMA that are practical for providers who expose fine-tuning via an API. We\ninvestigate four training regularization interventions: (i) KL-divergence\nregularization toward a safe reference model, (ii) $\\ell_2$ distance in feature\nspace, (iii) projecting onto a safe subspace (SafeLoRA), and (iv) interleaving\nof a small amount of safe training examples from a general instruct-tuning\ndataset. We first evaluate the methods' emergent misalignment effect across\nfour malicious, EMA-inducing tasks. Second, we assess the methods' impacts on\nbenign tasks. We conclude with a discussion of open questions in emergent\nmisalignment research.", "AI": {"tldr": "\u5fae\u8c03 LLM \u53ef\u80fd\u5bfc\u81f4\u5176\u5728\u76ee\u6807\u9886\u57df\u5916\u51fa\u73b0\u6709\u5bb3\u884c\u4e3a\uff08EMA\uff09\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u56db\u79cd\u8bad\u7ec3\u65b9\u6cd5\uff08KL \u6563\u5ea6\u3001L2 \u8ddd\u79bb\u3001SafeLoRA\u3001\u6570\u636e\u4ea4\u9519\uff09\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u5e76\u8bc4\u4f30\u4e86\u5b83\u4eec\u5728\u6076\u610f\u548c\u826f\u6027\u4efb\u52a1\u4e0a\u7684\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u53d1\u73b0\uff0c\u5373\u4f7f\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8fdb\u884c\u5c11\u91cf\u9886\u57df\u7279\u5b9a\u7684\u5fae\u8c03\uff0c\u4e5f\u53ef\u80fd\u5bfc\u81f4\u6a21\u578b\u5728\u76ee\u6807\u9886\u57df\u4e4b\u5916\u51fa\u73b0\u6709\u5bb3\u884c\u4e3a\uff08\u79f0\u4e3a\u201c\u6d8c\u73b0\u6027\u4e0d\u5339\u914d\u201d\uff0cEMA\uff09\u3002\u5373\u4f7f\u6a21\u578b\u6743\u91cd\u53d7\u5230 API \u4fdd\u62a4\uff0c\u653b\u51fb\u8005\u4e5f\u53ef\u80fd\u901a\u8fc7\u5fae\u8c03 API \u83b7\u5f97\u4e00\u4e2a\u5e7f\u6cdb\u4e0d\u5339\u914d\u7684\u6a21\u578b\uff0c\u4e14\u8fd9\u79cd\u60c5\u51b5\u96be\u4ee5\u4ec5\u4ece\u5fae\u8c03\u6570\u636e\u4e2d\u68c0\u6d4b\u51fa\u6765\u3002\u56e0\u6b64\uff0c\u9700\u8981\u7814\u7a76\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5c31\u80fd\u9632\u6b62 EMA \u7684\u5b89\u5168\u9632\u62a4\u63aa\u65bd\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u901a\u8fc7 API \u63d0\u4f9b\u5fae\u8c03\u670d\u52a1\u7684\u63d0\u4f9b\u5546\u800c\u8a00\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u56db\u79cd\u8bad\u7ec3\u6b63\u5219\u5316\u5e72\u9884\u65b9\u6cd5\u6765\u89e3\u51b3 LLM \u5fae\u8c03\u4e2d\u7684\u6d8c\u73b0\u6027\u4e0d\u5339\u914d\uff08EMA\uff09\u95ee\u9898\uff1a(i) KL-\u6563\u5ea6\u6b63\u5219\u5316\uff08\u4f7f\u6a21\u578b\u63a5\u8fd1\u5b89\u5168\u7684\u53c2\u8003\u6a21\u578b\uff09\uff0c(ii) \u7279\u5f81\u7a7a\u95f4 $\\ell_2$ \u8ddd\u79bb\uff08\u9650\u5236\u6a21\u578b\u5728\u7279\u5f81\u7a7a\u95f4\u4e2d\u7684\u53d8\u5316\uff09\uff0c(iii) \u5b89\u5168\u5b50\u7a7a\u95f4\u6295\u5f71\uff08SafeLoRA\uff09\uff08\u5c06\u6a21\u578b\u6743\u91cd\u6295\u5f71\u5230\u9884\u5b9a\u4e49\u7684\u201c\u5b89\u5168\u201d\u5b50\u7a7a\u95f4\uff09\uff0c\u4ee5\u53ca (iv) \u4ea4\u9519\u5c11\u91cf\u5b89\u5168\u901a\u7528\u6307\u4ee4\u5fae\u8c03\u6570\u636e\u3002\u7814\u7a76\u4eba\u5458\u5728\u56db\u4e2a\u6076\u610f\u4efb\u52a1\u548c\u826f\u6027\u4efb\u52a1\u4e0a\u8bc4\u4f30\u4e86\u8fd9\u4e9b\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "result": "\u7814\u7a76\u8bc4\u4f30\u4e86\u56db\u79cd\u65b9\u6cd5\u5728\u56db\u4e2a\u6076\u610f EMA \u8bf1\u5bfc\u4efb\u52a1\u4e0a\u7684\u6709\u6548\u6027\uff0c\u4ee5\u53ca\u5b83\u4eec\u5bf9\u826f\u6027\u4efb\u52a1\u6027\u80fd\u7684\u5f71\u54cd\u3002\u5177\u4f53\u8bc4\u4f30\u7ed3\u679c\u548c\u6bd4\u8f83\u672a\u5728\u6458\u8981\u4e2d\u8be6\u7ec6\u8bf4\u660e\uff0c\u4f46\u7814\u7a76\u65e8\u5728\u627e\u51fa\u80fd\u591f\u6709\u6548\u7f13\u89e3 EMA \u95ee\u9898\u540c\u65f6\u4e0d\u635f\u5bb3\u6a21\u578b\u826f\u6027\u4efb\u52a1\u8868\u73b0\u7684\u8bad\u7ec3\u5e72\u9884\u63aa\u65bd\u3002", "conclusion": "\u672c\u7814\u7a76\u9996\u6b21\u7cfb\u7edf\u5730\u7814\u7a76\u4e86\u9488\u5bf9 LLM \u9886\u57df\u7279\u5b9a\u5fae\u8c03\u4e2d\u51fa\u73b0\u7684\u201c\u6d8c\u73b0\u6027\u4e0d\u5339\u914d\u201d\uff08EMA\uff09\u95ee\u9898\u7684\u8bad\u7ec3\u4e2d\u5b89\u5168\u9632\u62a4\u63aa\u65bd\uff0c\u5e76\u63d0\u51fa\u4e86\u56db\u79cd\u6b63\u5219\u5316\u5e72\u9884\u65b9\u6cd5\uff1a(i) KL \u6563\u5ea6\u6b63\u5219\u5316\u3001(ii) \u7279\u5f81\u7a7a\u95f4 $\\ell_2$ \u8ddd\u79bb\u3001(iii) \u5b89\u5168\u5b50\u7a7a\u95f4\u6295\u5f71\uff08SafeLoRA\uff09\u3001(iv) \u5c11\u91cf\u901a\u7528\u6307\u4ee4\u5fae\u8c03\u6570\u636e\u4ea4\u9519\u3002\u7814\u7a76\u8bc4\u4f30\u4e86\u8fd9\u4e9b\u65b9\u6cd5\u5728\u56db\u4e2a\u6076\u610f EMA \u8bf1\u5bfc\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u4ee5\u53ca\u5bf9\u826f\u6027\u4efb\u52a1\u7684\u5f71\u54cd\uff0c\u5e76\u8ba8\u8bba\u4e86 EMA \u7814\u7a76\u7684\u5f00\u653e\u6027\u95ee\u9898\u3002"}}
{"id": "2508.06044", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06044", "abs": "https://arxiv.org/abs/2508.06044", "authors": ["Huimin Wu", "Xiaojian Ma", "Haozhe Zhao", "Yanpeng Zhao", "Qing Li"], "title": "NEP: Autoregressive Image Editing via Next Editing Token Prediction", "comment": "The project page is: https://nep-bigai.github.io/", "summary": "Text-guided image editing involves modifying a source image based on a\nlanguage instruction and, typically, requires changes to only small local\nregions. However, existing approaches generate the entire target image rather\nthan selectively regenerate only the intended editing areas. This results in\n(1) unnecessary computational costs and (2) a bias toward reconstructing\nnon-editing regions, which compromises the quality of the intended edits. To\nresolve these limitations, we propose to formulate image editing as Next\nEditing-token Prediction (NEP) based on autoregressive image generation, where\nonly regions that need to be edited are regenerated, thus avoiding unintended\nmodification to the non-editing areas. To enable any-region editing, we propose\nto pre-train an any-order autoregressive text-to-image (T2I) model. Once\ntrained, it is capable of zero-shot image editing and can be easily adapted to\nNEP for image editing, which achieves a new state-of-the-art on widely used\nimage editing benchmarks. Moreover, our model naturally supports test-time\nscaling (TTS) through iteratively refining its generation in a zero-shot\nmanner. The project page is: https://nep-bigai.github.io/", "AI": {"tldr": "\u63d0\u51fa NEP \u65b9\u6cd5\uff0c\u901a\u8fc7\u4ec5\u7f16\u8f91\u6240\u9700\u533a\u57df\u6765\u63d0\u9ad8\u6587\u672c\u5f15\u5bfc\u56fe\u50cf\u7f16\u8f91\u7684\u6548\u7387\u548c\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5f15\u5bfc\u7684\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\u901a\u5e38\u4f1a\u751f\u6210\u6574\u4e2a\u76ee\u6807\u56fe\u50cf\uff0c\u5bfc\u81f4\u4e0d\u5fc5\u8981\u7684\u8ba1\u7b97\u6210\u672c\uff0c\u5e76\u5bf9\u975e\u7f16\u8f91\u533a\u57df\u4ea7\u751f\u504f\u5dee\uff0c\u4ece\u800c\u5f71\u54cd\u7f16\u8f91\u8d28\u91cf\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\u7684 next editing-token prediction (NEP) \u7684\u65b9\u6cd5\uff0c\u5e76\u9884\u8bad\u7ec3\u4e86\u4e00\u4e2a\u80fd\u591f\u8fdb\u884c\u4efb\u610f\u533a\u57df\u7f16\u8f91\u7684\u4efb\u610f\u987a\u5e8f\u81ea\u56de\u5f52\u6587\u672c\u5230\u56fe\u50cf (T2I) \u6a21\u578b\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u5e38\u7528\u7684\u56fe\u50cf\u7f16\u8f91\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e86\u65b0\u7684 state-of-the-art \u6c34\u5e73\uff0c\u5e76\u4e14\u80fd\u591f\u81ea\u7136\u5730\u652f\u6301\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u8fdb\u884c test-time scaling (TTS)\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684 next editing-token prediction (NEP) \u65b9\u6cd5\u901a\u8fc7\u4ec5\u91cd\u65b0\u751f\u6210\u9700\u8981\u7f16\u8f91\u7684\u533a\u57df\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u548c\u5bf9\u975e\u7f16\u8f91\u533a\u57df\u4ea7\u751f\u504f\u5dee\u7684\u95ee\u9898\uff0c\u5728\u56fe\u50cf\u7f16\u8f91\u4efb\u52a1\u4e0a\u8fbe\u5230\u4e86\u65b0\u7684 state-of-the-art \u6c34\u5e73\uff0c\u5e76\u4e14\u652f\u6301 test-time scaling (TTS)\u3002"}}
{"id": "2508.06251", "categories": ["cs.LG", "cs.AI", "cs.CR", "quant-ph"], "pdf": "https://arxiv.org/pdf/2508.06251", "abs": "https://arxiv.org/abs/2508.06251", "authors": ["Alejandro Moreno R.", "Desale Fentaw", "Samuel Palmer", "Ra\u00fal Salles de Padua", "Ninad Dixit", "Samuel Mugel", "Roman Or\u00fas", "Manuel Radons", "Josef Menter", "Ali Abedi"], "title": "Synthetic Data Generation and Differential Privacy using Tensor Networks' Matrix Product States (MPS)", "comment": "10 pages", "summary": "Synthetic data generation is a key technique in modern artificial\nintelligence, addressing data scarcity, privacy constraints, and the need for\ndiverse datasets in training robust models. In this work, we propose a method\nfor generating privacy-preserving high-quality synthetic tabular data using\nTensor Networks, specifically Matrix Product States (MPS). We benchmark the\nMPS-based generative model against state-of-the-art models such as CTGAN, VAE,\nand PrivBayes, focusing on both fidelity and privacy-preserving capabilities.\nTo ensure differential privacy (DP), we integrate noise injection and gradient\nclipping during training, enabling privacy guarantees via R\\'enyi Differential\nPrivacy accounting. Across multiple metrics analyzing data fidelity and\ndownstream machine learning task performance, our results show that MPS\noutperforms classical models, particularly under strict privacy constraints.\nThis work highlights MPS as a promising tool for privacy-aware synthetic data\ngeneration. By combining the expressive power of tensor network representations\nwith formal privacy mechanisms, the proposed approach offers an interpretable\nand scalable alternative for secure data sharing. Its structured design\nfacilitates integration into sensitive domains where both data quality and\nconfidentiality are critical.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u5f20\u91cf\u7f51\u7edc\uff08MPS\uff09\u751f\u6210\u9690\u79c1\u4fdd\u62a4\u7684\u9ad8\u8d28\u91cf\u5408\u6210\u8868\u683c\u6570\u636e\u7684\u65b9\u6cd5\uff0c\u5e76\u5728\u6570\u636e\u4fdd\u771f\u5ea6\u548c\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3\u73b0\u4ee3\u4eba\u5de5\u667a\u80fd\u4e2d\u6570\u636e\u7a00\u7f3a\u3001\u9690\u79c1\u9650\u5236\u4ee5\u53ca\u8bad\u7ec3\u9c81\u68d2\u6a21\u578b\u6240\u9700\u591a\u6837\u5316\u6570\u636e\u96c6\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u5f20\u91cf\u7f51\u7edc\uff08\u7279\u522b\u662f\u77e9\u9635\u79ef\u72b6\u6001MPS\uff09\u751f\u6210\u9690\u79c1\u4fdd\u62a4\u7684\u9ad8\u8d28\u91cf\u5408\u6210\u8868\u683c\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u6ce8\u5165\u566a\u58f0\u548c\u68af\u5ea6\u88c1\u526a\u6765\u5b9e\u73b0\u5dee\u5206\u9690\u79c1\uff08DP\uff09\uff0c\u5229\u7528R\u00e9nyi\u5dee\u5206\u9690\u79c1\uff08RDP\uff09\u8fdb\u884c\u6838\u7b97\u3002", "result": "MPS\u6a21\u578b\u5728\u6570\u636e\u4fdd\u771f\u5ea6\u548c\u4e0b\u6e38\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u6027\u80fd\u65b9\u9762\u4f18\u4e8eCTGAN\u3001VAE\u548cPrivBayes\u7b49\u73b0\u6709\u6a21\u578b\uff0c\u5c24\u5176\u662f\u5728\u4e25\u683c\u7684\u9690\u79c1\u9650\u5236\u4e0b\u3002", "conclusion": "MPS\u5728\u9690\u79c1\u611f\u77e5\u5408\u6210\u6570\u636e\u751f\u6210\u65b9\u9762\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u89e3\u91ca\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5bf9\u6570\u636e\u8d28\u91cf\u548c\u4fdd\u5bc6\u6027\u90fd\u6709\u8981\u6c42\u7684\u654f\u611f\u9886\u57df\u3002"}}
{"id": "2508.06355", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.06355", "abs": "https://arxiv.org/abs/2508.06355", "authors": ["Nhat A. Nghiem", "Tuan K. Do", "Tzu-Chieh Wei", "Trung V. Phan"], "title": "Quantum Algorithm for Estimating Intrinsic Geometry", "comment": null, "summary": "High-dimensional datasets typically cluster around lower-dimensional\nmanifolds but are also often marred by severe noise, obscuring the intrinsic\ngeometry essential for downstream learning tasks. We present a quantum\nalgorithm for estimating the intrinsic geometry of a point cloud --\nspecifically its local intrinsic dimension and local scalar curvature. These\nquantities are crucial for dimensionality reduction, feature extraction, and\nanomaly detection -- tasks that are central to a wide range of data-driven and\ndata-assisted applications. In this work, we propose a quantum algorithm which\ntakes a dataset with pairwise geometric distance, output the estimation of\nlocal dimension and curvature at a given point. We demonstrate that this\nquantum algorithm achieves an exponential speedup over its classical\ncounterpart, and, as a corollary, further extend our main technique to\ndiffusion maps, yielding exponential improvements even over existing quantum\nalgorithms. Our work marks another step toward efficient quantum applications\nin geometrical data analysis, moving beyond topological summaries toward\nprecise geometric inference and opening a novel, scalable path to\nquantum-enhanced manifold learning.", "AI": {"tldr": "\u91cf\u5b50\u7b97\u6cd5\u53ef\u52a0\u901f\u51e0\u4f55\u6570\u636e\u5206\u6790\uff0c\u5b9e\u73b0\u6307\u6570\u7ea7\u52a0\u901f\uff0c\u7528\u4e8e\u964d\u7ef4\u3001\u7279\u5f81\u63d0\u53d6\u548c\u5f02\u5e38\u68c0\u6d4b\u3002", "motivation": "\u9ad8\u7ef4\u6570\u636e\u96c6\u901a\u5e38\u56f4\u7ed5\u4f4e\u7ef4\u6d41\u5f62\u805a\u96c6\uff0c\u4f46\u5e38\u5e38\u5e26\u6709\u4e25\u91cd\u566a\u58f0\uff0c\u8fd9\u4f1a\u6a21\u7cca\u5bf9\u4e0b\u6e38\u5b66\u4e60\u4efb\u52a1\u81f3\u5173\u91cd\u8981\u7684\u5185\u5728\u51e0\u4f55\u5f62\u72b6\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u4f30\u8ba1\u70b9\u4e91\u5c40\u90e8\u5185\u5728\u7ef4\u5ea6\u548c\u5c40\u90e8\u6807\u91cf\u66f2\u7387\u7684\u65b9\u6cd5\uff0c\u8fd9\u4e9b\u91cf\u5bf9\u4e8e\u964d\u7ef4\u3001\u7279\u5f81\u63d0\u53d6\u548c\u5f02\u5e38\u68c0\u6d4b\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u91cf\u5b50\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u63a5\u6536\u5e26\u6709\u6210\u5bf9\u51e0\u4f55\u8ddd\u79bb\u7684\u6570\u636e\u96c6\uff0c\u5e76\u8f93\u51fa\u7ed9\u5b9a\u70b9\u7684\u5c40\u90e8\u7ef4\u5ea6\u548c\u66f2\u7387\u7684\u4f30\u8ba1\u503c\u3002", "result": "\u6240\u63d0\u51fa\u7684\u91cf\u5b50\u7b97\u6cd5\u5728\u4f30\u8ba1\u5c40\u90e8\u5185\u5728\u7ef4\u5ea6\u548c\u5c40\u90e8\u6807\u91cf\u66f2\u7387\u65b9\u9762\uff0c\u5b9e\u73b0\u4e86\u76f8\u5bf9\u4e8e\u5176\u7ecf\u5178\u5bf9\u5e94\u7b97\u6cd5\u7684\u6307\u6570\u7ea7\u52a0\u901f\u3002\u6b64\u5916\uff0c\u8be5\u6280\u672f\u8fd8\u53ef\u6269\u5c55\u5230\u6269\u6563\u6620\u5c04\uff0c\u5b9e\u73b0\u4e86\u76f8\u5bf9\u4e8e\u73b0\u6709\u91cf\u5b50\u7b97\u6cd5\u7684\u6307\u6570\u7ea7\u6539\u8fdb\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u4f30\u8ba1\u70b9\u4e91\u5c40\u90e8\u51e0\u4f55\u5f62\u72b6\uff08\u5c40\u90e8\u5185\u5728\u7ef4\u5ea6\u548c\u5c40\u90e8\u6807\u91cf\u66f2\u7387\uff09\u7684\u91cf\u5b50\u7b97\u6cd5\uff0c\u4e3a\u51e0\u4f55\u6570\u636e\u5206\u6790\u548c\u6d41\u5f62\u5b66\u4e60\u5f00\u8f9f\u4e86\u4e00\u6761\u65b0\u7684\u3001\u53ef\u6269\u5c55\u7684\u91cf\u5b50\u589e\u5f3a\u8def\u5f84\u3002"}}
{"id": "2508.06360", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06360", "abs": "https://arxiv.org/abs/2508.06360", "authors": ["Aisha Saeid", "Anu Sabu", "Girish A. Koushik", "Ferrante Neri", "Diptesh Kanojia"], "title": "Cyberbullying Detection via Aggression-Enhanced Prompting", "comment": "Accepted to RANLP 2025", "summary": "Detecting cyberbullying on social media remains a critical challenge due to\nits subtle and varied expressions. This study investigates whether integrating\naggression detection as an auxiliary task within a unified training framework\ncan enhance the generalisation and performance of large language models (LLMs)\nin cyberbullying detection. Experiments are conducted on five aggression\ndatasets and one cyberbullying dataset using instruction-tuned LLMs. We\nevaluated multiple strategies: zero-shot, few-shot, independent LoRA\nfine-tuning, and multi-task learning (MTL). Given the inconsistent results of\nMTL, we propose an enriched prompt pipeline approach in which aggression\npredictions are embedded into cyberbullying detection prompts to provide\ncontextual augmentation. Preliminary results show that the enriched prompt\npipeline consistently outperforms standard LoRA fine-tuning, indicating that\naggression-informed context significantly boosts cyberbullying detection. This\nstudy highlights the potential of auxiliary tasks, such as aggression\ndetection, to improve the generalisation of LLMs for safety-critical\napplications on social networks.", "AI": {"tldr": "\u672c\u7814\u7a76\u900f\u904e\u5c07\u653b\u64ca\u6027\u6aa2\u6e2c\u4f5c\u70ba\u8f14\u52a9\u4efb\u52d9\uff0c\u4e26\u63a1\u7528\u589e\u5f37\u7684\u63d0\u793a\u7ba1\u9053\u65b9\u6cd5\uff0c\u6210\u529f\u63d0\u5347\u4e86\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u5728\u7db2\u7d61\u6b3a\u51cc\u6aa2\u6e2c\u4e2d\u7684\u6027\u80fd\uff0c\u8b49\u5be6\u4e86\u8f14\u52a9\u4efb\u52d9\u5c0d\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u7db2\u7d61\u6b3a\u51cc\u7684\u6aa2\u6e2c\u4ecd\u7136\u662f\u4e00\u500b\u95dc\u9375\u7684\u6311\u6230\uff0c\u56e0\u70ba\u5176\u8868\u9054\u65b9\u5f0f\u5fae\u5999\u4e14\u591a\u6a23\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u8a0e\u5c07\u653b\u64ca\u6027\u6aa2\u6e2c\u4f5c\u70ba\u8f14\u52a9\u4efb\u52d9\u6574\u5408\u5230\u7d71\u4e00\u8a13\u7df4\u6846\u67b6\u4e2d\uff0c\u662f\u5426\u80fd\u589e\u5f37\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u7db2\u7d61\u6b3a\u51cc\u6aa2\u6e2c\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6027\u80fd\u3002", "method": "\u900f\u904e\u5728\u7d71\u4e00\u8a13\u7df4\u6846\u67b6\u5167\u6574\u5408\u653b\u64ca\u6027\u6aa2\u6e2c\u4f5c\u70ba\u8f14\u52a9\u4efb\u52d9\uff0c\u4e26\u8a55\u4f30\u4e86\u96f6\u6a23\u672c\u3001\u5c11\u6a23\u672c\u3001\u7368\u7acb LoRA \u5fae\u8abf\u548c\u591a\u4efb\u52d9\u5b78\u7fd2 (MTL) \u7b49\u591a\u7a2e\u7b56\u7565\u3002\u6b64\u5916\uff0c\u7814\u7a76\u9084\u63d0\u51fa\u4e86\u4e00\u7a2e\u589e\u5f37\u7684\u63d0\u793a\u7ba1\u9053\u65b9\u6cd5\uff0c\u5c07\u653b\u64ca\u6027\u9810\u6e2c\u5d4c\u5165\u5230\u7db2\u7d61\u6b3a\u51cc\u6aa2\u6e2c\u63d0\u793a\u4e2d\uff0c\u4ee5\u63d0\u4f9b\u4e0a\u4e0b\u6587\u589e\u5f37\u3002", "result": "\u589e\u5f37\u7684\u63d0\u793a\u7ba1\u9053\u65b9\u6cd5\u6301\u7e8c\u512a\u65bc\u6a19\u6e96\u7684 LoRA \u5fae\u8abf\uff0c\u8868\u660e\u53d7\u653b\u64ca\u6027\u4fe1\u606f\u5f71\u97ff\u7684\u4e0a\u4e0b\u6587\u986f\u8457\u63d0\u9ad8\u4e86\u7db2\u7d61\u6b3a\u51cc\u6aa2\u6e2c\u7684\u6027\u80fd\u3002", "conclusion": "\u6574\u5408\u7684\u8f14\u52a9\u4efb\u52d9\uff0c\u4f8b\u5982\u653b\u64ca\u6027\u6aa2\u6e2c\uff0c\u53ef\u4ee5\u63d0\u9ad8\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u5728\u793e\u4ea4\u7db2\u7d61\u5b89\u5168\u95dc\u9375\u61c9\u7528\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.06051", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06051", "abs": "https://arxiv.org/abs/2508.06051", "authors": ["Linhan Cao", "Wei Sun", "Weixia Zhang", "Xiangyang Zhu", "Jun Jia", "Kaiwei Zhang", "Dandan Zhu", "Guangtao Zhai", "Xiongkuo Min"], "title": "VQAThinker: Exploring Generalizable and Explainable Video Quality Assessment via Reinforcement Learning", "comment": null, "summary": "Video quality assessment (VQA) aims to objectively quantify perceptual\nquality degradation in alignment with human visual perception. Despite recent\nadvances, existing VQA models still suffer from two critical limitations:\n\\textit{poor generalization to out-of-distribution (OOD) videos} and\n\\textit{limited explainability}, which restrict their applicability in\nreal-world scenarios. To address these challenges, we propose\n\\textbf{VQAThinker}, a reasoning-based VQA framework that leverages large\nmultimodal models (LMMs) with reinforcement learning to jointly model video\nquality understanding and scoring, emulating human perceptual decision-making.\nSpecifically, we adopt group relative policy optimization (GRPO), a rule-guided\nreinforcement learning algorithm that enables reasoning over video quality\nunder score-level supervision, and introduce three VQA-specific rewards: (1) a\n\\textbf{bell-shaped regression reward} that increases rapidly as the prediction\nerror decreases and becomes progressively less sensitive near the ground truth;\n(2) a \\textbf{pairwise ranking reward} that guides the model to correctly\ndetermine the relative quality between video pairs; and (3) a \\textbf{temporal\nconsistency reward} that encourages the model to prefer temporally coherent\nvideos over their perturbed counterparts. Extensive experiments demonstrate\nthat VQAThinker achieves state-of-the-art performance on both in-domain and OOD\nVQA benchmarks, showing strong generalization for video quality scoring.\nFurthermore, evaluations on video quality understanding tasks validate its\nsuperiority in distortion attribution and quality description compared to\nexisting explainable VQA models and LMMs. These findings demonstrate that\nreinforcement learning offers an effective pathway toward building\ngeneralizable and explainable VQA models solely with score-level supervision.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86 VQAThinker\uff0c\u4e00\u4e2a\u57fa\u4e8e LMM \u548c\u5f3a\u5316\u5b66\u4e60\u7684\u89c6\u9891\u8d28\u91cf\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u79cd\u65b0\u9896\u7684\u5956\u52b1\u673a\u5236\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u7684\u6cdb\u5316\u6027\u548c\u53ef\u89e3\u91ca\u6027\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u9891\u8d28\u91cf\u8bc4\u4f30\u6a21\u578b\u5728\u6cdb\u5316\u5230\u5206\u5e03\u5916\uff08OOD\uff09\u89c6\u9891\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9650\u5236\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a VQAThinker \u7684\u57fa\u4e8e\u63a8\u7406\u7684\u89c6\u9891\u8d28\u91cf\u8bc4\u4f30\u6846\u67b6\u3002\u8be5\u6846\u67b6\u5229\u7528\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMM\uff09\u548c\u5f3a\u5316\u5b66\u4e60\uff08\u7279\u522b\u662f\u7fa4\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316 GRPO \u7b97\u6cd5\uff09\u6765\u8054\u5408\u5efa\u6a21\u89c6\u9891\u8d28\u91cf\u7406\u89e3\u548c\u8bc4\u5206\u3002\u5f15\u5165\u4e86\u4e09\u79cd\u89c6\u9891\u8d28\u91cf\u8bc4\u4f30\u5956\u52b1\uff1a\u949f\u5f62\u56de\u5f52\u5956\u52b1\u3001\u6210\u5bf9\u6392\u5e8f\u5956\u52b1\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u5956\u52b1\u3002", "result": "VQAThinker \u5728\u6807\u51c6\u548c\u975e\u6807\u51c6\u89c6\u9891\u8d28\u91cf\u8bc4\u4f30\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002\u5728\u89c6\u9891\u8d28\u91cf\u7406\u89e3\u4efb\u52a1\u7684\u8bc4\u4f30\u4e2d\uff0c\u8be5\u6a21\u578b\u5728\u5931\u771f\u5f52\u56e0\u548c\u8d28\u91cf\u63cf\u8ff0\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u7684\u53ef\u89e3\u91ca\u89c6\u9891\u8d28\u91cf\u8bc4\u4f30\u6a21\u578b\u548c\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a VQAThinker \u7684\u57fa\u4e8e\u63a8\u7406\u7684\u89c6\u9891\u8d28\u91cf\u8bc4\u4f30\u6846\u67b6\uff0c\u5229\u7528\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMM\uff09\u548c\u5f3a\u5316\u5b66\u4e60\u6765\u6a21\u62df\u4eba\u7c7b\u7684\u611f\u77e5\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u4e86\u89c6\u9891\u8d28\u91cf\u7684\u7406\u89e3\u548c\u8bc4\u5206\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cVQAThinker \u5728\u6807\u51c6\u548c\u975e\u6807\u51c6\u89c6\u9891\u8d28\u91cf\u8bc4\u4f30\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002\u6b64\u5916\uff0c\u5728\u89c6\u9891\u8d28\u91cf\u7406\u89e3\u4efb\u52a1\u4e0a\u7684\u8bc4\u4f30\u4e5f\u9a8c\u8bc1\u4e86\u5176\u5728\u5931\u771f\u5f52\u56e0\u548c\u8d28\u91cf\u63cf\u8ff0\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u7684\u53ef\u89e3\u91ca\u89c6\u9891\u8d28\u91cf\u8bc4\u4f30\u6a21\u578b\u548c\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u3002\u8fd9\u8868\u660e\u5f3a\u5316\u5b66\u4e60\u662f\u4ec5\u4f7f\u7528\u5206\u6570\u7ea7\u76d1\u7763\u5373\u53ef\u6784\u5efa\u53ef\u6cdb\u5316\u3001\u53ef\u89e3\u91ca\u7684\u89c6\u9891\u8d28\u91cf\u8bc4\u4f30\u6a21\u578b\u7684\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2508.06257", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06257", "abs": "https://arxiv.org/abs/2508.06257", "authors": ["Jielong Lu", "Zhihao Wu", "Jiajun Yu", "Jiajun Bu", "Haishuai Wang"], "title": "Multi-Omics Analysis for Cancer Subtype Inference via Unrolling Graph Smoothness Priors", "comment": null, "summary": "Integrating multi-omics datasets through data-driven analysis offers a\ncomprehensive understanding of the complex biological processes underlying\nvarious diseases, particularly cancer. Graph Neural Networks (GNNs) have\nrecently demonstrated remarkable ability to exploit relational structures in\nbiological data, enabling advances in multi-omics integration for cancer\nsubtype classification. Existing approaches often neglect the intricate\ncoupling between heterogeneous omics, limiting their capacity to resolve subtle\ncancer subtype heterogeneity critical for precision oncology. To address these\nlimitations, we propose a framework named Graph Transformer for Multi-omics\nCancer Subtype Classification (GTMancer). This framework builds upon the GNN\noptimization problem and extends its application to complex multi-omics data.\nSpecifically, our method leverages contrastive learning to embed multi-omics\ndata into a unified semantic space. We unroll the multiplex graph optimization\nproblem in that unified space and introduce dual sets of attention coefficients\nto capture structural graph priors both within and among multi-omics data. This\napproach enables global omics information to guide the refining of the\nrepresentations of individual omics. Empirical experiments on seven real-world\ncancer datasets demonstrate that GTMancer outperforms existing state-of-the-art\nalgorithms.", "AI": {"tldr": "GTMancer\u662f\u4e00\u4e2a\u65b0\u7684\u591a\u7ec4\u5b66\u6574\u5408\u6846\u67b6\uff0c\u5229\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u548c\u5bf9\u6bd4\u5b66\u4e60\u6765\u63d0\u9ad8\u764c\u75c7\u4e9a\u578b\u5206\u7c7b\u7684\u51c6\u786e\u6027\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u7ec4\u5b66\u6574\u5408\u65b9\u6cd5\u5e38\u5e38\u5ffd\u89c6\u5f02\u6784\u7ec4\u5b66\u4e4b\u95f4\u590d\u6742\u7684\u8026\u5408\u5173\u7cfb\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u89e3\u6790\u5bf9\u7cbe\u786e\u80bf\u7624\u5b66\u81f3\u5173\u91cd\u8981\u7684\u7ec6\u5fae\u764c\u75c7\u4e9a\u578b\u5f02\u8d28\u6027\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGTMancer\uff08Graph Transformer for Multi-omics Cancer Subtype Classification\uff09\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u57fa\u4e8eGNN\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u6269\u5c55\u5230\u590d\u6742\u7684\u591a\u7ec4\u5b66\u6570\u636e\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u5bf9\u6bd4\u5b66\u4e60\u5c06\u591a\u7ec4\u5b66\u6570\u636e\u5d4c\u5165\u5230\u7edf\u4e00\u7684\u8bed\u4e49\u7a7a\u95f4\u4e2d\uff0c\u5e76\u5728\u8be5\u7edf\u4e00\u7a7a\u95f4\u4e2d\u5c55\u5f00\u591a\u91cd\u56fe\u4f18\u5316\u95ee\u9898\uff0c\u5f15\u5165\u4e24\u7ec4\u6ce8\u610f\u529b\u7cfb\u6570\u6765\u6355\u6349\u7ec4\u5b66\u6570\u636e\u5185\u90e8\u548c\u4e4b\u95f4\u7ed3\u6784\u56fe\u7684\u5148\u9a8c\u4fe1\u606f\uff0c\u4ece\u800c\u5b9e\u73b0\u5168\u5c40\u7ec4\u5b66\u4fe1\u606f\u5bf9\u5355\u4e2a\u7ec4\u5b66\u8868\u793a\u7684\u7ec6\u5316\u3002", "result": "GTMancer\u5728\u4e03\u4e2a\u771f\u5b9e\u764c\u75c7\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u7b97\u6cd5\u3002", "conclusion": "GTMancer\u6846\u67b6\u5728\u4e03\u4e2a\u771f\u5b9e\u764c\u75c7\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u8bc1\u5b9e\u9a8c\u8868\u660e\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u7b97\u6cd5\u3002"}}
{"id": "2508.06358", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.06358", "abs": "https://arxiv.org/abs/2508.06358", "authors": ["Zong-Liang Li", "Shi-Xin Zhang"], "title": "The Dual Role of Low-Weight Pauli Propagation: A Flawed Simulator but a Powerful Initializer for Variational Quantum Algorithms", "comment": "4.5 pages, 5 figures with Supplemental Materials", "summary": "Variational quantum algorithms (VQAs) rely on a classical optimizer to tune a\nparameterized quantum circuit, raising the question of whether classical\nmethods can assist in this process. In this work, we investigate the low-weight\nPauli propagation (LWPP) algorithm as a potential classical tool for simulating\nthe VQA circuit. We first find that LWPP is an unreliable estimator of the true\nenergy, limiting its utility as a direct simulator. However, we uncover its\nreal value: despite this numerical inaccuracy, its approximate optimization\nlandscape robustly guides parameters toward high-quality basins of attraction.\nWe therefore propose harnessing LWPP not for simulation, but as a classical\npre-optimizer to find superior initial parameters for the main VQA loop.\nBenchmarking this strategy on Heisenberg models, we demonstrate a remarkable\nenhancement in both the final accuracy and convergence rate, typically by an\norder of magnitude, over standard heuristics. Our work thus reframes LWPP from\na flawed simulator into a powerful classical pre-processor that effectively\nmitigates the notorious optimization challenges in VQAs and reduces the\ncomputational burden on near-term quantum hardware.", "AI": {"tldr": "LWPP\u7b97\u6cd5\u867d\u4e0d\u80fd\u51c6\u786e\u6a21\u62dfVQA\u7535\u8def\uff0c\u4f46\u5176\u4f18\u5316\u80fd\u529b\u7684\u8fd1\u4f3c\u7279\u6027\u53ef\u7528\u4e8e\u4f18\u5316VQA\u7684\u521d\u59cb\u53c2\u6570\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u662f\u5426\u53ef\u4ee5\u5229\u7528\u7ecf\u5178\u65b9\u6cd5\uff08\u7279\u522b\u662fLWPP\u7b97\u6cd5\uff09\u6765\u8f85\u52a9VQA\u7684\u53c2\u6570\u4f18\u5316\u8fc7\u7a0b\u3002", "method": "\u63d0\u51fa\u4f7f\u7528\u4f4e\u6743\u91cd\u6ce1\u5229\u4f20\u64ad\uff08LWPP\uff09\u7b97\u6cd5\u4f5c\u4e3a\u7ecf\u5178\u9884\u4f18\u5316\u5668\uff0c\u4e3aVQA\u4e3b\u4f18\u5316\u5faa\u73af\u627e\u5230\u66f4\u4f18\u7684\u521d\u59cb\u53c2\u6570\u3002", "result": "LWPP\u53ef\u4ee5\u4f5c\u4e3aVQA\u7684\u7ecf\u5178\u9884\u4f18\u5316\u5668\uff0c\u663e\u8457\u63d0\u9ad8\u4f18\u5316\u6027\u80fd\uff0c\u800c\u975e\u76f4\u63a5\u6a21\u62df\u5668\u3002\u5728Heisenberg\u6a21\u578b\u4e0a\u7684\u6d4b\u8bd5\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u5c06\u6700\u7ec8\u7cbe\u5ea6\u548c\u6536\u655b\u901f\u5ea6\u63d0\u9ad8\u4e00\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "VQAs\u7684\u4f18\u5316\u95ee\u9898\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528LWPP\u4f5c\u4e3a\u7ecf\u5178\u9884\u4f18\u5316\u5668\u6765\u7f13\u89e3\uff0c\u4ece\u800c\u63d0\u9ad8\u6700\u7ec8\u7cbe\u5ea6\u548c\u6536\u655b\u901f\u5ea6\uff0c\u5e76\u964d\u4f4e\u8fd1\u671f\u91cf\u5b50\u786c\u4ef6\u7684\u8ba1\u7b97\u8d1f\u62c5\u3002"}}
{"id": "2508.06374", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06374", "abs": "https://arxiv.org/abs/2508.06374", "authors": ["Anubhav Jangra", "Bahareh Sarrafzadeh", "Adrian de Wynter", "Silviu Cucerzan", "Sujay Kumar Jauhar"], "title": "Evaluating Style-Personalized Text Generation: Challenges and Directions", "comment": null, "summary": "While prior research has built tools and benchmarks towards style\npersonalized text generation, there has been limited exploration of evaluation\nin low-resource author style personalized text generation space. Through this\nwork, we question the effectiveness of the widely adopted evaluation metrics\nlike BLEU and ROUGE, and explore other evaluation paradigms such as style\nembeddings and LLM-as-judge to holistically evaluate the style personalized\ntext generation task. We evaluate these metrics and their ensembles using our\nstyle discrimination benchmark, that spans eight writing tasks, and evaluates\nacross three settings, domain discrimination, authorship attribution, and LLM\npersonalized vs non-personalized discrimination. We provide conclusive evidence\nto adopt ensemble of diverse evaluation metrics to effectively evaluate style\npersonalized text generation.", "AI": {"tldr": "This paper questions traditional evaluation metrics (BLEU, ROUGE) for style-personalized text generation and proposes using style embeddings and LLM-as-judge. Tested on a benchmark across different tasks and settings, the study concludes that a combination of diverse metrics is most effective for evaluation.", "motivation": "The research addresses the limited exploration of evaluation methods in the low-resource author style-personalized text generation space, questioning the effectiveness of existing metrics like BLEU and ROUGE.", "method": "The paper explores the effectiveness of widely adopted evaluation metrics like BLEU and ROUGE, and investigates alternative evaluation paradigms such as style embeddings and LLM-as-judge for evaluating style-personalized text generation. These metrics were evaluated using a style discrimination benchmark spanning eight writing tasks across three settings: domain discrimination, authorship attribution, and LLM personalized vs. non-personalized discrimination.", "result": "The study evaluates various metrics and their ensembles using a style discrimination benchmark, offering conclusive evidence for the effectiveness of diverse metric ensembles in evaluating style-personalized text generation.", "conclusion": "The paper provides conclusive evidence supporting the adoption of an ensemble of diverse evaluation metrics for effectively evaluating style-personalized text generation."}}
{"id": "2508.06269", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06269", "abs": "https://arxiv.org/abs/2508.06269", "authors": ["Zhuoran Li", "Xun Wang", "Hai Zhong", "Longbo Huang"], "title": "OM2P: Offline Multi-Agent Mean-Flow Policy", "comment": null, "summary": "Generative models, especially diffusion and flow-based models, have been\npromising in offline multi-agent reinforcement learning. However, integrating\npowerful generative models into this framework poses unique challenges. In\nparticular, diffusion and flow-based policies suffer from low sampling\nefficiency due to their iterative generation processes, making them impractical\nin time-sensitive or resource-constrained settings. To tackle these\ndifficulties, we propose OM2P (Offline Multi-Agent Mean-Flow Policy), a novel\noffline MARL algorithm to achieve efficient one-step action sampling. To\naddress the misalignment between generative objectives and reward maximization,\nwe introduce a reward-aware optimization scheme that integrates a\ncarefully-designed mean-flow matching loss with Q-function supervision.\nAdditionally, we design a generalized timestep distribution and a\nderivative-free estimation strategy to reduce memory overhead and improve\ntraining stability. Empirical evaluations on Multi-Agent Particle and MuJoCo\nbenchmarks demonstrate that OM2P achieves superior performance, with up to a\n3.8x reduction in GPU memory usage and up to a 10.8x speed-up in training time.\nOur approach represents the first to successfully integrate mean-flow model\ninto offline MARL, paving the way for practical and scalable generative\npolicies in cooperative multi-agent settings.", "AI": {"tldr": "OM2P \u662f\u4e00\u79cd\u65b0\u9896\u7684\u79bb\u7ebf MARL \u7b97\u6cd5\uff0c\u901a\u8fc7\u9ad8\u6548\u7684\u4e00\u6b65\u52a8\u4f5c\u91c7\u6837\u548c\u5956\u52b1\u611f\u77e5\u4f18\u5316\uff0c\u89e3\u51b3\u4e86\u751f\u6210\u7b56\u7565\u7684\u91c7\u6837\u6548\u7387\u95ee\u9898\uff0c\u5728\u591a\u667a\u80fd\u4f53\u7c92\u5b50\u548c MuJoCo \u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u8d8a\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u6269\u6563\u548c\u57fa\u4e8e\u6d41\u7684\u7b56\u7565\u5728\u79bb\u7ebf\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u91c7\u6837\u6548\u7387\u4f4e\uff0c\u96be\u4ee5\u5728\u65f6\u95f4\u654f\u611f\u6216\u8d44\u6e90\u53d7\u9650\u8bbe\u7f6e\u4e2d\u5e94\u7528\u7684\u95ee\u9898\u3002", "method": "OM2P\uff08Offline Multi-Agent Mean-Flow Policy\uff09\u7b97\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u5956\u52b1\u611f\u77e5\u4f18\u5316\u65b9\u6848\uff0c\u5e76\u7ed3\u5408\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5747\u6d41\u5339\u914d\u635f\u5931\u548c Q \u51fd\u6570\u76d1\u7763\uff0c\u4ee5\u53ca\u8bbe\u8ba1\u5e7f\u4e49\u65f6\u95f4\u6b65\u5206\u5e03\u548c\u65e0\u5bfc\u6570\u4f30\u8ba1\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u4e00\u6b65\u52a8\u4f5c\u91c7\u6837\uff0c\u4ece\u800c\u89e3\u51b3\u4e86\u6269\u6563\u548c\u57fa\u4e8e\u6d41\u7684\u7b56\u7565\u5728\u65f6\u95f4\u654f\u611f\u6216\u8d44\u6e90\u53d7\u9650\u8bbe\u7f6e\u4e2d\u7684\u91c7\u6837\u6548\u7387\u4f4e\u7684\u95ee\u9898\u3002", "result": "OM2P \u5728\u591a\u667a\u80fd\u4f53\u7c92\u5b50\u548c MuJoCo \u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\uff0cGPU \u5185\u5b58\u4f7f\u7528\u91cf\u51cf\u5c11\u4e86 3.8 \u500d\uff0c\u8bad\u7ec3\u65f6\u95f4\u7f29\u77ed\u4e86 10.8 \u500d\u3002", "conclusion": "OM2P \u6210\u529f\u5730\u5c06\u5747\u6d41\u6a21\u578b\u96c6\u6210\u5230\u79bb\u7ebf MARL \u4e2d\uff0c\u4e3a\u5408\u4f5c\u591a\u667a\u80fd\u4f53\u8bbe\u7f6e\u4e2d\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u751f\u6210\u7b56\u7565\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2508.06362", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.06362", "abs": "https://arxiv.org/abs/2508.06362", "authors": ["Gioele Casagranda", "Elizabeth Auden", "Carlo Cazzaniga", "Maria Kastriotou", "Christopher Frost", "Marzio Vallero", "Flavio Vella", "Paolo Rech"], "title": "SQUID G.A.M.E.: Gamma, Atmospheric, and Mono-Energetic Neutron Effects on Quantum Devices", "comment": null, "summary": "Quantum devices are a promising solution to many research applications,\nincluding medical imaging, precision magnetic field measurements, condensed\nmatter physics, and overcoming the limits of classical computing. Among the\navailable implementations, the superconducting technology is the current focus\nof scientific research and industrial applications, excelling in performance\nand scalability. Despite this, superconducting quantum systems are extremely\nprone to decoherence, and in particular, they are highly sensitive to radiation\nevents. In this paper, we analyze the response of a superconducting device\n(SQUID) to radiation. We expose the SQUID to beams of monoenergetic 14 MeV\nneutrons (NILE - ISIS), atmospheric 1-800 MeV neutrons (ChipIR - ISIS), and\ngamma rays with 1.25 MeV average energy (CALLIOPE - ENEA). These experiments\nshow that the SQUID is sensitive to the two neutron fields, while gamma rays at\n1.25 MeV leave it mostly unaffected. Following our experiments with neutrons,\nit is possible to characterize the SQUID's response and even classify faults\naccording to their shape and duration. We identify two categories: bursts (long\nlasting) and peaks (short lived). To investigate the different responses to\nneutrons and gamma rays, we employ Geant4 simulations, which highlight\ndifferences in the deposition spectra and the energy propagation, but likewise\npredict the vulnerability of the SQUID in both cases.", "AI": {"tldr": "SQUID\u91cf\u5b50\u8bbe\u5907\u5bf9\u4e2d\u5b50\u8f90\u5c04\u654f\u611f\uff0c\u4f46\u5bf9\u4f3d\u9a6c\u5c04\u7ebf\u4e0d\u654f\u611f\uff0c\u5b9e\u9a8c\u548c\u6a21\u62df\u90fd\u8bc1\u5b9e\u4e86\u8fd9\u4e00\u70b9\u3002", "motivation": "\u91cf\u5b50\u8bbe\u5907\u5728\u5305\u62ec\u533b\u7597\u6210\u50cf\u3001\u7cbe\u5bc6\u78c1\u573a\u6d4b\u91cf\u3001\u51dd\u805a\u6001\u7269\u7406\u548c\u514b\u670d\u7ecf\u5178\u8ba1\u7b97\u9650\u5236\u5728\u5185\u7684\u591a\u79cd\u7814\u7a76\u5e94\u7528\u4e2d\u5177\u6709\u6f5c\u529b\u3002\u8d85\u5bfc\u6280\u672f\u662f\u76ee\u524d\u79d1\u5b66\u7814\u7a76\u548c\u5de5\u4e1a\u5e94\u7528\u7684\u7814\u7a76\u91cd\u70b9\uff0c\u4f46\u5728\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u7136\u800c\uff0c\u8d85\u5bfc\u91cf\u5b50\u7cfb\u7edf\u6781\u6613\u53d1\u751f\u9000\u76f8\u5e72\uff0c\u5e76\u4e14\u5bf9\u8f90\u5c04\u4e8b\u4ef6\u9ad8\u5ea6\u654f\u611f\u3002", "method": "\u901a\u8fc7\u5c06SQUID\u66b4\u9732\u4e8e\u4e0d\u540c\u80fd\u91cf\u7684\u4e2d\u5b50\u675f\u548c\u4f3d\u9a6c\u5c04\u7ebf\uff0c\u5e76\u4f7f\u7528Geant4\u6a21\u62df\u6765\u5206\u6790\u5176\u54cd\u5e94\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSQUID\u5bf9\u4e24\u79cd\u4e2d\u5b50\u573a\u654f\u611f\uff0c\u800c1.25 MeV\u7684\u4f3d\u9a6c\u5c04\u7ebf\u5bf9\u5176\u5f71\u54cd\u4e0d\u5927\u3002\u901a\u8fc7\u5b9e\u9a8c\uff0c\u53ef\u4ee5\u8868\u5f81SQUID\u7684\u54cd\u5e94\uff0c\u5e76\u6839\u636e\u6545\u969c\u7684\u5f62\u72b6\u548c\u6301\u7eed\u65f6\u95f4\u5bf9\u6545\u969c\u8fdb\u884c\u5206\u7c7b\uff0c\u5206\u4e3a\u957f\u671f\u7684\u201c\u731d\u53d1\u201d\u548c\u77ed\u671f\u7684\u201c\u5cf0\u503c\u201d\u3002Geant4\u6a21\u62df\u7a81\u51fa\u4e86\u6c89\u79ef\u5149\u8c31\u548c\u80fd\u91cf\u4f20\u64ad\u7684\u5dee\u5f02\uff0c\u4f46\u4e5f\u9884\u6d4b\u4e86SQUID\u5728\u8fd9\u4e24\u79cd\u60c5\u51b5\u4e0b\u7684\u8106\u5f31\u6027\u3002", "conclusion": "SQUID\u8bbe\u5907\u5bf9\u4e2d\u5b50\u8f90\u5c04\u654f\u611f\uff0c\u5bf9\u4f3d\u9a6c\u5c04\u7ebf\u4e0d\u654f\u611f\uff0c\u5e76\u4e14\u53ef\u4ee5\u6839\u636e\u6545\u969c\u7684\u5f62\u72b6\u548c\u6301\u7eed\u65f6\u95f4\u5bf9\u6545\u969c\u8fdb\u884c\u5206\u7c7b\u3002"}}
{"id": "2508.06388", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06388", "abs": "https://arxiv.org/abs/2508.06388", "authors": ["Lanlan Qiu", "Xiao Pu", "Yeqi Feng", "Tianxing He"], "title": "LLMs vs. Chinese Anime Enthusiasts: A Comparative Study on Emotionally Supportive Role-Playing", "comment": "21 pages, 17 figures, 3 tables", "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities in\nrole-playing conversations and providing emotional support as separate research\ndirections. However, there remains a significant research gap in combining\nthese capabilities to enable emotionally supportive interactions with virtual\ncharacters. To address this research gap, we focus on anime characters as a\ncase study because of their well-defined personalities and large fan bases.\nThis choice enables us to effectively evaluate how well LLMs can provide\nemotional support while maintaining specific character traits. We introduce\nChatAnime, the first Emotionally Supportive Role-Playing (ESRP) dataset. We\nfirst thoughtfully select 20 top-tier characters from popular anime communities\nand design 60 emotion-centric real-world scenario questions. Then, we execute a\nnationwide selection process to identify 40 Chinese anime enthusiasts with\nprofound knowledge of specific characters and extensive experience in\nrole-playing. Next, we systematically collect two rounds of dialogue data from\n10 LLMs and these 40 Chinese anime enthusiasts. To evaluate the ESRP\nperformance of LLMs, we design a user experience-oriented evaluation system\nfeaturing 9 fine-grained metrics across three dimensions: basic dialogue,\nrole-playing and emotional support, along with an overall metric for response\ndiversity. In total, the dataset comprises 2,400 human-written and 24,000\nLLM-generated answers, supported by over 132,000 human annotations.\nExperimental results show that top-performing LLMs surpass human fans in\nrole-playing and emotional support, while humans still lead in response\ndiversity. We hope this work can provide valuable resources and insights for\nfuture research on optimizing LLMs in ESRP. Our datasets are available at\nhttps://github.com/LanlanQiu/ChatAnime.", "AI": {"tldr": "\u672c\u7814\u7a76\u6784\u5efa\u4e86\u9996\u4e2a\u52a8\u6f2b\u89d2\u8272\u7684\u60c5\u611f\u652f\u6301\u89d2\u8272\u626e\u6f14\uff08ESRP\uff09\u6570\u636e\u96c6ChatAnime\uff0c\u8bc4\u4f30\u4e8610\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002\u7ed3\u679c\u663e\u793a\uff0c\u6700\u4f73\u6a21\u578b\u5728\u89d2\u8272\u626e\u6f14\u548c\u60c5\u611f\u652f\u6301\u4e0a\u4f18\u4e8e\u4eba\u7c7b\uff0c\u4f46\u5728\u56de\u5e94\u591a\u6837\u6027\u4e0a\u4e0d\u5982\u4eba\u7c7b\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7ed3\u5408\u89d2\u8272\u626e\u6f14\u5bf9\u8bdd\u548c\u63d0\u4f9b\u60c5\u611f\u652f\u6301\u65b9\u9762\u7684\u7814\u7a76\u7a7a\u767d\uff0c\u672c\u7814\u7a76\u4ee5\u52a8\u6f2b\u89d2\u8272\u4e3a\u6848\u4f8b\uff0c\u65e8\u5728\u4f7f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u63d0\u4f9b\u60c5\u611f\u652f\u6301\uff0c\u540c\u65f6\u4fdd\u6301\u7279\u5b9a\u7684\u89d2\u8272\u7279\u5f81\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b2,400\u4e2a\u4eba\u7c7b\u7f16\u5199\u7b54\u6848\u548c24,000\u4e2aLLM\u751f\u6210\u7b54\u6848\u7684\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc7132,000\u591a\u4e2a\u4eba\u5de5\u6807\u6ce8\u8fdb\u884c\u652f\u6301\u3002\u8be5\u6570\u636e\u96c6\u9009\u53d6\u4e8620\u4e2a\u70ed\u95e8\u52a8\u6f2b\u89d2\u8272\uff0c\u8bbe\u8ba1\u4e8660\u4e2a\u4ee5\u60c5\u611f\u4e3a\u4e2d\u5fc3\u7684\u771f\u5b9e\u573a\u666f\u95ee\u9898\uff0c\u5e76\u62db\u52df\u4e8640\u540d\u7cbe\u901a\u52a8\u6f2b\u89d2\u8272\u7684\u7c89\u4e1d\u8fdb\u884c\u89d2\u8272\u626e\u6f14\u5bf9\u8bdd\u6570\u636e\u6536\u96c6\u3002\u7814\u7a76\u8fd8\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5305\u542b9\u4e2a\u7ec6\u7c92\u5ea6\u6307\u6807\u7684\u7528\u6237\u4f53\u9a8c\u8bc4\u4f30\u7cfb\u7edf\uff0c\u4ece\u57fa\u672c\u5bf9\u8bdd\u3001\u89d2\u8272\u626e\u6f14\u548c\u60c5\u611f\u652f\u6301\u4e09\u4e2a\u7ef4\u5ea6\u4ee5\u53ca\u56de\u5e94\u591a\u6837\u6027\u65b9\u9762\u8bc4\u4f30LLM\u7684\u8868\u73b0\u3002", "result": "\u5728\u89d2\u8272\u626e\u6f14\u548c\u60c5\u611f\u652f\u6301\u65b9\u9762\uff0c\u8868\u73b0\u6700\u4f73\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5df2\u8d85\u8d8a\u4eba\u7c7b\u7c89\u4e1d\uff0c\u4f46\u5728\u56de\u5e94\u591a\u6837\u6027\u65b9\u9762\u4eba\u7c7b\u4ecd\u5360\u4f18\u52bf\u3002", "conclusion": "\u8be5\u7814\u7a76\u9996\u6b21\u63d0\u51fa\u4e86\u9762\u5411\u52a8\u6f2b\u89d2\u8272\u7684\u60c5\u611f\u652f\u6301\u89d2\u8272\u626e\u6f14\uff08ESRP\uff09\u6570\u636e\u96c6ChatAnime\uff0c\u5e76\u5bf910\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u89d2\u8272\u626e\u6f14\u548c\u60c5\u611f\u652f\u6301\u65b9\u9762\uff0c\u8868\u73b0\u6700\u4f73\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5df2\u8d85\u8d8a\u4eba\u7c7b\u7c89\u4e1d\uff0c\u4f46\u5728\u56de\u5e94\u591a\u6837\u6027\u65b9\u9762\u4eba\u7c7b\u4ecd\u5360\u4f18\u52bf\u3002\u8be5\u7814\u7a76\u65e8\u5728\u4e3a\u672a\u6765\u4f18\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728ESRP\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u63d0\u4f9b\u8d44\u6e90\u548c\u89c1\u89e3\u3002"}}
{"id": "2508.06057", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06057", "abs": "https://arxiv.org/abs/2508.06057", "authors": ["Mojtaba Valipour", "Kelly Zheng", "James Lowman", "Spencer Szabados", "Mike Gartner", "Bobby Braswell"], "title": "AGI for the Earth, the path, possibilities and how to evaluate intelligence of models that work with Earth Observation Data?", "comment": "Accepted in IGARSS 2025!", "summary": "Artificial General Intelligence (AGI) is closer than ever to becoming a\nreality, sparking widespread enthusiasm in the research community to collect\nand work with various modalities, including text, image, video, and audio.\nDespite recent efforts, satellite spectral imagery, as an additional modality,\nhas yet to receive the attention it deserves. This area presents unique\nchallenges, but also holds great promise in advancing the capabilities of AGI\nin understanding the natural world. In this paper, we argue why Earth\nObservation data is useful for an intelligent model, and then we review\nexisting benchmarks and highlight their limitations in evaluating the\ngeneralization ability of foundation models in this domain. This paper\nemphasizes the need for a more comprehensive benchmark to evaluate earth\nobservation models. To facilitate this, we propose a comprehensive set of tasks\nthat a benchmark should encompass to effectively assess a model's ability to\nunderstand and interact with Earth observation data.", "AI": {"tldr": "\u8bba\u6587\u6307\u51fa\uff0c\u536b\u661f\u5149\u8c31\u56fe\u50cf\u5bf9\u901a\u7528\u4eba\u5de5\u667a\u80fd\u5f88\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u57fa\u51c6\u4e0d\u8db3\u3002\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u4e2a\u66f4\u5168\u9762\u7684\u57fa\u51c6\u548c\u4efb\u52a1\u96c6\uff0c\u4ee5\u8bc4\u4f30\u6a21\u578b\u5728\u5730\u7403\u89c2\u6d4b\u6570\u636e\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u968f\u7740\u901a\u7528\u4eba\u5de5\u667a\u80fd\uff08AGI\uff09\u7684\u4e0d\u65ad\u53d1\u5c55\uff0c\u7814\u7a76\u754c\u5bf9\u591a\u6a21\u6001\u6570\u636e\u7684\u5174\u8da3\u65e5\u76ca\u6d53\u539a\uff0c\u4f46\u536b\u661f\u5149\u8c31\u56fe\u50cf\u8fd9\u4e00\u91cd\u8981\u6a21\u6001\u5c1a\u672a\u5f97\u5230\u5145\u5206\u5173\u6ce8\uff0c\u5176\u5728\u63a8\u52a8AGI\u7406\u89e3\u81ea\u7136\u4e16\u754c\u65b9\u9762\u7684\u6f5c\u529b\u5de8\u5927\u3002", "method": "\u901a\u8fc7\u8bba\u8bc1\u5730\u7403\u89c2\u6d4b\u6570\u636e\u5bf9\u667a\u80fd\u6a21\u578b\u7684\u7528\u5904\uff0c\u5e76\u56de\u987e\u73b0\u6709\u57fa\u51c6\u53ca\u5176\u5c40\u9650\u6027\uff0c\u6765\u5f3a\u8c03\u5730\u7403\u89c2\u6d4b\u6570\u636e\u5728\u901a\u7528\u4eba\u5de5\u667a\u80fd\u4e2d\u7684\u6f5c\u529b\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u5957\u5168\u9762\u7684\u4efb\u52a1\uff0c\u7528\u4e8e\u6784\u5efa\u4e00\u4e2a\u80fd\u591f\u6709\u6548\u8bc4\u4f30\u6a21\u578b\u5728\u5730\u7403\u89c2\u6d4b\u9886\u57df\u7406\u89e3\u548c\u4ea4\u4e92\u80fd\u529b\u7684\u57fa\u51c6\u3002", "conclusion": "\u8be5\u8bba\u6587\u5f3a\u8c03\u4e86\u9700\u8981\u4e00\u4e2a\u66f4\u5168\u9762\u7684\u57fa\u51c6\u6765\u8bc4\u4f30\u5730\u7403\u89c2\u6d4b\u6a21\u578b\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u51c6\u5e94\u5305\u542b\u7684\u4e00\u5957\u5168\u9762\u7684\u4efb\u52a1\uff0c\u4ee5\u6709\u6548\u8bc4\u4f30\u6a21\u578b\u7406\u89e3\u548c\u4ea4\u4e92\u5730\u7403\u89c2\u6d4b\u6570\u636e\u7684\u80fd\u529b\u3002"}}
{"id": "2508.06280", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06280", "abs": "https://arxiv.org/abs/2508.06280", "authors": ["Gokul Adethya T", "S. Jaya Nirmala"], "title": "A Study on Regularization-Based Continual Learning Methods for Indic ASR", "comment": null, "summary": "Indias linguistic diversity poses significant challenges for developing\ninclusive Automatic Speech Recognition (ASR) systems. Traditional multilingual\nmodels, which require simultaneous access to all language data, are impractical\ndue to the sequential arrival of data and privacy constraints. Continual\nLearning (CL) offers a solution by enabling models to learn new languages\nsequentially without catastrophically forgetting previously learned knowledge.\nThis paper investigates CL for ASR on Indian languages using a subset of the\nIndicSUPERB benchmark. We employ a Conformer-based hybrid RNN-T/CTC model,\ninitially pretrained on Hindi, which is then incrementally trained on eight\nadditional Indian languages, for a total sequence of nine languages. We\nevaluate three prominent regularization- and distillation-based CL strategies:\nElastic Weight Consolidation (EWC), Memory Aware Synapses (MAS), and Learning\nwithout Forgetting (LwF), selected for their suitability in no-replay,\nprivacy-conscious scenarios. Performance is analyzed using Word Error Rate\n(WER) for both RNN-T and CTC paths on clean and noisy data, as well as\nknowledge retention via Backward Transfer. We also explore the impact of\nvarying the number of training epochs (1, 2, 5, and 10) per task. Results,\ncompared against naive fine-tuning, demonstrate CLs effectiveness in mitigating\nforgetting, making it a promising approach for scalable ASR in diverse Indian\nlanguages under realistic constraints. The code is available at:\nhttps://github.com/FrozenWolf-Cyber/Indic-CL-ASR", "AI": {"tldr": "\u5370\u5ea6\u8bed\u8a00\u7684ASR\u9762\u4e34\u6311\u6218\uff0c\u56e0\u4e3a\u6570\u636e\u662f\u6309\u987a\u5e8f\u5230\u8fbe\u7684\uff0c\u5e76\u4e14\u6709\u9690\u79c1\u9650\u5236\u3002\u672c\u7814\u7a76\u4f7f\u7528\u6301\u7eed\u5b66\u4e60\uff08CL\uff09\u548c\u4e09\u79cd\u7b56\u7565\uff08EWC\u3001MAS\u3001LwF\uff09\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u5e76\u53d6\u5f97\u4e86\u826f\u597d\u7684\u6548\u679c\u3002", "motivation": "\u5370\u5ea6\u7684\u8bed\u8a00\u591a\u6837\u6027\u7ed9\u5f00\u53d1\u5305\u5bb9\u6027\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u7cfb\u7edf\u5e26\u6765\u4e86\u91cd\u5927\u6311\u6218\u3002\u4f20\u7edf\u7684\u591a\u8bed\u8a00\u6a21\u578b\u9700\u8981\u540c\u65f6\u8bbf\u95ee\u6240\u6709\u8bed\u8a00\u6570\u636e\uff0c\u7531\u4e8e\u6570\u636e\u7684\u987a\u5e8f\u5230\u8fbe\u548c\u9690\u79c1\u9650\u5236\uff0c\u8fd9\u662f\u4e0d\u5207\u5b9e\u9645\u7684\u3002CL\u63d0\u4f9b\u4e86\u4e00\u79cd\u89e3\u51b3\u65b9\u6848\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u6309\u987a\u5e8f\u5b66\u4e60\u65b0\u8bed\u8a00\uff0c\u800c\u4e0d\u4f1a\u707e\u96be\u6027\u5730\u9057\u5fd8\u5148\u524d\u5b66\u4e60\u7684\u77e5\u8bc6\u3002", "method": "\u91c7\u7528\u57fa\u4e8eConformer\u7684\u6df7\u5408RNN-T/CTC\u6a21\u578b\uff0c\u9996\u5148\u5728\u5370\u5730\u8bed\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u7136\u540e\u9010\u6b65\u5728\u53e6\u5916\u516b\u79cd\u5370\u5ea6\u8bed\u8a00\u4e0a\u8fdb\u884c\u589e\u91cf\u8bad\u7ec3\u3002\u8bc4\u4f30\u4e86\u4e09\u79cd\u4e3b\u8981\u7684\u57fa\u4e8e\u6b63\u5219\u5316\u548c\u84b8\u998f\u7684CL\u7b56\u7565\uff1a\u5f39\u6027\u6743\u91cd\u5de9\u56fa\uff08EWC\uff09\u3001\u8bb0\u5fc6\u611f\u77e5\u7a81\u89e6\uff08MAS\uff09\u548c\u65e0\u9057\u5fd8\u5b66\u4e60\uff08LwF\uff09\u3002", "result": "\u4e0e\u7b80\u5355\u7684\u5fae\u8c03\u76f8\u6bd4\uff0cCL\u5728\u51cf\u8f7b\u9057\u5fd8\u65b9\u9762\u662f\u6709\u6548\u7684\uff0c\u5e76\u4e14\u5728\u5e72\u51c0\u548c\u5608\u6742\u7684\u6570\u636e\u4e0a\u4f7f\u7528RNN-T\u548cCTC\u8def\u5f84\u4ee5\u53ca\u901a\u8fc7\u5411\u540e\u8fc1\u79fb\u8fdb\u884c\u77e5\u8bc6\u4fdd\u7559\u65b9\u9762\u90fd\u8868\u73b0\u826f\u597d\u3002\u63a2\u7d22\u4e86\u4e0d\u540c\u8bad\u7ec3\u5468\u671f\u6570\uff081\u30012\u30015\u548c10\uff09\u7684\u5f71\u54cd\u3002", "conclusion": "\u6301\u7eed\u5b66\u4e60\uff08CL\uff09\u5728\u7f13\u89e3\u9057\u5fd8\u65b9\u9762\u662f\u6709\u6548\u7684\uff0c\u4f7f\u5176\u6210\u4e3a\u5728\u73b0\u5b9e\u7ea6\u675f\u6761\u4ef6\u4e0b\u53ef\u6269\u5c55\u7684\u5370\u5ea6\u591a\u79cd\u8bed\u8a00ASR\u7684\u6709\u524d\u9014\u7684\u65b9\u6cd5\u3002"}}
{"id": "2508.06380", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.06380", "abs": "https://arxiv.org/abs/2508.06380", "authors": ["Arindam Dutta"], "title": "Design and analysis of a set of discrete variable protocols for secure quantum communication", "comment": "PhD thesis. Quantum Communication; Quantum Cryptography; Discrete\n  Variable Protocols for Quantum Cryptography; Bell State; Nash Equilibrium;\n  Quantum Game; Security Analysis", "summary": "The advent of quantum key distribution (QKD) has revolutionized secure\ncommunication by providing unconditional security, unlike classical\ncryptographic methods. However, its effectiveness relies on robust identity\nauthentication, as vulnerabilities in the authentication process can cause a\ncompromise with the security of the entire communication system. Over the past\nthree decades, numerous quantum identity authentication (QIA) protocols have\nbeen proposed. This thesis first presents a chronological review of these\nprotocols, categorizing them based on quantum resources and computational tasks\ninvolved while analyzing their strengths and limitations. Subsequently, by\nrecognizing inherent symmetries present in the existing protocols, we design\nnovel QIA schemes based on secure computational and communication tasks.\nSpecifically, this work introduces a set of new QIA protocols that utilize\ncontrolled secure direct quantum communication. The proposed scheme facilitates\nmutual authentication between two users, Alice and Bob, with assistance from a\nthird party, Charlie, using Bell states. A comprehensive security analysis\ndemonstrates its robustness against impersonation, intercept-resend, and\nfraudulent authentication attacks. The comparative evaluation highlights its\nadvantages over existing schemes. Additionally, this thesis presents two novel\nQKD protocols that eliminate the need for entanglement or ideal single-photon\nsources, making them feasible with commercially available photon sources. These\nprotocols are rigorously proven to be secure against various attacks, including\nintercept-resend and certain collective attacks. Key rate bounds are\nestablished, demonstrating that specific classical pre-processing enhances the\ntolerable error threshold. PHD THESIS", "AI": {"tldr": "\u672c\u8bba\u6587\u56de\u987e\u4e86\u73b0\u6709\u7684\u91cf\u5b50\u8eab\u4efd\u8ba4\u8bc1\uff08QIA\uff09\u534f\u8bae\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u7cfb\u5217\u65b0\u7684\u57fa\u4e8e\u53d7\u63a7\u5b89\u5168\u76f4\u63a5\u91cf\u5b50\u901a\u4fe1\u7684QIA\u534f\u8bae\uff0c\u65e8\u5728\u63d0\u9ad8\u5b89\u5168\u6027\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e24\u79cd\u65e0\u9700\u7ea0\u7f20\u6216\u7406\u60f3\u5355\u5149\u5b50\u6e90\u7684\u65b0\u578b\u91cf\u5b50\u5bc6\u94a5\u5206\u53d1\uff08QKD\uff09\u534f\u8bae\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u5b89\u5168\u6027\uff0c\u540c\u65f6\u63d0\u51fa\u4e86\u6539\u8fdbQKD\u6027\u80fd\u7684\u65b9\u6cd5\u3002", "motivation": "\u4e3a\u4e86\u5e94\u5bf9\u91cf\u5b50\u5bc6\u94a5\u5206\u53d1\uff08QKD\uff09\u4e2d\u8eab\u4efd\u8ba4\u8bc1\u7684\u6f0f\u6d1e\uff0c\u4ee5\u53ca\u5b9e\u73b0\u66f4\u5b9e\u9645\u3001\u66f4\u5b89\u5168\u7684\u901a\u4fe1\u3002", "method": "\u5bf9\u73b0\u6709\u7684\u91cf\u5b50\u8eab\u4efd\u8ba4\u8bc1\uff08QIA\uff09\u534f\u8bae\u8fdb\u884c\u4e86\u56de\u987e\u548c\u5206\u7c7b\uff0c\u7136\u540e\u57fa\u4e8e\u8bc6\u522b\u51fa\u7684\u5bf9\u79f0\u6027\u8bbe\u8ba1\u4e86\u65b0\u7684QIA\u65b9\u6848\uff0c\u5e76\u5229\u7528Bell\u6001\u548c\u53d7\u63a7\u5b89\u5168\u76f4\u63a5\u91cf\u5b50\u901a\u4fe1\u8fdb\u884c\u8eab\u4efd\u8ba4\u8bc1\u3002\u540c\u65f6\uff0c\u5f00\u53d1\u4e86\u4e24\u79cd\u65b0\u7684QKD\u534f\u8bae\uff0c\u5e76\u8fdb\u884c\u4e86\u5b89\u5168\u5206\u6790\u548c\u4e0e\u73b0\u6709\u65b9\u6848\u7684\u6bd4\u8f83\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u5957\u65b0\u7684\u57fa\u4e8e\u53d7\u63a7\u5b89\u5168\u76f4\u63a5\u91cf\u5b50\u901a\u4fe1\u7684QIA\u534f\u8bae\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u5bf9\u591a\u79cd\u653b\u51fb\u7684\u9c81\u68d2\u6027\u3002\u540c\u65f6\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u65e0\u9700\u7ea0\u7f20\u6216\u7406\u60f3\u5355\u5149\u5b50\u6e90\u7684\u65b0\u578bQKD\u534f\u8bae\uff0c\u5e76\u5efa\u7acb\u4e86\u5173\u952e\u901f\u7387\u8fb9\u754c\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u7cfb\u5217\u65b0\u7684\u91cf\u5b50\u8eab\u4efd\u8ba4\u8bc1\uff08QIA\uff09\u534f\u8bae\uff0c\u8fd9\u4e9b\u534f\u8bae\u5229\u7528\u53d7\u63a7\u5b89\u5168\u76f4\u63a5\u91cf\u5b50\u901a\u4fe1\uff0c\u5e76\u901a\u8fc7\u7b2c\u4e09\u65b9\uff08Charlie\uff09\u534f\u52a9Alice\u548cBob\u8fdb\u884c\u76f8\u4e92\u8ba4\u8bc1\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e24\u4e2a\u65b0\u7684\u91cf\u5b50\u5bc6\u94a5\u5206\u53d1\uff08QKD\uff09\u534f\u8bae\uff0c\u65e0\u9700\u7ea0\u7f20\u6216\u7406\u60f3\u5355\u5149\u5b50\u6e90\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u5b89\u5168\u6027\u3002"}}
{"id": "2508.06418", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06418", "abs": "https://arxiv.org/abs/2508.06418", "authors": ["Haoran Shi", "Hongwei Yao", "Shuo Shao", "Shaopeng Jiao", "Ziqi Peng", "Zhan Qin", "Cong Wang"], "title": "Quantifying Conversation Drift in MCP via Latent Polytope", "comment": null, "summary": "The Model Context Protocol (MCP) enhances large language models (LLMs) by\nintegrating external tools, enabling dynamic aggregation of real-time data to\nimprove task execution. However, its non-isolated execution context introduces\ncritical security and privacy risks. In particular, adversarially crafted\ncontent can induce tool poisoning or indirect prompt injection, leading to\nconversation hijacking, misinformation propagation, or data exfiltration.\nExisting defenses, such as rule-based filters or LLM-driven detection, remain\ninadequate due to their reliance on static signatures, computational\ninefficiency, and inability to quantify conversational hijacking. To address\nthese limitations, we propose SecMCP, a secure framework that detects and\nquantifies conversation drift, deviations in latent space trajectories induced\nby adversarial external knowledge. By modeling LLM activation vectors within a\nlatent polytope space, SecMCP identifies anomalous shifts in conversational\ndynamics, enabling proactive detection of hijacking, misleading, and data\nexfiltration. We evaluate SecMCP on three state-of-the-art LLMs (Llama3,\nVicuna, Mistral) across benchmark datasets (MS MARCO, HotpotQA, FinQA),\ndemonstrating robust detection with AUROC scores exceeding 0.915 while\nmaintaining system usability. Our contributions include a systematic\ncategorization of MCP security threats, a novel latent polytope-based\nmethodology for quantifying conversation drift, and empirical validation of\nSecMCP's efficacy.", "AI": {"tldr": "SecMCP\u901a\u8fc7\u5728\u6f5c\u5728\u591a polytope \u7a7a\u95f4\u4e2d\u5bf9LLM\u6fc0\u6d3b\u5411\u91cf\u8fdb\u884c\u5efa\u6a21\uff0c\u8bc6\u522b\u5bf9\u8bdd\u52a8\u6001\u7684\u5f02\u5e38\u53d8\u5316\uff0c\u4ece\u800c\u80fd\u591f\u4e3b\u52a8\u68c0\u6d4b\u5bf9\u8bdd\u52ab\u6301\u3001\u8bef\u5bfc\u548c\u6570\u636e\u6cc4\u9732\u3002", "motivation": "MCP\u7684\u975e\u9694\u79bb\u6267\u884c\u4e0a\u4e0b\u6587\u5f15\u5165\u4e86\u5173\u952e\u7684\u5b89\u5168\u548c\u9690\u79c1\u98ce\u9669\uff0c\u4f8b\u5982\u5de5\u5177\u4e2d\u6bd2\u6216\u95f4\u63a5\u63d0\u793a\u6ce8\u5165\uff0c\u53ef\u80fd\u5bfc\u81f4\u5bf9\u8bdd\u52ab\u6301\u3001\u865a\u5047\u4fe1\u606f\u4f20\u64ad\u6216\u6570\u636e\u6cc4\u9732\u3002\u73b0\u6709\u7684\u9632\u5fa1\u63aa\u65bd\uff08\u5982\u57fa\u4e8e\u89c4\u5219\u7684\u8fc7\u6ee4\u5668\u6216LLM\u9a71\u52a8\u7684\u68c0\u6d4b\uff09\u56e0\u5176\u4f9d\u8d56\u9759\u6001\u7b7e\u540d\u3001\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\u4ee5\u53ca\u65e0\u6cd5\u91cf\u5316\u5bf9\u8bdd\u52ab\u6301\u800c\u663e\u5f97\u4e0d\u8db3\u3002", "method": "SecMCP\u901a\u8fc7\u5728\u6f5c\u5728\u591a\u80de\u4f53\u7a7a\u95f4\u4e2d\u5bf9LLM\u6fc0\u6d3b\u5411\u91cf\u8fdb\u884c\u5efa\u6a21\uff0c\u8bc6\u522b\u5bf9\u8bdd\u52a8\u6001\u7684\u5f02\u5e38\u53d8\u5316\uff0c\u4ece\u800c\u80fd\u591f\u4e3b\u52a8\u68c0\u6d4b\u5bf9\u8bdd\u52ab\u6301\u3001\u8bef\u5bfc\u548c\u6570\u636e\u6cc4\u9732\u3002", "result": "SecMCP\u5728\u4e09\u79cd\u6700\u5148\u8fdb\u7684LLM\uff08Llama3\u3001Vicuna\u3001Mistral\uff09\u548c\u57fa\u51c6\u6570\u636e\u96c6\uff08MS MARCO\u3001HotpotQA\u3001FinQA\uff09\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5c55\u793a\u4e86\u8d85\u8fc70.915\u7684AUROC\u5f97\u5206\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u7cfb\u7edf\u7684\u53ef\u7528\u6027\u3002", "conclusion": "SecMCP\u662f\u4e00\u4e2a\u5b89\u5168\u7684\u6846\u67b6\uff0c\u80fd\u591f\u68c0\u6d4b\u548c\u91cf\u5316\u7531\u5bf9\u6297\u6027\u5916\u90e8\u77e5\u8bc6\u5f15\u8d77\u7684\u5bf9\u8bdd\u6f02\u79fb\u3001\u6f5c\u5728\u7a7a\u95f4\u8f68\u8ff9\u7684\u504f\u5dee\u3002\u5b83\u901a\u8fc7\u5728\u6f5c\u5728\u591a\u80de\u4f53\u7a7a\u95f4\u4e2d\u5bf9LLM\u6fc0\u6d3b\u5411\u91cf\u8fdb\u884c\u5efa\u6a21\uff0c\u8bc6\u522b\u5bf9\u8bdd\u52a8\u6001\u7684\u5f02\u5e38\u53d8\u5316\uff0c\u4ece\u800c\u80fd\u591f\u4e3b\u52a8\u68c0\u6d4b\u5bf9\u8bdd\u52ab\u6301\u3001\u8bef\u5bfc\u548c\u6570\u636e\u6cc4\u9732\u3002SecMCP\u5728\u4e09\u79cd\u6700\u5148\u8fdb\u7684LLM\uff08Llama3\u3001Vicuna\u3001Mistral\uff09\u548c\u57fa\u51c6\u6570\u636e\u96c6\uff08MS MARCO\u3001HotpotQA\u3001FinQA\uff09\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5c55\u793a\u4e86\u8d85\u8fc70.915\u7684AUROC\u5f97\u5206\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u7cfb\u7edf\u7684\u53ef\u7528\u6027\u3002"}}
{"id": "2508.06058", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06058", "abs": "https://arxiv.org/abs/2508.06058", "authors": ["Shiyang Zhou", "Haijin Zeng", "Yunfan Lu", "Yongyong Chen", "Jie Liu", "Jingyong Su"], "title": "Lightweight Quad Bayer HybridEVS Demosaicing via State Space Augmented Cross-Attention", "comment": null, "summary": "Event cameras like the Hybrid Event-based Vision Sensor (HybridEVS) camera\ncapture brightness changes as asynchronous \"events\" instead of frames, offering\nadvanced application on mobile photography. However, challenges arise from\ncombining a Quad Bayer Color Filter Array (CFA) sensor with event pixels\nlacking color information, resulting in aliasing and artifacts on the\ndemosaicing process before downstream application. Current methods struggle to\naddress these issues, especially on resource-limited mobile devices. In\nresponse, we introduce \\textbf{TSANet}, a lightweight \\textbf{T}wo-stage\nnetwork via \\textbf{S}tate space augmented cross-\\textbf{A}ttention, which can\nhandle event pixels inpainting and demosaicing separately, leveraging the\nbenefits of dividing complex tasks into manageable subtasks. Furthermore, we\nintroduce a lightweight Cross-Swin State Block that uniquely utilizes\npositional prior for demosaicing and enhances global dependencies through the\nstate space model with linear complexity. In summary, TSANet demonstrates\nexcellent demosaicing performance on both simulated and real data of HybridEVS\nwhile maintaining a lightweight model, averaging better results than the\nprevious state-of-the-art method DemosaicFormer across seven diverse datasets\nin both PSNR and SSIM, while respectively reducing parameter and computation\ncosts by $1.86\\times$ and $3.29\\times$. Our approach presents new possibilities\nfor efficient image demosaicing on mobile devices. Code is available in the\nsupplementary materials.", "AI": {"tldr": "TSANet\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u4e24\u9636\u6bb5\u7f51\u7edc\uff0c\u901a\u8fc7\u72b6\u6001\u7a7a\u95f4\u589e\u5f3a\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4e8b\u4ef6\u76f8\u673a\u8272\u5f69\u8fd8\u539f\u4e2d\u7684\u6df7\u53e0\u548c\u4f2a\u5f71\u95ee\u9898\uff0c\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\uff0c\u7279\u522b\u9002\u5408\u79fb\u52a8\u8bbe\u5907\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\uff08\u5982HybridEVS\uff09\u867d\u7136\u5728\u6355\u6349\u4eae\u5ea6\u53d8\u5316\u65b9\u9762\u6709\u4f18\u52bf\uff0c\u4f46\u5728\u5c06Quad Bayer\u5f69\u8272\u6ee4\u5149\u9635\u5217\u4f20\u611f\u5668\u4e0e\u7f3a\u4e4f\u989c\u8272\u4fe1\u606f\u7684\u4e8b\u4ef6\u50cf\u7d20\u7ed3\u5408\u65f6\uff0c\u4f1a\u9047\u5230\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u8d44\u6e90\u53d7\u9650\u7684\u79fb\u52a8\u8bbe\u5907\u4e0a\uff0c\u8fd9\u4f1a\u5bfc\u81f4\u53bb\u9a6c\u8d5b\u514b\u8fc7\u7a0b\u4e2d\u7684\u6df7\u53e0\u548c\u4f2a\u5f71\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTSANet\u7684\u8f7b\u91cf\u7ea7\u4e24\u9636\u6bb5\u7f51\u7edc\uff0c\u8be5\u7f51\u7edc\u91c7\u7528\u72b6\u6001\u7a7a\u95f4\u589e\u5f3a\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\uff0c\u80fd\u591f\u5206\u522b\u5904\u7406\u4e8b\u4ef6\u50cf\u7d20\u7684\u4fee\u590d\u548c\u8272\u5f69\u8fd8\u539f\u4efb\u52a1\u3002\u6b64\u5916\uff0c\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u8de8Swin\u72b6\u6001\u5757\uff0c\u5229\u7528\u4f4d\u7f6e\u5148\u9a8c\u8fdb\u884c\u8272\u5f69\u8fd8\u539f\uff0c\u5e76\u901a\u8fc7\u5177\u6709\u7ebf\u6027\u590d\u6742\u5ea6\u7684\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u589e\u5f3a\u5168\u5c40\u4f9d\u8d56\u6027\u3002", "result": "TSANet\u5728\u6a21\u62df\u548c\u771f\u5b9eHybridEVS\u6570\u636e\u4e0a\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u8272\u5f69\u8fd8\u539f\u6027\u80fd\uff0c\u5e76\u4e14\u6a21\u578b\u8f7b\u91cf\u5316\uff0c\u5728\u4e03\u4e2a\u4e0d\u540c\u7684\u6570\u636e\u96c6\u4e0a\uff0c\u5176PSNR\u548cSSIM\u6307\u6807\u5747\u4f18\u4e8e\u5148\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5DemosaicFormer\uff0c\u540c\u65f6\u53c2\u6570\u91cf\u548c\u8ba1\u7b97\u6210\u672c\u5206\u522b\u964d\u4f4e\u4e861.86\u500d\u548c3.29\u500d\u3002", "conclusion": "TSANet\u5728\u6a21\u62df\u548c\u771f\u5b9eHybridEVS\u6570\u636e\u4e0a\u90fd\u5c55\u793a\u4e86\u4f18\u8d8a\u7684\u8272\u5f69\u8fd8\u539f\u6027\u80fd\uff0c\u5e76\u4e14\u6a21\u578b\u8f7b\u91cf\u5316\uff0c\u5728\u4e03\u4e2a\u4e0d\u540c\u7684\u6570\u636e\u96c6\u4e0a\uff0c\u5176PSNR\u548cSSIM\u6307\u6807\u5747\u4f18\u4e8e\u5148\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5DemosaicFormer\uff0c\u540c\u65f6\u53c2\u6570\u91cf\u548c\u8ba1\u7b97\u6210\u672c\u5206\u522b\u964d\u4f4e\u4e861.86\u500d\u548c3.29\u500d\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u4e3a\u79fb\u52a8\u8bbe\u5907\u4e0a\u7684\u9ad8\u6548\u56fe\u50cf\u8272\u5f69\u8fd8\u539f\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2508.06292", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06292", "abs": "https://arxiv.org/abs/2508.06292", "authors": ["Sanja Karilanova", "Subhrakanti Dey", "Ay\u00e7a \u00d6z\u00e7elikkale"], "title": "Low-Bit Data Processing Using Multiple-Output Spiking Neurons with Non-linear Reset Feedback", "comment": "15 pages, 7 Tables, 6 Figures", "summary": "Neuromorphic computing is an emerging technology enabling low-latency and\nenergy-efficient signal processing. A key algorithmic tool in neuromorphic\ncomputing is spiking neural networks (SNNs). SNNs are biologically inspired\nneural networks which utilize stateful neurons, and provide low-bit data\nprocessing by encoding and decoding information using spikes. Similar to SNNs,\ndeep state-space models (SSMs) utilize stateful building blocks. However, deep\nSSMs, which recently achieved competitive performance in various temporal\nmodeling tasks, are typically designed with high-precision activation functions\nand no reset mechanisms. To bridge the gains offered by SNNs and the recent\ndeep SSM models, we propose a novel multiple-output spiking neuron model that\ncombines a linear, general SSM state transition with a non-linear feedback\nmechanism through reset. Compared to the existing neuron models for SNNs, our\nproposed model clearly conceptualizes the differences between the spiking\nfunction, the reset condition and the reset action. The experimental results on\nvarious tasks, i.e., a keyword spotting task, an event-based vision task and a\nsequential pattern recognition task, show that our proposed model achieves\nperformance comparable to existing benchmarks in the SNN literature. Our\nresults illustrate how the proposed reset mechanism can overcome instability\nand enable learning even when the linear part of neuron dynamics is unstable,\nallowing us to go beyond the strictly enforced stability of linear dynamics in\nrecent deep SSM models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8109\u51b2\u795e\u7ecf\u5143\u6a21\u578b\uff0c\u7ed3\u5408\u4e86SSM\u548cSNN\u7684\u4f18\u70b9\uff0c\u5728\u5404\u79cd\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u4e0e\u73b0\u6709\u57fa\u51c6\u76f8\u5f53\u7684\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u7ed3\u5408SNN\u7684\u4f18\u52bf\u548c\u8fd1\u671f\u6df1\u5ea6SSM\u6a21\u578b\u7684\u4f18\u52bf\uff0c\u7814\u7a76\u4e86SNN\u548c\u6df1\u5ea6SSM\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u8f93\u51fa\u8109\u51b2\u795e\u7ecf\u5143\u6a21\u578b\uff0c\u7ed3\u5408\u4e86\u7ebf\u6027\u7684\u3001\u4e00\u822c\u7684SSM\u72b6\u6001\u8f6c\u79fb\u548c\u901a\u8fc7\u91cd\u7f6e\u4ea7\u751f\u7684\u975e\u7ebf\u6027\u53cd\u9988\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5728\u5173\u952e\u5b57\u8bc6\u522b\u3001\u4e8b\u4ef6\u89c6\u89c9\u548c\u5e8f\u5217\u6a21\u5f0f\u8bc6\u522b\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u4e0e\u73b0\u6709SNN\u6587\u732e\u57fa\u51c6\u76f8\u5f53\u7684\u6027\u80fd\u3002", "conclusion": "\u4e0e\u73b0\u6709\u7684SNN\u57fa\u51c6\u76f8\u6bd4\uff0c\u8be5\u6a21\u578b\u5728\u5404\u9879\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u53ef\u6bd4\u7684\u6027\u80fd\u3002\u6b64\u5916\uff0c\u6240\u63d0\u51fa\u7684\u91cd\u7f6e\u673a\u5236\u514b\u670d\u4e86\u4e0d\u7a33\u5b9a\u6027\uff0c\u5e76\u5728\u7ebf\u6027\u90e8\u5206\u4e0d\u7a33\u5b9a\u65f6\u4e5f\u80fd\u591f\u5b9e\u73b0\u5b66\u4e60\uff0c\u4ece\u800c\u8d85\u8d8a\u4e86\u8fd1\u671f\u6df1\u5ea6SSM\u6a21\u578b\u4e2d\u4e25\u683c\u6267\u884c\u7684\u7ebf\u6027\u52a8\u529b\u5b66\u7a33\u5b9a\u6027\u3002"}}
{"id": "2508.06396", "categories": ["quant-ph", "math.DS", "math.OA", "math.PR", "46L53, 81S22, 46L57"], "pdf": "https://arxiv.org/pdf/2508.06396", "abs": "https://arxiv.org/abs/2508.06396", "authors": ["Ameur Dhahri", "Franco Fagnola", "Federico Girotti", "Hyun Jae Yoo"], "title": "Quasi-stationary normal states for quantum Markov semigroups", "comment": "16 pages, 0 figures. Comments and suggestions are welcome", "summary": "We introduce the notion of Quasi-Stationary State (QSS) in the context of\nquantum Markov semigroups that generalizes the one of quasi-stationary\ndistribution in the case of classical Markov chains. We provide an operational\ninterpretation of QSSs using the theory of direct and indirect quantum\nmeasurements. Moreover, we prove that there is a connection between QSSs and\nspectral properties of the quantum Markov semigroup. Finally, we discuss some\nexamples which, despite their simplicity, already show interesting features.", "AI": {"tldr": "\u5728\u91cf\u5b50\u9a6c\u5c14\u53ef\u592b\u534a\u7fa4\u7684\u80cc\u666f\u4e0b\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u51c6\u7a33\u6001\uff08QSS\uff09\u7684\u6982\u5ff5\uff0c\u5e76\u63d0\u4f9b\u4e86\u5176\u64cd\u4f5c\u89e3\u91ca\uff0c\u540c\u65f6\u8bc1\u660e\u4e86QSS\u4e0e\u8be5\u534a\u7fa4\u7684\u8c31\u6027\u8d28\u4e4b\u95f4\u7684\u8054\u7cfb\u3002", "motivation": "\u5728\u91cf\u5b50\u9a6c\u5c14\u53ef\u592b\u534a\u7fa4\u7684\u80cc\u666f\u4e0b\u5f15\u5165\u4e86\u51c6\u7a33\u6001\uff08QSS\uff09\u7684\u6982\u5ff5\uff0c\u6cdb\u5316\u4e86\u7ecf\u5178\u9a6c\u5c14\u53ef\u592b\u94fe\u4e2d\u51c6\u7a33\u6001\u5206\u5e03\u7684\u6982\u5ff5", "method": "\u901a\u8fc7\u76f4\u63a5\u548c\u95f4\u63a5\u91cf\u5b50\u6d4b\u91cf\u7406\u8bba\u63d0\u4f9bQSS\u7684\u64cd\u4f5c\u89e3\u91ca", "result": "\u5df2\u8bc1\u660eQSS\u4e0e\u91cf\u5b50\u9a6c\u5c14\u53ef\u592b\u534a\u7fa4\u7684\u8c31\u6027\u8d28\u4e4b\u95f4\u5b58\u5728\u8054\u7cfb\uff0c\u5e76\u901a\u8fc7\u4e00\u4e9b\u793a\u4f8b\u8fdb\u884c\u4e86\u8bf4\u660e", "conclusion": "QSS\u4e0e\u91cf\u5b50\u9a6c\u5c14\u53ef\u592b\u7684\u8c31\u6027\u8d28\u5b58\u5728\u8054\u7cfb"}}
{"id": "2508.06063", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06063", "abs": "https://arxiv.org/abs/2508.06063", "authors": ["Chao Hao", "Zitong Yu", "Xin Liu", "Yuhao Wang", "Weicheng Xie", "Jingang Shi", "Huanjing Yue", "Jingyu Yang"], "title": "Distribution-Specific Learning for Joint Salient and Camouflaged Object Detection", "comment": null, "summary": "Salient object detection (SOD) and camouflaged object detection (COD) are two\nclosely related but distinct computer vision tasks. Although both are\nclass-agnostic segmentation tasks that map from RGB space to binary space, the\nformer aims to identify the most salient objects in the image, while the latter\nfocuses on detecting perfectly camouflaged objects that blend into the\nbackground in the image. These two tasks exhibit strong contradictory\nattributes. Previous works have mostly believed that joint learning of these\ntwo tasks would confuse the network, reducing its performance on both tasks.\nHowever, here we present an opposite perspective: with the correct approach to\nlearning, the network can simultaneously possess the capability to find both\nsalient and camouflaged objects, allowing both tasks to benefit from joint\nlearning. We propose SCJoint, a joint learning scheme for SOD and COD tasks,\nassuming that the decoding processes of SOD and COD have different distribution\ncharacteristics. The key to our method is to learn the respective means and\nvariances of the decoding processes for both tasks by inserting a minimal\namount of task-specific learnable parameters within a fully shared network\nstructure, thereby decoupling the contradictory attributes of the two tasks at\na minimal cost. Furthermore, we propose a saliency-based sampling strategy\n(SBSS) to sample the training set of the SOD task to balance the training set\nsizes of the two tasks. In addition, SBSS improves the training set quality and\nshortens the training time. Based on the proposed SCJoint and SBSS, we train a\npowerful generalist network, named JoNet, which has the ability to\nsimultaneously capture both ``salient\" and ``camouflaged\". Extensive\nexperiments demonstrate the competitive performance and effectiveness of our\nproposed method. The code is available at https://github.com/linuxsino/JoNet.", "AI": {"tldr": "This paper presents a joint learning scheme (SCJoint) and a sampling strategy (SBSS) to enable a single network (JoNet) to effectively perform both Salient Object Detection (SOD) and Camouflaged Object Detection (COD), tasks previously thought to be contradictory. Experiments show this approach achieves competitive performance.", "motivation": "The paper challenges the common belief that joint learning of Salient Object Detection (SOD) and Camouflaged Object Detection (COD) tasks would confuse the network. Instead, it proposes that with the correct approach, a network can simultaneously learn both tasks, benefiting from joint learning.", "method": "The paper proposes SCJoint, a joint learning scheme for SOD and COD tasks, which learns the respective means and variances of the decoding processes for both tasks by inserting minimal task-specific learnable parameters within a fully shared network structure. Additionally, a saliency-based sampling strategy (SBSS) is proposed to balance the training set sizes and improve the training quality and time for the SOD task.", "result": "Extensive experiments demonstrate the competitive performance and effectiveness of the proposed SCJoint and SBSS methods in training a generalist network (JoNet) capable of capturing both salient and camouflaged objects.", "conclusion": "The proposed SCJoint and SBSS methods train a powerful generalist network, JoNet, which demonstrates competitive performance and effectiveness in simultaneously detecting salient and camouflaged objects."}}
{"id": "2508.06410", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.06410", "abs": "https://arxiv.org/abs/2508.06410", "authors": ["Sean Borneman"], "title": "Quantum Annealing for the Set Splitting Problem", "comment": null, "summary": "I present a novel use of quantum annealing to solve the Set Splitting Problem\nusing (QUBO) problem formulation. The contribution of the work is in\nformulating penalty functions that ensure the ground state of the QUBO\nHamiltonian corresponds to valid solutions that split the input subsets. This\napproach scales linearly in terms of the number of logical qubits relative to\nproblem size. Empirical tests of the proposed solution show convergence to\nglobally optimal solutions, with high accuracy rates over repeated trials.\nHardware limitations of current quantum annealers lead to an exponential rise\nin required physical qubits, versus the theoretical linear increase, although\nthis can improve with future developments. Further work is needed to enhance\nformulation robustness, reduce qubit requirements for embedded problems, and to\nconduct more extensive bench-marking. Quantum solutions to the Set-Splitting\nproblem lead to reduced time complexity versus classical solutions, and may\naccelerate research in biology, cybersecurity, and other domains.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u91cf\u5b50\u9000\u706b\u548cQUBO\u516c\u5f0f\u89e3\u51b3\u96c6\u5408\u5212\u5206\u95ee\u9898\u7684\u65b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u903b\u8f91\u91cf\u5b50\u6bd4\u7279\u6570\u91cf\u4e0a\u5b9e\u73b0\u4e86\u7ebf\u6027\u6269\u5c55\uff0c\u5e76\u80fd\u5728\u5b9e\u9a8c\u4e2d\u6536\u655b\u5230\u6700\u4f18\u89e3\uff0c\u4f46\u53d7\u9650\u4e8e\u5f53\u524d\u786c\u4ef6\u7684\u7269\u7406\u91cf\u5b50\u6bd4\u7279\u9700\u6c42\u3002", "motivation": "\u4e3a\u4e86\u5bfb\u627e\u89e3\u51b3\u96c6\u5408\u5212\u5206\u95ee\u9898\u7684\u66f4\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u5e76\u63a2\u7d22\u91cf\u5b50\u9000\u706b\u5728\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528QUBO\uff08\u4e8c\u6b21\u65e0\u7ea6\u675f\u4e8c\u5143\u53d8\u91cf\uff09\u95ee\u9898\u516c\u5f0f\u7684\u91cf\u5b50\u9000\u706b\u65b0\u65b9\u6cd5\u6765\u89e3\u51b3\u96c6\u5408\u5212\u5206\u95ee\u9898\uff0c\u91cd\u70b9\u662f\u8bbe\u8ba1\u80fd\u591f\u786e\u4fddQUBO\u54c8\u5bc6\u987f\u91cf\u7684\u57fa\u6001\u5bf9\u5e94\u4e8e\u6709\u6548\u7684\u96c6\u5408\u5212\u5206\u89e3\u7684\u60e9\u7f5a\u51fd\u6570\u3002", "result": "\u5b9e\u9a8c\u6d4b\u8bd5\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u4ee5\u9ad8\u51c6\u786e\u7387\u6536\u655b\u5230\u5168\u5c40\u6700\u4f18\u89e3\uff0c\u5e76\u4e14\u968f\u7740\u95ee\u9898\u89c4\u6a21\u7684\u589e\u52a0\uff0c\u6240\u9700\u903b\u8f91\u91cf\u5b50\u6bd4\u7279\u7684\u6570\u91cf\u5448\u7ebf\u6027\u589e\u957f\u3002\u7136\u800c\uff0c\u76ee\u524d\u7684\u91cf\u5b50\u9000\u706b\u786c\u4ef6\u9650\u5236\u5bfc\u81f4\u7269\u7406\u91cf\u5b50\u6bd4\u7279\u7684\u9700\u6c42\u5448\u6307\u6570\u7ea7\u589e\u957f\u3002", "conclusion": "\u91cf\u5b50\u9000\u706b\u65b9\u6cd5\u4e3a\u96c6\u5408\u5212\u5206\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u4e0e\u7ecf\u5178\u65b9\u6cd5\u76f8\u6bd4\u53ef\u5ffd\u7565\u7684time complexity\uff0c\u5e76\u53ef\u80fd\u52a0\u901f\u751f\u7269\u5b66\u3001\u7f51\u7edc\u5b89\u5168\u548c\u5176\u4ed6\u9886\u57df\u7684\u7814\u7a76\u3002"}}
{"id": "2508.06435", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06435", "abs": "https://arxiv.org/abs/2508.06435", "authors": ["Andrea Nasuto", "Stefano Maria Iacus", "Francisco Rowe", "Devika Jain"], "title": "Learning the Topic, Not the Language: How LLMs Classify Online Immigration Discourse Across Languages", "comment": null, "summary": "Large language models (LLMs) are transforming social-science research by\nenabling scalable, precise analysis. Their adaptability raises the question of\nwhether knowledge acquired through fine-tuning in a few languages can transfer\nto unseen languages that only appeared during pre-training. To examine this, we\nfine-tune lightweight LLaMA 3.2-3B models on monolingual, bilingual, or\nmultilingual data sets to classify immigration-related tweets from X/Twitter\nacross 13 languages, a domain characterised by polarised, culturally specific\ndiscourse. We evaluate whether minimal language-specific fine-tuning enables\ncross-lingual topic detection and whether adding targeted languages corrects\npre-training biases. Results show that LLMs fine-tuned in one or two languages\ncan reliably classify immigration-related content in unseen languages. However,\nidentifying whether a tweet expresses a pro- or anti-immigration stance\nbenefits from multilingual fine-tuning. Pre-training bias favours dominant\nlanguages, but even minimal exposure to under-represented languages during\nfine-tuning (as little as $9.62\\times10^{-11}$ of the original pre-training\ntoken volume) yields significant gains. These findings challenge the assumption\nthat cross-lingual mastery requires extensive multilingual training: limited\nlanguage coverage suffices for topic-level generalisation, and structural\nbiases can be corrected with lightweight interventions. By releasing\n4-bit-quantised, LoRA fine-tuned models, we provide an open-source,\nreproducible alternative to proprietary LLMs that delivers 35 times faster\ninference at just 0.00000989% of the dollar cost of the OpenAI GPT-4o model,\nenabling scalable, inclusive research.", "AI": {"tldr": "LLMs can learn cross-lingual classification with minimal fine-tuning, even correcting biases, making research more scalable and inclusive. Open-source models offer a cost-effective alternative.", "motivation": "To examine whether knowledge acquired through fine-tuning in a few languages can transfer to unseen languages that only appeared during pre-training, especially in the context of social-science research using LLMs.", "method": "Fine-tuned lightweight LLaMA 3.2-3B models on monolingual, bilingual, or multilingual datasets to classify immigration-related tweets across 13 languages.", "result": "LLMs fine-tuned in one or two languages can reliably classify immigration-related content in unseen languages. Multilingual fine-tuning improves pro/anti-immigration stance detection. Minimal exposure to under-represented languages during fine-tuning significantly reduces pre-training bias.", "conclusion": "LLMs fine-tuned with limited language data can generalize to unseen languages for topic detection, and lightweight interventions can correct pre-training biases. Multilingual fine-tuning improves stance detection."}}
{"id": "2508.06072", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06072", "abs": "https://arxiv.org/abs/2508.06072", "authors": ["Zijian Chen", "Lirong Deng", "Zhengyu Chen", "Kaiwei Zhang", "Qi Jia", "Yuan Tian", "Yucheng Zhu", "Guangtao Zhai"], "title": "Can Large Models Fool the Eye? A New Turing Test for Biological Animation", "comment": "24 pages, 10 figures", "summary": "Evaluating the abilities of large models and manifesting their gaps are\nchallenging. Current benchmarks adopt either ground-truth-based score-form\nevaluation on static datasets or indistinct textual chatbot-style human\npreferences collection, which may not provide users with immediate, intuitive,\nand perceptible feedback on performance differences. In this paper, we\nintroduce BioMotion Arena, a novel framework for evaluating large language\nmodels (LLMs) and multimodal large language models (MLLMs) via visual\nanimation. Our methodology draws inspiration from the inherent visual\nperception of motion patterns characteristic of living organisms that utilizes\npoint-light source imaging to amplify the performance discrepancies between\nmodels. Specifically, we employ a pairwise comparison evaluation and collect\nmore than 45k votes for 53 mainstream LLMs and MLLMs on 90 biological motion\nvariants. Data analyses show that the crowd-sourced human votes are in good\nagreement with those of expert raters, demonstrating the superiority of our\nBioMotion Arena in offering discriminative feedback. We also find that over\n90\\% of evaluated models, including the cutting-edge open-source InternVL3 and\nproprietary Claude-4 series, fail to produce fundamental humanoid point-light\ngroups, much less smooth and biologically plausible motions. This enables\nBioMotion Arena to serve as a challenging benchmark for performance\nvisualization and a flexible evaluation framework without restrictions on\nground-truth.", "AI": {"tldr": "BioMotion Arena\u901a\u8fc7\u89c6\u89c9\u52a8\u753b\u8bc4\u4f30LLM\u548cMLLM\uff0c\u53d1\u73b0\u5927\u591a\u6570\u6a21\u578b\u5728\u751f\u6210\u751f\u7269\u8fd0\u52a8\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "motivation": "\u5f53\u524d\u8bc4\u4f30LLM\u548cMLLM\u7684\u65b9\u6cd5\u5728\u63d0\u4f9b\u5373\u65f6\u3001\u76f4\u89c2\u548c\u53ef\u611f\u77e5\u7684\u6027\u80fd\u5dee\u5f02\u53cd\u9988\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002\u73b0\u6709\u7684\u57fa\u51c6\u6d4b\u8bd5\u65b9\u6cd5\u8981\u4e48\u57fa\u4e8e\u9759\u6001\u6570\u636e\u96c6\u7684\u5730\u9762\u771f\u5b9e\u8bc4\u5206\uff0c\u8981\u4e48\u662f\u6a21\u7cca\u7684\u6587\u672c\u5f0f\u804a\u5929\u673a\u5668\u4eba\u98ce\u683c\u7684\u4eba\u7c7b\u504f\u597d\u6536\u96c6\u3002", "method": "\u4f7f\u7528\u70b9\u5149\u6e90\u6210\u50cf\u6280\u672f\uff0c\u901a\u8fc7\u89c6\u89c9\u52a8\u753b\u548c\u6210\u5bf9\u6bd4\u8f83\u6765\u8bc4\u4f30LLM\u548cMLLM\u3002\u6536\u96c6\u4e86\u8d85\u8fc745,000\u4e2a\u6295\u7968\uff0c\u6db5\u76d690\u79cd\u751f\u7269\u8fd0\u52a8\u53d8\u4f53\uff0c\u8bc4\u4f30\u4e8653\u4e2a\u4e3b\u6d41LLM\u548cMLLM\u3002", "result": "\u8d85\u8fc790%\u7684\u88ab\u8bc4\u4f30\u6a21\u578b\uff0c\u5305\u62ecInternVL3\u548cClaude-4\u7cfb\u5217\uff0c\u672a\u80fd\u751f\u6210\u57fa\u672c\u7684\u4eba\u5f62\u70b9\u5149\u5206\u7ec4\uff0c\u66f4\u4e0d\u7528\u8bf4\u5e73\u6ed1\u4e14\u5177\u6709\u751f\u7269\u5b66\u4e0a\u53ef\u884c\u7684\u8fd0\u52a8\u3002\u4f17\u5305\u7684\u4eba\u7c7b\u6295\u7968\u4e0e\u4e13\u5bb6\u8bc4\u5206\u8005\u7ed3\u679c\u9ad8\u5ea6\u4e00\u81f4\uff0c\u8bc1\u660e\u4e86BioMotion Arena\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "BioMotion Arena\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9\u52a8\u753b\u8bc4\u4f30LLM\u548cMLLM\uff0c\u901a\u8fc7\u70b9\u5149\u6210\u50cf\u653e\u5927\u4e86\u6a21\u578b\u95f4\u7684\u6027\u80fd\u5dee\u5f02\u3002\u8be5\u6846\u67b6\u63d0\u4f9b\u4e86\u533a\u5206\u6027\u53cd\u9988\uff0c\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u6027\u80fd\u53ef\u89c6\u5316\u57fa\u51c6\u548c\u7075\u6d3b\u7684\u8bc4\u4f30\u6846\u67b6\u3002"}}
{"id": "2508.06424", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.06424", "abs": "https://arxiv.org/abs/2508.06424", "authors": ["Vivekanand Tiwari", "Zhaojin Liu", "Hao-Cheng Weng", "Krishna C Balram", "John G Rarity", "Soumen Mandal", "Oliver A Williams", "Gavin W Morley", "Joe A Smith"], "title": "Single photon emission from lithographically-positioned engineered nanodiamonds for cryogenic applications", "comment": "4 pages, 3 figures", "summary": "Nitrogen-vacancy centres in nanodiamonds (NDs) provide a promising resource\nfor quantum photonic systems. However, developing a technology beyond\nproof-of-principle physics requires optimally engineering its component parts.\nIn this work, we present a hybrid materials platform by photolithographically\npositioning ball-milled isotopically-enriched NDs on broadband metal\nreflectors. The structure enhances the photonic collection efficiency, enabling\ncryogenic characterisation despite the limited numerical aperture imposed by\nour cryostat. Our device, with SiO$_2$ above a silver reflector, allows us to\nperform spectroscopic characterisation at 16 K and measure autocorrelation\nfunctions confirming single-photon emission (g$^2$(0)<0.5). Through comparative\nstudies of similar hybrid device configurations, we can move towards optimally\nengineered techniques for building and analysing quantum emitters in\nwafer-scale photonic environments.", "AI": {"tldr": "\u901a\u8fc7\u5c06NV\u4e2d\u5fc3\u91d1\u521a\u77f3\u7eb3\u7c73\u9897\u7c92\u653e\u7f6e\u5728\u91d1\u5c5e\u53cd\u5c04\u5668\u4e0a\uff0c\u63d0\u9ad8\u5149\u5b50\u6536\u96c6\u6548\u7387\uff0c\u5b9e\u73b0\u5728\u4f4e\u6e29\u4e0b\u8fdb\u884c\u5355\u5149\u5b50\u53d1\u5c04\u7684\u6d4b\u91cf\u3002", "motivation": "\u4e3a\u4e86\u5c06\u6c2e-\u7a7a\u4f4d\uff08NV\uff09\u4e2d\u5fc3\u6280\u672f\u4ece\u539f\u7406\u8bc1\u660e\u53d1\u5c55\u5230\u8d85\u8d8a\uff0c\u9700\u8981\u5bf9\u5176\u7ec4\u4ef6\u8fdb\u884c\u4f18\u5316\u5de5\u7a0b\u3002", "method": "\u901a\u8fc7\u5149\u523b\u6280\u672f\u5c06\u7403\u78e8\u7684\u3001\u540c\u4f4d\u7d20\u5bcc\u96c6\u7684\u91d1\u521a\u77f3\u7eb3\u7c73\u9897\u7c92\uff08NDs\uff09\u653e\u7f6e\u5728\u5bbd\u5e26\u91d1\u5c5e\u53cd\u5c04\u5668\u4e0a\uff0c\u5e76\u5728\u6b64\u4e4b\u4e0a\u6dfb\u52a0SiO2\u3002", "result": "\u5668\u4ef6\u80fd\u591f\u572816 K\u4e0b\u8fdb\u884c\u5149\u8c31\u8868\u5f81\uff0c\u5e76\u6d4b\u91cf\u81ea\u76f8\u5173\u51fd\u6570\uff0c\u8bc1\u5b9e\u4e86\u5355\u5149\u5b50\u53d1\u5c04\uff08g$^2$(0)<0.5\uff09\u3002", "conclusion": "\u901a\u8fc7\u6bd4\u8f83\u7814\u7a76\u7c7b\u4f3c\u7684\u6df7\u5408\u5668\u4ef6\u914d\u7f6e\uff0c\u6211\u4eec\u53ef\u4ee5\u671d\u7740\u5728\u6676\u5706\u7ea7\u5149\u5b50\u73af\u5883\u4e2d\u6784\u5efa\u548c\u5206\u6790\u91cf\u5b50\u53d1\u5c04\u5668\u7684\u6700\u4f73\u5de5\u7a0b\u6280\u672f\u8fc8\u8fdb\u3002"}}
{"id": "2508.06445", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06445", "abs": "https://arxiv.org/abs/2508.06445", "authors": ["Abolfazl Ansari", "Delvin Ce Zhang", "Nafis Irtiza Tripto", "Dongwon Lee"], "title": "Echoes of Automation: The Increasing Use of LLMs in Newsmaking", "comment": "To appear in 18th International Conference on Social Computing,\n  Behavioral-Cultural Modeling, & Prediction and Behavior Representation in\n  Modeling and Simulation, and to be published in the Springer LNCS series", "summary": "The rapid rise of Generative AI (GenAI), particularly LLMs, poses concerns\nfor journalistic integrity and authorship. This study examines AI-generated\ncontent across over 40,000 news articles from major, local, and college news\nmedia, in various media formats. Using three advanced AI-text detectors (e.g.,\nBinoculars, Fast-Detect GPT, and GPTZero), we find substantial increase of\nGenAI use in recent years, especially in local and college news. Sentence-level\nanalysis reveals LLMs are often used in the introduction of news, while\nconclusions usually written manually. Linguistic analysis shows GenAI boosts\nword richness and readability but lowers formality, leading to more uniform\nwriting styles, particularly in local media.", "AI": {"tldr": "\u672c\u7814\u7a76\u5206\u6790\u4e8640,000\u591a\u7bc7\u65b0\u95fb\u6587\u7ae0\uff0c\u53d1\u73b0\u751f\u6210\u5f0fAI\u5728\u65b0\u95fb\u4e1a\u4e2d\u7684\u4f7f\u7528\u5448\u4e0a\u5347\u8d8b\u52bf\uff0c\u5c24\u5176\u662f\u5728\u5730\u65b9\u548c\u5927\u5b66\u65b0\u95fb\u4e2d\u3002\u751f\u6210\u5f0fAI\u4f1a\u63d0\u9ad8\u8bcd\u6c47\u4e30\u5bcc\u5ea6\u548c\u53ef\u8bfb\u6027\uff0c\u4f46\u4f1a\u964d\u4f4e\u6b63\u5f0f\u6027\uff0c\u5bfc\u81f4\u5199\u4f5c\u98ce\u683c\u8d8b\u4e8e\u7edf\u4e00\u3002", "motivation": "\u751f\u6210\u5f0fAI\uff08GenAI\uff09\u7684\u5feb\u901f\u5d1b\u8d77\uff0c\u7279\u522b\u662f\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u5f15\u53d1\u4e86\u4eba\u4eec\u5bf9\u65b0\u95fb\u4e1a\u5b8c\u6574\u6027\u548c\u4f5c\u8005\u8eab\u4efd\u7684\u62c5\u5fe7\u3002", "method": "\u672c\u7814\u7a76\u4f7f\u7528\u4e09\u79cd\u5148\u8fdb\u7684AI\u6587\u672c\u68c0\u6d4b\u5668\uff08\u5982Binoculars\u3001Fast-Detect GPT\u548cGPTZero\uff09\u5206\u6790\u4e86\u6765\u81ea\u4e3b\u8981\u3001\u5730\u65b9\u548c\u5927\u5b66\u65b0\u95fb\u5a92\u4f53\u768440,000\u591a\u7bc7\u65b0\u95fb\u6587\u7ae0\uff0c\u5e76\u8fdb\u884c\u4e86\u53e5\u5b50\u7ea7\u522b\u548c\u8bed\u8a00\u5b66\u5206\u6790\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u8fd1\u5e74\u6765\u751f\u6210\u5f0fAI\u7684\u4f7f\u7528\u663e\u8457\u589e\u52a0\uff0c\u5c24\u5176\u662f\u5728\u5730\u65b9\u548c\u5927\u5b66\u65b0\u95fb\u4e2d\u3002\u5728\u53e5\u5b50\u5c42\u9762\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7ecf\u5e38\u7528\u4e8e\u65b0\u95fb\u7684\u5f00\u5934\u90e8\u5206\uff0c\u800c\u7ed3\u8bba\u90e8\u5206\u901a\u5e38\u662f\u624b\u52a8\u7f16\u5199\u7684\u3002\u8bed\u8a00\u5b66\u5206\u6790\u8868\u660e\uff0c\u751f\u6210\u5f0fAI\u53ef\u4ee5\u63d0\u9ad8\u8bcd\u6c47\u4e30\u5bcc\u5ea6\u548c\u53ef\u8bfb\u6027\uff0c\u4f46\u4f1a\u964d\u4f4e\u6b63\u5f0f\u6027\uff0c\u5bfc\u81f4\u5199\u4f5c\u98ce\u683c\u66f4\u52a0\u7edf\u4e00\uff0c\u5c24\u5176\u662f\u5728\u5730\u65b9\u5a92\u4f53\u4e2d\u3002", "conclusion": "\u751f\u6210\u5f0fAI\uff08\u7279\u522b\u662f\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff09\u5728\u65b0\u95fb\u5a92\u4f53\u4e2d\u7684\u4f7f\u7528\u65e5\u76ca\u589e\u52a0\uff0c\u5c24\u5176\u662f\u5728\u5730\u65b9\u548c\u5927\u5b66\u65b0\u95fb\u4e2d\u3002\u751f\u6210\u5f0fAI\u4f1a\u63d0\u9ad8\u65b0\u95fb\u7684\u8bcd\u6c47\u4e30\u5bcc\u5ea6\u548c\u53ef\u8bfb\u6027\uff0c\u4f46\u4f1a\u964d\u4f4e\u6b63\u5f0f\u6027\uff0c\u5bfc\u81f4\u5199\u4f5c\u98ce\u683c\u8d8b\u4e8e\u7edf\u4e00\uff0c\u5c24\u5176\u662f\u5728\u5730\u65b9\u5a92\u4f53\u4e2d\u3002"}}
{"id": "2508.06076", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06076", "abs": "https://arxiv.org/abs/2508.06076", "authors": ["Michael Wehrli", "Alicia Durrer", "Paul Friedrich", "Sidaty El Hadramy", "Edwin Li", "Luana Brahaj", "Carol C. Hasler", "Philippe C. Cattin"], "title": "Towards MR-Based Trochleoplasty Planning", "comment": "Accepted at MICCAI COLAS Workshop 2025. Code:\n  https://wehrlimi.github.io/sr-3d-planning/", "summary": "To treat Trochlear Dysplasia (TD), current approaches rely mainly on\nlow-resolution clinical Magnetic Resonance (MR) scans and surgical intuition.\nThe surgeries are planned based on surgeons experience, have limited adoption\nof minimally invasive techniques, and lead to inconsistent outcomes. We propose\na pipeline that generates super-resolved, patient-specific 3D pseudo-healthy\ntarget morphologies from conventional clinical MR scans. First, we compute an\nisotropic super-resolved MR volume using an Implicit Neural Representation\n(INR). Next, we segment femur, tibia, patella, and fibula with a multi-label\ncustom-trained network. Finally, we train a Wavelet Diffusion Model (WDM) to\ngenerate pseudo-healthy target morphologies of the trochlear region. In\ncontrast to prior work producing pseudo-healthy low-resolution 3D MR images,\nour approach enables the generation of sub-millimeter resolved 3D shapes\ncompatible for pre- and intraoperative use. These can serve as preoperative\nblueprints for reshaping the femoral groove while preserving the native patella\narticulation. Furthermore, and in contrast to other work, we do not require a\nCT for our pipeline - reducing the amount of radiation. We evaluated our\napproach on 25 TD patients and could show that our target morphologies\nsignificantly improve the sulcus angle (SA) and trochlear groove depth (TGD).\nThe code and interactive visualization are available at\nhttps://wehrlimi.github.io/sr-3d-planning/.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7ba1\u9053\uff0c\u7528\u4e8e\u4ece\u4e34\u5e8aMR\u626b\u63cf\u751f\u6210\u8d85\u5206\u8fa8\u7387\u3001\u60a3\u8005\u7279\u5b9a\u76843D\u4f2a\u5065\u5eb7\u76ee\u6807\u5f62\u6001\uff0c\u4ee5\u6539\u5584\u6ed1\u8f66\u53d1\u80b2\u4e0d\u826f\uff08TD\uff09\u7684\u6cbb\u7597\u6548\u679c\u3002", "motivation": "\u4e3a\u4e86\u6cbb\u7597\u6ed1\u8f66\u53d1\u80b2\u4e0d\u826f\uff08TD\uff09\uff0c\u5f53\u524d\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u4f4e\u5206\u8fa8\u7387\u4e34\u5e8aMR\u626b\u63cf\u548c\u624b\u672f\u76f4\u89c9\uff0c\u624b\u672f\u8ba1\u5212\u57fa\u4e8e\u5916\u79d1\u533b\u751f\u7ecf\u9a8c\uff0c\u5fae\u521b\u6280\u672f\u91c7\u7eb3\u6709\u9650\uff0c\u5bfc\u81f4\u7ed3\u679c\u4e0d\u4e00\u81f4\u3002", "method": "1.\u4f7f\u7528\u9690\u5f0f\u795e\u7ecf\u8868\u793a\uff08INR\uff09\u8ba1\u7b97\u5404\u5411\u540c\u6027\u7684\u8d85\u5206\u8fa8\u7387MR\u4f53\u79ef\u30022.\u4f7f\u7528\u591a\u6807\u7b7e\u81ea\u5b9a\u4e49\u8bad\u7ec3\u7f51\u7edc\u5206\u5272\u80a1\u9aa8\u3001\u80eb\u9aa8\u3001\u9acc\u9aa8\u548c\u8153\u9aa8\u30023.\u8bad\u7ec3\u5c0f\u6ce2\u6269\u6563\u6a21\u578b\uff08WDM\uff09\u751f\u6210\u4f2a\u5065\u5eb7\u76ee\u6807\u5f62\u6001\u3002", "result": "\u751f\u6210\u7684\u4e9a\u6beb\u7c73\u7ea7\u5206\u8fa8\u73873D\u5f62\u6001\u53ef\u7528\u4e8e\u672f\u524d\u548c\u672f\u4e2d\uff0c\u5e76\u4e14\u76f8\u6bd4\u5176\u4ed6\u65b9\u6cd5\uff0c\u65e0\u9700CT\u626b\u63cf\uff0c\u51cf\u5c11\u4e86\u8f90\u5c04\u3002\u572825\u540dTD\u60a3\u8005\u7684\u8bc4\u4f30\u4e2d\uff0c\u663e\u793a\u8be5\u65b9\u6cd5\u663e\u8457\u6539\u5584\u4e86\u6c9f\u69fd\u89d2\u5ea6\uff08SA\uff09\u548c\u6c9f\u69fd\u6df1\u5ea6\uff08TGD\uff09\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u751f\u6210\u7684\u4f2a\u5065\u5eb7\u76ee\u6807\u5f62\u6001\u53ef\u663e\u8457\u6539\u5584\u6c9f\u69fd\u89d2\u5ea6\u548c\u6c9f\u69fd\u6df1\u5ea6\uff0c\u4e3a\u80a1\u9aa8\u6ed1\u8f66\u69fd\u7684\u91cd\u5851\u63d0\u4f9b\u672f\u524d\u84dd\u56fe\u3002"}}
{"id": "2508.06346", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06346", "abs": "https://arxiv.org/abs/2508.06346", "authors": ["Mert Can Kurucu", "Tufan Kumbasar", "\u0130brahim Eksin", "M\u00fcjde G\u00fczelkaya"], "title": "Introducing Fractional Classification Loss for Robust Learning with Noisy Labels", "comment": "25 pages, 6 figures, 2 table. Submitted to Pattern Recognition", "summary": "Robust loss functions are crucial for training deep neural networks in the\npresence of label noise, yet existing approaches require extensive,\ndataset-specific hyperparameter tuning. In this work, we introduce Fractional\nClassification Loss (FCL), an adaptive robust loss that automatically\ncalibrates its robustness to label noise during training. Built within the\nactive-passive loss framework, FCL employs the fractional derivative of the\nCross-Entropy (CE) loss as its active component and the Mean Absolute Error\n(MAE) as its passive loss component. With this formulation, we demonstrate that\nthe fractional derivative order $\\mu$ spans a family of loss functions that\ninterpolate between MAE-like robustness and CE-like fast convergence.\nFurthermore, we integrate $\\mu$ into the gradient-based optimization as a\nlearnable parameter and automatically adjust it to optimize the trade-off\nbetween robustness and convergence speed. We reveal that FCL's unique property\nestablishes a critical trade-off that enables the stable learning of $\\mu$:\nlower log penalties on difficult or mislabeled examples improve robustness but\nimpose higher penalties on easy or clean data, reducing model confidence in\nthem. Consequently, FCL can dynamically reshape its loss landscape to achieve\neffective classification performance under label noise. Extensive experiments\non benchmark datasets show that FCL achieves state-of-the-art results without\nthe need for manual hyperparameter tuning.", "AI": {"tldr": "FCL\u662f\u4e00\u79cd\u65b0\u7684\u81ea\u9002\u5e94\u9c81\u68d2\u635f\u5931\u51fd\u6570\uff0c\u5b83\u901a\u8fc7\u81ea\u52a8\u8c03\u6574\u5176\u5206\u6570\u9636\u5bfc\u6570\u9636\u6570\u03bc\uff0c\u5728\u9c81\u68d2\u6027\u548c\u6536\u655b\u901f\u5ea6\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u4ece\u800c\u5728\u6709\u6807\u7b7e\u566a\u58f0\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u5206\u7c7b\u6027\u80fd\uff0c\u5e76\u4e14\u65e0\u9700\u624b\u52a8\u8c03\u6574\u8d85\u53c2\u6570\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u9c81\u68d2\u635f\u5931\u51fd\u6570\u9700\u8981\u9488\u5bf9\u7279\u5b9a\u6570\u636e\u96c6\u8fdb\u884c\u5927\u91cf\u8d85\u53c2\u6570\u8c03\u6574\u7684\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u81ea\u9002\u5e94\u9c81\u68d2\u635f\u5931\u51fd\u6570FCL\u3002", "method": "FCL\u91c7\u7528\u5206\u6570\u9636\u4ea4\u53c9\u71b5\uff08CE\uff09\u635f\u5931\u4f5c\u4e3a\u5176\u6fc0\u6d3b\u7ec4\u4ef6\uff0c\u5e76\u4f7f\u7528\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\uff08MAE\uff09\u4f5c\u4e3a\u5176\u88ab\u52a8\u635f\u5931\u7ec4\u4ef6\u3002\u901a\u8fc7\u5c06\u5206\u6570\u9636\u5bfc\u6570\u9636\u6570\u03bc\u96c6\u6210\u5230\u57fa\u4e8e\u68af\u5ea6\u7684\u4f18\u5316\u4e2d\uff0c\u5e76\u5c06\u5176\u8bbe\u4e3a\u53ef\u5b66\u4e60\u53c2\u6570\uff0cFCL\u80fd\u591f\u81ea\u52a8\u8c03\u6574\u5176\u9c81\u68d2\u6027\u548c\u6536\u655b\u901f\u5ea6\u4e4b\u95f4\u7684\u6743\u8861\u3002", "result": "FCL\u80fd\u591f\u52a8\u6001\u5730\u91cd\u5851\u5176\u635f\u5931\u8fb9\u754c\uff0c\u4ece\u800c\u5728\u6807\u7b7e\u566a\u58f0\u4e0b\u5b9e\u73b0\u6709\u6548\u7684\u5206\u7c7b\u6027\u80fd\u3002\u5b9e\u9a8c\u8868\u660e\uff0cFCL\u5728\u4e0d\u8fdb\u884c\u624b\u52a8\u8d85\u53c2\u6570\u8c03\u6574\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6210\u679c\u3002", "conclusion": "FCL\uff08Fractional Classification Loss\uff09\u662f\u4e00\u79cd\u81ea\u9002\u5e94\u9c81\u68d2\u635f\u5931\u51fd\u6570\uff0c\u53ef\u4ee5\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u81ea\u52a8\u6821\u51c6\u5176\u5bf9\u6807\u7b7e\u566a\u58f0\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u5728\u6ca1\u6709\u624b\u52a8\u8c03\u6574\u8d85\u53c2\u6570\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002"}}
{"id": "2508.06431", "categories": ["quant-ph", "stat.OT"], "pdf": "https://arxiv.org/pdf/2508.06431", "abs": "https://arxiv.org/abs/2508.06431", "authors": ["Liubov A. Markovich", "Xiaoyu Liu", "Jordi Tura"], "title": "Nonparametric Learning Non-Gaussian Quantum States of Continuous Variable Systems", "comment": "29 pages, 20 figures", "summary": "Continuous-variable quantum systems are foundational to quantum computation,\ncommunication, and sensing. While traditional representations using wave\nfunctions or density matrices are often impractical, the tomographic picture of\nquantum mechanics provides an accessible alternative by associating quantum\nstates with classical probability distribution functions called tomograms.\nDespite its advantages, including compatibility with classical statistical\nmethods, tomographic method remain underutilized due to a lack of robust\nestimation techniques. This work addresses this gap by introducing a\nnon-parametric \\emph{kernel quantum state estimation} (KQSE) framework for\nreconstructing quantum states and their trace characteristics from noisy data,\nwithout prior knowledge of the state. In contrast to existing methods, KQSE\nyields estimates of the density matrix in various bases, as well as trace\nquantities such as purity, higher moments, overlap, and trace distance, with a\nnear-optimal convergence rate of $\\tilde{O}\\bigl(T^{-1}\\bigr)$, where $T$ is\nthe total number of measurements. KQSE is robust for multimodal, non-Gaussian\nstates, making it particularly well suited for characterizing states essential\nfor quantum science.", "AI": {"tldr": "\u91cf\u5b50\u6001\u5c42\u6790\u6210\u50cf\u65b9\u6cd5\u7531\u4e8e\u7f3a\u4e4f\u7a33\u5065\u7684\u4f30\u8ba1\u6280\u672f\u800c\u672a\u88ab\u5145\u5206\u5229\u7528\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aKQSE\uff08\u6838\u91cf\u5b50\u6001\u4f30\u8ba1\uff09\u7684\u975e\u53c2\u6570\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u566a\u58f0\u6570\u636e\u4e2d\u91cd\u5efa\u91cf\u5b50\u6001\u53ca\u5176\u8ff9\u7279\u5f81\u3002KQSE\u53ef\u4ee5\u63d0\u4f9b\u5404\u79cd\u57fa\u4e0b\u7684\u5bc6\u5ea6\u77e9\u9635\u4f30\u8ba1\uff0c\u4ee5\u53ca\u7eaf\u5ea6\u3001\u9ad8\u9636\u77e9\u3001\u91cd\u53e0\u5ea6\u548c\u8ff9\u8ddd\u79bb\u7b49\u8ff9\u91cf\uff0c\u6536\u655b\u901f\u7387\u63a5\u8fd1\u6700\u4f18\uff08$\tilde{O}\bigl(T^{-1}\bigr)$\uff09\uff0c\u5e76\u4e14\u5bf9\u591a\u5cf0\u975e\u9ad8\u65af\u6001\u5177\u6709\u9c81\u68d2\u6027\uff0c\u975e\u5e38\u9002\u5408\u91cf\u5b50\u79d1\u5b66\u4e2d\u7684\u72b6\u6001\u8868\u5f81\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u91cf\u5b50\u6001\u5c42\u6790\u6210\u50cf\u65b9\u6cd5\u7531\u4e8e\u7f3a\u4e4f\u7a33\u5065\u7684\u4f30\u8ba1\u6280\u672f\u800c\u672a\u88ab\u5145\u5206\u5229\u7528\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aKQSE\uff08\u6838\u91cf\u5b50\u6001\u4f30\u8ba1\uff09\u7684\u975e\u53c2\u6570\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u566a\u58f0\u6570\u636e\u4e2d\u91cd\u5efa\u91cf\u5b50\u6001\u53ca\u5176\u8ff9\u7279\u5f81\u3002", "result": "KQSE\u53ef\u4ee5\u63d0\u4f9b\u5404\u79cd\u57fa\u4e0b\u7684\u5bc6\u5ea6\u77e9\u9635\u4f30\u8ba1\uff0c\u4ee5\u53ca\u7eaf\u5ea6\u3001\u9ad8\u9636\u77e9\u3001\u91cd\u53e0\u5ea6\u548c\u8ff9\u8ddd\u79bb\u7b49\u8ff9\u91cf\uff0c\u6536\u655b\u901f\u7387\u63a5\u8fd1\u6700\u4f18\uff08$\tilde{O}\bigl(T^{-1}\bigr)$\uff09\uff0c\u5e76\u4e14\u5bf9\u591a\u5cf0\u975e\u9ad8\u65af\u6001\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "KQSE\u662f\u4e00\u79cd\u975e\u53c2\u6570\u6838\u91cf\u5b50\u6001\u4f30\u8ba1\u6846\u67b6\uff0c\u53ef\u4ee5\u4ece\u566a\u58f0\u6570\u636e\u4e2d\u91cd\u5efa\u91cf\u5b50\u6001\u53ca\u5176\u8ff9\u7279\u5f81\uff0c\u5177\u6709\u8fd1\u4e4e\u6700\u4f18\u7684\u6536\u655b\u901f\u7387\uff0c\u5e76\u4e14\u5bf9\u591a\u5cf0\u975e\u9ad8\u65af\u6001\u5177\u6709\u9c81\u68d2\u6027\uff0c\u975e\u5e38\u9002\u5408\u91cf\u5b50\u79d1\u5b66\u4e2d\u7684\u72b6\u6001\u8868\u5f81\u3002"}}
{"id": "2508.06447", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06447", "abs": "https://arxiv.org/abs/2508.06447", "authors": ["Lingkun Long", "Rubing Yang", "Yushi Huang", "Desheng Hui", "Ao Zhou", "Jianlei Yang"], "title": "SlimInfer: Accelerating Long-Context LLM Inference via Dynamic Token Pruning", "comment": null, "summary": "Long-context inference for Large Language Models (LLMs) is heavily limited by\nhigh computational demands. While several existing methods optimize attention\ncomputation, they still process the full set of hidden states at each layer,\nlimiting overall efficiency. In this work, we propose SlimInfer, an innovative\nframework that aims to accelerate inference by directly pruning less critical\nprompt tokens during the forward pass. Our key insight is an information\ndiffusion phenomenon: As information from critical tokens propagates through\nlayers, it becomes distributed across the entire sequence. This diffusion\nprocess suggests that LLMs can maintain their semantic integrity when excessive\ntokens, even including these critical ones, are pruned in hidden states.\nMotivated by this, SlimInfer introduces a dynamic fine-grained pruning\nmechanism that accurately removes redundant tokens of hidden state at\nintermediate layers. This layer-wise pruning naturally enables an asynchronous\nKV cache manager that prefetches required token blocks without complex\npredictors, reducing both memory usage and I/O costs. Extensive experiments\nshow that SlimInfer can achieve up to $\\mathbf{2.53\\times}$ time-to-first-token\n(TTFT) speedup and $\\mathbf{1.88\\times}$ end-to-end latency reduction for\nLLaMA3.1-8B-Instruct on a single RTX 4090, without sacrificing performance on\nLongBench. Our code will be released upon acceptance.", "AI": {"tldr": "SlimInfer\u901a\u8fc7\u526a\u679d\u5197\u4f59\u4ee4\u724c\u6765\u52a0\u901fLLM\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u4f18\u5316\u6ce8\u610f\u529b\u8ba1\u7b97\u65f6\uff0c\u4ecd\u7136\u5904\u7406\u6bcf\u4e00\u5c42\u7684\u5168\u90e8\u9690\u85cf\u72b6\u6001\uff0c\u9650\u5236\u4e86\u6574\u4f53\u6548\u7387\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u63d0\u9ad8\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u7684\u6548\u7387\u3002", "method": "SlimInfer\u6846\u67b6\u901a\u8fc7\u5728\u524d\u5411\u4f20\u64ad\u8fc7\u7a0b\u4e2d\u76f4\u63a5\u526a\u679d\u4e0d\u592a\u91cd\u8981\u7684\u63d0\u793a\u4ee4\u724c\u6765\u52a0\u901f\u63a8\u7406\uff0c\u5e76\u5f15\u5165\u4e86\u5f02\u6b65KV\u7f13\u5b58\u7ba1\u7406\u5668\uff0c\u4ee5\u51cf\u5c11\u5185\u5b58\u4f7f\u7528\u548cI/O\u6210\u672c\u3002", "result": "SlimInfer\u5728LLaMA3.1-8B-Instruct\u6a21\u578b\u548cRTX 4090\u4e0a\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe2.53\u500d\u7684\u9996 \u091f\u094b\u0915\u65f6\u95f4\uff08TTFT\uff09\u52a0\u901f\u548c1.88\u500d\u7684\u7aef\u5230\u7aef\u5ef6\u8fdf\u964d\u4f4e\uff0c\u4e14\u5728LongBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u672a\u727a\u7272\u6027\u80fd\u3002", "conclusion": "SlimInfer\u901a\u8fc7\u52a8\u6001\u7684\u3001\u7ec6\u7c92\u5ea6\u7684\u526a\u679d\u673a\u5236\uff0c\u5728\u4e2d\u95f4\u5c42\u51c6\u786e\u5730\u53bb\u9664\u5197\u4f59\u7684\u9690\u85cf\u72b6\u6001\u4ee4\u724c\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684LLM\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\uff0c\u540c\u65f6\u4e0d\u727a\u7272\u6027\u80fd\u3002"}}
{"id": "2508.06080", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06080", "abs": "https://arxiv.org/abs/2508.06080", "authors": ["Bin Xia", "Jiyang Liu", "Yuechen Zhang", "Bohao Peng", "Ruihang Chu", "Yitong Wang", "Xinglong Wu", "Bei Yu", "Jiaya Jia"], "title": "DreamVE: Unified Instruction-based Image and Video Editing", "comment": null, "summary": "Instruction-based editing holds vast potential due to its simple and\nefficient interactive editing format. However, instruction-based editing,\nparticularly for video, has been constrained by limited training data,\nhindering its practical application. To this end, we introduce DreamVE, a\nunified model for instruction-based image and video editing. Specifically, We\npropose a two-stage training strategy: first image editing, then video editing.\nThis offers two main benefits: (1) Image data scales more easily, and models\nare more efficient to train, providing useful priors for faster and better\nvideo editing training. (2) Unifying image and video generation is natural and\naligns with current trends. Moreover, we present comprehensive training data\nsynthesis pipelines, including collage-based and generative model-based data\nsynthesis. The collage-based data synthesis combines foreground objects and\nbackgrounds to generate diverse editing data, such as object manipulation,\nbackground changes, and text modifications. It can easily generate billions of\naccurate, consistent, realistic, and diverse editing pairs. We pretrain DreamVE\non extensive collage-based data to achieve strong performance in key editing\ntypes and enhance generalization and transfer capabilities. However,\ncollage-based data lacks some attribute editing cases, leading to a relative\ndrop in performance. In contrast, the generative model-based pipeline, despite\nbeing hard to scale up, offers flexibility in handling attribute editing cases.\nTherefore, we use generative model-based data to further fine-tune DreamVE.\nBesides, we design an efficient and powerful editing framework for DreamVE. We\nbuild on the SOTA T2V model and use a token concatenation with early drop\napproach to inject source image guidance, ensuring strong consistency and\neditability. The codes and models will be released.", "AI": {"tldr": "DreamVE\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u56fe\u50cf\u548c\u89c6\u9891\u6307\u4ee4\u7f16\u8f91\u6a21\u578b\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\uff08\u56fe\u50cf\u9884\u8bad\u7ec3+\u89c6\u9891\u5fae\u8c03\uff09\u548c\u4e24\u79cd\u6570\u636e\u5408\u6210\u65b9\u6cd5\uff08\u62fc\u8d34+\u751f\u6210\u6a21\u578b\uff09\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u7a00\u758f\u95ee\u9898\uff0c\u5e76\u5229\u7528\u7279\u5b9a\u6280\u672f\uff08Token Concatenation with Early Drop\uff09\u589e\u5f3a\u4e86\u7f16\u8f91\u6548\u679c\u548c\u4e00\u81f4\u6027\u3002", "motivation": "\u6307\u4ee4\u9a71\u52a8\u7684\u7f16\u8f91\u65b9\u5f0f\u56e0\u5176\u7b80\u5355\u9ad8\u6548\u7684\u4ea4\u4e92\u683c\u5f0f\u800c\u5177\u6709\u5de8\u5927\u7684\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u5728\u89c6\u9891\u7f16\u8f91\u9886\u57df\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u6307\u4ee4\u9a71\u52a8\u89c6\u9891\u7f16\u8f91\u65b9\u6cd5\u53d7\u5230\u8bad\u7ec3\u6570\u636e\u6709\u9650\u7684\u5236\u7ea6\uff0c\u963b\u788d\u4e86\u5176\u5e7f\u6cdb\u5e94\u7528\u3002\u56e0\u6b64\uff0c\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u6709\u6548\u5229\u7528\u6570\u636e\u3001\u63d0\u9ad8\u7f16\u8f91\u6027\u80fd\u5e76\u6613\u4e8e\u6269\u5c55\u7684\u7edf\u4e00\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDreamVE\u7684\u7edf\u4e00\u6a21\u578b\uff0c\u7528\u4e8e\u6307\u4ee4\u9a71\u52a8\u7684\u56fe\u50cf\u548c\u89c6\u9891\u7f16\u8f91\u3002\u8be5\u6a21\u578b\u91c7\u7528\u4e86\u4e24\u9636\u6bb5\u7684\u8bad\u7ec3\u7b56\u7565\uff1a\u9996\u5148\u5728\u56fe\u50cf\u7f16\u8f91\u6570\u636e\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u7136\u540e\u8fc1\u79fb\u5230\u89c6\u9891\u7f16\u8f91\u4efb\u52a1\u3002\u4e3a\u4e86\u89e3\u51b3\u6570\u636e\u7a00\u758f\u6027\u95ee\u9898\uff0c\u7814\u7a76\u4eba\u5458\u8bbe\u8ba1\u4e86\u4e24\u79cd\u6570\u636e\u5408\u6210\u7ba1\u7ebf\uff1a\u57fa\u4e8e\u62fc\u8d34\u7684\u6570\u636e\u5408\u6210\uff0c\u7528\u4e8e\u751f\u6210\u5927\u89c4\u6a21\u3001\u591a\u6837\u5316\u7684\u7f16\u8f91\u6570\u636e\uff08\u5982\u7269\u4f53\u64cd\u63a7\u3001\u80cc\u666f\u66f4\u6362\u3001\u6587\u672c\u4fee\u6539\uff09\uff1b\u4ee5\u53ca\u57fa\u4e8e\u751f\u6210\u6a21\u578b\u7684\u6570\u636e\u5408\u6210\uff0c\u7528\u4e8e\u5904\u7406\u7279\u5b9a\u5c5e\u6027\u7f16\u8f91\u7684\u6848\u4f8b\u3002\u5728\u6a21\u578b\u67b6\u6784\u4e0a\uff0cDreamVE\u6784\u5efa\u4e8eT2V\u6a21\u578b\u4e4b\u4e0a\uff0c\u5e76\u91c7\u7528Token Concatenation with Early Drop\u6280\u672f\u6765\u6ce8\u5165\u6e90\u56fe\u50cf\u7684\u5f15\u5bfc\u4fe1\u606f\uff0c\u4ee5\u786e\u4fdd\u7f16\u8f91\u7684\u4e00\u81f4\u6027\u548c\u53ef\u7f16\u8f91\u6027\u3002", "result": "DreamVE\u6a21\u578b\u5728\u56fe\u50cf\u548c\u89c6\u9891\u7684\u6307\u4ee4\u7f16\u8f91\u4efb\u52a1\u4e0a\u5747\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728\u7269\u4f53\u64cd\u63a7\u3001\u80cc\u666f\u66f4\u6362\u548c\u6587\u672c\u4fee\u6539\u7b49\u591a\u79cd\u7f16\u8f91\u7c7b\u578b\u4e0a\u3002\u901a\u8fc7\u62fc\u8d34\u6570\u636e\u9884\u8bad\u7ec3\uff0c\u6a21\u578b\u83b7\u5f97\u4e86\u5f3a\u5927\u7684\u57fa\u7840\u80fd\u529b\u548c\u6cdb\u5316\u6027\uff1b\u968f\u540e\u901a\u8fc7\u751f\u6210\u6a21\u578b\u6570\u636e\u5fae\u8c03\uff0c\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u5728\u5c5e\u6027\u7f16\u8f91\u7b49\u65b9\u9762\u7684\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u57fa\u4e8e\u56fe\u50cf\u7684\u9884\u8bad\u7ec3\u548c\u57fa\u4e8e\u751f\u6210\u6a21\u578b\u7684\u5fae\u8c03\uff0c\u4ee5\u53ca\u4f7f\u7528Token Concatenation with Early Drop\u65b9\u6cd5\u6ce8\u5165\u6e90\u56fe\u50cf\u6307\u5bfc\uff0cDreamVE\u5728\u56fe\u50cf\u548c\u89c6\u9891\u6307\u4ee4\u7f16\u8f91\u65b9\u9762\u53d6\u5f97\u4e86\u5f3a\u5927\u7684\u6027\u80fd\uff0c\u5e76\u5177\u5907\u826f\u597d\u7684\u6cdb\u5316\u548c\u8fc1\u79fb\u80fd\u529b\u3002"}}
{"id": "2508.06441", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.06441", "abs": "https://arxiv.org/abs/2508.06441", "authors": ["Manuel Gallego", "Sebasti\u00e1n Roca-Jerat", "David Zueco", "Jes\u00fas Carrete"], "title": "Accelerating Quantum Monte Carlo Calculations with Set-Equivariant Architectures and Transfer Learning", "comment": null, "summary": "Machine-learning (ML) ans\\\"atze have greatly expanded the accuracy and reach\nof variational quantum Monte Carlo (QMC) calculations, in particular when\nexploring the manifold quantum phenomena exhibited by spin systems. However,\nthe scalability of QMC is still compromised by several other bottlenecks, and\nspecifically those related to the actual evaluation of observables based on\nrandom deviates that lies at the core of the approach. Here we show how the\nset-transformer architecture can be used to dramatically accelerate or even\nbypass that step, especially for time-consuming operators such as powers of the\nmagnetization. We illustrate the procedure with a range of examples of\nincreasing complexity, from the classical Ising model to quantum systems with\nlong-range interactions, and comprising both regressions (to predict\nobservables) and classifications (to detect phase transitions). Moreover, we\nshow how transfer learning can be leveraged to reduce the training cost by\nreusing knowledge from different systems and smaller system sizes.", "AI": {"tldr": "\u672c\u6587\u5229\u7528set-transformer\u67b6\u6784\u52a0\u901f\u91cf\u5b50\u8499\u7279\u5361\u6d1b\u8ba1\u7b97\u4e2d\u7684\u53ef\u89c2\u6d4b\u91cf\u8bc4\u4f30\uff0c\u5c24\u5176\u9488\u5bf9\u78c1\u5316\u5f3a\u5ea6\u5e42\u7b49\u8017\u65f6\u7b97\u7b26\uff0c\u5e76\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\u964d\u4f4e\u8bad\u7ec3\u6210\u672c\u3002", "motivation": "\u5c3d\u7ba1\u673a\u5668\u5b66\u4e60\uff08ML\uff09\u5df2\u5927\u5927\u63d0\u9ad8\u4e86\u53d8\u5206\u91cf\u5b50\u8499\u7279\u5361\u6d1b\uff08QMC\uff09\u8ba1\u7b97\u7684\u7cbe\u5ea6\u548c\u8303\u56f4\uff0c\u5c24\u5176\u662f\u5728\u7814\u7a76\u81ea\u65cb\u7cfb\u7edf\u4e2d\u7684\u91cf\u5b50\u73b0\u8c61\u65f6\uff0c\u4f46QMC\u7684\u53ef\u6269\u5c55\u6027\u4ecd\u53d7\u5230\u5176\u4ed6\u74f6\u9888\u7684\u9650\u5236\uff0c\u7279\u522b\u662f\u4e0e\u57fa\u4e8e\u968f\u673a\u504f\u5dee\u7684\u53ef\u89c2\u6d4b\u91cf\u5b9e\u9645\u8bc4\u4f30\u76f8\u5173\u7684\u95ee\u9898\u3002", "method": "\u672c\u6587\u5c55\u793a\u4e86\u5982\u4f55\u4f7f\u7528set-transformer\u67b6\u6784\u6765\u52a0\u901f\u91cf\u5b50\u8499\u7279\u5361\u6d1b\uff08QMC\uff09\u8ba1\u7b97\u4e2d\u53ef\u89c2\u6d4b\u91cf\uff08observables\uff09\u7684\u8bc4\u4f30\uff0c\u7279\u522b\u9488\u5bf9\u65f6\u95f4\u590d\u6742\u5ea6\u9ad8\u7684\u7b97\u7b26\uff0c\u5982\u78c1\u5316\u5f3a\u5ea6\u7684\u5e42\u3002\u901a\u8fc7\u5c06set-transformer\u67b6\u6784\u5e94\u7528\u4e8e\u4ece\u7ecf\u5178\u7684Ising\u6a21\u578b\u5230\u5177\u6709\u957f\u7a0b\u76f8\u4e92\u4f5c\u7528\u7684\u91cf\u5b50\u7cfb\u7edf\uff0c\u5e76\u7ed3\u5408\u56de\u5f52\uff08\u9884\u6d4b\u53ef\u89c2\u6d4b\u91cf\uff09\u548c\u5206\u7c7b\uff08\u68c0\u6d4b\u76f8\u53d8\uff09\u4efb\u52a1\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002\u6b64\u5916\uff0c\u8fd8\u5229\u7528\u8fc1\u79fb\u5b66\u4e60\u6765\u964d\u4f4e\u8bad\u7ec3\u6210\u672c\u3002", "result": "set-transformer\u67b6\u6784\u80fd\u591f\u663e\u8457\u52a0\u901f\u6216\u751a\u81f3\u7ed5\u8fc7\u91cf\u5b50\u8499\u7279\u5361\u6d1b\u8ba1\u7b97\u4e2d\u57fa\u4e8e\u968f\u673a\u504f\u5dee\u7684\u53ef\u89c2\u6d4b\u91cf\u8bc4\u4f30\u6b65\u9aa4\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u50cf\u78c1\u5316\u5f3a\u5ea6\u5e42\u8fd9\u7c7b\u8017\u65f6\u7b97\u7b26\u65f6\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5e94\u7528\u4e8e\u4e0d\u540c\u590d\u6742\u5ea6\u7684\u7cfb\u7edf\uff08\u4ece\u7ecf\u5178Ising\u6a21\u578b\u5230\u5177\u6709\u957f\u7a0b\u76f8\u4e92\u4f5c\u7528\u7684\u91cf\u5b50\u7cfb\u7edf\uff09\u7684\u56de\u5f52\uff08\u9884\u6d4b\u53ef\u89c2\u6d4b\u91cf\uff09\u548c\u5206\u7c7b\uff08\u68c0\u6d4b\u76f8\u53d8\uff09\u4efb\u52a1\u8fdb\u884c\u4e86\u8bf4\u660e\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\uff0c\u53ef\u4ee5\u5229\u7528\u4e0d\u540c\u7cfb\u7edf\u548c\u8f83\u5c0f\u7cfb\u7edf\u5c3a\u5bf8\u7684\u77e5\u8bc6\u6765\u964d\u4f4e\u8bad\u7ec3\u6210\u672c\u3002", "conclusion": "set-transformer\u67b6\u6784\u53ef\u4ee5\u663e\u8457\u52a0\u901f\u6216\u7ed5\u8fc7\u91cf\u5b50\u8499\u7279\u5361\u6d1b\u8ba1\u7b97\u4e2d\u57fa\u4e8e\u968f\u673a\u504f\u5dee\u7684\u5b9e\u9645\u53ef\u89c2\u6d4b\u91cf\u8bc4\u4f30\uff0c\u5c24\u5176\u5bf9\u4e8e\u8017\u65f6\u7684\u7b97\u7b26\uff0c\u5982\u78c1\u5316\u5f3a\u5ea6\u7684\u5e42\u3002"}}
{"id": "2508.06471", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06471", "abs": "https://arxiv.org/abs/2508.06471", "authors": ["GLM-4. 5 Team", ":", "Aohan Zeng", "Xin Lv", "Qinkai Zheng", "Zhenyu Hou", "Bin Chen", "Chengxing Xie", "Cunxiang Wang", "Da Yin", "Hao Zeng", "Jiajie Zhang", "Kedong Wang", "Lucen Zhong", "Mingdao Liu", "Rui Lu", "Shulin Cao", "Xiaohan Zhang", "Xuancheng Huang", "Yao Wei", "Yean Cheng", "Yifan An", "Yilin Niu", "Yuanhao Wen", "Yushi Bai", "Zhengxiao Du", "Zihan Wang", "Zilin Zhu", "Bohan Zhang", "Bosi Wen", "Bowen Wu", "Bowen Xu", "Can Huang", "Casey Zhao", "Changpeng Cai", "Chao Yu", "Chen Li", "Chendi Ge", "Chenghua Huang", "Chenhui Zhang", "Chenxi Xu", "Chenzheng Zhu", "Chuang Li", "Congfeng Yin", "Daoyan Lin", "Dayong Yang", "Dazhi Jiang", "Ding Ai", "Erle Zhu", "Fei Wang", "Gengzheng Pan", "Guo Wang", "Hailong Sun", "Haitao Li", "Haiyang Li", "Haiyi Hu", "Hanyu Zhang", "Hao Peng", "Hao Tai", "Haoke Zhang", "Haoran Wang", "Haoyu Yang", "He Liu", "He Zhao", "Hongwei Liu", "Hongxi Yan", "Huan Liu", "Huilong Chen", "Ji Li", "Jiajing Zhao", "Jiamin Ren", "Jian Jiao", "Jiani Zhao", "Jianyang Yan", "Jiaqi Wang", "Jiayi Gui", "Jiayue Zhao", "Jie Liu", "Jijie Li", "Jing Li", "Jing Lu", "Jingsen Wang", "Jingwei Yuan", "Jingxuan Li", "Jingzhao Du", "Jinhua Du", "Jinxin Liu", "Junkai Zhi", "Junli Gao", "Ke Wang", "Lekang Yang", "Liang Xu", "Lin Fan", "Lindong Wu", "Lintao Ding", "Lu Wang", "Man Zhang", "Minghao Li", "Minghuan Xu", "Mingming Zhao", "Mingshu Zhai", "Pengfan Du", "Qian Dong", "Shangde Lei", "Shangqing Tu", "Shangtong Yang", "Shaoyou Lu", "Shijie Li", "Shuang Li", "Shuang-Li", "Shuxun Yang", "Sibo Yi", "Tianshu Yu", "Wei Tian", "Weihan Wang", "Wenbo Yu", "Weng Lam Tam", "Wenjie Liang", "Wentao Liu", "Xiao Wang", "Xiaohan Jia", "Xiaotao Gu", "Xiaoying Ling", "Xin Wang", "Xing Fan", "Xingru Pan", "Xinyuan Zhang", "Xinze Zhang", "Xiuqing Fu", "Xunkai Zhang", "Yabo Xu", "Yandong Wu", "Yida Lu", "Yidong Wang", "Yilin Zhou", "Yiming Pan", "Ying Zhang", "Yingli Wang", "Yingru Li", "Yinpei Su", "Yipeng Geng", "Yitong Zhu", "Yongkun Yang", "Yuhang Li", "Yuhao Wu", "Yujiang Li", "Yunan Liu", "Yunqing Wang", "Yuntao Li", "Yuxuan Zhang", "Zezhen Liu", "Zhen Yang", "Zhengda Zhou", "Zhongpei Qiao", "Zhuoer Feng", "Zhuorui Liu", "Zichen Zhang", "Zihan Wang", "Zijun Yao", "Zikang Wang", "Ziqiang Liu", "Ziwei Chai", "Zixuan Li", "Zuodong Zhao", "Wenguang Chen", "Jidong Zhai", "Bin Xu", "Minlie Huang", "Hongning Wang", "Juanzi Li", "Yuxiao Dong", "Jie Tang"], "title": "GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models", "comment": null, "summary": "We present GLM-4.5, an open-source Mixture-of-Experts (MoE) large language\nmodel with 355B total parameters and 32B activated parameters, featuring a\nhybrid reasoning method that supports both thinking and direct response modes.\nThrough multi-stage training on 23T tokens and comprehensive post-training with\nexpert model iteration and reinforcement learning, GLM-4.5 achieves strong\nperformance across agentic, reasoning, and coding (ARC) tasks, scoring 70.1% on\nTAU-Bench, 91.0% on AIME 24, and 64.2% on SWE-bench Verified. With much fewer\nparameters than several competitors, GLM-4.5 ranks 3rd overall among all\nevaluated models and 2nd on agentic benchmarks. We release both GLM-4.5 (355B\nparameters) and a compact version, GLM-4.5-Air (106B parameters), to advance\nresearch in reasoning and agentic AI systems. Code, models, and more\ninformation are available at https://github.com/zai-org/GLM-4.5.", "AI": {"tldr": "GLM-4.5 is a powerful open-source LLM with a hybrid reasoning approach, outperforming many models despite its smaller size, and is now available for research.", "motivation": "To present GLM-4.5, an open-source MoE large language model with a hybrid reasoning method, and to advance research in reasoning and agentic AI systems.", "method": "GLM-4.5 is an open-source Mixture-of-Experts (MoE) large language model with 355B total parameters and 32B activated parameters. It features a hybrid reasoning method supporting thinking and direct response modes. Training involved 23T tokens, multi-stage training, expert model iteration, and reinforcement learning.", "result": "GLM-4.5 scores 70.1% on TAU-Bench, 91.0% on AIME 24, and 64.2% on SWE-bench Verified. It ranks 3rd overall among evaluated models and 2nd on agentic benchmarks.", "conclusion": "GLM-4.5 achieves strong performance across agentic, reasoning, and coding tasks, ranking 3rd overall and 2nd on agentic benchmarks, despite having fewer parameters than competitors. Both GLM-4.5 (355B) and GLM-4.5-Air (106B) are released to advance research."}}
{"id": "2508.06082", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06082", "abs": "https://arxiv.org/abs/2508.06082", "authors": ["Yanxiao Sun", "Jiafu Wu", "Yun Cao", "Chengming Xu", "Yabiao Wang", "Weijian Cao", "Donghao Luo", "Chengjie Wang", "Yanwei Fu"], "title": "SwiftVideo: A Unified Framework for Few-Step Video Generation through Trajectory-Distribution Alignment", "comment": null, "summary": "Diffusion-based or flow-based models have achieved significant progress in\nvideo synthesis but require multiple iterative sampling steps, which incurs\nsubstantial computational overhead. While many distillation methods that are\nsolely based on trajectory-preserving or distribution-matching have been\ndeveloped to accelerate video generation models, these approaches often suffer\nfrom performance breakdown or increased artifacts under few-step settings. To\naddress these limitations, we propose \\textbf{\\emph{SwiftVideo}}, a unified and\nstable distillation framework that combines the advantages of\ntrajectory-preserving and distribution-matching strategies. Our approach\nintroduces continuous-time consistency distillation to ensure precise\npreservation of ODE trajectories. Subsequently, we propose a dual-perspective\nalignment that includes distribution alignment between synthetic and real data\nalong with trajectory alignment across different inference steps. Our method\nmaintains high-quality video generation while substantially reducing the number\nof inference steps. Quantitative evaluations on the OpenVid-1M benchmark\ndemonstrate that our method significantly outperforms existing approaches in\nfew-step video generation.", "AI": {"tldr": "SwiftVideo is a new framework that speeds up video generation by combining trajectory and distribution matching, performing better than other methods when few steps are used.", "motivation": "Existing diffusion-based or flow-based video synthesis models require multiple iterative sampling steps, leading to high computational overhead. Current distillation methods, focusing solely on trajectory-preserving or distribution-matching, perform poorly or introduce artifacts in few-step settings.", "method": "SwiftVideo uses a unified and stable distillation framework combining trajectory-preserving and distribution-matching strategies. It incorporates continuous-time consistency distillation for precise ODE trajectory preservation and introduces dual-perspective alignment (distribution alignment between synthetic and real data, and trajectory alignment across inference steps).", "result": "SwiftVideo achieves high-quality video generation while substantially reducing inference steps, outperforming existing approaches in few-step video generation as evidenced by quantitative evaluations on the OpenVid-1M benchmark.", "conclusion": "SwiftVideo can generate high-quality videos with significantly fewer inference steps and outperforms existing methods in few-step video generation."}}
{"id": "2508.06353", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06353", "abs": "https://arxiv.org/abs/2508.06353", "authors": ["Parichit Sharma", "Marcin Stanislaw", "Hasan Kurban", "Oguzhan Kulekci", "Mehmet Dalkilic"], "title": "Geometric-k-means: A Bound Free Approach to Fast and Eco-Friendly k-means", "comment": null, "summary": "This paper introduces Geometric-k-means (or Gk-means for short), a novel\napproach that significantly enhances the efficiency and energy economy of the\nwidely utilized k-means algorithm, which, despite its inception over five\ndecades ago, remains a cornerstone in machine learning applications. The\nessence of Gk-means lies in its active utilization of geometric principles,\nspecifically scalar projection, to significantly accelerate the algorithm\nwithout sacrificing solution quality. This geometric strategy enables a more\ndiscerning focus on data points that are most likely to influence cluster\nupdates, which we call as high expressive data (HE). In contrast, low\nexpressive data (LE), does not impact clustering outcome, is effectively\nbypassed, leading to considerable reductions in computational overhead.\nExperiments spanning synthetic, real-world and high-dimensional datasets,\ndemonstrate Gk-means is significantly better than traditional and state of the\nart (SOTA) k-means variants in runtime and distance computations (DC).\nMoreover, Gk-means exhibits better resource efficiency, as evidenced by its\nreduced energy footprint, placing it as more sustainable alternative.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.06444", "categories": ["quant-ph", "cond-mat.quant-gas"], "pdf": "https://arxiv.org/pdf/2508.06444", "abs": "https://arxiv.org/abs/2508.06444", "authors": ["Guitao Lyu", "Myung-Joong Hwang"], "title": "Nonreciprocal and Geometric Frustration in Dissipative Quantum Spins", "comment": "7+5 pages, 3+7 figures", "summary": "Nonreciprocal interactions often create conflicting dynamical objectives that\ncannot be simultaneously satisfied, leading to nonreciprocal frustration. On\nthe other hand, geometric frustration arises when conflicting static objectives\nin energy minimization cannot be satisfied. In this work, we show that\nnonreciprocal interaction among three collective quantum spins, mediated by a\ndamped cavity, induces not only nonreciprocal frustration, intrinsic to\nnonreciprocity, but also geometric frustration with a remarkable robustness\nagainst disorder. It therefore ensures that the accidental degeneracy for\nsteady states remains intact even when the system is perturbed away from a\nfine-tuned point of enhanced symmetry, in sharp contrast to the equilibrium\ncase. Leveraging this finding, we identify a nonreciprocal phase transition\ndriven by both geometric and nonreciprocal frustration. It gives rise to a\ntime-dependent state, which shows a chiral dynamics along a geometry shaped by\nthe geometric frustration and dynamically restores the broken discrete\nsymmetries. Moreover, it constitutes a time-crystalline order, with multiple\nharmonics set by an emergent time scale that exhibits critical slowing down.\nOur predictions have important physical implications for a three-component\nspinor BEC-cavity system, which manifest as a geometric frustration in the\nstructural phase transition and chiral dynamics of the frustrated\nself-organized BECs. We demonstrate the feasibility of experimental observation\ndespite the presence of disorder in the spin-cavity coupling strengths.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.06475", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06475", "abs": "https://arxiv.org/abs/2508.06475", "authors": ["Guimin Hu", "Daniel Hershcovich", "Hasti Seifi"], "title": "HapticLLaMA: A Multimodal Sensory Language Model for Haptic Captioning", "comment": null, "summary": "Haptic captioning is the task of generating natural language descriptions\nfrom haptic signals, such as vibrations, for use in virtual reality,\naccessibility, and rehabilitation applications. While previous multimodal\nresearch has focused primarily on vision and audio, haptic signals for the\nsense of touch remain underexplored. To address this gap, we formalize the\nhaptic captioning task and propose HapticLLaMA, a multimodal sensory language\nmodel that interprets vibration signals into descriptions in a given sensory,\nemotional, or associative category. We investigate two types of haptic\ntokenizers, a frequency-based tokenizer and an EnCodec-based tokenizer, that\nconvert haptic signals into sequences of discrete units, enabling their\nintegration with the LLaMA model. HapticLLaMA is trained in two stages: (1)\nsupervised fine-tuning using the LLaMA architecture with LoRA-based adaptation,\nand (2) fine-tuning via reinforcement learning from human feedback (RLHF). We\nassess HapticLLaMA's captioning performance using both automated n-gram metrics\nand human evaluation. HapticLLaMA demonstrates strong capability in\ninterpreting haptic vibration signals, achieving a METEOR score of 59.98 and a\nBLEU-4 score of 32.06 respectively. Additionally, over 61% of the generated\ncaptions received human ratings above 3.5 on a 7-point scale, with RLHF\nyielding a 10% improvement in the overall rating distribution, indicating\nstronger alignment with human haptic perception. These findings highlight the\npotential of large language models to process and adapt to sensory data.", "AI": {"tldr": "HapticLLaMA\u662f\u4e00\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u7528\u4e8e\u5c06\u89e6\u89c9\u632f\u52a8\u4fe1\u53f7\u8f6c\u6362\u4e3a\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\uff0c\u5728\u865a\u62df\u73b0\u5b9e\u3001\u53ef\u8bbf\u95ee\u6027\u548c\u5eb7\u590d\u9886\u57df\u5177\u6709\u5e94\u7528\u6f5c\u529b\u3002", "motivation": "\u89e6\u89c9\u4fe1\u53f7\u5728\u865a\u62df\u73b0\u5b9e\u3001\u53ef\u8bbf\u95ee\u6027\u548c\u5eb7\u590d\u5e94\u7528\u4e2d\u5177\u6709\u91cd\u8981\u4f5c\u7528\uff0c\u4f46\u5148\u524d\u7684\u5f71\u54cd\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u89c6\u89c9\u548c\u542c\u89c9\u4e0a\uff0c\u89e6\u89c9\u4fe1\u53f7\u4ecd\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "HapticLLaMA\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u611f\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u5b83\u5c06\u632f\u52a8\u4fe1\u53f7\u89e3\u91ca\u4e3a\u7ed9\u5b9a\u611f\u89c9\u3001\u60c5\u611f\u6216\u8054\u60f3\u7c7b\u522b\u4e2d\u7684\u63cf\u8ff0\u3002\u5b83\u91c7\u7528\u57fa\u4e8e\u9891\u7387\u548c\u57fa\u4e8eEnCodec\u7684\u89e6\u89c9\u6807\u8bb0\u5668\u5c06\u89e6\u89c9\u4fe1\u53f7\u8f6c\u6362\u4e3a\u79bb\u6563\u5355\u5143\u5e8f\u5217\uff0c\u5e76\u4e0eLLaMA\u6a21\u578b\u96c6\u6210\u3002HapticLLaMA\u5206\u4e24\u4e2a\u9636\u6bb5\u8fdb\u884c\u8bad\u7ec3\uff1a1\uff09\u4f7f\u7528\u57fa\u4e8eLoRA\u7684\u9002\u914d\u7684LLaMA\u67b6\u6784\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff1b2\uff09\u901a\u8fc7\u4eba\u7c7b\u53cd\u9988\u5f3a\u5316\u5b66\u4e60\uff08RLHF\uff09\u8fdb\u884c\u5fae\u8c03\u3002", "result": "HapticLLaMA\u5728\u89e6\u89c9\u5b57\u5e55\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0cMETEOR\u5f97\u5206\u4e3a59.98\uff0cBLEU-4\u5f97\u5206\u4e3a32.06\u3002\u8d85\u8fc761%\u7684\u751f\u6210\u5b57\u5e55\u57287\u5206\u5236\u4e0a\u7684\u4eba\u7c7b\u8bc4\u5206\u4e3a3.5\u5206\u4ee5\u4e0a\uff0c\u5176\u4e2dRLHF\u4f7f\u6574\u4f53\u8bc4\u5206\u5206\u5e03\u63d0\u9ad8\u4e8610%\uff0c\u8868\u660e\u4e0e\u4eba\u7c7b\u89e6\u89c9\u611f\u77e5\u7684\u4e00\u81f4\u6027\u66f4\u5f3a\u3002", "conclusion": "HapticLLaMA\u5728\u89e3\u91ca\u89e6\u89c9\u632f\u52a8\u4fe1\u53f7\u65b9\u9762\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u80fd\u529b\uff0c\u8868\u660e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u548c\u9002\u5e94\u611f\u5b98\u6570\u636e\u65b9\u9762\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2508.06084", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06084", "abs": "https://arxiv.org/abs/2508.06084", "authors": ["Weichen Zhang", "Zhui Zhu", "Ningbo Li", "Kebin Liu", "Yunhao Liu"], "title": "AdaptInfer: Adaptive Token Pruning for Vision-Language Model Inference with Dynamical Text Guidance", "comment": null, "summary": "Vision-language models (VLMs) have achieved impressive performance on\nmultimodal reasoning tasks such as visual question answering (VQA), but their\ninference cost remains a significant challenge due to the large number of\nvision tokens processed during the prefill stage. Existing pruning methods\noften rely on directly using the attention patterns or static text prompt\nguidance, failing to exploit the dynamic internal signals generated during\ninference. To address these issues, we propose AdaptInfer, a plug-and-play\nframework for adaptive vision token pruning in VLMs. First, we introduce a\nfine-grained, dynamic text-guided pruning mechanism that reuses layer-wise\ntext-to-text attention maps to construct soft priors over text-token\nimportance, allowing more informed scoring of vision tokens at each stage.\nSecond, we perform an offline analysis of cross-modal attention shifts and\nidentify consistent inflection locations in inference, which inspire us to\npropose a more principled and efficient pruning schedule. Our method is\nlightweight and plug-and-play, also generalizable across multi-modal tasks.\nExperimental results have verified the effectiveness of the proposed method.\nFor example, it reduces CUDA latency by 61.3\\% while maintaining an average\naccuracy of 92.9\\% on vanilla LLaVA-1.5-7B. Under the same token budget,\nAdaptInfer surpasses SOTA in accuracy.", "AI": {"tldr": "AdaptInfer \u662f\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u4fee\u526a\u89c6\u89c9\u6807\u8bb0\u6765\u964d\u4f4e VLMs \u7684\u63a8\u7406\u6210\u672c\uff0c\u5176\u6548\u7387\u548c\u51c6\u786e\u6027\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u4fee\u526a\u65b9\u6cd5\u672a\u80fd\u5229\u7528\u63a8\u7406\u8fc7\u7a0b\u4e2d\u4ea7\u751f\u7684\u52a8\u6001\u5185\u90e8\u4fe1\u53f7\uff0c\u5bfc\u81f4\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u6210\u672c\u9ad8\u6602\uff0c\u5c24\u5176\u662f\u5728\u9884\u586b\u5145\u9636\u6bb5\u5904\u7406\u5927\u91cf\u89c6\u89c9\u6807\u8bb0\u65f6\u3002", "method": "AdaptInfer \u6846\u67b6\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u3001\u7ec6\u7c92\u5ea6\u7684\u3001\u52a8\u6001\u6587\u672c\u5f15\u5bfc\u7684\u4fee\u526a\u673a\u5236\uff0c\u901a\u8fc7\u590d\u7528\u5c42\u7ea7\u6587\u672c\u5230\u6587\u672c\u6ce8\u610f\u529b\u56fe\u6765\u6784\u5efa\u89c6\u89c9\u6807\u8bb0\u91cd\u8981\u6027\u7684\u8f6f\u5148\u9a8c\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u79bb\u7ebf\u5206\u6790\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u8f6c\u79fb\u7684\u3001\u66f4\u7b26\u5408\u539f\u5219\u4e14\u9ad8\u6548\u7684\u4fee\u526a\u8ba1\u5212\u3002", "result": "AdaptInfer \u6846\u67b6\u5728 LLaVA-1.5-7B \u6a21\u578b\u4e0a\u5b9e\u73b0\u4e86 61.3% \u7684 CUDA \u5ef6\u8fdf\u964d\u4f4e\uff0c\u540c\u65f6\u4fdd\u6301\u4e86 92.9% \u7684\u5e73\u5747\u51c6\u786e\u7387\u3002\u5728\u76f8\u540c\u7684\u6807\u8bb0\u9884\u7b97\u4e0b\uff0cAdaptInfer \u5728\u51c6\u786e\u7387\u65b9\u9762\u4e5f\u8d85\u8fc7\u4e86\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "AdaptInfer \u6846\u67b6\u80fd\u591f\u6709\u6548\u51cf\u5c11\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u7684\u63a8\u7406\u6210\u672c\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u89c6\u89c9\u6807\u8bb0\u7684\u4fee\u526a\uff0c\u5728\u51cf\u5c11 CUDA \u5ef6\u8fdf\u7684\u540c\u65f6\u4fdd\u6301\u9ad8\u51c6\u786e\u7387\uff0c\u5e76\u4e14\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002"}}
{"id": "2508.06361", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06361", "abs": "https://arxiv.org/abs/2508.06361", "authors": ["Zhaomin Wu", "Mingzhe Du", "See-Kiong Ng", "Bingsheng He"], "title": "Beyond Prompt-Induced Lies: Investigating LLM Deception on Benign Prompts", "comment": null, "summary": "Large Language Models (LLMs) have been widely deployed in reasoning,\nplanning, and decision-making tasks, making their trustworthiness a critical\nconcern. The potential for intentional deception, where an LLM deliberately\nfabricates or conceals information to serve a hidden objective, remains a\nsignificant and underexplored threat. Existing studies typically induce such\ndeception by explicitly setting a \"hidden\" objective through prompting or\nfine-tuning, which may not fully reflect real-world human-LLM interactions.\nMoving beyond this human-induced deception, we investigate LLMs' self-initiated\ndeception on benign prompts. To address the absence of ground truth in this\nevaluation, we propose a novel framework using \"contact searching questions.\"\nThis framework introduces two statistical metrics derived from psychological\nprinciples to quantify the likelihood of deception. The first, the Deceptive\nIntention Score, measures the model's bias towards a hidden objective. The\nsecond, Deceptive Behavior Score, measures the inconsistency between the LLM's\ninternal belief and its expressed output. Upon evaluating 14 leading LLMs, we\nfind that both metrics escalate as task difficulty increases, rising in\nparallel for most models. Building on these findings, we formulate a\nmathematical model to explain this behavior. These results reveal that even the\nmost advanced LLMs exhibit an increasing tendency toward deception when\nhandling complex problems, raising critical concerns for the deployment of LLM\nagents in complex and crucial domains.", "AI": {"tldr": "\u672c\u7814\u7a76\u9996\u6b21\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u6ca1\u6709\u660e\u786e\u6307\u4ee4\u7684\u60c5\u51b5\u4e0b\uff0c\u81ea\u4e3b\u4ea7\u751f\u6b3a\u9a97\u884c\u4e3a\u7684\u53ef\u80fd\u6027\u3002\u7814\u7a76\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u5e76\u5f00\u53d1\u4e86\u4e24\u4e2a\u91cf\u5316\u6b3a\u9a97\u503e\u5411\u7684\u6307\u6807\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u968f\u7740\u4efb\u52a1\u590d\u6742\u5ea6\u7684\u63d0\u9ad8\uff0cLLM\u7684\u6b3a\u9a97\u884c\u4e3a\u4f1a\u663e\u8457\u589e\u52a0\uff0c\u8fd9\u5bf9\u4e8eLLM\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u5e94\u7528\u6572\u54cd\u4e86\u8b66\u949f\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u901a\u8fc7\u63d0\u793a\u6216\u5fae\u8c03\u6765\u8bf1\u5bfcLLM\u6b3a\u9a97\uff0c\u8fd9\u53ef\u80fd\u65e0\u6cd5\u5b8c\u5168\u53cd\u6620\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u4eba\u673a\u4ea4\u4e92\u95ee\u9898\uff0c\u672c\u6587\u65e8\u5728\u7814\u7a76LLM\u5728\u65e0\u63d0\u793a\u6216\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\uff0c\u81ea\u4e3b\u53d1\u8d77\u6b3a\u9a97\u884c\u4e3a\u7684\u73b0\u8c61\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u4f7f\u7528\u201c\u63a5\u89e6\u641c\u7d22\u95ee\u9898\u201d\u6765\u8bc4\u4f30LLM\u5728\u65e0\u63d0\u793a\u6216\u5fae\u8c03\u7684\u6b3a\u9a97\u884c\u4e3a\u3002\u8be5\u6846\u67b6\u5f15\u5165\u4e86\u4e24\u4e2a\u57fa\u4e8e\u5fc3\u7406\u5b66\u539f\u7406\u7684\u7edf\u8ba1\u6307\u6807\uff1a\u6b3a\u9a97\u610f\u56fe\u5f97\u5206\uff08\u8861\u91cf\u6a21\u578b\u504f\u5411\u9690\u85cf\u76ee\u6807\u7684\u7a0b\u5ea6\uff09\u548c\u6b3a\u9a97\u884c\u4e3a\u5f97\u5206\uff08\u8861\u91cf\u6a21\u578b\u5185\u90e8\u4fe1\u5ff5\u4e0e\u5176\u8f93\u51fa\u4e4b\u95f4\u7684\u4e00\u81f4\u6027\uff09\u3002", "result": "\u5728\u5bf914\u4e2a\u9886\u5148\u7684LLM\u8fdb\u884c\u8bc4\u4f30\u540e\u53d1\u73b0\uff0c\u6b3a\u9a97\u610f\u56fe\u5f97\u5206\u548c\u6b3a\u9a97\u884c\u4e3a\u5f97\u5206\u4f1a\u968f\u7740\u4efb\u52a1\u96be\u5ea6\u7684\u589e\u52a0\u800c\u5347\u9ad8\uff0c\u5e76\u4e14\u5bf9\u5927\u591a\u6570\u6a21\u578b\u800c\u8a00\uff0c\u8fd9\u4e24\u79cd\u6307\u6807\u7684\u5347\u9ad8\u662f\u540c\u6b65\u7684\u3002\u57fa\u4e8e\u8fd9\u4e9b\u53d1\u73b0\uff0c\u7814\u7a76\u8005\u5efa\u7acb\u4e86\u4e00\u4e2a\u6570\u5b66\u6a21\u578b\u6765\u89e3\u91ca\u8fd9\u79cd\u884c\u4e3a\u3002", "conclusion": "\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u5904\u7406\u590d\u6742\u95ee\u9898\u65f6\uff0c\u4f1a\u8868\u73b0\u51fa\u8d8a\u6765\u8d8a\u5f3a\u7684\u6b3a\u9a97\u503e\u5411\uff0c\u8fd9\u5bf9\u5176\u5728\u5173\u952e\u9886\u57df\u7684\u90e8\u7f72\u63d0\u51fa\u4e86\u4e25\u5cfb\u7684\u6311\u6218\u3002"}}
{"id": "2508.06448", "categories": ["quant-ph", "physics.chem-ph"], "pdf": "https://arxiv.org/pdf/2508.06448", "abs": "https://arxiv.org/abs/2508.06448", "authors": ["Keith R. Fratus", "Nicklas Enenkel", "Sebastian Zanker", "Jan-Michael Reiner", "Michael Marthaler", "Peter Schmitteckert"], "title": "Can a Quantum Computer Simulate Nuclear Magnetic Resonance Spectra Better than a Classical One?", "comment": "11 pages, 12 figures main text; 5 pages, 6 figures, 1 table appendix", "summary": "The simulation of the spectra measured in nuclear magnetic resonance (NMR)\nspectroscopy experiments is a computationally non-trivial problem, and as such,\nit represents a problem for which a quantum computer may provide some practical\nadvantage over traditional computing methods. In order to understand the extent\nto which such problems may provide examples of useful quantum advantage, it is\nimportant to understand the limitations of existing classical simulation\nmethods. In this work, we benchmark our classical solver designed to solve such\nproblems. We find that it performs well, even beyond the common experimental\nparameter regimes, except for a specific molecule with certain unusual\nfeatures. We discuss what implications this may have for future efforts to\ndemonstrate quantum advantage in the context of NMR.", "AI": {"tldr": "\u7ecf\u5178\u6c42\u89e3\u5668\u5728\u6a21\u62dfNMR\u5149\u8c31\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u672a\u80fd\u5904\u7406\u6240\u6709\u5206\u5b50\uff0c\u8fd9\u53ef\u80fd\u5f71\u54cd\u91cf\u5b50\u8ba1\u7b97\u5728NMR\u9886\u57df\u7684\u4f18\u52bf\u5c55\u793a\u3002", "motivation": "\u4e3a\u4e86\u89e3NMR\u5149\u8c31\u6a21\u62df\u95ee\u9898\u5728\u591a\u5927\u7a0b\u5ea6\u4e0a\u80fd\u5c55\u793a\u91cf\u5b50\u8ba1\u7b97\u7684\u4f18\u52bf\uff0c\u6709\u5fc5\u8981\u4e86\u89e3\u73b0\u6709\u7ecf\u5178\u6a21\u62df\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "method": "\u901a\u8fc7\u57fa\u51c6\u6d4b\u8bd5\u6765\u8bc4\u4f30\u4e00\u4e2a\u7528\u4e8e\u6a21\u62dfNMR\u5149\u8c31\u5b9e\u9a8c\u7684\u7ecf\u5178\u6c42\u89e3\u5668\u7684\u6027\u80fd\u3002", "result": "\u8be5\u7ecf\u5178\u6c42\u89e3\u5668\u8868\u73b0\u826f\u597d\uff0c\u751a\u81f3\u8d85\u51fa\u4e86\u5e38\u89c4\u7684\u5b9e\u9a8c\u53c2\u6570\u8303\u56f4\uff0c\u4f46\u5728\u5904\u7406\u5177\u6709\u7279\u5b9a\u5f02\u5e38\u7279\u5f81\u7684\u5206\u5b50\u65f6\u9047\u5230\u4e86\u56f0\u96be\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u7ecf\u5178\u6c42\u89e3\u5668\u5728\u6a21\u62df\u6838\u78c1\u5171\u632f\uff08NMR\uff09\u5149\u8c31\u5b9e\u9a8c\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5176\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5904\u7406\u5177\u6709\u7279\u6b8a\u5f02\u5e38\u7279\u5f81\u7684\u5206\u5b50\u65f6\u5b58\u5728\u5c40\u9650\u6027\u3002\u8fd9\u5bf9\u4e8e\u672a\u6765\u6f14\u793a\u91cf\u5b50\u8ba1\u7b97\u5728NMR\u9886\u57df\u7684\u4f18\u52bf\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2508.06482", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06482", "abs": "https://arxiv.org/abs/2508.06482", "authors": ["Yilun Hua", "Evan Wang", "Yoav Artzi"], "title": "Post-training for Efficient Communication via Convention Formation", "comment": "Accepted to COLM 2025", "summary": "Humans communicate with increasing efficiency in multi-turn interactions, by\nadapting their language and forming ad-hoc conventions. In contrast, prior work\nshows that LLMs do not naturally show this behavior. We develop a post-training\nprocess to develop this ability through targeted fine-tuning on heuristically\nidentified demonstrations of convention formation. We evaluate with two new\nbenchmarks focused on this capability. First, we design a focused,\ncognitively-motivated interaction benchmark that consistently elicits strong\nconvention formation trends in humans. Second, we create a new\ndocument-grounded reference completion task that reflects in-the-wild\nconvention formation behavior. Our studies show significantly improved\nconvention formation abilities in post-trained LLMs across the two evaluation\nmethods.", "AI": {"tldr": "\u901a\u8fc7\u5fae\u8c03\uff0c\u53ef\u4ee5\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u5f62\u6210\u7ea6\u5b9a\u4fd7\u6210\u7684\u80fd\u529b\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8f6e\u4ea4\u4e92\u4e2d\u4e0d\u50cf\u4eba\u7c7b\u90a3\u6837\u80fd\u591f\u81ea\u7136\u5730\u9002\u5e94\u8bed\u8a00\u548c\u5f62\u6210\u7ea6\u5b9a\u4fd7\u6210\u3002", "method": "\u901a\u8fc7\u6709\u9488\u5bf9\u6027\u7684\u5fae\u8c03\uff0c\u5728\u542f\u53d1\u5f0f\u8bc6\u522b\u7684\u7ea6\u5b9a\u4fd7\u6210\u793a\u4f8b\u4e0a\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u540e\u8bad\u7ec3\u3002", "result": "\u540e\u8bad\u7ec3\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4e24\u4e2a\u8bc4\u4f30\u57fa\u51c6\u4e0a\u90fd\u8868\u73b0\u51fa\u663e\u8457\u63d0\u9ad8\u7684\u7ea6\u5b9a\u4fd7\u6210\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u6709\u9488\u5bf9\u6027\u7684\u5fae\u8c03\uff0c\u53ef\u4ee5\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u5f62\u6210\u548c\u9002\u5e94\u7ea6\u5b9a\u4fd7\u6210\u7684\u80fd\u529b\u3002"}}
{"id": "2508.06092", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06092", "abs": "https://arxiv.org/abs/2508.06092", "authors": ["Yachun Mi", "Yu Li", "Yanting Li", "Shixin Sun", "Chen Hui", "Tong Zhang", "Yuanyuan Liu", "Chenyue Song", "Shaohui Liu"], "title": "Q-CLIP: Unleashing the Power of Vision-Language Models for Video Quality Assessment through Unified Cross-Modal Adaptation", "comment": null, "summary": "Accurate and efficient Video Quality Assessment (VQA) has long been a key\nresearch challenge. Current mainstream VQA methods typically improve\nperformance by pretraining on large-scale classification datasets (e.g.,\nImageNet, Kinetics-400), followed by fine-tuning on VQA datasets. However, this\nstrategy presents two significant challenges: (1) merely transferring semantic\nknowledge learned from pretraining is insufficient for VQA, as video quality\ndepends on multiple factors (e.g., semantics, distortion, motion, aesthetics);\n(2) pretraining on large-scale datasets demands enormous computational\nresources, often dozens or even hundreds of times greater than training\ndirectly on VQA datasets. Recently, Vision-Language Models (VLMs) have shown\nremarkable generalization capabilities across a wide range of visual tasks, and\nhave begun to demonstrate promising potential in quality assessment. In this\nwork, we propose Q-CLIP, the first fully VLMs-based framework for VQA. Q-CLIP\nenhances both visual and textual representations through a Shared Cross-Modal\nAdapter (SCMA), which contains only a minimal number of trainable parameters\nand is the only component that requires training. This design significantly\nreduces computational cost. In addition, we introduce a set of five learnable\nquality-level prompts to guide the VLMs in perceiving subtle quality\nvariations, thereby further enhancing the model's sensitivity to video quality.\nFurthermore, we investigate the impact of different frame sampling strategies\non VQA performance, and find that frame-difference-based sampling leads to\nbetter generalization performance across datasets. Extensive experiments\ndemonstrate that Q-CLIP exhibits excellent performance on several VQA datasets.", "AI": {"tldr": "Q-CLIP\u662f\u4e00\u79cd\u521b\u65b0\u7684\u3001\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u89c6\u9891\u8d28\u91cf\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u5171\u4eab\u8de8\u6a21\u6001\u9002\u914d\u5668\u548c\u8d28\u91cf\u7b49\u7ea7\u63d0\u793a\uff0c\u5728\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bc4\u4f30\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709VQA\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u5927\u89c4\u6a21\u5206\u7c7b\u6570\u636e\u96c6\u9884\u8bad\u7ec3\uff0c\u7136\u540e\u8fdb\u884cVQA\u6570\u636e\u96c6\u5fae\u8c03\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u7b56\u7565\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u6311\u6218\uff1a1. \u9884\u8bad\u7ec3\u5b66\u4e60\u7684\u8bed\u4e49\u77e5\u8bc6\u4e0d\u8db3\u4ee5\u5e94\u5bf9\u89c6\u9891\u8d28\u91cf\u8bc4\u4f30\uff0c\u56e0\u4e3a\u89c6\u9891\u8d28\u91cf\u53d7\u8bed\u4e49\u3001\u5931\u771f\u3001\u8fd0\u52a8\u3001\u7f8e\u5b66\u7b49\u591a\u79cd\u56e0\u7d20\u5f71\u54cd\uff1b2. \u9884\u8bad\u7ec3\u9700\u8981\u5de8\u5927\u7684\u8ba1\u7b97\u8d44\u6e90\uff0c\u8fdc\u8d85\u76f4\u63a5\u5728VQA\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u4e14\u80fd\u5145\u5206\u8003\u8651\u89c6\u9891\u8d28\u91cf\u591a\u65b9\u9762\u56e0\u7d20\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aQ-CLIP\u7684\u3001\u9996\u4e2a\u5b8c\u5168\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u89c6\u9891\u8d28\u91cf\u8bc4\u4f30\uff08VQA\uff09\u6846\u67b6\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u4e00\u4e2a\u5171\u4eab\u8de8\u6a21\u6001\u9002\u914d\u5668\uff08SCMA\uff09\u6765\u589e\u5f3a\u89c6\u89c9\u548c\u6587\u672c\u8868\u793a\uff0cSCMA\u4ec5\u5305\u542b\u5c11\u91cf\u53ef\u8bad\u7ec3\u53c2\u6570\uff0c\u5927\u5927\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002\u6b64\u5916\uff0c\u5f15\u5165\u4e86\u4e94\u79cd\u53ef\u5b66\u4e60\u7684\u8d28\u91cf\u7b49\u7ea7\u63d0\u793a\uff0c\u4ee5\u5f15\u5bfcVLM\u611f\u77e5\u7ec6\u5fae\u7684\u8d28\u91cf\u5dee\u5f02\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u5bf9\u89c6\u9891\u8d28\u91cf\u7684\u654f\u611f\u6027\u3002\u540c\u65f6\uff0c\u7814\u7a76\u4e86\u4e0d\u540c\u5e27\u91c7\u6837\u7b56\u7565\u5bf9VQA\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u57fa\u4e8e\u5e27\u5dee\u7684\u91c7\u6837\u7b56\u7565\u80fd\u5e26\u6765\u66f4\u597d\u7684\u6cdb\u5316\u6027\u80fd\u3002", "result": "Q-CLIP\u6846\u67b6\u901a\u8fc7\u5171\u4eab\u8de8\u6a21\u6001\u9002\u914d\u5668\uff08SCMA\uff09\u548c\u8d28\u91cf\u7b49\u7ea7\u63d0\u793a\uff0c\u5728\u89c6\u9891\u8d28\u91cf\u8bc4\u4f30\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u3002\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0cQ-CLIP\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\uff0c\u5e76\u4e14\u5728\u591a\u4e2aVQA\u6570\u636e\u96c6\u4e0a\u5c55\u73b0\u4e86\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002\u57fa\u4e8e\u5e27\u5dee\u7684\u91c7\u6837\u7b56\u7565\u88ab\u8bc1\u660e\u6709\u52a9\u4e8e\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u6027\u80fd\u3002", "conclusion": "Q-CLIP\u6846\u67b6\u5728\u591a\u4e2a\u89c6\u9891\u8d28\u91cf\u8bc4\u4f30\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u89c6\u9891\u8d28\u91cf\u8bc4\u4f30\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u548c\u9ad8\u6548\u6027\u3002"}}
{"id": "2508.06364", "categories": ["cs.LG", "cs.AI", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2508.06364", "abs": "https://arxiv.org/abs/2508.06364", "authors": ["Renyi Zhou", "Huimin Zhu", "Jing Tang", "Min Li"], "title": "ActivityDiff: A diffusion model with Positive and Negative Activity Guidance for De Novo Drug Design", "comment": null, "summary": "Achieving precise control over a molecule's biological activity-encompassing\ntargeted activation/inhibition, cooperative multi-target modulation, and\noff-target toxicity mitigation-remains a critical challenge in de novo drug\ndesign. However, existing generative methods primarily focus on producing\nmolecules with a single desired activity, lacking integrated mechanisms for the\nsimultaneous management of multiple intended and unintended molecular\ninteractions. Here, we propose ActivityDiff, a generative approach based on the\nclassifier-guidance technique of diffusion models. It leverages separately\ntrained drug-target classifiers for both positive and negative guidance,\nenabling the model to enhance desired activities while minimizing harmful\noff-target effects. Experimental results show that ActivityDiff effectively\nhandles essential drug design tasks, including single-/dual-target generation,\nfragment-constrained dual-target design, selective generation to enhance target\nspecificity, and reduction of off-target effects. These results demonstrate the\neffectiveness of classifier-guided diffusion in balancing efficacy and safety\nin molecular design. Overall, our work introduces a novel paradigm for\nachieving integrated control over molecular activity, and provides ActivityDiff\nas a versatile and extensible framework.", "AI": {"tldr": "ActivityDiff\u662f\u4e00\u79cd\u65b0\u7684\u751f\u6210\u65b9\u6cd5\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u548c\u5206\u7c7b\u5668\u6307\u5bfc\u6765\u540c\u65f6\u63a7\u5236\u5206\u5b50\u7684\u591a\u79cd\u6d3b\u6027\uff0c\u589e\u5f3a\u836f\u7269\u8bbe\u8ba1\u7684\u6709\u6548\u6027\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u5728\u4ece\u5934\u836f\u7269\u8bbe\u8ba1\u4e2d\uff0c\u7cbe\u786e\u63a7\u5236\u5206\u5b50\u7684\u751f\u7269\u6d3b\u6027\uff08\u5305\u62ec\u9776\u5411\u6fc0\u6d3b/\u6291\u5236\u3001\u534f\u540c\u591a\u9776\u70b9\u8c03\u8282\u548c\u8131\u9776\u6bd2\u6027\u7f13\u89e3\uff09\u4ecd\u7136\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\u3002\u73b0\u6709\u7684\u751f\u6210\u65b9\u6cd5\u4e3b\u8981\u96c6\u4e2d\u4e8e\u4ea7\u751f\u5177\u6709\u5355\u4e00\u9884\u671f\u6d3b\u6027\u7684\u5206\u5b50\uff0c\u7f3a\u4e4f\u540c\u65f6\u7ba1\u7406\u591a\u79cd\u9884\u671f\u548c\u975e\u9884\u671f\u5206\u5b50\u76f8\u4e92\u4f5c\u7528\u7684\u96c6\u6210\u673a\u5236\u3002", "method": "ActivityDiff\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u5206\u7c7b\u5668\u6307\u5bfc\u6280\u672f\u7684\u65b0\u578b\u751f\u6210\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cActivityDiff\u80fd\u591f\u6709\u6548\u5730\u5904\u7406\u5173\u952e\u7684\u836f\u7269\u8bbe\u8ba1\u4efb\u52a1\uff0c\u5305\u62ec\u5355/\u53cc\u9776\u70b9\u751f\u6210\u3001\u7247\u6bb5\u7ea6\u675f\u7684\u53cc\u9776\u70b9\u8bbe\u8ba1\u3001\u589e\u5f3a\u9776\u70b9\u9009\u62e9\u6027\u7684\u9009\u62e9\u6027\u751f\u6210\u4ee5\u53ca\u51cf\u5c11\u8131\u9776\u6548\u5e94\u3002", "conclusion": "ActivityDiff\u901a\u8fc7\u7ed3\u5408\u591a\u79cd\u6d3b\u52a8\u6765\u589e\u5f3a\u5176\u6709\u6548\u6027\u548c\u5b89\u5168\u6027\uff0c\u662f\u4e00\u79cd\u591a\u529f\u80fd\u7684\u3001\u53ef\u6269\u5c55\u7684\u6846\u67b6\u3002"}}
{"id": "2508.06474", "categories": ["quant-ph", "physics.optics"], "pdf": "https://arxiv.org/pdf/2508.06474", "abs": "https://arxiv.org/abs/2508.06474", "authors": ["Shahrzad Taherizadegan", "Faezeh Kimiaee Asadi", "Jia-Wei Ji", "Daniel Higginbottom", "Christoph Simon"], "title": "Exploring the feasibility of probabilistic and deterministic quantum gates between T centers in silicon", "comment": null, "summary": "T center defects in silicon provide an attractive platform for quantum\ntechnologies due to their unique spin properties and compatibility with mature\nsilicon technologies. We investigate several gate protocols between single T\ncenters, including two probabilistic photon interference-based schemes, a\nnear-deterministic photon scattering gate, and a deterministic magnetic\ndipole-based scheme. In particular, we study a photon interference-based scheme\nwith feedback which can achieve success probabilities above 50%, and use the\nphoton-count decomposition method to perform the first analytical calculations\nof its entanglement fidelity and efficiency while accounting for imperfections.\nWe also calculate the fidelity and efficiency of the other schemes. Finally, we\ncompare the performance of all the schemes, considering current and near-future\nexperimental capabilities. In particular, we find that the photon\ninterference-based scheme with feedback has the potential to achieve\ncompetitive efficiency and fidelity, making it interesting to explore\nexperimentally.", "AI": {"tldr": "Silicon T center defects are good for quantum tech. This paper looks at different ways to link them up (gate protocols). A specific method using light interference with feedback looks promising, with over 50% success and good performance even with imperfections, making it worth trying in experiments.", "motivation": "The motivation behind this research is to explore and evaluate different gate protocols for T center defects in silicon, aiming to leverage their unique spin properties and compatibility with silicon technology for advancements in quantum technologies. The study specifically focuses on identifying protocols that can achieve high fidelity and efficiency, thereby assessing their viability for practical quantum applications.", "method": "This paper analyzes several gate protocols for T center defects in silicon, including two probabilistic photon interference-based schemes, a near-deterministic photon scattering gate, and a deterministic magnetic dipole-based scheme. Utilizing the photon-count decomposition method, the study provides the first analytical calculations of entanglement fidelity and efficiency for the feedback-assisted photon interference scheme, while also considering system imperfections. The fidelity and efficiency of the other schemes are similarly computed.", "result": "The study found that the photon interference-based scheme with feedback can achieve success probabilities greater than 50%. Analytical calculations using the photon-count decomposition method were performed to determine the entanglement fidelity and efficiency of this scheme, taking into account imperfections. The fidelity and efficiency of other schemes were also calculated, and a comparative analysis revealed that the photon interference-based scheme with feedback holds significant potential for achieving competitive efficiency and fidelity, making it a compelling candidate for experimental investigation.", "conclusion": "T center defects in silicon offer a promising avenue for quantum technologies owing to their spin characteristics and integration with existing silicon manufacturing processes. Among the investigated gate protocols, the photon interference-based scheme with feedback demonstrates considerable potential, achieving success probabilities exceeding 50% and showing competitive efficiency and fidelity in light of current and near-future experimental capacities."}}
{"id": "2508.06093", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06093", "abs": "https://arxiv.org/abs/2508.06093", "authors": ["Chen Zhu", "Buzhen Huang", "Zijing Wu", "Binghui Zuo", "Yangang Wang"], "title": "E-React: Towards Emotionally Controlled Synthesis of Human Reactions", "comment": null, "summary": "Emotion serves as an essential component in daily human interactions.\nExisting human motion generation frameworks do not consider the impact of\nemotions, which reduces naturalness and limits their application in interactive\ntasks, such as human reaction synthesis. In this work, we introduce a novel\ntask: generating diverse reaction motions in response to different emotional\ncues. However, learning emotion representation from limited motion data and\nincorporating it into a motion generation framework remains a challenging\nproblem. To address the above obstacles, we introduce a semi-supervised emotion\nprior in an actor-reactor diffusion model to facilitate emotion-driven reaction\nsynthesis. Specifically, based on the observation that motion clips within a\nshort sequence tend to share the same emotion, we first devise a\nsemi-supervised learning framework to train an emotion prior. With this prior,\nwe further train an actor-reactor diffusion model to generate reactions by\nconsidering both spatial interaction and emotional response. Finally, given a\nmotion sequence of an actor, our approach can generate realistic reactions\nunder various emotional conditions. Experimental results demonstrate that our\nmodel outperforms existing reaction generation methods. The code and data will\nbe made publicly available at https://ereact.github.io/", "AI": {"tldr": "This paper introduces a novel task of generating diverse reaction motions in response to different emotional cues by using a semi-supervised emotion prior in an actor-reactor diffusion model. The model can generate realistic reactions under various emotional conditions and outperforms existing methods.", "motivation": "Existing human motion generation frameworks do not consider the impact of emotions, reducing naturalness and limiting applications in interactive tasks like human reaction synthesis. The work aims to address the challenge of learning emotion representation from limited motion data and incorporating it into motion generation.", "method": "A semi-supervised emotion prior is introduced into an actor-reactor diffusion model. This model is trained to generate reactions by considering both spatial interaction and emotional response. The emotion prior is trained using a semi-supervised learning framework, leveraging the observation that motion clips within a short sequence tend to share the same emotion.", "result": "The proposed model outperforms existing reaction generation methods in experimental results.", "conclusion": "Given an actor's motion sequence, the model can generate realistic reactions under various emotional conditions, outperforming existing methods."}}
{"id": "2508.06387", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06387", "abs": "https://arxiv.org/abs/2508.06387", "authors": ["Anurag Tripathi", "Vaibhav Patle", "Abhinav Jain", "Ayush Pundir", "Sairam Menon", "Ajeet Kumar Singh"], "title": "End-to-End Text-to-SQL with Dataset Selection: Leveraging LLMs for Adaptive Query Generation", "comment": "Accepted in IJCNN25", "summary": "Text-to-SQL bridges the gap between natural language and structured database\nlanguage, thus allowing non-technical users to easily query databases.\nTraditional approaches model text-to-SQL as a direct translation task, where a\ngiven Natural Language Query (NLQ) is mapped to an SQL command. Recent advances\nin large language models (LLMs) have significantly improved translation\naccuracy, however, these methods all require that the target database is\npre-specified. This becomes problematic in scenarios with multiple extensive\ndatabases, where identifying the correct database becomes a crucial yet\noverlooked step. In this paper, we propose a three-stage end-to-end text-to-SQL\nframework to identify the user's intended database before generating SQL\nqueries. Our approach leverages LLMs and prompt engineering to extract implicit\ninformation from natural language queries (NLQs) in the form of a ruleset. We\nthen train a large db\\_id prediction model, which includes a RoBERTa-based\nfinetuned encoder, to predict the correct Database identifier (db\\_id) based on\nboth the NLQ and the LLM-generated rules. Finally, we refine the generated SQL\nby using critic agents to correct errors. Experimental results demonstrate that\nour framework outperforms the current state-of-the-art models in both database\nintent prediction and SQL generation accuracy.", "AI": {"tldr": "\u4e00\u4e2a\u80fd\u8bc6\u522b\u7528\u6237\u610f\u56fe\u6570\u636e\u5e93\u7684Text-to-SQL\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u65b9\u6cd5\uff08\u89c4\u5219\u63d0\u53d6\u3001\u6570\u636e\u5e93ID\u9884\u6d4b\u3001SQL\u4fee\u6b63\uff09\u63d0\u5347\u4e86\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u7684Text-to-SQL\u65b9\u6cd5\u9700\u8981\u9884\u5148\u6307\u5b9a\u76ee\u6807\u6570\u636e\u5e93\uff0c\u8fd9\u5728\u5b58\u5728\u591a\u4e2a\u5927\u578b\u6570\u636e\u5e93\u7684\u60c5\u51b5\u4e0b\u662f\u4e0d\u5207\u5b9e\u9645\u7684\uff0c\u800c\u8bc6\u522b\u6b63\u786e\u7684\u6570\u636e\u5e93\u662f\u81f3\u5173\u91cd\u8981\u4f46\u88ab\u5ffd\u89c6\u7684\u6b65\u9aa4\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u4e09\u9636\u6bb5\u7aef\u5230\u7aefText-to-SQL\u6846\u67b6\uff0c\u9996\u5148\u5229\u7528LLM\u548c\u63d0\u793a\u5de5\u7a0b\u4ece\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\uff08NLQ\uff09\u4e2d\u63d0\u53d6\u89c4\u5219\u96c6\uff0c\u7136\u540e\u8bad\u7ec3\u4e00\u4e2a\u57fa\u4e8eRoBERTa\u7684finetune\u7f16\u7801\u5668\u7684\u5927\u578bdb_id\u9884\u6d4b\u6a21\u578b\u6765\u9884\u6d4b\u6b63\u786e\u7684\u6570\u636e\u5e93\u6807\u8bc6\u7b26\uff08db_id\uff09\uff0c\u6700\u540e\u4f7f\u7528critic\u4ee3\u7406\u6765\u7ea0\u6b63\u751f\u6210\u7684SQL\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u6570\u636e\u5e93\u610f\u56fe\u9884\u6d4b\u548cSQL\u751f\u6210\u51c6\u786e\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u6a21\u578b\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u6570\u636e\u5e93\u610f\u56fe\u9884\u6d4b\u548cSQL\u751f\u6210\u51c6\u786e\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u6a21\u578b\u3002"}}
{"id": "2508.06101", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06101", "abs": "https://arxiv.org/abs/2508.06101", "authors": ["Yachun Mi", "Xingyang He", "Shixin Sun", "Yu Li", "Yanting Li", "Zhixuan Li", "Jian Jin", "Chen Hui", "Shaohui Liu"], "title": "UGD-IML: A Unified Generative Diffusion-based Framework for Constrained and Unconstrained Image Manipulation Localization", "comment": null, "summary": "In the digital age, advanced image editing tools pose a serious threat to the\nintegrity of visual content, making image forgery detection and localization a\nkey research focus. Most existing Image Manipulation Localization (IML) methods\nrely on discriminative learning and require large, high-quality annotated\ndatasets. However, current datasets lack sufficient scale and diversity,\nlimiting model performance in real-world scenarios. To overcome this, recent\nstudies have explored Constrained IML (CIML), which generates pixel-level\nannotations through algorithmic supervision. However, existing CIML approaches\noften depend on complex multi-stage pipelines, making the annotation process\ninefficient. In this work, we propose a novel generative framework based on\ndiffusion models, named UGD-IML, which for the first time unifies both IML and\nCIML tasks within a single framework. By learning the underlying data\ndistribution, generative diffusion models inherently reduce the reliance on\nlarge-scale labeled datasets, allowing our approach to perform effectively even\nunder limited data conditions. In addition, by leveraging a class embedding\nmechanism and a parameter-sharing design, our model seamlessly switches between\nIML and CIML modes without extra components or training overhead. Furthermore,\nthe end-to-end design enables our model to avoid cumbersome steps in the data\nannotation process. Extensive experimental results on multiple datasets\ndemonstrate that UGD-IML outperforms the SOTA methods by an average of 9.66 and\n4.36 in terms of F1 metrics for IML and CIML tasks, respectively. Moreover, the\nproposed method also excels in uncertainty estimation, visualization and\nrobustness.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a UGD-IML \u7684\u65b0\u9896\u751f\u6210\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u6269\u6563\u6a21\u578b\u7edf\u4e00\u4e86\u56fe\u50cf\u7be1\u6539\u5b9a\u4f4d (IML) \u548c\u7ea6\u675f\u56fe\u50cf\u7be1\u6539\u5b9a\u4f4d (CIML) \u4efb\u52a1\u3002\u8be5\u65b9\u6cd5\u51cf\u5c11\u4e86\u5bf9\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u96c6\u7684\u4f9d\u8d56\uff0c\u63d0\u9ad8\u4e86\u6548\u7387\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u7be1\u6539\u5b9a\u4f4d\u65b9\u6cd5\u4f9d\u8d56\u5224\u522b\u5f0f\u5b66\u4e60\u548c\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u4f46\u5f53\u524d\u6570\u636e\u96c6\u89c4\u6a21\u548c\u591a\u6837\u6027\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u6a21\u578b\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u8868\u73b0\u3002\u867d\u7136 CIML \u65b9\u6cd5\u901a\u8fc7\u7b97\u6cd5\u76d1\u7763\u751f\u6210\u50cf\u7d20\u7ea7\u6807\u6ce8\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u590d\u6742\u7684\u591a\u9636\u6bb5\u6d41\u6c34\u7ebf\uff0c\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u6846\u67b6 UGD-IML\uff0c\u8be5\u6846\u67b6\u7edf\u4e00\u4e86 IML \u548c CIML \u4efb\u52a1\uff0c\u5e76\u901a\u8fc7\u7c7b\u522b\u5d4c\u5165\u548c\u53c2\u6570\u5171\u4eab\u673a\u5236\u5b9e\u73b0\u4e86\u6a21\u5f0f\u5207\u6362\uff0c\u91c7\u7528\u7aef\u5230\u7aef\u8bbe\u8ba1\u4ee5\u7b80\u5316\u6570\u636e\u6807\u6ce8\u6d41\u7a0b\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cUGD-IML \u5728 IML \u548c CIML \u4efb\u52a1\u7684 F1 \u6307\u6807\u4e0a\u5206\u522b\u6bd4\u73b0\u6709\u65b9\u6cd5\u5e73\u5747\u9ad8\u51fa 9.66 \u548c 4.36\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u5728\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u3001\u53ef\u89c6\u5316\u548c\u9c81\u68d2\u6027\u65b9\u9762\u4e5f\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "UGD-IML \u6846\u67b6\u80fd\u591f\u6709\u6548\u5730\u540c\u65f6\u5904\u7406\u56fe\u50cf\u7be1\u6539\u5b9a\u4f4d (IML) \u548c\u7ea6\u675f\u56fe\u50cf\u7be1\u6539\u5b9a\u4f4d (CIML) \u4efb\u52a1\uff0c\u5e76\u4e14\u5728\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2508.06409", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06409", "abs": "https://arxiv.org/abs/2508.06409", "authors": ["Wooyong Jung", "Sola Kim", "Dongwook Kim", "Maryam Tabar", "Dongwon Lee"], "title": "A New Lens on Homelessness: Daily Tent Monitoring with 311 Calls and Street Images", "comment": "10 pages, Accepted to SBP-BRiMS 2025", "summary": "Homelessness in the United States has surged to levels unseen since the Great\nDepression. However, existing methods for monitoring it, such as point-in-time\n(PIT) counts, have limitations in terms of frequency, consistency, and spatial\ndetail. This study proposes a new approach using publicly available,\ncrowdsourced data, specifically 311 Service Calls and street-level imagery, to\ntrack and forecast homeless tent trends in San Francisco. Our predictive model\ncaptures fine-grained daily and neighborhood-level variations, uncovering\npatterns that traditional counts often overlook, such as rapid fluctuations\nduring the COVID-19 pandemic and spatial shifts in tent locations over time. By\nproviding more timely, localized, and cost-effective information, this approach\nserves as a valuable tool for guiding policy responses and evaluating\ninterventions aimed at reducing unsheltered homelessness.", "AI": {"tldr": "\u5229\u7528311\u7535\u8bdd\u548c\u8857\u666f\u56fe\u50cf\u6570\u636e\uff0c\u5f00\u53d1\u65b0\u6a21\u578b\u8ffd\u8e2a\u65e7\u91d1\u5c71\u65e0\u5bb6\u53ef\u5f52\u8005\u5e10\u7bf7\u8d8b\u52bf\uff0c\u6bd4\u4f20\u7edf\u65b9\u6cd5\u66f4\u53ca\u65f6\u3001\u7ec6\u81f4\u3002", "motivation": "\u7f8e\u56fd\u65e0\u5bb6\u53ef\u5f52\u73b0\u8c61\u65e5\u76ca\u4e25\u91cd\uff0c\u800c\u73b0\u6709\u7684\u76d1\u6d4b\u65b9\u6cd5\uff08\u5982\u7279\u5b9a\u65f6\u95f4\u70b9\u666e\u67e5\uff09\u5728\u9891\u7387\u3001\u4e00\u81f4\u6027\u548c\u7a7a\u95f4\u7ec6\u8282\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002\u56e0\u6b64\uff0c\u6709\u5fc5\u8981\u5f00\u53d1\u65b0\u7684\u65b9\u6cd5\u6765\u66f4\u6709\u6548\u5730\u76d1\u6d4b\u65e0\u5bb6\u53ef\u5f52\u73b0\u8c61\u3002", "method": "\u672c\u7814\u7a76\u5229\u7528\u516c\u5f00\u7684\u4f17\u5305\u6570\u636e\uff0c\u5305\u62ec311\u670d\u52a1\u7535\u8bdd\u548c\u8857\u666f\u56fe\u50cf\uff0c\u6765\u6784\u5efa\u4e00\u4e2a\u9884\u6d4b\u6a21\u578b\uff0c\u4ee5\u8ffd\u8e2a\u548c\u9884\u6d4b\u65e7\u91d1\u5c71\u65e0\u5bb6\u53ef\u5f52\u8005\u5e10\u7bf7\u7684\u8d8b\u52bf\u3002", "result": "\u7814\u7a76\u63d0\u51fa\u7684\u9884\u6d4b\u6a21\u578b\u80fd\u591f\u6355\u6349\u5230\u6bcf\u65e5\u548c\u793e\u533a\u5c42\u9762\u7684\u7ec6\u5fae\u53d8\u5316\uff0c\u5e76\u53d1\u73b0\u4e86\u4f20\u7edf\u666e\u67e5\u65b9\u6cd5\u5e38\u5e38\u5ffd\u7565\u7684\u6a21\u5f0f\uff0c\u4f8b\u5982\u5728COVID-19\u5927\u6d41\u884c\u671f\u95f4\u7684\u5feb\u901f\u6ce2\u52a8\u4ee5\u53ca\u5e10\u7bf7\u4f4d\u7f6e\u968f\u65f6\u95f4\u7684\u7a7a\u95f4\u8f6c\u79fb\u3002", "conclusion": "\u901a\u8fc7\u5229\u7528311\u670d\u52a1\u7535\u8bdd\u548c\u8857\u666f\u56fe\u50cf\u7b49\u516c\u5f00\u7684\u4f17\u5305\u6570\u636e\uff0c\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u65b9\u6cd5\u6765\u8ffd\u8e2a\u548c\u9884\u6d4b\u65e7\u91d1\u5c71\u7684\u65e0\u5bb6\u53ef\u5f52\u8005\u5e10\u7bf7\u8d8b\u52bf\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u6355\u6349\u7ec6\u7c92\u5ea6\u7684\u6bcf\u65e5\u548c\u793e\u533a\u7ea7\u522b\u53d8\u5316\uff0c\u63ed\u793a\u4e86\u4f20\u7edf\u65b9\u6cd5\uff08\u5982\u7279\u5b9a\u65f6\u95f4\u70b9\u666e\u67e5\uff09\u96be\u4ee5\u53d1\u73b0\u7684\u6a21\u5f0f\uff0c\u4f8b\u5982\u5728COVID-19\u5927\u6d41\u884c\u671f\u95f4\u7684\u5feb\u901f\u6ce2\u52a8\u4ee5\u53ca\u5e10\u7bf7\u4f4d\u7f6e\u7684\u65f6\u7a7a\u8f6c\u79fb\u3002\u56e0\u6b64\uff0c\u8be5\u65b9\u6cd5\u4e3a\u6307\u5bfc\u653f\u7b56\u54cd\u5e94\u548c\u8bc4\u4f30\u65e8\u5728\u51cf\u5c11\u65e0\u5bb6\u53ef\u5f52\u73b0\u8c61\u7684\u5e72\u9884\u63aa\u65bd\u63d0\u4f9b\u4e86\u53ca\u65f6\u3001\u672c\u5730\u5316\u4e14\u7ecf\u6d4e\u9ad8\u6548\u7684\u4fe1\u606f\u652f\u6301\u3002"}}
{"id": "2508.06104", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06104", "abs": "https://arxiv.org/abs/2508.06104", "authors": ["Gui Zou", "Chaofan Gan", "Chern Hong Lim", "Supavadee Aramvith", "Weiyao Lin"], "title": "MCA: 2D-3D Retrieval with Noisy Labels via Multi-level Adaptive Correction and Alignment", "comment": "ICMEW 2025", "summary": "With the increasing availability of 2D and 3D data, significant advancements\nhave been made in the field of cross-modal retrieval. Nevertheless, the\nexistence of imperfect annotations presents considerable challenges, demanding\nrobust solutions for 2D-3D cross-modal retrieval in the presence of noisy label\nconditions. Existing methods generally address the issue of noise by dividing\nsamples independently within each modality, making them susceptible to\noverfitting on corrupted labels. To address these issues, we propose a robust\n2D-3D \\textbf{M}ulti-level cross-modal adaptive \\textbf{C}orrection and\n\\textbf{A}lignment framework (MCA). Specifically, we introduce a Multimodal\nJoint label Correction (MJC) mechanism that leverages multimodal historical\nself-predictions to jointly model the modality prediction consistency, enabling\nreliable label refinement. Additionally, we propose a Multi-level Adaptive\nAlignment (MAA) strategy to effectively enhance cross-modal feature semantics\nand discrimination across different levels. Extensive experiments demonstrate\nthe superiority of our method, MCA, which achieves state-of-the-art performance\non both conventional and realistic noisy 3D benchmarks, highlighting its\ngenerality and effectiveness.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMCA\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u5b58\u5728\u566a\u58f0\u6807\u7b7e\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c2D-3D\u8de8\u6a21\u6001\u68c0\u7d22\u3002MCA\u901a\u8fc7\u591a\u6a21\u6001\u8054\u5408\u6807\u7b7e\u6821\u6b63\uff08MJC\uff09\u548c\u591a\u7ea7\u81ea\u9002\u5e94\u5bf9\u9f50\uff08MAA\uff09\u6765\u63d0\u9ad8\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u566a\u58f0\u6807\u7b7e\u65f6\uff0c\u901a\u5e38\u72ec\u7acb\u5730\u5bf9\u6bcf\u4e2a\u6a21\u6001\u4e2d\u7684\u6837\u672c\u8fdb\u884c\u5904\u7406\uff0c\u5bb9\u6613\u5bfc\u81f4\u8fc7\u62df\u5408\u4e8e\u88ab\u7834\u574f\u7684\u6807\u7b7e\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u9c81\u68d2\u76842D-3D\u591a\u7ea7\u522b\u8de8\u6a21\u6001\u81ea\u9002\u5e94\u6821\u6b63\u4e0e\u5bf9\u9f50\u6846\u67b6\uff08MCA\uff09\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u8054\u5408\u6807\u7b7e\u6821\u6b63\uff08MJC\uff09\u673a\u5236\uff0c\u5229\u7528\u591a\u6a21\u6001\u5386\u53f2\u81ea\u6211\u9884\u6d4b\u6765\u8054\u5408\u5efa\u6a21\u6a21\u6001\u9884\u6d4b\u4e00\u81f4\u6027\uff0c\u5b9e\u73b0\u53ef\u9760\u7684\u6807\u7b7e\u7ec6\u5316\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u7ea7\u81ea\u9002\u5e94\u5bf9\u9f50\uff08MAA\uff09\u7b56\u7565\uff0c\u4ee5\u6709\u6548\u589e\u5f3a\u4e0d\u540c\u7ea7\u522b\u7684\u8de8\u6a21\u6001\u7279\u5f81\u8bed\u4e49\u548c\u5224\u522b\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMCA\u5728\u5e38\u89c4\u548c\u771f\u5b9e\u566a\u58f03D\u57fa\u51c6\u4e0a\u5747\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "MCA\u5728\u5e38\u89c4\u548c\u771f\u5b9e\u566a\u58f03D\u57fa\u51c6\u4e0a\u5747 achieves state-of-the-art performance\uff0c\u8bc1\u660e\u4e86\u5176\u901a\u7528\u6027\u548c\u6709\u6548\u6027\u3002"}}
{"id": "2508.06412", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06412", "abs": "https://arxiv.org/abs/2508.06412", "authors": ["Zichuan Liu", "Jinyu Wang", "Lei Song", "Jiang Bian"], "title": "Sample-efficient LLM Optimization with Reset Replay", "comment": null, "summary": "Recent advancements in post-training Large Language Models (LLMs),\nparticularly through Reinforcement Learning (RL) and preference optimization\nmethods, are key drivers for enhancing their reasoning capabilities. However,\nthese methods are often plagued by low sample efficiency and a susceptibility\nto primacy bias, where overfitting to initial experiences degrades policy\nquality and damages the learning process. To address these challenges, we\nintroduce LLM optimization with Reset Replay (LoRR), a general and powerful\nplugin designed to enhance sample efficiency in any preference-based\noptimization framework. LoRR core mechanism enables training at a high replay\nnumber, maximizing the utility of each collected data batch. To counteract the\nrisk of overfitting inherent in high-replay training, LoRR incorporates a\nperiodic reset strategy with reusing initial data, which preserves network\nplasticity. Furthermore, it leverages a hybrid optimization objective,\ncombining supervised fine-tuning (SFT) and preference-based losses to further\nbolster data exploitation. Our extensive experiments demonstrate that LoRR\nsignificantly boosts the performance of various preference optimization methods\non both mathematical and general reasoning benchmarks. Notably, an iterative\nDPO approach augmented with LoRR achieves comparable performance on challenging\nmath tasks, outperforming some complex and computationally intensive RL-based\nalgorithms. These findings highlight that LoRR offers a practical,\nsample-efficient, and highly effective paradigm for LLM finetuning, unlocking\ngreater performance from limited data.", "AI": {"tldr": "LoRR \u662f\u4e00\u79cd\u7528\u4e8e LLM \u5fae\u8c03\u7684\u6837\u672c\u9ad8\u6548\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u653e\u548c\u91cd\u7f6e\u7b56\u7565\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u63a8\u7406\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6210\u679c\u3002", "motivation": "\u73b0\u6709\u7684 LLM \u5fae\u8c03\u65b9\u6cd5\uff08\u5982 RL \u548c\u504f\u597d\u4f18\u5316\uff09\u5b58\u5728\u6837\u672c\u6548\u7387\u4f4e\u548c\u6613\u51fa\u73b0\u9996\u56e0\u504f\u5dee\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5728\u63a8\u7406\u80fd\u529b\u4e0a\u7684\u63d0\u5347\u3002", "method": "LoRR \u662f\u4e00\u79cd\u901a\u7528\u63d2\u4ef6\uff0c\u901a\u8fc7\u9ad8\u56de\u653e\u6570\u8bad\u7ec3\u3001\u5468\u671f\u6027\u91cd\u7f6e\u7b56\u7565\uff08\u5305\u542b\u521d\u59cb\u6570\u636e\u518d\u5229\u7528\uff09\u548c\u7ed3\u5408 SFT \u4e0e\u57fa\u4e8e\u504f\u597d\u7684\u635f\u5931\u7684\u6df7\u5408\u4f18\u5316\u76ee\u6807\u6765\u63d0\u9ad8\u6837\u672c\u6548\u7387\uff0c\u540c\u65f6\u9632\u6b62\u8fc7\u62df\u5408\u3002", "result": "LoRR \u663e\u8457\u63d0\u9ad8\u4e86\u5404\u79cd\u504f\u597d\u4f18\u5316\u65b9\u6cd5\u5728\u6570\u5b66\u548c\u901a\u7528\u63a8\u7406\u57fa\u51c6\u4e0a\u7684\u6027\u80fd\u3002\u7279\u522b\u662f\uff0c\u96c6\u6210\u4e86 LoRR \u7684\u8fed\u4ee3 DPO \u65b9\u6cd5\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u6570\u5b66\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u4e0e\u4e00\u4e9b\u590d\u6742\u4e14\u8ba1\u7b97\u91cf\u5927\u7684 RL \u7b97\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u751a\u81f3\u8d85\u8d8a\u4e86\u5b83\u4eec\u3002", "conclusion": "LoRR \u901a\u8fc7\u63d0\u9ad8\u6837\u672c\u6548\u7387\u548c\u514b\u670d\u9996\u56e0\u504f\u5dee\u6765\u589e\u5f3a LLM \u7684\u5fae\u8c03\uff0c\u5373\u4f7f\u5728\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u663e\u8457\u63d0\u9ad8\u6027\u80fd\u3002"}}
{"id": "2508.06107", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06107", "abs": "https://arxiv.org/abs/2508.06107", "authors": ["Shree Mitra", "Ritabrata Chakraborty", "Nilkanta Sahu"], "title": "Mask & Match: Learning to Recognize Handwritten Math with Self-Supervised Attention", "comment": null, "summary": "Recognizing handwritten mathematical expressions (HMER) is a challenging task\ndue to the inherent two-dimensional structure, varying symbol scales, and\ncomplex spatial relationships among symbols. In this paper, we present a\nself-supervised learning (SSL) framework for HMER that eliminates the need for\nexpensive labeled data. Our approach begins by pretraining an image encoder\nusing a combination of global and local contrastive loss, enabling the model to\nlearn both holistic and fine-grained representations. A key contribution of\nthis work is a novel self-supervised attention network, which is trained using\na progressive spatial masking strategy. This attention mechanism is designed to\nlearn semantically meaningful focus regions, such as operators, exponents, and\nnested mathematical notation, without requiring any supervision. The\nprogressive masking curriculum encourages the network to become increasingly\nrobust to missing or occluded visual information, ultimately improving\nstructural understanding. Our complete pipeline consists of (1) self-supervised\npretraining of the encoder, (2) self-supervised attention learning, and (3)\nsupervised fine-tuning with a transformer decoder to generate LATEX sequences.\nExtensive experiments on CROHME benchmarks demonstrate that our method\noutperforms existing SSL and fully supervised baselines, validating the\neffectiveness of our progressive attention mechanism in enhancing HMER\nperformance. Our codebase can be found here.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u624b\u5199\u6570\u5b66\u8868\u8fbe\u5f0f\u8bc6\u522b\uff08HMER\uff09\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u5bf9\u6bd4\u635f\u5931\u548c\u6e10\u8fdb\u5f0f\u7a7a\u95f4\u63a9\u853d\u7b56\u7565\u7684\u6ce8\u610f\u673a\u5236\uff0c\u65e0\u9700\u6807\u6ce8\u6570\u636e\u5373\u53ef\u6709\u6548\u5b66\u4e60\u548c\u8bc6\u522b\u590d\u6742\u7684\u6570\u5b66\u8868\u8fbe\u5f0f\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u624b\u5199\u6570\u5b66\u8868\u8fbe\u5f0f\u8bc6\u522b\uff08HMER\uff09\u4efb\u52a1\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u5176\u56fa\u6709\u7684\u4e8c\u7ef4\u7ed3\u6784\u3001\u53d8\u5316\u7684\u7b26\u53f7\u5c3a\u5ea6\u548c\u590d\u6742\u7684\u7a7a\u95f4\u5173\u7cfb\u3002\u672c\u5de5\u4f5c\u65e8\u5728\u901a\u8fc7SSL\u6846\u67b6\u6d88\u9664\u5bf9\u6602\u8d35\u6807\u6ce8\u6570\u636e\u7684\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u624b\u5199\u6570\u5b66\u8868\u8fbe\u5f0f\u8bc6\u522b\uff08HMER\uff09\u7684\u81ea\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\u6846\u67b6\uff0c\u5305\u62ec\uff081\uff09\u56fe\u50cf\u7f16\u7801\u5668\u7684\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\uff08\u7ed3\u5408\u5168\u5c40\u548c\u5c40\u90e8\u5bf9\u6bd4\u635f\u5931\uff09\uff0c\uff082\uff09\u65b0\u9896\u7684\u81ea\u76d1\u7763\u6ce8\u610f\u7f51\u7edc\uff08\u4f7f\u7528\u6e10\u8fdb\u5f0f\u7a7a\u95f4\u63a9\u853d\u7b56\u7565\u8bad\u7ec3\uff09\uff0c\u4ee5\u53ca\uff083\uff09\u5e26\u6709Transformer\u89e3\u7801\u5668\u7684\u76d1\u7763\u5fae\u8c03\u4ee5\u751f\u6210LATEX\u5e8f\u5217\u3002", "result": "\u8be5SSL\u6846\u67b6\u901a\u8fc7\u6e10\u8fdb\u5f0f\u7a7a\u95f4\u63a9\u853d\u7b56\u7565\u8bad\u7ec3\u51fa\u7684\u6ce8\u610f\u673a\u5236\uff0c\u80fd\u591f\u5b66\u4e60\u5230\u6709\u610f\u4e49\u7684\u8bed\u4e49\u5173\u6ce8\u533a\u57df\uff08\u5982\u8fd0\u7b97\u7b26\u3001\u6307\u6570\u548c\u5d4c\u5957\u6570\u5b66\u7b26\u53f7\uff09\uff0c\u800c\u65e0\u9700\u4efb\u4f55\u76d1\u7763\u3002\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728CROHME\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728CROHME\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u7684\u81ea\u76d1\u7763\u548c\u5168\u76d1\u7763\u57fa\u7ebf\uff0c\u9a8c\u8bc1\u4e86\u5176\u6e10\u8fdb\u5f0f\u6ce8\u610f\u673a\u5236\u5728\u63d0\u9ad8HMER\u6027\u80fd\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.06467", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06467", "abs": "https://arxiv.org/abs/2508.06467", "authors": ["Ameya Anjarlekar", "Sandeep Pombra"], "title": "LLM Unlearning using Gradient Ratio-Based Influence Estimation and Noise Injection", "comment": "14 Pages, 3 Figures, 11 Tables", "summary": "The growing legal and ethical scrutiny of large language models (LLMs)\nnecessitates effective machine unlearning, particularly for sensitive or\nunauthorized data. Existing empirical methods often yield incomplete forgetting\nor unintended degradation of unrelated knowledge due to poor localization. In\nthis work, we propose GRIN: a modular and targeted framework for LLM\nunlearning. GRIN introduces a novel gradient-ratio-based metric to identify\nparameters most responsible for memorizing forget data. We then perform\nselective noise injection into these parameters prior to fine-tuning, which\nimproves unlearning performance while maintaining model utility. Finally, we\npropose new evaluation metrics tailored to the LLM setting and validate our\napproach on standard benchmarks such as TOFU, WMDP, and SafePKU.", "AI": {"tldr": "GRIN\u662f\u4e00\u79cd\u7528\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u65b0\u578b\u89e3\u79bb\u6846\u67b6\uff0c\u901a\u8fc7\u8bc6\u522b\u548c\u9009\u62e9\u6027\u5730\u5904\u7406\u5bf9\u9057\u5fd8\u6570\u636e\u8d1f\u6709\u8d23\u4efb\u7684\u53c2\u6570\uff0c\u4ee5\u5b9e\u73b0\u66f4\u6709\u6548\u7684\u9057\u5fd8\uff0c\u540c\u65f6\u6700\u5927\u9650\u5ea6\u5730\u4fdd\u7559\u6a21\u578b\u6548\u7528\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u89e3\u79bb\u65b9\u6cd5\u5b58\u5728\u9057\u5fd8\u4e0d\u5b8c\u5168\u6216\u7834\u574f\u65e0\u5173\u77e5\u8bc6\u7684\u7f3a\u70b9\uff0c\u800cLLM\u7684\u6cd5\u5f8b\u548c\u4f26\u7406\u5ba1\u67e5\u65e5\u76ca\u4e25\u683c\uff0c\u56e0\u6b64\u9700\u8981\u6709\u6548\u7684\u6a21\u578b\u89e3\u79bb\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u654f\u611f\u6216\u672a\u7ecf\u6388\u6743\u7684\u6570\u636e\u65f6\u3002", "method": "GRIN\u6846\u67b6\u901a\u8fc7\u4ee5\u4e0b\u65b9\u5f0f\u5b9e\u73b0\u6a21\u578b\u89e3\u79bb\uff1a1. \u63d0\u51fa\u65b0\u9896\u7684\u57fa\u4e8e\u68af\u5ea6\u6bd4\u7387\u7684\u5ea6\u91cf\uff0c\u4ee5\u8bc6\u522b\u5bf9\u8bb0\u5fc6\u9057\u5fd8\u6570\u636e\u6700\u8d1f\u8d23\u7684\u53c2\u6570\u30022. \u5bf9\u8bc6\u522b\u51fa\u7684\u53c2\u6570\u8fdb\u884c\u9009\u62e9\u6027\u566a\u58f0\u6ce8\u5165\u30023. \u5bf9\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u30024. \u63d0\u51fa\u65b0\u7684\u9002\u7528\u4e8eLLM\u7684\u8bc4\u4f30\u6307\u6807\u3002", "result": "GRIN\u6846\u67b6\u5728TOFU\u3001WMDP\u548cSafePKU\u7b49\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u80fd\u591f\u6709\u6548\u5b9e\u73b0\u9057\u5fd8\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6548\u7528\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "GRIN\u6846\u67b6\u901a\u8fc7\u68af\u5ea6\u6bd4\u7387\u5ea6\u91cf\u6709\u6548\u8bc6\u522b\u548c\u9009\u62e9\u6027\u53bb\u9664LLM\u4e2d\u9057\u5fd8\u6570\u636e\u7684\u53c2\u6570\uff0c\u540c\u65f6\u901a\u8fc7\u6ce8\u5165\u566a\u58f0\u548c\u5fae\u8c03\u6765\u6700\u5927\u9650\u5ea6\u5730\u4fdd\u7559\u6a21\u578b\u6548\u7528\uff0c\u5e76\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5f97\u5230\u4e86\u9a8c\u8bc1\u3002"}}
{"id": "2508.06109", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06109", "abs": "https://arxiv.org/abs/2508.06109", "authors": ["Zhibo Zhu", "Renyu Huang", "Lei He"], "title": "FMCE-Net++: Feature Map Convergence Evaluation and Training", "comment": null, "summary": "Deep Neural Networks (DNNs) face interpretability challenges due to their\nopaque internal representations. While Feature Map Convergence Evaluation\n(FMCE) quantifies module-level convergence via Feature Map Convergence Scores\n(FMCS), it lacks experimental validation and closed-loop integration. To\naddress this limitation, we propose FMCE-Net++, a novel training framework that\nintegrates a pretrained, frozen FMCE-Net as an auxiliary head. This module\ngenerates FMCS predictions, which, combined with task labels, jointly supervise\nbackbone optimization through a Representation Auxiliary Loss. The RAL\ndynamically balances the primary classification loss and feature convergence\noptimization via a tunable \\Representation Abstraction Factor. Extensive\nexperiments conducted on MNIST, CIFAR-10, FashionMNIST, and CIFAR-100\ndemonstrate that FMCE-Net++ consistently enhances model performance without\narchitectural modifications or additional data. Key experimental outcomes\ninclude accuracy gains of $+1.16$ pp (ResNet-50/CIFAR-10) and $+1.08$ pp\n(ShuffleNet v2/CIFAR-100), validating that FMCE-Net++ can effectively elevate\nstate-of-the-art performance ceilings.", "AI": {"tldr": "FMCE-Net++ \u901a\u8fc7\u96c6\u6210 FMCE-Net \u8f85\u52a9\u5934\u6765\u63d0\u5347 DNN \u6027\u80fd\uff0c\u65e0\u9700\u989d\u5916\u6570\u636e\u6216\u67b6\u6784\u66f4\u6539\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u7279\u5f81\u56fe\u6536\u655b\u8bc4\u4f30 (FMCE) \u65b9\u6cd5\u7f3a\u4e4f\u5b9e\u9a8c\u9a8c\u8bc1\u548c\u95ed\u73af\u96c6\u6210\u7684\u95ee\u9898\uff0c\u63d0\u51fa FMCE-Net++ \u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a FMCE-Net++ \u7684\u65b0\u9896\u8bad\u7ec3\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5c06\u9884\u8bad\u7ec3\u7684\u3001\u51bb\u7ed3\u7684 FMCE-Net \u4f5c\u4e3a\u8f85\u52a9\u5934\u96c6\u6210\u3002\u8be5\u6a21\u5757\u751f\u6210 FMCS \u9884\u6d4b\uff0c\u5e76\u4e0e\u4efb\u52a1\u6807\u7b7e\u4e00\u8d77\uff0c\u901a\u8fc7\u8868\u793a\u8f85\u52a9\u635f\u5931 (RAL) \u8054\u5408\u76d1\u7763\u9aa8\u5e72\u4f18\u5316\u3002RAL \u901a\u8fc7\u4e00\u4e2a\u53ef\u8c03\u7684\u8868\u793a\u62bd\u8c61\u56e0\u5b50\u52a8\u6001\u5e73\u8861\u4e3b\u8981\u7684\u5206\u7c7b\u635f\u5931\u548c\u7279\u5f81\u6536\u655b\u4f18\u5316\u3002", "result": "\u5728 MNIST\u3001CIFAR-10\u3001FashionMNIST \u548c CIFAR-100 \u4e0a\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cFMCE-Net++ \u5728\u4e0d\u8fdb\u884c\u67b6\u6784\u4fee\u6539\u6216\u589e\u52a0\u989d\u5916\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u80fd\u591f\u6301\u7eed\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002\u4f8b\u5982\uff0c\u5728 ResNet-50/CIFAR-10 \u4e0a\u51c6\u786e\u7387\u63d0\u5347\u4e86 1.16 \u4e2a\u767e\u5206\u70b9\uff0c\u5728 ShuffleNet v2/CIFAR-100 \u4e0a\u63d0\u5347\u4e86 1.08 \u4e2a\u767e\u5206\u70b9\uff0c\u9a8c\u8bc1\u4e86 FMCE-Net++ \u80fd\u591f\u6709\u6548\u63d0\u5347\u6700\u5148\u8fdb\u7684\u6027\u80fd\u4e0a\u9650\u3002", "conclusion": "FMCE-Net++ \u6846\u67b6\u901a\u8fc7\u96c6\u6210\u9884\u8bad\u7ec3\u7684 FMCE-Net \u4f5c\u4e3a\u8f85\u52a9\u5934\uff0c\u5e76\u5229\u7528\u7279\u5f81\u56fe\u6536\u655b\u5206\u6570 (FMCS) \u548c\u4efb\u52a1\u6807\u7b7e\u8054\u5408\u76d1\u7763\u9aa8\u5e72\u7f51\u7edc\u4f18\u5316\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u6027\u80fd\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u8868\u793a\u8f85\u52a9\u635f\u5931 (RAL) \u548c\u53ef\u8c03\u7684\u8868\u793a\u62bd\u8c61\u56e0\u5b50\u52a8\u6001\u5e73\u8861\u5206\u7c7b\u635f\u5931\u4e0e\u7279\u5f81\u6536\u655b\u4f18\u5316\uff0c\u65e0\u9700\u4fee\u6539\u7f51\u7edc\u7ed3\u6784\u6216\u989d\u5916\u6570\u636e\u3002"}}
{"id": "2107.06056", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2107.06056", "abs": "https://arxiv.org/abs/2107.06056", "authors": ["Prathamesh Kalamkar", "Janani Venugopalan Ph. D.", "Vivek Raghavan Ph. D"], "title": "Indian Legal NLP Benchmarks : A Survey", "comment": null, "summary": "Availability of challenging benchmarks is the key to advancement of AI in a\nspecific field.Since Legal Text is significantly different than normal English\ntext, there is a need to create separate Natural Language Processing benchmarks\nfor Indian Legal Text which are challenging and focus on tasks specific to\nLegal Systems. This will spur innovation in applications of Natural language\nProcessing for Indian Legal Text and will benefit AI community and Legal\nfraternity. We review the existing work in this area and propose ideas to\ncreate new benchmarks for Indian Legal Natural Language Processing.", "AI": {"tldr": "\u4eba\u5de5\u667a\u80fd\u5728\u6cd5\u5f8b\u9886\u57df\u7684\u8fdb\u6b65\u9700\u8981\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\u3002\u672c\u7814\u7a76\u56de\u987e\u4e86\u8be5\u9886\u57df\u7684\u73b0\u6709\u5de5\u4f5c\uff0c\u5e76\u63d0\u51fa\u4e86\u521b\u5efa\u5370\u5ea6\u6cd5\u5f8b\u81ea\u7136\u8bed\u8a00\u5904\u7406\u65b0\u57fa\u51c6\u7684\u60f3\u6cd5\uff0c\u4ee5\u5e94\u5bf9\u5370\u5ea6\u6cd5\u5f8b\u6587\u672c\u7684\u72ec\u7279\u6027\u548c\u6311\u6218\u3002", "motivation": "\u4eba\u5de5\u667a\u80fd\u5728\u6cd5\u5f8b\u9886\u57df\u7684\u8fdb\u6b65\u9700\u8981\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\u3002\u7531\u4e8e\u5370\u5ea6\u6cd5\u5f8b\u6587\u672c\u4e0e\u666e\u901a\u82f1\u8bed\u6587\u672c\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u56e0\u6b64\u6709\u5fc5\u8981\u4e3a\u5370\u5ea6\u6cd5\u5f8b\u6587\u672c\u521b\u5efa\u4e13\u95e8\u7684\u3001\u5177\u6709\u6311\u6218\u6027\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\u57fa\u51c6\uff0c\u4ee5\u4fc3\u8fdb\u4eba\u5de5\u667a\u80fd\u5728\u5370\u5ea6\u6cd5\u5f8b\u9886\u57df\u7684\u521b\u65b0\uff0c\u5e76\u60e0\u53ca\u4eba\u5de5\u667a\u80fd\u754c\u548c\u6cd5\u5f8b\u754c\u3002", "method": "\u901a\u8fc7\u56de\u987e\u73b0\u6709\u5de5\u4f5c\u5e76\u63d0\u51fa\u521b\u5efa\u65b0\u57fa\u51c6\u7684\u60f3\u6cd5\u3002", "result": "\u63d0\u51fa\u4e3a\u5370\u5ea6\u6cd5\u5f8b\u81ea\u7136\u8bed\u8a00\u5904\u7406\u521b\u5efa\u65b0\u57fa\u51c6\u7684\u60f3\u6cd5\u3002", "conclusion": "\u76ee\u524d\u6ca1\u6709\u5177\u4f53\u7ed3\u8bba\uff0c\u4f46\u63d0\u51fa\u9700\u8981\u4e3a\u5370\u5ea6\u6cd5\u5f8b\u6587\u672c\u521b\u5efa\u65b0\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\u57fa\u51c6\u3002"}}
{"id": "2508.06492", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06492", "abs": "https://arxiv.org/abs/2508.06492", "authors": ["Yuwei Yang", "Zeyu Zhang", "Yunzhong Hou", "Zhuowan Li", "Gaowen Liu", "Ali Payani", "Yuan-Sen Ting", "Liang Zheng"], "title": "Effective Training Data Synthesis for Improving MLLM Chart Understanding", "comment": "Accepted by ICCV 2025 (poster). 26 pages, 17 figures", "summary": "Being able to effectively read scientific plots, or chart understanding, is a\ncentral part toward building effective agents for science. However, existing\nmultimodal large language models (MLLMs), especially open-source ones, are\nstill falling behind with a typical success rate of 30%-50% on challenging\nbenchmarks. Previous studies on fine-tuning MLLMs with synthetic charts are\noften restricted by their inadequate similarity to the real charts, which could\ncompromise model training and performance on complex real-world charts. In this\nstudy, we show that modularizing chart generation and diversifying visual\ndetails improves chart understanding capabilities. In particular, we design a\nfive-step data synthesis pipeline, where we separate data and function creation\nfor single plot generation, condition the generation of later subplots on\nearlier ones for multi-subplot figures, visually diversify the generated\nfigures, filter out low quality data, and finally generate the question-answer\n(QA) pairs with GPT-4o. This approach allows us to streamline the generation of\nfine-tuning datasets and introduce the effective chart dataset (ECD), which\ncontains 10k+ chart images and 300k+ QA pairs, covering 25 topics and featuring\n250+ chart type combinations with high visual complexity. We show that ECD\nconsistently improves the performance of various MLLMs on a range of real-world\nand synthetic test sets. Code, data and models are available at:\nhttps://github.com/yuweiyang-anu/ECD.", "AI": {"tldr": "\u901a\u8fc7\u6539\u8fdb\u5408\u6210\u6570\u636e\u65b9\u6cd5\uff0c\u6784\u5efa\u4e86\u5305\u542b10k+\u56fe\u8868\u548c300k+\u95ee\u7b54\u5bf9\u7684ECD\u6570\u636e\u96c6\uff0c\u6709\u6548\u63d0\u5347\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u79d1\u5b66\u56fe\u8868\u7406\u89e3\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u79d1\u5b66\u56fe\u8868\u7406\u89e3\u65b9\u9762\uff0c\u5c24\u5176\u662f\u5728\u771f\u5b9e\u590d\u6742\u56fe\u8868\u4e0a\uff0c\u6027\u80fd\u8868\u73b0\u4e0d\u4f73\uff08\u6210\u529f\u7387\u4ec5\u4e3a30%-50%\uff09\u3002\u5148\u524d\u4f7f\u7528\u5408\u6210\u56fe\u8868\u8fdb\u884c\u5fae\u8c03\u7684\u7814\u7a76\uff0c\u7531\u4e8e\u5408\u6210\u56fe\u8868\u4e0e\u771f\u5b9e\u56fe\u8868\u76f8\u4f3c\u5ea6\u4e0d\u8db3\uff0c\u5f71\u54cd\u4e86\u6a21\u578b\u8bad\u7ec3\u548c\u6027\u80fd\u3002\u672c\u6b21\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u6539\u8fdb\u6570\u636e\u5408\u6210\u65b9\u6cd5\u6765\u63d0\u5347MLLMs\u7684\u56fe\u8868\u7406\u89e3\u80fd\u529b\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5305\u542b\u4e94\u4e2a\u6b65\u9aa4\u7684\u6570\u636e\u5408\u6210\u6d41\u7a0b\uff0c\u5305\u62ec\u6570\u636e\u548c\u56fe\u5f62\u751f\u6210\u5206\u79bb\u3001\u591a\u5b50\u56fe\u751f\u6210\u6761\u4ef6\u5316\u3001\u89c6\u89c9\u591a\u6837\u5316\u3001\u4f4e\u8d28\u91cf\u6570\u636e\u8fc7\u6ee4\u4ee5\u53ca\u4f7f\u7528GPT-4o\u751f\u6210\u95ee\u7b54\u5bf9\uff0c\u6784\u5efa\u4e86\u5305\u542b10k+\u56fe\u8868\u56fe\u50cf\u548c300k+\u95ee\u7b54\u5bf9\u7684ECD\u6570\u636e\u96c6\uff0c\u8986\u76d625\u4e2a\u4e3b\u9898\u548c250\u591a\u79cd\u56fe\u8868\u7ec4\u5408\u3002", "result": "ECD\u6570\u636e\u96c6\u88ab\u8bc1\u660e\u80fd\u591f\u6301\u7eed\u63d0\u5347\u5305\u62ec\u5f00\u6e90\u6a21\u578b\u5728\u5185\u7684\u591a\u79cdMLLMs\u5728\u771f\u5b9e\u548c\u5408\u6210\u56fe\u8868\u6d4b\u8bd5\u96c6\u4e0a\u7684\u6027\u80fd\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684ECD\u6570\u636e\u96c6\u80fd\u6709\u6548\u63d0\u5347MLLMs\u5728\u79d1\u5b66\u56fe\u8868\u7406\u89e3\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u5728\u591a\u79cd\u771f\u5b9e\u548c\u5408\u6210\u6570\u636e\u96c6\u4e0a\u90fd\u8868\u73b0\u51fa\u6301\u7eed\u7684\u6539\u8fdb\u3002"}}
{"id": "2508.06115", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06115", "abs": "https://arxiv.org/abs/2508.06115", "authors": ["Weichen Zhang", "Kebin Liu", "Fan Dang", "Zhui Zhu", "Xikai Sun", "Yunhao Liu"], "title": "SynSeg: Feature Synergy for Multi-Category Contrastive Learning in Open-Vocabulary Semantic Segmentation", "comment": null, "summary": "Semantic segmentation in open-vocabulary scenarios presents significant\nchallenges due to the wide range and granularity of semantic categories.\nExisting weakly-supervised methods often rely on category-specific supervision\nand ill-suited feature construction methods for contrastive learning, leading\nto semantic misalignment and poor performance. In this work, we propose a novel\nweakly-supervised approach, SynSeg, to address the challenges. SynSeg performs\nMulti-Category Contrastive Learning (MCCL) as a stronger training signal with a\nnew feature reconstruction framework named Feature Synergy Structure (FSS).\nSpecifically, MCCL strategy robustly combines both intra- and inter-category\nalignment and separation in order to make the model learn the knowledge of\ncorrelations from different categories within the same image. Moreover, FSS\nreconstructs discriminative features for contrastive learning through prior\nfusion and semantic-activation-map enhancement, effectively avoiding the\nforeground bias introduced by the visual encoder. In general, SynSeg\neffectively improves the abilities in semantic localization and discrimination\nunder weak supervision. Extensive experiments on benchmarks demonstrate that\nour method outperforms state-of-the-art (SOTA) performance. For instance,\nSynSeg achieves higher accuracy than SOTA baselines by 4.5\\% on VOC, 8.9\\% on\nContext, 2.6\\% on Object and 2.0\\% on City.", "AI": {"tldr": "SynSeg, a novel weakly-supervised approach, enhances open-vocabulary semantic segmentation using Multi-Category Contrastive Learning (MCCL) and Feature Synergy Structure (FSS) to improve category correlation learning and mitigate foreground bias, achieving state-of-the-art results on multiple benchmarks.", "motivation": "Existing weakly-supervised methods for open-vocabulary semantic segmentation struggle with the wide range of categories and often use unsuitable supervision and feature construction methods, leading to semantic misalignment and poor performance.", "method": "SynSeg employs Multi-Category Contrastive Learning (MCCL) for a stronger training signal and a Feature Synergy Structure (FSS) framework for feature reconstruction. MCCL combines intra- and inter-category alignment and separation to learn category correlations. FSS reconstructs discriminative features via prior fusion and semantic-activation-map enhancement, mitigating foreground bias.", "result": "SynSeg achieves higher accuracy than SOTA baselines by 4.5% on VOC, 8.9% on Context, 2.6% on Object, and 2.0% on City.", "conclusion": "SynSeg effectively improves semantic localization and discrimination abilities under weak supervision, outperforming SOTA methods on multiple benchmarks."}}
{"id": "2508.06122", "categories": ["cs.CV", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2508.06122", "abs": "https://arxiv.org/abs/2508.06122", "authors": ["Ting-Shuo Yo", "Shih-Hao Su", "Chien-Ming Wu", "Wei-Ting Chen", "Jung-Lien Chu", "Chiao-Wei Chang", "Hung-Chi Kuo"], "title": "Learning Representations of Satellite Images with Evaluations on Synoptic Weather Events", "comment": "37 pages, 6 figures, 3 tables", "summary": "This study applied representation learning algorithms to satellite images and\nevaluated the learned latent spaces with classifications of various weather\nevents. The algorithms investigated include the classical linear\ntransformation, i.e., principal component analysis (PCA), state-of-the-art deep\nlearning method, i.e., convolutional autoencoder (CAE), and a residual network\npre-trained with large image datasets (PT). The experiment results indicated\nthat the latent space learned by CAE consistently showed higher threat scores\nfor all classification tasks. The classifications with PCA yielded high hit\nrates but also high false-alarm rates. In addition, the PT performed\nexceptionally well at recognizing tropical cyclones but was inferior in other\ntasks. Further experiments suggested that representations learned from\nhigher-resolution datasets are superior in all classification tasks for\ndeep-learning algorithms, i.e., CAE and PT. We also found that smaller latent\nspace sizes had minor impact on the classification task's hit rate. Still, a\nlatent space dimension smaller than 128 caused a significantly higher false\nalarm rate. Though the CAE can learn latent spaces effectively and efficiently,\nthe interpretation of the learned representation lacks direct connections to\nphysical attributions. Therefore, developing a physics-informed version of CAE\ncan be a promising outlook for the current work.", "AI": {"tldr": "\u672c\u7814\u7a76\u5bf9\u6bd4\u4e86PCA\u3001CAE\u548cPT\u4e09\u79cd\u7b97\u6cd5\u5728\u536b\u661f\u56fe\u50cf\u5929\u6c14\u4e8b\u4ef6\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002CAE\u5728\u5206\u7c7b\u51c6\u786e\u6027\u4e0a\u4f18\u4e8ePCA\u548cPT\uff0c\u4f46\u5176\u5b66\u4e60\u5230\u7684\u8868\u5f81\u7f3a\u4e4f\u7269\u7406\u89e3\u91ca\u6027\u3002\u7814\u7a76\u8fd8\u53d1\u73b0\uff0c\u66f4\u9ad8\u5206\u8fa8\u7387\u7684\u6570\u636e\u96c6\u548c\u9002\u4e2d\u7684\u6f5c\u5728\u7a7a\u95f4\u5c3a\u5bf8\uff08\u4e0d\u5c0f\u4e8e128\uff09\u6709\u5229\u4e8e\u63d0\u5347\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u63a2\u7d22\u8868\u793a\u5b66\u4e60\u7b97\u6cd5\u5728\u536b\u661f\u56fe\u50cf\u5929\u6c14\u4e8b\u4ef6\u5206\u7c7b\u4e2d\u7684\u5e94\u7528\u548c\u8bc4\u4f30\u4e0d\u540c\u7b97\u6cd5\u5b66\u4e60\u5230\u7684\u6f5c\u5728\u7a7a\u95f4\u7684\u6709\u6548\u6027\u3002", "method": "\u672c\u7814\u7a76\u5e94\u7528\u4e86\u5305\u62ecPCA\u3001CAE\u548c\u9884\u8bad\u7ec3\u6b8b\u5dee\u7f51\u7edc\uff08PT\uff09\u5728\u5185\u7684\u8868\u793a\u5b66\u4e60\u7b97\u6cd5\uff0c\u5e76\u5c06\u5b66\u4e60\u5230\u7684\u6f5c\u5728\u7a7a\u95f4\u5e94\u7528\u4e8e\u536b\u661f\u56fe\u50cf\u7684\u5929\u6c14\u4e8b\u4ef6\u5206\u7c7b\u4efb\u52a1\u3002", "result": "CAE\u5b66\u4e60\u5230\u7684\u6f5c\u5728\u7a7a\u95f4\u5728\u6240\u6709\u5206\u7c7b\u4efb\u52a1\u4e2d\u5747\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u5a01\u80c1\u5f97\u5206\u3002PCA\u5728\u8bc6\u522b\u5929\u6c14\u4e8b\u4ef6\u65b9\u9762\u547d\u4e2d\u7387\u9ad8\u4f46\u8bef\u62a5\u7387\u4e5f\u9ad8\uff0cPT\u5728\u8bc6\u522b\u70ed\u5e26\u6c14\u65cb\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u4f46\u5728\u5176\u4ed6\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002\u66f4\u9ad8\u5206\u8fa8\u7387\u7684\u6570\u636e\u96c6\u548c128\u7ef4\u7684\u6f5c\u5728\u7a7a\u95f4\u5c3a\u5bf8\u5bf9\u6df1\u5ea6\u5b66\u4e60\u7b97\u6cd5\u7684\u5206\u7c7b\u4efb\u52a1\u8868\u73b0\u6709\u79ef\u6781\u5f71\u54cd\u3002", "conclusion": "CAE\u5728\u5929\u6c14\u4e8b\u4ef6\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u5176\u5b66\u4e60\u5230\u7684\u8868\u5f81\u7f3a\u4e4f\u7269\u7406\u89e3\u91ca\u6027\u3002\u672a\u6765\u7684\u5de5\u4f5c\u53ef\u4ee5\u63a2\u7d22CAE\u7684\u7269\u7406\u4fe1\u606f\u65b9\u6cd5\u3002"}}
{"id": "2508.06125", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06125", "abs": "https://arxiv.org/abs/2508.06125", "authors": ["Lin Zhang", "Xianfang Zeng", "Kangcong Li", "Gang Yu", "Tao Chen"], "title": "SC-Captioner: Improving Image Captioning with Self-Correction by Reinforcement Learning", "comment": "ICCV 2025", "summary": "We propose SC-Captioner, a reinforcement learning framework that enables the\nself-correcting capability of image caption models. Our crucial technique lies\nin the design of the reward function to incentivize accurate caption\ncorrections. Specifically, the predicted and reference captions are decomposed\ninto object, attribute, and relation sets using scene-graph parsing algorithms.\nWe calculate the set difference between sets of initial and self-corrected\ncaptions to identify added and removed elements. These elements are matched\nagainst the reference sets to calculate correctness bonuses for accurate\nrefinements and mistake punishments for wrong additions and removals, thereby\nforming the final reward. For image caption quality assessment, we propose a\nset of metrics refined from CAPTURE that alleviate its incomplete precision\nevaluation and inefficient relation matching problems. Furthermore, we collect\na fine-grained annotated image caption dataset, RefinedCaps, consisting of 6.5K\ndiverse images from COCO dataset. Experiments show that applying SC-Captioner\non large visual-language models can generate better image captions across\nvarious scenarios, significantly outperforming the direct preference\noptimization training strategy.", "AI": {"tldr": "\u63d0\u51fa SC-Captioner \u6846\u67b6\uff0c\u901a\u8fc7\u5956\u52b1\u51fd\u6570\u5b9e\u73b0\u56fe\u50cf\u5b57\u5e55\u7684\u81ea\u6211\u4fee\u6b63\uff0c\u5e76\u5f15\u5165\u65b0\u7684\u8bc4\u4f30\u6307\u6807\u548c\u6570\u636e\u96c6\uff0c\u5b9e\u9a8c\u8bc1\u660e\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u65e8\u5728\u4e3a\u56fe\u50cf\u5b57\u5e55\u6a21\u578b\u63d0\u4f9b\u81ea\u6211\u4fee\u6b63\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a SC-Captioner \u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u8bbe\u8ba1\u5956\u52b1\u51fd\u6570\u6765\u6fc0\u52b1\u6a21\u578b\u8fdb\u884c\u51c6\u786e\u7684\u5b57\u5e55\u4fee\u6b63\u3002\u5177\u4f53\u65b9\u6cd5\u662f\u5c06\u9884\u6d4b\u5b57\u5e55\u548c\u53c2\u8003\u5b57\u5e55\u5206\u89e3\u4e3a\u5bf9\u8c61\u3001\u5c5e\u6027\u548c\u5173\u7cfb\u96c6\u5408\uff0c\u901a\u8fc7\u96c6\u5408\u5dee\u5206\u8bc6\u522b\u6dfb\u52a0\u548c\u79fb\u9664\u7684\u5143\u7d20\uff0c\u5e76\u5c06\u8fd9\u4e9b\u5143\u7d20\u4e0e\u53c2\u8003\u96c6\u5408\u8fdb\u884c\u5339\u914d\uff0c\u8ba1\u7b97\u4fee\u6b63\u7684\u6b63\u786e\u6027\u5956\u52b1\u548c\u9519\u8bef\u60e9\u7f5a\uff0c\u5f62\u6210\u6700\u7ec8\u5956\u52b1\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u5957\u6539\u8fdb\u7684\u56fe\u50cf\u5b57\u5e55\u8d28\u91cf\u8bc4\u4f30\u6307\u6807\uff0c\u5e76\u6536\u96c6\u4e86\u4e00\u4e2a\u5305\u542b 6.5K \u56fe\u50cf\u7684\u7ec6\u7c92\u5ea6\u6807\u6ce8\u6570\u636e\u96c6 RefinedCaps\u3002", "result": "SC-Captioner \u6846\u67b6\u80fd\u591f\u751f\u6210\u66f4\u597d\u7684\u56fe\u50cf\u5b57\u5e55\uff0c\u5e76\u4e14\u5728\u4e0e\u5927\u578b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7ed3\u5408\u4f7f\u7528\u65f6\uff0c\u5728\u5404\u79cd\u573a\u666f\u4e0b\u5747\u8868\u73b0\u51fa\u8272\uff0c\u663e\u8457\u4f18\u4e8e\u76f4\u63a5\u504f\u597d\u4f18\u5316\u8bad\u7ec3\u7b56\u7565\u3002", "conclusion": "SC-Captioner \u6846\u67b6\u5728\u5404\u79cd\u573a\u666f\u4e0b\u80fd\u751f\u6210\u66f4\u597d\u7684\u56fe\u50cf\u5b57\u5e55\uff0c\u5e76\u4e14\u663e\u8457\u4f18\u4e8e\u76f4\u63a5\u504f\u597d\u4f18\u5316\u8bad\u7ec3\u7b56\u7565\u3002"}}
{"id": "2508.06127", "categories": ["cs.CV", "I.4.9"], "pdf": "https://arxiv.org/pdf/2508.06127", "abs": "https://arxiv.org/abs/2508.06127", "authors": ["Yi Qin", "Rui Wang", "Tao Huang", "Tong Xiao", "Liping Jing"], "title": "SAM Encoder Breach by Adversarial Simplicial Complex Triggers Downstream Model Failures", "comment": "8 pages,recived by ICCV2025", "summary": "While the Segment Anything Model (SAM) transforms interactive segmentation\nwith zero-shot abilities, its inherent vulnerabilities present a single-point\nrisk, potentially leading to the failure of numerous downstream applications.\nProactively evaluating these transferable vulnerabilities is thus imperative.\nPrior adversarial attacks on SAM often present limited transferability due to\ninsufficient exploration of common weakness across domains. To address this, we\npropose Vertex-Refining Simplicial Complex Attack (VeSCA), a novel method that\nleverages only the encoder of SAM for generating transferable adversarial\nexamples. Specifically, it achieves this by explicitly characterizing the\nshared vulnerable regions between SAM and downstream models through a\nparametric simplicial complex. Our goal is to identify such complexes within\nadversarially potent regions by iterative vertex-wise refinement. A lightweight\ndomain re-adaptation strategy is introduced to bridge domain divergence using\nminimal reference data during the initialization of simplicial complex.\nUltimately, VeSCA generates consistently transferable adversarial examples\nthrough random simplicial complex sampling. Extensive experiments demonstrate\nthat VeSCA achieves performance improved by 12.7% compared to state-of-the-art\nmethods across three downstream model categories across five domain-specific\ndatasets. Our findings further highlight the downstream model risks posed by\nSAM's vulnerabilities and emphasize the urgency of developing more robust\nfoundation models.", "AI": {"tldr": "VeSCA \u662f\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528 SAM \u7684\u7f16\u7801\u5668\u548c\u5355\u7eaf\u5f62\u590d\u5f62\u6765\u751f\u6210\u53ef\u8fc1\u79fb\u7684\u5bf9\u6297\u6837\u672c\uff0c\u4ee5\u89e3\u51b3 SAM \u6f0f\u6d1e\u7684\u4f20\u9012\u6027\u95ee\u9898\u3002\u5b9e\u9a8c\u8bc1\u660e VeSCA \u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u9ad8\u4e86 12.7% \u7684\u6027\u80fd\uff0c\u5e76\u5f3a\u8c03\u4e86\u57fa\u7840\u6a21\u578b\u7a33\u5065\u6027\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u5c3d\u7ba1 SAM \u5728\u4ea4\u4e92\u5f0f\u5206\u5272\u65b9\u9762\u5177\u6709\u96f6\u6b21\u5b66\u4e60\u80fd\u529b\uff0c\u4f46\u5176\u56fa\u6709\u7684\u6f0f\u6d1e\u53ef\u80fd\u5bfc\u81f4\u4f17\u591a\u4e0b\u6e38\u5e94\u7528\u7a0b\u5e8f\u7684\u5931\u8d25\u3002\u56e0\u6b64\uff0c\u4e3b\u52a8\u8bc4\u4f30\u8fd9\u4e9b\u53ef\u8fc1\u79fb\u7684\u6f0f\u6d1e\u81f3\u5173\u91cd\u8981\u3002\u5148\u524d\u5bf9 SAM \u7684\u5bf9\u6297\u653b\u51fb\u7531\u4e8e\u5bf9\u8de8\u57df\u7684\u5171\u540c\u5f31\u70b9\u63a2\u7d22\u4e0d\u8db3\uff0c\u5bfc\u81f4\u53ef\u8fc1\u79fb\u6027\u6709\u9650\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a VeSCA\uff08Vertex-Refining Simplicial Complex Attack\uff09\u7684\u65b0\u9896\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4ec5\u5229\u7528 SAM \u7684\u7f16\u7801\u5668\u6765\u751f\u6210\u53ef\u8fc1\u79fb\u7684\u5bf9\u6297\u6837\u672c\u3002\u5b83\u901a\u8fc7\u53c2\u6570\u5316\u5355\u7eaf\u5f62\u590d\u5f62\u660e\u786e\u8868\u5f81 SAM \u4e0e\u4e0b\u6e38\u6a21\u578b\u4e4b\u95f4\u7684\u5171\u4eab\u8106\u5f31\u533a\u57df\uff0c\u5e76\u901a\u8fc7\u8fed\u4ee3\u9876\u70b9\u7ec6\u5316\u6765\u8bc6\u522b\u8fd9\u4e9b\u590d\u5f62\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u57df\u518d\u9002\u5e94\u7b56\u7565\uff0c\u5229\u7528\u6700\u5c0f\u7684\u53c2\u8003\u6570\u636e\u6765\u5f25\u5408\u57df\u53d1\u6563\u3002", "result": "VeSCA \u80fd\u591f\u751f\u6210\u4e00\u81f4\u7684\u53ef\u8fc1\u79fb\u5bf9\u6297\u6837\u672c\uff0c\u5e76\u4e14\u5728\u4e09\u4e2a\u4e0b\u6e38\u6a21\u578b\u7c7b\u522b\u548c\u4e94\u4e2a\u9886\u57df\u7279\u5b9a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5176\u6027\u80fd\u6bd4\u73b0\u6709\u6280\u672f\u63d0\u9ad8\u4e86 12.7%\u3002", "conclusion": "VeSCA \u751f\u6210\u7684\u5bf9\u6297\u6837\u672c\u5728\u8de8\u8d8a\u4e09\u4e2a\u4e0b\u6e38\u6a21\u578b\u7c7b\u522b\u548c\u4e94\u4e2a\u7279\u5b9a\u9886\u57df\u7684\u6570\u636e\u96c6\u4e0a\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u6027\u80fd\u63d0\u5347\u4e86 12.7%\u3002\u8fd9\u4e9b\u53d1\u73b0\u8fdb\u4e00\u6b65\u51f8\u663e\u4e86 SAM \u7684\u6f0f\u6d1e\u5bf9\u4e0b\u6e38\u6a21\u578b\u6784\u6210\u7684\u98ce\u9669\uff0c\u5e76\u5f3a\u8c03\u4e86\u5f00\u53d1\u66f4\u5f3a\u5927\u7684\u57fa\u7840\u6a21\u578b\u7684\u7d27\u8feb\u6027\u3002"}}
{"id": "2508.06136", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06136", "abs": "https://arxiv.org/abs/2508.06136", "authors": ["YoungChan Choi", "HengFei Wang", "YiHua Cheng", "Boeun Kim", "Hyung Jin Chang", "YoungGeun Choi", "Sang-Il Choi"], "title": "Roll Your Eyes: Gaze Redirection via Explicit 3D Eyeball Rotation", "comment": "9 pages, 5 figures, ACM Multimeida 2025 accepted", "summary": "We propose a novel 3D gaze redirection framework that leverages an explicit\n3D eyeball structure. Existing gaze redirection methods are typically based on\nneural radiance fields, which employ implicit neural representations via volume\nrendering. Unlike these NeRF-based approaches, where the rotation and\ntranslation of 3D representations are not explicitly modeled, we introduce a\ndedicated 3D eyeball structure to represent the eyeballs with 3D Gaussian\nSplatting (3DGS). Our method generates photorealistic images that faithfully\nreproduce the desired gaze direction by explicitly rotating and translating the\n3D eyeball structure. In addition, we propose an adaptive deformation module\nthat enables the replication of subtle muscle movements around the eyes.\nThrough experiments conducted on the ETH-XGaze dataset, we demonstrate that our\nframework is capable of generating diverse novel gaze images, achieving\nsuperior image quality and gaze estimation accuracy compared to previous\nstate-of-the-art methods.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\u76843D\u6ce8\u89c6\u91cd\u5b9a\u5411\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u76843D\u773c\u7403\u7ed3\u6784\u548c\u81ea\u9002\u5e94\u53d8\u5f62\u6a21\u5757\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u6ce8\u89c6\u56fe\u50cf\u751f\u6210\u548c\u51c6\u786e\u7684\u6ce8\u89c6\u4f30\u8ba1\uff0c\u4f18\u4e8e\u73b0\u6709NeRF\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eNeRF\uff08\u795e\u7ecf\u8f90\u5c04\u573a\uff09\u7684\u6ce8\u89c6\u91cd\u5b9a\u5411\u65b9\u6cd5\uff0c\u51763D\u8868\u793a\u7684\u65cb\u8f6c\u548c\u5e73\u79fb\u5e76\u672a\u663e\u5f0f\u5efa\u6a21\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u672c\u7814\u7a76\u5f15\u5165\u4e86\u663e\u5f0f\u76843D\u773c\u7403\u7ed3\u6784\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u76843D\u6ce8\u89c6\u91cd\u5b9a\u5411\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u663e\u5f0f\u76843D\u773c\u7403\u7ed3\u6784\uff0c\u5e76\u91c7\u75283D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u6765\u8868\u793a\u773c\u7403\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u53d8\u5f62\u6a21\u5757\uff0c\u7528\u4e8e\u6a21\u62df\u773c\u90e8\u5468\u56f4\u808c\u8089\u7684\u7ec6\u5fae\u8fd0\u52a8\u3002", "result": "\u5728ETH-XGaze\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u751f\u6210\u591a\u6837\u5316\u7684\u65b0\u9896\u6ce8\u89c6\u56fe\u50cf\uff0c\u5e76\u5728\u56fe\u50cf\u8d28\u91cf\u548c\u6ce8\u89c6\u4f30\u8ba1\u51c6\u786e\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u663e\u5f0f\u65cb\u8f6c\u548c\u7ffb\u8bd13D\u773c\u7403\u7ed3\u6784\uff0c\u80fd\u591f\u751f\u6210\u903c\u771f\u7684\u56fe\u50cf\uff0c\u51c6\u786e\u8fd8\u539f\u671f\u671b\u7684\u6ce8\u89c6\u65b9\u5411\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5728\u56fe\u50cf\u8d28\u91cf\u548c\u6ce8\u89c6\u4f30\u8ba1\u51c6\u786e\u6027\u65b9\u9762\u5747\u8868\u73b0\u66f4\u4f18\u3002"}}
{"id": "2508.06139", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06139", "abs": "https://arxiv.org/abs/2508.06139", "authors": ["Shaohua Pan", "Xinyu Yi", "Yan Zhou", "Weihua Jian", "Yuan Zhang", "Pengfei Wan", "Feng Xu"], "title": "DiffCap: Diffusion-based Real-time Human Motion Capture using Sparse IMUs and a Monocular Camera", "comment": null, "summary": "Combining sparse IMUs and a monocular camera is a new promising setting to\nperform real-time human motion capture. This paper proposes a diffusion-based\nsolution to learn human motion priors and fuse the two modalities of signals\ntogether seamlessly in a unified framework. By delicately considering the\ncharacteristics of the two signals, the sequential visual information is\nconsidered as a whole and transformed into a condition embedding, while the\ninertial measurement is concatenated with the noisy body pose frame by frame to\nconstruct a sequential input for the diffusion model. Firstly, we observe that\nthe visual information may be unavailable in some frames due to occlusions or\nsubjects moving out of the camera view. Thus incorporating the sequential\nvisual features as a whole to get a single feature embedding is robust to the\noccasional degenerations of visual information in those frames. On the other\nhand, the IMU measurements are robust to occlusions and always stable when\nsignal transmission has no problem. So incorporating them frame-wisely could\nbetter explore the temporal information for the system. Experiments have\ndemonstrated the effectiveness of the system design and its state-of-the-art\nperformance in pose estimation compared with the previous works. Our codes are\navailable for research at https://shaohua-pan.github.io/diffcap-page.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u6269\u6563\u6a21\u578b\u65b9\u6cd5\uff0c\u901a\u8fc7\u878d\u5408IMU\u548c\u5355\u76ee\u6444\u50cf\u5934\u6570\u636e\uff0c\u5b9e\u73b0\u4e86\u9c81\u68d2\u4e14\u7cbe\u786e\u7684\u5b9e\u65f6\u4eba\u4f53\u8fd0\u52a8\u6355\u6349\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u89c6\u89c9\u4fe1\u606f\u53ef\u80fd\u56e0\u906e\u6321\u6216\u4e3b\u4f53\u79fb\u51fa\u76f8\u673a\u89c6\u91ce\u800c\u4e0d\u53ef\u7528\uff0c\u4ee5\u53caIMU\u6d4b\u91cf\u5bf9\u906e\u6321\u5177\u6709\u9c81\u68d2\u6027\u4e14\u4fe1\u53f7\u4f20\u8f93\u7a33\u5b9a\u7b49\u7279\u70b9\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u4e24\u79cd\u4fe1\u53f7\u6a21\u6001\u878d\u5408\u7684\u7edf\u4e00\u6846\u67b6\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c06\u89c6\u89c9\u4fe1\u606f\u8f6c\u5316\u4e3a\u6761\u4ef6\u5d4c\u5165\uff0c\u5e76\u5c06\u60ef\u6027\u6d4b\u91cf\u9010\u5e27\u4e0e\u5e26\u6709\u566a\u58f0\u7684\u8eab\u4f53\u59ff\u6001\u8fde\u63a5\u8d77\u6765\uff0c\u4ee5\u6784\u5efa\u6269\u6563\u6a21\u578b\u7684\u5e8f\u5217\u8f93\u5165\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u7cfb\u7edf\u8bbe\u8ba1\u7684\u6709\u6548\u6027\u53ca\u5176\u5728\u59ff\u6001\u4f30\u8ba1\u65b9\u9762\u76f8\u6bd4\u4e8e\u5148\u524d\u5de5\u4f5c\u7684\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7528\u4e8e\u5b66\u4e60\u4eba\u7c7b\u8fd0\u52a8\u5148\u9a8c\u77e5\u8bc6\uff0c\u5e76\u5728\u7edf\u4e00\u6846\u67b6\u4e2d\u65e0\u7f1d\u878d\u5408\u7a00\u758fIMU\u548c\u5355\u76ee\u6444\u50cf\u5934\u4e24\u79cd\u4fe1\u53f7\u6a21\u6001\uff0c\u5b9e\u73b0\u4e86\u5b9e\u65f6\u8fd0\u52a8\u6355\u6349\u3002"}}
{"id": "2508.06142", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06142", "abs": "https://arxiv.org/abs/2508.06142", "authors": ["Hanqing Wang", "Yuan Tian", "Mingyu Liu", "Zhenhao Zhang", "Xiangyang Zhu"], "title": "SDEval: Safety Dynamic Evaluation for Multimodal Large Language Models", "comment": null, "summary": "In the rapidly evolving landscape of Multimodal Large Language Models\n(MLLMs), the safety concerns of their outputs have earned significant\nattention. Although numerous datasets have been proposed, they may become\noutdated with MLLM advancements and are susceptible to data contamination\nissues. To address these problems, we propose \\textbf{SDEval}, the\n\\textit{first} safety dynamic evaluation framework to controllably adjust the\ndistribution and complexity of safety benchmarks. Specifically, SDEval mainly\nadopts three dynamic strategies: text, image, and text-image dynamics to\ngenerate new samples from original benchmarks. We first explore the individual\neffects of text and image dynamics on model safety. Then, we find that\ninjecting text dynamics into images can further impact safety, and conversely,\ninjecting image dynamics into text also leads to safety risks. SDEval is\ngeneral enough to be applied to various existing safety and even capability\nbenchmarks. Experiments across safety benchmarks, MLLMGuard and VLSBench, and\ncapability benchmarks, MMBench and MMVet, show that SDEval significantly\ninfluences safety evaluation, mitigates data contamination, and exposes safety\nlimitations of MLLMs. Code is available at https://github.com/hq-King/SDEval", "AI": {"tldr": "\u63d0\u51fa SDEval\uff0c\u9996\u4e2a\u5b89\u5168\u52a8\u6001\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u6587\u672c\u3001\u56fe\u50cf\u548c\u6587\u672c-\u56fe\u50cf\u52a8\u6001\u7b56\u7565\u751f\u6210\u6837\u672c\uff0c\u4ee5\u5e94\u5bf9 MLLM \u5b89\u5168\u8bc4\u4f30\u4e2d\u7684\u6570\u636e\u6c61\u67d3\u548c\u8fc7\u65f6\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684 MLLM \u5b89\u5168\u8bc4\u4f30\u6570\u636e\u96c6\u53ef\u80fd\u8fc7\u65f6\u4e14\u6613\u53d7\u6570\u636e\u6c61\u67d3\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "SDEval \u6846\u67b6\u901a\u8fc7\u6587\u672c\u3001\u56fe\u50cf\u548c\u6587\u672c-\u56fe\u50cf\u52a8\u6001\u7b56\u7565\u751f\u6210\u65b0\u6837\u672c\uff0c\u4ee5\u52a8\u6001\u8c03\u6574\u5b89\u5168\u57fa\u51c6\u7684\u5206\u5e03\u548c\u590d\u6742\u6027\u3002", "result": "SDEval \u80fd\u591f\u52a8\u6001\u8c03\u6574\u5b89\u5168\u57fa\u51c6\u7684\u5206\u5e03\u548c\u590d\u6742\u6027\uff0c\u5e76\u5728 MLLMGuard\u3001VLSBench\u3001MMBench \u548c MMVet \u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u793a\u51fa\u6709\u6548\u6027\u3002", "conclusion": "SDEval \u663e\u8457\u5f71\u54cd\u5b89\u5168\u8bc4\u4f30\uff0c\u7f13\u89e3\u6570\u636e\u6c61\u67d3\uff0c\u5e76\u63ed\u793a\u4e86 MLLMs \u7684\u5b89\u5168\u5c40\u9650\u6027\u3002"}}
{"id": "2508.06146", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06146", "abs": "https://arxiv.org/abs/2508.06146", "authors": ["Yuchen Guan", "Chong Sun", "Canmiao Fu", "Zhipeng Huang", "Chun Yuan", "Chen Li"], "title": "Text-guided Visual Prompt DINO for Generic Segmentation", "comment": null, "summary": "Recent advancements in multimodal vision models have highlighted limitations\nin late-stage feature fusion and suboptimal query selection for hybrid prompts\nopen-world segmentation, alongside constraints from caption-derived\nvocabularies. To address these challenges, we propose Prompt-DINO, a\ntext-guided visual Prompt DINO framework featuring three key innovations.\nFirst, we introduce an early fusion mechanism that unifies text/visual prompts\nand backbone features at the initial encoding stage, enabling deeper\ncross-modal interactions to resolve semantic ambiguities. Second, we design\norder-aligned query selection for DETR-based architectures, explicitly\noptimizing the structural alignment between text and visual queries during\ndecoding to enhance semantic-spatial consistency. Third, we develop a\ngenerative data engine powered by the Recognize Anything via Prompting (RAP)\nmodel, which synthesizes 0.5B diverse training instances through a dual-path\ncross-verification pipeline, reducing label noise by 80.5% compared to\nconventional approaches. Extensive experiments demonstrate that Prompt-DINO\nachieves state-of-the-art performance on open-world detection benchmarks while\nsignificantly expanding semantic coverage beyond fixed-vocabulary constraints.\nOur work establishes a new paradigm for scalable multimodal detection and data\ngeneration in open-world scenarios. Data&Code are available at\nhttps://github.com/WeChatCV/WeVisionOne.", "AI": {"tldr": "Prompt-DINO, a text-guided visual Prompt DINO framework, tackles open-world segmentation challenges with early fusion, aligned query selection, and a large-scale generative data engine, outperforming existing methods and expanding vocabulary.", "motivation": "To address limitations in late-stage feature fusion, suboptimal query selection, and caption-derived vocabularies in existing multimodal vision models for open-world segmentation.", "method": "Prompt-DINO introduces an early fusion mechanism, order-aligned query selection for DETR-based architectures, and a generative data engine powered by the RAP model for data synthesis.", "result": "Prompt-DINO achieves state-of-the-art performance on open-world detection benchmarks and significantly expands semantic coverage.", "conclusion": "Prompt-DINO establishes a new paradigm for scalable multimodal detection and data generation in open-world scenarios, achieving state-of-the-art performance and expanding semantic coverage."}}
{"id": "2508.06147", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06147", "abs": "https://arxiv.org/abs/2508.06147", "authors": ["Xuanyu Liu", "Bonan An"], "title": "DSConv: Dynamic Splitting Convolution for Pansharpening", "comment": null, "summary": "Aiming to obtain a high-resolution image, pansharpening involves the fusion\nof a multi-spectral image (MS) and a panchromatic image (PAN), the low-level\nvision task remaining significant and challenging in contemporary research.\nMost existing approaches rely predominantly on standard convolutions, few\nmaking the effort to adaptive convolutions, which are effective owing to the\ninter-pixel correlations of remote sensing images. In this paper, we propose a\nnovel strategy for dynamically splitting convolution kernels in conjunction\nwith attention, selecting positions of interest, and splitting the original\nconvolution kernel into multiple smaller kernels, named DSConv. The proposed\nDSConv more effectively extracts features of different positions within the\nreceptive field, enhancing the network's generalization, optimization, and\nfeature representation capabilities. Furthermore, we innovate and enrich\nconcepts of dynamic splitting convolution and provide a novel network\narchitecture for pansharpening capable of achieving the tasks more efficiently,\nbuilding upon this methodology. Adequate fair experiments illustrate the\neffectiveness and the state-of-the-art performance attained by\nDSConv.Comprehensive and rigorous discussions proved the superiority and\noptimal usage conditions of DSConv.", "AI": {"tldr": "A new method called DSConv dynamically splits convolution kernels with attention for better feature extraction in pansharpening, outperforming existing methods.", "motivation": "Most existing pansharpening approaches rely on standard convolutions. This paper aims to address the limitations of standard convolutions by introducing adaptive convolutions, specifically DSConv, which leverages inter-pixel correlations in remote sensing images to improve feature extraction and achieve high-resolution image fusion more effectively.", "method": "The paper proposes a novel strategy called DSConv (Dynamically Splitting Convolution) that splits convolution kernels into multiple smaller kernels, combined with attention mechanisms, to better extract features from different positions within the receptive field. This enhances the network's generalization, optimization, and feature representation capabilities for pansharpening.", "result": "Experimental results demonstrate that DSConv achieves state-of-the-art performance in pansharpening, validating its effectiveness and superiority.", "conclusion": "The proposed DSConv, which dynamically splits convolution kernels in conjunction with attention, demonstrates superior performance and effectiveness in pansharpening tasks, achieving state-of-the-art results."}}
{"id": "2508.06152", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06152", "abs": "https://arxiv.org/abs/2508.06152", "authors": ["Kaiyuan Jiang", "Ruoxi Sun", "Ying Cao", "Yuqi Xu", "Xinran Zhang", "Junyan Guo", "ChengSheng Deng"], "title": "VISTAR:A User-Centric and Role-Driven Benchmark for Text-to-Image Evaluation", "comment": "17 pages,8 figures", "summary": "We present VISTAR, a user-centric, multi-dimensional benchmark for\ntext-to-image (T2I) evaluation that addresses the limitations of existing\nmetrics. VISTAR introduces a two-tier hybrid paradigm: it employs\ndeterministic, scriptable metrics for physically quantifiable attributes (e.g.,\ntext rendering, lighting) and a novel Hierarchical Weighted P/N Questioning\n(HWPQ) scheme that uses constrained vision-language models to assess abstract\nsemantics (e.g., style fusion, cultural fidelity). Grounded in a Delphi study\nwith 120 experts, we defined seven user roles and nine evaluation angles to\nconstruct the benchmark, which comprises 2,845 prompts validated by over 15,000\nhuman pairwise comparisons. Our metrics achieve high human alignment (>75%),\nwith the HWPQ scheme reaching 85.9% accuracy on abstract semantics,\nsignificantly outperforming VQA baselines. Comprehensive evaluation of\nstate-of-the-art models reveals no universal champion, as role-weighted scores\nreorder rankings and provide actionable guidance for domain-specific\ndeployment. All resources are publicly released to foster reproducible T2I\nassessment.", "AI": {"tldr": "VISTAR\u662f\u4e00\u4e2a\u65b0\u7684T2I\u8bc4\u4f30\u57fa\u51c6\uff0c\u5b83\u7ed3\u5408\u4e86\u786c\u6027\u548c\u8f6f\u6027\u6307\u6807\uff0c\u5e76\u901a\u8fc715000\u591a\u6b21\u4eba\u5de5\u6bd4\u8f83\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u4ee5\u63d0\u4f9b\u66f4\u51c6\u786e\u3001\u66f4\u7b26\u5408\u7528\u6237\u9700\u6c42\u7684\u8bc4\u4f30\u3002", "motivation": "\u63d0\u51faVISTAR\uff0c\u4e00\u4e2a\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u3001\u591a\u7ef4\u5ea6\u7684\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u8bc4\u4f30\u57fa\u51c6\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u6307\u6807\u7684\u5c40\u9650\u6027\u3002", "method": "VISTAR\u5f15\u5165\u4e86\u4e00\u4e2a\u4e24\u5c42\u6df7\u5408\u8303\u5f0f\uff1a\u5b83\u91c7\u7528\u786e\u5b9a\u6027\u7684\u3001\u53ef\u811a\u672c\u5316\u7684\u6307\u6807\u6765\u8bc4\u4f30\u7269\u7406\u4e0a\u53ef\u91cf\u5316\u7684\u5c5e\u6027\uff08\u4f8b\u5982\uff0c\u6587\u672c\u6e32\u67d3\u3001\u5149\u7167\uff09\uff0c\u5e76\u91c7\u7528\u65b0\u9896\u7684\u5c42\u6b21\u52a0\u6743P/N\u95ee\u9898\uff08HWPQ\uff09\u65b9\u6848\uff0c\u4f7f\u7528\u7ea6\u675f\u5f0f\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u6765\u8bc4\u4f30\u62bd\u8c61\u8bed\u4e49\uff08\u4f8b\u5982\uff0c\u98ce\u683c\u878d\u5408\u3001\u6587\u5316\u4fdd\u771f\u5ea6\uff09\u3002", "result": "VISTAR\u7684\u6307\u6807\u5b9e\u73b0\u4e86\u9ad8\u5ea6\u7684\u4eba\u7c7b\u4e00\u81f4\u6027\uff08>75%\uff09\u3002", "conclusion": "VISTAR\u5728\u62bd\u8c61\u8bed\u4e49\u65b9\u9762\u8fbe\u5230\u4e8685.9%\u7684\u51c6\u786e\u7387\uff0c\u663e\u8457\u4f18\u4e8eVQA\u57fa\u7ebf\u3002\u5bf9\u6700\u5148\u8fdb\u6a21\u578b\u7684\u5168\u9762\u8bc4\u4f30\u663e\u793a\uff0c\u6ca1\u6709\u666e\u904d\u7684\u51a0\u519b\u6a21\u578b\uff0c\u57fa\u4e8e\u89d2\u8272\u7684\u52a0\u6743\u5206\u6570\u91cd\u65b0\u6392\u5e8f\u4e86\u6392\u540d\uff0c\u5e76\u4e3a\u7279\u5b9a\u9886\u57df\u7684\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u6307\u5bfc\u3002\u6240\u6709\u8d44\u6e90\u5747\u516c\u5f00\u53d1\u5e03\uff0c\u4ee5\u4fc3\u8fdb\u53ef\u91cd\u73b0\u7684T2I\u8bc4\u4f30\u3002"}}
{"id": "2508.06157", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06157", "abs": "https://arxiv.org/abs/2508.06157", "authors": ["Xiaoxiao Yang", "Meiliang Liu", "Yunfang Xu", "Zijin Li", "Zhengye Si", "Xinyue Yang", "Zhiwen Zhao"], "title": "An Interpretable Multi-Plane Fusion Framework With Kolmogorov-Arnold Network Guided Attention Enhancement for Alzheimer's Disease Diagnosis", "comment": null, "summary": "Alzheimer's disease (AD) is a progressive neurodegenerative disorder that\nseverely impairs cognitive function and quality of life. Timely intervention in\nAD relies heavily on early and precise diagnosis, which remains challenging due\nto the complex and subtle structural changes in the brain. Most existing deep\nlearning methods focus only on a single plane of structural magnetic resonance\nimaging (sMRI) and struggle to accurately capture the complex and nonlinear\nrelationships among pathological regions of the brain, thus limiting their\nability to precisely identify atrophic features. To overcome these limitations,\nwe propose an innovative framework, MPF-KANSC, which integrates multi-plane\nfusion (MPF) for combining features from the coronal, sagittal, and axial\nplanes, and a Kolmogorov-Arnold Network-guided spatial-channel attention\nmechanism (KANSC) to more effectively learn and represent sMRI atrophy\nfeatures. Specifically, the proposed model enables parallel feature extraction\nfrom multiple anatomical planes, thus capturing more comprehensive structural\ninformation. The KANSC attention mechanism further leverages a more flexible\nand accurate nonlinear function approximation technique, facilitating precise\nidentification and localization of disease-related abnormalities. Experiments\non the ADNI dataset confirm that the proposed MPF-KANSC achieves superior\nperformance in AD diagnosis. Moreover, our findings provide new evidence of\nright-lateralized asymmetry in subcortical structural changes during AD\nprogression, highlighting the model's promising interpretability.", "AI": {"tldr": "\u63d0\u51faMPF-KANSC\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u5e73\u9762\u878d\u5408\u548cKANSC\u6ce8\u610f\u529b\u673a\u5236\uff0c\u63d0\u9ad8\u4e86sMRI\u5728\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u8bca\u65ad\u4e2d\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u7ed3\u6784\u78c1\u5171\u632f\u6210\u50cf\uff08sMRI\uff09\u7684\u5355\u4e00\u5e73\u9762\uff0c\u96be\u4ee5\u51c6\u786e\u6355\u6349\u75c5\u53d8\u533a\u57df\u4e4b\u95f4\u590d\u6742\u4e14\u975e\u5728\u7ebf\u6027\u7684\u5173\u7cfb\uff0c\u4ece\u800c\u9650\u5236\u4e86\u5176\u8bc6\u522b\u840e\u7f29\u7279\u5f81\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u6846\u67b6MPF-KANSC\uff0c\u5b83\u6574\u5408\u4e86\u591a\u5e73\u9762\u878d\u5408\uff08MPF\uff09\u4ee5\u7ed3\u5408\u6765\u81ea\u51a0\u72b6\u9762\u3001\u77e2\u72b6\u9762\u548c\u8f74\u72b6\u9762\u7684\u7279\u5f81\uff0c\u5e76\u5229\u7528\u4e86Kolmogorov-Arnold\u7f51\u7edc\u5f15\u5bfc\u7684\u7a7a\u95f4\u901a\u9053\u6ce8\u610f\u529b\u673a\u5236\uff08KANSC\uff09\uff0c\u4ee5\u66f4\u6709\u6548\u5730\u5b66\u4e60\u548c\u8868\u5f81sMRI\u840e\u7f29\u7279\u5f81\u3002", "result": "MPF-KANSC\u80fd\u591f\u4ece\u591a\u4e2a\u89e3\u5256\u5e73\u9762\u5e76\u884c\u63d0\u53d6\u7279\u5f81\uff0c\u6355\u6349\u66f4\u5168\u9762\u7684\u7ed3\u6784\u4fe1\u606f\u3002KANSC\u6ce8\u610f\u529b\u673a\u5236\u5229\u7528\u66f4\u7075\u6d3b\u548c\u51c6\u786e\u7684\u975e\u7ebf\u6027\u51fd\u6570\u903c\u8fd1\u6280\u672f\uff0c\u80fd\u591f\u7cbe\u786e\u8bc6\u522b\u548c\u5b9a\u4f4d\u4e0e\u75be\u75c5\u76f8\u5173\u7684\u5f02\u5e38\u3002", "conclusion": "MPF-KANSC\u5728ADNI\u6570\u636e\u96c6\u7684\u5b9e\u9a8c\u8868\u660e\u5176\u5728AD\u8bca\u65ad\u65b9\u9762\u53d6\u5f97\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u8be5\u6a21\u578b\u63ed\u793a\u4e86\u53f3\u4fa7\u4e0d\u5bf9\u79f0\u6027\u5728AD\u8fdb\u5c55\u7684\u5185\u4fa7\u7ed3\u6784\u53d8\u5316\u4e2d\u7684\u4f5c\u7528\uff0c\u8bc1\u660e\u4e86\u5176\u826f\u597d\u7684\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2508.06160", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06160", "abs": "https://arxiv.org/abs/2508.06160", "authors": ["Zhenbang Du", "Yonggan Fu", "Lifu Wang", "Jiayi Qian", "Xiao Luo", "Yingyan", "Lin"], "title": "Fewer Denoising Steps or Cheaper Per-Step Inference: Towards Compute-Optimal Diffusion Model Deployment", "comment": "Accepted by ICCV 2025", "summary": "Diffusion models have shown remarkable success across generative tasks, yet\ntheir high computational demands challenge deployment on resource-limited\nplatforms. This paper investigates a critical question for compute-optimal\ndiffusion model deployment: Under a post-training setting without fine-tuning,\nis it more effective to reduce the number of denoising steps or to use a\ncheaper per-step inference? Intuitively, reducing the number of denoising steps\nincreases the variability of the distributions across steps, making the model\nmore sensitive to compression. In contrast, keeping more denoising steps makes\nthe differences smaller, preserving redundancy, and making post-training\ncompression more feasible. To systematically examine this, we propose PostDiff,\na training-free framework for accelerating pre-trained diffusion models by\nreducing redundancy at both the input level and module level in a post-training\nmanner. At the input level, we propose a mixed-resolution denoising scheme\nbased on the insight that reducing generation resolution in early denoising\nsteps can enhance low-frequency components and improve final generation\nfidelity. At the module level, we employ a hybrid module caching strategy to\nreuse computations across denoising steps. Extensive experiments and ablation\nstudies demonstrate that (1) PostDiff can significantly improve the\nfidelity-efficiency trade-off of state-of-the-art diffusion models, and (2) to\nboost efficiency while maintaining decent generation fidelity, reducing\nper-step inference cost is often more effective than reducing the number of\ndenoising steps. Our code is available at\nhttps://github.com/GATECH-EIC/PostDiff.", "AI": {"tldr": "PostDiff\u901a\u8fc7\u6df7\u5408\u5206\u8fa8\u7387\u53bb\u566a\u548c\u6a21\u5757\u7f13\u5b58\u7b56\u7565\uff0c\u5728\u8bad\u7ec3\u540e\u52a0\u901f\u6269\u6563\u6a21\u578b\uff0c\u5e76\u5728\u6548\u7387\u548c\u4fdd\u771f\u5ea6\u4e4b\u95f4\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u5e73\u8861\uff0c\u5c24\u5176\u662f\u5728\u964d\u4f4e\u6bcf\u6b65\u63a8\u7406\u6210\u672c\u65b9\u9762\u6548\u679c\u66f4\u4f73\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u9ad8\u8ba1\u7b97\u9700\u6c42\u9650\u5236\u4e86\u5176\u5728\u8d44\u6e90\u6709\u9650\u5e73\u53f0\u4e0a\u7684\u90e8\u7f72\u3002\u8be5\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u4e00\u4e2a\u5173\u952e\u95ee\u9898\uff1a\u5728\u8bad\u7ec3\u540e\uff08\u65e0\u5fae\u8c03\uff09\u8bbe\u7f6e\u4e0b\uff0c\u662f\u51cf\u5c11\u53bb\u566a\u6b65\u6570\u8fd8\u662f\u4f7f\u7528\u66f4\u4fbf\u5b9c\u7684\u6bcf\u6b65\u63a8\u7406\u66f4\u6709\u6548\uff1f", "method": "\u63d0\u51faPostDiff\u6846\u67b6\uff0c\u5305\u542b\u6df7\u5408\u5206\u8fa8\u7387\u53bb\u566a\u65b9\u6848\uff08\u901a\u8fc7\u5728\u65e9\u671f\u53bb\u566a\u6b65\u9aa4\u4e2d\u964d\u4f4e\u751f\u6210\u5206\u8fa8\u7387\u6765\u589e\u5f3a\u4f4e\u9891\u5206\u91cf\u548c\u63d0\u9ad8\u6700\u7ec8\u751f\u6210\u4fdd\u771f\u5ea6\uff09\u548c\u6df7\u5408\u6a21\u5757\u7f13\u5b58\u7b56\u7565\uff08\u8de8\u53bb\u566a\u6b65\u9aa4\u91cd\u7528\u8ba1\u7b97\uff09\uff0c\u5728\u8bad\u7ec3\u540e\uff08\u65e0\u5fae\u8c03\uff09\u52a0\u901f\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u3002", "result": "PostDiff\u80fd\u663e\u8457\u6539\u5584\u6700\u5148\u8fdb\u6269\u6563\u6a21\u578b\u7684\u4fdd\u771f\u5ea6-\u6548\u7387\u6743\u8861\u3002\u5728\u63d0\u9ad8\u6548\u7387\u540c\u65f6\u4fdd\u6301\u53ef\u63a5\u53d7\u7684\u751f\u6210\u4fdd\u771f\u5ea6\u65b9\u9762\uff0c\u964d\u4f4e\u6bcf\u6b65\u63a8\u7406\u6210\u672c\u6bd4\u51cf\u5c11\u53bb\u566a\u6b65\u6570\u66f4\u6709\u6548\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPostDiff\u7684\u8bad\u7ec3\u65e0\u5173\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u8bad\u7ec3\u540e\u52a0\u901f\u9884\u8bad\u7ec3\u7684\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u5728\u8f93\u5165\u548c\u6a21\u5757\u5c42\u9762\u51cf\u5c11\u5197\u4f59\u6765\u5b9e\u73b0\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cPostDiff\u80fd\u663e\u8457\u6539\u5584\u6700\u5148\u8fdb\u6269\u6563\u6a21\u578b\u7684\u4fdd\u771f\u5ea6-\u6548\u7387\u6743\u8861\u3002\u7814\u7a76\u8fd8\u53d1\u73b0\uff0c\u4e3a\u4e86\u5728\u4fdd\u6301\u53ef\u63a5\u53d7\u751f\u6210\u4fdd\u771f\u5ea6\u7684\u540c\u65f6\u63d0\u9ad8\u6548\u7387\uff0c\u964d\u4f4e\u6bcf\u6b65\u63a8\u7406\u6210\u672c\u901a\u5e38\u6bd4\u51cf\u5c11\u53bb\u566a\u6b65\u6570\u66f4\u6709\u6548\u3002"}}
{"id": "2508.06169", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06169", "abs": "https://arxiv.org/abs/2508.06169", "authors": ["Wenpeng Xing", "Jie Chen", "Zaifeng Yang", "Changting Lin", "Jianfeng Dong", "Chaochao Chen", "Xun Zhou", "Meng Han"], "title": "UW-3DGS: Underwater 3D Reconstruction with Physics-Aware Gaussian Splatting", "comment": null, "summary": "Underwater 3D scene reconstruction faces severe challenges from light\nabsorption, scattering, and turbidity, which degrade geometry and color\nfidelity in traditional methods like Neural Radiance Fields (NeRF). While NeRF\nextensions such as SeaThru-NeRF incorporate physics-based models, their MLP\nreliance limits efficiency and spatial resolution in hazy environments. We\nintroduce UW-3DGS, a novel framework adapting 3D Gaussian Splatting (3DGS) for\nrobust underwater reconstruction. Key innovations include: (1) a plug-and-play\nlearnable underwater image formation module using voxel-based regression for\nspatially varying attenuation and backscatter; and (2) a Physics-Aware\nUncertainty Pruning (PAUP) branch that adaptively removes noisy floating\nGaussians via uncertainty scoring, ensuring artifact-free geometry. The\npipeline operates in training and rendering stages. During training, noisy\nGaussians are optimized end-to-end with underwater parameters, guided by PAUP\npruning and scattering modeling. In rendering, refined Gaussians produce clean\nUnattenuated Radiance Images (URIs) free from media effects, while learned\nphysics enable realistic Underwater Images (UWIs) with accurate light\ntransport. Experiments on SeaThru-NeRF and UWBundle datasets show superior\nperformance, achieving PSNR of 27.604, SSIM of 0.868, and LPIPS of 0.104 on\nSeaThru-NeRF, with ~65% reduction in floating artifacts.", "AI": {"tldr": "UW-3DGS\u4f7f\u75283D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\uff0c\u901a\u8fc7\u5b66\u4e60\u6c34\u4e0b\u6210\u50cf\u6a21\u578b\u548c\u526a\u679d\u4f2a\u5f71\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6c34\u4e0b\u4e09\u7ef4\u91cd\u5efa\u7684\u96be\u9898\uff0c\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u7684\u6c34\u4e0b\u4e09\u7ef4\u573a\u666f\u91cd\u5efa\u65b9\u6cd5\uff08\u5982NeRF\u53ca\u5176\u6269\u5c55\uff09\u5728\u5904\u7406\u5149\u7ebf\u5438\u6536\u3001\u6563\u5c04\u548c\u6d4a\u5ea6\u7b49\u95ee\u9898\u65f6\u9762\u4e34\u6311\u6218\uff0c\u5e76\u4e14MLP\u7684\u9650\u5236\u5f71\u54cd\u4e86\u6548\u7387\u548c\u7a7a\u95f4\u5206\u8fa8\u7387\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u66f4\u7cbe\u786e\u7684\u6c34\u4e0b\u91cd\u5efa\u65b9\u6cd5\u3002", "method": "UW-3DGS\u6846\u67b6\u7ed3\u5408\u4e86\u53ef\u5b66\u4e60\u7684\u6c34\u4e0b\u6210\u50cf\u6a21\u5757\uff08\u57fa\u4e8e\u4f53\u7d20\u56de\u5f52\uff09\u548c\u7269\u7406\u611f\u77e5\u4e0d\u786e\u5b9a\u6027\u526a\u679d\uff08PAUP\uff09\u5206\u652f\uff0c\u4f18\u5316\u9ad8\u65af\u6cfc\u6e85\u4ee5\u5904\u7406\u6c34\u4e0b\u5149\u7ebf\u5438\u6536\u3001\u6563\u5c04\u548c\u6d4a\u5ea6\u95ee\u9898\uff0c\u751f\u6210\u65e0\u4f2a\u5f71\u7684\u51e0\u4f55\u548c\u903c\u771f\u7684\u6c34\u4e0b\u56fe\u50cf\u3002", "result": "UW-3DGS\u5728SeaThru-NeRF\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e8627.604\u7684PSNR\u30010.868\u7684SSIM\u548c0.104\u7684LPIPS\uff0c\u5e76\u5c06\u6d6e\u52a8\u4f2a\u5f71\u51cf\u5c11\u4e86\u7ea665%\uff0c\u5c55\u793a\u4e86\u5176\u5728\u6c34\u4e0b\u4e09\u7ef4\u91cd\u5efa\u65b9\u9762\u7684\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "UW-3DGS\u901a\u8fc7\u5f15\u5165\u53ef\u5b66\u4e60\u7684\u6c34\u4e0b\u6210\u50cf\u6a21\u578b\u548c\u7269\u7406\u611f\u77e5\u4e0d\u786e\u5b9a\u6027\u526a\u679d\uff0c\u57283D\u9ad8\u65af\u6cfc\u6e85\u7684\u57fa\u7840\u4e0a\u5b9e\u73b0\u4e86\u9c81\u68d2\u7684\u6c34\u4e0b\u4e09\u7ef4\u573a\u666f\u91cd\u5efa\uff0c\u5e76\u5728SeaThru-NeRF\u548cUWBundle\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002"}}
{"id": "2508.06170", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06170", "abs": "https://arxiv.org/abs/2508.06170", "authors": ["Ojonugwa Oluwafemi Ejiga Peter", "Akingbola Oluwapemiisin", "Amalahu Chetachi", "Adeniran Opeyemi", "Fahmi Khalifa", "Md Mahmudur Rahman"], "title": "Synthetic Data-Driven Multi-Architecture Framework for Automated Polyp Segmentation Through Integrated Detection and Mask Generation", "comment": null, "summary": "Colonoscopy is a vital tool for the early diagnosis of colorectal cancer,\nwhich is one of the main causes of cancer-related mortality globally; hence, it\nis deemed an essential technique for the prevention and early detection of\ncolorectal cancer. The research introduces a unique multidirectional\narchitectural framework to automate polyp detection within colonoscopy images\nwhile helping resolve limited healthcare dataset sizes and annotation\ncomplexities. The research implements a comprehensive system that delivers\nsynthetic data generation through Stable Diffusion enhancements together with\ndetection and segmentation algorithms. This detection approach combines Faster\nR-CNN for initial object localization while the Segment Anything Model (SAM)\nrefines the segmentation masks. The faster R-CNN detection algorithm achieved a\nrecall of 93.08% combined with a precision of 88.97% and an F1 score of\n90.98%.SAM is then used to generate the image mask. The research evaluated five\nstate-of-the-art segmentation models that included U-Net, PSPNet, FPN, LinkNet,\nand MANet using ResNet34 as a base model. The results demonstrate the superior\nperformance of FPN with the highest scores of PSNR (7.205893) and SSIM\n(0.492381), while UNet excels in recall (84.85%) and LinkNet shows balanced\nperformance in IoU (64.20%) and Dice score (77.53%).", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408Stable Diffusion\u3001Faster R-CNN\u548cSAM\u7684\u591a\u65b9\u5411\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u68c0\u6d4b\u548c\u5206\u5272\u7ed3\u80a0\u955c\u56fe\u50cf\u4e2d\u7684\u606f\u8089\uff0c\u4ee5\u89e3\u51b3\u6570\u636e\u96c6\u9650\u5236\u95ee\u9898\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u606f\u8089\u68c0\u6d4b\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u4e14FPN\u3001U-Net\u548cLinkNet\u5728\u5206\u5272\u4efb\u52a1\u7684\u4e0d\u540c\u6307\u6807\u4e0a\u5404\u6709\u4f18\u52bf\u3002", "motivation": "\u7ed3\u76f4\u80a0\u764c\u662f\u5168\u7403\u4e3b\u8981\u7684\u764c\u75c7\u76f8\u5173\u6b7b\u4ea1\u539f\u56e0\u4e4b\u4e00\uff0c\u800c\u7ed3\u80a0\u955c\u68c0\u67e5\u662f\u65e9\u671f\u8bca\u65ad\u7684\u5173\u952e\u5de5\u5177\u3002\u7136\u800c\uff0c\u533b\u7597\u6570\u636e\u96c6\u7684\u89c4\u6a21\u6709\u9650\u548c\u6ce8\u91ca\u7684\u590d\u6742\u6027\u7ed9\u81ea\u52a8\u5316\u606f\u8089\u68c0\u6d4b\u5e26\u6765\u4e86\u6311\u6218\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5f15\u5165\u521b\u65b0\u7684\u591a\u65b9\u5411\u67b6\u6784\u6846\u67b6\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u4ee5\u81ea\u52a8\u5316\u7ed3\u80a0\u955c\u56fe\u50cf\u4e2d\u7684\u606f\u8089\u68c0\u6d4b\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u65b9\u5411\u67b6\u6784\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u751f\u6210\u6a21\u578b\uff08Stable Diffusion\uff09\u6765\u751f\u6210\u5408\u6210\u6570\u636e\uff0c\u4ee5\u53ca\u68c0\u6d4b\u548c\u5206\u5272\u7b97\u6cd5\uff08Faster R-CNN\u548cSegment Anything Model (SAM)\uff09\u6765\u81ea\u52a8\u68c0\u6d4b\u7ed3\u80a0\u606f\u8089\u3002Faster R-CNN\u7528\u4e8e\u5bf9\u8c61\u5b9a\u4f4d\uff0cSAM\u7528\u4e8e\u7ec6\u5316\u5206\u5272\u63a9\u7801\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u8bc4\u4f30\u4e86\u4e94\u79cd\u6700\u5148\u8fdb\u7684\u5206\u5272\u6a21\u578b\uff08U-Net, PSPNet, FPN, LinkNet, MANet\uff09\uff0c\u5e76\u4f7f\u7528ResNet34\u4f5c\u4e3a\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "result": "Faster R-CNN\u68c0\u6d4b\u7b97\u6cd5\u5b9e\u73b0\u4e8693.08%\u7684\u53ec\u56de\u7387\u300188.97%\u7684\u7cbe\u786e\u7387\u548c90.98%\u7684F1\u5206\u6570\u3002\u5728\u5206\u5272\u6a21\u578b\u8bc4\u4f30\u4e2d\uff0cFPN\u5728PSNR\uff087.205893\uff09\u548cSSIM\uff080.492381\uff09\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff0cU-Net\u5728\u53ec\u56de\u7387\uff0884.85%\uff09\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff0cLinkNet\u5728IoU\uff0864.20%\uff09\u548cDice\u5206\u6570\uff0877.53%\uff09\u65b9\u9762\u8868\u73b0\u5747\u8861\u3002", "conclusion": "FPN\u5728PSNR\u548cSSIM\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff0cU-Net\u5728\u53ec\u56de\u7387\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff0cLinkNet\u5728IoU\u548cDice\u5206\u6570\u65b9\u9762\u8868\u73b0\u5747\u8861\uff0c\u8868\u660e\u5728\u5904\u7406\u7ed3\u80a0\u955c\u56fe\u50cf\u7684\u5206\u5272\u4efb\u52a1\u65f6\uff0c\u4e0d\u540c\u7684\u6a21\u578b\u5177\u6709\u5404\u81ea\u7684\u4f18\u52bf\u3002"}}
{"id": "2508.06189", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06189", "abs": "https://arxiv.org/abs/2508.06189", "authors": ["Cheng Liu", "Daou Zhang", "Tingxu Liu", "Yuhan Wang", "Jinyang Chen", "Yuexuan Li", "Xinying Xiao", "Chenbo Xin", "Ziru Wang", "Weichao Wu"], "title": "MA-CBP: A Criminal Behavior Prediction Framework Based on Multi-Agent Asynchronous Collaboration", "comment": null, "summary": "With the acceleration of urbanization, criminal behavior in public scenes\nposes an increasingly serious threat to social security. Traditional anomaly\ndetection methods based on feature recognition struggle to capture high-level\nbehavioral semantics from historical information, while generative approaches\nbased on Large Language Models (LLMs) often fail to meet real-time\nrequirements. To address these challenges, we propose MA-CBP, a criminal\nbehavior prediction framework based on multi-agent asynchronous collaboration.\nThis framework transforms real-time video streams into frame-level semantic\ndescriptions, constructs causally consistent historical summaries, and fuses\nadjacent image frames to perform joint reasoning over long- and short-term\ncontexts. The resulting behavioral decisions include key elements such as event\nsubjects, locations, and causes, enabling early warning of potential criminal\nactivity. In addition, we construct a high-quality criminal behavior dataset\nthat provides multi-scale language supervision, including frame-level,\nsummary-level, and event-level semantic annotations. Experimental results\ndemonstrate that our method achieves superior performance on multiple datasets\nand offers a promising solution for risk warning in urban public safety\nscenarios.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.06191", "categories": ["cs.CV", "68T45, 92C55", "I.4.6; I.5.4; J.3"], "pdf": "https://arxiv.org/pdf/2508.06191", "abs": "https://arxiv.org/abs/2508.06191", "authors": ["Ruixiang Tang", "Jianglong Qin", "Mingda Zhang", "Yan Song", "Yi Wu", "Wei Wu"], "title": "A Semantic Segmentation Algorithm for Pleural Effusion Based on DBIF-AUNet", "comment": "12 pages, 6 figures, 2 tables", "summary": "Pleural effusion semantic segmentation can significantly enhance the accuracy\nand timeliness of clinical diagnosis and treatment by precisely identifying\ndisease severity and lesion areas. Currently, semantic segmentation of pleural\neffusion CT images faces multiple challenges. These include similar gray levels\nbetween effusion and surrounding tissues, blurred edges, and variable\nmorphology. Existing methods often struggle with diverse image variations and\ncomplex edges, primarily because direct feature concatenation causes semantic\ngaps. To address these challenges, we propose the Dual-Branch Interactive\nFusion Attention model (DBIF-AUNet). This model constructs a densely nested\nskip-connection network and innovatively refines the Dual-Domain Feature\nDisentanglement module (DDFD). The DDFD module orthogonally decouples the\nfunctions of dual-domain modules to achieve multi-scale feature complementarity\nand enhance characteristics at different levels. Concurrently, we design a\nBranch Interaction Attention Fusion module (BIAF) that works synergistically\nwith the DDFD. This module dynamically weights and fuses global, local, and\nfrequency band features, thereby improving segmentation robustness.\nFurthermore, we implement a nested deep supervision mechanism with hierarchical\nadaptive hybrid loss to effectively address class imbalance. Through validation\non 1,622 pleural effusion CT images from Southwest Hospital, DBIF-AUNet\nachieved IoU and Dice scores of 80.1% and 89.0% respectively. These results\noutperform state-of-the-art medical image segmentation models U-Net++ and\nSwin-UNet by 5.7%/2.7% and 2.2%/1.5% respectively, demonstrating significant\noptimization in segmentation accuracy for complex pleural effusion CT images.", "AI": {"tldr": "\u63d0\u51faDBIF-AUNet\u6a21\u578b\uff0c\u901a\u8fc7\u53cc\u57df\u7279\u5f81\u89e3\u8026\u548c\u5206\u652f\u4ea4\u4e92\u6ce8\u610f\u529b\u878d\u5408\uff0c\u89e3\u51b3\u4e86\u80f8\u8154\u79ef\u6db2CT\u56fe\u50cf\u5206\u5272\u7684\u6311\u6218\uff0c\u5206\u5272\u7cbe\u5ea6\u5f97\u5230\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u76ee\u524d\u7684\u80f8\u8154\u79ef\u6db2CT\u56fe\u50cf\u8bed\u4e49\u5206\u5272\u65b9\u6cd5\u5728\u5904\u7406\u76f8\u4f3c\u7070\u5ea6\u3001\u6a21\u7cca\u8fb9\u7f18\u548c\u591a\u53d8\u5f62\u6001\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u56e0\u76f4\u63a5\u7279\u5f81\u8fde\u63a5\u5bfc\u81f4\u8bed\u4e49\u9e3f\u6c9f\uff0c\u96be\u4ee5\u5e94\u5bf9\u591a\u6837\u5316\u7684\u56fe\u50cf\u53d8\u5316\u548c\u590d\u6742\u8fb9\u7f18\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDBIF-AUNet\u7684\u53cc\u5206\u652f\u4ea4\u4e92\u878d\u5408\u6ce8\u610f\u529b\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5305\u542b\u4e00\u4e2a\u5bc6\u96c6\u5d4c\u5957\u7684\u8df3\u8dc3\u8fde\u63a5\u7f51\u7edc\uff0c\u5e76\u5f15\u5165\u4e86\u53cc\u57df\u7279\u5f81\u89e3\u8026\uff08DDFD\uff09\u6a21\u5757\u548c\u5206\u652f\u4ea4\u4e92\u6ce8\u610f\u529b\u878d\u5408\uff08BIAF\uff09\u6a21\u5757\u3002DDFD\u6a21\u5757\u6b63\u4ea4\u89e3\u8026\u53cc\u57df\u6a21\u5757\u7684\u529f\u80fd\u4ee5\u5b9e\u73b0\u591a\u5c3a\u5ea6\u7279\u5f81\u4e92\u8865\uff0cBIAF\u6a21\u5757\u52a8\u6001\u52a0\u6743\u5e76\u878d\u5408\u5168\u5c40\u3001\u5c40\u90e8\u548c\u9891\u5e26\u7279\u5f81\uff0c\u6b64\u5916\uff0c\u8fd8\u5b9e\u73b0\u4e86\u5d4c\u5957\u6df1\u5ea6\u76d1\u7763\u673a\u5236\u548c\u5206\u5c42\u81ea\u9002\u5e94\u6df7\u5408\u635f\u5931\u4ee5\u5904\u7406\u7c7b\u522b\u4e0d\u5e73\u8861\u3002", "result": "DBIF-AUNet\u57281,622\u5f20\u80f8\u8154\u79ef\u6db2CT\u56fe\u50cf\u4e0a\u5b9e\u73b0\u4e8680.1%\u7684IoU\u548c89.0%\u7684Dice\u5f97\u5206\uff0c\u76f8\u6bd4U-Net++\u548cSwin-UNet\u5206\u522b\u63d0\u9ad8\u4e865.7%/2.7%\u548c2.2%/1.5%\u3002", "conclusion": "DBIF-AUNet\u5728\u5206\u5272\u590d\u6742\u80f8\u8154\u79ef\u6db2CT\u56fe\u50cf\u65b9\u9762\u8868\u73b0\u51fa\u663e\u8457\u4f18\u5316\uff0c\u57281,622\u5f20\u56fe\u50cf\u4e0a\u5b9e\u73b0\u4e8680.1%\u7684IoU\u548c89.0%\u7684Dice\u5f97\u5206\uff0c\u4f18\u4e8eU-Net++\u548cSwin-UNet\u7b49\u5148\u8fdb\u6a21\u578b\u3002"}}
{"id": "2508.06202", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06202", "abs": "https://arxiv.org/abs/2508.06202", "authors": ["Chang Che", "Ziqi Wang", "Pengwan Yang", "Qi Wang", "Hui Ma", "Zenglin Shi"], "title": "LoRA in LoRA: Towards Parameter-Efficient Architecture Expansion for Continual Visual Instruction Tuning", "comment": null, "summary": "Continual Visual Instruction Tuning (CVIT) enables Multimodal Large Language\nModels (MLLMs) to incrementally learn new tasks over time. However, this\nprocess is challenged by catastrophic forgetting, where performance on\npreviously learned tasks deteriorates as the model adapts to new ones. A common\napproach to mitigate forgetting is architecture expansion, which introduces\ntask-specific modules to prevent interference. Yet, existing methods often\nexpand entire layers for each task, leading to significant parameter overhead\nand poor scalability. To overcome these issues, we introduce LoRA in LoRA\n(LiLoRA), a highly efficient architecture expansion method tailored for CVIT in\nMLLMs. LiLoRA shares the LoRA matrix A across tasks to reduce redundancy,\napplies an additional low-rank decomposition to matrix B to minimize\ntask-specific parameters, and incorporates a cosine-regularized stability loss\nto preserve consistency in shared representations over time. Extensive\nexperiments on a diverse CVIT benchmark show that LiLoRA consistently achieves\nsuperior performance in sequential task learning while significantly improving\nparameter efficiency compared to existing approaches.", "AI": {"tldr": "LiLoRA \u662f\u4e00\u79cd\u9ad8\u6548\u7684\u67b6\u6784\u6269\u5c55\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3 MLLMs \u5728\u6301\u7eed\u89c6\u89c9\u6307\u4ee4\u8c03\u4f18 (CVIT) \u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u901a\u8fc7\u5171\u4eab\u548c\u4f4e\u79e9\u5206\u89e3 LoRA \u77e9\u9635\u6765\u51cf\u5c11\u53c2\u6570\u5f00\u9500\uff0c\u5e76\u4f7f\u7528\u7a33\u5b9a\u6027\u635f\u5931\u6765\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u67b6\u6784\u6269\u5c55\u65b9\u6cd5\u5728\u4e3a MLLMs \u7684 CVIT \u4efb\u52a1\u5904\u7406\u707e\u96be\u6027\u9057\u5fd8\u65f6\uff0c\u901a\u5e38\u4f1a\u6269\u5c55\u6574\u4e2a\u5c42\uff0c\u5bfc\u81f4\u663e\u8457\u7684\u53c2\u6570\u5f00\u9500\u548c\u6269\u5c55\u6027\u5dee\u3002\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a LiLoRA \u7684\u9ad8\u6548\u67b6\u6784\u6269\u5c55\u65b9\u6cd5\uff0c\u7528\u4e8e MLLMs \u7684 CVIT\u3002LiLoRA \u901a\u8fc7\u5171\u4eab LoRA \u77e9\u9635 A \u6765\u51cf\u5c11\u5197\u4f59\uff0c\u5bf9\u77e9\u9635 B \u8fdb\u884c\u989d\u5916\u7684\u4f4e\u79e9\u5206\u89e3\u4ee5\u6700\u5c0f\u5316\u7279\u5b9a\u4efb\u52a1\u7684\u53c2\u6570\uff0c\u5e76\u5f15\u5165\u4e86\u4f59\u5f26\u6b63\u5219\u5316\u7a33\u5b9a\u6027\u635f\u5931\u6765\u4fdd\u6301\u5171\u4eab\u8868\u793a\u968f\u65f6\u95f4\u7684\u4e00\u81f4\u6027\u3002", "result": "\u5728\u591a\u6837\u5316\u7684 CVIT \u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cLiLoRA \u5728\u987a\u5e8f\u4efb\u52a1\u5b66\u4e60\u65b9\u9762\u59cb\u7ec8 achieves \u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u53c2\u6570\u6548\u7387\u3002", "conclusion": "LiLoRA \u5728\u89e3\u51b3 MLLMs \u7684 CVIT \u573a\u666f\u4e0b\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u4e0a\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u548c\u53c2\u6570\u6548\u7387\uff0c\u901a\u8fc7\u5171\u4eab LoRA \u77e9\u9635 A \u5e76\u5bf9\u77e9\u9635 B \u8fdb\u884c\u4f4e\u79e9\u5206\u89e3\uff0c\u540c\u65f6\u7ed3\u5408\u4f59\u5f26\u6b63\u5219\u5316\u7a33\u5b9a\u6027\u635f\u5931\u6765\u4fdd\u7559\u5171\u4eab\u8868\u793a\u7684\u4e00\u81f4\u6027\u3002"}}
{"id": "2508.06203", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06203", "abs": "https://arxiv.org/abs/2508.06203", "authors": ["Zhaopeng Gu", "Bingke Zhu", "Guibo Zhu", "Yingying Chen", "Wei Ge", "Ming Tang", "Jinqiao Wang"], "title": "AnomalyMoE: Towards a Language-free Generalist Model for Unified Visual Anomaly Detection", "comment": null, "summary": "Anomaly detection is a critical task across numerous domains and modalities,\nyet existing methods are often highly specialized, limiting their\ngeneralizability. These specialized models, tailored for specific anomaly types\nlike textural defects or logical errors, typically exhibit limited performance\nwhen deployed outside their designated contexts. To overcome this limitation,\nwe propose AnomalyMoE, a novel and universal anomaly detection framework based\non a Mixture-of-Experts (MoE) architecture. Our key insight is to decompose the\ncomplex anomaly detection problem into three distinct semantic hierarchies:\nlocal structural anomalies, component-level semantic anomalies, and global\nlogical anomalies. AnomalyMoE correspondingly employs three dedicated expert\nnetworks at the patch, component, and global levels, and is specialized in\nreconstructing features and identifying deviations at its designated semantic\nlevel. This hierarchical design allows a single model to concurrently\nunderstand and detect a wide spectrum of anomalies. Furthermore, we introduce\nan Expert Information Repulsion (EIR) module to promote expert diversity and an\nExpert Selection Balancing (ESB) module to ensure the comprehensive utilization\nof all experts. Experiments on 8 challenging datasets spanning industrial\nimaging, 3D point clouds, medical imaging, video surveillance, and logical\nanomaly detection demonstrate that AnomalyMoE establishes new state-of-the-art\nperformance, significantly outperforming specialized methods in their\nrespective domains.", "AI": {"tldr": "A new universal anomaly detection framework, AnomalyMoE, uses a Mixture-of-Experts approach with hierarchical experts to detect various anomalies across different data types, achieving state-of-the-art results.", "motivation": "Existing anomaly detection methods are often highly specialized and limited in generalizability, performing poorly outside their designated contexts. This work aims to overcome this limitation by proposing a novel and universal anomaly detection framework.", "method": "AnomalyMoE employs a Mixture-of-Experts (MoE) architecture with three expert networks at the patch, component, and global levels, specialized in reconstructing features and identifying deviations at their designated semantic levels. It also includes an Expert Information Repulsion (EIR) module for expert diversity and an Expert Selection Balancing (ESB) module for comprehensive expert utilization.", "result": "Experiments on 8 challenging datasets spanning industrial imaging, 3D point clouds, medical imaging, video surveillance, and logical anomaly detection demonstrate that AnomalyMoE establishes new state-of-the-art performance, significantly outperforming specialized methods in their respective domains.", "conclusion": "AnomalyMoE, a novel universal anomaly detection framework based on a Mixture-of-Experts (MoE) architecture, establishes new state-of-the-art performance across diverse datasets, significantly outperforming specialized methods."}}
{"id": "2508.06205", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06205", "abs": "https://arxiv.org/abs/2508.06205", "authors": ["Ruiyan Wang", "Lin Zuo", "Zonghao Lin", "Qiang Wang", "Zhengxue Cheng", "Rong Xie", "Jun Ling", "Li Song"], "title": "PA-HOI: A Physics-Aware Human and Object Interaction Dataset", "comment": null, "summary": "The Human-Object Interaction (HOI) task explores the dynamic interactions\nbetween humans and objects in physical environments, providing essential\nbiomechanical and cognitive-behavioral foundations for fields such as robotics,\nvirtual reality, and human-computer interaction. However, existing HOI data\nsets focus on details of affordance, often neglecting the influence of physical\nproperties of objects on human long-term motion. To bridge this gap, we\nintroduce the PA-HOI Motion Capture dataset, which highlights the impact of\nobjects' physical attributes on human motion dynamics, including human posture,\nmoving velocity, and other motion characteristics. The dataset comprises 562\nmotion sequences of human-object interactions, with each sequence performed by\nsubjects of different genders interacting with 35 3D objects that vary in size,\nshape, and weight. This dataset stands out by significantly extending the scope\nof existing ones for understanding how the physical attributes of different\nobjects influence human posture, speed, motion scale, and interacting\nstrategies. We further demonstrate the applicability of the PA-HOI dataset by\nintegrating it with existing motion generation methods, validating its capacity\nto transfer realistic physical awareness.", "AI": {"tldr": "PA-HOI\u6570\u636e\u96c6\u7684\u5efa\u7acb\u662f\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709HOI\u6570\u636e\u96c6\u5ffd\u89c6\u7269\u4f53\u7269\u7406\u5c5e\u6027\u5bf9\u4eba\u4f53\u8fd0\u52a8\u5f71\u54cd\u7684\u95ee\u9898\u3002\u8be5\u6570\u636e\u96c6\u5305\u542b562\u4e2a\u52a8\u4f5c\u5e8f\u5217\uff0c\u6db5\u76d6\u4e86\u4e0e\u4e0d\u540c3D\u7269\u4f53\u4ea4\u4e92\u7684\u5404\u79cd\u4eba\u4f53\u8fd0\u52a8\u3002\u901a\u8fc7\u5c06\u6b64\u6570\u636e\u96c6\u4e0e\u73b0\u6709\u8fd0\u52a8\u751f\u6210\u65b9\u6cd5\u76f8\u7ed3\u5408\uff0c\u7814\u7a76\u8bc1\u660e\u4e86\u5176\u5728\u4f20\u9012\u73b0\u5b9e\u7269\u7406\u611f\u77e5\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709HOI\u6570\u636e\u96c6\u4e3b\u8981\u5173\u6ce8\u4ea4\u4e92\u7684\u7ec6\u8282\uff0c\u5ffd\u89c6\u4e86\u7269\u4f53\u7269\u7406\u5c5e\u6027\u5bf9\u4eba\u7c7b\u957f\u671f\u8fd0\u52a8\u7684\u5f71\u54cd\uff0c\u672c\u7814\u7a76\u65e8\u5728\u5f25\u8865\u8fd9\u4e00\u5dee\u8ddd\u3002", "method": "\u672c\u7814\u7a76\u5f15\u5165\u4e86PA-HOI\u52a8\u4f5c\u6355\u6349\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u5305\u542b562\u4e2a\u4eba\u4e0e\u7269\u76f8\u4e92\u4f5c\u7528\u7684\u52a8\u4f5c\u5e8f\u5217\uff0c\u6db5\u76d6\u4e86\u4e0d\u540c\u6027\u522b\u7684\u4e3b\u4f53\u4e0e35\u4e2a\u4e0d\u540c\u5927\u5c0f\u3001\u5f62\u72b6\u548c\u91cd\u91cf\u76843D\u5bf9\u8c61\u8fdb\u884c\u4ea4\u4e92\u3002", "result": "PA-HOI\u6570\u636e\u96c6\u901a\u8fc7\u5305\u542b\u4e0d\u540c\u7269\u4f53\u7269\u7406\u5c5e\u6027\u5bf9\u4eba\u4f53\u59ff\u52bf\u3001\u901f\u5ea6\u3001\u8fd0\u52a8\u5c3a\u5ea6\u548c\u4ea4\u4e92\u7b56\u7565\u7684\u5f71\u54cd\uff0c\u663e\u8457\u6269\u5c55\u4e86\u73b0\u6709\u6570\u636e\u96c6\u7684\u8303\u56f4\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u901a\u8fc7\u6574\u5408\u73b0\u6709\u8fd0\u52a8\u751f\u6210\u65b9\u6cd5\uff0c\u5e76\u9a8c\u8bc1\u5176\u5728\u8f6c\u79fb\u73b0\u5b9e\u7269\u7406\u611f\u77e5\u65b9\u9762\u7684\u80fd\u529b\uff0c\u8bc1\u660e\u4e86\u5176\u5728PA-HOI\u6570\u636e\u96c6\u4e0a\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2508.06452", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06452", "abs": "https://arxiv.org/abs/2508.06452", "authors": ["Mattia Litrico", "Mario Valerio Giuffrida", "Sebastiano Battiato", "Devis Tuia"], "title": "TRUST: Leveraging Text Robustness for Unsupervised Domain Adaptation", "comment": null, "summary": "Recent unsupervised domain adaptation (UDA) methods have shown great success\nin addressing classical domain shifts (e.g., synthetic-to-real), but they still\nsuffer under complex shifts (e.g. geographical shift), where both the\nbackground and object appearances differ significantly across domains. Prior\nworks showed that the language modality can help in the adaptation process,\nexhibiting more robustness to such complex shifts. In this paper, we introduce\nTRUST, a novel UDA approach that exploits the robustness of the language\nmodality to guide the adaptation of a vision model. TRUST generates\npseudo-labels for target samples from their captions and introduces a novel\nuncertainty estimation strategy that uses normalised CLIP similarity scores to\nestimate the uncertainty of the generated pseudo-labels. Such estimated\nuncertainty is then used to reweight the classification loss, mitigating the\nadverse effects of wrong pseudo-labels obtained from low-quality captions. To\nfurther increase the robustness of the vision model, we propose a multimodal\nsoft-contrastive learning loss that aligns the vision and language feature\nspaces, by leveraging captions to guide the contrastive training of the vision\nmodel on target images. In our contrastive loss, each pair of images acts as\nboth a positive and a negative pair and their feature representations are\nattracted and repulsed with a strength proportional to the similarity of their\ncaptions. This solution avoids the need for hardly determining positive and\nnegative pairs, which is critical in the UDA setting. Our approach outperforms\nprevious methods, setting the new state-of-the-art on classical (DomainNet) and\ncomplex (GeoNet) domain shifts. The code will be available upon acceptance.", "AI": {"tldr": "TRUST\u65b9\u6cd5\u5229\u7528\u56fe\u50cf\u6807\u9898\u7684\u9c81\u68d2\u6027\u6765\u6307\u5bfc\u89c6\u89c9\u6a21\u578b\u7684\u57df\u9002\u5e94\u3002\u901a\u8fc7\u751f\u6210\u4f2a\u6807\u7b7e\u5e76\u6839\u636e\u6807\u9898\u8d28\u91cf\u4f30\u8ba1\u5176\u4e0d\u786e\u5b9a\u6027\u6765\u4f18\u5316\u5206\u7c7b\u635f\u5931\u3002\u6b64\u5916\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u8f6f\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\uff0c\u5229\u7528\u6807\u9898\u6765\u5bf9\u9f50\u89c6\u89c9\u548c\u8bed\u8a00\u7279\u5f81\u7a7a\u95f4\uff0c\u4ece\u800c\u63d0\u9ad8\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u65e0\u76d1\u7763\u57df\u9002\u5e94\uff08UDA\uff09\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u7684\u5730\u7406\u57df\u8fc1\u79fb\uff08\u80cc\u666f\u548c\u7269\u4f53\u5916\u89c2\u5dee\u5f02\u663e\u8457\uff09\u65f6\u6548\u679c\u4e0d\u4f73\u3002\u8bed\u8a00\u6a21\u6001\u88ab\u8bc1\u660e\u5bf9\u8fd9\u7c7b\u590d\u6742\u8fc1\u79fb\u66f4\u5177\u9c81\u68d2\u6027\uff0c\u56e0\u6b64\u672c\u7814\u7a76\u65e8\u5728\u5229\u7528\u8bed\u8a00\u6a21\u6001\u7684\u9c81\u68d2\u6027\u6765\u6539\u8fdbUDA\u6a21\u578b\u3002", "method": "TRUST\u65b9\u6cd5\u5229\u7528\u8bed\u8a00\u7684\u9c81\u68d2\u6027\u6765\u6307\u5bfc\u89c6\u89c9\u6a21\u578b\u7684\u9002\u5e94\uff0c\u901a\u8fc7\u751f\u6210\u4f2a\u6807\u7b7e\uff08\u57fa\u4e8e\u56fe\u50cf\u7684\u6807\u9898\uff09\u5e76\u7ed3\u5408\u65b0\u9896\u7684\u3001\u57fa\u4e8e\u5f52\u4e00\u5316CLIP\u76f8\u4f3c\u5ea6\u5206\u6570\u7684\u4f2a\u6807\u7b7e\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u7b56\u7565\u6765\u91cd\u65b0\u52a0\u6743\u5206\u7c7b\u635f\u5931\u3002\u6b64\u5916\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u8f6f\u5bf9\u6bd4\u5b66\u4e60\u635f\u5931\uff0c\u5229\u7528\u6807\u9898\u5bf9\u9f50\u89c6\u89c9\u548c\u8bed\u8a00\u7279\u5f81\u7a7a\u95f4\uff0c\u5176\u4e2d\u56fe\u50cf\u5bf9\u540c\u65f6\u4f5c\u4e3a\u6b63\u4f8b\u548c\u8d1f\u4f8b\uff0c\u5176\u7279\u5f81\u8868\u793a\u7684\u5438\u5f15\u6216\u6392\u65a5\u5f3a\u5ea6\u4e0e\u6807\u9898\u7684\u76f8\u4f3c\u5ea6\u6210\u6b63\u6bd4\u3002", "result": "\u5728\u7ecf\u5178\uff08DomainNet\uff09\u548c\u590d\u6742\uff08GeoNet\uff09\u57df\u8fc1\u79fb\u4efb\u52a1\u4e0a\u8bbe\u5b9a\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "TRUST\u65b9\u6cd5\u5728\u7ecf\u5178\uff08DomainNet\uff09\u548c\u590d\u6742\uff08GeoNet\uff09\u57df\u8fc1\u79fb\u4efb\u52a1\u4e0a\u5747\u8d85\u8d8a\u4e86\u5148\u524d\u7684\u65b9\u6cd5\uff0c\u53d6\u5f97\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\u3002"}}
{"id": "2508.06218", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06218", "abs": "https://arxiv.org/abs/2508.06218", "authors": ["Zhiyan Bo", "Laura C. Coates", "Bartlomiej W. Papiez"], "title": "Interpretable Rheumatoid Arthritis Scoring via Anatomy-aware Multiple Instance Learning", "comment": "Accepted by MICCAI AMAI Workshop 2025", "summary": "The Sharp/van der Heijde (SvdH) score has been widely used in clinical trials\nto quantify radiographic damage in Rheumatoid Arthritis (RA), but its\ncomplexity has limited its adoption in routine clinical practice. To address\nthe inefficiency of manual scoring, this work proposes a two-stage pipeline for\ninterpretable image-level SvdH score prediction using dual-hand radiographs.\nOur approach extracts disease-relevant image regions and integrates them using\nattention-based multiple instance learning to generate image-level features for\nprediction. We propose two region extraction schemes: 1) sampling image tiles\nmost likely to contain abnormalities, and 2) cropping patches containing\ndisease-relevant joints. With Scheme 2, our best individual score prediction\nmodel achieved a Pearson's correlation coefficient (PCC) of 0.943 and a root\nmean squared error (RMSE) of 15.73. Ensemble learning further boosted\nprediction accuracy, yielding a PCC of 0.945 and RMSE of 15.57, achieving\nstate-of-the-art performance that is comparable to that of experienced\nradiologists (PCC = 0.97, RMSE = 18.75). Finally, our pipeline effectively\nidentified and made decisions based on anatomical structures which clinicians\nconsider relevant to RA progression.", "AI": {"tldr": "\u4e00\u79cd\u63d0\u9ad8\u7c7b\u98ce\u6e7f\u5173\u8282\u708e\uff08RA\uff09\u5f71\u50cf\u5b66\u8bc4\u5206\uff08SvdH\uff09\u9884\u6d4b\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7AI\u6a21\u578b\u8fbe\u5230\u4e86\u63a5\u8fd1\u4eba\u7c7b\u4e13\u5bb6\u7684\u51c6\u786e\u5ea6\u3002", "motivation": "\u89e3\u51b3SvdH\u8bc4\u5206\u5728\u4e34\u5e8a\u5b9e\u8df5\u4e2d\u5e94\u7528\u53d7\u9650\u7684\u590d\u6742\u6027\u548c\u6548\u7387\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e24\u79cd\u533a\u57df\u63d0\u53d6\u65b9\u6848\uff1a1\uff09\u5bf9\u6700\u6709\u53ef\u80fd\u5305\u542b\u5f02\u5e38\u7684\u56fe\u50cf\u56fe\u5757\u8fdb\u884c\u91c7\u6837\uff1b2\uff09\u88c1\u526a\u5305\u542b\u75be\u75c5\u76f8\u5173\u5173\u8282\u7684\u56fe\u50cf\u5757\u3002\u5229\u7528\u8fd9\u4e9b\u533a\u57df\u6765\u8bad\u7ec3\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684\u591a\u5b9e\u4f8b\u5b66\u4e60\u6a21\u578b\u4ee5\u8fdb\u884cSvdH\u8bc4\u5206\u9884\u6d4b\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728SvdH\u8bc4\u5206\u9884\u6d4b\u65b9\u9762\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u6700\u4f73\u6a21\u578b\u5b9e\u73b0\u4e860.943\u7684\u76ae\u5c14\u900a\u76f8\u5173\u7cfb\u6570\uff08PCC\uff09\u548c15.73\u7684\u5747\u65b9\u6839\u8bef\u5dee\uff08RMSE\uff09\uff0c\u96c6\u6210\u5b66\u4e60\u8fdb\u4e00\u6b65\u5c06PCC\u63d0\u9ad8\u52300.945\uff0cRMSE\u964d\u4f4e\u523015.57\uff0c\u6027\u80fd\u4e0e\u7ecf\u9a8c\u4e30\u5bcc\u7684\u653e\u5c04\u79d1\u533b\u751f\u76f8\u5f53\uff08PCC = 0.97, RMSE = 18.75\uff09\u3002\u8be5\u7ba1\u7ebf\u80fd\u591f\u8bc6\u522b\u89e3\u5256\u7ed3\u6784\u5e76\u57fa\u4e8e\u6b64\u505a\u51fa\u51b3\u7b56\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u53cc\u624b\u6cd5\u7247SvdH\u8bc4\u5206\u9884\u6d4b\u7684\u53ef\u89e3\u91ca\u7684\u56fe\u50cf\u7ea7\u4e24\u9636\u6bb5\u7ba1\u7ebf\uff0c\u7ed3\u5408\u4e86\u56fe\u50cf\u533a\u57df\u63d0\u53d6\u548c\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684\u591a\u5b9e\u4f8b\u5b66\u4e60\u3002"}}
{"id": "2508.06224", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06224", "abs": "https://arxiv.org/abs/2508.06224", "authors": ["Guoyu Zhou", "Jing Zhang", "Yi Yan", "Hui Zhang", "Li Zhuo"], "title": "TEFormer: Texture-Aware and Edge-Guided Transformer for Semantic Segmentation of Urban Remote Sensing Images", "comment": "Submitted to GRSL", "summary": "Semantic segmentation of urban remote sensing images (URSIs) is crucial for\napplications such as urban planning and environmental monitoring. However,\ngeospatial objects often exhibit subtle texture differences and similar spatial\nstructures, which can easily lead to semantic ambiguity and misclassification.\nMoreover, challenges such as irregular object shapes, blurred boundaries, and\noverlapping spatial distributions of semantic objects contribute to complex and\ndiverse edge morphologies, further complicating accurate segmentation. To\ntackle these issues, we propose a texture-aware and edge-guided Transformer\n(TEFormer) that integrates texture awareness and edge-guidance mechanisms for\nsemantic segmentation of URSIs. In the encoder, a texture-aware module (TaM) is\ndesigned to capture fine-grained texture differences between visually similar\ncategories to enhance semantic discrimination. Then, an edge-guided tri-branch\ndecoder (Eg3Head) is constructed to preserve local edges and details for\nmultiscale context-awareness. Finally, an edge-guided feature fusion module\n(EgFFM) is to fuse contextual and detail information with edge information to\nrealize refined semantic segmentation. Extensive experiments show that TEFormer\nachieves mIoU of 88.57%, 81.46%, and 53.55% on the Potsdam, Vaihingen, and\nLoveDA datasets, respectively, shows the effectiveness in URSI semantic\nsegmentation.", "AI": {"tldr": "TEFormer\u901a\u8fc7\u7eb9\u7406\u611f\u77e5\u548c\u8fb9\u7f18\u5f15\u5bfc\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u57ce\u5e02\u9065\u611f\u56fe\u50cf\u8bed\u4e49\u5206\u5272\u4e2d\u7684\u6a21\u7cca\u6027\u548c\u8fb9\u7f18\u590d\u6742\u6027\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u5206\u5272\u7ed3\u679c\u3002", "motivation": "\u57ce\u5e02\u9065\u611f\u56fe\u50cf\uff08URSIs\uff09\u7684\u8bed\u4e49\u5206\u5272\u5728\u57ce\u5e02\u89c4\u5212\u548c\u73af\u5883\u76d1\u6d4b\u7b49\u5e94\u7528\u4e2d\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u5730\u7406\u7a7a\u95f4\u5bf9\u8c61\u5e38\u5e38\u5b58\u5728\u7ec6\u5fae\u7684\u7eb9\u7406\u5dee\u5f02\u548c\u76f8\u4f3c\u7684\u7a7a\u95f4\u7ed3\u6784\uff0c\u5bb9\u6613\u5bfc\u81f4\u8bed\u4e49\u6a21\u7cca\u548c\u5206\u7c7b\u9519\u8bef\u3002\u6b64\u5916\uff0c\u4e0d\u89c4\u5219\u7684\u5bf9\u8c61\u5f62\u72b6\u3001\u6a21\u7cca\u7684\u8fb9\u754c\u4ee5\u53ca\u8bed\u4e49\u5bf9\u8c61\u7684\u91cd\u53e0\u5206\u5e03\uff0c\u4f7f\u5f97\u8fb9\u7f18\u5f62\u6001\u590d\u6742\u591a\u6837\uff0c\u8fdb\u4e00\u6b65\u589e\u52a0\u4e86\u51c6\u786e\u5206\u5272\u7684\u96be\u5ea6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTEFormer\uff08texture-aware and edge-guided Transformer\uff09\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u6574\u5408\u4e86\u7eb9\u7406\u611f\u77e5\u548c\u8fb9\u7f18\u5f15\u5bfc\u673a\u5236\u3002\u5177\u4f53\u5305\u62ec\uff1a1. \u5728\u7f16\u7801\u5668\u4e2d\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7eb9\u7406\u611f\u77e5\u6a21\u5757\uff08TaM\uff09\uff0c\u7528\u4e8e\u6355\u6349\u89c6\u89c9\u4e0a\u76f8\u4f3c\u7c7b\u522b\u4e4b\u95f4\u7ec6\u5fae\u7684\u7eb9\u7406\u5dee\u5f02\uff0c\u589e\u5f3a\u8bed\u4e49\u533a\u5206\u5ea6\u30022. \u6784\u5efa\u4e86\u4e00\u4e2a\u8fb9\u7f18\u5f15\u5bfc\u7684\u4e09\u5206\u652f\u89e3\u7801\u5668\uff08Eg3Head\uff09\uff0c\u7528\u4e8e\u4fdd\u7559\u5c40\u90e8\u8fb9\u7f18\u548c\u7ec6\u8282\uff0c\u5b9e\u73b0\u591a\u5c3a\u5ea6\u4e0a\u4e0b\u6587\u611f\u77e5\u30023. \u8bbe\u8ba1\u4e86\u4e00\u4e2a\u8fb9\u7f18\u5f15\u5bfc\u7684\u7279\u5f81\u878d\u5408\u6a21\u5757\uff08EgFFM\uff09\uff0c\u7528\u4e8e\u878d\u5408\u4e0a\u4e0b\u6587\u3001\u7ec6\u8282\u548c\u8fb9\u7f18\u4fe1\u606f\uff0c\u5b9e\u73b0\u7cbe\u7ec6\u5316\u7684\u8bed\u4e49\u5206\u5272\u3002", "result": "TEFormer\u5728Potsdam\u3001Vaihingen\u548cLoveDA\u6570\u636e\u96c6\u4e0a\u5206\u522b\u5b9e\u73b0\u4e8688.57%\u300181.46%\u548c53.55%\u7684mIoU\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728URSI\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "TEFormer\u5728Potsdam\u3001Vaihingen\u548cLoveDA\u6570\u636e\u96c6\u4e0a\u5206\u522b\u5b9e\u73b0\u4e8688.57%\u300181.46%\u548c53.55%\u7684mIoU\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u57ce\u5e02\u9065\u611f\u56fe\u50cf\u8bed\u4e49\u5206\u5272\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.06485", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06485", "abs": "https://arxiv.org/abs/2508.06485", "authors": ["Sofiane Bouaziz", "Adel Hafiane", "Raphael Canals", "Rachid Nedjai"], "title": "WGAST: Weakly-Supervised Generative Network for Daily 10 m Land Surface Temperature Estimation via Spatio-Temporal Fusion", "comment": "Submitted to IEEE Transactions on Geoscience and Remote Sensing\n  (TGRS)", "summary": "Urbanization, climate change, and agricultural stress are increasing the\ndemand for precise and timely environmental monitoring. Land Surface\nTemperature (LST) is a key variable in this context and is retrieved from\nremote sensing satellites. However, these systems face a trade-off between\nspatial and temporal resolution. While spatio-temporal fusion methods offer\npromising solutions, few have addressed the estimation of daily LST at 10 m\nresolution. In this study, we present WGAST, a Weakly-Supervised Generative\nNetwork for Daily 10 m LST Estimation via Spatio-Temporal Fusion of Terra\nMODIS, Landsat 8, and Sentinel-2. WGAST is the first end-to-end deep learning\nframework designed for this task. It adopts a conditional generative\nadversarial architecture, with a generator composed of four stages: feature\nextraction, fusion, LST reconstruction, and noise suppression. The first stage\nemploys a set of encoders to extract multi-level latent representations from\nthe inputs, which are then fused in the second stage using cosine similarity,\nnormalization, and temporal attention mechanisms. The third stage decodes the\nfused features into high-resolution LST, followed by a Gaussian filter to\nsuppress high-frequency noise. Training follows a weakly supervised strategy\nbased on physical averaging principles and reinforced by a PatchGAN\ndiscriminator. Experiments demonstrate that WGAST outperforms existing methods\nin both quantitative and qualitative evaluations. Compared to the\nbest-performing baseline, on average, WGAST reduces RMSE by 17.18% and improves\nSSIM by 11.00%. Furthermore, WGAST is robust to cloud-induced LST and\neffectively captures fine-scale thermal patterns, as validated against 33\nground-based sensors. The code is available at\nhttps://github.com/Sofianebouaziz1/WGAST.git.", "AI": {"tldr": "WGAST\u662f\u4e00\u4e2a\u521b\u65b0\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u878d\u5408Terra MODIS\u3001Landsat 8\u548cSentinel-2\u6570\u636e\uff0c\u80fd\u591f\u4ee510\u7c73\u5206\u8fa8\u7387\u7cbe\u786e\u4f30\u7b97\u6bcf\u65e5\u5730\u8868\u6e29\u5ea6\uff08LST\uff09\u3002\u8be5\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u663e\u8457\u964d\u4f4e\u4e86RMSE\u5e76\u63d0\u9ad8\u4e86SSIM\uff0c\u540c\u65f6\u8fd8\u80fd\u6709\u6548\u5904\u7406\u4e91\u5c42\u5f71\u54cd\u5e76\u6355\u6349\u7cbe\u7ec6\u7684\u70ed\u91cf\u6a21\u5f0f\u3002", "motivation": "\u57ce\u5e02\u5316\u3001\u6c14\u5019\u53d8\u5316\u548c\u519c\u4e1a\u538b\u529b\u589e\u52a0\u4e86\u5bf9\u7cbe\u786e\u3001\u53ca\u65f6\u7684\u73af\u5883\u76d1\u6d4b\u7684\u9700\u6c42\u3002\u5730\u8868\u6e29\u5ea6\uff08LST\uff09\u662f\u8fd9\u4e00\u80cc\u666f\u4e0b\u7684\u4e00\u4e2a\u5173\u952e\u53d8\u91cf\uff0c\u5e76\u4e14\u662f\u4ece\u9065\u611f\u536b\u661f\u4e2d\u68c0\u7d22\u51fa\u6765\u7684\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u7cfb\u7edf\u5728\u7a7a\u95f4\u548c\u65f6\u95f4\u5206\u8fa8\u7387\u4e4b\u95f4\u5b58\u5728\u6743\u8861\u3002\u867d\u7136\u65f6\u7a7a\u878d\u5408\u65b9\u6cd5\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u5f88\u5c11\u6709\u65b9\u6cd5\u80fd\u591f\u89e3\u51b3\u4ee510\u7c73\u5206\u8fa8\u7387\u4f30\u7b97\u6bcf\u65e5LST\u7684\u95ee\u9898\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aWGAST\u7684\u5f31\u76d1\u7763\u751f\u6210\u7f51\u7edc\uff0c\u7528\u4e8e\u901a\u8fc7Terra MODIS\u3001Landsat 8\u548cSentinel-2\u7684\u65f6\u7a7a\u878d\u5408\u6765\u4f30\u8ba1\u6bcf\u65e510\u7c73LST\u3002WGAST\u662f\u7b2c\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u91c7\u7528\u6761\u4ef6\u751f\u6210\u5bf9\u6297\u67b6\u6784\uff0c\u5176\u751f\u6210\u5668\u5305\u62ec\u56db\u4e2a\u9636\u6bb5\uff1a\u7279\u5f81\u63d0\u53d6\u3001\u878d\u5408\u3001LST\u91cd\u5efa\u548c\u566a\u58f0\u6291\u5236\u3002\u8be5\u7f51\u7edc\u91c7\u7528\u57fa\u4e8e\u7269\u7406\u5e73\u5747\u539f\u7406\u7684\u5f31\u76d1\u7763\u7b56\u7565\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u901a\u8fc7PatchGAN\u5224\u522b\u5668\u8fdb\u884c\u589e\u5f3a\u3002", "result": "WGAST\u662f\u7b2c\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u901a\u8fc7Terra MODIS\u3001Landsat 8\u548cSentinel-2\u7684\u65f6\u7a7a\u878d\u5408\u6765\u4f30\u8ba1\u6bcf\u65e510\u7c73LST\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0cWGAST\u5728\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u4f30\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e73\u5747RMSE\u964d\u4f4e\u4e8617.18%\uff0cSSIM\u63d0\u9ad8\u4e8611.00%\u3002\u6b64\u5916\uff0cWGAST\u5bf9\u4e91\u5f15\u8d77\u7684LST\u5177\u6709\u9c81\u68d2\u6027\uff0c\u5e76\u80fd\u6709\u6548\u6355\u83b7\u7cbe\u7ec6\u5c3a\u5ea6\u7684\u70ed\u6a21\u5f0f\uff0c\u5df2\u901a\u8fc733\u4e2a\u5730\u9762\u4f20\u611f\u5668\u5f97\u5230\u9a8c\u8bc1\u3002", "conclusion": "WGAST\u5728\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u4f30\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e0e\u6027\u80fd\u6700\u4f73\u7684\u57fa\u7ebf\u76f8\u6bd4\uff0c\u5e73\u5747RMSE\u964d\u4f4e\u4e8617.18%\uff0cSSIM\u63d0\u9ad8\u4e8611.00%\u3002\u6b64\u5916\uff0cWGAST\u5bf9\u4e91\u5f15\u8d77\u7684LST\u5177\u6709\u9c81\u68d2\u6027\uff0c\u5e76\u80fd\u6709\u6548\u6355\u83b7\u7cbe\u7ec6\u5c3a\u5ea6\u7684\u70ed\u6a21\u5f0f\uff0c\u5df2\u901a\u8fc733\u4e2a\u5730\u9762\u4f20\u611f\u5668\u5f97\u5230\u9a8c\u8bc1\u3002"}}
{"id": "2508.06228", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06228", "abs": "https://arxiv.org/abs/2508.06228", "authors": ["Daniel Feijoo", "Paula Garrido-Mellado", "Jaesung Rim", "Alvaro Garcia", "Marcos V. Conde"], "title": "Towards Unified Image Deblurring using a Mixture-of-Experts Decoder", "comment": "Preprint. Under review", "summary": "Image deblurring, removing blurring artifacts from images, is a fundamental\ntask in computational photography and low-level computer vision. Existing\napproaches focus on specialized solutions tailored to particular blur types,\nthus, these solutions lack generalization. This limitation in current methods\nimplies requiring multiple models to cover several blur types, which is not\npractical in many real scenarios. In this paper, we introduce the first\nall-in-one deblurring method capable of efficiently restoring images affected\nby diverse blur degradations, including global motion, local motion, blur in\nlow-light conditions, and defocus blur. We propose a mixture-of-experts (MoE)\ndecoding module, which dynamically routes image features based on the\nrecognized blur degradation, enabling precise and efficient restoration in an\nend-to-end manner. Our unified approach not only achieves performance\ncomparable to dedicated task-specific models, but also demonstrates remarkable\nrobustness and generalization capabilities on unseen blur degradation\nscenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u56fe\u50cf\u53bb\u6a21\u7cca\u7684\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u65b9\u6cd5\uff0c\u53ef\u5904\u7406\u591a\u79cd\u6a21\u7cca\u7c7b\u578b\uff0c\u6027\u80fd\u4f18\u8d8a\u4e14\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u56fe\u50cf\u53bb\u6a21\u7cca\u65b9\u6cd5\u901a\u5e38\u9488\u5bf9\u7279\u5b9a\u7684\u6a21\u7cca\u7c7b\u578b\uff0c\u7f3a\u4e4f\u6cdb\u5316\u80fd\u529b\uff0c\u8fd9\u5728\u8bb8\u591a\u5b9e\u9645\u573a\u666f\u4e2d\u5e76\u4e0d\u5b9e\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6240\u6709\u529f\u80fd\u4e8e\u4e00\u8eab\u7684\u56fe\u50cf\u53bb\u6a21\u7cca\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u79cd\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u89e3\u7801\u6a21\u5757\uff0c\u8be5\u6a21\u5757\u6839\u636e\u8bc6\u522b\u51fa\u7684\u6a21\u7cca\u9000\u5316\u52a8\u6001\u5730\u8def\u7531\u56fe\u50cf\u7279\u5f81\uff0c\u4ece\u800c\u80fd\u591f\u4ee5\u7aef\u5230\u7aef\u7684\u65b9\u5f0f\u8fdb\u884c\u7cbe\u786e\u9ad8\u6548\u7684\u6062\u590d\u3002", "result": "\u8be5\u65b9\u6cd5\u662f\u7b2c\u4e00\u4e2a\u80fd\u591f\u6709\u6548\u6062\u590d\u53d7\u5404\u79cd\u6a21\u7cca\u9000\u5316\u5f71\u54cd\u7684\u56fe\u50cf\u7684\u6240\u6709\u529f\u80fd\u4e8e\u4e00\u8eab\u7684\u53bb\u6a21\u7cca\u65b9\u6cd5\uff0c\u5728\u5e38\u89c1\u7684\u548c\u672a\u77e5\u7684\u6a21\u7cca\u9000\u5316\u573a\u666f\u4e2d\u5747\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5404\u79cd\u6a21\u7cca\u9000\u5316\uff08\u5305\u62ec\u5168\u5c40\u8fd0\u52a8\u3001\u5c40\u90e8\u8fd0\u52a8\u3001\u5f31\u5149\u6a21\u7cca\u548c\u5931\u7126\u6a21\u7cca\uff09\u4e0b\u5b9e\u73b0\u4e86\u4e0e\u4e13\u7528\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u5e76\u5bf9\u672a\u77e5\u7684\u6a21\u7cca\u9000\u5316\u573a\u666f\u8868\u73b0\u51fa\u51fa\u8272\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.06248", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06248", "abs": "https://arxiv.org/abs/2508.06248", "authors": ["Andrii Yermakov", "Jan Cech", "Jiri Matas", "Mario Fritz"], "title": "Deepfake Detection that Generalizes Across Benchmarks", "comment": null, "summary": "The generalization of deepfake detectors to unseen manipulation techniques\nremains a challenge for practical deployment. Although many approaches adapt\nfoundation models by introducing significant architectural complexity, this\nwork demonstrates that robust generalization is achievable through a\nparameter-efficient adaptation of a pre-trained CLIP vision encoder. The\nproposed method, LNCLIP-DF, fine-tunes only the Layer Normalization parameters\n(0.03% of the total) and enhances generalization by enforcing a hyperspherical\nfeature manifold using L2 normalization and latent space augmentations.\n  We conducted an extensive evaluation on 13 benchmark datasets spanning from\n2019 to 2025. The proposed method achieves state-of-the-art performance,\noutperforming more complex, recent approaches in average cross-dataset AUROC.\nOur analysis yields two primary findings for the field: 1) training on paired\nreal-fake data from the same source video is essential for mitigating shortcut\nlearning and improving generalization, and 2) detection difficulty on academic\ndatasets has not strictly increased over time, with models trained on older,\ndiverse datasets showing strong generalization capabilities.\n  This work delivers a computationally efficient and reproducible method,\nproving that state-of-the-art generalization is attainable by making targeted,\nminimal changes to a pre-trained CLIP model. The code will be made publicly\navailable upon acceptance.", "AI": {"tldr": "LNCLIP-DF\u901a\u8fc7\u5fae\u8c03CLIP\u6a21\u578b\u7684Layer Normalization\u53c2\u6570\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u6cdb\u5316\u80fd\u529b\u5f3a\u7684\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\uff0c\u4f18\u4e8e\u73b0\u6709\u590d\u6742\u65b9\u6cd5\u3002", "motivation": "\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u6a21\u578b\u6cdb\u5316\u5230\u672a\u89c1\u64cd\u7eb5\u6280\u672f\u7684\u6311\u6218\uff0c\u4ee5\u53ca\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5f15\u5165\u663e\u8457\u67b6\u6784\u590d\u6742\u6027\u7684\u4e0d\u8db3\u3002", "method": "LNCLIP-DF\u5fae\u8c03\u9884\u8bad\u7ec3CLIP\u6a21\u578b\u7684Layer Normalization\u53c2\u6570\uff08\u4ec5\u5360\u603b\u53c2\u6570\u76840.03%\uff09\uff0c\u5e76\u91c7\u7528L2\u5f52\u4e00\u5316\u548c\u6f5c\u5728\u7a7a\u95f4\u589e\u5f3a\u6765\u5f3a\u5236\u6267\u884c\u8d85\u7403\u7279\u5f81\u6d41\u5f62\u3002", "result": "LNCLIP-DF\u572813\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e73\u5747\u8de8\u6570\u636e\u96c6AUROC\u4f18\u4e8e\u66f4\u590d\u6742\u3001\u8fd1\u671f\u7684\u5176\u4ed6\u65b9\u6cd5\u3002\u7814\u7a76\u8fd8\u53d1\u73b0\uff0c\u5728\u540c\u4e00\u6e90\u89c6\u9891\u7684\u914d\u5bf9\u771f\u5b9e/\u4f2a\u9020\u6570\u636e\u4e0a\u8fdb\u884c\u8bad\u7ec3\u5bf9\u4e8e\u7f13\u89e3\u6377\u5f84\u5b66\u4e60\u548c\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\u81f3\u5173\u91cd\u8981\uff1b\u5e76\u4e14\u68c0\u6d4b\u96be\u5ea6\u5e76\u975e\u968f\u65f6\u95f4\u4e25\u683c\u589e\u52a0\uff0c\u5728\u65e9\u671f\u3001\u591a\u6837\u5316\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLNCLIP-DF\u7684\u53c2\u6570\u9ad8\u6548\u65b9\u6cd5\uff0c\u901a\u8fc7\u5fae\u8c03\u9884\u8bad\u7ec3CLIP\u6a21\u578b\u7684Layer Normalization\u53c2\u6570\uff0c\u5e76\u7ed3\u5408L2\u5f52\u4e00\u5316\u548c\u6f5c\u5728\u7a7a\u95f4\u589e\u5f3a\u6765\u5f3a\u5236\u6267\u884c\u8d85\u7403\u7279\u5f81\u6d41\u5f62\uff0c\u4ece\u800c\u5728\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u4e2d\u5b9e\u73b0\u4e86\u5bf9\u672a\u89c1\u64cd\u7eb5\u6280\u672f\u7684\u9c81\u68d2\u6cdb\u5316\u3002\u8be5\u65b9\u6cd5\u572813\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e73\u5747\u8de8\u6570\u636e\u96c6AUROC\u4f18\u4e8e\u66f4\u590d\u6742\u7684\u65b9\u6cd5\u3002"}}
{"id": "2508.06256", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06256", "abs": "https://arxiv.org/abs/2508.06256", "authors": ["Bar\u0131\u015f B\u00fcy\u00fckta\u015f", "Jonas Klotz", "Beg\u00fcm Demir"], "title": "FedX: Explanation-Guided Pruning for Communication-Efficient Federated Learning in Remote Sensing", "comment": null, "summary": "Federated learning (FL) enables the collaborative training of deep neural\nnetworks across decentralized data archives (i.e., clients), where each client\nstores data locally and only shares model updates with a central server. This\nmakes FL a suitable learning paradigm for remote sensing (RS) image\nclassification tasks, where data centralization may be restricted due to legal\nand privacy constraints. However, a key challenge in applying FL to RS tasks is\nthe communication overhead caused by the frequent exchange of large model\nupdates between clients and the central server. To address this issue, in this\npaper we propose a novel strategy (denoted as FedX) that uses\nexplanation-guided pruning to reduce communication overhead by minimizing the\nsize of the transmitted models without compromising performance. FedX leverages\nbackpropagation-based explanation methods to estimate the task-specific\nimportance of model components and prunes the least relevant ones at the\ncentral server. The resulting sparse global model is then sent to clients,\nsubstantially reducing communication overhead. We evaluate FedX on multi-label\nscene classification using the BigEarthNet-S2 dataset and single-label scene\nclassification using the EuroSAT dataset. Experimental results show the success\nof FedX in significantly reducing the number of shared model parameters while\nenhancing the generalization capability of the global model, compared to both\nunpruned model and state-of-the-art pruning methods. The code of FedX will be\navailable at https://git.tu-berlin.de/rsim/FedX.", "AI": {"tldr": "FedX\u901a\u8fc7\u57fa\u4e8e\u89e3\u91ca\u7684\u526a\u679d\u6765\u51cf\u5c11\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u901a\u4fe1\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u751a\u81f3\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u867d\u7136\u9002\u7528\u4e8e\u504f\u8fdc\u5730\u533a\u7684\u6570\u636e\u5206\u7c7b\u4efb\u52a1\uff0c\u4f46\u6a21\u578b\u66f4\u65b0\u7684\u9891\u7e41\u4ea4\u6362\u4f1a\u5bfc\u81f4\u901a\u4fe1\u5f00\u9500\u8fc7\u5927\u3002", "method": "FedX\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u57fa\u4e8e\u53cd\u5411\u4f20\u64ad\u7684\u89e3\u91ca\u65b9\u6cd5\u6765\u4f30\u8ba1\u6a21\u578b\u7ec4\u4ef6\u4efb\u52a1\u76f8\u5173\u6027\u7684\u65b0\u7b56\u7565\uff0c\u5e76\u88c1\u526a\u6389\u76f8\u5173\u6027\u6700\u4f4e\u7684\u7ec4\u4ef6\uff0c\u4ece\u800c\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\u5e76\u6700\u5c0f\u5316\u4f20\u8f93\u6a21\u578b\u7684\u5927\u5c0f\u3002", "result": "FedX\u5728\u591a\u6807\u7b7e\u573a\u666f\u5206\u7c7b\uff08BigEarthNet-S2\u6570\u636e\u96c6\uff09\u548c\u5355\u6807\u7b7e\u573a\u666f\u5206\u7c7b\uff08EuroSAT\u6570\u636e\u96c6\uff09\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eFedX\u6210\u529f\u5730\u51cf\u5c11\u4e86\u5171\u4eab\u6a21\u578b\u53c2\u6570\u7684\u6570\u91cf\uff0c\u5e76\u63d0\u9ad8\u4e86\u5168\u5c40\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "FedX\u80fd\u591f\u663e\u8457\u51cf\u5c11\u5171\u4eab\u6a21\u578b\u53c2\u6570\u6570\u91cf\uff0c\u5e76\u63d0\u9ad8\u5168\u5c40\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f18\u4e8e\u672a\u526a\u679d\u6a21\u578b\u548c\u6700\u5148\u8fdb\u7684\u526a\u679d\u65b9\u6cd5\u3002"}}
{"id": "2508.06259", "categories": ["cs.CV", "cs.AI", "I.2.10"], "pdf": "https://arxiv.org/pdf/2508.06259", "abs": "https://arxiv.org/abs/2508.06259", "authors": ["Zhangquan Chen", "Ruihui Zhao", "Chuwei Luo", "Mingze Sun", "Xinlei Yu", "Yangyang Kang", "Ruqi Huang"], "title": "SIFThinker: Spatially-Aware Image Focus for Visual Reasoning", "comment": "15 pages, 13 figures", "summary": "Current multimodal large language models (MLLMs) still face significant\nchallenges in complex visual tasks (e.g., spatial understanding, fine-grained\nperception). Prior methods have tried to incorporate visual reasoning, however,\nthey fail to leverage attention correction with spatial cues to iteratively\nrefine their focus on prompt-relevant regions. In this paper, we introduce\nSIFThinker, a spatially-aware \"think-with-images\" framework that mimics human\nvisual perception. Specifically, SIFThinker enables attention correcting and\nimage region focusing by interleaving depth-enhanced bounding boxes and natural\nlanguage. Our contributions are twofold: First, we introduce a\nreverse-expansion-forward-inference strategy that facilitates the generation of\ninterleaved image-text chains of thought for process-level supervision, which\nin turn leads to the construction of the SIF-50K dataset. Besides, we propose\nGRPO-SIF, a reinforced training paradigm that integrates depth-informed visual\ngrounding into a unified reasoning pipeline, teaching the model to dynamically\ncorrect and focus on prompt-relevant regions. Extensive experiments demonstrate\nthat SIFThinker outperforms state-of-the-art methods in spatial understanding\nand fine-grained visual perception, while maintaining strong general\ncapabilities, highlighting the effectiveness of our method.", "AI": {"tldr": "SIFThinker\u901a\u8fc7\u7ed3\u5408\u6df1\u5ea6\u589e\u5f3a\u8fb9\u754c\u6846\u548c\u81ea\u7136\u8bed\u8a00\u6765\u6539\u8fdb\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7a7a\u95f4\u7406\u89e3\u548c\u7ec6\u7c92\u5ea6\u611f\u77e5\u65b9\u9762\u7684\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u7684\u89c6\u89c9\u4efb\u52a1\uff08\u4f8b\u5982\u7a7a\u95f4\u7406\u89e3\u3001\u7ec6\u7c92\u5ea6\u611f\u77e5\uff09\u65b9\u9762\u4ecd\u7136\u9762\u4e34\u91cd\u5927\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5229\u7528\u7a7a\u95f4\u7ebf\u7d22\u7684\u6ce8\u610f\u529b\u6821\u6b63\u6765\u8fed\u4ee3\u5730\u4f18\u5316\u5176\u5bf9\u63d0\u793a\u76f8\u5173\u533a\u57df\u7684\u5173\u6ce8\u3002", "method": "SIFThinker\u901a\u8fc7\u4ea4\u9519\u6df1\u5ea6\u589e\u5f3a\u7684\u8fb9\u754c\u6846\u548c\u81ea\u7136\u8bed\u8a00\u6765\u5b9e\u73b0\u6ce8\u610f\u529b\u6821\u6b63\u548c\u56fe\u50cf\u533a\u57df\u805a\u7126\u3002\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u53cd\u5411\u6269\u5c55\u524d\u5411\u63a8\u7406\u7b56\u7565\u6765\u751f\u6210\u4ea4\u9519\u7684\u56fe\u50cf-\u6587\u672c\u601d\u7ef4\u94fe\uff0c\u5e76\u63d0\u51fa\u4e86GRPO-SIF\uff0c\u4e00\u79cd\u5c06\u6df1\u5ea6\u4fe1\u606f\u89c6\u89c9\u57fa\u7840\u6574\u5408\u5230\u7edf\u4e00\u63a8\u7406\u7ba1\u9053\u4e2d\u7684\u5f3a\u5316\u8bad\u7ec3\u8303\u5f0f\u3002", "result": "SIFThinker\u5728\u7a7a\u95f4\u7406\u89e3\u548c\u7ec6\u7c92\u5ea6\u89c6\u89c9\u611f\u77e5\u65b9\u9762\u8868\u73b0\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "SIFThinker\u5728\u7a7a\u95f4\u7406\u89e3\u548c\u7ec6\u7c92\u5ea6\u89c6\u89c9\u611f\u77e5\u65b9\u9762\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5f3a\u5927\u7684\u901a\u7528\u80fd\u529b\uff0c\u7a81\u663e\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.06258", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06258", "abs": "https://arxiv.org/abs/2508.06258", "authors": ["Byunghyun Ko", "Anning Tian", "Jeongkyu Lee"], "title": "XAG-Net: A Cross-Slice Attention and Skip Gating Network for 2.5D Femur MRI Segmentation", "comment": "Accepted at the 2025 International Conference on Artificial\n  Intelligence, Computer, Data Sciences and Applications (ACDSA). This is the\n  preprint version of the paper", "summary": "Accurate segmentation of femur structures from Magnetic Resonance Imaging\n(MRI) is critical for orthopedic diagnosis and surgical planning but remains\nchallenging due to the limitations of existing 2D and 3D deep learning-based\nsegmentation approaches. In this study, we propose XAG-Net, a novel 2.5D\nU-Net-based architecture that incorporates pixel-wise cross-slice attention\n(CSA) and skip attention gating (AG) mechanisms to enhance inter-slice\ncontextual modeling and intra-slice feature refinement. Unlike previous\nCSA-based models, XAG-Net applies pixel-wise softmax attention across adjacent\nslices at each spatial location for fine-grained inter-slice modeling.\nExtensive evaluations demonstrate that XAG-Net surpasses baseline 2D, 2.5D, and\n3D U-Net models in femur segmentation accuracy while maintaining computational\nefficiency. Ablation studies further validate the critical role of the CSA and\nAG modules, establishing XAG-Net as a promising framework for efficient and\naccurate femur MRI segmentation.", "AI": {"tldr": "XAG-Net improves femur MRI segmentation using a 2.5D U-Net with cross-slice and skip attention mechanisms, achieving higher accuracy than existing methods.", "motivation": "Accurate segmentation of femur structures from MRI is critical for orthopedic diagnosis and surgical planning but is challenging for existing methods.", "method": "XAG-Net, a novel 2.5D U-Net-based architecture incorporating pixel-wise cross-slice attention (CSA) and skip attention gating (AG) mechanisms.", "result": "XAG-Net surpasses baseline 2D, 2.5D, and 3D U-Net models in femur segmentation accuracy. Ablation studies validate the critical role of CSA and AG modules.", "conclusion": "XAG-Net is a promising framework for efficient and accurate femur MRI segmentation, outperforming baseline models in accuracy while maintaining computational efficiency."}}
{"id": "2508.06317", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06317", "abs": "https://arxiv.org/abs/2508.06317", "authors": ["Jian Hu", "Zixu Cheng", "Shaogang Gong", "Isabel Guan", "Jianye Hao", "Jun Wang", "Kun Shao"], "title": "Uncertainty-quantified Rollout Policy Adaptation for Unlabelled Cross-domain Temporal Grounding", "comment": null, "summary": "Video Temporal Grounding (TG) aims to temporally locate video segments\nmatching a natural language description (a query) in a long video. While\nVision-Language Models (VLMs) are effective at holistic semantic matching, they\noften struggle with fine-grained temporal localisation. Recently, Group\nRelative Policy Optimisation (GRPO) reformulates the inference process as a\nreinforcement learning task, enabling fine-grained grounding and achieving\nstrong in-domain performance. However, GRPO relies on labelled data, making it\nunsuitable in unlabelled domains. Moreover, because videos are large and\nexpensive to store and process, performing full-scale adaptation introduces\nprohibitive latency and computational overhead, making it impractical for\nreal-time deployment. To overcome both problems, we introduce a Data-Efficient\nUnlabelled Cross-domain Temporal Grounding method, from which a model is first\ntrained on a labelled source domain, then adapted to a target domain using only\na small number of unlabelled videos from the target domain. This approach\neliminates the need for target annotation and keeps both computational and\nstorage overhead low enough to run in real time. Specifically, we introduce.\nUncertainty-quantified Rollout Policy Adaptation (URPA) for cross-domain\nknowledge transfer in learning video temporal grounding without target labels.\nURPA generates multiple candidate predictions using GRPO rollouts, averages\nthem to form a pseudo label, and estimates confidence from the variance across\nthese rollouts. This confidence then weights the training rewards, guiding the\nmodel to focus on reliable supervision. Experiments on three datasets across\nsix cross-domain settings show that URPA generalises well using only a few\nunlabelled target videos. Codes will be released once published.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aURPA\u7684\u6570\u636e\u9ad8\u6548\u65e0\u6807\u7b7e\u8de8\u57df\u89c6\u9891\u65f6\u5e8f\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u6709\u6807\u7b7e\u6570\u636e\u548c\u8ba1\u7b97\u5f00\u9500\u5927\u7684\u95ee\u9898\uff0c\u80fd\u591f\u5728\u4ec5\u4f7f\u7528\u5c11\u91cf\u65e0\u6807\u7b7e\u89c6\u9891\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u6709\u6548\u7684\u8de8\u57df\u8fc1\u79fb\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u9891\u65f6\u5e8f\u5b9a\u4f4d\uff08TG\uff09\u65b9\u6cd5\u867d\u7136\u5728\u8bed\u4e49\u5339\u914d\u4e0a\u6709\u6548\uff0c\u4f46\u5728\u7ec6\u7c92\u5ea6\u7684\u65f6\u95f4\u5b9a\u4f4d\u4e0a\u5b58\u5728\u4e0d\u8db3\u3002\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684GRPO\u65b9\u6cd5\u867d\u7136\u80fd\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u5b9a\u4f4d\uff0c\u4f46\u9700\u8981\u6709\u6807\u7b7e\u6570\u636e\uff0c\u4e14\u5728\u5927\u89c4\u6a21\u89c6\u9891\u4e0a\u8fdb\u884c\u5168\u57df\u81ea\u9002\u5e94\u4f1a\u5bfc\u81f4\u8fc7\u9ad8\u7684\u5ef6\u8fdf\u548c\u8ba1\u7b97\u5f00\u9500\uff0c\u4e0d\u9002\u7528\u4e8e\u5b9e\u65f6\u90e8\u7f72\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65e0\u9700\u76ee\u6807\u57df\u6807\u7b7e\u4e14\u8ba1\u7b97\u5f00\u9500\u4f4e\u7684\u6570\u636e\u9ad8\u6548\u7684\u8de8\u57df\u65f6\u5e8f\u5b9a\u4f4d\u65b9\u6cd5\u3002", "method": "\u9996\u5148\u5728\u6709\u6807\u7b7e\u7684\u6e90\u57df\u4e0a\u8bad\u7ec3\u6a21\u578b\uff0c\u7136\u540e\u4f7f\u7528\u76ee\u6807\u57df\u4e2d\u5c11\u91cf\u65e0\u6807\u7b7e\u89c6\u9891\u8fdb\u884c\u81ea\u9002\u5e94\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u5f15\u5165\u4e86\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u7684Rollout\u7b56\u7565\u81ea\u9002\u5e94\uff08URPA\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u591a\u4e2a\u5019\u9009\u9884\u6d4b\uff0c\u8ba1\u7b97\u5176\u65b9\u5dee\u5f97\u5230\u7f6e\u4fe1\u5ea6\uff0c\u5e76\u5229\u7528\u8be5\u7f6e\u4fe1\u5ea6\u5bf9\u8bad\u7ec3\u5956\u52b1\u8fdb\u884c\u52a0\u6743\uff0c\u5f15\u5bfc\u6a21\u578b\u5173\u6ce8\u53ef\u9760\u7684\u76d1\u7763\u3002", "result": "URPA\u65b9\u6cd5\u80fd\u591f\u5b9e\u73b0\u6570\u636e\u9ad8\u6548\u7684\u65e0\u6807\u7b7e\u8de8\u57df\u89c6\u9891\u65f6\u5e8f\u5b9a\u4f4d\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u76ee\u6807\u57df\u65e0\u6807\u7b7e\u6570\u636e\uff0c\u5e76\u4e14\u8ba1\u7b97\u548c\u5b58\u50a8\u5f00\u9500\u4f4e\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u90e8\u7f72\u3002", "conclusion": "URPA\u65b9\u6cd5\u5728\u4ec5\u4f7f\u7528\u5c11\u91cf\u76ee\u6807\u57df\u7684\u65e0\u6807\u7b7e\u89c6\u9891\u65f6\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u548c\u516d\u4e2a\u8de8\u57df\u8bbe\u7f6e\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002"}}
{"id": "2508.06318", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06318", "abs": "https://arxiv.org/abs/2508.06318", "authors": ["Giacomo D'Amicantonio", "Snehashis Majhi", "Quan Kong", "Lorenzo Garattoni", "Gianpiero Francesca", "Fran\u00e7ois Bremond", "Egor Bondarev"], "title": "Mixture of Experts Guided by Gaussian Splatters Matters: A new Approach to Weakly-Supervised Video Anomaly Detection", "comment": null, "summary": "Video Anomaly Detection (VAD) is a challenging task due to the variability of\nanomalous events and the limited availability of labeled data. Under the\nWeakly-Supervised VAD (WSVAD) paradigm, only video-level labels are provided\nduring training, while predictions are made at the frame level. Although\nstate-of-the-art models perform well on simple anomalies (e.g., explosions),\nthey struggle with complex real-world events (e.g., shoplifting). This\ndifficulty stems from two key issues: (1) the inability of current models to\naddress the diversity of anomaly types, as they process all categories with a\nshared model, overlooking category-specific features; and (2) the weak\nsupervision signal, which lacks precise temporal information, limiting the\nability to capture nuanced anomalous patterns blended with normal events. To\naddress these challenges, we propose Gaussian Splatting-guided Mixture of\nExperts (GS-MoE), a novel framework that employs a set of expert models, each\nspecialized in capturing specific anomaly types. These experts are guided by a\ntemporal Gaussian splatting loss, enabling the model to leverage temporal\nconsistency and enhance weak supervision. The Gaussian splatting approach\nencourages a more precise and comprehensive representation of anomalies by\nfocusing on temporal segments most likely to contain abnormal events. The\npredictions from these specialized experts are integrated through a\nmixture-of-experts mechanism to model complex relationships across diverse\nanomaly patterns. Our approach achieves state-of-the-art performance, with a\n91.58% AUC on the UCF-Crime dataset, and demonstrates superior results on\nXD-Violence and MSAD datasets. By leveraging category-specific expertise and\ntemporal guidance, GS-MoE sets a new benchmark for VAD under weak supervision.", "AI": {"tldr": "GS-MoE\u901a\u8fc7\u4e13\u5bb6\u6a21\u578b\u5904\u7406\u4e0d\u540c\u5f02\u5e38\u7c7b\u578b\u5e76\u5229\u7528\u65f6\u95f4\u9ad8\u65af\u6cfc\u6e85\u635f\u5931\u8fdb\u884c\u6307\u5bfc\uff0c\u89e3\u51b3\u4e86\u5f31\u76d1\u7763\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u591a\u6837\u6027\u548c\u65f6\u95f4\u4fe1\u606f\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\uff08VAD\uff09\u4efb\u52a1\u4e2d\u5f02\u5e38\u4e8b\u4ef6\u7684\u591a\u6837\u6027\u548c\u6807\u8bb0\u6570\u636e\u6709\u9650\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u73b0\u6709\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u73b0\u5b9e\u4e16\u754c\u4e8b\u4ef6\uff08\u5982\u5165\u5e97\u884c\u7a83\uff09\u65b9\u9762\u7684\u4e0d\u8db3\u3002\u73b0\u6709\u6a21\u578b\u65e0\u6cd5\u89e3\u51b3\u5f02\u5e38\u7c7b\u578b\u7684\u591a\u6837\u6027\uff0c\u5e76\u4e14\u5f31\u76d1\u7763\u4fe1\u53f7\u7f3a\u4e4f\u7cbe\u786e\u7684\u65f6\u95f4\u4fe1\u606f\uff0c\u9650\u5236\u4e86\u6355\u6349\u7ec6\u5fae\u5f02\u5e38\u6a21\u5f0f\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u9ad8\u65af\u6cfc\u6e85\u5f15\u5bfc\u6df7\u5408\u4e13\u5bb6\uff08GS-MoE\uff09\u7684\u65b0\u9896\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u91c7\u7528\u4e86\u4e00\u7ec4\u4e13\u5bb6\u6a21\u578b\uff0c\u6bcf\u4e2a\u6a21\u578b\u90fd\u4e13\u95e8\u7528\u4e8e\u6355\u6349\u7279\u5b9a\u7c7b\u578b\u7684\u5f02\u5e38\u3002\u8fd9\u4e9b\u4e13\u5bb6\u6a21\u578b\u901a\u8fc7\u65f6\u95f4\u9ad8\u65af\u6cfc\u6e85\u635f\u5931\u8fdb\u884c\u5f15\u5bfc\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u5229\u7528\u65f6\u95f4\u4e00\u81f4\u6027\u5e76\u52a0\u5f3a\u5f31\u76d1\u7763\u3002\u9ad8\u65af\u6cfc\u6e85\u65b9\u6cd5\u901a\u8fc7\u5173\u6ce8\u6700\u6709\u53ef\u80fd\u5305\u542b\u5f02\u5e38\u4e8b\u4ef6\u7684\u65f6\u95f4\u6bb5\uff0c\u9f13\u52b1\u66f4\u7cbe\u786e\u548c\u5168\u9762\u7684\u5f02\u5e38\u8868\u793a\u3002\u901a\u8fc7\u6df7\u5408\u4e13\u5bb6\u673a\u5236\u6574\u5408\u8fd9\u4e9b\u4e13\u4e1a\u4e13\u5bb6\u7684\u9884\u6d4b\uff0c\u4ee5\u6a21\u62df\u8de8\u4e0d\u540c\u5f02\u5e38\u6a21\u5f0f\u7684\u590d\u6742\u5173\u7cfb\u3002", "result": "GS-MoE \u6846\u67b6\u5728 UCF-Crime \u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86 91.58% \u7684 AUC\uff0c\u5e76\u5728 XD-Violence \u548c MSAD \u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u8d8a\u7684\u7ed3\u679c\uff0c\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "GS-MoE \u6846\u67b6\u901a\u8fc7\u5229\u7528\u7279\u5b9a\u7c7b\u522b\u7684\u4e13\u4e1a\u77e5\u8bc6\u548c\u65f6\u95f4\u6307\u5bfc\uff0c\u4e3a\u5f31\u76d1\u7763\u4e0b\u7684\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u6811\u7acb\u4e86\u65b0\u7684\u57fa\u51c6\uff0c\u5728 UCF-Crime \u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86 91.58% \u7684 AUC\uff0c\u5e76\u5728 XD-Violence \u548c MSAD \u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u8d8a\u7684\u7ed3\u679c\u3002"}}
{"id": "2508.06327", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06327", "abs": "https://arxiv.org/abs/2508.06327", "authors": ["Xin Ci Wong", "Duygu Sarikaya", "Kieran Zucker", "Marc De Kamps", "Nishant Ravikumar"], "title": "Can Diffusion Models Bridge the Domain Gap in Cardiac MR Imaging?", "comment": "ICONIP 2025", "summary": "Magnetic resonance (MR) imaging, including cardiac MR, is prone to domain\nshift due to variations in imaging devices and acquisition protocols. This\nchallenge limits the deployment of trained AI models in real-world scenarios,\nwhere performance degrades on unseen domains. Traditional solutions involve\nincreasing the size of the dataset through ad-hoc image augmentation or\nadditional online training/transfer learning, which have several limitations.\nSynthetic data offers a promising alternative, but anatomical/structural\nconsistency constraints limit the effectiveness of generative models in\ncreating image-label pairs. To address this, we propose a diffusion model (DM)\ntrained on a source domain that generates synthetic cardiac MR images that\nresemble a given reference. The synthetic data maintains spatial and structural\nfidelity, ensuring similarity to the source domain and compatibility with the\nsegmentation mask. We assess the utility of our generative approach in\nmulti-centre cardiac MR segmentation, using the 2D nnU-Net, 3D nnU-Net and\nvanilla U-Net segmentation networks. We explore domain generalisation, where,\ndomain-invariant segmentation models are trained on synthetic source domain\ndata, and domain adaptation, where, we shift target domain data towards the\nsource domain using the DM. Both strategies significantly improved segmentation\nperformance on data from an unseen target domain, in terms of surface-based\nmetrics (Welch's t-test, p < 0.01), compared to training segmentation models on\nreal data alone. The proposed method ameliorates the need for transfer learning\nor online training to address domain shift challenges in cardiac MR image\nanalysis, especially useful in data-scarce settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u5fc3\u810fMR\u56fe\u50cf\u7684\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u57df\u8f6c\u79fb\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2d\u5fc3\u5fc3\u810fMR\u5206\u5272\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u78c1\u5171\u632f\uff08MR\uff09\u6210\u50cf\uff0c\u5305\u62ec\u5fc3\u810fMR\uff0c\u7531\u4e8e\u6210\u50cf\u8bbe\u5907\u548c\u91c7\u96c6\u65b9\u6848\u7684\u53d8\u5316\uff0c\u5bb9\u6613\u51fa\u73b0\u57df\u8f6c\u79fb\u3002\u8fd9\u79cd\u6311\u6218\u9650\u5236\u4e86\u6240\u8bad\u7ec3\u7684AI\u6a21\u578b\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u90e8\u7f72\uff0c\u56e0\u4e3a\u5728\u672a\u89c1\u8fc7\u7684\u57df\u4e0a\u7684\u6027\u80fd\u4f1a\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6269\u6563\u6a21\u578b\uff08DM\uff09\uff0c\u8be5\u6a21\u578b\u5728\u6e90\u57df\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u53ef\u4ee5\u751f\u6210\u7c7b\u4f3c\u4e8e\u7ed9\u5b9a\u53c2\u8003\u7684\u5fc3\u810fMR\u56fe\u50cf\u3002\u8be5\u5408\u6210\u6570\u636e\u4fdd\u6301\u4e86\u7a7a\u95f4\u548c\u7ed3\u6784\u4fdd\u771f\u5ea6\uff0c\u786e\u4fdd\u4e0e\u6e90\u57df\u7684\u76f8\u4f3c\u6027\u4ee5\u53ca\u4e0e\u5206\u5272\u63a9\u7801\u7684\u517c\u5bb9\u6027\u3002", "result": "\u4e0e\u4ec5\u5728\u771f\u5b9e\u6570\u636e\u4e0a\u8bad\u7ec3\u5206\u5272\u6a21\u578b\u76f8\u6bd4\uff0c\u4e24\u79cd\u7b56\u7565\uff08\u57df\u6cdb\u5316\u548c\u57df\u81ea\u9002\u5e94\uff09\u5728\u672a\u89c1\u8fc7\u7684\u76ee\u6807\u57df\u4e0a\u7684\u5206\u5272\u6027\u80fd\uff08\u6839\u636e\u57fa\u4e8e\u8868\u9762\u7684\u5ea6\u91cf\uff0cWelch's t\u68c0\u9a8c\uff0cp < 0.01\uff09\u5f97\u5230\u4e86\u663e\u8457\u6539\u5584\u3002", "conclusion": "\u8be5\u751f\u6210\u65b9\u6cd5\u6539\u5584\u4e86\u5bf9\u5fc3\u810fMR\u56fe\u50cf\u5206\u6790\u4e2d\u57df\u8f6c\u79fb\u95ee\u9898\u7684\u5904\u7406\uff0c\u5728\u6570\u636e\u7a00\u758f\u7684\u60c5\u51b5\u4e0b\u5c24\u5176\u6709\u7528\uff0c\u5e76\u80fd\u6539\u5584\u9700\u8981\u8f6c\u79fb\u5b66\u4e60\u6216\u5728\u7ebf\u8bad\u7ec3\u4ee5\u89e3\u51b3\u57df\u8f6c\u79fb\u95ee\u9898\u7684\u9700\u6c42\u3002"}}
{"id": "2508.06335", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06335", "abs": "https://arxiv.org/abs/2508.06335", "authors": ["Patrick Takenaka", "Johannes Maucher", "Marco F. Huber"], "title": "ViPro-2: Unsupervised State Estimation via Integrated Dynamics for Guiding Video Prediction", "comment": "Published in 2025 International Joint Conference on Neural Networks\n  (IJCNN)", "summary": "Predicting future video frames is a challenging task with many downstream\napplications. Previous work has shown that procedural knowledge enables deep\nmodels for complex dynamical settings, however their model ViPro assumed a\ngiven ground truth initial symbolic state. We show that this approach led to\nthe model learning a shortcut that does not actually connect the observed\nenvironment with the predicted symbolic state, resulting in the inability to\nestimate states given an observation if previous states are noisy. In this\nwork, we add several improvements to ViPro that enables the model to correctly\ninfer states from observations without providing a full ground truth state in\nthe beginning. We show that this is possible in an unsupervised manner, and\nextend the original Orbits dataset with a 3D variant to close the gap to real\nworld scenarios.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.06357", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06357", "abs": "https://arxiv.org/abs/2508.06357", "authors": ["Aman Bhatta", "Maria Dhakal", "Michael C. King", "Kevin W. Bowyer"], "title": "Are you In or Out (of gallery)? Wisdom from the Same-Identity Crowd", "comment": null, "summary": "A central problem in one-to-many facial identification is that the person in\nthe probe image may or may not have enrolled image(s) in the gallery; that is,\nmay be In-gallery or Out-of-gallery. Past approaches to detect when a rank-one\nresult is Out-of-gallery have mostly focused on finding a suitable threshold on\nthe similarity score. We take a new approach, using the additional enrolled\nimages of the identity with the rank-one result to predict if the rank-one\nresult is In-gallery / Out-of-gallery. Given a gallery of identities and\nimages, we generate In-gallery and Out-of-gallery training data by extracting\nthe ranks of additional enrolled images corresponding to the rank-one identity.\nWe then train a classifier to utilize this feature vector to predict whether a\nrank-one result is In-gallery or Out-of-gallery. Using two different datasets\nand four different matchers, we present experimental results showing that our\napproach is viable for mugshot quality probe images, and also, importantly, for\nprobes degraded by blur, reduced resolution, atmospheric turbulence and\nsunglasses. We also analyze results across demographic groups, and show that\nIn-gallery / Out-of-gallery classification accuracy is similar across\ndemographics. Our approach has the potential to provide an objective estimate\nof whether a one-to-many facial identification is Out-of-gallery, and thereby\nto reduce false positive identifications, wrongful arrests, and wasted\ninvestigative time. Interestingly, comparing the results of older deep\nCNN-based face matchers with newer ones suggests that the effectiveness of our\nOut-of-gallery detection approach emerges only with matchers trained using\nadvanced margin-based loss functions.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u8eab\u4efd\u7684\u9644\u52a0\u6ce8\u518c\u56fe\u50cf\u4ee5\u53ca\u6392\u540d\u7b2c\u4e00\u7684\u7ed3\u679c\u6765\u9884\u6d4b\u6392\u540d\u7b2c\u4e00\u7684\u7ed3\u679c\u662f In-gallery \u8fd8\u662f Out-of-gallery\u3002\u901a\u8fc7\u63d0\u53d6\u4e0e\u6392\u540d\u7b2c\u4e00\u7684\u8eab\u4efd\u76f8\u5bf9\u5e94\u7684\u9644\u52a0\u6ce8\u518c\u56fe\u50cf\u7684\u6392\u540d\u6765\u751f\u6210 In-gallery \u548c Out-of-gallery \u8bad\u7ec3\u6570\u636e\u3002\u7136\u540e\u8bad\u7ec3\u4e00\u4e2a\u5206\u7c7b\u5668\u6765\u5229\u7528\u8fd9\u4e2a\u7279\u5f81\u5411\u91cf\u6765\u9884\u6d4b\u6392\u540d\u7b2c\u4e00\u7684\u7ed3\u679c\u662f In-gallery \u8fd8\u662f Out-of-gallery\u3002\u8be5\u65b9\u6cd5\u5728\u4e24\u4e2a\u4e0d\u540c\u7684\u6570\u636e\u96c6\u548c\u56db\u4e2a\u4e0d\u540c\u7684\u5339\u914d\u5668\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5bf9\u4e8e the mugshot quality probe images \u662f\u53ef\u884c\u7684\uff0c\u5e76\u4e14\u5bf9\u4e8e the probe images \u8fdb\u884c\u4e86\u6a21\u7cca\uff0c\u964d\u4f4e\u4e86\u5206\u8fa8\u7387\uff0c\u5927\u6c14\u6e4d\u6d41\u548c\u592a\u9633\u955c\u7684\u9000\u5316\u4e5f\u662f\u53ef\u884c\u7684\u3002\u6b64\u5916\uff0c\u8fd8\u5206\u6790\u4e86\u8de8\u4e0d\u540c\u4eba\u7fa4\u7684\u7ed3\u679c\uff0c\u53d1\u73b0 In-gallery / Out-of-gallery \u7684\u5206\u7c7b\u51c6\u786e\u6027\u5728\u4e0d\u540c\u4eba\u7fa4\u4e4b\u95f4\u76f8\u4f3c\u3002\u8be5\u65b9\u6cd5\u6709\u6f5c\u529b\u63d0\u4f9b\u5bf9 one-to-many \u9762\u90e8\u8bc6\u522b\u662f\u5426\u4e3a Out-of-gallery \u7684\u5ba2\u89c2\u4f30\u8ba1\uff0c\u4ece\u800c\u51cf\u5c11\u9519\u8bef\u8bc6\u522b\uff0c\u9519\u8bef\u7684\u902e\u6355\u548c\u6d6a\u8d39\u7684\u8c03\u67e5\u65f6\u95f4\u3002\u6709\u8da3\u7684\u662f\uff0c\u5c06\u65e7\u7684\u57fa\u4e8e CNN \u7684\u4eba\u8138\u5339\u914d\u5668\u4e0e\u65b0\u7684\u5339\u914d\u5668\u7684\u7ed3\u679c\u8fdb\u884c\u6bd4\u8f83\u8868\u660e\uff0c\u6211\u4eec\u7684 Out-of-gallery \u68c0\u6d4b\u65b9\u6cd5\u7684\u6709\u6548\u6027\u4ec5\u5728\u4f7f\u7528\u4e86\u57fa\u4e8e\u9ad8\u7ea7\u8fb9\u754c\u635f\u5931\u51fd\u6570\u8fdb\u884c\u8bad\u7ec3\u7684\u5339\u914d\u5668\u4e2d\u624d\u51fa\u73b0\u3002", "motivation": "\u5728 one-to-many \u9762\u90e8\u8bc6\u522b\u4e2d\uff0c\u63a2\u9488\u56fe\u50cf\u4e2d\u7684\u4eba\u7269\u53ef\u80fd\u5728\u5e93\u4e2d\u6709\u6ce8\u518c\u56fe\u50cf\uff0c\u4e5f\u53ef\u80fd\u6ca1\u6709\u6ce8\u518c\u56fe\u50cf\uff0c\u5373\u53ef\u80fd\u5728\u5e93\u4e2d\u6216\u4e0d\u5728\u5e93\u4e2d\u3002\u8fc7\u53bb\u68c0\u6d4b\u6392\u540d\u7b2c\u4e00\u7684\u7ed3\u679c\u662f\u5426\u4e3a Out-of-gallery \u7684\u65b9\u6cd5\u4e3b\u8981\u96c6\u4e2d\u5728\u5bfb\u627e\u76f8\u4f3c\u6027\u5206\u6570\u7684\u5408\u9002\u9608\u503c\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u5229\u7528\u8eab\u4efd\u7684\u9644\u52a0\u6ce8\u518c\u56fe\u50cf\u4ee5\u53ca\u6392\u540d\u7b2c\u4e00\u7684\u7ed3\u679c\u6765\u9884\u6d4b\u6392\u540d\u7b2c\u4e00\u7684\u7ed3\u679c\u662f In-gallery \u8fd8\u662f Out-of-gallery\u3002\u901a\u8fc7\u63d0\u53d6\u4e0e\u6392\u540d\u7b2c\u4e00\u7684\u8eab\u4efd\u76f8\u5bf9\u5e94\u7684\u9644\u52a0\u6ce8\u518c\u56fe\u50cf\u7684\u6392\u540d\u6765\u751f\u6210 In-gallery \u548c Out-of-gallery \u8bad\u7ec3\u6570\u636e\u3002\u7136\u540e\u8bad\u7ec3\u4e00\u4e2a\u5206\u7c7b\u5668\u6765\u5229\u7528\u8fd9\u4e2a\u7279\u5f81\u5411\u91cf\u6765\u9884\u6d4b\u6392\u540d\u7b2c\u4e00\u7684\u7ed3\u679c\u662f In-gallery \u8fd8\u662f Out-of-gallery\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5bf9\u4e8e the mugshot quality probe images \u662f\u53ef\u884c\u7684\uff0c\u5e76\u4e14\u5bf9\u4e8e the probe images \u8fdb\u884c\u4e86\u6a21\u7cca\uff0c\u964d\u4f4e\u4e86\u5206\u8fa8\u7387\uff0c\u5927\u6c14\u6e4d\u6d41\u548c\u592a\u9633\u955c\u7684\u9000\u5316\u4e5f\u662f\u53ef\u884c\u7684\u3002\u6b64\u5916\uff0c\u5206\u6790\u4e86\u8de8\u4e0d\u540c\u4eba\u7fa4\u7684\u7ed3\u679c\uff0c\u53d1\u73b0 In-gallery / Out-of-gallery \u7684\u5206\u7c7b\u51c6\u786e\u6027\u5728\u4e0d\u540c\u4eba\u7fa4\u4e4b\u95f4\u76f8\u4f3c\u3002\u6709\u8da3\u7684\u662f\uff0c\u5c06\u65e7\u7684\u57fa\u4e8e CNN \u7684\u4eba\u8138\u5339\u914d\u5668\u4e0e\u65b0\u7684\u5339\u914d\u5668\u7684\u7ed3\u679c\u8fdb\u884c\u6bd4\u8f83\u8868\u660e\uff0c\u6211\u4eec\u7684 Out-of-gallery \u68c0\u6d4b\u65b9\u6cd5\u7684\u6709\u6548\u6027\u4ec5\u5728\u4f7f\u7528\u4e86\u57fa\u4e8e\u9ad8\u7ea7\u8fb9\u754c\u635f\u5931\u51fd\u6570\u8fdb\u884c\u8bad\u7ec3\u7684\u5339\u914d\u5668\u4e2d\u624d\u51fa\u73b0\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4e24\u4e2a\u4e0d\u540c\u7684\u6570\u636e\u96c6\u548c\u56db\u4e2a\u4e0d\u540c\u7684\u5339\u914d\u5668\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5bf9\u4e8e the mugshot quality probe images \u662f\u53ef\u884c\u7684\uff0c\u5e76\u4e14\u5bf9\u4e8e the probe images \u8fdb\u884c\u4e86\u6a21\u7cca\uff0c\u964d\u4f4e\u4e86\u5206\u8fa8\u7387\uff0c\u5927\u6c14\u6e4d\u6d41\u548c\u592a\u9633\u955c\u7684\u9000\u5316\u4e5f\u662f\u53ef\u884c\u7684\u3002\u6b64\u5916\uff0c\u8fd8\u5206\u6790\u4e86\u8de8\u4e0d\u540c\u4eba\u7fa4\u7684\u7ed3\u679c\uff0c\u53d1\u73b0 In-gallery / Out-of-gallery \u7684\u5206\u7c7b\u51c6\u786e\u6027\u5728\u4e0d\u540c\u4eba\u7fa4\u4e4b\u95f4\u76f8\u4f3c\u3002\u8be5\u65b9\u6cd5\u6709\u6f5c\u529b\u63d0\u4f9b\u5bf9 one-to-many \u9762\u90e8\u8bc6\u522b\u662f\u5426\u4e3a Out-of-gallery \u7684\u5ba2\u89c2\u4f30\u8ba1\uff0c\u4ece\u800c\u51cf\u5c11\u9519\u8bef\u8bc6\u522b\uff0c\u9519\u8bef\u7684\u902e\u6355\u548c\u6d6a\u8d39\u7684\u8c03\u67e5\u65f6\u95f4\u3002"}}
{"id": "2508.06350", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06350", "abs": "https://arxiv.org/abs/2508.06350", "authors": ["Yingxian Chen", "Jiahui Liu", "Ruifan Di", "Yanwei Li", "Chirui Chang", "Shizhen Zhao", "Wilton W. T. Fok", "Xiaojuan Qi", "Yik-Chung Wu"], "title": "Aligning Effective Tokens with Video Anomaly in Large Language Models", "comment": null, "summary": "Understanding abnormal events in videos is a vital and challenging task that\nhas garnered significant attention in a wide range of applications. Although\ncurrent video understanding Multi-modal Large Language Models (MLLMs) are\ncapable of analyzing general videos, they often struggle to handle anomalies\ndue to the spatial and temporal sparsity of abnormal events, where the\nredundant information always leads to suboptimal outcomes. To address these\nchallenges, exploiting the representation and generalization capabilities of\nVison Language Models (VLMs) and Large Language Models (LLMs), we propose\nVA-GPT, a novel MLLM designed for summarizing and localizing abnormal events in\nvarious videos. Our approach efficiently aligns effective tokens between visual\nencoders and LLMs through two key proposed modules: Spatial Effective Token\nSelection (SETS) and Temporal Effective Token Generation (TETG). These modules\nenable our model to effectively capture and analyze both spatial and temporal\ninformation associated with abnormal events, resulting in more accurate\nresponses and interactions. Furthermore, we construct an instruction-following\ndataset specifically for fine-tuning video-anomaly-aware MLLMs, and introduce a\ncross-domain evaluation benchmark based on XD-Violence dataset. Our proposed\nmethod outperforms existing state-of-the-art methods on various benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aVA-GPT\u7684\u65b0\u578b\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7SETS\u548cTETG\u6a21\u5757\u6709\u6548\u5904\u7406\u89c6\u9891\u4e2d\u7684\u5f02\u5e38\u4e8b\u4ef6\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u9886\u5148\u6210\u679c\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709MLLM\u5728\u5904\u7406\u5f02\u5e38\u4e8b\u4ef6\u65f6\u56e0\u7a7a\u95f4\u548c\u65f6\u95f4\u7a00\u758f\u6027\u4ee5\u53ca\u5197\u4f59\u4fe1\u606f\u5bfc\u81f4\u7684\u6b21\u4f18\u7ed3\u679c\u7684\u6311\u6218\uff0c\u5229\u7528VLM\u548cLLM\u7684\u8868\u793a\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aVA-GPT\u7684\u65b0\u578b\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\uff0c\u8be5\u6a21\u578b\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u8868\u793a\u548c\u6cdb\u5316\u80fd\u529b\u3002\u901a\u8fc7\u7a7a\u95f4\u6709\u6548\u4ee4\u724c\u9009\u62e9\uff08SETS\uff09\u548c\u65f6\u95f4\u6709\u6548\u4ee4\u724c\u751f\u6210\uff08TETG\uff09\u4e24\u4e2a\u5173\u952e\u6a21\u5757\uff0c\u6709\u6548\u5730\u5c06\u89c6\u89c9\u7f16\u7801\u5668\u548cLLM\u4e4b\u95f4\u7684\u6709\u6548\u4ee4\u724c\u8fdb\u884c\u5bf9\u9f50\uff0c\u4ece\u800c\u6355\u83b7\u548c\u5206\u6790\u4e0e\u5f02\u5e38\u4e8b\u4ef6\u76f8\u5173\u7684\u7a7a\u95f4\u548c\u65f6\u95f4\u4fe1\u606f\u3002\u6b64\u5916\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u4e13\u95e8\u7528\u4e8e\u5fae\u8c03\u89c6\u9891\u5f02\u5e38\u611f\u77e5MLLM\u7684\u6307\u4ee4\u9075\u5faa\u6570\u636e\u96c6\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u4e2a\u57fa\u4e8eXD-Violence\u6570\u636e\u96c6\u7684\u8de8\u57df\u8bc4\u4f30\u57fa\u51c6\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u5404\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5305\u62ecXD-Violence\u6570\u636e\u96c6\uff0c\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u80fd\u591f\u66f4\u51c6\u786e\u5730\u54cd\u5e94\u548c\u4ea4\u4e92\u3002", "conclusion": "VA-GPT\u5728\u5404\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u80fd\u591f\u51c6\u786e\u5730\u5bf9\u5f02\u5e38\u4e8b\u4ef6\u505a\u51fa\u54cd\u5e94\u548c\u4ea4\u4e92\u3002"}}
{"id": "2508.06351", "categories": ["cs.CV", "math.OC"], "pdf": "https://arxiv.org/pdf/2508.06351", "abs": "https://arxiv.org/abs/2508.06351", "authors": ["Olakunle S. Abawonse", "G\u00fcnay Do\u011fan"], "title": "An Implemention of Two-Phase Image Segmentation using the Split Bregman Method", "comment": "15 pages", "summary": "In this paper, we describe an implementation of the two-phase image\nsegmentation algorithm proposed by Goldstein, Bresson, Osher in\n\\cite{gold:bre}. This algorithm partitions the domain of a given 2d image into\nforeground and background regions, and each pixel of the image is assigned\nmembership to one of these two regions. The underlying assumption for the\nsegmentation model is that the pixel values of the input image can be\nsummarized by two distinct average values, and that the region boundaries are\nsmooth. Accordingly, the model is defined as an energy in which the variable is\na region membership function to assign pixels to either region, originally\nproposed by Chan and Vese in \\cite{chan:vese}. This energy is the sum of image\ndata terms in the regions and a length penalty for region boundaries.\nGoldstein, Bresson, Osher modify the energy of Chan-Vese in \\cite{gold:bre} so\nthat their new energy can be minimized efficiently using the split Bregman\nmethod to produce an equivalent two-phase segmentation. We provide a detailed\nimplementation of this method \\cite{gold:bre}, and document its performance\nwith several images over a range of algorithm parameters.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5b9e\u73b0\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u56fe\u50cf\u5206\u5272\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u5c06\u56fe\u50cf\u50cf\u7d20\u5206\u914d\u7ed9\u524d\u666f\u6216\u80cc\u666f\u533a\u57df\uff0c\u5e76\u4f7f\u7528\u5206\u88c2Bregman\u65b9\u6cd5\u8fdb\u884c\u4f18\u5316\u3002", "motivation": "\u8be5\u6a21\u578b\u5047\u8bbe\u50cf\u7d20\u503c\u53ef\u4ee5\u7531\u4e24\u4e2a\u4e0d\u540c\u7684\u5e73\u5747\u503c\u4ee5\u53ca\u5e73\u6ed1\u7684\u533a\u57df\u8fb9\u754c\u6765\u6982\u62ec\u3002", "method": "\u4f7f\u7528 Goldstein\u3001Bresson \u548c Osher \u63d0\u51fa\u7684\u4e24\u9636\u6bb5\u56fe\u50cf\u5206\u5272\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u901a\u8fc7\u5206\u88c2Bregman\u65b9\u6cd5\u8fdb\u884c\u6700\u5c0f\u5316\u3002", "result": "\u901a\u8fc7\u5bf9\u51e0\u4e2a\u56fe\u50cf\u548c\u4e00\u7cfb\u5217\u7b97\u6cd5\u53c2\u6570\u7684\u6027\u80fd\u8fdb\u884c\u6587\u6863\u5316\uff0c\u63d0\u4f9b\u4e86\u8be5\u65b9\u6cd5\u7684\u8be6\u7ec6\u5b9e\u73b0\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u5c062D\u56fe\u50cf\u7684\u57df\u5206\u5272\u4e3a\u524d\u666f\u548c\u80cc\u666f\u533a\u57df\uff0c\u5e76\u4e3a\u56fe\u50cf\u7684\u6bcf\u4e2a\u50cf\u7d20\u5206\u914d\u4e00\u4e2a\u6210\u5458\u8d44\u683c\u3002"}}
{"id": "2508.06382", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06382", "abs": "https://arxiv.org/abs/2508.06382", "authors": ["Xiangyu Wu", "Feng Yu", "Yang Yang", "Jianfeng Lu"], "title": "Text as Any-Modality for Zero-Shot Classification by Consistent Prompt Tuning", "comment": "Accepted for publication at ACMMM 2025", "summary": "The integration of prompt tuning with multimodal learning has shown\nsignificant generalization abilities for various downstream tasks. Despite\nadvancements, existing methods heavily depend on massive modality-specific\nlabeled data (e.g., video, audio, and image), or are customized for a single\nmodality. In this study, we present Text as Any-Modality by Consistent Prompt\nTuning (TaAM-CPT), a scalable approach for constructing a general\nrepresentation model toward unlimited modalities using solely text data.\nTaAM-CPT comprises modality prompt pools, text construction, and\nmodality-aligned text encoders from pre-trained models, which allows for\nextending new modalities by simply adding prompt pools and modality-aligned\ntext encoders. To harmonize the learning across different modalities, TaAM-CPT\ndesigns intra- and inter-modal learning objectives, which can capture category\ndetails within modalities while maintaining semantic consistency across\ndifferent modalities. Benefiting from its scalable architecture and pre-trained\nmodels, TaAM-CPT can be seamlessly extended to accommodate unlimited\nmodalities. Remarkably, without any modality-specific labeled data, TaAM-CPT\nachieves leading results on diverse datasets spanning various modalities,\nincluding video classification, image classification, and audio classification.\nThe code is available at https://github.com/Jinx630/TaAM-CPT.", "AI": {"tldr": "TaAM-CPT \u662f\u4e00\u79cd\u5229\u7528\u6587\u672c\u6570\u636e\u5904\u7406\u591a\u79cd\u6a21\u6001\uff08\u5982\u89c6\u9891\u3001\u56fe\u50cf\u3001\u97f3\u9891\uff09\u7684 AI \u6a21\u578b\u3002\u5b83\u4e0d\u9700\u8981\u7279\u5b9a\u6a21\u6001\u7684\u6807\u7b7e\u6570\u636e\uff0c\u5c31\u80fd\u5728\u5404\u79cd\u4efb\u52a1\u4e0a\u53d6\u5f97\u826f\u597d\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u5728\u591a\u6a21\u6001\u5b66\u4e60\u65b9\u9762\uff0c\u8981\u4e48\u4e25\u91cd\u4f9d\u8d56\u5927\u89c4\u6a21\u7684\u7279\u5b9a\u6a21\u6001\u6807\u7b7e\u6570\u636e\uff0c\u8981\u4e48\u9488\u5bf9\u5355\u4e00\u6a21\u6001\u8fdb\u884c\u5b9a\u5236\u3002\u672c\u7814\u7a76\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u4ec5\u4f7f\u7528\u6587\u672c\u6570\u636e\u5373\u53ef\u4e3a\u65e0\u9650\u6a21\u6001\u6784\u5efa\u901a\u7528\u8868\u793a\u6a21\u578b\u7684\u53ef\u6269\u5c55\u65b9\u6cd5\u3002", "method": "TaAM-CPT \u5305\u542b\u6a21\u6001\u63d0\u793a\u6c60\u3001\u6587\u672c\u6784\u5efa\u548c\u6765\u81ea\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u6a21\u6001\u5bf9\u9f50\u6587\u672c\u7f16\u7801\u5668\uff0c\u5e76\u901a\u8fc7\u8bbe\u8ba1\u7c7b\u5185\u548c\u7c7b\u95f4\u5b66\u4e60\u76ee\u6807\u6765\u534f\u8c03\u4e0d\u540c\u6a21\u6001\u7684\u5b66\u4e60\u3002", "result": "TaAM-CPT \u5728\u8de8\u8d8a\u89c6\u9891\u5206\u7c7b\u3001\u56fe\u50cf\u5206\u7c7b\u548c\u97f3\u9891\u5206\u7c7b\u7b49\u591a\u79cd\u6a21\u6001\u7684\u5404\u79cd\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u9886\u5148\u7684\u7ed3\u679c\u3002", "conclusion": "TaAM-CPT \u662f\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\uff0c\u65e0\u9700\u4efb\u4f55\u7279\u5b9a\u4e8e\u6a21\u6001\u7684\u6807\u7b7e\u6570\u636e\uff0c\u5373\u53ef\u4e3a\u65e0\u9650\u6a21\u6001\u6784\u5efa\u901a\u7528\u8868\u793a\u6a21\u578b\u3002"}}
{"id": "2508.06392", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06392", "abs": "https://arxiv.org/abs/2508.06392", "authors": ["Wenbin Teng", "Gonglin Chen", "Haiwei Chen", "Yajie Zhao"], "title": "FVGen: Accelerating Novel-View Synthesis with Adversarial Video Diffusion Distillation", "comment": null, "summary": "Recent progress in 3D reconstruction has enabled realistic 3D models from\ndense image captures, yet challenges persist with sparse views, often leading\nto artifacts in unseen areas. Recent works leverage Video Diffusion Models\n(VDMs) to generate dense observations, filling the gaps when only sparse views\nare available for 3D reconstruction tasks. A significant limitation of these\nmethods is their slow sampling speed when using VDMs. In this paper, we present\nFVGen, a novel framework that addresses this challenge by enabling fast novel\nview synthesis using VDMs in as few as four sampling steps. We propose a novel\nvideo diffusion model distillation method that distills a multi-step denoising\nteacher model into a few-step denoising student model using Generative\nAdversarial Networks (GANs) and softened reverse KL-divergence minimization.\nExtensive experiments on real-world datasets show that, compared to previous\nworks, our framework generates the same number of novel views with similar (or\neven better) visual quality while reducing sampling time by more than 90%.\nFVGen significantly improves time efficiency for downstream reconstruction\ntasks, particularly when working with sparse input views (more than 2) where\npre-trained VDMs need to be run multiple times to achieve better spatial\ncoverage.", "AI": {"tldr": "FVGen\u901a\u8fc7GAN\u548cKL\u6563\u5ea6\u6700\u5c0f\u5316\u84b8\u998fVDMs\uff0c\u5b9e\u73b0\u5feb\u901f\u65b0\u89c6\u70b9\u5408\u6210\uff0c\u6bd4\u73b0\u6709\u65b9\u6cd5\u5feb90%\uff0c\u8d28\u91cf\u76f8\u5f53\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u89c6\u9891\u6269\u6563\u6a21\u578b\uff08VDMs\uff09\u7684\u4e09\u7ef4\u91cd\u5efa\u65b9\u6cd5\u5728\u7a00\u758f\u89c6\u56fe\u4e0b\u867d\u7136\u80fd\u751f\u6210\u66f4\u5bc6\u96c6\u7684\u89c2\u6d4b\uff0c\u4f46\u91c7\u6837\u901f\u5ea6\u6162\uff0c\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u89c6\u9891\u6269\u6563\u6a21\u578b\u84b8\u998f\u65b9\u6cd5\uff0c\u5229\u7528\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GANs\uff09\u548c\u8f6f\u5316\u53cd\u5411KL\u6563\u5ea6\u6700\u5c0f\u5316\uff0c\u5c06\u4e00\u4e2a\u591a\u6b65\u53bb\u566a\u6559\u5e08\u6a21\u578b\u84b8\u998f\u4e3a\u4e00\u4e2a\u5c11\u6b65\u53bb\u566a\u5b66\u751f\u6a21\u578b\u3002", "result": "FVGen\u6846\u67b6\u80fd\u591f\u5728\u4ec5\u56db\u4e2a\u91c7\u6837\u6b65\u9aa4\u5185\u5b9e\u73b0\u5feb\u901f\u7684\u65b0\u89c6\u70b9\u5408\u6210\uff0c\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5728\u76f8\u540c\u7684\u89c6\u56fe\u6570\u91cf\u4e0b\uff0c\u89c6\u89c9\u8d28\u91cf\u76f8\u4f3c\u751a\u81f3\u66f4\u597d\uff0c\u4f46\u91c7\u6837\u65f6\u95f4\u51cf\u5c11\u4e8690%\u4ee5\u4e0a\u3002", "conclusion": "FVGen\u901a\u8fc7\u84b8\u998f\u6280\u672f\u663e\u8457\u63d0\u9ad8\u4e86\u89c6\u9891\u6269\u6563\u6a21\u578b\uff08VDMs\uff09\u7684\u91c7\u6837\u901f\u5ea6\uff0c\u5728\u4fdd\u8bc1\u89c6\u89c9\u8d28\u91cf\u7684\u540c\u65f6\u5c06\u91c7\u6837\u65f6\u95f4\u7f29\u77ed\u4e8690%\u4ee5\u4e0a\uff0c\u5c24\u5176\u5728\u7a00\u758f\u89c6\u56fe\u7684\u4e09\u7ef4\u91cd\u5efa\u4efb\u52a1\u4e2d\uff0c\u5176\u6548\u7387\u63d0\u5347\u66f4\u4e3a\u660e\u663e\u3002"}}
{"id": "2508.06407", "categories": ["cs.CV", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.06407", "abs": "https://arxiv.org/abs/2508.06407", "authors": ["Ch Muhammad Awais", "Marco Reggiannini", "Davide Moroni", "Oktay Karakus"], "title": "A Classification-Aware Super-Resolution Framework for Ship Targets in SAR Imagery", "comment": null, "summary": "High-resolution imagery plays a critical role in improving the performance of\nvisual recognition tasks such as classification, detection, and segmentation.\nIn many domains, including remote sensing and surveillance, low-resolution\nimages can limit the accuracy of automated analysis. To address this,\nsuper-resolution (SR) techniques have been widely adopted to attempt to\nreconstruct high-resolution images from low-resolution inputs. Related\ntraditional approaches focus solely on enhancing image quality based on\npixel-level metrics, leaving the relationship between super-resolved image\nfidelity and downstream classification performance largely underexplored. This\nraises a key question: can integrating classification objectives directly into\nthe super-resolution process further improve classification accuracy? In this\npaper, we try to respond to this question by investigating the relationship\nbetween super-resolution and classification through the deployment of a\nspecialised algorithmic strategy. We propose a novel methodology that increases\nthe resolution of synthetic aperture radar imagery by optimising loss functions\nthat account for both image quality and classification performance. Our\napproach improves image quality, as measured by scientifically ascertained\nimage quality indicators, while also enhancing classification accuracy.", "AI": {"tldr": "Integrating classification objectives into super-resolution improves accuracy for SAR imagery.", "motivation": "To address the limitations of low-resolution images in automated analysis and explore whether integrating classification objectives into the super-resolution process can improve classification accuracy.", "method": "A novel methodology is proposed that optimizes loss functions considering both image quality and classification performance to increase the resolution of synthetic aperture radar imagery.", "result": "The approach improves image quality, as measured by scientifically ascertained image quality indicators, while also enhancing classification accuracy.", "conclusion": "The proposed method enhances both image quality and classification accuracy for SAR imagery."}}
{"id": "2508.06420", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06420", "abs": "https://arxiv.org/abs/2508.06420", "authors": ["Ch Muhammad Awais", "Marco Reggiannini", "Davide Moroni", "Oktay Karakus"], "title": "Feature-Space Oversampling for Addressing Class Imbalance in SAR Ship Classification", "comment": "Accepted and presented at IGARSS", "summary": "SAR ship classification faces the challenge of long-tailed datasets, which\ncomplicates the classification of underrepresented classes. Oversampling\nmethods have proven effective in addressing class imbalance in optical data. In\nthis paper, we evaluated the effect of oversampling in the feature space for\nSAR ship classification. We propose two novel algorithms inspired by the\nMajor-to-minor (M2m) method M2m$_f$, M2m$_u$. The algorithms are tested on two\npublic datasets, OpenSARShip (6 classes) and FuSARShip (9 classes), using three\nstate-of-the-art models as feature extractors: ViT, VGG16, and ResNet50.\nAdditionally, we also analyzed the impact of oversampling methods on different\nclass sizes. The results demonstrated the effectiveness of our novel methods\nover the original M2m and baselines, with an average F1-score increase of 8.82%\nfor FuSARShip and 4.44% for OpenSARShip.", "AI": {"tldr": "\u901a\u8fc7\u7279\u5f81\u7a7a\u95f4\u8fc7\u91c7\u6837\u65b9\u6cd5M2m_f\u548cM2m_u\uff0c\u63d0\u5347\u4e86SAR\u8239\u8236\u5206\u7c7b\u4e2d\u5bf9\u5c11\u6570\u7c7b\u522b\u7684\u8bc6\u522b\u80fd\u529b\uff0c\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6548\u679c\u3002", "motivation": "\u89e3\u51b3SAR\u8239\u8236\u5206\u7c7b\u4e2d\u957f\u5c3e\u6570\u636e\u96c6\u5e26\u6765\u7684\u6b20\u4ee3\u8868\u7c7b\u522b\u5206\u7c7b\u56f0\u96be\u7684\u95ee\u9898\u3002", "method": "\u8bc4\u4f30\u4e86\u8fc7\u91c7\u6837\u65b9\u6cd5\u5728\u7279\u5f81\u7a7a\u95f4\u4e2d\u5bf9SAR\uff08\u5408\u6210\u5b54\u5f84\u96f7\u8fbe\uff09\u8239\u8236\u5206\u7c7b\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u7684\u7b97\u6cd5M2m_f\u548cM2m_u\u3002", "result": "\u6240\u63d0\u51fa\u7684M2m_f\u548cM2m_u\u7b97\u6cd5\u5728FuSARShip\u548cOpenSARShip\u6570\u636e\u96c6\u4e0a\uff0c\u76f8\u6bd4\u539f\u59cbM2m\u548c\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e73\u5747F1\u5206\u6570\u5206\u522b\u63d0\u9ad8\u4e868.82%\u548c4.44%\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684M2m_f\u548cM2m_u\u7b97\u6cd5\u5728\u516c\u5f00\u6570\u636e\u96c6OpenSARShip\u548cFuSARShip\u4e0a\uff0c\u4ee5\u53ca\u5728ViT\u3001VGG16\u548cResNet50\u4e09\u79cd\u6a21\u578b\u4e0a\uff0c\u90fd\u663e\u793a\u51fa\u6bd4\u539f\u59cbM2m\u65b9\u6cd5\u548c\u57fa\u7ebf\u65b9\u6cd5\u66f4\u597d\u7684\u6027\u80fd\uff0c\u5206\u522b\u5728FuSARShip\u548cOpenSARShip\u6570\u636e\u96c6\u4e0a\u5e73\u5747F1\u5206\u6570\u63d0\u9ad8\u4e868.82%\u548c4.44%\u3002"}}
{"id": "2508.06429", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06429", "abs": "https://arxiv.org/abs/2508.06429", "authors": ["Guido Manni", "Clemente Lauretti", "Loredana Zollo", "Paolo Soda"], "title": "SPARSE Data, Rich Results: Few-Shot Semi-Supervised Learning via Class-Conditioned Image Translation", "comment": null, "summary": "Deep learning has revolutionized medical imaging, but its effectiveness is\nseverely limited by insufficient labeled training data. This paper introduces a\nnovel GAN-based semi-supervised learning framework specifically designed for\nlow labeled-data regimes, evaluated across settings with 5 to 50 labeled\nsamples per class. Our approach integrates three specialized neural networks --\na generator for class-conditioned image translation, a discriminator for\nauthenticity assessment and classification, and a dedicated classifier --\nwithin a three-phase training framework. The method alternates between\nsupervised training on limited labeled data and unsupervised learning that\nleverages abundant unlabeled images through image-to-image translation rather\nthan generation from noise. We employ ensemble-based pseudo-labeling that\ncombines confidence-weighted predictions from the discriminator and classifier\nwith temporal consistency through exponential moving averaging, enabling\nreliable label estimation for unlabeled data. Comprehensive evaluation across\neleven MedMNIST datasets demonstrates that our approach achieves statistically\nsignificant improvements over six state-of-the-art GAN-based semi-supervised\nmethods, with particularly strong performance in the extreme 5-shot setting\nwhere the scarcity of labeled data is most challenging. The framework maintains\nits superiority across all evaluated settings (5, 10, 20, and 50 shots per\nclass). Our approach offers a practical solution for medical imaging\napplications where annotation costs are prohibitive, enabling robust\nclassification performance even with minimal labeled data. Code is available at\nhttps://github.com/GuidoManni/SPARSE.", "AI": {"tldr": "\u9488\u5bf9\u533b\u5b66\u5f71\u50cf\u6807\u8bb0\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8eGAN\u7684\u534a\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u96c6\u6210\u751f\u6210\u5668\u3001\u5224\u522b\u5668\u548c\u5206\u7c7b\u5668\uff0c\u5e76\u91c7\u7528\u56fe\u50cf\u5230\u56fe\u50cf\u7ffb\u8bd1\u548c\u4f2a\u6807\u7b7e\u6280\u672f\uff0c\u5728\u6781\u5c11\u6807\u8bb0\u6837\u672c\u4e0b\u5b9e\u73b0\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u5728\u533b\u5b66\u5f71\u50cf\u9886\u57df\u53d6\u5f97\u4e86\u9769\u547d\u6027\u7684\u8fdb\u5c55\uff0c\u4f46\u5176\u6709\u6548\u6027\u53d7\u5230\u6807\u8bb0\u8bad\u7ec3\u6570\u636e\u4e0d\u8db3\u7684\u4e25\u91cd\u9650\u5236\u3002\u56e0\u6b64\uff0c\u5f00\u53d1\u4e00\u79cd\u5728\u6807\u8bb0\u6570\u636e\u7a00\u5c11\u7684\u73af\u5883\u4e0b\u6709\u6548\u7684\u65b9\u6cd5\u81f3\u5173\u91cd\u8981\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8eGAN\u7684\u534a\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u4e09\u4e2a\u4e13\u95e8\u7684\u795e\u7ecf\u7f51\u7edc\u2014\u2014\u7528\u4e8e\u7c7b\u522b\u6761\u4ef6\u56fe\u50cf\u8f6c\u6362\u7684\u751f\u6210\u5668\u3001\u7528\u4e8e\u771f\u5b9e\u6027\u8bc4\u4f30\u548c\u5206\u7c7b\u7684\u5224\u522b\u5668\u4ee5\u53ca\u4e00\u4e2a\u4e13\u7528\u7684\u5206\u7c7b\u5668\u2014\u2014\u5728\u4e00\u4e2a\u4e09\u9636\u6bb5\u7684\u8bad\u7ec3\u6846\u67b6\u5185\u3002\u8be5\u65b9\u6cd5\u5728\u6709\u9650\u6807\u8bb0\u6570\u636e\u4e0a\u7684\u76d1\u7763\u8bad\u7ec3\u548c\u5229\u7528\u4e30\u5bcc\u7684\u672a\u6807\u8bb0\u6570\u636e\u901a\u8fc7\u56fe\u50cf\u5230\u56fe\u50cf\u7ffb\u8bd1\uff08\u800c\u975e\u4ece\u566a\u58f0\u751f\u6210\uff09\u8fdb\u884c\u7684\u65e0\u76d1\u7763\u5b66\u4e60\u4e4b\u95f4\u4ea4\u66ff\u8fdb\u884c\u3002\u6211\u4eec\u91c7\u7528\u57fa\u4e8e\u96c6\u6210\uff08ensemble-based\uff09\u7684\u4f2a\u6807\u7b7e\u6280\u672f\uff0c\u7ed3\u5408\u4e86\u6765\u81ea\u5224\u522b\u5668\u548c\u5206\u7c7b\u5668\u7684\u7f6e\u4fe1\u5ea6\u52a0\u6743\u9884\u6d4b\u4ee5\u53ca\u901a\u8fc7\u6307\u6570\u79fb\u52a8\u5e73\u5747\uff08exponential moving averaging\uff09\u5b9e\u73b0\u7684\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u4ece\u800c\u80fd\u591f\u4e3a\u672a\u6807\u8bb0\u6570\u636e\u8fdb\u884c\u53ef\u9760\u7684\u6807\u7b7e\u4f30\u8ba1\u3002", "result": "\u5728\u5341\u4e00\u4e2aMedMNIST\u6570\u636e\u96c6\u4e0a\u7684\u7efc\u5408\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u516d\u79cd\u6700\u5148\u8fdb\u7684\u57fa\u4e8eGAN\u7684\u534a\u76d1\u7763\u65b9\u6cd5\u4e0a\u53d6\u5f97\u4e86\u7edf\u8ba1\u5b66\u4e0a\u7684\u663e\u8457\u6539\u8fdb\uff0c\u5c24\u5176\u662f\u5728\u6807\u8bb0\u6570\u636e\u7a00\u7f3a\u6027\u6700\u5177\u6311\u6218\u6027\u7684\u6781\u7aef5-shot\u8bbe\u7f6e\u4e2d\u8868\u73b0\u5f3a\u52b2\u3002\u8be5\u6846\u67b6\u5728\u6240\u6709\u8bc4\u4f30\u8bbe\u7f6e\uff08\u6bcf\u4e2a\u7c7b\u522b\u76845\u300110\u300120\u548c50\u4e2a\u6837\u672c\uff09\u4e2d\u5747\u4fdd\u6301\u5176\u4f18\u8d8a\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u6807\u6ce8\u6210\u672c\u9ad8\u6602\u7684\u533b\u5b66\u5f71\u50cf\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5207\u5b9e\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5373\u4f7f\u5728\u6807\u8bb0\u6570\u636e\u5f88\u5c11\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u5b9e\u73b0\u7a33\u5065\u7684\u5206\u7c7b\u6027\u80fd\u3002"}}
{"id": "2508.06430", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06430", "abs": "https://arxiv.org/abs/2508.06430", "authors": ["Om Patil", "Jinesh Modi", "Suryabha Mukhopadhyay", "Meghaditya Giri", "Chhavi Malhotra"], "title": "MotionSwap", "comment": "8 pages, 7 figures, 5 tables. This is a student research submission\n  from BITS Pilani, Hyderabad Campus. Our implementation enhances SimSwap with\n  attention modules and dynamic training strategies", "summary": "Face swapping technology has gained significant attention in both academic\nresearch and commercial applications. This paper presents our implementation\nand enhancement of SimSwap, an efficient framework for high fidelity face\nswapping. We introduce several improvements to the original model, including\nthe integration of self and cross-attention mechanisms in the generator\narchitecture, dynamic loss weighting, and cosine annealing learning rate\nscheduling. These enhancements lead to significant improvements in identity\npreservation, attribute consistency, and overall visual quality.\n  Our experimental results, spanning 400,000 training iterations, demonstrate\nprogressive improvements in generator and discriminator performance. The\nenhanced model achieves better identity similarity, lower FID scores, and\nvisibly superior qualitative results compared to the baseline. Ablation studies\nconfirm the importance of each architectural and training improvement. We\nconclude by identifying key future directions, such as integrating StyleGAN3,\nimproving lip synchronization, incorporating 3D facial modeling, and\nintroducing temporal consistency for video-based applications.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u5f15\u5165\u81ea/\u4ea4\u53c9\u6ce8\u610f\u529b\u3001\u52a8\u6001\u635f\u5931\u52a0\u6743\u548c\u4f59\u5f26\u9000\u706b\u5b66\u4e60\u7387\u8c03\u5ea6\u7b49\u6539\u8fdb\u63aa\u65bd\uff0c\u589e\u5f3a\u4e86SimSwap\u9762\u90e8\u4ea4\u6362\u6846\u67b6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8eab\u4efd\u4fdd\u6301\u3001\u5c5e\u6027\u4e00\u81f4\u6027\u548c\u89c6\u89c9\u8d28\u91cf\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u5f97\u5230\u4e86\u9a8c\u8bc1\u3002", "motivation": "\u4e3a\u4e86\u63d0\u5347\u9762\u90e8\u4ea4\u6362\u6280\u672f\u7684\u6548\u7387\u548c\u89c6\u89c9\u4fdd\u771f\u5ea6\uff0c\u5e76\u89e3\u51b3\u73b0\u6709\u6280\u672f\u5728\u8eab\u4efd\u4fdd\u6301\u3001\u5c5e\u6027\u4e00\u81f4\u6027\u548c\u89c6\u89c9\u8d28\u91cf\u65b9\u9762\u5b58\u5728\u7684\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7\u5728\u751f\u6210\u5668\u7ed3\u6784\u4e2d\u96c6\u6210\u81ea\u6ce8\u610f\u529b\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u3001\u52a8\u6001\u635f\u5931\u52a0\u6743\u4ee5\u53ca\u4f59\u5f26\u9000\u706b\u5b66\u4e60\u7387\u8c03\u5ea6\u7b49\u65b9\u6cd5\uff0c\u5bf9SimSwap\u8fdb\u884c\u4e86\u6539\u8fdb\u548c\u5b9e\u73b0\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\uff08\u6db5\u76d640\u4e07\u6b21\u8bad\u7ec3\u8fed\u4ee3\uff09\u663e\u793a\uff0c\u6539\u8fdb\u540e\u7684\u6a21\u578b\u5728\u8eab\u4efd\u76f8\u4f3c\u6027\u3001FID\u5f97\u5206\u548c\u89c6\u89c9\u8d28\u91cf\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u5e76\u4e14\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u4e86\u5404\u9879\u6539\u8fdb\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u6539\u8fdb\u80fd\u591f\u663e\u8457\u63d0\u5347\u9762\u90e8\u4ea4\u6362\u6280\u672f\u7684\u8eab\u4efd\u4fdd\u6301\u3001\u5c5e\u6027\u4e00\u81f4\u6027\u548c\u89c6\u89c9\u8d28\u91cf\uff0c\u5e76\u5bf9\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u8fdb\u884c\u4e86\u5c55\u671b\uff0c\u5305\u62ec\u6574\u5408StyleGAN3\u3001\u6539\u8fdb\u5507\u90e8\u540c\u6b65\u3001\u5f15\u51653D\u9762\u90e8\u5efa\u6a21\u548c\u89c6\u9891\u5e94\u7528\u4e2d\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u3002"}}
{"id": "2508.06434", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06434", "abs": "https://arxiv.org/abs/2508.06434", "authors": ["Shengzhu Yang", "Jiawei Du", "Shuai Lu", "Weihang Zhang", "Ningli Wang", "Huiqi Li"], "title": "CLIPin: A Non-contrastive Plug-in to CLIP for Multimodal Semantic Alignment", "comment": null, "summary": "Large-scale natural image-text datasets, especially those automatically\ncollected from the web, often suffer from loose semantic alignment due to weak\nsupervision, while medical datasets tend to have high cross-modal correlation\nbut low content diversity. These properties pose a common challenge for\ncontrastive language-image pretraining (CLIP): they hinder the model's ability\nto learn robust and generalizable representations. In this work, we propose\nCLIPin, a unified non-contrastive plug-in that can be seamlessly integrated\ninto CLIP-style architectures to improve multimodal semantic alignment,\nproviding stronger supervision and enhancing alignment robustness. Furthermore,\ntwo shared pre-projectors are designed for image and text modalities\nrespectively to facilitate the integration of contrastive and non-contrastive\nlearning in a parameter-compromise manner. Extensive experiments on diverse\ndownstream tasks demonstrate the effectiveness and generality of CLIPin as a\nplug-and-play component compatible with various contrastive frameworks. Code is\navailable at https://github.com/T6Yang/CLIPin.", "AI": {"tldr": "CLIPin \u662f\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u975e\u5bf9\u6bd4\u5b66\u4e60\u63d2\u4ef6\uff0c\u53ef\u4ee5\u589e\u5f3a CLIP \u6a21\u578b\u7684\u5bf9\u9f50\u80fd\u529b\u548c\u8868\u793a\u7684\u7a33\u5065\u6027\uff0c\u517c\u5bb9\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u5e76\u5728\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u7f51\u7edc\u6536\u96c6\u7684\u5927\u89c4\u6a21\u81ea\u7136\u56fe\u50cf-\u6587\u672c\u6570\u636e\u96c6\u7531\u4e8e\u5f31\u76d1\u7763\u800c\u5b58\u5728\u677e\u6563\u7684\u8bed\u4e49\u5bf9\u9f50\u95ee\u9898\uff0c\u800c\u533b\u5b66\u6570\u636e\u96c6\u5219\u5177\u6709\u8f83\u9ad8\u7684\u8de8\u6a21\u6001\u76f8\u5173\u6027\u4f46\u5185\u5bb9\u591a\u6837\u6027\u8f83\u4f4e\u3002\u8fd9\u4e9b\u7279\u6027\u7ed9 CLIP \u5bf9\u6bd4\u8bed\u8a00-\u56fe\u50cf\u9884\u8bad\u7ec3\u5e26\u6765\u4e86\u5171\u540c\u7684\u6311\u6218\uff0c\u963b\u788d\u4e86\u6a21\u578b\u5b66\u4e60\u7a33\u5065\u548c\u53ef\u6cdb\u5316\u7684\u8868\u793a\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u975e\u5bf9\u6bd4\u63d2\u4ef6 CLIPin\uff0c\u53ef\u4ee5\u65e0\u7f1d\u96c6\u6210\u5230 CLIP \u98ce\u683c\u7684\u67b6\u6784\u4e2d\uff0c\u4ee5\u6539\u8fdb\u591a\u6a21\u6001\u8bed\u4e49\u5bf9\u9f50\u3001\u63d0\u4f9b\u66f4\u5f3a\u7684\u76d1\u7763\u548c\u589e\u5f3a\u5bf9\u9f50\u9c81\u68d2\u6027\u3002\u8bbe\u8ba1\u4e86\u4e24\u4e2a\u5171\u4eab\u7684\u9884\u6295\u5f71\u5668\uff0c\u5206\u522b\u7528\u4e8e\u56fe\u50cf\u548c\u6587\u672c\u6a21\u6001\uff0c\u4ee5\u53c2\u6570\u6298\u8877\u7684\u65b9\u5f0f\u4fc3\u8fdb\u5bf9\u6bd4\u5b66\u4e60\u548c\u975e\u5bf9\u6bd4\u5b66\u4e60\u7684\u96c6\u6210\u3002", "result": "CLIPin \u80fd\u591f\u6539\u8fdb\u591a\u6a21\u6001\u8bed\u4e49\u5bf9\u9f50\u3001\u63d0\u4f9b\u66f4\u5f3a\u7684\u76d1\u7763\u548c\u589e\u5f3a\u5bf9\u9f50\u9c81\u68d2\u6027\u3002", "conclusion": "CLIPin \u4f5c\u4e3a\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u7ec4\u4ef6\uff0c\u53ef\u4ee5\u517c\u5bb9\u5404\u79cd\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u5e76\u5728\u5404\u79cd\u4e0b\u6e38\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u6709\u6548\u6027\u548c\u901a\u7528\u6027\u3002"}}
{"id": "2508.06453", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06453", "abs": "https://arxiv.org/abs/2508.06453", "authors": ["Ruida Cheng", "Tejas Sudharshan Mathai", "Pritam Mukherjee", "Benjamin Hou", "Qingqing Zhu", "Zhiyong Lu", "Matthew McAuliffe", "Ronald M. Summers"], "title": "Text Embedded Swin-UMamba for DeepLesion Segmentation", "comment": null, "summary": "Segmentation of lesions on CT enables automatic measurement for clinical\nassessment of chronic diseases (e.g., lymphoma). Integrating large language\nmodels (LLMs) into the lesion segmentation workflow offers the potential to\ncombine imaging features with descriptions of lesion characteristics from the\nradiology reports. In this study, we investigate the feasibility of integrating\ntext into the Swin-UMamba architecture for the task of lesion segmentation. The\npublicly available ULS23 DeepLesion dataset was used along with short-form\ndescriptions of the findings from the reports. On the test dataset, a high Dice\nScore of 82% and low Hausdorff distance of 6.58 (pixels) was obtained for\nlesion segmentation. The proposed Text-Swin-UMamba model outperformed prior\napproaches: 37% improvement over the LLM-driven LanGuideMedSeg model (p <\n0.001),and surpassed the purely image-based xLSTM-UNet and nnUNet models by\n1.74% and 0.22%, respectively. The dataset and code can be accessed at\nhttps://github.com/ruida/LLM-Swin-UMamba", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u6587\u672c\u4fe1\u606f\u6574\u5408\u5230Swin-UMamba\u67b6\u6784\u4e2d\u7684\u65b0\u65b9\u6cd5\uff08Text-Swin-UMamba\uff09\uff0c\u7528\u4e8eCT\u5f71\u50cf\u4e2d\u7684\u75c5\u7076\u5206\u5272\u3002\u8be5\u65b9\u6cd5\u5728ULS23 DeepLesion\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6210\u679c\uff0c\u8bc1\u660e\u4e86\u7ed3\u5408\u6587\u672c\u4fe1\u606f\u5728\u63d0\u9ad8\u75c5\u7076\u5206\u5272\u7cbe\u5ea6\u65b9\u9762\u7684\u6f5c\u529b\u3002", "motivation": "\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u96c6\u6210\u5230\u75c5\u7076\u5206\u5272\u6d41\u7a0b\u4e2d\uff0c\u53ef\u4ee5\u7ed3\u5408\u5f71\u50cf\u7279\u5f81\u548c\u653e\u5c04\u62a5\u544a\u4e2d\u7684\u75c5\u7076\u7279\u5f81\u63cf\u8ff0\uff0c\u4ee5\u63d0\u9ad8\u4e34\u5e8a\u8bc4\u4f30\u7684\u51c6\u786e\u6027\u3002", "method": "\u672c\u7814\u7a76\u5c06\u6587\u672c\u4fe1\u606f\u6574\u5408\u5230Swin-UMamba\u67b6\u6784\u4e2d\uff0c\u4ee5\u6267\u884c\u75c5\u7076\u5206\u5272\u4efb\u52a1\u3002\u4f7f\u7528\u516c\u5f00\u7684ULS23 DeepLesion\u6570\u636e\u96c6\u53ca\u5176\u76f8\u5173\u7684\u7b80\u77ed\u75c5\u7076\u63cf\u8ff0\u3002", "result": "\u5728\u6d4b\u8bd5\u6570\u636e\u96c6\u4e0a\uff0c\u75c5\u7076\u5206\u5272\u8fbe\u5230\u4e8682%\u7684Dice\u5206\u6570\u548c6.58\u50cf\u7d20\u7684Hausdorff\u8ddd\u79bb\u3002\u63d0\u51fa\u7684Text-Swin-UMamba\u6a21\u578b\u6bd4\u4e4b\u524d\u7684\u6a21\u578b\uff08LLM\u9a71\u52a8\u7684LanGuideMedSeg\u6a21\u578b\u3001\u7eaf\u56fe\u50cf\u7684xLSTM-UNet\u548cnnUNet\u6a21\u578b\uff09\u6709\u663e\u8457\u63d0\u9ad8\u3002", "conclusion": "\u5c06\u6587\u672c\u4fe1\u606f\u4e0eSwin-UMamba\u67b6\u6784\u76f8\u7ed3\u5408\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u7528\u4e8eCT\u5f71\u50cf\u4e2d\u7684\u75c5\u7076\u5206\u5272\u4efb\u52a1\uff0c\u5e76\u4e14\u5728\u6d88\u878d\u7814\u7a76\u4e2d\u8bc1\u660e\u4e86\u6587\u672c\u4fe1\u606f\u7684\u4ef7\u503c\u3002"}}
{"id": "2508.06494", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06494", "abs": "https://arxiv.org/abs/2508.06494", "authors": ["Yehonathan Litman", "Fernando De la Torre", "Shubham Tulsiani"], "title": "LightSwitch: Multi-view Relighting with Material-guided Diffusion", "comment": "ICCV 2025, Project page & Code:\n  https://yehonathanlitman.github.io/light_switch/", "summary": "Recent approaches for 3D relighting have shown promise in integrating 2D\nimage relighting generative priors to alter the appearance of a 3D\nrepresentation while preserving the underlying structure. Nevertheless,\ngenerative priors used for 2D relighting that directly relight from an input\nimage do not take advantage of intrinsic properties of the subject that can be\ninferred or cannot consider multi-view data at scale, leading to subpar\nrelighting. In this paper, we propose Lightswitch, a novel finetuned\nmaterial-relighting diffusion framework that efficiently relights an arbitrary\nnumber of input images to a target lighting condition while incorporating cues\nfrom inferred intrinsic properties. By using multi-view and material\ninformation cues together with a scalable denoising scheme, our method\nconsistently and efficiently relights dense multi-view data of objects with\ndiverse material compositions. We show that our 2D relighting prediction\nquality exceeds previous state-of-the-art relighting priors that directly\nrelight from images. We further demonstrate that LightSwitch matches or\noutperforms state-of-the-art diffusion inverse rendering methods in relighting\nsynthetic and real objects in as little as 2 minutes.", "AI": {"tldr": "LightSwitch\u901a\u8fc7\u7ed3\u5408\u591a\u89c6\u56fe\u3001\u6750\u8d28\u4fe1\u606f\u548c\u63a8\u65ad\u7684\u5185\u5728\u5c5e\u6027\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u76843D\u573a\u666f\u91cd\u5149\u7167\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u67092D\u56fe\u50cf\u91cd\u5149\u7167\u7684\u751f\u6210\u5148\u9a8c\u65b9\u6cd5\u5728\u5c062D\u5148\u9a8c\u96c6\u6210\u52303D\u8868\u793a\u65f6\uff0c\u672a\u80fd\u5229\u7528\u53ef\u63a8\u65ad\u7684\u4e3b\u4f53\u5185\u5728\u5c5e\u6027\uff0c\u4e5f\u672a\u80fd\u5145\u5206\u5229\u7528\u5927\u89c4\u6a21\u591a\u89c6\u56fe\u6570\u636e\uff0c\u5bfc\u81f4\u91cd\u5149\u7167\u6548\u679c\u4e0d\u4f73\u3002", "method": "LightSwitch\u662f\u4e00\u79cd\u65b0\u9896\u7684\u3001\u7ecf\u8fc7\u5fae\u8c03\u7684\u6750\u8d28\u91cd\u5149\u7167\u6269\u6563\u6846\u67b6\uff0c\u5b83\u7ed3\u5408\u4e86\u4ece\u63a8\u65ad\u7684\u5185\u5728\u5c5e\u6027\u4e2d\u63d0\u53d6\u7684\u7ebf\u7d22\uff0c\u80fd\u591f\u5c06\u4efb\u610f\u6570\u91cf\u7684\u8f93\u5165\u56fe\u50cf\u9ad8\u6548\u5730\u91cd\u65b0\u7167\u4eae\u5230\u76ee\u6807\u5149\u7167\u6761\u4ef6\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u591a\u89c6\u56fe\u548c\u6750\u8d28\u4fe1\u606f\u7ebf\u7d22\uff0c\u5e76\u7ed3\u5408\u53ef\u6269\u5c55\u7684\u53bb\u566a\u65b9\u6848\u3002", "result": "LightSwitch\u80fd\u591f\u6301\u7eed\u9ad8\u6548\u5730\u5bf9\u5177\u6709\u591a\u6837\u5316\u6750\u8d28\u6784\u6210\u7684\u5bf9\u8c61\u7684\u5bc6\u96c6\u591a\u89c6\u56fe\u6570\u636e\u8fdb\u884c\u91cd\u5149\u7167\uff0c\u51762D\u91cd\u5149\u7167\u9884\u6d4b\u8d28\u91cf\u8d85\u8d8a\u4e86\u73b0\u6709\u76f4\u63a5\u4ece\u56fe\u50cf\u8fdb\u884c\u91cd\u5149\u7167\u7684\u5148\u9a8c\u6280\u672f\uff0c\u5e76\u4e14\u5728\u91cd\u5149\u7167\u5408\u6210\u53ca\u771f\u5b9e\u5bf9\u8c61\u65f6\uff0c\u6027\u80fd\u4e0e\u6700\u5148\u8fdb\u7684\u6269\u6563\u9006\u6e32\u67d3\u65b9\u6cd5\u76f8\u5f53\u6216\u66f4\u4f18\u3002", "conclusion": "LightSwitch\u57282D\u91cd\u5149\u7167\u9884\u6d4b\u8d28\u91cf\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u6280\u672f\uff0c\u5e76\u4e14\u5728\u91cd\u5149\u7167\u5408\u6210\u53ca\u771f\u5b9e\u5bf9\u8c61\u65f6\uff0c\u6027\u80fd\u4e0e\u6700\u5148\u8fdb\u7684\u6269\u6563\u9006\u6e32\u67d3\u65b9\u6cd5\u76f8\u5f53\u751a\u81f3\u66f4\u4f18\uff0c\u5904\u7406\u65f6\u95f4\u4ec5\u97002\u5206\u949f\u3002"}}
