{"id": "2602.11324", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2602.11324", "abs": "https://arxiv.org/abs/2602.11324", "authors": ["Jonas Ellert", "Tomasz Kociumaka"], "title": "Time-Optimal Construction of String Synchronizing Sets", "comment": "Full version of a work to appear in the proceedings of STACS 2026. The abstract has been abridged to comply with arXiv format requirements", "summary": "A key principle in string processing is local consistency: using short contexts to handle matching fragments of a string consistently. String synchronizing sets [Kempa, Kociumaka; STOC 2019] are an influential instantiation of this principle. A $\u03c4$-synchronizing set of a length-$n$ string is a set of $O(n/\u03c4)$ positions, chosen via their length-$2\u03c4$ contexts, such that (outside highly periodic regions) at least one position in every length-$\u03c4$ window is selected. Among their applications are faster algorithms for data compression, text indexing, and string similarity in the word RAM model.\n  We show how to preprocess any string $T \\in [0..\u03c3)^n$ in $O(n\\log\u03c3/\\log n)$ time so that, for any $\u03c4\\in[1..n]$, a $\u03c4$-synchronizing set of $T$ can be constructed in $O((n\\log\u03c4)/(\u03c4\\log n))$ time. Both bounds are optimal in the word RAM model with word size $w=\u0398(\\log n)$. Previously, the construction time was $O(n/\u03c4)$, either after an $O(n)$-time preprocessing [Kociumaka, Radoszewski, Rytter, Wale\u0144; SICOMP 2024], or without preprocessing if $\u03c4<0.2\\log_\u03c3n$ [Kempa, Kociumaka; STOC 2019].\n  A simple version of our method outputs the set as a sorted list in $O(n/\u03c4)$ time, or as a bitmask in $O(n/\\log n)$ time. Our optimal construction produces a compact fully indexable dictionary, supporting select queries in $O(1)$ time and rank queries in $O(\\log(\\tfrac{\\log\u03c4}{\\log\\log n}))$ time, matching unconditional cell-probe lower bounds for $\u03c4\\le n^{1-\u03a9(1)}$.\n  We achieve this via a new framework for processing sparse integer sequences in a custom variable-length encoding. For rank and select queries, we augment the optimal variant of van Emde Boas trees [P\u0103tra\u015fcu, Thorup; STOC 2006] with a deterministic linear-time construction. The above query-time guarantees hold after preprocessing time proportional to the encoding size (in words).", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2602.11302", "categories": ["cond-mat.mes-hall", "cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2602.11302", "abs": "https://arxiv.org/abs/2602.11302", "authors": ["Nicol\u00f2 D'Anna", "Nareg Ghazikhanian", "Katherine Matthews", "Daseul Ham", "Su Yong Lee", "Alex Frano", "Ivan K. Schuller", "Oleg Shpyrko"], "title": "Jamming-controlled stochasticity in metal-insulator switching", "comment": "8 pages, 5 figures", "summary": "Understanding and controlling phase transitions is a fundamental part of physics and has been central to many technological revolutions, from steam engines to field-effect transistors. At present, there is strong interest in materials with strongly coupled structural and electronic phase transitions, which hold promise for energy-efficient technologies. Utilizing a structural phase transition and controlling its plasticity naturally leads to built-in memory, a key feature for emulating neurons and synapses in neuromorphic technologies. Here, $\\textit{operando}$ Bragg X-ray photon correlation spectroscopy is used to study the evolution of the nano-domain distribution at the micron-scale in neuromorphic devices made from the archetypal Mott insulator vanadium dioxide. It is found that after electrical switching, slow nano-domain reconfiguration occurs on timescales of thousands of seconds and that the domains undergo a jamming transition, offering control over switching stochasticity at the micron scale. More precisely, repetitive above-threshold currents plastically drive the system into a jammed/glassy state where switching becomes deterministic, while sub-threshold currents erase the short-term memory contained in the nano-domain distribution, recovering stochastic switching, thus offering a path for in-device learning. The results illustrate the importance of studying the nanoscale physics associated with phase transitions in strongly correlated materials, even for macroscopic devices, and offer guidance for future device operation schemes.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2602.11357", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.11357", "abs": "https://arxiv.org/abs/2602.11357", "authors": ["Xiaoling Yi", "Ryan Antonio", "Yunhao Deng", "Fanchen Kong", "Joren Dumoulin", "Jun Yin", "Marian Verhelst"], "title": "A 16 nm 1.60TOPS/W High Utilization DNN Accelerator with 3D Spatial Data Reuse and Efficient Shared Memory Access", "comment": "Accepted at ISCAS 2026 (2026 IEEE International Symposium on Circuits and Systems)", "summary": "Achieving high compute utilization across a wide range of AI workloads is crucial for the efficiency of versatile DNN accelerators. This paper presents the Voltra chip and its utilization-optimised DNN accelerator architecture, which leverages 3-Dimensional (3D) spatial data reuse along with efficient and flexible shared memory access. The 3D spatial dataflow enables balanced spatial data reuse across three dimensions, improving spatial utilization by up to 2.0x compared to a conventional 2D design. Inside the shared memory access architecture, Voltra incorporates flexible data streamers that enable mixed-grained hardware data pre-fetching and dynamic memory allocation, further improving the temporal utilization by 2.12-2.94x and achieving 1.15-2.36x total latency speedup compared with the non-prefetching and separated memory architecture, respectively. Fabricated in 16nm technology, our chip achieves 1.60 TOPS/W peak system energy efficiency and 1.25 TOPS/mm2 system area efficiency, which is competitive with state-of-the-art solutions while achieving high utilization across diverse workloads.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2602.11203", "categories": ["cs.LO", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.11203", "abs": "https://arxiv.org/abs/2602.11203", "authors": ["Peter Fettke", "Wolfgang Reisig"], "title": "Compositionality of Systems and Partially Ordered Runs", "comment": "15 pages, 16 figures, submitted to PETRI NETS 2026", "summary": "In the late 1970s, C.A. Petri introduced partially ordered event occurrences (runs), then called \\emph{processes}, as the appropriate model to describe the individual evolutions of distributed systems. Here, we present a unified framework for handling Petri nets and their runs, specifically to compose and decompose them. It is shown that, for nets $M$ and $N$, the set of runs of the composed net $M \\bullet N$ equals the composition of the runs of $M$ and $N$.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2602.11907", "categories": ["cs.LO"], "pdf": "https://arxiv.org/pdf/2602.11907", "abs": "https://arxiv.org/abs/2602.11907", "authors": ["Fabian Lenke", "Stefan Milius", "Henning Urbat"], "title": "A Unified Treatment of Substitution for Presheaves, Nominal Sets, Renaming Sets, and so on", "comment": null, "summary": "Presheaves and nominal sets provide alternative abstract models of sets of syntactic objects with free and bound variables, such as lambda-terms. One distinguishing feature of the presheaf-based perspective is its elegant syntax-free characterization of substitution using a closed monoidal structure. In this paper, we introduce a corresponding closed monoidal structure on nominal sets, modeling substitution in the spirit of Fiore et al.'s substitution tensor for presheaves over finite sets. To this end, we present a general method to derive a closed monoidal structure on a category from a given action of a monoidal category on that category. We demonstrate that this method not only uniformly recovers known substitution tensors for various kinds of presheaf categories, but also yields novel notions of substitution tensor for nominal sets and their relatives, such as renaming sets. In doing so, we shed new light on different incarnations of nominal sets and (pre-)sheaf categories and establish a number of novel correspondences between them.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2602.12054", "categories": ["cs.LO", "math.LO"], "pdf": "https://arxiv.org/pdf/2602.12054", "abs": "https://arxiv.org/abs/2602.12054", "authors": ["Lide Grotenhuis", "Dani\u00ebl Otten"], "title": "Unravelling Abstract Cyclic Proofs into Proofs by Induction", "comment": "15 pages", "summary": "Cyclic proof theory breaks tradition by allowing certain infinite proofs: those that can be represented by a finite graph, while satisfying a soundness condition. We reconcile cyclic proofs with traditional finite proofs: we extend abstract cyclic proof systems with a well-founded induction principle, and transform any cyclic proof into a finite proof in the extended system. Moreover, this transformation preserves the structure of the cyclic proof.\n  Our results leverage an annotated representation of cyclic proofs, which allows us to extract induction hypotheses and to determine their introduction order. The representation is essentially a reset proof with one key modification: names must be covered in a uniform way before a reset. This innovation allows us to handle cyclic proofs where the underlying inductive sort is non-linear.\n  Our framework is general enough to cover recursive functions satisfying the size-change termination principle, which are viewed as cyclic proofs under the Curry-Howard correspondence.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2602.10619", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.10619", "abs": "https://arxiv.org/abs/2602.10619", "authors": ["Guangjing Yang", "ZhangYuan Yu", "Ziyuan Qin", "Xinyuan Song", "Huahui Yi", "Qingbo Kang", "Jun Gao", "Yiyue Li", "Chenlin Du", "Qicheng Lao"], "title": "Improving Medical Visual Reinforcement Fine-Tuning via Perception and Reasoning Augmentation", "comment": "CPAL 2026", "summary": "While recent advances in Reinforcement Fine-Tuning (RFT) have shown that rule-based reward schemes can enable effective post-training for large language models, their extension to cross-modal, vision-centric domains remains largely underexplored. This limitation is especially pronounced in the medical imaging domain, where effective performance requires both robust visual perception and structured reasoning. In this work, we address this gap by proposing VRFT-Aug, a visual reinforcement fine-tuning framework tailored for the medical domain. VRFT-Aug introduces a series of training strategies designed to augment both perception and reasoning, including prior knowledge injection, perception-driven policy refinement, medically informed reward shaping, and behavioral imitation. Together, these methods aim to stabilize and improve the RFT process.\n  Through extensive experiments across multiple medical datasets, we show that our approaches consistently outperform both standard supervised fine-tuning and RFT baselines. Moreover, we provide empirically grounded insights and practical training heuristics that can be generalized to other medical image tasks. We hope this work contributes actionable guidance and fresh inspiration for the ongoing effort to develop reliable, reasoning-capable models for high-stakes medical applications.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
