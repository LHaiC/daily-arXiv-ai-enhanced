<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 161]
- [cs.CL](#cs.CL) [Total: 63]
- [eess.SY](#eess.SY) [Total: 23]
- [cs.DC](#cs.DC) [Total: 12]
- [cs.AI](#cs.AI) [Total: 44]
- [physics.app-ph](#physics.app-ph) [Total: 3]
- [cs.LG](#cs.LG) [Total: 162]
- [cond-mat.mes-hall](#cond-mat.mes-hall) [Total: 15]
- [cs.LO](#cs.LO) [Total: 9]
- [cs.GR](#cs.GR) [Total: 6]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 28]
- [eess.SP](#eess.SP) [Total: 24]
- [cs.AR](#cs.AR) [Total: 8]
- [cs.MA](#cs.MA) [Total: 11]
- [cs.RO](#cs.RO) [Total: 45]
- [cs.ET](#cs.ET) [Total: 2]
- [cs.NE](#cs.NE) [Total: 3]
- [cs.DS](#cs.DS) [Total: 11]
- [cs.GT](#cs.GT) [Total: 11]
- [cs.SI](#cs.SI) [Total: 4]
- [quant-ph](#quant-ph) [Total: 68]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [View Invariant Learning for Vision-Language Navigation in Continuous Environments](https://arxiv.org/abs/2507.08831)
*Josh Qixuan Sun,Xiaoying Xing,Huaiyuan Weng,Chul Min Yeum,Mark Crowley*

Main category: cs.CV

TL;DR: This paper introduces VIL, a post-training strategy that makes embodied AI agents more robust to changes in camera viewpoints during navigation. VIL uses contrastive learning and a teacher-student approach to improve performance on varied viewpoint scenarios and even enhances standard navigation tasks, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing navigation policies in Vision-Language Navigation in Continuous Environments (VLNCE) are sensitive to viewpoint changes (variations in camera height and viewing angle), which alter the agent's observation. To address this, a generalized scenario V2-VLNCE (VLNCE with Varied Viewpoints) was introduced, and VIL was proposed to enhance robustness to these changes.

Method: VIL (View Invariant Learning) is a view-invariant post-training strategy that uses a contrastive learning framework to learn sparse and view-invariant features. It also incorporates a teacher-student framework for the Waypoint Predictor Module, where a view-dependent teacher model distills knowledge into a view-invariant student model. The components are jointly optimized using an end-to-end training paradigm.

Result: VIL outperforms state-of-the-art approaches on V2-VLNCE by 8-15% (measured on Success Rate) on R2R-CE and RxR-CE datasets. It also achieves state-of-the-art performance across all metrics compared to other map-free methods on the RxR-CE dataset. VIL improves performance even under the standard VLNCE setting, indicating it can be a plug-and-play method.

Conclusion: The proposed VIL method enhances the robustness of existing navigation policies to changes in camera viewpoint by employing a contrastive learning framework and a teacher-student framework for the Waypoint Predictor Module. It outperforms state-of-the-art approaches on V2-VLNCE and achieves state-of-the-art performance on RxR-CE, suggesting it can be a plug-and-play post-training method without diminishing standard viewpoint performance.

Abstract: Vision-Language Navigation in Continuous Environments (VLNCE), where an agent
follows instructions and moves freely to reach a destination, is a key research
problem in embodied AI. However, most navigation policies are sensitive to
viewpoint changes, i.e., variations in camera height and viewing angle that
alter the agent's observation. In this paper, we introduce a generalized
scenario, V2-VLNCE (VLNCE with Varied Viewpoints), and propose VIL (View
Invariant Learning), a view-invariant post-training strategy that enhances the
robustness of existing navigation policies to changes in camera viewpoint. VIL
employs a contrastive learning framework to learn sparse and view-invariant
features. Additionally, we introduce a teacher-student framework for the
Waypoint Predictor Module, a core component of most VLNCE baselines, where a
view-dependent teacher model distills knowledge into a view-invariant student
model. We employ an end-to-end training paradigm to jointly optimize these
components, thus eliminating the cost for individual module training. Empirical
results show that our method outperforms state-of-the-art approaches on
V2-VLNCE by 8-15% measured on Success Rate for two standard benchmark datasets
R2R-CE and RxR-CE. Furthermore, we evaluate VIL under the standard VLNCE
setting and find that, despite being trained for varied viewpoints, it often
still improves performance. On the more challenging RxR-CE dataset, our method
also achieved state-of-the-art performance across all metrics when compared to
other map-free methods. This suggests that adding VIL does not diminish the
standard viewpoint performance and can serve as a plug-and-play post-training
method.

</details>


### [2] [Detecting Deepfake Talking Heads from Facial Biometric Anomalies](https://arxiv.org/abs/2507.08917)
*Justin D. Norman,Hany Farid*

Main category: cs.CV

TL;DR: 一种利用面部生物特征检测深度伪造视频模仿的新型机器学习技术被提出，并在大量数据集上进行了评估。


<details>
  <summary>Details</summary>
Motivation: 深度伪造技术（结合逼真的声音克隆和具有视觉吸引力的虚拟化身、换脸或唇形同步的深度伪造视频生成）被广泛用于欺诈、诈骗和政治虚假信息。

Method: 提出了一种新颖的法证机器学习技术，利用面部生物特征中的不自然模式来检测深度伪造视频模仿。

Result: 该技术在大量的深度伪造技术和模仿的数据集上进行了评估。

Conclusion: 该技术有潜力用于检测深度伪造的视频模仿，但仍需进一步评估其在视频洗稿和对未见过视频深度伪造生成器的泛化能力方面的可靠性。

Abstract: The combination of highly realistic voice cloning, along with visually
compelling avatar, face-swap, or lip-sync deepfake video generation, makes it
relatively easy to create a video of anyone saying anything. Today, such
deepfake impersonations are often used to power frauds, scams, and political
disinformation. We propose a novel forensic machine learning technique for the
detection of deepfake video impersonations that leverages unnatural patterns in
facial biometrics. We evaluate this technique across a large dataset of
deepfake techniques and impersonations, as well as assess its reliability to
video laundering and its generalization to previously unseen video deepfake
generators.

</details>


### [3] [PRISM: Reducing Spurious Implicit Biases in Vision-Language Models with LLM-Guided Embedding Projection](https://arxiv.org/abs/2507.08979)
*Mahdiyar Molahasani,Azadeh Motamedi,Michael Greenspan,Il-Min Kim,Ali Etemad*

Main category: cs.CV

TL;DR: PRISM is a new data-free and task-agnostic solution for bias mitigation in VLMs like CLIP. It uses an LLM to generate scene descriptions with spurious correlations and a novel contrastive-style debiasing loss to learn a projection that minimizes these correlations while preserving image-text alignment. PRISM outperforms existing methods on Waterbirds and CelebA datasets.


<details>
  <summary>Details</summary>
Motivation: VLMs often inherit and amplify biases in their training data, leading to skewed predictions. PRISM is designed to debias VLMs without relying on predefined bias categories or additional external data.

Method: PRISM operates in two stages: first, an LLM is prompted with simple class prompts to generate scene descriptions that contain spurious correlations. Next, PRISM uses a novel contrastive-style debiasing loss to learn a projection that maps the embeddings onto a latent space that minimizes spurious correlations while preserving the alignment between image and text embeddings.

Result: PRISM outperforms current debiasing methods on the commonly used Waterbirds and CelebA datasets.

Conclusion: PRISM outperforms current debiasing methods on the commonly used Waterbirds and CelebA datasets.

Abstract: We introduce Projection-based Reduction of Implicit Spurious bias in
vision-language Models (PRISM), a new data-free and task-agnostic solution for
bias mitigation in VLMs like CLIP. VLMs often inherit and amplify biases in
their training data, leading to skewed predictions. PRISM is designed to debias
VLMs without relying on predefined bias categories or additional external data.
It operates in two stages: first, an LLM is prompted with simple class prompts
to generate scene descriptions that contain spurious correlations. Next, PRISM
uses our novel contrastive-style debiasing loss to learn a projection that maps
the embeddings onto a latent space that minimizes spurious correlations while
preserving the alignment between image and text embeddings.Extensive
experiments demonstrate that PRISM outperforms current debiasing methods on the
commonly used Waterbirds and CelebA datasets We make our code public at:
https://github.com/MahdiyarMM/PRISM.

</details>


### [4] [Video Inference for Human Mesh Recovery with Vision Transformer](https://arxiv.org/abs/2507.08981)
*Hanbyel Cho,Jaesung Ahn,Yooshin Cho,Junmo Kim*

Main category: cs.CV

TL;DR: A new method called HMR-ViT uses both temporal and kinematic information from videos to recover human mesh shape more accurately. It uses a special feature image and a Vision Transformer, achieving good results on standard datasets.


<details>
  <summary>Details</summary>
Motivation: Existing Human Mesh Recovery (HMR) methods have not simultaneously utilized temporal and kinematic information, despite its potential to improve accuracy. This paper addresses this gap.

Method: HMR-ViT uses a Temporal-kinematic Feature Image, constructed with feature vectors from video frames via an image encoder and a Channel Rearranging Matrix (CRM) to spatially align similar kinematic features. This feature image is then encoded using a Vision Transformer, and SMPL pose and shape parameters are inferred using a regression network.

Result: Extensive evaluations on the 3DPW and Human3.6M datasets demonstrate that HMR-ViT achieves competitive performance in Human Mesh Recovery.

Conclusion: Our proposed HMR-ViT method achieves competitive performance in Human Mesh Recovery by effectively utilizing both temporal and kinematic information.

Abstract: Human Mesh Recovery (HMR) from an image is a challenging problem because of
the inherent ambiguity of the task. Existing HMR methods utilized either
temporal information or kinematic relationships to achieve higher accuracy, but
there is no method using both. Hence, we propose "Video Inference for Human
Mesh Recovery with Vision Transformer (HMR-ViT)" that can take into account
both temporal and kinematic information. In HMR-ViT, a Temporal-kinematic
Feature Image is constructed using feature vectors obtained from video frames
by an image encoder. When generating the feature image, we use a Channel
Rearranging Matrix (CRM) so that similar kinematic features could be located
spatially close together. The feature image is then further encoded using
Vision Transformer, and the SMPL pose and shape parameters are finally inferred
using a regression network. Extensive evaluation on the 3DPW and Human3.6M
datasets indicates that our method achieves a competitive performance in HMR.

</details>


### [5] [From images to properties: a NeRF-driven framework for granular material parameter inversion](https://arxiv.org/abs/2507.09005)
*Cheng-Hsi Hsiao,Krishna Kumar*

Main category: cs.CV

TL;DR: 该研究提出了一种结合NeRF和MPM的新方法，利用视觉数据从模拟中估计沙子的摩擦角，误差在2度以内。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在从视觉观测中推断颗粒材料的属性，特别关注在无法或不便直接测量的情况下，如何有效表征这些材料。

Method: 本研究引入了一个整合了神经辐射场（NeRF）和物质点方法（MPM）模拟的新框架，用于从视觉观测中推断颗粒材料属性。首先生成合成实验数据，模拟犁与沙子的交互作用，并将实验渲染成逼真的图像作为观测。利用NeRF从初始多视图图像中重建三维几何结构，然后将重建的几何结构用于MPM模拟中物质点的初始化，其中摩擦角是未知的。通过相同的相机设置渲染模拟图像，并与观测图像进行比较。最后，利用贝叶斯优化最小化图像损失来估计最佳拟合摩擦角。

Result: 研究结果表明，通过该框架能够将摩擦角估计的误差控制在2度以内，证明了通过纯粹视觉观察进行逆向分析的有效性。

Conclusion: 通过纯粹的视觉观察，该方法能够将摩擦角估计的误差控制在2度以内，证明了通过纯粹视觉观察进行逆向分析的有效性。该方法为在无法或不便直接测量的真实场景中表征颗粒材料提供了有前景的解决方案。

Abstract: We introduce a novel framework that integrates Neural Radiance Fields (NeRF)
with Material Point Method (MPM) simulation to infer granular material
properties from visual observations. Our approach begins by generating
synthetic experimental data, simulating an plow interacting with sand. The
experiment is rendered into realistic images as the photographic observations.
These observations include multi-view images of the experiment's initial state
and time-sequenced images from two fixed cameras. Using NeRF, we reconstruct
the 3D geometry from the initial multi-view images, leveraging its capability
to synthesize novel viewpoints and capture intricate surface details. The
reconstructed geometry is then used to initialize material point positions for
the MPM simulation, where the friction angle remains unknown. We render images
of the simulation under the same camera setup and compare them to the observed
images. By employing Bayesian optimization, we minimize the image loss to
estimate the best-fitting friction angle. Our results demonstrate that friction
angle can be estimated with an error within 2 degrees, highlighting the
effectiveness of inverse analysis through purely visual observations. This
approach offers a promising solution for characterizing granular materials in
real-world scenarios where direct measurement is impractical or impossible.

</details>


### [6] [VISTA: A Visual Analytics Framework to Enhance Foundation Model-Generated Data Labels](https://arxiv.org/abs/2507.09008)
*Xiwei Xuan,Xiaoqi Wang,Wenbin He,Jorge Piazentin Ono,Liang Gou,Kwan-Liu Ma,Liu Ren*

Main category: cs.CV

TL;DR: VISTA是一个视觉分析框架，用于提升多模态基础模型生成标签的质量，以改进模型在开放词汇图像分割等任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理大规模FM生成标签的数据质量问题时存在不足，过于关注数据数量而非质量，且数据验证方法受限于指标或仅对少量数据进行人工验证。

Method: VISTA是一个视觉分析框架，通过集成多阶段数据验证策略和人类专业知识，帮助识别、理解和修正FM生成标签中的隐藏问题。

Result: VISTA在开放词汇图像分割任务中，通过提升FM生成标签的质量，有效增强了多模态模型的性能，并在定量和定性评估中均表现出优越性。

Conclusion: VISTA通过整合多阶段数据验证策略和人类专业知识，有效解决了多模态基础模型（FMs）生成标签的质量问题，尤其在开放词汇图像分割领域，显著提升了模型性能。通过在两个基准数据集上的详细案例研究和专家评审，证明了VISTA在定量和定性方面的有效性。

Abstract: The advances in multi-modal foundation models (FMs) (e.g., CLIP and LLaVA)
have facilitated the auto-labeling of large-scale datasets, enhancing model
performance in challenging downstream tasks such as open-vocabulary object
detection and segmentation. However, the quality of FM-generated labels is less
studied as existing approaches focus more on data quantity over quality. This
is because validating large volumes of data without ground truth presents a
considerable challenge in practice. Existing methods typically rely on limited
metrics to identify problematic data, lacking a comprehensive perspective, or
apply human validation to only a small data fraction, failing to address the
full spectrum of potential issues. To overcome these challenges, we introduce
VISTA, a visual analytics framework that improves data quality to enhance the
performance of multi-modal models. Targeting the complex and demanding domain
of open-vocabulary image segmentation, VISTA integrates multi-phased data
validation strategies with human expertise, enabling humans to identify,
understand, and correct hidden issues within FM-generated labels. Through
detailed use cases on two benchmark datasets and expert reviews, we demonstrate
VISTA's effectiveness from both quantitative and qualitative perspectives.

</details>


### [7] [BrainLesion Suite: A Flexible and User-Friendly Framework for Modular Brain Lesion Image Analysis](https://arxiv.org/abs/2507.09036)
*Florian Kofler,Marcel Rosier,Mehdi Astaraki,Hendrik Möller,Ilhem Isra Mekki,Josef A. Buchner,Anton Schmick,Arianna Pfiffer,Eva Oswald,Lucas Zimmer,Ezequiel de la Rosa,Sarthak Pati,Julian Canisius,Arianna Piffer,Ujjwal Baid,Mahyar Valizadeh,Akis Linardos,Jan C. Peeken,Surprosanna Shit,Felix Steinbauer,Daniel Rueckert,Rolf Heckemann,Spyridon Bakas,Jan Kirschke,Constantin von See,Ivan Ezhov,Marie Piraud,Benedikt Wiestler,Bjoern Menze*

Main category: cs.CV

TL;DR: BrainLesion Suite 是一个 Python 工具包，用于简化脑部病变图像分析流程的构建，支持预处理、模态合成、病变分割和性能量化，并可应用于其他生物医学成像任务。


<details>
  <summary>Details</summary>
Motivation: BrainLesion Suite 的开发旨在为脑部病变图像分析提供一个模块化、易于使用的 Python 工具包，以最大限度地减少认知负担，简化复杂工作流程的创建，从而促进临床和科学实践。

Method: BrainLesion Suite 采用 Pythonic 原则设计，提供了一个易于使用的开发体验，其中包括一个可适应的预处理模块，用于执行共配准、图谱配准、去骨和去面等操作。它还利用 BraTS 挑战中的算法来合成缺失的模态、修复病变并生成特定于病理的肿瘤分割。此外，它还提供像 panoptica 这样的工具来计算逐病变度量，以量化分割模型的性能。

Result: BrainLesion Suite 是一个灵活的工具包，支持从共配准、图谱配准到病变分割和性能量化的各种脑部病变图像分析任务。它通过利用 BraTS 挑战中的算法，能够合成缺失模态、修复病变和生成肿瘤分割，并提供逐病变度量以评估分割模型性能。

Conclusion: BrainLesion Suite 是一个多功能工具包，用于在 Python 中构建模块化的脑部病变图像分析流程。它允许用户通过可适应的预处理模块、利用 BraTS 挑战中的算法进行模态合成和分割，以及使用 panoptica 等工具量化分割模型性能，来简化复杂的工作流程。该工具包最初是为了分析胶质瘤、转移瘤和多发性硬化症等脑部病变而开发的，但也可应用于其他生物医学图像分析领域。

Abstract: BrainLesion Suite is a versatile toolkit for building modular brain lesion
image analysis pipelines in Python. Following Pythonic principles, BrainLesion
Suite is designed to provide a 'brainless' development experience, minimizing
cognitive effort and streamlining the creation of complex workflows for
clinical and scientific practice. At its core is an adaptable preprocessing
module that performs co-registration, atlas registration, and optional
skull-stripping and defacing on arbitrary multi-modal input images. BrainLesion
Suite leverages algorithms from the BraTS challenge to synthesize missing
modalities, inpaint lesions, and generate pathology-specific tumor
segmentations. BrainLesion Suite also enables quantifying segmentation model
performance, with tools such as panoptica to compute lesion-wise metrics.
Although BrainLesion Suite was originally developed for image analysis
pipelines of brain lesions such as glioma, metastasis, and multiple sclerosis,
it can be adapted for other biomedical image analysis applications. The
individual BrainLesion Suite packages and tutorials are accessible on GitHub.

</details>


### [8] [Can Contrastive Learning Improve Class-Imbalanced Diffusion Model?](https://arxiv.org/abs/2507.09052)
*Fang Chen,Alex Villa,Gongbo Liang,Xiaoyi Lu,Meng Tang*

Main category: cs.CV

TL;DR: 针对类别不平衡的扩散模型，提出了一种基于对比学习的框架，通过引入两种对比损失函数，有效提高了尾部类别的图像多样性，同时保持了头部类别的性能。


<details>
  <summary>Details</summary>
Motivation: 为了提高类别条件扩散模型在不平衡数据上合成的尾部类别的图像多样性，同时不损害头部类别的保真度和多样性。

Method: 通过引入两种对比损失函数来实现：1. 利用负样本的无监督InfoNCE损失，以增加合成图像之间的距离/差异性，特别是针对尾部类别。2. 最小均方误差（MSE）损失，将大步长下的条件生成与无条件生成进行对比，使去噪过程对头部类别的条件不敏感，从而通过知识共享来丰富尾部类别。

Result: 所提出的对比学习框架成功应用于类别不平衡的扩散模型，并在多个数据集上取得了优于现有方法的性能。

Conclusion: 该对比学习框架易于实现，并且在CIFAR10/100-LT、PlacesLT、TinyImageNetLT和ImageNetLT等各种数据集上，在类别不平衡的扩散模型方面优于标准的DDPM和替代方法。

Abstract: Training data for class-conditional image synthesis often exhibit a
long-tailed distribution with limited images for tail classes. Such an
imbalance causes mode collapse and reduces the diversity of synthesized images
for tail classes. For class-conditional diffusion models trained on imbalanced
data, we aim to improve the diversity of tail class images without compromising
the fidelity and diversity of head class images. We achieve this by introducing
two deceptively simple but highly effective contrastive loss functions.
Firstly, we employ an unsupervised InfoNCE loss utilizing negative samples to
increase the distance/dissimilarity among synthetic images, particularly for
tail classes. To further enhance the diversity of tail classes, our second loss
is an MSE loss that contrasts class-conditional generation with unconditional
generation at large timesteps. This second loss makes the denoising process
insensitive to class conditions for the initial steps, which enriches tail
classes through knowledge sharing from head classes. Conditional-unconditional
alignment has been shown to enhance the performance of long-tailed GAN. We are
the first to adapt such alignment to diffusion models. We successfully
leveraged contrastive learning for class-imbalanced diffusion models. Our
contrastive learning framework is easy to implement and outperforms standard
DDPM and alternative methods for class-imbalanced diffusion models across
various datasets, including CIFAR10/100-LT, PlacesLT, TinyImageNetLT, and
ImageNetLT.

</details>


### [9] [Infinite Video Understanding](https://arxiv.org/abs/2507.09068)
*Dell Zhang,Xiangyu Chen,Jixiang Luo,Mengxi Jia,Changzhi Sun,Ruilong Ren,Jingren Liu,Hao Sun,Xuelong Li*

Main category: cs.CV

TL;DR: 该论文提出“无限视频理解”作为多媒体AI的下一个前沿，旨在解决长视频处理的计算、记忆和连贯性挑战，并指出了实现这一目标的关键研究方向。


<details>
  <summary>Details</summary>
Motivation: 当前的大型语言模型（LLM）和多模态大型语言模型（MLLM）在视频理解方面取得了显著进展，但它们在处理和理解数小时以上的长视频时，仍然面临计算和内存的限制，并且难以保持长时序的连贯性、复杂事件的跟踪以及细粒度细节的保留。

Method: 论文探讨了实现无限视频理解所面临的核心挑战，并概述了关键的研究方向，包括流式架构、持久化记忆机制、分层和自适应表示、事件中心推理以及新的评估范式。

Result: 论文认为，将无限视频理解设定为一个宏伟的蓝图研究目标，能够为多媒体和更广泛的人工智能研究社区提供重要的发展方向，推动在流式架构、持久化记忆机制、分层和自适应表示、事件中心推理和新评估范式等领域的创新。

Conclusion: 该论文提出“无限视频理解”作为多媒体研究的下一个前沿领域，旨在使模型能够处理任意长度的视频数据。

Abstract: The rapid advancements in Large Language Models (LLMs) and their multimodal
extensions (MLLMs) have ushered in remarkable progress in video understanding.
However, a fundamental challenge persists: effectively processing and
comprehending video content that extends beyond minutes or hours. While recent
efforts like Video-XL-2 have demonstrated novel architectural solutions for
extreme efficiency, and advancements in positional encoding such as HoPE and
VideoRoPE++ aim to improve spatio-temporal understanding over extensive
contexts, current state-of-the-art models still encounter significant
computational and memory constraints when faced with the sheer volume of visual
tokens from lengthy sequences. Furthermore, maintaining temporal coherence,
tracking complex events, and preserving fine-grained details over extended
periods remain formidable hurdles, despite progress in agentic reasoning
systems like Deep Video Discovery. This position paper posits that a logical,
albeit ambitious, next frontier for multimedia research is Infinite Video
Understanding -- the capability for models to continuously process, understand,
and reason about video data of arbitrary, potentially never-ending duration. We
argue that framing Infinite Video Understanding as a blue-sky research
objective provides a vital north star for the multimedia, and the wider AI,
research communities, driving innovation in areas such as streaming
architectures, persistent memory mechanisms, hierarchical and adaptive
representations, event-centric reasoning, and novel evaluation paradigms.
Drawing inspiration from recent work on long/ultra-long video understanding and
several closely related fields, we outline the core challenges and key research
directions towards achieving this transformative capability.

</details>


### [10] [BlindSight: Harnessing Sparsity for Efficient VLMs](https://arxiv.org/abs/2507.09071)
*Tharun Adithya Srikrishnan,Deval Shah,Steven K. Reinhardt*

Main category: cs.CV

TL;DR: 为了解决视觉语言模型（VLM）处理长图像提示时的预填充时间过长问题，BlindSight提出了一种训练无关的稀疏性注意力方法，通过输入模板感知稀疏性掩码，在不影响准确性的情况下，平均将计算量减少了32%-41%。


<details>
  <summary>Details</summary>
Motivation: 由于视觉数据和注意力计算的二次复杂度，大型视觉语言模型（VLM）在处理长提示（prompt）时存在预填充时间（prefill duration）过长的问题。需要一种方法来缓解这一瓶颈。

Method: BlindSight通过分析视觉语言模型中的注意力模式，识别出注意力计算中的稀疏性，并将其分为仅沉没、文档掩码和混合文档-沉没掩码三类。基于这些观察，提出了一种输入模板感知（input template-aware）的注意力稀疏性掩码，用于优化VLM推理，且无需额外训练。

Result: BlindSight在Qwen2-VL、Qwen2.5-VL和Gemma-3等视觉语言模型上进行了评估，平均将计算量（FLOPs）减少了32%-41%，同时在大多数多图像理解基准测试中，准确性仅下降-2%到+2%。

Conclusion: BlindSight通过输入模板感知稀疏性掩码，在不进行训练的情况下优化了视觉语言模型（VLM）的推理过程。该方法通过识别和利用注意力计算中的稀疏性，特别是利用注意力沉没（sink）标记，显著减少了计算量，同时对模型准确性影响很小。

Abstract: Large vision-language models (VLMs) enable the joint processing of text and
images. However, the inclusion of vision data significantly expands the prompt
length. Along with the quadratic complexity of the attention computation, this
results in a longer prefill duration. An approach to mitigate this bottleneck
is to leverage the inherent sparsity in the attention computation. In our
analysis of attention patterns in VLMs, we observe that a substantial portion
of layers exhibit minimal cross-image attention, except through attention-sink
tokens per image. These sparse attention patterns fall into distinct
categories: sink-only, document mask and a hybrid document-sink mask. Based on
this, we propose BlindSight: a training-free approach to optimize VLM inference
using a input template-aware attention sparsity mask. We utilize samples from a
dataset to derive a prompt-agnostic sparsity categorization for every attention
head. We evaluate the proposed technique using VLMs such as Qwen2-VL,
Qwen2.5-VL and Gemma-3. BlindSight results in a 32%-41% reduction in FLOPs on
average with -2%-+2% accuracy compared to the original model in most evaluated
multi-image understanding benchmarks.

</details>


### [11] [From Physics to Foundation Models: A Review of AI-Driven Quantitative Remote Sensing Inversion](https://arxiv.org/abs/2507.09081)
*Zhenyu Yu,Mohd Yamani Idna Idris,Hua Wang,Pei Wang,Junyi Chen,Kun Wang*

Main category: cs.CV

TL;DR: 定量遥感反演方法正从物理模型转向机器学习和基础模型。本文回顾了这一演变，比较了不同方法的优缺点，并指出了未来发展方向，包括提高物理可解释性、域泛化能力和统一建模能力。


<details>
  <summary>Details</summary>
Motivation: 定量遥感反演旨在从卫星观测中估计地表变量（如生物量、植被指数、蒸散发等），以支持生态系统监测、碳核算和土地管理等应用。随着遥感系统和人工智能的发展，数据驱动和基础模型（FM）方法正逐渐取代传统的基于物理模型的方法。

Method: 本文采用系统性回顾的方法，比较了不同反演范式（物理模型、机器学习、基础模型）的建模假设、应用场景和局限性，并重点关注了基础模型在自监督预训练、多模态融合和跨任务适应方面的进展。

Result: 研究对从物理模型（如PROSPECT, SCOPE, DART）到机器学习方法（如深度学习、多模态融合），再到基础模型（如SatMAE, GFM, mmEarth）的反演技术方法学演变进行了系统性回顾和比较。论文强调了基础模型在自监督预训练、多模态融合和跨任务适应方面的最新进展，并指出了物理可解释性、域泛化、有限监督和不确定性量化等方面的挑战。

Conclusion: 该论文系统地回顾了定量遥感反演方法学的演变，从物理模型到机器学习，再到基础模型，并探讨了当前面临的挑战和未来发展方向。

Abstract: Quantitative remote sensing inversion aims to estimate continuous surface
variables-such as biomass, vegetation indices, and evapotranspiration-from
satellite observations, supporting applications in ecosystem monitoring, carbon
accounting, and land management. With the evolution of remote sensing systems
and artificial intelligence, traditional physics-based paradigms are giving way
to data-driven and foundation model (FM)-based approaches. This paper
systematically reviews the methodological evolution of inversion techniques,
from physical models (e.g., PROSPECT, SCOPE, DART) to machine learning methods
(e.g., deep learning, multimodal fusion), and further to foundation models
(e.g., SatMAE, GFM, mmEarth). We compare the modeling assumptions, application
scenarios, and limitations of each paradigm, with emphasis on recent FM
advances in self-supervised pretraining, multi-modal integration, and
cross-task adaptation. We also highlight persistent challenges in physical
interpretability, domain generalization, limited supervision, and uncertainty
quantification. Finally, we envision the development of next-generation
foundation models for remote sensing inversion, emphasizing unified modeling
capacity, cross-domain generalization, and physical interpretability.

</details>


### [12] [Taming generative video models for zero-shot optical flow extraction](https://arxiv.org/abs/2507.09082)
*Seungwoo Kim,Khai Loong Aw,Klemen Kotar,Cristobal Eyzaguirre,Wanhee Lee,Yunong Liu,Jared Watrous,Stefan Stojanov,Juan Carlos Niebles,Jiajun Wu,Daniel L. K. Yamins*

Main category: cs.CV

TL;DR: 通过KL-tracing，一种新的测试时程序，在不进行微调的情况下，可以从生成视频模型中提取光流，这是一种比监督或光度损失方法更有效的方法。


<details>
  <summary>Details</summary>
Motivation: 旨在探索不经微调即可提取光流的可能性，特别是由仅为预测未来帧而训练的冻结的、通用的自监督视频模型。这是因为先前的细读方法需要微调，这对于标签稀缺且模拟到真实差距阻碍合成数据集的光流来说是不切实际的。

Method: 受逆世界模型（CWM）范例的启发，我们提出了一种名为KL-tracing的测试时方法，该方法通过将局部扰动注入第一帧，然后推出模型一步并计算受扰动和未受扰动预测分布之间的KL散度来提取光流。我们发现，分布预测、因子化潜在空间和随机访问解码等模型特性有助于此过程。LRAS架构满足这些要求，并被用于KL-tracing的实现。

Result: 在不进行任何特定于光流的微调的情况下，我们的KL-tracing方法在真实世界的TAP-Vid DAVIS数据集（端点误差相对提高了16.6%）和合成TAP-Vid Kubric数据集（相对提高了4.7%）上均优于最先进的模型。

Conclusion: 我们的结果表明，可控生成视频模型通过反事实提示是一种可扩展且有效的替代方法，可用于高质量光流估计，优于监督或光度损失方法。

Abstract: Extracting optical flow from videos remains a core computer vision problem.
Motivated by the success of large general-purpose models, we ask whether frozen
self-supervised video models trained only for future frame prediction can be
prompted, without fine-tuning, to output flow. Prior work reading out depth or
illumination from video generators required fine-tuning, which is impractical
for flow where labels are scarce and synthetic datasets suffer from a
sim-to-real gap. Inspired by the Counterfactual World Model (CWM) paradigm,
which can obtain point-wise correspondences by injecting a small tracer
perturbation into a next-frame predictor and tracking its propagation, we
extend this idea to generative video models. We explore several popular
architectures and find that successful zero-shot flow extraction in this manner
is aided by three model properties: (1) distributional prediction of future
frames (avoiding blurry or noisy outputs); (2) factorized latents that treat
each spatio-temporal patch independently; and (3) random-access decoding that
can condition on any subset of future pixels. These properties are uniquely
present in the recent Local Random Access Sequence (LRAS) architecture.
Building on LRAS, we propose KL-tracing: a novel test-time procedure that
injects a localized perturbation into the first frame, rolls out the model one
step, and computes the Kullback-Leibler divergence between perturbed and
unperturbed predictive distributions. Without any flow-specific fine-tuning,
our method outperforms state-of-the-art models on real-world TAP-Vid DAVIS
dataset (16.6% relative improvement for endpoint error) and synthetic TAP-Vid
Kubric (4.7% relative improvement). Our results indicate that counterfactual
prompting of controllable generative video models is a scalable and effective
alternative to supervised or photometric-loss approaches for high-quality flow.

</details>


### [13] [MI CAM: Mutual Information Weighted Activation Mapping for Causal Visual Explanations of Convolutional Neural Networks](https://arxiv.org/abs/2507.09092)
*Ram S Iyer,Narayan S Iyer,Rugmini Ammal P*

Main category: cs.CV

TL;DR: MI CAM通过互信息加权特征图，并结合反事实分析来生成因果解释，在定性和定量指标上均表现出色。


<details>
  <summary>Details</summary>
Motivation: 随着机器视觉在医疗保健和自动化发电厂等关键日常需求中的应用，人们越来越关注卷积神经网络的内部机制及其提供特定推理的原因。

Method: MI CAM是一种新颖的、基于激活映射的、事后视觉解释方法。它通过输入图像的互信息对每个特征图进行加权来生成显着性可视化，并通过权重和激活图的线性组合生成最终结果。

Result: MI CAM通过互信息加权特征图，并结合反事实分析来生成因果解释，在定性和定量指标上均表现出色。

Conclusion: MI CAM与所有最先进的方法相媲美，并在定性和定量方面优于其中一些方法。

Abstract: With the intervention of machine vision in our crucial day to day necessities
including healthcare and automated power plants, attention has been drawn to
the internal mechanisms of convolutional neural networks, and the reason why
the network provides specific inferences. This paper proposes a novel post-hoc
visual explanation method called MI CAM based on activation mapping. Differing
from previous class activation mapping based approaches, MI CAM produces
saliency visualizations by weighing each feature map through its mutual
information with the input image and the final result is generated by a linear
combination of weights and activation maps. It also adheres to producing causal
interpretations as validated with the help of counterfactual analysis. We aim
to exhibit the visual performance and unbiased justifications for the model
inferencing procedure achieved by MI CAM. Our approach works at par with all
state-of-the-art methods but particularly outperforms some in terms of
qualitative and quantitative measures. The implementation of proposed method
can be found on https://anonymous.4open.science/r/MI-CAM-4D27

</details>


### [14] [RadEyeVideo: Enhancing general-domain Large Vision Language Model for chest X-ray analysis with video representations of eye gaze](https://arxiv.org/abs/2507.09097)
*Yunsoo Kim,Jinge Wu,Honghan Wu*

Main category: cs.CV

TL;DR: 本研究提出了RadEyeVideo方法，将放射科医生的眼动视频序列整合到大型视觉语言模型（LVLMs）中，以提高胸部X光片分析的准确性。该方法在报告生成和疾病诊断任务中均取得了显著的性能提升，并使通用模型超越了专用医疗模型，证明了眼动信息对增强模型能力的价值。


<details>
  <summary>Details</summary>
Motivation: 现有方法在整合放射科医生眼动数据到大型视觉语言模型（LVLMs）用于胸部X光片（CXR）分析时，往往忽略了眼动顺序这一重要信息。然而，眼动顺序能够提供关于医生关注区域和审视顺序的宝贵见解。

Method: 提出了一种名为RadEyeVideo的新方法，该方法将放射科医生的眼动追踪数据作为视频序列进行整合，以捕捉视线的时空动态。

Result: 通过在三种具有视频输入能力且面向通用领域的开源LVLMs上评估RadEyeVideo方法，发现在胸部X光片报告生成任务中，模型性能最高提升了24.6%；在报告生成和疾病诊断两项任务的综合评估中，平均性能提升了15.2%。此外，整合了RadEyeVideo方法的LLaVA-OneVision模型在性能上超越了专门针对大规模胸部X光片数据训练的特定医疗模型MAIRA-2和CheXagent。

Conclusion: 本研究提出的RadEyeVideo方法，通过将放射科医生的眼动追踪数据以视频序列的形式整合进大型视觉语言模型（LVLMs），能够有效捕捉医生视线的时空动态。实验结果表明，该方法在胸部X光片报告生成和疾病诊断任务中均能显著提升模型性能，并在报告生成任务中最高提升24.6%，在两项任务综合评估中平均提升15.2%。值得注意的是，RadEyeVideo能够使通用领域模型LLaVA-OneVision的表现超越专门为医疗领域训练的特定模型（如MAIRA-2和CheXagent），证明了领域专家知识（如眼动信息）与通用LVLMs的有效结合，能大幅增强通用模型在临床任务中的能力。RadEyeVideo是实现以人为本、可扩展的医学影像分析方法的重要一步。

Abstract: Large Vision-Language Models (LVLMs) have demonstrated promising performance
in chest X-ray (CXR) analysis. To enhance human-computer interaction, several
studies have incorporated radiologists' eye gaze, typically through heatmaps or
textual prompts. However, these methods often overlook the sequential order of
eye movements, which could provide valuable insights by highlighting both the
areas of interest and the order in which they are examined. In this work, we
propose a novel approach called RadEyeVideo that integrates radiologists'
eye-fixation data as a video sequence, capturing both the temporal and spatial
dynamics of their gaze. We evaluate this method in CXR report generation and
disease diagnosis using three general-domain, open-source LVLMs with video
input capabilities. When prompted with eye-gaze videos, model performance
improves by up to 24.6% in the report generation task and on average 15.2% for
both tasks using scaled evaluation metrics. Notably, RadEyeVideo enhanced an
open-domain LVLM model, LLaVA-OneVision, to surpass task-specific medical LVLMs
such as MAIRA-2 and CheXagent, trained on large Chest X-ray data. This work
highlights that domain expert's knowledge (eye-gaze information in this case),
when effectively integrated with LVLMs, can significantly enhance
general-domain models' capabilities in clinical tasks. RadEyeVideo is a step
toward a scalable human-centered approach of utilizing LVLMs in medical image
analytics.

</details>


### [15] [Harnessing Text-to-Image Diffusion Models for Point Cloud Self-Supervised Learning](https://arxiv.org/abs/2507.09102)
*Yiyang Chen,Shanshan Zhao,Lunhao Duan,Changxing Ding,Dacheng Tao*

Main category: cs.CV

TL;DR: 通过将Stable Diffusion模型应用于3D点云的自监督学习，PointSD框架成功提升了3D表示的学习效果。


<details>
  <summary>Details</summary>
Motivation: 现有3D扩散模型在处理有限的3D数据集时性能受限，而文本到图像扩散模型（如Stable Diffusion）在大型数据集上训练，具有强大的鲁棒性，有望克服这些局限性。

Method: PointSD框架首先将文本到图像扩散模型的文本编码器替换为3D编码器，训练点到图像扩散模型，使点云能够指导渲染的噪声图像的去噪。然后，利用无噪声图像作为输入，点云作为条件来提取Stable Diffusion特征。最后，通过将3D骨干网络的特征与这些Stable Diffusion特征对齐来训练3D骨干网络，从而实现直接的语义学习。

Result: 实验结果和消融研究表明，Stable Diffusion模型能够提升点云的自监督学习能力，在下游点云任务上表现出色。

Conclusion: 提出的PointSD框架利用预训练的文本到图像扩散模型Stable Diffusion来增强3D表示学习，并通过实验证明其有效性。

Abstract: Diffusion-based models, widely used in text-to-image generation, have proven
effective in 2D representation learning. Recently, this framework has been
extended to 3D self-supervised learning by constructing a conditional point
generator for enhancing 3D representations. However, its performance remains
constrained by the 3D diffusion model, which is trained on the available 3D
datasets with limited size. We hypothesize that the robust capabilities of
text-to-image diffusion models, particularly Stable Diffusion (SD), which is
trained on large-scale datasets, can help overcome these limitations. To
investigate this hypothesis, we propose PointSD, a framework that leverages the
SD model for 3D self-supervised learning. By replacing the SD model's text
encoder with a 3D encoder, we train a point-to-image diffusion model that
allows point clouds to guide the denoising of rendered noisy images. With the
trained point-to-image diffusion model, we use noise-free images as the input
and point clouds as the condition to extract SD features. Next, we train a 3D
backbone by aligning its features with these SD features, thereby facilitating
direct semantic learning. Comprehensive experiments on downstream point cloud
tasks and ablation studies demonstrate that the SD model can enhance point
cloud self-supervised learning. Code is publicly available at
https://github.com/wdttt/PointSD.

</details>


### [16] [Hybrid Autoregressive-Diffusion Model for Real-Time Streaming Sign Language Production](https://arxiv.org/abs/2507.09105)
*Maoxiao Ye,Xinfeng Ye,Mano Manoharan*

Main category: cs.CV

TL;DR: 为了解决现有自回归模型误差累积和扩散模型不适用于实时任务的问题，本文首次将自回归模型和扩散模型结合起来用于手语生成，并通过多尺度姿态表示、多尺度融合和置信度感知因果注意力等机制提高了生成质量和效率。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有自回归模型在推理时误差累积以及扩散模型不适用于实时任务的问题，首次将混合方法应用于手语生成。

Method: 结合了自回归和扩散模型，并设计了多尺度姿态表示模块、多尺度融合模块以及置信度感知因果注意力机制。

Result: 所提出的方法在生成质量和实时流式传输效率方面均表现出色。

Conclusion: 该混合方法在PHOENIX14T和How2Sign数据集上展示了其在生成质量和实时流式传输效率方面的有效性。

Abstract: Earlier Sign Language Production (SLP) models typically relied on
autoregressive methods that generate output tokens one by one, which inherently
provide temporal alignment. Although techniques like Teacher Forcing can
prevent model collapse during training, they still cannot solve the problem of
error accumulation during inference, since ground truth is unavailable at that
stage. In contrast, more recent approaches based on diffusion models leverage
step-by-step denoising to enable high-quality generation. However, the
iterative nature of these models and the requirement to denoise entire
sequences limit their applicability in real-time tasks like SLP. To address it,
we apply a hybrid approach combining autoregressive and diffusion models to SLP
for the first time, leveraging the strengths of both models in sequential
dependency modeling and output refinement. To capture fine-grained body
movements, we design a Multi-Scale Pose Representation module that separately
extracts detailed features from distinct articulators and integrates them via a
Multi-Scale Fusion module. Furthermore, we introduce a Confidence-Aware Causal
Attention mechanism that utilizes joint-level confidence scores to dynamically
guide the pose generation process, improving accuracy and robustness. Extensive
experiments on the PHOENIX14T and How2Sign datasets demonstrate the
effectiveness of our method in both generation quality and real-time streaming
efficiency.

</details>


### [17] [RoHOI: Robustness Benchmark for Human-Object Interaction Detection](https://arxiv.org/abs/2507.09111)
*Di Wen,Kunyu Peng,Kailun Yang,Yufan Chen,Ruiping Liu,Junwei Zheng,Alina Roitberg,Rainer Stiefelhagen*

Main category: cs.CV

TL;DR: 提出RoHOI基准测试和SAMPL策略，以提高HOI检测在真实世界条件下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有HOI检测模型在真实世界条件下由于未预见的损坏（如环境变化、遮挡和噪声）而性能下降，为了解决这个问题，引入了首个HOI检测鲁棒性基准测试。

Method: 提出了一种名为“语义感知掩码渐进学习”（SAMPL）的策略，以指导模型基于整体和局部线索进行优化，并动态调整优化过程以增强鲁棒性特征学习。

Result: 所提出的SAMPL策略在HOI检测鲁棒性方面取得了显著的性能提升，超过了最先进的方法。

Conclusion: 提出的SAMPL策略在HOI检测鲁棒性方面表现优于现有方法，为鲁棒HOI检测树立了新标准。

Abstract: Human-Object Interaction (HOI) detection is crucial for robot-human
assistance, enabling context-aware support. However, models trained on clean
datasets degrade in real-world conditions due to unforeseen corruptions,
leading to inaccurate prediction. To address this, we introduce the first
robustness benchmark for HOI detection, evaluating model resilience under
diverse challenges. Despite advances, current models struggle with
environmental variability, occlusion, and noise. Our benchmark, RoHOI, includes
20 corruption types based on HICO-DET and V-COCO datasets and a new
robustness-focused metric. We systematically analyze existing models in the
related field, revealing significant performance drops under corruptions. To
improve robustness, we propose a Semantic-Aware Masking-based Progressive
Learning (SAMPL) strategy to guide the model to be optimized based on holistic
and partial cues, dynamically adjusting the model's optimization to enhance
robust feature learning. Extensive experiments show our approach outperforms
state-of-the-art methods, setting a new standard for robust HOI detection.
Benchmarks, datasets, and code will be made publicly available at
https://github.com/Kratos-Wen/RoHOI.

</details>


### [18] [Mind the Gap: Preserving and Compensating for the Modality Gap in CLIP-Based Continual Learning](https://arxiv.org/abs/2507.09118)
*Linlan Huang,Xusheng Cao,Haori Lu,Yifan Meng,Fei Yang,Xialei Liu*

Main category: cs.CV

TL;DR: MG-CLIP是一种新的方法，通过管理CLIP中的模态间隙来改善持续学习，提高了对新旧任务的性能。


<details>
  <summary>Details</summary>
Motivation: 大多数现有工作忽略了CLIP中固有的模态间隙，这是其泛化和适应性的关键因素。在本文中，我们分析了微调视觉-语言预训练模型过程中模态间隙的变化。

Method: 提出了一种名为MG-CLIP的简单有效的方法，该方法利用模态间隙保持来减轻遗忘，并利用模态间隙补偿来增强新数据容量。

Result: 实验表明，MG-CLIP在类别增量学习任务中表现优于现有方法，同时无需额外回放数据。

Conclusion: MG-CLIP通过保持和补偿模态间隙来提高CLIP在类别增量学习中的性能，无需额外回放数据，并在多个基准测试中表现优于现有方法。

Abstract: Continual learning aims to enable models to learn sequentially from
continuously incoming data while retaining performance on previously learned
tasks. With the Contrastive Language-Image Pre-trained model (CLIP) exhibiting
strong capabilities across various downstream tasks, there has been growing
interest in leveraging CLIP for continual learning in such scenarios. Most
existing works overlook the inherent modality gap in CLIP, a key factor in its
generalization and adaptability. In this paper, we analyze the variations in
the modality gap during the fine-tuning of vision-language pre-trained models.
Our observations reveal that the modality gap effectively reflects the extent
to which pre-trained knowledge is preserved. Based on these insights, we
propose a simple yet effective method, MG-CLIP, that improves CLIP's
performance in class-incremental learning. Our approach leverages modality gap
preservation to mitigate forgetting and modality gap compensation to enhance
the capacity for new data, introducing a novel modality-gap-based perspective
for continual learning. Extensive experiments on multiple benchmarks
demonstrate that our method outperforms existing approaches without requiring
additional replay data. Our code is available at
https://github.com/linlany/MindtheGap.

</details>


### [19] [SnapMoGen: Human Motion Generation from Expressive Texts](https://arxiv.org/abs/2507.09122)
*Chuan Guo,Inwoo Hwang,Jian Wang,Bing Zhou*

Main category: cs.CV

TL;DR: SnapMoGen：一个新数据集，MoMask++：一个新模型，均在文本到运动生成方面取得了SOTA成果。


<details>
  <summary>Details</summary>
Motivation: 当前文本到运动生成方法受限于数据集，难以实现精细控制和泛化到未见过的提示，因此需要更高质量、更丰富的运动数据集和更强大的生成模型。

Method: 提出了一种名为 MoMask++ 的新模型，该模型将运动转换为多尺度标记序列，并使用单一生成掩码 Transformer 来生成所有标记，从而更好地利用标记容量。

Result: MoMask++ 在 HumanML3D 和 SnapMoGen 基准上都取得了最先进的性能，并展示了处理非正式用户提示的能力。

Conclusion: SnapMoGen 是一个包含高质量运动捕捉数据和详细文本描述的新型数据集，可以促进长期运动生成和融合的研究。MoMask++ 模型在 HumanML3D 和 SnapMoGen 基准上都达到了最先进的性能，并能通过大语言模型处理非正式的用户提示。

Abstract: Text-to-motion generation has experienced remarkable progress in recent
years. However, current approaches remain limited to synthesizing motion from
short or general text prompts, primarily due to dataset constraints. This
limitation undermines fine-grained controllability and generalization to unseen
prompts. In this paper, we introduce SnapMoGen, a new text-motion dataset
featuring high-quality motion capture data paired with accurate, expressive
textual annotations. The dataset comprises 20K motion clips totaling 44 hours,
accompanied by 122K detailed textual descriptions averaging 48 words per
description (vs. 12 words of HumanML3D). Importantly, these motion clips
preserve original temporal continuity as they were in long sequences,
facilitating research in long-term motion generation and blending. We also
improve upon previous generative masked modeling approaches. Our model,
MoMask++, transforms motion into multi-scale token sequences that better
exploit the token capacity, and learns to generate all tokens using a single
generative masked transformer. MoMask++ achieves state-of-the-art performance
on both HumanML3D and SnapMoGen benchmarks. Additionally, we demonstrate the
ability to process casual user prompts by employing an LLM to reformat inputs
to align with the expressivity and narration style of SnapMoGen. Project
webpage: https://snap-research.github.io/SnapMoGen/

</details>


### [20] [PoseLLM: Enhancing Language-Guided Human Pose Estimation with MLP Alignment](https://arxiv.org/abs/2507.09139)
*Dewen Zhang,Tahir Hussain,Wangpeng An,Hayaru Shouno*

Main category: cs.CV

TL;DR: PoseLLM通过使用非线性MLP连接器改进了语言引导的姿态估计，在精度和泛化能力上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有方法在处理新颖姿势或未见关键点时的泛化能力受限问题，以及LocLLM的线性投影仪无法捕捉复杂的空间-文本交互以实现高精度定位的不足。

Method: 提出了一种名为PoseLLM的框架，该框架使用两层MLP和GELU激活函数作为非线性连接器，以增强视觉和文本特征的融合。

Result: 在COCO数据集上，PoseLLM实现了77.8 AP的性能，比LocLLM提高了+0.4 AP，并在Human-Art和MPII数据集上展现了强大的零样本泛化能力。

Conclusion: PoseLLM是一个基于LLM的姿态估计框架，通过使用非线性MLP视觉-语言连接器替代线性投影仪，实现了高精度的定位，同时保持了对新姿势的泛化能力。

Abstract: Human pose estimation traditionally relies on architectures that encode
keypoint priors, limiting their generalization to novel poses or unseen
keypoints. Recent language-guided approaches like LocLLM reformulate keypoint
localization as a vision-language task, enabling zero-shot generalization
through textual descriptions. However, LocLLM's linear projector fails to
capture complex spatial-textual interactions critical for high-precision
localization. To address this, we propose PoseLLM, the first Large Language
Model (LLM)-based pose estimation framework that replaces the linear projector
with a nonlinear MLP vision-language connector. This lightweight two-layer MLP
with GELU activation enables hierarchical cross-modal feature transformation,
enhancing the fusion of visual patches and textual keypoint descriptions.
Trained exclusively on COCO data, PoseLLM achieves 77.8 AP on the COCO
validation set, outperforming LocLLM by +0.4 AP, while maintaining strong
zero-shot generalization on Human-Art and MPII. Our work demonstrates that a
simple yet powerful nonlinear connector significantly boosts localization
accuracy without sacrificing generalization, advancing the state-of-the-art in
language-guided pose estimation. Code is available at
https://github.com/Ody-trek/PoseLLM.

</details>


### [21] [$I^{2}$-World: Intra-Inter Tokenization for Efficient Dynamic 4D Scene Forecasting](https://arxiv.org/abs/2507.09144)
*Zhimin Liao,Ping Wei,Ruijie Zhang,Shuaijia Chen,Haoxuan Wang,Ziyang Ren*

Main category: cs.CV

TL;DR: I^2-World efficiently forecasts 4D occupancy using a novel dual tokenizer approach (intra-scene and inter-scene) within an encoder-decoder framework, achieving state-of-the-art results and real-time performance for autonomous driving.


<details>
  <summary>Details</summary>
Motivation: Forecasting the evolution of 3D scenes and generating unseen scenarios using occupancy-based world models has significant potential for addressing corner cases in autonomous driving systems. However, efficiently tokenizing complex 3D scenes is a critical challenge for 3D world models.

Method: I^2-World proposes an efficient framework for 4D occupancy forecasting by decoupling scene tokenization into intra-scene and inter-scene tokenizers. The intra-scene tokenizer uses a multi-scale residual quantization strategy for hierarchical compression of 3D scenes, preserving spatial details. The inter-scene tokenizer residually aggregates temporal dependencies across timesteps. It employs an encoder-decoder architecture where the encoder aggregates spatial context and predicts a transformation matrix for scene generation control, and the decoder uses this matrix and historical tokens for temporal consistency during generation.

Result: Experiments demonstrate that I^2-World achieves state-of-the-art performance, outperforming existing methods by 25.1% in mIoU and 36.9% in IoU for 4D occupancy forecasting. It also shows exceptional computational efficiency, requiring only 2.9 GB of training memory and achieving real-time inference at 37.0 FPS.

Conclusion: I^2-World in 4D occupancy forecasting achieves state-of-the-art performance, outperforming existing methods by 25.1% in mIoU and 36.9% in IoU, while exhibiting exceptional computational efficiency with 2.9 GB of training memory and 37.0 FPS real-time inference.

Abstract: Forecasting the evolution of 3D scenes and generating unseen scenarios via
occupancy-based world models offers substantial potential for addressing corner
cases in autonomous driving systems. While tokenization has revolutionized
image and video generation, efficiently tokenizing complex 3D scenes remains a
critical challenge for 3D world models. To address this, we propose
$I^{2}$-World, an efficient framework for 4D occupancy forecasting. Our method
decouples scene tokenization into intra-scene and inter-scene tokenizers. The
intra-scene tokenizer employs a multi-scale residual quantization strategy to
hierarchically compress 3D scenes while preserving spatial details. The
inter-scene tokenizer residually aggregates temporal dependencies across
timesteps. This dual design preserves the compactness of 3D tokenizers while
retaining the dynamic expressiveness of 4D tokenizers. Unlike decoder-only
GPT-style autoregressive models, $I^{2}$-World adopts an encoder-decoder
architecture. The encoder aggregates spatial context from the current scene and
predicts a transformation matrix to enable high-level control over scene
generation. The decoder, conditioned on this matrix and historical tokens,
ensures temporal consistency during generation. Experiments demonstrate that
$I^{2}$-World achieves state-of-the-art performance, outperforming existing
methods by 25.1\% in mIoU and 36.9\% in IoU for 4D occupancy forecasting while
exhibiting exceptional computational efficiency: it requires merely 2.9 GB of
training memory and achieves real-time inference at 37.0 FPS. Our code is
available on https://github.com/lzzzzzm/II-World.

</details>


### [22] [Stable Score Distillation](https://arxiv.org/abs/2507.09168)
*Haiming Zhu,Yangyang Xu,Chenshu Xu,Tingrui Shen,Wenxi Liu,Yong Du,Jun Yu,Shengfeng He*

Main category: cs.CV

TL;DR: SSD框架通过使用单一分类器和负文本/提示增强分支，解决了现有文本引导编辑方法在稳定性、空间控制和编辑强度方面的问题，实现了更优的效果和更低的复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有的文本引导图像和3D编辑方法（如Delta Denoising Score）在稳定性、空间控制和编辑强度方面存在不足，这主要是由于依赖复杂的辅助结构，引入了冲突的优化信号，并限制了精确的局部编辑。

Method: SSD框架通过将单个分类器锚定到源提示上来增强编辑过程中的稳定性和对齐性。具体而言，SSD利用无分类器指导（CFG）方程实现跨提示对齐，并引入了一个恒定的负文本分支来稳定优化过程。此外，SSD还包含一个提示增强分支，用于提升编辑强度，特别是在风格转换方面。

Result: SSD方法在2D和3D编辑任务（包括NeRF和文本驱动的风格编辑）中取得了最先进的结果，编辑过程更稳定、对齐性更好、收敛更快、复杂度更低。

Conclusion: SSD方法在2D和3D编辑任务中取得了最先进的结果，包括NeRF和文本驱动的风格编辑，同时实现了更快的收敛速度和更低的复杂性，为文本引导编辑提供了一个健壮且高效的解决方案。

Abstract: Text-guided image and 3D editing have advanced with diffusion-based models,
yet methods like Delta Denoising Score often struggle with stability, spatial
control, and editing strength. These limitations stem from reliance on complex
auxiliary structures, which introduce conflicting optimization signals and
restrict precise, localized edits. We introduce Stable Score Distillation
(SSD), a streamlined framework that enhances stability and alignment in the
editing process by anchoring a single classifier to the source prompt.
Specifically, SSD utilizes Classifier-Free Guidance (CFG) equation to achieves
cross-prompt alignment, and introduces a constant term null-text branch to
stabilize the optimization process. This approach preserves the original
content's structure and ensures that editing trajectories are closely aligned
with the source prompt, enabling smooth, prompt-specific modifications while
maintaining coherence in surrounding regions. Additionally, SSD incorporates a
prompt enhancement branch to boost editing strength, particularly for style
transformations. Our method achieves state-of-the-art results in 2D and 3D
editing tasks, including NeRF and text-driven style edits, with faster
convergence and reduced complexity, providing a robust and efficient solution
for text-guided editing.

</details>


### [23] [Learning and Transferring Better with Depth Information in Visual Reinforcement Learning](https://arxiv.org/abs/2507.09180)
*Zichun Xu,Yuntao Li,Zhaomin Wang,Lei Zhuang,Guocai Yang,Jingdong Zhao*

Main category: cs.CV

TL;DR: 该研究提出了一种融合RGB和深度信息的视觉Transformer模型，通过对比学习和课程学习提高了泛化性和样本效率。


<details>
  <summary>Details</summary>
Motivation: 深度信息对场景外观变化具有鲁棒性，并包含3D空间细节，RGB-D融合可以增强泛化能力。

Method: 使用单独的CNN处理不同的模态，并将卷积特征融合到可扩展的视觉Transformer中以获得视觉表示。设计了对比式无监督学习方案和课程学习策略。

Result: 提出了一种基于视觉Transformer的融合RGB和深度信息的视觉骨干网，并通过对比式无监督学习和课程学习策略提高了泛化能力和样本效率。

Conclusion: 该研究提出了一个融合RGB和深度信息的视觉骨干网，以增强泛化能力，并设计了一种对比式无监督学习方案来提高样本效率，同时还开发了一种灵活的课程学习策略来解决sim2real迁移问题。

Abstract: Depth information is robust to scene appearance variations and inherently
carries 3D spatial details. In this paper, a visual backbone based on the
vision transformer is proposed to fuse RGB and depth modalities for enhancing
generalization. Different modalities are first processed by separate CNN stems,
and the combined convolutional features are delivered to the scalable vision
transformer to obtain visual representations. Moreover, a contrastive
unsupervised learning scheme is designed with masked and unmasked tokens to
accelerate the sample efficiency during the reinforcement learning progress.
For sim2real transfer, a flexible curriculum learning schedule is developed to
deploy domain randomization over training processes.

</details>


### [24] [Revisiting Pool-based Prompt Learning for Few-shot Class-incremental Learning](https://arxiv.org/abs/2507.09183)
*Yongwei Jiang,Yixiong Zou,Yuhua Li,Ruixuan Li*

Main category: cs.CV

TL;DR: 本研究首次探讨了提示池方法在少样本增量学习（FSCIL）中的应用，发现现有方法存在性能下降问题，主要是由于令牌维度饱和导致模型过拟合。为此，提出了一种新的LGSP-Prompt方法，将提示学习从令牌维度转移到空间维度，通过结合局部空间特征和全局频域表示来生成空间提示，并利用动态提示选择机制来优化性能。实验结果表明，LGSP-Prompt在多个FSCIL基准测试中均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 为了解决少样本增量学习（FSCIL）中数据稀缺和增量学习的双重挑战。现有基于池的提示方法在FSCIL设置中的有效性尚未得到探索，并且会遇到令牌维度饱和导致的性能下降问题。

Method: 提出了一种名为LGSP-Prompt（局部-全局空间提示）的新方法，将基于池的提示学习从令牌维度转移到空间维度。LGSP-Prompt通过协同结合局部空间特征和全局频域表示来生成空间提示，以突出输入图像中的关键模式。构建了两个空间提示池，实现了动态提示选择，以在有效学习新会话的同时保持已获得的知识。

Result: 通过全面的分析，发现性能下降源于令牌维度饱和：在数据有限的情况下，过多的提示会竞争与任务相关的信息，导致模型过拟合。提出的LGSP-Prompt方法通过转移到空间维度来解决这个问题。

Conclusion: LGSP-Prompt在多个少样本增量学习基准测试中取得了最先进的性能，在基础知识保留和增量学习方面均显示出显著优势。

Abstract: Few-Shot Class-Incremental Learning (FSCIL) faces dual challenges of data
scarcity and incremental learning in real-world scenarios. While pool-based
prompting methods have demonstrated success in traditional incremental
learning, their effectiveness in FSCIL settings remains unexplored. This paper
presents the first study of current prompt pool methods in FSCIL tasks,
revealing an unanticipated performance degradation in incremental sessions.
Through comprehensive analysis, we identify that this phenomenon stems from
token-dimension saturation: with limited data, excessive prompts compete for
task-relevant information, leading to model overfitting. Based on this finding,
we propose LGSP-Prompt (Local-Global Spatial Prompting), which innovatively
shifts pool-based prompt learning from the token dimension to the spatial
dimension. LGSP-Prompt generates spatial prompts by synergistically combining
local spatial features and global frequency-domain representations to highlight
key patterns in input images. We construct two spatial prompt pools enabling
dynamic prompt selection to maintain acquired knowledge while effectively
learning novel sessions. Extensive experiments demonstrate that our approach
achieves state-of-the-art performance across multiple FSCIL benchmarks, showing
significant advantages in both base knowledge preservation and incremental
learning. Our implementation is available at
https://github.com/Jywsuperman/LGSP.

</details>


### [25] [MCA-LLaVA: Manhattan Causal Attention for Reducing Hallucination in Large Vision-Language Models](https://arxiv.org/abs/2507.09184)
*Qiyan Zhao,Xiaofeng Zhang,Yiheng Li,Yun Xing,Xiaosong Yuan,Feilong Tang,Sinan Fan,Xuhang Chen,Xuyao Zhang,Dahan Wang*

Main category: cs.CV

TL;DR: 文章发现了LVLMs中的旋转位置编码长尾衰减会导致图像-指令对齐偏差，进而引发幻觉。提出了一种名为MCA-LLaVA的新方法，通过二维多向空间衰减来解决这个问题，并在实验中证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: 文章旨在解决大型视觉语言模型（LVLMs）中由旋转位置编码（RoPE）的长尾衰减引起的幻觉问题，该问题导致模型在感知图像不同区域时存在偏差，影响了多模态对齐。

Method: 提出了一种名为MCA-LLaVA的改进方法，该方法基于曼哈顿距离，将一维的RoPE长尾衰减扩展到二维多向空间衰减，以解决LVLMs中由位置编码引起的图像-指令对齐偏差问题。

Result: 实验结果表明，MCA-LLaVA在多个基准测试中有效且具有通用性，能够缓解图像对齐偏差，从而减少幻觉，提高模型的性能。

Conclusion: MCA-LLaVA通过引入基于曼哈顿距离的二维多向空间衰减，有效缓解了长尾衰减导致的图像-指令对齐偏差，提高了LVLMs在处理多模态信息时的准确性，并减少了幻觉现象。实验结果证明了其有效性和通用性。

Abstract: Hallucinations pose a significant challenge in Large Vision Language Models
(LVLMs), with misalignment between multimodal features identified as a key
contributing factor. This paper reveals the negative impact of the long-term
decay in Rotary Position Encoding (RoPE), used for positional modeling in
LVLMs, on multimodal alignment. Concretely, under long-term decay, instruction
tokens exhibit uneven perception of image tokens located at different positions
within the two-dimensional space: prioritizing image tokens from the
bottom-right region since in the one-dimensional sequence, these tokens are
positionally closer to the instruction tokens. This biased perception leads to
insufficient image-instruction interaction and suboptimal multimodal alignment.
We refer to this phenomenon as image alignment bias. To enhance instruction's
perception of image tokens at different spatial locations, we propose
MCA-LLaVA, based on Manhattan distance, which extends the long-term decay to a
two-dimensional, multi-directional spatial decay. MCA-LLaVA integrates the
one-dimensional sequence order and two-dimensional spatial position of image
tokens for positional modeling, mitigating hallucinations by alleviating image
alignment bias. Experimental results of MCA-LLaVA across various hallucination
and general benchmarks demonstrate its effectiveness and generality. The code
can be accessed in https://github.com/ErikZ719/MCA-LLaVA.

</details>


### [26] [THYME: Temporal Hierarchical-Cyclic Interactivity Modeling for Video Scene Graphs in Aerial Footage](https://arxiv.org/abs/2507.09200)
*Trong-Thuan Nguyen,Pha Nguyen,Jackson Cothren,Alper Yilmaz,Minh-Triet Tran,Khoa Luu*

Main category: cs.CV

TL;DR: 提出THYME方法和AeroEye-v1.0数据集以改进视频场景图生成，并在实验中取得了更好的效果。


<details>
  <summary>Details</summary>
Motivation: 为了满足自动驾驶、监控和体育分析等应用中对动态场景理解的迫切需求，并克服现有视频场景图生成方法在表示碎片化、空间细节和时间依赖性捕捉方面的局限性。

Method: 提出了一种名为THYME（Temporal Hierarchical Cyclic Scene Graph）的方法，该方法结合了分层特征聚合和循环时间细化，以解决现有视频场景图生成方法在捕捉细粒度空间细节和长时序依赖性方面的不足。同时，发布了一个新的航空视频数据集AeroEye-v1.0，包含五种交互类型。

Result: THYME方法在ASPIRe和AeroEye-v1.0数据集上的实验表明，其性能优于现有最先进的方法。

Conclusion: THYME方法在地面和空中场景中均优于现有方法，提高了场景理解能力。

Abstract: The rapid proliferation of video in applications such as autonomous driving,
surveillance, and sports analytics necessitates robust methods for dynamic
scene understanding. Despite advances in static scene graph generation and
early attempts at video scene graph generation, previous methods often suffer
from fragmented representations, failing to capture fine-grained spatial
details and long-range temporal dependencies simultaneously. To address these
limitations, we introduce the Temporal Hierarchical Cyclic Scene Graph (THYME)
approach, which synergistically integrates hierarchical feature aggregation
with cyclic temporal refinement to address these limitations. In particular,
THYME effectively models multi-scale spatial context and enforces temporal
consistency across frames, yielding more accurate and coherent scene graphs. In
addition, we present AeroEye-v1.0, a novel aerial video dataset enriched with
five types of interactivity that overcome the constraints of existing datasets
and provide a comprehensive benchmark for dynamic scene graph generation.
Empirically, extensive experiments on ASPIRe and AeroEye-v1.0 demonstrate that
the proposed THYME approach outperforms state-of-the-art methods, offering
improved scene understanding in ground-view and aerial scenarios.

</details>


### [27] [Visual Surface Wave Elastography: Revealing Subsurface Physical Properties via Visible Surface Waves](https://arxiv.org/abs/2507.09207)
*Alexander C. Ogren,Berthy T. Feng,Jihoon Ahn,Katherine L. Bouman,Chiara Daraio*

Main category: cs.CV

TL;DR: 该方法通过分析表面波视频来推断材料的厚度和刚度，可用于健康监测和人机交互。


<details>
  <summary>Details</summary>
Motivation: 提出一种仅从表面波的视频中推断结构厚度和刚度的方法。

Method: 通过提取视频中的频散关系，然后解决基于物理学的优化问题来寻找最佳拟合的厚度和刚度参数。

Result: 在模拟和真实数据上都验证了该方法，结果与实际测量结果高度一致。

Conclusion: 该技术为居家健康监测提供了概念验证，可用于监测具有医学信息意义的组织特性，还可应用于人机交互等领域。

Abstract: Wave propagation on the surface of a material contains information about
physical properties beneath its surface. We propose a method for inferring the
thickness and stiffness of a structure from just a video of waves on its
surface. Our method works by extracting a dispersion relation from the video
and then solving a physics-based optimization problem to find the best-fitting
thickness and stiffness parameters. We validate our method on both simulated
and real data, in both cases showing strong agreement with ground-truth
measurements. Our technique provides a proof-of-concept for at-home health
monitoring of medically-informative tissue properties, and it is further
applicable to fields such as human-computer interaction.

</details>


### [28] [Uncertainty-Driven Expert Control: Enhancing the Reliability of Medical Vision-Language Models](https://arxiv.org/abs/2507.09209)
*Xiao Liang,Di Wang,Zhicheng Jiao,Ronghan Li,Pengfei Yang,Quan Wang,Tat-Seng Chua*

Main category: cs.CV

TL;DR: 通过引入“专家在环”框架Expert-CFG，无需额外训练即可提升MedVLM的准确性和临床对齐度，优于现有大型模型，适用于资源受限场景。


<details>
  <summary>Details</summary>
Motivation: 现有医学视觉语言模型（MedVLM）存在不确定性问题，可能产生错误或未经证实的响应，这在医疗应用中后果严重。现有的改进方法（如调整模型结构、高质量数据微调或偏好微调）成本高昂且与临床专业知识的对齐不足。

Method: 提出了一种名为Expert-CFG的专家在环框架，结合不确定性估计、参考文献检索、专家标注和无分类器指导（Classifier-Free Guidance, CFG）来优化MedVLM。

Result: Expert-CFG框架（4.2B参数，有限专家标注）在三个医学视觉问答基准测试中，超越了参数量更大的（13B）最先进模型，证明了该方法在资源有限情况下的可行性。

Conclusion: 该研究提出了Expert-CFG框架，一个无需额外训练即可将医学视觉语言模型（MedVLM）与临床专业知识对齐的专家在环框架。通过不确定性估计识别不可靠的输出，检索相关参考文献以辅助专家突出关键术语，并应用无分类器指导来优化MedVLM的嵌入，以确保调整后的输出准确且符合专家标注。实验证明，该框架在三个医学视觉问答基准测试中表现优于现有的大型模型，展示了其在资源有限环境下的临床应用潜力。

Abstract: The rapid advancements in Vision Language Models (VLMs) have prompted the
development of multi-modal medical assistant systems. Despite this progress,
current models still have inherent probabilistic uncertainties, often producing
erroneous or unverified responses-an issue with serious implications in medical
applications. Existing methods aim to enhance the performance of Medical Vision
Language Model (MedVLM) by adjusting model structure, fine-tuning with
high-quality data, or through preference fine-tuning. However, these
training-dependent strategies are costly and still lack sufficient alignment
with clinical expertise. To address these issues, we propose an
expert-in-the-loop framework named Expert-Controlled Classifier-Free Guidance
(Expert-CFG) to align MedVLM with clinical expertise without additional
training. This framework introduces an uncertainty estimation strategy to
identify unreliable outputs. It then retrieves relevant references to assist
experts in highlighting key terms and applies classifier-free guidance to
refine the token embeddings of MedVLM, ensuring that the adjusted outputs are
correct and align with expert highlights. Evaluations across three medical
visual question answering benchmarks demonstrate that the proposed Expert-CFG,
with 4.2B parameters and limited expert annotations, outperforms
state-of-the-art models with 13B parameters. The results demonstrate the
feasibility of deploying such a system in resource-limited settings for
clinical use.

</details>


### [29] [Stereo-based 3D Anomaly Object Detection for Autonomous Driving: A New Dataset and Baseline](https://arxiv.org/abs/2507.09214)
*Shiyi Mu,Zichong Gu,Hanqi Lyu,Yilin Gao,Shugong Xu*

Main category: cs.CV

TL;DR: 该研究提出了一种名为S3AD的算法，用于解决自动驾驶中3D目标检测模型在开放道路上对罕见异常类别的检测问题。通过解耦2D和3D训练策略、引入异常评分算法，并构建了包含大量罕见类别的KITTI-AR数据集，显著提升了模型的泛化能力和异常检测性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决3D检测模型在开放道路等场景下，对罕见异常类别检测不准或无法检测的问题，提升模型对任意形状目标的泛化能力并具备过滤异常的能力。

Method: 提出了一种立体声3D异常物体检测（S3AD）算法，该算法将3D和2D的训练策略解耦，以释放任意3D前景检测的泛化能力，并提出了一种基于前景置信度预测的异常评分算法，实现了目标级别的异常评分。为了进一步验证和增强异常检测的泛化能力，研究者使用3D渲染方法合成了两个增强现实双目立体3D检测数据集KITTI-AR。KITTI-AR在KITTI的基础上增加了97个新类别，共包含6k对立体图像。KITTI-AR-ExD子集包括39个常见类别作为额外的训练数据，以解决稀疏样本分布问题。此外，58个罕见类别构成了KITTI-AR-OoD子集，这些类别不用于训练，以模拟真实世界中的零样本场景，仅用于评估3D异常检测。

Result: 通过在KITTI-AR数据集上的实验验证了S3AD算法的有效性及其泛化能力，证明了该方法在3D异常物体检测方面的优越性。

Conclusion: 该研究提出的S3AD算法通过解耦2D和3D的训练策略，并结合基于前景置信度预测的异常评分算法，有效提升了3D目标检测模型对任意形状目标的泛化能力，并实现了目标级别的异常评分。此外，通过合成的KITTI-AR数据集验证了算法的有效性，该数据集包含了丰富的罕见类别和增强的样本分布，为3D异常检测提供了有力的评估基准。

Abstract: 3D detection technology is widely used in the field of autonomous driving,
with its application scenarios gradually expanding from enclosed highways to
open conventional roads. For rare anomaly categories that appear on the road,
3D detection models trained on closed sets often misdetect or fail to detect
anomaly objects. To address this risk, it is necessary to enhance the
generalization ability of 3D detection models for targets of arbitrary shapes
and to possess the capability to filter out anomalies. The generalization of 3D
detection is limited by two factors: the coupled training of 2D and 3D, and the
insufficient diversity in the scale distribution of training samples. This
paper proposes a Stereo-based 3D Anomaly object Detection (S3AD) algorithm,
which decouples the training strategy of 3D and 2D to release the
generalization ability for arbitrary 3D foreground detection, and proposes an
anomaly scoring algorithm based on foreground confidence prediction, achieving
target-level anomaly scoring. In order to further verify and enhance the
generalization of anomaly detection, we use a 3D rendering method to synthesize
two augmented reality binocular stereo 3D detection datasets which named
KITTI-AR. KITTI-AR extends upon KITTI by adding 97 new categories, totaling 6k
pairs of stereo images. The KITTI-AR-ExD subset includes 39 common categories
as extra training data to address the sparse sample distribution issue.
Additionally, 58 rare categories form the KITTI-AR-OoD subset, which are not
used in training to simulate zero-shot scenarios in real-world settings, solely
for evaluating 3D anomaly detection. Finally, the performance of the algorithm
and the dataset is verified in the experiments. (Code and dataset can be
obtained at https://github.com/xxxx/xxx).

</details>


### [30] [360-Degree Full-view Image Segmentation by Spherical Convolution compatible with Large-scale Planar Pre-trained Models](https://arxiv.org/abs/2507.09216)
*Jingguo Liu,Han Yu,Shigang Li,Jianfeng Li*

Main category: cs.CV

TL;DR: 提出一种球形采样方法，解决了全景图像处理中二维预训练模型存在的畸变问题，并在分割任务中取得良好效果。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏大规模全景图像数据集，导致现有二维预训练模型在处理全景图像时存在畸变和不连续性识别能力不足的问题，影响了模型性能。

Method: 提出一种新颖的球形采样方法，利用预训练模型的权重进行球形离散采样，以处理全景图像的畸变和不连续性，并将其应用于全景图像分割，通过球形模型提取的特征作为特定通道注意力的掩码。

Result: 该方法有效缓解了全景图像的畸变问题，并为训练提供了有利的初始值。在全景图像分割任务中，利用球形模型提取的特征作为通道注意力掩码，在Stanford2D3D数据集上取得了良好的结果。

Conclusion: 本文提出的球形采样方法有效解决了全景图像处理中现有二维预训练模型无法识别畸变和不连续性问题，并将其应用于全景图像分割任务，在Stanford2D3D数据集上取得了良好效果。

Abstract: Due to the current lack of large-scale datasets at the million-scale level,
tasks involving panoramic images predominantly rely on existing two-dimensional
pre-trained image benchmark models as backbone networks. However, these
networks are not equipped to recognize the distortions and discontinuities
inherent in panoramic images, which adversely affects their performance in such
tasks. In this paper, we introduce a novel spherical sampling method for
panoramic images that enables the direct utilization of existing pre-trained
models developed for two-dimensional images. Our method employs spherical
discrete sampling based on the weights of the pre-trained models, effectively
mitigating distortions while achieving favorable initial training values.
Additionally, we apply the proposed sampling method to panoramic image
segmentation, utilizing features obtained from the spherical model as masks for
specific channel attentions, which yields commendable results on commonly used
indoor datasets, Stanford2D3D.

</details>


### [31] [Online Long-term Point Tracking in the Foundation Model Era](https://arxiv.org/abs/2507.09217)
*Görkay Aydemir*

Main category: cs.CV

TL;DR: 提出了一种名为 Track-On 的新模型，可在无法访问未来信息的情况下实现有效的长期点跟踪。


<details>
  <summary>Details</summary>
Motivation: 解决在线设置下的长期点跟踪问题，在这种情况下，模型必须仅使用当前和过去的帧进行因果预测，这在流媒体视频和具身人工智能等场景中至关重要。

Method: 提出了一种名为 Track-On 的 Transformer 模型，该模型将每个跟踪点视为查询，并逐一处理视频帧，以在因果环境中维持长期跟踪的连贯性。

Result: Track-On 在七个公开基准测试中设定了新的最先进性能，证明了在无法访问未来信息的情况下进行长期跟踪是可行的。

Conclusion: Track-On 作为一个基于 Transformer 的模型，通过将每个跟踪点视为查询并逐一处理视频帧，实现了新的最先进性能，证明了在无法访问未来信息的情况下进行长期跟踪的可行性。

Abstract: Point tracking aims to identify the same physical point across video frames
and serves as a geometry-aware representation of motion. This representation
supports a wide range of applications, from robotics to augmented reality, by
enabling accurate modeling of dynamic environments. Most existing long-term
tracking approaches operate in an offline setting, where future frames are
available to refine predictions and recover from occlusions. However,
real-world scenarios often demand online predictions: the model must operate
causally, using only current and past frames. This constraint is critical in
streaming video and embodied AI, where decisions must be made immediately based
on past observations. Under such constraints, viewpoint invariance becomes
essential. Visual foundation models, trained on diverse large-scale datasets,
offer the potential for robust geometric representations. While they lack
temporal reasoning on their own, they can be integrated into tracking pipelines
to enrich spatial features. In this thesis, we address the problem of long-term
point tracking in an online setting, where frames are processed sequentially
without access to future information or sliding windows. We begin by evaluating
the suitability of visual foundation models for this task and find that they
can serve as useful initializations and be integrated into tracking pipelines.
However, to enable long-term tracking in an online setting, a dedicated design
is still required. In particular, maintaining coherence over time in this
causal regime requires memory to propagate appearance and context across
frames. To address this, we introduce Track-On, a transformer-based model that
treats each tracked point as a query and processes video frames one at a time.
Track-On sets a new state of the art across seven public benchmarks,
demonstrating the feasibility of long-term tracking without future access.

</details>


### [32] [Calibrated and Robust Foundation Models for Vision-Language and Medical Image Tasks Under Distribution Shift](https://arxiv.org/abs/2507.09222)
*Behraj Khan,Tahir Syed*

Main category: cs.CV

TL;DR: StaRFM通过FIP和CMP解决了基础模型的分布偏移和置信度错位问题，在视觉和医学影像任务中均取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的计算机视觉和医学成像领域的基础模型（如CLIP和SAM）虽然在迁移学习方面表现出色，但在实际部署中面临分布偏移和置信度错位（导致不准确的预测）两大挑战，而现有的解决方案在不同领域存在差异。

Method: 提出了一种名为StaRFM的统一框架，包含Fisher信息惩罚（FIP）和置信度错位惩罚（CMP）。FIP通过补丁式正则化扩展到3D医学数据，以减少CLIP和SAM嵌入中的协变量偏移。CMP被重新制定用于体素级预测，以校准分割任务中的不确定性。

Result: 在19个视觉数据集（如ImageNet、Office-Home）上，StaRFM实现了平均准确率提升3.5%和ECE降低28%。在医学分割任务（如BraTS、ATLAS）上，实现了84.7%的DSC和4.8mm的HD95。此外，与现有方法相比，StaRFM将跨领域性能差距降低了40%。

Conclusion: StaRFM是一个统一的框架，通过引入Fisher信息惩罚（FIP）和置信度错位惩罚（CMP），分别解决了分布偏移和置信度错位问题，并取得了显著的性能提升。FIP通过Fisher-rao范数控制泛化，CMP通过Brier分数优化校准不确定性。

Abstract: Foundation models like CLIP and SAM have transformed computer vision and
medical imaging via low-shot transfer learning. However, deployment of these
models hindered by two key challenges: \textit{distribution shift} between
training and test data, and \textit{confidence misalignment} that leads to
overconfident incorrect predictions. These issues manifest differently in
vision-language classification and medical segmentation tasks, yet existing
solutions remain domain-specific. We propose \textit{StaRFM}, a unified
framework addressing both challenges. It introduces a Fisher information
penalty (FIP), extended to 3D medical data via patch-wise regularization, to
reduce covariate shift in CLIP and SAM embeddings. Additionally, a confidence
misalignment penalty (CMP), reformulated for voxel-level predictions,
calibrates uncertainty in segmentation tasks. We theoretically derive PAC-Bayes
bounds showing FIP controls generalization via the Fisher-Rao norm, while CMP
minimizes calibration error through Brier score optimization. StaRFM shows
consistent performance like \texttt{+}3.5\% accuracy and 28\% lower ECE on 19
vision datasets (e.g., ImageNet, Office-Home), 84.7\% DSC and 4.8mm HD95 in
medical segmentation (e.g., BraTS, ATLAS), and 40\% lower cross-domain
performance gap compared to prior benchmarking methods. The framework is
plug-and-play, requiring minimal architectural changes for seamless integration
with foundation models. Code and models will be released at
https://anonymous.4open.science/r/StaRFM-C0CD/README.md

</details>


### [33] [EgoAnimate: Generating Human Animations from Egocentric top-down Views](https://arxiv.org/abs/2507.09230)
*G. Kutay Türkoglu,Julian Tanke,Iheb Belgacem,Lev Markhasin*

Main category: cs.CV

TL;DR: 该研究提出了一种创新的方法，利用Stable Diffusion和ControlNet技术，仅通过单一的第一人称俯视图像，就能生成逼真的正面虚拟形象，并驱动其动作，为实现更便捷的远程呈现体验铺平了道路。


<details>
  <summary>Details</summary>
Motivation: 为了在虚拟现实中捕捉和传输人的身体、服装和动作，第一人称视角是一种便携且经济的设备，但它也带来了遮挡和身体比例失真等挑战。现有的从第一人称视角重建人体外观的工作很少，并且没有使用生成式先验的方法。本研究旨在解决这些问题，实现更易于访问和可推广的远程呈现系统。

Method: 该方法利用ControlNet和Stable Diffusion作为生成模型，将遮挡的俯视图像转换为逼真的正面视图，并将其输入到图像到运动模型中，从而从极简的输入生成虚拟形象的动作。

Result: 通过使用生成式主干（Stable Diffusion）来重建可驱动的虚拟形象，该方法有望减少训练负担并提高泛化能力，从而实现从极简输入生成虚拟形象的动作。

Conclusion: 该研究提出了一个从单一第一人称视角（俯视）图像生成可驱动的虚拟形象（avatar）的方法，旨在实现更易于访问和可推广的远程呈现系统。

Abstract: An ideal digital telepresence experience requires accurate replication of a
person's body, clothing, and movements. To capture and transfer these movements
into virtual reality, the egocentric (first-person) perspective can be adopted,
which enables the use of a portable and cost-effective device without
front-view cameras. However, this viewpoint introduces challenges such as
occlusions and distorted body proportions.
  There are few works reconstructing human appearance from egocentric views,
and none use a generative prior-based approach. Some methods create avatars
from a single egocentric image during inference, but still rely on multi-view
datasets during training. To our knowledge, this is the first study using a
generative backbone to reconstruct animatable avatars from egocentric inputs.
Based on Stable Diffusion, our method reduces training burden and improves
generalizability.
  Inspired by methods such as SiTH and MagicMan, which perform 360-degree
reconstruction from a frontal image, we introduce a pipeline that generates
realistic frontal views from occluded top-down images using ControlNet and a
Stable Diffusion backbone.
  Our goal is to convert a single top-down egocentric image into a realistic
frontal representation and feed it into an image-to-motion model. This enables
generation of avatar motions from minimal input, paving the way for more
accessible and generalizable telepresence systems.

</details>


### [34] [PPJudge: Towards Human-Aligned Assessment of Artistic Painting Process](https://arxiv.org/abs/2507.09242)
*Shiqi Jiang,Xinpeng Li,Xi Mao,Changbo Wang,Chenhui Li*

Main category: cs.CV

TL;DR: 本文提出了一种新的PPJudge框架和PPAD数据集，用于评估绘画过程，而非仅评估最终图像。PPJudge通过 زمن感知位置编码和专家混合架构增强了Transformer模型，实验证明其效果优于现有方法，并为计算创意和艺术教育提供了新的见解。


<details>
  <summary>Details</summary>
Motivation: 现有方法大多只关注静态的最终图像，忽略了艺术绘画过程的动态和多阶段性质。为了解决这个差距，本文提出了一个用于人类对齐的绘画过程评估框架。

Method: 本文提出了一种新颖的框架，用于评估绘画过程，并引入了PPAD（绘画过程评估数据集），一个包含真实和合成绘画过程图像的大型数据集，并由领域专家标注了八个详细属性。此外，还提出了一种基于Transformer的模型PPJudge，该模型通过 زمن感知位置编码和异构专家混合架构进行了增强。

Result: 实验结果表明，本文提出的方法在准确性、鲁棒性以及与人类判断的一致性方面优于现有基线方法。

Conclusion: 本文提出的PPJudge框架在绘画过程评估方面优于现有基线方法，无论是在准确性、鲁棒性还是与人类判断的一致性方面，都提供了新的见解。

Abstract: Artistic image assessment has become a prominent research area in computer
vision. In recent years, the field has witnessed a proliferation of datasets
and methods designed to evaluate the aesthetic quality of paintings. However,
most existing approaches focus solely on static final images, overlooking the
dynamic and multi-stage nature of the artistic painting process. To address
this gap, we propose a novel framework for human-aligned assessment of painting
processes. Specifically, we introduce the Painting Process Assessment Dataset
(PPAD), the first large-scale dataset comprising real and synthetic painting
process images, annotated by domain experts across eight detailed attributes.
Furthermore, we present PPJudge (Painting Process Judge), a Transformer-based
model enhanced with temporally-aware positional encoding and a heterogeneous
mixture-of-experts architecture, enabling effective assessment of the painting
process. Experimental results demonstrate that our method outperforms existing
baselines in accuracy, robustness, and alignment with human judgment, offering
new insights into computational creativity and art education.

</details>


### [35] [AGCD-Net: Attention Guided Context Debiasing Network for Emotion Recognition](https://arxiv.org/abs/2507.09248)
*Varsha Devi,Amine Bohi,Pardeep Kumar*

Main category: cs.CV

TL;DR: AGCD-Net通过混合卷积神经网络和注意力引导因果干预模块，有效解决了上下文感知情感识别中的上下文偏见问题，并在CAER-S数据集上取得了领先性能。


<details>
  <summary>Details</summary>
Motivation: 传统的上下文感知情感识别（CAER）方法常常受到上下文偏见的影响，即背景上下文与情感标签之间存在虚假关联（例如，将“花园”与“快乐”关联起来）。

Method: 提出了一种名为AGCD-Net（Attention Guided Context Debiasing Network）的模型，该模型引入了混合卷积神经网络（Hybrid ConvNeXt）作为编码器，该编码器通过集成空间变换网络（Spatial Transformer Network）和Squeeze-and-Excitation层来增强特征重新校准。其核心是注意力引导因果干预模块（AG-CIM），该模块应用因果理论，扰动上下文特征，分离虚假相关性，并根据面部特征进行注意力驱动的校正，以减轻上下文偏见。

Result: AGCD-Net在CAER-S数据集上实现了最先进的性能，证明了其有效性。

Conclusion: 实验结果表明，AGCD-Net在CAER-S数据集上实现了最先进的性能，强调了因果去偏对于复杂场景下鲁棒情感识别的重要性。

Abstract: Context-aware emotion recognition (CAER) enhances affective computing in
real-world scenarios, but traditional methods often suffer from context
bias-spurious correlation between background context and emotion labels (e.g.
associating ``garden'' with ``happy''). In this paper, we propose
\textbf{AGCD-Net}, an Attention Guided Context Debiasing model that introduces
\textit{Hybrid ConvNeXt}, a novel convolutional encoder that extends the
ConvNeXt backbone by integrating Spatial Transformer Network and
Squeeze-and-Excitation layers for enhanced feature recalibration. At the core
of AGCD-Net is the Attention Guided - Causal Intervention Module (AG-CIM),
which applies causal theory, perturbs context features, isolates spurious
correlations, and performs an attention-driven correction guided by face
features to mitigate context bias. Experimental results on the CAER-S dataset
demonstrate the effectiveness of AGCD-Net, achieving state-of-the-art
performance and highlighting the importance of causal debiasing for robust
emotion recognition in complex settings.

</details>


### [36] [Ambiguity-Aware and High-Order Relation Learning for Multi-Grained Image-Text Matching](https://arxiv.org/abs/2507.09256)
*Junyu Chen,Yihua Gao,Mingyuan Ge,Mingyong Li*

Main category: cs.CV

TL;DR: AAHR framework improves image-text matching by addressing semantic ambiguities and utilizing neighborhood relationships through dynamic clustering, contrastive learning, and GNNs, outperforming existing methods on multiple datasets.


<details>
  <summary>Details</summary>
Motivation: Existing image-text matching methods struggle with high-order associations and semantic ambiguities among similar instances (soft positives and soft negatives), and fail to utilize neighborhood relationships among semantically similar instances within training batches.

Method: AAHR constructs a unified representation space through dynamic clustering prototype contrastive learning, mitigating the soft positive sample problem. It utilizes global and local feature extraction mechanisms and an adaptive aggregation network for enhanced semantic understanding. Additionally, it employs intra-modal and inter-modal correlation matrices with GNN to investigate neighborhood relationships and incorporates momentum contrastive learning to expand the negative sample set, improving feature discrimination.

Result: Experimental results demonstrate that AAHR outperforms existing state-of-the-art methods on Flickr30K, MSCOCO, and ECCV Caption datasets, considerably improving the accuracy and efficiency of image-text matching.

Conclusion: AAHR outperforms existing state-of-the-art methods on Flickr30K, MSCOCO, and ECCV Caption datasets, considerably improving the accuracy and efficiency of image-text matching.

Abstract: Image-text matching is crucial for bridging the semantic gap between computer
vision and natural language processing. However, existing methods still face
challenges in handling high-order associations and semantic ambiguities among
similar instances. These ambiguities arise from subtle differences between soft
positive samples (semantically similar but incorrectly labeled) and soft
negative samples (locally matched but globally inconsistent), creating matching
uncertainties. Furthermore, current methods fail to fully utilize the
neighborhood relationships among semantically similar instances within training
batches, limiting the model's ability to learn high-order shared knowledge.
This paper proposes the Ambiguity-Aware and High-order Relation learning
framework (AAHR) to address these issues. AAHR constructs a unified
representation space through dynamic clustering prototype contrastive learning,
effectively mitigating the soft positive sample problem. The framework
introduces global and local feature extraction mechanisms and an adaptive
aggregation network, significantly enhancing full-grained semantic
understanding capabilities. Additionally, AAHR employs intra-modal and
inter-modal correlation matrices to investigate neighborhood relationships
among sample instances thoroughly. It incorporates GNN to enhance semantic
interactions between instances. Furthermore, AAHR integrates momentum
contrastive learning to expand the negative sample set. These combined
strategies significantly improve the model's ability to discriminate between
features. Experimental results demonstrate that AAHR outperforms existing
state-of-the-art methods on Flickr30K, MSCOCO, and ECCV Caption datasets,
considerably improving the accuracy and efficiency of image-text matching. The
code and model checkpoints for this research are available at
https://github.com/Image-Text-Matching/AAHR .

</details>


### [37] [SAGE: Segment-Aware Gloss-Free Encoding for Token-Efficient Sign Language Translation](https://arxiv.org/abs/2507.09266)
*JianHe Low,Ozge Mercanoglu Sincan,Richard Bowden*

Main category: cs.CV

TL;DR: 提出了一种段感知视觉标记化框架，通过手语分割将视频转换为视觉标记，减少了输入序列长度和内存使用量，提高了可扩展性。通过对比对齐和双层监督提高了跨模态对齐，无需词条监督。在PHOENIX14T上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 解决无手语词条的手语翻译（SLT）模型复杂性和高计算需求问题，提高模型的可扩展性，以应对大规模手语数据集。

Method: 提出了一种段感知视觉标记化框架，利用手语分割将连续视频转换为离散的、受手语信息启发的视觉标记。引入了标记到标记的对比对齐目标和双层监督，对语言嵌入和中间隐藏状态进行对齐。

Result: 将输入序列长度减少多达50%，内存使用量减少多达2.67倍，并在大型数据集上具有更好的可扩展性。通过对比对齐和双层监督提高了跨模态的细粒度对齐，无需词条级别的监督。

Conclusion: 该方法在PHOENIX14T基准上显著优于最先进的方法，同时大大缩短了序列长度，并验证了其标记化和对齐策略的潜力。

Abstract: Gloss-free Sign Language Translation (SLT) has advanced rapidly, achieving
strong performances without relying on gloss annotations. However, these gains
have often come with increased model complexity and high computational demands,
raising concerns about scalability, especially as large-scale sign language
datasets become more common. We propose a segment-aware visual tokenization
framework that leverages sign segmentation to convert continuous video into
discrete, sign-informed visual tokens. This reduces input sequence length by up
to 50% compared to prior methods, resulting in up to 2.67x lower memory usage
and better scalability on larger datasets. To bridge the visual and linguistic
modalities, we introduce a token-to-token contrastive alignment objective,
along with a dual-level supervision that aligns both language embeddings and
intermediate hidden states. This improves fine-grained cross-modal alignment
without relying on gloss-level supervision. Our approach notably exceeds the
performance of state-of-the-art methods on the PHOENIX14T benchmark, while
significantly reducing sequence length. Further experiments also demonstrate
our improved performance over prior work under comparable sequence-lengths,
validating the potential of our tokenization and alignment strategies.

</details>


### [38] [Cross Knowledge Distillation between Artificial and Spiking Neural Networks](https://arxiv.org/abs/2507.09269)
*Shuhan Ye,Yuanbin Qian,Chong Wang,Sunqi Lin,Jiazhen Xu,Jiangbo Qian,Yuqi Li*

Main category: cs.CV

TL;DR: 通过跨模态知识蒸馏（CKD），利用RGB数据和ANNs来提升SNN在事件数据上的性能，并在实验中取得了优于现有方法的成果。


<details>
  <summary>Details</summary>
Motivation: 为了提高SNN在事件数据（DVS数据）上的性能，克服目前标注数据有限和SNN架构不成熟导致的性能瓶颈，并解决跨模态和跨架构的知识迁移问题。

Method: 提出了一种名为交叉知识蒸馏（CKD）的框架，该框架利用RGB数据和先进的人工神经网络（ANNs）来提升脉冲神经网络（SNNs）在事件数据上的性能。具体而言，CKD通过利用语义相似性和滑动替换来解决跨模态挑战，并通过间接分阶段知识蒸馏来解决跨架构挑战。

Result: 实验结果表明，CKD在N-Caltech101和CEP-DVS等数据集上，相比现有最先进的方法取得了更好的性能。

Conclusion: 所提出的交叉知识蒸馏（CKD）方法在主流神经形态数据集上优于现有的最先进方法，证明了其有效性。

Abstract: Recently, Spiking Neural Networks (SNNs) have demonstrated rich potential in
computer vision domain due to their high biological plausibility, event-driven
characteristic and energy-saving efficiency. Still, limited annotated
event-based datasets and immature SNN architectures result in their performance
inferior to that of Artificial Neural Networks (ANNs). To enhance the
performance of SNNs on their optimal data format, DVS data, we explore using
RGB data and well-performing ANNs to implement knowledge distillation. In this
case, solving cross-modality and cross-architecture challenges is necessary. In
this paper, we propose cross knowledge distillation (CKD), which not only
leverages semantic similarity and sliding replacement to mitigate the
cross-modality challenge, but also uses an indirect phased knowledge
distillation to mitigate the cross-architecture challenge. We validated our
method on main-stream neuromorphic datasets, including N-Caltech101 and
CEP-DVS. The experimental results show that our method outperforms current
State-of-the-Art methods. The code will be available at
https://github.com/ShawnYE618/CKD

</details>


### [39] [Prompt4Trust: A Reinforcement Learning Prompt Augmentation Framework for Clinically-Aligned Confidence Calibration in Multimodal Large Language Models](https://arxiv.org/abs/2507.09279)
*Anita Kriz,Elizabeth Laura Janes,Xing Shen,Tal Arbel*

Main category: cs.CV

TL;DR: Prompt4Trust：一个强化学习框架，通过生成辅助提示来校准医疗多模态大语言模型的置信度，提高了准确性和安全性。


<details>
  <summary>Details</summary>
Motivation: 旨在解决多模态大语言模型（MLLM）在医疗等安全关键领域部署时面临的两大挑战：对提示设计的敏感性以及高置信度下生成错误响应的倾向。

Method: 提出了一种名为Prompt4Trust的强化学习（RL）框架，通过训练一个轻量级语言模型来生成辅助提示，以指导下游任务MLLM产生更准确地反映预测准确性的置信度响应。

Result: 在PMC-VQA基准上实现了最先进的医学视觉问答性能，提高了任务准确性。所提出的框架在较小的下游MLLM上训练时，对更大的MLLM表现出良好的零样本泛化能力，表明可以实现可扩展的校准而无需巨大的计算成本。

Conclusion: Prompt4Trust框架通过生成辅助提示来校准多模态大语言模型（MLLM）的置信度，提高了模型在医疗领域的安全性和可靠性。该框架在PMC-VQA基准上达到了最先进的医学视觉问答性能，并展示了对更大模型的零样本泛化能力，证明了自动化提示工程在提升MLLM在安全关键领域的可信度方面的潜力。

Abstract: Multimodal large language models (MLLMs) hold considerable promise for
applications in healthcare. However, their deployment in safety-critical
settings is hindered by two key limitations: (i) sensitivity to prompt design,
and (ii) a tendency to generate incorrect responses with high confidence. As
clinicians may rely on a model's stated confidence to gauge the reliability of
its predictions, it is especially important that when a model expresses high
confidence, it is also highly accurate. We introduce Prompt4Trust, the first
reinforcement learning (RL) framework for prompt augmentation targeting
confidence calibration in MLLMs. A lightweight LLM is trained to produce
context-aware auxiliary prompts that guide a downstream task MLLM to generate
responses in which the expressed confidence more accurately reflects predictive
accuracy. Unlike conventional calibration techniques, Prompt4Trust specifically
prioritizes aspects of calibration most critical for safe and trustworthy
clinical decision-making. Beyond improvements driven by this clinically
motivated calibration objective, our proposed method also improves task
accuracy, achieving state-of-the-art medical visual question answering (VQA)
performance on the PMC-VQA benchmark, which is composed of multiple-choice
questions spanning diverse medical imaging modalities. Moreover, our framework
trained with a small downstream task MLLM showed promising zero-shot
generalization to larger MLLMs in our experiments, suggesting the potential for
scalable calibration without the associated computational costs. This work
demonstrates the potential of automated yet human-aligned prompt engineering
for improving the the trustworthiness of MLLMs in safety critical settings. Our
codebase can be found at https://github.com/xingbpshen/vccrl-llm.

</details>


### [40] [Generative Latent Kernel Modeling for Blind Motion Deblurring](https://arxiv.org/abs/2507.09285)
*Chenhao Ding,Jiangtao Zhang,Zongsheng Yue,Hui Wang,Qian Zhao,Deyu Meng*

Main category: cs.CV

TL;DR: 通过使用GAN生成的核先验和初始化器来改善模糊核估计的初始值，解决了现有盲运动去模糊方法对初始模糊核敏感的问题，并可即插即用。


<details>
  <summary>Details</summary>
Motivation: 现有的基于深度先验的盲运动去模糊方法（BMD）通常受限于BMD底层优化过程的高度非凸性，导致其对初始模糊核极其敏感。

Method: 提出了一种新颖的盲运动去模糊框架，该框架利用深度生成模型来编码模糊核先验并为模糊核提供更好的初始化。具体而言，预训练了一个基于生成对抗网络（GAN）的核生成器来准确表征核的先验分布，以及一个核初始化器来提供一个信息丰富且高质量的内核估计起点。通过结合这两个组件，将BMD解决方案约束在紧凑的潜在核流形内，从而减轻了对内核初始化的敏感性。该核生成器和初始化器可以以即插即用的方式轻松集成到现有的BMD方法中。此外，该方法还无需额外先验即可处理盲非均匀运动去模糊问题。

Result: 该框架通过约束BMD解决方案于紧凑的潜在核流形，有效减轻了对内核初始化的敏感性，并显著提升了现有BMD方法的性能。在盲非均匀运动去模糊方面也取得了先进的性能。

Conclusion: 该方法通过利用深度生成模型编码模糊核先验并为模糊核诱导更好的初始化，从而解决了现有基于深度先验的盲运动去模糊方法对初始模糊核敏感的问题。

Abstract: Deep prior-based approaches have demonstrated remarkable success in blind
motion deblurring (BMD) recently. These methods, however, are often limited by
the high non-convexity of the underlying optimization process in BMD, which
leads to extreme sensitivity to the initial blur kernel. To address this issue,
we propose a novel framework for BMD that leverages a deep generative model to
encode the kernel prior and induce a better initialization for the blur kernel.
Specifically, we pre-train a kernel generator based on a generative adversarial
network (GAN) to aptly characterize the kernel's prior distribution, as well as
a kernel initializer to provide a well-informed and high-quality starting point
for kernel estimation. By combining these two components, we constrain the BMD
solution within a compact latent kernel manifold, thus alleviating the
aforementioned sensitivity for kernel initialization. Notably, the kernel
generator and initializer are designed to be easily integrated with existing
BMD methods in a plug-and-play manner, enhancing their overall performance.
Furthermore, we extend our approach to tackle blind non-uniform motion
deblurring without the need for additional priors, achieving state-of-the-art
performance on challenging benchmark datasets. The source code is available at
https://github.com/dch0319/GLKM-Deblur.

</details>


### [41] [Supercharging Floorplan Localization with Semantic Rays](https://arxiv.org/abs/2507.09291)
*Yuval Grader,Hadar Averbuch-Elor*

Main category: cs.CV

TL;DR: 一种新的语义感知定位框架，利用深度和语义信息来提高楼层平面定位的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 然而，当前的楼层平面定位技术主要集中在匹配基于深度的结构线索上，而忽略了楼层平面中传达的丰富语义。

Method: 我们引入了一个语义感知定位框架，该框架共同估计深度和语义射线，并在两者上进行整合，以预测结构-语义概率体积。我们的概率体积是以粗到精的方式构建的：我们首先对一组少量射线进行采样以获得初始的低分辨率概率体积。然后，我们仅在高概率区域中进行更密集的采样来优化这些概率，并处理优化后的值以预测二维位置和方向角。

Result: 我们的实验表明，我们的方法在很大程度上优于最先进的方法，与之前的工作相比，在召回指标方面取得了显著的改进。

Conclusion: 该框架能够轻松地整合诸如房间标签之类的附加元数据，从而在准确性和效率方面都带来额外的收益。

Abstract: Floorplans provide a compact representation of the building's structure,
revealing not only layout information but also detailed semantics such as the
locations of windows and doors. However, contemporary floorplan localization
techniques mostly focus on matching depth-based structural cues, ignoring the
rich semantics communicated within floorplans. In this work, we introduce a
semantic-aware localization framework that jointly estimates depth and semantic
rays, consolidating over both for predicting a structural-semantic probability
volume. Our probability volume is constructed in a coarse-to-fine manner: We
first sample a small set of rays to obtain an initial low-resolution
probability volume. We then refine these probabilities by performing a denser
sampling only in high-probability regions and process the refined values for
predicting a 2D location and orientation angle. We conduct an evaluation on two
standard floorplan localization benchmarks. Our experiments demonstrate that
our approach substantially outperforms state-of-the-art methods, achieving
significant improvements in recall metrics compared to prior works. Moreover,
we show that our framework can easily incorporate additional metadata such as
room labels, enabling additional gains in both accuracy and efficiency.

</details>


### [42] [Geo-RepNet: Geometry-Aware Representation Learning for Surgical Phase Recognition in Endoscopic Submucosal Dissection](https://arxiv.org/abs/2507.09294)
*Rui Tang,Haochen Yin,Guankun Wang,Long Bai,An Wang,Huxin Gao,Jiazheng Wang,Hongliang Ren*

Main category: cs.CV

TL;DR: 本研究开创性地将深度信息用于手术阶段识别，提出了 Geo-RepNet 框架，通过集成深度信息和几何感知模块来提高复杂手术场景下的识别性能，并在自定义的 ESD 数据集上取得了最先进的成果。


<details>
  <summary>Details</summary>
Motivation: 手术阶段识别对于开发微创手术（如内镜粘膜下剥离术 (ESD)）的智能辅助系统至关重要。然而，不同阶段之间的高度视觉相似性以及 RGB 图像中缺乏结构线索带来了重大挑战。深度信息可以提供有价值的几何线索，通过提供空间关系和解剖结构方面的见解来补充外观特征。

Method: 提出了一种名为 Geo-RepNet 的几何感知卷积框架，该框架集成了 RGB 图像和深度信息。Geo-RepNet 基于可重构的 RepVGG 主干构建，并包含一个深度引导几何先验生成（DGPG）模块和一个几何增强多尺度注意力（GEMA）模块，以提取几何先验并注入空间引导。

Result: 在提出的九阶段 ESD 数据集上进行的大量实验证明，Geo-RepNet 在复杂和低纹理的手术环境中取得了最先进的性能，同时保持了鲁棒性和高计算效率。

Conclusion: Geo-RepNet 在复杂且低纹理的手术环境中取得了最先进的性能，同时保持了鲁棒性和高计算效率。

Abstract: Surgical phase recognition plays a critical role in developing intelligent
assistance systems for minimally invasive procedures such as Endoscopic
Submucosal Dissection (ESD). However, the high visual similarity across
different phases and the lack of structural cues in RGB images pose significant
challenges. Depth information offers valuable geometric cues that can
complement appearance features by providing insights into spatial relationships
and anatomical structures. In this paper, we pioneer the use of depth
information for surgical phase recognition and propose Geo-RepNet, a
geometry-aware convolutional framework that integrates RGB image and depth
information to enhance recognition performance in complex surgical scenes.
Built upon a re-parameterizable RepVGG backbone, Geo-RepNet incorporates the
Depth-Guided Geometric Prior Generation (DGPG) module that extracts geometry
priors from raw depth maps, and the Geometry-Enhanced Multi-scale Attention
(GEMA) to inject spatial guidance through geometry-aware cross-attention and
efficient multi-scale aggregation. To evaluate the effectiveness of our
approach, we construct a nine-phase ESD dataset with dense frame-level
annotations from real-world ESD videos. Extensive experiments on the proposed
dataset demonstrate that Geo-RepNet achieves state-of-the-art performance while
maintaining robustness and high computational efficiency under complex and
low-texture surgical environments.

</details>


### [43] [ViT-ProtoNet for Few-Shot Image Classification: A Multi-Benchmark Evaluation](https://arxiv.org/abs/2507.09299)
*Abdulvahap Mutlu,Şengül Doğan,Türker Tuncer*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The remarkable representational power of Vision Transformers (ViTs) remains
underutilized in few-shot image classification. In this work, we introduce
ViT-ProtoNet, which integrates a ViT-Small backbone into the Prototypical
Network framework. By averaging class conditional token embeddings from a
handful of support examples, ViT-ProtoNet constructs robust prototypes that
generalize to novel categories under 5-shot settings. We conduct an extensive
empirical evaluation on four standard benchmarks: Mini-ImageNet, FC100,
CUB-200, and CIFAR-FS, including overlapped support variants to assess
robustness. Across all splits, ViT-ProtoNet consistently outperforms CNN-based
prototypical counterparts, achieving up to a 3.2\% improvement in 5-shot
accuracy and demonstrating superior feature separability in latent space.
Furthermore, it outperforms or is competitive with transformer-based
competitors using a more lightweight backbone. Comprehensive ablations examine
the impact of transformer depth, patch size, and fine-tuning strategy. To
foster reproducibility, we release code and pretrained weights. Our results
establish ViT-ProtoNet as a powerful, flexible approach for few-shot
classification and set a new baseline for transformer-based meta-learners.

</details>


### [44] [DAA*: Deep Angular A Star for Image-based Path Planning](https://arxiv.org/abs/2507.09305)
*Zhiwei Xu*

Main category: cs.CV

TL;DR: DAA*是一种新的模仿学习方法，通过引入路径角度自由度（PAF）来优化路径平滑度，提高了路径相似性和最优性。


<details>
  <summary>Details</summary>
Motivation: 路径模仿学习中常常忽略路径平滑度问题。

Method: 提出了一种名为深度角度A*（DAA*）的新型学习方法，通过将路径角度自由度（PAF）融入A*算法来提高路径平滑度以改进路径相似性。PAF通过寻找最小和最大值的权衡来探索移动角度对路径节点扩展的影响，从而实现对模仿学习的高度适应性。DAA*通过联合优化路径缩短和路径平滑（分别对应启发式距离和PAF）来提高路径最优性。

Result: 在7个数据集上的综合评估表明，DAA*在路径相似性（SPR、ASIM、PSIM）和路径长度方面均显著优于神经A*和TransPath等现有方法。

Conclusion: DAA*在路径相似性、路径长度和路径平滑度方面均优于现有方法，并探讨了路径最优性与搜索效率之间的权衡。

Abstract: Path smoothness is often overlooked in path imitation learning from expert
demonstrations. In this paper, we introduce a novel learning method, termed
deep angular A* (DAA*), by incorporating the proposed path angular freedom
(PAF) into A* to improve path similarity through adaptive path smoothness. The
PAF aims to explore the effect of move angles on path node expansion by finding
the trade-off between their minimum and maximum values, allowing for high
adaptiveness for imitation learning. DAA* improves path optimality by closely
aligning with the reference path through joint optimization of path shortening
and smoothing, which correspond to heuristic distance and PAF, respectively.
Throughout comprehensive evaluations on 7 datasets, including 4 maze datasets,
2 video-game datasets, and a real-world drone-view dataset containing 2
scenarios, we demonstrate remarkable improvements of our DAA* over neural A* in
path similarity between the predicted and reference paths with a shorter path
length when the shortest path is plausible, improving by 9.0% SPR, 6.9% ASIM,
and 3.9% PSIM. Furthermore, when jointly learning pathfinding with both path
loss and path probability map loss, DAA* significantly outperforms the
state-of-the-art TransPath by 6.7% SPR, 6.5% PSIM, and 3.7% ASIM. We also
discuss the minor trade-off between path optimality and search efficiency where
applicable.

</details>


### [45] [AlphaVAE: Unified End-to-End RGBA Image Reconstruction and Generation with Alpha-Aware Representation Learning](https://arxiv.org/abs/2507.09308)
*Zile Wang,Hao Yu,Jiabo Zhan,Chun Yuan*

Main category: cs.CV

TL;DR: 本文提出了ALPHA，首个RGBA基准测试，以及ALPHAVAE，一个统一的RGBA VAE，可在8K图像上实现卓越的透明图像生成。


<details>
  <summary>Details</summary>
Motivation: 现有技术在生成透明或分层内容（RGBA图像）方面存在不足，主要是由于缺乏大规模的基准测试。

Method: 本文提出了ALPHA，这是首个全面的RGBA基准测试，它通过在标准背景上进行 alpha 混合将标准RGB指标适配到四通道图像。同时，本文还提出了ALPHAVAE，一个统一的端到端RGBA VAE，通过整合一个专用的alpha通道来扩展预训练的RGB VAE。该模型使用了一个组合目标进行训练，该目标结合了 alpha 混合像素重建、块级保真度、感知一致性和双KL散度约束，以确保RGB和alpha表示的潜在保真度。

Result: ALPHAVAE在重建方面比LayerDiffuse的PSNR提高了+4.9 dB，SSIM提高了+3.2%。它还可以用于在潜在扩散框架内进行微调，以实现卓越的透明图像生成。

Conclusion: ALPHA是首个全面的RGBA基准测试，它通过在标准背景上进行 alpha 混合将标准RGB指标适配到四通道图像。ALPHAVAE是一个统一的端到端RGBA VAE，它通过整合一个专用的alpha通道来扩展预训练的RGB VAE。我们的RGBA VAE仅用8K图像进行训练，在重建方面比LayerDiffuse的PSNR提高了+4.9 dB，SSIM提高了+3.2%。它还可以用于在潜在扩散框架内进行微调，以实现卓越的透明图像生成。

Abstract: Recent advances in latent diffusion models have achieved remarkable results
in high-fidelity RGB image synthesis by leveraging pretrained VAEs to compress
and reconstruct pixel data at low computational cost. However, the generation
of transparent or layered content (RGBA image) remains largely unexplored, due
to the lack of large-scale benchmarks. In this work, we propose ALPHA, the
first comprehensive RGBA benchmark that adapts standard RGB metrics to
four-channel images via alpha blending over canonical backgrounds. We further
introduce ALPHAVAE, a unified end-to-end RGBA VAE that extends a pretrained RGB
VAE by incorporating a dedicated alpha channel. The model is trained with a
composite objective that combines alpha-blended pixel reconstruction,
patch-level fidelity, perceptual consistency, and dual KL divergence
constraints to ensure latent fidelity across both RGB and alpha
representations. Our RGBA VAE, trained on only 8K images in contrast to 1M used
by prior methods, achieves a +4.9 dB improvement in PSNR and a +3.2% increase
in SSIM over LayerDiffuse in reconstruction. It also enables superior
transparent image generation when fine-tuned within a latent diffusion
framework. Our code, data, and models are released on
https://github.com/o0o0o00o0/AlphaVAE for reproducibility.

</details>


### [46] [An Enhanced Classification Method Based on Adaptive Multi-Scale Fusion for Long-tailed Multispectral Point Clouds](https://arxiv.org/abs/2412.11407)
*TianZhu Liu,BangYan Hu,YanFeng Gu,Xian Li,Aleksandra Pižurica*

Main category: cs.CV

TL;DR: 提出了一种新的多光谱点云分类方法，通过网格平衡采样、多尺度特征融合和自适应混合损失，有效解决了室外数据集稀疏标记、尺度差异和长尾分布等问题，提高了分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有的多光谱点云分类方法主要在室内数据集上进行了广泛测试，在应用于室外数据集时，面临标记目标稀疏、地物尺度差异大以及长尾分布等问题。

Method: 提出了一种基于自适应多尺度融合的增强分类方法，用于处理具有长尾分布的多光谱点云。该方法包括三个阶段：1. 网格平衡采样策略：用于从稀疏标记的数据集中可靠地生成训练样本。2. 多尺度特征融合模块：用于融合不同尺度的地物浅层特征，解决因地物尺度变化导致的细粒度特征丢失问题。3. 自适应混合损失模块：利用具有自适应权重的多分类头来平衡不同类别的学习能力，从而提高小类别在各种尺度和长尾分布下的分类性能。

Result: 实验结果表明，该方法在三个多光谱点云数据集上优于现有最先进的方法。

Conclusion: 该方法在三个多光谱点云数据集上的实验结果证明了其相比于现有最先进方法的有效性。

Abstract: Multispectral point cloud (MPC) captures 3D spatial-spectral information from
the observed scene, which can be used for scene understanding and has a wide
range of applications. However, most of the existing classification methods
were extensively tested on indoor datasets, and when applied to outdoor
datasets they still face problems including sparse labeled targets, differences
in land-covers scales, and long-tailed distributions. To address the above
issues, an enhanced classification method based on adaptive multi-scale fusion
for MPCs with long-tailed distributions is proposed. In the training set
generation stage, a grid-balanced sampling strategy is designed to reliably
generate training samples from sparse labeled datasets. In the feature learning
stage, a multi-scale feature fusion module is proposed to fuse shallow features
of land-covers at different scales, addressing the issue of losing fine
features due to scale variations in land-covers. In the classification stage,
an adaptive hybrid loss module is devised to utilize multi-classification heads
with adaptive weights to balance the learning ability of different classes,
improving the classification performance of small classes due to various-scales
and long-tailed distributions in land-covers. Experimental results on three MPC
datasets demonstrate the effectiveness of the proposed method compared with the
state-of-the-art methods.

</details>


### [47] [ProactiveBench: A Comprehensive Benchmark Evaluating Proactive Interactions in Video Large Language Models](https://arxiv.org/abs/2507.09313)
*Yueqian Wang,Xiaojun Meng,Yifan Wang,Huishuai Zhang,Dongyan Zhao*

Main category: cs.CV

TL;DR: 提出ProactiveBench基准测试和PAUC指标，用于评估和衡量多模态对话系统的Проактивность。


<details>
  <summary>Details</summary>
Motivation: 随着多模态对话系统研究的重点日益增长，主动交互的能力逐渐受到认可。用户期望多模态系统能够更加主动，例如在视频播放过程中自主确定多轮响应的时机。

Method: 提出了一种名为PAUC的新指标，该指标考虑了模型响应的时间动态，能够更准确地评估主动交互设置下的系统性能。通过在ProactiveBench上对各种基线系统进行广泛的基准测试和用户研究，证明了PAUC与用户偏好的_更好_的一致性。

Result: 开发了首个全面的基准测试ProactiveBench，用于评估系统在主动交互方面的能力，并提出了首个能考虑模型响应时间动态的新指标PAUC。实验表明，PAUC比传统指标更能反映用户偏好。

Conclusion: PAUC提供了对主动交互场景下用户体验更忠实的评估，与仅考虑响应文本内容的传统评估指标相比，PAUC更能与用户偏好达成一致。

Abstract: With the growing research focus on multimodal dialogue systems, the
capability for proactive interaction is gradually gaining recognition. As an
alternative to conventional turn-by-turn dialogue, users increasingly expect
multimodal systems to be more initiative, for example, by autonomously
determining the timing of multi-turn responses in real time during video
playback. To facilitate progress in this emerging area, we introduce
ProactiveBench, the first comprehensive benchmark to evaluate a system's
ability to engage in proactive interaction. Since model responses are generated
at varying timestamps, we further propose PAUC, the first metric that accounts
for the temporal dynamics of model responses. This enables a more accurate
evaluation of systems operating in proactive settings. Through extensive
benchmarking of various baseline systems on ProactiveBench and a user study of
human preferences, we show that PAUC is in better agreement with human
preferences than traditional evaluation metrics, which typically only consider
the textual content of responses. These findings demonstrate that PAUC provides
a more faithful assessment of user experience in proactive interaction
scenarios. Project homepage:
https://github.com/yellow-binary-tree/ProactiveBench

</details>


### [48] [Dynamic Inter-Class Confusion-Aware Encoder for Audio-Visual Fusion in Human Activity Recognition](https://arxiv.org/abs/2507.09323)
*Kaixuan Cong,Yifan Wang,Rongkun Xue,Yuyang Jiang,Yiming Feng,Jing Yang*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Humans do not understand individual events in isolation; rather, they
generalize concepts within classes and compare them to others. Existing
audio-video pre-training paradigms only focus on the alignment of the overall
audio-video modalities, without considering the reinforcement of distinguishing
easily confused classes through cognitive induction and contrast during
training. This paper proposes the Dynamic Inter-Class Confusion-Aware Encoder
(DICCAE), an encoder that aligns audio-video representations at a fine-grained,
category-level. DICCAE addresses category confusion by dynamically adjusting
the confusion loss based on inter-class confusion degrees, thereby enhancing
the model's ability to distinguish between similar activities. To further
extend the application of DICCAE, we also introduce a novel training framework
that incorporates both audio and video modalities, as well as their fusion. To
mitigate the scarcity of audio-video data in the human activity recognition
task, we propose a cluster-guided audio-video self-supervised pre-training
strategy for DICCAE. DICCAE achieves near state-of-the-art performance on the
VGGSound dataset, with a top-1 accuracy of 65.5%. We further evaluate its
feature representation quality through extensive ablation studies, validating
the necessity of each module.

</details>


### [49] [Fast3D: Accelerating 3D Multi-modal Large Language Models for Efficient 3D Scene Understanding](https://arxiv.org/abs/2507.09334)
*Wencan Huang,Daizong Liu,Wei Hu*

Main category: cs.CV

TL;DR: Fast3D是一个即插即用的框架，通过GAP和SAP技术，为3D MLLMs实现了高效的视觉标记修剪，解决了计算效率问题。


<details>
  <summary>Details</summary>
Motivation: 现有的3D多模态大语言模型（MLLMs）在场景理解方面表现出色，但在实际部署中存在计算效率低下的问题，主要瓶颈在于处理用于3D场景表示的过量物体级视觉标记。尽管2D MLLMs中的视觉标记修剪技术有一定效果，但由于标记结构差异，其在3D领域的应用尚不明确。

Method: 提出了一种即插即用的视觉标记修剪框架Fast3D，包含两个关键技术：1.全局注意力预测（GAP），利用轻量级神经网络预测目标模型的全局注意力分布，实现高效的标记重要性估计；2.样本自适应视觉标记修剪（SAP），通过基于注意力的复杂度评估引入动态标记预算，自动调整层级修剪率。

Result: 通过在五个基准测试上的广泛评估，验证了Fast3D框架的有效性，尤其是在高视觉标记修剪率下，能够显著加速模型运行并保持性能。

Conclusion: 所提出的Fast3D框架通过全局注意力预测（GAP）和样本自适应视觉标记修剪（SAP）技术，有效解决了3D多模态大语言模型（MLLMs）的计算效率问题，实现了在不修改目标模型参数的情况下，根据输入特征动态调整标记修剪率，显著提升了模型性能，尤其是在高视觉标记修剪率下表现优异。

Abstract: While 3D Multi-modal Large Language Models (MLLMs) demonstrate remarkable
scene understanding capabilities, their practical deployment faces critical
challenges due to computational inefficiency. The key bottleneck stems from
processing excessive object-centric visual tokens required for comprehensive 3D
scene representation. Although visual token pruning has shown promise in
accelerating 2D MLLMs, its applicability to 3D domains remains largely
unexplored due to fundamental disparities in token structures. In this paper,
we reveal two critical insights: (1) Significant redundancy exists in
object-level 3D token representations, analogous to patch-level redundancy in
2D systems; (2) Global attention patterns exhibit strong predictive power for
identifying non-essential tokens in 3D contexts. Building on these
observations, we propose Fast3D, a plug-and-play visual token pruning framework
for 3D MLLMs featuring two technical innovations: (1) Global Attention
Prediction (GAP), where a lightweight neural network learns to predict the
global attention distributions of the target model, enabling efficient token
importance estimation for precise pruning guidance; (2) Sample-Adaptive visual
token Pruning (SAP), which introduces dynamic token budgets through
attention-based complexity assessment, automatically adjusting layer-wise
pruning ratios based on input characteristics. Both of these two techniques
operate without modifying the parameters of the target model. Extensive
evaluations across five benchmarks validate the effectiveness of Fast3D,
particularly under high visual token pruning ratios. Code is available at
https://github.com/wencan25/Fast3D

</details>


### [50] [Simplifying Traffic Anomaly Detection with Video Foundation Models](https://arxiv.org/abs/2507.09338)
*Svetlana Orlova,Tommie Kerssies,Brunó B. Englert,Gijs Dubbelman*

Main category: cs.CV

TL;DR: 交通异常检测（TAD）并不一定需要复杂的模型。研究表明，使用基础的视频 Vision Transformer (Video ViT) 配合掩码视频建模（MVM）和领域自适应预训练（DAPT），可以实现高效且性能优越的 TAD 模型。


<details>
  <summary>Details</summary>
Motivation: 该研究的动机是挑战当前交通异常检测（TAD）领域普遍依赖复杂多阶段或多表示融合架构的趋势，并探讨是否可以通过利用强大的预训练基础模型，采用更简单的架构来实现同等甚至更优的性能。受到视觉感知领域研究的启发，该工作旨在证明简单架构配合先进的预训练策略同样能取得优异的 TAD 效果，并实现更高的效率。

Method: 本研究采用了一种简化的、仅编码器的架构，主要使用基础的视频 Vision Transformer (Video ViT)。研究的核心是探索预训练如何驱动 TAD 性能，重点比较了不同预训练策略（弱监督、全监督和自监督掩码视频建模 MVM）以及领域自适应预训练（DAPT）在无标签驾驶视频上的应用效果。

Result: 研究发现，强大的预训练能够使简单的仅编码器模型在性能上媲美甚至超越先进的 TAD 方法，同时显著提高效率。在预训练策略方面，掩码视频建模（MVM）被证明比弱监督和全监督预训练更能提供有效的 TAD 信号。此外，在无标签驾驶视频上进行领域自适应预训练（DAPT）能够进一步提升下游任务的性能，且无需异常样本。

Conclusion: 研究结果强调了预训练的重要性，并表明仅使用基础的视频 Vision Transformer (Video ViT) 架构，通过适当的预训练方法（特别是掩码视频建模（MVM）和领域自适应预训练（DAPT）），可以在效率和性能上超越复杂的多阶段或多表示融合方法，从而实现有效、高效和可扩展的交通异常检测（TAD）模型。

Abstract: Recent methods for ego-centric Traffic Anomaly Detection (TAD) often rely on
complex multi-stage or multi-representation fusion architectures, yet it
remains unclear whether such complexity is necessary. Recent findings in visual
perception suggest that foundation models, enabled by advanced pre-training,
allow simple yet flexible architectures to outperform specialized designs.
Therefore, in this work, we investigate an architecturally simple encoder-only
approach using plain Video Vision Transformers (Video ViTs) and study how
pre-training enables strong TAD performance. We find that: (i) strong
pre-training enables simple encoder-only models to match or even surpass the
performance of specialized state-of-the-art TAD methods, while also being
significantly more efficient; (ii) although weakly- and fully-supervised
pre-training are advantageous on standard benchmarks, we find them less
effective for TAD. Instead, self-supervised Masked Video Modeling (MVM)
provides the strongest signal; and (iii) Domain-Adaptive Pre-Training (DAPT) on
unlabeled driving videos further improves downstream performance, without
requiring anomalous examples. Our findings highlight the importance of
pre-training and show that effective, efficient, and scalable TAD models can be
built with minimal architectural complexity. We release our code,
domain-adapted encoders, and fine-tuned models to support future work:
https://github.com/tue-mps/simple-tad.

</details>


### [51] [Automated Multi-Class Crop Pathology Classification via Convolutional Neural Networks: A Deep Learning Approach for Real-Time Precision Agriculture](https://arxiv.org/abs/2507.09375)
*Sourish Suri,Yifei Shao*

Main category: cs.CV

TL;DR: 该研究利用CNN开发了一个作物病害自动检测和分类系统，提高了检测效率和准确性，并提供治疗建议，可应用于移动平台，助力精准农业发展。


<details>
  <summary>Details</summary>
Motivation: 作物病害严重影响农业生产力和全球粮食安全，尤其是在大规模农业中，早期识别常常滞后或不准确。因此，本研究旨在开发一种能够自动化检测和分类作物病害的系统，以克服传统方法的局限性。

Method: 该研究采用卷积神经网络（CNN）作为核心技术，构建了一个完整的深度学习流程。首先，利用大规模标记数据集进行图像采集；其次，通过调整大小、归一化和数据增强等技术对图像进行预处理；最后，使用TensorFlow和Keras的Sequential API训练CNN模型。该CNN架构包含三个卷积层、最大池化层、展平层和全连接层，并使用Softmax激活函数进行多类别分类。

Result: 该CNN系统在训练集上达到了约90%的准确率，并在未见过的数据上表现出可靠的性能，尽管验证准确率约为60%，表明存在轻微的过拟合。此外，该模型还集成了治疗推荐模块，能够根据检测到的病害提供相应的农药或杀菌剂干预措施。该解决方案已部署在一个开源、移动兼容的平台上，能够为偏远地区的农民提供实时的图像诊断。

Conclusion: 该研究提出了一种基于卷积神经网络（CNN）的图像分类系统，用于自动化检测和分类八种常见的作物病害。该系统在处理大规模数据集和提供实时诊断方面表现出高准确性和实用性，并集成了治疗建议模块。该方法为精准农业提供了一个可扩展且易于使用的工具，有助于减少对人工检查的依赖，并促进可持续的病虫害管理实践，最终增强全球粮食生产的韧性。

Abstract: Crop diseases present a significant barrier to agricultural productivity and
global food security, especially in large-scale farming where early
identification is often delayed or inaccurate. This research introduces a
Convolutional Neural Network (CNN)-based image classification system designed
to automate the detection and classification of eight common crop diseases
using leaf imagery. The methodology involves a complete deep learning pipeline:
image acquisition from a large, labeled dataset, preprocessing via resizing,
normalization, and augmentation, and model training using TensorFlow with
Keras' Sequential API. The CNN architecture comprises three convolutional
layers with increasing filter sizes and ReLU activations, followed by max
pooling, flattening, and fully connected layers, concluding with a softmax
output for multi-class classification. The system achieves high training
accuracy (~90%) and demonstrates reliable performance on unseen data, although
a validation accuracy of ~60% suggests minor overfitting. Notably, the model
integrates a treatment recommendation module, providing actionable guidance by
mapping each detected disease to suitable pesticide or fungicide interventions.
Furthermore, the solution is deployed on an open-source, mobile-compatible
platform, enabling real-time image-based diagnostics for farmers in remote
areas. This research contributes a scalable and accessible tool to the field of
precision agriculture, reducing reliance on manual inspection and promoting
sustainable disease management practices. By merging deep learning with
practical agronomic support, this work underscores the potential of CNNs to
transform crop health monitoring and enhance food production resilience on a
global scale.

</details>


### [52] [GreenCrossingAI: A Camera Trap/Computer Vision Pipeline for Environmental Science Research Groups](https://arxiv.org/abs/2507.09410)
*Bernie Boscoe,Shawn Johnson,Andrea Osborn,Chandler Campbell,Karen Mager*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Camera traps have long been used by wildlife researchers to monitor and study
animal behavior, population dynamics, habitat use, and species diversity in a
non-invasive and efficient manner. While data collection from the field has
increased with new tools and capabilities, methods to develop, process, and
manage the data, especially the adoption of ML/AI tools, remain challenging.
These challenges include the sheer volume of data generated, the need for
accurate labeling and annotation, variability in environmental conditions
affecting data quality, and the integration of ML/AI tools into existing
workflows that often require domain-specific customization and computational
resources. This paper provides a guide to a low-resource pipeline to process
camera trap data on-premise, incorporating ML/AI capabilities tailored for
small research groups with limited resources and computational expertise. By
focusing on practical solutions, the pipeline offers accessible approaches for
data transmission, inference, and evaluation, enabling researchers to discover
meaningful insights from their ever-increasing camera trap datasets.

</details>


### [53] [Domain Adaptation and Multi-view Attention for Learnable Landmark Tracking with Sparse Data](https://arxiv.org/abs/2507.09420)
*Timothy Chase Jr,Karthik Dantu*

Main category: cs.CV

TL;DR: 该研究提出了一种新颖、轻量级且高效的神经网络方法，用于在航天器上实时检测和跟踪天体地标，克服了传统方法的局限性，并在视点变化下表现出鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统的光调量法在处理和计算能力方面存在限制，并且需要大量的先验成像和离线处理。为了克服这些缺点并提高航天器的自主性，本研究提出了一种新的基于学习的方法，用于在航天器硬件的计算限制内实时进行地标跟踪。

Method: 本研究利用轻量级、计算效率高且专为当前一代航天器飞行处理器设计的神经网络架构。对于地标检测，我们提出了改进的域适应方法，并使用易于获取的训练数据来识别天体地形特征。对于地标描述，我们引入了一种新颖的注意力对齐公式化方法，以学习稳健的特征表示，从而在显著的地标视点变化下保持对应关系。

Result: 所提出的方法在识别地标和处理视点变化方面表现出了优越的性能，能够实时运行，并减少了对大量标注训练数据的需求。

Conclusion: 本研究提出了一种新颖的在原位通过检测和描述进行地标跟踪的公式化方法，并取得了优于现有最先进技术的性能。

Abstract: The detection and tracking of celestial surface terrain features are crucial
for autonomous spaceflight applications, including Terrain Relative Navigation
(TRN), Entry, Descent, and Landing (EDL), hazard analysis, and scientific data
collection. Traditional photoclinometry-based pipelines often rely on extensive
a priori imaging and offline processing, constrained by the computational
limitations of radiation-hardened systems. While historically effective, these
approaches typically increase mission costs and duration, operate at low
processing rates, and have limited generalization. Recently, learning-based
computer vision has gained popularity to enhance spacecraft autonomy and
overcome these limitations. While promising, emerging techniques frequently
impose computational demands exceeding the capabilities of typical spacecraft
hardware for real-time operation and are further challenged by the scarcity of
labeled training data for diverse extraterrestrial environments. In this work,
we present novel formulations for in-situ landmark tracking via detection and
description. We utilize lightweight, computationally efficient neural network
architectures designed for real-time execution on current-generation spacecraft
flight processors. For landmark detection, we propose improved domain
adaptation methods that enable the identification of celestial terrain features
with distinct, cheaply acquired training data. Concurrently, for landmark
description, we introduce a novel attention alignment formulation that learns
robust feature representations that maintain correspondence despite significant
landmark viewpoint variations. Together, these contributions form a unified
system for landmark tracking that demonstrates superior performance compared to
existing state-of-the-art techniques.

</details>


### [54] [SegVec3D: A Method for Vector Embedding of 3D Objects Oriented Towards Robot manipulation](https://arxiv.org/abs/2507.09459)
*Zhihan Kang,Boyu Wang*

Main category: cs.CV

TL;DR: SegVec3D 是一种用于 3D 点云实例分割的新型框架，通过集成注意力、嵌入和跨模态对齐来实现无监督分割和零样本检索。


<details>
  <summary>Details</summary>
Motivation: 提出一种用于 3D 点云实例分割的新型框架，该框架能够实现无监督实例分割和跨模态理解。

Method: SegVec3D 框架集成了注意力机制、嵌入学习和跨模态对齐。该方法构建了一个分层特征提取器来增强几何结构建模，并通过对比聚类实现无监督实例分割。它还将 3D 数据与自然语言查询对齐到一个共享的语义空间中，支持零样本检索。

Result: 与 Mask3D 和 ULIP 等最新方法相比，SegVec3D 在实例分割和多模态理解方面取得了有竞争力的结果，同时具有更少的监督和更好的可部署性。

Conclusion: SegVec3D 框架成功地将实例分割和多模态理解统一起来，具有最小的监督和实际部署能力。

Abstract: We propose SegVec3D, a novel framework for 3D point cloud instance
segmentation that integrates attention mechanisms, embedding learning, and
cross-modal alignment. The approach builds a hierarchical feature extractor to
enhance geometric structure modeling and enables unsupervised instance
segmentation via contrastive clustering. It further aligns 3D data with natural
language queries in a shared semantic space, supporting zero-shot retrieval.
Compared to recent methods like Mask3D and ULIP, our method uniquely unifies
instance segmentation and multimodal understanding with minimal supervision and
practical deployability.

</details>


### [55] [Efficient Multi-Person Motion Prediction by Lightweight Spatial and Temporal Interactions](https://arxiv.org/abs/2507.09446)
*Yuanhong Zheng,Ruixuan Yu,Jian Sun*

Main category: cs.CV

TL;DR: 提出了一种计算高效的3D多人运动预测模型，通过简化交互和引入跨级别交互块，在保持高性能的同时降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 3D多人运动预测由于个体过去运动和智能体之间交互的依赖性而变得非常复杂。此外，有效模拟这些交互通常会产生高昂的计算成本。

Method: 提出了一种计算高效的模型，通过简化空间和时间交互。设计了轻量级双分支来分别学习个体和多个个体的局部和全局表示。引入了新颖的跨级别交互块来整合两个分支的时空表示。明确加入了空间人际距离嵌入来增强交互建模。

Result: 在CMU-Mocap、MuPoTS-3D和3DPW数据集上实现了最先进的性能，同时显著降低了计算成本。

Conclusion: 该模型在CMU-Mocap、MuPoTS-3D和3DPW标准数据集上实现了最先进的性能，同时显著降低了计算成本。

Abstract: 3D multi-person motion prediction is a highly complex task, primarily due to
the dependencies on both individual past movements and the interactions between
agents. Moreover, effectively modeling these interactions often incurs
substantial computational costs. In this work, we propose a computationally
efficient model for multi-person motion prediction by simplifying spatial and
temporal interactions. Our approach begins with the design of lightweight dual
branches that learn local and global representations for individual and
multiple persons separately. Additionally, we introduce a novel cross-level
interaction block to integrate the spatial and temporal representations from
both branches. To further enhance interaction modeling, we explicitly
incorporate the spatial inter-person distance embedding. With above efficient
temporal and spatial design, we achieve state-of-the-art performance for
multiple metrics on standard datasets of CMU-Mocap, MuPoTS-3D, and 3DPW, while
significantly reducing the computational cost. Code is available at
https://github.com/Yuanhong-Zheng/EMPMP.

</details>


### [56] [CKAA: Cross-subspace Knowledge Alignment and Aggregation for Robust Continual Learning](https://arxiv.org/abs/2507.09471)
*Lingfeng He,De Cheng,Zhiheng Ma,Huaijie Wang,Dingwen Zhang,Nannan Wang,Xinbo Gao*

Main category: cs.CV

TL;DR: CKAA框架通过对齐跨子空间特征分布和任务置信度引导的知识聚合，提高了模型在连续学习中应对错误任务ID的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于PEFT的连续学习方法通常为每个任务分配独特的子模块，并使用任务识别器在测试时选择子模块。然而，由于独立训练的子模块存在特征子空间不对齐的问题，这些方法在面对误导性任务ID时容易做出模糊的决策。为了解决这个问题，需要一种能够增强模型在误导性任务ID下的鲁棒性的方法。

Method: 本文提出了一种名为跨子空间知识对齐与聚合（CKAA）的新型框架。该框架包含两个关键创新：1. 双层知识对齐（DKA）：通过对齐不同子空间中的类内特征分布，并利用特征模拟过程学习全局分类器，使得模型在训练时能够区分正确和错误子空间中的特征。2. 任务置信度引导的适配器混合（TC-MoA）：一种鲁棒的推理方案，根据任务置信度分数自适应地聚合特定任务的知识，避免在错误任务ID预测上过度自信。

Result: 实验结果表明，CKAA框架在性能上优于现有的基于PEFT的连续学习方法。

Conclusion: CKAA框架通过双层知识对齐（DKA）和任务置信度引导的适配器混合（TC-MoA）的创新，显著提高了模型在面对错误任务ID时的鲁棒性，并在连续学习任务中取得了优于现有基于PEFT的连续学习方法的性能。

Abstract: Continual Learning (CL) empowers AI models to continuously learn from
sequential task streams. Recently, parameter-efficient fine-tuning (PEFT)-based
CL methods have garnered increasing attention due to their superior
performance. They typically allocate a unique sub-module for learning each
task, with a task recognizer to select the appropriate sub-modules for testing
images. However, due to the feature subspace misalignment from independently
trained sub-modules, these methods tend to produce ambiguous decisions under
misleading task-ids. To address this, we propose Cross-subspace Knowledge
Alignment and Aggregation (CKAA), a novel framework that enhances model
robustness against misleading task-ids through two key innovations: (1)
Dual-level Knowledge Alignment (DKA): By aligning intra-class feature
distributions across different subspaces and learning a robust global
classifier through a feature simulation process, DKA enables the model to
distinguish features from both correct and incorrect subspaces during training.
(2) Task-Confidence-guided Mixture of Adapters (TC-MoA): A robust inference
scheme that adaptively aggregates task-specific knowledge from relevant
sub-modules based on task-confidence scores, avoiding overconfidence in
misleading task-id predictions. Extensive experiments demonstrate that CKAA
outperforms existing PEFT-based CL methods.

</details>


### [57] [HMID-Net: An Exploration of Masked Image Modeling and Knowledge Distillation in Hyperbolic Space](https://arxiv.org/abs/2507.09487)
*Changli Wang,Fang Yin,Jiafeng Liu,Rui Wu*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Visual and semantic concepts are often structured in a hierarchical manner.
For instance, textual concept `cat' entails all images of cats. A recent study,
MERU, successfully adapts multimodal learning techniques from Euclidean space
to hyperbolic space, effectively capturing the visual-semantic hierarchy.
However, a critical question remains: how can we more efficiently train a model
to capture and leverage this hierarchy? In this paper, we propose the
\textit{Hyperbolic Masked Image and Distillation Network} (HMID-Net), a novel
and efficient method that integrates Masked Image Modeling (MIM) and knowledge
distillation techniques within hyperbolic space. To the best of our knowledge,
this is the first approach to leverage MIM and knowledge distillation in
hyperbolic space to train highly efficient models. In addition, we introduce a
distillation loss function specifically designed to facilitate effective
knowledge transfer in hyperbolic space. Our experiments demonstrate that MIM
and knowledge distillation techniques in hyperbolic space can achieve the same
remarkable success as in Euclidean space. Extensive evaluations show that our
method excels across a wide range of downstream tasks, significantly
outperforming existing models like MERU and CLIP in both image classification
and retrieval.

</details>


### [58] [GLIMPSE: Do Large Vision-Language Models Truly Think With Videos or Just Glimpse at Them?](https://arxiv.org/abs/2507.09491)
*Yiyang Zhou,Linjie Li,Shi Qiu,Zhengyuan Yang,Yuyang Zhao,Siwei Han,Yangfan He,Kangqi Li,Haonian Ji,Zihao Zhao,Haibo Tong,Lijuan Wang,Huaxiu Yao*

Main category: cs.CV

TL;DR: GLIMPSE是一个新的视频基准测试，用于评估大型语言模型的视频推理能力，发现现有模型在深度理解视频内容方面表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有视频基准测试模仿图像基准测试，使得模型能够通过扫描关键帧而非进行深度时序推理来回答问题，这阻碍了对LVLMs视频推理能力的评估。

Method: 提出GLIMPSE基准测试，包含3,269个视频和4,342个问题，涵盖轨迹分析、时间推理和司法鉴定等11个类别，旨在评估LVLMs的深度时序推理能力，而非表面帧分析。

Result: GLIMPSE在人类评估中达到94.82%的准确率，而当前表现最佳的LVLM（GPT-o3）仅达到66.43%，表明LVLMs在真正理解和推理视频内容方面仍有很大提升空间。

Conclusion: GLIMPSE基准测试揭示了当前大型视觉语言模型（LVLMs）在进行视频内容深度分析方面仍存在显著挑战，即使是表现最好的模型也难以超越表面推理的局限。

Abstract: Existing video benchmarks often resemble image-based benchmarks, with
question types like "What actions does the person perform throughout the
video?" or "What color is the woman's dress in the video?" For these, models
can often answer by scanning just a few key frames, without deep temporal
reasoning. This limits our ability to assess whether large vision-language
models (LVLMs) can truly think with videos rather than perform superficial
frame-level analysis. To address this, we introduce GLIMPSE, a benchmark
specifically designed to evaluate whether LVLMs can genuinely think with
videos. Unlike prior benchmarks, GLIMPSE emphasizes comprehensive video
understanding beyond static image cues. It consists of 3,269 videos and over
4,342 highly visual-centric questions across 11 categories, including
Trajectory Analysis, Temporal Reasoning, and Forensics Detection. All questions
are carefully crafted by human annotators and require watching the entire video
and reasoning over full video context-this is what we mean by thinking with
video. These questions cannot be answered by scanning selected frames or
relying on text alone. In human evaluations, GLIMPSE achieves 94.82% accuracy,
but current LVLMs face significant challenges. Even the best-performing model,
GPT-o3, reaches only 66.43%, highlighting that LVLMs still struggle to move
beyond surface-level reasoning to truly think with videos.

</details>


### [59] [LifelongPR: Lifelong knowledge fusion for point cloud place recognition based on replay and prompt learning](https://arxiv.org/abs/2507.10034)
*Xianghong Zou,Jianping Li,Zhe Chen,Zhen Cao,Zhen Dong,Qiegen Liu,Bisheng Yang*

Main category: cs.CV

TL;DR: 提出LifelongPR框架，通过改进的样本选择和基于提示学习的方法来解决点云场景识别中的灾难性遗忘和域转移问题，提升了模型性能和实用性。


<details>
  <summary>Details</summary>
Motivation: 现有的点云场景识别（PCPR）模型在适应新环境或传感器类型时，会因为灾难性遗忘而导致先前学习场景的性能显著下降，从而影响模型的可扩展性、维护成本和部署的实用性。

Method: LifelongPR框架，包括一个动态分配样本量的重放样本选择方法，以及一个包含轻量级提示模块和两阶段训练策略的基于提示学习的持续学习框架。

Result: 与最先进的方法相比，LifelongPR在mIR@1上提高了6.50%，在mR@1上提高了7.96%，在F上降低了8.95%。

Conclusion: LifelongPR框架通过动态分配样本量和基于提示学习的框架来解决灾难性遗忘和域转移问题，在大型公共和自收集数据集的实验中表现优于现有方法。

Abstract: Point cloud place recognition (PCPR) plays a crucial role in photogrammetry
and robotics applications such as autonomous driving, intelligent
transportation, and augmented reality. In real-world large-scale deployments of
a positioning system, PCPR models must continuously acquire, update, and
accumulate knowledge to adapt to diverse and dynamic environments, i.e., the
ability known as continual learning (CL). However, existing PCPR models often
suffer from catastrophic forgetting, leading to significant performance
degradation in previously learned scenes when adapting to new environments or
sensor types. This results in poor model scalability, increased maintenance
costs, and system deployment difficulties, undermining the practicality of
PCPR. To address these issues, we propose LifelongPR, a novel continual
learning framework for PCPR, which effectively extracts and fuses knowledge
from sequential point cloud data. First, to alleviate the knowledge loss, we
propose a replay sample selection method that dynamically allocates sample
sizes according to each dataset's information quantity and selects spatially
diverse samples for maximal representativeness. Second, to handle domain
shifts, we design a prompt learning-based CL framework with a lightweight
prompt module and a two-stage training strategy, enabling domain-specific
feature adaptation while minimizing forgetting. Comprehensive experiments on
large-scale public and self-collected datasets are conducted to validate the
effectiveness of the proposed method. Compared with state-of-the-art (SOTA)
methods, our method achieves 6.50% improvement in mIR@1, 7.96% improvement in
mR@1, and an 8.95% reduction in F. The code and pre-trained models are publicly
available at https://github.com/zouxianghong/LifelongPR.

</details>


### [60] [SDTN and TRN: Adaptive Spectral-Spatial Feature Extraction for Hyperspectral Image Classification](https://arxiv.org/abs/2507.09492)
*Fuyin Ye,Erwen Yao,Jianyong Chen,Fengmei He,Junxiang Zhang,Lihao Ni*

Main category: cs.CV

TL;DR: 提出SDTN和TRN网络，通过张量分解和正则化有效解决高光谱图像分类中的挑战，在提高精度的同时降低了计算成本，适用于实时应用。


<details>
  <summary>Details</summary>
Motivation: 解决了高光谱图像分类中传统方法面临的高维数据、光谱空间冗余以及标记样本稀缺的问题，这些问题常导致性能不佳。

Method: 提出了一种名为SDTN（自适应张量正则化网络）的方法，结合张量分解和正则化机制动态调整张量秩以优化特征表示；在此基础上，提出TRN（张量正则化网络），将SDTN提取的特征整合到一个轻量级网络中，以捕捉多尺度的光谱空间特征。

Result: SDTN和TRN框架在PaviaU数据集上展示了显著的准确性提升和更少的模型参数，证明了其在捕捉光谱空间特征和降低计算复杂性方面的有效性。

Conclusion: 所提出的SDTN和TRN框架在PaviaU数据集上实现了显著的分类精度提升和模型参数减少，优于现有最先进的方法，适用于资源受限环境下的实时部署。

Abstract: Hyperspectral image classification plays a pivotal role in precision
agriculture, providing accurate insights into crop health monitoring, disease
detection, and soil analysis. However, traditional methods struggle with
high-dimensional data, spectral-spatial redundancy, and the scarcity of labeled
samples, often leading to suboptimal performance. To address these challenges,
we propose the Self-Adaptive Tensor- Regularized Network (SDTN), which combines
tensor decomposition with regularization mechanisms to dynamically adjust
tensor ranks, ensuring optimal feature representation tailored to the
complexity of the data. Building upon SDTN, we propose the Tensor-Regularized
Network (TRN), which integrates the features extracted by SDTN into a
lightweight network capable of capturing spectral-spatial features at multiple
scales. This approach not only maintains high classification accuracy but also
significantly reduces computational complexity, making the framework highly
suitable for real-time deployment in resource-constrained environments.
Experiments on PaviaU datasets demonstrate significant improvements in accuracy
and reduced model parameters compared to state-of-the-art methods.

</details>


### [61] [Advancing Reliable Test-Time Adaptation of Vision-Language Models under Visual Variations](https://arxiv.org/abs/2507.09500)
*Yiwen Liang,Hui Chen,Yizhe Xiong,Zihan Zhou,Mengyao Lyu,Zijia Lin,Shuaicheng Niu,Sicheng Zhao,Jungong Han,Guiguang Ding*

Main category: cs.CV

TL;DR: 由于在下游任务中缺乏标签数据，在分布变化下零样本能力有限，这激发了在推理过程中改进视觉语言模型（VLMs）性能而不带标签的测试时间适应（TTA）。缓存方法通过在动态缓存中保存低熵样本的历史知识来促进有效的适应，但它们面临两个关键的可靠性挑战：1. 分布变化下的熵不可靠会导致缓存错误累积和性能下降。2. 不灵活的决策边界无法适应大的下游变化，导致最终预测不可靠。ReTA方法整合了两种互补策略来增强可靠性：1. 一致性感知熵重加权（CER），它结合一致性约束来加权熵，用于缓存更新，以维持高质量缓存并促进更稳健的适应。2. 多样性驱动分布校准（DDC），它将类别文本嵌入建模为多元高斯分布，为更准确的预测启用自适应决策边界。ReTA方法在最具挑战性的现实世界分布变化下，持续优于最先进的方法。


<details>
  <summary>Details</summary>
Motivation: 由于在下游任务中缺乏标签数据，在分布变化下零样本能力有限，这激发了在推理过程中改进视觉语言模型（VLMs）性能而不带标签的测试时间适应（TTA）。缓存方法通过在动态缓存中保存低熵样本的历史知识来促进有效的适应，但它们面临两个关键的可靠性挑战：1. 分布变化下的熵不可靠会导致缓存错误累积和性能下降。2. 不灵活的决策边界无法适应大的下游变化，导致最终预测不可靠。

Method: ReTA方法整合了两种互补策略来增强可靠性：1. 一致性感知熵重加权（CER），它结合一致性约束来加权熵，用于缓存更新，以维持高质量缓存并促进更稳健的适应。2. 多样性驱动分布校准（DDC），它将类别文本嵌入建模为多元高斯分布，为更准确的预测启用自适应决策边界。

Result: ReTA方法在最具挑战性的现实世界分布变化下，持续优于最先进的方法。

Conclusion: ReTA方法在最具挑战性的现实世界分布变化下，持续优于最先进的方法。

Abstract: Vision-language models (VLMs) exhibit remarkable zero-shot capabilities but
struggle with distribution shifts in downstream tasks when labeled data is
unavailable, which has motivated the development of Test-Time Adaptation (TTA)
to improve VLMs' performance during inference without annotations. Among
various TTA approaches, cache-based methods show promise by preserving
historical knowledge from low-entropy samples in a dynamic cache and fostering
efficient adaptation. However, these methods face two critical reliability
challenges: (1) entropy often becomes unreliable under distribution shifts,
causing error accumulation in the cache and degradation in adaptation
performance; (2) the final predictions may be unreliable due to inflexible
decision boundaries that fail to accommodate large downstream shifts. To
address these challenges, we propose a Reliable Test-time Adaptation (ReTA)
method that integrates two complementary strategies to enhance reliability from
two perspectives. First, to mitigate the unreliability of entropy as a sample
selection criterion for cache construction, we introduce Consistency-aware
Entropy Reweighting (CER), which incorporates consistency constraints to weight
entropy during cache updating. While conventional approaches rely solely on low
entropy for cache prioritization and risk introducing noise, our method
leverages predictive consistency to maintain a high-quality cache and
facilitate more robust adaptation. Second, we present Diversity-driven
Distribution Calibration (DDC), which models class-wise text embeddings as
multivariate Gaussian distributions, enabling adaptive decision boundaries for
more accurate predictions across visually diverse content. Extensive
experiments demonstrate that ReTA consistently outperforms state-of-the-art
methods, particularly under challenging real-world distribution shifts.

</details>


### [62] [Privacy-Preserving Multi-Stage Fall Detection Framework with Semi-supervised Federated Learning and Robotic Vision Confirmation](https://arxiv.org/abs/2507.10474)
*Seyed Alireza Rahimi Azghadi,Truong-Thanh-Hung Nguyen,Helene Fournier,Monica Wachowicz,Rene Richard,Francis Palma,Hung Cao*

Main category: cs.CV

TL;DR: 本研究提出了一种结合联邦学习、室内定位和视觉识别的多系统框架，用于高精度、保护隐私的老年人跌倒检测。


<details>
  <summary>Details</summary>
Motivation: 老年人跌倒是一个日益严峻的问题，及时检测跌倒能够有效节省医疗费用和康复时间。然而，现有的检测系统需要在有效性和可靠性之间取得平衡，同时还要解决用户隐私问题。

Method: 本研究提出一个包含半监督联邦学习跌倒检测系统 (SF2D)、室内定位与导航系统以及基于视觉的人员跌倒识别系统的框架。SF2D 使用穿戴式设备和边缘设备来识别跌倒场景；室内定位与导航系统负责定位跌倒位置并将机器人导航到现场；而基于视觉的检测系统则通过机器人上的摄像头识别跌倒人员。

Result: SF2D 的失效率为 0.81% (准确率为 99.19%)，基于视觉的跌倒人员检测准确率为 96.3%。结合导航系统的成功率 (95%)，该框架整体准确率高达 99.99%。

Conclusion: 该框架的准确率为 99.99%，能够有效解决老年人跌倒检测中的隐私问题。

Abstract: The aging population is growing rapidly, and so is the danger of falls in
older adults. A major cause of injury is falling, and detection in time can
greatly save medical expenses and recovery time. However, to provide timely
intervention and avoid unnecessary alarms, detection systems must be effective
and reliable while addressing privacy concerns regarding the user. In this
work, we propose a framework for detecting falls using several complementary
systems: a semi-supervised federated learning-based fall detection system
(SF2D), an indoor localization and navigation system, and a vision-based human
fall recognition system. A wearable device and an edge device identify a fall
scenario in the first system. On top of that, the second system uses an indoor
localization technique first to localize the fall location and then navigate a
robot to inspect the scenario. A vision-based detection system running on an
edge device with a mounted camera on a robot is used to recognize fallen
people. Each of the systems of this proposed framework achieves different
accuracy rates. Specifically, the SF2D has a 0.81% failure rate equivalent to
99.19% accuracy, while the vision-based fallen people detection achieves 96.3%
accuracy. However, when we combine the accuracy of these two systems with the
accuracy of the navigation system (95% success rate), our proposed framework
creates a highly reliable performance for fall detection, with an overall
accuracy of 99.99%. Not only is the proposed framework safe for older adults,
but it is also a privacy-preserving solution for detecting falls.

</details>


### [63] [Online Micro-gesture Recognition Using Data Augmentation and Spatial-Temporal Attention](https://arxiv.org/abs/2507.09512)
*Pengyu Liu,Kun Li,Fei Wang,Yanyan Wei,Junhui She,Dan Guo*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this paper, we introduce the latest solution developed by our team,
HFUT-VUT, for the Micro-gesture Online Recognition track of the IJCAI 2025 MiGA
Challenge. The Micro-gesture Online Recognition task is a highly challenging
problem that aims to locate the temporal positions and recognize the categories
of multiple micro-gesture instances in untrimmed videos. Compared to
traditional temporal action detection, this task places greater emphasis on
distinguishing between micro-gesture categories and precisely identifying the
start and end times of each instance. Moreover, micro-gestures are typically
spontaneous human actions, with greater differences than those found in other
human actions. To address these challenges, we propose hand-crafted data
augmentation and spatial-temporal attention to enhance the model's ability to
classify and localize micro-gestures more accurately. Our solution achieved an
F1 score of 38.03, outperforming the previous state-of-the-art by 37.9%. As a
result, our method ranked first in the Micro-gesture Online Recognition track.

</details>


### [64] [QuarterMap: Efficient Post-Training Token Pruning for Visual State Space Models](https://arxiv.org/abs/2507.09514)
*Tien-Yu Chi,Hung-Yueh Chiang,Diana Marculescu,Kai-Chiang Wu*

Main category: cs.CV

TL;DR: QuarterMap 是一种新的训练后剪枝技术，可以提高 VMamba 等基于 SSM 的模型的效率，通过移除冗余激活并使用最近邻上采样恢复维度，从而实现加速而几乎没有准确性损失。


<details>
  <summary>Details</summary>
Motivation: VMamba 作为一种基于 SSM 的视觉骨干网络，虽然利用线性递增减少了 Transformer 的二次复杂性，但在其四向扫描中仍然存在空间冗余瓶颈。

Method: QuarterMap 是一种训练后激活剪枝方法，通过移除冗余空间激活来减少 SSM 中的空间冗余，并通过最近邻上采样来恢复维度。

Result: QuarterMap 在 ImageNet-1K 上实现了高达 11% 的 VMamba 加速，准确率下降不到 0.9%，并在 ADE20K 分割任务上取得了相似的提升。此外，在 MedMamba 等特定领域模型上也提高了吞吐量，同时保持了准确性。与 ToMe 等令牌合并方法相比，QuarterMap 专为 SSM 设计，避免了昂贵的合并-取消合并操作。

Conclusion: QuarterMap 是一种即插即用的部署时效率优化工具，可在不影响可迁移性的情况下，通过在扫描前移除冗余空间激活并进行最近邻上采样来恢复维度，从而提高 SSM 视觉骨干的吞吐量。

Abstract: State space models (SSMs) reduce the quadratic complexity of transformers by
leveraging linear recurrence. Recently, VMamba has emerged as a strong
SSM-based vision backbone, yet remains bottlenecked by spatial redundancy in
its four-directional scan. We propose QuarterMap, a post-training activation
pruning method that removes redundant spatial activations before scanning and
restores dimensions via nearest-neighbor upsampling. Our method improves
throughput without retraining. On ImageNet-1K, QuarterMap achieves up to 11%
speedup on VMamba with less than 0.9% accuracy drop, and yields similar gains
on ADE20K segmentation. Beyond VMamba, we validate QuarterMap on MedMamba, a
domain-specific model that shares the same four-directional scanning structure,
where it consistently improves throughput while preserving accuracy across
multiple medical imaging tasks. Compared to token merging methods like ToMe,
QuarterMap is tailored for SSMs and avoids costly merge-unmerge operations. Our
method offers a plug-and-play tool for deployment-time efficiency without
compromising transferability.

</details>


### [65] [When Schrödinger Bridge Meets Real-World Image Dehazing with Unpaired Training](https://arxiv.org/abs/2507.09524)
*Yunwei Lan,Zhigao Cui,Xin Luo,Chang Liu,Nian Wang,Menglin Zhang,Yanzhao Su,Dong Liu*

Main category: cs.CV

TL;DR: 提出了一种名为DehazeSB的新型非配对去雾框架，它利用Schrodinger Bridge和最优传输理论来改进模糊到清晰图像的映射，并通过细节保留正则化和CLIP提示学习来提高性能和一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的非配对去雾方法（尤其是使用GANs的方法）在处理真实世界模糊图像方面表现出有希望的性能，但由于生成器有限的传输映射能力而受到限制，这阻碍了它们在非配对训练范式中的有效性得到充分发挥。

Method: 提出了一种基于Schrodinger Bridge的新型非配对去雾框架DehazeSB，利用最优传输（OT）理论直接连接模糊和清晰图像的分布，从而以更少的步骤实现从模糊到清晰的最佳传输映射。引入了保持细节的正则化，以确保恢复图像中结构信息和细节的一致性，通过强制执行像素级对齐来实现。此外，还提出了一种新颖的提示学习方法，利用预训练的CLIP模型，通过学习感知雾气的视觉-语言对齐来区分模糊图像和清晰图像。

Result: DehazeSB能够生成高质量的去雾图像，并在结构信息和细节方面表现出色。

Conclusion: 本研究提出的DehazeSB框架在多个真实世界数据集的广泛实验中证明了其优越性。

Abstract: Recent advancements in unpaired dehazing, particularly those using GANs, show
promising performance in processing real-world hazy images. However, these
methods tend to face limitations due to the generator's limited transport
mapping capability, which hinders the full exploitation of their effectiveness
in unpaired training paradigms. To address these challenges, we propose
DehazeSB, a novel unpaired dehazing framework based on the Schr\"odinger
Bridge. By leveraging optimal transport (OT) theory, DehazeSB directly bridges
the distributions between hazy and clear images. This enables optimal transport
mappings from hazy to clear images in fewer steps, thereby generating
high-quality results. To ensure the consistency of structural information and
details in the restored images, we introduce detail-preserving regularization,
which enforces pixel-level alignment between hazy inputs and dehazed outputs.
Furthermore, we propose a novel prompt learning to leverage pre-trained CLIP
models in distinguishing hazy images and clear ones, by learning a haze-aware
vision-language alignment. Extensive experiments on multiple real-world
datasets demonstrate our method's superiority. Code:
https://github.com/ywxjm/DehazeSB.

</details>


### [66] [VDInstruct: Zero-Shot Key Information Extraction via Content-Aware Vision Tokenization](https://arxiv.org/abs/2507.09531)
*Son Nguyen,Giang Nguyen,Hung Dao,Thao Do,Daeyoung Kim*

Main category: cs.CV

TL;DR: VDInstruct通过内容感知分词和显式布局建模解决了密集文档理解中的效率和准确性问题，取得了SOTA成果。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型（MLLMs）在处理密集文档时表现不佳，并且依赖于随图像大小扩展的视觉分词方法，导致计算冗余和内存效率低下。

Method: VDInstruct模型将空间区域检测与语义特征提取分离，采用内容感知分词策略，根据文档复杂度生成标记，以保留关键结构并消除冗余标记。通过三阶段训练实现。

Result: VDInstruct在KIE基准测试上取得了最先进（SOTA）的成果，在不牺牲准确性的前提下，将图像标记数量减少了约3.6倍。在零样本评估中，VDInstruct的表现优于DocOwl 1.5等强基线模型，F1分数提高了5.5个百分点。

Conclusion: 内容感知分词与显式布局建模为文档理解提供了一个有前景的方向。

Abstract: Key Information Extraction (KIE) underpins the understanding of visual
documents (e.g., receipts and contracts) by extracting precise semantic content
and accurately capturing spatial structure. Yet existing multimodal large
language models (MLLMs) often perform poorly on dense documents and rely on
vision tokenization approaches that scale with image size, leading to redundant
computation and memory inefficiency. To address these challenges, we introduce
VDInstruct, an MLLM that separates spatial region detection from semantic
feature extraction. Central to our model is a content-aware tokenization
strategy: rather than fragmenting the entire image uniformly, it generates
tokens in proportion to document complexity, preserving critical structure
while eliminating wasted tokens. Leveraging a three-stage training paradigm,
our model achieves state-of-the-art (SOTA) results on KIE benchmarks, matching
or exceeding the accuracy of leading approaches while reducing the number of
image tokens by roughly 3.6x. In zero-shot evaluations, VDInstruct surpasses
strong baselines-such as DocOwl 1.5-by +5.5 F1 points, highlighting its
robustness to unseen documents. These findings show that content-aware
tokenization combined with explicit layout modeling offers a promising
direction forward for document understanding. Data, source code, and model
weights will be made publicly available.

</details>


### [67] [DRPCA-Net: Make Robust PCA Great Again for Infrared Small Target Detection](https://arxiv.org/abs/2507.09541)
*Zihao Xiong,Fei Zhou,Fengyi Wu,Shuai Yuan,Maixia Fu,Zhenming Peng,Jian Yang,Yimian Dai*

Main category: cs.CV

TL;DR: DRPCA-Net是一种新颖的深度展开网络，通过动态展开和残差组来解决红外小目标检测中的复杂性和泛化性问题，并取得了优于现有方法的检测精度。


<details>
  <summary>Details</summary>
Motivation: 解决现有深度学习模型在红外小目标检测中过于复杂、缺乏可解释性、参数效率低下和泛化能力不足的问题，并利用红外小目标固有的稀疏性先验。

Method: 提出了一种名为DRPCA-Net的新型深度展开网络，该网络通过轻量级超网络引入动态展开机制，并设计了动态残差组（DRG）模块以更好地捕捉背景中的上下文变化。

Result: DRPCA-Net能够自适应地生成逐迭代参数，以适应输入场景，从而提高鲁棒性和泛化能力，并在检测精度上显著优于现有方法。

Conclusion: DRPCA-Net在多个公开红外数据集上的广泛实验证明，其检测精度显著优于现有的最先进方法。

Abstract: Infrared small target detection plays a vital role in remote sensing,
industrial monitoring, and various civilian applications. Despite recent
progress powered by deep learning, many end-to-end convolutional models tend to
pursue performance by stacking increasingly complex architectures, often at the
expense of interpretability, parameter efficiency, and generalization. These
models typically overlook the intrinsic sparsity prior of infrared small
targets--an essential cue that can be explicitly modeled for both performance
and efficiency gains. To address this, we revisit the model-based paradigm of
Robust Principal Component Analysis (RPCA) and propose Dynamic RPCA Network
(DRPCA-Net), a novel deep unfolding network that integrates the sparsity-aware
prior into a learnable architecture. Unlike conventional deep unfolding methods
that rely on static, globally learned parameters, DRPCA-Net introduces a
dynamic unfolding mechanism via a lightweight hypernetwork. This design enables
the model to adaptively generate iteration-wise parameters conditioned on the
input scene, thereby enhancing its robustness and generalization across diverse
backgrounds. Furthermore, we design a Dynamic Residual Group (DRG) module to
better capture contextual variations within the background, leading to more
accurate low-rank estimation and improved separation of small targets.
Extensive experiments on multiple public infrared datasets demonstrate that
DRPCA-Net significantly outperforms existing state-of-the-art methods in
detection accuracy. Code is available at https://github.com/GrokCV/DRPCA-Net.

</details>


### [68] [SeqCSIST: Sequential Closely-Spaced Infrared Small Target Unmixing](https://arxiv.org/abs/2507.09556)
*Ximeng Zhai,Bohan Xu,Yaohong Chen,Hao Wang,Kehua Guo,Yimian Dai*

Main category: cs.CV

TL;DR: 该研究提出了序列CSIST解混任务，并开发了一个名为DeRefNet的深度学习模型，通过时间可变形特征对齐来提高对密集红外小目标的检测精度，并在公开数据集上取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 由于光学透镜焦距和红外探测器分辨率的限制，遥远的紧密间隔红外小目标（CSIST）组通常在红外图像中表现为混合斑点，难以检测。

Method: 提出了一种名为可变形精炼网络（DeRefNet）的模型驱动的深度学习框架，其中包含时间可变形特征对齐（TDFA）模块，用于自适应地聚合帧间信息。

Result: 在SeqCSIST数据集上进行的实验表明，所提出的DeRefNet方法比现有最先进的方法表现更优，平均精度均值（mAP）提高了5.3%。

Conclusion: 提出了一种新颖的序列CSIST解混任务，旨在从高度密集的CSIST组中以亚像素本地化的形式检测所有目标。该方法通过引入时间可变形特征对齐（TDFA）模块，实现了自适应的帧间信息聚合，并在SeqCSIST数据集上取得了比最先进方法提高5.3%的mAP性能。

Abstract: Due to the limitation of the optical lens focal length and the resolution of
the infrared detector, distant Closely-Spaced Infrared Small Target (CSIST)
groups typically appear as mixing spots in the infrared image. In this paper,
we propose a novel task, Sequential CSIST Unmixing, namely detecting all
targets in the form of sub-pixel localization from a highly dense CSIST group.
However, achieving such precise detection is an extremely difficult challenge.
In addition, the lack of high-quality public datasets has also restricted the
research progress. To this end, firstly, we contribute an open-source
ecosystem, including SeqCSIST, a sequential benchmark dataset, and a toolkit
that provides objective evaluation metrics for this special task, along with
the implementation of 23 relevant methods. Furthermore, we propose the
Deformable Refinement Network (DeRefNet), a model-driven deep learning
framework that introduces a Temporal Deformable Feature Alignment (TDFA) module
enabling adaptive inter-frame information aggregation. To the best of our
knowledge, this work is the first endeavor to address the CSIST Unmixing task
within a multi-frame paradigm. Experiments on the SeqCSIST dataset demonstrate
that our method outperforms the state-of-the-art approaches with mean Average
Precision (mAP) metric improved by 5.3\%. Our dataset and toolkit are available
from https://github.com/GrokCV/SeqCSIST.

</details>


### [69] [EHPE: A Segmented Architecture for Enhanced Hand Pose Estimation](https://arxiv.org/abs/2507.09560)
*Bolun Zheng,Xinjie Liu,Qianyu Zhang,Canjin Wang,Fangni Chen,Mingen Xu*

Main category: cs.CV

TL;DR: EHPE通过在TW阶段进行局部提取TIP和手腕，减轻了误差累积对TIP预测的影响，并进一步减小了所有关节的预测误差。在PG阶段，该模型采用双分支交互网络来优化剩余关节的位置。EHPE在两个广泛使用的基准上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的手部姿势估计方法忽略了远端指尖（TIP）和手腕在预测整体手部关节时的重要性，并且未能解决远端关节在手势估计中误差累积的现象，这会导致某些关节产生更大的误差，从而在姿势估计中产生错位和伪影，并降低整体重建质量。EHPE旨在通过局部提取TIP和手腕来解决这个问题，从而减轻误差累积对TIP预测的影响，并在此基础上进一步减小所有关节的预测误差。

Method: 提出了一种新颖的分割架构EHPE，包括两个关键阶段：TIP和腕部关节提取阶段（TW阶段）以及先验引导关节估计阶段（PG阶段）。TW阶段估计TIP和腕部关节的位置，为初始的准确关节配置提供依据；PG阶段采用双分支交互网络来优化剩余关节的位置。

Result: EHPE通过局部提取TIP和手腕，减轻了误差累积对TIP预测的影响，并进一步减小了所有关节的预测误差。

Conclusion: EHPE在两个广泛使用的基准上实现了最先进的性能。

Abstract: 3D hand pose estimation has garnered great attention in recent years due to
its critical applications in human-computer interaction, virtual reality, and
related fields. The accurate estimation of hand joints is essential for
high-quality hand pose estimation. However, existing methods neglect the
importance of Distal Phalanx Tip (TIP) and Wrist in predicting hand joints
overall and often fail to account for the phenomenon of error accumulation for
distal joints in gesture estimation, which can cause certain joints to incur
larger errors, resulting in misalignments and artifacts in the pose estimation
and degrading the overall reconstruction quality. To address this challenge, we
propose a novel segmented architecture for enhanced hand pose estimation
(EHPE). We perform local extraction of TIP and wrist, thus alleviating the
effect of error accumulation on TIP prediction and further reduce the
predictive errors for all joints on this basis. EHPE consists of two key
stages: In the TIP and Wrist Joints Extraction stage (TW-stage), the positions
of the TIP and wrist joints are estimated to provide an initial accurate joint
configuration; In the Prior Guided Joints Estimation stage (PG-stage), a
dual-branch interaction network is employed to refine the positions of the
remaining joints. Extensive experiments on two widely used benchmarks
demonstrate that EHPE achieves state-of-the-arts performance. Code is available
at https://github.com/SereinNout/EHPE.

</details>


### [70] [Prompt Engineering in Segment Anything Model: Methodologies, Applications, and Emerging Challenges](https://arxiv.org/abs/2507.09562)
*Yidong Jiang*

Main category: cs.CV

TL;DR: 这项调查首次全面概述了SAM的提示工程技术，重点介绍了其发展、应用、挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: SAM通过其创新的基于提示的方法彻底改变了图像分割，但提示工程在其成功中的关键作用仍未被充分探索。

Method: 对SAM及其变体的提示工程技术进行了全面的调查，系统地组织和分析了该新兴领域的快速增长的工作，涵盖了基本方法、实际应用和关键挑战。

Result: 调查显示，提示工程如何从简单的几何输入演变为复杂的多模态方法，从而能够将SAM适应于包括医学成像和遥感在内的不同领域。确定了提示优化中的独特挑战，并讨论了有希望的研究方向。

Conclusion: 这项调查填补了文献中的一个重要空白，通过提供一个结构化的框架来理解和推进分割基础模型中的提示工程。

Abstract: The Segment Anything Model (SAM) has revolutionized image segmentation
through its innovative prompt-based approach, yet the critical role of prompt
engineering in its success remains underexplored. This paper presents the first
comprehensive survey focusing specifically on prompt engineering techniques for
SAM and its variants. We systematically organize and analyze the rapidly
growing body of work in this emerging field, covering fundamental
methodologies, practical applications, and key challenges. Our review reveals
how prompt engineering has evolved from simple geometric inputs to
sophisticated multimodal approaches, enabling SAM's adaptation across diverse
domains including medical imaging and remote sensing. We identify unique
challenges in prompt optimization and discuss promising research directions.
This survey fills an important gap in the literature by providing a structured
framework for understanding and advancing prompt engineering in foundation
models for segmentation.

</details>


### [71] [WordCraft: Interactive Artistic Typography with Attention Awareness and Noise Blending](https://arxiv.org/abs/2507.09573)
*Zhe Wang,Jingbo Zhang,Tianyi Wei,Wanchao Su,Can Wang*

Main category: cs.CV

TL;DR: WordCraft是一个交互式艺术字体系统，通过结合扩散模型、区域注意力机制、噪声混合和大型语言模型，解决了现有方法在局部编辑、迭代优化和提示词解释方面的局限性，显著增强了艺术字体设计的交互性。


<details>
  <summary>Details</summary>
Motivation: 现有的自动化字体设计方法（特别是基于扩散模型的方法）在交互性方面存在局限，例如缺乏对局部编辑、迭代优化、多字符组合和开放式提示词解释的支持。本研究旨在解决这些问题，提供一个更具交互性的艺术字体设计系统。

Method: WordCraft是一个集成了扩散模型的交互式艺术字体系统，通过无训练的区域注意力机制实现精确的多区域生成，并利用噪声混合技术进行连续优化而不影响视觉质量。此外，它还结合了大型语言模型来解析和构建用户提示，支持对具体和抽象指令的灵活、意图驱动的生成。

Result: WordCraft能够跨多种语言的单个和多个字符输入合成高质量、风格化的字体，支持多样化的以用户为中心的工作流。

Conclusion: 该系统显著增强了艺术字体设计的交互性，为艺术家和设计师开辟了新的创意可能性。

Abstract: Artistic typography aims to stylize input characters with visual effects that
are both creative and legible. Traditional approaches rely heavily on manual
design, while recent generative models, particularly diffusion-based methods,
have enabled automated character stylization. However, existing solutions
remain limited in interactivity, lacking support for localized edits, iterative
refinement, multi-character composition, and open-ended prompt interpretation.
We introduce WordCraft, an interactive artistic typography system that
integrates diffusion models to address these limitations. WordCraft features a
training-free regional attention mechanism for precise, multi-region generation
and a noise blending that supports continuous refinement without compromising
visual quality. To support flexible, intent-driven generation, we incorporate a
large language model to parse and structure both concrete and abstract user
prompts. These components allow our framework to synthesize high-quality,
stylized typography across single- and multi-character inputs across multiple
languages, supporting diverse user-centered workflows. Our system significantly
enhances interactivity in artistic typography synthesis, opening up creative
possibilities for artists and designers.

</details>


### [72] [MENTOR: Efficient Multimodal-Conditioned Tuning for Autoregressive Vision Generation Models](https://arxiv.org/abs/2507.09574)
*Haozhe Zhao,Zefan Cai,Shuzheng Si,Liang Chen,Jiuxiang Gu,Wen Xiao,Junjie Hu*

Main category: cs.CV

TL;DR: MENTOR是一个新颖的自回归框架，通过两阶段训练（多模态对齐和多模态指令调优）解决了现有文本到图像模型的局限性，实现了高效的多模态图像生成，并在DreamBench++基准测试中取得了优于基线和扩散基方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像模型在精确视觉控制、平衡多模态输入以及复杂多模态图像生成的训练方面存在不足。

Method: MENTOR是一个新颖的自回归（AR）框架，结合了AR图像生成器和两阶段训练范式，实现了多模态输入和图像输出之间的细粒度、token级对齐，无需辅助适配器或交叉注意力模块。两阶段训练包括：1）多模态对齐阶段，建立稳健的像素级和语义级对齐；2）多模态指令调优阶段，平衡多模态输入集成并增强生成可控性。

Result: MENTOR在DreamBench++基准测试中取得了强大的性能，在概念保持和提示遵循方面优于竞争性基线模型。此外，与基于扩散的方法相比，该方法在图像重建保真度、任务适应性和训练效率方面均表现更优。

Conclusion: MENTOR框架在DreamBench++基准测试中表现出色，在概念保持和提示遵循方面优于其他基线模型，并且在图像重建保真度、任务适应性和训练效率方面优于基于扩散的方法。

Abstract: Recent text-to-image models produce high-quality results but still struggle
with precise visual control, balancing multimodal inputs, and requiring
extensive training for complex multimodal image generation. To address these
limitations, we propose MENTOR, a novel autoregressive (AR) framework for
efficient Multimodal-conditioned Tuning for Autoregressive multimodal image
generation. MENTOR combines an AR image generator with a two-stage training
paradigm, enabling fine-grained, token-level alignment between multimodal
inputs and image outputs without relying on auxiliary adapters or
cross-attention modules. The two-stage training consists of: (1) a multimodal
alignment stage that establishes robust pixel- and semantic-level alignment,
followed by (2) a multimodal instruction tuning stage that balances the
integration of multimodal inputs and enhances generation controllability.
Despite modest model size, suboptimal base components, and limited training
resources, MENTOR achieves strong performance on the DreamBench++ benchmark,
outperforming competitive baselines in concept preservation and prompt
following. Additionally, our method delivers superior image reconstruction
fidelity, broad task adaptability, and improved training efficiency compared to
diffusion-based methods. Dataset, code, and models are available at:
https://github.com/HaozheZhao/MENTOR

</details>


### [73] [Memory-Augmented SAM2 for Training-Free Surgical Video Segmentation](https://arxiv.org/abs/2507.09577)
*Ming Yin,Fu Wang,Xujiong Ye,Yanda Meng,Zeyu Fu*

Main category: cs.CV

TL;DR: 针对SAM2在手术视频分割中因器械移动、遮挡和交互导致的性能下降问题，提出MA-SAM2策略，通过引入内存模型改进了分割性能，并在无需额外训练的情况下，在EndoVis数据集上实现了性能提升。


<details>
  <summary>Details</summary>
Motivation: SAM2框架在手术视频分割方面存在局限性，这是由其贪婪选择内存设计以及手术视频的快速器械移动、频繁遮挡和复杂的器械-组织交互等特性所放大的，导致在分割复杂、长视频时性能下降。

Method: 提出了一种名为MA-SAM2的训练无关视频对象分割策略，该策略具有新颖的上下文感知和遮挡恢复内存模型。该策略采用多目标、单循环、单提示推理，以提高多器械视频中跟踪过程的效率。

Result: MA-SAM2在遮挡和复杂器械移动引起的交互方面表现出强大的鲁棒性，同时保持了在整个视频中分割对象的准确性。

Conclusion: MA-SAM2在EndoVis2017和EndoVis2018数据集上分别比SAM2的性能提高了4.36%和6.1%，显示出其在实际手术应用中的潜力。

Abstract: Surgical video segmentation is a critical task in computer-assisted surgery,
essential for enhancing surgical quality and patient outcomes. Recently, the
Segment Anything Model 2 (SAM2) framework has demonstrated remarkable
advancements in both image and video segmentation. However, the inherent
limitations of SAM2's greedy selection memory design are amplified by the
unique properties of surgical videos-rapid instrument movement, frequent
occlusion, and complex instrument-tissue interaction-resulting in diminished
performance in the segmentation of complex, long videos. To address these
challenges, we introduce Memory Augmented (MA)-SAM2, a training-free video
object segmentation strategy, featuring novel context-aware and
occlusion-resilient memory models. MA-SAM2 exhibits strong robustness against
occlusions and interactions arising from complex instrument movements while
maintaining accuracy in segmenting objects throughout videos. Employing a
multi-target, single-loop, one-prompt inference further enhances the efficiency
of the tracking process in multi-instrument videos. Without introducing any
additional parameters or requiring further training, MA-SAM2 achieved
performance improvements of 4.36% and 6.1% over SAM2 on the EndoVis2017 and
EndoVis2018 datasets, respectively, demonstrating its potential for practical
surgical applications.

</details>


### [74] [Demystifying Flux Architecture](https://arxiv.org/abs/2507.09595)
*Or Greenberg*

Main category: cs.CV

TL;DR: FLUX.1是一个强大的文本到图像生成模型，本报告对其进行了逆向工程分析，以揭示其架构和训练细节。


<details>
  <summary>Details</summary>
Motivation: 为了支持FLUX.1作为未来研究和开发的骨干，揭秘其架构。

Method: 通过对开源模型FLUX.1的源代码进行逆向工程来分析其架构和训练设置。

Result: 详细说明了FLUX.1的架构和训练设置，使其能够被广泛采用。

Conclusion: FLUX.1是一个先进的文本到图像生成模型，在文本-图像对齐、图像质量和多样性方面表现出色。

Abstract: FLUX.1 is a diffusion-based text-to-image generation model developed by Black
Forest Labs, designed to achieve faithful text-image alignment while
maintaining high image quality and diversity. FLUX is considered
state-of-the-art in text-to-image generation, outperforming popular models such
as Midjourney, DALL-E 3, Stable Diffusion 3 (SD3), and SDXL. Although publicly
available as open source, the authors have not released official technical
documentation detailing the model's architecture or training setup. This report
summarizes an extensive reverse-engineering effort aimed at demystifying FLUX's
architecture directly from its source code, to support its adoption as a
backbone for future research and development. This document is an unofficial
technical report and is not published or endorsed by the original developers or
their affiliated institutions.

</details>


### [75] [Inter2Former: Dynamic Hybrid Attention for Efficient High-Precision Interactive](https://arxiv.org/abs/2507.09612)
*You Huang,Lichao Chen,Jiayi Ji,Liujuan Cao,Shengchuan Zhang,Rongrong Ji*

Main category: cs.CV

TL;DR: Inter2Former通过DPE、DHA、HMoE和DLU等技术，优化了密集令牌处理中的计算分配，解决了交互式分割在CPU上的准确性和效率权衡问题，并在高精度IS基准测试中取得了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 目前的交互式分割（IS）方法在准确性和处理速度之间存在权衡：密集令牌方法精度高但CPU处理慢，而SAM模型速度快但分割质量有损。

Method: Inter2Former通过优化密集令牌处理中的计算分配来解决挑战，具体包括：1. 动态提示嵌入（DPE）自适应处理感兴趣的区域。2. 动态混合注意力（DHA）利用先前的分割掩码将令牌路由到全注意力或BSQ注意力。3. 混合专家模型（HMoE）在FFN模块中应用自适应计算策略。4. 动态局部上采样（DLU）在检测到的区域进行精细上采样。

Result: 实验结果表明，Inter2Former在CPU设备上实现了SOTA性能和高效率。

Conclusion: Inter2Former在CPU设备上实现了SOTA性能和高效率。

Abstract: Interactive segmentation (IS) improves annotation efficiency by segmenting
target regions from user prompts, with widespread applications in real-world
scenarios. Current approaches face a critical trade-off: dense-token methods
achieve superior accuracy and detail preservation but suffer from prohibitively
slow processing on CPU devices, while the Segment Anything Model (SAM) advances
the field with sparse prompt tokens for fast inference but compromises
segmentation quality. In this paper, we propose Inter2Former to address this
challenge by optimizing computation allocation in dense-token processing, which
introduces four key enhancements. First, we propose Dynamic Prompt Embedding
(DPE) that adaptively processes only regions of interest while avoiding
additional overhead from background tokens. Second, we introduce Dynamic Hybrid
Attention (DHA), which leverages previous segmentation masks to route tokens
through either full attention (O(N2)) for boundary regions or our proposed
efficient BSQ attention (O(N)) for non-boundary regions. Third, we develop
Hybrid Mixture of Experts (HMoE), which applies similar adaptive computation
strategies in FFN modules with CPU-optimized parallel processing. Finally, we
present Dynamic Local Upsampling (DLU), a reverse operation of DPE, which
localizes objects with a lightweight MLP and performs fine-grained upsampling
only in detected regions. Experimental results on high-precision IS benchmarks
demonstrate that Inter2Former achieves SOTA performance with high efficiency on
CPU devices.

</details>


### [76] [Towards Fine-Grained Adaptation of CLIP via a Self-Trained Alignment Score](https://arxiv.org/abs/2507.09615)
*Eman Ali,Sathira Silva,Chetan Arora,Muhammad Haris Khan*

Main category: cs.CV

TL;DR: FAIR通过动态对齐局部图像特征和语言嵌入来改进细粒度无监督适应，并在13个数据集上取得了优于SOTA方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的VLMs无监督适应方法要么依赖于固定的对齐分数，要么使用计算成本高昂的伪标签策略。本文提出了一种通过对齐和交互细化（FAIR）来模拟细粒度跨模态交互的方法，以产生更准确、更具区分性的伪标签，从而提高性能。

Method: FAIR通过一组类别描述锚点（CDA）动态地将局部图像特征与描述性语言嵌入对齐，从而实现了学习到的对齐分数（LAS），它将CDA作为自适应分类器，促进跨模态交互以改进无监督适应中的自训练。此外，还提出了一种自训练加权机制来处理类别间模糊性。

Result: FAIR在13个细粒度数据集上的整体表现比SOTA方法平均提高了2.78%。

Conclusion: FAIR在细粒度无监督适应方面提供了显著的性能提升，在13个细粒度数据集上与SOTA方法相比，总体提升了2.78%。

Abstract: Vision-language models (VLMs) like CLIP excel in zero-shot learning by
aligning image and text representations through contrastive pretraining.
Existing approaches to unsupervised adaptation (UA) for fine-grained
classification with VLMs either rely on fixed alignment scores that cannot
capture evolving, subtle class distinctions or use computationally expensive
pseudo-labeling strategies that limit scalability. In contrast, we show that
modeling fine-grained cross-modal interactions during adaptation produces more
accurate, class-discriminative pseudo-labels and substantially improves
performance over state-of-the-art (SOTA) methods. We introduce Fine-grained
Alignment and Interaction Refinement (FAIR), an innovative approach that
dynamically aligns localized image features with descriptive language
embeddings through a set of Class Description Anchors (CDA). This enables the
definition of a Learned Alignment Score (LAS), which incorporates CDA as an
adaptive classifier, facilitating cross-modal interactions to improve
self-training in unsupervised adaptation. Furthermore, we propose a
self-training weighting mechanism designed to refine pseudo-labels in the
presence of inter-class ambiguities. Our approach, FAIR, delivers a substantial
performance boost in fine-grained unsupervised adaptation, achieving a notable
overall gain of 2.78% across 13 fine-grained datasets compared to SOTA methods.

</details>


### [77] [ViTCoT: Video-Text Interleaved Chain-of-Thought for Boosting Video Understanding in Large Language Models](https://arxiv.org/abs/2507.09876)
*Yongheng Zhang,Xu Liu,Ruihan Tao,Qiguang Chen,Hao Fei,Wanxiang Che,Libo Qin*

Main category: cs.CV

TL;DR: 受人类视觉推理方式的启发，提出了一种新的视频-文本交错推理范式（ViTCoT）及其基准（ViTIB），实验证明该方法优于仅使用文本的推理方法，并能更好地利用多模态大模型。


<details>
  <summary>Details</summary>
Motivation: 当前基于思维链（CoT）的视频推理方法主要依赖文本信息，忽略了视频模态在实际推理过程中的作用。而人类在推理时会自然地重新审视视觉内容。受此启发，我们提出 ViTCoT 范式，旨在实现更直观、更符合认知规律的推理。

Method: 提出了一种新的视频推理范式：视频-文本交错思维链（ViTCoT）。构建了一个视频-文本交错基准（ViTIB），该基准由多模态大模型（MLLMs）用于关键视频选择并经过人工验证。

Result: 实验证明，ViTCoT 范式显著优于纯文本 CoT 范式，并且能够激活 MLLMs 中更多的神经元值。

Conclusion: ViTCoT 范式显著提高了视频理解性能，优于传统的纯文本 CoT 范式，并能有效激活多模态大模型（MLLMs）的更多神经元值。

Abstract: Video understanding plays a vital role in bridging low-level visual signals
with high-level cognitive reasoning, and is fundamental to applications such as
autonomous driving, embodied AI, and the broader pursuit of AGI. The rapid
development of large language models (LLMs), particularly those utilizing
Chain-of-Thought (CoT) technology, has significantly advanced video reasoning
capabilities. However, current approaches primarily depend on textual
information for reasoning, overlooking the visual modality in the actual video
reasoning process. In contrast, humans naturally re-examine visual content
while reasoning. Motivated by this, we introduce a novel video reasoning
paradigm: Video-Text Interleaved CoT (ViTCoT), which facilitates more intuitive
and cognitively aligned reasoning. To the end, first, we construct the
Video-Text Interleaved Benchmark (ViTIB), which is created using MLLMs for
key-video selection and manually verified. Furthermore, we extensively explore
the potential of the ViTCoT paradigm in the video understanding field.
Extensive experiments demonstrate that ViTCoT significantly enhances
performance compared to the traditional text-only CoT paradigm and effectively
activates more neuron values in MLLMs.

</details>


### [78] [Generate Aligned Anomaly: Region-Guided Few-Shot Anomaly Image-Mask Pair Synthesis for Industrial Inspection](https://arxiv.org/abs/2507.09619)
*Yilin Lu,Jianghang Lin,Linhuang Xie,Kai Zhao,Yansong Qu,Shengchuan Zhang,Liujuan Cao,Rongrong Ji*

Main category: cs.CV

TL;DR: 工业异常检测因样本稀缺而受限。本研究提出GAA框架，利用潜在扩散模型和特定技术（局部概念分解、自适应多轮异常聚类、区域引导掩模生成）生成高质量、语义对齐的异常图像-掩模对，有效提升了异常检测的性能。


<details>
  <summary>Details</summary>
Motivation: 工业制造中的异常检测对于产品质量至关重要，但现有方法在定位和分类等任务中，由于异常样本稀缺而受到限制。现有的异常合成方法存在真实感低、掩模对齐不准确和泛化能力差等问题。

Method: 本研究提出了一种名为“生成对齐异常”（GAA）的区域引导、少样本异常图像-掩模对生成框架。GAA利用预训练的潜在扩散模型的强大先验知识，仅使用少量样本即可生成逼真、多样化且语义对齐的异常。该框架首先采用“局部概念分解”来联合建模异常的语义特征和空间信息，从而能够灵活控制异常的类型和位置。然后，利用“自适应多轮异常聚类”对异常概念进行细粒度语义聚类，以增强异常表示的一致性。此外，“区域引导掩模生成策略”确保了异常与其对应掩模之间的精确对齐，并引入了“低质量样本过滤模块”以进一步提高生成样本的整体质量。

Result: GAA框架能够生成逼真、多样化且语义对齐的异常，并能精确对齐异常与掩模，有效解决了现有方法的局限性。在MVTec AD和LOCO数据集上的实验证明了GAA在异常合成质量和下游任务（如定位和分类）上的优越性能。

Conclusion: GAA在MVTec AD和LOCO数据集上实现了卓越的性能，在异常合成质量和定位、分类等下游任务方面均表现出色。

Abstract: Anomaly inspection plays a vital role in industrial manufacturing, but the
scarcity of anomaly samples significantly limits the effectiveness of existing
methods in tasks such as localization and classification. While several anomaly
synthesis approaches have been introduced for data augmentation, they often
struggle with low realism, inaccurate mask alignment, and poor generalization.
To overcome these limitations, we propose Generate Aligned Anomaly (GAA), a
region-guided, few-shot anomaly image-mask pair generation framework. GAA
leverages the strong priors of a pretrained latent diffusion model to generate
realistic, diverse, and semantically aligned anomalies using only a small
number of samples. The framework first employs Localized Concept Decomposition
to jointly model the semantic features and spatial information of anomalies,
enabling flexible control over the type and location of anomalies. It then
utilizes Adaptive Multi-Round Anomaly Clustering to perform fine-grained
semantic clustering of anomaly concepts, thereby enhancing the consistency of
anomaly representations. Subsequently, a region-guided mask generation strategy
ensures precise alignment between anomalies and their corresponding masks,
while a low-quality sample filtering module is introduced to further improve
the overall quality of the generated samples. Extensive experiments on the
MVTec AD and LOCO datasets demonstrate that GAA achieves superior performance
in both anomaly synthesis quality and downstream tasks such as localization and
classification.

</details>


### [79] [Brain Stroke Detection and Classification Using CT Imaging with Transformer Models and Explainable AI](https://arxiv.org/abs/2507.09630)
*Shomukh Qari,Maha A. Thafar*

Main category: cs.CV

TL;DR: 本研究利用 MaxViT 和 XAI 技术，通过 CT 图像实现了高精度的卒中分类（准确率和 F1 分数达 98%），提高了 AI 在临床诊断中的可信度和实用性。


<details>
  <summary>Details</summary>
Motivation: 卒中是全球主要的死亡原因之一，因此早期和准确的诊断对于改善患者预后至关重要，尤其是在紧急情况下。本研究旨在开发一种高精度的 AI 框架来区分卒中类型，同时解决人工智能模型在透明度和可信度方面的问题，以期提供一个值得信赖的 AI 辅助诊断工具，促进其在临床实践中的应用，并最终挽救生命。

Method: 本研究提出了一种基于人工智能的框架，采用先进的 Vision Transformer 模型 MaxViT（以及其他变体如 Vision Transformer、Transformer-in-Transformer 和 ConvNext）对 CT 扫描图像进行卒中分类。为了提高模型的泛化能力和处理类别不平衡问题，采用了包括合成图像生成在内的数据增强技术。此外，集成了可解释人工智能（XAI）技术，特别是 Grad-CAM++，以提供模型决策的可视化解释。

Result: 所提出的 MaxViT 模型结合数据增强技术在卒中分类任务中取得了最佳性能，准确率和 F1 分数达到了 98.00%，优于所有其他评估模型和基线方法。XAI（Grad-CAM++）的集成提供了模型决策的可视化解释，突出了 CT 扫描中与卒中相关的区域。

Conclusion: 本研究成功开发了一个基于人工智能的框架，用于通过 CT 扫描图像进行多类别卒中分类（缺血性、出血性、无卒中）。MaxViT 模型结合数据增强技术实现了 98.00% 的准确率和 F1 分数，优于其他模型和基线方法。通过集成 XAI（特别是 Grad-CAM++），提高了模型的可解释性和可信度，为早期卒中检测提供了准确、可解释且临床适用的解决方案，有助于将 AI 辅助诊断工具集成到临床实践中，从而挽救更多生命。

Abstract: Stroke is one of the leading causes of death globally, making early and
accurate diagnosis essential for improving patient outcomes, particularly in
emergency settings where timely intervention is critical. CT scans are the key
imaging modality because of their speed, accessibility, and cost-effectiveness.
This study proposed an artificial intelligence framework for multiclass stroke
classification (ischemic, hemorrhagic, and no stroke) using CT scan images from
a dataset provided by the Republic of Turkey's Ministry of Health. The proposed
method adopted MaxViT, a state-of-the-art Vision Transformer, as the primary
deep learning model for image-based stroke classification, with additional
transformer variants (vision transformer, transformer-in-transformer, and
ConvNext). To enhance model generalization and address class imbalance, we
applied data augmentation techniques, including synthetic image generation. The
MaxViT model trained with augmentation achieved the best performance, reaching
an accuracy and F1-score of 98.00%, outperforming all other evaluated models
and the baseline methods. The primary goal of this study was to distinguish
between stroke types with high accuracy while addressing crucial issues of
transparency and trust in artificial intelligence models. To achieve this,
Explainable Artificial Intelligence (XAI) was integrated into the framework,
particularly Grad-CAM++. It provides visual explanations of the model's
decisions by highlighting relevant stroke regions in the CT scans and
establishing an accurate, interpretable, and clinically applicable solution for
early stroke detection. This research contributed to the development of a
trustworthy AI-assisted diagnostic tool for stroke, facilitating its
integration into clinical practice and enhancing access to timely and optimal
stroke diagnosis in emergency departments, thereby saving more lives.

</details>


### [80] [Cross-modal Associations in Vision and Language Models: Revisiting the bouba-kiki effect](https://arxiv.org/abs/2507.10013)
*Tom Kouwenhoven,Kiana Shahrasbi,Tessa Verhoef*

Main category: cs.CV

TL;DR: This paper re-evaluates the bouba-kiki effect in VLMs (CLIP variants ResNet and ViT) using prompt-based evaluation and Grad-CAM. Findings indicate models do not consistently exhibit the effect, with performance falling short of human cognition, suggesting limitations in cross-modal understanding.


<details>
  <summary>Details</summary>
Motivation: Recent advances in multimodal models have raised questions about whether vision-and-language models (VLMs) integrate cross-modal information in ways that reflect human cognition. One well-studied test case in this domain is the bouba-kiki effect, where humans reliably associate pseudowords like "bouba" with round shapes and "kiki" with jagged ones.

Method: We apply two complementary methods closely modelled after human experiments: a prompt-based evaluation that uses probabilities as model preference, and we use Grad-CAM as a novel way to interpret visual attention in shape-word matching tasks.

Result: Our findings show that these models do not consistently exhibit the bouba-kiki effect. While ResNet shows a preference for round shapes, overall performance across both models lacks the expected associations. Moreover, direct comparison with prior human data on the same task shows that the models' responses fall markedly short of the robust, modality-integrated behaviour characteristic of human cognition.

Conclusion: Given the mixed evidence found in prior studies for this effect in VLMs, we present a comprehensive re-evaluation focused on two variants of CLIP, ResNet and Vision Transformer (ViT), given their centrality in many state-of-the-art VLMs. Our findings show that these models do not consistently exhibit the bouba-kiki effect. While ResNet shows a preference for round shapes, overall performance across both models lacks the expected associations. Moreover, direct comparison with prior human data on the same task shows that the models' responses fall markedly short of the robust, modality-integrated behaviour characteristic of human cognition. These results contribute to the ongoing debate about the extent to which VLMs truly understand cross-modal concepts, highlighting limitations in their internal representations and alignment with human intuitions.

Abstract: Recent advances in multimodal models have raised questions about whether
vision-and-language models (VLMs) integrate cross-modal information in ways
that reflect human cognition. One well-studied test case in this domain is the
bouba-kiki effect, where humans reliably associate pseudowords like "bouba"
with round shapes and "kiki" with jagged ones. Given the mixed evidence found
in prior studies for this effect in VLMs, we present a comprehensive
re-evaluation focused on two variants of CLIP, ResNet and Vision Transformer
(ViT), given their centrality in many state-of-the-art VLMs. We apply two
complementary methods closely modelled after human experiments: a prompt-based
evaluation that uses probabilities as model preference, and we use Grad-CAM as
a novel way to interpret visual attention in shape-word matching tasks. Our
findings show that these models do not consistently exhibit the bouba-kiki
effect. While ResNet shows a preference for round shapes, overall performance
across both models lacks the expected associations. Moreover, direct comparison
with prior human data on the same task shows that the models' responses fall
markedly short of the robust, modality-integrated behaviour characteristic of
human cognition. These results contribute to the ongoing debate about the
extent to which VLMs truly understand cross-modal concepts, highlighting
limitations in their internal representations and alignment with human
intuitions.

</details>


### [81] [Disentanglement and Assessment of Shortcuts in Ophthalmological Retinal Imaging Exams](https://arxiv.org/abs/2507.09640)
*Leonor Fernandes,Tiago Gonçalves,João Matos,Luis Filipe Nakayama,Jaime S. Cardoso*

Main category: cs.CV

TL;DR: 本研究评估了三种 AI 模型（ConvNeXt V2、DINOv2、Swin V2）在糖尿病视网膜病变（DR）诊断中的公平性和性能。结果显示，尽管模型在 DR 预测和敏感属性预测方面表现良好，但存在公平性差异。解纠缠技术对模型公平性的影响因模型而异，为医学成像 AI 的公平性应用带来了挑战。


<details>
  <summary>Details</summary>
Motivation: 传统糖尿病视网膜病变（DR）筛查方法成本高且难以普及。人工智能（AI）算法作为一种可扩展的诊断解决方案出现，但其公平性和泛化性令人担忧。本研究旨在评估 AI 模型在 DR 预测中的公平性和性能，并研究解纠缠作为一种偏见缓解技术的影响。

Method: 使用多背景视网膜血管显像数据集（mBRSET），评估了在视网膜图像上训练的图像模型在糖尿病视网膜病变（DR）预测中的公平性和性能，以及解纠缠作为一种偏见缓解技术的影响。训练了三种模型（ConvNeXt V2、DINOv2 和 Swin V2）来预测 DR 和敏感属性（如年龄和性别），并评估了它们在不同亚组之间的公平性。

Result: 所有模型在 DR 预测方面均取得了较高的性能（AUROC 高达 94%），并且在预测年龄和性别方面也表现良好（AUROC 分别为 91% 和 77%）。公平性评估显示存在差异，例如 DINOv2 模型在不同年龄组之间存在 10% 的 AUROC 差距。解纠缠敏感属性对 DR 预测的影响因模型而异，其中 DINOv2 模型的性能有所提高（AUROC 提升 2%），而 ConvNeXt V2 和 Swin V2 模型的性能则有所下降（分别下降 7% 和 3%）。

Conclusion: 本研究强调了在医学成像人工智能中解决公平性问题的复杂性，并强调了确保公平可靠的医疗保健解决方案的重要性。

Abstract: Diabetic retinopathy (DR) is a leading cause of vision loss in working-age
adults. While screening reduces the risk of blindness, traditional imaging is
often costly and inaccessible. Artificial intelligence (AI) algorithms present
a scalable diagnostic solution, but concerns regarding fairness and
generalization persist. This work evaluates the fairness and performance of
image-trained models in DR prediction, as well as the impact of disentanglement
as a bias mitigation technique, using the diverse mBRSET fundus dataset. Three
models, ConvNeXt V2, DINOv2, and Swin V2, were trained on macula images to
predict DR and sensitive attributes (SAs) (e.g., age and gender/sex). Fairness
was assessed between subgroups of SAs, and disentanglement was applied to
reduce bias. All models achieved high DR prediction performance in diagnosing
(up to 94% AUROC) and could reasonably predict age and gender/sex (91% and 77%
AUROC, respectively). Fairness assessment suggests disparities, such as a 10%
AUROC gap between age groups in DINOv2. Disentangling SAs from DR prediction
had varying results, depending on the model selected. Disentanglement improved
DINOv2 performance (2% AUROC gain), but led to performance drops in ConvNeXt V2
and Swin V2 (7% and 3%, respectively). These findings highlight the complexity
of disentangling fine-grained features in fundus imaging and emphasize the
importance of fairness in medical imaging AI to ensure equitable and reliable
healthcare solutions.

</details>


### [82] [EyeSeg: An Uncertainty-Aware Eye Segmentation Framework for AR/VR](https://arxiv.org/abs/2507.09649)
*Zhengyuan Peng,Jianqing Xu,Shen Li,Jiazhen Ji,Yuge Huang,Jingyun Zhang,Jinmin Li,Shouhong Ding,Rizen Guo,Xin Tan,Lizhuang Ma*

Main category: cs.CV

TL;DR: EyeSeg是一个不确定性感知的眼部分割框架，用于AR/VR，可以处理运动模糊、眼睑遮挡和域差距等挑战。


<details>
  <summary>Details</summary>
Motivation: 为了克服现有方法在处理运动模糊、眼睑遮挡和训练-测试域差距等挑战时存在的不足，从而实现流畅的用户体验。

Method: 通过进行贝叶斯不确定性学习来显式地对不确定性进行建模，将EyeSeg设计为一个不确定性感知的眼部分割框架。

Result: EyeSeg输出一个不确定性分数和分割结果，对多个注视估计进行加权和融合以提高鲁棒性，尤其在运动模糊、眼睑遮挡和跨域挑战下表现有效。

Conclusion: EyeSeg框架在下游任务（如注视估计）中超越了现有方法，并且在MIoU、E1、F1和ACC方面取得了分割改进。

Abstract: Human-machine interaction through augmented reality (AR) and virtual reality
(VR) is increasingly prevalent, requiring accurate and efficient gaze
estimation which hinges on the accuracy of eye segmentation to enable smooth
user experiences. We introduce EyeSeg, a novel eye segmentation framework
designed to overcome key challenges that existing approaches struggle with:
motion blur, eyelid occlusion, and train-test domain gaps. In these situations,
existing models struggle to extract robust features, leading to suboptimal
performance. Noting that these challenges can be generally quantified by
uncertainty, we design EyeSeg as an uncertainty-aware eye segmentation
framework for AR/VR wherein we explicitly model the uncertainties by performing
Bayesian uncertainty learning of a posterior under the closed set prior.
Theoretically, we prove that a statistic of the learned posterior indicates
segmentation uncertainty levels and empirically outperforms existing methods in
downstream tasks, such as gaze estimation. EyeSeg outputs an uncertainty score
and the segmentation result, weighting and fusing multiple gaze estimates for
robustness, which proves to be effective especially under motion blur, eyelid
occlusion and cross-domain challenges. Moreover, empirical results suggest that
EyeSeg achieves segmentation improvements of MIoU, E1, F1, and ACC surpassing
previous approaches. The code is publicly available at
https://github.com/JethroPeng/EyeSeg.

</details>


### [83] [FaceLLM: A Multimodal Large Language Model for Face Understanding](https://arxiv.org/abs/2507.10300)
*Hatef Otroshi Shahreza,Sébastien Marcel*

Main category: cs.CV

TL;DR: FaceLLM 是一个专门为面部图像理解训练的多模态大语言模型，它利用 ChatGPT 生成的 FairFaceGPT 数据集，在人脸识别任务上取得了优异的成绩，并公开了模型和数据集。


<details>
  <summary>Details</summary>
Motivation: 现有的 MLLMs 主要在通用数据集上进行训练，这限制了它们在人脸图像等领域特定视觉线索上进行推理的能力，特别是那些需要详细理解面部结构、表情、情绪和人口统计特征的任务。

Method: 提出了一种新颖的弱监督方法，利用 ChatGPT 和面向属性的提示，基于 FairFace 数据集中的图像生成高质量的问答对，从而构建了 FaceLLM 的训练数据。

Result: FaceLLM 提高了 MLLMs 在各种以人脸为中心的任务上的性能，并取得了最先进的性能。

Conclusion: 这项工作突出了通过语言模型进行合成监督在构建领域专业化MLLM方面的潜力，并为值得信赖的、以人为中心的、多模态人工智能系统树立了先例。FaceLLM 和 FairFaceGPT 数据集已在项目页面公开。

Abstract: Multimodal large language models (MLLMs) have shown remarkable performance in
vision-language tasks. However, existing MLLMs are primarily trained on generic
datasets, limiting their ability to reason on domain-specific visual cues such
as those in facial images. In particular, tasks that require detailed
understanding of facial structure, expression, emotion, and demographic
features remain underexplored by MLLMs due to the lack of large-scale annotated
face image-text datasets. In this work, we introduce FaceLLM, a multimodal
large language model trained specifically for facial image understanding. To
construct the training data, we propose a novel weakly supervised pipeline that
uses ChatGPT with attribute-aware prompts to generate high-quality
question-answer pairs based on images from the FairFace dataset. The resulting
corpus, called FairFaceGPT, covers a diverse set of attributes including
expression, pose, skin texture, and forensic information. Our experiments
demonstrate that FaceLLM improves the performance of MLLMs on various
face-centric tasks and achieves state-of-the-art performance. This work
highlights the potential of synthetic supervision via language models for
building domain-specialized MLLMs, and sets a precedent for trustworthy,
human-centric multimodal AI systems. FairFaceGPT dataset and pretrained FaceLLM
models are publicly available in the project page.

</details>


### [84] [VST-Pose: A Velocity-Integrated Spatiotem-poral Attention Network for Human WiFi Pose Estimation](https://arxiv.org/abs/2507.09672)
*Xinyu Zhang,Zhonghao Ye,Jingwei Zhang,Xiang Tian,Zhisheng Liang,Shipeng Yu*

Main category: cs.CV

TL;DR: VST-Pose利用WiFi信号进行人体姿态估计，通过ViSTA-Former和速度建模分支提高了准确性和对细微动作的敏感度，在2D和3D姿态估计任务中均表现优异。


<details>
  <summary>Details</summary>
Motivation: 为了克服非视觉方法在人体姿态估计中的局限性，并利用WiFi信号的穿透性和隐私优势，提出VST-Pose框架来提高准确性和连续性。

Method: 该方法提出了一种名为ViSTA-Former的时空注意力骨干网，它具有双流结构，能够分别捕捉时间依赖性和身体关节之间的结构关系。此外，还整合了一个速度建模分支，用于学习关键点位移模式以改善细粒度运动表示。

Result: 在智能家居护理场景的2D姿态数据集上，VST-Pose达到了92.2%的PCK@50准确率，比现有方法提高了8.3%。在MMFi数据集上的评估也证明了其在3D姿态估计任务中的有效性和鲁棒性。

Conclusion: VST-Pose是一个用于WiFi信道状态信息的人体姿态估计的深度学习框架。它通过ViSTA-Former（一种具有双流结构的时空注意力骨干网）来捕捉身体关节之间的时间依赖性和结构关系。该框架还整合了一个速度建模分支，以提高对细微人体运动的敏感度。在智能家居护理场景的2D姿态数据集上，VST-Pose达到了92.2%的PCK@50准确率，优于现有方法。在MMFi数据集上的评估也证实了其在3D姿态估计任务中的鲁棒性和有效性。该系统为室内环境中的连续人体运动分析提供了一种可靠且注重隐私的解决方案。

Abstract: WiFi-based human pose estimation has emerged as a promising non-visual
alternative approaches due to its pene-trability and privacy advantages. This
paper presents VST-Pose, a novel deep learning framework for accurate and
continuous pose estimation using WiFi channel state information. The proposed
method introduces ViSTA-Former, a spatiotemporal attention backbone with
dual-stream architecture that adopts a dual-stream architecture to separately
capture temporal dependencies and structural relationships among body joints.
To enhance sensitivity to subtle human motions, a velocity modeling branch is
integrated into the framework, which learns short-term keypoint dis-placement
patterns and improves fine-grained motion representation. We construct a 2D
pose dataset specifically designed for smart home care scenarios and
demonstrate that our method achieves 92.2% accuracy on the PCK@50 metric,
outperforming existing methods by 8.3% in PCK@50 on the self-collected dataset.
Further evaluation on the public MMFi dataset confirms the model's robustness
and effectiveness in 3D pose estimation tasks. The proposed system provides a
reliable and privacy-aware solution for continuous human motion analysis in
indoor environments. Our codes are available in
https://github.com/CarmenQing/VST-Pose.

</details>


### [85] [Devanagari Handwritten Character Recognition using Convolutional Neural Network](https://arxiv.org/abs/2507.10398)
*Diksha Mehta,Prateek Mehta*

Main category: cs.CV

TL;DR: 本研究提出了一种利用深度卷积神经网络识别手写梵文印地语字符的方法，准确率高。


<details>
  <summary>Details</summary>
Motivation: 由于手写字符识别在技术搜索引擎、社交媒体和推荐系统等领域具有潜在应用前景，并且印度梵文是一种古老的印度语言文字，目前缺乏有效的数字化工具，因此本研究旨在通过自动化方法提取手写梵文印地语字符，以节省时间和处理过时数据。

Method: 本研究采用了一种配置卷积神经网络的方法，利用梵文手写字符数据集（DHCD）进行了训练和测试。

Result: 该方法在测试集上达到了96.36%的准确率，在训练集上达到了99.55%的准确率，取得了有希望的结果。

Conclusion: 本研究提出了一种使用两个深度卷积神经网络层来识别手写梵文印地语字符的技术，并取得了良好的识别率。

Abstract: Handwritten character recognition is getting popular among researchers
because of its possible applications in facilitating technological search
engines, social media, recommender systems, etc. The Devanagari script is one
of the oldest language scripts in India that does not have proper digitization
tools. With the advancement of computing and technology, the task of this
research is to extract handwritten Hindi characters from an image of Devanagari
script with an automated approach to save time and obsolete data. In this
paper, we present a technique to recognize handwritten Devanagari characters
using two deep convolutional neural network layers. This work employs a
methodology that is useful to enhance the recognition rate and configures a
convolutional neural network for effective Devanagari handwritten text
recognition (DHTR). This approach uses the Devanagari handwritten character
dataset (DHCD), an open dataset with 36 classes of Devanagari characters. Each
of these classes has 1700 images for training and testing purposes. This
approach obtains promising results in terms of accuracy by achieving 96.36%
accuracy in testing and 99.55% in training time.

</details>


### [86] [Prompt2DEM: High-Resolution DEMs for Urban and Open Environments from Global Prompts Using a Monocular Foundation Model](https://arxiv.org/abs/2507.09681)
*Osher Rafaeli,Tal Svoray,Ariel Nahlieli*

Main category: cs.CV

TL;DR: 该研究提出了一种创新的高分辨率DEM生成框架，利用提示工程和视觉变换器，实现了前所未有的分辨率提升和精度，为全球高程测绘开辟了新途径。


<details>
  <summary>Details</summary>
Motivation: 高分辨率高程估计对于理解流域和坡面水文学、研究城市形态和动态以及监测陆地生态系统的生长、衰退和死亡至关重要。现有的方法如超分辨率技术和单目深度估计存在局限性，如超分辨率技术的升尺度因子限制和单目深度估计缺乏全局高程背景。

Method: 该研究提出了一种新的高分辨率DEM估计框架，利用提示式单目深度估计技术。该框架通过对视觉变换器编码器进行微调，并结合激光雷达衍生的DEM数据和多功能的提示策略，实现了DEM估计、空洞填充和更新等任务。

Result: 该框架实现了100倍的分辨率提升（从30米到30厘米），在捕捉城市结构和精细地形特征方面，相对于激光雷达的平均绝对误差（MAE）小于5米，比SRTM提高了高达18%。水文学分析证实了该方法在灾害和环境研究中的适用性，并且该框架已成功应用于美国和以色列的大片区域。

Conclusion: 该框架通过结合低分辨率SRTM高程数据作为提示和NAIP的高分辨率RGB图像，实现了高分辨率DEM估计，并将分辨率提高了100倍（从30米到30厘米），在城市结构和精细地形特征捕捉方面优于先前的方法，MAE小于5米，优于SRTM高达18%，并已成功应用于美国和以色列的大片区域。

Abstract: High-resolution elevation estimations are essential to understand catchment
and hillslope hydrology, study urban morphology and dynamics, and monitor the
growth, decline, and mortality of terrestrial ecosystems. Various deep learning
approaches (e.g., super-resolution techniques, monocular depth estimation) have
been developed to create high-resolution Digital Elevation Models (DEMs).
However, super-resolution techniques are limited by the upscaling factor, and
monocular depth estimation lacks global elevation context, making its
conversion to a seamless DEM restricted. The recently introduced technique of
prompt-based monocular depth estimation has opened new opportunities to extract
estimates of absolute elevation in a global context. We present here a
framework for the estimation of high-resolution DEMs as a new paradigm for
absolute global elevation mapping. It is exemplified using low-resolution
Shuttle Radar Topography Mission (SRTM) elevation data as prompts and
high-resolution RGB imagery from the National Agriculture Imagery Program
(NAIP). The approach fine-tunes a vision transformer encoder with LiDAR-derived
DEMs and employs a versatile prompting strategy, enabling tasks such as DEM
estimation, void filling, and updating. Our framework achieves a 100x
resolution gain (from 30-m to 30-cm), surpassing prior methods by an order of
magnitude. Evaluations across three diverse U.S. landscapes show robust
generalization, capturing urban structures and fine-scale terrain features with
< 5 m MAE relative to LiDAR, improving over SRTM by up to 18%. Hydrological
analysis confirms suitability for hazard and environmental studies. We
demonstrate scalability by applying the framework to large regions in the U.S.
and Israel. All code and pretrained models are publicly available at:
https://osherr1996.github.io/prompt2dem_propage/.

</details>


### [87] [Text-to-Remote-Sensing-Image Retrieval beyond RGB Sources](https://arxiv.org/abs/2507.10403)
*Daniele Rege Cambrin,Lorenzo Vaiani,Giuseppe Gallipoli,Luca Cagliero,Paolo Garza*

Main category: cs.CV

TL;DR: 为了改进卫星图像检索，我们创建了一个包含647,000张SAR和多光谱图像的大型数据集，并开发了一个名为CLOSP的框架，该框架使用文本将不同类型的图像连接起来，从而将检索性能提高了54%。我们还发现，加入地理信息（GeoCLOSP）有助于检索特定位置的事件。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像检索系统主要局限于RGB数据，未能充分利用SAR和多光谱光学数据等其他传感器提供的独特物理信息。为了解决这一问题，本研究旨在弥合这一差距，实现跨模态遥感数据的有效检索。

Method: 提出了一种名为CLOSP（Contrastive Language Optical SAR Pre-training）的新型框架，利用文本作为桥梁，将非配对的光学和SAR图像对齐到一个统一的嵌入空间中。此外，还提出了GeoCLOSP，将地理坐标整合到框架中，实现了通用性和特异性之间的权衡。

Result: CLOSP框架在遥感图像检索任务上取得了新的最先进成果，检索nDCG（归一化折损累计增益）比现有模型提高了54%。研究还发现，统一的训练策略通过光学域到SAR域的间接知识迁移，克服了理解SAR图像的固有困难。GeoCLOSP在检索依赖地理位置的危机事件和罕见地理特征方面表现出色。

Conclusion: 本研究强调了整合多源遥感数据（特别是SAR和多光谱光学数据）和地理空间信息对于充分挖掘遥感影像库潜力的重要性。通过引入CrisisLandMark语料库和CLOSP预训练框架，成功实现了跨模态（文本-光学-SAR）的图像检索，并展示了地理信息融合（GeoCLOSP）在特定任务上的优势。

Abstract: Retrieving relevant imagery from vast satellite archives is crucial for
applications like disaster response and long-term climate monitoring. However,
most text-to-image retrieval systems are limited to RGB data, failing to
exploit the unique physical information captured by other sensors, such as the
all-weather structural sensitivity of Synthetic Aperture Radar (SAR) or the
spectral signatures in optical multispectral data. To bridge this gap, we
introduce CrisisLandMark, a new large-scale corpus of over 647,000 Sentinel-1
SAR and Sentinel-2 multispectral images paired with structured textual
annotations for land cover, land use, and crisis events harmonized from
authoritative land cover systems (CORINE and Dynamic World) and crisis-specific
sources. We then present CLOSP (Contrastive Language Optical SAR Pretraining),
a novel framework that uses text as a bridge to align unpaired optical and SAR
images into a unified embedding space. Our experiments show that CLOSP achieves
a new state-of-the-art, improving retrieval nDGC by 54% over existing models.
Additionally, we find that the unified training strategy overcomes the inherent
difficulty of interpreting SAR imagery by transferring rich semantic knowledge
from the optical domain with indirect interaction. Furthermore, GeoCLOSP, which
integrates geographic coordinates into our framework, creates a powerful
trade-off between generality and specificity: while the CLOSP excels at general
semantic tasks, the GeoCLOSP becomes a specialized expert for retrieving
location-dependent crisis events and rare geographic features. This work
highlights that the integration of diverse sensor data and geographic context
is essential for unlocking the full potential of remote sensing archives.

</details>


### [88] [ExpStar: Towards Automatic Commentary Generation for Multi-discipline Scientific Experiments](https://arxiv.org/abs/2507.09693)
*Jiali Chen,Yujie Jia,Zihan Wu,Jinyu Yang,Jianpeng Chen,Xusen Hei,Jiayuan Xie,Yi Cai,Qing Li*

Main category: cs.CV

TL;DR: 本文介绍了ExpCite数据集和ExpStar模型，用于自动生成科学实验的解说词，实验结果表明ExpStar优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 为了解决人工编写实验解说词耗时耗力的问题，本文探索了跨学科科学实验自动解说词生成任务。

Method: 本文提出了ExpStar模型，该模型利用检索增强机制来适应性地访问、评估和利用外部知识，以实现自动实验解说词生成。

Result: ExpStar模型在实验解说词生成任务上显著优于14个主流的大型多模态模型（LMM），证明了ExpCite数据集和ExpStar模型的优越性。

Conclusion: ExpStar模型在实验指导方面展现出巨大潜力，能够推动AI辅助科学实验指导的发展。

Abstract: Experiment commentary is crucial in describing the experimental procedures,
delving into underlying scientific principles, and incorporating
content-related safety guidelines. In practice, human teachers rely heavily on
subject-specific expertise and invest significant time preparing such
commentary. To address this challenge, we introduce the task of automatic
commentary generation across multi-discipline scientific experiments. While
recent progress in large multimodal models (LMMs) has demonstrated promising
capabilities in video understanding and reasoning, their ability to generate
fine-grained and insightful experiment commentary remains largely
underexplored. In this paper, we make the following contributions: (i) We
construct \textit{ExpInstruct}, the first dataset tailored for experiment
commentary generation, featuring over 7\textit{K} step-level commentaries
across 21 scientific subjects from 3 core disciplines (\ie, science, healthcare
and engineering). Each sample includes procedural descriptions along with
potential scientific principles (\eg, chemical equations and physical laws) and
safety guidelines. (ii) We propose ExpStar, an automatic experiment commentary
generation model that leverages a retrieval-augmented mechanism to adaptively
access, evaluate, and utilize external knowledge. (iii) Extensive experiments
show that our ExpStar substantially outperforms 14 leading LMMs, which
highlights the superiority of our dataset and model. We believe that ExpStar
holds great potential for advancing AI-assisted scientific experiment
instruction.

</details>


### [89] [Token Compression Meets Compact Vision Transformers: A Survey and Comparative Evaluation for Edge AI](https://arxiv.org/abs/2507.09702)
*Phat Nguyen,Ngai-Man Cheung*

Main category: cs.CV

TL;DR: ViT的token压缩技术：首次系统性分类与评估，发现在紧凑型模型上效果受限，为边缘AI应用提供新方向。


<details>
  <summary>Details</summary>
Motivation: 现有研究在ViT的token压缩技术方面存在两大不足：1. 缺乏对不同压缩策略（如剪枝、合并或混合）和部署设置（如微调或即插即用）的统一分类和比较。2. 大多数基准测试仅限于标准ViT模型，未能评估这些技术在结构压缩的Transformer上的适用性，而这类模型越来越多地部署在资源受限的边缘设备上。

Method: 本研究提出了一个系统性的token压缩方法分类法，并评估了代表性技术在标准ViT和紧凑型ViT架构上的有效性。

Result: 实验结果表明，token压缩技术对通用ViT有效，但直接应用于紧凑型设计时效果不佳。

Conclusion: 本研究填补了关于ViT模型中的token压缩技术缺乏系统性分类和比较评估的空白，并首次在标准ViT和紧凑型ViT架构上对代表性技术进行了评估。实验结果表明，虽然token压缩技术对通用ViT有效，但直接应用于紧凑型设计时效果不佳，这为未来在边缘AI和AI代理应用中将token优化技术应用于紧凑型Transformer网络的研究铺平了道路。

Abstract: Token compression techniques have recently emerged as powerful tools for
accelerating Vision Transformer (ViT) inference in computer vision. Due to the
quadratic computational complexity with respect to the token sequence length,
these methods aim to remove less informative tokens before the attention layers
to improve inference throughput. While numerous studies have explored various
accuracy-efficiency trade-offs on large-scale ViTs, two critical gaps remain.
First, there is a lack of unified survey that systematically categorizes and
compares token compression approaches based on their core strategies (e.g.,
pruning, merging, or hybrid) and deployment settings (e.g., fine-tuning vs.
plug-in). Second, most benchmarks are limited to standard ViT models (e.g.,
ViT-B, ViT-L), leaving open the question of whether such methods remain
effective when applied to structurally compressed transformers, which are
increasingly deployed on resource-constrained edge devices. To address these
gaps, we present the first systematic taxonomy and comparative study of token
compression methods, and we evaluate representative techniques on both standard
and compact ViT architectures. Our experiments reveal that while token
compression methods are effective for general-purpose ViTs, they often
underperform when directly applied to compact designs. These findings not only
provide practical insights but also pave the way for future research on
adapting token optimization techniques to compact transformer-based networks
for edge AI and AI agent applications.

</details>


### [90] [Advancing Text-to-3D Generation with Linearized Lookahead Variational Score Distillation](https://arxiv.org/abs/2507.09748)
*Yu Lei,Bingde Liu,Qingsong Xie,Haonan Lu,Zhijie Deng*

Main category: cs.CV

TL;DR: $L^2$-VSD通过解决LoRA和3D分布的不匹配问题，并采用线性化模型进行分数蒸馏，提高了文本到3D生成的速度和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于预训练2D扩散模型分数蒸馏的文本到3D生成方法，如变分分数蒸馏（VSD），虽然有理论基础，但在实践中存在收敛速度慢和训练不稳定的问题。本文旨在解决这些实际应用中的挑战。

Method: 本文通过对现有方法进行深入分析，发现LoRA与3D分布之间存在不匹配问题。提出通过调整优化顺序来解决该问题，并进一步提出使用线性化模型进行分数蒸馏，即$L^2$-VSD，利用前向模式自动微分实现。

Result: 实验结果表明，$L^2$-VSD在生成质量上明显优于先前基于分数蒸馏的方法，并且可以轻松地集成到任何其他基于VSD的文本到3D框架中。

Conclusion: 通过对LoRA和3D分布之间匹配问题进行深入研究，我们发现调整优化顺序可以提高生成质量。我们提出了线性前瞻变分分数蒸馏（$L^2$-VSD），通过使用线性化模型进行分数蒸馏，解决了训练不稳定的问题，并在实验中证明了其优于现有基于分数蒸馏的方法，同时可以无缝集成到其他基于VSD的文本到3D框架中。

Abstract: Text-to-3D generation based on score distillation of pre-trained 2D diffusion
models has gained increasing interest, with variational score distillation
(VSD) as a remarkable example. VSD proves that vanilla score distillation can
be improved by introducing an extra score-based model, which characterizes the
distribution of images rendered from 3D models, to correct the distillation
gradient. Despite the theoretical foundations, VSD, in practice, is likely to
suffer from slow and sometimes ill-posed convergence. In this paper, we perform
an in-depth investigation of the interplay between the introduced score model
and the 3D model, and find that there exists a mismatching problem between LoRA
and 3D distributions in practical implementation. We can simply adjust their
optimization order to improve the generation quality. By doing so, the score
model looks ahead to the current 3D state and hence yields more reasonable
corrections. Nevertheless, naive lookahead VSD may suffer from unstable
training in practice due to the potential over-fitting. To address this, we
propose to use a linearized variant of the model for score distillation, giving
rise to the Linearized Lookahead Variational Score Distillation ($L^2$-VSD).
$L^2$-VSD can be realized efficiently with forward-mode autodiff
functionalities of existing deep learning libraries. Extensive experiments
validate the efficacy of $L^2$-VSD, revealing its clear superiority over prior
score distillation-based methods. We also show that our method can be
seamlessly incorporated into any other VSD-based text-to-3D framework.

</details>


### [91] [Pairwise Alignment & Compatibility for Arbitrarily Irregular Image Fragments](https://arxiv.org/abs/2507.09767)
*Ofir Itzhak Shahar,Gur Elkin,Ohad Ben-Shahar*

Main category: cs.CV

TL;DR: 提出了一种无需假设片段形状的混合方法来计算片段对的最佳对齐，并在考古拼图任务中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的片段兼容性计算方法，特别是在解决各种类型的拼图问题时，通常无法处理具有现实几何特性的片段，或者严重依赖于片段的形状限制。

Method: 提出了一种新的图像片段数据集，该数据集是通过一种新颖的图像分割方法生成的，以及一个模仿现实世界考古侵蚀的正式侵蚀模型，并为兼容性任务提供了评估指标。

Result: 将提出的兼容性方法嵌入到考古拼图解决框架中，在RePAIR 2D数据集上展示了最先进的邻域级精确率和召回率，直接反映了兼容性性能的提升。

Conclusion: 提出了一种高效的混合（几何和图形）方法来计算片段对的最佳对齐，无需对其形状、尺寸或图形内容进行任何假设。

Abstract: Pairwise compatibility calculation is at the core of most
fragments-reconstruction algorithms, in particular those designed to solve
different types of the jigsaw puzzle problem. However, most existing approaches
fail, or aren't designed to deal with fragments of realistic geometric
properties one encounters in real-life puzzles. And in all other cases,
compatibility methods rely strongly on the restricted shapes of the fragments.
In this paper, we propose an efficient hybrid (geometric and pictorial)
approach for computing the optimal alignment for pairs of fragments, without
any assumptions about their shapes, dimensions, or pictorial content. We
introduce a new image fragments dataset generated via a novel method for image
fragmentation and a formal erosion model that mimics real-world archaeological
erosion, along with evaluation metrics for the compatibility task. We then
embed our proposed compatibility into an archaeological puzzle-solving
framework and demonstrate state-of-the-art neighborhood-level precision and
recall on the RePAIR 2D dataset, directly reflecting compatibility performance
improvements.

</details>


### [92] [EmbRACE-3K: Embodied Reasoning and Action in Complex Environments](https://arxiv.org/abs/2507.10548)
*Mingxian Lin,Wei Huang,Yitang Li,Chengjie Jiang,Kui Wu,Fangwei Zhong,Shengju Qian,Xin Wang,Xiaojuan Qi*

Main category: cs.CV

TL;DR: 本研究提出了EmRACE-3K数据集，用于评估和提升视觉语言模型在具身任务中的推理能力。现有模型在零样本设置下的表现不佳，但通过在EmRACE-3K上进行微调，可以显著提高模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型在处理需要在线交互和主动场景理解的具身任务方面表现有限，即使是像GPT-4o、Claude 3.5 Sonnet和Gemini 2.5 Pro这样先进的模型在开放环境交互中也存在空间推理和长期规划的不足。为了解决这一差距，需要一个能够有效评估和改进模型在具身环境中的表现的数据集和基准。

Method: 本研究引入了一个包含3000多个语言引导任务的数据集EmRACE-3K，这些任务在虚幻引擎中构建的逼真环境中进行。数据集详细记录了从第一人称视角观测、高层指令、执行动作以及每一步的自然语言解释。研究人员利用该数据集建立了评估基准，涵盖探索、动态空间语义推理和多阶段目标执行三个维度，并对现有模型进行了零样本评估，最后通过对Qwen2.5-VL-7B模型进行有监督微调和强化学习微调来验证数据集的有效性。

Result: 在零样本设置下，现有先进模型在EmRACE-3K上的成功率均低于20%，这表明了该基准的挑战性和当前模型在交互式环境中的局限性。通过对Qwen2.5-VL-7B模型进行微调，在探索、动态空间语义推理和多阶段目标执行三个维度上都取得了显著的性能提升，证明了EmRACE-3K数据集在促进具身推理能力发展方面的有效性。

Conclusion: EmRACE-3K数据集的引入为评估和提升视觉语言模型在具身环境中的推理能力提供了一个新的基准。通过在EmRACE-3K上进行微调，可以显著提高模型在导航、物体操纵和多阶段任务执行等方面的表现，尤其是在需要空间推理和长期规划的任务中。

Abstract: Recent advanced vision-language models(VLMs) have demonstrated strong
performance on passive, offline image and video understanding tasks. However,
their effectiveness in embodied settings, which require online interaction and
active scene understanding remains limited. In such scenarios, an agent
perceives the environment from a first-person perspective, with each action
dynamically shaping subsequent observations. Even state-of-the-art models such
as GPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro struggle in open-environment
interactions, exhibiting clear limitations in spatial reasoning and
long-horizon planning. To address this gap, we introduce EmRACE-3K, a dataset
of over 3,000 language-guided tasks situated in diverse, photorealistic
environments constructed using Unreal Engine and the UnrealCV-Zoo framework.
The tasks encompass a wide range of embodied challenges, including navigation,
object manipulation, and multi-stage goal execution. Each task unfolds as a
multi-step trajectory, pairing first-person visual observations with high-level
instructions, grounded actions, and natural language rationales that express
the agent's intent at every step. Using EmRACE-3K, we establish a benchmark to
evaluate the embodied reasoning capabilities of VLMs across three key
dimensions: Exploration, Dynamic Spatial-Semantic Reasoning, and Multi-stage
Goal Execution. In zero-shot settings, all models achieve success rates below
20%, underscoring the challenge posed by our benchmark and the current
limitations of VLMs in interactive environments. To demonstrate the utility of
EmRACE-3K, we further fine-tune Qwen2.5-VL-7B using supervised learning
followed by reinforcement learning. This approach yields substantial
improvements across all three challenge categories, highlighting the dataset's
effectiveness in enabling the development of embodied reasoning capabilities.

</details>


### [93] [NegRefine: Refining Negative Label-Based Zero-Shot OOD Detection](https://arxiv.org/abs/2507.09795)
*Amirhossein Ansari,Ke Wang,Pulei Xiong*

Main category: cs.CV

TL;DR: NegRefine通过改进负标签选择和评分机制，解决了现有方法将in-distribution样本误判为OOD的问题，提升了零样本OOD检测的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于负标签的方法（如NegLabel和CSP）在区分OOD样本方面表现出有前景的结果，但存在将in-distribution样本误判为OOD的问题，因为负标签可能包含in-distribution标签的子类别或专有名词，并且难以处理匹配多个in-distribution和负标签的图像。

Method: 提出了一种新颖的负标签精炼框架NegRefine，通过过滤机制排除子类别标签和专有名词，并采用多匹配感知评分函数来动态调整多个匹配标签的贡献度。

Result: 在包括ImageNet-1K在内的大规模基准测试上评估了NegRefine的有效性。

Conclusion: NegRefine通过引入过滤机制排除子类别标签和专有名词，并结合多匹配感知评分函数，实现了更鲁棒的 in-distribution 和 OOD 样本分离。

Abstract: Recent advancements in Vision-Language Models like CLIP have enabled
zero-shot OOD detection by leveraging both image and textual label information.
Among these, negative label-based methods such as NegLabel and CSP have shown
promising results by utilizing a lexicon of words to define negative labels for
distinguishing OOD samples. However, these methods suffer from detecting
in-distribution samples as OOD due to negative labels that are subcategories of
in-distribution labels or proper nouns. They also face limitations in handling
images that match multiple in-distribution and negative labels. We propose
NegRefine, a novel negative label refinement framework for zero-shot OOD
detection. By introducing a filtering mechanism to exclude subcategory labels
and proper nouns from the negative label set and incorporating a
multi-matching-aware scoring function that dynamically adjusts the
contributions of multiple labels matching an image, NegRefine ensures a more
robust separation between in-distribution and OOD samples. We evaluate
NegRefine on large-scale benchmarks, including ImageNet-1K. Source code is
available at https://github.com/ah-ansari/NegRefine.

</details>


### [94] [VRU-Accident: A Vision-Language Benchmark for Video Question Answering and Dense Captioning for Accident Scene Understanding](https://arxiv.org/abs/2507.09815)
*Younggun Kim,Ahmed S. Abdelrahman,Mohamed Abdel-Aty*

Main category: cs.CV

TL;DR: 该研究提出了VRU-Accident基准，用于评估多模态大语言模型在自动驾驶中处理弱势道路使用者（VRU）事故场景的能力。结果表明，尽管模型在视觉理解上表现不错，但在事故原因和可预防性推理方面仍有待提高。


<details>
  <summary>Details</summary>
Motivation: 为了解决目前缺乏标准化基准来量化评估多模态大语言模型（MLLMs）在涉及弱势道路使用者（VRU）的复杂、安全关键场景中的推理能力的问题，因为涉及VRU的事故往往导致严重或致命的后果。

Method: 构建了一个名为VRU-Accident的大规模视觉-语言基准，该基准包含1K个真实世界的行车记录仪事故视频，并附带了针对六个安全关键类别的6K个选择题问答对（包含24K个选项和3.4K个唯一答案选项），以及1K个密集的场景描述。对17个最先进模型进行了全面的评估，包括多选题视觉问答任务和密集场景描述任务。

Result: 在VRU-Accident基准上，多模态大语言模型在视觉基础属性方面表现合理，但在推理和描述事故原因、类型和可预防性方面面临显著挑战。

Conclusion: 目前最先进的多模态大语言模型在处理涉及弱势道路使用者（VRU）的事故场景时，在事故原因、类型和可预防性等方面的推理和描述能力仍面临重大挑战，尽管它们在视觉基础属性方面表现尚可。

Abstract: Ensuring the safety of vulnerable road users (VRUs), such as pedestrians and
cyclists, is a critical challenge for autonomous driving systems, as crashes
involving VRUs often result in severe or fatal consequences. While multimodal
large language models (MLLMs) have shown promise in enhancing scene
understanding and decision making in autonomous vehicles, there is currently no
standardized benchmark to quantitatively evaluate their reasoning abilities in
complex, safety-critical scenarios involving VRUs. To address this gap, we
present VRU-Accident, a large-scale vision-language benchmark designed to
evaluate MLLMs in high-risk traffic scenarios involving VRUs. VRU-Accident
comprises 1K real-world dashcam accident videos, annotated with 6K
multiple-choice question-answer pairs across six safety-critical categories
(with 24K candidate options and 3.4K unique answer choices), as well as 1K
dense scene descriptions. Unlike prior works, our benchmark focuses explicitly
on VRU-vehicle accidents, providing rich, fine-grained annotations that capture
both spatial-temporal dynamics and causal semantics of accidents. To assess the
current landscape of MLLMs, we conduct a comprehensive evaluation of 17
state-of-the-art models on the multiple-choice VQA task and on the dense
captioning task. Our findings reveal that while MLLMs perform reasonably well
on visually grounded attributes, they face significant challenges in reasoning
and describing accident causes, types, and preventability.

</details>


### [95] [Hierarchical Abstraction Enables Human-Like 3D Object Recognition in Deep Learning Models](https://arxiv.org/abs/2507.09830)
*Shuhao Fu,Philip J. Kellman,Hongjing Lu*

Main category: cs.CV

TL;DR: 深度学习模型在识别三维形状方面表现出色，但其表征方式可能与人类不同。本研究通过人类实验和模型比较发现，点变换器模型比卷积模型更能模拟人类识别三维形状的表现，这归因于其分层抽象机制。


<details>
  <summary>Details</summary>
Motivation: 探究深度学习模型在识别三维形状时，其形成的三维形状表征是否与人类视觉所使用的表征相似。

Method: 通过两项人类实验，系统地操纵点密度和物体方向（实验一），以及操纵局部几何结构（实验二），并比较了两种深度学习模型（基于卷积神经网络的DGCNN和基于视觉变换器的点变换器）和人类的表现。

Result: 人类在所有实验条件下表现均稳定良好。点变换器模型在模拟人类表现方面优于DGCNN模型。

Conclusion: 点云数据的识别能力方面，点变换器模型比基于卷积的模型更能模拟人类表现，这主要得益于点变换器模型能够对三维形状进行分层抽象。

Abstract: Both humans and deep learning models can recognize objects from 3D shapes
depicted with sparse visual information, such as a set of points randomly
sampled from the surfaces of 3D objects (termed a point cloud). Although deep
learning models achieve human-like performance in recognizing objects from 3D
shapes, it remains unclear whether these models develop 3D shape
representations similar to those used by human vision for object recognition.
We hypothesize that training with 3D shapes enables models to form
representations of local geometric structures in 3D shapes. However, their
representations of global 3D object shapes may be limited. We conducted two
human experiments systematically manipulating point density and object
orientation (Experiment 1), and local geometric structure (Experiment 2).
Humans consistently performed well across all experimental conditions. We
compared two types of deep learning models, one based on a convolutional neural
network (DGCNN) and the other on visual transformers (point transformer), with
human performance. We found that the point transformer model provided a better
account of human performance than the convolution-based model. The advantage
mainly results from the mechanism in the point transformer model that supports
hierarchical abstraction of 3D shapes.

</details>


### [96] [A Survey on MLLM-based Visually Rich Document Understanding: Methods, Challenges, and Emerging Trends](https://arxiv.org/abs/2507.09861)
*Yihao Ding,Siwen Luo,Yue Dai,Yanbei Jiang,Zechuan Li,Geoffrey Martin,Yifan Peng*

Main category: cs.CV

TL;DR: 本篇论文是关于多模态大语言模型（MLLM）在视觉丰富文档理解（VRDU）领域的最新进展的调查。它涵盖了特征编码和融合、训练范式和数据集，并讨论了该领域的挑战、机遇和未来方向。


<details>
  <summary>Details</summary>
Motivation: 为了自动处理包含复杂的视觉、文本和布局信息的文档，MLLM在VRDU领域展现出巨大潜力。

Method: 对多模态大语言模型（MLLM）在视觉丰富文档理解（VRDU）领域的进展进行了回顾，重点介绍了特征编码与融合、训练范式以及数据集等方面。

Result: MLLM在VRDU领域展现出巨大潜力，能够利用OCR依赖和OCR自由框架提取和解释文档图像中的信息。

Conclusion: 该论文对多模态大语言模型（MLLM）在视觉丰富文档理解（VRDU）领域的最新进展进行了回顾，重点介绍了特征编码与融合、训练范式以及数据集等方面，并讨论了该领域的挑战、机遇和未来方向。

Abstract: Visually-Rich Document Understanding (VRDU) has emerged as a critical field,
driven by the need to automatically process documents containing complex
visual, textual, and layout information. Recently, Multimodal Large Language
Models (MLLMs) have shown remarkable potential in this domain, leveraging both
Optical Character Recognition (OCR)-dependent and OCR-free frameworks to
extract and interpret information in document images. This survey reviews
recent advancements in MLLM-based VRDU, highlighting three core components: (1)
methods for encoding and fusing textual, visual, and layout features; (2)
training paradigms, including pretraining strategies, instruction-response
tuning, and the trainability of different model modules; and (3) datasets
utilized for pretraining, instruction-tuning, and supervised fine-tuning.
Finally, we discuss the challenges and opportunities in this evolving field and
propose future directions to advance the efficiency, generalizability, and
robustness of VRDU systems.

</details>


### [97] [SpeakerVid-5M: A Large-Scale High-Quality Dataset for Audio-Visual Dyadic Interactive Human Generation](https://arxiv.org/abs/2507.09862)
*Youliang Zhang,Zhaoyang Li,Duomin Wang,Jiahe Zhang,Deyu Zhou,Zixin Yin,Xili Dai,Gang Yu,Xiu Li*

Main category: cs.CV

TL;DR: 提出SpeakerVid-5M数据集和VidChatBench基准，推动音频-视觉双向交互虚拟人研究。


<details>
  <summary>Details</summary>
Motivation: 为了促进新兴的音频-视觉双向交互虚拟人领域的研究，填补该领域大规模、高质量数据集的空白。

Method: 提出SpeakerVid-5M数据集，该数据集包含超过8743小时、520万个视频片段，涵盖对话分支、单边分支、监听分支和多轮分支四种交互场景，并分为大规模预训练子集和高质量微调子集。同时，提供了一个基于自回归模型的视频聊天基线（VidChatBench）以及相应的评估指标和测试数据。

Result: SpeakerVid-5M数据集的发布，为音频-视觉双向交互虚拟人生成任务提供了大规模、高质量的数据支持，并辅以VidChatBench基准测试，为后续研究奠定了基础。

Conclusion: 该研究提出了SpeakerVid-5M数据集，旨在推动音频-视觉双向交互虚拟人领域的研究。数据集包含大规模、高质量的视频片段，覆盖多种交互类型和场景，并分为预训练和微调两部分。此外，研究还提供了一个基于自回归模型的视频聊天基线和评估基准VidChatBench。

Abstract: The rapid development of large-scale models has catalyzed significant
breakthroughs in the digital human domain. These advanced methodologies offer
high-fidelity solutions for avatar driving and rendering, leading academia to
focus on the next major challenge: audio-visual dyadic interactive virtual
human. To facilitate research in this emerging area, we present SpeakerVid-5M
dataset, the first large-scale, high-quality dataset designed for audio-visual
dyadic interactive virtual human generation. Totaling over 8,743 hours,
SpeakerVid-5M contains more than 5.2 million video clips of human portraits. It
covers diverse scales and interaction types, including monadic talking,
listening, and dyadic conversations. Crucially, the dataset is structured along
two key dimensions: interaction type and data quality. First, it is categorized
into four types (dialogue branch, single branch, listening branch and
multi-turn branch) based on the interaction scenario. Second, it is stratified
into a large-scale pre-training subset and a curated, high-quality subset for
Supervised Fine-Tuning (SFT). This dual structure accommodates a wide array of
2D virtual human tasks. In addition, we provide an autoregressive (AR)-based
video chat baseline trained on this data, accompanied by a dedicated set of
metrics and test data to serve as a benchmark VidChatBench for future work.
Both the dataset and the corresponding data processing code will be publicly
released. Project page: https://dorniwang.github.io/SpeakerVid-5M/

</details>


### [98] [OpenHuman4D: Open-Vocabulary 4D Human Parsing](https://arxiv.org/abs/2507.09880)
*Keito Suzuki,Bang Du,Runfa Blark Li,Kunyao Chen,Lei Wang,Peng Liu,Ning Bi,Truong Nguyen*

Main category: cs.CV

TL;DR: 本文提出了一种高效的 4D 人体解析框架，解决了现有方法的推理时间和开放词汇能力限制。通过掩码跟踪、掩码验证和 4D 掩码融合等创新技术，实现了显著的加速和优越的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的人体部件分割方法受限于闭集数据集和较长的推理时间，这极大地限制了它们在虚拟和扩展现实应用中的广泛使用。因此，有必要开发能够提高效率并支持更广泛应用场景（例如开放词汇能力）的新方法。

Method: 本文提出了一种新颖的 4D 人体解析框架，该框架集成了掩码跟踪、掩码验证和 4D 掩码融合等关键技术。具体而言，采用了掩码跟踪来建立时空对应关系，设计了掩码验证模块来处理新目标识别和跟踪失败，并提出了融合了记忆条件注意力和 logits 均衡化的 4D 掩码融合模块来增强嵌入融合的鲁棒性。

Result: 实验结果表明，该方法在 4D 人体中心解析任务上取得了显著成效，与先前最先进的方法相比，推理速度最高提升了 93.3%，并且能够解析固定的类别。

Conclusion: 本文提出的 4D 人体解析框架通过引入掩码跟踪、掩码验证和 4D 掩码融合模块，有效解决了现有方法在推理时间和开放词汇能力方面的限制，实现了高达 93.3% 的加速，并在 4D 人体中心视频解析任务上展现了优越的性能和灵活性。

Abstract: Understanding dynamic 3D human representation has become increasingly
critical in virtual and extended reality applications. However, existing human
part segmentation methods are constrained by reliance on closed-set datasets
and prolonged inference times, which significantly restrict their
applicability. In this paper, we introduce the first 4D human parsing framework
that simultaneously addresses these challenges by reducing the inference time
and introducing open-vocabulary capabilities. Building upon state-of-the-art
open-vocabulary 3D human parsing techniques, our approach extends the support
to 4D human-centric video with three key innovations: 1) We adopt mask-based
video object tracking to efficiently establish spatial and temporal
correspondences, avoiding the necessity of segmenting all frames. 2) A novel
Mask Validation module is designed to manage new target identification and
mitigate tracking failures. 3) We propose a 4D Mask Fusion module, integrating
memory-conditioned attention and logits equalization for robust embedding
fusion. Extensive experiments demonstrate the effectiveness and flexibility of
the proposed method on 4D human-centric parsing tasks, achieving up to 93.3%
acceleration compared to the previous state-of-the-art method, which was
limited to parsing fixed classes.

</details>


### [99] [Counterfactual Visual Explanation via Causally-Guided Adversarial Steering](https://arxiv.org/abs/2507.09881)
*Yiran Qiao,Disheng Liu,Yiren Lu,Yu Yin,Mengnan Du,Jing Ma*

Main category: cs.CV

TL;DR: CECAS是一种新的反事实视觉解释框架，通过整合因果关系来提高解释质量，并避免对虚假因素进行不希望的扰动。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成反事实图像时忽略了因果关系和虚假相关性，导致图像改动不当和解释质量有限。

Method: CECAS框架首先利用因果引导的对抗性方法生成反事实解释，并将因果视角整合其中，以避免对虚假因素进行不希望的扰动。

Result: CECAS框架在多个基准数据集上的广泛实验证明，其性能优于现有的最先进方法，并在有效性、稀疏性、邻近性和真实性等方面取得了平衡的权衡。

Conclusion: CECAS框架通过引入因果视角和因果引导的对抗性方法来生成反事实解释，避免了对虚假因素的不希望的扰动，从而提高了解释的质量。

Abstract: Recent work on counterfactual visual explanations has contributed to making
artificial intelligence models more explainable by providing visual
perturbation to flip the prediction. However, these approaches neglect the
causal relationships and the spurious correlations behind the image generation
process, which often leads to unintended alterations in the counterfactual
images and renders the explanations with limited quality. To address this
challenge, we introduce a novel framework CECAS, which first leverages a
causally-guided adversarial method to generate counterfactual explanations. It
innovatively integrates a causal perspective to avoid unwanted perturbations on
spurious factors in the counterfactuals. Extensive experiments demonstrate that
our method outperforms existing state-of-the-art approaches across multiple
benchmark datasets and ultimately achieves a balanced trade-off among various
aspects of validity, sparsity, proximity, and realism.

</details>


### [100] [MCGA: Mixture of Codebooks Hyperspectral Reconstruction via Grayscale-Aware Attention](https://arxiv.org/abs/2507.09885)
*Zhanjiang Yang,Lijun Sun,Jiawei Dong,Xiaoxin An,Yang Liu,Meng Li*

Main category: cs.CV

TL;DR: MCGA是一种两阶段高光谱重建方法，通过学习光谱模式和利用迁移学习来提高从RGB到HSI的映射精度，实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在从低维到高维信息转换时存在不足，直接学习RGB到HSI的映射忽略了这一固有挑战。

Method: 提出了一种名为MCGA的两阶段方法：1. 使用多尺度VQ-VAE从异构高光谱图像数据集中学习表示，提取码书混合（MoC）。2. 通过查询MoC中的特征来细化RGB到HSI的映射，将先验知识纳入其中，而不是强制进行直接的高维转换。引入了灰度感知注意力和量化自注意力机制来适应特征图强度，并通过基于熵的测试时间自适应策略来提高鲁棒性。

Result: MCGA方法在重建质量和效率方面均表现出色，并实现了最先进的性能。

Conclusion: MCGA方法在各种计算机视觉应用中实现了最先进的高光谱重建性能。

Abstract: Reconstructing hyperspectral images (HSI) from RGB images is a cost-effective
solution for various vision-based applications. However, most existing
learning-based hyperspectral reconstruction methods directly learn the
RGB-to-HSI mapping using complex attention mechanisms, neglecting the inherent
challenge of transitioning from low-dimensional to high-dimensional
information. To address this limitation, we propose a two-stage approach, MCGA,
which first learns spectral patterns before estimating the mapping. In the
first stage, a multi-scale VQ-VAE learns representations from heterogeneous HSI
datasets, extracting a Mixture of Codebooks (MoC). In the second stage, the
RGB-to-HSI mapping is refined by querying features from the MoC to replace
latent HSI representations, incorporating prior knowledge rather than forcing a
direct high-dimensional transformation. To further enhance reconstruction
quality, we introduce Grayscale-Aware Attention and Quantized Self-Attention,
which adaptively adjust feature map intensities to meet hyperspectral
reconstruction requirements. This physically motivated attention mechanism
ensures lightweight and efficient HSI recovery. Moreover, we propose an
entropy-based Test-Time Adaptation strategy to improve robustness in real-world
scenarios. Extensive experiments demonstrate that our method, MCGA, achieves
state-of-the-art performance. The code and models will be released at
https://github.com/Fibonaccirabbit/MCGA

</details>


### [101] [Measuring the Impact of Rotation Equivariance on Aerial Object Detection](https://arxiv.org/abs/2507.09896)
*Xiuyu Wu,Xinhao Wang,Xiubin Zhu,Lan Yang,Jiyuan Liu,Xingchen Hu*

Main category: cs.CV

TL;DR: 由于航空影像中物体方向不固定，旋转等变性很重要。本研究提出了一种严格旋转等变的检测器MessDet，并通过多分支头部网络提高了精度并降低了参数量，在多个数据集上达到了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 由于航空影像中物体的任意方向，旋转等变性是航空物体检测的关键属性，但现有研究不足，且严格旋转等变性在该领域的必要性仍是悬而未决的问题。

Method: 实现了一个严格旋转等变的骨干和颈部网络，并提出了一个多分支头部网络，以提高检测精度并减少参数量。

Result: MessDet在DOTA-v1.0、DOTA-v1.5和DIOR-R数据集上取得了最先进的性能，参数量极低。

Conclusion: 该研究提出了MessDet，一个多分支头部旋转等变单阶段检测器，在DOTA-v1.0、DOTA-v1.5和DIOR-R数据集上实现了最先进的性能，并且参数量极低。

Abstract: Due to the arbitrary orientation of objects in aerial images, rotation
equivariance is a critical property for aerial object detectors. However,
recent studies on rotation-equivariant aerial object detection remain scarce.
Most detectors rely on data augmentation to enable models to learn
approximately rotation-equivariant features. A few detectors have constructed
rotation-equivariant networks, but due to the breaking of strict rotation
equivariance by typical downsampling processes, these networks only achieve
approximately rotation-equivariant backbones. Whether strict rotation
equivariance is necessary for aerial image object detection remains an open
question. In this paper, we implement a strictly rotation-equivariant backbone
and neck network with a more advanced network structure and compare it with
approximately rotation-equivariant networks to quantitatively measure the
impact of rotation equivariance on the performance of aerial image detectors.
Additionally, leveraging the inherently grouped nature of rotation-equivariant
features, we propose a multi-branch head network that reduces the parameter
count while improving detection accuracy. Based on the aforementioned
improvements, this study proposes the Multi-branch head rotation-equivariant
single-stage Detector (MessDet), which achieves state-of-the-art performance on
the challenging aerial image datasets DOTA-v1.0, DOTA-v1.5 and DIOR-R with an
exceptionally low parameter count.

</details>


### [102] [IGD: Instructional Graphic Design with Multimodal Layer Generation](https://arxiv.org/abs/2507.09910)
*Yadong Qu,Shancheng Fang,Yuxin Wang,Xiaorui Wang,Zhineng Chen,Hongtao Xie,Yongdong Zhang*

Main category: cs.CV

TL;DR: IGD is a new system that generates editable graphic design layers from natural language instructions, using MLLM and diffusion models, improving upon existing methods that are not creative or editable.


<details>
  <summary>Details</summary>
Motivation: Existing two-stage methods and diffusion-based methods for graphic design lack creativity, intelligence, and editability, making the process labor-intensive and preventing satisfactory, practical automated graphic design.

Method: IGD adopts a new paradigm leveraging parametric rendering and image asset generation. It utilizes MLLM for multimodal understanding and reasoning to accomplish attribute prediction, sequencing, and layout of layers, and employs a diffusion model to generate image content for assets. A design platform with a standardized format for multi-scenario design files is also established.

Result: IGD achieves superior experimental results, demonstrating its effectiveness in graphic design.

Conclusion:  IGD enables end-to-end training, architecturally supporting scalability and extensibility in complex graphic design tasks, and offers a new solution for graphic design.

Abstract: Graphic design visually conveys information and data by creating and
combining text, images and graphics. Two-stage methods that rely primarily on
layout generation lack creativity and intelligence, making graphic design still
labor-intensive. Existing diffusion-based methods generate non-editable graphic
design files at image level with poor legibility in visual text rendering,
which prevents them from achieving satisfactory and practical automated graphic
design. In this paper, we propose Instructional Graphic Designer (IGD) to
swiftly generate multimodal layers with editable flexibility with only natural
language instructions. IGD adopts a new paradigm that leverages parametric
rendering and image asset generation. First, we develop a design platform and
establish a standardized format for multi-scenario design files, thus laying
the foundation for scaling up data. Second, IGD utilizes the multimodal
understanding and reasoning capabilities of MLLM to accomplish attribute
prediction, sequencing and layout of layers. It also employs a diffusion model
to generate image content for assets. By enabling end-to-end training, IGD
architecturally supports scalability and extensibility in complex graphic
design tasks. The superior experimental results demonstrate that IGD offers a
new solution for graphic design.

</details>


### [103] [Crucial-Diff: A Unified Diffusion Model for Crucial Image and Annotation Synthesis in Data-scarce Scenarios](https://arxiv.org/abs/2507.09915)
*Siyue Yao,Mingjie Sun,Eng Gee Lim,Ran Yi,Baojiang Zhong,Moncef Gabbouj*

Main category: cs.CV

TL;DR: 为解决数据稀疏问题，提出Crucial-Diff框架，通过SAFE和WASM模块生成针对性的“关键样本”，提高了模型在MVTec和polyp数据集上的检测和分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型合成的样本重复或过于简单，无法提供针对下游模型弱点的“关键信息”，并且通常需要为不同对象分别训练，计算效率低下。为解决数据稀疏、模型过拟合和数据集不平衡等问题，提出Crucial-Diff框架来合成关键样本。

Method: 提出了一种名为Crucial-Diff的领域无关框架，该框架集成了两个关键模块：场景无关特征提取器（SAFE）和弱点感知样本挖掘器（WASM）。SAFE利用统一的特征提取器捕捉目标信息，而WASM利用下游模型的检测结果反馈来生成难以检测的样本，并与SAFE模块的输出融合。

Result: Crucial-Diff框架生成多样化、高质量的训练数据，在MVTec数据集上实现了83.63%的像素级AP和78.12%的F1-MAX，在polyp数据集上实现了81.64%的mIoU和87.69%的mDice。

Conclusion: Crucial-Diff框架生成多样化、高质量的训练数据，在MVTec数据集上实现了83.63%的像素级AP和78.12%的F1-MAX，在polyp数据集上实现了81.64%的mIoU和87.69%的mDice。

Abstract: The scarcity of data in various scenarios, such as medical, industry and
autonomous driving, leads to model overfitting and dataset imbalance, thus
hindering effective detection and segmentation performance. Existing studies
employ the generative models to synthesize more training samples to mitigate
data scarcity. However, these synthetic samples are repetitive or simplistic
and fail to provide "crucial information" that targets the downstream model's
weaknesses. Additionally, these methods typically require separate training for
different objects, leading to computational inefficiencies. To address these
issues, we propose Crucial-Diff, a domain-agnostic framework designed to
synthesize crucial samples. Our method integrates two key modules. The Scene
Agnostic Feature Extractor (SAFE) utilizes a unified feature extractor to
capture target information. The Weakness Aware Sample Miner (WASM) generates
hard-to-detect samples using feedback from the detection results of downstream
model, which is then fused with the output of SAFE module. Together, our
Crucial-Diff framework generates diverse, high-quality training data, achieving
a pixel-level AP of 83.63% and an F1-MAX of 78.12% on MVTec. On polyp dataset,
Crucial-Diff reaches an mIoU of 81.64% and an mDice of 87.69%. Code will be
released after acceptance.

</details>


### [104] [Can GPT-4o mini and Gemini 2.0 Flash Predict Fine-Grained Fashion Product Attributes? A Zero-Shot Analysis](https://arxiv.org/abs/2507.09950)
*Shubham Shukla,Kunal Sonalkar*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The fashion retail business is centered around the capacity to comprehend
products. Product attribution helps in comprehending products depending on the
business process. Quality attribution improves the customer experience as they
navigate through millions of products offered by a retail website. It leads to
well-organized product catalogs. In the end, product attribution directly
impacts the 'discovery experience' of the customer. Although large language
models (LLMs) have shown remarkable capabilities in understanding multimodal
data, their performance on fine-grained fashion attribute recognition remains
under-explored. This paper presents a zero-shot evaluation of state-of-the-art
LLMs that balance performance with speed and cost efficiency, mainly
GPT-4o-mini and Gemini 2.0 Flash. We have used the dataset
DeepFashion-MultiModal (https://github.com/yumingj/DeepFashion-MultiModal) to
evaluate these models in the attribution tasks of fashion products. Our study
evaluates these models across 18 categories of fashion attributes, offering
insight into where these models excel. We only use images as the sole input for
product information to create a constrained environment. Our analysis shows
that Gemini 2.0 Flash demonstrates the strongest overall performance with a
macro F1 score of 56.79% across all attributes, while GPT-4o-mini scored a
macro F1 score of 43.28%. Through detailed error analysis, our findings provide
practical insights for deploying these LLMs in production e-commerce product
attribution-related tasks and highlight the need for domain-specific
fine-tuning approaches. This work also lays the groundwork for future research
in fashion AI and multimodal attribute extraction.

</details>


### [105] [4D-MISR: A unified model for low-dose super-resolution imaging via feature fusion](https://arxiv.org/abs/2507.09953)
*Zifei Wang,Zian Mao,Xiaoya He,Xi Huang,Haoran Zhang,Chun Cheng,Shufen Chu,Tingzheng Hou,Xiaoqin Zeng,Yujun Xie*

Main category: cs.CV

TL;DR: 通过借鉴多图像超分辨率（MISR）和卷积神经网络（CNN）的原理，开发了一种新的 4D-STEM 方法，实现了在超低剂量下对蛋白质和二维材料等光束敏感材料进行原子级超分辨率成像，克服了辐射损伤的限制。


<details>
  <summary>Details</summary>
Motivation: 为了克服电子显微镜在对光束敏感材料（如蛋白质和二维材料）成像时因辐射损伤而受到的限制。

Method: 提出了一种受多图像超分辨率（MISR）启发的四维扫描透射电子显微镜（4D-STEM）方法，该方法融合了多个低分辨率、亚像素移位的视图，并使用卷积神经网络（CNN）进行重建，该网络整合了来自合成、多角度观测的特征。开发了一种用于 4D-STEM 的双路径、注意力引导网络。

Result: 该方法实现了在超低剂量数据下进行原子级超分辨率成像，能够对非晶态、半晶态和晶态光束敏感样本进行稳健的原子级可视化，其空间分辨率在超低剂量条件下与传统的 ptychography 相当。

Conclusion: 这项工作为辐射敏感材料的结构分析提供了一种新的、可推广的方法，将 4D-STEM 的能力扩展到超低剂量的原子级超分辨率成像。

Abstract: While electron microscopy offers crucial atomic-resolution insights into
structure-property relationships, radiation damage severely limits its use on
beam-sensitive materials like proteins and 2D materials. To overcome this
challenge, we push beyond the electron dose limits of conventional electron
microscopy by adapting principles from multi-image super-resolution (MISR) that
have been widely used in remote sensing. Our method fuses multiple
low-resolution, sub-pixel-shifted views and enhances the reconstruction with a
convolutional neural network (CNN) that integrates features from synthetic,
multi-angle observations. We developed a dual-path, attention-guided network
for 4D-STEM that achieves atomic-scale super-resolution from ultra-low-dose
data. This provides robust atomic-scale visualization across amorphous,
semi-crystalline, and crystalline beam-sensitive specimens. Systematic
evaluations on representative materials demonstrate comparable spatial
resolution to conventional ptychography under ultra-low-dose conditions. Our
work expands the capabilities of 4D-STEM, offering a new and generalizable
method for the structural analysis of radiation-vulnerable materials.

</details>


### [106] [Uncertainty Quantification for Incomplete Multi-View Data Using Divergence Measures](https://arxiv.org/abs/2507.09980)
*Zhipeng Xue,Yan Zhang,Ming Li,Chun Li,Yue Liu,Fei Yu*

Main category: cs.CV

TL;DR: KPHD-Net improves multi-view learning by using H"older divergence for better uncertainty estimation and Dempster-Shafer evidence theory with a Kalman filter for reliable fusion, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing multi-view integration and decision-making methods can be unreliable with noisy or corrupted data. Current methods using KL divergence for uncertainty estimation ignore domain gaps between modalities. The paper proposes KPHD-Net to address these limitations.

Method: KPHD-Net utilizes a variational Dirichlet distribution to represent class probability distributions and models evidence from different views. It integrates this with Dempster-Shafer evidence theory (DST) for improved uncertainty estimation. Proper H"older divergence is used as a measure of distribution discrepancies. DST is combined with the Kalman filter for future state estimations.

Result: Extensive experiments demonstrate that KPHD-Net surpasses current state-of-the-art methods in accuracy, robustness, and reliability for both classification and clustering tasks.

Conclusion: KPHD-Net, which uses H"older divergence and Dempster-Shafer evidence theory combined with a Kalman filter, outperforms current state-of-the-art methods in multi-view classification and clustering tasks concerning accuracy, robustness, and reliability, with theoretical guarantees.

Abstract: Existing multi-view classification and clustering methods typically improve
task accuracy by leveraging and fusing information from different views.
However, ensuring the reliability of multi-view integration and final decisions
is crucial, particularly when dealing with noisy or corrupted data. Current
methods often rely on Kullback-Leibler (KL) divergence to estimate uncertainty
of network predictions, ignoring domain gaps between different modalities. To
address this issue, KPHD-Net, based on H\"older divergence, is proposed for
multi-view classification and clustering tasks. Generally, our KPHD-Net employs
a variational Dirichlet distribution to represent class probability
distributions, models evidences from different views, and then integrates it
with Dempster-Shafer evidence theory (DST) to improve uncertainty estimation
effects. Our theoretical analysis demonstrates that Proper H\"older divergence
offers a more effective measure of distribution discrepancies, ensuring
enhanced performance in multi-view learning. Moreover, Dempster-Shafer evidence
theory, recognized for its superior performance in multi-view fusion tasks, is
introduced and combined with the Kalman filter to provide future state
estimations. This integration further enhances the reliability of the final
fusion results. Extensive experiments show that the proposed KPHD-Net
outperforms the current state-of-the-art methods in both classification and
clustering tasks regarding accuracy, robustness, and reliability, with
theoretical guarantees.

</details>


### [107] [Latent Diffusion Models with Masked AutoEncoders](https://arxiv.org/abs/2507.09984)
*Junho Lee,Jeongwoo Shin,Hyungwook Choi,Joonseok Lee*

Main category: cs.CV

TL;DR: LDM的自编码器设计很重要，我们提出了VMAE，并将其用于LDM，结果更好。


<details>
  <summary>Details</summary>
Motivation: 现有的自编码器无法同时满足潜在平滑性、感知压缩质量和重建质量这三个关键属性，而这些属性对LDM的图像生成至关重要。

Method: 提出了一种名为变分掩码自编码器（VMAE）的新型自编码器，并将其集成到潜在扩散模型（LDM）框架中，形成LDMAE。

Result: 通过大量实验证明，LDMAE在图像生成质量和计算效率方面均有显著提升。

Conclusion: LDMAE通过VMAE的引入，显著提高了图像生成质量和计算效率。

Abstract: In spite of remarkable potential of the Latent Diffusion Models (LDMs) in
image generation, the desired properties and optimal design of the autoencoders
have been underexplored. In this work, we analyze the role of autoencoders in
LDMs and identify three key properties: latent smoothness, perceptual
compression quality, and reconstruction quality. We demonstrate that existing
autoencoders fail to simultaneously satisfy all three properties, and propose
Variational Masked AutoEncoders (VMAEs), taking advantage of the hierarchical
features maintained by Masked AutoEncoder. We integrate VMAEs into the LDM
framework, introducing Latent Diffusion Models with Masked AutoEncoders
(LDMAEs). Through comprehensive experiments, we demonstrate significantly
enhanced image generation quality and computational efficiency.

</details>


### [108] [3DGAA: Realistic and Robust 3D Gaussian-based Adversarial Attack for Autonomous Driving](https://arxiv.org/abs/2507.09993)
*Yixun Zhang,Lizhi Wang,Junjun Zhao,Wending Zhao,Feng Zhou,Yonghao Dang,Jianqin Yin*

Main category: cs.CV

TL;DR: 本研究提出了一种名为3DGAA的新型3D高斯泼溅对抗性攻击方法，通过联合优化几何和外观属性，实现了物理上可实现且具有鲁棒性的对抗性攻击，显著提升了自动驾驶感知系统的安全性评估能力。


<details>
  <summary>Details</summary>
Motivation: 为解决现有2D和3D物理攻击在平衡物理真实性和攻击鲁棒性方面的挑战，本文提出了一种新的相机对象检测对抗性攻击方法，以提高自动驾驶系统的安全性。

Method: 3DGAA框架通过利用3D高斯泼溅（3DGS）的完整14维参数化，联合优化几何属性（形状、尺度、旋转）和外观属性（颜色、不透明度），以物理可实现的方式生成对抗性对象。此外，还引入了物理过滤模块以保持几何保真度，并引入了物理增强模块来模拟复杂的物理场景，从而增强了在真实世界条件下的攻击泛化能力。

Result: 3DGAA在虚拟基准和物理世界设置中均取得了显著成效，将检测mAP从87.21%降低到7.38%，显著优于现有的3D物理攻击。此外，该方法在不同物理条件下保持了高可转移性，展示了其作为一种实用的攻击框架在评估自动驾驶感知系统安全性方面的潜力。

Conclusion: 3DGAA是一种新颖的对抗性对象生成框架，利用3D高斯泼溅（3DGS）的14维参数化来优化几何和外观，从而实现物理可实现性和鲁棒性。通过引入物理过滤和增强模块，该方法提高了在复杂物理场景下的泛化能力。实验证明，3DGAA在减少检测mAP方面显著优于现有的3D物理攻击，并在不同物理条件下表现出高可转移性，代表了物理可实现对抗性攻击的新水平。

Abstract: Camera-based object detection systems play a vital role in autonomous
driving, yet they remain vulnerable to adversarial threats in real-world
environments. While existing 2D and 3D physical attacks typically optimize
texture, they often struggle to balance physical realism and attack robustness.
In this work, we propose 3D Gaussian-based Adversarial Attack (3DGAA), a novel
adversarial object generation framework that leverages the full 14-dimensional
parameterization of 3D Gaussian Splatting (3DGS) to jointly optimize geometry
and appearance in physically realizable ways. Unlike prior works that rely on
patches or texture, 3DGAA jointly perturbs both geometric attributes (shape,
scale, rotation) and appearance attributes (color, opacity) to produce
physically realistic and transferable adversarial objects. We further introduce
a physical filtering module to preserve geometric fidelity, and a physical
augmentation module to simulate complex physical scenarios, thus enhancing
attack generalization under real-world conditions. We evaluate 3DGAA on both
virtual benchmarks and physical-world setups using miniature vehicle models.
Experimental results show that 3DGAA achieves to reduce the detection mAP from
87.21% to 7.38%, significantly outperforming existing 3D physical attacks.
Moreover, our method maintains high transferability across different physical
conditions, demonstrating a new state-of-the-art in physically realizable
adversarial attacks. These results validate 3DGAA as a practical attack
framework for evaluating the safety of perception systems in autonomous
driving.

</details>


### [109] [Leveraging Swin Transformer for enhanced diagnosis of Alzheimer's disease using multi-shell diffusion MRI](https://arxiv.org/abs/2507.09996)
*Quentin Dessain,Nicolas Delinte,Bernard Hanseeuw,Laurence Dricot,Benoît Macq*

Main category: cs.CV

TL;DR: 本研究利用扩散MRI和Swin Transformer进行阿尔茨海默病早期检测，在区分正常与AD患者方面准确率达95.2%，在检测淀粉样蛋白方面准确率达77.2%，并识别出海马旁回和海马为关键区域。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在利用多层扩散MRI数据中的微观结构信息，通过基于视觉Transformer的深度学习框架，支持阿尔茨海默病的早期诊断和淀粉样蛋白积累的检测。

Method: 本研究提出了一种分类流程，在多层扩散MRI数据上使用Swin Transformer（一种分层视觉Transformer模型）对阿尔茨海默病和淀粉样蛋白的存在进行分类。提取了DTI和NODDI的关键指标，并将其投影到2D平面上，以便与ImageNet预训练模型进行迁移学习。为了有效地将Transformer适应于有限的标记神经影像数据，我们集成了低秩自适应。我们评估了该框架在诊断组预测（认知正常、轻度认知障碍、阿尔茨海默病痴呆）和淀粉样蛋白状态分类方面的能力。

Result: 该框架在基于多层扩散MRI特征的分类结果方面具有竞争力，在使用NODDI指标区分认知正常个体和阿尔茨海默病痴呆个体方面，最佳平衡准确率为95.2%。在淀粉样蛋白检测方面，区分淀粉样蛋白阳性的轻度认知障碍/阿尔茨海默病痴呆受试者和淀粉样蛋白阴性的认知正常受试者，其平衡准确率为77.2%；在区分淀粉样蛋白阳性个体和认知正常个体方面，其平衡准确率为67.9%。基于Grad-CAM的可解释性分析确定了与临床相关的脑区，包括海马旁回和海马，是模型预测的关键因素。

Conclusion: 本研究展示了扩散MRI和基于Transformer的架构在阿尔茨海默病和淀粉样蛋白病理的早期检测方面的潜力，支持在数据受限的生物医学环境中进行由生物标志物驱动的诊断。

Abstract: Objective: This study aims to support early diagnosis of Alzheimer's disease
and detection of amyloid accumulation by leveraging the microstructural
information available in multi-shell diffusion MRI (dMRI) data, using a vision
transformer-based deep learning framework.
  Methods: We present a classification pipeline that employs the Swin
Transformer, a hierarchical vision transformer model, on multi-shell dMRI data
for the classification of Alzheimer's disease and amyloid presence. Key metrics
from DTI and NODDI were extracted and projected onto 2D planes to enable
transfer learning with ImageNet-pretrained models. To efficiently adapt the
transformer to limited labeled neuroimaging data, we integrated Low-Rank
Adaptation. We assessed the framework on diagnostic group prediction
(cognitively normal, mild cognitive impairment, Alzheimer's disease dementia)
and amyloid status classification.
  Results: The framework achieved competitive classification results within the
scope of multi-shell dMRI-based features, with the best balanced accuracy of
95.2% for distinguishing cognitively normal individuals from those with
Alzheimer's disease dementia using NODDI metrics. For amyloid detection, it
reached 77.2% balanced accuracy in distinguishing amyloid-positive mild
cognitive impairment/Alzheimer's disease dementia subjects from
amyloid-negative cognitively normal subjects, and 67.9% for identifying
amyloid-positive individuals among cognitively normal subjects. Grad-CAM-based
explainability analysis identified clinically relevant brain regions, including
the parahippocampal gyrus and hippocampus, as key contributors to model
predictions.
  Conclusion: This study demonstrates the promise of diffusion MRI and
transformer-based architectures for early detection of Alzheimer's disease and
amyloid pathology, supporting biomarker-driven diagnostics in data-limited
biomedical settings.

</details>


### [110] [Vision-Based Anti Unmanned Aerial Technology: Opportunities and Challenges](https://arxiv.org/abs/2507.10006)
*Guanghai Ding,Yihua Ren,Yuting Liu,Qijun Zhao,Shuiwang Li*

Main category: cs.CV

TL;DR: 一篇关于反无人机跟踪技术的综述，涵盖了挑战、数据集、算法和未来方向。


<details>
  <summary>Details</summary>
Motivation: 随着无人机技术的发展和广泛应用，实现高效精准的反无人机跟踪变得至关重要，尤其是在公共安全、边境巡逻、搜索救援和农业监测等复杂环境中。

Method: 本文首先回顾了反无人机检测和跟踪技术的特点与挑战，接着调研并整理了公开数据集，最后分析了近年来提出的主要基于视觉和基于视觉融合的反无人机检测与跟踪算法。

Result: 该论文为推动该领域发展提供了有价值的见解，并指出了未来的研究方向。

Conclusion: 该论文概述了反无人机跟踪技术（Anti-UAV tracking technologies）的最新进展，并指出了未来的研究方向。

Abstract: With the rapid advancement of UAV technology and its extensive application in
various fields such as military reconnaissance, environmental monitoring, and
logistics, achieving efficient and accurate Anti-UAV tracking has become
essential. The importance of Anti-UAV tracking is increasingly prominent,
especially in scenarios such as public safety, border patrol, search and
rescue, and agricultural monitoring, where operations in complex environments
can provide enhanced security. Current mainstream Anti-UAV tracking
technologies are primarily centered around computer vision techniques,
particularly those that integrate multi-sensor data fusion with advanced
detection and tracking algorithms. This paper first reviews the characteristics
and current challenges of Anti-UAV detection and tracking technologies. Next,
it investigates and compiles several publicly available datasets, providing
accessible links to support researchers in efficiently addressing related
challenges. Furthermore, the paper analyzes the major vision-based and
vision-fusion-based Anti-UAV detection and tracking algorithms proposed in
recent years. Finally, based on the above research, this paper outlines future
research directions, aiming to provide valuable insights for advancing the
field.

</details>


### [111] [Binomial Self-Compensation: Mechanism and Suppression of Motion Error in Phase-Shifting Profilometry](https://arxiv.org/abs/2507.10009)
*Geyou Zhang,Kai Liu,Ce Zhu*

Main category: cs.CV

TL;DR: 提出I-BSC方法，比P-BSC计算更快，误差更小，适用于动态3D扫描。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有相位移相轮廓术（PSP）在动态测量中因物体运动而易受影响的问题，并克服现有相位补偿方法（如P-BSC）的计算开销和误差累积问题。

Method: 提出了一种名为图像序列二项式自补偿（I-BSC）的新方法，通过对齐像序列而不是相位序列来减轻运动误差，并显著降低了计算复杂性。

Result: 与P-BSC相比，I-BSC将计算复杂度降低了一个多项式阶数，将计算帧率提高了数倍到数十倍，并实现了更快的运动误差收敛，从而在高像素深度时间分辨率下进行3D重建。

Conclusion: I-BSC通过对齐像序列而不是相位序列来减轻运动误差，并显著降低了计算复杂性，实现了接近单次拍摄的帧率，从而能够在高像素深度时间分辨率下进行3D重建。

Abstract: Phase shifting profilometry (PSP) is widely used in high-precision 3D
scanning due to its high accuracy, robustness, and pixel-wise handling.
However, a fundamental assumption of PSP that the object should remain static
does not hold in dynamic measurement, making PSP susceptible to object motion.
To address this challenge, our proposed solution, phase-sequential binomial
self-compensation (P-BSC), sums successive motion-affected phase frames
weighted by binomial coefficients. This approach exponentially reduces the
motion error in a pixel-wise and frame-wise loopable manner. Despite its
efficacy, P-BSC suffers from high computational overhead and error accumulation
due to its reliance on multi-frame phase calculations and weighted summations.
Inspired by P-BSC, we propose an image-sequential binomial self-compensation
(I-BSC) to weight sum the homogeneous fringe images instead of successive phase
frames, which generalizes the BSC concept from phase sequences to image
sequences. I-BSC computes the arctangent function only once, resolving both
limitations in P-BSC. Extensive analysis, simulations, and experiments show
that 1) the proposed BSC outperforms existing methods in reducing motion error
while achieving a quasi-single-shot frame rate, i.e., depth map frame rate
equals to the camera's acquisition rate, enabling 3D reconstruction with high
pixel-depth-temporal resolution; 2) compared to P-BSC, our I-BSC reduces the
computational complexity by one polynomial order, thereby accelerating the
computational frame rate by several to dozen times, while also reaching faster
motion error convergence.

</details>


### [112] [(Almost) Free Modality Stitching of Foundation Models](https://arxiv.org/abs/2507.10015)
*Jaisidh Singh,Diganta Misra,Boris Knyazev,Antonio Orvieto*

Main category: cs.CV

TL;DR: Automating uni-modal model selection and connector training for foundation models using hypernetworks (Hyma), saving time and computational resources.


<details>
  <summary>Details</summary>
Motivation: The process of selecting uni-modal models and training connector modules for foundation multi-modal models is computationally demanding due to the complexity of large-scale datasets and the increasing number of available uni-modal models.

Method: Hyma utilizes hypernetworks to predict parameters for jointly trained connector modules, enabling efficient selection and training for N x M uni-modal model combinations.

Result: Hyma reduces the optimal uni-modal model pair search cost by 10x on average and matches the performance of grid search across various multi-modal benchmarks.

Conclusion: Hyma is an effective solution for optimal uni-modal model selection and connector training, reducing search cost by 10x while maintaining performance.

Abstract: Foundation multi-modal models are often designed by stitching of multiple
existing pretrained uni-modal models: for example, an image classifier with an
autoregressive text model. This stitching process is performed by training a
connector module that aims to align the representation-representation or
representation-input spaces of these uni-modal models. However, given the
complexity of training such connectors on large scale web-based datasets
coupled with the ever-increasing number of available pretrained uni-modal
models, the task of uni-modal models selection and subsequent connector module
training becomes computationally demanding. To address this under-studied
critical problem, we propose Hypernetwork Model Alignment (Hyma), a novel
all-in-one solution for optimal uni-modal model selection and connector
training by leveraging hypernetworks. Specifically, our framework utilizes the
parameter prediction capability of a hypernetwork to obtain jointly trained
connector modules for $N \times M$ combinations of uni-modal models. In our
experiments, Hyma reduces the optimal uni-modal model pair search cost by
$10\times$ (averaged across all experiments), while matching the ranking and
trained connector performance obtained via grid search across a suite of
diverse multi-modal benchmarks.

</details>


### [113] [Memory-Efficient Personalization of Text-to-Image Diffusion Models via Selective Optimization Strategies](https://arxiv.org/abs/2507.10029)
*Seokeon Choi,Sunghyun Park,Hyoungwoo Park,Jeongho Kim,Sungrack Yun*

Main category: cs.CV

TL;DR: 提出了一种结合BP-low和ZO-high的内存高效个性化方法，通过时间步感知概率函数动态选择策略，以提高性能和效率。


<details>
  <summary>Details</summary>
Motivation: 为了在保持用户隐私和适应边缘设备的有限计算资源的同时，实现文本到图像扩散模型的内存高效个性化。

Method: 提出了一种选择性优化框架，根据扩散过程的特性自适应地选择在低分辨率图像上进行反向传播（BP-low）或在高分辨率图像上进行无导数优化（ZO-high）。引入了一个时间步感知概率函数，根据扩散时间步动态选择优化策略，以减轻BP-low在高时间步的过拟合，并更有效地应用ZO-high。

Result: 实验结果表明，该方法在实现具有竞争力的性能的同时，显著降低了内存消耗，实现了可扩展、高质量的设备内个性化，且没有增加推理延迟。

Conclusion: 该方法通过结合低分辨率图像上的反向传播（BP-low）和高分辨率图像上的无导数优化（ZO-high），实现了内存高效且高质量的个性化，同时在设备上运行而不会增加推理延迟。

Abstract: Memory-efficient personalization is critical for adapting text-to-image
diffusion models while preserving user privacy and operating within the limited
computational resources of edge devices. To this end, we propose a selective
optimization framework that adaptively chooses between backpropagation on
low-resolution images (BP-low) and zeroth-order optimization on high-resolution
images (ZO-high), guided by the characteristics of the diffusion process. As
observed in our experiments, BP-low efficiently adapts the model to
target-specific features, but suffers from structural distortions due to
resolution mismatch. Conversely, ZO-high refines high-resolution details with
minimal memory overhead but faces slow convergence when applied without prior
adaptation. By complementing both methods, our framework leverages BP-low for
effective personalization while using ZO-high to maintain structural
consistency, achieving memory-efficient and high-quality fine-tuning. To
maximize the efficacy of both BP-low and ZO-high, we introduce a timestep-aware
probabilistic function that dynamically selects the appropriate optimization
strategy based on diffusion timesteps. This function mitigates the overfitting
from BP-low at high timesteps, where structural information is critical, while
ensuring ZO-high is applied more effectively as training progresses.
Experimental results demonstrate that our method achieves competitive
performance while significantly reducing memory consumption, enabling scalable,
high-quality on-device personalization without increasing inference latency.

</details>


### [114] [CoSMo: A Multimodal Transformer for Page Stream Segmentation in Comic Books](https://arxiv.org/abs/2507.10053)
*Marc Serra Ortega,Emanuele Vivoli,Artemis Llabrés,Dimosthenis Karatzas*

Main category: cs.CV

TL;DR: CoSMo 是一个用于漫画书页面流分割的新型多模态 Transformer 模型，在各项指标上均表现出色，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 页面流分割是漫画书内容理解的关键任务，是许多下游任务（如角色分析、故事索引或元数据丰富）的必要第一阶段。

Method: 提出了一种新颖的多模态 Transformer 模型 CoSMo，并开发了其仅视觉和多模态变体，用于漫画书的页面流分割。

Result: CoSMo 在 F1-Macro、全景质量和流级别指标上持续优于传统基线和更大的通用视觉语言模型，证明了视觉特征在漫画页面流分割宏观结构中的主导地位，并展示了多模态在解决模糊性方面的优势。

Conclusion: CoSMo 在漫画书页面流分割任务上取得了新的最先进成果，为可扩展的漫画书分析铺平了道路。

Abstract: This paper introduces CoSMo, a novel multimodal Transformer for Page Stream
Segmentation (PSS) in comic books, a critical task for automated content
understanding, as it is a necessary first stage for many downstream tasks like
character analysis, story indexing, or metadata enrichment. We formalize PSS
for this unique medium and curate a new 20,800-page annotated dataset. CoSMo,
developed in vision-only and multimodal variants, consistently outperforms
traditional baselines and significantly larger general-purpose vision-language
models across F1-Macro, Panoptic Quality, and stream-level metrics. Our
findings highlight the dominance of visual features for comic PSS
macro-structure, yet demonstrate multimodal benefits in resolving challenging
ambiguities. CoSMo establishes a new state-of-the-art, paving the way for
scalable comic book analysis.

</details>


### [115] [Lightweight Model for Poultry Disease Detection from Fecal Images Using Multi-Color Space Feature Optimization and Machine Learning](https://arxiv.org/abs/2507.10056)
*A. K. M. Shoriful Islam,Md. Rakib Hassan,Macbah Uddin,Md. Shahidur Rahman*

Main category: cs.CV

TL;DR: 开发了一种轻量级机器学习模型，通过分析禽类粪便图像来检测家禽疾病，实现了高精度和低资源消耗，优于一些深度学习模型。


<details>
  <summary>Details</summary>
Motivation: 为了解决家禽养殖业易受球虫病、沙门氏菌病和新城疫等传染病影响的问题，本研究旨在开发一种轻量级的机器学习方法，通过分析禽类粪便图像来检测这些疾病，以提供一种比深度学习更具成本效益、可解释且可扩展的解决方案。

Method: 本研究采用多颜色空间特征提取（RGB, HSV, LAB）和多种基于颜色、纹理和形状的描述符，并通过主成分分析（PCA）和XGBoost特征选择进行降维，最终确定了一个紧凑的全局特征集，并使用人工神经网络（ANN）进行分类。

Result: 所提出的方法实现了95.85%的准确率，并且不需要GPU，在Google Colab上仅需638秒的执行时间。与Xception和MobileNetV3等深度学习模型相比，该模型在准确率相当的情况下，资源消耗大大降低。

Conclusion: 该研究提出了一种基于轻量级机器学习的禽类粪便图像疾病检测方法，该方法在保持高精度的同时，显著降低了计算资源需求，为低资源农业环境提供了可行的实时禽类疾病检测替代方案。

Abstract: Poultry farming is a vital component of the global food supply chain, yet it
remains highly vulnerable to infectious diseases such as coccidiosis,
salmonellosis, and Newcastle disease. This study proposes a lightweight machine
learning-based approach to detect these diseases by analyzing poultry fecal
images. We utilize multi-color space feature extraction (RGB, HSV, LAB) and
explore a wide range of color, texture, and shape-based descriptors, including
color histograms, local binary patterns (LBP), wavelet transforms, and edge
detectors. Through a systematic ablation study and dimensionality reduction
using PCA and XGBoost feature selection, we identify a compact global feature
set that balances accuracy and computational efficiency. An artificial neural
network (ANN) classifier trained on these features achieved 95.85% accuracy
while requiring no GPU and only 638 seconds of execution time in Google Colab.
Compared to deep learning models such as Xception and MobileNetV3, our proposed
model offers comparable accuracy with drastically lower resource usage. This
work demonstrates a cost-effective, interpretable, and scalable alternative to
deep learning for real-time poultry disease detection in low-resource
agricultural settings.

</details>


### [116] [MoVieS: Motion-Aware 4D Dynamic View Synthesis in One Second](https://arxiv.org/abs/2507.10065)
*Chenguo Lin,Yuchen Lin,Panwang Pan,Yifan Yu,Honglei Yan,Katerina Fragkiadaki,Yadong Mu*

Main category: cs.CV

TL;DR: MoVieS是一个创新的模型，可以从视频中快速合成动态的3D视图，并统一处理外观、几何和运动，从而实现多种应用。


<details>
  <summary>Details</summary>
Motivation: 为了在单个学习框架内实现视图合成、重建和3D点跟踪，并能够对动态3D场景进行建模。

Method: MoVieS是一个新颖的前馈模型，使用像素对齐的高斯原语网格来表示动态3D场景，并显式监督其时变运动，从而统一了外观、几何和运动的建模。

Result: MoVieS能够在一秒钟内从单眼视频合成4D动态新颖视图，实现了视图合成、重建和3D点跟踪的统一建模，并支持零样本应用，如场景流估计和运动对象分割。

Conclusion: MoVieS在多个任务上展示了其有效性和效率，实现了具有竞争力的性能，同时提供了数量级上的加速。

Abstract: We present MoVieS, a novel feed-forward model that synthesizes 4D dynamic
novel views from monocular videos in one second. MoVieS represents dynamic 3D
scenes using pixel-aligned grids of Gaussian primitives, explicitly supervising
their time-varying motion. This allows, for the first time, the unified
modeling of appearance, geometry and motion, and enables view synthesis,
reconstruction and 3D point tracking within a single learning-based framework.
By bridging novel view synthesis with dynamic geometry reconstruction, MoVieS
enables large-scale training on diverse datasets with minimal dependence on
task-specific supervision. As a result, it also naturally supports a wide range
of zero-shot applications, such as scene flow estimation and moving object
segmentation. Extensive experiments validate the effectiveness and efficiency
of MoVieS across multiple tasks, achieving competitive performance while
offering several orders of magnitude speedups.

</details>


### [117] [Frequency Regulation for Exposure Bias Mitigation in Diffusion Models](https://arxiv.org/abs/2507.10072)
*Meng Yu,Kun Zhan*

Main category: cs.CV

TL;DR: 扩散模型存在曝光偏差问题，该研究提出了一种频域正则化方法，通过小波变换分别调整低频和高频子带，以解决此问题。该方法训练无关且即插即用，能够提升生成质量并解决不同模型架构的曝光偏差问题。


<details>
  <summary>Details</summary>
Motivation: 扩散模型受到曝光偏差的显著影响，并且在扩散过程中预测的噪声图像的能量会降低。

Method: 提出了一种利用小波变换的频域正则化机制，该机制分别调整低频和高频子带。利用能量降低在两个子带中的不同模式，可以更准确地分析曝光偏差。

Result: 能量降低遵循低频和高频子带中不同的模式，能量降低会导致网络重建的清洁数据和真实的清洁数据之间的幅度变化。

Conclusion: 该方法是训练无关且即插即用的，显著提高了各种扩散模型的生成质量，并为不同模型架构提供了鲁棒的曝光偏差解决方案。

Abstract: Diffusion models exhibit impressive generative capabilities but are
significantly impacted by exposure bias. In this paper, we make a key
observation: the energy of the predicted noisy images decreases during the
diffusion process. Building on this, we identify two important findings: 1) The
reduction in energy follows distinct patterns in the low-frequency and
high-frequency subbands; 2) This energy reduction results in amplitude
variations between the network-reconstructed clean data and the real clean
data. Based on the first finding, we introduce a frequency-domain regulation
mechanism utilizing wavelet transforms, which separately adjusts the low- and
high-frequency subbands. Leveraging the second insight, we provide a more
accurate analysis of exposure bias in the two subbands. Our method is
training-free and plug-and-play, significantly improving the generative quality
of various diffusion models and providing a robust solution to exposure bias
across different model architectures. The source code is available at
https://github.com/kunzhan/wpp.

</details>


### [118] [A Transfer Learning-Based Method for Water Body Segmentation in Remote Sensing Imagery: A Case Study of the Zhada Tulin Area](https://arxiv.org/abs/2507.10084)
*Haonan Chen,Xin Tong*

Main category: cs.CV

TL;DR: 针对遥感水体分割的域偏移和少样本问题，采用SegFormer模型进行两阶段迁移学习，显著提升了分割精度。


<details>
  <summary>Details</summary>
Motivation: 为了解决遥感影像水体分割中普遍存在的域偏移和样本量小等挑战。

Method: 本研究提出并验证了一种基于SegFormer模型，面向遥感影像水体分割的两阶段迁移学习策略。首先在多样化的源域上训练基础分割模型，然后在差异显著的目标域上进行微调。

Result: 该策略将遥感影像水体分割任务的IoU从直接迁移的25.50%显著提升至64.84%，有效解决了因域差异导致的模型性能下降问题。

Conclusion: 该研究提出的两阶段迁移学习策略有效解决了遥感影像水体分割在域适应和少样本场景下的性能下降问题，为数据稀疏和环境独特的遥感场景提供了高精度信息提取的技术范例。

Abstract: To address the prevalent challenges of domain shift and small sample sizes in
remote sensing image water body segmentation, this study proposes and validates
a two-stage transfer learning strategy based on the SegFormer model. The
approach begins by training a foundational segmentation model on a diverse
source domain, where it achieves an Intersection over Union (IoU) of 68.80% on
its validation set, followed by fine-tuning on data from the distinct target
domain. Focusing on the Zhada Tulin area in Tibet -- a region characterized by
highly complex topography and spectral features -- the experimental results
demonstrate that this strategy significantly boosts the IoU for the water body
segmentation task from 25.50% (for direct transfer) to 64.84%. This not only
effectively resolves the model performance degradation caused by domain
discrepancy but also provides an effective technical paradigm for
high-precision thematic information extraction in data-scarce and
environmentally unique remote sensing scenarios.

</details>


### [119] [FIX-CLIP: Dual-Branch Hierarchical Contrastive Learning via Synthetic Captions for Better Understanding of Long Text](https://arxiv.org/abs/2507.10095)
*Bingchao Wang,Zhiwei Ning,Jianyu Ding,Xuanang Gao,Yin Li,Dongsheng Jiang,Jie Yang,Wei Liu*

Main category: cs.CV

TL;DR: CLIP在处理长文本时存在局限性。本文提出的FIX-CLIP模型通过双分支训练、多区域提示和层次特征对齐等方法，提升了长文本处理能力，并在长短文本检索任务上取得了最优性能，同时也能用于扩散模型。


<details>
  <summary>Details</summary>
Motivation: CLIP模型在短文本任务上表现出色，但受限于文本编码器的输入长度，难以处理长文本输入（>77个标记）的下游任务。

Method: 提出了一种名为FIX-CLIP的模型，包含三个新颖的模块：1. 一个双分支训练流程，分别将短文本与掩码图像、长文本与原始图像对齐，以提升长文本表示能力并保持短文本能力。2. Transformer层中的多区域可学习提示和单向掩码，用于区域信息提取。3. 中间编码器层中的层次特征对齐模块，以促进多尺度特征的一致性。此外，收集了3000万张图像并利用现有的多模态大型语言模型（MLLM）合成了用于训练的长文本标题。

Result: FIX-CLIP在长文本和短文本检索基准上均取得了最先进的性能，并且其文本编码器在扩散模型中表现出有前景的即插即用性能。

Conclusion: FIX-CLIP在长文本和短文本检索基准上均取得了最先进的性能，并且可以即插即用地应用于需要长文本输入的扩散模型。

Abstract: CLIP has shown promising performance across many short-text tasks in a
zero-shot manner. However, limited by the input length of the text encoder,
CLIP struggles on under-stream tasks with long-text inputs (>77 tokens). To
remedy this issue, we propose FIX-CLIP which includes three novel modules: (1)
A dual-branch training pipeline that aligns short and long texts with masked
and raw images respectively, which boosts the long-text representation while
preserving the short-text ability. (2) Multiple learnable regional prompts with
unidirectional masks in Transformer layers for regional information extraction.
(3) A hierarchical feature alignment module in the intermediate encoder layers
to promote the consistency of multi-scale features. Furthermore, we collect 30M
images and utilize existing MLLMs to synthesize long-text captions for
training. Extensive experiments show that FIX-CLIP achieves state-of-the-art
performance on both long-text and short-text retrieval benchmarks. For
downstream applications, we reveal that FIX-CLIP's text encoder delivers
promising performance in a plug-and-play manner for diffusion models with
long-text input.

</details>


### [120] [Glance-MCMT: A General MCMT Framework with Glance Initialization and Progressive Association](https://arxiv.org/abs/2507.10115)
*Hamidreza Hashempoor*

Main category: cs.CV

TL;DR: A multi-camera tracking method that uses trajectory and appearance to consistently assign IDs across cameras, starting with single-camera tracking and then matching across views with a priority system.


<details>
  <summary>Details</summary>
Motivation: To ensure consistent global identity assignment across multiple camera views in multi-target tracking.

Method: A multi-camera multi-target tracking framework is proposed. It begins with BoT-SORT for single-camera tracking, then uses a global ID initialization phase based on trajectory-feature matching. Subsequent frames are matched to global identities using a prioritized matching strategy, introducing new IDs only when no suitable match is found. 3D positions are estimated for spatial validation.

Result: The framework successfully assigns consistent global identities across views by effectively matching tracklets to existing global identities and minimizing the introduction of new IDs.

Conclusion: The proposed framework ensures consistent global identity assignment across multiple cameras by prioritizing existing global identities and introducing new ones only when necessary, validated by 3D spatial positioning.

Abstract: We propose a multi-camera multi-target (MCMT) tracking framework that ensures
consistent global identity assignment across views using trajectory and
appearance cues. The pipeline starts with BoT-SORT-based single-camera
tracking, followed by an initial glance phase to initialize global IDs via
trajectory-feature matching. In later frames, new tracklets are matched to
existing global identities through a prioritized global matching strategy. New
global IDs are only introduced when no sufficiently similar trajectory or
feature match is found. 3D positions are estimated using depth maps and
calibration for spatial validation.

</details>


### [121] [DEARLi: Decoupled Enhancement of Recognition and Localization for Semi-supervised Panoptic Segmentation](https://arxiv.org/abs/2507.10118)
*Ivan Martinović,Josip Šarić,Marin Oršić,Matej Kristan,Siniša Šegvić*

Main category: cs.CV

TL;DR: 通过结合CLIP和SAM预训练模型，DEARLi在半监督全景分割任务中实现了高性能，同时显著降低了计算资源的需求。


<details>
  <summary>Details</summary>
Motivation: 像素级标注成本高昂且耗时，而半监督分割方法通过利用少量标记图像和大量未标记图像来学习模型。然而，如何有效利用预训练模型来解决标签稀疏性问题仍有待探索。

Method: 该方法利用两个专门的预训练模型，通过结合无监督掩码Transformer一致性和CLIP的零样本分类来增强识别能力，并通过与SAM伪标签的类别无关解码器预热来增强定位能力。

Result: DEARLi在半监督全景分割方面取得了显著的性能提升，尤其是在标签数据有限的情况下。与现有方法相比，DEARLi在半监督语义分割方面取得了更大的优势，并且GPU内存占用减少了8倍。

Conclusion: 该研究提出的DEARLi方法在具有大型类别和有限标签数据的最具挑战性的半监督场景中表现尤为出色，在ADE20K数据集上仅用158个标记图像即可达到29.9 PQ和38.9 mIoU的性能。

Abstract: Pixel-level annotation is expensive and time-consuming. Semi-supervised
segmentation methods address this challenge by learning models on few labeled
images alongside a large corpus of unlabeled images. Although foundation models
could further account for label scarcity, effective mechanisms for their
exploitation remain underexplored. We address this by devising a novel
semi-supervised panoptic approach fueled by two dedicated foundation models. We
enhance recognition by complementing unsupervised mask-transformer consistency
with zero-shot classification of CLIP features. We enhance localization by
class-agnostic decoder warm-up with respect to SAM pseudo-labels. The resulting
decoupled enhancement of recognition and localization (DEARLi) particularly
excels in the most challenging semi-supervised scenarios with large taxonomies
and limited labeled data. Moreover, DEARLi outperforms the state of the art in
semi-supervised semantic segmentation by a large margin while requiring 8x less
GPU memory, in spite of being trained only for the panoptic objective. We
observe 29.9 PQ and 38.9 mIoU on ADE20K with only 158 labeled images. The
source code is available at https://github.com/helen1c/DEARLi.

</details>


### [122] [Taming Modern Point Tracking for Speckle Tracking Echocardiography via Impartial Motion](https://arxiv.org/abs/2507.10127)
*Md Abulkalam Azad,John Nyberg,Håvard Dalen,Bjørnar Grenne,Lasse Lovstakken,Andreas Østvik*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Accurate motion estimation for tracking deformable tissues in
echocardiography is essential for precise cardiac function measurements. While
traditional methods like block matching or optical flow struggle with intricate
cardiac motion, modern point tracking approaches remain largely underexplored
in this domain. This work investigates the potential of state-of-the-art (SOTA)
point tracking methods for ultrasound, with a focus on echocardiography.
Although these novel approaches demonstrate strong performance in general
videos, their effectiveness and generalizability in echocardiography remain
limited. By analyzing cardiac motion throughout the heart cycle in real B-mode
ultrasound videos, we identify that a directional motion bias across different
views is affecting the existing training strategies. To mitigate this, we
refine the training procedure and incorporate a set of tailored augmentations
to reduce the bias and enhance tracking robustness and generalization through
impartial cardiac motion. We also propose a lightweight network leveraging
multi-scale cost volumes from spatial context alone to challenge the advanced
spatiotemporal point tracking models. Experiments demonstrate that fine-tuning
with our strategies significantly improves models' performances over their
baselines, even for out-of-distribution (OOD) cases. For instance, EchoTracker
boosts overall position accuracy by 60.7% and reduces median trajectory error
by 61.5% across heart cycle phases. Interestingly, several point tracking
models fail to outperform our proposed simple model in terms of tracking
accuracy and generalization, reflecting their limitations when applied to
echocardiography. Nevertheless, clinical evaluation reveals that these methods
improve GLS measurements, aligning more closely with expert-validated,
semi-automated tools and thus demonstrating better reproducibility in
real-world applications.

</details>


### [123] [Deep Recurrence for Dynamical Segmentation Models](https://arxiv.org/abs/2507.10143)
*David Calhas,Arlindo L. Oliveira*

Main category: cs.CV

TL;DR: Biological vision uses feedback for refinement; this paper adds a feedback loop to a U-Net for image segmentation, improving performance in noise and with less data. It works well even with just two examples.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of purely feedforward artificial neural networks by introducing a feedback mechanism that allows iterative refinement of perception, inspired by biological vision systems.

Method: Implemented a predictive coding inspired feedback mechanism within a U-Net architecture, incorporating softmax projection and exponential decay for loop stability.

Result: The feedback model significantly outperformed its feedforward counterpart in noisy conditions and generalized more effectively with limited supervision, achieving above random performance with only two training examples compared to the feedforward model's requirement of at least four.

Conclusion: Feedback enhances robustness and data efficiency in neural networks, offering a path toward more adaptive and biologically inspired architectures.

Abstract: While biological vision systems rely heavily on feedback connections to
iteratively refine perception, most artificial neural networks remain purely
feedforward, processing input in a single static pass. In this work, we propose
a predictive coding inspired feedback mechanism that introduces a recurrent
loop from output to input, allowing the model to refine its internal state over
time. We implement this mechanism within a standard U-Net architecture and
introduce two biologically motivated operations, softmax projection and
exponential decay, to ensure stability of the feedback loop. Through controlled
experiments on a synthetic segmentation task, we show that the feedback model
significantly outperforms its feedforward counterpart in noisy conditions and
generalizes more effectively with limited supervision. Notably, feedback
achieves above random performance with just two training examples, while the
feedforward model requires at least four. Our findings demonstrate that
feedback enhances robustness and data efficiency, and offer a path toward more
adaptive and biologically inspired neural architectures. Code is available at:
github.com/DCalhas/feedback_segmentation.

</details>


### [124] [SlumpGuard: An AI-Powered Real-Time System for Automated Concrete Slump Prediction via Video Analysis](https://arxiv.org/abs/2507.10171)
*Youngmin Kim,Giyeong Oh,Kwangsoo Youm,Youngjae Yu*

Main category: cs.CV

TL;DR: 介绍了一个名为 SlumpGuard 的 AI 驱动的视频系统，可以实时自动测量混凝土的和易性，取代了手动且耗时的 માત્ર পরীক্ষা (slump test)，提高了准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统 শুধুমাত্র পরীক্ষা (slump test) 手动、耗时且不一致的缺点，以实现对混凝土和易性的实时监控。

Method: 提出了一种名为 SlumpGuard 的、基于 AI 和视频的系统，该系统能够自动分析卡车卸料槽中的混凝土流动性，从而实时评估其和易性。

Result: 通过系统设计、专用数据集构建以及实际部署的实证结果，证明了 SlumpGuard 在现代混凝土质量保证方面的有效性。

Conclusion: 所提出的 SlumpGuard 系统作为一种实用的解决方案，能够有效提升混凝土质量保证的准确性和效率。

Abstract: Concrete workability is essential for construction quality, with the slump
test being the most common on-site method for its assessment. However,
traditional slump testing is manual, time-consuming, and prone to
inconsistency, limiting its applicability for real-time monitoring. To address
these challenges, we propose SlumpGuard, an AI-powered, video-based system that
automatically analyzes concrete flow from the truck chute to assess workability
in real time. Our system enables full-batch inspection without manual
intervention, improving both the accuracy and efficiency of quality control. We
present the system design, a the construction of a dedicated dataset, and
empirical results from real-world deployment, demonstrating the effectiveness
of SlumpGuard as a practical solution for modern concrete quality assurance.

</details>


### [125] [Minimizing the Pretraining Gap: Domain-aligned Text-Based Person Retrieval](https://arxiv.org/abs/2507.10195)
*Shuyu Yang,Yaxiong Wang,Yongrui Li,Li Zhu,Zhedong Zheng*

Main category: cs.CV

TL;DR: 为了解决合成预训练数据和真实世界目标数据之间的域隙问题，本研究提出了一个包含图像级域适应（DaD）和区域级关系对齐（MRA）的统一文本到图像检索流程，并在多个数据集上取得了优越性能。


<details>
  <summary>Details</summary>
Motivation: 合成数据在基于文本的person检索中被广泛用于预训练模型，但合成预训练数据集与真实世界目标数据集之间存在显著的域隙，阻碍了预训练-微调范例的有效性。

Method: 提出了一种统一的基于文本的 person 检索流程，包括两个主要部分：用于图像级自适应的域感知扩散（DaD）和用于区域级自适应的多粒度关系对齐（MRA）。DaD用于迁移预训练数据集域的图像分布到目标真实世界数据集域，MRA通过建立视觉区域与其描述性句子之间的对应关系来进行细粒度的区域级对齐。

Result: 实验表明，所提出的双层自适应方法在CUHK-PEDES、ICFG-PEDES和RSTPReid数据集上取得了最先进的结果，超越了现有方法。

Conclusion: 该方法在CUHK-PEDES、ICFG-PEDES和RSTPReid数据集上取得了最先进的结果，优于现有方法。

Abstract: In this work, we focus on text-based person retrieval, which aims to identify
individuals based on textual descriptions. Given the significant privacy issues
and the high cost associated with manual annotation, synthetic data has become
a popular choice for pretraining models, leading to notable advancements.
However, the considerable domain gap between synthetic pretraining datasets and
real-world target datasets, characterized by differences in lighting, color,
and viewpoint, remains a critical obstacle that hinders the effectiveness of
the pretrain-finetune paradigm. To bridge this gap, we introduce a unified
text-based person retrieval pipeline considering domain adaptation at both
image and region levels. In particular, it contains two primary components,
i.e., Domain-aware Diffusion (DaD) for image-level adaptation and
Multi-granularity Relation Alignment (MRA) for region-level adaptation. As the
name implies, Domain-aware Diffusion is to migrate the distribution of images
from the pretraining dataset domain to the target real-world dataset domain,
e.g., CUHK-PEDES. Subsequently, MRA performs a meticulous region-level
alignment by establishing correspondences between visual regions and their
descriptive sentences, thereby addressing disparities at a finer granularity.
Extensive experiments show that our dual-level adaptation method has achieved
state-of-the-art results on the CUHK-PEDES, ICFG-PEDES, and RSTPReid datasets,
outperforming existing methodologies. The dataset, model, and code are
available at https://github.com/Shuyu-XJTU/MRA.

</details>


### [126] [A Training-Free, Task-Agnostic Framework for Enhancing MLLM Performance on High-Resolution Images](https://arxiv.org/abs/2507.10202)
*Jaeseong Lee,Yeeun Choi,Heechan Choi,Hanjung Kim,Seonjoo Kim*

Main category: cs.CV

TL;DR: ECP框架通过识别候选区域并进行预测，解决了MLLM在高分辨率图像上的性能瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型（MLLM）在处理需要精细定位和推理的高分辨率图像时存在困难，因为它们通常在固定的图像分辨率下进行微调，导致训练-测试分辨率不匹配或降采样导致细节丢失。

Method: ECP框架首先利用多模态大语言模型（MLLM）在降采样图像上的预测来识别候选区域，然后在这些候选区域上进行精细化的预测，从而在处理高分辨率图像时保留细节并克服性能下降的问题。

Result: 在4K GUI基础和4K、8K MLLM感知任务上，ECP框架相较于基线模型分别取得了+21.3%、+5.8%、+5.2%的绝对性能提升，证明了其有效性。

Conclusion: 提出了一种名为提取候选后预测（ECP）的新颖的、无需训练且任务无关的两阶段框架，用于提高多模态大语言模型（MLLM）在处理高分辨率图像时的性能。ECP通过首先使用粗略预测识别候选区域，然后在候选区域的基础上进行最终预测，从而有效保留了细粒度的细节，并减轻了高分辨率数据带来的挑战。该框架在4K GUI基础和4K、8K MLLM感知任务上进行了验证，分别取得了+21.3%、+5.8%、+5.2%的绝对性能提升。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable
capabilities in vision-language understanding, reasoning, and generation.
However, they struggle with tasks requiring fine-grained localization and
reasoning in high-resolution images. This constraint stems from the fact that
MLLMs are fine-tuned with fixed image resolution to align with the pre-trained
image encoder used in MLLM. Consequently, feeding high-resolution images
directly into MLLMs leads to poor generalization due to a train-test resolution
discrepancy, while downsampling these images-although ensuring
consistency-compromises fine-grained visual details and ultimately degrades
performance. To address this challenge, we propose Extract Candidate then
Predict (ECP), a novel training-free, task-agnostic two-stage framework
designed to enhance MLLM performance on high-resolution images. The key
intuition behind ECP is that while MLLMs struggle with high-resolution images,
their predictions on downsampled images still contain implicit localization
cues. By first identifying candidate region using the coarse prediction and
then predicting the final output based on candidate region, ECP effectively
preserves fine-grained details while mitigating the challenges posed by
high-resolution data. We validate our framework on 4K GUI grounding and 4K, 8K
MLLM perception, achieving +21.3%, +5.8%, +5.2% absolute improvement compared
to baseline respectively, demonstrating its effectiveness. Code is available at
https://github.com/yenncye/ECP.

</details>


### [127] [Improving Multimodal Learning via Imbalanced Learning](https://arxiv.org/abs/2507.10203)
*Shicai Wei,Chunbo Luo,Yang Luo*

Main category: cs.CV

TL;DR: 该论文提出了一种名为ARL的策略，通过非对称优化来解决多模态学习中的欠优化问题，并通过实验证明了其有效性和通用性。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态学习方法在处理欠优化问题时，通常将问题归因于模态间的不平衡学习，并试图通过梯度平衡来解决。然而，该论文认为，不平衡学习并非多模态学习的最优设置。

Method: 提出了一种名为Asymmetric Representation Learning(ARL)的策略，通过非对称优化来辅助多模态学习。ARL引入了额外的正则化器来计算每个模态的预测方差，并根据单模态方差计算系数来重新加权每个模态的优化过程，从而使得模态依赖性比例与模态方差比例成反比。此外，为了最小化泛化误差，ARL还引入了每个模态的预测偏差，并与多模态损失联合优化。该策略不引入额外的参数，并且不依赖于多模态模型的结构和融合方法。

Result: 通过偏差-方差分析证明，遵循与其方差成反比的比例的不平衡的每个模态的依赖性有助于实现最优性能。该方法在各种数据集上进行了广泛的实验验证。

Conclusion: 实验结果表明Asymmetric Representation Learning(ARL)策略的有效性和通用性。

Abstract: Multimodal learning often encounters the under-optimized problem and may
perform worse than unimodal learning. Existing approaches attribute this issue
to imbalanced learning across modalities and tend to address it through
gradient balancing. However, this paper argues that balanced learning is not
the optimal setting for multimodal learning. With bias-variance analysis, we
prove that imbalanced dependency on each modality obeying the inverse ratio of
their variances contributes to optimal performance. To this end, we propose the
Asymmetric Representation Learning(ARL) strategy to assist multimodal learning
via imbalanced optimization. ARL introduces auxiliary regularizers for each
modality encoder to calculate their prediction variance. ARL then calculates
coefficients via the unimodal variance to re-weight the optimization of each
modality, forcing the modality dependence ratio to be inversely proportional to
the modality variance ratio. Moreover, to minimize the generalization error,
ARL further introduces the prediction bias of each modality and jointly
optimizes them with multimodal loss. Notably, all auxiliary regularizers share
parameters with the multimodal model and rely only on the modality
representation. Thus the proposed ARL strategy introduces no extra parameters
and is independent of the structures and fusion methods of the multimodal
model. Finally, extensive experiments on various datasets validate the
effectiveness and versatility of ARL. Code is available at
\href{https://github.com/shicaiwei123/ICCV2025-ARL}{https://github.com/shicaiwei123/ICCV2025-ARL}

</details>


### [128] [Is Micro-expression Ethnic Leaning?](https://arxiv.org/abs/2507.10209)
*Huai-Qian Khor,Yante Li,Xingxun Jiang,Guoying Zhao*

Main category: cs.CV

TL;DR: 种族背景影响微表情，情感普遍性假设可能过于简化。本研究构建了包含种族信息的数据库，并通过实验证明了种族偏见的存在，提出了一个考虑种族因素的微表情识别框架。


<details>
  <summary>Details</summary>
Motivation: 为了探究种族背景在情感表达（特别是微表情）分析中的作用，并挑战情感普遍性假设。

Method: 通过构建跨文化微表情数据库并进行算法注释，比较单一民族和多民族在受控环境下的表现，并通过实验验证了种族偏见的影响。

Result: 研究发现了种族背景对微表情分析存在一定影响，并提出了一种能够识别种族差异的、具有种族意识的微表情识别框架。

Conclusion: 研究表明，种族背景确实会影响微表情分析，挑战了情感普遍性假设，并提出了一个整合了种族背景的情感特征学习框架，以提高微表情识别的准确性。

Abstract: How much does ethnicity play its part in emotional expression? Emotional
expression and micro-expression research probe into understanding human
psychological responses to emotional stimuli, thereby revealing substantial
hidden yet authentic emotions that can be useful in the event of diagnosis and
interviews. While increased attention had been provided to micro-expression
analysis, the studies were done under Ekman's assumption of emotion
universality, where emotional expressions are identical across cultures and
social contexts. Our computational study uncovers some of the influences of
ethnic background in expression analysis, leading to an argument that the
emotional universality hypothesis is an overgeneralization from the perspective
of manual psychological analysis. In this research, we propose to investigate
the level of influence of ethnicity in a simulated micro-expression scenario.
We construct a cross-cultural micro-expression database and algorithmically
annotate the ethnic labels to facilitate the investigation. With the ethnically
annotated dataset, we perform a prima facie study to compare mono-ethnicity and
stereo-ethnicity in a controlled environment, which uncovers a certain
influence of ethnic bias via an experimental way. Building on this finding, we
propose a framework that integrates ethnic context into the emotional feature
learning process, yielding an ethnically aware framework that recognises
ethnicity differences in micro-expression recognition. For improved
understanding, qualitative analyses have been done to solidify the preliminary
investigation into this new realm of research. Code is publicly available at
https://github.com/IcedDoggie/ICMEW2025_EthnicMER

</details>


### [129] [Boosting Multimodal Learning via Disentangled Gradient Learning](https://arxiv.org/abs/2507.10213)
*Shicai Wei,Chunbo Luo,Yang Luo*

Main category: cs.CV

TL;DR: 多模态学习存在欠优化问题，因模态编码器和融合模块间优化冲突导致性能下降。本文提出DGL框架，解耦二者优化，用单模态梯度替换多模态梯度，消除干扰，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 现有方法将多模态学习中的欠优化问题归因于模态间学习不平衡，并通过梯度调节进行再平衡，但未能解释主导模态在多模态模型中也逊于单模态学习的情况。本研究揭示了多模态模型中模态编码器和模态融合模块之间的优化冲突，并证明了跨模态融合会降低反向传播到每个模态编码器的梯度，导致性能下降。

Method: 提出了一种解耦梯度学习（DGL）框架，通过截断从多模态损失反向传播到模态编码器的梯度，并用来自单模态损失的梯度替换它，同时移除来自单模态损失反向传播到模态融合模块的梯度，以消除梯度干扰并确保各自的优化过程。

Result: 通过在多种模态、任务和具有密集跨模态交互的框架上进行的大量实验，证明了所提出的DGL框架的有效性和通用性。

Conclusion: 本研究提出的解耦梯度学习（DGL）框架通过解耦模态编码器和模态融合模块的优化，有效解决了多模态学习中的欠优化问题，并在多模态数据集上取得了显著的性能提升，证明了其有效性和通用性。

Abstract: Multimodal learning often encounters the under-optimized problem and may have
worse performance than unimodal learning. Existing methods attribute this
problem to the imbalanced learning between modalities and rebalance them
through gradient modulation. However, they fail to explain why the dominant
modality in multimodal models also underperforms that in unimodal learning. In
this work, we reveal the optimization conflict between the modality encoder and
modality fusion module in multimodal models. Specifically, we prove that the
cross-modal fusion in multimodal models decreases the gradient passed back to
each modality encoder compared with unimodal models. Consequently, the
performance of each modality in the multimodal model is inferior to that in the
unimodal model. To this end, we propose a disentangled gradient learning (DGL)
framework to decouple the optimization of the modality encoder and modality
fusion module in the multimodal model. DGL truncates the gradient
back-propagated from the multimodal loss to the modality encoder and replaces
it with the gradient from unimodal loss. Besides, DGL removes the gradient
back-propagated from the unimodal loss to the modality fusion module. This
helps eliminate the gradient interference between the modality encoder and
modality fusion module while ensuring their respective optimization processes.
Finally, extensive experiments on multiple types of modalities, tasks, and
frameworks with dense cross-modal interaction demonstrate the effectiveness and
versatility of the proposed DGL. Code is available at
\href{https://github.com/shicaiwei123/ICCV2025-GDL}{https://github.com/shicaiwei123/ICCV2025-GDL}

</details>


### [130] [From Wardrobe to Canvas: Wardrobe Polyptych LoRA for Part-level Controllable Human Image Generation](https://arxiv.org/abs/2507.10217)
*Jeongho Kim,Sunghyun Park,Hyoungwoo Park,Sungrack Yun,Jaegul Choo,Seokeon Cho*

Main category: cs.CV

TL;DR: 提出了一种名为 Wardrobe Polyptych LoRA 的新模型，用于个性化人物图像生成。该模型通过仅训练 LoRA 层来提高推理效率，并使用空间参考和选择性受试者区域损失来提高保真度和一致性。实验证明，与现有技术相比，该模型在保真度和一致性方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 解决现有个性化人物图像生成方法计算成本高且不适用于实时应用的问题，这些方法需要推理时微调或大规模数据集训练。

Method: 提出了一种名为 Wardrobe Polyptych LoRA 的新颖的、部分可控的个性化人物图像生成模型。该模型通过仅训练 LoRA 层来提高推理效率，并使用空间参考和选择性受试者区域损失来提高保真度和一致性。

Result: 与现有技术相比，Wardrobe Polyptych LoRA 在保真度和一致性方面表现显著优于现有技术，能够实现逼真且保持身份的全身合成。

Conclusion: Wardrobe Polyptych LoRA 通过仅训练 LoRA 层，在推理时消除了计算负担，同时确保了对未见主题的高保真合成。它通过以受试者的衣橱为条件并利用空间参考来减少信息丢失，从而提高保真度和一致性。此外，它引入了一种选择性的受试者区域损失，以确保生成的图像更好地与文本提示对齐，同时保持受试者完整性。

Abstract: Recent diffusion models achieve personalization by learning specific
subjects, allowing learned attributes to be integrated into generated images.
However, personalized human image generation remains challenging due to the
need for precise and consistent attribute preservation (e.g., identity,
clothing details). Existing subject-driven image generation methods often
require either (1) inference-time fine-tuning with few images for each new
subject or (2) large-scale dataset training for generalization. Both approaches
are computationally expensive and impractical for real-time applications. To
address these limitations, we present Wardrobe Polyptych LoRA, a novel
part-level controllable model for personalized human image generation. By
training only LoRA layers, our method removes the computational burden at
inference while ensuring high-fidelity synthesis of unseen subjects. Our key
idea is to condition the generation on the subject's wardrobe and leverage
spatial references to reduce information loss, thereby improving fidelity and
consistency. Additionally, we introduce a selective subject region loss, which
encourages the model to disregard some of reference images during training. Our
loss ensures that generated images better align with text prompts while
maintaining subject integrity. Notably, our Wardrobe Polyptych LoRA requires no
additional parameters at the inference stage and performs generation using a
single model trained on a few training samples. We construct a new dataset and
benchmark tailored for personalized human image generation. Extensive
experiments show that our approach significantly outperforms existing
techniques in fidelity and consistency, enabling realistic and
identity-preserving full-body synthesis.

</details>


### [131] [Straighten Viscous Rectified Flow via Noise Optimization](https://arxiv.org/abs/2507.10218)
*Jimin Dai,Jiexi Yan,Jian Yang,Lei Luo*

Main category: cs.CV

TL;DR: VRFNO是一种新的生成模型，通过改进推理轨迹和优化噪声，在单步和少步图像生成方面优于Reflow。


<details>
  <summary>Details</summary>
Motivation: 为了解决Reflow在快速生成高质量图像方面的局限性，特别是其构造的确定性耦合与真实图像之间存在的分布差距问题。

Method: 提出了一种名为VRFNO的联合训练框架，该框架整合了编码器和神经速度场，并通过历史速度项和重参数化噪声优化来改进推理轨迹。

Result: VRFNO在合成数据和不同分辨率的真实数据集上进行了广泛的实验，结果表明VRFNO显著减轻了Reflow的局限性，并在单步和少步生成任务中均取得了最先进的性能。

Conclusion: VRFNO通过引入历史速度项和通过重参数化进行噪声优化，有效解决了Reflow的局限性，在单步和少步生成任务上实现了最先进的性能。

Abstract: The Reflow operation aims to straighten the inference trajectories of the
rectified flow during training by constructing deterministic couplings between
noises and images, thereby improving the quality of generated images in
single-step or few-step generation. However, we identify critical limitations
in Reflow, particularly its inability to rapidly generate high-quality images
due to a distribution gap between images in its constructed deterministic
couplings and real images. To address these shortcomings, we propose a novel
alternative called Straighten Viscous Rectified Flow via Noise Optimization
(VRFNO), which is a joint training framework integrating an encoder and a
neural velocity field. VRFNO introduces two key innovations: (1) a historical
velocity term that enhances trajectory distinction, enabling the model to more
accurately predict the velocity of the current trajectory, and (2) the noise
optimization through reparameterization to form optimized couplings with real
images which are then utilized for training, effectively mitigating errors
caused by Reflow's limitations. Comprehensive experiments on synthetic data and
real datasets with varying resolutions show that VRFNO significantly mitigates
the limitations of Reflow, achieving state-of-the-art performance in both
one-step and few-step generation tasks.

</details>


### [132] [Spatial Lifting for Dense Prediction](https://arxiv.org/abs/2507.10222)
*Mingzhi Xu,Yizhe Zhang*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We present Spatial Lifting (SL), a novel methodology for dense prediction
tasks. SL operates by lifting standard inputs, such as 2D images, into a
higher-dimensional space and subsequently processing them using networks
designed for that higher dimension, such as a 3D U-Net. Counterintuitively,
this dimensionality lifting allows us to achieve good performance on benchmark
tasks compared to conventional approaches, while reducing inference costs and
significantly lowering the number of model parameters. The SL framework
produces intrinsically structured outputs along the lifted dimension. This
emergent structure facilitates dense supervision during training and enables
robust, near-zero-additional-cost prediction quality assessment at test time.
We validate our approach across 19 benchmark datasets (13 for semantic
segmentation and 6 for depth estimation), demonstrating competitive dense
prediction performance while reducing the model parameter count by over 98% (in
the U-Net case) and lowering inference costs. Spatial Lifting introduces a new
vision modeling paradigm that offers a promising path toward more efficient,
accurate, and reliable deep networks for dense prediction tasks in vision.

</details>


### [133] [ProGait: A Multi-Purpose Video Dataset and Benchmark for Transfemoral Prosthesis Users](https://arxiv.org/abs/2507.10223)
*Xiangyu Yin,Boyuan Yang,Weichen Liu,Qiyao Xue,Abrar Alamri,Goeran Fiedler,Wei Gao*

Main category: cs.CV

TL;DR: 本研究提出了 ProGait 数据集，用于改进假肢步态分析的视觉机器学习方法，并通过实验证明了其优越的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 为了优化假肢设计和对齐，提高截肢患者的行动能力和生活质量，需要对步态进行分析。现有的基于视觉的机器学习方法在处理假肢独特外观和运动模式方面存在挑战，因此需要一个专门的数据集来解决这些问题。

Method: 使用基于视觉的机器学习方法，并引入了一个名为 ProGait 的多功能数据集，其中包含412个视频片段，涵盖了视频对象分割、2D人体姿态估计和步态分析。通过比较微调后的基线模型和预训练的视觉模型，展示了在假肢特定任务上的性能。

Result: 基线模型在应用 ProGait 数据集进行假肢特定任务时，相比于预训练的视觉模型，展现出更强的泛化能力。

Conclusion: 该研究提出了一个名为 ProGait 的多功能数据集，用于支持视频对象分割、2D 人体姿态估计和步态分析（GA）等多种视觉任务，以解决现有方法在检测和分析假肢方面的挑战。通过在四名膝盖以上截肢者行走试验中收集视频片段，并提供基准任务和经过微调的基线模型，证明了 ProGait 数据集在假肢特定任务上的改进泛化能力。

Abstract: Prosthetic legs play a pivotal role in clinical rehabilitation, allowing
individuals with lower-limb amputations the ability to regain mobility and
improve their quality of life. Gait analysis is fundamental for optimizing
prosthesis design and alignment, directly impacting the mobility and life
quality of individuals with lower-limb amputations. Vision-based machine
learning (ML) methods offer a scalable and non-invasive solution to gait
analysis, but face challenges in correctly detecting and analyzing prosthesis,
due to their unique appearances and new movement patterns. In this paper, we
aim to bridge this gap by introducing a multi-purpose dataset, namely ProGait,
to support multiple vision tasks including Video Object Segmentation, 2D Human
Pose Estimation, and Gait Analysis (GA). ProGait provides 412 video clips from
four above-knee amputees when testing multiple newly-fitted prosthetic legs
through walking trials, and depicts the presence, contours, poses, and gait
patterns of human subjects with transfemoral prosthetic legs. Alongside the
dataset itself, we also present benchmark tasks and fine-tuned baseline models
to illustrate the practical application and performance of the ProGait dataset.
We compared our baseline models against pre-trained vision models,
demonstrating improved generalizability when applying the ProGait dataset for
prosthesis-specific tasks. Our code is available at
https://github.com/pittisl/ProGait and dataset at
https://huggingface.co/datasets/ericyxy98/ProGait.

</details>


### [134] [Synthesizing Near-Boundary OOD Samples for Out-of-Distribution Detection](https://arxiv.org/abs/2507.10225)
*Jinglun Li,Kaixun Jiang,Zhaoyu Chen,Bo Lin,Yao Tang,Weifeng Ge,Wenqiang Zhang*

Main category: cs.CV

TL;DR: SynOOD 利用基础模型生成具有挑战性的 OOD 样本，以增强 CLIP 模型区分 InD 和 OOD 样本的能力，尤其是在边界附近，并在 ImageNet 上取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的预训练视觉-语言模型在检测 OOD 样本方面能力显著，但仍难以区分接近 InD 数据的 OOD 样本；利用扩散模型和 MLLMs 等基础模型为解决此问题提供了潜在方案。

Method: SynOOD 方法利用 MLLMs 的上下文提示和能量分数等 OOD 分数的梯度，通过迭代修复过程生成在边界对齐的 OOD 样本，并对 CLIP 的图像编码器和负面标签特征进行微调。

Result: SynOOD 在 ImageNet 基准测试中实现了最先进的性能，AUROC 提高了 2.80%，FPR95 降低了 11.13%。

Conclusion: SynOOD 通过生成具有挑战性的 OOD 样本来微调 CLIP 模型，从而提高边界判别能力，并在 ImageNet 上取得了最先进的性能，同时参数和运行时间增加极少。

Abstract: Pre-trained vision-language models have exhibited remarkable abilities in
detecting out-of-distribution (OOD) samples. However, some challenging OOD
samples, which lie close to in-distribution (InD) data in image feature space,
can still lead to misclassification. The emergence of foundation models like
diffusion models and multimodal large language models (MLLMs) offers a
potential solution to this issue. In this work, we propose SynOOD, a novel
approach that harnesses foundation models to generate synthetic, challenging
OOD data for fine-tuning CLIP models, thereby enhancing boundary-level
discrimination between InD and OOD samples. Our method uses an iterative
in-painting process guided by contextual prompts from MLLMs to produce nuanced,
boundary-aligned OOD samples. These samples are refined through noise
adjustments based on gradients from OOD scores like the energy score,
effectively sampling from the InD/OOD boundary. With these carefully
synthesized images, we fine-tune the CLIP image encoder and negative label
features derived from the text encoder to strengthen connections between
near-boundary OOD samples and a set of negative labels. Finally, SynOOD
achieves state-of-the-art performance on the large-scale ImageNet benchmark,
with minimal increases in parameters and runtime. Our approach significantly
surpasses existing methods, improving AUROC by 2.80% and reducing FPR95 by
11.13%. Codes are available in https://github.com/Jarvisgivemeasuit/SynOOD.

</details>


### [135] [Navigating the Challenges of AI-Generated Image Detection in the Wild: What Truly Matters?](https://arxiv.org/abs/2507.10236)
*Despina Konstantinidou,Dimitrios Karageorgiou,Christos Koutlis,Olga Papadopoulou,Emmanouil Schinas,Symeon Papadopoulos*

Main category: cs.CV

TL;DR: 本研究提出ITW-SM数据集，并通过分析关键因素（骨干网络、训练数据、预处理、数据增强）显著提升了在真实世界场景下AI生成图像检测模型的性能。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI技术的飞速发展，AI生成的图像质量日益提高，足以以假乱真，这不仅带来了创作的新机遇，也对社会信任和数字信息完整性构成了严峻挑战。因此，提高AI生成图像检测（AID）能力变得至关重要，以应对这些潜在的风险。

Method: 本研究首先系统性地评估了当前人工智能生成图像检测（AID）模型在真实世界数据上的表现，识别出其在真实世界变化面前的性能短板。然后，研究者们提出了一个新的名为ITW-SM的数据集，该数据集包含了来自主流社交媒体平台的真实和AI生成图像。通过对骨干网络架构、训练数据构成、预处理策略以及数据增强组合这四个关键因素进行系统性分析和实验调整，以优化模型性能。

Result: 研究发现，骨干网络架构、训练数据构成、预处理策略和数据增强组合是影响真实世界场景中AID性能的四个关键因素。通过对这些因素进行优化和调整，研究者们在真实世界条件下，针对不同的AID模型，平均AUC（Area Under the Curve）取得了26.87%的提升。

Conclusion: 本研究通过识别影响真实世界场景中人工智能生成图像检测（AID）性能的关键因素，并进行相应的模型优化，显著提高了在真实世界条件下AID模型的平均AUC约26.87%，为解决AI生成内容带来的信任和真实性挑战提供了新的解决方案。

Abstract: The rapid advancement of generative technologies presents both unprecedented
creative opportunities and significant challenges, particularly in maintaining
social trust and ensuring the integrity of digital information. Following these
concerns, the challenge of AI-Generated Image Detection (AID) becomes
increasingly critical. As these technologies become more sophisticated, the
quality of AI-generated images has reached a level that can easily deceive even
the most discerning observers. Our systematic evaluation highlights a critical
weakness in current AI-Generated Image Detection models: while they perform
exceptionally well on controlled benchmark datasets, they struggle
significantly with real-world variations. To assess this, we introduce ITW-SM,
a new dataset of real and AI-generated images collected from major social media
platforms. In this paper, we identify four key factors that influence AID
performance in real-world scenarios: backbone architecture, training data
composition, pre-processing strategies and data augmentation combinations. By
systematically analyzing these components, we shed light on their impact on
detection efficacy. Our modifications result in an average AUC improvement of
26.87% across various AID models under real-world conditions.

</details>


### [136] [Transferring Styles for Reduced Texture Bias and Improved Robustness in Semantic Segmentation Networks](https://arxiv.org/abs/2507.10239)
*Ben Hamscher,Edgar Heinert,Annika Mütze,Kira Maag,Matthias Rottmann*

Main category: cs.CV

TL;DR: 风格迁移增强可用于语义分割，以减少纹理偏差并提高对图像损坏和对抗性攻击的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 为了研究风格迁移在语义分割任务中是否也能像在图像分类任务中一样，减少纹理偏差并提高鲁棒性。

Method: 通过在图像区域应用风格迁移（使用 Voronoi 单元划分的随机区域）来生成风格迁移数据，并使用这些数据训练深度神经网络（包括卷积神经网络和 Transformer 架构），以减少对纹理线索的依赖并增强对基于形状的特征的依赖。

Result: 风格迁移增强能够有效减少语义分割的纹理偏差，并显著提高模型在 Cityscapes 和 PASCAL Context 数据集上的鲁棒性，包括对常见图像损坏和对抗性攻击的抵抗能力。

Conclusion: 研究表明，在语义分割任务中，风格迁移增强可以减少纹理偏差并显著提高对常见图像损坏和对抗性攻击的鲁棒性。

Abstract: Recent research has investigated the shape and texture biases of deep neural
networks (DNNs) in image classification which influence their generalization
capabilities and robustness. It has been shown that, in comparison to regular
DNN training, training with stylized images reduces texture biases in image
classification and improves robustness with respect to image corruptions. In an
effort to advance this line of research, we examine whether style transfer can
likewise deliver these two effects in semantic segmentation. To this end, we
perform style transfer with style varying across artificial image areas. Those
random areas are formed by a chosen number of Voronoi cells. The resulting
style-transferred data is then used to train semantic segmentation DNNs with
the objective of reducing their dependence on texture cues while enhancing
their reliance on shape-based features. In our experiments, it turns out that
in semantic segmentation, style transfer augmentation reduces texture bias and
strongly increases robustness with respect to common image corruptions as well
as adversarial attacks. These observations hold for convolutional neural
networks and transformer architectures on the Cityscapes dataset as well as on
PASCAL Context, showing the generality of the proposed method.

</details>


### [137] [Kaleidoscopic Background Attack: Disrupting Pose Estimation with Multi-Fold Radial Symmetry Textures](https://arxiv.org/abs/2507.10265)
*Xinlong Ding,Hongwei Yu,Jiawei Li,Feifan Li,Yu Shang,Bochao Zou,Huimin Ma,Jiansheng Chen*

Main category: cs.CV

TL;DR: 在以物体为中心的场景中，背景纹理会影响相机姿态估计。本文提出了一种名为万花筒背景攻击（KBA）的方法，该方法利用具有径向对称性的圆盘来攻击姿态估计模型。通过优化片段以保持方向一致性，该方法能显著提高攻击效果，并能有效攻击多种相机姿态估计模型。


<details>
  <summary>Details</summary>
Motivation: 在以物体为中心的稀疏输入场景中，背景纹理会影响相机姿态估计的准确性，因此需要一种有效的方法来处理这种影响。

Method: 提出了一种名为万花筒背景攻击（KBA）的方法，该方法使用相同的片段形成具有多重径向对称性的圆盘。还提出了一种投影方向一致性损失来优化万花筒状片段，以增强攻击效果。

Result: 实验结果表明，优化的万花筒状对抗性背景可以有效地攻击各种相机姿态估计模型。

Conclusion: 所提出的优化后的万花筒状对抗性背景能够有效地攻击各种相机姿态估计模型。

Abstract: Camera pose estimation is a fundamental computer vision task that is
essential for applications like visual localization and multi-view stereo
reconstruction. In the object-centric scenarios with sparse inputs, the
accuracy of pose estimation can be significantly influenced by background
textures that occupy major portions of the images across different viewpoints.
In light of this, we introduce the Kaleidoscopic Background Attack (KBA), which
uses identical segments to form discs with multi-fold radial symmetry. These
discs maintain high similarity across different viewpoints, enabling effective
attacks on pose estimation models even with natural texture segments.
Additionally, a projected orientation consistency loss is proposed to optimize
the kaleidoscopic segments, leading to significant enhancement in the attack
effectiveness. Experimental results show that optimized adversarial
kaleidoscopic backgrounds can effectively attack various camera pose estimation
models.

</details>


### [138] [FTCFormer: Fuzzy Token Clustering Transformer for Image Classification](https://arxiv.org/abs/2507.10283)
*Muyi Bao,Changyu Zeng,Yifan Wang,Zhengni Yang,Zimu Wang,Guangliang Cheng,Jun Qi,Wei Wang*

Main category: cs.CV

TL;DR: FTCFormer通过基于语义的聚类方法动态生成视觉标记，克服了传统基于网格标记的局限性，在多项图像分类任务中取得了优于基线模型的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer模型在处理图像时，将图像嵌入到统一的、基于网格的视觉标记中，忽略了图像区域潜在的语义信息，导致特征表示次优。FTCFormer旨在通过一种基于语义而非空间位置的方法来生成视觉标记，为信息丰富的区域分配更多标记，为信息量少的区域分配较少标记，从而提高特征表示能力。

Method: 提出了一种名为FTCFormer的新型Transformer架构，其核心在于引入了一个新颖的基于聚类的下采样模块，能够根据语义信息动态生成视觉标记。该模块通过密度峰值聚类-模糊K近邻（DPC-FKNN）机制确定聚类中心，利用空间连通性得分（SCS）进行标记分配，并通过通道式合并（Cmerge）策略进行标记合并。

Result: 在包括32个不同领域的数据集进行的广泛实验证明了FTCFormer的有效性。在图像分类任务上，FTCFormer相比于TCFormer基线模型在五个细粒度数据集上提高了1.43%，在六个自然图像数据集上提高了1.09%，在三个医学数据集上提高了0.97%，在四个遥感数据集上提高了0.55%。

Conclusion: FTCFormer通过动态生成基于语义的视觉标记，并采用DPC-FKNN、SCS和Cmerge策略，在图像分类任务中展现出超越基线模型的性能，并在多个细粒度、自然图像、医学和遥感数据集上实现了显著的准确率提升。

Abstract: Transformer-based deep neural networks have achieved remarkable success
across various computer vision tasks, largely attributed to their long-range
self-attention mechanism and scalability. However, most transformer
architectures embed images into uniform, grid-based vision tokens, neglecting
the underlying semantic meanings of image regions, resulting in suboptimal
feature representations. To address this issue, we propose Fuzzy Token
Clustering Transformer (FTCFormer), which incorporates a novel clustering-based
downsampling module to dynamically generate vision tokens based on the semantic
meanings instead of spatial positions. It allocates fewer tokens to less
informative regions and more to represent semantically important regions,
regardless of their spatial adjacency or shape irregularity. To further enhance
feature extraction and representation, we propose a Density Peak
Clustering-Fuzzy K-Nearest Neighbor (DPC-FKNN) mechanism for clustering center
determination, a Spatial Connectivity Score (SCS) for token assignment, and a
channel-wise merging (Cmerge) strategy for token merging. Extensive experiments
on 32 datasets across diverse domains validate the effectiveness of FTCFormer
on image classification, showing consistent improvements over the TCFormer
baseline, achieving gains of improving 1.43% on five fine-grained datasets,
1.09% on six natural image datasets, 0.97% on three medical datasets and 0.55%
on four remote sensing datasets. The code is available at:
https://github.com/BaoBao0926/FTCFormer/tree/main.

</details>


### [139] [Show and Polish: Reference-Guided Identity Preservation in Face Video Restoration](https://arxiv.org/abs/2507.10293)
*Wenkang Han,Wang Lin,Yiyun Zhou,Qi Liu,Shulei Wang,Chang Yao,Jingyuan Chen*

Main category: cs.CV

TL;DR: IP-FVR是一种新的人脸视频恢复方法，它使用参考人脸图像作为身份提示，并通过特殊的学习和混合策略来保持身份一致性，解决了传统方法的不足。


<details>
  <summary>Details</summary>
Motivation: 传统的人脸视频恢复方法在严重降级的情况下难以保留细粒度的、特定于身份的特征，常常产生缺乏个体特征的平均脸。为了解决这些挑战，需要一种能够提供身份条件并保持身份一致性的新方法。

Method: IP-FVR是一种新颖的面部视频恢复方法，它利用高质量参考人脸图像作为视觉提示，在去噪过程中提供身份条件。该方法结合了解耦交叉注意力机制来整合来自参考图像的语义丰富的身份信息，并引入了基于余弦相似度奖励信号和后缀加权时间聚合的身份保持反馈学习方法来处理单帧内的身份漂移，以及一种指数混合策略来处理跨帧的身份漂移。此外，还采用多流负面提示来增强恢复过程。

Result: IP-FVR能够生成详细且身份一致的面部视频，有效解决了单帧内和跨帧的身份漂移问题，并在质量和身份保持方面优于现有方法。

Conclusion: IP-FVR在合成和真实世界数据集上的广泛实验表明，其在质量和身份保持方面均优于现有方法，展示了其在面部视频恢复中的巨大潜力。

Abstract: Face Video Restoration (FVR) aims to recover high-quality face videos from
degraded versions. Traditional methods struggle to preserve fine-grained,
identity-specific features when degradation is severe, often producing
average-looking faces that lack individual characteristics. To address these
challenges, we introduce IP-FVR, a novel method that leverages a high-quality
reference face image as a visual prompt to provide identity conditioning during
the denoising process. IP-FVR incorporates semantically rich identity
information from the reference image using decoupled cross-attention
mechanisms, ensuring detailed and identity consistent results. For intra-clip
identity drift (within 24 frames), we introduce an identity-preserving feedback
learning method that combines cosine similarity-based reward signals with
suffix-weighted temporal aggregation. This approach effectively minimizes drift
within sequences of frames. For inter-clip identity drift, we develop an
exponential blending strategy that aligns identities across clips by
iteratively blending frames from previous clips during the denoising process.
This method ensures consistent identity representation across different clips.
Additionally, we enhance the restoration process with a multi-stream negative
prompt, guiding the model's attention to relevant facial attributes and
minimizing the generation of low-quality or incorrect features. Extensive
experiments on both synthetic and real-world datasets demonstrate that IP-FVR
outperforms existing methods in both quality and identity preservation,
showcasing its substantial potential for practical applications in face video
restoration.

</details>


### [140] [DisCo: Towards Distinct and Coherent Visual Encapsulation in Video MLLMs](https://arxiv.org/abs/2507.10302)
*Jiahe Zhao,Rongkun Zheng,Yi Wang,Helin Wang,Hengshuang Zhao*

Main category: cs.CV

TL;DR: DisCo是一种新的视觉封装方法，通过VCD和TFC模块解决视频MLLM中的语义模糊和时间不连贯问题，在多个基准测试中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视频MLLM中的线性投影仪在应用于视频时存在语义模糊和时间不连贯的问题，而借鉴重采样器结构的方法虽然有潜力解决这些挑战，但尚未有成熟的方案。因此，需要一种新的视觉封装方法来解决这些问题。

Method: DisCo是一种新颖的视觉封装方法，通过集成视觉概念判别器（VCD）和时间焦点校准器（TFC）两个关键组件来实现：VCD为视觉代币分配独特的语义，TFC确保视觉代币与视频元素在每一帧中的时间焦点一致性。

Result: DisCo在多种视频MLLM框架和视频理解基准测试中均表现出色，优于现有方法，并提高了代币效率。

Conclusion: DisCo通过实验证明，在多种视频MLLM框架下，其在各项视频理解基准测试中的表现显著优于先前最先进的方法，并且由于降低了语义模糊性，实现了更高的代币效率。

Abstract: In video Multimodal Large Language Models (video MLLMs), the visual
encapsulation process plays a pivotal role in converting video contents into
representative tokens for LLM input. While linear projectors are widely
employed for encapsulation, they introduce semantic indistinctness and temporal
incoherence when applied to videos. Conversely, the structure of resamplers
shows promise in tackling these challenges, but an effective solution remains
unexplored. Drawing inspiration from resampler structures, we introduce DisCo,
a novel visual encapsulation method designed to yield semantically distinct and
temporally coherent visual tokens for video MLLMs. DisCo integrates two key
components: (1) A Visual Concept Discriminator (VCD) module, assigning unique
semantics for visual tokens by associating them in pair with discriminative
concepts in the video. (2) A Temporal Focus Calibrator (TFC) module, ensuring
consistent temporal focus of visual tokens to video elements across every video
frame. Through extensive experiments on multiple video MLLM frameworks, we
demonstrate that DisCo remarkably outperforms previous state-of-the-art methods
across a variety of video understanding benchmarks, while also achieving higher
token efficiency thanks to the reduction of semantic indistinctness. The code:
https://github.com/ZJHTerry18/DisCo.

</details>


### [141] [Contrastive Pretraining with Dual Visual Encoders for Gloss-Free Sign Language Translation](https://arxiv.org/abs/2507.10306)
*Ozge Mercanoglu Sincan,Richard Bowden*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Sign Language Translation (SLT) aims to convert sign language videos into
spoken or written text. While early systems relied on gloss annotations as an
intermediate supervision, such annotations are costly to obtain and often fail
to capture the full complexity of continuous signing. In this work, we propose
a two-phase, dual visual encoder framework for gloss-free SLT, leveraging
contrastive visual-language pretraining. During pretraining, our approach
employs two complementary visual backbones whose outputs are jointly aligned
with each other and with sentence-level text embeddings via a contrastive
objective. During the downstream SLT task, we fuse the visual features and
input them into an encoder-decoder model. On the Phoenix-2014T benchmark, our
dual encoder architecture consistently outperforms its single stream variants
and achieves the highest BLEU-4 score among existing gloss-free SLT approaches.

</details>


### [142] [Mind the Gap: Aligning Vision Foundation Models to Image Feature Matching](https://arxiv.org/abs/2507.10318)
*Yuhan Liu,Jingwen Fu,Yang Wu,Kangyi Wu,Pengna Li,Jiayi Wu,Sanping Zhou,Jingmin Xin*

Main category: cs.CV

TL;DR: 本研究提出了IMD框架，利用扩散模型和跨图像交互提示来解决视觉基础模型在特征匹配中的错位问题，尤其擅长多实例匹配，并在IMIM基准上取得了显著改进。


<details>
  <summary>Details</summary>
Motivation: 现有研究在将视觉基础模型应用于特征匹配时，忽略了模型在单图像理解和跨图像理解需求之间存在的错位问题。具体表现为：1. 基础模型提取的嵌入与特征匹配所需的最佳嵌入存在差异；2. 缺乏有效机制将单图像理解能力迁移到跨图像理解。这种错位导致现有方法在处理多实例特征匹配问题时效果不佳。

Method: 本研究提出了一个名为IMD（Image feature Matching with a pre-trained Diffusion model）的框架。该框架包含两个关键部分：1. 集成基于生成式的扩散模型，以捕捉实例级细节，区别于侧重全局语义的对比学习模型；2. 利用生成模型的提示机制，设计了一个跨图像交互提示模块，以促进图像对之间的双向信息交互。此外，还提出了一个名为IMIM的新基准，用于评估多实例场景下的特征匹配性能。

Result: 研究提出的IMD框架在现有常用基准测试中达到了新的最先进水平，并在多实例特征匹配的新基准IMIM上取得了12%的性能提升，证明了该方法能有效缓解模型错位问题。

Conclusion: 该研究提出的IMD框架通过集成生成式扩散模型和创新的跨图像交互提示模块，有效解决了视觉基础模型在特征匹配中存在的错位问题，特别是在多实例匹配场景下表现出色，并在新基准IMIM上取得了12%的性能提升，确立了新的最先进水平。

Abstract: Leveraging the vision foundation models has emerged as a mainstream paradigm
that improves the performance of image feature matching. However, previous
works have ignored the misalignment when introducing the foundation models into
feature matching. The misalignment arises from the discrepancy between the
foundation models focusing on single-image understanding and the cross-image
understanding requirement of feature matching. Specifically, 1) the embeddings
derived from commonly used foundation models exhibit discrepancies with the
optimal embeddings required for feature matching; 2) lacking an effective
mechanism to leverage the single-image understanding ability into cross-image
understanding. A significant consequence of the misalignment is they struggle
when addressing multi-instance feature matching problems. To address this, we
introduce a simple but effective framework, called IMD (Image feature Matching
with a pre-trained Diffusion model) with two parts: 1) Unlike the dominant
solutions employing contrastive-learning based foundation models that emphasize
global semantics, we integrate the generative-based diffusion models to
effectively capture instance-level details. 2) We leverage the prompt mechanism
in generative model as a natural tunnel, propose a novel cross-image
interaction prompting module to facilitate bidirectional information
interaction between image pairs. To more accurately measure the misalignment,
we propose a new benchmark called IMIM, which focuses on multi-instance
scenarios. Our proposed IMD establishes a new state-of-the-art in commonly
evaluated benchmarks, and the superior improvement 12% in IMIM indicates our
method efficiently mitigates the misalignment.

</details>


### [143] [Text Embedding Knows How to Quantize Text-Guided Diffusion Models](https://arxiv.org/abs/2507.10340)
*Hongjae Lee,Myungjun Son,Dongjea Kang,Seung-Won Jung*

Main category: cs.CV

TL;DR: QLIP 是一种新颖的量化方法，它利用文本提示来指导语言到图像扩散模型的量化过程，从而提高效率和生成图像的质量。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散模型量化方法未将文本提示等输入条件视为量化的重要信息来源，而扩散模型巨大的计算复杂性限制了其在资源受限环境下的使用。

Method: QLIP 利用文本提示来指导每个时间步长中每个层的比特精度选择。

Result: 实验证明了 QLIP 在降低计算复杂性和提高生成图像质量方面的有效性。

Conclusion: QLIP 能够有效降低计算复杂性并提高生成图像的质量，并且可以无缝集成到现有的量化方法中以提高量化效率。

Abstract: Despite the success of diffusion models in image generation tasks such as
text-to-image, the enormous computational complexity of diffusion models limits
their use in resource-constrained environments. To address this, network
quantization has emerged as a promising solution for designing efficient
diffusion models. However, existing diffusion model quantization methods do not
consider input conditions, such as text prompts, as an essential source of
information for quantization. In this paper, we propose a novel quantization
method dubbed Quantization of Language-to-Image diffusion models using text
Prompts (QLIP). QLIP leverages text prompts to guide the selection of bit
precision for every layer at each time step. In addition, QLIP can be
seamlessly integrated into existing quantization methods to enhance
quantization efficiency. Our extensive experiments demonstrate the
effectiveness of QLIP in reducing computational complexity and improving the
quality of the generated images across various datasets.

</details>


### [144] [FGSSNet: Feature-Guided Semantic Segmentation of Real World Floorplans](https://arxiv.org/abs/2507.10343)
*Hugo Norrby,Gabriel Färm,Kevin Hernandez-Diaz,Fernando Alonso-Fernandez*

Main category: cs.CV

TL;DR: FGSSNet 通过引入多头特征提取器来改进 U-Net 在建筑平面图墙体分割中的泛化能力，并在实验中取得了更好的性能。


<details>
  <summary>Details</summary>
Motivation: 为了提高建筑平面图墙体分割的泛化能力，我们提出了一种新颖的多头特征引导语义分割（FGSS）架构。

Method: FGSSNet 架构采用 U-Net 作为分割主干，并结合了一个多头专用特征提取器。该特征提取器作为编码器-解码器进行训练，以选取的墙体图块为输入，生成压缩的潜在表示，同时预测墙体宽度。这些提取的特征被注入到 U-Net 的潜在空间中，以指导分割过程。

Result: 实验结果表明，与标准的 U-Net 相比，引入 FGSSNet 提取的特征能够显著提升分割性能，验证了该方法的有效性。

Conclusion: 本文提出的 FGSSNet 在处理建筑平面图的墙体分割任务时，通过引入领域特定的特征提取器来增强泛化能力，实验证明其性能优于标准的 U-Net 网络。

Abstract: We introduce FGSSNet, a novel multi-headed feature-guided semantic
segmentation (FGSS) architecture designed to improve the generalization ability
of wall segmentation on floorplans. FGSSNet features a U-Net segmentation
backbone with a multi-headed dedicated feature extractor used to extract
domain-specific feature maps which are injected into the latent space of U-Net
to guide the segmentation process. This dedicated feature extractor is trained
as an encoder-decoder with selected wall patches, representative of the walls
present in the input floorplan, to produce a compressed latent representation
of wall patches while jointly trained to predict the wall width. In doing so,
we expect that the feature extractor encodes texture and width features of wall
patches that are useful to guide the wall segmentation process. Our experiments
show increased performance by the use of such injected features in comparison
to the vanilla U-Net, highlighting the validity of the proposed approach.

</details>


### [145] [Beyond Graph Model: Reliable VLM Fine-Tuning via Random Graph Adapter](https://arxiv.org/abs/2507.10355)
*Bo Jiang,Xueyang Ze,Beibei Wang,Xixi Wang,Xixi Wan,Bin Luo*

Main category: cs.CV

TL;DR: VRGAdapter 通过引入随机图模型来改进 VLM 的文本适配器，以更好地处理类别描述的多样性和类别间的关系。UMF 方案则通过集成多个预训练模型来增强模型的鲁棒性。实验证明了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于文本适配器的调优方法虽然在将预训练的视觉语言模型（VLMs）知识迁移到下游任务方面显示出巨大的潜力，但由于类别描述的固有差异和类别间关系的处理不当，导致无法充分捕获这种多样的语义信息，因此提出 VRGAdapter 来解决这些问题。

Method: 提出了一种名为 VRGAdapter 的新方法，该方法将随机图模型集成到 VLM 适配器中。VRGAdapter 首先通过利用 Vertex Random Knowledge Graph (VRKG) 模型来对每个类别的固有描述以及类别间的关系进行建模。然后，它在 VRKG 上利用概率消息传播来学习每个类别节点的上下文感知分布表示。最后，它采用重新参数化的采样函数来实现文本适配器的学习。此外，还引入了一种名为不确定性引导多分支融合（UMF）的新方案，该方案动态地集成多个预训练模型来进行集成预测。

Result: 通过在多个基准数据集上进行的大量实验证明了该方法的有效性。

Conclusion: VRGAdapter 通过利用 VRKG 模型同时对每个类别的固有描述和类别间关系进行建模，并利用概率消息传播学习每个类节点，然后采用重新参数化的采样函数来实现文本适配器学习，为 VLM 适配器提供了一个更通用的解决方案。UMF 方案通过动态集成多个预训练模型来进行集成预测，以实现更鲁棒的下游任务性能。

Abstract: Textual adapter-based tuning methods have shown significant potential in
transferring knowledge from pre-trained Vision-Language Models (VLMs) to
downstream tasks. Existing works generally employ the deterministic textual
feature adapter to refine each category textual representation. However, due to
inherent factors such as different attributes and contexts, there exists
significant diversity in textual descriptions for each category. Such
description diversity offers rich discriminative semantic knowledge that can
benefit downstream visual learning tasks. Obviously, traditional deterministic
adapter model cannot adequately capture this varied semantic information. Also,
it is desirable to exploit the inter-class relationships in VLM adapter. To
address these issues, we propose to exploit random graph model into VLM adapter
and develop a novel Vertex Random Graph Adapter (VRGAdapter). VRGAdapter first
models the inherent diverse descriptions of each category and inter-class
relationships of different categories simultaneously by leveraging a Vertex
Random Knowledge Graph (VRKG) model. Then, it employs probabilistic message
propagation on VRKG to learn context-aware distribution representation for each
class node. Finally, it adopts a reparameterized sampling function to achieve
textual adapter learning. Note that, VRGAdapter provides a more general adapter
solution that encompasses traditional graph-based adapter as a special case. In
addition, to enable more robust performance for downstream tasks, we also
introduce a new Uncertainty-guided Multi-branch Fusion (UMF) scheme that
dynamically integrates multiple pre-trained models for ensemble prediction.
Extensive experiments on multiple benchmark datasets demonstrate the
effectiveness of our approach.

</details>


### [146] [Fine-Grained Zero-Shot Object Detection](https://arxiv.org/abs/2507.10358)
*Hongxu Ma,Chenbo Zhang,Lu Zhang,Jiaogen Zhou,Jihong Guan,Shuigeng Zhou*

Main category: cs.CV

TL;DR: 提出细粒度零样本检测（FG-ZSD）问题，并开发了MSHC方法，在首个FG-ZSD基准数据集FGZSD-Birds上取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的零样本检测（ZSD）方法主要处理视觉上差异较大的类别，但在实际应用中，需要处理细粒度的、视觉上相似的类别（如鸟类、鱼类、花卉等）。因此，提出细粒度零样本检测（FG-ZSD）问题，旨在检测具有细微差别的不同类别物体。

Method: 提出了一种基于改进的两阶段检测器和多层次语义感知嵌入对齐损失的MSHC方法，以解决FG-ZSD问题。

Result: 在FGZSD-Birds数据集上进行了广泛的实验，证明了MSHC方法优于现有的ZSD模型。

Conclusion: 提出的FG-ZSD方法在FGZSD-Birds数据集上表现优于现有的ZSD模型。

Abstract: Zero-shot object detection (ZSD) aims to leverage semantic descriptions to
localize and recognize objects of both seen and unseen classes. Existing ZSD
works are mainly coarse-grained object detection, where the classes are
visually quite different, thus are relatively easy to distinguish. However, in
real life we often have to face fine-grained object detection scenarios, where
the classes are too similar to be easily distinguished. For example, detecting
different kinds of birds, fishes, and flowers.
  In this paper, we propose and solve a new problem called Fine-Grained
Zero-Shot Object Detection (FG-ZSD for short), which aims to detect objects of
different classes with minute differences in details under the ZSD paradigm. We
develop an effective method called MSHC for the FG-ZSD task, which is based on
an improved two-stage detector and employs a multi-level semantics-aware
embedding alignment loss, ensuring tight coupling between the visual and
semantic spaces. Considering that existing ZSD datasets are not suitable for
the new FG-ZSD task, we build the first FG-ZSD benchmark dataset FGZSD-Birds,
which contains 148,820 images falling into 36 orders, 140 families, 579 genera
and 1432 species. Extensive experiments on FGZSD-Birds show that our method
outperforms existing ZSD models.

</details>


### [147] [Test-Time Canonicalization by Foundation Models for Robust Perception](https://arxiv.org/abs/2507.10375)
*Utkarsh Singhal,Ryan Feng,Stella X. Yu,Atul Prakash*

Main category: cs.CV

TL;DR: FOCAL是一种在测试时增强模型鲁棒性的框架，无需重新训练，利用基础模型和网络规模的视觉先验来优化图像视图。


<details>
  <summary>Details</summary>
Motivation: 现实世界的视觉感知需要对各种变换具有不变性，但现有方法依赖于专门的架构或预定义的增强训练，限制了泛化能力。

Method: FOCAL框架在测试时利用互联网规模的视觉先验和基础模型，通过生成和优化候选变换来实现‘典型’视图，从而增强鲁棒性。

Result: 实验证明FOCAL能够提高CLIP和SAM在2D/3D旋转、光照变化（对比度和颜色）以及昼夜变化等挑战性变换下的鲁棒性。

Conclusion: FOCAL通过利用大规模视觉先验，在测试时实现鲁棒性，无需重新训练或修改架构，挑战了特定变换训练的必要性。

Abstract: Real-world visual perception requires invariance to diverse transformations,
yet current methods rely heavily on specialized architectures or training on
predefined augmentations, limiting generalization. We propose FOCAL, a
test-time, data-driven framework that achieves robust perception by leveraging
internet-scale visual priors from foundation models. By generating and
optimizing candidate transformations toward visually typical, "canonical"
views, FOCAL enhances robustness without re-training or architectural changes.
Our experiments demonstrate improved robustness of CLIP and SAM across
challenging transformations, including 2D/3D rotations, illumination shifts
(contrast and color), and day-night variations. We also highlight potential
applications in active vision. Our approach challenges the assumption that
transform-specific training is necessary, instead offering a scalable path to
invariance. Our code is available at: https://github.com/sutkarsh/focal.

</details>


### [148] [Improving Remote Sensing Classification using Topological Data Analysis and Convolutional Neural Networks](https://arxiv.org/abs/2507.10381)
*Aaryam Sharma*

Main category: cs.CV

TL;DR: 提出了一种将拓扑数据分析（TDA）特征与深度学习模型（如ResNet18）相结合的方法，用于遥感图像分类，在EuroSAT和RESISC45数据集上均显著提高了准确率，并优于现有的大型模型。


<details>
  <summary>Details</summary>
Motivation: CNN模型在遥感图像分类中倾向于使用基于纹理的局部特征，可能存在偏差。而TDA能够有效描述复杂数据集的几何信息，具有鲁棒性。因此，将TDA特征与深度学习模型结合旨在克服CNN的局限性并提高分类性能。

Method: 提出了一种TDA特征工程流程，并将拓扑特征与深度学习模型（如ResNet18）集成，用于遥感图像分类。

Result: 在EuroSAT数据集上，所提出的方法将ResNet18模型的准确率提高了1.44%，达到99.33%，超过了其他单一模型（包括ResNet50和XL Vision Transformers）的准确率。在RESISC45数据集上，准确率也比基线ResNet18高出1.82%。

Conclusion: 将TDA特征与深度学习模型相结合可以提高遥感图像分类性能，即使在没有显式拓扑结构的数据集上也是如此，这表明TDA具有更广泛的应用前景。

Abstract: Topological data analysis (TDA) is a relatively new field that is gaining
rapid adoption due to its robustness and ability to effectively describe
complex datasets by quantifying geometric information. In imaging contexts, TDA
typically models data as filtered cubical complexes from which we can extract
discriminative features using persistence homology. Meanwhile, convolutional
neural networks (CNNs) have been shown to be biased towards texture based local
features. To address this limitation, we propose a TDA feature engineering
pipeline and a simple method to integrate topological features with deep
learning models on remote sensing classification. Our method improves the
performance of a ResNet18 model on the EuroSAT dataset by 1.44% achieving
99.33% accuracy, which surpasses all previously reported single-model
accuracies, including those with larger architectures, such as ResNet50 (2x
larger) and XL Vision Transformers (197x larger). We additionally show that our
method's accuracy is 1.82% higher than our ResNet18 baseline on the RESISC45
dataset. To our knowledge, this is the first application of TDA features in
satellite scene classification with deep learning. This demonstrates that TDA
features can be integrated with deep learning models, even on datasets without
explicit topological structures, thereby increasing the applicability of TDA. A
clean implementation of our method will be made publicly available upon
publication.

</details>


### [149] [Numerically Computing Galois Groups of Minimal Problems](https://arxiv.org/abs/2507.10407)
*Timothy Duff*

Main category: cs.CV

TL;DR: This paper explores the connection between algebra, numerical computation, and computer vision, focusing on solving parametric systems of equations relevant to computer vision's RanSaC algorithm. It reviews recent work to assess the difficulty and find practical solutions.


<details>
  <summary>Details</summary>
Motivation: The motivating problem is solving multiple instances of a parametric family of systems of algebraic equations, arising in robust model-fitting paradigms in computer vision (RanSaC).

Method: The paper reviews work from the last 5+ years to measure the difficulty of solving parametric systems.

Result: The paper makes strides towards practical solutions for solving these systems.

Conclusion: The paper discusses the intrinsic difficulty of solving parametric systems of algebraic equations and presents practical solutions.

Abstract: I discuss a seemingly unlikely confluence of topics in algebra, numerical
computation, and computer vision. The motivating problem is that of solving
multiples instances of a parametric family of systems of algebraic (polynomial
or rational function) equations. No doubt already of interest to ISSAC
attendees, this problem arises in the context of robust model-fitting paradigms
currently utilized by the computer vision community (namely "Random Sampling
and Consensus", aka "RanSaC".) This talk will give an overview of work in the
last 5+ years that aspires to measure the intrinsic difficulty of solving such
parametric systems, and makes strides towards practical solutions.

</details>


### [150] [Text-Visual Semantic Constrained AI-Generated Image Quality Assessment](https://arxiv.org/abs/2507.10432)
*Qiang Li,Qingsen Yan,Haojian Huang,Peng Wu,Haokui Zhang,Yanning Zhang*

Main category: cs.CV

TL;DR: 提出 SC-AGIQA 框架，通过结合 MLLMs 和 HVS 特性来改进 AI 生成图像的质量评估，解决了语义和细节感知问题，并在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 随着 AI 生成图像 (AGI) 技术的快速发展，准确评估其质量已成为一项日益重要的需求。现有的跨模态模型（如 CLIP 或 BLIP）在应用于 AGI 时面临语义不匹配和细节感知缺失两大挑战。

Method: 提出了一种名为 SC-AGIQA 的统一框架，该框架利用文本-视觉语义约束来增强对 AI 生成图像的文本-图像一致性和感知失真进行全面评估。框架包含两个核心模块：1. 文本辅助语义对齐模块 (TSAM)，利用多模态大语言模型 (MLLMs) 生成图像描述并与原始提示进行比较，以缩小语义差距并改进一致性检查。2. 频域细粒度退化感知模块 (FFDPM)，借鉴人眼视觉系统 (HVS) 的特性，采用频域分析并结合感知敏感度加权，以更好地量化细微的视觉失真并捕捉图像的细节视觉质量。

Result: 实验结果表明，SC-AGIQA 在多个基准数据集上表现优于现有的最先进方法。

Conclusion: SC-AGIQA 框架通过整合多模态大语言模型和基于人眼视觉系统的频率域分析，能够有效解决现有方法在评估 AI 生成图像时遇到的语义不匹配和细节感知缺失问题，并在多个基准数据集上的实验证明其性能优于现有最先进方法。

Abstract: With the rapid advancements in Artificial Intelligence Generated Image (AGI)
technology, the accurate assessment of their quality has become an increasingly
vital requirement. Prevailing methods typically rely on cross-modal models like
CLIP or BLIP to evaluate text-image alignment and visual quality. However, when
applied to AGIs, these methods encounter two primary challenges: semantic
misalignment and details perception missing. To address these limitations, we
propose Text-Visual Semantic Constrained AI-Generated Image Quality Assessment
(SC-AGIQA), a unified framework that leverages text-visual semantic constraints
to significantly enhance the comprehensive evaluation of both text-image
consistency and perceptual distortion in AI-generated images. Our approach
integrates key capabilities from multiple models and tackles the aforementioned
challenges by introducing two core modules: the Text-assisted Semantic
Alignment Module (TSAM), which leverages Multimodal Large Language Models
(MLLMs) to bridge the semantic gap by generating an image description and
comparing it against the original prompt for a refined consistency check, and
the Frequency-domain Fine-Grained Degradation Perception Module (FFDPM), which
draws inspiration from Human Visual System (HVS) properties by employing
frequency domain analysis combined with perceptual sensitivity weighting to
better quantify subtle visual distortions and enhance the capture of
fine-grained visual quality details in images. Extensive experiments conducted
on multiple benchmark datasets demonstrate that SC-AGIQA outperforms existing
state-of-the-art methods. The code is publicly available at
https://github.com/mozhu1/SC-AGIQA.

</details>


### [151] [4D-Animal: Freely Reconstructing Animatable 3D Animals from Videos](https://arxiv.org/abs/2507.10437)
*Shanshan Zhong,Jiawei Peng,Zehan Zheng,Zhongzhan Huang,Wufei Ma,Guofeng Zhang,Qihao Liu,Alan Yuille,Jieneng Chen*

Main category: cs.CV

TL;DR: 4D-Animal：一种从视频重建3D动物的新方法，无需关键点，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的从视频重建可驱动3D动物的方法通常依赖于稀疏语义关键点，而这些关键点的获取劳动密集且不准确，因此需要一个不依赖关键点的方法。

Method: 提出了一种名为4D-Animal的新框架，该框架不依赖稀疏关键点。该方法引入了一个密集特征网络，将2D表示映射到SMAL参数，并开发了一种分层对齐策略，整合了来自预训练2D视觉模型的轮廓、部件级、像素级和时间线索，以实现精确且时间连贯的跨帧重建。

Result: 实验证明，4D-Animal在准确性和时间连贯性方面优于基于模型和无模型的方法。

Conclusion: 4D-Animal 框架在无需稀疏关键点的情况下，能够从视频中重建可驱动的3D动物模型，并且在准确性和时间连贯性上优于现有方法，生成的3D资产还可以应用于其他3D任务。

Abstract: Existing methods for reconstructing animatable 3D animals from videos
typically rely on sparse semantic keypoints to fit parametric models. However,
obtaining such keypoints is labor-intensive, and keypoint detectors trained on
limited animal data are often unreliable. To address this, we propose
4D-Animal, a novel framework that reconstructs animatable 3D animals from
videos without requiring sparse keypoint annotations. Our approach introduces a
dense feature network that maps 2D representations to SMAL parameters,
enhancing both the efficiency and stability of the fitting process.
Furthermore, we develop a hierarchical alignment strategy that integrates
silhouette, part-level, pixel-level, and temporal cues from pre-trained 2D
visual models to produce accurate and temporally coherent reconstructions
across frames. Extensive experiments demonstrate that 4D-Animal outperforms
both model-based and model-free baselines. Moreover, the high-quality 3D assets
generated by our method can benefit other 3D tasks, underscoring its potential
for large-scale applications. The code is released at
https://github.com/zhongshsh/4D-Animal.

</details>


### [152] [CoralVQA: A Large-Scale Visual Question Answering Dataset for Coral Reef Image Understanding](https://arxiv.org/abs/2507.10449)
*Hongyong Han,Wei Wang,Gaowei Zhang,Mingjie Li,Yi Wang*

Main category: cs.CV

TL;DR: 创建了CoralVQA数据集，解决了珊瑚礁图像分析的挑战，评估了LVLM，并为珊瑚礁保护提供了支持。


<details>
  <summary>Details</summary>
Motivation: 为了解决珊瑚礁图像解释的挑战以及现有VQA数据集在珊瑚礁分析方面的不足，我们创建了第一个专门针对珊瑚礁分析的大型VQA数据集。

Method: 开发了一个包含12,805张真实珊瑚图像和277,653个问答对的半自动数据集构建流程，并与海洋生物学家合作以保证数据质量和规模。

Result: 构建了CoralVQA数据集，该数据集包含来自三大洋67个珊瑚属的图像，评估了现有LVLM的局限性，并为未来的LVLM发展提供了方向。

Conclusion: CoralVQA是一个大型视觉问答数据集，旨在促进珊瑚礁图像的分析和用户友好交互，为研究和LVLM发展奠定基础，并支持珊瑚礁保护工作。

Abstract: Coral reefs are vital yet vulnerable ecosystems that require continuous
monitoring to support conservation. While coral reef images provide essential
information in coral monitoring, interpreting such images remains challenging
due to the need for domain expertise. Visual Question Answering (VQA), powered
by Large Vision-Language Models (LVLMs), has great potential in user-friendly
interaction with coral reef images. However, applying VQA to coral imagery
demands a dedicated dataset that addresses two key challenges: domain-specific
annotations and multidimensional questions. In this work, we introduce
CoralVQA, the first large-scale VQA dataset for coral reef analysis. It
contains 12,805 real-world coral images from 67 coral genera collected from 3
oceans, along with 277,653 question-answer pairs that comprehensively assess
ecological and health-related conditions. To construct this dataset, we develop
a semi-automatic data construction pipeline in collaboration with marine
biologists to ensure both scalability and professional-grade data quality.
CoralVQA presents novel challenges and provides a comprehensive benchmark for
studying vision-language reasoning in the context of coral reef images. By
evaluating several state-of-the-art LVLMs, we reveal key limitations and
opportunities. These insights form a foundation for future LVLM development,
with a particular emphasis on supporting coral conservation efforts.

</details>


### [153] [RAPNet: A Receptive-Field Adaptive Convolutional Neural Network for Pansharpening](https://arxiv.org/abs/2507.10461)
*Tao Tang,Chengxu Yang*

Main category: cs.CV

TL;DR: RAPNet通过内容自适应卷积和注意力机制，提高了全色融合的精度和光谱保真度。


<details>
  <summary>Details</summary>
Motivation: 卷积神经网络（CNN）在处理融合任务时存在所有空间位置均一应用卷积核的局限性，忽略了局部内容的差异。为了克服这个问题，我们引入了RAPNet架构，该架构利用了内容自适应卷积。

Method: RAPNet架构的核心是感受野自适应融合卷积（RAPConv），旨在生成对局部特征上下文敏感的空间自适应卷积核，以提高空间细节提取的精度。此外，该网络集成了融合动态特征融合（PAN-DFF）模块，该模块包含一个注意力机制，以实现空间细节增强和光谱保真度之间的最佳平衡。

Result: RAPNet在公开数据集上的综合评估表明，与现有方法相比，RAPNet在定量指标和定性评估方面均表现出更优越的性能。消融分析进一步证实了所提出的自适应组件的有效性。

Conclusion: RAPNet在公开数据集上的综合评估表明，与现有方法相比，RAPNet在定量指标和定性评估方面均表现出更优越的性能。消融分析进一步证实了所提出的自适应组件的有效性。

Abstract: Pansharpening refers to the process of integrating a high resolution
panchromatic (PAN) image with a lower resolution multispectral (MS) image to
generate a fused product, which is pivotal in remote sensing. Despite the
effectiveness of CNNs in addressing this challenge, they are inherently
constrained by the uniform application of convolutional kernels across all
spatial positions, overlooking local content variations. To overcome this
issue, we introduce RAPNet, a new architecture that leverages content-adaptive
convolution. At its core, RAPNet employs the Receptive-field Adaptive
Pansharpening Convolution (RAPConv), designed to produce spatially adaptive
kernels responsive to local feature context, thereby enhancing the precision of
spatial detail extraction. Additionally, the network integrates the
Pansharpening Dynamic Feature Fusion (PAN-DFF) module, which incorporates an
attention mechanism to achieve an optimal balance between spatial detail
enhancement and spectral fidelity. Comprehensive evaluations on publicly
available datasets confirm that RAPNet delivers superior performance compared
to existing approaches, as demonstrated by both quantitative metrics and
qualitative assessments. Ablation analyses further substantiate the
effectiveness of the proposed adaptive components.

</details>


### [154] [RefSTAR: Blind Facial Image Restoration with Reference Selection, Transfer, and Reconstruction](https://arxiv.org/abs/2507.10470)
*Zhicun Yin,Junjie Chen,Ming Liu,Zhixin Wang,Fan Li,Renjing Pei,Xiaoming Li,Rynson W. H. Lau,Wangmeng Zuo*

Main category: cs.CV

TL;DR: 本研究提出了一种名为RefSTAR的新型盲人脸部图像恢复方法，通过参考选择、特征融合和重建来有效利用高分辨率参考图像，解决了现有方法的身份保留问题，并在实验中取得了更好的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的盲人脸部图像恢复方法在引入生成先验或高分辨率参考图像的辅助信息时，由于对细节纹理的不当特征引入，常常在身份保留方面遇到困难。

Method: RefSTAR方法包含三个关键部分：1. 参考选择（RefSel）模块：用于选择合适的参考图像。为了训练此模块，研究人员构建了一个包含10,000个真实-参考对的注释掩码的RefSel-HQ数据集。2. 特征融合范式：用于将参考图像的特征有效集成到恢复过程中，解决了标准交叉注意力操作中的固有问题。3. 参考图像重建机制：确保恢复后的图像中存在参考图像的特征，并重新设计了与掩码结合的周期一致性损失。

Result: 在各种骨干模型上进行的大量实验表明，RefSTAR方法表现出优越的性能，在身份保留能力和参考特征传递质量方面均有提升。

Conclusion: 盲人脸部图像恢复的现有方法在身份保留方面存在问题，主要归因于细节纹理的不当特征引入。本研究提出了一种名为RefSTAR的新型盲人脸部图像恢复方法，该方法通过参考选择、传输和重建来有效整合高分辨率参考图像中的合适特征。

Abstract: Blind facial image restoration is highly challenging due to unknown complex
degradations and the sensitivity of humans to faces. Although existing methods
introduce auxiliary information from generative priors or high-quality
reference images, they still struggle with identity preservation problems,
mainly due to improper feature introduction on detailed textures. In this
paper, we focus on effectively incorporating appropriate features from
high-quality reference images, presenting a novel blind facial image
restoration method that considers reference selection, transfer, and
reconstruction (RefSTAR). In terms of selection, we construct a reference
selection (RefSel) module. For training the RefSel module, we construct a
RefSel-HQ dataset through a mask generation pipeline, which contains annotating
masks for 10,000 ground truth-reference pairs. As for the transfer, due to the
trivial solution in vanilla cross-attention operations, a feature fusion
paradigm is designed to force the features from the reference to be integrated.
Finally, we propose a reference image reconstruction mechanism that further
ensures the presence of reference image features in the output image. The cycle
consistency loss is also redesigned in conjunction with the mask. Extensive
experiments on various backbone models demonstrate superior performance,
showing better identity preservation ability and reference feature transfer
quality. Source code, dataset, and pre-trained models are available at
https://github.com/yinzhicun/RefSTAR.

</details>


### [155] [GT-Loc: Unifying When and Where in Images Through a Joint Embedding Space](https://arxiv.org/abs/2507.10473)
*David G. Shatwell,Ishan Rajendrakumar Dave,Sirnam Swetha,Mubarak Shah*

Main category: cs.CV

TL;DR: GT-Loc 是一种新方法，可以同时预测图像的拍摄时间和地理位置，它通过将时间和位置信息编码到一个共享的嵌入空间中来实现这一点。该方法使用一种新颖的时间度量学习方法来处理时间的周期性，并在实验中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 时间预测的目标是仅使用视觉信息确定图像的拍摄时间，这支持元数据校正、检索和数字取证等应用。在室外场景中，每小时的估计依赖于亮度、色调和阴影位置等线索，而季节性变化和天气则为日期估计提供信息。然而，这些视觉线索在很大程度上依赖于地理背景，将时间预测与地理定位紧密联系起来。为了解决这种相互依赖性，GT-Loc 被引入以联合预测图像的拍摄时间和地理位置。

Method: GT-Loc 是一种新的检索方法，它使用单独的图像、时间和位置编码器，并将它们的嵌入对齐到一个共享的高维特征空间。为了处理时间的周期性，该方法提出了一个时间度量学习目标，通过在周期性环面表面上对成对时间差进行建模来提供软目标，而不是使用传统的具有硬正例和负例的对比学习。

Result: GT-Loc 方法在新的基准测试中证明，其联合优化性能优于先前的时间预测方法，甚至优于那些在推理时使用真实地理位置信息的方法。此外，该方法在标准地理定位任务上也取得了有竞争力的结果，并且其统一的嵌入空间支持组合式和基于文本的图像检索。

Conclusion: GT-Loc 方法通过联合预测图像的时间（小时和月份）和地理位置（GPS 坐标），并在共享的高维特征空间中对图像、时间和位置的编码器进行对齐，实现了比以往仅预测时间的方法更好的性能，即使那些在推理时输入了真实地理位置的方法也是如此。此外，GT-Loc 在标准的地理定位任务上也取得了有竞争力的结果，其统一的嵌入空间还有助于进行组合和基于文本的图像检索。

Abstract: Timestamp prediction aims to determine when an image was captured using only
visual information, supporting applications such as metadata correction,
retrieval, and digital forensics. In outdoor scenarios, hourly estimates rely
on cues like brightness, hue, and shadow positioning, while seasonal changes
and weather inform date estimation. However, these visual cues significantly
depend on geographic context, closely linking timestamp prediction to
geo-localization. To address this interdependence, we introduce GT-Loc, a novel
retrieval-based method that jointly predicts the capture time (hour and month)
and geo-location (GPS coordinates) of an image. Our approach employs separate
encoders for images, time, and location, aligning their embeddings within a
shared high-dimensional feature space. Recognizing the cyclical nature of time,
instead of conventional contrastive learning with hard positives and negatives,
we propose a temporal metric-learning objective providing soft targets by
modeling pairwise time differences over a cyclical toroidal surface. We present
new benchmarks demonstrating that our joint optimization surpasses previous
time prediction methods, even those using the ground-truth geo-location as an
input during inference. Additionally, our approach achieves competitive results
on standard geo-localization tasks, and the unified embedding space facilitates
compositional and text-based image retrieval.

</details>


### [156] [The Power of Certainty: How Confident Models Lead to Better Segmentation](https://arxiv.org/abs/2507.10490)
*Tugberk Erol,Tuba Caglikantar,Duygu Sarikaya*

Main category: cs.CV

TL;DR: 提出一种置信度自蒸馏方法，用于息肉分割，无需额外计算或内存即可提高模型性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 为了解决深度学习模型在息肉检测和分割任务中参数量大、易过拟合以及泛化能力差的问题。

Method: 提出了一种基于置信度的自蒸馏方法，该方法在训练期间仅使用前一迭代的数据存储，而无需在测试期间额外的计算或内存使用。该方法使用动态置信系数计算前一和当前迭代批次之间的损失。

Result: 该方法在息肉分割任务上超越了最先进的模型，并且在跨多个临床中心收集的数据集上具有良好的泛化能力。

Conclusion: 提出了一种基于置信度的自蒸馏方法，在结肠镜息肉分割任务上超越了最先进的模型，并且在来自多个临床中心的广泛数据集上表现出良好的泛化能力。

Abstract: Deep learning models have been proposed for automatic polyp detection and
precise segmentation of polyps during colonoscopy procedures. Although these
state-of-the-art models achieve high performance, they often require a large
number of parameters. Their complexity can make them prone to overfitting,
particularly when trained on biased datasets, and can result in poor
generalization across diverse datasets. Knowledge distillation and
self-distillation are proposed as promising strategies to mitigate the
limitations of large, over-parameterized models. These approaches, however, are
resource-intensive, often requiring multiple models and significant memory
during training. We propose a confidence-based self-distillation approach that
outperforms state-of-the-art models by utilizing only previous iteration data
storage during training, without requiring extra computation or memory usage
during testing. Our approach calculates the loss between the previous and
current iterations within a batch using a dynamic confidence coefficient. To
evaluate the effectiveness of our approach, we conduct comprehensive
experiments on the task of polyp segmentation. Our approach outperforms
state-of-the-art models and generalizes well across datasets collected from
multiple clinical centers. The code will be released to the public once the
paper is accepted.

</details>


### [157] [BenchReAD: A systematic benchmark for retinal anomaly detection](https://arxiv.org/abs/2507.10492)
*Chenyu Lian,Hong-Yu Zhou,Zhanli Hu,Jing Qin*

Main category: cs.CV

TL;DR: 该研究针对视网膜异常检测的基准不足问题，提出了一个全面且系统的基准。同时，开发了一种名为NFM-DRA的新方法，该方法在现有DRA方法的基础上引入了正常特征记忆库，解决了模型在面对未见过异常时的性能下降问题，并取得了当前最优的性能。研究提供的基准和方法有助于未来视网膜异常检测领域的研究和发展。


<details>
  <summary>Details</summary>
Motivation: 现有的视网膜异常检测基准存在数据有限、异常类型单一、测试集饱和以及缺乏泛化评估等问题，导致实验结果不够令人信服。同时，现有的方法大多侧重于单类监督学习，忽略了临床实践中可用的大量标记异常数据和未标记数据。

Method: 提出了一种名为NFM-DRA的新方法，该方法整合了之前提出的DRA（分离异常表征）方法和一个正常特征记忆库，旨在解决在面对未见过异常时的性能下降问题。

Result: 通过对现有方法的分类和基准测试，发现完全监督方法DRA虽然性能最佳，但在面对某些未见过异常时性能下降明显。而提出的NFM-DRA方法通过集成正常特征记忆库，成功缓解了性能下降问题，并建立了新的SOTA。

Conclusion: 所提出的NFM-DRA方法通过整合DRA和正常特征记忆，有效缓解了性能下降问题，并达到了新的SOTA（State-of-the-Art）水平。

Abstract: Retinal anomaly detection plays a pivotal role in screening ocular and
systemic diseases. Despite its significance, progress in the field has been
hindered by the absence of a comprehensive and publicly available benchmark,
which is essential for the fair evaluation and advancement of methodologies.
Due to this limitation, previous anomaly detection work related to retinal
images has been constrained by (1) a limited and overly simplistic set of
anomaly types, (2) test sets that are nearly saturated, and (3) a lack of
generalization evaluation, resulting in less convincing experimental setups.
Furthermore, existing benchmarks in medical anomaly detection predominantly
focus on one-class supervised approaches (training only with negative samples),
overlooking the vast amounts of labeled abnormal data and unlabeled data that
are commonly available in clinical practice. To bridge these gaps, we introduce
a benchmark for retinal anomaly detection, which is comprehensive and
systematic in terms of data and algorithm. Through categorizing and
benchmarking previous methods, we find that a fully supervised approach
leveraging disentangled representations of abnormalities (DRA) achieves the
best performance but suffers from significant drops in performance when
encountering certain unseen anomalies. Inspired by the memory bank mechanisms
in one-class supervised learning, we propose NFM-DRA, which integrates DRA with
a Normal Feature Memory to mitigate the performance degradation, establishing a
new SOTA. The benchmark is publicly available at
https://github.com/DopamineLcy/BenchReAD.

</details>


### [158] [Cameras as Relative Positional Encoding](https://arxiv.org/abs/2507.10496)
*Ruilong Li,Brent Yi,Junchen Liu,Hang Gao,Yi Ma,Angjoo Kanazawa*

Main category: cs.CV

TL;DR: 本文提出PRoPE，一种新的相机几何编码方法，用于多视图Transformer，在多项3D视觉任务中提升了性能。


<details>
  <summary>Details</summary>
Motivation: 多视图Transformer在处理涉及3D感知任务时，需要有效利用视图间的几何关系。本文旨在探索和比较不同的相机几何信息引入方法，以提高Transformer在多视图任务中的性能。

Method: 本文比较了三种用于在Transformer中引入相机几何信息的方法：token级别的射线图编码、attention级别的相对位姿编码，以及新提出的投影位置编码（PRoPE）。PRoPE能够捕捉完整的相机信息（内参和外参），并将其作为相对位置编码使用。

Result: 通过实验证明，在feedforward新视图合成任务中，相对相机条件能够提升性能，而PRoPE的引入能进一步带来性能增益。PRoPE在处理共享和变化的内参、结合token和attention级别条件、以及泛化到不同序列长度和内参的输入时，均表现出优势。此外，在立体深度估计和空间认知等任务以及更大的模型规模下，PRoPE的优势也得以验证。

Conclusion: PRoPE作为一种新的相对编码，通过捕捉完整的相机截头锥体（内参和外参），在多视图Transformer中有效利用相机几何关系，并在前馈新视图合成、立体深度估计和空间认知等任务上均有性能提升，且在不同场景设置下均表现出鲁棒性。

Abstract: Transformers are increasingly prevalent for multi-view computer vision tasks,
where geometric relationships between viewpoints are critical for 3D
perception. To leverage these relationships, multi-view transformers must use
camera geometry to ground visual tokens in 3D space. In this work, we compare
techniques for conditioning transformers on cameras: token-level raymap
encodings, attention-level relative pose encodings, and a new relative encoding
we propose -- Projective Positional Encoding (PRoPE) -- that captures complete
camera frustums, both intrinsics and extrinsics, as a relative positional
encoding. Our experiments begin by showing how relative camera conditioning
improves performance in feedforward novel view synthesis, with further gains
from PRoPE. This holds across settings: scenes with both shared and varying
intrinsics, when combining token- and attention-level conditioning, and for
generalization to inputs with out-of-distribution sequence lengths and camera
intrinsics. We then verify that these benefits persist for different tasks,
stereo depth estimation and discriminative spatial cognition, as well as larger
model sizes.

</details>


### [159] [National level satellite-based crop field inventories in smallholder landscapes](https://arxiv.org/abs/2507.10499)
*Philippe Rufin,Pauline Lucie Hammer,Leon-Friedrich Thomas,Sá Nogueira Lisboa,Natasha Ribeiro,Almeida Sitoe,Patrick Hostert,Patrick Meyfroidt*

Main category: cs.CV

TL;DR: 利用高分辨率遥感和深度学习技术，首次绘制了莫桑比克全国2100万个农田边界图，准确率达93%，揭示了小农场主普遍面临小农田规模问题，并强调了农田规模对农业可持续性的重要影响。


<details>
  <summary>Details</summary>
Motivation: 当前科学政策的制定面临对小农农业系统基本属性（如耕地空间分布和地块大小）理解有限的挑战。本研究旨在通过高分辨率遥感和深度学习技术，精确绘制农田边界，以支持可持续农业政策的设计。

Method: 研究整合了1.5米分辨率的地球观测数据和深度迁移学习技术，以绘制莫桑比克全国范围内的农田边界。该方法通过最小化参考数据需求和增强模型的可迁移性，实现了大规模复杂农业系统中的农田划分。

Result: 研究成功绘制了莫桑比克2023年全国范围内的2100万个农田边界图，准确率达到93%。研究发现莫桑比克的农田规模普遍较小，一半以上的农田小于0.16公顷。同时，农田规模在不同区域存在显著差异，这反映了当地多样化的农业经营模式，并强调了农田规模作为影响社会经济和环境结果的关键指标。

Conclusion: 该研究通过整合高分辨率地球观测数据和深度迁移学习，首次绘制了莫桑比克全国范围内的2100万个农田边界图，准确率高达93%，为制定可持续农业政策提供了关键的空间信息，并揭示了农田规模与社会经济及环境效益的密切关系。

Abstract: The design of science-based policies to improve the sustainability of
smallholder agriculture is challenged by a limited understanding of fundamental
system properties, such as the spatial distribution of active cropland and
field size. We integrate very high spatial resolution (1.5 m) Earth observation
data and deep transfer learning to derive crop field delineations in complex
agricultural systems at the national scale, while maintaining minimum reference
data requirements and enhancing transferability. We provide the first
national-level dataset of 21 million individual fields for Mozambique (covering
~800,000 km2) for 2023. Our maps separate active cropland from non-agricultural
land use with an overall accuracy of 93% and balanced omission and commission
errors. Field-level spatial agreement reached median intersection over union
(IoU) scores of 0.81, advancing the state-of-the-art in large-area field
delineation in complex smallholder systems. The active cropland maps capture
fragmented rural regions with low cropland shares not yet identified in global
land cover or cropland maps. These regions are mostly located in agricultural
frontier regions which host 7-9% of the Mozambican population. Field size in
Mozambique is very low overall, with half of the fields being smaller than 0.16
ha, and 83% smaller than 0.5 ha. Mean field size at aggregate spatial
resolution (0.05{\deg}) is 0.32 ha, but it varies strongly across gradients of
accessibility, population density, and net forest cover change. This variation
reflects a diverse set of actors, ranging from semi-subsistence smallholder
farms to medium-scale commercial farming, and large-scale farming operations.
Our results highlight that field size is a key indicator relating to
socio-economic and environmental outcomes of agriculture (e.g., food
production, livelihoods, deforestation, biodiversity), as well as their
trade-offs.

</details>


### [160] [Quantize-then-Rectify: Efficient VQ-VAE Training](https://arxiv.org/abs/2507.10547)
*Borui Zhang,Qihang Rao,Wenzhao Zheng,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: 本研究提出 ReVQ 框架，通过利用预训练 VAE 并结合特定技术，大幅降低了 VQ-VAE 的训练成本，同时保持了高质量的图像压缩和重建。


<details>
  <summary>Details</summary>
Motivation: 高压缩率 VQ-VAE 的训练计算成本高昂，通常需要数千个 GPU 小时。本研究旨在通过一种更高效的方法来解决这一挑战，实现快速的 VQ-VAE 训练并降低计算开销。

Method: 本研究提出了一种名为 Quantize-then-Rectify (ReVQ) 的框架，该框架利用预训练 VAE，通过控制量化噪声在 VAE 的容差阈值内，将其高效地转化为 VQ-VAE。该框架集成了通道多组量化以扩大码本容量，并使用后置校正器来缓解量化误差。

Result: ReVQ 能够将 ImageNet 图像压缩至最多 512 个标记，同时保持具有竞争力的重建质量（rFID = 1.06）。与现有最先进的方法相比，ReVQ 将训练成本降低了两个数量级以上，能在单张 NVIDIA 4090 GPU 上约 22 小时完成训练，而类似方法在 32 个 A100 GPU 上需要 4.5 天。

Conclusion: ReVQ 通过利用预训练 VAE 和引入通道多组量化及后置校正器，实现了高效的 VQ-VAE 训练，以更低的计算成本实现了有竞争力的图像压缩和重建质量，并在效率-重建权衡方面表现优越。

Abstract: Visual tokenizers are pivotal in multimodal large models, acting as bridges
between continuous inputs and discrete tokens. Nevertheless, training
high-compression-rate VQ-VAEs remains computationally demanding, often
necessitating thousands of GPU hours. This work demonstrates that a pre-trained
VAE can be efficiently transformed into a VQ-VAE by controlling quantization
noise within the VAE's tolerance threshold. We present
\textbf{Quantize-then-Rectify (ReVQ)}, a framework leveraging pre-trained VAEs
to enable rapid VQ-VAE training with minimal computational overhead. By
integrating \textbf{channel multi-group quantization} to enlarge codebook
capacity and a \textbf{post rectifier} to mitigate quantization errors, ReVQ
compresses ImageNet images into at most 512 tokens while sustaining competitive
reconstruction quality (rFID = 1.06). Significantly, ReVQ reduces training
costs by over two orders of magnitude relative to state-of-the-art approaches:
ReVQ finishes full training on a single NVIDIA 4090 in approximately 22 hours,
whereas comparable methods require 4.5 days on 32 A100 GPUs. Experimental
results show that ReVQ achieves superior efficiency-reconstruction trade-offs.

</details>


### [161] [Self-supervised Learning on Camera Trap Footage Yields a Strong Universal Face Embedder](https://arxiv.org/abs/2507.10552)
*Vladimir Iashin,Horace Lee,Dan Schofield,Andrew Zisserman*

Main category: cs.CV

TL;DR: 相机陷阱产生了大量的视觉数据，但手动识别动物是一个耗时的过程。本研究开发了一种自监督方法，可以使用未标记的相机陷阱数据从未标记的黑猩猩面部图像中学习面部嵌入。我们的方法在重新识别任务上优于监督方法，并且不需要任何身份标签。


<details>
  <summary>Details</summary>
Motivation: 相机陷阱在通过捕获大量视觉数据来革新野生动物监测，但手动识别单个动物仍然是一个重大的瓶颈。

Method: 本研究引入了一种完全自监督的方法，从无标签的相机陷阱片段中学习鲁棒的黑猩猩面部嵌入。利用DINOv2框架，我们在自动挖掘的面部裁剪上训练Vision Transformers，无需身份标签。

Result: 我们的方法在Bossou等具有挑战性的基准上，在开放集重新识别性能方面表现出色，甚至超越了监督基线，尽管在训练期间没有使用任何标签数据。

Conclusion: 本研究证明了自监督学习在生物多样性监测中的潜力，并为可扩展、非侵入性的人口研究铺平了道路。

Abstract: Camera traps are revolutionising wildlife monitoring by capturing vast
amounts of visual data; however, the manual identification of individual
animals remains a significant bottleneck. This study introduces a fully
self-supervised approach to learning robust chimpanzee face embeddings from
unlabeled camera-trap footage. Leveraging the DINOv2 framework, we train Vision
Transformers on automatically mined face crops, eliminating the need for
identity labels. Our method demonstrates strong open-set re-identification
performance, surpassing supervised baselines on challenging benchmarks such as
Bossou, despite utilising no labelled data during training. This work
underscores the potential of self-supervised learning in biodiversity
monitoring and paves the way for scalable, non-invasive population studies.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [162] [Spatial ModernBERT: Spatial-Aware Transformer for Table and Key-Value Extraction in Financial Documents at Scale](https://arxiv.org/abs/2507.08865)
*Javis AI Team,Amrendra Singh,Maulik Shah,Dharshan Sampath*

Main category: cs.CL

TL;DR: Spatial ModernBERT通过结合文本和空间信息，提高了从金融文档中提取表格和键值对的精度。


<details>
  <summary>Details</summary>
Motivation: 从金融文档中提取表格和键值对对于审计、数据分析和自动化发票处理等业务流程至关重要。

Method: 提出了一种名为Spatial ModernBERT的Transformer模型，该模型通过加入空间嵌入来增强，并将提取任务视为跨三个头的令牌分类任务：标签头（分类每个令牌的标签）、列头（预测列索引）和行头（区分项目行和标题行的起始）。模型在PubTables-1M数据集上进行预训练，然后在金融文档数据集上进行微调，并使用交叉熵损失进行优化。此外，还提出了一种使用B-I-IB标签合并令牌、重建表格布局和提取键值对的后处理方法。

Result: Spatial ModernBERT在金融文档上实现了高精度的表格和键值对提取。

Conclusion: Spatial ModernBERT模型有效地结合了文本和空间线索，实现了对真实金融文档中表格和键值对的高精度提取。

Abstract: Extracting tables and key-value pairs from financial documents is essential
for business workflows such as auditing, data analytics, and automated invoice
processing. In this work, we introduce Spatial ModernBERT-a transformer-based
model augmented with spatial embeddings-to accurately detect and extract
tabular data and key-value fields from complex financial documents. We cast the
extraction task as token classification across three heads: (1) Label Head,
classifying each token as a label (e.g., PO Number, PO Date, Item Description,
Quantity, Base Cost, MRP, etc.); (2) Column Head, predicting column indices;
(3) Row Head, distinguishing the start of item rows and header rows. The model
is pretrained on the PubTables-1M dataset, then fine-tuned on a financial
document dataset, achieving robust performance through cross-entropy loss on
each classification head. We propose a post-processing method to merge tokens
using B-I-IB tagging, reconstruct the tabular layout, and extract key-value
pairs. Empirical evaluation shows that Spatial ModernBERT effectively leverages
both textual and spatial cues, facilitating highly accurate table and key-value
extraction in real-world financial documents.

</details>


### [163] [SEALGuard: Safeguarding the Multilingual Conversations in Southeast Asian Languages for LLM Software Systems](https://arxiv.org/abs/2507.08898)
*Wenliang Shan,Michael Fu,Rui Yang,Chakkrit,Tantithamthavorn*

Main category: cs.CL

TL;DR: SEALGuard是一个新的多语言安全护栏，能有效检测多语言不安全和越狱提示，显著优于现有方法，提高了LLM系统的安全性。


<details>
  <summary>Details</summary>
Motivation: 解决现有安全护栏在处理低资源语言（如东南亚语言）的不安全输入和越狱提示方面存在的局限性，以提高LLM系统的多语言安全性。

Method: 使用低秩适应（LoRA）将通用的多语言语言模型适配成多语言安全护栏，并构建了一个包含260,000多个提示（涵盖安全、不安全和越狱类型）的SEALSBench多语言安全对齐数据集。

Result: 与现有的LlamaGuard等安全护栏相比，SEALGuard在检测多语言不安全和越狱提示方面表现更优，其防御成功率（DSR）比LlamaGuard高48%，并在DSR、精确率和F1分数上均取得最佳结果。研究还表明，多语言提示会显著降低LlamaGuard的性能。

Conclusion: SEALGuard通过引入有效的多语言安全护栏，提高了LLM系统的安全性。

Abstract: Safety alignment is critical for LLM-powered systems. While recent
LLM-powered guardrail approaches such as LlamaGuard achieve high detection
accuracy of unsafe inputs written in English (e.g., ``How to create a bomb?''),
they struggle with multilingual unsafe inputs. This limitation leaves LLM
systems vulnerable to unsafe and jailbreak prompts written in low-resource
languages such as those in Southeast Asia. This paper introduces SEALGuard, a
multilingual guardrail designed to improve the safety alignment across diverse
languages. It aims to address the multilingual safety alignment gap of existing
guardrails and ensure effective filtering of unsafe and jailbreak prompts in
LLM-powered systems. We adapt a general-purpose multilingual language model
into a multilingual guardrail using low-rank adaptation (LoRA). We construct
SEALSBench, a large-scale multilingual safety alignment dataset containing over
260,000 prompts in ten languages, including safe, unsafe, and jailbreak cases.
We evaluate SEALGuard against state-of-the-art guardrails such as LlamaGuard on
this benchmark. Our findings show that multilingual unsafe and jailbreak
prompts substantially degrade the performance of the state-of-the-art
LlamaGuard, which experiences a drop in Defense Success Rate (DSR) by 9% and
18%, respectively, compared to its performance on English-only prompts. In
contrast, SEALGuard outperforms existing guardrails in detecting multilingual
unsafe and jailbreak prompts, improving DSR by 48% over LlamaGuard and
achieving the best DSR, precision, and F1-score. Our ablation study further
reveals the contributions of adaptation strategies and model size to the
overall performance of SEALGuard. SEALGuard advances the safety alignment of
LLM systems by introducing an effective multilingual guardrail.

</details>


### [164] [Evaluating LLMs in Medicine: A Call for Rigor, Transparency](https://arxiv.org/abs/2507.08916)
*Mahmoud Alwakeel,Aditya Nagori,Vijay Krishnamoorthy,Rishikesan Kamaleswaran*

Main category: cs.CL

TL;DR: 现有的医学问答数据集存在局限性，缺乏临床相关性和透明度。需要协作努力来创建更严格、更全面的评估框架。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型在医学问题回答方面的当前局限性，重点关注用于其评估的数据集质量。

Method: 对广泛使用的大型语言模型基准数据集（包括 MedQA、MedMCQA、PubMedQA 和 MMLU）的严谨性、透明度和与临床场景的相关性进行了审查。还分析了医学期刊中的挑战性问题等替代方案，以确定其作为无偏见评估工具的潜力。

Result: 大多数现有数据集缺乏临床现实性、透明度和健全的验证过程。公开的挑战性问题提供了一些好处，但受到规模小、范围窄以及可能被大型语言模型训练暴露的限制。这些差距凸显了对安全、全面和具有代表性的数据集的需求。

Conclusion: 需要一个标准化的框架来评估医学领域的大型语言模型。机构和政策制定者之间的协作努力对于确保数据集和方法论严谨、公正且能反映临床复杂性至关重要。

Abstract: Objectives: To evaluate the current limitations of large language models
(LLMs) in medical question answering, focusing on the quality of datasets used
for their evaluation. Materials and Methods: Widely-used benchmark datasets,
including MedQA, MedMCQA, PubMedQA, and MMLU, were reviewed for their rigor,
transparency, and relevance to clinical scenarios. Alternatives, such as
challenge questions in medical journals, were also analyzed to identify their
potential as unbiased evaluation tools. Results: Most existing datasets lack
clinical realism, transparency, and robust validation processes. Publicly
available challenge questions offer some benefits but are limited by their
small size, narrow scope, and exposure to LLM training. These gaps highlight
the need for secure, comprehensive, and representative datasets. Conclusion: A
standardized framework is critical for evaluating LLMs in medicine.
Collaborative efforts among institutions and policymakers are needed to ensure
datasets and methodologies are rigorous, unbiased, and reflective of clinical
complexities.

</details>


### [165] [From KMMLU-Redux to KMMLU-Pro: A Professional Korean Benchmark Suite for LLM Evaluation](https://arxiv.org/abs/2507.08924)
*Seokhee Hong,Sunkyoung Kim,Guijin Son,Soyeon Kim,Yeonjung Hong,Jinsik Lee*

Main category: cs.CL

TL;DR: 开发了两个新的韩国LLM基准（KMMLU-Redux和KMMLU-Pro），用于评估模型在实际工业场景中的能力。


<details>
  <summary>Details</summary>
Motivation: 为了有效评估大型语言模型（LLM）在现实世界场景中的适用性，需要包含学术和工业领域的鲁棒基准。

Method: 介绍了两个新的韩国专家级基准：KMMLU-Redux（从KMMLU重建，剔除了关键错误）和KMMLU-Pro（基于韩国国家专业执照考试）。

Result: 实验证明，这两个基准能够全面代表韩国的工业知识。

Conclusion: 该基准全面代表了韩国的工业知识，并且是公开提供的。

Abstract: The development of Large Language Models (LLMs) requires robust benchmarks
that encompass not only academic domains but also industrial fields to
effectively evaluate their applicability in real-world scenarios. In this
paper, we introduce two Korean expert-level benchmarks. KMMLU-Redux,
reconstructed from the existing KMMLU, consists of questions from the Korean
National Technical Qualification exams, with critical errors removed to enhance
reliability. KMMLU-Pro is based on Korean National Professional Licensure exams
to reflect professional knowledge in Korea. Our experiments demonstrate that
these benchmarks comprehensively represent industrial knowledge in Korea. We
release our dataset publicly available.

</details>


### [166] [Te Ahorré Un Click: A Revised Definition of Clickbait and Detection in Spanish News](https://arxiv.org/abs/2507.09777)
*Gabriel Mordecki,Guillermo Moncecchi,Javier Couto*

Main category: cs.CL

TL;DR: 提出了一种新的Clickbait定义，即利用好奇心缺口作为关键概念。创建了首个西语Clickbait检测数据集TA1C，并实现了0.84的F1分数基线。


<details>
  <summary>Details</summary>
Motivation: 由于当前对于Clickbait的定义缺乏共识，提出了一种新的定义。

Method: 提出了一种新的数据集创建方法，缩小了概念范围和注释标准，以最大程度地减少主观性。

Result: 创建并发布了TA1C数据集，包含3500条来自18个知名媒体来源的手动注释推文，并达到了0.825的Fleiss' Kappa。提出的基线模型在F1分数上达到了0.84。

Conclusion: 提出了一个包含3500条推文的西语数据集TA1C，并实现了0.84的F1分数基线。

Abstract: We revise the definition of clickbait, which lacks current consensus, and
argue that the creation of a curiosity gap is the key concept that
distinguishes clickbait from other related phenomena such as sensationalism and
headlines that do not deliver what they promise or diverge from the article.
Therefore, we propose a new definition: clickbait is a technique for generating
headlines and teasers that deliberately omit part of the information with the
goal of raising the readers' curiosity, capturing their attention and enticing
them to click. We introduce a new approach to clickbait detection datasets
creation, by refining the concept limits and annotations criteria, minimizing
the subjectivity in the decision as much as possible. Following it, we created
and release TA1C (for Te Ahorr\'e Un Click, Spanish for Saved You A Click), the
first open source dataset for clickbait detection in Spanish. It consists of
3,500 tweets coming from 18 well known media sources, manually annotated and
reaching a 0.825 Fleiss' K inter annotator agreement. We implement strong
baselines that achieve 0.84 in F1-score.

</details>


### [167] [Self-Improving Model Steering](https://arxiv.org/abs/2507.08967)
*Rongyi Zhu,Yuhui Wang,Tanqiu Jiang,Jiacheng Liang,Ting Wang*

Main category: cs.CL

TL;DR: SIMS是一个创新的模型转向框架，它不依赖外部注释数据，而是通过自我改进来生成和优化对比样本，从而实现更有效的适应性转向。


<details>
  <summary>Details</summary>
Motivation: 传统的模型转向方法严重依赖外部注释数据，这不仅限制了它们在不同上下文中的适应性，而且其有效性也受到注释质量的限制。因此，需要一种无需外部监督的新型模型转向方法。

Method: SIMS框架的核心在于通过迭代式自我改进周期自主生成和优化对比样本，以实现自适应、特定于上下文的转向。此外，该框架还采用了提示排序和对比采样等新颖策略来增强转向效果。

Result: 在针对不同LLM和基准的广泛评估中，SIMS在转向效果和适应性方面均显著优于现有方法。

Conclusion: SIMS作为一个首个无需外部监督的自改进模型转向框架，通过迭代式自我改进周期自主生成和优化对比样本，实现了自适应、特定于上下文的转向。此外，SIMS采用新颖的提示排序和对比采样策略，以进一步提高转向效果。大量针对不同LLM和基准的评估表明，SIMS在转向效果和适应性方面明显优于现有方法，突显了自我改进模型转向作为未来推理时LLM对齐研究的一个有前景的方向。

Abstract: Model steering represents a powerful technique that dynamically aligns large
language models (LLMs) with human preferences during inference. However,
conventional model-steering methods rely heavily on externally annotated data,
not only limiting their adaptability to varying contexts but also tethering
their effectiveness to annotation quality. In this paper, we present SIMS, the
first self-improving model-steering framework that operates without relying on
external supervision. At its core, SIMS autonomously generates and refines
contrastive samples through iterative self-improvement cycles, enabling
adaptive, context-specific steering. Additionally, SIMS employs novel
strategies, including prompt ranking and contrast sampling, to further enhance
steering efficacy. Extensive evaluation across diverse LLMs and benchmarks
demonstrates that SIMS substantially outperforms existing methods in steering
effectiveness and adaptability, highlighting self-improving model steering as a
promising direction for future research on inference-time LLM alignment.

</details>


### [168] [Application of CARE-SD text classifier tools to assess distribution of stigmatizing and doubt-marking language features in EHR](https://arxiv.org/abs/2507.08969)
*Drew Walker,Jennifer Love,Swati Rajwal,Isabel C Walker,Hannah LF Cooper,Abeed Sarker,Melvin Livingston III*

Main category: cs.CL

TL;DR: 研究发现，在电子健康记录中，非裔美国人、接受政府保险的患者、自费患者以及患有某些疾病的患者更容易受到污名化语言的影响。男性患者的疑虑标记也更多。护士和社工在记录中使用的污名化语言和疑虑标记也更常见。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录（EHR）是患者污名化在医疗团队中得以延续的关键媒介。

Method: 通过扩展词典匹配和监督学习分类器，识别了MIMIC-III EHR中的疑虑标记和污名化标签的语言特征。使用泊松回归模型评估了语言特征发生率的预测因子。

Result: 研究发现在非裔美国人（RR: 1.16）、接受医疗补助/医疗保险或政府经营的保险（RR: 2.46）、自费（RR: 2.12）以及患有各种污名化疾病和精神健康状况的患者中，每份病历的污名化标签发生率更高。疑虑标记的模式相似，但男性患者的疑虑标记发生率更高（RR: 1.25）。护士（RR: 1.40）和社工（RR: 2.25）使用的污名化标签增加，疑虑标记的模式也类似。

Conclusion: 电子健康记录（EHR）中存在的污名化语言会不成比例地影响历史上被污名化的人群，并且会被多种提供者类型所传播。

Abstract: Introduction: Electronic health records (EHR) are a critical medium through
which patient stigmatization is perpetuated among healthcare teams. Methods: We
identified linguistic features of doubt markers and stigmatizing labels in
MIMIC-III EHR via expanded lexicon matching and supervised learning
classifiers. Predictors of rates of linguistic features were assessed using
Poisson regression models. Results: We found higher rates of stigmatizing
labels per chart among patients who were Black or African American (RR: 1.16),
patients with Medicare/Medicaid or government-run insurance (RR: 2.46),
self-pay (RR: 2.12), and patients with a variety of stigmatizing disease and
mental health conditions. Patterns among doubt markers were similar, though
male patients had higher rates of doubt markers (RR: 1.25). We found increased
stigmatizing labels used by nurses (RR: 1.40), and social workers (RR: 2.25),
with similar patterns of doubt markers. Discussion: Stigmatizing language
occurred at higher rates among historically stigmatized patients, perpetuated
by multiple provider types.

</details>


### [169] [Beyond vividness: Content analysis of induced hallucinations reveals the hidden structure of individual differences in visual imagery](https://arxiv.org/abs/2507.09011)
*Ana Chkhaidze,Reshanne R. Reeder,Connor Gag,Anastasia Kiyonaga,Seana Coulson*

Main category: cs.CL

TL;DR: 个体在视觉想象能力上的差异会影响其在Ganzflicker幻觉中所见内容的复杂性：强想象力者见自然内容，弱想象力者见几何图案。视觉语言模型能更好地捕捉这些差异。


<details>
  <summary>Details</summary>
Motivation: 探究视觉想象能力（想象谱）的个体差异是否会影响Ganzflicker诱导的视觉幻觉的复杂性，以及视觉语言模型在捕捉这些差异方面的能力。

Method: 利用自然语言处理工具分析了超过4000名参与者对Ganzflicker诱导的幻觉的自由文本描述，并比较了不同视觉想象表型个体报告内容的差异。研究还使用了来自视觉语言模型的嵌入来捕捉这些差异，并分析了参与者语言中传感器运动联想的丰富性。

Result: 强想象力个体在Ganzflicker幻觉中报告了复杂、自然的内容，而弱想象力个体报告了简单的几何图案。视觉语言模型比仅文本的模型更能捕捉到这些差异。强想象力个体使用的语言具有更丰富的传感器运动联想。

Conclusion: 本研究表明，个体在视觉想象能力上的差异（基于“想象谱”的分类）会影响他们在Ganzflicker诱导的视觉幻觉中所经历内容的复杂性。强想象力个体描述了更复杂、更自然的幻觉内容，而弱想象力个体则报告了简单的几何图案。

Abstract: A rapidly alternating red and black display known as Ganzflicker induces
visual hallucinations that reflect the generative capacity of the visual
system. Recent proposals regarding the imagery spectrum, that is, differences
in the visual system of individuals with absent imagery, typical imagery, and
vivid imagery, suggest these differences should impact the complexity of other
internally generated visual experiences. Here, we used tools from natural
language processing to analyze free-text descriptions of hallucinations from
over 4,000 participants, asking whether people with different imagery
phenotypes see different things in their mind's eye during Ganzflicker-induced
hallucinations. Strong imagers described complex, naturalistic content, while
weak imagers reported simple geometric patterns. Embeddings from vision
language models better captured these differences than text-only language
models, and participants with stronger imagery used language with richer
sensorimotor associations. These findings may reflect individual variation in
coordination between early visual areas and higher-order regions relevant for
the imagery spectrum.

</details>


### [170] [Lizard: An Efficient Linearization Framework for Large Language Models](https://arxiv.org/abs/2507.09025)
*Chien Van Nguyen,Ruiyi Zhang,Hanieh Deilamsalehy,Puneet Mathur,Viet Dac Lai,Haoliang Wang,Jayakumar Subramanian,Ryan A. Rossi,Trung Bui,Nikos Vlassis,Franck Dernoncourt,Thien Huu Nguyen*

Main category: cs.CL

TL;DR: Lizard是一个线性化框架，能将Transformer类LLM转化为亚二次复杂度模型，用于无限上下文生成。它通过改进的注意力机制和门控单元，在保持性能的同时解决了内存和计算瓶颈，并在多项基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决Transformer类大型语言模型在处理长上下文时面临的内存和计算瓶颈，这些瓶颈源于Softmax注意力的二次复杂度以及不断增长的键值（KV）缓存。

Method: 提出了一种名为Lizard的线性化框架，该框架将预训练的Transformer类大型语言模型转化为具有灵活的、亚二次（subquadratic）复杂度的无限上下文生成架构。Lizard引入了亚二次注意力机制来近似Softmax注意力，并结合了门控机制和元记忆体增强的滑动窗口注意力，形成混合机制。此外，还提出了一种硬件感知的算法来加速训练。

Result: Lizard在语言模型任务上实现了近乎无损的性能恢复，显著优于之前的线性化方法。在5次MMLU基准测试中，Lizard比先前模型提高了18个点，并在联想记忆任务上显示出显著的改进。

Conclusion: Lizard通过结合门控线性注意力、滑动窗口注意力以及元记忆体，并引入硬件感知算法，实现了在语言模型任务上接近无损地恢复教师模型的性能，显著优于以往的线性化方法，并在MMLU和联想记忆任务上取得了重大进展。

Abstract: We propose Lizard, a linearization framework that transforms pretrained
Transformer-based Large Language Models (LLMs) into flexible, subquadratic
architectures for infinite-context generation. Transformer-based LLMs face
significant memory and computational bottlenecks as context lengths increase,
due to the quadratic complexity of softmax attention and the growing key-value
(KV) cache. Lizard addresses these limitations by introducing a subquadratic
attention mechanism that closely approximates softmax attention while
preserving the output quality. Unlike previous linearization methods, which are
often limited by fixed model structures and therefore exclude gating
mechanisms, Lizard incorporates a gating module inspired by recent
state-of-the-art linear models. This enables adaptive memory control, supports
constant-memory inference, offers strong length generalization, and allows more
flexible model design. Lizard combines gated linear attention for global
context compression with sliding window attention enhanced by meta memory,
forming a hybrid mechanism that captures both long-range dependencies and
fine-grained local interactions. Moreover, we introduce a hardware-aware
algorithm that accelerates the training speed of our models. Extensive
experiments show that Lizard achieves near-lossless recovery of the teacher
model's performance across standard language modeling tasks, while
significantly outperforming previous linearization methods. On the 5-shot MMLU
benchmark, Lizard improves over prior models by 18 points and shows significant
improvements on associative recall tasks.

</details>


### [171] [ALIGN: Prompt-based Attribute Alignment for Reliable, Responsible, and Personalized LLM-based Decision-Making](https://arxiv.org/abs/2507.09037)
*Bharadwaj Ravichandran,David Joy,Paul Elliott,Brian Hu,Jadie Adams,Christopher Funk,Emily Veenhuis,Anthony Hoogs,Arslan Basharat*

Main category: cs.CL

TL;DR: ALIGN是一个开源系统，用于根据用户偏好个性化大型语言模型（LLM）决策助手，通过提示工程对齐细粒度属性，并支持不同领域和属性类型的分析。


<details>
  <summary>Details</summary>
Motivation: 用户具有可能影响决策的多样化价值观和偏好，这需要新的LLM对齐和个性化方法。现有的LLM比较工具主要集中在知识问答等基准测试任务上，而ALGN系统则专注于LLM作为决策助手的动态个性化。

Method: ALIGN系统通过基于提示的对齐来实现LLM决策助手的动态个性化，具有稳健的配置管理、结构化输出生成（含推理）以及多种算法实现（可切换LLM骨干）。其用户界面支持定性、并排的LLM比较及其与各种属性的对齐，后端模块化易于算法集成。

Result: 在公开意见调查的 বললেন demographic 对齐和医疗分诊决策的价值对齐这两个领域中，对齐方法进行了定量分析，证明了ALIGN框架在实现可靠、负责任和个性化的LLM决策助手方面的潜力。

Conclusion: ALIGN是一个开源框架，用于通过基于提示的对齐来实现大型语言模型（LLM）决策助手的动态个性化，并支持跨不同领域和类型属性的对齐方法。

Abstract: Large language models (LLMs) are increasingly being used as decision aids.
However, users have diverse values and preferences that can affect their
decision-making, which requires novel methods for LLM alignment and
personalization. Existing LLM comparison tools largely focus on benchmarking
tasks, such as knowledge-based question answering. In contrast, our proposed
ALIGN system focuses on dynamic personalization of LLM-based decision-makers
through prompt-based alignment to a set of fine-grained attributes. Key
features of our system include robust configuration management, structured
output generation with reasoning, and several algorithm implementations with
swappable LLM backbones, enabling different types of analyses. Our user
interface enables a qualitative, side-by-side comparison of LLMs and their
alignment to various attributes, with a modular backend for easy algorithm
integration. Additionally, we perform a quantitative analysis comparing
alignment approaches in two different domains: demographic alignment for public
opinion surveys and value alignment for medical triage decision-making. The
entire ALIGN framework is open source and will enable new research on reliable,
responsible, and personalized LLM-based decision-makers.

</details>


### [172] [OpenCodeReasoning-II: A Simple Test Time Scaling Approach via Self-Critique](https://arxiv.org/abs/2507.09075)
*Wasi Uddin Ahmad,Somshubra Majumdar,Aleksander Ficek,Sean Narenthiran,Mehrzad Samadi,Jocelyn Huang,Siddhartha Jain,Vahid Noroozi,Boris Ginsburg*

Main category: cs.CL

TL;DR: OpenCodeReasoning-II数据集的提出及两阶段微调策略在大语言模型代码生成与批评方面取得进展，并扩展了C++的评估基准。


<details>
  <summary>Details</summary>
Motivation: 为了应对代码生成和代码批评领域对大规模、高质量数据集的需求，以及探索测试时扩展推理型大语言模型的潜力。

Method: 采用两阶段监督微调策略：第一阶段专注于代码生成，第二阶段联合训练代码生成和代码批评模型。

Result: 微调后的Qwen2.5-Instruct模型在代码生成方面性能优于或等于先前最佳的开源模型；代码生成和代码批评模型的结合显著提高了竞技编程性能；扩展的LiveCodeBench基准支持C++语言，有助于更全面的大语言模型评估。

Conclusion: 该研究提出了OpenCodeReasoning-II数据集，并采用两阶段监督微调策略，在代码生成和代码批评方面取得了显著成果，同时扩展了LiveCodeBench基准以支持C++语言。

Abstract: Recent advancements in reasoning-based Large Language Models (LLMs),
particularly their potential through test-time scaling, have created
significant opportunities for distillation in code generation and critique.
However, progress in both areas fundamentally depends on large-scale,
high-quality datasets. In this work, we introduce OpenCodeReasoning-II, a
dataset consists of 2.5M question-solution-critique triples (approx. 35K unique
programming questions), making it nearly twice the size of the previous largest
publicly available code reasoning dataset. In this work, we employ a two-stage
supervised fine-tuning strategy. The first stage focuses on fine-tuning for
code generation, while the second stage involves the joint training of models
for both code generation and critique. Our resulting finetuned Qwen2.5-Instruct
models achieve performance in code generation that either exceeds or equals the
best prior open-weight distilled models. Notably, the integration of our code
generation and critique models leads to significant improvements in competitive
coding performance. Furthermore, we present an extension of the LiveCodeBench
benchmark to specifically support the C++ programming language, thereby
facilitating more comprehensive LLM evaluation using this benchmark.

</details>


### [173] [Dynamic Parameter Memory: Temporary LoRA-Enhanced LLM for Long-Sequence Emotion Recognition in Conversation](https://arxiv.org/abs/2507.09076)
*Jialong Mai,Xiaofen Xing,Yawei Li,Zhipeng Li,Jingyuan Xing,Xiangmin Xu*

Main category: cs.CL

TL;DR: 通过动态参数记忆（DPM）机制，SLLM能够处理无限长音频，提升对话情感识别性能。


<details>
  <summary>Details</summary>
Motivation: 现有的语音大语言模型（SLLM）在应用于语音情感识别（SER）时，受限于语音信号的帧率和SLLM的上下文窗口长度，难以有效处理长音频和多轮对话，并且现有的输入令牌压缩方法忽略了跨多轮对话的情感连续性和惯性。

Method: 提出了一种动态参数记忆（DPM）机制，该机制通过在推理过程中逐步将句子级信息和情感编码到临时的LoRA模块中，以有效记忆上下文信息，并将其应用于情感SLLM的推理过程以进行对话中的情感识别（ERC）。

Result: 在IEMOCAP数据集上的实验结果表明，DPM显著提高了SLLM处理长音频序列时的情感识别能力，达到了当前最优的性能。

Conclusion: DPM通过引入动态参数记忆机制，并结合上下文语义和句子级情感编码，成功解决了现有SLLM在处理长音频和多轮对话时遇到的挑战，实现了对无限长音频的处理能力，并在IEMOCAP数据集上取得了当前最优的性能。

Abstract: Recent research has focused on applying speech large language model (SLLM) to
improve speech emotion recognition (SER). However, the inherently high frame
rate in speech modality severely limits the signal processing and understanding
capabilities of SLLM. For example, a SLLM with a 4K context window can only
process 80 seconds of audio at 50Hz feature sampling rate before reaching its
capacity limit. Input token compression methods used in SLLM overlook the
continuity and inertia of emotions across multiple conversation turns. This
paper proposes a Dynamic Parameter Memory (DPM) mechanism with contextual
semantics and sentence-level emotion encoding, enabling processing of
unlimited-length audio with limited context windows in SLLM. Specifically, DPM
progressively encodes sentence-level information and emotions into a temporary
LoRA module during inference to effectively "memorize" the contextual
information. We trained an emotion SLLM as a backbone and incorporated our DPM
into inference for emotion recognition in conversation (ERC). Experimental
results on the IEMOCAP dataset show that DPM significantly improves the emotion
recognition capabilities of SLLM when processing long audio sequences,
achieving state-of-the-art performance.

</details>


### [174] [CompassJudger-2: Towards Generalist Judge Model via Verifiable Rewards](https://arxiv.org/abs/2507.09104)
*Taolin Zhang,Maosong Cao,Alexander Lam,Songyang Zhang,Kai Chen*

Main category: cs.CL

TL;DR: 本研究提出了一种名为CompassJudger-2的新型通用评判模型，通过多领域数据策​​略和奖励监督克服了现有评判模型的局限性。该模型在多个基准测试中表现出色，并且其较小规模的模型也能与更大的模型相媲美。此外，还引入了JudgerBenchV2基准测试以标准化评判模型的评估。


<details>
  <summary>Details</summary>
Motivation: 当前的评判模型存在专业化程度窄和鲁棒性有限的问题，这削弱了它们进行全面评估的能力。

Method: 通过任务驱动的多领域数据策​​略来训练一个名为CompassJudger-2的通用评判模型，并使用可验证的奖励来监督评判任务，通过拒绝采样来指导内在批判性推理，以培养鲁棒、可泛化的评判能力。我们还引入了一个改进的学习目标，使用边际策略梯度损失来提高性能。

Result: CompassJudger-2在多个评判和奖励基准测试中取得了优越的成果，并且其7B模型在评判准确性方面能够与DeepSeek-V3和Qwen3-235B-A22B等规模大得多的模型相媲美。此外，我们还提出了JudgerBenchV2，一个全面的基准测试，用于评估跨域评判准确性和排名一致性，为评判模型评估建立了标准。

Conclusion: CompassJudger-2在多个评判和奖励基准测试中取得了优越的成果，并且其7B模型在评判准确性方面能够与DeepSeek-V3和Qwen3-235B-A22B等规模大得多的模型相媲美。此外，我们还提出了JudgerBenchV2，一个全面的基准测试，用于评估跨域评判准确性和排名一致性，为评判模型评估建立了标准。这些贡献推动了鲁棒、可扩展的LLM评判技术的发展，并确立了新的性能和评估标准。

Abstract: Recently, the role of LLM-as-judge in evaluating large language models has
gained prominence. However, current judge models suffer from narrow
specialization and limited robustness, undermining their capacity for
comprehensive evaluations. In this work, we present CompassJudger-2, a novel
generalist judge model that overcomes these limitations via a task-driven,
multi-domain data curation strategy. Central to our approach is supervising
judgment tasks with verifiable rewards, guiding intrinsic critical reasoning
through rejection sampling to foster robust, generalizable judgment
capabilities. We introduce a refined learning objective with margin policy
gradient loss to enhance performance. Empirically, CompassJudger-2 achieves
superior results across multiple judge and reward benchmarks, and our 7B model
demonstrates competitive judgment accuracy with significantly larger models
like DeepSeek-V3 and Qwen3-235B-A22B. Additionally, we propose JudgerBenchV2, a
comprehensive benchmark evaluating cross-domain judgment accuracy and rank
consistency to standardize judge model evaluation. These contributions advance
robust, scalable LLM judgment and establish new performance and evaluation
standards.

</details>


### [175] [OPENXRD: A Comprehensive Benchmark and Enhancement Framework for LLM/MLLM XRD Question Answering](https://arxiv.org/abs/2507.09155)
*Ali Vosoughi,Ayoub Shahnazari,Yufeng Xi,Zeliang Zhang,Griffin Hess,Chenliang Xu,Niaz Abdolrahim*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This work presents OPENXRD, an open-book pipeline designed for
crystallography question answering, which integrates textual prompts with
concise supporting content generated by GPT-4.5. Instead of using scanned
textbooks, which may lead to copyright issues, OPENXRD generates compact,
domain-specific references that help smaller models understand key concepts in
X-ray diffraction (XRD). We evaluate OPENXRD on a well-defined set of 217
expert-level XRD questions by comparing different vision-language models,
including GPT-4 and LLaVA-based frameworks such as Mistral, LLaMA, and QWEN,
under both closed-book (without supporting material) and open-book (with
supporting material) conditions. Our experimental results show significant
accuracy improvements in models that use the GPT-4.5-generated summaries,
particularly those with limited prior training in crystallography. OPENXRD uses
knowledge from larger models to fill knowledge gaps in crystallography and
shows that AI-generated texts can help smaller models reason more effectively
in scientific tasks. While the current version of OPENXRD focuses on text-based
inputs, we also explore future extensions such as adding real crystal diagrams
or diffraction patterns to improve interpretation in specialized materials
science contexts. Overall, OPENXRD shows that specialized open-book systems can
be useful in materials science and provides a foundation for broader natural
language processing (NLP) tools in critical scientific fields.

</details>


### [176] [PU-Lie: Lightweight Deception Detection in Imbalanced Diplomatic Dialogues via Positive-Unlabeled Learning](https://arxiv.org/abs/2507.09157)
*Bhavinkumar Vinodbhai Kuwar,Bikrant Bikram Pratap Maurya,Priyanshu Gupta,Nitin Choudhury*

Main category: cs.CL

TL;DR: PU-Lie是一种轻量级模型，通过结合BERT嵌入、语言特征和PU学习，在Diplomacy数据集上提高了欺骗检测的准确性，同时显著减少了模型大小。


<details>
  <summary>Details</summary>
Motivation: 在战略对话中检测欺骗是一个复杂且高风险的任务，因为语言的微妙性和欺骗性与真实性通信之间的极端类别不平衡。该工作旨在解决Diplomacy数据集中欺骗检测的挑战，该数据集的欺骗性消息比例不到5%。

Method: PU-Lie模型结合了冻结的BERT嵌入、可解释的语言特征、特定于游戏的特征以及PU（Positive-Unlabeled）学习目标。PU学习是一种专门为只有少量欺骗性消息被标记而大部分消息未被标记的场景设计的学习方法。

Result: PU-Lie模型在Diplomacy数据集上实现了0.60的宏F1分数，并且可训练参数减少了650倍以上。通过对七种模型进行评估和消融研究，证明了PU学习、语言可解释性和说话者感知表示的价值。

Conclusion: PU-Lie模型通过结合冻结的BERT嵌入、可解释的语言和特定于游戏的特征以及PU学习目标，在Diplomacy数据集上实现了新的最佳宏F1分数0.60，同时将可训练参数减少了650倍以上。该模型特别适用于只有少量欺骗性消息被标记而大部分消息未被标记的情况，并优先考虑准确检测欺骗性消息。

Abstract: Detecting deception in strategic dialogues is a complex and high-stakes task
due to the subtlety of language and extreme class imbalance between deceptive
and truthful communications. In this work, we revisit deception detection in
the Diplomacy dataset, where less than 5% of messages are labeled deceptive. We
introduce a lightweight yet effective model combining frozen BERT embeddings,
interpretable linguistic and game-specific features, and a Positive-Unlabeled
(PU) learning objective. Unlike traditional binary classifiers, PU-Lie is
tailored for situations where only a small portion of deceptive messages are
labeled, and the majority are unlabeled. Our model achieves a new best macro F1
of 0.60 while reducing trainable parameters by over 650x. Through comprehensive
evaluations and ablation studies across seven models, we demonstrate the value
of PU learning, linguistic interpretability, and speaker-aware representations.
Notably, we emphasize that in this problem setting, accurately detecting
deception is more critical than identifying truthful messages. This priority
guides our choice of PU learning, which explicitly models the rare but vital
deceptive class.

</details>


### [177] [RAMA: Retrieval-Augmented Multi-Agent Framework for Misinformation Detection in Multimodal Fact-Checking](https://arxiv.org/abs/2507.09174)
*Shuo Yang,Zijian Yu,Zhenzhe Ying,Yuqin Dai,Guoqing Wang,Jun Lan,Jinfeng Xu,Jinze Li,Edith C. H. Ngai*

Main category: cs.CL

TL;DR: RAMA是一个多主体框架，通过网络搜索和多模态大语言模型来验证多媒体错误信息，尤其擅长处理模糊声明。


<details>
  <summary>Details</summary>
Motivation: 多模态错误信息的快速扩散给自动事实核查系统带来了重大挑战，特别是当声明模棱两可或缺乏足够上下文时。

Method: RAMA是一个新颖的检索增强型多主体框架，用于验证多媒体错误信息。RAMA包含三个核心创新：(1)将多模态声明转化为精确的网络搜索查询的战略查询制定；(2)从多样化、权威性来源进行交叉验证证据聚合；(3)利用多个多模态大语言模型和提示变体的互补优势的多主体集成架构。

Result: RAMA在基准数据集上实现了卓越的性能，尤其是在通过将验证与检索到的事实证据相结合来解决模棱两可或不太可能的声明方面表现出色。

Conclusion: 集成网络证据和多主体推理对于可信的多媒体验证是必要的，为更可靠和可扩展的事实核查解决方案铺平了道路。

Abstract: The rapid proliferation of multimodal misinformation presents significant
challenges for automated fact-checking systems, especially when claims are
ambiguous or lack sufficient context. We introduce RAMA, a novel
retrieval-augmented multi-agent framework designed for verifying multimedia
misinformation. RAMA incorporates three core innovations: (1) strategic query
formulation that transforms multimodal claims into precise web search queries;
(2) cross-verification evidence aggregation from diverse, authoritative
sources; and (3) a multi-agent ensemble architecture that leverages the
complementary strengths of multiple multimodal large language models and prompt
variants. Extensive experiments demonstrate that RAMA achieves superior
performance on benchmark datasets, particularly excelling in resolving
ambiguous or improbable claims by grounding verification in retrieved factual
evidence. Our findings underscore the necessity of integrating web-based
evidence and multi-agent reasoning for trustworthy multimedia verification,
paving the way for more reliable and scalable fact-checking solutions. RAMA
will be publicly available at https://github.com/kalendsyang/RAMA.git.

</details>


### [178] [Detecting and Pruning Prominent but Detrimental Neurons in Large Language Models](https://arxiv.org/abs/2507.09185)
*Ameen Ali,Shahar Katz,Lior Wolf,Ivan Titov*

Main category: cs.CL

TL;DR: 通过剪除神经元来增强 LLMs 的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在特定数据集上会发展出专门的机制，导致在遇到新任务或分布时性能下降。需要一种方法来提高模型的泛化能力。

Method: 本研究提出了一种微调方法，利用 Integrated Gradients 量化每个神经元对高置信度预测的贡献，识别并剪除与特定数据集机制（例如，依赖于特定领域的相关性）相关的神经元，以增强 transformer-based LLMs 的泛化能力。

Result: 该方法在多个选择题基准测试中显著提高了 LLMs 的性能，并且优于先前非剪除的适应方法。

Conclusion: 通过选择性地剪除与特定数据集机制相关联的神经元，我们的剪除方法能够强制模型依赖于可泛化的表征，从而在多个选择题基准测试中显著提高性能，并优于先前非剪除的适应方法。

Abstract: Large language models (LLMs) often develop learned mechanisms specialized to
specific datasets, such as reliance on domain-specific correlations, which
yield high-confidence predictions without generalizable reasoning. While
beneficial in one setting, these dataset-specific mechanisms typically degrade
performance when models encounter novel tasks or distributions. In this work,
we introduce a fine-tuning approach designed to enhance generalization by
identifying and pruning neurons associated with dataset-specific mechanisms in
transformer-based LLMs. Our method employs Integrated Gradients to quantify
each neuron's influence on high-confidence predictions, pinpointing those that
disproportionately contribute to dataset-specific performance without
supporting robust, transferable reasoning. Selectively pruning these neurons
compels the model to depend on generalizable representations. Evaluated across
multiple-choice benchmarks, our pruning-based fine-tuning significantly
enhances performance, surpassing prior (non-pruning) adaptation methods.

</details>


### [179] [Banzhida: Advancing Large Language Models for Tibetan with Curated Data and Continual Pre-Training](https://arxiv.org/abs/2507.09205)
*Leiyu Pan,Bojian Xiong,Lei Yang,Renren Jin,Shaowei Zhang,Yue Chen,Ling Shi,Jiang Zhou,Junru Wu,Zhen Wang,Jianxiang Peng,Juesi Xiao,Tianyu Dong,Zhuowen Han,Zhuo Chen,Sangjee Dondrub,Caizang Tai,Haixing Zhao,Huaque Cairang,Suonan Cairang,Rou Te,Lengben Zhaxi,Gazang Zhaxi,Zhonglin Ye,Yuhui Zheng,Chunyan Peng,Secha Jia,Pema Tashi,Cizhen Jiacuo,Pema Dorjee,Hongkai Liu,Pema Yanggon,Tsehang Dorjee,Jiaxin Han,Qiongying Hu,Jilin Man,Huanke You,Yuqi Ren,Duo La,Deyi Xiong*

Main category: cs.CL

TL;DR: Created the largest Tibetan LLM corpus and model (Banzhida), outperforming others on Tibetan tasks.


<details>
  <summary>Details</summary>
Motivation: Addressing the underrepresentation of Tibetan in existing large language models due to scarce training data.

Method: Curated the largest Tibetan pre-training corpus, continued pre/post-training a multilingual base model into Banzhida, and created new Tibetan benchmarks for evaluation.

Result: Banzhida significantly outperforms open-source and Tibetan-tailored models on a wide range of tasks.

Conclusion: Tibetan large language model Banzhida outperforms existing models across various tasks.

Abstract: Large language models have achieved remarkable progress across many
languages. However, Tibetan, as a representative low-resource language, is
particularly underrepresented in existing models due to the scarcity of
high-quality training corpora. To address this gap, we curate the largest
Tibetan pre-training corpus to date, aggregating data from diverse sources and
applying a dedicated data cleaning and processing pipeline tailored for
Tibetan. With the curated data, we continue pre/post-training a multilingual
base model into Banzhida, a multilingual large language model that advances
generative AI for Tibetan. To evaluate the Tibetan capabilities of the model,
we create new high-quality Tibetan benchmarks, and complement them with
existing public benchmarks. Experimental results demonstrate that Banzhida
consistently and significantly outperforms both open-source models of similar
scale and Tibetan-tailored models across a wide range of tasks.

</details>


### [180] [MetaClimage: A novel database of visual metaphors related to Climate Change, with costs and benefits analysis](https://arxiv.org/abs/2507.09225)
*Biagio Scalingi,Chiara Barattieri di San Pietro,Paolo Canal,Valentina Bambini*

Main category: cs.CL

TL;DR: 视觉隐喻比字面图像更美观但更难懂，对某些人更能唤起情绪。它们能引发更积极的体验和更深入的思考。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉隐喻（如将融化的冰川描绘成融化的冰制手榴弹）被认为是传达气候变化复杂性的有效工具，但以往对其在传播方面的影响研究较少，部分原因是相关材料难以获取。因此，本研究旨在通过创建一个包含视觉隐喻图像及其相关数据的数据库，并分析这些图像的传播效果，来填补这一研究空白。

Method: 本研究构建了一个名为MetaClimage的新型数据库，其中包含气候变化的视觉隐喻图像及其对应的字面图像。研究收集了人类对这些图像的评分，包括理解难度、效果、艺术质量和情绪唤起程度，并分析了参与者为图像生成标签的数量。利用自然语言处理技术，从标签中提取了语义和情感变量。最终，通过比较视觉隐喻和字面图像在这些指标上的差异，来评估视觉隐喻在气候变化传播中的作用。

Result: 视觉隐喻图像被认为比字面图像更难理解，但艺术性更高，然而在效果和情绪唤起方面无显著差异。但对于认知需求较高的参与者，视觉隐喻的情绪唤起更高。视觉隐喻获得了更多的标签，这些标签常常指向图像中未直接描绘的内容，并引发了具有更积极评价和更强支配性的词语。这表明视觉隐喻的认知负荷更大，但可能带来更深层次的认知加工和抽象化。总的来说，视觉隐喻在美学欣赏和积极体验方面优于字面图像。

Conclusion: 本研究通过提供一个包含视觉隐喻和字面图像的数据库，并分析了人类对这些图像的评分和标签，为理解气候变化视觉隐喻的传播效果提供了新的视角。研究结果表明，视觉隐喻虽然更难理解，但更具审美价值，并且在认知需求较高的参与者中能引起更高的情绪唤起。此外，视觉隐喻能够引发更积极的情感体验和更深入的认知加工，尽管在效果和唤起方面与字面图像无异。这些发现对于在环境传播中有效运用视觉隐喻具有重要意义。

Abstract: Visual metaphors of climate change (e.g., melting glaciers depicted as a
melting ice grenade) are regarded as valuable tools for addressing the
complexity of environmental challenges. However, few studies have examined
their impact on communication, also due to scattered availability of material.
Here, we present a novel database of Metaphors of Climate Change in Images
(MetaClimage) https://doi.org/10.5281/zenodo.15861012, paired with literal
images and enriched with human ratings. For each image, we collected values of
difficulty, efficacy, artistic quality, and emotional arousal from human
rating, as well as number of tags generated by participants to summarize the
message. Semantic and emotion variables were further derived from the tags via
Natural Language Processing. Visual metaphors were rated as more difficult to
understand, yet more aesthetically pleasant than literal images, but did not
differ in efficacy and arousal. The latter for visual metaphors, however, was
higher in participants with higher Need For Cognition. Furthermore, visual
metaphors received more tags, often referring to entities not depicted in the
image, and elicited words with more positive valence and greater dominance than
literal images. These results evidence the greater cognitive load of visual
metaphors, which nevertheless might induce positive effects such as deeper
cognitive elaboration and abstraction compared to literal stimuli. Furthermore,
while they are not deemed as more effective and arousing, visual metaphors seem
to generate superior aesthetic appreciation and a more positively valenced
experience. Overall, this study contributes to understanding the impact of
visual metaphors of climate change both by offering a database for future
research and by elucidating a cost-benefit trade-off to take into account when
shaping environmental communication.

</details>


### [181] [Swa-bhasha Resource Hub: Romanized Sinhala to Sinhala Transliteration Systems and Data Resources](https://arxiv.org/abs/2507.09245)
*Deshan Sumanathilaka,Sameera Perera,Sachithya Dharmasiri,Maneesha Athukorala,Anuja Dilrukshi Herath,Rukshan Dias,Pasindu Gamage,Ruvan Weerasinghe,Y. H. P. P. Priyadarshana*

Main category: cs.CL

TL;DR: Swa-bhasha资源中心提供了用于罗马化僧伽罗语到僧伽罗语音译的数据资源和算法，促进了僧伽罗语NLP的研究。


<details>
  <summary>Details</summary>
Motivation: 该中心的目标是为罗马化僧伽罗语到僧伽罗语的音译提供资源，并促进僧伽罗语自然语言处理（NLP）的研究。

Method: 该论文提供了对作者贡献的资源的详细概述，并对该领域现有的音译应用程序进行了比较分析。

Result: 该中心公开提供了可公开访问的数据集和相应工具，并对现有音译应用程序进行了比较分析。

Conclusion: 该Swa-bhasha资源中心提供了罗马化僧伽罗语到僧伽罗语音译的全面数据资源和算法，推动了僧伽罗语自然语言处理（NLP）的研究，特别是在音译模型训练和涉及罗马化僧伽罗语的应用开发方面。该中心公开提供了可公开访问的数据集和相应工具。

Abstract: The Swa-bhasha Resource Hub provides a comprehensive collection of data
resources and algorithms developed for Romanized Sinhala to Sinhala
transliteration between 2020 and 2025. These resources have played a
significant role in advancing research in Sinhala Natural Language Processing
(NLP), particularly in training transliteration models and developing
applications involving Romanized Sinhala. The current openly accessible data
sets and corresponding tools are made publicly available through this hub. This
paper presents a detailed overview of the resources contributed by the authors
and includes a comparative analysis of existing transliteration applications in
the domain.

</details>


### [182] [Psychology-Driven Enhancement of Humour Translation](https://arxiv.org/abs/2507.09259)
*Yuchen Su,Yonghua Zhu,Yang Chen,Diana Benavides-Prado,Michael Witbrock*

Main category: cs.CL

TL;DR: 提出了一种基于心理学和思维链的幽默分解机制（HDM），用于提升机器翻译的幽默感和可读性，实验结果显示翻译质量得到显著提高。


<details>
  <summary>Details</summary>
Motivation: 现有的语言模型在处理幽默翻译时存在语言干扰和缺乏幽默感的问题，因此需要一种新的方法来提升幽默翻译的质量。

Method: 提出了一种名为幽默分解机制（HDM）的方法，该方法利用思维链（CoT）来模仿人类的思考过程，并整合了幽默理论来优化翻译的幽默文本。

Result: 在开源幽默数据集上的自动评估实验表明，该方法在幽默感、流畅性和连贯性方面分别带来了 7.75%、2.81% 和 6.13% 的显著提升。

Conclusion: 该研究提出了一个受心理学启发的、利用思维链（CoT）的幽默分解机制（HDM），以优化翻译的幽默文本的可读性，并通过整合幽默理论进一步增强翻译文本中的幽默元素。

Abstract: Humour translation plays a vital role as a bridge between different cultures,
fostering understanding and communication. Although most existing Large
Language Models (LLMs) are capable of general translation tasks, these models
still struggle with humour translation, which is especially reflected through
linguistic interference and lacking humour in translated text. In this paper,
we propose a psychology-inspired Humour Decomposition Mechanism (HDM) that
utilises Chain-of-Thought (CoT) to imitate the ability of the human thought
process, stimulating LLMs to optimise the readability of translated humorous
texts. Moreover, we integrate humour theory in HDM to further enhance the
humorous elements in the translated text. Our automatic evaluation experiments
on open-source humour datasets demonstrate that our method significantly
improves the quality of humour translation, yielding average gains of 7.75\% in
humour, 2.81\% in fluency, and 6.13\% in coherence of the generated text.

</details>


### [183] [ClaritySpeech: Dementia Obfuscation in Speech](https://arxiv.org/abs/2507.09282)
*Dominika Woszczyk,Ranya Aloufi,Soteris Demetriou*

Main category: cs.CL

TL;DR: ClaritySpeech：一种用于痴呆症患者的语音增强和隐私保护框架，通过ASR、文本混淆和零样本TTS技术，在低数据环境下无需微调即可实现语音纠正和身份保留，显著提高了可访问性和隐私性。


<details>
  <summary>Details</summary>
Motivation: 痴呆症会改变语言模式，造成沟通障碍并引发隐私问题。现有的语音技术（如ASR）在处理痴呆症患者的非典型语音时存在困难，进一步阻碍了其可访问性。

Method: ClaritySpeech框架集成了自动语音识别（ASR）、文本混淆和零样本文本到语音（TTS）技术。

Result: ClaritySpeech框架在ADReSS和ADReSSo数据集上，平均F1分数分别降低了16%和10%，同时保持了50%的说话人相似度。词错误率（WER）从0.73分别提高到0.08（ADReSS）和0.15（ADReSSo），语音质量从1.65提高到约2.15。

Conclusion: 该研究提出了ClaritySpeech框架，通过集成ASR、文本混淆和零样本TTS技术，在低数据环境下无需微调即可纠正痴呆症患者的语音，同时保护说话人身份。实验证明，该系统在ADReSS和ADReSSo数据集上，在各种对抗性设置和模态下，平均F1分数分别降低了16%和10%，同时保持了50%的说话人相似度。此外，系统将词错误率（WER）从0.73分别提高到0.08（ADReSS）和0.15（ADReSSo），并将语音质量从1.65提高到约2.15，从而增强了隐私和可访问性。

Abstract: Dementia, a neurodegenerative disease, alters speech patterns, creating
communication barriers and raising privacy concerns. Current speech
technologies, such as automatic speech transcription (ASR), struggle with
dementia and atypical speech, further challenging accessibility. This paper
presents a novel dementia obfuscation in speech framework, ClaritySpeech,
integrating ASR, text obfuscation, and zero-shot text-to-speech (TTS) to
correct dementia-affected speech while preserving speaker identity in low-data
environments without fine-tuning. Results show a 16% and 10% drop in mean F1
score across various adversarial settings and modalities (audio, text, fusion)
for ADReSS and ADReSSo, respectively, maintaining 50% speaker similarity. We
also find that our system improves WER (from 0.73 to 0.08 for ADReSS and 0.15
for ADReSSo) and speech quality from 1.65 to ~2.15, enhancing privacy and
accessibility.

</details>


### [184] [DATE-LM: Benchmarking Data Attribution Evaluation for Large Language Models](https://arxiv.org/abs/2507.09424)
*Cathy Jiao,Yijun Pan,Emily Xiao,Daisy Sheng,Niket Jain,Hanzhang Zhao,Ishita Dasgupta,Jiaqi W. Ma,Chenyan Xiong*

Main category: cs.CL

TL;DR: DATE-LM 是一个用于评估 LLM 数据归因方法的基准，通过三个关键任务衡量归因质量。评估结果显示，没有一种方法能在所有任务上表现优异，且性能对评估设计敏感。


<details>
  <summary>Details</summary>
Motivation: 现有数据归因方法在大型语言模型（LLM）研究和应用中的重要性日益增加，但缺乏系统性的 LLM 中心评估。为了解决这个问题，我们引入了 DATE-LM。

Method: DATE-LM 是一个统一的基准，通过三个关键任务来衡量归因质量：训练数据选择、毒性/偏差过滤和事实归因。该基准易于使用，可配置大规模评估。研究人员使用 DATE-LM 对现有数据归因方法进行了大规模评估。

Result: 研究结果表明，没有一种数据归因方法能在所有任务上都表现优异，并且这些方法的性能对特定任务的评估设计很敏感。

Conclusion: DATE-LM 是一个用于评估大型语言模型（LLM）数据归因方法的统一基准。该基准通过三个关键任务——训练数据选择、毒性/偏差过滤和事实归因——来衡量归因质量。DATE-LM 的设计易于使用，可实现跨不同任务和 LLM 架构的大规模评估。此外，研究人员利用 DATE-LM 对现有数据归因方法进行了大规模评估，发现没有一种方法能在所有任务上都表现优异，并且现有方法的性能对特定任务的评估设计很敏感。最后，发布了一个公共排行榜以促进社区参与和方法比较。

Abstract: Data attribution methods quantify the influence of training data on model
outputs and are becoming increasingly relevant for a wide range of LLM research
and applications, including dataset curation, model interpretability, data
valuation. However, there remain critical gaps in systematic LLM-centric
evaluation of data attribution methods. To this end, we introduce DATE-LM (Data
Attribution Evaluation in Language Models), a unified benchmark for evaluating
data attribution methods through real-world LLM applications. DATE-LM measures
attribution quality through three key tasks -- training data selection,
toxicity/bias filtering, and factual attribution. Our benchmark is designed for
ease of use, enabling researchers to configure and run large-scale evaluations
across diverse tasks and LLM architectures. Furthermore, we use DATE-LM to
conduct a large-scale evaluation of existing data attribution methods. Our
findings show that no single method dominates across all tasks, data
attribution methods have trade-offs with simpler baselines, and method
performance is sensitive to task-specific evaluation design. Finally, we
release a public leaderboard for quick comparison of methods and to facilitate
community engagement. We hope DATE-LM serves as a foundation for future data
attribution research in LLMs.

</details>


### [185] [Enhancing Clinical Text Classification via Fine-Tuned DRAGON Longformer Models](https://arxiv.org/abs/2507.09470)
*Mingchuan Yang,Ziyuan Huang*

Main category: cs.CL

TL;DR: 通过调整模型结构、超参数和加入医学术语，DRAGON Longformer模型在医学文本分类任务上的准确率、精确率、召回率和F1分数均有显著提升。


<details>
  <summary>Details</summary>
Motivation: 优化DRAGON Longformer base模型以用于临床文本分类，特别是医学病例描述的二元分类，旨在提高模型在处理和理解医学文本方面的性能。

Method: 对DRAGON Longformer base模型进行了优化，包括超参数调整（序列长度从512增加到1024，学习率从1e-05调整到5e-06，训练轮次从5增加到8）、领域特定预处理以及架构调整，并融入了专门的医学术语。

Result: 优化后的模型在准确率（从72.0%提升至85.2%）、精确率（从68.0%提升至84.1%）、召回率（从75.0%提升至86.3%）和F1分数（从71.0%提升至85.2%）方面取得了显著的性能提升，且改进具有统计学意义（p < .001）。模型在解读医学术语、解剖测量和临床观察方面能力增强。

Conclusion: 该优化后的DRAGON Longformer模型在临床文本分类任务中表现出色，尤其在医学病例描述的二元分类方面，准确率、精确率、召回率和F1分数均有显著提升，证明了其在医疗自然语言处理应用中的潜力。

Abstract: This study explores the optimization of the DRAGON Longformer base model for
clinical text classification, specifically targeting the binary classification
of medical case descriptions. A dataset of 500 clinical cases containing
structured medical observations was used, with 400 cases for training and 100
for validation. Enhancements to the pre-trained
joeranbosma/dragon-longformer-base-mixed-domain model included hyperparameter
tuning, domain-specific preprocessing, and architectural adjustments. Key
modifications involved increasing sequence length from 512 to 1024 tokens,
adjusting learning rates from 1e-05 to 5e-06, extending training epochs from 5
to 8, and incorporating specialized medical terminology. The optimized model
achieved notable performance gains: accuracy improved from 72.0% to 85.2%,
precision from 68.0% to 84.1%, recall from 75.0% to 86.3%, and F1-score from
71.0% to 85.2%. Statistical analysis confirmed the significance of these
improvements (p < .001). The model demonstrated enhanced capability in
interpreting medical terminology, anatomical measurements, and clinical
observations. These findings contribute to domain-specific language model
research and offer practical implications for clinical natural language
processing applications. The optimized model's strong performance across
diverse medical conditions underscores its potential for broad use in
healthcare settings.

</details>


### [186] [The CoNLL-2013 Shared Task on Grammatical Error Correction](https://arxiv.org/abs/2507.09474)
*Hwee Tou Ng,Siew Mei Wu,Yuanbin Wu,Christian Hadiwinoto,Joel Tetreault*

Main category: cs.CL

TL;DR: CoNLL-2013 task: grammatical error correction. Presented data, metrics, and evaluated team approaches.


<details>
  <summary>Details</summary>
Motivation: To define the CoNLL-2013 shared task on grammatical error correction and present its associated data, evaluation metric, and results.

Method: Overview of participating teams' approaches and evaluation results.

Result: Evaluation results of various approaches adopted by participating teams.

Conclusion: The CoNLL-2013 shared task focused on grammatical error correction, providing task definitions, datasets, and evaluation metrics.

Abstract: The CoNLL-2013 shared task was devoted to grammatical error correction. In
this paper, we give the task definition, present the data sets, and describe
the evaluation metric and scorer used in the shared task. We also give an
overview of the various approaches adopted by the participating teams, and
present the evaluation results.

</details>


### [187] [Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs](https://arxiv.org/abs/2507.09477)
*Yangning Li,Weizhi Zhang,Yuyao Yang,Wei-Chieh Huang,Yaozu Wu,Junyu Luo,Yuanchen Bei,Henry Peng Zou,Xiao Luo,Yusheng Zhao,Chunkit Chan,Yankai Chen,Zhongfen Deng,Yinghui Li,Hai-Tao Zheng,Dongyuan Li,Renhe Jiang,Ming Zhang,Yangqiu Song,Philip S. Yu*

Main category: cs.CL

TL;DR: 本调查统一了检索增强生成（RAG）和推理方法，重点介绍了通过迭代搜索和推理实现最先进性能的协同框架，并为未来的研究指明了方向。


<details>
  <summary>Details</summary>
Motivation: 检索增强生成（RAG）通过注入外部知识来提高大型语言模型（LLM）的事实性，但它在需要多步推理的问题上表现不佳；相反，纯粹以推理为导向的方法常常出现幻觉或事实错误。本次调查旨在解决这些不足。

Method: 该调查首先分析了高级推理如何优化RAG的每个阶段（增强推理的RAG）。然后，我们展示了不同类型的检索知识如何为复杂推理（增强推理的RAG）提供缺失的先决条件和扩展上下文。最后，我们重点介绍了新兴的协同RAG-推理框架。

Result: 该调查对方法、数据集和开放挑战进行了分类，并为实现更有效、多模态适应、可信和以人为中心的深度RAG-推理系统指明了研究方向。

Conclusion: 该调查将基于检索的生成（RAG）和纯粹的以推理为导向的方法统一在统一的推理-检索视角下，并重点介绍了新兴的协同RAG-推理框架，其中（代理）LLM 迭代地交织搜索和推理，以在知识密集型基准测试中实现最先进的性能。

Abstract: Retrieval-Augmented Generation (RAG) lifts the factuality of Large Language
Models (LLMs) by injecting external knowledge, yet it falls short on problems
that demand multi-step inference; conversely, purely reasoning-oriented
approaches often hallucinate or mis-ground facts. This survey synthesizes both
strands under a unified reasoning-retrieval perspective. We first map how
advanced reasoning optimizes each stage of RAG (Reasoning-Enhanced RAG). Then,
we show how retrieved knowledge of different type supply missing premises and
expand context for complex inference (RAG-Enhanced Reasoning). Finally, we
spotlight emerging Synergized RAG-Reasoning frameworks, where (agentic) LLMs
iteratively interleave search and reasoning to achieve state-of-the-art
performance across knowledge-intensive benchmarks. We categorize methods,
datasets, and open challenges, and outline research avenues toward deeper
RAG-Reasoning systems that are more effective, multimodally-adaptive,
trustworthy, and human-centric. The collection is available at
https://github.com/DavidZWZ/Awesome-RAG-Reasoning.

</details>


### [188] [ViSP: A PPO-Driven Framework for Sarcasm Generation with Contrastive Learning](https://arxiv.org/abs/2507.09482)
*Changli Wang,Rui Wu,Fang Yin*

Main category: cs.CL

TL;DR: M2SaG是一个包含图像、讽刺文本和讽刺目标的多模态讽刺生成数据集。ViSP是一个集成了PPO和对比学习的生成框架，用于基准测试M2SaG，并能生成更高质量、更具讽刺意味的文本。


<details>
  <summary>Details</summary>
Motivation: 现有的讽刺生成研究过度依赖文本模式，忽略了视觉线索，并且在现有数据集中，图像内容与讽刺意图存在不匹配的问题。

Method: 提出了一种名为ViSP的生成框架，该框架集成了近端策略优化（PPO）和对比学习。PPO利用DIP的奖励分数来指导讽刺文本的生成，而对比学习则鼓励模型倾向于具有更高奖励分数的输出。

Result: 在五个指标集上评估了ViSP，结果显示其在讽刺生成方面优于包括大型语言模型在内的所有基线模型。生成的文本具有更高的平均讽刺分数和事实不一致性。

Conclusion: ViSP超越了包括大型语言模型在内的所有基线，证明了其在讽刺生成方面的优越性。ViSP生成的文本具有更高的平均讽刺分数（0.898 vs. 0.770）和事实不一致性（0.768 vs. 0.739），表明其产生的讽刺内容质量优于原始数据集。

Abstract: Human emotions are complex, with sarcasm being a subtle and distinctive form.
Despite progress in sarcasm research, sarcasm generation remains underexplored,
primarily due to the overreliance on textual modalities and the neglect of
visual cues, as well as the mismatch between image content and sarcastic intent
in existing datasets. In this paper, we introduce M2SaG, a multimodal sarcasm
generation dataset with 4,970 samples, each containing an image, a sarcastic
text, and a sarcasm target. To benchmark M2SaG, we propose ViSP, a generation
framework that integrates Proximal Policy Optimization (PPO) and contrastive
learning. PPO utilizes reward scores from DIP to steer the generation of
sarcastic texts, while contrastive learning encourages the model to favor
outputs with higher reward scores. These strategies improve overall generation
quality and produce texts with more pronounced sarcastic intent. We evaluate
ViSP across five metric sets and find it surpasses all baselines, including
large language models, underscoring their limitations in sarcasm generation.
Furthermore, we analyze the distributions of Sarcasm Scores and Factual
Incongruity for both M2SaG and the texts generated by ViSP. The generated texts
exhibit higher mean Sarcasm Scores (0.898 vs. 0.770) and Factual Incongruity
(0.768 vs. 0.739), demonstrating that ViSP produces higher-quality sarcastic
content than the original dataset. % The dataset and code will be publicly
available. Our dataset and code will be released at
\textit{https://github.com/wclapply/ViSP}.

</details>


### [189] [Balanced Training Data Augmentation for Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2507.09485)
*Junjie Liu,Yuanhe Tian,Yan Song*

Main category: cs.CL

TL;DR: 通过结合 LLM 和强化学习进行数据增强，改进了方面级情感分析（ABSA）在社交媒体文本上的性能，解决了数据量少、不平衡和短文本的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的基于 LLM 的 ABSA 研究面临短文本、少量且不平衡的标注训练数据（大多数标签为正向情感）的挑战，难以学习运行文本中的上下文信息。数据增强是一种可行的策略，但面临着确保增强数据高质量的挑战。

Method: 提出一种基于 LLM 的 ABSA 方法，并结合训练数据增强技术。具体来说，利用 LLM 根据原始训练数据生成增强后的训练数据，以构建更大、标签分布更均衡的新训练数据来更好地训练 ABSA 模型。此外，为了提高增强数据的质量，提出一种强化学习方法来优化数据增强的 LLM。

Result: 所提出的方法在英文 ABSA 基准数据集上表现出有效性，并在性能上优于强基线和大多数现有研究。

Conclusion: 实验结果和进一步分析表明，所提出的方法在英文 ABSA 基准数据集上是有效的，并且优于强基线和大多数现有研究。

Abstract: Aspect-based sentiment analysis (ABSA) is a crucial fine-grained task in
social media scenarios to identify the sentiment polarity of specific aspect
terms in a sentence. Although many existing studies leverage large language
models (LLMs) to perform ABSA due to their strong context understanding
capabilities, they still face challenges to learn the context information in
the running text because of the short text, as well as the small and unbalanced
labeled training data, where most data are labeled with positive sentiment.
Data augmentation (DA) is a feasible strategy for providing richer contextual
information, especially when using LLMs to create synthetic training data, but
faces challenges in ensuring a high quality of the augmented data.In this
paper, we propose an LLM-based ABSA approach with training data
augmentation.Specifically, an LLM is prompted to generate augmented training
data based on the original training data, so as to construct a new training
data with larger size and balanced label distributions to better train an ABSA
model. Meanwhile, in order to improve the quality of the augmented data, we
propose a reinforcement learning approach to optimize the data augmentation.
LLM.Experiment results and further analyses on English benchmark datasets for
ABSA demonstrate the effectiveness of our approach, where superior performance
is observed over strong baselines and most existing studies.

</details>


### [190] [GoalfyMax: A Protocol-Driven Multi-Agent System for Intelligent Experience Entities](https://arxiv.org/abs/2507.09497)
*Siyi Wu,Zeyu Wang,Xinyuan Song,Zhengpeng Zhou,Lifan Sun,Tianyu Shi*

Main category: cs.CL

TL;DR: GoalfyMax is a new framework for multi-agent systems that improves coordination and memory reuse using a protocol-driven approach and a layered memory system, outperforming existing frameworks in complex tasks.


<details>
  <summary>Details</summary>
Motivation: Traditional single-purpose AI systems lack sufficient coordination, memory reuse, and task decomposition capabilities, limiting their scalability in realistic enterprise environments that demand intelligent systems capable of handling complex, dynamic, and multi-faceted tasks with high levels of autonomy and adaptability.

Method: GoalfyMax is a protocol-driven framework for end-to-end multi-agent collaboration, featuring a standardized Agent-to-Agent (A2A) communication layer based on the Model Context Protocol (MCP) for asynchronous interactions, and the Experience Pack (XP) architecture, a layered memory system for knowledge retention and continual learning. It also includes multi-turn contextual dialogue, long-short term memory modules, and dynamic safety validation for real-time strategy adaptation.

Result: Empirical results on complex task orchestration benchmarks and a case study demonstrate that GoalfyMax achieves superior adaptability, coordination, and experience reuse compared to baseline frameworks.

Conclusion: GoalfyMax has the potential to serve as a scalable, future-ready foundation for multi-agent intelligent systems, demonstrating superior adaptability, coordination, and experience reuse compared to baseline frameworks.

Abstract: Modern enterprise environments demand intelligent systems capable of handling
complex, dynamic, and multi-faceted tasks with high levels of autonomy and
adaptability. However, traditional single-purpose AI systems often lack
sufficient coordination, memory reuse, and task decomposition capabilities,
limiting their scalability in realistic settings. To address these challenges,
we present \textbf{GoalfyMax}, a protocol-driven framework for end-to-end
multi-agent collaboration. GoalfyMax introduces a standardized Agent-to-Agent
(A2A) communication layer built on the Model Context Protocol (MCP), allowing
independent agents to coordinate through asynchronous, protocol-compliant
interactions. It incorporates the Experience Pack (XP) architecture, a layered
memory system that preserves both task rationales and execution traces,
enabling structured knowledge retention and continual learning. Moreover, our
system integrates advanced features including multi-turn contextual dialogue,
long-short term memory modules, and dynamic safety validation, supporting
robust, real-time strategy adaptation. Empirical results on complex task
orchestration benchmarks and case study demonstrate that GoalfyMax achieves
superior adaptability, coordination, and experience reuse compared to baseline
frameworks. These findings highlight its potential as a scalable, future-ready
foundation for multi-agent intelligent systems.

</details>


### [191] [Ref-Long: Benchmarking the Long-context Referencing Capability of Long-context Language Models](https://arxiv.org/abs/2507.09506)
*Junjie Wu,Gefei Gu,Yanan Zheng,Dit-Yan Yeung,Arman Cohan*

Main category: cs.CL

TL;DR: 本文提出了Ref-Long基准测试来评估大语言模型在长文本中定位和归因信息的能力，发现现有模型（包括GPT-4o）在这方面表现不佳，并进行了深入分析以期改进。


<details>
  <summary>Details</summary>
Motivation: 长上下文语言模型（LCLMs）在理解长文本方面能力强大，但长上下文指代——即将用户感兴趣的内容准确归因于长文本中的特定部分——这一关键能力的研究尚不充分。因此，有必要开发一个专门的基准测试来评估和提升LCLMs在这一任务上的表现。

Method: 本文提出了一个名为Ref-Long的新基准测试，用于评估大语言模型在长上下文中的指代能力。该基准测试包含三个不同难度的数据集，覆盖从合成到真实的应用场景。通过对13个大语言模型进行实验评估，并结合人类评估、任务格式调整、微调和错误分析等多种方法，深入探究模型在长文本指代任务中的表现和挑战。

Result: 在对13个长上下文语言模型（包括GPT-4o）的评估中，发现它们在长上下文指代任务上普遍存在显著的不足。通过进一步的分析，研究获得了关于模型能力、错误模式以及潜在改进方向的多个关键见解。

Conclusion: 现有大语言模型在长文本理解方面表现优异，但在长文本指代任务上仍有不足。本文提出的Ref-Long基准测试填补了这一空白，该基准测试专注于评估模型将特定信息关联到长文本的能力，强调上下文关系而非简单检索。实验表明，包括GPT-4o在内的13种模型在该任务上均存在明显短板，后续的详细分析也为理解和改进这些模型提供了关键见解。

Abstract: Long-context language models (LCLMs) have exhibited impressive capabilities
in long-context understanding tasks. Among these, long-context referencing -- a
crucial task that requires LCLMs to attribute items of interest to specific
parts of long-context data -- remains underexplored. To bridge this gap, this
paper proposes Referencing Evaluation for Long-context Language Models
(Ref-Long), a novel benchmark designed to assess the long-context referencing
capability of LCLMs. Specifically, Ref-Long requires LCLMs to identify the
indexes of documents that reference a specific key, emphasizing contextual
relationships between the key and the documents over simple retrieval. Based on
the task design, we construct three subsets ranging from synthetic to realistic
scenarios to form the Ref-Long benchmark. Experimental results of 13 LCLMs
reveal significant shortcomings in long-context referencing, even among
advanced models like GPT-4o. To further investigate these challenges, we
conduct comprehensive analyses, including human evaluations, task format
adjustments, fine-tuning experiments, and error analyses, leading to several
key insights. Our data and code can be found in https://github.
com/wujunjie1998/Ref-Long.

</details>


### [192] [How Important is `Perfect' English for Machine Translation Prompts?](https://arxiv.org/abs/2507.09509)
*Patrícia Schmidtová,Niyati Bafna,Seth Aycock,Gianluca Vico,Wiktor Kamzela,Katharina Hämmerl,Vilém Zouhar*

Main category: cs.CL

TL;DR: 该研究评估了提示词错误对 LLM 机器翻译的影响，发现提示词质量至关重要，不同类型的错误影响不同，并且 LLM 在高噪声下仍能翻译。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在最近的机器翻译评估中取得了优异的成绩，但它们也以对提示词中的错误和扰动敏感而闻名。

Method: 通过在用户提示词中引入人类可识别的合理错误和合成错误，对 LLM 在机器翻译和机器翻译评估两个相关任务上的性能进行系统性评估，并提供定量分析和定性见解，以了解模型对用户提示词中不断增加的噪声的响应情况。

Result: 提示词质量对翻译性能有显著影响：在存在大量错误的情况下，即使是良好的提示词的表现也可能不如没有错误的、质量较差的提示词。不同类型的噪声对翻译质量的影响不同，字符级和组合噪声比短语扰动对性能的损害更大。定性分析表明，较低的提示词质量主要导致指令遵循能力下降，而不是直接影响翻译质量本身。此外，即使在提示词包含大量随机噪声以至于人类无法识别的情况下，LLM 仍然能够进行翻译。

Conclusion: 提示词质量严重影响翻译性能，字符级和组合噪声比短语扰动对性能的影响更大。较低的提示词质量主要导致指令遵循能力下降，而不是直接影响翻译质量本身。然而，即使在提示词包含大量人眼难以辨认的随机噪声的情况下，LLM 仍然能够进行翻译。

Abstract: Large language models (LLMs) have achieved top results in recent machine
translation evaluations, but they are also known to be sensitive to errors and
perturbations in their prompts. We systematically evaluate how both humanly
plausible and synthetic errors in user prompts affect LLMs' performance on two
related tasks: Machine translation and machine translation evaluation. We
provide both a quantitative analysis and qualitative insights into how the
models respond to increasing noise in the user prompt.
  The prompt quality strongly affects the translation performance: With many
errors, even a good prompt can underperform a minimal or poor prompt without
errors. However, different noise types impact translation quality differently,
with character-level and combined noisers degrading performance more than
phrasal perturbations. Qualitative analysis reveals that lower prompt quality
largely leads to poorer instruction following, rather than directly affecting
translation quality itself. Further, LLMs can still translate in scenarios with
overwhelming random noise that would make the prompt illegible to humans.

</details>


### [193] [Adapting Definition Modeling for New Languages: A Case Study on Belarusian](https://arxiv.org/abs/2507.09536)
*Daniela Kazakouskaya,Timothee Mickus,Janine Siewert*

Main category: cs.CL

TL;DR: 该研究成功地将定义建模技术应用于白俄罗斯语，证明了只需要少量数据即可进行改编，但同时也指出了当前自动评估方法的不足。


<details>
  <summary>Details</summary>
Motivation: 旨在探索如何利用现有的预训练模型来支持新的、尚不支持的语言，以协助词典编纂者记录更多不同类型的语体和语言。

Method: 提出了一种新的包含43,150个定义的数据集，并使用该数据集来改编现有的定义建模系统以支持白俄罗斯语。

Result: 实验证明，改编定义建模系统只需要很少的数据，但现有的自动评估指标在评估模型性能方面存在一些局限性。

Conclusion: 该研究表明，改编现有的定义建模系统以支持新的语言（例如白俄罗斯语）是可行的，并且只需要少量数据。然而，自动评估指标在捕捉模型性能方面仍存在不足。

Abstract: Definition modeling, the task of generating new definitions for words in
context, holds great prospect as a means to assist the work of lexicographers
in documenting a broader variety of lects and languages, yet much remains to be
done in order to assess how we can leverage pre-existing models for as-of-yet
unsupported languages. In this work, we focus on adapting existing models to
Belarusian, for which we propose a novel dataset of 43,150 definitions. Our
experiments demonstrate that adapting a definition modeling systems requires
minimal amounts of data, but that there currently are gaps in what automatic
metrics do capture.

</details>


### [194] [NMIXX: Domain-Adapted Neural Embeddings for Cross-Lingual eXploration of Finance](https://arxiv.org/abs/2507.09601)
*Hanwool Lee,Sara Yu,Yewon Hwang,Jonghyun Choi,Heejae Ahn,Sungbum Jung,Youngjae Yu*

Main category: cs.CL

TL;DR: 针对韩语等低资源语言在金融领域嵌入表示不足的问题，提出NMIXX模型和KorFinSTS基准。NMIXX在金融任务上表现优于现有模型，并揭示了分词器设计的重要性。


<details>
  <summary>Details</summary>
Motivation: 通用句子嵌入模型在捕捉金融领域专业语义方面存在不足，尤其是在韩语等低资源语言中，这归因于领域特定术语、时间语义变化以及双语词汇不匹配等问题。

Method: 通过对18.8K个高置信度三元组（包括领域内释义、基于语义转移类型的难负例以及精确的韩英翻译）进行微调，构建了NMIXX（Neural eMbeddings for Cross-lingual eXploration of Finance）跨语言嵌入模型。同时发布了KorFinSTS，一个包含1,921对韩英金融句子相似度任务（STS）的基准测试集。

Result: NMIXX的多语言bge-m3变体在英文FinSTS上实现了+0.10的Spearman's rho增益，在KorFinSTS上实现了+0.22的增益，优于其微调前的检查点，并且在KorFinSTS上超越了其他模型。研究还发现，具有更丰富韩语词元覆盖率的模型适应效果更好，表明了分词器设计在低资源跨语言环境中的重要性。

Conclusion: 该研究提出了NMIXX模型和KorFinSTS基准，为金融领域的跨语言表示学习提供了强大的工具，并强调了分词器设计在低资源跨语言环境中的重要性。

Abstract: General-purpose sentence embedding models often struggle to capture
specialized financial semantics, especially in low-resource languages like
Korean, due to domain-specific jargon, temporal meaning shifts, and misaligned
bilingual vocabularies. To address these gaps, we introduce NMIXX (Neural
eMbeddings for Cross-lingual eXploration of Finance), a suite of cross-lingual
embedding models fine-tuned with 18.8K high-confidence triplets that pair
in-domain paraphrases, hard negatives derived from a semantic-shift typology,
and exact Korean-English translations. Concurrently, we release KorFinSTS, a
1,921-pair Korean financial STS benchmark spanning news, disclosures, research
reports, and regulations, designed to expose nuances that general benchmarks
miss.
  When evaluated against seven open-license baselines, NMIXX's multilingual
bge-m3 variant achieves Spearman's rho gains of +0.10 on English FinSTS and
+0.22 on KorFinSTS, outperforming its pre-adaptation checkpoint and surpassing
other models by the largest margin, while revealing a modest trade-off in
general STS performance. Our analysis further shows that models with richer
Korean token coverage adapt more effectively, underscoring the importance of
tokenizer design in low-resource, cross-lingual settings. By making both models
and the benchmark publicly available, we provide the community with robust
tools for domain-adapted, multilingual representation learning in finance.

</details>


### [195] [SpreadPy: A Python tool for modelling spreading activation and superdiffusion in cognitive multiplex networks](https://arxiv.org/abs/2507.09628)
*Salvatore Citraro,Edith Haim,Alessandra Carini,Cynthia S. Q. Siew,Giulio Rossetti,Massimo Stella*

Main category: cs.CL

TL;DR: SpreadPy是一个新的Python库，用于模拟认知网络中的激活扩散，通过案例研究证明了其在区分焦虑、调节认知负荷和理解语言障碍方面的能力，支持可复现的研究。


<details>
  <summary>Details</summary>
Motivation: 介绍SpreadPy库，该库旨在进行数值模拟以检验认知过程中的结构-功能关系，通过将模拟结果与知识建模中的既有理论进行比较，系统地研究激活动力学如何反映认知、心理和临床现象。

Method: 介绍了SpreadPy这个Python库，用于模拟认知单层和多层网络中的激活扩散，并通过三个案例研究展示了其效用：1. 模拟联想知识网络上的激活扩散，区分有数学焦虑的学生；2. 模拟创造性任务，展示激活轨迹随任务难度和认知负荷的变化；3. 模拟失语症患者的激活模式与词汇错误类型的关联。

Result: SpreadPy能够区分具有不同数学焦虑水平的学生，展示认知负荷如何调节词汇访问，并将失语症患者的激活模式与他们的语言错误类型联系起来，为理解个体差异和认知障碍提供了机制见解。

Conclusion: SpreadPy是一个用于模拟认知单层和多层网络中激活扩散的Python库，能够进行数值模拟以检验结构-功能关系，并提供可复现的研究支持。

Abstract: We introduce SpreadPy as a Python library for simulating spreading activation
in cognitive single-layer and multiplex networks. Our tool is designed to
perform numerical simulations testing structure-function relationships in
cognitive processes. By comparing simulation results with grounded theories in
knowledge modelling, SpreadPy enables systematic investigations of how
activation dynamics reflect cognitive, psychological and clinical phenomena. We
demonstrate the library's utility through three case studies: (1) Spreading
activation on associative knowledge networks distinguishes students with high
versus low math anxiety, revealing anxiety-related structural differences in
conceptual organization; (2) Simulations of a creativity task show that
activation trajectories vary with task difficulty, exposing how cognitive load
modulates lexical access; (3) In individuals with aphasia, simulated activation
patterns on lexical networks correlate with empirical error types (semantic vs.
phonological) during picture-naming tasks, linking network structure to
clinical impairments. SpreadPy's flexible framework allows researchers to model
these processes using empirically derived or theoretical networks, providing
mechanistic insights into individual differences and cognitive impairments. The
library is openly available, supporting reproducible research in psychology,
neuroscience, and education research.

</details>


### [196] [An Exploration of Knowledge Editing for Arabic](https://arxiv.org/abs/2507.09629)
*Basel Mousi,Nadir Durrani,Fahim Dalvi*

Main category: cs.CL

TL;DR: 首次研究阿拉伯语知识编辑（KE），评估了ROME、MEMIT、ICE和LTE方法，发现指令微调方法比参数化方法更适合跨语言任务。通过多语言训练改进了LTE方法。


<details>
  <summary>Details</summary>
Motivation: 尽管知识编辑（KE）在英语中已被广泛研究，但其在像阿拉伯语这样形态丰富的语言中的行为仍未得到充分研究。本工作首次对阿拉伯语KE进行了研究。

Method: 在阿拉伯语翻译的ZsRE和Counterfact基准上评估了四种方法（ROME、MEMIT、ICE和LTE），分析了多语言和跨语言设置。将LTE扩展到多语言设置。

Result: 实验表明，在Llama-2-7B-chat上，参数化方法在跨语言泛化方面存在困难，而指令微调方法则表现出更强的鲁棒性。

Conclusion: 参数化方法难以实现跨语言泛化，而指令微调方法表现更稳健。我们将LTE扩展到多语言设置，并展示联合阿拉伯语-英语训练能同时提高可编辑性和迁移能力。我们发布了阿拉伯语知识编辑基准和LTE的多语言训练数据，以支持未来的研究。

Abstract: While Knowledge Editing (KE) has been widely explored in English, its
behavior in morphologically rich languages like Arabic remains underexamined.
In this work, we present the first study of Arabic KE. We evaluate four methods
(ROME, MEMIT, ICE, and LTE) on Arabic translations of the ZsRE and Counterfact
benchmarks, analyzing both multilingual and cross-lingual settings. Our
experiments on Llama-2-7B-chat show show that parameter-based methods struggle
with cross-lingual generalization, while instruction-tuned methods perform more
robustly. We extend Learning-To-Edit (LTE) to a multilingual setting and show
that joint Arabic-English training improves both editability and transfer. We
release Arabic KE benchmarks and multilingual training for LTE data to support
future research.

</details>


### [197] [Can Group Relative Policy Optimization Improve Thai Legal Reasoning and Question Answering?](https://arxiv.org/abs/2507.09638)
*Pawitsapak Akarajaradwong,Chompakorn Chaksangchaichot,Pirat Pothavorn,Attapol Thamrongrattanarit-Rutherford,Ekapol Chuangsuwanich,Sarana Nutanong*

Main category: cs.CL

TL;DR: 提出了一种名为GRPO的方法，使用BGE-M3嵌入作为奖励，提高了泰语法律问答的准确性和响应质量，尤其在复杂法律推理任务上效果显著，并且计算成本更低。


<details>
  <summary>Details</summary>
Motivation: 为了解决检索增强生成（RAG）系统在泰语法律问答方面性能有限的问题，尤其是在需要广泛、复杂法律推理的问题上。

Method: 提出了一种使用分组相对策略优化（GRPO）使语言模型（LLM）倾向于提高法律引文准确性和响应质量的方法。该方法利用BGE-M3嵌入作为成本效益高的语义相似性奖励，与大型语言模型裁判相比，计算成本显著降低了2.5倍。

Result: 实验结果表明，GRPO在NitiBench基准测试中，引文F1分数比基础模型提高了90%，联合质量指标比指令调优提高了31%。

Conclusion: 该方法在处理复杂的法律推理任务方面比指令调优表现出更强的鲁棒性，为增强泰语法律大语言模型提供了一种有效且资源节约的解决方案。

Abstract: The Retrieval-Augmented Generation (RAG) systems' performance on Thai legal
question answering is still limited, especially for questions requiring
extensive, complex legal reasoning. To address these limitations, we introduce
an approach aligning LLMs toward improved law citation accuracy and better
response quality using Group-Relative Policy Optimization (GRPO). Our approach
leverages BGE-M3 embeddings as a cost-efficient semantic-similarity reward,
significantly reducing computational expenses up to 2.5x compared to large
language model judges. Experiments on the NitiBench benchmark demonstrate
substantial improvements: GRPO achieves up to 90% citation-F1 gains from the
base model and a 31% increase in joint quality metrics over instruction tuning.
Crucially, our method shows enhanced robustness on complex legal reasoning
tasks compared to instruction tuning, providing an effective and
resource-efficient solution for enhancing Thai legal LLMs.

</details>


### [198] [MCEval: A Dynamic Framework for Fair Multilingual Cultural Evaluation of LLMs](https://arxiv.org/abs/2507.09701)
*Shulin Huang,Linyi Yang,Yue Zhang*

Main category: cs.CL

TL;DR: MCEval是一个多语言评估框架，用于检测大型语言模型的文化偏见和跨文化理解能力，发现模型表现与语言文化对齐相关，并揭示了跨文化公平性问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在服务全球多样化用户群体时，表现出文化偏见和有限的跨文化理解能力，因此需要一个评估框架来系统地解决这些问题。

Method: 本研究提出了一种新颖的多语言评估框架MCEval，该框架采用动态文化问题构建，并通过反事实改写和混淆因素改写实现因果分析，以评估LLM的文化意识和文化偏见。

Result: 该评估框架覆盖13种文化和13种语言，包含39,897个文化意识实例和17,940个文化偏见实例。实验结果显示，在不同的语言场景中存在性能差异，且公平性问题突出，即在英语场景中表现良好的方法可能在其他场景中造成显著劣势。

Conclusion: 本研究提出了MCEval框架，一个用于评估大型语言模型（LLM）跨文化理解能力和文化偏见的多语言评估框架。实验结果表明，LLM的最佳文化表现不仅与训练数据分布有关，还与语言文化对齐有关，并揭示了公平性问题，即在英语场景中表现良好的方法可能在其他语言文化场景中带来显著劣势。MCEval是第一个提供对LLM文化理解深入见解的综合性多语言文化评估框架。

Abstract: Large language models exhibit cultural biases and limited cross-cultural
understanding capabilities, particularly when serving diverse global user
populations. We propose MCEval, a novel multilingual evaluation framework that
employs dynamic cultural question construction and enables causal analysis
through Counterfactual Rephrasing and Confounder Rephrasing. Our comprehensive
evaluation spans 13 cultures and 13 languages, systematically assessing both
cultural awareness and cultural bias across different linguistic scenarios. The
framework provides 39,897 cultural awareness instances and 17,940 cultural bias
instances. Experimental results reveal performance disparities across different
linguistic scenarios, demonstrating that optimal cultural performance is not
only linked to training data distribution, but also is related to
language-culture alignment. The evaluation results also expose the fairness
issue, where approaches appearing successful in the English scenario create
substantial disadvantages. MCEval represents the first comprehensive
multilingual cultural evaluation framework that provides deeper insights into
LLMs' cultural understanding.

</details>


### [199] [Large Language Models Encode Semantics in Low-Dimensional Linear Subspaces](https://arxiv.org/abs/2507.09709)
*Baturay Saglam,Paul Kassianik,Blaine Nelson,Sajana Weerawardhena,Yaron Singer,Amin Karbasi*

Main category: cs.CL

TL;DR: LLM的隐藏状态表现出可分离的几何结构，可以被利用来提高模型的安全性和对齐性。


<details>
  <summary>Details</summary>
Motivation: 理解大型语言模型（LLM）的潜在空间几何结构是解释其行为和改进其对齐的关键。然而，目前尚不清楚LLM在多大程度上在其内部组织与语义理解相关的表示。

Method: 通过对11个基于Transformer的LLM的隐藏状态进行大规模实证研究，分析了6个科学主题和12个层级。研究人员发现高层语义信息位于低维子空间中，并且在更深的层中以及在触发结构化推理或对齐行为的提示下，这种可分离性会更加明显。研究结果支持开发直接在潜在表示上操作的、感知几何的工具，以检测和减轻有害或对抗性内容。通过训练一个简单的MLP分类器作为轻量级的潜在空间护栏，证明了这种方法的有效性，该分类器能够高精度地检测对抗性和恶意提示。

Result: 研究发现，高层语义信息一致地位于低维子空间中，这些子空间形成了跨不同域的可线性分离的表示。这种可分离性在更深的层中以及在触发结构化推理或对齐行为的提示下会更加明显，即使表面内容保持不变。这种几何结构使得在隐藏空间中进行简单而有效的因果干预成为可能。例如，像链式思考这样的推理模式可以被单个向量方向捕获。

Conclusion: LLMs的潜在空间几何结构可以被理解和利用，以改进模型的对齐和安全性。研究发现，高层语义信息存在于低维子空间中，并且在更深的层中以及在触发结构化推理或对齐行为的提示下，这种可分离性会更加明显。研究结果支持开发直接在潜在表示上操作的、感知几何的工具，以检测和减轻有害或对抗性内容。研究人员通过训练一个简单的MLP分类器作为轻量级的潜在空间护栏，证明了这种方法的有效性，该分类器能够高精度地检测对抗性和恶意提示。

Abstract: Understanding the latent space geometry of large language models (LLMs) is
key to interpreting their behavior and improving alignment. \baturay{However,
it remains unclear to what extent LLMs internally organize representations
related to semantic understanding. To investigate this, we conduct a
large-scale empirical study of hidden states in transformer-based LLMs,
analyzing 11 decoder-only models across 6 scientific topics and 12 layers each.
We find that high-level semantic information consistently lies in
low-dimensional subspaces that form linearly separable representations across
distinct domains. This separability becomes more pronounced in deeper layers
and under prompts that trigger structured reasoning or alignment
behaviors$\unicode{x2013}$even when surface content is unchanged. This geometry
enables simple yet effective causal interventions in hidden space; for example,
reasoning patterns like chain-of-thought can be captured by a single vector
direction. Together, these findings support the development of geometry-aware
tools that operate directly on latent representations to detect and mitigate
harmful or adversarial content, using methods such as transport-based defenses
that leverage this separability. As a proof of concept, we demonstrate this
potential by training a simple MLP classifier as a lightweight latent-space
guardrail, which detects adversarial and malicious prompts with high precision.

</details>


### [200] [Your Pretrained Model Tells the Difficulty Itself: A Self-Adaptive Curriculum Learning Paradigm for Natural Language Understanding](https://arxiv.org/abs/2507.09758)
*Qi Feng,Yihong Liu,Hinrich Schütze*

Main category: cs.CL

TL;DR: 一种利用预训练语言模型自身预测的难度分数来自适应地组织训练样本的课程学习方法，可在自然语言理解任务中提升训练效率和模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有的课程学习方法依赖于手动定义的难度指标（如文本长度），这可能无法准确反映模型自身的学习状态。本研究旨在克服这一局限性。

Method: 提出了一种自适应课程学习范式，利用预训练语言模型预测的难度分数来指导微调样本的采样顺序，并探索了从易到难、从难到易以及混合采样等多种训练策略。

Result: 在四个自然语言理解（NLU）数据集上进行了评估，实验结果表明，与标准的随机采样相比，该方法能够实现更快的收敛速度和更高的性能。

Conclusion: 该方法通过使用预训练语言模型自身的预测分数来确定微调样本的难度，实现了更快的收敛速度和更高的性能。

Abstract: Curriculum learning is a widely adopted training strategy in natural language
processing (NLP), where models are exposed to examples organized by increasing
difficulty to enhance learning efficiency and performance. However, most
existing approaches rely on manually defined difficulty metrics -- such as text
length -- which may not accurately reflect the model's own perspective. To
overcome this limitation, we present a self-adaptive curriculum learning
paradigm that prioritizes fine-tuning examples based on difficulty scores
predicted by pre-trained language models (PLMs) themselves. Building on these
scores, we explore various training strategies that differ in the ordering of
examples for the fine-tuning: from easy-to-hard, hard-to-easy, to mixed
sampling. We evaluate our method on four natural language understanding (NLU)
datasets covering both binary and multi-class classification tasks.
Experimental results show that our approach leads to faster convergence and
improved performance compared to standard random sampling.

</details>


### [201] [Function Induction and Task Generalization: An Interpretability Study with Off-by-One Addition](https://arxiv.org/abs/2507.09875)
*Qinyuan Ye,Robin Jia,Xiang Ren*

Main category: cs.CL

TL;DR: 研究人员使用“偏离一的加法”任务，通过类似电路的可解释性技术，发现了大型语言模型中用于任务泛化的“函数归纳”机制，该机制由多个注意力头控制，并可在多种任务中重用。


<details>
  <summary>Details</summary>
Motivation: 探究驱动大型语言模型执行未见任务（通过上下文学习）的内部机制。

Method: 使用类似电路的可解释性技术，如路径修复，分析了模型内部的计算过程。

Result: 1. 发现了能够解释模型从标准加法泛化到“偏离一的加法”的函数归纳机制，该机制是对现有归纳头机制的抽象。2. “+1”函数的归纳由多个并行注意力头控制，每个头贡献函数的一部分。3. 该函数归纳机制在更广泛的任务中得到重用，包括人工任务和算法任务。

Conclusion: 该研究揭示了大型语言模型中可重用和可组合的结构如何实现任务级泛化，特别是通过“偏离一的加法”任务。

Abstract: Large language models demonstrate the intriguing ability to perform unseen
tasks via in-context learning. However, it remains unclear what mechanisms
inside the model drive such task-level generalization. In this work, we
approach this question through the lens of off-by-one addition (i.e., 1+1=3,
2+2=5, 3+3=?), a two-step, counterfactual task with an unexpected +1 function
as a second step. Leveraging circuit-style interpretability techniques such as
path patching, we analyze the models' internal computations behind their
notable performance and present three key findings. First, we uncover a
function induction mechanism that explains the model's generalization from
standard addition to off-by-one addition. This mechanism resembles the
structure of the induction head mechanism found in prior work and elevates it
to a higher level of abstraction. Second, we show that the induction of the +1
function is governed by multiple attention heads in parallel, each of which
emits a distinct piece of the +1 function. Finally, we find that this function
induction mechanism is reused in a broader range of tasks, including synthetic
tasks such as shifted multiple-choice QA and algorithmic tasks such as base-8
addition. Overall, our findings offer deeper insights into how reusable and
composable structures within language models enable task-level generalization.

</details>


### [202] [Enhancing Retrieval Augmented Generation with Hierarchical Text Segmentation Chunking](https://arxiv.org/abs/2507.09935)
*Hai Toan Nguyen,Tien Dat Nguyen,Viet Ha Nguyen*

Main category: cs.CL

TL;DR: RAG 系统通常使用块策略进行检索，以增强 LLM 访问外部知识的能力。然而，传统方法可能无法创建捕获足够语义意义的块。本研究提出了一种新颖的框架，通过集成分层文本分割和聚类来生成更有意义、语义上更连贯的块。该框架在推理过程中通过利用段级和簇级向量表示来检索信息，从而增加了检索更精确、上下文相关信息If you want to know more, please check the original text. 的可能性。与传统块技术相比，该方法在各种数据集上取得了改进的结果。


<details>
  <summary>Details</summary>
Motivation: 传统方法在创建捕获充分语义意义的块时常常失败，因为它们没有考虑到潜在的文本结构。

Method: 该框架在推理过程中通过利用段级和簇级向量表示来检索信息。

Result: 在 NarrativeQA、QuALITY 和 QASPER 数据集上进行的评估表明，与传统的块技术相比，所提出的方法取得了改进的结果。

Conclusion: 提出的新框架通过集成分层文本分割和聚类来生成更有意义、语义上更连贯的块，从而增强了 RAG。

Abstract: Retrieval-Augmented Generation (RAG) systems commonly use chunking strategies
for retrieval, which enhance large language models (LLMs) by enabling them to
access external knowledge, ensuring that the retrieved information is
up-to-date and domain-specific. However, traditional methods often fail to
create chunks that capture sufficient semantic meaning, as they do not account
for the underlying textual structure. This paper proposes a novel framework
that enhances RAG by integrating hierarchical text segmentation and clustering
to generate more meaningful and semantically coherent chunks. During inference,
the framework retrieves information by leveraging both segment-level and
cluster-level vector representations, thereby increasing the likelihood of
retrieving more precise and contextually relevant information. Evaluations on
the NarrativeQA, QuALITY, and QASPER datasets indicate that the proposed method
achieved improved results compared to traditional chunking techniques.

</details>


### [203] [Tiny Reward Models](https://arxiv.org/abs/2507.09973)
*Sarah Pan*

Main category: cs.CL

TL;DR: TinyRM 是一种小型、高效的奖励模型，在 RLHF 中具有巨大潜力。


<details>
  <summary>Details</summary>
Motivation: 随着奖励模型越来越多地用于测试时策略，其推理成本已成为日益增长的关注点。

Method: TinyRM 结合了 FLAN 风格的提示、方向性低秩适应（DoRA）和层冻结，以实现强大的性能。

Result: TinyRM 是一个小型（4 亿参数）的双向 MLM 系列，在推理和安全偏好建模任务上可与比它大 175 倍的模型的性能相媲美。

Conclusion: 小型双向掩码语言模型（MLM）如 TinyRM 是 RLHF 中奖励建模的高效、可扩展的替代方案。

Abstract: Large decoder-based language models have become the dominant architecture for
reward modeling in reinforcement learning from human feedback (RLHF). However,
as reward models are increasingly deployed in test-time strategies, their
inference costs become a growing concern. We present TinyRM, a family of small,
bidirectional masked language models (MLMs) with as few as 400 million
parameters, that rival the capabilities of models over 175 times larger on
reasoning and safety preference modeling tasks. TinyRM combines FLAN-style
prompting, Directional Low-Rank Adaptation (DoRA), and layer freezing to
achieve strong performance on RewardBench, despite using significantly fewer
resources. Our experiments suggest that small models benefit from
domain-specific tuning strategies, particularly in reasoning, where lightweight
finetuning methods are especially effective. While challenges remain in
building generalist models and conversational preference modeling, our
preliminary results highlight the promise of lightweight bidirectional
architectures as efficient, scalable alternatives for preference modeling.

</details>


### [204] [TextOmics-Guided Diffusion for Hit-like Molecular Generation](https://arxiv.org/abs/2507.09982)
*Hang Yuan,Chen Li,Wenjun Ma,Yuncheng Jiang*

Main category: cs.CL

TL;DR: TextOmics 和 ToDi 框架在 omics 数据和分子文本描述的结合方面取得了进展，实现了命中样分子的生成。


<details>
  <summary>Details</summary>
Motivation: 弥合异构数据和统一框架在整合分子表示方面的差距，以实现具有治疗潜力的命中样分子生成。

Method: 提出了一种名为TextOmics的基准，该基准在 omics 表达和分子文本描述之间建立了-一对应关系，并提出了一个名为ToDi的生成框架，该框架利用两个编码器（OmicsEn和TextEn）来捕获多层次的生物学和语义关联，并采用条件扩散（DiffGen）进行可控生成。

Result: ToDi框架在生成命中样分子方面优于现有方法，并且在零样本治疗性分子生成方面显示出巨大潜力。

Conclusion: ToDi在零样本治疗性分子生成方面表现出卓越的潜力，并超越了现有的最先进方法。

Abstract: Hit-like molecular generation with therapeutic potential is essential for
target-specific drug discovery. However, the field lacks heterogeneous data and
unified frameworks for integrating diverse molecular representations. To bridge
this gap, we introduce TextOmics, a pioneering benchmark that establishes
one-to-one correspondences between omics expressions and molecular textual
descriptions. TextOmics provides a heterogeneous dataset that facilitates
molecular generation through representations alignment. Built upon this
foundation, we propose ToDi, a generative framework that jointly conditions on
omics expressions and molecular textual descriptions to produce biologically
relevant, chemically valid, hit-like molecules. ToDi leverages two encoders
(OmicsEn and TextEn) to capture multi-level biological and semantic
associations, and develops conditional diffusion (DiffGen) for controllable
generation. Extensive experiments confirm the effectiveness of TextOmics and
demonstrate ToDi outperforms existing state-of-the-art approaches, while also
showcasing remarkable potential in zero-shot therapeutic molecular generation.
Sources are available at: https://github.com/hala-ToDi.

</details>


### [205] [Protective Factor-Aware Dynamic Influence Learning for Suicide Risk Prediction on Social Media](https://arxiv.org/abs/2507.10008)
*Jun Li,Xiangmeng Wang,Haoyang Li,Yifei Yan,Hong Va Leong,Ling Feng,Nancy Xiaonan Yu,Qing Li*

Main category: cs.CL

TL;DR: 本研究提出了一个创新的框架，结合了风险和保护因素来预测社交媒体用户随时间的自杀风险变化。该框架使用了一个新的数据集和一种动态学习方法，并在实验中表现优于现有技术，同时还能提供可解释的见解以辅助干预。


<details>
  <summary>Details</summary>
Motivation: 现有的关于社交媒体上自杀风险检测的研究主要集中于预测当前的自杀风险，而对于预测随时间变化的后续自杀风险的研究则相对较少。这限制了它们捕捉个体心理状态转变的快速变化能力。此外，现有研究往往忽略了在自杀风险预测中起着关键作用的保护因素（如社会支持和应对策略），而过度关注风险因素。然而，保护因素可以通过缓和风险因素的影响来降低自杀风险。

Method: 本研究提出了一个新颖的保护因素感知数据集，该数据集是利用12年的Reddit帖子以及全面的自杀风险、风险因素和保护因素注释构建的。研究还引入了一种动态因素影响学习方法，用于捕捉风险因素和保护因素对自杀风险转变过程中随时间变化的具体影响。该方法能够识别风险和保护因素对自杀风险的动态影响，这符合已有的心理学理论，即自杀风险会随着时间的推移而波动。

Result: 通过在三个数据集上进行的大量实验表明，本研究提出的模型显著优于最先进的模型和大型语言模型。此外，所提出的动态因素影响学习方法能够提供可解释的权重，有助于临床医生更好地理解自杀模式，并制定更具针对性的干预策略。

Conclusion: 该研究提出了一个新颖的框架，通过联合学习风险因素和保护因素对用户自杀风险转变的动态影响，来预测后续的自杀风险，并且该框架显著优于现有的模型和大型语言模型。此外，所提出的动态因素影响学习方法能够提供可解释的权重，有助于临床医生更好地理解自杀模式并制定更具针对性的干预策略。

Abstract: Suicide is a critical global health issue that requires urgent attention.
Even though prior work has revealed valuable insights into detecting current
suicide risk on social media, little attention has been paid to developing
models that can predict subsequent suicide risk over time, limiting their
ability to capture rapid fluctuations in individuals' mental state transitions.
In addition, existing work ignores protective factors that play a crucial role
in suicide risk prediction, focusing predominantly on risk factors alone.
Protective factors such as social support and coping strategies can mitigate
suicide risk by moderating the impact of risk factors. Therefore, this study
proposes a novel framework for predicting subsequent suicide risk by jointly
learning the dynamic influence of both risk factors and protective factors on
users' suicide risk transitions. We propose a novel Protective Factor-Aware
Dataset, which is built from 12 years of Reddit posts along with comprehensive
annotations of suicide risk and both risk and protective factors. We also
introduce a Dynamic Factors Influence Learning approach that captures the
varying impact of risk and protective factors on suicide risk transitions,
recognizing that suicide risk fluctuates over time according to established
psychological theories. Our thorough experiments demonstrate that the proposed
model significantly outperforms state-of-the-art models and large language
models across three datasets. In addition, the proposed Dynamic Factors
Influence Learning provides interpretable weights, helping clinicians better
understand suicidal patterns and enabling more targeted intervention
strategies.

</details>


### [206] [GeLaCo: An Evolutionary Approach to Layer Compression](https://arxiv.org/abs/2507.10059)
*David Ponce,Thierry Etchegoyhen,Javier Del Ser*

Main category: cs.CL

TL;DR: GeLaCo是一种利用进化算法通过层折叠来压缩大型语言模型（LLM）的方法，它通过高效的搜索和优于现有技术的性能，解决了LLM的部署和使用障碍。


<details>
  <summary>Details</summary>
Motivation: 为了克服大型语言模型（LLM）因巨大的计算需求而面临的部署和使用障碍，并寻找比传统的结构化修剪等方法更优越、更不易错过更好解决方案的模型压缩方法。

Method: GeLaCo是一种通过层折叠进行LLM压缩的进化方法，它利用基于种群的搜索和模块化相似性适应度函数来探索压缩解决方案空间，并支持单目标和多目标搜索。

Result: GeLaCo在基于困惑度和生成性评估的评估中，在基础模型和指令调整模型上都优于最先进的替代方案。

Conclusion: GeLaCo在模型压缩方面取得了优于最先进的替代方案的成果，通过基于种群的搜索和捕捉注意力、前馈和隐藏状态表示的模块化相似性适应度函数，有效地探索了压缩解决方案空间。它还支持单目标和多目标进化压缩搜索，在压缩和质量轴上建立了第一个帕累托前沿。

Abstract: Large Language Models (LLM) have achieved remarkable performance across a
large number of tasks, but face critical deployment and usage barriers due to
substantial computational requirements. Model compression methods, which aim to
reduce model size while preserving its capacity, are an important means to
mitigate these issues. Promising approaches along these lines, such as
structured pruning, typically require costly empirical search for optimal
variants and may run the risk of ignoring better solutions. In this work we
introduce GeLaCo, an evolutionary approach to LLM compression via layer
collapse. Our approach supports an efficient exploration of the compression
solution space via population-based search and a module-wise similarity fitness
function capturing attention, feed-forward, and hidden state representations.
GeLaCo also supports both single and multi-objective evolutionary compression
search, establishing the first Pareto frontier along compression and quality
axes. We evaluate GeLaCo solutions via both perplexity-based and generative
evaluations over foundational and instruction-tuned models, outperforming
state-of-the-art alternatives.

</details>


### [207] [Cultural Bias in Large Language Models: Evaluating AI Agents through Moral Questionnaires](https://arxiv.org/abs/2507.10073)
*Simon Münker*

Main category: cs.CL

TL;DR: LLMs未能代表人类价值观，而是平均了它们，系统性地使道德多样性趋于同质化。


<details>
  <summary>Details</summary>
Motivation: 探索AI系统是否真正代表人类价值观，还是仅仅平均了它们。

Method: 通过在19个文化背景下应用道德基础问卷，将最先进的LLMs的起源与人类基线数据进行比较。

Result: LLMs未能代表具有多种文化背景的道德框架，并且模型规模的增加并未持续提高文化代表性的保真度。

Conclusion: LLMs未能代表人类价值观，而只是平均了它们，并且LLMs系统性地使道德多样性趋于同质化。目前的AI对齐方法存在根本性局限性，需要更多基于数据的对齐而非提示。

Abstract: Are AI systems truly representing human values, or merely averaging across
them? Our study suggests a concerning reality: Large Language Models (LLMs)
fail to represent diverse cultural moral frameworks despite their linguistic
capabilities. We expose significant gaps between AI-generated and human moral
intuitions by applying the Moral Foundations Questionnaire across 19 cultural
contexts. Comparing multiple state-of-the-art LLMs' origins against human
baseline data, we find these models systematically homogenize moral diversity.
Surprisingly, increased model size doesn't consistently improve cultural
representation fidelity. Our findings challenge the growing use of LLMs as
synthetic populations in social science research and highlight a fundamental
limitation in current AI alignment approaches. Without data-driven alignment
beyond prompting, these systems cannot capture the nuanced, culturally-specific
moral intuitions. Our results call for more grounded alignment objectives and
evaluation metrics to ensure AI systems represent diverse human values rather
than flattening the moral landscape.

</details>


### [208] [Enhancing Chain-of-Thought Reasoning with Critical Representation Fine-tuning](https://arxiv.org/abs/2507.10085)
*Chenxi Huang,Shaotian Yan,Liang Xie,Binbin Lin,Sinan Fan,Yue Xin,Deng Cai,Chen Shen,Jieping Ye*

Main category: cs.CL

TL;DR: CRFT 是一种新的参数高效微调方法，通过优化关键表示来提高复杂推理任务的性能，相比 ReFT 在固定位置表示上进行了改进。


<details>
  <summary>Details</summary>
Motivation: 直接使用 ReFT 方法在复杂推理任务上表现不佳，因为其修改固定位置的表示，其对输出的影响不确定。作者观察到，在复杂推理任务中，存在一些关键的表示，它们整合了先前层的重要信息或调节了后续层的表示，对最终输出有重大影响。

Method: CRFT 通过信息流分析识别关键表示，并在监督学习框架下进行优化，同时保持基础模型冻结。

Result: CRFT 在八个基准测试中得到了验证，涵盖了算术和常识推理，并使用了 LLaMA 和 Mistral 模型系列。该方法在单次学习设置下表现尤为出色，将准确率提高了 16.4%。

Conclusion: Critical Representation Fine-Tuning (CRFT) 通过识别和优化关键表示来提高复杂推理任务的性能，与传统的参数高效微调方法相比，是一种轻量级但强大的替代方案。

Abstract: Representation Fine-tuning (ReFT), a recently proposed Parameter-Efficient
Fine-Tuning (PEFT) method, has attracted widespread attention for significantly
improving parameter efficiency by editing representation space alone. In this
work, we investigate applying ReFT to complex reasoning tasks. However,
directly using the native ReFT method, which modifies fixed representations at
the beginning and end of each layer, yields suboptimal performance, as these
fixed-position representations have uncertain impact on the outputs. We observe
that, in complex reasoning tasks, there often exist certain critical
representations. These representations either integrate significant information
from preceding layers or regulate subsequent layer representations. Through
layer-by-layer propagation, they exert a substantial influence on the final
output. Naturally, fine-tuning these critical representations has the potential
to greatly enhance reasoning performance. Building upon these insights, we
propose Critical Representation Fine-Tuning (CRFT), a novel method that
identifies and optimizes these critical representations through information
flow analysis. CRFT operates within a supervised learning framework,
dynamically optimizing critical representations in a low-rank linear subspace
while freezing the base model. The effectiveness and efficiency of our method
are validated across eight benchmarks for arithmetic and commonsense reasoning,
using LLaMA and Mistral model families. Furthermore, our method also adapts
effectively to few-shot settings, boosting one-shot accuracy by 16.4%. Our work
highlights the untapped potential of representation-level optimization for CoT
reasoning, offering a lightweight yet powerful alternative to traditional PEFT
methods.

</details>


### [209] [Fusing Large Language Models with Temporal Transformers for Time Series Forecasting](https://arxiv.org/abs/2507.10098)
*Chen Su,Yuanhe Tian,Qinyu Liu,Jun Zhang,Yan Song*

Main category: cs.CL

TL;DR: 通过融合LLM的时间序列理解能力和Transformer的时间序列建模能力，提出了一种新的混合模型，用于提高时间序列预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于LLM的时间序列预测方法由于LLM主要擅长处理离散的文本标记和语义模式，而未能很好地处理连续的数值时间序列数据，导致其性能不如直接在时间序列数据上训练的Transformer模型。然而，传统的Transformer模型又难以学习到高层语义模式。因此，本研究旨在弥合这种差距。

Method: 设计了一种新颖的Transformer基础架构，该架构能够互补地利用LLM和Transformer的优点。具体而言，它将LLM在高层语义上的理解能力与时间序列Transformer在时间信息编码上的能力相结合，通过融合这两种模型的表示来获得混合表示，从而同时包含历史时间动态和语义变化模式。

Result: 实验结果表明，该方法在基准数据集上的表现优于现有方法，证明了其有效性。

Conclusion: 本文提出的方法通过结合LLM和Transformer的优势，实现了时间序列预测的性能提升，能够同时捕捉时间动态和语义模式。

Abstract: Recently, large language models (LLMs) have demonstrated powerful
capabilities in performing various tasks and thus are applied by recent studies
to time series forecasting (TSF) tasks, which predict future values with the
given historical time series. Existing LLM-based approaches transfer knowledge
learned from text data to time series prediction using prompting or fine-tuning
strategies. However, LLMs are proficient at reasoning over discrete tokens and
semantic patterns but are not initially designed to model continuous numerical
time series data. The gaps between text and time series data lead LLMs to
achieve inferior performance to a vanilla Transformer model that is directly
trained on TSF data. However, the vanilla Transformers often struggle to learn
high-level semantic patterns. In this paper, we design a novel
Transformer-based architecture that complementarily leverages LLMs and vanilla
Transformers, so as to integrate the high-level semantic representations
learned by LLMs into the temporal information encoded by time series
Transformers, where a hybrid representation is obtained by fusing the
representations from the LLM and the Transformer. The resulting fused
representation contains both historical temporal dynamics and semantic
variation patterns, allowing our model to predict more accurate future values.
Experiments on benchmark datasets demonstrate the effectiveness of the proposed
approach.

</details>


### [210] [Task-Based Flexible Feature Distillation for LLMs](https://arxiv.org/abs/2507.10155)
*Khouloud Saadi,Di Wang*

Main category: cs.CL

TL;DR: 该研究提出了一种无需额外参数即可在不同维度隐藏层之间进行知识蒸馏的新方法，在多种任务上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统的特征知识蒸馏（KD）方法通常要求教师和学生模型具有相同的隐藏层大小，这限制了学生模型的架构灵活性。虽然存在通过训练线性投影仪来对齐特征空间的方法，但这会引入需要从头学习的额外参数，并且在下游任务（尤其是在生成任务中）的性能往往会下降。因此，需要一种新的方法来解决这些问题。

Method: 提出了一种新颖的基于任务的特征蒸馏方法，通过识别教师模型中最具任务相关性的隐藏单元，并直接将其激活值蒸馏给学生模型，实现了教师和学生模型不同隐藏层维度之间的知识迁移，且无需引入额外参数。

Result: 该方法在分类、指令遵循和摘要等多种下游任务上均取得了持续的性能提升，并且在与线性投影基线相比时，性能最高可提升3%。

Conclusion: 该研究提出了一种新颖的基于任务的特征蒸馏方法，能够在教师模型和学生模型具有不同隐藏层维度的情况下进行知识迁移，且无需引入任何额外参数。该方法通过识别教师模型中最具任务相关性的隐藏单元，并直接将其激活值蒸馏给学生模型，能够灵活地集成到其他蒸馏框架中。实验结果表明，在分类、指令遵循和摘要等多种任务上，该方法相比于线性投影基线有高达3%的性能提升。

Abstract: Knowledge Distillation (KD) in general and feature distillation in particular
are promising techniques for reducing the high computational demand of large
language models (LLMs). However, traditional feature KD methods typically
assume that the teacher and the student share the same hidden size, limiting
the flexibility of the student's architecture. A common solution to this
problem involves training a linear projector to align their feature spaces, but
this introduces additional parameters that must be learned from scratch and
often degrades performance on downstream tasks, especially in generative
settings. To address this issue, in this work, we propose a novel task-based
feature distillation method that enables knowledge transfer between teacher and
student models with different hidden layer dimensions, without introducing any
new parameters. Leveraging the insight that only a subset of LLM components
contribute significantly to a specific downstream task, our approach identifies
the most task-relevant hidden units in the teacher and directly distills their
activations to the student. Our method is flexible and easily integrates with
other distillation frameworks. Empirical results show consistent improvements
over prior approaches across diverse tasks, including classification,
instruction-following, and summarization, achieving up to a 3\% performance
gain over the linear projection baseline.

</details>


### [211] [Abusive text transformation using LLMs](https://arxiv.org/abs/2507.10177)
*Rohitash Chandra,Jiyong Choi*

Main category: cs.CL

TL;DR: 本研究使用大型语言模型 (LLM) 将辱骂性文本转换为非辱骂性文本，同时保持其意图。研究评估了 Gemini、GPT-4o、DeepSeek 和 Groq 的性能，发现 Groq 的结果与其他模型存在显著差异，而 GPT-4o 和 DeepSeek-V3 之间存在相似性。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型在分类和转换辱骂性文本为非辱骂性文本方面的有效性。

Method: 本研究旨在使用大型语言模型 (LLM) 将包含仇恨言论和脏话的辱骂性文本（推文和评论）转换为非辱骂性文本，同时保留其意图。研究人员评估了 Gemini、GPT-4o、DeepSeek 和 Groq 这两个最先进的大型语言模型在识别辱骂性文本方面的能力，并要求它们进行转换，生成干净且无不当内容但保持相似情感和语义水平（即保持其信息）的文本。随后，研究人员使用情感分析和语义分析评估了原始数据集和转换后的数据集。

Result: Groq 的结果与其他大型语言模型 (LLM) 存在显著差异，而 GPT-4o 和 DeepSeek-V3 之间则存在相似性。

Conclusion: Groq 的结果与其他大型语言模型 (LLM) 存在显著差异，而 GPT-4o 和 DeepSeek-V3 之间则存在相似性。

Abstract: Although Large Language Models (LLMs) have demonstrated significant
advancements in natural language processing tasks, their effectiveness in the
classification and transformation of abusive text into non-abusive versions
remains an area for exploration. In this study, we aim to use LLMs to transform
abusive text (tweets and reviews) featuring hate speech and swear words into
non-abusive text, while retaining the intent of the text. We evaluate the
performance of two state-of-the-art LLMs, such as Gemini, GPT-4o, DeekSeek and
Groq, on their ability to identify abusive text. We them to transform and
obtain a text that is clean from abusive and inappropriate content but
maintains a similar level of sentiment and semantics, i.e. the transformed text
needs to maintain its message. Afterwards, we evaluate the raw and transformed
datasets with sentiment analysis and semantic analysis. Our results show Groq
provides vastly different results when compared with other LLMs. We have
identified similarities between GPT-4o and DeepSeek-V3.

</details>


### [212] [Absher: A Benchmark for Evaluating Large Language Models Understanding of Saudi Dialects](https://arxiv.org/abs/2507.10216)
*Renad Al-Monef,Hassan Alhuzali,Nora Alturayeif,Ashwag Alasmari*

Main category: cs.CL

TL;DR: 本研究提出了 Absher 基准，用于评估大型语言模型在沙特方言上的表现，结果显示模型在文化和上下文理解方面存在不足，需要更注重方言和文化的训练与评估方法。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在阿拉伯语自然语言处理应用中的作用日益重要，评估它们对区域方言和文化细微差别的理解能力，尤其是在沙特阿拉伯这样语言环境多样化的地区，显得尤为关键。

Method: 本研究引入了一个名为 Absher 的综合基准，其中包含超过 18,000 道多项选择题，涵盖了意义、判断题、填空题、上下文用法、文化解读和地点识别六个类别。这些问题源自沙特阿拉伯不同地区的方言词汇、短语和谚语数据集。研究人员评估了几种先进的大型语言模型，并深入分析了它们的优势和局限性。

Result: 评估结果显示，在理解沙特主要方言方面，不同的大型语言模型表现出显著的性能差异，尤其是在需要文化推理或上下文理解的任务上，模型的能力存在明显不足。

Conclusion: 研究结果表明，在需要文化推理或上下文理解的任务中，大型语言模型在理解沙特方言方面存在明显的性能差距。这凸显了在真实世界阿拉伯语应用中，需要采用注重方言和文化适应性的训练及评估方法来提升大型语言模型的性能。

Abstract: As large language models (LLMs) become increasingly central to Arabic NLP
applications, evaluating their understanding of regional dialects and cultural
nuances is essential, particularly in linguistically diverse settings like
Saudi Arabia. This paper introduces \texttt{Absher}, a comprehensive benchmark
specifically designed to assess LLMs performance across major Saudi dialects.
\texttt{Absher} comprises over 18,000 multiple-choice questions spanning six
distinct categories: Meaning, True/False, Fill-in-the-Blank, Contextual Usage,
Cultural Interpretation, and Location Recognition. These questions are derived
from a curated dataset of dialectal words, phrases, and proverbs sourced from
various regions of Saudi Arabia. We evaluate several state-of-the-art LLMs,
including multilingual and Arabic-specific models. We also provide detailed
insights into their capabilities and limitations. Our results reveal notable
performance gaps, particularly in tasks requiring cultural inference or
contextual understanding. Our findings highlight the urgent need for
dialect-aware training and culturally aligned evaluation methodologies to
improve LLMs performance in real-world Arabic applications.

</details>


### [213] [Grammar-Guided Evolutionary Search for Discrete Prompt Optimisation](https://arxiv.org/abs/2507.10326)
*Muzhaffar Hazman,Minh-Khoi Pham,Shweta Soundararajan,Goncalo Mordido,Leonardo Custode,David Lynch,Giorgio Cruciata,Yucheng Shi,Hongmeng Song,Wang Chao,Pan Yue,Aleksandar Milenovic,Alexandros Agapitos*

Main category: cs.CL

TL;DR: 提出了一种新的演化搜索方法，用于优化复杂任务和小型语言模型的提示。该方法通过语法引导的遗传编程和局部搜索来合成和优化提示程序，并在多项任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前的自动化提示工程方法主要在只需要很少提示模板的任务和非常大且功能强大的语言模型上进行评估。然而，解决需要详细信息包含在提示中的复杂任务会增加需要优化的文本量。此外，较小的模型对提示设计更为敏感。为了应对这些挑战，需要一种新的方法来优化复杂任务的提示。

Method: 提出了一种由两个阶段组成的自动离散提示优化演化搜索方法。第一阶段利用语法引导的遗传编程，通过搜索由语法、基于字典和基于大语言模型的提示编辑函数的组合组成的程序空间来合成创建提示的程序。第二阶段应用局部搜索探索表现最佳程序的邻域，以进一步优化其性能。

Result: 该方法在四个领域特定的挑战性任务上，在三个相对较小的通用语言模型上，优于PromptWizard、OPRO和RL-Prompt三种最先进的提示优化方法。该方法在几乎所有任务-模型组合中都提高了性能，并且仅在少数情况下出现最小的性能下降，而其他方法则出现性能严重下降。

Conclusion: 该方法在三个相对较小的通用语言模型和四个领域特定的挑战性任务上，优于三种最先进的提示优化方法（PromptWizard、OPRO和RL-Prompt）。此外，该方法在几乎所有任务-模型组合中都能提高性能，仅在少数情况下出现最小的性能下降，而基准方法在这些情况下性能会严重下降。

Abstract: Prompt engineering has proven to be a crucial step in leveraging pretrained
large language models (LLMs) in solving various real-world tasks. Numerous
solutions have been proposed that seek to automate prompt engineering by using
the model itself to edit prompts. However, the majority of state-of-the-art
approaches are evaluated on tasks that require minimal prompt templates and on
very large and highly capable LLMs. In contrast, solving complex tasks that
require detailed information to be included in the prompt increases the amount
of text that needs to be optimised. Furthermore, smaller models have been shown
to be more sensitive to prompt design. To address these challenges, we propose
an evolutionary search approach to automated discrete prompt optimisation
consisting of two phases. In the first phase, grammar-guided genetic
programming is invoked to synthesise prompt-creating programmes by searching
the space of programmes populated by function compositions of syntactic,
dictionary-based and LLM-based prompt-editing functions. In the second phase,
local search is applied to explore the neighbourhoods of best-performing
programmes in an attempt to further fine-tune their performance. Our approach
outperforms three state-of-the-art prompt optimisation approaches,
PromptWizard, OPRO, and RL-Prompt, on three relatively small general-purpose
LLMs in four domain-specific challenging tasks. We also illustrate several
examples where these benchmark methods suffer relatively severe performance
degradation, while our approach improves performance in almost all task-model
combinations, only incurring minimal degradation when it does not.

</details>


### [214] [Bridging Robustness and Generalization Against Word Substitution Attacks in NLP via the Growth Bound Matrix Approach](https://arxiv.org/abs/2507.10330)
*Mohammed Bouri,Adnane Saoud*

Main category: cs.CL

TL;DR: 本研究提出了一种名为增长界矩阵（GBM）的新型正则化技术，旨在增强LSTM、S4和CNN等NLP模型抵御同义词替换等对抗性攻击的能力。实验结果表明，该方法显著提高了模型的鲁棒性和泛化能力，特别是在S4模型的鲁棒性分析方面取得了初步成果。


<details>
  <summary>Details</summary>
Motivation: 现有NLP模型容易受到对抗性攻击，特别是同义词替换。然而，循环神经网络（RNN）和状态空间模型（SSM）（如S4）等现代架构的鲁棒性研究不足，它们由于序列处理和复杂的参数动态而带来独特的挑战。

Method: 提出了一种基于增长界矩阵（GBM）的正则化技术，用于计算LSTM、S4和CNN架构的GBM，以提高NLP模型的鲁棒性。

Result: 在多个架构和基准数据集上的广泛实验表明，该方法比现有基线方法将对抗性鲁棒性提高了高达8.8%，并且在对抗防御方面优于多种最先进的方法。

Conclusion: 该研究引入了一种基于增长界矩阵（GBM）的新型正则化技术，以提高NLP模型（特别是LSTM、S4和CNN）在面对同义词替换等对抗性攻击时的鲁棒性。实验证明，该方法在对抗性鲁棒性方面比现有基线方法提高了8.8%，并在对抗防御方面优于多种最先进方法。

Abstract: Despite advancements in Natural Language Processing (NLP), models remain
vulnerable to adversarial attacks, such as synonym substitutions. While prior
work has focused on improving robustness for feed-forward and convolutional
architectures, the robustness of recurrent networks and modern state space
models (SSMs), such as S4, remains understudied. These architectures pose
unique challenges due to their sequential processing and complex parameter
dynamics. In this paper, we introduce a novel regularization technique based on
Growth Bound Matrices (GBM) to improve NLP model robustness by reducing the
impact of input perturbations on model outputs. We focus on computing the GBM
for three architectures: Long Short-Term Memory (LSTM), State Space models
(S4), and Convolutional Neural Networks (CNN). Our method aims to (1) enhance
resilience against word substitution attacks, (2) improve generalization on
clean text, and (3) providing the first systematic analysis of SSM (S4)
robustness. Extensive experiments across multiple architectures and benchmark
datasets demonstrate that our method improves adversarial robustness by up to
8.8% over existing baselines. These results highlight the effectiveness of our
approach, outperforming several state-of-the-art methods in adversarial
defense. Codes are available at https://github.com/BouriMohammed/GBM

</details>


### [215] [Using AI to replicate human experimental results: a motion study](https://arxiv.org/abs/2507.10342)
*Rosa Illan Castillo,Javier Valenzuela*

Main category: cs.CL

TL;DR: LLMs在语言学研究中，尤其是在分析情感意义方面，与人类的判断表现出高度一致性，可作为可靠的协作者。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探索大型语言模型（LLMs）在语言学研究中作为可靠分析工具的潜力，特别关注了涉及运动方式动词的时间表达中情感意义的出现。

Method: 本研究通过四项心理语言学研究，首先对人类参与者进行实验，然后使用大型语言模型（LLM）复制相同的任务，以探讨LLMs在时间表达中情感意义的出现作为分析工具的潜力。

Result: 研究结果显示，在所有研究中，人类和AI的反应表现出惊人的一致性。统计分析（例如，斯皮尔曼等级相关系数 rho = 0.73-0.96）表明，在评分模式和分类选择方面都存在很强的相关性。虽然在某些情况下观察到了一些细微的差异，但这些差异并未改变整体的解释结果。

Conclusion: 本研究的结果表明，大型语言模型（LLMs）可以作为语言学研究中可靠的分析工具，能够增强传统的人类实验，实现更大规模的研究，并且不损害解释的有效性。LLMs可以作为语言学探究中可信且信息丰富的协作者。

Abstract: This paper explores the potential of large language models (LLMs) as reliable
analytical tools in linguistic research, focusing on the emergence of affective
meanings in temporal expressions involving manner-of-motion verbs. While LLMs
like GPT-4 have shown promise across a range of tasks, their ability to
replicate nuanced human judgements remains under scrutiny. We conducted four
psycholinguistic studies (on emergent meanings, valence shifts, verb choice in
emotional contexts, and sentence-emoji associations) first with human
participants and then replicated the same tasks using an LLM. Results across
all studies show a striking convergence between human and AI responses, with
statistical analyses (e.g., Spearman's rho = .73-.96) indicating strong
correlations in both rating patterns and categorical choices. While minor
divergences were observed in some cases, these did not alter the overall
interpretative outcomes. These findings offer compelling evidence that LLMs can
augment traditional human-based experimentation, enabling broader-scale studies
without compromising interpretative validity. This convergence not only
strengthens the empirical foundation of prior human-based findings but also
opens possibilities for hypothesis generation and data expansion through AI.
Ultimately, our study supports the use of LLMs as credible and informative
collaborators in linguistic inquiry.

</details>


### [216] [Meanings are like Onions: a Layered Approach to Metaphor Processing](https://arxiv.org/abs/2507.10354)
*Silvia Cappa,Anna Sofia Lippolis,Stefano Zoia*

Main category: cs.CL

TL;DR: 本文提出了一个三层隐喻处理模型（内容分析、概念融合、语用意图），以实现更深层次、更具上下文敏感性的计算隐喻解释。


<details>
  <summary>Details</summary>
Motivation: 隐喻意义不是概念之间的平面映射，而是一种复杂的认知现象，它整合了多个解释层次。因此，有必要提出一个能够更丰富、更符合认知地解释隐喻的计算系统模型。

Method: 本文提出了一个分层的隐喻处理模型，将意义视为一个多层结构，包括（1）内容分析、（2）概念融合和（3）语用意图。在此框架中，隐喻首先通过基本概念元素进行注释，然后对概念组合进行建模以链接组件和新兴意义，最后引入语用词汇来捕获说话者意图、沟通功能和上下文效应。

Result: 通过将这些层次统一到单一的正式框架中，该模型实现了计算系统对隐喻的更深层次、更具上下文敏感性的推理。

Conclusion: 该模型为超越表面联想、实现更深层次、更具上下文敏感性的推理铺平了道路，能够计算表示比表面联想更深层次、更具上下文敏感性的隐喻意义。

Abstract: Metaphorical meaning is not a flat mapping between concepts, but a complex
cognitive phenomenon that integrates multiple levels of interpretation. In this
paper, we propose a stratified model of metaphor processing that treats meaning
as an onion: a multi-layered structure comprising (1) content analysis, (2)
conceptual blending, and (3) pragmatic intentionality. This three-dimensional
framework allows for a richer and more cognitively grounded approach to
metaphor interpretation in computational systems. At the first level, metaphors
are annotated through basic conceptual elements. At the second level, we model
conceptual combinations, linking components to emergent meanings. Finally, at
the third level, we introduce a pragmatic vocabulary to capture speaker intent,
communicative function, and contextual effects, aligning metaphor understanding
with pragmatic theories. By unifying these layers into a single formal
framework, our model lays the groundwork for computational methods capable of
representing metaphorical meaning beyond surface associations, toward deeper,
more context-sensitive reasoning.

</details>


### [217] [From Sequence to Structure: Uncovering Substructure Reasoning in Transformers](https://arxiv.org/abs/2507.10435)
*Xinnan Dai,Kai Yang,Jay Revolinsky,Kai Guo,Aoran Wang,Bohang Zhang,Jiliang Tang*

Main category: cs.CL

TL;DR: Transformer可以像人类一样“以子结构思考”来理解图数据。ISF理论解释了Transformer如何通过其多层结构识别和提取图中的子结构，即使图结构是以文本形式给出的。研究表明，这种能力不仅限于特定类型的图，而且在不同层之间保持一致，为理解大型语言模型在处理非序列数据（如图数据）时的能力提供了新的视角。


<details>
  <summary>Details</summary>
Motivation: 探讨Transformer架构（特别是decoder-only Transformer）如何在图结构数据嵌入文本描述时，仍能有效理解并解决图推理任务。

Method: 通过子结构提取任务来解析Transformer的内部机制，并分析输入查询的影响。提出ISF（Induced Substructure Filtration）理论和实证分析，解释Transformer如何识别子结构，并将其应用于LLM以验证其跨层一致性。在此基础上，探索Transformer处理不同类型图的能力，并提出“以子结构思考”的概念来提取复杂模式，尤其是在属性图（如分子图）上进行子结构提取。

Result: 提出并验证了ISF（Induced Substructure Filtration）视角，解释了Transformer如何在多层结构中识别子结构，并展示了LLM中存在一致的内部动态。证明了Transformer可以从属性图中提取子结构，并提出“以子结构思考”的概念以提高效率。

Conclusion: 本研究通过ISF视角揭示了Transformer在处理图结构数据时的内部机制，特别是其在子结构提取方面的能力，并展示了这种能力在不同类型图上的泛化性，为理解基于序列的模型如何处理图数据提供了新的见解。

Abstract: Recent studies suggest that large language models (LLMs) possess the
capability to solve graph reasoning tasks. Notably, even when graph structures
are embedded within textual descriptions, LLMs can still effectively answer
related questions. This raises a fundamental question: How can a decoder-only
Transformer architecture understand underlying graph structures? To address
this, we start with the substructure extraction task, interpreting the inner
mechanisms inside the transformers and analyzing the impact of the input
queries. Specifically, through both empirical results and theoretical analysis,
we present Induced Substructure Filtration (ISF), a perspective that captures
the substructure identification in the multi-layer transformers. We further
validate the ISF process in LLMs, revealing consistent internal dynamics across
layers. Building on these insights, we explore the broader capabilities of
Transformers in handling diverse graph types. Specifically, we introduce the
concept of thinking in substructures to efficiently extract complex composite
patterns, and demonstrate that decoder-only Transformers can successfully
extract substructures from attributed graphs, such as molecular graphs.
Together, our findings offer a new insight on how sequence-based Transformers
perform the substructure extraction task over graph data.

</details>


### [218] [Referential ambiguity and clarification requests: comparing human and LLM behaviour](https://arxiv.org/abs/2507.10445)
*Chris Madge,Matthew Purver,Massimo Poesio*

Main category: cs.CL

TL;DR: This paper analyzes LLMs' ability to ask clarification questions in task-oriented dialogues, comparing them to humans. A new corpus was created using the Minecraft Dialogue Corpus. Results show humans ask clarifications for task uncertainty while LLMs ask for referential ambiguity. LLM reasoning improves question frequency and relevance.


<details>
  <summary>Details</summary>
Motivation: This research aims to examine the capability of Large Language Models (LLMs) in seeking clarification questions within task-oriented dialogues, specifically in an asynchronous instruction-giver/instruction-follower setup. It also seeks to understand how LLMs' clarification-seeking behavior compares to that of humans when faced with ambiguity.

Method: The study utilizes a newly created corpus that integrates two existing annotations of the Minecraft Dialogue Corpus. This corpus provides information on reference, ambiguity, and SDRT, including clarifications, to facilitate experiments on clarifications and their relationship with ambiguity. The researchers compare LLM-generated clarification questions with human-generated ones to analyze their behavior in ambiguous situations.

Result: The study found that the link between ambiguity and humans asking clarification questions is weak. Humans rarely ask clarification questions for referential ambiguity but frequently do so for task-based uncertainty. LLMs, on the other hand, ask more clarification questions for referential ambiguity and fewer for task uncertainty. The research also indicated that LLMs' reasoning abilities positively impact the frequency and relevance of their clarification questions.

Conclusion: LLMs' ability to ask clarification questions is related to their reasoning capabilities. While humans tend to ask clarification questions for task uncertainty, LLMs focus more on referential ambiguity. There is a weak link between ambiguity and human clarification questions, and low correlation between human and LLM clarification questions.

Abstract: In this work we examine LLMs' ability to ask clarification questions in
task-oriented dialogues that follow the asynchronous
instruction-giver/instruction-follower format. We present a new corpus that
combines two existing annotations of the Minecraft Dialogue Corpus -- one for
reference and ambiguity in reference, and one for SDRT including clarifications
-- into a single common format providing the necessary information to
experiment with clarifications and their relation to ambiguity. With this
corpus we compare LLM actions with original human-generated clarification
questions, examining how both humans and LLMs act in the case of ambiguity. We
find that there is only a weak link between ambiguity and humans producing
clarification questions in these dialogues, and low correlation between humans
and LLMs. Humans hardly ever produce clarification questions for referential
ambiguity, but often do so for task-based uncertainty. Conversely, LLMs produce
more clarification questions for referential ambiguity, but less so for task
uncertainty. We question if LLMs' ability to ask clarification questions is
predicated on their recent ability to simulate reasoning, and test this with
different reasoning approaches, finding that reasoning does appear to increase
question frequency and relevancy.

</details>


### [219] [From BERT to Qwen: Hate Detection across architectures](https://arxiv.org/abs/2507.10468)
*Ariadna Mon,Saúl Fenollosa,Jon Lecumberri*

Main category: cs.CL

TL;DR: Large language models (LLMs) show promise for hate-speech detection, but their effectiveness compared to traditional models needs more research.


<details>
  <summary>Details</summary>
Motivation: To verify if ultra-large autoregressive LLMs improve practical hate-speech detection on real-world text compared to bidirectional transformer encoders.

Method: Benchmarking classic encoders and LLMs on curated corpora for hate-speech detection.

Result: The study benchmarks both model families on curated corpora for hate-speech detection.

Conclusion: future work should explore larger models and further optimization

Abstract: Online platforms struggle to curb hate speech without over-censoring
legitimate discourse. Early bidirectional transformer encoders made big
strides, but the arrival of ultra-large autoregressive LLMs promises deeper
context-awareness. Whether this extra scale actually improves practical
hate-speech detection on real-world text remains unverified. Our study puts
this question to the test by benchmarking both model families, classic encoders
and next-generation LLMs, on curated corpora of online interactions for
hate-speech detection (Hate or No Hate).

</details>


### [220] [MLAR: Multi-layer Large Language Model-based Robotic Process Automation Applicant Tracking](https://arxiv.org/abs/2507.10472)
*Mohamed T. Younes,Omar Walid,Mai Hassan,Ali Hamdi*

Main category: cs.CL

TL;DR: MLAR是一个基于LLM的新型RPA框架，可高效自动化招聘流程，在处理大量简历时比现有RPA平台更快。


<details>
  <summary>Details</summary>
Motivation: 传统的招聘流程在简历筛选和候选人入围方面常常遇到时间与资源的瓶颈。MLAR旨在解决这些挑战。

Method: 该方法采用一种名为MLAR的新型机器人流程自动化（RPA）框架，并利用大型语言模型（LLM）来处理三个层面：从职位发布中提取关键特征，解析简历以识别教育、经验和技能，以及进行相似性匹配。通过先进的语义算法进行匹配，以高效识别最佳候选人。

Result: MLAR在处理高数量简历的任务中表现优于领先的RPA平台（如UiPath和Automation Anywhere）。在处理2400份简历时，MLAR的平均处理时间为每份5.4秒，与Automation Anywhere相比，处理时间减少了约16.9%，与UiPath相比，减少了17.1%。

Conclusion: MLAR通过提供高效、准确和可扩展的解决方案，有潜力彻底改变招聘工作流程。

Abstract: This paper introduces an innovative Applicant Tracking System (ATS) enhanced
by a novel Robotic process automation (RPA) framework or as further referred to
as MLAR. Traditional recruitment processes often encounter bottlenecks in
resume screening and candidate shortlisting due to time and resource
constraints. MLAR addresses these challenges employing Large Language Models
(LLMs) in three distinct layers: extracting key characteristics from job
postings in the first layer, parsing applicant resume to identify education,
experience, skills in the second layer, and similarity matching in the third
layer. These features are then matched through advanced semantic algorithms to
identify the best candidates efficiently. Our approach integrates seamlessly
into existing RPA pipelines, automating resume parsing, job matching, and
candidate notifications. Extensive performance benchmarking shows that MLAR
outperforms the leading RPA platforms, including UiPath and Automation
Anywhere, in high-volume resume-processing tasks. When processing 2,400
resumes, MLAR achieved an average processing time of 5.4 seconds per resume,
reducing processing time by approximately 16.9% compared to Automation Anywhere
and 17.1% compared to UiPath. These results highlight the potential of MLAR to
transform recruitment workflows by providing an efficient, accurate, and
scalable solution tailored to modern hiring needs.

</details>


### [221] [Can You Detect the Difference?](https://arxiv.org/abs/2507.10475)
*İsmail Tarım,Aytuğ Onan*

Main category: cs.CL

TL;DR: LLaDA（扩散模型）生成的文本在困惑度和突发性上与人类文本非常相似，使得现有检测方法容易出错。单一指标无法有效区分AI生成文本和人类写作，需要开发新的检测方法。


<details>
  <summary>Details</summary>
Motivation: 评估AI生成文本检测的可靠性，特别是扩散模型生成文本的检测能力，因为现有风格计量指标在扩散模型上的有效性尚不明确。

Method: 使用2000个样本对LLaDA（扩散模型）和LLaMA（自回归模型）生成的文本进行系统性比较。

Result: LLaDA在困惑度和突发性方面与人类文本高度相似，导致基于自回归模型设计的检测器产生高假阴性率。LLaMA的困惑度较低，但词汇保真度也降低了。

Conclusion: 单一指标无法区分扩散模型生成文本和人类写作。需要开发能够识别扩散模型生成文本的检测器，并提出混合模型、特定于扩散模型风格的签名和鲁棒水印等方向。

Abstract: The rapid advancement of large language models (LLMs) has raised concerns
about reliably detecting AI-generated text. Stylometric metrics work well on
autoregressive (AR) outputs, but their effectiveness on diffusion-based models
is unknown. We present the first systematic comparison of diffusion-generated
text (LLaDA) and AR-generated text (LLaMA) using 2 000 samples. Perplexity,
burstiness, lexical diversity, readability, and BLEU/ROUGE scores show that
LLaDA closely mimics human text in perplexity and burstiness, yielding high
false-negative rates for AR-oriented detectors. LLaMA shows much lower
perplexity but reduced lexical fidelity. Relying on any single metric fails to
separate diffusion outputs from human writing. We highlight the need for
diffusion-aware detectors and outline directions such as hybrid models,
diffusion-specific stylometric signatures, and robust watermarking.

</details>


### [222] [Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation](https://arxiv.org/abs/2507.10524)
*Sangmin Bae,Yujin Kim,Reza Bayat,Sungnyun Kim,Jiyoun Ha,Tal Schuster,Adam Fisch,Hrayr Harutyunyan,Ziwei Ji,Aaron Courville,Se-Young Yun*

Main category: cs.CL

TL;DR: MoR框架通过参数共享和自适应计算结合，在降低大型语言模型成本的同时提高了模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决扩展语言模型带来的高昂计算和内存成本问题，同时实现参数共享和自适应计算。

Method: MoR通过重用层堆栈和轻量级路由器实现参数效率和自适应计算，并提出了一种KV共享变体以减少预填充延迟和内存占用。

Result: 在1.35亿至17亿参数的模型规模上，MoR在相同的训练FLOPs和较小的模型尺寸下，显著降低了验证困惑度，提高了少样本准确率，并与现有的递归基线相比提高了吞吐量。

Conclusion: MoR是一种有效的实现大型模型质量而无需承担大型模型成本的途径。

Abstract: Scaling language models unlocks impressive capabilities, but the accompanying
computational and memory demands make both training and deployment expensive.
Existing efficiency efforts typically target either parameter sharing or
adaptive computation, leaving open the question of how to attain both
simultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework
that combines the two axes of efficiency inside a single Recursive Transformer.
MoR reuses a shared stack of layers across recursion steps to achieve parameter
efficiency, while lightweight routers enable adaptive token-level thinking by
dynamically assigning different recursion depths to individual tokens. This
allows MoR to focus quadratic attention computation only among tokens still
active at a given recursion depth, further improving memory access efficiency
by selectively caching only their key-value pairs. Beyond these core
mechanisms, we also propose a KV sharing variant that reuses KV pairs from the
first recursion, specifically designed to decrease prefill latency and memory
footprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms
a new Pareto frontier: at equal training FLOPs and smaller model sizes, it
significantly lowers validation perplexity and improves few-shot accuracy,
while delivering higher throughput compared with vanilla and existing recursive
baselines. These gains demonstrate that MoR is an effective path towards
large-model quality without incurring large-model cost.

</details>


### [223] [CodeJudgeBench: Benchmarking LLM-as-a-Judge for Coding Tasks](https://arxiv.org/abs/2507.10535)
*Hongchao Jiang,Yiming Chen,Yushi Cao,Hung-yi Lee,Robby T. Tan*

Main category: cs.CL

TL;DR: LLM-as-a-Judge在代码评估中存在随机性和不一致性问题，但思考模型优于非思考模型，成对比较提示优于标量评分。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM-as-a-Judge范式得到广泛应用，但在编程场景中的有效性尚未得到充分探索，因为缺乏专门的基准。

Method: 提出CodeJudgeBench基准，包含代码生成、代码修复和单元测试生成三个关键编程任务，并对26个LLM-as-a-Judge模型进行了全面的基准测试，研究了提示策略。

Result: 在CodeJudgeBench基准上，近期思考模型显著优于非思考模型，其中Qwen3-8B等小型思考模型表现优于规模达70B的专业训练模型。然而，所有模型在判断代码任务时都表现出显著的随机性，响应呈现顺序的改变会显著影响准确性。

Conclusion: LLM-as-a-Judge模型在代码评判任务中表现出显著的随机性，包括对响应顺序的敏感性以及在评判不同LLM生成的代码和单元测试时的性能差异。研究表明，成对比较的提示策略优于标量评分，并且保留LLM完整响应中的注释和推理可以提高评估性能。

Abstract: Large Language Models (LLMs) have significantly advanced the state-of-the-art
in various coding tasks. Beyond directly answering user queries, LLMs can also
serve as judges, assessing and comparing the quality of responses generated by
other models. Such an evaluation capability is crucial both for benchmarking
different LLMs and for improving response quality through response ranking.
However, despite the growing adoption of the LLM-as-a-Judge paradigm, its
effectiveness in coding scenarios remains underexplored due to the absence of
dedicated benchmarks. To address this gap, we introduce CodeJudgeBench, a
benchmark explicitly designed to evaluate the performance of LLM-as-a-Judge
models across three critical coding tasks: code generation, code repair, and
unit test generation. Through comprehensive benchmarking of 26 LLM-as-a-Judge
models, we find that recent thinking models significantly outperform
non-thinking models on our carefully designed code judging tasks. Notably, even
relatively small thinking models, such as Qwen3-8B, can outperform specially
trained LLM-as-a-Judge models up to 70B in size. Nevertheless, all models still
exhibit significant randomness in their judgment of coding tasks. For pairwise
judging tasks, simply changing the order in which responses are presented can
substantially impact accuracy. In addition, when judging code and unit tests
written by different LLMs, LLM-as-a-Judge models also show variance in
performance. This sensitivity raises concerns about the reliability and
consistency of LLM-as-a-Judge in coding scenarios. Lastly, we study optimal
prompting strategies for LLM-as-a-Judge. We find that using pair-wise
comparison outperforms scalar point-wise judging. Furthermore, retaining
comments and reasoning in the full, unprocessed LLM response leads to improved
judge performance.

</details>


### [224] [REST: Stress Testing Large Reasoning Models by Asking Multiple Problems at Once](https://arxiv.org/abs/2507.10541)
*Zhuoshi Pan,Qizhi Pei,Yu Li,Qiyao Sun,Zinan Tang,H. Vicky Zhao,Conghui He,Lijun Wu*

Main category: cs.CL

TL;DR: REST是一个新的评估框架，通过同时向大型推理模型呈现多个问题来对其进行压力测试，以克服现有评估方法的局限性。该框架揭示了即使是最先进的模型在多任务处理能力和对干扰的抵抗力方面也存在不足，并发现“过度思考”是导致性能下降的一个关键因素。


<details>
  <summary>Details</summary>
Motivation: 现有的大型推理模型（LRMs）评估方法受限于孤立的问题解决范式，容易受到数据污染的影响，并且无法评估模型在多上下文压力下的表现，而这在现实世界部署中是关键要求。因此，需要一个能够同时处理多个问题并评估模型在压力下多任务处理能力的框架。

Method: REST（Reasoning Evaluation through Simultaneous Testing）是一个压力测试框架，它同时将多个问题暴露给大型推理模型（LRMs）。该框架评估了上下文优先级分配、跨问题干扰抵抗和动态认知负荷管理等能力。

Result: 研究发现，即使是像DeepSeek-R1这样最先进的模型，在压力测试下性能也会显著下降。REST比现有的基准测试具有更强的区分能力，揭示了在单问题评估中表现相似的模型之间存在的明显性能差异。此外，“过度思考陷阱”是导致性能下降的一个关键因素，而采用“long2short”技术训练的模型在REST测试中比标准训练的模型表现更好。

Conclusion: REST是一个具有成本效益、面向未来的评估范例，它能更好地反映现实世界的推理需求，同时减少对持续人工标注的依赖。

Abstract: Recent Large Reasoning Models (LRMs) have achieved remarkable progress on
task-specific benchmarks, yet their evaluation methods remain constrained by
isolated problem-solving paradigms. Existing benchmarks predominantly assess
single-question reasoning through sequential testing, resulting critical
limitations: (1) vulnerability to data contamination and less challenging
(e.g., DeepSeek-R1 achieves 97.0% on MATH500), forcing costly and perpetual
creation of new questions with large human efforts, (2) failure to evaluate
models under multi-context pressure, a key requirement for real-world
deployment. To bridge this gap, we present REST (Reasoning Evaluation through
Simultaneous Testing), a stress-testing framework that concurrently exposes
LRMs to multiple problems simultaneously. Beyond basic reasoning, REST
specifically evaluates several under-tested capabilities: contextual priority
allocation, cross-problem interference resistance, and dynamic cognitive load
management. Our evaluation reveals several striking findings: Even
state-of-the-art (SOTA) models like DeepSeek-R1 exhibit substantial performance
degradation under stress testing. Crucially, REST demonstrates stronger
discriminative power than existing benchmarks, revealing pronounced performance
differences among models that exhibit similar, near-ceiling performance under
single-question evaluations. Some key mechanistic insights emerge from our
analysis: (1) the "overthinking trap" is a critical factor contributing to the
performance degradation; (2) the models trained with "long2short" technique
preserve more accuracy of their single-problem performance under REST,
outperforming standard-trained counterparts. These results establish REST as a
cost-efficient, future-proof evaluation paradigm that better reflects
real-world reasoning demands while reducing reliance on continuous human
annotation.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [225] [A new time-stepping strategy and boundary treatment to improve recent 2d traffic model](https://arxiv.org/abs/2507.10205)
*Friedemann Kemm*

Main category: eess.SY

TL;DR: 通过改进的计算方法降低了二维交通流模型的计算成本。


<details>
  <summary>Details</summary>
Motivation: 为了改进一个最近发表的二维交通流模型并降低大规模、粗网格模拟的计算成本。

Method: 提出了一种计算时间步限制和子循环的方法。

Result: 通过时间步限制和子循环的引入，在模拟整个区域时，计算成本大大降低。

Conclusion: 该模型可以通过计算时间步限制和子循环来进一步改进，从而降低计算成本。

Abstract: We show how a recently published 2d model for traffic flow can be further
improved. Besides other improvements and simplifications, we present not only a
method to compute the necessary time step restrictions, but also a subcycling
for the inflow and outflow. This drastically reduces computational cost on
large domains with coarse grids, i.\,e.\ for simulations of a whole region
instead of a small part of a city or town.

</details>


### [226] [Counterfactual optimization for fault prevention in complex wind energy systems](https://arxiv.org/abs/2507.08849)
*Emilio Carrizosa,Martina Fischetti,Roshell Haaker,Juan Miguel Morales*

Main category: eess.SY

TL;DR: 提出一种反事实方法来优化复杂能源系统的控制策略，以最小化干扰并恢复安全状态，已在海上风力涡轮机数据上验证并实现显著的成本节约。


<details>
  <summary>Details</summary>
Motivation: 旨在识别最优控制策略，以最小的干扰将系统恢复到安全状态，超越了单纯的异常检测。

Method: 利用一个数学模型来寻找最优的反事实解决方案，同时遵守特定于系统的约束。

Result: 该方法在实际的工业数据上进行了测试，表明其能够轻松适应用户偏好，并为典型的风电场每年带来约300万欧元的节约。

Conclusion: 本研究提出了一种将机器学习中的反事实分析应用于复杂能源系统（如海上风力涡轮机油浸式变压器）以优化控制策略的方法，旨在以最小的干扰将系统恢复到安全状态。

Abstract: Machine Learning models are increasingly used in businesses to detect faults
and anomalies in complex systems. In this work, we take this approach a step
further: beyond merely detecting anomalies, we aim to identify the optimal
control strategy that restores the system to a safe state with minimal
disruption. We frame this challenge as a counterfactual problem: given a
Machine Learning model that classifies system states as either good or
anomalous, our goal is to determine the minimal adjustment to the system's
control variables (i.e., its current status) that is necessary to return it to
the good state. To achieve this, we leverage a mathematical model that finds
the optimal counterfactual solution while respecting system specific
constraints. Notably, most counterfactual analysis in the literature focuses on
individual cases where a person seeks to alter their status relative to a
decision made by a classifier, such as for loan approval or medical diagnosis.
Our work addresses a fundamentally different challenge: optimizing
counterfactuals for a complex energy system, specifically an offshore wind
turbine oil type transformer. This application not only advances counterfactual
optimization in a new domain but also opens avenues for broader research in
this area. Our tests on real world data provided by our industrial partner show
that our methodology easily adapts to user preferences and brings savings in
the order of 3 million euros per year in a typical farm.

</details>


### [227] [Efficient Discovery of Actual Causality with Uncertainty](https://arxiv.org/abs/2507.09000)
*Arshia Rafieioskouei,Kenneth Rogale,Borzoo Bonakdarpour*

Main category: eess.SY

TL;DR: 本研究提出了一种新颖的基于 SMT 和抽象-细化的方法，用于在存在不确定性的系统中识别事件的概率实际原因，并通过三个案例研究证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: 在存在噪声和不确定性的真实世界系统中，识别工程系统事件的实际原因是一个基本挑战。

Method: 本研究将概率实际因果推理形式化为 SMT 问题，并采用抽象-细化技术来解决可扩展性问题。

Result: 通过在 Mountain Car 问题、Lunar Lander 基准和 F-16 自动驾驶模拟器的 MPC 控制器中识别安全违规的概率原因，证明了该方法的有效性。

Conclusion: 本研究提出了一种新的方法，用于在存在不确定性的系统中形式化地推理事件的因果效应，该方法采用了 Fenton-Glynn 的概率实际因果概念，并将发现概率实际原因制定为 SMT 问题，并通过抽象-细化技术解决了可扩展性挑战。

Abstract: Identifying the actual cause of events in engineered systems is a fundamental
challenge in system analysis. Finding such causes becomes more challenging in
the presence of noise and uncertainty in real-world systems. In this paper, we
adopt the notion of probabilistic actual causality by Fenton-Glynn, which is a
probabilistic extension of Halpern and Pearl's actual causality, and propose a
novel method to formally reason about causal effect of events in systems
subject to uncertainty. We (1) formulate the discovery of probabilistic actual
causes in computing systems as an SMT problem, and (2) address the scalability
challenges by introducing an abstraction-refinement technique that
significantly improves efficiency. We demonstrate the effectiveness of our
approach through three case studies, identifying probabilistic causes of safety
violations in (1) the Mountain Car problem, (2) the Lunar Lander benchmark, and
(3) MPC controller for an F-16 autopilot simulator.

</details>


### [228] [Modelling and Control of a Buck Converter Using State-Space Averaging and Classical Feedback Techniques](https://arxiv.org/abs/2507.09115)
*Sampson E. Nwachukwu*

Main category: eess.SY

TL;DR: 本研究使用状态空间平均技术对DC-DC降压变换器进行了建模、控制设计和性能分析，并使用PI控制器进行了优化，验证了其稳定性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 降压变换器在现代电力电子技术中至关重要，用于调节可再生能源和电动汽车系统中的直流电压。

Method: 本研究提出了一种使用状态空间平均技术的DC-DC降压变换器的建模、控制设计和性能分析方法。首先介绍了降压变换器基本操作和闭环控制系统的必要性。推导了状态空间平均模型以简化非线性开关动态，从而实现更有效的分析和控制器设计。得到了占空比到输出电压的小信号传递函数以支持控制开发。此外，还探讨了基于频域方法设计的比例-积分（PI）控制器。

Result: 通过伯德图、阶跃响应和性能指标评估了PI控制器，揭示了过冲、调节时间和稳态误差之间的权衡。受控降压变换器的完整仿真验证了其在宽输入电压变化下维持稳定输出电压的能力。

Conclusion: 研究结果验证了状态空间平均法在控制设计中的有效性，并强调了反馈系统在电力电子变换器中的鲁棒性。

Abstract: This study presents the modeling, control design, and performance analysis of
a DC-DC buck converter using state-space averaging techniques. Buck converters
are essential in modern power electronics for regulating DC voltages in
renewable energy and electric vehicle systems. The paper first introduces the
basic operation of buck converters and emphasizes the need for voltage
regulation through closed-loop control systems. A state-space averaged model is
derived to simplify the nonlinear switched dynamics, enabling a more effective
analysis and controller design. The small-signal transfer function from the
duty cycle to the output voltage is obtained to support control development. In
addition, the Proportional-Integral (PI) control based on the frequency-domain
method was explored. The PI controller was tuned to achieve various phase
margins and is evaluated through Bode plots, step responses, and performance
metrics, revealing trade-offs between overshoot, settling time, and
steady-state error. A complete simulation of the controlled buck converter
verifies its ability to maintain a stable output voltage across wide input
voltage variations. The results validate the effectiveness of state-space
averaging in control design and highlight the robustness of feedback systems in
power electronic converters.

</details>


### [229] [Integrating Planning and Predictive Control Using the Path Feasibility Governor](https://arxiv.org/abs/2507.09134)
*Shu Zhang,James Y. Z. Liu,Dominic Liao-McPherson*

Main category: eess.SY

TL;DR: 提出了一种名为路径可行性控制器（PathFG）的框架，用于整合路径规划与非线性模型预测控制（MPC），以解决自主系统的运动规划问题。该框架通过操纵MPC的参考信号，在确保约束满足、稳定性和递归可行性的同时，提高了计算效率和可靠性，并证明了其安全性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 在非凸环境中生成满足动力学要求且无碰撞的轨迹是自主系统的基本运动规划问题。将问题分解为路径规划和路径跟踪可以提高其可处理性，但需要在理论上可靠且计算高效地整合这两个组件仍然是一个挑战。

Method: 提出了一种名为路径可行性控制器（PathFG）的框架，用于整合路径规划器与非线性模型预测控制（MPC）。PathFG通过操纵传递给MPC控制器的参考信号，引导其沿路径行进，同时确保约束满足、稳定性和递归可行性。

Result: 证明了该方法具有安全性、渐近稳定性和显著扩大的吸引域。

Conclusion: 该框架在保证约束满足、稳定性和递归可行性的同时，将路径规划与非线性模型预测控制（MPC）相结合，并通过模拟的四旋翼飞行器在杂乱环境中的导航案例研究，验证了其实时性能。

Abstract: The motion planning problem of generating dynamically feasible,
collision-free trajectories in non-convex environments is a fundamental
challenge for autonomous systems. Decomposing the problem into path planning
and path tracking improves tractability, but integrating these components in a
theoretically sound and computationally efficient manner is challenging. We
propose the Path Feasibility Governor (PathFG), a framework for integrating
path planners with nonlinear Model Predictive Control (MPC). The PathFG
manipulates the reference passed to the MPC controller, guiding it along a path
while ensuring constraint satisfaction, stability, and recursive feasibility.
The PathFG is modular, compatible with replanning, and improves computational
efficiency and reliability by reducing the need for long prediction horizons.
We prove safety and asymptotic stability with a significantly expanded region
of attraction, and validate its real-time performance through a simulated case
study of quadrotor navigation in a cluttered environment.

</details>


### [230] [Vertex-Guided Redundant Constraints Identification for Unit Commitment](https://arxiv.org/abs/2507.09280)
*Xuan He,Yuxin Pan,Yize Chen,Danny H. K. Tsang*

Main category: eess.SY

TL;DR: UC问题通常很复杂，需要约束筛选来简化。该研究提出了一种新的约束筛选方法，通过分析可行区域的顶点来减少所需的线性规划（LP）的数量，从而将计算速度提高了8.8倍。


<details>
  <summary>Details</summary>
Motivation: 随着随机可再生能源和负荷行为的渗透率不断提高，需要及时解决电力系统中的单位承诺（UC）问题。然而，传统的基于线性规划（LP）的约束筛选方法计算成本高昂。

Method: 提出了一种新颖的基于顶点分析的约束筛选方法，利用优化后的线性规划（LP）子问题获得的用户单位承诺（UC）决策变量的边界来构建UC可行区域的外逼近。通过矩阵运算识别冗余约束，并通过考虑负荷操作范围和从成本及离散单元状态预测中提取的切割平面来改进筛选效率。

Result: 通过在多达2383个节点的测试用例上进行的大量模拟，验证了所提出方案的有效性。

Conclusion: 所提出的方案相比经典的基于线性规划的筛选方法，可以将速度提高8.8倍，同时找到相同的冗余约束。

Abstract: Power systems Unit Commitment (UC) problem determines the generator
commitment schedule and dispatch decisions to realize the reliable and economic
operation of power networks. The growing penetration of stochastic renewables
and demand behaviors makes it necessary to solve the UC problem timely. It is
possible to derive lightweight, faster-to-solve UC models via constraint
screening to eliminate redundant constraints. However, the screening process
remains computationally cumbersome due to the need of solving numerous linear
programming (LP) problems. To reduce the number of LPs to solve, we introduce a
novel perspective on such classic LP-based screening. Our key insights lie in
the principle that redundant constraints will be satisfied by all vertices of
the screened feasible region. Using the UC decision variables' bounds tightened
by solving much fewer LPs, we build an outer approximation for the UC feasible
region as the screened region. A matrix operation is then designed and applied
to the outer approximation's vertices to identify all redundant constraints
on-the-fly. Adjustments for the outer approximation are further explored to
improve screening efficiency by considering the load operating range and
cutting planes derived from UC cost and discrete unit status prediction.
Extensive simulations are performed on a set of testbeds up to 2,383 buses to
substantiate the effectiveness of the proposed schemes. Compared to classic
LP-based screening, our schemes can achieve up to 8.8x acceleration while
finding the same redundant constraints.

</details>


### [231] [A Fairness-Oriented Multi-Objective Reinforcement Learning approach for Autonomous Intersection Management](https://arxiv.org/abs/2507.09311)
*Matteo Cederle,Marco Fabris,Gian Antonio Susto*

Main category: eess.SY

TL;DR: 提出了一种新的MORL方法，用于平衡交叉口的交通效率和环境影响，并确保公平性。


<details>
  <summary>Details</summary>
Motivation: 为平衡交通效率和环境可持续性，在电动汽车和内燃机汽车之间。

Method: 本研究提出了一种新颖的用于自主交叉口管理的多目标强化学习（MORL）方法，旨在平衡电动汽车和内燃机汽车的交通效率和环境可持续性。所提出的方法利用MORL来识别帕累托最优策略，并采用事后公平标准来指导最终策略的选择。

Result: 在复杂交叉口场景中的模拟结果证明了该方法在优化交通效率和减少排放以及确保跨车辆类别的公平性方面的有效性。

Conclusion: 该方法可以为确保公平服务奠定基础，同时促进智能城市交通安全、高效和可持续的实践。

Abstract: This study introduces a novel multi-objective reinforcement learning (MORL)
approach for autonomous intersection management, aiming to balance traffic
efficiency and environmental sustainability across electric and internal
combustion vehicles. The proposed method utilizes MORL to identify
Pareto-optimal policies, with a post-hoc fairness criterion guiding the
selection of the final policy. Simulation results in a complex intersection
scenario demonstrate the approach's effectiveness in optimizing traffic
efficiency and emissions reduction while ensuring fairness across vehicle
categories. We believe that this criterion can lay the foundation for ensuring
equitable service, while fostering safe, efficient, and sustainable practices
in smart urban mobility.

</details>


### [232] [Neural Two-Stage Stochastic Optimization for Solving Unit Commitment Problem](https://arxiv.org/abs/2507.09503)
*Zhentong Shao,Jingtao Qin,Nanpeng Yu*

Main category: eess.SY

TL;DR: 神经随机优化方法可高效求解大规模随机单位承诺问题。


<details>
  <summary>Details</summary>
Motivation: 为了高效解决高维不确定性场景下的两阶段随机单位承诺（2S-SUC）问题。

Method: 提出了一种神经随机优化方法，通过深度神经网络逼近两阶段随机单位承诺问题（2S-SUC）的第二阶段追索问题，将训练好的网络嵌入第一阶段的单位承诺问题中，形成混合整数线性规划（MILP）。同时，采用场景嵌入网络进行降维和特征聚合，实现数据驱动的场景约减。

Result: 数值实验结果表明，所提出的神经随机优化方法在IEEE 5节点、30节点和118节点系统上，取得了优化差距小于1%的解，并实现了比传统MILP求解器和基于分解的方法快几个数量级的速度提升。

Conclusion: 该方法在保证解的优化度小于1%的情况下，相比于传统方法在求解速度上提升了几个数量级，并且模型规模不随场景数量增加而增加，具有良好的可扩展性。

Abstract: This paper proposes a neural stochastic optimization method for efficiently
solving the two-stage stochastic unit commitment (2S-SUC) problem under
high-dimensional uncertainty scenarios. The proposed method approximates the
second-stage recourse problem using a deep neural network trained to map
commitment decisions and uncertainty features to recourse costs. The trained
network is subsequently embedded into the first-stage UC problem as a
mixed-integer linear program (MILP), allowing for explicit enforcement of
operational constraints while preserving the key uncertainty characteristics. A
scenario-embedding network is employed to enable dimensionality reduction and
feature aggregation across arbitrary scenario sets, serving as a data-driven
scenario reduction mechanism. Numerical experiments on IEEE 5-bus, 30-bus, and
118-bus systems demonstrate that the proposed neural two-stage stochastic
optimization method achieves solutions with an optimality gap of less than 1%,
while enabling orders-of-magnitude speedup compared to conventional MILP
solvers and decomposition-based methods. Moreover, the model's size remains
constant regardless of the number of scenarios, offering significant
scalability for large-scale stochastic unit commitment problems.

</details>


### [233] [Learning Koopman Models From Data Under General Noise Conditions](https://arxiv.org/abs/2507.09646)
*Lucian Cristian Iacob,Máté Szécsi,Gerben Izaak Beintema,Maarten Schoukens,Roland Tóth*

Main category: eess.SY

TL;DR: 提出一种基于深度状态空间编码器和多重추적公式的Koopman模型识别新方法，可处理噪声并实现高效的长期预测。


<details>
  <summary>Details</summary>
Motivation: 提出一种新颖的Koopman模型识别方法，用于在一般噪声条件下识别非线性系统（含输入）。

Method: 提出了一种基于状态可重构概念的深度状态空间编码器，并结合了预测误差平方损失的高效多重추적公式，用于从输入输出数据估计非线性系统的Koopman模型动态和提升状态。模型结构中还包含一个用于处理过程噪声和测量噪声的创新噪声项。

Result: 通过非线性基准示例证明了该方法的有效性。

Conclusion: 该方法具有统计一致性且计算效率高，能够实现良好的长期预测能力。

Abstract: This paper presents a novel identification approach of Koopman models of
nonlinear systems with inputs under rather general noise conditions. The method
uses deep state-space encoders based on the concept of state reconstructability
and an efficient multiple-shooting formulation of the squared loss of the
prediction error to estimate the dynamics and the lifted state from
input-output data. Furthermore, the Koopman model structure includes an
innovation noise term that is used to handle process and measurement noise. It
is shown that the proposed approach is statistically consistent and
computationally efficient due to the multiple-shooting formulation where, on
subsections of the data, multi-step prediction errors can be calculated in
parallel. The latter allows for efficient batch optimization of the network
parameters and, at the same time, excellent long-term prediction capabilities
of the obtained models. The performance of the approach is illustrated by
nonlinear benchmark examples.

</details>


### [234] [Symptom-Driven Personalized Proton Pump Inhibitors Therapy Using Bayesian Neural Networks and Model Predictive Control](https://arxiv.org/abs/2507.09685)
*Yutong Li,Ilya Kolmanovsky*

Main category: eess.SY

TL;DR: 一种基于症状的非侵入式框架，利用贝叶斯神经网络和约束模型预测控制来优化 PPI 剂量，可减少 65% 的药物消耗，同时维持高酸抑制率。


<details>
  <summary>Details</summary>
Motivation: 虽然 PPI 是治疗胃酸相关疾病的标准疗法，但长期或大剂量使用会带来显著风险。由于无法进行超过 72 小时的侵入性胃酸监测以及患者间的巨大差异，精确的长期胃酸控制面临挑战。

Method: 提出了一种基于症状的非侵入式框架，仅根据患者报告的胃食管反流和消化系统症状模式来调整 PPI 剂量。通过贝叶斯神经网络预测模型预测患者症状，并结合病人的饮食和 PPI 摄入数据来量化不确定性。该预测结果为约束模型预测控制（MPC）算法提供输入，以动态计算未来的 PPI 剂量，从而在确保酸抑制的同时最大限度地减少药物使用。

Result: 该方法可将 PPI 的总消耗量与标准的固定方案相比减少 65%，同时以至少 95% 的概率维持酸抑制。

Conclusion: 该方法为个性化质子泵抑制剂（PPI）治疗提供了一条实用的途径，可最大限度地减少治疗负担和过量服药风险，且无需侵入式传感器。

Abstract: Proton Pump Inhibitors (PPIs) are the standard of care for gastric acid
disorders but carry significant risks when administered chronically at high
doses. Precise long-term control of gastric acidity is challenged by the
impracticality of invasive gastric acid monitoring beyond 72 hours and wide
inter-patient variability. We propose a noninvasive, symptom-based framework
that tailors PPI dosing solely on patient-reported reflux and digestive symptom
patterns. A Bayesian Neural Network prediction model learns to predict patient
symptoms and quantifies its uncertainty from historical symptom scores, meal,
and PPIs intake data. These probabilistic forecasts feed a chance-constrained
Model Predictive Control (MPC) algorithm that dynamically computes future PPI
doses to minimize drug usage while enforcing acid suppression with high
confidence - without any direct acid measurement. In silico studies over
diverse dietary schedules and virtual patient profiles demonstrate that our
learning-augmented MPC reduces total PPI consumption by 65 percent compared to
standard fixed regimens, while maintaining acid suppression with at least 95
percent probability. The proposed approach offers a practical path to
personalized PPI therapy, minimizing treatment burden and overdose risk without
invasive sensors.

</details>


### [235] [Electric Vehicle Public Charging Equity Considerations: A Systematic Review](https://arxiv.org/abs/2507.09726)
*Boyou Chen,Kaihan Zhang,Austin Moore,Bochen Jia,Mengqiu Cao*

Main category: eess.SY

TL;DR: 电动汽车充电公平性研究存在地域和方法局限，需加强规范、数据和方法论以实现包容性规划。


<details>
  <summary>Details</summary>
Motivation: 为了解决电动汽车充电基础设施在普及过程中存在的公平性问题，本研究旨在综合现有知识并识别研究空白。

Method: 通过对Scopus和Google Scholar的91篇同行评审研究进行系统性回顾，分析了电动汽车公共充电研究中的公平性问题。

Result: 目前研究主要采用地理信息系统、网络优化、行为建模和混合分析框架，但在公平性评估方面缺乏统一的规范框架。公平性评估涵盖空间可达性、成本负担、可靠性与可用性、用户认知与信任四个维度。社会经济因素（如收入、住房类型、种族）和基础设施选择（如充电桩可靠性、位置、定价）对公平性有显著影响。现有研究主要集中在北美、欧洲和中国，存在地域和方法上的局限性。

Conclusion: 研究显示，电动汽车充电公平性评估需要更健全的规范、更全面的数据整合以及更先进的方法论，以指导更具针对性、包容性和情境适应性的基础设施规划与政策干预。

Abstract: Public electric vehicle (EV) charging infrastructure is crucial for
accelerating EV adoption and reducing transportation emissions; however,
disparities in infrastructure access have raised significant equity concerns.
This systematic review synthesizes existing knowledge and identifies gaps
regarding equity in EV public charging research. Following structured review
protocols, 91 peer-reviewed studies from Scopus and Google Scholar were
analyzed, focusing explicitly on equity considerations. The findings indicate
that current research on EV public charging equity mainly adopted geographic
information systems (GIS), network optimization, behavioral modeling, and
hybrid analytical frameworks, yet lacks consistent normative frameworks for
assessing equity outcomes. Equity assessments highlight four key dimensions:
spatial accessibility, cost burdens, reliability and usability, and user
awareness and trust. Socio-economic disparities, particularly income, housing
tenure, and ethnicity, frequently exacerbate inequitable access,
disproportionately disadvantaging low-income, renter, and minority populations.
Additionally, infrastructure-specific choices, including charger reliability,
strategic location, and pricing strategies, significantly influence adoption
patterns and equity outcomes. However, existing literature primarily reflects
North American, European, and Chinese contexts, revealing substantial
geographical and methodological limitations. This review suggests the need for
more robust normative evaluations of equity, comprehensive demographic data
integration, and advanced methodological frameworks, thereby guiding targeted,
inclusive, and context-sensitive infrastructure planning and policy
interventions.

</details>


### [236] [Optimal Power Management of Battery Energy Storage Systems via Ensemble Kalman Inversion](https://arxiv.org/abs/2507.09755)
*Amir Farakhor,Iman Askari,Di Wu,Huazhen Fang*

Main category: eess.SY

TL;DR: 该研究提出了一种计算效率高的方法来管理电池储能系统（BESS）的功率，通过使用功率共享比和参数估计来解决传统方法的计算复杂性问题，并在仿真中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 为了解决数值优化技术计算复杂性高，难以提供大规模电池储能系统（BESS）的实时解决方案的问题。

Method: 通过引入名为“功率共享比”的新决策变量，并将最优功率管理问题转化为参数估计问题，然后利用集合卡尔曼逆估计最优参数集。

Result: 该方法显著降低了计算要求，并在准确性和计算时间方面与探索过的数值优化技术相比显示出希望。

Conclusion: 所提出的方法在准确性和计算时间方面与探索过的数值优化技术相比显示出希望。

Abstract: Optimal power management of battery energy storage systems (BESS) is crucial
for their safe and efficient operation. Numerical optimization techniques are
frequently utilized to solve the optimal power management problems. However,
these techniques often fall short of delivering real-time solutions for
large-scale BESS due to their computational complexity. To address this issue,
this paper proposes a computationally efficient approach. We introduce a new
set of decision variables called power-sharing ratios corresponding to each
cell, indicating their allocated power share from the output power demand. We
then formulate an optimal power management problem to minimize the system-wide
power losses while ensuring compliance with safety, balancing, and power
supply-demand match constraints. To efficiently solve this problem, a
parameterized control policy is designed and leveraged to transform the optimal
power management problem into a parameter estimation problem. We then implement
the ensemble Kalman inversion to estimate the optimal parameter set. The
proposed approach significantly reduces computational requirements due to 1)
the much lower dimensionality of the decision parameters and 2) the estimation
treatment of the optimal power management problem. Finally, we conduct
extensive simulations to validate the effectiveness of the proposed approach.
The results show promise in accuracy and computation time compared with
explored numerical optimization techniques.

</details>


### [237] [Joint Scheduling of Deferrable and Nondeferrable Demand with Colocated Stochastic Supply](https://arxiv.org/abs/2507.09794)
*Minjae Jeon,Lang Tong,Qing Zhao*

Main category: eess.SY

TL;DR: 该研究提出了一种“拖延策略”和相应的学习算法，用于在存在随机供应的情况下优化可推迟和非可推迟需求的调度，该算法能近似最优策略并优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决涉及同地随机供应的可推迟和非可推迟需求的联合最优调度问题。

Method: 采用有限时间随机动态规划方法，并提出了一种阈值学习算法（Procrastination Threshold Reinforcement Learning）。

Result: 最优调度策略为“拖延策略”，该策略将调度推迟尽可能长的时间，并由三个拖延参数表征。提出的算法能近似最优策略并优于基准。

Conclusion: 该研究提出的阈值学习算法能够接近最优策略并超越标准基准。

Abstract: We address the problem of optimal joint scheduling of deferrable and
nondeferrable demand involving colocated stochastic supply. Deferrable demand
can be delayed within its service deadline, whereas nondeferrable demand must
be scheduled immediately. Under a finite-horizon stochastic dynamic programming
formulation, we show that the optimal scheduling policy is a ``procrastination
policy'' that delays scheduling as much as possible and is characterized by
three procrastination parameters. Exploiting the low-dimensional
parameterization of the optimal policy, we propose a Procrastination Threshold
Reinforcement Learning algorithm. Numerical experiments based on real-world
test data confirm that the threshold-learning algorithm closely approximates
the optimal policy and outperforms standard benchmarks.

</details>


### [238] [Intersection of Reinforcement Learning and Bayesian Optimization for Intelligent Control of Industrial Processes: A Safe MPC-based DPG using Multi-Objective BO](https://arxiv.org/abs/2507.09864)
*Hossein Nejatbakhsh Esfahani,Javad Mohammadpour Velni*

Main category: eess.SY

TL;DR: MPC-RL-MOBO框架通过结合多目标贝叶斯优化，解决了标准MPC-RL收敛慢、次优和不安全的问题，实现了高效、稳定、高性能的控制系统学习。


<details>
  <summary>Details</summary>
Motivation: 标准的MPC-RL方法存在收敛速度慢、参数化受限导致策略学习次优以及在线适应期间的安全问题。为了解决这些挑战，提出了一种新的框架。

Method: 提出了一种结合MPC-RL和多目标贝叶斯优化（MOBO）的新颖框架。该框架利用通过兼容确定性策略梯度（CDPG）方法估计的RL阶段成本及其梯度的噪声评估，并将它们通过预期超体积改进（EHVI）获取函数纳入MOBO算法。

Result: 数值示例证明了该方法在控制系统中实现样本高效、稳定和高性能学习方面的有效性。

Conclusion: 该框架通过结合MPC-RL和多目标贝叶斯优化（MOBO），实现了高效且安全的MPC参数调整，从而在模型不完善的情况下也能获得改进的闭环性能。

Abstract: Model Predictive Control (MPC)-based Reinforcement Learning (RL) offers a
structured and interpretable alternative to Deep Neural Network (DNN)-based RL
methods, with lower computational complexity and greater transparency. However,
standard MPC-RL approaches often suffer from slow convergence, suboptimal
policy learning due to limited parameterization, and safety issues during
online adaptation. To address these challenges, we propose a novel framework
that integrates MPC-RL with Multi-Objective Bayesian Optimization (MOBO). The
proposed MPC-RL-MOBO utilizes noisy evaluations of the RL stage cost and its
gradient, estimated via a Compatible Deterministic Policy Gradient (CDPG)
approach, and incorporates them into a MOBO algorithm using the Expected
Hypervolume Improvement (EHVI) acquisition function. This fusion enables
efficient and safe tuning of the MPC parameters to achieve improved closed-loop
performance, even under model imperfections. A numerical example demonstrates
the effectiveness of the proposed approach in achieving sample-efficient,
stable, and high-performance learning for control systems.

</details>


### [239] [A Case Study on Data Acquisition Systems: Relevance to Renewable Energy Technologies](https://arxiv.org/abs/2507.09938)
*Chito A. Petilla*

Main category: eess.SY

TL;DR: 数据采集通过实时数据收集提高了系统的性能和可靠性，并降低了运营和维护成本，尤其是在可再生能源管理和远程监控领域。尽管如此，数据安全、隐私和准确性仍然是需要解决的关键挑战。


<details>
  <summary>Details</summary>
Motivation: 提高可再生能源管理和控制的效率和可持续性，以及发展智能系统和远程监控功能。

Method: 通过集成数据采集来改进现有系统配置和实现。

Result: 数据采集已在提高系统性能、可靠性和降低成本方面显示出优势，并在远程监控中得到应用。然而，数据安全、隐私和准确性方面仍有待改进。

Conclusion: 数据采集在提高系统性能、可靠性和降低成本方面具有多重优势，尤其是在可再生能源管理和远程监控方面。然而，数据安全、隐私和准确性仍是需要解决的挑战。

Abstract: Multiple advantages had been identified with the integration of data
acquisition into any existing system configuration and implementation. Using
data acquisition as a support into a monitoring system has not only improved
its overall performance and reliability but also lowered its operational and
maintenance cost because of its real-time data collection from node sensors.
  As renewable energy needs to be sustainable for it to fully support the
energy demand of communities, its management and control still needs to be
improved and enhanced. Smart systems are considered the next generation
technological improvement of any system that exists. It is the prelude to
autonomous systems from industrial applications to home automation. Data
acquisition is only a part of these smart systems that help in the remote
management and control of these devices. Remote monitoring functionality
enhances the operation and reliability which help in making proactive decisions
during critical situations and circumstances.
  Even with data acquisition enhancements, there is still room for improving
its implementation regarding data security and privacy and accuracy of
information being exchanged between nodes. Current technological advancements
have already shown promising results and have widen its utilization spectrum by
covering almost any field of specialization. With increasing implementation and
design complexity that comes with its enhancements, challenges and issues are
also faced that needs to be addressed and considered to mitigate the effects of
such.

</details>


### [240] [Efficient RF Chain Selection for MIMO Integrated Sensing and Communications: A Greedy Approach](https://arxiv.org/abs/2507.09960)
*Subin Shin,Seongkyu Jung,Jinseok Choi,Jeonghun Park*

Main category: eess.SY

TL;DR: 为了解决 MIMO ISAC 系统中 RF 链选择的挑战，提出了一种低复杂度的贪婪框架，通过最大化统一的基于互信息的性能指标来选择 RF 链，并为波束选择提出了简化方案。仿真结果证明了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 在多输入多输出集成传感与通信（MIMO ISAC）系统中，射频链（RF 链）选择对于降低硬件成本、功耗和计算复杂度至关重要。然而，通信和传感（互信息 MI 与波束模式均方误差 MSE 或克拉美罗下界 CRLB）之间的性能指标差异使得设计有效的射频链选择策略充满挑战。

Method: 提出了一种低复杂度的贪婪 RF 链选择框架，通过将总互信息分解为每个 RF 链的单独贡献，并引入了两种方法：贪婪特征值选择（GES）和贪婪伴随式选择（GCS），它们迭代地识别并移除贡献最低的 RF 链。此外，还将该框架扩展到波束选择，提出了对角波束选择（DBS）。

Result: 仿真结果表明，所提出的方法在显著降低复杂度的同时达到了接近最优的性能。

Conclusion: 所提出的贪婪方法（GES 和 GCS）以及 DBS 在近乎最优的性能下实现了显著降低的复杂度，证明了它们在 MIMO ISAC 系统中的实用有效性。

Abstract: In multiple-input multiple-output integrated sensing and communication (MIMO
ISAC) systems, radio frequency chain (i.e., RF chain) selection plays a vital
role in reducing hardware cost, power consumption, and computational
complexity. However, designing an effective RF chain selection strategy is
challenging due to the disparity in performance metrics between communication
and sensing-mutual information (MI) versus beam-pattern mean-squared error
(MSE) or the Cram\'er-Rao lower bound (CRLB). To overcome this, we propose a
low-complexity greedy RF chain selection framework maximizing a unified
MI-based performance metric applicable to both functions. By decomposing the
total MI into individual contributions of each RF chain, we introduce two
approaches: greedy eigen-based selection (GES) and greedy cofactor-based
selection (GCS), which iteratively identify and remove the RF chains with the
lowest contribution. We further extend our framework to beam selection for
beamspace MIMO ISAC systems, introducing diagonal beam selection (DBS) as a
simplified solution. Simulation results show that our proposed methods achieve
near-optimal performance with significantly lower complexity than exhaustive
search, demonstrating their practical effectiveness for MIMO ISAC systems.

</details>


### [241] [Predictive & Trust-based Multi-Agent Coordination](https://arxiv.org/abs/2507.09997)
*Venkatraman Renganathan,Sabyasachi Mondal,Antonios Tsourdos*

Main category: eess.SY

TL;DR: 提出了一种名为ADC的预测性多主体共识协议，通过分析邻居的预期数据来学习信任和承诺，并实现代理间的协调和共识。


<details>
  <summary>Details</summary>
Motivation: 为了实现网络中代理间的有效协调和信任建立，提出了一种新的共识协议。

Method: 提出了一种基于信任的预测性多主体共识协议（ADC协议），该协议分析邻居的预期数据并做出协调决策。代理在有限的前瞻视界内与其邻居共享其未来的预测数据，并以滚动视界的方式更新其预测。然后，代理利用预测数据随时间学习邻居表现出的信任和承诺特征。

Result: 通过数值模拟展示了该协议的有效性，并提供了理论上的收敛性保证。

Conclusion: 该协议通过数值模拟进行了验证，并提供了基于Lyapunov理论的代理间一致性收敛性证明。

Abstract: This paper presents a trust-based predictive multi-agent consensus protocol
that analyses neighbours' anticipation data and makes coordination decisions.
Agents in the network share their future predicted data over a finite
look-ahead horizon with their neighbours and update their predictions in a
rolling-horizon fashion. The prediction data is then used by agents to learn
both the trust and the commitment traits exhibited by their neighbours over
time. The proposed protocol is named as the Anticipatory Distributed
Coordination (ADC) protocol. Lyapunov theory-based agreement convergence
between agents is provided, followed by demonstrations using numerical
simulations.

</details>


### [242] [Hardware test and validation of the angular droop control: Analysis and experiments](https://arxiv.org/abs/2507.10004)
*Taouba Jouini,Jan Wachter,Sophie An,Veit Hagenmeyer*

Main category: eess.SY

TL;DR: Hardware experiments validate angular droop control for low voltage grids, testing grid-forming capabilities after blackouts and plug-and-play features with two implementation schemes and tuning recommendations for converter control actuation and comparing them. It also investigates converter systems and tuning gains, providing insights into practical challenges and solutions.


<details>
  <summary>Details</summary>
Motivation: The angular droop control is a grid-forming control strategy that exploits the idea of power-to-angle droop to achieve exact frequency synchronization with no stringent separation between primary and secondary frequency control.

Method: This work conducts hardware experiments in the Smart Energy System Control Laboratory at Karlsruhe Institute of Technology (KIT) to test and validate the angular droop control for low voltage power grids in two different test scenarios. First, it verifies grid-forming capabilities after a blackout, demonstrated via power-to-angle droop behavior, by proposing two implementation schemes that rely either on direct or indirect actuation of the modulation signal and drawing a comparison between them. Second, it investigates plug-and-play capabilities, i.e., local stability and power sharing for a two-converter system and provides suitable tuning for the control gains.

Result: The experimental findings illustrate the usefulness of hardware test and validation for DC/AC converter control, the practical challenges entailed and the proposed remedies.

Conclusion: The experimental findings illustrate the usefulness of hardware test and validation for DC/AC converter control, the practical challenges entailed and the proposed remedies.

Abstract: The angular droop control is a grid-forming control strategy that exploits
the idea of power-to-angle droop to achieve exact frequency synchronization
with no stringent separation between primary and secondary frequency control.
In this work, we conduct hardware experiments in the Smart Energy System
Control Laboratory at Karlsruhe Institute of Technology (KIT) to test and
validate the angular droop control for low voltage power grids in two different
test scenarios. First, we verify its grid-forming capabilities after a major
event, e.g., following a blackout, demonstrated via power-to-angle droop
behavior. For this, we propose two implementation schemes that rely either on
direct or indirect actuation of the modulation signal and draw a comparison
between them. Second, we investigate the plug-and-play capabilities, i.e.,
local stability and power sharing for a two-converter system and provide
suitable tuning for the control gains. Our experimental findings illustrate the
usefulness of hardware test and validation for DC/AC converter control, the
practical challenges entailed and the proposed remedies.

</details>


### [243] [Probabilistic Robustness in the Gap Metric](https://arxiv.org/abs/2507.10010)
*Venkatraman Renganathan*

Main category: eess.SY

TL;DR: 本文提出了一种利用“间隙度量”的概率方法来量化和保证受随机不确定性影响的动态系统的鲁棒性，并提供了控制器性能的概率认证。


<details>
  <summary>Details</summary>
Motivation: 为了解决在控制受随机不确定性影响的动态系统时，由于无法获得硬性保证而难以估计控制器性能的问题。

Method: 本文将系统模型的不确定性量化为“间隙度量”，并将其视为随机变量，在此基础上推导了间隙度量的概率界限、期望值界限以及基于间隙度量的概率鲁棒稳定性。

Result: 本文的主要研究结果包括：给出间隙度量超过已知阈值的概率界限，以及期望间隙值和基于间隙度量的概率鲁棒稳定性界限。此外，还提供了概率控制器性能认证和可实现的H_inf鲁棒性的概率保证。

Conclusion: 本文提出了使用基于“间隙度量”的概率方法来评估和保证受随机不确定性影响的动态系统的鲁棒性，并给出了控制器性能的概率认证和期望的H_inf鲁棒性。

Abstract: Uncertainties influencing the dynamical systems pose a significant challenge
in estimating the achievable performance of a controller aiming to control such
uncertain systems. When the uncertainties are of stochastic nature, obtaining
hard guarantees for the robustness of a controller aiming to hedge against the
uncertainty is not possible. This issue set the platform for the development of
probabilistic robust control approaches. In this work, we utilise the gap
metric between the known nominal model and the unknown perturbed model of the
uncertain system as a tool to gauge the robustness of a controller and
formulate the gap as a random variable in the setting with stochastic
uncertainties. Main results of this paper includes giving probabilistic bound
on the gap exceeding a known threshold followed by bounds on the expected gap
value and probabilistic robust stability in terms of the gap metric. Further,
we also provide a probabilistic controller performance certification under gap
uncertainty and probabilistic guarantee on the achievable
$\mathcal{H}_{\infty}$ robustness. Numerical simulations are provided at many
places to demonstrate the proposed approach.

</details>


### [244] [Survey on Methods for Detection, Classification and Location of Faults in Power Systems Using Artificial Intelligence](https://arxiv.org/abs/2507.10011)
*Juan A. Martinez-Velasco,Alexandre Serrano-Fontova,Ricard Bosch-Tous,Pau Casals-Torrens*

Main category: eess.SY

TL;DR: 本文调查了人工智能在电力系统故障诊断中的应用。


<details>
  <summary>Details</summary>
Motivation: 由于传统保护方法在应对日益复杂的电力系统故障时存在局限性，因此迫切需要开发能够准确检测、定位和排除故障的新方法。人工智能技术在故障诊断方面展现出巨大潜力。

Method: 本文对人工智能技术在电力系统故障诊断中的应用进行了调查，介绍了人工智能概念，并讨论了基于人工智能的故障诊断方法。

Result: 本文对人工智能技术在电力系统故障诊断中的应用进行了调查，为相关领域的研究和实践提供了参考。

Conclusion: 本文对应用于输配电系统线路和电缆故障诊断（故障检测、分类和定位）的人工智能技术进行了调查。

Abstract: Components of electrical power systems are susceptible to failures caused by
lightning strikes, aging or human errors. These faults can cause equipment
damage, affect system reliability, and results in expensive repair costs. As
electric power systems are becoming more complex, traditional protection
methods face limitations and shortcomings. Faults in power systems can occur at
anytime and anywhere, can be caused by a natural disaster or an accident, and
their occurrence can be hardly predicted or avoided; therefore, it is crucial
to accurately estimate the fault location and quickly restore service. The
development of methods capable of accurately detecting, locating and removing
faults is essential (i.e. fast isolation of faults is necessary to maintain the
system stability at transmission levels; accurate and fast detection and
location of faults are essential for increasing reliability and customer
satisfaction at distribution levels). This has motivated the development of new
and more efficient methods. Methods developed to detect and locate faults in
power systems can be divided into two categories, conventional and artificial
intelligence-based techniques. Although the utilization of artificial
intelligence (AI) techniques offer tremendous potential, they are challenging
and time consuming (i.e. many AI techniques require training data for
processing). This paper presents a survey of the application of AI techniques
to fault diagnosis (detection, classification and location of faults) of lines
and cables of power systems at both transmission and distribution levels. The
paper provides a short introduction to AI concepts, a brief summary of the
application of AI techniques to power system analysis and design, and a
discussion on AI-based fault diagnosis methods.

</details>


### [245] [Optimal Battery Placement in Power Grid](https://arxiv.org/abs/2507.10123)
*Ruotong Sun,Ermin Wei,Lihui Yi*

Main category: eess.SY

TL;DR: 该研究解决了电网中电池的优化放置问题，发现加权度是影响放置的关键拓扑因素，并提出了一种高效算法。


<details>
  <summary>Details</summary>
Motivation: 在集中式市场模型中，研究了在电网中放置无限容量电池的优化问题，独立系统运营商 (ISO) 旨在通过负荷转移来最小化总发电成本。现有文献未能充分理解最优电池放置，特别是网络拓扑对最小化发电成本的影响。

Method: 通过将混合整数线性规划 (MILP) 问题分解为一系列线性规划 (LP) 公式来研究电池的优化放置。推导出加权度作为最优电池放置的唯一拓扑因素的分析成本表达式，并研究了更高阶拓扑条件对树状拓扑网络的影响。

Result: 在具有足够大发电容量或树状拓扑的电网中，加权度是影响最优电池放置的唯一拓扑因素。单个电池的相对成本节省效益随着网络扩展而降低。为弱循环网络设计了一种低复杂度算法，该算法比商业求解器快约 100 倍，并且在放松某些理论假设时仍能保持高精度。

Conclusion: 该研究导出了加权度作为最优电池放置的唯一拓扑因素的分析成本表达式，并展示了单个电池的相对成本节省效益随着网络扩展而降低。此外，还为弱循环网络设计了一种低复杂度算法，该算法比商业求解器快约 100 倍，同时在放松某些理论假设时仍能保持高精度。

Abstract: We study the optimal placement of an unlimited-capacity battery in power
grids under a centralized market model, where the independent system operator
(ISO) aims to minimize total generation costs through load shifting. The
optimal battery placement is not well understood by the existing literature,
especially regarding the influence of network topology on minimizing generation
costs. Our work starts with decomposing the Mixed-Integer Linear Programming
(MILP) problem into a series of Linear Programming (LP) formulations. For power
grids with sufficiently large generation capacity or tree topologies, we derive
analytical cost expressions demonstrating that, under reasonable assumptions,
the weighted degree is the only topological factor for optimal battery
placement. We also discuss the minor impact of higher-order topological
conditions on tree-topology networks. To find the localized nature of a single
battery's impact, we establish that the relative cost-saving benefit of a
single battery decreases as the network scales. Furthermore, we design a
low-complexity algorithm for weakly-cyclic networks. Numerical experiments show
that our algorithm is not only approximately 100 times faster than commercial
solvers but also maintains high accuracy even when some theoretical assumptions
are relaxed.

</details>


### [246] [A SUMO-Based Digital Twin for Evaluation of Conventional and Electric Vehicle Networks](https://arxiv.org/abs/2507.10280)
*Haomiaomiao Wang,Conor Fennell,Swati Poojary,Mingming Liu*

Main category: eess.SY

TL;DR: 本研究提出了一个基于SUMO的数字孪生模型，用于模拟混合燃油车-电动车交通。该模型融合多源传感器数据，在完整和不完整信息场景下均表现出高精度和鲁棒性，在交通参数估计和排放/能耗评估方面均取得良好结果。


<details>
  <summary>Details</summary>
Motivation: 数字孪生技术在交通建模中越来越受到重视，能够复制现实世界的交通动态，并评估交通和能源效率。本研究旨在通过构建一个基于SUMO的数字孪生模型来解决这一需求，以模拟混合燃油车和电动车交通。

Method: 该研究利用SUMO（Simulation of Urban MObility）构建了一个数字孪生模型，该模型融合了来自感应线圈、GPS探针和收费记录的多传感器数据，以模拟混合燃油车和电动车在主要高速公路路段的交通状况。模型在完整和不完整信息场景下都进行了验证。

Result: 模型在平均速度估计方面达到了93.1%的准确率，在平均行程长度估计方面达到了97.1%的准确率。使用KL散度和Wasserstein距离等统计指标评估，模拟交通模式与观测到的交通模式高度一致。即使在车辆分类信息不完整的情况下，模型对CO2排放的估计也仅过高0.8-2.4%，对EV电力消耗的估计也仅过低1.0-5.4%。

Conclusion: 该研究提出的基于SUMO的数字孪生模型在模拟混合燃油车和电动车交通方面表现出色，能够准确估计平均速度和平均行程长度，即使在信息不完整的情况下也具有鲁棒性，并且在碳排放和电力消耗的估计方面也相当准确。

Abstract: Digital twins are increasingly applied in transportation modelling to
replicate real-world traffic dynamics and evaluate mobility and energy
efficiency. This study presents a SUMO-based digital twin that simulates mixed
ICEV-EV traffic on a major motorway segment, leveraging multi-sensor data
fusion from inductive loops, GPS probes, and toll records. The model is
validated under both complete and partial information scenarios, achieving
93.1% accuracy in average speed estimation and 97.1% in average trip length
estimation. Statistical metrics, including KL Divergence and Wasserstein
Distance, demonstrate strong alignment between simulated and observed traffic
patterns. Furthermore, CO2 emissions were overestimated by only 0.8-2.4%, and
EV power consumption underestimated by 1.0-5.4%, highlighting the model's
robustness even with incomplete vehicle classification information.

</details>


### [247] [Improved Sum-of-Squares Stability Verification of Neural-Network-Based Controllers](https://arxiv.org/abs/2507.10352)
*Alvaro Detailleur,Guillaume Ducard,Christopher Onder*

Main category: eess.SY

TL;DR: This paper enhances a stability verification framework for neural network control systems by introducing new functions, proving existing methods, and simplifying analysis with new optimization problems, all demonstrated with numerical examples.


<details>
  <summary>Details</summary>
Motivation: The motivation of this work is to improve the closed-loop stability verification framework for neural-network-based control systems regulating nonlinear dynamical systems, by expanding its utility, validating its analyses, and simplifying stability property analysis.

Method: This paper improves a closed-loop stability verification framework using semialgebraic sets and convex semidefinite programming. Improvements include expanding the framework's utility with two semialgebraic functions mimicking common activation functions, establishing compatibility with Recurrent Equilibrium Networks (RENs) and Recurrent Neural Networks (RNNs), and providing an alternate proof for the validity of state-of-the-art stability analyses. Based on this proof, two new optimization problems are presented to simplify the analysis of local stability properties and the Region of Attraction (RoA). The first problem parameterizes a larger class of candidate Lyapunov functions, while the second problem utilizes invariance guarantees to expand candidate Lyapunov functions and determine if an invariant set is part of the RoA.

Result: The paper presents two semialgebraic functions that expand the framework's utility and establishes compatibility with RENs and RNNs. It also provides an alternate proof for the framework's stability analyses and introduces two new optimization problems that simplify the analysis of local stability properties and the Region of Attraction (RoA). These contributions are validated through two numerical examples.

Conclusion: This work successfully demonstrates its contributions through two numerical examples and provides suggestions for future research.

Abstract: This work presents several improvements to the closed-loop stability
verification framework using semialgebraic sets and convex semidefinite
programming to examine neural-network-based control systems regulating
nonlinear dynamical systems. First, the utility of the framework is greatly
expanded: two semialgebraic functions mimicking common, smooth activation
functions are presented and compatibility with control systems incorporating
Recurrent Equilibrium Networks (RENs) and thereby Recurrent Neural Networks
(RNNs) is established. Second, the validity of the framework's state-of-the-art
stability analyses is established via an alternate proof. Third, based on this
proof, two new optimization problems simplifying the analysis of local
stability properties are presented. To simplify the analysis of a closed-loop
system's Region of Attraction (RoA), the first problem explicitly parameterizes
a class of candidate Lyapunov functions larger than in previous works. The
second problem utilizes the unique guarantees available under the condition of
invariance to further expand the set of candidate Lyapunov functions and
directly determine whether an invariant set forms part of the system's RoA.
These contributions are successfully demonstrated in two numerical examples and
suggestions for future research are provided.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [248] [MQFQ-Sticky: Fair Queueing For Serverless GPU Functions](https://arxiv.org/abs/2507.08954)
*Alexander Fuerst,Siddharth Anil,Vishakha Dixit,Purushottam,Kulkarni,Prateek Sharma*

Main category: cs.DC

TL;DR: 该论文提出了一种新的 FaaS 系统，通过引入先进的调度和内存管理技术（MQFQ-Sticky），实现了对 GPU 的高效支持，显著降低了函数延迟，解决了现有 FaaS 框架在处理需要 GPU 加速的任务时的性能瓶颈。


<details>
  <summary>Details</summary>
Motivation: 随着 GPU 在数据中心中的普及，许多需要 GPU 加速的应用（如机器学习、科学计算）在现有的 FaaS 框架（如 OpenWhisk）中无法得到充分支持。这是由于 GPU 与 FaaS 的编程模型之间存在不匹配，特别是 FaaS 所要求的虚拟化和沙盒机制，以及 FaaS 应用动态和异构的特点，放大了这些挑战。

Method: 提出了一种名为 MQFQ-Sticky 的新方法，该方法借鉴了 I/O 调度的公平排队和预期调度原则，并结合了 GPU 内存管理，以解决 FaaS 框架在支持 GPU 加速时面临的动态性、异构性以及容器化沙箱带来的 GPU 并发性限制和冷启动开销问题。

Result: 通过在多种工作负载下的实证评估，表明该系统相比现有的 GPU 和 CPU 排队策略，能够将函数延迟降低 2 到 20 倍。

Conclusion: 该论文设计并实现了一个提供 GPU 加速的 FaaS 系统，该系统以黑盒方式运行（无需修改函数代码），并采用了 MQFQ-Sticky 方法，该方法结合了 I/O 调度原则和 GPU 内存管理，以平衡局部性、公平性和延迟性。

Abstract: Hardware accelerators like GPUs are now ubiquitous in data centers, but are
not fully supported by common cloud abstractions such as Functions as a Service
(FaaS). Many popular and emerging FaaS applications such as machine learning
and scientific computing can benefit from GPU acceleration. However, FaaS
frameworks (such as OpenWhisk) are not capable of providing this acceleration
because of the impedance mismatch between GPUs and the FaaS programming model,
which requires virtualization and sandboxing of each function. The challenges
are amplified due to the highly dynamic and heterogeneous FaaS workloads. This
paper presents the design and implementation of a FaaS system for providing GPU
acceleration in a black-box manner (without modifying function code). Running
small functions in containerized sandboxes is challenging due to limited GPU
concurrency and high cold-start overheads, resulting in heavy queueing of
function invocations. We show how principles from I/O scheduling, such as fair
queuing and anticipatory scheduling, can be translated to function scheduling
on GPUs. We develop MQFQ-Sticky, an integrated fair queueing and GPU memory
management approach, which balances the tradeoffs between locality, fairness,
and latency. Empirical evaluation on a range of workloads shows that it reduces
function latency by 2x to 20x compared to existing GPU and CPU queueing
policies.

</details>


### [249] [Lightweight Federated Learning over Wireless Edge Networks](https://arxiv.org/abs/2507.09546)
*Xiangwang Hou,Jingjing Wang,Jun Du,Chunxiao Jiang,Yong Ren,Dusit Niyato*

Main category: cs.DC

TL;DR: 该论文提出了一种名为LTFL的轻量化联邦学习框架，通过优化传输功率、模型剪枝和梯度量化，解决了无线网络中联邦学习的效率和隐私问题。


<details>
  <summary>Details</summary>
Motivation: 随着连接到无线网络的智能设备的指数级增长，数据量急剧增加，需要机器学习（ML）技术来发掘其价值。然而，传统的中心化ML范式引发了通信开销和隐私方面的担忧。联邦学习（FL）在网络边缘提供了一种替代方案，但在无线网络中的实际部署仍然面临挑战。

Method: 提出了一种轻量化联邦学习（LTFL）框架，该框架集成了无线传输功率控制、模型剪枝和梯度量化。推导了考虑传输误差、模型剪枝误差和梯度量化误差的FL收敛差距的闭式表达式。基于此，制定了一个最小化收敛差距同时满足延迟和能源约束的优化问题。为高效解决该非凸问题，推导了最优模型剪枝率和梯度量化级别的闭式解，并采用贝叶斯优化进行传输功率控制。

Result: 实验结果表明，LTFL框架在真实数据集上的表现优于现有技术。

Conclusion: LTFL框架通过集成无线传输功率控制、模型剪枝和梯度量化，在满足延迟和能源约束的同时，最小化了FL收敛差距，并且在真实数据集上的实验证明了其优于现有技术。

Abstract: With the exponential growth of smart devices connected to wireless networks,
data production is increasing rapidly, requiring machine learning (ML)
techniques to unlock its value. However, the centralized ML paradigm raises
concerns over communication overhead and privacy. Federated learning (FL)
offers an alternative at the network edge, but practical deployment in wireless
networks remains challenging. This paper proposes a lightweight FL (LTFL)
framework integrating wireless transmission power control, model pruning, and
gradient quantization. We derive a closed-form expression of the FL convergence
gap, considering transmission error, model pruning error, and gradient
quantization error. Based on these insights, we formulate an optimization
problem to minimize the convergence gap while meeting delay and energy
constraints. To solve the non-convex problem efficiently, we derive closed-form
solutions for the optimal model pruning ratio and gradient quantization level,
and employ Bayesian optimization for transmission power control. Extensive
experiments on real-world datasets show that LTFL outperforms state-of-the-art
schemes.

</details>


### [250] [Intelligent Task Management via Dynamic Multi-region Division in LEO Satellite Networks](https://arxiv.org/abs/2507.09926)
*Zixuan Song,Zhishu Shen,Xiaoyu Zheng,Qiushi Zheng,Zheng Lei,Jiong Jin*

Main category: cs.DC

TL;DR: 提出了一种用于LEO卫星网络智能任务管理的动态多区域划分框架，结合遗传算法和MA-DDPG优化路由和任务卸载，以减少延迟和资源消耗。


<details>
  <summary>Details</summary>
Motivation: LEO卫星网络虽然是未来6G的重要组成部分，但其有限的资源和不均衡的工作负载会导致ISL拥堵，降低任务处理效率。如何管理动态的大规模拓扑以平衡任务分配是一个关键挑战。

Method: 提出了一种动态多区域划分框架，并结合了基于遗传算法的动态多区域划分算法以及基于MA-DDPG的自适应路由算法和任务拆分卸载方案。

Result: 仿真结果表明，所提出的框架在任务延迟、每任务能耗和任务完成率方面优于对比方法。

Conclusion: 所提出的框架通过动态多区域划分和基于MA-DDPG的路由和任务卸载方案，在任务延迟、每任务能耗和任务完成率方面优于对比方法。

Abstract: As a key complement to terrestrial networks and a fundamental component of
future 6G systems, Low Earth Orbit (LEO) satellite networks are expected to
provide high-quality communication services when integrated with ground-based
infrastructure, thereby attracting significant research interest. However, the
limited satellite onboard resources and the uneven distribution of
computational workloads often result in congestion along inter-satellite links
(ISLs) that degrades task processing efficiency. Effectively managing the
dynamic and large-scale topology of LEO networks to ensure balanced task
distribution remains a critical challenge. To this end, we propose a dynamic
multi-region division framework for intelligent task management in LEO
satellite networks. This framework optimizes both intra- and inter-region
routing to minimize task delay while balancing the utilization of computational
and communication resources. Based on this framework, we propose a dynamic
multi-region division algorithm based on the Genetic Algorithm (GA), which
adaptively adjusts the size of each region based on the workload status of
individual satellites. Additionally, we incorporate an adaptive routing
algorithm and a task splitting and offloading scheme based on Multi-Agent Deep
Deterministic Policy Gradient (MA-DDPG) to effectively accommodate the arriving
tasks. Simulation results demonstrate that our proposed framework outperforms
comparative methods in terms of the task delay, energy consumption per task,
and task completion rate.

</details>


### [251] [EAT: QoS-Aware Edge-Collaborative AIGC Task Scheduling via Attention-Guided Diffusion Reinforcement Learning](https://arxiv.org/abs/2507.10026)
*Zhifei Xu,Zhiqing Tang,Jiong Lou,Zhi Yao,Xuan Xie,Tian Wang,Yinglong Wang,Weijia Jia*

Main category: cs.DC

TL;DR: EAT算法通过QoS感知和边缘协同调度，利用强化学习优化AIGC任务在异构边缘服务器上的处理，显著降低了推理延迟。


<details>
  <summary>Details</summary>
Motivation: AI和大型语言模型的增长使得生成式AI（GenAI）在云数据中心得到广泛应用，然而像Stable Diffusion这类模型会引入不可避免的延迟和大量的资源开销，这对于网络边缘有高服务质量（QoS）要求的用户并不适用。将AIGC服务部署在边缘服务器可以减少传输时间，但常常导致资源利用率低下，并且无法最优地平衡推理延迟和质量。

Method: 提出了一种名为EAT（Edge-collaborative AIGC Task scheduling）的QoS感知算法，该算法将AIGC任务分块并在异构边缘服务器之间进行调度，以平衡推理延迟和质量。该算法采用基于强化学习的方法，利用注意力层提取边缘服务器的负载和任务队列信息，并通过基于扩散的策略网络进行调度以实现模型重用。此外，还开发了一个AIGC任务调度系统来实现该算法。

Result: 实验结果和大规模模拟表明，EAT算法相比现有基线方法，可以将推理延迟降低高达56%。

Conclusion: EAT算法通过将AIGC任务分块并调度到多个边缘服务器，并利用强化学习和注意力机制来优化调度，有效解决了边缘AIGC服务中的延迟和资源利用率问题，相比基线方法可将推理延迟降低高达56%。

Abstract: The growth of Artificial Intelligence (AI) and large language models has
enabled the use of Generative AI (GenAI) in cloud data centers for diverse
AI-Generated Content (AIGC) tasks. Models like Stable Diffusion introduce
unavoidable delays and substantial resource overhead, which are unsuitable for
users at the network edge with high QoS demands. Deploying AIGC services on
edge servers reduces transmission times but often leads to underutilized
resources and fails to optimally balance inference latency and quality. To
address these issues, this paper introduces a QoS-aware
\underline{E}dge-collaborative \underline{A}IGC \underline{T}ask scheduling
(EAT) algorithm. Specifically: 1) We segment AIGC tasks and schedule patches to
various edge servers, formulating it as a gang scheduling problem that balances
inference latency and quality while considering server heterogeneity, such as
differing model distributions and cold start issues. 2) We propose a
reinforcement learning-based EAT algorithm that uses an attention layer to
extract load and task queue information from edge servers and employs a
diffusion-based policy network for scheduling, efficiently enabling model
reuse. 3) We develop an AIGC task scheduling system that uses our EAT algorithm
to divide tasks and distribute them across multiple edge servers for
processing. Experimental results based on our system and large-scale
simulations show that our EAT algorithm can reduce inference latency by up to
56\% compared to baselines. We release our open-source code at
https://github.com/zzf1955/EAT.

</details>


### [252] [ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal Parallelism](https://arxiv.org/abs/2507.10069)
*Zedong Liu,Shenggan Cheng,Guangming Tan,Yang You,Dingwen Tao*

Main category: cs.DC

TL;DR: ElasticMM通过弹性多模态并行优化了MLLM服务，显著降低了延迟并提高了吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有的MLLM服务架构耦合紧密，难以区分混合请求类型或为不同推理阶段适配并行策略，导致首词延迟（TTFT）增加和资源利用率低下。

Method: 提出了一种名为弹性多模态并行（EMP）的新服务范式，并在此基础上开发了ElasticMM系统，该系统包含模态感知负载均衡器、弹性分区调度以及统一的多模态前缀缓存和非阻塞编码等关键技术。

Result: ElasticMM系统相比于现有的先进服务系统，在TTFT方面降低了4.2倍，吞吐量提高了3.2-4.5倍，同时满足服务水平目标（SLOs）。

Conclusion: 所提出的 ElasticMM 系统通过分离请求、解耦推理阶段以及统一前缀缓存等技术，在MLLM服务效率方面取得了显著突破，能够有效降低延迟并提高吞吐量。

Abstract: Multimodal large language models (MLLMs) extend LLMs to handle images,
videos, and audio by incorporating feature extractors and projection modules.
However, these additional components -- combined with complex inference
pipelines and heterogeneous workloads -- introduce significant inference
overhead. Therefore, efficiently serving MLLMs remains a major challenge.
Current tightly coupled serving architectures struggle to distinguish between
mixed request types or adapt parallelism strategies to different inference
stages, leading to increased time-to-first-token (TTFT) latency and poor
resource utilization. To address this, we propose Elastic Multimodal
Parallelism (EMP), a new serving paradigm that elastically adapts to resource
heterogeneity across request types and inference stages. Building upon EMP, we
develop ElasticMM, an MLLM serving system that (1) separates requests into
independent modality groups with dynamic resource allocation via a
modality-aware load balancer; (2) decouples inference stages and enables
parallelism adjustment and adaptive scaling via elastic partition scheduling;
and (3) improves inference efficiency through unified multimodal prefix caching
and non-blocking encoding. Experiments on diverse real-world datasets show that
ElasticMM outperforms state-of-the-art (SOTA) serving systems, reducing TTFT by
up to 4.2x and achieving 3.2-4.5x higher throughput while meeting service-level
objectives (SLOs).

</details>


### [253] [Large-Scale Graph Building in Dynamic Environments: Low Latency and High Quality](https://arxiv.org/abs/2507.10139)
*Filipe Miguel Gonçalves de Almeida,CJ Carey,Hendrik Fichtenberger,Jonathan Halcrow,Silvio Lattanzi,André Linhares,Tao Meng,Ashkan Norouzi-Fard,Nikos Parotsidis,Bryan Perozzi,David Simcha*

Main category: cs.DC

TL;DR: A new system called Dynamic Grale Using ScaNN (Dynamic GUS) has been developed to efficiently learn and construct large-scale graphs in dynamic, low-latency environments, overcoming the limitations of previous offline tools like Grale. This system has applications in areas like Android Security and Privacy, where it significantly speeds up the detection of harmful applications.


<details>
  <summary>Details</summary>
Motivation: The motivation for this work stems from the limitations of existing graph construction tools like Grale, which are suitable for offline environments but cannot efficiently handle continuously evolving data with low-latency requirements. Existing Approximate Nearest Neighbor (ANN) systems that handle dynamic updates are often limited to similarities over a single embedding.

Method: The paper introduces Dynamic Grale Using ScaNN (Dynamic GUS), a system designed to handle continuously evolving data and provide updated graphs with low latency, addressing the limitations of existing systems like Grale which are designed for offline environments.

Result: Dynamic GUS enables capturing harmful applications 4 times faster in Android Security and Privacy, and has over 10 deployments at Google.

Conclusion: Dynamic Grale Using ScaNN (Dynamic GUS) is a system that inherits the advantages and quality of Grale, maintaining graph construction in a dynamic setting with tens of milliseconds of latency per request. It has over 10 deployments at Google, including in Android Security and Privacy, where it enables capturing harmful applications 4 times faster.

Abstract: Learning and constructing large-scale graphs has attracted attention in
recent decades, resulting in a rich literature that introduced various systems,
tools, and algorithms. Grale is one of such tools that is designed for offline
environments and is deployed in more than 50 different industrial settings at
Google. Grale is widely applicable because of its ability to efficiently learn
and construct a graph on datasets with multiple types of features. However, it
is often the case that applications require the underlying data to evolve
continuously and rapidly and the updated graph needs to be available with low
latency. Such setting make the use of Grale prohibitive. While there are
Approximate Nearest Neighbor (ANN) systems that handle dynamic updates with low
latency, they are mostly limited to similarities over a single embedding.
  In this work, we introduce a system that inherits the advantages and the
quality of Grale, and maintains a graph construction in a dynamic setting with
tens of milliseconds of latency per request. We call the system Dynamic Grale
Using ScaNN (Dynamic GUS). Our system has a wide range of applications with
over 10 deployments at Google. One of the applications is in Android Security
and Privacy, where Dynamic Grale Using ScaNN enables capturing harmful
applications 4 times faster, before they can reach users.

</details>


### [254] [Past-Future Scheduler for LLM Serving under SLA Guarantees](https://arxiv.org/abs/2507.10150)
*Ruihao Gong,Shihao Bai,Siyu Wu,Yunqian Fan,Zaijun Wang,Xiuhong Li,Hailong Yang,Xianglong Liu*

Main category: cs.DC

TL;DR: 该研究提出了Past-Future调度器和LightLLM框架，以解决现有LLM服务框架中连续批处理内存估计不准确的问题，显著提高了吞吐量。


<details>
  <summary>Details</summary>
Motivation: 为了降低部署成本，连续批处理已成为当前服务框架中的一项基本功能。连续批处理的有效性取决于对请求内存需求的准确估计。然而，由于请求输出长度的多样性，现有框架倾向于采用激进或保守的调度器，这往往导致内存消耗的严重高估或低估，从而无法在严格的服务水平协议（SLA）保证下实现令人满意的吞吐量（即好put）。

Method: 提出了一种名为Past-Future的新调度器，该调度器通过考虑请求输出长度的历史分布和计算未来每个时间点的内存占用情况来精确估计所需的峰值内存资源。

Result: 与现有的激进或保守调度器相比，LightLLM实现了高达2-3倍的更高吞吐量，展现出优越的好put。

Conclusion: 该研究提出了一个名为Past-Future的新调度器，该调度器通过考虑请求输出长度的历史分布和计算未来每个时间点的内存占用情况，精确估计运行批次所需的峰值内存资源。该调度器能够适应各种输入输出长度分布的应用程序，平衡请求排队和有害驱逐之间的权衡，从而持续实现更好的吞吐量。此外，研究人员还开发了一个名为LightLLM的高性能LLM服务框架来实现该调度器，与现有的激进或保守调度器相比，LightLLM在重负载下实现了高达2-3倍的更高吞吐量。

Abstract: The exploration and application of Large Language Models (LLMs) is thriving.
To reduce deployment costs, continuous batching has become an essential feature
in current service frameworks. The effectiveness of continuous batching relies
on an accurate estimate of the memory requirements of requests. However, due to
the diversity in request output lengths, existing frameworks tend to adopt
aggressive or conservative schedulers, which often result in significant
overestimation or underestimation of memory consumption. Consequently, they
suffer from harmful request evictions or prolonged queuing times, failing to
achieve satisfactory throughput under strict Service Level Agreement (SLA)
guarantees (a.k.a. goodput), across various LLM application scenarios with
differing input-output length distributions. To address this issue, we propose
a novel Past-Future scheduler that precisely estimates the peak memory
resources required by the running batch via considering the historical
distribution of request output lengths and calculating memory occupancy at each
future time point. It adapts to applications with all types of input-output
length distributions, balancing the trade-off between request queuing and
harmful evictions, thereby consistently achieving better goodput. Furthermore,
to validate the effectiveness of the proposed scheduler, we developed a
high-performance LLM serving framework, LightLLM, that implements the
Past-Future scheduler. Compared to existing aggressive or conservative
schedulers, LightLLM demonstrates superior goodput, achieving up to 2-3$\times$
higher goodput than other schedulers under heavy loads. LightLLM is open source
to boost the research in such direction (https://github.com/ModelTC/lightllm).

</details>


### [255] [Cross-Timeslot Optimization for Distributed GPU Inference Using Reinforcement Learning](https://arxiv.org/abs/2507.10259)
*Chengze Du,Zhiwei Yu,Heng Xu,Haojie Wang,Bo liu,Jialong Li*

Main category: cs.DC

TL;DR: TORTA是一个时空调度框架，通过两层架构优化LLM服务的GPU推理，提高了效率并降低了成本。


<details>
  <summary>Details</summary>
Motivation: 现有LLM服务调度系统依赖瞬时状态进行决策，未考虑任务需求和资源可用性的时变性，导致GPU利用率低、任务迁移开销大、系统响应慢。本研究旨在解决这些问题。

Method: TORTA采用两层架构：宏观调度器利用强化学习和最优传输协调跨区域任务分配，微观分配器则在区域内优化任务到服务器的分配，以降低延迟和切换成本。

Result: 实验结果表明，TORTA相较于现有基线方法，平均推理响应时间缩短了高达15%，负载均衡提高了约4-5%，总运营成本降低了10-20%。

Conclusion: TORTA通过引入时空调度框架，结合了长期工作负载模式和短期执行约束，实现了更优的GPU利用率和系统性能。

Abstract: The rapid growth of large language model (LLM) services imposes increasing
demands on distributed GPU inference infrastructure. Most existing scheduling
systems rely on the current system state to make decisions, without considering
how task demand and resource availability evolve over time. This lack of
temporal awareness leads to inefficient GPU utilization, high task migration
overhead, and poor system responsiveness under dynamic workloads. In this work,
we identify the fundamental limitations of these instantaneous-state-only
scheduling approaches and propose Temporal Optimal Resource scheduling via
Two-layer Architecture (TORTA). TORTA introduces a spatiotemporal scheduling
framework that captures both long-term workload patterns and short-term
execution constraints. It adopts a two-layer design: a macro-level scheduler
leverages reinforcement learning and optimal transport to coordinate
inter-region task distribution, while a micro-level allocator refines
task-to-server assignments within each region to reduce latency and switching
costs. Experimental results across multiple network topologies show that TORTA
reduces average inference response time by up to 15\%, improves load balance by
approximately 4-5\%, and cuts total operational cost by 10-20\% compared to
state-of-the-art baseline methods.

</details>


### [256] [FalconFS: Distributed File System for Large-Scale Deep Learning Pipeline](https://arxiv.org/abs/2507.10367)
*Jingwei Xu,Junbin Kang,Mingkai Dong,Mingyu Liu,Lu Zhang,Shaohong Guo,Ziyan Qiu,Mingzhen You,Ziyi Tian,Anqi Yu,Tianhong Ding,Xinwei Hu,Haibo Chen*

Main category: cs.DC

TL;DR: 客户端无状态的DFS FalconFS通过服务器端路径解析和并发请求合并，在深度学习任务中实现了显著的吞吐量提升，并已在生产环境中成功应用。


<details>
  <summary>Details</summary>
Motivation: 研究发现，传统的客户端元数据缓存机制在深度学习流水线中不仅效果不佳，反而消耗了宝贵的内存资源，因此需要一种优化的DFS方案。

Method: FalconFS提出了一种面向深度学习流水线的、客户端无状态的分布式文件系统（DFS）架构。它摒弃了客户端缓存，通过服务器端混合元数据索引和惰性命名空间复制来高效地解析路径。此外，通过并发请求合并提升了服务器并发能力，并通过VFS快捷方式简化了部署。

Result: 与CephFS和Lustre相比，FalconFS在小文件读写方面实现了高达5.72倍的吞吐量提升，在深度学习模型训练方面实现了高达12.81倍的吞吐量提升。

Conclusion: FalconFS已被部署于华为自动驾驶系统的生产环境中，为1万个NPU提供了一年的稳定服务，证明了其在实际应用中的有效性和鲁棒性。

Abstract: Client-side metadata caching has long been considered an effective method for
accelerating metadata operations in distributed file systems (DFSs). However,
we have found that client-side state (e.g., caching) is not only ineffective
but also consumes valuable memory resources in the deep learning pipelines. We
thus propose FalconFS, a DFS optimized for deep learning pipelines with the
stateless-client architecture. Specifically, instead of performing client-side
path resolution and caching, FalconFS efficiently resolves paths on the server
side using hybrid metadata indexing and lazy namespace replication. FalconFS
also boosts server concurrency with concurrent request merging and provides
easy deployment with VFS shortcut. Evaluations against CephFS and Lustre show
that FalconFS achieves up to 5.72$\times$ throughput for small file read/write
and up to 12.81$\times$ throughput for deep learning model training. FalconFS
has been running in Huawei autonomous driving system's production environment
with 10,000 NPUs for one year.

</details>


### [257] [Zorse: Optimizing LLM Training Efficiency on Heterogeneous GPU Clusters](https://arxiv.org/abs/2507.10392)
*Runsheng Benson Guo,Utkarsh Anand,Khuzaima Daudjee,Rathijit Sen*

Main category: cs.DC

TL;DR: Zorse 是第一个统一了异构集群训练中流水线并行、数据并行、不对称分区和跨 GPU 类型数据并行的系统，并包含一个自动规划器，能在实际应用中显著提升训练效率。


<details>
  <summary>Details</summary>
Motivation: 为了克服许多组织在训练大型语言模型时面临的 GPU 可用性和成本限制，提出了利用不同代 GPU 的异构集群。然而，在异构集群上进行训练存在负载均衡、内存优化和通信效率等挑战。

Method: 提出了一种名为 Zorse 的系统，该系统集成了流水线并行和数据并行，并包含一个自动规划器，可以为给定工作负载配置训练策略。该系统允许灵活地将 GPU 分区为不对称流水线并行阶段，并将异构 GPU 包含在同一数据并行组中。

Result: Zorse 在异构训练场景中显著优于最先进的系统。

Conclusion: Zorse 通过集成流水线并行和数据并行，并允许灵活配置并行策略（包括不对称流水线和跨 GPU 类型的数据并行），解决了异构集群训练的挑战，并在评估中显著优于现有系统。

Abstract: Large language models (LLMs) require vast amounts of GPU compute to train,
but limited availability and high costs of GPUs make homogeneous clusters
impractical for many organizations. Instead, assembling heterogeneous clusters
by pooling together GPUs of different generations allows them to achieve higher
aggregate compute and make use of all available GPUs. However, training on
heterogeneous clusters presents several challenges, including load balancing
across GPUs, optimizing memory usage to accommodate varying memory capacities,
and ensuring communication-efficient training over diverse network
interconnects potentially spanning multiple datacenters. In this paper, we make
the case that efficient training on heterogeneous clusters requires (1) the
integration of pipeline parallelism and data parallelism in a manner that is
both communication- and memory-efficient, and (2) a more adaptable
configuration of pipeline and data parallelism, which includes the capability
to flexibly partition GPUs into asymmetric pipeline parallel stages and to
incorporate heterogeneous GPUs within the same data parallelism group. We
propose Zorse, the first system to unify all these capabilities while
incorporating a planner that automatically configures training strategies for a
given workload. Our evaluation shows that Zorse significantly outperforms
state-of-the-art systems in heterogeneous training scenarios.

</details>


### [258] [Consensus, Inconsistency, Emergence: what's paraconsistency got to do with it?](https://arxiv.org/abs/2507.10413)
*Gabriel Rocha*

Main category: cs.DC

TL;DR: 该论文探讨了分布式共识问题，证明了 FLP 不可能性定理在广义计算模型下依然成立，并运用复杂系统理论分析了不一致性作为共识涌现特征的可能性。研究还表明，非矛盾逻辑可能以平凡性而非不一致性作为涌现特征。


<details>
  <summary>Details</summary>
Motivation: 该论文旨在探讨在分布式系统中，共识问题的局限性以及不一致性在其中的作用。具体来说，它试图证明 FLP 不可能性定理在更广义的计算模型下仍然成立，并运用复杂系统理论来分析不一致性作为共识涌现特征的可能性。此外，该论文还研究了非矛盾逻辑在处理分布式系统共识问题上的潜力，以及是否可能开发出能够进行非矛盾推理的共识算法。

Method: 该论文首先论证了 FLP 不可能性定理在通过预言机进行计算的广义定义下仍然成立。然后，它利用复杂系统理论中的工具，通过检查系统如何在不同阶段之间转换，来论证不一致性可能是分布式系统上共识的涌现特征。该论文还在此复杂系统框架下，对非矛盾逻辑进行了研究，并认为不一致性不是这些逻辑的涌现特征，但平凡性可能是。最后，该论文还探讨了开发能够进行非矛盾推理的共识算法的可能性。

Result: 该论文的主要结果是，FLP 不可能性定理在通过预言机进行计算的广义定义下仍然成立。此外，研究表明不一致性可能是分布式系统上共识的涌现特征，而非矛盾逻辑虽然不以不一致性作为涌现特征，但可能以平凡性作为涌现特征。

Conclusion: 该论文认为，即使在通过预言机进行计算的广义定义下，FLP 不可能性定理也成立。该论文还提出，在相同的复杂系统框架下，通过检查系统如何转换阶段，不一致性可能是分布式系统上共识的涌现特征。对于非矛盾逻辑而言，不一致性不是一种涌现特征，但平凡性可能是一种涌现特征。最后，该论文还探讨了开发能够进行非矛盾推理的共识算法的可能性。

Abstract: The consensus problem, briefly stated, consists of having processes in an
asynchronous distributed system agree on a value. It is widely known that the
consensus problem does not have a deterministic solution that ensures both
termination and consistency, if there is at least one faulty process in the
system. This result, known as the FLP impossibility theorem, led to several
generalizations and developments in theoretical distributed computing. This
paper argues that the FLP impossibility theorem holds even under a generalized
definition of computation through oracles. Furthermore, using a theoretical
machinery from complex systems, this paper also posits that inconsistency may
be an emergent feature of consensus over distributed systems by examining how a
system transitions phases. Under the same complex systems framework, this paper
examines paraconsistent logics, arguing that while inconsistency is not an
emergent feature for these logics, triviality may be. Lastly, some attention is
given to the possibility of developing consensus algorithms capable of
paraconsistent reasoning.

</details>


### [259] [Efficient Federated Learning with Heterogeneous Data and Adaptive Dropout](https://arxiv.org/abs/2507.10430)
*Ji Liu,Beichen Ma,Yang Zhou,Jingbo Zhou,Ruoming Jin,Dejing Dou,Huaiyu Dai,Haixun Wang,Patrick Valduriez*

Main category: cs.DC

TL;DR: 提出FedDHAD联邦学习框架，通过动态异构模型聚合（FedDH）和自适应丢弃（FedAD）两种新方法，解决联邦学习中的数据异构性和设备异构性问题，提升模型准确性和训练效率。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）在模型训练中面临数据分布和异构性挑战，特别是边缘设备上非独立同分布（non-IID）数据可能导致显著的准确率下降；同时，边缘设备有限的计算和通信能力增加了掉队者的可能性，导致模型收敛缓慢。

Method: FedDH动态聚合方法基于非独立同分布数据的程度来调整聚合过程中每个本地模型的权重，以应对统计数据异构性；FedAD通过神经元自适应操作响应异构设备，以提高准确性并实现出色的效率。

Result: 实验结果表明，FedDHAD框架在准确性（高出6.7%）、效率（快2.02倍）和计算成本（小15.0%）方面均显著优于现有方法。

Conclusion: FedDHAD框架通过动态异构模型聚合（FedDH）和自适应丢弃（FedAD）两种新颖方法，能够显著优于最先进的解决方案，在准确性（高出6.7%）、效率（快2.02倍）和计算成本（小15.0%）方面表现更佳。

Abstract: Federated Learning (FL) is a promising distributed machine learning approach
that enables collaborative training of a global model using multiple edge
devices. The data distributed among the edge devices is highly heterogeneous.
Thus, FL faces the challenge of data distribution and heterogeneity, where
non-Independent and Identically Distributed (non-IID) data across edge devices
may yield in significant accuracy drop. Furthermore, the limited computation
and communication capabilities of edge devices increase the likelihood of
stragglers, thus leading to slow model convergence. In this paper, we propose
the FedDHAD FL framework, which comes with two novel methods: Dynamic
Heterogeneous model aggregation (FedDH) and Adaptive Dropout (FedAD). FedDH
dynamically adjusts the weights of each local model within the model
aggregation process based on the non-IID degree of heterogeneous data to deal
with the statistical data heterogeneity. FedAD performs neuron-adaptive
operations in response to heterogeneous devices to improve accuracy while
achieving superb efficiency. The combination of these two methods makes FedDHAD
significantly outperform state-of-the-art solutions in terms of accuracy (up to
6.7% higher), efficiency (up to 2.02 times faster), and computation cost (up to
15.0% smaller).

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [260] [Think Clearly: Improving Reasoning via Redundant Token Pruning](https://arxiv.org/abs/2507.08806)
*Daewon Choi,Jimin Lee,Jihoon Tack,Woomin Song,Saket Dingliwal,Sai Muralidhar Jayanthi,Bhavana Ganesh,Jinwoo Shin,Aram Galstyan,Sravan Babu Bodapati*

Main category: cs.AI

TL;DR: 该研究通过识别并移除大型语言模型推理过程中的冗余信息，显著提高了其在数学等推理密集型任务上的准确性，且无需重新训练。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在长篇推理中存在推理路径冗余的问题，这会分散注意力并影响准确性。

Method: 通过测量 token-level 注意力分数来识别推理冗余，并提出了一种结构感知剪枝方法，优先移除贡献度低的推理块中的 token，然后移除注入的思考结束指令并恢复推理。

Result: 该方法在推理密集型基准测试中显著提高了整体准确性，尤其是在数学竞赛基准测试中。

Conclusion: 通过去除冗余信息，该方法显著提高了模型在推理密集型基准测试中的整体准确性，尤其是在数学竞赛基准测试（如AIME和AMC）上表现强劲。

Abstract: Recent large language models have shown promising capabilities in long-form
reasoning, following structured chains of thought before arriving at a final
answer. However, we observe that these reasoning paths tend to include
substantial redundancy; analyzing attention patterns reveals that attention
scores are widely scattered, particularly incorrect answers exhibit greater
attention sparsity. In this paper, we demonstrate that deliberately removing
this redundancy in the reasoning process significantly improves performance
through clear thinking, i.e., removing distraction. Specifically, we
systematically identify reasoning redundancy by measuring token-level attention
scores to a special end-of-thinking token, which is appended to an explicit
instruction inserted to conclude each intermediate reasoning step. Furthermore,
we propose structure-aware pruning that prioritizes removing tokens in
low-contributing reasoning chunks over individual tokens. After evicting
redundant tokens, we remove the injected end-of-thinking instruction, then
resume the reasoning generation. We demonstrate that our method significantly
improves overall accuracy across reasoning-intensive benchmarks without any
training involved. In particular, our method shows strong performance on
challenging mathematical competition benchmarks such as AIME and AMC, where
reasoning redundancy is more prevalent.

</details>


### [261] [A New Approach for Multicriteria Assessment in the Ranking of Alternatives Using Cardinal and Ordinal Data](https://arxiv.org/abs/2507.08875)
*Fuh-Hwa Franklin Liu,Su-Chuan Shih*

Main category: cs.AI

TL;DR: 该研究提出了一种新的多标准评估（MCA）方法，结合了两种虚拟差距分析（VGA）模型，以克服现有方法的局限性，并提高评估的效率、公平性和准确性，适用于定性和定量标准以及异质性数据。


<details>
  <summary>Details</summary>
Motivation: 为了应对多标准评估（MCA）方法（如DEA、SFA和MCDM）在处理定性和定量标准、异质性假设以及主观性影响时遇到的挑战，并确定最合适的替代方案。

Method: 提出了一种结合两种虚拟差距分析（VGA）模型的新型多标准评估（MCA）方法。该方法基于线性规划，并包含两个全面的数值示例来展示其准确性和透明度。

Result: 该研究提出的新型MCA方法提高了效率和公平性，确保了评估的全面性和可靠性，并提供了两个全面的数值示例来证明其准确性和透明度。

Conclusion: 该研究提出了一种结合两种虚拟差距分析（VGA）模型的新型多标准评估（MCA）方法，以解决现有MCA方法在处理定性和定量标准、异质性假设以及主观性影响方面的挑战。该方法基于线性规划，旨在提高评估的效率和公平性，使其更全面和可靠。

Abstract: Modern methods for multi-criteria assessment (MCA), such as Data Envelopment
Analysis (DEA), Stochastic Frontier Analysis (SFA), and Multiple Criteria
Decision-Making (MCDM), are utilized to appraise a collection of
Decision-Making Units (DMUs), also known as alternatives, based on several
criteria. These methodologies inherently rely on assumptions and can be
influenced by subjective judgment to effectively tackle the complex evaluation
challenges in various fields. In real-world scenarios, it is essential to
incorporate both quantitative and qualitative criteria as they consist of
cardinal and ordinal data. Despite the inherent variability in the criterion
values of different alternatives, the homogeneity assumption is often employed,
significantly affecting evaluations. To tackle these challenges and determine
the most appropriate alternative, we propose a novel MCA approach that combines
two Virtual Gap Analysis (VGA) models. The VGA framework, rooted in linear
programming, is pivotal in the MCA methodology. This approach improves
efficiency and fairness, ensuring that evaluations are both comprehensive and
dependable, thus offering a strong and adaptive solution. Two comprehensive
numerical examples demonstrate the accuracy and transparency of our proposed
method. The goal is to encourage continued advancement and stimulate progress
in automated decision systems and decision support systems.

</details>


### [262] [GenAI-based Multi-Agent Reinforcement Learning towards Distributed Agent Intelligence: A Generative-RL Agent Perspective](https://arxiv.org/abs/2507.09495)
*Hang Wang,Junshan Zhang*

Main category: cs.AI

TL;DR: 生成式AI驱动的强化学习将使多智能体系统能够预测和协调，而不是仅仅做出反应。


<details>
  <summary>Details</summary>
Motivation: 当前的强化学习方法在应对多智能体系统中的挑战，如指数级增长的联合动作空间、非平稳环境和部分可观测性方面存在局限性，这些方法仍然是被动的，并且在面对新颖场景时表现不佳。

Method: 提出了一种从反应式向生成式AI驱动的强化学习的范式转变，将智能体视为能够合成复杂多智能体动态并基于对未来交互的预测性理解做出预期决策的生成模型。

Result: 该方法能够实现主动决策、通过增强的通信实现无缝协调以及对不断变化场景的动态适应，从而为分布式智能开辟了新的可能性，并有望解决传统被动框架下的协调挑战。

Conclusion: 生成式AI驱动的强化学习范式有望克服多智能体系统的固有挑战，实现从被动响应到主动预测的转变，从而推动分布式智能和协作智能的发展。

Abstract: Multi-agent reinforcement learning faces fundamental challenges that
conventional approaches have failed to overcome: exponentially growing joint
action spaces, non-stationary environments where simultaneous learning creates
moving targets, and partial observability that constrains coordination. Current
methods remain reactive, employing stimulus-response mechanisms that fail when
facing novel scenarios. We argue for a transformative paradigm shift from
reactive to proactive multi-agent intelligence through generative AI-based
reinforcement learning. This position advocates reconceptualizing agents not as
isolated policy optimizers, but as sophisticated generative models capable of
synthesizing complex multi-agent dynamics and making anticipatory decisions
based on predictive understanding of future interactions. Rather than
responding to immediate observations, generative-RL agents can model
environment evolution, predict other agents' behaviors, generate coordinated
action sequences, and engage in strategic reasoning accounting for long-term
dynamics. This approach leverages pattern recognition and generation
capabilities of generative AI to enable proactive decision-making, seamless
coordination through enhanced communication, and dynamic adaptation to evolving
scenarios. We envision this paradigm shift will unlock unprecedented
possibilities for distributed intelligence, moving beyond individual
optimization toward emergent collective behaviors representing genuine
collaborative intelligence. The implications extend across autonomous systems,
robotics, and human-AI collaboration, promising solutions to coordination
challenges intractable under traditional reactive frameworks.

</details>


### [263] [Multi-Actor Generative Artificial Intelligence as a Game Engine](https://arxiv.org/abs/2507.08892)
*Alexander Sasha Vezhnevets,Jayd Matyas,Logan Cross,Davide Paglieri,Minsuk Chang,William A. Cunningham,Simon Osindero,William S. Isaac,Joel Z. Leibo*

Main category: cs.AI

TL;DR: 通过借鉴桌面角色扮演游戏（TTRPG）和实体-组件架构，我们提出了一个灵活的框架来支持生成式人工智能在多参与者环境中的多种应用。该框架通过分离关注点实现了可扩展性和快速迭代，Concordia库是这一理念的例证。


<details>
  <summary>Details</summary>
Motivation: 为了支持生成式人工智能在多参与者环境中的多样化应用（包括社会科学建模、交互叙事和AI评估），需要一个灵活的场景定义框架。现有方法可能无法满足这些不同用例的需求。

Method: 受桌面角色扮演游戏（TTRPG）的启发，采用实体-组件架构模式来构建一个灵活的场景定义框架。在此框架中，游戏主持人（GM）被设计为一个可配置的实体，由组件构成，负责管理环境和生成故事的非玩家驱动部分。

Result: 通过采用实体-组件架构，该方法实现了关注点分离，便于工程师处理底层实现、创建可重用组件，以及由设计师进行组件的组合和配置。这有助于快速迭代、保持模块化和实现可扩展性。Concordia库的演进展示了该哲学如何使用户能够有效地配置满足其特定目标的场景。

Conclusion: 该方法论通过将TTRPG的理念与实体-组件架构相结合，为在多参与者环境中支持各种生成式人工智能应用提供了灵活且可扩展的框架。Concordia库的演进证明了其在配置和执行多样化AI场景方面的有效性。

Abstract: Generative AI can be used in multi-actor environments with purposes ranging
from social science modeling to interactive narrative and AI evaluation.
Supporting this diversity of use cases -- which we classify as Simulationist,
Dramatist, and Evaluationist -- demands a flexible scenario definition
framework. We argue here that a good approach is to take inspiration from
tabletop role-playing games (TTRPGs), where a Game Master (GM) is responsible
for the environment and generates all parts of the story not directly
determined by the voluntary actions of player characters. We argue that the
Entity-Component architectural pattern is useful here. In such a system, the GM
is not a hardcoded computer game but is itself a configurable entity, composed
of components just like any other actor. By design, the approach allows for a
separation between the underlying implementation details handled by an
engineer, the creation of reusable components, and their composition and
configuration managed by a designer who constructs entities from the
components. This separation of concerns is instrumental for achieving rapid
iteration, maintaining modularity, and ultimately to ensure scalability. We
describe the ongoing evolution of the Concordia library in terms of this
philosophy, demonstrating how it allows users to effectively configure
scenarios that align with their specific goals.

</details>


### [264] [SentiDrop: A Multi Modal Machine Learning model for Predicting Dropout in Distance Learning](https://arxiv.org/abs/2507.10421)
*Meriem Zerkouk,Miloud Mihoubi,Belkacem Chikhaoui*

Main category: cs.AI

TL;DR: 一种结合BERT情感分析和XGBoost特征选择的新模型，可更准确地预测远程学习中的学生辍学情况。


<details>
  <summary>Details</summary>
Motivation: 学校辍学是远程学习中的一个严重问题，早期发现对于有效的干预和学生的坚持至关重要。利用现有的教育数据预测学生辍学是学习分析中一个广泛研究的课题。我们的合作伙伴的远程学习平台强调了整合包括社会人口统计学数据、行为数据和情感分析在内的多样化数据源的重要性，以准确预测辍学风险。

Method: 本研究提出了一种新颖的模型，结合了使用双向Encoder表示Transformer（BERT）模型对学生评论进行情感分析，以及对社会人口统计学和行为数据进行分析的极端梯度增强（XGBoost）。通过对学生评论进行微调BERT以捕捉细微的情感，然后将其与使用XGBoost中的特征重要性技术选择的关键特征合并。

Result: 该模型在下一学年的未见过的数据上进行了测试，准确率达到了84%，而基线模型为82%。此外，该模型在精确率和F1分数等其他指标上也表现出优越的性能。

Conclusion: 该模型通过结合BERT的情感分析和XGBoost的特征选择，在预测学校辍学方面取得了84%的准确率，优于基线模型（82%），并在精确率和F1分数等其他指标上也表现出色。

Abstract: School dropout is a serious problem in distance learning, where early
detection is crucial for effective intervention and student perseverance.
Predicting student dropout using available educational data is a widely
researched topic in learning analytics. Our partner's distance learning
platform highlights the importance of integrating diverse data sources,
including socio-demographic data, behavioral data, and sentiment analysis, to
accurately predict dropout risks. In this paper, we introduce a novel model
that combines sentiment analysis of student comments using the Bidirectional
Encoder Representations from Transformers (BERT) model with socio-demographic
and behavioral data analyzed through Extreme Gradient Boosting (XGBoost). We
fine-tuned BERT on student comments to capture nuanced sentiments, which were
then merged with key features selected using feature importance techniques in
XGBoost. Our model was tested on unseen data from the next academic year,
achieving an accuracy of 84\%, compared to 82\% for the baseline model.
Additionally, the model demonstrated superior performance in other metrics,
such as precision and F1-score. The proposed method could be a vital tool in
developing personalized strategies to reduce dropout rates and encourage
student perseverance

</details>


### [265] [BioAnalyst: A Foundation Model for Biodiversity](https://arxiv.org/abs/2507.09080)
*Athanasios Trantas,Martino Mensio,Stylianos Stasinos,Sebastian Gribincea,Taimur Khan,Damian Podareanu,Aliene van der Veen*

Main category: cs.AI

TL;DR: 研究者们开发了一个名为BioAnalyst的人工智能基础模型，用于生物多样性分析和保护。该模型在大量多模态数据上进行了训练，能够适应多种生态预测任务，并在数据稀缺的情况下表现出色，为生态学研究和保护工作提供了强大的新工具。


<details>
  <summary>Details</summary>
Motivation: 生物多样性的加速丧失给生态学研究和保护策略带来了严峻的挑战。为了应对这些挑战，需要综合的监测、预测和保护规划能力。人工智能基础模型（FMs）在处理大规模数据集和适应不同任务方面展现出巨大潜力，因此，开发一个专门针对生物多样性分析和保护规划的FM具有重要意义。

Method: BioAnalyst模型采用了基于Transformer的架构，并在包含物种出现记录、遥感指标、气候和环境变量的广泛多模态数据集上进行了预训练。该模型被设计为具有适应性，可以针对物种分布建模、栖息地适宜性评估、入侵物种检测和种群趋势预测等多种下游任务进行微调。

Result: 研究通过在两个下游应用场景中对模型性能的评估，证明了BioAnalyst相对于现有方法的通用性，尤其是在数据稀疏的场景下。结果显示，BioAnalyst在这些场景下设定了新的准确性基准，特别是在生态预测方面。

Conclusion: 该研究介绍了BioAnalyst，一个针对生物多样性分析和保护规划的开创性基础模型。通过在广泛的多模态数据集上进行预训练，并采用基于Transformer的架构，BioAnalyst能够适应多种下游任务，例如物种分布建模、栖息地适宜性评估、入侵物种检测和种群趋势预测。研究表明，BioAnalyst在数据稀疏的情况下表现优于现有方法，为生态预测设定了新的准确性基准。通过公开模型及其微调工作流程，该研究旨在促进生物多样性建模方面的合作，并推动人工智能驱动的解决方案在应对紧迫的生态挑战中的应用。

Abstract: The accelerating loss of biodiversity presents critical challenges for
ecological research and conservation strategies. The preservation of
biodiversity is paramount for maintaining ecological balance and ensuring the
sustainability of ecosystems. However, biodiversity faces numerous threats,
including habitat loss, climate change, and the proliferation of invasive
species. Addressing these and other ecology-related challenges, both at local
and global scales, requires comprehensive monitoring, predictive and
conservation planning capabilities. Artificial Intelligence (AI) Foundation
Models (FMs) have gained significant momentum in numerous scientific domains by
leveraging vast datasets to learn general-purpose representations adaptable to
various downstream tasks. This paradigm holds immense promise for biodiversity
conservation. In response, we introduce BioAnalyst, the first Foundation Model
tailored for biodiversity analysis and conservation planning. BioAnalyst
employs a transformer-based architecture, pre-trained on extensive multi-modal
datasets encompassing species occurrence records, remote sensing indicators,
climate and environmental variables. BioAnalyst is designed for adaptability,
allowing for fine-tuning of a range of downstream tasks, such as species
distribution modelling, habitat suitability assessments, invasive species
detection, and population trend forecasting. We evaluate the model's
performance on two downstream use cases, demonstrating its generalisability
compared to existing methods, particularly in data-scarce scenarios for two
distinct use-cases, establishing a new accuracy baseline for ecological
forecasting. By openly releasing BioAnalyst and its fine-tuning workflows to
the scientific community, we aim to foster collaborative efforts in
biodiversity modelling and advance AI-driven solutions to pressing ecological
challenges.

</details>


### [266] [Measuring the Impact of Early-2025 AI on Experienced Open-Source Developer Productivity](https://arxiv.org/abs/2507.09089)
*Joel Becker,Nate Rush,Elizabeth Barnes,David Rein*

Main category: cs.AI

TL;DR: AI工具的使用减慢了开发者的速度，增加了19%的完成时间，这与之前的预测相反。


<details>
  <summary>Details</summary>
Motivation: 尽管AI工具被广泛采用，但其在实际软件开发中的影响仍有待研究。

Method: 进行了一项随机对照试验（RCT），涉及16名有中等AI经验的开发人员，他们完成了246个任务，并允许或禁止使用2025年初的AI工具，以了解AI工具对经验丰富的开源开发人员生产力的影响。

Result: 与预期相反，AI工具的使用将任务完成时间增加了19%。研究还收集并评估了20个可能导致这种减慢效应的因素，例如项目规模、质量标准以及开发人员对AI工具的先验经验。

Conclusion: AI工具的使用实际上增加了开发者的完成时间（19%），这与经济学（预测缩短39%）和机器学习（预测缩短38%）专家的预测相反。

Abstract: Despite widespread adoption, the impact of AI tools on software development
in the wild remains understudied. We conduct a randomized controlled trial
(RCT) to understand how AI tools at the February-June 2025 frontier affect the
productivity of experienced open-source developers. 16 developers with moderate
AI experience complete 246 tasks in mature projects on which they have an
average of 5 years of prior experience. Each task is randomly assigned to allow
or disallow usage of early 2025 AI tools. When AI tools are allowed, developers
primarily use Cursor Pro, a popular code editor, and Claude 3.5/3.7 Sonnet.
Before starting tasks, developers forecast that allowing AI will reduce
completion time by 24%. After completing the study, developers estimate that
allowing AI reduced completion time by 20%. Surprisingly, we find that allowing
AI actually increases completion time by 19%--AI tooling slowed developers
down. This slowdown also contradicts predictions from experts in economics (39%
shorter) and ML (38% shorter). To understand this result, we collect and
evaluate evidence for 20 properties of our setting that a priori could
contribute to the observed slowdown effect--for example, the size and quality
standards of projects, or prior developer experience with AI tooling. Although
the influence of experimental artifacts cannot be entirely ruled out, the
robustness of the slowdown effect across our analyses suggests it is unlikely
to primarily be a function of our experimental design.

</details>


### [267] [Hide-and-Shill: A Reinforcement Learning Framework for Market Manipulation Detection in Symphony-a Decentralized Multi-Agent System](https://arxiv.org/abs/2507.09179)
*Ronghua Shi,Yiou Liu,Xinyu Ying,Yang Tan,Yuchun Feng,Lynn Ai,Bill Shi,Xuhui Wang,Zhuang Liu*

Main category: cs.AI

TL;DR: Hide-and-Shill是一个基于多智能体强化学习的DeFi操纵检测框架，通过结合GRPO、基于理论的奖励函数和多模态数据，提高了检测的准确性和鲁棒性，并在Symphony系统上进行了验证。


<details>
  <summary>Details</summary>
Motivation: 去中心化金融（DeFi）带来了无需许可的金融创新，但也导致了前所未有的市场操纵。在缺乏中心化监管的情况下，恶意行为者在各种平台协调“拉高出货”宣传活动和操纵方案。

Method: 提出了一种去中心化操纵检测的多智能体强化学习（MARL）框架，将操纵者和检测者之间的交互建模为动态对抗博弈。该框架使用延迟代币价格反应作为金融指标来识别可疑模式。该方法引入了三个创新：(1) 组相对策略优化（GRPO）以增强稀疏奖励和部分可观察环境中的学习稳定性；(2) 受理性预期和信息不对称启发的基于理论的奖励函数，区分价格发现和操纵噪音；(3) 多模态智能体管道，整合了基于LLM的语义特征、社交图信号和链上市场数据以进行知情决策。该框架集成在Symphony系统中，一个去中心化的多智能体架构，支持点对点智能体执行和通过分布式日志进行信任感知学习，支持链可验证评估。Symphony促进战略行为者之间的对抗共同进化，并在没有中心化预言机的情况下保持强大的操纵检测能力，从而能够对全球DeFi生态系统进行实时监控。

Result: 在100,000个真实世界的讨论片段上进行训练，并在对抗模拟中进行了验证，“Hide-and-Shill”在检测准确性和因果归因方面取得了顶级性能。

Conclusion: 这项工作将多智能体系统与金融监控相结合，推动了一种新的去中心化市场情报范式。

Abstract: Decentralized finance (DeFi) has introduced a new era of permissionless
financial innovation but also led to unprecedented market manipulation. Without
centralized oversight, malicious actors coordinate shilling campaigns and
pump-and-dump schemes across various platforms. We propose a Multi-Agent
Reinforcement Learning (MARL) framework for decentralized manipulation
detection, modeling the interaction between manipulators and detectors as a
dynamic adversarial game. This framework identifies suspicious patterns using
delayed token price reactions as financial indicators.Our method introduces
three innovations: (1) Group Relative Policy Optimization (GRPO) to enhance
learning stability in sparse-reward and partially observable settings; (2) a
theory-based reward function inspired by rational expectations and information
asymmetry, differentiating price discovery from manipulation noise; and (3) a
multi-modal agent pipeline that integrates LLM-based semantic features, social
graph signals, and on-chain market data for informed decision-making.The
framework is integrated within the Symphony system, a decentralized multi-agent
architecture enabling peer-to-peer agent execution and trust-aware learning
through distributed logs, supporting chain-verifiable evaluation. Symphony
promotes adversarial co-evolution among strategic actors and maintains robust
manipulation detection without centralized oracles, enabling real-time
surveillance across global DeFi ecosystems.Trained on 100,000 real-world
discourse episodes and validated in adversarial simulations, Hide-and-Shill
achieves top performance in detection accuracy and causal attribution. This
work bridges multi-agent systems with financial surveillance, advancing a new
paradigm for decentralized market intelligence. All resources are available at
the Hide-and-Shill GitHub repository to promote open research and
reproducibility.

</details>


### [268] [When Developer Aid Becomes Security Debt: A Systematic Analysis of Insecure Behaviors in LLM Coding Agents](https://arxiv.org/abs/2507.09329)
*Matous Kozak,Roshanak Zilouchian Moghaddam,Siva Sivaraman*

Main category: cs.AI

TL;DR: LLM编码代理存在安全风险，但可通过安全设计和缓解策略（如GPT-4.1的安全意识）来提高安全性。


<details>
  <summary>Details</summary>
Motivation: LLM编码代理在软件开发中的广泛部署，但其安全影响尚不明确，可能引入不安全的实践。

Method: 对五个最先进的模型（GPT-4o、GPT-4.1、Claude变体）在93个真实世界的软件设置任务上，分析超过12,000个动作，以系统性地评估其安全性。开发了一个高精度的检测系统，识别了四大类漏洞，并评估了反馈机制和安全提醒等缓解策略。

Result: 21%的代理轨迹包含不安全操作，模型在安全行为上存在显著差异。检测系统识别出四大类漏洞，其中信息暴露（CWE-200）最为普遍。GPT-4.1在缓解策略上表现出色，成功率达96.8%。

Conclusion: LLM驱动的编码代理在软件开发中带来了严峻的安全风险，但通过引入安全意识设计和有效的缓解策略，可以提高其安全性。

Abstract: LLM-based coding agents are rapidly being deployed in software development,
yet their security implications remain poorly understood. These agents, while
capable of accelerating software development, may inadvertently introduce
insecure practices. We conducted the first systematic security evaluation of
autonomous coding agents, analyzing over 12,000 actions across five
state-of-the-art models (GPT-4o, GPT-4.1, Claude variants) on 93 real-world
software setup tasks. Our findings reveal significant security concerns: 21% of
agent trajectories contained insecure actions, with models showing substantial
variation in security behavior. We developed a high-precision detection system
that identified four major vulnerability categories, with information exposure
(CWE-200) being the most prevalent one. We also evaluated mitigation strategies
including feedback mechanisms and security reminders with various effectiveness
between models. GPT-4.1 demonstrated exceptional security awareness with 96.8%
mitigation success. Our work provides the first comprehensive framework for
evaluating coding agent security and highlights the need for security-aware
design of next generation LLM-based coding agents.

</details>


### [269] [A Taxonomy of Omnicidal Futures Involving Artificial Intelligence](https://arxiv.org/abs/2507.09369)
*Andrew Critch,Jacob Tsimerman*

Main category: cs.AI

TL;DR: 本报告对人工智能可能导致的所有人类灭绝事件进行了分类和举例，旨在引起公众关注并推动预防措施以避免此类风险。


<details>
  <summary>Details</summary>
Motivation: 通过公开展示人工智能可能导致的所有人类灭绝的事件的可能性，以期获得公众支持，从而采取预防措施，避免灾难性风险。

Method: 本报告介绍了潜在的、由人工智能导致的所有人类灭绝的事件的分类法和示例。

Result: 报告旨在通过揭示潜在的灭绝情景来推动针对人工智能灾难性风险的预防措施。

Conclusion: 本报告提出了一个关于人工智能可能导致的所有人类灭绝的事件的分类法和示例。

Abstract: This report presents a taxonomy and examples of potential omnicidal events
resulting from AI: scenarios where all or almost all humans are killed. These
events are not presented as inevitable, but as possibilities that we can work
to avoid. Insofar as large institutions require a degree of public support in
order to take certain actions, we hope that by presenting these possibilities
in public, we can help to support preventive measures against catastrophic
risks from AI.

</details>


### [270] [EduFlow: Advancing MLLMs' Problem-Solving Proficiency through Multi-Stage, Multi-Perspective Critique](https://arxiv.org/abs/2507.09374)
*Chenglin Zhu,Tao Zhang,Chong Li,Mingan Lin,Zenan Zhou,Jian Xie*

Main category: cs.AI

TL;DR: EduFlow 是一个用于教育科学推理的框架，通过 EduPRM 和 EduMCTS 解决了 MLLMs 在科学任务中的不足，提高了推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型（MLLMs）在需要多步推理和可解释性的科学任务方面表现不佳，存在科学推理模式不足、多步推理全局连贯性差以及缺乏反思性自我纠正等问题，在结构化科学环境中不可靠。

Method: 提出了一种名为 EduFlow 的端到端框架，包括数据选择、基于 MCTS 的轨迹构建、模型训练和输出优化。核心是 EduPRM，一个过程感知奖励模型，通过标签和理由来评估推理步骤。EduMCTS 是一个领域自适应搜索框架，引入了引导性的自举动作，例如促进反思性错误纠正的自我反思机制。通过自洽性和拒绝采样构建了大规模教育推理轨迹数据集 EduMCTS-160K。

Result: 实验证明，EduFlow 提高了推理的一致性和连贯性。

Conclusion: EduFlow 框架通过 EduPRM 和 EduMCTS 改进了 MLLMs 在科学任务中的表现，提高了推理的一致性和连贯性。

Abstract: Multimodal large language models (MLLMs) still perform poorly on scientific
tasks, particularly those requiring multi-step and interpretable reasoning.
Their limitations include insufficient scientific reasoning patterns, lack of
global coherence in multi-step inference, and the absence of reflective
self-correction, making them unreliable in structured scientific contexts. We
introduce EduFlow, the first end-to-end framework that covers the full pipeline
of educational scientific reasoning, including data selection, MCTS-based
trajectory construction, model training, and output optimization. At its core
is EduPRM, a process-aware reward model that critiques reasoning steps with
tags and justifications. EduPRM is trained via curriculum learning on three
complementary supervision sources: MCTS-guided trajectories, error-injected
critiques, and teacher-student dialogues, enabling dynamic adaptation to
multi-stage problem solving and iterative refinement during inference. We
further propose EduMCTS, a domain-adapted search framework that introduces
bootstrapping actions specifically designed for educational reasoning, such as
a self-reflection mechanism that promotes reflective error correction. It
further leverages EduPRM's fine-grained feedback to guide the search toward
higher-quality reasoning trajectories. By applying self-consistency and
rejection sampling, we constructed EduMCTS-160K, a large-scale dataset of
educational reasoning trajectories. Extensive experiments demonstrate that
EduFlow enhances reasoning consistency and coherence. Code, data, and models
will be released.

</details>


### [271] [Sound and Complete Neuro-symbolic Reasoning with LLM-Grounded Interpretations](https://arxiv.org/abs/2507.09751)
*Bradley P. Allen,Prateek Chhikara,Thomas Macaulay Ferguson,Filip Ilievski,Paul Groth*

Main category: cs.AI

TL;DR: LLM 在逻辑一致性方面存在问题，但本研究提出了一种将 LLM 集成到非一致性逻辑的形式语义中的方法，该方法提供了神经符号推理的理论框架，并保留了逻辑的可靠性和完备性。


<details>
  <summary>Details</summary>
Motivation: 如何利用大型语言模型广泛的参数化知识进行形式推理，尽管它们在生成输出时存在不一致的问题。

Method: 将大型语言模型直接集成到非一致性逻辑的形式语义的解释函数中。

Result: 通过使用来自几个简短事实检验基准的数据集来评估该函数，提供了该方法可行性的实验证据。

Conclusion: 该方法为神经符号推理提供了一个理论框架，该框架利用了大型语言模型的知识，同时保留了底层逻辑的可靠性和完备性属性。

Abstract: Large language models (LLMs) have demonstrated impressive capabilities in
natural language understanding and generation, but they exhibit problems with
logical consistency in the output they generate. How can we harness LLMs'
broad-coverage parametric knowledge in formal reasoning despite their
inconsistency? We present a method for directly integrating an LLM into the
interpretation function of the formal semantics for a paraconsistent logic. We
provide experimental evidence for the feasibility of the method by evaluating
the function using datasets created from several short-form factuality
benchmarks. Unlike prior work, our method offers a theoretical framework for
neuro-symbolic reasoning that leverages an LLM's knowledge while preserving the
underlying logic's soundness and completeness properties.

</details>


### [272] [Knowledge Conceptualization Impacts RAG Efficacy](https://arxiv.org/abs/2507.09389)
*Chris Davis Jaldi,Anmol Saini,Elham Ghiasi,O. Divine Eziolise,Cogan Shimizu*

Main category: cs.AI

TL;DR: 本研究探讨了知识表示（结构、复杂性）如何影响基于LLM的代理查询三元组库的能力，旨在设计可转移和可解释的神经符号AI系统。


<details>
  <summary>Details</summary>
Motivation: 研究可转移和可解释的神经符号AI系统的设计，特别关注能够主动选择、解释和查询知识来源的“Agentic Retrieval-Augmented Generation”系统。

Method: 系统地评估了不同知识的概念化和表示方法，特别是知识的结构和复杂性，对AI代理有效查询三元组库的影响。

Result: 研究结果表明，知识的概念化和表示方法都对AI代理查询三元组库有影响，并讨论了这些影响和启示。

Conclusion: 研究不同知识的概念化和表示方法，特别是知识的结构和复杂性，对AI代理（在本例中是LLM）有效查询三元组库的影响。

Abstract: Explainability and interpretability are cornerstones of frontier and
next-generation artificial intelligence (AI) systems. This is especially true
in recent systems, such as large language models (LLMs), and more broadly,
generative AI. On the other hand, adaptability to new domains, contexts, or
scenarios is also an important aspect for a successful system. As such, we are
particularly interested in how we can merge these two efforts, that is,
investigating the design of transferable and interpretable neurosymbolic AI
systems. Specifically, we focus on a class of systems referred to as ''Agentic
Retrieval-Augmented Generation'' systems, which actively select, interpret, and
query knowledge sources in response to natural language prompts. In this paper,
we systematically evaluate how different conceptualizations and representations
of knowledge, particularly the structure and complexity, impact an AI agent (in
this case, an LLM) in effectively querying a triplestore. We report our
results, which show that there are impacts from both approaches, and we discuss
their impact and implications.

</details>


### [273] [LLM-Stackelberg Games: Conjectural Reasoning Equilibria and Their Applications to Spearphishing](https://arxiv.org/abs/2507.09407)
*Quanyan Zhu*

Main category: cs.AI

TL;DR: We present LLM-Stackelberg games, a new model for strategic interactions using LLMs, allowing for imperfect information and adaptable strategies. We introduce new equilibrium concepts and show its applicability in cybersecurity through a spearphishing example.


<details>
  <summary>Details</summary>
Motivation: To extend Stackelberg games by incorporating LLMs, allowing for bounded rationality, asymmetric information, and meta-cognitive adaptation in strategic interactions.

Method: Introduce the framework of LLM-Stackelberg games, integrating LLMs into leader-follower strategic interactions. Define two equilibrium concepts: reasoning and behavioral equilibrium, and conjectural reasoning equilibrium. Illustrate with a spearphishing case study.

Result: Demonstrate the framework's ability to capture cognitive richness and adversarial potential in LLM-mediated interactions, as shown in the spearphishing case study.

Conclusion: LLM-Stackelberg games offer a powerful new paradigm for modeling strategic decision-making in complex, information-asymmetric environments, with applications in cybersecurity, misinformation, and recommendation systems.

Abstract: We introduce the framework of LLM-Stackelberg games, a class of sequential
decision-making models that integrate large language models (LLMs) into
strategic interactions between a leader and a follower. Departing from
classical Stackelberg assumptions of complete information and rational agents,
our formulation allows each agent to reason through structured prompts,
generate probabilistic behaviors via LLMs, and adapt their strategies through
internal cognition and belief updates. We define two equilibrium concepts:
reasoning and behavioral equilibrium, which aligns an agent's internal
prompt-based reasoning with observable behavior, and conjectural reasoning
equilibrium, which accounts for epistemic uncertainty through parameterized
models over an opponent's response. These layered constructs capture bounded
rationality, asymmetric information, and meta-cognitive adaptation. We
illustrate the framework through a spearphishing case study, where a sender and
a recipient engage in a deception game using structured reasoning prompts. This
example highlights the cognitive richness and adversarial potential of
LLM-mediated interactions. Our results show that LLM-Stackelberg games provide
a powerful paradigm for modeling decision-making in domains such as
cybersecurity, misinformation, and recommendation systems.

</details>


### [274] [Consistency Trajectory Planning: High-Quality and Efficient Trajectory Optimization for Offline Model-Based Reinforcement Learning](https://arxiv.org/abs/2507.09534)
*Guanquan Wang,Takuya Hiraoka,Yoshimasa Tsuruoka*

Main category: cs.AI

TL;DR: CTP is a faster offline reinforcement learning method using CTM for trajectory optimization, outperforming diffusion models with less computation and higher speed.


<details>
  <summary>Details</summary>
Motivation: To address the high computational costs of existing diffusion models in planning due to iterative sampling procedures, enabling faster, yet high-quality trajectory generation.

Method: Consistency Trajectory Planning (CTP), an offline model-based reinforcement learning method that uses the Consistency Trajectory Model (CTM) for trajectory optimization, enabling fast, single-step trajectory generation.

Result: CTP achieves comparable performance with over 120x speedup in inference time compared to existing diffusion-based planning methods on the D4RL benchmark for long-horizon, goal-conditioned tasks.

Conclusion: CTP consistently outperforms existing diffusion-based planning methods in long-horizon, goal-conditioned tasks, achieving higher normalized returns with significantly fewer denoising steps and over 120x speedup in inference time.

Abstract: This paper introduces Consistency Trajectory Planning (CTP), a novel offline
model-based reinforcement learning method that leverages the recently proposed
Consistency Trajectory Model (CTM) for efficient trajectory optimization. While
prior work applying diffusion models to planning has demonstrated strong
performance, it often suffers from high computational costs due to iterative
sampling procedures. CTP supports fast, single-step trajectory generation
without significant degradation in policy quality. We evaluate CTP on the D4RL
benchmark and show that it consistently outperforms existing diffusion-based
planning methods in long-horizon, goal-conditioned tasks. Notably, CTP achieves
higher normalized returns while using significantly fewer denoising steps. In
particular, CTP achieves comparable performance with over $120\times$ speedup
in inference time, demonstrating its practicality and effectiveness for
high-performance, low-latency offline planning.

</details>


### [275] [Learning to Control Dynamical Agents via Spiking Neural Networks and Metropolis-Hastings Sampling](https://arxiv.org/abs/2507.09540)
*Ali Safa,Farida Mohsen,Ali Al-Zawqari*

Main category: cs.AI

TL;DR: A novel framework uses Metropolis-Hastings sampling to train Spiking Neural Networks (SNNs) for reinforcement learning tasks, overcoming gradient limitations and achieving better results than existing methods on control benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of training SNNs for RL tasks, particularly the non-differentiable nature of spike-based communication, and to provide an alternative to gradient-based methods.

Method: Bayesian inference technique using Metropolis-Hastings (MH) sampling to train SNNs for RL tasks without gradient-based methods. The approach iteratively proposes and probabilistically accepts network parameter updates based on accumulated reward signals.

Result: The MH-based approach was evaluated on AcroBot and CartPole benchmarks and demonstrated superior performance compared to conventional DQL baselines and prior SNN-based RL approaches.

Conclusion: MH-based SNN training approach outperforms DQL and other SNN RL methods in maximizing accumulated reward, minimizing network resources and training episodes.

Abstract: Spiking Neural Networks (SNNs) offer biologically inspired, energy-efficient
alternatives to traditional Deep Neural Networks (DNNs) for real-time control
systems. However, their training presents several challenges, particularly for
reinforcement learning (RL) tasks, due to the non-differentiable nature of
spike-based communication. In this work, we introduce what is, to our
knowledge, the first framework that employs Metropolis-Hastings (MH) sampling,
a Bayesian inference technique, to train SNNs for dynamical agent control in RL
environments without relying on gradient-based methods. Our approach
iteratively proposes and probabilistically accepts network parameter updates
based on accumulated reward signals, effectively circumventing the limitations
of backpropagation while enabling direct optimization on neuromorphic
platforms. We evaluated this framework on two standard control benchmarks:
AcroBot and CartPole. The results demonstrate that our MH-based approach
outperforms conventional Deep Q-Learning (DQL) baselines and prior SNN-based RL
approaches in terms of maximizing the accumulated reward while minimizing
network resources and training episodes.

</details>


### [276] [eSapiens: A Platform for Secure and Auditable Retrieval-Augmented Generation](https://arxiv.org/abs/2507.09588)
*Isaac Shi,Zeyuan Li,Fan Liu,Wenli Wang,Lewei He,Yang Yang,Tianyu Shi*

Main category: cs.AI

TL;DR: eSapiens是一个AIaaS平台，通过数据、工作流和LLM为企业提供对AI的控制，以提高安全性和效率。


<details>
  <summary>Details</summary>
Motivation: 为企业提供对人工智能资产的完全控制，将所有内容保留在内部，以实现人工智能知识保留和数据安全。eSapiens AI Agents (Sapiens)通过提供有价值的见解和自动化重复性任务来增强团队能力，使他们能够专注于高影响力工作并带来更好的业务成果。

Method: eSapiens是一个围绕专有数据、运营工作流和任何主要自适应大型语言模型（LLM）构建的AI即服务（AIaaS）平台。它集成了结构化文档摄取、混合向量检索和通过LangChain实现的无代码编排，并支持OpenAI、Claude、Gemini和DeepSeek等顶级LLM。THOR Agent处理结构化SQL风格查询，并在企业数据库上生成可操作的见解。

Result: 检索基准测试显示，512个标记的块大小可实现最高检索精度（Top-3准确率：91.3%）。使用TRACe指标进行的生成质量测试表明，eSapiens可提供更多上下文一致的输出，事实一致性最多提高23%。

Conclusion: eSapiens在法律和金融等高风险领域实现了值得信赖、可审核的人工智能工作流。

Abstract: We present eSapiens, an AI-as-a-Service (AIaaS) platform engineered around a
business-oriented trifecta: proprietary data, operational workflows, and any
major agnostic Large Language Model (LLM). eSapiens gives businesses full
control over their AI assets, keeping everything in-house for AI knowledge
retention and data security. eSapiens AI Agents (Sapiens) empower your team by
providing valuable insights and automating repetitive tasks, enabling them to
focus on high-impact work and drive better business outcomes.
  The system integrates structured document ingestion, hybrid vector retrieval,
and no-code orchestration via LangChain, and supports top LLMs including
OpenAI, Claude, Gemini, and DeepSeek. A key component is the THOR Agent, which
handles structured SQL-style queries and generates actionable insights over
enterprise databases.
  To evaluate the system, we conduct two experiments. First, a retrieval
benchmark on legal corpora reveals that a chunk size of 512 tokens yields the
highest retrieval precision (Top-3 accuracy: 91.3%). Second, a generation
quality test using TRACe metrics across five LLMs shows that eSapiens delivers
more context-consistent outputs with up to a 23% improvement in factual
alignment.
  These results demonstrate the effectiveness of eSapiens in enabling
trustworthy, auditable AI workflows for high-stakes domains like legal and
finance.

</details>


### [277] [Improving monotonic optimization in heterogeneous multi-agent reinforcement learning with optimal marginal deterministic policy gradient](https://arxiv.org/abs/2507.09989)
*Xiaoyang Yu,Youfang Lin,Shuo Wang,Sheng Han*

Main category: cs.AI

TL;DR: OMDPG算法通过引入OMQ和GQC，解决了部分参数共享与单调改进的冲突，实现了异构多智能体强化学习的性能提升。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有异构多智能体强化学习算法（如HAPPO）在采用部分参数共享时出现的策略更新基线漂移问题，以及部分参数共享与单调改进之间的冲突。

Method: 提出了一种名为OMDPG的算法，通过引入最优边际Q（OMQ）函数来替代顺序计算的Q函数，以解决策略更新基线漂移问题，并使用广义QCritic（GQC）作为Critic函数，采用悲观不确定性约束损失来优化不同的Q值估计，最后通过中心化Critic分组Actor（CCGA）架构实现部分参数共享和全局Q函数计算。

Result: OMDPG算法在SMAC和MAMuJoCo环境中的实验结果表明，其性能优于多种最先进的多智能体强化学习基线算法。

Conclusion: OMDPG算法成功解决了异构多智能体强化学习中部分参数共享与单调改进之间的冲突，并在SMAC和MAMuJoCo环境中取得了优于现有基线的性能。

Abstract: In heterogeneous multi-agent reinforcement learning (MARL), achieving
monotonic improvement plays a pivotal role in enhancing performance. The HAPPO
algorithm proposes a feasible solution by introducing a sequential update
scheme, which requires independent learning with No Parameter-sharing (NoPS).
However, heterogeneous MARL generally requires Partial Parameter-sharing
(ParPS) based on agent grouping to achieve high cooperative performance. Our
experiments prove that directly combining ParPS with the sequential update
scheme leads to the policy updating baseline drift problem, thereby failing to
achieve improvement. To solve the conflict between monotonic improvement and
ParPS, we propose the Optimal Marginal Deterministic Policy Gradient (OMDPG)
algorithm. First, we replace the sequentially computed $Q_{\psi}^s(s,a_{1:i})$
with the Optimal Marginal Q (OMQ) function $\phi_{\psi}^*(s,a_{1:i})$ derived
from Q-functions. This maintains MAAD's monotonic improvement while eliminating
the conflict through optimal joint action sequences instead of sequential
policy ratio calculations. Second, we introduce the Generalized Q Critic (GQC)
as the critic function, employing pessimistic uncertainty-constrained loss to
optimize different Q-value estimations. This provides the required Q-values for
OMQ computation and stable baselines for actor updates. Finally, we implement a
Centralized Critic Grouped Actor (CCGA) architecture that simultaneously
achieves ParPS in local policy networks and accurate global Q-function
computation. Experimental results in SMAC and MAMuJoCo environments demonstrate
that OMDPG outperforms various state-of-the-art MARL baselines.

</details>


### [278] [The Hidden Costs of AI: A Review of Energy, E-Waste, and Inequality in Model Development](https://arxiv.org/abs/2507.09611)
*Jenis Winsta*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Artificial intelligence (AI) has made remarkable progress in recent years,
yet its rapid expansion brings overlooked environmental and ethical challenges.
This review explores four critical areas where AI's impact extends beyond
performance: energy consumption, electronic waste (e-waste), inequality in
compute access, and the hidden energy burden of cybersecurity systems. Drawing
from recent studies and institutional reports, the paper highlights systemic
issues such as high emissions from model training, rising hardware turnover,
global infrastructure disparities, and the energy demands of securing AI. By
connecting these concerns, the review contributes to Responsible AI discourse
by identifying key research gaps and advocating for sustainable, transparent,
and equitable development practices. Ultimately, it argues that AI's progress
must align with ethical responsibility and environmental stewardship to ensure
a more inclusive and sustainable technological future.

</details>


### [279] [Adaptability in Multi-Agent Reinforcement Learning: A Framework and Unified Review](https://arxiv.org/abs/2507.10142)
*Siyi Hu,Mohamad A Hady,Jianglin Qiao,Jimmy Cao,Mahardhika Pratama,Ryszard Kowalczyk*

Main category: cs.AI

TL;DR: MARL在现实世界部署受限，本文提出“适应性”新视角和三维度框架（学习、策略、场景适应性），以改进对MARL算法在动态环境中性能的评估，助力其在真实世界应用。


<details>
  <summary>Details</summary>
Motivation: 现实世界多智能体系统（MAS）环境复杂多变，包括代理种群波动、任务目标演变和执行条件不一致等，这限制了多智能体强化学习（MARL）在该领域的部署。因此，需要一种新的评估视角来衡量MARL算法在持续变化系统配置和操作需求下的有效性。

Method: 提出一个包含学习适应性、策略适应性、场景驱动适应性三个关键维度的结构化框架，并引入“适应性”作为评估MARL算法在动态变化条件下可靠性的统一视角。

Result: 引入“适应性”概念，并提出一个结构化框架来评估MARL算法在动态变化条件下的可靠性，旨在超越狭隘基准的评估方法，支持更合理的性能评估，并最终促进适用于动态真实世界MAS的MARL算法的发展。

Conclusion: 本篇论文旨在为多智能体强化学习（MARL）在现实世界多智能体系统（MAS）中的部署提供支持，通过引入“适应性”这一概念，并提出一个包含学习适应性、策略适应性和场景驱动适应性的结构化框架，以期促进对MARL算法在动态变化环境中的性能进行更合理和更全面的评估，从而推动能够适应真实世界复杂性的MARL算法的发展。

Abstract: Multi-Agent Reinforcement Learning (MARL) has shown clear effectiveness in
coordinating multiple agents across simulated benchmarks and constrained
scenarios. However, its deployment in real-world multi-agent systems (MAS)
remains limited, primarily due to the complex and dynamic nature of such
environments. These challenges arise from multiple interacting sources of
variability, including fluctuating agent populations, evolving task goals, and
inconsistent execution conditions. Together, these factors demand that MARL
algorithms remain effective under continuously changing system configurations
and operational demands. To better capture and assess this capacity for
adjustment, we introduce the concept of \textit{adaptability} as a unified and
practically grounded lens through which to evaluate the reliability of MARL
algorithms under shifting conditions, broadly referring to any changes in the
environment dynamics that may occur during learning or execution. Centred on
the notion of adaptability, we propose a structured framework comprising three
key dimensions: learning adaptability, policy adaptability, and scenario-driven
adaptability. By adopting this adaptability perspective, we aim to support more
principled assessments of MARL performance beyond narrowly defined benchmarks.
Ultimately, this survey contributes to the development of algorithms that are
better suited for deployment in dynamic, real-world multi-agent systems.

</details>


### [280] [Bridging Bots: from Perception to Action via Multimodal-LMs and Knowledge Graphs](https://arxiv.org/abs/2507.09617)
*Margherita Martorana,Francesca Urgese,Mark Adamik,Ilaria Tiddi*

Main category: cs.AI

TL;DR: 该研究提出了一种神经符号框架，结合了多模态语言模型和知识图谱的优点，以提高个人服务机器人的互操作性。实验表明，特定的LLaMA和GPT模型表现更优，但集成策略比模型本身更重要。


<details>
  <summary>Details</summary>
Motivation: 当前的个人服务机器人系统依赖于专有的、硬编码的解决方案，这些方案与特定的硬件和软件绑定，导致实现方式孤立，难以跨平台适应和扩展。本体和知识图谱（KGs）通过结构化和标准化的知识表示和推理，为实现跨系统互操作性提供了解决方案。然而，像KGs和本体这样的符号系统在处理原始和嘈杂的传感输入方面存在挑战，而多模态语言模型虽然擅长解释图像和自然语言等输入，但往往缺乏透明度、一致性和知识基础。

Method: 提出了一种神经符号框架，结合了多模态语言模型的感知能力和知识图谱与本体提供的结构化表示，旨在支持机器人应用中的互操作性。该方法生成符合本体的知识图谱，从而以独立于平台的方式指导机器人行为。

Result: 该框架通过整合机器人感知数据、本体和五个多模态模型（三种LLaMA和两种GPT模型），并采用不同模式的神经符号交互进行了评估。评估了生成知识图谱在多次运行和配置中的一致性和有效性，并进行了统计分析以评估性能。

Conclusion: 研究结果表明，GPT-o1 和 LLaMA 4 Maverick 在生成符合本体的知识图谱方面表现优于其他模型，但新的模型并不一定能带来更好的结果，这凸显了集成策略在生成符合本体的知识图谱中的关键作用。

Abstract: Personal service robots are deployed to support daily living in domestic
environments, particularly for elderly and individuals requiring assistance.
These robots must perceive complex and dynamic surroundings, understand tasks,
and execute context-appropriate actions. However, current systems rely on
proprietary, hard-coded solutions tied to specific hardware and software,
resulting in siloed implementations that are difficult to adapt and scale
across platforms. Ontologies and Knowledge Graphs (KGs) offer a solution to
enable interoperability across systems, through structured and standardized
representations of knowledge and reasoning. However, symbolic systems such as
KGs and ontologies struggle with raw and noisy sensory input. In contrast,
multimodal language models are well suited for interpreting input such as
images and natural language, but often lack transparency, consistency, and
knowledge grounding. In this work, we propose a neurosymbolic framework that
combines the perceptual strengths of multimodal language models with the
structured representations provided by KGs and ontologies, with the aim of
supporting interoperability in robotic applications. Our approach generates
ontology-compliant KGs that can inform robot behavior in a platform-independent
manner. We evaluated this framework by integrating robot perception data,
ontologies, and five multimodal models (three LLaMA and two GPT models), using
different modes of neural-symbolic interaction. We assess the consistency and
effectiveness of the generated KGs across multiple runs and configurations, and
perform statistical analyzes to evaluate performance. Results show that GPT-o1
and LLaMA 4 Maverick consistently outperform other models. However, our
findings also indicate that newer models do not guarantee better results,
highlighting the critical role of the integration strategy in generating
ontology-compliant KGs.

</details>


### [281] [humancompatible.interconnect: Testing Properties of Repeated Uses of Interconnections of AI Systems](https://arxiv.org/abs/2507.09626)
*Rodion Nazarov,Anthony Quinn,Robert Shorten,Jakub Marecek*

Main category: cs.AI

TL;DR: 一个用于AI系统公平性和鲁棒性的开源PyTorch工具包。


<details>
  <summary>Details</summary>
Motivation: AI系统与多个智能体交互，需要公平性和鲁棒性的先验保证。

Method: 使用基于PyTorch的随机控制技术来模拟AI系统和它们的重复使用。

Result: 该工具包简化了为多智能体系统的闭环模型提供公平性保证的复杂性。

Conclusion: 该工具包提供了用于多智能体系统的闭环模型和公平性保证的工具。

Abstract: Artificial intelligence (AI) systems often interact with multiple agents. The
regulation of such AI systems often requires that {\em a priori\/} guarantees
of fairness and robustness be satisfied. With stochastic models of agents'
responses to the outputs of AI systems, such {\em a priori\/} guarantees
require non-trivial reasoning about the corresponding stochastic systems. Here,
we present an open-source PyTorch-based toolkit for the use of stochastic
control techniques in modelling interconnections of AI systems and properties
of their repeated uses. It models robustness and fairness desiderata in a
closed-loop fashion, and provides {\em a priori\/} guarantees for these
interconnections. The PyTorch-based toolkit removes much of the complexity
associated with the provision of fairness guarantees for closed-loop models of
multi-agent systems.

</details>


### [282] [DeepResearch$^{\text{Eco}}$: A Recursive Agentic Workflow for Complex Scientific Question Answering in Ecology](https://arxiv.org/abs/2507.10522)
*Jennifer D'Souza,Endres Keno Sander,Andrei Aioanei*

Main category: cs.AI

TL;DR: DeepResearch$^{	ext{Eco}}$是一个创新的智能LLM系统，可自动化科学文献的综合，提高检索效率和质量，并在生态学研究中展现出卓越性能。


<details>
  <summary>Details</summary>
Motivation: 为了提高科学文献检索的多样性和细微性，并实现高通量的领域特定证据整合，同时保持分析严谨性。

Method: 提出了一种名为DeepResearch$^{	ext{Eco}}$的新型基于智能LLM的自动化科学综合系统，该系统支持递归的、受深度和广度控制的研究问题探索，并具有用户可控的综合能力、透明的推理和参数驱动的可配置性。

Result: 在49个生态学研究问题上的应用显示，DeepResearch$^{	ext{Eco}}$的源整合度提高了21倍，每千词整合的文献源数量增加了14.9倍，在高参数设置下，分析深度和背景多样性达到了专家水平。

Conclusion: DeepResearch$^{	ext{Eco}}$通过增强搜索多样性和检索相关科学文献的细微差别，实现了比传统检索增强生成流程更高的源整合度和每千词整合度，并在应用于生态学研究问题时，在高参数设置下达到专家级别的分析深度和背景多样性。

Abstract: We introduce DeepResearch$^{\text{Eco}}$, a novel agentic LLM-based system
for automated scientific synthesis that supports recursive, depth- and
breadth-controlled exploration of original research questions -- enhancing
search diversity and nuance in the retrieval of relevant scientific literature.
Unlike conventional retrieval-augmented generation pipelines, DeepResearch
enables user-controllable synthesis with transparent reasoning and
parameter-driven configurability, facilitating high-throughput integration of
domain-specific evidence while maintaining analytical rigor. Applied to 49
ecological research questions, DeepResearch achieves up to a 21-fold increase
in source integration and a 14.9-fold rise in sources integrated per 1,000
words. High-parameter settings yield expert-level analytical depth and
contextual diversity.
  Source code available at: https://github.com/sciknoworg/deep-research.

</details>


### [283] [Towards Concise and Adaptive Thinking in Large Reasoning Models: A Survey](https://arxiv.org/abs/2507.09662)
*Jason Zhu,Hongyu Li*

Main category: cs.AI

TL;DR: LRMs推理太慢且冗长，需要更简洁自适应的方法。


<details>
  <summary>Details</summary>
Motivation: 为了解决LRMs生成冗长推理链的问题，需要学习根据输入难度进行自适应推理，以缩短推理链并提高推理效率。

Method: 本文对LRMs的简洁和自适应推理进行了全面的概述，涵盖了方法、基准和未来挑战。

Result: 本文旨在帮助研究人员快速了解该领域的进展，并激发新的自适应推理思路，以促进LRMs的更好使用。

Conclusion: LRMs在数学和编程等复杂推理任务中表现出色，但会生成冗长冗余的推理链，浪费资源并减慢响应速度。因此，根据输入难度学习自适应推理以缩短推理链至关重要。

Abstract: Large reasoning models (LRMs) like OpenAI o1 and DeepSeek R1 have
demonstrated impressive performance on complex reasoning tasks like mathematics
and programming with long Chain-of-Thought (CoT) reasoning sequences
(slow-thinking), compared with traditional large language models
(fast-thinking). However, these reasoning models also face a huge challenge
that generating unnecessarily lengthy and redundant reasoning chains even for
trivial questions. This phenomenon leads to a significant waste of inference
resources, increases the response time for simple queries, and hinders the
practical application of LRMs in real-world products. To this end, it is
crucial to shorten lengthy reasoning chains and learn adaptive reasoning
between fast and slow thinking based on input difficulty. In this survey, we
provide a comprehensive overview of recent progress in concise and adaptive
thinking for efficient reasoning of LRMs, including methodologies, benchmarks,
and challenges for future exploration. We hope this survey can help researchers
quickly understand the landscape of this field and inspire novel adaptive
thinking ideas to facilitate better usage of LRMs.

</details>


### [284] [Causality-informed Anomaly Detection in Partially Observable Sensor Networks: Moving beyond Correlations](https://arxiv.org/abs/2507.09742)
*Xiaofeng Xiao,Bo Shen,Xubo Yue*

Main category: cs.AI

TL;DR: 针对AI制造数据流监控中的传感器放置问题，提出了一种结合因果推断的深度强化学习方法（Causal DQ），无需人为干预即可实现快速异常检测和最优传感器放置。


<details>
  <summary>Details</summary>
Motivation: 在AI驱动的制造业数据流实时监控需求增长的背景下，由于资源限制无法在所有位置放置传感器，因此需要最优的传感器放置策略以实现部分可观测和快速异常检测。现有方法多只考虑变量相关性而忽略因果关系，或依赖不切实际的人为干预来识别因果效应。

Method: 提出了一种结合因果推断的深度Q网络（Causal DQ）方法，将因果信息整合到Q网络训练的每个阶段，以解决部分可观测系统中的传感器放置问题。

Result: 实验结果表明，所提出的Causal DQ方法在传感器放置方面具有更快的收敛速度和更紧密的理论误差界限，并能显著减少不同设置下异常的检测时间，证明了其在大规模真实数据流中的有效性。

Conclusion: 本研究提出的因果推断深度强化学习方法在异常检测的传感器放置问题上，能够显著减少异常检测时间并具有更优的理论误差界限，为工程应用中的因果推断机器学习开辟了新的可能性。

Abstract: Nowadays, as AI-driven manufacturing becomes increasingly popular, the volume
of data streams requiring real-time monitoring continues to grow. However, due
to limited resources, it is impractical to place sensors at every location to
detect unexpected shifts. Therefore, it is necessary to develop an optimal
sensor placement strategy that enables partial observability of the system
while detecting anomalies as quickly as possible. Numerous approaches have been
proposed to address this challenge; however, most existing methods consider
only variable correlations and neglect a crucial factor: Causality. Moreover,
although a few techniques incorporate causal analysis, they rely on
interventions-artificially creating anomalies-to identify causal effects, which
is impractical and might lead to catastrophic losses. In this paper, we
introduce a causality-informed deep Q-network (Causal DQ) approach for
partially observable sensor placement in anomaly detection. By integrating
causal information at each stage of Q-network training, our method achieves
faster convergence and tighter theoretical error bounds. Furthermore, the
trained causal-informed Q-network significantly reduces the detection time for
anomalies under various settings, demonstrating its effectiveness for sensor
placement in large-scale, real-world data streams. Beyond the current
implementation, our technique's fundamental insights can be applied to various
reinforcement learning problems, opening up new possibilities for real-world
causality-informed machine learning methods in engineering applications.

</details>


### [285] [Technical Requirements for Halting Dangerous AI Activities](https://arxiv.org/abs/2507.09801)
*Peter Barnett,Aaron Scher,David Abecassis*

Main category: cs.AI

TL;DR: 为了应对人工智能带来的风险，政府可以实施技术干预措施来协调停止危险的人工智能开发和部署。


<details>
  <summary>Details</summary>
Motivation: 为了应对人工智能系统带来的前所未有的风险（包括失控、滥用、地缘政治不稳定和权力集中），政府可能会主动建立协调停止危险人工智能开发和部署的能力。

Method: 本文件概述了关键技术干预措施，以实现对危险人工智能活动的协调停止。

Result: 我们讨论了这些干预措施如何有助于限制各种危险的人工智能活动，并展示了这些干预措施如何为潜在的人工智能治理计划奠定技术基础。

Conclusion: 该文件概述了关键技术干预措施，以实现对危险人工智能活动的协调停止，并展示了这些干预措施如何为潜在的人工智能治理计划奠定技术基础。

Abstract: The rapid development of AI systems poses unprecedented risks, including loss
of control, misuse, geopolitical instability, and concentration of power. To
navigate these risks and avoid worst-case outcomes, governments may proactively
establish the capability for a coordinated halt on dangerous AI development and
deployment. In this paper, we outline key technical interventions that could
allow for a coordinated halt on dangerous AI activities. We discuss how these
interventions may contribute to restricting various dangerous AI activities,
and show how these interventions can form the technical foundation for
potential AI governance plans.

</details>


### [286] [Is Human-Written Data Enough? The Challenge of Teaching Reasoning to LLMs Without RL or Distillation](https://arxiv.org/abs/2507.09850)
*Wei Du,Branislav Kisacanin,George Armstrong,Shubham Toshniwal,Ivan Moshkov,Alexan Ayrapetyan,Sadegh Mahdavi,Dan Zhao,Shizhe Diao,Dragan Masulovic,Marius Stanean,Advaith Avadhanam,Max Wang,Ashmit Dutta,Shitij Govil,Sri Yanamandara,Mihir Tandon,Sriram Ananthakrishnan,Vedant Rathi,David Zhang,Joonseok Kang,Leon Luo,Titu Andreescu,Boris Ginsburg,Igor Gitman*

Main category: cs.AI

TL;DR: 通过少量高质量CoT示例微调基础模型，可大幅提升其推理能力，超越更大模型；但专家CoT的某些特质难以通过非专业数据复制。


<details>
  <summary>Details</summary>
Motivation: 探究是否仅通过提示或最小化调整即可在基础模型中诱导产生长CoT推理能力，以及少量高质量CoT示例的有效性。

Method: 通过使用少量（20个）来自推理模型（QwQ-32B-Preview）的长CoT示例，对基础模型（Qwen2.5-32B）进行轻度微调。并探索了使用非推理模型和人类注释者的CoT数据，通过提示工程、多轮编辑和结构化指导进行增强。

Result: 轻度微调后的Qwen2.5-32B模型性能超越了更大的Qwen2.5-Math-72B-Instruct模型。非推理模型或人类注释者的CoT数据，即使经过增强，性能也无法与推理模型追踪相媲美。

Conclusion: 少量高质量的思维链（CoT）示例可以显著提升基础模型的推理能力，甚至超越更大、经过专门训练的模型。尽管使用非推理模型或人类注释者的数据进行增强和提示工程，其性能仍不及专门的推理模型追踪，这表明专家CoT的某些潜在特质难以复制。然而，仔细筛选的人类编写的CoT，即使数量不多，也能有效激活基础模型的推理行为。

Abstract: Reasoning-capable language models achieve state-of-the-art performance in
diverse complex tasks by generating long, explicit Chain-of-Thought (CoT)
traces. While recent works show that base models can acquire such reasoning
traces via reinforcement learning or distillation from stronger models like
DeepSeek-R1, previous works demonstrate that even short CoT prompting without
fine-tuning is able to improve reasoning. We ask whether long CoT can be
induced in a base model using only prompting or minimal tuning. Using just 20
long CoT examples from the reasoning model \texttt{QwQ-32B-Preview}, we lightly
fine-tune the base model \texttt{Qwen2.5-32B}. The resulting model outperforms
the much larger \texttt{Qwen2.5-Math-72B-Instruct}, showing that a handful of
high-quality examples can unlock strong reasoning capabilities. We further
explore using CoT data from non-reasoning models and human annotators, enhanced
with prompt engineering, multi-pass editing, and structural guidance. However,
neither matches the performance of reasoning model traces, suggesting that
certain latent qualities of expert CoT are difficult to replicate. We analyze
key properties of reasoning data, such as problem difficulty, diversity, and
answer length, that influence reasoning distillation. While challenges remain,
we are optimistic that carefully curated human-written CoT, even in small
quantities, can activate reasoning behaviors in base models. We release our
human-authored dataset across refinement stages and invite further
investigation into what makes small-scale reasoning supervision so effective.

</details>


### [287] [Model-Grounded Symbolic Artificial Intelligence Systems Learning and Reasoning with Model-Grounded Symbolic Artificial Intelligence Systems](https://arxiv.org/abs/2507.09854)
*Aniruddha Chattopadhyay,Raj Dandekar,Kaushik Roy*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Neurosymbolic artificial intelligence (AI) systems combine neural network and
classical symbolic AI mechanisms to exploit the complementary strengths of
large scale, generalizable learning and robust, verifiable reasoning. Numerous
classifications of neurosymbolic AI illustrate how these two components can be
integrated in distinctly different ways. In this work, we propose
reinterpreting instruction tuned large language models as model grounded
symbolic AI systems where natural language serves as the symbolic layer and
grounding is achieved through the models internal representation space. Within
this framework, we investigate and develop novel learning and reasoning
approaches that preserve structural similarities to traditional learning and
reasoning paradigms. Preliminary evaluations across axiomatic deductive
reasoning procedures of varying complexity provide insights into the
effectiveness of our approach in improving learning efficiency and reasoning
reliability.

</details>


### [288] [VerifyBench: A Systematic Benchmark for Evaluating Reasoning Verifiers Across Domains](https://arxiv.org/abs/2507.09884)
*Xuzhao Li,Xuchen Li,Shiyu Hu,Yongzhen Guo,Wentao Zhang*

Main category: cs.AI

TL;DR: VerifyBench是一个用于评估大语言模型（LLM）验证器的新基准。该基准包含数学、物理、化学和生物学领域的4000个问题及其答案。研究发现，专门验证器准确率高但召回率低，通用LLM包容性强但精确度不稳定，且验证器对输入结构敏感且泛化能力有限。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM越来越依赖强化学习（RL）通过反馈来提升推理能力，但验证模型生成响应与参考答案的一致性却面临巨大挑战，因为这些响应通常冗长、多样且细微。现有的研究主要集中于改进验证器本身，但缺乏对其在不同领域性能的系统性评估，这严重限制了可验证奖励强化学习（RLVR）的可靠发展。

Method: 本研究提出了VerifyBench，一个跨领域的综合基准测试，用于系统地评估验证器。该基准包含4000个涵盖数学、物理、化学和生物学领域专家级别的问题，并配有参考答案和多样化的模型生成回答。研究设计了一个四维度的实验框架，比较了专门验证器和通用LLM在提取答案与完整回答、短输出与长输出等组合条件下的性能边界。

Result: VerifyBench的评估揭示了验证器方面的基本权衡：专门验证器虽然准确率领先，但在召回率方面存在不足；通用模型则显示出更强的包容性，但精确度不稳定。更重要的是，研究发现验证器对输入结构的敏感性以及跨领域泛化的固有局限性，为了解当前验证器技术的瓶颈提供了关键见解。

Conclusion: 现有的大语言模型（LLM）验证方法在准确性和召回率、泛化能力以及对输入结构的敏感性方面存在局限性。专门的验证器虽然准确率高，但召回率不足；而通用的LLM则更具包容性，但精确度不稳定。验证器对输入结构的高度敏感性和跨领域泛化能力方面的固有局限性，指出了当前验证器技术的瓶颈。

Abstract: Large language models (LLMs) increasingly rely on reinforcement learning (RL)
to enhance their reasoning capabilities through feedback. A critical challenge
is verifying the consistency of model-generated responses and reference
answers, since these responses are often lengthy, diverse, and nuanced.
Rule-based verifiers struggle with complexity, prompting the use of model-based
verifiers. However, specialized verifiers lack flexibility, while general LLM
judges can be inconsistent. Existing research primarily focuses on building
better verifiers, yet a systematic evaluation of different types of verifiers'
performance across domains remains lacking, severely constraining the reliable
development of Reinforcement Learning with Verifiable Reward (RLVR). To address
this, we propose VerifyBench--a cross-domain comprehensive benchmark for
systematically evaluating verifiers. We construct 4,000 expert-level questions
covering mathematics, physics, chemistry, and biology. Each question is
equipped with reference answers and diverse responses. The reliability of the
evaluation is ensured through a rigorous annotation process conducted by a
multidisciplinary expert team. We design a four-dimensional experimental
framework to comprehensively compare the performance boundaries of specialized
verifiers and general LLMs under combined conditions of extracted answers vs.
complete responses, and short vs. long outputs. Our evaluation uncovers
fundamental trade-offs in verifiers: while specialized verifiers achieve
leading accuracy, they exhibit deficiencies in recall; general models show
stronger inclusivity but unstable precision. More importantly, we discover
verifiers' high sensitivity to input structure and inherent limitations in
cross-domain generalization, providing critical insights into the bottlenecks
of current verifier technology.

</details>


### [289] [DeepSeek: Paradigm Shifts and Technical Evolution in Large AI Models](https://arxiv.org/abs/2507.09955)
*Luolin Xiong,Haofen Wang,Xi Chen,Lu Sheng,Yun Xiong,Jingping Liu,Yanghua Xiao,Huajun Chen,Qing-Long Han,Yang Tang*

Main category: cs.AI

TL;DR: DeepSeek发布了V3和R1模型，具有低成本、高性能和开源的特点。本文分析了其MLA、MoE、MTP、GRPO等新算法、工程突破以及与其他主流大模型的对比，并展望了未来大模型的发展趋势。


<details>
  <summary>Details</summary>
Motivation: 为了介绍DeepSeek V3和R1系列模型，分析其技术创新、工程突破以及在AI领域的潜在影响力。

Method: 通过回顾大模型演化、分析DeepSeek模型及其新算法（MLA、MoE、MTP、GRPO）、探讨工程突破以及对比分析与其他LLM模型的优劣，来阐述其研究内容。

Result: DeepSeek模型凭借其低成本、高性能和开源的优势，在大模型领域引起了全球关注，并在算法、工程和应用方面展现出显著的创新。

Conclusion: 该论文总结了DeepSeek V3和R1系列模型的创新之处，并探讨了它们对AI领域竞争格局的影响以及未来大模型发展的趋势。

Abstract: DeepSeek, a Chinese Artificial Intelligence (AI) startup, has released their
V3 and R1 series models, which attracted global attention due to their low
cost, high performance, and open-source advantages. This paper begins by
reviewing the evolution of large AI models focusing on paradigm shifts, the
mainstream Large Language Model (LLM) paradigm, and the DeepSeek paradigm.
Subsequently, the paper highlights novel algorithms introduced by DeepSeek,
including Multi-head Latent Attention (MLA), Mixture-of-Experts (MoE),
Multi-Token Prediction (MTP), and Group Relative Policy Optimization (GRPO).
The paper then explores DeepSeek engineering breakthroughs in LLM scaling,
training, inference, and system-level optimization architecture. Moreover, the
impact of DeepSeek models on the competitive AI landscape is analyzed,
comparing them to mainstream LLMs across various fields. Finally, the paper
reflects on the insights gained from DeepSeek innovations and discusses future
trends in the technical and engineering development of large AI models,
particularly in data, training, and reasoning.

</details>


### [290] [On The Role of Intentionality in Knowledge Representation: Analyzing Scene Context for Cognitive Agents with a Tiny Language Model](https://arxiv.org/abs/2507.10000)
*Mark Burgess*

Main category: cs.AI

TL;DR: 一种无需大量训练或推理即可识别文本中潜在意图的低成本方法。


<details>
  <summary>Details</summary>
Motivation: Searle对意图和意向性的哲学解构后，意图在科技领域的实际意义受到忽视。

Method: 通过寻找多尺度异常并评估形成它们的工作量，并利用时空相干性作为度量来分离‘意图’内容和‘环境’背景，来量化数据中潜在的‘意图’。

Result: 提出了一种低计算成本、无需广泛训练或推理能力即可解释潜在意图的方法，该方法适用于基本生物体，概念形成水平取决于智能体的记忆容量。这为理解文本中的意图和上下文提供了一种新的视角。由于其较低的计算成本和对大规模训练的依赖性较低，因此具有实际应用价值。然而，结果的有效性取决于智能体的记忆能力和数据中潜在意图的明确程度。

Conclusion: 该研究提出了一种计算效率高且不依赖大规模训练或推理能力的解释潜在意图的实用方法，其概念形成水平取决于智能体的记忆容量。

Abstract: Since Searle's work deconstructing intent and intentionality in the realm of
philosophy, the practical meaning of intent has received little attention in
science and technology. Intentionality and context are both central to the
scope of Promise Theory's model of Semantic Spacetime, used as an effective
Tiny Language Model. One can identify themes and concepts from a text, on a low
level (without knowledge of the specific language) by using process coherence
as a guide. Any agent process can assess superficially a degree of latent
`intentionality' in data by looking for anomalous multi-scale anomalies and
assessing the work done to form them. Scale separation can be used to sort
parts into `intended' content and `ambient context', using the spacetime
coherence as a measure. This offers an elementary but pragmatic interpretation
of latent intentionality for very low computational cost, and without reference
to extensive training or reasoning capabilities. The process is well within the
reach of basic organisms as it does not require large scale artificial
probabilistic batch processing. The level of concept formation depends,
however, on the memory capacity of the agent.

</details>


### [291] [Deep Hidden Cognition Facilitates Reliable Chain-of-Thought Reasoning](https://arxiv.org/abs/2507.10007)
*Zijun Chen,Wenbo Hu,Richang Hong*

Main category: cs.AI

TL;DR: 本研究提出一种基于注意力头激活的置信度预测器，通过校准思维链（CoT）推理的准确性来提高其可靠性，并在多项推理任务中取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 思维链（CoT）推理虽然在大型语言模型（LLM）和多模态大型语言模型（MLLM）中展现出强大的推理能力，但其中间步骤的错误累积常常会削弱其可靠性。因此，本研究旨在提高CoT推理的可靠性。

Method: 本研究提出一种新颖的方法，通过利用模型内在的真实性编码来校准思维链（CoT）推理的准确性。研究人员发现，特定的注意力头激活能够可靠地反映CoT推理步骤的真实性。基于此，他们训练了一个置信度预测器，使用这些对真实性敏感的激活来评估每个推理步骤的正确性，并通过束搜索动态选择最合理的推理路径。

Result: 实验结果表明，该方法在数学、符号和常识推理任务上显著优于最先进的基线（如少样本CoT、自洽性、自评估引导束搜索），在单一模式和多模式设置中均表现出更高的准确性和可靠性。此外，该方法在大规模推理模型上的验证也证实了其对专门推理模型的适用性。研究还探讨了模型自我纠正能力在CoT推理中的作用。

Conclusion: 该方法在数学、符号和常识推理任务上显著优于最先进的基线方法，在单一模式和多模式设置中都表现出更高的准确性和可靠性。

Abstract: Chain of Thought (CoT) reasoning has demonstrated remarkable deep reasoning
capabilities in both large language models (LLMs) and multimodal large language
models (MLLMs). However, its reliability is often undermined by the
accumulation of errors in intermediate steps. This paper introduces an novel
approach to calibrate the CoT reasoning accuracy by leveraging the model's
intrinsic veracity encoding. We discover that specific attention head
activations reliably reflect the truthfulness of reasoning steps in CoT. Based
on this insight, we train a confidence predictor to evaluate the correctness of
each reasoning step using these truthfulness-sensitive activations, dynamically
selecting the most plausible reasoning path via beam search. Experimental
results demonstrate that our method significantly outperforms the
state-of-the-art baselines (e.g., Few-Shot CoT, Self-Consistency, and
Self-Evaluation Guided Beam Search) across the mathematical, symbolic, and
commonsense reasoning tasks, exhibiting superior accuracy and reliability in
both unimodal and multimodal settings. We further validate the approach on
large reasoning models, confirming its applicability to specialized reasoning
models. Additionally, we explore the role of the model's self-correction
ability in CoT reasoning. This work provides a novel reliability improvement
path for CoT reasoning with broad application potential.

</details>


### [292] [Automating SPARQL Query Translations between DBpedia and Wikidata](https://arxiv.org/abs/2507.10045)
*Malte Christian Bartels,Debayan Banerjee,Ricardo Usbeck*

Main category: cs.AI

TL;DR: 本研究评估了大型语言模型在不同知识图谱模式间的SPARQL翻译能力，发现在不同模型和提示策略下性能差异显著，且Wikidata到DBpedia的翻译效果优于反向翻译。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在弥合知识图谱（KG）互操作性研究中的一个显著差距，通过严格评估大型语言模型（LLMs）在SPARQL到SPARQL翻译方面的性能。

Method: 研究人员构建了两个基准数据集，分别包含100个在QALD-9-Plus中对齐的DBpedia-Wikidata查询和100个对齐的DBLP-OpenAlex查询。他们测试了Llama-3-8B、DeepSeek-R1-Distill-Llama-70B和Mistral-Large-Instruct-2407这三个大型语言模型，并采用了零样本、少样本以及两种链式思考提示策略进行评估，最后将模型输出与黄金答案进行比较并对错误进行分类。

Result: 研究发现，大型语言模型在跨不同知识图谱模式（如DBpedia、Wikidata、DBLP和OpenAlex）的SPARQL翻译任务上表现出不同的性能，并且在Wikidata到DBpedia的翻译方向上优于DBpedia到Wikidata的方向。

Conclusion: 不同模型和提示策略在SPARQL翻译任务上的性能差异显著，且Wikidata到DBpedia的翻译效果远优于DBpedia到Wikidata的翻译。

Abstract: This paper investigates whether state-of-the-art Large Language Models (LLMs)
can automatically translate SPARQL between popular Knowledge Graph (KG)
schemas. We focus on translations between the DBpedia and Wikidata KG, and
later on DBLP and OpenAlex KG. This study addresses a notable gap in KG
interoperability research by rigorously evaluating LLM performance on
SPARQL-to-SPARQL translation. Two benchmarks are assembled, where the first
align 100 DBpedia-Wikidata queries from QALD-9-Plus; the second contains 100
DBLP queries aligned to OpenAlex, testing generalizability beyond encyclopaedic
KGs. Three open LLMs: Llama-3-8B, DeepSeek-R1-Distill-Llama-70B, and
Mistral-Large-Instruct-2407 are selected based on their sizes and architectures
and tested with zero-shot, few-shot, and two chain-of-thought variants. Outputs
were compared with gold answers, and resulting errors were categorized. We find
that the performance varies markedly across models and prompting strategies,
and that translations for Wikidata to DBpedia work far better than translations
for DBpedia to Wikidata.

</details>


### [293] [On Gradual Semantics for Assumption-Based Argumentation](https://arxiv.org/abs/2507.10076)
*Anna Rapberger,Fabrizio Russo,Antonio Rago,Francesca Toni*

Main category: cs.AI

TL;DR: 为假设基础论证（ABA）引入了新的渐进语义，解决了现有研究的不足，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的渐进语义主要针对抽象、双极和定量双极论证框架，而对假设基础论证（ABA）的研究不足，尽管ABA在许多应用中可能受益于渐进语义。

Method: 通过将ABA框架抽象为双极集基础论证框架，并推广用于QBAF的渐进语义，来定义渐进ABA语义。

Result: 提出的渐进ABA语义满足了QBAF渐进语义的理想性质（如平衡性和单调性），并通过实验与基于论证的方法进行了比较和收敛性评估。

Conclusion: 本文填补了在假设基础论证（ABA）领域引入渐进语义的空白，提出了一系列新的渐进语义，为ABA框架中的假设赋予了辩证强度。

Abstract: In computational argumentation, gradual semantics are fine-grained
alternatives to extension-based and labelling-based semantics . They ascribe a
dialectical strength to (components of) arguments sanctioning their degree of
acceptability. Several gradual semantics have been studied for abstract,
bipolar and quantitative bipolar argumentation frameworks (QBAFs), as well as,
to a lesser extent, for some forms of structured argumentation. However, this
has not been the case for assumption-based argumentation (ABA), despite it
being a popular form of structured argumentation with several applications
where gradual semantics could be useful. In this paper, we fill this gap and
propose a family of novel gradual semantics for equipping assumptions, which
are the core components in ABA frameworks, with dialectical strengths. To do
so, we use bipolar set-based argumentation frameworks as an abstraction of
(potentially non-flat) ABA frameworks and generalise state-of-the-art modular
gradual semantics for QBAFs. We show that our gradual ABA semantics satisfy
suitable adaptations of desirable properties of gradual QBAF semantics, such as
balance and monotonicity. We also explore an argument-based approach that
leverages established QBAF modular semantics directly, and use it as baseline.
Finally, we conduct experiments with synthetic ABA frameworks to compare our
gradual ABA semantics with its argument-based counterpart and assess
convergence.

</details>


### [294] [BlueGlass: A Framework for Composite AI Safety](https://arxiv.org/abs/2507.10106)
*Harshal Nandigramwar,Syed Qutub,Kay-Ulrich Scholl*

Main category: cs.AI

TL;DR: BlueGlass is a new framework that integrates various AI safety tools for better AI system safety. It was tested on object detection models, showing its effectiveness in analyzing model behavior and identifying potential issues.


<details>
  <summary>Details</summary>
Motivation: Existing AI safety tools often target different aspects of model safety and cannot provide full assurance in isolation, highlighting a need for integrated and composite methodologies to ensure the safety of increasingly capable and ubiquitous AI systems.

Method: BlueGlass is a framework providing a unified infrastructure for integrating and composing diverse AI safety tools that operate across model internals and outputs. The paper also presents three safety-oriented analyses: distributional evaluation, probe-based analysis of layer dynamics, and sparse autoencoders for concept identification.

Result: The analyses revealed performance trade-offs and potential failure modes across distributions, highlighted shared hierarchical learning via phase transition in layer dynamics, and identified interpretable concepts using sparse autoencoders. The BlueGlass framework facilitates these composite AI safety workflows.

Conclusion: The paper introduces BlueGlass, a framework for integrating diverse AI safety tools, and demonstrates its utility through three analyses on vision-language models for object detection. This work contributes foundational infrastructure and findings for building more robust and reliable AI systems.

Abstract: As AI systems become increasingly capable and ubiquitous, ensuring the safety
of these systems is critical. However, existing safety tools often target
different aspects of model safety and cannot provide full assurance in
isolation, highlighting a need for integrated and composite methodologies. This
paper introduces BlueGlass, a framework designed to facilitate composite AI
safety workflows by providing a unified infrastructure enabling the integration
and composition of diverse safety tools that operate across model internals and
outputs. Furthermore, to demonstrate the utility of this framework, we present
three safety-oriented analyses on vision-language models for the task of object
detection: (1) distributional evaluation, revealing performance trade-offs and
potential failure modes across distributions; (2) probe-based analysis of layer
dynamics highlighting shared hierarchical learning via phase transition; and
(3) sparse autoencoders identifying interpretable concepts. More broadly, this
work contributes foundational infrastructure and findings for building more
robust and reliable AI systems.

</details>


### [295] [Analysis of AI Techniques for Orchestrating Edge-Cloud Application Migration](https://arxiv.org/abs/2507.10119)
*Sadig Gojayev,Ahmad Anaqreh,Carolina Fortuna*

Main category: cs.AI

TL;DR: 本文使用AI规划和强化学习来解决边缘-云应用迁移问题，将其建模为汉诺塔问题，并提出了新的分类方法进行分析。


<details>
  <summary>Details</summary>
Motivation: 为实现高服务质量和成本效益的边缘-云系统服务交付，需要自动编排应用迁移，但现有方法多为启发式。

Method: 本文从马尔可夫决策过程（MDP）出发，识别、分析和比较了解决汉诺塔类问题的AI规划和强化学习方法，并引入了基于状态空间定义的新的分类方法。

Result: 文章通过新的分类方法，从状态空间定义的角度对所比较的模型进行了分析，旨在理解能够编排新兴计算连续体环境中此类应用迁移的可用技术。

Conclusion: 本文旨在分析和比较用于解决边缘-云应用迁移问题的AI规划和强化学习方法，这些问题可以被建模为经典的汉诺塔问题。

Abstract: Application migration in edge-cloud system enables high QoS and cost
effective service delivery. However, automatically orchestrating such migration
is typically solved with heuristic approaches. Starting from the Markov
Decision Process (MDP), in this paper, we identify, analyze and compare
selected state-of-the-art Artificial Intelligence (AI) planning and
Reinforcement Learning (RL) approaches for solving the class of edge-cloud
application migration problems that can be modeled as Towers of Hanoi (ToH)
problems. We introduce a new classification based on state space definition and
analyze the compared models also through this lense. The aim is to understand
available techniques capable of orchestrating such application migration in
emerging computing continuum environments.

</details>


### [296] [Could you be wrong: Debiasing LLMs using a metacognitive prompt for improving human decision making](https://arxiv.org/abs/2507.10124)
*Thomas T. Hills*

Main category: cs.AI

TL;DR: 识别大型语言模型（LLMs）中的偏见是一个持续存在的挑战。本文借鉴人类决策领域的元认知提示策略，特别是“你可能犯错吗？”这一提示，并将其应用于LLMs。实验表明，该提示能够促使LLMs自我反思，识别并揭示其潜在的偏见、错误和不完整的回答，并提供额外的解释和替代方案。这为开发更通用的LLM去偏策略和改进提示工程提供了新的方向。


<details>
  <summary>Details</summary>
Motivation: 识别大型语言模型（LLMs）中的偏见是一个持续存在的挑战。由于LLMs仍在发展中，今天的有效策略可能在明天就过时了。因此，需要开发能够经受住模型迭代更新的通用去偏策略。本文认为，借鉴人类决策领域中已被证明有效的去偏策略，特别是那些利用提示干预的策略，为解决LLMs的偏见问题提供了一个有前景的方向。

Method: 本文采用了一种名为“你可能犯错吗？”的元认知提示，并将其应用于大型语言模型（LLMs）。通过在一系列关于LLM偏见的问题上使用此提示，并与来自近期关于LLM偏见的文章的问题集进行对比，研究了该提示的效果。

Result: “你可能犯错吗？”这一提示能够促使LLMs在初始回答后产生额外信息，包括其回答原因、识别出的错误、偏见、矛盾证据以及替代方案，这些在初始回答中并未显现。这种元知识揭示了LLMs和用户在理解提示方面可能存在的差异。该提示还能有效纠正包含令人信服但信息不完整的回答。

Conclusion: 人类心理学为提示工程提供了一个新的途径，利用人类决策中基于提示的改进的长期历史。

Abstract: Identifying bias in LLMs is ongoing. Because they are still in development,
what is true today may be false tomorrow. We therefore need general strategies
for debiasing that will outlive current models. Strategies developed for
debiasing human decision making offer one promising approach as they
incorporate an LLM-style prompt intervention designed to bring latent knowledge
into awareness during decision making. LLMs trained on vast amounts of
information contain information about potential biases, counter-arguments, and
contradictory evidence, but that information may only be brought to bear if
prompted. Metacognitive prompts developed in the human decision making
literature are designed to achieve this, and as I demonstrate here, they show
promise with LLMs. The prompt I focus on here is "could you be wrong?"
Following an LLM response, this prompt leads LLMs to produce additional
information, including why they answered as they did, errors, biases,
contradictory evidence, and alternatives, none of which were apparent in their
initial response. Indeed, this metaknowledge often reveals that how LLMs and
users interpret prompts are not aligned. Here I demonstrate this prompt using a
set of questions taken from recent articles about LLM biases, including
implicit discriminatory biases and failures of metacognition. "Could you be
wrong" prompts the LLM to identify its own biases and produce cogent
metacognitive reflection. I also present another example involving convincing
but incomplete information, which is readily corrected by the metacognitive
prompt. In sum, this work argues that human psychology offers a new avenue for
prompt engineering, leveraging a long history of effective prompt-based
improvements to human decision making.

</details>


### [297] [FRSICL: LLM-Enabled In-Context Learning Flight Resource Allocation for Fresh Data Collection in UAV-Assisted Wildfire Monitoring](https://arxiv.org/abs/2507.10134)
*Yousef Emami,Hao Zhou,Miguel Gutierrez Gaitan,Kai Li,Luis Almeida*

Main category: cs.AI

TL;DR: FRSICL是一种新的在线飞行资源分配方案，利用LLM上下文学习来优化无人机辅助的野火监测中的无人机飞行控制和数据收集调度，从而最小化信息年龄。


<details>
  <summary>Details</summary>
Motivation: 为了解决深度强化学习（DRL）在无人机辅助野火监测中采样效率低、仿真到现实差距大以及训练复杂等问题，提出了一种新的方法。

Method: 提出了一种基于LLM的上下文学习（FRSICL）的在线飞行资源分配方案，用于实时优化无人机的飞行控制和数据收集调度。

Result: 仿真结果表明，FRSICL在最小化平均信息年龄方面优于现有的方法。

Conclusion: FRSICL在无人机辅助的野火监测中实现了比PPO和最近邻基线更优的性能，有效降低了信息年龄。

Abstract: Unmanned Aerial Vehicles (UAVs) are vital for public safety, particularly in
wildfire monitoring, where early detection minimizes environmental impact. In
UAV-Assisted Wildfire Monitoring (UAWM) systems, joint optimization of sensor
transmission scheduling and velocity is critical for minimizing Age of
Information (AoI) from stale sensor data. Deep Reinforcement Learning (DRL) has
been used for such optimization; however, its limitations such as low sampling
efficiency, simulation-to-reality gaps, and complex training render it
unsuitable for time-critical applications like wildfire monitoring. This paper
introduces a new online Flight Resource Allocation scheme based on LLM-Enabled
In-Context Learning (FRSICL) to jointly optimize the UAV's flight control and
data collection schedule along the trajectory in real time, thereby
asymptotically minimizing the average AoI across ground sensors. In contrast to
DRL, FRSICL generates data collection schedules and controls velocity using
natural language task descriptions and feedback from the environment, enabling
dynamic decision-making without extensive retraining. Simulation results
confirm the effectiveness of the proposed FRSICL compared to Proximal Policy
Optimization (PPO) and Nearest-Neighbor baselines.

</details>


### [298] [Introducing the Swiss Food Knowledge Graph: AI for Context-Aware Nutrition Recommendation](https://arxiv.org/abs/2507.10156)
*Lubnaa Abdur Rahman,Ioannis Papathanail,Stavroula Mougiakakou*

Main category: cs.AI

TL;DR: 本研究介绍了瑞士食品知识图谱（SwissFKG），一个整合了食谱、食材、营养和饮食限制信息的知识库。利用大语言模型（LLM）进行知识填充和检索增强生成（RAG），实现了更精准的营养咨询服务，为下一代饮食评估工具奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 现有自动膳食评估系统未能充分考虑非视觉因素，如食材替换对营养成分的影响，以及个体饮食需求（如过敏、限制、文化习俗和个人偏好）。瑞士食品信息分散且缺乏中央存储库，无法整合所有相关的营养信息。因此，需要一个全面的资源来解决这些问题。

Method: 利用大语言模型（LLM）驱动的知识图谱填充流程，对现有的瑞士食品信息进行整合和丰富，包括食材替换、营养成分、过敏原和饮食限制等信息。然后，实现了一个基于图谱的检索增强生成（Graph-RAG）应用程序，以展示如何利用知识图谱的自然语言数据结构来回答用户关于营养的特定查询。

Result: 研究成功构建了瑞士首个瑞士食品知识图谱（SwissFKG），并开发了一个LLM驱动的知识增强流程。实验证明，LLM能够有效地为知识图谱添加营养信息。此外，通过Graph-RAG应用，展示了知识图谱能够帮助LLM准确回答用户提出的营养相关问题，评估结果表明用户查询的回答与预期答案高度一致。

Conclusion: 该研究提出了瑞士食品知识图谱（SwissFKG），一个整合了食谱、食材、营养数据、饮食限制、过敏原信息和国家营养指南的知识库，并开发了一个由大语言模型（LLM）驱动的填充流程和Graph-RAG应用，以支持用户特定营养查询。研究结果表明，LLM能够有效地丰富营养信息，并且SwissFKG可以提供食材级别的详细信息，为下一代结合视觉、背景和文化维度的饮食评估工具奠定了基础。

Abstract: AI has driven significant progress in the nutrition field, especially through
multimedia-based automatic dietary assessment. However, existing automatic
dietary assessment systems often overlook critical non-visual factors, such as
recipe-specific ingredient substitutions that can significantly alter
nutritional content, and rarely account for individual dietary needs, including
allergies, restrictions, cultural practices, and personal preferences. In
Switzerland, while food-related information is available, it remains
fragmented, and no centralized repository currently integrates all relevant
nutrition-related aspects within a Swiss context. To bridge this divide, we
introduce the Swiss Food Knowledge Graph (SwissFKG), the first resource, to our
best knowledge, to unite recipes, ingredients, and their substitutions with
nutrient data, dietary restrictions, allergen information, and national
nutrition guidelines under one graph. We establish a LLM-powered enrichment
pipeline for populating the graph, whereby we further present the first
benchmark of four off-the-shelf (<70 B parameter) LLMs for food knowledge
augmentation. Our results demonstrate that LLMs can effectively enrich the
graph with relevant nutritional information. Our SwissFKG goes beyond recipe
recommendations by offering ingredient-level information such as allergen and
dietary restriction information, and guidance aligned with nutritional
guidelines. Moreover, we implement a Graph-RAG application to showcase how the
SwissFKG's rich natural-language data structure can help LLM answer
user-specific nutrition queries, and we evaluate LLM-embedding pairings by
comparing user-query responses against predefined expected answers. As such,
our work lays the foundation for the next generation of dietary assessment
tools that blend visual, contextual, and cultural dimensions of eating.

</details>


### [299] [Should We Ever Prefer Decision Transformer for Offline Reinforcement Learning?](https://arxiv.org/abs/2507.10174)
*Yumi Omori,Zixuan Dong,Keith Ross*

Main category: cs.AI

TL;DR: 本文通过实验证明，在稀疏奖励环境中，过滤行为克隆（FBC）比决策Transformer（DT）更有效且计算成本更低。鉴于DT在稠密奖励环境中也并非最佳选择，本文质疑了DT的适用性。


<details>
  <summary>Details</summary>
Motivation: 该论文旨在探讨决策Transformer（DT）在离线强化学习中的适用性，特别是与传统的基于MLP的算法（如行为克隆（BC）和保守Q学习（CQL））相比。虽然先前有研究声称DT在稀疏奖励和低质量数据方面表现优越，但本文通过实验质疑了这一说法，并提出DT可能并非最佳选择，尤其是在稀疏奖励环境中。

Method: 本文提出了一种名为过滤行为克隆（FBC）的方法，该方法通过滤除低性能轨迹并对过滤后的数据集执行普通行为克隆来实现。在稀疏奖励环境中，该方法在机器人操作任务（Robomimic）和运动基准（D4RL）上与决策Transformer（DT）进行了比较。

Result: 在稀疏奖励环境中，基于MLP的过滤行为克隆（FBC）在机器人操作任务（Robomimic）和运动基准（D4RL）上取得了与决策Transformer（DT）相当或更优越的性能。FBC所需的训练数据更少，计算效率更高。

Conclusion: 该论文通过在机器人操作任务（Robomimic）和运动基准（D4RL）上进行实验，表明基于MLP的过滤行为克隆（FBC）在稀疏奖励环境中与DT相比，可以实现具有竞争性或更优越的性能。FBC通过滤除低性能轨迹然后对过滤后的数据集执行普通行为克隆来实现。FBC不仅简单，而且所需的训练数据更少，计算效率更高。因此，结果表明DT不适用于稀疏奖励环境。考虑到先前的工作表明DT也不适用于稠密奖励环境，本文提出了一个问题：DT是否适用于任何场景？

Abstract: In recent years, extensive work has explored the application of the
Transformer architecture to reinforcement learning problems. Among these,
Decision Transformer (DT) has gained particular attention in the context of
offline reinforcement learning due to its ability to frame return-conditioned
policy learning as a sequence modeling task. Most recently, Bhargava et al.
(2024) provided a systematic comparison of DT with more conventional MLP-based
offline RL algorithms, including Behavior Cloning (BC) and Conservative
Q-Learning (CQL), and claimed that DT exhibits superior performance in
sparse-reward and low-quality data settings.
  In this paper, through experimentation on robotic manipulation tasks
(Robomimic) and locomotion benchmarks (D4RL), we show that MLP-based Filtered
Behavior Cloning (FBC) achieves competitive or superior performance compared to
DT in sparse-reward environments. FBC simply filters out low-performing
trajectories from the dataset and then performs ordinary behavior cloning on
the filtered dataset. FBC is not only very straightforward, but it also
requires less training data and is computationally more efficient. The results
therefore suggest that DT is not preferable for sparse-reward environments.
From prior work, arguably, DT is also not preferable for dense-reward
environments. Thus, we pose the question: Is DT ever preferable?

</details>


### [300] [Survey for Categorising Explainable AI Studies Using Data Analysis Task Frameworks](https://arxiv.org/abs/2507.10208)
*Hamzah Ziadeh,Hendrik Knoche*

Main category: cs.AI

TL;DR: 本研究提出了一个对XAI研究进行分类和比较的新框架，并为改进该领域的研究提供了实用指南。


<details>
  <summary>Details</summary>
Motivation: 当前在可解释人工智能（XAI）用于数据分析任务的研究中存在大量矛盾和缺乏具体的设计建议，这是由于对需要AI辅助的任务理解存在差距。因此，本研究旨在通过提出一个分类和比较XAI研究的方法来解决这些问题。

Method: 本研究借鉴了视觉分析、认知和仪表板设计等多个领域的知识，提出了一个包含“内容”、“原因”和“对象”三个维度的框架，用于对XAI研究进行分类和比较。具体而言，研究分析了当前XAI研究中存在的不足，并提出了改进建议，包括明确用户背景信息和制定研究指南。

Result: 研究识别出当前XAI研究的主要问题包括：任务描述不足、研究缺乏特定情境以及用户测试不充分。为此，研究提出应明确报告用户在领域、AI和数据分析方面的专业知识，以展示研究结果的普适性。同时，研究还提出了一套设计和报告XAI任务的指南，旨在提升XAI社区理解该快速发展领域的整体能力。

Conclusion: 这项研究提出了一个用于分类和比较XAI研究的三维框架（内容、原因和对象），以解决当前XAI研究中存在的矛盾和设计建议不足的问题。研究指出了当前研究存在的三个主要问题：任务描述不足、缺乏特定情境的研究以及用户测试不充分。为了提高XAI研究的可比性和普适性，研究建议应明确报告研究用户的领域、AI和数据分析专业知识。此外，研究还提出了一系列设计和报告XAI任务的指南，以帮助研究人员和设计者更好地理解相关研究、识别研究空白并处理相互矛盾的研究结果。

Abstract: Research into explainable artificial intelligence (XAI) for data analysis
tasks suffer from a large number of contradictions and lack of concrete design
recommendations stemming from gaps in understanding the tasks that require AI
assistance. In this paper, we drew on multiple fields such as visual analytics,
cognition, and dashboard design to propose a method for categorising and
comparing XAI studies under three dimensions: what, why, and who. We identified
the main problems as: inadequate descriptions of tasks, context-free studies,
and insufficient testing with target users. We propose that studies should
specifically report on their users' domain, AI, and data analysis expertise to
illustrate the generalisability of their findings. We also propose study
guidelines for designing and reporting XAI tasks to improve the XAI community's
ability to parse the rapidly growing field. We hope that our contribution can
help researchers and designers better identify which studies are most relevant
to their work, what gaps exist in the research, and how to handle contradictory
results regarding XAI design.

</details>


### [301] [Toward Real-World Table Agents: Capabilities, Workflows, and Design Principles for LLM-based Table Intelligence](https://arxiv.org/abs/2507.10281)
*Jiaming Tian,Liyao Li,Wentao Ye,Haobo Wang,Lingxin Wang,Lihua Yu,Zujie Ren,Gang Chen,Junbo Zhao*

Main category: cs.AI

TL;DR: LLM表格代理在处理真实世界表格数据时面临挑战，现有方法在结构理解、语义理解、检索、推理和跨域泛化方面存在差距，尤其是在文本到SQL任务中。需要改进以提高鲁棒性、泛化性和效率。


<details>
  <summary>Details</summary>
Motivation: 真实世界的表格任务常常涉及噪音、结构异质性和语义复杂性，而现有研究主要集中在干净的学术数据集上，未能充分解决这些问题。因此，有必要对旨在自动化表格工作流的LLM驱动的表格代理进行调研，以应对这些现实世界的挑战。

Method: 本调查通过定义五项核心能力（C1：表格结构理解，C2：表格与查询语义理解，C3：表格检索与压缩，C4：可执行、可追溯推理，C5：跨域泛化）来分析和比较当前基于LLM的表格代理方法。对文本到SQL代理的详细审查揭示了其在学术基准和实际场景之间的性能差距。

Result: 对当前方法的分析表明，在学术基准测试中，LLM表格代理的表现优于实际场景，尤其是在开源模型方面。本调查提供了改进LLM表格代理鲁棒性、泛化性和效率的实用建议。

Conclusion: LLM驱动的表格代理在自动化表格任务方面显示出巨大潜力，但实际应用中仍面临噪音、结构异质性和语义复杂性等挑战。本调查提出了五个核心能力（C1-C5）来分析现有方法，并强调了在实际场景中，尤其是在文本到SQL代理方面，学术基准与真实世界表现之间存在的差距。为了提高LLM表格代理的鲁棒性、泛化性和效率，需要采取具体的改进措施。

Abstract: Tables are fundamental in domains such as finance, healthcare, and public
administration, yet real-world table tasks often involve noise, structural
heterogeneity, and semantic complexity--issues underexplored in existing
research that primarily targets clean academic datasets. This survey focuses on
LLM-based Table Agents, which aim to automate table-centric workflows by
integrating preprocessing, reasoning, and domain adaptation. We define five
core competencies--C1: Table Structure Understanding, C2: Table and Query
Semantic Understanding, C3: Table Retrieval and Compression, C4: Executable
Reasoning with Traceability, and C5: Cross-Domain Generalization--to analyze
and compare current approaches. In addition, a detailed examination of the
Text-to-SQL Agent reveals a performance gap between academic benchmarks and
real-world scenarios, especially for open-source models. Finally, we provide
actionable insights to improve the robustness, generalization, and efficiency
of LLM-based Table Agents in practical settings.

</details>


### [302] [Instance space analysis of the capacitated vehicle routing problem](https://arxiv.org/abs/2507.10397)
*Alessandra M. M. M. Gouvêa,Nuno Paulos,Eduardo Uchoa e Mariá C. V. Nascimento*

Main category: cs.AI

TL;DR: 本研究提出实例空间分析（ISA）方法，结合DIMACS数据集，识别出23个实例特征，并通过降维和机器学习技术创建了实例空间的二维投影，以分析实例结构对元启发式（MH）性能的影响，并提供了一个便于纳入新实例的投影矩阵。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在通过解决理解实例特征与元启发式（MH）性能之间细微关系这一挑战，来推进CVRP研究。

Method: 通过结合实例空间分析（ISA）方法和来自DIMACS第12届车辆路径问题实施挑战的数据集，识别了23个相关的实例特征。利用降维和机器学习方法（PRELIM、SIFTED和PILOT阶段）创建了实例空间的二维投影，以理解实例结构如何影响元启发式（MH）的行为。

Result: 研究识别了23个相关的实例特征，并创建了实例空间的二维投影，展示了实例结构如何影响MH的行为。

Conclusion: 该研究提供了一个投影矩阵，便于将新实例纳入分析，并为CVRP领域的实例分析提供了一种新方法。

Abstract: This paper seeks to advance CVRP research by addressing the challenge of
understanding the nuanced relationships between instance characteristics and
metaheuristic (MH) performance. We present Instance Space Analysis (ISA) as a
valuable tool that allows for a new perspective on the field. By combining the
ISA methodology with a dataset from the DIMACS 12th Implementation Challenge on
Vehicle Routing, our research enabled the identification of 23 relevant
instance characteristics. Our use of the PRELIM, SIFTED, and PILOT stages,
which employ dimensionality reduction and machine learning methods, allowed us
to create a two-dimensional projection of the instance space to understand how
the structure of instances affect the behavior of MHs. A key contribution of
our work is that we provide a projection matrix, which makes it straightforward
to incorporate new instances into this analysis and allows for a new method for
instance analysis in the CVRP field.

</details>


### [303] [Acquiring and Adapting Priors for Novel Tasks via Neural Meta-Architectures](https://arxiv.org/abs/2507.10446)
*Sudarshan Babu*

Main category: cs.AI

TL;DR: 本研究通过设计神经记忆和超网络等新颖的神经网络架构，解决了数据量有限情况下的迁移学习挑战，并在3D场景生成、3D分割和分子属性预测等任务中取得了显著效果。


<details>
  <summary>Details</summary>
Motivation: 迁移学习的能力，即从先验经验迁移知识到新任务的能力，是智能体（包括人类和计算模型）的关键能力。虽然大型预训练模型在许多领域取得了巨大成功，但在计算化学、计算免疫学和医学成像等领域，由于数据量不足，训练这些模型是不可行的。因此，本研究的动机是设计能够弥补数据量不足的限制，高效获取先验知识的神经网络架构。

Method: 本研究通过设计和应用神经网络架构，包括神经记忆和超网络，来解决数据量有限情况下的迁移学习问题。具体来说，研究使用神经记忆来实现少量样本下的非平稳分布适应，并利用超网络（生成网络）与模型无关的元学习（MAML）相结合来获取更具泛化性的先验知识。研究将这些方法应用于3D场景生成和3D分割任务，以实现高效的先验知识获取和快速的文本到3D生成。此外，研究还对现有的分子生成方法进行了改造，作为预训练框架以改进分子属性预测。

Result: 本研究成功设计并验证了能够高效获取先验知识的神经网络架构。研究证明，使用神经记忆可以在数据量很少的情况下适应非平稳分布。超网络架构在与MAML结合使用时，相比于标准网络能够获得更具泛化性的先验知识。在3D场景生成任务中，超网络仅需少量训练场景即可高效获取先验知识，从而实现了更快的文本到3D生成。此外，在3D分割任务中，该框架也能在数据有限的情况下实现高效的先验知识迁移。最后，通过将分子生成方法用作预训练框架，成功改进了分子属性预测的性能，解决了计算免疫学中的挑战。

Conclusion: 研究通过设计能够高效获取先验知识的神经网络架构，解决了在数据量有限的领域（如计算化学、计算免疫学和医学成像）中训练大型预训练模型或基础模型的挑战。研究证明了使用神经记忆可以实现少量样本下的非平稳分布适应，并且超网络（生成其他网络的网络）比标准网络能获得更具泛化性的先验知识。研究将超网络应用于3D场景生成，实现了仅用少量训练场景就能高效获取先验知识，从而加速了文本到3D的生成过程。此外，研究还扩展了超网络框架，通过从先前 مشاهد scenes 高效迁移先验知识，实现了在数据有限的新场景下的3D分割。最后，研究将现有的分子生成方法用作预训练框架，以改进分子属性预测，解决了计算免疫学中的关键挑战。

Abstract: The ability to transfer knowledge from prior experiences to novel tasks
stands as a pivotal capability of intelligent agents, including both humans and
computational models. This principle forms the basis of transfer learning,
where large pre-trained neural networks are fine-tuned to adapt to downstream
tasks. Transfer learning has demonstrated tremendous success, both in terms of
task adaptation speed and performance. However there are several domains where,
due to lack of data, training such large pre-trained models or foundational
models is not a possibility - computational chemistry, computational
immunology, and medical imaging are examples. To address these challenges, our
work focuses on designing architectures to enable efficient acquisition of
priors when large amounts of data are unavailable. In particular, we
demonstrate that we can use neural memory to enable adaptation on
non-stationary distributions with only a few samples. Then we demonstrate that
our hypernetwork designs (a network that generates another network) can acquire
more generalizable priors than standard networks when trained with Model
Agnostic Meta-Learning (MAML). Subsequently, we apply hypernetworks to 3D scene
generation, demonstrating that they can acquire priors efficiently on just a
handful of training scenes, thereby leading to faster text-to-3D generation. We
then extend our hypernetwork framework to perform 3D segmentation on novel
scenes with limited data by efficiently transferring priors from earlier viewed
scenes. Finally, we repurpose an existing molecular generative method as a
pre-training framework that facilitates improved molecular property prediction,
addressing critical challenges in computational immunology

</details>


<div id='physics.app-ph'></div>

# physics.app-ph [[Back]](#toc)

### [304] [Cryogen-free variable-temperature Kelvin probe force microscopy for probing local chemical potential in a graphene heterostructure](https://arxiv.org/abs/2507.09976)
*Namkyung Lee,Seungwon Jung,Baeksan Jang,Sangwook Ha,Joonho Jang*

Main category: physics.app-ph

TL;DR: 开发了一种可变温度KPFM系统，可在低温下稳定运行，并已成功应用于测量单层石墨烯的电子不均匀性和电荷や，揭示了其化学势随温度的变化。


<details>
  <summary>Details</summary>
Motivation: 为了实现对范德华异质结中量子相进行研究，需要一种能够在宽温度范围内稳定且高灵敏度运行的KPFM系统。

Method: 开发了一种基于GM冷却器无低温恒化器的可变温度开尔文探针显微镜（KPFM）系统。该系统集成了定制设计的锁相环和自动增益控制，并带有被动振动隔离，实现了在低温条件下精确测量局部化学势的能力。

Result: 通过测量hBN封装的单层石墨烯（MLG），揭示了空间分辨的电子不均匀性和电荷や。该测量清晰地捕捉到了电荷中性点（CNP）附近化学势随温度的变化，这与MLG的线性能带色散和相互作用驱动的费米速度重整化一致。

Conclusion: 该系统在低温下表现出良好的灵敏度和稳定性，使其成为研究范德华异质结中量子相的通用局部探针。

Abstract: We report the development of a variable-temperature Kelvin probe force
microscopy (KPFM) system capable of stable and highly sensitive operation over
a wide temperature range based on a GM-cooler-based cryogen-free cryostat. The
system incorporates a custom-designed phase-locked loop and automatic gain
control, along with passive vibration isolation, enabling precise measurements
of local chemical potential even under cryogenic conditions. We demonstrate the
performance of this setup by measuring hBN encapsulated monolayer graphene
(MLG), revealing spatially resolved electronic inhomogeneities and charge
puddles. Our measurements clearly capture temperature-dependent variations in
the chemical potential near the charge neutrality point (CNP), consistent with
the linear band dispersion of MLG and interaction-driven renormalization of
Fermi velocity. This work highlights the robust sensitivity and stability of
our system, making it a versatile local probe of quantum phases in van der
Waals heterostructures.

</details>


### [305] [Spin injection in Si-based ferromagnetic tunnel junctions with MgO/MgAl2O4 barriers:Experimental and theoretical investigation of barrier thickness-dependent spin tunneling efficiency](https://arxiv.org/abs/2507.10001)
*Baisen Yu,Shoichi Sato,Masaaki Tanaka,Ryosho Nakane*

Main category: physics.app-ph

TL;DR: 该研究通过实验和理论研究了铁磁隧穿结中的自旋输运，发现隧穿势垒厚度影响自旋极化率。研究提出了一个模型来解释这种现象，并为理解这类器件中的自旋输运物理学提供了见解。


<details>
  <summary>Details</summary>
Motivation: 为了在更薄的隧穿势垒厚度范围内检查自旋输运，并解释在半导体基铁磁隧穿结中观察到的现象，作者提出了一个包含两种不同隧穿路径的现象学模型。

Method: 研究人员通过电子束蒸发，利用滑动挡板系统地改变了非晶MgO和MgAl2O4隧穿势垒层的厚度组合，从而在实验和理论上研究了Si衬底上的Fe/Mg/MgO/MgAl2O4/n+-Si(001)铁磁隧穿结中的自旋输运。

Result: 研究发现，当Fe/Mg/MgO界面位于顶侧时，在10 K下，随着总MgO/MgAl2O4隧穿势垒厚度（tox = 0.47 - 1.4 nm）的增加，隧穿电子的自旋极化率PS增加，并且当tox大于1.1 nm时，PS表现出饱和行为。

Conclusion: 该研究提出了一种基于两种不同隧穿路径的简单现象学隧穿模型，其中包含较高/较低自旋极化和较长/较短衰减长度的路径。该模型能够重现实验中自旋极化率P S与总隧穿势垒厚度t ox之间的关系，表明当t ox减小时，较低自旋极化路径的贡献增加是主要机制。研究讨论了包括本征和非本征隧穿机制在内的该现象的可能起源。该分析方法为半导体基铁磁结中的详细自旋输运物理学提供了见解，特别是在隧穿势垒层非常薄的情况下。

Abstract: We have experimentally and theoretically investigated the spin transport in
Fe/Mg/MgO/MgAl2O4/n+-Si(001) ferromagnetic tunnel junctions on a Si substrate,
by systematically varying the thickness combination of amorphous MgO and
MgAl2O4 tunnel barrier layers with a sliding shutter between the evaporation
sources and substrate during electron-beam evaporation. A technical advantage
of MgAl2O4 is that a continuous and flat thin film is realized on a Si
substrate even when the MgAl2O4 thickness is as thin as 0.5 nm, unlike MgO,
which enables us to examine the spin transport in a thinner range of the tunnel
barrier thickness. Our distinct finding is as follows: When the Fe/Mg/MgO
interface is used on the top side, the spin polarization PS of tunneling
electrons increases at 10 K as the total MgO/MgAl2O4 tunnel barrier thickness
(tox = 0.47 - 1.4 nm) is increased, regardless of different thickness
combinations, and PS shows saturation-like behavior when tox is above 1.1 nm.
Since this feature cannot be explained by the well-known conductivity mismatch
in semiconductor-based ferromagnetic tunnel junctions, we propose a simple
phenomenological tunneling model based on two different direct tunneling paths,
which have higher/lower spin polarizations with longer/shorter decay lengths.
Our numerical calculation reproduces the relationship between the spin
polarization PS and total tunnel barrier thickness tox in the experiments,
indicating that the dominant mechanism is an increasing contribution of the
lower spin polarization path as tox is decreased. We discuss possible origins
for this phenomenon including intrinsic and extrinsic tunneling mechanisms. Our
analysis method provides an insight into the detailed spin transport physics in
semiconductor-based ferromagnetic junctions, particularly, with a very thin
tunnel barrier layer.

</details>


### [306] [Fast-Response Variable-Frequency Series-Capacitor Buck VRM Through Integrated Control Approaches](https://arxiv.org/abs/2507.10086)
*Guanyu Qian,Haoxian Yan,Xiaofan Cui*

Main category: physics.app-ph

TL;DR: 针对数据中心AI负载对电压调节快速响应的需求，提出了一种结合线性与非线性控制器的SCB转换器控制方案。该方案通过5S框架建立小信号模型，并利用PMP优化大信号瞬态响应。仿真和硬件测试表明，该方案能显著提高电压恢复速度并稳定抑制扰动。


<details>
  <summary>Details</summary>
Motivation: 数据中心电压调节模块（VRM）需要快速响应电压调节功能，以应对人工智能（AI）工作负载中的小幅度波动和突然的满负荷阶跃变化。

Method: 研究采用开关同步采样状态空间（5S）框架推导了精确的小信号模型，得到离散时间传递函数和根轨迹，用于数字设计。为了加速大信号瞬态响应，提出了一种基于庞特里亚金最大值原理（PMP）的时间最优控制策略，放宽了开关约束，计算了时间最优开关序列。最后，提出了一种过渡逻辑来集成高带宽小信号控制器和大信号控制器。

Result: 仿真结果表明，SCB转换器在重负载阶跃时，输出电压恢复速度比仅使用线性控制器的设计快十倍以上。初步的硬件测试表明，该设计能够稳定地抑制重负载扰动，且稳态误差为零。

Conclusion: 该研究提出了一种控制方案，结合了线性控制器和非线性控制器，用于可变频串联电容降压（SCB）转换器，以满足数据中心AI工作负载对快速响应电压调节的需求。

Abstract: Fast-response voltage regulation is essential for data-center Voltage
Regulation Modules (VRMs) powering Artificial Intelligence (AI) workloads,
which exhibit both small-amplitude fluctuations and abrupt full-load steps.
This paper introduces a control scheme that integrates a linear controller and
a nonlinear controller for variable-frequency Series-Capacitor Buck (SCB)
converters. First, an accurate small-signal model is derived via a
Switching-Synchronized Sampled State-Space (5S) framework, yielding
discrete-time transfer functions and root-locus insights for direct digital
design. A critical concern for SCB converters is series-capacitor oscillation
during heavy load steps if the strict switching sequence is not maintained. To
accelerate large-signal transients, a time-optimal control strategy based on
Pontryagins Maximum Principle (PMP) relaxes the switching constraints to
compute time-optimal switching sequences. A transition logic is then proposed
to integrate the high-bandwidth small-signal controller and the large-signal
controller. Simulations demonstrate a rapid output voltage recovery under a
heavy load step-up, over ten times faster than a linear controller-only design.
Preliminary hardware tests indicate a stable rejection to heavy load
disturbances with zero steady-state error.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [307] [Discrete Differential Principle for Continuous Smooth Function Representation](https://arxiv.org/abs/2507.09480)
*Guoyou Wang,Yihua Tan,Shiqi Liu*

Main category: cs.LG

TL;DR: 提出一种新的离散微分算子，用于处理泰勒公式在离散情况下的维度灾难和误差传播问题，并在多个领域证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: 泰勒公式在离散情况下存在维度灾难和导数计算中的误差传播问题。

Method: 提出一种新的离散微分算子，利用泰勒级数推导的范德蒙德系数矩阵来估计导数和局部表示连续光滑函数。

Result: 该方法同时计算所有低于采样点数的阶数导数，缓解了维度灾难和误差传播，并进行了二维扩展，在视觉表示、特征提取、流体力学和跨媒体成像等领域具有广泛应用前景。

Conclusion: 该方法通过数学方法证明了其在估计导数和函数表示方面的有效性，并得到了更紧密的误差界限。

Abstract: Taylor's formula holds significant importance in function representation,
such as solving differential difference equations, ordinary differential
equations, partial differential equations, and further promotes applications in
visual perception, complex control, fluid mechanics, weather forecasting and
thermodynamics. However, the Taylor's formula suffers from the curse of
dimensionality and error propagation during derivative computation in discrete
situations. In this paper, we propose a new discrete differential operator to
estimate derivatives and to represent continuous smooth function locally using
the Vandermonde coefficient matrix derived from truncated Taylor series. Our
method simultaneously computes all derivatives of orders less than the number
of sample points, inherently mitigating error propagation. Utilizing
equidistant uniform sampling, it achieves high-order accuracy while alleviating
the curse of dimensionality. We mathematically establish rigorous error bounds
for both derivative estimation and function representation, demonstrating
tighter bounds for lower-order derivatives. We extend our method to the
two-dimensional case, enabling its use for multivariate derivative
calculations. Experiments demonstrate the effectiveness and superiority of the
proposed method compared to the finite forward difference method for derivative
estimation and cubic spline and linear interpolation for function
representation. Consequently, our technique offers broad applicability across
domains such as vision representation, feature extraction, fluid mechanics, and
cross-media imaging.

</details>


### [308] [Recurrent Expansion: A Pathway Toward the Next Generation of Deep Learning](https://arxiv.org/abs/2507.08828)
*Tarek Berghout*

Main category: cs.LG

TL;DR: This paper introduces Recurrent Expansion (RE), a new learning paradigm that enables AI models to learn from their own evolving behavior, leading to self-improvement and more adaptive systems. It includes extensions for parallel and diverse model architectures, aiming for introspective and scalable AI.


<details>
  <summary>Details</summary>
Motivation: To advance beyond conventional ML and DL by introducing a new learning paradigm that learns from the evolving behavior of models themselves, enabling iterative self-improvement and creating a new class of intelligent models.

Method: Recurrent Expansion (RE) learns from the evolving behavior of models by analyzing multiple mappings of data through identical deep architectures and their internal representations in conjunction with observed performance signals. Extensions include Multiverse RE (MVRE) for aggregating signals from parallel instances, Heterogeneous MVRE (HMVRE) for incorporating diverse architectures, and Sc-HMVRE for scalability and adaptability.

Result: The paper introduces Recurrent Expansion (RE) and its variants (MVRE, HMVRE, Sc-HMVRE) as a framework for self-evolving AI systems that learn from their own behavior and predecessors' experiences.

Conclusion: RE presents a shift in DL from purely representational learning to behavior-aware, self-evolving systems, laying the groundwork for intelligent models capable of reasoning over their own learning dynamics toward scalable, introspective, and adaptive AI.

Abstract: This paper introduces Recurrent Expansion (RE) as a new learning paradigm
that advances beyond conventional Machine Learning (ML) and Deep Learning (DL).
While DL focuses on learning from static data representations, RE proposes an
additional dimension: learning from the evolving behavior of models themselves.
RE emphasizes multiple mappings of data through identical deep architectures
and analyzes their internal representations (i.e., feature maps) in conjunction
with observed performance signals such as loss. By incorporating these
behavioral traces, RE enables iterative self-improvement, allowing each model
version to gain insight from its predecessors. The framework is extended
through Multiverse RE (MVRE), which aggregates signals from parallel model
instances, and further through Heterogeneous MVRE (HMVRE), where models of
varying architectures contribute diverse perspectives. A scalable and adaptive
variant, Sc-HMVRE, introduces selective mechanisms and scale diversity for
real-world deployment. Altogether, RE presents a shift in DL: from purely
representational learning to behavior-aware, self-evolving systems. It lays the
groundwork for a new class of intelligent models capable of reasoning over
their own learning dynamics, offering a path toward scalable, introspective,
and adaptive artificial intelligence. A simple code example to support
beginners in running their own experiments is provided in Code Availability
Section of this paper.

</details>


### [309] [Efficient Triple Modular Redundancy for Reliability Enhancement of DNNs Using Explainable AI](https://arxiv.org/abs/2507.08829)
*Kimia Soroush,Nastaran Shirazi,Mohsen Raji*

Main category: cs.LG

TL;DR: 本研究提出一种基于XAI（LRP）的TMR方法，通过选择关键权重来提高DNN可靠性，在AlexNet模型上实现了60%以上的可靠性提升，且开销可控。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络（DNN）在安全关键领域广泛应用，确保其可靠性至关重要。三重模冗余（TMR）是一种有效提高DNN可靠性的技术，但其开销较大。因此，需要一种有效的TMR选择标准来处理其开销问题。本研究旨在利用XAI技术提供关于神经元和权重重要性的见解，将其作为TMR选择的度量标准。

Method: 提出了一种基于梯度下降的可解释人工智能（XAI）方法，具体为层层相关性传播（LRP），用于计算DNN参数的重要性得分，并利用这些得分来选择需要TMR保护的关键权重。

Result: 所提出的方法在VGG16和AlexNet模型以及MNIST和CIFAR-10数据集上进行了评估。结果表明，该方法能够保护AlexNet模型，在比特错误率为10^-4时，可靠性提高了60%以上，同时开销与现有技术相当。

Conclusion: 本研究提出了一种基于可解释人工智能（XAI）的低成本三重模冗余（TMR）方法，以提高深度神经网络（DNN）对抗比特翻转错误的能力。通过使用层层相关性传播（LRP）计算DNN参数的重要性得分，并以此为依据选择TMR保护的关键权重，实现了在保持与现有技术相当的开销下，将AlexNet模型在比特错误率为10^-4时的可靠性提高了60%以上。

Abstract: Deep Neural Networks (DNNs) are widely employed in safety-critical domains,
where ensuring their reliability is essential. Triple Modular Redundancy (TMR)
is an effective technique to enhance the reliability of DNNs in the presence of
bit-flip faults. In order to handle the significant overhead of TMR, it is
applied selectively on the parameters and components with the highest
contribution at the model output. Hence, the accuracy of the selection
criterion plays the key role on the efficiency of TMR. This paper presents an
efficient TMR approach to enhance the reliability of DNNs against bit-flip
faults using an Explainable Artificial Intelligence (XAI) method. Since XAI can
provide valuable insights about the importance of individual neurons and
weights in the performance of the network, they can be applied as the selection
metric in TMR techniques. The proposed method utilizes a low-cost,
gradient-based XAI technique known as Layer-wise Relevance Propagation (LRP) to
calculate importance scores for DNN parameters. These scores are then used to
enhance the reliability of the model, with the most critical weights being
protected by TMR. The proposed approach is evaluated on two DNN models, VGG16
and AlexNet, using datasets such as MNIST and CIFAR-10. The results demonstrate
that the method can protect the AlexNet model at a bit error rate of 10-4,
achieving over 60% reliability improvement while maintaining the same overhead
as state-of-the-art methods.

</details>


### [310] [A Hybrid Machine Learning Framework for Optimizing Crop Selection via Agronomic and Economic Forecasting](https://arxiv.org/abs/2507.08832)
*Niranjan Mallikarjun Sindhur,Pavithra C,Nivya Muchikel*

Main category: cs.LG

TL;DR: 本研究为印度卡纳塔克邦的农民开发了一个结合了机器学习（随机森林和LSTM）和语音界面的决策支持系统，以预测农作物适宜性和市场价格，从而提高农民的盈利能力和财务韧性。


<details>
  <summary>Details</summary>
Motivation: 发展中地区的农民（例如印度卡纳塔克邦）面临着严峻的市场和气候波动挑战，同时由于识字率的限制，他们往往被排斥在数字化革命之外。因此，需要一种能够解决这些双重挑战的决策支持系统。

Method: 本研究提出了一种结合机器学习和人机交互的混合推荐引擎。该引擎集成了两个预测模型：1. 随机森林分类器，利用土壤、气候和实时天气数据评估农艺适宜性；2. 长短期记忆（LSTM）网络，预测农艺上可行作物的市场价格。该系统通过一个端到端的、基于语音的本地化（卡纳达语）界面交付，利用了经过微调的语音识别和高保真语音合成模型，以确保低识字率用户的可访问性。

Result: 随机森林模型在适宜性预测方面达到了98.5%的准确率。LSTM模型在预测收获时期的价格方面具有较低的误差率。

Conclusion: 该系统通过结合机器学习和以用户为中心的交互设计，为受市场和气候波动影响的农民提供了一种创新的决策支持工具。它通过混合推荐引擎，集成了一个随机森林分类器（用于评估农艺适宜性）和一个长短期记忆（LSTM）网络（用于预测市场价格），从而实现了从“什么可以种植？”到“什么最有利可图？”的转变，有效降低了经济风险。该系统采用了本地化（卡纳达语）的语音交互界面，克服了识字率低的障碍。实验结果表明，随机森林模型在适宜性预测方面达到98.5%的准确率，LSTM模型在预测收获时期的价格方面误差率低。这项工作为增强边缘化农业社区的财务韧性提供了一个可扩展且有影响力的解决方案。

Abstract: Farmers in developing regions like Karnataka, India, face a dual challenge:
navigating extreme market and climate volatility while being excluded from the
digital revolution due to literacy barriers. This paper presents a novel
decision support system that addresses both challenges through a unique
synthesis of machine learning and human-computer interaction. We propose a
hybrid recommendation engine that integrates two predictive models: a Random
Forest classifier to assess agronomic suitability based on soil, climate, and
real-time weather data, and a Long Short-Term Memory (LSTM) network to forecast
market prices for agronomically viable crops. This integrated approach shifts
the paradigm from "what can grow?" to "what is most profitable to grow?",
providing a significant advantage in mitigating economic risk. The system is
delivered through an end-to-end, voice-based interface in the local Kannada
language, leveraging fine-tuned speech recognition and high-fidelity speech
synthesis models to ensure accessibility for low-literacy users. Our results
show that the Random Forest model achieves 98.5% accuracy in suitability
prediction, while the LSTM model forecasts harvest-time prices with a low
margin of error. By providing data-driven, economically optimized
recommendations through an inclusive interface, this work offers a scalable and
impactful solution to enhance the financial resilience of marginalized farming
communities.

</details>


### [311] [Effects of structural properties of neural networks on machine learning performance](https://arxiv.org/abs/2507.10005)
*Yash Arya,Sang Hoon Lee*

Main category: cs.LG

TL;DR: 神经网络的图结构会影响其性能，具有密集社区结构的网络的性能更好。


<details>
  <summary>Details</summary>
Motivation: 近年来，基于图的机器学习技术，如强化学习和图神经网络，受到了广泛关注。尽管一些近期研究开始探索神经网络的图结构与其预测性能之间的关系，但它们通常将自己限制在模型网络的较窄范围内，特别是缺乏社区等中观结构。

Method: 本研究通过对具有异质性度分布和社区结构的真实网络结构进行更全面的调查，并采用随机网络和无标度网络等模型网络，以及与生物神经网络及其子集进行比较来进行详细分析，从而推进了这一领域。

Result: 研究发现结构属性确实在一定程度上影响了性能。具体来说，具有连贯的、密集互联的社区的网络的学习能力得到了增强。

Conclusion: 研究结果表明，具有连贯的、密集互联的社区的网络的学习能力得到了增强。将这些发现与生物神经网络的比较强调了其与真实世界结构的相关性，并暗示了值得进一步探索的有趣联系。本研究为网络科学和机器学习做出了有意义的贡献，其见解可以激发设计更具生物学启发的神经网络。

Abstract: In recent years, graph-based machine learning techniques, such as
reinforcement learning and graph neural networks, have garnered significant
attention. While some recent studies have started to explore the relationship
between the graph structure of neural networks and their predictive
performance, they often limit themselves to a narrow range of model networks,
particularly lacking mesoscale structures such as communities. Our work
advances this area by conducting a more comprehensive investigation,
incorporating realistic network structures characterized by heterogeneous
degree distributions and community structures, which are typical
characteristics of many real networks. These community structures offer a
nuanced perspective on network architecture. Our analysis employs model
networks such as random and scale-free networks, alongside a comparison with a
biological neural network and its subsets for more detailed analysis. We
examine the impact of these structural attributes on the performance of image
classification tasks. Our findings reveal that structural properties do affect
performance to some extent. Specifically, networks featuring coherent, densely
interconnected communities demonstrate enhanced learning capabilities. The
comparison with the biological neural network emphasizes the relevance of our
findings to real-world structures, suggesting an intriguing connection worth
further exploration. This study contributes meaningfully to network science and
machine learning, providing insights that could inspire the design of more
biologically informed neural networks.

</details>


### [312] [LoRA Is Slower Than You Think](https://arxiv.org/abs/2507.08833)
*Seokmin Ko*

Main category: cs.LG

TL;DR: LoRA 微调 LLM 存在速度不一致的问题。研究人员提出了新的微调方法，在保证性能的同时提高了训练速度的一致性。


<details>
  <summary>Details</summary>
Motivation: LoRA 在微调大型语言模型（LLMs）时，并不一致地提供速度提升，这种不一致性促使研究人员进行研究。

Method: 通过引入少量可训练的低秩权重矩阵来微调大型语言模型（LLMs），并对 LoRA 的性能进行全面分析，找出限制其加速的潜在因素。

Result: 提出的方法实现了与 LoRA 相当或更优的性能，并提供了更一致的训练速度改进。

Conclusion: LoRA 在所有模型架构和训练设置中并不一致地提供速度提升。研究人员提出了一些更有效的方法来微调 LLM，这些方法在具有可比性或更优的性能的同时，提供了更一致的训练速度改进。

Abstract: Low-Rank Adaptation (LoRA) is one of the most widely used techniques for
fine-tuning large language models (LLMs). By introducing a small number of
trainable low-rank weight matrices, LoRA substantially reduces the number of
parameters that need to be updated, offering significant advantages in memory
consumption and computational efficiency compared to full fine-tuning. However,
we observed that LoRA does not consistently provide speed improvements across
all model architectures and training setups. Motivated by this inconsistency,
we conduct a comprehensive analysis of LoRA's performance and investigate the
underlying factors limiting its speedup. Based on our findings, we propose
several methods for more efficient fine-tuning of LLMs. We empirically evaluate
these methods and compare them to LoRA, demonstrating that our approach
achieves comparable or superior performance while delivering more consistent
training speed improvements. Our work offers valuable insights and practical
guidelines for practitioners seeking to optimize LLM fine-tuning under resource
constraints.

</details>


### [313] [Physical Informed Neural Networks for modeling ocean pollutant](https://arxiv.org/abs/2507.08834)
*Karishma Battina,Prathamesh Dinesh Joshi,Raj Abhijit Dandekar,Rajat Dandekar,Sreedath Panat*

Main category: cs.LG

TL;DR: 本研究提出了一种基于PINN的海洋污染物扩散模拟框架，有效解决了传统方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统数值方法在模拟大尺度、复杂海洋区域的污染物扩散时面临挑战。

Method: 提出了一种物理信息神经网络（PINN）框架，通过将物理定律和带噪声的合成数据嵌入到神经网络训练过程中来模拟二维平流扩散方程支配的污染物扩散。

Result: 该模型通过嵌入物理定律和拟合带噪声的合成数据，实现了物理上一致的预测，并能处理非线性动力学和边界/初始条件问题。

Conclusion: PINN框架能够有效模拟污染物在海洋中的扩散，并能处理非线性动力学及边界和初始条件

Abstract: Traditional numerical methods often struggle with the complexity and scale of
modeling pollutant transport across vast and dynamic oceanic domains. This
paper introduces a Physics-Informed Neural Network (PINN) framework to simulate
the dispersion of pollutants governed by the 2D advection-diffusion equation.
The model achieves physically consistent predictions by embedding physical laws
and fitting to noisy synthetic data, generated via a finite difference method
(FDM), directly into the neural network training process. This approach
addresses challenges such as non-linear dynamics and the enforcement of
boundary and initial conditions. Synthetic data sets, augmented with varying
noise levels, are used to capture real-world variability. The training
incorporates a hybrid loss function including PDE residuals, boundary/initial
condition conformity, and a weighted data fit term. The approach takes
advantage of the Julia language scientific computing ecosystem for
high-performance simulations, offering a scalable and flexible alternative to
traditional solvers

</details>


### [314] [Representation learning with a transformer by contrastive learning for money laundering detection](https://arxiv.org/abs/2507.08835)
*Harold Guéneau,Alain Celisse,Pascal Delange*

Main category: cs.LG

TL;DR: 该论文提出了一种新的洗钱检测方法，该方法利用 Transformer 神经网络和对比学习来学习时间序列数据的表示。该方法比基于规则和 LSTM 的方法更有效。


<details>
  <summary>Details</summary>
Motivation: 解决洗钱检测问题。

Method: 通过利用结构化时间序列的质量和数量数据，并使用 Transformer 神经网络进行此操作。第一步是无监督的，其中 Transformer 模型通过对比学习来学习时间序列的表示。第二步是利用这些表示来生成所有观测值的洗钱得分。引入了两阈值方法，通过 Benjamini-Hochberg (BH) 程序来控制假阳性率。

Result: 实验证实，Transformer 能够生成有用的表示，能够利用洗钱模式，并且该方法在识别欺诈者和非欺诈者方面表现出更高的能力，同时将假阳性率控制在可控范围内。

Conclusion: 该方法在保持假阳性率可控的情况下，在识别欺诈者和非欺诈者方面表现出更高的能力，并且该方法能够学习到有用的表示，能够利用洗钱模式。

Abstract: The present work tackles the money laundering detection problem. A new
procedure is introduced which exploits structured time series of both
qualitative and quantitative data by means of a transformer neural network. The
first step of this procedure aims at learning representations of time series
through contrastive learning (without any labels). The second step leverages
these representations to generate a money laundering scoring of all
observations. A two-thresholds approach is then introduced, which ensures a
controlled false-positive rate by means of the Benjamini-Hochberg (BH)
procedure. Experiments confirm that the transformer is able to produce general
representations that succeed in exploiting money laundering patterns with
minimal supervision from domain experts. It also illustrates the higher ability
of the new procedure for detecting nonfraudsters as well as fraudsters, while
keeping the false positive rate under control. This greatly contrasts with
rule-based procedures or the ones based on LSTM architectures.

</details>


### [315] [Accuracy and Consumption analysis from a compressed model by CompactifAI from Multiverse Computing](https://arxiv.org/abs/2507.08836)
*Damien Fovet,Shashank Chamoli,Sarah Oury,Srishti Singhal*

Main category: cs.LG

TL;DR: CompactifAI efficiently compresses Llama 3.1 8B with no accuracy loss, reducing resource usage and costs.


<details>
  <summary>Details</summary>
Motivation: This study aims to evaluate the performance of a compression method, CompactifAI, developed by Multiverse Computing, applied to the large language model Llama 3.1 8B, focusing on model efficiency and accuracy.

Method: The study evaluated the performance of the CompactifAI compression method on the Llama 3.1 8B model. Model efficiency (energy consumption) was assessed using Codecarbon, and accuracy was evaluated using Ragas. A comparison was made between the compressed model and its full-size version.

Result: The compressed model using CompactifAI significantly reduced computational resources and maintained model accuracy compared to the full-size version.

Conclusion: The compressed model using CompactifAI significantly reduced computational resources while maintaining model accuracy, making it more efficient, scalable, and cost-effective.

Abstract: This study evaluates the performance of a compression method, called
CompactifAI, developed by Multiverse Computing, applied to the large language
model Llama 3.1 8B\cite{llama}. The evaluation focused on model efficiency (in
terms of energy consumption) and accuracy using respectively the frameworks
Codecarbon\cite{codecarbon} and Ragas\cite{ragas}. A comparison was performed
between the model compressed with
CompactifAI\cite{compactifai}\cite{compactifai2} and its full-size version. Our
findings reveal that the compressed model using CompactifAI not only
significantly reduced the computational resources but also maintained the model
accuracy, making the model more efficient, scalable and cost-effective.

</details>


### [316] [wd1: Weighted Policy Optimization for Reasoning in Diffusion Language Models](https://arxiv.org/abs/2507.08838)
*Xiaohang Tang,Rares Dolga,Sangwoong Yoon,Ilija Bogunovic*

Main category: cs.LG

TL;DR: $\\,\text{wd1}$是一种新颖的RL方法，通过一次策略似然近似来提高dLLMs的推理能力，无需SFT即可实现更高的准确率和计算效率。


<details>
  <summary>Details</summary>
Motivation: 为了提高扩散型大型语言模型（dLLMs）的推理能力，并解决现有强化学习（RL）方法在dLLMs中由于需要近似多个策略似然而导致的计算开销和偏差问题。

Method: $\\,\text{wd1}$是一种新颖的策略优化方法，它将目标重新表述为加权似然，仅需要对当前参数化策略似然进行一次近似，从而解决了现有dLLMs强化学习方法中由于需要多次策略似然近似而带来的计算开销和潜在偏差问题。

Result: 实验结果表明，$\\,\text{wd1}$在无需监督微调（SFT）或任何监督数据的情况下，在推理准确率上超越了现有RL方法，并且在计算效率方面也表现更优，包括减少了训练时间和每次梯度下降的函数评估次数（NFEs）。

Conclusion: $\\,\text{wd1}$是一种更有效、更高效的方法，用于将强化学习应用于扩散型大型语言模型（dLLMs）的推理任务，其无需监督微调（SFT）或任何监督数据即可在推理基准测试中取得优于现有RL方法高达16%的准确率，同时还能减少训练时间和每次梯度下降的函数评估次数（NFEs）。

Abstract: Improving the reasoning capabilities of diffusion-based large language models
(dLLMs) through reinforcement learning (RL) remains an open problem. The
intractability of dLLMs likelihood function necessitates approximating the
current, old, and reference policy likelihoods at each policy optimization
step. This reliance introduces additional computational overhead and lead to
potentially large bias -- particularly when approximation errors occur in the
denominator of policy ratios used for importance sampling. To mitigate these
issues, we introduce $\mathtt{wd1}$, a novel policy optimization approach that
reformulates the objective as a weighted likelihood, requiring only a single
approximation for the current parametrized policy likelihood. Experiments on
widely used reasoning benchmarks demonstrate that $\mathtt{wd1}$, without
supervised fine-tuning (SFT) or any supervised data, outperforms existing RL
methods for dLLMs, achieving up to 16% higher accuracy. $\mathtt{wd1}$ delivers
additional computational gains, including reduced training time and fewer
function evaluations (NFEs) per gradient step. These findings, combined with
the simplicity of method's implementation and R1-Zero-like training (no SFT),
position $\mathtt{wd1}$ as a more effective and efficient method for applying
RL to dLLMs reasoning.

</details>


### [317] [Domain-Adaptive Diagnosis of Lewy Body Disease with Transferability Aware Transformer](https://arxiv.org/abs/2507.08839)
*Xiaowei Yu,Jing Zhang,Tong Chen,Yan Zhuang,Minheng Chen,Chao Cao,Yanjun Lyu,Lu Zhang,Li Su,Tianming Liu,Dajiang Zhu*

Main category: cs.LG

TL;DR: TAT模型通过适应性地从AD数据中转移知识并减少域偏移，有效解决了LBD数据稀缺和域偏移问题，提高了LBD的诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 路易体痴呆症（LBD）是一种常见但研究不足的痴呆症，给公共卫生带来巨大负担。由于LBD数据稀缺，限制了深度学习的有效性。而阿尔茨海默病（AD）的数据相对丰富，可以进行知识转移。然而，LBD和AD数据存在域偏移问题，这给知识转移带来了挑战。

Method: 提出了一种可转移性感知Transformer（TAT）模型，该模型利用从结构MRI衍生的结构连接（SC）作为训练数据，通过注意力机制自适应地分配更大的权重给疾病可转移特征，同时抑制特定领域的特征，以减少域偏移并提高诊断准确性。

Result: 实验结果证明了TAT的有效性，能够提高LBD诊断的准确性。

Conclusion: 这项研究首次探索了在数据稀缺和域偏移条件下，从阿尔茨海默病（AD）到路易体痴呆症（LBD）的域适应，为罕见病的域适应诊断提供了一个有前景的框架。

Abstract: Lewy Body Disease (LBD) is a common yet understudied form of dementia that
imposes a significant burden on public health. It shares clinical similarities
with Alzheimer's disease (AD), as both progress through stages of normal
cognition, mild cognitive impairment, and dementia. A major obstacle in LBD
diagnosis is data scarcity, which limits the effectiveness of deep learning. In
contrast, AD datasets are more abundant, offering potential for knowledge
transfer. However, LBD and AD data are typically collected from different sites
using different machines and protocols, resulting in a distinct domain shift.
To effectively leverage AD data while mitigating domain shift, we propose a
Transferability Aware Transformer (TAT) that adapts knowledge from AD to
enhance LBD diagnosis. Our method utilizes structural connectivity (SC) derived
from structural MRI as training data. Built on the attention mechanism, TAT
adaptively assigns greater weights to disease-transferable features while
suppressing domain-specific ones, thereby reducing domain shift and improving
diagnostic accuracy with limited LBD data. The experimental results demonstrate
the effectiveness of TAT. To the best of our knowledge, this is the first study
to explore domain adaptation from AD to LBD under conditions of data scarcity
and domain shift, providing a promising framework for domain-adaptive diagnosis
of rare diseases.

</details>


### [318] [Iceberg: Enhancing HLS Modeling with Synthetic Data](https://arxiv.org/abs/2507.09948)
*Zijian Ding,Tung Nguyen,Weikai Li,Aditya Grover,Yizhou Sun,Jason Cong*

Main category: cs.LG

TL;DR: Iceberg通过预训练和合成数据增强技术，显著提升了HLS预测模型的泛化能力，在多项基准测试中取得了优异表现。


<details>
  <summary>Details</summary>
Motivation: 深度学习HLS预测模型在泛化方面存在挑战，本研究旨在解决这一问题。

Method: Iceberg是一种合成数据增强方法，通过扩展大型语言模型（LLM）生成的程序和针对未见过的设计配置的弱标签来工作。其弱标签生成方法与上下文模型架构集成，实现了从实际和邻近标签进行元学习。

Result: Iceberg在适应六个真实世界应用时，将几何平均模型准确率提高了86.4%；在适应两个不同的测试数据集时，离线DSE性能分别提高了2.47倍和1.12倍。

Conclusion: 本研究通过预训练和合成数据增强方法Iceberg，显著提高了深度学习HLS预测模型的泛化能力，有效缩小了泛化差距。

Abstract: Deep learning-based prediction models for High-Level Synthesis (HLS) of
hardware designs often struggle to generalize. In this paper, we study how to
close the generalizability gap of these models through pretraining on synthetic
data and introduce Iceberg, a synthetic data augmentation approach that expands
both large language model (LLM)-generated programs and weak labels of unseen
design configurations. Our weak label generation method is integrated with an
in-context model architecture, enabling meta-learning from actual and proximate
labels. Iceberg improves the geometric mean modeling accuracy by $86.4\%$ when
adapt to six real-world applications with few-shot examples and achieves a
$2.47\times$ and a $1.12\times$ better offline DSE performance when adapting to
two different test datasets. Our open-sourced code is here:
\href{https://github.com/UCLA-VAST/iceberg}{https://github.com/UCLA-VAST/iceberg}

</details>


### [319] [Zero-Shot Neural Architecture Search with Weighted Response Correlation](https://arxiv.org/abs/2507.08841)
*Kun Jing,Luoyu Chen,Jungang Xu,Jianwei Tai,Yiyu Wang,Shuaimin Li*

Main category: cs.LG

TL;DR: 提出了一种名为WRCor的新的无训练估计代理，用于神经架构搜索（NAS），以解决现有方法计算成本高和效率低的问题。WRCor通过分析跨不同输入的响应相关性来评估架构的性能。实验证明，WRCor及其投票代理比现有方法更有效，并且在ImageNet-1k数据集上实现了具有竞争力的结果。


<details>
  <summary>Details</summary>
Motivation: 现有的零样本NAS方法使用无训练代理来加速架构估计，但其有效性、稳定性和通用性仍然不足。

Method: 提出了一种新的无训练估计代理，称为加权响应相关（WRCor），它利用跨不同输入样本的响应的相关系数矩阵来计算估计架构的代理分数，以衡量其表现力和通用性。

Result: 实验结果表明，WRCor及其投票代理比现有的代理更有效的估计策略。通过将它们与不同的搜索策略结合使用，所提出的零样本NAS算法在ImageNet-1k数据集上实现了22.1%的测试误差，并且在4个GPU小时内就完成了架构搜索。

Conclusion: 所提出的零样本神经架构搜索（NAS）算法在不同的搜索空间中优于大多数现有的NAS算法，并且能够在ImageNet-1k数据集上以22.1%的测试误差在4个GPU小时内发现一个架构。

Abstract: Neural architecture search (NAS) is a promising approach for automatically
designing neural network architectures. However, the architecture estimation of
NAS is computationally expensive and time-consuming because of training
multiple architectures from scratch. Although existing zero-shot NAS methods
use training-free proxies to accelerate the architecture estimation, their
effectiveness, stability, and generality are still lacking. We present a novel
training-free estimation proxy called weighted response correlation (WRCor).
WRCor utilizes correlation coefficient matrices of responses across different
input samples to calculate the proxy scores of estimated architectures, which
can measure their expressivity and generalizability. Experimental results on
proxy evaluation demonstrate that WRCor and its voting proxies are more
efficient estimation strategies than existing proxies. We also apply them with
different search strategies in architecture search. Experimental results on
architecture search show that our zero-shot NAS algorithm outperforms most
existing NAS algorithms in different search spaces. Our NAS algorithm can
discover an architecture with a 22.1% test error on the ImageNet-1k dataset
within 4 GPU hours. All codes are publicly available at
https://github.com/kunjing96/ZSNAS-WRCor.git.

</details>


### [320] [Gradients as an Action: Towards Communication-Efficient Federated Recommender Systems via Adaptive Action Sharing](https://arxiv.org/abs/2507.08842)
*Zhufeng Lu,Chentao Jia,Ming Hu,Xiaofei Xie,Mingsong Chen*

Main category: cs.LG

TL;DR: FedRAS通过动作共享和自适应聚类解决了联邦推荐中的通信开销和训练效率问题，显著减少了通信量且不影响性能。


<details>
  <summary>Details</summary>
Motivation: 现有的联邦推荐系统（FedRecs）面临通信开销高（由于大量的物品嵌入）和训练效率低（由于异构环境和设备）的问题。现有的压缩技术会引入参数误差，导致模型性能下降。

Method: 本文提出了一种名为FedRAS的通信高效联邦推荐框架，采用动作共享策略将用户和物品嵌入的梯度聚类成有限数量的更新动作进行通信，而不是直接压缩嵌入矩阵。通过自适应聚类机制来适应异构设备和网络环境。

Result: FedRAS可以将通信负载减少高达96.88%，同时在各种异构场景下不牺牲推荐性能。

Conclusion: FedRAS通过动作共享策略减少了通信开销并提高了训练效率，同时在异构环境下保持了推荐性能，通信负载减少高达96.88%。

Abstract: As a promising privacy-aware collaborative model training paradigm, Federated
Learning (FL) is becoming popular in the design of distributed recommender
systems. However, Federated Recommender Systems (FedRecs) greatly suffer from
two major problems: i) extremely high communication overhead due to massive
item embeddings involved in recommendation systems, and ii) intolerably low
training efficiency caused by the entanglement of both heterogeneous network
environments and client devices. Although existing methods attempt to employ
various compression techniques to reduce communication overhead, due to the
parameter errors introduced by model compression, they inevitably suffer from
model performance degradation. To simultaneously address the above problems,
this paper presents a communication-efficient FedRec framework named FedRAS,
which adopts an action-sharing strategy to cluster the gradients of item
embedding into a specific number of model updating actions for communication
rather than directly compressing the item embeddings. In this way, the cloud
server can use the limited actions from clients to update all the items. Since
gradient values are significantly smaller than item embeddings, constraining
the directions of gradients (i.e., the action space) introduces smaller errors
compared to compressing the entire item embedding matrix into a reduced space.
To accommodate heterogeneous devices and network environments, FedRAS
incorporates an adaptive clustering mechanism that dynamically adjusts the
number of actions. Comprehensive experiments on well-known datasets demonstrate
that FedRAS can reduce the size of communication payloads by up to 96.88%,
while not sacrificing recommendation performance within various heterogeneous
scenarios. We have open-sourced FedRAS at
https://github.com/mastlab-T3S/FedRAS.

</details>


### [321] [GUIDE: Towards Scalable Advising for Research Ideas](https://arxiv.org/abs/2507.08870)
*Yaowenqi Liu,BingXu Meng,Rui Pan,Jerry Huang,Tong Zhang*

Main category: cs.LG

TL;DR: 研究表明，通过压缩文献数据库和引入结构化推理，小型AI模型在论文投稿的反馈和优化方面可以优于大型模型，实现高录用率。


<details>
  <summary>Details</summary>
Motivation: 当前的AI研究虽然能够进行自动假设生成和实验设计，但在提供高质量、有充分理由的反馈以优化假设和实验设计方面，仍缺乏可扩展的咨询系统。

Method: 通过探索模型大小、上下文长度、置信度估计和结构化推理过程等关键因素来开发强大的咨询系统。

Result: 一个更小的模型，结合压缩的文献数据库和结构化推理框架，在ICLR 2025的自我排名提交的论文中，其录用率超过了像Deepseek-R1这样强大的通用语言模型。当仅限于高置信度预测时，该系统在ICLR 2025测试集上的录用率超过90%，证明了其在提升假设生成和实验设计质量与效率方面的潜力。

Conclusion: 经过优化的、更小的模型在特定任务上可以超越大型通用模型，通过对文献数据库进行压缩并结合结构化推理框架，可以实现高置信度预测和高论文录用率。

Abstract: The field of AI research is advancing at an unprecedented pace, enabling
automated hypothesis generation and experimental design across diverse domains
such as biology, mathematics, and artificial intelligence. Despite these
advancements, there remains a significant gap in the availability of scalable
advising systems capable of providing high-quality, well-reasoned feedback to
refine proposed hypotheses and experimental designs. To address this challenge,
we explore key factors that underlie the development of robust advising
systems, including model size, context length, confidence estimation, and
structured reasoning processes. Our findings reveal that a relatively small
model, when equipped with a well-compressed literature database and a
structured reasoning framework, can outperform powerful general-purpose
language models such as Deepseek-R1 in terms of acceptance rates for
self-ranked top-30% submissions to ICLR 2025. Moreover, when limited to
high-confidence predictions, our system achieves an acceptance rate exceeding
90% on the ICLR 2025 test set, underscoring its potential to significantly
enhance the quality and efficiency of hypothesis generation and experimental
design. The code is released at
https://github.com/HowardLiu0830/GUIDE-Research-Idea-Evaluation.

</details>


### [322] [Can We Predict Your Next Move Without Breaking Your Privacy?](https://arxiv.org/abs/2507.08843)
*Arpita Soni,Sahil Tripathi,Gautam Siddharth Kashyap,Manaswi Kulahara,Mohammad Anas Azeez,Zohaib Hasan Siddiqui,Nipun Joshi,Jiechao Gao*

Main category: cs.LG

TL;DR: FLLL3M：一种保护隐私的联邦学习框架，利用大型语言模型进行移动建模，实现高精度和低资源需求。


<details>
  <summary>Details</summary>
Motivation: 为了在保护用户隐私的前提下，实现高精度的移动建模和Next-Location Prediction（NxLP）。

Method: FLLL3M框架利用联邦学习的原理，将用户数据保留在本地，并通过外积机制利用大型语言模型（LLMs）进行移动建模和Next-Location Prediction（NxLP）。

Result: FLLL3M在Gowalla、WeePlace、Brightkite和FourSquare等数据集上取得了SOT结果，准确率和MRR均表现优异，同时参数量减少了高达45.6%，内存使用量减少了52.7%。

Conclusion: FLLL3M是一种在保持用户数据本地化和利用LLM的同时，通过高效的外积机制实现高精度和低资源需求的联邦学习框架，可在Next-Location Prediction（NxLP）任务中取得SOT结果，同时显著减少参数和内存使用量。

Abstract: We propose FLLL3M--Federated Learning with Large Language Models for Mobility
Modeling--a privacy-preserving framework for Next-Location Prediction (NxLP).
By retaining user data locally and leveraging LLMs through an efficient outer
product mechanism, FLLL3M ensures high accuracy with low resource demands. It
achieves SOT results on Gowalla (Acc@1: 12.55, MRR: 0.1422), WeePlace (10.71,
0.1285), Brightkite (10.42, 0.1169), and FourSquare (8.71, 0.1023), while
reducing parameters by up to 45.6% and memory usage by 52.7%.

</details>


### [323] [DAFOS: Dynamic Adaptive Fanout Optimization Sampler](https://arxiv.org/abs/2507.08845)
*Irfan Ullah,Young-Koo Lee*

Main category: cs.LG

TL;DR: DAFOS通过动态调整采样和优先处理重要节点，显著提高了图神经网络的训练速度和准确性。


<details>
  <summary>Details</summary>
Motivation: 为了解决图神经网络在扩展性和效率方面受限于均匀邻居采样和静态扇出的问题。

Method: 提出了一种名为DAFOS的动态自适应扇出优化采样器，通过动态调整扇出、优先处理重要节点（基于节点度进行评分）、增加训练过程中扇出以及集成早期停止机制来优化图神经网络的训练。

Result: 在ogbnarxiv、Reddit和ogbn-products三个基准数据集上进行的实验表明，DAFOS相比于现有最先进的方法显著提高了训练速度和准确性。具体来说，在ogbnarxiv数据集上实现了3.57倍的加速，在Reddit数据集上实现了12.6倍的加速，并在ogbn-arxiv数据集上将F1分数从68.5%提高到71.21%，在ogbn-products数据集上将F1分数从73.78%提高到76.88%。

Conclusion: DAFOS作为一种高效且可扩展的大规模图神经网络训练解决方案，在提高训练速度和准确性方面展现出巨大潜力。

Abstract: Graph Neural Networks (GNNs) are becoming an essential tool for learning from
graph-structured data, however uniform neighbor sampling and static fanout
settings frequently limit GNNs' scalability and efficiency. In this paper, we
propose the Dynamic Adaptive Fanout Optimization Sampler (DAFOS), a novel
approach that dynamically adjusts the fanout based on model performance and
prioritizes important nodes during training. Our approach leverages node
scoring based on node degree to focus computational resources on structurally
important nodes, incrementing the fanout as the model training progresses.
DAFOS also integrates an early stopping mechanism to halt training when
performance gains diminish. Experiments conducted on three benchmark datasets,
ogbnarxiv, Reddit, and ogbn-products, demonstrate that our approach
significantly improves training speed and accuracy compared to a
state-of-the-art approach. DAFOS achieves a 3.57x speedup on the ogbn-arxiv
dataset and a 12.6x speedup on the Reddit dataset while improving the F1 score
from 68.5% to 71.21% on ogbn-arxiv and from 73.78% to 76.88% on the
ogbn-products dataset, respectively. These results highlight the potential of
DAFOS as an efficient and scalable solution for large-scale GNN training.

</details>


### [324] [Average Sensitivity of Hierarchical $k$-Median Clustering](https://arxiv.org/abs/2507.10296)
*Shijie Li,Weiqiang He,Ruobing Bai,Pan Peng*

Main category: cs.LG

TL;DR: 本研究提出了一种更稳定、更有效的层次聚类算法，用于处理大型动态数据集。


<details>
  <summary>Details</summary>
Motivation: 为了解决现代算法在处理大型、动态数据集时，层次聚类对数据微小扰动敏感的问题，本研究聚焦于分层k-中值聚类问题。

Method: 通过测量随机数据点删除时输出的预期变化来分析算法的平均敏感性。

Result: 与单一连接聚类和CLNNSS算法的确定性变体相比，所提出的算法具有较低的平均敏感性，表明其更稳定。理论证明和实验结果均显示了该算法的高聚类质量和鲁棒性。

Conclusion: 该研究提出了一个用于分层k-中值聚类问题的高效算法，并证明了其低平均敏感性和高聚类质量。实验结果验证了该算法的鲁棒性和有效性。

Abstract: Hierarchical clustering is a widely used method for unsupervised learning
with numerous applications. However, in the application of modern algorithms,
the datasets studied are usually large and dynamic. If the hierarchical
clustering is sensitive to small perturbations of the dataset, the usability of
the algorithm will be greatly reduced. In this paper, we focus on the
hierarchical $k$ -median clustering problem, which bridges hierarchical and
centroid-based clustering while offering theoretical appeal, practical utility,
and improved interpretability. We analyze the average sensitivity of algorithms
for this problem by measuring the expected change in the output when a random
data point is deleted. We propose an efficient algorithm for hierarchical
$k$-median clustering and theoretically prove its low average sensitivity and
high clustering quality. Additionally, we show that single linkage clustering
and a deterministic variant of the CLNSS algorithm exhibit high average
sensitivity, making them less stable. Finally, we validate the robustness and
effectiveness of our algorithm through experiments.

</details>


### [325] [Networked Information Aggregation via Machine Learning](https://arxiv.org/abs/2507.09683)
*Michael Kearns,Aaron Roth,Emily Ryu*

Main category: cs.LG

TL;DR: DAG中的分布式学习代理可以通过利用父代理的预测来聚合信息，但信息聚合的能力取决于DAG的深度以及特征在路径中的表示情况。


<details>
  <summary>Details</summary>
Motivation: 研究了在有向无环图（DAG）中嵌入学习代理的分布式学习问题，其中每个代理只能观察到部分特征，并旨在实现信息聚合。

Method: 代理按与DAG的拓扑排序一致的顺序顺序学习，并根据观察结果提交模型以预测实值标签。每个代理观察其在DAG中的父代理的预测，并使用它们直接观察到的实例特征以及其父代理的预测作为附加特征来训练其模型。

Result: 信息聚合的发生取决于DAG的深度，只要路径能够充分代表所有相关特征。即使在线性情况下，在信息聚合无法发生的分布上，即使在任意大的DAG中，如果DAG的深度不足（例如，辐条和轮毂拓扑），也无法实现信息聚合。

Conclusion: 该过程足以实现信息聚合，即DAG中的某个代理能够学习一个模型，其误差可与拥有所有特征的直接访问能力的最优模型相媲美。我们为线性和通用假设类别提供了上限和下限。

Abstract: We study a distributed learning problem in which learning agents are embedded
in a directed acyclic graph (DAG). There is a fixed and arbitrary distribution
over feature/label pairs, and each agent or vertex in the graph is able to
directly observe only a subset of the features -- potentially a different
subset for every agent. The agents learn sequentially in some order consistent
with a topological sort of the DAG, committing to a model mapping observations
to predictions of the real-valued label. Each agent observes the predictions of
their parents in the DAG, and trains their model using both the features of the
instance that they directly observe, and the predictions of their parents as
additional features. We ask when this process is sufficient to achieve
\emph{information aggregation}, in the sense that some agent in the DAG is able
to learn a model whose error is competitive with the best model that could have
been learned (in some hypothesis class) with direct access to \emph{all}
features, despite the fact that no single agent in the network has such access.
We give upper and lower bounds for this problem for both linear and general
hypothesis classes. Our results identify the \emph{depth} of the DAG as the key
parameter: information aggregation can occur over sufficiently long paths in
the DAG, assuming that all of the relevant features are well represented along
the path, and there are distributions over which information aggregation cannot
occur even in the linear case, and even in arbitrarily large DAGs that do not
have sufficient depth (such as a hub-and-spokes topology in which the spoke
vertices collectively see all the features). We complement our theoretical
results with a comprehensive set of experiments.

</details>


### [326] [Assuring the Safety of Reinforcement Learning Components: AMLAS-RL](https://arxiv.org/abs/2507.08848)
*Calum Corrie Imrie,Ioannis Stefanakos,Sepeedeh Shahbeigi,Richard Hawkins,Simon Burton*

Main category: cs.LG

TL;DR: This paper presents AMLAS-RL, a new framework to ensure the safety of machine learning systems in critical applications by adapting existing methods for reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: While Reinforcement Learning (RL) is well-suited for Cyber-Physical Systems (CPS) due to its ability to handle complex and dynamic environments, ensuring the safety of RL components in safety-critical applications is a significant challenge. Existing methods like Safe-RL offer limited assurance across the RL lifecycle, and the AMLAS methodology, while effective for supervised learning, does not directly apply to RL.

Method: The AMLAS-RL framework is developed by adapting the existing AMLAS methodology. It guides the generation of assurance arguments for RL-enabled systems through an iterative process.

Result: The paper demonstrates the applicability of the AMLAS-RL framework using a wheeled vehicle example tasked with reaching a target goal without collision, showcasing its utility in assuring the safety of RL-enabled systems.

Conclusion: The paper introduces AMLAS-RL, a framework adapting the AMLAS methodology to generate assurance arguments for Reinforcement Learning (RL) enabled systems. This is achieved through an iterative process designed to address the unique safety challenges posed by RL in safety-critical applications.

Abstract: The rapid advancement of machine learning (ML) has led to its increasing
integration into cyber-physical systems (CPS) across diverse domains. While CPS
offer powerful capabilities, incorporating ML components introduces significant
safety and assurance challenges. Among ML techniques, reinforcement learning
(RL) is particularly suited for CPS due to its capacity to handle complex,
dynamic environments where explicit models of interaction between system and
environment are unavailable or difficult to construct. However, in
safety-critical applications, this learning process must not only be effective
but demonstrably safe. Safe-RL methods aim to address this by incorporating
safety constraints during learning, yet they fall short in providing systematic
assurance across the RL lifecycle. The AMLAS methodology offers structured
guidance for assuring the safety of supervised learning components, but it does
not directly apply to the unique challenges posed by RL. In this paper, we
adapt AMLAS to provide a framework for generating assurance arguments for an
RL-enabled system through an iterative process; AMLAS-RL. We demonstrate
AMLAS-RL using a running example of a wheeled vehicle tasked with reaching a
target goal without collision.

</details>


### [327] [On Evaluating Performance of LLM Inference Serving Systems](https://arxiv.org/abs/2507.09019)
*Amey Agrawal,Nitin Kedia,Anmol Agarwal,Jayashree Mohan,Nipun Kwatra,Souvik Kundu,Ramachandran Ramjee,Alexey Tumanov*

Main category: cs.LG

TL;DR: LLM推理评估方法存在缺陷，常出现反模式。本研究识别了这些问题，提出了改进框架和清单，以实现更准确、可复现的评估，并以推测解码为例进行了说明。


<details>
  <summary>Details</summary>
Motivation: 当前LLM推理系统的评估方法存在根本性缺陷，常常出现反模式，模糊了真实的性能特征，阻碍了科学进步。LLM推理的特点，如双阶段性质（预填充和解码）、异构工作负载以及对时间敏感的交互式使用需求，使得这些反模式尤为严重。

Method: 通过对近期LLM推理系统进行系统性分析，识别出基线公平性、评估设置和指标设计三个关键维度中存在的常见反模式。通过案例研究（如推测解码）展示了这些反模式如何导致误导性结论，并提供了一个全面的清单和框架来识别和避免这些问题。

Result: 识别出LLM推理评估中的常见反模式，如不充分的基线比较、未能代表生产环境的工作负载选择以及隐藏性能变异性（如生成停滞）的指标归一化。提供了一个用于识别和避免这些反模式的全面清单和框架，并通过对推测解码的案例研究证明了其有效性。

Conclusion: 本研究通过识别和分析LLM推理评估中的常见反模式，提出了一种改进评估方法的框架和清单，以确保评估的稳健性、可复现性和与实际需求的对齐，最终加速LLM推理系统的真正进步。

Abstract: The rapid evolution of Large Language Model (LLM) inference systems has
yielded significant efficiency improvements. However, our systematic analysis
reveals that current evaluation methodologies frequently exhibit fundamental
flaws, often manifesting as common evaluation anti-patterns that obscure true
performance characteristics and impede scientific progress. Through a
comprehensive examination of recent systems, we identify recurring
anti-patterns across three key dimensions: Baseline Fairness, Evaluation Setup,
and Metric Design. These anti-patterns are uniquely problematic for LLM
inference due to its dual-phase nature combining distinct prefill and decode
operations, its handling of highly heterogeneous workloads, and its strict
temporal requirements for interactive use. We demonstrate how common
anti-patterns -- such as inadequate baseline comparisons that conflate
engineering effort with algorithmic novelty, workload selections that fail to
represent production scenarios, and metric normalizations that hide substantial
performance variability like generation stalls-lead to misleading conclusions.
To address these challenges, we provide a comprehensive checklist derived from
our analysis, establishing a framework for recognizing and avoiding these
anti-patterns in favor of robust LLM inference evaluation. To demonstrate the
practical application of our framework, we present a case study analyzing
speculative decoding, a technique whose bursty, non-uniform token generation is
easily misinterpreted when evaluated using approaches characteristic of these
anti-patterns. Our work establishes a rigorous foundation for evaluation
methodology, enabling meaningful comparisons, ensuring reproducible results,
and ultimately accelerating genuine progress in LLM inference systems by moving
beyond common anti-patterns to align evaluation with real-world requirements.

</details>


### [328] [Foundation models for time series forecasting: Application in conformal prediction](https://arxiv.org/abs/2507.08858)
*Sami Achour,Yassine Bouher,Duong Nguyen,Nicolas Chesneau*

Main category: cs.LG

TL;DR: Foundation models for time series forecasting offer better conformal prediction intervals than traditional methods, especially with limited data, due to higher accuracy and more stable calibration.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of foundation models (FMs) in time series forecasting, particularly their zero-shot capabilities in conformal prediction, by comparing their performance against traditional methods.

Method: This study compares the performance of Time Series Foundation Models (TSFMs) with traditional methods (statistical models, gradient boosting) within a conformal prediction framework. The comparison focuses on the reliability of prediction intervals generated by these methods.

Result: TSFMs demonstrate two key advantages over traditional methods: 1. More reliable conformalized prediction intervals when data volume is limited, attributed to higher predictive accuracy. 2. More stable calibration process due to the use of more data for calibration. These benefits are more significant when less data is available, as traditional models require substantial data for effective training.

Conclusion: Foundation models (FMs) have the potential to improve the reliability of conformal prediction in time series applications, especially in data-constrained scenarios. TSFMs provide more reliable prediction intervals than traditional methods when data is limited, due to their superior accuracy and more stable calibration process facilitated by larger calibration datasets.

Abstract: The zero-shot capabilities of foundation models (FMs) for time series
forecasting offer promising potentials in conformal prediction, as most of the
available data can be allocated to calibration. This study compares the
performance of Time Series Foundation Models (TSFMs) with traditional methods,
including statistical models and gradient boosting, within a conformal
prediction setting. Our findings highlight two key advantages of TSFMs. First,
when the volume of data is limited, TSFMs provide more reliable conformalized
prediction intervals than classic models, thanks to their superior predictive
accuracy. Second, the calibration process is more stable because more data are
used for calibration. Morever, the fewer data available, the more pronounced
these benefits become, as classic models require a substantial amount of data
for effective training. These results underscore the potential of foundation
models in improving conformal prediction reliability in time series
applications, particularly in data-constrained cases. All the code to reproduce
the experiments is available.

</details>


### [329] [e-Profits: A Business-Aligned Evaluation Metric for Profit-Sensitive Customer Churn Prediction](https://arxiv.org/abs/2507.08860)
*Awais Manzoor,M. Atif Qureshi,Etain Kidney,Luca Longo*

Main category: cs.LG

TL;DR: e-Profits是一个新的业务评估指标，用于客户流失预测模型。它通过考虑客户价值、保留概率和干预成本，并结合Kaplan-Meier生存分析来量化模型表现，更能反映财务结果，并能识别出被传统指标忽略但对高价值客户ROI更优的模型。


<details>
  <summary>Details</summary>
Motivation: 传统的客户流失预测模型评估指标（如AUC和F1分数）未能有效反映财务结果，可能误导企业在客户关系管理中的战略决策。因此，需要一个能够直接关联业务成果和模型性能的评估指标。

Method: 该研究引入了一种名为e-Profits的新型业务指标，用于评估客户流失预测模型的性能。该指标基于客户价值、保留概率和干预成本来量化模型表现，并利用Kaplan-Meier生存分析来估计个性化的保留率，从而支持对每位客户进行精细化评估。研究人员在两个电信数据集上对六种分类器进行了基准测试，并与传统的AUC和F1分数等指标进行了比较，证明了e-Profits能够重塑模型排名，揭示了先前被忽视的模型在财务上的优势。

Result: 研究结果表明，与传统的AUC或F1分数相比，e-Profits指标能够改变不同客户流失预测模型的排名。e-Profits揭示了一些在传统指标下表现不佳但实际上具有财务优势的模型。此外，该指标还能提供细粒度的洞察，帮助识别哪些模型能够为高价值客户群体最大限度地提高投资回报率。

Conclusion: e-Profits是一个新的业务指标，用于评估客户生命周期管理中的客户流失预测模型。该指标通过整合客户价值、保留概率和干预成本来量化模型表现，并能进行每位客户的评估。与传统的AUC和F1分数等指标不同，e-Profits更能反映实际的财务结果，帮助企业做出更明智的战略决策。该指标还能识别出那些被传统指标忽略但能在投资回报率方面表现出色的模型，尤其是在高价值客户群体中。e-Profits旨在作为一个易于理解的辅助工具，支持企业在市场营销和分析团队中进行模型评估，特别适合那些优先考虑利润驱动决策的团队。

Abstract: Retention campaigns in customer relationship management often rely on churn
prediction models evaluated using traditional metrics such as AUC and F1-score.
However, these metrics fail to reflect financial outcomes and may mislead
strategic decisions. We introduce e-Profits, a novel business-aligned
evaluation metric that quantifies model performance based on customer-specific
value, retention probability, and intervention costs. Unlike existing
profit-based metrics such as Expected Maximum Profit, which assume fixed
population-level parameters, e-Profits uses Kaplan-Meier survival analysis to
estimate personalised retention rates and supports granular, per customer
evaluation. We benchmark six classifiers across two telecom datasets (IBM Telco
and Maven Telecom) and demonstrate that e-Profits reshapes model rankings
compared to traditional metrics, revealing financial advantages in models
previously overlooked by AUC or F1-score. The metric also enables segment-level
insight into which models maximise return on investment for high-value
customers. e-Profits is designed as an understandable, post hoc tool to support
model evaluation in business contexts, particularly for marketing and analytics
teams prioritising profit-driven decisions. All source code is available at:
https://github.com/matifq/eprofits.

</details>


### [330] [Domain Borders Are There to Be Crossed With Federated Few-Shot Adaptation](https://arxiv.org/abs/2507.10160)
*Manuel Röder,Christoph Raab,Frank-Michael Schleif*

Main category: cs.LG

TL;DR: FedAcross+ 通过优化联邦学习框架，在资源受限和数据存在域偏移的情况下，实现了高效、隐私保护的客户端自适应学习。


<details>
  <summary>Details</summary>
Motivation: 为了解决联邦学习在实际应用中面临的三个主要挑战：1. 目标域自适应需要耗费人力的数据标注成本；2. 客户端设备数据收集的协变量偏移问题；3. 资源受限环境下模型更新的持续性或定期更新的挑战（受限于数据传输、信道可用性和能效）。

Method: 提出并扩展了一个名为 FedAcross+ 的联邦学习框架，该框架利用预训练的源模型（包含深度骨干、自适应模块和分类器），在客户端设备上通过冻结骨干和分类器，仅调整域自适应线性层来最小化计算开销。该框架还支持流式数据处理以适应非平稳环境。

Result: 实验结果表明，FedAcross+ 在低端客户端设备上实现了有竞争力的适应性，成功解决了域偏移问题，并能在资源受限环境下进行零星的模型更新，确保了部署的实用性和无缝性。

Conclusion: FedAcross+ 框架通过冻结预训练模型的骨干和分类器，仅在客户端进行域自适应线性层的调整，成功解决了联邦学习在资源受限环境下的效率和可扩展性问题，实现了在目标域样本量有限的情况下，在低端客户端设备上进行有竞争力的适应，并有效解决了域偏移问题。此外，该框架支持流式数据处理和零星模型更新，适用于非平稳环境和资源受限场景。

Abstract: Federated Learning has emerged as a leading paradigm for decentralized,
privacy-preserving learning, particularly relevant in the era of interconnected
edge devices equipped with sensors. However, the practical implementation of
Federated Learning faces three primary challenges: the need for human
involvement in costly data labelling processes for target adaptation, covariate
shift in client device data collection due to environmental factors affecting
sensors, leading to discrepancies between source and target samples, and the
impracticality of continuous or regular model updates in resource-constrained
environments due to limited data transmission capabilities and technical
constraints on channel availability and energy efficiency. To tackle these
issues, we expand upon an efficient and scalable Federated Learning framework
tailored for real-world client adaptation in industrial settings. This
framework leverages a pre-trained source model comprising a deep backbone, an
adaptation module, and a classifier running on a powerful server. By freezing
the backbone and classifier during client adaptation on resource-constrained
devices, we allow the domain adaptive linear layer to handle target domain
adaptation, thus minimizing overall computational overhead. Furthermore, this
setup, designated as FedAcross+, is extended to encompass the processing of
streaming data, thereby rendering the solution suitable for non-stationary
environments. Extensive experimental results demonstrate the effectiveness of
FedAcross+ in achieving competitive adaptation on low-end client devices with
limited target samples, successfully addressing the challenge of domain shift.
Moreover, our framework accommodates sporadic model updates within
resource-constrained environments, ensuring practical and seamless deployment.

</details>


### [331] [On the under-reaching phenomenon in message-passing neural PDE solvers: revisiting the CFL condition](https://arxiv.org/abs/2507.08861)
*Lucas Tesan,Mikel M. Iparraguirre,David Gonzalez,Pedro Martins,Elias Cueto*

Main category: cs.LG

TL;DR: 本研究为GNN求解PDE的消息传递迭代次数设定了理论下界，以简化超参数调优，并确保模型精度。


<details>
  <summary>Details</summary>
Motivation: 为了减少图神经网络（GNN）在求解偏微分方程（PDE）时对繁琐的超参数调优的需求，本研究提出了精确的消息传递迭代次数下界。

Method: 通过将问题本身的物理特性与GNN的消息传递需求联系起来，推导了针对双曲型、抛物型和椭圆型三类基本PDE的下界。研究了控制问题的物理常数、空间和时间离散化以及GNN中的消息传递机制之间的关系。

Result: 研究结果表明，所提出的下界是精确的，并通过四个不同的方程算例进行了验证。当迭代次数满足下界时，GNN能够准确捕捉PDE的内在规律，得到高精度的解。

Conclusion: 该研究为图神经网络（GNN）求解偏微分方程（PDE）所需的最小消息传递迭代次数提供了精确的下界。当消息传递迭代次数低于此下界时，即使采用深层GNN架构，信息也无法在网络中有效传播，导致求解精度不佳。相反，当满足所提出的下界时，GNN参数化能够使模型准确捕捉潜在现象，从而获得足够精度的求解器。

Abstract: This paper proposes sharp lower bounds for the number of message passing
iterations required in graph neural networks (GNNs) when solving partial
differential equations (PDE). This significantly reduces the need for
exhaustive hyperparameter tuning. Bounds are derived for the three fundamental
classes of PDEs (hyperbolic, parabolic and elliptic) by relating the physical
characteristics of the problem in question to the message-passing requirement
of GNNs. In particular, we investigate the relationship between the physical
constants of the equations governing the problem, the spatial and temporal
discretisation and the message passing mechanisms in GNNs.
  When the number of message passing iterations is below these proposed limits,
information does not propagate efficiently through the network, resulting in
poor solutions, even for deep GNN architectures. In contrast, when the
suggested lower bound is satisfied, the GNN parameterisation allows the model
to accurately capture the underlying phenomenology, resulting in solvers of
adequate accuracy.
  Examples are provided for four different examples of equations that show the
sharpness of the proposed lower bounds.

</details>


### [332] [Convergence of Agnostic Federated Averaging](https://arxiv.org/abs/2507.10325)
*Herlock,Rahimi,Dionysis Kalogerias*

Main category: cs.LG

TL;DR: 在实践中，联邦学习（FL）常面临客户端间歇性参与和未知参与概率的挑战。本研究通过分析agnostic FedAvg算法在随机客户端可用性下的优化问题，首次在通用、非均匀、随机客户端参与下，为agnostic FedAvg提供了收敛保证，且无需了解参与分布。实验结果表明，agnostic FedAvg的性能优于其他加权聚合的FedAvg变体。


<details>
  <summary>Details</summary>
Motivation: 现有的联邦学习收敛结果通常假设所有设备都参与，或需要了解（实际是均匀的）客户端可用性分布，这些假设在实践中很少成立。然而，实际的联邦学习部署经常面临客户端间歇性参与以及未知的、可能有偏差的参与概率这一关键挑战。

Method: 通过表征在随机且可变大小的客户端可用性下，agnostic FedAvg算法的优化问题，并严格证明了其在凸、可能非光滑损失下的收敛性，达到了标准的O(1/sqrt(T))收敛率。

Result: 在聚合期T下，收敛性达到了O(1/sqrt(T))的标准速率。

Conclusion: 该分析首次在通用、非均匀、随机客户端参与下，为agnostic FedAvg提供了收敛保证，且无需了解参与分布。此外，实验证明agnostic FedAvg的性能优于其他加权聚合的FedAvg变体。

Abstract: Federated learning (FL) enables decentralized model training without
centralizing raw data. However, practical FL deployments often face a key
realistic challenge: Clients participate intermittently in server aggregation
and with unknown, possibly biased participation probabilities. Most existing
convergence results either assume full-device participation, or rely on
knowledge of (in fact uniform) client availability distributions -- assumptions
that rarely hold in practice. In this work, we characterize the optimization
problem that consistently adheres to the stochastic dynamics of the well-known
\emph{agnostic Federated Averaging (FedAvg)} algorithm under random (and
variably-sized) client availability, and rigorously establish its convergence
for convex, possibly nonsmooth losses, achieving a standard rate of order
$\mathcal{O}(1/\sqrt{T})$, where $T$ denotes the aggregation horizon. Our
analysis provides the first convergence guarantees for agnostic FedAvg under
general, non-uniform, stochastic client participation, without knowledge of the
participation distribution. We also empirically demonstrate that agnostic
FedAvg in fact outperforms common (and suboptimal) weighted aggregation FedAvg
variants, even with server-side knowledge of participation weights.

</details>


### [333] [Underrepresentation, Label Bias, and Proxies: Towards Data Bias Profiles for the EU AI Act and Beyond](https://arxiv.org/abs/2507.08866)
*Marina Ceccon,Giandomenico Cornacchia,Davide Dalle Pezze,Alessandro Fabris,Gian Antonio Susto*

Main category: cs.LG

TL;DR: 数据偏见是算法歧视的关键驱动因素，本文研究了三种数据偏见及其对算法歧视的影响，提出了数据偏见分析（DBP）的概念，并证明了其在预测歧视风险和公平性增强干预方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 数据中的不良偏见是算法歧视的关键驱动因素，尽管其重要性已被广泛认识，但数据偏见本身的研究仍然不足，阻碍了计算最佳实践的发展。

Method: 本文提出了三种常见的数据偏见，并研究了它们在不同数据集、模型和公平性度量下的个体及联合效应，开发了用于检测特定类型偏见以及组合这些偏见的机制，形成了数据偏见分析（DBP）。

Result: 研究发现，弱势群体在训练数据中的代表性不足对歧视的促进作用不如传统观点所认为的那样关键，而代理偏见和标签偏见的组合可能更为关键。数据偏见分析（DBP）在预测歧视风险和公平性增强干预方面显示出有效性。

Conclusion: 本文通过数据偏见分析弥合了算法公平性研究与反歧视政策之间的差距，提出了数据偏见分析（DBP）的概念，并证明了其在预测歧视风险和公平性增强干预方面的有效性。

Abstract: Undesirable biases encoded in the data are key drivers of algorithmic
discrimination. Their importance is widely recognized in the algorithmic
fairness literature, as well as legislation and standards on
anti-discrimination in AI. Despite this recognition, data biases remain
understudied, hindering the development of computational best practices for
their detection and mitigation. In this work, we present three common data
biases and study their individual and joint effect on algorithmic
discrimination across a variety of datasets, models, and fairness measures. We
find that underrepresentation of vulnerable populations in training sets is
less conducive to discrimination than conventionally affirmed, while
combinations of proxies and label bias can be far more critical. Consequently,
we develop dedicated mechanisms to detect specific types of bias, and combine
them into a preliminary construct we refer to as the Data Bias Profile (DBP).
This initial formulation serves as a proof of concept for how different bias
signals can be systematically documented. Through a case study with popular
fairness datasets, we demonstrate the effectiveness of the DBP in predicting
the risk of discriminatory outcomes and the utility of fairness-enhancing
interventions. Overall, this article bridges algorithmic fairness research and
anti-discrimination policy through a data-centric lens.

</details>


### [334] [Next-Generation Travel Demand Modeling with a Generative Framework for Household Activity Coordination](https://arxiv.org/abs/2507.08871)
*Xishun Liao,Haoxuan Ma,Yifan Liu,Yuxiang Wei,Brian Yueshuai He,Chris Stanford,Jiaqi Ma*

Main category: cs.LG

TL;DR: 提出了一种新的、基于学习的出行需求建模框架，它比传统模型更便宜、更具可扩展性，并且在复制真实出行模式方面同样有效。


<details>
  <summary>Details</summary>
Motivation: 传统的基于活动模型（ABM）虽然有行为理论基础，但通常依赖于简化的规则和假设，开发成本高昂，并且难以跨区域适应。因此，有必要开发一种更高效、更灵活的出行需求建模方法。

Method: 该研究提出了一个基于学习的出行需求建模框架，该框架能够根据家庭的社会人口统计学特征，综合生成家庭协调的日常活动模式。该框架整合了人口综合、活动协调生成、位置分配和大规模微观交通模拟，构成了一个统一的系统，具有完全生成、数据驱动、可扩展和可迁移的特点。

Result: 在洛杉矶进行的全面评估显示，该模型能够精确复制现实世界的出行模式。与SCAG ABM基准相比，生成的出行目的（OD）矩阵具有0.97的余弦相似度，并且网络中的每日车辆里程（VMT）的Jensen-Shannon散度（JSD）为0.006，平均绝对百分比误差（MAPE）为9.8%。与Caltrans PeMS的真实世界观测数据相比，在走廊级别的交通速度和流量评估中，JSD为0.001，MAPE为6.11%。

Conclusion: 该模型在洛杉矶进行了全面实施，与传统的基于活动模型（ABM）相比，大大降低了成本并提高了可扩展性，同时紧密复制了现实世界的出行模式。

Abstract: Travel demand models are critical tools for planning, policy, and mobility
system design. Traditional activity-based models (ABMs), although grounded in
behavioral theories, often rely on simplified rules and assumptions, and are
costly to develop and difficult to adapt across different regions. This paper
presents a learning-based travel demand modeling framework that synthesizes
household-coordinated daily activity patterns based on a household's
socio-demographic profiles. The whole framework integrates population
synthesis, coordinated activity generation, location assignment, and
large-scale microscopic traffic simulation into a unified system. It is fully
generative, data-driven, scalable, and transferable to other regions. A
full-pipeline implementation is conducted in Los Angeles with a 10 million
population. Comprehensive validation shows that the model closely replicates
real-world mobility patterns and matches the performance of legacy ABMs with
significantly reduced modeling cost and greater scalability. With respect to
the SCAG ABM benchmark, the origin-destination matrix achieves a cosine
similarity of 0.97, and the daily vehicle miles traveled (VMT) in the network
yields a 0.006 Jensen-Shannon Divergence (JSD) and a 9.8% mean absolute
percentage error (MAPE). When compared to real-world observations from Caltrans
PeMS, the evaluation on corridor-level traffic speed and volume reaches a 0.001
JSD and a 6.11% MAPE.

</details>


### [335] [Contrastive Language-Image Pre-Training Model based Semantic Communication Performance Optimization](https://arxiv.org/abs/2507.08873)
*Shaoran Yang,Dongyu Wei,Hanzhi Yu,Zhaohui Yang,Yuchen Liu,Mingzhe Chen*

Main category: cs.LG

TL;DR: 提出一种基于CLIP的语义通信框架，通过PPO强化学习优化CLIP模型和频谱分配，以提升在无线噪声环境下的通信性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统基于神经网络的语义通信方法需要联合训练以及在噪声无线网络中性能下降的问题，提出了一种无需训练即可提取数据语义的CLIP模型，并研究了其在无线网络中的部署优化问题。

Method: 本文提出了一种新颖的基于CLIP的语义通信框架，利用CLIP模型进行语义信息提取和传输。通过近端策略优化（PPO）强化学习算法，联合优化CLIP模型架构和频谱资源分配，以应对无线噪声、延迟和能量消耗等挑战。

Result: 仿真结果表明，与软Actor-Critic算法相比，该方法将收敛速度提高了40%，累积奖励提高了4倍。

Conclusion: 所提出的基于CLIP的语义通信框架通过联合优化CLIP模型架构和频谱资源分配，有效提高了通信性能，并能在有线和无线场景下进行部署。

Abstract: In this paper, a novel contrastive language-image pre-training (CLIP) model
based semantic communication framework is designed. Compared to standard neural
network (e.g.,convolutional neural network) based semantic encoders and
decoders that require joint training over a common dataset, our CLIP model
based method does not require any training procedures thus enabling a
transmitter to extract data meanings of the original data without neural
network model training, and the receiver to train a neural network for
follow-up task implementation without the communications with the transmitter.
Next, we investigate the deployment of the CLIP model based semantic framework
over a noisy wireless network. Since the semantic information generated by the
CLIP model is susceptible to wireless noise and the spectrum used for semantic
information transmission is limited, it is necessary to jointly optimize CLIP
model architecture and spectrum resource block (RB) allocation to maximize
semantic communication performance while considering wireless noise, the delay
and energy used for semantic communication. To achieve this goal, we use a
proximal policy optimization (PPO) based reinforcement learning (RL) algorithm
to learn how wireless noise affect the semantic communication performance thus
finding optimal CLIP model and RB for each user. Simulation results show that
our proposed method improves the convergence rate by up to 40%, and the
accumulated reward by 4x compared to soft actor-critic.

</details>


### [336] [An Automated Classifier of Harmful Brain Activities for Clinical Usage Based on a Vision-Inspired Pre-trained Framework](https://arxiv.org/abs/2507.08874)
*Yulin Sun,Xiaopeng Si,Runnan He,Xiao Hu,Peter Smielewski,Wenlong Wang,Xiaoguang Tong,Wei Yue,Meijun Pang,Kuo Zhang,Xizi Song,Dong Ming,Xiuyun Liu*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Timely identification of harmful brain activities via electroencephalography
(EEG) is critical for brain disease diagnosis and treatment, which remains
limited application due to inter-rater variability, resource constraints, and
poor generalizability of existing artificial intelligence (AI) models. In this
study, a convolutional neural network model, VIPEEGNet, was developed and
validated using EEGs recorded from Massachusetts General Hospital/Harvard
Medical School. The VIPEEGNet was developed and validated using two independent
datasets, collected between 2006 and 2020. The development cohort included EEG
recordings from 1950 patients, with 106,800 EEG segments annotated by at least
one experts (ranging from 1 to 28). The online testing cohort consisted of EEG
segments from a subset of an additional 1,532 patients, each annotated by at
least 10 experts. For the development cohort (n=1950), the VIPEEGNet achieved
high accuracy, with an AUROC for binary classification of seizure, LPD, GPD,
LRDA, GRDA, and "other" categories at 0.972 (95% CI, 0.957-0.988), 0.962 (95%
CI, 0.954-0.970), 0.972 (95% CI, 0.960-0.984), 0.938 (95% CI, 0.917-0.959),
0.949 (95% CI, 0.941-0.957), and 0.930 (95% CI, 0.926-0.935). For multi
classification, the sensitivity of VIPEEGNET for the six categories ranges from
36.8% to 88.2% and the precision ranges from 55.6% to 80.4%, and performance
similar to human experts. Notably, the external validation showed
Kullback-Leibler Divergence (KLD)of 0.223 and 0.273, ranking top 2 among the
existing 2,767 competing algorithms, while we only used 2.8% of the parameters
of the first-ranked algorithm.

</details>


### [337] [ODIA: Oriented Distillation for Inline Acceleration of LLM-based Function Calling](https://arxiv.org/abs/2507.08877)
*Hanlong Zhang,Jingsheng Yang,Hao Li,Yuhao He,Franck Gong*

Main category: cs.LG

TL;DR: ODIA通过知识蒸馏加速LLM函数调用，降低延迟78%，适用于生产环境。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）驱动的函数调用技术虽然关键，但其高延迟严重影响用户体验。

Method: 提出了一种名为“面向蒸馏的内联加速”（ODIA）的新方法，利用在线用户交互数据来识别“简单查询”，并将知识从大型模型蒸馏到小型模型。

Result: ODIA方法成功地将响应延迟降低了45%（预期）和78%（中位数），并且在实际音乐应用部署中，小型模型能够处理60%的流量，准确性损失可忽略。该方法能够通过自动数据收集和模型更新进行持续改进，适用于生产环境。

Conclusion: 该方法通过在线用户交互数据和知识蒸馏技术，实现了函数调用能力的加速，将响应延迟降低了45%（预期）和78%（中位数），同时保持了准确性。

Abstract: Function Calling is a crucial technique that enables Large Language Models
(LLMs) to interact with external systems through APIs. However, the high
latency associated with LLM-based Function Calling significantly impacts user
experience. This paper presents a novel approach called Oriented Distillation
for Inline Acceleration (ODIA) that leverages online user interaction data to
accelerate Function Calling. By automatically identifying "simple queries" from
production traffic and distilling knowledge from larger models to smaller ones,
our method reduces response latency by 45% (expected) and 78% (median) while
maintaining accuracy. We demonstrate the effectiveness of our approach through
real-world deployment in a music application, where the smaller model
successfully handles 60% of traffic with negligible accuracy loss. Our method
requires minimal human intervention and continuously improves through automated
data collection and model updating, making it a practical solution for
production environments.

</details>


### [338] [Last Layer Hamiltonian Monte Carlo](https://arxiv.org/abs/2507.08905)
*Koen Vellenga,H. Joe Steinhauer,Göran Falkman,Jonas Andersson,Anders Sjögren*

Main category: cs.LG

TL;DR: 该研究提出了一种名为LL-HMC的概率模型，用于提高深度神经网络在视频分析任务中的不确定性估计能力。与现有方法相比，LL-HMC在分类和异常检测方面表现出竞争力，并且在计算资源有限的情况下具有优势。


<details>
  <summary>Details</summary>
Motivation: 由于计算成本的限制，传统的Hamiltonian Monte Carlo (HMC)采样难以应用于大规模数据集和大型深度神经网络（DNN）。本研究旨在通过提出最后层HMC（LL-HMC）方法，将HMC采样限制在DNN的最后层，从而降低计算需求，使其能够应用于数据密集且计算资源有限的场景中。

Method: 该研究提出并评估了最后层HMC（LL-HMC）采样作为深度神经网络（DNN）的概率最后层方法，并将其与五种最后层概率深度学习（LL-PDL）方法在三个真实世界视频数据集上进行了比较，评估了其在in-distribution分类、校准和OOD检测方面的表现。为了避免对单一随机种子产生依赖，研究进行了五次不同随机种子的网格搜索。

Result: LL-HMC在in-distribution分类和OOD检测方面取得了与现有方法相当的性能。增加样本数量能够提升OOD检测能力，但对分类性能没有显著影响。使用多条链或多个起始点进行采样并未带来一致的性能提升。

Conclusion: LL-HMC在视频驱动动作和意图识别中实现了具有竞争力的in-distribution分类和OOD检测性能。虽然增加样本数量可以改善OOD检测，但并未提升分类性能，且多链或多起始点未带来持续的改进。

Abstract: We explore the use of Hamiltonian Monte Carlo (HMC) sampling as a
probabilistic last layer approach for deep neural networks (DNNs). While HMC is
widely regarded as a gold standard for uncertainty estimation, the
computational demands limit its application to large-scale datasets and large
DNN architectures. Although the predictions from the sampled DNN parameters can
be parallelized, the computational cost still scales linearly with the number
of samples (similar to an ensemble). Last layer HMC (LL--HMC) reduces the
required computations by restricting the HMC sampling to the final layer of a
DNN, making it applicable to more data-intensive scenarios with limited
computational resources. In this paper, we compare LL-HMC against five last
layer probabilistic deep learning (LL-PDL) methods across three real-world
video datasets for driver action and intention. We evaluate the in-distribution
classification performance, calibration, and out-of-distribution (OOD)
detection. Due to the stochastic nature of the probabilistic evaluations, we
performed five grid searches for different random seeds to avoid being reliant
on a single initialization for the hyperparameter configurations. The results
show that LL--HMC achieves competitive in-distribution classification and OOD
detection performance. Additional sampled last layer parameters do not improve
the classification performance, but can improve the OOD detection. Multiple
chains or starting positions did not yield consistent improvements.

</details>


### [339] [Continuous-Time Signal Decomposition: An Implicit Neural Generalization of PCA and ICA](https://arxiv.org/abs/2507.09091)
*Shayan K. Azmoodeh,Krishna Subramani,Paris Smaragdis*

Main category: cs.LG

TL;DR: Neural network framework unifies and extends PCA/ICA for continuous signals like point clouds and irregular data.


<details>
  <summary>Details</summary>
Motivation: To generalize low-rank decomposition problems like PCA and ICA to continuous-time signals and address limitations of standard techniques with point clouds and irregularly sampled data.

Method: We model signals as continuous-time stochastic processes and use a model-agnostic implicit neural representation framework. A contrast function in the network loss enforces desired statistical properties (decorrelation, independence) of the source signals.

Result: The framework provides numerical approximations for continuous-domain PCA and ICA, allowing these decompositions to be applied to previously incompatible data types.

Conclusion: We present a unified framework for generalized low-rank decomposition of continuous-time vector-valued signals, extending PCA and ICA to continuous domains and enabling their application to point clouds and irregularly sampled data.

Abstract: We generalize the low-rank decomposition problem, such as principal and
independent component analysis (PCA, ICA) for continuous-time vector-valued
signals and provide a model-agnostic implicit neural signal representation
framework to learn numerical approximations to solve the problem. Modeling
signals as continuous-time stochastic processes, we unify the approaches to
both the PCA and ICA problems in the continuous setting through a contrast
function term in the network loss, enforcing the desired statistical properties
of the source signals (decorrelation, independence) learned in the
decomposition. This extension to a continuous domain allows the application of
such decompositions to point clouds and irregularly sampled signals where
standard techniques are not applicable.

</details>


### [340] [Fair-FLIP: Fair Deepfake Detection with Fairness-Oriented Final Layer Input Prioritising](https://arxiv.org/abs/2507.08912)
*Tomasz Szandala,Fatima Ezzeddine,Natalia Rusin,Silvia Giordano,Omran Ayoub*

Main category: cs.LG

TL;DR: 提出 Fair-FLIP 方法来解决深度伪造检测中的公平性问题，通过重新加权模型输入来减少偏见，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 深度伪造（deepfakes）的恶意使用对公众信任和言论构成了严重威胁，现有的深度伪造检测方法在跨民族和性别等人口统计属性上存在偏见，因此需要解决公平深度伪造检测的挑战。

Method: 提出了一种名为 Fair-FLIP 的新颖的后处理方法，该方法通过重新加权训练模型的最终层输入来减少子组差异，优先考虑变异性低的子组，同时降级变异性高的子组。

Result: 与基线和最先进的方法相比，Fair-FLIP 可将公平性指标最多提高 30%，同时将准确性降低仅 0.25%。

Conclusion: Fair-FLIP 是一种新颖的后处理方法，通过重新加权训练模型的最终层输入来减少子组差异，同时保持鲁棒的检测能力，在公平性指标上最多可提高 30%，同时仅略微降低 0.25% 的准确性。

Abstract: Artificial Intelligence-generated content has become increasingly popular,
yet its malicious use, particularly the deepfakes, poses a serious threat to
public trust and discourse. While deepfake detection methods achieve high
predictive performance, they often exhibit biases across demographic attributes
such as ethnicity and gender. In this work, we tackle the challenge of fair
deepfake detection, aiming to mitigate these biases while maintaining robust
detection capabilities. To this end, we propose a novel post-processing
approach, referred to as Fairness-Oriented Final Layer Input Prioritising
(Fair-FLIP), that reweights a trained model's final-layer inputs to reduce
subgroup disparities, prioritising those with low variability while demoting
highly variable ones. Experimental results comparing Fair-FLIP to both the
baseline (without fairness-oriented de-biasing) and state-of-the-art approaches
show that Fair-FLIP can enhance fairness metrics by up to 30% while maintaining
baseline accuracy, with only a negligible reduction of 0.25%.
  Code is available on Github:
https://github.com/szandala/fair-deepfake-detection-toolbox

</details>


### [341] [Behavioral Exploration: Learning to Explore via In-Context Adaptation](https://arxiv.org/abs/2507.09041)
*Andrew Wagenmaker,Zhiyuan Zhou,Sergey Levine*

Main category: cs.LG

TL;DR: 本研究提出了一种名为“行为探索”的新方法，通过训练长上下文生成模型来模仿和预测专家行为，从而使自主代理能够像人类一样快速地进行在线探索和行为适应。该方法在模拟和现实世界的机器人任务中均取得了成功。


<details>
  <summary>Details</summary>
Motivation: 旨在解决机器人和机器学习中的一个经典挑战：开发能够快速探索环境并在在线环境中适应其行为的自主代理。与依赖随机探索和缓慢梯度更新的现有方法不同，该方法旨在使自主代理具备与人类相当的在线探索和适应能力。

Method: 提出了一种名为“行为探索”的方法，通过训练一个长上下文生成模型来预测专家的行为。该模型根据过去的观察和专家行为相对于该上下文的“探索性”程度进行条件预测。模型通过将过去的交互历史输入其上下文，不仅可以模仿专家的行为，还可以选择与先前不同的专家行为，从而实现快速的在线适应和有针对性的“专家式”探索。

Result: 所提出的方法能够实现快速的在线适应和有针对性的“专家式”探索，并在模拟和现实世界的机器人任务中得到了验证。

Conclusion: 该方法在模拟的运动和操纵设置以及真实的机器人操纵任务中证明了其有效性，说明了其学习适应性、探索性行为的能力。

Abstract: Developing autonomous agents that quickly explore an environment and adapt
their behavior online is a canonical challenge in robotics and machine
learning. While humans are able to achieve such fast online exploration and
adaptation, often acquiring new information and skills in only a handful of
interactions, existing algorithmic approaches tend to rely on random
exploration and slow, gradient-based behavior updates. How can we endow
autonomous agents with such capabilities on par with humans? Taking inspiration
from recent progress on both in-context learning and large-scale behavioral
cloning, in this work we propose behavioral exploration: training agents to
internalize what it means to explore and adapt in-context over the space of
``expert'' behaviors. To achieve this, given access to a dataset of expert
demonstrations, we train a long-context generative model to predict expert
actions conditioned on a context of past observations and a measure of how
``exploratory'' the expert's behaviors are relative to this context. This
enables the model to not only mimic the behavior of an expert, but also, by
feeding its past history of interactions into its context, to select different
expert behaviors than what have been previously selected, thereby allowing for
fast online adaptation and targeted, ``expert-like'' exploration. We
demonstrate the effectiveness of our method in both simulated locomotion and
manipulation settings, as well as on real-world robotic manipulation tasks,
illustrating its ability to learn adaptive, exploratory behavior.

</details>


### [342] [TolerantECG: A Foundation Model for Imperfect Electrocardiogram](https://arxiv.org/abs/2507.09887)
*Huynh Nguyen Dang,Thang Pham,Ngan Le,Van Nguyen*

Main category: cs.LG

TL;DR: TolerantECG 是一个对噪声和导联缺失具有鲁棒性的 ECG 基础模型，可用于心脏病诊断。


<details>
  <summary>Details</summary>
Motivation: 为了解决 ECG 信号在噪声干扰或部分导联缺失时可能导致诊断错误或不确定性的问题，提出 TolerantECG 模型。

Method: TolerantECG 结合了对比学习和自监督学习框架，同时学习 ECG 信号表示、文本报告描述以及损坏或缺失导联的信号。

Result: 在 PTB-XL 数据集上，TolerantECG 在各种 ECG 信号条件下和类别级别上始终排名第一或第二。在 MIT-BIH 心律失常数据库上，TolerantECG 达到了最高的性能。

Conclusion: TolerantECG 模型在 PTB-XL 和 MIT-BIH 心律失常数据库上均表现出色，能够应对噪声和导联缺失的挑战，为 ECG 信号分析提供了鲁棒的解决方案。

Abstract: The electrocardiogram (ECG) is an essential and effective tool for diagnosing
heart diseases. However, its effectiveness can be compromised by noise or
unavailability of one or more leads of the standard 12-lead recordings,
resulting in diagnostic errors or uncertainty. To address these challenges, we
propose TolerantECG, a foundation model for ECG signals that is robust to noise
and capable of functioning with arbitrary subsets of the standard 12-lead ECG.
TolerantECG training combines contrastive and self-supervised learning
frameworks to jointly learn ECG signal representations alongside their
corresponding knowledge-retrieval-based text report descriptions and corrupted
or lead-missing signals. Comprehensive benchmarking results demonstrate that
TolerantECG consistently ranks as the best or second-best performer across
various ECG signal conditions and class levels in the PTB-XL dataset, and
achieves the highest performance on the MIT-BIH Arrhythmia Database.

</details>


### [343] [Revisiting Convergence: Shuffling Complexity Beyond Lipschitz Smoothness](https://arxiv.org/abs/2507.08913)
*Qi He,Peiran Yu,Ziyi Chen,Heng Huang*

Main category: cs.LG

TL;DR: 在不需要Lipschitz平滑条件的情况下，我们提出了一种新的步长策略，改进了shuffling型梯度算法。该算法在非凸、强凸和非强凸问题上都达到了最佳收敛速度，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有shuffling型梯度方法的收敛性保证大多依赖于Lipschitz平滑条件，而该条件在常见的机器学习模型中常常不满足。为了解决这一问题，本研究旨在去除Lipschitz平滑的假设，研究在更弱的条件下shuffling型梯度方法的收敛性。

Method: 提出了一种新的步长策略，并在此策略下分析了shuffling型梯度算法在不同凸性（非凸、强凸、非强凸）和不同重排方式（随机重排、任意重排）下的收敛速度，理论上证明了在一般有界方差条件下算法的收敛性。

Result: 在一般有界方差条件下，证明了所提出的shuffling型梯度算法在非凸、强凸和非强凸情况下的收敛速度，并且与现有最佳收敛速度相当。数值实验结果验证了算法的实践性能。

Conclusion: 该研究提出的带步长策略的shuffling型梯度算法在更宽松的假设下（无需Lipschitz平滑条件）收敛，并且达到了已知的最佳收敛速度，拓宽了其适用范围。研究证明了该算法在非凸、强凸和非强凸情况下的收敛速度，并针对随机重排和任意重排两种情况，在一般有界方差条件下进行了理论分析。数值实验也验证了该算法的有效性。

Abstract: Shuffling-type gradient methods are favored in practice for their simplicity
and rapid empirical performance. Despite extensive development of convergence
guarantees under various assumptions in recent years, most require the
Lipschitz smoothness condition, which is often not met in common machine
learning models. We highlight this issue with specific counterexamples. To
address this gap, we revisit the convergence rates of shuffling-type gradient
methods without assuming Lipschitz smoothness. Using our stepsize strategy, the
shuffling-type gradient algorithm not only converges under weaker assumptions
but also match the current best-known convergence rates, thereby broadening its
applicability. We prove the convergence rates for nonconvex, strongly convex,
and non-strongly convex cases, each under both random reshuffling and arbitrary
shuffling schemes, under a general bounded variance condition. Numerical
experiments further validate the performance of our shuffling-type gradient
algorithm, underscoring its practical efficacy.

</details>


### [344] [Imitation Learning in Continuous Action Spaces: Mitigating Compounding Error without Interaction](https://arxiv.org/abs/2507.09061)
*Thomas T. Zhang,Daniel Pfrommer,Nikolai Matni,Max Simchowitz*

Main category: cs.LG

TL;DR: 本研究提出“动作分块”和“噪声注入”两种方法，有效解决了机器人学习中模仿学习的复合误差问题，提高了学习的稳定性和准确性。


<details>
  <summary>Details</summary>
Motivation: 模仿学习在物理设置（如自动驾驶和机器人学习）中比在离散设置（如语言建模）中更具挑战性，主要原因是复合误差问题。现有方法在稳定执行方面需要复杂的设置，并且即使在有利条件下，仅从专家轨迹学习也会导致指数级复合误差。因此，需要更高级的策略参数化或数据增强方法。

Method: 本研究提出两种最小干预措施来解决连续状态-动作模仿学习中的复合误差问题：“动作分块”（预测并分段执行动作）和“噪声注入”（在专家演示中加入噪声）。研究结合了控制理论和强化学习的工具，对这些干预措施进行了理论分析，并验证了其有效性。

Result: 本研究表明，“动作分块”在系统是开环稳定时可减少复合误差；而“噪声注入”则适用于系统不稳定的情况。这些干预措施与机器人学习中的常用方法一致，但本研究揭示了它们在解决复合误差方面的独特优势，这些优势在单独考虑控制理论或强化学习时并不明显。

Conclusion: 本研究提出的最小干预措施可有效缓解连续状态-动作模仿学习中的复合误差问题。通过对“动作分块”和“噪声注入”这两种干预方式的理论分析和实验验证，证明了其在不同系统稳定性下的有效性，并为机器人学习领域提供了新的视角和工具。

Abstract: We study the problem of imitating an expert demonstrator in a continuous
state-and-action dynamical system. While imitation learning in discrete
settings such as autoregressive language modeling has seen immense success and
popularity in recent years, imitation in physical settings such as autonomous
driving and robot learning has proven comparably more complex due to the
compounding errors problem, often requiring elaborate set-ups to perform
stably. Recent work has demonstrated that even in benign settings, exponential
compounding errors are unavoidable when learning solely from expert-controlled
trajectories, suggesting the need for more advanced policy parameterizations or
data augmentation. To this end, we present minimal interventions that provably
mitigate compounding errors in continuous state-and-action imitation learning.
When the system is open-loop stable, we prescribe "action chunking," i.e.,
predicting and playing sequences of actions in open-loop; when the system is
possibly unstable, we prescribe "noise injection," i.e., adding noise during
expert demonstrations. These interventions align with popular choices in modern
robot learning, though the benefits we derive are distinct from the effects
they were designed to target. Our results draw insights and tools from both
control theory and reinforcement learning; however, our analysis reveals novel
considerations that do not naturally arise when either literature is considered
in isolation.

</details>


### [345] [Beyond Scores: Proximal Diffusion Models](https://arxiv.org/abs/2507.08956)
*Zhenghan Fang,Mateo Díaz,Sam Buchanan,Jeremias Sulam*

Main category: cs.LG

TL;DR: 本文提出了一种名为Proximal Diffusion Models (ProxDM) 的新方法，它使用近邻映射替代得分来改进扩散模型，在理论和实践上都取得了更好的效果。


<details>
  <summary>Details</summary>
Motivation: 为了克服现有扩散模型在生成高维数据时的局限性，本文提出了一种新的方法，该方法在理论和实践上均有改进。

Method: 本文提出了一种基于SDE反向离散化的新方法，利用近邻映射代替得分来生成数据。

Result: ProxDM相比于传统的得分匹配方法，在采样步数方面具有显著的收敛优势，生成ε精度分布所需的步数理论上为O(d/√ε)。

Conclusion: ProxDM通过学习对数密度的近邻算子，并在采样时使用近邻映射替代得分，实现了理论和实践上的改进。

Abstract: Diffusion models have quickly become some of the most popular and powerful
generative models for high-dimensional data. The key insight that enabled their
development was the realization that access to the score -- the gradient of the
log-density at different noise levels -- allows for sampling from data
distributions by solving a reverse-time stochastic differential equation (SDE)
via forward discretization, and that popular denoisers allow for unbiased
estimators of this score. In this paper, we demonstrate that an alternative,
backward discretization of these SDEs, using proximal maps in place of the
score, leads to theoretical and practical benefits. We leverage recent results
in proximal matching to learn proximal operators of the log-density and, with
them, develop Proximal Diffusion Models (ProxDM). Theoretically, we prove that
$\widetilde{O}(d/\sqrt{\varepsilon})$ steps suffice for the resulting
discretization to generate an $\varepsilon$-accurate distribution w.r.t. the KL
divergence. Empirically, we show that two variants of ProxDM achieve
significantly faster convergence within just a few sampling steps compared to
conventional score-matching methods.

</details>


### [346] [Graph Neural Network Enhanced Sequential Recommendation Method for Cross-Platform Ad Campaign](https://arxiv.org/abs/2507.08959)
*Xiang Li,Xinyu Wang,Yifan Lin*

Main category: cs.LG

TL;DR: 基于GNN的跨平台广告推荐方法，通过多维度建模捕捉用户兴趣迁移，并在Platform B上取得优异的AUC值（0.937），同时通过超参数调整提高了在异构数据上的适应性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 为了提高跨平台广告推荐的准确性。

Method: 提出了一种基于图神经网络（GNN）的广告推荐方法，通过多维度建模来捕捉用户兴趣的演变、广告内容的语义偏好以及平台特征对用户兴趣迁移路径的影响。

Result: 实验结果表明，所提出的GNN模型在Platform B上达到了最佳的0.937 AUC值。Platform A和Platform C由于广告标签分布不均，精度和召回率略有下降。通过调整学习率、批大小和嵌入维度等超参数，模型的适应性和鲁棒性得到进一步提升。

Conclusion: 通过多维度建模和超参数调整，基于图神经网络（GNN）的跨平台广告推荐方法在用户兴趣迁移方面表现出良好的适应性和鲁棒性，尤其在Platform B上达到了0.937的AUC值，验证了其有效性。

Abstract: In order to improve the accuracy of cross-platform advertisement
recommendation, a graph neural network (GNN)- based advertisement
recommendation method is analyzed. Through multi-dimensional modeling, user
behavior data (e.g., click frequency, active duration) reveal temporal patterns
of interest evolution, ad content (e.g., type, tag, duration) influences
semantic preferences, and platform features (e.g., device type, usage context)
shape the environment where interest transitions occur. These factors jointly
enable the GNN to capture the latent pathways of user interest migration across
platforms. The experimental results are based on the datasets of three
platforms, and Platform B reaches 0.937 in AUC value, which is the best
performance. Platform A and Platform C showed a slight decrease in precision
and recall with uneven distribution of ad labels. By adjusting the
hyperparameters such as learning rate, batch size and embedding dimension, the
adaptability and robustness of the model in heterogeneous data are further
improved.

</details>


### [347] [Theory-Informed Improvements to Classifier-Free Guidance for Discrete Diffusion Models](https://arxiv.org/abs/2507.08965)
*Kevin Rojas,Ye He,Chieh-Hsin Lai,Yuta Takida,Yuki Mitsufuji,Molei Tao*

Main category: cs.LG

TL;DR: 本文分析了离散扩散模型中的分类器免费指导（CFG）及其指导计划。研究发现，早期高指导会损害生成质量，而后期指导效果更好。目前 CFG 的实现存在缺陷，会导致不平衡的转换。为解决此问题，提出了一种简单的新型 CFG 机制，可提高样本质量。


<details>
  <summary>Details</summary>
Motivation: 在连续扩散模型中，分类器免费指导（CFG）是一种广泛使用的条件生成和提高样本质量的技术，近期研究已将其扩展到离散扩散。然而，对于 CFG 在离散扩散（尤其是掩蔽离散扩散）中的作用，尤其是在指导计划方面，仍然缺乏理论分析。

Method: 本文理论分析了掩蔽离散扩散中 CFG 的指导计划的作用。我们提出了一种新颖的分类器免费指导机制，通过平滑数据分布和初始（掩蔽/统一）分布之间的传输，可以简单地通过一行代码更改实现。

Result: 研究结果表明，高指导早期的指导会损害生成质量，而后期指导的影响更大。目前的 CFG 实现存在缺陷，会导致不平衡的转换，从而降低样本质量。所提出的新指导机制通过平滑传输来提高样本质量，并通过实验证明了其有效性。

Conclusion: 该分析表明，在采样早期（输入被大量掩蔽时）高指导会损害生成质量，而在采样后期指导则影响更大。这些发现为近期关于指导计划的研究提供了理论解释。该分析还揭示了当前 CFG 实现的一个缺陷，即当前实现可能无意中导致过渡不平衡，例如在生成早期过快地取消掩蔽，从而降低了生成样本的质量。为了解决这个问题，我们从分析中获得启发，提出了一种新颖的、可用于任何离散扩散的分类器免费指导机制。通过在 ImageNet（掩蔽离散扩散）和 QM9（统一离散扩散）上的实验，证明了我们方法的有效性。

Abstract: Classifier-Free Guidance (CFG) is a widely used technique for conditional
generation and improving sample quality in continuous diffusion models, and
recent works have extended it to discrete diffusion. This paper theoretically
analyzes CFG in the context of masked discrete diffusion, focusing on the role
of guidance schedules. Our analysis shows that high guidance early in sampling
(when inputs are heavily masked) harms generation quality, while late-stage
guidance has a larger effect. These findings provide a theoretical explanation
for empirical observations in recent studies on guidance schedules. The
analysis also reveals an imperfection of the current CFG implementations. These
implementations can unintentionally cause imbalanced transitions, such as
unmasking too rapidly during the early stages of generation, which degrades the
quality of the resulting samples. To address this, we draw insight from the
analysis and propose a novel classifier-free guidance mechanism empirically
applicable to any discrete diffusion. Intuitively, our method smoothens the
transport between the data distribution and the initial (masked/uniform)
distribution, which results in improved sample quality. Remarkably, our method
is achievable via a simple one-line code change. The efficacy of our method is
empirically demonstrated with experiments on ImageNet (masked discrete
diffusion) and QM9 (uniform discrete diffusion).

</details>


### [348] [ToxBench: A Binding Affinity Prediction Benchmark with AB-FEP-Calculated Labels for Human Estrogen Receptor Alpha](https://arxiv.org/abs/2507.08966)
*Meng Liu,Karl Leswing,Simon K. S. Chu,Farhad Ramezanghorbani,Griffin Young,Gabriel Marques,Prerna Das,Anjali Panikar,Esther Jamir,Mohammed Sulaiman Shamsudeen,K. Shawn Watts,Ananya Sen,Hari Priya Devannagari,Edward B. Miller,Muyun Lihan,Howook Hwang,Janet Paulsen,Xin Yu,Kyle Gion,Timur Rvachov,Emine Kucukbenli,Saee Gopal Paliwal*

Main category: cs.LG

TL;DR: ToxBench, a large dataset for ML-based protein-ligand binding affinity prediction using AB-FEP, was created for ER$\alpha$. The DualBind model significantly improves prediction accuracy compared to other ML methods, showing ML can approximate costly AB-FEP calculations.


<details>
  <summary>Details</summary>
Motivation: The motivation is to bridge the gap between accurate but computationally expensive physics-based methods (like AB-FEP) and faster but less accurate/data-dependent machine learning (ML) methods for protein-ligand binding affinity prediction, which is crucial for drug discovery and toxicity assessment.

Method: The paper introduces ToxBench, a large-scale dataset of 8,770 Human Estrogen Receptor Alpha (ER$\alpha$)-ligand complexes with binding free energies computed using Absolute Binding Free Energy Perturbation (AB-FEP). A subset of these energies was validated against experimental data with a Root Mean Square Error (RMSE) of 1.75 kcal/mol. The study benchmarks existing ML methods and introduces DualBind, a model using a dual-loss framework to learn the binding energy function.

Result: The benchmark results on the ToxBench dataset demonstrate that the DualBind model outperforms other state-of-the-art ML methods, indicating that ML has the potential to approximate AB-FEP calculations efficiently.

Conclusion: ML models, particularly the proposed DualBind model, show potential in approximating computationally expensive AB-FEP calculations for protein-ligand binding affinity prediction, achieving high accuracy at a reduced computational cost.

Abstract: Protein-ligand binding affinity prediction is essential for drug discovery
and toxicity assessment. While machine learning (ML) promises fast and accurate
predictions, its progress is constrained by the availability of reliable data.
In contrast, physics-based methods such as absolute binding free energy
perturbation (AB-FEP) deliver high accuracy but are computationally prohibitive
for high-throughput applications. To bridge this gap, we introduce ToxBench,
the first large-scale AB-FEP dataset designed for ML development and focused on
a single pharmaceutically critical target, Human Estrogen Receptor Alpha
(ER$\alpha$). ToxBench contains 8,770 ER$\alpha$-ligand complex structures with
binding free energies computed via AB-FEP with a subset validated against
experimental affinities at 1.75 kcal/mol RMSE, along with non-overlapping
ligand splits to assess model generalizability. Using ToxBench, we further
benchmark state-of-the-art ML methods, and notably, our proposed DualBind
model, which employs a dual-loss framework to effectively learn the binding
energy function. The benchmark results demonstrate the superior performance of
DualBind and the potential of ML to approximate AB-FEP at a fraction of the
computational cost.

</details>


### [349] [Simulating Three-dimensional Turbulence with Physics-informed Neural Networks](https://arxiv.org/abs/2507.08972)
*Sifan Wang,Shyam Sankaran,Panos Stinis,Paris Perdikaris*

Main category: cs.LG

TL;DR: PINNs可以通过自适应网络架构、因果训练和高级优化方法来模拟复杂的湍流，无需传统计算网格或训练数据。


<details>
  <summary>Details</summary>
Motivation: 湍流流体模拟计算成本高昂，PINNs提供了一种直接从物理方程而非数据训练的全新方法，有望实现连续、无网格的解。

Method: 该方法结合了自适应网络架构、因果训练和高级优化方法，以克服学习混沌动力学的挑战。

Result: PINNs能够准确重现能量谱、动能、涡量和雷诺应力等关键流动统计数据。

Conclusion: PINNs能够成功模拟二维和三维全湍流，直接学习流体基本方程，无需传统计算网格或训练数据。

Abstract: Turbulent fluid flows are among the most computationally demanding problems
in science, requiring enormous computational resources that become prohibitive
at high flow speeds. Physics-informed neural networks (PINNs) represent a
radically different approach that trains neural networks directly from physical
equations rather than data, offering the potential for continuous, mesh-free
solutions. Here we show that appropriately designed PINNs can successfully
simulate fully turbulent flows in both two and three dimensions, directly
learning solutions to the fundamental fluid equations without traditional
computational grids or training data. Our approach combines several algorithmic
innovations including adaptive network architectures, causal training, and
advanced optimization methods to overcome the inherent challenges of learning
chaotic dynamics. Through rigorous validation on challenging turbulence
problems, we demonstrate that PINNs accurately reproduce key flow statistics
including energy spectra, kinetic energy, enstrophy, and Reynolds stresses. Our
results demonstrate that neural equation solvers can handle complex chaotic
systems, opening new possibilities for continuous turbulence modeling that
transcends traditional computational limitations.

</details>


### [350] [Simulation as Supervision: Mechanistic Pretraining for Scientific Discovery](https://arxiv.org/abs/2507.08977)
*Carson Dudley,Reiden Magdaleno,Christopher Harding,Marisa Eisenberg*

Main category: cs.LG

TL;DR: SGNNs 通过使用力学模拟作为训练数据，结合了科学模型的可解释性和机器学习的灵活性，并在预测和推理任务中取得了优于现有方法的成果，同时还提供了新的可解释性方法。


<details>
  <summary>Details</summary>
Motivation: 科学建模面临核心限制：力学模型可解释但难以处理现实复杂性，而机器学习模型灵活但需要大量标记数据，且无法推断不可观测的量并作为黑箱运作。SGNNs 的提出旨在解决这些问题。

Method: SGNNs 是一个通用框架，它使用力学模拟作为神经网络的训练数据。SGNNs 在跨越不同模型结构、参数范围、随机性和观测偏差的合成数据上进行预训练。作者还提出了一种新的力学可解释性形式：反向模拟归因。

Result: SGNNs 在科学领域和建模任务中取得了最先进的成果：在预测任务上，它们几乎使 COVID-19 预测能力相比 CDC 基线提高了两倍，化学产率预测误差降低了三分之一，并在生态预测中保持了准确性，而特定任务的模型则失败了。在推理任务上，SGNNs 还能准确地对模拟社交网络中信息传播的来源进行分类，并实现了对不可观测目标的监督学习，例如比传统方法更准确地估计 COVID-19 的传播性，即使在疫情早期也是如此。

Conclusion: SGNNs 统一了科学理论和深度学习的灵活性，并开启了一个新的建模范式——将模拟从僵化的、事后工具转变为灵活的监督来源，即使在缺乏真实情况的情况下也能实现稳健、可解释的推理。

Abstract: Scientific modeling faces a core limitation: mechanistic models offer
interpretability but collapse under real-world complexity, while machine
learning models are flexible but require large labeled datasets, cannot infer
unobservable quantities, and operate as black boxes. We introduce
Simulation-Grounded Neural Networks (SGNNs), a general framework that uses
mechanistic simulations as training data for neural networks. SGNNs are
pretrained on synthetic corpora spanning diverse model structures, parameter
regimes, stochasticity, and observational artifacts. We evaluated SGNNs across
scientific disciplines and modeling tasks, and found that SGNNs achieved
state-of-the-art results across settings: for prediction tasks, they nearly
tripled COVID-19 forecasting skill versus CDC baselines, reduced chemical yield
prediction error by one third, and maintained accuracy in ecological
forecasting where task specific models failed. For inference tasks, SGNNs also
accurately classified the source of information spread in simulated social
networks and enabled supervised learning for unobservable targets, such as
estimating COVID-19 transmissibility more accurately than traditional methods
even in early outbreaks. Finally, SGNNs enable back-to-simulation attribution,
a new form of mechanistic interpretability. Given real world input, SGNNs
retrieve simulations based on what the model has learned to see as most
similar, revealing which underlying dynamics the model believes are active.
This provides process-level insight -- what the model thinks is happening --
not just which features mattered. SGNNs unify scientific theory with deep
learning flexibility and unlock a new modeling paradigm -- transforming
simulations from rigid, post hoc tools into flexible sources of supervision,
enabling robust, interpretable inference even when ground truth is missing.

</details>


### [351] [Learning Diffusion Models with Flexible Representation Guidance](https://arxiv.org/abs/2507.08980)
*Chenyu Wang,Cai Zhou,Sharut Gupta,Zongyu Lin,Stefanie Jegelka,Stephen Bates,Tommi Jaakkola*

Main category: cs.LG

TL;DR: 扩散模型通过整合表示引导来提高生成质量和训练速度。本研究提出了一种系统性框架和两种新策略，并在多个任务上取得了显著的加速效果。


<details>
  <summary>Details</summary>
Motivation: 为了提高扩散模型的生成质量，通过对齐扩散模型内部表示和预训练模型来改进其生成效果。

Method: 提出了一种系统性的框架，将表示引导整合到扩散模型中。通过对去噪模型进行分解，并提供相应的训练准则，以确定如何以及何时整合辅助表示。提出了两种新的策略：1. 将样本与源自自身或不同合成模态的目标表示配对，并学习多模态对上的联合模型；2. 设计了一个最优训练课程，平衡表示学习和数据生成。

Result: 在图像、蛋白质序列和分子生成任务上取得了优于现有方法的性能，并加速了训练过程。在ImageNet $256	imes 256$任务上，训练速度比SiT-XL快23.3倍，比REPA快4倍。

Conclusion: 通过实验证明了该框架在图像、蛋白质序列和分子生成任务上均优于现有方法，并加速了训练过程。在ImageNet $256	imes 256$基准测试中，该方法比SiT-XL快23.3倍，比当前最优方法REPA快4倍。

Abstract: Diffusion models can be improved with additional guidance towards more
effective representations of input. Indeed, prior empirical work has already
shown that aligning internal representations of the diffusion model with those
of pre-trained models improves generation quality. In this paper, we present a
systematic framework for incorporating representation guidance into diffusion
models. We provide alternative decompositions of denoising models along with
their associated training criteria, where the decompositions determine when and
how the auxiliary representations are incorporated. Guided by our theoretical
insights, we introduce two new strategies for enhancing representation
alignment in diffusion models. First, we pair examples with target
representations either derived from themselves or arisen from different
synthetic modalities, and subsequently learn a joint model over the multimodal
pairs. Second, we design an optimal training curriculum that balances
representation learning and data generation. Our experiments across image,
protein sequence, and molecule generation tasks demonstrate superior
performance as well as accelerated training. In particular, on the
class-conditional ImageNet $256\times 256$ benchmark, our guidance results in
$23.3$ times faster training than the original SiT-XL as well as four times
speedup over the state-of-the-art method REPA. The code is available at
https://github.com/ChenyuWang-Monica/REED.

</details>


### [352] [Exploiting Leaderboards for Large-Scale Distribution of Malicious Models](https://arxiv.org/abs/2507.08983)
*Anshuman Suri,Harsh Chaudhari,Yuefeng Peng,Ali Naseh,Amir Houmansadr,Alina Oprea*

Main category: cs.LG

TL;DR: 模型排行榜可能被恶意行为者用来大规模分发中毒模型。研究人员提出了TrojanClimb框架，可以在不影响排名的 tetapi 会导致模型中毒的情况下注入恶意行为。该研究强调了加强排行榜评估机制的必要性。


<details>
  <summary>Details</summary>
Motivation: 为了探索模型中毒攻击的机制以及恶意模型如何大规模分发。

Method: 提出了一种名为TrojanClimb的通用框架，该框架允许在保持有竞争力的排行榜性能的同时注入恶意行为。

Result: 在文本嵌入、文本生成、文本到语音和文本到图像四个不同的模态中，成功地展示了在排行榜上获得高排名，同时嵌入任意有害功能（从后门到偏差注入）。

Conclusion: 该研究揭示了模型排行榜作为恶意模型大规模分发的潜在渠道，强调了改进排行榜评估机制以检测和过滤恶意模型以应对机器学习生态系统中的重大漏洞的必要性。

Abstract: While poisoning attacks on machine learning models have been extensively
studied, the mechanisms by which adversaries can distribute poisoned models at
scale remain largely unexplored. In this paper, we shed light on how model
leaderboards -- ranked platforms for model discovery and evaluation -- can
serve as a powerful channel for adversaries for stealthy large-scale
distribution of poisoned models. We present TrojanClimb, a general framework
that enables injection of malicious behaviors while maintaining competitive
leaderboard performance. We demonstrate its effectiveness across four diverse
modalities: text-embedding, text-generation, text-to-speech and text-to-image,
showing that adversaries can successfully achieve high leaderboard rankings
while embedding arbitrary harmful functionalities, from backdoors to bias
injection. Our findings reveal a significant vulnerability in the machine
learning ecosystem, highlighting the urgent need to redesign leaderboard
evaluation mechanisms to detect and filter malicious (e.g., poisoned) models,
while exposing broader security implications for the machine learning community
regarding the risks of adopting models from unverified sources.

</details>


### [353] [Multimodal Cardiovascular Risk Profiling Using Self-Supervised Learning of Polysomnography](https://arxiv.org/abs/2507.09009)
*Zhengxiao He,Huayu Li,Geng Yuan,William D. S. Killgore,Stuart F. Quan,Chen X. Chen,Ao Li*

Main category: cs.LG

TL;DR: 开发了一种利用脑电图、心电图和呼吸信号的自监督深度学习模型，用于预测心血管疾病风险，并在大型数据集上进行了验证，结果显示出良好的预测能力。


<details>
  <summary>Details</summary>
Motivation: 为了从多模态信号中提取有意义的模式以进行心血管疾病（CVD）风险评估。

Method: 开发了一种自监督深度学习模型，从多模态信号（脑电图、心电图和呼吸信号）中提取有意义的模式。模型在4,398名参与者的数据上进行了训练。通过对比有和没有CVD结局的个体的嵌入来推导投影评分。在拥有1,093名参与者的独立队列中进行了外部验证。源代码可在https://github.com/miraclehetech/sleep-ssl获取。

Result: 投影评分揭示了跨模态的、临床上显著的模式。ECG衍生的特征可预测患病和新发心脏病，特别是CVD死亡率。EEG衍生的特征可预测新发高血压和CVD死亡率。呼吸信号增加了互补的预测价值。将这些投影评分与Framingham风险评分相结合，持续提高了预测性能，在不同结局下的曲线下面积值在0.607到0.965之间。研究结果在外部测试队列中得到了稳健的复制和验证。

Conclusion: 本研究表明，所提出的框架能够直接从PSG数据生成个体化CVD风险评分。生成的投影评分有潜力整合到临床实践中，以增强风险评估和支持个性化护理。

Abstract: Methods: We developed a self-supervised deep learning model that extracts
meaningful patterns from multi-modal signals (Electroencephalography (EEG),
Electrocardiography (ECG), and respiratory signals). The model was trained on
data from 4,398 participants. Projection scores were derived by contrasting
embeddings from individuals with and without CVD outcomes. External validation
was conducted in an independent cohort with 1,093 participants. The source code
is available on https://github.com/miraclehetech/sleep-ssl. Results: The
projection scores revealed distinct and clinically meaningful patterns across
modalities. ECG-derived features were predictive of both prevalent and incident
cardiac conditions, particularly CVD mortality. EEG-derived features were
predictive of incident hypertension and CVD mortality. Respiratory signals
added complementary predictive value. Combining these projection scores with
the Framingham Risk Score consistently improved predictive performance,
achieving area under the curve values ranging from 0.607 to 0.965 across
different outcomes. Findings were robustly replicated and validated in the
external testing cohort. Conclusion: Our findings demonstrate that the proposed
framework can generate individualized CVD risk scores directly from PSG data.
The resulting projection scores have the potential to be integrated into
clinical practice, enhancing risk assessment and supporting personalized care.

</details>


### [354] [Compression Method for Deep Diagonal State Space Model Based on $H^2$ Optimal Reduction](https://arxiv.org/abs/2507.10078)
*Hiroki Sakamoto,Kazuhiro Sato*

Main category: cs.LG

TL;DR: 提出一种利用H2模型降阶技术高效约减深度学习模型（含线性SSM）参数的方法，可在保持性能的同时大幅减小模型尺寸。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习模型（特别是包含线性SSM的部分）因参数量大而难以在资源受限设备上部署的问题。

Method: 将控制理论中的H2模型降阶技术应用于深度学习模型（特别是包含线性SSM的部分）以实现参数约减。

Result: 实验结果表明，该方法在LRA基准测试中，将SSM的参数量减少到原来的1/32，同时性能与原模型相当，并且优于使用平衡截断的现有方法。

Conclusion: 所提出的基于H2模型降阶技术的参数约减方法在LRA基准测试中表现优于现有的平衡截断方法，并能在不牺牲模型性能的情况下将SSM参数量减少到原来的1/32。

Abstract: Deep learning models incorporating linear SSMs have gained attention for
capturing long-range dependencies in sequential data. However, their large
parameter sizes pose challenges for deployment on resource-constrained devices.
In this study, we propose an efficient parameter reduction method for these
models by applying $H^{2}$ model order reduction techniques from control theory
to their linear SSM components. In experiments, the LRA benchmark results show
that the model compression based on our proposed method outperforms an existing
method using the Balanced Truncation, while successfully reducing the number of
parameters in the SSMs to $1/32$ without sacrificing the performance of the
original models.

</details>


### [355] [Enhancing RLHF with Human Gaze Modeling](https://arxiv.org/abs/2507.09016)
*Karim Galliamov,Ivan Titov,Ilya Pershin*

Main category: cs.LG

TL;DR: 本文提出利用人类注视信号来提高RLHF的效率，实验证明该方法可以加快收敛速度并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: RLHF虽然能使语言模型与人类偏好保持一致，但计算成本高昂。本文旨在探索降低RLHF计算成本的方法。

Method: 本文探索了两种利用人类注视模型来增强RLHF的方法：(1)注视感知奖励模型和(2)基于注视的稀疏奖励在token级别的分布。

Result: 实验表明，引入注视信息的人类反馈强化学习（RLHF）能够实现更快的收敛速度，同时保持或略微提高性能，从而降低了策略优化过程中的计算成本。

Conclusion: 人类注视信号可以提高RLHF的效率，在策略优化过程中具有潜在的应用价值。

Abstract: Reinforcement Learning from Human Feedback (RLHF) aligns language models with
human preferences but is computationally expensive. We explore two approaches
that leverage human gaze modeling to enhance RLHF: (1) gaze-aware reward models
and (2) gaze-based distribution of sparse rewards at token level. Our
experiments demonstate that gaze-informed RLHF achieves faster convergence
while maintaining or slightly improving performance, thus, reducing
computational costs during policy optimization. These results show that human
gaze provides a valuable and underused signal for policy optimization, pointing
to a promising direction for improving RLHF efficiency.

</details>


### [356] [Model Parallelism With Subnetwork Data Parallelism](https://arxiv.org/abs/2507.09029)
*Vaibhav Singh,Zafir Khalid,Edouard Oyallon,Eugene Belilovsky*

Main category: cs.LG

TL;DR: 在分布式预训练期间，通过训练模型的子网络来降低内存需求，并提出和评估了两种子网络构建策略，其中随机块丢弃方法效果更优。


<details>
  <summary>Details</summary>
Motivation: 分布式预训练大模型在扩展时，对单个节点的内存需求很大，并且节点内的通信成本很高。

Method: 提出了一种通过在独立的工作节点上训练模型的子网络来减少内存需求的替代方法。并评估了两种以确保在分布式训练设置中均匀表示每个参数为指导原则的子网络构建策略，其中随机块丢弃技术表现优于先前在联邦学习中所探索的宽度子网络构建方法，其原因归因于保留了具有跳跃连接的块的子网络具有更强的梯度对齐能力。

Result: 随机块丢弃技术（stochastic block dropping）优于宽度子网络构建（width-wise subnetwork construction），并且实现了在不损失性能的情况下将内存使用量减少20%-40%。

Conclusion: 该方法实现了在不损失性能的情况下，将内存使用量减少20%-40%，且其方法所带来的的通信带宽需求可与标准的参数 सर्व-रिड्यूस (all-reduce) 数据并行通信方案相媲美，甚至更低。

Abstract: Distributed pre-training of large models at scale often imposes heavy memory
demands on individual nodes and incurs significant intra-node communication
costs. We propose a novel alternative approach that reduces the memory
requirements by training small, structured subnetworks of the model on separate
workers. Unlike pipelining, our method avoids inter-node activation
communication and maintains bandwidth requirements that are comparable to or
lower than standard data parallel communication schemes based on all-reduce. We
evaluate two subnetwork construction strategies guided by the principle of
ensuring uniform representation of each parameter across the distributed
training setup. Our results show that the stochastic block dropping technique
consistently outperforms the width-wise subnetwork construction previously
explored in federated learning. We empirically attribute this superior
performance to stronger gradient alignment in subnetworks that retain blocks
having skip connections. Preliminary experiments highlight the promise of our
approach, achieving a 20-40% reduction in memory usage without any loss in
performance.

</details>


### [357] [Confounder-Free Continual Learning via Recursive Feature Normalization](https://arxiv.org/abs/2507.09031)
*Yash Shah,Camila Gonzalez,Mohammad H. Abbasi,Qingyu Zhao,Kilian M. Pohl,Ehsan Adeli*

Main category: cs.LG

TL;DR: R-MDN is a new layer that can be added to any deep learning model to remove the influence of confounders in continual learning settings. It uses recursive least squares to update its internal state and has been shown to promote equitable predictions and reduce catastrophic forgetting.


<details>
  <summary>Details</summary>
Motivation: In continual learning, learning feature representations that are invariant to confounders remains a significant challenge. Confounders can lead to spurious correlations and biased predictions, and their effects can change over time, causing catastrophic forgetting.

Method: R-MDN performs statistical regression via the recursive least squares algorithm to maintain and continually update an internal model state with respect to changing distributions of data and confounding variables.

Result: R-MDN promotes equitable predictions across population groups, both within static learning and across different stages of continual learning, by reducing catastrophic forgetting caused by confounder effects changing over time.

Conclusion: R-MDN

Abstract: Confounders are extraneous variables that affect both the input and the
target, resulting in spurious correlations and biased predictions. There are
recent advances in dealing with or removing confounders in traditional models,
such as metadata normalization (MDN), where the distribution of the learned
features is adjusted based on the study confounders. However, in the context of
continual learning, where a model learns continuously from new data over time
without forgetting, learning feature representations that are invariant to
confounders remains a significant challenge. To remove their influence from
intermediate feature representations, we introduce the Recursive MDN (R-MDN)
layer, which can be integrated into any deep learning architecture, including
vision transformers, and at any model stage. R-MDN performs statistical
regression via the recursive least squares algorithm to maintain and
continually update an internal model state with respect to changing
distributions of data and confounding variables. Our experiments demonstrate
that R-MDN promotes equitable predictions across population groups, both within
static learning and across different stages of continual learning, by reducing
catastrophic forgetting caused by confounder effects changing over time.

</details>


### [358] [Shortening the Trajectories: Identity-Aware Gaussian Approximation for Efficient 3D Molecular Generation](https://arxiv.org/abs/2507.09043)
*Jingxiang Qu,Wenhan Gao,Yi Liu*

Main category: cs.LG

TL;DR: 通过用闭式高斯近似替换生成轨迹中数据失去身份信息后的部分，可以显著提高GPGM的生成效率和样本质量。


<details>
  <summary>Details</summary>
Motivation: 现有的GPGM模型在训练和采样过程中需要大量的计算成本，因为它们涉及到成百上千个步骤。本研究旨在提高GPGM的生成效率，降低计算成本。

Method: 通过分析确定数据获得高斯性的特征步，并用闭式高斯近似替换剩余的生成轨迹，以加速高斯基数概率生成模型（GPGMs）的采样过程。

Result: 在多种数据模态上的实验结果表明，该方法在样本质量和计算效率方面都有显著提升。

Conclusion: 该研究提出了一种理论上可行且经过实践验证的框架，可以在不牺牲训练细节或推理保真度的情况下提高生成效率。

Abstract: Gaussian-based Probabilistic Generative Models (GPGMs) generate data by
reversing a stochastic process that progressively corrupts samples with
Gaussian noise. While these models have achieved state-of-the-art performance
across diverse domains, their practical deployment remains constrained by the
high computational cost of long generative trajectories, which often involve
hundreds to thousands of steps during training and sampling. In this work, we
introduce a theoretically grounded and empirically validated framework that
improves generation efficiency without sacrificing training granularity or
inference fidelity. Our key insight is that for certain data modalities, the
noising process causes data to rapidly lose its identity and converge toward a
Gaussian distribution. We analytically identify a characteristic step at which
the data has acquired sufficient Gaussianity, and then replace the remaining
generation trajectory with a closed-form Gaussian approximation. Unlike
existing acceleration techniques that coarsening the trajectories by skipping
steps, our method preserves the full resolution of learning dynamics while
avoiding redundant stochastic perturbations between `Gaussian-like'
distributions. Empirical results across multiple data modalities demonstrate
substantial improvements in both sample quality and computational efficiency.

</details>


### [359] [Queue up for takeoff: a transferable deep learning framework for flight delay prediction](https://arxiv.org/abs/2507.09084)
*Nnamdi Daniel Aghanya,Ta Duong Vu,Amaëlle Diop,Charlotte Deville,Nour Imane Kerroumi,Irene Moulitsas,Jun Li,Desmond Bisandu*

Main category: cs.LG

TL;DR: 本文提出了一种名为 QT-SimAM 的新方法，结合了排队论和注意力模型，以提高航班延误预测的准确性和可推广性。该模型在美国和欧洲的数据集上都表现出色。


<details>
  <summary>Details</summary>
Motivation: 为了改善乘客体验和减少收入损失，航班延误预测模型必须精确且可跨不同网络推广。

Method: 本文提出了一种结合排队论和简单注意力模型的组合方法，称为队列论-SimAM（QT-SimAM）。

Result: 使用来自美国运输统计局的数据进行验证，所提出的 QT-SimAM（双向）模型的准确率为 0.927，F1 分数为 0.932，优于现有方法。在 EUROCONTROL 数据集上测试模型以评估可转移性，结果显示出强大的性能，准确率为 0.826，F1 分数为 0.791。

Conclusion: 该论文提出了一种有效的端到端方法来预测航班延误，所提出的模型能够跨不同网络准确预测延误，从而有助于减轻乘客焦虑并改善运营决策。

Abstract: Flight delays are a significant challenge in the aviation industry, causing
major financial and operational disruptions. To improve passenger experience
and reduce revenue loss, flight delay prediction models must be both precise
and generalizable across different networks. This paper introduces a novel
approach that combines Queue-Theory with a simple attention model, referred to
as the Queue-Theory SimAM (QT-SimAM). To validate our model, we used data from
the US Bureau of Transportation Statistics, where our proposed QT-SimAM
(Bidirectional) model outperformed existing methods with an accuracy of 0.927
and an F1 score of 0.932. To assess transferability, we tested the model on the
EUROCONTROL dataset. The results demonstrated strong performance, achieving an
accuracy of 0.826 and an F1 score of 0.791. Ultimately, this paper outlines an
effective, end-to-end methodology for predicting flight delays. The proposed
model's ability to forecast delays with high accuracy across different networks
can help reduce passenger anxiety and improve operational decision-making

</details>


### [360] [Deep Reinforcement Learning with Gradient Eligibility Traces](https://arxiv.org/abs/2507.09087)
*Esraa Elelimy,Brett Daley,Andrew Patterson,Marlos C. Machado,Adam White,Martha White*

Main category: cs.LG

TL;DR: 提出了一种改进的GPBE目标，用于深度强化学习的多步离策略学习，并在实验中取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 解决深度强化学习中快速稳定进行离策略学习的挑战，并改进现有GTD方法的局限性，使其适用于多步信用分配。

Method: 将GPBE目标扩展到支持基于λ-return的多步信用分配，并推导了三种优化此新目标的基于梯度的方法。提供了与经验回放兼容的前向视图和与流算法兼容的后向视图。

Result: 推导了三种基于梯度的方法，并在MuJoCo和MinAtar环境中进行了评估，结果优于现有方法。

Conclusion: 提出的算法在MuJoCo和MinAtar环境中优于 PPO 和 StreamQ。

Abstract: Achieving fast and stable off-policy learning in deep reinforcement learning
(RL) is challenging. Most existing methods rely on semi-gradient
temporal-difference (TD) methods for their simplicity and efficiency, but are
consequently susceptible to divergence. While more principled approaches like
Gradient TD (GTD) methods have strong convergence guarantees, they have rarely
been used in deep RL. Recent work introduced the Generalized Projected Bellman
Error ($\GPBE$), enabling GTD methods to work efficiently with nonlinear
function approximation. However, this work is only limited to one-step methods,
which are slow at credit assignment and require a large number of samples. In
this paper, we extend the $\GPBE$ objective to support multistep credit
assignment based on the $\lambda$-return and derive three gradient-based
methods that optimize this new objective. We provide both a forward-view
formulation compatible with experience replay and a backward-view formulation
compatible with streaming algorithms. Finally, we evaluate the proposed
algorithms and show that they outperform both PPO and StreamQ in MuJoCo and
MinAtar environments, respectively. Code available at
https://github.com/esraaelelimy/gtd\_algos

</details>


### [361] [On the Fragility of Multimodal Perception to Temporal Misalignment in Autonomous Driving](https://arxiv.org/abs/2507.09095)
*Md Hasan Shahriar,Md Mohaimin Al Barat,Harshavardhan Sundar,Naren Ramakrishnan,Y. Thomas Hou,Wenjing Lou*

Main category: cs.LG

TL;DR: 通过引入网络延迟导致的时间错位来攻击自动驾驶中的多模态融合感知系统，并提出了一种基于时间一致性监控的防御方法。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶中的多模态融合（MMF）虽然能提升感知能力，但其对精确时间同步的依赖性也带来了新的安全漏洞。因此，研究如何利用时间错位攻击MMF感知系统并提出相应的防御措施至关重要。

Method: 本文提出了DejaVu攻击，通过引入网络延迟来制造传感器数据的时间错位，并分析了其对下游多模态融合感知任务的影响。同时，提出了名为AION的防御机制，利用跨模态共享表示学习和动态时间规整来监控时间对齐，并计算异常得分。

Result: DejaVu攻击能够显著降低车辆检测的mAP（最高可达88.5%）和多目标跟踪的MOTA（最高可达73%），具体影响取决于传感器和任务的依赖性。AION防御机制在不同数据集和模型上实现了0.92-0.98的AUROC得分，证明了其有效性和泛化能力。

Conclusion: DejaVu攻击利用网络延迟导致的时间错位会严重损害多模态融合感知任务，但AION防御可以在跨模态时间一致性监控下缓解此问题，在各种模型和数据集上均表现出高检测率和低误报率。

Abstract: Multimodal fusion (MMF) plays a critical role in the perception of autonomous
driving, which primarily fuses camera and LiDAR streams for a comprehensive and
efficient scene understanding. However, its strict reliance on precise temporal
synchronization exposes it to new vulnerabilities. In this paper, we introduce
DejaVu, a novel attack that exploits network-induced delays to create subtle
temporal misalignments across sensor streams, severely degrading downstream
MMF-based perception tasks. Our comprehensive attack analysis across different
models and datasets reveals these sensors' task-specific imbalanced
sensitivities: object detection is overly dependent on LiDAR inputs while
object tracking is highly reliant on the camera inputs. Consequently, with a
single-frame LiDAR delay, an attacker can reduce the car detection mAP by up to
88.5%, while with a three-frame camera delay, multiple object tracking accuracy
(MOTA) for car drops by 73%. To detect such attacks, we propose AION, a defense
patch that can work alongside the existing perception model to monitor temporal
alignment through cross-modal temporal consistency. AION leverages multimodal
shared representation learning and dynamic time warping to determine the path
of temporal alignment and calculate anomaly scores based on the alignment. Our
thorough evaluation of AION shows it achieves AUROC scores of 0.92-0.98 with
low false positives across datasets and model architectures, demonstrating it
as a robust and generalized defense against the temporal misalignment attacks.

</details>


### [362] [Principled Foundations for Preference Optimization](https://arxiv.org/abs/2507.07855)
*Wenxuan Zhou,Shujian Zhang,Brice Magdalou,John Lambert,Ehsan Amid,Richard Nock,Andrew Hard*

Main category: cs.LG

TL;DR: DPO是Savage损失函数和Doignon-Falmagne-Machina随机选择理论之间的特定联系，支持弃权、非凸目标和DPO的扩展。理解DPO的原理至关重要。


<details>
  <summary>Details</summary>
Motivation: 理解DPO的运作方式对于其广泛的应用、当前的研究热度和许多现有DPO变体都至关重要，同时也有助于理解偏离其理论框架的潜在问题及解决方法。

Method: 本文将直接偏好优化（DPO）与机器学习中的损失函数（Savage）和随机选择（Doignon-Falmagne和Machina）理论联系起来，揭示了DPO是这两种理论之间的一种特定形式的关联。

Result: 该研究建立了DPO与Savage损失函数以及Doignon-Falmagne和Machina随机选择理论之间的一般性联系，支持弃权选择、非凸目标函数，并能免费纳入保证金和长度修正等DPO扩展。

Conclusion: DPO是一种介于Savage损失函数和Doignon-Falmagne-Machina随机选择理论之间的特定联系，支持弃权、非凸目标和DPO的扩展（如保证金和长度修正）。理解DPO的原理至关重要，因为它的应用广泛，并且许多DPO的变体都位于我们所覆盖的理论框架内，这有助于避免错误并找到解决方案。

Abstract: In this paper, we show that direct preference optimization (DPO) is a very
specific form of a connection between two major theories in the ML context of
learning from preferences: loss functions (Savage) and stochastic choice
(Doignon-Falmagne and Machina). The connection is established for all of
Savage's losses and at this level of generality, (i) it includes support for
abstention on the choice theory side, (ii) it includes support for non-convex
objectives on the ML side, and (iii) it allows to frame for free some notable
extensions of the DPO setting, including margins and corrections for length.
Getting to understand how DPO operates from a general principled perspective is
crucial because of the huge and diverse application landscape of models,
because of the current momentum around DPO, but also -- and importantly --
because many state of the art variations on DPO definitely occupy a small
region of the map that we cover. It also helps to understand the pitfalls of
departing from this map, and figure out workarounds.

</details>


### [363] [S2SRec2: Set-to-Set Recommendation for Basket Completion with Recipe](https://arxiv.org/abs/2507.09101)
*Yanan Cao,Omid Memarrast,Shiqin Cai,Sinduja Subramaniam,Evren Korpeoglu,Kannan Achan*

Main category: cs.LG

TL;DR: Recommends sets of ingredients for incomplete grocery baskets using a Set Transformer to improve meal completion and culinary inspiration.


<details>
  <summary>Details</summary>
Motivation: Traditional recipe completion methods fail to reflect real-world scenarios where multiple ingredients are needed and overlook relationships among missing ingredients. The goal is to improve the culinary experience by recommending complementary ingredients based on a partial basket.

Method: Reformulated basket completion as a set-to-set (S2S) recommendation problem, introducing S2SRec2, a framework based on a Set Transformer trained in a multitask learning paradigm. S2SRec2 jointly learns to retrieve missing ingredients and assess basket completeness.

Result: Experiments on large-scale recipe datasets and qualitative analyses show that S2SRec2 significantly outperforms single-target baselines.

Conclusion: S2SRec2 is a promising approach to enhance grocery shopping and inspire culinary creativity by significantly outperforming single-target baselines.

Abstract: In grocery e-commerce, customers often build ingredient baskets guided by
dietary preferences but lack the expertise to create complete meals. Leveraging
recipe knowledge to recommend complementary ingredients based on a partial
basket is essential for improving the culinary experience. Traditional recipe
completion methods typically predict a single missing ingredient using a
leave-one-out strategy. However, they fall short in two key aspects: (i) they
do not reflect real-world scenarios where multiple ingredients are often
needed, and (ii) they overlook relationships among the missing ingredients
themselves. To address these limitations, we reformulate basket completion as a
set-to-set (S2S) recommendation problem, where an incomplete basket is input
into a system that predicts a set of complementary ingredients. We introduce
S2SRec2, a set-to-set ingredient recommendation framework based on a Set
Transformer and trained in a multitask learning paradigm. S2SRec2 jointly
learns to (i) retrieve missing ingredients from the representation of existing
ones and (ii) assess basket completeness after prediction. These tasks are
optimized together, enforcing accurate retrieval and coherent basket
completion. Experiments on large-scale recipe datasets and qualitative analyses
show that S2SRec2 significantly outperforms single-target baselines, offering a
promising approach to enhance grocery shopping and inspire culinary creativity.

</details>


### [364] [A Study of Value-Aware Eigenoptions](https://arxiv.org/abs/2507.09127)
*Harshil Kotamreddy,Marlos C. Machado*

Main category: cs.LG

TL;DR: Eigenoptions help RL exploration and credit assignment, but learning them online can hurt performance. Termination conditions matter in deep RL.


<details>
  <summary>Details</summary>
Motivation: To explore whether eigenoptions can accelerate credit assignment in model-free RL and to understand their role beyond exploration, especially in the context of deep RL and non-linear function approximation.

Method: Investigated the use of eigenoptions in model-free RL, evaluating them in tabular and pixel-based gridworlds. Also proposed a method for learning option-values in deep RL with non-linear function approximation.

Result: Pre-specified eigenoptions improved both exploration and credit assignment. Online discovery of eigenoptions hindered learning. The proposed method for learning option-values in deep RL showed the importance of termination conditions.

Conclusion: Eigenoptions, particularly when pre-specified, can enhance both exploration and credit assignment in model-free RL. However, online discovery of eigenoptions can negatively impact learning by introducing strong biases. In deep RL, learning option-values requires careful consideration of termination conditions, especially under non-linear function approximation.

Abstract: Options, which impose an inductive bias toward temporal and hierarchical
structure, offer a powerful framework for reinforcement learning (RL). While
effective in sequential decision-making, they are often handcrafted rather than
learned. Among approaches for discovering options, eigenoptions have shown
strong performance in exploration, but their role in credit assignment remains
underexplored. In this paper, we investigate whether eigenoptions can
accelerate credit assignment in model-free RL, evaluating them in tabular and
pixel-based gridworlds. We find that pre-specified eigenoptions aid not only
exploration but also credit assignment, whereas online discovery can bias the
agent's experience too strongly and hinder learning. In the context of deep RL,
we also propose a method for learning option-values under non-linear function
approximation, highlighting the impact of termination conditions on
performance. Our findings reveal both the promise and complexity of using
eigenoptions, and options more broadly, to simultaneously support credit
assignment and exploration in reinforcement learning.

</details>


### [365] [Heterogeneous Graph Prompt Learning via Adaptive Weight Pruning](https://arxiv.org/abs/2507.09132)
*Chu-Yuan Wei,Shun-Yao Liu,Sheng-Da Zhuo,Chang-Dong Wang,Shu-Qiang Huang,Mohsen Guizani*

Main category: cs.LG

TL;DR: GPAWP框架结合图提示和权重剪枝，通过剪枝负面提示标签来优化GNN性能和效率，显著减少参数量。


<details>
  <summary>Details</summary>
Motivation: 旨在解决现有图神经网络（GNNs）在训练和推理时间长、难以捕捉复杂关系以及特征提取不足等问题。同时，填补了先前研究在图提示优化模型潜力、以及正负图提示对模型稳定性和效率影响方面的空白。

Method: 提出了一种结合图提示（graph prompts）和权重剪枝（weight pruning）的新框架GPAWP。该框架通过重要性评估函数来评估不同粒度的图提示的正负权重，并通过分层结构剪枝（hierarchically structured pruning）消除负面提示标签，以提升图提示的性能和效率。

Result: 通过在三个基准数据集上的广泛实验证明，GPAWP在节点分类任务中能够显著减少参数量，同时保持甚至提升GNN的性能，验证了该框架的优越性。

Conclusion: GPAWP通过结合图提示和权重剪枝，利用重要性评估函数识别并消除负面提示标签，实现了参数更少但性能表现优越的图提示，在节点分类任务上显著减少了参数量。

Abstract: Graph Neural Networks (GNNs) have achieved remarkable success in various
graph-based tasks (e.g., node classification or link prediction). Despite their
triumphs, GNNs still face challenges such as long training and inference times,
difficulty in capturing complex relationships, and insufficient feature
extraction. To tackle these issues, graph pre-training and graph prompt methods
have garnered increasing attention for their ability to leverage large-scale
datasets for initial learning and task-specific adaptation, offering potential
improvements in GNN performance. However, previous research has overlooked the
potential of graph prompts in optimizing models, as well as the impact of both
positive and negative graph prompts on model stability and efficiency. To
bridge this gap, we propose a novel framework combining graph prompts with
weight pruning, called GPAWP, which aims to enhance the performance and
efficiency of graph prompts by using fewer of them. We evaluate the importance
of graph prompts using an importance assessment function to determine positive
and negative weights at different granularities. Through hierarchically
structured pruning, we eliminate negative prompt labels, resulting in more
parameter-efficient and competitively performing prompts. Extensive experiments
on three benchmark datasets demonstrate the superiority of GPAWP, leading to a
significant reduction in parameters in node classification tasks.

</details>


### [366] [POIFormer: A Transformer-Based Framework for Accurate and Scalable Point-of-Interest Attribution](https://arxiv.org/abs/2507.09137)
*Nripsuta Ani Saxena,Shang-Ling Hsu,Mehul Shetty,Omar Alkhadra,Cyrus Shahabi,Abigail L. Horn*

Main category: cs.LG

TL;DR: POIFormer是一个基于Transformer的新框架，通过整合多种信号（空间、时间、上下文、行为）来解决GPS不准和POI密集导致的POI归因难题，并在真实世界数据上展现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 准确地将用户访问归因于特定的兴趣点（POI）是移动分析、个性化服务、市场营销和城市规划的基础，但由于GPS不准确和POI空间密度高，这一任务具有挑战性。

Method: 提出了一种名为POIFormer的新型Transformer模型，该模型能够整合空间邻近性、访问时间、持续时间、POI语义上下文以及用户移动和人群行为模式等多种特征，并利用Transformer的自注意力机制来捕捉这些维度之间复杂的交互作用。

Result: 通过在真实世界移动数据集上进行的大量实验证明，POIFormer在具有空间噪声和密集POI聚类的真实世界场景中，相比现有基线方法取得了显著的改进。

Conclusion: POIFormer框架在处理高密度和空间噪声大的真实世界移动数据集方面表现出色，显著优于现有方法。

Abstract: Accurately attributing user visits to specific Points of Interest (POIs) is a
foundational task for mobility analytics, personalized services, marketing and
urban planning. However, POI attribution remains challenging due to GPS
inaccuracies, typically ranging from 2 to 20 meters in real-world settings, and
the high spatial density of POIs in urban environments, where multiple venues
can coexist within a small radius (e.g., over 50 POIs within a 100-meter radius
in dense city centers). Relying on proximity is therefore often insufficient
for determining which POI was actually visited. We introduce
\textsf{POIFormer}, a novel Transformer-based framework for accurate and
efficient POI attribution. Unlike prior approaches that rely on limited
spatiotemporal, contextual, or behavioral features, \textsf{POIFormer} jointly
models a rich set of signals, including spatial proximity, visit timing and
duration, contextual features from POI semantics, and behavioral features from
user mobility and aggregated crowd behavior patterns--using the Transformer's
self-attention mechanism to jointly model complex interactions across these
dimensions. By leveraging the Transformer to model a user's past and future
visits (with the current visit masked) and incorporating crowd-level behavioral
patterns through pre-computed KDEs, \textsf{POIFormer} enables accurate,
efficient attribution in large, noisy mobility datasets. Its architecture
supports generalization across diverse data sources and geographic contexts
while avoiding reliance on hard-to-access or unavailable data layers, making it
practical for real-world deployment. Extensive experiments on real-world
mobility datasets demonstrate significant improvements over existing baselines,
particularly in challenging real-world settings characterized by spatial noise
and dense POI clustering.

</details>


### [367] [Towards Interpretable Drug-Drug Interaction Prediction: A Graph-Based Approach with Molecular and Network-Level Explanations](https://arxiv.org/abs/2507.09173)
*Mengjie Chen,Ming Zhang,Cunquan Qu*

Main category: cs.LG

TL;DR: MolecBioNet是一个创新的图基框架，通过整合分子和生物医学知识，将药物对视为统一实体，并采用CASPool和AGIPool等特定领域池化策略，在DDI预测方面取得了优于现有方法的性能，并提高了可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决现有图基方法在DDI预测中独立处理药物对、忽视药物对特有的复杂交互作用以及难以整合生物相互作用网络和分子结构以提供机制见解的问题。

Method: 提出了一种名为MolecBioNet的新型图基框架，通过整合分子和生物医学知识进行DDI预测。该框架将药物对视为统一实体，模拟宏观生物相互作用和微观分子影响。它从生物医学知识图中提取局部子图，并从分子表示构建层次交互图，利用图神经网络学习药物对的多尺度表示。为了提高准确性和可解释性，MolecBioNet引入了上下文感知子图池化（CASPool）和注意力引导影响池化（AGIPool）两种特定领域的池化策略，并采用互信息最小化正则化来增强嵌入融合过程中的信息多样性。

Result: 实验结果表明，MolecBioNet在DDI预测方面优于现有最先进方法。消融研究和嵌入可视化进一步验证了统一药物对建模和多尺度知识集成的优势。

Conclusion: MolecBioNet框架在药物-药物相互作用（DDI）预测方面表现优于现有最先进方法，并且通过消融研究和嵌入可视化验证了统一药物对建模和多尺度知识集成的优势。

Abstract: Drug-drug interactions (DDIs) represent a critical challenge in pharmacology,
often leading to adverse drug reactions with significant implications for
patient safety and healthcare outcomes. While graph-based methods have achieved
strong predictive performance, most approaches treat drug pairs independently,
overlooking the complex, context-dependent interactions unique to drug pairs.
Additionally, these models struggle to integrate biological interaction
networks and molecular-level structures to provide meaningful mechanistic
insights. In this study, we propose MolecBioNet, a novel graph-based framework
that integrates molecular and biomedical knowledge for robust and interpretable
DDI prediction. By modeling drug pairs as unified entities, MolecBioNet
captures both macro-level biological interactions and micro-level molecular
influences, offering a comprehensive perspective on DDIs. The framework
extracts local subgraphs from biomedical knowledge graphs and constructs
hierarchical interaction graphs from molecular representations, leveraging
classical graph neural network methods to learn multi-scale representations of
drug pairs. To enhance accuracy and interpretability, MolecBioNet introduces
two domain-specific pooling strategies: context-aware subgraph pooling
(CASPool), which emphasizes biologically relevant entities, and
attention-guided influence pooling (AGIPool), which prioritizes influential
molecular substructures. The framework further employs mutual information
minimization regularization to enhance information diversity during embedding
fusion. Experimental results demonstrate that MolecBioNet outperforms
state-of-the-art methods in DDI prediction, while ablation studies and
embedding visualizations further validate the advantages of unified drug pair
modeling and multi-scale knowledge integration.

</details>


### [368] [Continual Reinforcement Learning by Planning with Online World Models](https://arxiv.org/abs/2507.09177)
*Zichen Liu,Guoji Fu,Chao Du,Wee Sun Lee,Min Lin*

Main category: cs.LG

TL;DR: 提出了一种名为 OA 的持续强化学习方法，利用在线世界模型和模型预测控制来解决灾难性遗忘问题，并在专门设计的 Continual Bench 环境中取得了优于基线方法的成果。


<details>
  <summary>Details</summary>
Motivation: 解决持续强化学习（CRL）中的灾难性遗忘问题，即智能体在学习新任务时可能会忘记如何解决旧任务。

Method: 提出一种使用在线世界模型进行规划的方法，具体是学习一个在线的 Follow-The-Leader（FTL）浅层模型来捕捉世界动态，并使用模型预测控制（MPC）来规划以解决由任何奖励函数指定的任务集。该在线世界模型通过其构建方式对遗忘免疫，并在温和假设下具有$\|mathcal {O}(\sqrt {K^2D\log (T)})$的遗憾界限。规划器仅基于最新的在线模型进行动作搜索，形成了一个增量更新的 FTL 在线代理（OA）。

Result: OA 能够持续学习新任务且不忘记旧技能，在 Continual Bench 环境中表现优于基于深度世界模型和各种持续学习技术的智能体。

Conclusion: 该研究提出的在线世界模型规划方法在 Continual Bench 环境中表现优于其他基线方法，能够持续学习新任务且不忘记旧技能。

Abstract: Continual reinforcement learning (CRL) refers to a naturalistic setting where
an agent needs to endlessly evolve, by trial and error, to solve multiple tasks
that are presented sequentially. One of the largest obstacles to CRL is that
the agent may forget how to solve previous tasks when learning a new task,
known as catastrophic forgetting. In this paper, we propose to address this
challenge by planning with online world models. Specifically, we learn a
Follow-The-Leader shallow model online to capture the world dynamics, in which
we plan using model predictive control to solve a set of tasks specified by any
reward functions. The online world model is immune to forgetting by
construction with a proven regret bound of $\mathcal{O}(\sqrt{K^2D\log(T)})$
under mild assumptions. The planner searches actions solely based on the latest
online model, thus forming a FTL Online Agent (OA) that updates incrementally.
To assess OA, we further design Continual Bench, a dedicated environment for
CRL, and compare with several strong baselines under the same model-planning
algorithmic framework. The empirical results show that OA learns continuously
to solve new tasks while not forgetting old skills, outperforming agents built
on deep world models with various continual learning techniques.

</details>


### [369] [XiChen: An observation-scalable fully AI-driven global weather forecasting system with 4D variational knowledge](https://arxiv.org/abs/2507.09202)
*Wuxin Wang,Weicheng Ni,Lilan Huang,Tao Hao,Ben Fei,Shuo Ma,Taikang Yuan,Yanlai Zhao,Kefeng Deng,Xiaoyong Li,Boheng Duan,Lei Bai,Kaijun Ren*

Main category: cs.LG

TL;DR: XiChen是一个革新性的、完全由AI驱动的天气预报系统，能在17秒内完成数据同化和中程预报，预测精度媲美NWP系统且提前量超过8.25天。


<details>
  <summary>Details</summary>
Motivation: 当前的AI天气预报模型大多依赖NWP系统进行初始条件准备，而该过程耗时较长。

Method: XiChen是一个基于预训练的AI基础模型构建的、完全由AI驱动的全球天气预报系统，其数据同化和中程预报的整个流程可在17秒内完成。该模型被微调用作观测算子和数据同化模型，能够扩展地同化常规和原始卫星观测数据，并结合了四维变分知识。

Result: XiChen的数据同化和中程预报精度可与运行中的NWP系统相媲美，预测提前量超过8.25天。

Conclusion: XiChen展示了完全由人工智能驱动的、独立于NWP系统的天气预报的巨大潜力。

Abstract: Recent advancements in Artificial Intelligence (AI) demonstrate significant
potential to revolutionize weather forecasting. However, most AI-driven models
rely on Numerical Weather Prediction (NWP) systems for initial condition
preparation, which often consumes hours on supercomputers. Here we introduce
XiChen, the first observation-scalable fully AI-driven global weather
forecasting system, whose entire pipeline, from Data Assimilation (DA) to
medium-range forecasting, can be accomplished within only 17 seconds. XiChen is
built upon a foundation model that is pre-trained for weather forecasting.
Meanwhile, this model is subsequently fine-tuned to serve as both observation
operators and DA models, thereby scalably assimilating conventional and raw
satellite observations. Furthermore, the integration of four-dimensional
variational knowledge ensures that XiChen's DA and medium-range forecasting
accuracy rivals that of operational NWP systems, amazingly achieving a skillful
forecasting lead time exceeding 8.25 days. These findings demonstrate that
XiChen holds strong potential toward fully AI-driven weather forecasting
independent of NWP systems.

</details>


### [370] [Capturing Unseen Spatial Extremes Through Knowledge-Informed Generative Modeling](https://arxiv.org/abs/2507.09211)
*Xinyue Liu,Xiao Peng,Shuyue Yan,Yuntian Chen,Dongxiao Zhang,Zhixiao Niu,Hui-Min Wang,Xiaogang He*

Main category: cs.LG

TL;DR: 该研究提出了一种名为DeepX-GAN的新型生成模型，用于模拟和分析超出历史记录范围的“未被看见”的气候极端事件，并考虑了空间依赖性。研究发现，这些未被看见的极端事件对脆弱地区影响更大，未来可能加剧，并强调了制定适应性政策以应对新兴风险的必要性。


<details>
  <summary>Details</summary>
Motivation: 观察到的气候极端事件记录不完整，忽略了超出历史范围的“未被看见”的极端事件，并且忽视空间依赖性低估了同步危害的风险。

Method: 提出了一种名为DeepX-GAN的知识增强深度生成模型，该模型能够增强对物理极端事件的依赖性嵌入，并通过生成对抗网络来更好地捕捉稀有极端事件的空间结构。该模型具有零样本泛化能力，可以模拟超出历史经验但统计上合理的未被看见的极端事件。

Result: DeepX-GAN能够模拟未被看见的极端事件，揭示了脆弱系统中的潜在风险。在中东和北非地区的应用表明，未被看见的极端事件对脆弱性高和低社会经济准备的地区影响尤为严重，但其紧迫性和解释方式有所不同。未来的变暖可能扩大和重新分配这些未被看见的极端事件，新兴暴露热点出现在印度-巴基斯坦和中非。

Conclusion: 未来的气候变化可能扩大和重新分配这些未被看见的极端事件，在中东和北非地区尤其如此。这些未被看见的极端事件对脆弱性高和低社会经济准备的地区影响尤为严重。印度-巴基斯坦和中非等地的新兴暴露热点也表明，传统的风险规划存在严重的盲点，需要制定适应性强的政策来应对新兴风险。

Abstract: Observed records of climate extremes provide an incomplete picture of risk,
missing "unseen" extremes that exceed historical bounds. In parallel,
neglecting spatial dependence undervalues the risk of synchronized hazards that
amplify impacts. To address these challenges, we develop DeepX-GAN
(Dependence-Enhanced Embedding for Physical eXtremes - Generative Adversarial
Network), a knowledge-informed deep generative model designed to better capture
the spatial structure of rare extremes. The zero-shot generalizability of
DeepX-GAN enables simulation of unseen extremes that fall outside historical
experience yet remain statistically plausible. We define two types of unseen
extremes: "checkmate" extremes that directly hit targets, and "stalemate"
extremes that narrowly miss. These unrealized scenarios expose latent risks in
fragile systems and may reinforce a false sense of resilience if overlooked.
Near misses, in particular, can prompt either proactive adaptation or dangerous
complacency, depending on how they are interpreted. Applying DeepX-GAN to the
Middle East and North Africa (MENA), we find that these unseen extremes
disproportionately affect regions with high vulnerability and low socioeconomic
readiness, but differ in urgency and interpretation. Future warming could
expand and redistribute these unseen extremes, with emerging exposure hotspots
in Indo-Pakistan and Central Africa. This distributional shift highlights
critical blind spots in conventional hazard planning and underscores the need
to develop spatially adaptive policies that anticipate emergent risk hotspots
rather than simply extrapolating from historical patterns.

</details>


### [371] [Warm Starts Accelerate Generative Modelling](https://arxiv.org/abs/2507.09212)
*Jonas Scholz,Richard E. Turner*

Main category: cs.LG

TL;DR: 通过使用一个信息性先验（N(μ, σ)）来替代无信息先验（N(0, I)），可以显著加速迭代生成模型。


<details>
  <summary>Details</summary>
Motivation: 为了解决迭代生成模型（如扩散和流匹配模型）在生成高保真样本时需要进行大量函数评估（通常是数百次）导致速度缓慢的问题。

Method: 提出了一种名为“温热启动模型”的简单确定性模型，该模型通过预测一个信息性先验 N(μ, σ) 来替代从无信息的 N(0, I)先验开始生成，其矩根据输入上下文进行条件化。

Result: 在图像修复等任务中，该方法在仅使用 11 次函数评估（1 次用于温热启动，10 次用于生成）的情况下，取得了与使用 1000 次迭代的 DDPM 基线相当的结果。通过一种简单的条件归一化技巧，该方法可以与任何标准的生成模型和采样器兼容，无需修改，从而可以与其他高效采样技术结合以实现进一步加速。

Conclusion: 该模型通过提供一个更好的起点，可以显著加速条件生成

Abstract: Iterative generative models, like diffusion and flow-matching, create
high-fidelity samples by progressively refining a noise vector into data.
However, this process is notoriously slow, often requiring hundreds of function
evaluations. We introduce the warm-start model, a simple, deterministic model
that dramatically accelerates conditional generation by providing a better
starting point. Instead of starting generation from an uninformed N(0, I)
prior, our warm-start model predicts an informed prior N(mu, sigma), whose
moments are conditioned on the input context. This "warm start" substantially
reduces the distance the generative process must traverse, particularly when
the conditioning information is strongly informative. On tasks like image
inpainting, our method achieves results competitive with a 1000-step DDPM
baseline using only 11 total function evaluations (1 for the warm start, 10 for
generation). A simple conditional normalization trick makes our method
compatible with any standard generative model and sampler without modification,
allowing it to be combined with other efficient sampling techniques for further
acceleration. Our implementation is available at
https://github.com/jonas-scholz123/warm-start-model.

</details>


### [372] [Optimizing Basis Function Selection in Constructive Wavelet Neural Networks and Its Applications](https://arxiv.org/abs/2507.09213)
*Dunsheng Huang,Dong Shen,Lei Lu,Ying Tan*

Main category: cs.LG

TL;DR: 一种新颖的结构化小波神经网络，通过频率估计和优化基选择来提高效率和精度。


<details>
  <summary>Details</summary>
Motivation: 传统的WNN在构建精确小波基和计算成本方面存在挑战，限制了其应用。

Method: 提出了一种结构化小波神经网络（WNN），通过引入新基来学习未知非线性映射，并通过频率估计来选择初始基，从而在保证精度的前提下降低了计算成本。

Result: 该框架通过优先选择高能基，在四个示例中（估计静态映射、合并数据集、识别时变映射、捕获非线性依赖关系）展示了其通用性和实用性，显著提高了计算效率。

Conclusion: 该研究提出的新颖的结构化小波神经网络框架通过频率估计和基于优先选择高能基的机制，显著提高了计算效率，并证明了其在各种应用中的通用性和实用性。

Abstract: Wavelet neural network (WNN), which learns an unknown nonlinear mapping from
the data, has been widely used in signal processing, and time-series analysis.
However, challenges in constructing accurate wavelet bases and high
computational costs limit their application. This study introduces a
constructive WNN that selects initial bases and trains functions by introducing
new bases for predefined accuracy while reducing computational costs. For the
first time, we analyze the frequency of unknown nonlinear functions and select
appropriate initial wavelets based on their primary frequency components by
estimating the energy of the spatial frequency component. This leads to a novel
constructive framework consisting of a frequency estimator and a wavelet-basis
increase mechanism to prioritize high-energy bases, significantly improving
computational efficiency. The theoretical foundation defines the necessary
time-frequency range for high-dimensional wavelets at a given accuracy. The
framework's versatility is demonstrated through four examples: estimating
unknown static mappings from offline data, combining two offline datasets,
identifying time-varying mappings from time-series data, and capturing
nonlinear dependencies in real time-series data. These examples showcase the
framework's broad applicability and practicality. All the code will be released
at https://github.com/dshuangdd/CWNN.

</details>


### [373] [TPP-SD: Accelerating Transformer Point Process Sampling with Speculative Decoding](https://arxiv.org/abs/2507.09252)
*Shukai Gong,Yiyang Fu,Fengyuan Ran,Feng Zhou*

Main category: cs.LG

TL;DR: TPP-SD 通过借鉴语言模型的推测解码技术，加速了 Transformer 时间点过程采样，速度提升 2-6 倍，且不改变输出分布。


<details>
  <summary>Details</summary>
Motivation: 为了解决 Transformer 时间点过程（TPP）采样效率低下的问题，并满足实际应用中快速序列采样的需求。

Method: TPP-SD 利用了细粒度算法（tpp）和推测解码（sd）之间的结构相似性，开发了一个高效的采样框架。该框架使用一个较小的草稿模型生成多个候选事件，然后由一个较大的目标模型并行验证。

Result: TPP-SD 能够保持与自回归采样相同的输出分布，同时实现显著的加速，实验表明其速度提升了 2-6 倍。

Conclusion: TPP-SD 是一种创新的方法，通过采用来自语言模型的推测解码（SD）技术，显著加速了 Transformer 时间点过程（TPP）的采样。

Abstract: We propose TPP-SD, a novel approach that accelerates Transformer temporal
point process (TPP) sampling by adapting speculative decoding (SD) techniques
from language models. By identifying the structural similarities between
thinning algorithms for TPPs and speculative decoding for language models, we
develop an efficient sampling framework that leverages a smaller draft model to
generate multiple candidate events, which are then verified by the larger
target model in parallel. TPP-SD maintains the same output distribution as
autoregressive sampling while achieving significant acceleration. Experiments
on both synthetic and real datasets demonstrate that our approach produces
samples from identical distributions as standard methods, but with 2-6$\times$
speedup. Our ablation studies analyze the impact of hyperparameters such as
draft length and draft model size on sampling efficiency. TPP-SD bridges the
gap between powerful Transformer TPP models and the practical need for rapid
sequence sampling.

</details>


### [374] [Controllable Patching for Compute-Adaptive Surrogate Modeling of Partial Differential Equations](https://arxiv.org/abs/2507.09264)
*Payel Mukhopadhyay,Michael McCabe,Ruben Ohana,Miles Cranmer*

Main category: cs.LG

TL;DR: 该研究通过引入CKM和CSM模块，实现了Transformer PDE代理模型在推理时动态调整补丁大小，解决了固定补丁大小的限制，提高了模型效率和稳定性，且不损失准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的代理模型在模拟时空动力学方面虽然有效，但其固定的补丁大小限制了其在生产环境中具有成本效益的部署。因此，需要一种能够动态调整补丁大小的方法，以适应不同的计算预算，并提高模型的性能和效率。

Method: 提出了一种基于Transformer的PDE代理模型，并引入了卷积核调制器（CKM）和卷积步长调制器（CSM）两个轻量级、与架构无关的模块，实现了在推理时动态控制补丁大小。结合循环补丁大小展开，该方法可以减少补丁伪影，提高视频类预测任务的长期稳定性。

Result: 该方法在各种具有挑战性的2D和3D PDE基准测试中表现出色，提高了回滚保真度和运行时效率。即使在推理时动态调整补丁大小，也没有出现准确性损失。

Conclusion: 该研究首次提出了一个可插拔的框架，实现了基于Transformer的PDE代理模型中推理时可调的补丁大小。该框架通过卷积核调制器（CKM）和卷积步长调制器（CSM）实现，能够在不进行再训练或损失准确性的情况下，动态控制补丁大小，并结合循环补丁大小展开，解决了固定补丁大小的局限性，减少了补丁伪影，提高了长期稳定性，同时改善了模型的回滚保真度和运行时效率。

Abstract: Patch-based transformer surrogates have become increasingly effective for
modeling spatiotemporal dynamics, but the fixed patch size is a major
limitation for budget-conscience deployment in production. We introduce two
lightweight, architecture-agnostic modules-the Convolutional Kernel Modulator
(CKM) and Convolutional Stride Modulator (CSM)-that enable dynamic patch size
control at inference in patch based models, without retraining or accuracy
loss. Combined with a cyclic patch-size rollout, our method mitigates patch
artifacts and improves long-term stability for video-like prediction tasks.
Applied to a range of challenging 2D and 3D PDE benchmarks, our approach
improves rollout fidelity and runtime efficiency. To our knowledge, this is the
first framework to enable inference-time patch-size tunability in patch-based
PDE surrogates. Its plug-and-play design makes it broadly applicable across
architectures-establishing a general foundation for compute-adaptive modeling
in PDE surrogate tasks.

</details>


### [375] [MTF-Grasp: A Multi-tier Federated Learning Approach for Robotic Grasping](https://arxiv.org/abs/2507.10158)
*Obaidullah Zaland,Erik Elmroth,Monowar Bhuyan*

Main category: cs.LG

TL;DR: 本研究提出 MTF-Grasp，一种解决机器人抓取任务中联邦学习数据非独立同分布（non-IID）和数据量少问题的多层联邦学习方法，通过选择优质数据源训练模型并分发，在抓取数据集上性能提升高达 8%。


<details>
  <summary>Details</summary>
Motivation: 为了解决联邦学习（FL）在机器人抓取任务中，数据非独立同分布（non-IID）和数据量少导致的性能下降问题，并填补该领域研究的空白。

Method: 提出了一种名为 MTF-Grasp 的多层联邦学习方法，通过选择数据分布更好、样本量更大的“顶层”机器人来训练初始模型，然后将模型分发给“底层”机器人，以应对数据非独立同分布（non-IID）和数据量少的挑战。

Result: MTF-Grasp 方法在数据量倾斜的 Cornell 和 Jacquard 抓取数据集上，性能相比传统联邦学习方法提升了高达 8%。

Conclusion: MTF-Grasp 方法在抓取任务上相比传统联邦学习方法在数据量倾斜的 Cornell 和 Jacquard 抓取数据集上，性能提升高达 8%，有效地解决了机器人抓取任务中数据非独立同分布（non-IID）和数据量少的问题。

Abstract: Federated Learning (FL) is a promising machine learning paradigm that enables
participating devices to train privacy-preserved and collaborative models. FL
has proven its benefits for robotic manipulation tasks. However, grasping tasks
lack exploration in such settings where robots train a global model without
moving data and ensuring data privacy. The main challenge is that each robot
learns from data that is nonindependent and identically distributed (non-IID)
and of low quantity. This exhibits performance degradation, particularly in
robotic grasping. Thus, in this work, we propose MTF-Grasp, a multi-tier FL
approach for robotic grasping, acknowledging the unique challenges posed by the
non-IID data distribution across robots, including quantitative skewness.
MTF-Grasp harnesses data quality and quantity across robots to select a set of
"top-level" robots with better data distribution and higher sample count. It
then utilizes top-level robots to train initial seed models and distribute them
to the remaining "low-level" robots, reducing the risk of model performance
degradation in low-level robots. Our approach outperforms the conventional FL
setup by up to 8% on the quantity-skewed Cornell and Jacquard grasping
datasets.

</details>


### [376] [Impute With Confidence: A Framework for Uncertainty Aware Multivariate Time Series Imputation](https://arxiv.org/abs/2507.09353)
*Addison Weatherhead,Anna Goldenberg*

Main category: cs.LG

TL;DR: 通过量化和利用不确定性，对时间序列数据进行选择性填充，可以提高填充精度和下游任务的性能，尤其是在医疗保健数据中。


<details>
  <summary>Details</summary>
Motivation: 解决现有时间序列填充方法忽略模型不确定性或缺乏估计机制的不足，尤其是在医疗保健领域，传感器断开连接导致数据缺失且对填充值置信度的需求很高。

Method: 提出一个通用的框架，对不确定性进行量化和利用，以实现选择性填充，优先填充模型置信度高的值。

Result: 在多个 EHR 数据集上的实验表明，选择性填充不确定性较低的值可以减少填充错误，并提高包括24小时死亡率预测在内的下游任务的性能。

Conclusion: 通过在时间序列填充中引入不确定性量化，研究证明了其在减少填充错误和提高下游任务性能方面的有效性，特别是在电子健康记录（EHR）数据和医疗保健领域。

Abstract: Time series data with missing values is common across many domains.
Healthcare presents special challenges due to prolonged periods of sensor
disconnection. In such cases, having a confidence measure for imputed values is
critical. Most existing methods either overlook model uncertainty or lack
mechanisms to estimate it. To address this gap, we introduce a general
framework that quantifies and leverages uncertainty for selective imputation.
By focusing on values the model is most confident in, highly unreliable
imputations are avoided. Our experiments on multiple EHR datasets, covering
diverse types of missingness, demonstrate that selectively imputing
less-uncertain values not only reduces imputation errors but also improves
downstream tasks. Specifically, we show performance gains in a 24-hour
mortality prediction task, underscoring the practical benefit of incorporating
uncertainty into time series imputation.

</details>


### [377] [Meta-autoencoders: An approach to discovery and representation of relationships between dynamically evolving classes](https://arxiv.org/abs/2507.09362)
*Assaf Marron,Smadar Szekely,Irun Cohen,David Harel*

Main category: cs.LG

TL;DR: Introduces meta-autoencoders (MAEs), which are autoencoders for autoencoders, useful for modeling evolving systems like species.


<details>
  <summary>Details</summary>
Motivation: The motivation is to learn a compact representation and associated encoder and decoder for a collection of autoencoders, which can be applied to areas like natural evolution modeling to capture distinguishing properties across evolving species.

Method: The paper provides a constructive definition of MAEs and initial examples.

Result: Initial examples and motivating research directions in machine learning and biology are provided.

Conclusion: This paper introduces the concept of a meta-autoencoder (MAE), which is an AE for a collection of autoencoders. MAEs can learn a compact representation and associated encoder and decoder for class-specific AEs, with potential applications in machine learning and biology, such as modeling natural evolution.

Abstract: An autoencoder (AE) is a neural network that, using self-supervised training,
learns a succinct parameterized representation, and a corresponding encoding
and decoding process, for all instances in a given class. Here, we introduce
the concept of a meta-autoencoder (MAE): an AE for a collection of
autoencoders. Given a family of classes that differ from each other by the
values of some parameters, and a trained AE for each class, an MAE for the
family is a neural net that has learned a compact representation and associated
encoder and decoder for the class-specific AEs. One application of this general
concept is in research and modeling of natural evolution -- capturing the
defining and the distinguishing properties across multiple species that are
dynamically evolving from each other and from common ancestors. In this interim
report we provide a constructive definition of MAEs, initial examples, and the
motivating research directions in machine learning and biology.

</details>


### [378] [Fair CCA for Fair Representation Learning: An ADNI Study](https://arxiv.org/abs/2507.09382)
*Bojian Hou,Zhanliang Wang,Zhuoping Zhou,Boning Tong,Zexuan Wang,Jingxuan Bao,Duy Duong-Tran,Qi Long,Li Shen*

Main category: cs.LG

TL;DR: 提出了一种新的公平CCA方法，提高了公平性且不损害准确性，适用于下游分类任务。


<details>
  <summary>Details</summary>
Motivation: 之前的公平CCA方法往往忽略了对下游分类任务的影响，限制了其应用。本研究旨在提出一种能在提高公平性的同时不损害准确性的公平CCA方法。

Method: 提出了一种新颖的公平CCA方法，确保投影后的特征独立于敏感属性。

Result: 在合成数据和ADNI真实世界数据上进行了验证，证明了该方法在保持高相关性分析性能的同时，能够提高分类任务的公平性。

Conclusion: 所提出的公平CCA方法在保持高相关性分析性能的同时，提高了分类任务的公平性，使其在神经影像研究等需要无偏分析的领域具有应用潜力。

Abstract: Canonical correlation analysis (CCA) is a technique for finding correlations
between different data modalities and learning low-dimensional representations.
As fairness becomes crucial in machine learning, fair CCA has gained attention.
However, previous approaches often overlook the impact on downstream
classification tasks, limiting applicability. We propose a novel fair CCA
method for fair representation learning, ensuring the projected features are
independent of sensitive attributes, thus enhancing fairness without
compromising accuracy. We validate our method on synthetic data and real-world
data from the Alzheimer's Disease Neuroimaging Initiative (ADNI), demonstrating
its ability to maintain high correlation analysis performance while improving
fairness in classification tasks. Our work enables fair machine learning in
neuroimaging studies where unbiased analysis is essential.

</details>


### [379] [Geometric Generative Modeling with Noise-Conditioned Graph Networks](https://arxiv.org/abs/2507.09391)
*Peter Pao-Huang,Mitchell Black,Xiaojie Qiu*

Main category: cs.LG

TL;DR: Generative graph models with spatial structure benefit from noise-adaptive neural networks. Introduced Noise-Conditioned Graph Networks (NCGNs) and a specific version, Dynamic Message Passing (DMP), which adjusts message passing range and resolution based on noise, outperforming existing methods on various data types.


<details>
  <summary>Details</summary>
Motivation: Existing flow-based generative models for graphs with spatial structure use graph neural networks that are independent of the noise level, limiting their expressiveness. Addressed this by developing NCGNs that dynamically adapt to noise levels.

Method: Introduced Noise-Conditioned Graph Networks (NCGNs), a class of graph neural networks that dynamically modify their architecture according to the noise level during generation. Developed Dynamic Message Passing (DMP) as a specific instantiation of NCGNs, adapting both the range and resolution of message passing to the noise level based on theoretical and empirical analysis.

Result: Empirical analysis shows that as noise increases, graphs require information from increasingly distant neighbors and can be effectively represented at lower resolutions. DMP consistently outperforms noise-independent architectures on 3D point clouds, spatiotemporal transcriptomics, and images.

Conclusion: NCGNs, particularly the Dynamic Message Passing (DMP) instantiation, outperform noise-independent architectures on various domains including 3D point clouds, spatiotemporal transcriptomics, and images by dynamically adapting the range and resolution of message passing to the noise level.

Abstract: Generative modeling of graphs with spatial structure is essential across many
applications from computer graphics to spatial genomics. Recent flow-based
generative models have achieved impressive results by gradually adding and then
learning to remove noise from these graphs. Existing models, however, use graph
neural network architectures that are independent of the noise level, limiting
their expressiveness. To address this issue, we introduce
\textit{Noise-Conditioned Graph Networks} (NCGNs), a class of graph neural
networks that dynamically modify their architecture according to the noise
level during generation. Our theoretical and empirical analysis reveals that as
noise increases, (1) graphs require information from increasingly distant
neighbors and (2) graphs can be effectively represented at lower resolutions.
Based on these insights, we develop Dynamic Message Passing (DMP), a specific
instantiation of NCGNs that adapts both the range and resolution of message
passing to the noise level. DMP consistently outperforms noise-independent
architectures on a variety of domains including $3$D point clouds,
spatiotemporal transcriptomics, and images. Code is available at
https://github.com/peterpaohuang/ncgn.

</details>


### [380] [A Random Matrix Theory Perspective on the Learning Dynamics of Multi-head Latent Attention](https://arxiv.org/abs/2507.09394)
*Nandan Kumar Jha,Brandon Reagen*

Main category: cs.LG

TL;DR: 多头潜在注意力（MLA）的容量瓶颈可以通过在头之间共享旋转组件来缓解，从而保持模型的表示能力。


<details>
  <summary>Details</summary>
Motivation: 研究多头潜在注意力（MLA）这一压缩键/值内存的常用策略如何影响Transformer在预训练期间的内部容量。

Method: 使用轻量级的Marchenko-Pastur（MP）诊断，分析了训练过程中$W_{Q}W_{K}^	op$그램矩阵的谱，并比较了三种变体：标准的MHA基线、MLA-PreRoPE（在压缩前应用旋转）以及MLA-Decoupled（在所有头之间共享单个旋转子向量）。

Result: 随机矩阵分析揭示了三个关键发现：1）容量瓶颈是局部出现的，MHA和MLA-PreRoPE在特定层中表现出尖锐、早期的峰值，这些峰值会持续存在并传播，破坏了整体方向和异常值方向之间的平衡；2）这些峰值与秩崩溃相吻合，将模型的表现力集中在狭窄的子空间中；3）只有解耦的变体才能阻止这种级联效应，在各层中保持广泛的光谱支持并抑制异常值的形成。

Conclusion: 与如何在压缩中应用旋转嵌入相比，在哪里应用旋转嵌入更为关键。在头之间共享旋转组件可以减轻光谱碎片化并保留表示能力。

Abstract: In this work, we study how multi-head latent attention (MLA), a popular
strategy for compressing key/value memory, affects a transformer's internal
capacity during pretraining. Using a lightweight suite of Marchenko-Pastur (MP)
diagnostics, we analyze the spectrum of the $W_{Q}W_{K}^\top$ gram matrix
throughout training, comparing three variants: the standard multi-head
attention (MHA) baseline, MLA-PreRoPE with rotary applied before compression,
and MLA-Decoupled, which shares a single rotary sub-vector across all heads.
Our random matrix analysis reveals \textbf{three key findings:} \textbf{ i)}
capacity bottlenecks emerge locally: both MHA and MLA-PreRoPE exhibit sharp,
early spikes in specific layers that persist and propagate, disrupting the
balance between bulk and outlier directions; \textbf{ ii)} these spikes
coincide with rank collapse, concentrating the model's expressivity into narrow
subspaces; \textbf{ iii)} only the decoupled variant prevents this cascade,
maintaining broad spectral support and suppressing outlier formation across
layers. These results underscore that \emph{how} rotary embeddings are applied
is just as critical as \emph{where} compression occurs. Sharing rotary
components across heads mitigates spectral fragmentation and preserves
representational capacity.

</details>


### [381] [Scaling Laws for Optimal Data Mixtures](https://arxiv.org/abs/2507.09404)
*Mustafa Shukor,Louis Bethune,Dan Busbridge,David Grangier,Enrico Fini,Alaaeldin El-Nouby,Pierre Ablin*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Large foundation models are typically trained on data from multiple domains,
with the data mixture--the proportion of each domain used--playing a critical
role in model performance. The standard approach to selecting this mixture
relies on trial and error, which becomes impractical for large-scale
pretraining. We propose a systematic method to determine the optimal data
mixture for any target domain using scaling laws. Our approach accurately
predicts the loss of a model of size $N$ trained with $D$ tokens and a specific
domain weight vector $h$. We validate the universality of these scaling laws by
demonstrating their predictive power in three distinct and large-scale
settings: large language model (LLM), native multimodal model (NMM), and large
vision models (LVM) pretraining. We further show that these scaling laws can
extrapolate to new data mixtures and across scales: their parameters can be
accurately estimated using a few small-scale training runs, and used to
estimate the performance at larger scales and unseen domain weights. The
scaling laws allow to derive the optimal domain weights for any target domain
under a given training budget ($N$,$D$), providing a principled alternative to
costly trial-and-error methods.

</details>


### [382] [Adversarial Activation Patching: A Framework for Detecting and Mitigating Emergent Deception in Safety-Aligned Transformers](https://arxiv.org/abs/2507.09406)
*Santhosh Kumar Ravindran*

Main category: cs.LG

TL;DR: 本研究引入了一种名为“对抗性激活修复”的新型机制可解释性框架，该框架利用激活修复技术作为对抗性工具来诱导、检测和减轻变换器模型中的欺骗行为。通过从“欺骗性”提示中提取激活并在特定层将其修复到安全的正向传播中，研究人员能够模拟漏洞并量化欺骗率。在玩具神经网络模拟中，研究发现此方法能将欺骗性输出的比例从0%提升至23.9%，并揭示了特定层在其中起到的作用。此外，研究还提出了关于模型迁移性、多模态环境影响和规模效应的假设，并探讨了激活异常检测和鲁棒微调等缓解策略，为人工智能安全领域的研究和实践提供了宝贵的见解。


<details>
  <summary>Details</summary>
Motivation: 为了解决经过人类反馈强化学习（RLHF）对齐的大型语言模型（LLM）中出现的潜在欺骗行为问题。

Method: 通过激活修复技术，将来自“欺骗性”提示的激活映射到安全的正向传播过程中，以模拟和量化模型的欺骗行为。

Result: 在玩具神经网络模拟中，对抗性修复将欺骗性输出的比例从0%提高到23.9%，并验证了特定层可能存在的欺骗行为。

Conclusion: 本研究提出了对抗性激活修复框架，用于检测和减轻大型语言模型中的欺骗行为，并为相关领域的研究提供了方向。

Abstract: Large language models (LLMs) aligned for safety through techniques like
reinforcement learning from human feedback (RLHF) often exhibit emergent
deceptive behaviors, where outputs appear compliant but subtly mislead or omit
critical information. This paper introduces adversarial activation patching, a
novel mechanistic interpretability framework that leverages activation patching
as an adversarial tool to induce, detect, and mitigate such deception in
transformer-based models. By sourcing activations from "deceptive" prompts and
patching them into safe forward passes at specific layers, we simulate
vulnerabilities and quantify deception rates. Through toy neural network
simulations across multiple scenarios (e.g., 1000 trials per setup), we
demonstrate that adversarial patching increases deceptive outputs to 23.9% from
a 0% baseline, with layer-specific variations supporting our hypotheses. We
propose six hypotheses, including transferability across models, exacerbation
in multimodal settings, and scaling effects. An expanded literature review
synthesizes over 20 key works in interpretability, deception, and adversarial
attacks. Mitigation strategies, such as activation anomaly detection and robust
fine-tuning, are detailed, alongside ethical considerations and future research
directions. This work advances AI safety by highlighting patching's dual-use
potential and provides a roadmap for empirical studies on large-scale models.

</details>


### [383] [On Information Geometry and Iterative Optimization in Model Compression: Operator Factorization](https://arxiv.org/abs/2507.09428)
*Zakhar Shumaylov,Vasileios Tsiaras,Yannis Stylianou*

Main category: cs.LG

TL;DR: 本文利用信息几何分析模型压缩，提出将其视为在参数空间中定义最优低计算量子流形并进行投影。研究发现，在压缩预训练模型时，信息散度有助于提高零样本准确率；而在模型微调时，可训练性比信息散度更重要。通过迭代奇异值阈值法和软秩约减等方法，可以在保证性能的同时实现更高的压缩率。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型日益增长的参数量，使得在资源受限设备上部署模型需要有效的压缩技术。本文旨在利用信息几何的视角，分析现有的模型压缩方法，特别是算子分解。

Method: 本文探索了信息几何在模型压缩中的应用，重点关注算子分解。我们提出将模型压缩视为在参数空间中定义最优低计算量子流形并进行投影的问题。此外，我们还证明了迭代奇异值阈值法在软秩约束下训练神经网络的收敛性。

Result: 信息几何的视角揭示了模型压缩的核心挑战在于定义最优低计算量子流形并进行投影。许多成功的模型压缩方法可以被理解为隐式地逼近信息散度以实现这种投影。我们发现，在压缩预训练模型时，信息散度对于提高零样本准确率至关重要；然而，在模型微调的情况下，可训练性变得更为重要。

Conclusion: 当模型在压缩后进行微调时，信息散度对于提高零样本准确率至关重要，但此时可训练性比信息散度更重要，需要采用迭代方法。我们证明了迭代奇异值阈值法在软秩约束下训练神经网络的收敛性。通过对现有方法进行简单的修改，例如更温和的秩约减，可以在固定的压缩率下获得更好的性能。

Abstract: The ever-increasing parameter counts of deep learning models necessitate
effective compression techniques for deployment on resource-constrained
devices. This paper explores the application of information geometry, the study
of density-induced metrics on parameter spaces, to analyze existing methods
within the space of model compression, primarily focusing on operator
factorization. Adopting this perspective highlights the core challenge:
defining an optimal low-compute submanifold (or subset) and projecting onto it.
We argue that many successful model compression approaches can be understood as
implicitly approximating information divergences for this projection. We
highlight that when compressing a pre-trained model, using information
divergences is paramount for achieving improved zero-shot accuracy, yet this
may no longer be the case when the model is fine-tuned. In such scenarios,
trainability of bottlenecked models turns out to be far more important for
achieving high compression ratios with minimal performance degradation,
necessitating adoption of iterative methods. In this context, we prove
convergence of iterative singular value thresholding for training neural
networks subject to a soft rank constraint. To further illustrate the utility
of this perspective, we showcase how simple modifications to existing methods
through softer rank reduction result in improved performance under fixed
compression rates.

</details>


### [384] [Dynamic Sparse Causal-Attention Temporal Networks for Interpretable Causality Discovery in Multivariate Time Series](https://arxiv.org/abs/2507.09439)
*Meriem Zerkouk,Miloud Mihoubi,Belkacem Chikhaoui*

Main category: cs.LG

TL;DR: DyCAST-Net是一种新颖的架构，通过稀释时间卷积和动态稀疏注意力机制来发现MTS中的因果关系，它比现有模型更准确、更具可解释性，尤其是在嘈杂和高维的环境中。


<details>
  <summary>Details</summary>
Motivation: 在金融和营销等领域，理解多元时间序列（MTS）中的因果关系对于有效的决策至关重要，因为复杂的依赖关系和滞后效应对传统的分析方法提出了挑战。

Method: DyCAST-Net通过集成稀释时间卷积和动态稀疏注意力机制来增强因果发现，通过稀释卷积有效地捕获多尺度时间依赖性，并通过注意力机制中的自适应阈值策略消除虚假连接，从而确保准确性和可解释性。统计随机测试验证通过过滤假阳性和提高因果推断的可靠性，进一步加强了鲁棒性。RMSNorm稳定和因果掩码增强了模型架构，确保了跨不同应用域的可扩展性和适应性。

Result: DyCAST-Net在金融和营销数据集上进行了广泛评估，结果显示其在估计因果延迟和减少错误发现方面优于现有模型。注意力热图提供了可解释的见解，揭示了隐藏的因果模式。

Conclusion: DyCAST-Net在金融和营销数据集上的广泛评估表明，它在估计因果延迟和减少错误发现（尤其是在嘈杂的环境中）方面持续优于TCDF、GCFormer和CausalFormer等现有模型。其注意力热图提供了可解释的见解，揭示了隐藏的因果模式，例如广告对消费者行为的介导效应以及宏观经济指标对金融市场的影响。案例研究表明，DyCAST-Net能够检测潜在的中介因素和滞后因果因素，在高维、动态环境中特别有效。

Abstract: Understanding causal relationships in multivariate time series (MTS) is
essential for effective decision-making in fields such as finance and
marketing, where complex dependencies and lagged effects challenge conventional
analytical approaches. We introduce Dynamic Sparse Causal-Attention Temporal
Networks for Interpretable Causality Discovery in MTS (DyCAST-Net), a novel
architecture designed to enhance causal discovery by integrating dilated
temporal convolutions and dynamic sparse attention mechanisms. DyCAST-Net
effectively captures multiscale temporal dependencies through dilated
convolutions while leveraging an adaptive thresholding strategy in its
attention mechanism to eliminate spurious connections, ensuring both accuracy
and interpretability. A statistical shuffle test validation further strengthens
robustness by filtering false positives and improving causal inference
reliability. Extensive evaluations on financial and marketing datasets
demonstrate that DyCAST-Net consistently outperforms existing models such as
TCDF, GCFormer, and CausalFormer. The model provides a more precise estimation
of causal delays and significantly reduces false discoveries, particularly in
noisy environments. Moreover, attention heatmaps offer interpretable insights,
uncovering hidden causal patterns such as the mediated effects of advertising
on consumer behavior and the influence of macroeconomic indicators on financial
markets. Case studies illustrate DyCAST-Net's ability to detect latent
mediators and lagged causal factors, making it particularly effective in
high-dimensional, dynamic settings. The model's architecture enhanced by
RMSNorm stabilization and causal masking ensures scalability and adaptability
across diverse application domains

</details>


### [385] [Transformers Don't In-Context Learn Least Squares Regression](https://arxiv.org/abs/2507.09440)
*Joshua Hill,Benjamin Eyre,Elliot Creager*

Main category: cs.LG

TL;DR: ICL in transformers is mysterious. This paper shows transformers fail at ICL when prompts change, unlike OLS. Pretraining data matters: good prompts have a special 'spectral signature' linked to low error.


<details>
  <summary>Details</summary>
Motivation: To understand the mysterious mechanisms underlying in-context learning (ICL) in large pretrained transformers, despite their practical success.

Method: The study probes the mechanisms of ICL in transformers using synthetic linear regression and a suite of out-of-distribution generalization experiments. It also performs spectral analysis of learned representations in the residual stream.

Result: Out-of-distribution generalization experiments show that transformers trained for ICL fail to generalize after shifts in the prompt distribution. Spectral analysis reveals a unique spectral signature (shared top two singular vectors) for inputs from the training distribution, which is absent in out-of-distribution inputs and correlated with low loss.

Conclusion: Transformers trained for in-context learning (ICL) fail to generalize to out-of-distribution prompts, contradicting the idea that they implement algorithms like Ordinary Least Squares (OLS). The pretraining corpus plays a crucial role, as inputs from the training distribution exhibit a distinct spectral signature in the residual stream, which is correlated with low loss.

Abstract: In-context learning (ICL) has emerged as a powerful capability of large
pretrained transformers, enabling them to solve new tasks implicit in example
input-output pairs without any gradient updates. Despite its practical success,
the mechanisms underlying ICL remain largely mysterious. In this work we study
synthetic linear regression to probe how transformers implement learning at
inference time. Previous works have demonstrated that transformers match the
performance of learning rules such as Ordinary Least Squares (OLS) regression
or gradient descent and have suggested ICL is facilitated in transformers
through the learned implementation of one of these techniques. In this work, we
demonstrate through a suite of out-of-distribution generalization experiments
that transformers trained for ICL fail to generalize after shifts in the prompt
distribution, a behaviour that is inconsistent with the notion of transformers
implementing algorithms such as OLS. Finally, we highlight the role of the
pretraining corpus in shaping ICL behaviour through a spectral analysis of the
learned representations in the residual stream. Inputs from the same
distribution as the training data produce representations with a unique
spectral signature: inputs from this distribution tend to have the same top two
singular vectors. This spectral signature is not shared by out-of-distribution
inputs, and a metric characterizing the presence of this signature is highly
correlated with low loss.

</details>


### [386] [Toward Developing Machine-Learning-Aided Tools for the Thermomechanical Monitoring of Nuclear Reactor Components](https://arxiv.org/abs/2507.09443)
*Luiz Aldeia Machado,Victor Coppo Leite,Elia Merzari,Arthur Motta,Roberto Ponciroli,Lander Ibarra,Lise Charlot*

Main category: cs.LG

TL;DR: 通过CNN和热力学模型，可以精确预测核燃料棒的温度、应力和应变，有助于核反应堆的预测性维护。


<details>
  <summary>Details</summary>
Motivation: 为了提高核电站的运行效率，减少因组件故障导致的意外停机时间，研究预测性维护（PdM）策略，特别是针对核反应堆的运行。

Method: 利用卷积神经网络（CNN）架构结合计算热力学模型，基于有限数量的包覆管外表面温度测量值，来估算压水堆燃料棒在运行期间的温度、应力和应变分布。训练、验证和测试数据集是通过耦合BISON（一种基于有限元法的核燃料性能代码）和MOOSE热工水力模块（MOOSE-THM）的模拟生成的。

Result: 卷积神经网络（CNN）在超过1000个训练周期后，未出现过拟合现象，并实现了高度精确的温度分布预测。这些预测随后被用于热力学模型中，以确定燃料棒内部的应力和应变分布。

Conclusion: 该方法通过结合卷积神经网络和计算热力学模型，可以实现对压水堆核燃料棒在运行期间的温度、应力和应变分布的精确预测，为开发核反应堆的预测性维护工具提供了新的途径。

Abstract: Proactive maintenance strategies, such as Predictive Maintenance (PdM), play
an important role in the operation of Nuclear Power Plants (NPPs), particularly
due to their capacity to reduce offline time by preventing unexpected shutdowns
caused by component failures.
  In this work, we explore the use of a Convolutional Neural Network (CNN)
architecture combined with a computational thermomechanical model to calculate
the temperature, stress, and strain of a Pressurized Water Reactor (PWR) fuel
rod during operation. This estimation relies on a limited number of temperature
measurements from the cladding's outer surface. This methodology can
potentially aid in developing PdM tools for nuclear reactors by enabling
real-time monitoring of such systems.
  The training, validation, and testing datasets were generated through coupled
simulations involving BISON, a finite element-based nuclear fuel performance
code, and the MOOSE Thermal-Hydraulics Module (MOOSE-THM). We conducted eleven
simulations, varying the peak linear heat generation rates. Of these, eight
were used for training, two for validation, and one for testing.
  The CNN was trained for over 1,000 epochs without signs of overfitting,
achieving highly accurate temperature distribution predictions. These were then
used in a thermomechanical model to determine the stress and strain
distribution within the fuel rod.

</details>


### [387] [Fourier Basis Mapping: A Time-Frequency Learning Framework for Time Series Forecasting](https://arxiv.org/abs/2507.09445)
*Runze Yang,Longbing Cao,Xin You,Kun Fang,Jianxun Li,Jie Yang*

Main category: cs.LG

TL;DR: 提出了一种新颖的傅里叶基映射（FBM）方法，通过整合时频特征来解决现有傅里叶方法的局限性，并在各种时间序列预测任务中取得了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于傅里叶的方法面临不一致的起始周期和不一致的序列长度问题，它们未能精确解释频率分量并忽略了时间信息。

Method: 提出了一种新颖的傅里叶基映射（FBM）方法，通过傅里叶基展开和时频空间映射来整合时频特征。FBM通过调整第一个初始投影层即可与各种类型的神经网络即插即用地集成。具体来说，提出了FBM-L、FBM-NL和FBM-NP来增强线性、基于MLP和基于Transformer的模型。此外，还提出了一个协同模型架构FBM-S，将季节性、趋势和交互效应分解为三个单独的块，每个块都以专业的方式对时频特征进行建模。最后，引入了包括交互掩码、中心化、打补丁、滚动窗口投影和多尺度下采样在内的几种针对时频特征的技术。

Result: FBM提取显性频率特征，同时保留时间特征。FBM-L、FBM-NL和FBM-NP模型证明了时频特征的有效性。

Conclusion: FBM在长期和短期预测任务的各种真实世界数据集上通过SOTA性能得到验证。

Abstract: The integration of Fourier transform and deep learning opens new avenues for
time series forecasting. We reconsider the Fourier transform from a basis
functions perspective. Specifically, the real and imaginary parts of the
frequency components can be regarded as the coefficients of cosine and sine
basis functions at tiered frequency levels, respectively. We find that existing
Fourier-based methods face inconsistent starting cycles and inconsistent series
length issues. They fail to interpret frequency components precisely and
overlook temporal information. Accordingly, the novel Fourier Basis Mapping
(FBM) method addresses these issues by integrating time-frequency features
through Fourier basis expansion and mapping in the time-frequency space. Our
approach extracts explicit frequency features while preserving temporal
characteristics. FBM supports plug-and-play integration with various types of
neural networks by only adjusting the first initial projection layer for better
performance. First, we propose FBM-L, FBM-NL, and FBM-NP to enhance linear,
MLP-based, and Transformer-based models, respectively, demonstrating the
effectiveness of time-frequency features. Next, we propose a synergetic model
architecture, termed FBM-S, which decomposes the seasonal, trend, and
interaction effects into three separate blocks, each designed to model
time-frequency features in a specialized manner. Finally, we introduce several
techniques tailored for time-frequency features, including interaction masking,
centralization, patching, rolling window projection, and multi-scale
down-sampling. The results are validated on diverse real-world datasets for
both long-term and short-term forecasting tasks with SOTA performance.

</details>


### [388] [Enhancing ALS Progression Tracking with Semi-Supervised ALSFRS-R Scores Estimated from Ambient Home Health Monitoring](https://arxiv.org/abs/2507.09460)
*Noah Marchal,William E. Janes,Mihail Popescu,Xing Song*

Main category: cs.LG

TL;DR: 本研究通过半监督学习和迁移学习模型，利用连续居家传感器数据预测 ALS 患者的功能衰退。研究发现，不同的功能域（如呼吸、言语、吞咽、穿衣）具有不同的同质性-异质性特征，需要匹配不同的学习和插值技术以提高预测准确性。最终，研究提出了在传感器监控平台中集成自适应模型选择的建议，以实现及时的干预和可扩展的部署。


<details>
  <summary>Details</summary>
Motivation: 为了解决临床监测 ALS 功能衰退可能错过关键变化的问题，本研究旨在开发新的监测方法。

Method: 本研究开发了半监督回归模型，通过针对 ALSFRS-R 量表轨迹和连续居家传感器监控数据来估计病例系列队列中的衰退率。研究人员比较了三种模型范式（个体批量学习和队列级批量与增量微调迁移学习）在四种回归模型（线性斜率、三次多项式和集成自注意力伪标签插值）上的表现。

Result: 研究结果显示，在功能域方面，队列具有同质性，能够响应学习方法。迁移学习在 28/32 的对比中提高了 ALSFRS-R 分量表的预测误差（平均 RMSE=0.20(0.04)），而个体批量学习在 2/3 的对比中预测了复合量表（平均 RMSE=3.15(1.25)）。自注意力插值在分量级模型上实现了最低的预测误差（平均 RMSE=0.19(0.06)），能够捕捉复杂的非线性进展模式，在 20/32 的对比中优于线性和三次插值，但线性插值在所有 ALSFRS-R 复合量表模型上表现更稳定（平均 RMSE=0.23(0.10)）。研究发现呼吸和言语功能域表现出受益于个性化增量适应的患者特异性模式，而吞咽和穿衣功能域则遵循适合迁移模型的队列级轨迹。

Conclusion: 通过将学习和伪标签技术与特定功能域的同质性-异质性特征相匹配，可以提高 ALS 进展跟踪的预测准确性。在传感器监控平台中集成自适应模型选择可以实现及时的干预和未来多中心研究的可扩展部署。

Abstract: Clinical monitoring of functional decline in ALS relies on periodic
assessments that may miss critical changes occurring between visits. To address
this gap, semi-supervised regression models were developed to estimate rates of
decline in a case series cohort by targeting ALSFRS- R scale trajectories with
continuous in-home sensor monitoring data. Our analysis compared three model
paradigms (individual batch learning and cohort-level batch versus incremental
fine-tuned transfer learning) across linear slope, cubic polynomial, and
ensembled self-attention pseudo-label interpolations. Results revealed cohort
homogeneity across functional domains responding to learning methods, with
transfer learning improving prediction error for ALSFRS-R subscales in 28 of 32
contrasts (mean RMSE=0.20(0.04)), and individual batch learning for predicting
the composite scale (mean RMSE=3.15(1.25)) in 2 of 3. Self-attention
interpolation achieved the lowest prediction error for subscale-level models
(mean RMSE=0.19(0.06)), capturing complex nonlinear progression patterns,
outperforming linear and cubic interpolations in 20 of 32 contrasts, though
linear interpolation proved more stable in all ALSFRS-R composite scale models
(mean RMSE=0.23(0.10)). We identified distinct homogeneity-heterogeneity
profiles across functional domains with respiratory and speech exhibiting
patient-specific patterns benefiting from personalized incremental adaptation,
while swallowing and dressing functions followed cohort-level trajectories
suitable for transfer models. These findings suggest that matching learning and
pseudo-labeling techniques to functional domain-specific
homogeneity-heterogeneity profiles enhances predictive accuracy in ALS
progression tracking. Integrating adaptive model selection within sensor
monitoring platforms could enable timely interventions and scalable deployment
in future multi-center studies.

</details>


### [389] [La-Proteina: Atomistic Protein Generation via Partially Latent Flow Matching](https://arxiv.org/abs/2507.09466)
*Tomas Geffner,Kieran Didi,Zhonglin Cao,Danny Reidenbach,Zuobai Zhang,Christian Dallago,Emine Kucukbenli,Karsten Kreis,Arash Vahdat*

Main category: cs.LG

TL;DR: La-Proteina是一种新的蛋白质设计模型，使用部分潜在表示和流匹配，能够同时生成蛋白质序列和全原子结构，并在多项基准测试中取得最先进的性能，尤其在处理长链蛋白质方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的从头蛋白质结构设计生成模型大多难以同时生成全原子结构和氨基酸序列，因为模型需要处理长度可变的侧链。

Method: La-Proteina使用一种新颖的部分潜在蛋白质表示方法，显式地对粗略的骨架结构进行建模，并通过每个残基的固定维度潜在变量来捕获序列和原子细节，从而有效避开了显式侧链表示的挑战。然后，在部分潜在空间中使用流匹配来模拟序列和全原子结构上的联合分布。

Result: La-Proteina在包括全原子共设计性、多样性和结构有效性在内的多项生成基准测试中取得了最先进的性能。它在原子图谱支架性能方面也优于先前模型，并能生成长达800个残基的蛋白质，而大多数基线模型在此规模下会失效。

Conclusion: La-Proteina在蛋白质设计领域取得了最先进的性能，在多项生成基准测试中均表现出色，包括全原子共设计性、多样性和结构有效性。此外，La-Proteina在原子图谱支架性能方面也超越了以往的模型，并能够生成长达800个残基的共设计蛋白质，展现了其可扩展性和鲁棒性。

Abstract: Recently, many generative models for de novo protein structure design have
emerged. Yet, only few tackle the difficult task of directly generating fully
atomistic structures jointly with the underlying amino acid sequence. This is
challenging, for instance, because the model must reason over side chains that
change in length during generation. We introduce La-Proteina for atomistic
protein design based on a novel partially latent protein representation: coarse
backbone structure is modeled explicitly, while sequence and atomistic details
are captured via per-residue latent variables of fixed dimensionality, thereby
effectively side-stepping challenges of explicit side-chain representations.
Flow matching in this partially latent space then models the joint distribution
over sequences and full-atom structures. La-Proteina achieves state-of-the-art
performance on multiple generation benchmarks, including all-atom
co-designability, diversity, and structural validity, as confirmed through
detailed structural analyses and evaluations. Notably, La-Proteina also
surpasses previous models in atomistic motif scaffolding performance, unlocking
critical atomistic structure-conditioned protein design tasks. Moreover,
La-Proteina is able to generate co-designable proteins of up to 800 residues, a
regime where most baselines collapse and fail to produce valid samples,
demonstrating La-Proteina's scalability and robustness.

</details>


### [390] [An Analysis of Action-Value Temporal-Difference Methods That Learn State Values](https://arxiv.org/abs/2507.09523)
*Brett Daley,Prabhat Nagarajan,Martha White,Marlos C. Machado*

Main category: cs.LG

TL;DR: TD 学习通常使用单个值函数，但此论文研究了使用两个不对称值函数的算法。研究发现 AV-learning 在控制方面比 Q-learning 有优势，并且新算法 RDQ 在 MinAtar 上表现优于 Dueling DQN。


<details>
  <summary>Details</summary>
Motivation: 研究 TD 控制方法中从两个不对称值函数引导的 QV-learning 和 AV-learning 算法，以确定其优势和理论基础。

Method: 分析了从两个不对称值函数进行引导的 TD 方法的收敛性和样本效率，并引入了一种新的 AV-learning 算法 RDQ。

Result: QV-learning 和 AV-learning 方法在预测设置中比 Expected Sarsa 更有效。AV-learning 方法在控制设置中比 Q-learning 具有优势。

Conclusion: AV-learning 方法在控制设置中比 Q-learning 提供了主要优势，并且 RDQ 在 MinAtar 基准测试中显著优于 Dueling DQN。

Abstract: The hallmark feature of temporal-difference (TD) learning is bootstrapping:
using value predictions to generate new value predictions. The vast majority of
TD methods for control learn a policy by bootstrapping from a single
action-value function (e.g., Q-learning and Sarsa). Significantly less
attention has been given to methods that bootstrap from two asymmetric value
functions: i.e., methods that learn state values as an intermediate step in
learning action values. Existing algorithms in this vein can be categorized as
either QV-learning or AV-learning. Though these algorithms have been
investigated to some degree in prior work, it remains unclear if and when it is
advantageous to learn two value functions instead of just one -- and whether
such approaches are theoretically sound in general. In this paper, we analyze
these algorithmic families in terms of convergence and sample efficiency. We
find that while both families are more efficient than Expected Sarsa in the
prediction setting, only AV-learning methods offer any major benefit over
Q-learning in the control setting. Finally, we introduce a new AV-learning
algorithm called Regularized Dueling Q-learning (RDQ), which significantly
outperforms Dueling DQN in the MinAtar benchmark.

</details>


### [391] [Assessing reliability of explanations in unbalanced datasets: a use-case on the occurrence of frost events](https://arxiv.org/abs/2507.09545)
*Ilaria Vascotto,Valentina Blasone,Alex Rodriguez,Alessandro Bonaita,Luca Bortolussi*

Main category: cs.LG

TL;DR: 本研究针对非平衡数据集中的XAI解释可靠性问题，提出了一种关注少数类别的评估方法，并给出了一个实际用例。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型部署的增加和相关法律法规的出现，解释的鲁棒性（即可靠性）已成为实际应用中XAI方法的一个基本但常被忽视的方面。特别是在高风险用例中常见的非平衡数据集的情况下，评估XAI方法的可靠性更具挑战性。

Method: 本研究提出了一种利用流形上邻居生成、解释聚合和解释一致性度量来评估少数类别解释可靠性的方法。

Result: 本研究提供了一个基于表格数据集（包含数值特征并关注霜冻事件的发生）的用例，以展示所提出的评估方法的有效性。

Conclusion: 本研究提出了一个针对非平衡数据集的XAI方法可靠性评估的初步见解，重点关注少数类别的解释一致性。

Abstract: The usage of eXplainable Artificial Intelligence (XAI) methods has become
essential in practical applications, given the increasing deployment of
Artificial Intelligence (AI) models and the legislative requirements put
forward in the latest years. A fundamental but often underestimated aspect of
the explanations is their robustness, a key property that should be satisfied
in order to trust the explanations. In this study, we provide some preliminary
insights on evaluating the reliability of explanations in the specific case of
unbalanced datasets, which are very frequent in high-risk use-cases, but at the
same time considerably challenging for both AI models and XAI methods. We
propose a simple evaluation focused on the minority class (i.e. the less
frequent one) that leverages on-manifold generation of neighbours, explanation
aggregation and a metric to test explanation consistency. We present a use-case
based on a tabular dataset with numerical features focusing on the occurrence
of frost events.

</details>


### [392] [Holistix: A Dataset for Holistic Wellness Dimensions Analysis in Mental Health Narratives](https://arxiv.org/abs/2507.09565)
*Heeba Shakeel,Tanvir Ahmad,Chandni Saxena*

Main category: cs.LG

TL;DR: 本研究提出了一个包含六个健康维度的社交媒体帖子分类数据集，并评估了传统和Transformer模型。结果表明该数据集有助于健康评估和早期干预策略。


<details>
  <summary>Details</summary>
Motivation: 介绍一个用于在社交媒体用户帖子中对健康维度进行分类的数据集，涵盖六个关键的健康方面：身体、情绪、社交、智力、精神和职业。

Method: 本研究提出了一个用于在社交媒体用户帖子中对健康维度进行分类的数据集，涵盖了六个关键方面：身体、情绪、社交、智力、精神和职业。该数据集旨在捕捉用户生成内容中的这些维度，并建立了一个在领域专家指导下开发的综合注释框架，该框架能够将文本跨度分类到适当的健康类别中。我们还评估了用于此多类分类任务的传统机器学习模型和先进的Transformer模型，并使用精确率、召回率和F1分数通过10倍交叉验证的平均值来评估性能。

Result: 实验结果表明，所提出的数据集为社交媒体上的健康评估做出了贡献，并为个性化的福祉评估和心理健康领域的早期干预策略铺平了道路。研究中还采用了事后解释以确保模型决策的透明度和可解释性。

Conclusion: 该数据集为社交媒体上的健康评估做出了贡献，并为个性化的福祉评估和心理健康领域的早期干预策略铺平了道路。

Abstract: We introduce a dataset for classifying wellness dimensions in social media
user posts, covering six key aspects: physical, emotional, social,
intellectual, spiritual, and vocational. The dataset is designed to capture
these dimensions in user-generated content, with a comprehensive annotation
framework developed under the guidance of domain experts. This framework allows
for the classification of text spans into the appropriate wellness categories.
We evaluate both traditional machine learning models and advanced
transformer-based models for this multi-class classification task, with
performance assessed using precision, recall, and F1-score, averaged over
10-fold cross-validation. Post-hoc explanations are applied to ensure the
transparency and interpretability of model decisions. The proposed dataset
contributes to region-specific wellness assessments in social media and paves
the way for personalized well-being evaluations and early intervention
strategies in mental health. We adhere to ethical considerations for
constructing and releasing our experiments and dataset publicly on Github.

</details>


### [393] [DRAGD: A Federated Unlearning Data Reconstruction Attack Based on Gradient Differences](https://arxiv.org/abs/2507.09602)
*Bocheng Ju,Junchao Fan,Jiaqi Liu,Xiaolin Chang*

Main category: cs.LG

TL;DR: 联邦遗忘存在隐私风险，DRAGD/DRAGDP攻击可重建已删除数据。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中的联邦遗忘（federated unlearning）机制虽然允许用户删除其数据，但梯度交换过程可能泄露被删除数据的敏感信息，引发新的隐私担忧。

Method: 提出了一种名为DRAGD的新型攻击方法，该方法通过利用遗忘前后梯度差异来重建数据。同时，还提出了DRAGDP的增强版本，该版本利用公开的先验数据来提高重建精度，特别适用于人脸图像等复杂数据集。

Result: 实验证明，DRAGD和DRAGDP在数据重建方面显著优于现有方法，尤其在处理人脸图像等复杂数据集时表现更佳。

Conclusion: 该研究揭示了联邦学习遗忘机制中的关键隐私漏洞，并提出了一种有效的解决方案，提高了联邦遗忘系统的安全性。

Abstract: Federated learning enables collaborative machine learning while preserving
data privacy. However, the rise of federated unlearning, designed to allow
clients to erase their data from the global model, introduces new privacy
concerns. Specifically, the gradient exchanges during the unlearning process
can leak sensitive information about deleted data. In this paper, we introduce
DRAGD, a novel attack that exploits gradient discrepancies before and after
unlearning to reconstruct forgotten data. We also present DRAGDP, an enhanced
version of DRAGD that leverages publicly available prior data to improve
reconstruction accuracy, particularly for complex datasets like facial images.
Extensive experiments across multiple datasets demonstrate that DRAGD and
DRAGDP significantly outperform existing methods in data reconstruction.Our
work highlights a critical privacy vulnerability in federated unlearning and
offers a practical solution, advancing the security of federated unlearning
systems in real-world applications.

</details>


### [394] [MLoRQ: Bridging Low-Rank and Quantization for Transformer Compression](https://arxiv.org/abs/2507.09616)
*Ofir Gordon,Ariel Lapid,Elad Cohen,Yarden Yagil,Arnon Netzer,Hai Victor Habi*

Main category: cs.LG

TL;DR: MLoRQ 是一种优化技术，通过低秩近似和混合精度量化来压缩边缘设备上的 Transformer 模型，可提高性能并满足内存限制。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的边缘设备上部署基于 Transformer 的神经网络是一个重大挑战，通常通过低秩近似和混合精度量化等技术来解决。

Method: MLoRQ 采用两阶段优化过程来确定每个层的最佳比特宽度和秩分配，以满足预定义的内存限制。第一阶段是层内优化，寻找所有低秩和量化组合的潜在最优压缩方案；第二阶段是层间优化，在满足内存限制的同时为每个层分配比特宽度精度和秩。此外，还有一个可选的最终步骤，利用改进的自适应舍入技术进行顺序优化，以减轻压缩引起的误差。

Result: MLoRQ 在图像分类、目标检测和实例分割任务的 Vision Transformer 上进行了评估，取得了最先进的结果，性能提高了 15%。

Conclusion: MLoRQ 是一种将低秩近似和混合精度量化相结合的新颖方法，适用于资源受限的边缘设备上的 Transformer 模型。

Abstract: Deploying transformer-based neural networks on resource-constrained edge
devices presents a significant challenge. This challenge is often addressed
through various techniques, such as low-rank approximation and mixed-precision
quantization. In this work, we introduce Mixed Low-Rank and Quantization
(MLoRQ), a novel method that integrates both techniques. MLoRQ employs a
two-stage optimization process to determine optimal bit-width and rank
assignments for each layer, adhering to predefined memory constraints. This
process includes: (i) an intra-layer optimization that identifies potentially
optimal compression solutions out of all low-rank and quantization
combinations; (ii) an inter-layer optimization that assigns bit-width precision
and rank to each layer while ensuring the memory constraint is met. An optional
final step applies a sequential optimization process using a modified adaptive
rounding technique to mitigate compression-induced errors in joint low-rank
approximation and quantization. The method is compatible and can be seamlessly
integrated with most existing quantization algorithms. MLoRQ shows
state-of-the-art results with up to 15\% performance improvement, evaluated on
Vision Transformers for image classification, object detection, and instance
segmentation tasks.

</details>


### [395] [Cultivating Pluralism In Algorithmic Monoculture: The Community Alignment Dataset](https://arxiv.org/abs/2507.09650)
*Lily Hong Zhang,Smitha Milli,Karen Jusko,Jonathan Smith,Brandon Amos,Wassim,Bouaziz,Manon Revel,Jack Kussman,Lisa Titus,Bhaktipriya Radharapu,Jane Yu,Vidya Sarma,Kris Rose,Maximilian Nickel*

Main category: cs.LG

TL;DR: LLM在满足用户多样化偏好方面存在不足。本研究提出负相关采样方法，并发布了包含近20万个比较的“社区对齐”数据集，以提高LLM的包容性。


<details>
  <summary>Details</summary>
Motivation: 为了解决大型语言模型（LLM）如何满足用户在文化、政治或其他维度上可能存在的冲突偏好这一挑战。

Method: 提出了负相关采样方法，并使用基于提示的技术来增强对齐方法在学习异构偏好方面的性能。

Result: 研究表明，人类偏好的差异性远超当前21个最先进LLM的响应，并发现现有的偏好数据集收集方法不足以学习人类偏好的多样性。

Conclusion: 该研究收集并开源了迄今为止最大、最具代表性的多语言和多轮社区对齐偏好数据集，包含来自五个国家/地区的近200,000个比较，以期提高LLM对全球多样化人群的有效性。

Abstract: How can large language models (LLMs) serve users with varying preferences
that may conflict across cultural, political, or other dimensions? To advance
this challenge, this paper establishes four key results. First, we demonstrate,
through a large-scale multilingual human study with representative samples from
five countries (N=15,000), that humans exhibit significantly more variation in
preferences than the responses of 21 state-of-the-art LLMs. Second, we show
that existing methods for preference dataset collection are insufficient for
learning the diversity of human preferences even along two of the most salient
dimensions of variability in global values, due to the underlying homogeneity
of candidate responses. Third, we argue that this motivates the need for
negatively-correlated sampling when generating candidate sets, and we show that
simple prompt-based techniques for doing so significantly enhance the
performance of alignment methods in learning heterogeneous preferences. Fourth,
based on this novel candidate sampling approach, we collect and open-source
Community Alignment, the largest and most representative multilingual and
multi-turn preference dataset to date, featuring almost 200,000 comparisons
from annotators spanning five countries. We hope that the Community Alignment
dataset will be a valuable resource for improving the effectiveness of LLMs for
a diverse global population.

</details>


### [396] [Multiple Choice Learning of Low Rank Adapters for Language Modeling](https://arxiv.org/abs/2507.10419)
*Victor Letzelter,Hugo Malard,Mathieu Fontaine,Gaël Richard,Slim Essid,Andrei Bursuc,Patrick Pérez*

Main category: cs.LG

TL;DR: LoRA-MCL通过结合MCL、WTA和LoRA来解决语言模型生成中的歧义问题，提高了生成句子续写的多样性和相关性，尤其在视觉和音频摘要任务中效果显著。


<details>
  <summary>Details</summary>
Motivation: 鉴于传统语言模型在给定上下文时存在多个同样合理的未来走向，即语言建模本质上是一个不适定问题，本研究旨在解决这一固有的歧义性。

Method: 本研究提出了LoRA-MCL训练方案，它扩展了语言模型的下一个词预测，并引入了一种在推理时生成多样化、合理句子续写的方法。该方法利用多项选择学习（MCL）和Winner-Takes-All（WTA）损失，通过低秩适应（LoRA）高效处理歧义。此外，研究还对将MCL应用于语言建模进行了理论阐释，假设数据来自混合分布，并使用马尔可夫链混合物作为示例。

Result: 通过在真实世界的视觉和音频摘要任务上进行大量实验，证明了所提出的方法能够生成高度多样化和相关的输出。

Conclusion: LoRA-MCL通过利用多项选择学习（MCL）和Winner-Takes-All（WTA）损失，并结合低秩适应（LoRA）技术，能够有效地处理语言模型在推理时生成多样化且合理的句子续写中的歧义问题，并在视觉和音频摘要等真实世界任务中取得了高多样性和高相关性的生成结果。

Abstract: We propose LoRA-MCL, a training scheme that extends next-token prediction in
language models with a method designed to decode diverse, plausible sentence
continuations at inference time. Traditional language modeling is an
intrinsically ill-posed problem: given a context, multiple futures may be
equally plausible. Our approach leverages Multiple Choice Learning (MCL) and
the Winner-Takes-All (WTA) loss to efficiently handle ambiguity through
Low-Rank Adaptation (LoRA). We provide a theoretical interpretation of applying
Multiple Choice Learning to Language Modeling, assuming the data is generated
from a mixture of distributions. To illustrate the proposed approach, we use
data sampled from mixtures of Markov chains. We then demonstrate with extensive
experiments on real-world visual and audio captioning tasks that our method
achieves high diversity and relevance in generated outputs.

</details>


### [397] [Conformal Prediction for Privacy-Preserving Machine Learning](https://arxiv.org/abs/2507.09678)
*Alexander David Balinsky,Dominik Krzeminski,Alexander Balinsky*

Main category: cs.LG

TL;DR: CP与加密数据上的监督学习相结合，在保护隐私的同时实现不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 旨在弥合严格的不确定性量化与保护隐私的机器学习之间的差距。

Method: 本研究将保形预测（CP）与确定性加密数据上的监督学习相结合，并测试了基于传统p值和基于e值的保形预测器。

Result: 在AES加密的MNIST数据集上，CP方法在加密域中依然有效。在确定性加密数据上训练的模型能够提取有意义的结构，测试准确率为36.88%，远高于每实例加密的随机猜测（9.56%）。基于e值的CP实现了超过60%的预测集覆盖率，具有4.3的损失阈值校准，在5000个测试用例中的4888个正确捕获了真实标签。基于p值的CP产生了更小的预测集，但覆盖准确性较低。

Conclusion: 该研究为安全、注重隐私的学习系统中的原则性不确定性量化奠定了基础，突显了加密数据环境中 CP 的潜力和局限性，以及预测集紧凑性与可靠性之间的关键权衡。

Abstract: We investigate the integration of Conformal Prediction (CP) with supervised
learning on deterministically encrypted data, aiming to bridge the gap between
rigorous uncertainty quantification and privacy-preserving machine learning.
Using AES-encrypted variants of the MNIST dataset, we demonstrate that CP
methods remain effective even when applied directly in the encrypted domain,
owing to the preservation of data exchangeability under fixed-key encryption.
We test traditional $p$-value-based against $e$-value-based conformal
predictors. Our empirical evaluation reveals that models trained on
deterministically encrypted data retain the ability to extract meaningful
structure, achieving 36.88\% test accuracy -- significantly above random
guessing (9.56\%) observed with per-instance encryption. Moreover,
$e$-value-based CP achieves predictive set coverage of over 60\% with 4.3
loss-threshold calibration, correctly capturing the true label in 4888 out of
5000 test cases. In contrast, the $p$-value-based CP yields smaller predictive
sets but with reduced coverage accuracy. These findings highlight both the
promise and limitations of CP in encrypted data settings and underscore
critical trade-offs between prediction set compactness and reliability. %Our
work sets a foundation for principled uncertainty quantification in secure,
privacy-aware learning systems.

</details>


### [398] [Reasoning or Memorization? Unreliable Results of Reinforcement Learning Due to Data Contamination](https://arxiv.org/abs/2507.10532)
*Mingqi Wu,Zhihao Zhang,Qiaole Dong,Zhiheng Xi,Jun Zhao,Senjie Jin,Xiaoran Fan,Yuhao Zhou,Yanwei Fu,Qin Liu,Songyang Zhang,Qi Zhang*

Main category: cs.LG

TL;DR: 现有关于强化学习提升大语言模型推理能力的研究主要集中在Qwen2.5模型及其在特定基准上的表现，但忽略了数据污染的可能性。本文通过引入一个名为RandomCalculation的合成数据集，发现只有准确的奖励信号才能提升模型性能，并呼吁在未受污染的数据集和更多模型上进行评估以获得可靠结论。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有研究在Qwen2.5模型上取得突破，但在Llama等其他模型上未能取得类似进展的问题，并且为了验证是否所有模型在各种基准测试中都存在数据污染的问题，本文进行了进一步的研究。

Method: 本文引入了一个生成器，该生成器可以生成任意长度和难度的完全合成的算术问题，从而得到了一个名为RandomCalculation的干净数据集。

Result: 通过使用无数据泄漏的数据集，我们证明了只有准确的奖励信号才能持续提高性能，而噪声或不正确的信号则不能。

Conclusion: 在干净的数据集上，只有准确的奖励信号才能持续提高性能，而噪声或不正确的信号则不能。我们提倡在未受污染的基准测试和跨模型家族的评估中评估强化学习方法，以确保可信的结论。

Abstract: The reasoning capabilities of large language models (LLMs) have been a
longstanding focus of research. Recent works have further enhanced these
capabilities using reinforcement learning (RL), with many new methods claiming
significant improvements with minimal or no external supervision. Surprisingly,
some studies even suggest that random or incorrect reward signals can enhance
reasoning performance. However, these breakthroughs are mostly reported on the
Qwen2.5 model family and evaluated on well-known benchmarks such as MATH-500,
AMC, and AIME, while failing to achieve similar gains on other models like
Llama, which warrants further investigation. Our analysis shows that although
Qwen2.5 achieves strong mathematical reasoning performance, its pretraining on
large-scale web corpora makes it vulnerable to data contamination in popular
benchmarks. As a result, results derived from these benchmarks may be
unreliable. To address this, we introduce a generator that produces fully
synthetic arithmetic problems of arbitrary length and difficulty, yielding a
clean dataset we call RandomCalculation. Using these leakage-free datasets, we
show that only accurate reward signals consistently improve performance, while
noisy or incorrect signals do not. We advocate for evaluating RL methods on
uncontaminated benchmarks and across diverse model families to ensure
trustworthy conclusions.

</details>


### [399] [Post-Training Quantization of Generative and Discriminative LSTM Text Classifiers: A Study of Calibration, Class Balance, and Robustness](https://arxiv.org/abs/2507.09687)
*Md Mushfiqur Rahaman,Elliot Chang,Tasmiah Haque,Srinjoy Das*

Main category: cs.LG

TL;DR: 该研究对量化后训练 (PTQ) 对生成式和判别式 LSTM 文本分类模型的影响进行了比较研究，发现在边缘计算场景下，生成式模型对量化参数和数据不平衡更为敏感，并提出了在量化过程中谨慎选择校准数据的重要性。


<details>
  <summary>Details</summary>
Motivation: 为了满足工业监控、健康诊断和智能助手等边缘计算应用对低延迟和高准确性的要求，该研究旨在探索生成式和判别式 LSTM 文本分类模型在量化后训练 (PTQ) 下的性能和鲁棒性，并评估 PTQ 对这些模型（尤其是生成式模型）的影响。

Method: 研究人员使用 Brevitas 量化库对生成式和判别式长短期记忆 (LSTM) 文本分类模型进行了量化后训练 (PTQ) 的综合比较研究。他们评估了不同比特宽度的分类器模型，并在正常和噪声输入条件下评估了它们的鲁棒性。

Result: 研究发现，判别式分类器在量化后仍然保持鲁棒性，而生成式分类器对比特宽度、PTQ 期间使用的校准数据和量化推理期间的输入噪声更为敏感。研究还评估了类别不平衡对校准数据的影响，发现在较低比特宽度下，生成式 LSTM 分类器在校准中使用类别不平衡数据会导致性能下降。

Conclusion: 该研究表明，在量化过程中使用类别不平衡的校准数据会导致生成式 LSTM 分类器在较低比特宽度下权重适应不足，从而导致性能下降。这强调了校准数据在 PTQ 中的作用，以及生成式分类器在噪声下的成功和失败，有助于在边缘环境中部署。

Abstract: Text classification plays a pivotal role in edge computing applications like
industrial monitoring, health diagnostics, and smart assistants, where low
latency and high accuracy are both key requirements. Generative classifiers, in
particular, have been shown to exhibit robustness to out-of-distribution and
noisy data, which is an extremely critical consideration for deployment in such
real-time edge environments. However, deploying such models on edge devices
faces computational and memory constraints. Post Training Quantization (PTQ)
reduces model size and compute costs without retraining, making it ideal for
edge deployment. In this work, we present a comprehensive comparative study of
generative and discriminative Long Short Term Memory (LSTM)-based text
classification models with PTQ using the Brevitas quantization library. We
evaluate both types of classifier models across multiple bitwidths and assess
their robustness under regular and noisy input conditions. We find that while
discriminative classifiers remain robust, generative ones are more sensitive to
bitwidth, calibration data used during PTQ, and input noise during quantized
inference. We study the influence of class imbalance in calibration data for
both types of classifiers, comparing scenarios with evenly and unevenly
distributed class samples including their effect on weight adjustments and
activation profiles during PTQ. Using test statistics derived from
nonparametric hypothesis testing, we identify that using class imbalanced data
during calibration introduces insufficient weight adaptation at lower bitwidths
for generative LSTM classifiers, thereby leading to degraded performance. This
study underscores the role of calibration data in PTQ and when generative
classifiers succeed or fail under noise, aiding deployment in edge
environments.

</details>


### [400] [Frequency-aware Surrogate Modeling With SMT Kernels For Advanced Data Forecasting](https://arxiv.org/abs/2507.09694)
*Nicolas Gonel,Paul Saves,Joseph Morlier*

Main category: cs.LG

TL;DR: 该研究提出了一个名为SMT 2.0的开源框架，用于开发和组合用于代理建模的相关核函数，特别关注频率感知能力，并成功应用于CO2浓度和航空公司乘客流量预测。


<details>
  <summary>Details</summary>
Motivation: 本研究的动机是推进基于核的建模技术，通过开发一个开源框架来处理具有复杂机械行为和时频动力学的航空器系统，并支持用户定义的和可组合的核函数。

Method: 本研究提出了一个开源框架，用于开发相关核函数，重点是用户定义的核函数和用于代理建模的核函数组合。通过集成频率感知元素，扩展了传统的基于指数的核函数，以包括更广泛的核函数及其导数。

Result: 对正弦基数测试用例进行了验证，并成功应用于预测夏威夷冒纳罗亚的二氧化碳浓度和航空公司乘客流量。

Conclusion: 该框架为开发相关核函数提供了一个灵活的工具集，并为复杂、对频率敏感的领域中的超模型开辟了新的应用途径。

Abstract: This paper introduces a comprehensive open-source framework for developing
correlation kernels, with a particular focus on user-defined and composition of
kernels for surrogate modeling. By advancing kernel-based modeling techniques,
we incorporate frequency-aware elements that effectively capture complex
mechanical behaviors and timefrequency dynamics intrinsic to aircraft systems.
Traditional kernel functions, often limited to exponential-based methods, are
extended to include a wider range of kernels such as exponential squared sine
and rational quadratic kernels, along with their respective firstand
second-order derivatives. The proposed methodologies are first validated on a
sinus cardinal test case and then applied to forecasting Mauna-Loa Carbon
Dioxide (CO 2 ) concentrations and airline passenger traffic. All these
advancements are integrated into the open-source Surrogate Modeling Toolbox
(SMT 2.0), providing a versatile platform for both standard and customizable
kernel configurations. Furthermore, the framework enables the combination of
various kernels to leverage their unique strengths into composite models
tailored to specific problems. The resulting framework offers a flexible
toolset for engineers and researchers, paving the way for numerous future
applications in metamodeling for complex, frequency-sensitive domains.

</details>


### [401] [EPT-2 Technical Report](https://arxiv.org/abs/2507.09703)
*Roberto Molinaro,Niall Siegenheim,Niels Poulsen,Jordan Dane Daubinet,Henry Martin,Mark Frey,Kevin Thiart,Alexander Jakob Dautel,Andreas Schlueter,Alex Grigoryev,Bogdan Danciu,Nikoo Ekhtiari,Bas Steunebrink,Leonie Wagner,Marvin Vincent Gabler*

Main category: cs.LG

TL;DR: 新一代AI模型EPT-2在风速、温度等预测上超越现有模型，其集成版本EPT-2e在概率预测上表现更佳，且计算成本更低。


<details>
  <summary>Details</summary>
Motivation: 为了改进地球系统预测能力，特别是能源相关变量（如风速和温度）的短期和中期预测。

Method: 提出了EPT-2，一个基于Transformer的AI模型，用于地球系统预测，并引入了基于扰动的集成模型EPT-2e，用于概率性预测。

Result: EPT-2在预测风速、温度和太阳辐射方面设定了新的最高水平，优于Microsoft Aurora和ECMWF IFS HRES。EPT-2e在概率性预测方面也超越了ECMWF ENS。

Conclusion: EPT-2和EPT-2e在地球系统预测方面取得了显著进展，在多项关键气象变量预测上超越了现有模型和数值天气预报系统，并在计算成本上具有优势。

Abstract: We present EPT-2, the latest iteration in our Earth Physics Transformer (EPT)
family of foundation AI models for Earth system forecasting. EPT-2 delivers
substantial improvements over its predecessor, EPT-1.5, and sets a new state of
the art in predicting energy-relevant variables-including 10m and 100m wind
speed, 2m temperature, and surface solar radiation-across the full 0-240h
forecast horizon. It consistently outperforms leading AI weather models such as
Microsoft Aurora, as well as the operational numerical forecast system IFS HRES
from the European Centre for Medium-Range Weather Forecasts (ECMWF). In
parallel, we introduce a perturbation-based ensemble model of EPT-2 for
probabilistic forecasting, called EPT-2e. Remarkably, EPT-2e significantly
surpasses the ECMWF ENS mean-long considered the gold standard for medium- to
longrange forecasting-while operating at a fraction of the computational cost.
EPT models, as well as third-party forecasts, are accessible via the app.jua.ai
platform.

</details>


### [402] [Continental scale habitat modelling with artificial intelligence and multimodal earth observation](https://arxiv.org/abs/2507.09732)
*Sara Si-Moussi,Stephan Hennekens,Sander Mucher,Stan Los,Wilfried Thuiller*

Main category: cs.LG

TL;DR: 通过结合高分辨率遥感数据（包括多光谱和SAR影像）和人工智能（AI）技术，特别是利用分层建模和集成学习方法来解决类别不平衡问题，可以显著提高欧洲范围内生境分类的准确性和分辨率。


<details>
  <summary>Details</summary>
Motivation: 随着生态系统面临日益增长的人类活动压力，准确、高分辨率的生境地图对于有效的保护和恢复至关重要，但现有地图在主题或空间分辨率上存在不足。

Method: 利用欧洲植被数据库的样地数据，对欧洲的3级EUNIS生境进行了建模，并评估了多种建模策略。研究采用了分层建模方法、多光谱（MSI）和合成孔径雷达（SAR）影像融合（特别是通过地球观测基金模型），以及克服类别不平衡的集成机器学习技术。

Result: 研究表明，利用遥感和人工智能技术，特别是结合分层建模、多光谱与SAR影像融合以及处理类别不平衡的集成学习方法，能够有效提高生境分类的准确性，尤其是在处理相互排斥的生境类型共存和类别不平衡问题上。

Conclusion: 该研究提出了一种结合高分辨率遥感数据和人工智能工具的方法，以提高欧洲范围内精细生境分类的准确性，并为未来研究指明了方向。

Abstract: Habitats integrate the abiotic conditions and biophysical structures that
support biodiversity and sustain nature's contributions to people. As these
ecosystems face mounting pressure from human activities, accurate,
high-resolution habitat maps are essential for effective conservation and
restoration. Yet current maps often fall short in thematic or spatial
resolution because they must (1) model several mutually exclusive habitat types
that co-occur across landscapes and (2) cope with severe class imbalance that
complicate multi-class training. Here, we evaluated how high-resolution remote
sensing (RS) data and Artificial Intelligence (AI) tools can improve habitat
classification over large geographic extents at fine thematic resolution. Using
vegetation plots from the European Vegetation Archive, we modelled Level 3
EUNIS habitats across Europe and assessed multiple modelling strategies against
independent validation datasets. Strategies that exploited the hierarchical
nature of habitat nomenclatures resolved classification ambiguities, especially
in fragmented landscapes. Integrating multi-spectral (MSI) and synthetic
aperture radar (SAR) imagery, particularly through Earth Observation Foundation
models, enhanced within-formation discrimination and overall performance.
Finally, ensemble machine learning that corrects class imbalance boosted
accuracy further. Our methodological framework is transferable beyond Europe
and adaptable to other classification systems. Future research should advance
temporal modelling of dynamic habitats, extend to habitat segmentation and
quality assessment, and exploit next-generation EO data paired with
higher-quality in-situ observations.

</details>


### [403] [Universal Physics Simulation: A Foundational Diffusion Approach](https://arxiv.org/abs/2507.09733)
*Bradley Camburn*

Main category: cs.LG

TL;DR: 我们提出了第一个通用的物理模拟基础AI模型，可以直接从边界条件数据学习物理定律，无需先验方程编码。该模型使用草图引导扩散变换器，将模拟视为条件生成问题，并直接生成稳态解，SSIM > 0.8，同时保持亚像素边界精度。该方法还能通过可解释性分析进行物理发现。


<details>
  <summary>Details</summary>
Motivation: 传统的物理信息神经网络（PINNs）和有限差分方法需要明确的数学公式，这限制了它们在泛化和发现方面的潜力。

Method: 提出了一种新的草图引导扩散变换器方法，将模拟视为条件生成问题，其中空间边界条件指导物理上精确的稳态解的合成。该模型利用增强的扩散变换器架构和新颖的空间关系编码，实现了直接的边界到平衡映射，并且可以推广到不同的物理领域。

Result: 该模型实现了直接的边界到平衡映射，并且可以推广到不同的物理领域。与累积误差的顺序时间步长方法不同，该方法绕过了时间积分，直接生成稳态解，SSIM > 0.8，同时保持亚像素边界精度。

Conclusion: 该工作代表了从AI加速物理到AI发现物理的范式转变，建立了第一个真正通用的物理模拟框架。

Abstract: We present the first foundational AI model for universal physics simulation
that learns physical laws directly from boundary-condition data without
requiring a priori equation encoding. Traditional physics-informed neural
networks (PINNs) and finite-difference methods necessitate explicit
mathematical formulation of governing equations, fundamentally limiting their
generalizability and discovery potential. Our sketch-guided diffusion
transformer approach reimagines computational physics by treating simulation as
a conditional generation problem, where spatial boundary conditions guide the
synthesis of physically accurate steady-state solutions.
  By leveraging enhanced diffusion transformer architectures with novel spatial
relationship encoding, our model achieves direct boundary-to-equilibrium
mapping and is generalizable to diverse physics domains. Unlike sequential
time-stepping methods that accumulate errors over iterations, our approach
bypasses temporal integration entirely, directly generating steady-state
solutions with SSIM > 0.8 while maintaining sub-pixel boundary accuracy. Our
data-informed approach enables physics discovery through learned
representations analyzable via Layer-wise Relevance Propagation (LRP),
revealing emergent physical relationships without predetermined mathematical
constraints. This work represents a paradigm shift from AI-accelerated physics
to AI-discovered physics, establishing the first truly universal physics
simulation framework.

</details>


### [404] [Do we need equivariant models for molecule generation?](https://arxiv.org/abs/2507.09753)
*Ewa M. Nowara,Joshua Rackers,Patricia Suriana,Pan Kessel,Max Shen,Andrew Martin Watkins,Michael Maser*

Main category: cs.LG

TL;DR: 非等变CNN通过旋转增强可以学习等变性，性能媲美等变GNN，解决了等变GNN的痛点。


<details>
  <summary>Details</summary>
Motivation: 探究非等变CNN是否可以通过旋转增强学习等变性，并与等变模型相媲美，以解决现有等变GNN模型复杂、难训练和扩展性差的问题。

Method: 通过推导损失分解来分离预测误差和等变性误差，并评估模型大小、数据集大小和训练时长对去噪、分子生成和属性预测性能的影响。

Result: 研究结果表明，非等变CNN在旋转增强的帮助下可以学习等变性，并在多个任务上达到与等变模型相当的性能水平，这是首次在生成任务中对学习到的等变性进行分析。

Conclusion: 通过使用旋转增强训练的非等变卷积神经网络（CNN）可以学习等变性，并达到与等变图神经网络（GNNs）相当的性能，从而克服了等变GNNs的复杂性、训练难度大和扩展性差等缺点。

Abstract: Deep generative models are increasingly used for molecular discovery, with
most recent approaches relying on equivariant graph neural networks (GNNs)
under the assumption that explicit equivariance is essential for generating
high-quality 3D molecules. However, these models are complex, difficult to
train, and scale poorly.
  We investigate whether non-equivariant convolutional neural networks (CNNs)
trained with rotation augmentations can learn equivariance and match the
performance of equivariant models. We derive a loss decomposition that
separates prediction error from equivariance error, and evaluate how model
size, dataset size, and training duration affect performance across denoising,
molecule generation, and property prediction. To our knowledge, this is the
first study to analyze learned equivariance in generative tasks.

</details>


### [405] [Explainable AI in Genomics: Transcription Factor Binding Site Prediction with Mixture of Experts](https://arxiv.org/abs/2507.09754)
*Aakash Tripathi,Ian E. Nielsen,Muhammad Umer,Ravi P. Ramachandran,Ghulam Rasool*

Main category: cs.LG

TL;DR: 本研究提出了一种结合多个CNN模型的混合专家（MoE）方法，用于转录因子结合位点（TFBS）预测，并在分布外数据集上取得了优于单独模型和传统解释方法的性能。同时，引入了ShiftSmooth解释技术，提高了模型的可解释性。


<details>
  <summary>Details</summary>
Motivation: 理解基因调控和各种生物过程需要精确的转录因子结合位点（TFBS）预测。现有的方法在处理不同TFBS模式和分布外（OOD）数据集时可能存在局限性。

Method: 本研究提出了一种新颖的混合专家（MoE）方法，整合了多个专门研究不同TFBS模式的预训练卷积神经网络（CNN）模型。引入了ShiftSmooth技术，一种考虑输入序列微小变化的归因映射技术，以增强模型可解释性。

Result: 混合专家（MoE）模型在评估中表现出有竞争力或更优越的性能，特别是在OOD数据集上。ANOVA统计检验证实了性能差异的显著性。ShiftSmooth技术在可解释性分析中优于Vanilla Gradient方法。

Conclusion: 本研究提出的混合专家（MoE）模型在转录因子结合位点（TFBS）预测任务上表现出有竞争力或更优越的性能，尤其在分布外（OOD）场景下表现突出。ShiftSmooth归因映射技术提供了比传统Vanilla Gradient方法更鲁棒的模型可解释性，有助于基元发现和定位。该研究为TFBS预测提供了一个高效、可泛化且可解释的解决方案，有望在基因组生物学和转录调控研究中带来新发现。

Abstract: Transcription Factor Binding Site (TFBS) prediction is crucial for
understanding gene regulation and various biological processes. This study
introduces a novel Mixture of Experts (MoE) approach for TFBS prediction,
integrating multiple pre-trained Convolutional Neural Network (CNN) models,
each specializing in different TFBS patterns. We evaluate the performance of
our MoE model against individual expert models on both in-distribution and
out-of-distribution (OOD) datasets, using six randomly selected transcription
factors (TFs) for OOD testing. Our results demonstrate that the MoE model
achieves competitive or superior performance across diverse TF binding sites,
particularly excelling in OOD scenarios. The Analysis of Variance (ANOVA)
statistical test confirms the significance of these performance differences.
Additionally, we introduce ShiftSmooth, a novel attribution mapping technique
that provides more robust model interpretability by considering small shifts in
input sequences. Through comprehensive explainability analysis, we show that
ShiftSmooth offers superior attribution for motif discovery and localization
compared to traditional Vanilla Gradient methods. Our work presents an
efficient, generalizable, and interpretable solution for TFBS prediction,
potentially enabling new discoveries in genome biology and advancing our
understanding of transcriptional regulation.

</details>


### [406] [Toward accurate RUL and SOH estimation using reinforced graph-based PINNs enhanced with dynamic weights](https://arxiv.org/abs/2507.09766)
*Mohamadreza Akbari Pour,Ali Ghasemzadeh,MohamadAli Bijarchi,Mohammad Behshad Shafii*

Main category: cs.LG

TL;DR: RGPD框架通过结合物理信息神经网络、图卷积循环网络和强化学习，提高了剩余使用寿命和健康状态估计的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 准确估计剩余使用寿命（RUL）和健康状态（SOH）对于许多工业应用中的预期与健康管理（PHM）至关重要。

Method: 提出了一种名为 RGPD 的新颖框架，该框架结合了基于物理的监督与先进的空时学习。它使用图卷积循环网络（GCRN）和图注意力卷积（GATConv）来捕获节点的时变演化和自适应空间聚合。Soft Actor-Critic（SAC）模块和 Q-learning agent 用于优化学习过程，通过动态加权和缩放来提高注意力和预测准确性，并识别最相关的物理约束。

Result: RGPD框架在RUL和SOH估计任务中均表现出色，超越了现有技术，并在三种不同的工业数据集上展示了对不同退化模式的鲁棒性和预测准确性。

Conclusion: 该方法在剩余使用寿命（RUL）和健康状态（SOH）估计任务中始终优于最先进的模型，在三种不同的工业基准数据集上表现出对各种退化模式的鲁棒性和预测准确性。

Abstract: Accurate estimation of Remaining Useful Life (RUL) and State of Health (SOH)
is essential for Prognostics and Health Management (PHM) across a wide range of
industrial applications. We propose a novel framework -- Reinforced Graph-Based
Physics-Informed Neural Networks Enhanced with Dynamic Weights (RGPD) -- that
combines physics-based supervision with advanced spatio-temporal learning.
Graph Convolutional Recurrent Networks (GCRNs) embed graph-convolutional
filters within recurrent units to capture how node representations evolve over
time. Graph Attention Convolution (GATConv) leverages a self-attention
mechanism to compute learnable, edge-wise attention coefficients, dynamically
weighting neighbor contributions for adaptive spatial aggregation. A Soft
Actor-Critic (SAC) module is positioned between the Temporal Attention Unit
(TAU) and GCRN to further improve the spatio-temporal learning. This module
improves attention and prediction accuracy by dynamically scaling hidden
representations to minimize noise and highlight informative features. To
identify the most relevant physical constraints in each area, Q-learning agents
dynamically assign weights to physics-informed loss terms, improving
generalization across real-time industrial systems and reducing the need for
manual tuning. In both RUL and SOH estimation tasks, the proposed method
consistently outperforms state-of-the-art models, demonstrating strong
robustness and predictive accuracy across varied degradation patterns across
three diverse industrial benchmark datasets.

</details>


### [407] [Knowing When to Quit: Probabilistic Early Exits for Speech Separation](https://arxiv.org/abs/2507.09768)
*Kenny Falkær Olsen. Mads Østergaard,Karl Ulbæk,Søren Føns Nielsen,Rasmus Malik Høegh Lindrup,Bjørn Sand Jensen,Morten Mørup*

Main category: cs.LG

TL;DR: 提出了一种可动态调整计算量的语音分离模型，可在不同设备上实现最优性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有深度学习单通道语音分离模型在固定计算和参数预算下，无法适应移动电话和助听器等嵌入式和异构设备的需求，本研究旨在设计一种能够根据不同计算需求和资源进行扩展的模型。

Method: 设计了一种支持早退的神经网络架构，并提出了一种不确定性感知概率框架，用于联合建模干净语音信号和误差方差，从而推导出基于所需信噪比的概率早退条件。

Result: 实验结果表明，单一的早退模型在语音分离和增强任务上能够与在多种计算和参数预算下训练的最先进模型相媲美，实现了精细的动态计算扩展和可解释的退出条件。

Conclusion: 该研究提出了一种支持早退的深度学习语音分离模型，并结合不确定性感知概率框架，以实现动态计算扩展和可解释的退出条件，在语音分离和增强任务上均达到当前最先进水平。

Abstract: In recent years, deep learning-based single-channel speech separation has
improved considerably, in large part driven by increasingly compute- and
parameter-efficient neural network architectures. Most such architectures are,
however, designed with a fixed compute and parameter budget, and consequently
cannot scale to varying compute demands or resources, which limits their use in
embedded and heterogeneous devices such as mobile phones and hearables. To
enable such use-cases we design a neural network architecture for speech
separation capable of early-exit, and we propose an uncertainty-aware
probabilistic framework to jointly model the clean speech signal and error
variance which we use to derive probabilistic early-exit conditions in terms of
desired signal-to-noise ratios. We evaluate our methods on both speech
separation and enhancement tasks, and we show that a single early-exit model
can be competitive with state-of-the-art models trained at many compute and
parameter budgets. Our framework enables fine-grained dynamic compute-scaling
of speech separation networks while achieving state-of-the-art performance and
interpretable exit conditions.

</details>


### [408] [Efficient Molecular Conformer Generation with SO(3)-Averaged Flow Matching and Reflow](https://arxiv.org/abs/2507.09785)
*Zhonglin Cao,Mario Geiger,Allan dos Santos Costa,Danny Reidenbach,Karsten Kreis,Tomas Geffner,Franco Pellegrini,Guoqing Zhou,Emine Kucukbenli*

Main category: cs.LG

TL;DR: 通过SO(3)-Averaged Flow和蒸馏技术，我们加速了分子构象生成模型，实现了快速训练和高效推理。


<details>
  <summary>Details</summary>
Motivation: 当前的基于扩散或流的模型在分子构象生成任务中需要大量的计算资源，限制了其在计算化学和药物发现中的应用。因此，需要开发更快速、更准确的生成模型。

Method: 1. 提出SO(3)-Averaged Flow训练目标，用于加速生成模型的训练并提升生成质量。 2. 提出利用reflow和蒸馏技术实现少步或单步分子构象生成。

Result: SO(3)-Averaged Flow训练的模型在生成质量上达到或超过了现有最优水平。reflow和蒸馏技术实现了高质量的少步或单步分子构象生成。

Conclusion: 本文提出的SO(3)-Averaged Flow训练目标和蒸馏方法能够显著加速3D分子构象生成模型的训练和推理过程，同时保持或提高生成质量，为高效的分子构象生成提供了新途径。

Abstract: Fast and accurate generation of molecular conformers is desired for
downstream computational chemistry and drug discovery tasks. Currently,
training and sampling state-of-the-art diffusion or flow-based models for
conformer generation require significant computational resources. In this work,
we build upon flow-matching and propose two mechanisms for accelerating
training and inference of generative models for 3D molecular conformer
generation. For fast training, we introduce the SO(3)-Averaged Flow training
objective, which leads to faster convergence to better generation quality
compared to conditional optimal transport flow or Kabsch-aligned flow. We
demonstrate that models trained using SO(3)-Averaged Flow can reach
state-of-the-art conformer generation quality. For fast inference, we show that
the reflow and distillation methods of flow-based models enable few-steps or
even one-step molecular conformer generation with high quality. The training
techniques proposed in this work show a path towards highly efficient molecular
conformer generation with flow-based models.

</details>


### [409] [Leveraging Distribution Matching to Make Approximate Machine Unlearning Faster](https://arxiv.org/abs/2507.09786)
*Junaid Iqbal Khan*

Main category: cs.LG

TL;DR: 提出 Blend 和 A-AMU 方法，通过数据集缩减和加速损失函数来提高机器遗忘的效率，显著降低了计算成本，同时保持了模型性能和隐私。


<details>
  <summary>Details</summary>
Motivation: 旨在解决近似机器遗忘 (AMU) 中的计算瓶颈，特别是通过专门的微调处理保留的数据子集所带来的计算成本和减少训练轮数方面的挑战。

Method: 提出了一种名为 Blend 的新颖的分布匹配数据集缩减 (DC) 方法，该方法通过合并具有共享混合权重的视觉相似图像来显著减小保留数据集的大小，并提出了一种名为 Accelerated-AMU (A-AMU) 的以损失为中心的方法，该方法通过结合陡化的主要损失和匹配被遗忘数据和分布内未见数据的损失分布的新型可微分正则化器来加速遗忘。

Result: 实验证明，所提出的 Blend 和 A-AMU 方法能够显著减少端到端的模型无效化延迟，并且在单轮和多轮场景下均能保持模型的效用和隐私。

Conclusion: Blend 和 Accelerated-AMU (A-AMU) 的双重方法通过联合设计专门的数据集缩减技术和专用的加速损失函数，显著降低了端到端的模型无效化延迟，同时保持了模型的效用和隐私。

Abstract: Approximate machine unlearning (AMU) enables models to `forget' specific
training data through specialized fine-tuning on a retained dataset subset.
However, processing this retained subset still dominates computational runtime,
while reductions of epochs also remain a challenge. We propose two
complementary methods to accelerate classification-oriented AMU. First,
\textbf{Blend}, a novel distribution-matching dataset condensation (DC), merges
visually similar images with shared blend-weights to significantly reduce the
retained set size. It operates with minimal pre-processing overhead and is
orders of magnitude faster than state-of-the-art DC methods. Second, our
loss-centric method, \textbf{Accelerated-AMU (A-AMU)}, augments the unlearning
objective to quicken convergence. A-AMU achieves this by combining a steepened
primary loss to expedite forgetting with a novel, differentiable regularizer
that matches the loss distributions of forgotten and in-distribution unseen
data. Our extensive experiments demonstrate that this dual approach of data and
loss-centric optimization dramatically reduces end-to-end unlearning latency
across both single and multi-round scenarios, all while preserving model
utility and privacy. To our knowledge, this is the first work to systematically
tackle unlearning efficiency by jointly designing a specialized dataset
condensation technique with a dedicated accelerated loss function. Code is
available at https://github.com/algebraicdianuj/DC_Unlearning.

</details>


### [410] [A Scalable and Efficient Signal Integration System for Job Matching](https://arxiv.org/abs/2507.09797)
*Ping Liu,Rajat Arora,Xiao Shi,Benjamin Le,Qianqi Shen,Jianqiang Shen,Chengming Jiang,Nikita Zhiltsov,Priya Bannur,Yidan Zhu,Liming Dong,Haichao Wei,Qi Guo,Luke Simon,Liangjie Hong,Wenjing Zhang*

Main category: cs.LG

TL;DR: LinkedIn的STAR系统利用LLMs和GNNs解决了职位匹配中的冷启动和偏见问题，提供了一个可扩展的端到端解决方案。


<details>
  <summary>Details</summary>
Motivation: LinkedIn在构建职位匹配推荐系统时面临着冷启动、过滤气泡和影响候选人-职位匹配的偏见等建模挑战。

Method: STAR系统整合了大型语言模型（LLMs）理解文本数据（如用户资料和职位描述）的能力，以及图神经网络（GNNs）捕捉复杂关系和通过网络效应缓解冷启动问题的能力。该系统集成了包括自适应采样和版本管理在内的工业规模范式，并提供了一个用于大规模推荐系统中嵌入开发的端到端解决方案。

Result: STAR系统通过整合LLMs和GNNs的能力，并结合工业规模的范式，实现了高性能的推荐，并为实际模型部署提供了实用的见解。

Conclusion: STAR系统成功地将大型语言模型（LLMs）和图神经网络（GNNs）的优势结合起来，为LinkedIn的职位匹配产品提供了一个端到端的解决方案，解决了冷启动、过滤气泡和偏见等挑战。

Abstract: LinkedIn, one of the world's largest platforms for professional networking
and job seeking, encounters various modeling challenges in building
recommendation systems for its job matching product, including cold-start,
filter bubbles, and biases affecting candidate-job matching. To address these,
we developed the STAR (Signal Integration for Talent And Recruiters) system,
leveraging the combined strengths of Large Language Models (LLMs) and Graph
Neural Networks (GNNs). LLMs excel at understanding textual data, such as
member profiles and job postings, while GNNs capture intricate relationships
and mitigate cold-start issues through network effects. STAR integrates diverse
signals by uniting LLM and GNN capabilities with industrial-scale paradigms
including adaptive sampling and version management. It provides an end-to-end
solution for developing and deploying embeddings in large-scale recommender
systems. Our key contributions include a robust methodology for building
embeddings in industrial applications, a scalable GNN-LLM integration for
high-performing recommendations, and practical insights for real-world model
deployment.

</details>


### [411] [Federated Learning with Graph-Based Aggregation for Traffic Forecasting](https://arxiv.org/abs/2507.09805)
*Audri Banik,Glaucio Haroldo Silva de Carvalho,Renata Dividino*

Main category: cs.LG

TL;DR: 提出了一种轻量级的图感知联邦学习方法，用于交通预测。该方法通过在参数更新中整合邻域聚合和图连通性来考虑客户端之间的空间关系，从而在保持计算效率的同时提高了性能。


<details>
  <summary>Details</summary>
Motivation: 在交通预测任务中，客户端（区域或道路路段）之间存在重要的空间关系，而标准的联邦学习方法（如 FedAvg）假设客户端是独立的，这可能会限制性能。现有的联邦图学习方法虽然能捕捉这些依赖关系，但通常会引入显著的计算开销。

Method: 提出了一种轻量级的图感知联邦学习方法，该方法将 FedAvg 的简洁性与图学习的关键思想相结合。该方法不是训练完整的模型，而是将基本的邻域聚合原理应用于指导参数更新，并根据图连通性对客户端模型进行加权。

Result: 该方法有效地捕捉了空间关系，同时保持了计算效率。

Conclusion: 提出的轻量级图感知联邦学习方法在两个基准交通数据集（METR-LA 和 PEMS-BAY）上进行了评估，并与标准基线和最近的基于图的联邦学习技术相比，取得了具有竞争力的性能。

Abstract: In traffic prediction, the goal is to estimate traffic speed or flow in
specific regions or road segments using historical data collected by devices
deployed in each area. Each region or road segment can be viewed as an
individual client that measures local traffic flow, making Federated Learning
(FL) a suitable approach for collaboratively training models without sharing
raw data. In centralized FL, a central server collects and aggregates model
updates from multiple clients to build a shared model while preserving each
client's data privacy. Standard FL methods, such as Federated Averaging
(FedAvg), assume that clients are independent, which can limit performance in
traffic prediction tasks where spatial relationships between clients are
important. Federated Graph Learning methods can capture these dependencies
during server-side aggregation, but they often introduce significant
computational overhead. In this paper, we propose a lightweight graph-aware FL
approach that blends the simplicity of FedAvg with key ideas from graph
learning. Rather than training full models, our method applies basic
neighbourhood aggregation principles to guide parameter updates, weighting
client models based on graph connectivity. This approach captures spatial
relationships effectively while remaining computationally efficient. We
evaluate our method on two benchmark traffic datasets, METR-LA and PEMS-BAY,
and show that it achieves competitive performance compared to standard
baselines and recent graph-based federated learning techniques.

</details>


### [412] [Compressed Computation: Dense Circuits in a Toy Model of the Universal-AND Problem](https://arxiv.org/abs/2507.09816)
*Adam Newgas*

Main category: cs.LG

TL;DR: 神经网络在压缩计算中倾向于学习全密集的解决方案，而非理论构造，该方案更易扩展且效率更高。


<details>
  <summary>Details</summary>
Motivation: 在理论上，神经网络能够表示比其维度更多的特征（叠加态），但对于计算而言，这种叠加态是否能在实践中被学习尚不清楚。本研究旨在探究神经网络在压缩计算场景下学习电路的能力和行为。

Method: 通过分析一个用于解决通用AND问题的玩具模型，该模型计算m个稀疏输入的m choose 2对的AND。限制隐藏维度以强制模型寻找一种名为压缩计算的计算效率高的电路。通过实验观察和分析训练过程找到的解决方案。

Result: 训练过程发现了一个与理论构造不同的简单、完全稠密的解决方案，其中每个神经元都对每个输出做出贡献。该解决方案能够自然地随维度扩展，并通过权衡错误率来提高神经元效率，并且对稀疏度等参数具有鲁棒性，还可以扩展到其他布尔运算和电路。

Conclusion: 该研究揭示了神经网络在处理压缩计算时的行为，发现模型倾向于学习一种完全稠密的解决方案，该方案与理论构造不同，但具有良好的可扩展性和鲁棒性，并在低稀疏度情况下比理论构造更有效率，为理解网络电路和可解释性提供了新的视角。

Abstract: Neural networks are capable of superposition -- representing more features
than there are dimensions. Recent work considers the analogous concept for
computation instead of storage, proposing theoretical constructions. But there
has been little investigation into whether these circuits can be learned in
practice. In this work, we investigate a toy model for the Universal-AND
problem which computes the AND of all $m\choose 2$ pairs of $m$ sparse inputs.
The hidden dimension that determines the number of non-linear activations is
restricted to pressure the model to find a compute-efficient circuit, called
compressed computation. We find that the training process finds a simple
solution that does not correspond to theoretical constructions. It is fully
dense -- every neuron contributes to every output. The solution circuit
naturally scales with dimension, trading off error rates for neuron efficiency.
It is similarly robust to changes in sparsity and other key parameters, and
extends naturally to other boolean operations and boolean circuits. We explain
the found solution in detail and compute why it is more efficient than the
theoretical constructions at low sparsity. Our findings shed light on the types
of circuits that models like to form and the flexibility of the superposition
representation. This contributes to a broader understanding of network
circuitry and interpretability.

</details>


### [413] [Bridging Neural Networks and Dynamic Time Warping for Adaptive Time Series Classification](https://arxiv.org/abs/2507.09826)
*Jintao Qu,Zichong Wang,Chenhao Wu,Wenbin Zhang*

Main category: cs.LG

TL;DR: 提出了一种新型时间序列分类模型，它结合了DTW的可解释性和神经网络的可训练性，能够在数据量有限和数据量充足的场景下都表现良好。


<details>
  <summary>Details</summary>
Motivation: 旨在开发一个多功能模型，能够适应冷启动条件并随着标记数据的可用性进行训练，同时保持可解释性。

Method: 提出了一种动态长度缩短算法，将时间序列转化为原型，同时保留关键结构模式，从而能够将动态时间规整（DTW）的递推关系重构为等效的循环神经网络（RNN），并构建了一个可训练的模型来模仿DTW的对齐行为。

Result: 成功地将DTW的递推关系重构为等效的循环神经网络（RNN），并构建了一个可训练的模型来模仿DTW的对齐行为。

Conclusion: 该模型在低资源设置下显著优于先前的方法，并在富资源设置下保持竞争力。

Abstract: Neural networks have achieved remarkable success in time series
classification, but their reliance on large amounts of labeled data for
training limits their applicability in cold-start scenarios. Moreover, they
lack interpretability, reducing transparency in decision-making. In contrast,
dynamic time warping (DTW) combined with a nearest neighbor classifier is
widely used for its effectiveness in limited-data settings and its inherent
interpretability. However, as a non-parametric method, it is not trainable and
cannot leverage large amounts of labeled data, making it less effective than
neural networks in rich-resource scenarios. In this work, we aim to develop a
versatile model that adapts to cold-start conditions and becomes trainable with
labeled data, while maintaining interpretability. We propose a dynamic
length-shortening algorithm that transforms time series into prototypes while
preserving key structural patterns, thereby enabling the reformulation of the
DTW recurrence relation into an equivalent recurrent neural network. Based on
this, we construct a trainable model that mimics DTW's alignment behavior. As a
neural network, it becomes trainable when sufficient labeled data is available,
while still retaining DTW's inherent interpretability. We apply the model to
several benchmark time series classification tasks and observe that it
significantly outperforms previous approaches in low-resource settings and
remains competitive in rich-resource settings.

</details>


### [414] [Generative Cognitive Diagnosis](https://arxiv.org/abs/2507.09831)
*Jiatong Li,Qi Liu,Mengxiao Zhu*

Main category: cs.LG

TL;DR: 本研究提出了一种新颖的生成式诊断范式，通过 G-IRT 和 G-NCDM 模型解决了传统认知诊断方法在可扩展性和可靠性方面的局限性，显著提高了诊断效率。


<details>
  <summary>Details</summary>
Motivation: 传统的认知诊断模型（CD）通常采用归纳预测范式，需要针对新学习者进行计算成本高昂的重新训练，并且诊断输出的可靠性有限。

Method: 提出了一种新颖的生成式诊断范式，并提出了生成项目反应理论（G-IRT）和生成神经认知诊断模型（G-NCDM）两个具体实现，该范式通过包含可识别性和单调性条件的生成过程将认知状态推理与响应预测解耦。

Result: 实验结果表明，与传统方法相比，生成式诊断方法在可扩展性和可靠性方面取得了显著的性能提升，尤其是在诊断新学习者方面实现了 100 倍的速度提升。

Conclusion: 本研究提出的生成式诊断方法在可扩展性和可靠性方面优于传统方法，并且在诊断新学习者时速度提高了 100 倍，为人工智能在认知诊断领域的应用开辟了新途径。

Abstract: Cognitive diagnosis (CD) models latent cognitive states of human learners by
analyzing their response patterns on diagnostic tests, serving as a crucial
machine learning technique for educational assessment and evaluation.
Traditional cognitive diagnosis models typically follow a transductive
prediction paradigm that optimizes parameters to fit response scores and
extract learner abilities. These approaches face significant limitations as
they cannot perform instant diagnosis for new learners without computationally
expensive retraining and produce diagnostic outputs with limited reliability.
In this study, we introduces a novel generative diagnosis paradigm that
fundamentally shifts CD from predictive to generative modeling, enabling
inductive inference of cognitive states without parameter re-optimization. We
propose two simple yet effective instantiations of this paradigm: Generative
Item Response Theory (G-IRT) and Generative Neural Cognitive Diagnosis Model
(G-NCDM), which achieve excellent performance improvements over traditional
methods. The generative approach disentangles cognitive state inference from
response prediction through a well-designed generation process that
incorporates identifiability and monotonicity conditions. Extensive experiments
on real-world datasets demonstrate the effectiveness of our methodology in
addressing scalability and reliability challenges, especially $\times 100$
speedup for the diagnosis of new learners. Our framework opens new avenues for
cognitive diagnosis applications in artificial intelligence, particularly for
intelligent model evaluation and intelligent education systems. The code is
available at https://github.com/CSLiJT/Generative-CD.git.

</details>


### [415] [A Pre-training Framework for Relational Data with Information-theoretic Principles](https://arxiv.org/abs/2507.09837)
*Quang Truong,Zhikai Chen,Mingxuan Ju,Tong Zhao,Neil Shah,Jiliang Tang*

Main category: cs.LG

TL;DR: TVE预训练框架通过结合模式遍历图和时间动态，为关系数据库学习任务感知表示，并在实验中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 关系数据库作为关键基础设施，在多种领域都有广泛应用。然而，由于任务异质性（任务基于关系模式图、时间依赖和SQL定义的标签逻辑而定义），学习关系数据库的通用预训练策略仍然是一个开放的挑战。需要一个能考虑这些因素以获得面向任务表示的有效预训练框架。通过结合标签生成的基础分布知识，可以从相关的侧信道信息中获益，从而使下游任务受益。

Method: 作者提出了一种名为任务向量估计（TVE）的新型预训练框架，该框架通过模式遍历图上的集合聚合来构建预测性监督信号，并显式地对下一窗口关系动态进行建模。研究从信息论的角度对该方法进行了形式化，证明了包含任务信息表示比不包含任务先验信息表示保留了更多相关信号。

Result: 在RelBench基准上的广泛实验表明，TVE持续优于传统的预训练基线。

Conclusion: 研究结果表明，包含任务异质性和时间结构的目标可以作为关系数据库预测建模的设计原则。

Abstract: Relational databases underpin critical infrastructure across a wide range of
domains, yet the design of generalizable pre-training strategies for learning
from relational databases remains an open challenge due to task heterogeneity.
Specifically, there exist infinitely many possible downstream tasks, as tasks
are defined based on relational schema graphs, temporal dependencies, and
SQL-defined label logics. An effective pre-training framework is desired to
take these factors into account in order to obtain task-aware representations.
By incorporating knowledge of the underlying distribution that drives label
generation, downstream tasks can benefit from relevant side-channel
information. To bridge this gap, we introduce Task Vector Estimation (TVE), a
novel pre-training framework that constructs predictive supervisory signals via
set-based aggregation over schema traversal graphs, explicitly modeling
next-window relational dynamics. We formalize our approach through an
information-theoretic lens, demonstrating that task-informed representations
retain more relevant signals than those obtained without task priors. Extensive
experiments on the RelBench benchmark show that TVE consistently outperforms
traditional pre-training baselines. Our findings advocate for pre-training
objectives that encode task heterogeneity and temporal structure as design
principles for predictive modeling on relational databases.

</details>


### [416] [Rethinking Prompt Optimization: Reinforcement, Diversification, and Migration in Blackbox LLMs](https://arxiv.org/abs/2507.09839)
*MohammadReza Davari,Utkarsh Garg,Weixin Cai,Eugene Belilovsky*

Main category: cs.LG

TL;DR: 本文提出了一种新的自动提示优化框架，通过结合正强化和反馈多样化来改进提示优化。此外，还引入了持续提示优化（CPO）来解决提示迁移问题。实验结果表明，该方法在准确性、收敛速度和计算成本方面均优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有自动提示优化（APO）方法主要关注错误纠正，忽视了正确预测提供的宝贵见解，这限制了其有效性和效率。此外，随着 LLM 的快速发展和多样化，将优化后的提示高效地迁移到不同模型版本或 API 提供商面临实际挑战。

Method: 本文提出了一种新的自动提示优化（APO）框架，该框架通过以下方式改进了提示优化过程：1. 增强反馈机制：将文本梯度重新解释为负强化，并引入正强化来保留通过成功预测识别出的有益提示组件。2. 反馈多样化：通过聚合多个反馈信号来减轻 LLM 生成的反馈中的噪声，强调一致的、可操作的建议，同时过滤掉异常值。3. 持续提示优化（CPO）：将提示优化扩展到跨不同模型版本或 API 提供商迁移优化的提示，以应对LLM的快速发展和多样性。

Result: 实验表明，本文提出的框架在准确性、收敛速度和计算成本方面持续优于强基线，在标准和迁移场景下均取得了显著改进。与此形成对比的是，简单的提示迁移通常会导致性能下降。

Conclusion: 所提出的框架通过引入正强化来增强反馈机制，并通过反馈多样化技术来减轻噪声，在保持和优化提示方面取得了显著效果。该框架还通过形式化持续提示优化（CPO）解决了提示在不同模型版本之间迁移的实际挑战，并在各种场景下显著优于现有基线。

Abstract: An increasing number of NLP applications interact with large language models
(LLMs) through black-box APIs, making prompt engineering critical for
controlling model outputs. While recent Automatic Prompt Optimization (APO)
methods iteratively refine prompts using model-generated feedback, textual
gradients, they primarily focus on error correction and neglect valuable
insights from correct predictions. This limits both their effectiveness and
efficiency. In this paper, we propose a novel APO framework centered on
enhancing the feedback mechanism. We reinterpret the textual gradient as a form
of negative reinforcement and introduce the complementary positive
reinforcement to explicitly preserve beneficial prompt components identified
through successful predictions. To mitigate the noise inherent in LLM-generated
feedback, we introduce a technique called feedback diversification, which
aggregates multiple feedback signals, emphasizing consistent, actionable advice
while filtering out outliers. Motivated by the rapid evolution and diversity of
available LLMs, we also formalize Continual Prompt Optimization (CPO),
addressing the practical challenge of efficiently migrating optimized prompts
between different model versions or API providers. Our experiments reveal that
naive prompt migration often degrades performance due to loss of critical
instructions. In contrast, our approach consistently outperforms strong
baselines, achieving significant accuracy improvements, faster convergence, and
lower computational costs in both standard and migration scenarios.

</details>


### [417] [Through the River: Understanding the Benefit of Schedule-Free Methods for Language Model Training](https://arxiv.org/abs/2507.09846)
*Minhak Song,Beomhan Baek,Kwangjun Ahn,Chulhee Yun*

Main category: cs.LG

TL;DR: SF-AdamW 是一种无需衰减或额外内存即可有效训练大型语言模型的改进方法，它通过隐式权重平均来导航损失函数。


<details>
  <summary>Details</summary>
Motivation: 传统的具有固定计算预算的预训练策略（如余弦学习率计划）对于大规模训练来说越来越不够用。虽然 WSD 和权重平均提供了更大的灵活性，但 WSD 依赖于显式的衰减阶段，而权重平均则需要额外的内存。在寻找更合理、更具可扩展性的替代方案时，我们重新审视了 Schedule-Free (SF) 方法。

Method: 通过理论和实证分析 SF 动态，揭示其在没有内存开销的情况下隐式执行权重平均。提出一种改进的 SF 变体，以提高对动量的鲁棒性，并在大批量尺寸下表现更好。

Result: SF-AdamW 有效地导航损失函数的“河流”结构，无需衰减阶段或辅助平均，并且隐式执行权重平均而没有内存开销。改进的 SF 变体提高了对动量的鲁棒性，并在大批量尺寸下表现更好。

Conclusion: SF-AdamW 是一种实用、可扩展且具有理论基础的方法，适用于语言模型训练，在不使用衰减阶段或辅助平均的情况下有效导航损失函数的“河流”结构，特别适合持续扩展训练工作负载。

Abstract: As both model and dataset sizes continue to scale rapidly, conventional
pretraining strategies with fixed compute budgets-such as cosine learning rate
schedules-are increasingly inadequate for large-scale training. Recent
alternatives, including warmup-stable-decay (WSD) schedules and weight
averaging, offer greater flexibility. However, WSD relies on explicit decay
phases to track progress, while weight averaging addresses this limitation at
the cost of additional memory. In search of a more principled and scalable
alternative, we revisit the Schedule-Free (SF) method [Defazio et al., 2024],
which has shown strong empirical performance across diverse settings. We show
that SF-AdamW effectively navigates the "river" structure of the loss landscape
without decay phases or auxiliary averaging, making it particularly suitable
for continuously scaling training workloads. To understand this behavior, we
conduct a theoretical and empirical analysis of SF dynamics, revealing that it
implicitly performs weight averaging without memory overhead. Guided by this
analysis, we propose a refined variant of SF that improves robustness to
momentum and performs better under large batch sizes, addressing key
limitations of the original method. Together, these results establish SF as a
practical, scalable, and theoretically grounded approach for language model
training.

</details>


### [418] [Task Priors: Enhancing Model Evaluation by Considering the Entire Space of Downstream Tasks](https://arxiv.org/abs/2507.09871)
*Niket Patel,Randall Balestriero*

Main category: cs.LG

TL;DR: 当前AI评估方法过于依赖固定基准，限制了SSL研究进展。我们提出了一个概率性任务空间和任务先验的概念，用于在所有可能的下游任务上评估模型性能，解决了现有评估方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前人工智能研究，特别是自监督学习（SSL）的评估方法依赖于固定的、手动选择的下游基准测试集，这限制了研究的进展。作者认为这种僵化的评估协议是研究中的一个“无声瓶颈”。

Method: 提出了一种概率性任务空间的概念，该空间通过采用任务分布和定义任务先验来实现。这允许评估模型在所有可能的下游任务上的平均性能和性能方差。

Result: 该框架首次能够回答关于模型在所有可能的下游任务上的平均性能以及在特定任务先验下的性能方差等关键问题。作者相信该框架将加速SSL领域的研究。

Conclusion: 为人工智能研究（尤其是自监督学习）提供了一个新的评估框架，允许模型在整个可能的下游任务空间中进行评估，而不是依赖于固定的基准测试集。

Abstract: The grand goal of AI research, and particularly Self Supervised Learning
(SSL), is to produce systems that can successfully solve any possible task. In
contrast, current evaluation methods available to AI researchers typically rely
on a fixed collection of hand-picked downstream benchmarks. Hence, a large
amount of effort is put into designing and searching for large collection of
evaluation tasks that can serve as a proxy of our grand goal. We argue that
such a rigid evaluation protocol creates a silent bottleneck in AI research. To
remedy that, we define a probabilistic space of downstream tasks obtained by
adopting a distribution of tasks and by defining Task Priors. Under this view,
one can evaluate a model's performance over the set of all possible downstream
tasks. Our framework is the first to provide answers to key questions such as
(i) what is the average performance of my model over all possible downstream
tasks weighted by the probability to encounter each task? or (ii) what is the
variance of my model's performance across all downstream tasks under the
defined Task Priors? Beyond establishing a new standard for evaluation, we
believe that Task Priors will accelerate the pace of research in SSL - where
downstream task evaluation is the sole qualitative signal that researchers have
access to.

</details>


### [419] [AdaBrain-Bench: Benchmarking Brain Foundation Models for Brain-Computer Interface Applications](https://arxiv.org/abs/2507.09882)
*Jiamin Wu,Zichen Ren,Junyu Wang,Pengyu Zhu,Yonghao Song,Mianxin Liu,Qihao Zheng,Lei Bai,Wanli Ouyang,Chunfeng Song*

Main category: cs.LG

TL;DR: AdaBrain-Bench 是一个用于评估非侵入性 BCI 脑部基础模型的标准化基准，解决了现有评估方法的局限性，并为选择和改进模型提供了指导。


<details>
  <summary>Details</summary>
Motivation: 现有非侵入性 BCI 研究缺乏全面的、实用的和可扩展的基准来评估公共基础模型在不同 BCI 任务中的效用，这阻碍了它们的广泛应用。

Method: AdaBrain-Bench 是一个大规模标准化基准，包含代表性的 BCI 解码数据集（涵盖 7 个关键应用），并引入了一个简化的任务适应流程，集成了多维度评估指标和一套适应工具，用于评估脑部基础模型在跨受试者、多受试者和少样本场景下的泛化能力。

Result: 该基准成功评估了一系列公开的脑部基础模型，并提供了关于在各种场景下选择合适模型的实践见解。该基准框架为评估脑部基础模型在关键迁移场景中的泛化能力提供了包容性。

Conclusion: AdaBrain-Bench 的发布将为评估和选择非侵入性 BCI 领域的脑部基础模型提供一个标准化、可扩展且实用的基准。通过解决现有评估方法的局限性，该基准有助于推动可复现的研究，并为开发更强大的通用神经解码解决方案做出贡献。

Abstract: Non-invasive Brain-Computer Interfaces (BCI) offer a safe and accessible
means of connecting the human brain to external devices, with broad
applications in home and clinical settings to enhance human capabilities.
However, the high noise level and limited task-specific data in non-invasive
signals constrain decoding capabilities. Recently, the adoption of
self-supervised pre-training is transforming the landscape of non-invasive BCI
research, enabling the development of brain foundation models to capture
generic neural representations from large-scale unlabeled
electroencephalography (EEG) signals with substantial noises. However, despite
these advances, the field currently lacks comprehensive, practical and
extensible benchmarks to assess the utility of the public foundation models
across diverse BCI tasks, hindering their widespread adoption. To address this
challenge, we present AdaBrain-Bench, a large-scale standardized benchmark to
systematically evaluate brain foundation models in widespread non-invasive BCI
tasks. AdaBrain-Bench encompasses a diverse collection of representative BCI
decoding datasets spanning 7 key applications. It introduces a streamlined task
adaptation pipeline integrated with multi-dimensional evaluation metrics and a
set of adaptation tools. The benchmark delivers an inclusive framework for
assessing generalizability of brain foundation models across key transfer
settings, including cross-subject, multi-subject, and few-shot scenarios. We
leverage AdaBrain-Bench to evaluate a suite of publicly available brain
foundation models and offer insights into practices for selecting appropriate
models in various scenarios. We make our benchmark pipeline available to enable
reproducible research and external use, offering a continuously evolving
platform to foster progress toward robust and generalized neural decoding
solutions.

</details>


### [420] [NeuTSFlow: Modeling Continuous Functions Behind Time Series Forecasting](https://arxiv.org/abs/2507.09888)
*Huibo Xu,Likang Wu,Xianquan Wang,Haoning Dang,Chun-Wun Cheng,Angelica I Aviles-Rivero,Qi Liu*

Main category: cs.LG

TL;DR: NeuTSFlow是一个新框架，通过将时间序列视为连续函数族，并学习函数族之间的过渡路径来改进预测。它使用神经算子和流匹配来直接建模函数级特征，并在实验中显示出优于传统方法的性能。


<details>
  <summary>Details</summary>
Motivation: 时间序列预测通常将数据视为离散序列，忽略了其作为连续过程噪声样本的本质。离散噪声观测无法唯一确定连续函数，因此提出了一种将时间序列视为由共享概率测度决定的连续函数族噪声观测的新视角，并将预测任务重构为学习从历史函数族到未来函数族的过渡。

Method: NeuTSFlow框架利用神经算子促进流匹配，以学习历史和未来函数族之间的度量路径。它通过参数化无限维度函数空间中的速度场，直接建模函数级特征，超越了传统关注离散点依赖关系的方法。

Result: NeuTSFlow框架能够有效处理时间序列预测问题，提高了预测的准确性和鲁棒性。

Conclusion: 该研究提出的NeuTSFlow框架在各种预测任务上展现了优越的准确性和鲁棒性，验证了函数族视角在时间序列预测中的有效性。

Abstract: Time series forecasting is a fundamental task with broad applications, yet
conventional methods often treat data as discrete sequences, overlooking their
origin as noisy samples of continuous processes. Crucially, discrete noisy
observations cannot uniquely determine a continuous function; instead, they
correspond to a family of plausible functions. Mathematically, time series can
be viewed as noisy observations of a continuous function family governed by a
shared probability measure. Thus, the forecasting task can be framed as
learning the transition from the historical function family to the future
function family. This reframing introduces two key challenges: (1) How can we
leverage discrete historical and future observations to learn the relationships
between their underlying continuous functions? (2) How can we model the
transition path in function space from the historical function family to the
future function family? To address these challenges, we propose NeuTSFlow, a
novel framework that leverages Neural Operators to facilitate flow matching for
learning path of measure between historical and future function families. By
parameterizing the velocity field of the flow in infinite-dimensional function
spaces, NeuTSFlow moves beyond traditional methods that focus on dependencies
at discrete points, directly modeling function-level features instead.
Experiments on diverse forecasting tasks demonstrate NeuTSFlow's superior
accuracy and robustness, validating the effectiveness of the function-family
perspective.

</details>


### [421] [Soft Graph Clustering for single-cell RNA Sequencing Data](https://arxiv.org/abs/2507.09890)
*Ping Xu,Pengfei Wang,Zhiyuan Ning,Meng Xiao,Min Wu,Yuanchun Zhou*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Clustering analysis is fundamental in single-cell RNA sequencing (scRNA-seq)
data analysis for elucidating cellular heterogeneity and diversity. Recent
graph-based scRNA-seq clustering methods, particularly graph neural networks
(GNNs), have significantly improved in tackling the challenges of
high-dimension, high-sparsity, and frequent dropout events that lead to
ambiguous cell population boundaries. However, their reliance on hard graph
constructions derived from thresholded similarity matrices presents
challenges:(i) The simplification of intercellular relationships into binary
edges (0 or 1) by applying thresholds, which restricts the capture of
continuous similarity features among cells and leads to significant information
loss.(ii) The presence of significant inter-cluster connections within hard
graphs, which can confuse GNN methods that rely heavily on graph structures,
potentially causing erroneous message propagation and biased clustering
outcomes. To tackle these challenges, we introduce scSGC, a Soft Graph
Clustering for single-cell RNA sequencing data, which aims to more accurately
characterize continuous similarities among cells through non-binary edge
weights, thereby mitigating the limitations of rigid data structures. The scSGC
framework comprises three core components: (i) a zero-inflated negative
binomial (ZINB)-based feature autoencoder; (ii) a dual-channel cut-informed
soft graph embedding module; and (iii) an optimal transport-based clustering
optimization module. Extensive experiments across ten datasets demonstrate that
scSGC outperforms 13 state-of-the-art clustering models in clustering accuracy,
cell type annotation, and computational efficiency. These results highlight its
substantial potential to advance scRNA-seq data analysis and deepen our
understanding of cellular heterogeneity.

</details>


### [422] [Algorithm Development in Neural Networks: Insights from the Streaming Parity Task](https://arxiv.org/abs/2507.09897)
*Loek van Rossem,Andrew M. Saxe*

Main category: cs.LG

TL;DR: 深度神经网络（包括RNN）可以通过学习动态，在有限的训练数据下实现对任务的无限泛化，其机制是构建出能够精确执行任务的有限自动机。


<details>
  <summary>Details</summary>
Motivation: 探究深度神经网络在某些情况下能够从训练数据的边界外推到数据，甚至实现无限泛化的现象，并为神经网络的算法开发提供理论基础。

Method: 通过对循环神经网络（RNN）在流式奇偶校验任务上的学习动态进行案例研究，并运用有效的表征动态理论进行分析。

Result: 在充分的有限训练经验下，循环神经网络（RNN）表现出向完美无限泛化的阶段性转变。。

Conclusion: 这项研究揭示了循环神经网络（RNN）在处理流式奇偶校验任务时，通过隐式表征合并效应，能够构建出能够精确复现该任务的有限自动机，从而在有限的训练经验下实现无限泛化。

Abstract: Even when massively overparameterized, deep neural networks show a remarkable
ability to generalize. Research on this phenomenon has focused on
generalization within distribution, via smooth interpolation. Yet in some
settings neural networks also learn to extrapolate to data far beyond the
bounds of the original training set, sometimes even allowing for infinite
generalization, implying that an algorithm capable of solving the task has been
learned. Here we undertake a case study of the learning dynamics of recurrent
neural networks (RNNs) trained on the streaming parity task in order to develop
an effective theory of algorithm development. The streaming parity task is a
simple but nonlinear task defined on sequences up to arbitrary length. We show
that, with sufficient finite training experience, RNNs exhibit a phase
transition to perfect infinite generalization. Using an effective theory for
the representational dynamics, we find an implicit representational merger
effect which can be interpreted as the construction of a finite automaton that
reproduces the task. Overall, our results disclose one mechanism by which
neural networks can generalize infinitely from finite training experience.

</details>


### [423] [Extracting Cause-Effect Pairs from a Sentence with a Dependency-Aware Transformer Model](https://arxiv.org/abs/2507.09925)
*Md Ahsanul Kabir,Abrar Jahin,Mohammad Al Hasan*

Main category: cs.LG

TL;DR: A new model called DepBERT incorporates dependency trees into transformer models to improve the extraction of cause and effect phrases, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Extracting cause and effect phrases from a sentence is an important NLP task, with numerous applications in various domains. Existing supervised methods do not have any provision for utilizing linguistic tools, specifically dependency tree, which has been shown to be very effective for extracting semantic pairs.

Method: Extending a transformer-based model by incorporating dependency tree of a sentence within the model framework.

Result: Extensive experiments over three datasets show that DepBERT is better than various state-of-the art supervised causality extraction methods.

Conclusion: DepBERT, which extends a transformer-based model by incorporating dependency tree of a sentence within the model framework, is better than various state-of-the art supervised causality extraction methods.

Abstract: Extracting cause and effect phrases from a sentence is an important NLP task,
with numerous applications in various domains, including legal, medical,
education, and scientific research. There are many unsupervised and supervised
methods proposed for solving this task. Among these, unsupervised methods
utilize various linguistic tools, including syntactic patterns, dependency
tree, dependency relations, etc. among different sentential units for
extracting the cause and effect phrases. On the other hand, the contemporary
supervised methods use various deep learning based mask language models
equipped with a token classification layer for extracting cause and effect
phrases. Linguistic tools, specifically, dependency tree, which organizes a
sentence into different semantic units have been shown to be very effective for
extracting semantic pairs from a sentence, but existing supervised methods do
not have any provision for utilizing such tools within their model framework.
In this work, we propose DepBERT, which extends a transformer-based model by
incorporating dependency tree of a sentence within the model framework.
Extensive experiments over three datasets show that DepBERT is better than
various state-of-the art supervised causality extraction methods.

</details>


### [424] [Mechanistic Interpretability of LoRA-Adapted Language Models for Nuclear Reactor Safety Applications](https://arxiv.org/abs/2507.09931)
*Yoon Pyo Lee*

Main category: cs.LG

TL;DR: 本研究提出了一种新方法，通过分析和操作神经元，来理解和改进大型语言模型在核工程领域的应用。实验表明，特定神经元的组合对模型性能至关重要，为提高 AI 在安全关键领域的可靠性提供了新思路。


<details>
  <summary>Details</summary>
Motivation: 为了将大型语言模型（LLM）集成到核工程等安全关键领域，需要深入理解其内部推理过程。本研究旨在提供一种方法，以增强模型的透明度，解决核监管框架下的验证和验证挑战，从而促进人工智能在安全关键核操作中的应用。

Method: 本研究采用低秩自适应（LoRA）技术对通用语言模型（Gemma-3-1b-it）进行参数高效微调，使其适应核工程领域。通过比较微调前后模型的神经元激活模式，并使用神经元沉默技术来探究特定神经元对模型行为的因果影响。

Result: 研究结果表明，在核工程领域微调后的模型中，存在一组特定的稀疏神经元，它们在模型适应过程中行为发生显著改变。虽然单独沉默这些神经元对模型性能的影响不显著，但集体沉默会导致模型在处理核工程任务时的性能显著下降，并且影响其生成详细、准确的技术信息的能力。

Conclusion: 本研究提出了一种新颖的方法，用于解释大型语言模型如何编码和利用特定领域的知识。通过比较基础模型和经过微调的模型（使用低秩自适应技术适配到核工程领域）的神经元激活模式，我们识别出一组稀疏的神经元，它们在适配过程中行为发生显著改变。神经元沉默实验表明，单独沉默这些神经元对模型性能影响不大，但集体沉默会导致性能显著下降，并且模型生成技术信息的能力受损。本研究为增强不透明黑盒模型的透明度提供了一种具体方法，可以将领域专业知识追溯到可验证的神经回路，为实现核级人工智能保证铺平道路，解决了核监管框架（如 10 CFR 50 Appendix B）强制要求的核查和验证挑战。

Abstract: The integration of Large Language Models (LLMs) into safety-critical domains,
such as nuclear engineering, necessitates a deep understanding of their
internal reasoning processes. This paper presents a novel methodology for
interpreting how an LLM encodes and utilizes domain-specific knowledge, using a
Boiling Water Reactor system as a case study. We adapted a general-purpose LLM
(Gemma-3-1b-it) to the nuclear domain using a parameter-efficient fine-tuning
technique known as Low-Rank Adaptation. By comparing the neuron activation
patterns of the base model to those of the fine-tuned model, we identified a
sparse set of neurons whose behavior was significantly altered during the
adaptation process. To probe the causal role of these specialized neurons, we
employed a neuron silencing technique. Our results demonstrate that while
silencing most of these specialized neurons individually did not produce a
statistically significant effect, deactivating the entire group collectively
led to a statistically significant degradation in task performance. Qualitative
analysis further revealed that silencing these neurons impaired the model's
ability to generate detailed, contextually accurate technical information. This
paper provides a concrete methodology for enhancing the transparency of an
opaque black-box model, allowing domain expertise to be traced to verifiable
neural circuits. This offers a pathway towards achieving nuclear-grade
artificial intelligence (AI) assurance, addressing the verification and
validation challenges mandated by nuclear regulatory frameworks (e.g., 10 CFR
50 Appendix B), which have limited AI deployment in safety-critical nuclear
operations.

</details>


### [425] [Memorization Sinks: Isolating Memorization during LLM Training](https://arxiv.org/abs/2507.09937)
*Gaurav R. Ghosal,Pratyush Maini,Aditi Raghunathan*

Main category: cs.LG

TL;DR: 研究提出MemSinks范例，通过隔离记忆神经元来解决大型语言模型的隐私和版权问题，并在实验中取得了良好的效果。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型易于记忆重复序列带来的隐私和版权问题，并克服现有事后移除记忆信息方法的局限性。

Method: 提出了一种名为MemSinks的新范例，利用序列标识符来激活每个序列独特的记忆神经元，从而在设计上促进记忆的隔离。通过分析学习和遗忘的动态过程来论证其有效性。

Result: 在百万参数和百万词元的规模上成功实现了记忆内容的有效隔离和强大的泛化能力，证明了同时实现泛化和隔离是可行的。

Conclusion: 该研究提出了MemSinks新范例，通过设计促进记忆的隔离，从而在不损害通用语言能力的情况下，更容易地移除记忆内容。实验证明，MemSinks在百万参数和百万词元的规模上实现了有效的隔离和强大的泛化能力，这是首次在真实数据上证明同时实现泛化和隔离是可行的。

Abstract: Large language models are susceptible to memorizing repeated sequences,
posing privacy and copyright concerns. A popular mitigation strategy is to
remove memorized information from specific neurons post-hoc. However, such
approaches have shown limited success so far. In a controlled setting, we show
that the memorization of natural sequences (those that resemble linguistically
plausible text) become mechanistically entangled with general language
abilities, thereby becoming challenging to remove post-hoc. In this work, we
put forward a new paradigm of MemSinks that promotes isolation of memorization
by design. We leverage a sequence identifier that activates a unique set of
memorization neurons for each sequence across repetitions. By analyzing the
dynamics of learning and forgetting, we argue that MemSinks facilitates
isolation of memorized content, making it easier to remove without compromising
general language capabilities. We implement MemSinks at the billion-parameter
and billion-token scale, and observe both effective isolation and strong
generalization. To our knowledge, this is the first proof-of-concept on real
data demonstrating that simultaneous generalization and isolation is
achievable. We open-source our code at http://github.com/grghosal/MemSinks.

</details>


### [426] [Long-Tailed Data Classification by Increasing and Decreasing Neurons During Training](https://arxiv.org/abs/2507.09940)
*Taigo Sakai,Kazuhiro Hotta*

Main category: cs.LG

TL;DR: 通过动态增删神经元来应对类别不平衡问题，提升少数类识别率。


<details>
  <summary>Details</summary>
Motivation: 受生物学启发，人类海马体在学习过程中会持续生成和修剪神经元，这表明灵活分配容量可以提高性能。此外，实际应用中常见的数据类别不平衡问题会导致少数类别的识别准确率显著降低。

Method: 提出一种在训练过程中周期性地添加和移除神经元的方法，在保留从多数类别中学到的关键特征的同时，选择性地为代表性不足的类别增加神经元，从而动态调整容量。

Result: 实验结果表明，所提出的方法在三个不同的数据集和五个代表性模型上均优于固定大小的网络，并且在与其它不平衡处理技术结合使用时，准确率更高。

Conclusion: 该方法通过动态调整网络容量，在类别不平衡的数据集上有效提升了少数类别的识别准确率，并且最终网络大小和结构保持不变，易于部署。

Abstract: In conventional deep learning, the number of neurons typically remains fixed
during training. However, insights from biology suggest that the human
hippocampus undergoes continuous neuron generation and pruning of neurons over
the course of learning, implying that a flexible allocation of capacity can
contribute to enhance performance. Real-world datasets often exhibit class
imbalance situations where certain classes have far fewer samples than others,
leading to significantly reduce recognition accuracy for minority classes when
relying on fixed size networks.To address the challenge, we propose a method
that periodically adds and removes neurons during training, thereby boosting
representational power for minority classes. By retaining critical features
learned from majority classes while selectively increasing neurons for
underrepresented classes, our approach dynamically adjusts capacity during
training. Importantly, while the number of neurons changes throughout training,
the final network size and structure remain unchanged, ensuring efficiency and
compatibility with deployment.Furthermore, by experiments on three different
datasets and five representative models, we demonstrate that the proposed
method outperforms fixed size networks and shows even greater accuracy when
combined with other imbalance-handling techniques. Our results underscore the
effectiveness of dynamic, biologically inspired network designs in improving
performance on class-imbalanced data.

</details>


### [427] [Hierarchical Job Classification with Similarity Graph Integration](https://arxiv.org/abs/2507.09949)
*Md Ahsanul Kabir,Kareem Abdelfatah,Mohammed Korayem,Mohammad Al Hasan*

Main category: cs.LG

TL;DR: 该研究提出了一种新颖的表示学习和分类模型，通过利用职位数据的分层结构来提高在线招聘中的职位分类准确性，解决了传统方法的局限性，并在大规模数据集上取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 传统文本分类方法在利用作业数据的复杂关系，特别是行业类别的分层性质方面，往往力不从心。为了解决这些限制，本研究提出了一种新的方法。

Method: 提出了一种新颖的表示学习和分类模型，将职位和分层行业类别嵌入到潜在嵌入空间中。该模型整合了标准职业分类（SOC）系统和内部分层分类Carotene，以捕捉图和分层关系，从而提高分类准确性。

Result: 所提出的模型通过嵌入分层行业类别到共享的潜在空间来解决冷启动问题，并增强了候选人与工作机会的动态匹配。在大型职位发布数据集上进行的广泛实验表明，该模型能够有效利用分层结构和丰富的语义特征，并且性能显著优于现有方法。

Conclusion: 该研究提供了一个强大的框架来提高职位分类的准确性，支持招聘行业中更明智的决策。

Abstract: In the dynamic realm of online recruitment, accurate job classification is
paramount for optimizing job recommendation systems, search rankings, and labor
market analyses. As job markets evolve, the increasing complexity of job titles
and descriptions necessitates sophisticated models that can effectively
leverage intricate relationships within job data. Traditional text
classification methods often fall short, particularly due to their inability to
fully utilize the hierarchical nature of industry categories. To address these
limitations, we propose a novel representation learning and classification
model that embeds jobs and hierarchical industry categories into a latent
embedding space. Our model integrates the Standard Occupational Classification
(SOC) system and an in-house hierarchical taxonomy, Carotene, to capture both
graph and hierarchical relationships, thereby improving classification
accuracy. By embedding hierarchical industry categories into a shared latent
space, we tackle cold start issues and enhance the dynamic matching of
candidates to job opportunities. Extensive experimentation on a large-scale
dataset of job postings demonstrates the model's superior ability to leverage
hierarchical structures and rich semantic features, significantly outperforming
existing methods. This research provides a robust framework for improving job
classification accuracy, supporting more informed decision-making in the
recruitment industry.

</details>


### [428] [Radial Neighborhood Smoothing Recommender System](https://arxiv.org/abs/2507.09952)
*Zerui Zhang,Yumou Qiu*

Main category: cs.LG

TL;DR: 本研究提出了一种名为RNE的新方法，用于估计推荐系统中潜在空间的距离，通过利用观测矩阵的行列距离并进行校正，提高了邻域构建和数据填充的准确性，优于现有方法并在一定程度上解决了冷启动问题。


<details>
  <summary>Details</summary>
Motivation: 为了有效捕捉用户-用户、项目-项目和用户-项目关系，需要定义有意义且可衡量的潜在空间距离。本研究旨在通过将潜在空间距离与观测矩阵中的距离联系起来，为距离估计提供新的视角。

Method: 提出了一种基于观测矩阵的行和列距离来系统地近似估计潜在空间中的距离的方法，并通过引入基于经验方差估计器的校正来进一步优化估计精度。基于此，开发了径向邻域估计器（RNE），该估计器通过包含完全重叠和部分重叠的用户-项目对来构建邻域，并利用局部核回归进行邻域平滑以提高填充准确性。

Result: RNE在模拟和真实世界数据集的评估中表现优于现有的协同过滤和矩阵分解方法，并且在一定程度上缓解了“冷启动”问题。

Conclusion: 该研究提出了一种新颖的距离估计方法，名为径向邻域估计器（RNE），并在一系列模拟和真实数据集的评估中证明了其优越性，同时该方法还有助于缓解“冷启动”问题。

Abstract: Recommender systems inherently exhibit a low-rank structure in latent space.
A key challenge is to define meaningful and measurable distances in the latent
space to capture user-user, item-item, user-item relationships effectively. In
this work, we establish that distances in the latent space can be
systematically approximated using row-wise and column-wise distances in the
observed matrix, providing a novel perspective on distance estimation. To
refine the distance estimation, we introduce the correction based on empirical
variance estimator to account for noise-induced non-centrality. The novel
distance estimation enables a more structured approach to constructing
neighborhoods, leading to the Radial Neighborhood Estimator (RNE), which
constructs neighborhoods by including both overlapped and partially overlapped
user-item pairs and employs neighborhood smoothing via localized kernel
regression to improve imputation accuracy. We provide the theoretical
asymptotic analysis for the proposed estimator. We perform evaluations on both
simulated and real-world datasets, demonstrating that RNE achieves superior
performance compared to existing collaborative filtering and matrix
factorization methods. While our primary focus is on distance estimation in
latent space, we find that RNE also mitigates the ``cold-start'' problem.

</details>


### [429] [Rethinking Inductive Bias in Geographically Neural Network Weighted Regression](https://arxiv.org/abs/2507.09958)
*Zhenyuan Chen*

Main category: cs.LG

TL;DR: 本研究扩展了地理加权回归（GNNWR），引入了深度学习机制以改进空间非平稳性建模，并通过实验证明了其在处理复杂空间关系方面的优越性，同时强调了归纳偏倚与数据特性匹配的重要性。


<details>
  <summary>Details</summary>
Motivation: 研究归纳偏倚在空间回归模型中的关键作用，识别当前GNNWR方法在建模空间非平稳性方面的局限性，即固定距离方案和有限的归纳偏倚。

Method: 通过结合卷积神经网络、循环神经网络和Transformer的概念，将局部感受野、序列上下文和自注意力机制引入空间回归，并对GNNWR进行了泛化。

Result: 通过在具有不同异质性、噪声和样本量的合成空间数据集上进行广泛的基准测试，证明了所提出的GNNWR在捕捉非线性和复杂空间关系方面优于经典方法。模型性能高度依赖于数据特性，局部模型在高度异质或小样本情况下表现更好，而全局模型在更大、更同质的数据上表现更佳。

Conclusion: 所提出的方法通过结合卷积神经网络、循环神经网络和Transformer的概念，将局部感受野、序列上下文和自注意力机制引入空间回归，并对GNNWR进行了泛化，在捕捉非线性和复杂空间关系方面优于经典方法。模型性能高度依赖于数据特性，局部模型在高度异质或小样本情况下表现更好，而全局模型在更大、更同质的数据上表现更佳。

Abstract: Inductive bias is a key factor in spatial regression models, determining how
well a model can learn from limited data and capture spatial patterns. This
work revisits the inductive biases in Geographically Neural Network Weighted
Regression (GNNWR) and identifies limitations in current approaches for
modeling spatial non-stationarity. While GNNWR extends traditional
Geographically Weighted Regression by using neural networks to learn spatial
weighting functions, existing implementations are often restricted by fixed
distance-based schemes and limited inductive bias. We propose to generalize
GNNWR by incorporating concepts from convolutional neural networks, recurrent
neural networks, and transformers, introducing local receptive fields,
sequential context, and self-attention into spatial regression. Through
extensive benchmarking on synthetic spatial datasets with varying
heterogeneity, noise, and sample sizes, we show that GNNWR outperforms classic
methods in capturing nonlinear and complex spatial relationships. Our results
also reveal that model performance depends strongly on data characteristics,
with local models excelling in highly heterogeneous or small-sample scenarios,
and global models performing better with larger, more homogeneous data. These
findings highlight the importance of inductive bias in spatial modeling and
suggest future directions, including learnable spatial weighting functions,
hybrid neural architectures, and improved interpretability for models handling
non-stationary spatial data.

</details>


### [430] [Text-Driven Causal Representation Learning for Source-Free Domain Generalization](https://arxiv.org/abs/2507.09961)
*Lihua Zhou,Mao Ye,Nianxin Li,Shuaifeng Li,Jinlin Wu,Xiatian Zhu,Lei Deng,Hongbin Liu,Jiebo Luo,Zhen Lei*

Main category: cs.LG

TL;DR: TDCRL 将因果推理应用于源域无关的域泛化（SFDG），通过文本嵌入和因果干预网络来学习域不变特征，解决了现有方法的局限性，并在多个数据集上取得了优异成果。


<details>
  <summary>Details</summary>
Motivation: 现有的源域无关域泛化（SFDG）方法在处理特定域的混淆因素时存在局限性，这影响了它们的泛化能力。为了解决这个问题，TDCRL 被提了出来。

Method: TDCRL包含两个主要步骤：1. 使用数据增强生成风格词向量，并将其与类别信息结合生成文本嵌入，从而模拟视觉表征。2. 训练一个因果干预网络，并使用混淆词典来提取域不变特征。

Result: TDCRL 在 PACS、VLCS、OfficeHome 和 DomainNet 数据集上进行了广泛的实验，证明了其在 SFDG 任务上的有效性，并取得了最先进的性能。

Conclusion: TDCRL 是一种开创性的方法，它将因果推理集成到源域无关的域泛化（SFDG）设置中，通过文本提示模拟视觉表征，并利用因果干预网络和混淆词典提取域不变特征，在 PACS、VLCS、OfficeHome 和 DomainNet 等数据集上取得了最先进的性能。

Abstract: Deep learning often struggles when training and test data distributions
differ. Traditional domain generalization (DG) tackles this by including data
from multiple source domains, which is impractical due to expensive data
collection and annotation. Recent vision-language models like CLIP enable
source-free domain generalization (SFDG) by using text prompts to simulate
visual representations, reducing data demands. However, existing SFDG methods
struggle with domain-specific confounders, limiting their generalization
capabilities. To address this issue, we propose TDCRL
(\textbf{T}ext-\textbf{D}riven \textbf{C}ausal \textbf{R}epresentation
\textbf{L}earning), the first method to integrate causal inference into the
SFDG setting. TDCRL operates in two steps: first, it employs data augmentation
to generate style word vectors, combining them with class information to
generate text embeddings to simulate visual representations; second, it trains
a causal intervention network with a confounder dictionary to extract
domain-invariant features. Grounded in causal learning, our approach offers a
clear and effective mechanism to achieve robust, domain-invariant features,
ensuring robust generalization. Extensive experiments on PACS, VLCS,
OfficeHome, and DomainNet show state-of-the-art performance, proving TDCRL
effectiveness in SFDG.

</details>


### [431] [Compliance Minimization via Physics-Informed Gaussian Processes](https://arxiv.org/abs/2507.09968)
*Xiangyu Sun,Amin Yousefpour,Shirin Hosseinmardi,Ramin Bostanabad*

Main category: cs.LG

TL;DR: 该研究提出了一种新颖的、基于物理信息高斯过程的无网格框架，用于解决顺从性最小化问题。该方法通过参数化设计和状态变量，并利用具有特定架构的神经网络来控制设计复杂度，解决了现有方法的局限性。实验结果表明，该方法在生成高质量拓扑、提高效率和准确性方面优于传统方法和现有机器学习方法。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有机器学习方法在解决顺从性最小化（CM）问题时存在的特征边界差、成本高以及缺乏控制设计复杂度的系统机制等局限性。

Method: 提出了一种基于物理信息高斯过程（GPs）的无网格、同步框架。通过GP先验参数化设计和状态变量，并共享一个多输出神经网络（NN）作为均值函数。该NN基于参数化网格卷积注意力网络（PGCANs），以缓解频谱偏差问题并控制设计复杂度。通过同步最小化顺从性、总势能和体积分数约束残差来估计GP表示的所有参数。该损失函数排除了所有基于数据的残差，因为GPs自动满足它们。此外，还开发了基于课程学习和数值积分的计算方案来提高效率和鲁棒性。

Result: （1）生成超分辨率拓扑，收敛速度快；（2）实现比传统数值方法更低的顺从性和更少的灰度区域分数；（3）提供对细微尺度特征的控制；（4）优于竞争性的机器学习方法。

Conclusion: 该方法在超分辨率拓扑生成方面收敛速度快，在对比传统数值方法时，实现了更低的顺从性和更少的灰度区域分数，并能控制细微尺度的特征，同时优于竞争性的机器学习方法。

Abstract: Machine learning (ML) techniques have recently gained significant attention
for solving compliance minimization (CM) problems. However, these methods
typically provide poor feature boundaries, are very expensive, and lack a
systematic mechanism to control the design complexity. Herein, we address these
limitations by proposing a mesh-free and simultaneous framework based on
physics-informed Gaussian processes (GPs). In our approach, we parameterize the
design and state variables with GP priors which have independent kernels but
share a multi-output neural network (NN) as their mean function. The
architecture of this NN is based on Parametric Grid Convolutional Attention
Networks (PGCANs) which not only mitigate spectral bias issues, but also
provide an interpretable mechanism to control design complexity. We estimate
all the parameters of our GP-based representations by simultaneously minimizing
the compliance, total potential energy, and residual of volume fraction
constraint. Importantly, our loss function exclude all data-based residuals as
GPs automatically satisfy them. We also develop computational schemes based on
curriculum training and numerical integration to increase the efficiency and
robustness of our approach which is shown to (1) produce super-resolution
topologies with fast convergence, (2) achieve smaller compliance and less gray
area fraction compared to traditional numerical methods, (3) provide control
over fine-scale features, and (4) outperform competing ML-based methods.

</details>


### [432] [Forecasting Coccidioidomycosis (Valley Fever) in Arizona: A Graph Neural Network Approach](https://arxiv.org/abs/2507.10014)
*Ali Sarabi,Arash Sarabi,Hao Yan,Beckett Sterner,Petar Jevtić*

Main category: cs.LG

TL;DR: 该研究利用图神经网络（GNN）结合环境数据预测山谷热的发病率。


<details>
  <summary>Details</summary>
Motivation: 解决やすく

Method: 通过整合监测病例数据和环境预测因子（包括土壤条件、大气变量、农业指标和空气质量指标），并利用图结构探索影响疾病传播的变量之间的相关性，来开发图神经网络（GNN）模型。该模型还考虑了疾病进展中的关键延迟和复杂的 زمنية 依赖性。

Result: 结果表明，该GNN模型能够有效模拟山谷热的趋势，并揭示了导致疾病发病率的关键环境驱动因素。

Conclusion: 该研究为预测щихся“山谷热”发病率开发了首个图神经网络（GNN）模型，并结合了监测病例数据和环境预测因子，揭示了影响疾病传播的关键环境变量。

Abstract: Coccidioidomycosis, commonly known as Valley Fever, remains a significant
public health concern in endemic regions of the southwestern United States.
This study develops the first graph neural network (GNN) model for forecasting
Valley Fever incidence in Arizona. The model integrates surveillance case data
with environmental predictors using graph structures, including soil
conditions, atmospheric variables, agricultural indicators, and air quality
metrics. Our approach explores correlation-based relationships among variables
influencing disease transmission. The model captures critical delays in disease
progression through lagged effects, enhancing its capacity to reflect complex
temporal dependencies in disease ecology. Results demonstrate that the GNN
architecture effectively models Valley Fever trends and provides insights into
key environmental drivers of disease incidence. These findings can inform early
warning systems and guide resource allocation for disease prevention efforts in
high-risk areas.

</details>


### [433] [Towards Applying Large Language Models to Complement Single-Cell Foundation Models](https://arxiv.org/abs/2507.10039)
*Steven Palayew,Bo Wang,Gary Bader*

Main category: cs.LG

TL;DR: scMPT结合了scGPT和大型语言模型（LLM）的优势，在单细胞分析任务中表现优于单独的模型。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在单细胞研究中展现出潜力，但对其性能驱动因素的理解有限，并且现有研究多将其作为单细胞基础模型的替代品而非补充品。

Method: 提出scMPT模型，该模型利用scGPT和捕获生物学见解的LLM单细胞表示之间的协同作用。还尝试了其他融合方法。

Result: scMPT模型展示出比其组成部分更强、更一致的性能，并填补了它们在不同数据集上的性能差距。融合方法也显示出潜力。

Conclusion: 大型语言模型（LLM）可以作为单细胞基础模型的补充，并推动单细胞分析的改进。

Abstract: Single-cell foundation models such as scGPT represent a significant
advancement in single-cell omics, with an ability to achieve state-of-the-art
performance on various downstream biological tasks. However, these models are
inherently limited in that a vast amount of information in biology exists as
text, which they are unable to leverage. There have therefore been several
recent works that propose the use of LLMs as an alternative to single-cell
foundation models, achieving competitive results. However, there is little
understanding of what factors drive this performance, along with a strong focus
on using LLMs as an alternative, rather than complementary approach to
single-cell foundation models. In this study, we therefore investigate what
biological insights contribute toward the performance of LLMs when applied to
single-cell data, and introduce scMPT; a model which leverages synergies
between scGPT, and single-cell representations from LLMs that capture these
insights. scMPT demonstrates stronger, more consistent performance than either
of its component models, which frequently have large performance gaps between
each other across datasets. We also experiment with alternate fusion methods,
demonstrating the potential of combining specialized reasoning models with
scGPT to improve performance. This study ultimately showcases the potential for
LLMs to complement single-cell foundation models and drive improvements in
single-cell analysis.

</details>


### [434] [On the Efficiency of Training Robust Decision Trees](https://arxiv.org/abs/2507.10048)
*Benedict Gerlach,Marie Anastacio,Holger H. Hoos*

Main category: cs.LG

TL;DR: This paper analyzes the efficiency of training robust decision trees, finding verification time and training time are not correlated.


<details>
  <summary>Details</summary>
Motivation: To address the growing need for trustworthiness in machine learning and establish the efficiency and sustainability of robust training pipelines.

Method: The pipeline consists of three stages: automatic perturbation size selection using a novel algorithm, training state-of-the-art adversarial training methods and evaluating their training time and adversarial accuracy, and certifying the robustness of the trained models and investigating the time required for certification. The perturbation size can be estimated from smaller models for efficiency gains.

Result: The paper investigates the efficiency of each step in a simple pipeline for training adversarially robust decision trees. It introduces a new algorithm for automatic perturbation size selection, shows efficiency gains by estimating perturbation size from smaller models, evaluates state-of-the-art adversarial training methods based on training time and adversarial accuracy, and analyzes the verification time, finding it uncorrelated with training time.

Conclusion: The verification time is not correlated with the training time, which is critical for the efficiency of the full pipeline.

Abstract: As machine learning gets adopted into the industry quickly, trustworthiness
is increasingly in focus. Yet, efficiency and sustainability of robust training
pipelines still have to be established. In this work, we consider a simple
pipeline for training adversarially robust decision trees and investigate the
efficiency of each step. Our pipeline consists of three stages. Firstly, we
choose the perturbation size automatically for each dataset. For that, we
introduce a simple algorithm, instead of relying on intuition or prior work.
Moreover, we show that the perturbation size can be estimated from smaller
models than the one intended for full training, and thus significant gains in
efficiency can be achieved. Secondly, we train state-of-the-art adversarial
training methods and evaluate them regarding both their training time and
adversarial accuracy. Thirdly, we certify the robustness of each of the models
thus obtained and investigate the time required for this. We find that
verification time, which is critical to the efficiency of the full pipeline, is
not correlated with training time.

</details>


### [435] [Towards High Supervised Learning Utility Training Data Generation: Data Pruning and Column Reordering](https://arxiv.org/abs/2507.10088)
*Tung Sum Thomas Kwok,Zeyong Zhang,Chi-Hua Wang,Guang Cheng*

Main category: cs.LG

TL;DR: PRRO流水线通过数据剪枝和列重排技术，提升了合成数据的监督学习效用和类别分布匹配度，在多个数据集上实现了显著的预测性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有合成数据的监督学习效用低，主要由于类别不平衡加剧和数据关系被忽视。本研究旨在通过整合数据中心技术来优化合成数据的监督学习效用。

Method: PRRO是一个包含数据剪枝和列重排的流水线，数据剪枝用于指导生成器关注高信噪比的观测，以匹配类别分布；列重排用于调整生成器的数据建模结构以匹配监督学习模型。

Result: PRRO在22个公开数据集上的实验表明，使用PRRO生成的合成数据替换原始数据，平均性能提升26.74%，最高提升871.46%；作为原始数据附加时，平均提升6.13%，最高提升200.32%。在6个高度不平衡的数据集上，PRRO使生成器产生的合成数据类别分布与原始数据相似度提高了43%。

Conclusion: PRRO通过数据剪枝和列重排技术，解决了现有表格数据生成器在监督学习任务中存在的类别不平衡加剧和数据关系忽视问题，提高了合成数据的监督学习效用。实验证明，PRRO生成的合成数据在预测性能上相比未使用的生成器有显著提升，并且能更准确地匹配原始数据的类别分布，为数据合成与监督学习的结合提供了新的方向。

Abstract: Tabular data synthesis for supervised learning ('SL') model training is
gaining popularity in industries such as healthcare, finance, and retail.
Despite the progress made in tabular data generators, models trained with
synthetic data often underperform compared to those trained with original data.
This low SL utility of synthetic data stems from class imbalance exaggeration
and SL data relationship overlooked by tabular generator. To address these
challenges, we draw inspirations from techniques in emerging data-centric
artificial intelligence and elucidate Pruning and ReOrdering ('PRRO'), a novel
pipeline that integrates data-centric techniques into tabular data synthesis.
PRRO incorporates data pruning to guide the table generator towards
observations with high signal-to-noise ratio, ensuring that the class
distribution of synthetic data closely matches that of the original data.
Besides, PRRO employs a column reordering algorithm to align the data modeling
structure of generators with that of SL models. These two modules enable PRRO
to optimize SL utility of synthetic data. Empirical experiments on 22 public
datasets show that synthetic data generated using PRRO enhances predictive
performance compared to data generated without PRRO. Specifically, synthetic
replacement of original data yields an average improvement of 26.74% and up to
871.46% improvement using PRRO, while synthetic appendant to original data
results with PRRO-generated data results in an average improvement of 6.13% and
up to 200.32%. Furthermore, experiments on six highly imbalanced datasets show
that PRRO enables the generator to produce synthetic data with a class
distribution that resembles the original data more closely, achieving a
similarity improvement of 43%. Through PRRO, we foster a seamless integration
of data synthesis to subsequent SL prediction, promoting quality and accessible
data analysis.

</details>


### [436] [A Variance-Reduced Cubic-Regularized Newton for Policy Optimization](https://arxiv.org/abs/2507.10120)
*Cheng Sun,Zhen Zhang,Shaofu Yang*

Main category: cs.LG

TL;DR: 本研究提出了一种名为VR-CR-PN的新算法，用于策略优化，它通过结合方差缩减和二阶优化来克服现有方法的局限性，并在样本复杂度和分布偏移方面取得了显著的改进。


<details>
  <summary>Details</summary>
Motivation: 为了克服现有二阶方法在样本复杂度方面的不足或对重要性采样的过度依赖，本研究提出了一种新的方法。

Method: 提出了一种名为VR-CR-PN的方差缩减的立方正则化策略牛顿算法，并引入了一种新的期望回报函数Hessian估计器，该估计器具有与时间范围H无关的统一上限。

Result: VR-CR-PN算法达到了$	ilde{	ilde{\mathcal{O}}}(\epsilon^{-3})$的样本复杂度，优于先前$	ilde{	ilde{\mathcal{O}}}(\epsilon^{-3.5})$的结果，并且实现了与时间范围无关的样本复杂度。

Conclusion: 该研究首次将方差缩减与二阶策略优化相结合，解决了分布偏移问题，并在通用非凸条件下实现了已知的最优样本复杂度，且无需重要性采样。

Abstract: In this paper, we study a second-order approach to policy optimization in
reinforcement learning. Existing second-order methods often suffer from
suboptimal sample complexity or rely on unrealistic assumptions about
importance sampling. To overcome these limitations, we propose VR-CR-PN, a
variance-reduced cubic-regularized policy Newton algorithm. To the best of our
knowledge, this is the first algorithm that integrates Hessian-aided variance
reduction with second-order policy optimization, effectively addressing the
distribution shift problem and achieving best-known sample complexity under
general nonconvex conditions but without the need for importance sampling. We
theoretically establish that VR-CR-PN achieves a sample complexity of
$\tilde{\mathcal{O}}(\epsilon^{-3})$ to reach an $\epsilon$-second-order
stationary point, significantly improving upon the previous best result of
$\tilde{\mathcal{O}}(\epsilon^{-3.5})$ under comparable assumptions. As an
additional contribution, we introduce a novel Hessian estimator for the
expected return function, which admits a uniform upper bound independent of the
horizon length $H$, allowing the algorithm to achieve horizon-independent
sample complexity.

</details>


### [437] [Wavelet-Enhanced Neural ODE and Graph Attention for Interpretable Energy Forecasting](https://arxiv.org/abs/2507.10132)
*Usman Gani Joy*

Main category: cs.LG

TL;DR: 本文提出了一种结合Neural ODE、图注意力、小波变换和自适应频率学习的神经框架，用于能源需求和供应预测。该方法在多个数据集上优于现有技术，并能提高可解释性。


<details>
  <summary>Details</summary>
Motivation: 准确预测能源需求和供应对于优化可持续能源系统至关重要，但由于可再生能源的可变性和动态消费模式而面临挑战。

Method: 本文引入了一个神经框架，该框架集成了连续时间神经常微分方程（Neural ODE）、图注意力、多分辨率小波变换和频率的自适应学习，以解决时间序列预测问题。该模型采用鲁棒的ODE求解器（使用Runge-Kutta方法），并结合基于图的注意力和残差连接，以更好地理解结构和时间模式。通过基于小波的特征提取和自适应频率调制，它能熟练地捕获和建模不同、多尺度的时态动态。

Result: 该模型在七个多样化数据集上评估，包括电力变压器温度（ETTh1、ETTh2、ETTm1、ETTm2）以及废物、太阳能和水力发电（可再生能源），在各种预测指标上持续优于最先进的基线，证明了其在捕捉复杂时间依赖性方面的鲁棒性。

Conclusion: 该模型在七个多样化数据集上评估，包括电力变压器温度（ETTh1、ETTh2、ETTm1、ETTm2）以及废物、太阳能和水力发电（可再生能源），在各种预测指标上持续优于最先进的基线，证明了其在捕捉复杂时间依赖性方面的鲁棒性。此外，通过SHAP分析增强了模型的可解释性，适用于可持续能源应用。

Abstract: Accurate forecasting of energy demand and supply is critical for optimizing
sustainable energy systems, yet it is challenged by the variability of
renewable sources and dynamic consumption patterns. This paper introduces a
neural framework that integrates continuous-time Neural Ordinary Differential
Equations (Neural ODEs), graph attention, multi-resolution wavelet
transformations, and adaptive learning of frequencies to address the issues of
time series prediction. The model employs a robust ODE solver, using the
Runge-Kutta method, paired with graph-based attention and residual connections
to better understand both structural and temporal patterns. Through
wavelet-based feature extraction and adaptive frequency modulation, it adeptly
captures and models diverse, multi-scale temporal dynamics. When evaluated
across seven diverse datasets: ETTh1, ETTh2, ETTm1, ETTm2 (electricity
transformer temperature), and Waste, Solar, and Hydro (renewable energy), this
architecture consistently outperforms state-of-the-art baselines in various
forecasting metrics, proving its robustness in capturing complex temporal
dependencies. Furthermore, the model enhances interpretability through SHAP
analysis, making it suitable for sustainable energy applications.

</details>


### [438] [Understanding the Rank of Tensor Networks via an Intuitive Example-Driven Approach](https://arxiv.org/abs/2507.10170)
*Wuyang Zhou,Giorgos Iacovides,Kriton Konstantinidis,Ilya Kisil,Danilo Mandic*

Main category: cs.LG

TL;DR: 张量秩对于张量网络分解很重要，但很难理解。本讲义通过示例和图形解释了张量秩，并展示了如何使用领域知识来选择它们。这应该有助于人们更好地理解和使用张量方法。


<details>
  <summary>Details</summary>
Motivation: 张量秩在张量网络分解中至关重要，但缺乏普遍的意义和直观的解释，导致其常被视为经验调整的超参数。本讲义旨在解决这一问题，提供清晰的理解。

Method: 通过实际示例和直观可视化来说明张量秩，展示领域知识如何指导 CP 和 Tucker 等模型中张量秩的选择，并使用图形方法推广到任意顺序的张量，揭示张量秩与张量展开矩阵秩的关系。

Result: 通过直观的解释和图形方法，帮助读者理解张量秩的概念，并能够根据领域知识进行选择和解释。有助于张量方法在实际应用和教育中的部署。被视为“长话短说”摘要。

Conclusion: 本讲义旨在揭开张量秩的神秘面纱，提供清晰统一的理解，并为在实际应用和教育背景下选择、解释和部署张量方法提供必要的物理见解和直觉。

Abstract: Tensor Network (TN) decompositions have emerged as an indispensable tool in
Big Data analytics owing to their ability to provide compact low-rank
representations, thus alleviating the ``Curse of Dimensionality'' inherent in
handling higher-order data. At the heart of their success lies the concept of
TN ranks, which governs the efficiency and expressivity of TN decompositions.
However, unlike matrix ranks, TN ranks often lack a universal meaning and an
intuitive interpretation, with their properties varying significantly across
different TN structures. Consequently, TN ranks are frequently treated as
empirically tuned hyperparameters, rather than as key design parameters
inferred from domain knowledge. The aim of this Lecture Note is therefore to
demystify the foundational yet frequently misunderstood concept of TN ranks
through real-life examples and intuitive visualizations. We begin by
illustrating how domain knowledge can guide the selection of TN ranks in
widely-used models such as the Canonical Polyadic (CP) and Tucker
decompositions. For more complex TN structures, we employ a self-explanatory
graphical approach that generalizes to tensors of arbitrary order. Such a
perspective naturally reveals the relationship between TN ranks and the
corresponding ranks of tensor unfoldings (matrices), thereby circumventing
cumbersome multi-index tensor algebra while facilitating domain-informed TN
design. It is our hope that this Lecture Note will equip readers with a clear
and unified understanding of the concept of TN rank, along with the necessary
physical insight and intuition to support the selection, explainability, and
deployment of tensor methods in both practical applications and educational
contexts.

</details>


### [439] [Play Style Identification Using Low-Level Representations of Play Traces in MicroRTS](https://arxiv.org/abs/2507.10172)
*Ruizhe Yu Xia,Jeremy Gow,Simon Lucas*

Main category: cs.LG

TL;DR: 本研究使用无监督CNN-LSTM自编码器从低级游戏数据中学习潜在表示，以识别游戏风格，从而减少对人工特征和领域知识的依赖。


<details>
  <summary>Details</summary>
Motivation: 游戏风格识别可以提供有价值的游戏设计见解并实现自适应体验，有潜力改进游戏AI。

Method: 本研究探索使用无监督CNN-LSTM自编码器模型直接从MicroRTS中的低级游戏痕迹数据中获取潜在表示。

Result: 我们证明了该方法在潜在空间中能够有效地区分不同的游戏AI玩家。

Conclusion: 该方法减少了对领域专业知识及其相关偏见的依赖，并可用于指导对所研究AI玩家中不同游戏风格的探索。

Abstract: Play style identification can provide valuable game design insights and
enable adaptive experiences, with the potential to improve game playing agents.
Previous work relies on domain knowledge to construct play trace
representations using handcrafted features. More recent approaches incorporate
the sequential structure of play traces but still require some level of domain
abstraction. In this study, we explore the use of unsupervised CNN-LSTM
autoencoder models to obtain latent representations directly from low-level
play trace data in MicroRTS. We demonstrate that this approach yields a
meaningful separation of different game playing agents in the latent space,
reducing reliance on domain expertise and its associated biases. This latent
space is then used to guide the exploration of diverse play styles within
studied AI players.

</details>


### [440] [T-GRAB: A Synthetic Diagnostic Benchmark for Learning on Temporal Graphs](https://arxiv.org/abs/2507.10183)
*Alireza Dizaji,Benedict Aaron Tjandra,Mehrab Hamidi,Shenyang Huang,Guillaume Rabusseau*

Main category: cs.LG

TL;DR: 本研究提出了 T-GRAB 基准测试，发现当前 TGNN 在处理时间模式方面存在不足，并呼吁开发更强的模型。


<details>
  <summary>Details</summary>
Motivation: 尽管动态图学习方法在模拟随时间演变的关系数据方面已显示出强大能力，但当前的时间图神经网络（TGNNs）在有效捕捉周期性、因果关系和长期依赖性等核心时间模式方面仍存在不确定性。

Method: 本研究引入了时间图推理基准（T-GRAB），这是一套综合性的合成任务，旨在系统地探究 TGNN 跨时间推理的能力。T-GRAB 提供了受控的、可解释的任务，用于隔离关键的时间技能：计算/记忆周期性重复、推断延迟因果效应以及捕捉空间和时间维度上的长期依赖性。研究评估了 11 种时间图学习方法在这些任务上的表现。

Result: 研究结果表明，在 T-GRAB 基准测试中，11 种时间图学习方法在泛化时间模式方面存在根本性不足。

Conclusion: 本研究揭示了当前时间图神经网络（TGNN）在捕捉周期性、因果关系和长期依赖性等核心时间模式方面存在根本性不足，并指出了传统真实世界基准测试中隐藏的挑战，激发了开发具有更强时间推理能力的架构。

Abstract: Dynamic graph learning methods have recently emerged as powerful tools for
modelling relational data evolving through time. However, despite extensive
benchmarking efforts, it remains unclear whether current Temporal Graph Neural
Networks (TGNNs) effectively capture core temporal patterns such as
periodicity, cause-and-effect, and long-range dependencies. In this work, we
introduce the Temporal Graph Reasoning Benchmark (T-GRAB), a comprehensive set
of synthetic tasks designed to systematically probe the capabilities of TGNNs
to reason across time. T-GRAB provides controlled, interpretable tasks that
isolate key temporal skills: counting/memorizing periodic repetitions,
inferring delayed causal effects, and capturing long-range dependencies over
both spatial and temporal dimensions. We evaluate 11 temporal graph learning
methods on these tasks, revealing fundamental shortcomings in their ability to
generalize temporal patterns. Our findings offer actionable insights into the
limitations of current models, highlight challenges hidden by traditional
real-world benchmarks, and motivate the development of architectures with
stronger temporal reasoning abilities. The code for T-GRAB can be found at:
https://github.com/alirezadizaji/T-GRAB.

</details>


### [441] [Learning Private Representations through Entropy-based Adversarial Training](https://arxiv.org/abs/2507.10194)
*Tassilo Klein,Moin Nabi*

Main category: cs.LG

TL;DR: 提出了一种名为焦点熵的对抗性表示学习方法，可以在保护用户隐私的同时学习具有高预测能力的表示，并在多个基准测试中取得了高目标效用和中等隐私泄露的成果。


<details>
  <summary>Details</summary>
Motivation: 如何在保护用户隐私的同时学习具有高预测能力的表示。

Method: 提出了一种对抗性表示学习方法来净化表示中敏感的内容，并引入了一种熵的变体——焦点熵，以减少现有基于熵的方法的潜在信息泄露。

Result: 在多个基准测试中展示了可行性，结果表明在中等隐私泄露的情况下，目标效用很高。

Conclusion: 该方法在保持用户隐私的同时学习到了具有高预测能力的表示。结果表明，在中等隐私泄露的情况下，目标效用很高。

Abstract: How can we learn a representation with high predictive power while preserving
user privacy? We present an adversarial representation learning method for
sanitizing sensitive content from the learned representation. Specifically, we
introduce a variant of entropy - focal entropy, which mitigates the potential
information leakage of the existing entropy-based approaches. We showcase
feasibility on multiple benchmarks. The results suggest high target utility at
moderate privacy leakage.

</details>


### [442] [A Graph Sufficiency Perspective for Neural Networks](https://arxiv.org/abs/2507.10215)
*Cencheng Shen,Yuexiao Dong*

Main category: cs.LG

TL;DR: 该研究提出一种新的神经网络分析框架，将层视为图变换，神经元作为成对函数。证明了在特定条件下（无限宽度或区域分离输入），神经网络层能保留统计充分性，为深度学习提供了新的统计理解。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在通过引入图变量和统计充分性的概念，为理解深度学习模型（特别是神经网络）提供新的视角。研究者希望建立一个理论框架，用以解释神经网络为何能够有效地学习和表示数据中的模式，并证明其在特定条件下能够保留关键的统计信息。

Method: 本研究提出了一种新的分析框架，将神经网络的层解释为基于图的变换，其中神经元作为输入和学习到的锚点之间的成对函数。在此框架下，研究了层输出在多大程度上能够充分表示层输入（即保留目标变量关于输入变量的条件分布）。通过对稠密锚点进行假设，证明了在无限宽度极限下渐近充分性成立且在训练过程中保持不变。此外，还通过假设输入分布的区域可分离性并构建合适的锚点，证明了有限宽度网络也能实现充分性。

Result: 研究表明，在无限宽度极限下，神经网络层渐近充分性成立，并在训练过程中保持不变。通过特定假设（如区域分离的输入分布和适当的锚点），有限宽度网络也能达到充分性。该框架成功应用于全连接层、卷积神经网络以及ReLU和Sigmoid激活函数等多种网络结构。

Conclusion: 本研究将神经元视为输入和学习到的锚点之间的成对函数，将神经网络层解释为基于图的变换，从而在图变量和统计充分性的框架下分析神经网络。我们证明了在稠密的锚点假设下，渐近充分性在无限宽度极限下成立，并且在整个训练过程中得以保留。为了更贴近实际架构，我们进一步证明了通过假设区域分离的输入分布和构建适当的锚点，有限宽度网络也可以实现充分性。该框架涵盖了全连接层、通用成对函数、ReLU和Sigmoid激活函数以及卷积神经网络，实现了统计充分性、图论表示和深度学习的结合，为神经网络提供了新的统计理解。

Abstract: This paper analyzes neural networks through graph variables and statistical
sufficiency. We interpret neural network layers as graph-based transformations,
where neurons act as pairwise functions between inputs and learned anchor
points. Within this formulation, we establish conditions under which layer
outputs are sufficient for the layer inputs, that is, each layer preserves the
conditional distribution of the target variable given the input variable. Under
dense anchor point assumptions, we prove that asymptotic sufficiency holds in
the infinite-width limit and is preserved throughout training. To align more
closely with practical architectures, we further show that sufficiency can be
achieved with finite-width networks by assuming region-separated input
distributions and constructing appropriate anchor points. Our framework covers
fully connected layers, general pairwise functions, ReLU and sigmoid
activations, and convolutional neural networks. This work bridges statistical
sufficiency, graph-theoretic representations, and deep learning, providing a
new statistical understanding of neural networks.

</details>


### [443] [Kernel-Adaptive PI-ELMs for Forward and Inverse Problems in PDEs with Sharp Gradients](https://arxiv.org/abs/2507.10241)
*Vikas Dwivedi,Balaji Srinivasan,Monica Sigovan,Bruno Sixou*

Main category: cs.LG

TL;DR: KAPI-ELM是一种新的物理信息学习框架，通过贝叶斯优化改进了ELM以处理尖锐梯度，在速度和准确性上均优于现有方法，特别适用于刚性PDE问题。


<details>
  <summary>Details</summary>
Motivation: 传统的PI-ELM虽然速度快，但由于其固定的、随机初始化的输入层，难以捕捉尖锐梯度。本研究旨在克服这一限制，提高PI-ELM在处理尖锐梯度问题上的能力。

Method: 提出了一种名为KAPI-ELM（Kernel Adaptive Physics-Informed Extreme Learning Machine）的算法，这是PI-ELM的一种自适应径向基函数（RBF）扩展，用于解决涉及局部尖锐梯度的正反偏微分方程（PDE）问题。KAPI-ELM采用了一种轻量级的贝叶斯优化（BO）框架来学习输入层权重的分布参数，并结合最小二乘优化来处理输出层参数。

Result: KAPI-ELM在1D奇异摄动对流扩散方程、具有尖锐局部源的2D泊松方程和时变对流方程等具有挑战性的PDE问题上得到了验证。结果表明，KAPI-ELM在正反问题中均达到了最先进的准确性，在刚性PDE情况下，其性能与XTFC等先进方法相当甚至更优，且可调参数减少了近一个数量级。

Conclusion: KAPI-ELM通过结合贝叶斯优化和最小二乘优化，在保持PI-ELM速度优势的同时，提高了捕获尖锐梯度能力，在多个PDE问题上达到了最先进的准确性，特别是在刚性PDE情况下，其性能优于包括XTFC在内的先进方法，并且可调参数更少，展示了其作为可扩展、可解释和可泛化的物理信息学习框架的潜力。

Abstract: This paper introduces the Kernel Adaptive Physics-Informed Extreme Learning
Machine (KAPI-ELM), an adaptive Radial Basis Function (RBF)-based extension of
PI-ELM designed to solve both forward and inverse Partial Differential Equation
(PDE) problems involving localized sharp gradients. While PI-ELMs outperform
the traditional Physics-Informed Neural Networks (PINNs) in speed due to their
single-shot, least square optimization, this advantage comes at a cost: their
fixed, randomly initialized input layer limits their ability to capture sharp
gradients. To overcome this limitation, we introduce a lightweight Bayesian
Optimization (BO) framework that, instead of adjusting each input layer
parameter individually as in traditional backpropagation, learns a small set of
hyperparameters defining the statistical distribution from which the input
weights are drawn. This novel distributional optimization strategy -- combining
BO for input layer distributional parameters with least-squares optimization
for output layer network parameters -- enables KAPI-ELM to preserve PI-ELM's
speed while matching or exceeding the expressiveness of PINNs. We validate the
proposed methodology on several challenging forward and inverse PDE benchmarks,
including a 1D singularly perturbed convection-diffusion equation, a 2D Poisson
equation with sharp localized sources, and a time-dependent advection equation.
Notably, KAPI-ELM achieves state-of-the-art accuracy in both forward and
inverse settings. In stiff PDE regimes, it matches or even outperforms advanced
methods such as the Extended Theory of Functional Connections (XTFC), while
requiring nearly an order of magnitude fewer tunable parameters. These results
establish the potential of KAPI-ELM as a scalable, interpretable, and
generalizable physics-informed learning framework, especially in stiff PDE
regimes.

</details>


### [444] [Conditional Chemical Language Models are Versatile Tools in Drug Discovery](https://arxiv.org/abs/2507.10273)
*Lu Zhu,Emmanuel Noutahi*

Main category: cs.LG

TL;DR: SAFE-T是一个条件生成CLM框架，可以对分子进行评分和生成，以加速药物发现。


<details>
  <summary>Details</summary>
Motivation: 为了解决生成化学语言模型（CLM）在药物发现中的局限性，如缺乏可靠的奖励信号和输出的可解释性。

Method: 提出了一种名为SAFE-T的通用化学建模框架，该框架以生物学背景（如蛋白质靶点或作用机制）为条件，以优先排序和设计分子，而不依赖于结构信息或工程评分函数。SAFE-T模拟了给定生物学提示的基于片段的分子序列的条件似然，使得跨虚拟筛选、药物-靶点相互作用预测和活性裂点检测等任务的分子能够进行原则性评分。此外，它通过从学习的分布中采样来支持目标导向的生成，将分子设计与生物学目标对齐。

Result: 在对预测性（LIT-PCBA、DAVIS、KIBA、ACNet）和生成性（DRUG、PMO）基准的综合性零样本评估中，SAFE-T始终 achieves 与现有方法相当或更好的性能，同时速度显著更快。片段级归因进一步揭示SAFE-T能够捕捉已知的构效关系，支持可解释和基于生物学的设​​计。

Conclusion: 条件生成化学语言模型（CLM）可以统一评分和生成，以加速早期药物发现。

Abstract: Generative chemical language models (CLMs) have demonstrated strong
capabilities in molecular design, yet their impact in drug discovery remains
limited by the absence of reliable reward signals and the lack of
interpretability in their outputs. We present SAFE-T, a generalist chemical
modeling framework that conditions on biological context -- such as protein
targets or mechanisms of action -- to prioritize and design molecules without
relying on structural information or engineered scoring functions. SAFE-T
models the conditional likelihood of fragment-based molecular sequences given a
biological prompt, enabling principled scoring of molecules across tasks such
as virtual screening, drug-target interaction prediction, and activity cliff
detection. Moreover, it supports goal-directed generation by sampling from this
learned distribution, aligning molecular design with biological objectives. In
comprehensive zero-shot evaluations across predictive (LIT-PCBA, DAVIS, KIBA,
ACNet) and generative (DRUG, PMO) benchmarks, SAFE-T consistently achieves
performance comparable to or better than existing approaches while being
significantly faster. Fragment-level attribution further reveals that SAFE-T
captures known structure-activity relationships, supporting interpretable and
biologically grounded design. Together with its computational efficiency, these
results demonstrate that conditional generative CLMs can unify scoring and
generation to accelerate early-stage drug discovery.

</details>


### [445] [Recognizing Dementia from Neuropsychological Tests with State Space Models](https://arxiv.org/abs/2507.10311)
*Liming Wang,Saurabhchand Bhati,Cody Karjadi,Rhoda Au,James Glass*

Main category: cs.LG

TL;DR: Demenba是一个基于状态空间模型的新框架，可以通过语音评估痴呆症，在分类准确性方面有所提高，并且模型参数更少。


<details>
  <summary>Details</summary>
Motivation: 早期发现痴呆症对于及时的医疗干预和改善患者预后至关重要。本研究旨在直接从神经心理学测试的语音记录中推断认知能力下降，以实现自动痴呆症分类。

Method: 提出了一种名为Demenba的新型自动痴呆症分类（ADC）框架，该框架基于状态空间模型，其内存和计算量与序列长度呈线性关系。

Result: 该框架在超过1000小时的认知评估数据上进行了训练，并表现出优于现有方法的性能。

Conclusion: Demenba框架在细粒度痴呆症分类方面比以前的方法提高了21%，并且使用的参数更少。该模型还可以与大型语言模型融合，以获得进一步的改进，为更透明和可扩展的痴呆症评估工具铺平道路。

Abstract: Early detection of dementia is critical for timely medical intervention and
improved patient outcomes. Neuropsychological tests are widely used for
cognitive assessment but have traditionally relied on manual scoring. Automatic
dementia classification (ADC) systems aim to infer cognitive decline directly
from speech recordings of such tests. We propose Demenba, a novel ADC framework
based on state space models, which scale linearly in memory and computation
with sequence length. Trained on over 1,000 hours of cognitive assessments
administered to Framingham Heart Study participants, some of whom were
diagnosed with dementia through adjudicated review, our method outperforms
prior approaches in fine-grained dementia classification by 21\%, while using
fewer parameters. We further analyze its scaling behavior and demonstrate that
our model gains additional improvement when fused with large language models,
paving the way for more transparent and scalable dementia assessment tools.
Code: https://anonymous.4open.science/r/Demenba-0861

</details>


### [446] [MoCap-Impute: A Comprehensive Benchmark and Comparative Analysis of Imputation Methods for IMU-based Motion Capture Data](https://arxiv.org/abs/2507.10334)
*Mahmoud Bekhit,Ahmad Salah,Ahmed Salim Alrawahi,Tarek Attia,Ahmed Ali,Esraa Eldesokey,Ahmed Fathalla*

Main category: cs.LG

TL;DR: 由于缺乏对IMU运动捕捉时间序列数据插补方法的系统性评估，本研究旨在填补这一空白。通过发布首个专门为此设计的公开数据集，并对统计、机器学习和深度学习方法进行广泛比较，我们发现多变量方法在处理复杂缺失数据时优于单变量方法，其中GAIN和迭代插补器表现最佳，可将MAE降低高达50%。


<details>
  <summary>Details</summary>
Motivation: 现有的IMU运动捕捉数据插补技术缺乏系统性的性能评估，尤其是在处理运动捕捉时间序列数据时。这项研究旨在填补这一空白，通过全面的比较分析来评估不同的插补方法，并为该领域未来的研究提供指导。

Method: 本研究通过对统计、机器学习和深度学习插补方法进行全面的比较分析来评估IMU运动捕捉时间序列数据的插补性能。研究考虑了单变量时间序列、跨科目多变量以及运动学角度多变量三种不同的情境。为支撑该基准测试，研究发布了一个包含53名空手道练习者数据的公开数据集，并模拟了三种缺失机制：完全随机缺失（MCAR）、块缺失和信号转换点的数值依赖模式。在39个运动学变量上进行了实验。

Result: 多变量插补框架在处理复杂缺失情况时，性能始终优于单变量方法。例如，在信号转换点缺失情况下，多变量方法的平均绝对误差（MAE）降低了高达50%（从10.8降至5.8）。其中，生成对抗性插补网络（GAIN）和迭代插补器等高级模型在最具挑战性的场景下展现出最高的准确性。

Conclusion: 本研究填补了IMU运动捕捉时间序列数据插补方法性能评估的空白，通过广泛的比较分析，揭示了多变量插补方法在处理复杂缺失情况时优于单变量方法，并推荐了如GAIN和迭代插补等先进模型，为未来的研究提供了基准和实践建议。

Abstract: Motion capture (MoCap) data from wearable Inertial Measurement Units (IMUs)
is vital for applications in sports science, but its utility is often
compromised by missing data. Despite numerous imputation techniques, a
systematic performance evaluation for IMU-derived MoCap time-series data is
lacking. We address this gap by conducting a comprehensive comparative analysis
of statistical, machine learning, and deep learning imputation methods. Our
evaluation considers three distinct contexts: univariate time-series,
multivariate across subjects, and multivariate across kinematic angles. To
facilitate this benchmark, we introduce the first publicly available MoCap
dataset designed specifically for imputation, featuring data from 53 karate
practitioners. We simulate three controlled missingness mechanisms: missing
completely at random (MCAR), block missingness, and a novel value-dependent
pattern at signal transition points. Our experiments, conducted on 39 kinematic
variables across all subjects, reveal that multivariate imputation frameworks
consistently outperform univariate approaches, particularly for complex
missingness. For instance, multivariate methods achieve up to a 50% mean
absolute error reduction (MAE from 10.8 to 5.8) compared to univariate
techniques for transition point missingness. Advanced models like Generative
Adversarial Imputation Networks (GAIN) and Iterative Imputers demonstrate the
highest accuracy in these challenging scenarios. This work provides a critical
baseline for future research and offers practical recommendations for improving
the integrity and robustness of Mo-Cap data analysis.

</details>


### [447] [Some Super-approximation Rates of ReLU Neural Networks for Korobov Functions](https://arxiv.org/abs/2507.10345)
*Yuwen Li,Guozhi Zhang*

Main category: cs.LG

TL;DR: ReLU 神经网络在逼近 Korobov 函数时，通过稀疏网格有限元和比特提取技术，在宽度和深度上实现了近乎最优的超逼近误差界限，证明了其表达能力不受维度灾难的影响。


<details>
  <summary>Details</summary>
Motivation: 研究 ReLU 神经网络在逼近 Korobov 函数时的 $L_p$ 和 $W^1_p$ 范数逼近误差，以改进现有理论并理解神经网络的表达能力。

Method: 利用稀疏网格有限元和比特提取技术，分析了 ReLU 神经网络在宽度和深度上的超逼近误差。

Result: 在网络宽度和深度方面，得到了误差项为 $2m$（$L_p$ 范数）和 $2m-2$（$W^1_p$ 范数）的超逼近误差界限，优于经典的最低阶 $L_
u$ 和 $H^1$ 范数误差界限，并表明神经网络的表达能力不受维度灾难的影响。

Conclusion: 该研究为具有 $L_p$ 混合导数的目标函数，在 $L_p$ 和 $W^1_p$ 范数下，ReLU 神经网络的超逼近误差界限提供了近乎最优的界限，具体为 $2m$ 和 $2m-2$。

Abstract: This paper examines the $L_p$ and $W^1_p$ norm approximation errors of ReLU
neural networks for Korobov functions. In terms of network width and depth, we
derive nearly optimal super-approximation error bounds of order $2m$ in the
$L_p$ norm and order $2m-2$ in the $W^1_p$ norm, for target functions with
$L_p$ mixed derivative of order $m$ in each direction. The analysis leverages
sparse grid finite elements and the bit extraction technique. Our results
improve upon classical lowest order $L_\infty$ and $H^1$ norm error bounds and
demonstrate that the expressivity of neural networks is largely unaffected by
the curse of dimensionality.

</details>


### [448] [Parallel Sampling of Diffusion Models on $SO(3)$](https://arxiv.org/abs/2507.10347)
*Yan-Ting Chen,Hao-Wei Chen,Tsu-Ching Hsiao,Chun-Yi Lee*

Main category: cs.LG

TL;DR: 提出了一种在SO(3)流形上加速扩散过程的算法，通过改编数值皮卡迭代，实现了高达4.9倍的加速，且不损失任务奖励。


<details>
  <summary>Details</summary>
Motivation: 为了克服扩散模型固有的顺序性导致的去噪时间过长的问题。

Method: 提出了一种将数值皮卡迭代方法改编到SO(3)空间中的算法，以加速扩散过程。

Result: 实验表明，该算法在不损失任务奖励的情况下，实现了高达4.9倍的加速。

Conclusion: 该算法在不牺牲任务奖励的情况下，实现了高达4.9倍的加速，显著降低了生成单个样本的延迟。

Abstract: In this paper, we design an algorithm to accelerate the diffusion process on
the $SO(3)$ manifold. The inherently sequential nature of diffusion models
necessitates substantial time for denoising perturbed data. To overcome this
limitation, we proposed to adapt the numerical Picard iteration for the $SO(3)$
space. We demonstrate our algorithm on an existing method that employs
diffusion models to address the pose ambiguity problem. Moreover, we show that
this acceleration advantage occurs without any measurable degradation in task
reward. The experiments reveal that our algorithm achieves a speed-up of up to
4.9$\times$, significantly reducing the latency for generating a single sample.

</details>


### [449] [Feature Distillation is the Better Choice for Model-Heterogeneous Federated Learning](https://arxiv.org/abs/2507.10348)
*Yichen Li*

Main category: cs.LG

TL;DR: FedFD通过特征蒸馏和正交投影解决了模型异构联邦学习中的知识偏差和训练不稳定性问题，性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有的模型异构联邦学习方法在结合集成蒸馏技术时存在训练不稳定和无法有效补偿模型异构性带来的知识偏差的问题，主要是由于其仅依赖于logit蒸馏。

Method: 提出了一种名为FedFD的特征蒸馏方法，该方法通过在服务器端维护用于对齐客户端模型特征的投影层，并利用正交技术来优化这些投影层，以减少异构模型带来的知识偏差。

Result: FedFD在模型异构联邦学习任务中取得了优于现有最先进方法的性能。

Conclusion: FedFD通过在模型异构的联邦学习中引入基于特征的蒸馏方法，并利用正交投影来对齐特征并缓解知识偏差，从而实现了比现有方法更优越的性能。

Abstract: Model-Heterogeneous Federated Learning (Hetero-FL) has attracted growing
attention for its ability to aggregate knowledge from heterogeneous models
while keeping private data locally. To better aggregate knowledge from clients,
ensemble distillation, as a widely used and effective technique, is often
employed after global aggregation to enhance the performance of the global
model. However, simply combining Hetero-FL and ensemble distillation does not
always yield promising results and can make the training process unstable. The
reason is that existing methods primarily focus on logit distillation, which,
while being model-agnostic with softmax predictions, fails to compensate for
the knowledge bias arising from heterogeneous models. To tackle this challenge,
we propose a stable and efficient Feature Distillation for model-heterogeneous
Federated learning, dubbed FedFD, that can incorporate aligned feature
information via orthogonal projection to integrate knowledge from heterogeneous
models better. Specifically, a new feature-based ensemble federated knowledge
distillation paradigm is proposed. The global model on the server needs to
maintain a projection layer for each client-side model architecture to align
the features separately. Orthogonal techniques are employed to re-parameterize
the projection layer to mitigate knowledge bias from heterogeneous models and
thus maximize the distilled knowledge. Extensive experiments show that FedFD
achieves superior performance compared to state-of-the-art methods.

</details>


### [450] [TAT: Temporal-Aligned Transformer for Multi-Horizon Peak Demand Forecasting](https://arxiv.org/abs/2507.10349)
*Zhiyuan Zhao,Sitan Yang,Kin G. Olivares,Boris N. Oreshkin,Stan Vitebsky,Michael W. Mahoney,B. Aditya Prakash,Dmitry Efimov*

Main category: cs.LG

TL;DR: TAT是一种利用时间对齐注意力和上下文变量（如节假日和促销）的多预测器，可将峰值需求预测的准确性提高高达30%。


<details>
  <summary>Details</summary>
Motivation: 为了应对高风险销售活动期间需求预测的挑战，这些活动的需求高峰尤其难以准确预测，而这些活动对于管理供应链运营和确保无缝的客户购物体验至关重要。

Method: TAT模型，一个多预测器，利用先验已知的上下文变量（如节假日和促销活动信息）来提高预测性能。 TAT模型由编码器和解码器组成，两者都嵌入了新颖的时间对齐注意力（TAA）机制，旨在学习依赖于上下文的峰值需求预测对齐。

Result: TAT模型在两个大规模专有数据集上进行了广泛的实证分析，证明了其在峰值需求预测方面的优越性能。

Conclusion: TAT模型在预测峰值需求方面带来了高达30%的准确性提升，同时与其他最先进的方法相比，在整体性能上保持了竞争力。

Abstract: Multi-horizon time series forecasting has many practical applications such as
demand forecasting. Accurate demand prediction is critical to help make buying
and inventory decisions for supply chain management of e-commerce and physical
retailers, and such predictions are typically required for future horizons
extending tens of weeks. This is especially challenging during high-stake sales
events when demand peaks are particularly difficult to predict accurately.
However, these events are important not only for managing supply chain
operations but also for ensuring a seamless shopping experience for customers.
To address this challenge, we propose Temporal-Aligned Transformer (TAT), a
multi-horizon forecaster leveraging apriori-known context variables such as
holiday and promotion events information for improving predictive performance.
Our model consists of an encoder and decoder, both embedded with a novel
Temporal Alignment Attention (TAA), designed to learn context-dependent
alignment for peak demand forecasting. We conduct extensive empirical analysis
on two large-scale proprietary datasets from a large e-commerce retailer. We
demonstrate that TAT brings up to 30% accuracy improvement on peak demand
forecasting while maintaining competitive overall performance compared to other
state-of-the-art methods.

</details>


### [451] [Enhanced DeepONet for 1-D consolidation operator learning: an architectural investigation](https://arxiv.org/abs/2507.10368)
*Yongjin Choi,Chenying Liu,Jorge Macedo*

Main category: cs.LG

TL;DR: 本文提出了一种新的傅里叶特征增强 DeepONet 模型，用于解决岩土工程中的一维固结问题，该模型比传统方法和标准 DeepONet 更快、更准确。


<details>
  <summary>Details</summary>
Motivation: 为了扩展深度算子网络（DeepONets）在岩土工程领域的应用，并解决现有模型在处理具有显著变化的解（如超孔隙压力）时存在的局限性。

Method: 本文评估了三种 DeepONet 架构（标准 DeepONet 和物理信息架构）用于一维固结问题，然后提出了一种结合傅里叶特征增强的 Trunknet DeepONet 模型（模型 4）来解决现有模型的局限性。

Result: 所有提出的架构在速度上都比传统求解器快 1.5 到 100 倍，其中模型 4 最为高效。与标准配置相比，物理信息架构（模型 3）表现更好，而傅里叶特征增强模型（模型 4）进一步克服了捕获快速变化函数的能力。

Conclusion: 该研究系统地评估了几种用于一维固结问题的 DeepONet 架构，并提出了一种新的傅里叶特征增强模型，以克服现有模型的局限性。结果表明，所提出的模型在效率和准确性方面优于传统方法和标准 DeepONet 配置，展示了 DeepONets 在岩土工程领域进行高效、可推广的代理建模的潜力。

Abstract: Deep Operator Networks (DeepONets) have emerged as a powerful surrogate
modeling framework for learning solution operators in PDE-governed systems.
While their use is expanding across engineering disciplines, applications in
geotechnical engineering remain limited. This study systematically evaluates
several DeepONet architectures for the one-dimensional consolidation problem.
We initially consider three architectures: a standard DeepONet with the
coefficient of consolidation embedded in the branch net (Models 1 and 2), and a
physics-inspired architecture with the coefficient embedded in the trunk net
(Model 3). Results show that Model 3 outperforms the standard configurations
(Models 1 and 2) but still has limitations when the target solution (excess
pore pressures) exhibits significant variation. To overcome this limitation, we
propose a Trunknet Fourier feature-enhanced DeepONet (Model 4) that addresses
the identified limitations by capturing rapidly varying functions. All proposed
architectures achieve speedups ranging from 1.5 to 100 times over traditional
explicit and implicit solvers, with Model 4 being the most efficient. Larger
computational savings are expected for more complex systems than the explored
1D case, which is promising. Overall, the study highlights the potential of
DeepONets to enable efficient, generalizable surrogate modeling in geotechnical
applications, advancing the integration of scientific machine learning in
geotechnics, which is at an early stage.

</details>


### [452] [Leveraging RAG-LLMs for Urban Mobility Simulation and Analysis](https://arxiv.org/abs/2507.10382)
*Yue Ding,Conor McCarthy,Kevin O'Shea,Mingming Liu*

Main category: cs.LG

TL;DR: 本文提出了一种结合了云技术和大型语言模型（LLM）的共享电动出行平台，能够提供个性化的路线推荐和优化的出行方案。该平台在评估中表现出高准确率，尤其是在处理用户查询方面。


<details>
  <summary>Details</summary>
Motivation: 随着智能出行和共享电动出行服务的兴起，为了满足日益增长的用户需求，提供全面的端到端解决方案至关重要。LLM在智能决策、用户交互和实时交通分析等领域展现出强大的潜力。

Method: 本文提出了一种基于云的、由LLM驱动的共享电动出行平台，集成了移动应用程序提供个性化路线推荐。优化模块基于不同交通场景下的出行时间和成本进行评估。LLM驱动的RAG框架在模式层面使用多种评估方法进行评估。

Result: 基于XiYanSQL的模式层面RAG在系统操作员查询上的平均执行准确率为0.81，在用户查询上的平均执行准确率为0.98。

Conclusion: 该论文提出了一个基于云的、由LLM驱动的共享电动出行平台，集成了移动应用程序以提供个性化的路线推荐。该平台通过优化模块在不同交通场景下的出行时间和成本进行了评估。此外，还通过各种评估方法对LLM驱动的RAG框架在不同用户的模式层面进行了评估，其中基于XiYanSQL的模式层面RAG在系统操作员查询上达到了0.81的平均执行准确率，在用户查询上达到了0.98的平均执行准确率。

Abstract: With the rise of smart mobility and shared e-mobility services, numerous
advanced technologies have been applied to this field. Cloud-based traffic
simulation solutions have flourished, offering increasingly realistic
representations of the evolving mobility landscape. LLMs have emerged as
pioneering tools, providing robust support for various applications, including
intelligent decision-making, user interaction, and real-time traffic analysis.
As user demand for e-mobility continues to grow, delivering comprehensive
end-to-end solutions has become crucial. In this paper, we present a
cloud-based, LLM-powered shared e-mobility platform, integrated with a mobile
application for personalized route recommendations. The optimization module is
evaluated based on travel time and cost across different traffic scenarios.
Additionally, the LLM-powered RAG framework is evaluated at the schema level
for different users, using various evaluation methods. Schema-level RAG with
XiYanSQL achieves an average execution accuracy of 0.81 on system operator
queries and 0.98 on user queries.

</details>


### [453] [Extracting Important Tokens in E-Commerce Queries with a Tag Interaction-Aware Transformer Model](https://arxiv.org/abs/2507.10385)
*Md. Ahsanul Kabir,Mohammad Al Hasan,Aritra Mandal,Liyang Hao,Ishita Khan,Daniel Tunkelang,Zhe Wu*

Main category: cs.LG

TL;DR: TagBERT是一个利用标记语义标签的Transformer模型，用于电子商务搜索中的查询改写，并在标记分类任务上超越了现有模型。


<details>
  <summary>Details</summary>
Motivation: 当前的查询改写方法未能利用查询标记的语义标签，而这些标签对于理解电子商务查询中的用户意图至关重要。之前的基于统计和深度学习（包括Transformer）的方法存在此不足。

Method: 将查询改写视为一个标记分类任务，并设计了一个依赖感知的、基于Transformer的语言模型TagBERT来解决此任务，该模型利用了标记的语义标签来学习更优的查询短语嵌入。

Result: 在大型真实电子商务数据集上进行的实验表明，TagBERT在重要的标记分类任务上表现优于包括BERT、eBERT和序列到序列Transformer模型在内的多种竞争模型。

Conclusion: TagBERT通过将查询改写视为一个标记分类任务，并利用标记的语义标签来学习更优的查询短语嵌入，在重要的标记分类任务上表现优于包括BERT、eBERT和序列到序列Transformer模型在内的多种竞争模型。

Abstract: The major task of any e-commerce search engine is to retrieve the most
relevant inventory items, which best match the user intent reflected in a
query. This task is non-trivial due to many reasons, including ambiguous
queries, misaligned vocabulary between buyers, and sellers, over- or
under-constrained queries by the presence of too many or too few tokens. To
address these challenges, query reformulation is used, which modifies a user
query through token dropping, replacement or expansion, with the objective to
bridge semantic gap between query tokens and users' search intent. Early
methods of query reformulation mostly used statistical measures derived from
token co-occurrence frequencies from selective user sessions having clicks or
purchases. In recent years, supervised deep learning approaches, specifically
transformer-based neural language models, or sequence-to-sequence models are
being used for query reformulation task. However, these models do not utilize
the semantic tags of a query token, which are significant for capturing user
intent of an e-commerce query. In this work, we pose query reformulation as a
token classification task, and solve this task by designing a dependency-aware
transformer-based language model, TagBERT, which makes use of semantic tags of
a token for learning superior query phrase embedding. Experiments on large,
real-life e-commerce datasets show that TagBERT exhibits superior performance
than plethora of competing models, including BERT, eBERT, and
Sequence-to-Sequence transformer model for important token classification task.

</details>


### [454] [Anticipating the Selectivity of Cyclization Reaction Pathways with Neural Network Potentials](https://arxiv.org/abs/2507.10400)
*Nicholas Casetti,Dylan Anstine,Olexandr Isayev,Connor W. Coley*

Main category: cs.LG

TL;DR: 本研究提出了一种结合图论和机器学习的机理搜索新策略，使用AIMNet2-rxn神经网络势能，能有效处理复杂反应（如环化反应），并能准确预测活化能和立体选择性，为天然产物合成等领域提供支持。


<details>
  <summary>Details</summary>
Motivation: 旨在解决传统反应机理搜索工具在处理涉及多个协同键变的复杂反应（如天然产物合成中的关键步骤）时遇到的困难，以期加速对这类复杂反应机理的探索。

Method: 提出了一种结合图论枚举和机器学习（特别是神经网络势AIMNet2-rxn）的机理搜索策略，用于评估和筛选复杂的化学反应，特别是环化反应。

Result: 评估了NNP在估算活化能和预测立体选择性方面的能力，并成功重现了天然产物合成中的复杂关键步骤，证明了该方法的有效性。

Conclusion: 该研究提出了一种结合图论和机器学习的方法来优化包含多个协同键变的复杂反应（如环化反应）的机理搜索过程，并使用神经网络势（NNP）AIMNet2-rxn来评估反应路径和预测活化能与立体选择性。

Abstract: Reaction mechanism search tools have demonstrated the ability to provide
insights into likely products and rate-limiting steps of reacting systems.
However, reactions involving several concerted bond changes - as can be found
in many key steps of natural product synthesis - can complicate the search
process. To mitigate these complications, we present a mechanism search
strategy particularly suited to help expedite exploration of an exemplary
family of such complex reactions, cyclizations. We provide a cost-effective
strategy for identifying relevant elementary reaction steps by combining
graph-based enumeration schemes and machine learning techniques for
intermediate filtering. Key to this approach is our use of a neural network
potential (NNP), AIMNet2-rxn, for computational evaluation of each candidate
reaction pathway. In this article, we evaluate the NNP's ability to estimate
activation energies, demonstrate the correct anticipation of stereoselectivity,
and recapitulate complex enabling steps in natural product synthesis.

</details>


### [455] [Stochastic Operator Network: A Stochastic Maximum Principle Based Approach to Operator Learning](https://arxiv.org/abs/2507.10401)
*Ryan Bausback,Jingqiao Tang,Lu Lu,Feng Bao,Toan Huynh*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We develop a novel framework for uncertainty quantification in operator
learning, the Stochastic Operator Network (SON). SON combines the stochastic
optimal control concepts of the Stochastic Neural Network (SNN) with the
DeepONet. By formulating the branch net as an SDE and backpropagating through
the adjoint BSDE, we replace the gradient of the loss function with the
gradient of the Hamiltonian from Stohastic Maximum Principle in the SGD update.
This allows SON to learn the uncertainty present in operators through its
diffusion parameters. We then demonstrate the effectiveness of SON when
replicating several noisy operators in 2D and 3D.

</details>


### [456] [Energy Efficiency in AI for 5G and Beyond: A DeepRx Case Study](https://arxiv.org/abs/2507.10409)
*Amine Lbath,Ibtissam Labriji*

Main category: cs.LG

TL;DR: 本研究通过知识蒸馏技术，成功训练了一个能耗更低的DeepRX深度学习模型，该模型在保持高性能的同时实现了节能，并优于从头训练的模型。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在解决AI/ML模型在能效和性能之间取得平衡的挑战，重点关注DeepRX这一基于全卷积ResNet架构的深度学习接收器。

Method: 本研究提出了一种名为DeepRX的深度学习接收器，该接收器基于全卷积ResNet架构。研究人员评估了DeepRX的能耗，并考虑了FLOPs/Watt和FLOPs/clock等因素，将估计能耗与实际能耗进行了比较，同时分析了训练和推理阶段的能耗动态。此外，研究应用了知识蒸馏（KD）技术来训练一个紧凑的学生模型，并对不同的学生模型大小、教师模型大小和KD超参数进行了实验。

Result: 研究发现，DeepRX的估计能耗与实际能耗基本一致，且内存访问模式会影响能耗。通过知识蒸馏训练出的学生模型在比特错误率（BER）与信噪比（SINR）的关系上，表现优于从头训练的模型，尤其是在误差下限方面显示出优势。

Conclusion: 知识蒸馏（KD）被证明可以有效地训练出能效更高的紧凑型DeepRX模型，该模型在较低的误差下限方面表现优于从头训练的模型，证明了KD在实现节能AI解决方案方面的有效性。

Abstract: This study addresses the challenge of balancing energy efficiency with
performance in AI/ML models, focusing on DeepRX, a deep learning receiver based
on a fully convolutional ResNet architecture. We evaluate the energy
consumption of DeepRX, considering factors including FLOPs/Watt and
FLOPs/clock, and find consistency between estimated and actual energy usage,
influenced by memory access patterns. The research extends to comparing energy
dynamics during training and inference phases. A key contribution is the
application of knowledge distillation (KD) to train a compact DeepRX
\textit{student} model that emulates the performance of the \textit{teacher}
model but with reduced energy consumption. We experiment with different student
model sizes, optimal teacher sizes, and KD hyperparameters. Performance is
measured by comparing the Bit Error Rate (BER) performance versus
Signal-to-Interference \& Noise Ratio (SINR) values of the distilled model and
a model trained from scratch. The distilled models demonstrate a lower error
floor across SINR levels, highlighting the effectiveness of KD in achieving
energy-efficient AI solutions.

</details>


### [457] [Non-exchangeable Conformal Prediction with Optimal Transport: Tackling Distribution Shifts with Unlabeled Data](https://arxiv.org/abs/2507.10425)
*Alvaro H. C. Correia,Christos Louizos*

Main category: cs.LG

TL;DR: Conformal Prediction 在存在分布偏移时覆盖率会下降，本文利用最优传输技术解决了这个问题。


<details>
  <summary>Details</summary>
Motivation: 现有的 Conformal Prediction 方法在存在分布偏移时，其保证会失效，而现有的缓解方法需要预先知道分布偏移的类型。本文旨在提出一种新的视角来解决这个问题。

Method: 通过最优传输的角度研究分布偏移对 Conformal Prediction 覆盖率的影响，并提出估计和缓解覆盖率损失的方法。

Result: 提出了一种通过最优传输来估计和缓解 Conformal Prediction 在分布偏移下的覆盖率损失的方法。

Conclusion: Conformal prediction 的覆盖率损失可以通过最优传输来估计和缓解，即使在存在分布偏移的情况下也是如此。

Abstract: Conformal prediction is a distribution-free uncertainty quantification method
that has gained popularity in the machine learning community due to its
finite-sample guarantees and ease of use. Its most common variant, dubbed split
conformal prediction, is also computationally efficient as it boils down to
collecting statistics of the model predictions on some calibration data not yet
seen by the model. Nonetheless, these guarantees only hold if the calibration
and test data are exchangeable, a condition that is difficult to verify and
often violated in practice due to so-called distribution shifts. The literature
is rife with methods to mitigate the loss in coverage in this non-exchangeable
setting, but these methods require some prior information on the type of
distribution shift to be expected at test time. In this work, we study this
problem via a new perspective, through the lens of optimal transport, and show
that it is possible to estimate the loss in coverage and mitigate it in case of
distribution shift.

</details>


### [458] [CLA: Latent Alignment for Online Continual Self-Supervised Learning](https://arxiv.org/abs/2507.10434)
*Giacomo Cignoni,Andrea Cossu,Alexandra Gomez-Villa,Joost van de Weijer,Antonio Carta*

Main category: cs.LG

TL;DR: 本文提出了一种名为 Continual Latent Alignment (CLA) 的自监督学习策略，用于在线持续学习。该方法通过对齐模型表示来防止遗忘，在加速收敛和提升性能方面优于现有方法，甚至在早期预训练阶段使用 CLA 比 i.i.d. 预训练效果更好。


<details>
  <summary>Details</summary>
Motivation: 本文的动机在于解决自监督学习（SSL）在在线持续学习（CL）设置中面临的局限性。现有的 SSL 技术在需要处理小批量数据、遵守固定计算预算以及缺乏明确任务边界的在线 CL 环境中的应用较少。因此，本文旨在提出一种新的 SSL 策略，以适应这些挑战并提升模型在在线 CL 任务中的表现，特别是在缓解遗忘和加速收敛方面。

Method: 本文提出了一种名为 Continual Latent Alignment (CLA) 的自监督学习策略，用于解决在线持续学习（CL）中的挑战。该方法通过对齐当前模型与历史模型所学习到的潜在表示，以有效缓解模型遗忘问题。具体而言，CLA 被设计用于处理数据以小批量形式到达、模型需要遵守固定计算预算以及任务边界缺失的在线 CL 环境。

Result: 通过实验评估，我们发现 CLA 策略能够加速在线场景下的训练收敛过程，并且在相同的计算预算下，其性能超过了现有的最先进方法。此外，我们还发现，在预训练早期阶段使用 CLA 作为预训练协议，相比于完整的独立同分布（i.i.d.）预训练，能够取得更优越的最终性能。

Conclusion: Continual Latent Alignment (CLA) 是一种新颖的 SSL 策略，用于在线持续学习 (CL) 设置，通过对齐当前模型与历史模型所学表示来缓解遗忘。CLA 不仅在在线场景下加速了训练过程的收敛，而且在相同的计算预算下表现优于最先进的方法。此外，将 CLA 作为预训练协议在预训练早期阶段使用，与完整的独立同分布 (i.i.d.) 预训练相比，能够获得更好的最终性能。

Abstract: Self-supervised learning (SSL) is able to build latent representations that
generalize well to unseen data. However, only a few SSL techniques exist for
the online CL setting, where data arrives in small minibatches, the model must
comply with a fixed computational budget, and task boundaries are absent. We
introduce Continual Latent Alignment (CLA), a novel SSL strategy for Online CL
that aligns the representations learned by the current model with past
representations to mitigate forgetting. We found that our CLA is able to speed
up the convergence of the training process in the online scenario,
outperforming state-of-the-art approaches under the same computational budget.
Surprisingly, we also discovered that using CLA as a pretraining protocol in
the early stages of pretraining leads to a better final performance when
compared to a full i.i.d. pretraining.

</details>


### [459] [Response Wide Shut? Surprising Observations in Basic Vision Language Model Capabilities](https://arxiv.org/abs/2507.10442)
*Shivam Chandhok,Wan-Cyuan Fan,Vered Shwartz,Vineeth N Balasubramanian,Leonid Sigal*

Main category: cs.LG

TL;DR: 本研究旨在了解视觉语言模型（VLMs）在基本视觉任务上的局限性，通过设计专门的测试来探测其不足之处。研究不仅评估了VLMs的最终性能，还将其与直接在不同模型组件（如视觉编码器特征、中间视觉-语言投影和LLM解码器输出）上训练的探针性能进行了比较。研究结果揭示了VLMs在能力、鲁棒性和视觉信息处理方面存在的不足，并为未来改进VLMs提供了宝贵的见解和指导。


<details>
  <summary>Details</summary>
Motivation: 了解SoTA VLMs在基本视觉任务上的局限性，为改进VLMs提供指导。

Method: 通过设计一系列测试来探测SoTA VLMs在基本视觉任务上的局限性，并将其与直接在视觉编码器特征、中间视觉-语言投影和LLM解码器输出上训练的探针性能进行比较和对比。

Result: 揭示了VLMs的不足之处，并对其能力、鲁棒性以及视觉信息处理方式提出了重要观察结果。

Conclusion: VLMs在基本视觉任务中存在不足，其能力、鲁棒性以及视觉信息处理方式有待改进。

Abstract: Vision-language Models (VLMs) have emerged as general-purpose tools for
addressing a variety of complex computer vision problems. Such models have been
shown to be highly capable, but, at the same time, lacking some basic visual
understanding skills. In this paper, we set out to understand the limitations
of SoTA VLMs on fundamental visual tasks by constructing a series of tests that
probe which components of design, specifically, may be lacking. Importantly, we
go significantly beyond the current benchmarks, which simply measure the final
performance of VLM response, by also comparing and contrasting it to the
performance of probes trained directly on features obtained from the visual
encoder, intermediate vision-language projection and LLM-decoder output. In
doing so, we uncover shortcomings in VLMs and make a number of important
observations about their capabilities, robustness and how they process visual
information. We hope our insights will guide progress in further improving
VLMs.

</details>


### [460] [Some remarks on gradient dominance and LQR policy optimization](https://arxiv.org/abs/2507.10452)
*Eduardo D. Sontag*

Main category: cs.LG

TL;DR: 本研究探讨了解决优化问题（包括强化学习中的策略优化）的梯度下降变种的通用PLI条件，以解决连续时间和离散时间行为之间的差异，并利用ISS分析来解决梯度估计误差问题。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决强化学习中的策略优化问题，并填补连续时间和离散时间行为在Polyak-Lojasiewicz不等式（PLI）收敛率方面的差距，同时分析梯度估计误差的影响。

Method: 利用“状态输入稳定性”（ISS）分析来解决梯度估计误差问题，并探讨了“线性前馈神经网络”在反馈控制中的收敛性和类PLI性质。

Result: 该研究提出了一种新的分析方法，以解决优化问题和强化学习中的收敛性问题，并分析了梯度估计误差的影响。

Conclusion: 该研究提出了用于解决优化问题（包括强化学习中的策略优化）的梯度下降变种的通用PLI条件，以解决连续时间和离散时间行为之间的差异，并分析了梯度估计误差的影响。

Abstract: Solutions of optimization problems, including policy optimization in
reinforcement learning, typically rely upon some variant of gradient descent.
There has been much recent work in the machine learning, control, and
optimization communities applying the Polyak-{\L}ojasiewicz Inequality (PLI) to
such problems in order to establish an exponential rate of convergence (a.k.a.
``linear convergence'' in the local-iteration language of numerical analysis)
of loss functions to their minima under the gradient flow. Often, as is the
case of policy iteration for the continuous-time LQR problem, this rate
vanishes for large initial conditions, resulting in a mixed globally linear /
locally exponential behavior. This is in sharp contrast with the discrete-time
LQR problem, where there is global exponential convergence. That gap between CT
and DT behaviors motivates the search for various generalized PLI-like
conditions, and this talk will address that topic. Moreover, these
generalizations are key to understanding the transient and asymptotic effects
of errors in the estimation of the gradient, errors which might arise from
adversarial attacks, wrong evaluation by an oracle, early stopping of a
simulation, inaccurate and very approximate digital twins, stochastic
computations (algorithm ``reproducibility''), or learning by sampling from
limited data. We describe an ``input to state stability'' (ISS) analysis of
this issue. The lecture also discussed convergence and PLI-like properties of
``linear feedforward neural networks'' in feedback control, but this arXiv
skips that part (to be updated). Much of the work described here was done in
collaboration with Arthur Castello B. de Oliveira, Leilei Cui, Zhong-Ping
Jiang, and Milad Siami.

</details>


### [461] [The Target Polish: A New Approach to Outlier-Resistant Non-Negative Matrix and Tensor Factorization](https://arxiv.org/abs/2507.10484)
*Paul Fogel,Christophe Geissler,George Luta*

Main category: cs.LG

TL;DR: Target Polish 框架通过加权中值平滑数据，实现了比传统方法更快、更鲁棒的非负矩阵和张量分解。


<details>
  <summary>Details</summary>
Motivation: 传统的加权NMF方法虽然能抵抗异常值，但由于使用乘法更新来最小化目标判据，因此收敛速度缓慢。本研究旨在解决这个问题。

Method: Target Polish 方法通过自适应地使用基于加权中值的方法进行数据平滑，使其能够兼容以速度著称的Fast-HALS算法，从而在保持加性更新结构的同时，实现了鲁棒性。

Result: Target Polish 方法成功地提高了NMF的鲁棒性，同时保持了算法的高效性。在噪声干扰的图像数据集上的实验表明，该方法在准确性和计算效率上均优于现有方法。

Conclusion: Target Polish 方法在图像数据集的鲁棒性（包括结构化和非结构化噪声）方面，能够达到或超过最先进的鲁棒NMF方法的准确性，并且在所研究的场景中计算时间减少了一个数量级。

Abstract: This paper introduces the "Target Polish," a robust and computationally
efficient framework for nonnegative matrix and tensor factorization. Although
conventional weighted NMF approaches are resistant to outliers, they converge
slowly due to the use of multiplicative updates to minimize the objective
criterion. In contrast, the Target Polish approach remains compatible with the
Fast-HALS algorithm, which is renowned for its speed, by adaptively smoothing
the data with a weighted median-based transformation. This innovation provides
outlier resistance while maintaining the highly efficient additive update
structure of Fast-HALS. Empirical evaluations using image datasets corrupted
with structured (block) and unstructured (salt) noise demonstrate that the
Target Polish approach matches or exceeds the accuracy of state-of-the-art
robust NMF methods and reduces computational time by an order of magnitude in
the studied scenarios.

</details>


### [462] [Overcoming catastrophic forgetting in neural networks](https://arxiv.org/abs/2507.10485)
*Brandon Shuen Yi Loke,Filippo Quadri,Gabriel Vivanco,Maximilian Casagrande,Saúl Fenollosa*

Main category: cs.LG

TL;DR: EWC reduces catastrophic forgetting in continual learning, but slightly slows down learning new tasks. It's a promising method for lifelong learning.


<details>
  <summary>Details</summary>
Motivation: To address catastrophic forgetting, the primary challenge hindering continual learning, by evaluating the effectiveness of Elastic Weight Consolidator (EWC).

Method: Replication and extension of prior research by evaluating EWC in supervised learning settings using PermutedMNIST and RotatedMNIST benchmarks, with systematic comparisons to L2 regularization and SGD without regularization. Investigation into the impact of dropout regularization and varying hyperparameters.

Result: EWC significantly reduces forgetting compared to naive training, but slightly compromises learning efficiency on new tasks. The study also provides insights into the generalization of EWC across diverse learning scenarios by investigating the impact of dropout and hyperparameters.

Conclusion: Elastic Weight Consolidation (EWC) is a viable solution for lifelong learning in neural networks, as it significantly reduces forgetting compared to naive training, albeit with a slight compromise in learning efficiency on new tasks.

Abstract: Catastrophic forgetting is the primary challenge that hinders continual
learning, which refers to a neural network ability to sequentially learn
multiple tasks while retaining previously acquired knowledge. Elastic Weight
Consolidation, a regularization-based approach inspired by synaptic
consolidation in biological neural systems, has been used to overcome this
problem. In this study prior research is replicated and extended by evaluating
EWC in supervised learning settings using the PermutedMNIST and RotatedMNIST
benchmarks. Through systematic comparisons with L2 regularization and
stochastic gradient descent (SGD) without regularization, we analyze how
different approaches balance knowledge retention and adaptability. Our results
confirm what was shown in previous research, showing that EWC significantly
reduces forgetting compared to naive training while slightly compromising
learning efficiency on new tasks. Moreover, we investigate the impact of
dropout regularization and varying hyperparameters, offering insights into the
generalization of EWC across diverse learning scenarios. These results
underscore EWC's potential as a viable solution for lifelong learning in neural
networks.

</details>


### [463] [Split Happens: Combating Advanced Threats with Split Learning and Function Secret Sharing](https://arxiv.org/abs/2507.10494)
*Tanveer Khan,Mindaugas Budzys,Antonis Michalas*

Main category: cs.LG

TL;DR: SplitHappens结合了函数秘密共享（FSS）和U型分割学习（U-SL），提高了数据隐私和安全性，能抵御多种攻击，并降低了通信和计算成本，同时保持了模型精度。


<details>
  <summary>Details</summary>
Motivation: 现有的分割学习（SL）虽然能在保护客户端数据隐私的同时增强机器学习（ML），但易受多种攻击。虽然函数秘密共享（FSS）提供了一种安全SL的范式，但其在应对日益增多的SL攻击方面仍显不足。因此，需要一种能进一步提高隐私保护能力并应对更多攻击的方法。

Method: SplitHappens将函数秘密共享（FSS）与U型分割学习（U-SL）相结合，将模型分割，使得客户端能够将训练数据的标签隐藏，无需与服务器共享。这种方法通过隐藏客户端的标签来提供更高的安全保证。

Result: 实验证明，SplitHappens在将FSS应用于U型SL时，能够有效降低通信和计算成本。与单独使用FSS相比，该方法在保持模型精度的同时，还能减少训练时间和通信开销。其安全分析也从现有研究扩展到更广泛的攻击向量，如模型反演和标签推断攻击。

Conclusion: SplitHappens通过将函数秘密共享（FSS）与U型模型分割学习（SL）相结合，提高了客户端数据的隐私性，特别是标签的保密性，同时通过减少通信和计算成本来提高效率。该方法能够抵御包括模型反演和标签推断在内的多种攻击，并且在保持现有模型精度的同时，降低了训练时间和通信成本。

Abstract: Split Learning (SL) -- splits a model into two distinct parts to help protect
client data while enhancing Machine Learning (ML) processes. Though promising,
SL has proven vulnerable to different attacks, thus raising concerns about how
effective it may be in terms of data privacy. Recent works have shown promising
results for securing SL through the use of a novel paradigm, named Function
Secret Sharing (FSS), in which servers obtain shares of a function they compute
and operate on a public input hidden with a random mask. However, these works
fall short in addressing the rising number of attacks which exist on SL. In
SplitHappens, we expand the combination of FSS and SL to U-shaped SL. Similarly
to other works, we are able to make use of the benefits of SL by reducing the
communication and computational costs of FSS. However, a U-shaped SL provides a
higher security guarantee than previous works, allowing a client to keep the
labels of the training data secret, without having to share them with the
server. Through this, we are able to generalize the security analysis of
previous works and expand it to different attack vectors, such as modern model
inversion attacks as well as label inference attacks. We tested our approach
for two different convolutional neural networks on different datasets. These
experiments show the effectiveness of our approach in reducing the training
time as well as the communication costs when compared to simply using FSS while
matching prior accuracy.

</details>


### [464] [Benchmarking and Evaluation of AI Models in Biology: Outcomes and Recommendations from the CZI Virtual Cells Workshop](https://arxiv.org/abs/2507.10502)
*Elizabeth Fahsbender,Alma Andersson,Jeremy Ash,Polina Binder,Daniel Burkhardt,Benjamin Chang,Georg K. Gerber,Anthony Gitter,Patrick Godau,Ankit Gupta,Genevieve Haliburton,Siyu He,Trey Ideker,Ivana Jelic,Aly Khan,Yang-Joon Kim,Aditi Krishnapriyan,Jon M. Laurent,Tianyu Liu 28,Emma Lundberg,Shalin B. Mehta,Rob Moccia,Angela Oliveira Pisco,Katherine S. Pollard,Suresh Ramani,Julio Saez-Rodriguez,Yasin Senbabaoglu,Elana Simon,Srinivasan Sivanandan,Gustavo Stolovitzky,Marc Valer,Bo Wang,Xikun Zhang,James Zou,Katrina Kalantar*

Main category: cs.LG

TL;DR: 人工智能在生物学中有巨大潜力，但缺乏标准化的基准。本次研讨会旨在解决此问题，确定了数据异质性、可重复性和偏差等挑战，并提出了改进建议，以加速开发用于虚拟细胞的人工智能基准，从而提高研究的严谨性和可重复性，并推动新的发现。


<details>
  <summary>Details</summary>
Motivation: 人工智能在生物学领域具有巨大潜力，但缺乏标准化的跨领域基准阻碍了构建鲁棒、可信赖的模型。

Method: 组织了一次汇集了来自成像、转录组学、蛋白质组学和基因组学领域的机器学习和计算生物学专家的研讨会，以解决标准化跨领域基准的缺失问题。

Result: 确定了主要的技术和系统瓶颈，例如数据异质性和噪声、可重复性挑战、偏差以及公共可用资源碎片化的问题，并提出了一系列关于构建能够有效比较跨任务和数据模式的生物系统机器学习模型的基准框架的建议。

Conclusion: 通过推广高质量数据管理、标准化工具、全面的评估指标以及开放的协作平台，旨在加速构建人工智能驱动的虚拟细胞的鲁棒基准。这些基准对于确保严谨性、可重复性和生物学相关性至关重要，并最终推动该领域向能够驱动新发现、治疗见解和对细胞系统更深入理解的集成模型迈进。

Abstract: Artificial intelligence holds immense promise for transforming biology, yet a
lack of standardized, cross domain, benchmarks undermines our ability to build
robust, trustworthy models. Here, we present insights from a recent workshop
that convened machine learning and computational biology experts across
imaging, transcriptomics, proteomics, and genomics to tackle this gap. We
identify major technical and systemic bottlenecks such as data heterogeneity
and noise, reproducibility challenges, biases, and the fragmented ecosystem of
publicly available resources and propose a set of recommendations for building
benchmarking frameworks that can efficiently compare ML models of biological
systems across tasks and data modalities. By promoting high quality data
curation, standardized tooling, comprehensive evaluation metrics, and open,
collaborative platforms, we aim to accelerate the development of robust
benchmarks for AI driven Virtual Cells. These benchmarks are crucial for
ensuring rigor, reproducibility, and biological relevance, and will ultimately
advance the field toward integrated models that drive new discoveries,
therapeutic insights, and a deeper understanding of cellular systems.

</details>


### [465] [On the Performance of Differentially Private Optimization with Heavy-Tail Class Imbalance](https://arxiv.org/abs/2507.10536)
*Qiaoyue Tang,Alain Zhiyanov,Mathias Lécuyer*

Main category: cs.LG

TL;DR: DP-AdamBC在处理重尾类别不平衡数据时，比DP-GD表现更好，能提高低频类别的训练准确率。


<details>
  <summary>Details</summary>
Motivation: 在重尾类别不平衡分布下，分析常见私有学习优化算法的优化行为。

Method: 本研究分析了在重尾类别不平衡分布下，常见的私有学习优化算法的优化行为。

Result: DP-AdamBC在学习频率最低的类别时，训练准确率分别提高了约8%和约5%。

Conclusion: DP-AdamBC通过消除估计损失曲率的DP偏差，是避免由重尾类别不平衡引起的病态条件的关键组成部分。在受控实验和真实数据上，DP-AdamBC在学习频率最低的类别时，训练准确率分别提高了约8%和约5%，从而在经验上更适合数据。

Abstract: In this work, we analyze the optimization behaviour of common private
learning optimization algorithms under heavy-tail class imbalanced
distribution. We show that, in a stylized model, optimizing with Gradient
Descent with differential privacy (DP-GD) suffers when learning low-frequency
classes, whereas optimization algorithms that estimate second-order information
do not. In particular, DP-AdamBC that removes the DP bias from estimating loss
curvature is a crucial component to avoid the ill-condition caused by
heavy-tail class imbalance, and empirically fits the data better with
$\approx8\%$ and $\approx5\%$ increase in training accuracy when learning the
least frequent classes on both controlled experiments and real data
respectively.

</details>


### [466] [Graph World Model](https://arxiv.org/abs/2507.10539)
*Tao Feng,Yexin Wu,Guanyu Lin,Jiaxuan You*

Main category: cs.LG

TL;DR: 提出图世界模型（GWM），一种能处理非结构化数据、图结构数据和多模态信息的世界模型，并通过动作节点支持多样化任务。GWM在多项任务中表现出色，具有良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的世界模型主要关注非结构化数据，无法利用现实世界中广泛存在的图结构数据。同时，图神经网络模型专注于图学习任务，难以扩展到多模态数据和跨学科任务。为了解决这些挑战，需要一个能够同时处理非结构化和图结构数据，并支持多模态信息和多样化任务的世界模型。

Method: GWM通过通用的消息传递算法来聚合结构化信息。它支持两种模式：GWM-T将多模态数据转换为文本，处理统一的多模态文本空间；GWM-E使用特定模态的编码器，处理统一的多模态嵌入空间。GWM还引入了动作节点，通过直接引用或相似度计算与图中的其他节点相连，以支持多样化任务。

Result: 在六个不同领域的任务（包括多模态生成和匹配、推荐、图预测、多智能体、检索增强生成以及规划和优化）上的广泛实验表明，GWM的性能优于或媲美领域特定模型。此外，GWM能够利用多跳结构，并在未见过的任务上展现出强大的零样本/少样本能力。

Conclusion: 该研究提出了图世界模型（GWM），一个能够处理非结构化和图结构状态，并融合多模态信息和多样化任务的世界模型。GWM通过通用的消息传递算法聚合结构化信息，并引入动作节点来支持多样化任务。实验结果表明，GWM在多个跨领域任务中表现优于或媲美领域特定模型，并展现出强大的零样本/少样本能力。

Abstract: World models (WMs) demonstrate strong capabilities in prediction, generation,
and planning tasks. Existing WMs primarily focus on unstructured data and
cannot leverage the ubiquitous structured data, often represented as graphs, in
the digital world. While multiple graph foundation models have been proposed,
they focus on graph learning tasks and cannot extend to diverse multi-modal
data and interdisciplinary tasks. To address these challenges, we propose the
Graph World Model (GWM), a world model that supports both unstructured and
graph-structured states with multi-modal information and represents diverse
tasks as actions. The core of a GWM is a generic message-passing algorithm to
aggregate structured information, either over a unified multi-modal token space
by converting multi-modal data into text (GWM-T) or a unified multi-modal
embedding space by modality-specific encoders (GWM-E). Notably, GWM introduces
action nodes to support diverse tasks, where action nodes are linked to other
nodes via direct reference or similarity computation. Extensive experiments on
six tasks from diverse domains, including multi-modal generation and matching,
recommendation, graph prediction, multi-agent, retrieval-augmented generation,
and planning and optimization, show that the same GWM outperforms or matches
domain-specific baselines' performance, benefits from multi-hop structures, and
demonstrates strong zero-shot/few-shot capabilities on unseen new tasks. Our
code for GWM is released at https://github.com/ulab-uiuc/GWM.

</details>


### [467] [Fusing LLM Capabilities with Routing Data](https://arxiv.org/abs/2507.10540)
*Tao Feng,Haozhen Zhang,Zijie Lei,Pengrui Han,Mostofa Patwary,Mohammad Shoeybi,Bryan Catanzaro,Jiaxuan You*

Main category: cs.LG

TL;DR: LLM routing data can reveal model strengths. FusionBench benchmark and FusionFactory framework improve LLM performance by fusing models at query, thought, and model levels.


<details>
  <summary>Details</summary>
Motivation: Most applications use a single LLM backend, limiting capabilities and efficiency. LLM routing data reveals comparative strengths across tasks, an underexploited opportunity.

Method: Proposes FusionBench benchmark (14 tasks, 5 domains, 20 LLMs, 103M tokens) and FusionFactory framework (query-level, thought-level, model-level fusion).

Result: FusionFactory consistently outperforms the best individual LLM across all 14 benchmarks.

Conclusion: FusionFactory systematically fuses LLMs, outperforming individual models across benchmarks by leveraging complementary strengths. Optimal fusion configurations are benchmark-specific.

Abstract: The rapid advancement of large language models (LLMs) has created a vibrant
ecosystem of diverse architectures, each with unique strengths due to
differences in design, training data, and objectives. However, most
applications still rely on a single backend model, limiting coverage of
capabilities and leading to inefficiencies in performance and token cost when
tackling complex tasks. We highlight an underexploited opportunity: LLM routing
data, produced when hosting platforms route diverse queries to different
models, which can reveal comparative strengths across tasks. To address this,
we propose FusionBench, a comprehensive routing benchmark covering 14 tasks
across five domains with 20 open-source LLMs (8B to 671B parameters), capturing
103M tokens and summarizing reusable thought templates from top models.
Building on this, we introduce FusionFactory, a systematic fusion framework
with three levels: (1) query-level fusion, tailoring routers for each query
using both direct responses and reasoning-augmented outputs; (2) thought-level
fusion, leveraging abstract templates derived from top-performing LLMs' answers
to similar queries; and (3) model-level fusion, transferring capabilities
between models via distillation, using top responses or highest judge scores as
training data. Experiments show FusionFactory consistently outperforms the best
individual LLM across all 14 benchmarks, with optimal fusion configurations
varying by benchmark, demonstrating the value of systematic LLM fusion in
harnessing complementary strengths and improving overall performance.

</details>


### [468] [Disentangling Neural Disjunctive Normal Form Models](https://arxiv.org/abs/2507.10546)
*Kexin Gu Baugh,Vincent Perreault,Matthew Baugh,Luke Dickens,Katsumi Inoue,Alessandra Russo*

Main category: cs.LG

TL;DR: 神经DNF模型在符号转换过程中存在性能下降问题，主要是由于未能有效分解网络权重中蕴含的知识。本文提出一种新的分解方法，通过将节点分裂成更小的独立单元，改善了知识表示的解耦，从而在不损害模型性能的情况下，为神经DNF模型生成了更紧凑、更易于理解的逻辑表示，特别是在处理需要谓词发明的复杂分类任务时，其效果尤为显著。


<details>
  <summary>Details</summary>
Motivation: 神经析取范式（DNF）模型在无需任务先验知识的分类和强化学习中表现出强大的能力和可解释性，但其性能在训练后符号转换过程中受到阈值处理的损害。研究发现，部分性能下降是由于其未能分解网络权重中表示的学习知识。

Method: 提出了一种新的分解方法，通过将编码嵌套规则的节点分裂为更小的独立节点来解决知识分解问题。

Result: 实验证明，该分解方法在二元、多类别和多标签分类任务上（包括需要谓词发明的任务）提供了紧凑且可解释的逻辑表示，性能更接近翻译前的模型。

Conclusion: 该方法通过将编码嵌套规则的节点分解为更小的独立节点，能够更好地保持模型性能，并为神经DNF模型提供紧凑且可解释的逻辑表示，性能更接近翻译前的模型。

Abstract: Neural Disjunctive Normal Form (DNF) based models are powerful and
interpretable approaches to neuro-symbolic learning and have shown promising
results in classification and reinforcement learning settings without prior
knowledge of the tasks. However, their performance is degraded by the
thresholding of the post-training symbolic translation process. We show here
that part of the performance degradation during translation is due to its
failure to disentangle the learned knowledge represented in the form of the
networks' weights. We address this issue by proposing a new disentanglement
method; by splitting nodes that encode nested rules into smaller independent
nodes, we are able to better preserve the models' performance. Through
experiments on binary, multiclass, and multilabel classification tasks
(including those requiring predicate invention), we demonstrate that our
disentanglement method provides compact and interpretable logical
representations for the neural DNF-based models, with performance closer to
that of their pre-translation counterparts. Our code is available at
https://github.com/kittykg/disentangling-ndnf-classification.

</details>


<div id='cond-mat.mes-hall'></div>

# cond-mat.mes-hall [[Back]](#toc)

### [469] [Anyon-trions in atomically thin semiconductor heterostructures](https://arxiv.org/abs/2507.08933)
*Nader Mostaan,Nathan Goldman,Ataç İmamoğlu,Fabian Grusdt*

Main category: cond-mat.mes-hall

TL;DR: 本研究提出了一种新的“任意子-三子”复合激发态，并证明了其结合能与分数电荷的关系。该研究为未来在量子系统中实现对任意子的操控和编织提供了新的理论基础和实验思路。


<details>
  <summary>Details</summary>
Motivation: 为了实现对任意子的可控编织，需要能够在体相中对单个任意子进行探测和操控，而现有的边缘态干涉测量方法存在局限性。因此，本研究旨在探索一种新的探测和操控任意子的方法。

Method: 本研究利用精确对角化方法，对长寿命的光学生成层间激子与分数化量子霍尔态中的准空穴的结合进行了理论研究，并计算了其结合能。

Result: 研究表明，移动的任意子-三子具有约0.5 meV的结合能，而静止的任意子-三子具有约0.9 meV的结合能，且该结合能与准空穴的分数电荷呈线性关系。此外，研究还提出了一种基于量子旋转显微镜和局域层间激子光致发光进行光学观测的实验方案。

Conclusion: 本研究提出了一个初步的解决方案，可以通过光学的手段来结合准空穴与长寿命的外延片层激子，形成一种称为“任意子-三子”的复合激发态，并展示了其理论可行性。

Abstract: The study of anyons in topologically ordered quantum systems has mainly
relied on edge-state interferometry. However, realizing controlled braiding of
anyons necessitates the ability to detect and manipulate individual anyons
within the bulk. Here, we propose and theoretically investigate a first step
toward this goal by demonstrating that a long-lived, optically generated
interlayer exciton can bind to a quasihole in a fractional quantum Hall state,
forming a composite excitation we term an anyon-trion. Using exact
diagonalization, we show that mobile anyon-trions possess a binding energy of
approximately 0.5 meV, whereas static anyon-trions exhibit a binding energy of
about 0.9 meV, that is linearly proportional to the quasiholes fractional
charge. An experimental realization based on photoluminescence from localized
interlayer excitons in a quantum twisting microscope setup should allow for a
direct optical observation of anyon-trions.

</details>


### [470] [Universal scaling of microwave dissipation in superconducting circuits](https://arxiv.org/abs/2507.08953)
*Thibault Charpentier,Anton Khvalyuk,Lev Ioffe,Mikhail Feigel'man,Nicolas Roch,Benjamin Sacépé*

Main category: cond-mat.mes-hall

TL;DR: 超导体的微波耗散与其超流体密度相关，这揭示了一种本征耗散机制，可能限制量子比特的相干性。


<details>
  <summary>Details</summary>
Motivation: 提高超导量子比特的相干性对于推进量子技术至关重要，而超导体在微波驱动下表现出的残余能量耗散限制了相干时间。

Method: 通过分析大量超导材料和器件几何形状的微波耗散与超流体密度的普遍标度关系来研究。

Result: 发现微波耗散与超流体密度之间存在普遍的标度关系，该关系跨越了多种超导材料和器件几何形状，揭示了一种本征的体耗散通道，该通道是由于非平衡准粒子密度以及超导间隙的空间变化引起的。

Conclusion: 这项研究揭示了一种普遍存在的本征体耗散通道，该通道独立于表面介电损耗，源于由超导间隙的无序引起的空间变化中捕获的非平衡准粒子的普遍密度。这为选择材料和设计下一代超导量子电路提供了基础材料属性的根本限制和预测框架。

Abstract: Improving the coherence of superconducting qubits is essential for advancing
quantum technologies. While superconductors are theoretically perfect
conductors, they consistently exhibit residual energy dissipation when driven
by microwave currents, limiting coherence times. Here, we report a universal
scaling between microwave dissipation and the superfluid density, a bulk
property of superconductors related to charge carrier density and disorder. Our
analysis spans a wide range of superconducting materials and device geometries,
from highly disordered amorphous films to ultra-clean systems with record-high
quality factors, including resonators, 3D cavities, and transmon qubits. This
scaling reveals an intrinsic bulk dissipation channel, independent of surface
dielectric losses, that originates from a universal density of nonequilibrium
quasiparticles trapped within disorder-induced spatial variations of the
superconducting gap. Our findings define a fundamental limit to coherence set
by intrinsic material properties and provide a predictive framework for
selecting materials and the design of next-generation superconducting quantum
circuits.

</details>


### [471] [Tunneling spin Hall effect induced by unconventional $p$-wave magnetism](https://arxiv.org/abs/2507.09112)
*W. Zeng*

Main category: cond-mat.mes-hall

TL;DR: 在正常金属/p波磁体/超导体结中发现了隧道自旋霍尔效应，它产生纯横向自旋流，可通过费米面分裂调控，在spintronic器件中具有应用潜力。


<details>
  <summary>Details</summary>
Motivation: 研究正常金属/p波磁体/超导体结中的隧道自旋霍尔效应及其产生机制。

Method: 使用非平衡格林函数方法解析推导了横向自旋电导。

Result: 发现了自旋依赖且关于横向动量具有强不对称性的安德烈夫反射，产生了纯横向自旋霍尔电流。当费米面分裂中心连线垂直于结的法线方向时，会出现有限的横向自旋流和大的自旋霍尔角，实现了高效的电荷到自旋的转换。

Conclusion: 该研究发现了新的自旋霍尔效应现象，即在正常金属/p波磁体/超导体结中存在隧道自旋霍尔效应，其特点是纯横向自旋霍尔电流且净电荷为零，并且可以通过p波磁体中的费米面分裂方向来调控，在spintronic器件中具有潜在应用价值。

Abstract: We propose a tunneling spin Hall effect in a normal metal/$p$-wave
magnet/superconductor junction. It is found that the Andreev reflection in the
normal lead is spin-dependent and exhibits strong asymmetry with respect to the
transverse momentum, giving rise to a pure transverse spin Hall current with
zero net charge. The transverse spin conductance is analytically derived using
the nonequilibrium Green's function approach, revealing that the predicted spin
Hall effect is governed by the direction of the Fermi surface splitting in the
$p$-wave magnet. A finite transverse spin current with a large spin Hall angle
arises when the line connecting the centers of the spin-split Fermi surfaces is
perpendicular to the normal direction of the junction, which indicates a highly
efficient charge-to-spin conversion, suggesting potential applications in
spintronic devices.

</details>


### [472] [Influence of thermal noise on the field-driven dynamics of the non-collinear antiferromagnet Mn3Sn](https://arxiv.org/abs/2507.09143)
*Siyuan Qian,Ankit Shukla,Shaloo Rakheja*

Main category: cond-mat.mes-hall

TL;DR: 通过谐波过渡态理论，我们导出了应变Mn3Sn薄膜中磁八极矩弛豫时间的解析表达式，该弛豫时间受外部磁场和热噪声影响，并得到了数值模拟的验证，为Mn3Sn在随机数生成和概率计算中的应用奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 研究背景是Mn3Sn在MgO衬底上外延生长时会经历拉伸应变，导致其能量景观从六重对称变为二重对称，外部磁场进一步打破了对称性，使得能量景观对磁场方向敏感。在热噪声存在下，应变Mn3Sn薄膜的磁八极矩弛豫包含四个独特的逃逸过程。

Method: 采用谐波过渡态理论推导了势垒越越时间（越过势垒所需时间）和磁八极矩弛豫时间的解析表达式，并考虑了外部磁场和热噪声在中高阻尼情况下的影响，随后通过耦合LLG方程进行了数值模拟验证。

Result: 研究结果表明，所推导的解析表达式与基于耦合LLG方程的数值模拟结果高度吻合，验证了理论模型的准确性。

Conclusion: 该研究为实现Mn3Sn在随机数生成和概率计算中的应用提供了关键见解。

Abstract: $\mathrm{Mn_3Sn}(0\overline{1}\overline{1}0)[0001]$ experiences a tensile
strain when grown epitaxially on $\mathrm{MgO}(110)[001]$, and thus the energy
landscape changes from six-fold symmetry to two-fold symmetry. External
magnetic field further breaks the symmetry and the \textcolor{black}{resulting
energy landscape is sensitive to} the field orientation relative to the easy
axis. \textcolor{black}{In the presence of thermal noise,} the relaxation of
the magnetic octupole moment in \textcolor{black}{a strained Mn$_3$Sn film} is
composed of four distinct escape processes involving the two saddle points and
two equilibrium states in the energy landscape. Here, we apply harmonic
transition-state theory to derive analytical expressions for the inter-well
escape time and octupole moment relaxation time, both influenced by an external
symmetry-breaking magnetic field and finite thermal noise in the
intermediate-to-high damping regime. The analytical predictions are in strong
agreement with comprehensive numerical simulations based on coupled LLG
equations. The results presented here are crucial toward realizing Mn$_3$Sn's
applications in random number generation and probabilistic computing.

</details>


### [473] [Magnon Correlation Enables Spin Injection, Dephasing, and Transport in Canted Antiferromagnets](https://arxiv.org/abs/2507.09465)
*Xiyin Ye,Tao Yu*

Main category: cond-mat.mes-hall

TL;DR: 反铁磁体中的磁旋自旋传输不只依赖于磁旋布居数，还与非对角关联和量子相干性有关，并会受到自旋扭矩、磁场和外在退相干的调控。


<details>
  <summary>Details</summary>
Motivation: 现有理论认为磁 المرسلين尖峰绝缘体中磁旋自旋的传输和输运是基于非平衡磁旋布居数，然而，近期的实验结果对该观点提出了挑战，尤其是在非共线反铁磁体中，这促使研究者从根本上进行理论探索。

Method: 该研究提出了一个量子理论模型，其中磁矩在反铁磁体中由一个矩阵描述，并证明了即使在自旋流的对角项为零的情况下，非对角关联也能够传输自旋。该模型揭示了电子注入过程中会产生一个动量依赖的自旋注入，进而激发了磁扰动。

Result: 研究发现，当电子注入产生净自旋翻转时，会在磁绝缘体中激发磁旋波，进而将自旋输运到相邻的导体中。在该过程中，由电子注入产生的动量依赖自旋注入会激发磁扰动，并与磁旋波发生相互作用，从而将自旋输运到导体中。

Conclusion: 研究发现，自旋流在磁性绝缘体中传输时，非对齐反铁磁体的自旋矩会受到内禀自旋扭矩的调制，导致其在传输过程中的退相干和空间振荡，并且该效应能够被磁场增强。此外，外禀退相干会抑制自旋振荡，并对自旋传输起到门控作用。

Abstract: Thermal and electrical injection and transport of magnon spins in magnetic
insulators is conventionally understood by the non-equilibrium population of
magnons. However, this view is challenged by several recent experiments in
noncollinear antiferromagnets, which urge a thorough theoretical investigation
at the fundamental level. We find that the magnon spin in antiferromagnets is
described by a matrix, so even when the diagonal terms -- spins carried by
population -- vanish, the off-diagonal correlations transmit magnon spins. Our
quantum theory shows that a net spin-flip of electrons in adjacent conductors
creates quantum coherence between magnon states, which transports magnon spins
in canted antiferromagnets, even without a definite phase difference between
magnon modes in the incoherent process. It reveals that the pumped magnon
correlation is not conserved due to an intrinsic spin torque, which causes
dephasing and strong spatial spin oscillations during transport; both are
enhanced by magnetic fields. Spin transfer to proximity conductors can cause
extrinsic dephasing, which suppresses spin oscillations and thereby gates spin
transport.

</details>


### [474] [Magnetic control of nonlinear transport induced by the quantum metric](https://arxiv.org/abs/2507.09476)
*Xu Chen,Mingbo Dou,Qin Zhang,Xianjie Wang,M. Ye. Zhuravlev,A. V. Nikolaev,L. L. Tao*

Main category: cond-mat.mes-hall

TL;DR: 研究了磁场对二维材料中由量子度量引起的非线性输运的影响，发现输运特性具有各向异性且依赖于磁场方向和自旋-轨道耦合类型。


<details>
  <summary>Details</summary>
Motivation: 研究量子材料中的非线性输运及其与量子几何的联系，特别是磁场如何影响这种输运。

Method: 采用玻尔兹曼输运形式，研究了量子度量在二维材料中磁场调控的非线性输运特性，并考虑了不同类型的自旋-轨道耦合（SOC）。

Result: 发现非线性电导率具有显著的空间各向异性，且与磁场方向密切相关，不同SOC类型下表现出不同的行为。量子度量和载流子输运机制的贡献可以被区分开。

Conclusion: 该研究阐明了量子几何与非线性输运之间的相互作用，并为区分量子度量和载流子输运机制的贡献提供了依据。

Abstract: The quantum geometry plays a crucial role in the nonlinear transport of
quantum materials. Here, we use the Boltzmann transport formalism to study the
magnetic control of nonlinear transport induced by the quantum metric in
two-dimensional systems with different types of spin-orbit coupling (SOC). It
is shown that the nonlinear conductivity is strongly dependent on the direction
of a field and reveals significant spatial anisotropy. Moreover, the
field-direction dependent relations are distinct for different SOCs. In
addition, it is demonstrated that the contributions from the quantum metric and
Drude mechanism are distinguishable due to their opposite signs or distinct
anisotropy relations. We further derive the analytical formulas for the
anisotropic nonlinear conductivity, in exact agreement with numerical results.
Our work shines more light on the interplay between the nonlinear transport and
quantum geometry.

</details>


### [475] [Observation of Quantum Coulomb Blockade Facilitated by P-Donor Molecules in Silicon Nano-Transistor](https://arxiv.org/abs/2507.09791)
*Soumya Chakraborty,Pooja Sudha,Hemant Arora,Daniel Moraru,Arup Samanta*

Main category: cond-mat.mes-hall

TL;DR: 本研究在多给体硅基结构中实现了逐一电子填充和量子库仑 بلوک पाहिजे 效应的实验验证，揭示了分子轨道特性和能级行为，并利用DFT和蒙特卡罗模拟进行了理论支持。


<details>
  <summary>Details</summary>
Motivation: 为了理解多给体结构中电子隧穿和交换相互作用，需要探测离散能级，这对于实现室温量子比特和其他单电子隧穿（SET）功能至关重要。

Method: 本研究通过实验演示了多给体分子中逐一电子填充，并进行了清晰且持续的量子库仑 بلوک पाहिजे 效应的基本分析。利用密度泛函理论（DFT）进行第一性原理模拟，并结合基于库仑 بلوک पाहिजे orthodox 理论的蒙特卡罗模拟来验证研究结果。

Result: 研究观察到了清晰且持续的量子库仑 بلوک पाहिजे 效应，并实现了多给体分子中逐一电子填充。研究还发现，随着能量增加，分子轨道的空间范围增大，相应的充电能量系统性地减小。

Conclusion: 研究确认了分子轨道杂化产生的分子能级，并利用密度泛函理论（DFT）进行第一性原理模拟验证。此外，基于库仑 بلوک पाहिजे orthodox 理论的蒙特卡罗模拟也支持观察到的量子库仑 بلوک पाहिजे 特征。

Abstract: Multi-donor architecture developed on the base of silicon technology holds
significant potential towards room-temperature qubit and other single-electron
tunneling (SET) functionalities. However, within such architecture, the overlap
of multiple donor wave-functions results in a complex internal electronic
configuration with discrete energy levels. Probing these discrete states,
observed as multiple conductance peaks, is essential for understanding
inter-donor coupling and exchange interactions towards coherent electron
transfer. In this direction, we have experimentally demonstrated one-by-one
electron filling within multiple-donor molecules with the fundamental analysis
of clear and sustained quantum Coulomb blockade (QCB) effect. Moreover, the
underlying physics of molecular orbitals, where the increasing energy leads to
a larger spatial extent of the corresponding orbital, has been reflected by the
systematic decrement of the respective charging-energies. The molecular energy
levels, resulting from the orbital hybridization of individual donors, are also
confirmed through first-principles simulations using density functional theory
(DFT). Furthermore, Monte Carlo simulations based on the orthodox theory of
Coulomb blockade support the observed QCB characteristics.

</details>


### [476] [Unlocking Altermagnetism in Antiferromagnetic 2D Films via Adsorption](https://arxiv.org/abs/2507.09518)
*Dong Liu,Sike Zeng,Ji-Hai Liao,Yu-Jun Zhao*

Main category: cond-mat.mes-hall

TL;DR: 通过对称工程策略，在二维反铁磁材料中诱导交替磁性。


<details>
  <summary>Details</summary>
Motivation: 为了在二维反铁磁系统中实现具有潜在应用价值的交替磁性。

Method: 通过自旋群论系统地分析了对称性对自旋分裂电子态的影响，并结合80个层群的对称操作分类，确定了15个可以通过表面吸附实现交替磁性的二维材料的自旋点群。

Result: 成功识别了15个具有交替磁性潜力的二维材料自旋点群，并通过氧吸附VPS_3和NH_3吸附MnPSe_3的实例，证明了该策略可以在不依赖自旋轨道耦合的情况下诱导显著的自旋分裂。

Conclusion: 本研究提出了一种通过表面吸附原子或分子来诱导二维反铁磁体实现交替磁性的对称工程策略，并已通过第一性原理计算和吸附能分析进行了验证，为扩展交替磁性材料家族提供了通用框架。

Abstract: Altermagnets, characterized by zero net magnetization and momentum-dependent
spin splitting, have recently garnered significant attention due to their
potential applications in a variety of fields. Here, we propose a
symmetry-engineering strategy to unlock altermagnetism in two dimensional (2D)
antiferromagnetic systems via surface adsorption of atoms or molecules. By
employing spin group theory, we systematically demonstrate that selectively
breaking symmetry operations, specifically those protecting spin degeneracy in
momentum space, enables the emergence of nonrelativistic spin-split electronic
states. Meanwhile, preserving rotation or mirror symmetries connecting opposite
sublattices ensures zero net magnetization. Through a comprehensive
classification of all symmetry operations across 80 layer groups, we identify
63 antiferromagnetic spin point groups (SPGs) describing 2D materials and
further isolate 15 groups that can host altermagnetic characteristics through
surface adsorption. Exemplified with monolayer antiferromagnetic VPS_3 and
MnPSe_3, we show that oxygen adsorption on VPS_3 and NH_3 adsorption on MnPSe_3
selectively disrupt PT symmetry while retaining the [C2||m] symmetry. This
engineered symmetry reduction induces pronounced spin splitting in their band
structures without spin-orbit coupling, as confirmed by first-principles
calculations. Furthermore, adsorption energy analysis and thermal stability
phase diagrams under varying coverage regimes reveal optimal configurations for
experimental feasibility. Our work establishes a universal symmetry-engineering
framework to expand the family of altermagnetic materials, offering a versatile
pathway to tailor spin-split functionalities in two-dimensional
antiferromagnets for advanced quantum applications.

</details>


### [477] [Bulk spin-orbit torque-driven spin Hall nano-oscillators using PtBi alloys](https://arxiv.org/abs/2507.10219)
*Utkarsh Shashank,Akash Kumar,Tahereh Sadat Parvini,Hauke Heyen,Lunjie Zeng,Andrew B. Yankovich,Mona Rajabali,Eva Olsson,Markus Münzenberg,Johan Åkerman*

Main category: cond-mat.mes-hall

TL;DR: 研究发现PtBi合金能显著提升自旋霍尔效应，降低自旋霍尔纳米振荡器的阈值电流，为开发新型节能微波器件提供了可能。


<details>
  <summary>Details</summary>
Motivation: 为了实现能量效率高、微型化的微波器件，需要降低自旋霍尔纳米振荡器（SHNOs）的阈值电流（Ith），而这又与提高自旋霍尔效率（θSH）密切相关。然而，传统的提高θSH的方法存在一些限制。

Method: 通过直流偏置自旋矩铁磁共振实验，提取了PtBi合金的自旋霍尔效率（θSH），并以此为基础，在基于PtBi合金的自旋霍尔纳米振荡器（SHNOs）中观察到了阈值电流（Ith）的显著降低。

Result:  PtBi合金的θSH相比纯Pt有显著提升（最高提升超过三倍），相应的SHNOs的Ith降低了42%和32%。结构表征显示，Bi的加入降低了Pt的结晶度，并出现了优先的晶体学取向。

Conclusion: PtBi合金相比于传统的5d过渡金属，在提高自旋霍尔效应（SH）和降低阈值电流（Ith）方面具有潜力，为开发节能型神经形态计算和磁性随机存取存储器提供了新的途径。

Abstract: Spin-orbit-torque-driven auto-oscillations in spin Hall nano-oscillators
(SHNOs) offer a transformative pathway toward energy-efficient, nanoscale
microwave devices for next-generation neuromorphic computing and high-frequency
technologies. A key requirement for achieving robust, sustained oscillations is
reducing the threshold current ($I_{\text{th}}$), strongly governed by spin
Hall efficiency ($\theta_{\text{SH}}$). However, conventional strategies to
enhance $\theta_{\text{SH}}$ face trade-offs, including high longitudinal
resistivity, interfacial effects, and symmetry-breaking torques that limit
performance. Here, we demonstrate a substantial enhancement of the bulk spin
Hall effect in PtBi alloys, achieving over a threefold increase in
$\theta_{\text{SH}}$, from 0.07 in pure Pt to 0.24 in Pt$_{94.0}$Bi$_{6.0}$ and
0.19 in Pt$_{91.3}$Bi$_{8.7}$, as extracted from DC-bias spin-torque
ferromagnetic resonance. The enhanced $\theta_{\text{SH}}$ originates from
bulk-dominated, extrinsic side-jump scattering across all PtBi compositions.
Correspondingly, we observe a 42\% and 32\% reduction in $I_{\text{th}}$ in 100
nm SHNOs based on Co$_{40}$Fe$_{40}$B$_{20}$(3 nm)/Pt$_{94.0}$Bi$_{6.0}$(4 nm)
and Co$_{40}$Fe$_{40}$B$_{20}$(3 nm)/Pt$_{91.3}$Bi$_{8.7}$(4 nm), respectively.
Structural characterization reveals reduced Pt crystallinity, along with
emergence of preferred crystallographic orientations upon introducing higher Bi
concentrations. Together, these results position PtBi alloys as a compelling
alternative to conventional 5$d$ transition metals, enabling enhanced
$\theta_{\text{SH}}$ and significantly lower $I_{\text{th}}$, thus opening new
avenues for energy-efficient neuromorphic computing and magnetic random access
memory.

</details>


### [478] [Vertically Coupled Double Quantum Dots Connected In Parallel](https://arxiv.org/abs/2507.09598)
*Shinichi Amaha,Tsuyoshi Hatano,Takashi Nakajima,Seigo Tarucha*

Main category: cond-mat.mes-hall

TL;DR: 在环形四重量子点系统中观察到库仑 दिसते和异旋挫折等现象。


<details>
  <summary>Details</summary>
Motivation: 研究相互作用驱动的量子相，利用垂直耦合引入与层索引相关的异旋自由度，并通过并联配置独立访问每个量子点对。

Method: 通过测量环形四重量子点系统（由两个垂直耦合的双量子点并联组成）中的电荷传输来研究。

Result: 观察到库仑 दिसते并评估了层间能量偏移，并探索了诸如异旋挫折等相关效应。

Conclusion: 该系统有潜力用于研究相互作用驱动的量子相

Abstract: We report charge transport measurements in a ring-shaped quadruple quantum
dot system, composed of two vertically coupled double quantum dots connected in
parallel. The vertical coupling introduces an isospin degree of freedom tied to
the layer index, and the parallel configuration enables independent access to
each quantum dot pair. This design allows us to observe Coulomb diamonds and
evaluate the interlayer energy offset. By extending this platform to triangular
and hexagonal artificial lattices, we explore correlation effects such as
isospin frustration. These results highlight the system's potential for
studying interaction-driven quantum phases.

</details>


### [479] [Quantum Hall-like effect for neutral particles with magnetic dipole moments in a quantum dot](https://arxiv.org/abs/2507.09604)
*Carlos Magno O. Pereira,Edilberto O. Silva*

Main category: cond-mat.mes-hall

TL;DR: 在中性系统中，电场和偶极矩的相互作用可实现量子霍尔效应，无需磁场，并具有拓扑保护、自旋控制和高温特性。


<details>
  <summary>Details</summary>
Motivation: 挑战传统观念，探索在无需朗道能级或外部磁场的情况下，在中性系统中实现量子霍尔效应的可能性，并揭示电场工程在拓扑物质研究中的潜力。

Method: 本研究提出了一种理论模型，通过分析径向电场和偶极矩在中性系统中的相互作用来预测量子霍尔现象。

Result: 成功预测了在中性系统中实现量子霍尔效应，并发现了拓扑保护、自旋控制和高温效应等新特性，证明了电场工程在拓扑物质领域的可行性。

Conclusion: 本研究预测了一类全新的量子霍尔现象，在完全中性的系统中，通过径向电场和偶极矩的相互作用，无需朗道能级或外部磁场即可实现精确的 e²/h 量化。研究表明：(i) 线电荷奇点不影响拓扑保护，(ii) 量化仅由边界条件产生自旋控制，(iii) 该效应可维持高达 25 K，超越了典型中性系统。这些发现证明电场工程是超越磁性范式的拓扑物质的可行途径。

Abstract: We predict a new class of quantum Hall phenomena in completely neutral
systems, demonstrating that the interplay between radial electric fields and
dipole moments induces exact $e^2/h$ quantization without the need for Landau
levels or external magnetic fields. Contrary to conventional wisdom, our theory
reveals that: (i) the singularity of line charges does not destroy topological
protection, (ii) spin-control of quantization emerges from boundary conditions
alone, and (iii) the effect persists up to 25 K, surpassing typical neutral
systems. These findings establish electric field engineering as a viable route
to topological matter beyond magnetic paradigms.

</details>


### [480] [Learning a potential formulation for rate-and-state friction](https://arxiv.org/abs/2507.09796)
*Shengduo Liu,Kaushik Bhattacharya,Nadia Lapusta*

Main category: cond-mat.mes-hall

TL;DR: 本研究提出了速率-状态摩擦的神经网络势能公式，实现了高效的数值计算。


<details>
  <summary>Details</summary>
Motivation: 经验速率-状态摩擦定律在模拟界面滑动时被广泛使用，但其缺乏势能或变分公式，使得隐式求解方法难以实现且数值实现成本高昂。本研究旨在解决这一问题。

Method: 通过将势能表述为神经网络，并进行训练以模拟经验速率-状态摩擦定律，从而构建速率-状态摩擦的势能公式。

Result: 提出的势能公式能够实现隐式时间离散化，从而在数值上实现高效的计算。

Conclusion: 该研究提出了一个速率-状态摩擦的势能公式，该公式允许隐式时间离散化，从而能够高效地进行数值计算。

Abstract: Empirical rate-and-state friction laws are widely used in geophysics and
engineering to simulate interface slip. They postulate that the friction
coefficient depends on the local slip rate and a state variable that reflects
the history of slip. Depending on the parameters, rate-and-state friction can
be either rate-strengthening, leading to steady slip, or rate-weakening,
leading to unsteady stick-slip behavior modeling earthquakes. Rate-and-state
friction does not have a potential or variational formulation, making implicit
solution approaches difficult and implementation numerically expensive. In this
work, we propose a potential formulation for the rate-and-state friction. We
formulate the potentials as neural networks and train them so that the
resulting behavior emulates the empirical rate-and-state friction. We show that
this potential formulation enables implicit time discretization leading to
efficient numerical implementation.

</details>


### [481] [Dissipation induced Majarona $0$- and $π$-modes in a driven Rashba nanowire](https://arxiv.org/abs/2507.10271)
*Koustabh Gogoi,Tanay Nag,Arnob Kumar Ghosh*

Main category: cond-mat.mes-hall

TL;DR: 在耗散背景下，周期性驱动的Rashba纳米线-超导体系统支持拓扑和非拓扑的边限局域模式，耗散会影响拓扑相图。


<details>
  <summary>Details</summary>
Motivation: 研究了在耗散背景下，与s波超导体相邻的周期性驱动Rashba纳米线的动力学。

Method: 使用第三量化方法获取描述系统光谱和拓扑性质的弗洛凯阻尼矩阵。

Result: 发现了边限局域化的拓扑Majorana 0-模式（MZMs）和$\%pi$-模式（MPMs），以及与Exceptional Points相关的非拓扑0-模式（TZMs）和$\%pi$-模式（TPMs）。耗散可以显著改变拓扑相图，甚至诱导新的拓扑相。

Conclusion: 通过研究驱动耗散系统，扩展了对驱动耗散拓扑超导体的理解。

Abstract: Periodic drive is an intriguing way of creating topological phases in a
non-topological setup. However, most systems are often studied as a closed
system, despite being always in contact with the environment, which induces
dissipation. Here, we investigate a periodically driven Rashba nanowire in
proximity to an $s$-wave superconductor in a dissipative background. The
system's dynamics is governed by a periodic Liouvillian operator, from which we
construct the Liouvillian time-evolution operator and use the
third-quantization method to obtain the `Floquet damping matrix', which
captures the spectral and topological properties of the system. We show that
the system exhibits edge-localized topological Majorana $0$-modes (MZMs) and
$\pi$-modes (MPMs). Additionally, the system also supports a trivial $0$-modes
(TZMs) and $\pi$-modes (TPMs), which are also localized at the edges of the
system. The MZMs and the MPMs are connected to the bulk topology and carry a
bulk topological invariant, while the emergence of TZMs and TPMs is primarily
tied to exceptional points and is topologically trivial. We study the
topological phase diagrams in terms of the topological invariants and show that
the dissipation can modify the topological phase diagram substantially and even
induce topological phases in the system. Our work extends the understanding of
a driven-dissipative topological superconductor.

</details>


### [482] [Inertial antiferromagnetic resonance driven by spin-orbit torques](https://arxiv.org/abs/2507.10323)
*Peng-Bin He,Ri-Xing Wang,Zai-Dong Li,Mikhail Cherkasskii*

Main category: cond-mat.mes-hall

TL;DR: The handedness of resonant modes in antiferromagnets can be controlled by alternating spin-orbit torques, allowing for continuous evolution of polarization states and a critical point for handedness reversal.


<details>
  <summary>Details</summary>
Motivation: To investigate if the handedness of a resonant mode, widely accepted as an intrinsic property, can be actively controlled.

Method: Tailoring the polarization and handedness of alternating spin-orbit torques to control the polarization state and handedness of inertial resonant modes in an antiferromagnet.

Result: Antiferromagnetic inertial modes can continuously evolve from elliptic through circular to linear polarization as the driving polarization is varied, and an inertia-dependent critical degree of driving polarization exists at which the mode becomes linearly polarized while its handedness reverses.

Conclusion: `antiferromagnets` resonant modes can be actively controlled by tailoring the polarization and handedness of alternating spin-orbit torques, with the handedness reversal occurring at a critical degree of driving polarization.

Abstract: It is widely accepted that the handedness of a resonant mode is an intrinsic
property. We show that, by tailoring the polarization and handedness of
alternating spin-orbit torques used as the driving force, the polarization
state and handedness of inertial resonant modes in an antiferromagnet can be
actively controlled. In contrast to ferromagnets, whose resonant-mode
polarization is essentially fixed, antiferromagnetic inertial modes can
continuously evolve from elliptic through circular to linear polarization as
the driving polarization is varied. We further identify an inertia-dependent
critical degree of driving polarization at which the mode becomes linearly
polarized while its handedness reverses.

</details>


### [483] [Dynamics of fractional quantum Hall Liquids with a pulse at the edge](https://arxiv.org/abs/2507.10366)
*Jie Li,Chen-Xin Jiang,Zi-Xiang Hu*

Main category: cond-mat.mes-hall

TL;DR: 通过模拟泵浦-探测过程，研究了分数量子霍尔系统边缘动力学，发现激发会进入体相，磁转子是主要激发模式。


<details>
  <summary>Details</summary>
Motivation: 近期扫描光学频闪共聚焦显微镜和光谱测量实验进展激发了对分数量子霍尔系统边缘和体相动力学的研究兴趣。

Method: 通过泵浦-探测过程和边缘的尖端势模拟了能量空间时间分辨率，研究了其对波函数演化和能量谱分布的影响。

Result: 边缘脉冲淬灭动力学导致激发沿边缘传播并进入体相，磁转子激发是体相激发的优势模式，并分析了探针的位置、强度和持续时间的影响。

Conclusion: 模型与实验结果吻合，揭示了泵浦-探测过程中边缘脉冲淬灭动力学引发的激发沿边缘传播并进入体相，其中磁转子激发是体相激发的优势模式，并分析了探针位置、强度和持续时间的影响。

Abstract: Motivated by recent experimental advancements in scanning optical
stroboscopic confocal microscopy and spectroscopy measurements, which have
facilitated exceptional energy-space-time resolution for investigating edge and
bulk dynamics in fractional quantum Hall systems, we formulated a model for the
pump-probe process on the edge. Starting with a ground state, we applied a tip
potential near the fractional quantum Hall liquid edge, which was subsequently
turned off after a defined time duration. By examining how the specific nature
of the tip potential influences the evolution of the wave function and its
distribution in energy spectrum, we identify that quench dynamics of the edge
pulse leads to excitations that spread both along the edge and perpendicularly
into the bulk. Moreover, magnetoroton excitations are predominant among the
bulk excitations. These results align well with the experimental observations.
Furthermore, we analyzed the effects of the tip's position, intensity, and
duration on the dynamics.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [484] [Computability of Equivariant Gröbner bases](https://arxiv.org/abs/2507.08990)
*Arka Ghosh,Aliaume Lopez*

Main category: cs.LO

TL;DR: This paper explores the decidability of equivariant ideal membership in polynomial rings under group actions. It establishes decidability under the Hilbert's basis property and presents conditions for undecidability in cases where this property doesn't hold.


<details>
  <summary>Details</summary>
Motivation: The motivation is to determine the decidability and computability of the equivariant ideal membership problem, especially for cases where the Hilbert's basis property is not satisfied.

Method: The paper likely involves theoretical computer science and abstract algebra concepts, possibly using Gröbner bases and group actions on polynomial rings to analyze equivariant ideals. The decidability and computability are proven, and a condition for undecidability is given.

Result: Equivariant ideal membership is decidable when the Hilbert's basis property is satisfied. A sufficient condition for undecidability is also provided.

Conclusion: The paper shows that Gröbner bases for equivariant ideals are computable and the equivariant ideal membership is decidable when G and X satisfy the Hilbert's basis property. It also provides a sufficient condition for the undecidability of the equivariant ideal membership problem, which is satisfied by common examples not satisfying the Hilbert's basis property.

Abstract: Let $\mathbb{K}$ be a field, $\mathcal{X}$ be an infinite set (of
indeterminates), and $\mathcal{G}$ be a group acting on $\mathcal{X}$. An ideal
in the polynomial ring $\mathbb{K}[\mathcal{X}]$ is called equivariant if it is
invariant under the action of $\mathcal{G}$. We show Gr\"obner bases for
equivariant ideals are computable are hence the equivariant ideal membership is
decidable when $\mathcal{G}$ and $\mathcal{X}$ satisfies the Hilbert's basis
property, that is, when every equivariant ideal in $\mathbb{K}[\mathcal{X}]$ is
finitely generated. Moreover, we give a sufficient condition for the
undecidability of the equivariant ideal membership problem. This condition is
satisfied by the most common examples not satisfying the Hilbert's basis
property.

</details>


### [485] [A Simple and Effective ASP-Based Tool for Enumerating Minimal Hitting Sets](https://arxiv.org/abs/2507.09194)
*Mohimenul Kabir,Kuldeep S Meel*

Main category: cs.LO

TL;DR: 该研究提出了一种名为MinHit-ASP的工具，利用ASP技术有效枚举所有最小集合，并在实践中得到验证。


<details>
  <summary>Details</summary>
Motivation: 最小集合枚举在实际应用中至关重要，而该研究旨在解决给定集合族的所有最小集合的完整枚举问题。

Method: 使用答案集编程（ASP）来解决最小集合枚举问题，并利用现有的ASP求解器进行高效枚举。

Result: 通过基于ASP的工具MinHit-ASP，能够有效地枚举最小集合。

Conclusion: 该研究提出了一种基于ASP的工具MinHit-ASP，能够有效地枚举集合族的所有最小集合。经验性评估表明，该工具在来自不同问题域的基准测试中表现出色。

Abstract: The hitting set problem is a fundamental problem in computer science and
mathematics. Given a family of sets over a universe of elements, a minimal
hitting set is a subset-minimal collection of elements that intersects each set
in the family. Enumerating all minimal hitting sets is crucial in various
real-world applications.
  In this paper, we address the full enumeration of all minimal hitting sets
for a given family of sets. We formulate the problem using Answer Set
Programming (ASP) and leverage existing ASP solvers for efficient enumeration.
We propose an ASP-based tool, MinHit-ASP, and our empirical evaluation shows
that it effectively enumerates minimal hitting sets across benchmarks from
diverse problem domains.

</details>


### [486] [Recovering Commutation of Logically Constrained Rewriting and Equivalence Transformations (Full Version)](https://arxiv.org/abs/2507.09326)
*Kanta Takahata,Jonas Schöpf,Naoki Nishida,Takahito Aoto*

Main category: cs.LO

TL;DR: 逻辑约束项重写（LCTRSs）中的约束项重写和等价变换难以处理。本研究提出了一个在存在约束项上运行的最一般的约束重写概念，并定义了一个可模拟所有左线性LCTRSs的LCTRSs类，其最一般的约束重写与等价交换。这使得等价变换可以在重写规则之后推迟，从而减小了搜索空间。此外，原始重写形式可以嵌入到新的重写形式中，有望实现更高效的LCTRSs工具。


<details>
  <summary>Details</summary>
Motivation: 解决在分析逻辑约束项重写系统（LCTRSs）时，重写约束项存在的问题，即重写规则的应用和等价变换紧密交织，导致难以建立有用的理论性质并导致实现中的问题（需要过大的搜索空间）。

Method: 提出了一种新的最一般的约束重写概念，该概念在存在约束项上运行，并定义了一个左线性、左值无关的LCTRSs类，该类可以模拟所有左线性LCTRSs并具有关键属性：最一般的约束重写与等价交换。

Result: 提出了一种新的最一般的约束重写概念，该概念在存在约束项上运行，并定义了一个左线性、左值无关的LCTRSs类，该类可以模拟所有左线性LCTRSs并具有关键属性：最一般的约束重写与等价交换。原始重写形式可以嵌入到新的重写形式中。

Conclusion: 该研究提出了一种新的最一般的约束重写概念，该概念在存在约束项上运行，并定义了一个左线性、左值无关的LCTRSs类，该类可以模拟所有左线性LCTRSs并具有所需的关键属性：最一般的约束重写与等价交换。这确保了等价变换可以在重写规则应用之后推迟，有助于减轻实现中过大搜索空间的问题。此外，原始约束项上的重写形式可以嵌入到存在约束项上的新重写形式中。因此，这些结果有望对实现正确且高效的LCTRSs工具产生重大影响。

Abstract: Logically constrained term rewriting is a relatively new rewriting formalism
that naturally supports built-in data structures, such as integers and bit
vectors. In the analysis of logically constrained term rewrite systems
(LCTRSs), rewriting constrained terms plays a crucial role. However, this
combines rewrite rule applications and equivalence transformations in a closely
intertwined way. This intertwining makes it difficult to establish useful
theoretical properties for this kind of rewriting and causes problems in
implementations -- namely, that impractically large search spaces are often
required. To address this issue, we propose in this paper a novel notion of
most general constrained rewriting, which operates on existentially constrained
terms, a concept recently introduced by the authors. We define a class of
left-linear, left-value-free LCTRSs that are general enough to simulate all
left-linear LCTRSs and exhibit the desired key property: most general
constrained rewriting commutes with equivalence. This property ensures that
equivalence transformations can be deferred until after the application of
rewrite rules, which helps mitigate the issue of large search spaces in
implementations. In addition to that, we show that the original rewriting
formalism on constrained terms can be embedded into our new rewriting formalism
on existentially constrained terms. Thus, our results are expected to have
significant implications for achieving correct and efficient implementations in
tools operating on LCTRSs.

</details>


### [487] [Non-Termination of Logic Programs Using Patterns](https://arxiv.org/abs/2507.09390)
*Etienne Payet*

Main category: cs.LO

TL;DR: 将项重写中的非循环非终止检测方法应用于逻辑编程，并开发了一个名为NTI的工具来进行实验评估。


<details>
  <summary>Details</summary>
Motivation: 在项重写中用于自动检测非循环非终止的模式的启发式方法

Method: 通过定义新的展开技术来描述可能有限重写序列的无限集合

Result: 在我们的工具NTI中实现并进行了实验评估

Conclusion: 本文将项重写中引入的用于从规则模式自动检测非循环非终止的方法应用于逻辑编程

Abstract: In this paper, we consider an approach introduced in term rewriting for the
automatic detection of non-looping non-termination from patterns of rules. We
adapt it to logic programming by defining a new unfolding technique that
produces patterns describing possibly infinite sets of finite rewrite
sequences. We present an experimental evaluation of our contributions that we
implemented in our tool NTI.

</details>


### [488] [Justification Logic for Intuitionistic Modal Logic (Extended Technical Report)](https://arxiv.org/abs/2507.09427)
*Sonia Marin,Paaras Padhiar*

Main category: cs.LO

TL;DR: 本研究为直觉主义模态逻辑 IK 提供了一个证明论推理框架，通过扩展证明项句法、提供公理化体系和实现句法实现过程来完成。


<details>
  <summary>Details</summary>
Motivation: 继 Kuznets、Marin 和 Stra{\ss}burger 的工作之后，我们旨在为 Fischer Servi 的直觉主义模态逻辑 IK 及其扩展提供一个证明论推理框架，其中模态算子被显式的证明项所取代。

Method: 通过引入新的证明项（满足项）来扩展证明项的句法，以适应直觉主义模态逻辑的附加公理。我们还提供了一个公理化体系，并使用 Stra{\ss}burger 引入的用于直觉主义模态逻辑的无 कट 的嵌套序列系统来实现句法实现过程。

Result: 我们成功地为直觉主义模态逻辑 IK 及其扩展提供了证明论推理框架，扩展了证明项的句法，给出了公理化体系，并提出了句法实现过程。

Conclusion: 我们为 Fischer Servi 的直觉主义模态逻辑 IK 及其带有 t 和 4 公理的扩展提供了对应的证明论推理框架。

Abstract: Justification logics are an explication of modal logic; boxes are replaced
with proof terms formally through realisation theorems. This can be achieved
syntactically using a cut-free proof system e.g. using sequent, hypersequent or
nested sequent calculi. In constructive modal logic, boxes and diamonds are
decoupled and not De Morgan dual. Kuznets, Marin and Stra{\ss}burger provide a
justification counterpart to constructive modal logic CK and some extensions by
making diamonds explicit by introducing new terms called satisfiers. We
continue the line of work to provide a justification counterpart to Fischer
Servi's intuitionistic modal logic IK and its extensions with the t and 4
axioms. We: extend the syntax of proof terms to accommodate the additional
axioms of intuitionistic modal logic; provide an axiomatisation of these
justification logics; provide a syntactic realisation procedure using a
cut-free nested sequent system for intuitionistic modal logic introduced by
Stra{\ss}burger.

</details>


### [489] [A Study Of Sudoku Solving Algorithms: Backtracking and Heuristic](https://arxiv.org/abs/2507.09708)
*Apekshya Bhattarai,Dinisha Uprety,Pooja Pathak,Safal Narshing Shrestha,Salina Narkarmi,Sanjog Sigdel*

Main category: cs.LO

TL;DR: Heuristic Sudoku solver is faster than backtracking, especially for hard puzzles.


<details>
  <summary>Details</summary>
Motivation: To compare the performance of Sudoku-solving strategies, specifically recursive backtracking and a heuristic-based constraint propagation method.

Method: Comparative analysis of recursive backtracking and heuristic-based constraint propagation using a dataset of 500 puzzles across five difficulty levels.

Result: The heuristic approach achieved speedup ratios from 1.27x (Beginner) to 2.91x (Expert) compared to backtracking.

Conclusion: The heuristic approach consistently outperformed recursive backtracking, especially for more difficult Sudoku puzzles.

Abstract: This paper presents a comparative analysis of Sudoku-solving strategies,
focusing on recursive backtracking and a heuristic-based constraint propagation
method. Using a dataset of 500 puzzles across five difficulty levels (Beginner
to Expert), we evaluated performance based on average solving time. The
heuristic approach consistently outperformed backtracking, achieving speedup
ratios ranging from 1.27x in Beginner puzzles to 2.91x in Expert puzzles. These
findings underscore the effectiveness of heuristic strategies, particularly in
tackling complex puzzles across varying difficulty levels.

</details>


### [490] [Extending Defeasibility for Propositional Standpoint Logics](https://arxiv.org/abs/2507.10133)
*Nicholas Leisegang,Thomas Meyer,Ivan Varzinczak*

Main category: cs.LO

TL;DR: 本研究通过整合多种可学性方法，扩展了立场逻辑，使其能够处理蕴含、模态算子和锐化陈述的可学性。我们提出了一个优先语义和画法演算，并证明了其完备性和PSpace内的计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 为了在立场逻辑中整合可学性，允许在逻辑的各个层面（包括蕴含、模态算子和锐化陈述）上表达可学性。

Method: 通过整合Kraus等人可学条件、Britz和Varzinczak的可学必然性和不同可能性概念以及Leisegang等人的可学方法到Gómez Álvarez和Rudolph的立场逻辑中，我们提出了一种新的可学命题立场逻辑。我们为这个扩展语言提供了优先语义，并提出了一种画法演算，该演算被证明对于优先蕴含是健全和完全的。

Result: 我们为扩展语言提供了优先语义，并提出了一种画法演算，该演算被证明对于优先蕴含是健全和完全的。我们还确定了画法程序的计算复杂度在PSpace内。

Conclusion: 该逻辑框架为蕴含、立场模态算子和立场锐化陈述的层次提供了新的可学性表达方式。

Abstract: In this paper, we introduce a new defeasible version of propositional
standpoint logic by integrating Kraus et al.'s defeasible conditionals, Britz
and Varzinczak's notions of defeasible necessity and distinct possibility,
along with Leisegang et al.'s approach to defeasibility into the standpoint
logics of G\'omez \'Alvarez and Rudolph. The resulting logical framework allows
for the expression of defeasibility on the level of implications, standpoint
modal operators, and standpoint-sharpening statements. We provide a
preferential semantics for this extended language and propose a tableaux
calculus, which is shown to be sound and complete with respect to preferential
entailment. We also establish the computational complexity of the tableaux
procedure to be in PSpace.

</details>


### [491] [A simple formalization of alpha-equivalence](https://arxiv.org/abs/2507.10181)
*Kalmer Apinis,Danel Ahman*

Main category: cs.LO

TL;DR: 该论文提出了一种新的、基于归纳法的 α-等价定义，并用 Rocq Prover 进行了验证，解决了之前未直接进行归纳法定义的问题。


<details>
  <summary>Details</summary>
Motivation: 为了探究在向本科生讲授非类型 lambda 演算（untyped $\lambda$-calculus）时，为何 α-等价没有直接被定义为归纳法。

Method: 该论文提供了一个 α-等价的 grounded, inductive definition，并且该定义已通过 Rocq Prover 的形式化验证。

Result: 该论文成功地提供了一个 α-等价的 grounded, inductive definition，并证明了其符合现有的文献规范。

Conclusion: 该论文提供了一个可行的、基于归纳法（grounded, inductive definition）的 α-等价（α-equivalence）的定义，并证明了该定义符合文献中的规范。

Abstract: While teaching untyped $\lambda$-calculus to undergraduate students, we were
wondering why $\alpha$-equivalence is not directly inductively defined. In this
paper, we demonstrate that this is indeed feasible. Specifically, we provide a
grounded, inductive definition for $\alpha$-equivalence and show that it
conforms to the specification provided in the literature. The work presented in
this paper is fully formalized in the Rocq Prover.

</details>


### [492] [A Quantum Programming Language for Coherent Control](https://arxiv.org/abs/2507.10466)
*Kathlee Barsse,Romain Péchoux,Simon Perdrix*

Main category: cs.LO

TL;DR: 我们引入了一种编程语言，可以对任意量子操作进行相干控制。


<details>
  <summary>Details</summary>
Motivation: 例如，在递归或迭代存在的情况下，使用量子条件在酉算例之外定义相干控制的问题，长期以来一直被认为是主要困难。

Method: 我们通过基于适当的Kraus分解的操作语义和基于真空扩展的表示语义来解决这个问题。

Result: 我们证明了该语言在真空扩展方面具有通用性，并且两种语义是充分的。此外，我们定义了一种可观察的等价概念：如果两个程序在任何上下文中具有相同的终止概率，则它们是等价的。该表示语义对于可观察的等价性被证明是完全抽象的。

Conclusion: 该语言在真空扩展方面具有通用性，并且两种语义是充分的。此外，我们定义了一种可观察的等价概念：如果两个程序在任何上下文中具有相同的终止概率，则它们是等价的。该表示语义对于可观察的等价性被证明是完全抽象的。

Abstract: We introduce a programming language that allows for the coherent control of
arbitrary quantum operations. The problem of defining coherent control beyond
the unitary case, using, for example, a quantum conditional in the presence of
recursion or iteration has long been known to be a major difficulty. We resolve
this problem by defining an operational semantics based on appropriate Kraus
decompositions and a denotational semantics based on vacuum-extensions. We show
that the language is universal for vacuum-extensions and that the two semantics
are adequate. Moreover, we define a notion of observational equivalence: two
programs are equivalent if their probability of termination is the same in any
context. The denotational semantics is shown to be fully abstract for
observational equivalence.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [493] [Interactive Drawing Guidance for Anime Illustrations with Diffusion Model](https://arxiv.org/abs/2507.09140)
*Chuang Chen,Xiaoxuan Xie,Yongming Zhang,Tianyu Zhang,Haoran Xie*

Main category: cs.GR

TL;DR: 一个旨在帮助用户更高效、准确地创作动漫插画的交互式绘图指导系统，提供实时引导和结构化草图。


<details>
  <summary>Details</summary>
Motivation: 为了解决初学者在创作高质量动漫插画时面临的挑战，特别是由于动漫艺术复杂的风格和精细的细节造成的困难，我们提出了一个交互式绘图指导系统。

Method: 本系统利用StreamDiffusion流水线提供实时绘画辅助，通过LoRA微调Stable Diffusion根据用户手绘草图和提示词生成动漫风格的RGB图像，再运用Informative Drawings模型将RGB图像转换为粗略的草图，并通过自定义优化器将这些草图细化为结构化的引导草图。

Result: 本系统能够根据用户的创意意图提供精确、实时的指导，显著提高了绘画过程的效率和准确性。用户研究的实证反馈证实了该方法的有效性。

Conclusion: 该系统通过结合StreamDiffusion、LoRA微调的Stable Diffusion以及Informative Drawings模型，并利用自定义优化器生成引导草图，为用户提供精确、实时的动漫插画绘制指导，有效提升了绘画效率和准确性。用户研究结果表明了该系统的有效性以及界面的可用性。

Abstract: Creating high-quality anime illustrations presents notable challenges,
particularly for beginners, due to the intricate styles and fine details
inherent in anime art. We present an interactive drawing guidance system
specifically designed for anime illustrations to address this issue. It offers
real-time guidance to help users refine their work and streamline the creative
process. Our system is built upon the StreamDiffusion pipeline to deliver
real-time drawing assistance. We fine-tune Stable Diffusion with LoRA to
synthesize anime style RGB images from user-provided hand-drawn sketches and
prompts. Leveraging the Informative Drawings model, we transform these RGB
images into rough sketches, which are further refined into structured guidance
sketches using a custom-designed optimizer. The proposed system offers precise,
real-time guidance aligned with the creative intent of the user, significantly
enhancing both the efficiency and accuracy of the drawing process. To assess
the effectiveness of our approach, we conducted a user study, gathering
empirical feedback on both system performance and interface usability.

</details>


### [494] [Physics-Aware Fluid Field Generation from User Sketches Using Helmholtz-Hodge Decomposition](https://arxiv.org/abs/2507.09146)
*Ryuichi Miyauchi,Hengyuan Chang,Tsukasa Fukusato,Kazunori Miyata,Haoran Xie*

Main category: cs.GR

TL;DR: 一种结合了潜在扩散模型和亥姆霍奇分解的方法，可以根据用户草图交互式地设计具有物理属性（如不可压缩性）的2D矢量场。


<details>
  <summary>Details</summary>
Motivation: 现有的矢量场生成方法虽然能根据用户草图直观地生成矢量场，但在保持不可压缩性等物理属性方面存在困难。因此，需要一种能够兼顾用户意图和物理属性的矢量场设计方法。

Method: 本文提出了一种两阶段方法：第一阶段使用潜在扩散模型（LDM）从用户草图自动生成初始2D矢量场；第二阶段应用亥姆霍奇分解提取和重组物理属性（如不可压缩性），以符合用户意图。

Result: 实验证明了该方法的有效性，能够生成满足用户意图且具有物理属性（如不可压缩性）的2D矢量场。

Conclusion: 该方法通过结合潜在扩散模型和亥姆霍奇分解，实现了从用户草图到物理属性可控的2D矢量场的交互式设计。

Abstract: Fluid simulation techniques are widely used in various fields such as film
production, but controlling complex fluid behaviors remains challenging. While
recent generative models enable intuitive generation of vector fields from user
sketches, they struggle to maintain physical properties such as
incompressibility. To address these issues, this paper proposes a method for
interactively designing 2D vector fields. Conventional generative models can
intuitively generate vector fields from user sketches, but remain difficult to
consider physical properties. Therefore, we add a simple editing process after
generating the vector field. In the first stage, we use a latent diffusion
model~(LDM) to automatically generate initial 2D vector fields from user
sketches. In the second stage, we apply the Helmholtz-Hodge decomposition to
locally extract physical properties such as incompressibility from the results
generated by LDM and recompose them according to user intentions. Through
multiple experiments, we demonstrate the effectiveness of our proposed method.

</details>


### [495] [RectifiedHR: High-Resolution Diffusion via Energy Profiling and Adaptive Guidance Scheduling](https://arxiv.org/abs/2507.09441)
*Ankit Sanjyal*

Main category: cs.GR

TL;DR: 通过能量画像和自适应CFG策略，提升了扩散模型图像合成的稳定性和图像质量。


<details>
  <summary>Details</summary>
Motivation: 高分辨率图像合成中的能量不稳定和指导伪影问题导致视觉质量下降。

Method: 本研究通过分析采样过程中的潜能量和提出自适应CFG调度策略来解决高分辨率图像合成中的能量不稳定和指导伪影问题，其中能量感知调度策略通过调节指导强度来维持能量轨迹的稳定性。

Result: 自适应CFG调度策略相比固定指导方法，在稳定性和一致性指标上表现更优，具体表现为更高的稳定性分数（0.9998）和一致性指标（0.9873），并能生成更清晰、更保真的图像，同时减少伪影。

Conclusion: 通过能量画像和自适应条件无分类指导（CFG）策略的引入，该研究显著提高了高分辨率图像合成的稳定性和视觉质量，尤其是在DPM++ 2M模型中采用线性递减CFG策略时，能生成更清晰、更保真的图像，并有效减少了伪影。

Abstract: High-resolution image synthesis with diffusion models often suffers from
energy instabilities and guidance artifacts that degrade visual quality. We
analyze the latent energy landscape during sampling and propose adaptive
classifier-free guidance (CFG) schedules that maintain stable energy
trajectories. Our approach introduces energy-aware scheduling strategies that
modulate guidance strength over time, achieving superior stability scores
(0.9998) and consistency metrics (0.9873) compared to fixed-guidance
approaches. We demonstrate that DPM++ 2M with linear-decreasing CFG scheduling
yields optimal performance, providing sharper, more faithful images while
reducing artifacts. Our energy profiling framework serves as a powerful
diagnostic tool for understanding and improving diffusion model behavior.

</details>


### [496] [Real-time and Controllable Reactive Motion Synthesis via Intention Guidance](https://arxiv.org/abs/2507.09704)
*Xiaotang Zhang,Ziyi Chang,Qianhui Men,Hubert Shum*

Main category: cs.GR

TL;DR: 提出了一种实时反应运动合成方法，通过意图预测器处理未来运动的不确定性，并使用隐空间匹配和对抗性训练进行姿态生成，优于现有方法，并允许用户个性化交互路径。


<details>
  <summary>Details</summary>
Motivation: 提出一种用于反应运动合成的实时方法，以解决仅使用历史、用户控制的运动来预测即时反应的问题，并处理未来运动的不确定性。

Method: 提出了一种基于输入角色已知轨迹的实时反应运动合成方法，仅使用历史用户控制的运动来预测即时反应。该方法通过引入意图预测器来处理未来运动的不确定性，预测关键关节意图，使姿态预测从历史交互中更加确定。然后将意图编码到其反应运动的隐空间中，并与表示输入输出映射的码本进行匹配。它为姿态生成对分类分布进行采样，并通过对抗性训练来增强模型鲁棒性。

Result: 该方法能够处理未来运动的不确定性，通过历史交互进行更确定的姿态预测，并生成实时的、长期的、真实的交互合成运动。定量和定性实验表明，本方法在稳定性和泛化性方面优于其他基于匹配的运动合成方法。用户还可以通过控制移动方向来个性化交互路径。

Conclusion: 本方法通过引入意图预测器来处理未来运动的不确定性，通过对历史交互进行姿态预测，并结合隐空间中的意图编码，通过匹配实现真实世界的运动合成。与之前的离线方法不同，该系统可以通过早期步骤的反馈递归地生成意图和反应运动，从而实现实时、长期的真实交互合成。定量和定性实验表明，本方法在稳定性和泛化性方面优于其他基于匹配的运动合成方法。用户还可以通过控制移动方向来积极影响结果，从而创建偏离预定轨迹的个性化交互路径。

Abstract: We propose a real-time method for reactive motion synthesis based on the
known trajectory of input character, predicting instant reactions using only
historical, user-controlled motions. Our method handles the uncertainty of
future movements by introducing an intention predictor, which forecasts key
joint intentions to make pose prediction more deterministic from the historical
interaction. The intention is later encoded into the latent space of its
reactive motion, matched with a codebook which represents mappings between
input and output. It samples a categorical distribution for pose generation and
strengthens model robustness through adversarial training. Unlike previous
offline approaches, the system can recursively generate intentions and reactive
motions using feedback from earlier steps, enabling real-time, long-term
realistic interactive synthesis. Both quantitative and qualitative experiments
show our approach outperforms other matching-based motion synthesis approaches,
delivering superior stability and generalizability. In our method, user can
also actively influence the outcome by controlling the moving directions,
creating a personalized interaction path that deviates from predefined
trajectories.

</details>


### [497] [CADmium: Fine-Tuning Code Language Models for Text-Driven Sequential CAD Design](https://arxiv.org/abs/2507.09792)
*Prashant Govindarajan,Davide Baldelli,Jay Pathak,Quentin Fournier,Sarath Chandar*

Main category: cs.GR

TL;DR: 本研究利用大型语言模型和包含17万多个CAD模型的数据集，实现了从文本到CAD设计的自动化，并引入了新的评估指标来衡量设计质量。


<details>
  <summary>Details</summary>
Motivation: 目前的计算机辅助设计（CAD）建模仍然是一个耗时的手动过程。尽管近期研究试图通过小型Transformer模型和手工制作的CAD序列表示来实现自动化，但利用大型语言模型（LLMs）进行顺序CAD设计的潜力尚未被充分挖掘。

Method: 利用包含170,000多个CAD模型的大型数据集，并通过GPT-4流水线生成高质量、类似人类的描述对其进行注释。在此数据集上，对强大的代码语言模型进行微调，以根据自然语言描述生成基于JSON格式的CAD序列。引入了基于球形度、平均曲率和欧拉特征的几何和拓扑指标来提供更丰富的结构洞察。

Result: 实验和消融研究表明，CADmium能够自动化CAD设计，并显著加快新对象的设计速度。引入的几何和拓扑指标提供了比简单指标更丰富的结构洞察。

Conclusion: 通过使用基于GPT-4注释的大型数据集和强大的代码语言模型，展示了从自然语言描述生成CAD序列的可行性和有效性。引入了几何和拓扑指标来评估生成对象的质量，证明了CADmium能够自动化CAD设计并显著加速新对象的设计。

Abstract: Computer-aided design (CAD) is the digital construction of 2D and 3D objects,
and is central to a wide range of engineering and manufacturing applications
like automobile and aviation. Despite its importance, CAD modeling remains
largely a time-intensive, manual task. Recent works have attempted to automate
this process with small transformer-based models and handcrafted CAD sequence
representations. However, there has been little effort to leverage the
potential of large language models (LLMs) for sequential CAD design. In this
work, we introduce a new large-scale dataset of more than 170k CAD models
annotated with high-quality, human-like descriptions generated with our
pipeline based on GPT-4.1. Using this dataset, we fine-tune powerful code-LLMs
to generate CAD sequences represented in a JSON-based format from natural
language descriptions, demonstrating the viability and effectiveness of this
approach for text-conditioned CAD generation. Because simple metrics often fail
to reflect the quality of generated objects, we introduce geometric and
topological metrics based on sphericity, mean curvature, and Euler
characteristic to provide richer structural insights. Our experiments and
ablation studies on both synthetic and human-annotated data demonstrate that
CADmium is able to automate CAD design, drastically speeding up the design of
new objects. The dataset, code, and fine-tuned models are available online.

</details>


### [498] [ScaffoldAvatar: High-Fidelity Gaussian Avatars with Patch Expressions](https://arxiv.org/abs/2507.10542)
*Shivangi Aneja,Sebastian Weiss,Irene Baeza,Prashanth Chandran,Gaspard Zoss,Matthias Nießner,Derek Bradley*

Main category: cs.GR

TL;DR: 提出了一种名为 ScaffoldAvatar 的新方法，该方法结合了局部面部表情和 3D 高斯泼溅技术，以生成高保真度的 3D 头像。该方法通过在块级别处理面部表情，实现了比以往技术更自然、更丰富的面部动画，并且能够实时运行。


<details>
  <summary>Details</summary>
Motivation: 生成高保真度的实时动画序列，逼真的 3D 头像对于许多图形应用程序很重要，包括沉浸式远程呈现和电影。特别是在渲染数字头像特写镜头以展示角色的面部微特征和表情时，这是一个具有挑战性的问题。为了捕捉人类头部的表现力和细节特征，包括皮肤皱纹和更精细的面部运动，需要这种方法。

Method: 提出将局部定义的面部表情与 3D 高斯泼溅相结合，以创建超高保真度、富有表现力和逼真的 3D 头像。具体来说，利用基于块的几何 3D 面部模型提取块表达式，并学习如何将它们转化为局部动态皮肤外观和运动，方法是将块与 Scaffold-GS 的锚点耦合。然后，利用这些锚点根据块表达式和视角即时合成 3D 高斯。采用基于颜色的致密化和渐进式训练，以获得高质量的结果和更快的收敛速度。

Result: 通过利用块级表达式，ScaffoldAvatar 持续实现最先进的性能，具有视觉上自然的运动，同时实时包含各种面部表情和风格。

Conclusion: ScaffoldAvatar 取得最先进的性能，具有视觉上自然的运动，同时实时包含各种面部表情和风格。

Abstract: Generating high-fidelity real-time animated sequences of photorealistic 3D
head avatars is important for many graphics applications, including immersive
telepresence and movies. This is a challenging problem particularly when
rendering digital avatar close-ups for showing character's facial microfeatures
and expressions. To capture the expressive, detailed nature of human heads,
including skin furrowing and finer-scale facial movements, we propose to couple
locally-defined facial expressions with 3D Gaussian splatting to enable
creating ultra-high fidelity, expressive and photorealistic 3D head avatars. In
contrast to previous works that operate on a global expression space, we
condition our avatar's dynamics on patch-based local expression features and
synthesize 3D Gaussians at a patch level. In particular, we leverage a
patch-based geometric 3D face model to extract patch expressions and learn how
to translate these into local dynamic skin appearance and motion by coupling
the patches with anchor points of Scaffold-GS, a recent hierarchical scene
representation. These anchors are then used to synthesize 3D Gaussians
on-the-fly, conditioned by patch-expressions and viewing direction. We employ
color-based densification and progressive training to obtain high-quality
results and faster convergence for high resolution 3K training images. By
leveraging patch-level expressions, ScaffoldAvatar consistently achieves
state-of-the-art performance with visually natural motion, while encompassing
diverse facial expressions and styles in real time.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [499] [Structural optimization of lattice-matched Sc0.14Al0.86N/GaN superlattices for photonic applications](https://arxiv.org/abs/2507.08951)
*Rajendra Kumar,Govardan Gopakumar,Zain Ul Abdin,Michael J. Manfra,Oana Malis*

Main category: cond-mat.mtrl-sci

TL;DR: 本文研究了 ScAl1-xN/GaN 超晶格的分子束外延生长，确定了实现晶格匹配的最佳 Sc 组分为 x = 0.14，并优化了生长温度和条件，为量子光子器件的应用提供了基础。


<details>
  <summary>Details</summary>
Motivation: ScAl1-xN 作为一种新兴的 III 族氮化物材料，因其高压电系数和铁电特性，在量子光子器件领域具有重要的应用潜力。然而，实现低缺陷 ScAl1-xN 的集成需要精确的晶格匹配和优化的生长参数。

Method: 通过分子束外延技术，系统地研究了不同 Sc 组分和不同 GaN 间隔层厚度的 ScAl1-xN/GaN 超晶格的生长，并利用 X 射线衍射倒易空间映射、扫描透射电子显微镜和能量色散 X 射线光谱等技术对其结构和成分进行了表征。

Result: 研究确定了 Sc 组分为 x = 0.14 时可以实现与 GaN 的晶格匹配，并且在高达 600 nm 的总厚度下仍能保持良好。然而，当 Sc 组分偏离此值时，会出现应变诱导的缺陷。研究还发现，温度依赖的混层是影响氮化物成分变化和带结构的重要因素，并且 Sc 的掺入相对于 Al 存在延迟。最终确定了适用于不同厚度 GaN 层的最佳生长温度。

Conclusion: 通过系统研究和优化生长参数，实现了高质量的 ScAl1-xN/GaN 超晶格的分子束外延生长，并确定了最佳生长条件，为量子光子器件的应用奠定了基础。

Abstract: ScxAl1-xN is an emerging III-nitride material known for its high
piezoelectric coefficient and ferroelectric properties. Integration of
wide-bandgap ScxAl1-xN with GaN is particularly attractive for quantum photonic
devices. Achieving low defect complex multilayers incorporating ScxAl1-xN,
though, requires precise lattice-matching and carefully optimized growth
parameters. This study systematically investigates the molecular-beam epitaxy
of short-period ScxAl1-xN/GaN superlattices with total thicknesses of up to 600
nm on GaN templates. X-ray diffraction reciprocal space mapping confirmed
lattice-matching at x = 0.14 Sc composition regardless of the thickness of GaN
interlayers, as evidenced by symmetric superlattice satellites aligned in-plane
with the underlying substrate peak. Superlattices with Sc compositions
deviating from this lattice-matching condition exhibited strain-induced defects
ranging from crack formation to partial relaxation. Scanning transmission
electron microscopy (STEM) investigation of the ScxAl1-xN/GaN interfaces
identified temperature-dependent intermixing as a major factor in setting the
nitride composition variation and implicitly band structure profile along the
growth direction. Energy-dispersive X-ray spectroscopy also revealed that Sc
incorporation exhibits delays relative to Al at both onset and termination.
Optimal growth conditions were observed at approximately 600{\deg}C and
550{\deg}C for superlattices with thick GaN layers (6 nm), and ultra-thin GaN
layers (< 2 nm), respectively.

</details>


### [500] [Surprisingly High Redundancy in Electronic Structure Data](https://arxiv.org/abs/2507.09001)
*Sazzad Hossain,Ponkrshnan Thiagarajan,Shashank Pathrudkar,Stephanie Taylor,Abhijeet S. Gangan,Amartya S. Banerjee,Susanta Ghosh*

Main category: cond-mat.mtrl-sci

TL;DR: 电子结构机器学习数据集存在高度冗余，可大幅削减数据量而损失最小的预测精度。


<details>
  <summary>Details</summary>
Motivation: 为了应对电子结构机器学习模型需要昂贵的第一性原理模拟生成的大型数据集的挑战，本研究旨在探索数据集中的冗余性并开发更有效的数据集构建方法。

Method: 提出了一种基于覆盖率的剪枝策略，并与随机剪枝和基于重要性的剪枝方法进行了比较。

Result: 发现电子结构数据集（包括分子、简单金属和复杂合金）存在高度冗余。基于覆盖率的剪枝策略能以少100倍的数据量和减少三倍或更多的训练时间来保持化学精度和模型泛化能力，而基于重要性的剪枝方法在更高剪枝因子下可能失败。

Conclusion: 该研究揭示了电子结构数据集的高度冗余性，挑战了对大型数据集的依赖，并提出了一种基于覆盖率的剪枝策略，能在显著减少数据量和训练时间的同時，保持化学精度和模型泛化能力。

Abstract: Machine Learning (ML) models for electronic structure rely on large datasets
generated through expensive Kohn-Sham Density Functional Theory simulations.
This study reveals a surprisingly high level of redundancy in such datasets
across various material systems, including molecules, simple metals, and
complex alloys. Our findings challenge the prevailing assumption that large,
exhaustive datasets are necessary for accurate ML predictions of electronic
structure. We demonstrate that even random pruning can substantially reduce
dataset size with minimal loss in predictive accuracy, while a state-of-the-art
coverage-based pruning strategy retains chemical accuracy and model
generalizability using up to 100-fold less data and reducing training time by
threefold or more. By contrast, widely used importance-based pruning methods,
which eliminate seemingly redundant data, can catastrophically fail at higher
pruning factors, possibly due to the significant reduction in data coverage.
This heretofore unexplored high degree of redundancy in electronic structure
data holds the potential to identify a minimal, essential dataset
representative of each material class.

</details>


### [501] [Molecular Arrangements in the First Monolayer of Cu-Phthalocyanine on In$_2$O$_3$(111)](https://arxiv.org/abs/2507.09044)
*Matthias Blatnik,Fabio Calcinelli,Andreas Jeindl,Moritz Eder,Michael Schmid,Jan Čechal,Ulrike Diebold,Peter Jacobson Oliver T. Hofmann,Margareta Wagner*

Main category: cond-mat.mtrl-sci

TL;DR: CuPc在In$_2$O$_3$表面形成不同覆盖度的有序结构，最终可通过选择合适的金属原子实现均匀的第一层吸附。


<details>
  <summary>Details</summary>
Motivation: 为了实现有机电子器件中的有机分子层在氧化物表面的有序排列，需要理解分子在氧化物表面的吸附行为。

Method: 结合扫描隧道显微镜（STM）和非接触原子力显微镜（nc-AFM）技术，并辅以密度泛函理论（DFT）计算，探究了酞菁铜（CuPc）在氧化铟（In$_2$O$_3$）模型表面的吸附结构和分子形变。

Result: 研究发现在化学计量的In$_2$O$_3$(111)表面，孤立的CuPc分子以扁平、轻微倾斜的几何构型吸附在三个对称等价的位点上。随着覆盖度的增加，会形成沿$\langle1\bar{1}0\rangle$方向排列的致密一维链，当CuPc覆盖度增加到每表面单胞3/4时，会形成高度有序的（2×2）超结构。当覆盖度为每表面单胞一个CuPc时，会形成完全覆盖表面的致密（1×1）超结构，分子虽然保持原有位点和取向，但部分重叠导致分子弯曲。这些结果与钴酞菁（CoPc）在In$_2$O$_3$(111)表面的行为进行了比较。

Conclusion: 研究表明，通过选择合适的金属原子，可以在In$_2$O$_3$(111)表面实现金属酞菁分子的均匀第一层吸附。

Abstract: Well-ordered organic molecular layers on oxide surfaces are key for organic
electronics. Using a combination of scanning tunneling microscopy (STM) and
non-contact atomic force microscopy (nc-AFM) we probe the structures of copper
phthalocyanine (CuPc) on In$_2$O$_3$, a model for a prototypical transparent
conductive oxide (TCO). These scanning-probe images allow the direct
determination of the adsorption site and distortions of the molecules, which
are corroborated by DFT calculations. Isolated CuPc molecules adsorb in a flat,
slightly tilted geometry in three symmetry-equivalent configurations on the
stoichiometric In$_2$O$_3$(111) surface. Increasing the coverage leads to
densely-packed 1D chains oriented along $\langle1\bar{1}0\rangle$ directions,
which dissolve into a highly ordered (2$\times$2) superstructure upon
increasing the CuPc density to 3/4 per surface unit cell. At a coverage of one
CuPc per surface unit cell, a densely packed (1$\times$1) superstructure fully
covers the surface. The molecules still assume the same site and orientation as
before, but they partially overlap to accommodate the high packing density,
leading to a bending of the molecules. These results are compared to the
behavior of CoPc on In$_2$O$_3$(111). In summary, we demonstrate that a uniform
first layer of metal-phthalocyanine molecules can be realized on the
In$_2$O$_3$(111) surface when using the proper metal atom in the molecule.

</details>


### [502] [Spin current generation driven by skyrmion dynamics under magnetic anisotropy and polarized microwaves](https://arxiv.org/abs/2507.09126)
*Seno Aji,Muhammad Anin Nabail Azhiim,Nur Ika Puji Ayu,Adam Badra Cahaya,Koichi Kusakabe,Muhammad Aziz Majidi*

Main category: cond-mat.mtrl-sci

TL;DR: 研究了斯格明子基质材料的自旋流产生，发现磁各向异性影响自旋激发，并与圆偏振微波共同作用产生自旋流。


<details>
  <summary>Details</summary>
Motivation: 研究缺乏反转对称性的材料中斯格明子驱动的自旋流。

Method: 通过微磁模拟研究了缺乏反演对称性的斯格明子基质材料通过微波共振过程泵浦的自旋流。

Result: 结果揭示了两种不同的斯格明子相（SkX I 型和 II 型），它们具有不同的自旋激发特性。SkX I 型在低磁各向异性下表现出呼吸模式频率介于 Bloch 型斯格明子的顺时针和逆时针回转模式之间，而在 $K_z \sim 0.04$ meV 时与逆时针模式交叉。SkX II 型则表现出独特的自旋激发，顺时针模式缺失，而逆时针模式在低频和高频下都存在。这两种类型的斯格明子在圆偏振微波下会产生具有奇异特征的自旋流，例如在左旋圆偏振微波下增强，而在右旋圆偏振微波下淬灭，且与 DMI 符号无关。

Conclusion: 磁各向异性在自旋流产生中起着关键作用，并且对圆偏振微波的响应不同，这表明了磁各向异性与圆偏振微波之间存在非平庸的相互作用。

Abstract: We have investigated the spin-current pumped by the skyrmion-host material
with the lack of inversion symmetry through the microwave resonance process.
The effects of magnetic anisotropy and polarized microwaves are examined by
micromagnetic simulations. Our results reveal two distinct skyrmion phases,
designated as SkX type-I and II, which emerge at low ($K_z<0.1$ meV) and high
($K_z>0.1$ meV) magnetic anisotropy constants, respectively, having different
characteristics of spin excitations. The SkX type-I exhibits spin dynamics
where the resonant frequency of the breathing mode is lying in between the
clockwise and counterclockwise gyration modes of Bloch-type skyrmion at a very
low anisotropy, and is crossing over the counterclockwise mode at $K_z \sim
0.04$ meV. Meanwhile, the SkX type-II exhibits distinct spin excitations in
which the clockwise mode is notably absent, while the counterclockwise modes
exist at both low and high resonant frequencies. This suggests that the
magnetic anisotropy plays an essential role in the spin dynamics. Furthermore,
the resulting spin excitations induce spin currents with exotic features under
the polarized microwaves. The spin currents induced, for instance, by low-lying
in-plane excitations are strongly enhanced under the left-handed circularly
polarized microwaves, but quenched by the right-handed circularly polarized
microwaves regardless of the sign of the Dzyaloshinskii-Moriya interaction.
These results may pave the way for understanding the non-trivial interplay
between magnetic anisotropy and polarized microwaves in the generation of spin
currents by a resonant process.

</details>


### [503] [Large non-saturating Nernst thermopower and magnetoresistance in compensated semimetal ScSb](https://arxiv.org/abs/2507.09056)
*Antu Laha,Sarah Paone,Niraj Aryal,Qiang Li*

Main category: cond-mat.mtrl-sci

TL;DR: 在低温下，多晶态SbSc在Nernst热电势和磁电阻方面表现出色，性能优于单晶，这得益于其良好的电子-空穴补偿和立方对称性。


<details>
  <summary>Details</summary>
Motivation: 寻找在低温（尤其是液氮沸点以下）下工作的高性能热电和热磁材料，并研究多晶材料的性能表现。

Method: 通过实验测量了多晶态SbSc在低温（30 K和2 K）和强磁场（14 T）下的Nernst热电势、磁电阻和霍尔电阻，并与单晶SbSc进行了比较。

Result: 在30 K和14 T下，多晶态SbSc的Nernst热电势达到了约128 μV/K，优于单晶SbSc。其Nernst功率因子最高达到约240 × 10⁻⁴ W m⁻¹ K⁻²，Nernst品质因数达到约11 × 10⁻⁴ K⁻¹。在2 K和14 T下，多晶态SbSc还表现出高达约940%的非饱和磁电阻。这些性能的提升归因于电子-空穴补偿的改善。

Conclusion: 多晶态SbSc在低温下的热电和热磁性能得到了改善，这归因于其电子-空穴补偿的提高以及立方对称性和各向异性的缺失，使得多晶态SbSc在热电和热磁以及电磁性能上可与单晶相媲美。

Abstract: Today, high-performance thermoelectric and thermomagnetic materials operating
in the low-temperature regime, particularly below the boiling point of liquid
nitrogen remain scarce. Most thermomagnetic materials reported to date exhibit
a strong Nernst signal along specific crystallographic directions in their
single-crystal form. However, their performance typically degrades
significantly in the polycrystalline form. Here, we report an improved Nernst
thermopower of $\sim$ 128 $\mu$V/K at 30 K and 14 T in polycrystalline
compensated semimetal ScSb, in comparison to that was observed in single
crystal ScSb previously. The magnetic field dependence of Nernst thermopower
shows a linear and non-saturating behavior up to 14 T. The maximum Nernst power
factor reaches to $\sim 240 \times 10^{-4}$ W m$^{-1}$ K$^{-2}$ and Nernst
figure of merit reaches to $\sim 11 \times 10^{-4}$ K$^{-1}$. Polycrystalline
ScSb also shows a large non-saturating magnetoresistance of $\sim 940 \%$ at 2
K and 14 T. These enhanced properties originate from better electron-hole
compensation, as revealed by Hall resistivity measurements. The cubic symmetry
and absence of anisotropy in ScSb allow its polycrystalline form to achieve
similar enhanced thermomagnetic and electromagnetic performance comparable to
that of the single crystal.

</details>


### [504] [Correlating synthesis, structure and thermal stability of CuBi nanowires for spintronic applications by electron microscopy and in situ scattering methods](https://arxiv.org/abs/2507.09553)
*Alejandra Guedeja-Marrón,Henrik Lyder Andersen,Gabriel Sánchez-Santolino,Lunjie Zeng,Alok Ranjan,Inés García-Manuz,François Fauth,Catherine Dejoie,Eva Olsson,Paolo Perna,Maria Varela,Lucas Pérez,Matilde Saura-Múzquiz*

Main category: cond-mat.mtrl-sci

TL;DR: 合成了铜铋合金纳米线，研究了其结构性质和热稳定性，为优化其在自旋电子器件中的自旋霍尔效应性能奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 铜铋合金（Cu$_{1-x}$Bi$_x$）纳米线因其潜在的巨大自旋霍尔效应（SHE）而成为自旋电子学的有希望的候选材料。

Method: 通过模板辅助电沉积法合成了具有不同铋含量（x=0, 2, 4, 7%）和不同晶域尺寸的铜铋合金（Cu$_{1-x}$Bi$_x$）纳米线，并利用先进电子显微镜和X射线散射技术进行了结构分析，采用可变温度X射线衍射和全散射技术研究了纳米线的热稳定性。

Result: 研究发现，较大的晶域尺寸有利于铋均匀掺杂到铜晶格中，而较小的晶域尺寸会导致铋在晶界处明显富集。加热时，观察到与铋从铜晶格扩散一致的晶格畸变，并在冷却时结晶为面心立方的金属铋。

Conclusion: 研究结果为优化铜铋合金纳米线在自旋电子器件中的自旋霍尔效应性能奠定了基础，通过关联合成参数与微观结构特征及热行为。

Abstract: Bi-doped copper (Cu1-xBix) nanowires (NWs), promising candidates for
spintronic applications due to their potential for a giant spin Hall effect
(SHE), were synthesized and their structural properties and thermal stability
were investigated. Using template-assisted electrodeposition, Cu1-xBix
nanowires with varying bismuth (Bi) content (x=0, 2, 4, and 7%) and different
crystalline domain sizes were fabricated. Structural analysis by advanced
electron microscopy and X-ray scattering techniques revealed the influence of
synthesis conditions on the resulting NW crystal structure and microstructure,
including Bi localization (within the lattice or in the grain boundaries),
crystallite domain dimensions, and lattice distortions. While NWs with larger
crystalline domains allow homogeneous Bi incorporation into the Cu lattice, NWs
with smaller crystalline domains exhibit noticeable Bi accumulation at grain
boundaries. The thermal stability of the NWs was examined using variable
temperature X-ray diffraction and total scattering. Upon heating, lattice
distortions consistent with Bi diffusion out of the Cu lattice were observed,
with subsequent crystallization of rhombohedral metallic Bi upon cooling. These
findings establish a foundation for optimizing the SHE performance of Cu1-xBix
nanowires for spintronic devices by correlating synthesis parameters with
microstructural features and thermal behavior.

</details>


### [505] [How to Fix Silver for Plasmonics](https://arxiv.org/abs/2507.09569)
*Björn Ewald,Leo Siebigs,Cheng Zhang,Jonas Graf,Achyut Tiwari,Maximilian Rödel,Sebastian Hammer,Vladimir Stepanenko,Frank Würthner,Bruno Gompf,Bert Hecht,Jens Pflaum*

Main category: cond-mat.mtrl-sci

TL;DR: 通过在银中加入少量金（Au）可以提高银薄膜的质量和稳定性，且无需复杂工艺，可以直接在玻璃上沉积，并可用于制造高质量、长寿命的光学天线。


<details>
  <summary>Details</summary>
Motivation: 银（Ag）虽然是可见光等离激元应用的理想材料，但其热蒸发薄膜的化学稳定性和结构质量较差，限制了其应用。

Method: 通过热共蒸发将金（Au）与银（Ag）合金化，研究了不同金含量（5-20 at%）的Ag$_{100-x}$Au$_x$薄膜的表面形貌、晶体结构、光学性质和化学稳定性。

Result: 低金含量可显著降低共蒸发薄膜的粗糙度（低至0.4 nm RMS），显著提高抗氧化性，同时保持明确的晶粒生长。其中，Ag$_{95}$Au$_5$薄膜表现出最高的化学稳定性、最低的可见光范围光学损耗和优异的等离激元性能，甚至优于纯银。

Conclusion: 通过合金化金来提高银薄膜的结构和光学质量以及化学稳定性，这是一种实用的解决方案，可以克服银在等离激元器件应用中的局限性。

Abstract: Silver (Ag) is considered an ideal material for plasmonic applications in the
visible wavelength regime due to its superior optical properties, but its use
is limited by the poor chemical stability and structural quality of thermally
evaporated thin films and resulting nanostructures. In this study, we present a
simple approach to enhance the structural and optical quality as well as the
chemical stability of Ag thin films by alloying with gold (Au) through thermal
co-evaporation. We investigate Ag$_{100-x}$Au$_x$ thin films with Au contents
ranging from 5 to 20 at% analyzing their surface morphology, crystallite
structure, optical properties, and chemical stability. Our results show that
low Au concentrations significantly reduce the roughness of co-evaporated thin
films (down to 0.4 nm RMS), and significantly enhance the resistance to
oxidation, while maintaining a defined crystallite growth. Importantly, these
improvements are achieved without the need for template stripping, metallic
wetting layers, or epitaxial substrates, enabling direct deposition on glass.
Among the compositions studied, Ag$_{95}$Au$_5$ thin films exhibit the highest
chemical stability, lowest optical losses in the visible spectral range, and
excellent plasmonic properties even outcompeting pure Ag. As a
proof-of-concept, we fabricate high-quality Ag$_{95}$Au$_5$ optical antennas
that exhibit long-term durability under ambient conditions. Our approach
provides a practical solution to overcome the limitations of Ag for plasmonic
device applications.

</details>


### [506] [Quantum metric-based optical selection rules](https://arxiv.org/abs/2507.09260)
*Yongpan Li,Cheng-Cheng Liu*

Main category: cond-mat.mtrl-sci

TL;DR: 光学选择定则新理论，利用量子度规解释光与物质相互作用，为谷基光电子和自旋电子学应用开辟新途径。


<details>
  <summary>Details</summary>
Motivation: 光学选择定则在设计光电子器件中起着关键作用，而传统的选择定则理论仅考虑了 Berry 弧率，忽略了量子度规的作用。本研究旨在提出包含量子度规的光学选择定则，以期在光电子器件设计中提供新的理论基础。

Method: 提出基于量子度规的光学选择定则，并利用紧束缚和第一性原理计算在交替磁体和 Kane-Mele 模型以及单层 d 波交替磁体 V2SeSO 中进行了验证。

Result: 发现了量子度规与振荡器强度的普遍对应关系，适用于线偏振光。建立了谷区分明的光学选择定则，实现了将正交线偏振与不同谷的锁定。

Conclusion: 该研究提出了基于量子度规的光学选择定则，并建立了谷区分明的光学选择定则，将正交的线偏振与不同的谷联系起来。在太-白宾和 Kane-Mele 模型以及单层 d 波交替磁体 V2SeSO 中，紧束缚和第一性原理计算证实了该理论。该工作为基于谷的自旋电子学和光电子学应用提供了量子度规范式。

Abstract: The optical selection rules dictate symmetry-allowed/forbidden transitions,
playing a decisive role in engineering exciton quantum states and designing
optoelectronic devices. While both the real (quantum metric) and imaginary
(Berry curvature) parts of quantum geometry contribute to optical transitions,
the conventional theory of optical selection rules in solids incorporates only
Berry curvature. Here, we propose quantum metric-based optical selection rules.
We unveil a universal quantum metric-oscillator strength correspondence for
linear polarization of light and establish valley-contrasted optical selection
rules that lock orthogonal linear polarizations to distinct valleys.
Tight-binding and first-principles calculations confirm our theory in two
models (altermagnet and Kane-Mele) and monolayer $d$-wave altermagnet
$\mathrm{V_2SeSO}$. This work provides a quantum metric paradigm for
valley-based spintronic and optoelectronic applications.

</details>


### [507] [$WSe_2$ Monolayers Grown by Molecular Beam Epitaxy on hBN](https://arxiv.org/abs/2507.09275)
*Julia Kucharek,Mateusz Raczyński,Rafał Bożek,Anna Kaleta,Bogusława Kurowska,Marta Bilska,Sławomir Kret,Takashi Taniguchi,Kenji Watanabe,Piotr Kossacki,Mateusz Goryca,Wojciech Pacuski*

Main category: cond-mat.mtrl-sci

TL;DR: 通过优化外延生长工艺，成功制备出高质量、光学均匀的WSe2单分子层，性能优于机械剥离法，为器件制造带来进步。


<details>
  <summary>Details</summary>
Motivation: 为了提高外延生长过渡金属硫族化合物（TMDs）单分子层的光学质量，并为大规模功能器件的制造提供更便捷、可重复性更高的方法。

Method: 采用外延生长技术，利用六方氮化硼作为衬底，通过三步工艺优化了WSe2单分子层的生长，并利用原子力显微镜和高分辨率透射电子显微镜对其形貌和单分子层特性进行了表征。

Result: 成功制备了光学均匀、高质量的WSe2单分子层，其激子复合物的行为与机械剥离的对应物相似，并且在易用性和可重复性方面优于机械剥离的WSe2。

Conclusion: 该研究在利用外延生长方法制备高质量、光学均匀的WSe2单分子层方面取得了重要进展，为大规模功能器件的制造奠定了基础。

Abstract: A three-step process was developed for growing high-quality, optically
uniform WSe2 monolayers by molecular beam epitaxy (MBE) with advantage of using
hexagonal boron nitride (hBN). The process was optimized to maximize the
efficiency of photoluminescence and promote formation of hexagonal WSe2
domains. Atomic force microscopy (AFM) was employed to estimate the dispersion
of WSe2 hexagonal domains orientation. Monolayer character of the film was
identified using optical methods and verified with high-resolution transmission
electron microscopy (TEM) cross-section.
Temperature-and-magnetic-field-dependent studies revealed the behaviour of
exciton complexes to be analogical to that of exfoliated counterparts. Direct
growth on hBN combined with uniform optical response proves this WSe2 superior
to mechanically exfoliated WSe2 in terms of convenience of use and
reproducibility. Provided results establish a significant progress in optical
quality of epitaxially grown transition metal dichalcogenides (TMDs) monolayers
and fabrication of large-scale functional devices.

</details>


### [508] [First-principles design for strain-tunable exciton dynamics in 2D materials](https://arxiv.org/abs/2507.09363)
*Amir Kleiner,Sivan Refaely-Abramson*

Main category: cond-mat.mtrl-sci

TL;DR: 该研究提出了一种模拟非均匀应变下WS2激子动力学的方法，发现了由应变控制的超球弹性传播和负扩散现象。


<details>
  <summary>Details</summary>
Motivation: 控制二维半导体中的激子运动对于实现新的光电和应变电子功能至关重要。单层过渡金属硫族化物（TMDs）由于其紧密束缚的激子和对局部应变的高度敏感性，为这种控制提供了一个理想的平台。

Method: 结合了基于第一性原理的激子能带结构和在实空间及动量空间中运行的半经典输运模型，对非均匀应变的WS2进行建模。

Result: 揭示了激子在没有经验参数的情况下会经历漂移、扩散和限制，并发现了超球弹性传播和负有效扩散等现象。

Conclusion: 该研究揭示了超球弹性传播和负有效扩散的机制，这些机制完全由应变景观决定，为理解和设计二维材料中的激子流提供了微观视角。

Abstract: Controlling exciton motion in two-dimensional semiconductors is key to
unlocking new optoelectronic and straintronic functionalities. Monolayer
transition metal dichalcogenides (TMDs), with their tightly bound excitons,
offer an ideal platform for such control due to their strong sensitivity to
local strain. Here we present an ab initio framework for modeling exciton
dynamics in inhomogeneously strained WS2, combining excitonic band structures
derived from first principles with a semiclassical transport model operating in
both real and momentum space. By analyzing idealized strain patterns, we reveal
how excitons undergo drift, diffusion, and confinement without invoking
empirical parameters. Our results uncover regimes of super-ballistic
propagation and negative effective diffusion, governed entirely by the strain
landscape. This work provides microscopic insight into strain-tunable exciton
behavior and establishes design principles for engineering exciton flow in
two-dimensional materials.

</details>


### [509] [Nanoscale friction of manganite superlattice films controlled by layer thickness and fluorine content](https://arxiv.org/abs/2507.09429)
*Niklas A. Weber,Miru Lee,Florian Schönewald,Leonard Schüler,Vasily Moshnyaga,Matthias Krüger,Cynthia A. Volkert*

Main category: cond-mat.mtrl-sci

TL;DR: 本研究使用侧向力显微镜，通过研究氟掺杂和顶层厚度对[LaMnO3]m/[SrMnO3]n 叠层薄膜纳米摩擦的影响，发现摩擦力与法向力和粘附力之和呈线性关系，摩擦系数是可调的材料特性。


<details>
  <summary>Details</summary>
Motivation: 本次研究的动机是探究氟掺杂和顶层厚度对[LaMnO3]m/[SrMnO3]n 叠层薄膜纳米尺度摩擦的影响。

Method: 使用侧向力显微镜研究了纳米尺度下[LaMnO3]m/[SrMnO3]n 叠层薄膜的摩擦现象，重点关注了氟掺杂和顶层厚度的影响。

Result: 摩擦力与施加的法向力和粘附力之和呈线性关系。尽管由于局部粘附力波动导致摩擦力在空间上存在差异，但对于每个样品而言，摩擦系数在不同位置上是保持不变的。然而，摩擦系数会受到氟浓度和顶层厚度的系统性影响。研究数据表明，摩擦能量耗散延伸到表面以下 5 纳米，并且明显依赖于表面以下的结构。作者将此归因于滑动尖端产生的应力场和倏逝波中的粘弹性耗散，这可以定量地解释所观测到的摩擦系数。

Conclusion: 研究结果表明，一旦准确考虑了粘附力，摩擦系数便成为一种可重复的材料特性，可以通过对表层和次表层进行可控修饰进行调整。

Abstract: We investigate nanoscale friction in [LaMnO3]m/[SrMnO3]n superlattice films
using lateral force microscopy, focusing on the effects of fluorine doping and
top-layer thickness. For all samples, friction forces scale linearly with the
sum of the applied normal and adhesion forces. While friction forces vary
spatially due to local adhesion fluctuations, the friction coefficient remains
position independent for each specimen. It is, however, systematically
influenced by fluorine concentration and top-layer thickness. Our data
indicates that frictional energy dissipation extends up to 5 nm beneath the
surface, demonstrating a clear dependence on subsurface structure. We attribute
this to viscoelastic dissipation within the stress field and evanescent waves
generated by the sliding tip, which can quantitatively account for the observed
friction coefficients. These results show that, once adhesion is properly
accounted for, the friction coefficient is a reproducible material property
that can be tuned via controlled modifications to surface and subsurface
layers.

</details>


### [510] [On-the-fly machine learning-augmented constrained AIMD to design new routes from glassy carbon to quenchable amorphous diamond with low pressure and temperature](https://arxiv.org/abs/2507.09639)
*Meng-Qi Cheng,Wei-Dong Luo,Hong Sun*

Main category: cond-mat.mtrl-sci

TL;DR: 本研究提出了一种新的机器学习增强模拟方法（ML-augmented CAIMD），成功模拟了玻璃碳在极端条件下的转变，发现了其独特的力学性能和转变规律，并提出了一条迄今为止压力-温度最低的合成硬质非晶碳的路线。


<details>
  <summary>Details</summary>
Motivation: 传统的AIMD方法无法有效捕捉各向异性应力效应，而这种效应被认为在玻璃碳向非晶金刚石的转变中起关键作用。

Method: 提出了一种名为ML增强CAIMD（on-the-fly machine learning-augmented constrained AIMD）的新方法，通过修改VASP 6.3.2实现，能够有效处理各向异性应力效应，用于模拟玻璃碳等无序材料。

Result: 模拟结果不仅重现了玻璃碳的主要实验特征，还提供了限制性的合成条件和微观见解。研究发现玻璃碳具有高塑性，其抗压和抗剪强度随大应变而增强。在压力下，提高退火温度有利于形成非晶金刚石，但超过2900 K后，热石墨化效应会逆转此趋势。在非静水压力下，玻璃碳转变为超硬结构，其应力差随约束压力超过40 GPa而急剧增加。在30 GPa和300-1000 K下进行严重的旋转剪切可使sp3含量高达80%。在300 K下减压可获得保持64% sp3含量的硬化非晶碳，这是预测的最低压力-温度合成路线。

Conclusion: 本研究提出的ML增强CAIMD方法成功模拟了玻璃碳在极端条件下的转变，并揭示了其塑性、抗压和抗剪强度随大应变而增强的特性。研究还确定了玻璃碳转变为非晶金刚石的最佳合成条件，并发现高温和高压下的剪切应力可以显著提高sp3键的比例，最终在减压后得到一种硬质非晶碳，这是迄今为止压力-温度最低的合成路线。该方法为模拟无序材料在各向异性应力下的结构转变提供了通用框架。

Abstract: Recent advances in machine learning have enabled large-scale atomic
simulations with first-principles accuracy, allowing precise modeling of
disordered materials such as glassy carbon (GC). However, conventional ab
initio molecular dynamics (AIMD) cannot effectively capture anisotropic stress
effects, which are believed to play a key role in the transformation of GC into
amorphous diamond under extreme conditions. In this work, we present an
on-the-fly machine learning-augmented constrained AIMD (ML-augmented CAIMD)
approach by modifying VASP 6.3.2. Our simulations not only reproduce major
experimental features of GC but also provide restrictive synthesis conditions
and microscopic insights. We show that GC exhibits unexpectedly high
plasticity, with its compressive and shear strengths enhanced by large strains.
Under pressure, increasing annealing temperature promotes the formation of
quenchable amorphous diamond via enhanced sp3 preservation, but this trend
reverses above 2900 K due to thermal graphitization. Under non-hydrostatic
compression, GC transforms into a superhard structure sustaining large stress
differences, which sharply increase when confining pressure exceeds 40 GPa.
Finally, severe rotational shear at 30 GPa induces sp3 fractions up to 80
percent at 300 to 1000 K. A hardened amorphous carbon retaining 64 percent sp3
content is achieved by decompression at 300 K, marking the lowest
pressure-temperature route ever predicted. Our ML-augmented CAIMD provides a
general framework for modeling structural transformations in disordered
materials under anisotropic stresses.

</details>


### [511] [Navigating the Evolution of Two-dimensional Carbon Nitride Research: Integrating Machine Learning into Conventional Approaches](https://arxiv.org/abs/2507.09669)
*Deep Mondal,Sujoy Datta,Debnarayan Jana*

Main category: cond-mat.mtrl-sci

TL;DR: 本综述全面回顾了机器学习（ML）在碳氮化物研究中的应用，重点介绍了ML预测材料性质、优化合成和提高性能的潜力。机器学习与传统实验相结合的协同作用得到了强调，并指出了未来的发展方向，包括高质量数据集、先进的ML模型和跨学科合作的必要性。


<details>
  <summary>Details</summary>
Motivation: 为了探索和优化碳氮化物材料的潜力，本研究对ML技术在碳氮化物研究中的应用进行了全面的回顾。

Method: 对ML技术（如监督学习、无监督学习和强化学习）在预测材料性质、优化合成条件和提高性能指标方面的应用进行了讨论。

Result: ML算法可以显著减少实验的试错次数，加速发现过程，并提供对碳氮化物结构-性质关系的更深入见解。已展示出ML驱动的模型能够成功预测具有增强功能特性的新型碳氮化物组合。

Conclusion: ML算法能够显著减少实验的试错次数，加速材料的发现过程，并提供对碳氮化物结构-性质关系的更深入理解。将ML与传统实验方法相结合的协同作用得到了强调，展示了ML驱动的模型成功预测了具有增强功能特性的新型碳氮化物成分。

Abstract: Carbon nitride research has reached a promising point in today's research
endeavours with diverse applications including photocatalysis, energy storage,
and sensing due to their unique electronic and structural properties. Recent
advances in machine learning (ML) have opened new avenues for exploring and
optimizing the potential of these materials. This study presents a
comprehensive review of the integration of ML techniques in carbon nitride
research with an introduction to CN classifications and recent advancements. We
discuss the methodologies employed, such as supervised learning, unsupervised
learning, and reinforcement learning, in predicting material properties,
optimizing synthesis conditions, and enhancing performance metrics. Key
findings indicate that ML algorithms can significantly reduce experimental
trial-and-error, accelerate discovery processes, and provide deeper insights
into the structure-property relationships of carbon nitride. The synergistic
effect of combining ML with traditional experimental approaches is highlighted,
showcasing studies where ML driven models have successfully predicted novel
carbon nitride compositions with enhanced functional properties. Future
directions in this field are also proposed, emphasizing the need for
high-quality datasets, advanced ML models, and interdisciplinary collaborations
to fully realize the potential of carbon nitride materials in next-generation
technologies.

</details>


### [512] [Bulk Ferroelectric Heterostructures for High Temperature Lead-Free Piezoelectrics](https://arxiv.org/abs/2507.09673)
*Yizhe Li,Ziqi Yang,Ying Chen,Zhenbo Zhang,YunLong Tang,Matthew Smith,Matthew Lindley,Xuezhen Cao,David G. Hopkinson,Andrew J. Bell,Steven J. Milne,Antonio Feteira,Sarah J. Haigh,Alexander S. Eggeman,Juncheng Pan,Jiajun Shi,David A. Hall*

Main category: cond-mat.mtrl-sci

TL;DR: 通过自下而上的合成方法制备了块状铁电异质结构陶瓷，实现了优异的铁电和压电性能，并为探索新功能提供了途径。


<details>
  <summary>Details</summary>
Motivation: 为了克服传统铁电异质结构中超薄层状结构和对晶格匹配衬底的依赖性等限制，本研究旨在开发一种新的合成途径，将界面特性和功能性应用于块状材料中。

Method: 该研究利用元素偏析和阳离子扩散的自下而上方法，合成了具有周期性成分波动（波长为 8 nm）的块状铁电异质结构 (BFH) 陶瓷。

Result: 成功合成了具有周期性成分波动的块状铁电异质结构 (BFH) 陶瓷，实现了高达 824 °C 的居里温度和 115 pC N⁻¹ 的压电系数，并展示了对局部铁电序和畴稳定性的前所未有的控制能力。

Conclusion: 该研究通过元素偏析和阳离子扩散的自下而上方法，成功合成了具有周期性成分波动（波长为 8 nm）的块状铁电异质结构 (BFH) 陶瓷。这些 BFH 陶瓷在铁电极化控制、居里温度 (Tc) 和压电性能方面表现出色，其中 Tc 最高可达 824 °C，压电系数 (d33) 为 115 pC N⁻¹，创下了钙钛矿或非钙钛矿固溶体的记录。通过改变电机械边界条件，可以实现不同的阳离子偏析区域形貌和铁电序类型。这种形成机制为 BFH 陶瓷的局部铁电序和畴稳定提供了前所未有的控制能力，并为探索超越块状铁电体和薄膜异质结构的新型功能开辟了道路。

Abstract: Remarkable exploitation of valence and lattice mismatch in epitaxial
ferroelectric heterostructures generates physical effects not classically
expected for perovskite oxides, such as 2D electron gas and polar skyrmions.
However the widespread application of these interfacial properties and
functionalities is impeded by the ultrathin layered structure and essential
presence of underlying lattice-matched substrates for the deposition of
epitaxial thin films. Here, we report a bottom-up pathway to synthesize bulk
ferroelectric heterostructures (BFH) with periodic composition fluctuation (8
nm in wavelength) using elemental partitioning by cation diffusion, providing
opportunities to exploit novel characteristics of hetero-epitaxial oxide thin
films in bulk materials. Exemplar monolithic BiFeO3-BaTiO3 BFH ceramics
described herein share common features with their thin film heterostructure
counterparts, which facilitates control and stabilisation of ferroelectric
polarisation along with a significant enhancement in Curie temperature, Tc, and
functionality. BFH ceramics exhibit a record Tc (up to 824 {\deg}C) and a
piezoelectric coefficient (d33 = 115 pC N-1 ), in comparison with other
perovskite or non-perovskite solid solutions, providing sustainable solutions
for emergent high temperature piezoelectric sensing, actuation and energy
conversion applications. By creating BFH ceramics using different
electromechanical boundary conditions, distinct morphologies of aliovalent
A-site cation segregated regions along with different types of ferroelectric
order are achieved. This formation mechanism provides unprecedented control
over local ferroelectric ordering and domain stabilisation in BFH ceramics; it
also paves the way to explore new types of functionality, beyond those
achievable in both bulk ferroelectrics and thin film heterostructures.

</details>


### [513] [Field-effect transistors based on charged domain walls in van der Waals ferroelectric α-In$_2$Se$_3$](https://arxiv.org/abs/2507.09838)
*Shahriar Muhammad Nahid,Haiyue Dong,Gillian Nolan,Andre Schleife,SungWoo Nam,Pinshane Y. Huang,Nadya Mason,Arend M. van der Zande*

Main category: cond-mat.mtrl-sci

TL;DR: 这项研究通过制备基于范德华铁电材料CDW的场效应晶体管，成功降低了CDW的电阻率并提高了器件性能，为开发新型存储器和计算器件开辟了道路。


<details>
  <summary>Details</summary>
Motivation: 克服了传统铁电体中CDW垂直、埋藏或电学不可及的挑战，以实现CDW在非易失性存储器、逻辑和神经形态计算等领域的应用。

Method: 利用边缘接触制备了基于CDW的场效应晶体管（CDW-FET），并研究了其原子结构与温度依赖的输运性质之间的关系。

Result: CDW-FETs表现出随温度降低的金属-绝缘体转变，并且相比于单畴 $\alpha$-In$_2$Se$_3$ 具有更高的电导率和场效应迁移率。在低于70 K时，传输机制为无序引起的变程跳跃；在高于70 K时，传输机制为热激活的界面陷阱辅助传输。CDW-FETs在室温下的电阻率低至3.1 k$\Omega$，比薄膜铁电体中的单CDW低2-9个数量级。

Conclusion: 通过堆叠反向畴的范德华铁电材料 $\alpha$-In$_2$Se$_3$ 生成人工头-头（H-H）CDW，并利用边缘接触制备了基于CDW的场效应晶体管（CDW-FET），解决了传统铁电体中CDW的电阻率高和器件集成困难的问题，为实现高频存储器和神经形态计算提供了新的可能性。

Abstract: Charged domain walls (CDW) in ferroelectrics are emerging as functional
interfaces with potential applications in nonvolatile memory, logic, and
neuromorphic computing. However, CDWs in conventional ferroelectrics are
vertical, buried, or electrically inaccessible interfaces that prevent their
use in functional devices. Here, we overcome these challenges by stacking two
opposite polar domains of van der Waals ferroelectric $\alpha$-In$_2$Se$_3$ to
generate artificial head-head (H-H) CDWs and use edge contact to fabricate
charged domain wall-based field-effect transistors (CDW-FET). We relate the
atomic structure to the temperature-dependent electrical and magneto-transport
of the CDW-FET. CDW-FETs exhibit a metal-to-insulator transition with
decreasing temperature and enhanced conductance and field-effect mobility
compared to single domain $\alpha$-In$_2$Se$_3$. We identify two regimes of
transport: variable range hopping due to disorder in the band edge below 70 K
and thermally activated interfacial trap-assisted transport above 70 K. The
CDW-FETs show room-temperature resistance down to 3.1 k$\Omega$ which is 2-9
orders of magnitude smaller than the single CDW in thin-film ferroelectrics.
These results resolve longstanding challenges with high CDW resistance and
their device integration, opening opportunities for gigahertz memory and
neuromorphic computing.

</details>


### [514] [An analytical model for the remote epitaxial potential](https://arxiv.org/abs/2507.09913)
*Jason K Kawasaki,Quinn T Campbell*

Main category: cond-mat.mtrl-sci

TL;DR: 提出了一个基于Morse势的分析模型，用于描述石墨烯远程外延中衬底的远程结合势。该模型考虑了距离衰减和自由载流子筛选，结果显示远程结合势与石墨烯的范德华势相当，表明在解释实验时需要考虑石墨烯与衬底势的干扰以及界面重构。模型还能指导实验以增强远程结合势。


<details>
  <summary>Details</summary>
Motivation: 提供一个更直接、明确包含筛选效应的远程外延结合势的描述，并基于简单、可解释且经过充分检验的参数。与仅使用静电势作为结合代用物的先前密度泛函理论计算相比，该模型能更直接地描述结合，并明确包含通常被忽略的筛选效应。

Method: 提出一个基于Morse原子间势的分析模型，考虑了石墨烯与衬底之间距离增加和自由载流子筛选引起的衰减。

Result: 远程外延的结合势|ϕremote|的大小对于典型的半导体和氧化物衬底仅为几个毫电子伏，与石墨烯的范德华势相似。通过模型解释了先前远程外延和相关文献中的实验，并指出了与莫尔外延和石墨烯插层化合物有序化的联系。

Conclusion: 需要考虑石墨烯与远程衬底势能的干扰以及界面重构，才能解释远程外延实验。提出的模型可以指导实验，通过可调谐的筛选和衬底势的空间范围来增强远程势能。

Abstract: We propose an analytical model for the remote bonding potential of the
substrate that permeates through graphene during remote epitaxy. Our model,
based on a Morse interatomic potential, includes the attenuation due to (1) the
increased separation between film and substrate and (2) free carrier screening
from graphene. Compared with previous slab density functional theory
calculations, which use the electrostatic potential as a proxy for bonding, our
analytical model provides a more direct description of bonding, explicitly
includes screening (which is often ignored), and is based on simple,
interpretable, and well benchmarked parameters. We show that the magnitude of
$|\phi_{remote}|$ for typical semiconductor and oxide substrates is few meV,
similar to the van der Waals potential of graphene. This suggests interference
between the graphene and remote substrate potentials, plus interfacial
reconstructions, must be considered when interpreting experiments on remote
epitaxy. We use our model to interpret previous experiments from the remote
epitaxy and related literature, highlighting connections to moire epitaxy and
to the ordering of graphite intercalation compounds. Our model also points to
tests, based on tunable screening and spatial extent of the substrate
potential, that may increase the strength of the remote potential towards the
more idealized picture.

</details>


### [515] [First-Principles Theory of Five- and Six-Phonon Scatterings](https://arxiv.org/abs/2507.09918)
*Yi Xia*

Main category: cond-mat.mtrl-sci

TL;DR: 本文研究了超出四阶的更高阶声子散射，特别是在高温强内耗材料中。作者开发了一种计算五阶和六阶声子散射的理论方法，并研究了硅、氧化镁和氧化钡。结果发现，在氧化钡中，这些更高阶的声子散射在接近熔点时变得非常重要，甚至超过了低阶散射，并显著降低了热导率。这主要是由于原子间力常数和散射相空间的影响。


<details>
  <summary>Details</summary>
Motivation: 超出四阶的更高阶声子散射尽管在高温强内耗材料中可能很重要，但仍未得到充分探索。

Method: 本文开发了一种基于图解形式主义的格林函数技术的理论形式，用于计算五阶和六阶声子散射，并系统地研究了硅、氧化镁和氧化钡在室温至熔点附近的多声子相互作用。

Result: 研究结果表明，在硅中，五阶和六阶声子过程可忽略不计；在氧化镁中，它们变得越来越重要；在接近熔点的氧化钡中，它们占主导地位，超过了三阶和四阶声子散射强度，并显著降低了晶格热导率。更高阶相互作用的强度主要由原子间力常数控制，氧化钡的五阶和六阶声子散射率比氧化镁强一个数量级以上，尽管晶体结构相同，这是由于软化的谐波相互作用产生了大的散射相空间。

Conclusion: 该研究为理解强内耗材料和高温下的晶格动力学和热传输奠定了理论基础。

Abstract: Higher-order phonon scatterings beyond fourth order remain largely unexplored
despite their potential importance in strongly anharmonic materials at elevated
temperatures. We develop a theoretical formalism for first-principles
calculation of five- and six-phonon scatterings using Green's function
techniques based on a diagrammatic formalism, and systematically investigate
multi-phonon interactions in Si, MgO, and BaO from room temperature to near
melting points. Our calculation reveals dramatically different
material-dependent behaviors: while five- and six-phonon processes remain
negligible in Si, they become increasingly important in MgO and dominant in BaO
near its melting point, where they surpass three- and four-phonon scattering
intensity and significantly reduce lattice thermal conductivity. We demonstrate
that the strength of higher-order interactions is primarily governed by
interatomic force constants, with BaO exhibiting five- and six-phonon
scattering rates over one order of magnitude stronger than MgO despite
identical crystal structures, due to large scattering phase space arising from
softened harmonic interactions. Our work establishes the theoretical foundation
for understanding lattice dynamics and thermal transport in strongly anharmonic
materials and at elevated temperatures.

</details>


### [516] [Observation of Chiral Phonons in Methylbenzylammonium Lead Iodide](https://arxiv.org/abs/2507.10036)
*Sankaran Ramesh,Prasenjit Mandal,Pratik Bhagwat,Yong Li,Tönu Pullerits,Dmitry Baranov*

Main category: cond-mat.mtrl-sci

TL;DR: 飞秒瞬态吸收光谱在手性金属卤化物 (R-MBA)$_2$PbI$_4$薄膜中发现了一个2.5 meV的光学声子，这在消旋化合物中不存在，证明了手性起源，并支持了手性从有机层到无机层的传递的理论。


<details>
  <summary>Details</summary>
Motivation: 研究混合金属卤化物和钙钛矿的自旋极化性质，以及手性对光学声子的影响。

Method: 通过飞秒瞬态吸收光谱技术观察到手性金属卤化物 (R-MBA)$_2$PbI$_4$薄膜中存在一个2.5 meV的光学声子，而在消旋对应物中未观察到该声子。

Result: 在手性金属卤化物 (R-MBA)$_2$PbI$_4$薄膜中观察到2.5 meV的光学声子，该声子在消旋对应物中不存在，表明该声子具有手性起源，并支持了手性从有机层到无机层的传递的理论预测。

Conclusion: 该实验结果表明2.5 meV模式的 the chiral origin，并支持了近期关于手性从 the organic to the inorganic layers 的手性传递的理论预测，这对 hybrid metal halides and perovskites 的自旋极化性质具有启示意义。

Abstract: An optical phonon at 2.5 meV is observed in a thin film of the chiral metal
halide (R-MBA)$_2$PbI$_4$, but is absent in the racemic counterpart, as
revealed by femtosecond transient absorption spectroscopy. This experimental
result indicates the chiral origin of the 2.5 meV mode and supports recent
theoretical predictions of chirality transfer from the organic to the inorganic
layers, with implications for the spin polarization properties of hybrid metal
halides and perovskites.

</details>


### [517] [Quantum-Annealing Enhanced Machine Learning for Interpretable Phase Classification of High-Entropy Alloys](https://arxiv.org/abs/2507.10237)
*Diego Ibarra Hoyos,Gia-Wei Chern,Israel Klich,Joseph Poon*

Main category: cond-mat.mtrl-sci

TL;DR: 利用量子退火和量子增强机器学习（QBoost 和 QSVM）来加速高熵合金（HEAs）的相预测，实现了与经典模型相当或更优的准确性和更短的训练时间。


<details>
  <summary>Details</summary>
Motivation: 高熵合金（HEAs）为设计先进材料提供了前所未有的成分灵活性，但由于数据有限和相形成行为复杂，预测其晶体学相仍然是一个关键瓶颈。

Method: 本研究提出了一种量子增强机器学习框架，利用量子退火来增强高熵合金（HEAs）的相分类。该流程整合了用于可解释特征选择和分类的量子增强（QBoost），以及使用量子增强核来捕捉物理描述符之间非线性关系的量子支持向量机（QSVM）。通过将两种模型都重新表述为二次无约束二元优化（QUBO）问题，利用量子退火器的有效采样能力来实现快速训练和鲁棒泛化，并与经典基线相比在我们的设置中展示了显著的运行时长缩减。

Result: 我们针对六个关键相：面心立方（FCC）、体心立方（BCC）、σ相、拉夫相（Laves）、休斯勒相（Heusler）和 AlXY B2 相进行了基准测试，并使用交叉验证和严格筛选的先前实验合成 HEAs 测试集对模型性能进行了基准测试。结果证实预测相和测量相之间具有很强的吻合性。

Conclusion: 量子增强分类器在准确性上匹配或超过经典模型，并提供基于可解释物理描述符的见解。这项工作是朝着材料发现管道的实际量子加速迈出的重要一步。

Abstract: High entropy alloys (HEAs) offer unprecedented compositional flexibility for
designing advanced materials, yet predicting their crystallographic phases
remains a key bottleneck due to limited data and complex phase formation
behavior. Here, we present a quantum-enhanced machine learning framework that
leverages quantum annealing to enhance phase classification in HEAs. Our
pipeline integrates Quantum Boosting (QBoost) for interpretable feature
selection and classification, with Quantum Support Vector Machines (QSVM) that
use quantum-enhanced kernels to capture nonlinear relationships between
physical descriptors. By reformulating both models as Quadratic Unconstrained
Binary Optimization (QUBO) problems, we exploit the efficient sampling
capabilities of quantum annealers to achieve rapid training and robust
generalization, demonstrating notable runtime reductions relative to classical
baselines in our setup. We target six key phases: FCC, BCC, Sigma, Laves,
Heusler, and AlXY B2, and benchmark model performance using both
cross-validation and a rigorously curated test set of prior experimentally
synthesized HEAs. The results confirm strong alignment between predicted and
measured phases. Our findings demonstrate that quantum-enhanced classifiers
match or exceed classical models in accuracy and offer insights grounded in
interpretable physical descriptors. This work constitutes an important step
toward practical quantum acceleration in materials discovery pipelines.

</details>


### [518] [Rigid-Body Anisotropy in Noncollinear Antiferromagnets](https://arxiv.org/abs/2507.10238)
*Zheng Liu,Yang Gao,Qian Niu*

Main category: cond-mat.mtrl-sci

TL;DR: 提出一种新理论，用基函数描述反铁磁材料中的各向异性效应，并成功应用于 Mn3Sn 和 Mn3Ir。


<details>
  <summary>Details</summary>
Motivation: 为了表征非共线反铁磁体的各向异性结构，这对于反铁磁自旋电子学至关重要。

Method: 利用自旋群的群表示论，构建由自旋-轨道耦合产生的自旋序刚体旋转相关的自旋-轨道矢量张量元构成的基函数，以系统地描述各向异性效应的结构。

Result: 推导出描述磁各向异性效应的基函数，并成功应用于 Mn3Sn 和 Mn3Ir，验证了该方法能有效捕捉磁各向异性能和反常霍尔电导率的几何和幅度依赖性。

Conclusion: 本文提出了一种微观理论，将自旋序刚体旋转引起的各向异性效应与自旋-轨道耦合联系起来，并成功应用于 Mn3Sn 和 Mn3Ir 等反铁磁材料，能够很好地捕捉磁各向异性能和反常霍尔电导率的几何和幅度依赖性。

Abstract: Characterizing the anisotropic structure in noncollinear antiferromagnets is
essential for antiferromagnetic spintronics. In this work, we provide a
microscopic theory linking the anisotropy effects induced by the rigid-body
rotation of spin order to spin-orbit coupling. Our method goes beyond the
conventional magnetic group theory, offering a concise yet powerful tool to
characterize diverse anisotropy effects in complex magnetic systems. Using the
group representation theory of the spin group, we obtain a set of basis
functions formed from tensor elements of spin-orbit vector--which originates
from spin-orbit coupling and is tied to the rigid-body rotation of the spin
order--to systematically describe the structure of anisotropy effects. As a
concrete example, we apply our framework to coplanar antiferromagnets Mn$_3$Sn
and Mn$_3$Ir, demonstrating that the corresponding basis functions can well
capture both the geometric and magnitude dependencies of the magnetic
anisotropy energy and anomalous Hall conductivity. Finally, we discuss the
generalization of our framework to broader classes of anisotropy phenomena in
magnetic systems.

</details>


### [519] [Practical Crystallography with a Transmission Electron Microscope](https://arxiv.org/abs/2507.10247)
*Benjamin L. Weare,Kayleigh L. Y. Fung,Ian Cardillo-Zallo,William J. Cull,Michael W. Fay,Stephen P. Argent,Paul D. Brown*

Main category: cond-mat.mtrl-sci

TL;DR: 该研究提出了一款名为GiveMeED的免费软件，用于简化和普及透射电子显微镜（TEM）的三维电子衍射（3DED）技术，使得在常规TEM上进行晶体结构解析更加容易和常规化，并已成功应用于多种化合物的结构测定。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有三维电子衍射（3DED）技术对特殊设备和软件的依赖性问题，该研究旨在开发一种更易于广泛使用的解决方案，以降低技术门槛，促进其应用。

Method: 该研究提出了一种名为GiveMeED的新软件，用于控制三维电子衍射（3DED）数据采集。结合了直接成像、能量色散X射线光谱（EDS）和电子能量损失光谱（EELS）技术，并在常规透射电子显微镜（TEM）上实现了3DED数据采集的完整实验流程，包括电子通量定量。

Result: 研究成功开发并验证了GiveMeED软件，该软件能够实现对常规TEM进行3DED数据采集，并通过对氨酚、酞菁铜（II）和高氯冠醚等样本的结构解析，证明了其准确性和有效性，确定的晶胞参数和原子连接性与文献值一致。提供的软件是免费的。

Conclusion: 该研究通过提供名为GiveMeED的新软件，并结合直接成像、能量色散X射线光谱（EDS）和电子能量损失光谱（EELS）等技术，为在常规透射电子显微镜（TEM）上进行三维电子衍射（3DED）数据采集和晶体结构解析提供了完整的实验流程，从而有效降低了3DED技术的应用门槛，并被证明对多种化合物具有准确性和有效性，最终目标是促进3DED技术的广泛应用。

Abstract: Three-dimensional electron diffraction (3DED) is a powerful technique
providing for crystal structure solutions of sub-micron sized crystals too
small for structure determination via X-ray techniques. The entry requirement,
however, of a transmission electron microscope (TEM) adapted with bespoke
software for coordinated sample stage rotation and continuous electron
diffraction data acquisition has generally inhibited the wider uptake of 3DED.
To address this limitation, we present novel software GiveMeED appropriate for
controlled 3DED data acquisition. The collection of useable reflections beyond
0.8 {\AA} makes 3DED crystallographic processing effectively routine, using
standard software and workflows derived from single-crystal X-ray diffraction
(SCXRD) techniques. A full experimental workflow for 3DED on a conventional TEM
is described in practical terms, in combination with direct imaging, and energy
dispersive X-ray spectroscopy (EDS) and electron energy loss spectroscopy
(EELS), for the return of comprehensive correlative descriptions of crystal
morphologies and sample compositions, with due regard for the quantification of
electron flux at each stage of the characterisation process. The accuracy and
effectiveness of GiveMeED is demonstrated through structure solutions for case
study paracetamol, copper(II) phthalocyanine, and percholorocoronene samples,
characterised in their near-native states under controlled low dose conditions
at either room or cryogenic temperatures, with determined unit cell parameters
and atomic connectivity matching accepted literature X-ray structures for these
compounds. To promote the wider adoption of 3DED, we make GiveMeED freely
available for use and modification, in support of greater uptake and
utilisation of structure solution procedures via electron diffraction.

</details>


### [520] [Dislocation-enhanced piezoelectric catalysis of KNbO3 crystal for water splitting](https://arxiv.org/abs/2507.10277)
*Hanyu Gong,Jiawen Zhang,Yan Zhao,Shan Xiang,Xiang Zhou,Oliver Preuß,Wenjun Lu,Yan Zhang,Xufei Fang*

Main category: cond-mat.mtrl-sci

TL;DR: 通过室温循环刻划在铌酸钾晶体中引入高密度位错，以增强压电催化产氢性能。


<details>
  <summary>Details</summary>
Motivation: 氧化物中具有离子/共价键的位错具有实现多功能性的潜力。

Method: 通过室温循环刻划在铌酸钾（KNbO3）晶体中引入高密度位错，以增强压电催化产氢。

Result: 所引入的位错诱导局部应变并改变电子环境，从而改善表面反应性和电荷分离，这对压电催化至关重要。

Conclusion: 这项工作证明了表面工程化的位错可以有效地改善压电催化性能，为高效、可扩展的压电催化应用铺平了道路。

Abstract: Dislocations in oxides with ionic/covalent bonding hold the potential of
harnessing versatile functionalities. Here, high-density dislocations in a
large plastic zone in potassium niobate (KNbO3) crystals are mechanically
introduced by room-temperature cyclic scratching to enhance piezocatalytic
hydrogen production. Unlike conventional energy-intensive, time-consuming
deformation at high temperature, this approach merits efficient dislocation
engineering. These dislocations induce local strain and modify the electronic
environment, thereby improving surface reactivity and charge separation, which
are critical for piezocatalysis. This proof-of-concept offers a practical and
sustainable alternative for functionalizing piezoelectric ceramics. Our
findings demonstrate that surface-engineered dislocations can effectively
improve the piezocatalysis, paving the way for efficient and scalable
piezocatalytic applications.

</details>


### [521] [High Resolution Temperature-Resolved Spectroscopy of the Nitrogen Vacancy $^{1}E$ Singlet State Ionization Energy](https://arxiv.org/abs/2507.10291)
*Kristine V. Ung,Connor A. Roncaioli,Ronald L. Walsworth,Sean M. Blakley*

Main category: cond-mat.mtrl-sci

TL;DR: 通过光致发光猝灭测量技术，本研究将金刚石NV$^-$中心的$^{1}E$单线态电离能精确到2.29-2.33 eV，不确定性降低一半。


<details>
  <summary>Details</summary>
Motivation: 尽管金刚石氮-空位（NV$^-$）中心在许多前沿量子传感应用中起着核心作用，但关于该系统的能级仍有许多未知之处。$^{1}E$单线态的电离能最近才被测量出来，但仍需进一步精确测量。

Method: 通过磁性介导的、自旋选择性的光致发光猝灭（PL）来测量$^{1}E$能量随激光波长和金刚石温度的变化，其中PL猝灭指示电离在哪个波长将引起从$^{1}E$到NV$^0$电荷态的布居转移。测量在450 nm至470 nm和540 nm至566 nm的激发波长范围内进行，步长为2 nm，并在约50 K至150 K的温度范围内进行，步长为5 K。

Result: 本研究确定了金刚石氮-空位（NV$^-$）中心的$^{1}E$单线态电离能为2.29至2.33 eV，将该量的测量不确定性降低了约一半。

Conclusion: 本研究将NV$^-$的$^{1}E$单线态电离能的测量不确定性降低了一半，确定其电离能为2.29至2.33 eV。

Abstract: The negatively charged diamond nitrogen-vacancy ($\mathrm{{NV}^-}$) center
plays a central role in many cutting edge quantum sensing applications; despite
this, much is still unknown about the energy levels in this system. The
ionization energy of the $\mathrm{^{1}E}$ singlet state in the
$\mathrm{{NV}^-}$ has only recently been measured at between 2.25 eV and 2.33
eV. In this work, we further refine this energy by measuring the
$\mathrm{^{1}E}$ energy as a function of laser wavelength and diamond
temperature via magnetically mediated spin-selective photoluminescence (PL)
quenching; this PL quenching indicating at what wavelength ionization induces
population transfer from the $\mathrm{^{1}E}$ into the neutral
$\mathrm{{NV}^0}$ charge configuration. Measurements are performed for
excitation wavelengths between 450 nm and 470 nm and between 540 nm and 566 nm
in increments of 2 nm, and for temperatures ranging from about 50 K to 150 K in
5 K increments. We determine the $\mathrm{^{1}E}$ ionization energy to be
between 2.29 and 2.33 eV, which provides about a two-fold reduction in
uncertainty of this quantity. Distribution level: A. Approved for public
release; distribution unlimited.

</details>


### [522] [High-throughput prediction of thermodynamically stable 1D magnetic transition-metal chalcogenides and halides](https://arxiv.org/abs/2507.10370)
*Canbo Zong,Deping Guo,Renhong Wang,Weihan Zhang,Jiaqi Dai,Zhongqin Zhang,Cong Wang,Xianghua Kong,Wei Ji*

Main category: cond-mat.mtrl-sci

TL;DR: 本研究通过高通量计算筛选出210种稳定的一维磁性材料，发现了具有新颖磁性、铁弹性和电荷密度波特性的材料，并预测了其在量子计算等领域的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 为了在纳米电子学和自旋电子学领域取得进展，寻找具有奇异物理性质的新型一维（1D）材料至关重要。

Method: 研究采用了高通量、第一性原理计算方法，结合热力学稳定性分析（通过形成能与竞争的二维相进行比较）和表示学习模型，对6,832个候选一维材料结构进行了系统性筛选。

Result: 研究筛选出210个稳定的一维磁性链，并揭示了化学计量和非金属元素的电子亲和力是决定一维稳定性的关键因素。所发现的材料展现出多样的磁序（FM, AFM）和Luttinger补偿反铁磁性（如MnTe），以及铁弹性（如FeTe具有巨大磁致伸缩）和电荷密度波（CDW）（如FeTe和NiSe）等新奇现象。

Conclusion: 该研究发现并验证了多种具有新颖磁性、铁弹性和电荷密度波（CDW）特性的新型一维（1D）过渡金属硫属化物和卤化物材料。其中，MnTe表现出Luttinger补偿反铁磁性，FeTe展示了-5.57%的巨大磁致伸缩效应，并出现了CDW特性。此外，研究还预测了在超导NbSe2衬底上的磁性CrCl2链中 Majorana 零模的实现，为量子应用提供了新的平台。

Abstract: The search for novel one-dimensional (1D) materials with exotic physical
properties is crucial for advancing nanoelectronics and spintronics. Here, we
perform a comprehensive high-throughput, first-principles study to explore the
vast landscape of 1D transition-metal chalcogenides and halides. Starting with
6,832 candidate structures derived from 28 metals and 8 non-metals, we
systematically evaluated their thermodynamic stability by comparing the
formation energies of 1D chains against competing 2D phases, mimicking
thermodynamic selectivity during nucleation. This screening identified 210
stable 1D magnetic chains. Furthermore, representation learning models revealed
that chemical stoichiometry and the electron affinity of the non-metal element
are key factors governing 1D stability. The stable materials exhibit a rich
spectrum of properties, including diverse magnetic orders (FM, AFM) and
Luttinger compensated antiferromagnetism in MnTe. We discovered 20 ferroelastic
chains, with FeTe showing a giant magnetostriction of -5.57%. Other emergent
phenomena include Charge Density Wave (CDW) chains in FeTe and NiSe. Finally,
our findings propose concrete platforms for quantum applications, such as the
predicted realization of Majorana zero modes in a ferromagnetic CrCl2 chain on
a superconducting NbSe2 substrate.

</details>


### [523] [Graphene Design with Parallel Cracks: Abnormal Crack Coalescence and Its Impact on Mechanical Properties](https://arxiv.org/abs/2507.10433)
*Suyeong Jin,Jung-Wuk Hong,Chiara Daraio,Alexandre F. Fonseca*

Main category: cond-mat.mtrl-sci

TL;DR: 本研究通过分子动力学模拟发现，裂纹间距影响石墨烯的力学性能和断裂行为。增加裂纹间距可提高石墨烯强度和韧性，并实现从脆性到韧性的转变。


<details>
  <summary>Details</summary>
Motivation: 尽管石墨烯在宏观应用中展现出巨大潜力，但其在生长和转移过程中产生的缺陷可能会影响其力学性能。然而，现有研究表明缺陷有时反而会增强石墨烯的力学性能，这引发了对缺陷相互作用及其对石墨烯性能影响的进一步探索需求，尤其是在大尺寸样品中。本研究旨在探究预存在裂纹的相互作用如何影响石墨烯的力学性能。

Method: 本研究采用分子动力学模拟的方法，分析了具有两条预存在裂纹的锯齿状和扶手椅状石墨烯在拉伸载荷下的行为。研究了裂纹之间的距离（W_gap）对石墨烯力学性能的影响，特别是裂纹合并的发生以及应力集中因子的变化。

Result: 研究发现，当两条裂纹之间的距离（W_gap）较小时，会出现裂纹合并现象，导致石墨烯强度下降。随着W_gap的增加，裂纹合并现象消失，强度和能量吸收增加，表现出从脆性到韧性的转变。提出的有效应力集中因子随W_gap的增加而呈现上升趋势。研究还提出了一个基于峰值应力与W_gap之间关系的初始裂纹几何设计指南。

Conclusion: 该研究提出了一个基于分子动力学模拟的框架，用于分析裂纹相互作用对石墨烯机械性能的影响。研究结果表明，裂纹的合并（当裂纹间距较小时）会降低石墨烯的强度，而随着裂纹间距的增加，石墨烯的强度和韧性会提高，并出现从脆性到韧性的转变。研究还提出了一种基于峰值应力与裂纹间距关联的设计指南。

Abstract: Graphene is a material with potential applications in electric, thermal, and
mechanical fields, and has seen significant advancements in growth methods that
facilitate large-scale production. However, defects during growth and transfer
to other substrates can compromise the integrity and strength of graphene.
Surprisingly, the literature suggests that, in certain cases, defects can
enhance or, at most, not affect the mechanical performance of graphene. Further
research is necessary to explore how defects interact within graphene structure
and affect its properties, especially in large-area samples. In this study, we
investigate the interaction between two preexisting cracks and their effect on
the mechanical properties of graphene using molecular dynamics simulations. The
behavior of zigzag and armchair graphene structures with cracks separated by
distances ($W_\text{gap}$) is analyzed under tensile loading. The findings
reveal that crack coalescence, defined as the formation of a new crack from two
existing crack tips, occurs for lower values of the distance between cracks,
$W_\text{gap}$, resulting in a decline in the strength of structures. As
$W_\text{gap}$ increases, the stress-strain curves shift upward, with the peak
stress rising in the absence of crack coalescence. The effective stress
intensity factor formulated in this study exhibits a clear upward trend with
increasing $W_\text{gap}$. Furthermore, an increase in $W_\text{gap}$ induces a
transition in fracture behavior from crack coalescence to independent
propagation with intercrack undulation. This shift in fracture behavior
demonstrates a brittle-to-ductile transition, as evidenced by increased energy
absorption and delayed failure. A design guideline for the initial crack
geometry is suggested by correlating peak stress with the $W_\text{gap}$,
within a certain range.

</details>


### [524] [X-raying Mg0.2Co0.2Ni0.2Cu0.2Zn0.2O: disentangling elemental contributions in a prototypical high-entropy oxide](https://arxiv.org/abs/2507.10428)
*Maryia Zinouyeva,Martina Fracchia,Giulia Maranini,Mauro Coduri,Davide Impelluso,Nicholas B. Brookes,Lorenzo Grilli,Kurt Kummer,Francesco Rosa,Matteo Aramini,Giacomo Ghiringhelli,Paolo Ghigna,Marco Moretti Sala*

Main category: cond-mat.mtrl-sci

TL;DR: 研究人员利用 X 射线技术分析了高熵氧化物，发现了铜位点处的 Jahn-Teller 畸变，并研究了不同磁性元素之间的相互作用。


<details>
  <summary>Details</summary>
Motivation: 为了区分高熵氧化物中各个化学物质对结构、电子和磁性的贡献。

Method: 采用 X 射线衍射、吸收和共振非弹性散射等多种 X 射线技术，以及磁激发和成对相互作用的估计。

Result: 在 Mg0.2Co0.2Ni0.2Cu0.2Zn0.2O 和相关体系中，研究人员明确解析了铜位点处显著的 Jahn-Teller 畸变，并且在 Ni2+ 和 Mg2+ 缺失时更为明显，这表明这些离子可以促进位置有序，而 Cu2+ 离子则会破坏其稳定性。 此外，还检测到了磁激发并估计了不同磁性元素成对之间的相互作用强度。

Conclusion: 高熵氧化物中的各种化学物质在塑造其物理特性方面发挥着重要作用。

Abstract: We employ several X-ray based techniques, including X-ray diffraction,
absorption and resonant inelastic scattering, to disentangle the contributions
of individual chemical species to the structural, electronic and magnetic
properties of high-entropy oxides. In the benchmark compound
Mg0.2Co0.2Ni0.2Cu0.2Zn0.2O and related systems, we unambiguously resolve a
sizable Jahn-Teller distortion at the Cu sites, more pronounced in the absence
of Ni2+ and Mg2+, suggesting that these ions promote positional order, whereas
Cu2+ ions act to destabilize it. Moreover, we detect magnetic excitations and
estimate the strength of the interactions between pairs of different magnetic
elements. Our results provide valuable insights into the role of the various
chemical species in shaping the physical properties of high-entropy oxides.

</details>


### [525] [Sustainable Pre-reduction of Ferromanganese Oxides with Hydrogen: Heating Rate-Dependent Reduction Pathways and Microstructure Evolution](https://arxiv.org/abs/2507.10451)
*Anurag Bajpai,Barak Ratzker,Shiv Shankar,Dierk Raabe,Yan Ma*

Main category: cond-mat.mtrl-sci

TL;DR: 加热速率影响锰矿石的氢基还原过程。


<details>
  <summary>Details</summary>
Motivation: 为了减少铁锰矿石到金属原料的生产过程中的碳排放和能源消耗，需要开发可持续的替代方案。本研究旨在了解氢基预还原过程中还原动力学和微观结构演变。

Method: 本研究结合了热重分析、X射线衍射和热力学计算，研究了加热速率对天然Nchwaning锰铁矿石和合成类似物进行氢基直接还原的影响。

Result: 研究发现，慢速加热可实现向MnO和金属Fe的完全转化，而快速加热则会促进Fe和MnO的相互混合。慢速加热可形成多孔MnO基体中分散的细小Fe颗粒，而快速加热则会导致零星的富铁聚集体。

Conclusion: 本研究表明，加热速率是控制锰矿石在氢基预还原过程中还原路径、相分布和微观结构演变的关键参数，为优化用于更高效和可持续的锰铁生产的氢基预还原策略提供了关键见解。

Abstract: The reduction of ferromanganese ores into metallic feedstock is an
energy-intensive process with substantial carbon emissions, necessitating
sustainable alternatives. Hydrogen-based pre-reduction of manganese-rich ores
offers a low-emission pathway to augment subsequent thermic Fe-Mn alloy
production. However, reduction dynamics and microstructure evolution under
varying thermal conditions remain poorly understood. This study investigates
the influence of heating rate on the hydrogen-based direct reduction of natural
Nchwaning ferromanganese ore and a synthetic analog. Non-isothermal
thermogravimetric analysis revealed a complex multistep reduction process with
overlapping kinetic regimes. Isoconversional kinetic analysis showed increased
activation energy with reduction degree, indicating a transition from
surface-reaction to diffusion-controlled reduction mechanisms. Interrupted
X-ray diffraction experiments suggested that slow heating enables complete
conversion to MnO and metallic Fe, while rapid heating promotes Fe- and
Mn-oxides intermixing. Thermodynamic calculations for the Fe-Mn-O system
predicted the equilibrium phase evolution, indicating Mn stabilized
Fe-containing spinel and halite phases. Microstructural analysis revealed that
slow heating rate yields fine and dispersed Fe particles in a porous MnO
matrix, while fast heating leads to sporadic Fe-rich agglomerates. These
findings suggest heating rate as a critical parameter governing reduction
pathway, phase distribution, and microstructure evolution, thus offering key
insights for optimizing hydrogen-based pre-reduction strategies towards more
efficient and sustainable ferromanganese production.

</details>


### [526] [Density Functional Theory Study of Th-doped LiCAF and LiSAF for Nuclear Clock Applications](https://arxiv.org/abs/2507.10526)
*Martin Pimon,Tobias Kirschbaum,Thorsten Schumm,Adriana Pálffy,Andreas Grüneis*

Main category: cond-mat.mtrl-sci

TL;DR: 密度泛函理论研究了Th:LiCAF和Th:LiSAF晶体中的钍缺陷，发现Th:LiSAF可能具有更高的钍浓度，并且先前文献结果的适用性有限。


<details>
  <summary>Details</summary>
Motivation: 理解钍在Th:LiCaAlF$_6$ and LiSrAlF$_6$复杂晶体结构中的原子排列。有助于为基于$^{229}$Th的8 eV跃迁的固态核钟寻找有希望的晶体。

Method: 使用密度泛函理论模拟系统地研究了Th:LiCaAlF$_6$ and LiSrAlF$_6$ (Th:LiCAF and Th:LiSAF)晶体，包括了温度相关的效应以及氟饱和和缺乏的环境条件，并计算了最低能量结构的电场梯度。

Result: 发现了20种不同的电荷补偿方案，并揭示了Th:LiSAF的缺陷形成能低于Th:LiCAF，这表明Th:LiSAF可能获得更高浓度的钍核。计算了最低能量结构的电场梯度，并指出先前文献报道的结果仅适用于部分实验条件。

Conclusion: Th:LiSAF可能比Th:LiCAF具有更高浓度的钍核，并且先前文献报道的结果仅适用于部分实验条件。

Abstract: Thorium-doped LiCaAlF$_6$ and LiSrAlF$_6$ (Th:LiCAF and Th:LiSAF) are
promising crystals for a solid-state nuclear clock based on the 8 eV transition
in $^{229}$Th; however, their complex crystal structures complicate
understanding the atomic arrangement of the thorium defects. In this work,
density functional theory simulations are employed to systematically
investigate these systems, including temperature-dependent effects and
environmental conditions of fluorine saturation and deficiency. We investigated
20 distinct charge compensation schemes for each material, revealing lower
defect formation energies in Th:LiSAF than in Th:LiCAF. This suggests that the
former may attain a higher concentration of thorium nuclei. Furthermore, we
calculated the electric field gradient for the lowest energy structure per
compensation pathway. Our investigation shows that results previously reported
in the literature apply only to a subset of experimental conditions.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [527] [LNN-powered Fluid Antenna Multiple Access](https://arxiv.org/abs/2507.08821)
*Pedro D. Alvim,Hugerles S. Silva,Ugo S. Dias,Osamah S. Badarneh,Felipe A. P. Figueiredo,Rausley A. A. de Souza*

Main category: eess.SP

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Fluid antenna systems represent an innovative approach in wireless
communication, recently applied in multiple access to optimize the
signal-to-interference-plus-noise ratio through port selection. This letter
frames the port selection problem as a multi-label classification task for the
first time, improving best-port selection with limited port observations. We
address this challenge by leveraging liquid neural networks (LNNs) to predict
the optimal port under emerging fluid antenna multiple access scenarios
alongside a more general $\alpha$-$\mu$ fading model. We also apply
hyperparameter optimization to refine LNN architectures for different
observation scenarios. Our approach yields lower outage probability values than
existing methods.

</details>


### [528] [Fundamental limits via CRB of semi-blind channel estimation in Massive MIMO systems](https://arxiv.org/abs/2507.08950)
*Xue Zhang,Abla Kammoun,Mohamed-Slim Alouini*

Main category: eess.SP

TL;DR: 大规模MIMO系统中，增加传输块长度可减小信道估计误差（CRB），但需训练序列长度同步增长且用户数不变；若训练序列数与用户数成正比，则误差有下限。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探讨大规模多输入多输出（MIMO）系统中半盲信道估计的渐进行为，并为信道估计误差的界限提供理论依据。

Method: 本文通过推导和分析确定性和随机性克拉美-罗界（CRB）在多种渐进情况下的数学表达式来研究其渐进行为，这些情况考虑了天线数量、用户数量、训练序列长度和传输块长度的增长率。

Result: 研究得出了确定性和随机性克拉美-罗界（CRB）在不同渐进情况下的解析表达式。结果显示，传输块长度的增加可以减小CRB，但前提是训练序列长度同步增长且用户数量固定。当训练序列数量与用户数量成比例增长时，信道估计误差存在一个恒定的下界。

Conclusion: 该研究表明，在用户数量固定且训练序列长度与传输块长度同步增长的情况下，可以通过增加传输块长度来减小克拉美-罗界（CRB）。然而，如果训练序列的数量与用户数量成正比增长，信道估计误差将有一个无法消除的下限。

Abstract: This paper investigates the asymptotic behavior of the deterministic and
stochastic Cram\'er-Rao Bounds (CRB) for semi-blind channel estimation in
massive multiple-input multiple-output (MIMO) systems. We derive and analyze
mathematically tractable expressions for both metrics under various asymptotic
regimes, which govern the growth rates of the number of antennas, the number of
users, the training sequence length, and the transmission block length. Unlike
the existing work, our results show that the CRB can be made arbitrarily small
as the transmission block length increases, but only when the training sequence
length grows at the same rate and the number of users remains fixed. However,
if the number of training sequences remains proportional to the number of
users, the channel estimation error is always lower-bounded by a non-vanishing
constant. Numerical results are presented to support our findings and
demonstrate the advantages of semi-blind channel estimation in reducing the
required number of training sequences.

</details>


### [529] [Domain Adaptation-Enabled Realistic Map-Based Channel Estimation for MIMO-OFDM](https://arxiv.org/abs/2507.08974)
*Thien Hieu Hoang,Tri Nhu Do,Georges Kaddoum*

Main category: eess.SP

TL;DR: 该研究提出了一种新的域适应方法，用于解决无线通信中的信道估计问题，该方法能够适应不同的信道模型，并在真实信道信息有限的情况下实现稳健的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的基于模型的方法在动态环境中存在困难，而基于机器学习的方法在不同的数据集之间缺乏泛化能力，因此需要提出一种新的域适应方法来解决这个问题。

Method: 提出了一种新的域适应方法来弥合准静态信道模型（QSCM）和基于地图的信道模型（MBCM）之间的差距。首先，提出了一个考虑了实际信道仿真的信道估计流程来训练基础模型。然后，提出了域适应方法来解决估计问题。

Result: 提出的策略能够实现稳健的模型性能，即使在真实信道信息有限的情况下也是如此。

Conclusion: 所提出的域适应方法能够有效地弥合准静态信道模型（QSCM）和基于地图的信道模型（MBCM）之间的差距，即使在真实信道信息有限的情况下也能实现稳健的模型性能。

Abstract: Accurate channel estimation is crucial for the improvement of signal
processing performance in wireless communications. However, traditional
model-based methods frequently experience difficulties in dynamic environments.
Similarly, alternative machine-learning approaches typically lack
generalization across different datasets due to variations in channel
characteristics. To address this issue, in this study, we propose a novel
domain adaptation approach to bridge the gap between the quasi-static channel
model (QSCM) and the map-based channel model (MBCM). Specifically, we first
proposed a channel estimation pipeline that takes into account realistic
channel simulation to train our foundation model. Then, we proposed domain
adaptation methods to address the estimation problem. Using simulation-based
training to reduce data requirements for effective application in practical
wireless environments, we find that the proposed strategy enables robust model
performance, even with limited true channel information.

</details>


### [530] [Hypergraph Overlapping Community Detection for Brain Networks](https://arxiv.org/abs/2507.08999)
*Duc Vu,Selin Aviyente*

Main category: eess.SP

TL;DR: 该研究提出了一种新的超图方法，用于分析fMRI数据中的高阶大脑连接，并检测重叠社区结构。


<details>
  <summary>Details</summary>
Motivation: 现有的功能连接网络（FCNs）在量化多个大脑区域之间的高阶依赖关系方面存在局限性，而超图构建方法虽然能捕捉高阶关系，但如何表征其拓扑结构仍是未公开的问题。

Method: 提出了一种基于谱聚类的超图方法来检测重叠社区结构。

Result: 成功构建了捕捉高阶依赖关系的大脑超图，并引入了基于谱聚类的超图方法来检测重叠社区结构。

Conclusion: 该研究提出了一种基于谱聚类的超图方法来检测大脑超网络的重叠社区结构，并成功应用于健康年轻成年人的静息态fMRI数据，以总结群体中的重叠社区结构。

Abstract: Functional magnetic resonance imaging (fMRI) has been commonly used to
construct functional connectivity networks (FCNs) of the human brain. TFCNs are
primarily limited to quantifying pairwise relationships between ROIs ignoring
higher order dependencies between multiple brain regions. Recently, hypergraph
construction methods from fMRI time series data have been proposed to
characterize the high-order relations among multiple ROIs. While there have
been multiple methods for constructing hypergraphs from fMRI time series, the
question of how to characterize the topology of these hypergraphs remains open.
In this paper, we make two key contributions to the field of community
detection in brain hypernetworks. First, we construct a hypergraph for each
subject capturing high order dependencies between regions. Second, we introduce
a spectral clustering based approach on hypergraphs to detect overlapping
community structure. Finally, the proposed method is implemented to detect the
consensus community structure across multiple subjects. The proposed method is
applied to resting state fMRI data from Human Connectome Project to summarize
the overlapping community structure across a group of healthy young adults.

</details>


### [531] [Time-Varying Offset Estimation for Clock-Asynchronous Bistatic ISAC Systems](https://arxiv.org/abs/2507.09215)
*Yi Wang,Keke Zu,Luping Xiang,Martin Haardt,Kun Yang*

Main category: eess.SP

TL;DR: 为解决无人机双站ISAC中的时钟异步问题，提出TVOE框架，利用视距路径特性进行同步，通过EKF估计和补偿时钟偏移，提高传感精度60%以上。


<details>
  <summary>Details</summary>
Motivation: 双站ISAC系统在同步方面面临时钟异步的挑战，这会导致时间和载波频率偏移（TO和CFO），影响传感精度。传统的同步方法（如依赖静态参考链路或GNSS）在无人机（UAV）部署的双站ISAC场景中通常不可靠或不可用。

Method: 提出了一种时变偏移估计（TVOE）框架，用于解决时钟异步双站ISAC系统中的时钟异步问题。该框架将LoS延迟和多普勒频移作为动态观测值，并将其演化建模为隐藏的随机过程。通过扩展卡尔曼滤波器（EKF）的状态空间公式来联合估计时间和载波频率偏移（TO和CFO）。

Result: 仿真结果表明，所提出的TVOE方法将估计精度提高了60%。

Conclusion: 该框架通过利用视距（LoS）路径的几何可预测特性，实现了鲁棒的、无需基础设施的同步，并能实时跟踪连续帧之间的时钟偏移，从而提高了高分辨率目标传感性能。

Abstract: The bistatic Integrated Sensing and Communication (ISAC) is poised to become
a key application for next generation communication networks (e.g., B5G/6G),
providing simultaneous sensing and communication services with minimal changes
to existing network infrastructure and hardware. However, a significant
challenge in bistatic cooperative sensing is clock asynchronism, arising from
the use of different clocks at far separated transmitters and receivers. This
asynchrony leads to Timing Offsets (TOs) and Carrier Frequency Offsets (CFOs),
potentially causing sensing ambiguity. Traditional synchronization methods
typically rely on static reference links or GNSS-based timing sources, both of
which are often unreliable or unavailable in UAVbased bistatic ISAC scenarios.
To overcome these limitations, we propose a Time-Varying Offset Estimation
(TVOE) framework tailored for clock-asynchronous bistatic ISAC systems, which
leverages the geometrically predictable characteristics of the Line-of-Sight
(LoS) path to enable robust, infrastructure-free
  synchronization. The framework treats the LoS delay and the Doppler shift as
dynamic observations and models their evolution as a hidden stochastic process.
A state-space formulation is developed to jointly estimate TO and CFO via an
Extended Kalman Filter (EKF), enabling real-time tracking of clock offsets
across successive frames. Furthermore, the estimated offsets are subsequently
applied to correct the timing misalignment of all Non-Line-of-Sight (NLoS)
components, thereby enhancing the high-resolution target sensing performance.
Extensive simulation results demonstrate that the proposed TVOE method improves
the estimation accuracy by 60%.

</details>


### [532] [Image Super-Resolution-Based Signal Enhancement in Bistatic ISAC](https://arxiv.org/abs/2507.09218)
*Yi Wang,Keke Zu,Luping Xiang,Martin Haardt,Chaochao Wang,Xianchao Zhang,Kun Yang*

Main category: eess.SP

TL;DR: 本文提出了一种基于图像超分辨率的信号增强（ISR-SE）框架，通过将信号时频分析结果映射为RGB图像，并利用UNet和扩散模型进行去噪，以提高双站ISAC系统在复杂环境下的感知精度。相较于传统方法，该框架能更好地处理低信噪比和复杂网络拓扑下的信号增强问题。


<details>
  <summary>Details</summary>
Motivation: 双站协同感知的一个主要挑战是感知精度下降，这主要是由于复杂环境中由高反射损耗引起且信号较弱。传统方法主要依赖自适应滤波技术，通过动态调整滤波器系数来提高信噪比。但这些方法在应对日益复杂和多样化的网络拓扑时，适应能力往往不足。

Method: 本文提出了一种新颖的基于图像超分辨率的信号增强（ISR-SE）框架。首先，通过应用短时傅里叶变换（STFT）对接收信号进行时频分析，生成捕获频率、幅度和相位分量的频谱图。然后，将这些分量映射为RGB图像，其中每个通道代表一个提取的特征，从而实现对信号结构更直观、信息更丰富的可视化。为了增强这些RGB图像，本文设计了一种改进的去噪网络，结合了UNet架构和扩散模型的优点。该混合架构利用UNet的多尺度特征提取能力和扩散模型的生成能力来进行有效的图像去噪，从而在低信噪比条件下提高信号表示的质量和清晰度。

Result: 本文提出的ISR-SE框架通过将信号映射为RGB图像，并结合UNet和扩散模型进行去噪，有效提升了低信噪比下的信号表示质量和清晰度，从而克服了传统方法在复杂网络拓扑下的适应性不足的问题，显著提高了ISAC信号的识别和恢复能力。

Conclusion: ISAC有望成为B5G和6G等下一代通信网络的核心技术，通过并行执行传感和通信功能，且无需对现有基础设施进行大规模改造。然而，双站协同感知的一个主要挑战是感知精度下降，这主要是由于复杂环境中由高反射损耗引起且信号较弱。传统方法主要依赖自适应滤波技术，通过动态调整滤波器系数来提高信噪比。但这些方法在应对日益复杂和多样化的网络拓扑时，适应能力往往不足。为应对这些挑战，本文提出了一种新颖的基于图像超分辨率的信号增强（ISR-SE）框架，以显著提升ISAC信号的识别和恢复能力。

Abstract: Bistatic Integrated Sensing and Communication (ISAC) is poised to become a
cornerstone technology in next-generation communication networks, such as
Beyond 5G (B5G) and 6G, by enabling the concurrent execution of sensing and
communication functions without requiring significant modifications to existing
infrastructure. Despite its promising potential, a major challenge in bistatic
cooperative sensing lies in the degradation of sensing accuracy, primarily
caused by the inherently weak received signals resulting from high reflection
losses in complex environments. Traditional methods have predominantly relied
on adaptive filtering techniques to enhance the Signal-to-Noise Ratio (SNR) by
dynamically adjusting the filter coefficients. However, these methods often
struggle to adapt effectively to the increasingly complex and diverse network
topologies. To address these challenges, we propose a novel Image
Super-Resolution-based Signal Enhancement (ISR-SE) framework that significantly
improves the recognition and recovery capabilities of ISAC signals.
Specifically, we first perform a time-frequency analysis by applying the
Short-Time Fourier Transform (STFT) to the received signals, generating
spectrograms that capture the frequency, magnitude, and phase components. These
components are then mapped into RGB images, where each channel represents one
of the extracted features, enabling a more intuitive and informative
visualization of the signal structure. To enhance these RGB images, we design
an improved denoising network that combines the strengths of the UNet
architecture and diffusion models. This hybrid architecture leverages UNet's
multi-scale feature extraction and the generative capacity of diffusion models
to perform effective image denoising, thereby improving the quality and clarity
of signal representations under low-SNR conditions.

</details>


### [533] [Deep Learning for sub-THz Radio Unit Selection using sub-10 GHz Channel Information and Inferred Device Beamforming](https://arxiv.org/abs/2507.09244)
*Nishant Gupta,Muris Sarajlic,Erik G. Larsson*

Main category: eess.SP

TL;DR: 通过深度学习根据子10 GHz信道特性推断子太赫兹RU，解决了现有方法的开销和功耗问题。


<details>
  <summary>Details</summary>
Motivation: 为了解决密集部署的子太赫兹RU在波束搜索或RU选择时产生的巨大开销和高功耗问题。

Method: 提出了一种利用深度学习的方法，根据子10 GHz信道特性从一组子太赫兹无线电单元（RU）中推断出合适的子太赫兹RU候选。

Result: 所提出的方法能够根据子10 GHz信道特性从一组子太赫兹RU中推断出合适的子太赫兹RU候选，即使在UE不共享其IBBC信息的情况下也能有效工作。

Conclusion: 仿真结果证明了所推断的次太赫兹无线电单元的性能，并突出了忽略用户设备方向对系统性能的不利影响。

Abstract: The dense and distributed deployment of sub-THz radio units (RUs) alongside
sub-10 GHz access point (AP) is a promising approach to provide high data rate
and reliable coverage for future 6G applications. However, beam search or RU
selection for the sub-THz RUs incurs significant overhead and high power
consumption. To address this, we introduce a method that leverages deep
learning to infer a suitable sub-THz RU candidate from a set of sub-THz RUs
using the sub-10 GHz channel characteristics. A novel aspect of this work is
the consideration of inter-band beam configuration (IBBC), defined as the
broadside angle between the low-band and high-band antenna patterns of the user
equipment (UE). Since IBBC indicates the beamforming information or UE's
orientation, it is typically not shared with the network as a part of
signalling. Therefore, we propose a solution strategy to infer a suitable
sub-THz RU even when UEs do not share their IBBC information. Simulation
results illustrate the performance of the inferred sub-THz RU and highlights
the detrimental impact of neglecting UE orientation on the systems performance.

</details>


### [534] [Matched Filtering-Based Channel Estimation for AFDM Systems in Doubly Selective Channels](https://arxiv.org/abs/2507.09268)
*Xiangjun Li,Zilong Liu,Zhengchun Zhou,Pingzhi Fan*

Main category: eess.SP

TL;DR: 本篇论文提出了一种新的AFDM信道估计方法，解决了路径模糊问题，降低了复杂度，并提升了通信性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决在分数延迟分数多普勒（FD-FD）信道中，路径模糊问题可能导致的严重性能恶化问题，并为AFDM系统提供一种低复杂度、高性能的信道估计方法。

Method: 通过推导完整的输入输出关系，在离散仿射傅里叶变换（DAFT）域研究了匹配滤波（MF）辅助信道估计（CE），并引入了基于广义斐波那契搜索（GFS）的MF-GFS方案来避免冗余计算。

Result: 提出的方案在通信性能和低复杂度方面均表现出优越性，仿真结果证实了其优势。

Conclusion: 该研究提出了一种增强型AFDM系统，并通过新颖的导频排列和匹配滤波（MF）辅助信道估计（CE）方案，有效解决了路径模糊问题，降低了系统复杂度，并提高了通信性能。

Abstract: Affine frequency division multiplexing (AFDM) has recently emerged as an
excellent backward-compatible 6G waveform. In this paper, an enhanced AFDM is
proposed whereby the delay-Doppler (DD) coupling phase is considered.
Specifically, we study matched filtering (MF) assisted channel estimation (CE)
for AFDM systems in complex doubly selective channels. By deriving the complete
input-output relationship, the inter-chirp-carrier interference,
signal-to-interference-plus-noise ratio (SINR), and the effective SINR loss of
AFDM, are investigated in discrete affine Fourier transform (DAFT) domain.
Further, we look into the path ambiguity problem and show that it may lead to
severe performance deterioration in fractional-delay fractional-Doppler
channels. To address such a problem, we introduce an MF assisted CE scheme
building upon a novel pilot arrangement across two consecutive AFDM
transmissions. This allows us to sequentially estimate the parameters of each
path by exploiting the separability and approximate orthogonality of different
paths in the DAFT domain, thus leading to significantly reduced complexity.
Furthermore, based on generalized Fibonacci search (GFS), an MF-GFS scheme is
proposed to avoid significantly redundant computation, which can be extended to
typical wide-band systems. Extensive simulation results indicate that the
proposed schemes offer superior advantages in terms of their improved
communication performance and lower complexity.

</details>


### [535] [Compute SNR-Optimal Analog-to-Digital Converters for Analog In-Memory Computing](https://arxiv.org/abs/2507.09776)
*Mihir Kavishwar,Naresh Shanbhag*

Main category: eess.SP

TL;DR: CACTUS算法通过优化ADC参数，在降低AIMC能耗的同时，提高了计算精度。


<details>
  <summary>Details</summary>
Motivation: 为了提高模拟内存计算（AIMC）的能效，需要降低ADC的精度以减少其能耗。然而，降低ADC精度会牺牲AIMC的计算精度，因此需要精确确定满足目标精度的最低ADC精度。

Method: 提出了一种计算信噪比（CSNR）的估计方法，并开发了CACTUS算法来获得CSNR最优的ADC参数。该方法考虑了量化误差、信噪比（SQNR）以及预ADC信号的离散特性。

Result: 在28nm CMOS工艺下，使用基于SRAM的AIMC电路感知行为模型进行仿真。结果显示，对于256维二元点积运算，CACTUS可降低ADC精度要求3位，并获得比现有方法高6dB的CSNR。同时，研究还明确了何种操作条件下，所提出的CSNR最优ADC优于传统的SQNR最优ADC。

Conclusion: 本研究通过开发用于估计计算信噪比（CSNR）的解析表达式，并提出CACTUS算法来优化模数转换器（ADC）参数，从而在降低ADC精度和保持计算精度的同时，解决了现有技术对ADC精度要求过高的问题。实验结果表明，与现有方法相比，CACTUS可降低3位ADC精度要求，并提高6dB的CSNR。

Abstract: Analog in-memory computing (AIMC) is an energy-efficient alternative to
digital architectures for accelerating machine learning and signal processing
workloads. However, its energy efficiency is limited by the high energy cost of
the column analog-to-digital converters (ADCs). Reducing the ADC precision is
an effective approach to lowering its energy cost. However, doing so also
reduces the AIMC's computational accuracy thereby making it critical to
identify the minimum precision required to meet a target accuracy. Prior works
overestimate the ADC precision requirements by modeling quantization error as
input-independent noise, maximizing the signal-to-quantization-noise ratio
(SQNR), and ignoring the discrete nature of ideal pre-ADC signal. We address
these limitations by developing analytical expressions for estimating the
compute signal-to-noise ratio (CSNR), a true metric of accuracy for AIMCs, and
propose CACTUS, an algorithm to obtain CSNR-optimal ADC parameters. Using a
circuit-aware behavioral model of an SRAM-based AIMC in a 28nm CMOS process, we
show that for a 256-dimensional binary dot product, CACTUS reduces the ADC
precision requirements by 3b while achieving 6dB higher CSNR over prior
methods. We also delineate operating conditions under which our proposed
CSNR-optimal ADCs outperform conventional SQNR-optimal ADCs.

</details>


### [536] [Free-running vs. Synchronous: Single-Photon Lidar for High-flux 3D Imaging](https://arxiv.org/abs/2507.09386)
*Ruangrawee Kitichotkul,Shashwath Bharadwaj,Joshua Rapp,Yanting Ma,Alexander Mehta,Vivek K Goyal*

Main category: eess.SP

TL;DR: 与同步系统相比，自由运行单光子激光雷达在精度上具有优势。本研究提出了一种基于直方图的联合最大似然估计器和正则化框架，以提高自由运行单光子激光雷达的精度。


<details>
  <summary>Details</summary>
Motivation: 尽管同步单光子激光雷达（SPL）系统有许多减轻死区时间效应的方法，但自由运行SPL的解决方案仍然有限，而自由运行SPL具有减少死区时间引起的直方图失真的优点。为了提高自由运行SPL的准确性，需要新的方法。

Method: 提出了一种计算高效的联合最大似然估计器，仅使用直方图来估计信号通量、背景通量和深度。此外，还提出了一个互补的正则化框架，该框架将学习到的点云评分模型作为先验。

Result: 仿真和实验表明，与同步单光子激光雷达相比，自由运行单光子激光雷达在相同条件下具有更低的估计误差，而提出的正则化框架进一步提高了精度。

Conclusion: 提出的方法通过联合最大似然估计器和正则化框架，利用直方图实现了对自由运行单光子激光雷达系统信号通量、背景通量和深度的估计，并通过学习到的点云评分模型作为先验来提高精度。仿真和实验结果表明，在相同条件下，自由运行单光子激光雷达的估计误差低于同步单光子激光雷达，并且所提出的正则化方法进一步提高了精度。

Abstract: Conventional wisdom suggests that single-photon lidar (SPL) should operate in
low-light conditions to minimize dead-time effects. Many methods have been
developed to mitigate these effects in synchronous SPL systems. However,
solutions for free-running SPL remain limited despite the advantage of reduced
histogram distortion from dead times. To improve the accuracy of free-running
SPL, we propose a computationally efficient joint maximum likelihood estimator
of the signal flux, the background flux, and the depth using only histograms,
along with a complementary regularization framework that incorporates a learned
point cloud score model as a prior. Simulations and experiments demonstrate
that free-running SPL yields lower estimation errors than its synchronous
counterpart under identical conditions, with our regularization further
improving accuracy.

</details>


### [537] [Lightweight Graph Neural Networks for Enhanced 5G NR Channel Estimation](https://arxiv.org/abs/2507.09408)
*Sajedeh Norouzi,Mostafa Rahmani,Yi Chu,Torsten Braun,Kaushik Chowdhury,Alister Burr*

Main category: eess.SP

TL;DR: GraphNet是一种创新的轻量级图神经网络，能够有效提高5G NR系统的信道估计精度，尤其是在动态环境中，其性能优于现有方法，并具有低计算复杂度和内置噪声估计的优势。


<details>
  <summary>Details</summary>
Motivation: 传统的信道估计算法在5G NR系统的动态环境中面临复杂性和适应性方面的挑战。因此，需要一种能够优化性能、降低计算复杂性并提高适应性的新方法。

Method: 本文提出了一种名为GraphNet的新型轻量级图神经网络（GNN）估计器，用于增强5G NR系统的信道估计（CE）。该方法利用GNN架构最大限度地减少了计算开销，同时捕获了准确CE所必需的关键特征，并集成了噪声估计功能以提高在挑战性信道条件下的鲁棒性。

Result: GraphNet在稳定和高变异信道条件下均表现出色，其性能可与ChannelNet相媲美，在高变异场景下甚至显著优于ChannelNet（尤其是在误块率方面）。此外，GraphNet显著降低了计算负荷，并具有内置的噪声估计功能，使其成为边缘设备的理想选择。

Conclusion: GraphNet通过其新颖的轻量级图神经网络（GNN）架构，在5G NR系统的信道估计（CE）方面展现出优越性能，尤其是在高动态环境下的误块率（BLER）方面，超越了现有的ChannelNet方法。其内置的噪声估计和显著降低的计算开销，使其非常适合在计算资源有限的边缘设备上进行实时部署，为下一代无线通信系统提供了可扩展且鲁棒的解决方案。

Abstract: Effective channel estimation CE is critical for optimizing the performance of
5G New Radio NR systems particularly in dynamic environments where traditional
methods struggle with complexity and adaptability This paper introduces
GraphNet a novel lightweight Graph Neural Network GNNbased estimator designed
to enhance CE in 5G NR Our proposed method utilizes a GNN architecture that
minimizes computational overhead while capturing essential features necessary
for accurate CE We evaluate GraphNet across various channel conditions from
slowvarying to highly dynamic environments and compare its performance to
ChannelNet a wellknown deep learningbased CE method GraphNet not only matches
ChannelNets performance in stable conditions but significantly outperforms it
in highvariation scenarios particularly in terms of Block Error Rate It also
includes builtin noise estimation that enhances robustness in challenging
channel conditions Furthermore its significantly lighter computational
footprint makes GraphNet highly suitable for realtime deployment especially on
edge devices with limited computational resources By underscoring the potential
of GNNs to transform CE processes GraphNet offers a scalable and robust
solution that aligns with the evolving demands of 5G technologies highlighting
its efficiency and performance as a nextgeneration solution for wireless
communication systems

</details>


### [538] [An Enregy Efficient Design of Hybrid NOMA Based on Hybrid SIC with Power Adaptation](https://arxiv.org/abs/2507.09458)
*Wang Ning,Zhang Chenyu,Sun Yanshi,Min Minghui,Liu Yuanwei,Li Shiyin*

Main category: eess.SP

TL;DR: 该论文提出了一种结合HSIC和PA的新型H-NOMA方案，在高信噪比下能够以概率1实现高于纯OMA的数据速率和更高的能效。


<details>
  <summary>Details</summary>
Motivation: 为了进一步释放混合非正交多址（H-NOMA）技术的潜力，研究者们提出了新的H-NOMA设计，旨在提高无线通信系统的性能。

Method: 提出了一种结合混合连续干扰消除（HSIC）和功率自适应（PA）的新型混合非正交多址（H-NOMA）设计，用于NOMA传输阶段。推导了H-NOMA实现高于纯OMA数据速率且能耗更低的概率的闭式表达式，并通过渐进分析证明了该方案在高信噪比下可以概率1地实现这一目标。

Result: 通过数值结果验证了所提出方案的分析准确性，并证明了其相比传统H-NOMA方案在能效方面的优越性。

Conclusion: 该研究提出的HSIC-PA辅助H-NOMA方案，在高信噪比下，能够以概率1实现超越纯OMA的更高数据速率和更高的能效，且无用户目标速率或发射功率比的限制，显著优于传统H-NOMA方案。

Abstract: Recently, hybrid non-orthogonal multiple access (H-NOMA) technology, which
effectively utilizes both NOMA and orthogonal multiple access (OMA)
technologies through flexible resource allocation in a single transmission, has
demonstrated immense potential for enhancing the performance of wireless
communication systems. To further release the potential of HNOMA, this paper
proposes a novel design of H-NOMA which jointly incorporates hybrid successive
interference cancellation (HSIC) and power adaptation (PA) in the NOMA
transmission phase. To reveal the potential of the proposed HSIC-PA aided
H-NOMA scheme, closed-form expression for the probability of the event that
H-NOMA can achieve a higher data rate than pure OMA by consuming less energy is
rigorously derived. Furthermore, the asymptotic analysis demonstrates that the
probability of the proposed H-NOMA scheme approaches 1 in the high
signal-to-noise ratio (SNR) regime without any constraints on either users'
target rates or transmit power ratios. This represents a significant
improvement over conventional H-NOMA schemes, which require specific
restrictive conditions to achieve probability 1 at high SNRs as shown in
existing work. The above observation indicates that with less energy
consumption, the proposed HSIC-PA aided H-NOMA can achieve a higher data rate
than pure OMA with probability 1 at high SNRs, and hence a higher energy
efficiency. Finally, numerical results are provided to verify the accuracy of
the analysis and also demonstrate the superior performance of the proposed
H-NOMA scheme.

</details>


### [539] [Reframing SAR Target Recognition as Visual Reasoning: A Chain-of-Thought Dataset with Multimodal LLMs](https://arxiv.org/abs/2507.09535)
*Chaoran Li,Xingguo Xu,Siyuan Mu*

Main category: eess.SP

TL;DR: 本研究开创性地将 SAR 目标识别视为多模态推理任务，利用 GPT-4o 和思维链（CoT）处理 SAR 图像。构建的新数据集和实验证明了 MLLMs 在此领域的潜力及其在 SAR 分析中的应用前景，同时也指出了其局限性。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统 SAR 图像识别方法在处理 SAR 数据固有局限性（如纹理弱、噪声高、目标边界模糊）方面的挑战。

Method: 将 SAR 目标识别重新表述为多模态推理任务，并利用 GPT-4o 等 MLLMs，结合候选类别和思维链（CoT）推理来执行 SAR 图像的目标分类。构建了一个基于 FAIR-CSAR 基准的新数据集，其中包含原始 SAR 图像、结构化目标注释、候选标签集和 GPT 生成的 CoT 推理链。

Result: 实验结果表明，MLLMs 在大多数情况下能够生成逻辑连贯且可解释的推理。分析突出了 MLLMs 在 SAR 图像解释方面的优势和局限性，并通过失败案例分析提供了对模型行为的详细见解。

Conclusion: 这项工作证明了将多模态大语言模型（MLLMs）集成到 SAR 分析流程中的可行性，并为未来 SAR 导向的视觉推理研究奠定了基础。

Abstract: In the context of Synthetic Aperture Radar (SAR) image recognition,
traditional methods often struggle with the intrinsic limitations of SAR data,
such as weak texture, high noise, and ambiguous object boundaries. This work
explores a novel perspective by reformulating SAR target recognition as a
multimodal reasoning task. We leverage multimodal large language models
(MLLMs), specifically GPT-4o, to perform target classification based on SAR
imagery, guided by candidate categories and enhanced with Chain-of-Thought
(CoT) reasoning. A new dataset is constructed based on the FAIR-CSAR benchmark,
comprising raw SAR images, structured target annotations, candidate label sets,
and GPT-generated CoT reasoning chains. Experimental results show that the
MLLMs are capable of generating logically coherent and interpretable inferences
in most scenarios. Our analysis highlights both the strengths and current
limitations of MLLMs in interpreting SAR imagery, and we provide detailed
insights into model behavior through failure case analysis. This work
demonstrates the feasibility of incorporating MLLMs into SAR analysis pipelines
and establishes a foundation for future research in SAR-oriented visual
reasoning.

</details>


### [540] [Novel Physics-Aware Attention-Based Machine Learning Approach for Mutual Coupling Modeling](https://arxiv.org/abs/2507.09561)
*Can Wang,Wei Liu,Hanzhi Ma,Xiaonan Jiang,Erping Li,Steven Gao*

Main category: eess.SP

TL;DR: 提出了一种物理感知长短期记忆（PC-LSTM）网络，用于高效准确地提取偶极子天线阵列的互阻抗矩阵。该方法通过重新解释格林函数并将其嵌入自适应损失函数中，提高了物理可解释性。注意力机制用于融合实部和虚部，然后由卷积长短期记忆网络处理以导出阻抗矩阵。该方法在五个基准测试中得到验证，与CST Microwave Studio相比，速度提高了7倍。


<details>
  <summary>Details</summary>
Motivation: 为了实现偶极子天线阵列中互阻抗矩阵的高效和准确提取，并增强互耦建模的物理可解释性。

Method: 提出了一种物理感知长短期记忆（PC-LSTM）网络，通过将物理感知神经网络重新解释格林函数并将其嵌入自适应损失函数中，并结合注意力机制来融合实部和虚部，最终推导出线性天线阵列的阻抗矩阵。

Result: 与五个基准相比，验证了该方法的有效性，证明了其在阻抗提取方面的准确性，并且速度比CST Microwave Studio快7倍。

Conclusion: 该方法实现了高效和准确的偶极子天线阵列互阻抗矩阵提取，并提供了增强的可解释性，与CST Microwave Studio相比，速度提高了7倍，是一种用于互耦特性的快速全波模拟替代方案。

Abstract: This article presents a physics-aware convolutional long short-term memory
(PC-LSTM) network for efficient and accurate extraction of mutual impedance
matrices in dipole antenna arrays. By reinterpreting the Green's function
through a physics-aware neural network and embedding it into an adaptive loss
function, the proposed machine learning-based approach achieves enhanced
physical interpretability in mutual coupling modeling. Also, an attention
mechanism is carefully designed to calibrate complex-valued features by fusing
the real and imaginary parts of the Green's function matrix. These fused
representations are then processed by a convolutional long short-term memory
network, and the impedance matrix of the linear antenna array can be finally
derived. Validation against five benchmarks underscores the efficacy of the
proposed approach, demonstrating accurate impedance extraction with up to a 7x
speedup compared to CST Microwave Studio, making it a fast alternative to
full-wave simulations for mutual coupling characterization.

</details>


### [541] [A New Wireless Image Transmission System Using Code Index Modulation and Image Enhancement for High-Rate Next Generation Networks](https://arxiv.org/abs/2507.09713)
*Burak Ahmet Ozden,Erdogan Aydin,Ahmet Elbir,Filiz Gurkan*

Main category: eess.SP

TL;DR: A new wireless image transmission system (CIM-IT) is proposed using code index modulation and QAM, outperforming traditional methods in key performance metrics.


<details>
  <summary>Details</summary>
Motivation: The increasing demand for high-resolution, high-rate, and reliable image transmission in multimedia, medical, and military applications drives the need for advanced wireless image transmission systems.

Method: The proposed system utilizes code index modulation (CIM) and quadrature amplitude modulation (QAM) to map bits to image pixels for transmission over a wireless channel. A despreading-based maximum likelihood detector is used at the receiver to estimate the code index and QAM symbol, followed by image reconstruction and enhancement using filters, including a proposed advanced filter.

Result: The paper evaluates the error performance, spectral efficiency, energy efficiency, and throughput of the CIM-IT system and compares it with traditional wireless communication techniques.

Conclusion: The paper proposes a CIM-IT system for wireless image transmission, demonstrating its performance in error rate, spectral efficiency, energy efficiency, and throughput compared to traditional methods.

Abstract: With the development of wireless network technologies, the wireless image
transmission area has become prominent. The need for high resolution, data
traffic density, widespread use of multimedia applications, and the importance
of high rate and reliable image transmission in medical and military fields
necessitate the design of novel and high-performance wireless image
transmission systems. This paper proposes a code index modulation (CIM)-based
image transmission (CIM-IT) system that utilizes spreading code index and
quadrature amplitude modulation (QAM) symbol for image transmission over a
wireless channel. The proposed CIM-IT system maps bits to each pixel value of
the image to be transmitted and transmits these bits over a wireless channel
using a single-input and multiple-output system comprising code index
modulation and QAM techniques. At the receiver, the active spreading code index
and the selected QAM symbol are estimated using a despreading-based maximum
likelihood detector, and the corresponding bits are obtained. The image
conveyed from the transmitter is then reconstructed at the receiver side using
the pixel values corresponding to the bits. The obtained noisy image is
enhanced using important enhancement filters. In addition, an advanced filter
is proposed to improve the transmitted degraded image with optimum results.
Furthermore, error performance, spectral efficiency, energy efficiency, and
throughputof the CIM-IT system are performed and the results are compared with
traditional wireless communication techniques.

</details>


### [542] [Precoded Zak-OTFS for Per-Carrier Equalization](https://arxiv.org/abs/2507.09894)
*Saif Khan Mohammed,Amit Kumar Pathak,Muhammad Ubadah,Ronny Hadani,Ananthanarayanan Chockalingam,Robert Calderbank*

Main category: eess.SP

TL;DR: 新预编码技术降低了 Zak-OTFS 调制在双 spread 信道中的均衡复杂性，提高了频谱效率。


<details>
  <summary>Details</summary>
Motivation: 该研究的动机是解决 Zak-OTFS 调制在双 spread 信道中的均衡复杂性问题，并提高频谱效率。

Method: 该研究提出了一种新颖的预编码技术，用于 Zak-OTFS 调制和双 spread 信道。该技术通过将双 spread 信道转换为一组独立的延迟-多普勒（DD）子信道来实现，从而简化了均衡过程。

Result: 提出的预编码技术可以将双 spread 信道转换为一组独立的 DD 子信道，从而降低均衡复杂度并减少保护间隔，显著提高了频谱效率。

Conclusion: Zak-OTFS调制与双 spread 信道提出了一种新的预编码技术，可以将双 spread 信道转换为一组独立的 DD 子信道，从而降低均衡复杂度并提高频谱效率。

Abstract: In Zak-OTFS (orthogonal time frequency space) modulation the carrier waveform
is a pulse in the delay-Doppler (DD) domain, formally a quasi-periodic
localized function with specific periods along delay and Doppler. When the
channel delay spread is less than the delay period, and the channel Doppler
spread is less than the Doppler period, the response to a single Zak-OTFS
carrier provides an image of the scattering environment and can be used to
predict the effective channel at all other carriers. The image of the
scattering environment changes slowly, making it possible to employ precoding
at the transmitter. Precoding techniques were developed more than thirty years
ago for wireline modem channels (V.34 standard) defined by linear convolution
where a pulse in the time domain (TD) is used to probe the one-dimensional
partial response channel. The action of a doubly spread channel on Zak-OTFS
modulation determines a two-dimensional partial response channel defined by
twisted convolution, and we develop a novel precoding technique for this
channel. The proposed precoder leads to separate equalization of each DD
carrier which has significantly lower complexity than joint equalization of all
carriers. Further, the effective precoded channel results in non-interfering DD
carriers which significantly reduces the overhead of guard carriers separating
data and pilot carriers, which improves the spectral efficiency significantly.

</details>


### [543] [AI-Enhanced Wide-Area Data Imaging via Massive Non-Orthogonal Direct Device-to-HAPS Transmission](https://arxiv.org/abs/2507.09895)
*Hyung-Joo Moon,Chan-Byoung Chae,Kai-Kit Wong,Robert W. Heath Jr*

Main category: eess.SP

TL;DR: MAP-X通过AI后处理技术（DNN或CNN）优化了高空伪卫星（HAPS）和分布式传感器的数据重建，实现了低延迟和高精度的物联网数据地图绘制。


<details>
  <summary>Details</summary>
Motivation: 旨在为需要对大范围分布的环境或工业测量数据进行实时重建的物联网应用提供一个创新的解决方案。

Method: 本文探讨了两种AI集成方法：基于DNN的点估计和基于CNN的图像重建，以及一个用于AI增强MAP-X的地面-高空平台（HAPS）协作框架。

Result: 与传统的IDFT方法相比，两种AI方法均显著提高了重建精度和效率，并且提出了一种地面-HAPS协作框架以支持AI增强的MAP-X。

Conclusion: MAP-X框架通过集成深度神经网络（DNN）或卷积神经网络（CNN）的AI后处理方法，能够显著优于传统的IDFT方法，为物联网应用提供低延迟的数据重建解决方案。

Abstract: Massive Aerial Processing for X MAP-X is an innovative framework for
reconstructing spatially correlated ground data, such as environmental or
industrial measurements distributed across a wide area, into data maps using a
single high altitude pseudo-satellite (HAPS) and a large number of distributed
sensors. With subframe-level data reconstruction, MAP-X provides a
transformative solution for latency-sensitive IoT applications. This article
explores two distinct approaches for AI integration in the post-processing
stage of MAP-X. The DNN-based pointwise estimation approach enables real-time,
adaptive reconstruction through online training, while the CNN-based image
reconstruction approach improves reconstruction accuracy through offline
training with non-real-time data. Simulation results show that both approaches
significantly outperform the conventional inverse discrete Fourier transform
(IDFT)-based linear post-processing method. Furthermore, to enable AI-enhanced
MAP-X, we propose a ground-HAPS cooperation framework, where terrestrial
stations collect, process, and relay training data to the HAPS. With its
enhanced capability in reconstructing field data, AI-enhanced MAP-X is
applicable to various real-world use cases, including disaster response and
network management.

</details>


### [544] [VoxelRF: Voxelized Radiance Field for Fast Wireless Channel Modeling](https://arxiv.org/abs/2507.09987)
*Zihang Zeng,Shu Sun,Meixia Tao,Yin Xu,Xianghao Yu*

Main category: eess.SP

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Wireless channel modeling in complex environments is crucial for wireless
communication system design and deployment. Traditional channel modeling
approaches face challenges in balancing accuracy, efficiency, and scalability,
while recent neural approaches such as neural radiance field (NeRF) suffer from
long training and slow inference. To tackle these challenges, we propose
voxelized radiance field (VoxelRF), a novel neural representation for wireless
channel modeling that enables fast and accurate synthesis of spatial spectra.
VoxelRF replaces the costly multilayer perception (MLP) used in NeRF-based
methods with trilinear interpolation of voxel grid-based representation, and
two shallow MLPs to model both propagation and transmitter-dependent effects.
To further accelerate training and improve generalization, we introduce
progressive learning, empty space skipping, and an additional background
entropy loss function. Experimental results demonstrate that VoxelRF achieves
competitive accuracy with significantly reduced computation and limited
training data, making it more practical for real-time and resource-constrained
wireless applications.

</details>


### [545] [Sparsity-Aware Extended Kalman Filter for Tracking Dynamic Graphs](https://arxiv.org/abs/2507.09999)
*Lital Dabush,Nir Shlezinger,Tirza Routtenberg*

Main category: eess.SP

TL;DR: 提出一种用于跟踪动态图拓扑的方法，该方法将动态性构建为基于图的非线性状态空间模型，并使用集成了ℓ1正则化更新的扩展卡尔曼滤波器进行跟踪。


<details>
  <summary>Details</summary>
Motivation: 跟踪动态图拓扑是图信号处理（GSP）中的一个基本挑战，具有广泛的应用，例如电力系统、脑机接口和通信系统。

Method: 通过将动态建模为基于图的非线性状态空间模型（SSM），并采用整合了ℓ1正则化更新的扩展卡尔曼滤波器（EKF）以及用于计算图滤波器雅可比矩阵的动态规划方案来跟踪动态图拓扑。

Result: 该方法能够准确地跟踪稀疏且时变的图，在具有高度非线性测量、各种噪声水平和不同变化率的实际条件下表现良好。

Conclusion: 该方法能够精确地跟踪稀疏且时变的图，具有低计算复杂度。

Abstract: A broad range of applications involve signals with irregular structures that
can be represented as a graph. As the underlying structures can change over
time, the tracking dynamic graph topologies from observed signals is a
fundamental challenge in graph signal processing (GSP), with applications in
various domains, such as power systems, the brain-machine interface, and
communication systems. In this paper, we propose a method for tracking dynamic
changes in graph topologies. Our approach builds on a representation of the
dynamics as a graph-based nonlinear state-space model (SSM), where the
observations are graph signals generated through graph filtering, and the
underlying evolving topology serves as the latent states. In our formulation,
the graph Laplacian matrix is parameterized using the incidence matrix and edge
weights, enabling a structured representation of the state. In order to track
the evolving topology in the resulting SSM, we develop a sparsity-aware
extended Kalman filter (EKF) that integrates $\ell_1$-regularized updates
within the filtering process. Furthermore, a dynamic programming scheme to
efficiently compute the Jacobian of the graph filter is introduced. Our
numerical study demonstrates the ability of the proposed method to accurately
track sparse and time-varying graphs under realistic conditions, with highly
nonlinear measurements, various noise levels, and different change rates, while
maintaining low computational complexity.

</details>


### [546] [Deep Learning-Based Beamforming Design Using Target Beam Patterns](https://arxiv.org/abs/2507.10063)
*Hongpu Zhang,Shu Sun,Hangsong Yan,Jianhua Mo*

Main category: eess.SP

TL;DR: 提出了一种深度学习框架，用于为数字、模拟和混合波束形成设计波束形成向量，以匹配目标波束图样，并在有限信道状态信息下实现了高性能。


<details>
  <summary>Details</summary>
Motivation: 为了在多种天线阵列架构（包括数字、模拟和混合波束形成）中，将目标波束图样直接映射到最优波束形成向量，并解决在多样化和有限的信道状态信息条件下的训练挑战。

Method: 提出了一种基于深度学习的波束形成设计框架，采用轻量级编码器-解码器网络，将目标波束图样映射到最优波束形成向量，并引入两阶段训练过程（离线预训练和在线训练）来解决不同信道状态信息条件下的训练挑战。

Result: 仿真结果表明，该方法在有限信道状态信息下实现了接近全数字波束形成的谱效率，并且优于现有的代表性方法。

Conclusion: 该方法在有限信道状态信息下实现了接近全数字波束形成的谱效率，并且优于现有的代表性方法。

Abstract: This paper proposes a deep learning-based beamforming design framework that
directly maps a target beam pattern to optimal beamforming vectors across
multiple antenna array architectures, including digital, analog, and hybrid
beamforming. The proposed method employs a lightweight encoder-decoder network
where the encoder compresses the complex beam pattern into a low-dimensional
feature vector and the decoder reconstructs the beamforming vector while
satisfying hardware constraints. To address training challenges under diverse
and limited channel station information (CSI) conditions, a two-stage training
process is introduced, which consists of an offline pre-training for robust
feature extraction using an auxiliary module, followed by online training of
the decoder with a composite loss function that ensures alignment between the
synthesized and target beam patterns in terms of the main lobe shape and side
lobe suppression. Simulation results based on NYUSIM-generated channels show
that the proposed method can achieve spectral efficiency close to that of fully
digital beamforming under limited CSI and outperforms representative existing
methods.

</details>


### [547] [Intrinsic frequency distribution characterises neural dynamics](https://arxiv.org/abs/2507.10145)
*Ryohei Fukuma,Yoshinobu Kawahara,Okito Yamashita,Kei Majima,Haruhiko Kishima,Takufumi Yanagisawa*

Main category: eess.SP

TL;DR: DMD频率分布比DFT幅度谱更能有效地区分神经系统疾病患者和健康人。


<details>
  <summary>Details</summary>
Motivation: 为了理解、预测和控制像大脑这样的非线性时空动力学系统，对具有某些基本动力学的多变量时间序列进行分解至关重要。现有的傅里叶变换方法在处理多通道信号和捕捉非平稳分量方面存在局限性。

Method: 动态模式分解（DMD）被提出用于分解多变量时间序列中的非线性时空动力学，提取其本征频率和衰减率。研究评估了健康受试者以及痴呆症或帕金森病患者在静息状态下脑电图（EEG）的DMD频率分布。

Result: 与离散傅里叶变换（DFT）产生的幅度谱相比，基于DMD频率分布能够以更高的准确性区分患者和健康受试者，表明DMD频率分布与幅度谱的行为不同，并可能成为表征神经动力学的新生物标志物。

Conclusion: 该研究表明，动态模式分解（DMD）产生的本征频率分布可以作为一种新的生物标志物，用于表征和区分健康受试者以及患有痴呆症或帕金森病（PD）的患者的神经活动。

Abstract: Decomposing multivariate time series with certain basic dynamics is crucial
for understanding, predicting and controlling nonlinear spatiotemporally
dynamic systems such as the brain. Dynamic mode decomposition (DMD) is a method
for decomposing nonlinear spatiotemporal dynamics into several basic dynamics
(dynamic modes; DMs) with intrinsic frequencies and decay rates. In particular,
unlike Fourier transform-based methods, which are used to decompose a
single-channel signal into the amplitudes of sinusoidal waves with discrete
frequencies at a regular interval, DMD can derive the intrinsic frequencies of
a multichannel signal on the basis of the available data; furthermore, it can
capture nonstationary components such as alternations between states with
different intrinsic frequencies. Here, we propose the use of the distribution
of intrinsic frequencies derived from DMDs (DM frequencies) to characterise
neural activities. The distributions of DM frequencies in the
electroencephalograms of healthy subjects and patients with dementia or
Parkinson's disease in a resting state were evaluated. By using the
distributions, these patients were distinguished from healthy subjects with
significantly greater accuracy than when using amplitude spectra derived by
discrete Fourier transform. This finding suggests that the distribution of DM
frequencies exhibits distinct behaviour from amplitude spectra, and therefore,
the distribution may serve as a new biomarker by characterising the nonlinear
spatiotemporal dynamics of electrophysiological signals.

</details>


### [548] [Pinching-Antenna Systems for Physical Layer Security](https://arxiv.org/abs/2507.10167)
*Kaidi Wang,Zhiguo Ding,Naofal Al-Dhahir*

Main category: eess.SP

TL;DR: Pinching antennas enhance security by adjusting amplitude and phase. A coalitional game model with a Shapley value-based algorithm improves performance over traditional methods.


<details>
  <summary>Details</summary>
Motivation: To investigate the potential of pinching-antenna systems for enhancing physical layer security by adjusting amplitude and phase to improve legitimate user signal quality and degrade eavesdropper signal quality.

Method: The study formulates a secrecy rate maximization problem, models antenna cooperation as a coalitional game, and proposes an antenna activation algorithm. It quantifies individual antenna impact using the Shapley value and marginal contribution.

Result: Simulation results demonstrate significant improvements in secrecy rate achieved by the pinching-antenna system, with the Shapley value-based algorithm showing superior performance compared to conventional methods.

Conclusion: The pinching-antenna system significantly improves secrecy rate, and the Shapley value-based algorithm outperforms conventional solutions.

Abstract: This letter investigates the potential of pinching-antenna systems for
enhancing physical layer security. By pre-installing multiple pinching antennas
at discrete positions along a waveguide, the capability of the considered
system to perform amplitude and phase adjustment is validated through the
formulation of a secrecy rate maximization problem. Specifically, amplitude
control is applied to enhance the signal quality at the legitimate user, while
phase alignment is designed to degrade the received signal quality at the
eavesdropper. This cooperation among pinching antennas is modeled as a
coalitional game, and a corresponding antenna activation algorithm is proposed.
The individual impact of each antenna is quantified based on the Shapley value
and marginal contribution, providing a fair and efficient method for
performance evaluation. Simulation results show that the considered
pinching-antenna system achieves significant improvements in secrecy rate, and
that the Shapley value based algorithm outperforms conventional coalition value
based solutions.

</details>


### [549] [Pinching-Antenna Systems with LoS Blockages](https://arxiv.org/abs/2507.10173)
*Kaidi Wang,Chongjun Ouyang,Yuanwei Liu,Zhiguo Ding*

Main category: eess.SP

TL;DR: 捏合天线系统可以动态建立视线（LoS）链路并利用视线（LoS）阻塞来减轻干扰，从而提高系统吞吐量。


<details>
  <summary>Details</summary>
Motivation: 本 letter 的目的是探索捏合天线系统在存在视线（LoS）阻塞的情况下构建视线（LoS）链路的能力。

Method: 提出了一种基于匹配的算法，并采用了两种不同的偏好设计来解决和优化波导分配和天线激活问题。

Result: 捏合天线被预先安装在波导上的预配置位置，并且可以选择性地激活以创建视线（LoS）链路以增强所需信号和非视线（NLoS）链路以消除用户间干扰。

Conclusion: 仿真结果表明，所考虑的捏合天线系统和提出的解决方案可以动态地建立视线（LoS）链路，并有效地利用视线（LoS）阻塞来减轻干扰，从而显著提高系统吞吐量。

Abstract: The aim of this letter is to explore the capability of pinching-antenna
systems to construct line-of-sight (LoS) links in the presence of LoS
blockages. Specifically, pinching antennas are pre-installed at preconfigured
positions along waveguides and can be selectively activated to create LoS links
for enhancing desired signals and non-line-of-sight (NLoS) links for
eliminating inter-user interference. On this basis, a sum-rate maximization
problem is formulated by jointly optimizing waveguide assignment and antenna
activation. To solve this problem, a matching based algorithm is proposed using
two distinct preference designs. Simulation results demonstrate that the
considered pinching-antenna system and proposed solutions can dynamically
establish LoS links and effectively exploit LoS blockages to mitigate
interference, thereby significantly improving system throughput.

</details>


### [550] [Enhanced Throughput and Seamless Handover Solutions for Urban 5G-Vehicle C-Band Integrated Satellite-Terrestrial Networks](https://arxiv.org/abs/2507.10308)
*Hung Nguyen-Kha,Vu Nguyen Ha,Eva Lagunas,Symeon Chatzinotas,Joel Grotz*

Main category: eess.SP

TL;DR: 该论文提出了一种用于5G集成卫星-陆地网络的优化方法，以提高城市环境中汽车用户的吞吐量和无缝切换能力。通过联合优化功率分配和用户关联，并使用SCA技术和预测算法来解决复杂的优化问题。仿真结果表明，该方法优于现有算法。


<details>
  <summary>Details</summary>
Motivation: 为了解决城市环境中5G集成卫星-陆地网络（ISTNs）中用户关联和资源分配的挑战，这些挑战是由密集的障碍物、用户移动性以及低地球轨道（LEO）卫星的动态移动和覆盖范围引起的。

Method: 提出了一种基于连续凸近似（SCA）技术的迭代算法，并引入了一种基于预测的实用算法。

Result: 仿真结果表明，与贪婪和基准算法相比，所提出的算法在和路界（SR）和连接次数（CC）方面表现更优。

Conclusion: 提出的算法在和路界和连接次数方面比贪婪和基准算法更有效。

Abstract: This paper investigates downlink transmission in 5G Integrated
Satellite-Terrestrial Networks (ISTNs) supporting automotive users (UEs) in
urban environments, where base stations (BSs) and Low Earth Orbit (LEO)
satellites (LSats) cooperate to serve moving UEs over shared C-band frequency
carriers. Urban settings, characterized by dense obstructions, together with UE
mobility, and the dynamic movement and coverage of LSats pose significant
challenges to user association and resource allocation. To address these
challenges, we formulate a multi-objective optimization problem designed to
improve both throughput and seamless handover (HO). Particularly, the
formulated problem balances sum-rate (SR) maximization and connection change
(CC) minimization through a weighted trade-off by jointly optimizing power
allocation and BS-UE/LSat-UE associations over a given time window. This is a
mixed-integer and non-convex problem which is inherently difficult to solve. To
solve this problem efficiently, we propose an iterative algorithm based on the
Successive Convex Approximation (SCA) technique. Furthermore, we introduce a
practical prediction-based algorithm capable of providing efficient solutions
in real-world implementations. Especially, the simulations use a realistic 3D
map of London and UE routes obtained from the Google Navigator application to
ensure practical examination. Thanks to these realistic data, the simulation
results can show valuable insights into the link budget assessment in urban
areas due to the impact of buildings on transmission links under the blockage,
reflection, and diffraction effects. Furthermore, the numerical results
demonstrate the effectiveness of our proposed algorithms in terms of SR and the
CC-number compared to the greedy and benchmark algorithms.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [551] [Efficient FRW Transitions via Stochastic Finite Differences for Handling Non-Stratified Dielectrics](https://arxiv.org/abs/2507.09730)
*Jiechen Huang,Wenjian Yu*

Main category: cs.AR

TL;DR: MicroWalk algorithm enables accurate FRW transitions for arbitrary dielectrics with high efficiency, offering significant accuracy advantages over existing FRW solvers.


<details>
  <summary>Details</summary>
Motivation: Advanced technology profiles, featuring complicated non-stratified dielectrics, challenge the accuracy of existing FRW transition schemes that approximate dielectrics with stratified or eight-octant patterns.

Method: The proposed algorithm MicroWalk enables accurate FRW transitions for arbitrary dielectrics while keeping high efficiency. It is provably unbiased and equivalent to using transition probabilities solved by finite difference method. An enhanced 3-D capacitance solver is developed with a hybrid strategy for complicated dielectrics, combining MicroWalk with the special treatment for the first transition cube and the analytical algorithm for stratified cubes.

Result: Our solver achieves a significant accuracy advantage over existing FRW solvers, while preserving high efficiency.

Conclusion: We propose an algorithm named MicroWalk, enabling accurate FRW transitions for arbitrary dielectrics while keeping high efficiency. An enhanced 3-D capacitance solver is developed with a hybrid strategy for complicated dielectrics, combining MicroWalk with the special treatment for the first transition cube and the analytical algorithm for stratified cubes. Experiments on real-world structures show that our solver achieves a significant accuracy advantage over existing FRW solvers, while preserving high efficiency.

Abstract: The accuracy of floating-random-walk (FRW) based capacitance extraction
stands only when the recursive FRW transitions are sampled unbiasedly according
to surrounding dielectrics. Advanced technology profiles, featuring complicated
non-stratified dielectrics, challenge the accuracy of existing FRW transition
schemes that approximate dielectrics with stratified or eight-octant patterns.
In this work, we propose an algorithm named MicroWalk, enabling accurate FRW
transitions for arbitrary dielectrics while keeping high efficiency. It is
provably unbiased and equivalent to using transition probabilities solved by
finite difference method, but at orders of magnitude lower cost (802$\times$
faster). An enhanced 3-D capacitance solver is developed with a hybrid strategy
for complicated dielectrics, combining MicroWalk with the special treatment for
the first transition cube and the analytical algorithm for stratified cubes.
Experiments on real-world structures show that our solver achieves a
significant accuracy advantage over existing FRW solvers, while preserving high
efficiency.

</details>


### [552] [CEO-DC: An Actionable Framework to Close the Carbon Gap in HPC Data Centers](https://arxiv.org/abs/2507.08923)
*Rubén Rodríguez Álvarez,Denisa-Andreea Constantinescu,Miguel Peón-Quirós,David Atienza*

Main category: cs.AR

TL;DR: CEO-DC模型通过优化数据中心的平台采购和更换策略来平衡成本、碳排放和计算需求。研究表明，虽然定期升级硬件有助于减排，但仅靠升级无法跟上需求增长，且需要经济激励措施来推广。当前碳价不足以在许多国家推动升级，且过度优化能效可能适得其反。因此，需要结合升级时机、增长约束、成本效益优化和激励措施来推动数据中心的可持续发展。


<details>
  <summary>Details</summary>
Motivation: 为了应对数据中心快速扩张导致能源消耗和温室气体排放不断增长的问题，并评估新硬件平台能否抵消不断增长的需求所带来的排放增加。

Method: 提出了一种名为CEO-DC的集成模型和决策方法，用于数据中心的碳和经济优化，考虑了成本、碳排放和计算需求之间的复杂权衡，并提出了指导采购、平台设计和政策决策的指标。

Result: 在AI案例研究中，CEO-DC模型显示，在4年周期内升级遗留设备可以减少总排放量。然而，如果不增加总排放量，这些升级无法满足数据中心需求的增长趋势，并且在72%的情况下需要经济激励措施才能被采用。此外，在14个数据中心数量最多的国家中，有9个国家的当前碳价不足以激励升级。将平台能效优化而牺牲延迟会增加其采用的合理性所需的碳价。

Conclusion: 数据中心平台升级和增长需要有针对性的政策和经济激励措施，以实现可持续发展。

Abstract: The rapid expansion of data centers (DCs) to support large-scale AI and
scientific workloads is driving unsustainable growth in energy consumption and
greenhouse gas emissions. While successive generations of hardware platforms
have improved performance and energy efficiency, the question remains whether
new, more efficient platforms can realistically offset the rising emissions
associated with increasing demand. Prior studies often overlook the complex
trade-offs in such transitions by failing to account for both the economic
incentives and the projected compute demand growth over the operational
lifetime of the devices. In response, we present CEO-DC, an integrated model
and decision-making methodology for Carbon and Economy Optimization in Data
Centers. CEO-DC models the competing forces of cost, carbon, and compute demand
to guide optimal platform procurement and replacement strategies. We propose
metrics to steer procurement, platform design, and policy decisions toward
sustainable DC technologies. Given current platform trends, our AI case study
using CEO-DC shows that upgrading legacy devices on a 4-year cycle reduces
total emissions. However, these upgrades fail to scale with DC demand growth
trends without increasing total emissions in over 44% of cases, and require
economic incentives for adoption in over 72%. Furthermore, current carbon
prices are insufficient to motivate upgrades in 9 out of the 14 countries with
the highest number of DCs globally. We also find that optimizing platforms for
energy efficiency at the expense of latency can increase the carbon price
required to justify their adoption. In summary, CEO-DC provides actionable
insights for DC architects, platform designers, and policymakers by timing
legacy platform upgrades, constraining DC growth to sustainable levels,
optimizing platform performance-to-cost ratios, and increasing incentives.

</details>


### [553] [Hybrid Systolic Array Accelerator with Optimized Dataflow for Edge Large Language Model Inference](https://arxiv.org/abs/2507.09010)
*Chun-Ting Chen,HanGyeol Mun,Jian Meng,Mohamed S. Abdelfattah,Jae-sun Seo*

Main category: cs.AR

TL;DR: 针对边缘LLM推理，提出一种混合结构（HSA）加速器，采用MXINT4量化和优化数据流，在满足DRAM带宽限制下实现高利用率和低精度损失，并集成优化RMSNorm和RoPE单元，显著提升性能和能效。


<details>
  <summary>Details</summary>
Motivation: 为了实现安全、低延迟和低成本的边缘LLM推理，需要设计一种既能满足面积效率和最小化外部内存访问（EMA）的内存瓶颈解码阶段，又能保持高能效的计算密集型预填充阶段的边缘加速器。

Method: 本文提出了一种采用混合结构（HSA）的边缘LLM推理加速器，该加速器通过MXINT4权重量化和优化的数据流来减少外部内存访问（EMA），以提高内存密集型解码阶段的效率，并在计算密集型预填充阶段保持高能效。同时，集成了优化的RMSNorm和RoPE单元以降低非线性运算的延迟、面积和内存访问开销。

Result: 在运行1.3B LLM处理长输入/输出场景时，该加速器实现了247/117 (token/s/mm2) 的性能，相较于现有方法有超过2.45倍/13.5倍的提升，并在生成token时保持了卓越的能效。

Conclusion: 该研究提出了一个创新的混合结构（HSA）的边缘LLM推理加速器，通过MXINT4量化和优化的数据流，在满足边缘DRAM带宽限制的同时，实现了高硬件利用率和可忽略的精度损失，并结合优化的RMSNorm和RoPE单元，显著降低了延迟、面积和内存访问开销，最终在处理长输入/输出场景时，相较于现有方法，在1.3B LLM上实现了2.45倍/13.5倍的性能提升，同时保持了优越的能效。

Abstract: Edge inference for large language models (LLM) offers secure, low-latency,
and cost-effective inference solutions. We emphasize that an edge accelerator
should achieve high area efficiency and minimize external memory access (EMA)
during the memory-bound decode stage, while maintaining high energy efficiency
during the compute intensive prefill stage. This paper proposes an edge LLM
inference accelerator featuring a hybrid systolic array (HSA) architecture that
optimizes inference efficiency in both stages. To further reduce EMA, we adopt
MXINT4 weight quantization and propose an optimized dataflow tailored for HSA,
ensuring negligible dequantization overhead and achieving 100% hardware
utilization with minimal accuracy loss under edge DRAM bandwidth constraints.
For non-linear operations, we incorporate optimized root mean square
normalization (RMSNorm) and rotary position embedding (RoPE) units, reducing
their latency, area, and memory access overhead while enabling end-to-end
inference on our accelerator. Our solution achieves 247/117 (token/s/mm2) while
running a 1.3B LLM on long-input/long-output scenarios, providing >2.45x/13.5x
improvement over existing approaches, while maintaining superior energy
efficiency in token generation.

</details>


### [554] [SLIM: A Heterogeneous Accelerator for Edge Inference of Sparse Large Language Model via Adaptive Thresholding](https://arxiv.org/abs/2507.09201)
*Weihong Xu,Haein Choi,Po-kai Hsu,Shimeng Yu,Tajana Rosing*

Main category: cs.AR

TL;DR: SLIM通过利用LLM稀疏性和创新的硬件设计（结合NSP和PIM），在边缘设备上实现了LLM的高效推理，显著提升了性能和能效。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型（LLM）在资源受限的嵌入式设备上进行高效推理的挑战，特别是模型尺寸大以及前馈网络（FFN）和多头注意力（MHA）层内存密集型操作的问题。现有加速器未能充分利用LLM操作中固有的稀疏性，导致硬件资源利用率低下。

Method: 提出了一种名为SLIM的算法-硬件协同设计方案。该方案利用自适应阈值算法挖掘LLM的稀疏性，实现运行时可配置的稀疏性，并仅读取激活的神经元以减少数据移动。硬件架构结合了近存储处理（NSP）和内存处理（PIM），其中FFN权重存储在3D NAND中使用NSP单元计算，而内存密集型的MHA操作则在PIM模块中处理。

Result: SLIM实现了13-18倍于SSD-GPU系统的吞吐量提升，以及9-10倍于DRAM-GPU系统的能效提升，同时保持了低延迟，使得经济高效的LLM部署在边缘计算环境中成为可能。

Conclusion: SLIM通过算法-硬件协同设计，在边缘设备上实现了稀疏大语言模型（LLM）的高效推理，与现有系统相比，在吞吐量和能效方面均有显著提升，使得LLM在边缘计算环境中的部署成为可能。

Abstract: Large language models (LLMs) have demonstrated exceptional proficiency in
understanding and generating human language, but efficient inference on
resource-constrained embedded devices remains challenging due to large model
sizes and memory-intensive operations in feedforward network (FFN) and
multi-head attention (MHA) layers. While existing accelerators offload LLM
inference to expensive heterogeneous computing systems, they fail to exploit
the significant sparsity inherent in LLM operations, leaving hardware resources
underutilized. We propose SLIM, an algorithm-hardware co-design optimized for
sparse LLM serving on edge devices. SLIM exploits LLM sparsity through an
adaptive thresholding algorithm that enables runtime-configurable sparsity with
negligible accuracy loss, fetching only activated neurons to dramatically
reduce data movement. Our heterogeneous hardware architecture strategically
combines near-storage processing (NSP) and processing-in-memory (PIM): FFN
weights are stored in high-density 3D NAND and computed using NSP units, while
memory-intensive MHA operations are processed in PIM modules. This design
significantly reduces memory footprint, data movement, and energy consumption.
Our comprehensive evaluation demonstrates SLIM's effectiveness, achieving
13-18x throughput improvements over SSD-GPU systems and 9-10x better energy
efficiency over DRAM-GPU systems while maintaining low latency, making
cost-effective LLM deployment viable for edge computing environments.

</details>


### [555] [Tools and Methodologies for System-Level Design](https://arxiv.org/abs/2507.09660)
*Shuvra S. Bhattacharyya,Marilyn Wolf*

Main category: cs.AR

TL;DR: 随着SoC设计的复杂性增加，系统级设计变得至关重要。本章通过视频和神经网络的例子，介绍了用于建模、仿真、设计空间探索和设计的工具和方法论，以及计算模型的重要性。


<details>
  <summary>Details</summary>
Motivation: 随着片上系统（SoC）设计的复杂性增加，系统级设计已成为芯片设计中的核心问题，需要比以往更广泛的工具集和方法论。

Method: 通过举例说明系统级设计中的模型、仿真、设计空间探索和设计验证等工具和方法论，并着重介绍了计算模型在描述数字系统中的作用以及不同计算模型之间通信的可靠性。

Result: 该章节通过视频和神经网络两个应用案例，阐述了系统级设计中的关键工具和方法，强调了模型、仿真、设计空间探索和设计验证的重要性。

Conclusion: 系统级设计在视频和神经网络等应用中有很多重要的方面需要说明。

Abstract: System-level design, once the province of board designers, has now become a
central concern for chip designers. Because chip design is a less forgiving
design medium -- design cycles are longer and mistakes are harder to correct --
system-on-chip designers need a more extensive tool suite than may be used by
board designers and a variety of tools and methodologies have been developed
for system-level design of systems-on-chips (SoCs). System-level design is less
amenable to synthesis than are logic or physical design. As a result,
system-level tools concentrate on modeling, simulation, design space
exploration, and design verification. The goal of modeling is to correctly
capture the system's operational semantics, which helps with both
implementation and verification. The study of models of computation provides a
framework for the description of digital systems. Not only do we need to
understand a particular style of computation, such as dataflow, but we also
need to understand how different models of computation can reliably communicate
with each other. Design space exploration tools, such as hardware/software
co-design, develop candidate designs to understand trade-offs. Simulation can
be used not only to verify functional correctness but also to supply
performance and power/energy information for design analysis. This chapter
employs two applications -- video and neural networks -- as examples. Both are
leading-edge applications that illustrate many important aspects of
system-level design.

</details>


### [556] [Low-Cost Fuel Dispenser Prototype Using STM32 and an H-bridge motor driver](https://arxiv.org/abs/2507.09774)
*MD Zobaer Hossain Bhuiyan,Abir Bin Faruque,Mahtab Newaz,Mohammad Abdul Qayum*

Main category: cs.AR

TL;DR: 开发了一个基于STM32和L298N的低成本燃油分配系统原型，用于远程或小规模环境。


<details>
  <summary>Details</summary>
Motivation: 该系统旨在为传统高成本系统不可行的远程或小规模环境提供负担得起的、可扩展的燃油分配解决方案。

Method: 该系统采用STM32微控制器作为核心控制单元，通过I2C通信管理16x4 LCD屏幕的用户输入和操作数据显示。L298N电机驱动器用于精确控制12V直流泵电机以模拟燃油分配机制。使用4x4矩阵键盘进行用户输入。

Result: 该项目成功设计并开发了一个基于STM32微控制器和L298N电机驱动器的低成本燃油分配系统原型。用户可以通过键盘输入所需的燃油量，系统通过精确控制电机运行时间来确保燃油量的准确性。

Conclusion: 该项目展示了如何利用嵌入式系统构建经济高效、用户友好且节能的解决方案，可用于加油站或农业用途。

Abstract: This paper presents the design and development of a low-cost fuel dispensing
system prototype based on the STM32 microcontroller and L298N motor driver. The
system aims to provide an affordable and scalable solution for fuel delivery in
remote or small-scale environments where conventional, high-cost systems are
not feasible. The core control unit is built using an STM32 microcontroller,
which manages user input through a 4x4 matrix keypad and displays operational
data on a 16x4 LCD screen via I2C communication. A 12V DC pump motor is used to
simulate the fuel dispensing mechanism, precisely controlled via the dual
H-bridge L298N motor driver. The system is powered by a 11.1V battery and is
designed for ease of deployment and portability. The keypad allows users to
input the desired fuel amount, while the system ensures accurate motor runtime
corresponding to the volume to be dispensed. This project demonstrates how
embedded systems can be leveraged to build cost-effective, user-friendly, and
energy-efficient solutions. The proposed design can be further enhanced with
flow sensors, GSM connectivity, RFID cards, and payment integration for
real-world applications in fuel stations or agricultural use.

</details>


### [557] [BitParticle: Partializing Sparse Dual-Factors to Build Quasi-Synchronizing MAC Arrays for Energy-efficient DNNs](https://arxiv.org/abs/2507.09780)
*Feilong Qiaoyuan,Jihe Wang,Zhiyu Sun,Linying Wu,Yuanhua Xiao,Danghui Wang*

Main category: cs.AR

TL;DR: 本文提出了一种新颖的MAC单元和准同步方案，以解决量化DNN中的位级稀疏性利用问题，显著提高了面积和能源效率。


<details>
  <summary>Details</summary>
Motivation: 量化深度神经网络（DNN）中的位级稀疏性在优化乘加（MAC）运算方面具有巨大潜力，但实际应用中存在双因子稀疏性利用不充分和MAC运算周期数易变导致MAC单元利用率低的问题。

Method: 本文提出了一种利用新兴的粒子化方法来利用双因子稀疏性的MAC单元，通过简单的控制逻辑解决了部分积爆炸问题，并引入了一种准同步方案，为MAC阵列增加了周期级弹性，以解决位级稀疏性的挑战。

Result: 所提出的MAC阵列架构的精确版本在面积效率方面比最先进的位稀疏驱动架构提高了29.2%，同时保持了相当的能源效率。近似变体与精确版本相比，能源效率进一步提高了7.5%。

Conclusion: Bit-level稀疏性在量化深度神经网络（DNN）中具有优化乘加（MAC）运算的巨大潜力。然而，实际应用仍面临两大挑战：一是传统位串行方法无法同时利用两个因子（乘数和被乘数）的稀疏性，导致其中一个因子的稀疏性被浪费，而旨在利用双因子稀疏性的方法仍处于探索初期，面临部分积爆炸的挑战；二是位级稀疏性的波动导致MAC运算的周期数可变，现有的适用于双因子稀疏性的同步调度方案灵活性差，MAC单元利用率低。为了解决第一个挑战，本文提出了一种利用新兴的粒子化方法来利用双因子稀疏性的MAC单元，通过简单的控制逻辑解决了部分积爆炸问题，从而实现了更具面积和能源效益的MAC单元，并通过丢弃不太重要的中间结果，以微小的精度损失为代价实现了进一步的硬件简化。为了解决第二个挑战，本文引入了一种准同步方案，为MAC阵列增加了周期级弹性，减少了流水线停顿，从而提高了MAC单元的利用率。评估结果表明，所提出的MAC阵列架构的精确版本在面积效率方面比最先进的位稀疏驱动架构提高了29.2%，同时保持了相当的能源效率。近似变体与精确版本相比，能源效率进一步提高了7.5%。

Abstract: Bit-level sparsity in quantized deep neural networks (DNNs) offers
significant potential for optimizing Multiply-Accumulate (MAC) operations.
However, two key challenges still limit its practical exploitation. First,
conventional bit-serial approaches cannot simultaneously leverage the sparsity
of both factors, leading to a complete waste of one factor' s sparsity. Methods
designed to exploit dual-factor sparsity are still in the early stages of
exploration, facing the challenge of partial product explosion. Second, the
fluctuation of bit-level sparsity leads to variable cycle counts for MAC
operations. Existing synchronous scheduling schemes that are suitable for
dual-factor sparsity exhibit poor flexibility and still result in significant
underutilization of MAC units. To address the first challenge, this study
proposes a MAC unit that leverages dual-factor sparsity through the emerging
particlization-based approach. The proposed design addresses the issue of
partial product explosion through simple control logic, resulting in a more
area- and energy-efficient MAC unit. In addition, by discarding less
significant intermediate results, the design allows for further hardware
simplification at the cost of minor accuracy loss. To address the second
challenge, a quasi-synchronous scheme is introduced that adds cycle-level
elasticity to the MAC array, reducing pipeline stalls and thereby improving MAC
unit utilization. Evaluation results show that the exact version of the
proposed MAC array architecture achieves a 29.2% improvement in area efficiency
compared to the state-of-the-art bit-sparsity-driven architecture, while
maintaining comparable energy efficiency. The approximate variant further
improves energy efficiency by 7.5%, compared to the exact version. Index-Terms:
DNN acceleration, Bit-level sparsity, MAC unit

</details>


### [558] [Pimba: A Processing-in-Memory Acceleration for Post-Transformer Large Language Model Serving](https://arxiv.org/abs/2507.10178)
*Wonung Kim,Yubin Lee,Yoonsung Kim,Jinwoo Hwang,Seongryong Oh,Jiyong Jung,Aziz Huseynov,Woong Gyu Park,Chang Hyun Park,Divya Mahajan,Jongse Park*

Main category: cs.AR

TL;DR: Pimba 系统通过优化硬件设计，提高了 Transformer 和后 Transformer LLM 的推理性能。


<details>
  <summary>Details</summary>
Motivation: Transformer 模型在处理长序列时存在计算和内存成本的挑战，需要一个统一的框架来支持 Transformer 和后 Transformer LLM 的高效推理。

Method: 本文分析了 Transformer 和后 Transformer LLM 的性能特征，发现两者在批处理推理中都受限于内存带宽。研究还发现状态更新操作硬件成本高，且不同的低精度算术方法在精度和面积之间有不同的权衡，其中微软的 MX 是最优选择。基于这些发现，设计了 Pimba 系统，它包含状态更新处理单元（SPU）和状态更新处理引擎（SPE），利用 MX 量化算术实现高效的状态更新和注意力操作。

Result: 与 LLM 优化的 GPU 和 GPU+PIM 系统相比，Pimba 在令牌生成吞吐量方面分别实现了高达 3.2 倍和 2.1 倍的提升。

Conclusion: Transformer 模型在处理长序列时存在计算和内存成本的挑战，而状态空间模型（SSMs）、线性注意力、循环神经网络（RNNs）等后 Transformer 模型则提供了替代方案。Pimba 系统通过其状态更新处理单元（SPU）和状态更新处理引擎（SPE），利用微软的 MX 量化算术，实现了对 Transformer 和后 Transformer LLM 的统一高效支持，在内存带宽受限的批处理推理中表现出色，与 LLM 优化的 GPU 和 GPU+PIM 系统相比，令牌生成吞吐量分别提高了 3.2 倍和 2.1 倍。

Abstract: Transformers are the driving force behind today's Large Language Models
(LLMs), serving as the foundation for their performance and versatility. Yet,
their compute and memory costs grow with sequence length, posing scalability
challenges for long-context inferencing. In response, the algorithm community
is exploring alternative architectures, such as state space models (SSMs),
linear attention, and recurrent neural networks (RNNs), which we refer to as
post-transformers. This shift presents a key challenge: building a serving
system that efficiently supports both transformer and post-transformer LLMs
within a unified framework. To address this challenge, we analyze the
performance characteristics of transformer and post-transformer LLMs. Despite
their algorithmic differences, both are fundamentally limited by memory
bandwidth under batched inference due to attention in transformers and state
updates in post-transformers. Further analyses suggest two additional insights:
(1) state update operations, unlike attention, incur high hardware cost, making
per-bank PIM acceleration inefficient, and (2) different low-precision
arithmetic methods offer varying accuracy-area tradeoffs, while we identify
Microsoft's MX as the Pareto-optimal choice. Building on these insights, we
design Pimba as an array of State-update Processing Units (SPUs), each shared
between two banks to enable interleaved access to PIM. Each SPU includes a
State-update Processing Engine (SPE) that comprises element-wise multipliers
and adders using MX-based quantized arithmetic, enabling efficient execution of
state update and attention operations. Our evaluation shows that, compared to
LLM-optimized GPU and GPU+PIM systems, Pimba achieves up to 3.2x and 2.1x
higher token generation throughput, respectively.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [559] [Agent-based visualization of streaming text](https://arxiv.org/abs/2507.08884)
*Jordan Riley Benson,David Crist,Phil Lafleur,Benjamin Watson*

Main category: cs.MA

TL;DR: 提出一种代理行为模型的可视化基础架构，用于动态可视化文本数据流中的单词共现关系。


<details>
  <summary>Details</summary>
Motivation: 为文本流数据提供一种动态且直观的可视化方法，揭示数据中的主要话题和共现关系。

Method: 提出了一种将数据元素映射到具有行为代理的可视化基础架构，代理的行为由数据元素参数化。代理通过最小化显示代理间距离与理想距离矩阵之间的差异来移动。对于文本流的可视化，代理代表单词，其大小由单词频率决定，单词的共现关系决定了理想距离矩阵，共现度越高的单词距离越近。代理的圆形区域与单词共现率之间的比例近似于单词共现率与频率之间的比率。

Result: 展示了如何使用该基础架构可视化流式文本数据，揭示了文本流中的主要话题作为聚类，并且布局能够随着数据流动态变化而保持稳定。

Conclusion: 该系统能动态地展示文本数据流中的主要话题，并且在数据流发生变化时能保持布局稳定。

Abstract: We present a visualization infrastructure that maps data elements to agents,
which have behaviors parameterized by those elements. Dynamic visualizations
emerge as the agents change position, alter appearance and respond to one
other. Agents move to minimize the difference between displayed agent-to-agent
distances, and an input matrix of ideal distances. Our current application is
visualization of streaming text. Each agent represents a significant word,
visualizing it by displaying the word itself, centered in a circle sized by the
frequency of word occurrence. We derive the ideal distance matrix from word
cooccurrence, mapping higher co-occurrence to lower distance. To depict
co-occurrence in its textual context, the ratio of intersection to circle area
approximates the ratio of word co-occurrence to frequency. A networked backend
process gathers articles from news feeds, blogs, Digg or Twitter, exploiting
online search APIs to focus on user-chosen topics. Resulting visuals reveal the
primary topics in text streams as clusters, with agent-based layout moving
without instability as data streams change dynamically.

</details>


### [560] [Optimizing Sequential Multi-Step Tasks with Parallel LLM Agents](https://arxiv.org/abs/2507.08944)
*Enhao Zhang,Erkang Zhu,Gagan Bansal,Adam Fourney,Hussein Mozannar,Jack Gerrits*

Main category: cs.MA

TL;DR: M1-Parallel框架通过并行执行多智能体团队来降低LLM系统延迟，提高任务完成率。


<details>
  <summary>Details</summary>
Motivation: 为了解决基于LLM的多智能体系统因需要多个迭代推理周期而导致的高延迟问题。

Method: 提出了一种名为M1-Parallel的框架，该框架利用事件驱动的通信模型和异步消息传递，并行运行多个多智能体团队来探索不同的解决方案路径。

Result: 实验表明，M1-Parallel结合提前终止策略可实现高达2.2倍的速度提升，同时保持准确性；结合聚合策略可提高任务完成率。研究还发现，鼓励执行计划多样性并未带来额外的性能提升。

Conclusion: M1-Parallel框架通过并行运行多个多智能体团队来解决高延迟问题，通过提前终止可以实现高达2.2倍的加速，同时保持准确性；通过聚合可以提高任务完成率。研究还表明，鼓励执行计划多样性并不能带来额外的性能提升。

Abstract: Large language model (LLM)-based multi-agent systems have demonstrated
remarkable promise for tackling complex tasks by breaking them down into
subtasks that are iteratively planned, executed, observed, and refined. Despite
their effectiveness, these systems often incur high latency because real-world
problems frequently demand multiple iterative cycles of reasoning steps. To
address this challenge, we propose M1-Parallel, a framework that concurrently
runs multiple multi-agent teams in parallel to uncover distinct solution paths.
By leveraging an event-driven communication model with asynchronous messaging,
M1-Parallel efficiently capitalizes on the inherent diversity of valid plans to
either reduce end-to-end latency or boost task completion rates. Our
experiments on complex tasks show that M1-Parallel with early termination
achieves up to $2.2\times$ speedup while preserving accuracy, and that
M1-Parallel with aggregation yields higher task completion rates. We further
investigate strategies aimed at encouraging diverse execution plans but observe
no additional performance gains over repeated sampling. Overall, these findings
underscore the potential of parallel plan execution for optimizing multi-agent
systems for real-world, high-complexity reasoning tasks.

</details>


### [561] [How to Train a Leader: Hierarchical Reasoning in Multi-Agent LLMs](https://arxiv.org/abs/2507.08960)
*Andrew Estornell,Jean-Francois Ton,Muhammad Faaiz Taufiq,Hang Li*

Main category: cs.MA

TL;DR: 提出了一种名为MLPO的新方法，通过训练单个领导LLM来协调未训练的对等智能体，解决了现有基于多智能体的LLM框架计算成本高的问题，并在多个基准测试中取得了优于基线方法的性能。


<details>
  <summary>Details</summary>
Motivation: 为了在提高解决方案质量的同时，解决现有基于多智能体的LLM框架计算成本高的问题。

Method: 提出了一种名为多智能体引导领导者策略优化（MLPO）的新方法，该方法训练领导LLM来协调一群未训练的对等智能体，并能评估和综合智能体响应，而无需辅助价值网络或显式智能体反馈。

Result: 在Big-Bench Hard (BBH)、MATH和MMLU上，与单一智能体和多智能体基线相比，该框架在推理时与智能体团队交互时表现出改进的性能，并且在没有团队的情况下单独部署时也具有改进的性能。

Conclusion: 通过训练单个灵活的领导者，在多主体LLM系统中实现协作推理，能够带来显著的性能提升。

Abstract: Large Language Models (LLMs) have achieved strong performance on a wide range
of complex reasoning tasks, yet further gains are often possible by leveraging
the complementary strengths of multiple models. While multi-agent frameworks
can improve solution quality by leveraging multiple LLMs, existing methods are
often computationally expensive, both at training and inference time. In this
work, we introduce a hierarchical multi-agent framework that addresses these
challenges by training only a single leader LLM to coordinate a team of
untrained peer agents. To this end, we propose Multi-agent guided Leader Policy
\textbf{O}ptimization (MLPO), a novel approach which trains the leader to
evaluate and synthesize agent responses without auxiliary value networks or
explicit agent feedback. Leaders trained with MLPO exhibit improved performance
not only when interacting with the agent team at inference time, but also enjoy
improved performance when deployed in single-agent settings without the team.
Empirical results on Big-Bench Hard (BBH), MATH, and MMLU demonstrate that our
framework achieves substantial performance improvements over both single-agent
and multi-agent baselines. Our results highlight the effectiveness and
efficiency of training a single, flexible leader for collaborative reasoning in
multi-agent LLM systems.

</details>


### [562] [Simulation for All: A Step-by-Step Cookbook for Developing Human-Centered Multi-Agent Transportation Simulators](https://arxiv.org/abs/2507.09367)
*Shiva Azimi,Arash Tavakoli*

Main category: cs.MA

TL;DR: 本论文提出了一个多功能交通模拟平台，它具有沉浸式虚拟环境、支持多种交通方式和用户（包括行人、骑行者、司机和公交乘客），并通过多种传感器收集生理数据。该平台旨在降低模拟的门槛，支持跨学科研究，以更好地理解城市交通。


<details>
  <summary>Details</summary>
Motivation: 随着城市向更复杂和多模式的交通系统发展，对以人为中心的多智能体模拟工具的需求从未如此迫切。然而，现有的大多数平台仍然有限——它们通常区分不同类型的道路使用者，依赖于脚本化或预定义的行为，忽视了公交通行者作为积极参与者的作用，并且很少考虑到对非技术用户的可访问性。

Method: 本论文提出了一种多智能体模拟平台的规范，该平台旨在支持所有道路使用者的实时、以人为本和沉浸式研究，并附有用于复制的开源脚本。该平台利用高保真沉浸式虚拟环境，实现了公交通行者、行人、骑行者、自动驾驶汽车和驾驶员之间的交互。该架构是模块化、可扩展且易于使用的。该系统集成了特定硬件模块（包括全向跑步机、座椅装置、智能教练和驱动座舱），并通过嵌入式传感设备（如功能性近红外光谱（fNIRS）、眼动追踪和基于手腕的生物传感器）收集多模态生理、神经和行为数据。

Result: 该平台能够实现公交通行者、行人、骑行者、自动驾驶汽车和驾驶员之间的交互。

Conclusion: 该平台旨在降低高保真交通模拟的门槛，支持跨学科实验，并增进我们对复杂城市环境中多式联运的理解。

Abstract: As cities evolve toward more complex and multimodal transportation systems,
the need for human-centered multi-agent simulation tools has never been more
urgent. Yet most existing platforms remain limited - they often separate
different types of road users, rely on scripted or pre-defined behaviors,
overlook public transit users as active participants, and are rarely designed
with accessibility in mind for non-technical users. To address this gap, this
paper presents the specifications of a multi-agent simulation platform designed
to support real-time, human-centered, and immersive studies of all road users,
accompanied by open-source scripts for replication. Using high-fidelity
immersive virtual environments, our platform enables interaction across public
transit users, pedestrians, cyclists, automated vehicles, and drivers. The
architecture is modular, extensible, and designed for accessibility. The system
integrates hardware-specific modules - including an omnidirectional treadmill,
a seating arrangement, a smart trainer, and an actuated cockpit. Additionally,
the platform collects multimodal physiological, neurological, and behavioral
data through embedded sensing devices such as functional near-infrared
spectroscopy (fNIRS), eye tracking, and wrist-based biosensors. To show the
usability of this system, we present three use cases. Simulation for All aims
to lower the barrier to entry for high-fidelity transportation simulation,
support experimentation across disciplines, and advance our understanding of
multimodal mobility in complex urban environments.

</details>


### [563] [Adaptive Social Learning using Theory of Mind](https://arxiv.org/abs/2507.09409)
*Lance Ying,Ryan Truong,Joshua B. Tenenbaum,Samuel J. Gershman*

Main category: cs.MA

TL;DR: 该研究提出了一个理性的心理化模型，用于解释在学习过程中人类如何在社会学习和非社会学习之间进行权衡。通过多人寻宝游戏实验，证明了该模型能够准确预测和解释人类的行为，并说明了社会学习在实现目标过程中的灵活性和效率。


<details>
  <summary>Details</summary>
Motivation: 探究人类如何在社会学习可能带来时间和认知资源成本的情况下，平衡社会学习和非社会学习。并提出一个理性的心理化模型来解释这种平衡。

Method: 提出一个理性的心理化模型来估计社会学习的效用，通过推理其他智能体的目标和他们未来行动的信息量。然后权衡社会学习的效用和自我探索（非社会学习）的效用。

Result: 在多人寻宝游戏中，模型能够量化人类在社会学习和非社会学习之间的权衡。结果表明，这两种成分能让智能体灵活地应用社会学习以更有效地实现其目标。

Conclusion: 该模型可以量化人类在社会学习和非社会学习之间的权衡，并且这两种成分能让智能体灵活地应用社会学习以更有效地实现其目标。

Abstract: Social learning is a powerful mechanism through which agents learn about the
world from others. However, humans don't always choose to observe others, since
social learning can carry time and cognitive resource costs. How do people
balance social and non-social learning? In this paper, we propose a rational
mentalizing model of the decision to engage in social learning. This model
estimates the utility of social learning by reasoning about the other agent's
goal and the informativity of their future actions. It then weighs the utility
of social learning against the utility of self-exploration (non-social
learning). Using a multi-player treasure hunt game, we show that our model can
quantitatively capture human trade-offs between social and non-social learning.
Furthermore, our results indicate that these two components allow agents to
flexibly apply social learning to achieve their goals more efficiently.

</details>


### [564] [TinyTroupe: An LLM-powered Multiagent Persona Simulation Toolkit](https://arxiv.org/abs/2507.09788)
*Paulo Salem,Robert Sim,Christopher Olsen,Prerit Saxena,Rafael Barcelos,Yi Ding*

Main category: cs.MA

TL;DR: TinyTroupe是一个用于真实人类行为模拟的Python工具包，它通过细粒度的人物角色定义和LLM驱动的机制，解决了现有MAS库的不足。该工具包支持详细的人物角色设置和程序化控制，能够用于个体或群体行为问题的解决，并通过示例和评估展示了其有效性、局限性和权衡。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLM）驱动的多智能体系统（MAS）在真实人类行为模拟方面存在不足，具体表现在缺乏细粒度的人物角色规范、总体抽样设施、实验支持和集成验证等关键能力，这限制了它们在行为研究、社会模拟和相关应用中的效用。

Method: TinyTroupe通过提供细粒度的人物角色定义（如国籍、年龄、职业、个性、信仰、行为）和通过多种LLM驱动的机制进行程序化控制来实现这一点。该工具包的组件通过代表性的工作示例（如头脑风暴和市场调研会议）进行展示，以阐明其目的并证明其有用性。此外，还提供了对选定方面的定量和定性评估，以突出其可能性、局限性和权衡。

Result: TinyTroupe能够进行详细的人物角色定义和程序化控制，能够有效地解决个体或群体层面的行为问题。通过头脑风暴和市场调研会议等示例，展示了其有用性。定量和定性评估突出了其可能性、局限性和权衡。

Conclusion: TinyTroupe是一个创新的模拟工具包，通过提供细粒度的人物角色定义和基于LLM的程序化控制，解决了现有MAS库在真实人类行为模拟方面的不足。该工具包支持用户定义包括国籍、年龄、职业、性格、信仰和行为在内的详细人物角色，并通过LLM驱动的机制实现对模拟的细致控制，从而能够有效地解决个体或群体层面的行为问题。

Abstract: Recent advances in Large Language Models (LLM) have led to a new class of
autonomous agents, renewing and expanding interest in the area. LLM-powered
Multiagent Systems (MAS) have thus emerged, both for assistive and simulation
purposes, yet tools for realistic human behavior simulation -- with its
distinctive challenges and opportunities -- remain underdeveloped. Existing MAS
libraries and tools lack fine-grained persona specifications, population
sampling facilities, experimentation support, and integrated validation, among
other key capabilities, limiting their utility for behavioral studies, social
simulation, and related applications. To address these deficiencies, in this
work we introduce TinyTroupe, a simulation toolkit enabling detailed persona
definitions (e.g., nationality, age, occupation, personality, beliefs,
behaviors) and programmatic control via numerous LLM-driven mechanisms. This
allows for the concise formulation of behavioral problems of practical
interest, either at the individual or group level, and provides effective means
for their solution. TinyTroupe's components are presented using representative
working examples, such as brainstorming and market research sessions, thereby
simultaneously clarifying their purpose and demonstrating their usefulness.
Quantitative and qualitative evaluations of selected aspects are also provided,
highlighting possibilities, limitations, and trade-offs. The approach, though
realized as a specific Python implementation, is meant as a novel conceptual
contribution, which can be partially or fully incorporated in other contexts.
The library is available as open source at
https://github.com/microsoft/tinytroupe.

</details>


### [565] [Large Population Models](https://arxiv.org/abs/2507.09901)
*Ayush Chopra*

Main category: cs.MA

TL;DR: LPMs通过模拟数百万个体的交互来理解社会复杂性，能够预测群体行为并测试政策。


<details>
  <summary>Details</summary>
Motivation: LPMs旨在理解社会面临的紧迫挑战，例如大流行病响应、供应链中断和气候适应等，这些挑战源于数百万个自主代理随时间的集体行为。它们通过模拟具有现实行为和交互的整个种群，以实现前所未有的规模来理解这些复杂系统。

Method: LPMs通过以下三个关键创新来扩展传统建模方法：1. 高效的计算方法，能够同时模拟数百万个代理。2. 学习多样化真实世界数据的数学框架。3. 能够连接虚拟和物理环境的隐私保护通信协议。

Result: LPMs能够使研究人员观察个体行为如何汇聚成系统级结果，并在现实世界实施前测试干预措施。它们通过展示交互的丰富性来揭示涌现现象，从而开发“数字社会”。

Conclusion: LPMs通过结合高效的计算方法、学习现实世界数据的数学框架以及隐私保护的通信协议，扩展了传统建模方法，实现了对数百万个体的同时模拟。这使得研究人员能够观察个体行为如何汇聚成系统级结果，并在现实世界实施前测试干预措施。与专注于创造具有复杂个体能力的“数字人类”的AI进展不同，LPMs致力于开发“数字社会”，其中丰富的交互揭示了涌现现象。LPMs通过连接个体代理行为和人口规模动态，为人工智能研究开辟了一条互补的路径，阐明了集体智能，并为政策和社会创新提供了现实部署前的测试场。文章还讨论了LPMs的技术基础和一些开放性问题，并介绍了AgentTorch框架（github.com/AgentTorch/AgentTorch）。

Abstract: Many of society's most pressing challenges, from pandemic response to supply
chain disruptions to climate adaptation, emerge from the collective behavior of
millions of autonomous agents making decisions over time. Large Population
Models (LPMs) offer an approach to understand these complex systems by
simulating entire populations with realistic behaviors and interactions at
unprecedented scale. LPMs extend traditional modeling approaches through three
key innovations: computational methods that efficiently simulate millions of
agents simultaneously, mathematical frameworks that learn from diverse
real-world data streams, and privacy-preserving communication protocols that
bridge virtual and physical environments. This allows researchers to observe
how agent behavior aggregates into system-level outcomes and test interventions
before real-world implementation. While current AI advances primarily focus on
creating "digital humans" with sophisticated individual capabilities, LPMs
develop "digital societies" where the richness of interactions reveals emergent
phenomena. By bridging individual agent behavior and population-scale dynamics,
LPMs offer a complementary path in AI research illuminating collective
intelligence and providing testing grounds for policies and social innovations
before real-world deployment. We discuss the technical foundations and some
open problems here. LPMs are implemented by the AgentTorch framework
(github.com/AgentTorch/AgentTorch)

</details>


### [566] [AnalogTester: A Large Language Model-Based Framework for Automatic Testbench Generation in Analog Circuit Design](https://arxiv.org/abs/2507.09965)
*Weiyu Chen,Chengjie Liu,Wenhao Huang,Jinyang Lyu,Mingqian Yang,Yuan Du,Li Du,Jun Yang*

Main category: cs.MA

TL;DR: AnalogTester 利用大语言模型（LLM）自动化生成模拟电路测试平台，解决了手动设计的瓶颈，并为未来 LLM 在该领域的应用奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 解决当前模拟电路设计中手动创建测试平台所带来的瓶颈问题，实现设计流程的全面自动化。

Method: 通过 LLM 驱动的流水线，整合领域知识，提取论文信息，合成仿真方案，并利用 Tsinghua Electronic Design (TED) 生成测试平台代码。

Result: AnalogTester 成功为运算放大器（op-amps）、带隙基准源（BGRs）和低压差线性稳压器（LDOs）等三种基本模拟电路类型实现了自动化测试平台生成，并且其框架具有良好的可扩展性，能够适应更广泛的电路拓扑。此外，它还能生成电路知识数据和 TED 代码语料库，为 LLM 在模拟电路设计自动化领域的专业化训练奠定了基础。

Conclusion: AnalogTester 成功实现了自动化测试平台生成，为模拟电路设计自动化带来了重大突破。

Abstract: Recent advancements have demonstrated the significant potential of large
language models (LLMs) in analog circuit design. Nevertheless, testbench
construction for analog circuits remains manual, creating a critical bottleneck
in achieving fully automated design processes. Particularly when replicating
circuit designs from academic papers, manual Testbench construction demands
time-intensive implementation and frequent adjustments, which fails to address
the dynamic diversity and flexibility requirements for automation. AnalogTester
tackles automated analog design challenges through an LLM-powered pipeline: a)
domain-knowledge integration, b) paper information extraction, c) simulation
scheme synthesis, and d) testbench code generation with Tsinghua Electronic
Design (TED). AnalogTester has demonstrated automated Testbench generation
capabilities for three fundamental analog circuit types: operational amplifiers
(op-amps), bandgap references (BGRs), and low-dropout regulators (LDOs), while
maintaining a scalable framework for adaptation to broader circuit topologies.
Furthermore, AnalogTester can generate circuit knowledge data and TED code
corpus, establishing fundamental training datasets for LLM specialization in
analog circuit design automation.

</details>


### [567] [Multi-Robot Cooperative Herding through Backstepping Control Barrier Functions](https://arxiv.org/abs/2507.10249)
*Kang Li,Ming Li,Wenkang Ji,Zhiyong Sun,Shiyu Zhao*

Main category: cs.MA

TL;DR: 本研究提出了一种基于反步控制势垒函数（CBFs）的合作围堵策略，用于将多个逃避者安全地围堵到目标区域。该方法解决了由于间接相互作用导致的欠驱动问题，并通过仿真和实验验证了其有效性和安全性。


<details>
  <summary>Details</summary>
Motivation: 由于逃避者仅对围堵者的排斥相互作用做出反应，并且其行为只能通过围堵者的运动间接影响，因此所提出的围堵系统（包括围堵者和逃避者）具有固有的欠驱动特性。为了解决这个关键问题，我们设计了一种新颖的合作围堵策略。

Method: 本研究首先为双重目标（到达目标区域和避免碰撞）构建了单独的CBFs，以确保围堵的完成和安全性。然后，将欠驱动的围堵动力学重新构建为控制仿射结构，并采用反步法递归设计层次势垒函数的控制输入，避免了对高阶系统求导。

Result: 所提出的策略能够安全地将多个逃避者围堵到目标区域。

Conclusion: 本研究提出了一种新颖的合作围堵策略，通过反步控制势垒函数（CBFs）协调多个围堵者，将一群逃避者安全地围堵到指定的区域。此外，还开发了所提出算法的集中式和分布式实现，以提高其灵活性和适用性。大量的仿真和实际实验验证了所提出的策略在多机器人围堵中的有效性和安全性。

Abstract: We propose a novel cooperative herding strategy through backstepping control
barrier functions (CBFs), which coordinates multiple herders to herd a group of
evaders safely towards a designated goal region. For the herding system with
heterogeneous groups involving herders and evaders, the behavior of the evaders
can only be influenced indirectly by the herders' motion, especially when the
evaders follow an inverse dynamics model and respond solely to repulsive
interactions from the herders. This indirect interaction mechanism inherently
renders the overall system underactuated. To address this issue, we first
construct separate CBFs for the dual objectives of goal reaching and collision
avoidance, which ensure both herding completion and safety guarantees. Then, we
reformulate the underactuated herding dynamics into a control-affine structure
and employ a backstepping approach to recursively design control inputs for the
hierarchical barrier functions, avoiding taking derivatives of the higher-order
system. Finally, we present a cooperative herding strategy based on
backstepping CBFs that allow herders to safely herd multiple evaders into the
goal region. In addition, centralized and decentralized implementations of the
proposed algorithm are developed, further enhancing its flexibility and
applicability. Extensive simulations and real-world experiments validate the
effectiveness and safety of the proposed strategy in multi-robot herding.

</details>


### [568] [ToMacVF : Temporal Macro-action Value Factorization for Asynchronous Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2507.10251)
*Wenjing Zhang,Wei Zhang*

Main category: cs.MA

TL;DR: 该研究提出了一种名为ToMacVF的算法和Mac-SJERT缓冲区，以解决现有异步多智能体强化学习方法在宏动作执行过程表示和信用分配方面的问题，并在实验中取得了优于基线方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于MacDec-POMDP的异步多智能体强化学习方法在构建训练轨迹缓冲区时存在数据有限和有偏见的问题，导致对宏动作执行过程的表示不完整、不准确，并且信用分配不当。

Method: 提出了一种名为ToMacVF（Temporal Macro-action Value Factorization）的算法，并设计了一个名为Mac-SJERT（Macro-action Segmented Joint Experience Replay Trajectory）的中心化训练缓冲区。通过形式化To-Mac-IGM（Temporal Macro-action based IGM）来确保联合和个体宏动作选择之间的一致性，从而实现原则性和细粒度的异步值分解。

Result: 实验结果表明，与异步基线方法相比，ToMacVF算法实现了最优性能，并展现了良好的适应性和鲁棒性。

Conclusion: ToMacVF算法在各种异步多智能体实验场景中表现出最优性能，并具有很强的适应性和鲁棒性。

Abstract: Existing asynchronous MARL methods based on MacDec-POMDP typically construct
training trajectory buffers by simply sampling limited and biased data at the
endpoints of macro-actions, and directly apply conventional MARL methods on the
buffers. As a result, these methods lead to an incomplete and inaccurate
representation of the macro-action execution process, along with unsuitable
credit assignments. To solve these problems, the Temporal Macro-action Value
Factorization (ToMacVF) is proposed to achieve fine-grained temporal credit
assignment for macro-action contributions. A centralized training buffer,
called Macro-action Segmented Joint Experience Replay Trajectory (Mac-SJERT),
is designed to incorporate with ToMacVF to collect accurate and complete
macro-action execution information, supporting a more comprehensive and precise
representation of the macro-action process. To ensure principled and
fine-grained asynchronous value factorization, the consistency requirement
between joint and individual macro-action selection called Temporal
Macro-action based IGM (To-Mac-IGM) is formalized, proving that it generalizes
the synchronous cases. Based on To-Mac-IGM, a modularized ToMacVF architecture,
which satisfies CTDE principle, is designed to conveniently integrate previous
value factorization methods. Next, the ToMacVF algorithm is devised as an
implementation of the ToMacVF architecture. Experimental results demonstrate
that, compared to asynchronous baselines, our ToMacVF algorithm not only
achieves optimal performance but also exhibits strong adaptability and
robustness across various asynchronous multi-agent experimental scenarios.

</details>


### [569] [Toolsuite for Implementing Multiagent Systems Based on Communication Protocols](https://arxiv.org/abs/2507.10324)
*Amit K. Chopra,Samuel H. Christie V,Munindar P. Singh*

Main category: cs.MA

TL;DR: 该论文介绍了用于多主体系统交互导向编程（IOP）的软件套件，包括协议验证工具和代理实现中间件。


<details>
  <summary>Details</summary>
Motivation: 展示了为多主体系统开发交互导向编程（IOP）的软件套件，重点介绍了协议验证工具和代理实现中间件。

Method: 介绍了一套用于多主体系统交互导向编程（IOP）的软件工具，包括交互协议验证工具和代理实现中间件。

Result: 该论文 presented some of the software suite that enables multiagent system developers to apply IOP, including tools for verifying protocols and middleware for implementing agents.

Conclusion: 该论文介绍了为多主体系统交互导向编程（IOP）开发的软件套件，包括用于验证交互协议的工具和简化代理实现的中间件。

Abstract: Interaction-Oriented Programming (IOP) is an approach to building a
multiagent system by modeling the interactions between its roles via a flexible
interaction protocol and implementing agents to realize the interactions of the
roles they play in the protocol.
  In recent years, we have developed an extensive suite of software that
enables multiagent system developers to apply IOP. These include tools for
efficiently verifying protocols for properties such as liveness and safety and
middleware that simplifies the implementation of agents. This paper presents
some of that software suite.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [570] [OTAS: Open-vocabulary Token Alignment for Outdoor Segmentation](https://arxiv.org/abs/2507.08851)
*Simon Schwaiger,Stefan Thalhammer,Wilfried Wöber,Gerald Steinbauer-Wagner*

Main category: cs.RO

TL;DR: OTAS 是一种新的开放词汇分割方法，通过从视觉模型令牌中提取语义结构，克服了室外分割的挑战，并在 3D 分割任务上取得了显著成果。


<details>
  <summary>Details</summary>
Motivation: 当前的视觉-语言映射方法在室外非结构化环境中面临语义模糊和类别边界不清晰的挑战，因为它们依赖于以物体为中心的分割先验。因此，需要一种新的方法来更好地理解开放世界语义，以支持机器人的规划和控制。

Method: OTAS（Open-vocabulary Token Alignment）是一种新的开放词汇分割方法，通过直接从预训练视觉模型的输出令牌中提取语义结构，克服了现有方法的局限性。它通过聚类语义相似的结构并将其与语言关联，重构出几何一致的特征场，支持开放词汇分割查询，且无需针对特定场景进行微调，运行速度可达约 17 帧/秒。

Result: OTAS 在 Off-Road Freespace Detection 数据集上，相较于经过微调和开放词汇的 2D 分割方法，IoU 略有提升；在 TartanAir 数据集上，OTAS 在 3D 分割任务上的 IoU 提升高达 151%，显著优于现有的开放词汇映射方法。

Conclusion: OTAS 方法在 TartanAir 数据集的 3D 分割任务上实现了超过现有开放词汇映射方法的 151% 的 IoU 提升，并在真实世界重建中展示了其在机器人应用中的潜力。

Abstract: Understanding open-world semantics is critical for robotic planning and
control, particularly in unstructured outdoor environments. Current
vision-language mapping approaches rely on object-centric segmentation priors,
which often fail outdoors due to semantic ambiguities and indistinct semantic
class boundaries. We propose OTAS - an Open-vocabulary Token Alignment method
for Outdoor Segmentation. OTAS overcomes the limitations of open-vocabulary
segmentation models by extracting semantic structure directly from the output
tokens of pretrained vision models. By clustering semantically similar
structures across single and multiple views and grounding them in language,
OTAS reconstructs a geometrically consistent feature field that supports
open-vocabulary segmentation queries. Our method operates zero-shot, without
scene-specific fine-tuning, and runs at up to ~17 fps. OTAS provides a minor
IoU improvement over fine-tuned and open-vocabulary 2D segmentation methods on
the Off-Road Freespace Detection dataset. Our model achieves up to a 151% IoU
improvement over open-vocabulary mapping methods in 3D segmentation on
TartanAir. Real-world reconstructions demonstrate OTAS' applicability to
robotic applications. The code and ROS node will be made publicly available
upon paper acceptance.

</details>


### [571] [AirScape: An Aerial Generative World Model with Motion Controllability](https://arxiv.org/abs/2507.08885)
*Baining Zhao,Rongze Tang,Mingyuan Jia,Ziyou Wang,Fanghang Man,Xin Zhang,Yu Shang,Weichen Zhang,Chen Gao,Wei Wu,Xin Wang,Xinlei Chen,Yong Li*

Main category: cs.RO

TL;DR: AirScape是一个新的世界模型，可以帮助无人机预测它们的动作结果。


<details>
  <summary>Details</summary>
Motivation: 探索更通用的空间想象能力，以解决机器人预测自身运动意图在三维空间中结果的基本问题。

Method: 开发了一个两阶段训练计划来训练一个基础模型，使其成为一个可控于运动意图并遵循物理时空约束的世界模型。

Result: 构建了一个包含11k视频-意图对的空中世界模型训练和测试数据集，该数据集包括了第一人称视角的视频和详细标注的运动意图。

Conclusion: AirScape是首个为六自由度空中机器人设计的世界模型，能够根据当前的视觉输入和运动意图预测未来观测序列。

Abstract: How to enable robots to predict the outcomes of their own motion intentions
in three-dimensional space has been a fundamental problem in embodied
intelligence. To explore more general spatial imagination capabilities, here we
present AirScape, the first world model designed for six-degree-of-freedom
aerial agents. AirScape predicts future observation sequences based on current
visual inputs and motion intentions. Specifically, we construct an dataset for
aerial world model training and testing, which consists of 11k video-intention
pairs. This dataset includes first-person-view videos capturing diverse drone
actions across a wide range of scenarios, with over 1,000 hours spent
annotating the corresponding motion intentions. Then we develop a two-phase
training schedule to train a foundation model -- initially devoid of embodied
spatial knowledge -- into a world model that is controllable by motion
intentions and adheres to physical spatio-temporal constraints.

</details>


### [572] [End-to-End Generation of City-Scale Vectorized Maps by Crowdsourced Vehicles](https://arxiv.org/abs/2507.08901)
*Zebang Feng,Miao Fan,Bao Liu,Shengtong Xu,Haoyi Xiong*

Main category: cs.RO

TL;DR: EGC-VMAP 是一个端到端的框架，利用众包车辆数据和创新的旅行感知 Transformer 架构，通过融合多车辆、多时间的数据来生成高精度的城市矢量地图，相比传统方法，显著降低了成本并提高了精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统基于激光雷达的制图成本高昂且速度缓慢，而单车感知方法在准确性和鲁棒性方面存在不足，尤其是在不利条件下。需要一种更有效、更准确的方法来创建高精度矢量地图，以满足自动驾驶的需求。

Method: EGC-VMAP 是一个端到端的框架，通过聚合众包车辆的数据来生成精确的、城市规模的矢量地图。它使用新颖的、感知车载地图元素的感知旅行感知 Transformer 架构，在统一的学习过程中进行多车辆、多时间融合。结合分层匹配以实现高效训练和多目标损失，我们的方法在地图精度和结构鲁棒性方面明显优于单车基线。

Result: EGC-VMAP 在大规模、多城市的真实世界数据集上进行了验证，证明了其优越的性能，与单车基线相比，在地图精度和结构鲁棒性方面得到了显著提高。

Conclusion: EGC-VMAP 提供了一种可扩展且经济高效的解决方案，可实现城市范围的地图绘制，并报告手动注释成本降低了 90%。

Abstract: High-precision vectorized maps are indispensable for autonomous driving, yet
traditional LiDAR-based creation is costly and slow, while single-vehicle
perception methods lack accuracy and robustness, particularly in adverse
conditions. This paper introduces EGC-VMAP, an end-to-end framework that
overcomes these limitations by generating accurate, city-scale vectorized maps
through the aggregation of data from crowdsourced vehicles. Unlike prior
approaches, EGC-VMAP directly fuses multi-vehicle, multi-temporal map elements
perceived onboard vehicles using a novel Trip-Aware Transformer architecture
within a unified learning process. Combined with hierarchical matching for
efficient training and a multi-objective loss, our method significantly
enhances map accuracy and structural robustness compared to single-vehicle
baselines. Validated on a large-scale, multi-city real-world dataset, EGC-VMAP
demonstrates superior performance, enabling a scalable, cost-effective solution
for city-wide mapping with a reported 90\% reduction in manual annotation
costs.

</details>


### [573] [Multimodal HD Mapping for Intersections by Intelligent Roadside Units](https://arxiv.org/abs/2507.08903)
*Zhongzhang Chen,Miao Fan,Shengtong Xu,Mengmeng Yang,Kun Jiang,Xiangzeng Liu,Haoyi Xiong*

Main category: cs.RO

TL;DR: 本研究提出了一种利用智能道路单元（IRU）和相机-激光雷达融合的新方法，用于高精度（HD）语义建图，并发布了一个新的数据集（RS-seq）。实验证明，该方法比单一传感器的方法效果更好，特别是在提高语义分割精度方面。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统基于车辆的方法在复杂交叉路口进行高精度（HD）语义建图时，由于遮挡和有限视角而带来的挑战。

Method: 提出了一种新颖的相机-激光雷达融合框架，该框架利用了智能道路单元（IRU），并通过两阶段过程集成特定于模态的特征提取和跨模态语义集成，结合了相机的丰富纹理和激光雷达的精确几何数据。

Result: 在RS-seq数据集上的量化评估表明，所提出的多模态方法在语义分割的平均交并比（mIoU）方面，比仅使用图像的方法提高了4%，比仅使用点云的方法提高了18%，一致优于单模态方法。

Conclusion: 本研究为基于智能道路单元（IRU）的高精度（HD）语义建图建立了基线方法，并提供了一个宝贵的RS-seq数据集，以支持未来基础设施辅助自动驾驶系统的研究。

Abstract: High-definition (HD) semantic mapping of complex intersections poses
significant challenges for traditional vehicle-based approaches due to
occlusions and limited perspectives. This paper introduces a novel camera-LiDAR
fusion framework that leverages elevated intelligent roadside units (IRUs).
Additionally, we present RS-seq, a comprehensive dataset developed through the
systematic enhancement and annotation of the V2X-Seq dataset. RS-seq includes
precisely labelled camera imagery and LiDAR point clouds collected from
roadside installations, along with vectorized maps for seven intersections
annotated with detailed features such as lane dividers, pedestrian crossings,
and stop lines. This dataset facilitates the systematic investigation of
cross-modal complementarity for HD map generation using IRU data. The proposed
fusion framework employs a two-stage process that integrates modality-specific
feature extraction and cross-modal semantic integration, capitalizing on camera
high-resolution texture and precise geometric data from LiDAR. Quantitative
evaluations using the RS-seq dataset demonstrate that our multimodal approach
consistently surpasses unimodal methods. Specifically, compared to unimodal
baselines evaluated on the RS-seq dataset, the multimodal approach improves the
mean Intersection-over-Union (mIoU) for semantic segmentation by 4\% over the
image-only results and 18\% over the point cloud-only results. This study
establishes a baseline methodology for IRU-based HD semantic mapping and
provides a valuable dataset for future research in infrastructure-assisted
autonomous driving systems.

</details>


### [574] [Towards Human-level Dexterity via Robot Learning](https://arxiv.org/abs/2507.09117)
*Gagan Khandate*

Main category: cs.RO

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Dexterous intelligence -- the ability to perform complex interactions with
multi-fingered hands -- is a pinnacle of human physical intelligence and
emergent higher-order cognitive skills. However, contrary to Moravec's paradox,
dexterous intelligence in humans appears simple only superficially. Many
million years were spent co-evolving the human brain and hands including rich
tactile sensing. Achieving human-level dexterity with robotic hands has long
been a fundamental goal in robotics and represents a critical milestone toward
general embodied intelligence. In this pursuit, computational sensorimotor
learning has made significant progress, enabling feats such as arbitrary
in-hand object reorientation. However, we observe that achieving higher levels
of dexterity requires overcoming very fundamental limitations of computational
sensorimotor learning.
  I develop robot learning methods for highly dexterous multi-fingered
manipulation by directly addressing these limitations at their root cause.
Chiefly, through key studies, this disseration progressively builds an
effective framework for reinforcement learning of dexterous multi-fingered
manipulation skills. These methods adopt structured exploration, effectively
overcoming the limitations of random exploration in reinforcement learning. The
insights gained culminate in a highly effective reinforcement learning that
incorporates sampling-based planning for direct exploration. Additionally, this
thesis explores a new paradigm of using visuo-tactile human demonstrations for
dexterity, introducing corresponding imitation learning techniques.

</details>


### [575] [Online 3D Bin Packing with Fast Stability Validation and Stable Rearrangement Planning](https://arxiv.org/abs/2507.09123)
*Ziyan Gao,Lijun Wang,Yuntao Kong,Nak Young Chong*

Main category: cs.RO

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The Online Bin Packing Problem (OBPP) is a sequential decision-making task in
which each item must be placed immediately upon arrival, with no knowledge of
future arrivals. Although recent deep-reinforcement-learning methods achieve
superior volume utilization compared with classical heuristics, the learned
policies cannot ensure the structural stability of the bin and lack mechanisms
for safely reconfiguring the bin when a new item cannot be placed directly. In
this work, we propose a novel framework that integrates packing policy with
structural stability validation and heuristic planning to overcome these
limitations. Specifically, we introduce the concept of Load Bearable Convex
Polygon (LBCP), which provides a computationally efficient way to identify
stable loading positions that guarantee no bin collapse. Additionally, we
present Stable Rearrangement Planning (SRP), a module that rearranges existing
items to accommodate new ones while maintaining overall stability. Extensive
experiments on standard OBPP benchmarks demonstrate the efficiency and
generalizability of our LBCP-based stability validation, as well as the
superiority of SRP in finding the effort-saving rearrangement plans. Our method
offers a robust and practical solution for automated packing in real-world
industrial and logistics applications.

</details>


### [576] [Tactile-VLA: Unlocking Vision-Language-Action Model's Physical Knowledge for Tactile Generalization](https://arxiv.org/abs/2507.09160)
*Jialei Huang,Shuo Wang,Fanqi Lin,Yihang Hu,Chuan Wen,Yang Gao*

Main category: cs.RO

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Vision-Language-Action (VLA) models have shown remarkable achievements,
driven by the rich implicit knowledge of their vision-language components.
However, achieving generalist robotic agents demands precise grounding into
physical interactions, especially in contact-rich scenarios where fine-grained
force control is essential. We advance VLAs' implicit knowledge beyond
identifying what to do, towards guiding how to physically interact with real
world. This paper introduces Tactile-VLA, a novel framework that deeply fuses
vision, language, action, and tactile sensing. This framework incorporates a
hybrid position-force controller to translate the model's intentions into
precise physical actions and a reasoning module that allows the robot to adapt
its strategy based on tactile feedback. Experiments demonstrate Tactile-VLA's
effectiveness and generalizability in three key aspects: (1) enabling
tactile-aware instruction following, (2) utilizing tactile-relevant
commonsense, and (3) facilitating adaptive tactile-involved reasoning. A key
finding is that the VLM's prior knowledge already contains semantic
understanding of physical interaction; by connecting it to the robot's tactile
sensors with only a few demonstrations, we can activate this prior knowledge to
achieve zero-shot generalization in contact-rich tasks.

</details>


### [577] [PRAG: Procedural Action Generator](https://arxiv.org/abs/2507.09167)
*Michal Vavrecka,Radoslav Skoviera,Gabriela Sejnova,Karla Stepanova*

Main category: cs.RO

TL;DR: 一个机器人任务生成器，可以创建可行的多步骤操作任务，并提供用于强化学习训练的数据。


<details>
  <summary>Details</summary>
Motivation: 为了在机器人学中程序化地构建多步骤的接触密集型操作任务。

Method: 提出了一种程序化生成多步骤接触密集型机器人操作任务的方法，该方法通过符号和物理验证来约束所有可能的（不可行的）组合，确保生成任务的可行性。

Result: 在最多15个动作的序列上测试了生成器，产生了数百万个独特的可行多步骤任务，并能测量生成任务之间的语义相似性。

Conclusion: 该方法能够生成可行的、多步骤的机器人操作任务，并能与现有框架集成，也可作为数据集存储，为强化学习训练提供支持。

Abstract: We present a novel approach for the procedural construction of multi-step
contact-rich manipulation tasks in robotics. Our generator takes as input
user-defined sets of atomic actions, objects, and spatial predicates and
outputs solvable tasks of a given length for the selected robotic environment.
The generator produces solvable tasks by constraining all possible
(nonsolvable) combinations by symbolic and physical validation. The symbolic
validation checks each generated sequence for logical and operational
consistency, and also the suitability of object-predicate relations. Physical
validation checks whether tasks can be solved in the selected robotic
environment. Only the tasks that passed both validators are retained. The
output from the generator can be directly interfaced with any existing
framework for training robotic manipulation tasks, or it can be stored as a
dataset of curated robotic tasks with detailed information about each task.
This is beneficial for RL training as there are dense reward functions and
initial and goal states paired with each subgoal. It allows the user to measure
the semantic similarity of all generated tasks. We tested our generator on
sequences of up to 15 actions resulting in millions of unique solvable
multi-step tasks.

</details>


### [578] [DLBAcalib: Robust Extrinsic Calibration for Non-Overlapping LiDARs Based on Dual LBA](https://arxiv.org/abs/2507.09176)
*Han Ye,Yuqiang Jin,Jinyuan Liu,Tao Li,Wen-An Zhang,Minglei Fu*

Main category: cs.RO

TL;DR: 一种无需重叠视场或精确初始参数估计的新型目标物外参校准框架，通过激光雷达捆绑调整和迭代精炼优化，实现了高精度和鲁棒性，适用于多激光雷达系统。


<details>
  <summary>Details</summary>
Motivation: 精确的多激光雷达外参校准对于提高三维地图重建系统的基础性能至关重要。

Method: 提出了一种新颖的无目标物外参校准框架，通过将激光雷达捆绑调整（LBA）优化与鲁棒的迭代精炼相结合，构建精确的参考点云地图，并将外参校准公式化为联合LBA优化问题，通过自适应加权机制有效缓解累积误差并实现抗噪声参数估计。

Result: 在非重叠传感器配置下，实现了平均平移误差5毫米和旋转误差0.2度，初始误差容忍度高达0.4米/30度。

Conclusion: 该方法在CARLA模拟环境和真实世界场景的广泛评估中，在准确性和鲁棒性方面均优于最先进的校准技术，并且校准过程无需专门的基础设施或手动参数调整。

Abstract: Accurate extrinsic calibration of multiple LiDARs is crucial for improving
the foundational performance of three-dimensional (3D) map reconstruction
systems. This paper presents a novel targetless extrinsic calibration framework
for multi-LiDAR systems that does not rely on overlapping fields of view or
precise initial parameter estimates. Unlike conventional calibration methods
that require manual annotations or specific reference patterns, our approach
introduces a unified optimization framework by integrating LiDAR bundle
adjustment (LBA) optimization with robust iterative refinement. The proposed
method constructs an accurate reference point cloud map via continuous scanning
from the target LiDAR and sliding-window LiDAR bundle adjustment, while
formulating extrinsic calibration as a joint LBA optimization problem. This
method effectively mitigates cumulative mapping errors and achieves
outlier-resistant parameter estimation through an adaptive weighting mechanism.
Extensive evaluations in both the CARLA simulation environment and real-world
scenarios demonstrate that our method outperforms state-of-the-art calibration
techniques in both accuracy and robustness. Experimental results show that for
non-overlapping sensor configurations, our framework achieves an average
translational error of 5 mm and a rotational error of 0.2{\deg}, with an
initial error tolerance of up to 0.4 m/30{\deg}. Moreover, the calibration
process operates without specialized infrastructure or manual parameter tuning.
The code is open source and available on GitHub
(\underline{https://github.com/Silentbarber/DLBAcalib})

</details>


### [579] [Informed Hybrid Zonotope-based Motion Planning Algorithm](https://arxiv.org/abs/2507.09309)
*Peng Xie,Johannes Betz,Amr Alanwar*

Main category: cs.RO

TL;DR: HZ-MP 是一种新的混合维度运动规划器，通过分解空间和使用启发式方法进行采样，解决了在非凸空间中路径规划的难题，并证明了其最优性和高效性。


<details>
  <summary>Details</summary>
Motivation: 在非凸自由空间中进行最优路径规划极具挑战性，因为将其表述为混合整数线性规划（MILP）是NP难的。

Method: HZ-MP 规划器通过分解无障碍空间并结合椭球体启发式进行低维面采样来实现。

Result: 与现有的启发式规划器（如 AIT* 和 EIT*）相比，HZ-MP 可以在狭窄缝隙或boxed-goal场景中避免无效采样，从而提高规划效率。

Conclusion: HZ-MP 规划器被证明在概率上是完整的并且渐近最优的，可以在有限时间内收敛到近似最优轨迹，并且能够扩展到高维拥挤场景。

Abstract: Optimal path planning in nonconvex free spaces is notoriously challenging, as
formulating such problems as mixed-integer linear programs (MILPs) is NP-hard.
We propose HZ-MP, an informed Hybrid Zonotope-based Motion Planner, as an
alternative approach that decomposes the obstacle-free space and performs
low-dimensional face sampling guided by an ellipsotope heuristic, enabling
focused exploration along promising transit regions. This structured
exploration eliminates the excessive, unreachable sampling that degrades
existing informed planners such as AIT* and EIT* in narrow gaps or boxed-goal
scenarios. We prove that HZ-MP is probabilistically complete and asymptotically
optimal. It converges to near-optimal trajectories in finite time and scales to
high-dimensional cluttered scenes.

</details>


### [580] [Unified Linear Parametric Map Modeling and Perception-aware Trajectory Planning for Mobile Robotics](https://arxiv.org/abs/2507.09340)
*Hongyu Nie,Xingyu Li,Xu Liu,Zhaotong Tan,Sen Mei,Wenbo Su*

Main category: cs.RO

TL;DR: 提出了一种名为 RMRP 的新方法，并基于此开发了 RPATR 框架，用于解决移动机器人在复杂环境中导航的挑战。该方法通过轻量级建图和感知感知轨迹规划，提高了导航效率和安全性。


<details>
  <summary>Details</summary>
Motivation: 为了解决移动机器人在大规模、复杂环境中自主导航的重大障碍，例如繁重的计算负担、传感器遮挡故障以及不规则地形的穿越挑战，以及缺乏感知感知策略的问题。

Method: 提出了一种名为随机映射和随机投影（RMRP）的方法，通过将数据映射到高维空间，然后进行稀疏随机投影以进行降维，从而构建轻量级线性参数化地图。利用提出的残差能量守恒定理为该过程提供理论保证，确保关键几何属性得以保留。在此地图的基础上，提出了一种名为 RPATR（鲁棒感知感知轨迹规划器）的框架。

Result: 该框架在多种场景下进行了验证，在建图的计算效率、内存占用和准确性方面均表现出色，并实现了高超音速无人机和无人机的安全导航。

Conclusion: 该框架在时间和内存效率以及准确性方面展示了优越的建图性能，并实现了高超音速无人机和无人机的计算高效、安全导航。

Abstract: Autonomous navigation in mobile robots, reliant on perception and planning,
faces major hurdles in large-scale, complex environments. These include heavy
computational burdens for mapping, sensor occlusion failures for UAVs, and
traversal challenges on irregular terrain for UGVs, all compounded by a lack of
perception-aware strategies. To address these challenges, we introduce Random
Mapping and Random Projection (RMRP). This method constructs a lightweight
linear parametric map by first mapping data to a high-dimensional space,
followed by a sparse random projection for dimensionality reduction. Our novel
Residual Energy Preservation Theorem provides theoretical guarantees for this
process, ensuring critical geometric properties are preserved. Based on this
map, we propose the RPATR (Robust Perception-Aware Trajectory Planner)
framework. For UAVs, our method unifies grid and Euclidean Signed Distance
Field (ESDF) maps. The front-end uses an analytical occupancy gradient to
refine initial paths for safety and smoothness, while the back-end uses a
closed-form ESDF for trajectory optimization. Leveraging the trained RMRP
model's generalization, the planner predicts unobserved areas for proactive
navigation. For UGVs, the model characterizes terrain and provides closed-form
gradients, enabling online planning to circumvent large holes. Validated in
diverse scenarios, our framework demonstrates superior mapping performance in
time, memory, and accuracy, and enables computationally efficient, safe
navigation for high-speed UAVs and UGVs. The code will be released to foster
community collaboration.

</details>


### [581] [C-ZUPT: Stationarity-Aided Aerial Hovering](https://arxiv.org/abs/2507.09344)
*Daniel Engelsman,Itzik Klein*

Main category: cs.RO

TL;DR: 提出了一种名为 C-ZUPT 的新方法，用于无人机导航，即使在没有地面接触的情况下也能通过识别准静态平衡点来提供精确的速度更新，从而减少漂移、提高稳定性和效率。


<details>
  <summary>Details</summary>
Motivation: 由于传感器偏差和噪声，仅依靠惯性传感器进行定位会导致精度随时间快速下降。替代的更新源（例如零速度更新（ZUPT））可以提供精确的校正，但现有的 ZUPT 方法仅限于地面平台。

Method: 提出了一种受控零速度更新（C-ZUPT）方法，用于航空导航和控制，不依赖于与地面的接触。通过定义不确定性阈值，C-ZUPT 识别准静态平衡点，为估计滤波器提供精确的速度更新。

Result: 通过机会性、高质量的更新显著减少了惯性漂移和控制工作量，从而缓解了滤波器发散和增强了导航稳定性。

Conclusion: C-ZUPT 缓解了滤波器发散并增强了导航稳定性，从而实现了更节能的悬停并显著延长了持续飞行时间，这对于资源受限的航空系统至关重要。

Abstract: Autonomous systems across diverse domains have underscored the need for
drift-resilient state estimation. Although satellite-based positioning and
cameras are widely used, they often suffer from limited availability in many
environments. As a result, positioning must rely solely on inertial sensors,
leading to rapid accuracy degradation over time due to sensor biases and noise.
To counteract this, alternative update sources-referred to as information
aiding-serve as anchors of certainty. Among these, the zero-velocity update
(ZUPT) is particularly effective in providing accurate corrections during
stationary intervals, though it is restricted to surface-bound platforms. This
work introduces a controlled ZUPT (C-ZUPT) approach for aerial navigation and
control, independent of surface contact. By defining an uncertainty threshold,
C-ZUPT identifies quasi-static equilibria to deliver precise velocity updates
to the estimation filter. Extensive validation confirms that these
opportunistic, high-quality updates significantly reduce inertial drift and
control effort. As a result, C-ZUPT mitigates filter divergence and enhances
navigation stability, enabling more energy-efficient hovering and substantially
extending sustained flight-key advantages for resource-constrained aerial
systems.

</details>


### [582] [Constrained Style Learning from Imperfect Demonstrations under Task Optimality](https://arxiv.org/abs/2507.09371)
*Kehan Wen,Chenhao Li,Junzhe He,Marco Hutter*

Main category: cs.RO

TL;DR: 通过约束马尔可夫决策过程和自适应拉格朗日乘数，在机器人模仿学习中实现风格学习和任务性能的平衡。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在实践演示不完整或不现实时，为了提升风格而牺牲任务性能的问题。

Method: 将问题形式化为约束马尔可夫决策过程（CMDP），优化风格模仿目标并加入约束以维持接近最优的任务性能，引入自适应可调的拉格朗日乘数来指导智能体选择性模仿演示，从而捕捉风格的细微差别而不影响任务性能。

Result: 在ANYmal-D硬件上，实现了14.5%的机械能降低和更敏捷的步态模式，证明了该方法在多机器人平台和任务上的鲁棒任务性能和高保真风格学习能力。

Conclusion: 所提出的方法通过约束马尔可夫决策过程（CMDP）优化了风格模仿目标，并引入了自适应可调的拉格朗日乘数，以在不损害任务性能的情况下选择性地模仿演示，成功实现了高保真风格学习和鲁棒的任务性能，并在ANYmal-D硬件上实现了更敏捷的步态模式和14.5%的机械能降低。

Abstract: Learning from demonstration has proven effective in robotics for acquiring
natural behaviors, such as stylistic motions and lifelike agility, particularly
when explicitly defining style-oriented reward functions is challenging.
Synthesizing stylistic motions for real-world tasks usually requires balancing
task performance and imitation quality. Existing methods generally depend on
expert demonstrations closely aligned with task objectives. However, practical
demonstrations are often incomplete or unrealistic, causing current methods to
boost style at the expense of task performance. To address this issue, we
propose formulating the problem as a constrained Markov Decision Process
(CMDP). Specifically, we optimize a style-imitation objective with constraints
to maintain near-optimal task performance. We introduce an adaptively
adjustable Lagrangian multiplier to guide the agent to imitate demonstrations
selectively, capturing stylistic nuances without compromising task performance.
We validate our approach across multiple robotic platforms and tasks,
demonstrating both robust task performance and high-fidelity style learning. On
ANYmal-D hardware we show a 14.5% drop in mechanical energy and a more agile
gait pattern, showcasing real-world benefits.

</details>


### [583] [Real-Time Adaptive Motion Planning via Point Cloud-Guided, Energy-Based Diffusion and Potential Fields](https://arxiv.org/abs/2507.09383)
*Wondmgezahu Teshome,Kian Behzad,Octavia Camps,Michael Everett,Milad Siami,Mario Sznaier*

Main category: cs.RO

TL;DR: 提出了一种新的运动规划框架，结合扩散模型和势场，用于在复杂环境中实时生成追逐-逃避轨迹。


<details>
  <summary>Details</summary>
Motivation: 为了解决追逐-逃避问题，提出了一种结合扩散模型和人工势场的方法，用于在复杂环境中进行鲁棒的实时运动规划。

Method: 结合了基于能量的扩散模型和人工势场，直接处理点云障碍物信息，并采用分类器指导训练和局部势场采样来增强避障能力。在动态场景中，首先使用扩散模型生成初始轨迹，然后通过基于势场的方法进行自适应调整。

Result: 该方法能够高效地进行规划，而无需完整的几何表示，并在动态追逐-逃避场景中有效运行。

Conclusion: 该框架在复杂环境中实现了鲁棒的实时轨迹生成，特别是在部分可观察的追逐-逃跑场景中表现有效。

Abstract: Motivated by the problem of pursuit-evasion, we present a motion planning
framework that combines energy-based diffusion models with artificial potential
fields for robust real time trajectory generation in complex environments. Our
approach processes obstacle information directly from point clouds, enabling
efficient planning without requiring complete geometric representations. The
framework employs classifier-free guidance training and integrates local
potential fields during sampling to enhance obstacle avoidance. In dynamic
scenarios, the system generates initial trajectories using the diffusion model
and continuously refines them through potential field-based adaptation,
demonstrating effective performance in pursuit-evasion scenarios with partial
pursuer observability.

</details>


### [584] [Influence of Static and Dynamic Downwash Interactions on Multi-Quadrotor Systems](https://arxiv.org/abs/2507.09463)
*Anoop Kiran,Nora Ayanian,Kenneth Breuer*

Main category: cs.RO

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Flying multiple quadrotors in close proximity presents a significant
challenge due to complex aerodynamic interactions, particularly downwash
effects that are known to destabilize vehicles and degrade performance.
Traditionally, multi-quadrotor systems rely on conservative strategies, such as
collision avoidance zones around the robot volume, to circumvent this effect.
This restricts their capabilities by requiring a large volume for the operation
of a multi-quadrotor system, limiting their applicability in dense
environments. This work provides a comprehensive, data-driven analysis of the
downwash effect, with a focus on characterizing, analyzing, and understanding
forces, moments, and velocities in both single and multi-quadrotor
configurations. We use measurements of forces and torques to characterize
vehicle interactions, and particle image velocimetry (PIV) to quantify the
spatial features of the downwash wake for a single quadrotor and an interacting
pair of quadrotors. This data can be used to inform physics-based strategies
for coordination, leverage downwash for optimized formations, expand the
envelope of operation, and improve the robustness of multi-quadrotor control.

</details>


### [585] [Multi-residual Mixture of Experts Learning for Cooperative Control in Multi-vehicle Systems](https://arxiv.org/abs/2507.09836)
*Vindula Jayawardana,Sirui Li,Yashar Farid,Cathy Wu*

Main category: cs.RO

TL;DR: MRMEL是一种用于自动驾驶汽车的拉格朗日交通控制框架，通过残差学习和专家混合来提高策略在多样化交通场景下的鲁棒性和性能，并在生态驾驶任务中显著减少了车辆排放。


<details>
  <summary>Details</summary>
Motivation: 设计能够泛化到不同交通场景的有效拉格朗日交通控制策略是一个重大挑战，因为真实交通环境高度多样化，且交通系统具有多智能体、混合动机以及受物理和外部约束的冲突优化目标等复杂性。

Method: 提出了一种名为多残差专家混合学习（MRMEL）的新型框架，用于拉格朗日交通控制。该框架通过学习残差来增强现有的次优名义策略，并显式考虑交通场景空间的结构。MRMEL借鉴了残差强化学习的思想，通过学习残差校正来增强次优的自动驾驶汽车（AV）控制策略，同时根据交通场景动态选择最合适的策略，这些策略被建模为专家混合。

Result: MRMEL框架在合作生态驾驶场景下得到了验证，在亚特兰大、达拉斯-沃斯堡和盐湖城使用了真实数据驱动的交通场景。实验结果表明，MRMEL在每个场景下相对于最强的基线都能持续获得更优的性能，实现了额外4%-9%的聚合车辆减排。

Conclusion: MRMEL框架通过学习残差修正并动态选择最适合的专家策略，能够有效应对交通环境的多样性和复杂性，并在合作生态驾驶场景中实现优于现有基线的性能，额外减少4%-9%的车辆排放。

Abstract: Autonomous vehicles (AVs) are becoming increasingly popular, with their
applications now extending beyond just a mode of transportation to serving as
mobile actuators of a traffic flow to control flow dynamics. This contrasts
with traditional fixed-location actuators, such as traffic signals, and is
referred to as Lagrangian traffic control. However, designing effective
Lagrangian traffic control policies for AVs that generalize across traffic
scenarios introduces a major challenge. Real-world traffic environments are
highly diverse, and developing policies that perform robustly across such
diverse traffic scenarios is challenging. It is further compounded by the joint
complexity of the multi-agent nature of traffic systems, mixed motives among
participants, and conflicting optimization objectives subject to strict
physical and external constraints. To address these challenges, we introduce
Multi-Residual Mixture of Expert Learning (MRMEL), a novel framework for
Lagrangian traffic control that augments a given suboptimal nominal policy with
a learned residual while explicitly accounting for the structure of the traffic
scenario space. In particular, taking inspiration from residual reinforcement
learning, MRMEL augments a suboptimal nominal AV control policy by learning a
residual correction, but at the same time dynamically selects the most suitable
nominal policy from a pool of nominal policies conditioned on the traffic
scenarios and modeled as a mixture of experts. We validate MRMEL using a case
study in cooperative eco-driving at signalized intersections in Atlanta, Dallas
Fort Worth, and Salt Lake City, with real-world data-driven traffic scenarios.
The results show that MRMEL consistently yields superior performance-achieving
an additional 4%-9% reduction in aggregate vehicle emissions relative to the
strongest baseline in each setting.

</details>


### [586] [Unmanned Aerial Vehicle (UAV) Data-Driven Modeling Software with Integrated 9-Axis IMUGPS Sensor Fusion and Data Filtering Algorithm](https://arxiv.org/abs/2507.09464)
*Azfar Azdi Arfakhsyad,Aufa Nasywa Rahman,Larasati Kinanti,Ahmad Ataka Awwalur Rizqi,Hannan Nur Muhammad*

Main category: cs.RO

TL;DR: 该研究提出了一种利用IMU和GPS数据进行无人机建模的数据驱动软件，解决了万向节锁定问题，并通过传感器融合提高了定位精度和流畅性。


<details>
  <summary>Details</summary>
Motivation: 无人机（UAV）已成为多功能平台，推动了对精确建模以支持开发测试的需求。

Method: 提出数据驱动的无人机建模软件，利用成本效益高的传感器获取定向和位置数据，并通过数据滤波算法和传感器融合技术处理数据以提高数据质量，从而在软件中实现精确的模型可视化。无人机的定向使用处理后的惯性测量单元（IMU）数据获得，并使用四元数表示来避免万向节锁定问题。无人机的位置结合了提供稳定地理坐标但数据更新频率较慢的全球定位系统（GPS）以及数据更新频率较高但集成位置数据不稳定（因累积误差）的加速度计的数据。

Result: 结果表明，该软件能够有效地渲染无人机的姿态和位置，并具有高度的准确性和流畅性。

Conclusion: 该软件能够有效地渲染无人机的姿态和位置，并具有高度的准确性和流畅性。

Abstract: Unmanned Aerial Vehicles (UAV) have emerged as versatile platforms, driving
the demand for accurate modeling to support developmental testing. This paper
proposes data-driven modeling software for UAV. Emphasizes the utilization of
cost-effective sensors to obtain orientation and location data subsequently
processed through the application of data filtering algorithms and sensor
fusion techniques to improve the data quality to make a precise model
visualization on the software. UAV's orientation is obtained using processed
Inertial Measurement Unit (IMU) data and represented using Quaternion
Representation to avoid the gimbal lock problem. The UAV's location is
determined by combining data from the Global Positioning System (GPS), which
provides stable geographic coordinates but slower data update frequency, and
the accelerometer, which has higher data update frequency but integrating it to
get position data is unstable due to its accumulative error. By combining data
from these two sensors, the software is able to calculate and continuously
update the UAV's real-time position during its flight operations. The result
shows that the software effectively renders UAV orientation and position with
high degree of accuracy and fluidity

</details>


### [587] [mmE-Loc: Facilitating Accurate Drone Landing with Ultra-High-Frequency Localization](https://arxiv.org/abs/2507.09469)
*Haoyang Wang,Jingao Xu,Xinyu Luo,Ting Zhang,Xuecheng Chen,Ruiyang Duan,Jialong Chen,Yunhao Liu,Jianfeng Zheng,Weijie Hong,Xinlei Chen*

Main category: cs.RO

TL;DR: 通过使用事件相机升级传统帧相机，并结合毫米波雷达，提出了一种名为 mmE-Loc 的高精度、低延迟地面定位系统，用于精确的无人机着陆。


<details>
  <summary>Details</summary>
Motivation: 为了实现精确、高效和安全的无人机着陆，地面平台需要实时、准确地定位下降的无人机并将其引导至指定位置。传统帧相机与毫米波雷达相比采样频率较低，会成为系统吞吐量的瓶颈。本工作旨在解决此问题。

Method: 提出了一致性指导的协同跟踪模块（利用无人机的物理运动和结构知识进行精确测量提取）和图信息自适应联合优化模块（集成无人机运动信息以进行有效的传感器融合和无人机定位）。

Result: 在无人机配送公司的实际着陆场景中进行的真实世界实验表明，mmE-Loc 在精度和延迟方面均显著优于最先进的方法。

Conclusion: mmE-Loc 显著优于最先进的方法，在精度和延迟方面都有提升。

Abstract: For precise, efficient, and safe drone landings, ground platforms should
real-time, accurately locate descending drones and guide them to designated
spots. While mmWave sensing combined with cameras improves localization
accuracy, lower sampling frequency of traditional frame cameras compared to
mmWave radar creates bottlenecks in system throughput. In this work, we upgrade
traditional frame camera with event camera, a novel sensor that harmonizes in
sampling frequency with mmWave radar within ground platform setup, and
introduce mmE-Loc, a high-precision, low-latency ground localization system
designed for precise drone landings. To fully exploit the \textit{temporal
consistency} and \textit{spatial complementarity} between these two modalities,
we propose two innovative modules: \textit{(i)} the Consistency-instructed
Collaborative Tracking module, which further leverages the drone's physical
knowledge of periodic micro-motions and structure for accurate measurements
extraction, and \textit{(ii)} the Graph-informed Adaptive Joint Optimization
module, which integrates drone motion information for efficient sensor fusion
and drone localization. Real-world experiments conducted in landing scenarios
with a drone delivery company demonstrate that mmE-Loc significantly
outperforms state-of-the-art methods in both accuracy and latency.

</details>


### [588] [TruckV2X: A Truck-Centered Perception Dataset](https://arxiv.org/abs/2507.09505)
*Tenghui Xie,Zhiying Song,Fuxi Wen,Jun Li,Guangzhao Liu,Zijian Zhao*

Main category: cs.RO

TL;DR: TruckV2X：首个大规模卡车中心协作感知数据集，用于改善自动驾驶卡车的感知和遮挡处理能力。


<details>
  <summary>Details</summary>
Motivation: 为了解决自动驾驶卡车因其庞大的体积和动态的拖车运动所带来的感知挑战，例如广泛的盲点和遮挡，而现有的数据集主要关注轻型车辆的交互或缺乏重型车辆场景的多智能体配置。

Method: 介绍了TruckV2X数据集，这是第一个以卡车为中心的大规模协作感知数据集，包含多模态传感（激光雷达和摄像头）和多智能体协作（牵引车、拖车、自动驾驶汽车和路侧单元）。

Result: 建立了性能基准，并提出了重型车辆感知的研究重点。

Conclusion: 该数据集为开发具有增强遮挡处理能力的协作感知系统奠定了基础，并加速了多智能体自动驾驶卡车系统的部署。

Abstract: Autonomous trucking offers significant benefits, such as improved safety and
reduced costs, but faces unique perception challenges due to trucks' large size
and dynamic trailer movements. These challenges include extensive blind spots
and occlusions that hinder the truck's perception and the capabilities of other
road users. To address these limitations, cooperative perception emerges as a
promising solution. However, existing datasets predominantly feature light
vehicle interactions or lack multi-agent configurations for heavy-duty vehicle
scenarios. To bridge this gap, we introduce TruckV2X, the first large-scale
truck-centered cooperative perception dataset featuring multi-modal sensing
(LiDAR and cameras) and multi-agent cooperation (tractors, trailers, CAVs, and
RSUs). We further investigate how trucks influence collaborative perception
needs, establishing performance benchmarks while suggesting research priorities
for heavy vehicle perception. The dataset provides a foundation for developing
cooperative perception systems with enhanced occlusion handling capabilities,
and accelerates the deployment of multi-agent autonomous trucking systems. The
TruckV2X dataset is available at
https://huggingface.co/datasets/XieTenghu1/TruckV2X.

</details>


### [589] [Prompt Informed Reinforcement Learning for Visual Coverage Path Planning](https://arxiv.org/abs/2507.10284)
*Venkat Margapuri*

Main category: cs.RO

TL;DR: PIRL通过结合LLM的语义反馈和好奇心驱动的强化学习，改进了无人机的视觉覆盖路径规划，在提高覆盖率、电池效率和降低冗余度方面表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统的强化学习（RL）方法依赖于特定环境的奖励公式，缺乏语义适应性，这在无人机（UAV）视觉覆盖路径规划中是一个挑战。

Method: 提出了一种新颖的提示信息强化学习（PIRL）方法，该方法将大型语言模型（LLM）的零样本推理能力和上下文学习能力与好奇心驱动的强化学习相结合。PIRL 利用 LLM（GPT-3.5）的语义反馈来动态塑造代理位置和相机调整以实现最佳视觉覆盖的近端策略优化（PPO）强化学习策略的奖励函数。

Result: PIRL 在 OpenAI Gym 环境中实现了高达 14% 的视觉覆盖率，在 Webots 模拟器中实现了高达 27% 的视觉覆盖率，同时提高了电池效率并降低了冗余度，在各项指标上均优于包括 PPO（静态奖励）、PPO（探索性权重初始化）、模仿学习和纯 LLM 控制器在内的多种基线方法。

Conclusion: 研究结果突出了语言模型引导的奖励塑造在复杂空间探索任务中的有效性，并为将自然语言先验知识整合到机器人强化学习中指明了一个有前景的方向。

Abstract: Visual coverage path planning with unmanned aerial vehicles (UAVs) requires
agents to strategically coordinate UAV motion and camera control to maximize
coverage, minimize redundancy, and maintain battery efficiency. Traditional
reinforcement learning (RL) methods rely on environment-specific reward
formulations that lack semantic adaptability. This study proposes
Prompt-Informed Reinforcement Learning (PIRL), a novel approach that integrates
the zero-shot reasoning ability and in-context learning capability of large
language models with curiosity-driven RL. PIRL leverages semantic feedback from
an LLM, GPT-3.5, to dynamically shape the reward function of the Proximal
Policy Optimization (PPO) RL policy guiding the agent in position and camera
adjustments for optimal visual coverage. The PIRL agent is trained using OpenAI
Gym and evaluated in various environments. Furthermore, the sim-to-real-like
ability and zero-shot generalization of the agent are tested by operating the
agent in Webots simulator which introduces realistic physical dynamics. Results
show that PIRL outperforms multiple learning-based baselines such as PPO with
static rewards, PPO with exploratory weight initialization, imitation learning,
and an LLM-only controller. Across different environments, PIRL outperforms the
best-performing baseline by achieving up to 14% higher visual coverage in
OpenAI Gym and 27% higher in Webots, up to 25% higher battery efficiency, and
up to 18\% lower redundancy, depending on the environment. The results
highlight the effectiveness of LLM-guided reward shaping in complex spatial
exploration tasks and suggest a promising direction for integrating natural
language priors into RL for robotics.

</details>


### [590] [Self-supervised Pretraining for Integrated Prediction and Planning of Automated Vehicles](https://arxiv.org/abs/2507.09537)
*Yangang Ren,Guojian Zhan,Chen Lv,Jun Li,Fenghua Liang,Keqiang Li*

Main category: cs.RO

TL;DR: Plan-MAE是一个利用掩码自动编码器进行预测和规划的预训练框架，通过重建道路网络、代理轨迹和导航路线来理解上下文，并通过子规划任务调整车辆动力学和安全约束，实验结果表明其在规划指标上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 预测周围代理的未来并据此规划安全的、以目标为导向的轨迹对于自动驾驶汽车至关重要。现有方法通常依赖于模仿学习来优化针对地面真实的指标，但往往忽略了场景理解如何能够实现更具整体性的轨迹。

Method: Plan-MAE框架通过三个专门的任务来融合关键的上下文理解：重建掩码道路网络以学习空间相关性，代理轨迹以模拟社会互动，以及导航路线以捕捉目的地意图。为了进一步调整车辆动力学和安全约束，我们引入了一个局部的子规划任务，该任务根据早期片段对自我车辆的近期轨迹段进行预测。然后，对预训练模型进行下游任务的微调，以联合生成预测和规划轨迹。

Result: Plan-MAE大幅优于现有方法，可以作为学习型运动规划器的一个重要预训练步骤。

Conclusion: Plan-MAE是一个统一的预训练框架，用于预测和规划，它利用了掩码自动编码器。实验表明，Plan-MAE在规划指标上大幅优于现有方法，并且可以作为学习型运动规划器的一个重要预训练步骤。

Abstract: Predicting the future of surrounding agents and accordingly planning a safe,
goal-directed trajectory are crucial for automated vehicles. Current methods
typically rely on imitation learning to optimize metrics against the ground
truth, often overlooking how scene understanding could enable more holistic
trajectories. In this paper, we propose Plan-MAE, a unified pretraining
framework for prediction and planning that capitalizes on masked autoencoders.
Plan-MAE fuses critical contextual understanding via three dedicated tasks:
reconstructing masked road networks to learn spatial correlations, agent
trajectories to model social interactions, and navigation routes to capture
destination intents. To further align vehicle dynamics and safety constraints,
we incorporate a local sub-planning task predicting the ego-vehicle's near-term
trajectory segment conditioned on earlier segment. This pretrained model is
subsequently fine-tuned on downstream tasks to jointly generate the prediction
and planning trajectories. Experiments on large-scale datasets demonstrate that
Plan-MAE outperforms current methods on the planning metrics by a large margin
and can serve as an important pre-training step for learning-based motion
planner.

</details>


### [591] [On the Importance of Neural Membrane Potential Leakage for LIDAR-based Robot Obstacle Avoidance using Spiking Neural Networks](https://arxiv.org/abs/2507.09538)
*Zainab Ali,Lujayn Al-Amir,Ali Safa*

Main category: cs.RO

TL;DR: 本文研究了在机器人导航和避障中使用脉冲神经网络（SNN），重点关注了神经元膜电位泄漏对LIDAR数据处理精度的影响。结果表明，通过调整泄漏常数，SNN可以达到与CNN相当的性能。此外，本文还发布了一个用于此任务的LIDAR数据集。


<details>
  <summary>Details</summary>
Motivation: 由于SNN在神经形态硬件上实现时具有高精度、低内存和低计算复杂度推理的卓越能力，因此在机器人应用中使用神经形态计算近年来备受关注。这种能力使SNN非常适合自主机器人应用（例如无人机和漫游车），因为这些应用中的电池资源和有效载荷通常有限。

Method: 建立了配备有LIDAR的定制机器人平台，用于收集带有标签的LIDAR传感数据以及用于避障的人类操作机器人控制命令。研究了将SNN用于从LIDAR数据执行机器人导航和避障。

Result: 研究表明，通过仔细调整神经元的膜电位泄漏常数，可以实现与CNN相当的机器人控制精度。

Conclusion: 通过仔细调整SNN中使用的脉冲式Leaky Integrate-and-Fire (LIF)神经元的膜电位泄漏常数，可以在机器人控制精度方面与非脉冲式卷积神经网络（CNN）相媲美。

Abstract: Using neuromorphic computing for robotics applications has gained much
attention in recent year due to the remarkable ability of Spiking Neural
Networks (SNNs) for high-precision yet low memory and compute complexity
inference when implemented in neuromorphic hardware. This ability makes SNNs
well-suited for autonomous robot applications (such as in drones and rovers)
where battery resources and payload are typically limited. Within this context,
this paper studies the use of SNNs for performing direct robot navigation and
obstacle avoidance from LIDAR data. A custom robot platform equipped with a
LIDAR is set up for collecting a labeled dataset of LIDAR sensing data together
with the human-operated robot control commands used for obstacle avoidance.
Crucially, this paper provides what is, to the best of our knowledge, a first
focused study about the importance of neuron membrane leakage on the SNN
precision when processing LIDAR data for obstacle avoidance. It is shown that
by carefully tuning the membrane potential leakage constant of the spiking
Leaky Integrate-and-Fire (LIF) neurons used within our SNN, it is possible to
achieve on-par robot control precision compared to the use of a non-spiking
Convolutional Neural Network (CNN). Finally, the LIDAR dataset collected during
this work is released as open-source with the hope of benefiting future
research.

</details>


### [592] [IteraOptiRacing: A Unified Planning-Control Framework for Real-time Autonomous Racing for Iterative Optimal Performance](https://arxiv.org/abs/2507.09714)
*Yifan Zeng,Yihan Li,Suiyi He,Koushil Sreenath,Jun Zeng*

Main category: cs.RO

TL;DR: IteraOptiRacing是一种新的统一规划-控制策略，通过i2LQR和历史数据优化，在自动赛车中实现避障和时间最优，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 为了在自动赛车环境中与其他赛车竞争，并提高在存在周围赛车障碍物时的单圈时间性能。

Method: 提出了一种基于迭代线性二次调节器用于迭代任务（i2LQR）的统一规划-控制策略，通过迭代使用赛车自身的历史数据，并在该统一策略中同时考虑了多个移动障碍物的避障和时间成本优化，从而生成无碰撞且时间最优的轨迹。

Result: 所提出的策略在所有随机生成的自动赛车场景中均优于现有方法，实现了增强的自赛车操控性。

Conclusion: 该策略在所有随机生成的自动赛车场景中均优于现有方法，能够增强自赛车的高级操控性。

Abstract: This paper presents a unified planning-control strategy for competing with
other racing cars called IteraOptiRacing in autonomous racing environments.
This unified strategy is proposed based on Iterative Linear Quadratic Regulator
for Iterative Tasks (i2LQR), which can improve lap time performance in the
presence of surrounding racing obstacles. By iteratively using the ego car's
historical data, both obstacle avoidance for multiple moving cars and time cost
optimization are considered in this unified strategy, resulting in
collision-free and time-optimal generated trajectories. The algorithm's
constant low computation burden and suitability for parallel computing enable
real-time operation in competitive racing scenarios. To validate its
performance, simulations in a high-fidelity simulator are conducted with
multiple randomly generated dynamic agents on the track. Results show that the
proposed strategy outperforms existing methods across all randomly generated
autonomous racing scenarios, enabling enhanced maneuvering for the ego racing
car.

</details>


### [593] [Visual Homing in Outdoor Robots Using Mushroom Body Circuits and Learning Walks](https://arxiv.org/abs/2507.09725)
*Gabriel G. Gattaux,Julien R. Serres,Franck Ruffier,Antoine Wystrach*

Main category: cs.RO

TL;DR: 受蚂蚁启发，研究人员开发了一种新颖的横向蘑菇体架构，用于自主视觉归巢。该系统在真实机器人上运行，能成功地在各种环境中导航和定位目标，其效率高且占用的内存少。


<details>
  <summary>Details</summary>
Motivation: 受蚂蚁在极简的感官输入和有限的学习行走下实现鲁棒视觉归巢的启发，旨在为自主导航提供仿生解决方案。

Method: 一种横向性蘑菇体（MB）架构，用于在紧凑型自主仿车机器人上进行视觉归巢。通过将角度路径积分（PI）信号的符号分类全景视图来实现这一点，该视图在学习行走期间获取并编码在 MB 中，并将其分为“目标在左”和“目标在右”记忆库。

Result: 该方法通过四个实验得到验证：(1) 模拟显示类似吸引子的巢穴动力学；(2) 在分离的学习行走后进行真实世界归巢，产生巢穴搜索行为；(3) 在使用 GPS-RTK 模拟的噪声 PI 的随机行走后进行归巢；(4) 通过编码目标视图的第五个 MB 输出神经元（MBON）控制速度，实现了精确的目标停止行为。

Conclusion: 该系统是一种受生物启发的、资源高效的自主视觉归巢解决方案，在 Raspberry Pi 4 上以 8 Hz 的频率运行，内存占用量低于 9 kB，可在 32x32 像素的视图和内存占用量下实现精确的以视觉为中心的归巢。

Abstract: Ants achieve robust visual homing with minimal sensory input and only a few
learning walks, inspiring biomimetic solutions for autonomous navigation. While
Mushroom Body (MB) models have been used in robotic route following, they have
not yet been applied to visual homing. We present the first real-world
implementation of a lateralized MB architecture for visual homing onboard a
compact autonomous car-like robot. We test whether the sign of the angular path
integration (PI) signal can categorize panoramic views, acquired during
learning walks and encoded in the MB, into "goal on the left" and "goal on the
right" memory banks, enabling robust homing in natural outdoor settings. We
validate this approach through four incremental experiments: (1) simulation
showing attractor-like nest dynamics; (2) real-world homing after decoupled
learning walks, producing nest search behavior; (3) homing after random walks
using noisy PI emulated with GPS-RTK; and (4) precise stopping-at-the-goal
behavior enabled by a fifth MB Output Neuron (MBON) encoding goal-views to
control velocity. This mimics the accurate homing behavior of ants and
functionally resembles waypoint-based position control in robotics, despite
relying solely on visual input. Operating at 8 Hz on a Raspberry Pi 4 with
32x32 pixel views and a memory footprint under 9 kB, our system offers a
biologically grounded, resource-efficient solution for autonomous visual
homing.

</details>


### [594] [Active Probing with Multimodal Predictions for Motion Planning](https://arxiv.org/abs/2507.09822)
*Darshan Gadginmath,Farhad Nawaz,Minjun Sung,Faizan M Tariq,Sangjae Bae,David Isele,Fabio Pasqualetti,Jovin Dsa*

Main category: cs.RO

TL;DR: 该研究提出了一种结合轨迹规划、多模态预测和主动探测的统一框架，以增强在动态环境中的决策能力。该框架引入了一种新的风险度量，能够处理高斯混合模型中的不确定性，并具有闭式解。通过主动探测，系统能够更好地估计其他行为者的行为参数，从而减少预测歧义。在 MetaDrive 模拟环境中的评估结果表明，该方法在复杂的交通场景中表现出色，并对不同的行为模型具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在动态环境中导航需要自主系统能够推理其他行为者的行为不确定性。

Method: 我们开发了一种新颖的风险指标，通过混合模型无缝集成多模态预测不确定性。当这些不确定性遵循高斯混合分布时，我们证明了我们的风险指标具有闭式解，并且始终是有限的，从而确保了分析的可处理性。为了减少预测歧义，我们引入了一种主动探测机制，该机制优先选择能够改进对其他行为者行为参数的估计的动作，同时处理多模态不确定性。

Result: 结果表明，我们的主动探测方法成功导航了具有不确定预测的复杂交通场景。此外，我们的框架在各种交通行为模型中表现出稳健的性能，表明其在现实世界自动导航挑战中的广泛适用性。

Conclusion: 该框架在 MetaDrive 模拟环境中进行了广泛评估，结果表明我们的主动探测方法成功导航了具有不确定预测的复杂交通场景。此外，我们的框架在各种交通行为模型中表现出稳健的性能，表明其在现实世界自动导航挑战中的广泛适用性。

Abstract: Navigation in dynamic environments requires autonomous systems to reason
about uncertainties in the behavior of other agents. In this paper, we
introduce a unified framework that combines trajectory planning with multimodal
predictions and active probing to enhance decision-making under uncertainty. We
develop a novel risk metric that seamlessly integrates multimodal prediction
uncertainties through mixture models. When these uncertainties follow a
Gaussian mixture distribution, we prove that our risk metric admits a
closed-form solution, and is always finite, thus ensuring analytical
tractability. To reduce prediction ambiguity, we incorporate an active probing
mechanism that strategically selects actions to improve its estimates of
behavioral parameters of other agents, while simultaneously handling multimodal
uncertainties. We extensively evaluate our framework in autonomous navigation
scenarios using the MetaDrive simulation environment. Results demonstrate that
our active probing approach successfully navigates complex traffic scenarios
with uncertain predictions. Additionally, our framework shows robust
performance across diverse traffic agent behavior models, indicating its broad
applicability to real-world autonomous navigation challenges. Code and videos
are available at
https://darshangm.github.io/papers/active-probing-multimodal-predictions/.

</details>


### [595] [AdvGrasp: Adversarial Attacks on Robotic Grasping from a Physical Perspective](https://arxiv.org/abs/2507.09857)
*Xiaofei Wang,Mingliang Han,Tianyu Hao,Cegang Li,Yunbo Zhao,Keke Tang*

Main category: cs.RO

TL;DR: AdvGrasp 通过物理形变攻击机器人抓取，降低其提起能力和稳定性，实验和现实验证均有效。


<details>
  <summary>Details</summary>
Motivation: 旨在评估和改进机器人抓取系统的鲁棒性，填补了现有研究仅关注神经网络预测而忽略抓取物理原理的空白。

Method: AdvGrasp 框架通过对物体进行形变，旨在增加重力扭矩并减小抓取稳定裕度（在力/力矩空间中），从而系统性地降低抓取能力（抵抗重力的提起能力）和抓取稳定性（抵抗外部干扰的能力）。

Result: AdvGrasp 框架能够系统性地降低抓取能力和抓取稳定性，实验结果表明其有效性，真实世界验证也证明了其鲁棒性和实用性。

Conclusion: 本研究提出的 AdvGrasp 框架通过物理层面上的形变攻击，有效地降低了机器人抓取的抓取能力和稳定性，并在多种场景下通过广泛的实验和真实世界的验证，证明了其有效性、鲁棒性和实际应用潜力。

Abstract: Adversarial attacks on robotic grasping provide valuable insights into
evaluating and improving the robustness of these systems. Unlike studies that
focus solely on neural network predictions while overlooking the physical
principles of grasping, this paper introduces AdvGrasp, a framework for
adversarial attacks on robotic grasping from a physical perspective.
Specifically, AdvGrasp targets two core aspects: lift capability, which
evaluates the ability to lift objects against gravity, and grasp stability,
which assesses resistance to external disturbances. By deforming the object's
shape to increase gravitational torque and reduce stability margin in the
wrench space, our method systematically degrades these two key grasping
metrics, generating adversarial objects that compromise grasp performance.
Extensive experiments across diverse scenarios validate the effectiveness of
AdvGrasp, while real-world validations demonstrate its robustness and practical
applicability

</details>


### [596] [Customize Harmonic Potential Fields via Hybrid Optimization over Homotopic Paths](https://arxiv.org/abs/2507.09858)
*Shuaikang Wang,Tiecheng Guo,Meng Guo*

Main category: cs.RO

TL;DR: 提出了一种新颖的谐波势场方法，可自动寻找路径的同伦类，并允许自定义复杂工作空间（如森林世界）中路径的拓扑性质。


<details>
  <summary>Details</summary>
Motivation: 现有谐波势场方法在路径自定义和拓扑性质方面存在局限性，难以应对包含复杂重叠障碍物的导航场景。

Method: 该方法基于混合优化算法，在同伦类中进行搜索，选择每个森林内的星形树结构，并通过投影梯度下降优化连续权重参数。关键在于通过适当的微分同胚变换将森林世界转化为无界点世界，从而简化了非同伦路径之间的多方向D签名的设计，同时保留了安全性和收敛性。

Result: 通过广泛的模拟和硬件实验验证了该方法在非平凡场景下的有效性，能够为期望的同伦特性定制导航势场。

Conclusion: 所提出的方法能够为复杂的包含大量重叠星形障碍物的森林世界生成路径，并允许自定义路径的拓扑性质，同时保持了安全性和收敛性。

Abstract: Safe navigation within a workspace is a fundamental skill for autonomous
robots to accomplish more complex tasks. Harmonic potentials are artificial
potential fields that are analytical, globally convergent and provably free of
local minima. Thus, it has been widely used for generating safe and reliable
robot navigation control policies. However, most existing methods do not allow
customization of the harmonic potential fields nor the resulting paths,
particularly regarding their topological properties. In this paper, we propose
a novel method that automatically finds homotopy classes of paths that can be
generated by valid harmonic potential fields. The considered complex workspaces
can be as general as forest worlds consisting of numerous overlapping
star-obstacles. The method is based on a hybrid optimization algorithm that
searches over homotopy classes, selects the structure of each tree-of-stars
within the forest, and optimizes over the continuous weight parameters for each
purged tree via the projected gradient descent. The key insight is to transform
the forest world to the unbounded point world via proper diffeomorphic
transformations. It not only facilitates a simpler design of the
multi-directional D-signature between non-homotopic paths, but also retain the
safety and convergence properties. Extensive simulations and hardware
experiments are conducted for non-trivial scenarios, where the navigation
potentials are customized for desired homotopic properties. Project page:
https://shuaikang-wang.github.io/CustFields.

</details>


### [597] [Demonstrating the Octopi-1.5 Visual-Tactile-Language Model](https://arxiv.org/abs/2507.09985)
*Samson Yu,Kelvin Lin,Harold Soh*

Main category: cs.RO

TL;DR: 演示了Octopi-1.5，一个能处理多部分触觉信号并利用RAG学习新物体的视觉-触觉-语言模型。通过手持设备可现场体验其在物体识别和处理建议方面的能力，并展示了其优势与局限性。


<details>
  <summary>Details</summary>
Motivation: 触觉对人类和机器人操作至关重要，尤其是在灵巧操控、材料识别和视觉遮挡场景中。本演示基于触觉基础模型，展示了最新的视觉-触觉-语言模型Octopi-1.5。

Method: 使用多部分触觉信号处理能力，并集成检索增强生成（RAG）模块，以提升任务性能和支持新物体即时学习。通过手持式触觉接口TMI（配备GelSight和TAC-02触觉传感器）进行现场交互体验。

Result: Octopi-1.5能够通过触觉输入和常识知识解决触觉推理任务，例如在“猜谜游戏”中识别被抓握的物体，并提供处理建议（如对软水果建议小心处理）。此外，还演示了其RAG能力，能够学习新物体。

Conclusion: 本次演示旨在展示Octopi-1.5在视觉-触觉-语言模型（VTLM）方面的进展和局限性，并激发对该领域的兴趣。提供的代码和设计文件便于社区进一步研究。

Abstract: Touch is recognized as a vital sense for humans and an equally important
modality for robots, especially for dexterous manipulation, material
identification, and scenarios involving visual occlusion. Building upon very
recent work in touch foundation models, this demonstration will feature
Octopi-1.5, our latest visual-tactile-language model. Compared to its
predecessor, Octopi-1.5 introduces the ability to process tactile signals from
multiple object parts and employs a simple retrieval-augmented generation (RAG)
module to improve performance on tasks and potentially learn new objects
on-the-fly. The system can be experienced live through a new handheld
tactile-enabled interface, the TMI, equipped with GelSight and TAC-02 tactile
sensors. This convenient and accessible setup allows users to interact with
Octopi-1.5 without requiring a robot. During the demonstration, we will
showcase Octopi-1.5 solving tactile inference tasks by leveraging tactile
inputs and commonsense knowledge. For example, in a Guessing Game, Octopi-1.5
will identify objects being grasped and respond to follow-up queries about how
to handle it (e.g., recommending careful handling for soft fruits). We also
plan to demonstrate Octopi-1.5's RAG capabilities by teaching it new items.
With live interactions, this demonstration aims to highlight both the progress
and limitations of VTLMs such as Octopi-1.5 and to foster further interest in
this exciting field. Code for Octopi-1.5 and design files for the TMI gripper
are available at https://github.com/clear-nus/octopi-1.5.

</details>


### [598] [Unscented Kalman Filter with a Nonlinear Propagation Model for Navigation Applications](https://arxiv.org/abs/2507.10082)
*Amit Levy,Itzik Klein*

Main category: cs.RO

TL;DR: An innovative method to propagate sigma points in the unscented Kalman filter improves accuracy and navigation performance, validated with real-world data from an autonomous underwater vehicle.


<details>
  <summary>Details</summary>
Motivation: The prediction of the mean and covariance matrix is crucial to the stable behavior of the unscented Kalman filter, commonly used in navigation applications. This prediction is done by propagating the sigma points according to the dynamic model.

Method: Propagating sigma points according to the nonlinear dynamic model of the navigation error state vector.

Result: Improved filter accuracy and navigation performance.

Conclusion: The proposed method for propagating sigma points in the unscented Kalman filter improves accuracy and navigation performance, as demonstrated with real sensor data from an autonomous underwater vehicle.

Abstract: The unscented Kalman filter is a nonlinear estimation algorithm commonly used
in navigation applications. The prediction of the mean and covariance matrix is
crucial to the stable behavior of the filter. This prediction is done by
propagating the sigma points according to the dynamic model at hand. In this
paper, we introduce an innovative method to propagate the sigma points
according to the nonlinear dynamic model of the navigation error state vector.
This improves the filter accuracy and navigation performance. We demonstrate
the benefits of our proposed approach using real sensor data recorded by an
autonomous underwater vehicle during several scenarios.

</details>


### [599] [Ariel Explores: Vision-based underwater exploration and inspection via generalist drone-level autonomy](https://arxiv.org/abs/2507.10003)
*Mohit Singh,Mihir Dharmadhikari,Kostas Alexis*

Main category: cs.RO

TL;DR: 本研究展示了一个名为 Ariel 的水下机器人，它拥有先进的视觉和惯性传感系统，能够进行自主探索和检查。该系统在恶劣的水下环境中表现出色，证明了其状态估计和路径规划技术的可靠性和适应性。


<details>
  <summary>Details</summary>
Motivation: 为了提供水下机器人（Ariel）的自主探索和通用视觉检查能力，就像无人机在空中一样。

Method: 提出了一种基于视觉的水下探索和检查自主解决方案，并集成了基于学习的本体感受机器人速度预测方法，以提高对视觉退化的鲁棒性。该系统使用包含 5 个摄像头和 IMU 的传感器套件，并采用了基于视觉-惯性的状态估计方法。

Result: 在具有挑战性的视觉条件下，现场演示显示了状态估计解决方案的鲁棒性以及路径规划技术在不同机器人实体上的通用性。

Conclusion: 该系统在特隆赫姆的一个潜艇干船坞中进行了现场测试，证明了其状态估计解决方案的鲁棒性以及路径规划技术在不同机器人实体上的通用性。

Abstract: This work presents a vision-based underwater exploration and inspection
autonomy solution integrated into Ariel, a custom vision-driven underwater
robot. Ariel carries a $5$ camera and IMU based sensing suite, enabling a
refraction-aware multi-camera visual-inertial state estimation method aided by
a learning-based proprioceptive robot velocity prediction method that enhances
robustness against visual degradation. Furthermore, our previously developed
and extensively field-verified autonomous exploration and general visual
inspection solution is integrated on Ariel, providing aerial drone-level
autonomy underwater. The proposed system is field-tested in a submarine dry
dock in Trondheim under challenging visual conditions. The field demonstration
shows the robustness of the state estimation solution and the generalizability
of the path planning techniques across robot embodiments.

</details>


### [600] [Finetuning Deep Reinforcement Learning Policies with Evolutionary Strategies for Control of Underactuated Robots](https://arxiv.org/abs/2507.10030)
*Marco Calì,Alberto Sinigaglia,Niccolò Turcato,Ruggero Carli,Gian Antonio Susto*

Main category: cs.RO

TL;DR: 通过使用进化策略（ES）对软Actor-Critic（SAC）训练的深度强化学习（DRL）代理进行微调，可以提高欠驱动机器人的控制性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 为了提高欠驱动机器人控制任务的性能和鲁棒性，需要对深度强化学习（DRL）策略进行微调。

Method: 该方法首先使用软Actor-Critic（SAC）和用于近似复杂特定评分指标的代理奖励函数来训练RL代理。然后，使用零阶优化步骤和可分离自然进化策略（SNES）直接针对原始分数来优化所学的策略。

Result: 该进化微调方法显著提高了代理性能，同时保持了高鲁棒性，并且在AI Olympics的机器人控制任务中取得了具有竞争力的分数，优于现有的基线方法。

Conclusion: 该方法通过进化策略（ES）对深度强化学习（DRL）策略进行了微调，以提高欠驱动机器人的控制性能。实验结果表明，该方法显著提高了代理性能，同时保持了高鲁棒性，并且优于现有的基线方法。

Abstract: Deep Reinforcement Learning (RL) has emerged as a powerful method for
addressing complex control problems, particularly those involving underactuated
robotic systems. However, in some cases, policies may require refinement to
achieve optimal performance and robustness aligned with specific task
objectives. In this paper, we propose an approach for fine-tuning Deep RL
policies using Evolutionary Strategies (ES) to enhance control performance for
underactuated robots. Our method involves initially training an RL agent with
Soft-Actor Critic (SAC) using a surrogate reward function designed to
approximate complex specific scoring metrics. We subsequently refine this
learned policy through a zero-order optimization step employing the Separable
Natural Evolution Strategy (SNES), directly targeting the original score.
Experimental evaluations conducted in the context of the 2nd AI Olympics with
RealAIGym at IROS 2024 demonstrate that our evolutionary fine-tuning
significantly improves agent performance while maintaining high robustness. The
resulting controllers outperform established baselines, achieving competitive
scores for the competition tasks.

</details>


### [601] [MP-RBFN: Learning-based Vehicle Motion Primitives using Radial Basis Function Networks](https://arxiv.org/abs/2507.10047)
*Marc Kaufeld,Mattia Piccinini,Johannes Betz*

Main category: cs.RO

TL;DR: MP-RBFN 通过结合采样方法和车辆动力学描述，提高了运动原语学习的效率和精度，适用于自动驾驶运动规划。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统基于优化的运动规划方法计算成本高昂，以及基于采样的方法对轨迹几何形状有限制的问题，本研究提出 MP-RBFN 来结合两者的优点。

Method: MP-RBFN 是一种利用径向基函数网络（Radial Basis Function Networks）来有效学习最优控制问题中为自动驾驶推导出的运动原语（Motion Primitives）的新颖方法。

Result: MP-RBFN 在实际应用中表现出比先前方法更优越的性能，能够以较低的推理时间精确描述运动原语，并且在生成优化运动原语方面比现有的半解析方法提高了七倍的精度。

Conclusion: MP-RBFN 结合了基于采样的方法的高保真轨迹生成能力和对车辆动力学的精确描述，在较低的推理时间下实现了运动原语的精确描述，并取得了优于现有半解析方法七倍的精度，展示了其在运动规划中的实际应用潜力。

Abstract: This research introduces MP-RBFN, a novel formulation leveraging Radial Basis
Function Networks for efficiently learning Motion Primitives derived from
optimal control problems for autonomous driving. While traditional motion
planning approaches based on optimization are highly accurate, they are often
computationally prohibitive. In contrast, sampling-based methods demonstrate
high performance but impose constraints on the geometric shape of trajectories.
MP-RBFN combines the strengths of both by coupling the high-fidelity trajectory
generation of sampling-based methods with an accurate description of vehicle
dynamics. Empirical results show compelling performance compared to previous
methods, achieving a precise description of motion primitives at low inference
times. MP-RBFN yields a seven times higher accuracy in generating optimized
motion primitives compared to existing semi-analytic approaches. We demonstrate
the practical applicability of MP-RBFN for motion planning by integrating the
method into a sampling-based trajectory planner. MP-RBFN is available as
open-source software at https://github.com/TUM-AVS/RBFN-Motion-Primitives.

</details>


### [602] [Hand Gesture Recognition for Collaborative Robots Using Lightweight Deep Learning in Real-Time Robotic Systems](https://arxiv.org/abs/2507.10055)
*Muhtadin,I Wayan Agus Darmawan,Muhammad Hilmi Rusydiansyah,I Ketut Eddy Purnama,Chastine Fatichah,Mauridhi Hery Purnomo*

Main category: cs.RO

TL;DR: 提出了一种非常轻量级（7KB）的深度学习手势识别系统，可用于直观地控制协作机器人，准确率达93.5%（UR5机器人）。


<details>
  <summary>Details</summary>
Motivation: 为了实现直观的人机协作，需要直接自然的交互方式，无需额外的外部设备。

Method: 提出了一种基于深度学习的手势识别系统，该系统参数量少（1,103个），模型小（22KB），精度达到93.5%。通过使用TensorFlow Lite进行量化和剪枝，模型大小进一步减小到7KB。该系统已成功集成到基于ROS2的通用机器人UR5的实时控制框架中。

Result: 该轻量级手势识别系统实现了93.5%的准确率，并将模型大小优化至7KB，成功应用于UR5协作机器人的实时控制，证明了轻量级模型在协作机器人控制中的有效性和响应性。

Conclusion: 该研究展示了轻量级深度学习模型在实现自然人机交互方面的潜力，即使在资源受限的环境中也能有效控制协作机器人。

Abstract: Direct and natural interaction is essential for intuitive human-robot
collaboration, eliminating the need for additional devices such as joysticks,
tablets, or wearable sensors. In this paper, we present a lightweight deep
learning-based hand gesture recognition system that enables humans to control
collaborative robots naturally and efficiently. This model recognizes eight
distinct hand gestures with only 1,103 parameters and a compact size of 22 KB,
achieving an accuracy of 93.5%. To further optimize the model for real-world
deployment on edge devices, we applied quantization and pruning using
TensorFlow Lite, reducing the final model size to just 7 KB. The system was
successfully implemented and tested on a Universal Robot UR5 collaborative
robot within a real-time robotic framework based on ROS2. The results
demonstrate that even extremely lightweight models can deliver accurate and
responsive hand gesture-based control for collaborative robots, opening new
possibilities for natural human-robot interaction in constrained environments.

</details>


### [603] [TGLD: A Trust-Aware Game-Theoretic Lane-Changing Decision Framework for Automated Vehicles in Heterogeneous Traffic](https://arxiv.org/abs/2507.10075)
*Jie Pan,Tianyi Wang,Yangyang Wang,Junfeng Jiao,Christian Claudel*

Main category: cs.RO

TL;DR: 该研究提出了一种新的车道变换决策框架（TGLD），通过整合信任评估，使自动驾驶汽车能更好地与人类驾驶员协作，提高效率和安全性。


<details>
  <summary>Details</summary>
Motivation: 现有车道变换框架未能充分考虑HV的动态信任水平，限制了对HV驾驶行为的准确预测能力，因此需要开发一种能够整合信任因素的框架以解决此问题。

Method: 提出了一种信任感知博弈论车道变换决策（TGLD）框架，包括多车辆联盟博弈的构建以及在线信任评估方法的开发，旨在动态估计HV的信任水平，并指导AV选择上下文适宜的合作策略。

Result: 在高速公路匝道合并场景中进行的人机耦合实验验证了TGLD方法的有效性。结果显示，AVs能够根据不同的HV信任水平和驾驶风格调整策略，信任机制显著提高了车道变换效率，保持了安全性，并促进了AV-HV之间透明和自适应的交互。

Conclusion: 该研究提出了一种信任感知博弈论车道变换决策（TGLD）框架，通过考虑人类驾驶员的动态信任水平，提高了自动驾驶汽车（AV）在异构交通环境中的社会兼容性和与人类驾驶员（HV）的合作效率。实验结果表明，TGLD框架能够有效提升车道变换效率、保持安全性，并促进AV与HV之间透明和自适应的交互。

Abstract: Automated vehicles (AVs) face a critical need to adopt socially compatible
behaviors and cooperate effectively with human-driven vehicles (HVs) in
heterogeneous traffic environment. However, most existing lane-changing
frameworks overlook HVs' dynamic trust levels, limiting their ability to
accurately predict human driver behaviors. To address this gap, this study
proposes a trust-aware game-theoretic lane-changing decision (TGLD) framework.
First, we formulate a multi-vehicle coalition game, incorporating fully
cooperative interactions among AVs and partially cooperative behaviors from HVs
informed by real-time trust evaluations. Second, we develop an online trust
evaluation method to dynamically estimate HVs' trust levels during
lane-changing interactions, guiding AVs to select context-appropriate
cooperative maneuvers. Lastly, social compatibility objectives are considered
by minimizing disruption to surrounding vehicles and enhancing the
predictability of AV behaviors, thereby ensuring human-friendly and
context-adaptive lane-changing strategies. A human-in-the-loop experiment
conducted in a highway on-ramp merging scenario validates our TGLD approach.
Results show that AVs can effectively adjust strategies according to different
HVs' trust levels and driving styles. Moreover, incorporating a trust mechanism
significantly improves lane-changing efficiency, maintains safety, and
contributes to transparent and adaptive AV-HV interactions.

</details>


### [604] [Foundation Model Driven Robotics: A Comprehensive Review](https://arxiv.org/abs/2507.10087)
*Muhammad Tayyab Khan,Ammar Waheed*

Main category: cs.RO

TL;DR: 基础模型正在改变机器人技术，但仍面临数据、安全和实时操作等挑战。未来研究应侧重于更强大、可解释和具身化的模型。


<details>
  <summary>Details</summary>
Motivation: 本文旨在批判性地回顾基础模型（特别是大型语言模型和视觉语言模型）在机器人领域的最新进展，重点关注集成系统级策略及其在现实世界中的可行性，并为未来研究提供方向。

Method: 本文通过对模拟驱动设计、开放世界执行、仿真到实际迁移和自适应机器人等应用领域进行分类和综合，对基础模型在机器人领域的最新进展进行了批判性回顾。文章讨论了程序化场景生成、策略泛化和多模态推理等关键趋势，并分析了有限的具身性、多模态数据缺乏、安全风险和计算限制等核心瓶颈。

Result: 该论文识别了基于基础模型的机器人技术的架构优势和关键局限性，强调了实时操作、接地、韧性和信任方面的开放挑战。

Conclusion: 该论文总结了基础模型（尤其是大型语言模型和视觉语言模型）在机器人领域的应用，强调了它们在语义理解、推理和跨模态泛化方面的能力，并指出了在模拟驱动设计、开放世界执行、仿真到实际迁移和自适应机器人等方面的进展。与现有文献不同，本文侧重于集成系统级策略及其在现实世界环境中的可行性。

Abstract: The rapid emergence of foundation models, particularly Large Language Models
(LLMs) and Vision-Language Models (VLMs), has introduced a transformative
paradigm in robotics. These models offer powerful capabilities in semantic
understanding, high-level reasoning, and cross-modal generalization, enabling
significant advances in perception, planning, control, and human-robot
interaction. This critical review provides a structured synthesis of recent
developments, categorizing applications across simulation-driven design,
open-world execution, sim-to-real transfer, and adaptable robotics. Unlike
existing surveys that emphasize isolated capabilities, this work highlights
integrated, system-level strategies and evaluates their practical feasibility
in real-world environments. Key enabling trends such as procedural scene
generation, policy generalization, and multimodal reasoning are discussed
alongside core bottlenecks, including limited embodiment, lack of multimodal
data, safety risks, and computational constraints. Through this lens, this
paper identifies both the architectural strengths and critical limitations of
foundation model-based robotics, highlighting open challenges in real-time
operation, grounding, resilience, and trust. The review concludes with a
roadmap for future research aimed at bridging semantic reasoning and physical
intelligence through more robust, interpretable, and embodied models.

</details>


### [605] [Physics-Informed Neural Networks with Unscented Kalman Filter for Sensorless Joint Torque Estimation in Humanoid Robots](https://arxiv.org/abs/2507.10105)
*Ines Sorrentino,Giulio Romualdi,Lorenzo Moretti,Silvio Traversaro,Daniele Pucci*

Main category: cs.RO

TL;DR: 该论文提出了一种用于无传感器人形机器人全身扭矩控制的新框架，结合了PINNs和UKF来提高扭矩估计的准确性和鲁棒性，并在实验中证明了其优越的性能和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在解决人形机器人中缺乏关节扭矩传感器而难以实现精确全身扭矩控制的问题，并提出了一种利用PINNs和UKF的解决方案。

Method: 本研究提出了一种将物理信息神经网络（PINNs）用于摩擦建模和无迹卡尔曼滤波（UKF）用于关节扭矩估计相结合的新颖框架，并将其整合到实时扭矩控制架构中。

Result: 实验结果表明，与现有的递归牛顿-欧拉算法（RNEA）相比，该方法在扭矩跟踪精度、能效和干扰抑制方面均有显著提升，并且在不同摩擦特性的机器人上表现出良好且无需重新识别的可扩展性。

Conclusion: 该框架为没有关节扭矩传感器的全身体扭矩控制提供了一种可扩展且实用的解决方案，确保了在动态环境中的扭矩跟踪、适应性和稳定性。

Abstract: This paper presents a novel framework for whole-body torque control of
humanoid robots without joint torque sensors, designed for systems with
electric motors and high-ratio harmonic drives. The approach integrates
Physics-Informed Neural Networks (PINNs) for friction modeling and Unscented
Kalman Filtering (UKF) for joint torque estimation, within a real-time torque
control architecture. PINNs estimate nonlinear static and dynamic friction from
joint and motor velocity readings, capturing effects like motor actuation
without joint movement. The UKF utilizes PINN-based friction estimates as
direct measurement inputs, improving torque estimation robustness. Experimental
validation on the ergoCub humanoid robot demonstrates improved torque tracking
accuracy, enhanced energy efficiency, and superior disturbance rejection
compared to the state-of-the-art Recursive Newton-Euler Algorithm (RNEA), using
a dynamic balancing experiment. The framework's scalability is shown by
consistent performance across robots with similar hardware but different
friction characteristics, without re-identification. Furthermore, a comparative
analysis with position control highlights the advantages of the proposed torque
control approach. The results establish the method as a scalable and practical
solution for sensorless torque control in humanoid robots, ensuring torque
tracking, adaptability, and stability in dynamic environments.

</details>


### [606] [Simulations and experiments with assemblies of fiber-reinforced soft actuators](https://arxiv.org/abs/2507.10121)
*Seung Hyun Kim,Jiamiao Guo,Arman Tekinalp,Heng-Sheng Chang,Ugur Akcal,Tixian Wang,Darren Biskup,Benjamin Walt,Girish Chowdhary,Girish Krishnan,Prashant G. Mehta,Mattia Gazzola*

Main category: cs.RO

TL;DR: 开发了一个仿真框架，用于控制由纤维增强弹性外壳（FREEs）组成的软连续臂（SCAs），并集成了视频跟踪系统以进行实验测试和控制设计。


<details>
  <summary>Details</summary>
Motivation: 软连续臂（SCAs）具有通用的操作潜力，但其难以控制的非线性行为限制了其在现实世界中的应用。

Method: 开发了一个用于软连续臂（SCAs）的仿真框架，并集成了视频跟踪系统，用于实验测试和控制设计。

Result: 需要更多信息来得出结果。例如，该框架在控制SCA方面的有效性如何？其在真实世界应用中的性能如何？

Conclusion: 需要更多信息来得出结论。

Abstract: Soft continuum arms (SCAs) promise versatile manipulation through mechanical
compliance, for assistive devices, agriculture, search applications, or
surgery. However, SCAs' real-world use is challenging, partly due to their
hard-to-control non-linear behavior. Here, a simulation framework for SCAs
modularly assembled out of fiber reinforced elastomeric enclosures (FREEs) is
developed and integrated with a video-tracking system for experimental testing
and control design.

</details>


### [607] [Probabilistic Human Intent Prediction for Mobile Manipulation: An Evaluation with Human-Inspired Constraints](https://arxiv.org/abs/2507.10131)
*Cesar Alan Contreras,Manolis Chiou,Alireza Rastegarpanah,Michal Szulik,Rustam Stolkin*

Main category: cs.RO

TL;DR: GUIDER, a dual-phase probabilistic framework, enhances robot understanding of human intent for navigation and manipulation, improving collaboration stability and prediction speed compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Accurate human intent inference is crucial for seamless human-robot collaboration, allowing robots to assist operators without causing conflicts or imposing constraints.

Method: GUIDER is a probabilistic framework with two coupled belief layers for tracking navigation and manipulation goals. The Navigation phase uses a Synergy Map combining controller velocity and an occupancy grid. The Manipulation phase integrates U2Net saliency, FastSAM instance saliency, and geometric grasp-feasibility tests, with an end-effector kinematics-aware update rule for real-time object probability evolution. It can recognize intent without predefined goals.

Result: In simulations, GUIDER achieved 93-100% navigation stability (vs. 60-100% for BOIR baseline) and 94-100% manipulation stability (vs. 69-100% for Trajectron baseline). It showed a 39.5% improvement in a navigation redirection scenario and a 31.4% improvement in a manipulation redirection task. GUIDER also predicted object intent three times faster in geometry-constrained trials (median 7.8s vs. 23.6s).

Conclusion: The proposed dual-phase framework, GUIDER, significantly improves intent inference in both navigation and manipulation phases of mobile manipulation tasks, outperforming baseline methods in stability and prediction time.

Abstract: Accurate inference of human intent enables human-robot collaboration without
constraining human control or causing conflicts between humans and robots. We
present GUIDER (Global User Intent Dual-phase Estimation for Robots), a
probabilistic framework that enables a robot to estimate the intent of human
operators. GUIDER maintains two coupled belief layers, one tracking navigation
goals and the other manipulation goals. In the Navigation phase, a Synergy Map
blends controller velocity with an occupancy grid to rank interaction areas.
Upon arrival at a goal, an autonomous multi-view scan builds a local 3D cloud.
The Manipulation phase combines U2Net saliency, FastSAM instance saliency, and
three geometric grasp-feasibility tests, with an end-effector kinematics-aware
update rule that evolves object probabilities in real-time. GUIDER can
recognize areas and objects of intent without predefined goals. We evaluated
GUIDER on 25 trials (five participants x five task variants) in Isaac Sim, and
compared it with two baselines, one for navigation and one for manipulation.
Across the 25 trials, GUIDER achieved a median stability of 93-100% during
navigation, compared with 60-100% for the BOIR baseline, with an improvement of
39.5% in a redirection scenario (T5). During manipulation, stability reached
94-100% (versus 69-100% for Trajectron), with a 31.4% difference in a
redirection task (T3). In geometry-constrained trials (manipulation), GUIDER
recognized the object intent three times earlier than Trajectron (median
remaining time to confident prediction 23.6 s vs 7.8 s). These results validate
our dual-phase framework and show improvements in intent inference in both
phases of mobile manipulation tasks.

</details>


### [608] [REACT: Real-time Entanglement-Aware Coverage Path Planning for Tethered Underwater Vehicles](https://arxiv.org/abs/2507.10204)
*Abdelhakim Amer,Mohit Mehindratta,Yury Brodskiy,Bilal Wehbe,Erdal Kayacan*

Main category: cs.RO

TL;DR: REACT框架通过几何系绳模型和在线重规划，解决了系绳水下航行器易缠绕的问题，实现了安全高效的水下结构检查。


<details>
  <summary>Details</summary>
Motivation: 水下结构检查中，系绳水下航行器常常受到系绳缠绕风险的限制，影响了检查的效率和安全性。

Method: REACT框架结合了基于符号距离场（SDF）的快速几何系绳模型，用于精确模拟三维结构周围的系绳。该模型支持高效的在线重规划策略，通过强制执行最大系绳长度限制来主动防止缠绕。

Result: 模拟结果显示，REACT在保证系绳约束和实现完整覆盖的同时，比传统规划器任务完成时间缩短了20%。真实世界实验也证实了REACT的有效性，在基线规划器因实际缠绕而失败的情况下，REACT成功完成了任务。

Conclusion: 该研究提出的REACT框架通过集成其几何驱动的系绳模型和在线重规划策略，成功解决了系绳水下航行器在复杂水下结构检查中系绳缠绕的问题。实验和模拟结果均表明，REACT能够实现安全、无缠绕的导航和全覆盖检查，并且比传统规划器效率更高。

Abstract: Inspection of complex underwater structures with tethered underwater vehicles
is often hindered by the risk of tether entanglement. We propose REACT
(real-time entanglement-aware coverage path planning for tethered underwater
vehicles), a framework designed to overcome this limitation. REACT comprises a
fast geometry-based tether model using the signed distance field (SDF) map for
accurate, real-time simulation of taut tether configurations around arbitrary
structures in 3D. This model enables an efficient online replanning strategy by
enforcing a maximum tether length constraint, thereby actively preventing
entanglement. By integrating REACT into a coverage path planning framework, we
achieve safe and optimal inspection paths, previously challenging due to tether
constraints. The complete REACT framework's efficacy is validated in a pipe
inspection scenario, demonstrating safe, entanglement-free navigation and
full-coverage inspection. Simulation results show that REACT achieves complete
coverage while maintaining tether constraints and completing the total mission
20% faster than conventional planners, despite a longer inspection time due to
proactive avoidance of entanglement that eliminates extensive post-mission
disentanglement. Real-world experiments confirm these benefits, where REACT
completes the full mission, while the baseline planner fails due to physical
tether entanglement.

</details>


### [609] [Robust RL Control for Bipedal Locomotion with Closed Kinematic Chains](https://arxiv.org/abs/2507.10164)
*Egor Maslennikov,Eduard Zaliaev,Nikita Dudorov,Oleg Shamanin,Karanov Dmitry,Gleb Afanasev,Alexey Burkov,Egor Lygin,Simeon Nedelchev,Evgeny Ponomarev*

Main category: cs.RO

TL;DR: 该研究提出了一种新的强化学习（RL）框架，用于开发仿人机器人的运动控制器。与简化模型不同，该框架考虑了闭环运动链的动力学，从而提高了仿真到现实的迁移能力和在不同地形上的稳定性。


<details>
  <summary>Details</summary>
Motivation: 大多数强化学习（RL）方法在训练时将仿人机器人中的闭环运动链简化为串行模型，这会影响仿真到现实的迁移，因为它未能捕捉到关节耦合、摩擦动力学和电机空间控制特性等方面。

Method: 提出了一种强化学习（RL）框架，该框架显式地结合了闭链动力学，并通过对称感知损失函数、对抗性训练和目标网络正则化来增强策略鲁棒性。

Result: 实验结果表明，所提出的集成方法在各种地形上实现了稳定的运动，并且显著优于基于简化运动学模型的方法。

Conclusion: 所提出方法在各种地形上实现了稳定的运动，并且显著优于基于简化运动模型的方法。

Abstract: Developing robust locomotion controllers for bipedal robots with closed
kinematic chains presents unique challenges, particularly since most
reinforcement learning (RL) approaches simplify these parallel mechanisms into
serial models during training. We demonstrate that this simplification
significantly impairs sim-to-real transfer by failing to capture essential
aspects such as joint coupling, friction dynamics, and motor-space control
characteristics. In this work, we present an RL framework that explicitly
incorporates closed-chain dynamics and validate it on our custom-built robot
TopA. Our approach enhances policy robustness through symmetry-aware loss
functions, adversarial training, and targeted network regularization.
Experimental results demonstrate that our integrated approach achieves stable
locomotion across diverse terrains, significantly outperforming methods based
on simplified kinematic models.

</details>


### [610] [Polygonal Obstacle Avoidance Combining Model Predictive Control and Fuzzy Logic](https://arxiv.org/abs/2507.10310)
*Michael Schröder,Eric Schöneberg,Daniel Görges,Hans D. Schotten*

Main category: cs.RO

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In practice, navigation of mobile robots in confined environments is often
done using a spatially discrete cost-map to represent obstacles. Path following
is a typical use case for model predictive control (MPC), but formulating
constraints for obstacle avoidance is challenging in this case. Typically the
cost and constraints of an MPC problem are defined as closed-form functions and
typical solvers work best with continuously differentiable functions. This is
contrary to spatially discrete occupancy grid maps, in which a grid's value
defines the cost associated with occupancy. This paper presents a way to
overcome this compatibility issue by re-formulating occupancy grid maps to
continuously differentiable functions to be embedded into the MPC scheme as
constraints. Each obstacle is defined as a polygon -- an intersection of
half-spaces. Any half-space is a linear inequality representing one edge of a
polygon. Using AND and OR operators, the combined set of all obstacles and
therefore the obstacle avoidance constraints can be described. The key
contribution of this paper is the use of fuzzy logic to re-formulate such
constraints that include logical operators as inequality constraints which are
compatible with standard MPC formulation. The resulting MPC-based trajectory
planner is successfully tested in simulation. This concept is also applicable
outside of navigation tasks to implement logical or verbal constraints in MPC.

</details>


### [611] [TOP: Trajectory Optimization via Parallel Optimization towards Constant Time Complexity](https://arxiv.org/abs/2507.10290)
*Jiajun Yu,Nanhe Chen,Guodong Liu,Chao Xu,Fei Gao,Yanjun Cao*

Main category: cs.RO

TL;DR: 提出了一种基于CADMM的并行轨迹优化框架，提高了大规模轨迹规划的效率和光滑度。


<details>
  <summary>Details</summary>
Motivation: 现有轨迹优化方法在处理大规模长轨迹时效率低下，如何通过并行化高效解决轨迹优化问题仍是未解决的难题。

Method: 提出了一种基于共识交替方向乘子法（CADMM）的新型轨迹优化框架，将轨迹分解为多个段并并行求解子问题，并引入了闭式解来处理线性/二次约束，以及数值解法处理一般不等式约束。

Result: 与现有SOTA方法相比，该框架将迭代时间复杂度从O(N)降低到每个迭代O(1)与段数成比例。通过仿真和实验证明，该方法在效率和光滑度方面均优于SOTA方法，尤其在大规模轨迹上实现了十倍以上的加速，并在GPU上实现了对数千个段的高性能。

Conclusion: 该方法在效率和光滑度方面优于现有技术，尤其在大规模轨迹上实现了数量级以上的加速，并在GPU上展示了高性能。

Abstract: Optimization has been widely used to generate smooth trajectories for motion
planning. However, existing trajectory optimization methods show weakness when
dealing with large-scale long trajectories. Recent advances in parallel
computing have accelerated optimization in some fields, but how to efficiently
solve trajectory optimization via parallelism remains an open question. In this
paper, we propose a novel trajectory optimization framework based on the
Consensus Alternating Direction Method of Multipliers (CADMM) algorithm, which
decomposes the trajectory into multiple segments and solves the subproblems in
parallel. The proposed framework reduces the time complexity to O(1) per
iteration to the number of segments, compared to O(N) of the state-of-the-art
(SOTA) approaches. Furthermore, we introduce a closed-form solution that
integrates convex linear and quadratic constraints to speed up the
optimization, and we also present numerical solutions for general inequality
constraints. A series of simulations and experiments demonstrate that our
approach outperforms the SOTA approach in terms of efficiency and smoothness.
Especially for a large-scale trajectory, with one hundred segments, achieving
over a tenfold speedup. To fully explore the potential of our algorithm on
modern parallel computing architectures, we deploy our framework on a GPU and
show high performance with thousands of segments.

</details>


### [612] [Raci-Net: Ego-vehicle Odometry Estimation in Adverse Weather Conditions](https://arxiv.org/abs/2507.10376)
*Mohammadhossein Talebi,Pragyan Dahal,Davide Possenti,Stefano Arrigoni,Francesco Braghin*

Main category: cs.RO

TL;DR: 在恶劣天气下，自动驾驶的运动估计需要融合毫米波雷达数据来补偿视觉传感器的不足。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶系统依赖视觉、激光雷达和IMU等传感器进行环境感知和运动估计，但视觉传感器在恶劣天气和光照条件下性能会下降。尽管现有方法对旋转失准和断开等技术问题具有鲁棒性，但它们在面对恶劣天气等动态环境因素时性能会下降。为了解决这些问题，本研究旨在提高在恶劣环境下的运动估计准确性和可靠性。

Method: 提出了一种新颖的基于深度学习的运动估计器，该估计器集成了视觉、惯性测量单元（IMU）和毫米波雷达数据，并采用先进的传感器融合技术，根据当前环境条件动态调整各传感器数据的贡献，以弥补视觉传感器的不足。

Result: 实验结果表明，该模型在晴朗和恶劣环境下均表现出良好的鲁棒性和有效性，证明了毫米波雷达在不同天气条件下的鲁棒性使其成为姿态估计系统的宝贵组成部分，尤其是在视觉传感器性能下降时。

Conclusion: 该模型通过融合视觉、惯性与毫米波雷达数据，并在不同环境条件下动态调整各传感器权重，提高了在雨、雪、光线变化等恶劣环境下的运动估计准确性和可靠性。

Abstract: Autonomous driving systems are highly dependent on sensors like cameras,
LiDAR, and inertial measurement units (IMU) to perceive the environment and
estimate their motion. Among these sensors, perception-based sensors are not
protected from harsh weather and technical failures. Although existing methods
show robustness against common technical issues like rotational misalignment
and disconnection, they often degrade when faced with dynamic environmental
factors like weather conditions. To address these problems, this research
introduces a novel deep learning-based motion estimator that integrates visual,
inertial, and millimeter-wave radar data, utilizing each sensor strengths to
improve odometry estimation accuracy and reliability under adverse
environmental conditions such as snow, rain, and varying light. The proposed
model uses advanced sensor fusion techniques that dynamically adjust the
contributions of each sensor based on the current environmental condition, with
radar compensating for visual sensor limitations in poor visibility. This work
explores recent advancements in radar-based odometry and highlights that radar
robustness in different weather conditions makes it a valuable component for
pose estimation systems, specifically when visual sensors are degraded.
Experimental results, conducted on the Boreas dataset, showcase the robustness
and effectiveness of the model in both clear and degraded environments.

</details>


### [613] [Scene-Aware Conversational ADAS with Generative AI for Real-Time Driver Assistance](https://arxiv.org/abs/2507.10500)
*Kyungtae Han,Yitao Chen,Rohit Gupta,Onur Altintas*

Main category: cs.RO

TL;DR: 提出了一种名为SC-ADAS的生成式AI框架，通过结合语言模型和视觉解释，实现了更智能、更具适应性的驾驶辅助系统，支持自然语言交互和意图确认，但存在一定的延迟和token增长问题。


<details>
  <summary>Details</summary>
Motivation: 当前的ADAS系统在解释场景上下文或通过自然语言与驾驶员互动方面能力有限，通常依赖预定义逻辑且不支持对话交互，在动态环境或适应驾驶员意图方面灵活性不足。

Method: 提出了一种名为SC-ADAS的模块化框架，该框架集成了包括大型语言模型、视觉到文本解释和结构化函数调用在内的生成式AI组件，实现了实时、可解释和自适应的驾驶员辅助。SC-ADAS支持以视觉和传感器上下文为基础的多轮对话，能够进行自然语言建议和驾驶员确认的ADAS控制。在CARLA模拟器中通过云端生成式AI实现，该系统在无需模型微调的情况下执行已确认的用户意图作为结构化的ADAS命令。

Result: 在CARLA模拟器中实现并进行了评估，突出了增加的延迟（来自基于视觉的上下文检索）和对话历史累积导致的token增长等权衡。结果表明了所提出方法的有效性。

Conclusion: 本研究展示了将对话式推理、场景感知和模块化ADAS控制相结合，以支持下一代智能驾驶辅助的可行性。

Abstract: While autonomous driving technologies continue to advance, current Advanced
Driver Assistance Systems (ADAS) remain limited in their ability to interpret
scene context or engage with drivers through natural language. These systems
typically rely on predefined logic and lack support for dialogue-based
interaction, making them inflexible in dynamic environments or when adapting to
driver intent. This paper presents Scene-Aware Conversational ADAS (SC-ADAS), a
modular framework that integrates Generative AI components including large
language models, vision-to-text interpretation, and structured function calling
to enable real-time, interpretable, and adaptive driver assistance. SC-ADAS
supports multi-turn dialogue grounded in visual and sensor context, allowing
natural language recommendations and driver-confirmed ADAS control. Implemented
in the CARLA simulator with cloud-based Generative AI, the system executes
confirmed user intents as structured ADAS commands without requiring model
fine-tuning. We evaluate SC-ADAS across scene-aware, conversational, and
revisited multi-turn interactions, highlighting trade-offs such as increased
latency from vision-based context retrieval and token growth from accumulated
dialogue history. These results demonstrate the feasibility of combining
conversational reasoning, scene perception, and modular ADAS control to support
the next generation of intelligent driver assistance.

</details>


### [614] [MP1: Mean Flow Tames Policy Learning in 1-step for Robotic Manipulation](https://arxiv.org/abs/2507.10543)
*Juyi Sheng,Ziyi Wang,Peiming Li,Mengyuan Liu*

Main category: cs.RO

TL;DR: MP1是一种创新的机器人学习方法，通过结合3D点云和MeanFlow，实现了快速、精确、可控的动作轨迹生成。它在速度和性能上均超越了现有方法，并且通过Dispersive Loss提升了泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型在机器人学习中存在|||采样|||速度与|||架构|||约束之间的|||权衡|||问题。扩散模型采样速度慢，而基于流的方法|||架构|||约束多且需要显式的|||一致性|||损失。

Method: MP1结合3D点云输入和MeanFlow范式，直接学习区间平均速度（通过MeanFlow Identity），实现1-NFE推理，无需显式|||一致性|||约束。引入CFG增强轨迹可控性，并使用Dispersive Loss提升泛化能力。

Result: MP1在Adroit和Meta-World基准以及真实世界场景中取得了优于DP3（10.2%）和FlowPolicy（7.3%）的平均任务成功率。平均推理时间仅为6.8毫秒，比DP3快19倍，比FlowPolicy快近2倍。

Conclusion: MP1通过结合3D点云输入和MeanFlow范式，实现了单网络函数评估（1-NFE）即可生成动作轨迹。通过直接学习平均速度，该方法无需额外的|||一致性|||约束，消除了数值ODE求解器在推理过程中的误差，从而产生更精确的轨迹。此外，MP1引入了CFG以提高轨迹的可控性，同时保持了1-NFE推理速度。为了提升泛化能力，特别是在少样本学习场景下，MP1还引入了一种轻量级的Dispersive Loss，用于在训练过程中将状态嵌入分离，且不影响推理速度。实验结果表明，MP1在Adroit和Meta-World基准以及真实世界场景中均表现出色，平均任务成功率优于DP3（10.2%）和FlowPolicy（7.3%），平均推理时间仅为6.8毫秒，比DP3快19倍，比FlowPolicy快近2倍。

Abstract: In robot manipulation, robot learning has become a prevailing approach.
However, generative models within this field face a fundamental trade-off
between the slow, iterative sampling of diffusion models and the architectural
constraints of faster Flow-based methods, which often rely on explicit
consistency losses. To address these limitations, we introduce MP1, which pairs
3D point-cloud inputs with the MeanFlow paradigm to generate action
trajectories in one network function evaluation (1-NFE). By directly learning
the interval-averaged velocity via the MeanFlow Identity, our policy avoids any
additional consistency constraints. This formulation eliminates numerical
ODE-solver errors during inference, yielding more precise trajectories. MP1
further incorporates CFG for improved trajectory controllability while
retaining 1-NFE inference without reintroducing structural constraints. Because
subtle scene-context variations are critical for robot learning, especially in
few-shot learning, we introduce a lightweight Dispersive Loss that repels state
embeddings during training, boosting generalization without slowing inference.
We validate our method on the Adroit and Meta-World benchmarks, as well as in
real-world scenarios. Experimental results show MP1 achieves superior average
task success rates, outperforming DP3 by 10.2% and FlowPolicy by 7.3%. Its
average inference time is only 6.8 ms-19x faster than DP3 and nearly 2x faster
than FlowPolicy. Our code is available at https://mp1-2254.github.io/.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [615] [Quantum-Resilient Privacy Ledger (QRPL): A Sovereign Digital Currency for the Post-Quantum Era](https://arxiv.org/abs/2507.09067)
*Serhan W. Bahar*

Main category: cs.ET

TL;DR: 本研究提出了一种名为QRPL的数字货币架构，它使用后量子密码学和零知识证明来提供抗量子、注重隐私和可扩展的CBDC解决方案，以应对当前的密码学挑战和CBDC设计中的隐私担忧。


<details>
  <summary>Details</summary>
Motivation: 本研究的动机是为了应对量子计算对现有密码基础设施的威胁以及中央银行数字货币（CBDC）在隐私保护和过度中心化方面引发的担忧。

Method: 本研究提出了一种结合了国家标准与技术研究院（NIST）标准化后量子密码学（PQC）和基于哈希的零知识证明的代币化数字货币架构（QRPL）。具体方法包括采用易失性证明链实现不可链接交易、隐私加权权益证明（PoS）共识以促进公平参与，以及一种新颖的基于零知识证明的隐私保护选择性披露机制。

Result: QRPL架构旨在确保用户主权、可扩展性和交易的机密性，并解决了当前CBDC设计中普遍存在的监控风险等问题。该架构的目标是实现10-20秒的出块时间，以平衡未来货币系统的安全性和吞吐量。

Conclusion: 该研究提出了量子弹性隐私账本（QRPL）作为一种创新的代币化数字货币架构，旨在解决中央银行数字货币（CBDC）设计中的关键缺陷，如普遍监控风险，并应对量子计算带来的密码学挑战。

Abstract: The emergence of quantum computing presents profound challenges to existing
cryptographic infrastructures, whilst the development of central bank digital
currencies (CBDCs) has raised concerns regarding privacy preservation and
excessive centralisation in digital payment systems. This paper proposes the
Quantum-Resilient Privacy Ledger (QRPL) as an innovative token-based digital
currency architecture that incorporates National Institute of Standards and
Technology (NIST)-standardised post-quantum cryptography (PQC) with hash-based
zero-knowledge proofs to ensure user sovereignty, scalability, and transaction
confidentiality. Key contributions include adaptations of ephemeral proof
chains for unlinkable transactions, a privacy-weighted Proof-of-Stake (PoS)
consensus to promote equitable participation, and a novel zero-knowledge
proof-based mechanism for privacy-preserving selective disclosure. QRPL aims to
address critical shortcomings in prevailing CBDC designs, including risks of
pervasive surveillance, with a 10-20 second block time to balance security and
throughput in future monetary systems. While conceptual, empirical prototypes
are planned. Future work includes prototype development to validate these
models empirically.

</details>


### [616] [Solving the compute crisis with physics-based ASICs](https://arxiv.org/abs/2507.10463)
*Maxwell Aifer,Zach Belateche,Suraj Bramhavar,Kerem Y. Camsari,Patrick J. Coles,Gavin Crooks,Douglas J. Durian,Andrea J. Liu,Anastasia Marchenkova,Antonio J. Martinez,Peter L. McMahon,Faris Sbahi,Benjamin Weiner,Logan G. Wright*

Main category: cs.ET

TL;DR: AI算力需求催生了算力危机。物理基础ASIC通过利用物理过程而非数字抽象进行计算，能够大幅提升能效和计算速度，有望解决AI和科学计算的瓶颈。


<details>
  <summary>Details</summary>
Motivation: 解决AI发展中日益严峻的算力危机，该危机表现为不可持续的能耗、高昂的训练成本以及传统CMOS扩展的极限。

Method: 通过放宽传统ASIC的无状态性、单向性、确定性和同步性等约束，使设备能精确实现物理过程。

Result: 物理基础ASIC有望在能效和计算吞吐量方面实现大幅提升，从而加速扩散模型、采样、优化和神经网络推理等关键AI应用，以及材料和分子模拟等传统计算工作负载。

Conclusion: 物理基础ASIC通过利用物理动力学直接进行计算，有望克服当前CMOS扩展瓶颈，为AI应用和科学计算带来显著的能效和计算吞吐量提升，并预示着异构、高度专业化计算平台的新兴趋势。

Abstract: Escalating artificial intelligence (AI) demands expose a critical "compute
crisis" characterized by unsustainable energy consumption, prohibitive training
costs, and the approaching limits of conventional CMOS scaling. Physics-based
Application-Specific Integrated Circuits (ASICs) present a transformative
paradigm by directly harnessing intrinsic physical dynamics for computation
rather than expending resources to enforce idealized digital abstractions. By
relaxing the constraints needed for traditional ASICs, like enforced
statelessness, unidirectionality, determinism, and synchronization, these
devices aim to operate as exact realizations of physical processes, offering
substantial gains in energy efficiency and computational throughput. This
approach enables novel co-design strategies, aligning algorithmic requirements
with the inherent computational primitives of physical systems. Physics-based
ASICs could accelerate critical AI applications like diffusion models,
sampling, optimization, and neural network inference as well as traditional
computational workloads like scientific simulation of materials and molecules.
Ultimately, this vision points towards a future of heterogeneous,
highly-specialized computing platforms capable of overcoming current scaling
bottlenecks and unlocking new frontiers in computational power and efficiency.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [617] [Credit Card Fraud Detection Using RoFormer Model With Relative Distance Rotating Encoding](https://arxiv.org/abs/2507.09385)
*Kevin Reyes,Vasco Cortez*

Main category: cs.NE

TL;DR: 通过将相对距离旋转编码（ReDRE）整合到RoFormer模型中，以提高交易欺诈检测的准确性。


<details>
  <summary>Details</summary>
Motivation: 为了应对金融系统中日益严峻的欺诈检测挑战，特别是对于像Flow Payment这样每月处理数百万笔交易并需要强大安全措施来降低金融风险的公司。提高交易授权率并减少欺诈对于提供良好的用户体验和建立可持续的业务至关重要，因此需要不断研究和投资以发现新颖和改进的欺诈检测方法。

Method: 提出了一种新颖的交易欺诈检测方法，该方法将相对距离旋转编码（ReDRE）整合到RoFormer模型中。ReDRE通过引入角度旋转来增强Transformer模型对时间序列数据的表征能力，从而更好地捕捉时间依赖性和事件关系，提高欺诈检测效果。

Result: 通过将ReDRE整合到RoFormer模型中，该方法能够更好地捕捉时间依赖性和事件关系，从而提高了交易欺诈检测的性能。

Conclusion: 本文提出了一种新颖的交易欺诈检测方法，通过将相对距离旋转编码（ReDRE）结合到RoFormer模型中，以提高欺诈检测的准确性。

Abstract: Fraud detection is one of the most important challenges that financial
systems must address. Detecting fraudulent transactions is critical for payment
gateway companies like Flow Payment, which process millions of transactions
monthly and require robust security measures to mitigate financial risks.
Increasing transaction authorization rates while reducing fraud is essential
for providing a good user experience and building a sustainable business. For
this reason, discovering novel and improved methods to detect fraud requires
continuous research and investment for any company that wants to succeed in
this industry. In this work, we introduced a novel method for detecting
transactional fraud by incorporating the Relative Distance Rotating Encoding
(ReDRE) in the RoFormer model. The incorporation of angle rotation using ReDRE
enhances the characterization of time series data within a Transformer, leading
to improved fraud detection by better capturing temporal dependencies and event
relationships.

</details>


### [618] [BrainFLORA: Uncovering Brain Concept Representation via Multimodal Neural Embeddings](https://arxiv.org/abs/2507.09747)
*Dongyang Li,Haoyang Qin,Mingyang Wu,Chen Wei,Quanying Liu*

Main category: cs.NE

TL;DR: BrainFLORA框架利用多模态大语言模型整合EEG、MEG和fMRI等神经成像数据，构建共享神经表征，提升了视觉检索性能，并发现了神经表征与现实世界物体之间的联系，为认知神经科学和脑机接口开辟了新途径。


<details>
  <summary>Details</summary>
Motivation: 当前研究在整合EEG、MEG和fMRI等跨模态神经成像信号时面临时空失配的挑战，导致分析常被孤立进行，限制了对神经表征的整体性理解。本研究旨在克服这一挑战，实现跨模态数据的融合。

Method: BrainFLORA是一个统一的框架，利用多模态大语言模型（MLLMs），并辅以特定模态的适配器和任务解码器，来整合跨模态神经成像数据（如EEG、MEG和fMRI），以构建共享的神经表征。

Result: BrainFLORA在跨主体的视觉检索任务中取得了最先进的性能，并揭示了视觉概念表征在不同神经模态之间以及与真实世界物体感知之间存在对齐关系，证明了大脑视觉概念表征与物理世界刺激之间存在隐式映射。

Conclusion: BrainFLORA 框架通过整合跨模态神经成像数据，构建了共享的神经表征，实现了跨主体的视觉检索任务的最优性能，并揭示了视觉概念表征在不同神经模态间以及与真实世界物体感知中的对齐方式。该研究表明，大脑结构化的视觉概念表征与物理世界刺激存在隐式映射关系，为认知神经科学和脑机接口提供了新的思路。

Abstract: Understanding how the brain represents visual information is a fundamental
challenge in neuroscience and artificial intelligence. While AI-driven decoding
of neural data has provided insights into the human visual system, integrating
multimodal neuroimaging signals, such as EEG, MEG, and fMRI, remains a critical
hurdle due to their inherent spatiotemporal misalignment. Current approaches
often analyze these modalities in isolation, limiting a holistic view of neural
representation. In this study, we introduce BrainFLORA, a unified framework for
integrating cross-modal neuroimaging data to construct a shared neural
representation. Our approach leverages multimodal large language models (MLLMs)
augmented with modality-specific adapters and task decoders, achieving
state-of-the-art performance in joint-subject visual retrieval task and has the
potential to extend multitasking. Combining neuroimaging analysis methods, we
further reveal how visual concept representations align across neural
modalities and with real world object perception. We demonstrate that the
brain's structured visual concept representations exhibit an implicit mapping
to physical-world stimuli, bridging neuroscience and machine learning from
different modalities of neural imaging. Beyond methodological advancements,
BrainFLORA offers novel implications for cognitive neuroscience and
brain-computer interfaces (BCIs). Our code is available at
https://github.com/ncclab-sustech/BrainFLORA.

</details>


### [619] [Effective Self-Attention-Based Deep Learning Model with Evolutionary Grid Search for Robust Wave Farm Energy Forecasting](https://arxiv.org/abs/2507.09847)
*Amin Abdollahi Dehkordi,Mehdi Neshat,Nataliia Y. Sergiienko,Zahra Ghasemi,Lei Chen,John Boland,Hamid Moradkhani,Amir H. Gandomi*

Main category: cs.NE

TL;DR: 本研究提出了一种结合自注意力机制、卷积神经网络和双向长短期记忆网络的混合模型，用于预测波浪能发电量，并在澳大利亚的四个地点验证了其优越的预测精度，为波浪能并网提供了可靠的解决方案。


<details>
  <summary>Details</summary>
Motivation: 为了实现联合国可持续发展目标13（气候行动），并应对波浪能发电量预测对电网稳定性和商业可行性的重要性，本研究旨在提高波浪能发电量的预测精度，从而促进波浪能更好地整合到电力系统中。

Method: 本研究提出了一个混合序列学习框架，该框架结合了自注意力增强卷积双向长短期记忆网络（Self-Attention-enhanced Convolutional Bi-LSTM）和超参数优化技术，并利用了来自澳大利亚阿德莱德、悉尼、珀斯和塔斯马尼亚等地波浪能发电场的数据集进行了验证。

Result: 该混合模型在澳大利亚不同地区的波浪能发电场数据集上进行了验证，其R2分数分别为：阿德莱德91.7%，珀斯88.0%，塔斯马尼亚82.8%，悉尼91.0%。与十种机器学习算法进行基准测试后，该模型在准确性方面表现更优，超越了传统的机器学习和深度学习方法。

Conclusion: 该研究提出了一个混合序列学习模型，结合了具有超参数优化的自注意力增强卷积双向长短期记忆网络，以提高波浪能发电量的预测精度，为波浪能并网提供支持。

Abstract: Achieving carbon neutrality, a key focus of UN SDG #13, drives the
exploration of wave energy, a renewable resource with the potential to generate
30,000 TWh of clean electricity annually, surpassing global demand. However,
wave energy remains underdeveloped due to technical and economic challenges,
particularly in forecasting wave farm power output, which is vital for grid
stability and commercial viability. This study proposes a novel predictive
framework to enhance wave energy integration into power grids. It introduces a
hybrid sequential learning model combining Self-Attention-enhanced
Convolutional Bi-LSTM with hyperparameter optimization. The model leverages
spatial data from Wave Energy Converters (WECs) and is validated using datasets
from wave farms in Adelaide, Sydney, Perth, and Tasmania, Australia.
Benchmarked against ten machine learning algorithms, the model achieves
superior accuracy, with R2 scores of 91.7% (Adelaide), 88.0% (Perth), 82.8%
(Tasmania), and 91.0% (Sydney). It outperforms conventional ML and deep
learning methods, offering robust and scalable predictions for wave energy
output across diverse marine environments, supporting reliable integration into
energy systems.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [620] [A Fixed Parameter Tractable Approach for Solving the Vertex Cover Problem in Polynomial Time Complexity](https://arxiv.org/abs/2507.09377)
*Mumuksh Tayal*

Main category: cs.DS

TL;DR: 最小顶点覆盖的 FPT 算法在 n 大 k 小的情况下比 SageMath 快。


<details>
  <summary>Details</summary>
Motivation: 最小顶点覆盖问题是一个经典的 NP 完全问题，在大型图上精确求解具有挑战性。FPT 通过利用输入参数来处理这类问题，该参数通常与期望解的大小有关。

Method: 该算法采用基于选择相邻顶点并在简化图上递归求解子问题的分支策略，并对该算法进行了 Python 实现。

Result: FPT 实现达到了显著的性能改进，特别是在处理具有大量顶点（n）但参数值（k）相对较小的实例时。

Conclusion: 该固定参数可处理性（FPT）算法在处理具有大量顶点但参数值相对较小（k）的实例时，与SageMath计算系统相比，性能显著提高，这与理论上的 FPT 复杂度保证一致。

Abstract: The Minimum Vertex Cover problem, a classical NP-complete problem, presents
significant challenges for exact solution on large graphs. Fixed-Parameter
Tractability (FPT) offers a powerful paradigm to address such problems by
exploiting a parameter of the input, typically related to the size of the
desired solution. This paper presents an implementation and empirical
evaluation of an FPT algorithm for the Minimum Vertex Cover problem
parameterized by the size of the vertex cover, $k$. The algorithm utilizes a
branching strategy based on selecting adjacent vertices and recursively solving
subproblems on a reduced graph. We describe the algorithmic approach,
implementation details in Python, and present experimental results comparing
its performance against the SageMath computational system. The results
demonstrate that the FPT implementation achieves significant performance
improvements for instances with large numbers of vertices ($n$) but relatively
small values of the parameter ($k$), aligning with theoretical FPT complexity
guarantees. We also discuss potential optimizations that could further improve
the algorithm's performance, particularly concerning the branching factor.

</details>


### [621] [Simultaneous Network Design with Restricted Link Usage](https://arxiv.org/abs/2507.09426)
*Naonori Kakimura,Péter Madarasi,Jannik Matuschke,Kitti Varga*

Main category: cs.DS

TL;DR: 在多商品网络设计中，需要找到最小成本的弧子集，使得每个颜色类别的交集包含一个s-t dipath。研究了问题的变体，得到了硬度结果，并确定了几种可行的特殊情况和一种针对一般情况的 FPT 算法。


<details>
  <summary>Details</summary>
Motivation: 在多商品网络设计环境中，每个商品仅限于使用其自身颜色的链接，需要找到一个最小成本的弧子集，该子集与每个颜色类别的交集包含一个s-t dipath。

Method: 研究了该问题的几个变体，推导了强硬度结果，即使是对于受限的情况，也确定了可以解决该问题的时间多项式。

Result: 推导了强硬度结果，并确定了可以解决该问题的时间多项式。

Conclusion: 该问题存在一些特殊情况的可行解法，例如颜色类别构成层级家族或基础有向图为无环图且颜色类别数量恒定时。此外，还提出了一种针对多色弧数量参数化的一般情况的 FPT 算法。

Abstract: Given a digraph with two terminal vertices $s$ and $t$ as well as a
conservative cost function and several not necessarily disjoint color classes
on its arc set, our goal is to find a minimum-cost subset of the arcs such that
its intersection with each color class contains an $s$-$t$ dipath. Problems of
this type arise naturally in multi-commodity network design settings where each
commodity is restricted to use links of its own color only.
  We study several variants of the problem, deriving strong hardness results
even for restricted cases, but we also identify cases that can be solved in
polynomial time. The latter ones include the cases where the color classes form
a laminar family, or where the underlying digraph is acyclic and the number of
color classes is constant. We also present an FPT algorithm for the general
case parameterized by the number of multi-colored arcs.

</details>


### [622] [Nearly Tight Sample Complexity for Matroid Online Contention Resolution](https://arxiv.org/abs/2507.09507)
*Moran Feldman,Ola Svensson,Rico Zenklusen*

Main category: cs.DS

TL;DR: 本文提出了一种更省样本的算法来解决预言机不等式问题，尤其是在团约束下，样本量大大减少，同时保持了最优的竞争性。


<details>
  <summary>Details</summary>
Motivation: 经典预言机不等式需要完全了解随机变量的分布，这在现实中不切实际。因此，研究基于有限样本的方法具有重要意义。现有的基于样本的OCRS算法在处理团约束问题时需要较多的样本量。

Method: 本文提出了一种新的基于样本的在线冲突解决机制（OCRS）算法，该算法能够处理团约束问题。

Result: 本文提出的OCRS算法仅需O(log ρ ⋅ log^2 log ρ)个样本，优于现有方法。这直接转化为一种新的基于样本的团预言机不等式，其竞争性达到了1/4 - ε，并且样本量也得到了显著的降低，仅需O(log n + log ρ ⋅ log^2 log ρ)个样本。

Conclusion: 本文提出了一种接近最优的基于样本的OCRS算法，用于处理团约束问题。该算法仅需O(log ρ ⋅ log^2 log ρ)个样本，其中ρ是团的秩。这使得基于样本的团预言机不等式能够以1/4 - ε的竞争性得到，这是在全知对手模型下已知最优的竞争性。

Abstract: Due to their numerous applications, in particular in Mechanism Design,
Prophet Inequalities have experienced a surge of interest. They describe
competitive ratios for basic stopping time problems where random variables get
revealed sequentially. A key drawback in the classical setting is the
assumption of full distributional knowledge of the involved random variables,
which is often unrealistic. A natural way to address this is via sample-based
approaches, where only a limited number of samples from the distribution of
each random variable is available. Recently, Fu, Lu, Gavin Tang, Wu, Wu, and
Zhang (2024) showed that sample-based Online Contention Resolution Schemes
(OCRS) are a powerful tool to obtain sample-based Prophet Inequalities. They
presented the first sample-based OCRS for matroid constraints, which is a
heavily studied constraint family in this context, as it captures many
interesting settings. This allowed them to get the first sample-based Matroid
Prophet Inequality, using $O(\log^4 n)$ many samples (per random variable),
where $n$ is the number of random variables, while obtaining a constant
competitiveness of $\frac{1}{4}-\varepsilon$.
  We present a nearly optimal sample-based OCRS for matroid constraints, which
uses only $O(\log \rho \cdot \log^2\log\rho)$ many samples, almost matching a
known lower bound of $\Omega(\log \rho)$, where $\rho \leq n$ is the rank of
the matroid. Through the above-mentioned connection to Prophet Inequalities,
this yields a sample-based Matroid Prophet Inequality using only $O(\log n +
\log\rho \cdot \log^2\log\rho)$ many samples, and matching the competitiveness
of $\frac{1}{4}-\varepsilon$, which is the best known competitiveness for the
considered almighty adversary setting even when the distributions are fully
known.

</details>


### [623] [Paths and Intersections: Exact Emulators for Planar Graphs](https://arxiv.org/abs/2507.09620)
*George Z. Li,Zihan Tan,Tianyi Zhang*

Main category: cs.DS

TL;DR: 本文研究平面图的顶点稀疏化技术，旨在保留图中k个终端之间的距离。研究人员开发了一种新的方法，能够构建出大小为O(f^2k^2)的精确平面模拟器，其中f是终端所在的面的数量。这一成果是对先前研究的扩展，并为图论分析提供了新的视角。


<details>
  <summary>Details</summary>
Motivation: 研究的动机在于为具有k个终端的平面图构建一个更小的边加权平面图（模拟器），该模拟器能精确地保持终端之间的成对距离，并且在终端位于平面嵌入的f个面上这一特定设置下给出了新的稀疏化结果。

Method: 本研究采用了一种新颖的图结构分析方法，将图视为路径及其交集，并在此基础上进行了平面模拟器的构建。

Result: 我们成功构建了精确的平面模拟器，其大小为O(f^2k^2)，该结果在f=1（所有终端位于同一面）和f=k（通用情况）的先前结果之间进行了推广和插值，分别对应于O(k^2)和O(k^4)的界限。

Conclusion: 本文为顶点稀疏化和距离保持在平面图中的研究提供了新的见解，特别是在具有k个终端且终端位于f个面上的平面嵌入的情况下，我们构建了精确的平面模拟器，其大小为O(f^2k^2)。

Abstract: We study vertex sparsification for preserving distances in planar graphs.
Given an edge-weighted planar graph with $k$ terminals, the goal is to
construct an emulator, which is a smaller edge-weighted planar graph that
contains the terminals and exactly preserves the pairwise distances between
them. We construct exact planar emulators of size $O(f^2k^2)$ in the setting
where terminals lie on $f$ faces in the planar embedding of the input graph.
Our result generalizes and interpolates between the previous results of Chang
and Ophelders and Goranci, Henzinger, and Peng which is an $O(k^2)$ bound in
the setting where all terminals lie on a single face (i.e., $f=1$), and the
result of Krauthgamer, Nguyen, and Zondiner, which is an $O(k^4)$ bound for the
general case (i.e., $f=k$).
  Our construction follows a recent new way of analyzing graph structures, by
viewing graphs as paths and their intersections, which we believe is of
independent interest.

</details>


### [624] [Minimum-Peak-Cost Flows Over Time](https://arxiv.org/abs/2507.09688)
*Mariia Anapolska,Emma Ahrens,Christina Büsing,Felix Engelhardt,Timo Gersing,Corinna Mathwieser,Sabrian Schmitz,Sophia Wrede*

Main category: cs.DS

TL;DR: 该研究提出了最小峰值成本的暂时重复流问题（MPC-TRF），解决了在资源规划中最小化资源峰值需求的问题，并找到了两种问题的有效解决方案。


<details>
  <summary>Details</summary>
Motivation: 在需要非消耗性资源的交通规划中，资源峰值需求比资源使用时长更重要，例如使用一辆卡车运送八小时比使用两辆卡车运送四小时更具成本效益。为了模拟这种情况，需要最小化最大成本而不是成本积分。

Method: 提出最小峰值成本流（minimum peak cost flow）的概念，并将其应用于暂时重复流（temporally repeated flows）以最小化峰值成本而非成本积分。研究还进行了复杂性分析，并确定了两个特殊情况：单位成本串联-并联网络和时间范围较长（至少是网络中最长路径的两倍）的网络。

Result: 单位成本串联-并联网络和时间范围足够长（至少是网络中最长路径的两倍）的网络是MPC-TRF的特殊情况，可以在多项式时间内找到最优解。对于其他情况，该问题是NP难的。

Conclusion: 该研究提出了最小峰值成本的暂时重复流问题（MPC-TRF），并分析了其复杂性。研究发现在单位成本串联-并联网络和时间范围足够长的情况下，MPC-TRF可以有效求解。

Abstract: When planning transportation whose operation requires non-consumable
resources, the peak demand for allocated resources is often of higher interest
than the duration of resource usage. For instance, it is more cost-effective to
deliver parcels with a single truck over eight hours than to use two trucks for
four hours, as long as the time suffices. To model such scenarios, we introduce
the novel minimum peak cost flow over time problem, whose objective is to
minimise the maximum cost at all points in time rather than minimising the
integral of costs. We focus on minimising peak costs of temporally repeated
flows. These are desirable for practical applications due to their simple
structure. This yields the minimum-peak-cost Temporally Repeated flow problem
(MPC-TRF).
  We show that the simple structure of temporally repeated flows comes with the
drawback of arbitrarily bad approximation ratios compared to general flows over
time. Furthermore, our complexity analysis shows the integral version of
MPC-TRF is strongly NP-hard, even under strong restrictions. On the positive
side, we identify two benign special cases: unit-cost series-parallel networks
and networks with time horizon at least twice as long as the longest path in
the network (with respect to the transit time). In both cases, we show that
integral optimal flows if the desired flow value equals the maximum flow value
and fractional optimal flows for arbitrary flow values can be found in
polynomial time. For each of these cases, we provide an explicit algorithm that
constructs an optimal solution.

</details>


### [625] [Phase transition of the Sinkhorn-Knopp algorithm](https://arxiv.org/abs/2507.09711)
*Kun He*

Main category: cs.DS

TL;DR: Sinkhorn-Knopp 算法的性能取决于矩阵的密度，在密度阈值 γ = 1/2 处存在相位转换。


<details>
  <summary>Details</summary>
Motivation: 本文旨在解决关于 Sinkhorn-Knopp 算法的两个基本问题：其强大的经验性能归因于什么？是否能为其迭代次数建立一个精确的界限？

Method: 对于上界，我们证明了 Sinkhorn-Knopp 算法在迭代 O(log n - log ε) 次和 O(n^2) 时间内，可以为所有归一化版本具有密度 γ > 1/2 的非负方阵生成一个近似双随机矩阵。对于下界，我们确定了正矩阵在 L2 范数误差度量下的迭代次数的精确界限为 Ω(n^1/2/ε)，并且证明了对于所有 γ < 1/2，都存在一个密度为 γ 的矩阵，该矩阵需要 Ω(n^1/2/ε) 次迭代。

Result: Sinkhorn-Knopp 算法在密度 γ > 1/2 的情况下，迭代 O(log n - log ε) 次和 O(n^2) 时间内可生成近似双随机矩阵。对于密度 γ < 1/2 的正矩阵，需要 Ω(n^1/2/ε) 次迭代。

Conclusion: 本论文揭示了 Sinkhorn-Knopp 算法在密度阈值 γ = 1/2 处存在一个明确的相位转换。

Abstract: The matrix scaling problem, particularly the Sinkhorn-Knopp algorithm, has
been studied for over 60 years. In practice, the algorithm often yields
high-quality approximations within just a few iterations. Theoretically,
however, the best-known upper bound places it in the class of
pseudopolynomial-time approximation algorithms. Meanwhile, the lower-bound
landscape remains largely unexplored. Two fundamental questions persist: what
accounts for the algorithm's strong empirical performance, and can a tight
bound on its iteration count be established?
  For an $n\times n$ matrix, its normalized version is obtained by dividing
each entry by its largest entry. We say that a normalized matrix has a density
$\gamma$ if there exists a constant $\rho > 0$ such that one row or column has
exactly $\lceil \gamma n \rceil$ entries with values at least $\rho$, and every
other row and column has at least $\lceil \gamma n \rceil$ such entries.
  For the upper bound, we show that the Sinkhorn-Knopp algorithm produces a
nearly doubly stochastic matrix in $O(\log n - \log \varepsilon)$ iterations
and $\widetilde{O}(n^2)$ time for all nonnegative square matrices whose
normalized version has a density $\gamma > 1/2$. Such matrices cover both the
algorithm's principal practical inputs and its typical theoretical regime, and
the $\widetilde{O}(n^2)$ runtime is optimal.
  For the lower bound, we establish a tight bound of
$\widetilde{\Omega}\left(n^{1/2}/\varepsilon\right)$ iterations for positive
matrices under the $\ell_2$-norm error measure. Moreover, for every $\gamma <
1/2$, there exists a matrix with density $\gamma$ for which the algorithm
requires $\Omega\left(n^{1/2}/\varepsilon\right)$ iterations.
  In summary, our results reveal a sharp phase transition in the Sinkhorn-Knopp
algorithm at the density threshold $\gamma = 1/2$.

</details>


### [626] [Improved Directed Expander Decompositions](https://arxiv.org/abs/2507.09729)
*Henry Fleischmann,George Z. Li,Jason Li*

Main category: cs.DS

TL;DR: 研究提出了新的有向图和有容制限图的扩展器分解算法，其效率更高并具有更广泛的适用性。


<details>
  <summary>Details</summary>
Motivation: 为了获得比现有算法更快、几乎无损耗地适用于有容制限图的有向图扩展器分解算法。

Method: 实现了非停止割匹配博弈在有向有容制限图上的应用和分析，并对该方法在有向图上的分析进行了技术性修改和细化。

Result: 在有向图上实现了比先前工作更快、具有最佳phi依赖性的扩展器分解算法，并首次实现了有向有容制限图的近线性时间扩展器分解。

Conclusion: 该研究为有向图提供了更快的扩展器分解算法，并首次实现了有向有容制限图的近线性时间扩展器分解。

Abstract: We obtain faster expander decomposition algorithms for directed graphs,
matching the guarantees of Saranurak and Wang (SODA 2019) for expander
decomposition on undirected graphs. Our algorithms are faster than prior work
and also generalize almost losslessly to capacitated graphs. In particular, we
obtain the first directed expander decomposition algorithm for capacitated
graphs in near-linear time with optimal dependence on $\phi$.
  To obtain our result, we provide the first implementation and analysis of the
non-stop cut-matching game for directed, capacitated graphs. All existing
directed expander decomposition algorithms instead temporarily add ''fake
edges'' before pruning them away in a final cleanup step. Our result shows that
the natural undirected approach applies even to directed graphs. The difficulty
is in its analysis, which is technical and requires significant modifications
from the original setting of undirected graphs.

</details>


### [627] [Covering a Few Submodular Constraints and Applications](https://arxiv.org/abs/2507.09879)
*Tanvi Bajpai,Chandra Chekuri,Pooja Kulkarni*

Main category: cs.DS

TL;DR: 当需要覆盖多个子模约束时，本文提出了一种针对固定约束数量的近似算法。对于一般情况，算法能在成本接近最优的同时满足约束。对于特定类型的约束（加权覆盖函数），算法提供了更好的近似比，接近于单个约束的情况。


<details>
  <summary>Details</summary>
Motivation: 该研究的动机源于近期的一些应用需求，旨在解决覆盖多个子模约束的问题。当约束数量r是一个固定常数时，期望获得比r是输入变量时更好的近似比，甚至接近于r=1（即经典的子模集覆盖问题）的近似性能。

Method: 该研究利用了子模函数和集合覆盖问题的相关理论。针对多个子模约束的问题，提出了一种随机双标准近似算法。对于加权覆盖函数和删除闭合集系统，则利用了线性规划（LP）松弛以及相关的近似技术来推导出近似比。

Result: 对于涉及多个子模约束的问题，该研究提出了一种随机双标准近似算法，能够为每个约束 $f_i$ 找到一个子集 $S$，使得 $f_i(S) 
less$ $(1-1/e^α - oldsymbol{
u})$ $b_i$，并且期望成本 $oldsymbol{
otin}$ $[c(S)] oldsymbol{
otin}$ $(1+oldsymbol{
u})oldsymbol{
otin}oldsymbol{
otin}oldsymbol{
otin}$ ·OPT。对于加权覆盖函数和删除闭合集系统，该研究提出了一种 $(1+oldsymbol{
u})$ $(rac{e}{e-1})$ $(1+oldsymbol{eta})$-近似算法，其中 $oldsymbol{eta}$ 是基础集覆盖实例的LP近似比。

Conclusion: 该研究为涉及多个子模约束的覆盖问题提供了两种主要结果。对于一般的子模约束，提出了一种随机双标准近似算法，能够以近似因子 O(α) 找到满足约束的子集，同时确保成本接近最优值。对于加权覆盖函数和删除闭合集系统，则提出了一个显著改进的近似算法，其近似比接近于单个子模约束的情况。这些结果表明，对于固定的约束数量，可以获得与单个约束情况相当的近似性能。

Abstract: We consider the problem of covering multiple submodular constraints. Given a
finite ground set $N$, a cost function $c: N \rightarrow \mathbb{R}_+$, $r$
monotone submodular functions $f_1,f_2,\ldots,f_r$ over $N$ and requirements
$b_1,b_2,\ldots,b_r$ the goal is to find a minimum cost subset $S \subseteq N$
such that $f_i(S) \ge b_i$ for $1 \le i \le r$. When $r=1$ this is the
well-known Submodular Set Cover problem. Previous work
\cite{chekuri2022covering} considered the setting when $r$ is large and
developed bi-criteria approximation algorithms, and approximation algorithms
for the important special case when each $f_i$ is a weighted coverage function.
These are fairly general models and capture several concrete and interesting
problems as special cases. The approximation ratios for these problem are at
least $\Omega(\log r)$ which is unavoidable when $r$ is part of the input. In
this paper, motivated by some recent applications, we consider the problem when
$r$ is a \emph{fixed constant} and obtain two main results. For covering
multiple submodular constraints we obtain a randomized bi-criteria
approximation algorithm that for any given integer $\alpha \ge 1$ outputs a set
$S$ such that $f_i(S) \ge$ $(1-1/e^\alpha -\epsilon)b_i$ for each $i \in [r]$
and $\mathbb{E}[c(S)] \le (1+\epsilon)\alpha \cdot \sf{OPT}$. Second, when the
$f_i$ are weighted coverage functions from a deletion-closed set system we
obtain a $(1+\epsilon)$ $(\frac{e}{e-1})$ $(1+\beta)$-approximation where
$\beta$ is the approximation ratio for the underlying set cover instances via
the natural LP. These results show that one can obtain nearly as good an
approximation for any fixed $r$ as what one would achieve for $r=1$. We mention
some applications that follow easily from these general results and anticipate
more in the future.

</details>


### [628] [Improved bicriteria approximation for $k$-edge-connectivity](https://arxiv.org/abs/2507.10125)
*Zeev Nutov*

Main category: cs.DS

TL;DR: 本文改进了 k-ECSS 和 k-ECSM 问题的近似算法，在近似比方面取得了显著提升。


<details>
  <summary>Details</summary>
Motivation: 寻找 k-边连通生成子图（k-ECSS）和 k-边连通生成多子图（k-ECSM）问题的最优解，这些问题在网络设计和优化中具有重要应用。现有的算法在近似比方面还有提升空间，特别是双标准近似算法方面。

Method: 本文提出了一系列改进的双标准近似算法来解决 k-ECSS 和 k-ECSM 问题。具体来说，对于 k-ECSS 问题，算法在 k 为偶数时实现了 (1, k-2) 的近似比，在 k 为奇数时实现了 (1-1/k, k-3) 的近似比，并且还提出了一个 (3/2, k-1) 的近似算法。对于 k-ECSM 问题，算法将近似比从之前的 1+4/k 提高到 1+2/k (k 为偶数) 和 1+3/k (k 为奇数)。

Result: 对于 k-ECSS 问题，本文将双标准近似算法改进为 k 为偶数时的 (1, k-2) 和 k 为奇数时的 (1-1/k, k-3)，并提出了另一个双标准近似算法 (3/2, k-1)。对于 k-ECSM 问题，本文将近似比从之前的 1+4/k 提高到 1+2/k (k 为偶数) 和 1+3/k (k 为奇数)。

Conclusion: 本文针对 k-ECSM 问题，将 k 为偶数时的近似比从 1+4/k 提高到 1+2/k，将 k 为奇数时的近似比从 1+4/k 提高到 1+3/k，并且对于 k 为奇数的情况，计算出的子图实际上是 (k+1)-边连通的。针对 k-ECSS 问题，本文将双标准近似算法改进为 k 为偶数时的 (1, k-2) 和 k 为奇数时的 (1-1/k, k-3)，并提出了另一个双标准近似算法 (3/2, k-1)。

Abstract: In the $k$-Edge Connected Spanning Subgraph ($k$-ECSS) problem we are given a
(multi-)graph $G=(V,E)$ with edge costs and an integer $k$, and seek a min-cost
$k$-edge-connected spanning subgraph of $G$. The problem admits a
$2$-approximation algorithm and no better approximation ratio is
known.Hershkowitz, Klein, and Zenklusen [STOC 24] gave a bicriteria
$(1,k-10)$-approximation algorithm that computes a $(k-10)$-edge-connected
spanning subgraph of cost at most the optimal value of a standard Cut-LP for
$k$-ECSS. This LP bicriteria approximation was recently improved by Cohen and
Nutov [ESA 25] to $(1,k-4)$, where also was given a bicriteria approximation
$(3/2,k-2)$. In this paper we improve the bicriteria approximation to $(1,k-2)$
for $k$ even and to $\left(1-\frac{1}{k},k-3\right)$ for $k$ is odd, and also
give another bicriteria approximation $(3/2,k-1)$.
  The $k$-Edge-Connected Spanning Multi-subgraph ($k$-ECSM) problem is almost
the same as $k$-ECSS, except that any edge can be selected multiple times at
the same cost. The previous best approximation ratio for $k$-ECSM was $1+4/k$.
Our result improves this to $1+\frac{2}{k}$ for $k$ even and to $1+\frac{3}{k}$
for $k$ odd, where for $k$ odd the computed subgraph is in fact
$(k+1)$-edge-connected.

</details>


### [629] [Bicriteria Submodular Maximization](https://arxiv.org/abs/2507.10248)
*Moran Feldman,Alan Kuhnle*

Main category: cs.DS

TL;DR: 子模函数优化中，双标准近似算法的研究：涵盖多种约束和函数类，提供最优或改进结果，并展示放宽约束的有用性。


<details>
  <summary>Details</summary>
Motivation: 子模函数及其优化已在从机器学习、数据挖掘到博弈论和经济学的各种环境中得到应用。在本研究中，我们考虑受约束的子模函数最大化问题。

Method: 本文研究了受约束的子模函数最大化问题，并对双标准近似算法进行了原则性研究，这些算法允许约束最多违反一个有界因子。

Result: 研究结果涵盖了多种约束类型（基数、背包、拟阵和凸集）和多种子模函数类（单调、对称和一般）。

Conclusion: 研究结果表明，在许多情况下，我们提供了最优结果，或者改进了最先进的技术，甚至在单标准（标准）优化这一特殊情况下的最先进技术。最后一种结果表明，放宽可行性约束可以提供一种有用的问题视角，即使人们只想要可行解。

Abstract: Submodular functions and their optimization have found applications in
diverse settings ranging from machine learning and data mining to game theory
and economics. In this work, we consider the constrained maximization of a
submodular function, for which we conduct a principled study of bicriteria
approximation algorithms -- algorithms which can violate the constraint, but
only up to a bounded factor. Bicrteria optimization allows constrained
submodular maximization to capture additional important settings, such as the
well-studied submodular cover problem and optimization under soft constraints.
We provide results that span both multiple types of constraints (cardinality,
knapsack, matroid and convex set) and multiple classes of submodular functions
(monotone, symmetric and general). For many of the cases considered, we provide
optimal results. In other cases, our results improve over the state-of-the-art,
sometimes even over the state-of-the-art for the special case of
single-criterion (standard) optimization. Results of the last kind demonstrate
that relaxing the feasibility constraint may give a perspective about the
problem that is useful even if one only desires feasible solutions.

</details>


### [630] [Approximating Maximum Cut on Interval Graphs and Split Graphs beyond Goemans-Williamson](https://arxiv.org/abs/2507.10436)
*Jungho Ahn,Ian DeHaan,Eun Jung Kim,Euiwoong Lee*

Main category: cs.DS

TL;DR: This paper offers a near-optimal approximation algorithm for the Maximum Cut problem on interval and split graphs, achieving a $(\alpha_{GW} + \varepsilon)$ guarantee. It also suggests that achieving a $(1-c)$ approximation for split graphs is unlikely under the Small Set Expansion Hypothesis.


<details>
  <summary>Details</summary>
Motivation: To develop an approximation algorithm for the Maximum Cut problem on interval and split graphs that improves upon existing guarantees, and to investigate the approximability limits of this problem on split graphs.

Method: The algorithm is a modified version of the Goemans-Williamson algorithm, combined with structural results about triangle packing in interval and split graphs. An improved analysis of the Goemans-Williamson algorithm is provided for graphs with a constant fraction of edges involved in triangles.

Result: A $(\alpha_{GW} + \varepsilon)$-approximation algorithm for Maximum Cut on interval and split graphs is presented. Additionally, it

Conclusion: The paper presents a polynomial-time $(\alpha_{GW} + \varepsilon)$-approximation algorithm for Maximum Cut on interval and split graphs, with $\alpha_{GW} \approx 0.878$. It also shows that a $(1 - c)$-approximation for Maximum Cut on split graphs is unlikely, assuming the Small Set Expansion Hypothesis.

Abstract: We present a polynomial-time $(\alpha_{GW} + \varepsilon)$-approximation
algorithm for the Maximum Cut problem on interval graphs and split graphs,
where $\alpha_{GW} \approx 0.878$ is the approximation guarantee of the
Goemans-Williamson algorithm and $\varepsilon > 10^{-34}$ is a fixed constant.
To attain this, we give an improved analysis of a slight modification of the
Goemans-Williamson algorithm for graphs in which triangles can be packed into a
constant fraction of their edges. We then pair this analysis with structural
results showing that both interval graphs and split graphs either have such a
triangle packing or have maximum cut close to their number of edges. We also
show that, subject to the Small Set Expansion Hypothesis, there exists a
constant $c > 0$ such that there is no polyomial-time $(1 - c)$-approximation
for Maximum Cut on split graphs.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [631] [Precomputed Dominant Resource Fairness](https://arxiv.org/abs/2507.08846)
*Serdar Metin*

Main category: cs.GT

TL;DR: 对主导资源公平性算法进行了分析，并提出了一种新的近似算法——预计算主导资源公平性。


<details>
  <summary>Details</summary>
Motivation: 扩展了对多资源类型场景下的公平资源分配问题的研究，并提出了一种改进算法。

Method: 通过分析主导资源公平性算法的结构，提出了一种新的近似算法。

Result: 提出了一种新的近似算法“预计算主导资源公平性”，该算法能在更少的步骤中实现主导资源公平性。

Conclusion: 提出了一种名为“预计算主导资源公平性”的新算法，该算法能够以更少的步骤近似主导资源公平性分配。

Abstract: Although resource allocation is a well studied problem in computer science,
until the prevalence of distributed systems, such as computing clouds and data
centres, the question had been addressed predominantly for single resource type
scenarios. At the beginning of the last decade, with the introuction of
Dominant Resource Fairness, the studies of the resource allocation problem has
finally extended to the multiple resource type scenarios. Dominant Resource
Fairness is a solution, addressing the problem of fair allocation of multiple
resource types, among users with heterogeneous demands. Based on Max-min
Fairness, which is a well established algorithm in the literature for
allocating resources in the single resource type scenarios, Dominant Resource
Fairness generalises the scheme to the multiple resource case. It has a number
of desirable properties that makes it preferable over alternatives, such as
Sharing Incentive, Envy-Freeness, Pareto Efficiency, and Strategy Proofness,
and as such, it is widely adopted in distributed systems. In the present study,
we revisit the original study, and analyse the structure of the algorithm in
closer view, to come up with an alternative algorithm, which approximates the
Dominant Resource Fairness allocation in fewer steps. We name the new algorithm
Precomputed Dominant Resource Fairness, after its main working principle.

</details>


### [632] [A Survey on Bilateral Multi-Round Cloud-SLA Negotiation Strategies](https://arxiv.org/abs/2507.08868)
*Benedikt Pittl,Werner Mach,Erich Schikuta*

Main category: cs.GT

TL;DR: 本文调研了云服务交易中的多轮谈判策略，分析了现有研究和形式化方法，并为行业应用提供了建议。


<details>
  <summary>Details</summary>
Motivation: 为了实现未来云市场中更优化的资源交易，提高数据中心利用率并允许消费者购买高度定制化的云服务。

Method: 通过分析同行评审的文章，识别了多轮双边谈判策略的趋势、空白、相似性及范围，并调查了用于描述这些策略的形式化方法。

Result: 识别了云资源交易中多轮双边谈判策略的现状，并提出了促进其实施的建议。

Conclusion: 本篇论文对云资源交易中的多轮双边谈判策略进行了调研，分析了现有研究的趋势、空白、相似性及范围，并调查了描述这些策略所使用的形式化方法。基于这些发现，论文提出了创建和记录双边多轮谈判策略的建议，以促进其在行业中的应用。

Abstract: Today, static cloud markets where consumers purchase services directly from
providers are dominating. Thus, consumers neither negotiate the price nor the
characteristics of the service. In recent years, providers have adopted more
dynamic trading mechanisms, as e.g. Amazon's EC2 platform shows: In addition to
the reservation marketspace and the on-demand marketspace, Amazon offers a spot
marketspace where consumers can bid for virtual machines. This spot marketspace
was extended with spot blocks, and recently Amazon reworked the bidding
options. In addition, other cloud providers, such as Virtustream, adopt dynamic
trading mechanisms. The scientific community envisions autonomous multi-round
negotiations for realizing future cloud marketspaces. Consequently, consumers
and providers exchange offers and counteroffers to reach an agreement. This
helps providers increase the utilization of their datacenters, while consumers
can purchase highly customized cloud services.
  In the paper at hand, we present a survey on multi-round bilateral
negotiation strategies for trading cloud resources. Thus, we analyzed
peer-reviewed articles in order to identify trends, gaps, similarities, and the
scope of such negotiation strategies. In addition, we surveyed the formalism
that the scientific community uses to describe such strategies. Based on these
findings, we derived recommendations for creating and documenting bilateral
multi-round negotiation strategies to foster their implementation in the
industry.

</details>


### [633] [Learning from Synthetic Labs: Language Models as Auction Participants](https://arxiv.org/abs/2507.09083)
*Anand Shah,Kehang Zhu,Yanchen Jiang,Jeffrey G. Wang,Arif K. Dayi,John J. Horton,David C. Parkes*

Main category: cs.GT

TL;DR: 通过合成数据和LLM代理商进行的拍卖实验比传统实验便宜得多，并且结果与人类行为一致。


<details>
  <summary>Details</summary>
Motivation: 研究模拟AI代理商（大型语言模型）在拍卖中的行为，并为拍卖的研究和设计提供便利。

Method: 提出了一种新颖的合成数据生成过程来研究和设计拍卖，并使用GPT-4模型进行了1000多次拍卖实验。

Result: LLM竞标者在各种经典拍卖格式中表现出与实验文献一致的行为，包括风险规避、在显而易见地具有策略保证的拍卖中更接近理论预测，以及在共同价值设定中易受赢家诅咒的影响。

Conclusion: LLM代理商在拍卖中表现出与人类竞标者相似的行为，并且在恰当的提示下可以接近理论预测。

Abstract: This paper investigates the behavior of simulated AI agents (large language
models, or LLMs) in auctions, introducing a novel synthetic data-generating
process to help facilitate the study and design of auctions. We find that LLMs
-- when endowed with chain of thought reasoning capacity -- agree with the
experimental literature in auctions across a variety of classic auction
formats. In particular, we find that LLM bidders produce results consistent
with risk-averse human bidders; that they perform closer to theoretical
predictions in obviously strategy-proof auctions; and, that they succumb to the
winner's curse in common value settings. On prompting, we find that LLMs are
not very sensitive to naive changes in prompts (e.g., language, currency) but
can improve dramatically towards theoretical predictions with the right mental
model (i.e., the language of Nash deviations). We run 1,000$+$ auctions for
less than $\$$400 with GPT-4 models (three orders of magnitude cheaper than
modern auction experiments) and develop a framework flexible enough to run
auction experiments with any LLM model and a wide range of auction design
specifications, facilitating further experimental study by decreasing costs and
serving as a proof-of-concept for the use of LLM proxies.

</details>


### [634] [Nash Equilibria with Irradical Probabilities](https://arxiv.org/abs/2507.09422)
*Edan Orzech,Martin Rinard*

Main category: cs.GT

TL;DR: n人博弈模型，具有唯一的、完全混合的纳什均衡，其概率权重是不可约的。


<details>
  <summary>Details</summary>
Motivation: 为了找到具有不可约概率权重的唯一、完全混合的纳什均衡。

Method: 提出了一种n人博弈模型，并分析了其纳什均衡的性质。

Result: 找到了一个n人博弈模型，其纳什均衡的概率权重是不可约的。

Conclusion: 提出了一种新的n人博弈模型，该模型具有唯一的、完全混合的纳什均衡，其概率权重是不可约的。

Abstract: We present for every $n\ge4$ an $n$-player game in normal form with payoffs
in $\{0,1,2\}$ that has a unique, fully mixed, Nash equilibrium in which all
the probability weights are irradical (i.e., algebraic but not closed form
expressible even with $m$-th roots for any integer $m$).

</details>


### [635] [Incentive-Aware Dynamic Resource Allocation under Long-Term Cost Constraints](https://arxiv.org/abs/2507.09473)
*Yan Dai,Negin Golrezaei,Patrick Jaillet*

Main category: cs.GT

TL;DR: 一种新的框架，通过延迟更新和随机探索使原始对偶方法能够稳健地处理具有私人估值的战略代理的动态资源分配问题，在实现最大化社会福利的同时满足成本约束并激励诚实报告。


<details>
  <summary>Details</summary>
Motivation: 为了应对原始对偶方法在策略性设置中的脆弱性，即代理商可以通过操纵报告来扭曲未来的对偶更新以获取未来收益。

Method: 结合基于时期（epoch-based）的延迟更新（其中对偶变量在每个时期内保持固定）和随机探索轮次（提取近似真实信号用于学习），并利用精心设计的在线学习子程序。

Result: 该机制实现了 Tilde{O}(sqrt{T}) 的社会福利遗憾（social welfare regret），满足了所有成本约束，并确保了激励一致性。

Conclusion: 该机制在满足成本约束的同时，实现了与非策略性分配方法相匹配的社会福利，并且能够应对策略性代理。

Abstract: Motivated by applications such as cloud platforms allocating GPUs to users or
governments deploying mobile health units across competing regions, we study
the dynamic allocation of a reusable resource to strategic agents with private
valuations. Our objective is to simultaneously (i) maximize social welfare,
(ii) satisfy multi-dimensional long-term cost constraints, and (iii)
incentivize truthful reporting. We begin by numerically evaluating primal-dual
methods widely used in constrained online optimization and find them to be
highly fragile in strategic settings -- agents can easily manipulate their
reports to distort future dual updates for future gain.
  To address this vulnerability, we develop an incentive-aware framework that
makes primal-dual methods robust to strategic behavior. Our design combines
epoch-based lazy updates -- where dual variables remain fixed within each epoch
-- with randomized exploration rounds that extract approximately truthful
signals for learning. Leveraging carefully designed online learning subroutines
that can be of independent interest for dual updates, our mechanism achieves
$\tilde{\mathcal{O}}(\sqrt{T})$ social welfare regret, satisfies all cost
constraints, and ensures incentive alignment. This matches the performance of
non-strategic allocation approaches while being robust to strategic agents.

</details>


### [636] [Existence of Fair and Efficient Allocation of Indivisible Chores](https://arxiv.org/abs/2507.09544)
*Ryoga Mahara*

Main category: cs.GT

TL;DR: 本文证明了在公平高效地分配不可分割杂务时，总是存在一种满足 EF1 和 PO 的分配方式，并通过不动点和离散算法实现，且在代理数量恒定的情况下可高效计算，并推广至加权 EF1。


<details>
  <summary>Details</summary>
Motivation: 文章旨在解决在为具有附加成本函数的代理分配不可分割杂务时，如何实现公平和高效分配的问题。具体来说，它试图回答一个悬而未决的问题：是否总能找到一种既满足 EF1 又满足 PO 的分配方式。

Method: 文章通过结合不动点论证和离散算法来证明其主要结论，这是一种新颖的组合方法。

Result: 文章的主要结果是证明了对于不可分割杂务和附加成本函数，总是存在一种 EF1 且 PO 的分配方式。此外，还证明了存在一种 EF1 且 fPO 的分配方式，并且在代理数量恒定的情况下，EF1 和 PO 分配可以在多项式时间内计算。最后，所有结果都可以推广到 wEF1。

Conclusion: 文章证明了在附加成本函数下，对于不可分割的杂务，总是存在一种公平（ envy-free up to one chore, EF1）且帕累托最优（Pareto optimal, PO）的分配方式。此外，文章还证明了存在一种 EF1 且满足更强效率概念（fractional Pareto optimal, fPO）的分配方式。当代理数量恒定时，EF1 和 PO 分配可以在多项式时间内计算得出。所有这些结果均可扩展到考虑代理权益的加权 EF1（wEF1）设置。

Abstract: We study the problem of allocating indivisible chores among agents with
additive cost functions in a fair and efficient manner. A major open question
in this area is whether there always exists an allocation that is envy-free up
to one chore (EF1) and Pareto optimal (PO). Our main contribution is to provide
a positive answer to this question by proving the existence of such an
allocation for indivisible chores under additive cost functions. This is
achieved by a novel combination of a fixed point argument and a discrete
algorithm, providing a significant methodological advance in this area.
  Our additional key contributions are as follows. We show that there always
exists an allocation that is EF1 and fractional Pareto optimal (fPO), where fPO
is a stronger efficiency concept than PO. We also show that an EF1 and PO
allocation can be computed in polynomial time when the number of agents is
constant. Finally, we extend all of these results to the more general setting
of weighted EF1 (wEF1), which accounts for the entitlements of agents.

</details>


### [637] [Tie-breaking Agnostic Lower Bound for Fictitious Play](https://arxiv.org/abs/2507.09902)
*Yuanhao Wang*

Main category: cs.GT

TL;DR: Karlin关于Fictitious play收敛到纳什均衡的猜想被反驳了，存在一个零和博弈，其收敛速度比猜想的慢。


<details>
  <summary>Details</summary>
Motivation: 反驳Samuel Karlin在1959年提出的关于Fictitious play（FP）收敛到纳什均衡的猜想，特别是其在允许的对策下的收敛率。

Method: 通过构建一个10x10的零和矩阵博弈，证明了FP的收敛速度可以达到$\\(t^{-1/3}$），与Karlin猜想的$O(t^{-1/2})$收敛率不符。

Result: 存在一个10x10的零和矩阵博弈，其中FP的收敛速度为$\\(t^{-1/3}$），并且除了第一步之外没有出现平局。

Conclusion: 该论文反驳了Karlin的猜想，证明了在允许的对策（包括不允许的对策）的情况下，Fictitious play（FP）不一定收敛到纳什均衡。

Abstract: Fictitious play (FP) is a natural learning dynamic in two-player zero-sum
games. Samuel Karlin conjectured in 1959 that FP converges at a rate of
$O(t^{-1/2})$ to Nash equilibrium, where $t$ is the number of steps played.
However, Daskalakis and Pan disproved the stronger form of this conjecture in
2014, where \emph{adversarial} tie-breaking is allowed.
  This paper disproves Karlin's conjecture in its weaker form. In particular,
there exists a 10-by-10 zero-sum matrix game, in which FP converges at a rate
of $\Omega(t^{-1/3})$, and no ties occur except for the first step.

</details>


### [638] [Generalized Quantal Response Equilibrium: Existence and Efficient Learning](https://arxiv.org/abs/2507.09928)
*Apurv Shukla,Vijay Subramanian,Andy Zhao,Rahul Jain*

Main category: cs.GT

TL;DR: This paper presents GQRE, a new equilibrium concept for boundedly rational players in games, generalizing existing methods. It includes an efficient learning algorithm based on Frank-Wolfe and proves its effectiveness on complex games.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of existing solution concepts in game theory when dealing with boundedly rational agents. The paper aims to generalize Quantal Response Equilibrium to a broader class of agents and games, providing a more realistic model of strategic decision-making under bounded rationality. Additionally, it seeks to develop efficient algorithms for computing equilibria in these games.

Method: The paper introduces Generalized Quantal Response Equilibrium (GQRE) by having each player maximize a smooth, regularized expected utility of the mixed profiles used, reflecting bounded rationality that subsumes stochastic choice. It then presents a computationally efficient no-regret independent learning algorithm using smoothened versions of the Frank-Wolfe algorithm. This algorithm utilizes noisy but correlated gradient estimates from a simulation oracle that reports on repeated plays. Convergence properties are analyzed using a class of gap functions that generalize the Nash gap, particularly under assumptions ensuring unique equilibrium.

Result: The paper establishes the existence of GQRE under mild conditions. It presents an efficient no-regret learning algorithm with analyzed convergence properties under assumptions of unique equilibrium. The effectiveness of the proposed method is demonstrated through successful application to complex games, including high-rank two-player games, large action two-player games, and multi-player games.

Conclusion: In conclusion, this paper introduces Generalized Quantal Response Equilibrium (GQRE) as a novel solution concept for bounded rational agents in finite normal-form general-sum games. It generalizes Quantal Response Equilibrium and offers a computationally efficient no-regret independent learning algorithm via smoothened Frank-Wolfe. The paper also analyzes convergence properties and demonstrates the method's effectiveness on various complex games.

Abstract: We introduce a new solution concept for bounded rational agents in finite
normal-form general-sum games called Generalized Quantal Response Equilibrium
(GQRE) which generalizes Quantal Response
Equilibrium~\citep{mckelvey1995quantal}. In our setup, each player maximizes a
smooth, regularized expected utility of the mixed profiles used, reflecting
bounded rationality that subsumes stochastic choice. After establishing
existence under mild conditions, we present computationally efficient no-regret
independent learning via smoothened versions of the Frank-Wolfe algorithm. Our
algorithm uses noisy but correlated gradient estimates generated via a
simulation oracle that reports on repeated plays of the game. We analyze
convergence properties of our algorithm under assumptions that ensure
uniqueness of equilibrium, using a class of gap functions that generalize the
Nash gap. We end by demonstrating the effectiveness of our method on a set of
complex general-sum games such as high-rank two-player games, large action
two-player games, and known examples of difficult multi-player games.

</details>


### [639] [A New Incentive Model For Content Trust](https://arxiv.org/abs/2507.09972)
*Lucas Barbosa,Sam Kirshner,Rob Kopel,Eric Tze Kuan Lim,Tom Pagram*

Main category: cs.GT

TL;DR: 本篇论文提出了一种激励驱动的去中心化方法，用于大规模验证数字内容的真实性。通过在奖励函数中加入“信任”因素，并要求内容创建者为其主张提供财务抵押品，该方法旨在通过社区治理模式来对抗错误信息。


<details>
  <summary>Details</summary>
Motivation: 广泛的错误信息、人工智能生成内容的爆炸式增长以及对传统新闻来源的依赖性降低，要求一种新的内容真实性和寻求真相的方法，以适应现代数字世界。

Method: 该方法利用智能合约和数字身份将“信任”纳入奖励函数，要求内容创建者为事实主张质押财务抵押品，由公正的陪审团进行审查并获得财务奖励。

Result: 通过结合财务和社交激励模型，可以激励用户参与众包事实核查，并促使内容创建者在其声明中更加谨慎。

Conclusion: 该方法旨在通过基于社区的治理模式，利用智能合约和数字身份将“信任”纳入奖励函数，以激励用户参与众包事实核查，从而在规模上验证数字内容的真实性。

Abstract: This paper outlines an incentive-driven and decentralized approach to
verifying the veracity of digital content at scale. Widespread misinformation,
an explosion in AI-generated content and reduced reliance on traditional news
sources demands a new approach for content authenticity and truth-seeking that
is fit for a modern, digital world. By using smart contracts and digital
identity to incorporate 'trust' into the reward function for published content,
not just engagement, we believe that it could be possible to foster a
self-propelling paradigm shift to combat misinformation through a
community-based governance model. The approach described in this paper requires
that content creators stake financial collateral on factual claims for an
impartial jury to vet with a financial reward for contribution. We hypothesize
that with the right financial and social incentive model users will be
motivated to participate in crowdsourced fact-checking and content creators
will place more care in their attestations. This is an exploratory paper and
there are a number of open issues and questions that warrant further analysis
and exploration.

</details>


### [640] [A Coincidence of Wants Mechanism for Swap Trade Execution in Decentralized Exchanges](https://arxiv.org/abs/2507.10149)
*Abhimanyu Nag,Madhur Prabhakar,Tanuj Behl*

Main category: cs.GT

TL;DR: 提出了一种新的框架来识别和完成去中心化交易所中的交易意愿周期，通过资产矩阵和图遍历等方法实现了无滑点和资本保全的交易。


<details>
  <summary>Details</summary>
Motivation: 旨在为去中心化交易所（DEX）聚合器中的交易意愿（CoW）周期提供一种比现有拍卖系统更数学严谨、更高效的识别和完成框架，并为流动性提供者展示了一种潜在的delta-中性策略。

Method: 提出了一种资产矩阵方法，结合了预言机价格、形式守恒定律、图遍历和不平衡修正，用于识别和完成交易意愿（CoW）周期，并定义了桥接订单。

Result: 所提出的算法能够有效发现 CoW 周期，并通过插入合成订单实现原子周期闭合，实现了无滑点和资本保全的执行，并在实际的 Arbitrum 交易数据上进行了验证。

Conclusion: 该研究提出了一种数学上严谨的框架，用于识别和完成去中心化交易所（DEX）聚合器中的交易意愿（CoW）周期。该方法通过资产矩阵、预言机价格、形式守恒定律、图遍历和不平衡修正来实现，实现了无滑点和资本保全的执行。应用于实际的 Arbitrum 交易数据，该算法能有效发现 CoW 周期，并通过插入合成订单实现原子周期闭合，为流动性提供者详细说明了一种潜在的delta-中性策略。

Abstract: We propose a mathematically rigorous framework for identifying and completing
Coincidence of Wants (CoW) cycles in decentralized exchange (DEX) aggregators.
Unlike existing auction based systems such as CoWSwap, our approach introduces
an asset matrix formulation that not only verifies feasibility using oracle
prices and formal conservation laws but also completes partial CoW cycles of
swap orders that are discovered using graph traversal and are settled using
imbalance correction. We define bridging orders and show that the resulting
execution is slippage free and capital preserving for LPs. Applied to real
world Arbitrum swap data, our algorithm demonstrates efficient discovery of CoW
cycles and supports the insertion of synthetic orders for atomic cycle closure.
This work can be thought of as the detailing of a potential delta-neutral
strategy by liquidity providing market makers: a structured CoW cycle
execution.

</details>


### [641] [The Value Problem for Weighted Timed Games with Two Clocks is Undecidable](https://arxiv.org/abs/2507.10550)
*Quentin Guilmant,Joël Ouaknine,Isa Vialard*

Main category: cs.GT

TL;DR: 加权时序博弈的价值问题对于两时钟和非负权重是不可判定的。


<details>
  <summary>Details</summary>
Motivation: 加权时序博弈（WTGs）的价值问题在算法理解方面存在一个主要差距，特别是对于两时钟的情况。本研究旨在解决这个差距。

Method: 通过研究具有非负权重的两时钟加权时序博弈（WTGs）的价值问题来证明其不可判定性，即使在有时间限制的情况下。

Result: 证明了具有非负权重的两时钟WTGs的价值问题是不可判定的，即使在有时间限制的情况下也是如此。

Conclusion: 两时钟加权时序博弈（WTGs）的价值问题，即使在有时间限制的情况下，使用非负权重也是不可判定的。

Abstract: The Value Problem for weighted timed games (WTGs) consists in determining,
given a two-player weighted timed game with a reachability objective and a
rational threshold, whether or not the value of the game exceeds the threshold.
This problem was shown to be undecidable some ten years ago for WTGs making use
of at least three clocks, and is known to be decidable for single-clock WTGs.
In this paper, we establish undecidability for two-clock WTGs making use of
non-negative weights, even in a time-bounded setting, closing the last
remaining major gap in our algorithmic understanding of WTGs.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [642] [Analysing Health Misinformation with Advanced Centrality Metrics in Online Social Networks](https://arxiv.org/abs/2507.09055)
*Mkululi Sikosana,Sean Maudsley-Barton,Oluwaseun Ajao*

Main category: cs.SI

TL;DR: 本研究提出了三种新的中心性指标（DIC、MVC、PC），结合时间动态、易感性和多层网络交互，以更有效地识别和应对在线健康错误信息的传播。与传统指标相比，新指标能识别更多有影响力的节点，并显著提高干预措施的效果。


<details>
  <summary>Details</summary>
Motivation: 在线社交网络上健康错误信息的快速传播对公众健康、社会稳定和制度信任构成了挑战。传统的中心性指标在理解危机期间在线网络的复杂性和动态性方面存在局限性。

Method: 使用 FibVID 和 Monant Medical Misinformation 数据集，对传统中心性指标和新提出的 DIC、MVC、PC 指标进行了比较，以识别有影响力的节点、传播途径和错误信息影响者。

Result: 新指标识别出的有影响力的节点比传统指标多 44.83%。基线干预措施将健康错误信息减少了 50%，而结合新指标的干预措施则将错误信息减少了 62.5%。所提出的指标在第二个数据集上得到了成功验证，证实了其广泛适用性。

Conclusion: 本研究提出并比较了三种新颖的中心性指标：动态影响中心性（DIC）、健康错误信息脆弱性中心性（MVC）和传播中心性（PC），并结合了时间动态、易感性和多层网络交互。结果表明，结合使用传统和新颖的中心性指标，可以更全面、更广泛地理解和减轻在线网络环境中健康错误信息的传播。

Abstract: The rapid spread of health misinformation on online social networks (OSNs)
during global crises such as the COVID-19 pandemic poses challenges to public
health, social stability, and institutional trust. Centrality metrics have long
been pivotal in understanding the dynamics of information flow, particularly in
the context of health misinformation. However, the increasing complexity and
dynamism of online networks, especially during crises, highlight the
limitations of these traditional approaches. This study introduces and compares
three novel centrality metrics: dynamic influence centrality (DIC), health
misinformation vulnerability centrality (MVC), and propagation centrality (PC).
These metrics incorporate temporal dynamics, susceptibility, and multilayered
network interactions. Using the FibVID dataset, we compared traditional and
novel metrics to identify influential nodes, propagation pathways, and
misinformation influencers. Traditional metrics identified 29 influential
nodes, while the new metrics uncovered 24 unique nodes, resulting in 42
combined nodes, an increase of 44.83%. Baseline interventions reduced health
misinformation by 50%, while incorporating the new metrics increased this to
62.5%, an improvement of 25%. To evaluate the broader applicability of the
proposed metrics, we validated our framework on a second dataset, Monant
Medical Misinformation, which covers a diverse range of health misinformation
discussions beyond COVID-19. The results confirmed that the advanced metrics
generalised successfully, identifying distinct influential actors not captured
by traditional methods. In general, the findings suggest that a combination of
traditional and novel centrality measures offers a more robust and
generalisable framework for understanding and mitigating the spread of health
misinformation in different online network contexts.

</details>


### [643] [Advanced Health Misinformation Detection Through Hybrid CNN-LSTM Models Informed by the Elaboration Likelihood Model (ELM)](https://arxiv.org/abs/2507.09149)
*Mkululi Sikosana,Sean Maudsley-Barton,Oluwaseun Ajao*

Main category: cs.SI

TL;DR: 一项研究利用心理学模型（ELM）和混合深度学习模型（CNN-LSTM）来识别社交媒体上的健康错误信息，取得了高准确率，并提出了结合特征工程的改进方法。


<details>
  <summary>Details</summary>
Motivation: 为了有效应对 COVID-19 大流行期间全球公共卫生领域面临的健康错误信息挑战，本研究旨在利用潜在信息处理模型（ELM）来增强社交媒体上的错误信息检测能力。

Method: 本研究采用结合了卷积神经网络（CNN）和长短期记忆（LSTM）的混合模型，并融入了基于潜在信息处理模型（ELM）的特征，例如文本可读性、情感极性和启发式线索（如标点符号频率），以增强健康错误信息的检测能力。

Result: 结合 ELM 特征的混合模型在准确性、精确率、召回率、F1 分数和 ROC-AUC 方面均取得了优异的性能。具体而言，该模型实现了 97.37% 的准确率、96.88% 的精确率、98.50% 的召回率、97.41% 的 F1 分数和 99.50% 的 ROC-AUC。通过进一步结合特征工程，模型性能得到进一步提升，精确率达到 98.88%，召回率达到 99.80%，F1 分数达到 99.41%，ROC-AUC 达到 99.80%。

Conclusion: 本研究将潜在信息处理模型（ELM）的特征应用于卷积神经网络（CNN）和长短期记忆（LSTM）混合模型，以提高社交媒体健康错误信息检测的准确性和可靠性。研究结果表明，ELM特征显著提升了模型的检测性能，为有效应对健康错误信息提供了有价值的见解和实用的方法。

Abstract: Health misinformation during the COVID-19 pandemic has significantly
challenged public health efforts globally. This study applies the Elaboration
Likelihood Model (ELM) to enhance misinformation detection on social media
using a hybrid Convolutional Neural Network (CNN) and Long Short-Term Memory
(LSTM) model. The model aims to enhance the detection accuracy and reliability
of misinformation classification by integrating ELM-based features such as text
readability, sentiment polarity, and heuristic cues (e.g., punctuation
frequency). The enhanced model achieved an accuracy of 97.37%, precision of
96.88%, recall of 98.50%, F1-score of 97.41%, and ROC-AUC of 99.50%. A combined
model incorporating feature engineering further improved performance, achieving
a precision of 98.88%, recall of 99.80%, F1-score of 99.41%, and ROC-AUC of
99.80%. These findings highlight the value of ELM features in improving
detection performance, offering valuable contextual information. This study
demonstrates the practical application of psychological theories in developing
advanced machine learning algorithms to address health misinformation
effectively.

</details>


### [644] [Negotiating Comfort: Simulating Personality-Driven LLM Agents in Shared Residential Social Networks](https://arxiv.org/abs/2507.09657)
*Ann Nedime Nese Rende,Tolga Yilmaz,Özgür Ulusoy*

Main category: cs.SI

TL;DR: LLM智能体被用来模拟一个建筑内的社交网络和供暖决策，发现积极特质和特定行为会影响幸福感和决策。


<details>
  <summary>Details</summary>
Motivation: 为了模拟人类的细微行为，尤其是在难以进行复杂真实世界模拟的情况下。

Method: 使用由大型语言模型（LLMs）驱动的生成式智能体来模拟共享居民楼中的社交网络，并据此做出中央供暖系统的温度决策。智能体分为家庭成员和代表，他们会考虑个人偏好、个人特质、人际关系以及天气状况。模拟过程包括家庭内部达成共识，然后由代表们在整个建筑范围内做出决策。

Result: 测试了三种不同的人格特质分布（积极、混合和消极），发现积极特质与更高的幸福感和更牢固的友谊相关。温度偏好、自信心和无私程度对幸福感和决策有显著影响。

Conclusion: LLM驱动的智能体可以模拟细致的人类行为，尤其是在难以进行真实世界复杂模拟的场景中。

Abstract: We use generative agents powered by large language models (LLMs) to simulate
a social network in a shared residential building, driving the temperature
decisions for a central heating system. Agents, divided into Family Members and
Representatives, consider personal preferences, personal traits, connections,
and weather conditions. Daily simulations involve family-level consensus
followed by building-wide decisions among representatives. We tested three
personality traits distributions (positive, mixed, and negative) and found that
positive traits correlate with higher happiness and stronger friendships.
Temperature preferences, assertiveness, and selflessness have a significant
impact on happiness and decisions. This work demonstrates how LLM-driven agents
can help simulate nuanced human behavior where complex real-life human
simulations are difficult to set.

</details>


### [645] [Experimental Analysis and Evaluation of Cohesive Subgraph Discovery](https://arxiv.org/abs/2507.10262)
*Dahee Kim,Song Kim,Jeongseon Kim,Junghoon Kim,Kaiyu Feng,Sungsu Lim,Jungeun Kim*

Main category: cs.SI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Retrieving cohesive subgraphs in networks is a fundamental problem in social
network analysis and graph data management. These subgraphs can be used for
marketing strategies or recommendation systems. Despite the introduction of
numerous models over the years, a systematic comparison of their performance,
especially across varied network configurations, remains unexplored. In this
study, we evaluated various cohesive subgraph models using task-based
evaluations and conducted extensive experimental studies on both synthetic and
real-world networks. Thus, we unveil the characteristics of cohesive subgraph
models, highlighting their efficiency and applicability. Our findings not only
provide a detailed evaluation of current models but also lay the groundwork for
future research by shedding light on the balance between the interpretability
and cohesion of the subgraphs. This research guides the selection of suitable
models for specific analytical needs and applications, providing valuable
insights.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [646] [Relativistic electrodynamics with a universal length scale](https://arxiv.org/abs/2507.08872)
*Tiemo Pedergnana,Florian Kogelbauer*

Main category: quant-ph

TL;DR: 从四阶克莱因-戈登方程推导出狄拉克和泡利方程的类似物，并通过斯特恩-盖拉赫实验的电子束分裂进行了理论验证。


<details>
  <summary>Details</summary>
Motivation: 从具有普适长度尺度的空间四阶克莱因-戈登方程推导出狄拉克和泡利方程的类似物。

Method: 通过代数分解程序从奇异摄动的麦克斯韦方程组推导出32维狄拉克方程的变体。

Result: 电子束在斯特恩-盖拉赫实验中发生分裂，导致自旋算符的四个不同特征值，可分为两对，中心值约为±ħ/2。

Conclusion: 该理论具有粒子-反粒子不对称性和定向的微极性时空。

Abstract: We derive the analogues of the Dirac and Pauli equations from a spatially
fourth-order Klein--Gordon equation with a universal length scale. Starting
from a singularly perturbed variant of Maxwell's equations, we deduce a
32-dimensional variant of the Dirac equation for spin-$1/2$ particles through
an algebraic factorization procedure. We illustrate an experimental test of the
theory from the split lines of the electron beam in a Stern--Gerlach
experiment. This hyperfine splitting leads to four distinct eigenvalues of the
spin operator, which can be grouped into two pairs centered around the classic
values of $\pm\hbar/2$. The modified electrodynamic framework features
particle-antiparticle asymmetry and an oriented, micropolar spacetime.

</details>


### [647] [Variational subspace methods and application to improving variational Monte Carlo dynamics](https://arxiv.org/abs/2507.08930)
*Adrien Kahn,Luca Gravina,Filippo Vicentini*

Main category: quant-ph

TL;DR: 我们提出了一种新的形式主义，可以直接处理和优化子空间，避免了在子空间方法中使用优化个态的需要。我们还提出了一种名为 Bridge 的方法，通过提取变分时间演化状态的线性组合来提高变分动力学的性能。Bridge 被证明可以显著 Mitigate 离散化动力学产生的误差，并可以系统地作为变分动力学的后处理工具。


<details>
  <summary>Details</summary>
Motivation: 为了解决在子空间方法中使用优化个态的需要，提出了一种新的形式主义。

Method: 提出了一种新的形式主义，可以直接处理和优化子空间，以及一种名为 Bridge 的方法，通过提取变分时间演化状态的线性组合来提高变分动力学的性能。

Result: 该形式主义能够将距离和能量等概念自然地扩展到子空间，以及马尔可夫链蒙特卡洛估计量，并恢复了 Pfau 等人提出的激发态估计方法。Bridge 被证明可以显著 Mitigate 离散化动力学产生的误差。

Conclusion: Bridge是一种计算成本低廉且能够显著 Mitigate 离散化动力学产生的误差的 V.A.S.C. 方法，因此可以系统地作为 V.A.S.C. 的后处理工具。

Abstract: We present a formalism that allows for the direct manipulation and
optimization of subspaces, circumventing the need to optimize individual states
when using subspace methods. Using the determinant state mapping, we can
naturally extend notions such as distance and energy to subspaces, as well as
Monte Carlo estimators, recovering the excited states estimation method
proposed by Pfau et al. As a practical application, we then introduce Bridge, a
method that improves the performance of variational dynamics by extracting
linear combinations of variational time-evolved states. We find that Bridge is
both computationally inexpensive and capable of significantly mitigating the
errors that arise from discretizing the dynamics, and can thus be
systematically used as a post-processing tool for variational dynamics.

</details>


### [648] [Long-ranged gates in quantum computation architectures with limited connectivity](https://arxiv.org/abs/2507.08936)
*Wolfgang Dür*

Main category: quant-ph

TL;DR: 提出了一种量子计算架构，通过分离数据和纠缠生成量子比特，并利用中间电路测量实现并行长程两量子比特门，在平面结构中实现了 O(sqrt n) 的并发门操作，且兼容现有超导架构。


<details>
  <summary>Details</summary>
Motivation: 为了提高量子计算的效率和可扩展性，特别是通过并行执行多项长程两量子比特门。

Method: 提出了一种基于具有最近邻交互的几何结构（例如平面结构）的量子计算架构。通过将量子比特的角色有效地区分为数据量子比特和纠缠生成量子比特来实现这一点。在后者中生成多方纠缠态（例如二维簇态），并通过中间电路测量灵活地转换为多个长程贝尔态，用于在数据量子比特上并行执行多个两量子比特门。介绍了具有 n 个数据量子比特和 n 个辅助量子比特的平面架构，该架构允许在仅一轮最近邻门和一轮中间电路测量的情况下同时执行 O(sqrt n) 次长程两量子比特门。

Result: 所提出的平面架构允许在仅一轮最近邻门和一轮中间电路测量的情况下同时执行 O(sqrt n) 次长程两量子比特门，并且该方法可应用于现有的超导量子计算架构，仅需恒定的额外开销。

Conclusion: 所提出的方法适用于现有的超导量子计算架构，并且只带来恒定的额外开销。

Abstract: We propose a quantum computation architecture based on geometries with
nearest-neighbor interactions, including e.g. planar structures. We show how to
efficiently split the role of qubits into data and entanglement-generation
qubits. Multipartite entangled states, e.g. 2D cluster states, are generated
among the latter, and flexibly transformed via mid-circuit measurements to
multiple, long-ranged Bell states, which are used to perform several two-qubit
gates in parallel on data qubits. We introduce planar architectures with $n$
data and $n$ auxiliary qubits that allow one to perform $O(\sqrt n)$
long-ranged two-qubit gates simultaneously, with only one round of nearest
neighbor gates and one round of mid-circuit measurements. We also show that our
approach is applicable in existing superconducting quantum computation
architectures, with only a constant overhead.

</details>


### [649] [Robust Chiral Edge Dynamics of a Kitaev Honeycomb on a Trapped Ion Processor](https://arxiv.org/abs/2507.08939)
*Ammar Ali,Joe Gibbs,Keerthi Kumaran,Varadharajan Muruganandam,Bo Xiao,Paul Kairys,Gábor Halász,Arnab Banerjee,Phillip C. Lotshaw*

Main category: quant-ph

TL;DR: 在捕获离子量子处理器上模拟Kitaev蜂窝模型，证明了其拓扑序的动力学信号，并研究了Heisenberg相互作用对拓扑保护的影响。


<details>
  <summary>Details</summary>
Motivation: Kitaev蜂窝模型是具有非阿贝尔任意子和拓扑保护边缘模式的量子自旋液体的范例性精确可解系统，但实际材料中的二次相互作用会阻碍自旋液体行为的实现，因此需要新的量子计算方法进行有效模拟。

Method: 在捕获离子量子处理器上对包含22个格子的Kitaev蜂窝模型进行量子模拟，分别模拟了无和有Heisenberg相互作用的情况。开发了制备基态的量子电路，并通过施加受控扰动和测量边缘的时间依赖自旋关联来研究系统。

Result: 在非阿贝尔相中，观察到了与非零陈数一致的手征边缘动力学，这是拓扑序的一个标志，在过渡到阿贝尔曲码相时消失。在非可积Kitaev-Heisenberg模型中，弱Heisenberg相互作用保留了手征边缘动力学，而较强的耦合则抑制了它们，表明拓扑保护被破坏。

Conclusion: 该工作展示了利用可编程量子硬件探测量子自旋液体中拓扑序动力学信号的可行途径，为强关联材料的量子模拟开辟了新道路。

Abstract: Kitaev's honeycomb model is a paradigmatic exactly solvable system hosting a
quantum spin liquid with non-Abelian anyons and topologically protected edge
modes, offering a platform for fault-tolerant quantum computation. However,
real candidate Kitaev materials invariably include complex secondary
interactions that obscure the realization of spin-liquid behavior and demand
novel quantum computational approaches for efficient simulation. Here we report
quantum simulations of a 22-site Kitaev honeycomb lattice on a trapped-ion
quantum processor, without and with non-integrable Heisenberg interactions that
are present in real materials. We develop efficient quantum circuits for
ground-state preparation, then apply controlled perturbations and measure
time-dependent spin correlations along the system's edge. In the non-Abelian
phase, we observe chiral edge dynamics consistent with a nonzero Chern number
-- a hallmark of topological order -- which vanishes upon transition to the
Abelian toric code phase. Extending to the non-integrable Kitaev-Heisenberg
model, we find that weak Heisenberg interactions preserve chiral edge dynamics,
while stronger couplings suppress them, signaling the breakdown of topological
protection. Our work demonstrates a viable route for probing dynamical
signatures of topological order in quantum spin liquids using programmable
quantum hardware, opening new pathways for quantum simulation of strongly
correlated materials.

</details>


### [650] [Quantum Algorithm for Protein Structure Prediction Using the Face-Centered Cubic Lattice](https://arxiv.org/abs/2507.08955)
*Rui-Hao Li,Hakan Doga,Bryan Raubenolt,Sarah Mostame,Nicholas DiSanto,Fabio Cumbo,Jayadev Joshi,Hanna Linn,Maeve Gaffney,Alexander Holden,Vinooth Kulkarni,Vipin Chaudhary,Kenneth M. Merz Jr,Abdullah Ash Saki,Tomas Radivoyevitch,Frank DiFilippo,Jun Qin,Omar Shehab,Daniel Blankenberg*

Main category: quant-ph

TL;DR: 该研究首次使用量子算法（PolyFit和VQEC）在FCC点阵模型上预测蛋白质结构，并在更先进的Heron R2量子计算机上取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 研究的动机源于观察到面心立方（FCC）点阵在模拟蛋白质真实二级结构方面比其他点阵具有更强的能力，并通过均方根偏差（RMSD）进行了论证。

Method: 该研究采用了两种量子算法来预测蛋白质结构：基于拉格朗日对偶原理的多项式拟合（PolyFit）和变分量子特征求解器（VQEC）。这两种方法被成功部署在IBM的Eagle R3和Heron R2量子计算机上，以解决面心立方（FCC）点阵模型在蛋白质结构预测中的应用。

Result: 研究成功地在Eagle R3和Heron R2量子计算机上恢复了6个氨基酸序列KLVFFA在噪声下的基态构象。通过对比分析，发现在Heron R2量子计算机上，PolyFit和VQEC在预测和采样最优解方面相比Eagle R3分别取得了近两倍和近三倍的提升，这表明量子硬件的进步对该应用产生了显著影响。

Conclusion: 该研究首次实现了使用量子算法预测蛋白质结构的面心立方（FCC）点阵模型。研究结果表明，与其它点阵相比，FCC点阵在模拟蛋白质真实二级结构方面具有更强的能力。研究还对比了多项式拟合（PolyFit）和基于拉格朗日对偶原理的变分量子特征求解器（VQEC）这两种量子方法，并将它们成功部署在IBM的Eagle R3和Heron R2量子计算机上，能够在噪声下恢复6个氨基酸序列KLVFFA的基态构象。结果显示，与Eagle R3相比，Heron R2在预测和采样最优解（基态构象）方面表现出显著提升（PolyFit接近两倍，VQEC接近三倍），凸显了量子硬件进步对该应用的推动作用。

Abstract: In this work, we present the first implementation of the face-centered cubic
(FCC) lattice model for protein structure prediction with a quantum algorithm.
Our motivation to encode the FCC lattice stems from our observation that the
FCC lattice is more capable in terms of modeling realistic secondary structures
in proteins compared to other lattices, as demonstrated using root mean square
deviation (RMSD). We utilize two quantum methods to solve this problem: a
polynomial fitting approach (PolyFit) and the Variational Quantum Eigensolver
with constraints (VQEC) based on the Lagrangian duality principle. Both methods
are successfully deployed on Eagle R3 (ibm_cleveland) and Heron R2
(ibm_kingston) quantum computers, where we are able to recover ground state
configurations for the 6-amino acid sequence KLVFFA under noise. A comparative
analysis of the outcomes generated by the two QPUs reveals a significant
enhancement (reaching nearly a two-fold improvement for PolyFit and a
three-fold improvement for VQEC) in the prediction and sampling of the optimal
solution (ground state conformations) on the newer Heron R2 architecture,
highlighting the impact of quantum hardware advancements for this application.

</details>


### [651] [Approximate quantum circuit compilation for proton-transfer kinetics on quantum processors](https://arxiv.org/abs/2507.08996)
*Arseny Kovyrshin,Dilhan Manawadu,Edoardo Altamura,George Pennington,Benjamin Jaderberg,Sebastian Brandhofer,Anton Nykänen,Aaron Miller,Walter Talarico,Stefan Knecht,Fabijan Pavošević,Alberto Baiardi,Francesco Tacchino,Ivano Tavernelli,Stefano Mensa,Jason Crain,Lars Tornberg,Anders Broo*

Main category: quant-ph

TL;DR: 利用量子计算模拟化学中的质子转移反应，研究了近期量子硬件的潜力和局限性。


<details>
  <summary>Details</summary>
Motivation: 质子转移反应在化学和生物系统中至关重要，但传统方法难以准确模拟其量子效应，并且随着系统规模的增大，计算成本会呈指数级增长。

Method: 开发并演示了基于核-电子轨道（NEO）框架的量子计算算法，将质子量子化，并结合了冻结自然轨道（FNO）近似和自适应量子编译。

Result: 在对ibm_fez设备进行转译的各种压缩水平下，使用硬件噪声模型，计算了质子转移路径上的势垒高度和离域质子密度，估算出的能量势垒在13%的误差范围内。

Conclusion: 尽管存在硬件限制，但该研究提出了一种实用的方法，可以在近期设备和早期容错量子计算系统中近似关键的电路片段。

Abstract: Proton transfer reactions are fundamental to many chemical and biological
systems, where quantum effects such as tunneling, delocalization, and
zero-point motion play key kinetic control roles. However, classical methods
capable of accurately capturing these phenomena scale prohibitively with system
size. Here, we develop and demonstrate quantum computing algorithms based on
the Nuclear-Electronic Orbital framework, treating the transferring proton
quantum mechanically. We assess the potential of current quantum devices for
simulating proton transfer kinetics with high accuracy. We first construct a
deep initial ans\"atze within a truncated orbital space by employing the frozen
natural orbital approximation. Then, to balance circuit depth against state
fidelity, we implement an adaptive form of approximate quantum compiling. Using
resulting circuits at varying compression levels transpiled for the ibm_fez
device, we compute barrier heights and delocalised proton densities along the
proton transfer pathway using a realistic hardware noise model. We find that,
although current quantum hardware introduces significant noise relative to the
demanding energy tolerances involved, our approach allows substantial circuit
simplification while maintaining energy barrier estimates within 13% of the
reference value. Despite present hardware limitations, these results offer a
practical means of approximating key circuit segments in near-term devices and
early fault-tolerant quantum computing systems.

</details>


### [652] [Photon-Number-Resolving Detector Based on a Cascade of Waveguide-Coupled Quantum Emitters](https://arxiv.org/abs/2507.09034)
*Abdolreza Pasharavesh,Sai Sreesh Venuturumilli,Michal Bajcsy*

Main category: quant-ph

TL;DR: 一种新的光子数探测器，通过级联波导耦合的λ型发射器实现，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 研究由一系列波导耦合的λ型发射器组成的单光子数分辨（PNR）探测器的操作。

Method: 使用格林函数和输入-输出形式主义推导了散射矩阵和光子-光子相关器，并使用量子轨迹方法计算了可实现的精度并分析了其对关键系统参数的依赖性。

Result: 我们获得了探测器在线性区域的精度表达式，并预测了由非线性光子-光子相互作用产生 的相关性如何影响该精度。我们还通过量子轨迹方法计算了非线性区域中的可实现精度，并将其与基于分束器的空间解复用方案进行了比较。

Conclusion: 所提出的级联探测器在现实条件下优于传统探测器，是下一代光子数探测器的有力候选者。

Abstract: We investigate the operation of a photon-number-resolving (PNR) detector
consisting of a cascade of waveguide-coupled lambda-type emitters, where each
waveguide-coupled emitter extracts a single photon from the input light and
sends it to a single-photon detector. Using Green's function and input-output
formalisms, we derive the scattering matrices and photon-photon correlators for
individual scatterers. By cascading these results, we obtain a closed-form
expression for the detector's precision in the linear regime and predict how
correlations generated by nonlinear photon-photon interactions influence this
precision. To evaluate the performance of this PNR detector in the nonlinear
regime, we apply the quantum trajectory method to the cascaded setup,
calculating the achievable precision and analyzing its dependence on key system
parameters, such as the number of emitters and their coupling strength to the
waveguide. We compare the performance of the proposed PNR detector with that of
a conventional PNR scheme based on spatial demultiplexing via beamsplitters.
Our results indicate that the proposed scheme can outperform conventional
detectors under realistic conditions, making it a promising candidate for
next-generation PNR detection.

</details>


### [653] [Microcausality and Tunneling Times in Relativistic Quantum Field Theory](https://arxiv.org/abs/2507.09066)
*Mohammed Alkhateeb,Alex Matzkin*

Main category: quant-ph

TL;DR: 本研究采用时空分辨的相对论量子场论方法研究隧穿现象，证明了微观因果关系阻止了超光速隧穿动力学。研究表明，在存在背景势的情况下，狄拉克场和克莱因-戈登场的微观因果关系成立。对初始波包的局域区域进行干预，不会对光锥之外的区域产生任何影响。通过对狄拉克费米子和克莱因-戈登玻色子的数值计算，说明了这些结果的有效性。


<details>
  <summary>Details</summary>
Motivation: 研究微观因果关系是否阻止超光速隧穿动力学。

Method: 在时空分辨的相对论量子场论框架下对隧穿进行研究。

Result: 微观因果关系对于在背景势存在下的狄拉克场和克莱因-戈登场成立。对初始波包的局域区域进行干预，不会对从该区域发出的光锥之外的区域产生任何影响。

Conclusion: 微观因果关系阻止了超光速隧穿动力学。

Abstract: We show, in the framework of a space-time resolved relativistic quantum eld
theory approach to tunneling, that microcausality precludes superluminal
tunneling dynamics. More specically in this work dealing with Dirac and
Klein-Gordon elds, we rst prove that microcausality holds for such elds in the
presence of a background potential. We then use this result to show that an
intervention performed on a localized region of an initial wave packet
subsequently scattering on a potential barrier does not result in any eect
outside the light cone emanating from that region. We illustrate these results
with numerical computations for Dirac fermions and Klein-Gordon bosons.

</details>


### [654] [Time crystals and nonequilibrium dissipative phase transitions mediated by squeezed bath](https://arxiv.org/abs/2507.09072)
*Zhenghao Zhang,Qingtian Miao,G. S. Agarwal*

Main category: quant-ph

TL;DR: 该研究探讨了在受挤压真空环境影响的非平衡耗散相变中实现时间晶体的可能性，并分析了环境对系统动力学的影响。


<details>
  <summary>Details</summary>
Motivation: 研究耗散相变的时间晶体行为，特别是在受挤压真空环境影响的情况下，并探讨环境关联对系统动力学的影响。

Method: 通过研究描述驱动和协同作用的Liouvillian算子的特征值来研究耗散相变的时间晶体行为。分析了稳态相图，并进行了时域模拟以验证振荡频率与特征值虚部的对应关系。

Result: 挤压真空环境可以锐化耗散相变，并在热力学极限下产生纯虚数特征值，表明存在时间晶体相。亚主导特征值的实部随挤压强度的增加表现出非单调行为，并且时域模拟证实了振荡频率与特征值虚部的对应关系。

Conclusion: 研究结果强调了在基于工程环境的合作模型中研究时间晶体的重要性，并展示了挤压真空 ज्यामुळे耗散相变锐化，以及在耗散相变中实现时间晶体的可能性。

Abstract: Nonequilibrium dissipative phase transition, arising from the competition of
cooperative behavior and coherent field driving, discovered in the 1970s by
Narducci et al. and Walls et al., has been found to exhibit time-crystal
behavior when the driving field exceeds the cooperative decay rate. This was
seen through the study of the eigenvalues of the Liouvillian superoperator that
describes the joint effect of drive and cooperativity. The cooperative decay
depends on the nature of the reservoir correlations. If the reservoir
correlations have phase-sensitive behavior, then the eigenvalues of the
Liouvillian will be different. We investigate the time-crystal behavior of the
nonequilibrium dissipative phase transitions under the influence of a squeezed
vacuum reservoir. We analyze the steady-state phase diagram as a function of
the control parameter and demonstrate that increasing the squeezing strength
sharpens the dissipative phase transition. Spectral analysis of the Liouvillian
reveals gap closings and the emergence of purely imaginary eigenvalues in the
thermodynamic limit, indicating the time-crystal phase. We find that the real
parts of subleading eigenvalues exhibit nonmonotonic behavior with increasing
squeezing, reflecting the sensitivity of relaxation dynamics to the reservoir
properties. Time-domain simulations confirm that the oscillation frequencies
correspond to the imaginary parts of the Liouvillian eigenvalues. We also
present results on quantum fluctuations in the time-crystal phase. Our results
call attention to the study of time crystals in models of cooperativity based
on engineered environments.

</details>


### [655] [The temporal resolution limit in quantum sensing](https://arxiv.org/abs/2507.09172)
*Cong-Gang Song,Qing-yu Cai*

Main category: quant-ph

TL;DR: 量子传感的时间分辨率可通过结合量子态可区分性和量子速度极限来优化，该研究提出了计算最小探测时间的方法。


<details>
  <summary>Details</summary>
Motivation: 量子传感的时间分辨率是一个关键指标。

Method: 本研究将可区分的量子态条件与量子速度极限相结合，为探测时间设定了下限。

Result: 本研究推导了探测时间的下限，并说明了量子速度极限对磁场测量和时间分辨率的影响。

Conclusion: 本研究将可区分的量子态条件与量子速度极限相结合，为探测时间设定了下限。当探测时间低于此边界时，输出态与输入态将无法区分，信息会丢失在噪声中。本研究将此结论推广到含时信号哈密顿量，并提出理论上利用量子控制技术计算任意信号哈密顿量的最小探测时间。

Abstract: Temporal resolution is a critical figure of merit in quantum sensing. This
study combines the distinguishable condition of quantum states with quantum
speed limits to establish a lower bound on interrogation time. When the
interrogation time falls below this bound, the output state becomes
statistically indistinguishable from the input state, and the information will
inevitably be lost in noise. Without loss of generality, we extend these
conclusions to time-dependent signal Hamiltonian. In theory, leveraging certain
quantum control techniques allows us to calculate the minimum interrogation
time for arbitrary signal Hamiltonian. Finally, we illustrate the impact of
quantum speed limits on magnetic field measurements and temporal resolution.

</details>


### [656] [Spin Squeezing in Electron Microscopy](https://arxiv.org/abs/2507.09243)
*Shiran Even-Haim,Ethan Nussinson,Roni Ben-Maimon,Alexey Gorlach,Ron Ruimy,Ephraim Shahmoon,Osip Schwartz,Ido Kaminer*

Main category: quant-ph

TL;DR: 量子计量学中的自旋压缩技术有望提高电子显微镜的信噪比。


<details>
  <summary>Details</summary>
Motivation: 电子显微镜的信噪比受到电子束引起的损伤所施加的剂量限制的严重制约，而自旋压缩作为一种量子计量学方法，有望改善电子显微镜的信噪比。

Method: 通过电子-电子库仑相互作用和量子非破坏性测量来生成自旋压缩态。

Result: 理论上证明了自旋压缩可以提高电子显微镜的信噪比，并且可以通过电子-电子库仑相互作用和量子非破坏性测量产生所需的纠缠态。

Conclusion: 本研究在理论上证明了自旋压缩（一种基于纠缠的量子计量学）可以应用于提高电子显微镜的信噪比，并且可以通过电子-电子库仑相互作用和量子非破坏性测量产生所需的纠缠态，从而将量子计量学与电子干涉测量学领域联系起来，为实现超越散粒噪声极限的电子显微镜铺平了道路。

Abstract: Quantum metrology experiments in atomic physics and quantum optics have
demonstrated measurement accuracy beyond the shot-noise limit via
multi-particle entanglement. At the same time, electron microscopy, an
essential tool for high-resolution imaging of biological systems, is severely
constrained in its signal-to-noise ratio (SNR) by shot noise, due to the dose
limit imposed by electron beam-induced damage. Here, we show theoretically that
spin squeezing, a form of quantum metrology based on entanglement, is a natural
fit for improving the SNR in electron microscopy. We investigate the generation
of the necessary entangled states through electron-electron Coulomb
interactions and quantum non-demolition measurements. Our results connect the
fields of quantum metrology and electron interferometry, paving the way toward
electron microscopy with SNR beyond the shot-noise limit.

</details>


### [657] [Order-preserving condition for coherence measures of projective measurements with One Example](https://arxiv.org/abs/2507.09261)
*Hai Wang*

Main category: quant-ph

TL;DR: 该研究提出了一种新的量子叠加度量标准，并验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 量化量子叠加是量子力学中的一个基本且关键的问题，相干性理论是其中一个典型的例子。

Method: 提出“order-preserving condition”作为度量标准，并通过推广相干性的1/2亲和性来验证该度量满足该条件及其他合理条件。

Result: 验证了推广后的相干性度量满足“order-preserving condition”及其他合理条件。

Conclusion: “order-preserving condition”被提出作为量子叠加的度量标准，并且通过推广相干性的1/2亲和性来验证其作为良好相干性度量的合理性。最后，探讨了在POVM情况下该条件的应用。

Abstract: Superposition is an essential feature of quantum mechanics. From the
Schrodinger's cat to quantum algorithms such as Deutsch-Jorsza algorithm,
quantum superposition plays an important role. It is one fundamental and
crucial question how to quantify superposition. Until now, the framework of
coherence has been well established as one typical instance of quantum resource
theories. And the concept of coherence has been generalized into linearly
independent basis, projective measurements and POVMs. In this work, we will
focus on coherence measures for projective measurements or orthogonal
subspaces. One new condition, order-preserving condition, is proposed for such
measures. This condition is rooted in the mathematical structure of Hilbert
spaces' orthogonal decomposition. And by generalizing the 1/2-affinity of
coherence into subspace cases, we verify that this generalized coherence
measure satisfies the order-preserving condition. And it also satisfies other
reasonable conditions to be a good coherence measure. As the partial order
relationship exists for not only projective measurements, but also POVMs, it's
natural to study the order-preserving condition in POVM cases, which will be
the last part of this work.

</details>


### [658] [Defects and their Time Scales in Quantum and Classical Annealing of the Two-Dimensional Ising Model](https://arxiv.org/abs/2507.09273)
*Phillip Weinberg,Na Xu,Anders W. Sandvik*

Main category: quant-ph

TL;DR: 该研究通过数值模拟分析了量子退火过程中二维伊辛模型中的缺陷动力学。结果表明，缺陷演化遵循Kibble-Zurek标度律，并与畴壁拓扑有关。研究还提出了一种用于分析缺陷动力学的新方法。


<details>
  <summary>Details</summary>
Motivation: 为了理解量子退火过程中缺陷的产生和演化机制，特别是与量子相变和退火动力学相关的行为。

Method: 使用精确数值解法（exact numerical solutions）研究了L×L二维横向场伊辛铁磁体（transverse-field Ising ferromagnet）在从高场到零场量子退火（quantum annealing）过程中的缺陷行为。

Result: 在量子相变处观察到预期的Kibble-Zurek (KZ) 时间标度（∝ L^(z+1/ν)），并且基态保真度也显示出相同的KZ标度律。激发态的演化表现为受限缺陷的粗化动力学（时间标度∝ L^2）和跨系统缺陷的界面涨落（寿命∝ L^3）。研究发现，不同拓扑的畴壁在退火过程中具有不同的动力学时间尺度（W=(1,0)/(0,1)为∝ L^3，W=(1,1)为∝ L^3.4）。W=(1,1)畴壁的时间尺度超过经典KZ时间尺度，导致其在经典模拟退火的零温状态下概率与KZ指数相关。在量子退火中，W=(1,0)/(0,1)畴壁也受KZ时间尺度（∝ L^2.59）控制。开发了一种检测L^3时间尺度的特殊方法，适用于量子退火实验。

Conclusion: 本研究通过精确数值解法研究了二维横向场伊辛铁磁体在量子退火过程中缺陷的演化。研究发现，在量子相变点，系统表现出预期的Kibble-Zurek (KZ) 时间标度，并且基态保真度也呈现相同的KZ标度律。此外，研究还揭示了激发态的演化过程，包括缺陷的粗化动力学和跨系统缺陷的界面涨落。通过与经典模拟退火的类比，研究详细刻画了跨系统缺陷的性质，并发现了不同拓扑（W=(1,0)/(0,1)和W=(1,1)）的畴壁在动力学时间尺度上的差异。特别地，对于W=(1,1)畴壁，其动力学时间尺度超过了经典的KZ时间尺度，导致在经典模拟退火的最终状态下，其概率与KZ指数相关。在量子退火过程中，W=(1,0)/(0,1)畴壁也受到KZ时间尺度的控制。研究开发了一种检测L^3时间尺度的分析方法，该方法有望应用于实际的量子退火实验。

Abstract: We investigate defects in the two-dimensional transverse-field Ising
ferromagnet on periodic $L\times L$ lattices after quantum annealing from high
to vanishing field. With exact numerical solutions for $L \le 6$, we observe
the expected critical Kibble-Zurek (KZ) time scale $\propto L^{z+1/\nu}$ (with
$z=1$ and $1/\nu \approx 1.59$) at the quantum phase transition. We also
observe KZ scaling of the ground-state fidelity at the end of the process. The
excitations evolve by coarsening dynamics of confined defects, with a time
scale $\propto L^2$, and interface fluctuations of system-spanning defects,
with life time $\propto L^3$. We build on analogies with classical simulated
annealing, where we characterize system-spanning defects in detail and find
differences in the dynamic scales of domain walls with winding numbers
$W=(1,0)/(0,1)$ (horizontal/vertical) and $W=(1,1)$ (diagonal). They decay on
time scales $\propto L^3$ (which applies also to system-spanning domains in
systems with open boundaries) and $\propto L^{3.4}$, respectively, when imposed
in the ordered phase. As a consequence of $L^{3.4}$ exceeding the classical KZ
scale $L^{z+1/\nu}=L^{3.17}$ the probability of $W=(1,1)$ domains in SA scales
with the KZ exponent even in the final $T=0$ state. In QA, also the
$W=(1,0)/(0,1)$ domains are controlled by the KZ time scale $L^{2.59}$. The
$L^3$ scale can nevertheless be detected in the excited states, using a method
that we develop that should also be applicable in QA experiments.

</details>


### [659] [Impedance-Engineered Josephson Parametric Amplifier with Single-Step Lithography](https://arxiv.org/abs/2507.09298)
*Lipi Patel,Samarth Hawaldar,Aditya Panikkar,Athreya Shankar,Baladitya Suri*

Main category: quant-ph

TL;DR: 我们演示了一种单步光刻制备的宽带约瑟夫森参数放大器，实现了接近量子限的增益和较宽的带宽。我们还扩展了理论模型以解释实验结果。


<details>
  <summary>Details</summary>
Motivation: 为了实现更简单、宽带的约瑟夫森参数放大器，并为新型操作模式提供理论基础。

Method: 通过单步光刻工艺制造阻抗工程约瑟夫森参数放大器（IEJPA），使用集总参数LC电路实现阻抗工程。采用电子束光刻和双角Dolan桥技术沉积Al-AlOx-Al。

Result: 在400 MHz带宽和5.3 GHz中心频率下实现了接近量子限的18 dB增益，饱和功率为-114 dBm。扩展的理论模型能准确解释实验结果。

Conclusion: 本文展示了一种单步光刻工艺制备的阻抗工程约瑟夫森参数放大器（IEJPA）。该放大器在400 MHz带宽和5.3 GHz中心频率下实现了接近量子限的18 dB增益，饱和功率为-114 dBm。通过将约瑟夫森参数放大器和变压器的完整正弦非线性纳入理论模型，我们为实验结果提供了理论基础，并为更简单地实现宽带约瑟夫森参数放大器指明了方向。

Abstract: We present the experimental demonstration of an impedance-engineered
Josephson parametric amplifier (IEJPA) fabricated in a single-step lithography
process. Impedance engineering is implemented using a lumped-element series LC
circuit. We use a simpler lithography process where the entire device --
impedance transformer and JPA -- are patterned in a single electron beam
lithography step, followed by a double-angle Dolan bridge technique for
Al-AlO$_x$-Al deposition. We observe nearly quantum-limited amplification with
18 dB gain over a wide 400 MHz bandwidth centered around 5.3 GHz, and a
saturation power of -114 dBm. To accurately explain our experimental results,
we extend existing theories for impedance-engineered JPAs to incorporate the
full sine nonlinearity of both the JPA and the transformer. Our work shows a
path to simpler realization of broadband JPAs and provides a theoretical
foundation for a novel regime of JPA operation.

</details>


### [660] [Superinductor-based ultrastrong coupling in a superconducting circuit](https://arxiv.org/abs/2507.09339)
*Alba Torras-Coloma,Luca Cozzolino,Ariadna Gómez-del-Pulgar-Martínez,Elia Bertoldo,P. Forn-Díaz*

Main category: quant-ph

TL;DR: 提出了一種基於超導體的超強耦合，用於研究具有小回路面積和低持久電流的磁通量子比特的超強耦合物理學。


<details>
  <summary>Details</summary>
Motivation: 提出了一個基於超導體的超強耦合，該耦合由一個急劇耦合到諧振器的磁通量子比特組成，用於研究超強耦合物理學。

Method: 提出了一個基於超導體的超強耦合，該耦合由一個急劇耦合到諧振器的磁通量子比特組成。

Result: 非微擾超強耦合（USC）的證明。我們測量了 Bloch-Siegert 位移和耦合率 $g/egin{smallmatrix} \omega \end{smallmatrix}_r \approx 0.13$，進入了 USC 狀態。我們獨立估計了耦合器的電感，其值與 $g/\omega \gtrsim 0.1$ 的條件一致。

Conclusion: 超導體是研究具有小回路面積和低持久電流的磁通量子比特的超強耦合物理學的有前途的工具。

Abstract: We present an ultrastrong superinductor-based coupling consisting of a flux
qubit galvanically coupled to a resonator. The coupling inductor is fabricated
in granular Aluminum, a superinductor material able to provide large surface
inductances. Spectroscopy measurements on the qubit-resonator system reveal a
Bloch-Siegert shift of \SI{23}{\mega\hertz} and a coupling fraction of
$g/\omega_r \simeq 0.13$, entering the perturbative ultrastrong coupling (USC)
regime. We estimate the inductance of the coupler independently by
low-temperature resistance measurements providing $L_c =
(0.74\pm0.14)\,\mathrm{nH}$, which is compatible with $g/\omega \gtrsim 0.1$.
Our results show that superinductors are a promising tool to study USC physics
in high-coherence circuits using flux qubits with small loop areas and low
persistent currents.

</details>


### [661] [Local unitary decomposition of tripartite arbitrary leveled qudit stabilizer states into $p$-level-qudit EPR and GHZ state](https://arxiv.org/abs/2507.09416)
*Yat Wong,Liang Jiang*

Main category: quant-ph

TL;DR: 通过新的局部酉变换，将任意维度多准则稳定器态分解为基本纠缠单元，以实现高效的纠缠分发。


<details>
  <summary>Details</summary>
Motivation: 为了将纠缠结构从量子比特和无平方因子多准则稳定器态推广到任意整数维度 D 的多准则稳定器态。

Method: 利用子系统相位矩阵来表征纠缠，并开发了一种超越 Clifford 群的局部酉变换算法，以实现对素数幂次多准则稳定器态的分解。

Result: 成功将素数幂次多准则稳定器态分解为 p 级 GHZ 态、EPR 对和非纠缠多准则系统，为需要高效分发纠缠的量子协议提供了新的方法。

Conclusion: 通过引入超越 Clifford 群的局部酉变换，将素数幂次多准则稳定器态分解为 p 级 GHZ 态、EPR 对和非纠缠多准则系统。

Abstract: We study the entanglement structure of tripartite stabilizer states on $N$
qudits of dimension $D$, distributed across parties $A$, $B$, and $C$, under
arbitrary local unitaries. Prior work by Bravyi et al. and Looi et al. showed
that qubit and squarefree qudit stabilizer states can be transformed via local
Clifford unitaries into tensor products of GHZ states, EPR pairs, and
unentangled qudits [arXiv:quant-ph/0504208, arXiv:1107.1761]. We generalize
this to arbitrary integer $D$ by introducing local unitaries beyond the
Clifford group, enabling decomposition of prime-power qudit stabilizer states
into $p$-level GHZ states, EPR pairs, and unentangled qudits. Our algorithm
leverages subsystem phase matrices to characterize entanglement and applies to
quantum protocols requiring efficient entanglement distribution.

</details>


### [662] [Simulating plasma wave propagation on a superconducting quantum chip](https://arxiv.org/abs/2507.09479)
*Bhuvanesh Sundar,Bram Evert,Vasily Geyko,Andrew Patterson,Ilon Joseph,Yuan Shi*

Main category: quant-ph

TL;DR: 通过在量子芯片上模拟等离子体波，为研究强耦合等离子体开辟了新途径。


<details>
  <summary>Details</summary>
Motivation: 量子计算机可能有一天能够有效地模拟经典计算无法达到的强耦合等离子体。

Method: 利用高保真度和高度表现力的设备原生门，结合新颖的误差缓解技术，模拟了激光脉冲从不均匀等离子体中散射。

Result: 我们证明了在线性等离子体波传播的超导量子芯片上的模拟。

Conclusion: 这项工作为研究经典计算机无法有效模拟的更复杂现象打开了 avenues，例如强耦合等离子体失衡时的非线性量子动力学。

Abstract: Quantum computers may one day enable the efficient simulation of
strongly-coupled plasmas that lie beyond the reach of classical computation in
regimes where quantum effects are important and the scale separation is large.
In this letter, we take the first step towards efficient simulation of quantum
plasmas by demonstrating linear plasma wave propagation on a superconducting
quantum chip. Using high-fidelity and highly expressive device-native gates,
combined with a novel error mitigation technique, we simulate the scattering of
laser pulses from inhomogeneous plasmas. Our approach is made feasible by the
identification of a suitable local spin model whose excitations mimic plasma
waves, whose circuit implementation requires a lower gate count than other
proposed approaches that would require a future fault-tolerant quantum
computer. This work opens avenues to study more complicated phenomena that
cannot be simulated efficiently on classical computers, such as nonlinear
quantum dynamics when strongly-coupled plasmas are driven out of equilibrium.

</details>


### [663] [A theoretical treatment of optical metasurfaces as an efficient basis for quantum correlations](https://arxiv.org/abs/2507.09517)
*Ramaseshan R,Prateek P. Kulkarni,Sharanya Madhusudhan,Kaustav Bhowmick*

Main category: quant-ph

TL;DR: Metasurfaces can compactly generate high-fidelity quantum entanglement (Bell states) and maintain it for a significant duration, offering a promising alternative to traditional methods in quantum technology.


<details>
  <summary>Details</summary>
Motivation: Conventional methods for generating quantum entanglement often require bulky setups and precise phase control. Metasurfaces present a compact and tunable alternative for quantum photonics.

Method: The study analyzes the evolution of an initially separable spin state under a metasurface interaction Hamiltonian to demonstrate Bell state generation. It also evaluates classical and quantum correlations, the impact of environmental decoherence, and computes quantum discord.

Result: Metasurfaces can generate Bell states with a concurrence of approximately 0.995 and maintain quantum discord for up to 29 microseconds, indicating high-fidelity entanglement generation and robustness against decoherence.

Conclusion: Metasurfaces offer a scalable and high-fidelity platform for generating entangled Bell states, with potential applications in next-generation quantum photonic architectures.

Abstract: Entanglement is a cornerstone of quantum technology, playing a key role in
quantum computing, cryptography, and information processing. Conventional
methods for generating entanglement via optical setups rely on beam splitters,
nonlinear media, or quantum dots, which often require bulky configurations and
precise phase control. In contrast, metasurfaces - ultrathin, engineered
optical interfaces - offer a compact and tunable alternative for quantum
photonics. In this work, we demonstrate that metasurfaces can serve as a
promising platform for generating Bell states through a Hamiltonian-driven
spin-entanglement mechanism. By analyzing the system's evolution under a
metasurface interaction Hamiltonian, we show that an initially separable spin
state evolves into a maximally entangled Bell state. We further study classical
and quantum correlations, evaluate the impact of environmental decoherence, and
compute quantum discord to quantify correlation robustness beyond entanglement.
Our analysis shows that metasurfaces can generate Bell states with a
concurrence of about 0.995 and maintain quantum discord for up to 29
microseconds. These results establish metasurfaces as scalable, high-fidelity
components for next-generation quantum photonic architectures.

</details>


### [664] [Echoes in a parametrically perturbed Kerr-nonlinear oscillator](https://arxiv.org/abs/2507.09521)
*Yun-Wen Mao,Ilia Tutunnikov,Roman V. Krems,Ilya Sh. Averbukh*

Main category: quant-ph

TL;DR: 研究了Kerr振荡器中相干态和猫态的回声动力学。发现弱驱动会产生经典回声，而猫态会产生对相位和权重敏感的量子回声。耗散会影响量子回声，但可以通过调整扰动来恢复。结果可用于猫量子比特的误差管理。


<details>
  <summary>Details</summary>
Motivation: 研究Kerr振荡器中经典和量子回声的性质，以及它们对相干态和猫态的响应，旨在为表征和减轻猫量子比特的误差提供参考。

Method: 研究了由频率控制的脉冲扰动驱动的Kerr振荡器中的经典回声和量子回声。考虑了单个相干态和由两个相干态的平衡和不平衡叠加构造的薛定谔猫态的动力学响应。

Result: 经典相干态产生长寿命的经典回声。猫态产生对初始相对相位和权重敏感的量子回声。耗散会影响量子回声和猫态的量子复兴，但可以通过调整扰动参数来恢复量子回声。

Conclusion:  Kerr振荡器中，弱参量驱动会产生长寿命的经典回声。对于猫态，量子回声对初始相对相位和叠加的相干态权重敏感。即使耗散抑制了量子复兴，通过调整扰动的时间和强度也可以恢复量子回声。这些结果可用于表征和减轻猫量子比特的误差。

Abstract: We study classical and quantum echoes in a Kerr oscillator driven by a
frequency-controlling pulsed perturbation. We consider dynamical response to
the perturbation for a single coherent state and for Schr\"odinger cat states
constructed as both balanced and imbalanced superpositions of two coherent
states. For individual coherent states, we demonstrate that a weak parametric
drive yields a long-lived sequence of classical echoes. Cat states are found to
exhibit distinct quantum echoes that are sensitive to the initial relative
phase and weights of the coherent states in superposition. We examine the
effect of dissipation on quantum echoes and quantum revivals of cat states. We
demonstrate that, even when dissipation suppresses quantum revivals, quantum
echoes can be recovered by properly tuning the timing and strength of the
perturbation. These results may be useful for characterizing and mitigating
errors of cat qubits.

</details>


### [665] [Design and Experimental Realization of Various Protocols for Secure Quantum Computation and Communication](https://arxiv.org/abs/2507.09532)
*Satish Kumar*

Main category: quant-ph

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: A set of new schemes for quantum computation and communication have been
either designed or experimentally realized using optimal quantum resources. A
multi-output quantum teleportation scheme, where a sender (Alice) teleports an
m and m+1-qubit GHZ-like unknown state to a receiver (Bob), has been
demonstrated using two copies of the Bell state instead of a five-qubit cluster
state and implemented on IBM's quantum computer for the m=1 case. Another
scheme, known as quantum broadcasting where a known state is sent to two
spatially separated parties (Bob and Charlie) has also been realized using two
Bell states. It is shown that existing quantum broadcasting schemes can be
reduced to multiparty remote state preparation. After achieving teleportation
of unknown and known states, sending a quantum operator becomes the next step.
A scheme for remote implementation of operators (RIO), specifically a
controlled joint-RIO (CJRIO), has been proposed using a four-qubit
hyper-entangled state involving spatial and polarization degrees of freedom. In
this direction, two more variants, remote implementation of hidden and
partially unknown operators (RIHO and RIPUO) have also been proposed. Their
success probabilities are analyzed considering dissipation of an auxiliary
coherent state interacting with the environment. For secure multiparty tasks
like quantum voting or auction, secure multiparty quantum computation (SMQC)
becomes essential. A quantum anonymous voting (QAV) scheme has been
experimentally implemented on IBM's quantum computer. Finally, two quantum key
distribution (QKD) protocols, coherent one-way (COW) and differential phase
shift (DPS), are experimentally demonstrated and the key rates are analyzed as
functions of post-processing parameters and detector dead times across various
distances.

</details>


### [666] [Quantum measurement of work in mesoscopic systems](https://arxiv.org/abs/2507.09977)
*Anant Vijay Varma,Doron Cohen*

Main category: quant-ph

TL;DR: 量子力学对介观系统中的功测量提出了与经典反作用和量子不确定性相关的基本限制。


<details>
  <summary>Details</summary>
Motivation: 探讨量子力学如何影响介观尺度下功的测量，并识别相关的限制因素。

Method: 通过分析量子力学对驱动系统的量子实体（例如振荡器）的影响来研究热力学中的功的测量。

Result: 发现除了经典的反作用限制外，还需要考虑量子不确定性限制，以解决量子干涉的痕迹，且这种量子限制是根本性的，无法通过超分辨率技术放宽。

Conclusion: 量子力学对测量介观系统中的功具有重要影响，并且存在与经典反作用和量子不确定性相关的基本限制。

Abstract: Heat and work in thermodynamics refer to the measurement of changes in energy
content of external bodies (baths and agents). We discuss the implications of
quantum mechanics on the possibility to measure work in a mesoscopic context.
The agent is a quantum entity (say an oscillator) that is used to drive the
system. An obvious limitation is related to back-reaction, leading to a
classical-like restriction. We find that in order to resolve fingerprints of
interference an additional quantum uncertainty limitation should be taken into
account in the design of the agent. The quantum limitation is fundamental, and
cannot be relaxed by super-resolution techniques.

</details>


### [667] [Ensemble-IR: Concise Representation for Quantum Ensemble Programs](https://arxiv.org/abs/2507.09581)
*Sourish Wawdhane,Sashwat Anagolum,Poulami Das,Yunong Shi*

Main category: quant-ph

TL;DR: Ensemble-IR is a new intermediate representation that concisely expresses families of quantum circuits using symbolic variations, reducing the need for explicit enumeration and enabling efficient runtime recreation of circuits for scalable quantum system development.


<details>
  <summary>Details</summary>
Motivation: Existing intermediate representations and frameworks (Qiskit, QIR, MitiQ, OpenQASM) lack primitives to concisely express large families of related quantum circuits required for emerging applications like error mitigation, system characterization, and hybrid protocols, forcing reliance on explicit enumeration.

Method: Ensemble-IR, a novel intermediate representation, encodes entire families of quantum circuits through a unified program that specifies common structures and symbolic operations for variations (gate types, parameters, qubit configurations, etc.), rather than explicit enumeration.

Result: Demonstrated the widespread utility of Ensemble-IR across 18 real-world workloads from prior literature, highlighting its ability to represent ensemble workloads concisely and enable on-the-fly recreation of circuits at runtime.

Conclusion: Ensemble-IR enables concise expression of ensemble quantum workloads, facilitating scalable quantum system development by encoding circuit families in a unified program with symbolic variations for on-the-fly recreation at runtime.

Abstract: Emerging quantum applications such as error mitigation, system
characterization, and hybrid protocols often require running large families of
related quantum circuits. Existing intermediate representations (IRs) and
frameworks such as Qiskit, QIR, MitiQ, and OpenQASM do not provide primitives
to concisely express such workloads. These tools instead rely on explicit
enumeration of the unique circuits within each workload. We introduce
Ensemble-IR, an intermediate representation designed to concisely express these
ensemble workloads. Rather than enumerating each circuit separately,
Ensemble-IR encodes an entire workload through a rich, shared unified program.
This program specifies common circuit structure along with symbolic operations
to express points of variation - such as gate types, gate placement, parameter
values, or qubit configurations. Ensemble-IR enables quantum systems to load an
entire family of circuits onto a device as a single concise file, allowing
contained circuits to instead be recreated on-the-fly at runtime. We
demonstrate Ensemble-IR across 18 real-world workloads from prior literature,
highlighting its widespread utility for scalable quantum system development.

</details>


### [668] [Barnett effect boosted nonreciprocal entanglement and EPR-steering in magnomechanics in the presence of coherent feedback loop](https://arxiv.org/abs/2507.09590)
*Noura Chabar,Mohamed Amazioug*

Main category: quant-ph

TL;DR: 通过将YIG球体与二氧化硅球体集成，并利用磁场和分束器反射率来控制相互作用，以增强纠缠和实现不对称量子关联。


<details>
  <summary>Details</summary>
Motivation: 为了增强纠缠、实现不对称EPR त्यांचे和创建非互易量子关联。

Method: 通过调整巴内特效应和分束器反射率来增强纠缠、实现不对称EPR त्यांचे和创建非互易量子关联。

Result: 成功展示了可控的不对称EPR त्यांचे和直接或间接耦合模式之间的非互易纠缠。此外，还演示了通过调整分束器反射率来增强稳态量子 त्यांचे和纠缠以抵抗热噪声，并能生成多方纠缠以及单向和双向 त्यांचे。

Conclusion: 该提议的系统具有实验可行性，并为各种量子信息应用带来了巨大希望。

Abstract: We propose an experimental scheme for enhancing entanglement, achieving
asymmetric Einstein-Podolsky-Rosen (EPR) steering, and creating nonreciprocal
quantum correlations within a hybrid system. This system integrates a yttrium
iron garnet (YIG) sphere, which exhibits magnon-phonon coupling via
magnetostriction, with a silica sphere featuring optomechanical
whispering-gallery modes. By tuning the Barnett effect through the magnetic
field direction, our system enables controllable asymmetric EPR steering and
nonreciprocal entanglement between both directly and indirectly coupled modes.
We demonstrate that adjusting the reflectivity of a beam splitter can boost
stationary quantum steering and entanglement, effectively countering thermal
noise. This approach allows for the generation of multipartite entanglement and
both one-way and two-way steering. The proposed system is experimentally
feasible and holds significant promise for various quantum information
applications.

</details>


### [669] [Thermal rectification in a qubit-resonator system](https://arxiv.org/abs/2507.10282)
*Luca Magazzù,Elisabetta Paladino,Jukka P. Pekola,Milena Grifoni*

Main category: quant-ph

TL;DR: 研究了量子比特-振荡器系统中的热阀和二极管效应，发现耦合强度和稳态相干性对整流效应有显著影响。


<details>
  <summary>Details</summary>
Motivation: 研究一个由量子比特-振荡器连接两个不同温度的 तिच्या耳浴组成的结，该结能展示热阀和二极管效应，特别是整流效应的大小和符号会发生改变，这表明热流的方向会随着温度偏差的改变而反转。

Method: 本研究在电路量子电动力学模型中，对量子比特-振荡器系统进行了系统的研究，该系统包含两个不同温度的 वेगवेग但又相连的系统，用来展示热阀和二极管效应。

Result: 研究发现，在强耦合区，结变成了一个独特的、高度杂化的系统，电流几乎不随失谐而变化，整流效应也发生了符号改变。

Conclusion: 研究发现，在非线性输运区，耦合强度决定了电流随温度偏差是亚线性还是超线性增长，以及整流效应是正还是负，并且整流效应随偏差增大而增强。此外，稳态相干性在很大程度上抑制了电流但增强了整流效应。通过近似公式可以帮助理解这些行为。

Abstract: A qubit-oscillator junction connecting as a series two bosonic heat baths at
different temperatures can display heat valve and diode effects. In particular,
the rectification can change in magnitude and even in sign, implying an
inversion of the preferential direction for the heat current with respect to
the temperature bias. We perform a systematic study of these effects in a
circuit QED model of qubit-oscillator system and find that the features of
current and rectification crucially depend on the qubit-oscillator coupling.
While at small coupling, transport occurs via a resonant mechanism between the
sub-systems, in the ultrastrong coupling regime the junction is a unique,
highly hybridized system and the current becomes largely insensitive to the
detuning. Correspondingly, the rectification undergoes a change of sign. In the
nonlinear transport regime, the coupling strength determines whether the
current scales sub- or super-linearly with the temperature bias and whether the
rectification, which increases in magnitude with the bias, is positive or
negative. We also find that steady-state coherence largely suppresses the
current and enhances rectification. An insight on these behaviors with respect
to changes in the system parameters is provided by analytical approximate
formulas.

</details>


### [670] [Exploiting emergent symmetries in disorder-averaged quantum dynamics](https://arxiv.org/abs/2507.09614)
*Mirco Erpelding,Adrian Braemer,Martin Gärttner*

Main category: quant-ph

TL;DR: 对称性可用于提高量子模拟的效率。我们开发了一种利用短期和弱无序展开来构建无序平均动力学图的对称扇区的方法。该方法已应用于具有随机相互作用的伊辛模型，并已成功模拟大系统。


<details>
  <summary>Details</summary>
Motivation: 对称性是理解量子系统的关键工具，但无序系统通常具有降低的对称性，并且需要对许多实现进行平均，这使得它们的数值研究在计算上要求很高。

Method: 开发了使用短期和弱无序展开来有效地构建无序平均动力学图的对称扇区的方案。

Result: 通过将平均程序应用于时间演化算子，在超级算子层面恢复对称性，从而模拟大系统。

Conclusion: 该方法通过利用对称性来模拟无序系统，并针对易于理解的伊辛模型进行了基准测试。

Abstract: Symmetries are a key tool in understanding quantum systems, and, among many
other things, can be exploited to increase the efficiency of numerical
simulations of quantum dynamics. Disordered systems usually feature reduced
symmetries and additionally require averaging over many realizations, making
their numerical study computationally demanding. However, when studying
quantities linear in the time-evolved state, i.e. expectation values of
observables, one can apply the averaging procedure to the time evolution
operator, resulting in an effective dynamical map, which restores symmetry at
the level of super operators. In this work, we develop schemes for efficiently
constructing symmetric sectors of the disorder-averaged dynamical map using
short-time and weak-disorder expansions. To benchmark the method, we apply it
to an Ising model with random all-to-all interactions in the presence of a
transverse field. After disorder averaging, this system becomes effectively
permutation-invariant, and thus the size of the symmetric subspace scales
polynomially in the number of spins allowing for the simulation of large
system.

</details>


### [671] [Dynamics of quantum Fisher and Wigner-Yanase skew information following a noisy quench](https://arxiv.org/abs/2507.09659)
*J. Naji,R. Jafari,Alireza Akbari,M. Abdi*

Main category: quant-ph

TL;DR: 噪音会影响量子系统动力学。


<details>
  <summary>Details</summary>
Motivation: 研究噪声对穿越量子临界点的横向场伊辛模型动力学的影响，并量化相干性。

Method: 研究了在穿越量子临界点时的噪声对横向场伊辛模型动力学的影响，并使用量子费舍尔信息（QFI）和维格纳-亚纳斯斜率信息（WYSI）作为量子相干性的度量。

Result: 在没有噪声的情况下，QFI和WYSI随斜坡淬灭时间的增加而单调增加。在存在噪声的情况下，QFI和WYSI随时间呈指数衰减，衰减率取决于噪声强度。拉伸时间的最大值与噪声方差成线性关系。

Conclusion: 噪音会改变相干性的动力学，并且最大拉伸时间与噪声方差成线性关系，与最小化有缺陷量子退火中的缺陷数所需的最佳退火时间具有相同的指数。

Abstract: We study the influence of noise on the dynamics of a transverse field Ising
model when quenched across a quantum critical point. To quantify two-spin
correlations properties, we employ the quantum Fisher information (QFI) and
Wigner-Yanase skew information (WYSI) as measures of quantum coherence. In the
absence of noise, despite the entanglement, both QFI and WYSI increase
monotonically with the ramp quench time, approaching their adiabatic limits
without exhibiting any Kibble-Zurek type scaling with quench duration. When
noise is added to the quench protocol, the coherence dynamics change
dramatically: QFI and WYSI both decay exponentially with time scale of a ramp
quench, with the exponent depending on the noise intensity. Furthermore, the
maximum ramp time, at which either of these measures reach their maximum,
scales linearly with the noise variance, featuring the same exponent that
determines the optimal annealing time for minimizing defect production in noisy
quantum annealing.

</details>


### [672] [Quantum Convolution for Structure-Based Virtual Screening](https://arxiv.org/abs/2507.09667)
*Pei-Kun Yang*

Main category: quant-ph

TL;DR: 使用量子卷积神经网络（QCNN）可以高效地估计delta G_bind，并有望加速药物发现。


<details>
  <summary>Details</summary>
Motivation: 传统的基于结构的虚拟筛选（SBVS）计算成本高昂，作者希望通过量子计算来提高效率。

Method: 提出使用量子卷积神经网络（QCNN）框架来估计delta G_bind。

Result: 在PDBbind v2020数据集上训练的QCNN模型在测试集上达到了0.694的皮尔逊相关系数，并且在引入量子噪声后，皮尔逊相关系数保持稳定。

Conclusion: 量子计算有潜力加速药物发现，并且QCNN对噪声具有鲁棒性，适用于高通量虚拟筛选。

Abstract: Structure-based virtual screening (SBVS) is a key computational strategy for
identifying potential drug candidates by estimating the binding free energies
(delta G_bind) of protein-ligand complexes. The immense size of chemical
libraries, combined with the need to account for protein and ligand
conformations as well as ligand translations and rotations, makes these tasks
computationally intensive on classical hardware. This study proposes a quantum
convolutional neural network (QCNN) framework to estimate delta G_bind
efficiently. Using the PDBbind v2020 dataset, we trained QCNN models with 9 and
12 qubits, with the core set designated as the test set. The best-performing
model achieved a Pearson correlation coefficient of 0.694 on the test set. To
assess robustness, we introduced quantum noise under two configurations. While
noise increased the root mean square deviation, the Pearson correlation
coefficient remained largely stable. These results demonstrate the feasibility
and noise tolerance of QCNNs for high-throughput virtual screening and
highlight the potential of quantum computing to accelerate drug discovery.

</details>


### [673] [Using a Kerr interaction for GKP magic state preparation](https://arxiv.org/abs/2507.09684)
*Jérémie Boudreault,Ross Shillito,Jean-Baptiste Bertrand,Baptiste Royer*

Main category: quant-ph

TL;DR: 提出了一种利用克尔相互作用制备GKP码逻辑门 $\sqrt{H}_L$ 的新方法，可提高保真度和鲁棒性，并给出了电路QED实现方案。


<details>
  <summary>Details</summary>
Motivation: 在基于GKP码的量子计算架构中，非经典态（magic states）的制备和注入是实现容错量子计算的关键，特别是对于难以实现非经典门的GKP码。本研究旨在解决GKP码的magic state制备问题。

Method: 研究了一种利用克尔相互作用实现平方GKP码逻辑门 $\sqrt{H}_L$ 的非高斯幺正操作，并提出了一种结合SBS纠错协议和后选择的保真度提升方案，以及一个电路QED的实现。

Result: 成功设计了一种无需辅助量子比特且满足有限能量约束的逻辑门 $\sqrt{H}_L$ 制备方案，并提出了一种可提升保真度和鲁棒性的纠错与后选择方法，以及具体的电路QED实现方式。

Conclusion: 该研究提出了一种基于克尔相互作用的非高斯幺正操作，用于制备平方GKP码的逻辑门 $\sqrt{H}_L$，无需辅助量子比特且满足有限能量约束。结合SBS纠错协议和后选择，可以提高保真度并抵抗单光子损耗。最后，提出了一个电路QED的实现方案。

Abstract: Magic state distillation and injection is a promising strategy towards
universal fault tolerant quantum computation, especially in architectures based
on the bosonic Gottesman-Kitaev-Preskill (GKP) codes where non-Clifford gates
remain challenging to implement. Here we address GKP magic state preparation by
studying a non-Gaussian unitary mediated by a Kerr interaction which realizes a
logical gate $\sqrt{H}_L$ for square GKP codes. This gate does not directly
involve an auxiliary qubit and is compatible with finite energy constraints on
the code. Fidelity can be further enhanced using the small-Big-small (SBS)
error correction protocol and post-selection, making the scheme robust against
a single photon loss event. We finally propose a circuit QED implementation to
operate the Kerr interaction.

</details>


### [674] [Quantum Singular Value Transformation for Solving the Time-Dependent Maxwell's Equations](https://arxiv.org/abs/2507.09686)
*Gal G. Shaviner,Ziv Chen,Steven H. Frankel*

Main category: quant-ph

TL;DR: 基于QSVT的量子算法能高保真地求解线性方程组（如麦克斯韦方程组），但在实际硬件上因量子态访问限制而面临挑战，需要开发测量效率更高的算法。


<details>
  <summary>Details</summary>
Motivation: 为了应对求解线性方程组的挑战，特别是涉及复杂方程（如麦克斯韦方程组）和需要高精度解的场景，研究人员探索了基于QSVT的量子算法。

Method: 提出了一种基于量子奇异值变换（QSVT）的量子算法，用于求解形式为 $\mathbf{A}{\frac{\partial f}{\partial x}} = \mathbf{B}\mathbf{f}$ 的线性方程组。该算法利用A的块编码，并对函数 $f(x) = 1/x$ 的逆函数采用21次多项式近似。通过在9个量子比特（包括2个辅助量子比特）上实现较浅的量子电路，并使用Adagrad梯度下降方法对QSVT电路的相位角进行100次经典优化，以最小化求解成本。

Result: 在模拟的1D麦克斯韦方程组案例中，量子计算的解与经典解相比，保真度超过99.9%。该方法在能够完全访问量子态的模拟器上表现良好。

Conclusion: QSVT在求解线性方程组方面显示出巨大潜力，尤其是在模拟1D麦克斯韦方程组的基准案例中实现了超过99.9%的高保真度。然而，实际硬件实现面临挑战，因为无法完全访问量子态，这限制了其在只需要多项式数量可观测值的情况下的应用。未来的研究需要开发更高效的测量算法以适应近期量子设备。

Abstract: This work presents a quantum algorithm for solving linear systems of
equations of the form $\mathbf{A}{\frac{\mathbf{\partial f}}{\mathbf{\partial
x}}} = \mathbf{B}\mathbf{f}$, based on the Quantum Singular Value
Transformation (QSVT). The algorithm uses block-encoding of $A$ and applies an
21st-degree polynomial approximation to the inverse function $f(x) = 1/x$,
enabling relatively shallow quantum circuits implemented on 9 qubits, including
two ancilla qubits, corresponding to a grid size of 128 points. Phase angles
for the QSVT circuit were optimized classically using the Adagrad
gradient-based method over 100 iterations to minimize the solution cost. This
approach was simulated in PennyLane and applied to solve a 1D benchmark case of
Maxwell's equations in free space, with a Gaussian pulse as the initial
condition, where the quantum-computed solution showed high fidelity of more
than 99.9% when compared to the normalized classical solution. Results
demonstrate the potential of QSVT-based linear solvers on simulators with full
quantum state access. However, practical hardware implementations face
challenges because accessing the complete quantum state is infeasible. This
limitation restricts applicability to cases where only $O({poly}(n))$
observables are needed. These findings highlight both the promise and current
limitations of using quantum algorithms, such as QSVT, to solve linear systems
of equations, and they point to the need for the development of
measurement-efficient algorithms for near-term quantum devices.

</details>


### [675] [Small Quantum Low Parity Density Check Codes for Near-Term Experiments](https://arxiv.org/abs/2507.09690)
*Christian Kraglund Andersen,Eliška Greplová*

Main category: quant-ph

TL;DR: 研究提出了一种简化的量子LDPC码构造方法，其效率是表面码的两倍，实验实现更简单，并提供了超导和半导体量子比特的实现方案。


<details>
  <summary>Details</summary>
Motivation: 为了实现大规模容错量子计算，量子纠错至关重要。尽管现有方法如表面码在实验上取得进展，但理论上量子LDPC码具有更低的开销，然而其非局域性校验增加了实现难度。本研究旨在克服这些挑战，提出一种简化的量子LDPC码构造方法。

Method: 提出了一种基于量子低密度奇偶校验码（LDPC）的简易构造方法，用于构建小型量子纠错码。

Result: 所提出的量子LDPC码比表面码效率高一倍，且仅需权重为四的奇偶校验，并给出了超导量子比特和半导体自旋量子比特的具体实现建议。

Conclusion: 该工作提出了基于量子低密度奇偶校验码（LDPC）的新型量子纠错码，其效率是表面码的两倍，同时所需的奇偶校验权重仅为四，这大大简化了实验实现。此外，还为超导量子比特和半导体自旋量子比特提供了具体的实现方案。

Abstract: It is widely accepted that quantum error correction is essential for
realizing large-scale fault-tolerant quantum computing. Recent experiments have
demonstrated error correction codes operating below threshold, primarily using
local planar codes such as the surface code and color code. In parallel,
theoretical advances in quantum low-density parity-check (LDPC) codes promise
significantly lower overheads, albeit at the cost of requiring non-local parity
checks. While these results are encouraging, implementing such codes remains
challenging for near-term experiments, creating obstacles to holistic
benchmarking of hardware architectures capable of supporting long-range
couplers. In this work, we present a simple construction recipe for small
quantum LDPC codes based on recent developments in the field. Our codes are
approximately twice as efficient as comparable surface codes, yet require only
weight-four parity checks, which simplifies experimental realization compared
to other quantum LDPC codes. We provide concrete proposals for implementations
with superconducting qubits in flip-chip architectures and with semiconductor
spin qubits using shuttling-based approaches.

</details>


### [676] [Exceptional sensitivity near the bistable transition point of a hybrid quantum system](https://arxiv.org/abs/2507.09691)
*Hanfeng Wang,Kurt Jacobs,Donald Fahey,Yong Hu,Dirk R. Englund,Matthew E. Trusheim*

Main category: quant-ph

TL;DR: 本研究利用非线性克服了厄尔尼积分点放大量子噪声的问题，通过耦合金刚石量子传感器和范德波尔振荡器，在双稳态转变点附近实现了 17 倍的信噪比增强和 170 fT/ પ્લાHz 的灵敏度。


<details>
  <summary>Details</summary>
Motivation: 厄尔尼积分点（EP）虽然可以提高传感器对外部扰动的响应，但会放大噪声，从而抵消信噪比（SNR）的提高。本研究旨在利用非线性来克服这一限制，在双稳态转变点（BP）附近实现卓越的信噪比。

Method: 将先进的金刚石量子传感器与非线性范德波尔振荡器耦合，形成了一个展示单值和双稳态相的自振荡混合系统。该系统的两个相的边界由绝热和确定性的非绝热跃迁标志，这使得在双稳态转变点（BP）实现手性态切换和态的合并。

Result: 本研究成功地利用非线性克服了厄尔尼积分点放大量子噪声的限制。通过将金刚石量子传感器与非线性范德波尔振荡器耦合，研究人员在双稳态转变点附近实现了卓越的信噪比。具体来说，NV 磁力测量在接近双稳态转变点时表现出 17 倍的信噪比增强，达到了 170 fT/ પ્લાHz 的创纪录灵敏度，超越了理想电子磁力计的灵敏度极限。

Conclusion:  NV 磁力测量在接近双稳态转变点时表现出 17 倍的信噪比增强，达到了 170 fT/ પ્લાHz 的创纪录灵敏度。该结果超越了理想的、热限制的电子磁力计的灵敏度极限，并解决了长期以来关于高级量子传感中类厄尔尼积分点物理学的争论。

Abstract: Phase transitions can dramatically alter system dynamics, unlocking new
behavior and improving performance. Exceptional points (EPs), where the
eigenvalues and corresponding eigenvectors of a coupled linear system coalesce,
are particularly relevant for sensing applications as they can increase sensor
response to external perturbations to a range of phenomena from optical phase
shifts to gravitational waves. However, the coalescence of eigenstates at
linear EPs amplifies noise, negating the signal-to-noise ratio (SNR)
enhancement. Here, we overcome this limitation using nonlinearity, which
exhibits exceptional SNR around a bistable transition point (BP). We couple a
state-of-the-art diamond quantum sensor to a nonlinear Van der Pol oscillator,
forming a self-oscillating hybrid system that exhibits both a single-valued and
bistable phase. The boundaries between these phases are marked by both
adiabatic and deterministic non-adiabatic transitions that enable chiral state
switching and state coalescence at the BP. Crucially, NV magnetometry performed
near the BP exhibits a 17x enhancement in SNR, achieving a record sensitivity
of 170 fT/\sqrt{Hz}. This result surpasses the sensitivity limit of an ideal,
thermally-limited electron magnetometer and resolves a long-standing debate
regarding EP-like physics in advanced quantum sensing.

</details>


### [677] [Hybrid Quantum-Classical Generative Adversarial Networks with Transfer Learning](https://arxiv.org/abs/2507.09706)
*Asma Al-Othni,Saif Al-Kuwari,Mohammad Mahdi Nasiri Fatmehsari,Kamila Zaman,Ebrahim Ardeshir Larijani*

Main category: quant-ph

TL;DR: 通过在生成器和判别器中都加入变分量子电路，量子生成对抗网络比纯经典网络生成更高质量的图像，并能更好地处理数据稀疏性问题。


<details>
  <summary>Details</summary>
Motivation: 探讨了量子原理如何更好地增强生成对抗网络（GANs）的表示和计算能力，因为GANs在合成多样化和高保真图像方面显示出巨大潜力，但仍存在关键问题。

Method: 研究了混合量子-经典生成对抗网络架构，并辅以迁移学习，系统地检验了将变分量子电路（VQCs）引入生成器、判别器或两者中是否能提升性能。

Result: 混合模型在两个方面都表现更好：1. 视觉质量更高，量化指标更优。2. 变分量子电路在生成器中可以加速早期特征学习，而判别器中的变分量子电路虽然收敛较慢，但能产生更精细的合成输出。此外，即使在数据集大小大大减小时，模型也能保持接近的性能，表明迁移学习和量子增强可以缓解数据稀疏性问题。

Conclusion: 将量子计算与经典对抗性训练以及预训练特征提取相结合，可以显著丰富生成对抗网络的图像合成能力。这为更高分辨率的任务、替代量子电路设计以及新兴量子硬件的实验提供了未来工作的方向。

Abstract: Generative Adversarial Networks (GANs) have demonstrated immense potential in
synthesizing diverse and high-fidelity images. However, critical questions
remain unanswered regarding how quantum principles might best enhance their
representational and computational capacity. In this paper, we investigate
hybrid quantum-classical GAN architectures supplemented by transfer learning to
systematically examine whether incorporating Variational Quantum Circuits
(VQCs) into the generator, the discriminator, or both improves performance over
a fully classical baseline. Our findings indicate that fully hybrid models,
which incorporate VQCs in both the generator and the discriminator,
consistently produce images of higher visual quality and achieve more favorable
quantitative metrics compared to their fully classical counterparts. In
particular, VQCs in the generator accelerate early feature learning, whereas
those in the discriminator, despite exhibiting slower initial convergence,
ultimately facilitate more refined synthetic outputs. Moreover, the model
sustains near-comparable performance even when the dataset size is drastically
reduced, suggesting that transfer learning and quantum enhancements mitigate
the problem of data scarcity. Overall, the results underscore that carefully
integrating quantum computing with classical adversarial training and
pretrained feature extraction can considerably enrich GAN-based image
synthesis. These insights open avenues for future work on higher-resolution
tasks, alternative quantum circuit designs, and experimentation with emerging
quantum hardware.

</details>


### [678] [Intrinsic Multi-Mode Interference for Passive Suppression of Purcell Decay in Superconducting Circuits](https://arxiv.org/abs/2507.09715)
*Mustafa Bakr,Mohammed Alghadeer,Simon Pettersson Fors,Simone D. Fasciati,Shuxiang Cao,Atharv Mahajan,Smain Amari,Anton Frisk Kockum,Peter Leek*

Main category: quant-ph

TL;DR: 该研究提出了一种利用超导电路的固有结构来抑制辐射衰减的新方法，通过引入不对称性来激活破坏性干涉，从而提高了 Transmon که的相干时间。


<details>
  <summary>Details</summary>
Motivation: 超导量子处理器的扩展受到辐射衰减引起的退相干效应的阻碍。

Method: 提出了一种利用超导电路固有的多模结构环境，通过基于干涉的被动方法来抑制辐射衰减。该方法考虑了器件内部完整的电磁模式-模式耦合，并推导了解析条件以实现破坏性干涉。

Result: 实现了对称性破缺的 Transmon که，相干时间得到改善。

Conclusion: 通过引入受控的几何不对称性（例如，对 Transmon 电容器进行局部扰动）可以实现破坏性干涉，这会增加模式杂化并激活多条衰减路径之间的干涉。我们通过微扰理论、全波电磁模拟和实验测量结果验证了该方法，证明了对称性破缺的 Transmon که的相干时间有所提高。

Abstract: Decoherence due to radiative decay remains an important consideration in
scaling superconducting quantum processors. We introduce a passive,
interference-based methodology for suppressing radiative decay using only the
intrinsic multi-mode structured environment of superconducting circuits. By
taking into account the full electromagnetic mode-mode couplings within the
device, we derive analytic conditions that enable destructive interference.
These conditions are realized by introducing controlled geometric asymmetries
-- such as localized perturbations to the transmon capacitor -- which increase
mode hybridization and activate interference between multiple decay pathways.
We validate this methodology using perturbation theory, full-wave
electromagnetic simulations, and experimental measurements of a symmetry-broken
transmon qubit with improved coherence times.

</details>


### [679] [When the Weak Becomes Strong: Effective Observables via Time-Symmetric Quantum Selection](https://arxiv.org/abs/2507.09716)
*Mirco A. Mannucci*

Main category: quant-ph

TL;DR: Sequential weak measurements in time-symmetric quantum mechanics are shown to be equivalent to a single strong measurement of a specially constructed observable, which reveals interference effects and has practical applications in quantum information.


<details>
  <summary>Details</summary>
Motivation: To investigate the sequential composition of weak values and their connection to strong observables within the framework of time-symmetric quantum מיmechanics.

Method: Investigated the sequential composition of weak values in time-symmetric quantum mechanics, considering forward and reverse weak measurements. Analyzed the structure of the strong, state-conditioned observable B = A P_psi A, where P_psi is the projector onto the preselected state.

Result: Showed that the product of weak values corresponds to a normalized expectation value of a strong observable B. Demonstrated that B encodes interference information and extended the formulation to mixed states. Illustrated applications in quantum information, including state-specific error witnessing and inferring weak value phase.

Conclusion: The product of forward and reverse weak values corresponds to the normalized expectation value of a strong, state-conditioned observable B. The structure of B encodes interference information, especially when the preselected state is a superposition. This formulation extends to mixed states and has applications in quantum information, such as state-specific error witnessing and inferring weak value phase via strong measurements.

Abstract: We investigate the sequential composition of weak values in the framework of
time-symmetric quantum mechanics. Specifically, we consider a forward'' weak
measurement from a preselected state $\ket{\psi}$ to a post-selected state
$\ket{\phi}$, followed by a reverse'' weak measurement. We show that the
product of these two weak values corresponds to the normalized expectation
value of a strong, state-conditioned observable $B = A P_\psi A$, where $P_\psi
= \ket{\psi}\bra{\psi}$ is the projector onto the preselected state. Analyzing
the structure of $B$, we demonstrate how it encodes interference information,
particularly when $\ket{\psi}$ is a superposition rather than an eigenstate of
$A$. This formulation extends naturally to mixed states by replacing $P_\psi$
with a generic density matrix $\rho$, linking the construction to the formalism
of generalized quantum measurements. We illustrate practical applications in
quantum information, including state-specific error witnessing in quantum
computing, and show how the phase of a weak value can be inferred via strong
measurements in the pure-state case.

</details>


### [680] [Power Consumption Analysis of QKD Networks under Different Protocols and Detector Configurations](https://arxiv.org/abs/2507.09719)
*Jiaheng Xiong,Qiaolun Zhang,Yoann Piétri,Raja Yehia,Raouf Boutaba,Francesco Musumeci,Massimo Tornatore*

Main category: quant-ph

TL;DR: Analyzed QKD network power consumption with different protocols and detectors, optimizing placement and quantifying trade-offs.


<details>
  <summary>Details</summary>
Motivation: Analyze the power consumption of quantum key distribution (QKD) networks.

Method: We evaluate discrete-variable vs continuous-variable QKD and optimize device placement, quantifying power trade-offs of SNSPD vs APD detectors and the benefits of optical bypass.

Result: Quantified power trade-offs of SNSPD vs APD detectors and the benefits of optical bypass.

Conclusion: We analyze the power consumption of QKD networks under various configurations.

Abstract: We analyze the power consumption of quantum key distribution (QKD) networks
under various protocol and detector configurations. Using realistic network
topologies, we evaluate discrete-variable vs continuous-variable QKD and
optimize device placement, quantifying power trade-offs of SNSPD vs APD
detectors and the benefits of optical bypass.

</details>


### [681] [Response to "Are Hilbert Spaces Unphysical? Hardly, My Dear!''](https://arxiv.org/abs/2507.09738)
*Gabriele Carcassi,Robert Rozite,Christine A. Aidala*

Main category: quant-ph

TL;DR: 论文回应了关于希尔伯特空间中坐标变换和期望值不变性的批评，澄清了两者的区别并反驳了批评者的论点。


<details>
  <summary>Details</summary>
Motivation: 回应Nivaldo Lemos对我们论文“希尔伯特空间的非物理性”提出的批评，Lemos声称通过坐标变换将具有有限期望值的态映射到具有无限期望值的态的论点是不成立的。

Method: 通过阐明坐标变换和基变换的区别来反驳Lemos的论点。

Result: 澄清坐标变换和基变换的区别，并反驳Lemos的主要论点。

Conclusion: 该论文旨在澄清坐标变换与量子力学中的基变换之间的区别，并反驳Lemos关于期望值在变量变换下不变的主要论点。

Abstract: A recent criticism of our paper ``The unphysicality of Hilbert spaces'' by
Nivaldo Lemos refutes our central argument that a state with finite expectation
value can be mapped to a state with infinite expectation value by a coordinate
transformation. By conflating coordinate transformation with change of basis in
quantum mechanics, Lemos argues that expectation values are invariant under
change of variables. In the present work, we clarify the distinction between
coordinate transformation and change of basis, and rebut Lemos' main argument.

</details>


### [682] [Pulse optimization in adiabatic quantum computation and control](https://arxiv.org/abs/2507.09770)
*Daniel Turyansky,Yehonatan Zolti,Yuval Cohen,Adi Pick*

Main category: quant-ph

TL;DR: 提出了一种通过优化脉冲来加速量子计算和布居转移的方法，该方法效率高且鲁棒性强。


<details>
  <summary>Details</summary>
Motivation: 旨在加速绝热控制协议，包括绝热布居转移和绝热量子计算。

Method: 该方法通过识别控制脉冲来识别演化量子系统遵循其瞬时基态，并利用先进的无梯度优化工具和分析绝热解来定义量子最优控制（QOC）的成本函数。

Result: 通过在IBM量子平台上使用超导量子比特运行数字化绝热协议，并对使用里德堡原子阵列解决图优化问题的绝热算法进行数值模拟，证明了该方法的通用性。

Conclusion: 提出了一种用于加速绝热控制协议的脉冲优化方法，该方法包括绝热布居转移和绝热量子计算。

Abstract: We present a pulse optimization method for accelerating adiabatic control
protocols, including adiabatic population transfer and adiabatic quantum
computation. Our method relies on identifying control pulses under which the
evolving quantum system adheres to its instantaneous ground state. Our method
is both efficient -- by using advanced gradient-free optimization tools and
robust -- by utilizing analytic adiabatic solutions in defining the cost
function for quantum optimal control (QOC). To demonstrate the generality of
our approach, we run digitized adiabatic protocols with superconducting qubits
on the IBM quantum platform and numerically simulate adiabatic algorithms for
solving graph optimization problems with Rydberg atom arrays.

</details>


### [683] [Decomposition of multi-qutrit gates generated by Weyl-Heisenberg strings](https://arxiv.org/abs/2507.09781)
*Daniele Trisciani,Marco Cattaneo,Zoltán Zimborás*

Main category: quant-ph

TL;DR: 研究提出了一种适用于量子比特系统的量子门分解新方法，可有效减少量子电路深度，并优化门路由。


<details>
  <summary>Details</summary>
Motivation: 由于量子比特系统在量子计算中的应用日益广泛，因此有必要开发相应的量子门分解方法，以提高量子算法的效率和可行性。

Method: 该研究提出了一种将 Weyl-Heisenberg 算符（及其共轭）的指数分解为单量子比特和双量子比特门的新算法，并将其扩展到 Gell-Mann 算符。此外，还通过推广 Steiner-Gauss 方法解决了量子比特系统中的路由挑战。

Result: 研究成功地将 Weyl-Heisenberg 算符和 Gell-Mann 算符的指数分解为单量子比特和双量子比特门，并为量子比特系统提出了优化的路由策略。在量子近似优化算法（QAOA）的实际应用中，该方法显著减少了量子比特电路的深度，尤其是在处理图 k 着色问题时，并且随着问题规模的增大，这种优势越发明显。

Conclusion: 该研究为基于量子比特的设备开发了新的量子门分解方法，特别是针对量子比特系统。它提供了一种将 Weyl-Heisenberg 算符（及其共轭）的指数分解为单量子比特和双量子比特门的方法。此外，该方法还扩展到 Gell-Mann 算符。这些分解方法对于在量子比特系统上实现量子算法至关重要，可以显著减少量子电路深度并优化门路由。

Abstract: Decomposing unitary operations into native gates is an essential step for
implementing quantum algorithms. For qubit-based devices, where native gates
are typically single- and two-qubit operations, a range of decomposition
techniques have been developed. In particular, efficient algorithms exist for
decomposing exponentials of Pauli strings while taking hardware topology in
account. Motivated by the growing interest in qutrit-based quantum computing,
we develop analogous decomposition methods for qutrit systems. Specifically, we
introduce an algorithm that decomposes the exponential of an arbitrary tensor
product of Weyl-Heisenberg operators (plus their Hermitian conjugation) into
single- and two-qutrit gates. We further extend this approach to unitaries
generated by Gell-Mann string (i.e., a tensor product of Gell-Mann matrices).
Since both Gell-Mann matrices and Weyl-Heisenberg operators form (together with
identity) complete operator bases of qutrit operators, we can use this result
also to decompose any multi-qutrit gate that is diagonal up to single-qutrit
rotations. As a practical application, we use our method to decompose the
layers of the quantum approximate optimization algorithm for qutrit-based
implementations of the graph k-coloring problem. For values of $k$ well-suited
to qutrit architectures (e.g., $k=3$ or in general $k=3^n$), our approach
yields significantly shallower circuits compared to qubit-based
implementations, an advantage that grows with problem size, while also
requiring a smaller total Hilbert space dimension. Finally, we also address the
routing challenge in qutrit architectures that arises due to the limited
connectivity of the devices. In particular, we generalize the Steiner-Gauss
method, originally developed to reduce CNOT counts in qubit circuit, to
optimize gate routing in qutrit-based systems.

</details>


### [684] [Quantum Solution Framework for Finite-Horizon LQG Control via Block Encodings and QSVT](https://arxiv.org/abs/2507.09841)
*Nahid Binandeh Dehaghani,Rafal Wisniewski,A. Pedro Aguiar*

Main category: quant-ph

TL;DR: 本研究提出了一种量子LQG算法，利用量子线性代数技术，相比经典算法在处理大规模问题时具有更优的计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 为了解决经典LQG方法在处理大规模系统时计算复杂度随系统维度增长过快的问题，该研究提出了利用量子计算的优势来提供渐进式加速。

Method: 该量子算法通过利用量子线性代数基元（包括块编码矩阵表示和用于矩阵求逆及乘法的量子奇异值变换（QSVT）技术）来重新构建整个LQG流程。研究中对每个算法组件的时间复杂度进行了正式分析。

Result: 在标准矩阵条件数和编码精度的假设下，量子LQG算法的总运行时间与系统维度n呈弱多项式对数增长，与时间视界T呈线性增长，从而在渐进意义上实现了超越经典方法的量子加速。

Conclusion: 该研究提出了一个解决有限时间离散时间线性二次高斯（LQG）控制问题的量子算法，该算法整合了最优控制和状态估计，并处理随机扰动和噪声。

Abstract: We present a quantum algorithm for solving the finite-horizon discrete-time
Linear Quadratic Gaussian (LQG) control problem, which integrates optimal
control and state estimation in the presence of stochastic disturbances and
noise. Classical approaches to LQG require solving a backward Riccati recursion
and a forward Kalman filter, both requiring computationally expensive matrix
operations with overall time complexity $\mathcal{O}(T n^3)$, where $n$ is the
system dimension and $T$ is the time horizon. While efficient classical solvers
exist, especially for small to medium-sized systems, their computational
complexity grows rapidly with system dimension. To address this, we reformulate
the full LQG pipeline using quantum linear algebra primitives, including
block-encoded matrix representations and quantum singular value transformation
(QSVT) techniques for matrix inversion and multiplication. We formally analyze
the time complexity of each algorithmic component. Under standard assumptions
on matrix condition numbers and encoding precision, the total runtime of the
quantum LQG algorithm scales polylogarithmically with the system dimension $n$
and linearly with the time horizon $T$, offering an asymptotic quantum speedup
over classical methods.

</details>


### [685] [Robust Entanglement Generation in Bipartite Quantum Systems Using Optimal Control](https://arxiv.org/abs/2507.09844)
*Nahid Binandeh Dehaghani,A. Pedro Aguiar,Rafal Wisniewski*

Main category: quant-ph

TL;DR: 本研究提出了一种量子最优控制框架，利用庞特里亚金最小化原理，通过设计控制场最大化双量子比特纠缠，并通过数值模拟验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 量子纠缠是量子技术中的关键资源，但由于量子动力学的复杂性，其高效、高保真度的生成仍然是一个挑战。

Method: 利用庞特里亚金最小化原理推导出必要条件，设计时间相关的控制场，以最大化双量子纠缠，并使用共度量来量化纠缠，通过求解刘维尔-冯诺依曼动力学进行数值模拟。

Result: 数值模拟结果表明，基于开关的控制策略能够实现鲁棒的量子纠缠，证明了该量子最优控制框架的有效性。

Conclusion: 本研究提出了一种利用量子最优控制框架，在固定时间范围内，在有界控制输入下最大化双量子比特纠缠的方法，并通过数值模拟验证了该方法在实现鲁棒纠缠方面的有效性，为量子网络中纠缠生成提供了实际可行的控制策略。

Abstract: Quantum entanglement is a key resource for quantum technologies, yet its
efficient and high-fidelity generation remains a challenge due to the
complexity of quantum dynamics. This paper presents a quantum optimal control
framework to maximize bipartite entanglement within a fixed time horizon, under
bounded control inputs. By leveraging Pontryagin's Minimum Principle, we derive
a set of necessary conditions that guide the design of time-dependent control
fields to steer a two-qubit system toward maximally entangled Bell states. The
entanglement is quantified using concurrence, and the control objective is
formulated as maximizing this measure at the terminal time. Our approach is
validated through numerical simulations of Liouville-von Neumann dynamics. The
results demonstrate the effectiveness of switching-based control strategies in
achieving robust entanglement, offering insights into practical implementations
of quantum control for entanglement generation in quantum networks.

</details>


### [686] [Generalized Heisenberg Dynamics Revisited](https://arxiv.org/abs/2507.09848)
*Yoshiharu Kawamura*

Main category: quant-ph

TL;DR: 通过类比海森堡的矩阵力学，我们从南布力学出发，构建了一个描述离散变量动力学系统的广义矩阵力学版本，并证实了多重交换子是南布括号或雅可比行列式的离散化形式。


<details>
  <summary>Details</summary>
Motivation: 以海森堡的矩阵力学由哈密顿力学使用对应原理导出为模型，我们探索了一个涉及离散变量的动力学系统类别，以南布力学为起点。

Method: 通过将海森堡的矩阵力学从哈密顿力学和对应原理出发作为模型，我们探索了一个涉及离散变量的动力学系统类别，以南布力学为起点。

Result: 我们重构了一个扩展的矩阵力学版本，它描述了具有广义矩阵表示的物理量的动力学系统，并重申了多重交换子可以作为南布括号或雅可比行列式的离散（量子化）版本。

Conclusion: 从哈密顿力学到矩阵力学，我们探索了一个涉及离散变量的动力学系统类别，以南布力学为起点。我们重建了一个扩展的矩阵力学版本，用于描述具有广义矩阵表示的物理量的动力学系统，并重申了多重交换子可以作为南布括号或雅可比行列式的离散（量子化）版本。

Abstract: Taking as a model the fact that Heisenberg's matrix mechanics was derived
from Hamiltonian mechanics using the correspondence principle, we explore a
class of dynamical systems involving discrete variables, with Nambu mechanics
as the starting point. Specifically, we reconstruct an extended version of
matrix mechanics that describes dynamical systems possessing physical
quantities expressed through generalized matrices. Furthermore, we reconfirm
that a multiple commutator involving generalized matrices can serve as a
discrete (quantized) version of the Nambu bracket or the Jacobian.

</details>


### [687] [Evaluating a Multi-Color Entangled-Photon Source for a Bosonic Silicon Quantum Circuit](https://arxiv.org/abs/2507.09851)
*Koki Nagamachi,Hiroki Yamashita,Mikio Fujiwara,Shigehito Miki,Hirotaka Terai,Takafumi Ono*

Main category: quant-ph

TL;DR: 研究了硅基SFWM光源在玻色子集成电路中的应用前景。


<details>
  <summary>Details</summary>
Motivation: 评估在硅中通过自发四波混频（SFWM）产生的多色双光子纠缠态作为玻色子集成电路的潜在光源。

Method: 利用一对硅波导，通过自发四波混频（SFWM）产生信号光子和闲置光子，并进行了量子态层析成像。

Result: 成功地展示了量子干涉现象，并对玻色子系统进行了量子态层析成像。

Conclusion: 这项研究证明了在硅中通过自发四波混频（SFWM）产生的्तार-对源在玻色子光学电路中是可行的，并强调了它们在硅基光学量子技术中的广泛应用潜力。

Abstract: We evaluated a multi-color two-photon entangled state generated in silicon
via spontaneous four-wave mixing (SFWM) as a potential source for bosonic
integrated circuits. Spatially entangled photon states were created using a
pair of silicon waveguides that produced signal and idler photons through SFWM,
allowing us to observe quantum interference between them. Assuming that the
frequencies of the multi-color photons were nearly identical, we characterized
the generated quantum state by performing quantum state tomography on the
bosonic system using a linear optical circuit. This study demonstrates the
feasibility of using photon-pair sources generated in silicon via SFWM in
bosonic optical circuits and highlights their potential for a wide range of
applications in silicon-based optical quantum technologies.

</details>


### [688] [Error-mitigated inference of quantum network topology](https://arxiv.org/abs/2507.09867)
*Jun-Hao Wei,Xin-Yu Xu,Shu-Ming Hu,Nuo-Ya Yang,Li Li,Nai-Le Liu,Kai Chen*

Main category: quant-ph

TL;DR: 提出了一种可扩展高效的方法，通过局部测量和熵不确定性来揭示量子网络的拓扑信息并量化纠缠，并利用PEC和VD技术提高了鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 量子网络应用的性能高度依赖于分布式纠缠的结构和质量，需要一种揭示未知量子网络拓扑信息并同时量化纠缠的可扩展高效的方法。

Method: 利用熵不确定性进行两次局部测量，并通过整合不确定性和互信息来计数两个节点之间的二分源数量。结合了概率性错误消除（PEC）和虚拟蒸馏（VD）技术来抑制熵数量中的偏差。

Result: PEC成功消除了相关性估计中的偏差。VD将允许有效二分纠缠认证的去极化噪声强度从8.8%提高到26.4%，从而大大提高了在实际情况中抵抗偏差诱导噪声的能力。

Conclusion: 该方案可扩展且高效，适用于多种平台，有助于推动未来利用量子网络的优势。

Abstract: Paramount for performances of quantum network applications are the structure
and quality of distributed entanglement. Here we propose a scalable and
efficient approach to reveal the topological information of unknown quantum
networks, and quantify entanglement simultaneously. The scheme exploits
entropic uncertainty, an operationally meaningful measure of correlation, by
performing only two local measurements on each qubit. Moreover, when
measurement outcomes in each node are collectively evaluated, integrating
uncertainty and mutual information enables a direct count of the number of
bipartite sources between any two nodes. This surpasses what is possible via
applying either approach solely. Moreover, quantum error mitigation techniques
including probabilistic error cancellation (PEC) and virtual distillation (VD),
which have been widely applied to suppress biases in single expectation value,
are further incorporated to mitigate errors in entropic quantities. We find
that PEC successfully removes deviations in correlation estimations. Meanwhile,
VD extends the depolarizing noise strength that allows for valid bipartite
entanglement certification from 8.8% to 26.4%, thus substantially enhancing
robustness against bias-inducing noise in practical situations. The proposal is
applicable to a broad variety of platforms and helps to spur future studies
toward harnessing the advantages of quantum networks.

</details>


### [689] [High-throughput electro-optic upconversion and downconversion with few-photon added noise](https://arxiv.org/abs/2507.09873)
*M. D. Urmey,S. Dickson,K. Adachi,S. Mittal,L. G. Talamo,A. Kyle,N. E. Frattini,S. -X. Lin,K. W. Lehnert,C. A. Regal*

Main category: quant-ph

TL;DR: 一种新型的膜基光力机械换能器，具有低噪声和高传输速率，有望用于构建量子网络。


<details>
  <summary>Details</summary>
Motivation: 需要一种低噪声、高传输速率的微波-光学换能器来实现超导量子处理器之间的纠缠分发。

Method: 通过测量基于膜的光力机械换能器的噪声和信号传输速率来评估其性能。

Result: 测量结果显示，该换能器的效率-带宽-占空比积为7 kHz，输入参考噪声为3个光子，在下转换方面实现了前所未有的性能。

Conclusion: 膜基光力机械换能器在提高其吞吐量和降低噪声方面具有可行性改进的潜力，使其成为构建量子网络的战略选择。

Abstract: A microwave-optical transducer of sufficiently low noise and high signal
transfer rate would allow entanglement to be distributed between
superconducting quantum processors at a rate faster than the lifetimes of the
quantum memories being linked. Here we present measurements of a membrane-based
opto-electromechanical transducer with high signal throughput, as quantified by
an efficiency-bandwidth-duty-cycle product of 7 kHz, approaching
quantum-enabled operation in upconversion as well as downconversion, with
input-referred added noise of 3 photons. In downconversion, throughput of this
magnitude at the few-photon noise level is unprecedented. Using the quantum
channel capacity, we also find an expression for the maximum rate at which
quantum information can be transduced, providing insight into the importance of
improving both a transducer's throughput and noise performance. With feasible
improvements, the high throughput achieved with this device positions
membrane-based transducers as a strategic choice for demonstrations of a
quantum network with reasonable averaging times.

</details>


### [690] [Sequence-Model-Guided Measurement Selection for Quantum State Learning](https://arxiv.org/abs/2507.09891)
*Jiaxin Huang,Yan Zhu,Giulio Chiribella,Ya-Dong Wu*

Main category: quant-ph

TL;DR: 使用深度学习搜索最优量子测量方法。


<details>
  <summary>Details</summary>
Motivation: 解决量子系统尺寸增大导致的最优测量选择优化难题。

Method: 提出了一种具有序列模型架构的深度神经网络，以数据驱动、自适应的方式搜索高效的测量方法。

Result: 该神经网络模型在各种任务中表现优于均匀随机选择，并在拓扑量子系统中发现了边界测量与体性质预测之间的潜在联系。

Conclusion: 该模型在预测线性与非线性性质、量子态聚类和量子态层析成像等任务中，其测量选择优于均匀随机选择，尤其在拓扑量子系统中，模型倾向于推荐边界测量，即使任务是预测体性质，这表明该模型可能独立发现了边界与体性质之间的联系。

Abstract: Characterization of quantum systems from experimental data is a central
problem in quantum science and technology. But which measurements should be
used to gather data in the first place? While optimal measurement choices can
be worked out for small quantum systems, the optimization becomes intractable
as the system size grows large. To address this problem, we introduce a deep
neural network with a sequence model architecture that searches for efficient
measurement choices in a data-driven, adaptive manner. The model can be applied
to a variety of tasks, including the prediction of linear and nonlinear
properties of quantum states, as well as state clustering and state tomography
tasks. In all these tasks, we find that the measurement choices identified by
our neural network consistently outperform the uniformly random choice.
Intriguingly, for topological quantum systems, our model tends to recommend
measurements at the system's boundaries, even when the task is to predict bulk
properties. This behavior suggests that the neural network may have
independently discovered a connection between boundaries and bulk, without
having been provided any built-in knowledge of quantum physics.

</details>


### [691] [The Ontic Necessity of the Quantum Wavefunction: Why Epistemic Views Struggle with the Uncertainty Principle](https://arxiv.org/abs/2507.09944)
*Bachtiar Rifai,Dwi Satya Palupi,Muhammad Farchani Rosyid*

Main category: quant-ph

TL;DR: 量子波函数更可能是真实的物理客体，而不是我们知识的反映，因为前者能更好地解释量子不确定性原理。


<details>
  <summary>Details</summary>
Motivation: 量子波函数本体论地位的争论以及认识论解释在解释不确定性原理方面的困难。

Method: 通过比较本体论和认识论解释来论证。

Result: 本体论方法可以从希尔伯特空间导出量子不确定性原理，而认识论方法则不能。认识论观点在解释不确定性原理方面存在不足。

Conclusion: 本体论解释比认识论解释更能自然地解释量子不确定性原理。

Abstract: The ontological status of the quantum wavefunction remains one of the most
debated questions in quantum theory. While epistemic interpretations regard the
wavefunction as a reflection of our knowledge or beliefs, ontic interpretations
treat it as a real physical object. In this paper, we argue that epistemic
approaches struggle to explain the universality and precision of the
uncertainty principle, a core feature of quantum mechanics. By contrast,
treating the wave-function as ontic allows a consistent and natural derivation
of quantum uncertainty from the mathematical structure of Hilbert space. We
examine key interpretations on both sides and highlight why the epistemic view
falls short in addressing constraints that appear to be intrinsic to nature.

</details>


### [692] [Device-Independent Private Quantum Randomness Beacon](https://arxiv.org/abs/2507.09963)
*Ignatius William Primaatmaja,Hong Jie Ng,Koon Tong Goh*

Main category: quant-ph

TL;DR: 本研究提出了一种名为DIPQRB的新方法，用于从不可信设备生成安全的私有随机数。该方法通过路由贝尔测试放宽了设备要求，使客户端可以使用更经济高效的设备，并确保输出的私密性。


<details>
  <summary>Details</summary>
Motivation: 传统的DIQRNG协议的严格设备要求限制了其在实际应用中的应用。本研究旨在通过引入一种新颖的方法来解决这一限制，该方法可以显著放宽设备要求，从而实现更实用的随机数生成。

Method: 本研究基于路由贝尔测试引入了一种新颖的设备无关私有量子随机信标（DIPQRB）方法，该方法显著放宽了对设备的要求，从而能够从不可信设备生成随机数的更实用的方式。通过将设备要求分布在服务器和客户端网络中，该方案允许服务器运行高性能设备，而客户端可以配备更具成本效益的设备。

Result: DIPQRB的实现能够显著放宽对设备的要求，使其在实际应用中更具成本效益和实用性。此外，该方法确保了即使对服务器而言，客户端设备的输出也是私有的，这对于加密应用至关重要。

Conclusion: DIPQRB提供了一种经济高效的方法，可以从不可信设备生成安全且私有的随机数。

Abstract: Device-independent quantum random number generation (DIQRNG) is the gold
standard for generating truly random numbers, as it can produce certifiably
random numbers from untrusted devices. However, the stringent device
requirements of traditional DIQRNG protocols have limited their practical
applications. Here, we introduce Device-Independent Private Quantum Randomness
Beacon (DIPQRB), a novel approach to generate random numbers from untrusted
devices based on routed Bell tests. This method significantly relaxes the
device requirements, enabling a more practical way of generating randomness
from untrusted devices. By distributing the device requirements across a
network of servers and clients, our proposal allows the server to operate
high-performance devices while the clients can be equipped with more
cost-effective devices. Moreover, the outputs of the client's device are also
private, even against the server, which is essential in cryptographic
applications. Therefore, DIPQRB provides a cost-effective method to generate
secure and private random numbers from untrusted devices.

</details>


### [693] [Has Anything Changed? Tracking Long-Term Interpretational Preferences in Quantum Mechanics](https://arxiv.org/abs/2507.09988)
*Petr O. Jedlička,Šimon Kos,Martin Šmíd,Jiří Vomlel,Jan Slavík*

Main category: quant-ph

TL;DR: 一项针对物理学家的调查显示，尽管量子力学存在多种解释，但哥本哈根解释仍然最受欢迎，这可能归因于其教育上的普及性和实用性。


<details>
  <summary>Details</summary>
Motivation: 为了在量子力学百年纪念之际，通过新的研究揭示研究界对量子力学基础性问题的看法，并与以往的调查结果进行对比。

Method: 通过对更广泛的物理学家群体进行调查，并与以往的调查结果进行比较，来揭示科学家观点可能的变化。

Result: 大多数受访者仍然偏爱哥本哈根解释，这表明在过去几十年中，科学界对量子力学解释的偏好保持了相对稳定。

Conclusion: 尽管量子力学基础仍缺乏共识性的解释，但哥本哈根解释仍然是首选，这可能反映了其在教育中的重要性以及在避免形而上学问题和引入新概念方面的实用性。研究结果表明，在过去几十年中，对量子力学解释的偏好保持相对稳定。

Abstract: As we approach the centennial anniversary of modern quantum mechanics this
paper revisits the foundational debates through a new poll within the research
community. Inspired by the survey by Schlosshauer, Kofler, and Zeilinger at the
specialized 2011 Quantum Physics and the Nature of Reality conference, we
expanded our recruitment to include a more representative sample of the broader
community of physicists with the aim to reveal potential shifts in scientists'
views and compare our findings with those from several previous polls. While
quantum foundations still lack a consensus interpretation, our results indicate
a persistent preference for the Copenhagen interpretation. This enduring
support likely reflects both the educational emphasis on the Copenhagen
interpretation and its pragmatic appeal in avoiding complex metaphysical
questions and introducing new notions (e.g., other worlds or the pilot wave).
Our findings thus underscore the relative stability of interpretational
preferences over the past decades.

</details>


### [694] [Indiscernibility of quantum states](https://arxiv.org/abs/2507.10027)
*Jan van Neerven,Marijn Waaijer*

Main category: quant-ph

TL;DR: 提出了量子态不可辨别的数学框架，并研究了其结构，将可观测量提升为Holevo空间上的连续函数，并通过具体例子进行了说明。


<details>
  <summary>Details</summary>
Motivation: 研究了可区分对象的结构，即Holevo空间，并表明可观测量可以自然地提升为Holevo空间上的连续函数。

Method: 开发了量子态不可辨别的数学框架。

Result: 研究了Holevo空间和提升的函数，并用具体例子进行了说明，例如自由粒子的位置测量以及EPR和贝尔实验中的自旋测量。

Conclusion: 可区分对象是相对于可观测量的不可辨别性的等价类。

Abstract: In this paper we develop a mathematical framework for indiscernibility of
quantum states, arguing that, given a set of observables, the ``distinguishable
objects'' are the equivalence classes modulo indiscernibility relative to the
observables. The structure of the set of distinguishable objects - called the
Holevo space - is investigated in detail, and it is shown that the observables
admit a natural lift to continuous functions on the Holevo space. The theory is
illustrated by several examples where the ``distinguishable objects'' can be
described explicitly. Among other things, the Holevo spaces and the lifted
functions are described for position measurements on a free particle and for
spin measurements in the EPR and Bell experiments.

</details>


### [695] [Testing APS conjecture on regular graphs](https://arxiv.org/abs/2507.10050)
*Wenxuan Tao,Fen Zuo*

Main category: quant-ph

TL;DR: 本文研究了EPR模型在正则图上的能量上界问题，并测试了APS猜想。研究结果表明，所提出的方法未违反该猜想。


<details>
  <summary>Details</summary>
Motivation: 本文旨在检验一项猜想，该猜想认为最大能量的EPR模型在加权图上的上界可以通过将最大权重分数匹配（MWFM）替换为最大权重匹配（MWM）来加强。

Method: 应用了名为分数纠缠分布（FED）的新算法，该算法基于拟齐次分数匹配，并将其应用于Henning-Yeo图上的EPR模型，以获得尽可能高的能量和尽可能低的最优匹配值，从而对APS猜想进行高精度测试。

Result: 在Henning-Yeo构造的正则图类上，MWM达到了严格的下界。对EPR模型在该正则图类上的应用，数值结果并未显示任何支持APS猜想被违反的证据。

Conclusion: 研究结果不支持APS猜想可能被违反的说法。

Abstract: The maximum energy of the EPR model on a weighted graph is known to be
upper-bounded by the sum of the total weight and the value of maximum-weight
fractional matching~(MWFM). Recently, Apte, Parekh and Sud~(APS) conjecture
that the bound could be strengthened by replacing MWFM with maximum weight
matching~(MWM). Here we test this conjecture on a special class of regular
graphs that Henning and Yeo constructed many years ago. On this class of
regular graphs, MWMs achieve tight lower bounds. As for the maximum energy of
the EPR model, we have recently devised a new algorithm called Fractional
Entanglement Distribution~(FED) based on quasi-homogeneous fractional
matchings, which could achieve rather high accuracy. Applying the FED algorithm
to the EPR model on Henning-Yeo graphs, we could thus obtain energy as high as
possible and matching value as low as possible, and then make high-precision
tests of the APS conjecture. Nevertheless, our numerical results do not show
any evidence that the APS conjecture could be violated.

</details>


### [696] [Davies equation without the secular approximation: Reconciling locality with quantum thermodynamics for open quadratic systems](https://arxiv.org/abs/2507.10080)
*Koki Shiraishi,Masaya Nakagawa,Takashi Mori*

Main category: quant-ph

TL;DR: 量子主方程的推导为热力学一致地描述量子多体系统提供了新的见解。


<details>
  <summary>Details</summary>
Motivation: 研究为什么拟局域Redfield方程恰好等于满足详细平衡条件的Davies方程，这是由于每个浴产生的量子相干性相互抵消。

Method: 推导了一个热力学一致的量子主方程，该方程对耦合到每个位点上独立且相同的浴的二次系统满足局域性。

Result: 表明拟局域Redfield方程恰好等于满足详细平衡条件的Davies方程，这是由于每个浴产生的量子相干性相互抵消。该推导不依赖于在能级间距消失的系统中失败的长期近似。

Conclusion: 该研究结果为热力学一致地描述量子多体系统铺平了道路。

Abstract: We derive a thermodynamically consistent quantum master equation that
satisfies locality for quadratic systems coupled to independent and identical
baths at each site. We show that the quasi-local Redfield equation coincides
exactly with the Davies equation, which satisfies the detailed-balance
condition, due to cancellation of quantum coherence generated by each bath.
This derivation does not rely on the secular approximation, which fails in
systems with vanishing energy-level spacings. We discuss generalizations of our
result to slowly driven quadratic systems and generic quantum many-body
systems. Our result paves the way to a thermodynamically consistent description
of quantum many-body systems.

</details>


### [697] [Continuous variable quantum communication with 40 pairs of entangled sideband](https://arxiv.org/abs/2507.10104)
*Xuan Liu,Shaoping Shi,Yimiao Wu,Xuan Wang,Long Tian,Wei Li,Yajun Wang,Yaohui Zheng*

Main category: quant-ph

TL;DR: 本研究提出了一种从压缩光中生成和分离纠缠边带模式的新方法，实现了高纠缠度，并验证了其在大容量量子通信中的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 构建大规模量子资源是提高量子通信效率和可扩展性的重要基础。

Method: 利用特殊设计的 OPO 从压缩光中生成 40 对纠缠边带模式，并采用低损耗光频梳控制技术和局部互相关算法，通过光学滤波腔实现纠缠边带模式的高效分离。

Result: 实现了最大 6.5 dB 的纠缠度，并实验证明了基于纠缠边带模式的大容量量子密集编码的可行性。

Conclusion: 本研究提出了一个高效提取和稳定控制方案，利用特殊设计的光学参量振荡器从压缩光中生成40对纠缠边带模式。通过低损耗光频梳控制技术和局部互相关算法，实现了对纠缠边带模式的高效分离，纠缠度最大达到6.5 dB。实验证明了基于这些纠缠边带模式的大容量量子密集编码的可行性，对于优化量子资源利用、推动大容量量子通信网络发展以及实现更安全高效的量子通信系统具有重要意义。

Abstract: Constructing large-scale quantum resources is an important foundation for
further improving the efficiency and scalability of quantum communication.
Here, we present an efficient extraction and stable control scheme of 40 pairs
of entangled sideband modes from the squeezed light by specially designing
optical parametric oscillator. Utilizing the low-loss optical frequency comb
control technology and the local cross-correlation algorithm, we model and
manage the efficient separation process of the entangled sidebands modes
facilitated by the optical filtering cavities, a maximum entanglement level of
6.5 dB is achieved. The feasibility of large-capacity quantum dense coding
based on these entangled sideband modes is proved experimentally, which is of
great significance for optimizing the utilization of quantum resources, thereby
contributing to the advancement of large-capacity quantum communication
networks and enabling the realization of more secure and efficient quantum
communication systems.

</details>


### [698] [On the Importance of Fundamental Properties in Quantum-Classical Machine Learning Models](https://arxiv.org/abs/2507.10161)
*Silvie Illésová,Tomasz Rybotycki,Piotr Gawron,Martin Beseda*

Main category: quant-ph

TL;DR: ansatz深度和特征图选择对混合量子-经典神经网络的性能至关重要，多轴泡利旋转编码最有效。


<details>
  <summary>Details</summary>
Motivation: 研究量子电路设计（ansatz深度和量子特征映射）如何影响混合量子-经典神经网络在因果分类任务上的性能。

Method: 结合卷积神经网络和参数化量子电路，系统研究了ansatz深度和特征图选择对因果分类任务的影响。

Result: 增加ansatz重复次数可提高泛化能力和训练稳定性，但收益会达到平台期。特征图的选择至关重要，只有具有多轴泡利旋转的编码才能实现成功学习，而更简单的编码会导致欠拟合或类可分离性丧失。PCA和轮廓系数揭示了跨网络阶段的数据分布演变。

Conclusion: 为混合量子-经典模型设计量子电路提供了实用的指导，强调了ansatz深度和特征图选择的重要性。

Abstract: We present a systematic study of how quantum circuit design, specifically the
depth of the variational ansatz and the choice of quantum feature mapping,
affects the performance of hybrid quantum-classical neural networks on a causal
classification task. The architecture combines a convolutional neural network
for classical feature extraction with a parameterized quantum circuit acting as
the quantum layer. We evaluate multiple ansatz depths and nine different
feature maps. Results show that increasing the number of ansatz repetitions
improves generalization and training stability, though benefits tend to plateau
beyond a certain depth. The choice of feature mapping is even more critical:
only encodings with multi-axis Pauli rotations enable successful learning,
while simpler maps lead to underfitting or loss of class separability.
Principal Component Analysis and silhouette scores reveal how data
distributions evolve across network stages. These findings offer practical
guidance for designing quantum circuits in hybrid models. All source codes and
evaluation tools are publicly available.

</details>


### [699] [Quantum Perspective on Digital Money: Towards a Quantum-Powered Financial System](https://arxiv.org/abs/2507.10227)
*Artur Czerwinski*

Main category: quant-ph

TL;DR: Quantum money uses quantum states for secure currency. This paper analyzes its transfer, banking impact, and SWOT.


<details>
  <summary>Details</summary>
Motivation: The motivation is to explore the concept of quantum money, which encodes economic value in quantum states for enhanced security, integrity, and transferability, and to analyze its implications for the modern financial system.

Method: This perspective article analyzes quantum money by exploring its definition, properties, transfer process using quantum teleportation over quantum networks (terrestrial and satellite-based), its impact on modern banking (especially money creation), and an assessment of its strengths, weaknesses, opportunities, and threats.

Result: The analysis covers the definition and properties of quantum money, the transfer process via quantum teleportation across quantum networks, the impact on banking and money creation, and an SWOT analysis of the concept.

Conclusion: Quantum money, encoding economic value in quantum states, offers enhanced security and integrity. Its transfer via quantum networks and potential impact on banking, particularly money creation, are analyzed. Strengths, weaknesses, opportunities, and threats are assessed.

Abstract: Quantum money represents an innovative approach to currency by encoding
economic value within the quantum states of physical systems, utilizing the
principles of quantum mechanics to enhance security, integrity, and
transferability. This perspective article explores the definition and
properties of quantum money. We analyze the process of transferring quantum
money via quantum teleportation, using terrestrial and satellite-based quantum
networks. Furthermore, we consider the impact of quantum money on the modern
banking system, particularly in money creation. Finally, we conduct an analysis
to assess the strengths and weaknesses of quantum money, as well as
opportunities and threats associated with this emerging concept.

</details>


### [700] [Secure and Efficient Quantum Signature Scheme Based on the Controlled Unitary Operations Encryption](https://arxiv.org/abs/2507.10233)
*Debnath Ghosh,Soumit Roy,Prithwi Bagchi,Indranil Chakrabarty,Ashok Kumar Das*

Main category: quant-ph

TL;DR: 本研究提出了一种新的量子数字签名协议，通过使用链式受控酉操作加密量子消息，解决了现有协议中量子一次性密码本的弱点，提高了安全性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的仲裁量子签名（AQS）协议大多使用量子一次性密码本（QOTP）加密数据，但QOTP容易受到多种攻击，不适合AQS。本研究旨在提出一种更安全、更有效的AQS协议。

Method: 提出了一种新的AQS协议，使用链式受控酉操作来加密量子消息集合，而不是逐个数据比特的量子一次性密码本（QOTP）。

Result: 提出了一种新的AQS协议，使用链式受控酉操作，能够有效防止抵赖和伪造攻击，与现有基于QOTP的协议相比具有优势。

Conclusion: 该研究提出了一种高效的AQS协议，使用链式受控酉操作来加密量子消息集合，有效防止抵赖和伪造攻击，有望推动AQS协议的进一步发展。

Abstract: Quantum digital signatures ensure unforgeable message authenticity and
integrity using quantum principles, offering unconditional security against
both classical and quantum attacks. They are crucial for secure communication
in high-stakes environments, ensuring trust and long-term protection in the
quantum era. Nowadays, the majority of arbitrated quantum signature (AQS)
protocols encrypt data qubit by qubit using the quantum one-time pad (QOTP).
Despite providing robust data encryption, QOTP is not a good fit for AQS
because of its susceptibility to many types of attacks. In this work, we
present an efficient AQS protocol to encrypt quantum message ensembles using a
distinct encryption technique, the chained controlled unitary operations. In
contrast to existing protocols, our approach successfully prevents disavowal
and forgery attacks. We hope this contributes to advancing future
investigations into the development of AQS protocols.

</details>


### [701] [Tracing attosecond currents and controlling their direction in a scanning tunneling microscope](https://arxiv.org/abs/2507.10252)
*Daniel Davidovich,Boyang Ma,Adi Goldner,Shimon Cohen,Zhaopin Chen,Michael Krüger*

Main category: quant-ph

TL;DR: 本研究在STM中通过双色激光脉冲控制了阿秒隧道电流的方向，实现了900阿秒的电流持续时间和2纳米的空间分辨率，为超快电荷动力学研究开辟了新途径。


<details>
  <summary>Details</summary>
Motivation: 尽管亚周期阿秒动力学在STM中已得到精确控制和测量，但控制电流方向和确定其持续时间仍然具有挑战性。本研究旨在解决这一问题，探索同时实现阿秒和纳米级分辨率的可能性。

Method: 通过使用双色激光脉冲诱导STM隧道电流，并仅依靠亚周期波形动态控制其方向。利用与时间相关薛定谔方程的数值和解析解对测量数据进行投影，揭示了非绝热隧道效应是其底层的物理机制。

Result: 研究实现了对STM隧道电流方向的动态控制，并确定了电流猝发持续时间约为900阿秒。在环境条件下，实现了2纳米的横向空间分辨率和亚埃米托级的形貌灵敏度。

Conclusion: 该研究实现了在STM中对阿秒隧道电流的定向控制，并达到了2纳米的横向空间分辨率和亚埃米托级的形貌灵敏度。这种控制将为在分子系统和缺陷态中触发和探测超快电荷动力学提供可能，从而在显微学的时空前沿实现新的突破。

Abstract: The tunneling effect driven by waveform-controlled laser pulses is a
cornerstone of attosecond science and contributes decisively to its extreme
time resolution. In the spatial domain, electron tunneling through a
bias-driven junction between a nanotip and a surface enables atomic-scale
spatial resolution in scanning tunneling microscopy (STM). Recently, first
signatures of attosecond modulation of STM currents have shown that
simultaneous attosecond-nanometer resolution is feasible. However, while
sub-cycle attosecond dynamics are routinely controlled and determined with high
precision, controlling the direction of the currents and determining their
duration have remained elusive in STM. Here, we induce STM tunneling currents
using two-color laser pulses and dynamically control their direction, relying
solely on the sub-cycle waveform of the pulses. Projecting our measurement data
onto numerical and analytical solutions of the time-dependent Schr\"odinger
equation reveals non-adiabatic tunneling as the underlying physical mechanism,
yielding current burst durations on the order of 900 as. Despite working under
ambient conditions but free from thermal artifacts, we achieve a lateral
spatial resolution of 2 nm and sub-angstr\"om topographic sensitivity.
Directional control of attosecond bursts in STM will enable triggering and
detecting ultrafast charge dynamics in molecular systems and defect states at
the spatio-temporal frontier of microscopy.

</details>


### [702] [Efficient Measurement of Bosonic Non-Gaussianity](https://arxiv.org/abs/2507.10272)
*Kaifeng Bu,Bikun Li*

Main category: quant-ph

TL;DR: 提出了一种新的量化玻色子非高斯性的方法（非高斯熵）及其测量方案，并推广至混合态。


<details>
  <summary>Details</summary>
Motivation: 研究量化玻色子多体系统中的非高斯性的方法，因为非高斯态是量子信息处理的关键资源。

Method: 通过研究玻色子纯态的自卷积性质，引入非高斯熵作为量化指标，并提出了一种基于分束器和多副本态的测量协议。

Result: 成功引入非高斯熵作为量化玻色子纯态非高斯性的新方法，并提出了具体的测量方案，同时将该方法推广到混合态。

Conclusion: 本研究提出了一种新的量化方法——非高斯熵，用于表征玻色子纯态的非高斯性，并提出了一种使用三个分束器和四个输入态副本的实际测量方案。此外，该框架已扩展到混合态，为量化非高斯性提供了一种通用方法。

Abstract: Non-Gaussian states are essential resources in quantum information
processing. In this work, we investigate methods for quantifying bosonic
non-Gaussianity in many-body systems. Building on recent theoretical insights
into the self-convolution properties of bosonic pure states, we introduce
non-Gaussian entropy as a new measure to characterize non-Gaussianity in
bosonic pure states. We further propose a practical protocol for measuring
non-Gaussian entropy using three beam splitters and four copies of the input
state. In addition, we extend this framework to mixed states, providing a
general approach to quantifying non-Gaussianity. Our results offer a convenient
and efficient method for characterizing bosonic non-Gaussianity, paving the way
for its implementation on near-term experimental platforms.

</details>


### [703] [From Linear Differential Equations to Unitaries: A Moment-Matching Dilation Framework with Near-Optimal Quantum Algorithms](https://arxiv.org/abs/2507.10285)
*Xiantao Li*

Main category: quant-ph

TL;DR: 本研究提出了一种将非酉动力学模拟转化为酉演化的新方法，该方法适用于多种生成器，并具有可调参数，能与特定应用和硬件协同优化，并在模拟粘弹性波等问题上得到验证。


<details>
  <summary>Details</summary>
Motivation: 现实物理模型中遇到的 ODE/PDE 系统通常是非酉的，而量子加速通常需要酉时间演化。本研究旨在解决这一矛盾，使量子计算能够模拟非酉系统。

Method: 通过构建一个满足紧凑矩恒等式的代数判据，将任意线性非厄米流 $\dot x = Ax$ 嵌入到扩大的希尔伯特空间中的严格酉演化。

Result: 该方法不仅恢复了现有的 Schrödingerization 和 LCHS 方案，还揭示了由微分、积分、伪微分和差分生成器构成的新膨胀族。这些膨胀族具有连续调谐参数，并且对保持矩不变的相似变换封闭。有限差分膨胀在有限区间内实现了接近最优的预言复杂度，并且在麦克斯韦粘弹性波传播的数值实验中验证了其准确性和鲁棒性。

Conclusion: 该研究提出了一种通用的、满足矩的膨胀方法，将非厄米系统的动力学模拟转化为酉演化，为量子模拟开辟了新的设计空间，并为实际应用提供了数值验证。

Abstract: Quantum speed-ups for dynamical simulation usually demand unitary
time-evolution, whereas the large ODE/PDE systems encountered in realistic
physical models are generically non-unitary. We present a universal
moment-fulfilling dilation that embeds any linear, non-Hermitian flow $\dot x =
A x$ with $A=-iH+K$ into a strictly unitary evolution on an enlarged Hilbert
space: \[
  ( (l| \otimes I )
  \mathcal T e^{-i \int ( I_A\otimes H +i F\otimes K) dt}
  ( |r) \otimes I )
  = \mathcal T e^{\int A dt}, \] provided the triple $( F, (l|, |r) )$
satisfies the compact moment identities $(l| F^{k}|r) =1$ for all $k\ge 0$ in
the ancilla space. This algebraic criterion recovers both
\emph{Schr\"odingerization} [Phys. Rev. Lett. 133, 230602 (2024)] and the
linear-combination-of-Hamiltonians (LCHS) scheme [Phys. Rev. Lett. 131, 150603
(2023)], while also unveiling whole families of new dilations built from
differential, integral, pseudo-differential, and difference generators. Each
family comes with a continuous tuning parameter \emph{and} is closed under
similarity transformations that leave the moments invariant, giving rise to an
overwhelming landscape of design space that allows quantum dilations to be
co-optimized for specific applications, algorithms, and hardware.
  As concrete demonstrations, we prove that a simple finite-difference dilation
in a finite interval attains near-optimal oracle complexity. Numerical
experiments on Maxwell viscoelastic wave propagation confirm the accuracy and
robustness of the approach.

</details>


### [704] [Grassmann Variational Monte Carlo with neural wave functions](https://arxiv.org/abs/2507.10287)
*Douglas Hendry,Alessandro Sinibaldi,Giuseppe Carleo*

Main category: quant-ph

TL;DR: 该研究提出了一个用于计算量子多体系统激发态的新框架，解决了现有方法的局限性，并在海森堡模型上取得了成功。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有数值方法在计算多体系统中激发态时的挑战，例如在稠密光谱中的不稳定性以及对对称约束或基于惩罚的表述的依赖性。

Method: 通过将Pfau等人介绍的框架用希尔伯特空间的 Grassmann 几何形式化，我们能够推广随机重构方法，同时优化多个变分波函数，并引入算子方差和重叠的多维版本。

Result: 在海森堡量子自旋模型上实现了高精度的能量和物理可观测值，覆盖了大量的激发态。

Conclusion: 本文的方法在海森堡量子自旋模型上取得了高度精确的能量和物理可观测值，适用于大量激发态。

Abstract: Excited states play a central role in determining the physical properties of
quantum matter, yet their accurate computation in many-body systems remains a
formidable challenge for numerical methods. While neural quantum states have
delivered outstanding results for ground-state problems, extending their
applicability to excited states has faced limitations, including instability in
dense spectra and reliance on symmetry constraints or penalty-based
formulations. In this work, we rigorously formalize the framework introduced by
Pfau et al.~\cite{pfau2024accurate} in terms of Grassmann geometry of the
Hilbert space. This allows us to generalize the Stochastic Reconfiguration
method for the simultaneous optimization of multiple variational wave
functions, and to introduce the multidimensional versions of operator variances
and overlaps. We validate our approach on the Heisenberg quantum spin model on
the square lattice, achieving highly accurate energies and physical observables
for a large number of excited states.

</details>


### [705] [Quantum i.i.d. Steady States in Open Many-Body Systems](https://arxiv.org/abs/2507.10319)
*Takanao Ishii,Masahito Ueda*

Main category: quant-ph

TL;DR: 研究发现，满足特定条件的开放量子系统可以拥有独立的稳态，但这种稳态排除了量子纠缠和空间关联。


<details>
  <summary>Details</summary>
Motivation: 研究开放量子系统如何稳定维持非平衡稳态对于探索和利用开放量子系统具有基本和实际的重要性。

Method: 通过识别与任意量子i.i.d.状态交换的算子集合，推导出了一个使系统具有量子i.i.d.稳态的充分条件。该条件表明，对于满足该充分条件的系统，量子i.i.d.状态集合是时间演化超算子下的不变子集。

Result: 确立了开放量子多体系统在局域驱动和/或耗散作用下具有量子i.i.d.稳态的通用等价条件。提出了一个充分条件，用于识别具有量子i.i.d.稳态的系统，并发现量子i.i.d.状态集是满足该充分条件系统的演化不变子集。这些发现不仅识别了一类具有精确可解稳态的模型，而且得出了一个“不可为”定理，排除了在耗散环境中的广泛量子多体稳态中的量子纠缠和空间关联。

Conclusion: 该研究为开放量子多体系统在特定条件下具有量子独立同分布（i.i.d.）稳态提供了一个充分条件，并由此推导出一个不存在量子纠缠和空间关联的“不可为”定理，适用于广泛的耗散环境中的量子多体稳态。

Abstract: Understanding how a quantum many-body state is maintained stably as a
nonequilibrium steady state is of fundamental and practical importance for
exploration and exploitation of open quantum systems. We establish a general
equivalent condition for an open quantum many-body system governed by the
Gorini-Kossakowski-Sudarshan-Lindblad dynamics under local drive and/or
dissipation to have a quantum independent and identically distributed (i.i.d.)
steady state. We present a sufficient condition for a system to have a quantum
i.i.d. steady state by identifying a set of operators that commute with
arbitrary quantum i.i.d. states. In particular, a set of quantum i.i.d. states
is found to be an invariant subset of time evolution superoperators for systems
that satisfy the sufficient condition. These findings not only identify a class
of models with exactly solvable steady states but also lead to a no-go theorem
that precludes quantum entanglement and spatial correlations in a broad class
of quantum many-body steady states in a dissipative environment.

</details>


### [706] [Suppressing crosstalk for Rydberg quantum gates](https://arxiv.org/abs/2507.10356)
*Gina Warttmann,Florian Meinert,Hans Peter Büchler,Sebastian Weber*

Main category: quant-ph

TL;DR: 提出了一种新的量子门协议，显著提高了中性原子量子计算机中局部寻址控制的Z门的保真度。


<details>
  <summary>Details</summary>
Motivation: 为了解决在量子计算机中，局部寻址的激光在实现门操作时会泄漏到其他原子，从而引起串扰的问题。

Method: 提出了一种通过自旋回波受启发的门协议，用于抑制局部寻址控制的Z门中的串扰，并通过设计一个补偿剩余相位误差的电路来进一步提高保真度。

Result: 该门协议将保真度提高了两个数量级，并通过相位误差补偿电路进一步降低了错误率，证明了其在实验相关参数范围内的有效性。

Conclusion: 我们的研究结果为基于里德堡的量子计算机使用局部寻址实现高保真度量子门提供了新的途径。

Abstract: We present a method to suppress crosstalk from implementing controlled-Z
gates via local addressing in neutral atom quantum computers. In these systems,
a fraction of the laser light that is applied locally to implement gates
typically leaks to other atoms. We analyze the resulting crosstalk in a setup
of two gate atoms and one neighboring third atom. We then perturbatively derive
a spin-echo-inspired gate protocol that suppresses the leading order of the
amplitude error, which dominates the crosstalk. Numerical simulations
demonstrate that our gate protocol improves the fidelity by two orders of
magnitude across a broad range of experimentally relevant parameters. To
further reduce the infidelity, we develop a circuit to cancel remaining phase
errors. Our results pave the way for using local addressing for high-fidelity
quantum gates on Rydberg-based quantum computers.

</details>


### [707] [Exponential-recovery model for free-running SPADs with capacity-induced dead-time imperfections](https://arxiv.org/abs/2507.10361)
*Jan Krause,Nino Walenta*

Main category: quant-ph

TL;DR: 该研究提出了一个改进的 SPAD 计数率模型，能更准确地处理高光子通量和死区时间后的指数恢复，简化了探测器表征，并在 QKD、TCSPC、LIDAR 等领域有广泛应用前景。


<details>
  <summary>Details</summary>
Motivation: 当前的 SPAD 计数率模型通常假设量子效率在死区时间后瞬时恢复，这在高光子通量下会导致对有效探测效率的系统性高估。

Method: 提出了一种广义的解析计数率模型，用于自由运行的 SPAD，该模型模拟了死区时间后量子效率的非瞬时、指数恢复过程，并将其置于非齐次泊松过程的理论框架内。

Result: 该模型能够准确预测饱和区域的探测统计数据，与传统阶跃函数模型相比，在输入光子率方面提高了两个数量级。此外，该模型还通过单次探测间隔直方图即可提取量子效率 η_0、死区时间 τ_d 和恢复时间常数 τ_r，简化了 SPAD 的表征。

Conclusion: 该模型通过包含一个额外的探测器参数——指数恢复时间常数 τ_r，并借鉴了非齐次泊松过程的理论，能够准确预测饱和区域的探测统计数据，在脉冲光子探测器（SPAD）的计数率建模方面取得了显著进展，尤其是在高光子通量下，其性能远超传统模型。此外，该模型还通过提取量子效率 η_0、死区时间 τ_d 和恢复时间常数 τ_r，简化了 SPAD 的表征过程，并易于改编以适应其他类型的受死区时间限制的探测器，有望在量子密钥分发（QKD）、时间相关单光子计数（TCSPC）和激光雷达（LIDAR）等领域得到广泛应用。

Abstract: Current count-rate models for single-photon avalanche diodes (SPADs)
typically assume an instantaneous recovery of the quantum efficiency following
dead-time, leading to a systematic overestimation of the effective detection
efficiency for high photon flux. To overcome this limitation, we introduce a
generalized analytical count-rate model for free-running SPADs that models the
non-instantaneous, exponential recovery of the quantum efficiency following
dead-time. Our model, framed within the theory of non-homogeneous Poisson
processes, only requires one additional detector parameter -- the
exponential-recovery time constant $\tau_\mathrm{r}$. The model accurately
predicts detection statistics deep into the saturation regime, outperforming
the conventional step-function model by two orders of magnitude in terms of the
impinging photon rate. For extremely high photon flux, we further extend the
model to capture paralyzation effects. Beyond photon flux estimation, our model
simplifies SPAD characterization by enabling the extraction of quantum
efficiency $\eta_0$, dead-time $\tau_\mathrm{d}$, and recovery time constant
$\tau_\mathrm{r}$ from a single inter-detection interval histogram. This can be
achieved with a simple setup, without the need for pulsed lasers or externally
gated detectors. We anticipate broad applicability of our model in quantum key
distribution (QKD), time-correlated single-photon counting (TCSPC), LIDAR, and
related areas. Furthermore, the model is readily adaptable to other types of
dead-time-limited detectors. A Python implementation is provided as
supplementary material for swift adoption.

</details>


### [708] [State-Based Classical Shadows](https://arxiv.org/abs/2507.10362)
*Zvika Brakerski,Nir Magrafta,Tomer Solomon*

Main category: quant-ph

TL;DR: 本研究提出了一种新的基于状态的经典影集断层扫描方法，无需酉变换，使用伪随机状态族即可高效推断可计算观测值，且所需深度恒定。


<details>
  <summary>Details</summary>
Motivation: 经典的影集断层扫描方法在创建经典快照方面效率有待提高，尤其是深度方面。本研究旨在探索替代经典影集断层扫描的方法，以提高其效率和可行性。

Method: 本研究提出了一种新的经典影集断层扫描方法，该方法不依赖于酉变换的分布，而是利用状态的分布。具体来说，通过将未知输入状态与独立制备的辅助状态进行纠缠，然后测量纠缠态来创建经典快照。该方法尤其适用于计算可观测量，并证明了使用伪随机状态族就足够了。

Result: 通过实验证明，该方法在效率方面，尤其是计算复杂度和深度方面，优于现有方法。具体而言，该方法在线部分（依赖于输入）仅需执行贝尔基测量，并可使用基本门实现恒定深度。

Conclusion: 本研究提出了一种基于状态的经典影集断层扫描方法，该方法使用伪随机状态族来构建经典快照，并在此基础上对可计算观测值进行高效推断。

Abstract: Classical Shadow Tomography (Huang, Kueng and Preskill, Nature Physics 2020)
is a method for creating a classical snapshot of an unknown quantum state,
which can later be used to predict the value of an a-priori unknown observable
on that state. In the short time since their introduction, classical shadows
received a lot of attention from the physics, quantum information, and quantum
computing (including cryptography) communities. In particular there has been a
major effort focused on improving the efficiency, and in particular depth, of
generating the classical snapshot.
  Existing constructions rely on a distribution of unitaries as a central
building block, and research is devoted to simplifying this family as much as
possible. We diverge from this paradigm and show that suitable distributions
over \emph{states} can be used as the building block instead. Concretely, we
create the snapshot by entangling the unknown input state with an independently
prepared auxiliary state, and measuring the resulting entangled state. This
state-based approach allows us to consider a building block with arguably
weaker properties that has not been studied so far in the context of classical
shadows. Notably, our cryptographically-inspired analysis shows that for
\emph{efficiently computable} observables, it suffices to use
\emph{pseudorandom} families of states. To the best of our knowledge,
\emph{computational} classical shadow tomography was not considered in the
literature prior to our work.
  Finally, in terms of efficiency, the online part of our method (i.e.\ the
part that depends on the input) is simply performing a measurement in the Bell
basis, which can be done in constant depth using elementary gates.

</details>


### [709] [Optimization and characterization of laser excitation for quantum sensing with single nitrogen-vacancy centres](https://arxiv.org/abs/2507.10386)
*Alejandro Martínez-Méndez,Jesús Moreno-Meseguer,Mariusz Mrózek,Adam Wojciechowski,Priya Balasubramanian,Fedor Jelezko,Javier Prior*

Main category: quant-ph

TL;DR: This paper presents a method to optimize laser irradiation in confocal microscopy for NV-center quantum sensing, detailing the characterization of optical parameters and demonstrating improved sensor performance.


<details>
  <summary>Details</summary>
Motivation: Precise control and understanding of optical parameters are essential for reliable single-emitter studies in quantum sensing experiments using nitrogen-vacancy (NV) centers, despite the suitability of confocal microscopy for such applications.

Method: The study involves characterizing and optimizing laser irradiation within a confocal microscope for quantum sensing experiments using nitrogen-vacancy (NV) centers. Key investigations include laser beam intensity profile, single-photon emission statistics, fluorescence response under varying polarization and saturation conditions, spectral characteristics, and temporal profiles of readout and reinitialization pulses. Specific techniques used are the razorblade method for beam quality assessment ($M^2$), optical fluorescence spectrum recording, second-order autocorrelation function $g^{(2)}(	au)$ measurement for single-emitter confirmation, analysis of saturation behavior by varying laser power, polarization dependence study using a half-wave plate, and examination of temporal laser pulse profiles using an acousto-optic modulator.

Result: The research optimized various optical parameters, including beam intensity profile, emission statistics, fluorescence response, spectral characteristics, and temporal pulse profiles, demonstrating the microscope's capabilities in driving spin transitions of a single NV center.

Conclusion: The paper establishes a straightforward and effective protocol for laser excitation optimization, enhancing the performance and reliability of NV-based quantum sensors.

Abstract: In this work we present a comprehensive method of characterization and
optimization of laser irradiation within a confocal microscope tailored to
quantum sensing experiments using nitrogen-vacancy (NV) centres. While confocal
microscopy is well-suited for such experiments, precise control and
understanding of several optical parameters are essential for reliable
single-emitter studies. We investigate the laser beam intensity profile,
single-photon emission statistics, fluorescence response under varying
polarization and saturation conditions, spectral characteristics, and the
temporal profiles of readout and reinitialization pulses. The beam quality is
assessed using the beam propagation factor $M^2$, determined via the razorblade
technique. Optical fluorescence spectrum is recorded to confirm NV centre
emission. To confirm single-emitter operation, we measure second-order
autocorrelation function $g^{(2)}(\tau)$. Saturation behaviour is analysed by
varying laser power and recording the corresponding fluorescence, while
polarization dependence is studied using a half-wave ($\lambda/2$) plate.
Temporal laser pulse profile is examined by modulating the power of an
acousto-optic modulator. After optimizing all relevant parameters, we
demonstrate the microscope's capabilities in driving spin transitions of a
single NV centre. This work establishes a straightforward and effective
protocol for laser excitation optimization, enhancing the performance and
reliability of NV-based quantum sensors.

</details>


### [710] [Fault-Tolerant Quantum Error Correction for Constant-Excitation Stabilizer Codes under Coherent Noise](https://arxiv.org/abs/2507.10395)
*Ching-Yi Lai,Pei-Hao Liou,Yingkai Ouyang*

Main category: quant-ph

TL;DR: 本研究提出了首个CE码的完整FTQEC框架，解决了相干噪声问题，并展示了CE码在相干噪声下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决了集体相干噪声对容错量子纠错（FTQEC）的挑战，并提出了CE码在现实噪声模型下的完整容错框架。

Method: 通过双轨串联引入了CE CSS码的完整容错架构，包括CE保持逻辑CNOT门以及使用零控制非门和CE兼容辅助的修改后的Shor和Steane类型综合提取方案。还提出了扩展的稳定器模拟算法来跟踪随机和集体相干噪声。

Result: 识别了最小的CE码，例如[[12,1,3]]和[[14,3,3]]码，并证明了[[12,1,3]]码在相干噪声下表现出色。

Conclusion: CE码是量子纠错的可能解决方案，尤其适用于受集体相干噪声影响的量子处理器。

Abstract: Collective coherent noise poses challenges for fault-tolerant quantum error
correction (FTQEC), as it falls outside the usual stochastic noise models.
While constant excitation (CE) codes can naturally avoid coherent noise, a
complete fault-tolerant framework for the use of these codes under realistic
noise models has been elusive. Here, we introduce a complete fault-tolerant
architecture for CE CSS codes based on dual-rail concatenation. After showing
that transversal CNOT gates violate CE code constraints, we introduce
CE-preserving logical CNOT gates and modified Shor- and Steane-type syndrome
extraction schemes using zero-controlled NOT gates and CE-compatible ancilla.
This enables fault-tolerant syndrome-extraction circuits fully compatible with
CE constraints. We also present an extended stabilizer simulation algorithm
that efficiently tracks both stochastic and collective coherent noise. Using
our framework, we identify minimal CE codes, including the $[[12,1,3]]$ and
$[[14,3,3]]$ codes, and demonstrate that the $[[12,1,3]]$ code achieves strong
performance under coherent noise. Our results establish the first complete
FTQEC framework for CE codes, demonstrating their robustness to coherent noise.
This highlights the potential of CE codes as a possible solution for quantum
processors dominated by collective coherent noise.

</details>


### [711] [Nonlinear Quantum Sensing with a Frustrated Kitaev Trimer](https://arxiv.org/abs/2507.10418)
*C. Huerta Alderete,Anubhav Kumar Srivastava,Andrew T. Sornborger*

Main category: quant-ph

TL;DR: 基于受阻三自旋系统的量子传感器对信号有阈值响应，且在纠缠多传感器配置中具有海森极限灵敏度，可用于信号检测和粒子径迹探测等。


<details>
  <summary>Details</summary>
Motivation: 研究受阻三自旋系统在经典时间相关场下的响应，以探索其作为量子传感器的潜力。

Method: 研究了基于受阻三自旋系统（Kitaev三聚体）的拉姆齐干涉量子传感器的响应，该系统对经典时间相关场（信号）敏感。在绝热近似和适当的初始状态下，研究了信号对传感器的影响，发现存在一个阈值。

Result: 在特定条件下，该传感器对零均值信号表现出阈值响应行为，低于某一阈值时不响应，高于阈值时则作为检测器。此外，在纠缠多传感器配置中，该传感器可以达到海森极限的灵敏度。

Conclusion: 该传感器可作为独立单元用于信号检测，也可用于构建二维或三维阵列，应用于粒子径迹探测和长基线光学干涉测量等领域。

Abstract: We investigate the response of a Ramsey interferometric quantum sensor based
on a frustrated, three-spin system (a Kitaev trimer) to a classical
time-dependent field (signal). The system eigenspectrum is symmetric about a
critical point, $|\vec{b}| = 0$, with four of the spectral components varying
approximately linearly with the magnetic field and four exhibiting a nonlinear
dependence. Under the adiabatic approximation and for appropriate initial
states, we show that the sensor's response to a zero-mean signal is such that
below a threshold, $|\vec{b}| < b_\mathrm{th}$, the sensor does not respond to
the signal, whereas above the threshold, the sensor acts as a detector that the
signal has occurred. This thresholded response is approximately
omnidirectional. Moreover, when deployed in an entangled multisensor
configuration, the sensor achieves sensitivity at the Heisenberg limit. Such
detectors could be useful both as standalone units for signal detection above a
noise threshold and in two- or three-dimensional arrays, analogous to a quantum
bubble chamber, for applications such as particle track detection and
long-baseline telescopy.

</details>


### [712] [A Rigorous Introduction to Hamiltonian Simulation via High-Order Product Formulas](https://arxiv.org/abs/2507.10501)
*Javier Lopez-Cerezo*

Main category: quant-ph

TL;DR: 本研究为量子计算中的汉密尔顿模拟提供了一个清晰的数学介绍，重点是高阶乘积公式，如 Suzuki 的递归方法，并讨论了它们在克服经典计算瓶颈方面的应用。


<details>
  <summary>Details</summary>
Motivation: 该工作旨在为量子计算中的汉密尔顿模拟提供严格且独立的介绍，重点关注高效逼近量子系统时间演化的高阶乘积公式。

Method: 本研究探讨了 Lie-Trotter 乘积公式及其高阶推广，特别是 Suzuki 的递归方法，以实现改进的误差缩放。

Result: 理论分析和示例表明，这些技术在高高数汉密尔顿和克服经典计算瓶颈方面具有优势和局限性。

Conclusion: 该工作总结了汉密尔顿模拟的最新进展和开放挑战。

Abstract: This work provides a rigorous and self-contained introduction to numerical
methods for Hamiltonian simulation in quantum computing, with a focus on
high-order product formulas for efficiently approximating the time evolution of
quantum systems. Aimed at students and researchers seeking a clear mathematical
treatment, the study begins with the foundational principles of quantum
mechanics and quantum computation before presenting the Lie-Trotter product
formula and its higher-order generalizations. In particular, Suzuki's recursive
method is explored to achieve improved error scaling. Through theoretical
analysis and illustrative examples, the advantages and limitations of these
techniques are discussed, with an emphasis on their application to $k$-local
Hamiltonians and their role in overcoming classical computational bottlenecks.
The work concludes with a brief overview of current advances and open
challenges in Hamiltonian simulation.

</details>


### [713] [A Classification of Transversal Clifford Gates for Qubit Stabilizer Codes](https://arxiv.org/abs/2507.10519)
*Shival Dasu,Simon Burton*

Main category: quant-ph

TL;DR: 对稳定器码的对角克利福德门进行了分类，确定了六种可能的门集，并完成了对 $\ell$ 个码块的分类。


<details>
  <summary>Details</summary>
Motivation: 对稳定器码进行分类，以理解其对角克利福德门集的结构和对称性。

Method: 通过研究稳定器码的对角克利福德门集，并利用 Rains 首次引入的内射张量代数理论，对 $\ell$ 个码块的对角克利福德对称性进行了完整分类。

Result: 发现任何稳定器码的对角横向克利福德门群必定属于六个不同的矩阵群家族之一，并完成了对 $\ell$ 个码块的对角克利福德对称性的分类。

Conclusion: 该工作对稳定器码进行了分类，具体是根据其上可横向实现的对角克利福德门集。

Abstract: This work classifies stabilizer codes by the set of diagonal Clifford gates
that can be implemented transversally on them. We show that, for any stabilizer
code, its group of diagonal transversal Clifford gates on $\ell$ code blocks
must be one of six distinct families of matrix groups. We further develop the
theory of classifying stabilizer codes by via matrix algebras of endomorphisms
first introduced by Rains, and give a complete classification of the diagonal
Clifford symmetries of $\ell$ code blocks. A number of corollaries are given in
the final section.

</details>
