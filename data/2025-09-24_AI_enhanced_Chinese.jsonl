{"id": "2509.18100", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.18100", "abs": "https://arxiv.org/abs/2509.18100", "authors": ["Shishir Lamichhane", "Anamika Dubey"], "title": "Stochastic Economic Dispatch with Battery Energy Storage considering Wind and Load Uncertainty", "comment": "to be published in NAPS", "summary": "With the integration of renewable energy resources in power systems, managing\noperational flexibility and reliability while minimizing operational costs has\nbecome increasingly challenging. Battery energy storage system (BESS) offers a\npromising solution to address these issues. This paper presents a stochastic\ndynamic economic dispatch with storage (SDED-S) framework to assess the impact\nof BESS in managing uncertainty. The temporal correlation between wind and load\nuncertainties is captured, with scenarios generated using a method inspired by\nstratified and importance sampling. The proposed approach is demonstrated on a\nmodified IEEE 39-bus system, where selected conventional generators are\nconverted to wind power plants. Case studies show that strategic BESS\ndeployment significantly improves system flexibility by reducing renewable\ncurtailments and dispatch costs. Renewable energy curtailments decrease upon\nincreasing BESS size and approach zero depending on wind penetration level.\nHigher wind penetrations result in greater curtailments without storage and\nyield larger cost savings when BESS is deployed, highlighting the growing need\nfor flexibility as renewable energy penetrations increase.", "AI": {"tldr": "BESS\u901a\u8fc7SDED-S\u6846\u67b6\u6539\u5584\u7535\u529b\u7cfb\u7edf\u7075\u6d3b\u6027\u548c\u964d\u4f4e\u6210\u672c\uff0c\u7279\u522b\u662f\u5728\u9ad8\u98ce\u80fd\u6e17\u900f\u7387\u4e0b\u3002", "motivation": "\u5728\u6574\u5408\u98ce\u80fd\u7b49\u53ef\u518d\u751f\u80fd\u6e90\u7684\u7535\u529b\u7cfb\u7edf\u4e2d\uff0c\u7ba1\u7406\u8fd0\u8425\u7075\u6d3b\u6027\u3001\u53ef\u9760\u6027\u5e76\u540c\u65f6\u6700\u5c0f\u5316\u8fd0\u8425\u6210\u672c\u9762\u4e34\u6311\u6218\u3002BESS\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u5305\u542b\u50a8\u80fd\u7684\u968f\u673a\u52a8\u6001\u7ecf\u6d4e\u8c03\u5ea6\uff08SDED-S\uff09\u6846\u67b6\uff0c\u6355\u6349\u98ce\u80fd\u548c\u8d1f\u8377\u4e0d\u786e\u5b9a\u6027\u4e4b\u95f4\u7684\u65f6\u95f4\u76f8\u5173\u6027\uff0c\u5e76\u4f7f\u7528\u53d7\u5206\u5c42\u548c\u91cd\u8981\u6027\u91c7\u6837\u542f\u53d1\u7684\u573a\u666f\u751f\u6210\u65b9\u6cd5\u3002", "result": "\u5728\u6539\u8fdb\u7684IEEE 39\u603b\u7ebf\u7cfb\u7edf\u4e0a\u8fdb\u884c\u4e86\u6848\u4f8b\u7814\u7a76\uff0c\u663e\u793aBESS\u90e8\u7f72\u663e\u8457\u63d0\u9ad8\u4e86\u7cfb\u7edf\u7075\u6d3b\u6027\uff0c\u51cf\u5c11\u4e86\u53ef\u518d\u751f\u80fd\u6e90\u5f03\u7f6e\u548c\u8c03\u5ea6\u6210\u672c\u3002BESS\u5c3a\u5bf8\u7684\u589e\u52a0\u51cf\u5c11\u4e86\u53ef\u518d\u751f\u80fd\u6e90\u7684\u5f03\u7f6e\uff0c\u5728\u9ad8\u98ce\u80fd\u6e17\u900f\u7387\u4e0b\uff0cBESS\u90e8\u7f72\u5e26\u6765\u7684\u6210\u672c\u8282\u7ea6\u66f4\u5927\u3002", "conclusion": "BESS\u7684\u6218\u7565\u90e8\u7f72\u80fd\u6709\u6548\u63d0\u5347\u7535\u529b\u7cfb\u7edf\u7684\u7075\u6d3b\u6027\uff0c\u964d\u4f4e\u6210\u672c\uff0c\u5e76\u4e14\u5728\u9ad8\u98ce\u80fd\u6e17\u900f\u7387\u4e0b\u5176\u91cd\u8981\u6027\u66f4\u4e3a\u51f8\u663e\u3002"}}
{"id": "2509.18224", "categories": ["eess.SY", "cs.RO", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.18224", "abs": "https://arxiv.org/abs/2509.18224", "authors": ["Svyatoslav Covanov", "Cedric Pradalier"], "title": "Reversible Kalman Filter for state estimation with Manifold", "comment": null, "summary": "This work introduces an algorithm for state estimation on manifolds within\nthe framework of the Kalman filter. Its primary objective is to provide a\nmethodology enabling the evaluation of the precision of existing Kalman filter\nvariants with arbitrary accuracy on synthetic data, something that, to the best\nof our knowledge, has not been addressed in prior work. To this end, we develop\na new filter that exhibits favorable numerical properties, thereby correcting\nthe divergences observed in previous Kalman filter variants. In this\nformulation, the achievable precision is no longer constrained by the\nsmall-velocity assumption and is determined solely by sensor noise. In\naddition, this new filter assumes high precision on the sensors, which, in real\nscenarios require a detection step that we define heuristically, allowing one\nto extend this approach to scenarios, using either a 9-axis IMU or a\ncombination of odometry, accelerometer, and pressure sensors. The latter\nconfiguration is designed for the reconstruction of trajectories in underwater\nenvironments.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5361\u5c14\u66fc\u6ee4\u6ce2\u7684\u6d41\u5f62\u4e0a\u72b6\u6001\u4f30\u8ba1\u7b97\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u73b0\u6709\u5361\u5c14\u66fc\u6ee4\u6ce2\u65b9\u6cd5\u7684\u7cbe\u5ea6\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u5177\u6709\u826f\u597d\u6570\u503c\u7279\u6027\u7684\u6ee4\u6ce2\u5668\uff0c\u4ee5\u89e3\u51b3\u4ee5\u5f80\u65b9\u6cd5\u4e2d\u5b58\u5728\u7684\u53d1\u6563\u95ee\u9898\u3002", "motivation": "\u8bc4\u4f30\u73b0\u6709\u5361\u5c14\u66fc\u6ee4\u6ce2\u53d8\u4f53\u5728\u6d41\u5f62\u4e0a\u7684\u7cbe\u5ea6\uff0c\u5e76\u89e3\u51b3\u4ee5\u5f80\u65b9\u6cd5\u4e2d\u7684\u53d1\u6563\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5361\u5c14\u66fc\u6ee4\u6ce2\u7b97\u6cd5\uff0c\u7528\u4e8e\u6d41\u5f62\u4e0a\u7684\u72b6\u6001\u4f30\u8ba1\uff0c\u8be5\u7b97\u6cd5\u5177\u6709\u826f\u597d\u7684\u6570\u503c\u7279\u6027\uff0c\u5e76\u4e14\u4e0d\u4f9d\u8d56\u4e8e\u5c0f\u901f\u5ea6\u5047\u8bbe\u3002", "result": "\u65b0\u7684\u6ee4\u6ce2\u5668\u6d88\u9664\u4e86\u4ee5\u5f80\u65b9\u6cd5\u4e2d\u7684\u53d1\u6563\u95ee\u9898\uff0c\u7cbe\u5ea6\u4ec5\u53d7\u4f20\u611f\u5668\u566a\u58f0\u9650\u5236\uff0c\u5e76\u63d0\u51fa\u4e86\u9002\u7528\u4e8e\u6c34\u4e0b\u73af\u5883\u7684\u4f20\u611f\u5668\u914d\u7f6e\u3002", "conclusion": "\u65b0\u63d0\u51fa\u7684\u5361\u5c14\u66fc\u6ee4\u6ce2\u7b97\u6cd5\u5728\u6d41\u5f62\u72b6\u6001\u4f30\u8ba1\u65b9\u9762\u5177\u6709\u4f18\u52bf\uff0c\u7279\u522b\u662f\u5728\u4f20\u611f\u5668\u7cbe\u5ea6\u8981\u6c42\u8f83\u9ad8\u6216\u9700\u8981\u5904\u7406\u975e\u7ebf\u6027\u8fd0\u52a8\u7684\u573a\u666f\u4e0b\uff0c\u5e76\u4e14\u53ef\u4ee5\u901a\u8fc7\u7279\u5b9a\u7684\u4f20\u611f\u5668\u7ec4\u5408\u6269\u5c55\u5230\u6c34\u4e0b\u7b49\u590d\u6742\u73af\u5883\u3002"}}
{"id": "2509.18292", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.18292", "abs": "https://arxiv.org/abs/2509.18292", "authors": ["Shuaiting Huang", "Haodong Jiang", "Chengcheng Zhao", "Peng Cheng", "Junfeng Wu"], "title": "Fully Distributed State Estimation for Multi-agent Systems and its Application in Cooperative Localization", "comment": null, "summary": "In this paper, we investigate the distributed state estimation problem for a\ncontinuous-time linear multi-agent system (MAS) composed of $\\mathit{m}$ agents\nand monitored by the agents themselves. To address this problem, we propose a\ndistributed observer that enables each agent to reconstruct the state of the\nMAS. The main idea is to let each agent $\\mathit{i}$ recover the state of agent\n$\\mathit{j}$ by using leader-follower consensus rules to track agent\n$\\mathit{j}$'s state estimate, which is generated by agent $\\mathit{j}$ itself\nusing a Luenberger-like estimation rule. Under the assumptions of node-level\nobservability and topological ordering consistency, we show that the estimation\nerror dynamics are stabilizable if and only if the communication graph is\nstrongly connected. Moreover, we discuss the fully distributed design of the\nproposed observer, assuming that the agents only know basic MAS configuration\ninformation, such as the homogeneity and the maximum number of allowable\nagents. This design ensures that the proposed observer functions correctly when\nagents are added or removed. Building on this, we consider cooperative\nlocalization as a distributed estimation problem and develop two fully\ndistributed localization algorithms that allow agents to track their own and\nother agents' positions (and velocities) within the MAS. Finally, we conduct\nsimulations to demonstrate the effectiveness of our proposed theoretical\nresults.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5e03\u5f0f\u4f30\u8ba1\u7b97\u6cd5\uff0c\u7528\u4e8e\u7ebf\u6027\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08MAS\uff09\uff0c\u5176\u4e2d\u6bcf\u4e2a\u667a\u80fd\u4f53\u90fd\u53ef\u4ee5\u4f30\u8ba1\u6574\u4e2a\u7cfb\u7edf\u7684\u72b6\u6001\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u9886\u5bfc-\u8ddf\u968f\u5171\u8bc6\u89c4\u5219\u548c Luenberger \u4f30\u8ba1\u5668\uff0c\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u8ddf\u8e2a\u5176\u4ed6\u667a\u80fd\u4f53\u7684\u72b6\u6001\u4f30\u8ba1\u3002\u7814\u7a76\u8868\u660e\uff0c\u53ea\u8981\u901a\u4fe1\u56fe\u662f\u5f3a\u8fde\u901a\u7684\uff0c\u4f30\u8ba1\u7b97\u6cd5\u5c31\u662f\u53ef\u7a33\u5b9a\u5316\u7684\u3002\u6b64\u5916\uff0c\u8be5\u7b97\u6cd5\u662f\u5b8c\u5168\u5206\u5e03\u5f0f\u7684\uff0c\u5e76\u4e14\u80fd\u591f\u9002\u5e94\u667a\u80fd\u4f53\u7684\u52a8\u6001\u6dfb\u52a0\u6216\u5220\u9664\u3002\u8be5\u65b9\u6cd5\u8fd8\u5e94\u7528\u4e8e\u5408\u4f5c\u5b9a\u4f4d\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u5206\u5e03\u5f0f\u5b9a\u4f4d\u7b97\u6cd5\u3002\u4eff\u771f\u7ed3\u679c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5e03\u5f0f\u4f30\u8ba1\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u8fde\u7eed\u65f6\u95f4\u7ebf\u6027\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08MAS\uff09\u7684\u72b6\u6001\u4f30\u8ba1\u95ee\u9898\uff0c\u4f7f\u6bcf\u4e2a\u667a\u80fd\u4f53\u80fd\u591f\u81ea\u4e3b\u5730\u91cd\u6784\u6574\u4e2a\u7cfb\u7edf\u7684\u72b6\u6001\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5e03\u5f0f\u89c2\u5bdf\u5668\uff0c\u5176\u4e2d\u6bcf\u4e2a\u667a\u80fd\u4f53 i \u901a\u8fc7\u5229\u7528\u9886\u5bfc-\u8ddf\u968f\u5171\u8bc6\u89c4\u5219\u6765\u8ddf\u8e2a\u667a\u80fd\u4f53 j \u7684\u72b6\u6001\u4f30\u8ba1\uff0c\u800c\u667a\u80fd\u4f53 j \u4f7f\u7528\u7c7b\u4f3c Luenberger \u7684\u4f30\u8ba1\u89c4\u5219\u6765\u751f\u6210\u81ea\u5df1\u7684\u72b6\u6001\u4f30\u8ba1\u3002", "result": "\u5728\u8282\u70b9\u7ea7\u53ef\u89c2\u6d4b\u6027\u548c\u62d3\u6251\u6392\u5e8f\u4e00\u81f4\u6027\u7684\u5047\u8bbe\u4e0b\uff0c\u8bc1\u660e\u4e86\u5f53\u4e14\u4ec5\u5f53\u901a\u4fe1\u56fe\u662f\u5f3a\u8fde\u901a\u7684\u65f6\uff0c\u4f30\u8ba1\u8bef\u5dee\u7684\u52a8\u6001\u662f\u53ef\u7a33\u5b9a\u5316\u7684\u3002\u6b64\u5916\uff0c\u8fd8\u8ba8\u8bba\u4e86\u8be5\u89c2\u5bdf\u5668\u7684\u5b8c\u5168\u5206\u5e03\u5f0f\u8bbe\u8ba1\uff0c\u5373\u4f7f\u5728\u667a\u80fd\u4f53\u6570\u91cf\u52a8\u6001\u53d8\u5316\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u786e\u4fdd\u5176\u6b63\u786e\u8fd0\u884c\u3002\u5e76\u5c06\u8be5\u65b9\u6cd5\u5e94\u7528\u4e8e\u5408\u4f5c\u5b9a\u4f4d\u95ee\u9898\uff0c\u5f00\u53d1\u4e86\u4e24\u79cd\u5b8c\u5168\u5206\u5e03\u5f0f\u7684\u5b9a\u4f4d\u7b97\u6cd5\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u5206\u5e03\u5f0f\u89c2\u5bdf\u5668\u5728\u5047\u8bbe\u901a\u4fe1\u56fe\u5f3a\u8fde\u901a\u7684\u60c5\u51b5\u4e0b\uff0c\u80fd\u591f\u6709\u6548\u5730\u4f30\u8ba1\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u72b6\u6001\uff0c\u5e76\u4e14\u8be5\u65b9\u6cd5\u5177\u6709\u826f\u597d\u7684\u9c81\u68d2\u6027\uff0c\u80fd\u591f\u9002\u5e94\u667a\u80fd\u4f53\u7684\u52a8\u6001\u589e\u51cf\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8e\u5408\u4f5c\u5b9a\u4f4d\u95ee\u9898\u3002"}}
{"id": "2509.18346", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.18346", "abs": "https://arxiv.org/abs/2509.18346", "authors": ["M Parimi", "Rachit Mehra", "S. R. Wagh", "Amol Yerudkar", "Navdeep Singh"], "title": "On the Dynamics of Acceleration in First order Gradient Methods", "comment": null, "summary": "Ever since the original algorithm by Nesterov (1983), the true nature of the\nacceleration phenomenon has remained elusive, with various interpretations of\nwhy the method is actually faster. The diagnosis of the algorithm through the\nlens of Ordinary Differential Equations (ODEs) and the corresponding dynamical\nsystem formulation to explain the underlying dynamics has a rich history. In\nthe literature, the ODEs that explain algorithms are typically derived by\nconsidering the limiting case of the algorithm maps themselves, that is, an ODE\nformulation follows the development of an algorithm. This obfuscates the\nunderlying higher order principles and thus provides little evidence of the\nworking of the algorithm. Such has been the case with Nesterov algorithm and\nthe various analogies used to describe the acceleration phenomena, viz,\nmomentum associated with the rolling of a Heavy-Ball down a slope, Hessian\ndamping etc. The main focus of our work is to ideate the genesis of the\nNesterov algorithm from the viewpoint of dynamical systems leading to\ndemystifying the mathematical rigour behind the algorithm. Instead of reverse\nengineering ODEs from discrete algorithms, this work explores tools from the\nrecently developed control paradigm titled Passivity and Immersion approach and\nthe Geometric Singular Perturbation theory which are applied to arrive at the\nformulation of a dynamical system that explains and models the acceleration\nphenomena. This perspective helps to gain insights into the various terms\npresent and the sequence of steps used in Nesterovs accelerated algorithm for\nthe smooth strongly convex and the convex case. The framework can also be\nextended to derive the acceleration achieved using the triple momentum method\nand provides justifications for the non-convergence to the optimal solution in\nthe Heavy-Ball method.", "AI": {"tldr": "Nesterov\u52a0\u901f\u7b97\u6cd5\u7684\u52a0\u901f\u73b0\u8c61\uff0c\u901a\u8fc7\u63a7\u5236\u7406\u8bba\u548c\u51e0\u4f55\u5947\u5f02\u6444\u52a8\u7406\u8bba\u7684\u52a8\u529b\u7cfb\u7edf\u89c6\u89d2\u6765\u63ed\u793a\u5176\u6570\u5b66\u539f\u7406\u3002", "motivation": "\u89e3\u91caNesterov\u52a0\u901f\u7b97\u6cd5\u7684\u52a0\u901f\u73b0\u8c61\u7684\u672c\u8d28\uff0c\u5e76\u63ed\u793a\u5176\u80cc\u540e\u7684\u6570\u5b66\u539f\u7406\u3002", "method": "\u5229\u7528\u63a7\u5236\u7406\u8bba\u4e2d\u7684Passivity and Immersion\u65b9\u6cd5\u548c\u51e0\u4f55\u5947\u5f02\u6444\u52a8\u7406\u8bba\u6765\u6784\u5efa\u89e3\u91ca\u52a0\u901f\u73b0\u8c61\u7684\u52a8\u529b\u7cfb\u7edf\u6a21\u578b\u3002", "result": "\u901a\u8fc7\u52a8\u529b\u7cfb\u7edf\u6a21\u578b\u89e3\u91ca\u4e86Nesterov\u52a0\u901f\u7b97\u6cd5\uff08\u5149\u6ed1\u5f3a\u51f8\u548c\u51f8\u60c5\u51b5\uff09\u7684\u52a0\u901f\u673a\u5236\uff0c\u5e76\u4e3a\u4e09\u91cd\u52a8\u91cf\u6cd5\u548c\u91cd\u7403\u6cd5\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\u3002", "conclusion": "\u8be5\u52a8\u529b\u5b66\u7cfb\u7edf\u6846\u67b6\u80fd\u591f\u89e3\u91caNesterov\u52a0\u901f\u7b97\u6cd5\u7684\u52a0\u901f\u673a\u5236\uff0c\u5e76\u4e3a\u76f8\u5173\u7b97\u6cd5\u63d0\u4f9b\u65b0\u7684\u89c6\u89d2\u548c\u7406\u8bba\u652f\u6301\u3002"}}
{"id": "2509.18121", "categories": ["cs.ET", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18121", "abs": "https://arxiv.org/abs/2509.18121", "authors": ["Nikhil Garg", "Paul Uriarte Vicandi", "Yanming Zhang", "Alexandre Baigol", "Donato Francesco Falcone", "Saketh Ram Mamidala", "Bert Jan Offrein", "Laura B\u00e9gon-Lours"], "title": "Energy-convergence trade off for the training of neural networks on bio-inspired hardware", "comment": null, "summary": "The increasing deployment of wearable sensors and implantable devices is\nshifting AI processing demands to the extreme edge, necessitating ultra-low\npower for continuous operation. Inspired by the brain, emerging memristive\ndevices promise to accelerate neural network training by eliminating costly\ndata transfers between compute and memory. Though, balancing performance and\nenergy efficiency remains a challenge. We investigate ferroelectric synaptic\ndevices based on HfO2/ZrO2 superlattices and feed their experimentally measured\nweight updates into hardware-aware neural network simulations. Across pulse\nwidths from 20 ns to 0.2 ms, shorter pulses lower per-update energy but require\nmore training epochs while still reducing total energy without sacrificing\naccuracy. Classification accuracy using plain stochastic gradient descent (SGD)\nis diminished compared to mixed-precision SGD. We analyze the causes and\npropose a ``symmetry point shifting'' technique, addressing asymmetric updates\nand restoring accuracy. These results highlight a trade-off among accuracy,\nconvergence speed, and energy use, showing that short-pulse programming with\ntailored training significantly enhances on-chip learning efficiency.", "AI": {"tldr": "\u57fa\u4e8eHfO2/ZrO2\u8d85\u6676\u683c\u7684\u94c1\u7535\u7a81\u89e6\u5668\u4ef6\u901a\u8fc7\u77ed\u8109\u51b2\u7f16\u7a0b\u548c\u5b9a\u5236\u8bad\u7ec3\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u7247\u4e0a\u5b66\u4e60\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u964d\u4f4e\u4e86\u80fd\u8017\u3002", "motivation": "\u5728\u6781\u7aef\u8fb9\u7f18\u90e8\u7f72AI\u5904\u7406\u9700\u8981\u8d85\u4f4e\u529f\u8017\uff0c\u94c1\u7535\u7a81\u89e6\u5668\u4ef6\u6709\u6f5c\u529b\u901a\u8fc7\u51cf\u5c11\u6570\u636e\u4f20\u8f93\u6765\u52a0\u901f\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\uff0c\u4f46\u9700\u8981\u5728\u6027\u80fd\u548c\u80fd\u6548\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "method": "\u7814\u7a76\u57fa\u4e8eHfO2/ZrO2\u8d85\u6676\u683c\u7684\u94c1\u7535\u7a81\u89e6\u5668\u4ef6\uff0c\u5e76\u5c06\u5b9e\u9a8c\u6d4b\u91cf\u7684\u6743\u91cd\u66f4\u65b0\u8f93\u5165\u5230\u786c\u4ef6\u611f\u77e5\u795e\u7ecf\u7f51\u7edc\u6a21\u62df\u4e2d\uff0c\u5206\u6790\u4e0d\u540c\u8109\u51b2\u5bbd\u5ea6\u4e0b\u7684\u80fd\u91cf\u548c\u51c6\u786e\u6027\uff0c\u5e76\u63d0\u51fa\u201c\u5bf9\u79f0\u70b9\u504f\u79fb\u201d\u6280\u672f\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u8f83\u77ed\u7684\u8109\u51b2\u5bbd\u5ea6\u53ef\u4ee5\u964d\u4f4e\u5355\u6b21\u66f4\u65b0\u7684\u80fd\u8017\uff0c\u4f46\u9700\u8981\u66f4\u591a\u7684\u8bad\u7ec3\u5468\u671f\uff0c\u4e0d\u8fc7\u603b\u80fd\u8017\u6709\u6240\u964d\u4f4e\u4e14\u51c6\u786e\u6027\u672a\u53d7\u5f71\u54cd\u3002\u4e0e\u6df7\u5408\u7cbe\u5ea6SGD\u76f8\u6bd4\uff0c\u666e\u901aSGD\u7684\u5206\u7c7b\u51c6\u786e\u6027\u6709\u6240\u4e0b\u964d\u3002\u63d0\u51fa\u7684\u201c\u5bf9\u79f0\u70b9\u504f\u79fb\u201d\u6280\u672f\u80fd\u591f\u6062\u590d\u51c6\u786e\u6027\u3002", "conclusion": "\u5728\u51c6\u786e\u6027\u3001\u6536\u655b\u901f\u5ea6\u548c\u80fd\u8017\u4e4b\u95f4\u5b58\u5728\u6743\u8861\u3002\u77ed\u8109\u51b2\u7f16\u7a0b\u7ed3\u5408\u5b9a\u5236\u8bad\u7ec3\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u7247\u4e0a\u5b66\u4e60\u7684\u6548\u7387\u3002"}}
{"id": "2509.18338", "categories": ["cs.GT", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.18338", "abs": "https://arxiv.org/abs/2509.18338", "authors": ["Tarun Chitra", "Paolo Penna", "Manvir Schneider"], "title": "On Sybil-proofness in Restaking Networks", "comment": null, "summary": "Restaking protocols expand validator responsibilities beyond consensus, but\ntheir security depends on resistance to Sybil attacks. We introduce a formal\nframework for Sybil-proofness in restaking networks, distinguishing between two\ntypes of attacks, one in which other Sybil identities are kept out of an attack\nand one where multiple Sybil identities attack. We analyze marginal and\nmultiplicative slashing mechanisms and characterize the conditions under which\neach deters Sybil strategies. We then prove an impossibility theorem: no\nslashing mechanism can simultaneously prevent both attack types. Finally, we\nstudy the impact of network structure through random graph models: while\nErd\\\"os-R\\'enyi networks remain Sybil-proof, even minimal heterogeneity in a\ntwo-block stochastic block model makes Sybil attacks profitable. These results\nreveal fundamental limits of mechanism design for restaking and highlight the\ncritical role of network topology.", "AI": {"tldr": "Restaking\u534f\u8bae\u7684Sybil\u653b\u51fb\u9632\u5fa1\u5b58\u5728\u6839\u672c\u6027\u9650\u5236\uff0c\u65e0\u6cd5\u540c\u65f6\u9632\u5fa1\u4e24\u79cd\u653b\u51fb\u7c7b\u578b\uff0c\u7f51\u7edc\u62d3\u6251\u7ed3\u6784\u81f3\u5173\u91cd\u8981\u3002", "motivation": "Restaking\u534f\u8bae\u6269\u5c55\u4e86\u9a8c\u8bc1\u8005\u7684\u804c\u8d23\uff0c\u4f46\u5176\u5b89\u5168\u6027\u4f9d\u8d56\u4e8e\u5bf9Sybil\u653b\u51fb\u7684\u62b5\u6297\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u5f62\u5f0f\u5316\u6846\u67b6\u6765\u8bc4\u4f30Restaking\u7f51\u7edc\u4e2d\u7684Sybil\u653b\u51fb\u9632\u5fa1\u80fd\u529b\uff0c\u533a\u5206\u4e24\u79cd\u653b\u51fb\u7c7b\u578b\uff0c\u5e76\u5206\u6790\u4e86\u8fb9\u9645\u548c\u4e58\u6cd5\u7f5a\u6b3e\u673a\u5236\u3002\u8bc1\u660e\u4e86\u4e00\u4e2a\u4e0d\u53ef\u80fd\u5b9a\u7406\uff0c\u5373\u6ca1\u6709\u7f5a\u6b3e\u673a\u5236\u80fd\u591f\u540c\u65f6\u963b\u6b62\u4e24\u79cd\u653b\u51fb\u7c7b\u578b\u3002\u6700\u540e\uff0c\u7814\u7a76\u4e86\u968f\u673a\u56fe\u6a21\u578b\u4e0b\u7684\u7f51\u7edc\u7ed3\u6784\u5bf9Sybil\u653b\u51fb\u7684\u5f71\u54cd\u3002", "result": "\u5206\u6790\u8868\u660e\uff0cErdos-Renyi\u7f51\u7edc\u4ecd\u7136\u662fSybil\u653b\u51fb\u7684\u9632\u5fa1\u6027\u7684\uff0c\u800c\u5177\u6709\u6700\u5c0f\u5f02\u8d28\u6027\u7684\u4e24\u5757\u968f\u673a\u5757\u6a21\u578b\u4f1a\u4f7fSybil\u653b\u51fb\u53d8\u5f97\u6709\u5229\u53ef\u56fe\u3002", "conclusion": "Restaking\u534f\u8bae\u7684Sybil\u653b\u51fb\u9632\u5fa1\u5b58\u5728\u6839\u672c\u6027\u9650\u5236\uff0c\u65e0\u6cd5\u540c\u65f6\u9632\u5fa1\u4e24\u79cd\u653b\u51fb\u7c7b\u578b\u3002\u7f51\u7edc\u62d3\u6251\u7ed3\u6784\u5728Sybil\u653b\u51fb\u9632\u5fa1\u4e2d\u8d77\u7740\u5173\u952e\u4f5c\u7528\u3002"}}
{"id": "2509.18295", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.18295", "abs": "https://arxiv.org/abs/2509.18295", "authors": ["Allen Boston", "Biruk Seyoum", "Luca Carloni", "Pierre-Emmanuel Gaillardon"], "title": "Lightweight Congruence Profiling for Early Design Exploration of Heterogeneous FPGAs", "comment": "This paper has been accepted for presentation at VLSI-SoC in October", "summary": "Field-Programmable Gate Arrays (FPGAs) have evolved from uniform logic arrays\ninto heterogeneous fabrics integrating digital signal processors (DSPs),\nmemories, and specialized accelerators to support emerging workloads such as\nmachine learning. While these enhancements improve power, performance, and area\n(PPA), they complicate design space exploration and application optimization\ndue to complex resource interactions.\n  To address these challenges, we propose a lightweight profiling methodology\ninspired by the Roofline model. It introduces three congruence scores that\nquickly identify bottlenecks related to heterogeneous resources, fabric, and\napplication logic. Evaluated on the Koios and VPR benchmark suites using a\nStratix 10 like FPGA, this approach enables efficient FPGA architecture\nco-design to improve heterogeneous FPGA performance.", "AI": {"tldr": "FPGA \u67b6\u6784\u53d8\u5f97\u590d\u6742\uff0c\u9700\u8981\u65b0\u7684\u5206\u6790\u65b9\u6cd5\u6765\u4f18\u5316\u8bbe\u8ba1\u3002", "motivation": "FPGA \u67b6\u6784\u96c6\u6210\u8d8a\u6765\u8d8a\u590d\u6742\uff0c\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u6765\u4f18\u5316\u8bbe\u8ba1\u548c\u5206\u6790\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e Roofline \u6a21\u578b\u7684\u8f7b\u91cf\u7ea7\u5206\u6790\u65b9\u6cd5\uff0c\u5f15\u5165\u4e86\u4e09\u4e2a\u4e00\u81f4\u6027\u5206\u6570\u6765\u8bc6\u522b\u5f02\u6784\u8d44\u6e90\u3001FPGA  fabric \u548c\u5e94\u7528\u903b\u8f91\u76f8\u5173\u7684\u74f6\u9888\u3002", "result": "\u5728 Koios \u548c VPR \u57fa\u51c6\u5957\u4ef6\u4e0a\u4f7f\u7528 Stratix 10 \u7c7b\u4f3c FPGA \u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5730\u8fdb\u884c\u5f02\u6784 FPGA \u7684\u67b6\u6784\u534f\u540c\u8bbe\u8ba1\uff0c\u4ee5\u63d0\u9ad8\u6027\u80fd\u3002"}}
{"id": "2509.18332", "categories": ["cond-mat.mes-hall", "cond-mat.dis-nn", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2509.18332", "abs": "https://arxiv.org/abs/2509.18332", "authors": ["L. E. Golub"], "title": "Interplay of Rashba and valley-Zeeman splittings in weak localization of spin-orbit coupled graphene", "comment": "5+3 pages, 2 figures", "summary": "Weak localization theory is developed for graphene heterostructures with\ntransition metal dichalcogenides and topological insulators where the Rashba\nand valey-Zeeman spin-splittings of the energy spectrum are large enough. The\nanomalous magnetoresistance in low fields caused by weak localization is\ncalculated. It is shown that the valley-Zeeman splitting has no effect on weak\nlocalization in the absence of Rashba splitting but it results in the change of\nthe magnetoconductivity sign in the Rashba-coupled graphene. Inter-valley\nscattering also affects the quantum correction to the conductivity resulting in\nits sign reversal. Analytical expressions are obtained for the anomalous\nmagnetoconductivity at arbitrary relations between the Rashba and valley-Zeeman\nsplittings as well as the inter-valley scattering rates.", "AI": {"tldr": "\u5bf9\u4e8e\u5177\u6709\u5927Rashba\u548c\u80fd\u8c37-Zeeman\u81ea\u65cb\u5206\u88c2\u7684\u77f3\u58a8\u70ef\u5f02\u8d28\u7ed3\u6784\uff0c\u53d1\u5c55\u4e86\u5f31\u5c40\u57df\u5316\u7406\u8bba\uff0c\u5e76\u8ba1\u7b97\u4e86\u4f4e\u573a\u4e0b\u7684\u53cd\u5e38\u78c1\u963b\u3002", "motivation": "\u7814\u7a76\u5177\u6709\u5927Rashba\u548c\u80fd\u8c37-Zeeman\u81ea\u65cb\u5206\u88c2\u7684\u77f3\u58a8\u70ef\u5f02\u8d28\u7ed3\u6784\u4e2d\u7684\u5f31\u5c40\u57df\u5316\u7406\u8bba\u53ca\u5176\u5bf9\u78c1\u963b\u7684\u5f71\u54cd\u3002", "method": "\u53d1\u5c55\u5f31\u5c40\u57df\u5316\u7406\u8bba\uff0c\u8ba1\u7b97\u53cd\u5e38\u78c1\u963b\uff0c\u5f97\u5230\u4efb\u610f\u53c2\u6570\u5173\u7cfb\u4e0b\u7684\u89e3\u6790\u8868\u8fbe\u5f0f\u3002", "result": "\u53d1\u73b0\u80fd\u8c37-Zeeman\u5206\u88c2\u5728\u65e0Rashba\u5206\u88c2\u65f6\u5bf9\u5f31\u5c40\u57df\u5316\u65e0\u5f71\u54cd\uff0c\u4f46\u5728\u5b58\u5728Rashba\u8026\u5408\u65f6\u4f1a\u6539\u53d8\u78c1\u7535\u5bfc\u7b26\u53f7\u3002 the valley-Zeeman splitting has no effect on weak localization in the absence of Rashba splitting but it results in the change of the magnetoconductivity sign in the Rashba-coupled graphene. \u8c37\u95f4\u6563\u5c04\u4e5f\u4f1a\u5bfc\u81f4\u91cf\u5b50\u4fee\u6b63\u7b26\u53f7\u53cd\u8f6c\u3002", "conclusion": "\u80fd\u8c37-Zeeman\u5206\u88c2\u548c\u8c37\u95f4\u6563\u5c04\u4f1a\u5f71\u54cd\u77f3\u58a8\u70ef\u5f02\u8d28\u7ed3\u6784\u4e2d\u7684\u5f31\u5c40\u57df\u5316\u548c\u78c1\u7535\u5bfc\u7b26\u53f7\uff0c\u53ef\u4ee5\u901a\u8fc7\u7406\u8bba\u8ba1\u7b97\u5f97\u5230\u4efb\u610f\u53c2\u6570\u4e0b\u7684\u89e3\u6790\u8868\u8fbe\u5f0f\u3002"}}
{"id": "2509.18452", "categories": ["cs.LG", "cs.NA", "math.NA", "stat.ML", "D.2.0; G.4; B.8.2"], "pdf": "https://arxiv.org/pdf/2509.18452", "abs": "https://arxiv.org/abs/2509.18452", "authors": ["Anton Lebedev", "Won Kyung Lee", "Soumyadip Ghosh", "Olha I. Yaman", "Vassilis Kalantzis", "Yingdong Lu", "Tomasz Nowicki", "Shashanka Ubaru", "Lior Horesh", "Vassil Alexandrov"], "title": "Fast Linear Solvers via AI-Tuned Markov Chain Monte Carlo-based Matrix Inversion", "comment": "8 pages, 3 figures, 1 algorithm, 1 table of experiment cases", "summary": "Large, sparse linear systems are pervasive in modern science and engineering,\nand Krylov subspace solvers are an established means of solving them. Yet\nconvergence can be slow for ill-conditioned matrices, so practical deployments\nusually require preconditioners. Markov chain Monte Carlo (MCMC)-based matrix\ninversion can generate such preconditioners and accelerate Krylov iterations,\nbut its effectiveness depends on parameters whose optima vary across matrices;\nmanual or grid search is costly. We present an AI-driven framework recommending\nMCMC parameters for a given linear system. A graph neural surrogate predicts\npreconditioning speed from $A$ and MCMC parameters. A Bayesian acquisition\nfunction then chooses the parameter sets most likely to minimise iterations. On\na previously unseen ill-conditioned system, the framework achieves better\npreconditioning with 50\\% of the search budget of conventional methods,\nyielding about a 10\\% reduction in iterations to convergence. These results\nsuggest a route for incorporating MCMC-based preconditioners into large-scale\nsystems.", "AI": {"tldr": "AI\u6846\u67b6\u901a\u8fc7\u56fe\u795e\u7ecf\u7f51\u7edc\u548c\u8d1d\u53f6\u65af\u4f18\u5316\u6765\u9009\u62e9\u9a6c\u5c14\u53ef\u592b\u94fe\u8499\u7279\u5361\u6d1b\uff08MCMC\uff09\u53c2\u6570\uff0c\u4ee5\u52a0\u901f\u6c42\u89e3\u5927\u578b\u7a00\u758f\u7ebf\u6027\u7cfb\u7edf\uff0c\u5e76\u5728\u641c\u7d22\u9884\u7b97\u51cf\u5c1150%\u7684\u60c5\u51b5\u4e0b\u63d0\u9ad8\u4e86\u9884\u5904\u7406\u6548\u679c\uff0c\u51cf\u5c11\u4e8610%\u7684\u6536\u655b\u8fed\u4ee3\u6b21\u6570\u3002", "motivation": "\u73b0\u6709\u7684Krylov\u5b50\u7a7a\u95f4\u6c42\u89e3\u5668\u5728\u5904\u7406\u75c5\u6001\u77e9\u9635\u65f6\u6536\u655b\u901f\u5ea6\u6162\uff0c\u9700\u8981\u9884\u5904\u7406\u3002\u57fa\u4e8e\u9a6c\u5c14\u53ef\u592b\u94fe\u8499\u7279\u5361\u6d1b\uff08MCMC\uff09\u7684\u77e9\u9635\u6c42\u9006\u53ef\u4ee5\u751f\u6210\u9884\u5904\u7406\u5668\uff0c\u4f46\u5176\u6709\u6548\u6027\u4f9d\u8d56\u4e8e\u53c2\u6570\uff0c\u624b\u52a8\u8c03\u6574\u6210\u672c\u9ad8\u3002", "method": "\u63d0\u51fa\u4e00\u4e2aAI\u9a71\u52a8\u7684\u6846\u67b6\uff0c\u4f7f\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u4f5c\u4e3a\u4ee3\u7406\u6a21\u578b\u6765\u9884\u6d4bMCMC\u53c2\u6570\u5bf9\u9884\u5904\u7406\u901f\u5ea6\u7684\u5f71\u54cd\uff0c\u5e76\u7ed3\u5408\u8d1d\u53f6\u65af\u83b7\u53d6\u51fd\u6570\u6765\u9009\u62e9\u6700\u6709\u53ef\u80fd\u51cf\u5c11\u8fed\u4ee3\u6b21\u6570\u7684\u53c2\u6570\u96c6\u3002", "result": "\u5728\u5148\u524d\u672a\u89c1\u8fc7\u7684\u75c5\u6001\u7ebf\u6027\u7cfb\u7edf\u4e0a\uff0c\u8be5\u6846\u67b6\u5728\u641c\u7d22\u9884\u7b97\u51cf\u5c1150%\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86\u6bd4\u4f20\u7edf\u65b9\u6cd5\u66f4\u597d\u7684\u9884\u5904\u7406\u6548\u679c\uff0c\u6536\u655b\u8fed\u4ee3\u6b21\u6570\u51cf\u5c11\u4e86\u7ea610%\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5c06MCMC\u9884\u5904\u7406\u65b9\u6cd5\u96c6\u6210\u5230\u5927\u89c4\u6a21\u7cfb\u7edf\u4e2d\u63d0\u4f9b\u4e86\u4e00\u6761\u9014\u5f84\u3002"}}
{"id": "2509.18299", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2509.18299", "abs": "https://arxiv.org/abs/2509.18299", "authors": ["Jhionathan de Lima", "Cristiano Francisco Woellner"], "title": "Cyclo-Graphyne: A Highly Porous and Semimetallic 2D Carbon Allotrope with Dirac Cones", "comment": null, "summary": "We present a comprehensive characterization of Cyclo-graphyne (CGY), an\nemerging 2D carbon allotrope with a porous structure of sp/sp$^2$-hybridized\ncarbon atoms. Using density functional theory, we systematically investigate\nits structural, energetic, dynamical, thermal, electronic, mechanical, optical,\nand vibrational properties. The calculated cohesive and formation energies are\nboth comparable to those of other synthesized graphynes, confirming its\nenergetic viability. Phonon dispersion calculations confirm its dynamical\nstability, while ab initio molecular dynamics simulations indicate thermal\nstability up to at least 1000 K. Electronic results reveal that CGY is a\nsemimetal with an ultranarrow band gap and features two Dirac cones in its\nelectronic structure. Mechanically, CGY is highly compliant and isotropic,\nexhibiting a Young's modulus an order of magnitude lower than that of graphene.\nThe optical spectrum reveals strong ultraviolet absorption and infrared\nreflectivity with an isotropic response, while the vibrational spectra show\ndistinct Raman peaks and rich infrared activity. These properties position CGY\nas a promising candidate for future applications in areas such as gas capture\nand separation, flexible nanoelectronics, and optoelectronics.", "AI": {"tldr": "Cyclo-graphyne (CGY)\u662f\u4e00\u79cd\u5177\u6709\u591a\u5b54\u7ed3\u6784\u76842D\u78b3\u540c\u7d20\u5f02\u5f62\u4f53\uff0c\u5177\u6709sp/sp^2\u6742\u5316\u7684\u78b3\u539f\u5b50\u3002\u7814\u7a76\u7cfb\u7edf\u5730\u7814\u7a76\u4e86\u5176\u7ed3\u6784\u3001\u80fd\u91cf\u3001\u52a8\u529b\u5b66\u3001\u70ed\u5b66\u3001\u7535\u5b50\u3001\u673a\u68b0\u3001\u5149\u5b66\u548c\u632f\u52a8\u7279\u6027\u3002\u7ed3\u679c\u8868\u660eCGY\u5728\u80fd\u91cf\u4e0a\u662f\u53ef\u884c\u7684\uff0c\u52a8\u529b\u5b66\u548c\u70ed\u5b66\u4e0a\u662f\u7a33\u5b9a\u7684\uff08\u9ad8\u8fbe1000K\uff09\u3002\u5b83\u662f\u4e00\u79cd\u5177\u6709\u8d85\u7a84\u5e26\u9699\u548c\u4e24\u4e2a\u72c4\u62c9\u514b\u9525\u7684\u534a\u91d1\u5c5e\u3002CGY\u5177\u6709\u9ad8\u5ea6\u7684\u987a\u5e94\u6027\u548c\u5404\u5411\u540c\u6027\uff0c\u6768\u6c0f\u6a21\u91cf\u6bd4\u77f3\u58a8\u70ef\u4f4e\u4e00\u4e2a\u6570\u91cf\u7ea7\u3002\u5149\u5b66\u5149\u8c31\u663e\u793a\u51fa\u5f3a\u70c8\u7684\u7d2b\u5916\u5438\u6536\u548c\u7ea2\u5916\u53cd\u5c04\uff0c\u632f\u52a8\u5149\u8c31\u5219\u663e\u793a\u51fa\u72ec\u7279\u7684\u62c9\u66fc\u5cf0\u548c\u4e30\u5bcc\u7684\u7ea2\u5916\u6d3b\u6027\u3002\u8fd9\u4e9b\u7279\u6027\u4f7fCGY\u5728\u6c14\u4f53\u6355\u83b7\u548c\u5206\u79bb\u3001\u67d4\u6027\u7eb3\u7c73\u7535\u5b50\u5b66\u548c\u5149\u7535\u5b50\u5b66\u7b49\u9886\u57df\u5177\u6709\u5e94\u7528\u524d\u666f\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u5168\u9762\u8868\u5f81\u65b0\u5174\u76842D\u78b3\u540c\u7d20\u5f02\u5f62\u4f53\u2014\u2014\u73af\u72b6\u7094\u70c3\uff08CGY\uff09\uff0c\u5e76\u7cfb\u7edf\u7814\u7a76\u5176\u5404\u79cd\u7269\u7406\u5316\u5b66\u6027\u8d28\uff0c\u4ee5\u8bc4\u4f30\u5176\u4f5c\u4e3a\u6f5c\u5728\u5e94\u7528\u6750\u6599\u7684\u53ef\u884c\u6027\u3002", "method": "\u5229\u7528\u5bc6\u5ea6\u6cdb\u51fd\u7406\u8bba\uff08DFT\uff09\u7cfb\u7edf\u7814\u7a76\u4e86CGY\u7684\u7ed3\u6784\u3001\u80fd\u91cf\u3001\u52a8\u529b\u5b66\u3001\u70ed\u5b66\u3001\u7535\u5b50\u3001\u673a\u68b0\u3001\u5149\u5b66\u548c\u632f\u52a8\u7279\u6027\uff0c\u5e76\u901a\u8fc7\u4ece\u5934\u7b97\u5206\u5b50\u52a8\u529b\u5b66\u6a21\u62df\u8fdb\u884c\u70ed\u7a33\u5b9a\u6027\u5206\u6790\u3002", "result": "\u8ba1\u7b97\u5f97\u5230\u7684\u5185\u805a\u80fd\u548c\u5f62\u6210\u80fd\u4e0e\u5df2\u5408\u6210\u7684\u77f3\u58a8\u7094\u76f8\u5f53\uff0c\u8bc1\u5b9e\u4e86\u5176\u80fd\u91cf\u4e0a\u7684\u53ef\u884c\u6027\u3002\u58f0\u5b50\u8272\u6563\u8ba1\u7b97\u8bc1\u5b9e\u4e86\u5176\u52a8\u529b\u5b66\u7a33\u5b9a\u6027\uff0c\u800c\u4ece\u5934\u7b97\u5206\u5b50\u52a8\u529b\u5b66\u6a21\u62df\u8868\u660e\u5176\u5728\u81f3\u5c111000 K\u7684\u6e29\u5ea6\u4e0b\u5177\u6709\u70ed\u7a33\u5b9a\u6027\u3002\u7535\u5b50\u6027\u8d28\u8868\u660eCGY\u662f\u4e00\u79cd\u5177\u6709\u8d85\u7a84\u5e26\u9699\u548c\u4e24\u4e2a\u72c4\u62c9\u514b\u9525\u7684\u534a\u91d1\u5c5e\u3002\u673a\u68b0\u6027\u80fd\u663e\u793aCGY\u5177\u6709\u9ad8\u5ea6\u7684\u987a\u5e94\u6027\u548c\u5404\u5411\u540c\u6027\uff0c\u6768\u6c0f\u6a21\u91cf\u6bd4\u77f3\u58a8\u70ef\u4f4e\u4e00\u4e2a\u6570\u91cf\u7ea7\u3002\u5149\u5b66\u5149\u8c31\u663e\u793a\u51fa\u5f3a\u70c8\u7684\u7d2b\u5916\u5438\u6536\u548c\u7ea2\u5916\u53cd\u5c04\uff0c\u4ee5\u53ca\u5404\u5411\u540c\u6027\u7684\u54cd\u5e94\u3002\u632f\u52a8\u5149\u8c31\u5219\u663e\u793a\u51fa\u72ec\u7279\u7684\u62c9\u66fc\u5cf0\u548c\u4e30\u5bcc\u7684\u7ea2\u5916\u6d3b\u6027\u3002", "conclusion": "CGY\u662f\u4e00\u79cd\u5177\u6709\u4f18\u5f02\u7269\u7406\u5316\u5b66\u7279\u6027\u76842D\u78b3\u540c\u7d20\u5f02\u5f62\u4f53\uff0c\u5305\u62ec\u80fd\u91cf\u53ef\u884c\u6027\u3001\u52a8\u529b\u5b66\u548c\u70ed\u5b66\u7a33\u5b9a\u6027\u3001\u534a\u91d1\u5c5e\u7279\u6027\u3001\u9ad8\u987a\u5e94\u6027\u548c\u5404\u5411\u540c\u6027\u4ee5\u53ca\u72ec\u7279\u7684\u5149\u5b66\u548c\u632f\u52a8\u54cd\u5e94\u3002\u8fd9\u4e9b\u7279\u6027\u4f7f\u5176\u5728\u6c14\u4f53\u6355\u83b7\u4e0e\u5206\u79bb\u3001\u67d4\u6027\u7eb3\u7c73\u7535\u5b50\u5b66\u548c\u5149\u7535\u5b50\u5b66\u7b49\u9886\u57df\u5177\u6709\u91cd\u8981\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.18289", "categories": ["cs.SI"], "pdf": "https://arxiv.org/pdf/2509.18289", "abs": "https://arxiv.org/abs/2509.18289", "authors": ["Akrati Saxena", "Gaurav Kumar", "Chandrakala Meena"], "title": "Homophily in Complex Networks: Measures, Models, and Applications", "comment": null, "summary": "Homophily, the tendency of individuals to connect with others who share\nsimilar attributes, is a defining feature of social networks. Understanding how\ngroups interact, both within and across, is crucial for uncovering the dynamics\nof network evolution and the emergence of structural inequalities in these\nnetwork. This tutorial offers a comprehensive overview of homophily, covering\nits various definitions, key properties, and the limitations of widely used\nmetrics. Extending beyond traditional pairwise interactions, we will discuss\nhomophily in higher-order network structures such as hypergraphs and simplicial\ncomplexes. We will further discuss network generating models capable of\nproducing different types of homophilic networks with tunable levels of\nhomophily and highlight their relevance in real-world contexts. The tutorial\nconcludes with a discussion of open challenges, emerging directions, and\nopportunities for further research in this area.", "AI": {"tldr": "\u672c\u6559\u7a0b\u5168\u9762\u6982\u8ff0\u4e86\u540c\u8d28\u6027\uff0c\u5305\u62ec\u5176\u5b9a\u4e49\u3001\u6027\u8d28\u3001\u5ea6\u91cf\u7684\u5c40\u9650\u6027\u3001\u9ad8\u9636\u7f51\u7edc\u7ed3\u6784\u4e2d\u7684\u540c\u8d28\u6027\u3001\u7f51\u7edc\u751f\u6210\u6a21\u578b\u4ee5\u53ca\u5b9e\u9645\u5e94\u7528\u3002\u6700\u540e\u8ba8\u8bba\u4e86\u8be5\u9886\u57df\u7684\u5f00\u653e\u6027\u6311\u6218\u3001\u65b0\u5174\u65b9\u5411\u548c\u7814\u7a76\u673a\u4f1a\u3002", "motivation": "\u7406\u89e3\u7fa4\u4f53\u5185\u90e8\u548c\u8de8\u7fa4\u4f53\u4e92\u52a8\u5bf9\u4e8e\u63ed\u793a\u7f51\u7edc\u6f14\u53d8\u52a8\u6001\u548c\u7ed3\u6784\u4e0d\u5e73\u7b49\u7684\u51fa\u73b0\u81f3\u5173\u91cd\u8981\u3002", "method": "\u672c\u6559\u7a0b\u5c06\u8ba8\u8bba\u540c\u8d28\u6027\u7684\u4e0d\u540c\u5b9a\u4e49\u3001\u5173\u952e\u6027\u8d28\u3001\u5e38\u7528\u5ea6\u91cf\u7684\u5c40\u9650\u6027\uff0c\u5e76\u6269\u5c55\u5230\u8d85\u56fe\u548c\u5355\u7eaf\u590d\u5f62\u7b49\u9ad8\u9636\u7f51\u7edc\u7ed3\u6784\u3002\u6b64\u5916\uff0c\u8fd8\u5c06\u8ba8\u8bba\u80fd\u591f\u751f\u6210\u5177\u6709\u53ef\u8c03\u540c\u8d28\u6027\u6c34\u5e73\u7684\u4e0d\u540c\u7c7b\u578b\u540c\u8d28\u6027\u7f51\u7edc\u4ee5\u53ca\u5176\u5b9e\u9645\u5e94\u7528\u7684\u7f51\u7edc\u751f\u6210\u6a21\u578b\u3002", "result": "\u672c\u6559\u7a0b\u63d0\u4f9b\u4e86\u5173\u4e8e\u540c\u8d28\u6027\u7684\u5168\u9762\u6982\u8ff0\uff0c\u5305\u62ec\u5176\u5b9a\u4e49\u3001\u6027\u8d28\u3001\u5ea6\u91cf\u5c40\u9650\u6027\u3001\u9ad8\u9636\u7f51\u7edc\u7ed3\u6784\u3001\u7f51\u7edc\u751f\u6210\u6a21\u578b\u548c\u5b9e\u9645\u5e94\u7528\u3002", "conclusion": "\u672c\u6559\u7a0b\u6700\u540e\u8ba8\u8bba\u4e86\u540c\u8d28\u6027\u7814\u7a76\u9886\u57df\u7684\u5f00\u653e\u6027\u6311\u6218\u3001\u65b0\u5174\u65b9\u5411\u548c\u672a\u6765\u7814\u7a76\u673a\u4f1a\u3002"}}
{"id": "2509.18310", "categories": ["eess.SP", "cs.LG", "stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2509.18310", "abs": "https://arxiv.org/abs/2509.18310", "authors": ["Bahar Kor", "Bipin Gaikwad", "Abani Patra", "Eric L. Miller"], "title": "On Multi-entity, Multivariate Quickest Change Point Detection", "comment": null, "summary": "We propose a framework for online Change Point Detection (CPD) from\nmulti-entity, multivariate time series data, motivated by applications in crowd\nmonitoring where traditional sensing methods (e.g., video surveillance) may be\ninfeasible. Our approach addresses the challenge of detecting system-wide\nbehavioral shifts in complex, dynamic environments where the number and\nbehavior of individual entities may be uncertain or evolve. We introduce the\nconcept of Individual Deviation from Normality (IDfN), computed via a\nreconstruction-error-based autoencoder trained on normal behavior. We aggregate\nthese individual deviations using mean, variance, and Kernel Density Estimates\n(KDE) to yield a System-Wide Anomaly Score (SWAS). To detect persistent or\nabrupt changes, we apply statistical deviation metrics and the Cumulative Sum\n(CUSUM) technique to these scores. Our unsupervised approach eliminates the\nneed for labeled data or feature extraction, enabling real-time operation on\nstreaming input. Evaluations on both synthetic datasets and crowd simulations,\nexplicitly designed for anomaly detection in group behaviors, demonstrate that\nour method accurately detects significant system-level changes, offering a\nscalable and privacy-preserving solution for monitoring complex multi-agent\nsystems. In addition to this methodological contribution, we introduce new,\nchallenging multi-entity multivariate time series datasets generated from crowd\nsimulations in Unity and coupled nonlinear oscillators. To the best of our\nknowledge, there is currently no publicly available dataset of this type\ndesigned explicitly to evaluate CPD in complex collective and interactive\nsystems, highlighting an essential gap that our work addresses.", "AI": {"tldr": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u5728\u7ebf\u53d8\u66f4\u70b9\u68c0\u6d4b\uff08CPD\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u5904\u7406\u591a\u5b9e\u4f53\u3001\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0c\u89e3\u51b3\u4e86\u5728\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u68c0\u6d4b\u7cfb\u7edf\u884c\u4e3a\u53d8\u5316\u7684\u6311\u6218\u3002", "motivation": "\u5728\u4f20\u7edf\u4f20\u611f\u65b9\u6cd5\uff08\u5982\u89c6\u9891\u76d1\u63a7\uff09\u4e0d\u53ef\u884c\u7684\u60c5\u51b5\u4e0b\uff0c\u4f8b\u5982\u5728\u4eba\u7fa4\u76d1\u63a7\u5e94\u7528\u4e2d\uff0c\u68c0\u6d4b\u7cfb\u7edf\u8303\u56f4\u7684\u884c\u4e3a\u8f6c\u53d8\u3002", "method": "\u901a\u8fc7\u57fa\u4e8e\u91cd\u5efa\u8bef\u5dee\u7684\u81ea\u7f16\u7801\u5668\u8ba1\u7b97\u4e2a\u4f53\u504f\u79bb\u5e38\u6001\uff08IDfN\uff09\uff0c\u5e76\u805a\u5408\u4e2a\u4f53\u504f\u79bb\u4ee5\u83b7\u5f97\u7cfb\u7edf\u8303\u56f4\u5f02\u5e38\u5f97\u5206\uff08SWAS\uff09\u3002\u5bf9SWAS\u5e94\u7528\u7edf\u8ba1\u504f\u5dee\u6307\u6807\u548c\u7d2f\u79ef\u548c\uff08CUSUM\uff09\u6280\u672f\u6765\u68c0\u6d4b\u53d8\u5316\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u96c6\u548c\u4e13\u95e8\u4e3a\u7fa4\u4f53\u884c\u4e3a\u5f02\u5e38\u68c0\u6d4b\u8bbe\u8ba1\u7684\u4eba\u7fa4\u6a21\u62df\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u8bc1\u660e\u8be5\u65b9\u6cd5\u80fd\u591f\u51c6\u786e\u68c0\u6d4b\u91cd\u8981\u7684\u7cfb\u7edf\u7ea7\u53d8\u5316\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u76d1\u63a7\u590d\u6742\u7684\u3001\u591a\u667a\u80fd\u4f53\u7684\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u3001\u6ce8\u91cd\u9690\u79c1\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u4e14\u6211\u4eec\u5f15\u5165\u4e86\u65b0\u7684\u3001\u5177\u6709\u6311\u6218\u6027\u7684\u591a\u5b9e\u4f53\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u96c6\u6765\u8bc4\u4f30CPD\u3002"}}
{"id": "2509.18282", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18282", "abs": "https://arxiv.org/abs/2509.18282", "authors": ["Jesse Zhang", "Marius Memmel", "Kevin Kim", "Dieter Fox", "Jesse Thomason", "Fabio Ramos", "Erdem B\u0131y\u0131k", "Abhishek Gupta", "Anqi Li"], "title": "PEEK: Guiding and Minimal Image Representations for Zero-Shot Generalization of Robot Manipulation Policies", "comment": "11 pages", "summary": "Robotic manipulation policies often fail to generalize because they must\nsimultaneously learn where to attend, what actions to take, and how to execute\nthem. We argue that high-level reasoning about where and what can be offloaded\nto vision-language models (VLMs), leaving policies to specialize in how to act.\nWe present PEEK (Policy-agnostic Extraction of Essential Keypoints), which\nfine-tunes VLMs to predict a unified point-based intermediate representation:\n1. end-effector paths specifying what actions to take, and 2. task-relevant\nmasks indicating where to focus. These annotations are directly overlaid onto\nrobot observations, making the representation policy-agnostic and transferable\nacross architectures. To enable scalable training, we introduce an automatic\nannotation pipeline, generating labeled data across 20+ robot datasets spanning\n9 embodiments. In real-world evaluations, PEEK consistently boosts zero-shot\ngeneralization, including a 41.4x real-world improvement for a 3D policy\ntrained only in simulation, and 2-3.5x gains for both large VLAs and small\nmanipulation policies. By letting VLMs absorb semantic and visual complexity,\nPEEK equips manipulation policies with the minimal cues they need--where, what,\nand how. Website at https://peek-robot.github.io/.", "AI": {"tldr": "PEEK\u901a\u8fc7\u5fae\u8c03\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u6765\u63d0\u53d6\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u5173\u952e\u70b9\uff0c\u4ece\u800c\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\uff0c\u5c06\u2018\u4f55\u5904\u2019\u548c\u2018\u4f55\u4e8b\u2019\u7684\u63a8\u7406\u4ea4\u7ed9VLM\uff0c\u8ba9\u7b56\u7565\u4e13\u6ce8\u4e8e\u2018\u5982\u4f55\u884c\u52a8\u2019\u3002", "motivation": "\u73b0\u6709\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u56e0\u4e3a\u5b83\u4eec\u9700\u8981\u540c\u65f6\u5b66\u4e60\u5173\u6ce8\u70b9\u3001\u91c7\u53d6\u7684\u884c\u52a8\u4ee5\u53ca\u5982\u4f55\u6267\u884c\u3002", "method": "\u63d0\u51faPEEK\uff08Policy-agnostic Extraction of Essential Keypoints\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u5fae\u8c03VLM\u6765\u9884\u6d4b\u7edf\u4e00\u7684\u70b9\u72b6\u4e2d\u95f4\u8868\u793a\uff1a1. \u6307\u5b9a\u884c\u52a8\u7684\u672b\u7aef\u6267\u884c\u5668\u8def\u5f84\uff1b2. \u6307\u793a\u5173\u6ce8\u70b9\u7684\u4efb\u52a1\u76f8\u5173\u63a9\u7801\u3002\u8be5\u8868\u793a\u53ef\u76f4\u63a5\u53e0\u52a0\u5728\u673a\u5668\u4eba\u89c2\u6d4b\u4e0a\uff0c\u5b9e\u73b0\u7b56\u7565\u65e0\u5173\u6027\u548c\u8de8\u67b6\u6784\u53ef\u8f6c\u79fb\u6027\u3002\u4e3a\u5b9e\u73b0\u53ef\u6269\u5c55\u8bad\u7ec3\uff0c\u5f15\u5165\u4e86\u81ea\u52a8\u6807\u6ce8\u6d41\u7a0b\uff0c\u6db5\u76d6\u4e8620\u591a\u4e2a\u8de89\u79cd\u5177\u8eab\u673a\u5668\u4eba\u7684\u6570\u636e\u96c6\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u8bc4\u4f30\u4e2d\uff0cPEEK\u4e00\u81f4\u6027\u5730\u63d0\u5347\u4e86\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u5305\u62ec\u5728\u4ec5\u6a21\u62df\u8bad\u7ec3\u76843D\u7b56\u7565\u4e0a\u5b9e\u73b0\u4e8641.4\u500d\u7684\u771f\u5b9e\u4e16\u754c\u6027\u80fd\u63d0\u5347\uff0c\u4ee5\u53ca\u5728\u5927\u578bVLA\u548c\u5c0f\u578b\u64cd\u4f5c\u7b56\u7565\u4e0a\u5b9e\u73b0\u4e862-3.5\u500d\u7684\u6536\u76ca\u3002", "conclusion": "\u901a\u8fc7\u8ba9VLM\u5438\u6536\u8bed\u4e49\u548c\u89c6\u89c9\u590d\u6742\u6027\uff0cPEEK\u4e3a\u64cd\u4f5c\u7b56\u7565\u63d0\u4f9b\u4e86\u5fc5\u9700\u7684\u6700\u5c11\u7ebf\u7d22\u2014\u2014\u2018\u4f55\u5904\u2019\u3001\u2018\u4f55\u4e8b\u2019\u548c\u2018\u5982\u4f55\u2019\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2509.18159", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18159", "abs": "https://arxiv.org/abs/2509.18159", "authors": ["Akwasi Asare", "Ulas Bagci"], "title": "PolypSeg-GradCAM: Towards Explainable Computer-Aided Gastrointestinal Disease Detection Using U-Net Based Segmentation and Grad-CAM Visualization on the Kvasir Dataset", "comment": null, "summary": "Colorectal cancer (CRC) remains one of the leading causes of cancer-related\nmorbidity and mortality worldwide, with gastrointestinal (GI) polyps serving as\ncritical precursors according to the World Health Organization (WHO). Early and\naccurate segmentation of polyps during colonoscopy is essential for reducing\nCRC progression, yet manual delineation is labor-intensive and prone to\nobserver variability. Deep learning methods have demonstrated strong potential\nfor automated polyp analysis, but their limited interpretability remains a\nbarrier to clinical adoption. In this study, we present PolypSeg-GradCAM, an\nexplainable deep learning framework that integrates the U-Net architecture with\nGradient-weighted Class Activation Mapping (Grad-CAM) for transparent polyp\nsegmentation. The model was trained and evaluated on the Kvasir-SEG dataset of\n1000 annotated endoscopic images. Experimental results demonstrate robust\nsegmentation performance, achieving a mean Intersection over Union (IoU) of\n0.9257 on the test set and consistently high Dice coefficients (F-score > 0.96)\non training and validation sets. Grad-CAM visualizations further confirmed that\npredictions were guided by clinically relevant regions, enhancing transparency\nand trust in the model's decisions. By coupling high segmentation accuracy with\ninterpretability, PolypSeg-GradCAM represents a step toward reliable,\ntrustworthy AI-assisted colonoscopy and improved early colorectal cancer\nprevention.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aPolypSeg-GradCAM\u7684\u53ef\u89e3\u91ca\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u7ed3\u80a0\u955c\u56fe\u50cf\u4e2d\u7684\u606f\u8089\u5206\u5272\uff0c\u7ed3\u5408\u4e86U-Net\u548cGrad-CAM\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u9ad8\u5206\u5272\u7cbe\u5ea6\uff08IoU\u4e3a0.9257\uff09\u548c\u826f\u597d\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8AI\u8f85\u52a9\u7ed3\u80a0\u955c\u68c0\u67e5\u7684\u53ef\u9760\u6027\u548c\u4fe1\u4efb\u5ea6\u3002", "motivation": "\u624b\u52a8\u5206\u5272\u7ed3\u76f4\u80a0\u606f\u8089\u8017\u65f6\u4e14\u6613\u51fa\u9519\uff0c\u800c\u73b0\u6709\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u53ef\u89e3\u91ca\u6027\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u5176\u4e34\u5e8a\u5e94\u7528\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u51c6\u786e\u5206\u5272\u606f\u8089\u53c8\u80fd\u63d0\u4f9b\u53ef\u89e3\u91ca\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faPolypSeg-GradCAM\u6846\u67b6\uff0c\u5c06U-Net\u67b6\u6784\u4e0eGrad-CAM\u6280\u672f\u76f8\u7ed3\u5408\uff0c\u7528\u4e8e\u900f\u660e\u7684\u606f\u8089\u5206\u5272\u3002\u5728Kvasir-SEG\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002", "result": "\u5728Kvasir-SEG\u6d4b\u8bd5\u96c6\u4e0a\u5b9e\u73b0\u4e860.9257\u7684\u5e73\u5747\u4ea4\u5e76\u6bd4\uff08IoU\uff09\uff0c\u8bad\u7ec3\u548c\u9a8c\u8bc1\u96c6\u7684Dice\u7cfb\u6570\uff08F-score\uff09\u5747\u8d85\u8fc70.96\u3002Grad-CAM\u53ef\u89c6\u5316\u8bc1\u660e\u6a21\u578b\u9884\u6d4b\u5173\u6ce8\u4e8e\u4e34\u5e8a\u76f8\u5173\u533a\u57df\u3002", "conclusion": "PolypSeg-GradCAM\u901a\u8fc7\u7ed3\u5408\u9ad8\u5206\u5272\u7cbe\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u5f00\u53d1\u53ef\u9760\u3001\u53ef\u4fe1\u7684AI\u8f85\u52a9\u7ed3\u80a0\u955c\u68c0\u67e5\u548c\u6539\u5584\u7ed3\u76f4\u80a0\u764c\u65e9\u671f\u9884\u9632\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2509.18659", "categories": ["cs.NE"], "pdf": "https://arxiv.org/pdf/2509.18659", "abs": "https://arxiv.org/abs/2509.18659", "authors": ["Rodrigo Moreno", "Andres Faina", "Shyam Sudhakaran", "Kathryn Walker", "Sebastian Risi"], "title": "Smart Cellular Bricks for Decentralized Shape Classification and Damage Recovery", "comment": null, "summary": "Biological systems possess remarkable capabilities for self-recognition and\nmorphological regeneration, often relying solely on local interactions.\nInspired by these decentralized processes, we present a novel system of\nphysical 3D bricks--simple cubic units equipped with local communication,\nprocessing, and sensing--that are capable of inferring their global shape class\nand detecting structural damage. Leveraging Neural Cellular Automata (NCA), a\nlearned, fully-distributed algorithm, our system enables each module to\nindependently execute the same neural network without access to any global\nstate or positioning information. We demonstrate the ability of collections of\nhundreds of these cellular bricks to accurately classify a variety of 3D shapes\nthrough purely local interactions. The approach shows strong robustness to\nout-of-distribution shape variations and high tolerance to communication faults\nand failed modules. In addition to shape inference, the same decentralized\nframework is extended to detect missing or damaged components, allowing the\ncollective to localize structural disruptions and to guide a recovery process.\nThis work provides a physical realization of large-scale, decentralized\nself-recognition and damage detection, advancing the potential of robust,\nadaptive, and bio-inspired modular systems.", "AI": {"tldr": "\u53d7\u751f\u7269\u7cfb\u7edf\u53bb\u4e2d\u5fc3\u5316\u81ea\u6211\u8bc6\u522b\u548c\u5f62\u6001\u518d\u751f\u80fd\u529b\u7684\u542f\u53d1\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u76843D\u7ec6\u80de\u7816\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u4ec5\u901a\u8fc7\u5c40\u90e8\u4ea4\u4e92\u5373\u53ef\u5b9e\u73b0\u5168\u5c40\u5f62\u72b6\u5206\u7c7b\u548c\u635f\u4f24\u68c0\u6d4b\uff0c\u5e76\u5177\u6709\u826f\u597d\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u53d7\u751f\u7269\u7cfb\u7edf\u4e2d\u4ec5\u4f9d\u8d56\u5c40\u90e8\u76f8\u4e92\u4f5c\u7528\u7684\u53bb\u4e2d\u5fc3\u5316\u81ea\u6211\u8bc6\u522b\u548c\u5f62\u6001\u518d\u751f\u80fd\u529b\u7684\u542f\u53d1\u3002", "method": "\u5229\u7528\u795e\u7ecf\u7f51\u7edc\u7ec6\u80de\u81ea\u52a8\u673a\uff08NCA\uff09\uff0c\u4e00\u4e2a\u5b66\u4e60\u5230\u7684\u3001\u5b8c\u5168\u53bb\u4e2d\u5fc3\u5316\u7684\u7b97\u6cd5\uff0c\u4f7f\u6bcf\u4e2a\u6a21\u5757\u80fd\u591f\u72ec\u7acb\u6267\u884c\u76f8\u540c\u7684\u795e\u7ecf\u7f51\u7edc\uff0c\u800c\u65e0\u9700\u8bbf\u95ee\u4efb\u4f55\u5168\u5c40\u72b6\u6001\u6216\u5b9a\u4f4d\u4fe1\u606f\u3002", "result": "\u7531\u6570\u767e\u4e2a\u7ec6\u80de\u7816\u7ec4\u6210\u7684\u96c6\u5408\u80fd\u591f\u901a\u8fc7\u7eaf\u7cb9\u7684\u5c40\u90e8\u4ea4\u4e92\u51c6\u786e\u5730\u5bf9\u5404\u79cd3D\u5f62\u72b6\u8fdb\u884c\u5206\u7c7b\uff0c\u5e76\u4e14\u5bf9\u5f02\u7c7b\u5f62\u72b6\u53d8\u5316\u3001\u901a\u4fe1\u6545\u969c\u548c\u6a21\u5757\u6545\u969c\u5177\u6709\u5f88\u5f3a\u7684\u9c81\u68d2\u6027\u3002\u8be5\u6846\u67b6\u8fd8\u53ef\u7528\u4e8e\u68c0\u6d4b\u7f3a\u5931\u6216\u635f\u574f\u7684\u7ec4\u4ef6\uff0c\u4ece\u800c\u5b9e\u73b0\u5bf9\u7ed3\u6784\u4e2d\u65ad\u7684\u672c\u5730\u5316\u548c\u6062\u590d\u8fc7\u7a0b\u7684\u6307\u5bfc\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u63d0\u4f9b\u4e86\u5927\u89c4\u6a21\u3001\u53bb\u4e2d\u5fc3\u5316\u81ea\u6211\u8bc6\u522b\u548c\u635f\u4f24\u68c0\u6d4b\u7684\u7269\u7406\u5b9e\u73b0\uff0c\u63a8\u8fdb\u4e86\u9c81\u68d2\u3001\u81ea\u9002\u5e94\u548c\u53d7\u751f\u7269\u542f\u53d1\u7684\u6a21\u5757\u5316\u7cfb\u7edf\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.18204", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2509.18204", "abs": "https://arxiv.org/abs/2509.18204", "authors": ["Sijo K. Joseph", "Sudhir Singh"], "title": "Generalized Gottesman-Kitaev-Preskill States on a Quantum Torus", "comment": "1 Figures", "summary": "We introduce a novel formulation of a Generalized Gottesman-Kitaev-Preskill\n(GKP) state that resolves all of its foundational pathologies, such as infinite\nenergy, non-normalizability, and or- thogonality. We demonstrate that these\nissues are artifacts of defining the code on an unbounded phase-space. By\nconsidering the compact quantum-phase-space intrinsic to systems like coupled\nquantum harmonic oscillators, we have obtained a Generalized GKP (GGKP) state\nthat is both exact and physically realizable. This is achieved by applying an\nobtained Quantum Zak Transform (QZT) to Squeezed Coherent States, which reveals\nthat Riemann-Theta functions are the natural representation of these states on\nthe quantum torus phase-space. This framework not only provides a well-behaved\nquantum state, but also reveals a deep connection between quantum error correc-\ntion, non-commutative geometry, and the theory of generalized Theta functions.\nThis opens a new avenue for fault-tolerant photonic quantum computing.", "AI": {"tldr": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5e7f\u4e49GKP\uff08GGKP\uff09\u6001\u7684\u7cbe\u786e\u4e14\u53ef\u5b9e\u73b0\u7684\u91cf\u5b50\u6001\uff0c\u89e3\u51b3\u4e86\u73b0\u6709GKP\u6001\u7684\u56fa\u6709\u75c5\u6001\u95ee\u9898\uff0c\u5e76\u63ed\u793a\u4e86\u91cf\u5b50\u7ea0\u9519\u3001\u975e\u4ea4\u6362\u51e0\u4f55\u548cTheta\u51fd\u6570\u7406\u8bba\u4e4b\u95f4\u7684\u8054\u7cfb\u3002", "motivation": "\u73b0\u6709\u5e7f\u4e49GKP\u6001\u7684\u5b9a\u4e49\u5b58\u5728\u80fd\u91cf\u65e0\u9650\u3001\u4e0d\u53ef\u5f52\u4e00\u5316\u548c\u4e0d\u76f8\u4e92\u6b63\u4ea4\u7b49\u6839\u672c\u6027\u95ee\u9898\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5728\u7269\u7406\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u901a\u8fc7\u8003\u8651\u8026\u5408\u91cf\u5b50\u8c10\u632f\u5b50\u7b49\u7cfb\u7edf\u56fa\u6709\u7684\u7d27\u51d1\u91cf\u5b50\u76f8\u4f4d\u7a7a\u95f4\uff0c\u5e76\u5e94\u7528\u91cf\u5b50Zak\u53d8\u6362\uff08QZT\uff09\u5230\u538b\u7f29\u76f8\u5e72\u6001\uff0c\u6211\u4eec\u5f97\u5230\u4e86GGKP\u6001\u3002\u8fd9\u79cd\u65b9\u6cd5\u63ed\u793a\u4e86\u9ece\u66fcTheta\u51fd\u6570\u5728\u91cf\u5b50\u73af\u76f8\u4f4d\u7a7a\u95f4\u4e0a\u662f\u8fd9\u4e9b\u6001\u7684\u81ea\u7136\u8868\u793a\u3002", "result": "\u6211\u4eec\u5f97\u5230\u4e86\u4e00\u4e2a\u7cbe\u786e\u4e14\u53ef\u5b9e\u73b0\u7684GGKP\u6001\uff0c\u5b83\u6ca1\u6709\u65e0\u9650\u80fd\u91cf\u3001\u4e0d\u53ef\u5f52\u4e00\u5316\u548c\u4e0d\u76f8\u4e92\u6b63\u4ea4\u7684\u95ee\u9898\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u53d1\u73b0\u4e86\u91cf\u5b50\u7ea0\u9519\u3001\u975e\u4ea4\u6362\u51e0\u4f55\u548c\u5e7f\u4e49Theta\u51fd\u6570\u7406\u8bba\u4e4b\u95f4\u7684\u6df1\u523b\u8054\u7cfb\u3002", "conclusion": "\u6211\u4eec\u63d0\u51fa\u7684GGKP\u6001\u7684\u6846\u67b6\u4e0d\u4ec5\u63d0\u4f9b\u4e86\u4e00\u4e2a\u8868\u73b0\u826f\u597d\u7684\u91cf\u5b50\u6001\uff0c\u8fd8\u4e3a\u5bb9\u9519\u5149\u91cf\u5b50\u8ba1\u7b97\u5f00\u8f9f\u4e86\u65b0\u7684\u9014\u5f84\uff0c\u5e76\u63ed\u793a\u4e86\u91cf\u5b50\u7ea0\u9519\u3001\u975e\u4ea4\u6362\u51e0\u4f55\u548cTheta\u51fd\u6570\u7406\u8bba\u4e4b\u95f4\u7684\u6df1\u523b\u8054\u7cfb\u3002"}}
{"id": "2509.18103", "categories": ["cs.LG", "math.NT"], "pdf": "https://arxiv.org/pdf/2509.18103", "abs": "https://arxiv.org/abs/2509.18103", "authors": ["Jennifer Dodgson", "Michael Joedhitya", "Adith Ramdas", "Surender Suresh Kumar", "Adarsh Singh Chauhan", "Akira Rafhael", "Wang Mingshu", "Nordine Lotfi"], "title": "Machine Learnability as a Measure of Order in Aperiodic Sequences", "comment": null, "summary": "Research on the distribution of prime numbers has revealed a dual character:\ndeterministic in definition yet exhibiting statistical behavior reminiscent of\nrandom processes. In this paper we show that it is possible to use an\nimage-focused machine learning model to measure the comparative regularity of\nprime number fields at specific regions of an Ulam spiral. Specifically, we\ndemonstrate that in pure accuracy terms, models trained on blocks extracted\nfrom regions of the spiral in the vicinity of 500m outperform models trained on\nblocks extracted from the region representing integers lower than 25m. This\nimplies existence of more easily learnable order in the former region than in\nthe latter. Moreover, a detailed breakdown of precision and recall scores seem\nto imply that the model is favouring a different approach to classification in\ndifferent regions of the spiral, focusing more on identifying prime patterns at\nlower numbers and more on eliminating composites at higher numbers. This aligns\nwith number theory conjectures suggesting that at higher orders of magnitude we\nshould see diminishing noise in prime number distributions, with averages\n(density, AP equidistribution) coming to dominate, while local randomness\nregularises after scaling by log x. Taken together, these findings point toward\nan interesting possibility: that machine learning can serve as a new\nexperimental instrument for number theory. Notably, the method shows potential\n1 for investigating the patterns in strong and weak primes for cryptographic\npurposes.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u56fe\u50cf\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5206\u6790Ulam\u87ba\u65cb\u56fe\u4e2d\u7d20\u6570\u5206\u5e03\u7684\u89c4\u5f8b\u6027\uff0c\u53d1\u73b0\u5728\u8f83\u5927\u7684\u6570\u57df\uff08\u7ea6500m\u9644\u8fd1\uff09\u7684\u7d20\u6570\u5206\u5e03\u6bd4\u5c0f\u6570\u57df\uff08\u7ea625m\u4ee5\u4e0b\uff09\u66f4\u6613\u4e8e\u5b66\u4e60\u548c\u8bc6\u522b\u3002", "motivation": "\u7d20\u6570\u5206\u5e03\u5177\u6709\u786e\u5b9a\u6027\u5b9a\u4e49\u7684\u5185\u5728\u7279\u5f81\uff0c\u540c\u65f6\u4e5f\u8868\u73b0\u51fa\u7c7b\u4f3c\u968f\u673a\u8fc7\u7a0b\u7684\u7edf\u8ba1\u884c\u4e3a\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u4f7f\u7528\u56fe\u50cf\u673a\u5668\u5b66\u4e60\u6a21\u578b\u6765\u91cf\u5316\u7d20\u6570\u5206\u5e03\u5728\u7279\u5b9a\u533a\u57df\u7684\u89c4\u5f8b\u6027\uff0c\u5e76\u9a8c\u8bc1\u673a\u5668\u5b66\u4e60\u662f\u5426\u53ef\u4f5c\u4e3a\u6570\u8bba\u7814\u7a76\u7684\u65b0\u578b\u5b9e\u9a8c\u5de5\u5177\u3002", "method": "\u4f7f\u7528\u56fe\u50cf\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u5bf9Ulam\u87ba\u65cb\u56fe\u4e2d\u4e0d\u540c\u533a\u57df\uff08\u7ea6500m\u9644\u8fd1\u548c\u5c0f\u4e8e25m\uff09\u7684\u7d20\u6570\u5206\u5e03\u56fe\u50cf\u5757\u8fdb\u884c\u8bad\u7ec3\u548c\u6d4b\u8bd5\uff0c\u901a\u8fc7\u6bd4\u8f83\u6a21\u578b\u7684\u51c6\u786e\u7387\u3001\u7cbe\u786e\u7387\u548c\u53ec\u56de\u7387\u6765\u8bc4\u4f30\u4e0d\u540c\u533a\u57df\u7d20\u6570\u5206\u5e03\u7684\u89c4\u5f8b\u6027\u3002", "result": "\u5728\u7eaf\u51c6\u786e\u7387\u65b9\u9762\uff0c\u5728\u7ea6500m\u533a\u57df\u8bad\u7ec3\u7684\u6a21\u578b\u4f18\u4e8e\u5728\u5c0f\u4e8e25m\u533a\u57df\u8bad\u7ec3\u7684\u6a21\u578b\uff0c\u8868\u660e\u524d\u8005\u533a\u57df\u7684\u7d20\u6570\u5206\u5e03\u66f4\u6613\u4e8e\u5b66\u4e60\u3002\u7cbe\u786e\u7387\u548c\u53ec\u56de\u7387\u7684\u5206\u6570\u8868\u660e\uff0c\u6a21\u578b\u5728\u4e0d\u540c\u533a\u57df\u91c7\u7528\u4e0d\u540c\u7684\u5206\u7c7b\u7b56\u7565\uff1a\u5728\u5c0f\u6570\u57df\u4fa7\u91cd\u4e8e\u8bc6\u522b\u7d20\u6570\u6a21\u5f0f\uff0c\u5728\u8f83\u5927\u6570\u57df\u4fa7\u91cd\u4e8e\u6392\u9664\u5408\u6570\u3002\u8fd9\u4e0e\u9ad8\u6570\u91cf\u7ea7\u4e0b\u7d20\u6570\u5206\u5e03\u566a\u58f0\u51cf\u5c0f\u3001\u5e73\u5747\u6027\u8d28\uff08\u5bc6\u5ea6\u3001\u7b49\u5206\u5206\u4f48\uff09\u5360\u4e3b\u5bfc\u5730\u4f4d\u7684\u6570\u8bba\u731c\u60f3\u4e00\u81f4\u3002", "conclusion": "\u673a\u5668\u5b66\u4e60\u6a21\u578b\u80fd\u591f\u6709\u6548\u8861\u91cfUlam\u87ba\u65cb\u56fe\u4e2d\u4e0d\u540c\u533a\u57df\u7d20\u6570\u5206\u5e03\u7684\u89c4\u5f8b\u6027\u5dee\u5f02\uff0c\u5927\u6570\u57df\u7684\u7d20\u6570\u5206\u5e03\u89c4\u5f8b\u6027\u66f4\u5f3a\u3002\u673a\u5668\u5b66\u4e60\u6709\u671b\u6210\u4e3a\u6570\u8bba\u7814\u7a76\u7684\u65b0\u578b\u5b9e\u9a8c\u5de5\u5177\uff0c\u5e76\u53ef\u80fd\u5e94\u7528\u4e8e\u5f3a\u7d20\u6570\u548c\u5f31\u7d20\u6570\u7684\u6a21\u5f0f\u8bc6\u522b\uff0c\u670d\u52a1\u4e8e\u5bc6\u7801\u5b66\u7814\u7a76\u3002"}}
{"id": "2509.18198", "categories": ["cs.AI", "cs.MA", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18198", "abs": "https://arxiv.org/abs/2509.18198", "authors": ["Rui Liu", "Zikang Wang", "Peng Gao", "Yu Shen", "Pratap Tokekar", "Ming Lin"], "title": "MMCD: Multi-Modal Collaborative Decision-Making for Connected Autonomy with Knowledge Distillation", "comment": null, "summary": "Autonomous systems have advanced significantly, but challenges persist in\naccident-prone environments where robust decision-making is crucial. A single\nvehicle's limited sensor range and obstructed views increase the likelihood of\naccidents. Multi-vehicle connected systems and multi-modal approaches,\nleveraging RGB images and LiDAR point clouds, have emerged as promising\nsolutions. However, existing methods often assume the availability of all data\nmodalities and connected vehicles during both training and testing, which is\nimpractical due to potential sensor failures or missing connected vehicles. To\naddress these challenges, we introduce a novel framework MMCD (Multi-Modal\nCollaborative Decision-making) for connected autonomy. Our framework fuses\nmulti-modal observations from ego and collaborative vehicles to enhance\ndecision-making under challenging conditions. To ensure robust performance when\ncertain data modalities are unavailable during testing, we propose an approach\nbased on cross-modal knowledge distillation with a teacher-student model\nstructure. The teacher model is trained with multiple data modalities, while\nthe student model is designed to operate effectively with reduced modalities.\nIn experiments on $\\textit{connected autonomous driving with ground vehicles}$\nand $\\textit{aerial-ground vehicles collaboration}$, our method improves\ndriving safety by up to ${\\it 20.7}\\%$, surpassing the best-existing baseline\nin detecting potential accidents and making safe driving decisions. More\ninformation can be found on our website https://ruiiu.github.io/mmcd.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMMCD\u7684\u65b0\u578b\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u5b58\u5728\u4f20\u611f\u5668\u6545\u969c\u6216\u901a\u4fe1\u4e22\u5931\u7b49\u6311\u6218\u6027\u6761\u4ef6\u4e0b\uff0c\u5b9e\u73b0\u591a\u6a21\u6001\u548c\u591a\u8f66\u8f86\u534f\u540c\u51b3\u7b56\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u8de8\u6a21\u6001\u77e5\u8bc6\u84b8\u998f\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u5728\u6570\u636e\u6a21\u6001\u4e0d\u5b8c\u6574\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u6709\u6548\u8fd0\u884c\uff0c\u5e76\u5728\u76f8\u5173\u7684\u81ea\u52a8\u9a7e\u9a76\u548c\u7a7a\u5730\u534f\u540c\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u884c\u8f66\u5b89\u5168\u6027\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5728\u4e8b\u6545\u591a\u53d1\u73af\u5883\u4e2d\u9762\u4e34\u51b3\u7b56\u9c81\u68d2\u6027\u6311\u6218\uff0c\u5355\u4e00\u8f66\u8f86\u7684\u4f20\u611f\u5668\u8303\u56f4\u6709\u9650\u4e14\u89c6\u91ce\u53d7\u963b\uff0c\u589e\u52a0\u4e86\u4e8b\u6545\u53d1\u751f\u7684\u53ef\u80fd\u6027\u3002\u867d\u7136\u591a\u8f66\u8f86\u534f\u540c\u548c\u591a\u6a21\u6001\uff08RGB\u56fe\u50cf\u548cLiDAR\u70b9\u4e91\uff09\u65b9\u6cd5\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u5b83\u4eec\u901a\u5e38\u5047\u8bbe\u8bad\u7ec3\u548c\u6d4b\u8bd5\u671f\u95f4\u6240\u6709\u6570\u636e\u6a21\u6001\u548c\u534f\u540c\u8f66\u8f86\u90fd\u53ef\u7528\uff0c\u8fd9\u5728\u5b9e\u9645\u4e2d\u96be\u4ee5\u5b9e\u73b0\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMMCD\uff08Multi-Modal Collaborative Decision-making\uff09\u7684\u65b0\u578b\u6846\u67b6\uff0c\u7528\u4e8e\u534f\u540c\u81ea\u4e3b\u9a7e\u9a76\u3002\u8be5\u6846\u67b6\u878d\u5408\u4e86\u6765\u81ea\u672c\u8f66\u548c\u534f\u540c\u8f66\u8f86\u7684\u591a\u6a21\u6001\u89c2\u6d4b\u4fe1\u606f\uff0c\u4ee5\u589e\u5f3a\u5728\u6311\u6218\u6027\u6761\u4ef6\u4e0b\u7684\u51b3\u7b56\u80fd\u529b\u3002\u4e3a\u4e86\u786e\u4fdd\u5728\u6d4b\u8bd5\u671f\u95f4\u67d0\u4e9b\u6570\u636e\u6a21\u6001\u4e0d\u53ef\u7528\u65f6\u4ecd\u80fd\u4fdd\u6301\u9c81\u68d2\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8de8\u6a21\u6001\u77e5\u8bc6\u84b8\u998f\u7684\u65b9\u6cd5\uff0c\u91c7\u7528\u6559\u5e08-\u5b66\u751f\u6a21\u578b\u7ed3\u6784\u3002\u6559\u5e08\u6a21\u578b\u4f7f\u7528\u591a\u79cd\u6570\u636e\u6a21\u6001\u8fdb\u884c\u8bad\u7ec3\uff0c\u800c\u5b66\u751f\u6a21\u578b\u5219\u88ab\u8bbe\u8ba1\u4e3a\u80fd\u591f\u6709\u6548\u5730\u5904\u7406\u51cf\u5c11\u540e\u7684\u6570\u636e\u6a21\u6001\u3002", "result": "\u5728\u201c\u8f66\u8f86\u534f\u540c\u81ea\u52a8\u9a7e\u9a76\u201d\u548c\u201c\u7a7a\u5730\u534f\u540c\u201d\u7684\u5b9e\u9a8c\u4e2d\uff0cMMCD\u6846\u67b6\u5c06\u884c\u8f66\u5b89\u5168\u6027\u63d0\u9ad8\u4e86\u591a\u8fbe20.7%\uff0c\u5728\u68c0\u6d4b\u6f5c\u5728\u4e8b\u6545\u548c\u505a\u51fa\u5b89\u5168\u9a7e\u9a76\u51b3\u7b56\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u4f73\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "MMCD\u6846\u67b6\u901a\u8fc7\u878d\u5408\u591a\u6a21\u6001\u4fe1\u606f\u548c\u5229\u7528\u8de8\u6a21\u6001\u77e5\u8bc6\u84b8\u998f\uff0c\u6709\u6548\u5730\u89e3\u51b3\u4e86\u8fde\u63a5\u81ea\u52a8\u9a7e\u9a76\u4e2d\u56e0\u6570\u636e\u6a21\u6001\u7f3a\u5931\u6216\u534f\u540c\u8f66\u8f86\u4e0d\u53ef\u7528\u800c\u5e26\u6765\u7684\u6311\u6218\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u9a7e\u9a76\u5b89\u5168\u6027\u3002"}}
{"id": "2509.18101", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18101", "abs": "https://arxiv.org/abs/2509.18101", "authors": ["Guanzhong Pan", "Haibo Wang"], "title": "A Cost-Benefit Analysis of On-Premise Large Language Model Deployment: Breaking Even with Commercial LLM Services", "comment": null, "summary": "Large language models (LLMs) are becoming increasingly widespread.\nOrganizations that want to use AI for productivity now face an important\ndecision. They can subscribe to commercial LLM services or deploy models on\ntheir own infrastructure. Cloud services from providers such as OpenAI,\nAnthropic, and Google are attractive because they provide easy access to\nstate-of-the-art models and are easy to scale. However, concerns about data\nprivacy, the difficulty of switching service providers, and long-term operating\ncosts have driven interest in local deployment of open-source models. This\npaper presents a cost-benefit analysis framework to help organizations\ndetermine when on-premise LLM deployment becomes economically viable compared\nto commercial subscription services. We consider the hardware requirements,\noperational expenses, and performance benchmarks of the latest open-source\nmodels, including Qwen, Llama, Mistral, and etc. Then we compare the total cost\nof deploying these models locally with the major cloud providers subscription\nfee. Our findings provide an estimated breakeven point based on usage levels\nand performance needs. These results give organizations a practical framework\nfor planning their LLM strategies.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.18113", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18113", "abs": "https://arxiv.org/abs/2509.18113", "authors": ["Xin Hu", "Yue Kang", "Guanzi Yao", "Tianze Kang", "Mengjie Wang", "Heyao Liu"], "title": "Dynamic Prompt Fusion for Multi-Task and Cross-Domain Adaptation in LLMs", "comment": null, "summary": "This study addresses the generalization limitations commonly observed in\nlarge language models under multi-task and cross-domain settings. Unlike prior\nmethods such as SPoT, which depends on fixed prompt templates, our study\nintroduces a unified multi-task learning framework with dynamic prompt\nscheduling mechanism. By introducing a prompt pool and a task-aware scheduling\nstrategy, the method dynamically combines and aligns prompts for different\ntasks. This enhances the model's ability to capture semantic differences across\ntasks. During prompt fusion, the model uses task embeddings and a gating\nmechanism to finely control the prompt signals. This ensures alignment between\nprompt content and task-specific demands. At the same time, it builds flexible\nsharing pathways across tasks. In addition, the proposed optimization objective\ncenters on joint multi-task learning. It incorporates an automatic learning\nstrategy for scheduling weights, which effectively mitigates task interference\nand negative transfer. To evaluate the effectiveness of the method, a series of\nsensitivity experiments were conducted. These experiments examined the impact\nof prompt temperature parameters and task number variation. The results confirm\nthe advantages of the proposed mechanism in maintaining model stability and\nenhancing transferability. Experimental findings show that the prompt\nscheduling method significantly improves performance on a range of language\nunderstanding and knowledge reasoning tasks. These results fully demonstrate\nits applicability and effectiveness in unified multi-task modeling and\ncross-domain adaptation.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5305\u542b\u52a8\u6001\u63d0\u793a\u8c03\u5ea6\u673a\u5236\u7684\u7edf\u4e00\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u4efb\u52a1\u548c\u8de8\u57df\u8bbe\u7f6e\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u9650\u5236\u95ee\u9898\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u4efb\u52a1\u548c\u8de8\u57df\u8bbe\u7f6e\u4e0b\u5b58\u5728\u6cdb\u5316\u80fd\u529b\u9650\u5236\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\uff0c\u5305\u542b\u4e00\u4e2a\u63d0\u793a\u6c60\u548c\u4e00\u4e2a\u4efb\u52a1\u611f\u77e5\u8c03\u5ea6\u7b56\u7565\uff0c\u80fd\u591f\u52a8\u6001\u5730\u7ec4\u5408\u548c\u5bf9\u9f50\u4e0d\u540c\u4efb\u52a1\u7684\u63d0\u793a\u3002\u901a\u8fc7\u4f7f\u7528\u4efb\u52a1\u5d4c\u5165\u548c\u95e8\u63a7\u673a\u5236\u6765\u878d\u5408\u63d0\u793a\uff0c\u5e76\u91c7\u7528\u8054\u5408\u591a\u4efb\u52a1\u5b66\u4e60\u7684\u4f18\u5316\u76ee\u6807\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u7f13\u89e3\u4efb\u52a1\u5e72\u6270\u548c\u8d1f\u8fc1\u79fb\u3002", "result": "\u8be5\u6846\u67b6\u5728\u591a\u9879\u8bed\u8a00\u7406\u89e3\u548c\u77e5\u8bc6\u63a8\u7406\u4efb\u52a1\u4e0a\u663e\u8457\u63d0\u9ad8\u4e86\u6027\u80fd\uff0c\u5e76\u8868\u73b0\u51fa\u826f\u597d\u7684\u6a21\u578b\u7a33\u5b9a\u6027\u548c\u8fc1\u79fb\u80fd\u529b\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u63d0\u793a\u8c03\u5ea6\u65b9\u6cd5\u5728\u7edf\u4e00\u591a\u4efb\u52a1\u5efa\u6a21\u548c\u8de8\u57df\u9002\u5e94\u65b9\u9762\u5177\u6709\u5f88\u9ad8\u7684\u9002\u7528\u6027\u548c\u6709\u6548\u6027\u3002"}}
{"id": "2509.18357", "categories": ["cs.LO"], "pdf": "https://arxiv.org/pdf/2509.18357", "abs": "https://arxiv.org/abs/2509.18357", "authors": ["Michael Johnson", "David Jaz Myers"], "title": "Proceedings Seventh International Conference on Applied Category Theory 2024", "comment": null, "summary": "Proceedings of the Seventh International Conference on Applied Category\nTheory, held at the University of Oxford on 17 - 21 June 2024. The\ncontributions to ACT 2024 ranged from pure to applied and included\ncontributions in a wide range of disciplines in science and engineering. ACT\n2024 included talks in classical mechanics, quantum physics, probability\ntheory, linguistics, decision theory, machine learning, epidemiology,\nthermodynamics, engineering, and logic.", "AI": {"tldr": "ACT 2024\u4f1a\u8bae\u6db5\u76d6\u4e86\u4ece\u7eaf\u7406\u8bba\u5230\u5e94\u7528\u7814\u7a76\u7684\u5e7f\u6cdb\u4e3b\u9898\uff0c\u6d89\u53ca\u79d1\u5b66\u548c\u5de5\u7a0b\u9886\u57df\u7684\u591a\u4e2a\u5b66\u79d1\u3002", "motivation": "\u4ecb\u7ecdACT 2024\u4f1a\u8bae\u7684\u5e7f\u6cdb\u6027\u548c\u5b66\u79d1\u4ea4\u53c9\u6027\u3002", "method": "\u5217\u4e3e\u4e86\u4f1a\u8bae\u6d89\u53ca\u7684\u5177\u4f53\u5b66\u79d1\u9886\u57df\uff0c\u5982\u7ecf\u5178\u529b\u5b66\u3001\u91cf\u5b50\u7269\u7406\u3001\u6982\u7387\u8bba\u3001\u8bed\u8a00\u5b66\u3001\u51b3\u7b56\u8bba\u3001\u673a\u5668\u5b66\u4e60\u3001\u6d41\u884c\u75c5\u5b66\u3001\u70ed\u529b\u5b66\u3001\u5de5\u7a0b\u5b66\u548c\u903b\u8f91\u5b66\u3002", "result": "\u5c55\u793a\u4e86\u4f1a\u8bae\u5185\u5bb9\u7684\u4e30\u5bcc\u6027\u548c\u591a\u6837\u6027\u3002", "conclusion": "\u5f3a\u8c03\u4e86\u5e94\u7528\u8303\u7574\u7406\u8bba\u5728\u4e0d\u540c\u79d1\u5b66\u548c\u5de5\u7a0b\u9886\u57df\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.19060", "categories": ["physics.app-ph", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2509.19060", "abs": "https://arxiv.org/abs/2509.19060", "authors": ["Bill D. Aparicio Huacarpuma", "Jos\u00e9 A. S. Laranjeira", "Nicolas F. Martins", "Julio R. Sambrano", "F\u00e1bio L. Lopes de Mendon\u00e7a", "Alexandre C. Dias", "Luiz A. Ribeiro Junior"], "title": "Sodium-Decorated Ennea-Graphene: A Novel 2D Carbon Allotrope for High-Capacity Hydrogen Storage", "comment": "In preparation for Journal Submission", "summary": "The development of safe, efficient, and reversible hydrogen storage materials\nis critical for advancing hydrogen-based energy technologies and achieving\ncarbon-neutral goals. Ennea-Graphene, a new 2D carbon allotrope made of 4-, 5-,\n6-, and mainly 9-membered carbon rings (nonagons), is introduced via Density\nFunctional Theory (DFT) calculations. Phonon dispersion and ab initio molecular\ndynamics demonstrate that the monolayer is mechanically and dynamically stable\nat 300 K, as no imaginary modes are detected. The pristine system further\nexhibits metallic-like electronic behavior. The material exhibits high in-plane\nstiffness (Young modulus of 255 N/m). Sodium adsorption at the centers of the\nnonagonal rings is energetically favorable, with a binding energy of\napproximately -1.56 eV, leading to the formation of the Na@Ennea-Graphene\ncomplex. The calculated H2 adsorption energies range from -0.15 eV to -0.18 eV.\nThe Na-decorated structure demonstrates excellent hydrogen storage performance,\nreversibly adsorbing up to four H2 molecules per Na atom (8.8 wt\\% H2). This\ncapacity surpasses the U.S. Department of Energy's 2025 target for onboard\nhydrogen storage materials. The adsorbed H2 remains molecular (H-H bond of\n0.76~\\AA) and can be released under near-ambient conditions, as verified by 300\nK ab initio molecular dynamics simulations. These findings position\nsodium-decorated Ennea-Graphene as a promising nanomaterial for next-generation\nhydrogen storage technologies.", "AI": {"tldr": "Ennea-Graphene\u662f\u4e00\u79cd\u65b0\u578b\u4e8c\u7ef4\u78b3\u540c\u7d20\u5f02\u5f62\u4f53\uff0c\u901a\u8fc7DFT\u8ba1\u7b97\u5f97\u5230\uff0c\u5177\u6709\u826f\u597d\u7684\u673a\u68b0\u548c\u52a8\u529b\u5b66\u7a33\u5b9a\u6027\u3002\u5176\u94a0\u4fee\u9970\u5f62\u5f0f\u5728\u50a8\u6c22\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u53ef\u9006\u5438\u9644\u9ad8\u8fbe\u6bcfNa\u539f\u5b504\u4e2aH2\u5206\u5b50\uff088.8 wt% H2\uff09\uff0c\u8d85\u8fc7\u4e86\u7f8e\u56fd\u80fd\u6e90\u90e82025\u5e74\u7684\u76ee\u6807\u3002", "motivation": "\u5f00\u53d1\u5b89\u5168\u3001\u9ad8\u6548\u3001\u53ef\u9006\u7684\u50a8\u6c22\u6750\u6599\u5bf9\u4e8e\u63a8\u8fdb\u6c22\u57fa\u80fd\u6e90\u6280\u672f\u548c\u5b9e\u73b0\u78b3\u4e2d\u548c\u76ee\u6807\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5229\u7528\u5bc6\u5ea6\u6cdb\u51fd\u7406\u8bba\uff08DFT\uff09\u8ba1\u7b97\uff0c\u5e76\u901a\u8fc7\u58f0\u5b50\u8272\u6563\u548c\u4ece\u5934\u5206\u5b50\u52a8\u529b\u5b66\u6a21\u62df\u9a8c\u8bc1\u4e86Ennea-Graphene\u7684\u7a33\u5b9a\u6027\u548c\u50a8\u6c22\u6027\u80fd\u3002", "result": "\u8ba1\u7b97\u8868\u660eEnnea-Graphene\u5177\u6709\u826f\u597d\u7684\u673a\u68b0\u548c\u52a8\u529b\u5b66\u7a33\u5b9a\u6027\uff0c\u8868\u73b0\u51fa\u7c7b\u91d1\u5c5e\u7684\u7535\u5b50\u7279\u6027\uff0c\u5e76\u5177\u6709\u9ad8\u9762\u5185\u521a\u5ea6\uff08\u6768\u6c0f\u6a21\u91cf\u4e3a255 N/m\uff09\u3002\u94a0\u5438\u9644\u5728\u975e\u4e5d\u5143\u73af\u4e2d\u5fc3\u5904\u5177\u6709\u80fd\u91cf\u4f18\u52bf\uff08\u7ed3\u5408\u80fd\u7ea6\u4e3a-1.56 eV\uff09\uff0c\u5f62\u6210Na@Ennea-Graphene\u590d\u5408\u7269\u3002\u8be5\u590d\u5408\u7269\u53ef\u9006\u5438\u9644\u9ad8\u8fbe\u6bcfNa\u539f\u5b504\u4e2aH2\u5206\u5b50\uff088.8 wt% H2\uff09\uff0c\u8d85\u8fc7\u4e86\u7f8e\u56fd\u80fd\u6e90\u90e82025\u5e74\u7684\u76ee\u6807\u3002\u5438\u9644\u7684H2\u4fdd\u6301\u5206\u5b50\u72b6\u6001\uff0c\u5e76\u5728\u63a5\u8fd1\u73af\u5883\u7684\u6761\u4ef6\u4e0b\u53ef\u91ca\u653e\u3002", "conclusion": "\u94a0\u4fee\u9970\u7684Ennea-Graphene\u662f\u4e00\u79cd\u6709\u524d\u9014\u7684\u7eb3\u7c73\u6750\u6599\uff0c\u53ef\u7528\u4e8e\u4e0b\u4e00\u4ee3\u50a8\u6c22\u6280\u672f\u3002"}}
{"id": "2509.18936", "categories": ["cs.DS", "cs.CC"], "pdf": "https://arxiv.org/pdf/2509.18936", "abs": "https://arxiv.org/abs/2509.18936", "authors": ["Arun Kumar Das", "Michal Opler", "Tom\u00e1\u0161 Valla"], "title": "Precoloring extension with demands on paths", "comment": "Full version of paper accepted to ISAAC", "summary": "Let $G$ be a graph with a set of precolored vertices, and let us be given an\ninteger distance parameter $d$ and a set of integer demands $d_1,\\dots,d_c$.\nThe Distance Precoloring Extension with Demands (DPED) problem is to compute a\nvertex $c$-coloring of $G$ such that the following three conditions hold: (i)\nthe resulting coloring respects the colors of the precolored vertices, (ii) the\ndistance of two vertices of the same color is at least $d$, and (iii) the\nnumber of vertices colored by color $i$ is exactly $d_i$. This problem is\nmotivated by a program scheduling in commercial broadcast channels with\nconstraints on content repetition and placement, which leads precisely to the\nDPED problem for paths.\n  In this paper, we study DPED on paths and present a polynomial time exact\nalgorithm when precolored vertices are restricted to the two ends of the path\nand devise an approximation algorithm for DPED with an additive approximation\nfactor polynomially bounded by $d$ and the number of precolored vertices. Then,\nwe prove that the Distance Precoloring Extension problem on paths, a less\nrestrictive version of DPED without the demand constraints, and then DPED\nitself, is NP-complete. Motivated by this result, we further study the\nparameterized complexity of DPED on paths. We establish that the DPED problem\non paths is $W[1]$-hard when parameterized by the number of colors and the\ndistance. On the positive side, we devise a fixed parameter tractable (FPT)\nalgorithm for DPED on paths when the number of colors, the distance, and the\nnumber of precolored vertices are considered as the parameters. Moreover, we\nprove that Distance Precoloring Extension is FPT parameterized by the distance.\nAs a byproduct, we also obtain several results for the Distance List Coloring\nproblem on paths.", "AI": {"tldr": "DPED\u95ee\u9898\u5728\u8def\u5f84\u4e0a\u88ab\u7814\u7a76\uff0c\u63d0\u51fa\u4e86\u591a\u9879\u5f0f\u65f6\u95f4\u7cbe\u786e\u7b97\u6cd5\u3001\u8fd1\u4f3c\u7b97\u6cd5\uff0c\u5e76\u8bc1\u660e\u4e86\u5176NP-\u5b8c\u5168\u6027\uff0c\u7814\u7a76\u4e86\u53c2\u6570\u5316\u590d\u6742\u6027\uff0c\u5e76\u63d0\u51fa\u4e86FPT\u7b97\u6cd5\u3002", "motivation": "DPED\u95ee\u9898\u6e90\u4e8e\u5546\u4e1a\u5e7f\u64ad\u9891\u9053\u4e2d\u7684\u8282\u76ee\u8c03\u5ea6\uff0c\u9700\u8981\u6ee1\u8db3\u5185\u5bb9\u91cd\u590d\u548c\u653e\u7f6e\u7684\u7ea6\u675f\u3002", "method": "\u7814\u7a76DPED\u5728\u8def\u5f84\u4e0a\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u7cbe\u786e\u7b97\u6cd5\u3001\u8fd1\u4f3c\u7b97\u6cd5\u3001NP-\u5b8c\u5168\u6027\u8bc1\u660e\u3001\u53c2\u6570\u5316\u590d\u6742\u6027\u5206\u6790\u548cFPT\u7b97\u6cd5\u3002", "result": "\u63d0\u51fa\u4e86DPED\u5728\u8def\u5f84\u4e0a\u7684\u591a\u9879\u5f0f\u65f6\u95f4\u7cbe\u786e\u7b97\u6cd5\u3001\u8fd1\u4f3c\u7b97\u6cd5\uff0c\u8bc1\u660e\u4e86DPED\u662fNP-\u5b8c\u5168\u7684\uff0c\u5e76\u63d0\u51fa\u4e86\u9488\u5bf9\u989c\u8272\u6570\u3001\u8ddd\u79bb\u548c\u9884\u67d3\u8272\u9876\u70b9\u6570\u7684FPT\u7b97\u6cd5\u3002", "conclusion": "DPED\u95ee\u9898\u5728\u8def\u5f84\u4e0a\u7684\u7814\u7a76\u53d6\u5f97\u4e86\u591a\u9879\u8fdb\u5c55\uff0c\u5305\u62ec\u7cbe\u786e\u7b97\u6cd5\u3001\u8fd1\u4f3c\u7b97\u6cd5\u3001NP-\u5b8c\u5168\u6027\u8bc1\u660e\u4ee5\u53caFPT\u7b97\u6cd5\uff0c\u4e3a\u76f8\u5173\u8c03\u5ea6\u95ee\u9898\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2509.18461", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.18461", "abs": "https://arxiv.org/abs/2509.18461", "authors": ["Ayan Sar", "Sampurna Roy", "Tanupriya Choudhury", "Ajith Abraham"], "title": "Zero-Shot Visual Deepfake Detection: Can AI Predict and Prevent Fake Content Before It's Created?", "comment": "Published in Foundations and Trends in Signal Processing (#1 in\n  Signal Processing, #3 in Computer Science)", "summary": "Generative adversarial networks (GANs) and diffusion models have dramatically\nadvanced deepfake technology, and its threats to digital security, media\nintegrity, and public trust have increased rapidly. This research explored\nzero-shot deepfake detection, an emerging method even when the models have\nnever seen a particular deepfake variation. In this work, we studied\nself-supervised learning, transformer-based zero-shot classifier, generative\nmodel fingerprinting, and meta-learning techniques that better adapt to the\never-evolving deepfake threat. In addition, we suggested AI-driven prevention\nstrategies that mitigated the underlying generation pipeline of the deepfakes\nbefore they occurred. They consisted of adversarial perturbations for creating\ndeepfake generators, digital watermarking for content authenticity\nverification, real-time AI monitoring for content creation pipelines, and\nblockchain-based content verification frameworks. Despite these advancements,\nzero-shot detection and prevention faced critical challenges such as\nadversarial attacks, scalability constraints, ethical dilemmas, and the absence\nof standardized evaluation benchmarks. These limitations were addressed by\ndiscussing future research directions on explainable AI for deepfake detection,\nmultimodal fusion based on image, audio, and text analysis, quantum AI for\nenhanced security, and federated learning for privacy-preserving deepfake\ndetection. This further highlighted the need for an integrated defense\nframework for digital authenticity that utilized zero-shot learning in\ncombination with preventive deepfake mechanisms. Finally, we highlighted the\nimportant role of interdisciplinary collaboration between AI researchers,\ncybersecurity experts, and policymakers to create resilient defenses against\nthe rising tide of deepfake attacks.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u96f6\u6837\u672c\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u548c\u9884\u9632\u7b56\u7565\uff0c\u4ee5\u5e94\u5bf9\u65e5\u76ca\u589e\u957f\u7684\u6df1\u5ea6\u4f2a\u9020\u5a01\u80c1\u3002", "motivation": "\u6df1\u5ea6\u4f2a\u9020\u6280\u672f\uff08\u5982GANs\u548c\u6269\u6563\u6a21\u578b\uff09\u7684\u5feb\u901f\u53d1\u5c55\u5bf9\u6570\u5b57\u5b89\u5168\u3001\u5a92\u4f53\u8bda\u4fe1\u548c\u516c\u4f17\u4fe1\u4efb\u6784\u6210\u4e86\u4e25\u91cd\u5a01\u80c1\uff0c\u9700\u8981\u65b0\u7684\u68c0\u6d4b\u548c\u9884\u9632\u65b9\u6cd5\u3002", "method": "\u7814\u7a76\u4e86\u81ea\u76d1\u7763\u5b66\u4e60\u3001\u57fa\u4e8eTransformer\u7684\u96f6\u6837\u672c\u5206\u7c7b\u5668\u3001\u751f\u6210\u6a21\u578b\u6307\u7eb9\u8bc6\u522b\u548c\u5143\u5b66\u4e60\u6280\u672f\uff0c\u5e76\u63d0\u51fa\u4e86\u5305\u62ec\u5bf9\u6297\u6027\u6270\u52a8\u3001\u6570\u5b57\u6c34\u5370\u3001\u5b9e\u65f6AI\u76d1\u63a7\u548c\u57fa\u4e8e\u533a\u5757\u94fe\u7684\u5185\u5bb9\u9a8c\u8bc1\u5728\u5185\u7684AI\u9a71\u52a8\u7684\u9884\u9632\u7b56\u7565\u3002", "result": "\u5728\u96f6\u6837\u672c\u68c0\u6d4b\u65b9\u9762\uff0c\u7814\u7a76\u63a2\u7d22\u4e86\u591a\u79cd\u5148\u8fdb\u6280\u672f\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u7cfb\u5217\u9884\u9632\u63aa\u65bd\u3002\u7136\u800c\uff0c\u4ecd\u9762\u4e34\u5bf9\u6297\u6027\u653b\u51fb\u3001\u53ef\u6269\u5c55\u6027\u3001\u4f26\u7406\u56f0\u5883\u548c\u7f3a\u4e4f\u6807\u51c6\u5316\u8bc4\u4f30\u57fa\u51c6\u7b49\u6311\u6218\u3002", "conclusion": "\u96f6\u6837\u672c\u68c0\u6d4b\u548c\u9884\u9632\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u5305\u62ec\u53ef\u89e3\u91caAI\u3001\u591a\u6a21\u6001\u878d\u5408\u3001\u91cf\u5b50AI\u548c\u8054\u90a6\u5b66\u4e60\u3002\u9700\u8981\u4e00\u4e2a\u7ed3\u5408\u96f6\u6837\u672c\u5b66\u4e60\u548c\u9884\u9632\u673a\u5236\u7684\u7efc\u5408\u6027\u6570\u5b57\u771f\u5b9e\u6027\u9632\u5fa1\u6846\u67b6\uff0c\u5e76\u5f3a\u8c03\u8de8\u5b66\u79d1\u5408\u4f5c\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2509.18472", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.18472", "abs": "https://arxiv.org/abs/2509.18472", "authors": ["Atanu Barai", "Kamalavasan Kamalakkannan", "Patrick Diehl", "Maxim Moraru", "Jered Dominguez-Trujillo", "Howard Pritchard", "Nandakishore Santhi", "Farzad Fatollahi-Fard", "Galen Shipman"], "title": "Bridging Simulation and Silicon: A Study of RISC-V Hardware and FireSim Simulation", "comment": null, "summary": "RISC-V ISA-based processors have recently emerged as both powerful and\nenergy-efficient computing platforms. The release of the MILK-V Pioneer marked\na significant milestone as the first desktop-grade RISC-V system. With\nincreasing engagement from both academia and industry, such platforms exhibit\nstrong potential for adoption in high-performance computing (HPC) environments.\n  The open-source, FPGA-accelerated FireSim framework has emerged as a flexible\nand scalable tool for architectural exploration, enabling simulation of various\nsystem configurations using RISC-V cores. Despite its capabilities, there\nremains a lack of systematic evaluation regarding the feasibility and\nperformance prediction accuracy of FireSim when compared to physical hardware.\n  In this study, we address this gap by modeling a commercially available\nsingle-board computer and a desktop-grade RISC-V CPU within FireSim. To ensure\nfidelity between simulation and real hardware, we first measure the performance\nof a series of benchmarks to compare runtime behavior under single-core and\nfour-core configurations. Based on the closest matching simulation parameters,\nwe subsequently evaluate performance using a representative mini-application\nand the LAMMPS molecular dynamics code.\n  Our findings indicate that while FireSim provides valuable insights into\narchitectural performance trends, discrepancies remain between simulated and\nmeasured runtimes. These deviations stem from both inherent limitations of the\nsimulation environment and the restricted availability of detailed performance\nspecifications from CPU manufacturers, which hinder precise configuration\nmatching.", "AI": {"tldr": "\u5c3d\u7ba1FireSim\u5728RISC-V\u5904\u7406\u5668\u548cHPC\u9886\u57df\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u5176\u6a21\u62df\u4e0e\u771f\u5b9e\u786c\u4ef6\u4e4b\u95f4\u7684\u6027\u80fd\u9884\u6d4b\u4ecd\u5b58\u5728\u5dee\u8ddd\u3002", "motivation": "\u8bc4\u4f30FireSim\u6a21\u62df\u6846\u67b6\u5728RISC-V\u684c\u9762\u7ea7\u5904\u7406\u5668\u4e0a\u7684\u53ef\u884c\u6027\u4e0e\u6027\u80fd\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u5e76\u4e0e\u7269\u7406\u786c\u4ef6\u8fdb\u884c\u5bf9\u6bd4\u3002", "method": "\u5728FireSim\u4e2d\u5efa\u6a21\u5546\u7528\u5355\u677f\u8ba1\u7b97\u673a\u548c\u684c\u9762\u7ea7RISC-V CPU\uff0c\u901a\u8fc7\u57fa\u51c6\u6d4b\u8bd5\u548cLAMMPS\u5206\u5b50\u52a8\u529b\u5b66\u4ee3\u7801\uff0c\u6bd4\u8f83\u5355\u6838\u548c\u56db\u6838\u914d\u7f6e\u4e0b\u7684\u6a21\u62df\u4e0e\u5b9e\u9645\u786c\u4ef6\u6027\u80fd\u3002", "result": "FireSim\u80fd\u63d0\u4f9b\u67b6\u6784\u6027\u80fd\u8d8b\u52bf\u7684\u89c1\u89e3\uff0c\u4f46\u5728\u6a21\u62df\u548c\u5b9e\u9645\u8fd0\u884c\u65f6\u95f4\u4e4b\u95f4\u5b58\u5728\u5dee\u5f02\uff0c\u8fd9\u6e90\u4e8e\u6a21\u62df\u73af\u5883\u7684\u5c40\u9650\u6027\u548cCPU\u5236\u9020\u5546\u63d0\u4f9b\u7684\u6027\u80fd\u89c4\u683c\u4e0d\u5b8c\u6574\u3002", "conclusion": "FireSim\u5728RISC-V HPC\u9886\u57df\u6709\u5e94\u7528\u6f5c\u529b\uff0c\u4f46\u8981\u5b9e\u73b0\u7cbe\u786e\u7684\u6027\u80fd\u9884\u6d4b\uff0c\u9700\u8981\u514b\u670d\u6a21\u62df\u73af\u5883\u7684\u5c40\u9650\u6027\u5e76\u83b7\u5f97\u66f4\u8be6\u7ec6\u7684\u786c\u4ef6\u89c4\u683c\u3002"}}
{"id": "2509.18356", "categories": ["eess.SY", "cs.NI", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.18356", "abs": "https://arxiv.org/abs/2509.18356", "authors": ["Darin Jeff", "Eytan Modiano"], "title": "Optimal Service Mode Assignment in a Simple Computation Offloading System: Extended Version", "comment": null, "summary": "We consider a simple computation offloading model where jobs can either be\nfully processed in the cloud or be partially processed at a local server before\nbeing sent to the cloud to complete processing. Our goal is to design a policy\nfor assigning jobs to service modes, i.e., full offloading or partial\noffloading, based on the state of the system, in order to minimize delay in the\nsystem. We show that when the cloud server is idle, the optimal policy is to\nassign the next job in the system queue to the cloud for processing. However,\nwhen the cloud server is busy, we show that, under mild assumptions, the\noptimal policy is of a threshold type, that sends the next job in the system\nqueue to the local server if the queue exceeds a certain threshold. Finally, we\ndemonstrate this policy structure through simulations.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u6700\u5c0f\u5316\u8ba1\u7b97\u4efb\u52a1\u5ef6\u8fdf\u7684\u4f5c\u4e1a\u5206\u914d\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u533a\u5206\u4e86\u4e91\u7aef\u5904\u7406\u548c\u672c\u5730-\u4e91\u7aef\u534f\u540c\u5904\u7406\u4e24\u79cd\u6a21\u5f0f\u3002", "motivation": "\u8bbe\u8ba1\u4e00\u79cd\u72b6\u6001\u76f8\u5173\u7684\u7b56\u7565\uff0c\u4ee5\u6700\u5c0f\u5316\u5305\u542b\u4e91\u5904\u7406\u548c\u672c\u5730-\u4e91\u7aef\u534f\u540c\u5904\u7406\u4e24\u79cd\u6a21\u5f0f\u7684\u8ba1\u7b97\u5378\u8f7d\u6a21\u578b\u4e2d\u7684\u7cfb\u7edf\u5ef6\u8fdf\u3002", "method": "\u5f53\u4e91\u670d\u52a1\u5668\u7a7a\u95f2\u65f6\uff0c\u5c06\u4e0b\u4e00\u4e2a\u4efb\u52a1\u5206\u914d\u7ed9\u4e91\u8fdb\u884c\u5904\u7406\uff1b\u5f53\u4e91\u670d\u52a1\u5668\u7e41\u5fd9\u65f6\uff0c\u5982\u679c\u961f\u5217\u957f\u5ea6\u8d85\u8fc7\u67d0\u4e2a\u9608\u503c\uff0c\u5219\u5c06\u4efb\u52a1\u5206\u914d\u7ed9\u672c\u5730\u670d\u52a1\u5668\u8fdb\u884c\u9884\u5904\u7406\uff0c\u7136\u540e\u53d1\u9001\u5230\u4e91\u7aef\u5b8c\u6210\u5904\u7406\u3002", "result": "\u63a8\u5bfc\u5e76\u8bc1\u660e\u4e86\u5728\u4e91\u670d\u52a1\u5668\u7a7a\u95f2\u548c\u7e41\u5fd9\u4e24\u79cd\u60c5\u51b5\u4e0b\uff0c\u6700\u4f18\u7b56\u7565\u7684\u7ed3\u6784\u3002\u8bc1\u660e\u4e86\u5f53\u4e91\u670d\u52a1\u5668\u7e41\u5fd9\u65f6\uff0c\u5b58\u5728\u4e00\u4e2a\u9608\u503c\uff0c\u53ea\u6709\u5f53\u961f\u5217\u957f\u5ea6\u8d85\u8fc7\u8be5\u9608\u503c\u65f6\uff0c\u624d\u5c06\u4efb\u52a1\u5206\u914d\u7ed9\u672c\u5730\u670d\u52a1\u5668\u3002", "conclusion": "\u6700\u4f18\u7b56\u7565\u5177\u6709\u4e00\u79cd\u9608\u503c\u7ed3\u6784\uff1a\u5f53\u4e91\u670d\u52a1\u5668\u7a7a\u95f2\u65f6\uff0c\u603b\u662f\u5c06\u4efb\u52a1\u5378\u8f7d\u5230\u4e91\u7aef\uff1b\u5f53\u4e91\u670d\u52a1\u5668\u7e41\u5fd9\u65f6\uff0c\u4ec5\u5f53\u961f\u5217\u957f\u5ea6\u8d85\u8fc7\u67d0\u4e2a\u9608\u503c\u65f6\uff0c\u624d\u5c06\u4efb\u52a1\u5378\u8f7d\u5230\u672c\u5730\u670d\u52a1\u5668\u8fdb\u884c\u90e8\u5206\u5904\u7406\u3002\u8be5\u7b56\u7565\u901a\u8fc7\u4eff\u771f\u5f97\u5230\u4e86\u9a8c\u8bc1\u3002"}}
{"id": "2509.18143", "categories": ["cs.ET", "cs.AI", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2509.18143", "abs": "https://arxiv.org/abs/2509.18143", "authors": ["Mike Smart", "Sachin Maheshwari", "Himadri Singh Raghav", "Alexander Serb"], "title": "Weight Mapping Properties of a Dual Tree Single Clock Adiabatic Capacitive Neuron", "comment": "11 pages, 10 figures, 6 tables. This work has been submitted to the\n  IEEE for possible publication", "summary": "Dual Tree Single Clock (DTSC) Adiabatic Capacitive Neuron (ACN) circuits\noffer the potential for highly energy-efficient Artificial Neural Network (ANN)\ncomputation in full custom analog IC designs. The efficient mapping of\nArtificial Neuron (AN) abstract weights, extracted from the software-trained\nANNs, onto physical ACN capacitance values has, however, yet to be fully\nresearched. In this paper, we explore the unexpected hidden complexities,\nchallenges and properties of the mapping, as well as, the ramifications for IC\ndesigners in terms accuracy, design and implementation. We propose an optimal,\nAN to ACN methodology, that promotes smaller chip sizes and improved overall\nclassification accuracy, necessary for successful practical deployment. Using\nTensorFlow and Larq software frameworks, we train three different ANN networks\nand map their weights into the energy-efficient DTSC ACN capacitance value\ndomain to demonstrate 100% functional equivalency. Finally, we delve into the\nimpact of weight quantization on ACN performance using novel metrics related to\npractical IC considerations, such as IC floor space and comparator\ndecision-making efficacy.", "AI": {"tldr": "DTSC ACN\u7535\u8def\u5728ANN\u8ba1\u7b97\u4e2d\u5177\u6709\u9ad8\u80fd\u6548\u6f5c\u529b\uff0c\u4f46\u5c06AN\u6743\u91cd\u6620\u5c04\u5230ACN\u7535\u5bb9\u503c\u4ecd\u6709\u5f85\u7814\u7a76\u3002\u672c\u6587\u63a2\u8ba8\u4e86\u6620\u5c04\u7684\u590d\u6742\u6027\u3001\u6311\u6218\u548c\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u4f18\u5316\u65b9\u6cd5\uff0c\u53ef\u51cf\u5c0f\u82af\u7247\u5c3a\u5bf8\u5e76\u63d0\u9ad8\u5206\u7c7b\u51c6\u786e\u6027\u3002\u6211\u4eec\u4f7f\u7528TensorFlow\u548cLarq\u8bad\u7ec3\u4e86\u4e09\u4e2aANN\u7f51\u7edc\uff0c\u5e76\u5c06\u5176\u6743\u91cd\u6620\u5c04\u5230DTSC ACN\u7535\u5bb9\u503c\u57df\uff0c\u9a8c\u8bc1\u4e86100%\u7684\u529f\u80fd\u7b49\u6548\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u7814\u7a76\u4e86\u6743\u91cd\u91cf\u5316\u5bf9ACN\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684IC\u8bbe\u8ba1\u6307\u6807\u3002", "motivation": "\u5c06\u8f6f\u4ef6\u8bad\u7ec3\u7684ANN\u4e2d\u7684\u62bd\u8c61\u6743\u91cd\u6709\u6548\u6620\u5c04\u5230\u7269\u7406ACN\u7535\u5bb9\u503c\u662f\u4e00\u4e2a\u672a\u88ab\u5145\u5206\u7814\u7a76\u7684\u9886\u57df\uff0c\u5177\u6709\u6f5c\u5728\u7684\u6311\u6218\u548c\u5f71\u54cd\uff0c\u9700\u8981\u89e3\u51b3\u4ee5\u5b9e\u73b0\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u7814\u7a76\u5c06AN\u6743\u91cd\u6620\u5c04\u5230ACN\u7535\u5bb9\u503c\u7684\u590d\u6742\u6027\u3001\u6311\u6218\u548c\u7279\u6027\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u4f18\u5316\u7684AN\u5230ACN\u6620\u5c04\u65b9\u6cd5\u3002\u4f7f\u7528TensorFlow\u548cLarq\u8bad\u7ec3\u4e86\u4e09\u4e2aANN\u7f51\u7edc\uff0c\u5e76\u5c06\u5b83\u4eec\u7684\u6743\u91cd\u6620\u5c04\u5230DTSC ACN\u7535\u5bb9\u503c\u57df\u3002\u5f15\u5165\u4e86\u4e0eIC\u8bbe\u8ba1\u76f8\u5173\u7684\u6027\u80fd\u6307\u6807\uff0c\u5982\u82af\u7247\u9762\u79ef\u548c\u6bd4\u8f83\u5668\u51b3\u7b56\u6548\u7387\u3002", "result": "\u6210\u529f\u5730\u5c06\u4e09\u4e2aANN\u7f51\u7edc\u7684\u6743\u91cd\u6620\u5c04\u5230DTSC ACN\u7535\u5bb9\u503c\u57df\uff0c\u5e76\u9a8c\u8bc1\u4e86100%\u7684\u529f\u80fd\u7b49\u6548\u6027\u3002\u63d0\u51fa\u7684\u4f18\u5316\u65b9\u6cd5\u6709\u671b\u51cf\u5c0f\u82af\u7247\u5c3a\u5bf8\u5e76\u63d0\u9ad8\u5206\u7c7b\u51c6\u786e\u6027\u3002\u7814\u7a76\u4e86\u6743\u91cd\u91cf\u5316\u5bf9ACN\u6027\u80fd\u7684\u5f71\u54cd\u3002", "conclusion": "\u5c06AN\u6743\u91cd\u6620\u5c04\u5230ACN\u7535\u5bb9\u503c\u5177\u6709\u9690\u85cf\u7684\u590d\u6742\u6027\uff0c\u4f46\u901a\u8fc7\u4f18\u5316\u7684\u6620\u5c04\u65b9\u6cd5\u53ef\u4ee5\u514b\u670d\u8fd9\u4e9b\u6311\u6218\u3002\u8fd9\u79cd\u65b9\u6cd5\u53ef\u4ee5\u5b9e\u73b0\u9ad8\u80fd\u6548\u7684ANN\u8ba1\u7b97\uff0c\u5e76\u4e3aIC\u8bbe\u8ba1\u5e26\u6765\u66f4\u5c0f\u7684\u82af\u7247\u5c3a\u5bf8\u548c\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u3002\u5bf9\u6743\u91cd\u91cf\u5316\u5bf9IC\u8bbe\u8ba1\u8003\u8651\u56e0\u7d20\u7684\u5f71\u54cd\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u662f\u5fc5\u8981\u7684\u3002"}}
{"id": "2509.18343", "categories": ["cs.GT", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.18343", "abs": "https://arxiv.org/abs/2509.18343", "authors": ["Joel Miller", "E. Glen Weyl", "Chris Kanich"], "title": "Fair Decisions through Plurality: Results from a Crowdfunding Platform", "comment": "Appearing in the Fifth ACM Conference on Equity and Access in\n  Algorithms, Mechanisms, and Optimization (EAAMO '25)", "summary": "We discuss an algorithmic intervention aimed at increasing equity and\neconomic efficiency at a crowdfunding platform that gives cash subsidies to\ngrantees. Through a blend of technical and qualitative methods, we show that\nthe previous algorithm used by the platform -- Quadratic Funding (QF) --\nsuffered problems because its design was rooted in a model of individuals as\nisolated and selfish. We present an alternative algorithm --\nConnection-Oriented Quadratic Funding (CO-QF) -- rooted in a theory of\nplurality and prosocial utilities, and show that it qualitatively and\nquantitatively performs better than QF. CO-QF has achieved an 89% adoption rate\nat the platform and has distributed over $4 Million to date. In simulations we\nshow that it provides better social welfare than QF. While our design for CO-QF\nwas responsive to the needs of a specific community, we also extrapolate out of\nthis context to show that CO-QF is a potentially helpful tool for\ngeneral-purpose public decision making.", "AI": {"tldr": "QF \u7b97\u6cd5\u5b58\u5728\u95ee\u9898\uff0cCO-QF \u7b97\u6cd5\u5728\u6548\u7387\u548c\u516c\u5e73\u6027\u65b9\u9762\u8868\u73b0\u66f4\u597d\u3002", "motivation": "\u63d0\u9ad8\u4f17\u7b79\u5e73\u53f0\u7684\u516c\u5e73\u6027\u548c\u7ecf\u6d4e\u6548\u7387\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u9762\u5411\u8fde\u63a5\u7684\u4e8c\u6b21\u65b9\u8d44\u52a9\u201d\uff08CO-QF\uff09\u7684\u65b0\u7b97\u6cd5\uff0c\u5e76\u4e0e\u4e4b\u524d\u7684\u201c\u65b9 QF\u201d\u7b97\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "result": "CO-QF \u7b97\u6cd5\u7684\u91c7\u7528\u7387\u4e3a 89%\uff0c\u5df2\u5206\u914d\u8d85\u8fc7 400 \u4e07\u7f8e\u5143\u3002\u6a21\u62df\u663e\u793a CO-QF \u63d0\u4f9b\u4e86\u6bd4 QF \u66f4\u597d\u7684\u793e\u4f1a\u798f\u5229\u3002", "conclusion": "CO-QF \u7b97\u6cd5\u662f\u4e00\u79cd\u6bd4 QF \u66f4\u6709\u5229\u7684\u4f17\u7b79\u5e73\u53f0\u7b97\u6cd5\uff0c\u5e76\u4e14\u53ef\u80fd\u662f\u4e00\u79cd\u901a\u7528\u7684\u516c\u5171\u51b3\u7b56\u5de5\u5177\u3002"}}
{"id": "2509.18355", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18355", "abs": "https://arxiv.org/abs/2509.18355", "authors": ["P. Ramkumar", "S. S. Bharadwaj"], "title": "Chiplet-Based RISC-V SoC with Modular AI Acceleration", "comment": "3 pages, 3 figures and 2 tables", "summary": "Achieving high performance, energy efficiency, and cost-effectiveness while\nmaintaining architectural flexibility is a critical challenge in the\ndevelopment and deployment of edge AI devices. Monolithic SoC designs struggle\nwith this complex balance mainly due to low manufacturing yields (below 16%) at\nadvanced 360 mm^2 process nodes. This paper presents a novel chiplet-based\nRISC-V SoC architecture that addresses these limitations through modular AI\nacceleration and intelligent system level optimization. Our proposed design\nintegrates 4 different key innovations in a 30mm x 30mm silicon interposer:\nadaptive cross-chiplet Dynamic Voltage and Frequency Scaling (DVFS); AI-aware\nUniversal Chiplet Interconnect Express (UCIe) protocol extensions featuring\nstreaming flow control units and compression-aware transfers; distributed\ncryptographic security across heterogeneous chiplets; and intelligent\nsensor-driven load migration. The proposed architecture integrates a 7nm RISC-V\nCPU chiplet with dual 5nm AI accelerators (15 TOPS INT8 each), 16GB HBM3 memory\nstacks, and dedicated power management controllers. Experimental results across\nindustry standard benchmarks like MobileNetV2, ResNet-50 and real-time video\nprocessing demonstrate significant performance improvements. The AI-optimized\nconfiguration achieves ~14.7% latency reduction, 17.3% throughput improvement,\nand 16.2% power reduction compared to previous basic chiplet implementations.\nThese improvements collectively translate to a 40.1% efficiency gain\ncorresponding to ~3.5 mJ per MobileNetV2 inference (860 mW/244 images/s), while\nmaintaining sub-5ms real-time capability across all experimented workloads.\nThese performance upgrades demonstrate that modular chiplet designs can achieve\nnear-monolithic computational density while enabling cost efficiency,\nscalability and upgradeability, crucial for next-generation edge AI device\napplications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e chiplet \u7684 RISC-V SoC \u67b6\u6784\uff0c\u901a\u8fc7\u6a21\u5757\u5316 AI \u52a0\u901f\u548c\u667a\u80fd\u7cfb\u7edf\u7ea7\u4f18\u5316\u6765\u89e3\u51b3\u9ad8\u6027\u80fd\u3001\u9ad8\u80fd\u6548\u548c\u6210\u672c\u6548\u76ca\u7684\u6311\u6218\uff0c\u5e76\u5728\u884c\u4e1a\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u793a\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u5728\u5148\u8fdb\u7684 360 mm^2 \u5de5\u827a\u8282\u70b9\u4e0a\uff0c\u5355\u7247 SoC \u8bbe\u8ba1\u7684\u4f4e\u826f\u7387\uff08\u4f4e\u4e8e 16%\uff09\u963b\u788d\u4e86\u5728\u4fdd\u6301\u67b6\u6784\u7075\u6d3b\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u6027\u80fd\u3001\u9ad8\u80fd\u6548\u548c\u6210\u672c\u6548\u76ca\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5305\u542b 4 \u9879\u5173\u952e\u521b\u65b0\u7684 chiplet \u67b6\u6784\uff1a\u81ea\u9002\u5e94\u8de8 chiplet \u52a8\u6001\u7535\u538b\u548c\u9891\u7387\u8c03\u6574 (DVFS)\uff1b\u652f\u6301\u6d41\u63a7\u5355\u5143\u548c\u538b\u7f29\u611f\u77e5\u4f20\u8f93\u7684 AI \u611f\u77e5\u901a\u7528 chiplet \u4e92\u8054 Express (UCIe) \u534f\u8bae\u6269\u5c55\uff1b\u8de8\u5f02\u6784 chiplet \u7684\u5206\u5e03\u5f0f\u52a0\u5bc6\u5b89\u5168\uff1b\u4ee5\u53ca\u667a\u80fd\u4f20\u611f\u5668\u9a71\u52a8\u7684\u8d1f\u8f7d\u8fc1\u79fb\u3002\u8be5\u67b6\u6784\u96c6\u6210\u4e86 7nm RISC-V CPU chiplet\u3001\u53cc 5nm AI \u52a0\u901f\u5668\uff08\u5404 15 TOPS INT8\uff09\u300116GB HBM3 \u5185\u5b58\u5806\u6808\u548c\u4e13\u7528\u7535\u6e90\u7ba1\u7406\u63a7\u5236\u5668\u3002", "result": "\u4e0e\u4e4b\u524d\u7684 basic chiplet \u5b9e\u73b0\u76f8\u6bd4\uff0cAI \u4f18\u5316\u914d\u7f6e\u5b9e\u73b0\u4e86\u7ea6 14.7% \u7684\u5ef6\u8fdf\u964d\u4f4e\u300117.3% \u7684\u541e\u5410\u91cf\u63d0\u9ad8\u548c 16.2% \u7684\u529f\u8017\u964d\u4f4e\uff0c\u6574\u4f53\u6548\u7387\u63d0\u9ad8\u4e86 40.1%\uff08MobileNetV2 \u63a8\u7406\u7ea6\u4e3a 3.5 mJ\uff0c\u529f\u8017\u4e3a 860 mW\uff0c\u541e\u5410\u91cf\u4e3a 244 images/s\uff09\uff0c\u540c\u65f6\u5728\u6240\u6709\u5b9e\u9a8c\u5de5\u4f5c\u8d1f\u8f7d\u4e2d\u4fdd\u6301\u4e86\u4f4e\u4e8e 5ms \u7684\u5b9e\u65f6\u80fd\u529b\u3002", "conclusion": "\u6a21\u5757\u5316 chiplet \u8bbe\u8ba1\u53ef\u4ee5\u5b9e\u73b0\u63a5\u8fd1\u5355\u7247 SoC \u7684\u8ba1\u7b97\u5bc6\u5ea6\uff0c\u540c\u65f6\u63d0\u4f9b\u6210\u672c\u6548\u76ca\u3001\u53ef\u6269\u5c55\u6027\u548c\u53ef\u5347\u7ea7\u6027\uff0c\u8fd9\u5bf9\u4e8e\u4e0b\u4e00\u4ee3\u8fb9\u7f18 AI \u8bbe\u5907\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2509.18363", "categories": ["cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2509.18363", "abs": "https://arxiv.org/abs/2509.18363", "authors": ["K. V. Samokhin", "M. Sigrist", "M. H. Fischer"], "title": "Spin currents in crystals with spin-orbit coupling: multi-band effects in an effective Hamiltonian formalism", "comment": "20 pages, 2 figures", "summary": "When focusing on a few essential bands in an effective description of a\nmaterial to calculate observable quantities, the respective operators have to\nbe adjusted accordingly. Ignoring contributions arising from integrating out\nremote bands can lead to qualitatively wrong results. We present a detailed\nanalysis of the interband mixing effects on spin currents. Specifically, we\ncalculate the intrinsic spin current in a time-reversal invariant\nnoncentrosymmetric crystal in the presence of electron-lattice spin-orbit\ncoupling. Starting from formally exact microscopic expressions, we derive the\nspin current operator restricted to one or more essential bands by iterative\nelimination of the contributions from distant bands. We show that the standard\ndefinition of the spin current operator in terms of the group velocity obtained\nfrom an effective band Hamiltonian cannot be justified using a microscopic\ntheory. The modified expression for the spin current operator contains\nadditional terms, which dominate the equilibrium spin current in a uniform\ncrystal. We show that the magnitude of these additional terms can considerably\nexceed the spin current obtained using the standard definition.", "AI": {"tldr": "\u901a\u8fc7\u8fed\u4ee3\u6d88\u9664\u8fdc\u7a0b\u5e26\u7684\u8d21\u732e\uff0c\u6211\u4eec\u63a8\u5bfc\u4e86\u4ec5\u9650\u4e8e\u4e00\u4e2a\u6216\u591a\u4e2a\u57fa\u672c\u5e26\u7684\u81ea\u65cb\u6d41\u7b97\u7b26\uff0c\u5e76\u8868\u660e\u5176\u4fee\u6b63\u8868\u8fbe\u5f0f\u53ef\u4ee5\u4e3b\u5bfc\u5e73\u8861\u81ea\u65cb\u6d41\u3002", "motivation": "\u5ffd\u7565\u4ece\u8fdc\u7a0b\u5e26\u79ef\u5206\u4e2d\u5f97\u51fa\u7684\u8d21\u732e\u53ef\u80fd\u5bfc\u81f4\u5b9a\u6027\u9519\u8bef\u7684\u7ed3\u679c\uff0c\u56e0\u6b64\u9700\u8981\u5206\u6790\u5e26\u95f4\u6df7\u5408\u6548\u5e94\u3002", "method": "\u4ece\u5f62\u5f0f\u4e0a\u7cbe\u786e\u7684\u5fae\u89c2\u8868\u8fbe\u5f0f\u51fa\u53d1\uff0c\u901a\u8fc7\u8fed\u4ee3\u6d88\u9664\u8fdc\u7a0b\u5e26\u7684\u8d21\u732e\uff0c\u63a8\u5bfc\u51fa\u4ec5\u9650\u4e8e\u4e00\u4e2a\u6216\u591a\u4e2a\u57fa\u672c\u5e26\u7684\u81ea\u65cb\u6d41\u7b97\u7b26\u3002", "result": "\u6807\u51c6\u81ea\u65cb\u6d41\u7b97\u7b26\u7684\u5b9a\u4e49\u65e0\u6cd5\u7528\u5fae\u89c2\u7406\u8bba\u8bc1\u660e\uff0c\u4fee\u6b63\u540e\u7684\u7b97\u7b26\u5305\u542b\u989d\u5916\u7684\u3001\u53ef\u4ee5\u4e3b\u5bfc\u5e73\u8861\u81ea\u65cb\u6d41\u7684\u9879\uff0c\u5e76\u4e14\u5176\u5e45\u5ea6\u53ef\u4ee5\u663e\u8457\u8d85\u8fc7\u6807\u51c6\u5b9a\u4e49\u5f97\u5230\u7684\u81ea\u65cb\u6d41\u3002", "conclusion": "\u5fc5\u987b\u8c03\u6574\u7b97\u7b26\u4ee5\u9002\u5e94\u6240\u9009\u7684\u57fa\u672c\u5e26\uff0c\u5e76\u4e14\u5fc5\u987b\u8003\u8651\u5e26\u95f4\u6df7\u5408\u6548\u5e94\u4ee5\u83b7\u5f97\u51c6\u786e\u7684\u81ea\u65cb\u6d41\u8ba1\u7b97\u3002"}}
{"id": "2509.18614", "categories": ["quant-ph", "cs.NA", "math.NA", "q-fin.CP", "stat.CO"], "pdf": "https://arxiv.org/pdf/2509.18614", "abs": "https://arxiv.org/abs/2509.18614", "authors": ["Jose Blanchet", "Mark S. Squillante", "Mario Szegedy", "Guanyang Wang"], "title": "Connecting Quantum Computing with Classical Stochastic Simulation", "comment": "15 pages, tutorial paper prepared for the 2025 Winter Simulation\n  Conference", "summary": "This tutorial paper introduces quantum approaches to Monte Carlo computation\nwith applications in computational finance. We outline the basics of quantum\ncomputing using Grover's algorithm for unstructured search to build intuition.\nWe then move slowly to amplitude estimation problems and applications to\ncounting and Monte Carlo integration, again using Grover-type iterations. A\nhands-on Python/Qiskit implementation illustrates these concepts applied to\nfinance. The paper concludes with a discussion on current challenges in scaling\nquantum simulation techniques.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u91cf\u5b50\u8ba1\u7b97\u5728\u8499\u7279\u5361\u6d1b\u8ba1\u7b97\u4e2d\u7684\u5e94\u7528\uff0c\u7279\u522b\u662f\u5728\u8ba1\u7b97\u91d1\u878d\u9886\u57df\u3002", "motivation": "\u4ecb\u7ecd\u91cf\u5b50\u8ba1\u7b97\u5728\u8499\u7279\u5361\u6d1b\u8ba1\u7b97\u4e2d\u7684\u5e94\u7528\uff0c\u7279\u522b\u662f\u5728\u8ba1\u7b97\u91d1\u878d\u9886\u57df\u3002", "method": "\u4eceGrover\u7b97\u6cd5\u5165\u624b\uff0c\u4ecb\u7ecd\u5e45\u503c\u4f30\u7b97\u95ee\u9898\uff0c\u5e76\u5e94\u7528\u4e8e\u8ba1\u6570\u548c\u8499\u7279\u5361\u6d1b\u79ef\u5206\uff0c\u6700\u540e\u901a\u8fc7Python/Qiskit\u5b9e\u73b0\u8fdb\u884c\u6f14\u793a\u3002", "result": "\u901a\u8fc7Python/Qiskit\u5b9e\u73b0\u6f14\u793a\u4e86\u91cf\u5b50\u8499\u7279\u5361\u6d1b\u8ba1\u7b97\u5728\u91d1\u878d\u9886\u57df\u7684\u5e94\u7528\u3002", "conclusion": "\u8ba8\u8bba\u4e86\u5f53\u524d\u5728\u6269\u5c55\u91cf\u5b50\u6a21\u62df\u6280\u672f\u65b9\u9762\u9762\u4e34\u7684\u6311\u6218\u3002"}}
{"id": "2509.18321", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2509.18321", "abs": "https://arxiv.org/abs/2509.18321", "authors": ["David Breitbach", "Moritz Bechberger", "Hanadi Mortada", "Bj\u00f6rn Heinz", "Roman Verba", "Qi Wang", "Carsten Dubs", "Mario Carpentieri", "Giovanni Finocchio", "Davi Rodrigues", "Alexandre Abbass Hamadeh", "Philipp Pirro"], "title": "All-magnonic neurons for analog artificial neural networks", "comment": "17 pages, 6 figures", "summary": "Analog neuromorphic hardware is gaining traction as conventional digital\nsystems struggle to keep pace with the growing energy and scalability demands\nof modern neural networks. Here, we present analog, fully magnonic, artificial\nneurons, which exploit a nonlinear magnon excitation mechanism based on the\nnonlinear magnonic frequency shift. This yields a sharp trigger response and\ntunable fading memory, as well as synaptic connections to other neurons via\npropagating magnons. Using micro-focused Brillouin light scattering\nspectroscopy on a Gallium-substituted yttrium iron garnet thin film, we show\nmulti-neuron triggering, cascadability, and multi-input integration across\ninterconnected neurons. Finally, we implement the experimentally verified\nneuron activation function in a neural network simulation, yielding high\nclassification accuracy on standard benchmarks. The results establish\nall-magnonic neurons as promising devices for scalable, low-power, wave-based\nneuromorphic computing, highlighting their potential as building blocks for\nfuture physical neural networks.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5168\u78c1\u6027\u6a21\u62df\u795e\u7ecf\u5143\uff0c\u5229\u7528\u975e\u7ebf\u6027\u78c1\u9891\u79fb\u5b9e\u73b0\u89e6\u53d1\u548c\u8bb0\u5fc6\u529f\u80fd\uff0c\u5e76\u53ef\u901a\u8fc7\u4f20\u64ad\u78c1\u632f\u5b50\u5b9e\u73b0\u7a81\u89e6\u8fde\u63a5\uff0c\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u9ad8\u5206\u7c7b\u7cbe\u5ea6\uff0c\u6709\u671b\u7528\u4e8e\u4f4e\u529f\u8017\u3001\u53ef\u6269\u5c55\u7684\u6ce2\u57fa\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u3002", "motivation": "\u4f20\u7edf\u6570\u5b57\u7cfb\u7edf\u5728\u5904\u7406\u73b0\u4ee3\u795e\u7ecf\u7f51\u7edc\u7684\u80fd\u8017\u548c\u53ef\u6269\u5c55\u6027\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u6a21\u62df\u795e\u7ecf\u5f62\u6001\u786c\u4ef6\u6b63\u53d7\u5230\u5173\u6ce8\u3002", "method": "\u5229\u7528\u9553\u53d6\u4ee3\u7684\u9487\u94c1\u77f3\u69b4\u77f3\u8584\u819c\u4e0a\u7684\u975e\u7ebf\u6027\u78c1\u9891\u79fb\uff0c\u5b9e\u73b0\u5168\u78c1\u6027\u6a21\u62df\u795e\u7ecf\u5143\uff0c\u5e76\u901a\u8fc7\u5fae\u7126\u5e03\u91cc\u6e0a\u5149\u6563\u5c04\u5149\u8c31\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u5305\u62ec\u591a\u795e\u7ecf\u5143\u89e6\u53d1\u3001\u7ea7\u8054\u548c\u591a\u8f93\u5165\u96c6\u6210\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u591a\u795e\u7ecf\u5143\u89e6\u53d1\u3001\u7ea7\u8054\u548c\u591a\u8f93\u5165\u96c6\u6210\uff0c\u5e76\u5728\u795e\u7ecf\u7f51\u7edc\u6a21\u62df\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u5206\u7c7b\u7cbe\u5ea6\u3002", "conclusion": "\u5168\u78c1\u6027\u795e\u7ecf\u5143\u4f5c\u4e3a\u6709\u6f5c\u529b\u7684\u5668\u4ef6\uff0c\u53ef\u7528\u4e8e\u6784\u5efa\u4f4e\u529f\u8017\u3001\u53ef\u6269\u5c55\u7684\u6ce2\u57fa\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u7cfb\u7edf\uff0c\u5e76\u4f5c\u4e3a\u672a\u6765\u7269\u7406\u795e\u7ecf\u7f51\u7edc\u7684\u6784\u5efa\u6a21\u5757\u3002"}}
{"id": "2509.18303", "categories": ["cs.SI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.18303", "abs": "https://arxiv.org/abs/2509.18303", "authors": ["Ozgur Can Seckin", "Bao Tran Truong", "Alessandro Flammini", "Filippo Menczer"], "title": "Identifying Constructive Conflict in Online Discussions through Controversial yet Toxicity Resilient Posts", "comment": "14 pages, 7 figures, 4 tables, will be published in ICWSM 2026", "summary": "Bridging content that brings together individuals with opposing viewpoints on\nsocial media remains elusive, overshadowed by echo chambers and toxic\nexchanges. We propose that algorithmic curation could surface such content by\nconsidering constructive conflicts as a foundational criterion. We\noperationalize this criterion through controversiality to identify challenging\ndialogues and toxicity resilience to capture respectful conversations. We\ndevelop high-accuracy models to capture these dimensions. Analyses based on\nthese models demonstrate that assessing resilience to toxic responses is not\nthe same as identifying low-toxicity posts. We also find that political posts\nare often controversial and tend to attract more toxic responses. However, some\nposts, even the political ones, are resilient to toxicity despite being highly\ncontroversial, potentially sparking civil engagement. Toxicity resilient posts\ntend to use politeness cues, such as showing gratitude and hedging. These\nfindings suggest the potential for framing the tone of posts to encourage\nconstructive political discussions.", "AI": {"tldr": "\u7b97\u6cd5\u63a8\u8350\u53ef\u4ee5\u901a\u8fc7\u8003\u8651\u5efa\u8bbe\u6027\u51b2\u7a81\u6765\u4fc3\u8fdb\u793e\u4ea4\u5a92\u4f53\u4e0a\u4e0d\u540c\u89c2\u70b9\u4e4b\u95f4\u7684\u5bf9\u8bdd\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u4e0a\u7684\u56de\u97f3\u5ba4\u548c\u6709\u6bd2\u7684\u4ea4\u6d41\u963b\u788d\u4e86\u4e0d\u540c\u89c2\u70b9\u4e4b\u95f4\u7684\u5bf9\u8bdd\uff0c\u800c\u7b97\u6cd5\u63a8\u8350\u6709\u53ef\u80fd\u901a\u8fc7\u5c06\u5efa\u8bbe\u6027\u51b2\u7a81\u4f5c\u4e3a\u4e00\u4e2a\u57fa\u672c\u6807\u51c6\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u201c\u4e89\u8bae\u6027\u201d\u548c\u201c\u6bd2\u6027\u62b5\u6297\u529b\u201d\u4e24\u4e2a\u7ef4\u5ea6\u6765\u64cd\u4f5c\u5316\u201c\u5efa\u8bbe\u6027\u51b2\u7a81\u201d\u8fd9\u4e00\u6807\u51c6\uff0c\u5206\u522b\u7528\u4e8e\u8bc6\u522b\u6311\u6218\u6027\u5bf9\u8bdd\u548c\u5c0a\u91cd\u6027\u5bf9\u8bdd\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u8bc4\u4f30\u5bf9\u6709\u6bd2\u56de\u5e94\u7684\u62b5\u6297\u529b\u4e0d\u540c\u4e8e\u8bc6\u522b\u4f4e\u6bd2\u6027\u5e16\u5b50\u3002\u653f\u6cbb\u5e16\u5b50\u901a\u5e38\u66f4\u5177\u4e89\u8bae\u6027\uff0c\u5e76\u4e14\u66f4\u5bb9\u6613\u5438\u5f15\u6709\u6bd2\u56de\u5e94\u3002\u7136\u800c\uff0c\u4e00\u4e9b\u5e16\u5b50\uff0c\u5373\u4f7f\u662f\u653f\u6cbb\u5e16\u5b50\uff0c\u5c3d\u7ba1\u4e89\u8bae\u5f88\u5927\uff0c\u4f46\u5374\u80fd\u62b5\u6297\u6bd2\u6027\uff0c\u5e76\u6709\u53ef\u80fd\u5f15\u53d1\u516c\u6c11\u53c2\u4e0e\u3002\u6bd2\u6027\u62b5\u6297\u6027\u5e16\u5b50\u503e\u5411\u4e8e\u4f7f\u7528\u793c\u8c8c\u7684\u7ebf\u7d22\uff0c\u4f8b\u5982\u8868\u793a\u611f\u8c22\u548c\u8c28\u614e\u7528\u8bcd\u3002", "conclusion": "\u901a\u8fc7\u5bf9\u5e16\u5b50\u8bed\u6c14\u8fdb\u884c\u8c03\u6574\uff0c\u6709\u53ef\u80fd\u9f13\u52b1\u5efa\u8bbe\u6027\u7684\u653f\u6cbb\u8ba8\u8bba\u3002"}}
{"id": "2509.18381", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.18381", "abs": "https://arxiv.org/abs/2509.18381", "authors": ["Nicholas L. K. Goradia", "Harpreet S. Dhillon", "R. Michael Buehrer"], "title": "Multi-Target Detection for Cognitive MIMO Radar Networks", "comment": "12 pages, 16 figures", "summary": "In this work, we develop centralized and decentralized signal fusion\ntechniques for constant false alarm rate (CFAR) multi-target detection with a\ncognitive radar network in unknown noise and clutter distributions. Further, we\nfirst develop a detection statistic for co-located monostatic MIMO radar in\nunknown noise and clutter distributions which is asymptotically CFAR as the\nnumber of received pulses over all antennas grows large, and we provide\nconditions under which this detection statistic is valid. We leverage\nreinforcement learning (RL) for improved multi-target detection performance,\nwhere the radar learns likely target locations in a search area. These results\nare then generalized to the setting of cognitive radar networks, where radars\ncollaborate to learn where targets are likely to appear in a search area. We\nshow a fundamental tradeoff between the spatial and temporal domain for CFAR\ndetection in unknown noise and clutter distributions; in other words, we show a\ntradeoff between the number of radar antennas and the number of temporal\nsamples. We show the benefits and tradeoffs with centralized and decentralized\ndetection with a network of cognitive radars.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u8ba4\u77e5\u96f7\u8fbe\u7f51\u7edc\u4e2d\u6052\u865a\u8b66\u7387\uff08CFAR\uff09\u591a\u76ee\u6807\u68c0\u6d4b\u7684\u4fe1\u53f7\u878d\u5408\u6280\u672f\uff0c\u5e76\u5229\u7528\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u6765\u4f18\u5316\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u5728\u672a\u77e5\u7684\u566a\u58f0\u548c\u6742\u6ce2\u5206\u5e03\u4e0b\uff0c\u4e3a\u8ba4\u77e5\u96f7\u8fbe\u7f51\u7edc\u5f00\u53d1\u4e2d\u5fc3\u5316\u548c\u53bb\u4e2d\u5fc3\u5316\u7684CFAR\u591a\u76ee\u6807\u68c0\u6d4b\u4fe1\u53f7\u878d\u5408\u6280\u672f\uff0c\u5e76\u63a2\u7d22\u5176\u6027\u80fd\u3002", "method": "\u5f00\u53d1\u4e86\u7528\u4e8e\u672a\u77e5\u566a\u58f0\u548c\u6742\u6ce2\u5206\u5e03\u4e0b\u5171\u4f4d\u5355\u57fa\u5730MIMO\u96f7\u8fbe\u7684\u68c0\u6d4b\u7edf\u8ba1\u91cf\uff0c\u5e76\u63a8\u5e7f\u5230\u8ba4\u77e5\u96f7\u8fbe\u7f51\u7edc\uff1b\u5229\u7528\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4f7f\u96f7\u8fbe\u5b66\u4e60\u76ee\u6807\u53ef\u80fd\u51fa\u73b0\u7684\u4f4d\u7f6e\uff1b\u7814\u7a76\u4e86\u7a7a\u95f4\u57df\uff08\u96f7\u8fbe\u5929\u7ebf\u6570\u91cf\uff09\u548c\u65f6\u95f4\u57df\uff08\u65f6\u95f4\u91c7\u6837\u70b9\u6570\uff09\u4e4b\u95f4\u7684\u6743\u8861\u3002", "result": "\u8bc1\u660e\u4e86\u5728\u672a\u77e5\u566a\u58f0\u548c\u6742\u6ce2\u5206\u5e03\u4e0b\uff0cCFAR\u68c0\u6d4b\u5728\u7a7a\u95f4\u57df\u548c\u65f6\u95f4\u57df\u4e4b\u95f4\u5b58\u5728\u57fa\u672c\u6743\u8861\uff1b\u5c55\u793a\u4e86\u4e2d\u5fc3\u5316\u548c\u53bb\u4e2d\u5fc3\u5316\u68c0\u6d4b\u7684\u4f18\u52bf\u548c\u6743\u8861\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u4fe1\u53f7\u878d\u5408\u548c\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u63d0\u5347\u8ba4\u77e5\u96f7\u8fbe\u7f51\u7edc\u5728\u672a\u77e5\u566a\u58f0\u548c\u6742\u6ce2\u5206\u5e03\u4e0b\u7684\u591a\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\uff0c\u5e76\u63ed\u793a\u4e86\u7a7a\u95f4\u548c\u65f6\u95f4\u57df\u7684\u6743\u8861\u5173\u7cfb\u3002"}}
{"id": "2509.18311", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18311", "abs": "https://arxiv.org/abs/2509.18311", "authors": ["Benjamin A. Christie", "Sagar Parekh", "Dylan P. Losey"], "title": "Fine-Tuning Robot Policies While Maintaining User Privacy", "comment": null, "summary": "Recent works introduce general-purpose robot policies. These policies provide\na strong prior over how robots should behave -- e.g., how a robot arm should\nmanipulate food items. But in order for robots to match an individual person's\nneeds, users typically fine-tune these generalized policies -- e.g., showing\nthe robot arm how to make their own preferred dinners. Importantly, during the\nprocess of personalizing robots, end-users leak data about their preferences,\nhabits, and styles (e.g., the foods they prefer to eat). Other agents can\nsimply roll-out the fine-tuned policy and see these personally-trained\nbehaviors. This leads to a fundamental challenge: how can we develop robots\nthat personalize actions while keeping learning private from external agents?\nWe here explore this emerging topic in human-robot interaction and develop\nPRoP, a model-agnostic framework for personalized and private robot policies.\nOur core idea is to equip each user with a unique key; this key is then used to\nmathematically transform the weights of the robot's network. With the correct\nkey, the robot's policy switches to match that user's preferences -- but with\nincorrect keys, the robot reverts to its baseline behaviors. We show the\ngeneral applicability of our method across multiple model types in imitation\nlearning, reinforcement learning, and classification tasks. PRoP is practically\nadvantageous because it retains the architecture and behaviors of the original\npolicy, and experimentally outperforms existing encoder-based approaches. See\nvideos and code here: https://prop-icra26.github.io.", "AI": {"tldr": "PRoP\u662f\u4e00\u4e2a\u6a21\u578b\u65e0\u5173\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4e2a\u6027\u5316\u548c\u79c1\u6709\u673a\u5668\u4eba\u7b56\u7565\uff0c\u5b83\u4f7f\u7528\u552f\u4e00\u7684\u5bc6\u94a5\u6765\u8f6c\u6362\u7f51\u7edc\u6743\u91cd\uff0c\u4ee5\u5339\u914d\u7528\u6237\u7684\u504f\u597d\uff0c\u540c\u65f6\u9632\u6b62\u5916\u90e8\u4ee3\u7406\u8bbf\u95ee\u8fd9\u4e9b\u6570\u636e\u3002", "motivation": "\u73b0\u6709\u7684\u901a\u7528\u673a\u5668\u4eba\u7b56\u7565\u5728\u4e2a\u6027\u5316\u65f6\u4f1a\u6cc4\u9732\u7528\u6237\u7684\u504f\u597d\u6570\u636e\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u5728\u4e2a\u6027\u5316\u673a\u5668\u4eba\u52a8\u4f5c\u7684\u540c\u65f6\u4fdd\u6301\u5b66\u4e60\u79c1\u5bc6\u6027\u7684\u65b9\u6cd5\u3002", "method": "PRoP\u6846\u67b6\u901a\u8fc7\u4e3a\u6bcf\u4e2a\u7528\u6237\u914d\u5907\u4e00\u4e2a\u552f\u4e00\u7684\u5bc6\u94a5\u6765\u8f6c\u6362\u673a\u5668\u4eba\u7f51\u7edc\u6743\u91cd\u3002\u6b63\u786e\u7684\u5bc6\u94a5\u4f1a\u542f\u7528\u7528\u6237\u504f\u597d\u7684\u7b56\u7565\uff0c\u800c\u9519\u8bef\u7684\u5bc6\u94a5\u5219\u4f7f\u673a\u5668\u4eba\u6062\u590d\u5230\u57fa\u7ebf\u884c\u4e3a\u3002", "result": "PRoP\u6846\u67b6\u5728\u6a21\u4eff\u5b66\u4e60\u3001\u5f3a\u5316\u5b66\u4e60\u548c\u5206\u7c7b\u4efb\u52a1\u4e2d\u88ab\u8bc1\u660e\u5177\u6709\u5e7f\u6cdb\u7684\u9002\u7528\u6027\uff0c\u5e76\u4e14\u5728\u5b9e\u8df5\u4e2d\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u4e8e\u7f16\u7801\u5668\u7684\u65b9\u6cd5\u3002", "conclusion": "PRoP\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u79cd\u5728\u4e0d\u6cc4\u9732\u7528\u6237\u504f\u597d\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u673a\u5668\u4eba\u4e2a\u6027\u5316\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u539f\u59cb\u7b56\u7565\u7684\u67b6\u6784\u548c\u884c\u4e3a\u3002"}}
{"id": "2509.18160", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18160", "abs": "https://arxiv.org/abs/2509.18160", "authors": ["Akwasi Asare", "Isaac Baffour Senkyire", "Emmanuel Freeman", "Simon Hilary Ayinedenaba Aluze-Ele", "Kelvin Kwao"], "title": "PerceptronCARE: A Deep Learning-Based Intelligent Teleopthalmology Application for Diabetic Retinopathy Diagnosis", "comment": null, "summary": "Diabetic retinopathy is a leading cause of vision loss among adults and a\nmajor global health challenge, particularly in underserved regions. This study\npresents PerceptronCARE, a deep learning-based teleophthalmology application\ndesigned for automated diabetic retinopathy detection using retinal images. The\nsystem was developed and evaluated using multiple convolutional neural\nnetworks, including ResNet-18, EfficientNet-B0, and SqueezeNet, to determine\nthe optimal balance between accuracy and computational efficiency. The final\nmodel classifies disease severity with an accuracy of 85.4%, enabling real-time\nscreening in clinical and telemedicine settings. PerceptronCARE integrates\ncloud-based scalability, secure patient data management, and a multi-user\nframework, facilitating early diagnosis, improving doctor-patient interactions,\nand reducing healthcare costs. This study highlights the potential of AI-driven\ntelemedicine solutions in expanding access to diabetic retinopathy screening,\nparticularly in remote and resource-constrained environments.", "AI": {"tldr": "PerceptronCARE\u662f\u4e00\u4e2a\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u8fdc\u7a0b\u773c\u79d1\u5e94\u7528\u7a0b\u5e8f\uff0c\u4f7f\u7528\u89c6\u7f51\u819c\u56fe\u50cf\u81ea\u52a8\u68c0\u6d4b\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\uff0c\u51c6\u786e\u7387\u4e3a85.4%\uff0c\u53ef\u7528\u4e8e\u5b9e\u65f6\u7b5b\u67e5\u3002", "motivation": "\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\u662f\u5bfc\u81f4\u6210\u5e74\u4eba\u5931\u660e\u7684\u4e3b\u8981\u539f\u56e0\uff0c\u5e76\u4e14\u662f\u5168\u7403\u6027\u7684\u5065\u5eb7\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u670d\u52a1\u6b20\u7f3a\u7684\u5730\u533a\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u81ea\u52a8\u68c0\u6d4b\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\u7684\u8fdc\u7a0b\u773c\u79d1\u5e94\u7528\u7a0b\u5e8f\uff0c\u4ee5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u7814\u7a76\u4e2d\u4f7f\u7528\u4e86\u5305\u62ecResNet-18\u3001EfficientNet-B0\u548cSqueezeNet\u5728\u5185\u7684\u591a\u79cd\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff0c\u4ee5\u786e\u5b9a\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u7684\u6700\u4f73\u5e73\u8861\u3002\u6700\u7ec8\u6a21\u578b\u88ab\u7528\u4e8e\u75be\u75c5\u4e25\u91cd\u7a0b\u5ea6\u7684\u5206\u7c7b\u3002", "result": "\u6700\u7ec8\u6a21\u578b\u5728\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\u68c0\u6d4b\u4e2d\u7684\u51c6\u786e\u7387\u4e3a85.4%\uff0c\u80fd\u591f\u652f\u6301\u4e34\u5e8a\u548c\u8fdc\u7a0b\u533b\u7597\u73af\u5883\u4e2d\u7684\u5b9e\u65f6\u7b5b\u67e5\u3002PerceptronCARE\u96c6\u6210\u4e86\u4e91\u53ef\u6269\u5c55\u6027\u3001\u5b89\u5168\u7684\u6570\u636e\u7ba1\u7406\u548c\u591a\u7528\u6237\u6846\u67b6\u3002", "conclusion": "\u672c\u7814\u7a76\u5f3a\u8c03\u4e86\u4eba\u5de5\u667a\u80fd\u9a71\u52a8\u7684\u8fdc\u7a0b\u533b\u7597\u89e3\u51b3\u65b9\u6848\u5728\u6269\u5927\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\u7b5b\u67e5\u8303\u56f4\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u8d44\u6e90\u53d7\u9650\u548c\u504f\u8fdc\u5730\u533a\uff0c\u6709\u52a9\u4e8e\u65e9\u671f\u8bca\u65ad\u3001\u6539\u5584\u533b\u60a3\u4e92\u52a8\u5e76\u964d\u4f4e\u533b\u7597\u6210\u672c\u3002"}}
{"id": "2509.18205", "categories": ["quant-ph", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.18205", "abs": "https://arxiv.org/abs/2509.18205", "authors": ["HongZheng Liu", "YiNuo Tian", "Zhiyue Wu"], "title": "Structure-Fair Quantum Circuit Complexity: An Information-Theoretic Lower Bound", "comment": "41 pages (16 pages main text, 25 pages appendices), Comments welcome", "summary": "Fairly quantifying the complexity of quantum states that possess intrinsic\nstructures (such as symmetries or encodings) is a fundamental challenge for\nbenchmarking quantum technologies. We introduce the \"Reference-Contingent\nComplexity\" (RCC). Its core idea is to utilize the quantum relative entropy to\nmeasure the extent to which a state deviates from its \"structured vacuum\" (the\nmaximum entropy state within its constrained subspace), thereby only pricing\nthe generation of non-trivial information. We establish a central theorem,\nestablishing that the RCC is a rigorous information-theoretic lower bound on\nuniversal quantum circuit complexity, a bound that features a linear leading\nterm, a universal logarithmic correction, and a precise physical correction for\nthe non-uniformity of the spectral distribution. Furthermore, we establish an\noperational protocol based on quantum hypothesis testing, which connects this\ntheoretical lower bound to experimentally accessible observables. This work\nprovides a structurally-fair yardstick for quantum technologies and offers a\nnew perspective for exploring the intrinsic connection between computational\ncost and the generation of non-trivial information.", "AI": {"tldr": "\u91cf\u5b50\u4fe1\u606f\u8bba\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u91cf\u5b50\u6001\u590d\u6742\u5ea6\u5ea6\u91cf\u65b9\u6cd5-\u53c2\u8003\u4f9d\u8d56\u590d\u6742\u5ea6\uff08RCC\uff09\uff0c\u5b83\u80fd\u516c\u5e73\u5730\u91cf\u5316\u5177\u6709\u5185\u5728\u7ed3\u6784\uff08\u5982\u5bf9\u79f0\u6027\u6216\u7f16\u7801\uff09\u7684\u91cf\u5b50\u6001\u7684\u590d\u6742\u5ea6\uff0c\u5e76\u901a\u8fc7\u91cf\u5b50\u76f8\u5bf9\u71b5\u8861\u91cf\u91cf\u5b50\u6001\u504f\u79bb\u5176\u201c\u7ed3\u6784\u5316\u771f\u7a7a\u201d\u7684\u7a0b\u5ea6\uff0c\u4ece\u800c\u53ea\u8bc4\u4f30\u975e\u5e73\u51e1\u4fe1\u606f\u7684\u751f\u6210\u6210\u672c\u3002\u8be5\u65b9\u6cd5\u5728\u7406\u8bba\u4e0a\u5efa\u7acb\u4e86RCC\u4f5c\u4e3a\u901a\u7528\u91cf\u5b50\u7535\u8def\u590d\u6742\u5ea6\u7684\u4e25\u683c\u4fe1\u606f\u8bba\u4e0b\u754c\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u91cf\u5b50\u5047\u8bbe\u68c0\u9a8c\u7684\u64cd\u4f5c\u534f\u8bae\uff0c\u4f7f\u5176\u53ef\u88ab\u5b9e\u9a8c\u6d4b\u91cf\u3002\u8fd9\u4e3a\u91cf\u5b50\u6280\u672f\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7ed3\u6784\u516c\u5e73\u7684\u8bc4\u4ef7\u6807\u51c6\uff0c\u5e76\u52a0\u6df1\u4e86\u5bf9\u8ba1\u7b97\u6210\u672c\u4e0e\u975e\u5e73\u51e1\u4fe1\u606f\u751f\u6210\u4e4b\u95f4\u5185\u5728\u8054\u7cfb\u7684\u7406\u89e3\u3002", "motivation": "Fairly quantifying the complexity of quantum states that possess intrinsic structures (such as symmetries or encodings) is a fundamental challenge for benchmarking quantum technologies.", "method": "We introduce the \"Reference-Contingent Complexity\" (RCC). Its core idea is to utilize the quantum relative entropy to measure the extent to which a state deviates from its \"structured vacuum\" (the maximum entropy state within its constrained subspace), thereby only pricing the generation of non-trivial information. We establish a central theorem, establishing that the RCC is a rigorous information-theoretic lower bound on universal quantum circuit complexity, a bound that features a linear leading term, a universal logarithmic correction, and a precise physical correction for the non-uniformity of the spectral distribution. Furthermore, we establish an operational protocol based on quantum hypothesis testing, which connects this theoretical lower bound to experimentally accessible observables.", "result": "The RCC is a rigorous information-theoretic lower bound on universal quantum circuit complexity, featuring a linear leading term, a universal logarithmic correction, and a precise physical correction. An operational protocol based on quantum hypothesis testing connects this bound to experimentally accessible observables.", "conclusion": "This work provides a structurally-fair yardstick for quantum technologies and offers a new perspective for exploring the intrinsic connection between computational cost and the generation of non-trivial information."}}
{"id": "2509.18104", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18104", "abs": "https://arxiv.org/abs/2509.18104", "authors": ["Wenqian Li", "Youjia Yang", "Ruoxi Jia", "Yan Pang"], "title": "Data Valuation and Selection in a Federated Model Marketplace", "comment": null, "summary": "In the era of Artificial Intelligence (AI), marketplaces have become\nessential platforms for facilitating the exchange of data products to foster\ndata sharing. Model transactions provide economic solutions in data\nmarketplaces that enhance data reusability and ensure the traceability of data\nownership. To establish trustworthy data marketplaces, Federated Learning (FL)\nhas emerged as a promising paradigm to enable collaborative learning across\nsiloed datasets while safeguarding data privacy. However, effective data\nvaluation and selection from heterogeneous sources in the FL setup remain key\nchallenges. This paper introduces a comprehensive framework centered on a\nWasserstein-based estimator tailored for FL. The estimator not only predicts\nmodel performance across unseen data combinations but also reveals the\ncompatibility between data heterogeneity and FL aggregation algorithms. To\nensure privacy, we propose a distributed method to approximate Wasserstein\ndistance without requiring access to raw data. Furthermore, we demonstrate that\nmodel performance can be reliably extrapolated under the neural scaling law,\nenabling effective data selection without full-scale training. Extensive\nexperiments across diverse scenarios, such as label skew, mislabeled, and\nunlabeled sources, show that our approach consistently identifies\nhigh-performing data combinations, paving the way for more reliable FL-based\nmodel marketplaces.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8eWasserstein\u8ddd\u79bb\u7684\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u6570\u636e\u5e02\u573a\u4e2d\u8bc4\u4f30\u548c\u9009\u62e9\u6570\u636e\u6e90\uff0c\u4ee5\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u548c\u6570\u636e\u9690\u79c1\u6027\u3002", "motivation": "\u5728AI\u65f6\u4ee3\uff0c\u6570\u636e\u5e02\u573a\u548c\u6a21\u578b\u4ea4\u6613\u5bf9\u4e8e\u4fc3\u8fdb\u6570\u636e\u5171\u4eab\u548c\u53ef\u8ffd\u6eaf\u6027\u81f3\u5173\u91cd\u8981\u3002\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u5728\u4fdd\u62a4\u9690\u79c1\u65b9\u9762\u6709\u4f18\u52bf\uff0c\u4f46\u5982\u4f55\u6709\u6548\u8bc4\u4f30\u548c\u9009\u62e9\u5f02\u6784\u6570\u636e\u6e90\u4ecd\u662f\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8eWasserstein\u8ddd\u79bb\u7684\u4f30\u8ba1\u5668\uff0c\u7528\u4e8e\u9884\u6d4b\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u7ec4\u5408\u4e0b\u7684\u6027\u80fd\uff0c\u5e76\u63ed\u793a\u6570\u636e\u5f02\u6784\u6027\u4e0eFL\u805a\u5408\u7b97\u6cd5\u7684\u517c\u5bb9\u6027\u3002\u901a\u8fc7\u5206\u5e03\u5f0f\u65b9\u6cd5\u5728\u4e0d\u8bbf\u95ee\u539f\u59cb\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u8fd1\u4f3cWasserstein\u8ddd\u79bb\u3002\u5229\u7528\u795e\u7ecf\u7f29\u653e\u5b9a\u5f8b\u63a8\u65ad\u6a21\u578b\u6027\u80fd\uff0c\u5b9e\u73b0\u65e0\u9700\u5b8c\u5168\u8bad\u7ec3\u7684\u6570\u636e\u9009\u62e9\u3002", "result": "\u5728\u5305\u542b\u6807\u7b7e\u503e\u659c\u3001\u9519\u8bef\u6807\u7b7e\u548c\u65e0\u6807\u7b7e\u6570\u636e\u6e90\u7684\u591a\u79cd\u573a\u666f\u4e0b\uff0c\u8be5\u6846\u67b6\u80fd\u53ef\u9760\u5730\u8bc6\u522b\u51fa\u9ad8\u6027\u80fd\u7684\u6570\u636e\u7ec4\u5408\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u591f\u6709\u6548\u5e94\u5bf9FL\u4e2d\u7684\u6570\u636e\u8bc4\u4f30\u548c\u9009\u62e9\u6311\u6218\uff0c\u4e3a\u6784\u5efa\u66f4\u53ef\u9760\u7684\u57fa\u4e8eFL\u7684\u6a21\u578b\u5e02\u573a\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.18215", "categories": ["cs.AI", "cs.LO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.18215", "abs": "https://arxiv.org/abs/2509.18215", "authors": ["Timotheus Kampik", "Kristijonas \u010cyras", "Jos\u00e9 Ruiz Alarc\u00f3n"], "title": "Change in Quantitative Bipolar Argumentation: Sufficient, Necessary, and Counterfactual Explanations", "comment": "The publisher's version contains a notation glitch in Example 3, 5th\n  line, first sub-script G should be G'. This has always been G' in authors'\n  version. Thanks to J. Lanser for pointing this out", "summary": "This paper presents a formal approach to explaining change of inference in\nQuantitative Bipolar Argumentation Frameworks (QBAFs). When drawing conclusions\nfrom a QBAF and updating the QBAF to then again draw conclusions (and so on),\nour approach traces changes -- which we call strength inconsistencies -- in the\npartial order over argument strengths that a semantics establishes on some\narguments of interest, called topic arguments. We trace the causes of strength\ninconsistencies to specific arguments, which then serve as explanations. We\nidentify sufficient, necessary, and counterfactual explanations for strength\ninconsistencies and show that strength inconsistency explanations exist if and\nonly if an update leads to strength inconsistency. We define a heuristic-based\napproach to facilitate the search for strength inconsistency explanations, for\nwhich we also provide an implementation.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u5b9a\u91cf\u53cc\u6781\u8bba\u8bc1\u6846\u67b6\uff08QBAF\uff09\u4e2d\u89e3\u91ca\u63a8\u7406\u53d8\u5316\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8ffd\u8e2a\u8bba\u8bc1\u5f3a\u5ea6\u504f\u5e8f\u4e2d\u7684\u4e0d\u4e00\u81f4\u6027\uff08\u79f0\u4e3a\u5f3a\u5ea6\u4e0d\u4e00\u81f4\uff09\uff0c\u5e76\u5c06\u5176\u5f52\u56e0\u4e8e\u7279\u5b9a\u8bba\u8bc1\uff0c\u4ece\u800c\u63d0\u4f9b\u89e3\u91ca\u3002", "motivation": "\u5728\u5b9a\u91cf\u53cc\u6781\u8bba\u8bc1\u6846\u67b6\uff08QBAF\uff09\u4e2d\uff0c\u5f53\u8fdb\u884c\u63a8\u7406\u66f4\u65b0\u5e76\u518d\u6b21\u63a8\u7406\u65f6\uff0c\u7406\u89e3\u63a8\u7406\u7ed3\u679c\u53d8\u5316\u7684\u539f\u56e0\u662f\u4e00\u4e2a\u6311\u6218\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u901a\u8fc7\u8ffd\u8e2a\u548c\u89e3\u91ca\u63a8\u7406\u53d8\u5316\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8ffd\u8e2aQBAF\u4e2d\u5f3a\u5ea6\u4e0d\u4e00\u81f4\u6027\u7684\u65b9\u6cd5\uff0c\u5c06\u4e0d\u4e00\u81f4\u6027\u5f52\u56e0\u4e8e\u7279\u5b9a\u8bba\u8bc1\uff0c\u5e76\u8bc6\u522b\u5145\u5206\u3001\u5fc5\u8981\u548c\u53cd\u4e8b\u5b9e\u7684\u89e3\u91ca\u3002\u6b64\u5916\uff0c\u8fd8\u5b9a\u4e49\u4e86\u4e00\u79cd\u57fa\u4e8e\u542f\u53d1\u5f0f\u7684\u641c\u7d22\u65b9\u6cd5\u3002", "result": "\u8bc6\u522b\u4e86\u5f3a\u5ea6\u4e0d\u4e00\u81f4\u7684\u5145\u5206\u3001\u5fc5\u8981\u548c\u53cd\u4e8b\u5b9e\u7684\u89e3\u91ca\uff0c\u5e76\u8bc1\u660e\u4e86\u5f53\u4e14\u4ec5\u5f53\u66f4\u65b0\u5bfc\u81f4\u5f3a\u5ea6\u4e0d\u4e00\u81f4\u65f6\uff0c\u5f3a\u5ea6\u4e0d\u4e00\u81f4\u7684\u89e3\u91ca\u624d\u5b58\u5728\u3002\u8fd8\u5b9e\u73b0\u4e86\u4e00\u79cd\u7528\u4e8e\u641c\u7d22\u8fd9\u4e9b\u89e3\u91ca\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u5f62\u5f0f\u5316\u7684\u65b9\u6cd5\u6765\u89e3\u91caQBAF\u4e2d\u63a8\u7406\u7684\u53d8\u5316\uff0c\u901a\u8fc7\u8bc6\u522b\u548c\u5f52\u56e0\u5f3a\u5ea6\u4e0d\u4e00\u81f4\u6027\uff0c\u4e3a\u7406\u89e3\u548c\u8c03\u8bd5\u8bba\u8bc1\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002"}}
{"id": "2509.18123", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18123", "abs": "https://arxiv.org/abs/2509.18123", "authors": ["Yeonju Lee", "Rui Qi Chen", "Joseph Oboamah", "Po Nien Su", "Wei-zhen Liang", "Yeyin Shi", "Lu Gan", "Yongsheng Chen", "Xin Qiao", "Jing Li"], "title": "SPADE: A Large Language Model Framework for Soil Moisture Pattern Recognition and Anomaly Detection in Precision Agriculture", "comment": null, "summary": "Accurate interpretation of soil moisture patterns is critical for irrigation\nscheduling and crop management, yet existing approaches for soil moisture\ntime-series analysis either rely on threshold-based rules or data-hungry\nmachine learning or deep learning models that are limited in adaptability and\ninterpretability. In this study, we introduce SPADE (Soil moisture Pattern and\nAnomaly DEtection), an integrated framework that leverages large language\nmodels (LLMs) to jointly detect irrigation patterns and anomalies in soil\nmoisture time-series data. SPADE utilizes ChatGPT-4.1 for its advanced\nreasoning and instruction-following capabilities, enabling zero-shot analysis\nwithout requiring task-specific annotation or fine-tuning. By converting\ntime-series data into a textual representation and designing domain-informed\nprompt templates, SPADE identifies irrigation events, estimates net irrigation\ngains, detects, classifies anomalies, and produces structured, interpretable\nreports. Experiments were conducted on real-world soil moisture sensor data\nfrom commercial and experimental farms cultivating multiple crops across the\nUnited States. Results demonstrate that SPADE outperforms the existing method\nin anomaly detection, achieving higher recall and F1 scores and accurately\nclassifying anomaly types. Furthermore, SPADE achieved high precision and\nrecall in detecting irrigation events, indicating its strong capability to\ncapture irrigation patterns accurately. SPADE's reports provide\ninterpretability and usability of soil moisture analytics. This study\nhighlights the potential of LLMs as scalable, adaptable tools for precision\nagriculture, which is capable of integrating qualitative knowledge and\ndata-driven reasoning to produce actionable insights for accurate soil moisture\nmonitoring and improved irrigation scheduling from soil moisture time-series\ndata.", "AI": {"tldr": "SPADE\u662f\u4e00\u4e2a\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5206\u6790\u571f\u58e4\u6e7f\u5ea6\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u7684\u6846\u67b6\uff0c\u80fd\u591f\u8fdb\u884c\u704c\u6e89\u6a21\u5f0f\u548c\u5f02\u5e38\u68c0\u6d4b\uff0c\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u7684\u6807\u6ce8\u6216\u5fae\u8c03\uff0c\u5e76\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u571f\u58e4\u6e7f\u5ea6\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u9608\u503c\u89c4\u5219\u6216\u6570\u636e\u9700\u6c42\u91cf\u5927\u7684\u673a\u5668\u5b66\u4e60/\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u8fd9\u4e9b\u6a21\u578b\u5728\u9002\u5e94\u6027\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u66f4\u5177\u9002\u5e94\u6027\u548c\u53ef\u89e3\u91ca\u6027\u7684\u65b9\u6cd5\u3002", "method": "SPADE\u6846\u67b6\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u5982ChatGPT-4.1\uff09\u5c06\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u8f6c\u6362\u4e3a\u6587\u672c\u8868\u793a\uff0c\u5e76\u8bbe\u8ba1\u4e86\u9886\u57df\u9a71\u52a8\u7684\u63d0\u793a\u6a21\u677f\uff0c\u4ee5\u5b9e\u73b0\u96f6\u6837\u672c\u5206\u6790\uff0c\u65e0\u9700\u8fdb\u884c\u4efb\u52a1\u7279\u5b9a\u7684\u6807\u6ce8\u6216\u5fae\u8c03\u3002\u8be5\u6846\u67b6\u80fd\u591f\u68c0\u6d4b\u704c\u6e89\u4e8b\u4ef6\u3001\u4f30\u7b97\u51c0\u704c\u6e89\u589e\u76ca\u3001\u68c0\u6d4b\u548c\u5206\u7c7b\u5f02\u5e38\uff0c\u5e76\u751f\u6210\u7ed3\u6784\u5316\u7684\u3001\u53ef\u89e3\u91ca\u7684\u62a5\u544a\u3002", "result": "SPADE\u5728\u5f02\u5e38\u68c0\u6d4b\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u53ec\u56de\u7387\u548cF1\u5206\u6570\uff0c\u5e76\u80fd\u51c6\u786e\u5206\u7c7b\u5f02\u5e38\u7c7b\u578b\u3002\u5728\u704c\u6e89\u4e8b\u4ef6\u68c0\u6d4b\u65b9\u9762\uff0cSPADE\u4e5f\u53d6\u5f97\u4e86\u5f88\u9ad8\u7684\u7cbe\u786e\u7387\u548c\u53ec\u56de\u7387\u3002SPADE\u751f\u6210\u7684\u62a5\u544a\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u7528\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u8bc1\u660e\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7cbe\u51c6\u519c\u4e1a\u9886\u57df\u7684\u6f5c\u529b\uff0c\u4f5c\u4e3a\u53ef\u6269\u5c55\u3001\u9002\u5e94\u6027\u5f3a\u7684\u5de5\u5177\uff0c\u80fd\u591f\u6574\u5408\u5b9a\u6027\u77e5\u8bc6\u548c\u6570\u636e\u9a71\u52a8\u7684\u63a8\u7406\uff0c\u4ece\u800c\u4e3a\u51c6\u786e\u7684\u571f\u58e4\u6e7f\u5ea6\u76d1\u6d4b\u548c\u6539\u8fdb\u704c\u6e89\u8ba1\u5212\u63d0\u4f9b\u53ef\u884c\u7684\u89c1\u89e3\u3002"}}
{"id": "2509.18122", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18122", "abs": "https://arxiv.org/abs/2509.18122", "authors": ["Yue Zhang", "Jiaxin Zhang", "Qiuyu Ren", "Tahsin Saffat", "Xiaoxuan Liu", "Zitong Yang", "Banghua Zhu", "Yi Ma"], "title": "GAUSS: Benchmarking Structured Mathematical Skills for Large Language Models", "comment": "120 pages (including appendix)", "summary": "We introduce \\textbf{GAUSS} (\\textbf{G}eneral \\textbf{A}ssessment of\n\\textbf{U}nderlying \\textbf{S}tructured \\textbf{S}kills in Mathematics), a\nbenchmark that evaluates LLMs' mathematical abilities across twelve core skill\ndimensions, grouped into three domains: knowledge and understanding, problem\nsolving and communication, and meta-skills and creativity. By categorizing\nproblems according to cognitive skills and designing tasks that isolate\nspecific abilities, GAUSS constructs comprehensive, fine-grained, and\ninterpretable profiles of models' mathematical abilities. These profiles\nfaithfully represent their underlying mathematical intelligence. To exemplify\nhow to use the \\textsc{GAUSS} benchmark, we have derived the skill profile of\n\\textsc{GPT-5-thinking}, revealing its strengths and weaknesses as well as its\ndifferences relative to \\textsc{o4-mini-high}, thereby underscoring the value\nof multidimensional, skill-based evaluation.", "AI": {"tldr": "GAUSS\u662f\u4e00\u4e2a\u8bc4\u4f30LLM\u6570\u5b66\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u6db5\u76d6\u5341\u4e8c\u9879\u6838\u5fc3\u6280\u80fd\u7ef4\u5ea6\uff0c\u5206\u4e3a\u77e5\u8bc6\u7406\u89e3\u3001\u95ee\u9898\u89e3\u51b3\u4e0e\u6c9f\u901a\u3001\u5143\u6280\u80fd\u4e0e\u521b\u9020\u529b\u4e09\u4e2a\u9886\u57df\u3002\u5b83\u901a\u8fc7\u5bf9\u95ee\u9898\u6309\u8ba4\u77e5\u6280\u80fd\u5206\u7c7b\u548c\u8bbe\u8ba1\u9694\u79bb\u7279\u5b9a\u80fd\u529b\u7684\u4efb\u52a1\uff0c\u4e3a\u6a21\u578b\u6784\u5efa\u5168\u9762\u3001\u7ec6\u81f4\u4e14\u53ef\u89e3\u91ca\u7684\u6570\u5b66\u80fd\u529b\u753b\u50cf\uff0c\u771f\u5b9e\u53cd\u6620\u5176\u6f5c\u5728\u7684\u6570\u5b66\u667a\u80fd\u3002\u901a\u8fc7\u793a\u4f8b\uff0c\u5c55\u793a\u4e86\u5982\u4f55\u4f7f\u7528GAUSS\u57fa\u51c6\u5206\u6790GPT-5-thinking\u7684\u6280\u80fd\u753b\u50cf\uff0c\u63ed\u793a\u4e86\u5176\u4f18\u52a3\u52bf\u4ee5\u53ca\u4e0eo4-mini-high\u7684\u5dee\u5f02\uff0c\u5f3a\u8c03\u4e86\u591a\u7ef4\u5ea6\u3001\u57fa\u4e8e\u6280\u80fd\u7684\u8bc4\u4f30\u4ef7\u503c\u3002", "motivation": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u6570\u5b66\u65b9\u9762\u7684\u80fd\u529b\uff0c\u5e76\u63d0\u4f9b\u4e00\u4e2a\u5168\u9762\u3001\u7ec6\u81f4\u4e14\u53ef\u89e3\u91ca\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aGAUSS\uff08General Assessment of Underlying Structured Skills in Mathematics\uff09\u7684\u57fa\u51c6\uff0c\u8be5\u57fa\u51c6\u6db5\u76d6\u5341\u4e8c\u9879\u6838\u5fc3\u6280\u80fd\u7ef4\u5ea6\uff0c\u5206\u4e3a\u77e5\u8bc6\u548c\u7406\u89e3\u3001\u95ee\u9898\u89e3\u51b3\u548c\u6c9f\u901a\u3001\u5143\u6280\u80fd\u548c\u521b\u9020\u529b\u4e09\u4e2a\u9886\u57df\u3002\u901a\u8fc7\u5bf9\u95ee\u9898\u8fdb\u884c\u8ba4\u77e5\u6280\u80fd\u5206\u7c7b\u548c\u8bbe\u8ba1\u9694\u79bb\u7279\u5b9a\u80fd\u529b\u7684\u4efb\u52a1\u6765\u6784\u5efa\u8bc4\u4f30\u6846\u67b6\u3002", "result": "\u5f97\u51fa\u4e86GPT-5-thinking\u7684\u6280\u80fd\u753b\u50cf\uff0c\u5e76\u63ed\u793a\u4e86\u5176\u76f8\u5bf9\u4e8eo4-mini-high\u7684\u4f18\u52bf\u548c\u52a3\u52bf\u3002", "conclusion": "GAUSS\u57fa\u51c6\u80fd\u591f\u63d0\u4f9b\u591a\u7ef4\u5ea6\u3001\u57fa\u4e8e\u6280\u80fd\u7684\u8bc4\u4f30\uff0c\u80fd\u591f\u771f\u5b9e\u53cd\u6620LLM\u7684\u6f5c\u5728\u6570\u5b66\u667a\u80fd\uff0c\u5e76\u6709\u52a9\u4e8e\u7406\u89e3\u6a21\u578b\u5728\u4e0d\u540c\u6570\u5b66\u80fd\u529b\u4e0a\u7684\u8868\u73b0\u5dee\u5f02\u3002"}}
{"id": "2509.18434", "categories": ["cs.LO", "cs.CC"], "pdf": "https://arxiv.org/pdf/2509.18434", "abs": "https://arxiv.org/abs/2509.18434", "authors": ["Dmitriy Zhuk"], "title": "Singleton algorithms for the Constraint Satisfaction Problem", "comment": null, "summary": "A natural strengthening of an algorithm for the (promise) constraint\nsatisfaction problem is its singleton version: we first fix a constraint to\nsome tuple from the constraint relation, then run the algorithm, and remove the\ntuple from the constraint if the answer is negative. We characterize the power\nof the singleton versions of standard universal algorithms for the (promise)\nCSP over a fixed template in terms of the existence of a minion homomorphism.\nUsing the Hales-Jewett theorem, we show that for finite relational structures\nthis minion condition is equivalent to the existence of polymorphisms with\ncertain symmetries, called palette block symmetric polymorphisms. By proving\nthe existence of such polymorphisms we establish that the singleton version of\nthe BLP+AIP algorithm solves all tractable CSPs over domains of size at most 7.\nFinally, by providing concrete CSP templates, we illustrate the limitations of\nlinear programming, the power of the singleton versions, and the elegance of\nthe palette block symmetric polymorphisms.", "AI": {"tldr": "\u7814\u7a76\u4e86(promise)\u7ea6\u675f\u6ee1\u8db3\u95ee\u9898\uff08CSP\uff09\u7684\u5355\u4f8b\u7b97\u6cd5\uff0c\u5e76\u5c06\u5176\u80fd\u529b\u4e0eminion\u540c\u6001\u8054\u7cfb\u8d77\u6765\u3002\u8bc1\u660e\u4e86\u5bf9\u4e8e\u6709\u9650\u5173\u7cfb\u7ed3\u6784\uff0c\u8be5\u6761\u4ef6\u7b49\u4ef7\u4e8e\u5b58\u5728\u5177\u6709\u7279\u5b9a\u5bf9\u79f0\u6027\u7684\u591a\u6001\u51fd\u6570\uff08palette block symmetric polymorphisms\uff09\u3002\u8fd9\u8868\u660e\u5355\u4f8bBLP+AIP\u7b97\u6cd5\u53ef\u4ee5\u89e3\u51b3\u5927\u5c0f\u4e0d\u8d85\u8fc77\u7684\u57df\u4e0a\u7684\u6240\u6709\u53ef\u5904\u7406CSP\u3002\u6700\u540e\uff0c\u901a\u8fc7\u5177\u4f53CSP\u6a21\u677f\u8bf4\u660e\u4e86\u7ebf\u6027\u89c4\u5212\u7684\u5c40\u9650\u6027\u3001\u5355\u4f8b\u7b97\u6cd5\u7684\u5f3a\u5927\u4ee5\u53capalette block symmetric polymorphisms\u7684\u4f18\u7f8e\u6027\u3002", "motivation": "\u5bf9\uff08promise\uff09\u7ea6\u675f\u6ee1\u8db3\u95ee\u9898\uff08CSP\uff09\u7684\u6807\u51c6\u901a\u7528\u7b97\u6cd5\u8fdb\u884c\u4e86\u81ea\u7136\u52a0\u5f3a\uff0c\u5373\u5176\u5355\u4f8b\u7248\u672c\uff0c\u65e8\u5728\u7406\u89e3\u8fd9\u79cd\u52a0\u5f3a\u5bf9\u7b97\u6cd5\u80fd\u529b\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u5c06\u7b97\u6cd5\u56fa\u5b9a\u5230\u4e00\u4e2a\u7ea6\u675f\u5143\u7ec4\uff0c\u7136\u540e\u8fd0\u884c\u7b97\u6cd5\uff0c\u5982\u679c\u7ed3\u679c\u4e3a\u5426\u5b9a\u5219\u79fb\u9664\u8be5\u5143\u7ec4\uff0c\u6765\u7814\u7a76CSP\u7684\u5355\u4f8b\u7248\u672c\u3002\u5229\u7528Hales-Jewett\u5b9a\u7406\uff0c\u5c06\u5355\u4f8b\u7248\u672c\u7684\u80fd\u529b\u4e0eminion\u540c\u6001\u8054\u7cfb\u8d77\u6765\uff0c\u5e76\u8fdb\u4e00\u6b65\u8bc1\u660e\u4e86\u8be5\u6761\u4ef6\u7b49\u4ef7\u4e8e\u5b58\u5728\u5177\u6709\u8c03\u8272\u677f\u5757\u5bf9\u79f0\u6027\u7684\u591a\u6001\u51fd\u6570\u3002", "result": "\u8bc1\u660e\u4e86\u5bf9\u4e8e\u6709\u9650\u5173\u7cfb\u7ed3\u6784\uff0c\u5355\u4f8b\u7b97\u6cd5\u7684\u80fd\u529b\u7b49\u4ef7\u4e8e\u5b58\u5728\u8c03\u8272\u677f\u5757\u5bf9\u79f0\u6027\u591a\u6001\u51fd\u6570\u3002\u8fdb\u4e00\u6b65\u8bc1\u660e\u4e86\u5355\u4f8bBLP+AIP\u7b97\u6cd5\u53ef\u4ee5\u89e3\u51b3\u57df\u5927\u5c0f\u4e0d\u8d85\u8fc77\u7684\u6240\u6709\u53ef\u5904\u7406CSP\u3002", "conclusion": "\u5355\u4f8b\u7248\u672c\u7684BLP+AIP\u7b97\u6cd5\u80fd\u591f\u89e3\u51b3\u57df\u5927\u5c0f\u4e0d\u8d85\u8fc77\u7684\u6240\u6709\u53ef\u5904\u7406CSP\u3002\u901a\u8fc7\u5177\u4f53CSP\u6a21\u677f\u7684\u4f8b\u5b50\uff0c\u8bf4\u660e\u4e86\u7ebf\u6027\u89c4\u5212\u7684\u5c40\u9650\u6027\uff0c\u4ee5\u53ca\u5355\u4f8b\u7b97\u6cd5\u548c\u8c03\u8272\u677f\u5757\u5bf9\u79f0\u6027\u591a\u6001\u51fd\u6570\u7684\u4f18\u8d8a\u6027\u3002"}}
{"id": "2509.18445", "categories": ["cs.LG", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2509.18445", "abs": "https://arxiv.org/abs/2509.18445", "authors": ["Kangzheng Liu", "Leixin Ma"], "title": "MeshODENet: A Graph-Informed Neural Ordinary Differential Equation Neural Network for Simulating Mesh-Based Physical Systems", "comment": "9 pages, 7 figures", "summary": "The simulation of complex physical systems using a discretized mesh is a\ncornerstone of applied mechanics, but traditional numerical solvers are often\ncomputationally prohibitive for many-query tasks. While Graph Neural Networks\n(GNNs) have emerged as powerful surrogate models for mesh-based data, their\nstandard autoregressive application for long-term prediction is often plagued\nby error accumulation and instability. To address this, we introduce\nMeshODENet, a general framework that synergizes the spatial reasoning of GNNs\nwith the continuous-time modeling of Neural Ordinary Differential Equations. We\ndemonstrate the framework's effectiveness and versatility on a series of\nchallenging structural mechanics problems, including one- and two-dimensional\nelastic bodies undergoing large, non-linear deformations. The results\ndemonstrate that our approach significantly outperforms baseline models in\nlong-term predictive accuracy and stability, while achieving substantial\ncomputational speed-ups over traditional solvers. This work presents a powerful\nand generalizable approach for developing data-driven surrogates to accelerate\nthe analysis and modeling of complex structural systems.", "AI": {"tldr": "GNNs\u7ed3\u5408\u795e\u7ecfODE\uff0c\u63d0\u51faMeshODENet\u6846\u67b6\uff0c\u7528\u4e8e\u52a0\u901f\u590d\u6742\u7269\u7406\u7cfb\u7edf\u7684\u6a21\u62df\uff0c\u5c24\u5176\u5728\u7ed3\u6784\u529b\u5b66\u9886\u57df\uff0c\u80fd\u663e\u8457\u63d0\u5347\u957f\u671f\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u4f20\u7edf\u7684\u6570\u503c\u6c42\u89e3\u5668\u5728\u6a21\u62df\u590d\u6742\u7269\u7406\u7cfb\u7edf\u65f6\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u4e0d\u9002\u7528\u4e8e\u591a\u67e5\u8be2\u4efb\u52a1\u3002\u5c3d\u7ba1GNNs\u53ef\u7528\u4e8e\u5904\u7406\u7f51\u683c\u6570\u636e\uff0c\u4f46\u5176\u5728\u957f\u671f\u9884\u6d4b\u65b9\u9762\u5b58\u5728\u8bef\u5dee\u7d2f\u79ef\u548c\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\u3002", "method": "\u63d0\u51faMeshODENet\u6846\u67b6\uff0c\u7ed3\u5408\u4e86GNN\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u548c\u795e\u7ecfODE\u7684\u8fde\u7eed\u65f6\u95f4\u5efa\u6a21\u80fd\u529b\u3002", "result": "\u5728\u5904\u7406\u4e00\u7ef4\u548c\u4e8c\u7ef4\u5f39\u6027\u4f53\u5927\u53d8\u5f62\u7b49\u7ed3\u6784\u529b\u5b66\u95ee\u9898\u65f6\uff0cMeshODENet\u5728\u957f\u671f\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u7a33\u5b9a\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u5e76\u5927\u5e45\u7f29\u77ed\u4e86\u8ba1\u7b97\u65f6\u95f4\u3002", "conclusion": "MeshODENet\u63d0\u4f9b\u4e86\u4e00\u79cd\u5f3a\u5927\u4e14\u901a\u7528\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u53ef\u52a0\u901f\u590d\u6742\u7ed3\u6784\u7cfb\u7edf\u7684\u5206\u6790\u548c\u5efa\u6a21\u3002"}}
{"id": "2509.18984", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2509.18984", "abs": "https://arxiv.org/abs/2509.18984", "authors": ["Hayden Jananthan", "Jeremy Kepner", "Michael Jones", "Vijay Gadepally", "Michael Houle", "Peter Michaleas", "Chasen Milner", "Alex Pentland"], "title": "GraphBLAS Mathematical Opportunities: Parallel Hypersparse, Matrix Based Graph Streaming, and Complex-Index Matrices", "comment": "HPEC 2025", "summary": "The GraphBLAS high performance library standard has yielded capabilities\nbeyond enabling graph algorithms to be readily expressed in the language of\nlinear algebra. These GraphBLAS capabilities enable new performant ways of\nthinking about algorithms that include leveraging hypersparse matrices for\nparallel computation, matrix-based graph streaming, and complex-index matrices.\nFormalizing these concepts mathematically provides additional opportunities to\napply GraphBLAS to new areas. This paper formally develops parallel hypersparse\nmatrices, matrix-based graph streaming, and complex-index matrices and\nillustrates these concepts with various examples to demonstrate their potential\nmerits.", "AI": {"tldr": "GraphBLAS \u5e93\u6807\u51c6\u4e0d\u4ec5\u80fd\u7528\u7ebf\u6027\u4ee3\u6570\u8868\u8fbe\u56fe\u7b97\u6cd5\uff0c\u8fd8\u80fd\u5b9e\u73b0\u65b0\u7684\u9ad8\u6027\u80fd\u8ba1\u7b97\u65b9\u6cd5\uff0c\u5982\u5229\u7528\u8d85\u7a00\u758f\u77e9\u9635\u8fdb\u884c\u5e76\u884c\u8ba1\u7b97\u3001\u57fa\u4e8e\u77e9\u9635\u7684\u56fe\u6d41\u548c\u590d\u6570\u7d22\u5f15\u77e9\u9635\u3002\u672c\u6587\u5c06\u5bf9\u8fd9\u4e9b\u6982\u5ff5\u8fdb\u884c\u6570\u5b66\u5f62\u5f0f\u5316\uff0c\u5e76\u901a\u8fc7\u5b9e\u4f8b\u5c55\u793a\u5176\u6f5c\u529b\u3002", "motivation": "GraphBLAS \u5e93\u6807\u51c6\u5728\u56fe\u7b97\u6cd5\u5b9e\u73b0\u65b9\u9762\u63d0\u4f9b\u4e86\u8d85\u8d8a\u7ebf\u6027\u4ee3\u6570\u8868\u8fbe\u7684\u80fd\u529b\uff0c\u6fc0\u53d1\u4e86\u65b0\u7684\u9ad8\u6027\u80fd\u8ba1\u7b97\u601d\u8def\uff0c\u5305\u62ec\u5229\u7528\u8d85\u7a00\u758f\u77e9\u9635\u8fdb\u884c\u5e76\u884c\u8ba1\u7b97\u3001\u57fa\u4e8e\u77e9\u9635\u7684\u56fe\u6d41\u5904\u7406\u4ee5\u53ca\u590d\u6570\u7d22\u5f15\u77e9\u9635\u7b49\u3002\u672c\u6587\u65e8\u5728\u5bf9\u8fd9\u4e9b\u65b0\u65b9\u6cd5\u8fdb\u884c\u6570\u5b66\u5f62\u5f0f\u5316\uff0c\u5e76\u63a2\u7d22\u5176\u5728\u4e0d\u540c\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\u3002", "method": "\u672c\u6587\u5f62\u5f0f\u5316\u5730\u53d1\u5c55\u4e86\u5e76\u884c\u8d85\u7a00\u758f\u77e9\u9635\u3001\u57fa\u4e8e\u77e9\u9635\u7684\u56fe\u6d41\u548c\u590d\u6570\u7d22\u5f15\u77e9\u9635\u7684\u6982\u5ff5\u3002", "result": "\u6587\u7ae0\u901a\u8fc7\u5177\u4f53\u793a\u4f8b\uff0c\u5c55\u793a\u4e86\u5e76\u884c\u8d85\u7a00\u758f\u77e9\u9635\u3001\u57fa\u4e8e\u77e9\u9635\u7684\u56fe\u6d41\u548c\u590d\u6570\u7d22\u5f15\u77e9\u9635\u7b49\u6982\u5ff5\u7684\u6f5c\u5728\u4f18\u52bf\u3002", "conclusion": "\u672c\u6587\u5bf9 GraphBLAS \u5e93\u6807\u51c6\u5e26\u6765\u7684\u65b0\u8ba1\u7b97\u8303\u5f0f\u8fdb\u884c\u4e86\u5f62\u5f0f\u5316\u9610\u8ff0\uff0c\u5e76\u8f85\u4ee5\u5b9e\u4f8b\uff0c\u8bc1\u660e\u4e86\u8fd9\u4e9b\u65b0\u65b9\u6cd5\u5728\u56fe\u8ba1\u7b97\u9886\u57df\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2509.18497", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18497", "abs": "https://arxiv.org/abs/2509.18497", "authors": ["Kaiwen Jiang", "Jia-Mu Sun", "Zilu Li", "Dan Wang", "Tzu-Mao Li", "Ravi Ramamoorthi"], "title": "Differentiable Light Transport with Gaussian Surfels via Adapted Radiosity for Efficient Relighting and Geometry Reconstruction", "comment": null, "summary": "Radiance fields have gained tremendous success with applications ranging from\nnovel view synthesis to geometry reconstruction, especially with the advent of\nGaussian splatting. However, they sacrifice modeling of material reflective\nproperties and lighting conditions, leading to significant geometric\nambiguities and the inability to easily perform relighting. One way to address\nthese limitations is to incorporate physically-based rendering, but it has been\nprohibitively expensive to include full global illumination within the inner\nloop of the optimization. Therefore, previous works adopt simplifications that\nmake the whole optimization with global illumination effects efficient but less\naccurate. In this work, we adopt Gaussian surfels as the primitives and build\nan efficient framework for differentiable light transport, inspired from the\nclassic radiosity theory. The whole framework operates in the coefficient space\nof spherical harmonics, enabling both diffuse and specular materials. We extend\nthe classic radiosity into non-binary visibility and semi-opaque primitives,\npropose novel solvers to efficiently solve the light transport, and derive the\nbackward pass for gradient optimizations, which is more efficient than\nauto-differentiation. During inference, we achieve view-independent rendering\nwhere light transport need not be recomputed under viewpoint changes, enabling\nhundreds of FPS for global illumination effects, including view-dependent\nreflections using a spherical harmonics representation. Through extensive\nqualitative and quantitative experiments, we demonstrate superior geometry\nreconstruction, view synthesis and relighting than previous inverse rendering\nbaselines, or data-driven baselines given relatively sparse datasets with known\nor unknown lighting conditions.", "AI": {"tldr": "\u9ad8\u65af\u8868\u9762\u5143\u7ed3\u5408\u53ef\u5fae\u5149\u7ebf\u4f20\u8f93\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u5168\u5c40\u5149\u7167\u6e32\u67d3\uff0c\u5305\u62ec\u955c\u9762\u53cd\u5c04\uff0c\u5e76\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709 radiance fields \u5728\u6750\u8d28\u53cd\u5c04\u548c\u5149\u7167\u6761\u4ef6\u5efa\u6a21\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5bfc\u81f4\u51e0\u4f55\u6b67\u4e49\u548c\u91cd\u5149\u7167\u56f0\u96be\u3002\u4f20\u7edf\u57fa\u4e8e\u7269\u7406\u7684\u6e32\u67d3\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u7b80\u5316\u65b9\u6cd5\u7cbe\u5ea6\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9ad8\u65af\u8868\u9762\u5143\u548c\u7403\u8c10\u7cfb\u6570\u7a7a\u95f4\u7684\u53ef\u5fae\u5149\u7ebf\u4f20\u8f93\u6846\u67b6\uff0c\u6269\u5c55\u4e86\u7ecf\u5178\u8f90\u5c04\u5ea6\u7406\u8bba\uff0c\u89e3\u51b3\u4e86\u975e\u4e8c\u5143\u53ef\u89c1\u6027\u548c\u534a\u900f\u660e\u95ee\u9898\uff0c\u5e76\u63a8\u5bfc\u4e86\u9ad8\u6548\u7684\u68af\u5ea6\u4f18\u5316\u53cd\u5411\u4f20\u64ad\u3002", "result": "\u5b9e\u73b0\u4e86\u89c6\u56fe\u65e0\u5173\u7684\u6e32\u67d3\uff0c\u6bcf\u79d2\u5e27\u6570\u53ef\u8fbe\u6570\u767e\uff0c\u80fd\u591f\u5904\u7406\u5305\u62ec\u955c\u9762\u53cd\u5c04\u5728\u5185\u7684\u5168\u5c40\u5149\u7167\u6548\u679c\u3002\u5728\u51e0\u4f55\u91cd\u5efa\u3001\u89c6\u56fe\u5408\u6210\u548c\u91cd\u5149\u7167\u65b9\u9762\uff0c\u76f8\u8f83\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u9ad8\u6548\u7684\u53ef\u5fae\u5149\u7ebf\u4f20\u8f93\uff0c\u5728\u4fdd\u6301\u9ad8\u6e32\u67d3\u6548\u7387\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86 radiance fields \u5728\u6750\u8d28\u548c\u5149\u7167\u5efa\u6a21\u65b9\u9762\u7684\u80fd\u529b\uff0c\u5e76\u5728\u591a\u79cd\u8bc4\u4f30\u6307\u6807\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u6280\u672f\u3002"}}
{"id": "2509.18735", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.18735", "abs": "https://arxiv.org/abs/2509.18735", "authors": ["Muhammad Ahmed Mohsin", "Muhammad Umer", "Ahsan Bilal", "Muhammad Ali Jamshed", "Dean F. Hougen", "John M. Cioffi"], "title": "6G Twin: Hybrid Gaussian Radio Fields for Channel Estimation and Non-Linear Precoder Design for Radio Access Networks", "comment": "Submitted to IEEE Transactions on Wireless Communications", "summary": "This work introduces 6G Twin, the first end-to-end artificial intelligence\n(AI)-native radio access network (RAN) design that unifies (i) neural Gaussian\nRadio Fields (GRF) for compressed channel state information (CSI) acquisition,\n(ii) continual channel prediction with handover persistence, and (iii) an\nenergy-optimal nonlinear precoder (minPMAC). GRF replaces dense pilots with a\nsparse Gaussian field, cutting pilot overhead by about 100x while delivering\n1.1 ms inference and less than 2 minutes on-site training, thus enabling\nmillisecond-scale closed-loop operation. A replay-driven continual learner\nsustains accuracy under mobility and cell transitions, improving channel\nnormalized mean square error (NMSE) by more than 10 dB over frozen predictors\nand an additional 2-5 dB over uniform replay, thereby stabilizing performance\nacross UMi/UMa handovers. Finally, minPMAC solves a convex, order-free MAC\nprecoder design that recovers the globally optimal order from Broadcast Channel\n(BC) duals and minimizes transmit energy subject to minimum-rate guarantees,\nachieving 4-10 times lower energy (scenario dependent) with monotonically\nincreasing bits per joule as SNR grows. This translates to up to 5 times higher\ndata rate at comparable power or the same rates at substantially lower power.\nTogether, these components form a practical, GPU-ready framework that attains\nreal-time CSI, robust tracking in dynamic networks with efficient handovers,\nand state-of-the-art throughput-energy tradeoffs under 3GPP-style settings.", "AI": {"tldr": "6G Twin\u662f\u4e00\u4e2aAI\u539f\u751f\u7684RAN\u8bbe\u8ba1\uff0c\u901a\u8fc7GRF\u3001\u6301\u7eed\u4fe1\u9053\u9884\u6d4b\u548cminPMAC\u4f18\u5316\u4e86CSI\u83b7\u53d6\u3001\u4fe1\u9053\u9884\u6d4b\u548c\u9884\u7f16\u7801\uff0c\u5b9e\u73b0\u4e86\u4f4e\u5f00\u9500\u3001\u9ad8\u7cbe\u5ea6\u3001\u4f4e\u529f\u8017\u548c\u9ad8\u6570\u636e\u901f\u7387\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u8bbe\u8ba1\u4e00\u4e2a\u7aef\u5230\u7aef\u7684AI\u539f\u751f6G\u65e0\u7ebf\u63a5\u5165\u7f51\u7edc\uff08RAN\uff09\uff0c\u4ee5\u89e3\u51b3\u5f53\u524dRAN\u5728CSI\u83b7\u53d6\u3001\u4fe1\u9053\u9884\u6d4b\u548c\u80fd\u6548\u65b9\u9762\u7684\u6311\u6218\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a6G Twin\u7684RAN\u8bbe\u8ba1\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\uff081\uff09\u795e\u7ecf\u9ad8\u65af\u65e0\u7ebf\u7535\u573a\uff08GRF\uff09\u7528\u4e8e\u538b\u7f29CSI\u83b7\u53d6\uff1b\uff082\uff09\u6301\u7eed\u4fe1\u9053\u9884\u6d4b\uff0c\u5177\u5907\u5207\u6362\u6301\u4e45\u6027\uff1b\uff083\uff09\u80fd\u91cf\u6700\u4f18\u7684\u975e\u7ebf\u6027\u9884\u7f16\u7801\u5668\uff08minPMAC\uff09\u3002GRF\u4f7f\u7528\u7a00\u758f\u9ad8\u65af\u573a\u66ff\u4ee3\u5bc6\u96c6\u5bfc\u9891\uff0c\u5c06\u5bfc\u9891\u5f00\u9500\u51cf\u5c11\u7ea6100\u500d\uff0c\u5e76\u5b9e\u73b0\u6beb\u79d2\u7ea7\u63a8\u7406\u548c\u5206\u949f\u7ea7\u8bad\u7ec3\u3002\uff082\uff09\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u91cd\u653e\u7684\u6301\u7eed\u5b66\u4e60\u5668\uff0c\u7528\u4e8e\u5728\u79fb\u52a8\u548c\u5c0f\u533a\u5207\u6362\u573a\u666f\u4e0b\u4fdd\u6301\u7cbe\u5ea6\uff0c\u63d0\u9ad8\u4e86\u4fe1\u9053\u5f52\u4e00\u5316\u5747\u65b9\u8bef\u5dee\uff08NMSE\uff09\u8d85\u8fc710 dB\u3002\uff083\uff09minPMAC\u89e3\u51b3\u4e86MAC\u9884\u7f16\u7801\u5668\u8bbe\u8ba1\u95ee\u9898\uff0c\u5728\u6ee1\u8db3\u6700\u5c0f\u901f\u7387\u8981\u6c42\u7684\u540c\u65f6\u6700\u5c0f\u5316\u4f20\u8f93\u80fd\u91cf\uff0c\u5b9e\u73b0\u4e864-10\u500d\u7684\u4f4e\u80fd\u8017\u3002", "result": "GRF\u5c06\u5bfc\u9891\u5f00\u9500\u51cf\u5c11\u7ea6100\u500d\uff0c\u63a8\u7406\u65f6\u95f4\u4e3a1.1\u6beb\u79d2\uff0c\u8bad\u7ec3\u65f6\u95f4\u5c11\u4e8e2\u5206\u949f\u3002\u6301\u7eed\u5b66\u4e60\u5668\u5c06NMSE\u63d0\u9ad8\u4e8610 dB\u4ee5\u4e0a\uff0c\u5e76\u989d\u5916\u63d0\u9ad8\u4e862-5 dB\u3002minPMAC\u5b9e\u73b0\u4e864-10\u500d\u7684\u4f4e\u80fd\u8017\uff0c\u5e76\u4e14\u968f\u7740\u4fe1\u566a\u6bd4\u7684\u589e\u957f\uff0c\u6bcf\u7126\u8033\u6bd4\u7279\u6570\u5355\u8c03\u589e\u52a0\u3002\u6574\u4f53\u6846\u67b6\u5b9e\u73b0\u4e86\u5b9e\u65f6CSI\u3001\u52a8\u6001\u7f51\u7edc\u4e0b\u7684\u9c81\u68d2\u8ddf\u8e2a\u4ee5\u53ca\u9ad8\u6548\u5207\u6362\uff0c\u5e76\u57283GPP\u573a\u666f\u4e0b\u5b9e\u73b0\u4e86\u5148\u8fdb\u7684\u541e\u5410\u91cf-\u80fd\u91cf\u6743\u8861\u3002", "conclusion": "6G Twin\u662f\u4e00\u4e2a\u5b9e\u7528\u7684\u3001GPU\u5c31\u7eea\u7684\u6846\u67b6\uff0c\u5b83\u901a\u8fc7GRF\u3001\u6301\u7eed\u4fe1\u9053\u9884\u6d4b\u548cminPMAC\u7684\u534f\u540c\u4f5c\u7528\uff0c\u5728CSI\u83b7\u53d6\u3001\u52a8\u6001\u7f51\u7edc\u4e0b\u7684\u9c81\u68d2\u8ddf\u8e2a\u4ee5\u53ca\u541e\u5410\u91cf-\u80fd\u91cf\u6548\u7387\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6539\u8fdb\uff0c\u4e3a6G RAN\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.18371", "categories": ["eess.SY", "cs.MA", "cs.RO", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.18371", "abs": "https://arxiv.org/abs/2509.18371", "authors": ["Eduardo Sebasti\u00e1n", "Maitrayee Keskar", "Eeman Iqbal", "Eduardo Montijano", "Carlos Sag\u00fc\u00e9s", "Nikolay Atanasov"], "title": "Policy Gradient with Self-Attention for Model-Free Distributed Nonlinear Multi-Agent Games", "comment": null, "summary": "Multi-agent games in dynamic nonlinear settings are challenging due to the\ntime-varying interactions among the agents and the non-stationarity of the\n(potential) Nash equilibria. In this paper we consider model-free games, where\nagent transitions and costs are observed without knowledge of the transition\nand cost functions that generate them. We propose a policy gradient approach to\nlearn distributed policies that follow the communication structure in\nmulti-team games, with multiple agents per team. Our formulation is inspired by\nthe structure of distributed policies in linear quadratic games, which take the\nform of time-varying linear feedback gains. In the nonlinear case, we model the\npolicies as nonlinear feedback gains, parameterized by self-attention layers to\naccount for the time-varying multi-agent communication topology. We demonstrate\nthat our distributed policy gradient approach achieves strong performance in\nseveral settings, including distributed linear and nonlinear regulation, and\nsimulated and real multi-robot pursuit-and-evasion games.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7b56\u7565\u68af\u5ea6\u7684\u6a21\u578b\u65e0\u5173\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u52a8\u6001\u975e\u7ebf\u6027\u591a\u4eba\u535a\u5f08\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5904\u7406\u65f6\u53d8\u901a\u4fe1\u7ed3\u6784\u3002", "motivation": "\u591a\u4eba\u535a\u5f08\u5728\u52a8\u6001\u975e\u7ebf\u6027\u73af\u5883\u4e0b\u5145\u6ee1\u6311\u6218\uff0c\u56e0\u4e3a\u4ee3\u7406\u4e4b\u95f4\u7684\u4ea4\u4e92\u662f\u65f6\u53d8\u7684\uff0c\u4e14\uff08\u6f5c\u5728\u7684\uff09\u7eb3\u4ec0\u5747\u8861\u662f\u975e\u5e73\u7a33\u7684\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\uff0c\u7528\u4e8e\u5b66\u4e60\u5206\u5e03\u5f0f\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u9075\u5faa\u591a\u56e2\u961f\u535a\u5f08\u4e2d\u7684\u901a\u4fe1\u7ed3\u6784\uff0c\u5e76\u91c7\u7528\u81ea\u6ce8\u610f\u529b\u5c42\u53c2\u6570\u5316\u975e\u7ebf\u6027\u53cd\u9988\u589e\u76ca\u6765\u5904\u7406\u65f6\u53d8\u901a\u4fe1\u62d3\u6251\u3002", "result": "\u6240\u63d0\u51fa\u7684\u5206\u5e03\u5f0f\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u5728\u5206\u5e03\u5f0f\u7ebf\u6027\u4e0e\u975e\u7ebf\u6027\u8c03\u8282\u3001\u4ee5\u53ca\u6a21\u62df\u548c\u771f\u5b9e\u591a\u673a\u5668\u4eba\u8ffd\u9010\u4e0e\u89c4\u907f\u535a\u5f08\u7b49\u591a\u79cd\u573a\u666f\u4e2d\u5747\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u89e3\u51b3\u52a8\u6001\u975e\u7ebf\u6027\u591a\u4eba\u535a\u5f08\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u5206\u5e03\u5f0f\u7b56\u7565\u68af\u5ea6\u5b66\u4e60\u65b9\u6cd5\u3002"}}
{"id": "2509.18679", "categories": ["cs.ET"], "pdf": "https://arxiv.org/pdf/2509.18679", "abs": "https://arxiv.org/abs/2509.18679", "authors": ["Shikhar Srivastava", "Ritajit Majumdar", "Padmanabha Venkatagiri Seshadri", "Anupama Ray", "Yogesh Simmhan"], "title": "Lightweight Targeted Estimation of Layout Noise in a Quantum Computer using Quality Indicator Circuits", "comment": null, "summary": "In the current era of quantum computing, minimizing noise is essential for\nreliably executing quantum circuits on hardware. A key factor affecting circuit\nperformance is the mapping of the abstract quantum circuit to the physical\nlayout of the quantum hardware. This mapping can significantly influence output\nquality, especially since hardware noise profiles are non-uniform and dynamic.\nExisting solutions such as Mapomatic and Just-In-Time (JIT) Transpilation\nattempt to address this issue but are limited either by relying on stale\ncalibration data or high hardware usage, respectively. In this article, we\npropose Quality Indicator Circuits (QICs) as a lightweight, real-time method\nfor assessing layout quality. A QIC is a small probe circuit that is designed\nto retain the basic structure of the user's circuit and whose ideal noiseless\noutcome is known. It is used to evaluate which region of the quantum hardware\nis best suited for executing the circuit of interest. We first propose a basic\nmethod where a QIC is executed for each isomorphic layout to detect the best\namong them. Although this requires several targeted circuit executions, we show\nthat it still, in most cases, reduces the execution overheads as compared with\nJIT. To reduce the overheads further, we propose the union of multiple layouts\nwith a Union QIC approach that has no overlaps, and a Distortion Threshold\nbased approach allowing some overlap. Our results show that these outperform\nMapomatic in the quality of layout selection while reducing the hardware\noverhead of JIT by 79 percent on average. This makes our proposed method\nlightweight and reliable, and a viable technique for layout selection in\nnear-term quantum devices.", "AI": {"tldr": "QIC\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u5b9e\u65f6\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u91cf\u5b50\u7535\u8def\u5e03\u5c40\u8d28\u91cf\uff0c\u4ee5\u51cf\u5c11\u566a\u58f0\u5e76\u63d0\u9ad8\u6027\u80fd\u3002", "motivation": "\u91cf\u5b50\u8ba1\u7b97\u4e2d\u7684\u566a\u58f0\u6700\u5c0f\u5316\u5bf9\u4e8e\u53ef\u9760\u6267\u884c\u91cf\u5b50\u7535\u8def\u81f3\u5173\u91cd\u8981\uff0c\u800c\u7535\u8def\u5230\u786c\u4ef6\u7684\u6620\u5c04\u662f\u5f71\u54cd\u6027\u80fd\u7684\u5173\u952e\u56e0\u7d20\uff0c\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u8d28\u91cf\u6307\u793a\u7535\u8def\uff08QIC\uff09\u7684\u8f7b\u91cf\u7ea7\u3001\u5b9e\u65f6\u65b9\u6cd5\u3002QIC\u662f\u5c0f\u578b\u63a2\u6d4b\u7535\u8def\uff0c\u4fdd\u7559\u7528\u6237\u7535\u8def\u7684\u57fa\u672c\u7ed3\u6784\uff0c\u5176\u7406\u60f3\u7684\u65e0\u566a\u58f0\u7ed3\u679c\u662f\u5df2\u77e5\u7684\uff0c\u7528\u4e8e\u8bc4\u4f30\u54ea\u4e2a\u786c\u4ef6\u533a\u57df\u6700\u9002\u5408\u6267\u884c\u76ee\u6807\u7535\u8def\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\uff1a1. \u57fa\u672c\u65b9\u6cd5\uff1a\u4e3a\u6bcf\u4e2a\u540c\u6784\u5e03\u5c40\u6267\u884cQIC\u4ee5\u9009\u62e9\u6700\u4f73\u5e03\u5c40\u30022. \u6539\u8fdb\u65b9\u6cd5\uff1a\u901a\u8fc7\u5408\u5e76\u591a\u4e2a\u5e03\u5c40\u7684Union QIC\u65b9\u6cd5\uff0c\u4ee5\u53ca\u5141\u8bb8\u91cd\u53e0\u7684\u57fa\u4e8e\u5931\u771f\u9608\u503c\u7684\u65b9\u6cd5\uff0c\u8fdb\u4e00\u6b65\u964d\u4f4e\u5f00\u9500\u3002", "result": "\u4e0eMapomatic\u76f8\u6bd4\uff0cQIC\u5728\u5e03\u5c40\u9009\u62e9\u8d28\u91cf\u4e0a\u66f4\u4f18\uff1b\u4e0eJIT\u76f8\u6bd4\uff0c\u786c\u4ef6\u5f00\u9500\u5e73\u5747\u51cf\u5c1179%\u3002", "conclusion": "QIC\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u53ef\u9760\u7684\u91cf\u5b50\u8bbe\u5907\u5e03\u5c40\u9009\u62e9\u6280\u672f\uff0c\u9002\u7528\u4e8e\u8fd1\u671f\u91cf\u5b50\u8bbe\u5907\u3002"}}
{"id": "2509.18551", "categories": ["cs.GT", "econ.TH"], "pdf": "https://arxiv.org/pdf/2509.18551", "abs": "https://arxiv.org/abs/2509.18551", "authors": ["Chenlan Wang", "Jimin Han", "Diana Jue-Rajasingh"], "title": "Group Formation through Game Theory and Agent-Based Modeling: Spatial Cohesion, Heterogeneity, and Resource Pooling", "comment": "This work has been accepted by the 61st Allerton Conference on\n  Communication, Control, and Computing", "summary": "This paper develops a game-theoretic model and an agent-based model to study\ngroup formation driven by resource pooling, spatial cohesion, and\nheterogeneity. We focus on cross-sector partnerships (CSPs) involving public,\nprivate, and nonprofit organizations, each contributing distinct resources.\nGroup formation occurs as agents strategically optimize their choices in\nresponse to others within a competitive setting. We prove the existence of\nstable group equilibria and simulate formation dynamics under varying spatial\nand resource conditions. The results show that limited individual resources\nlead to groups that form mainly among nearby actors, while abundant resources\nallow groups to move across larger distances. Increased resource heterogeneity\nand spatial proximity promote the formation of larger and more diverse groups.\nThese findings reveal key trade-offs shaping group size and composition,\nguiding strategies for effective cross-sector collaborations and multi-agent\nsystems.", "AI": {"tldr": "\u7814\u7a76\u4eba\u5458\u5f00\u53d1\u4e86\u535a\u5f08\u8bba\u6a21\u578b\u548c\u57fa\u4e8e\u4ee3\u7406\u7684\u6a21\u578b\u6765\u7814\u7a76\u7531\u8d44\u6e90\u6574\u5408\u3001\u7a7a\u95f4\u805a\u96c6\u548c\u5f02\u8d28\u6027\u9a71\u52a8\u7684\u5c0f\u7ec4\u5f62\u6210\u3002\u8de8\u90e8\u95e8\u4f19\u4f34\u5173\u7cfb\uff08CSP\uff09\u6d89\u53ca\u516c\u5171\u3001\u79c1\u4eba\u548c\u975e\u8425\u5229\u7ec4\u7ec7\uff0c\u5b83\u4eec\u5404\u81ea\u8d21\u732e\u4e0d\u540c\u7684\u8d44\u6e90\u3002\u5f53\u53c2\u4e0e\u8005\u5728\u7ade\u4e89\u73af\u5883\u4e2d\u6839\u636e\u4ed6\u4eba\u7684\u9009\u62e9\u8fdb\u884c\u6218\u7565\u6027\u4f18\u5316\u65f6\uff0c\u5c31\u4f1a\u5f62\u6210\u5c0f\u7ec4\u3002\u7814\u7a76\u8bc1\u660e\u4e86\u7a33\u5b9a\u5c0f\u7ec4\u5747\u8861\u7684\u5b58\u5728\u6027\uff0c\u5e76\u6a21\u62df\u4e86\u5728\u4e0d\u540c\u7a7a\u95f4\u548c\u8d44\u6e90\u6761\u4ef6\u4e0b\u7684\u5c0f\u7ec4\u5f62\u6210\u52a8\u6001\u3002\u7ed3\u679c\u8868\u660e\uff0c\u6709\u9650\u7684\u4e2a\u4eba\u8d44\u6e90\u4f1a\u5bfc\u81f4\u4e3b\u8981\u5728\u9644\u8fd1\u53c2\u4e0e\u8005\u4e4b\u95f4\u5f62\u6210\u5c0f\u7ec4\uff0c\u800c\u4e30\u5bcc\u7684\u8d44\u6e90\u5219\u5141\u8bb8\u5c0f\u7ec4\u8de8\u8d8a\u66f4\u8fdc\u7684\u8ddd\u79bb\u3002\u589e\u52a0\u7684\u8d44\u6e90\u5f02\u8d28\u6027\u548c\u7a7a\u95f4\u90bb\u8fd1\u6027\u4fc3\u8fdb\u4e86\u66f4\u5927\u3001\u66f4\u591a\u6837\u5316\u7684\u5c0f\u7ec4\u7684\u5f62\u6210\u3002\u8fd9\u4e9b\u53d1\u73b0\u63ed\u793a\u4e86\u5f71\u54cd\u5c0f\u7ec4\u89c4\u6a21\u548c\u6784\u6210\u7684\u91cd\u8981\u6743\u8861\uff0c\u4e3a\u6709\u6548\u7684\u8de8\u90e8\u95e8\u5408\u4f5c\u548c\u591a\u4e3b\u4f53\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7b56\u7565\u3002", "motivation": "\u672c\u7814\u7a76\u7684\u52a8\u673a\u662f\u7406\u89e3\u548c\u6a21\u62df\u7531\u8d44\u6e90\u6574\u5408\u3001\u7a7a\u95f4\u805a\u96c6\u548c\u5f02\u8d28\u6027\u9a71\u52a8\u7684\u8de8\u90e8\u95e8\u4f19\u4f34\u5173\u7cfb\uff08CSP\uff09\u4e2d\u7684\u5c0f\u7ec4\u5f62\u6210\u8fc7\u7a0b\u3002\u7814\u7a76\u65e8\u5728\u63ed\u793a\u5f71\u54cd\u5c0f\u7ec4\u89c4\u6a21\u3001\u6784\u6210\u548c\u5f62\u6210\u52a8\u6001\u7684\u5173\u952e\u56e0\u7d20\uff0c\u4e3a\u4f18\u5316\u8de8\u90e8\u95e8\u5408\u4f5c\u63d0\u4f9b\u89c1\u89e3\u3002", "method": "\u672c\u7814\u7a76\u7ed3\u5408\u4e86\u535a\u5f08\u8bba\u6a21\u578b\u548c\u57fa\u4e8e\u4ee3\u7406\u7684\u6a21\u578b\u6765\u7814\u7a76\u5c0f\u7ec4\u5f62\u6210\u3002\u53c2\u4e0e\u8005\uff08\u4ee3\u8868\u516c\u5171\u3001\u79c1\u4eba\u548c\u975e\u8425\u5229\u7ec4\u7ec7\uff09\u6839\u636e\u8d44\u6e90\u6574\u5408\u3001\u7a7a\u95f4\u90bb\u8fd1\u6027\u548c\u5f02\u8d28\u6027\u7b49\u56e0\u7d20\uff0c\u5728\u7ade\u4e89\u73af\u5883\u4e2d\u6218\u7565\u6027\u5730\u4f18\u5316\u5176\u9009\u62e9\u3002\u7814\u7a76\u8bc1\u660e\u4e86\u7a33\u5b9a\u5c0f\u7ec4\u5747\u8861\u7684\u5b58\u5728\u6027\uff0c\u5e76\u901a\u8fc7\u6a21\u62df\u6765\u63a2\u7d22\u4e0d\u540c\u7a7a\u95f4\u548c\u8d44\u6e90\u6761\u4ef6\u4e0b\u7684\u5f62\u6210\u52a8\u6001\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u6709\u9650\u7684\u4e2a\u4eba\u8d44\u6e90\u4f1a\u4fc3\u4f7f\u5c0f\u7ec4\u4e3b\u8981\u5728\u7a7a\u95f4\u90bb\u8fd1\u7684\u53c2\u4e0e\u8005\u4e4b\u95f4\u5f62\u6210\uff0c\u800c\u5145\u8db3\u7684\u8d44\u6e90\u5219\u5141\u8bb8\u5c0f\u7ec4\u5728\u66f4\u5e7f\u6cdb\u7684\u533a\u57df\u5185\u5f62\u6210\u3002\u6b64\u5916\uff0c\u589e\u52a0\u7684\u8d44\u6e90\u5f02\u8d28\u6027\u548c\u7a7a\u95f4\u90bb\u8fd1\u6027\u80fd\u591f\u4fc3\u8fdb\u66f4\u5927\u3001\u66f4\u591a\u6837\u5316\u7684\u5c0f\u7ec4\u7684\u5f62\u6210\u3002\u8fd9\u4e9b\u53d1\u73b0\u63ed\u793a\u4e86\u5f71\u54cd\u5c0f\u7ec4\u89c4\u6a21\u548c\u6784\u6210\u7684\u5173\u952e\u6743\u8861\u3002", "conclusion": "\u672c\u7814\u7a76\u7684\u7ed3\u8bba\u662f\uff0c\u5c0f\u7ec4\u7684\u5f62\u6210\uff08\u5c24\u5176\u662f\u5728\u8de8\u90e8\u95e8\u5408\u4f5c\u80cc\u666f\u4e0b\uff09\u53d7\u5230\u8d44\u6e90\u53ef\u7528\u6027\u3001\u7a7a\u95f4\u56e0\u7d20\u548c\u53c2\u4e0e\u8005\u5f02\u8d28\u6027\u7684\u663e\u8457\u5f71\u54cd\u3002\u6709\u9650\u7684\u8d44\u6e90\u548c\u8f83\u5f3a\u7684\u7a7a\u95f4\u805a\u96c6\u6027\u503e\u5411\u4e8e\u5f62\u6210\u89c4\u6a21\u8f83\u5c0f\u3001\u66f4\u96c6\u4e2d\u7684\u5408\u4f5c\u5c0f\u7ec4\uff0c\u800c\u4e30\u5bcc\u7684\u8d44\u6e90\u548c\u8f83\u4f4e\u7684\u7a7a\u95f4\u9650\u5236\u5219\u5141\u8bb8\u5f62\u6210\u66f4\u5927\u3001\u66f4\u591a\u6837\u5316\u7684\u5408\u4f5c\u7f51\u7edc\u3002\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u4e86\u89e3\u548c\u6307\u5bfc\u6709\u6548\u7684\u8de8\u90e8\u95e8\u5408\u4f5c\u4ee5\u53ca\u8bbe\u8ba1\u591a\u4e3b\u4f53\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u7406\u8bba\u548c\u5b9e\u8df5\u6307\u5bfc\u3002"}}
{"id": "2509.18118", "categories": ["cs.LG", "cs.AR"], "pdf": "https://arxiv.org/pdf/2509.18118", "abs": "https://arxiv.org/abs/2509.18118", "authors": ["Marcelo Ribeiro", "Diogo Costa", "Gon\u00e7alo Moreira", "Sandro Pinto", "Tiago Gomes"], "title": "Decentor-V: Lightweight ML Training on Low-Power RISC-V Edge Devices", "comment": null, "summary": "Modern IoT devices increasingly rely on machine learning solutions to process\ndata locally. However, the lack of graphics processing units (GPUs) or\ndedicated accelerators on most platforms makes on-device training largely\ninfeasible, often requiring cloud-based services to perform this task. This\nprocedure often raises privacy-related concerns, and creates dependency on\nreliable and always-on connectivity. Federated Learning (FL) is a new trend\nthat addresses these issues by enabling decentralized and collaborative\ntraining directly on devices, but it requires highly efficient optimization\nalgorithms. L-SGD, a lightweight variant of stochastic gradient descent, has\nenabled neural network training on Arm Cortex-M Microcontroller Units (MCUs).\nThis work extends L-SGD to RISC-V-based MCUs, an open and emerging architecture\nthat still lacks robust support for on-device training. L-SGD was evaluated on\nboth Arm and RISC-V platforms using 32-bit floating-point arithmetic,\nhighlighting the performance impact of the absence of Floating-Point Units\n(FPUs) in RISC-V MCUs. To mitigate these limitations, we introduce an 8-bit\nquantized version of L-SGD for RISC-V, which achieves nearly 4x reduction in\nmemory usage and a 2.2x speedup in training time, with negligible accuracy\ndegradation.", "AI": {"tldr": "\u672c\u6587\u5c06 L-SGD \u4f18\u5316\u7b97\u6cd5\u6269\u5c55\u5230 RISC-V MCU\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd 8 \u4f4d\u91cf\u5316\u7248\u672c\uff0c\u4ee5\u63d0\u9ad8\u5185\u5b58\u4f7f\u7528\u7387\u548c\u8bad\u7ec3\u901f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u5ffd\u7565\u7684\u51c6\u786e\u6027\u635f\u5931\u3002", "motivation": "\u7531\u4e8e\u5927\u591a\u6570\u7269\u8054\u7f51\u8bbe\u5907\u7f3a\u4e4f GPU\uff0c\u4f20\u7edf\u7684\u57fa\u4e8e\u4e91\u7684\u673a\u5668\u5b66\u4e60\u8bad\u7ec3\u65b9\u6cd5\u5b58\u5728\u9690\u79c1\u548c\u8fde\u63a5\u6027\u95ee\u9898\u3002\u867d\u7136\u8054\u90a6\u5b66\u4e60 (FL) \u63d0\u51fa\u4e86\u4e00\u4e2a\u53bb\u4e2d\u5fc3\u5316\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u5b83\u9700\u8981\u9ad8\u6548\u7684\u4f18\u5316\u7b97\u6cd5\u3002L-SGD \u5df2\u8bc1\u660e\u53ef\u4ee5\u5728 Arm MCU \u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u4f46\u9700\u8981\u5c06\u5176\u6269\u5c55\u5230\u65b0\u5174\u7684 RISC-V \u67b6\u6784\u3002", "method": "\u5c06 L-SGD \u7b97\u6cd5\u6269\u5c55\u5230 RISC-V MCU\uff0c\u5e76\u9488\u5bf9 RISC-V \u5e73\u53f0\u4e0a\u7f3a\u4e4f FPU \u7684\u60c5\u51b5\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd 8 \u4f4d\u91cf\u5316\u7248\u672c\u7684 L-SGD\u3002", "result": "\u5728 Arm \u548c RISC-V \u5e73\u53f0\u4e0a\u8bc4\u4f30\u4e86 L-SGD\u30028 \u4f4d\u91cf\u5316 L-SGD \u5728 RISC-V \u4e0a\u5b9e\u73b0\u4e86\u5185\u5b58\u4f7f\u7528\u91cf\u51cf\u5c11 4 \u500d\uff0c\u8bad\u7ec3\u901f\u5ea6\u63d0\u9ad8 2.2 \u500d\uff0c\u4e14\u51c6\u786e\u6027\u635f\u5931\u53ef\u5ffd\u7565\u3002", "conclusion": "L-SGD \u7684\u6269\u5c55\u548c\u91cf\u5316\u7248\u672c\u6709\u6548\u5730\u89e3\u51b3\u4e86 RISC-V MCU \u4e0a\u8bbe\u5907\u7aef\u8bad\u7ec3\u7684\u6311\u6218\uff0c\u5728\u5185\u5b58\u548c\u901f\u5ea6\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\uff0c\u4e3a\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8bbe\u5907\u4e0a\u8fdb\u884c FL \u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2509.18432", "categories": ["cond-mat.mes-hall", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2509.18432", "abs": "https://arxiv.org/abs/2509.18432", "authors": ["Deepika Gill", "Sangeeta Sharma", "Sam Shallcross"], "title": "Generation of pure, spin polarized, and unpolarized charge currents at the few cycle limit of circularly polarized light", "comment": null, "summary": "In certain members of the transition metal dichalcogenide (TMDC) family,\nlaser pulses of oppositely circularly polarized light excite electrons of\nopposite spin. Here we show that in the few cycle limit such pulses generate\nnot only a spin density excitation, but also a spin current excitation.\nEmploying the example of the TMDC WSe$_2$ we show that pure spin currents, the\nflow of spin in the absence of net charge flow, 100% spin polarized currents,\nand charge currents are all accessible and controllable by tuning the amplitude\nof ~ 5 femtosecond gap tuned light pulses. Underpinning this physics is a\nsymmetry lowering of the valley charge excitation from C3 at long duration to\nC2 in the few cycle limit, imbuing the excitation with net current. Our results\nboth highlight the emergence of a rich light-spin current coupling at ultrafast\ntimes in the TMDC family, as well presenting a route to the all-optical\ngeneration of pure spin currents.", "AI": {"tldr": "\u77ed\u8109\u51b2\u6fc0\u5149\u53ef\u7528\u4e8e\u5728 WSe2 \u4e2d\u751f\u6210\u7eaf\u51c0\u7684\u81ea\u65cb\u7535\u6d41\u3002", "motivation": "\u63a2\u7d22\u8fc7\u6e21\u91d1\u5c5e\u4e8c\u5364\u4ee3\u7269 (TMDC) \u5728\u5c11\u6570\u5468\u671f\u6781\u9650\u4e0b\u5bf9\u5706\u504f\u632f\u5149\u8109\u51b2\u7684\u54cd\u5e94\uff0c\u7279\u522b\u662f\u7814\u7a76\u81ea\u65cb\u5bc6\u5ea6\u548c\u81ea\u65cb\u6d41\u7684\u4ea7\u751f\u3002", "method": "\u4f7f\u7528 WSe2 \u4f5c\u4e3a\u6a21\u578b\u7cfb\u7edf\uff0c\u901a\u8fc7\u8c03\u8282~5\u98de\u79d2\u7684\u7d2b\u5916-\u53ef\u89c1\u5149\u8109\u51b2\u7684\u5e45\u5ea6\u6765\u7814\u7a76\u7eaf\u81ea\u65cb\u6d41\u3001100% \u81ea\u65cb\u6781\u5316\u7535\u6d41\u548c\u7535\u8377\u6d41\u3002", "result": "\u53d1\u73b0\u5c11\u6570\u5468\u671f\u8109\u51b2\u4e0d\u4ec5\u4ea7\u751f\u81ea\u65cb\u5bc6\u5ea6\u6fc0\u53d1\uff0c\u8fd8\u4ea7\u751f\u81ea\u65cb\u6d41\u6fc0\u53d1\u3002\u901a\u8fc7\u8c03\u8c10\u8109\u51b2\u5e45\u5ea6\uff0c\u53ef\u4ee5\u63a7\u5236\u7eaf\u51c0\u81ea\u65cb\u6d41\u3001100% \u81ea\u65cb\u6781\u5316\u7535\u6d41\u548c\u7535\u8377\u6d41\u3002", "conclusion": "\u5c11\u6570\u5468\u671f\u6fc0\u5149\u8109\u51b2\u53ef\u7528\u4e8e\u5728 TMDC \u4e2d\u5b9e\u73b0\u7eaf\u51c0\u81ea\u65cb\u6d41\u7684\u5168\u5149\u751f\u6210\uff0c\u8fd9\u5f52\u56e0\u4e8e\u5bf9\u79f0\u6027\u4ece C3 \u53d8\u4e3a C2\uff0c\u4ece\u800c\u5728\u6fc0\u53d1\u4e2d\u4ea7\u751f\u4e86\u51c0\u6d41\u3002"}}
{"id": "2509.18335", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2509.18335", "abs": "https://arxiv.org/abs/2509.18335", "authors": ["Giovanni Marini"], "title": "Nonthermal magnetization pathways in photoexcited semiconductors", "comment": null, "summary": "The stabilization of long-range magnetic order in nominally non-magnetic\nsemi- conductors using femtosecond light pulses is an exciting yet\nexperimentally challenging goal. Theoretical studies indicate that certain\nnon-magnetic semi- conductors can exhibit transient magnetic instabilities\nfollowing above-gap laser excitation, but the dynamical pathways leading to\nthese states remain largely unexplored. In this work, I introduce a minimal\nreal-time spin-orbital model and identify the fundamental microscopic\nmechanisms that enable the emergence of a transient magnetic order. I then\ndiscuss the relevance of these findings for real materials employing a\nphenomenological time-dependent Ginzburg- Landau model. Finally, I analyze the\nstrengths and limitations of current first-principles methodologies for\ninvestigating dynamically induced broken- symmetry states in the light of the\npresent results.", "AI": {"tldr": "\u5229\u7528\u98de\u79d2\u5149\u8109\u51b2\u5728\u540d\u4e49\u4e0a\u975e\u78c1\u6027\u7684\u534a\u5bfc\u4f53\u4e2d\u7a33\u5b9a\u957f\u7a0b\u78c1\u5e8f\u662f\u4e00\u4e2a\u4ee4\u4eba\u5174\u594b\u4f46\u5b9e\u9a8c\u4e0a\u5177\u6709\u6311\u6218\u6027\u7684\u76ee\u6807\u3002\u7406\u8bba\u7814\u7a76\u8868\u660e\uff0c\u67d0\u4e9b\u975e\u78c1\u6027\u534a\u5bfc\u4f53\u5728\u6fc0\u5b50\u6fc0\u5149\u6fc0\u53d1\u540e\u4f1a\u8868\u73b0\u51fa\u77ac\u6001\u78c1\u4e0d\u7a33\u5b9a\u6027\uff0c\u4f46\u5bfc\u81f4\u8fd9\u4e9b\u72b6\u6001\u7684\u52a8\u529b\u5b66\u9014\u5f84\u4ecd\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u6700\u5c0f\u7684\u5b9e\u65f6\u81ea\u65cb\u8f68\u9053\u6a21\u578b\uff0c\u5e76\u786e\u5b9a\u4e86\u5b9e\u73b0\u77ac\u6001\u78c1\u5e8f\u6d8c\u73b0\u7684\u57fa\u672c\u5fae\u89c2\u673a\u5236\u3002\u7136\u540e\uff0c\u5229\u7528\u73b0\u8c61\u5b66\u7684\u65f6\u95f4\u76f8\u5173 Ginzburg-Landau \u6a21\u578b\u8ba8\u8bba\u4e86\u8fd9\u4e9b\u53d1\u73b0\u4e0e\u5b9e\u9645\u6750\u6599\u7684\u76f8\u5173\u6027\u3002\u6700\u540e\uff0c\u7ed3\u5408\u5f53\u524d\u7ed3\u679c\uff0c\u5206\u6790\u4e86\u7528\u4e8e\u7814\u7a76\u52a8\u6001\u8bf1\u5bfc\u7684\u5bf9\u79f0\u6027\u7834\u7f3a\u72b6\u6001\u7684\u5f53\u524d\u7b2c\u4e00\u6027\u539f\u7406\u65b9\u6cd5\u7684\u4f18\u7f3a\u70b9\u3002", "motivation": "\u65e8\u5728\u63a2\u7d22\u5229\u7528\u98de\u79d2\u5149\u8109\u51b2\u5728\u975e\u78c1\u6027\u534a\u5bfc\u4f53\u4e2d\u5b9e\u73b0\u77ac\u6001\u957f\u7a0b\u78c1\u5e8f\u7684\u7406\u8bba\u548c\u5b9e\u9a8c\u6311\u6218\uff0c\u7279\u522b\u662f\u7814\u7a76\u5bfc\u81f4\u8fd9\u4e9b\u78c1\u6001\u7684\u52a8\u529b\u5b66\u9014\u5f84\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u6700\u5c0f\u7684\u5b9e\u65f6\u81ea\u65cb\u8f68\u9053\u6a21\u578b\uff0c\u5e76\u5229\u7528\u73b0\u8c61\u5b66\u7684\u65f6\u95f4\u76f8\u5173 Ginzburg-Landau \u6a21\u578b\u6765\u8bc6\u522b\u5fae\u89c2\u673a\u5236\u5e76\u8ba8\u8bba\u5176\u5728\u5b9e\u9645\u6750\u6599\u4e2d\u7684\u76f8\u5173\u6027\u3002", "result": "\u8bc6\u522b\u4e86\u5b9e\u73b0\u77ac\u6001\u78c1\u5e8f\u6d8c\u73b0\u7684\u57fa\u672c\u5fae\u89c2\u673a\u5236\uff0c\u5e76\u8ba8\u8bba\u4e86\u5176\u5728\u5b9e\u9645\u6750\u6599\u4e2d\u7684\u76f8\u5173\u6027\u3002", "conclusion": "\u5206\u6790\u4e86\u5f53\u524d\u7b2c\u4e00\u6027\u539f\u7406\u65b9\u6cd5\u5728\u7814\u7a76\u52a8\u6001\u8bf1\u5bfc\u7684\u5bf9\u79f0\u6027\u7834\u7f3a\u72b6\u6001\u65b9\u9762\u7684\u4f18\u52bf\u548c\u5c40\u9650\u6027\u3002"}}
{"id": "2509.18325", "categories": ["cs.SI", "physics.soc-ph"], "pdf": "https://arxiv.org/pdf/2509.18325", "abs": "https://arxiv.org/abs/2509.18325", "authors": ["Huaizhi Liao", "Tian Qiu", "Guang Chen"], "title": "A Graph-Neural-Network-Entropy model of vital node identification on network attack and propagation", "comment": null, "summary": "Vital nodes usually play a key role in complex networks. Uncovering these\nnodes is an important task in protecting the network, especially when the\nnetwork suffers intentional attack. Many existing methods have not fully\nintegrated the node feature, interaction and state. In this article, we propose\na novel method (GNNE) based on graph neural networks and information entropy.\nThe method employs a Graph Convolutional Network (GCN) to learn the nodes'\nfeatures, which are input into a Graph Attention Network (GAT) to obtain the\ninfluence factor of nodes, and the node influence factors are used to calculate\nthe nodes' entropy to evaluate the node importance. The GNNE takes advantage of\nthe GCN and GAT, with the GCN well extracting the nodes' features and the GAT\naggregating the features of the nodes' neighbors by using the attention\nmechanism to assign different weights to the neighbors with different\nimportance, and the nodes' entropy quantifies the nodes' state in the network.\nThe proposed method is trained on a synthetic Barabasi-Albert network, and\ntested on six real datasets. Compared with eight traditional topology-based\nmethods and four graph-machine-learning-based methods, the GNNE shows an\nadvantage for the vital node identification in the perspectives of network\nattack and propagation.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\u548c\u4fe1\u606f\u71b5\u7684\u65b0\u65b9\u6cd5\uff08GNNE\uff09\uff0c\u7528\u4e8e\u8bc6\u522b\u590d\u6742\u7f51\u7edc\u4e2d\u7684\u5173\u952e\u8282\u70b9\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u6574\u5408\u8282\u70b9\u7279\u5f81\u3001\u4ea4\u4e92\u548c\u72b6\u6001\uff0c\u800c\u5173\u952e\u8282\u70b9\u7684\u8bc6\u522b\u5bf9\u4e8e\u7f51\u7edc\u9632\u62a4\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u7f51\u7edc\u906d\u53d7\u653b\u51fb\u65f6\u3002", "method": "GNNE\u65b9\u6cd5\u7ed3\u5408\u4e86\u56fe\u5377\u79ef\u7f51\u7edc\uff08GCN\uff09\u548c\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\uff08GAT\uff09\u3002GCN\u7528\u4e8e\u5b66\u4e60\u8282\u70b9\u7279\u5f81\uff0c\u7136\u540e\u5c06\u8fd9\u4e9b\u7279\u5f81\u8f93\u5165GAT\u4ee5\u83b7\u5f97\u8282\u70b9\u7684\u201c\u5f71\u54cd\u56e0\u5b50\u201d\u3002\u6700\u540e\uff0c\u5229\u7528\u8fd9\u4e9b\u5f71\u54cd\u56e0\u5b50\u8ba1\u7b97\u8282\u70b9\u7684\u71b5\uff0c\u4ee5\u8bc4\u4f30\u8282\u70b9\u7684\u91cd\u8981\u6027\u3002GCN\u8d1f\u8d23\u63d0\u53d6\u8282\u70b9\u7279\u5f81\uff0cGAT\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u4e3a\u4e0d\u540c\u90bb\u5c45\u5206\u914d\u4e0d\u540c\u6743\u91cd\u6765\u805a\u5408\u90bb\u5c45\u7279\u5f81\uff0c\u800c\u8282\u70b9\u71b5\u5219\u91cf\u5316\u4e86\u8282\u70b9\u5728\u7f51\u7edc\u4e2d\u7684\u72b6\u6001\u3002", "result": "\u5728\u5408\u6210\u7684Barabasi-Albert\u7f51\u7edc\u4e0a\u8bad\u7ec3\uff0c\u5e76\u5728\u516d\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u6d4b\u8bd5\u3002\u4e0e\u516b\u79cd\u4f20\u7edf\u7684\u57fa\u4e8e\u62d3\u6251\u7684\u65b9\u6cd5\u548c\u56db\u79cd\u57fa\u4e8e\u56fe\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u76f8\u6bd4\uff0cGNNE\u5728\u7f51\u7edc\u653b\u51fb\u548c\u4f20\u64ad\u65b9\u9762\u663e\u793a\u51fa\u8bc6\u522b\u5173\u952e\u8282\u70b9\u7684\u4f18\u52bf\u3002", "conclusion": "GNNE\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5730\u8bc6\u522b\u590d\u6742\u7f51\u7edc\u4e2d\u7684\u5173\u952e\u8282\u70b9\uff0c\u5e76\u5728\u7f51\u7edc\u653b\u51fb\u548c\u4f20\u64ad\u573a\u666f\u4e0b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2509.18426", "categories": ["eess.SP", "physics.ins-det"], "pdf": "https://arxiv.org/pdf/2509.18426", "abs": "https://arxiv.org/abs/2509.18426", "authors": ["Ziad Hatab", "Michael Ernst Gadringer", "Arash Arsanjani", "Wolfgang Boesch"], "title": "Automatic Model Extraction of the Match Standard in Symmetric--Reciprocal--Match Calibration", "comment": "https://github.com/ZiadHatab/srm-calibration", "summary": "This paper addresses the modeling of parasitics of the match standard in the\nsymmetric-reciprocal-match (SRM) calibration method of vector network analyzers\n(VNAs). In the general SRM procedure, the match standard is assumed to be fully\nknown. Here, we demonstrate that the match can be modeled with an arbitrary\nfrequency-dependent model using a non-linear global optimization procedure. To\nhighlight the validity of the suggested approach, numerical tests were\nconducted, demonstrating the ability to recover the match standard parasitic\nmodel down to software numerical precision. Additionally, we performed\nmicrostrip line measurements to compare the SRM calibration with match modeling\nto the multiline thru-reflect-line (TRL) calibration one, showing that\nautomatic model extraction can achieve accuracy similar to using a match\nstandard defined through multiline TRL calibration.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.18327", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18327", "abs": "https://arxiv.org/abs/2509.18327", "authors": ["Katherine H. Allen", "Chris Rogers", "Elaine S. Short"], "title": "Haptic Communication in Human-Human and Human-Robot Co-Manipulation", "comment": "9 pages, 18 figures, ROMAN 2025", "summary": "When a human dyad jointly manipulates an object, they must communicate about\ntheir intended motion plans. Some of that collaboration is achieved through the\nmotion of the manipulated object itself, which we call \"haptic communication.\"\nIn this work, we captured the motion of human-human dyads moving an object\ntogether with one participant leading a motion plan about which the follower is\nuninformed. We then captured the same human participants manipulating the same\nobject with a robot collaborator. By tracking the motion of the shared object\nusing a low-cost IMU, we can directly compare human-human shared manipulation\nto the motion of those same participants interacting with the robot.\nIntra-study and post-study questionnaires provided participant feedback on the\ncollaborations, indicating that the human-human collaborations are\nsignificantly more fluent, and analysis of the IMU data indicates that it\ncaptures objective differences in the motion profiles of the conditions. The\ndifferences in objective and subjective measures of accuracy and fluency\nbetween the human-human and human-robot trials motivate future research into\nimproving robot assistants for physical tasks by enabling them to send and\nreceive anthropomorphic haptic signals.", "AI": {"tldr": "\u4eba\u7c7b\u4e24\u4eba\u4e00\u7ec4\u5171\u540c\u64cd\u4f5c\u4e00\u4e2a\u7269\u4f53\u65f6\uff0c\u9700\u8981\u5c31\u5404\u81ea\u610f\u56fe\u7684\u8fd0\u52a8\u8ba1\u5212\u8fdb\u884c\u6c9f\u901a\u3002\u5176\u4e2d\u4e00\u90e8\u5206\u534f\u4f5c\u662f\u901a\u8fc7\u88ab\u64cd\u7eb5\u7269\u4f53\u7684\u8fd0\u52a8\u672c\u8eab\u6765\u5b9e\u73b0\u7684\uff0c\u6211\u4eec\u79f0\u4e4b\u4e3a\u201c\u89e6\u89c9\u901a\u4fe1\u201d\u3002\u672c\u7814\u7a76\u6355\u6349\u4e86\u4eba\u7c7b\u4e24\u4eba\u4e00\u7ec4\u5171\u540c\u79fb\u52a8\u4e00\u4e2a\u7269\u4f53\u65f6\u7684\u8fd0\u52a8\uff0c\u5176\u4e2d\u4e00\u540d\u53c2\u4e0e\u8005\u9886\u5148\u6267\u884c\u4e00\u9879\u8ddf\u968f\u8005\u4e0d\u77e5\u60c5\u7684\u8fd0\u52a8\u8ba1\u5212\u3002\u7136\u540e\uff0c\u6211\u4eec\u6355\u6349\u4e86\u540c\u4e00\u6279\u53c2\u4e0e\u8005\u5728\u4e0e\u673a\u5668\u4eba\u534f\u4f5c\u8005\u5171\u540c\u64cd\u4f5c\u540c\u4e00\u7269\u4f53\u65f6\u7684\u8fd0\u52a8\u3002\u901a\u8fc7\u4f7f\u7528\u4f4e\u6210\u672c\u7684\u60ef\u6027\u6d4b\u91cf\u5355\u5143\uff08IMU\uff09\u8ddf\u8e2a\u5171\u4eab\u7269\u4f53\u7684\u8fd0\u52a8\uff0c\u6211\u4eec\u53ef\u4ee5\u76f4\u63a5\u6bd4\u8f83\u4eba\u7c7b\u4e4b\u95f4\u7684\u5171\u4eab\u64cd\u4f5c\u4e0e\u540c\u4e00\u6279\u53c2\u4e0e\u8005\u4e0e\u673a\u5668\u4eba\u4ea4\u4e92\u65f6\u7684\u8fd0\u52a8\u3002", "motivation": "\u7814\u7a76\u4eba\u7c7b\u4e24\u4eba\u4e00\u7ec4\u5171\u540c\u64cd\u4f5c\u7269\u4f53\u65f6\u7684\u89e6\u89c9\u901a\u4fe1\uff0c\u5e76\u4e0e\u4eba\u7c7b\u4e0e\u673a\u5668\u4eba\u534f\u4f5c\u8fdb\u884c\u6bd4\u8f83\uff0c\u65e8\u5728\u6539\u8fdb\u673a\u5668\u4eba\u6267\u884c\u7269\u7406\u4efb\u52a1\u7684\u80fd\u529b\u3002", "method": "\u6355\u6349\u4eba\u7c7b\u4e24\u4eba\u4e00\u7ec4\u5171\u540c\u64cd\u4f5c\u7269\u4f53\uff08\u5176\u4e2d\u4e00\u4eba\u9886\u5148\uff0c\u53e6\u4e00\u4eba\u8ddf\u968f\uff09\u4ee5\u53ca\u4eba\u7c7b\u4e0e\u673a\u5668\u4eba\u534f\u4f5c\u64cd\u4f5c\u540c\u4e00\u7269\u4f53\u7684\u8fd0\u52a8\u3002\u4f7f\u7528\u4f4e\u6210\u672c\u60ef\u6027\u6d4b\u91cf\u5355\u5143\uff08IMU\uff09\u8ddf\u8e2a\u5171\u4eab\u7269\u4f53\u7684\u8fd0\u52a8\uff0c\u5e76\u6536\u96c6\u53c2\u4e0e\u8005\u7684\u95ee\u5377\u53cd\u9988\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u4eba\u7c7b\u4e4b\u95f4\u7684\u534f\u4f5c\u660e\u663e\u66f4\u6d41\u7545\u3002IMU \u6570\u636e\u5206\u6790\u663e\u793a\uff0c\u5728\u8fd0\u52a8\u6a21\u5f0f\u65b9\u9762\uff0c\u4eba\u7c7b\u534f\u4f5c\u4e0e\u4eba\u673a\u534f\u4f5c\u4e4b\u95f4\u5b58\u5728\u5ba2\u89c2\u5dee\u5f02\u3002\u5728\u51c6\u786e\u6027\u548c\u6d41\u7545\u6027\u65b9\u9762\uff0c\u4eba\u7c7b\u534f\u4f5c\u548c\u4eba\u673a\u534f\u4f5c\u5728\u4e3b\u5ba2\u89c2\u6d4b\u91cf\u6307\u6807\u4e0a\u5747\u5b58\u5728\u5dee\u5f02\u3002", "conclusion": "\u4eba\u7c7b\u4e4b\u95f4\u7684\u534f\u4f5c\u6bd4\u4e0e\u673a\u5668\u4eba\u7684\u534f\u4f5c\u66f4\u6d41\u7545\uff0c\u5e76\u4e14\u5728\u8fd0\u52a8\u6a21\u5f0f\u4e0a\u5b58\u5728\u5ba2\u89c2\u5dee\u5f02\u3002\u8fd9\u4e9b\u5dee\u5f02\u8868\u660e\uff0c\u672a\u6765\u7684\u673a\u5668\u4eba\u52a9\u624b\u9700\u8981\u5728\u6267\u884c\u7269\u7406\u4efb\u52a1\u65f6\uff0c\u80fd\u591f\u53d1\u9001\u548c\u63a5\u6536\u62df\u4eba\u5316\u7684\u89e6\u89c9\u4fe1\u53f7\uff0c\u4ee5\u63d0\u9ad8\u534f\u4f5c\u6548\u679c\u3002"}}
{"id": "2509.18165", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18165", "abs": "https://arxiv.org/abs/2509.18165", "authors": ["Xiuding Cai", "Yaoyao Zhu", "Linjie Fu", "Dong Miao", "Yu Yao"], "title": "Self Identity Mapping", "comment": "Early accepted by Neural Networks 2025", "summary": "Regularization is essential in deep learning to enhance generalization and\nmitigate overfitting. However, conventional techniques often rely on\nheuristics, making them less reliable or effective across diverse settings. We\npropose Self Identity Mapping (SIM), a simple yet effective, data-intrinsic\nregularization framework that leverages an inverse mapping mechanism to enhance\nrepresentation learning. By reconstructing the input from its transformed\noutput, SIM reduces information loss during forward propagation and facilitates\nsmoother gradient flow. To address computational inefficiencies, We instantiate\nSIM as $ \\rho\\text{SIM} $ by incorporating patch-level feature sampling and\nprojection-based method to reconstruct latent features, effectively lowering\ncomplexity. As a model-agnostic, task-agnostic regularizer, SIM can be\nseamlessly integrated as a plug-and-play module, making it applicable to\ndifferent network architectures and tasks.\n  We extensively evaluate $\\rho\\text{SIM}$ across three tasks: image\nclassification, few-shot prompt learning, and domain generalization.\nExperimental results show consistent improvements over baseline methods,\nhighlighting $\\rho\\text{SIM}$'s ability to enhance representation learning\nacross various tasks. We also demonstrate that $\\rho\\text{SIM}$ is orthogonal\nto existing regularization methods, boosting their effectiveness. Moreover, our\nresults confirm that $\\rho\\text{SIM}$ effectively preserves semantic\ninformation and enhances performance in dense-to-dense tasks, such as semantic\nsegmentation and image translation, as well as in non-visual domains including\naudio classification and time series anomaly detection. The code is publicly\navailable at https://github.com/XiudingCai/SIM-pytorch.", "AI": {"tldr": "SIM\u662f\u4e00\u79cd\u6570\u636e\u5185\u5728\u6b63\u5219\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u9006\u6620\u5c04\u673a\u5236\u589e\u5f3a\u8868\u793a\u5b66\u4e60\uff0c\u51cf\u5c11\u4fe1\u606f\u635f\u5931\uff0c\u4fc3\u8fdb\u68af\u5ea6\u6d41\u52a8\u3002\u5176\u8ba1\u7b97\u6548\u7387\u7248\u672c$\rho$SIM\u901a\u8fc7\u7279\u5f81\u91c7\u6837\u548c\u6295\u5f71\u5b9e\u73b0\uff0c\u53ef\u5373\u63d2\u5373\u7528\uff0c\u5e76\u80fd\u4e0e\u5176\u4ed6\u6b63\u5219\u5316\u65b9\u6cd5\u534f\u540c\u589e\u6548\uff0c\u5728\u591a\u79cd\u89c6\u89c9\u548c\u975e\u89c6\u89c9\u4efb\u52a1\u4e2d\u5747\u6709\u63d0\u5347\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u6b63\u5219\u5316\u5bf9\u4e8e\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\u548c\u51cf\u5c11\u8fc7\u62df\u5408\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u591a\u4f9d\u8d56\u542f\u53d1\u5f0f\uff0c\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u6548\u679c\u4e0d\u7a33\u5b9a\u3002", "method": "\u63d0\u51fa\u81ea\u8eab\u4efd\u6620\u5c04\uff08SIM\uff09\u6846\u67b6\uff0c\u5229\u7528\u9006\u6620\u5c04\u673a\u5236\uff08\u5c06\u8f93\u5165\u4ece\u5176\u53d8\u6362\u540e\u7684\u8f93\u51fa\u4e2d\u91cd\u5efa\uff09\u6765\u589e\u5f3a\u8868\u793a\u5b66\u4e60\u3002\u8ba1\u7b97\u6548\u7387\u7248\u672c$\rho$SIM\u901a\u8fc7\u5f15\u5165\u5757\u7ea7\u7279\u5f81\u91c7\u6837\u548c\u57fa\u4e8e\u6295\u5f71\u7684\u65b9\u6cd5\u6765\u964d\u4f4e\u590d\u6742\u6027\u3002", "result": "$\rho$SIM\u5728\u56fe\u50cf\u5206\u7c7b\u3001\u5c11\u6837\u672c\u63d0\u793a\u5b66\u4e60\u548c\u57df\u6cdb\u5316\u4efb\u52a1\u4e0a\u5747\u53d6\u5f97\u4e86\u4e00\u81f4\u6027\u63d0\u5347\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u80fd\u4e0e\u5176\u4ed6\u6b63\u5219\u5316\u65b9\u6cd5\u534f\u540c\u589e\u6548\u3002\u8be5\u65b9\u6cd5\u5728\u5bc6\u96c6\u5230\u5bc6\u96c6\u4efb\u52a1\uff08\u5982\u8bed\u4e49\u5206\u5272\u3001\u56fe\u50cf\u7ffb\u8bd1\uff09\u4ee5\u53ca\u975e\u89c6\u89c9\u57df\uff08\u5982\u97f3\u9891\u5206\u7c7b\u3001\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\uff09\u4e2d\u5747\u6709\u6548\u3002", "conclusion": "SIM\u662f\u4e00\u79cd\u6709\u6548\u4e14\u901a\u7528\u7684\u6b63\u5219\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u91cd\u5efa\u8f93\u5165\u6765\u6539\u5584\u8868\u793a\u5b66\u4e60\u3002\u5176\u8ba1\u7b97\u6548\u7387\u7248\u672c$\rho$SIM\u6613\u4e8e\u96c6\u6210\uff0c\u5e76\u80fd\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u63d0\u5347\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u8bed\u4e49\u4fe1\u606f\u3002"}}
{"id": "2509.18206", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2509.18206", "abs": "https://arxiv.org/abs/2509.18206", "authors": ["Nomenjanahary Tanjonirina Manampisoa", "Ravo Tokiniaina Ranaivoson", "Roland Raboanary", "Raoelina Andriambololona", "Rivo Herivola Manjakamanana Ravelonjato"], "title": "Joint momenta-coordinates states as pointer states in quantum decoherence", "comment": "16 pages", "summary": "Quantum decoherence provides a framework to study the emergence of\nclassicality from quantum system by showing how interactions with the\nenvironments suppress interferences and select robust states known as pointer\nstates. Earlier studies have linked Gaussian coherent states with pointer\nstates. More recently, it was conjectured that more general quantum states\ncalled joint momenta-coordinates states could also be considered as more\nsuitable candidates to be pointer states. These states are associated to the\nconcept of quantum phase space and saturate, by definition, generalized\nuncertainty relations. In this work, we rigorously prove this conjecture.\nBuilding on the Lindblad framework for the damped harmonic oscillator and\napplying Zurek's predictability-sieve criterion, we analyze both underdamped\nand overdamped regimes. We show that only in the underdamped case do joint\nmomenta-coordinates states remain pure and robust for all times, establishing\nthem as the true pointer states. This extends Isar's earlier underdamped\ntreatment, generalizes the concept beyond Gaussian approximations, and embeds\nclassical robustness in the quantum phase space formalism, with potential\napplications in error-resilient quantum information.", "AI": {"tldr": "\u8054\u5408\u52a8\u91cf-\u5750\u6807\u6001\u88ab\u4e25\u683c\u8bc1\u660e\u662f\u91cf\u5b50\u9000\u76f8\u5e72\u4e2d\u7684\u6307\u9488\u6001\uff0c\u7279\u522b\u662f\u5728\u6b20\u963b\u5c3c\u60c5\u51b5\u4e0b\uff0c\u8fd9\u6269\u5c55\u4e86\u5148\u524d\u5173\u4e8e\u9ad8\u65af\u6001\u7684\u7814\u7a76\uff0c\u5e76\u5bf9\u91cf\u5b50\u4fe1\u606f\u6709\u6f5c\u5728\u5e94\u7528\u3002", "motivation": "\u91cf\u5b50\u9000\u76f8\u5e72\u5982\u4f55\u4ece\u91cf\u5b50\u7cfb\u7edf\u4e2d\u4ea7\u751f\u7ecf\u5178\u6027\uff0c\u7279\u522b\u662f\u786e\u5b9a\u54ea\u4e9b\u91cf\u5b50\u6001\u53ef\u4ee5\u88ab\u89c6\u4e3a\u6307\u9488\u6001\u3002", "method": "\u5728\u5170\u5e03\u62c9\u5fb7\u6846\u67b6\u4e0b\uff0c\u7814\u7a76\u4e86\u6b20\u963b\u5c3c\u548c\u8fc7\u963b\u5c3c\u4e24\u79cd\u60c5\u51b5\u4e0b\u7684\u53d7\u963b\u5c3c\u8c10\u632f\u5b50\uff0c\u5e76\u5e94\u7528\u4e86 Zurek \u7684\u53ef\u9884\u6d4b\u6027\u7b5b\u9009\u51c6\u5219\u3002", "result": "\u53ea\u6709\u5728\u6b20\u963b\u5c3c\u60c5\u51b5\u4e0b\uff0c\u8054\u5408\u52a8\u91cf-\u5750\u6807\u6001\u624d\u80fd\u5728\u6240\u6709\u65f6\u95f4\u5185\u4fdd\u6301\u7eaf\u51c0\u548c\u7a33\u5065\uff0c\u4ece\u800c\u88ab\u786e\u5b9a\u4e3a\u771f\u6b63\u7684\u6307\u9488\u6001\u3002", "conclusion": "\u8054\u5408\u52a8\u91cf-\u5750\u6807\u6001\u662f\u91cf\u5b50\u9000\u76f8\u5e72\u4e2d\u7684\u6307\u9488\u6001\uff0c\u7279\u522b\u662f\u5728\u6b20\u963b\u5c3c\u60c5\u51b5\u4e0b\uff0c\u8fd9\u6269\u5c55\u4e86\u5148\u524d\u7684\u7814\u7a76\uff0c\u5e76\u5c06\u7ecf\u5178\u7a33\u5065\u6027\u878d\u5165\u4e86\u91cf\u5b50\u76f8\u7a7a\u95f4\u5f62\u5f0f\u4e3b\u4e49\uff0c\u5bf9\u5bb9\u9519\u91cf\u5b50\u4fe1\u606f\u5177\u6709\u6f5c\u5728\u5e94\u7528\u3002"}}
{"id": "2509.18105", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18105", "abs": "https://arxiv.org/abs/2509.18105", "authors": ["Nachiket N. Naik", "Prathamesh Dinesh Joshi", "Raj Abhijit Dandekar", "Rajat Dandekar", "Sreedath Panat"], "title": "BULL-ODE: Bullwhip Learning with Neural ODEs and Universal Differential Equations under Stochastic Demand", "comment": null, "summary": "We study learning of continuous-time inventory dynamics under stochastic\ndemand and quantify when structure helps or hurts forecasting of the bullwhip\neffect. BULL-ODE compares a fully learned Neural ODE (NODE) that models the\nentire right-hand side against a physics-informed Universal Differential\nEquation (UDE) that preserves conservation and order-up-to structure while\nlearning a small residual policy term. Classical supply chain models explain\nthe bullwhip through control/forecasting choices and information sharing, while\nrecent physics-informed and neural differential equation methods blend domain\nconstraints with learned components. It is unclear whether structural bias\nhelps or hinders forecasting under different demand regimes. We address this by\nusing a single-echelon testbed with three demand regimes - AR(1)\n(autocorrelated), i.i.d. Gaussian, and heavy-tailed lognormal. Training is done\non varying fractions of each trajectory, followed by evaluation of multi-step\nforecasts for inventory I, order rate O, and demand D. Across the structured\nregimes, UDE consistently generalizes better: with 90% of the training horizon,\ninventory RMSE drops from 4.92 (NODE) to 0.26 (UDE) under AR(1) and from 5.96\nto 0.95 under Gaussian demand. Under heavy-tailed lognormal shocks, the\nflexibility of NODE is better. These trends persist as train18 ing data\nshrinks, with NODE exhibiting phase drift in extrapolation while UDE remains\nstable but underreacts to rare spikes. Our results provide concrete guidance:\nenforce structure when noise is light-tailed or temporally correlated; relax\nstructure when extreme events dominate. Beyond inventory control, the results\noffer guidance for hybrid modeling in scientific and engineering systems:\nenforce known structure when conservation laws and modest noise dominate, and\nrelax structure to capture extremes in settings where rare events drive\ndynamics.", "AI": {"tldr": "\u5f53\u9700\u6c42\u670d\u4ece\u8f7b\u5c3e\u6216\u65f6\u95f4\u76f8\u5173\u566a\u58f0\u65f6\uff0c\u5f3a\u5236\u6267\u884c\u7ed3\u6784\uff1b\u5f53\u6781\u7aef\u4e8b\u4ef6\u5360\u4e3b\u5bfc\u5730\u4f4d\u65f6\uff0c\u653e\u5bbd\u7ed3\u6784\u3002", "motivation": "\u5728\u968f\u673a\u9700\u6c42\u4e0b\u5b66\u4e60\u8fde\u7eed\u65f6\u95f4\u5e93\u5b58\u52a8\u6001\uff0c\u5e76\u91cf\u5316\u7ed3\u6784\u4f55\u65f6\u6709\u52a9\u4e8e\u6216\u963b\u788d\u725b\u97ad\u6548\u5e94\u7684\u9884\u6d4b\u3002", "method": "\u6bd4\u8f83\u4e86\u5b8c\u5168\u5b66\u4e60\u7684\u795e\u7ecf\u7f51\u7edcODE\uff08NODE\uff09\u548c\u4fdd\u7559\u5b88\u6052\u548c\u8ba2\u5355\u5230\u7ed3\u6784\u4f46\u5b66\u4e60\u4e00\u5c0f\u90e8\u5206\u6b8b\u5dee\u7b56\u7565\u9879\u7684\u7269\u7406\u4fe1\u606f\u901a\u7528\u5fae\u5206\u65b9\u7a0b\uff08UDE\uff09\u3002", "result": "\u5728AR(1)\u548c\u9ad8\u65af\u9700\u6c42\u4e0b\uff0cUDE\u5728\u5e93\u5b58RMSE\u65b9\u9762\u4f18\u4e8eNODE\u3002\u7136\u800c\uff0c\u5728\u91cd\u5c3e\u5bf9\u6570\u6b63\u6001\u51b2\u51fb\u4e0b\uff0cNODE\u7684\u7075\u6d3b\u6027\u66f4\u597d\uff0c\u800cUDE\u5728\u9884\u6d4b\u65f6\u4f1a\u6ede\u540e\u3002", "conclusion": "\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0c\u7ed3\u6784\u504f\u5dee\u6709\u52a9\u4e8e\u9884\u6d4b\uff0c\u4f46\u5728\u5176\u4ed6\u60c5\u51b5\u4e0b\uff0c\u5b83\u53ef\u80fd\u4f1a\u53d7\u5230\u635f\u5bb3\u3002\u5f53\u566a\u58f0\u4e3a\u8f7b\u5c3e\u6216\u65f6\u95f4\u76f8\u5173\u65f6\uff0c\u5e94\u5f3a\u5236\u6267\u884c\u7ed3\u6784\uff1b\u5f53\u6781\u7aef\u4e8b\u4ef6\u5360\u4e3b\u5bfc\u5730\u4f4d\u65f6\uff0c\u5e94\u653e\u5bbd\u7ed3\u6784\u3002"}}
{"id": "2509.18132", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18132", "abs": "https://arxiv.org/abs/2509.18132", "authors": ["Xiuyi Fan"], "title": "Position Paper: Integrating Explainability and Uncertainty Estimation in Medical AI", "comment": "Accepted at the International Joint Conference on Neural Networks,\n  IJCNN 2025", "summary": "Uncertainty is a fundamental challenge in medical practice, but current\nmedical AI systems fail to explicitly quantify or communicate uncertainty in a\nway that aligns with clinical reasoning. Existing XAI works focus on\ninterpreting model predictions but do not capture the confidence or reliability\nof these predictions. Conversely, uncertainty estimation (UE) techniques\nprovide confidence measures but lack intuitive explanations. The disconnect\nbetween these two areas limits AI adoption in medicine. To address this gap, we\npropose Explainable Uncertainty Estimation (XUE) that integrates explainability\nwith uncertainty quantification to enhance trust and usability in medical AI.\nWe systematically map medical uncertainty to AI uncertainty concepts and\nidentify key challenges in implementing XUE. We outline technical directions\nfor advancing XUE, including multimodal uncertainty quantification,\nmodel-agnostic visualization techniques, and uncertainty-aware decision support\nsystems. Lastly, we propose guiding principles to ensure effective XUE\nrealisation. Our analysis highlights the need for AI systems that not only\ngenerate reliable predictions but also articulate confidence levels in a\nclinically meaningful way. This work contributes to the development of\ntrustworthy medical AI by bridging explainability and uncertainty, paving the\nway for AI systems that are aligned with real-world clinical complexities.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a XUE \u7684\u65b0\u65b9\u6cd5\uff0c\u5b83\u7ed3\u5408\u4e86\u53ef\u89e3\u91ca\u6027\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff0c\u65e8\u5728\u63d0\u9ad8\u533b\u7597 AI \u7684\u53ef\u4fe1\u5ea6\u548c\u53ef\u7528\u6027\u3002", "motivation": "\u76ee\u524d\u7684\u533b\u7597 AI \u7cfb\u7edf\u5728\u91cf\u5316\u548c\u6c9f\u901a\u4e0d\u786e\u5b9a\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u65e0\u6cd5\u4e0e\u4e34\u5e8a\u63a8\u7406\u4fdd\u6301\u4e00\u81f4\u3002\u73b0\u6709\u7684 XAI \u6280\u672f\u4fa7\u91cd\u4e8e\u89e3\u91ca\u6a21\u578b\u9884\u6d4b\uff0c\u4f46\u672a\u80fd\u6355\u6349\u9884\u6d4b\u7684\u7f6e\u4fe1\u5ea6\u6216\u53ef\u9760\u6027\u3002\u800c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff08UE\uff09\u6280\u672f\u867d\u7136\u63d0\u4f9b\u4e86\u7f6e\u4fe1\u5ea6\u5ea6\u91cf\uff0c\u4f46\u7f3a\u4e4f\u76f4\u89c2\u7684\u89e3\u91ca\u3002\u8fd9\u79cd\u8131\u8282\u9650\u5236\u4e86 AI \u5728\u533b\u5b66\u9886\u57df\u7684\u5e94\u7528\u3002", "method": "\u672c\u7814\u7a76\u7cfb\u7edf\u5730\u5c06\u533b\u5b66\u4e0d\u786e\u5b9a\u6027\u6620\u5c04\u5230 AI \u4e0d\u786e\u5b9a\u6027\u6982\u5ff5\uff0c\u5e76\u786e\u5b9a\u4e86\u5b9e\u73b0 XUE \u7684\u5173\u952e\u6311\u6218\u3002\u7814\u7a76\u6982\u8ff0\u4e86\u63a8\u8fdb XUE \u7684\u6280\u672f\u65b9\u5411\uff0c\u5305\u62ec\u591a\u6a21\u6001\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3001\u6a21\u578b\u65e0\u5173\u7684\u53ef\u89c6\u5316\u6280\u672f\u4ee5\u53ca\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u3002\u6700\u540e\uff0c\u63d0\u51fa\u6307\u5bfc\u539f\u5219\u4ee5\u786e\u4fdd XUE \u7684\u6709\u6548\u5b9e\u73b0\u3002", "result": "\u7814\u7a76\u5206\u6790\u5f3a\u8c03\u4e86\u9700\u8981\u5f00\u53d1\u4e0d\u4ec5\u80fd\u751f\u6210\u53ef\u9760\u9884\u6d4b\uff0c\u8fd8\u80fd\u4ee5\u4e34\u5e8a\u76f8\u5173\u65b9\u5f0f\u9610\u660e\u7f6e\u4fe1\u5ea6\u6c34\u5e73\u7684 AI \u7cfb\u7edf\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u5f25\u5408\u53ef\u89e3\u91ca\u6027\u548c\u4e0d\u786e\u5b9a\u6027\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u5f00\u53d1\u503c\u5f97\u4fe1\u8d56\u7684\u533b\u7597 AI \u505a\u51fa\u4e86\u8d21\u732e\uff0c\u4e3a\u5f00\u53d1\u80fd\u591f\u5e94\u5bf9\u73b0\u5b9e\u4e16\u754c\u4e34\u5e8a\u590d\u6742\u6027\u7684 AI \u7cfb\u7edf\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2509.18156", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18156", "abs": "https://arxiv.org/abs/2509.18156", "authors": ["Haoyu Wang", "Fengze Liu", "Jiayao Zhang", "Dan Roth", "Kyle Richardson"], "title": "Event Causality Identification with Synthetic Control", "comment": null, "summary": "Event causality identification (ECI), a process that extracts causal\nrelations between events from text, is crucial for distinguishing causation\nfrom correlation. Traditional approaches to ECI have primarily utilized\nlinguistic patterns and multi-hop relational inference, risking false causality\nidentification due to informal usage of causality and specious graphical\ninference. In this paper, we adopt the Rubin Causal Model to identify event\ncausality: given two temporally ordered events, we see the first event as the\ntreatment and the second one as the observed outcome. Determining their\ncausality involves manipulating the treatment and estimating the resultant\nchange in the likelihood of the outcome. Given that it is only possible to\nimplement manipulation conceptually in the text domain, as a work-around, we\ntry to find a twin for the protagonist from existing corpora. This twin should\nhave identical life experiences with the protagonist before the treatment but\nundergoes an intervention of treatment. However, the practical difficulty of\nlocating such a match limits its feasibility. Addressing this issue, we use the\nsynthetic control method to generate such a twin' from relevant historical\ndata, leveraging text embedding synthesis and inversion techniques. This\napproach allows us to identify causal relations more robustly than previous\nmethods, including GPT-4, which is demonstrated on a causality benchmark,\nCOPES-hard.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9c81\u5bbe\u56e0\u679c\u6a21\u578b\u548c\u5408\u6210\u63a7\u5236\u6cd5\u6765\u8bc6\u522b\u4e8b\u4ef6\u56e0\u679c\u5173\u7cfb\u7684\u65b0\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u6613\u53d7\u8bed\u8a00\u8bef\u7528\u548c\u56fe\u63a8\u7406\u8c2c\u8bef\u5f71\u54cd\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u4e8b\u4ef6\u56e0\u679c\u5173\u7cfb\u62bd\u53d6\u65b9\u6cd5\u5728\u533a\u5206\u56e0\u679c\u548c\u76f8\u5173\u6027\u65f6\u5b58\u5728\u4e0d\u8db3\uff0c\u5bb9\u6613\u53d7\u5230\u8bed\u8a00\u975e\u6b63\u5f0f\u4f7f\u7528\u548c\u56fe\u63a8\u7406\u8c2c\u8bef\u7684\u5f71\u54cd\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u9c81\u68d2\u7684\u65b9\u6cd5\u3002", "method": "\u672c\u7814\u7a76\u5c06\u9c81\u5bbe\u56e0\u679c\u6a21\u578b\u5e94\u7528\u4e8e\u4e8b\u4ef6\u56e0\u679c\u5173\u7cfb\u8bc6\u522b\uff0c\u5c06\u65f6\u95f4\u4e0a\u5148\u53d1\u751f\u7684\u4e8b\u4ef6\u89c6\u4e3a\u201c\u5904\u7406\u201d\uff0c\u540e\u53d1\u751f\u7684\u4e8b\u4ef6\u89c6\u4e3a\u201c\u7ed3\u679c\u201d\u3002\u4e3a\u4e86\u5728\u6587\u672c\u57df\u6a21\u62df\u5904\u7406\u7684\u5e72\u9884\uff0c\u7814\u7a76\u4eba\u5458\u63d0\u51fa\u5bfb\u627e\u4e0e\u4e3b\u4eba\u516c\u7ecf\u5386\u76f8\u4f3c\u4f46\u63a5\u53d7\u4e86\u5e72\u9884\u7684\u201c\u53cc\u80de\u80ce\u201d\u6848\u4f8b\uff0c\u5e76\u91c7\u7528\u6587\u672c\u5d4c\u5165\u5408\u6210\u4e0e\u53cd\u6f14\u6280\u672f\uff0c\u5229\u7528\u5408\u6210\u63a7\u5236\u6cd5\u6765\u751f\u6210\u8fd9\u79cd\u201c\u53cc\u80de\u80ce\u201d\u3002", "result": "\u5728COPES-hard\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u6bd4\u5305\u62ecGPT-4\u5728\u5185\u7684\u73b0\u6709\u65b9\u6cd5\u66f4\u6709\u6548\u5730\u8bc6\u522b\u4e86\u56e0\u679c\u5173\u7cfb\u3002", "conclusion": "\u57fa\u4e8e\u9c81\u5bbe\u56e0\u679c\u6a21\u578b\u548c\u5408\u6210\u63a7\u5236\u6cd5\u7684\u65b9\u6cd5\u80fd\u591f\u66f4\u53ef\u9760\u5730\u8bc6\u522b\u4e8b\u4ef6\u56e0\u679c\u5173\u7cfb\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2509.18634", "categories": ["cond-mat.mtrl-sci", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2509.18634", "abs": "https://arxiv.org/abs/2509.18634", "authors": ["S. V. Naydenov", "O. K. Kapustnyk", "I. M. Pritula", "D. S. Sofronov", "I. S. Terzin", "N. O. Kovalenko"], "title": "Quaternary crystals CdZnTeSe: Growth via the vertical Bridgman method with different compositions of raw materials", "comment": "12 pages, 7 figures", "summary": "Indium-doped semiconductor crystals CdZnTeSe with several different\ncompositions of raw materials were grown via the vertical Bridgman method under\nhigh-pressure argon. For the first time, these crystals were obtained via a\ncombined method from a mixture of simple and binary starting components. A\ntheoretical analysis of the permissible reactions for obtaining multicomponent\nCdZnTeSe crystals from different compositions of starting materials was\nperformed. The homogeneity of the distribution of the atomic composition and\nelectrical resistance (in the dark and under illumination) of the obtained\ncrystals was studied. Crystals grown via the new combined method presented the\nbest homogeneity of composition and electrophysical properties.", "AI": {"tldr": "Indium-doped CdZnTeSe crystals were grown using a new combined vertical Bridgman method with improved homogeneity and electrophysical properties.", "motivation": "To explore a novel method for growing indium-doped CdZnTeSe crystals with enhanced homogeneity and electrophysical properties.", "method": "Grew indium-doped CdZnTeSe crystals with various compositions using the vertical Bridgman method under high-pressure argon, employing a combined approach with simple and binary starting components. Theoretically analyzed permissible reactions for multicomponent crystal formation. Studied the homogeneity of atomic composition and electrical resistance (dark and illuminated).", "result": "Crystals grown via the new combined method exhibited superior homogeneity in composition and electrophysical properties compared to other methods.", "conclusion": "The new combined vertical Bridgman method is effective for growing indium-doped CdZnTeSe crystals with improved homogeneity and electrophysical properties."}}
{"id": "2509.19021", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2509.19021", "abs": "https://arxiv.org/abs/2509.19021", "authors": ["Atharv Pandey", "Lakshmanan Kuppusamy"], "title": "Optimization of Base-n Radix Sort for Skewed Datasets", "comment": "17 pages including Appendix", "summary": "The importance and applications of sorting is apparent and needs no\nexplanation. In this paper, we analyse a non-comparison sorting algorithm,\nBase-n Radix Sort (BNRS) and introduce an optimized vari- ant of BNRS, namely,\nStable Logical Partition Radix Sort (SLPR). The complexity of these algorithms\nis measured by the input size $n$ and the maximum value $k$. We show that with\nrespect to time complexity, these algorithms are more succinct than traditional\ncomparison-based sorting algorithms for representing the sorted order of\ncertain integer distribu- tions, specifically, when $k <nlog_2^n$ is met. We\nalso show that the SLPR optimization, which uses in-place stable partitioning\nto reduce the active problem size in each pass, resulting in highly effective\nsorting for skewed datasets that contain a majority of small numbers and mix of\nvery large numbers.", "AI": {"tldr": "BNRS\u548cSLPR\u662f\u6bd4\u4f20\u7edf\u6bd4\u8f83\u6392\u5e8f\u66f4\u5feb\u7684\u975e\u6bd4\u8f83\u6392\u5e8f\u7b97\u6cd5\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u5177\u6709\u5927\u91cf\u5c0f\u503c\u548c\u5c11\u91cf\u5927\u503c\u7684\u504f\u659c\u6570\u636e\u96c6\u3002", "motivation": "\u5206\u6790\u4e86\u975e\u6bd4\u8f83\u6392\u5e8f\u7b97\u6cd5Base-n Radix Sort (BNRS)\uff0c\u5e76\u5f15\u5165\u4e86\u5176\u4f18\u5316\u53d8\u4f53Stable Logical Partition Radix Sort (SLPR)\uff0c\u65e8\u5728\u4e3a\u7279\u5b9a\u6574\u6570\u5206\u5e03\u63d0\u4f9b\u6bd4\u4f20\u7edf\u6bd4\u8f83\u6392\u5e8f\u66f4\u5feb\u7684\u6392\u5e8f\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5206\u6790\u4e86Base-n Radix Sort (BNRS)\u7b97\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aStable Logical Partition Radix Sort (SLPR)\u7684\u4f18\u5316\u53d8\u4f53\uff0c\u8be5\u53d8\u4f53\u4f7f\u7528\u539f\u5730\u7a33\u5b9a\u5206\u533a\u6765\u51cf\u5c0f\u6bcf\u6b21\u4f20\u9012\u4e2d\u7684\u6d3b\u52a8\u95ee\u9898\u89c4\u6a21\u3002", "result": "\u8bc1\u660e\u4e86BNRS\u548cSLPR\u7b97\u6cd5\u5728\u65f6\u95f4\u590d\u6742\u5ea6\u4e0a\uff0c\u5f53\u6ee1\u8db3k < nlog(n)\u65f6\uff0c\u5bf9\u4e8e\u67d0\u4e9b\u6574\u6570\u5206\u5e03\u6bd4\u4f20\u7edf\u6bd4\u8f83\u6392\u5e8f\u7b97\u6cd5\u66f4\u6709\u6548\u3002SLPR\u4f18\u5316\u901a\u8fc7\u539f\u5730\u7a33\u5b9a\u5206\u533a\uff0c\u6709\u6548\u5730\u5904\u7406\u4e86\u5305\u542b\u5927\u591a\u6570\u5c0f\u6570\u548c\u6df7\u5408\u5c11\u91cf\u5927\u6570\u7684\u504f\u659c\u6570\u636e\u96c6\u3002", "conclusion": "BNRS\u548cSLPR\u662f\u975e\u6bd4\u8f83\u6392\u5e8f\u7b97\u6cd5\uff0c\u5728k < nlog(n)\u7684\u6761\u4ef6\u4e0b\uff0c\u5bf9\u4e8e\u67d0\u4e9b\u6574\u6570\u5206\u5e03\uff0c\u5176\u65f6\u95f4\u590d\u6742\u5ea6\u4f18\u4e8e\u6bd4\u8f83\u6392\u5e8f\u7b97\u6cd5\u3002SLPR\u7684\u539f\u5730\u7a33\u5b9a\u5206\u533a\u4f18\u5316\u4f7f\u5176\u5728\u5904\u7406\u504f\u659c\u6570\u636e\u96c6\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2509.18831", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.18831", "abs": "https://arxiv.org/abs/2509.18831", "authors": ["Pin-Yen Chiu", "I-Sheng Fang", "Jun-Cheng Chen"], "title": "Text Slider: Efficient and Plug-and-Play Continuous Concept Control for Image/Video Synthesis via LoRA Adapters", "comment": null, "summary": "Recent advances in diffusion models have significantly improved image and\nvideo synthesis. In addition, several concept control methods have been\nproposed to enable fine-grained, continuous, and flexible control over\nfree-form text prompts. However, these methods not only require intensive\ntraining time and GPU memory usage to learn the sliders or embeddings but also\nneed to be retrained for different diffusion backbones, limiting their\nscalability and adaptability. To address these limitations, we introduce Text\nSlider, a lightweight, efficient and plug-and-play framework that identifies\nlow-rank directions within a pre-trained text encoder, enabling continuous\ncontrol of visual concepts while significantly reducing training time, GPU\nmemory consumption, and the number of trainable parameters. Furthermore, Text\nSlider supports multi-concept composition and continuous control, enabling\nfine-grained and flexible manipulation in both image and video synthesis. We\nshow that Text Slider enables smooth and continuous modulation of specific\nattributes while preserving the original spatial layout and structure of the\ninput. Text Slider achieves significantly better efficiency: 5$\\times$ faster\ntraining than Concept Slider and 47$\\times$ faster than Attribute Control,\nwhile reducing GPU memory usage by nearly 2$\\times$ and 4$\\times$,\nrespectively.", "AI": {"tldr": "Text Slider\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u9ad8\u6548\u4e14\u5373\u63d2\u5373\u7528\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u9884\u8bad\u7ec3\u7684\u6587\u672c\u7f16\u7801\u5668\u4e2d\u8bc6\u522b\u4f4e\u79e9\u65b9\u5411\uff0c\u5b9e\u73b0\u5bf9\u89c6\u89c9\u6982\u5ff5\u7684\u8fde\u7eed\u63a7\u5236\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e86\u8bad\u7ec3\u65f6\u95f4\u3001GPU\u5185\u5b58\u6d88\u8017\u548c\u53ef\u8bad\u7ec3\u53c2\u6570\u6570\u91cf\u3002\u5b83\u8fd8\u652f\u6301\u591a\u6982\u5ff5\u7ec4\u5408\u548c\u8fde\u7eed\u63a7\u5236\uff0c\u53ef\u7528\u4e8e\u56fe\u50cf\u548c\u89c6\u9891\u5408\u6210\u3002", "motivation": "\u73b0\u6709\u7684\u6982\u5ff5\u63a7\u5236\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u7684\u8bad\u7ec3\u65f6\u95f4\u548cGPU\u5185\u5b58\uff0c\u5e76\u4e14\u9700\u8981\u4e3a\u4e0d\u540c\u7684\u6269\u6563\u6a21\u578b\u8fdb\u884c\u91cd\u65b0\u8bad\u7ec3\uff0c\u9650\u5236\u4e86\u5176\u53ef\u6269\u5c55\u6027\u548c\u9002\u5e94\u6027\u3002", "method": "Text Slider\u901a\u8fc7\u8bc6\u522b\u9884\u8bad\u7ec3\u6587\u672c\u7f16\u7801\u5668\u4e2d\u7684\u4f4e\u79e9\u65b9\u5411\u6765\u5b9e\u73b0\u5bf9\u89c6\u89c9\u6982\u5ff5\u7684\u8fde\u7eed\u63a7\u5236\u3002", "result": "Text Slider\u6bd4Concept Slider\u5feb5\u500d\uff0c\u6bd4Attribute Control\u5feb47\u500d\uff0c\u540c\u65f6GPU\u5185\u5b58\u4f7f\u7528\u91cf\u5206\u522b\u51cf\u5c11\u4e86\u8fd12\u500d\u548c4\u500d\u3002", "conclusion": "Text Slider\u5b9e\u73b0\u4e86\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u9ad8\u6548\u7684\u8bad\u7ec3\u548c\u66f4\u4f4e\u7684\u5185\u5b58\u5360\u7528\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5bf9\u56fe\u50cf\u548c\u89c6\u9891\u5408\u6210\u7684\u7cbe\u7ec6\u63a7\u5236\u80fd\u529b\u3002"}}
{"id": "2509.18869", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.18869", "abs": "https://arxiv.org/abs/2509.18869", "authors": ["Baiqiang Wang", "Dongfang Zhao", "Nathan R Tallent", "Luanzheng Guo"], "title": "On The Reproducibility Limitations of RAG Systems", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) is increasingly employed in generative\nAI-driven scientific workflows to integrate rapidly evolving scientific\nknowledge bases, yet its reliability is frequently compromised by\nnon-determinism in their retrieval components. This paper introduces ReproRAG,\na comprehensive benchmarking framework designed to systematically measure and\nquantify the reproducibility of vector-based retrieval systems. ReproRAG\ninvestigates sources of uncertainty across the entire pipeline, including\ndifferent embedding models, precision, retrieval algorithms, hardware\nconfigurations, and distributed execution environments. Utilizing a suite of\nmetrics, such as Exact Match Rate, Jaccard Similarity, and Kendall's Tau, the\nproposed framework effectively characterizes the trade-offs between\nreproducibility and performance. Our large-scale empirical study reveals\ncritical insights; for instance, we observe that different embedding models\nhave remarkable impact on RAG reproducibility. The open-sourced ReproRAG\nframework provides researchers and engineers productive tools to validate\ndeployments, benchmark reproducibility, and make informed design decisions,\nthereby fostering more trustworthy AI for science.", "AI": {"tldr": "ReproRAG\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u548c\u91cf\u5316\u57fa\u4e8e\u5411\u91cf\u7684\u68c0\u7d22\u7cfb\u7edf\u53ef\u590d\u73b0\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u5206\u6790\u4e0d\u540c\u7ec4\u4ef6\uff08\u5982\u5d4c\u5165\u6a21\u578b\u3001\u68c0\u7d22\u7b97\u6cd5\u3001\u786c\u4ef6\u914d\u7f6e\u7b49\uff09\u7684\u4e0d\u786e\u5b9a\u6027\u6765\u6e90\uff0c\u5e76\u4f7f\u7528\u7cbe\u786e\u5339\u914d\u7387\u3001\u96c5\u5361\u5c14\u76f8\u4f3c\u5ea6\u3001\u80af\u5fb7\u5c14 \u03c4 \u7b49\u6307\u6807\u6765\u8861\u91cf\u53ef\u590d\u73b0\u6027\u4e0e\u6027\u80fd\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u65e8\u5728\u63d0\u9ad8\u79d1\u5b66\u9886\u57df\u4e2d\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u5728\u79d1\u5b66\u5de5\u4f5c\u6d41\u4e2d\u7528\u4e8e\u6574\u5408\u4e0d\u65ad\u53d1\u5c55\u7684\u79d1\u5b66\u77e5\u8bc6\uff0c\u4f46\u5176\u68c0\u7d22\u7ec4\u4ef6\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\uff08\u975e\u786e\u5b9a\u6027\uff09\u5e38\u5e38\u5f71\u54cd\u5176\u53ef\u9760\u6027\u3002", "method": "ReproRAG\u6846\u67b6\u901a\u8fc7\u5206\u6790\u5d4c\u5165\u6a21\u578b\u3001\u7cbe\u5ea6\u3001\u68c0\u7d22\u7b97\u6cd5\u3001\u786c\u4ef6\u914d\u7f6e\u548c\u5206\u5e03\u5f0f\u6267\u884c\u73af\u5883\u7b49\u6574\u4e2a\u6d41\u7a0b\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u6765\u6e90\uff0c\u5e76\u91c7\u7528\u7cbe\u786e\u5339\u914d\u7387\u3001\u96c5\u5361\u5c14\u76f8\u4f3c\u5ea6\u3001\u80af\u5fb7\u5c14 \u03c4 \u7b49\u4e00\u7cfb\u5217\u6307\u6807\u6765\u91cf\u5316\u68c0\u7d22\u7cfb\u7edf\u7684\u53ef\u590d\u73b0\u6027\u3002", "result": "\u5927\u89c4\u6a21\u5b9e\u8bc1\u7814\u7a76\u8868\u660e\uff0c\u4e0d\u540c\u7684\u5d4c\u5165\u6a21\u578b\u5bf9RAG\u7684\u53ef\u590d\u73b0\u6027\u6709\u663e\u8457\u5f71\u54cd\u3002ReproRAG\u6846\u67b6\u80fd\u591f\u6709\u6548\u5730\u8868\u5f81\u53ef\u590d\u73b0\u6027\u4e0e\u6027\u80fd\u4e4b\u95f4\u7684\u6743\u8861\u3002", "conclusion": "ReproRAG\u6846\u67b6\u4e3a\u7814\u7a76\u4eba\u5458\u548c\u5de5\u7a0b\u5e08\u63d0\u4f9b\u4e86\u9a8c\u8bc1\u90e8\u7f72\u3001\u57fa\u51c6\u6d4b\u8bd5\u53ef\u590d\u73b0\u6027\u4ee5\u53ca\u505a\u51fa\u660e\u667a\u8bbe\u8ba1\u51b3\u7b56\u7684\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u6784\u5efa\u66f4\u503c\u5f97\u4fe1\u8d56\u7684\u79d1\u5b66\u4eba\u5de5\u667a\u80fd\u3002"}}
{"id": "2509.18475", "categories": ["eess.SY", "cs.SY", "math.CT", "math.DS"], "pdf": "https://arxiv.org/pdf/2509.18475", "abs": "https://arxiv.org/abs/2509.18475", "authors": ["Xiaoyan Li", "Evan Patterson", "Patricia L. Mabry", "Nathaniel D. Osgood"], "title": "Compositional System Dynamics: The Higher Mathematics Underlying System Dynamics Diagrams & Practice", "comment": null, "summary": "This work establishes a robust mathematical foundation for compositional\nSystem Dynamics modeling, leveraging category theory to formalize and enhance\nthe representation, analysis, and composition of system models. Here, System\nDynamics diagrams, such as stock & flow diagrams, system structure diagrams,\nand causal loop diagrams, are formulated as categorical constructs, enabling\nscalable, transparent, and systematic reasoning. By encoding these diagrams as\ndata using attributed C-sets and utilizing advanced categorical tools like\nstructured cospans, pushouts, pullbacks, and functor mappings, the framework\nsupports modular composition, stratification, and seamless mapping between\nsyntax and semantics.\n  The approach underwrites traditional practice with firm mathematical\nstructure, facilitates the identification of certain forms of pathways and\nfeedback loops, the detection of simple patterns within complex diagrams,\ncommon structure between diagrams, and structure-preserving mappings between\ndiverse diagram types. Additionally, this framework supports alternative\nsemantics, such as stochastic transition dynamics, extending beyond traditional\nordinary differential equation (ODE) representations. Applications in\ncompositional modeling, modularity, and team-based collaboration demonstrate\nthe practical advantages of this advanced framework.\n  Future directions include integrating dimensional annotations, supporting\nhybrid and agent-based modeling paradigms, and expanding the framework's\napplicability to global and local temporal reasoning through temporal sheaves.\nBy revealing and formalizing the hidden mathematical structure of System\nDynamics diagrams, this work empowers practitioners to tackle complex systems\nwith clarity, scalability, and rigor.", "AI": {"tldr": "\u672c\u6587\u5229\u7528\u8303\u7574\u8bba\u7684\u5f62\u5f0f\u5316\u65b9\u6cd5\uff0c\u4e3a\u7cfb\u7edf\u52a8\u529b\u5b66\uff08SD\uff09\u5efa\u6a21\u5960\u5b9a\u4e86\u575a\u5b9e\u7684\u6570\u5b66\u57fa\u7840\uff0c\u5b9e\u73b0\u4e86\u6a21\u578b\u7684\u53ef\u6269\u5c55\u3001\u900f\u660e\u548c\u7cfb\u7edf\u7684\u63a8\u7406\u3002", "motivation": "\u4e3a\u7cfb\u7edf\u52a8\u529b\u5b66\u5efa\u6a21\u63d0\u4f9b\u4e00\u4e2a\u7a33\u5065\u7684\u6570\u5b66\u57fa\u7840\uff0c\u901a\u8fc7\u8303\u7574\u8bba\u7684\u5f62\u5f0f\u5316\u65b9\u6cd5\u6765\u589e\u5f3a\u6a21\u578b\u8868\u793a\u3001\u5206\u6790\u548c\u7ec4\u5408\u80fd\u529b\u3002", "method": "\u5c06\u7cfb\u7edf\u52a8\u529b\u5b66\u56fe\uff08\u5982\u5b58\u91cf-\u6d41\u91cf\u56fe\u3001\u7cfb\u7edf\u7ed3\u6784\u56fe\u3001\u56e0\u679c\u56de\u8def\u56fe\uff09\u5f62\u5f0f\u5316\u4e3a\u8303\u7574\u6784\u9020\uff0c\u4f7f\u7528\u5e26\u5c5e\u6027\u7684C-sets\u7f16\u7801\u56fe\uff0c\u5e76\u5229\u7528\u7ed3\u6784\u5316\u7684\u5171\u8de8\u3001\u63a8\u51fa\u3001\u62c9\u56de\u548c\u51fd\u5b50\u6620\u5c04\u7b49\u8303\u7574\u5de5\u5177\u6765\u5b9e\u73b0\u6a21\u5757\u5316\u7ec4\u5408\u3001\u5206\u5c42\u548c\u8bed\u6cd5\u4e0e\u8bed\u4e49\u4e4b\u95f4\u7684\u6620\u5c04\u3002", "result": "\u8be5\u65b9\u6cd5\u4e3a\u4f20\u7edf\u5b9e\u8df5\u63d0\u4f9b\u4e86\u575a\u5b9e\u7684\u6570\u5b66\u7ed3\u6784\u652f\u6491\uff0c\u80fd\u591f\u8bc6\u522b\u7279\u5b9a\u8def\u5f84\u548c\u53cd\u9988\u56de\u8def\u3001\u68c0\u6d4b\u590d\u6742\u56fe\u4e2d\u7684\u7b80\u5355\u6a21\u5f0f\u3001\u53d1\u73b0\u56fe\u4e4b\u95f4\u7684\u5171\u540c\u7ed3\u6784\uff0c\u4ee5\u53ca\u5728\u4e0d\u540c\u56fe\u7c7b\u578b\u4e4b\u95f4\u8fdb\u884c\u7ed3\u6784\u4fdd\u6301\u6620\u5c04\u3002\u6b64\u5916\uff0c\u8be5\u6846\u67b6\u8fd8\u652f\u6301\u968f\u673a\u8f6c\u79fb\u52a8\u529b\u5b66\u7b49\u66ff\u4ee3\u8bed\u4e49\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u7684\u5e38\u5fae\u5206\u65b9\u7a0b\uff08ODE\uff09\u8868\u793a\u3002", "conclusion": "\u901a\u8fc7\u63ed\u793a\u548c\u5f62\u5f0f\u5316\u7cfb\u7edf\u52a8\u529b\u5b66\u56fe\u7684\u9690\u85cf\u6570\u5b66\u7ed3\u6784\uff0c\u8be5\u5de5\u4f5c\u4f7f\u4ece\u4e1a\u8005\u80fd\u591f\u4ee5\u6e05\u6670\u3001\u53ef\u6269\u5c55\u548c\u4e25\u8c28\u7684\u65b9\u5f0f\u5904\u7406\u590d\u6742\u7cfb\u7edf\u3002"}}
{"id": "2509.18906", "categories": ["cs.ET", "cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.18906", "abs": "https://arxiv.org/abs/2509.18906", "authors": ["Kyriakos Stylianopoulos", "George C. Alexandropoulos"], "title": "Integrating Stacked Intelligent Metasurfaces and Power Control for Dynamic Edge Inference via Over-The-Air Neural Networks", "comment": "Submitted to IEEE ICASSP 2026", "summary": "This paper introduces a novel framework for Edge Inference (EI) that bypasses\nthe conventional practice of treating the wireless channel as noise. We utilize\nStacked Intelligent Metasurfaces (SIMs) to control wireless propagation,\nenabling the channel itself to perform over-the-air computation. This\neliminates the need for symbol estimation at the receiver, significantly\nreducing computational and communication overhead. Our approach models the\ntransmitter-channel-receiver system as an end-to-end Deep Neural Network (DNN)\nwhere the response of the SIM elements are trainable parameters. To address\nchannel variability, we incorporate a dedicated DNN module responsible for\ndynamically adjusting transmission power leveraging user location information.\nOur performance evaluations showcase that the proposed metasurfaces-integrated\nDNN framework with deep SIM architectures are capable of balancing\nclassification accuracy and power consumption under diverse scenarios, offering\nsignificant energy efficiency improvements.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5806\u53e0\u667a\u80fd\u8d85\u8868\u9762\uff08SIM\uff09\u6765\u63a7\u5236\u65e0\u7ebf\u4fe1\u53f7\u4f20\u64ad\uff0c\u4f7f\u4fe1\u9053\u80fd\u591f\u6267\u884c\u7a7a\u4e2d\u8ba1\u7b97\u7684\u65b0\u578b\u8fb9\u7f18\u63a8\u7406\uff08EI\uff09\u6846\u67b6\uff0c\u4ece\u800c\u65e0\u9700\u63a5\u6536\u5668\u7aef\u7684\u7b26\u53f7\u4f30\u8ba1\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u548c\u901a\u4fe1\u5f00\u9500\u3002", "motivation": "\u4f20\u7edf\u8fb9\u7f18\u63a8\u7406\u5c06\u65e0\u7ebf\u4fe1\u9053\u89c6\u4e3a\u566a\u58f0\uff0c\u800c\u672c\u7814\u7a76\u65e8\u5728\u5229\u7528SIM\u63a7\u5236\u65e0\u7ebf\u4f20\u64ad\uff0c\u4f7f\u4fe1\u9053\u672c\u8eab\u5b9e\u73b0\u7a7a\u4e2d\u8ba1\u7b97\uff0c\u4ece\u800c\u6d88\u9664\u7b26\u53f7\u4f30\u8ba1\u7684\u9700\u8981\uff0c\u964d\u4f4e\u8ba1\u7b97\u548c\u901a\u4fe1\u5f00\u9500\u3002", "method": "\u5c06\u53d1\u5c04\u5668-\u4fe1\u9053-\u63a5\u6536\u5668\u7cfb\u7edf\u5efa\u6a21\u4e3a\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNN\uff09\uff0c\u5176\u4e2dSIM\u5355\u5143\u7684\u54cd\u5e94\u662f\u53ef\u8bad\u7ec3\u53c2\u6570\uff0c\u5e76\u5f15\u5165\u4e00\u4e2a\u4e13\u95e8\u7684DNN\u6a21\u5757\uff0c\u5229\u7528\u7528\u6237\u4f4d\u7f6e\u4fe1\u606f\u52a8\u6001\u8c03\u6574\u4f20\u8f93\u529f\u7387\u4ee5\u5e94\u5bf9\u4fe1\u9053\u53d8\u5316\u3002", "result": "\u63d0\u51fa\u7684\u96c6\u6210\u8d85\u8868\u9762\u7684DNN\u6846\u67b6\uff0c\u91c7\u7528\u6df1\u5ea6SIM\u67b6\u6784\uff0c\u80fd\u591f\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u5e73\u8861\u5206\u7c7b\u51c6\u786e\u6027\u548c\u529f\u8017\uff0c\u5b9e\u73b0\u663e\u8457\u7684\u80fd\u6548\u63d0\u5347\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u901a\u8fc7\u5229\u7528SIM\u63a7\u5236\u65e0\u7ebf\u4f20\u64ad\u5b9e\u73b0\u7a7a\u4e2d\u8ba1\u7b97\uff0c\u5e76\u7ed3\u5408\u52a8\u6001\u529f\u7387\u8c03\u6574\uff0c\u5728\u4fdd\u6301\u5206\u7c7b\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u4e86\u80fd\u6e90\u6548\u7387\u3002"}}
{"id": "2509.18673", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2509.18673", "abs": "https://arxiv.org/abs/2509.18673", "authors": ["Siddharth Barman", "Paritosh Verma"], "title": "Proximately Envy-Free and Efficient Allocation of Mixed Manna", "comment": null, "summary": "The existence of fair and efficient allocations of indivisible items is a\ncentral problem in fair division. For indivisible goods, the existence of\nPareto efficient (PO) and envy free up to one item (EF1) allocations was\nestablished by Caragiannis et al. In a recent breakthrough, Mahara established\nthe existence of PO and EF1 allocations for indivisible chores.\n  However, the existence of PO and EF1 allocations of mixed manna remains an\nintriguing open problem. In this paper, we make significant progress in this\ndirection. We establish the existence of allocations that are PO and\nintrospective envy free up to one item (IEF1) for mixed manna. In an IEF1\nallocation, each agent can eliminate its envy towards all the other agents by\neither adding an item or removing an item from its own bundle. The notion of\nIEF1 coincides with EF1 for indivisible chores, and hence, our existence result\ngeneralizes the aforementioned result of Mahara.", "AI": {"tldr": "\u5728\u6df7\u5408\u4eba`manna`\uff08\u7269\u54c1\u548c`chores`\u7684\u7ec4\u5408\uff09\u7684\u516c\u5e73\u5206\u914d\u95ee\u9898\u4e2d\uff0c\u672c\u7814\u7a76\u8bc1\u660e\u4e86\u5b58\u5728\u6ee1\u8db3", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u6df7\u5408`manna`\uff08\u53ef\u5206\u914d\u7269\u54c1\u548c\u4e0d\u53ef\u5206\u914d`chores`\u7684\u7ec4\u5408\uff09\u7684\u516c\u5e73\u5206\u914d\u95ee\u9898\uff0c\u7279\u522b\u662f\u8bc1\u660e\u5b58\u5728\u540c\u65f6\u6ee1\u8db3", "method": "\u5f15\u5165\u4e86\u201c\u5185\u7701\u5f0f\u201d\u7684\u65e0\u5ac9\u5992\uff08\u7269\u54c1\u6570\u91cf-1\uff09\u5206\u914d\uff08`IEF1`\uff09\u6982\u5ff5\uff0c\u5e76\u8bc1\u660e\u4e86\u5bf9\u4e8e\u6df7\u5408`manna`\uff0c\u5b58\u5728\u6ee1\u8db3\u201c\u5e15\u7d2f\u6258\u6700\u4f18\u201d\uff08`PO`\uff09\u548c`IEF1`\u7684\u5206\u914d\u3002", "result": "\u8bc1\u660e\u4e86\u5b58\u5728\u540c\u65f6\u6ee1\u8db3\u201c\u5e15\u7d2f\u6258\u6700\u4f18\u201d\uff08`PO`\uff09\u548c\u201c\u5185\u7701\u5f0f\u201d\u7684\u65e0\u5ac9\u5992\uff08\u7269\u54c1\u6570\u91cf-1\uff09\u5206\u914d\uff08`IEF1`\uff09\u7684\u6df7\u5408`manna`\u5206\u914d\u3002", "conclusion": "\u672c\u7814\u7a76\u5728\u6df7\u5408`manna`\u7684\u516c\u5e73\u5206\u914d\u95ee\u9898\u4e0a\u53d6\u5f97\u4e86\u91cd\u8981\u8fdb\u5c55\uff0c\u8bc1\u660e\u4e86\u5b58\u5728\u6ee1\u8db3`PO`\u548c`IEF1`\u7684\u5206\u914d\uff0c\u8be5\u7ed3\u679c\u63a8\u5e7f\u4e86\u5148\u524d\u5728\u4e0d\u53ef\u5206\u914d`chores`\u5206\u914d\u95ee\u9898\u4e0a\u7684\u7814\u7a76\u6210\u679c\u3002"}}
{"id": "2509.18539", "categories": ["cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2509.18539", "abs": "https://arxiv.org/abs/2509.18539", "authors": ["Phusit Nualpijit", "Bumned Soodchomshom"], "title": "Strain-Tuned Optical Properties of a Two-Dimensional Hexagonal Lattice: Exploiting Saddle Degrees of Freedom and Saddle Filtering Effects", "comment": "22 pages, 12 Figures", "summary": "The deformation of hexagonal lattices has attracted considerable attention\ndue to its promising applications in straintronics. This study employs the\ntight-binding model to investigate the anisotropic spectrum, where electronic\ntransport can be manipulated by the degree of deformation. The longitudinal\nconductivities, light transmittance, and absorbance are analyzed, revealing\nenhancement along one direction and suppression along the other. The findings\nindicate that the direction and magnitude of strain can be determined by\nmeasuring transmittance and absorbance, showing significant deviations from the\nunstrained condition. Furthermore, a strong absorbance is observed due to the\ninterband transition of electrons near the M-point saddles, linked to van Hove\nsingularities for specific values of nearest and next-nearest hoping energy.\nThe unexpected characteristics of saddle polarization-analogous to valley\npolarization at K- and K'-become particularly prominent when strain affects the\nselection of M-point saddle. Notably, the demonstration indicates that a highly\nefficient M-point saddle filtering effect takes place, induced by linearly\npolarized light. This model paves the way for exploring the optical properties\nof anisotropic hexagonal lattices, such as black phosphorus and borophene\noxide. These results also open a pathway to strain-programmable optoelectronic\ndevices, such as polarization-selective photodetectors, tunable absorbers, and\nultrathin optical filters.", "AI": {"tldr": "\u516d\u65b9\u6676\u683c\u7684\u5f62\u53d8\u53ef\u4ee5\u901a\u8fc7\u5e94\u53d8\u7535\u5b50\u5b66\u5b9e\u73b0\uff0c\u672c\u7814\u7a76\u5229\u7528\u7d27\u675f\u7f1a\u6a21\u578b\u7814\u7a76\u4e86\u5e94\u53d8\u5bf9\u7535\u5b50\u8c31\u3001\u5bfc\u7535\u6027\u3001\u900f\u5149\u7387\u548c\u5438\u5149\u7387\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5e94\u53d8\u53ef\u4ee5\u88ab\u7528\u6765\u64cd\u63a7\u8fd9\u4e9b\u6027\u8d28\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5149\u5b66\u6d4b\u91cf\u7684\u5e94\u53d8\u4f20\u611f\u65b9\u6cd5\u3002\u901a\u8fc7\u5206\u6790\u7535\u5b50\u5e26\u95f4\u8dc3\u8fc1\u548c\u8303\u970d\u592b\u5947\u70b9\uff0c\u63ed\u793a\u4e86\u5f62\u53d8\u8bf1\u5bfc\u7684\u7c7b\u4f3c\u8c37\u6781\u5316\u7684\u978d\u70b9\u6781\u5316\u73b0\u8c61\uff0c\u5e76\u5c55\u793a\u4e86\u7ebf\u504f\u632f\u5149\u8bf1\u5bfc\u7684\u9ad8\u6548\u978d\u70b9\u6ee4\u6ce2\u6548\u5e94\uff0c\u4e3a\u8bbe\u8ba1\u5e94\u53d8\u53ef\u8c03\u7684\u5149\u7535\u5668\u4ef6\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002", "motivation": "\u516d\u65b9\u6676\u683c\u56e0\u5728\u5e94\u53d8\u7535\u5b50\u5b66\u4e2d\u7684\u5e94\u7528\u524d\u666f\u800c\u53d7\u5230\u5173\u6ce8\uff0c\u9700\u8981\u7814\u7a76\u5176\u5f62\u53d8\u5bf9\u5176\u7535\u5b50\u548c\u5149\u5b66\u6027\u8d28\u7684\u5f71\u54cd\u3002", "method": "\u91c7\u7528\u7d27\u675f\u7f1a\u6a21\u578b\u7814\u7a76\u53d8\u5f62\u516d\u65b9\u6676\u683c\u7684\u5404\u5411\u5f02\u6027\u8c31\u3001\u7eb5\u5411\u7535\u5bfc\u7387\u3001\u900f\u5149\u7387\u548c\u5438\u5149\u7387\uff0c\u5e76\u5206\u6790\u4e86\u7535\u5b50\u5e26\u95f4\u8dc3\u8fc1\u548c\u8303\u970d\u592b\u5947\u70b9\u3002", "result": "\u53d1\u73b0\u7535\u5b50\u8f93\u8fd0\u6027\u8d28\u3001\u900f\u5149\u7387\u548c\u5438\u5149\u7387\u968f\u5f62\u53d8\u7a0b\u5ea6\u5448\u5404\u5411\u5f02\u6027\u53d8\u5316\uff0c\u5176\u4e2d\u4e00\u65b9\u5411\u589e\u5f3a\uff0c\u53e6\u4e00\u65b9\u5411\u88ab\u6291\u5236\uff1b\u63d0\u51fa\u4e86\u901a\u8fc7\u6d4b\u91cf\u900f\u5149\u7387\u548c\u5438\u5149\u7387\u6765\u786e\u5b9a\u5e94\u53d8\u65b9\u5411\u548c\u5927\u5c0f\u7684\u65b9\u6cd5\uff1b\u89c2\u5bdf\u5230\u8fd1M\u70b9\u978d\u70b9\u7535\u5b50\u8dc3\u8fc1\u5f15\u8d77\u7684\u5f3a\u5438\u5149\u73b0\u8c61\uff0c\u5e76\u4e0e\u8303\u970d\u592b\u5947\u70b9\u76f8\u5173\uff1b\u53d1\u73b0\u4e86\u5f62\u53d8\u8bf1\u5bfc\u7684\u978d\u70b9\u6781\u5316\u73b0\u8c61\uff0c\u7c7b\u4f3c\u4e8eK\u70b9\u548cK'\u70b9\u7684\u8c37\u6781\u5316\uff1b\u5c55\u793a\u4e86\u7ebf\u504f\u632f\u5149\u8bf1\u5bfc\u7684\u9ad8\u6548\u978d\u70b9\u6ee4\u6ce2\u6548\u5e94\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u53d8\u5f62\u516d\u65b9\u6676\u683c\u7684\u5149\u5b66\u6027\u8d28\u53ef\u4ee5\u901a\u8fc7\u5e94\u53d8\u8fdb\u884c\u8c03\u63a7\uff0c\u4e3a\u8bbe\u8ba1\u5e94\u53d8\u53ef\u7f16\u7a0b\u7684\u5149\u7535\u5668\u4ef6\uff08\u5982\u504f\u632f\u9009\u62e9\u6027\u5149\u7535\u63a2\u6d4b\u5668\u3001\u53ef\u8c03\u8c10\u5438\u6536\u5668\u548c\u8d85\u8584\u5149\u5b66\u6ee4\u6ce2\u5668\uff09\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2509.18352", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2509.18352", "abs": "https://arxiv.org/abs/2509.18352", "authors": ["Evelyna Wang", "Marco-Tulio F. Rodrigues", "Baris Key"], "title": "Quantifying the reactivity of isolated LixSi domains in Si anodes using operando NMR", "comment": null, "summary": "The use of Si anodes can greatly improve the energy density of Li-ion\nbatteries. However, understanding and mitigation of calendar aging remains a\nbarrier to commercialization. In this short report, we utilize operando Nuclear\nMagnetic Resonance (NMR) spectroscopy to detect and quantify lithium silicides\n(LixSi) as they form and react within Si anodes in pouch cells during calendar\naging. We provide direct experimental evidence of complex aging phenomena in\nthe Si anodes, including both SEI growth and dissolution during storage.\nFormation of electrochemically isolated LixSi is also observed, as indicated by\nthe partial persistence of highly lithiated phases after the cell is\ndischarged. Remarkably, we show that these isolated domains can themselves\nself-discharge over time, suggesting that their detection can be challenging in\npost-mortem studies. Finally, we show that aging outcomes depend heavily on the\ntype of silicon particles contained within the electrode, and that certain\nsurface coatings can help decrease the reactivity between lithium silicides and\nthe electrolyte.", "AI": {"tldr": "Si\u8d1f\u6781\u7684\u65e5\u5386\u8001\u5316\u662f\u5546\u4e1a\u5316\u7684\u969c\u788d\uff0c\u672c\u7814\u7a76\u5229\u7528\u539f\u4f4d\u6838\u78c1\u5171\u632f\u6ce2\u8c31\u6280\u672f\u7814\u7a76\u4e86Li\u79bb\u5b50\u7535\u6c60\u7684Si\u8d1f\u6781\u5728\u65e5\u5386\u8001\u5316\u8fc7\u7a0b\u4e2d\u7684\u884c\u4e3a\uff0c\u53d1\u73b0\u4e86\u590d\u6742\u7684 the SEI\u5c42\u751f\u957f\u4e0e\u6eb6\u89e3\uff0c\u4ee5\u53ca\u9502\u7684\u7845\u5316\u7269\uff08LixSi\uff09\u7684\u5f62\u6210\u548c\u53cd\u5e94\uff0c\u5176\u4e2d\u4e00\u4e9bLixSi\u4f1a\u5f62\u6210\u7535\u5316\u5b66\u9694\u79bb\u5e76\u81ea\u653e\u7535\uff0c\u4e14\u8001\u5316\u7ed3\u679c\u4e0e\u7845\u9897\u7c92\u7c7b\u578b\u548c\u8868\u9762\u6d82\u5c42\u6709\u5173\u3002", "motivation": "Si\u8d1f\u6781\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u9502\u79bb\u5b50\u7535\u6c60\u7684\u80fd\u91cf\u5bc6\u5ea6\uff0c\u4f46\u65e5\u5386\u8001\u5316\u662f\u5176\u5546\u4e1a\u5316\u7684\u4e3b\u8981\u969c\u788d\u3002", "method": "\u5229\u7528\u539f\u4f4d\u6838\u78c1\u5171\u632f\u6ce2\u8c31\uff08NMR\uff09\u6280\u672f\uff0c\u5728\u65e5\u5386\u8001\u5316\u8fc7\u7a0b\u4e2d\u68c0\u6d4b\u548c\u91cf\u5316Si\u8d1f\u6781\u4e2d\u9502\u7684\u7845\u5316\u7269\uff08LixSi\uff09\u7684\u5f62\u6210\u548c\u53cd\u5e94\u3002", "result": "\u5b9e\u9a8c\u76f4\u63a5\u8bc1\u660e\u4e86Si\u8d1f\u6781\u4e2d\u5b58\u5728\u590d\u6742\u7684\u65e5\u5386\u8001\u5316\u73b0\u8c61\uff0c\u5305\u62ecSEI\u5c42\u7684\u751f\u957f\u4e0e\u6eb6\u89e3\uff0c\u4ee5\u53ca\u7535\u5316\u5b66\u9694\u79bb\u7684LixSi\u7684\u5f62\u6210\uff08\u5373\u4f7f\u5728\u653e\u7535\u540e\u4ecd\u6709\u9ad8\u5ea6\u9502\u5316\u7684\u76f8\u5b58\u5728\uff09\u3002\u7814\u7a76\u8fd8\u53d1\u73b0\uff0c\u8fd9\u4e9b\u9694\u79bb\u7684LixSi\u4f1a\u968f\u7740\u65f6\u95f4\u81ea\u653e\u7535\u3002\u8001\u5316\u7ed3\u679c\u4e0e\u7845\u9897\u7c92\u7684\u7c7b\u578b\u5bc6\u5207\u76f8\u5173\uff0c\u67d0\u4e9b\u8868\u9762\u6d82\u5c42\u53ef\u4ee5\u964d\u4f4e\u9502\u7684\u7845\u5316\u7269\u4e0e\u7535\u89e3\u6db2\u7684\u53cd\u5e94\u6027\u3002", "conclusion": "Si\u8d1f\u6781\u7684\u65e5\u5386\u8001\u5316\u662f\u4e00\u4e2a\u590d\u6742\u7684\u8fc7\u7a0b\uff0c\u6d89\u53caSEI\u5c42\u7684\u52a8\u6001\u53d8\u5316\u548c\u9502\u7845\u5316\u7269\u7684\u5f62\u6210\u4e0e\u81ea\u653e\u7535\u3002\u901a\u8fc7\u63a7\u5236\u7845\u9897\u7c92\u7684\u7c7b\u578b\u548c\u4f7f\u7528\u8868\u9762\u6d82\u5c42\uff0c\u53ef\u4ee5\u6539\u5584Si\u8d1f\u6781\u7684\u65e5\u5386\u8001\u5316\u6027\u80fd\u3002"}}
{"id": "2509.18985", "categories": ["cs.SI"], "pdf": "https://arxiv.org/pdf/2509.18985", "abs": "https://arxiv.org/abs/2509.18985", "authors": ["Elisa Composta", "Nicolo' Fontana", "Francesco Corso", "Francesco Pierri"], "title": "Simulating Online Social Media Conversations on Controversial Topics Using AI Agents Calibrated on Real-World Data", "comment": null, "summary": "Online social networks offer a valuable lens to analyze both individual and\ncollective phenomena. Researchers often use simulators to explore controlled\nscenarios, and the integration of Large Language Models (LLMs) makes these\nsimulations more realistic by enabling agents to understand and generate\nnatural language content. In this work, we investigate the behavior of\nLLM-based agents in a simulated microblogging social network. We initialize\nagents with realistic profiles calibrated on real-world online conversations\nfrom the 2022 Italian political election and extend an existing simulator by\nintroducing mechanisms for opinion modeling. We examine how LLM agents simulate\nonline conversations, interact with others, and evolve their opinions under\ndifferent scenarios. Our results show that LLM agents generate coherent\ncontent, form connections, and build a realistic social network structure.\nHowever, their generated content displays less heterogeneity in tone and\ntoxicity compared to real data. We also find that LLM-based opinion dynamics\nevolve over time in ways similar to traditional mathematical models. Varying\nparameter configurations produces no significant changes, indicating that\nsimulations require more careful cognitive modeling at initialization to\nreplicate human behavior more faithfully. Overall, we demonstrate the potential\nof LLMs for simulating user behavior in social environments, while also\nidentifying key challenges in capturing heterogeneity and complex dynamics.", "AI": {"tldr": "LLM \u4ee3\u7406\u5728\u6a21\u62df\u793e\u4ea4\u7f51\u7edc\u4e2d\u8868\u73b0\u51fa\u8fde\u8d2f\u6027\u4f46\u7f3a\u4e4f\u591a\u6837\u6027\u3002", "motivation": "\u5229\u7528 LLM \u6a21\u62df\u7528\u6237\u5728\u793e\u4ea4\u7f51\u7edc\u4e2d\u7684\u884c\u4e3a\uff0c\u4ee5\u63a2\u7d22\u53d7\u63a7\u573a\u666f\u5e76\u63d0\u9ad8\u6a21\u62df\u7684\u771f\u5b9e\u6027\u3002", "method": "\u5728\u6a21\u62df\u7684\u5fae\u535a\u793e\u4ea4\u7f51\u7edc\u4e2d\uff0c\u4f7f\u7528\u6839\u636e\u771f\u5b9e\u653f\u6cbb\u5bf9\u8bdd\u6821\u51c6\u7684 LLM \u4ee3\u7406\uff0c\u5e76\u5f15\u5165\u4e86\u610f\u89c1\u6a21\u578b\u3002", "result": "LLM \u4ee3\u7406\u751f\u6210\u8fde\u8d2f\u7684\u5185\u5bb9\u5e76\u5f62\u6210\u73b0\u5b9e\u7684\u793e\u4ea4\u7f51\u7edc\u7ed3\u6784\uff0c\u4f46\u5728\u8bed\u6c14\u548c\u6bd2\u6027\u65b9\u9762\u4e0d\u5982\u771f\u5b9e\u6570\u636e\u591a\u6837\u3002LLM \u9a71\u52a8\u7684\u610f\u89c1\u52a8\u6001\u4e0e\u4f20\u7edf\u6a21\u578b\u76f8\u4f3c\uff0c\u4f46\u53c2\u6570\u53d8\u5316\u5bf9\u7ed3\u679c\u5f71\u54cd\u4e0d\u5927\uff0c\u8868\u660e\u9700\u8981\u66f4\u4ed4\u7ec6\u7684\u8ba4\u77e5\u5efa\u6a21\u3002", "conclusion": "LLM \u5728\u6a21\u62df\u793e\u4ea4\u7528\u6237\u884c\u4e3a\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u8981\u5728\u6355\u6349\u591a\u6837\u6027\u548c\u590d\u6742\u52a8\u6001\u65b9\u9762\u8fdb\u884c\u6539\u8fdb\u3002"}}
{"id": "2509.18555", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.18555", "abs": "https://arxiv.org/abs/2509.18555", "authors": ["Ping Wang", "Zulin Wang", "Yuanfang Ma", "Xiaosi Tian", "Yuanhan Ni"], "title": "A Secure Affine Frequency Division Multiplexing for Wireless Communication Systems", "comment": "6 pages, 5 figures, 2025 IEEE International Conference on\n  Communications", "summary": "This paper introduces a secure affine frequency division multiplexing\n(SE-AFDM) for wireless communication systems to enhance communication security.\nBesides configuring the parameter c1 to obtain communication reliability under\ndoubly selective channels, we also utilize the time-varying parameter c2 to\nimprove the security of the communications system. The derived input-output\nrelation shows that the legitimate receiver can eliminate the nonlinear impact\nintroduced by the time-varying c2 without losing the bit error rate (BER)\nperformance. Moreover, it is theoretically proved that the eavesdropper cannot\nseparate the time-varying c2 and random information symbols, such that the BER\nperformance of the eavesdropper is severely deteriorated. Meanwhile, the\nanalysis of the effective signal-to-interference-plus-noise ratio (SINR) of the\neavesdropper illustrates that the SINR decreases as the value range of c2\nexpands. Numerical results verify that the proposed SE-AFDM waveform has\nsignificant security while maintaining good BER performance in high-mobility\nscenarios.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5b89\u5168\u4eff\u5c04\u9891\u5206\u590d\u7528\uff08SE-AFDM\uff09\u6280\u672f\uff0c\u901a\u8fc7\u5f15\u5165\u65f6\u53d8\u53c2\u6570 c2 \u6765\u63d0\u9ad8\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\u7684\u5b89\u5168\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u826f\u597d\u7684\u6bd4\u7279\u8bef\u7801\u7387\uff08BER\uff09\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u79fb\u52a8\u6027\u573a\u666f\u4e0b\u3002", "motivation": "\u4e3a\u4e86\u589e\u5f3a\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\u7684\u901a\u4fe1\u5b89\u5168\u6027\uff0c\u5e76\u89e3\u51b3\u5728\u53cc\u9009\u62e9\u6027\u4fe1\u9053\u4e0b\u901a\u4fe1\u53ef\u9760\u6027\u548c\u5b89\u5168\u6027\u4e4b\u95f4\u7684\u5e73\u8861\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5b89\u5168\u4eff\u5c04\u9891\u5206\u590d\u7528\uff08SE-AFDM\uff09\u6280\u672f\uff0c\u5229\u7528\u65f6\u53d8\u53c2\u6570 c2 \u6765\u63d0\u9ad8\u7cfb\u7edf\u5b89\u5168\u6027\u3002\u63a8\u5bfc\u4e86\u8f93\u5165\u8f93\u51fa\u5173\u7cfb\uff0c\u8bc1\u660e\u4e86\u5408\u6cd5\u63a5\u6536\u8005\u53ef\u4ee5\u6d88\u9664 c2 \u5f15\u5165\u7684\u975e\u7ebf\u6027\u5f71\u54cd\u800c\u4e0d\u635f\u5931 BER \u6027\u80fd\u3002\u7406\u8bba\u8bc1\u660e\u4e86\u7a83\u542c\u8005\u65e0\u6cd5\u5206\u79bb c2 \u548c\u968f\u673a\u4fe1\u606f\u7b26\u53f7\uff0c\u5bfc\u81f4\u5176 BER \u6027\u80fd\u4e25\u91cd\u4e0b\u964d\u3002\u5206\u6790\u4e86\u7a83\u542c\u8005\u7684\u6709\u6548\u4fe1\u566a\u6bd4\uff08SINR\uff09\uff0c\u8868\u660e SINR \u968f c2 \u503c\u57df\u7684\u6269\u5c55\u800c\u51cf\u5c0f\u3002", "result": "SE-AFDM \u80fd\u591f\u5728\u53cc\u9009\u62e9\u6027\u4fe1\u9053\u4e0b\u5b9e\u73b0\u901a\u4fe1\u53ef\u9760\u6027\u3002\u65f6\u53d8\u53c2\u6570 c2 \u80fd\u591f\u63d0\u9ad8\u901a\u4fe1\u7cfb\u7edf\u7684\u5b89\u5168\u6027\uff0c\u56e0\u4e3a\u7a83\u542c\u8005\u65e0\u6cd5\u5206\u79bb c2 \u548c\u968f\u673a\u4fe1\u606f\u7b26\u53f7\u3002\u7a83\u542c\u8005\u7684\u6709\u6548 SINR \u968f\u7740 c2 \u503c\u57df\u7684\u6269\u5c55\u800c\u51cf\u5c0f\u3002\u6570\u503c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u7684 SE-AFDM \u6ce2\u5f62\u5728\u9ad8\u79fb\u52a8\u6027\u573a\u666f\u4e0b\u5177\u6709\u663e\u8457\u7684\u5b89\u5168\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u826f\u597d\u7684 BER \u6027\u80fd\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684 SE-AFDM \u6280\u672f\u901a\u8fc7\u5f15\u5165\u53ef\u8c03\u7684\u65f6\u53d8\u53c2\u6570 c2\uff0c\u6210\u529f\u5730\u5728\u63d0\u9ad8\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\u5b89\u5168\u6027\u7684\u540c\u65f6\uff0c\u4fdd\u6301\u4e86\u4e0e\u4f20\u7edf\u7cfb\u7edf\u76f8\u5f53\u7684 BER \u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u9ad8\u79fb\u52a8\u6027\u5e94\u7528\u4e2d\uff0c\u4e3a\u5b89\u5168\u901a\u4fe1\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u3002"}}
{"id": "2509.18330", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18330", "abs": "https://arxiv.org/abs/2509.18330", "authors": ["Marsette Vona"], "title": "The Landform Contextual Mesh: Automatically Fusing Surface and Orbital Terrain for Mars 2020", "comment": null, "summary": "The Landform contextual mesh fuses 2D and 3D data from up to thousands of\nMars 2020 rover images, along with orbital elevation and color maps from Mars\nReconnaissance Orbiter, into an interactive 3D terrain visualization.\nContextual meshes are built automatically for each rover location during\nmission ground data system processing, and are made available to mission\nscientists for tactical and strategic planning in the Advanced Science\nTargeting Tool for Robotic Operations (ASTTRO). A subset of them are also\ndeployed to the \"Explore with Perseverance\" public access website.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aLandform contextual mesh\u7684\u4e09\u7ef4\u5730\u5f62\u53ef\u89c6\u5316\u6280\u672f\uff0c\u8be5\u6280\u672f\u878d\u5408\u4e86\u706b\u661f\u63a2\u6d4b\u5668\u56fe\u50cf\u548c\u8f68\u9053\u9065\u611f\u6570\u636e\uff0c\u53ef\u7528\u4e8e\u706b\u661f\u4efb\u52a1\u7684\u89c4\u5212\u548c\u516c\u4f17\u5c55\u793a\u3002", "motivation": "\u4e3a\u706b\u661f\u4efb\u52a1\u7684\u79d1\u5b66\u89c4\u5212\u548c\u516c\u4f17\u6559\u80b2\u63d0\u4f9b\u4ea4\u4e92\u5f0f\u4e09\u7ef4\u5730\u5f62\u53ef\u89c6\u5316\u5de5\u5177\u3002", "method": "\u901a\u8fc7\u878d\u5408\u706b\u661f\u63a2\u6d4b\u5668\u56fe\u50cf\uff082D\uff09\u548c\u8f68\u9053\u9ad8\u7a0b/\u989c\u8272\u56fe\uff083D\uff09\u6570\u636e\uff0c\u81ea\u52a8\u6784\u5efa\u4e0a\u4e0b\u6587\u7f51\u683c\uff0c\u5e76\u7528\u4e8e\u4ea4\u4e92\u5f0f\u4e09\u7ef4\u5730\u5f62\u53ef\u89c6\u5316\u3002", "result": "\u521b\u5efa\u4e86\u7528\u4e8e\u706b\u661f\u4efb\u52a1\u89c4\u5212\u7684\u4e0a\u4e0b\u6587\u7f51\u683c\uff0c\u5e76\u90e8\u5206\u90e8\u7f72\u5230\u516c\u4f17\u8bbf\u95ee\u7f51\u7ad9\u3002", "conclusion": "Landform contextual mesh\u6280\u672f\u4e3a\u706b\u661f\u63a2\u6d4b\u4efb\u52a1\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u53ef\u89c6\u5316\u652f\u6301\uff0c\u6709\u52a9\u4e8e\u79d1\u5b66\u89c4\u5212\u548c\u516c\u4f17\u53c2\u4e0e\u3002"}}
{"id": "2509.18170", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18170", "abs": "https://arxiv.org/abs/2509.18170", "authors": ["Zhanting Zhou", "Jinbo Wang", "Zeqin Wu", "Fengli Zhang"], "title": "MAGIA: Sensing Per-Image Signals from Single-Round Averaged Gradients for Label-Inference-Free Gradient Inversion", "comment": null, "summary": "We study gradient inversion in the challenging single round averaged gradient\nSAG regime where per sample cues are entangled within a single batch mean\ngradient. We introduce MAGIA a momentum based adaptive correction on gradient\ninversion attack a novel label inference free framework that senses latent per\nimage signals by probing random data subsets. MAGIA objective integrates two\ncore innovations 1 a closed form combinatorial rescaling that creates a\nprovably tighter optimization bound and 2 a momentum based mixing of whole\nbatch and subset losses to ensure reconstruction robustness. Extensive\nexperiments demonstrate that MAGIA significantly outperforms advanced methods\nachieving high fidelity multi image reconstruction in large batch scenarios\nwhere prior works fail. This is all accomplished with a computational footprint\ncomparable to standard solvers and without requiring any auxiliary information.", "AI": {"tldr": "MAGIA\u662f\u4e00\u79cd\u65b0\u9896\u7684\u3001\u65e0\u9700\u6807\u7b7e\u63a8\u7406\u7684\u57fa\u4e8e\u52a8\u91cf\u7684\u81ea\u9002\u5e94\u68af\u5ea6\u53cd\u6f14\u653b\u51fb\u65b9\u6cd5\uff0c\u5728\u5355\u8f6e\u5e73\u5747\u68af\u5ea6SAG\u6a21\u578b\u4e0b\uff0c\u80fd\u591f\u5b9e\u73b0\u9ad8\u4fdd\u771f\u591a\u56fe\u50cf\u91cd\u5efa\uff0c\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u7814\u7a76\u5355\u8f6e\u5e73\u5747\u68af\u5ea6SAG\u6a21\u578b\u4e0b\u7684\u68af\u5ea6\u53cd\u6f14\u95ee\u9898\uff0c\u89e3\u51b3\u5355\u4e2a\u6837\u672c\u7ebf\u7d22\u7ea0\u7f20\u5728\u5355\u4e00\u6279\u91cf\u5e73\u5747\u68af\u5ea6\u4e2d\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faMAGIA\u6846\u67b6\uff0c\u5229\u7528\u57fa\u4e8e\u52a8\u91cf\u7684\u81ea\u9002\u5e94\u6821\u6b63\u6765\u53cd\u6f14\u68af\u5ea6\u3002MAGIA\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u521b\u65b0\uff1a1\uff09\u95ed\u5f0f\u7ec4\u5408\u91cd\u7f29\u653e\uff0c\u4ee5\u83b7\u5f97\u66f4\u4f18\u7684\u4f18\u5316\u754c\u9650\uff1b2\uff09\u57fa\u4e8e\u52a8\u91cf\u7684\u6279\u91cf\u635f\u5931\u548c\u5b50\u96c6\u635f\u5931\u6df7\u5408\uff0c\u4ee5\u786e\u4fdd\u91cd\u5efa\u7684\u9c81\u68d2\u6027\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u63a2\u6d4b\u968f\u673a\u6570\u636e\u5b50\u96c6\u6765\u611f\u77e5\u6f5c\u5728\u7684\u6bcf\u5f20\u56fe\u50cf\u4fe1\u53f7\uff0c\u4e14\u65e0\u9700\u6807\u7b7e\u63a8\u7406\u3002", "result": "\u5728\u5927\u578b\u6279\u91cf\u573a\u666f\u4e0b\uff0cMAGIA\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u5148\u8fdb\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u591a\u56fe\u50cf\u91cd\u5efa\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u5728\u6b64\u573a\u666f\u4e0b\u5931\u6548\u3002MAGIA\u7684\u8ba1\u7b97\u5f00\u9500\u4e0e\u6807\u51c6\u6c42\u89e3\u5668\u76f8\u5f53\uff0c\u5e76\u4e14\u4e0d\u9700\u8981\u4efb\u4f55\u8f85\u52a9\u4fe1\u606f\u3002", "conclusion": "MAGIA\u5728\u5355\u8f6e\u5e73\u5747\u68af\u5ea6SAG\u6a21\u578b\u4e0b\uff0c\u901a\u8fc7\u5176\u521b\u65b0\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u5730\u8fdb\u884c\u68af\u5ea6\u53cd\u6f14\uff0c\u5b9e\u73b0\u9ad8\u4fdd\u771f\u591a\u56fe\u50cf\u91cd\u5efa\uff0c\u4e14\u8ba1\u7b97\u6548\u7387\u9ad8\uff0c\u65e0\u9700\u989d\u5916\u4fe1\u606f\u3002"}}
{"id": "2509.18259", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2509.18259", "abs": "https://arxiv.org/abs/2509.18259", "authors": ["Bibek Pokharel", "Haining Pan", "Kemal Aziz", "Luke C. G. Govia", "Sriram Ganeshan", "Thomas Iadecola", "Justin H. Wilson", "Barbara A. Jones", "Abhinav Deshpande", "Jedediah H. Pixley", "Maika Takita"], "title": "Order from chaos with adaptive circuits on quantum hardware", "comment": "21 pages, 11 figures", "summary": "Programmable quantum devices provide a platform to control the coherent\ndynamics of quantum wavefunctions. Here we experimentally realize adaptive\nmonitored quantum circuits, which incorporate conditional feedback into\nnon-unitary evolution, to control quantum chaotic dynamics using a combination\nof local mid-circuit measurements and resets. The experiments are performed\nwith an IBM superconducting quantum processor using up to 100 qubits that\nsamples a quantum version of the classically chaotic Bernoulli map. This map\nscrambles quantum information, while local measurements and feedback attempt to\nsteer the dynamics toward a state that is a fixed point of the map. This\ncompetition drives a dynamical phase transition between quantum and classical\ndynamics that we observe experimentally and describe theoretically using noisy\nsimulations, matrix product states, and mappings to statistical mechanics\nmodels. Estimates of the universal critical properties are obtained to high\naccuracy on the quantum computer thanks to the large number of qubits utilized\nin the calculation. By successfully applying up to nearly 5000 entangling gates\nand 5000 non-unitary mid-circuit operations on systems up to 100 qubits, this\nexperiment serves as a signpost on the route towards fault tolerance.", "AI": {"tldr": "\u901a\u8fc7\u7ed3\u5408\u4e2d\u9014\u6d4b\u91cf\u548c\u590d\u4f4d\u64cd\u4f5c\uff0c\u5728IBM\u8d85\u5bfc\u91cf\u5b50\u5904\u7406\u5668\u4e0a\u5b9e\u73b0\u4e86\u81ea\u9002\u5e94\u76d1\u6d4b\u7684\u91cf\u5b50\u7535\u8def\uff0c\u7528\u4e8e\u63a7\u5236\u91cf\u5b50\u6df7\u6c8c\u52a8\u529b\u5b66\uff0c\u5e76\u89c2\u5bdf\u5230\u4e86\u91cf\u5b50-\u7ecf\u5178\u52a8\u529b\u5b66\u4e4b\u95f4\u7684\u76f8\u53d8\u3002", "motivation": "\u5728\u53ef\u7f16\u7a0b\u91cf\u5b50\u8bbe\u5907\u4e0a\u5b9e\u73b0\u81ea\u9002\u5e94\u76d1\u6d4b\u7684\u91cf\u5b50\u7535\u8def\uff0c\u4ee5\u63a7\u5236\u91cf\u5b50\u6df7\u6c8c\u52a8\u529b\u5b66\u3002", "method": "\u5728IBM\u8d85\u5bfc\u91cf\u5b50\u5904\u7406\u5668\u4e0a\uff0c\u4f7f\u7528\u591a\u8fbe100\u4e2a\u91cf\u5b50\u6bd4\u7279\uff0c\u901a\u8fc7\u5c40\u90e8\u4e2d\u9014\u6d4b\u91cf\u548c\u590d\u4f4d\u64cd\u4f5c\uff0c\u7ed3\u5408\u6761\u4ef6\u53cd\u9988\uff0c\u5b9e\u73b0\u81ea\u9002\u5e94\u76d1\u6d4b\u91cf\u5b50\u7535\u8def\uff0c\u4ee5\u63a7\u5236\u91cf\u5b50\u6df7\u6c8c\u52a8\u529b\u5b66\uff0c\u8be5\u52a8\u529b\u5b66\u6a21\u62df\u4e86\u7ecf\u5178\u4e0a\u6df7\u6c8c\u7684\u4f2f\u52aa\u5229\u6620\u5c04\u3002", "result": "\u89c2\u5bdf\u5230\u4e86\u91cf\u5b50-\u7ecf\u5178\u52a8\u529b\u5b66\u4e4b\u95f4\u7684\u76f8\u53d8\uff0c\u5e76\u5229\u7528\u566a\u58f0\u6a21\u62df\u3001\u77e9\u9635\u4e58\u79ef\u6001\u548c\u7edf\u8ba1\u529b\u5b66\u6a21\u578b\u8fdb\u884c\u4e86\u7406\u8bba\u63cf\u8ff0\u3002\u5229\u7528\u91cf\u5b50\u8ba1\u7b97\u673a\u4e0a\u5927\u89c4\u6a21\u7684\u91cf\u5b50\u6bd4\u7279\u6570\u91cf\uff0c\u9ad8\u7cbe\u5ea6\u5730\u4f30\u8ba1\u4e86\u666e\u9002\u4e34\u754c\u6027\u8d28\u3002", "conclusion": "\u901a\u8fc7\u6210\u529f\u5730\u5728\u591a\u8fbe100\u4e2a\u91cf\u5b50\u6bd4\u7279\u7684\u7cfb\u7edf\u4e0a\u5e94\u7528\u8fd15000\u4e2a\u7ea0\u7f20\u95e8\u548c5000\u4e2a\u975e\u9149\u4e2d\u9014\u64cd\u4f5c\uff0c\u8be5\u5b9e\u9a8c\u4e3a\u5b9e\u73b0\u5bb9\u9519\u91cf\u5b50\u8ba1\u7b97\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2509.18106", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18106", "abs": "https://arxiv.org/abs/2509.18106", "authors": ["Elisa Tomassini", "Enrique Garc\u00eda-Mac\u00edas", "Filippo Ubertini"], "title": "Model-Based Transfer Learning for Real-Time Damage Assessment of Bridge Networks", "comment": null, "summary": "The growing use of permanent monitoring systems has increased data\navailability, offering new opportunities for structural assessment but also\nposing scalability challenges, especially across large bridge networks.\nManaging multiple structures requires tracking and comparing long-term\nbehaviour efficiently. To address this, knowledge transfer between similar\nstructures becomes essential. This study proposes a model-based transfer\nlearning approach using neural network surrogate models, enabling a model\ntrained on one bridge to be adapted to another with similar characteristics.\nThese models capture shared damage mechanisms, supporting a scalable and\ngeneralizable monitoring framework. The method was validated using real data\nfrom two bridges. The transferred model was integrated into a Bayesian\ninference framework for continuous damage assessment based on modal features\nfrom monitoring data. Results showed high sensitivity to damage location,\nseverity, and extent. This approach enhances real-time monitoring and enables\ncross-structure knowledge transfer, promoting smart monitoring strategies and\nimproved resilience at the network level.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u8fc1\u79fb\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u795e\u7ecf\u7f51\u7edc\u4ee3\u7406\u6a21\u578b\uff0c\u5c06\u4e00\u4e2a\u6865\u6881\u7684\u76d1\u6d4b\u6a21\u578b\u8fc1\u79fb\u5230\u53e6\u4e00\u4e2a\u76f8\u4f3c\u7684\u6865\u6881\u4e0a\uff0c\u4ee5\u5e94\u5bf9\u5927\u89c4\u6a21\u6865\u6881\u7f51\u7edc\u7ed3\u6784\u76d1\u6d4b\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u6311\u6218\u3002", "motivation": "\u968f\u7740\u6c38\u4e45\u76d1\u6d4b\u7cfb\u7edf\u5728\u5927\u578b\u6865\u6881\u7f51\u7edc\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u6570\u636e\u53ef\u83b7\u5f97\u6027\u589e\u52a0\uff0c\u4f46\u540c\u65f6\u4e5f\u5e26\u6765\u4e86\u53ef\u6269\u5c55\u6027\u6311\u6218\uff0c\u9700\u8981\u9ad8\u6548\u5730\u8ddf\u8e2a\u548c\u6bd4\u8f83\u957f\u671f\u884c\u4e3a\uff0c\u56e0\u6b64\uff0c\u5728\u76f8\u4f3c\u7ed3\u6784\u4e4b\u95f4\u8fdb\u884c\u77e5\u8bc6\u8f6c\u79fb\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u8fc1\u79fb\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u795e\u7ecf\u7f51\u7edc\u4ee3\u7406\u6a21\u578b\uff0c\u5c06\u4e00\u4e2a\u6865\u6881\u7684\u76d1\u6d4b\u6a21\u578b\u9002\u5e94\u5230\u53e6\u4e00\u4e2a\u5177\u6709\u76f8\u4f3c\u7279\u5f81\u7684\u6865\u6881\u4e0a\uff0c\u8fd9\u4e9b\u6a21\u578b\u80fd\u591f\u6355\u6349\u5171\u4eab\u7684\u635f\u4f24\u673a\u5236\u3002", "result": "\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\u548c\u8d1d\u53f6\u65af\u63a8\u65ad\uff0c\u5b9e\u73b0\u4e86\u5bf9\u7ed3\u6784\u635f\u4f24\u7684\u4f4d\u7f6e\u3001\u4e25\u91cd\u7a0b\u5ea6\u548c\u8303\u56f4\u7684\u9ad8\u5ea6\u654f\u611f\u6027\uff0c\u5e76\u96c6\u6210\u4e86\u7528\u4e8e\u57fa\u4e8e\u76d1\u6d4b\u6570\u636e\u7684\u6a21\u6001\u7279\u5f81\u7684\u8fde\u7eed\u635f\u4f24\u8bc4\u4f30\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u8de8\u7ed3\u6784\u77e5\u8bc6\u8f6c\u79fb\uff0c\u589e\u5f3a\u4e86\u5b9e\u65f6\u76d1\u6d4b\u80fd\u529b\uff0c\u652f\u6301\u4e86\u667a\u80fd\u76d1\u6d4b\u7b56\u7565\u548c\u7f51\u7edc\u5c42\u9762\u7684\u5f39\u6027\u63d0\u5347\u3002"}}
{"id": "2509.18793", "categories": ["cs.RO", "cs.MA", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.18793", "abs": "https://arxiv.org/abs/2509.18793", "authors": ["Lukas Zanger", "Bastian Lampe", "Lennart Reiher", "Lutz Eckstein"], "title": "Application Management in C-ITS: Orchestrating Demand-Driven Deployments and Reconfigurations", "comment": "7 pages, 2 figures, 2 tables; Accepted to be published as part of the\n  2025 IEEE International Conference on Intelligent Transportation Systems\n  (ITSC 2025), Gold Coast, Australia, November 18-21, 2025", "summary": "Vehicles are becoming increasingly automated and interconnected, enabling the\nformation of cooperative intelligent transport systems (C-ITS) and the use of\noffboard services. As a result, cloud-native techniques, such as microservices\nand container orchestration, play an increasingly important role in their\noperation. However, orchestrating applications in a large-scale C-ITS poses\nunique challenges due to the dynamic nature of the environment and the need for\nefficient resource utilization. In this paper, we present a demand-driven\napplication management approach that leverages cloud-native techniques -\nspecifically Kubernetes - to address these challenges. Taking into account the\ndemands originating from different entities within the C-ITS, the approach\nenables the automation of processes, such as deployment, reconfiguration,\nupdate, upgrade, and scaling of microservices. Executing these processes on\ndemand can, for example, reduce computing resource consumption and network\ntraffic. A demand may include a request for provisioning an external supporting\nservice, such as a collective environment model. The approach handles changing\nand new demands by dynamically reconciling them through our proposed\napplication management framework built on Kubernetes and the Robot Operating\nSystem (ROS 2). We demonstrate the operation of our framework in the C-ITS use\ncase of collective environment perception and make the source code of the\nprototypical framework publicly available at\nhttps://github.com/ika-rwth-aachen/application_manager .", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eKubernetes\u7684\u6309\u9700\u5e94\u7528\u7a0b\u5e8f\u7ba1\u7406\u65b9\u6cd5\uff0c\u4ee5\u5e94\u5bf9\u5927\u89c4\u6a21\u534f\u4f5c\u5f0f\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\uff08C-ITS\uff09\u4e2d\u52a8\u6001\u73af\u5883\u548c\u8d44\u6e90\u5229\u7528\u7387\u7684\u6311\u6218\u3002", "motivation": "\u968f\u7740\u8f66\u8f86\u81ea\u52a8\u5316\u548c\u4e92\u8054\u5316\u7a0b\u5ea6\u7684\u63d0\u9ad8\uff0cC-ITS\u548c\u79bb\u7ebf\u670d\u52a1\u7684\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u4e91\u539f\u751f\u6280\u672f\u5728\u5176\u4e2d\u626e\u6f14\u7740\u8d8a\u6765\u8d8a\u91cd\u8981\u7684\u89d2\u8272\u3002\u7136\u800c\uff0c\u5927\u89c4\u6a21C-ITS\u4e2d\u7684\u5e94\u7528\u7a0b\u5e8f\u7f16\u6392\u9762\u4e34\u52a8\u6001\u73af\u5883\u548c\u8d44\u6e90\u5229\u7528\u6548\u7387\u7684\u72ec\u7279\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6309\u9700\u5e94\u7528\u7a0b\u5e8f\u7ba1\u7406\u65b9\u6cd5\uff0c\u5229\u7528Kubernetes\u6765\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\u3002\u8be5\u65b9\u6cd5\u8003\u8651\u4e86C-ITS\u5185\u4e0d\u540c\u5b9e\u4f53\u7684\u9700\u6c42\uff0c\u5b9e\u73b0\u4e86\u5fae\u670d\u52a1\u90e8\u7f72\u3001\u91cd\u65b0\u914d\u7f6e\u3001\u66f4\u65b0\u3001\u5347\u7ea7\u548c\u6269\u5c55\u7b49\u6d41\u7a0b\u7684\u81ea\u52a8\u5316\u3002\u901a\u8fc7\u6309\u9700\u6267\u884c\u8fd9\u4e9b\u6d41\u7a0b\uff0c\u53ef\u4ee5\u51cf\u5c11\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u548c\u7f51\u7edc\u6d41\u91cf\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u6240\u63d0\u51fa\u7684\u57fa\u4e8eKubernetes\u548cROS 2\u7684\u5e94\u7528\u7a0b\u5e8f\u7ba1\u7406\u6846\u67b6\uff0c\u52a8\u6001\u5730\u534f\u8c03\u4e0d\u65ad\u53d8\u5316\u7684\u9700\u6c42\u3002", "result": "\u8be5\u6846\u67b6\u5728C-ITS\u96c6\u4f53\u73af\u5883\u611f\u77e5\u7528\u4f8b\u4e2d\u8fdb\u884c\u4e86\u6f14\u793a\uff0c\u5e76\u516c\u5f00\u4e86\u539f\u578b\u6846\u67b6\u7684\u6e90\u4ee3\u7801\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6309\u9700\u5e94\u7528\u7a0b\u5e8f\u7ba1\u7406\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5730\u5e94\u5bf9\u5927\u89c4\u6a21C-ITS\u4e2d\u7684\u6311\u6218\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u548c\u52a8\u6001\u534f\u8c03\u6ee1\u8db3\u4e0d\u65ad\u53d8\u5316\u7684\u9700\u6c42\uff0c\u4ece\u800c\u4f18\u5316\u8d44\u6e90\u5229\u7528\u7387\u3002"}}
{"id": "2509.18168", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18168", "abs": "https://arxiv.org/abs/2509.18168", "authors": ["Dong Liu", "Yanxuan Yu"], "title": "HSGM: Hierarchical Segment-Graph Memory for Scalable Long-Text Semantics", "comment": null, "summary": "Semantic parsing of long documents remains challenging due to quadratic\ngrowth in pairwise composition and memory requirements. We introduce\n\\textbf{Hierarchical Segment-Graph Memory (HSGM)}, a novel framework that\ndecomposes an input of length $N$ into $M$ meaningful segments, constructs\n\\emph{Local Semantic Graphs} on each segment, and extracts compact\n\\emph{summary nodes} to form a \\emph{Global Graph Memory}. HSGM supports\n\\emph{incremental updates} -- only newly arrived segments incur local graph\nconstruction and summary-node integration -- while \\emph{Hierarchical Query\nProcessing} locates relevant segments via top-$K$ retrieval over summary nodes\nand then performs fine-grained reasoning within their local graphs.\n  Theoretically, HSGM reduces worst-case complexity from $O(N^2)$ to\n$O\\!\\left(N\\,k + (N/k)^2\\right)$, with segment size $k \\ll N$, and we derive\nFrobenius-norm bounds on the approximation error introduced by node\nsummarization and sparsification thresholds. Empirically, on three benchmarks\n-- long-document AMR parsing, segment-level semantic role labeling (OntoNotes),\nand legal event extraction -- HSGM achieves \\emph{2--4$\\times$ inference\nspeedup}, \\emph{$>60\\%$ reduction} in peak memory, and \\emph{$\\ge 95\\%$} of\nbaseline accuracy. Our approach unlocks scalable, accurate semantic modeling\nfor ultra-long texts, enabling real-time and resource-constrained NLP\napplications.", "AI": {"tldr": "HSGM\u662f\u4e00\u4e2a\u65b0\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u957f\u6587\u6863\u5206\u89e3\u4e3a\u591a\u4e2a\u6bb5\u843d\uff0c\u5e76\u5728\u6bcf\u4e2a\u6bb5\u843d\u4e0a\u6784\u5efa\u5c40\u90e8\u8bed\u4e49\u56fe\uff0c\u518d\u63d0\u53d6\u6458\u8981\u8282\u70b9\u5f62\u6210\u5168\u5c40\u56fe\u8bb0\u5fc6\uff0c\u6765\u89e3\u51b3\u957f\u6587\u6863\u8bed\u4e49\u89e3\u6790\u7684\u6311\u6218\u3002\u5b83\u652f\u6301\u589e\u91cf\u66f4\u65b0\u548c\u5206\u5c42\u67e5\u8be2\u5904\u7406\uff0c\u5c06\u590d\u6742\u5ea6\u4eceO(N^2)\u964d\u4f4e\u5230O(N*k + (N/k)^2)\uff0c\u5e76\u5b9e\u73b0\u4e862-4\u500d\u7684\u63a8\u7406\u52a0\u901f\u548c\u8d85\u8fc760%\u7684\u5185\u5b58\u51cf\u5c11\uff0c\u540c\u65f6\u4fdd\u6301\u4e8695%\u4ee5\u4e0a\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u957f\u6587\u6863\u7684\u8bed\u4e49\u89e3\u6790\u5b58\u5728\u4e8c\u6b21\u589e\u957f\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u9700\u6c42\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u5206\u5c42\u6bb5\u56fe\u8bb0\u5fc6\uff08HSGM\uff09\u7684\u65b0\u6846\u67b6\u3002\u8be5\u6846\u67b6\u5c06\u8f93\u5165\u6587\u6863\u5206\u89e3\u4e3aM\u4e2a\u6709\u610f\u4e49\u7684\u6bb5\u843d\uff0c\u5728\u6bcf\u4e2a\u6bb5\u843d\u4e0a\u6784\u5efa\u5c40\u90e8\u8bed\u4e49\u56fe\uff0c\u5e76\u63d0\u53d6\u7d27\u51d1\u7684\u6458\u8981\u8282\u70b9\u4ee5\u5f62\u6210\u5168\u5c40\u56fe\u8bb0\u5fc6\u3002HSGM\u652f\u6301\u589e\u91cf\u66f4\u65b0\u548c\u5206\u5c42\u67e5\u8be2\u5904\u7406\u3002", "result": "\u5728AMR\u89e3\u6790\u3001\u8bed\u4e49\u89d2\u8272\u6807\u6ce8\u548c\u6cd5\u5f8b\u4e8b\u4ef6\u63d0\u53d6\u7b49\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cHSGM\u5b9e\u73b0\u4e862-4\u500d\u7684\u63a8\u7406\u52a0\u901f\uff0c\u5cf0\u503c\u5185\u5b58\u51cf\u5c11\u8d85\u8fc760%\uff0c\u51c6\u786e\u7387\u8fbe\u5230\u57fa\u7ebf\u51c6\u786e\u7387\u768495%\u4ee5\u4e0a\u3002", "conclusion": "HSGM\u80fd\u591f\u5bf9\u8d85\u957f\u6587\u672c\u8fdb\u884c\u53ef\u6269\u5c55\u3001\u51c6\u786e\u7684\u8bed\u4e49\u5efa\u6a21\uff0c\u80fd\u591f\u6ee1\u8db3\u5b9e\u65f6\u548c\u8d44\u6e90\u53d7\u9650\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5e94\u7528\u7684\u9700\u6c42\u3002"}}
{"id": "2509.18158", "categories": ["cs.CL", "cs.LG", "I.2.7"], "pdf": "https://arxiv.org/pdf/2509.18158", "abs": "https://arxiv.org/abs/2509.18158", "authors": ["Seungyoun Yi", "Minsoo Khang", "Sungrae Park"], "title": "ZERA: Zero-init Instruction Evolving Refinement Agent - From Zero Instructions to Structured Prompts via Principle-based Optimization", "comment": "9 pages, 4 figures. To appear in EMNLP 2025 Main Conference (Oral\n  Presentation)", "summary": "Automatic Prompt Optimization (APO) improves large language model (LLM)\nperformance by refining prompts for specific tasks. However, prior APO methods\ntypically focus only on user prompts, rely on unstructured feedback, and\nrequire large sample sizes and long iteration cycles-making them costly and\nbrittle. We propose ZERA (Zero-init Instruction Evolving Refinement Agent), a\nnovel framework that jointly optimizes both system and user prompts through\nprincipled, low-overhead refinement. ZERA scores prompts using eight\ngeneralizable criteria with automatically inferred weights, and revises prompts\nbased on these structured critiques. This enables fast convergence to\nhigh-quality prompts using minimal examples and short iteration cycles. We\nevaluate ZERA across five LLMs and nine diverse datasets spanning reasoning,\nsummarization, and code generation tasks. Experimental results demonstrate\nconsistent improvements over strong baselines. Further ablation studies\nhighlight the contribution of each component to more effective prompt\nconstruction. Our implementation including all prompts is publicly available at\nhttps://github.com/younatics/zera-agent.", "AI": {"tldr": "ZERA\u662f\u4e00\u4e2a\u521b\u65b0\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u7cfb\u7edf\u548c\u7528\u6237\u63d0\u793a\u6765\u6539\u8fdb\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u7279\u5b9a\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u514b\u670d\u4e86\u73b0\u6709\u81ea\u52a8\u63d0\u793a\u4f18\u5316\uff08APO\uff09\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709APO\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u7528\u6237\u63d0\u793a\uff0c\u4f9d\u8d56\u975e\u7ed3\u6784\u5316\u53cd\u9988\uff0c\u4e14\u9700\u8981\u5927\u91cf\u6837\u672c\u548c\u957f\u65f6\u95f4\u8fed\u4ee3\uff0c\u5bfc\u81f4\u6210\u672c\u9ad8\u6602\u4e14\u4e0d\u591f\u7a33\u5b9a\u3002ZERA\u65e8\u5728\u901a\u8fc7\u4f4e\u5f00\u9500\u7684\u7cbe\u70bc\u65b9\u6cd5\uff0c\u8054\u5408\u4f18\u5316\u7cfb\u7edf\u548c\u7528\u6237\u63d0\u793a\uff0c\u4ee5\u66f4\u9ad8\u6548\u3001\u66f4\u7a33\u5b9a\u7684\u65b9\u5f0f\u63d0\u5347LLM\u6027\u80fd\u3002", "method": "ZERA\u4f7f\u7528\u516b\u4e2a\u53ef\u6cdb\u5316\u7684\u6807\u51c6\u6765\u8bc4\u4f30\u63d0\u793a\uff0c\u5e76\u81ea\u52a8\u63a8\u65ad\u6743\u91cd\uff0c\u7136\u540e\u6839\u636e\u8fd9\u4e9b\u7ed3\u6784\u5316\u7684\u8bc4\u4f30\u6765\u4fee\u6539\u63d0\u793a\u3002\u8fd9\u79cd\u65b9\u6cd5\u80fd\u591f\u5feb\u901f\u6536\u655b\u5230\u9ad8\u8d28\u91cf\u7684\u63d0\u793a\uff0c\u4ec5\u9700\u5c11\u91cf\u6837\u672c\u548c\u77ed\u8fed\u4ee3\u5468\u671f\u3002", "result": "\u5728\u4e94\u4e2aLLM\u548c\u4e5d\u4e2a\u6db5\u76d6\u63a8\u7406\u3001\u6458\u8981\u548c\u4ee3\u7801\u751f\u6210\u7684\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u8bc4\u4f30\u663e\u793a\uff0cZERA\u5728\u5404\u9879\u4efb\u52a1\u4e0a\u5747\u80fd\u5b9e\u73b0\u76f8\u6bd4\u5f3a\u6709\u529b\u57fa\u7ebf\u7684\u6301\u7eed\u6539\u8fdb\u3002\u6d88\u878d\u7814\u7a76\u4e5f\u8fdb\u4e00\u6b65\u8bc1\u660e\u4e86ZERA\u5404\u7ec4\u6210\u90e8\u5206\u5728\u6709\u6548\u6784\u5efa\u63d0\u793a\u65b9\u9762\u7684\u8d21\u732e\u3002", "conclusion": "ZERA\u901a\u8fc7\u8054\u5408\u4f18\u5316\u7cfb\u7edf\u548c\u7528\u6237\u63d0\u793a\uff0c\u5e76\u91c7\u7528\u57fa\u4e8e\u7ed3\u6784\u5316\u8bc4\u4f30\u7684\u4f4e\u5f00\u9500\u7cbe\u70bc\u65b9\u6cd5\uff0c\u80fd\u591f\u9ad8\u6548\u3001\u7a33\u5b9a\u5730\u63d0\u5347LLM\u5728\u591a\u79cd\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5177\u6709\u660e\u663e\u4f18\u52bf\u3002"}}
{"id": "2509.19232", "categories": ["cond-mat.mtrl-sci", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2509.19232", "abs": "https://arxiv.org/abs/2509.19232", "authors": ["Tyler D. Dole\u017eal", "Rodrigo Freitas", "Ju Li"], "title": "Atomistic mechanisms of oxidation and chlorine corrosion in Ni-based superalloys: The role of boron and light interstitial segregation", "comment": null, "summary": "Hybrid Monte Carlo and molecular dynamics simulations were used to\ninvestigate the interaction of light interstitials in multi-element Ni-based\nalloys. We show that light interstitials such as boron and oxygen fundamentally\nalter interfacial chemistry by reshaping alloy-element distribution and\nsegregation. Oxygen adsorption drove boron migration from the grain boundary to\nthe free surface, where it co-enriched with Cr, Fe, and Mo and formed BO3\ntrigonal motifs embedded within mixed-metal oxide networks. Oxygen also\npromoted M-O-M chain formation, including Nb2O5 clusters at the free surface.\nIn the absence of oxygen, boron segregated to the grain boundary, altering\nlocal metal chemistry and underscoring a dynamic, environment-sensitive\nbehavior. Following chlorine exposure, the oxidized surfaces retained strong\nO-mediated connectivity while forming new Cl-M associations, particularly with\nNb and Cr, and exhibited further surface enrichment in Cr, Fe, and Mo.\nHigh-temperature MD simulations revealed a dynamic tug-of-war: chlorine exerted\nupward pull and disrupted weakly anchored sites, while Nb- and BO3-rich oxide\nmotifs resisted deformation. A new stabilization mechanism was identified in\nwhich subsurface boron atoms anchored overlying Cr centers, suppressing their\nmobility and mitigating chlorine-driven displacement. These results demonstrate\nboron's dual role as a modifier of alloy-element segregation and a stabilizer\nof oxide networks, and identify Nb as a key element in reinforcing cohesion\nunder halogen attack. More broadly, this study highlights the need to track\nlight interstitial cross-talk and solute migration under reactive conditions,\noffering atomistic criteria for designing corrosion-resistant surface\nchemistries in Ni-based superalloys exposed to halogenated or oxidative\nenvironments.", "AI": {"tldr": "\u5149\u95f4\u9699\u7269\uff08\u5982\u787c\u548c\u6c27\uff09\u901a\u8fc7\u91cd\u5851\u5408\u91d1\u5143\u7d20\u5206\u5e03\u548c\u504f\u6790\uff0c\u4ece\u6839\u672c\u4e0a\u6539\u53d8\u4e86\u591a\u7ec4\u5143\u954d\u57fa\u5408\u91d1\u7684\u754c\u9762\u5316\u5b66\u3002\u6c27\u5438\u9644\u4fc3\u4f7f\u787c\u4ece\u6676\u754c\u8fc1\u79fb\u5230\u81ea\u7531\u8868\u9762\uff0c\u5e76\u4e0eCr\u3001Fe\u3001Mo\u5171\u5bcc\u96c6\u5f62\u6210BO3\u4e09\u65b9\u7ed3\u6784\u3002\u5728\u65e0\u6c27\u6761\u4ef6\u4e0b\uff0c\u787c\u504f\u6790\u81f3\u6676\u754c\u3002\u6c2f\u66b4\u9732\u540e\uff0c\u6c27\u5316\u8868\u9762\u5f62\u6210\u4e86\u65b0\u7684Cl-M\u7ed3\u5408\u3002\u9ad8\u6e29MD\u6a21\u62df\u63ed\u793a\u4e86\u6c2f\u548cNb\u53caBO3\u7ed3\u6784\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u3002\u53d1\u73b0\u4e86\u4e00\u79cd\u65b0\u7684\u7a33\u5b9a\u673a\u5236\uff0c\u5176\u4e2d\u4e9a\u8868\u9762\u787c\u539f\u5b50\u951a\u5b9a\u4e0a\u5c42Cr\u4e2d\u5fc3\uff0c\u6291\u5236\u5176\u8fc1\u79fb\uff0c\u51cf\u8f7b\u6c2f\u9a71\u52a8\u7684\u4f4d\u79fb\u3002\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u53cd\u5e94\u6761\u4ef6\u4e0b\u8ffd\u8e2a\u5149\u95f4\u9699\u7269\u4e32\u6270\u548c\u6eb6\u8d28\u8fc1\u79fb\u7684\u5fc5\u8981\u6027\uff0c\u4e3a\u8bbe\u8ba1\u8010\u8150\u8680\u8868\u9762\u5316\u5b66\u63d0\u4f9b\u4e86\u539f\u5b50\u6807\u51c6\u3002", "motivation": "\u7814\u7a76\u5149\u95f4\u9699\u7269\uff08\u5982\u787c\u548c\u6c27\uff09\u5728\u591a\u7ec4\u5143\u954d\u57fa\u5408\u91d1\u4e2d\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u53ca\u5176\u5bf9\u754c\u9762\u5316\u5b66\u3001\u5143\u7d20\u5206\u5e03\u548c\u504f\u6790\u7684\u5f71\u54cd\uff0c\u5e76\u63ed\u793a\u5176\u5728\u4e0d\u540c\u73af\u5883\uff08\u5982\u6c27\u5316\u548c\u5364\u7d20\u66b4\u9732\uff09\u4e0b\u7684\u52a8\u6001\u884c\u4e3a\u548c\u7a33\u5b9a\u673a\u5236\u3002", "method": "\u91c7\u7528\u6df7\u5408\u8499\u7279\u5361\u6d1b\uff08HMC\uff09\u548c\u5206\u5b50\u52a8\u529b\u5b66\uff08MD\uff09\u6a21\u62df\u65b9\u6cd5\uff0c\u7ed3\u5408\u9ad8\u6e29MD\u6a21\u62df\u3002", "result": "\u6c27\u5438\u9644\u9a71\u52a8\u787c\u4ece\u6676\u754c\u8fc1\u79fb\u5230\u81ea\u7531\u8868\u9762\uff0c\u5e76\u4e0eCr\u3001Fe\u3001Mo\u5171\u5bcc\u96c6\u5f62\u6210BO3\u4e09\u65b9\u7ed3\u6784\uff0c\u540c\u65f6\u4fc3\u8fdbM-O-M\u94fe\u7684\u5f62\u6210\u3002\u65e0\u6c27\u6761\u4ef6\u4e0b\uff0c\u787c\u504f\u6790\u81f3\u6676\u754c\u3002\u6c2f\u66b4\u9732\u540e\uff0c\u6c27\u5316\u8868\u9762\u5f62\u6210Cl-M\u7ed3\u5408\uff0cCr\u3001Fe\u3001Mo\u8fdb\u4e00\u6b65\u5bcc\u96c6\u3002\u9ad8\u6e29MD\u6a21\u62df\u63ed\u793a\u4e86\u6c2f\u7684\u5411\u4e0a\u62c9\u529b\u548cNb/BO3\u7ed3\u6784\u7684\u6297\u53d8\u5f62\u80fd\u529b\u4e4b\u95f4\u7684\u62c9\u952f\u6218\uff0c\u5e76\u53d1\u73b0\u4e9a\u8868\u9762\u787c\u539f\u5b50\u951a\u5b9aCr\u4e2d\u5fc3\u7684\u65b0\u7a33\u5b9a\u673a\u5236\u3002", "conclusion": "\u787c\u5728\u5408\u91d1\u5143\u7d20\u504f\u6790\u548c\u6c27\u5316\u7269\u7f51\u7edc\u7a33\u5b9a\u4e2d\u8d77\u53cc\u91cd\u4f5c\u7528\uff0c\u94cc\u662f\u589e\u5f3a\u5364\u7d20\u4fb5\u8680\u4e0b\u5185\u805a\u529b\u7684\u5173\u952e\u5143\u7d20\u3002\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u53cd\u5e94\u6761\u4ef6\u4e0b\u8ffd\u8e2a\u5149\u95f4\u9699\u7269\u4e32\u6270\u548c\u6eb6\u8d28\u8fc1\u79fb\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u8bbe\u8ba1\u954d\u57fa\u9ad8\u6e29\u5408\u91d1\u7684\u8010\u8150\u8680\u8868\u9762\u5316\u5b66\u63d0\u4f9b\u4e86\u539f\u5b50\u6807\u51c6\u3002"}}
{"id": "2509.19242", "categories": ["cs.DS", "cs.LG", "math.ST", "stat.ML", "stat.TH"], "pdf": "https://arxiv.org/pdf/2509.19242", "abs": "https://arxiv.org/abs/2509.19242", "authors": ["Ilias Diakonikolas", "Jelena Diakonikolas", "Daniel M. Kane", "Jasper C. H. Lee", "Thanasis Pittas"], "title": "Linear Regression under Missing or Corrupted Coordinates", "comment": null, "summary": "We study multivariate linear regression under Gaussian covariates in two\nsettings, where data may be erased or corrupted by an adversary under a\ncoordinate-wise budget. In the incomplete data setting, an adversary may\ninspect the dataset and delete entries in up to an $\\eta$-fraction of samples\nper coordinate; a strong form of the Missing Not At Random model. In the\ncorrupted data setting, the adversary instead replaces values arbitrarily, and\nthe corruption locations are unknown to the learner. Despite substantial work\non missing data, linear regression under such adversarial missingness remains\npoorly understood, even information-theoretically. Unlike the clean setting,\nwhere estimation error vanishes with more samples, here the optimal error\nremains a positive function of the problem parameters. Our main contribution is\nto characterize this error up to constant factors across essentially the entire\nparameter range. Specifically, we establish novel information-theoretic lower\nbounds on the achievable error that match the error of (computationally\nefficient) algorithms. A key implication is that, perhaps surprisingly, the\noptimal error in the missing data setting matches that in the corruption\nsetting-so knowing the corruption locations offers no general advantage.", "AI": {"tldr": "\u7814\u7a76\u5728\u6709\u5220\u5931\u6216\u88ab\u5bf9\u624b\u7834\u574f\u7684\u591a\u5143\u7ebf\u6027\u56de\u5f52\u6a21\u578b\uff0c\u5e76\u63a8\u5bfc\u4e86\u4fe1\u606f\u8bba\u4e0b\u754c\uff0c\u4e0e\u7b97\u6cd5\u7684\u8bef\u5dee\u76f8\u5339\u914d\uff0c\u5e76\u4e14\u5728\u5220\u5931\u548c\u7834\u574f\u8bbe\u7f6e\u4e0b\u6700\u4f18\u8bef\u5dee\u662f\u76f8\u540c\u7684\u3002", "motivation": "\u7814\u7a76\u5728\u5220\u5931\u6216\u88ab\u5bf9\u624b\u7834\u574f\u7684\u591a\u5143\u7ebf\u6027\u56de\u5f52\u6a21\u578b\uff0c\u5e76\u63a8\u5bfc\u4e86\u4fe1\u606f\u8bba\u4e0b\u754c\uff0c\u4e0e\u7b97\u6cd5\u7684\u8bef\u5dee\u76f8\u5339\u914d\uff0c\u5e76\u4e14\u5728\u5220\u5931\u548c\u7834\u574f\u8bbe\u7f6e\u4e0b\u6700\u4f18\u8bef\u5dee\u662f\u76f8\u540c\u7684\u3002", "method": "\u63a8\u5bfc\u4e86\u4fe1\u606f\u8bba\u4e0b\u754c\uff0c\u5e76\u4e0e\u7b97\u6cd5\u7684\u8bef\u5dee\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "result": "\u5728\u5220\u5931\u548c\u7834\u574f\u8bbe\u7f6e\u4e0b\uff0c\u6700\u4f18\u8bef\u5dee\u662f\u76f8\u540c\u7684\u3002", "conclusion": "\u5728\u5220\u5931\u548c\u7834\u574f\u8bbe\u7f6e\u4e0b\uff0c\u6700\u4f18\u8bef\u5dee\u662f\u76f8\u540c\u7684\u3002"}}
{"id": "2509.18948", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18948", "abs": "https://arxiv.org/abs/2509.18948", "authors": ["Jun Ma", "Qian He", "Gaofeng He", "Huang Chen", "Chen Liu", "Xiaogang Jin", "Huamin Wang"], "title": "One-shot Embroidery Customization via Contrastive LoRA Modulation", "comment": "Accepted to ACM Transactions on Graphics (TOG), SIGGRAPH Asia 2025", "summary": "Diffusion models have significantly advanced image manipulation techniques,\nand their ability to generate photorealistic images is beginning to transform\nretail workflows, particularly in presale visualization. Beyond artistic style\ntransfer, the capability to perform fine-grained visual feature transfer is\nbecoming increasingly important. Embroidery is a textile art form characterized\nby intricate interplay of diverse stitch patterns and material properties,\nwhich poses unique challenges for existing style transfer methods. To explore\nthe customization for such fine-grained features, we propose a novel\ncontrastive learning framework that disentangles fine-grained style and content\nfeatures with a single reference image, building on the classic concept of\nimage analogy. We first construct an image pair to define the target style, and\nthen adopt a similarity metric based on the decoupled representations of\npretrained diffusion models for style-content separation. Subsequently, we\npropose a two-stage contrastive LoRA modulation technique to capture\nfine-grained style features. In the first stage, we iteratively update the\nwhole LoRA and the selected style blocks to initially separate style from\ncontent. In the second stage, we design a contrastive learning strategy to\nfurther decouple style and content through self-knowledge distillation.\nFinally, we build an inference pipeline to handle image or text inputs with\nonly the style blocks. To evaluate our method on fine-grained style transfer,\nwe build a benchmark for embroidery customization. Our approach surpasses prior\nmethods on this task and further demonstrates strong generalization to three\nadditional domains: artistic style transfer, sketch colorization, and\nappearance transfer.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u8026\u7ec6\u7c92\u5ea6\u98ce\u683c\u548c\u5185\u5bb9\u7279\u5f81\uff0c\u4ee5\u5b9e\u73b0\u7ee3\u82b1\u7b49\u7eba\u7ec7\u827a\u672f\u7684\u5b9a\u5236\u5316\u3002", "motivation": "\u73b0\u6709\u7684\u98ce\u683c\u8fc1\u79fb\u65b9\u6cd5\u5728\u5904\u7406\u7ee3\u82b1\u8fd9\u79cd\u5177\u6709\u590d\u6742\u9488\u6cd5\u548c\u6750\u8d28\u7279\u6027\u7684\u7eba\u7ec7\u827a\u672f\u65f6\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u80fd\u591f\u8fdb\u884c\u7ec6\u7c92\u5ea6\u89c6\u89c9\u7279\u5f81\u8fc1\u79fb\u7684\u6280\u672f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u7684\u6269\u6563\u6a21\u578b\u89e3\u8026\u98ce\u683c\u548c\u5185\u5bb9\u7279\u5f81\u3002\u901a\u8fc7\u6784\u5efa\u56fe\u50cf\u5bf9\u5b9a\u4e49\u76ee\u6807\u98ce\u683c\uff0c\u5e76\u91c7\u7528\u57fa\u4e8e\u89e3\u8026\u8868\u793a\u7684\u76f8\u4f3c\u6027\u5ea6\u91cf\u3002\u7136\u540e\uff0c\u5229\u7528\u4e24\u9636\u6bb5\u5bf9\u6bd4LoRA\u8c03\u5236\u6280\u672f\u6355\u6349\u7ec6\u7c92\u5ea6\u98ce\u683c\u7279\u5f81\uff0c\u7b2c\u4e00\u9636\u6bb5\u66f4\u65b0\u6574\u4e2aLoRA\u548c\u9009\u5b9a\u7684\u98ce\u683c\u5757\u4ee5\u521d\u6b65\u5206\u79bb\u98ce\u683c\u548c\u5185\u5bb9\uff0c\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u81ea\u77e5\u8bc6\u84b8\u998f\u8fdb\u4e00\u6b65\u89e3\u8026\u3002\u6700\u540e\u6784\u5efa\u63a8\u7406\u6d41\u7a0b\uff0c\u4ec5\u4f7f\u7528\u98ce\u683c\u5757\u5904\u7406\u56fe\u50cf\u6216\u6587\u672c\u8f93\u5165\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u7ee3\u82b1\u5b9a\u5236\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5728\u827a\u672f\u98ce\u683c\u8fc1\u79fb\u3001\u7d20\u63cf\u7740\u8272\u548c\u5916\u89c2\u8fc1\u79fb\u4e09\u4e2a\u9644\u52a0\u9886\u57df\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u5b9e\u73b0\u4e86\u7ec6\u7c92\u5ea6\u98ce\u683c\u8fc1\u79fb\uff0c\u7279\u522b\u662f\u5728\u7ee3\u82b1\u5b9a\u5236\u5316\u65b9\u9762\uff0c\u5e76\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u6027\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u7684\u56fe\u50cf\u5904\u7406\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.18957", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.18957", "abs": "https://arxiv.org/abs/2509.18957", "authors": ["Shengye Song", "Minxian Xu", "Kan Hu", "Wenxia Guo", "Kejiang Ye"], "title": "TD3-Sched: Learning to Orchestrate Container-based Cloud-Edge Resources via Distributed Reinforcement Learning", "comment": "14 pages, 5 figures", "summary": "Resource scheduling in cloud-edge systems is challenging as edge nodes run\nlatency-sensitive workloads under tight resource constraints, while existing\ncentralized schedulers can suffer from performance bottlenecks and user\nexperience degradation. To address the issues of distributed decisions in\ncloud-edge environments, we present TD3-Sched, a distributed reinforcement\nlearning (DRL) scheduler based on Twin Delayed Deep Deterministic Policy\nGradient (TD3) for continuous control of CPU and memory allocation, which can\nachieve optimized decisions for resource provisioning under dynamic workloads.\nOn a realistic cloud-edge testbed with SockShop application and Alibaba traces,\nTD3-Sched achieves reductions of 17.9% to 38.6% in latency under same loads\ncompared with other reinforcement-learning and rule-based baselines, and 16% to\n31.6% under high loads. TD3-Sched also shows superior Service Level Objective\n(SLO) compliance with only 0.47% violations. These results indicate faster\nconvergence, lower latency, and more stable performance while preserving\nservice quality in container-based cloud-edge environment compared with the\nbaselines.", "AI": {"tldr": "TD3-Sched\u662f\u4e00\u4e2a\u57fa\u4e8eTD3\u7684\u5206\u5e03\u5f0f\u5f3a\u5316\u5b66\u4e60\u8c03\u5ea6\u5668\uff0c\u7528\u4e8e\u4e91\u8fb9\u534f\u540c\u73af\u5883\u4e0b\u7684CPU\u548c\u5185\u5b58\u8d44\u6e90\u5206\u914d\uff0c\u80fd\u6709\u6548\u964d\u4f4e\u5ef6\u8fdf\u5e76\u63d0\u9ad8\u670d\u52a1\u8d28\u91cf\u3002", "motivation": "\u4e91\u8fb9\u534f\u540c\u73af\u5883\u4e2d\u7684\u8d44\u6e90\u8c03\u5ea6\u9762\u4e34\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u8fb9\u7f18\u8282\u70b9\u8d44\u6e90\u53d7\u9650\u4e14\u5bf9\u5ef6\u8fdf\u654f\u611f\u7684\u573a\u666f\u4e0b\uff0c\u73b0\u6709\u7684\u96c6\u4e2d\u5f0f\u8c03\u5ea6\u5668\u53ef\u80fd\u5bfc\u81f4\u6027\u80fd\u74f6\u9888\u548c\u7528\u6237\u4f53\u9a8c\u4e0b\u964d\u3002", "method": "\u63d0\u51faTD3-Sched\uff0c\u4e00\u4e2a\u57fa\u4e8eTwin Delayed Deep Deterministic Policy Gradient (TD3) \u7684\u5206\u5e03\u5f0f\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u8c03\u5ea6\u5668\uff0c\u7528\u4e8e\u8fde\u7eed\u63a7\u5236CPU\u548c\u5185\u5b58\u7684\u5206\u914d\uff0c\u4ee5\u4f18\u5316\u52a8\u6001\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u7684\u8d44\u6e90\u914d\u7f6e\u3002", "result": "\u5728\u57fa\u4e8eSockShop\u5e94\u7528\u548c\u963f\u91cc\u5df4\u5df4\u6570\u636e\u7684\u4e91\u8fb9\u6d4b\u8bd5\u53f0\u4e0a\uff0cTD3-Sched\u76f8\u6bd4\u4e8e\u5176\u4ed6\u5f3a\u5316\u5b66\u4e60\u548c\u57fa\u4e8e\u89c4\u5219\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u76f8\u540c\u8d1f\u8f7d\u4e0b\u5ef6\u8fdf\u964d\u4f4e\u4e8617.9%\u81f338.6%\uff0c\u5728\u9ad8\u8d1f\u8f7d\u4e0b\u964d\u4f4e\u4e8616%\u81f331.6%\u3002\u5176\u670d\u52a1\u6c34\u5e73\u76ee\u6807\uff08SLO\uff09\u5408\u89c4\u6027\u4e5f\u8868\u73b0\u4f18\u5f02\uff0c\u4ec5\u67090.47%\u7684\u8fdd\u89c4\u3002", "conclusion": "TD3-Sched\u5728\u5bb9\u5668\u5316\u7684\u4e91\u8fb9\u73af\u5883\u4e2d\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\uff0c\u80fd\u591f\u5b9e\u73b0\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u3001\u66f4\u4f4e\u7684\u5ef6\u8fdf\u548c\u66f4\u7a33\u5b9a\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u670d\u52a1\u8d28\u91cf\u3002"}}
{"id": "2509.18518", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.18518", "abs": "https://arxiv.org/abs/2509.18518", "authors": ["Bai Xue", "Luke Ong", "Dominik Wagner", "Peixin Wang"], "title": "Refined Barrier Conditions for Finite-Time Safety and Reach-Avoid Guarantees in Stochastic Systems", "comment": null, "summary": "Providing finite-time probabilistic safety and reach-avoid guarantees is\ncrucial for safety-critical stochastic systems. Existing barrier certificate\nmethods often rely on a restrictive boundedness assumption for auxiliary\nfunctions, limiting their applicability. This paper presents refined\nbarrier-like conditions that remove this assumption. Specifically, we establish\nconditions for deriving upper bounds on finite-time safety probabilities in\ndiscrete-time systems and lower bounds on finite-time reach-avoid probabilities\nin continuous-time systems. This key relaxation significantly expands the class\nof verifiable systems, especially those with unbounded state spaces, and\nfacilitates the application of advanced optimization techniques, such as\nsemi-definite programming with polynomial functions. The efficacy of our\napproach is validated through numerical examples.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u6539\u8fdb\u7684\u969c\u788d\u51fd\u6570\u6761\u4ef6\uff0c\u653e\u5bbd\u4e86\u5bf9\u8f85\u52a9\u51fd\u6570\u7684\u6709\u754c\u6027\u5047\u8bbe\uff0c\u4ece\u800c\u5728\u6709\u9650\u65f6\u95f4\u5185\u4e3a\u968f\u673a\u7cfb\u7edf\u63d0\u4f9b\u6982\u7387\u5b89\u5168\u548c\u53ef\u8fbe-\u907f\u514d\u4fdd\u8bc1\uff0c\u5e76\u6269\u5c55\u4e86\u53ef\u9a8c\u8bc1\u7cfb\u7edf\u7684\u8303\u56f4\u3002", "motivation": "\u73b0\u6709\u969c\u788d\u8bc1\u4e66\u65b9\u6cd5\u5728\u5904\u7406\u968f\u673a\u7cfb\u7edf\u65f6\uff0c\u901a\u5e38\u9700\u8981\u8f85\u52a9\u51fd\u6570\u5177\u6709\u6709\u754c\u6027\u5047\u8bbe\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5e94\u7528\u8303\u56f4\u3002\u672c\u7814\u7a76\u65e8\u5728\u514b\u670d\u8fd9\u4e00\u9650\u5236\uff0c\u4e3a\u66f4\u5e7f\u6cdb\u7684\u968f\u673a\u7cfb\u7edf\u63d0\u4f9b\u6709\u9650\u65f6\u95f4\u6982\u7387\u5b89\u5168\u548c\u53ef\u8fbe-\u907f\u514d\u4fdd\u8bc1\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u6539\u8fdb\u7684\u969c\u788d\u7c7b\u6761\u4ef6\uff0c\u653e\u5bbd\u4e86\u5bf9\u8f85\u52a9\u51fd\u6570\u7684\u6709\u754c\u6027\u5047\u8bbe\u3002\u5177\u4f53\u800c\u8a00\uff0c\u7814\u7a76\u5728\u79bb\u6563\u65f6\u95f4\u7cfb\u7edf\u4e2d\u5efa\u7acb\u4e86\u63a8\u5bfc\u6709\u9650\u65f6\u95f4\u5b89\u5168\u6982\u7387\u4e0a\u754c\u7684\u6761\u4ef6\uff0c\u5728\u8fde\u7eed\u65f6\u95f4\u7cfb\u7edf\u4e2d\u5efa\u7acb\u4e86\u63a8\u5bfc\u6709\u9650\u65f6\u95f4\u53ef\u8fbe-\u907f\u514d\u6982\u7387\u4e0b\u754c\u7684\u6761\u4ef6\u3002", "result": "\u901a\u8fc7\u653e\u5bbd\u6709\u754c\u6027\u5047\u8bbe\uff0c\u672c\u7814\u7a76\u663e\u8457\u6269\u5c55\u4e86\u53ef\u9a8c\u8bc1\u7cfb\u7edf\u7684\u7c7b\u522b\uff0c\u7279\u522b\u662f\u90a3\u4e9b\u5177\u6709\u65e0\u754c\u72b6\u6001\u7a7a\u95f4\u7684\u7cfb\u7edf\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u8fd8\u4fc3\u8fdb\u4e86\u8bf8\u5982\u5e26\u6709\u591a\u9879\u5f0f\u51fd\u6570\u7684\u534a\u5b9a\u89c4\u5212\u7b49\u5148\u8fdb\u4f18\u5316\u6280\u672f\u7684\u5e94\u7528\u3002\u901a\u8fc7\u6570\u503c\u793a\u4f8b\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u6539\u8fdb\u969c\u788d\u7c7b\u6761\u4ef6\u80fd\u591f\u53bb\u9664\u5bf9\u8f85\u52a9\u51fd\u6570\u7684\u6709\u754c\u6027\u5047\u8bbe\uff0c\u4ece\u800c\u5728\u6709\u9650\u65f6\u95f4\u5185\u4e3a\u968f\u673a\u7cfb\u7edf\u63d0\u4f9b\u6982\u7387\u5b89\u5168\u548c\u53ef\u8fbe-\u907f\u514d\u4fdd\u8bc1\uff0c\u5e76\u4e14\u6269\u5c55\u4e86\u53ef\u9a8c\u8bc1\u7cfb\u7edf\u7684\u8303\u56f4\uff0c\u80fd\u591f\u5e94\u7528\u4e8e\u5177\u6709\u65e0\u754c\u72b6\u6001\u7a7a\u95f4\u7684\u7cfb\u7edf\uff0c\u5e76\u4fc3\u8fdb\u4e86\u5148\u8fdb\u4f18\u5316\u6280\u672f\u7684\u5e94\u7528\u3002"}}
{"id": "2509.19257", "categories": ["cs.ET", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.19257", "abs": "https://arxiv.org/abs/2509.19257", "authors": ["Juan E. Gilbert", "Jean D. Louis"], "title": "A Stateless Transparent Voting Machine", "comment": "11 pages, 2 figures", "summary": "Transparency and security are essential in our voting system, and voting\nmachines. This paper describes an implementation of a stateless, transparent\nvoting machine (STVM). The STVM is a ballot marking device (BMD) that uses a\ntransparent, interactive printing interface where voters can verify their paper\nballots as they fill out the ballot. The transparent interface turns the paper\nballot into an interactive interface. In this architecture, stateless describes\nthe machine's boot sequence, where no information is stored or passed forward\nbetween reboots. The machine does not have a hard drive. Instead, it boots and\nruns from read-only media. This STVM design utilizes a Blu-ray Disc ROM (BD-R)\nto boot the voting software. This system's statelessness and the transparent\ninteractive printing interface make this design the most secure BMD for voting.\nUnlike other voting methods, this system incorporates high usability,\naccessibility, and security for all voters. The STVM uses an open-source voting\nsystem that has a universally designed interface, making the system accessible\nfor all voters independent of their ability or disability. This system can make\nvoting safer by simultaneously addressing the issue of voters noticing a vote\nflip and making it difficult for a hack to persist or go unmitigated.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u65e0\u72b6\u6001\u3001\u900f\u660e\u6295\u7968\u673a\u201d\uff08STVM\uff09\u7684\u6295\u7968\u673a\u8bbe\u8ba1\uff0c\u5b83\u901a\u8fc7\u4ea4\u4e92\u5f0f\u6253\u5370\u754c\u9762\u8ba9\u9009\u6c11\u5728\u6295\u7968\u65f6\u5c31\u80fd\u6838\u5bf9\u81ea\u5df1\u7684\u7eb8\u8d28\u9009\u7968\uff0c\u5e76\u5229\u7528\u53ea\u8bfb\u5a92\u4f53\uff08BD-R\uff09\u542f\u52a8\uff0c\u786e\u4fdd\u4e86\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u65e0\u72b6\u6001\u6027\u3002", "motivation": "\u63d0\u9ad8\u6295\u7968\u7cfb\u7edf\u7684\u900f\u660e\u5ea6\u548c\u5b89\u5168\u6027\uff0c\u89e3\u51b3\u9009\u7968\u88ab\u7be1\u6539\u548c\u7cfb\u7edf\u88ab\u653b\u51fb\u7684\u95ee\u9898\u3002", "method": "\u5b9e\u73b0\u4e86\u4e00\u79cd\u65e0\u72b6\u6001\u3001\u900f\u660e\u7684\u6295\u7968\u673a\uff08STVM\uff09\uff0c\u4f7f\u7528\u4ea4\u4e92\u5f0f\u6253\u5370\u754c\u9762\u8ba9\u9009\u6c11\u6838\u5bf9\u7eb8\u8d28\u9009\u7968\uff0c\u5e76\u4ece\u53ea\u8bfb\u5a92\u4f53\uff08BD-R\uff09\u542f\u52a8\uff0c\u4e0d\u4f7f\u7528\u786c\u76d8\u3002", "result": "STVM\u901a\u8fc7\u5176\u65e0\u72b6\u6001\u8bbe\u8ba1\u548c\u900f\u660e\u4ea4\u4e92\u5f0f\u6253\u5370\u754c\u9762\uff0c\u63d0\u4f9b\u4e86\u9ad8\u53ef\u7528\u6027\u3001\u53ef\u8bbf\u95ee\u6027\u548c\u5b89\u5168\u6027\uff0c\u89e3\u51b3\u4e86\u9009\u7968\u88ab\u7be1\u6539\u548c\u7cfb\u7edf\u88ab\u653b\u51fb\u7684\u95ee\u9898\u3002", "conclusion": "STVM\u662f\u4e00\u79cd\u5b89\u5168\u3001\u6613\u7528\u4e14\u65e0\u969c\u788d\u7684\u6295\u7968\u673a\u8bbe\u8ba1\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u6295\u7968\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u53ef\u4fe1\u5ea6\u3002"}}
{"id": "2509.19279", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2509.19279", "abs": "https://arxiv.org/abs/2509.19279", "authors": ["Huy Vu Bui", "Michael C. Chavrimootoo", "Trung Kien Le", "Son M. Nguyen"], "title": "Approximating Electoral Control Problems", "comment": null, "summary": "Much research in electoral control -- one of the most studied form of\nelectoral attacks, in which an entity running an election alters the structure\nof that election to yield a preferred outcome -- has focused on giving decision\ncomplexity results, e.g., membership in P, NP-completeness, or fixed-parameter\ntractability. Approximation algorithms on the other hand have received little\nattention in electoral control, despite their prevalence in the study of other\nforms of electoral attacks, such as manipulation and bribery. Early work\nestablished some preliminary results with respect to popular voting rules such\nas plurality, approval, and Condorcet. In this paper, we establish for each of\nthe ``standard'' control problems under plurality, approval, and Condorcet,\nwhether they are approximable, and we prove our results in both the weighted\nand unweighted voter settings. For each problem we study under either approval\nor Condorcet, we show that any approximation algorithm we give is optimal,\nunless P=NP. Our approximation algorithms leverage the fact that Covering\nInteger Programs (CIPs) can be approximated within a factor of $O(\\log n)$.\nUnder plurality, we give an $O(m)$-approximation algorithm, and give as lower\nbound $\\Omega(m^{1/4})$, by using a known lower bound on the Minimum $k$-Union\n(M$k$U) problem. To our knowledge, this is the first application of M$k$U in\ncomputational social choice. We also generalize our $O(m)$-approximation\nalgorithm to work with respect to an infinite family of voting rules using an\naxiomatic approach. Our work closes a long list of open problems established 18\nyears ago.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u9009\u4e3e\u63a7\u5236\u95ee\u9898\uff08\u4e00\u79cd\u9009\u4e3e\u653b\u51fb\u5f62\u5f0f\uff09\u7684\u8fd1\u4f3c\u7b97\u6cd5\uff0c\u91cd\u70b9\u5173\u6ce8\u6295\u7968\u89c4\u5219\u4e2d\u7684\u591a\u9009\u3001\u6279\u51c6\u548c\u5b54\u591a\u585e\u89c4\u5219\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5728 P!=NP \u7684\u524d\u63d0\u4e0b\uff0c\u5bf9\u4e8e\u6279\u51c6\u548c\u5b54\u591a\u585e\u89c4\u5219\uff0c\u6587\u4e2d\u6240\u63d0\u51fa\u7684\u8fd1\u4f3c\u7b97\u6cd5\u662f\u6700\u4f18\u7684\u3002\u5bf9\u4e8e\u591a\u9009\u89c4\u5219\uff0c\u6587\u4e2d\u7ed9\u51fa\u4e86\u4e00\u4e2a O(m) \u7684\u8fd1\u4f3c\u7b97\u6cd5\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u8fd1\u4f3c\u5ea6\u4e0b\u754c\u4e3a Omega(m^{1/4})\u3002", "motivation": "\u5927\u591a\u6570\u5173\u4e8e\u9009\u4e3e\u63a7\u5236\u7684\u7814\u7a76\u90fd\u96c6\u4e2d\u5728\u8ba1\u7b97\u590d\u6742\u6027\u65b9\u9762\uff0c\u800c\u5ffd\u7565\u4e86\u8fd1\u4f3c\u7b97\u6cd5\u3002\u5c3d\u7ba1\u8fd1\u4f3c\u7b97\u6cd5\u5728\u7814\u7a76\u64cd\u7eb5\u548c\u8d3f\u8d42\u7b49\u5176\u4ed6\u9009\u4e3e\u653b\u51fb\u5f62\u5f0f\u65f6\u5f97\u5230\u4e86\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5728\u9009\u4e3e\u63a7\u5236\u9886\u57df\u5374\u9c9c\u6709\u5173\u6ce8\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\uff0c\u4e3a\u9009\u4e3e\u63a7\u5236\u95ee\u9898\u63d0\u4f9b\u8fd1\u4f3c\u7b97\u6cd5\u65b9\u9762\u7684\u5206\u6790\u3002", "method": "\u672c\u6587\u5229\u7528\u8986\u76d6\u6574\u6570\u89c4\u5212\uff08CIP\uff09\u53ef\u4ee5\u88ab\u8fd1\u4f3c\u5230 O(log n) \u7684\u4e8b\u5b9e\uff0c\u6765\u8bbe\u8ba1\u548c\u5206\u6790\u8fd1\u4f3c\u7b97\u6cd5\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u5bf9\u4e8e\u6279\u51c6\u548c\u5b54\u591a\u585e\u6295\u7968\u89c4\u5219\uff0c\u8bc1\u660e\u4e86\u6240\u7ed9\u7b97\u6cd5\u7684\u6700\u4f18\u6027\uff08\u9664\u975e P=NP\uff09\u3002\u5bf9\u4e8e\u591a\u9009\u6295\u7968\u89c4\u5219\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd O(m) \u8fd1\u4f3c\u7b97\u6cd5\uff0c\u5e76\u5229\u7528\u6700\u5c0f k-\u5e76\u96c6\uff08M k U\uff09\u95ee\u9898\u7684\u5df2\u77e5\u4e0b\u754c\u6765\u8bc1\u660e\u5176\u8fd1\u4f3c\u5ea6\u4e0b\u754c\u4e3a Omega(m^{1/4})\u3002\u6b64\u5916\uff0c\u8fd8\u91c7\u7528\u516c\u7406\u5316\u65b9\u6cd5\u5c06 O(m) \u8fd1\u4f3c\u7b97\u6cd5\u63a8\u5e7f\u5230\u65e0\u9650\u591a\u7684\u6295\u7968\u89c4\u5219\u3002", "result": "\u5bf9\u4e8e\u6279\u51c6\u548c\u5b54\u591a\u585e\u6295\u7968\u89c4\u5219\uff0c\u8bc1\u660e\u4e86\u5728 P!=NP \u7684\u524d\u63d0\u4e0b\uff0c\u6240\u63d0\u51fa\u7684\u8fd1\u4f3c\u7b97\u6cd5\u662f\u8fd1\u4f3c\u6700\u4f18\u7684\u3002\u5bf9\u4e8e\u591a\u9009\u6295\u7968\u89c4\u5219\uff0c\u7ed9\u51fa\u4e86\u4e00\u4e2a O(m) \u7684\u8fd1\u4f3c\u7b97\u6cd5\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u8fd1\u4f3c\u5ea6\u4e0b\u754c\u4e3a Omega(m^{1/4})\u3002\u7814\u7a76\u7ed3\u679c\u89e3\u51b3\u4e86 18 \u5e74\u524d\u63d0\u51fa\u7684\u4e00\u4e2a\u957f\u671f\u60ac\u800c\u672a\u51b3\u7684\u95ee\u9898\u3002", "conclusion": "\u672c\u6587\u4e3a\u9009\u4e3e\u63a7\u5236\u95ee\u9898\uff08\u7279\u522b\u662f\u591a\u9009\u3001\u6279\u51c6\u548c\u5b54\u591a\u585e\u89c4\u5219\uff09\u63d0\u4f9b\u4e86\u8fd1\u4f3c\u7b97\u6cd5\u7684\u5206\u6790\uff0c\u5e76\u53d6\u5f97\u4e86\u6700\u4f18\u6216\u63a5\u8fd1\u6700\u4f18\u7684\u7ed3\u679c\u3002\u7814\u7a76\u7ed3\u679c\u4e0d\u4ec5\u586b\u8865\u4e86\u8be5\u9886\u57df\u7684\u7a7a\u767d\uff0c\u8fd8\u4e3a\u672a\u6765\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\uff0c\u4f8b\u5982\u5c06\u8fd1\u4f3c\u7b97\u6cd5\u5e94\u7528\u4e8e\u66f4\u5e7f\u6cdb\u7684\u6295\u7968\u89c4\u5219\u548c\u9009\u4e3e\u653b\u51fb\u5f62\u5f0f\u3002"}}
{"id": "2509.18828", "categories": ["cond-mat.mes-hall", "cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2509.18828", "abs": "https://arxiv.org/abs/2509.18828", "authors": ["Zhesen Yang", "Zihan Wang", "Juntao Huang", "Zijian Zheng", "Jiangping Hu"], "title": "Complex Frequency Fingerprint: Interacting Driven Non-Hermitian Skin Effect", "comment": "8 pages, 3 figures,", "summary": "The excitation properties of quantum many-body systems are encoded in their\nresponse functions. These functions define an associated response Hamiltonian,\nwhich is intrinsically non-Hermitian due to the dissipative nature of retarded\nresponses, even in closed systems. By analyzing its eigenvalues and\neigenstates, one obtains a unique characterization of the system, referred to\nas the complex frequency fingerprint. Using this framework, we demonstrate that\ninteractions alone can give rise to both point-gap topology and the\nnon-Hermitian skin effect. Unlike the dissipation-induced skin effect, this\ninteraction-driven phenomenon exhibits pronounced frequency dependence. We\nfurther introduce a complex-frequency density of states framework that\ndistinctly separates non-Hermitian skin modes from topological edge modes.", "AI": {"tldr": "The paper introduces the concept of a ", "motivation": "quantum many-body systems", "method": "complex frequency fingerprint derived from response functions to demonstrate interaction-induced non-Hermitian phenomena like point-gap topology and skin effect, and a complex-frequency density of states to distinguish skin and edge modes.", "result": "Interactions alone can cause point-gap topology and the non-Hermitian skin effect, which is frequency-dependent, unlike dissipation-induced effects. A complex-frequency density of states can differentiate non-Hermitian skin modes from topological edge modes.", "conclusion": "The complex frequency fingerprint provides a new perspective for understanding excitation properties and non-Hermitian phenomena in quantum many-body systems."}}
{"id": "2509.18409", "categories": ["cond-mat.mtrl-sci", "cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2509.18409", "abs": "https://arxiv.org/abs/2509.18409", "authors": ["Mahtab A. Khan", "Jayden D. Craft", "Hari P. Paudel", "Yuhua Duan", "Dirk R. Englund", "Michael N. Leuenberger"], "title": "Er$_\\mathrm{Al}$:Al$_2$O$_3$ for Telecom-Band Photonics: Electronic Structure and Optical Properties", "comment": "10 pages, 4 figures", "summary": "Er-doped Al$_2$O$_3$ is a promising host for telecom-band integrated\nphotonics. Here we combine ab initio calculations with a symmetry-resolved\nanalysis to elucidate substitutional Er on the Al site (Er$_\\mathrm{Al}$) in\n$\\alpha$-Al$_2$O$_3$. First-principles relaxations confirm the structural\nstability of Er$_\\mathrm{Al}$. We then use the local trigonal crystal-field\nsymmetry to classify the Er-derived impurity levels by irreducible\nrepresentations and to derive polarization-resolved electric-dipole selection\nrules, explicitly identifying the symmetry-allowed $f$\\textendash$d$\nhybridization channels. Kubo--Greenwood absorption spectra computed from\nKohn--Sham states quantitatively corroborate these symmetry predictions.\nFurthermore, we connect the calculated intra-$4f$ line strengths to Judd--Ofelt\ntheory, clarifying the role of $4f$\\textendash$5d$ admixture in enabling\noptical activity. Notably, we predict a characteristic absorption near\n$1.47~\\mu\\mathrm{m}$ (telecom band), relevant for on-chip amplification and\nemission. To our knowledge, a symmetry-resolved first-principles treatment of\nEr:Al$_2$O$_3$ with an explicit Judd--Ofelt interpretation has not been\nreported, providing a transferable framework for tailoring rare-earth dopants\nin wide-band-gap oxides for integrated photonics. Our results for the optical\nspectra are in good agreement with experimental data.", "AI": {"tldr": "Er-doped Al2O3\u662f\u4e00\u79cd\u6709\u524d\u9014\u7684\u7535\u4fe1\u6ce2\u6bb5\u96c6\u6210\u5149\u5b50\u5b66\u6750\u6599\u3002\u672c\u7814\u7a76\u5229\u7528\u4ece\u5934\u7b97\u548c\u5bf9\u79f0\u6027\u5206\u6790\u6765\u7814\u7a76Er\u5728Al\u4f4d\u70b9\u7684\u53d6\u4ee3\uff08ErAl\uff09\u5728\u03b1-Al2O3\u4e2d\u7684\u884c\u4e3a\u3002\u8ba1\u7b97\u8bc1\u5b9e\u4e86ErAl\u7684\u7ed3\u6784\u7a33\u5b9a\u6027\uff0c\u5e76\u5229\u7528\u5c40\u90e8\u4e09\u65b9\u6676\u683c\u5bf9\u79f0\u6027\u5bf9Er\u80fd\u7ea7\u8fdb\u884c\u4e86\u5206\u7c7b\uff0c\u63a8\u5bfc\u4e86\u504f\u632f\u5206\u8fa8\u7684\u7535\u5076\u6781\u9009\u62e9\u89c4\u5219\uff0c\u660e\u786e\u4e86\u5bf9\u79f0\u6027\u5141\u8bb8\u7684f-d\u6742\u5316\u901a\u9053\u3002\u901a\u8fc7Kubo-Greenwood\u5438\u6536\u5149\u8c31\u8ba1\u7b97\uff0c\u5b9a\u91cf\u9a8c\u8bc1\u4e86\u8fd9\u4e9b\u5bf9\u79f0\u6027\u9884\u6d4b\u3002\u6b64\u5916\uff0c\u8fd8\u5c06\u8ba1\u7b97\u5f97\u5230\u76844f\u5185\u8c31\u7ebf\u5f3a\u5ea6\u4e0eJudd-Ofelt\u7406\u8bba\u8054\u7cfb\u8d77\u6765\uff0c\u9610\u660e\u4e864f-5d\u6df7\u5408\u5728\u5149\u5b66\u6d3b\u6027\u4e2d\u7684\u4f5c\u7528\u3002\u7814\u7a76\u9884\u6d4b\u4e86\u57281.47\u03bcm\uff08\u7535\u4fe1\u6ce2\u6bb5\uff09\u9644\u8fd1\u7684\u7279\u5f81\u5438\u6536\uff0c\u8fd9\u5bf9\u4e8e\u7247\u4e0a\u653e\u5927\u548c\u53d1\u5c04\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002\u8be5\u7814\u7a76\u9996\u6b21\u5bf9Er:Al2O3\u8fdb\u884c\u4e86\u5bf9\u79f0\u6027\u5206\u8fa8\u7684\u7b2c\u4e00\u6027\u539f\u7406\u5904\u7406\uff0c\u5e76\u7ed3\u5408\u4e86Judd-Ofelt\u7406\u8bba\u89e3\u91ca\uff0c\u4e3a\u5728\u5bbd\u5e26\u9699\u6c27\u5316\u7269\u4e2d\u5b9a\u5236\u7a00\u571f\u63ba\u6742\u5242\u4ee5\u7528\u4e8e\u96c6\u6210\u5149\u5b50\u5b66\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u8fc1\u79fb\u7684\u6846\u67b6\u3002\u8ba1\u7b97\u7684\u5149\u8c31\u7ed3\u679c\u4e0e\u5b9e\u9a8c\u6570\u636e\u543b\u5408\u826f\u597d\u3002", "motivation": "\u7814\u7a76Er\u5728Al\u4f4d\u70b9\u7684\u53d6\u4ee3\uff08ErAl\uff09\u5728\u03b1-Al2O3\u4e2d\u7684\u884c\u4e3a\uff0c\u65e8\u5728\u4e3a\u7535\u4fe1\u6ce2\u6bb5\u96c6\u6210\u5149\u5b50\u5b66\u63d0\u4f9b\u6709\u524d\u666f\u7684\u6750\u6599\uff0c\u5e76\u9610\u660e\u5176\u5149\u5b66\u6027\u8d28\u7684\u5fae\u89c2\u673a\u5236\u3002", "method": "\u5229\u7528\u4ece\u5934\u7b97\uff08ab initio calculations\uff09\u548c\u5bf9\u79f0\u6027\u5206\u6790\uff0c\u7ed3\u5408\u7b2c\u4e00\u6027\u539f\u7406\u5f1b\u8c6b\u3001\u5bf9\u79f0\u6027\u5206\u7c7b\u3001\u504f\u632f\u5206\u8fa8\u7535\u5076\u6781\u9009\u62e9\u89c4\u5219\u63a8\u5bfc\u3001Kubo-Greenwood\u5438\u6536\u5149\u8c31\u8ba1\u7b97\u4ee5\u53caJudd-Ofelt\u7406\u8bba\u5206\u6790\u3002", "result": "\u8bc1\u5b9e\u4e86ErAl\u7684\u7ed3\u6784\u7a33\u5b9a\u6027\uff1b\u660e\u786e\u4e86\u5bf9\u79f0\u6027\u5141\u8bb8\u7684f-d\u6742\u5316\u901a\u9053\uff1b\u5b9a\u91cf\u9a8c\u8bc1\u4e86\u5bf9\u79f0\u6027\u9884\u6d4b\uff1b\u9610\u660e\u4e864f-5d\u6df7\u5408\u5728\u5149\u5b66\u6d3b\u6027\u4e2d\u7684\u4f5c\u7528\uff1b\u9884\u6d4b\u4e86\u57281.47\u03bcm\uff08\u7535\u4fe1\u6ce2\u6bb5\uff09\u9644\u8fd1\u5b58\u5728\u7279\u5f81\u5438\u6536\uff1b\u8ba1\u7b97\u7684\u5149\u8c31\u7ed3\u679c\u4e0e\u5b9e\u9a8c\u6570\u636e\u543b\u5408\u826f\u597d\u3002", "conclusion": "Er-doped Al2O3\u662f\u4e00\u79cd\u6709\u524d\u9014\u7684\u7535\u4fe1\u6ce2\u6bb5\u96c6\u6210\u5149\u5b50\u5b66\u6750\u6599\u3002\u672c\u7814\u7a76\u901a\u8fc7\u5bf9\u79f0\u6027\u5206\u8fa8\u7684\u7b2c\u4e00\u6027\u539f\u7406\u5904\u7406\u548cJudd-Ofelt\u7406\u8bba\u89e3\u91ca\uff0c\u4e3a\u7406\u89e3\u548c\u5b9a\u5236\u7a00\u571f\u63ba\u6742\u5242\u5728\u5bbd\u5e26\u9699\u6c27\u5316\u7269\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\uff0c\u5e76\u9884\u6d4b\u4e86\u5176\u5728\u5149\u5b50\u5668\u4ef6\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.18376", "categories": ["cs.LG", "cs.SI"], "pdf": "https://arxiv.org/pdf/2509.18376", "abs": "https://arxiv.org/abs/2509.18376", "authors": ["Burouj Armgaan", "Eshan Jain", "Harsh Pandey", "Mahesh Chandran", "Sayan Ranu"], "title": "GnnXemplar: Exemplars to Explanations - Natural Language Rules for Global GNN Interpretability", "comment": "31 pages, 20 figures, NeurIPS 2025 (Oral)", "summary": "Graph Neural Networks (GNNs) are widely used for node classification, yet\ntheir opaque decision-making limits trust and adoption. While local\nexplanations offer insights into individual predictions, global explanation\nmethods, those that characterize an entire class, remain underdeveloped.\nExisting global explainers rely on motif discovery in small graphs, an approach\nthat breaks down in large, real-world settings where subgraph repetition is\nrare, node attributes are high-dimensional, and predictions arise from complex\nstructure-attribute interactions. We propose GnnXemplar, a novel global\nexplainer inspired from Exemplar Theory from cognitive science. GnnXemplar\nidentifies representative nodes in the GNN embedding space, exemplars, and\nexplains predictions using natural language rules derived from their\nneighborhoods. Exemplar selection is framed as a coverage maximization problem\nover reverse k-nearest neighbors, for which we provide an efficient greedy\napproximation. To derive interpretable rules, we employ a self-refining prompt\nstrategy using large language models (LLMs). Experiments across diverse\nbenchmarks show that GnnXemplar significantly outperforms existing methods in\nfidelity, scalability, and human interpretability, as validated by a user study\nwith 60 participants.", "AI": {"tldr": "GnnXemplar\u662f\u4e00\u79cd\u65b0\u7684\u5168\u5c40\u56fe\u795e\u7ecf\u7f51\u7edc\u89e3\u91ca\u5668\uff0c\u5b83\u5229\u7528\u8ba4\u77e5\u79d1\u5b66\u4e2d\u7684\u5178\u578b\u7406\u8bba\uff0c\u901a\u8fc7\u8bc6\u522b\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u4ee3\u8868\u6027\u8282\u70b9\uff08\u5178\u578b\u8282\u70b9\uff09\u5e76\u6839\u636e\u5b83\u4eec\u7684\u90bb\u57df\u63a8\u5bfc\u51fa\u81ea\u7136\u8bed\u8a00\u89c4\u5219\u6765\u89e3\u91ca\u6574\u4e2a\u7c7b\u522b\u7684\u9884\u6d4b\u3002\u8be5\u65b9\u6cd5\u5728\u771f\u5b9e\u4e16\u754c\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5728\u4fdd\u771f\u5ea6\u3001\u53ef\u6269\u5c55\u6027\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u5f97\u5230\u4e86\u7528\u6237\u7814\u7a76\u7684\u9a8c\u8bc1\u3002", "motivation": "\u73b0\u6709\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u7684\u5168\u5c40\u89e3\u91ca\u65b9\u6cd5\u5728\u5904\u7406\u5927\u578b\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u56e0\u4e3a\u5b83\u4eec\u4f9d\u8d56\u4e8e\u5728\u5c0f\u56fe\u4e0a\u53d1\u73b0\u7684\u6a21\u5f0f\uff0c\u800c\u8fd9\u4e9b\u6a21\u5f0f\u5728\u5927\u89c4\u6a21\u56fe\u4e0a\u5f88\u5c11\u91cd\u590d\uff0c\u5e76\u4e14\u96be\u4ee5\u5904\u7406\u9ad8\u7ef4\u5c5e\u6027\u548c\u590d\u6742\u7684\u7ed3\u6784-\u5c5e\u6027\u4ea4\u4e92\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6709\u6548\u89e3\u91ca\u6574\u4e2a\u7c7b\u522b\u7684\u9884\u6d4b\uff0c\u5e76\u4e14\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u771f\u5b9e\u4e16\u754c\u573a\u666f\u7684\u5168\u5c40\u89e3\u91ca\u65b9\u6cd5\u3002", "method": "GnnXemplar\u5229\u7528\u8ba4\u77e5\u79d1\u5b66\u4e2d\u7684\u5178\u578b\u7406\u8bba\uff0c\u9996\u5148\u901a\u8fc7\u6700\u5927\u5316\u8986\u76d6\u53cd\u5411k\u8fd1\u90bb\u7684\u65b9\u5f0f\u9009\u62e9\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u4ee3\u8868\u6027\u8282\u70b9\uff08\u5178\u578b\u8282\u70b9\uff09\u3002\u7136\u540e\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u4e00\u79cd\u81ea\u4f18\u5316\u63d0\u793a\u7b56\u7565\uff0c\u6839\u636e\u8fd9\u4e9b\u5178\u578b\u8282\u70b9\u7684\u90bb\u57df\u4fe1\u606f\u751f\u6210\u53ef\u89e3\u91ca\u7684\u81ea\u7136\u8bed\u8a00\u89c4\u5219\uff0c\u4ece\u800c\u89e3\u91caGNN\u5bf9\u6574\u4e2a\u7c7b\u522b\u7684\u9884\u6d4b\u3002", "result": "GnnXemplar\u5728\u5404\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5728\u89e3\u91ca\u7684\u4fdd\u771f\u5ea6\u3001\u53ef\u6269\u5c55\u6027\u548c\u4eba\u7c7b\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u4e00\u9879\u5305\u542b60\u540d\u53c2\u4e0e\u8005\u7684\u7528\u6237\u7814\u7a76\u9a8c\u8bc1\u4e86\u5176\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "GnnXemplar\u662f\u4e00\u79cd\u6709\u6548\u7684\u5168\u5c40\u56fe\u795e\u7ecf\u7f51\u7edc\u89e3\u91ca\u65b9\u6cd5\uff0c\u901a\u8fc7\u501f\u9274\u5178\u578b\u7406\u8bba\uff0c\u5e76\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u80fd\u591f\u751f\u6210\u53ef\u4fe1\u3001\u53ef\u6269\u5c55\u4e14\u6613\u4e8e\u4eba\u7c7b\u7406\u89e3\u7684\u89e3\u91ca\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u5927\u89c4\u6a21\u771f\u5b9e\u4e16\u754c\u56fe\u6570\u636e\u65f6\u7684\u4e0d\u8db3\u3002"}}
{"id": "2509.18727", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.18727", "abs": "https://arxiv.org/abs/2509.18727", "authors": ["Yasaman Ettefagh", "Sharief Saleh", "Musa Furkan Keskin", "Hui Chen", "Gonzalo Seco-Granados", "Henk Wymeersch"], "title": "Integrated Cellular and LEO-based Positioning and Synchronization under User Mobility", "comment": null, "summary": "This paper investigates the localization, synchronization, and speed\nestimation of a mobile user equipment (UE) leveraging integrated terrestrial\nand non-terrestrial networks (NTNs), in particular low Earth orbit (LEO)\nsatellites. We focus on a minimal setup in which the UE received signal from\nonly one base station (BS) and one LEO satellite. We derive a generic signal\nmodel accounting for mobility, clock and frequency offsets, based on which a\nhierarchy of simplified models are proposed and organized by computational\ncomplexity. Estimation algorithms are developed for each model to facilitate\nefficient and accurate parameter recovery. Rigorous simulations validate the\neffectiveness of the proposed models, demonstrating their suitability across\ndiverse scenarios. The findings highlight how the trade-off between complexity\nand performance can be optimized for varying deployment environments and\napplication requirements, offering valuable insights for 6G positioning and\nsynchronization systems under user mobility.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u5730\u9762\u548c\u975e\u5730\u9762\u7f51\u7edc\uff08\u7279\u522b\u662f\u4f4e\u5730\u7403\u8f68\u9053\u536b\u661f\uff09\u878d\u5408\u73af\u5883\u4e0b\uff0c\u79fb\u52a8\u7528\u6237\u8bbe\u5907\u7684\u5b9a\u4f4d\u3001\u540c\u6b65\u548c\u901f\u5ea6\u4f30\u8ba1\u3002\u5728\u4ec5\u63a5\u6536\u6765\u81ea\u4e00\u4e2a\u57fa\u7ad9\u548c\u4e00\u4e2a\u4f4e\u5730\u7403\u8f68\u9053\u536b\u661f\u4fe1\u53f7\u7684\u6700\u5c0f\u914d\u7f6e\u4e0b\uff0c\u63a8\u5bfc\u4e86\u8003\u8651\u79fb\u52a8\u6027\u3001\u65f6\u949f\u548c\u9891\u7387\u504f\u79fb\u7684\u901a\u7528\u4fe1\u53f7\u6a21\u578b\uff0c\u5e76\u63d0\u51fa\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u9012\u51cf\u7684\u7b80\u5316\u6a21\u578b\u3002\u4e3a\u6bcf\u4e2a\u6a21\u578b\u5f00\u53d1\u4e86\u4f30\u8ba1\u7b97\u6cd5\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u6548\u7cbe\u786e\u7684\u53c2\u6570\u6062\u590d\u3002\u4eff\u771f\u7ed3\u679c\u9a8c\u8bc1\u4e86\u6240\u63d0\u6a21\u578b\u7684\u6709\u6548\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u3002\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u5728\u7528\u6237\u79fb\u52a8\u73af\u5883\u4e0b\uff0c\u5982\u4f55\u9488\u5bf9\u4e0d\u540c\u7684\u90e8\u7f72\u73af\u5883\u548c\u5e94\u7528\u9700\u6c42\u4f18\u5316\u590d\u6742\u5ea6\u4e0e\u6027\u80fd\u7684\u6743\u8861\uff0c\u4e3a6G\u5b9a\u4f4d\u548c\u540c\u6b65\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002", "motivation": "\u5728\u5730\u9762\u548c\u975e\u5730\u9762\u7f51\u7edc\uff08\u7279\u522b\u662f\u4f4e\u5730\u7403\u8f68\u9053\u536b\u661f\uff09\u878d\u5408\u73af\u5883\u4e0b\uff0c\u4e3a\u79fb\u52a8\u7528\u6237\u8bbe\u5907\uff08UE\uff09\u63d0\u4f9b\u5b9a\u4f4d\u3001\u540c\u6b65\u548c\u901f\u5ea6\u4f30\u8ba1\u89e3\u51b3\u65b9\u6848\u3002", "method": "1. \u5efa\u7acb\u4e00\u4e2a\u5305\u542b\u79fb\u52a8\u6027\u3001\u65f6\u949f\u548c\u9891\u7387\u504f\u79fb\u7684\u901a\u7528\u4fe1\u53f7\u6a21\u578b\u3002\n2. \u63d0\u51fa\u4e00\u7cfb\u5217\u8ba1\u7b97\u590d\u6742\u5ea6\u9012\u51cf\u7684\u7b80\u5316\u6a21\u578b\u3002\n3. \u4e3a\u6bcf\u4e2a\u6a21\u578b\u5f00\u53d1\u4f30\u8ba1\u7b97\u6cd5\u4ee5\u5b9e\u73b0\u53c2\u6570\u6062\u590d\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u9a8c\u8bc1\u4e86\u6240\u63d0\u6a21\u578b\u7684\u6709\u6548\u6027\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u3002\u6a21\u578b\u5728\u8ba1\u7b97\u590d\u6742\u5ea6\u4e0e\u6027\u80fd\u4e4b\u95f4\u5b9e\u73b0\u4e86\u4f18\u5316\uff0c\u4e3a6G\u5b9a\u4f4d\u548c\u540c\u6b65\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6a21\u578b\u548c\u4f30\u8ba1\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u5730\u5728\u5730\u9762\u548c\u975e\u5730\u9762\u7f51\u7edc\u878d\u5408\u73af\u5883\u4e0b\uff0c\u4e3a\u79fb\u52a8\u7528\u6237\u8bbe\u5907\u5b9e\u73b0\u5b9a\u4f4d\u3001\u540c\u6b65\u548c\u901f\u5ea6\u4f30\u8ba1\u3002\u901a\u8fc7\u6743\u8861\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u6027\u80fd\uff0c\u53ef\u4ee5\u6ee1\u8db3\u4e0d\u540c\u90e8\u7f72\u73af\u5883\u548c\u5e94\u7528\u9700\u6c42\uff0c\u4e3a6G\u7cfb\u7edf\u63d0\u4f9b\u652f\u6301\u3002"}}
{"id": "2509.18342", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18342", "abs": "https://arxiv.org/abs/2509.18342", "authors": ["Rajitha de Silva", "Jonathan Cox", "James R. Heselden", "Marija Popovic", "Cesar Cadena", "Riccardo Polvara"], "title": "Semantic-Aware Particle Filter for Reliable Vineyard Robot Localisation", "comment": "Sumbitted to ICRA 2026", "summary": "Accurate localisation is critical for mobile robots in structured outdoor\nenvironments, yet LiDAR-based methods often fail in vineyards due to repetitive\nrow geometry and perceptual aliasing. We propose a semantic particle filter\nthat incorporates stable object-level detections, specifically vine trunks and\nsupport poles into the likelihood estimation process. Detected landmarks are\nprojected into a birds eye view and fused with LiDAR scans to generate semantic\nobservations. A key innovation is the use of semantic walls, which connect\nadjacent landmarks into pseudo-structural constraints that mitigate row\naliasing. To maintain global consistency in headland regions where semantics\nare sparse, we introduce a noisy GPS prior that adaptively supports the filter.\nExperiments in a real vineyard demonstrate that our approach maintains\nlocalisation within the correct row, recovers from deviations where AMCL fails,\nand outperforms vision-based SLAM methods such as RTAB-Map.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4e86\u7a33\u5b9a\u7269\u4f53\u7ea7\u522b\u68c0\u6d4b\uff08\u8461\u8404\u85e4\u6811\u5e72\u548c\u652f\u6491\u6746\uff09\u7684\u8bed\u4e49\u7c92\u5b50\u6ee4\u6ce2\u5668\uff0c\u7528\u4e8e\u89e3\u51b3\u5728\u8461\u8404\u56ed\u7b49\u7ed3\u6784\u5316\u6237\u5916\u73af\u5883\u4e2d\uff0c\u57fa\u4e8e LiDAR \u7684\u65b9\u6cd5\u56e0\u91cd\u590d\u7684\u884c\u51e0\u4f55\u548c\u611f\u77e5\u6df7\u6dc6\u800c\u5bfc\u81f4\u7684\u5b9a\u4f4d\u5931\u8d25\u95ee\u9898\u3002", "motivation": "\u5728\u7ed3\u6784\u5316\u6237\u5916\u73af\u5883\u4e2d\uff0c\u79fb\u52a8\u673a\u5668\u4eba\u7684\u7cbe\u786e\u5b9a\u4f4d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u57fa\u4e8e LiDAR \u7684\u65b9\u6cd5\u5728\u8461\u8404\u56ed\u4e2d\u5e38\u56e0\u91cd\u590d\u7684\u884c\u51e0\u4f55\u548c\u611f\u77e5\u6df7\u6dc6\u800c\u5931\u8d25\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8bed\u4e49\u7c92\u5b50\u6ee4\u6ce2\u5668\uff0c\u5c06\u7a33\u5b9a\u7684\u7269\u4f53\u7ea7\u522b\u68c0\u6d4b\uff08\u8461\u8404\u85e4\u6811\u5e72\u548c\u652f\u6491\u6746\uff09\u7eb3\u5165\u4f3c\u7136\u4f30\u8ba1\u8fc7\u7a0b\u3002\u68c0\u6d4b\u5230\u7684\u5730\u6807\u88ab\u6295\u5f71\u5230\u9e1f\u77b0\u56fe\u5e76\u4e0e LiDAR \u626b\u63cf\u878d\u5408\uff0c\u751f\u6210\u8bed\u4e49\u89c2\u6d4b\u3002\u901a\u8fc7\u4f7f\u7528\u8fde\u63a5\u76f8\u90bb\u5730\u6807\u7684\u8bed\u4e49\u5899\u6765\u7f13\u89e3\u884c\u7684\u6df7\u6dc6\uff0c\u5e76\u5728\u8bed\u4e49\u7a00\u758f\u7684\u533a\u57df\u5f15\u5165\u5e26\u6709\u566a\u58f0\u7684 GPS \u5148\u9a8c\u4ee5\u4fdd\u6301\u5168\u5c40\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5c06\u5b9a\u4f4d\u4fdd\u6301\u5728\u6b63\u786e\u7684\u884c\u5185\uff0c\u5e76\u5728 AMCL \u5931\u8d25\u7684\u504f\u5dee\u4e2d\u6062\u590d\uff0c\u6027\u80fd\u4f18\u4e8e RTAB-Map \u7b49\u89c6\u89c9 SLAM \u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u8bed\u4e49\u7c92\u5b50\u6ee4\u6ce2\u5668\u80fd\u591f\u6709\u6548\u5730\u89e3\u51b3\u8461\u8404\u56ed\u7b49\u91cd\u590d\u51e0\u4f55\u73af\u5883\u4e2d\u7684\u5b9a\u4f4d\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u878d\u5408\u8bed\u4e49\u4fe1\u606f\u548c\u5229\u7528 GPS \u5148\u9a8c\u6765\u63d0\u9ad8\u5b9a\u4f4d\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2509.18174", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18174", "abs": "https://arxiv.org/abs/2509.18174", "authors": ["Khalil Hennara", "Muhammad Hreden", "Mohamed Motasim Hamed", "Ahmad Bastati", "Zeina Aldallal", "Sara Chrouf", "Safwan AlModhayan"], "title": "Baseer: A Vision-Language Model for Arabic Document-to-Markdown OCR", "comment": null, "summary": "Arabic document OCR remains a challenging task due to the language's cursive\nscript, diverse fonts, diacritics, and right-to-left orientation. While modern\nMultimodal Large Language Models (MLLMs) have advanced document understanding\nfor high-resource languages, their performance on Arabic remains limited. In\nthis work, we introduce Baseer, a vision-language model fine- tuned\nspecifically for Arabic document OCR. Leveraging a large-scale dataset\ncombining synthetic and real-world documents, Baseer is trained using a\ndecoder-only fine-tuning strategy to adapt a pre-trained MLLM while preserving\ngeneral visual features. We also present Misraj-DocOCR, a high-quality,\nexpert-verified benchmark designed for rigorous evaluation of Arabic OCR\nsystems. Our experiments show that Baseer significantly outperforms existing\nopen-source and commercial solutions, achieving a WER of 0.25 and establishing\na new state-of-the-art in the domain of Arabic document OCR. Our results\nhighlight the benefits of domain-specific adaptation of general-purpose MLLMs\nand establish a strong baseline for high-accuracy OCR on morphologically rich\nlanguages like Arabic.", "AI": {"tldr": "Baseer\u662f\u4e00\u4e2a\u9488\u5bf9\u963f\u62c9\u4f2f\u6587\u6863OCR\u4f18\u5316\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff0c\u5728Misraj-DocOCR\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u963f\u62c9\u4f2f\u6587\u6863OCR\u7531\u4e8e\u5176\u8fde\u5b57\u811a\u672c\u3001\u591a\u6837\u5316\u7684\u5b57\u4f53\u3001\u53d1\u97f3\u7b26\u53f7\u548c\u4ece\u53f3\u5230\u5de6\u7684\u4e66\u5199\u65b9\u5411\u800c\u5177\u6709\u6311\u6218\u6027\u3002\u73b0\u6709\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u5728\u963f\u62c9\u4f2f\u8bed\u4e0a\u7684\u8868\u73b0\u6709\u9650\u3002", "method": "\u5229\u7528\u5305\u542b\u5408\u6210\u548c\u771f\u5b9e\u6587\u6863\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u91c7\u7528\u4ec5\u89e3\u7801\u5668\u7684\u5fae\u8c03\u7b56\u7565\uff0c\u5bf9\u9884\u8bad\u7ec3\u7684MLLM\u8fdb\u884c\u5fae\u8c03\uff0c\u5e76\u63d0\u51fa\u4e86Misraj-DocOCR\u57fa\u51c6\u3002\u4f7f\u7528WER\u4f5c\u4e3a\u8bc4\u4f30\u6307\u6807\u3002", "result": "Baseer\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u5f00\u6e90\u548c\u5546\u4e1a\u89e3\u51b3\u65b9\u6848\uff0cWER\u4e3a0.25\uff0c\u5728\u963f\u62c9\u4f2f\u6587\u6863OCR\u9886\u57df\u521b\u4e0b\u65b0\u7eaa\u5f55\u3002", "conclusion": "\u9488\u5bf9\u7279\u5b9a\u9886\u57df\uff08\u5982\u963f\u62c9\u4f2f\u8bed\uff09\u7684\u901a\u7528MLLM\u8fdb\u884c\u9002\u5e94\u6027\u8c03\u6574\uff0c\u53ef\u4ee5\u4e3a\u5f62\u6001\u4e30\u5bcc\u7684\u8bed\u8a00\u5b9e\u73b0\u9ad8\u7cbe\u5ea6OCR\uff0c\u5e76\u4e3a\u8be5\u9886\u57df\u5efa\u7acb\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u57fa\u51c6\u3002"}}
{"id": "2509.18262", "categories": ["quant-ph", "cond-mat.dis-nn", "cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2509.18262", "abs": "https://arxiv.org/abs/2509.18262", "authors": ["Mario Boneberg", "Simon Kochsiek", "Gabriele Perfetto", "Igor Lesanovsky"], "title": "Training the classification capability of large-scale quantum cellular automata", "comment": "14 pages, 6 figures", "summary": "In the vicinity of a phase transition ergodicity can be broken. Here,\ndifferent initial many-body configurations evolve towards one of several fixed\npoints, which are macroscopically distinguishable through an order parameter.\nThis mechanism enables state classification in quantum cellular automata and\nfeed-forward quantum neural networks. We demonstrate that this capability can\nbe efficiently learned from training data even in extremely high-dimensional\nstate spaces. We illustrate this using a quantum cellular automaton that allows\nbinary classification, which is closely connected to the dynamics of a\n$\\mathbb{Z}_2$-symmetric Ising model with local interactions and dissipation.\nThis approach can be generalized beyond binary classification and offers a\nnatural framework for exploring the link between emergent many-body phenomena\nand the interpretation of data processing capabilities in the context of\nquantum machine learning.", "AI": {"tldr": "\u5728\u76f8\u53d8\u9644\u8fd1\uff0c\u52a8\u529b\u5b66\u53ef\u80fd\u4e0d\u518d\u662f\u904d\u5386\u6027\u7684\uff0c\u8fd9\u4f7f\u5f97\u7cfb\u7edf\u80fd\u591f\u88ab\u5f52\u7c7b\u5230\u51e0\u4e2a\u5b8f\u89c2\u4e0a\u53ef\u533a\u5206\u7684\u56fa\u5b9a\u70b9\u4e4b\u4e00\u3002\u8fd9\u79cd\u673a\u5236\u88ab\u5e94\u7528\u4e8e\u91cf\u5b50\u5143\u80de\u81ea\u52a8\u673a\u548c\u91cf\u5b50\u795e\u7ecf\u7f51\u7edc\uff0c\u7528\u4e8e\u72b6\u6001\u5206\u7c7b\u3002\u7814\u7a76\u8868\u660e\uff0c\u5373\u4f7f\u5728\u9ad8\u7ef4\u72b6\u6001\u7a7a\u95f4\u4e2d\uff0c\u4e5f\u53ef\u4ee5\u901a\u8fc7\u8bad\u7ec3\u6570\u636e\u6709\u6548\u5730\u5b66\u4e60\u8fd9\u79cd\u5206\u7c7b\u80fd\u529b\u3002\u6587\u7ae0\u4ee5\u4e00\u4e2a\u4e0eZ2\u5bf9\u79f0\u4f0a\u8f9b\u6a21\u578b\u76f8\u5173\u7684\u91cf\u5b50\u5143\u80de\u81ea\u52a8\u673a\u4e3a\u4f8b\uff0c\u8bf4\u660e\u4e86\u4e8c\u5143\u5206\u7c7b\u7684\u53ef\u80fd\u6027\uff0c\u5e76\u6307\u51fa\u8be5\u65b9\u6cd5\u53ef\u4ee5\u63a8\u5e7f\u5230\u66f4\u5e7f\u6cdb\u7684\u5206\u7c7b\u4efb\u52a1\uff0c\u4e3a\u7406\u89e3\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u6d8c\u73b0\u591a\u4f53\u73b0\u8c61\u548c\u6570\u636e\u5904\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002", "motivation": "Ergodicity breaking near a phase transition provides a mechanism for classifying states, which can be leveraged for quantum machine learning tasks like state classification in quantum cellular automata and feed-forward quantum neural networks.", "method": "The paper demonstrates that the state classification capability enabled by ergodicity breaking can be efficiently learned from training data, even in high-dimensional state spaces. This is illustrated using a quantum cellular automaton connected to the dynamics of a $\\mathbb{Z}_2$-symmetric Ising model with local interactions and dissipation.", "result": "The study shows that binary classification can be achieved using the described approach, which is linked to the dynamics of a $\\mathbb{Z}_2$-symmetric Ising model. The method is shown to be efficiently learnable from training data even in extremely high-dimensional state spaces.", "conclusion": "The proposed approach, leveraging ergodicity breaking near phase transitions, offers a viable and efficiently learnable method for state classification in quantum machine learning. It generalizes beyond binary classification and provides a framework for connecting emergent many-body phenomena with data processing capabilities in quantum systems."}}
{"id": "2509.18107", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18107", "abs": "https://arxiv.org/abs/2509.18107", "authors": ["Huanyao Zhang", "Jiaye Lin", "Wentao Zhang", "Haitao Yuan", "Guoliang Li"], "title": "AdaMixT: Adaptive Weighted Mixture of Multi-Scale Expert Transformers for Time Series Forecasting", "comment": null, "summary": "Multivariate time series forecasting involves predicting future values based\non historical observations. However, existing approaches primarily rely on\npredefined single-scale patches or lack effective mechanisms for multi-scale\nfeature fusion. These limitations hinder them from fully capturing the complex\npatterns inherent in time series, leading to constrained performance and\ninsufficient generalizability. To address these challenges, we propose a novel\narchitecture named Adaptive Weighted Mixture of Multi-Scale Expert Transformers\n(AdaMixT). Specifically, AdaMixT introduces various patches and leverages both\nGeneral Pre-trained Models (GPM) and Domain-specific Models (DSM) for\nmulti-scale feature extraction. To accommodate the heterogeneity of temporal\nfeatures, AdaMixT incorporates a gating network that dynamically allocates\nweights among different experts, enabling more accurate predictions through\nadaptive multi-scale fusion. Comprehensive experiments on eight widely used\nbenchmarks, including Weather, Traffic, Electricity, ILI, and four ETT\ndatasets, consistently demonstrate the effectiveness of AdaMixT in real-world\nscenarios.", "AI": {"tldr": "AdaMixT\u901a\u8fc7\u81ea\u9002\u5e94\u52a0\u6743\u878d\u5408\u591a\u5c3a\u5ea6\u4e13\u5bb6Transformer\u6765\u63d0\u5347\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u6355\u6349\u591a\u5c3a\u5ea6\u7279\u5f81\u548c\u878d\u5408\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u5bfc\u81f4\u6cdb\u5316\u80fd\u529b\u53d7\u9650\u3002", "method": "\u63d0\u51faAdaMixT\u67b6\u6784\uff0c\u5229\u7528\u4e0d\u540c\u5c3a\u5ea6\u7684patches\uff0c\u7ed3\u5408\u901a\u7528\u9884\u8bad\u7ec3\u6a21\u578b\uff08GPM\uff09\u548c\u9886\u57df\u7279\u5b9a\u6a21\u578b\uff08DSM\uff09\u8fdb\u884c\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\uff0c\u5e76\u901a\u8fc7\u95e8\u63a7\u7f51\u7edc\u52a8\u6001\u5206\u914d\u6743\u91cd\u4ee5\u5b9e\u73b0\u81ea\u9002\u5e94\u878d\u5408\u3002", "result": "\u5728\u516b\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\uff08\u5305\u62ecWeather, Traffic, Electricity, ILI\u548c\u56db\u4e2aETT\u6570\u636e\u96c6\uff09\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e86AdaMixT\u5728\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "AdaMixT\u901a\u8fc7\u81ea\u9002\u5e94\u591a\u5c3a\u5ea6\u878d\u5408\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u6311\u6218\u3002"}}
{"id": "2509.19246", "categories": ["cs.RO", "cs.MA", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.19246", "abs": "https://arxiv.org/abs/2509.19246", "authors": ["Sinan O\u011fuz", "Emanuele Garone", "Marco Dorigo", "Mary Katherine Heinrich"], "title": "Proactive-reactive detection and mitigation of intermittent faults in robot swarms", "comment": null, "summary": "Intermittent faults are transient errors that sporadically appear and\ndisappear. Although intermittent faults pose substantial challenges to\nreliability and coordination, existing studies of fault tolerance in robot\nswarms focus instead on permanent faults. One reason for this is that\nintermittent faults are prohibitively difficult to detect in the fully\nself-organized ad-hoc networks typical of robot swarms, as their network\ntopologies are transient and often unpredictable. However, in the recently\nintroduced self-organizing nervous systems (SoNS) approach, robot swarms are\nable to self-organize persistent network structures for the first time, easing\nthe problem of detecting intermittent faults. To address intermittent faults in\nrobot swarms that have persistent networks, we propose a novel\nproactive-reactive strategy to detection and mitigation, based on\nself-organized backup layers and distributed consensus in a multiplex network.\nProactively, the robots self-organize dynamic backup paths before faults occur,\nadapting to changes in the primary network topology and the robots' relative\npositions. Reactively, robots use one-shot likelihood ratio tests to compare\ninformation received along different paths in the multiplex network, enabling\nearly fault detection. Upon detection, communication is temporarily rerouted in\na self-organized way, until the detected fault resolves. We validate the\napproach in representative scenarios of faulty positional data occurring during\nformation control, demonstrating that intermittent faults are prevented from\ndisrupting convergence to desired formations, with high fault detection\naccuracy and low rates of false positives.", "AI": {"tldr": "\u73b0\u6709\u7684\u673a\u5668\u4eba\u7fa4\u53ef\u9760\u6027\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u6c38\u4e45\u6027\u6545\u969c\uff0c\u800c\u5ffd\u7565\u4e86\u95f4\u6b47\u6027\u6545\u969c\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u65b0\u63d0\u51fa\u7684\u81ea\u7ec4\u7ec7\u795e\u7ecf\u7cfb\u7edf\uff08SoNS\uff09\u65b9\u6cd5\u57fa\u7840\u4e0a\uff0c\u9488\u5bf9\u673a\u5668\u4eba\u7fa4\u95f4\u6b47\u6027\u6545\u969c\u7684\u4e3b\u52a8-\u53cd\u5e94\u7b56\u7565\uff0c\u5229\u7528\u81ea\u7ec4\u7ec7\u5907\u4efd\u5c42\u548c\u591a\u5c42\u7f51\u7edc\u4e2d\u7684\u5206\u5e03\u5f0f\u5171\u8bc6\u6765\u68c0\u6d4b\u548c\u7f13\u89e3\u95f4\u6b47\u6027\u6545\u969c\u3002", "motivation": "\u73b0\u6709\u7684\u673a\u5668\u4eba\u7fa4\u53ef\u9760\u6027\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u6c38\u4e45\u6027\u6545\u969c\uff0c\u4f46\u5ffd\u7565\u4e86\u5728\u673a\u5668\u4eba\u7fa4\u4e2d\u666e\u904d\u5b58\u5728\u4e14\u96be\u4ee5\u68c0\u6d4b\u7684\u95f4\u6b47\u6027\u6545\u969c\u3002SoNS\u65b9\u6cd5\u7684\u51fa\u73b0\u4e3a\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e3b\u52a8-\u53cd\u5e94\u7b56\u7565\uff0c\u5305\u62ec\uff1a1. \u4e3b\u52a8\u6784\u5efa\u52a8\u6001\u5907\u4efd\u8def\u5f84\u4ee5\u5e94\u5bf9\u7f51\u7edc\u62d3\u6251\u548c\u673a\u5668\u4eba\u4f4d\u7f6e\u7684\u53d8\u5316\u30022. \u53cd\u5e94\u5f0f\u5730\u4f7f\u7528\u5355\u6b21\u4f3c\u7136\u6bd4\u68c0\u9a8c\u6765\u68c0\u6d4b\u6545\u969c\u30023. \u6545\u969c\u68c0\u6d4b\u540e\uff0c\u901a\u4fe1\u88ab\u6682\u65f6\u91cd\u8def\u7531\u30024. \u5229\u7528\u81ea\u7ec4\u7ec7\u5907\u4efd\u5c42\u548c\u591a\u5c42\u7f51\u7edc\u4e2d\u7684\u5206\u5e03\u5f0f\u5171\u8bc6\u3002", "result": "\u5728\u6709\u4ee3\u8868\u6027\u7684\u6545\u969c\u4f4d\u7f6e\u6570\u636e\u573a\u666f\u4e0b\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u673a\u5668\u4eba\u7f16\u961f\u63a7\u5236\u4e2d\u7684\u6709\u6548\u6027\u3002\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u9632\u6b62\u95f4\u6b47\u6027\u6545\u969c\u5e72\u6270\u7f16\u961f\u6536\u655b\uff0c\u5e76\u4e14\u5177\u6709\u9ad8\u6545\u969c\u68c0\u6d4b\u51c6\u786e\u7387\u548c\u4f4e\u8bef\u62a5\u7387\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u4e3b\u52a8-\u53cd\u5e94\u7b56\u7565\u80fd\u591f\u6709\u6548\u5730\u68c0\u6d4b\u548c\u7f13\u89e3\u673a\u5668\u4eba\u7fa4\u4e2d\u7684\u95f4\u6b47\u6027\u6545\u969c\uff0c\u786e\u4fdd\u4e86\u7f16\u961f\u63a7\u5236\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2509.18178", "categories": ["cs.AI", "cs.CE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18178", "abs": "https://arxiv.org/abs/2509.18178", "authors": ["Ling Yue", "Nithin Somasekharan", "Tingwen Zhang", "Yadi Cao", "Shaowu Pan"], "title": "Foam-Agent: An End-to-End Composable Multi-Agent Framework for Automating CFD Simulation in OpenFOAM", "comment": null, "summary": "Computational Fluid Dynamics (CFD) is an essential simulation tool in\nengineering, yet its steep learning curve and complex manual setup create\nsignificant barriers. To address these challenges, we introduce Foam-Agent, a\nmulti-agent framework that automates the entire end-to-end OpenFOAM workflow\nfrom a single natural language prompt. Our key innovations address critical\ngaps in existing systems: 1. An Comprehensive End-to-End Simulation Automation:\nFoam-Agent is the first system to manage the full simulation pipeline,\nincluding advanced pre-processing with a versatile Meshing Agent capable of\nhandling external mesh files and generating new geometries via Gmsh, automatic\ngeneration of HPC submission scripts, and post-simulation visualization via\nParaView. 2. Composable Service Architecture: Going beyond a monolithic agent,\nthe framework uses Model Context Protocol (MCP) to expose its core functions as\ndiscrete, callable tools. This allows for flexible integration and use by other\nagentic systems, such as Claude-code, for more exploratory workflows. 3.\nHigh-Fidelity Configuration Generation: We achieve superior accuracy through a\nHierarchical Multi-Index RAG for precise context retrieval and a\ndependency-aware generation process that ensures configuration consistency.\nEvaluated on a benchmark of 110 simulation tasks, Foam-Agent achieves an 88.2%\nsuccess rate with Claude 3.5 Sonnet, significantly outperforming existing\nframeworks (55.5% for MetaOpenFOAM). Foam-Agent dramatically lowers the\nexpertise barrier for CFD, demonstrating how specialized multi-agent systems\ncan democratize complex scientific computing. The code is public at\nhttps://github.com/csml-rpi/Foam-Agent.", "AI": {"tldr": "Foam-Agent \u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u53ef\u4ee5\u6839\u636e\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u81ea\u52a8\u5b8c\u6210\u6574\u4e2a OpenFOAM \u5de5\u4f5c\u6d41\uff0c\u89e3\u51b3\u4e86 CFD \u5b66\u4e60\u66f2\u7ebf\u9661\u5ced\u548c\u8bbe\u7f6e\u590d\u6742\u7684\u95ee\u9898\u3002", "motivation": "CFD \u5b66\u4e60\u66f2\u7ebf\u9661\u5ced\u4e14\u8bbe\u7f6e\u590d\u6742\uff0cFoam-Agent \u65e8\u5728\u81ea\u52a8\u5316\u6574\u4e2a OpenFOAM \u5de5\u4f5c\u6d41\uff0c\u964d\u4f4e CFD \u7684\u5165\u95e8\u95e8\u69db\u3002", "method": "Foam-Agent \u91c7\u7528\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u4e00\u4e2a\u7efc\u5408\u6027\u7684\u7aef\u5230\u7aef\u6a21\u62df\u81ea\u52a8\u5316\u6d41\u7a0b\uff0c\u5305\u62ec\u9884\u5904\u7406\uff08\u7f51\u683c\u751f\u6210\uff09\u3001HPC \u811a\u672c\u751f\u6210\u548c\u540e\u5904\u7406\uff08\u53ef\u89c6\u5316\uff09\u3002\u5b83\u4f7f\u7528\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae\uff08MCP\uff09\u5c06\u6838\u5fc3\u529f\u80fd\u66b4\u9732\u4e3a\u53ef\u8c03\u7528\u7684\u5de5\u5177\uff0c\u5e76\u5229\u7528\u5206\u5c42\u591a\u7d22\u5f15 RAG \u548c\u4f9d\u8d56\u611f\u77e5\u751f\u6210\u8fc7\u7a0b\u6765\u63d0\u9ad8\u914d\u7f6e\u7684\u51c6\u786e\u6027\u3002", "result": "\u5728 110 \u4e2a\u6a21\u62df\u4efb\u52a1\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFoam-Agent \u4f7f\u7528 Claude 3.5 Sonnet \u53d6\u5f97\u4e86 88.2% \u7684\u6210\u529f\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6846\u67b6\uff08MetaOpenFOAM \u6210\u529f\u7387\u4e3a 55.5%\uff09\u3002", "conclusion": "Foam-Agent \u6781\u5927\u5730\u964d\u4f4e\u4e86 CFD \u7684\u4e13\u4e1a\u77e5\u8bc6\u95e8\u69db\uff0c\u5c55\u793a\u4e86\u4e13\u4e1a\u5316\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5982\u4f55\u80fd\u591f\u666e\u53ca\u590d\u6742\u79d1\u5b66\u8ba1\u7b97\u3002"}}
{"id": "2509.18163", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18163", "abs": "https://arxiv.org/abs/2509.18163", "authors": ["Haodong Zhao", "Chenyan Zhao", "Yansi Li", "Zhuosheng Zhang", "Gongshen Liu"], "title": "Thinking in a Crowd: How Auxiliary Information Shapes LLM Reasoning", "comment": "Work in progress", "summary": "The capacity of Large Language Models (LLMs) to reason is fundamental to\ntheir application in complex, knowledge-intensive domains. In real-world\nscenarios, LLMs are often augmented with external information that can be\nhelpful, irrelevant, or even misleading. This paper investigates the causal\nimpact of such auxiliary information on the reasoning process of LLMs with\nexplicit step-by-step thinking capabilities. We introduce SciAux, a new dataset\nderived from ScienceQA, to systematically test the robustness of the model\nagainst these types of information. Our findings reveal a critical\nvulnerability: the model's deliberative \"thinking mode\" is a double-edged\nsword. While helpful context improves accuracy, misleading information causes a\ncatastrophic drop in performance, which is amplified by the thinking process.\nInstead of conferring robustness, thinking reinforces the degree of error when\nprovided with misinformation. This highlights that the challenge is not merely\nto make models \"think\", but to endow them with the critical faculty to evaluate\nthe information upon which their reasoning is based. The SciAux dataset is\navailable at https://huggingface.co/datasets/billhdzhao/SciAux.", "AI": {"tldr": "LLM\u5728\u6709\u5916\u90e8\u4fe1\u606f\u8f85\u52a9\u65f6\uff0c\u5176", "motivation": "LLM\u5728\u6709\u5916\u90e8\u4fe1\u606f\u8f85\u52a9\u65f6\uff0c\u5176\u63a8\u7406\u80fd\u529b\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5916\u90e8\u4fe1\u606f\u53ef\u80fd\u662f\u6709\u5bb3\u7684\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7a76\u8fd9\u79cd\u4fe1\u606f\u5bf9LLM\u63a8\u7406\u8fc7\u7a0b\u7684\u56e0\u679c\u5f71\u54cd\u3002", "method": "\u521b\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aSciAux\u7684\u65b0\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u6e90\u81eaScienceQA\uff0c\u7528\u4e8e\u7cfb\u7edf\u5730\u6d4b\u8bd5\u6a21\u578b\u5728\u9762\u5bf9\u6709\u76ca\u3001\u65e0\u5173\u6216\u8bef\u5bfc\u6027\u4fe1\u606f\u65f6\u7684\u9c81\u68d2\u6027\u3002", "result": "\u6a21\u578b\u7684\u201c\u601d\u8003\u6a21\u5f0f\u201d\u662f\u4e00\u628a\u53cc\u5203\u5251\uff1a\u6709\u76ca\u4fe1\u606f\u80fd\u63d0\u9ad8\u51c6\u786e\u6027\uff0c\u4f46\u8bef\u5bfc\u4fe1\u606f\u4f1a\u5bfc\u81f4\u6027\u80fd\u707e\u96be\u6027\u4e0b\u964d\uff0c\u4e14\u8be5\u6548\u5e94\u88ab\u601d\u8003\u8fc7\u7a0b\u653e\u5927\u3002\u6a21\u578b\u5e76\u672a\u56e0\u6b64\u83b7\u5f97\u9c81\u68d2\u6027\uff0c\u53cd\u800c\u53ef\u80fd\u56e0\u601d\u8003\u800c\u52a0\u5267\u9519\u8bef\u3002", "conclusion": "\u5173\u952e\u6311\u6218\u5728\u4e8e\u4e0d\u4ec5\u8981\u8ba9\u6a21\u578b\u201c\u601d\u8003\u201d\uff0c\u8fd8\u8981\u4f7f\u5176\u5177\u5907\u8bc4\u4f30\u63a8\u7406\u6240\u4f9d\u636e\u4fe1\u606f\u771f\u4f2a\u7684\u80fd\u529b\u3002"}}
{"id": "2509.19296", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2509.19296", "abs": "https://arxiv.org/abs/2509.19296", "authors": ["Sherwin Bahmani", "Tianchang Shen", "Jiawei Ren", "Jiahui Huang", "Yifeng Jiang", "Haithem Turki", "Andrea Tagliasacchi", "David B. Lindell", "Zan Gojcic", "Sanja Fidler", "Huan Ling", "Jun Gao", "Xuanchi Ren"], "title": "Lyra: Generative 3D Scene Reconstruction via Video Diffusion Model Self-Distillation", "comment": "Project Page: https://research.nvidia.com/labs/toronto-ai/lyra/", "summary": "The ability to generate virtual environments is crucial for applications\nranging from gaming to physical AI domains such as robotics, autonomous\ndriving, and industrial AI. Current learning-based 3D reconstruction methods\nrely on the availability of captured real-world multi-view data, which is not\nalways readily available. Recent advancements in video diffusion models have\nshown remarkable imagination capabilities, yet their 2D nature limits the\napplications to simulation where a robot needs to navigate and interact with\nthe environment. In this paper, we propose a self-distillation framework that\naims to distill the implicit 3D knowledge in the video diffusion models into an\nexplicit 3D Gaussian Splatting (3DGS) representation, eliminating the need for\nmulti-view training data. Specifically, we augment the typical RGB decoder with\na 3DGS decoder, which is supervised by the output of the RGB decoder. In this\napproach, the 3DGS decoder can be purely trained with synthetic data generated\nby video diffusion models. At inference time, our model can synthesize 3D\nscenes from either a text prompt or a single image for real-time rendering. Our\nframework further extends to dynamic 3D scene generation from a monocular input\nvideo. Experimental results show that our framework achieves state-of-the-art\nperformance in static and dynamic 3D scene generation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u84b8\u998f\u6846\u67b6\uff0c\u5229\u7528\u89c6\u9891\u6269\u6563\u6a21\u578b\u751f\u62103D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u8868\u793a\uff0c\u65e0\u9700\u591a\u89c6\u56fe\u8bad\u7ec3\u6570\u636e\uff0c\u53ef\u4ece\u6587\u672c\u6216\u5355\u56fe\u50cf\u751f\u6210\u9759\u6001\u548c\u52a8\u60013D\u573a\u666f\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5b66\u4e60\u76843D\u91cd\u5efa\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u771f\u5b9e\u4e16\u754c\u7684\u591a\u89c6\u56fe\u6570\u636e\uff0c\u800c\u89c6\u9891\u6269\u6563\u6a21\u578b\u867d\u6709\u5f3a\u5927\u7684\u751f\u6210\u80fd\u529b\u4f46\u4ec5\u9650\u4e8e2D\uff0c\u9650\u5236\u4e86\u5176\u5728\u9700\u8981\u5bfc\u822a\u548c\u4ea4\u4e92\u7684AI\u9886\u57df\uff08\u5982\u673a\u5668\u4eba\u3001\u81ea\u52a8\u9a7e\u9a76\uff09\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u81ea\u84b8\u998f\u6846\u67b6\uff0c\u5c06\u89c6\u9891\u6269\u6563\u6a21\u578b\u4e2d\u7684\u9690\u5f0f3D\u77e5\u8bc6\u84b8\u998f\u5230\u663e\u5f0f3D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u8868\u793a\u4e2d\u3002\u8be5\u6846\u67b6\u5728\u5178\u578b\u7684RGB\u89e3\u7801\u5668\u57fa\u7840\u4e0a\u589e\u52a0\u4e86\u4e00\u4e2a3DGS\u89e3\u7801\u5668\uff0c\u5e76\u7531RGB\u89e3\u7801\u5668\u7684\u8f93\u51fa\u6765\u76d1\u7763\uff0c\u4ece\u800c\u53ef\u4ee5\u4f7f\u7528\u89c6\u9891\u6269\u6563\u6a21\u578b\u751f\u6210\u7684\u5408\u6210\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u9759\u6001\u548c\u52a8\u60013D\u573a\u666f\u751f\u6210\u65b9\u9762\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u80fd\u591f\u4ece\u6587\u672c\u63d0\u793a\u6216\u5355\u5f20\u56fe\u50cf\u5b9e\u65f6\u6e32\u67d33D\u573a\u666f\uff0c\u5e76\u80fd\u4ece\u5355\u76ee\u89c6\u9891\u751f\u6210\u52a8\u60013D\u573a\u666f\u3002", "conclusion": "\u8be5\u81ea\u84b8\u998f\u6846\u67b6\u6210\u529f\u5730\u5c06\u89c6\u9891\u6269\u6563\u6a21\u578b\u76843D\u77e5\u8bc6\u63d0\u53d6\u52303DGS\u8868\u793a\u4e2d\uff0c\u89e3\u51b3\u4e86\u591a\u89c6\u56fe\u6570\u636e\u4f9d\u8d56\u95ee\u9898\uff0c\u5e76\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u9759\u6001\u548c\u52a8\u60013D\u573a\u666f\u751f\u6210\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2509.19086", "categories": ["cs.DC", "D.4.1; C.4; C.1.4; D.1.3"], "pdf": "https://arxiv.org/pdf/2509.19086", "abs": "https://arxiv.org/abs/2509.19086", "authors": ["Michal Konopa", "Jan Fesl", "Ladislav Ber\u00e1nek"], "title": "Scheduler-Driven Job Atomization", "comment": "22 pages", "summary": "Modern GPU clusters, particularly those built on NVIDIA's Multi-Instance GPU\n(MIG) architecture, often suffer from inefficiencies because jobs are treated\nas rigid, indivisible blocks that occupy a fixed slice until completion. The\nreliance on static peak memory estimates exacerbates fragmentation,\nunderutilization, and job rejections. We propose Scheduler-Driven Job\nAtomization (SJA), a new paradigm that establishes a bidirectional interaction\nbetween scheduler and jobs. In SJA, the scheduler advertises available\nexecution gaps, and jobs respond by signaling interest if they can potentially\ngenerate a subjob that fits the offered time-capacity window. The scheduler may\ncollect multiple signals for the same slot and, based on its allocation policy\n(e.g., fairness, efficiency, or SLA priorities), selects which job is granted\nthe slot. Only then does the chosen job materialize a safe, self-contained\nsubjob tailored to that opportunity. Unlike migration or preemption, SJA\nproactively shapes workloads before execution, thereby avoiding costly state\ntransfers and unpredictable interruptions. It aims to increase GPU utilization,\nreduce wait times, and minimize migration overhead by aligning jobs with\nopportunities in real time, ensuring that each admitted subjob is correct by\nconstruction. This paper is presented as a concept paper: it introduces the\nparadigm, defines its building blocks, and outlines future research directions,\nrather than offering a full experimental evaluation.", "AI": {"tldr": "SJA\u662f\u4e00\u79cd\u65b0\u7684GPU\u96c6\u7fa4\u8c03\u5ea6\u8303\u5f0f\uff0c\u901a\u8fc7\u5c06\u4f5c\u4e1a\u5206\u89e3\u4e3a\u53ef\u53d8\u5927\u5c0f\u7684\u5b50\u4f5c\u4e1a\u6765\u63d0\u9ad8GPU\u5229\u7528\u7387\u3002", "motivation": "\u73b0\u6709\u7684GPU\u96c6\u7fa4\u8c03\u5ea6\u65b9\u6cd5\u7531\u4e8e\u5c06\u4f5c\u4e1a\u89c6\u4e3a\u56fa\u5b9a\u5927\u5c0f\u7684\u5757\u800c\u5bfc\u81f4\u788e\u7247\u5316\u3001\u5229\u7528\u7387\u4f4e\u4e0b\u548c\u4f5c\u4e1a\u62d2\u7edd\u3002", "method": "SJA\u901a\u8fc7\u8c03\u5ea6\u5668\u5ba3\u544a\u53ef\u7528\u6267\u884c\u7a7a\u95f4\uff0c\u4f5c\u4e1a\u54cd\u5e94\u5e76\u751f\u6210\u9002\u5408\u8be5\u7a7a\u95f4\u7684\u5b50\u4f5c\u4e1a\uff0c\u4ece\u800c\u5b9e\u73b0\u8c03\u5ea6\u5668\u548c\u4f5c\u4e1a\u4e4b\u95f4\u7684\u53cc\u5411\u4ea4\u4e92\u3002", "result": "SJA\u65e8\u5728\u63d0\u9ad8GPU\u5229\u7528\u7387\uff0c\u51cf\u5c11\u7b49\u5f85\u65f6\u95f4\uff0c\u5e76\u6700\u5c0f\u5316\u8fc1\u79fb\u5f00\u9500\uff0c\u4f46\u672c\u6587\u4ec5\u63d0\u51fa\u6982\u5ff5\uff0c\u672a\u8fdb\u884c\u5b9e\u9a8c\u8bc4\u4f30\u3002", "conclusion": "SJA\u662f\u4e00\u79cd\u6709\u524d\u666f\u7684GPU\u96c6\u7fa4\u8c03\u5ea6\u65b9\u6cd5\uff0c\u4f46\u9700\u8981\u8fdb\u4e00\u6b65\u7684\u7814\u7a76\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u3002"}}
{"id": "2509.18526", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.18526", "abs": "https://arxiv.org/abs/2509.18526", "authors": ["Han Zeng", "Haibo Wang", "Luhao Fan", "Bingcheng Zhu", "Xiaohu You", "Zaichen Zhang"], "title": "AI Agent Access (A\\^3) Network: An Embodied, Communication-Aware Multi-Agent Framework for 6G Coverage", "comment": null, "summary": "The vision of 6G communication demands autonomous and resilient networking in\nenvironments without fixed infrastructure. Yet most multi-agent reinforcement\nlearning (MARL) approaches focus on isolated stages - exploration, relay\nformation, or access - under static deployments and centralized control,\nlimiting adaptability. We propose the AI Agent Access (A\\^3) Network, a\nunified, embodied intelligence-driven framework that transforms multi-agent\nnetworking into a dynamic, decentralized, and end-to-end system. Unlike prior\nschemes, the A\\^3 Network integrates exploration, target user access, and\nbackhaul maintenance within a single learning process, while supporting\non-demand agent addition during runtime. Its decentralized policies ensure that\neven a single agent can operate independently with limited observations, while\ncoordinated agents achieve scalable, communication-optimized coverage. By\nembedding link-level communication metrics into actor-critic learning, the A\\^3\nNetwork couples topology formation with robust decision-making. Numerical\nsimulations demonstrate that the A\\^3 Network not only balances exploration and\ncommunication efficiency but also delivers system-level adaptability absent in\nexisting MARL frameworks, offering a new paradigm for 6G multi-agent networks.", "AI": {"tldr": "6G\u7f51\u7edc\u9700\u8981\u81ea\u4e3b\u548c\u6709\u5f39\u6027\u7684\u7f51\u7edc\uff0c\u4f46\u73b0\u6709\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u65b9\u6cd5\u5728\u9759\u6001\u90e8\u7f72\u548c\u96c6\u4e2d\u63a7\u5236\u4e0b\uff0c\u5728\u5b64\u7acb\u7684\u9636\u6bb5\uff08\u5982\u63a2\u7d22\u3001\u4e2d\u7ee7\u5f62\u6210\u6216\u63a5\u5165\uff09\u8fdb\u884c\u4f18\u5316\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u9002\u5e94\u6027\u3002\u6211\u4eec\u63d0\u51fa\u4e86AI Agent Access (A^3) Network\uff0c\u4e00\u4e2a\u7edf\u4e00\u7684\u3001\u7531\u5177\u8eab\u667a\u80fd\u9a71\u52a8\u7684\u6846\u67b6\uff0c\u5c06\u591a\u667a\u80fd\u4f53\u7f51\u7edc\u8f6c\u53d8\u4e3a\u4e00\u4e2a\u52a8\u6001\u7684\u3001\u53bb\u4e2d\u5fc3\u5316\u7684\u3001\u7aef\u5230\u7aef\u7684\u7cfb\u7edf\u3002\u4e0e\u4ee5\u5f80\u7684\u65b9\u6cd5\u4e0d\u540c\uff0cA^3 Network\u5c06\u63a2\u7d22\u3001\u76ee\u6807\u7528\u6237\u63a5\u5165\u548c\u56de\u4f20\u7ef4\u62a4\u6574\u5408\u5230\u5355\u4e00\u5b66\u4e60\u8fc7\u7a0b\u4e2d\uff0c\u5e76\u652f\u6301\u8fd0\u884c\u65f6\u6309\u9700\u6dfb\u52a0\u4ee3\u7406\u3002\u5176\u53bb\u4e2d\u5fc3\u5316\u7684\u7b56\u7565\u786e\u4fdd\u4e86\u5373\u4f7f\u5355\u4e2a\u4ee3\u7406\u4e5f\u80fd\u5728\u6709\u9650\u7684\u89c2\u6d4b\u4e0b\u72ec\u7acb\u8fd0\u884c\uff0c\u800c\u534f\u8c03\u7684\u4ee3\u7406\u5219\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u3001\u901a\u4fe1\u4f18\u5316\u7684\u8986\u76d6\u3002\u901a\u8fc7\u5c06\u94fe\u8def\u7ea7\u901a\u4fe1\u5ea6\u91cf\u5d4c\u5165Actor-Critic\u5b66\u4e60\u4e2d\uff0cA^3 Network\u5c06\u62d3\u6251\u5f62\u6210\u4e0e\u9c81\u68d2\u7684\u51b3\u7b56\u76f8\u7ed3\u5408\u3002\u6570\u503c\u6a21\u62df\u8868\u660e\uff0cA^3 Network\u4e0d\u4ec5\u80fd\u5e73\u8861\u63a2\u7d22\u548c\u901a\u4fe1\u6548\u7387\uff0c\u8fd8\u80fd\u63d0\u4f9b\u73b0\u6709MARL\u6846\u67b6\u6240\u7f3a\u4e4f\u7684\u7cfb\u7edf\u7ea7\u9002\u5e94\u6027\uff0c\u4e3a6G\u591a\u667a\u80fd\u4f53\u7f51\u7edc\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u8303\u4f8b\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u65b9\u6cd5\u5728\u5904\u74066G\u901a\u4fe1\u6240\u9700\u7684\u81ea\u4e3b\u548c\u5f39\u6027\u7f51\u7edc\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u56e0\u4e3a\u5b83\u4eec\u901a\u5e38\u4e13\u6ce8\u4e8e\u5b64\u7acb\u7684\u7f51\u7edc\u9636\u6bb5\uff0c\u5e76\u5728\u9759\u6001\u90e8\u7f72\u548c\u4e2d\u5fc3\u5316\u63a7\u5236\u4e0b\u8fdb\u884c\u4f18\u5316\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u9002\u5e94\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAI Agent Access (A^3) Network\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u5177\u8eab\u667a\u80fd\uff0c\u5c06\u63a2\u7d22\u3001\u76ee\u6807\u7528\u6237\u63a5\u5165\u548c\u56de\u4f20\u7ef4\u62a4\u6574\u5408\u5230\u4e00\u4e2a\u7edf\u4e00\u7684\u5b66\u4e60\u8fc7\u7a0b\u4e2d\uff0c\u5e76\u652f\u6301\u8fd0\u884c\u65f6\u52a8\u6001\u6dfb\u52a0\u4ee3\u7406\u3002\u8be5\u6846\u67b6\u91c7\u7528\u53bb\u4e2d\u5fc3\u5316\u7b56\u7565\uff0c\u5e76\u5c06\u94fe\u8def\u7ea7\u901a\u4fe1\u5ea6\u91cf\u5d4c\u5165Actor-Critic\u5b66\u4e60\u4e2d\uff0c\u4ee5\u5b9e\u73b0\u62d3\u6251\u5f62\u6210\u4e0e\u51b3\u7b56\u7684\u7ed3\u5408\u3002", "result": "\u6570\u503c\u6a21\u62df\u7ed3\u679c\u8868\u660e\uff0cA^3 Network\u5728\u5e73\u8861\u63a2\u7d22\u4e0e\u901a\u4fe1\u6548\u7387\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u5e76\u5c55\u73b0\u4e86\u73b0\u6709MARL\u6846\u67b6\u6240\u7f3a\u4e4f\u7684\u7cfb\u7edf\u7ea7\u9002\u5e94\u6027\u3002", "conclusion": "A^3 Network\u4e3a6G\u591a\u667a\u80fd\u4f53\u7f51\u7edc\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u7684\u8303\u4f8b\uff0c\u5176\u7279\u70b9\u662f\u7edf\u4e00\u7684\u5b66\u4e60\u8fc7\u7a0b\u3001\u53bb\u4e2d\u5fc3\u5316\u7b56\u7565\u548c\u5f3a\u5927\u7684\u9002\u5e94\u6027\uff0c\u80fd\u591f\u6ee1\u8db36G\u901a\u4fe1\u5bf9\u81ea\u4e3b\u548c\u5f39\u6027\u7f51\u7edc\u7684\u9700\u6c42\u3002"}}
{"id": "2509.19241", "categories": ["quant-ph", "cs.ET"], "pdf": "https://arxiv.org/pdf/2509.19241", "abs": "https://arxiv.org/abs/2509.19241", "authors": ["Jeremie Pope", "Swaroop Ghosh"], "title": "Not All Qubits are Utilized Equally", "comment": null, "summary": "Improvements to the functionality of modern Noisy Intermediate-Scale Quantum\n(NISQ) computers have coincided with an increase in the total number of\nphysical qubits. Quantum programmers do not commonly design circuits that\ndirectly utilize these qubits; instead, they rely on various software suites to\nalgorithmically transpile the circuit into one compatible with a target\nmachine's architecture. For connectivity-constrained superconducting\narchitectures in particular, the chosen syntheses, layout, and routing\nalgorithms used to transpile a circuit drastically change the average\nutilization patterns of physical qubits. In this paper, we analyze average\nqubit utilization of a quantum hardware as a means to identify how various\ntranspiler configurations change utilization patterns. We present the\npreliminary results of this analysis using IBM's 27-qubit Falcon R4\narchitecture on the Qiskit platform for a subset of qubits, gate distributions,\nand optimization configurations. We found a persistent bias towards trivial\nmapping, which can be addressed through increased optimization provided that\nthe overall utilization of an architecture remains below a certain threshold.\nAs a result, some qubits are overused whereas other remain underused. The\nimplication of our study are many-fold namely, (a) potential reduction in\ncalibration overhead by focusing on overused qubits, (b) refining optimization,\nmapping and routing algorithms to maximize the hardware utilization and (c)\npricing underused qubits at low rate to motivate their usage and improve\nhardware throughput (applicable in multi-tenant environments).", "AI": {"tldr": "\u8be5\u8bba\u6587\u5206\u6790\u4e86\u91cf\u5b50\u8ba1\u7b97\u673a\u786c\u4ef6\u7684\u5e73\u5747\u91cf\u5b50\u6bd4\u7279\u5229\u7528\u7387\uff0c\u4ee5\u8bc6\u522b\u5404\u79cd\u7f16\u8bd1\u5668\u914d\u7f6e\u5982\u4f55\u6539\u53d8\u5229\u7528\u7387\u6a21\u5f0f\uff0c\u5e76\u63d0\u51fa\u4e86\u4f18\u5316\u548c\u5b9a\u4ef7\u7b56\u7565\u3002", "motivation": "\u968f\u7740\u91cf\u5b50\u8ba1\u7b97\u673a\u529f\u80fd\u7684\u63d0\u9ad8\u548c\u7269\u7406\u91cf\u5b50\u6bd4\u7279\u6570\u91cf\u7684\u589e\u52a0\uff0c\u91cf\u5b50\u7a0b\u5e8f\u5458\u901a\u5e38\u4f9d\u8d56\u8f6f\u4ef6\u5957\u4ef6\u5c06\u7535\u8def\u8f6c\u8bd1\u4e3a\u76ee\u6807\u673a\u5668\u67b6\u6784\u517c\u5bb9\u7684\u7535\u8def\u3002\u5bf9\u4e8e\u5177\u6709\u8fde\u63a5\u6027\u9650\u5236\u7684\u8d85\u5bfc\u67b6\u6784\uff0c\u6240\u9009\u7684\u7efc\u5408\u3001\u5e03\u5c40\u548c\u5e03\u7ebf\u7b97\u6cd5\u4f1a\u6781\u5927\u5730\u6539\u53d8\u7269\u7406\u91cf\u5b50\u6bd4\u7279\u7684\u5e73\u5747\u5229\u7528\u6a21\u5f0f\u3002\u56e0\u6b64\uff0c\u6709\u5fc5\u8981\u5206\u6790\u5e73\u5747\u91cf\u5b50\u6bd4\u7279\u5229\u7528\u7387\uff0c\u4ee5\u4e86\u89e3\u5404\u79cd\u7f16\u8bd1\u5668\u914d\u7f6e\u5982\u4f55\u6539\u53d8\u5229\u7528\u7387\u6a21\u5f0f\u3002", "method": "\u901a\u8fc7\u5206\u6790\u91cf\u5b50\u786c\u4ef6\u7684\u5e73\u5747\u91cf\u5b50\u6bd4\u7279\u5229\u7528\u7387\uff0c\u7814\u7a76\u5404\u79cd\u7f16\u8bd1\u5668\u914d\u7f6e\u5982\u4f55\u6539\u53d8\u5229\u7528\u7387\u6a21\u5f0f\u3002\u4f7f\u7528IBM\u768427\u91cf\u5b50\u6bd4\u7279Falcon R4\u67b6\u6784\u548cQiskit\u5e73\u53f0\uff0c\u5bf9\u4e00\u90e8\u5206\u91cf\u5b50\u6bd4\u7279\u3001\u95e8\u5206\u5e03\u548c\u4f18\u5316\u914d\u7f6e\u8fdb\u884c\u4e86\u521d\u6b65\u5206\u6790\u3002", "result": "\u5206\u6790\u663e\u793a\uff0c\u5b58\u5728\u4e00\u79cd\u6301\u7eed\u5b58\u5728\u7684\u503e\u5411\u4e8e\u5fae\u4e0d\u8db3\u9053\u7684\u6620\u5c04\uff0c\u4f46\u53ef\u4ee5\u901a\u8fc7\u589e\u52a0\u4f18\u5316\u6765\u89e3\u51b3\uff0c\u524d\u63d0\u662f\u67b6\u6784\u7684\u6574\u4f53\u5229\u7528\u7387\u4fdd\u6301\u5728\u67d0\u4e2a\u9608\u503c\u4ee5\u4e0b\u3002\u8fd9\u4f1a\u5bfc\u81f4\u4e00\u4e9b\u91cf\u5b50\u6bd4\u7279\u88ab\u8fc7\u5ea6\u4f7f\u7528\uff0c\u800c\u53e6\u4e00\u4e9b\u91cf\u5b50\u6bd4\u7279\u5219\u4f7f\u7528\u4e0d\u8db3\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u53ef\u4ee5\u901a\u8fc7\u5173\u6ce8\u8fc7\u5ea6\u4f7f\u7528\u7684\u91cf\u5b50\u6bd4\u7279\u6765\u51cf\u5c11\u6821\u51c6\u5f00\u9500\uff1b\u901a\u8fc7\u4f18\u5316\u3001\u6620\u5c04\u548c\u5e03\u7ebf\u7b97\u6cd5\u6765\u6700\u5927\u5316\u786c\u4ef6\u5229\u7528\u7387\uff1b\u4ee5\u53ca\u5728\u591a\u79df\u6237\u73af\u5883\u4e2d\u901a\u8fc7\u4ee5\u8f83\u4f4e\u7684\u4ef7\u683c\u4e3a\u672a\u4f7f\u7528\u7684\u91cf\u5b50\u6bd4\u7279\u5b9a\u4ef7\u6765\u6fc0\u52b1\u4f7f\u7528\u5e76\u63d0\u9ad8\u786c\u4ef6\u541e\u5410\u91cf\u3002"}}
{"id": "2509.18120", "categories": ["cs.LG", "cs.AI", "cs.CE", "cs.DC", "cs.GT"], "pdf": "https://arxiv.org/pdf/2509.18120", "abs": "https://arxiv.org/abs/2509.18120", "authors": ["Thanh Linh Nguyen", "Quoc-Viet Pham"], "title": "A Coopetitive-Compatible Data Generation Framework for Cross-silo Federated Learning", "comment": "Accepted in IEEE GLOBECOM 2025", "summary": "Cross-silo federated learning (CFL) enables organizations (e.g., hospitals or\nbanks) to collaboratively train artificial intelligence (AI) models while\npreserving data privacy by keeping data local. While prior work has primarily\naddressed statistical heterogeneity across organizations, a critical challenge\narises from economic competition, where organizations may act as market rivals,\nmaking them hesitant to participate in joint training due to potential utility\nloss (i.e., reduced net benefit). Furthermore, the combined effects of\nstatistical heterogeneity and inter-organizational competition on\norganizational behavior and system-wide social welfare remain underexplored. In\nthis paper, we propose CoCoGen, a coopetitive-compatible data generation\nframework, leveraging generative AI (GenAI) and potential game theory to model,\nanalyze, and optimize collaborative learning under heterogeneous and\ncompetitive settings. Specifically, CoCoGen characterizes competition and\nstatistical heterogeneity through learning performance and utility-based\nformulations and models each training round as a weighted potential game. We\nthen derive GenAI-based data generation strategies that maximize social\nwelfare. Experimental results on the Fashion-MNIST dataset reveal how varying\nheterogeneity and competition levels affect organizational behavior and\ndemonstrate that CoCoGen consistently outperforms baseline methods.", "AI": {"tldr": "CoCoGen\u662f\u4e00\u4e2a\u5229\u7528\u751f\u6210\u5f0fAI\u548c\u535a\u5f08\u8bba\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u8de8\u7ec4\u7ec7\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u7edf\u8ba1\u5f02\u8d28\u6027\u548c\u7ecf\u6d4e\u7ade\u4e89\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u8de8\u7ec4\u7ec7\u8054\u90a6\u5b66\u4e60\uff08CFL\uff09\u4e3b\u8981\u5173\u6ce8\u7edf\u8ba1\u5f02\u8d28\u6027\uff0c\u4f46\u5ffd\u89c6\u4e86\u7ec4\u7ec7\u95f4\u7684\u7ecf\u6d4e\u7ade\u4e89\u3002\u7ec4\u7ec7\u53ef\u80fd\u56e0\u4e3a\u62c5\u5fc3\u6548\u7528\u635f\u5931\uff08\u51c0\u6536\u76ca\u964d\u4f4e\uff09\u800c\u4e0d\u613f\u53c2\u4e0e\u8054\u5408\u8bad\u7ec3\u3002\u6b64\u5916\uff0c\u7edf\u8ba1\u5f02\u8d28\u6027\u548c\u7ec4\u7ec7\u95f4\u7ade\u4e89\u5bf9\u7ec4\u7ec7\u884c\u4e3a\u548c\u7cfb\u7edf\u6574\u4f53\u793e\u4f1a\u798f\u5229\u7684\u5f71\u54cd\u4e5f\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002", "method": "\u63d0\u51faCoCoGen\u6846\u67b6\uff0c\u5229\u7528\u751f\u6210\u5f0fAI\uff08GenAI\uff09\u548c\u6f5c\u5728\u535a\u5f08\u8bba\u6765\u6a21\u62df\u3001\u5206\u6790\u548c\u4f18\u5316\u5728\u5f02\u8d28\u548c\u7ade\u4e89\u73af\u5883\u4e0b\u7684\u534f\u540c\u5b66\u4e60\u3002CoCoGen\u901a\u8fc7\u57fa\u4e8e\u5b66\u4e60\u6027\u80fd\u548c\u6548\u7528\u7684\u516c\u5f0f\u6765\u523b\u753b\u7ade\u4e89\u548c\u7edf\u8ba1\u5f02\u8d28\u6027\uff0c\u5e76\u5c06\u6bcf\u4e2a\u8bad\u7ec3\u8f6e\u6b21\u5efa\u6a21\u4e3a\u4e00\u4e2a\u52a0\u6743\u6f5c\u5728\u535a\u5f08\u3002\u8fdb\u800c\u63a8\u5bfc\u51fa\u6700\u5927\u5316\u793e\u4f1a\u798f\u5229\u7684GenAI\u6570\u636e\u751f\u6210\u7b56\u7565\u3002", "result": "\u5728Fashion-MNIST\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0d\u540c\u7684\u5f02\u8d28\u6027\u548c\u7ade\u4e89\u6c34\u5e73\u4f1a\u5f71\u54cd\u7ec4\u7ec7\u884c\u4e3a\uff0c\u5e76\u4e14CoCoGen\u7684\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "CoCoGen\u6210\u529f\u5730\u89e3\u51b3\u4e86\u8de8\u7ec4\u7ec7\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u7ecf\u6d4e\u7ade\u4e89\u548c\u7edf\u8ba1\u5f02\u8d28\u6027\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2509.18952", "categories": ["cond-mat.mes-hall", "quant-ph"], "pdf": "https://arxiv.org/pdf/2509.18952", "abs": "https://arxiv.org/abs/2509.18952", "authors": ["Hui-Qiang Liang", "Zuxuan Ou", "Linhu Li", "Guo-Fu Xu"], "title": "Intrinsic-perturbation induced anomalous higher-order boundary states in non-Hermitian systems", "comment": "4.7 pages, 3 figures in the main text; 16 pages, 10 figures in the\n  Supplemental Material", "summary": "The behavior of higher-order boundary states in non-Hermitian systems is\nelusive and thereby finding the mechanism behind these states is both essential\nand significant. Here, we uncover a novel mechanism that induces anomalous\nhigher-order boundary states. The mechanism originates from the sensitivity of\nthe non-normal boundary Hamiltonian to intrinsic perturbations, where intrinsic\nperturbations here refer to the influence of the bulk on the topological\nboundaries. Based on the mechanism, we reveal a new kind of phase transition,\ni.e., the transition between hybrid skin-topological states and scale-free\ntopological boundary states. We also find that scale-free topological boundary\nstates exhibit size-dependent spectra, influencing the existence of\nhigher-order topological boundary states. Unlike conventional hybrid\nskin-topological states or higher-order non-Hermitian skin effect, the above\ntwo kinds of anomalous higher-order boundary states exhibit size-dependent\ncharacteristics. Our work opens a new horizon for the control of higher-order\nboundary states and topological properties of non-Hermitian systems.", "AI": {"tldr": "\u975e\u5384\u7c73\u7cfb\u7edf\u4e2d\u7684\u9ad8\u9636\u8fb9\u754c\u6001\u884c\u4e3a\u96be\u4ee5\u6349\u6478\uff0c\u56e0\u6b64\u5bfb\u627e\u8fd9\u4e9b\u6001\u80cc\u540e\u7684\u673a\u5236\u81f3\u5173\u91cd\u8981\u4e14\u610f\u4e49\u91cd\u5927\u3002\u672c\u6587\u63ed\u793a\u4e86\u4e00\u79cd\u8bf1\u5bfc\u5f02\u5e38\u9ad8\u9636\u8fb9\u754c\u6001\u7684\u65b0\u673a\u5236\uff0c\u8be5\u673a\u5236\u6e90\u4e8e\u975e\u6b63\u5e38\u8fb9\u754c\u54c8\u5bc6\u987f\u91cf\u5bf9\u5185\u5728\u6270\u52a8\u7684\u654f\u611f\u6027\uff08\u5185\u5728\u6270\u52a8\u6307\u4f53\u5757\u5bf9\u62d3\u6251\u8fb9\u754c\u7684\u5f71\u54cd\uff09\u3002\u5728\u6b64\u673a\u5236\u57fa\u7840\u4e0a\uff0c\u7814\u7a76\u63ed\u793a\u4e86\u4e00\u79cd\u65b0\u578b\u76f8\u53d8\uff0c\u5373\u6df7\u5408\u8fb9\u754c\u6001\u4e0e\u65e0\u6807\u5ea6\u8fb9\u754c\u6001\u4e4b\u95f4\u7684\u76f8\u53d8\u3002\u7814\u7a76\u8fd8\u53d1\u73b0\uff0c\u65e0\u6807\u5ea6\u8fb9\u754c\u6001\u8868\u73b0\u51fa\u5c3a\u5bf8\u4f9d\u8d56\u7684\u8c31\uff0c\u5f71\u54cd\u9ad8\u9636\u62d3\u6251\u8fb9\u754c\u6001\u7684\u5b58\u5728\u3002\u4e0e\u4f20\u7edf\u7684\u6df7\u5408\u8fb9\u754c\u6001\u6216\u9ad8\u9636\u975e\u5384\u7c73\u8fb9\u754c\u6548\u5e94\u4e0d\u540c\uff0c\u4e0a\u8ff0\u4e24\u79cd\u5f02\u5e38\u9ad8\u9636\u8fb9\u754c\u6001\u5747\u8868\u73b0\u51fa\u5c3a\u5bf8\u4f9d\u8d56\u7684\u7279\u6027\u3002\u672c\u7814\u7a76\u4e3a\u8c03\u63a7\u975e\u5384\u7c73\u7cfb\u7edf\u4e2d\u7684\u9ad8\u9636\u8fb9\u754c\u6001\u548c\u62d3\u6251\u7279\u6027\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002", "motivation": "\u975e\u5384\u7c73\u7cfb\u7edf\u4e2d\u9ad8\u9636\u8fb9\u754c\u6001\u7684\u884c\u4e3a\u96be\u4ee5\u6349\u6478\uff0c\u56e0\u6b64\u627e\u5230\u8fd9\u4e9b\u6001\u80cc\u540e\u7684\u673a\u5236\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": "\u901a\u8fc7\u5206\u6790\u975e\u6b63\u5e38\u8fb9\u754c\u54c8\u5bc6\u987f\u91cf\u5bf9\u5185\u5728\u6270\u52a8\u7684\u654f\u611f\u6027\u6765\u63ed\u793a\u65b0\u673a\u5236\u3002", "result": "\u63ed\u793a\u4e86\u6df7\u5408\u8fb9\u754c\u6001\u4e0e\u65e0\u6807\u5ea6\u8fb9\u754c\u6001\u4e4b\u95f4\u7684\u4e00\u79cd\u65b0\u578b\u76f8\u53d8\uff0c\u5e76\u53d1\u73b0\u65e0\u6807\u5ea6\u8fb9\u754c\u6001\u5177\u6709\u5c3a\u5bf8\u4f9d\u8d56\u7684\u8c31\uff0c\u5f71\u54cd\u9ad8\u9636\u62d3\u6251\u8fb9\u754c\u6001\u7684\u5b58\u5728\u3002\u8fd9\u4e9b\u5f02\u5e38\u9ad8\u9636\u8fb9\u754c\u6001\u8868\u73b0\u51fa\u5c3a\u5bf8\u4f9d\u8d56\u7684\u7279\u6027\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bf1\u5bfc\u5f02\u5e38\u9ad8\u9636\u8fb9\u754c\u6001\u7684\u673a\u5236\uff0c\u5e76\u53d1\u73b0\u4e86\u4e0e\u4f20\u7edf\u8fb9\u754c\u6001\u4e0d\u540c\u7684\u5c3a\u5bf8\u4f9d\u8d56\u7279\u6027\uff0c\u4e3a\u8c03\u63a7\u975e\u5384\u7c73\u7cfb\u7edf\u4e2d\u7684\u9ad8\u9636\u8fb9\u754c\u6001\u548c\u62d3\u6251\u7279\u6027\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2509.18485", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2509.18485", "abs": "https://arxiv.org/abs/2509.18485", "authors": ["Xin Gu", "Ruoqi Wang", "Bo Zhao", "Haofu Wen", "Kunquan Hong", "Shijun Yuan", "Taishi Chen", "Jinlan Wang"], "title": "Zero-field Anomalous Hall Effect in Bulk Single Crystal Mn3Ir", "comment": "18 pages, 5 figures", "summary": "The L1_2-phase non-collinear antiferromagnet (AFM) Mn_3Ir has emerged as a\npioneering platform for realizing the zero-field anomalous Hall effect (AHE),\nthereby catalyzing rapid advances in antiferromagnetic spintronics. Despite its\nsignificant potential, experimental investigations of the intrinsic magnetic\nand electronic properties of Mn_3Ir have been greatly hindered by the\nformidable challenges in growing bulk single crystals. Here, we report the\ngrowth of stoichiometric Mn_3Ir bulk single crystals and their characterization\nin terms of magnetization and the AHE. Using a high-throughput flux method, we\nobtained (111)-oriented hexagonal Mn_3Ir single crystals. A small AHE signal\nwas detected, which we attribute to the coexistence of A- and B-type\nantiferromagnetic domains that mutually cancel the net AHE response. Our\nresults reveal key aspects of the intrinsic magnetic properties and AHE in bulk\nMn_3Ir, providing a critical material platform for the development of advanced\nspintronic devices.", "AI": {"tldr": "Mn_3Ir\u662f\u96f6\u573a\u53cd\u5e38\u970d\u5c14\u6548\u5e94\u7684\u9886\u5148\u5e73\u53f0\uff0c\u4f46\u4f53\u5355\u6676\u751f\u957f\u56f0\u96be\u3002\u672c\u6587\u6210\u529f\u751f\u957f\u4e86\u5316\u5b66\u8ba1\u91cf\u6bd4\u7684Mn_3Ir\u4f53\u5355\u6676\uff0c\u5e76\u901a\u8fc7\u78c1\u5316\u548cAHE\u8868\u5f81\u53d1\u73b0A\u578b\u548cB\u578b\u53cd\u94c1\u78c1\u7574\u5171\u5b58\u5bfc\u81f4AHE\u4fe1\u53f7\u5fae\u5f31\u3002", "motivation": "\u4f53\u5355\u6676Mn_3Ir\u7684\u751f\u957f\u548c\u8868\u5f81\u5bf9\u4e8e\u7406\u89e3\u5176\u5185\u5728\u78c1\u6027\u548c\u7535\u5b50\u6027\u8d28\u81f3\u5173\u91cd\u8981\uff0c\u4ee5\u63a8\u52a8\u53cd\u94c1\u78c1\u81ea\u65cb\u7535\u5b50\u5b66\u7684\u53d1\u5c55\u3002", "method": "\u4f7f\u7528\u9ad8\u901a\u91cf\u901a\u91cf\u6cd5\u751f\u957f\u4e86(111)-\u53d6\u5411\u7684\u516d\u65b9Mn_3Ir\u5355\u6676\uff0c\u5e76\u901a\u8fc7\u78c1\u5316\u548cAHE\u8fdb\u884c\u4e86\u8868\u5f81\u3002", "result": "\u751f\u957f\u51fa\u4e86\u5316\u5b66\u8ba1\u91cf\u6bd4\u7684Mn_3Ir\u4f53\u5355\u6676\uff0c\u5e76\u68c0\u6d4b\u5230\u5fae\u5f31\u7684AHE\u4fe1\u53f7\uff0c\u5f52\u56e0\u4e8eA\u578b\u548cB\u578b\u53cd\u94c1\u78c1\u7574\u7684\u5171\u5b58\uff0c\u5b83\u4eec\u76f8\u4e92\u62b5\u6d88\u4e86\u51c0AHE\u54cd\u5e94\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u4f53Mn_3Ir\u5185\u5728\u78c1\u6027\u548cAHE\u7684\u5173\u952e\u65b9\u9762\uff0c\u4e3a\u5f00\u53d1\u5148\u8fdb\u7684\u81ea\u65cb\u7535\u5b50\u5668\u4ef6\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5173\u952e\u7684\u6750\u6599\u5e73\u53f0\u3002"}}
{"id": "2509.18753", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.18753", "abs": "https://arxiv.org/abs/2509.18753", "authors": ["Hao Wu", "Xinyuan Yao", "Rui Ni", "Chen Gong", "Kaibin Huang"], "title": "Detection Capability Comparison Between Intensity Detection and Splitting Detection for Rydberg-Atomic Sensors", "comment": null, "summary": "Rydberg atomic quantum receivers have been seen as novel radio frequency\nmeasurements and the high sensitivity to a large range of frequencies makes it\nattractive for communications reception. However, their unique physical\ncharacteristics enable two fundamental signal readout schemes: intensity-based\ndetection and splitting-based detection. The former measures the electric\nfields through laser intensity, while the latter utilizes Autler-Townes\nsplitting. In this work, we systematically categorize and model existing signal\nreadout methods, classifying them into these two paradigms. Then, we derive the\nmaximum likelihood estimation procedures and corresponding Cram\\'er-Rao lower\nbounds (CRLB) for each detection modality. Through the analysis of the CRLB, we\npropose strategy for both readout schemes to enhance sensitivity and minimize\nestimation variance: acquiring data in regions with maximal slope magnitudes.\nWhile this approach has been implemented in intensity-based detection (e.g.,\nsuperheterodyne schemes), its application to splitting-based detection remains\nunexplored. Implementation of non-uniform frequency scanning, with preferential\nsampling at regions exhibiting maximum peak slopes combined with our proposed\nmaximum likelihood splitting estimation method, achieves significantly reduced\nestimation variance compared to conventional polynomial fitting. The\ncomparative analysis reveals the optimal detection performance of the two\ndetection schemes. This work also contributes to enhancing the accuracy of\nmicrowave calibration. Numerical results reveal that both fundamental signal\nreadout methods achieve lower estimation variance based on our proposed maximum\nlikelihood estimation approach.", "AI": {"tldr": "\u91cc\u5fb7\u5821\u539f\u5b50\u91cf\u5b50\u63a5\u6536\u5668\u6709\u4e24\u79cd\u4e3b\u8981\u7684\u4fe1\u53f7\u8bfb\u53d6\u65b9\u6848\uff1a\u57fa\u4e8e\u5f3a\u5ea6\u548c\u57fa\u4e8e\u5206\u88c2\u3002\u672c\u6587\u5bf9\u8fd9\u4e24\u79cd\u65b9\u6848\u8fdb\u884c\u4e86\u5efa\u6a21\u548c\u5206\u6790\uff0c\u5e76\u63d0\u51fa\u4e86\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\uff08MLE\uff09\u65b9\u6cd5\u548c\u76f8\u5e94\u7684Cram\u00e9r-Rao\u4e0b\u754c\uff08CRLB\uff09\u3002\u7814\u7a76\u8868\u660e\uff0c\u5728\u6700\u5927\u659c\u7387\u533a\u57df\u8fdb\u884c\u6570\u636e\u91c7\u96c6\u53ef\u4ee5\u63d0\u9ad8\u7075\u654f\u5ea6\u5e76\u6700\u5c0f\u5316\u4f30\u8ba1\u65b9\u5dee\uff0c\u5c24\u5176\u662f\u5728\u5206\u88c2\u68c0\u6d4b\u65b9\u6848\u4e2d\uff0c\u8fd9\u6bd4\u4f20\u7edf\u7684\u591a\u9879\u5f0f\u62df\u5408\u6548\u679c\u66f4\u597d\u3002", "motivation": "\u91cc\u5fb7\u5821\u539f\u5b50\u91cf\u5b50\u63a5\u6536\u5668\u56e0\u5176\u9ad8\u7075\u654f\u5ea6\u548c\u5bbd\u9891\u7387\u8303\u56f4\u800c\u5728\u901a\u4fe1\u63a5\u6536\u9886\u57df\u5177\u6709\u5438\u5f15\u529b\uff0c\u4f46\u5176\u4fe1\u53f7\u8bfb\u53d6\u65b9\u6848\uff08\u57fa\u4e8e\u5f3a\u5ea6\u548c\u57fa\u4e8e\u5206\u88c2\uff09\u7684\u4f18\u5316\u548c\u5206\u6790\u5c1a\u4e0d\u5145\u5206\u3002", "method": "\u5bf9\u57fa\u4e8e\u5f3a\u5ea6\u548c\u57fa\u4e8e\u5206\u88c2\u7684\u4fe1\u53f7\u8bfb\u53d6\u65b9\u6848\u8fdb\u884c\u5efa\u6a21\u548c\u5206\u7c7b\uff0c\u63a8\u5bfc\u4e86\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\uff08MLE\uff09\u7a0b\u5e8f\u548cCram\u00e9r-Rao\u4e0b\u754c\uff08CRLB\uff09\uff0c\u5e76\u63d0\u51fa\u5728\u6700\u5927\u659c\u7387\u533a\u57df\u8fdb\u884c\u6570\u636e\u91c7\u96c6\u4ee5\u63d0\u9ad8\u7075\u654f\u5ea6\u548c\u6700\u5c0f\u5316\u4f30\u8ba1\u65b9\u5dee\u7684\u7b56\u7565\u3002", "result": "\u57fa\u4e8e\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u548c\u5728\u6700\u5927\u659c\u7387\u533a\u57df\u8fdb\u884c\u6570\u636e\u91c7\u96c6\u7684\u7b56\u7565\uff0c\u5206\u88c2\u68c0\u6d4b\u65b9\u6848\u7684\u4f30\u8ba1\u65b9\u5dee\u663e\u8457\u964d\u4f4e\uff0c\u4f18\u4e8e\u4f20\u7edf\u7684\u591a\u9879\u5f0f\u62df\u5408\u3002\u4e24\u79cd\u65b9\u6848\u90fd\u901a\u8fc7\u63d0\u51fa\u7684MLE\u65b9\u6cd5\u5b9e\u73b0\u4e86\u8f83\u4f4e\u7684\u4f30\u8ba1\u65b9\u5dee\uff0c\u63d0\u9ad8\u4e86\u7cbe\u5ea6\uff0c\u5e76\u6709\u52a9\u4e8e\u5fae\u6ce2\u6821\u51c6\u3002", "conclusion": "\u672c\u6587\u5bf9\u91cc\u5fb7\u5821\u539f\u5b50\u91cf\u5b50\u63a5\u6536\u5668\u7684\u4e24\u79cd\u4fe1\u53f7\u8bfb\u53d6\u65b9\u6848\u8fdb\u884c\u4e86\u7cfb\u7edf\u5206\u6790\uff0c\u63d0\u51fa\u4e86\u6539\u8fdb\u7684\u4fe1\u53f7\u8bfb\u53d6\u548c\u4f30\u8ba1\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u5176\u5728\u63d0\u9ad8\u7075\u654f\u5ea6\u548c\u964d\u4f4e\u4f30\u8ba1\u65b9\u5dee\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u4e3a\u901a\u4fe1\u63a5\u6536\u548c\u5fae\u6ce2\u6821\u51c6\u63d0\u4f9b\u4e86\u65b0\u7684\u9014\u5f84\u3002"}}
{"id": "2509.18384", "categories": ["cs.RO", "cs.FL"], "pdf": "https://arxiv.org/pdf/2509.18384", "abs": "https://arxiv.org/abs/2509.18384", "authors": ["Yunhao Yang", "Junyuan Hong", "Gabriel Jacob Perin", "Zhiwen Fan", "Li Yin", "Zhangyang Wang", "Ufuk Topcu"], "title": "AD-VF: LLM-Automatic Differentiation Enables Fine-Tuning-Free Robot Planning from Formal Methods Feedback", "comment": null, "summary": "Large language models (LLMs) can translate natural language instructions into\nexecutable action plans for robotics, autonomous driving, and other domains.\nYet, deploying LLM-driven planning in the physical world demands strict\nadherence to safety and regulatory constraints, which current models often\nviolate due to hallucination or weak alignment. Traditional data-driven\nalignment methods, such as Direct Preference Optimization (DPO), require costly\nhuman labeling, while recent formal-feedback approaches still depend on\nresource-intensive fine-tuning. In this paper, we propose LAD-VF, a\nfine-tuning-free framework that leverages formal verification feedback for\nautomated prompt engineering. By introducing a formal-verification-informed\ntext loss integrated with LLM-AutoDiff, LAD-VF iteratively refines prompts\nrather than model parameters. This yields three key benefits: (i) scalable\nadaptation without fine-tuning; (ii) compatibility with modular LLM\narchitectures; and (iii) interpretable refinement via auditable prompts.\nExperiments in robot navigation and manipulation tasks demonstrate that LAD-VF\nsubstantially enhances specification compliance, improving success rates from\n60% to over 90%. Our method thus presents a scalable and interpretable pathway\ntoward trustworthy, formally-verified LLM-driven control systems.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.18176", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18176", "abs": "https://arxiv.org/abs/2509.18176", "authors": ["Wendong Yao", "Saeed Azadnejad", "Binhua Huang", "Shane Donohue", "Soumyabrata Dev"], "title": "A Deep Learning Approach for Spatio-Temporal Forecasting of InSAR Ground Deformation in Eastern Ireland", "comment": "This paper is submitted to IEEE Transactions on Geoscience and Remote\n  Sensing", "summary": "Monitoring ground displacement is crucial for urban infrastructure stability\nand mitigating geological hazards. However, forecasting future deformation from\nsparse Interferometric Synthetic Aperture Radar (InSAR) time-series data\nremains a significant challenge. This paper introduces a novel deep learning\nframework that transforms these sparse point measurements into a dense\nspatio-temporal tensor. This methodological shift allows, for the first time,\nthe direct application of advanced computer vision architectures to this\nforecasting problem. We design and implement a hybrid Convolutional Neural\nNetwork and Long-Short Term Memory (CNN-LSTM) model, specifically engineered to\nsimultaneously learn spatial patterns and temporal dependencies from the\ngenerated data tensor. The model's performance is benchmarked against powerful\nmachine learning baselines, Light Gradient Boosting Machine and LASSO\nregression, using Sentinel-1 data from eastern Ireland. Results demonstrate\nthat the proposed architecture provides significantly more accurate and\nspatially coherent forecasts, establishing a new performance benchmark for this\ntask. Furthermore, an interpretability analysis reveals that baseline models\noften default to simplistic persistence patterns, highlighting the necessity of\nour integrated spatio-temporal approach to capture the complex dynamics of\nground deformation. Our findings confirm the efficacy and potential of\nspatio-temporal deep learning for high-resolution deformation forecasting.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u5c06\u7a00\u758f\u7684InSAR\u6d4b\u91cf\u6570\u636e\u8f6c\u6362\u4e3a\u5bc6\u96c6\u65f6\u7a7a\u5f20\u91cf\uff0c\u5e76\u4f7f\u7528CNN-LSTM\u6a21\u578b\u8fdb\u884c\u5730\u9762\u6c89\u964d\u9884\u6d4b\uff0c\u53d6\u5f97\u4e86\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u597d\u7684\u6548\u679c\u3002", "motivation": "\u57ce\u5e02\u57fa\u7840\u8bbe\u65bd\u7a33\u5b9a\u548c\u5730\u8d28\u707e\u5bb3\u51cf\u7f13\u9700\u8981\u76d1\u6d4b\u5730\u9762\u6c89\u964d\uff0c\u4f46\u4ece\u7a00\u758f\u7684InSAR\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u9884\u6d4b\u672a\u6765\u53d8\u5f62\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002", "method": "\u5c06\u7a00\u758f\u7684InSAR\u6d4b\u91cf\u6570\u636e\u8f6c\u6362\u4e3a\u5bc6\u96c6\u65f6\u7a7a\u5f20\u91cf\uff0c\u5e76\u8bbe\u8ba1\u548c\u5b9e\u73b0\u4e86\u4e00\u4e2a\u6df7\u5408\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u548c\u957f\u77ed\u671f\u8bb0\u5fc6\uff08LSTM\uff09\u6a21\u578b\u6765\u5b66\u4e60\u7a7a\u95f4\u6a21\u5f0f\u548c\u65f6\u95f4\u4f9d\u8d56\u6027\u3002", "result": "\u8be5\u6a21\u578b\u5728Sentinel-1\u6570\u636e\u4e0a\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c\u4e0eLight Gradient Boosting Machine\u548cLASSO\u56de\u5f52\u7b49\u57fa\u7ebf\u6a21\u578b\u76f8\u6bd4\uff0c\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u3001\u7a7a\u95f4\u4e0a\u66f4\u8fde\u8d2f\u7684\u9884\u6d4b\uff0c\u5e76\u8bbe\u7acb\u4e86\u65b0\u7684\u6027\u80fd\u57fa\u51c6\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8bc1\u5b9e\u4e86\u65f6\u7a7a\u6df1\u5ea6\u5b66\u4e60\u5728\u63d0\u4f9b\u9ad8\u5206\u8fa8\u7387\u53d8\u5f62\u9884\u6d4b\u65b9\u9762\u7684\u6709\u6548\u6027\u548c\u6f5c\u529b\u3002"}}
{"id": "2509.18263", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2509.18263", "abs": "https://arxiv.org/abs/2509.18263", "authors": ["Hanna Linn", "Rui-Hao Li", "Alexander Holden", "Abdullah Ash Saki", "Frank DiFilippo", "Tomas Radivoyevitch", "Daniel Blankenberg", "Laura Garc\u00eda-\u00c1lvarez", "G\u00f6ran Johansson"], "title": "Efficient Quantum Protein Structure Prediction with Problem-Agnostic Ansatzes", "comment": "18 pages, 8 figures, 4 tables", "summary": "Accurately predicting protein structures from amino acid sequences remains a\nfundamental challenge in computational biology, with profound implications for\nunderstanding biological functions and enabling structure-based drug discovery.\nQuantum computing approaches based on coarse-grained lattice models combined\nwith variational algorithms have been proposed as an initial step towards\npredicting protein structures using quantum computers. In this work, we\nintroduce a more efficient quantum protein structure prediction workflow that\nbypasses the need for explicit Hamiltonian construction by employing a\nproblem-agnostic ansatz. The ansatz is trained to minimize an energy-based cost\nfunction that can be efficiently computed on classical computers, eliminating\nthe need for ancillary qubits and reducing circuit depth compared to previous\nHamiltonian-based methods. This enables a more scalable approach for larger\nproteins and facilitates the inclusion of higher-order interactions, previously\nhard to achieve in quantum approaches. We validate our method by benchmarking a\nhardware-efficient ansatz on a large set of proteins with up to 26 amino acids,\nmodeled on the tetrahedral, body-centered cubic, and face-centered cubic\nlattices, incorporating up to second-nearest-neighbor interactions. We assess\nthe performance on both a noise-free simulator and the ibm_kingston quantum\ncomputer using a set of distinct metrics to probe different aspects of the\nprediction quality. These experiments push the boundaries of quantum methods\nfor protein structure prediction, targeting sequences that are longer than\nthose typically addressed in prior studies. Overall, the results highlight the\nscalability and versatility of our approach, while also identifying key areas\nfor improvement to inform future algorithm development and hardware\nadvancements.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u6709\u6548\u7684\u91cf\u5b50\u86cb\u767d\u7ed3\u6784\u9884\u6d4b\u5de5\u4f5c\u6d41\u7a0b\uff0c\u901a\u8fc7\u4f7f\u7528\u95ee\u9898\u65e0\u5173\u7684ansatz\u6765\u7ed5\u8fc7\u663e\u5f0f\u54c8\u5bc6\u987f\u91cf\u6784\u5efa\u7684\u9700\u8981\uff0c\u5e76\u5728\u7ecf\u5178\u8ba1\u7b97\u673a\u4e0a\u9ad8\u6548\u8ba1\u7b97\u80fd\u91cf\u6210\u672c\u51fd\u6570\uff0c\u4ece\u800c\u5b9e\u73b0\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u51c6\u786e\u9884\u6d4b\u6c28\u57fa\u9178\u5e8f\u5217\u7684\u86cb\u767d\u8d28\u7ed3\u6784\u662f\u8ba1\u7b97\u751f\u7269\u5b66\u4e2d\u7684\u4e00\u4e2a\u57fa\u672c\u6311\u6218\uff0c\u5bf9\u7406\u89e3\u751f\u7269\u529f\u80fd\u548c\u57fa\u4e8e\u7ed3\u6784\u7684\u836f\u7269\u53d1\u73b0\u5177\u6709\u6df1\u8fdc\u5f71\u54cd\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u66f4\u6709\u6548\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u901a\u8fc7\u4f7f\u7528\u95ee\u9898\u65e0\u5173\u7684ansatz\u5e76\u6700\u5c0f\u5316\u53ef\u4ee5\u5728\u7ecf\u5178\u8ba1\u7b97\u673a\u4e0a\u9ad8\u6548\u8ba1\u7b97\u7684\u57fa\u4e8e\u80fd\u91cf\u7684\u6210\u672c\u51fd\u6570\uff0c\u4ece\u800c\u7ed5\u8fc7\u663e\u5f0f\u54c8\u5bc6\u987f\u91cf\u6784\u5efa\u7684\u9700\u8981\u3002\u6211\u4eec\u5728\u5404\u79cd\u6676\u683c\u4e0a\u4f7f\u7528\u5305\u542b\u9ad8\u8fbe\u4e8c\u9636\u8fd1\u90bb\u76f8\u4e92\u4f5c\u7528\u768426\u4e2a\u6c28\u57fa\u9178\u7684\u86cb\u767d\u8d28\u96c6\u6765\u6d4b\u8bd5\u6211\u4eec\u7684\u65b9\u6cd5\u3002", "result": "\u5728\u65e0\u566a\u58f0\u6a21\u62df\u5668\u548cIBM Kingston\u91cf\u5b50\u8ba1\u7b97\u673a\u4e0a\u8bc4\u4f30\u4e86\u8be5\u65b9\u6cd5\uff0c\u4f7f\u7528\u4e86\u591a\u79cd\u6307\u6807\u6765\u8bc4\u4f30\u9884\u6d4b\u8d28\u91cf\uff0c\u9488\u5bf9\u6bd4\u5148\u524d\u7814\u7a76\u4e2d\u901a\u5e38\u5904\u7406\u7684\u66f4\u957f\u7684\u5e8f\u5217\u3002", "conclusion": "\u7ed3\u679c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6269\u5c55\u6027\u548c\u591a\u529f\u80fd\u6027\uff0c\u5e76\u6307\u51fa\u4e86\u6539\u8fdb\u7684\u9886\u57df\uff0c\u4e3a\u672a\u6765\u7684\u7b97\u6cd5\u5f00\u53d1\u548c\u786c\u4ef6\u8fdb\u6b65\u63d0\u4f9b\u4e86\u4fe1\u606f\u3002"}}
{"id": "2509.18108", "categories": ["cs.LG", "cs.AI", "I.2.2; I.2.7"], "pdf": "https://arxiv.org/pdf/2509.18108", "abs": "https://arxiv.org/abs/2509.18108", "authors": ["Adam Viktorin", "Tomas Kadavy", "Jozef Kovac", "Michal Pluhacek", "Roman Senkerik"], "title": "Solve it with EASE", "comment": "EASE framework landing paper", "summary": "This paper presents EASE (Effortless Algorithmic Solution Evolution), an\nopen-source and fully modular framework for iterative algorithmic solution\ngeneration leveraging large language models (LLMs). EASE integrates generation,\ntesting, analysis, and evaluation into a reproducible feedback loop, giving\nusers full control over error handling, analysis, and quality assessment. Its\narchitecture supports the orchestration of multiple LLMs in complementary\nroles-such as generator, analyst, and evaluator. By abstracting the complexity\nof prompt design and model management, EASE provides a transparent and\nextensible platform for researchers and practitioners to co-design algorithms\nand other generative solutions across diverse domains.", "AI": {"tldr": "EASE\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u3001\u5b8c\u5168\u6a21\u5757\u5316\u7684\u6846\u67b6\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8fdb\u884c\u8fed\u4ee3\u7b97\u6cd5\u89e3\u51b3\u65b9\u6848\u7684\u751f\u6210\uff0c\u96c6\u6210\u4e86\u751f\u6210\u3001\u6d4b\u8bd5\u3001\u5206\u6790\u548c\u8bc4\u4f30\uff0c\u7528\u6237\u53ef\u4ee5\u5b8c\u5168\u63a7\u5236\u9519\u8bef\u5904\u7406\u3001\u5206\u6790\u548c\u8d28\u91cf\u8bc4\u4f30\uff0c\u5e76\u652f\u6301\u591a\u4e2aLLM\u534f\u540c\u5de5\u4f5c\uff0c\u7b80\u5316\u4e86\u63d0\u793a\u8bbe\u8ba1\u548c\u6a21\u578b\u7ba1\u7406\uff0c\u4e3a\u7814\u7a76\u4eba\u5458\u548c\u4ece\u4e1a\u8005\u63d0\u4f9b\u4e86\u4e00\u4e2a\u900f\u660e\u4e14\u53ef\u6269\u5c55\u7684\u5e73\u53f0\u3002", "motivation": "\u8be5\u8bba\u6587\u7684\u52a8\u673a\u662f\u4e3a\u8fed\u4ee3\u7b97\u6cd5\u89e3\u51b3\u65b9\u6848\u7684\u751f\u6210\u63d0\u4f9b\u4e00\u4e2a\u5f00\u653e\u6e90\u7801\u3001\u5b8c\u5168\u6a21\u5757\u5316\u7684\u6846\u67b6\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u529f\u80fd\u3002", "method": "\u8be5\u65b9\u6cd5\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEASE\uff08Effortless Algorithmic Solution Evolution\uff09\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u751f\u6210\u3001\u6d4b\u8bd5\u3001\u5206\u6790\u548c\u8bc4\u4f30\uff0c\u5f62\u6210\u4e00\u4e2a\u53ef\u91cd\u590d\u7684\u53cd\u9988\u5faa\u73af\uff0c\u5e76\u652f\u6301\u591a\u4e2aLLM\u5728\u751f\u6210\u5668\u3001\u5206\u6790\u5668\u548c\u8bc4\u4f30\u5668\u7b49\u4e92\u8865\u89d2\u8272\u4e2d\u8fdb\u884c\u7f16\u6392\u3002", "result": "EASE\u6846\u67b6\u901a\u8fc7\u62bd\u8c61\u5316\u63d0\u793a\u8bbe\u8ba1\u548c\u6a21\u578b\u7ba1\u7406\u7684\u590d\u6742\u6027\uff0c\u4e3a\u7814\u7a76\u4eba\u5458\u548c\u4ece\u4e1a\u8005\u63d0\u4f9b\u4e86\u4e00\u4e2a\u900f\u660e\u4e14\u53ef\u6269\u5c55\u7684\u5e73\u53f0\uff0c\u4ee5\u4fbf\u5728\u4e0d\u540c\u9886\u57df\u5171\u540c\u8bbe\u8ba1\u7b97\u6cd5\u548c\u5176\u4ed6\u751f\u6210\u5f0f\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "EASE\u6846\u67b6\u901a\u8fc7\u63d0\u4f9b\u4e00\u4e2a\u96c6\u6210\u7684\u3001\u53ef\u63a7\u7684\u3001\u53ef\u6269\u5c55\u7684\u5e73\u53f0\uff0c\u7b80\u5316\u4e86\u5229\u7528LLM\u8fdb\u884c\u7b97\u6cd5\u89e3\u51b3\u65b9\u6848\u7684\u8fed\u4ee3\u751f\u6210\u8fc7\u7a0b\u3002"}}
{"id": "2509.18180", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18180", "abs": "https://arxiv.org/abs/2509.18180", "authors": ["Yang Wang", "Kai Li"], "title": "Large Language Models and Operations Research: A Structured Survey", "comment": null, "summary": "Operations research (OR) provides fundamental methodologies for complex\nsystem decision-making, with established applications in transportation, supply\nchain management, and production scheduling. Traditional approaches, which\ndepend on expert-based modeling and manual parameter adjustment, often face\nchallenges in handling large-scale, dynamic, and multi-constraint problems.\nRecently, large language models (LLMs) have shown potential to address these\nlimitations through semantic understanding, structured generation, and\nreasoning control. LLMs can translate natural language descriptions into\nmathematical models or executable code, generate heuristics, evolve algorithms,\nand directly tackle optimization tasks. This paper surveys recent progress on\nthe integration of LLMs into OR, organizing methods into three main directions:\nautomatic modeling, auxiliary optimization, and direct solving. It further\nreviews evaluation benchmarks and domain-specific applications, and summarizes\nkey open issues such as unstable semantic-to-structure mapping, fragmented\nresearch progress, limited generalization, and insufficient evaluation systems.\nFinally, the survey outlines possible research avenues for advancing the role\nof LLMs in OR.", "AI": {"tldr": "LLMs\u5728\u8fd0\u7b79\u5b66\uff08OR\uff09\u4e2d\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u53ef\u7528\u4e8e\u81ea\u52a8\u5efa\u6a21\u3001\u8f85\u52a9\u4f18\u5316\u548c\u76f4\u63a5\u6c42\u89e3\uff0c\u4f46\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u5982\u8bed\u4e49\u6620\u5c04\u4e0d\u7a33\u5b9a\u3001\u7814\u7a76\u788e\u7247\u5316\u3001\u6cdb\u5316\u80fd\u529b\u6709\u9650\u548c\u8bc4\u4f30\u4e0d\u8db3\u3002", "motivation": "\u4f20\u7edf\u8fd0\u7b79\u5b66\u65b9\u6cd5\u5728\u5904\u7406\u5927\u89c4\u6a21\u3001\u52a8\u6001\u548c\u591a\u7ea6\u675f\u95ee\u9898\u65f6\u9762\u4e34\u6311\u6218\uff0c\u800cLLMs\u51ed\u501f\u5176\u8bed\u4e49\u7406\u89e3\u3001\u7ed3\u6784\u5316\u751f\u6210\u548c\u63a8\u7406\u63a7\u5236\u80fd\u529b\uff0c\u6709\u671b\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u3002", "method": "\u672c\u8bba\u6587\u5c06LLMs\u5728OR\u4e2d\u7684\u5e94\u7528\u65b9\u6cd5\u5f52\u7eb3\u4e3a\u4e09\u4e2a\u4e3b\u8981\u65b9\u5411\uff1a\u81ea\u52a8\u5efa\u6a21\u3001\u8f85\u52a9\u4f18\u5316\u548c\u76f4\u63a5\u6c42\u89e3\u3002", "result": "\u8bba\u6587\u56de\u987e\u4e86LLMs\u5728OR\u4e2d\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u5e76\u8ba8\u8bba\u4e86\u8bc4\u4f30\u57fa\u51c6\u548c\u7279\u5b9a\u9886\u57df\u5e94\u7528\u3002", "conclusion": "\u5c3d\u7ba1LLMs\u5728OR\u9886\u57df\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u4ecd\u5b58\u5728\u8bed\u4e49\u5230\u7ed3\u6784\u7684\u6620\u5c04\u4e0d\u7a33\u5b9a\u3001\u7814\u7a76\u8fdb\u5c55\u788e\u7247\u5316\u3001\u6cdb\u5316\u80fd\u529b\u6709\u9650\u4ee5\u53ca\u8bc4\u4f30\u4f53\u7cfb\u4e0d\u5b8c\u5584\u7b49\u5173\u952e\u95ee\u9898\u3002\u672a\u6765\u7684\u7814\u7a76\u5e94\u81f4\u529b\u4e8e\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u4ee5\u63a8\u52a8LLMs\u5728OR\u4e2d\u7684\u4f5c\u7528\u3002"}}
{"id": "2509.18167", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18167", "abs": "https://arxiv.org/abs/2509.18167", "authors": ["Junlin Wang", "Zehao Wu", "Shaowei Lu", "Yanlan Li", "Xinghao Huang"], "title": "SIRAG: Towards Stable and Interpretable RAG with A Process-Supervised Multi-Agent Framework", "comment": "5 pages,2 figures, IRAC under review", "summary": "Retrieval-Augmented Generation (RAG) enables large language models (LLMs) to\naccess external knowledge sources, but the effectiveness of RAG relies on the\ncoordination between the retriever and the generator. Since these components\nare developed independently, their interaction is often suboptimal: the\nretriever may return irrelevant or redundant documents, while the generator may\nfail to fully leverage retrieved evidence. In this work, we propose a\nprocess-supervised multi-agent framework to bridge the gap between retriever\nand generator. The framework introduces two lightweight agents: a Decision\nMaker, which determines when to continue retrieval or stop for answer\ngeneration, and a Knowledge Selector, which filters retrieved documents to\nretain only the most useful evidence. To provide fine-grained supervision, we\nemploy an LLM-as-a-Judge that evaluates each intermediate action with\nprocess-level rewards, ensuring more accurate credit assignment than relying\nsolely on final answer correctness. We further adopt a tree-structured rollout\nstrategy to explore diverse reasoning paths, and train both agents with\nProximal Policy Optimization (PPO) in an end-to-end manner. Experiments on\nsingle-hop and multi-hop question answering benchmarks show that our approach\nachieves higher accuracy, more stable convergence, and produces more\ninterpretable reasoning trajectories compared with standard RAG baselines.\nImportantly, the proposed framework is modular and plug-and-play, requiring no\nmodification to the retriever or generator, making it practical for real-world\nRAG applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u51b3\u7b56\u8005\u548c\u77e5\u8bc6\u9009\u62e9\u5668\u4ee3\u7406\u6765\u6539\u8fdb\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6a21\u578b\uff0c\u5e76\u4f7f\u7528 LLM-as-a-Judge \u8fdb\u884c\u8fc7\u7a0b\u76d1\u7763\u548c PPO \u8fdb\u884c\u8bad\u7ec3\uff0c\u4ee5\u63d0\u9ad8\u68c0\u7d22\u548c\u751f\u6210\u7ec4\u4ef6\u4e4b\u95f4\u7684\u534f\u8c03\u6027\u3002", "motivation": "\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7684\u6709\u6548\u6027\u4f9d\u8d56\u4e8e\u68c0\u7d22\u5668\u548c\u751f\u6210\u5668\u4e4b\u95f4\u7684\u534f\u8c03\uff0c\u4f46\u7531\u4e8e\u5b83\u4eec\u662f\u72ec\u7acb\u5f00\u53d1\u7684\uff0c\u8fd9\u79cd\u4ea4\u4e92\u901a\u5e38\u4e0d\u662f\u6700\u4f18\u7684\uff0c\u5bfc\u81f4\u68c0\u7d22\u5230\u7684\u6587\u6863\u4e0d\u76f8\u5173\u6216\u5197\u4f59\uff0c\u6216\u8005\u751f\u6210\u5668\u672a\u80fd\u5145\u5206\u5229\u7528\u68c0\u7d22\u5230\u7684\u8bc1\u636e\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u4e2a\u7531\u51b3\u7b56\u8005\uff08\u51b3\u5b9a\u4f55\u65f6\u7ee7\u7eed\u68c0\u7d22\u6216\u505c\u6b62\u751f\u6210\uff09\u548c\u77e5\u8bc6\u9009\u62e9\u5668\uff08\u8fc7\u6ee4\u68c0\u7d22\u5230\u7684\u6587\u6863\u4ee5\u4fdd\u7559\u6700\u76f8\u5173\u7684\u8bc1\u636e\uff09\u7ec4\u6210\u7684\u53cc\u4ee3\u7406\u6846\u67b6\u3002\u4f7f\u7528 LLM-as-a-Judge \u5bf9\u6bcf\u4e2a\u4e2d\u95f4\u52a8\u4f5c\u8fdb\u884c\u8fc7\u7a0b\u7ea7\u522b\u5956\u52b1\u7684\u7ec6\u7c92\u5ea6\u76d1\u7763\u3002\u91c7\u7528\u6811\u72b6\u6eda\u52a8\u7b56\u7565\u63a2\u7d22\u63a8\u7406\u8def\u5f84\uff0c\u5e76\u4f7f\u7528 PPO \u7b97\u6cd5\u5bf9\u4e24\u4e2a\u4ee3\u7406\u8fdb\u884c\u7aef\u5230\u7aef\u8bad\u7ec3\u3002", "result": "\u5728\u5355\u8df3\u548c\u591a\u8df3\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u4e0e\u6807\u51c6\u7684 RAG \u57fa\u7ebf\u76f8\u6bd4\uff0c\u5728\u51c6\u786e\u6027\u3001\u6536\u655b\u7a33\u5b9a\u6027\u548c\u63a8\u7406\u8f68\u8ff9\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u5747\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u591a\u4ee3\u7406\u6846\u67b6\u80fd\u591f\u6709\u6548\u5f25\u5408\u68c0\u7d22\u5668\u548c\u751f\u6210\u5668\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u63d0\u9ad8 RAG \u6a21\u578b\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u7531\u4e8e\u5176\u6a21\u5757\u5316\u548c\u5373\u63d2\u5373\u7528\u7279\u6027\uff0c\u6613\u4e8e\u5728\u5b9e\u9645 RAG \u5e94\u7528\u4e2d\u63a8\u5e7f\u4f7f\u7528\u3002"}}
{"id": "2509.19150", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.19150", "abs": "https://arxiv.org/abs/2509.19150", "authors": ["Harikrishna Tummalapalli", "Riccardo Balin", "Christine M. Simpson", "Andrew Park", "Aymen Alsaadi", "Andrew E. Shao", "Wesley Brewer", "Shantenu Jha"], "title": "In-Transit Data Transport Strategies for Coupled AI-Simulation Workflow Patterns", "comment": null, "summary": "Coupled AI-Simulation workflows are becoming the major workloads for HPC\nfacilities, and their increasing complexity necessitates new tools for\nperformance analysis and prototyping of new in-situ workflows. We present\nSimAI-Bench, a tool designed to both prototype and evaluate these coupled\nworkflows. In this paper, we use SimAI-Bench to benchmark the data transport\nperformance of two common patterns on the Aurora supercomputer: a one-to-one\nworkflow with co-located simulation and AI training instances, and a\nmany-to-one workflow where a single AI model is trained from an ensemble of\nsimulations. For the one-to-one pattern, our analysis shows that node-local and\nDragonHPC data staging strategies provide excellent performance compared Redis\nand Lustre file system. For the many-to-one pattern, we find that data\ntransport becomes a dominant bottleneck as the ensemble size grows. Our\nevaluation reveals that file system is the optimal solution among the tested\nstrategies for the many-to-one pattern.", "AI": {"tldr": "SimAI-Bench\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u548c\u539f\u578b\u5316\u8026\u5408AI-\u6a21\u62df\u5de5\u4f5c\u6d41\u7684\u5de5\u5177\uff0c\u5e76\u5728Aurora\u8d85\u7ea7\u8ba1\u7b97\u673a\u4e0a\u5206\u6790\u4e86\u4e24\u79cd\u5e38\u89c1\u6570\u636e\u4f20\u8f93\u6a21\u5f0f\u7684\u6027\u80fd\u3002", "motivation": "HPC\u8bbe\u65bd\u4e2d\u8026\u5408AI-\u6a21\u62df\u5de5\u4f5c\u8d1f\u8f7d\u7684\u65e5\u76ca\u590d\u6742\uff0c\u9700\u8981\u65b0\u7684\u5de5\u5177\u6765\u652f\u6301\u6027\u80fd\u5206\u6790\u548c\u6a21\u62df\u5de5\u4f5c\u6d41\u7684\u539f\u578b\u8bbe\u8ba1\u3002", "method": "\u4f7f\u7528SimAI-Bench\u5728Aurora\u8d85\u7ea7\u8ba1\u7b97\u673a\u4e0a\u5bf9\u4e24\u79cd\u5de5\u4f5c\u6d41\u6a21\u5f0f\uff08\u4e00\u5bf9\u4e00\u548c\u591a\u5bf9\u4e00\uff09\u7684\u6570\u636e\u4f20\u8f93\u6027\u80fd\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u5728\u4e00\u5bf9\u4e00\u6a21\u5f0f\u4e0b\uff0c\u8282\u70b9\u672c\u5730\u548cDragonHPC\u6570\u636e\u6682\u5b58\u7b56\u7565\u76f8\u6bd4Redis\u548cLustre\u6587\u4ef6\u7cfb\u7edf\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u3002\u5728\u591a\u5bf9\u4e00\u6a21\u5f0f\u4e0b\uff0c\u968f\u7740\u96c6\u5408\u89c4\u6a21\u7684\u589e\u957f\uff0c\u6570\u636e\u4f20\u8f93\u6210\u4e3a\u4e3b\u8981\u7684\u74f6\u9888\uff0c\u6587\u4ef6\u7cfb\u7edf\u662f\u6700\u4f73\u9009\u62e9\u3002", "conclusion": "SimAI-Bench\u80fd\u591f\u6709\u6548\u5730\u8bc4\u4f30\u8026\u5408AI-\u6a21\u62df\u5de5\u4f5c\u6d41\uff0c\u5e76\u4e3a\u4f18\u5316\u8fd9\u4e9b\u5de5\u4f5c\u6d41\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002"}}
{"id": "2509.18624", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.18624", "abs": "https://arxiv.org/abs/2509.18624", "authors": ["Yue Zhang", "Xinzhi Zhong", "Soyoung Ahn", "Yajie Zou", "Zhengbing He"], "title": "Interaction-aware Lane-Changing Early Warning System in Congested Traffic", "comment": null, "summary": "Lane changes (LCs) in congested traffic are complex, multi-vehicle\ninteractive events that pose significant safety concerns. Providing early\nwarnings can enable more proactive driver assistance system and support more\ninformed decision-making for drivers under LCs. This paper presents an\ninteraction-aware Lane-Changing Early Warning (LCEW) system designed to issue\nreliable early warning signals based on future trajectory predictions. We first\ninvestigate the stochastic nature of LCs, characterized by (i) variable-size\nmulti-vehicle interactions and (ii) the direct and indirect risks resulting\nfrom these interactions. To model these stochastic interactions, a Social\nSpatio-Temporal Graph Convolutional Neural Network framework informed by mutual\ninformation (STGCNN-MI) is introduced to predict multi-vehicle trajectories. By\nleveraging a MI-based adjacency matrix, the framework enhances trajectory\nprediction accuracy while providing interpretable representations of vehicle\ninteractions. Then, potential collisions between the LC vehicle and adjacent\nvehicles (direct risks) or among the non-adjacent vehicles (indirect risks) are\nidentified using oriented bounding box detection applied to the predicted\ntrajectories. Finally, a warning signal is generated to inform the LC driver of\nlocation of potential collisions within the predicted time window. Traffic\nsimulation experiments conducted in SUMO demonstrate that the proposed\ninteraction-aware LCEW improves both vehicle-level safety and overall traffic\nefficiency, while also promoting more natural behavioral adaptation.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u4ea4\u4e92\u611f\u77e5\u8f66\u9053\u53d8\u6362\u9884\u8b66\uff08LCEW\uff09\u7cfb\u7edf\uff0c\u901a\u8fc7\u9884\u6d4b\u591a\u8f66\u8f68\u8ff9\u6765\u63d0\u524d\u53d1\u51fa\u9884\u8b66\u4fe1\u53f7\uff0c\u4ee5\u63d0\u9ad8\u62e5\u5835\u4ea4\u901a\u4e2d\u7684\u5b89\u5168\u6027\u3002", "motivation": "\u62e5\u5835\u4ea4\u901a\u4e2d\u7684\u8f66\u9053\u53d8\u6362\uff08LC\uff09\u662f\u6d89\u53ca\u591a\u8f66\u4ea4\u4e92\u7684\u590d\u6742\u4e8b\u4ef6\uff0c\u5b58\u5728\u663e\u8457\u5b89\u5168\u9690\u60a3\u3002\u63d0\u524d\u9884\u8b66\u80fd\u4e3a\u4e3b\u9a7e\u63d0\u4f9b\u66f4\u4e3b\u52a8\u7684\u9a7e\u9a76\u8f85\u52a9\u548c\u66f4\u660e\u667a\u7684\u51b3\u7b56\u3002", "method": "\u5f15\u5165\u4e86\u57fa\u4e8e\u4e92\u4fe1\u606f\u7684\u793e\u4ea4\u65f6\u7a7a\u56fe\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08STGCNN-MI\uff09\u6846\u67b6\u6765\u9884\u6d4b\u591a\u8f66\u8f68\u8ff9\uff0c\u901a\u8fc7MI\u4e3a\u90bb\u63a5\u77e9\u9635\u6765\u63d0\u5347\u9884\u6d4b\u7cbe\u5ea6\u5e76\u89e3\u91ca\u8f66\u8f86\u4ea4\u4e92\u3002\u7136\u540e\uff0c\u4f7f\u7528\u5b9a\u5411\u8fb9\u754c\u6846\u68c0\u6d4b\u6765\u8bc6\u522b\u7531\u9884\u6d4b\u8f68\u8ff9\u5e26\u6765\u7684\u76f4\u63a5\u548c\u95f4\u63a5\u78b0\u649e\u98ce\u9669\u3002\u6700\u540e\uff0c\u751f\u6210\u9884\u8b66\u4fe1\u53f7\u544a\u77e5LC\u8f66\u8f86\u6f5c\u5728\u78b0\u649e\u7684\u4f4d\u7f6e\u3002", "result": "\u5728SUMO\u4e2d\u8fdb\u884c\u7684\u4ea4\u901a\u4eff\u771f\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u4ea4\u4e92\u611f\u77e5LCEW\u7cfb\u7edf\u80fd\u591f\u63d0\u5347\u8f66\u8f86\u5b89\u5168\u548c\u6574\u4f53\u4ea4\u901a\u6548\u7387\uff0c\u5e76\u4fc3\u8fdb\u66f4\u81ea\u7136\u7684\u9a7e\u9a76\u884c\u4e3a\u9002\u5e94\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u4ea4\u4e92\u611f\u77e5LCEW\u7cfb\u7edf\u80fd\u591f\u901a\u8fc7\u51c6\u786e\u9884\u6d4b\u591a\u8f66\u8f68\u8ff9\u548c\u8bc6\u522b\u6f5c\u5728\u98ce\u9669\uff0c\u6709\u6548\u5730\u63d0\u5347\u62e5\u5835\u4ea4\u901a\u4e2d\u8f66\u9053\u53d8\u6362\u7684\u5b89\u5168\u6027\u4e0e\u4ea4\u901a\u6548\u7387\u3002"}}
{"id": "2509.19287", "categories": ["cond-mat.mes-hall", "cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2509.19287", "abs": "https://arxiv.org/abs/2509.19287", "authors": ["Zhongdong Han", "Yiyu Xia", "Kenji Watanabe", "Takashi Taniguchi", "Kin Fai Mak", "Jie Shan"], "title": "Quantum oscillations between excitonic and quantum spin Hall insulators in moir\u00e9 WSe2", "comment": null, "summary": "Quantum spin Hall insulators (QSHIs) and excitonic insulators (EIs) are\nprototypical topological and correlated states of matter, respectively. The\ntopological phase transition between the two has attracted much theoretical\ninterest but experimental studies have been hindered by the availability of\ntunable materials that can access such a transition. Here, by utilizing the\ninteraction-enhanced g-factor and the flat moir\\'e bands in twisted bilayer\nWSe2 (tWSe2), we realize tunable electron-like and hole-like Landau levels\n(LLs) in the opposite valleys of tWSe2 under a perpendicular magnetic field. At\nhalf-band-filling, which corresponds to electron-hole charge neutrality,\nperiodic oscillations between QSHIs (for fully filled LLs) and EIs (for\nhalf-filled LLs) are observed due to the interplay between the cyclotron energy\nand the intervalley correlation; QSHIs with up to four pairs of helical edge\nstates can be resolved. We further analyze the effect of Fermi surface nesting\non the stability of EIs via electric field-tuning of the moir\\'e band\nstructure. Our results demonstrate a novel QSHI-to-EI topological phase\ntransition and provide a comprehensive understanding of the fermiology of\ntWSe2.", "AI": {"tldr": "\u626d\u66f2\u53cc\u5c42WSe2 (tWSe2) \u4e2d\u7684\u91cf\u5b50\u81ea\u65cb\u970d\u5c14\u7edd\u7f18\u4f53 (QSHI) \u548c\u6fc0\u5b50\u7edd\u7f18\u4f53 (EI) \u4e4b\u95f4\u7684\u53ef\u8c03\u62d3\u6251\u76f8\u53d8\u3002", "motivation": "\u5b9e\u9a8c\u4e0a\u5b9e\u73b0QSHI\u548cEI\u4e4b\u95f4\u7684\u62d3\u6251\u76f8\u53d8\uff0c\u56e0\u4e3a\u76ee\u524d\u7f3a\u4e4f\u53ef\u7528\u4e8e\u5b9e\u73b0\u8fd9\u79cd\u8f6c\u53d8\u7684\u53ef\u8c03\u6750\u6599\u3002", "method": "\u5229\u7528tWSe2\u4e2d\u76f8\u4e92\u4f5c\u7528\u589e\u5f3a\u7684g\u56e0\u5b50\u548c\u6241\u5e73\u83ab\u5c14\u5e26\uff0c\u5728\u5782\u76f4\u78c1\u573a\u4e0b\u5b9e\u73b0tWSe2\u7684\u4e24\u4e2a\u76f8\u53cd\u8c37\u4e2d\u7684\u7535\u5b50\u578b\u548c\u7a7a\u7a74\u578b\u6717\u9053\u80fd\u7ea7 (LL)\u3002", "result": "\u5728\u534a\u586b\u5145\u72b6\u6001\u4e0b\uff0c\u89c2\u5bdf\u5230QSHI\uff08\u5bf9\u4e8e\u5168\u586b\u5145LL\uff09\u548cEI\uff08\u5bf9\u4e8e\u534a\u586b\u5145LL\uff09\u4e4b\u95f4\u7684\u5468\u671f\u6027\u632f\u8361\u3002QSHI\u5177\u6709\u591a\u8fbe\u56db\u5bf9\u87ba\u65cb\u8fb9\u7f18\u6001\u3002\u901a\u8fc7\u7535\u573a\u8c03\u8c10\u83ab\u5c14\u5e26\u7ed3\u6784\uff0c\u5206\u6790\u4e86\u8d39\u7c73\u9762\u5d4c\u5957\u5bf9EI\u7a33\u5b9a\u6027\u7684\u5f71\u54cd\u3002", "conclusion": "\u9996\u6b21\u6f14\u793a\u4e86QSHI\u5230EI\u7684\u62d3\u6251\u76f8\u53d8\uff0c\u5e76\u5168\u9762\u7406\u89e3\u4e86tWSe2\u7684\u8d39\u7c73\u9762\u7279\u6027\u3002"}}
{"id": "2509.18549", "categories": ["cond-mat.mtrl-sci", "cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2509.18549", "abs": "https://arxiv.org/abs/2509.18549", "authors": ["Jaekyung Jang", "Yu-Seong Seo", "Jeonghun Lee", "Eundeok Mun", "Jungseek Hwang"], "title": "Optical properties of RCd3P3 (R: Ce or La) compounds: Insulator-metal transition induced by displacement of atoms in the unit cell", "comment": "20 pages, 5 figures", "summary": "We examined the electronic structures and optical properties of single\ncrystals of RCd3P3 (R = Ce or La). Our first-principles analysis indicates that\nCeCd3P3 and LaCd3P3 exhibit semiconductor characteristics with narrow energy\ngaps of approximately 0.51 and 0.70 eV, respectively. Notably, a slight\ndisplacement of the Cd and P atoms within the unit cell significantly\ntransforms the electronic structure from insulating to metallic state. Optical\nspectroscopy of both compounds reveals a metallic state with a low charge\ncarrier density, suggesting a finite density of states at the Fermi level. A\ncomparison between the theoretical electronic structures and experimental\noptical properties elucidates the observed metallic behavior. Additionally, the\nnotable modification of the infrared-active phonons strongly indicates a\nstructural phase transition in these compounds. Our findings also suggest that\nCeCd3P3 serves as a suitable platform for investigating the photoinduced Kondo\neffect due to its metallic ground state with limited charge carriers.", "AI": {"tldr": "CeCd3P3 and LaCd3P3 are semiconductors, but atomic displacement can make them metallic. Optical spectroscopy confirms metallic behavior, and changes in phonons suggest a structural phase transition. CeCd3P3 is promising for studying the photoinduced Kondo effect.", "motivation": "Investigate the electronic structures and optical properties of RCd3P3 (R = Ce or La) single crystals to understand their properties and potential applications.", "method": "Performed first-principles analysis and optical spectroscopy on CeCd3P3 and LaCd3P3 single crystals.", "result": "Both compounds exhibit semiconductor characteristics with narrow energy gaps (0.51 eV for CeCd3P3, 0.70 eV for LaCd3P3) theoretically. Atomic displacement leads to a metallic state. Optical spectroscopy reveals a metallic state with low charge carrier density. Significant modification of infrared-active phonons indicates a structural phase transition. CeCd3P3 has a metallic ground state with limited charge carriers.", "conclusion": "The study elucidates the electronic and optical properties of CeCd3P3 and LaCd3P3, highlighting the impact of atomic structure on their electronic states. The compounds show potential for further research, particularly CeCd3P3 for the photoinduced Kondo effect."}}
{"id": "2509.18799", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.18799", "abs": "https://arxiv.org/abs/2509.18799", "authors": ["Sijia Cheng", "Liang Liu", "Ove Edfors", "Juan Vidal Alegria"], "title": "Highly Parallel Singular Value Decomposition for Low-Latency MIMO Processing", "comment": "5 pages, 6 figures, accepted to SiPS2025", "summary": "Singular value decomposition (SVD) is widely used in wireless systems,\nincluding multiple-input multiple-output (MIMO) processing and dimension\nreduction in distributed MIMO (D-MIMO). However, the iterative nature of\ndecomposition methods results in increased execution time as system size grows,\nposing challenges for real-time and low-latency applications. To address this,\nwe analyze the latency of state-of-art SVD methods, and highlight the\nefficiency of a 4-step highly parallel method based on Gram matrix\ntridiagonalization. Furthermore, we develop a time complexity (processing\nlatency) analysis framework with hardware profiling, allowing scalable and\nrealistic evaluation without full implementation. The numerical results\ndemonstrate the superior time efficiency of the selected parallel method,\nparticularly in massive MIMO scenarios.", "AI": {"tldr": "SVD\u5728\u65e0\u7ebf\u7cfb\u7edf\u4e2d\u6709\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5176\u8fed\u4ee3\u6027\u8d28\u9650\u5236\u4e86\u5b9e\u65f6\u6027\u3002\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eGram\u77e9\u9635\u4e09\u5bf9\u89d2\u5316\u76844\u6b65\u5e76\u884c\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u786c\u4ef6\u5256\u6790\u8fdb\u884c\u65f6\u95f4\u590d\u6742\u5ea6\u5206\u6790\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u6d77\u91cfMIMO\u573a\u666f\u4e0b\u7684\u4f18\u8d8a\u65f6\u95f4\u6548\u7387\u3002", "motivation": "SVD\u5728MIMO\u7b49\u65e0\u7ebf\u7cfb\u7edf\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u9762\u4e34\u8fed\u4ee3\u6027\u8d28\u5e26\u6765\u7684\u9ad8\u6267\u884c\u65f6\u95f4\u95ee\u9898\uff0c\u8fd9\u5bf9\u4e8e\u5b9e\u65f6\u548c\u4f4e\u5ef6\u8fdf\u5e94\u7528\u6784\u6210\u4e86\u6311\u6218\u3002", "method": "\u5206\u6790\u4e86\u73b0\u6709SVD\u65b9\u6cd5\u7684\u5ef6\u8fdf\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eGram\u77e9\u9635\u4e09\u5bf9\u89d2\u5316\u76844\u6b65\u9ad8\u5ea6\u5e76\u884c\u65b9\u6cd5\u3002\u5f00\u53d1\u4e86\u4e00\u4e2a\u5305\u542b\u786c\u4ef6\u5256\u6790\u7684\u65f6\u95f4\u590d\u6742\u5ea6\u5206\u6790\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u5b8c\u5168\u5b9e\u73b0\u5373\u53ef\u8fdb\u884c\u53ef\u6269\u5c55\u548c\u73b0\u5b9e\u7684\u8bc4\u4f30\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u9009\u7684\u5e76\u884c\u65b9\u6cd5\u5728\u65f6\u95f4\u6548\u7387\u4e0a\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u5c24\u5176\u662f\u5728\u6d77\u91cfMIMO\u573a\u666f\u4e0b\u3002", "conclusion": "\u57fa\u4e8eGram\u77e9\u9635\u4e09\u5bf9\u89d2\u5316\u76844\u6b65\u5e76\u884cSVD\u65b9\u6cd5\u5728\u5904\u7406\u5927\u89c4\u6a21MIMO\u7cfb\u7edf\u65f6\uff0c\u6bd4\u4f20\u7edf\u65b9\u6cd5\u5177\u6709\u66f4\u4f18\u8d8a\u7684\u65f6\u95f4\u6548\u7387\u3002"}}
{"id": "2509.18407", "categories": ["cs.RO", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.18407", "abs": "https://arxiv.org/abs/2509.18407", "authors": ["Navya Tiwari", "Joseph Vazhaeparampil", "Victoria Preston"], "title": "Assistive Decision-Making for Right of Way Navigation at Uncontrolled Intersections", "comment": "6 pages, 5 figures. Accepted as a poster at Northeast Robotics\n  Colloquium (NERC 2025). Extended abstract", "summary": "Uncontrolled intersections account for a significant fraction of roadway\ncrashes due to ambiguous right-of-way rules, occlusions, and unpredictable\ndriver behavior. While autonomous vehicle research has explored\nuncertainty-aware decision making, few systems exist to retrofit human-operated\nvehicles with assistive navigation support. We present a driver-assist\nframework for right-of-way reasoning at uncontrolled intersections, formulated\nas a Partially Observable Markov Decision Process (POMDP). Using a custom\nsimulation testbed with stochastic traffic agents, pedestrians, occlusions, and\nadversarial scenarios, we evaluate four decision-making approaches: a\ndeterministic finite state machine (FSM), and three probabilistic planners:\nQMDP, POMCP, and DESPOT. Results show that probabilistic planners outperform\nthe rule-based baseline, achieving up to 97.5 percent collision-free navigation\nunder partial observability, with POMCP prioritizing safety and DESPOT\nbalancing efficiency and runtime feasibility. Our findings highlight the\nimportance of uncertainty-aware planning for driver assistance and motivate\nfuture integration of sensor fusion and environment perception modules for\nreal-time deployment in realistic traffic environments.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u65e0\u63a7\u5236\u4ea4\u53c9\u8def\u53e3\u7684\u9a7e\u9a76\u5458\u8f85\u52a9\u6846\u67b6\uff0c\u901a\u8fc7\u90e8\u5206\u53ef\u89c2\u5bdf\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08POMDP\uff09\u89e3\u51b3\u8def\u6743\u95ee\u9898\uff0c\u5e76\u5728\u6a21\u62df\u73af\u5883\u4e2d\u8bc4\u4f30\u4e86\u56db\u79cd\u51b3\u7b56\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u6982\u7387\u89c4\u5212\u5668\u5728\u51cf\u5c11\u78b0\u649e\u65b9\u9762\u7684\u4f18\u8d8a\u6027\u3002", "motivation": "\u65e0\u63a7\u5236\u4ea4\u53c9\u8def\u53e3\u56e0\u89c4\u5219\u6a21\u7cca\u3001\u89c6\u91ce\u906e\u6321\u548c\u4e0d\u53ef\u9884\u6d4b\u7684\u9a7e\u9a76\u884c\u4e3a\u5bfc\u81f4\u4ea4\u901a\u4e8b\u6545\u9891\u53d1\u3002\u73b0\u6709\u81ea\u52a8\u9a7e\u9a76\u7814\u7a76\u867d\u5173\u6ce8\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u51b3\u7b56\uff0c\u4f46\u9488\u5bf9\u4eba\u7c7b\u9a7e\u9a76\u8f66\u8f86\u7684\u5bfc\u822a\u8f85\u52a9\u7cfb\u7edf\u5374\u5f88\u5c11\u3002", "method": "\u5c06\u65e0\u63a7\u5236\u4ea4\u53c9\u8def\u53e3\u7684\u8b72\u8def\u95ee\u9898\u6784\u5efa\u4e3a\u90e8\u5206\u53ef\u89c2\u5bdf\u9a6c\u5c14\u53ef\u5728\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08POMDP\uff09\u3002\u4f7f\u7528\u5305\u542b\u968f\u673a\u4ea4\u901a\u3001\u884c\u4eba\u3001\u906e\u6321\u548c\u5bf9\u6297\u6027\u573a\u666f\u7684\u81ea\u5b9a\u4e49\u6a21\u62df\u6d4b\u8bd5\u5e73\u53f0\uff0c\u8bc4\u4f30\u4e86\u786e\u5b9a\u6027\u6709\u9650\u72b6\u6001\u673a\uff08FSM\uff09\u3001QMDP\u3001POMCP\u548cDESPOT\u8fd9\u56db\u79cd\u51b3\u7b56\u65b9\u6cd5\u3002", "result": "\u6982\u7387\u89c4\u5212\u5668\u4f18\u4e8e\u57fa\u4e8e\u89c4\u5219\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u90e8\u5206\u53ef\u89c2\u5bdf\u6027\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u8fbe97.5%\u7684\u65e0\u78b0\u649e\u5bfc\u822a\u3002\u5176\u4e2d\uff0cPOMCP\u4f18\u5148\u8003\u8651\u5b89\u5168\uff0c\u800cDESPOT\u5728\u6548\u7387\u548c\u8fd0\u884c\u65f6\u95f4\u53ef\u884c\u6027\u4e4b\u95f4\u53d6\u5f97\u4e86\u5e73\u8861\u3002", "conclusion": "\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u89c4\u5212\u5bf9\u4e8e\u9a7e\u9a76\u5458\u8f85\u52a9\u81f3\u5173\u91cd\u8981\u3002\u672a\u6765\u7684\u7814\u7a76\u5e94\u4fa7\u91cd\u4e8e\u878d\u5408\u4f20\u611f\u5668\u548c\u73af\u5883\u611f\u77e5\u6a21\u5757\uff0c\u4ee5\u5b9e\u73b0\u771f\u5b9e\u7684\u5b9e\u65f6\u4ea4\u901a\u73af\u5883\u90e8\u7f72\u3002"}}
{"id": "2509.18177", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18177", "abs": "https://arxiv.org/abs/2509.18177", "authors": ["George Corr\u00eaa de Ara\u00fajo", "Helena de Almeida Maia", "Helio Pedrini"], "title": "A Framework for Generating Artificial Datasets to Validate Absolute and Relative Position Concepts", "comment": "WIP", "summary": "In this paper, we present the Scrapbook framework, a novel methodology\ndesigned to generate extensive datasets for probing the learned concepts of\nartificial intelligence (AI) models. The framework focuses on fundamental\nconcepts such as object recognition, absolute and relative positions, and\nattribute identification. By generating datasets with a large number of\nquestions about individual concepts and a wide linguistic variation, the\nScrapbook framework aims to validate the model's understanding of these basic\nelements before tackling more complex tasks. Our experimental findings reveal\nthat, while contemporary models demonstrate proficiency in recognizing and\nenumerating objects, they encounter challenges in comprehending positional\ninformation and addressing inquiries with additional constraints. Specifically,\nthe MobileVLM-V2 model showed significant answer disagreements and plausible\nwrong answers, while other models exhibited a bias toward affirmative answers\nand struggled with questions involving geometric shapes and positional\ninformation, indicating areas for improvement in understanding and consistency.\nThe proposed framework offers a valuable instrument for generating diverse and\ncomprehensive datasets, which can be utilized to systematically assess and\nenhance the performance of AI models.", "AI": {"tldr": "Scrapbook\u6846\u67b6\u751f\u6210\u6570\u636e\u96c6\u4ee5\u63a2\u6d4bAI\u6a21\u578b\u5bf9\u57fa\u672c\u6982\u5ff5\u7684\u7406\u89e3\uff0c\u53d1\u73b0\u6a21\u578b\u5728\u7269\u4f53\u8bc6\u522b\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u4f4d\u7f6e\u7406\u89e3\u548c\u7ea6\u675f\u95ee\u9898\u65b9\u9762\u5b58\u5728\u6311\u6218\u3002", "motivation": "\u4e3a\u4e86\u751f\u6210\u7528\u4e8e\u63a2\u6d4bAI\u6a21\u578b\u6240\u5b66\u6982\u5ff5\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u7279\u522b\u662f\u9488\u5bf9\u7269\u4f53\u8bc6\u522b\u3001\u7edd\u5bf9/\u76f8\u5bf9\u4f4d\u7f6e\u548c\u5c5e\u6027\u8bc6\u522b\u7b49\u57fa\u672c\u6982\u5ff5\u3002", "method": "\u63d0\u51faScrapbook\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u751f\u6210\u5927\u91cf\u5173\u4e8e\u5355\u4e2a\u6982\u5ff5\u7684\u95ee\u9898\uff0c\u5e76\u91c7\u7528\u5e7f\u6cdb\u7684\u8bed\u8a00\u53d8\u5f02\u6027\u6765\u751f\u6210\u6570\u636e\u96c6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5f53\u4ee3\u6a21\u578b\u5728\u7269\u4f53\u8bc6\u522b\u548c\u679a\u4e3e\u65b9\u9762\u8868\u73b0\u51fa\u719f\u7ec3\u5ea6\uff0c\u4f46\u5728\u7406\u89e3\u4f4d\u7f6e\u4fe1\u606f\u548c\u5904\u7406\u9644\u52a0\u7ea6\u675f\u7684\u95ee\u9898\u65b9\u9762\u9047\u5230\u6311\u6218\u3002MobileVLM-V2\u6a21\u578b\u5728\u7b54\u6848\u4e00\u81f4\u6027\u548c\u5408\u7406\u9519\u8bef\u7b54\u6848\u65b9\u9762\u8868\u73b0\u51fa\u663e\u8457\u95ee\u9898\uff0c\u5176\u4ed6\u6a21\u578b\u5219\u503e\u5411\u4e8e\u80af\u5b9a\u6027\u7b54\u6848\uff0c\u5e76\u96be\u4ee5\u5904\u7406\u6d89\u53ca\u51e0\u4f55\u5f62\u72b6\u548c\u4f4d\u7f6e\u4fe1\u606f\u7684\u95ee\u9898\u3002", "conclusion": "Scrapbook\u6846\u67b6\u4e3a\u751f\u6210\u591a\u6837\u5316\u3001\u5168\u9762\u7684\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u4ef7\u503c\u7684\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u7cfb\u7edf\u5730\u8bc4\u4f30\u548c\u63d0\u9ad8AI\u6a21\u578b\u7684\u6027\u80fd\u3002"}}
{"id": "2509.18294", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2509.18294", "abs": "https://arxiv.org/abs/2509.18294", "authors": ["Refaat Ismail", "I-Chi Chen", "Chen Zhao", "Ronen Weiss", "Fangli Liu", "Hengyun Zhou", "Sheng-Tao Wang", "Andrew Sornborger", "Milan Kornja\u010da"], "title": "Transversal STAR architecture for megaquop-scale quantum simulation with neutral atoms", "comment": "32 pages, 15 figures", "summary": "Quantum computing experiments have made remarkable progress in demonstrating\nkey components of quantum error correction, a prerequisite for scalable quantum\ncomputation. While we anticipate the arrival of early fault-tolerant quantum\nhardware capable of a million reliable quantum operations, the cost of\npreparing low-noise `magic resource states' presents a formidable challenge.\nThe recently proposed partially-fault-tolerant architecture based on a\nspace-time efficient analog rotation (STAR) approach attempts to address this\nchallenge by using post-selection to prepare low-noise, small-angle magic\nstates. Its proposed physical implementation, however, assumes fixed qubit\nconnectivity, resulting in implementation costs closer to leading\nfully-fault-tolerant approaches. Here, we propose the transversal STAR\narchitecture and co-design it with neutral-atom quantum hardware, deriving\nsignificant savings in logical layout, time, and space overhead. Through\ncircuit-level simulations, we derive the logical noise model for\nsurface-code-based transversal STAR gadgets and verify their composability. At\nits limit, the transversal STAR architecture can efficiently simulate local\nHamiltonians with a total simulation volume exceeding 600. Achieving this limit\nwould require approximately 10,000 physical qubits at a physical error rate of\n$10^{-3}$. This is equivalent to a fully-fault-tolerant computation requiring\nover $10^6$-$10^7$ $T$ gates. Finally, we extend the transversal STAR\narchitecture to high-rate quantum codes, demonstrating how a limited set of\nhighly parallel transversal Clifford gates and generalized small-angle magic\ninjection can be utilized for effective quantum simulation. We anticipate that\nthe co-designed transversal STAR architecture could substantially reduce the\nphysical resources necessary for early-fault-tolerant quantum simulation at the\nmegaquop scale.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e0e\u4e2d\u6027\u539f\u5b50\u91cf\u5b50\u786c\u4ef6\u5171\u540c\u8bbe\u8ba1\u7684\u201c\u6a2a\u8d2fSTAR\u201d\u67b6\u6784\uff0c\u4ee5\u964d\u4f4e\u65e9\u671f\u5bb9\u9519\u91cf\u5b50\u6a21\u62df\u7684\u8d44\u6e90\u5f00\u9500\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8eSTAR\u7684\u91cf\u5b50\u7ea0\u9519\u65b9\u6cd5\u5728\u5b9e\u73b0\u4f4e\u566a\u58f0\u201c\u9b54\u672f\u8d44\u6e90\u6001\u201d\u5236\u5907\u65f6\uff0c\u7531\u4e8e\u56fa\u5b9a\u7684\u91cf\u5b50\u6bd4\u7279\u8fde\u63a5\u6027\uff0c\u5bfc\u81f4\u6210\u672c\u8f83\u9ad8\u3002", "method": "\u63d0\u51fa\u201c\u6a2a\u8d2fSTAR\u201d\u67b6\u6784\uff0c\u5e76\u4e0e\u4e2d\u6027\u539f\u5b50\u91cf\u5b50\u786c\u4ef6\u8fdb\u884c\u534f\u540c\u8bbe\u8ba1\uff0c\u5229\u7528\u7535\u8def\u7ea7\u6a21\u62df\u63a8\u5bfc\u4e86\u57fa\u4e8e\u8868\u9762\u7801\u7684\u6a2a\u8d2fSTAR\u5c0f\u5de5\u5177\u7684\u903b\u8f91\u566a\u58f0\u6a21\u578b\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u53ef\u7ec4\u5408\u6027\u3002", "result": "\u201c\u6a2a\u8d2fSTAR\u201d\u67b6\u6784\u5728\u903b\u8f91\u5e03\u5c40\u3001\u65f6\u95f4\u548c\u7a7a\u95f4\u5f00\u9500\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u8282\u7701\uff0c\u5728\u6781\u9650\u60c5\u51b5\u4e0b\u53ef\u6a21\u62df\u5c40\u90e8\u54c8\u5bc6\u987f\u91cf\uff0c\u6a21\u62df\u603b\u91cf\u8d85600\u3002\u5b9e\u73b0\u8be5\u6781\u9650\u7ea6\u970010,000\u4e2a\u7269\u7406\u91cf\u5b50\u6bd4\u7279\uff0c\u7269\u7406\u9519\u8bef\u7387\u4e3a10^-3\uff0c\u8fd9\u76f8\u5f53\u4e8e\u5b8c\u5168\u5bb9\u9519\u8ba1\u7b97\u6240\u9700\u768410^6-10^7 T\u95e8\u3002\u6b64\u5916\uff0c\u8be5\u67b6\u6784\u53ef\u6269\u5c55\u81f3\u9ad8\u7801\u7387\u91cf\u5b50\u7801\u3002", "conclusion": "\u534f\u540c\u8bbe\u8ba1\u7684\u201c\u6a2a\u8d2fSTAR\u201d\u67b6\u6784\u6709\u671b\u5927\u5e45\u51cf\u5c11\u65e9\u671f\u5bb9\u9519\u91cf\u5b50\u6a21\u62df\uff08\u8fbe\u5230\u767e\u4e07\u91cf\u5b50\u64cd\u4f5c\u89c4\u6a21\uff09\u6240\u9700\u7684\u7269\u7406\u8d44\u6e90\u3002"}}
{"id": "2509.18109", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18109", "abs": "https://arxiv.org/abs/2509.18109", "authors": ["Jonatan Katz Nielsen"], "title": "Machine Learning-Based Classification of Vessel Types in Straits Using AIS Tracks", "comment": null, "summary": "Accurate recognition of vessel types from Automatic Identification System\n(AIS) tracks is essential for safety oversight and combating illegal,\nunreported, and unregulated (IUU) activity. This paper presents a strait-scale,\nmachine-learning pipeline that classifies moving vessels using only AIS data.\nWe analyze eight days of historical AIS from the Danish Maritime Authority\ncovering the Bornholm Strait in the Baltic Sea (January 22-30, 2025). After\nforward/backward filling voyage records, removing kinematic and geospatial\noutliers, and segmenting per-MMSI tracks while excluding stationary periods\n($\\ge 1$ h), we derive 31 trajectory-level features spanning kinematics (e.g.,\nSOG statistics), temporal, geospatial (Haversine distances, spans), and\nship-shape attributes computed from AIS A/B/C/D reference points (length,\nwidth, aspect ratio, bridge-position ratio). To avoid leakage, we perform\ngrouped train/test splits by MMSI and use stratified 5-fold cross-validation.\nAcross five classes (cargo, tanker, passenger, high-speed craft, fishing;\nN=1{,}910 trajectories; test=382), tree-based models dominate: a Random Forest\nwith SMOTE attains 92.15% accuracy (macro-precision 94.11%, macro-recall\n92.51%, macro-F1 93.27%) on the held-out test set, while a tuned RF reaches\none-vs-rest ROC-AUC up to 0.9897. Feature-importance analysis highlights the\nbridge-position ratio and maximum SOG as the most discriminative signals;\nprincipal errors occur between cargo and tanker, reflecting similar transit\nbehavior. We demonstrate operational value by backfilling missing ship types on\nunseen data and discuss improvements such as DBSCAN based trip segmentation and\ngradient-boosted ensembles to handle frequent-stop ferries and further lift\nperformance. The results show that lightweight features over AIS trajectories\nenable real-time vessel type classification in straits.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eAIS\u6570\u636e\u7684\u8239\u8236\u7c7b\u578b\u8bc6\u522b\u65b9\u6cd5\uff0c\u5229\u7528\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5bf9\u822a\u884c\u8f68\u8ff9\u8fdb\u884c\u5206\u7c7b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u51c6\u786e\u7684\u8239\u8236\u7c7b\u578b\u8bc6\u522b\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u6d77\u4e0a\u5b89\u5168\u548c\u6253\u51fb\u975e\u6cd5\u3001\u672a\u62a5\u544a\u548c\u672a\u89c4\u8303\uff08IUU\uff09\u6d3b\u52a8\uff0c\u9700\u8981\u51c6\u786e\u8bc6\u522b\u81ea\u52a8\u8bc6\u522b\u7cfb\u7edf\uff08AIS\uff09\u8f68\u8ff9\u4e2d\u7684\u8239\u8236\u7c7b\u578b\u3002", "method": "\u8be5\u65b9\u6cd5\u4f7f\u75288\u5929\u7684AIS\u5386\u53f2\u6570\u636e\uff0c\u7ecf\u8fc7\u6570\u636e\u9884\u5904\u7406\uff08\u586b\u5145\u3001\u53bb\u566a\u3001\u5206\u6bb5\uff09\uff0c\u63d0\u53d6\u4e8631\u4e2a\u8f68\u8ff9\u7ea7\u522b\u7279\u5f81\uff08\u8fd0\u52a8\u5b66\u3001\u65f6\u95f4\u3001\u5730\u7406\u7a7a\u95f4\u3001\u8239\u4f53\u5f62\u72b6\uff09\uff0c\u5e76\u91c7\u7528\u968f\u673a\u68ee\u6797\u6a21\u578b\u8fdb\u884c\u5206\u7c7b\uff0c\u901a\u8fc75\u6298\u4ea4\u53c9\u9a8c\u8bc1\u548c\u6309MMSI\u5206\u7ec4\u7684\u8bad\u7ec3/\u6d4b\u8bd5\u5206\u5272\u6765\u907f\u514d\u6570\u636e\u6cc4\u9732\u3002", "result": "\u968f\u673a\u68ee\u6797\u6a21\u578b\u7ed3\u5408SMOTE\u7b97\u6cd5\u5728\u6d4b\u8bd5\u96c6\u4e0a\u8fbe\u5230\u4e8692.15%\u7684\u51c6\u786e\u7387\uff0c\u6700\u5927\u5316ROC-AUC\u8fbe\u52300.9897\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u6865\u4f4d\u6bd4\u548c\u6700\u5927\u8239\u901f\u662f\u533a\u5206\u8239\u8236\u7c7b\u578b\u7684\u5173\u952e\u7279\u5f81\uff0c\u800c\u8d27\u8239\u548c\u6cb9\u8239\u4e4b\u95f4\u6700\u5bb9\u6613\u6df7\u6dc6\u3002", "conclusion": "\u8f7b\u91cf\u7ea7\u7279\u5f81\u63d0\u53d6\u548c\u57fa\u4e8eAIS\u8f68\u8ff9\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u80fd\u591f\u5b9e\u65f6\u51c6\u786e\u5730\u5bf9\u6d77\u5ce1\u4e2d\u7684\u8239\u8236\u7c7b\u578b\u8fdb\u884c\u5206\u7c7b\uff0c\u5e76\u5177\u6709\u8fdb\u4e00\u6b65\u6539\u8fdb\u7684\u7a7a\u95f4\u3002"}}
{"id": "2509.18181", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18181", "abs": "https://arxiv.org/abs/2509.18181", "authors": ["Mustafa Sameen", "Xiaojian Zhang", "Xilei Zhao"], "title": "Synthesizing Attitudes, Predicting Actions (SAPA): Behavioral Theory-Guided LLMs for Ridesourcing Mode Choice Modeling", "comment": null, "summary": "Accurate modeling of ridesourcing mode choices is essential for designing and\nimplementing effective traffic management policies for reducing congestion,\nimproving mobility, and allocating resources more efficiently. Existing models\nfor predicting ridesourcing mode choices often suffer from limited predictive\naccuracy due to their inability to capture key psychological factors, and are\nfurther challenged by severe class imbalance, as ridesourcing trips comprise\nonly a small fraction of individuals' daily travel. To address these\nlimitations, this paper introduces the Synthesizing Attitudes, Predicting\nActions (SAPA) framework, a hierarchical approach that uses Large Language\nModels (LLMs) to synthesize theory-grounded latent attitudes to predict\nridesourcing choices. SAPA first uses an LLM to generate qualitative traveler\npersonas from raw travel survey data and then trains a propensity-score model\non demographic and behavioral features, enriched by those personas, to produce\nan individual-level score. Next, the LLM assigns quantitative scores to\ntheory-driven latent variables (e.g., time and cost sensitivity), and a final\nclassifier integrates the propensity score, latent-variable scores (with their\ninteraction terms), and observable trip attributes to predict ridesourcing mode\nchoice. Experiments on a large-scale, multi-year travel survey show that SAPA\nsignificantly outperforms state-of-the-art baselines, improving ridesourcing\nchoice predictions by up to 75.9% in terms of PR-AUC on a held-out test set.\nThis study provides a powerful tool for accurately predicting ridesourcing mode\nchoices, and provides a methodology that is readily transferable to various\napplications.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSAPA\u7684\u65b0\u6846\u67b6\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6765\u5408\u6210\u6f5c\u5728\u6001\u5ea6\uff0c\u4ee5\u63d0\u9ad8\u7f51\u7ea6\u8f66\u51fa\u884c\u65b9\u5f0f\u9009\u62e9\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u56e0\u96be\u4ee5\u6355\u6349\u5fc3\u7406\u56e0\u7d20\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u800c\u5bfc\u81f4\u7684\u9884\u6d4b\u7cbe\u5ea6\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u51c6\u786e\u7684\u7f51\u7ea6\u8f66\u51fa\u884c\u6a21\u5f0f\u9009\u62e9\u6a21\u578b\u5bf9\u4e8e\u8bbe\u8ba1\u6709\u6548\u7684\u4ea4\u901a\u7ba1\u7406\u653f\u7b56\u81f3\u5173\u91cd\u8981\uff0c\u4ee5\u51cf\u5c11\u62e5\u5835\u3001\u6539\u5584\u51fa\u884c\u548c\u66f4\u6709\u6548\u5730\u5206\u914d\u8d44\u6e90\u3002\u73b0\u6709\u6a21\u578b\u5728\u8fd9\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "SAPA\u6846\u67b6\u9996\u5148\u5229\u7528LLM\u6839\u636e\u539f\u59cb\u51fa\u884c\u8c03\u67e5\u6570\u636e\u751f\u6210\u5b9a\u6027\u51fa\u884c\u8005\u753b\u50cf\uff0c\u7136\u540e\u8bad\u7ec3\u4e00\u4e2a\u503e\u5411\u5f97\u5206\u6a21\u578b\uff0c\u5e76\u7ed3\u5408\u753b\u50cf\u4fe1\u606f\u6765\u751f\u6210\u4e2a\u4f53\u5f97\u5206\u3002\u63a5\u7740\uff0cLLM\u4e3a\u6f5c\u5728\u53d8\u91cf\uff08\u5982\u65f6\u95f4\u548c\u6210\u672c\u654f\u611f\u6027\uff09\u5206\u914d\u5b9a\u91cf\u5206\u6570\u3002\u6700\u540e\uff0c\u4e00\u4e2a\u5206\u7c7b\u5668\u6574\u5408\u503e\u5411\u5f97\u5206\u3001\u6f5c\u5728\u53d8\u91cf\u5f97\u5206\uff08\u53ca\u5176\u4ea4\u4e92\u9879\uff09\u4ee5\u53ca\u53ef\u89c2\u5bdf\u7684\u51fa\u884c\u5c5e\u6027\u6765\u9884\u6d4b\u7f51\u7ea6\u8f66\u51fa\u884c\u65b9\u5f0f\u9009\u62e9\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSAPA\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u6a21\u578b\uff0c\u5728\u6d4b\u8bd5\u96c6\u4e0a\u7684PR-AUC\u65b9\u9762\uff0c\u7f51\u7ea6\u8f66\u51fa\u884c\u9009\u62e9\u9884\u6d4b\u7684\u51c6\u786e\u6027\u63d0\u9ad8\u4e86\u9ad8\u8fbe75.9%\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u51c6\u786e\u9884\u6d4b\u7f51\u7ea6\u8f66\u51fa\u884c\u65b9\u5f0f\u9009\u62e9\u7684\u5f3a\u5927\u5de5\u5177\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u79cd\u6613\u4e8e\u8fc1\u79fb\u5230\u5404\u79cd\u5e94\u7528\u7684\u65b9\u6cd5\u3002"}}
{"id": "2509.18175", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18175", "abs": "https://arxiv.org/abs/2509.18175", "authors": ["Aditi Debsharma", "Bhushan Jagyasi", "Surajit Sen", "Priyanka Pandey", "Devicharith Dovari", "Yuvaraj V. C", "Rosalin Parida", "Gopali Contractor"], "title": "ERFC: Happy Customers with Emotion Recognition and Forecasting in Conversation in Call Centers", "comment": "7 pages, 6 Figures, 4 Tables, 18 References", "summary": "Emotion Recognition in Conversation has been seen to be widely applicable in\ncall center analytics, opinion mining, finance, retail, healthcare, and other\nindustries. In a call center scenario, the role of the call center agent is not\njust confined to receiving calls but to also provide good customer experience\nby pacifying the frustration or anger of the customers. This can be achieved by\nmaintaining neutral and positive emotion from the agent. As in any\nconversation, the emotion of one speaker is usually dependent on the emotion of\nother speaker. Hence the positive emotion of an agent, accompanied with the\nright resolution will help in enhancing customer experience. This can change an\nunhappy customer to a happy one. Imparting the right resolution at right time\nbecomes easier if the agent has the insight of the emotion of future\nutterances. To predict the emotions of the future utterances we propose a novel\narchitecture, Emotion Recognition and Forecasting in Conversation. Our proposed\nERFC architecture considers multi modalities, different attributes of emotion,\ncontext and the interdependencies of the utterances of the speakers in the\nconversation. Our intensive experiments on the IEMOCAP dataset have shown the\nfeasibility of the proposed ERFC. This approach can provide a tremendous\nbusiness value for the applications like call center, where the happiness of\ncustomer is utmost important.", "AI": {"tldr": "\u901a\u8fc7 ERFC \u67b6\u6784\u9884\u6d4b\u5bf9\u8bdd\u4e2d\u7684\u672a\u6765\u60c5\u7eea\uff0c\u4ee5\u63d0\u9ad8\u5ba2\u6237\u6ee1\u610f\u5ea6\u3002", "motivation": "\u60c5\u7eea\u8bc6\u522b\u5728\u547c\u53eb\u4e2d\u5fc3\u7b49\u884c\u4e1a\u6709\u5e7f\u6cdb\u5e94\u7528\uff0c\u5c24\u5176\u662f\u5728\u5ba2\u670d\u573a\u666f\u4e2d\uff0c\u5ba2\u670d\u9700\u8981\u5b89\u629a\u5ba2\u6237\u4ee5\u63d0\u4f9b\u66f4\u597d\u7684\u5ba2\u6237\u4f53\u9a8c\u3002\u4e86\u89e3\u5ba2\u6237\u7684\u672a\u6765\u60c5\u7eea\u6709\u52a9\u4e8e\u5ba2\u670d\u63d0\u4f9b\u66f4\u53ca\u65f6\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a ERFC \u7684\u65b0\u67b6\u6784\uff0c\u8be5\u67b6\u6784\u80fd\u591f\u8003\u8651\u591a\u6a21\u6001\u3001\u4e0d\u540c\u7684\u60c5\u7eea\u5c5e\u6027\u3001\u4e0a\u4e0b\u6587\u4ee5\u53ca\u8bf4\u8bdd\u8005\u4e4b\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\uff0c\u6765\u9884\u6d4b\u5bf9\u8bdd\u4e2d\u672a\u6765\u8bdd\u8bed\u7684\u60c5\u7eea\u3002", "result": "\u5728 IEMOCAP \u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86 ERFC \u67b6\u6784\u7684\u53ef\u884c\u6027\u3002", "conclusion": "ERFC \u67b6\u6784\u5728\u547c\u53eb\u4e2d\u5fc3\u7b49\u6ce8\u91cd\u5ba2\u6237\u6ee1\u610f\u5ea6\u7684\u5e94\u7528\u4e2d\u5177\u6709\u5de8\u5927\u7684\u5546\u4e1a\u4ef7\u503c\u3002"}}
{"id": "2509.19187", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.19187", "abs": "https://arxiv.org/abs/2509.19187", "authors": ["J\u00e9r\u00e9mie Chalopin", "Yi-Jun Chang", "Lyuting Chen", "Giuseppe A. Di Luna", "Haoran Zhou"], "title": "Non-Uniform Content-Oblivious Leader Election on Oriented Asynchronous Rings", "comment": null, "summary": "We study the leader election problem in oriented ring networks under\ncontent-oblivious asynchronous message-passing systems, where an adversary may\narbitrarily corrupt message contents.\n  Frei et al. (DISC 2024) presented a uniform terminating leader election\nalgorithm for oriented rings in this setting, with message complexity $O(n\n\\cdot \\mathsf{ID}_{\\max})$ on a ring of size $n$, where $\\mathsf{ID}_{\\max}$ is\nthe largest identifier in the system, this result has been recently extended by\nChalopin et al. (DISC 2025) to unoriented rings.\n  In this paper, we investigate the message complexity of leader election on\nring networks in the content-oblivious model, showing that no uniform algorithm\ncan solve the problem if each process is limited to sending a constant number\nof messages in one direction.\n  Interestingly, this limitation hinges on the uniformity assumption. In the\nnon-uniform setting, where processes know an upper bound $U \\geq n$ on the ring\nsize, we present an algorithm with message complexity $O(n \\cdot U \\cdot\n\\mathsf{ID}_{\\min})$, in which each process sends $O(U \\cdot\n\\mathsf{ID}_{\\min})$ messages clockwise and only three messages\ncounter-clockwise. Here, $\\mathsf{ID}_{\\min}$ is the smallest identifier in the\nsystem. This dependence on the identifiers compares favorably with the\ndependence on $\\mathsf{ID}_{\\max}$ of Frei et al.\n  We also show a non-uniform algorithm where each process sends $O(U \\cdot\n\\log\\mathsf{ID}_{\\min})$ messages in one direction and\n$O(\\log\\mathsf{ID}_{\\min})$ in the other. The factor $\\log \\mathsf{ID}_{\\min}$\nis optimal, matching the lower bound of Frei et al.\n  Finally, in the anonymous setting, where processes do not have identifiers,\nwe propose a randomized algorithm where each process sends only $O(\\log^2 U)$\nmessages, with a success probability of $1 - U^{-c}$.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u6709\u5411\u73af\u7f51\u7edc\u4e2d\u7684\u9886\u5bfc\u8005\u9009\u4e3e\u95ee\u9898\uff0c\u5e76\u5728\u5185\u5bb9\u65e0\u5173\u7684\u5f02\u6b65\u6d88\u606f\u4f20\u9012\u7cfb\u7edf\u4e2d\u63d0\u51fa\u4e86\u65b0\u7684\u7b97\u6cd5\u548c\u4e0b\u754c\u3002", "motivation": "\u73b0\u6709\u7b97\u6cd5\u5728\u6709\u5411\u73af\u7f51\u7edc\u4e2d\u7684\u9886\u5bfc\u8005\u9009\u4e3e\u5b58\u5728\u6d88\u606f\u590d\u6742\u5ea6\u548c\u7edf\u4e00\u6027\u65b9\u9762\u7684\u5c40\u9650\uff0c\u672c\u6587\u65e8\u5728\u63a2\u7d22\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5728\u7edf\u4e00\u6a21\u578b\u4e0b\uff0c\u8bc1\u660e\u4e86\u5728\u53d1\u9001\u6d88\u606f\u6570\u91cf\u53d7\u9650\u7684\u60c5\u51b5\u4e0b\u65e0\u6cd5\u89e3\u51b3\u8be5\u95ee\u9898\u3002\u5728\u975e\u7edf\u4e00\u6a21\u578b\u4e0b\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u7b97\u6cd5\uff0c\u5206\u522b\u5177\u6709 $O(n \tf{cdot} U \tf{cdot} \tf{ID}_{\tf{min}})$ \u548c $O(U \tf{cdot} \tf{ID}_{\tf{min}})$ \u7684\u6d88\u606f\u590d\u6742\u5ea6\u3002\u5728\u533f\u540d\u6a21\u578b\u4e0b\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u968f\u673a\u5316\u7b97\u6cd5\u3002", "result": "\u5728\u7edf\u4e00\u6a21\u578b\u4e0b\uff0c\u8bc1\u660e\u4e86\u65e0\u5e38\u6570\u6d88\u606f\u53d1\u9001\u7684\u7edf\u4e00\u7b97\u6cd5\u4e0d\u5b58\u5728\u3002\u5728\u975e\u7edf\u4e00\u6a21\u578b\u4e0b\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u7b97\u6cd5\uff0c\u6d88\u606f\u590d\u6742\u5ea6\u4f18\u4e8e\u73b0\u6709\u7b97\u6cd5\uff0c\u5e76\u5339\u914d\u4e86 $\tf{log} \tf{ID}_{\tf{min}}$ \u7684\u6700\u4f18\u4e0b\u754c\u3002\u5728\u533f\u540d\u6a21\u578b\u4e0b\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u968f\u673a\u5316\u7b97\u6cd5\uff0c\u6d88\u606f\u590d\u6742\u5ea6\u4e3a $O(\tf{log}^2 U)$\u3002", "conclusion": "\u672c\u6587\u5728\u5185\u5bb9\u65e0\u5173\u6a21\u578b\u4e0b\uff0c\u5bf9\u73af\u7f51\u7edc\u9886\u5bfc\u8005\u9009\u4e3e\u95ee\u9898\u8fdb\u884c\u4e86\u6df1\u5165\u7814\u7a76\uff0c\u5206\u522b\u5728\u7edf\u4e00\u3001\u975e\u7edf\u4e00\u548c\u533f\u540d\u8bbe\u7f6e\u4e0b\u53d6\u5f97\u4e86\u65b0\u7684\u7406\u8bba\u7ed3\u679c\uff0c\u5e76\u5bf9\u7b97\u6cd5\u7684\u590d\u6742\u5ea6\u8fdb\u884c\u4e86\u5206\u6790\u548c\u4f18\u5316\u3002"}}
{"id": "2509.18723", "categories": ["eess.SY", "cs.RO", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.18723", "abs": "https://arxiv.org/abs/2509.18723", "authors": ["Jan-Hendrik Ewering", "Alessandro Papa", "Simon F. G. Ehlers", "Thomas Seel", "Michael Meindl"], "title": "Dual Iterative Learning Control for Multiple-Input Multiple-Output Dynamics with Validation in Robotic Systems", "comment": "11 pages, 4 figures", "summary": "Solving motion tasks autonomously and accurately is a core ability for\nintelligent real-world systems. To achieve genuine autonomy across multiple\nsystems and tasks, key challenges include coping with unknown dynamics and\novercoming the need for manual parameter tuning, which is especially crucial in\ncomplex Multiple-Input Multiple-Output (MIMO) systems.\n  This paper presents MIMO Dual Iterative Learning Control (DILC), a novel\ndata-driven iterative learning scheme for simultaneous tracking control and\nmodel learning, without requiring any prior system knowledge or manual\nparameter tuning. The method is designed for repetitive MIMO systems and\nintegrates seamlessly with established iterative learning control methods. We\nprovide monotonic convergence conditions for both reference tracking error and\nmodel error in linear time-invariant systems.\n  The DILC scheme -- rapidly and autonomously -- solves various motion tasks in\nhigh-fidelity simulations of an industrial robot and in multiple nonlinear\nreal-world MIMO systems, without requiring model knowledge or manually tuning\nthe algorithm. In our experiments, many reference tracking tasks are solved\nwithin 10-20 trials, and even complex motions are learned in less than 100\niterations. We believe that, because of its rapid and autonomous learning\ncapabilities, DILC has the potential to serve as an efficient building block\nwithin complex learning frameworks for intelligent real-world systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMIMO\u53cc\u8fed\u4ee3\u5b66\u4e60\u63a7\u5236\uff08DILC\uff09\u7684\u65b0\u578b\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u8f93\u5165\u591a\u8f93\u51fa\uff08MIMO\uff09\u7cfb\u7edf\u7684\u81ea\u4e3b\u8fd0\u52a8\u4efb\u52a1\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u5148\u9a8c\u77e5\u8bc6\u6216\u624b\u52a8\u8c03\u53c2\uff0c\u5373\u53ef\u540c\u65f6\u5b9e\u73b0\u8ddf\u8e2a\u63a7\u5236\u548c\u6a21\u578b\u5b66\u4e60\uff0c\u5e76\u5728\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u7684MIMO\u7cfb\u7edf\u4e2d\u9a8c\u8bc1\u4e86\u5176\u5feb\u901f\u5b66\u4e60\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u667a\u80fd\u7cfb\u7edf\u5728\u591a\u4efb\u52a1\u548c\u591a\u7cfb\u7edf\u4e2d\u7684\u81ea\u4e3b\u8fd0\u52a8\u80fd\u529b\uff0c\u7279\u522b\u662f\u5e94\u5bf9\u672a\u77e5\u52a8\u529b\u5b66\u548c\u590d\u6742MIMO\u7cfb\u7edf\u624b\u52a8\u8c03\u53c2\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMIMO\u53cc\u8fed\u4ee3\u5b66\u4e60\u63a7\u5236\uff08DILC\uff09\u7684\u6570\u636e\u9a71\u52a8\u8fed\u4ee3\u5b66\u4e60\u65b9\u6848\uff0c\u7528\u4e8e\u540c\u65f6\u8fdb\u884c\u8ddf\u8e2a\u63a7\u5236\u548c\u6a21\u578b\u5b66\u4e60\uff0c\u65e0\u9700\u5148\u9a8c\u7cfb\u7edf\u77e5\u8bc6\u6216\u624b\u52a8\u8c03\u53c2\u3002\u8be5\u65b9\u6848\u9002\u7528\u4e8e\u91cd\u590d\u6027MIMO\u7cfb\u7edf\uff0c\u5e76\u80fd\u4e0e\u73b0\u6709\u7684\u8fed\u4ee3\u5b66\u4e60\u63a7\u5236\u65b9\u6cd5\u878d\u5408\u3002\u7814\u7a76\u7ed9\u51fa\u4e86\u7ebf\u65f6\u4e0d\u53d8\u7cfb\u7edf\u5728\u53c2\u8003\u8ddf\u8e2a\u8bef\u5dee\u548c\u6a21\u578b\u8bef\u5dee\u4e0a\u7684\u5355\u8c03\u6536\u655b\u6761\u4ef6\u3002", "result": "DILC\u65b9\u6848\u5728\u5de5\u4e1a\u673a\u5668\u4eba\u9ad8\u4fdd\u771f\u6a21\u62df\u548c\u591a\u4e2a\u975e\u7ebf\u6027\u771f\u5b9e\u4e16\u754cMIMO\u7cfb\u7edf\u4e2d\uff0c\u5b9e\u73b0\u4e86\u8fd0\u52a8\u4efb\u52a1\u7684\u5feb\u901f\u81ea\u4e3b\u6c42\u89e3\uff0c\u65e0\u9700\u6a21\u578b\u77e5\u8bc6\u6216\u624b\u52a8\u8c03\u53c2\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8bb8\u591a\u53c2\u8003\u8ddf\u8e2a\u4efb\u52a1\u572810-20\u6b21\u8bd5\u9a8c\u5185\u5b8c\u6210\uff0c\u590d\u6742\u8fd0\u52a8\u5b66\u4e60\u5728100\u6b21\u8fed\u4ee3\u5185\u5b8c\u6210\u3002", "conclusion": "DILC\u56e0\u5176\u5feb\u901f\u81ea\u4e3b\u5b66\u4e60\u80fd\u529b\uff0c\u6709\u6f5c\u529b\u6210\u4e3a\u590d\u6742\u667a\u80fd\u771f\u5b9e\u4e16\u754c\u7cfb\u7edf\u5b66\u4e60\u6846\u67b6\u4e2d\u7684\u9ad8\u6548\u6784\u5efa\u6a21\u5757\u3002"}}
{"id": "2509.18556", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2509.18556", "abs": "https://arxiv.org/abs/2509.18556", "authors": ["Minjae Kim", "Hong Gu Lee", "Jungseek Hwang"], "title": "Optical properties of popular dielectric substrate materials in a wide spectral range from far-infrared to ultraviolet", "comment": "16 pages, 6 figures", "summary": "We investigated the optical properties of 13 different dielectric materials\n(slide glass, quartz, Al2O3 (c-cut), DyScO3 (110), KTaO3 (001), LaAlO3 (001),\n(LaAlO3)0.3-(Sr2AlTaO6)0.7 (001) (LSAT), MgF2 (100), MgO (100), SiC, SrTiO3\n(001), TbScO3 (110), and TiO2). The single-bounce reflectance spectra of the\nbulk samples were measured using Fourier transform infrared (FTIR) and\nmonochromatic spectrometers across a wide spectral range, from far infrared to\nultraviolet (80-50,000 cm-1). Using the Kramers-Kronig analysis, we obtained\nthe optical conductivity and dielectric function of the dielectric materials\nfrom their measured reflectance spectra. Moreover, we measured the\ntransmittance spectra of the materials to obtain their bandgaps. We fitted the\nmeasured reflectance spectra using the Lorentz model to obtain phononic\nstructures. Each dielectric material exhibits unique phononic structures and\noptical bandgaps, associated with the composition and crystal structure of the\nmaterial. The observed optical properties of these dielectric materials provide\nvaluable information for the optical analysis of thin films grown on them.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8c03\u67e5\u4e8613\u79cd\u4e0d\u540c\u4ecb\u7535\u6750\u6599\u7684\u5149\u5b66\u7279\u6027\uff0c\u5305\u62ec\u53cd\u5c04\u5149\u8c31\u3001\u5149\u5b66\u7535\u5bfc\u7387\u3001\u4ecb\u7535\u51fd\u6570\u548c\u5e26\u9699\u3002", "motivation": "\u7814\u7a7613\u79cd\u4e0d\u540c\u4ecb\u7535\u6750\u6599\u7684\u5149\u5b66\u7279\u6027\uff0c\u4ee5\u83b7\u5f97\u5176\u5149\u5b66\u7535\u5bfc\u7387\u3001\u4ecb\u7535\u51fd\u6570\u548c\u5e26\u9699\uff0c\u5e76\u4e3a\u751f\u957f\u5728\u8fd9\u4e9b\u6750\u6599\u4e0a\u7684\u8584\u819c\u63d0\u4f9b\u5149\u5b66\u5206\u6790\u4fe1\u606f\u3002", "method": "\u4f7f\u7528\u5085\u91cc\u53f6\u53d8\u6362\u7ea2\u5916\u5149\u8c31\u548c\u5355\u8272\u5149\u8c31\u4eea\u6d4b\u91cf\u4e8613\u79cd\u4e0d\u540c\u4ecb\u7535\u6750\u6599\uff08\u5305\u62ec13\u79cd\u4e0d\u540c\u7684\u6750\u6599\uff09\u7684\u5355\u6b21\u53cd\u5c04\u5149\u8c31\uff0c\u8986\u76d6\u4e86\u4ece\u8fdc\u7ea2\u5916\u5230\u7d2b\u5916\uff0880-50,000 cm-1\uff09\u7684\u5bbd\u5149\u8c31\u8303\u56f4\u3002\u5229\u7528Kramers-Kronig\u5206\u6790\u83b7\u5f97\u5149\u5b66\u7535\u5bfc\u7387\u548c\u4ecb\u7535\u51fd\u6570\u3002\u6d4b\u91cf\u4e86\u900f\u5c04\u5149\u8c31\u4ee5\u83b7\u5f97\u5e26\u9699\u3002\u4f7f\u7528\u6d1b\u4f26\u5179\u6a21\u578b\u62df\u5408\u53cd\u5c04\u5149\u8c31\u4ee5\u83b7\u5f97\u58f0\u5b50\u7ed3\u6784\u3002", "result": "\u6bcf\u79cd\u4ecb\u7535\u6750\u6599\u90fd\u8868\u73b0\u51fa\u72ec\u7279\u7684\u5149\u5b50\u7ed3\u6784\u548c\u5149\u5b66\u5e26\u9699\uff0c\u8fd9\u4e0e\u6750\u6599\u7684\u6210\u5206\u548c\u6676\u4f53\u7ed3\u6784\u6709\u5173\u3002\u7814\u7a76\u7ed3\u679c\u4e3a\u751f\u957f\u5728\u8fd9\u4e9b\u6750\u6599\u4e0a\u7684\u8584\u819c\u7684\u5149\u5b66\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u4fe1\u606f\u3002", "conclusion": "\u6240\u7814\u7a76\u768413\u79cd\u4ecb\u7535\u6750\u6599\u7684\u5149\u5b66\u7279\u6027\uff08\u5305\u62ec\u58f0\u5b50\u7ed3\u6784\u548c\u5149\u5b66\u5e26\u9699\uff09\u4e0e\u5b83\u4eec\u7684\u6210\u5206\u548c\u6676\u4f53\u7ed3\u6784\u76f8\u5173\u3002"}}
{"id": "2509.18853", "categories": ["eess.SP", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2509.18853", "abs": "https://arxiv.org/abs/2509.18853", "authors": ["Xiaolei Li", "Pengyu Wang", "Wenhua Song", "Yangjin Xu", "Wei Gao"], "title": "Normal mode parameters estimation by a VLA in single-shooting", "comment": null, "summary": "This paper proposes an orthogonality-constrained modal search (OCMS) method\nfor estimating modal wavenumbers and modal depth functions using a vertical\nlinear array (VLA). Under the assumption of a known sound speed profile, OCMS\nleverages the orthogonality of distinct modal depth functions to extract both\nthe modal depth functions and their corresponding wavenumbers, even when the\nVLA and a monochromatic sound source remain stationary.The performance of OCMS\nis evaluated through numerical simulations under varying signal-to-noise ratios\n(SNRs), different VLA apertures, varying numbers of VLA elements, VLA tilt and\nsound speed profile (SSP) uncertainty. The results demonstrate that OCMS is\nrobust against noise, VLA aperture variations, and changes in the number of VLA\nelements, meanwhile, the algorithm maintains reliable performance when SSP\nuncertainty < 1 m/s and VLA tilt angle <5{\\deg}. Furthermore, the effectiveness\nof OCMS is validated using SwellEx96 experimental data. The relative error\nbetween the modal wavenumbers derived from experimental data and those computed\nvia Kraken is on the order of $10^{-4}$.", "AI": {"tldr": "OCMS\u662f\u4e00\u79cd\u5229\u7528VLA\u4f30\u8ba1\u6a21\u6001\u6ce2\u6570\u548c\u6a21\u6001\u6df1\u5ea6\u51fd\u6570\u7684\u65b0\u65b9\u6cd5\uff0c\u5728\u5df2\u77e5\u58f0\u901f\u5256\u9762\u7684\u60c5\u51b5\u4e0b\uff0c\u5229\u7528\u6a21\u6001\u6df1\u5ea6\u51fd\u6570\u7684\u6b63\u4ea4\u6027\u8fdb\u884c\u63d0\u53d6\u3002\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u4fe1\u566a\u6bd4\u3001VLA\u5b54\u5f84\u3001VLA\u9635\u5143\u6570\u3001VLA\u503e\u659c\u548c\u58f0\u901f\u5256\u9762\u4e0d\u786e\u5b9a\u6027\u4e0b\u8868\u73b0\u7a33\u5065\uff0c\u5e76\u901a\u8fc7SwellEx96\u5b9e\u9a8c\u6570\u636e\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4ece\u5782\u76f4\u7ebf\u6027\u9635\u5217\uff08VLA\uff09\u6570\u636e\u4e2d\u4f30\u8ba1\u6a21\u6001\u6ce2\u6570\u548c\u6a21\u6001\u6df1\u5ea6\u51fd\u6570\uff0c\u5c24\u5176\u662f\u5728VLA\u548c\u58f0\u6e90\u9759\u6b62\u4e14\u58f0\u901f\u5256\u9762\u5df2\u77e5\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u6b63\u4ea4\u7ea6\u675f\u6a21\u6001\u641c\u7d22\uff08OCMS\uff09\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u4e0d\u540c\u6a21\u6001\u6df1\u5ea6\u51fd\u6570\u7684\u6b63\u4ea4\u6027\u6765\u63d0\u53d6\u6a21\u6001\u6df1\u5ea6\u51fd\u6570\u53ca\u5176\u5bf9\u5e94\u7684\u6ce2\u6570\u3002", "result": "OCMS\u5728\u4e0d\u540c\u4fe1\u566a\u6bd4\u3001VLA\u5b54\u5f84\u3001VLA\u9635\u5143\u6570\u3001VLA\u503e\u659c\u548c\u58f0\u901f\u5256\u9762\u4e0d\u786e\u5b9a\u6027\u4e0b\u8868\u73b0\u51fa\u7a33\u5065\u6027\u3002\u5728SSP\u4e0d\u786e\u5b9a\u6027\u5c0f\u4e8e1 m/s\u4e14VLA\u503e\u659c\u89d2\u5c0f\u4e8e5\u5ea6\u65f6\uff0c\u7b97\u6cd5\u6027\u80fd\u53ef\u9760\u3002SwellEx96\u5b9e\u9a8c\u6570\u636e\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5b9e\u9a8c\u6570\u636e\u4e0eKraken\u8ba1\u7b97\u7ed3\u679c\u7684\u6a21\u6001\u6ce2\u6570\u76f8\u5bf9\u8bef\u5dee\u572810^{-4}\u7ea7\u522b\u3002", "conclusion": "OCMS\u662f\u4e00\u79cd\u6709\u6548\u7684\u3001\u7a33\u5065\u7684\u4eceVLA\u6570\u636e\u4e2d\u4f30\u8ba1\u6a21\u6001\u6ce2\u6570\u548c\u6a21\u6001\u6df1\u5ea6\u51fd\u6570\u7684\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5df2\u77e5\u58f0\u901f\u5256\u9762\u7684\u60c5\u51b5\uff0c\u5e76\u5728\u5b9e\u9645\u5b9e\u9a8c\u6570\u636e\u4e2d\u5f97\u5230\u4e86\u9a8c\u8bc1\u3002"}}
{"id": "2509.18428", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18428", "abs": "https://arxiv.org/abs/2509.18428", "authors": ["Bahey Tharwat", "Yara Nasser", "Ali Abouzeid", "Ian Reid"], "title": "Latent Action Pretraining Through World Modeling", "comment": null, "summary": "Vision-Language-Action (VLA) models have gained popularity for learning\nrobotic manipulation tasks that follow language instructions. State-of-the-art\nVLAs, such as OpenVLA and $\\pi_{0}$, were trained on large-scale, manually\nlabeled action datasets collected through teleoperation. More recent\napproaches, including LAPA and villa-X, introduce latent action representations\nthat enable unsupervised pretraining on unlabeled datasets by modeling abstract\nvisual changes between frames. Although these methods have shown strong\nresults, their large model sizes make deployment in real-world settings\nchallenging. In this work, we propose LAWM, a model-agnostic framework to\npretrain imitation learning models in a self-supervised way, by learning latent\naction representations from unlabeled video data through world modeling. These\nvideos can be sourced from robot recordings or videos of humans performing\nactions with everyday objects. Our framework is designed to be effective for\ntransferring across tasks, environments, and embodiments. It outperforms models\ntrained with ground-truth robotics actions and similar pretraining methods on\nthe LIBERO benchmark and real-world setup, while being significantly more\nefficient and practical for real-world settings.", "AI": {"tldr": "LAWM\u662f\u4e00\u4e2a\u6a21\u578b\u65e0\u5173\u7684\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u7528\u4e8e\u5b66\u4e60\u673a\u5668\u4eba\u6a21\u4eff\u5b66\u4e60\u6a21\u578b\u4e2d\u7684\u6f5c\u5728\u52a8\u4f5c\u8868\u793a\uff0c\u901a\u8fc7\u4e16\u754c\u5efa\u6a21\u4ece\u65e0\u6807\u7b7e\u89c6\u9891\u6570\u636e\u4e2d\u5b66\u4e60\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u66f4\u9ad8\u6548\u5b9e\u7528\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u578b\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5176\u5de8\u5927\u7684\u6a21\u578b\u5c3a\u5bf8\u7ed9\u5b9e\u9645\u90e8\u7f72\u5e26\u6765\u4e86\u6311\u6218\u3002\u867d\u7136\u4e00\u4e9b\u65b9\u6cd5\u5f15\u5165\u4e86\u6f5c\u5728\u52a8\u4f5c\u8868\u793a\u4ee5\u5b9e\u73b0\u65e0\u76d1\u7763\u9884\u8bad\u7ec3\uff0c\u4f46\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002", "method": "\u63d0\u51faLAWM\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u6a21\u578b\u65e0\u5173\uff0c\u901a\u8fc7\u4e16\u754c\u5efa\u6a21\u4ece\u65e0\u6807\u7b7e\u89c6\u9891\u6570\u636e\uff08\u5305\u62ec\u673a\u5668\u4eba\u5f55\u50cf\u548c\u4eba\u7c7b\u64cd\u4f5c\u89c6\u9891\uff09\u4e2d\u5b66\u4e60\u6f5c\u5728\u52a8\u4f5c\u8868\u793a\uff0c\u7528\u4e8e\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u6a21\u4eff\u5b66\u4e60\u6a21\u578b\u3002", "result": "LAWM\u5728LIBERO\u57fa\u51c6\u6d4b\u8bd5\u548c\u5b9e\u9645\u8bbe\u7f6e\u4e2d\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u4f7f\u7528\u771f\u5b9e\u673a\u5668\u4eba\u52a8\u4f5c\u548c\u7c7b\u4f3c\u9884\u8bad\u7ec3\u65b9\u6cd5\u8bad\u7ec3\u7684\u6a21\u578b\uff0c\u5e76\u4e14\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u66f4\u9ad8\u6548\u3001\u66f4\u5b9e\u7528\u3002", "conclusion": "LAWM\u662f\u4e00\u4e2a\u6709\u6548\u7684\u6a21\u578b\u65e0\u5173\u6846\u67b6\uff0c\u53ef\u4ee5\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u6f5c\u5728\u52a8\u4f5c\u8868\u793a\uff0c\u4e3a\u673a\u5668\u4eba\u6a21\u4eff\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u66f4\u5b9e\u7528\u7684\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u80fd\u591f\u8de8\u4efb\u52a1\u3001\u73af\u5883\u548c\u5177\u8eab\u8fdb\u884c\u8fc1\u79fb\u3002"}}
{"id": "2509.18179", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18179", "abs": "https://arxiv.org/abs/2509.18179", "authors": ["Sai Varun Kodathala", "Rakesh Vunnam"], "title": "The Describe-Then-Generate Bottleneck: How VLM Descriptions Alter Image Generation Outcomes", "comment": "13 pages, 7 Figures", "summary": "With the increasing integration of multimodal AI systems in creative\nworkflows, understanding information loss in vision-language-vision pipelines\nhas become important for evaluating system limitations. However, the\ndegradation that occurs when visual content passes through textual\nintermediation remains poorly quantified. In this work, we provide empirical\nanalysis of the describe-then-generate bottleneck, where natural language\nserves as an intermediate representation for visual information. We generated\n150 image pairs through the describe-then-generate pipeline and applied\nexisting metrics (LPIPS, SSIM, and color distance) to measure information\npreservation across perceptual, structural, and chromatic dimensions. Our\nevaluation reveals that 99.3% of samples exhibit substantial perceptual\ndegradation and 91.5% demonstrate significant structural information loss,\nproviding empirical evidence that the describe-then-generate bottleneck\nrepresents a measurable and consistent limitation in contemporary multimodal\nsystems.", "AI": {"tldr": "\u89c6\u89c9\u5185\u5bb9\u7ecf\u8fc7\u6587\u672c\u4fe1\u606f\u4f20\u9012\u65f6\u4f1a\u635f\u5931\u5927\u91cf\u4fe1\u606f\uff0c\u5f71\u54cd\u591a\u6a21\u6001AI\u5728\u521b\u610f\u5de5\u4f5c\u6d41\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u7406\u89e3\u89c6\u89c9-\u8bed\u8a00-\u89c6\u89c9\uff08vision-language-vision\uff09\u7ba1\u9053\u4e2d\u7684\u4fe1\u606f\u635f\u5931\u5bf9\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001AI\u7cfb\u7edf\u7684\u5c40\u9650\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u5bf9\u89c6\u89c9\u5185\u5bb9\u7ecf\u8fc7\u6587\u672c\u4fe1\u606f\u4f20\u9012\u65f6\u7684\u9000\u5316\u73b0\u8c61\u91cf\u5316\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7\"\u63cf\u8ff0-\u751f\u6210\"\uff08describe-then-generate\uff09\u7ba1\u9053\u751f\u6210150\u5bf9\u56fe\u50cf\uff0c\u5e76\u4f7f\u7528LPIPS\u3001SSIM\u548c\u989c\u8272\u8ddd\u79bb\u7b49\u73b0\u6709\u6307\u6807\uff0c\u4ece\u611f\u77e5\u3001\u7ed3\u6784\u548c\u8272\u5f69\u7b49\u7ef4\u5ea6\u8861\u91cf\u4fe1\u606f\u4fdd\u7559\u60c5\u51b5\u3002", "result": "99.3%\u7684\u6837\u672c\u51fa\u73b0\u660e\u663e\u611f\u77e5\u9000\u5316\uff0c91.5%\u7684\u6837\u672c\u51fa\u73b0\u663e\u8457\u7ed3\u6784\u4fe1\u606f\u635f\u5931\u3002", "conclusion": "\"\u63cf\u8ff0-\u751f\u6210\"\u74f6\u9888\u662f\u5f53\u524d\u591a\u6a21\u6001\u7cfb\u7edf\u4e2d\u4e00\u4e2a\u53ef\u8861\u91cf\u4e14\u6301\u7eed\u5b58\u5728\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2509.18312", "categories": ["quant-ph", "math-ph", "math.MP"], "pdf": "https://arxiv.org/pdf/2509.18312", "abs": "https://arxiv.org/abs/2509.18312", "authors": ["Harriet Apel", "Toby Cubitt", "Emilio Onorati"], "title": "A sharper Magnus expansion bound woven in binary branches", "comment": null, "summary": "The Magnus expansion provides an exponential representation of one-parameter\noperator families, expressed as a series expansion in its generators. This is\nuseful for example in quantum mechanics for expressing a unitary evolution\ndetermined by a time-dependent Hamiltonian generator of the dynamics. The\nsolution is constructed as a series expansion in terms of increasingly complex\nnested commutators that rapidly become challenging to compute directly. This\nwork establishes a universal upper bound, agnostic to the generator, on the\nerror incurred when the Magnus expansion is truncated at an arbitrary given\norder. The main technical ingredient of the proof is the binary tree\nrepresentation introduced by Iserles and Norsett from which we derive a\nrecursion formula to delimit the magnitude of any term in the expansion. We\ncomplement our analytic results for the truncation error with explicit\ncalculation of the first 24 terms in the Magnus series, illustrating that they\nfollow the scaling behaviour we have derived. With these findings we aim to\ncontribute to the understanding of the accuracy and limitations of the Magnus\nexpansion technique, and to provide a sharper bound for approximating quantum\ndynamics without requiring assumptions on the structure of their generators.", "AI": {"tldr": "Magnus\u5c55\u5f00\u63d0\u4f9b\u4e86\u4e00\u79cd\u6307\u6570\u8868\u793a\uff0c\u53ef\u7528\u4e8e\u8868\u793a\u7b97\u7b26\u65cf\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86Magnus\u5c55\u5f00\u622a\u65ad\u8bef\u5dee\u7684\u901a\u7528\u4e0a\u754c\uff0c\u5e76\u63d0\u4f9b\u4e86\u8ba1\u7b97\u548c\u5206\u6790\u65b9\u6cd5\u3002", "motivation": "Magnus\u5c55\u5f00\u5728\u91cf\u5b50\u529b\u5b66\u7b49\u9886\u57df\u4e2d\u7528\u4e8e\u8868\u793a\u7531\u542b\u65f6\u54c8\u5bc6\u987f\u91cf\u51b3\u5b9a\u7684\u5e7a\u6b63\u6f14\u5316\uff0c\u4f46\u5176\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u3002\u672c\u7814\u7a76\u65e8\u5728\u4e3aMagnus\u5c55\u5f00\u7684\u622a\u65ad\u8bef\u5dee\u63d0\u4f9b\u4e00\u4e2a\u901a\u7528\u7684\u3001\u4e0e\u751f\u6210\u5143\u65e0\u5173\u7684\u4e0a\u754c\uff0c\u4ee5\u6539\u5584\u5176\u7cbe\u5ea6\u548c\u7406\u89e3\u5176\u5c40\u9650\u6027\u3002", "method": "\u5229\u7528Iserles\u548cNorsett\u5f15\u5165\u7684\u4e8c\u53c9\u6811\u8868\u793a\uff0c\u63a8\u5bfc\u51fa\u4e00\u79cd\u9012\u63a8\u516c\u5f0f\u6765\u754c\u5b9aMagnus\u5c55\u5f00\u4e2d\u5404\u9879\u7684\u5e45\u5ea6\uff0c\u4ece\u800c\u5efa\u7acb\u622a\u65ad\u8bef\u5dee\u7684\u4e0a\u754c\u3002", "result": "\u63a8\u5bfc\u51fa\u4e86Magnus\u5c55\u5f00\u622a\u65ad\u8bef\u5dee\u7684\u901a\u7528\u4e0a\u754c\uff0c\u5e76\u901a\u8fc7\u8ba1\u7b97\u524d24\u9879\u7684Magnus\u7ea7\u6570\uff0c\u9a8c\u8bc1\u4e86\u5176\u6570\u503c\u7ed3\u679c\u4e0e\u7406\u8bba\u5206\u6790\u7684\u8d8b\u52bf\u4e00\u81f4\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3aMagnus\u5c55\u5f00\u7684\u622a\u65ad\u8bef\u5dee\u63d0\u4f9b\u4e86\u7406\u8bba\u4e0a\u754c\u7684\u5206\u6790\uff0c\u5e76\u8f85\u4ee5\u6570\u503c\u8ba1\u7b97\u7684\u9a8c\u8bc1\uff0c\u65e8\u5728\u589e\u8fdb\u5bf9\u8be5\u6280\u672f\u51c6\u786e\u6027\u548c\u5c40\u9650\u6027\u7684\u7406\u89e3\uff0c\u5e76\u4e3a\u8fd1\u4f3c\u91cf\u5b50\u52a8\u529b\u5b66\u63d0\u4f9b\u66f4\u7cbe\u786e\u7684\u754c\u9650\u3002"}}
{"id": "2509.18110", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18110", "abs": "https://arxiv.org/abs/2509.18110", "authors": ["Mrigank Dhingra", "Romit Maulik", "Adil Rasheed", "Omer San"], "title": "Localized PCA-Net Neural Operators for Scalable Solution Reconstruction of Elliptic PDEs", "comment": null, "summary": "Neural operator learning has emerged as a powerful approach for solving\npartial differential equations (PDEs) in a data-driven manner. However,\napplying principal component analysis (PCA) to high-dimensional solution fields\nincurs significant computational overhead. To address this, we propose a\npatch-based PCA-Net framework that decomposes the solution fields into smaller\npatches, applies PCA within each patch, and trains a neural operator in the\nreduced PCA space. We investigate two different patch-based approaches that\nbalance computational efficiency and reconstruction accuracy: (1)\nlocal-to-global patch PCA, and (2) local-to-local patch PCA. The trade-off\nbetween computational cost and accuracy is analyzed, highlighting the\nadvantages and limitations of each approach. Furthermore, within each approach,\nwe explore two refinements for the most computationally efficient method: (i)\nintroducing overlapping patches with a smoothing filter and (ii) employing a\ntwo-step process with a convolutional neural network (CNN) for refinement. Our\nresults demonstrate that patch-based PCA significantly reduces computational\ncomplexity while maintaining high accuracy, reducing end-to-end pipeline\nprocessing time by a factor of 3.7 to 4 times compared to global PCA, thefore\nmaking it a promising technique for efficient operator learning in PDE-based\nsystems.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5757\u7684PCA-Net\u6846\u67b6\uff0c\u5c06\u9ad8\u7ef4\u6c42\u89e3\u573a\u5206\u89e3\u4e3a\u5c0f\u5757\uff0c\u5728\u6bcf\u4e2a\u5757\u5185\u5e94\u7528PCA\uff0c\u5e76\u5728\u964d\u7ef4\u540e\u7684PCA\u7a7a\u95f4\u4e2d\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u7b97\u5b50\uff0c\u4ee5\u89e3\u51b3\u6570\u636e\u9a71\u52a8\u7684\u504f\u5fae\u5206\u65b9\u7a0b\uff08PDE\uff09\u6c42\u89e3\u95ee\u9898\uff0c\u5e76\u5206\u6790\u4e86\u4e24\u79cd\u5757\u72b6\u65b9\u6cd5\u7684\u8ba1\u7b97\u6548\u7387\u548c\u91cd\u6784\u7cbe\u5ea6\u4e4b\u95f4\u7684\u6743\u8861\u3002", "motivation": "\u9ad8\u7ef4\u6c42\u89e3\u573a\u4e2d\u7684\u4e3b\u6210\u5206\u5206\u6790\uff08PCA\uff09\u4f1a\u5e26\u6765\u663e\u8457\u7684\u8ba1\u7b97\u5f00\u9500\uff0c\u800c\u795e\u7ecf\u7b97\u5b50\u5b66\u4e60\u662f\u6c42\u89e3\u504f\u5fae\u5206\u65b9\u7a0b\uff08PDE\uff09\u7684\u6709\u6548\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u5757\u72b6PCA-Net\u6846\u67b6\uff0c\u5c06\u6c42\u89e3\u573a\u5206\u89e3\u4e3a\u5c0f\u5757\uff0c\u5728\u6bcf\u4e2a\u5757\u5185\u5e94\u7528PCA\uff0c\u5e76\u5728\u964d\u7ef4\u7684PCA\u7a7a\u95f4\u4e2d\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u7b97\u5b50\u3002\u7814\u7a76\u4e86\u4e24\u79cd\u5757\u72b6\u65b9\u6cd5\uff1a\u5c40\u90e8\u5230\u5168\u5c40\u5757PCA\u548c\u5c40\u90e8\u5230\u5c40\u90e8\u5757PCA\uff0c\u5e76\u5bf9\u8ba1\u7b97\u6210\u672c\u548c\u7cbe\u5ea6\u8fdb\u884c\u4e86\u5206\u6790\u3002\u5bf9\u8ba1\u7b97\u6548\u7387\u6700\u9ad8\u7684\u65b9\u6cd5\u8fdb\u884c\u4e86\u4e24\u79cd\u6539\u8fdb\uff1a\u4f7f\u7528\u91cd\u53e0\u5757\u548c\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u8fdb\u884c\u5e73\u6ed1\u6ee4\u6ce2\u548c\u4e24\u6b65\u6cd5\u3002", "result": "\u5757\u72b6PCA\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u7cbe\u5ea6\uff0c\u4e0e\u5168\u5c40PCA\u76f8\u6bd4\uff0c\u7aef\u5230\u7aef\u7ba1\u9053\u5904\u7406\u65f6\u95f4\u51cf\u5c11\u4e863.7\u81f34\u500d\u3002", "conclusion": "\u5757\u72b6PCA-Net\u6846\u67b6\u662f\u4e00\u79cd\u7528\u4e8ePDE\u7cfb\u7edf\u7684\u9ad8\u6548\u7b97\u5b50\u5b66\u4e60\u6280\u672f\uff0c\u80fd\u591f\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u6027\u5e76\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u3002"}}
{"id": "2509.18186", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18186", "abs": "https://arxiv.org/abs/2509.18186", "authors": ["Nursultan Askarbekuly", "Timur Fayzrakhmanov", "Sladjan Babarogi\u0107", "Ivan Lukovi\u0107"], "title": "An Outcome-Based Educational Recommender System", "comment": null, "summary": "Most educational recommender systems are tuned and judged on click- or\nrating-based relevance, leaving their true pedagogical impact unclear. We\nintroduce OBER-an Outcome-Based Educational Recommender that embeds learning\noutcomes and assessment items directly into the data schema, so any algorithm\ncan be evaluated on the mastery it fosters. OBER uses a minimalist\nentity-relation model, a log-driven mastery formula, and a plug-in\narchitecture. Integrated into an e-learning system in non-formal domain, it was\nevaluated trough a two-week randomized split test with over 5 700 learners\nacross three methods: fixed expert trajectory, collaborative filtering (CF),\nand knowledge-based (KB) filtering. CF maximized retention, but the fixed path\nachieved the highest mastery. Because OBER derives business, relevance, and\nlearning metrics from the same logs, it lets practitioners weigh relevance and\nengagement against outcome mastery with no extra testing overhead. The\nframework is method-agnostic and readily extensible to future adaptive or\ncontext-aware recommenders.", "AI": {"tldr": "OBER\u662f\u4e00\u4e2a\u57fa\u4e8e\u7ed3\u679c\u7684\u6559\u80b2\u63a8\u8350\u7cfb\u7edf\uff0c\u53ef\u4ee5\u6839\u636e\u5b66\u4e60\u6210\u679c\u8bc4\u4f30\u63a8\u8350\u7b97\u6cd5\uff0c\u5e76\u5df2\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u8fdb\u884c\u4e86\u6d4b\u8bd5\u3002", "motivation": "\u5927\u591a\u6570\u6559\u80b2\u63a8\u8350\u7cfb\u7edf\u4f9d\u8d56\u70b9\u51fb\u6216\u8bc4\u5206\u6765\u8bc4\u4f30\u76f8\u5173\u6027\uff0c\u4f46\u5176\u771f\u5b9e\u7684\u6559\u5b66\u6548\u679c\u5c1a\u4e0d\u6e05\u695a\u3002", "method": "OBER\u4f7f\u7528\u57fa\u4e8e\u5b9e\u4f53\u7684\u5173\u7cfb\u6a21\u578b\u3001\u65e5\u5fd7\u9a71\u52a8\u7684\u638c\u63e1\u5ea6\u516c\u5f0f\u548c\u63d2\u4ef6\u67b6\u6784\u3002\u5b83\u901a\u8fc7\u8bb0\u5f55\u5b66\u4e60\u8005\u7684\u4e92\u52a8\u548c\u8bc4\u4f30\u6765\u8ddf\u8e2a\u5b66\u4e60\u8fdb\u5ea6\uff0c\u5e76\u6839\u636e\u9884\u5b9a\u4e49\u5b66\u4e60\u6210\u679c\u7684\u638c\u63e1\u60c5\u51b5\u6765\u8bc4\u4f30\u63a8\u8350\u7b97\u6cd5\u7684\u6709\u6548\u6027\u3002", "result": "\u5728\u4e3a\u671f\u4e24\u5468\u7684A/B\u6d4b\u8bd5\u4e2d\uff0c\u867d\u7136\u534f\u540c\u8fc7\u6ee4\uff08CF\uff09\u5728\u7528\u6237\u7559\u5b58\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u56fa\u5b9a\u8def\u5f84\u7684\u6559\u5b66\u65b9\u6cd5\u5728\u5b66\u4e60\u8005\u638c\u63e1\u5ea6\u65b9\u9762\u53d6\u5f97\u4e86\u6700\u9ad8\u5206\u3002\u8fd9\u8868\u660e\uff0c\u4ec5\u5173\u6ce8\u7528\u6237\u4e92\u52a8\uff08\u5982\u70b9\u51fb\u7387\uff09\u53ef\u80fd\u65e0\u6cd5\u5145\u5206\u4f53\u73b0\u6559\u5b66\u6548\u679c\u3002", "conclusion": "OBER\u901a\u8fc7\u5c06\u5b66\u4e60\u6210\u679c\u548c\u8bc4\u4f30\u9879\u76ee\u76f4\u63a5\u5d4c\u5165\u6570\u636e\u6a21\u5f0f\uff0c\u4e3a\u6559\u80b2\u63a8\u8350\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30\u7ef4\u5ea6\uff0c\u53ef\u4ee5\u8861\u91cf\u5176\u4fc3\u8fdb\u7684\u638c\u63e1\u5ea6\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u7528\u6237\u4e92\u52a8\u3002\u8be5\u6846\u67b6\u5177\u6709\u65b9\u6cd5\u65e0\u5173\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u53ef\u7528\u4e8e\u8bc4\u4f30\u5404\u79cd\u63a8\u8350\u7b97\u6cd5\u7684\u6559\u5b66\u5f71\u54cd\u3002"}}
{"id": "2509.18293", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.18293", "abs": "https://arxiv.org/abs/2509.18293", "authors": ["Jay Patel", "Hrudayangam Mehta", "Jeremy Blackburn"], "title": "Evaluating Large Language Models for Detecting Antisemitism", "comment": "Accepted to EMNLP 2025 Main Conference", "summary": "Detecting hateful content is a challenging and important problem. Automated\ntools, like machine-learning models, can help, but they require continuous\ntraining to adapt to the ever-changing landscape of social media. In this work,\nwe evaluate eight open-source LLMs' capability to detect antisemitic content,\nspecifically leveraging in-context definition as a policy guideline. We explore\nvarious prompting techniques and design a new CoT-like prompt, Guided-CoT.\nGuided-CoT handles the in-context policy well, increasing performance across\nall evaluated models, regardless of decoding configuration, model sizes, or\nreasoning capability. Notably, Llama 3.1 70B outperforms fine-tuned GPT-3.5.\nAdditionally, we examine LLM errors and introduce metrics to quantify semantic\ndivergence in model-generated rationales, revealing notable differences and\nparadoxical behaviors among LLMs. Our experiments highlight the differences\nobserved across LLMs' utility, explainability, and reliability.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u516b\u79cd\u5f00\u6e90\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u68c0\u6d4b\u53cd\u72b9\u5185\u5bb9\u65b9\u9762\u7684\u80fd\u529b\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGuided-CoT\u7684\u65b0\u63d0\u793a\u6280\u672f\uff0c\u8be5\u6280\u672f\u901a\u8fc7\u5728\u4e0a\u4e0b\u6587\u4e2d\u63d0\u4f9b\u5b9a\u4e49\u4f5c\u4e3a\u7b56\u7565\u6307\u5357\uff0c\u63d0\u9ad8\u4e86\u6240\u6709\u8bc4\u4f30\u6a21\u578b\u7684\u6027\u80fd\u3002\u7814\u7a76\u8fd8\u5206\u6790\u4e86LLM\u7684\u9519\u8bef\uff0c\u5e76\u91cf\u5316\u4e86\u6a21\u578b\u751f\u6210\u89e3\u91ca\u4e2d\u7684\u8bed\u4e49\u5dee\u5f02\uff0c\u63ed\u793a\u4e86\u4e0d\u540cLLM\u5728\u6548\u7528\u3001\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u9760\u6027\u65b9\u9762\u7684\u5dee\u5f02\u3002", "motivation": "\u81ea\u52a8\u68c0\u6d4b\u793e\u4ea4\u5a92\u4f53\u4e0a\u7684\u4ec7\u6068\u5185\u5bb9\u662f\u4e00\u4e2a\u91cd\u8981\u4f46\u5145\u6ee1\u6311\u6218\u7684\u95ee\u9898\uff0c\u9700\u8981\u6a21\u578b\u4e0d\u65ad\u9002\u5e94\u4e0d\u65ad\u53d8\u5316\u7684\u4ec7\u6068\u8a00\u8bba\u6a21\u5f0f\u3002\u672c\u7814\u7a76\u7684\u52a8\u673a\u662f\u8bc4\u4f30\u548c\u6539\u8fdb\u5f00\u6e90\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u68c0\u6d4b\u7279\u5b9a\u7c7b\u578b\u7684\u4ec7\u6068\u5185\u5bb9\uff08\u53cd\u72b9\u5185\u5bb9\uff09\u65b9\u9762\u7684\u80fd\u529b\u3002", "method": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u516b\u79cd\u5f00\u6e90LLMs\u68c0\u6d4b\u53cd\u72b9\u5185\u5bb9\u7684\u8868\u73b0\uff0c\u91cd\u70b9\u4f7f\u7528\u4e86\u4e0a\u4e0b\u6587\u5b9a\u4e49\u4f5c\u4e3a\u7b56\u7565\u6307\u5357\u3002\u7814\u7a76\u63a2\u7d22\u4e86\u4e0d\u540c\u7684\u63d0\u793a\u6280\u672f\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u540d\u4e3aGuided-CoT\u7684\u65b0\u63d0\u793a\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u5904\u7406\u4e0a\u4e0b\u6587\u7b56\u7565\u65f6\u8868\u73b0\u826f\u597d\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u68c0\u67e5\u4e86LLM\u7684\u9519\u8bef\uff0c\u5e76\u5f15\u5165\u4e86\u91cf\u5316\u6a21\u578b\u751f\u6210\u89e3\u91ca\u4e2d\u8bed\u4e49\u5dee\u5f02\u7684\u65b0\u6307\u6807\u3002", "result": "Guided-CoT\u63d0\u793a\u65b9\u6cd5\u63d0\u9ad8\u4e86\u6240\u6709\u8bc4\u4f30\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u4e0d\u53d7\u89e3\u7801\u914d\u7f6e\u3001\u6a21\u578b\u5927\u5c0f\u6216\u63a8\u7406\u80fd\u529b\u7684\u5f71\u54cd\u3002Llama 3.1 70B\u6a21\u578b\u7684\u8868\u73b0\u4f18\u4e8e\u5fae\u8c03\u7684GPT-3.5\u6a21\u578b\u3002\u7814\u7a76\u8fd8\u53d1\u73b0\u4e86\u4e0d\u540cLLM\u5728\u6548\u7528\u3001\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u9760\u6027\u65b9\u9762\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u5e76\u901a\u8fc7\u91cf\u5316\u8bed\u4e49\u5dee\u5f02\u7684\u6307\u6807\u63ed\u793a\u4e86\u6a21\u578b\u884c\u4e3a\u7684\u5dee\u5f02\u548c\u77db\u76fe\u4e4b\u5904\u3002", "conclusion": "\u672c\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u4f7f\u7528\u4e0a\u4e0b\u6587\u5b9a\u4e49\u548cGuided-CoT\u7b49\u63d0\u793a\u6280\u672f\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u5f00\u6e90LLMs\u68c0\u6d4b\u53cd\u72b9\u5185\u5bb9\u7684\u6027\u80fd\u3002\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u4e0d\u540cLLM\u5728\u51c6\u786e\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u9760\u6027\u65b9\u9762\u7684\u5dee\u5f02\uff0c\u4e3a\u672a\u6765\u5f00\u53d1\u66f4\u6709\u6548\u7684\u4ec7\u6068\u5185\u5bb9\u68c0\u6d4b\u5de5\u5177\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002"}}
{"id": "2509.19294", "categories": ["cs.DC", "astro-ph.IM", "D.1.3; J.2"], "pdf": "https://arxiv.org/pdf/2509.19294", "abs": "https://arxiv.org/abs/2509.19294", "authors": ["Jenny Lynn Almerol", "Elisabetta Boella", "Mario Spera", "Daniele Gregori"], "title": "Accelerating Gravitational $N$-Body Simulations Using the RISC-V-Based Tenstorrent Wormhole", "comment": null, "summary": "Although originally developed primarily for artificial intelligence\nworkloads, RISC-V-based accelerators are also emerging as attractive platforms\nfor high-performance scientific computing. In this work, we present our\napproach to accelerating an astrophysical $N$-body code on the RISC-V-based\nWormhole n300 card developed by Tenstorrent. Our results show that this\nplatform can be highly competitive for astrophysical simulations employing this\nclass of algorithms, delivering more than a $2 \\times$ speedup and\napproximately $2 \\times$ energy savings compared to a highly optimized CPU\nimplementation of the same code.", "AI": {"tldr": "RISC-V\u52a0\u901f\u5668\u5728\u79d1\u5b66\u8ba1\u7b97\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5728\u5929\u4f53\u7269\u7406N\u4f53\u6a21\u62df\u4e2d\u5b9e\u73b0\u4e862\u500d\u4ee5\u4e0a\u7684\u52a0\u901f\u548c\u63a5\u8fd12\u500d\u7684\u80fd\u8017\u8282\u7701\u3002", "motivation": "\u5c55\u793aRISC-V\u5e73\u53f0\u5728\u9ad8\u6027\u80fd\u79d1\u5b66\u8ba1\u7b97\uff0c\u7279\u522b\u662f\u5929\u4f53\u7269\u7406N\u4f53\u6a21\u62df\u65b9\u9762\u7684\u6f5c\u529b\u3002", "method": "\u5728Tenstorrent\u7684Wormhole n300 RISC-V\u5361\u4e0a\u52a0\u901f\u4e86\u4e00\u4e2a\u5929\u4f53\u7269\u7406N\u4f53\u4ee3\u7801\u3002", "result": "\u4e0e\u9ad8\u5ea6\u4f18\u5316\u7684CPU\u5b9e\u73b0\u76f8\u6bd4\uff0c\u5b9e\u73b0\u4e86\u8d85\u8fc72\u500d\u7684\u901f\u5ea6\u63d0\u5347\u548c\u7ea62\u500d\u7684\u80fd\u8017\u8282\u7701\u3002", "conclusion": "RISC-V\u5e73\u53f0\u5728\u5929\u4f53\u7269\u7406\u6a21\u62df\u65b9\u9762\u5177\u6709\u7ade\u4e89\u529b\u3002"}}
{"id": "2509.18749", "categories": ["eess.SY", "cs.RO", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.18749", "abs": "https://arxiv.org/abs/2509.18749", "authors": ["Maxwell M. Varley", "Timothy L. Molloy", "Girish N. Nair"], "title": "An Extended Kalman Filter for Systems with Infinite-Dimensional Measurements", "comment": "8 pages", "summary": "This article examines state estimation in discrete-time nonlinear stochastic\nsystems with finite-dimensional states and infinite-dimensional measurements,\nmotivated by real-world applications such as vision-based localization and\ntracking. We develop an extended Kalman filter (EKF) for real-time state\nestimation, with the measurement noise modeled as an infinite-dimensional\nrandom field. When applied to vision-based state estimation, the measurement\nJacobians required to implement the EKF are shown to correspond to image\ngradients. This result provides a novel system-theoretic justification for the\nuse of image gradients as features for vision-based state estimation,\ncontrasting with their (often heuristic) introduction in many computer-vision\npipelines. We demonstrate the practical utility of the EKF on a public\nreal-world dataset involving the localization of an aerial drone using video\nfrom a downward-facing monocular camera. The EKF is shown to outperform\nVINS-MONO, an established visual-inertial odometry algorithm, in some cases\nachieving mean squared error reductions of up to an order of magnitude.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5177\u6709\u6709\u9650\u72b6\u6001\u548c\u65e0\u9650\u7ef4\u6d4b\u91cf\u7684\u79bb\u6563\u65f6\u95f4\u975e\u7ebf\u6027\u968f\u673a\u7cfb\u7edf\u7684\u72b6\u6001\u4f30\u8ba1\u95ee\u9898\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\uff08EKF\uff09\u3002", "motivation": "\u8be5\u7814\u7a76\u7684\u52a8\u673a\u6e90\u4e8e\u8bf8\u5982\u56fe\u8c61\u5236\u5bfc\u5b9a\u4f4d\u4e0e\u8ddf\u8e2a\u7b49\u73b0\u5b9e\u4e16\u754c\u5e94\u7528\u3002\u6587\u4e2d\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a\u5728\u89c6\u89c9\u5236\u5bfc\u72b6\u6001\u4f30\u8ba1\u4e2d\u5e94\u7528\u56fe\u50cf\u68af\u5ea6\u63d0\u4f9b\u4e86\u65b0\u7684\u7cfb\u7edf\u7406\u8bba\u4f9d\u636e\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u7528\u4e8e\u5b9e\u65f6\u72b6\u6001\u4f30\u8ba1\u7684\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\uff08EKF\uff09\uff0c\u5e76\u5c06\u6d4b\u91cf\u566a\u58f0\u5efa\u6a21\u4e3a\u65e0\u9650\u7ef4\u968f\u673a\u573a\u3002\u5728\u5e94\u7528\u4e8e\u89c6\u89c9\u5236\u5bfc\u72b6\u6001\u4f30\u8ba1\u65f6\uff0c\u5b9e\u73b0EKF\u6240\u9700\u7684\u6d4b\u91cf\u96c5\u53ef\u6bd4\u77e9\u9635\u88ab\u8bc1\u660e\u5bf9\u5e94\u4e8e\u56fe\u50cf\u68af\u5ea6\u3002", "result": "\u5728\u6d89\u53ca\u4f7f\u7528\u4e0b\u89c6\u5355\u76ee\u76f8\u673a\u89c6\u9891\u5bf9\u65e0\u4eba\u673a\u8fdb\u884c\u5b9a\u4f4d\u7684\u516c\u5f00\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\uff0c\u8bc1\u660e\u4e86EKF\u7684\u5b9e\u7528\u6027\u3002EKF\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u4f18\u4e8e\u5df2\u5efa\u7acb\u7684\u89c6\u89c9-\u60ef\u6027\u91cc\u7a0b\u8ba1\u7b97\u6cd5VINS-MONO\uff0c\u5747\u65b9\u8bef\u5dee\u964d\u4f4e\u4e8610\u500d\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684EKF\u5728\u89c6\u89c9\u5236\u5bfc\u72b6\u6001\u4f30\u8ba1\u65b9\u9762\u663e\u793a\u51fa\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\uff0c\u5e76\u4f18\u4e8e\u73b0\u6709\u7684\u5148\u8fdb\u7b97\u6cd5\u3002"}}
{"id": "2509.18737", "categories": ["quant-ph", "cond-mat.mes-hall", "math-ph", "math.MP", "physics.atom-ph"], "pdf": "https://arxiv.org/pdf/2509.18737", "abs": "https://arxiv.org/abs/2509.18737", "authors": ["Hoang-Anh Le", "Saba Taherpour", "Denis Jankovi\u0107", "Christoph Wolf"], "title": "Overcoming limitations on gate fidelity in noisy static exchange-coupled surface qubits", "comment": null, "summary": "Recent experiments demonstrated that the spin state of individual atoms on\nsurfaces can be quantum-coherently controlled through all-electric electron\nspin resonance. By constructing interacting arrays of atoms this results in an\natomic-scale qubit platform. However, the static exchange coupling between\nqubits, limited lifetime and polarization of the initial state, impose\nsignificant limits on high-fidelity quantum control. We address this issue\nusing open quantum systems simulation and quantum optimal control theory. We\ndemonstrate the conditions under which high-fidelity operations ($\\mathcal{F}\n\\gtrsim 0.9$) are feasible in this qubit platform, and show how the Krotov\nmethod of quantum optimal control theory adapts to specific noise sources to\noutperform the conventional Rabi drivings. Finally, we re-examine the\nexperimental setup used in the initial demonstration of this qubit platform and\npropose optimized experimental designs to maximize gate fidelity in this\nplatform.", "AI": {"tldr": "\u901a\u8fc7\u5f00\u653e\u91cf\u5b50\u7cfb\u7edf\u6a21\u62df\u548c\u91cf\u5b50\u6700\u4f18\u63a7\u5236\u7406\u8bba\uff0c\u7814\u7a76\u4e86\u539f\u5b50\u5c3a\u5ea6\u91cf\u5b50\u6bd4\u7279\u5e73\u53f0\u7684\u9ad8\u4fdd\u771f\u5ea6\u91cf\u5b50\u63a7\u5236\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4f18\u5316\u5b9e\u9a8c\u8bbe\u8ba1\u7684\u5efa\u8bae\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u539f\u5b50\u5c3a\u5ea6\u91cf\u5b50\u6bd4\u7279\u5e73\u53f0\u4e2d\u5b58\u5728\u7684\u9759\u6001\u4ea4\u6362\u8026\u5408\u3001\u6709\u9650\u7684\u5bff\u547d\u548c\u521d\u59cb\u72b6\u6001\u6781\u5316\u9650\u5236\u9ad8\u4fdd\u771f\u5ea6\u91cf\u5b50\u63a7\u5236\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5f00\u653e\u91cf\u5b50\u7cfb\u7edf\u6a21\u62df\u548c\u91cf\u5b50\u6700\u4f18\u63a7\u5236\u7406\u8bba\uff0c\u7279\u522b\u662fKrotov\u65b9\u6cd5\uff0c\u6765\u7814\u7a76\u91cf\u5b50\u6bd4\u7279\u5e73\u53f0\u7684\u9ad8\u4fdd\u771f\u5ea6\u64cd\u4f5c\u6761\u4ef6\uff0c\u5e76\u4e0e\u4f20\u7edf\u7684Rabi\u9a71\u52a8\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u8bc1\u660e\u4e86\u5728\u8be5\u91cf\u5b50\u6bd4\u7279\u5e73\u53f0\u4e2d\u5b9e\u73b0\u9ad8\u4fdd\u771f\u5ea6\u64cd\u4f5c\uff08\u4fdd\u771f\u5ea6\u5927\u4e8e0.9\uff09\u7684\u53ef\u884c\u6027\u6761\u4ef6\uff0c\u5e76\u5c55\u793a\u4e86Krotov\u65b9\u6cd5\u5982\u4f55\u9002\u5e94\u7279\u5b9a\u7684\u566a\u58f0\u6e90\u4ee5\u4f18\u4e8e\u4f20\u7edf\u7684Rabi\u9a71\u52a8\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4f18\u5316\u5b9e\u9a8c\u8bbe\u8ba1\u7684\u5efa\u8bae\uff0c\u4ee5\u6700\u5927\u5316\u8be5\u91cf\u5b50\u6bd4\u7279\u5e73\u53f0\u4e2d\u7684\u95e8\u4fdd\u771f\u5ea6\u3002"}}
{"id": "2509.18590", "categories": ["cond-mat.mtrl-sci", "cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2509.18590", "abs": "https://arxiv.org/abs/2509.18590", "authors": ["Chunqiang Xu", "Shuvankar Gupta", "Hengxin Tan", "Hyeonhu Bae", "Olajumoke Oluwatobiloba Emmanuel", "Mingyu Xu", "Yan Wu", "Xiaofeng Xu", "Pengpeng Zhang", "Weiwei Xie", "Binghai Yan", "Xianglin Ke"], "title": "Large Anomalous and Topological Hall Effect and Nernst Effect in a Dirac Kagome Magnet Fe3Ge", "comment": "Accepted in Advanced Functional Materials", "summary": "The search for kagome magnets with unconventional magnetic and electronic\nproperties has gained significant attention in recent years. We report the\nmagnetic, electronic, and thermoelectric properties of Fe3Ge single crystals,\nwhere the Fe atoms form a slightly distorted kagome lattice. Fe3Ge exhibits a\nlarge anomalous Hall effect and anomalous Nernst effect. The anomalous\ntransverse thermoelectric conductivity reaches about 4.6 A m^-1 K^-1, exceeding\nvalues reported for conventional ferromagnets and most topological\nferromagnets. First-principles calculations indicate that these transport\nresponses are primarily governed by intrinsic mechanisms, highlighting the\ndominant role of Berry curvature arising from massive Dirac gaps in momentum\nspace. In addition, we observe a topological Hall resistivity of about 0.9\nmicroOhm cm and a topological Nernst coefficient of 1.2 microvolt K^-1, which\nare attributed to the Berry phase associated with field-induced scalar spin\nchirality. These findings demonstrate the combined influence of Berry phases in\nboth momentum and real space, establishing Fe3Ge as a promising candidate for\nroom-temperature transverse thermoelectric applications.", "AI": {"tldr": "Fe3Ge\u5355\u6676\u5728\u78c1\u6027\u3001\u7535\u5b50\u548c\u70ed\u7535\u6027\u8d28\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u8f83\u5927\u7684\u53cd\u5e38\u970d\u5c14\u6548\u5e94\u548c\u53cd\u5e38\u80fd\u5c14\u65af\u7279\u6548\u5e94\uff0c\u4ee5\u53ca\u7531\u6807\u91cf\u81ea\u65cb\u624b\u5f81\u5f15\u8d77\u7684\u62d3\u6251\u970d\u5c14\u6548\u5e94\u548c\u62d3\u6251\u80fd\u5c14\u65af\u7279\u7cfb\u6570\uff0c\u662f\u5ba4\u6e29\u4e0b\u6a2a\u5411\u70ed\u7535\u5e94\u7528\u7684\u6709\u529b\u5019\u9009\u8005\u3002", "motivation": "\u5bfb\u627e\u5177\u6709\u975e\u5e38\u89c4\u78c1\u6027\u548c\u7535\u5b50\u7279\u6027\u7684kagome\u78c1\u4f53\u3002\u5728Fe3Ge\u5355\u6676\u4e2d\u7814\u7a76\u5176\u78c1\u6027\u3001\u7535\u5b50\u548c\u70ed\u7535\u6027\u8d28\uff0c\u7279\u522b\u662f\u5176\u53cd\u5e38\u970d\u5c14\u6548\u5e94\u3001\u53cd\u5e38\u80fd\u5c14\u65af\u7279\u6548\u5e94\u548c\u62d3\u6251\u970d\u5c14\u6548\u5e94\u3002", "method": "\u5b9e\u9a8c\u6d4b\u91cf\u4e86Fe3Ge\u5355\u6676\u7684\u78c1\u6027\u3001\u7535\u5b50\u548c\u70ed\u7535\u6027\u8d28\uff0c\u5e76\u8fdb\u884c\u4e86\u7b2c\u4e00\u6027\u539f\u7406\u8ba1\u7b97\uff0c\u4ee5\u63ed\u793a\u5176\u53cd\u5e38\u970d\u5c14\u6548\u5e94\u548c\u53cd\u5e38\u80fd\u5c14\u65af\u7279\u6548\u5e94\u7684\u5185\u5728\u673a\u5236\u3002", "result": "Fe3Ge\u5355\u6676\u5177\u6709\u5927\u7684\u53cd\u5e38\u970d\u5c14\u6548\u5e94\u548c\u53cd\u5e38\u80fd\u5c14\u65af\u7279\u6548\u5e94\uff0c\u5176\u53cd\u5e38\u6a2a\u5411\u70ed\u7535\u5bfc\u7387\u8fbe\u52304.6 A m^-1 K^-1\u3002\u6b64\u5916\uff0c\u8fd8\u89c2\u5bdf\u5230\u4e860.9 microOhm cm\u7684\u62d3\u6251\u970d\u5c14\u7535\u963b\u7387\u548c1.2 microvolt K^-1\u7684\u62d3\u6251\u80fd\u5c14\u65af\u7279\u7cfb\u6570\u3002\u7b2c\u4e00\u6027\u539f\u7406\u8ba1\u7b97\u8868\u660e\uff0c\u8fd9\u4e9b\u54cd\u5e94\u4e3b\u8981\u7531\u52a8\u91cf\u7a7a\u95f4\u4e2d\u7684\u8d1d\u91cc\u66f2\u7387\u9a71\u52a8\uff0c\u800c\u62d3\u6251\u6548\u5e94\u5219\u5f52\u56e0\u4e8e\u6807\u91cf\u81ea\u65cb\u624b\u5f81\u76f8\u5173\u7684\u8d1d\u91cc\u76f8\u4f4d\u3002", "conclusion": "Fe3Ge\u5355\u6676\u7ed3\u5408\u4e86\u52a8\u91cf\u7a7a\u95f4\u548c\u5b9e\u7a7a\u95f4\u7684\u8d1d\u91cc\u76f8\u6548\u5e94\uff0c\u5c55\u73b0\u51fa\u4f18\u5f02\u7684\u6a2a\u5411\u70ed\u7535\u6027\u8d28\uff0c\u662f\u5ba4\u6e29\u4e0b\u6a2a\u5411\u70ed\u7535\u5e94\u7528\u7684\u6709\u524d\u666f\u7684\u6750\u6599\u3002"}}
{"id": "2509.18918", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.18918", "abs": "https://arxiv.org/abs/2509.18918", "authors": ["Hamideh-Sadat Fazael-Ardekani", "Hadi Zayyani", "Hamid Soltanian-Zadeh"], "title": "Quaternion LMS for Graph Signal Recovery", "comment": null, "summary": "This letter generalizes the Graph Signal Recovery (GSR) problem in Graph\nSignal Processing (GSP) to the Quaternion domain. It extends the Quaternion\nLeast Mean Square (QLMS) in adaptive filtering literature, and Graph LMS (GLMS)\nalgorithm in GSP literature, to an algorithm called Quaternion GLMS (QGLMS).\nThe basic adaptation formula using Quaternion-based algebra is derived.\nMoreover, mean convergence analysis and mean-square convergence analysis are\nmathematically performed. Hence, a sufficient condition on the step-size\nparameter of QGLMS is suggested. Also, simulation results demonstrate the\neffectiveness of the proposed algorithm in graph signal reconstruction.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u56db\u5143\u6570\u56fe\u6700\u5c0f\u5747\u65b9\uff08QGLMS\uff09\u7684\u65b0\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u56fe\u4fe1\u53f7\u5904\u7406\u4e2d\u7684\u56db\u5143\u6570\u56fe\u4fe1\u53f7\u6062\u590d\u95ee\u9898\u3002", "motivation": "\u5c06\u56fe\u4fe1\u53f7\u6062\u590d\uff08GSR\uff09\u95ee\u9898\u63a8\u5e7f\u5230\u56db\u5143\u6570\u57df\uff0c\u4ee5\u6269\u5c55\u81ea\u9002\u5e94\u6ee4\u6ce2\u548c\u56fe\u4fe1\u53f7\u5904\u7406\u4e2d\u7684\u73b0\u6709\u65b9\u6cd5\u3002", "method": "\u63a8\u5bfc\u4e86\u57fa\u4e8e\u56db\u5143\u6570\u4ee3\u6570\u7684QGLMS\u57fa\u672c\u81ea\u9002\u5e94\u516c\u5f0f\uff0c\u5e76\u8fdb\u884c\u4e86\u5747\u503c\u6536\u655b\u6027\u548c\u5747\u65b9\u6536\u655b\u6027\u5206\u6790\u3002", "result": "\u901a\u8fc7\u6570\u5b66\u5206\u6790\u5f97\u5230\u4e86QGLMS\u6b65\u957f\u53c2\u6570\u7684\u5145\u5206\u6761\u4ef6\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u7ed3\u679c\u9a8c\u8bc1\u4e86\u8be5\u7b97\u6cd5\u5728\u56fe\u4fe1\u53f7\u91cd\u6784\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "QGLMS\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u5730\u89e3\u51b3\u56db\u5143\u6570\u57df\u7684\u56fe\u4fe1\u53f7\u6062\u590d\u95ee\u9898\uff0c\u5e76\u4e3a\u7b97\u6cd5\u53c2\u6570\u9009\u62e9\u63d0\u4f9b\u4e86\u7406\u8bba\u6307\u5bfc\u3002"}}
{"id": "2509.18447", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18447", "abs": "https://arxiv.org/abs/2509.18447", "authors": ["Rishabh Madan", "Jiawei Lin", "Mahika Goel", "Angchen Xie", "Xiaoyu Liang", "Marcus Lee", "Justin Guo", "Pranav N. Thakkar", "Rohan Banerjee", "Jose Barreiros", "Kate Tsui", "Tom Silver", "Tapomayukh Bhattacharjee"], "title": "PrioriTouch: Adapting to User Contact Preferences for Whole-Arm Physical Human-Robot Interaction", "comment": "Conference on Robot Learning (CoRL)", "summary": "Physical human-robot interaction (pHRI) requires robots to adapt to\nindividual contact preferences, such as where and how much force is applied.\nIdentifying preferences is difficult for a single contact; with whole-arm\ninteraction involving multiple simultaneous contacts between the robot and\nhuman, the challenge is greater because different body parts can impose\nincompatible force requirements. In caregiving tasks, where contact is frequent\nand varied, such conflicts are unavoidable. With multiple preferences across\nmultiple contacts, no single solution can satisfy all objectives--trade-offs\nare inherent, making prioritization essential. We present PrioriTouch, a\nframework for ranking and executing control objectives across multiple\ncontacts. PrioriTouch can prioritize from a general collection of controllers,\nmaking it applicable not only to caregiving scenarios such as bed bathing and\ndressing but also to broader multi-contact settings. Our method combines a\nnovel learning-to-rank approach with hierarchical operational space control,\nleveraging simulation-in-the-loop rollouts for data-efficient and safe\nexploration. We conduct a user study on physical assistance preferences, derive\npersonalized comfort thresholds, and incorporate them into PrioriTouch. We\nevaluate PrioriTouch through extensive simulation and real-world experiments,\ndemonstrating its ability to adapt to user contact preferences, maintain task\nperformance, and enhance safety and comfort. Website:\nhttps://emprise.cs.cornell.edu/prioritouch.", "AI": {"tldr": "PrioriTouch\u662f\u4e00\u4e2a\u7528\u4e8e\u5904\u7406\u7269\u7406\u4eba\u673a\u4ea4\u4e92\u4e2d\u591a\u63a5\u89e6\u70b9\u4e0d\u517c\u5bb9\u529b\u53cd\u9988\u9700\u6c42\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5b66\u4e60\u6392\u5e8f\u548c\u5206\u5c42\u64cd\u4f5c\u7a7a\u95f4\u63a7\u5236\uff0c\u80fd\u591f\u6839\u636e\u7528\u6237\u504f\u597d\u8fdb\u884c\u8c03\u6574\uff0c\u5e76\u5728\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u9002\u5e94\u6027\u3001\u4efb\u52a1\u6027\u80fd\u3001\u5b89\u5168\u6027\u548c\u8212\u9002\u6027\u3002", "motivation": "\u5728\u7269\u7406\u4eba\u673a\u4ea4\u4e92\uff08pHRI\uff09\u4e2d\uff0c\u673a\u5668\u4eba\u9700\u8981\u9002\u5e94\u4e2a\u4f53\u63a5\u89e6\u504f\u597d\uff08\u5982\u63a5\u89e6\u4f4d\u7f6e\u548c\u529b\u7684\u5927\u5c0f\uff09\uff0c\u4f46\u591a\u4e2a\u63a5\u89e6\u70b9\u53ef\u80fd\u5e26\u6765\u4e0d\u517c\u5bb9\u7684\u529b\u53cd\u9988\u9700\u6c42\uff0c\u5c24\u5176\u662f\u5728\u62a4\u7406\u4efb\u52a1\u4e2d\uff0c\u51b2\u7a81\u96be\u4ee5\u907f\u514d\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u4e2a\u80fd\u591f\u5904\u7406\u591a\u91cd\u504f\u597d\u3001\u8fdb\u884c\u6743\u8861\u548c\u4f18\u5148\u7ea7\u6392\u5e8f\u7684\u7cfb\u7edf\u3002", "method": "PrioriTouch\u6846\u67b6\u7ed3\u5408\u4e86\u65b0\u9896\u7684\u5b66\u4e60\u6392\u5e8f\u65b9\u6cd5\u548c\u5206\u5c42\u64cd\u4f5c\u7a7a\u95f4\u63a7\u5236\uff0c\u5e76\u5229\u7528\u4e86\u4eff\u771f\u5185\u5faa\u73af\uff08simulation-in-the-loop\uff09\u8fdb\u884c\u6570\u636e\u9ad8\u6548\u4e14\u5b89\u5168\u7684\u63a2\u7d22\u3002\u6b64\u5916\uff0c\u8fd8\u7ed3\u5408\u4e86\u7528\u6237\u7814\u7a76\u4e2d\u83b7\u5f97\u7684\u4e2a\u6027\u5316\u8212\u9002\u9608\u503c\u3002", "result": "PrioriTouch\u5728\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u9002\u5e94\u7528\u6237\u7684\u63a5\u89e6\u504f\u597d\uff0c\u4fdd\u6301\u4efb\u52a1\u6027\u80fd\uff0c\u5e76\u63d0\u9ad8\u5b89\u5168\u6027\u548c\u8212\u9002\u6027\u3002", "conclusion": "PrioriTouch\u6846\u67b6\u6210\u529f\u5730\u89e3\u51b3\u4e86\u7269\u7406\u4eba\u673a\u4ea4\u4e92\u4e2d\u591a\u63a5\u89e6\u70b9\u4e0d\u517c\u5bb9\u7684\u529b\u53cd\u9988\u9700\u6c42\u95ee\u9898\uff0c\u901a\u8fc7\u4f18\u5148\u7ea7\u6392\u5e8f\u548c\u81ea\u9002\u5e94\u63a7\u5236\uff0c\u5b9e\u73b0\u4e86\u5bf9\u7528\u6237\u504f\u597d\u7684\u4e2a\u6027\u5316\u9002\u5e94\uff0c\u5e76\u5728\u5404\u79cd\u5e94\u7528\u573a\u666f\u4e2d\u5c55\u73b0\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2509.18182", "categories": ["cs.CV", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2509.18182", "abs": "https://arxiv.org/abs/2509.18182", "authors": ["Isabelle Tingzon", "Yoji Toriumi", "Caroline Gevaert"], "title": "AI-Derived Structural Building Intelligence for Urban Resilience: An Application in Saint Vincent and the Grenadines", "comment": "Accepted at the 2nd Workshop on Computer Vision for Developing\n  Countries (CV4DC) at ICCV 2025", "summary": "Detailed structural building information is used to estimate potential damage\nfrom hazard events like cyclones, floods, and landslides, making them critical\nfor urban resilience planning and disaster risk reduction. However, such\ninformation is often unavailable in many small island developing states (SIDS)\nin climate-vulnerable regions like the Caribbean. To address this data gap, we\npresent an AI-driven workflow to automatically infer rooftop attributes from\nhigh-resolution satellite imagery, with Saint Vincent and the Grenadines as our\ncase study. Here, we compare the utility of geospatial foundation models\ncombined with shallow classifiers against fine-tuned deep learning models for\nrooftop classification. Furthermore, we assess the impact of incorporating\nadditional training data from neighboring SIDS to improve model performance.\nOur best models achieve F1 scores of 0.88 and 0.83 for roof pitch and roof\nmaterial classification, respectively. Combined with local capacity building,\nour work aims to provide SIDS with novel capabilities to harness AI and Earth\nObservation (EO) data to enable more efficient, evidence-based urban\ngovernance.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u4eba\u5de5\u667a\u80fd\u4ece\u9ad8\u5206\u8fa8\u7387\u536b\u661f\u56fe\u50cf\u4e2d\u81ea\u52a8\u63a8\u65ad\u5c4b\u9876\u5c5e\u6027\uff08\u5982\u5c4b\u9876\u5761\u5ea6\u548c\u5c4b\u9876\u6750\u6599\uff09\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u4ee5\u89e3\u51b3\u5c0f\u5c9b\u5c7f\u53d1\u5c55\u4e2d\u56fd\u5bb6\uff08SIDS\uff09\u7f3a\u4e4f\u8be6\u7ec6\u5efa\u7b51\u7ed3\u6784\u4fe1\u606f\u7684\u95ee\u9898\uff0c\u5e76\u4ee5\u5723\u6587\u68ee\u7279\u548c\u683c\u6797\u7eb3\u4e01\u65af\u4e3a\u6848\u4f8b\u7814\u7a76\u3002", "motivation": "\u5c0f\u5c9b\u5c7f\u53d1\u5c55\u4e2d\u56fd\u5bb6\uff08SIDS\uff09\u7f3a\u4e4f\u8be6\u7ec6\u7684\u5efa\u7b51\u7ed3\u6784\u4fe1\u606f\uff0c\u8fd9\u963b\u788d\u4e86\u57ce\u5e02\u97e7\u6027\u89c4\u5212\u548c\u707e\u5bb3\u98ce\u9669\u51cf\u5c11\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u6570\u636e\u7f3a\u53e3\u3002", "method": "\u7814\u7a76\u4eba\u5458\u6bd4\u8f83\u4e86\u5730\u7406\u7a7a\u95f4\u57fa\u7840\u6a21\u578b\u7ed3\u5408\u6d45\u5c42\u5206\u7c7b\u5668\u4e0e\u5fae\u8c03\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u5c4b\u9876\u5206\u7c7b\u65b9\u9762\u7684\u6548\u7528\uff0c\u5e76\u8bc4\u4f30\u4e86\u6574\u5408\u90bb\u8fd1SIDS\u7684\u989d\u5916\u8bad\u7ec3\u6570\u636e\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u6700\u4f73\u6a21\u578b\u5728\u5c4b\u9876\u5761\u5ea6\u548c\u5c4b\u9876\u6750\u6599\u5206\u7c7b\u65b9\u9762\u5206\u522b\u8fbe\u5230\u4e860.88\u548c0.83\u7684F1\u5206\u6570\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u65e8\u5728\u4e3aSIDS\u63d0\u4f9b\u65b0\u7684\u80fd\u529b\uff0c\u5229\u7528\u4eba\u5de5\u667a\u80fd\u548c\u5730\u7403\u89c2\u6d4b\uff08EO\uff09\u6570\u636e\uff0c\u5b9e\u73b0\u66f4\u9ad8\u6548\u3001\u5faa\u8bc1\u7684\u57ce\u5e02\u6cbb\u7406\uff0c\u5e76\u7ed3\u5408\u5f53\u5730\u80fd\u529b\u5efa\u8bbe\u3002"}}
{"id": "2509.18324", "categories": ["quant-ph", "cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2509.18324", "abs": "https://arxiv.org/abs/2509.18324", "authors": ["Dongjin Lee", "Beni Yoshida"], "title": "Chiral Color Code : Single-shot error correction for exotic topological order", "comment": "56 pages", "summary": "We present a family of simple three-dimensional stabilizer codes, called the\nchiral color codes, that realize fermionic and chiral topological orders. In\nthe qubit case, the code realizes the topological phase of a single copy of the\nfermionic toric code. For qudit systems with local dimension $d$, the model\nfeatures a chiral parameter $\\alpha$ and realizes 3D topological phases\ncharacterized by $\\mathbb{Z}_d^{(\\alpha)}$ anyon theories with anomalous chiral\nsurface topological order. On closed manifolds, the code has a unique ground\nstate after removing bulk transparent fermions or bosons. Furthermore, we prove\nthat the bulk is short-range entangled (for odd $d$, coprime $\\alpha$) by\nconstructing an explicit local quantum channel that prepares the ground state.\nThe chiral color codes are constructed within the gauge color code, and hence\ninherit its fault-tolerant features: they admit single-shot error correction\nand allow code switching to other stabilizer color codes. These properties\nposition the chiral color codes as particularly useful platforms for realizing\nand manipulating fermions and chiral anyons.", "AI": {"tldr": "Chiral color codes are a family of 3D stabilizer codes that realize fermionic and chiral topological orders. They are fault-tolerant and can be used to manipulate fermions and chiral anyons.", "motivation": "The paper presents a new family of 3D stabilizer codes, the chiral color codes, designed to realize fermionic and chiral topological orders.", "method": "The chiral color codes are constructed within the gauge color code framework. The paper proves the bulk is short-range entangled by constructing a local quantum channel to prepare the ground state. They also prove that the code has a unique ground state after removing bulk transparent fermions or bosons on closed manifolds.", "result": "The codes realize $\\mathbb{Z}_d^{(\\alpha)}$ anyon theories with anomalous chiral surface topological order. In the qubit case, it realizes the topological phase of a single copy of the fermionic toric code. The codes are fault-tolerant, admitting single-shot error correction and code switching.", "conclusion": "The chiral color codes are a promising platform for realizing and manipulating fermions and chiral anyons due to their unique topological properties and fault-tolerant features."}}
{"id": "2509.18111", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18111", "abs": "https://arxiv.org/abs/2509.18111", "authors": ["Faizul Rakib Sayem", "Shahana Ibrahim"], "title": "Prompt Optimization Meets Subspace Representation Learning for Few-shot Out-of-Distribution Detection", "comment": null, "summary": "The reliability of artificial intelligence (AI) systems in open-world\nsettings depends heavily on their ability to flag out-of-distribution (OOD)\ninputs unseen during training. Recent advances in large-scale vision-language\nmodels (VLMs) have enabled promising few-shot OOD detection frameworks using\nonly a handful of in-distribution (ID) samples. However, existing prompt\nlearning-based OOD methods rely solely on softmax probabilities, overlooking\nthe rich discriminative potential of the feature embeddings learned by VLMs\ntrained on millions of samples. To address this limitation, we propose a novel\ncontext optimization (CoOp)-based framework that integrates subspace\nrepresentation learning with prompt tuning. Our approach improves ID-OOD\nseparability by projecting the ID features into a subspace spanned by prompt\nvectors, while projecting ID-irrelevant features into an orthogonal null space.\nTo train such OOD detection framework, we design an easy-to-handle end-to-end\nlearning criterion that ensures strong OOD detection performance as well as\nhigh ID classification accuracy. Experiments on real-world datasets showcase\nthe effectiveness of our approach.", "AI": {"tldr": "\u73b0\u6709\u7684\u57fa\u4e8e\u63d0\u793a\u5b66\u4e60\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684 out-of-distribution\uff08OOD\uff09\u68c0\u6d4b\u65b9\u6cd5\u4ec5\u4f9d\u8d56 softmax \u6982\u7387\uff0c\u5ffd\u7565\u4e86 VLM \u5b66\u5230\u7684\u7279\u5f81\u5d4c\u5165\u7684\u6f5c\u529b\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u4e0a\u4e0b\u6587\u4f18\u5316\uff08CoOp\uff09\u7684\u6846\u67b6\uff0c\u5c06\u5b50\u7a7a\u95f4\u8868\u793a\u5b66\u4e60\u4e0e\u63d0\u793a\u8c03\u4f18\u76f8\u7ed3\u5408\uff0c\u901a\u8fc7\u5c06 ID \u7279\u5f81\u6295\u5f71\u5230\u7531\u63d0\u793a\u5411\u91cf\u6784\u6210\u7684\u5b50\u7a7a\u95f4\u4e2d\uff0c\u5e76\u5c06 ID \u4e0d\u76f8\u5173\u7684\u7279\u5f81\u6295\u5f71\u5230\u6b63\u4ea4\u8865\u7a7a\u95f4\u4e2d\uff0c\u6765\u63d0\u9ad8 ID-OOD \u5206\u79bb\u5ea6\u3002\u8be5\u6846\u67b6\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u5b66\u4e60\u6807\u51c6\uff0c\u4ee5\u786e\u4fdd OOD \u68c0\u6d4b\u6027\u80fd\u548c ID \u5206\u7c7b\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u63d0\u793a\u5b66\u4e60\u7684 OOD \u68c0\u6d4b\u65b9\u6cd5\u4ec5\u4f9d\u8d56 softmax \u6982\u7387\uff0c\u5ffd\u7565\u4e86 VLM \u5b66\u5230\u7684\u7279\u5f81\u5d4c\u5165\u7684\u6f5c\u529b\u3002", "method": "\u5c06\u5b50\u7a7a\u95f4\u8868\u793a\u5b66\u4e60\u4e0e\u63d0\u793a\u8c03\u4f18\u76f8\u7ed3\u5408\uff0c\u901a\u8fc7\u5c06 ID \u7279\u5f81\u6295\u5f71\u5230\u7531\u63d0\u793a\u5411\u91cf\u6784\u6210\u7684\u5b50\u7a7a\u95f4\u4e2d\uff0c\u5e76\u5c06 ID \u4e0d\u76f8\u5173\u7684\u7279\u5f81\u6295\u5f71\u5230\u6b63\u4ea4\u8865\u7a7a\u95f4\u4e2d\uff0c\u6765\u63d0\u9ad8 ID-OOD \u5206\u79bb\u5ea6\u3002\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u5b66\u4e60\u6807\u51c6\uff0c\u4ee5\u786e\u4fdd OOD \u68c0\u6d4b\u6027\u80fd\u548c ID \u5206\u7c7b\u51c6\u786e\u6027\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b0\u9896\u7684\u4e0a\u4e0b\u6587\u4f18\u5316\uff08CoOp\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u96c6\u6210\u5b50\u7a7a\u95f4\u8868\u793a\u5b66\u4e60\u548c\u63d0\u793a\u8c03\u4f18\uff0c\u80fd\u591f\u6709\u6548\u5730\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5f00\u653e\u4e16\u754c\u8bbe\u7f6e\u4e0b\u7684 OOD \u68c0\u6d4b\u80fd\u529b\u3002"}}
{"id": "2509.18314", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18314", "abs": "https://arxiv.org/abs/2509.18314", "authors": ["Hieu Tran", "Zonghai Yao", "Hong Yu"], "title": "Exploiting Tree Structure for Credit Assignment in RL Training of LLMs", "comment": "15 pages", "summary": "Reinforcement learning improves LLM reasoning, yet sparse delayed reward over\nlong sequences makes token-level credit assignment the key bottleneck. We study\nthe verifiable-reward setting, where the final answer is checkable and multiple\nresponses can be drawn per prompt. Reasoning tasks in math and medical QA align\nwith this setup, where only a few decision tokens significantly impact the\noutcome. PPO offers token-level advantages with a learned value model, but it\nis complex to train both the actor and critic models simultaneously, and it is\nnot easily generalizable, as the token-level values from the critic model can\nmake training prone to overfitting. GRPO is critic-free and supports verifiable\nrewards, but spreads a single sequence-level return across tokens and ignores\nbranching. We introduce \\textbf{Prefix-to-Tree (P2T)}, a simple procedure that\nconverts a group of responses into a prefix tree and computes\n\\emph{nonparametric} prefix values \\(V(s)\\) by aggregating descendant outcomes.\nBuilt on P2T, we propose \\textbf{TEMPO} (\\emph{\\textbf{T}ree-\\textbf{E}stimated\n\\textbf{M}ean Prefix Value for \\textbf{P}olicy \\textbf{O}ptimization}), a\ncritic-free algorithm that augments the group-relative outcome signal of GRPO\nwith \\emph{branch-gated} temporal-difference corrections derived from the tree.\nAt non-branch tokens, the temporal-difference (TD) term is zero, so TEMPO\nreduces to GRPO; at branching tokens, it supplies precise token-level credit\nwithout a learned value network or extra judges/teachers. On Qwen3-1.7B/4B,\nTEMPO outperforms PPO and GRPO on in-distribution (MATH, MedQA) and\nout-of-distribution (GSM-HARD, AMC23, MedMCQA, MMLU-Medical) benchmarks, and\nreaches higher validation accuracy with roughly the same wall-clock time.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTEMPO\u7684\u65e0\u8bc4\u8bba\u5458\u7b97\u6cd5\uff0c\u7528\u4e8e\u4f18\u5316\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u63a8\u7406\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u5956\u52b1\u7a00\u758f\u548c\u5ef6\u8fdf\u7684\u60c5\u51b5\u4e0b\u3002TEMPO\u901a\u8fc7\u5c06\u591a\u4e2a\u54cd\u5e94\u8f6c\u6362\u4e3a\u524d\u7f00\u6811\uff0c\u5e76\u5229\u7528\u975e\u53c2\u6570\u524d\u7f00\u503c\u6765\u89e3\u51b3\u4ee3\u5e01\u7ea7\u4fe1\u7528\u5206\u914d\u95ee\u9898\uff0c\u4ece\u800c\u5728\u6570\u5b66\u548c\u533b\u5b66\u95ee\u7b54\u7b49\u9886\u57df\u53d6\u5f97\u4e86\u4f18\u4e8ePPO\u548cGRPO\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff08\u5982PPO\u548cGRPO\uff09\u5728\u5904\u7406\u957f\u5e8f\u5217\u548c\u7a00\u758f\u5956\u52b1\u65f6\u5b58\u5728\u74f6\u9888\uff0c\u5982\u4ee3\u5e01\u7ea7\u4fe1\u7528\u5206\u914d\u56f0\u96be\u3001\u8bad\u7ec3\u590d\u6742\u3001\u6cdb\u5316\u6027\u5dee\u6216\u5956\u52b1\u4fe1\u53f7\u5206\u914d\u4e0d\u7cbe\u786e\u7b49\u95ee\u9898\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u8bbe\u7f6e\u4e0b\uff0c\u4e3aLLM\u63a8\u7406\u63d0\u4f9b\u66f4\u6709\u6548\u7684\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86Prefix-to-Tree (P2T) \u65b9\u6cd5\uff0c\u5c06\u4e00\u7ec4\u54cd\u5e94\u8f6c\u6362\u4e3a\u524d\u7f00\u6811\uff0c\u5e76\u8ba1\u7b97\u975e\u53c2\u6570\u524d\u7f00\u503c\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTEMPO\uff08Tree-Estimated Mean Prefix Value for Policy Optimization\uff09\u7684\u65e0\u8bc4\u8bba\u5458\u7b97\u6cd5\u3002TEMPO\u901a\u8fc7\u5229\u7528\u524d\u7f00\u6811\u7ed3\u6784\uff0c\u5728\u5206\u652f\u4ee3\u5e01\u5904\u8fdb\u884c\u5206\u652f\u95e8\u63a7\u65f6\u95f4\u5dee\u5206\u6821\u6b63\uff0c\u4ee5\u63d0\u4f9b\u7cbe\u786e\u7684\u4ee3\u5e01\u7ea7\u4fe1\u7528\uff0c\u540c\u65f6\u5728\u975e\u5206\u652f\u4ee3\u5e01\u5904\u9000\u5316\u4e3aGRPO\u3002", "result": "\u5728Qwen3-1.7B/4B\u6a21\u578b\u4e0a\uff0cTEMPO\u5728MATH\u3001MedQA\uff08\u5206\u5e03\u5185\uff09\u548cGSM-HARD\u3001AMC23\u3001MedMCQA\u3001MMLU-Medical\uff08\u5206\u5e03\u5916\uff09\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5747\u53d6\u5f97\u4e86\u4f18\u4e8ePPO\u548cGRPO\u7684\u6027\u80fd\uff0c\u5e76\u5728\u76f8\u4f3c\u7684\u8bad\u7ec3\u65f6\u95f4\u5185\u8fbe\u5230\u4e86\u66f4\u9ad8\u7684\u9a8c\u8bc1\u51c6\u786e\u7387\u3002", "conclusion": "TEMPO\u662f\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u65e0\u8bc4\u8bba\u5458\u7b97\u6cd5\uff0c\u80fd\u591f\u89e3\u51b3LLM\u63a8\u7406\u4e2d\u7684\u4ee3\u5e01\u7ea7\u4fe1\u7528\u5206\u914d\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u53ef\u9a8c\u8bc1\u5956\u52b1\u548c\u957f\u5e8f\u5217\u573a\u666f\u4e0b\u3002\u5b83\u901a\u8fc7P2T\u65b9\u6cd5\u548c\u5206\u652f\u95e8\u63a7\u65f6\u95f4\u5dee\u5206\u6821\u6b63\uff0c\u5728\u591a\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u5e76\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2509.18769", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.18769", "abs": "https://arxiv.org/abs/2509.18769", "authors": ["Hadi Nemati", "Pedro S\u00e1nchez-Mart\u00edn", "\u00c1lvaro Ortega", "Lukas Sigrist", "Luis Rouco"], "title": "Integration of Concentrated Solar Power Plants in Renewable-Only VPP with Electrical and Thermal Demands: A Two-Stage Robust Bidding Approach", "comment": null, "summary": "This paper proposes the integration of Concentrated Solar Power Plant (CSP)\nin the Renewable-only virtual power plant (RVPP) for bidding in the electricity\nday-ahead and secondary reserve markets, as well as trading thermal energy\nthrough a heat purchase agreement. A reformulated two-stage robust optimization\napproach is introduced to account for multiple uncertainties, including\nelectricity prices, non-dispatchable renewable energy sources electrical\nproduction, CSP thermal production, and uncertainties in electrical and thermal\ndemand consumption. The provision of energy and reserve by the thermal storage\nof CSP is modeled using an adjustable approach, which allocates a share of\nenergy for up and down reserves based on the profitability of the RVPP.\nSimulations are conducted for several case studies to demonstrate the\neffectiveness and computational efficiency of the proposed approach under\ndifferent RVPP operator decisions against uncertain parameters and various\ntrading strategies for electricity and thermal energy. The simulation results\nshow that integrating CSP into RVPP enhances RVPP flexibility for both\nelectrical and thermal trading. Furthermore, the results indicate that the\nprofitability of the RVPP increases when all trading options are considered,\nacross different levels of conservatism adopted by the RVPP operator in\nresponse to uncertain parameters.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u5c06\u5149\u70ed\u53d1\u7535\u5382\uff08CSP\uff09\u6574\u5408\u5230\u4ec5\u53ef\u518d\u751f\u80fd\u6e90\u7684\u865a\u62df\u7535\u5382\uff08RVPP\uff09\u4e2d\uff0c\u4ee5\u53c2\u4e0e\u7535\u529b\u65e5\u524d\u548c\u5907\u7528\u5e02\u573a\uff0c\u5e76\u901a\u8fc7\u70ed\u80fd\u8d2d\u4e70\u534f\u8bae\u4ea4\u6613\u70ed\u80fd\u3002", "motivation": "\u6574\u5408 CSP \u5230 RVPP \u4ee5\u589e\u5f3a\u5176\u5728\u7535\u529b\u548c\u70ed\u80fd\u5e02\u573a\u7684\u7ade\u4e89\u529b\uff0c\u5e76\u5e94\u5bf9\u591a\u79cd\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u6539\u8fdb\u7684\u4e24\u9636\u6bb5\u9c81\u68d2\u4f18\u5316\u65b9\u6cd5\uff0c\u8003\u8651\u4e86\u7535\u529b\u4ef7\u683c\u3001\u53ef\u518d\u751f\u80fd\u6e90\u53d1\u7535\u91cf\u3001CSP \u70ed\u53d1\u7535\u91cf\u4ee5\u53ca\u7535\u529b\u548c\u70ed\u80fd\u9700\u6c42\u7684\u4e0d\u786e\u5b9a\u6027\u3002\u5229\u7528\u53ef\u8c03\u65b9\u6cd5\u4e3a CSP \u7684\u50a8\u70ed\u5206\u914d\u7535\u80fd\u548c\u5907\u7528\u5bb9\u91cf\uff0c\u4ee5\u6700\u5927\u5316 RVPP \u7684\u5229\u6da6\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u6848\u4f8b\u7814\u7a76\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002\u7ed3\u679c\u8868\u660e\uff0c\u6574\u5408 CSP \u63d0\u9ad8\u4e86 RVPP \u7684\u7075\u6d3b\u6027\uff0c\u5e76\u589e\u52a0\u4e86\u5176\u5728\u8003\u8651\u6240\u6709\u4ea4\u6613\u9009\u9879\u548c\u4e0d\u540c\u98ce\u9669\u89c4\u907f\u6c34\u5e73\u4e0b\u7684\u76c8\u5229\u80fd\u529b\u3002", "conclusion": "\u6574\u5408 CSP \u5230 RVPP \u662f\u63d0\u9ad8 RVPP \u7075\u6d3b\u6027\u548c\u76c8\u5229\u80fd\u529b\u7684\u4e00\u79cd\u6709\u6548\u7b56\u7565\u3002"}}
{"id": "2509.19205", "categories": ["cond-mat.mtrl-sci", "cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2509.19205", "abs": "https://arxiv.org/abs/2509.19205", "authors": ["Quinn T. Campbell", "Andrew D. Baczewski", "Shashank Misra", "Evan M. Anderson"], "title": "First principles band structure of interacting phosphorus and boron/aluminum $\u03b4$-doped layers in silicon", "comment": null, "summary": "Silicon can be heavily doped with phosphorus in a single atomic layer (a\n$\\delta$ layer), significantly altering the electronic structure of the\nconduction bands within the material. Recent progress has also made it possible\nto further dope silicon with acceptor-based $\\delta$ layers using either boron\nor aluminum, making it feasible to create devices with interacting $\\delta$\nlayers with opposite polarity. It is not known, however, how these $\\delta$\nlayers will interact, particularly at small separation distances. Using Density\nFunctional Theory, we calculate the electronic structure of a phosphorus-based\n$\\delta$ layer interacting with a boron or aluminum $\\delta$ layer, varying the\ndistances between the $\\delta$ layers. At separations 10 \\AA\\ and smaller, the\ndopant potentials overlap and largely cancel each other out, leading to an\nelectronic structure closely mimicking bulk silicon. At separations greater\nthan 10 \\AA, the two layers behave independently of one another, forming a p-n\ndiode with an intrinsic layer taking the place of the depletion region. One\nmechanism for charge transfer between $\\delta$ layers at larger distances could\nbe tunneling, where we see a greater than 3\\% probability for tunneling between\na phosphorus and boron layer at 20 \\AA\\ separation. This tunneling rate exceeds\nwhat would be seen for a standard silicon 1.1 eV triangular barrier, indicating\nthat the interaction between delta layers creates enhanced tunneling at larger\nseparation distances compared to a traditional junction. These calculations\nprovide a foundation for the design of silicon-based electronics based on\ninteracting $\\delta$ layers.", "AI": {"tldr": "\u7845\u4e2d\u7684\u78f7\u6216\u787c/\u94dd $\\delta$ \u5c42\u5728\u5c0f\u8ddd\u79bb\uff08<10 \u00c5\uff09\u4f1a\u76f8\u4e92\u91cd\u53e0\u5e76\u62b5\u6d88\uff0c\u5728\u8f83\u5927\u7684\u8ddd\u79bb\uff08>10 \u00c5\uff09\u4f1a\u72ec\u7acb\u5de5\u4f5c\uff0c\u5f62\u6210\u7c7b P-N \u7ed3\uff0c\u5e76\u4e14\u5728 20 \u00c5 \u7684\u95f4\u8ddd\u4e0b\uff0c\u78f7\u548c\u787c\u5c42\u4e4b\u95f4\u7684\u96a7\u7a7f\u6982\u7387\u4f1a\u589e\u52a0\u3002", "motivation": "\u7814\u7a76\u78f7 $\\delta$ \u5c42\u4e0e\u787c\u6216\u94dd $\\delta$ \u5c42\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u4ee5\u4e86\u89e3\u5b83\u4eec\u5728\u4e0d\u540c\u5206\u79bb\u8ddd\u79bb\u4e0b\u7684\u884c\u4e3a\uff0c\u4e3a\u8bbe\u8ba1\u57fa\u4e8e\u76f8\u4e92\u4f5c\u7528\u7684 $\\delta$ \u5c42\u7684\u7845\u57fa\u7535\u5b50\u5668\u4ef6\u5960\u5b9a\u57fa\u7840\u3002", "method": "\u4f7f\u7528\u5bc6\u5ea6\u6cdb\u51fd\u7406\u8bba\u8ba1\u7b97\u78f7 $\\delta$ \u5c42\u4e0e\u787c\u6216\u94dd $\\delta$ \u5c42\u76f8\u4e92\u4f5c\u7528\u7684\u7535\u5b50\u7ed3\u6784\uff0c\u5e76\u6539\u53d8 $\\delta$ \u5c42\u4e4b\u95f4\u7684\u8ddd\u79bb\u3002", "result": "\u5f53 $\\delta$ \u5c42\u4e4b\u95f4\u7684\u8ddd\u79bb\u4e3a 10 \u00c5 \u6216\u66f4\u5c0f\u65f6\uff0c\u63ba\u6742\u5242\u7535\u4f4d\u4f1a\u91cd\u53e0\u5e76\u76f8\u4e92\u62b5\u6d88\uff0c\u7535\u5b50\u7ed3\u6784\u4e0e\u5757\u72b6\u7845\u975e\u5e38\u76f8\u4f3c\u3002\u5f53\u8ddd\u79bb\u5927\u4e8e 10 \u00c5 \u65f6\uff0c\u4e24\u5c42\u72ec\u7acb\u5de5\u4f5c\uff0c\u5f62\u6210\u4e00\u4e2a\u7531\u672c\u5f81\u5c42\u53d6\u4ee3\u8017\u5c3d\u5c42\u7684 P-N \u4e8c\u6781\u7ba1\u3002\u5728 20 \u00c5 \u7684\u8ddd\u79bb\u4e0b\uff0c\u78f7\u548c\u787c\u5c42\u4e4b\u95f4\u7684\u96a7\u7a7f\u6982\u7387\u8d85\u8fc7 3%\uff0c\u9ad8\u4e8e\u4f20\u7edf\u7ed3\u7684\u96a7\u7a7f\u7387\u3002", "conclusion": "$\\delta$ \u5c42\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u4f1a\u589e\u5f3a\u8f83\u5927\u5206\u79bb\u8ddd\u79bb\u4e0b\u7684\u96a7\u7a7f\u6548\u5e94\uff0c\u8fd9\u4e3a\u8bbe\u8ba1\u57fa\u4e8e\u76f8\u4e92\u4f5c\u7528\u7684 $\\delta$ \u5c42\u7684\u7845\u57fa\u7535\u5b50\u5668\u4ef6\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.18604", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2509.18604", "abs": "https://arxiv.org/abs/2509.18604", "authors": ["Kangyu Ji", "Tianran Liu", "Fang Sheng", "Shaun Tan", "Moungi Bawendi", "Tonio Buonassisi"], "title": "A closed-loop AI framework for hypothesis-driven and interpretable materials design", "comment": null, "summary": "Scientific hypothesis generation is central to materials discovery, yet\ncurrent approaches often emphasize either conceptual (idea-to-data) reasoning\nor data-driven (data-to-idea) analysis, rarely achieving an effective\nintegration of both. Here, we present a generalizable active learning workflow\nthat integrates top-down, theory-driven hypothesis generation, guided by a\nlarge language model. This is complemented by bottom-up, data-driven hypothesis\ntesting through a root-cause association study. We demonstrate this approach\nthrough the design of equimolar quinary-cation two-dimensional perovskite, a\nchemically complex system with over 850,000 possible cation combinations. In\nthe top-down component, the large language model drives closed-loop\noptimization by proposing candidates that are likely to achieve phase purity,\nleveraging domain knowledge and chain-of-thought reasoning. With each\niteration, the model identifies an increasing number of near phase-pure\ncompositions, sampling less than 0.004% of the design space. In parallel, the\nbottom-up association study identifies molecular features with statistically\nsignificant influences on phase purity. The integration of these approaches\nenables the convergence of conceptual and statistical hypotheses, leading to\ngeneralizable and rational design rules for phase-pure quinary-cation\ntwo-dimensional perovskites. As a proof of concept, we applied the optimized\nphase-pure quinary-cation two-dimensional perovskite film as a surface capping\nlayer in perovskite solar cells, achieving good performance and stability. Our\nframework enables the development of interpretable and generalizable design\nrules that are applicable to a wide range of optimization processes within\ncomplex design spaces, providing a foundational strategy for rational,\nscalable, and efficient materials discovery.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u5408\u4e86\u7406\u8bba\u9a71\u52a8\u7684\u5047\u8bbe\u751f\u6210\u548c\u6570\u636e\u9a71\u52a8\u7684\u5047\u8bbe\u68c0\u9a8c\u7684\u901a\u7528\u6d3b\u6027\u5b66\u4e60\u5de5\u4f5c\u6d41\uff0c\u7528\u4e8e\u6750\u6599\u53d1\u73b0\u3002", "motivation": "\u5f53\u524d\u7684\u6750\u6599\u53d1\u73b0\u65b9\u6cd5\u5728\u6574\u5408\u6982\u5ff5\u63a8\u7406\u548c\u6570\u636e\u9a71\u52a8\u5206\u6790\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u672c\u7814\u7a76\u65e8\u5728\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\u3002", "method": "\u8be5\u5de5\u4f5c\u6d41\u7ed3\u5408\u4e86\u7531\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6307\u5bfc\u7684\u81ea\u4e0a\u800c\u4e0b\u7684\u7406\u8bba\u9a71\u52a8\u5047\u8bbe\u751f\u6210\uff0c\u4ee5\u53ca\u901a\u8fc7\u6839\u6e90\u5173\u8054\u7814\u7a76\u8fdb\u884c\u7684\u81ea\u4e0b\u800c\u4e0a\u7684\u6570\u636e\u9a71\u52a8\u5047\u8bbe\u68c0\u9a8c\u3002", "result": "\u7814\u7a76\u6210\u529f\u8bbe\u8ba1\u4e86\u7b49\u6469\u5c14\u4e94\u5143\u9633\u79bb\u5b50\u4e8c\u7ef4\u9499\u949b\u77ff\uff0c\u5e76\u901a\u8fc7\u6d3b\u6027\u5b66\u4e60\u548c\u5173\u8054\u7814\u7a76\uff0c\u5728\u63a2\u7d22\u4e86\u6781\u5c0f\u8bbe\u8ba1\u7a7a\u95f4\uff08\u5c0f\u4e8e0.004%\uff09\u7684\u60c5\u51b5\u4e0b\uff0c\u8bc6\u522b\u51fa\u4e86\u5177\u6709\u9ad8\u76f8\u7eaf\u5ea6\u7684\u5019\u9009\u6750\u6599\u3002\u540c\u65f6\uff0c\u5173\u8054\u7814\u7a76\u53d1\u73b0\u4e86\u5f71\u54cd\u76f8\u7eaf\u5ea6\u7684\u5206\u5b50\u7279\u5f81\u3002", "conclusion": "\u8be5\u96c6\u6210\u65b9\u6cd5\u80fd\u591f\u878d\u5408\u6982\u5ff5\u548c\u7edf\u8ba1\u5047\u8bbe\uff0c\u4e3a\u7b49\u6469\u5c14\u4e94\u5143\u9633\u79bb\u5b50\u4e8c\u7ef4\u9499\u949b\u77ff\u7684\u76f8\u7eaf\u5ea6\u63d0\u4f9b\u53ef\u63a8\u5e7f\u548c\u7406\u6027\u7684\u8bbe\u8ba1\u89c4\u5219\u3002\u8be5\u6846\u67b6\u4e3a\u590d\u6742\u8bbe\u8ba1\u7a7a\u95f4\u5185\u7684\u4f18\u5316\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u3001\u53ef\u63a8\u5e7f\u7684\u8bbe\u8ba1\u89c4\u5219\uff0c\u4e3a\u7406\u6027\u3001\u53ef\u6269\u5c55\u548c\u9ad8\u6548\u7684\u6750\u6599\u53d1\u73b0\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.19056", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.19056", "abs": "https://arxiv.org/abs/2509.19056", "authors": ["Razieh Torkamani", "Arash Amini", "Hadi Zayyani", "Mehdi Korki"], "title": "Bayesian Convolutional Neural Networks for Prior Learning in Graph Signal Recovery", "comment": null, "summary": "Graph signal recovery (GSR) is a fundamental problem in graph signal\nprocessing, where the goal is to reconstruct a complete signal defined over a\ngraph from a subset of noisy or missing observations. A central challenge in\nGSR is that the underlying statistical model of the graph signal is often\nunknown or too complex to specify analytically. To address this, we propose a\nflexible, data-driven framework that learns the signal prior directly from\ntraining samples. We develop a Bayesian convolutional neural network (BCNN)\narchitecture that models the prior distribution of graph signals using\ngraph-aware filters based on Chebyshev polynomials. By interpreting the hidden\nlayers of the CNN as Gibbs distributions and employing Gaussian mixture model\n(GMM) nonlinearities, we obtain a closed-form and expressive prior. This prior\nis integrated into a variational Bayesian (VB) inference framework to estimate\nthe posterior distribution of the signal and noise precision. Extensive\nexperiments on synthetic and real-world graph datasets demonstrate that the\nproposed BCNN-GSR algorithm achieves accurate and robust recovery across a\nvariety of signal distributions. The method generalizes well to complex,\nnon-Gaussian signal models and remains computationally efficient, making it\nsuitable for practical large-scale graph recovery tasks.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d1d\u53f6\u65af\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08BCNN\uff09\u7684\u56fe\u4fe1\u53f7\u6062\u590d\uff08GSR\uff09\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u4ece\u4e0d\u5b8c\u6574\u7684\u89c2\u6d4b\u4e2d\u51c6\u786e\u3001\u7a33\u5065\u5730\u91cd\u5efa\u56fe\u4fe1\u53f7\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u590d\u6742\u3001\u975e\u9ad8\u65af\u4fe1\u53f7\u6a21\u578b\u548c\u5927\u89c4\u6a21\u5e94\u7528\u3002", "motivation": "\u56fe\u4fe1\u53f7\u6062\u590d\uff08GSR\uff09\u9762\u4e34\u7740\u5e95\u5c42\u7edf\u8ba1\u6a21\u578b\u672a\u77e5\u6216\u8fc7\u4e8e\u590d\u6742\u7684\u6311\u6218\u3002\u672c\u7814\u7a76\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u7075\u6d3b\u3001\u6570\u636e\u9a71\u52a8\u7684\u6846\u67b6\uff0c\u76f4\u63a5\u4ece\u8bad\u7ec3\u6837\u672c\u4e2d\u5b66\u4e60\u4fe1\u53f7\u7684\u5148\u9a8c\u77e5\u8bc6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8d1d\u53f6\u65af\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08BCNN\uff09\u67b6\u6784\uff0c\u4f7f\u7528\u57fa\u4e8e\u5207\u6bd4\u96ea\u592b\u591a\u9879\u5f0f\u7684\u56fe\u611f\u77e5\u6ee4\u6ce2\u5668\u6765\u5efa\u6a21\u56fe\u4fe1\u53f7\u7684\u5148\u9a8c\u5206\u5e03\u3002\u901a\u8fc7\u5c06CNN\u7684\u9690\u85cf\u5c42\u89e3\u91ca\u4e3a\u5409\u5e03\u65af\u5206\u5e03\u5e76\u91c7\u7528\u9ad8\u65af\u6df7\u5408\u6a21\u578b\uff08GMM\uff09\u975e\u7ebf\u6027\uff0c\u83b7\u5f97\u4e86\u4e00\u4e2a\u95ed\u5f0f\u4e14\u5bcc\u6709\u8868\u73b0\u529b\u7684\u5148\u9a8c\u3002\u8be5\u5148\u9a8c\u88ab\u6574\u5408\u5230\u53d8\u5206\u8d1d\u53f6\u65af\uff08VB\uff09\u63a8\u65ad\u6846\u67b6\u4e2d\uff0c\u7528\u4e8e\u4f30\u8ba1\u4fe1\u53f7\u548c\u566a\u58f0\u7cbe\u5ea6\u7684\u540e\u9a8c\u5206\u5e03\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684BCNN-GSR\u7b97\u6cd5\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u7684\u56fe\u6570\u636e\u96c6\u4e0a\uff0c\u80fd\u591f\u9488\u5bf9\u5404\u79cd\u4fe1\u53f7\u5206\u5e03\u5b9e\u73b0\u7cbe\u786e\u4e14\u7a33\u5065\u7684\u6062\u590d\u3002", "conclusion": "BCNN-GSR\u7b97\u6cd5\u80fd\u591f\u51c6\u786e\u3001\u7a33\u5065\u5730\u6062\u590d\u56fe\u4fe1\u53f7\uff0c\u80fd\u591f\u6cdb\u5316\u5230\u590d\u6742\u3001\u975e\u9ad8\u65af\u4fe1\u53f7\u6a21\u578b\uff0c\u5e76\u4e14\u8ba1\u7b97\u6548\u7387\u9ad8\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u7684\u5927\u89c4\u6a21\u56fe\u6062\u590d\u4efb\u52a1\u3002"}}
{"id": "2509.18455", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18455", "abs": "https://arxiv.org/abs/2509.18455", "authors": ["Yunshuang Li", "Yiyang Ling", "Gaurav S. Sukhatme", "Daniel Seita"], "title": "Learning Geometry-Aware Nonprehensile Pushing and Pulling with Dexterous Hands", "comment": null, "summary": "Nonprehensile manipulation, such as pushing and pulling, enables robots to\nmove, align, or reposition objects that may be difficult to grasp due to their\ngeometry, size, or relationship to the robot or the environment. Much of the\nexisting work in nonprehensile manipulation relies on parallel-jaw grippers or\ntools such as rods and spatulas. In contrast, multi-fingered dexterous hands\noffer richer contact modes and versatility for handling diverse objects to\nprovide stable support over the objects, which compensates for the difficulty\nof modeling the dynamics of nonprehensile manipulation. Therefore, we propose\nGeometry-aware Dexterous Pushing and Pulling (GD2P) for nonprehensile\nmanipulation with dexterous robotic hands. We study pushing and pulling by\nframing the problem as synthesizing and learning pre-contact dexterous hand\nposes that lead to effective manipulation. We generate diverse hand poses via\ncontact-guided sampling, filter them using physics simulation, and train a\ndiffusion model conditioned on object geometry to predict viable poses. At test\ntime, we sample hand poses and use standard motion planners to select and\nexecute pushing and pulling actions. We perform 840 real-world experiments with\nan Allegro Hand, comparing our method to baselines. The results indicate that\nGD2P offers a scalable route for training dexterous nonprehensile manipulation\npolicies. We further demonstrate GD2P on a LEAP Hand, highlighting its\napplicability to different hand morphologies. Our pre-trained models and\ndataset, including 1.3 million hand poses across 2.3k objects, will be\nopen-source to facilitate further research. Our project website is available\nat: geodex2p.github.io.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGD2P\uff08Geometry-aware Dexterous Pushing and Pulling\uff09\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528\u591a\u6307\u7075\u5de7\u624b\u8fdb\u884c\u975e\u6293\u53d6\u5f0f\u64cd\u4f5c\uff08\u5982\u63a8\u548c\u62c9\uff09\uff0c\u4ee5\u5e94\u5bf9\u51e0\u4f55\u5f62\u72b6\u3001\u5c3a\u5bf8\u6216\u4e0e\u673a\u5668\u4eba/\u73af\u5883\u5173\u7cfb\u800c\u96be\u4ee5\u6293\u53d6\u7684\u7269\u4f53\u3002", "motivation": "\u73b0\u6709\u975e\u6293\u53d6\u5f0f\u64cd\u4f5c\u7814\u7a76\u591a\u4f9d\u8d56\u4e8e\u5e73\u884c\u989a\u5939\u5177\u6216\u5de5\u5177\uff0c\u800c\u591a\u6307\u7075\u5de7\u624b\u867d\u64cd\u4f5c\u6a21\u5f0f\u4e30\u5bcc\uff0c\u4f46\u5176\u52a8\u529b\u5b66\u5efa\u6a21\u56f0\u96be\u3002\u672c\u7814\u7a76\u65e8\u5728\u5229\u7528\u7075\u5de7\u624b\u7684\u4f18\u52bf\uff0c\u514b\u670d\u5efa\u6a21\u96be\u9898\uff0c\u5b9e\u73b0\u66f4\u901a\u7528\u7684\u975e\u6293\u53d6\u5f0f\u64cd\u4f5c\u3002", "method": "GD2P\u65b9\u6cd5\u901a\u8fc7\u7efc\u5408\u548c\u5b66\u4e60\u9884\u63a5\u89e6\u7684\u7075\u5de7\u624b\u59ff\u52bf\u6765\u5408\u6210\u6709\u6548\u7684\u64cd\u4f5c\u3002\u5177\u4f53\u6b65\u9aa4\u5305\u62ec\uff1a1. \u4f7f\u7528\u63a5\u89e6\u5f15\u5bfc\u91c7\u6837\u751f\u6210\u591a\u6837\u5316\u7684\u624b\u90e8\u59ff\u52bf\uff1b2. \u901a\u8fc7\u7269\u7406\u6a21\u62df\u8fc7\u6ee4\u8fd9\u4e9b\u59ff\u52bf\uff1b3. \u8bad\u7ec3\u4e00\u4e2a\u4ee5\u7269\u4f53\u51e0\u4f55\u4e3a\u6761\u4ef6\u7684\u6269\u6563\u6a21\u578b\u6765\u9884\u6d4b\u53ef\u884c\u7684\u59ff\u52bf\uff1b4. \u5728\u6d4b\u8bd5\u65f6\uff0c\u91c7\u6837\u624b\u90e8\u59ff\u52bf\u5e76\u5229\u7528\u6807\u51c6\u8fd0\u52a8\u89c4\u5212\u5668\u9009\u62e9\u548c\u6267\u884c\u63a8\u62c9\u52a8\u4f5c\u3002", "result": "\u5728Allegro Hand\u4e0a\u8fdb\u884c\u4e86840\u6b21\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\uff0c\u5e76\u5c06GD2P\u4e0e\u57fa\u7ebf\u65b9\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\uff0c\u7ed3\u679c\u8868\u660eGD2P\u4e3a\u8bad\u7ec3\u7075\u5de7\u7684\u975e\u6293\u53d6\u5f0f\u64cd\u4f5c\u7b56\u7565\u63d0\u4f9b\u4e86\u4e00\u6761\u53ef\u6269\u5c55\u7684\u9014\u5f84\u3002\u8be5\u65b9\u6cd5\u8fd8\u6210\u529f\u5e94\u7528\u4e8eLEAP Hand\uff0c\u8bc1\u660e\u4e86\u5176\u5bf9\u4e0d\u540c\u624b\u90e8\u5f62\u6001\u7684\u9002\u7528\u6027\u3002", "conclusion": "GD2P\u662f\u4e00\u79cd\u5229\u7528\u7075\u5de7\u624b\u8fdb\u884c\u975e\u6293\u53d6\u5f0f\u64cd\u4f5c\uff08\u63a8\u548c\u62c9\uff09\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u901a\u8fc7\u51e0\u4f55\u611f\u77e5\u548c\u5b66\u4e60\u9884\u63a5\u89e6\u59ff\u52bf\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002\u8be5\u65b9\u6cd5\u5177\u6709\u53ef\u6269\u5c55\u6027\uff0c\u5e76\u80fd\u9002\u5e94\u4e0d\u540c\u7684\u624b\u90e8\u5f62\u6001\uff0c\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002\u7814\u7a76\u8fd8\u5f00\u6e90\u4e86\u9884\u8bad\u7ec3\u6a21\u578b\u548c\u6570\u636e\u96c6\uff0c\u4ee5\u4fc3\u8fdb\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2509.18183", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18183", "abs": "https://arxiv.org/abs/2509.18183", "authors": ["Jinyue Bian", "Zhaoxing Zhang", "Zhengyu Liang", "Shiwei Zheng", "Shengtao Zhang", "Rong Shen", "Chen Yang", "Anzhou Hou"], "title": "VLA-LPAF: Lightweight Perspective-Adaptive Fusion for Vision-Language-Action to Enable More Unconstrained Robotic Manipulation", "comment": null, "summary": "The Visual-Language-Action (VLA) models can follow text instructions\naccording to visual observations of the surrounding environment. This ability\nto map multimodal inputs to actions is derived from the training of the VLA\nmodel on extensive standard demonstrations. These visual observations captured\nby third-personal global and in-wrist local cameras are inevitably varied in\nnumber and perspective across different environments, resulting in significant\ndifferences in the visual features. This perspective heterogeneity constrains\nthe generality of VLA models. In light of this, we first propose the\nlightweight module VLA-LPAF to foster the perspective adaptivity of VLA models\nusing only 2D data. VLA-LPAF is finetuned using images from a single view and\nfuses other multiview observations in the latent space, which effectively and\nefficiently bridge the gap caused by perspective inconsistency. We instantiate\nour VLA-LPAF framework with the VLA model RoboFlamingo to construct\nRoboFlamingo-LPAF. Experiments show that RoboFlamingo-LPAF averagely achieves\naround 8% task success rate improvement on CALVIN, 15% on LIBERO, and 30% on a\ncustomized simulation benchmark. We also demonstrate the developed viewadaptive\ncharacteristics of the proposed RoboFlamingo-LPAF through real-world tasks.", "AI": {"tldr": "VLA\u6a21\u578b\u5b58\u5728\u89c6\u89d2\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u63d0\u51faVLA-LPAF\u6a21\u5757\uff0c\u901a\u8fc7\u5355\u89c6\u89d2\u5fae\u8c03\u5e76\u878d\u5408\u591a\u89c6\u89d2\u4fe1\u606f\uff0c\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u663e\u8457\u6548\u679c\u3002", "motivation": "\u73b0\u6709VLA\u6a21\u578b\u56e0\u89c6\u89d2\u4e0d\u4e00\u81f4\u5bfc\u81f4\u6cdb\u5316\u80fd\u529b\u53d7\u9650\u3002", "method": "\u63d0\u51fa\u8f7b\u91cf\u7ea7\u6a21\u5757VLA-LPAF\uff0c\u91c7\u75282D\u6570\u636e\uff0c\u901a\u8fc7\u5355\u89c6\u89d2\u5fae\u8c03\u5e76\u878d\u5408\u591a\u89c6\u89d2\u4fe1\u606f\u6765\u89e3\u51b3\u89c6\u89d2\u4e0d\u4e00\u81f4\u95ee\u9898\u3002", "result": "\u5728CALVIN\u3001LIBERO\u548c\u81ea\u5b9a\u4e49\u6a21\u62df\u57fa\u51c6\u4e0a\u5206\u522b\u5e73\u5747\u63d0\u9ad8\u4e868%\u300115%\u548c30%\u7684\u4efb\u52a1\u6210\u529f\u7387\uff0c\u5e76\u5728\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\u5c55\u793a\u4e86\u89c6\u56fe\u9002\u5e94\u6027\u3002", "conclusion": "VLA-LPAF\u6a21\u5757\u80fd\u6709\u6548\u63d0\u5347VLA\u6a21\u578b\u7684\u89c6\u89d2\u9002\u5e94\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2509.18329", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2509.18329", "abs": "https://arxiv.org/abs/2509.18329", "authors": ["Mark Carney", "Victoria Kumaran"], "title": "Uncut Gem - An Open-Source Hackable Quantum Sensor", "comment": "9 pages, 3 figures, 1 table", "summary": "This work presents an overview of our fully open-source, hackable quantum\nsensor platform based on nitrogen-vacancy (NV) center diamond magnetometry.\nThis initiative aims to democratize access to quantum sensing by providing a\ncomprehensive, modular, and cost-effective system. The design leverages\nconsumer off-the-shelf (COTS) components in a novel hardware configuration,\ncomplemented by open-source firmware written in the Arduino IDE, facilitating\nportability, ease of customization, and future-proofing the design. By lowering\nthe barriers to entry, our sensor serves as a compact platform for education,\nresearch, and innovation in quantum technologies, embodying the ethos of open\nscience and community-driven development.", "AI": {"tldr": "\u4e00\u4e2a\u5f00\u6e90\u3001\u53ef\u5b9a\u5236\u7684\u57fa\u4e8e\u91d1\u521a\u77f3NV\u8272\u5fc3\u78c1\u529b\u8ba1\u7684\u91cf\u5b50\u4f20\u611f\u5668\u5e73\u53f0\uff0c\u65e8\u5728\u964d\u4f4e\u91cf\u5b50\u4f20\u611f\u6280\u672f\u7684\u5165\u95e8\u95e8\u69db\uff0c\u4fc3\u8fdb\u6559\u80b2\u3001\u7814\u7a76\u548c\u521b\u65b0\u3002", "motivation": "\u4e3a\u4e86\u666e\u53ca\u91cf\u5b50\u4f20\u611f\u6280\u672f\uff0c\u63d0\u4f9b\u4e00\u4e2a\u5168\u9762\u3001\u6a21\u5757\u5316\u4e14\u6210\u672c\u6548\u76ca\u9ad8\u7684\u7cfb\u7edf\u3002", "method": "\u5229\u7528\u73b0\u6210\u7684\u6d88\u8d39\u7ea7\uff08COTS\uff09\u7ec4\u4ef6\uff0c\u7ed3\u5408\u65b0\u9896\u7684\u786c\u4ef6\u914d\u7f6e\u548c\u5f00\u6e90\u7684Arduino\u56fa\u4ef6\uff0c\u5b9e\u73b0\u4fbf\u643a\u6027\u3001\u6613\u5b9a\u5236\u6027\u548c\u524d\u77bb\u6027\u3002", "result": "\u6210\u529f\u6784\u5efa\u4e86\u4e00\u4e2a\u7d27\u51d1\u7684\u5e73\u53f0\uff0c\u53ef\u7528\u4e8e\u6559\u80b2\u3001\u7814\u7a76\u548c\u91cf\u5b50\u6280\u672f\u521b\u65b0\u3002", "conclusion": "\u8be5\u5e73\u53f0\u4f53\u73b0\u4e86\u5f00\u653e\u79d1\u5b66\u548c\u793e\u533a\u9a71\u52a8\u5f00\u53d1\u7684\u7406\u5ff5\uff0c\u901a\u8fc7\u964d\u4f4e\u6280\u672f\u58c1\u5792\uff0c\u4f7f\u66f4\u591a\u4eba\u80fd\u591f\u63a5\u89e6\u548c\u4f7f\u7528\u91cf\u5b50\u4f20\u611f\u6280\u672f\u3002"}}
{"id": "2509.18112", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18112", "abs": "https://arxiv.org/abs/2509.18112", "authors": ["Sheng Wong", "Ravi Shankar", "Beth Albert", "Gabriel Davis Jones"], "title": "Large language models surpass domain-specific architectures for antepartum electronic fetal monitoring analysis", "comment": "Preparing for journal", "summary": "Foundation models (FMs) and large language models (LLMs) demonstrate\nremarkable capabilities across diverse domains through training on massive\ndatasets. These models have demonstrated exceptional performance in healthcare\napplications, yet their potential for electronic fetal monitoring\n(EFM)/cardiotocography (CTG) analysis, a critical technology for evaluating\nfetal well-being, remains largely underexplored. Antepartum CTG interpretation\npresents unique challenges due to the complex nature of fetal heart rate (FHR)\npatterns and uterine activity, requiring sophisticated analysis of long\ntime-series data. The assessment of CTG is heavily based on subjective clinical\ninterpretation, often leading to variability in diagnostic accuracy and\ndeviation from timely pregnancy care. This study presents the first\ncomprehensive comparison of state-of-the-art AI approaches for automated\nantepartum CTG analysis. We systematically compare time-series FMs and LLMs\nagainst established CTG-specific architectures. Our evaluation encompasses over\n500 CTG recordings of varying durations reflecting real-world clinical\nrecordings, providing robust performance benchmarks across different modelling\nparadigms. Our results demonstrate that fine-tuned LLMs achieve superior\nperformance compared to both foundation models and domain-specific approaches,\noffering a promising alternative pathway for clinical CTG interpretation. These\nfindings provide critical insights into the relative strengths of different AI\nmethodologies for fetal monitoring applications and establish a foundation for\nfuture clinical AI development in prenatal care.", "AI": {"tldr": "\u672c\u7814\u7a76\u9996\u6b21\u5168\u9762\u6bd4\u8f83\u4e86\u7528\u4e8e\u4ea7\u524d\u80ce\u5fc3\u76d1\u62a4\u56fe\uff08CTG\uff09\u5206\u6790\u7684\u5404\u79cd\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u65b9\u6cd5\uff0c\u53d1\u73b0\u5fae\u8c03\u540e\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\uff0c\u4e3a\u4ea7\u524d\u62a4\u7406\u4e2d\u7684AI\u4e34\u5e8a\u5e94\u7528\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "motivation": "\u7535\u5b50\u80ce\u513f\u76d1\u62a4\uff08EFM\uff09/\u5fc3\u7535\u56fe\uff08CTG\uff09\u5206\u6790\u5bf9\u4e8e\u8bc4\u4f30\u80ce\u513f\u5065\u5eb7\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u4e3b\u8981\u4f9d\u8d56\u4e3b\u89c2\u7684\u4e34\u5e8a\u89e3\u91ca\uff0c\u5bfc\u81f4\u8bca\u65ad\u51c6\u786e\u6027\u4e0d\u4e00\u3002\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\uff08FMs\uff09\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u533b\u7597\u9886\u57df\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5728CTG\u5206\u6790\u4e2d\u7684\u6f5c\u529b\u5c1a\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "\u672c\u7814\u7a76\u7cfb\u7edf\u5730\u6bd4\u8f83\u4e86\u65f6\u95f4\u5e8f\u5217FMs\u548cLLMs\u4e0e\u73b0\u6709\u7684\u3001\u4e13\u95e8\u7528\u4e8eCTG\u7684AI\u67b6\u6784\u3002\u8bc4\u4f30\u6db5\u76d6\u4e86\u8d85\u8fc7500\u4efd\u4e0d\u540c\u957f\u5ea6\u7684\u771f\u5b9e\u4e34\u5e8aCTG\u8bb0\u5f55\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u7ecf\u8fc7\u5fae\u8c03\u7684LLMs\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u57fa\u7840\u6a21\u578b\u548c\u9886\u57df\u7279\u5b9a\u7684\u65b9\u6cd5\u3002", "conclusion": "\u5fae\u8c03\u540e\u7684LLMs\u4e3a\u4e34\u5e8aCTG\u89e3\u91ca\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u9014\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5e76\u4e3a\u672a\u6765\u4ea7\u524d\u62a4\u7406\u4e2d\u7684\u4e34\u5e8aAI\u53d1\u5c55\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.18316", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18316", "abs": "https://arxiv.org/abs/2509.18316", "authors": ["Saksham Khatwani", "He Cheng", "Majid Afshar", "Dmitriy Dligach", "Yanjun Gao"], "title": "Brittleness and Promise: Knowledge Graph Based Reward Modeling for Diagnostic Reasoning", "comment": null, "summary": "Large language models (LLMs) show promise for diagnostic reasoning but often\nlack reliable, knowledge grounded inference. Knowledge graphs (KGs), such as\nthe Unified Medical Language System (UMLS), offer structured biomedical\nknowledge that can support trustworthy reasoning. Prior approaches typically\nintegrate KGs via retrieval augmented generation or fine tuning, inserting KG\ncontent into prompts rather than enabling structured reasoning. We explore an\nalternative paradigm: treating the LLM as a reward model of KG reasoning paths,\nwhere the model learns to judge whether a candidate path leads to correct\ndiagnosis for a given patient input. This approach is inspired by recent work\nthat leverages reward training to enhance model reasoning abilities, and\ngrounded in computational theory, which suggests that verifying a solution is\noften easier than generating one from scratch. It also parallels physicians'\ndiagnostic assessment, where they judge which sequences of findings and\nintermediate conditions most plausibly support a diagnosis. We first\nsystematically evaluate five task formulation for knowledge path judging and\neight training paradigm. Second, we test whether the path judging abilities\ngeneralize to downstream diagnostic tasks, including diagnosis summarization\nand medical question answering. Experiments with three open source\ninstruct-tuned LLMs reveal both promise and brittleness: while specific reward\noptimization and distillation lead to strong path-judging performance, the\ntransferability to downstream tasks remain weak. Our finding provides the first\nsystematic assessment of \"reward model style\" reasoning over clinical KGs,\noffering insights into how structured, reward-based supervision influences\ndiagnostic reasoning in GenAI systems for healthcare.", "AI": {"tldr": "LLMs\u5728\u8bca\u65ad\u63a8\u7406\u65b9\u9762\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46\u7f3a\u4e4f\u53ef\u9760\u7684\u77e5\u8bc6\u652f\u6301\u3002\u672c\u6587\u63d0\u51fa\u5c06LLM\u4f5c\u4e3a\u77e5\u8bc6\u56fe\u8c31\uff08KG\uff09\u63a8\u7406\u8def\u5f84\u7684\u5956\u52b1\u6a21\u578b\uff0c\u4ee5\u5224\u65ad\u5019\u9009\u8def\u5f84\u662f\u5426\u80fd\u6b63\u786e\u8bca\u65ad\u60a3\u8005\uff0c\u800c\u975e\u76f4\u63a5\u751f\u6210\u3002\u7814\u7a76\u8bc4\u4f30\u4e86\u4e94\u79cd\u4efb\u52a1\u5236\u5b9a\u548c\u516b\u79cd\u8bad\u7ec3\u8303\u5f0f\uff0c\u5e76\u6d4b\u8bd5\u4e86\u5176\u5728\u8bca\u65ad\u603b\u7ed3\u548c\u533b\u5b66\u95ee\u7b54\u7b49\u4e0b\u6e38\u4efb\u52a1\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u3002\u7ed3\u679c\u8868\u660e\uff0c\u867d\u7136\u7279\u5b9a\u5956\u52b1\u4f18\u5316\u548c\u84b8\u998f\u80fd\u63d0\u9ad8\u8def\u5f84\u5224\u65ad\u6027\u80fd\uff0c\u4f46\u8fc1\u79fb\u5230\u4e0b\u6e38\u4efb\u52a1\u7684\u6548\u679c\u6709\u9650\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u8bca\u65ad\u63a8\u7406\u65b9\u9762\u6709\u6f5c\u529b\uff0c\u4f46\u7f3a\u4e4f\u53ef\u9760\u7684\u3001\u57fa\u4e8e\u77e5\u8bc6\u7684\u63a8\u7406\u80fd\u529b\u3002\u77e5\u8bc6\u56fe\u8c31\uff08KG\uff09\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u7684\u751f\u7269\u533b\u5b66\u77e5\u8bc6\uff0c\u53ef\u4ee5\u652f\u6301\u53ef\u4fe1\u8d56\u7684\u63a8\u7406\u3002\u4ee5\u5f80\u7684\u65b9\u6cd5\u901a\u5e38\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6216\u5fae\u8c03\u6765\u96c6\u6210KG\uff0c\u4f46\u672a\u80fd\u5b9e\u73b0\u7ed3\u6784\u5316\u63a8\u7406\u3002\u56e0\u6b64\uff0c\u63a2\u7d22\u4e00\u79cd\u65b0\u7684\u8303\u5f0f\uff0c\u5c06LLM\u4f5c\u4e3aKG\u63a8\u7406\u8def\u5f84\u7684\u5956\u52b1\u6a21\u578b\uff0c\u4ee5\u63d0\u9ad8\u5176\u63a8\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u5c06LLM\u89c6\u4e3a\u77e5\u8bc6\u56fe\u8c31\uff08KG\uff09\u63a8\u7406\u8def\u5f84\u7684\u5956\u52b1\u6a21\u578b\uff0c\u8ba9\u6a21\u578b\u5b66\u4e60\u5224\u65ad\u5019\u9009\u8def\u5f84\u662f\u5426\u80fd\u6b63\u786e\u8bca\u65ad\u60a3\u8005\u3002\u8be5\u65b9\u6cd5\u53d7\u8fd1\u671f\u5229\u7528\u5956\u52b1\u8bad\u7ec3\u589e\u5f3a\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u5de5\u4f5c\u542f\u53d1\uff0c\u5e76\u501f\u9274\u4e86\u8ba1\u7b97\u7406\u8bba\u4e2d\u9a8c\u8bc1\u89e3\u901a\u5e38\u6bd4\u4ece\u5934\u751f\u6210\u89e3\u66f4\u5bb9\u6613\u7684\u89c2\u70b9\u3002\u5b9e\u9a8c\u4e2d\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e86\u4e94\u79cd\u7528\u4e8e\u77e5\u8bc6\u8def\u5f84\u5224\u65ad\u7684\u4efb\u52a1\u5236\u5b9a\u548c\u516b\u79cd\u8bad\u7ec3\u8303\u5f0f\uff0c\u5e76\u6d4b\u8bd5\u4e86\u8fd9\u4e9b\u5224\u65ad\u80fd\u529b\u5728\u8bca\u65ad\u603b\u7ed3\u548c\u533b\u5b66\u95ee\u7b54\u7b49\u4e0b\u6e38\u4efb\u52a1\u4e0a\u7684\u6cdb\u5316\u6027\u3002", "result": "\u901a\u8fc7\u5bf9\u4e09\u4e2a\u5f00\u6e90\u6307\u4ee4\u5fae\u8c03LLM\u7684\u5b9e\u9a8c\uff0c\u7814\u7a76\u53d1\u73b0\uff0c\u867d\u7136\u7279\u5b9a\u7684\u5956\u52b1\u4f18\u5316\u548c\u84b8\u998f\u65b9\u6cd5\u5e26\u6765\u4e86\u5f3a\u5927\u7684\u8def\u5f84\u5224\u65ad\u6027\u80fd\uff0c\u4f46\u8fd9\u4e9b\u80fd\u529b\u5728\u8fc1\u79fb\u5230\u4e0b\u6e38\u8bca\u65ad\u4efb\u52a1\uff08\u5982\u8bca\u65ad\u603b\u7ed3\u548c\u533b\u5b66\u95ee\u7b54\uff09\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u6cdb\u5316\u80fd\u529b\u8f83\u5f31\u3002", "conclusion": "\u672c\u7814\u7a76\u9996\u6b21\u7cfb\u7edf\u8bc4\u4f30\u4e86\u5728\u4e34\u5e8a\u77e5\u8bc6\u56fe\u8c31\u4e0a\u8fdb\u884c\u201c\u5956\u52b1\u6a21\u578b\u98ce\u683c\u201d\u63a8\u7406\uff0c\u5e76\u63a2\u8ba8\u4e86\u7ed3\u6784\u5316\u3001\u57fa\u4e8e\u5956\u52b1\u7684\u76d1\u7763\u5982\u4f55\u5f71\u54cd\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u533b\u7597\u7cfb\u7edf\u7684\u8bca\u65ad\u63a8\u7406\u80fd\u529b\u3002\u7814\u7a76\u7ed3\u679c\u63ed\u793a\u4e86\u8be5\u65b9\u6cd5\u7684\u6f5c\u529b\u548c\u5c40\u9650\u6027\uff0c\u4e3a\u672a\u6765\u5728\u533b\u7597\u9886\u57df\u5e94\u7528LLM\u548cKG\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002"}}
{"id": "2509.19120", "categories": ["cs.LG", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2509.19120", "abs": "https://arxiv.org/abs/2509.19120", "authors": ["Ferdinand Kahenga", "Antoine Bagula", "Sajal K. Das", "Patrick Sello"], "title": "FedFiTS: Fitness-Selected, Slotted Client Scheduling for Trustworthy Federated Learning in Healthcare AI", "comment": null, "summary": "Federated Learning (FL) has emerged as a powerful paradigm for\nprivacy-preserving model training, yet deployments in sensitive domains such as\nhealthcare face persistent challenges from non-IID data, client unreliability,\nand adversarial manipulation. This paper introduces FedFiTS, a trust and\nfairness-aware selective FL framework that advances the FedFaSt line by\ncombining fitness-based client election with slotted aggregation. FedFiTS\nimplements a three-phase participation strategy-free-for-all training, natural\nselection, and slotted team participation-augmented with dynamic client\nscoring, adaptive thresholding, and cohort-based scheduling to balance\nconvergence efficiency with robustness. A theoretical convergence analysis\nestablishes bounds for both convex and non-convex objectives under standard\nassumptions, while a communication-complexity analysis shows reductions\nrelative to FedAvg and other baselines. Experiments on diverse datasets-medical\nimaging (X-ray pneumonia), vision benchmarks (MNIST, FMNIST), and tabular\nagricultural data (Crop Recommendation)-demonstrate that FedFiTS consistently\noutperforms FedAvg, FedRand, and FedPow in accuracy, time-to-target, and\nresilience to poisoning attacks. By integrating trust-aware aggregation with\nfairness-oriented client selection, FedFiTS advances scalable and secure FL,\nmaking it well suited for real-world healthcare and cross-domain deployments.", "AI": {"tldr": "FedFiTS\u662f\u4e00\u4e2a\u7ed3\u5408\u4e86\u9002\u5e94\u6027\u9009\u62e9\u548c\u5206\u65f6\u805a\u5408\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u63d0\u9ad8\u4e86\u5728\u533b\u7597\u7b49\u654f\u611f\u9886\u57df\u7684\u9690\u79c1\u4fdd\u62a4\u3001\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u5728\u533b\u7597\u7b49\u654f\u611f\u9886\u57df\u9762\u4e34\u7684\u975eIID\u6570\u636e\u3001\u5ba2\u6237\u7aef\u4e0d\u53ef\u9760\u548c\u5bf9\u6297\u6027\u653b\u51fb\u7b49\u6311\u6218\u3002", "method": "FedFiTS\u91c7\u7528\u4e09\u9636\u6bb5\u53c2\u4e0e\u7b56\u7565\uff08\u81ea\u7531\u7ade\u4e89\u3001\u81ea\u7136\u9009\u62e9\u3001\u5206\u65f6\u7ec4\u961f\uff09\uff0c\u7ed3\u5408\u52a8\u6001\u5ba2\u6237\u7aef\u8bc4\u5206\u3001\u81ea\u9002\u5e94\u9608\u503c\u548c\u961f\u5217\u8c03\u5ea6\uff0c\u4ee5\u5e73\u8861\u6536\u655b\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\uff0cFedFiTS\u5728\u51f8\u548c\u975e\u51f8\u76ee\u6807\u4e0b\u90fd\u5177\u6709\u6536\u655b\u6027\u754c\u9650\uff0c\u5e76\u4e14\u901a\u4fe1\u590d\u6742\u5ea6\u4f4e\u4e8eFedAvg\u7b49\u57fa\u7ebf\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0cFedFiTS\u5728\u51c6\u786e\u6027\u3001\u8fbe\u5230\u76ee\u6807\u65f6\u95f4\u4ee5\u53ca\u62b5\u5fa1\u6295\u6bd2\u653b\u51fb\u65b9\u9762\u4f18\u4e8eFedAvg\u3001FedRand\u548cFedPow\u3002", "conclusion": "FedFiTS\u901a\u8fc7\u6574\u5408\u4fe1\u4efb\u611f\u77e5\u805a\u5408\u548c\u9762\u5411\u516c\u5e73\u6027\u7684\u5ba2\u6237\u7aef\u9009\u62e9\uff0c\u63d0\u5347\u4e86\u53ef\u6269\u5c55\u548c\u5b89\u5168\u7684\u8054\u90a6\u5b66\u4e60\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u73b0\u5b9e\u4e16\u754c\u7684\u533b\u7597\u548c\u8de8\u9886\u57df\u90e8\u7f72\u3002"}}
{"id": "2509.18935", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.18935", "abs": "https://arxiv.org/abs/2509.18935", "authors": ["Yiqiao Xu", "Quan Wan", "Alessandra Parisio"], "title": "Frequency-Varying Optimization: A Control Framework for New Dynamic Frequency Response Services", "comment": null, "summary": "To address the variability of renewable generation, initiatives have been\nlaunched globally to provide faster and more effective frequency responses. In\nthe UK, the National Energy System Operator (NESO) has introduced a suite of\nthree new dynamic services, where aggregation of assets is expected to play a\nkey role. For an Aggregated Response Unit (ARU), the required level of\nfrequency response varies with grid frequency, resulting in a frequency-varying\nequality constraint that assets should meet collectively. We show that the\noptimal coordination of an ARU constitutes a Frequency-Varying Optimization\n(FVO) problem, in which the optimal trajectory for each asset evolves\ndynamically. To facilitate online optimization, we reformulate the FVO problem\ninto Tracking of the Optimal Trajectory (TOT) problems, with algorithms\nproposed for two scenarios: one where the asset dynamics are negligible, and\nanother where they must be accounted for. Under reasonable conditions, the ARU\nconverges to the optimal trajectory within a fixed time, and within the maximum\ndelivery time requested by NESO. The proposed framework can be readily\ndistributed to coordinate a large number of assets. Numerical results verify\nthe effectiveness and scalability of the proposed control framework.", "AI": {"tldr": "\u53ef\u518d\u751f\u80fd\u6e90\u53d1\u7535\u5177\u6709\u6ce2\u52a8\u6027\uff0c\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u82f1\u56fd\u56fd\u5bb6\u7535\u7f51\u8fd0\u8425\u5546\uff08NESO\uff09\u5f15\u5165\u4e86\u4e09\u79cd\u65b0\u7684\u52a8\u6001\u670d\u52a1\uff0c\u5176\u4e2d\u8d44\u4ea7\u805a\u5408\u5728\u5176\u4e2d\u8d77\u7740\u5173\u952e\u4f5c\u7528\u3002\u672c\u6587\u5c06\u805a\u5408\u54cd\u5e94\u5355\u5143\uff08ARU\uff09\u7684\u9891\u7387\u54cd\u5e94\u95ee\u9898\u5efa\u6a21\u4e3a\u4e00\u4e2a\u9891\u7387\u53d8\u5316\u4f18\u5316\uff08FVO\uff09\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u8ddf\u8e2a\u6700\u4f18\u8f68\u8ff9\uff08TOT\uff09\u7684\u7b97\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u8be5\u7b97\u6cd5\u9002\u7528\u4e8e\u8d44\u4ea7\u52a8\u6001\u53ef\u5ffd\u7565\u548c\u9700\u8981\u8003\u8651\u8d44\u4ea7\u52a8\u6001\u7684\u4e24\u79cd\u60c5\u51b5\u3002\u7ed3\u679c\u8868\u660e\uff0cARU\u53ef\u4ee5\u5728\u56fa\u5b9a\u65f6\u95f4\u5185\u6536\u655b\u5230\u6700\u4f18\u8f68\u8ff9\uff0c\u5e76\u6ee1\u8db3NESO\u8981\u6c42\u7684\u6700\u5927\u4ea4\u4ed8\u65f6\u95f4\u3002\u8be5\u6846\u67b6\u6613\u4e8e\u5206\u5e03\u5f0f\u90e8\u7f72\uff0c\u53ef\u7528\u4e8e\u534f\u8c03\u5927\u91cf\u8d44\u4ea7\u3002\u6570\u503c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u63a7\u5236\u6846\u67b6\u7684\u6709\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u53ef\u518d\u751f\u80fd\u6e90\u53d1\u7535\u7684\u6ce2\u52a8\u6027\u95ee\u9898\uff0c\u9700\u8981\u66f4\u5feb\u901f\u6709\u6548\u7684\u9891\u7387\u54cd\u5e94\u3002\u82f1\u56fd\u56fd\u5bb6\u7535\u7f51\u8fd0\u8425\u5546\uff08NESO\uff09\u5f15\u5165\u4e86\u65b0\u7684\u52a8\u6001\u670d\u52a1\uff0c\u5176\u4e2d\u8d44\u4ea7\u805a\u5408\uff08ARU\uff09\u662f\u5173\u952e\u3002ARU\u7684\u9891\u7387\u54cd\u5e94\u9700\u6c42\u968f\u7535\u7f51\u9891\u7387\u53d8\u5316\uff0c\u5f62\u6210\u9891\u7387\u53d8\u5316\u7684\u7b49\u5f0f\u7ea6\u675f\u3002", "method": "\u5c06ARU\u7684\u4f18\u5316\u534f\u8c03\u95ee\u9898\u5efa\u6a21\u4e3a\u9891\u7387\u53d8\u5316\u4f18\u5316\uff08FVO\uff09\u95ee\u9898\u3002\u5c06FVO\u95ee\u9898\u91cd\u65b0\u5236\u5b9a\u4e3a\u8ddf\u8e2a\u6700\u4f18\u8f68\u8ff9\uff08TOT\uff09\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e24\u79cd\u7b97\u6cd5\uff1a\u4e00\u79cd\u9002\u7528\u4e8e\u8d44\u4ea7\u52a8\u6001\u53ef\u5ffd\u7565\u7684\u60c5\u51b5\uff0c\u53e6\u4e00\u79cd\u9002\u7528\u4e8e\u9700\u8981\u8003\u8651\u8d44\u4ea7\u52a8\u6001\u7684\u60c5\u51b5\u3002", "result": "\u5728\u5408\u7406\u6761\u4ef6\u4e0b\uff0cARU\u53ef\u4ee5\u5728\u56fa\u5b9a\u65f6\u95f4\u5185\u6536\u655b\u5230\u6700\u4f18\u8f68\u8ff9\uff0c\u5e76\u5728NESO\u8981\u6c42\u7684\u6700\u5927\u4ea4\u4ed8\u65f6\u95f4\u5185\u6ee1\u8db3\u8981\u6c42\u3002\u8be5\u6846\u67b6\u53ef\u5206\u5e03\u5f0f\u90e8\u7f72\uff0c\u6613\u4e8e\u534f\u8c03\u5927\u91cf\u8d44\u4ea7\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8eTOT\u7684\u6846\u67b6\u80fd\u591f\u6709\u6548\u5730\u534f\u8c03ARU\u4ee5\u63d0\u4f9b\u6240\u9700\u7684\u9891\u7387\u54cd\u5e94\uff0c\u5e76\u4e14\u5177\u6709\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\uff0c\u80fd\u591f\u6ee1\u8db3NESO\u7684\u8981\u6c42\u3002"}}
{"id": "2509.19223", "categories": ["quant-ph", "cond-mat.mes-hall", "cond-mat.supr-con"], "pdf": "https://arxiv.org/pdf/2509.19223", "abs": "https://arxiv.org/abs/2509.19223", "authors": ["V. Iaia", "E. S. Joseph", "S. Im", "N. Hagopian", "S. O'Kelley", "C. Kim", "N. Materise", "S. Patra", "V. Lordi", "M. A. Eriksson", "P. M. Voyles", "K. G. Ray", "Y. J. Rosen"], "title": "Non-equilibrium Dynamics of Two-level Systems directly after Cryogenic Alternating Bias", "comment": "14 pages, 9 figures", "summary": "Two-level systems (TLSs) are tunneling states commonly found in amorphous\nmaterials that electrically couple to qubits, resonators, and vibrational modes\nin materials, leading to energy loss in those systems. Recent studies suggest\nthat applying a large alternating electric field changes the oxide structure,\npotentially improving the performance of qubits and resonators. In this study,\nwe probe the effect of alternating bias at cryogenic temperatures on TLS\ndynamics within amorphous oxide parallel-plate capacitors operating in the\nstrongly coupled regime. We bias the TLSs in the capacitors using an electric\nfield. This allows us to spectroscopically image TLSs and extract their\ndensities and dipole moments. When an in-situ alternating bias is applied, the\nsteady-state spectra from the standard TLS model disappear. Post-alternating\nbias TLS spectroscopy reveals transient behavior, in which the TLS frequency\nfluctuates on the order of minutes. Thermal cycling above 10 K reverses these\neffects, restoring the TLS spectrum to its original state, indicating a\nreversible mechanism. Importantly, the intrinsic loss tangent of the LC\noscillator remains unchanged before and after the application of the\nalternating bias. We propose that the disappearance of the steady-state\nspectrum are caused by non-equilibrium energy build up from strain in the oxide\nfilm introduced by the pulsed voltage bias sequence. Understanding this\nnon-equilibrium energy could inform future models of time-dependent TLS\ndynamics.", "AI": {"tldr": "\u5927\u578b\u4ea4\u6d41\u7535\u573a\u53ef\u9006\u5730\u6539\u53d8\u975e\u6676\u6c27\u5316\u7269\u4e2d\u7684\u4e24\u7ea7\u7cfb\u7edf\u52a8\u529b\u5b66\uff0c\u4f46\u4e0d\u4f1a\u5f71\u54cdLC\u632f\u8361\u5668\u7684\u635f\u8017\u3002", "motivation": "\u4e24\u7ea7\u7cfb\u7edf\uff08TLS\uff09\u4f1a\u964d\u4f4e\u91cf\u5b50\u6bd4\u7279\u548c\u8c10\u632f\u5668\u7684\u6027\u80fd\uff0c\u4f46\u4e4b\u524d\u7684\u7814\u7a76\u8868\u660e\uff0c\u65bd\u52a0\u5927\u578b\u4ea4\u6d41\u7535\u573a\u53ef\u4ee5\u6539\u53d8\u5176\u6027\u8d28\u3002", "method": "\u5728\u4f4e\u6e29\u4e0b\uff0c\u5bf9\u975e\u6676\u6c27\u5316\u7269\u5e73\u884c\u677f\u7535\u5bb9\u5668\u4e2d\u7684TLS\u65bd\u52a0\u4ea4\u6d41\u7535\u573a\uff0c\u5e76\u901a\u8fc7\u5149\u8c31\u5b66\u6210\u50cf\u6765\u7814\u7a76TLS\u3002", "result": "\u4ea4\u6d41\u7535\u573a\u5bfc\u81f4TLS\u7a33\u6001\u5149\u8c31\u6d88\u5931\uff0c\u5e76\u51fa\u73b0\u6301\u7eed\u6570\u5206\u949f\u7684\u77ac\u6001\u884c\u4e3a\u3002\u8fd9\u79cd\u6548\u5e94\u572810K\u4ee5\u4e0a\u53ef\u901a\u8fc7\u70ed\u5faa\u73af\u9006\u8f6c\u3002LC\u632f\u8361\u5668\u7684\u56fa\u6709\u635f\u8017\u56e0\u5b50\u4fdd\u6301\u4e0d\u53d8\u3002", "conclusion": "\u4ea4\u6d41\u7535\u573a\u5f15\u8d77\u7684TLS\u884c\u4e3a\u6539\u53d8\u53ef\u80fd\u662f\u7531\u8109\u51b2\u7535\u538b\u5f15\u8d77\u7684\u5e94\u53d8\u5bfc\u81f4\u7684\u975e\u5e73\u8861\u80fd\u91cf\u79ef\u7d2f\u9020\u6210\u7684\u3002\u672a\u6765\u7684\u7814\u7a76\u5e94\u5173\u6ce8\u8fd9\u79cd\u975e\u5e73\u8861\u80fd\u91cf\u53ca\u5176\u5bf9TLS\u52a8\u529b\u5b66\u7684\u5f71\u54cd\u3002"}}
{"id": "2509.18617", "categories": ["cond-mat.mtrl-sci", "quant-ph"], "pdf": "https://arxiv.org/pdf/2509.18617", "abs": "https://arxiv.org/abs/2509.18617", "authors": ["H. Joshi", "K. C. Bhamu", "A. Shankar", "Rana Biswas", "M. Wlaz\u0142o"], "title": "Octahedral dynamics and local symmetry in hybrid perovskite FAPbI3 under thermal excitation", "comment": "14 pages, 5 figures", "summary": "Density Functional Theory (DFT) and ab initio molecular dynamics (AIMD)\nsimulations have been employed to investigate the evolution of local motifs\nwithin the tetragonal phase of FAPbI3 under thermal excitation. Our results\nreveal a distinct broadening in the distribution of PbI6 octahedral volumes\nwith increasing temperature, indicating a gradual breakdown of symmetry and\nemergence of diverse local environments. These octahedral volume distortions\nare primarily driven by the dynamic behaviour of the FA cation leading to\nsoftening of PbI6 octahedra, evident from calculated mean octahedral volume and\nPb-I-Pb bond angles. The examination of electronic structure confirmed that\nthis dynamic structural phenomenon is directly responsible for the change in\nfundamental band gap value, highlighting the role of PbI6 octahedra in\nmodifying and modulating the electronic properties in FAPbI3. The results\ndemonstrate the microscopic origin of thermally induced dynamical behaviour to\nthe macroscopic electronic properties and underscore the pivotal role of local\nmotifs in hybrid perovskites.", "AI": {"tldr": "DFT\u548cAIMD\u6a21\u62df\u63ed\u793a\u4e86FAPbI3\u4e2dPbI6\u516b\u9762\u4f53\u4f53\u79ef\u968f\u6e29\u5ea6\u53d8\u5316\u7684\u52a8\u529b\u5b66\u884c\u4e3a\uff0c\u5e76\u89e3\u91ca\u4e86\u5176\u5bf9\u5e26\u9699\u7684\u5f71\u54cd\u3002", "motivation": "\u7814\u7a76\u70ed\u6fc0\u53d1\u4e0bFAPbI3\u4e2d\u5c40\u90e8\u7ed3\u6784\u5355\u5143\uff08PbI6\u516b\u9762\u4f53\uff09\u7684\u6f14\u53d8\u53ca\u5176\u5bf9\u5b8f\u89c2\u7535\u5b50\u6027\u8d28\u7684\u5f71\u54cd\u3002", "method": "\u4f7f\u7528\u5bc6\u5ea6\u6cdb\u51fd\u7406\u8bba\uff08DFT\uff09\u548c\u4ece\u5934\u5206\u5b50\u52a8\u529b\u5b66\uff08AIMD\uff09\u6a21\u62df\u7814\u7a76\u4e86FAPbI3\u5728\u70ed\u6fc0\u53d1\u4e0b\u7684\u7ed3\u6784\u6f14\u53d8\u548c\u7535\u5b50\u7ed3\u6784\u3002", "result": "\u968f\u7740\u6e29\u5ea6\u5347\u9ad8\uff0cPbI6\u516b\u9762\u4f53\u4f53\u79ef\u5206\u5e03\u53d8\u5bbd\uff0c\u516b\u9762\u4f53\u4f53\u79ef\u548c\u952e\u89d2\u51cf\u5c0f\uff0c\u8868\u660e\u5bf9\u79f0\u6027\u7834\u574f\u548c\u5c40\u90e8\u73af\u5883\u591a\u6837\u5316\u3002FA\u9633\u79bb\u5b50\u7684\u52a8\u529b\u5b66\u884c\u4e3a\u662f\u5bfc\u81f4\u516b\u9762\u4f53\u4f53\u79ef\u53d8\u5316\u548cPbI6\u516b\u9762\u4f53\u8f6f\u5316\u7684\u4e3b\u8981\u539f\u56e0\u3002\u7ed3\u6784\u53d8\u5316\u76f4\u63a5\u5bfc\u81f4\u5e26\u9699\u53d8\u5316\u3002", "conclusion": "FAPbI3\u4e2d\u70ed\u8bf1\u5bfc\u7684\u52a8\u529b\u5b66\u884c\u4e3a\u662f\u5176\u5b8f\u89c2\u7535\u5b50\u6027\u8d28\uff08\u5982\u5e26\u9699\uff09\u53d8\u5316\u7684\u539f\u56e0\uff0cPbI6\u516b\u9762\u4f53\u5728\u8c03\u8282\u5176\u7535\u5b50\u6027\u8d28\u65b9\u9762\u8d77\u7740\u5173\u952e\u4f5c\u7528\u3002"}}
{"id": "2509.19092", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.19092", "abs": "https://arxiv.org/abs/2509.19092", "authors": ["Abolfazl Zakeri", "Nhan Thanh Nguyen", "Ahmed Alkhateeb", "Markku Juntti"], "title": "Data-Free Knowledge Distillation for LiDAR-Aided Beam Tracking in MmWave Systems", "comment": "Submitted for possible publication", "summary": "Multimodal sensing reduces beam training overhead but is constrained by\nmachine learning complexity and dataset demands. To address this, we propose a\ndata-free (DF) knowledge distillation (KD) framework for efficient LiDAR-aided\nmmWave beam tracking, i.e., predicting the best current and future beams.\nSpecifically, we propose a knowledge inversion framework, where a generator\nsynthesizes LiDAR input data from random noise, guided by a loss function\ndefined on the features and outputs of a pre-trained teacher model. The student\nmodel is then trained using the synthetic data and knowledge distilled from the\nteacher. The generator loss combines three terms, called metadata loss,\nactivation loss, and entropy loss. For student training, in addition to the\nstandard Kullback-Leibler divergence loss, we also consider a mean-squared\nerror (MSE) loss between the teacher and student logits. Simulation results\nshow that the proposed DF-KD (slightly) outperforms the teacher in Top-1 and\nTop-5 accuracies. Moreover, we observe that the metadata loss contributes\nsignificantly to the generator performance, and that the MSE loss for the\nstudent can effectively replace the standard KD loss while requiring fewer\nfine-tuned hyperparameters.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u6570\u636e\u65e0\u5173\u7684\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u6548\u7684\u6fc0\u5149\u96f7\u8fbe\u8f85\u52a9\u6beb\u7c73\u6ce2\u6ce2\u675f\u8ddf\u8e2a\uff0c\u53d6\u5f97\u4e86\u4f18\u4e8e\u6559\u5e08\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u591a\u6a21\u6001\u4f20\u611f\u867d\u7136\u80fd\u964d\u4f4e\u8bad\u7ec3\u5f00\u9500\uff0c\u4f46\u53d7\u9650\u4e8e\u673a\u5668\u5b66\u4e60\u590d\u6742\u5ea6\u548c\u6570\u636e\u96c6\u9700\u6c42\u3002\u73b0\u6709\u7684\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6570\u636e\uff0c\u800c\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u6570\u636e\u65e0\u5173\u7684\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\u3002\u901a\u8fc7\u751f\u6210\u5668\u5408\u6210\u6fc0\u5149\u96f7\u8fbe\u8f93\u5165\u6570\u636e\uff0c\u5e76\u7ed3\u5408\u5143\u6570\u636e\u635f\u5931\u3001\u6fc0\u6d3b\u635f\u5931\u548c\u71b5\u635f\u5931\u6765\u6307\u5bfc\u751f\u6210\u5668\u3002\u5b66\u751f\u6a21\u578b\u901a\u8fc7\u5408\u6210\u6570\u636e\u548c\u6559\u5e08\u6a21\u578b\u8fdb\u884c\u77e5\u8bc6\u84b8\u998f\uff0c\u540c\u65f6\u8003\u8651KL\u6563\u5ea6\u548c\u6559\u5e08-\u5b66\u751f\u6a21\u578b\u9884\u6d4b\u4e4b\u95f4\u7684\u5747\u65b9\u8bef\u5dee\uff08MSE\uff09\u635f\u5931\u3002", "result": "\u6240\u63d0\u51fa\u7684\u6570\u636e\u65e0\u5173\u77e5\u8bc6\u84b8\u998f\uff08DF-KD\uff09\u6846\u67b6\u5728Top-1\u548cTop-5\u51c6\u786e\u7387\u4e0a\u7565\u4f18\u4e8e\u6559\u5e08\u6a21\u578b\u3002\u5143\u6570\u636e\u635f\u5931\u5bf9\u751f\u6210\u5668\u6027\u80fd\u6709\u663e\u8457\u8d21\u732e\uff0c\u5e76\u4e14MSE\u635f\u5931\u53ef\u4ee5\u6709\u6548\u66ff\u4ee3\u6807\u51c6\u7684\u77e5\u8bc6\u84b8\u998f\u635f\u5931\uff0c\u540c\u65f6\u9700\u8981\u66f4\u5c11\u7684\u8d85\u53c2\u6570\u8c03\u6574\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u9ad8\u6548\u5730\u8fdb\u884c\u6fc0\u5149\u96f7\u8fbe\u8f85\u52a9\u7684\u6beb\u7c73\u6ce2\u6ce2\u675f\u8ddf\u8e2a\uff0c\u5e76\u4e14\u5728\u4e0d\u4f9d\u8d56\u771f\u5b9e\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u80fd\u591f\u53d6\u5f97\u4f18\u4e8e\u6559\u5e08\u6a21\u578b\u7684\u6027\u80fd\u3002\u5143\u6570\u636e\u635f\u5931\u548cMSE\u635f\u5931\u662f\u63d0\u5347\u6027\u80fd\u7684\u5173\u952e\u56e0\u7d20\u3002"}}
{"id": "2509.18460", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.18460", "abs": "https://arxiv.org/abs/2509.18460", "authors": ["Haeyoon Han", "Mahdi Taheri", "Soon-Jo Chung", "Fred Y. Hadaegh"], "title": "A Counterfactual Reasoning Framework for Fault Diagnosis in Robot Perception Systems", "comment": null, "summary": "Perception systems provide a rich understanding of the environment for\nautonomous systems, shaping decisions in all downstream modules. Hence,\naccurate detection and isolation of faults in perception systems is important.\nFaults in perception systems pose particular challenges: faults are often tied\nto the perceptual context of the environment, and errors in their multi-stage\npipelines can propagate across modules. To address this, we adopt a\ncounterfactual reasoning approach to propose a framework for fault detection\nand isolation (FDI) in perception systems. As opposed to relying on physical\nredundancy (i.e., having extra sensors), our approach utilizes analytical\nredundancy with counterfactual reasoning to construct perception reliability\ntests as causal outcomes influenced by system states and fault scenarios.\nCounterfactual reasoning generates reliability test results under hypothesized\nfaults to update the belief over fault hypotheses. We derive both passive and\nactive FDI methods. While the passive FDI can be achieved by belief updates,\nthe active FDI approach is defined as a causal bandit problem, where we utilize\nMonte Carlo Tree Search (MCTS) with upper confidence bound (UCB) to find\ncontrol inputs that maximize a detection and isolation metric, designated as\nEffective Information (EI). The mentioned metric quantifies the informativeness\nof control inputs for FDI. We demonstrate the approach in a robot exploration\nscenario, where a space robot performing vision-based navigation actively\nadjusts its attitude to increase EI and correctly isolate faults caused by\nsensor damage, dynamic scenes, and perceptual degradation.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u53cd\u4e8b\u5b9e\u63a8\u7406\u8fdb\u884c\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u7cfb\u7edf\u6545\u969c\u68c0\u6d4b\u4e0e\u9694\u79bb\uff08FDI\uff09\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u4e0d\u4f9d\u8d56\u7269\u7406\u5197\u4f59\uff0c\u800c\u662f\u5229\u7528\u5206\u6790\u5197\u4f59\u548c\u53cd\u4e8b\u5b9e\u63a8\u7406\u6765\u6784\u5efa\u611f\u77e5\u53ef\u9760\u6027\u6d4b\u8bd5\uff0c\u5e76\u5c06\u5176\u89c6\u4e3a\u53d7\u7cfb\u7edf\u72b6\u6001\u548c\u6545\u969c\u573a\u666f\u5f71\u54cd\u7684\u56e0\u679c\u7ed3\u679c\u3002", "motivation": "\u611f\u77e5\u7cfb\u7edf\u5bf9\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u51b3\u7b56\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u6b64\u51c6\u786e\u68c0\u6d4b\u548c\u9694\u79bb\u611f\u77e5\u7cfb\u7edf\u4e2d\u7684\u6545\u969c\u975e\u5e38\u91cd\u8981\u3002\u611f\u77e5\u7cfb\u7edf\u6545\u969c\u7684\u7279\u70b9\u662f\u4e0e\u73af\u5883\u611f\u77e5\u4e0a\u4e0b\u6587\u76f8\u5173\uff0c\u5e76\u4e14\u591a\u7ea7\u6d41\u6c34\u7ebf\u4e2d\u7684\u9519\u8bef\u4f1a\u8de8\u6a21\u5757\u4f20\u64ad\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u53cd\u4e8b\u5b9e\u63a8\u7406\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u611f\u77e5\u7cfb\u7edf\u7684\u6545\u969c\u68c0\u6d4b\u4e0e\u9694\u79bb\uff08FDI\uff09\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u5206\u6790\u5197\u4f59\u548c\u53cd\u4e8b\u5b9e\u63a8\u7406\u6765\u6784\u5efa\u611f\u77e5\u53ef\u9760\u6027\u6d4b\u8bd5\uff0c\u5e76\u5c06\u5176\u89c6\u4e3a\u53d7\u7cfb\u7edf\u72b6\u6001\u548c\u6545\u969c\u573a\u666f\u5f71\u54cd\u7684\u56e0\u679c\u7ed3\u679c\u3002\u901a\u8fc7\u53cd\u4e8b\u5b9e\u63a8\u7406\u751f\u6210\u5047\u8bbe\u6545\u969c\u4e0b\u7684\u53ef\u9760\u6027\u6d4b\u8bd5\u7ed3\u679c\uff0c\u4ee5\u66f4\u65b0\u6545\u969c\u5047\u8bbe\u7684\u7f6e\u4fe1\u5ea6\u3002\u63a8\u5bfc\u4e86\u88ab\u52a8\u548c\u4e3b\u52a8FDI\u65b9\u6cd5\uff0c\u5176\u4e2d\u4e3b\u52a8FDI\u88ab\u5b9a\u4e49\u4e3a\u56e0\u679c\u8001\u864e\u673a\u95ee\u9898\uff0c\u5229\u7528\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09\u548c\u4e0a\u9650\u4fe1\u8d56\uff08UCB\uff09\u6765\u5bfb\u627e\u6700\u5927\u5316\u6709\u6548\u4fe1\u606f\uff08EI\uff09\u7684\u63a7\u5236\u8f93\u5165\uff0cEI\u91cf\u5316\u4e86\u63a7\u5236\u8f93\u5165\u5bf9FDI\u7684\u4fe1\u606f\u91cf\u3002", "result": "\u5728\u673a\u5668\u4eba\u63a2\u7d22\u573a\u666f\u4e2d\uff0c\u901a\u8fc7\u89c6\u89c9\u5bfc\u822a\u8fdb\u884c\u64cd\u4f5c\u7684\u592a\u7a7a\u673a\u5668\u4eba\u4e3b\u52a8\u8c03\u6574\u5176\u59ff\u6001\u4ee5\u6700\u5927\u5316EI\uff0c\u5e76\u6210\u529f\u9694\u79bb\u4e86\u7531\u4f20\u611f\u5668\u635f\u574f\u3001\u52a8\u6001\u573a\u666f\u548c\u611f\u77e5\u9000\u5316\u5f15\u8d77\u7684\u6545\u969c\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8e\u53cd\u4e8b\u5b9e\u63a8\u7406\u7684FDI\u6846\u67b6\u80fd\u591f\u6709\u6548\u5730\u68c0\u6d4b\u548c\u9694\u79bb\u673a\u5668\u4eba\u611f\u77e5\u7cfb\u7edf\u4e2d\u7684\u6545\u969c\uff0c\u5e76\u4e14\u5728\u4e3b\u52a8FDI\u4e2d\u901a\u8fc7\u6700\u5927\u5316EI\u53ef\u4ee5\u63d0\u9ad8\u6545\u969c\u8bca\u65ad\u7684\u6548\u7387\u3002"}}
{"id": "2509.18184", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18184", "abs": "https://arxiv.org/abs/2509.18184", "authors": ["Yifeng Cheng", "Alois Knoll", "Hu Cao"], "title": "URNet: Uncertainty-aware Refinement Network for Event-based Stereo Depth Estimation", "comment": "This work is accepted by Visual Intelligence Journal", "summary": "Event cameras provide high temporal resolution, high dynamic range, and low\nlatency, offering significant advantages over conventional frame-based cameras.\nIn this work, we introduce an uncertainty-aware refinement network called URNet\nfor event-based stereo depth estimation. Our approach features a local-global\nrefinement module that effectively captures fine-grained local details and\nlong-range global context. Additionally, we introduce a Kullback-Leibler (KL)\ndivergence-based uncertainty modeling method to enhance prediction reliability.\nExtensive experiments on the DSEC dataset demonstrate that URNet consistently\noutperforms state-of-the-art (SOTA) methods in both qualitative and\nquantitative evaluations.", "AI": {"tldr": "URNet\u662f\u4e00\u4e2a\u7528\u4e8e\u4e8b\u4ef6\u7acb\u4f53\u6d4b\u8ddd\u7684\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7ec6\u5316\u7f51\u7edc\uff0c\u901a\u8fc7\u5c40\u90e8-\u5168\u5c40\u7ec6\u5316\u6a21\u5757\u548c\u57fa\u4e8eKL\u6563\u5ea6\u7684\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\uff0c\u5728DSEC\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u7684\u6027\u80fd\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u76f8\u6bd4\u4f20\u7edf\u5e27\u5f0f\u76f8\u673a\u5177\u6709\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\u3001\u9ad8\u52a8\u6001\u8303\u56f4\u548c\u4f4e\u5ef6\u8fdf\u7b49\u4f18\u52bf\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u9002\u7528\u4e8e\u4e8b\u4ef6\u6570\u636e\u7684\u6df1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aURNet\u7684\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7ec6\u5316\u7f51\u7edc\uff0c\u8be5\u7f51\u7edc\u5305\u542b\u4e00\u4e2a\u5c40\u90e8-\u5168\u5c40\u7ec6\u5316\u6a21\u5757\uff0c\u7528\u4e8e\u6355\u6349\u5c40\u90e8\u7ec6\u8282\u548c\u5168\u5c40\u4e0a\u4e0b\u6587\uff0c\u5e76\u5f15\u5165\u4e86\u57fa\u4e8eKL\u6563\u5ea6\u7684\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u65b9\u6cd5\u3002", "result": "URNet\u5728DSEC\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8bc4\u4f30\uff0c\u5728\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u4f30\u4e2d\u5747\u8868\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "URNet\u901a\u8fc7\u5176\u72ec\u7279\u7684\u5c40\u90e8-\u5168\u5c40\u7ec6\u5316\u548c\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u65b9\u6cd5\uff0c\u6210\u529f\u5730\u63d0\u5347\u4e86\u4e8b\u4ef6\u7acb\u4f53\u6d4b\u8ddd\u7684\u7cbe\u5ea6\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2509.18334", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2509.18334", "abs": "https://arxiv.org/abs/2509.18334", "authors": ["Zhiyao Hu", "Allen Zang", "Jianwei Wang", "Tian Zhong", "Haidong Yuan", "Liang Jiang", "Zain H. Saleem"], "title": "Optimal scheme for distributed quantum metrology", "comment": "11 pages, 3 figures", "summary": "Optimal strategies for local quantum metrology -- including the preparation\nof optimal probe states, implementation of optimal control and measurement\nstrategies, are well established. However, for distributed quantum metrology,\nwhere the goal is to estimate global properties of multiple spatially\ndistributed parameters, the optimal scheme -- particularly the role of optimal\ncontrol -- remains poorly understood. In this work, we address this challenge\nby developing optimal schemes for distributed quantum metrology that\ncharacterize the ultimate precision limits in distributed systems. We derive\nthe optimal probe state, optimal control protocols, and measurement strategies\nin estimating a linear combination of $N$ independent unknown parameters\ncoupled to $d$ networked sensors. Crucially, we prove that the optimal control\noperations can be implemented locally on each sensor, eliminating the need for\nnon-local control operations across distant nodes. This result significantly\nreduces the complexity of implementing optimal strategies in distributed\nquantum metrology. To demonstrate the power of our framework, we apply it to\nseveral key scenarios.", "AI": {"tldr": "\u672c\u7814\u7a76\u89e3\u51b3\u4e86\u5206\u5e03\u5f0f\u91cf\u5b50\u8ba1\u91cf\u5b66\u4e2d\u7684\u6700\u4f18\u63a7\u5236\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u6700\u4f18\u63a7\u5236\u53ef\u4ee5\u672c\u5730\u5b9e\u73b0\uff0c\u4ece\u800c\u7b80\u5316\u4e86\u5206\u5e03\u5f0f\u91cf\u5b50\u8ba1\u91cf\u5b66\u7684\u5b9e\u65bd\u3002", "motivation": "\u76ee\u524d\u6700\u4f18\u7684\u91cf\u5b50\u8ba1\u91cf\u7b56\u7565\u4e3b\u8981\u96c6\u4e2d\u5728\u672c\u5730\u573a\u666f\uff0c\u800c\u5bf9\u4e8e\u5206\u5e03\u5f0f\u573a\u666f\uff0c\u7279\u522b\u662f\u5176\u4e2d\u6700\u4f18\u63a7\u5236\u7684\u4f5c\u7528\uff0c\u4ecd\u7f3a\u4e4f\u7406\u89e3\u3002", "method": "\u63a8\u5bfc\u51fa\u4f30\u8ba1N\u4e2a\u72ec\u7acb\u672a\u77e5\u53c2\u6570\u7684\u7ebf\u6027\u7ec4\u5408\u7684\u6700\u4f18\u63a2\u6d4b\u6001\u3001\u6700\u4f18\u63a7\u5236\u534f\u8bae\u548c\u6d4b\u91cf\u7b56\u7565\uff0c\u8fd9\u4e9b\u53c2\u6570\u4e0ed\u4e2a\u7f51\u7edc\u5316\u4f20\u611f\u5668\u8026\u5408\u3002\u8bc1\u660e\u4e86\u6700\u4f18\u63a7\u5236\u53ef\u4ee5\u5728\u6bcf\u4e2a\u4f20\u611f\u5668\u4e0a\u672c\u5730\u5b9e\u73b0\u3002", "result": "\u6700\u4f18\u63a7\u5236\u53ef\u4ee5\u5728\u6bcf\u4e2a\u4f20\u611f\u5668\u4e0a\u672c\u5730\u5b9e\u73b0\uff0c\u65e0\u9700\u8de8\u8282\u70b9\u7684\u975e\u672c\u5730\u63a7\u5236\u64cd\u4f5c\uff0c\u5927\u5927\u964d\u4f4e\u4e86\u5b9e\u65bd\u5206\u5e03\u5f0f\u91cf\u5b50\u8ba1\u91cf\u6700\u4f18\u7b56\u7565\u7684\u590d\u6742\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u5206\u5e03\u5f0f\u91cf\u5b50\u8ba1\u91cf\u5b66\u63d0\u4f9b\u4e86\u6700\u4f18\u65b9\u6848\uff0c\u5e76\u8bc1\u660e\u4e86\u6700\u4f18\u63a7\u5236\u53ef\u4ee5\u5728\u6bcf\u4e2a\u4f20\u611f\u5668\u4e0a\u672c\u5730\u6267\u884c\uff0c\u8fd9\u5bf9\u5b9e\u9645\u5e94\u7528\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2509.18114", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18114", "abs": "https://arxiv.org/abs/2509.18114", "authors": ["Javed I. Khan an Henry Uwabor Moye"], "title": "A Study of Skews, Imbalances, and Pathological Conditions in LLM Inference Deployment on GPU Clusters detectable from DPU", "comment": "12 pages, Technical Report 2025-07-01, Internetworking and Media\n  Communications Research Laboratories, Department of Computer Science, Kent\n  State University", "summary": "Autoregressive inference in large transformer-based language models (LLMs)\npresents significant challenges for runtime efficiency, particularly during the\ndecode phase where load imbalance across GPU shards can cause throughput\ndegradation and latency spikes. A DPU-assisted framework leveraged by\nBlueField-3 Data Processing Units can enable real-time detection and mitigation\nof load imbalance in multi-node tensor-parallel inference. By offloading\nmonitoring tasks to the DPU and analyzing GPU telemetry and inter-node\ncommunication patterns, the resulting system can provide actionable feedback to\ninference controllers and schedulers. The goal of this study is three-fold i)\nidentify the reported skews/imbalances/pathological conditions that arise in\nmuti-GPU execution of a) LLM tensor computing (both during training and\ninference), b) identify their impact on computational performance, and c) make\na critical assessment if those can be tracked for potential mitigation from a\nDPU network.", "AI": {"tldr": "LLM\u63a8\u7406\u4e2d\u7684\u8d1f\u8f7d\u4e0d\u5e73\u8861\u95ee\u9898\u53ef\u4ee5\u901a\u8fc7DPU\u8fdb\u884c\u5b9e\u65f6\u68c0\u6d4b\u548c\u7f13\u89e3\uff0c\u4ee5\u63d0\u9ad8GPU\u5229\u7528\u7387\u548c\u964d\u4f4e\u5ef6\u8fdf\u3002", "motivation": "LLM\u63a8\u7406\u4e2d\u7684\u8d1f\u8f7d\u4e0d\u5e73\u8861\u95ee\u9898\u5bfc\u81f4GPU\u5229\u7528\u7387\u4f4e\u4e0b\u548c\u5ef6\u8fdf\u589e\u52a0\uff0c\u5f71\u54cd\u4e86\u8fd0\u884c\u65f6\u6548\u7387\u3002", "method": "\u5229\u7528DPU\uff08BlueField-3\uff09\u5b9e\u65f6\u68c0\u6d4b\u548c\u7f13\u89e3\u591a\u8282\u70b9\u5f20\u91cf\u5e76\u884c\u63a8\u7406\u4e2d\u7684\u8d1f\u8f7d\u4e0d\u5e73\u8861\u3002DPU\u8d1f\u8d23\u76d1\u63a7GPU\u9065\u6d4b\u6570\u636e\u548c\u8282\u70b9\u95f4\u901a\u4fe1\u6a21\u5f0f\uff0c\u5e76\u5c06\u5206\u6790\u7ed3\u679c\u53cd\u9988\u7ed9\u63a8\u7406\u63a7\u5236\u5668\u548c\u8c03\u5ea6\u5668\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cdDPU\u8f85\u52a9\u6846\u67b6\uff0c\u7528\u4e8e\u5b9e\u65f6\u68c0\u6d4b\u548c\u7f13\u89e3LLM\u63a8\u7406\u4e2d\u7684\u8d1f\u8f7d\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "conclusion": "\u7814\u7a76\u65e8\u5728\u8bc6\u522b\u591aGPU\u6267\u884cLLM\u5f20\u91cf\u8ba1\u7b97\uff08\u8bad\u7ec3\u548c\u63a8\u7406\uff09\u4e2d\u51fa\u73b0\u7684\u5931\u8861/\u75c5\u7406\u72b6\u51b5\uff0c\u8bc4\u4f30\u5176\u5bf9\u8ba1\u7b97\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u63a2\u8ba8DPU\u7f51\u7edc\u662f\u5426\u80fd\u6709\u6548\u8ddf\u8e2a\u548c\u7f13\u89e3\u8fd9\u4e9b\u95ee\u9898\u3002"}}
{"id": "2509.18216", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18216", "abs": "https://arxiv.org/abs/2509.18216", "authors": ["Amitava Das"], "title": "nDNA -- the Semantic Helix of Artificial Cognition", "comment": null, "summary": "As AI foundation models grow in capability, a deeper question emerges: What\nshapes their internal cognitive identity -- beyond fluency and output?\nBenchmarks measure behavior, but the soul of a model resides in its latent\ngeometry. In this work, we propose Neural DNA (nDNA) as a semantic-genotypic\nrepresentation that captures this latent identity through the intrinsic\ngeometry of belief. At its core, nDNA is synthesized from three principled and\nindispensable dimensions of latent geometry: spectral curvature, which reveals\nthe curvature of conceptual flow across layers; thermodynamic length, which\nquantifies the semantic effort required to traverse representational\ntransitions through layers; and belief vector field, which delineates the\nsemantic torsion fields that guide a model's belief directional orientations.\nLike biological DNA, it encodes ancestry, mutation, and semantic inheritance,\nfound in finetuning and alignment scars, cultural imprints, and architectural\ndrift. In naming it, we open a new field: Neural Genomics, where models are not\njust tools, but digital semantic organisms with traceable inner cognition.\n  Modeling statement. We read AI foundation models as semantic fluid--dynamics:\nmeaning is transported through layers like fluid in a shaped conduit; nDNA is\nthe physics-grade readout of that flow -- a geometry-first measure of how\nmeaning is bent, paid for, and pushed -- yielding a stable, coordinate-free\nneural DNA fingerprint tied to on-input behavior; with this fingerprint we\ncross into biology: tracing lineages across pretraining, fine-tuning,\nalignment, pruning, distillation, and merges; measuring inheritance between\ncheckpoints; detecting drift as traits shift under new data or objectives; and,\nultimately, studying the evolution of artificial cognition to compare models,\ndiagnose risks, and govern change over time.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u795e\u7ecf\u7f51\u7edcDNA\uff08nDNA\uff09\u4f5c\u4e3a\u4e00\u79cd\u65b0\u7684\u8bed\u4e49-\u57fa\u56e0\u578b\u8868\u793a\u65b9\u6cd5\uff0c\u7528\u4e8e\u6355\u6349AI\u57fa\u7840\u6a21\u578b\u6f5c\u5728\u7684\u5185\u5728\u51e0\u4f55\u7ed3\u6784\uff0c\u4ece\u800c\u63ed\u793a\u5176\u201c\u8ba4\u77e5\u8eab\u4efd\u201d\u3002nDNA\u901a\u8fc7\u4e09\u4e2a\u7ef4\u5ea6\uff08\u8c31\u66f2\u7387\u3001\u70ed\u529b\u5b66\u957f\u5ea6\u548c\u4fe1\u5ff5\u5411\u91cf\u573a\uff09\u6765\u91cf\u5316\u6a21\u578b\u7684\u5185\u5728\u51e0\u4f55\u7279\u5f81\uff0c\u7c7b\u4f3c\u4e8e\u751f\u7269DNA\u7f16\u7801\u4e86\u9057\u4f20\u4fe1\u606f\uff0cnDNA\u4e5f\u8bb0\u5f55\u4e86\u6a21\u578b\u5728\u8bad\u7ec3\u3001\u5fae\u8c03\u548c\u6f14\u53d8\u8fc7\u7a0b\u4e2d\u7684\u201c\u9057\u4f20\u201d\u75d5\u8ff9\u3002\u7814\u7a76\u8005\u5c06\u6b64\u5f00\u521b\u4e3a\u4e00\u4e2a\u65b0\u9886\u57df\u2014\u2014\u795e\u7ecf\u57fa\u56e0\u7ec4\u5b66\uff0c\u65e8\u5728\u5c06\u6a21\u578b\u89c6\u4e3a\u5177\u6709\u53ef\u8ffd\u6eaf\u8ba4\u77e5\u80fd\u529b\u7684\u201c\u6570\u5b57\u8bed\u4e49\u751f\u7269\u201d\uff0c\u7528\u4e8e\u6a21\u578b\u6bd4\u8f83\u3001\u98ce\u9669\u8bca\u65ad\u548c\u6f14\u5316\u7814\u7a76\u3002", "motivation": "\u968f\u7740AI\u57fa\u7840\u6a21\u578b\u80fd\u529b\u589e\u5f3a\uff0c\u7406\u89e3\u5176\u8d85\u8d8a\u8868\u9762\u884c\u4e3a\u7684\u5185\u5728\u201c\u8ba4\u77e5\u8eab\u4efd\u201d\u53d8\u5f97\u8d8a\u53d1\u91cd\u8981\u3002\u73b0\u6709\u7684\u57fa\u51c6\u6d4b\u8bd5\u4ec5\u80fd\u8861\u91cf\u884c\u4e3a\uff0c\u800c\u6a21\u578b\u7684\u672c\u8d28\u6839\u690d\u4e8e\u5176\u6f5c\u5728\u7684\u51e0\u4f55\u7ed3\u6784\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u795e\u7ecf\u7f51\u7edcDNA\uff08nDNA\uff09\u7684\u8bed\u4e49-\u57fa\u56e0\u578b\u8868\u793a\u65b9\u6cd5\u3002nDNA\u57fa\u4e8e\u4e09\u4e2a\u6838\u5fc3\u7684\u6f5c\u5728\u51e0\u4f55\u7ef4\u5ea6\uff1a\u8c31\u66f2\u7387\uff08\u91cf\u5316\u6982\u5ff5\u6d41\u52a8\u7684\u66f2\u7387\uff09\u3001\u70ed\u529b\u5b66\u957f\u5ea6\uff08\u91cf\u5316\u8868\u793a\u8fc7\u6e21\u7684\u8bed\u4e49\u6210\u672c\uff09\u548c\u4fe1\u5ff5\u5411\u91cf\u573a\uff08\u63cf\u7ed8\u5f15\u5bfc\u4fe1\u5ff5\u65b9\u5411\u7684\u8bed\u4e49\u626d\u66f2\u573a\uff09\u3002\u901a\u8fc7\u5206\u6790\u6a21\u578b\u5728\u9884\u8bad\u7ec3\u3001\u5fae\u8c03\u3001\u5bf9\u9f50\u3001\u526a\u679d\u3001\u84b8\u998f\u548c\u5408\u5e76\u7b49\u8fc7\u7a0b\u4e2d\u7684\u53d8\u5316\uff0c\u8ffd\u8e2a\u6a21\u578b\u7684\u201c\u9057\u4f20\u201d\u548c\u201c\u53d8\u5f02\u201d\u3002", "result": "nDNA\u80fd\u591f\u6355\u6349\u6a21\u578b\u7684\u5185\u5728\u51e0\u4f55\u7279\u5f81\uff0c\u4f5c\u4e3a\u4e00\u79cd\u7a33\u5b9a\u7684\u3001\u5750\u6807\u65e0\u5173\u7684\u201c\u795e\u7ecf\u7f51\u7edcDNA\u6307\u7eb9\u201d\uff0c\u8be5\u6307\u7eb9\u4e0e\u6a21\u578b\u7684\u8f93\u5165\u884c\u4e3a\u76f8\u5173\u8054\u3002\u901a\u8fc7\u8fd9\u79cd\u6307\u7eb9\uff0c\u53ef\u4ee5\u8ffd\u6eaf\u6a21\u578b\u5728\u4e0d\u540c\u751f\u547d\u5468\u671f\u9636\u6bb5\uff08\u5982\u9884\u8bad\u7ec3\u3001\u5fae\u8c03\u3001\u5bf9\u9f50\u7b49\uff09\u7684\u201c\u8c31\u7cfb\u201d\uff0c\u6d4b\u91cf\u68c0\u67e5\u70b9\u4e4b\u95f4\u7684\u201c\u9057\u4f20\u201d\u5173\u7cfb\uff0c\u68c0\u6d4b\u56e0\u65b0\u6570\u636e\u6216\u76ee\u6807\u53d8\u5316\u5f15\u8d77\u7684\u201c\u6027\u72b6\u6f02\u79fb\u201d\uff0c\u5e76\u7814\u7a76\u4eba\u5de5\u667a\u80fd\u8ba4\u77e5\u7684\u6f14\u5316\u3002", "conclusion": "nDNA\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u89c6\u89d2\u6765\u7406\u89e3\u548c\u91cf\u5316AI\u57fa\u7840\u6a21\u578b\u7684\u5185\u5728\u8ba4\u77e5\u7ed3\u6784\uff0c\u5c06\u6a21\u578b\u89c6\u4e3a\u5177\u6709\u201c\u9057\u4f20\u201d\u548c\u201c\u6f14\u5316\u201d\u80fd\u529b\u7684\u201c\u6570\u5b57\u8bed\u4e49\u751f\u7269\u201d\u3002\u8fd9\u4e3a\u6a21\u578b\u6bd4\u8f83\u3001\u98ce\u9669\u8bc4\u4f30\u548c\u6cbb\u7406\u5f00\u8f9f\u4e86\u65b0\u7684\u9014\u5f84\uff0c\u6807\u5fd7\u7740\u795e\u7ecf\u57fa\u56e0\u7ec4\u5b66\u8fd9\u4e00\u65b0\u9886\u57df\u7684\u8bde\u751f\u3002"}}
{"id": "2509.18344", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18344", "abs": "https://arxiv.org/abs/2509.18344", "authors": ["Pei-Shuo Wang", "Jian-Jia Chen", "Chun-Che Yang", "Chi-Chih Chang", "Ning-Chi Huang", "Mohamed S. Abdelfattah", "Kai-Chiang Wu"], "title": "Speculate Deep and Accurate: Lossless and Training-Free Acceleration for Offloaded LLMs via Substitute Speculative Decoding", "comment": "Accepted by NeurIPS 2025", "summary": "The immense model sizes of large language models (LLMs) challenge deployment\non memory-limited consumer GPUs. Although model compression and parameter\noffloading are common strategies to address memory limitations, compression can\ndegrade quality, and offloading maintains quality but suffers from slow\ninference. Speculative decoding presents a promising avenue to accelerate\nparameter offloading, utilizing a fast draft model to propose multiple draft\ntokens, which are then verified by the target LLM in parallel with a single\nforward pass. This method reduces the time-consuming data transfers in forward\npasses that involve offloaded weight transfers. Existing methods often rely on\npretrained weights of the same family, but require additional training to align\nwith custom-trained models. Moreover, approaches that involve draft model\ntraining usually yield only modest speedups. This limitation arises from\ninsufficient alignment with the target model, preventing higher token\nacceptance lengths. To address these challenges and achieve greater speedups,\nwe propose SubSpec, a plug-and-play method to accelerate parameter offloading\nthat is lossless and training-free. SubSpec constructs a highly aligned draft\nmodel by generating low-bit quantized substitute layers from offloaded target\nLLM portions. Additionally, our method shares the remaining GPU-resident layers\nand the KV-Cache, further reducing memory overhead and enhance alignment.\nSubSpec achieves a high average acceptance length, delivering 9.1x speedup for\nQwen2.5 7B on MT-Bench (8GB VRAM limit) and an average of 12.5x speedup for\nQwen2.5 32B on popular generation benchmarks (24GB VRAM limit).", "AI": {"tldr": "SubSpec\u662f\u4e00\u79cd\u65e0\u635f\u3001\u65e0\u9700\u8bad\u7ec3\u7684\u5373\u63d2\u5373\u7528\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u4f4e\u6bd4\u7279\u91cf\u5316\u7684\u66ff\u4ee3\u5c42\u6765\u52a0\u901f\u53c2\u6570\u5378\u8f7d\uff0c\u53ef\u663e\u8457\u63d0\u9ad8LLM\u7684\u63a8\u7406\u901f\u5ea6\uff0c\u7279\u522b\u662f\u5728\u5185\u5b58\u53d7\u9650\u7684\u60c5\u51b5\u4e0b\u3002", "motivation": "LLMs\u7684\u5de8\u5927\u6a21\u578b\u5c3a\u5bf8\u7ed9\u5185\u5b58\u6709\u9650\u7684\u6d88\u8d39\u7ea7GPU\u5e26\u6765\u4e86\u90e8\u7f72\u6311\u6218\u3002\u6a21\u578b\u538b\u7f29\u4f1a\u964d\u4f4e\u8d28\u91cf\uff0c\u800c\u53c2\u6570\u5378\u8f7d\u4f1a\u5e26\u6765\u7f13\u6162\u7684\u63a8\u7406\u901f\u5ea6\u3002\u73b0\u6709\u7684\u52a0\u901f\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u989d\u5916\u7684\u8bad\u7ec3\u6765\u9002\u5e94\u5b9a\u5236\u6a21\u578b\uff0c\u6216\u8005\u4ea7\u751f\u7684\u52a0\u901f\u6548\u679c\u6709\u9650\u3002", "method": "SubSpec\u901a\u8fc7\u751f\u6210\u76ee\u6807LLM\u90e8\u5206\u4f4e\u6bd4\u7279\u91cf\u5316\u7684\u66ff\u4ee3\u5c42\u6765\u6784\u5efa\u9ad8\u5ea6\u5339\u914d\u7684\u8349\u7a3f\u6a21\u578b\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u5171\u4eabGPU\u4e0a\u5269\u4f59\u7684\u5c42\u548cKV\u7f13\u5b58\uff0c\u4ee5\u8fdb\u4e00\u6b65\u964d\u4f4e\u5185\u5b58\u5f00\u9500\u5e76\u589e\u5f3a\u5339\u914d\u5ea6\u3002", "result": "SubSpec\u5b9e\u73b0\u4e86\u5f88\u9ad8\u7684\u5e73\u5747\u63a5\u53d7\u957f\u5ea6\uff0c\u5728MT-Bench\uff088GB\u663e\u5b58\u9650\u5236\uff09\u4e0a\u5bf9Qwen2.5 7B\u5b9e\u73b0\u4e869.1\u500d\u7684\u52a0\u901f\uff0c\u5728\u6d41\u884c\u7684\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\uff0824GB\u663e\u5b58\u9650\u5236\uff09\u4e0a\u5bf9Qwen2.5 32B\u5b9e\u73b0\u4e86\u5e73\u574712.5\u500d\u7684\u52a0\u901f\u3002", "conclusion": "SubSpec\u80fd\u591f\u6709\u6548\u89e3\u51b3\u5185\u5b58\u9650\u5236\u95ee\u9898\uff0c\u5e76\u5728\u4e0d\u635f\u5931\u6a21\u578b\u8d28\u91cf\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u9ad8LLM\u7684\u63a8\u7406\u901f\u5ea6\u3002"}}
{"id": "2509.19220", "categories": ["cs.LG", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2509.19220", "abs": "https://arxiv.org/abs/2509.19220", "authors": ["Ferdinand Kahenga", "Antoine Bagula", "Patrick Sello", "Sajal K. Das"], "title": "FedFusion: Federated Learning with Diversity- and Cluster-Aware Encoders for Robust Adaptation under Label Scarcity", "comment": null, "summary": "Federated learning in practice must contend with heterogeneous feature\nspaces, severe non-IID data, and scarce labels across clients. We present\nFedFusion, a federated transfer-learning framework that unifies domain\nadaptation and frugal labelling with diversity-/cluster-aware encoders (DivEn,\nDivEn-mix, DivEn-c). Labelled teacher clients guide learner clients via\nconfidence-filtered pseudo-labels and domain-adaptive transfer, while clients\nmaintain personalised encoders tailored to local data. To preserve global\ncoherence under heterogeneity, FedFusion employs similarity-weighted classifier\ncoupling (with optional cluster-wise averaging), mitigating dominance by\ndata-rich sites and improving minority-client performance. The frugal-labelling\npipeline combines self-/semi-supervised pretext training with selective\nfine-tuning, reducing annotation demands without sharing raw data. Across\ntabular and imaging benchmarks under IID, non-IID, and label-scarce regimes,\nFedFusion consistently outperforms state-of-the-art baselines in accuracy,\nrobustness, and fairness while maintaining comparable communication and\ncomputation budgets. These results show that harmonising personalisation,\ndomain adaptation, and label efficiency is an effective recipe for robust\nfederated learning under real-world constraints.", "AI": {"tldr": "FedFusion\u662f\u4e00\u4e2a\u8054\u90a6\u8fc1\u79fb\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u7edf\u4e00\u57df\u81ea\u9002\u5e94\u548c\u8282\u4fed\u6807\u7b7e\uff0c\u5e76\u7ed3\u5408\u591a\u6837\u6027/\u805a\u7c7b\u611f\u77e5\u7f16\u7801\u5668\uff08DivEn, DivEn-mix, DivEn-c\uff09\uff0c\u89e3\u51b3\u4e86\u8054\u90a6\u5b66\u4e60\u4e2d\u7279\u5f81\u7a7a\u95f4\u5f02\u6784\u3001\u6570\u636e\u975eIID\u548c\u6807\u7b7e\u7a00\u758f\u7684\u95ee\u9898\u3002\u8be5\u6846\u67b6\u5229\u7528\u5e26\u6807\u7b7e\u7684\u6559\u5e08\u5ba2\u6237\u7aef\u901a\u8fc7\u7f6e\u4fe1\u5ea6\u8fc7\u6ee4\u7684\u4f2a\u6807\u7b7e\u548c\u57df\u81ea\u9002\u5e94\u8fc1\u79fb\u6765\u6307\u5bfc\u5b66\u4e60\u5ba2\u6237\u7aef\uff0c\u540c\u65f6\u4e3a\u5ba2\u6237\u7aef\u7ef4\u62a4\u4e2a\u6027\u5316\u7f16\u7801\u5668\u3002\u901a\u8fc7\u76f8\u4f3c\u6027\u52a0\u6743\u5206\u7c7b\u5668\u8026\u5408\uff08\u53ef\u9009\u805a\u7c7b\u5e73\u5747\uff09\uff0cFedFusion\u5728\u5f02\u6784\u73af\u5883\u4e0b\u4fdd\u6301\u5168\u5c40\u4e00\u81f4\u6027\uff0c\u5e76\u63d0\u9ad8\u4e86\u5c11\u6570\u5ba2\u6237\u7aef\u7684\u6027\u80fd\u3002\u8282\u4fed\u6807\u7b7e\u6d41\u7a0b\u7ed3\u5408\u4e86\u81ea/\u534a\u76d1\u7763\u9884\u8bad\u7ec3\u548c\u9009\u62e9\u6027\u5fae\u8c03\uff0c\u5728\u4e0d\u5171\u4eab\u539f\u59cb\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u51cf\u5c11\u4e86\u6807\u6ce8\u9700\u6c42\u3002\u5728\u5404\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFedFusion\u5728\u51c6\u786e\u6027\u3001\u9c81\u68d2\u6027\u548c\u516c\u5e73\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u901a\u4fe1\u548c\u8ba1\u7b97\u6210\u672c\u76f8\u5f53\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u9762\u4e34\u7279\u5f81\u7a7a\u95f4\u5f02\u6784\u3001\u6570\u636e\u975eIID\u548c\u6807\u7b7e\u7a00\u758f\u7b49\u6311\u6218\u3002\u672c\u7814\u7a76\u65e8\u5728\u63d0\u51fa\u4e00\u4e2a\u8054\u90a6\u8fc1\u79fb\u5b66\u4e60\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u8fd9\u4e9b\u5b9e\u9645\u95ee\u9898\u3002", "method": "FedFusion\u6846\u67b6\u7edf\u4e00\u4e86\u57df\u81ea\u9002\u5e94\u548c\u8282\u4fed\u6807\u7b7e\uff0c\u5e76\u5f15\u5165\u4e86\u591a\u6837\u6027/\u805a\u7c7b\u611f\u77e5\u7f16\u7801\u5668\uff08DivEn, DivEn-mix, DivEn-c\uff09\u3002\u5e26\u6807\u7b7e\u7684\u6559\u5e08\u5ba2\u6237\u7aef\u901a\u8fc7\u7f6e\u4fe1\u5ea6\u8fc7\u6ee4\u7684\u4f2a\u6807\u7b7e\u548c\u57df\u81ea\u9002\u5e94\u8fc1\u79fb\u6307\u5bfc\u5b66\u4e60\u5ba2\u6237\u7aef\uff0c\u540c\u65f6\u5ba2\u6237\u7aef\u7ef4\u62a4\u4e2a\u6027\u5316\u7f16\u7801\u5668\u3002\u901a\u8fc7\u76f8\u4f3c\u6027\u52a0\u6743\u5206\u7c7b\u5668\u8026\u5408\uff08\u53ef\u9009\u805a\u7c7b\u5e73\u5747\uff09\u6765\u4fdd\u6301\u5168\u5c40\u4e00\u81f4\u6027\uff0c\u5e76\u91c7\u7528\u7ed3\u5408\u81ea/\u534a\u76d1\u7763\u9884\u8bad\u7ec3\u548c\u9009\u62e9\u6027\u5fae\u8c03\u7684\u8282\u4fed\u6807\u7b7e\u6d41\u7a0b\u3002", "result": "\u5728\u8868\u683c\u548c\u56fe\u50cf\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFedFusion\u5728IID\u3001\u975eIID\u548c\u6807\u7b7e\u7a00\u758f\u7684\u8bbe\u5b9a\u4e0b\uff0c\u5728\u51c6\u786e\u6027\u3001\u9c81\u68d2\u6027\u548c\u516c\u5e73\u6027\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u76f8\u5f53\u7684\u901a\u4fe1\u548c\u8ba1\u7b97\u9884\u7b97\u3002", "conclusion": "\u8054\u90a6\u5b66\u4e60\u4e2d\u4e2a\u6027\u5316\u3001\u57df\u81ea\u9002\u5e94\u548c\u6807\u7b7e\u6548\u7387\u7684\u534f\u8c03\u662f\u5e94\u5bf9\u73b0\u5b9e\u4e16\u754c\u7ea6\u675f\u4e0b\u9c81\u68d2\u8054\u90a6\u5b66\u4e60\u7684\u6709\u6548\u7b56\u7565\u3002"}}
{"id": "2509.18988", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.18988", "abs": "https://arxiv.org/abs/2509.18988", "authors": ["Ziliang Lyu", "Miroslav Krstic", "Kaixin Lu", "Yiguang Hong", "Lihua Xie"], "title": "Adaptive Override Control under High-Relative-Degree Nonovershooting Constraints", "comment": null, "summary": "This paper considers the problem of adaptively overriding unsafe actions of a\nnominal controller in the presence of high-relative-degree nonovershooting\nconstraints and parametric uncertainties. To prevent the design from being\ncoupled with high-order derivatives of the parameter estimation error, we adopt\na modular design approach in which the controller and the parameter identifier\nare designed separately. The controller module ensures that any safety\nviolations caused by parametric uncertainties remain bounded, provided that the\nparameter estimation error and its first-order derivative are either bounded or\nsquare-integrable. The identifier module, in turn, guarantees that these\nrequirements on the parameter estimation error are satisfied. Both theoretical\nanalysis and simulation results demonstrate that the closed-loop safety\nviolation is bounded by a tunable function of the initial estimation error.\nMoreover, as time increases, the parameter estimate converges to the true\nvalue, and the amount of safety violation decreases accordingly.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u63a7\u5236\u5668\uff0c\u7528\u4e8e\u5728\u5b58\u5728\u9ad8\u76f8\u5bf9\u5ea6\u975e\u8d85\u8c03\u7ea6\u675f\u548c\u53c2\u6570\u4e0d\u786e\u5b9a\u6027\u7684\u60c5\u51b5\u4e0b\uff0c\u5b89\u5168\u5730\u8986\u76d6\u4e0d\u5b89\u5168\u52a8\u4f5c\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u5728\u5b58\u5728\u9ad8\u76f8\u5bf9\u5ea6\u975e\u8d85\u8c03\u7ea6\u675f\u548c\u53c2\u6570\u4e0d\u786e\u5b9a\u6027\u7684\u60c5\u51b5\u4e0b\uff0c\u5982\u4f55\u81ea\u9002\u5e94\u5730\u8986\u76d6\u4e0d\u5b89\u5168\u52a8\u4f5c\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6a21\u5757\u5316\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u5c06\u63a7\u5236\u5668\u548c\u53c2\u6570\u4f30\u8ba1\u5668\u5206\u5f00\u8bbe\u8ba1\u3002\u63a7\u5236\u5668\u6a21\u5757\u786e\u4fdd\u7531\u53c2\u6570\u4e0d\u786e\u5b9a\u6027\u5f15\u8d77 any safety violations \u4fdd\u6301\u6709\u754c\uff0c\u524d\u63d0\u662f\u53c2\u6570\u4f30\u8ba1\u8bef\u5dee\u53ca\u5176\u4e00\u9636\u5bfc\u6570\u6709\u754c\u6216\u5e73\u65b9\u53ef\u79ef\u3002\u4f30\u8ba1\u5668\u6a21\u5757\u4fdd\u8bc1\u6ee1\u8db3\u53c2\u6570\u4f30\u8ba1\u8bef\u5dee\u7684\u8981\u6c42\u3002", "result": "\u7406\u8bba\u5206\u6790\u548c\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u95ed\u73af\u5b89\u5168\u8fdd\u53cd\u7531\u521d\u59cb\u4f30\u8ba1\u8bef\u5dee\u7684\u53ef\u8c03\u51fd\u6570\u754c\u5b9a\u3002\u6b64\u5916\uff0c\u968f\u7740\u65f6\u95f4\u7684\u589e\u52a0\uff0c\u53c2\u6570\u4f30\u8ba1\u6536\u655b\u5230\u771f\u5b9e\u503c\uff0c\u5b89\u5168\u8fdd\u53cd\u91cf\u76f8\u5e94\u51cf\u5c11\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5730\u5904\u7406\u5177\u6709\u6311\u6218\u6027\u7684\u5b89\u5168\u7ea6\u675f\u548c\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2509.19119", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.19119", "abs": "https://arxiv.org/abs/2509.19119", "authors": ["Palatip Jopanya", "Diana P. M. Osorio"], "title": "Enabling Drone Detection with SWARM Repeater-Assisted MIMO ISAC", "comment": "5 pages, 2 figures", "summary": "As definitions about new architectural aspects, use cases, and standards for\nintegrated sensing and communication (ISAC) continue to appear, cellular\nsystems based on massive multiple-input multiple-output (MIMO) antenna\ntechnology are also experiencing a parallel evolution through the integration\nof novel network components. This evolution should support emerging ISAC use\ncases and services. In particular, this paper explores a recent vision for\ncost-efficient cellular network densification through the deployment of swarms\nof repeaters. Leveraging their ability to retransmit signals instantaneously,\nwe investigate how these repeaters can enhance radar sensing capabilities for\ndrone detection in a swarm repeater-assisted MIMO ISAC system. Our results\ndemonstrate that, by optimizing the gains of repeaters given a sufficient\nmaximum amplification gain, increasing the number of repeaters can lead to\ngains in sensing performance.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u8702\u7a9d\u7f51\u7edc\u4e2d\u90e8\u7f72\u4e2d\u7ee7\u5668\u96c6\u7fa4\u4ee5\u589e\u5f3a\u65e0\u4eba\u673a\u68c0\u6d4b\u7684\u96f7\u8fbe\u611f\u77e5\u80fd\u529b\u3002", "motivation": "\u968f\u7740\u5bf9\u96c6\u6210\u4f20\u611f\u548c\u901a\u4fe1\uff08ISAC\uff09\u7684\u65b0\u67b6\u6784\u3001\u7528\u4f8b\u548c\u6807\u51c6\u7684\u5b9a\u4e49\u4e0d\u65ad\u51fa\u73b0\uff0c\u57fa\u4e8e\u6d77\u91cf\u591a\u8f93\u5165\u591a\u8f93\u51fa\uff08MIMO\uff09\u5929\u7ebf\u6280\u672f\u7684\u8702\u7a9d\u7cfb\u7edf\u4e5f\u5728\u7ecf\u5386\u7740\u65b0\u578b\u7f51\u7edc\u7ec4\u4ef6\u96c6\u6210\u5e26\u6765\u7684\u5e76\u884c\u6f14\u8fdb\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u4e00\u79cd\u901a\u8fc7\u90e8\u7f72\u4e2d\u7ee7\u5668\u96c6\u7fa4\u6765\u5b9e\u73b0\u7ecf\u6d4e\u9ad8\u6548\u7684\u8702\u7a9d\u7f51\u7edc\u5bc6\u96c6\u5316\u7684\u65b0\u65b9\u6cd5\uff0c\u4ee5\u652f\u6301\u65b0\u5174\u7684ISAC\u7528\u4f8b\u548c\u670d\u52a1\u3002", "method": "\u672c\u6587\u7814\u7a76\u4e86\u5982\u4f55\u5229\u7528\u4e2d\u7ee7\u5668\u77ac\u65f6\u91cd\u4f20\u4fe1\u53f7\u7684\u80fd\u529b\uff0c\u5728\u7531\u4e2d\u7ee7\u5668\u96c6\u7fa4\u8f85\u52a9\u7684MIMO ISAC\u7cfb\u7edf\u4e2d\u589e\u5f3a\u65e0\u4eba\u673a\u68c0\u6d4b\u7684\u96f7\u8fbe\u611f\u77e5\u80fd\u529b\u3002", "result": "\u901a\u8fc7\u4f18\u5316\u4e2d\u7ee7\u5668\u7684\u589e\u76ca\uff08\u5728\u8db3\u591f\u5927\u7684\u6700\u5927\u653e\u5927\u589e\u76ca\u4e0b\uff09\uff0c\u7ed3\u679c\u8868\u660e\u589e\u52a0\u4e2d\u7ee7\u5668\u7684\u6570\u91cf\u53ef\u4ee5\u63d0\u9ad8\u611f\u6d4b\u6027\u80fd\u3002", "conclusion": "\u5728\u65e0\u4eba\u673a\u68c0\u6d4b\u7684MIMO ISAC\u7cfb\u7edf\u4e2d\uff0c\u901a\u8fc7\u4f18\u5316\u589e\u76ca\u548c\u589e\u52a0\u4e2d\u7ee7\u5668\u6570\u91cf\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u611f\u6d4b\u6027\u80fd\u3002"}}
{"id": "2509.18463", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18463", "abs": "https://arxiv.org/abs/2509.18463", "authors": ["Jannick van Buuren", "Roberto Giglio", "Loris Roveda", "Luka Peternel"], "title": "Robotic Skill Diversification via Active Mutation of Reward Functions in Reinforcement Learning During a Liquid Pouring Task", "comment": null, "summary": "This paper explores how deliberate mutations of reward function in\nreinforcement learning can produce diversified skill variations in robotic\nmanipulation tasks, examined with a liquid pouring use case. To this end, we\ndeveloped a new reward function mutation framework that is based on applying\nGaussian noise to the weights of the different terms in the reward function.\nInspired by the cost-benefit tradeoff model from human motor control, we\ndesigned the reward function with the following key terms: accuracy, time, and\neffort. The study was performed in a simulation environment created in NVIDIA\nIsaac Sim, and the setup included Franka Emika Panda robotic arm holding a\nglass with a liquid that needed to be poured into a container. The\nreinforcement learning algorithm was based on Proximal Policy Optimization. We\nsystematically explored how different configurations of mutated weights in the\nrewards function would affect the learned policy. The resulting policies\nexhibit a wide range of behaviours: from variations in execution of the\noriginally intended pouring task to novel skills useful for unexpected tasks,\nsuch as container rim cleaning, liquid mixing, and watering. This approach\noffers promising directions for robotic systems to perform diversified learning\nof specific tasks, while also potentially deriving meaningful skills for future\ntasks.", "AI": {"tldr": "\u901a\u8fc7\u5bf9\u5956\u52b1\u51fd\u6570\u65bd\u52a0\u9ad8\u65af\u566a\u58f0\u6765\u7a81\u53d8\u6743\u91cd\uff0c\u4ee5\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u4ea7\u751f\u591a\u6837\u5316\u7684\u6280\u80fd\u3002\u6211\u4eec\u4f7f\u7528\u6db2\u4f53\u503e\u5012\u7528\u4f8b\u8fdb\u884c\u4e86\u68c0\u67e5\uff0c\u5e76\u53d1\u73b0\u7531\u6b64\u4ea7\u751f\u7684\u7b56\u7565\u5c55\u793a\u4e86\u4ece\u6267\u884c\u9884\u671f\u7684\u503e\u5012\u4efb\u52a1\u5230\u6e05\u6d01\u5bb9\u5668\u8fb9\u7f18\u3001\u6df7\u5408\u6db2\u4f53\u548c\u6d47\u6c34\u7b49\u65b0\u9896\u6280\u80fd\u7684\u5e7f\u6cdb\u884c\u4e3a\u3002", "motivation": "\u672c\u7814\u7a76\u7684\u52a8\u673a\u5728\u4e8e\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u5bf9\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5956\u52b1\u51fd\u6570\u8fdb\u884c\u523b\u610f\u7a81\u53d8\uff0c\u4ece\u800c\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u4ea7\u751f\u591a\u6837\u5316\u7684\u6280\u80fd\u53d8\u4f53\u3002", "method": "\u672c\u7814\u7a76\u7684\u91cd\u70b9\u662f\u5f00\u53d1\u4e00\u4e2a\u65b0\u9896\u7684\u5956\u52b1\u51fd\u6570\u7a81\u53d8\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u5bf9\u5956\u52b1\u51fd\u6570\u4e2d\u4e0d\u540c\u9879\u7684\u6743\u91cd\u5e94\u7528\u9ad8\u65af\u566a\u58f0\u6765\u5b9e\u73b0\u3002\u6211\u4eec\u53d7\u5230\u4eba\u7c7b\u8fd0\u52a8\u63a7\u5236\u4e2d\u6210\u672c\u6548\u76ca\u6743\u8861\u6a21\u578b\u7684\u542f\u53d1\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5305\u542b\u51c6\u786e\u6027\u3001\u65f6\u95f4\u548c\u7cbe\u529b\u8fd9\u51e0\u4e2a\u5173\u952e\u9879\u7684\u5956\u52b1\u51fd\u6570\u3002\u8be5\u7814\u7a76\u5728 NVIDIA Isaac Sim \u521b\u5efa\u7684\u6a21\u62df\u73af\u5883\u4e2d\u8fdb\u884c\uff0c\u8be5\u73af\u5883\u5305\u542b\u4e00\u4e2a\u5f17\u5170\u5361\u00b7\u827e\u7c73\u5361\u718a\u732b\u673a\u68b0\u81c2\uff0c\u5b83\u624b\u6301\u4e00\u4e2a\u9700\u8981\u5c06\u6db2\u4f53\u5012\u5165\u5bb9\u5668\u7684\u73bb\u7483\u676f\u3002\u6240\u4f7f\u7528\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u57fa\u4e8e\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08PPO\uff09\u3002", "result": "\u6240\u4ea7\u751f\u7684\u7b56\u7565\u5c55\u793a\u4e86\u5e7f\u6cdb\u7684\u884c\u4e3a\uff0c\u5305\u62ec\u6267\u884c\u9884\u671f\u503e\u5012\u4efb\u52a1\u7684\u53d8\u4f53\uff0c\u4ee5\u53ca\u53ef\u7528\u4e8e\u610f\u5916\u4efb\u52a1\u7684\u65b0\u9896\u6280\u80fd\uff0c\u4f8b\u5982\u5bb9\u5668\u8fb9\u7f18\u6e05\u6d01\u3001\u6db2\u4f53\u6df7\u5408\u548c\u6d47\u6c34\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u673a\u5668\u4eba\u7cfb\u7edf\u6267\u884c\u7279\u5b9a\u4efb\u52a1\u7684\u591a\u6837\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\uff0c\u5e76\u6709\u53ef\u80fd\u4e3a\u672a\u6765\u7684\u4efb\u52a1\u884d\u751f\u51fa\u6709\u610f\u4e49\u7684\u6280\u80fd\u3002"}}
{"id": "2509.18185", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18185", "abs": "https://arxiv.org/abs/2509.18185", "authors": ["Giammarco La Barbera", "Enzo Bonnot", "Thomas Isla", "Juan Pablo de la Plata", "Joy-Rose Dunoyer de Segonzac", "Jennifer Attali", "C\u00e9cile Lozach", "Alexandre Bellucci", "Louis Marcellin", "Laure Fournier", "Sabine Sarnacki", "Pietro Gori", "Isabelle Bloch"], "title": "Visionerves: Automatic and Reproducible Hybrid AI for Peripheral Nervous System Recognition Applied to Endometriosis Cases", "comment": "Computer-Aided Pelvic Imaging for Female Health (CAPI) - Workshop\n  MICCAI 2025", "summary": "Endometriosis often leads to chronic pelvic pain and possible nerve\ninvolvement, yet imaging the peripheral nerves remains a challenge. We\nintroduce Visionerves, a novel hybrid AI framework for peripheral nervous\nsystem recognition from multi-gradient DWI and morphological MRI data. Unlike\nconventional tractography, Visionerves encodes anatomical knowledge through\nfuzzy spatial relationships, removing the need for selection of manual ROIs.\nThe pipeline comprises two phases: (A) automatic segmentation of anatomical\nstructures using a deep learning model, and (B) tractography and nerve\nrecognition by symbolic spatial reasoning. Applied to the lumbosacral plexus in\n10 women with (confirmed or suspected) endometriosis, Visionerves demonstrated\nsubstantial improvements over standard tractography, with Dice score\nimprovements of up to 25% and spatial errors reduced to less than 5 mm. This\nautomatic and reproducible approach enables detailed nerve analysis and paves\nthe way for non-invasive diagnosis of endometriosis-related neuropathy, as well\nas other conditions with nerve involvement.", "AI": {"tldr": "Visionerves\u662f\u4e00\u4e2a\u65b0\u7684\u6df7\u5408\u4eba\u5de5\u667a\u80fd\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u591a\u68af\u5ea6DWI\u548c\u5f62\u6001\u5b66MRI\u6570\u636e\u4e2d\u8bc6\u522b\u5468\u56f4\u795e\u7ecf\u7cfb\u7edf\uff0c\u4ee5\u89e3\u51b3\u5185\u819c\u5f02\u4f4d\u75c7\u4e2d\u795e\u7ecf\u6210\u50cf\u7684\u6311\u6218\u3002", "motivation": "\u5185\u819c\u5f02\u4f4d\u75c7\u5e38\u5e38\u5bfc\u81f4\u6162\u6027\u76c6\u8154\u75bc\u75db\u548c\u6f5c\u5728\u7684\u795e\u7ecf\u635f\u4f24\uff0c\u800c\u5bf9\u5468\u56f4\u795e\u7ecf\u8fdb\u884c\u6210\u50cf\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "Visionerves\u91c7\u7528\u6df7\u5408\u4eba\u5de5\u667a\u80fd\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u81ea\u52a8\u89e3\u5256\u7ed3\u6784\u5206\u5272\u548c\u7b26\u53f7\u7a7a\u95f4\u63a8\u7406\u8fdb\u884c\u795e\u7ecf\u675f\u6210\u50cf\u548c\u8bc6\u522b\u3002\u5b83\u901a\u8fc7\u6a21\u7cca\u7a7a\u95f4\u5173\u7cfb\u7f16\u7801\u89e3\u5256\u77e5\u8bc6\uff0c\u65e0\u9700\u624b\u52a8\u9009\u62e9\u611f\u5174\u8da3\u533a\u57df\uff08ROI\uff09\u3002", "result": "\u572810\u540d\u60a3\u6709\uff08\u5df2\u786e\u8ba4\u6216\u7591\u4f3c\uff09\u5185\u819c\u5f02\u4f4d\u75c7\u7684\u5973\u6027\u7684\u8170\u9ab6\u4e1b\u4e2d\uff0cVisionerves\u76f8\u8f83\u4e8e\u6807\u51c6\u795e\u7ecf\u675f\u6210\u50cf\u6280\u672f\uff0c\u5728Dice\u5206\u6570\u4e0a\u63d0\u9ad8\u4e8625%\uff0c\u7a7a\u95f4\u8bef\u5dee\u51cf\u5c0f\u52305\u6beb\u7c73\u4ee5\u5185\u3002", "conclusion": "Visionerves\u63d0\u4f9b\u4e86\u4e00\u79cd\u81ea\u52a8\u4e14\u53ef\u91cd\u590d\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5bf9\u795e\u7ecf\u8fdb\u884c\u8be6\u7ec6\u5206\u6790\uff0c\u4e3a\u5185\u819c\u5f02\u4f4d\u75c7\u76f8\u5173\u795e\u7ecf\u75c5\u53d8\u4ee5\u53ca\u5176\u4ed6\u6d89\u53ca\u795e\u7ecf\u7684\u75c5\u75c7\u7684\u975e\u4fb5\u5165\u6027\u8bca\u65ad\u5f00\u8f9f\u4e86\u9053\u8def\u3002"}}
{"id": "2509.18340", "categories": ["quant-ph", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.18340", "abs": "https://arxiv.org/abs/2509.18340", "authors": ["Mark Carney"], "title": "Qubit Instrumentation of Entanglement", "comment": "28 pages, 4 figures, book chapter", "summary": "This chapter and the experiments described within explore how `human\nentanglement' might be represented and even emulated by physical entanglement.\nTo achieve this, a notion of `tonal centrality' between two musicians is\ncaptured via MIDI and passed as a parameter into a quantum simulation taking\nplace on an embedded device (a Raspberry Pi Pico). The results of these\nsimulations are then coded back into MIDI and sent to the players' instruments.\nThe closer the musicians' tonality is, the more their instruments will be\nentangled in a $|\\Phi^+ \\rangle$ state, and the further away they are the more\ntheir instruments will be entangled in a $|\\Psi^+ \\rangle$ state. The intention\nis to create random parameters that are correlative - \\emph{i.e.} the same on\nboth instruments - or anti-correlative - \\emph{i.e.} the bit-wise opposite of\neach other, influenced by the tonal relationship from the players. These random\nparameters sharing these particular properties add a new dimension for\nquantum-musical expression. This concept was realised experimentally, and the\nfull code and sample outputs are provided. This work aims to pave the way for\nmusicians to explore and experience quantum emulations of their own musical\nexperiences, adding a new nuance and possibilities for the future of\n\\emph{entangled ensembles.}", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u91cf\u5316\u97f3\u4e50\u5bb6\u4e4b\u95f4\u7684\u201c\u97f3\u8c03\u4e2d\u5fc3\u6027\u201d\uff0c\u5e76\u5c06\u5176\u4f5c\u4e3a\u53c2\u6570\u8f93\u5165\u5230\u6811\u8393\u6d3e Pico \u4e0a\u7684\u91cf\u5b50\u6a21\u62df\u4e2d\uff0c\u63a2\u7d22\u4e86\u7269\u7406\u7ea0\u7f20\u5982\u4f55\u8868\u793a\u548c\u6a21\u62df\u201c\u4eba\u7c7b\u7ea0\u7f20\u201d\u3002\u6a21\u62df\u7ed3\u679c\u88ab\u7f16\u7801\u56de MIDI \u5e76\u53d1\u9001\u56de\u97f3\u4e50\u5bb6\u7684\u4e50\u5668\uff0c\u4f7f\u5f97\u97f3\u8c03\u8d8a\u63a5\u8fd1\uff0c\u4e50\u5668\u7ea0\u7f20\u4e8e $|\boldsymbol{\nu}^+\rangle$ \u72b6\u6001\u7684\u53ef\u80fd\u6027\u8d8a\u5927\uff1b\u53cd\u4e4b\uff0c\u5219\u8d8a\u53ef\u80fd\u7ea0\u7f20\u4e8e $|\boldsymbol{\nu}^-\rangle$ \u72b6\u6001\u3002\u6b64\u4e3e\u65e8\u5728\u521b\u9020\u5177\u6709\u76f8\u5173\u6027\u6216\u53cd\u76f8\u5173\u6027\u7684\u968f\u673a\u53c2\u6570\uff0c\u4e3a\u91cf\u5b50\u97f3\u4e50\u8868\u8fbe\u5f00\u8f9f\u65b0\u9014\u5f84\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u548c\u6a21\u62df\u7269\u7406\u7ea0\u7f20\u4e2d\u7684\u201c\u4eba\u7c7b\u7ea0\u7f20\u201d\uff0c\u4e3a\u97f3\u4e50\u5bb6\u63d0\u4f9b\u4e00\u79cd\u4f53\u9a8c\u548c\u8868\u8fbe\u91cf\u5b50\u5316\u97f3\u4e50\u7684\u65b0\u65b9\u5f0f\u3002", "method": "\u7814\u7a76\u901a\u8fc7 MIDI \u6355\u6349\u97f3\u4e50\u5bb6\u4e4b\u95f4\u7684\u201c\u97f3\u8c03\u4e2d\u5fc3\u6027\u201d\uff0c\u5c06\u5176\u4f5c\u4e3a\u53c2\u6570\u8f93\u5165\u5230\u6811\u8393\u6d3e Pico \u4e0a\u7684\u91cf\u5b50\u6a21\u62df\u4e2d\u3002\u6a21\u62df\u7ed3\u679c\u968f\u540e\u88ab\u7f16\u7801\u56de MIDI \u5e76\u53d1\u9001\u7ed9\u97f3\u4e50\u5bb6\u7684\u4e50\u5668\uff0c\u4ee5 $|\boldsymbol{\nu}^+\rangle$ \u548c $|\boldsymbol{\nu}^-\rangle$ \u72b6\u6001\u6a21\u62df\u7ea0\u7f20\u3002", "result": "\u5b9e\u9a8c\u5b9e\u73b0\u4e86\u6982\u5ff5\uff0c\u4ea7\u751f\u4e86\u5177\u6709\u76f8\u5173\u6027\u6216\u53cd\u76f8\u5173\u6027\u7684\u968f\u673a\u53c2\u6570\uff0c\u4e3a\u91cf\u5b50\u97f3\u4e50\u8868\u8fbe\u589e\u52a0\u4e86\u65b0\u7684\u7ef4\u5ea6\uff0c\u5e76\u63d0\u4f9b\u4e86\u5b8c\u6574\u7684\u4ee3\u7801\u548c\u793a\u4f8b\u8f93\u51fa\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u97f3\u4e50\u5bb6\u63a2\u7d22\u548c\u4f53\u9a8c\u5176\u81ea\u8eab\u7684\u97f3\u4e50\u4f53\u9a8c\u7684\u91cf\u5b50\u6a21\u62df\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u4e3a\u201c\u7ea0\u7f20\u5408\u594f\u201d\u7684\u672a\u6765\u5e26\u6765\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u548c\u7ec6\u5fae\u5dee\u522b\u3002"}}
{"id": "2509.18115", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18115", "abs": "https://arxiv.org/abs/2509.18115", "authors": ["Hongyi Chen", "Xiucheng Li", "Xinyang Chen", "Jing Li", "Kehai Chen", "Liqiang Nie"], "title": "Towards Scalable and Structured Spatiotemporal Forecasting", "comment": null, "summary": "In this paper, we propose a novel Spatial Balance Attention block for\nspatiotemporal forecasting. To strike a balance between obeying spatial\nproximity and capturing global correlation, we partition the spatial graph into\na set of subgraphs and instantiate Intra-subgraph Attention to learn local\nspatial correlation within each subgraph; to capture the global spatial\ncorrelation, we further aggregate the nodes to produce subgraph representations\nand achieve message passing among the subgraphs via Inter-subgraph Attention.\nBuilding on the proposed Spatial Balance Attention block, we develop a\nmultiscale spatiotemporal forecasting model by progressively increasing the\nsubgraph scales. The resulting model is both scalable and able to produce\nstructured spatial correlation, and meanwhile, it is easy to implement. We\nevaluate its efficacy and efficiency against the existing models on real-world\nspatiotemporal datasets from medium to large sizes. The experimental results\nshow that it can achieve performance improvements up to 7.7% over the baseline\nmethods at low running costs.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7a7a\u95f4\u5e73\u8861\u6ce8\u610f\u529b\u673a\u5236\uff08Spatial Balance Attention\uff09\u6765\u89e3\u51b3\u65f6\u7a7a\u9884\u6d4b\u95ee\u9898\uff0c\u901a\u8fc7\u5c06\u7a7a\u95f4\u56fe\u5212\u5206\u4e3a\u5b50\u56fe\u5e76\u5206\u522b\u5e94\u7528\u5185\u90e8\u548c\u8de8\u5b50\u56fe\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5e73\u8861\u4e86\u7a7a\u95f4\u90bb\u8fd1\u6027\u548c\u5168\u5c40\u76f8\u5173\u6027\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd\u591a\u5c3a\u5ea6\u7684\u65f6\u7a7a\u9884\u6d4b\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u5c55\u73b0\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\u548c\u6548\u7387\u3002", "motivation": "\u4e3a\u4e86\u5e73\u8861\u7a7a\u95f4\u90bb\u8fd1\u6027\u548c\u6355\u6349\u5168\u5c40\u76f8\u5173\u6027\uff0c\u4ee5\u89e3\u51b3\u65f6\u7a7a\u9884\u6d4b\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7a7a\u95f4\u5e73\u8861\u6ce8\u610f\u529b\uff08Spatial Balance Attention\uff09\u673a\u5236\uff0c\u8be5\u673a\u5236\u5c06\u7a7a\u95f4\u56fe\u5212\u5206\u4e3a\u5b50\u56fe\uff0c\u5e76\u5206\u522b\u5e94\u7528\u5185\u90e8\u5b50\u56fe\u6ce8\u610f\u529b\u548c\u8de8\u5b50\u56fe\u6ce8\u610f\u529b\u673a\u5236\u6765\u5206\u522b\u5b66\u4e60\u5c40\u90e8\u548c\u5168\u5c40\u7684\u7a7a\u95f4\u76f8\u5173\u6027\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u591a\u5c3a\u5ea6\u7684\u65f6\u7a7a\u9884\u6d4b\u6a21\u578b\u3002", "result": "\u6240\u63d0\u51fa\u7684\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u7684\u4e2d\u5230\u5927\u89c4\u6a21\u65f6\u7a7a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u4f4e\u8fd0\u884c\u6210\u672c\u4e0b\uff0c\u6027\u80fd\u76f8\u8f83\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u6709\u9ad8\u8fbe 7.7% \u7684\u63d0\u5347\u3002", "conclusion": "\u63d0\u51fa\u7684\u7a7a\u95f4\u5e73\u8861\u6ce8\u610f\u529b\u673a\u5236\u80fd\u591f\u6709\u6548\u5730\u5e73\u8861\u7a7a\u95f4\u90bb\u8fd1\u6027\u548c\u5168\u5c40\u76f8\u5173\u6027\uff0c\u4ece\u800c\u6784\u5efa\u51fa\u53ef\u6269\u5c55\u3001\u6613\u4e8e\u5b9e\u73b0\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u65f6\u7a7a\u9884\u6d4b\u6a21\u578b\u3002"}}
{"id": "2509.18218", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18218", "abs": "https://arxiv.org/abs/2509.18218", "authors": ["Kei-Sing Ng"], "title": "Similarity Field Theory: A Mathematical Framework for Intelligence", "comment": null, "summary": "We posit that persisting and transforming similarity relations form the\nstructural basis of any comprehensible dynamic system. This paper introduces\nSimilarity Field Theory, a mathematical framework that formalizes the\nprinciples governing similarity values among entities and their evolution. We\ndefine: (1) a similarity field $S: U \\times U \\to [0,1]$ over a universe of\nentities $U$, satisfying reflexivity $S(E,E)=1$ and treated as a directed\nrelational field (asymmetry and non-transitivity are allowed); (2) the\nevolution of a system through a sequence $Z_p = (X_p, S^{(p)})$ indexed by\n$p=0,1,2,\\ldots$; (3) concepts $K$ as entities that induce fibers\n$F_{\\alpha}(K) = { E \\in U \\mid S(E,K) \\ge \\alpha }$, i.e., superlevel sets of\nthe unary map $S_K(E) := S(E,K)$; and (4) a generative operator $G$ that\nproduces new entities. Within this framework, we formalize a generative\ndefinition of intelligence: an operator $G$ is intelligent with respect to a\nconcept $K$ if, given a system containing entities belonging to the fiber of\n$K$, it generates new entities that also belong to that fiber. Similarity Field\nTheory thus offers a foundational language for characterizing, comparing, and\nconstructing intelligent systems. We prove two theorems: (i) asymmetry blocks\nmutual inclusion; and (ii) stability requires either an anchor coordinate or\neventual confinement within a level set of $f$. These results ensure that the\nevolution of similarity fields is both constrained and interpretable,\nculminating in an exploration of how the framework allows us to interpret large\nlanguage models and use them as experimental probes into societal cognition.", "AI": {"tldr": "\u672c\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u76f8\u4f3c\u6027\u573a\u7406\u8bba\uff08Similarity Field Theory, SFT\uff09\uff0c\u4e00\u4e2a\u5f62\u5f0f\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u63cf\u8ff0\u52a8\u6001\u7cfb\u7edf\u4e2d\u5b9e\u4f53\u4e4b\u95f4\u76f8\u4f3c\u6027\u5173\u7cfb\u53ca\u5176\u6f14\u5316\u89c4\u5f8b\u3002\u7406\u8bba\u6838\u5fc3\u5305\u62ec\u76f8\u4f3c\u6027\u573aS\u3001\u7cfb\u7edf\u6f14\u5316Z_p\u3001\u6982\u5ff5K\u53ca\u5176\u8bf1\u5bfc\u7684\u7ea4\u7ef4F_\u03b1(K)\uff0c\u4ee5\u53ca\u751f\u6210\u65b0\u5b9e\u4f53\u7684\u751f\u6210\u7b97\u5b50G\u3002\u57fa\u4e8e\u6b64\uff0c\u8bba\u6587\u63d0\u51fa\u4e86\u667a\u80fd\u7684\u751f\u6210\u5b9a\u4e49\uff1a\u4e00\u4e2a\u7b97\u5b50G\u82e5\u80fd\u57fa\u4e8e\u6982\u5ff5K\u7684\u7ea4\u7ef4\u4e2d\u7684\u5b9e\u4f53\u751f\u6210\u65b0\u7684\u4e5f\u5c5e\u4e8e\u8be5\u7ea4\u7ef4\u7684\u5b9e\u4f53\uff0c\u5219\u8be5\u7b97\u5b50\u662f\u667a\u80fd\u7684\u3002SFT\u4e3a\u8868\u5f81\u3001\u6bd4\u8f83\u548c\u6784\u5efa\u667a\u80fd\u7cfb\u7edf\u63d0\u4f9b\u4e86\u57fa\u7840\u8bed\u8a00\u3002\u8bba\u6587\u8bc1\u660e\u4e86\u4e24\u4e2a\u5b9a\u7406\uff1a(i) \u4e0d\u5bf9\u79f0\u6027\u963b\u6b62\u4e86\u76f8\u4e92\u5305\u542b\uff1b(ii) \u7a33\u5b9a\u6027\u9700\u8981\u951a\u5b9a\u5750\u6807\u6216\u6700\u7ec8\u9650\u5236\u5728f\u7684\u67d0\u4e2a\u5c42\u96c6\u5185\u3002\u8fd9\u4e9b\u7ed3\u679c\u786e\u4fdd\u4e86\u76f8\u4f3c\u6027\u573a\u7684\u6f14\u5316\u662f\u53d7\u7ea6\u675f\u4e14\u53ef\u89e3\u91ca\u7684\u3002\u6700\u540e\uff0c\u8bba\u6587\u63a2\u8ba8\u4e86SFT\u5982\u4f55\u7528\u4e8e\u89e3\u91ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u5c06\u5176\u7528\u4f5c\u63a2\u7d22\u793e\u4f1a\u8ba4\u77e5\u7684\u5b9e\u9a8c\u5de5\u5177\u3002", "motivation": "\u6211\u4eec\u8ba4\u4e3a\uff0c\u6301\u7eed\u5b58\u5728\u7684\u548c\u8f6c\u5316\u7684\u76f8\u4f3c\u6027\u5173\u7cfb\u662f\u4efb\u4f55\u53ef\u7406\u89e3\u7684\u52a8\u6001\u7cfb\u7edf\u7684\u7ed3\u6784\u57fa\u7840\u3002\u672c\u7bc7\u8bba\u6587\u65e8\u5728\u63d0\u51fa\u4e00\u4e2a\u540d\u4e3a\u201c\u76f8\u4f3c\u6027\u573a\u7406\u8bba\u201d\u7684\u6570\u5b66\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5f62\u5f0f\u5316\u4e86\u5b9e\u4f53\u95f4\u76f8\u4f3c\u6027\u503c\u53ca\u5176\u6f14\u5316\u6240\u9075\u5faa\u7684\u539f\u7406\u3002", "method": "\u672c\u7814\u7a76\u5b9a\u4e49\u4e86\u4ee5\u4e0b\u6838\u5fc3\u6982\u5ff5\uff1a(1) \u76f8\u4f3c\u6027\u573a S: U \u00d7 U \u2192 [0,1]\uff0c\u8fd9\u662f\u4e00\u4e2a\u5728\u5b9e\u4f53\u96c6\u5408U\u4e0a\u5b9a\u4e49\u7684\u573a\uff0c\u6ee1\u8db3\u81ea\u53cd\u6027 S(E,E)=1\uff0c\u5e76\u4e14\u53ef\u4ee5\u662f\u4e0d\u5bf9\u79f0\u548c\u975e\u4f20\u9012\u7684\u3002(2) \u7cfb\u7edf\u7684\u6f14\u5316 Z_p = (X_p, S^(p))\uff0c\u5176\u4e2d p=0,1,2,...\u3002(3) \u6982\u5ff5 K\uff0c\u88ab\u5b9a\u4e49\u4e3a\u8bf1\u5bfc\u7ea4\u7ef4 F_\u03b1(K) = { E \u2208 U | S(E,K) \u2265 \u03b1 } \u7684\u5b9e\u4f53\uff0c\u5373\u4e00\u5143\u6620\u5c04 S_K(E) := S(E,K) \u7684\u8d85\u6c34\u5e73\u96c6\u3002(4) \u751f\u6210\u7b97\u5b50 G\uff0c\u7528\u4e8e\u4ea7\u751f\u65b0\u7684\u5b9e\u4f53\u3002\u5728\u6b64\u6846\u67b6\u4e0b\uff0c\u7814\u7a76\u5f62\u5f0f\u5316\u4e86\u667a\u80fd\u7684\u751f\u6210\u5b9a\u4e49\uff1a\u5982\u679c\u4e00\u4e2a\u7b97\u5b50G\u80fd\u591f\u57fa\u4e8e\u5305\u542b\u6982\u5ff5K\u7684\u7ea4\u7ef4\u4e2d\u7684\u5b9e\u4f53\u751f\u6210\u65b0\u7684\u4e5f\u5c5e\u4e8e\u8be5\u7ea4\u7ef4\u7684\u5b9e\u4f53\uff0c\u5219\u79f0\u8be5\u7b97\u5b50\u662f\u667a\u80fd\u7684\u3002\u6b64\u5916\uff0c\u8fd8\u8bc1\u660e\u4e86\u4e24\u4e2a\u5b9a\u7406\uff1a(i) \u4e0d\u5bf9\u79f0\u6027\u963b\u6b62\u76f8\u4e92\u5305\u542b\uff1b(ii) \u7a33\u5b9a\u6027\u9700\u8981\u951a\u5b9a\u5750\u6807\u6216\u6700\u7ec8\u88ab\u9650\u5236\u5728\u67d0\u4e2a\u5c42\u96c6\u5185\u3002", "result": "\u672c\u7814\u7a76\u8bc1\u660e\u4e86\u4e24\u4e2a\u5b9a\u7406\uff1a(i) \u4e0d\u5bf9\u79f0\u6027\u53ef\u4ee5\u963b\u6b62\u5b9e\u4f53\u95f4\u7684\u76f8\u4e92\u5305\u542b\u5173\u7cfb\uff1b(ii) \u7cfb\u7edf\u7684\u7a33\u5b9a\u6027\u9700\u8981\u6ee1\u8db3\u4e24\u4e2a\u6761\u4ef6\u4e4b\u4e00\uff1a\u8981\u4e48\u5b58\u5728\u4e00\u4e2a\u951a\u5b9a\u5750\u6807\uff0c\u8981\u4e48\u7cfb\u7edf\u6700\u7ec8\u88ab\u9650\u5236\u5728\u67d0\u4e2a\u5c42\u96c6\u5185\u3002\u8fd9\u4e9b\u7ed3\u679c\u4fdd\u8bc1\u4e86\u76f8\u4f3c\u6027\u573a\u7684\u6f14\u5316\u662f\u53d7\u5230\u7ea6\u675f\u4e14\u53ef\u89e3\u91ca\u7684\u3002", "conclusion": "\u76f8\u4f3c\u6027\u573a\u7406\u8bba\u4e3a\u8868\u5f81\u3001\u6bd4\u8f83\u548c\u6784\u5efa\u667a\u80fd\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u4e2a\u57fa\u7840\u6027\u7684\u8bed\u8a00\u3002\u901a\u8fc7\u8bc1\u660e\u7684\u5b9a\u7406\uff0c\u8be5\u6846\u67b6\u786e\u4fdd\u4e86\u76f8\u4f3c\u6027\u573a\u6f14\u5316\u7684\u7ea6\u675f\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002\u6700\u540e\uff0c\u8bba\u6587\u5c06\u8be5\u7406\u8bba\u5e94\u7528\u4e8e\u89e3\u91ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u63d0\u51fa\u5229\u7528\u5b83\u4eec\u4f5c\u4e3a\u63a2\u7d22\u793e\u4f1a\u8ba4\u77e5\u7684\u5b9e\u9a8c\u5de5\u5177\u3002"}}
{"id": "2509.18360", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18360", "abs": "https://arxiv.org/abs/2509.18360", "authors": ["Chutong Meng", "Philipp Koehn"], "title": "Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents", "comment": "Accepted by EMNLP 2025 (main)", "summary": "We present Speech Vecalign, a parallel speech document alignment method that\nmonotonically aligns speech segment embeddings and does not depend on text\ntranscriptions. Compared to the baseline method Global Mining, a variant of\nspeech mining, Speech Vecalign produces longer speech-to-speech alignments. It\nalso demonstrates greater robustness than Local Mining, another speech mining\nvariant, as it produces less noise. We applied Speech Vecalign to 3,000 hours\nof unlabeled parallel English-German (En-De) speech documents from VoxPopuli,\nyielding about 1,000 hours of high-quality alignments. We then trained En-De\nspeech-to-speech translation models on the aligned data. Speech Vecalign\nimproves the En-to-De and De-to-En performance over Global Mining by 0.37 and\n0.18 ASR-BLEU, respectively. Moreover, our models match or outperform\nSpeechMatrix model performance, despite using 8 times fewer raw speech\ndocuments.", "AI": {"tldr": "Speech Vecalign\u662f\u4e00\u79cd\u65b0\u7684\u8bed\u97f3-\u8bed\u97f3\u5bf9\u9f50\u65b9\u6cd5\uff0c\u65e0\u9700\u6587\u672c\u5373\u53ef\u5b9e\u73b0\u5355\u8c03\u5bf9\u9f50\uff0c\u5e76\u4e14\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u9c81\u68d2\u3002", "motivation": "\u73b0\u6709\u8bed\u97f3\u5bf9\u9f50\u65b9\u6cd5\u4f9d\u8d56\u6587\u672c\u6216\u4e0d\u591f\u9c81\u68d2\uff0c\u9650\u5236\u4e86\u8bed\u97f3\u7ffb\u8bd1\u6a21\u578b\u7684\u8bad\u7ec3\u3002", "method": "Speech Vecalign\u662f\u4e00\u79cd\u5e76\u884c\u8bed\u97f3\u6587\u6863\u5bf9\u9f50\u65b9\u6cd5\uff0c\u5bf9\u8bed\u97f3\u6bb5\u5d4c\u5165\u8fdb\u884c\u5355\u8c03\u5bf9\u9f50\uff0c\u4e0d\u4f9d\u8d56\u6587\u672c\u3002", "result": "\u57283000\u5c0f\u65f6\u7684\u672a\u6807\u8bb0\u82f1\u8bed-\u5fb7\u8bed\u5e76\u884c\u8bed\u97f3\u6570\u636e\u4e0a\uff0cSpeech Vecalign\u4ea7\u751f\u4e86\u7ea61000\u5c0f\u65f6\u7684\u9ad8\u8d28\u91cf\u5bf9\u9f50\u3002\u57fa\u4e8e\u6b64\u6570\u636e\u8bad\u7ec3\u7684\u8bed\u97f3\u7ffb\u8bd1\u6a21\u578b\u5728En-De\u548cDe-En\u65b9\u5411\u4e0a\u6bd4\u57fa\u7ebf\u65b9\u6cd5Global Mining\u7684ASR-BLEU\u5206\u522b\u63d0\u9ad8\u4e860.37\u548c0.18\u3002\u540c\u65f6\uff0c\u8be5\u6a21\u578b\u5728\u4f7f\u7528\u7684\u539f\u59cb\u8bed\u97f3\u6587\u6863\u6570\u91cf\u662fSpeechMatrix\u76841/8\u7684\u60c5\u51b5\u4e0b\uff0c\u6027\u80fd\u76f8\u5f53\u6216\u66f4\u4f18\u3002", "conclusion": "Speech Vecalign\u662f\u4e00\u79cd\u6709\u6548\u7684\u8bed\u97f3-\u8bed\u97f3\u5bf9\u9f50\u65b9\u6cd5\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u8bed\u97f3\u7ffb\u8bd1\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u5728\u6570\u636e\u6548\u7387\u65b9\u9762\u5177\u6709\u4f18\u52bf\u3002"}}
{"id": "2509.19045", "categories": ["eess.SY", "cs.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.19045", "abs": "https://arxiv.org/abs/2509.19045", "authors": ["Dakota Thompson", "Amro M. Farid"], "title": "A Weighted Least Squares Error Hetero-functional Graph State Estimator of the American Multi-modal Energy System", "comment": "27 pages, 3 Tables, 11 Figures", "summary": "As one of the most pressing challenges of the 21st century, global climate\nchange demands a host of changes across at least four critical energy\ninfrastructures: the electric grid, the natural gas system, the oil system, and\nthe coal system. In the context of the United States, this paper refers to this\nsystem-of-systems as ``The American Multi-Modal Energy System (AMES)\". These\ncombined changes necessitate an understanding of the AMES interdependencies,\nboth structurally and behaviorally, to develop and enact effective policies.\nThis work focuses on behavioral analysis methods to provide examples of how to\nanalyze system behavior and the critical matter and energy flows through the\nsystem. Building upon past works, two regions of the AMES are modeled, and\ntheir behavior is analyzed using Hetero-functional Graph Theory (HFGT). More\nspecifically, the work presents a weighted least square error state estimation\nmodel of the AMES. State estimation has played a major role in the operation\nand development of the American Electric Power System. This work extends the\nstate estimation analysis beyond the single-operand electric grid environment\ninto the heterogeneous environment of the AMES. Employing a data-driven and\nmodel-based systems engineering approach in combination with HFGT, a Weighted\nLeast Squares Error Hetero-functional Graph State Estimation (WLSEHFGSE)\noptimization program is developed to estimate the optimal flows of mass and\nenergy through the AMES. This work is the first to integrate state estimation\nmethods with HFGT. Furthermore, it demonstrates how such a WLSEHFGSE recovers\nthe mass and energy flows in a system-of-systems like the AMES with asset-level\ngranularity.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a\u201c\u7f8e\u56fd\u591a\u6a21\u5f0f\u80fd\u6e90\u7cfb\u7edf\uff08AMES\uff09\u201d\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u6790\u7f8e\u56fd\u80fd\u6e90\u57fa\u7840\u8bbe\u65bd\uff08\u7535\u7f51\u3001\u5929\u7136\u6c14\u3001\u77f3\u6cb9\u548c\u7164\u70ad\uff09\u5728\u6c14\u5019\u53d8\u5316\u80cc\u666f\u4e0b\u7684\u76f8\u4e92\u4f9d\u8d56\u6027\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f02\u6784\u56fe\u8bba\uff08HFGT\uff09\u548c\u52a0\u6743\u6700\u5c0f\u4e8c\u4e58\u8bef\u5dee\uff08WLSE\uff09\u7684\u72b6\u6001\u4f30\u8ba1\u65b9\u6cd5\uff08WLSEHFGSE\uff09\uff0c\u4ee5\u8d44\u4ea7\u7ea7\u7c92\u5ea6\u6062\u590d\u7cfb\u7edf\u4e2d\u7684\u7269\u8d28\u548c\u80fd\u91cf\u6d41\u3002", "motivation": "\u9274\u4e8e 21 \u4e16\u7eaa\u5168\u7403\u6c14\u5019\u53d8\u5316\u5e26\u6765\u7684\u4e25\u5cfb\u6311\u6218\uff0c\u9700\u8981\u5bf9\u5305\u62ec\u7535\u7f51\u3001\u5929\u7136\u6c14\u3001\u77f3\u6cb9\u548c\u7164\u70ad\u5728\u5185\u7684\u7f8e\u56fd\u591a\u6a21\u5f0f\u80fd\u6e90\u7cfb\u7edf\uff08AMES\uff09\u8fdb\u884c\u6839\u672c\u6027\u53d8\u9769\u3002\u4e3a\u4e86\u5236\u5b9a\u6709\u6548\u7684\u653f\u7b56\uff0c\u5fc5\u987b\u7406\u89e3 AMES \u7684\u7ed3\u6784\u548c\u884c\u4e3a\u76f8\u4e92\u4f9d\u8d56\u6027\u3002", "method": "\u8be5\u7814\u7a76\u91c7\u7528\u5f02\u6784\u56fe\u8bba\uff08HFGT\uff09\u5bf9 AMES \u7684\u4e24\u4e2a\u533a\u57df\u8fdb\u884c\u5efa\u6a21\u548c\u884c\u4e3a\u5206\u6790\u3002\u5177\u4f53\u800c\u8a00\uff0c\u8bba\u6587\u5f00\u53d1\u4e86\u4e00\u79cd\u52a0\u6743\u6700\u5c0f\u4e8c\u4e58\u8bef\u5dee\u72b6\u6001\u4f30\u8ba1\u6a21\u578b\uff08WLSEHFGSE\uff09\uff0c\u8be5\u6a21\u578b\u7ed3\u5408\u4e86\u6570\u636e\u9a71\u52a8\u3001\u57fa\u4e8e\u6a21\u578b\u7684\u7cfb\u7edf\u5de5\u7a0b\u65b9\u6cd5\u548c HFGT\uff0c\u65e8\u5728\u4f18\u5316 AMES \u4e2d\u7684\u7269\u8d28\u548c\u80fd\u91cf\u6d41\u3002", "result": "\u8be5\u7814\u7a76\u9996\u6b21\u5c06\u72b6\u6001\u4f30\u8ba1\u65b9\u6cd5\u4e0e HFGT \u7ed3\u5408\uff0c\u5e76\u5f00\u53d1\u4e86 WLSEHFGSE \u4f18\u5316\u7a0b\u5e8f\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u4ee5\u8d44\u4ea7\u7ea7\u7c92\u5ea6\u6062\u590d AMES \u8fd9\u79cd\u590d\u6742\u7cfb\u7edf\u4e2d\u7684\u7269\u8d28\u548c\u80fd\u91cf\u6d41\u3002", "conclusion": "\u8be5\u8bba\u6587\u6210\u529f\u5730\u5c06\u72b6\u6001\u4f30\u8ba1\u65b9\u6cd5\u4e0e\u5f02\u6784\u56fe\u8bba\u76f8\u7ed3\u5408\uff0c\u5f00\u53d1\u51fa\u4e00\u79cd\u80fd\u591f\u7cbe\u786e\u6062\u590d\u7f8e\u56fd\u591a\u6a21\u5f0f\u80fd\u6e90\u7cfb\u7edf\u4e2d\u7269\u8d28\u548c\u80fd\u91cf\u6d41\u7684\u65b9\u6cd5\uff0c\u4e3a\u5e94\u5bf9\u6c14\u5019\u53d8\u5316\u76f8\u5173\u7684\u80fd\u6e90\u7cfb\u7edf\u6311\u6218\u63d0\u4f9b\u4e86\u65b0\u7684\u9014\u5f84\u3002"}}
{"id": "2509.18663", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2509.18663", "abs": "https://arxiv.org/abs/2509.18663", "authors": ["Miguel Lagos", "Miguel Kiwi", "Rodrigo Paredes"], "title": "Beyond Bloch: A Theoretical Blueprint for Conjugated Polymer Optoelectronics", "comment": null, "summary": "Conjugated polymers are experiencing a surge of renewed interest due to their\npromising applications in various organic electronic devices. These include\norganic light-emitting diodes (OLEDs), field-effect transistors (FETs), and\norganic photovoltaic (OPV) devices, among many others. Their appeal stems from\ndistinct advantages they hold over traditional inorganic semiconductors. Unlike\ninorganic semiconductors, where electrons are often considered to be in\ndelocalized, free, or quasi-free states (as described by Bloch's theory),\nelectrons in conjugated polymers behave differently. They are strongly coupled\nwithin highly localized $\\sigma$ or $\\pi$-orbitals and interact significantly\nwith the ionic cores. This means they are far from the idealized delocalized\nstates presumed by Bloch's theory approaches. Consequently, after nearly a\ncentury of applying Bloch's theory to the electronic transport properties of\ninorganic materials, there is a clear need for a new theoretical framework to\nexplain efficient charge transport in these organic solids. Our presented model\naddresses this need by incorporating crucial electron-electron interactions.\nSpecifically, it accounts for both intra-site interactions and interactions\nbetween the $\\pi$-states located at alternating sites along the polymer chain.\nThis framework provides a many-body charge conduction mechanism and explains\nthe semiconducting properties of the undoped material. A significant outcome of\nour model is the prediction of two novel flat bands of excited bonding states.\nIntriguingly, these states obey Bose--Einstein statistics and facilitate charge\ntransport. Furthermore, our model accurately reproduces experimental data,\nproviding an excellent fit for measured UV-Vis absorption and\nelectroluminescent spectra.", "AI": {"tldr": "\u5171\u8f6d\u805a\u5408\u7269\u5728\u6709\u673a\u7535\u5b50\u5668\u4ef6\u4e2d\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46\u9700\u8981\u65b0\u7684\u7406\u8bba\u6846\u67b6\u6765\u89e3\u91ca\u5176\u7535\u8377\u4f20\u8f93\u3002\u8be5\u6a21\u578b\u8003\u8651\u4e86\u7535\u5b50-\u7535\u5b50\u76f8\u4e92\u4f5c\u7528\uff0c\u5e76\u9884\u6d4b\u4e86\u4e24\u79cd\u65b0\u7684\u6fc0\u53d1\u6001\uff0c\u8fd9\u4e9b\u6001\u9075\u5faa\u73bb\u8272-\u7231\u56e0\u65af\u5766\u7edf\u8ba1\u5e76\u4fc3\u8fdb\u7535\u8377\u4f20\u8f93\u3002", "motivation": "\u9700\u8981\u4e00\u4e2a\u65b0\u7684\u7406\u8bba\u6846\u67b6\u6765\u89e3\u91ca\u5171\u8f6d\u805a\u5408\u7269\u4e2d\u4e0e\u5e03\u6d1b\u8d6b\u7406\u8bba\u6240\u63cf\u8ff0\u7684\u65e0\u673a\u6750\u6599\u4e0d\u540c\u7684\u7535\u8377\u4f20\u8f93\u673a\u5236\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u5305\u542b\u7535\u5b50-\u7535\u5b50\u76f8\u4e92\u4f5c\u7528\uff08\u5305\u62ec\u7ad9\u5185\u76f8\u4e92\u4f5c\u7528\u548c\u4ea4\u66ff\u4f4d\u70b9\u4e4b\u95f4\u7684\u03c0\u6001\u76f8\u4e92\u4f5c\u7528\uff09\u7684\u6a21\u578b\u3002", "result": "\u6a21\u578b\u9884\u6d4b\u4e86\u4e24\u79cd\u65b0\u7684\u3001\u9075\u5faa\u73bb\u8272-\u7231\u56e0\u65af\u5766\u7edf\u8ba1\u7684\u5e73\u5e26\u6fc0\u53d1\u6001\uff0c\u8fd9\u4e24\u79cd\u72b6\u6001\u6709\u52a9\u4e8e\u7535\u8377\u4f20\u8f93\u3002\u8be5\u6a21\u578b\u8fd8\u80fd\u7cbe\u786e\u91cd\u73b0\u5b9e\u9a8c\u6d4b\u5f97\u7684\u7d2b\u5916-\u53ef\u89c1\u5438\u6536\u548c\u7535\u81f4\u53d1\u5149\u5149\u8c31\u3002", "conclusion": "\u8be5\u6a21\u578b\u6210\u529f\u5730\u89e3\u91ca\u4e86\u5171\u8f6d\u805a\u5408\u7269\u7684\u534a\u5bfc\u4f53\u7279\u6027\u548c\u7535\u8377\u4f20\u8f93\u673a\u5236\uff0c\u5e76\u5f97\u5230\u4e86\u5b9e\u9a8c\u6570\u636e\u7684\u652f\u6301\u3002"}}
{"id": "2509.19130", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.19130", "abs": "https://arxiv.org/abs/2509.19130", "authors": ["Abolfazl Zakeri", "Nhan Thanh Nguyen", "Ahmed Alkhateeb", "Markku Juntti"], "title": "Deep Reinforcement Learning for Dynamic Sensing and Communications", "comment": "Under review for possible publication", "summary": "Environmental sensing can significantly enhance mmWave communications by\nassisting beam training, yet its benefits must be balanced against the\nassociated sensing costs. To this end, we propose a unified machine learning\nframework that dynamically determines when to sense and leverages sensory data\nfor beam prediction. Specifically, we formulate a joint sensing and beamforming\nproblem that maximizes the av- erage signal-to-noise ratio under an average\nsensing budget. Lyapunov optimization is employed to enforce the sensing\nconstraint, while a deep Q-Network determines the sensing slots. A pretrained\ndeep neural network then maps the sens- ing data to optimal beams in the\ncodebook. Simulations based on the real-world DeepSense dataset demonstrate\nthat the pro- posed approach substantially reduces sensing overhead while\nmaintaining satisfactory communications performance.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u7684\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u52a8\u6001\u786e\u5b9a\u4f55\u65f6\u8fdb\u884c\u611f\u77e5\u4ee5\u53ca\u5229\u7528\u611f\u5b98\u6570\u636e\u8fdb\u884c\u6ce2\u675f\u9884\u6d4b\uff0c\u4ee5\u5728\u5e73\u5747\u611f\u77e5\u9884\u7b97\u4e0b\u6700\u5927\u5316\u5e73\u5747\u4fe1\u566a\u6bd4\u3002", "motivation": "\u5728\u6beb\u7c73\u6ce2\u901a\u4fe1\u4e2d\uff0c\u73af\u5883\u611f\u77e5\u53ef\u4ee5\u5e2e\u52a9\u6ce2\u675f\u8bad\u7ec3\uff0c\u4f46\u9700\u8981\u6743\u8861\u611f\u77e5\u6210\u672c\u3002", "method": "\u4f7f\u7528Lyapunov\u4f18\u5316\u6765\u6267\u884c\u611f\u77e5\u7ea6\u675f\uff0c\u5e76\u4f7f\u7528\u6df1\u5ea6Q\u7f51\u7edc\u6765\u786e\u5b9a\u611f\u77e5\u65f6\u9699\u3002\u7136\u540e\uff0c\u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5c06\u611f\u77e5\u6570\u636e\u6620\u5c04\u5230\u7801\u672c\u4e2d\u7684\u6700\u4f73\u6ce2\u675f\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u611f\u77e5\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4ee4\u4eba\u6ee1\u610f\u7684\u901a\u4fe1\u6027\u80fd\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5730\u5e73\u8861\u611f\u77e5\u6210\u672c\u548c\u901a\u4fe1\u6027\u80fd\u3002"}}
{"id": "2509.18466", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18466", "abs": "https://arxiv.org/abs/2509.18466", "authors": ["Junnosuke Kamohara", "Feiyang Wu", "Chinmayee Wamorkar", "Seth Hutchinson", "Ye Zhao"], "title": "RL-augmented Adaptive Model Predictive Control for Bipedal Locomotion over Challenging Terrain", "comment": null, "summary": "Model predictive control (MPC) has demonstrated effectiveness for humanoid\nbipedal locomotion; however, its applicability in challenging environments,\nsuch as rough and slippery terrain, is limited by the difficulty of modeling\nterrain interactions. In contrast, reinforcement learning (RL) has achieved\nnotable success in training robust locomotion policies over diverse terrain,\nyet it lacks guarantees of constraint satisfaction and often requires\nsubstantial reward shaping. Recent efforts in combining MPC and RL have shown\npromise of taking the best of both worlds, but they are primarily restricted to\nflat terrain or quadrupedal robots. In this work, we propose an RL-augmented\nMPC framework tailored for bipedal locomotion over rough and slippery terrain.\nOur method parametrizes three key components of\nsingle-rigid-body-dynamics-based MPC: system dynamics, swing leg controller,\nand gait frequency. We validate our approach through bipedal robot simulations\nin NVIDIA IsaacLab across various terrains, including stairs, stepping stones,\nand low-friction surfaces. Experimental results demonstrate that our\nRL-augmented MPC framework produces significantly more adaptive and robust\nbehaviors compared to baseline MPC and RL.", "AI": {"tldr": "\u7ed3\u5408MPC\u548cRL\u7684\u5f3a\u5316\u5b66\u4e60\u589e\u5f3aMPC\u6846\u67b6\uff0c\u7528\u4e8e\u5b9e\u73b0\u4eba\u5f62\u53cc\u8db3\u673a\u5668\u4eba\u5728\u5d0e\u5c96\u6e7f\u6ed1\u5730\u5f62\u4e0a\u7684\u9c81\u68d2\u884c\u8d70\u3002", "motivation": "\u4f20\u7edfMPC\u5728\u5d0e\u5c96\u6e7f\u6ed1\u5730\u5f62\u4e0a\u5e94\u7528\u53d7\u9650\uff0cRL\u7f3a\u4e4f\u7ea6\u675f\u6ee1\u8db3\u4fdd\u8bc1\u3002\u73b0\u6709MPC\u548cRL\u7684\u7ed3\u5408\u4e3b\u8981\u7528\u4e8e\u5e73\u5766\u5730\u5f62\u6216\u56db\u8db3\u673a\u5668\u4eba\u3002", "method": "\u63d0\u51fa\u4e00\u4e2aRL\u589e\u5f3a\u7684MPC\u6846\u67b6\uff0c\u4e3a\u5355\u521a\u4f53\u52a8\u529b\u5b66MPC\u7684\u7cfb\u7edf\u52a8\u529b\u5b66\u3001\u6446\u52a8\u817f\u63a7\u5236\u5668\u548c\u6b65\u6001\u9891\u7387\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\u8fdb\u884c\u53c2\u6570\u5316\uff0c\u4ee5\u9002\u5e94\u5d0e\u5c96\u6e7f\u6ed1\u5730\u5f62\u7684\u53cc\u8db3\u884c\u8d70\u3002", "result": "\u901a\u8fc7\u5728NVIDIA IsaacLab\u4e2d\u5bf9\u673a\u5668\u4eba\u8fdb\u884c\u6a21\u62df\uff0c\u5305\u62ec\u697c\u68af\u3001\u8e0f\u811a\u77f3\u548c\u4f4e\u6469\u64e6\u8868\u9762\u7b49\u591a\u79cd\u5730\u5f62\u7684\u6d4b\u8bd5\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "RL\u589e\u5f3a\u7684MPC\u6846\u67b6\u76f8\u6bd4\u4e8e\u57fa\u7ebfMPC\u548cRL\uff0c\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\uff0c\u80fd\u591f\u5b9e\u73b0\u66f4\u4f18\u7684\u53cc\u8db3\u884c\u8d70\u3002"}}
{"id": "2509.18187", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18187", "abs": "https://arxiv.org/abs/2509.18187", "authors": ["Muhammad Naveed", "Nazia Perwaiz", "Sidra Sultana", "Mohaira Ahmad", "Muhammad Moazam Fraz"], "title": "V-SenseDrive: A Privacy-Preserving Road Video and In-Vehicle Sensor Fusion Framework for Road Safety & Driver Behaviour Modelling", "comment": null, "summary": "Road traffic accidents remain a major public health challenge, particularly\nin countries with heterogeneous road conditions, mixed traffic flow, and\nvariable driving discipline, such as Pakistan. Reliable detection of unsafe\ndriving behaviours is a prerequisite for improving road safety, enabling\nadvanced driver assistance systems (ADAS), and supporting data driven decisions\nin insurance and fleet management. Most of existing datasets originate from the\ndeveloped countries with limited representation of the behavioural diversity\nobserved in emerging economies and the driver's face recording voilates the\nprivacy preservation. We present V-SenseDrive, the first privacy-preserving\nmultimodal driver behaviour dataset collected entirely within the Pakistani\ndriving environment. V-SenseDrive combines smartphone based inertial and GPS\nsensor data with synchronized road facing video to record three target driving\nbehaviours (normal, aggressive, and risky) on multiple types of roads,\nincluding urban arterials, secondary roads, and motorways. Data was gathered\nusing a custom Android application designed to capture high frequency\naccelerometer, gyroscope, and GPS streams alongside continuous video, with all\nsources precisely time aligned to enable multimodal analysis. The focus of this\nwork is on the data acquisition process, covering participant selection,\ndriving scenarios, environmental considerations, and sensor video\nsynchronization techniques. The dataset is structured into raw, processed, and\nsemantic layers, ensuring adaptability for future research in driver behaviour\nclassification, traffic safety analysis, and ADAS development. By representing\nreal world driving in Pakistan, V-SenseDrive fills a critical gap in the global\nlandscape of driver behaviour datasets and lays the groundwork for context\naware intelligent transportation solutions.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86V-SenseDrive\u6570\u636e\u96c6\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u5728\u5df4\u57fa\u65af\u5766\u9a7e\u9a76\u73af\u5883\u4e2d\u6536\u96c6\u7684\u3001\u6ce8\u91cd\u9690\u79c1\u7684\u591a\u6a21\u6001\u9a7e\u9a76\u884c\u4e3a\u6570\u636e\u96c6\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u6570\u636e\u96c6\u5728\u4ee3\u8868\u6027\u548c\u9690\u79c1\u4fdd\u62a4\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u76ee\u524d\u7f3a\u4e4f\u80fd\u4ee3\u8868\u65b0\u5174\u7ecf\u6d4e\u4f53\u9a7e\u9a76\u884c\u4e3a\u591a\u6837\u6027\u4e14\u6ce8\u91cd\u9690\u79c1\u7684\u6570\u636e\u96c6\uff0c\u5c24\u5176\u662f\u5728\u5df4\u57fa\u65af\u5766\u8fd9\u6837\u7684\u56fd\u5bb6\uff0c\u590d\u6742\u7684\u9053\u8def\u6761\u4ef6\u548c\u9a7e\u9a76\u4e60\u60ef\u4f7f\u5f97\u53ef\u9760\u7684\u9a7e\u9a76\u884c\u4e3a\u68c0\u6d4b\u5c24\u4e3a\u91cd\u8981\uff0c\u8fd9\u5bf9\u4e8e\u63d0\u5347\u9053\u8def\u5b89\u5168\u3001\u5f00\u53d1\u9ad8\u7ea7\u9a7e\u9a76\u8f85\u52a9\u7cfb\u7edf\uff08ADAS\uff09\u4ee5\u53ca\u652f\u6301\u4fdd\u9669\u548c\u8f66\u961f\u7ba1\u7406\u7684\u6570\u636e\u9a71\u52a8\u51b3\u7b56\u81f3\u5173\u91cd\u8981\u3002", "method": "V-SenseDrive\u6570\u636e\u96c6\u6536\u96c6\u4e86\u7ed3\u5408\u667a\u80fd\u624b\u673a\u4f20\u611f\u5668\uff08\u60ef\u6027\u6d4b\u91cf\u5355\u5143\u548cGPS\uff09\u6570\u636e\u4ee5\u53ca\u9053\u8def\u89c6\u9891\u6570\u636e\uff0c\u8bb0\u5f55\u4e86\u6b63\u5e38\u3001\u6fc0\u8fdb\u548c\u5371\u9669\u4e09\u79cd\u9a7e\u9a76\u884c\u4e3a\u3002\u6570\u636e\u901a\u8fc7\u81ea\u5b9a\u4e49\u7684Android\u5e94\u7528\u7a0b\u5e8f\u91c7\u96c6\uff0c\u5305\u542b\u9ad8\u9891\u7387\u7684\u52a0\u901f\u5ea6\u8ba1\u3001\u9640\u87ba\u4eea\u548cGPS\u6d41\u4ee5\u53ca\u8fde\u7eed\u89c6\u9891\uff0c\u6240\u6709\u6570\u636e\u6e90\u7cbe\u786e\u540c\u6b65\u3002\u7814\u7a76\u91cd\u70b9\u5728\u4e8e\u6570\u636e\u91c7\u96c6\u8fc7\u7a0b\uff0c\u5305\u62ec\u53c2\u4e0e\u8005\u9009\u62e9\u3001\u9a7e\u9a76\u573a\u666f\u3001\u73af\u5883\u8003\u8651\u548c\u4f20\u611f\u5668\u89c6\u9891\u540c\u6b65\u6280\u672f\u3002\u6570\u636e\u96c6\u5206\u4e3a\u539f\u59cb\u3001\u5904\u7406\u548c\u8bed\u4e49\u4e09\u4e2a\u5c42\u6b21\u3002", "result": "V-SenseDrive\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u591a\u6a21\u6001\u7684\u9a7e\u9a76\u884c\u4e3a\u6570\u636e\uff0c\u5305\u62ec\u4f20\u611f\u5668\u6570\u636e\u548c\u89c6\u9891\uff0c\u8986\u76d6\u4e86\u4e0d\u540c\u7c7b\u578b\u7684\u9053\u8def\uff08\u57ce\u5e02\u4e3b\u5e72\u9053\u3001\u6b21\u5e72\u9053\u548c\u9ad8\u901f\u516c\u8def\uff09\uff0c\u8bb0\u5f55\u4e86\u4e09\u79cd\u9a7e\u9a76\u884c\u4e3a\uff08\u6b63\u5e38\u3001\u6fc0\u8fdb\u548c\u5371\u9669\uff09\u3002\u6570\u636e\u96c6\u7ed3\u6784\u5316\u5206\u5c42\uff0c\u4fbf\u4e8e\u672a\u6765\u7814\u7a76\u3002", "conclusion": "V-SenseDrive\u6570\u636e\u96c6\u586b\u8865\u4e86\u5168\u7403\u9a7e\u9a76\u884c\u4e3a\u6570\u636e\u96c6\u5728\u4ee3\u8868\u5df4\u57fa\u65af\u5766\u771f\u5b9e\u9a7e\u9a76\u73af\u5883\u65b9\u9762\u7684\u7a7a\u767d\uff0c\u4e3a\u5f00\u53d1\u60c5\u5883\u611f\u77e5\u548c\u667a\u80fd\u4ea4\u901a\u89e3\u51b3\u65b9\u6848\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u9053\u8def\u5b89\u5168\u548cADAS\u7684\u5f00\u53d1\u3002"}}
{"id": "2509.18379", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2509.18379", "abs": "https://arxiv.org/abs/2509.18379", "authors": ["F. Q. Guo", "Shi-Lei Su", "Weibin Li", "X. Q. Shao"], "title": "Scalable Steady-State Entanglement with Floquet-Engineered Stabilizer Pumping in Neutral Atom Arrays", "comment": "8 pages + 17 pages; comments are welcome", "summary": "We propose a dissipative protocol for preparing nonequilibrium steady-state\nentanglement in neutral atom arrays within a Floquet-Lindblad framework.\nStabilizer pumping is implemented through noninstantaneous kicks, where each\nperiod consists of a short resonant laser pulse followed by a detuned strong\n$\\pi$ pulse that couples the atomic ground state to a Rydberg state. This\nscheme is intrinsically fast and robust against the Doppler shifts and\ninteratomic spatial fluctuations, as adiabatic requirements on the laser field\nare avoided. As such the engineered dissipation channels induce a fast decay\nrate, dramatically accelerating convergence toward the desired steady states.\nWe show that this approach is inherently scalable and enables high-fidelity\npreparation of arbitrary multipartite graph states in the neutral atom array at\nzero and finite temperatures. Our study not only facilitates the preparation of\nresource states for measurement-based quantum computation but also provides a\npassive error-correction mechanism in the undergoing computation.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5728Floquet-Lindblad\u6846\u67b6\u4e0b\uff0c\u901a\u8fc7\u8017\u6563\u534f\u8bae\u5236\u5907\u4e2d\u6027\u539f\u5b50\u9635\u5217\u4e2d\u975e\u5e73\u8861\u7a33\u6001\u7ea0\u7f20\u7684\u65b9\u6cd5\u3002", "motivation": "\u5236\u5907\u975e\u5e73\u8861\u7a33\u6001\u7ea0\u7f20\uff0c\u4e3a\u6d4b\u91cf\u57fa\u91cf\u5b50\u8ba1\u7b97\u63d0\u4f9b\u8d44\u6e90\u72b6\u6001\uff0c\u5e76\u5b9e\u73b0\u88ab\u52a8\u7ea0\u9519\u3002", "method": "\u901a\u8fc7\u975e\u77ac\u65f6\u8bf1\u5bfc\u7684\u7a33\u5b9a\u5668\u6cf5\u6d66\uff0c\u5305\u62ec\u5171\u632f\u6fc0\u5149\u8109\u51b2\u548c\u5931\u8c10\u7684$\\\text{pi}$\u8109\u51b2\uff0c\u5728Floquet-Lindblad\u6846\u67b6\u4e0b\u5b9e\u73b0\u3002", "result": "\u6210\u529f\u5236\u5907\u4e86\u4efb\u610f\u591a\u65b9\u56fe\u6001\uff0c\u5177\u6709\u9ad8\u4fdd\u771f\u5ea6\uff0c\u4e14\u8be5\u65b9\u6848\u5177\u6709\u53ef\u6269\u5c55\u6027\uff0c\u5bf9\u591a\u666e\u52d2\u9891\u79fb\u548c\u539f\u5b50\u7a7a\u95f4\u6da8\u843d\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u8017\u6563\u65b9\u6848\u53ef\u6269\u5c55\u4e14\u9c81\u68d2\uff0c\u80fd\u591f\u9ad8\u6548\u5236\u5907\u7528\u4e8e\u91cf\u5b50\u8ba1\u7b97\u7684\u8d44\u6e90\u72b6\u6001\uff0c\u5e76\u63d0\u4f9b\u6f5c\u5728\u7684\u7ea0\u9519\u673a\u5236\u3002"}}
{"id": "2509.18116", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18116", "abs": "https://arxiv.org/abs/2509.18116", "authors": ["Nathan Egbuna", "Saatvik Gaur", "Sunishchal Dev", "Ashwinee Panda", "Maheep Chaudhary"], "title": "Amortized Latent Steering: Low-Cost Alternative to Test-Time Optimization", "comment": null, "summary": "Test-time optimization remains impractical at scale due to prohibitive\ninference costs\\textemdash techniques like iterative refinement and multi-step\nverification can require $10$--$100\\times$ more compute per query than standard\ndecoding. Latent space test-time optimization methods like LatentSeek offer a\nmore direct approach by steering hidden representations, but still demand\nexpensive per-query optimization loops with multiple backward passes. We\npropose Amortized Latent Steering (ALS), which collapses this iterative\noptimization into a single offline-computed vector applied at constant cost\nduring inference. ALS computes the mean difference between hidden states from\nsuccessful versus unsuccessful generations, then uses this direction to\ncalibrate the model's hidden representations: when decoding drifts away from\nthe success manifold, ALS nudges activations back toward it. Across GSM8K and\nMATH-$500$ benchmarks, ALS achieves $2$--$5\\times$ speedup over iterative\nmethods while matching or surpassing greedy Chain-of-Thought (CoT) and\nSelf-Consistency baselines, yielding up to 101\\% improvement in\nefficiency--accuracy trade-off. These results show that much of latent\noptimization's benefit can be captured offline, making sophisticated reasoning\ntechniques viable for production deployment. Code is available\nat~\\href{https://anonymous.4open.science/r/steering-17F2}{https://anonymous.4open.science/r/steering-17F2}", "AI": {"tldr": "ALS \u662f\u4e00\u79cd\u901a\u8fc7\u79bb\u7ebf\u8ba1\u7b97\u7684\u5411\u91cf\u5728\u63a8\u7406\u65f6\u6821\u51c6\u6a21\u578b\u9690\u85cf\u8868\u793a\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u4e0d\u589e\u52a0\u989d\u5916\u63a8\u7406\u6210\u672c\u7684\u60c5\u51b5\u4e0b\u63d0\u9ad8\u6a21\u578b\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u6d4b\u8bd5\u65f6\u4f18\u5316\u65b9\u6cd5\uff08\u5982\u8fed\u4ee3\u4f18\u5316\u548c\u591a\u6b65\u9a8c\u8bc1\uff09\u63a8\u7406\u6210\u672c\u9ad8\u6602\uff0c\u800c\u6f5c\u7a7a\u95f4\u6d4b\u8bd5\u65f6\u4f18\u5316\u65b9\u6cd5\uff08\u5982 LatentSeek\uff09\u867d\u7136\u66f4\u76f4\u63a5\uff0c\u4f46\u4ecd\u9700\u8981\u6602\u8d35\u7684\u6bcf\u67e5\u8be2\u4f18\u5316\u5faa\u73af\u3002", "method": "ALS \u8ba1\u7b97\u6210\u529f\u4e0e\u4e0d\u6210\u529f\u751f\u6210\u4e4b\u95f4\u7684\u9690\u85cf\u72b6\u6001\u7684\u5e73\u5747\u5dee\u503c\uff0c\u7136\u540e\u5229\u7528\u8fd9\u4e2a\u65b9\u5411\u6765\u6821\u51c6\u6a21\u578b\u7684\u9690\u85cf\u8868\u793a\uff0c\u5c06\u504f\u79bb\u6210\u529f\u6d41\u5f62\uff08success manifold\uff09\u7684\u6fc0\u6d3b\u5f15\u5bfc\u56de\u8be5\u6d41\u5f62\u3002", "result": "\u5728 GSM8K \u548c MATH-500 \u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cALS \u6bd4\u8fed\u4ee3\u65b9\u6cd5\u5feb 2-5 \u500d\uff0c\u540c\u65f6\u5728\u6548\u7387-\u51c6\u786e\u6027\u6743\u8861\u65b9\u9762\u53d6\u5f97\u4e86\u9ad8\u8fbe 101% \u7684\u6539\u8fdb\uff0c\u4e0e\u8d2a\u5a6a\u7684\u601d\u7ef4\u94fe\uff08CoT\uff09\u548c\u81ea\u6d3d\u6027\uff08Self-Consistency\uff09\u57fa\u7ebf\u76f8\u5339\u914d\u6216\u8d85\u8d8a\u3002", "conclusion": "ALS \u8868\u660e\uff0c\u6f5c\u7a7a\u95f4\u4f18\u5316\u7684\u8bb8\u591a\u597d\u5904\u53ef\u4ee5\u5728\u79bb\u7ebf\u5b8c\u6210\uff0c\u4f7f\u5f97\u590d\u6742\u7684\u63a8\u7406\u6280\u672f\u53ef\u4ee5\u6295\u5165\u751f\u4ea7\u4f7f\u7528\u3002"}}
{"id": "2509.18221", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18221", "abs": "https://arxiv.org/abs/2509.18221", "authors": ["Dingxin Lu", "Shurui Wu", "Xinyi Huang"], "title": "Multimodal Health Risk Prediction System for Chronic Diseases via Vision-Language Fusion and Large Language Models", "comment": null, "summary": "With the rising global burden of chronic diseases and the multimodal and\nheterogeneous clinical data (medical imaging, free-text recordings, wearable\nsensor streams, etc.), there is an urgent need for a unified multimodal AI\nframework that can proactively predict individual health risks. We propose\nVL-RiskFormer, a hierarchical stacked visual-language multimodal Transformer\nwith a large language model (LLM) inference head embedded in its top layer. The\nsystem builds on the dual-stream architecture of existing visual-linguistic\nmodels (e.g., PaLM-E, LLaVA) with four key innovations: (i) pre-training with\ncross-modal comparison and fine-grained alignment of radiological images,\nfundus maps, and wearable device photos with corresponding clinical narratives\nusing momentum update encoders and debiased InfoNCE losses; (ii) a time fusion\nblock that integrates irregular visit sequences into the causal Transformer\ndecoder through adaptive time interval position coding; (iii) a disease\nontology map adapter that injects ICD-10 codes into visual and textual channels\nin layers and infers comorbid patterns with the help of a graph attention\nmechanism. On the MIMIC-IV longitudinal cohort, VL-RiskFormer achieved an\naverage AUROC of 0.90 with an expected calibration error of 2.7 percent.", "AI": {"tldr": "VL-RiskFormer\u662f\u4e00\u4e2a\u5305\u542b\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u5934\u7684\u5206\u5c42\u5806\u53e0\u89c6\u89c9-\u8bed\u8a00\u591a\u6a21\u6001Transformer\uff0c\u7528\u4e8e\u9884\u6d4b\u4e2a\u4f53\u5065\u5eb7\u98ce\u9669\u3002", "motivation": "\u968f\u7740\u6162\u6027\u75c5\u8d1f\u62c5\u7684\u589e\u52a0\u548c\u4e34\u5e8a\u6570\u636e\u7684\u591a\u6a21\u6001\u5f02\u6784\u6027\uff0c\u8feb\u5207\u9700\u8981\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u6a21\u6001\u4eba\u5de5\u667a\u80fd\u6846\u67b6\u6765\u9884\u6d4b\u4e2a\u4f53\u5065\u5eb7\u98ce\u9669\u3002", "method": "VL-RiskFormer\u5728\u53cc\u6d41\u67b6\u6784\u7684\u57fa\u7840\u4e0a\u8fdb\u884c\u4e86\u56db\u9879\u5173\u952e\u521b\u65b0\uff1a(i) \u4f7f\u7528\u52a8\u91cf\u66f4\u65b0\u7f16\u7801\u5668\u548c\u53bb\u504f\u7f6eInfoNCE\u635f\u5931\u5bf9\u653e\u5c04\u56fe\u50cf\u3001\u773c\u5e95\u56fe\u548c\u53ef\u7a7f\u6234\u8bbe\u5907\u7167\u7247\u53ca\u5176\u4e34\u5e8a\u53d9\u8ff0\u8fdb\u884c\u8de8\u6a21\u6001\u6bd4\u8f83\u548c\u7ec6\u7c92\u5ea6\u5bf9\u9f50\u9884\u8bad\u7ec3\uff1b(ii) \u5f15\u5165\u65f6\u95f4\u878d\u5408\u5757\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u65f6\u95f4\u95f4\u9694\u4f4d\u7f6e\u7f16\u7801\u5c06\u4e0d\u89c4\u5219\u7684\u5c31\u8bca\u5e8f\u5217\u6574\u5408\u5230\u56e0\u679cTransformer\u89e3\u7801\u5668\u4e2d\uff1b(iii) \u96c6\u6210\u75be\u75c5\u672c\u4f53\u56fe\u9002\u914d\u5668\uff0c\u5c06ICD-10\u4ee3\u7801\u6ce8\u5165\u89c6\u89c9\u548c\u6587\u672c\u901a\u9053\uff0c\u5e76\u501f\u52a9\u56fe\u6ce8\u610f\u529b\u673a\u5236\u63a8\u65ad\u5171\u75c5\u6a21\u5f0f\u3002", "result": "\u5728MIMIC-IV\u7eb5\u5411\u961f\u5217\u4e0a\uff0cVL-RiskFormer\u5b9e\u73b0\u4e860.90\u7684\u5e73\u5747AUROC\u548c2.7%\u7684\u9884\u671f\u6821\u51c6\u8bef\u5dee\u3002", "conclusion": "VL-RiskFormer\u5728\u9884\u6d4b\u4e2a\u4f53\u5065\u5eb7\u98ce\u9669\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u8bc1\u660e\u4e86\u5176\u4f5c\u4e3a\u7edf\u4e00\u591a\u6a21\u6001AI\u6846\u67b6\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.18377", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18377", "abs": "https://arxiv.org/abs/2509.18377", "authors": ["Xinlu He", "Yiwen Guan", "Badrivishal Paurana", "Zilin Dai", "Jacob Whitehill"], "title": "Interactive Real-Time Speaker Diarization Correction with Human Feedback", "comment": null, "summary": "Most automatic speech processing systems operate in \"open loop\" mode without\nuser feedback about who said what; yet, human-in-the-loop workflows can\npotentially enable higher accuracy. We propose an LLM-assisted speaker\ndiarization correction system that lets users fix speaker attribution errors in\nreal time. The pipeline performs streaming ASR and diarization, uses an LLM to\ndeliver concise summaries to the users, and accepts brief verbal feedback that\nis immediately incorporated without disrupting interactions. Moreover, we\ndevelop techniques to make the workflow more effective: First, a\nsplit-when-merged (SWM) technique detects and splits multi-speaker segments\nthat the ASR erroneously attributes to just a single speaker. Second, online\nspeaker enrollments are collected based on users' diarization corrections, thus\nhelping to prevent speaker diarization errors from occurring in the future.\nLLM-driven simulations on the AMI test set indicate that our system\nsubstantially reduces DER by 9.92% and speaker confusion error by 44.23%. We\nfurther analyze correction efficacy under different settings, including summary\nvs full transcript display, the number of online enrollments limitation, and\ncorrection frequency.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7531\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8f85\u52a9\u7684\u8bf4\u8bdd\u4eba\u65e5\u5fd7\u7ea0\u9519\u7cfb\u7edf\uff0c\u5229\u7528\u7528\u6237\u5b9e\u65f6\u53cd\u9988\u6765\u63d0\u9ad8\u8bed\u97f3\u5904\u7406\u7cfb\u7edf\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u8bed\u97f3\u5904\u7406\u7cfb\u7edf\u5728\u7f3a\u4e4f\u7528\u6237\u53cd\u9988\u7684\u60c5\u51b5\u4e0b\u8fd0\u884c\uff0c\u800c\u5f15\u5165\u7528\u6237\u53cd\u9988\u6709\u671b\u63d0\u9ad8\u51c6\u786e\u6027\u3002", "method": "\u7cfb\u7edf\u7ed3\u5408\u4e86\u6d41\u5f0f\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u548c\u8bf4\u8bdd\u4eba\u65e5\u5fd7\uff0c\u5229\u7528LLM\u751f\u6210\u7b80\u6d01\u7684\u6458\u8981\u4f9b\u7528\u6237\u5ba1\u67e5\uff0c\u5e76\u63a5\u53d7\u7528\u6237\u7684\u7b80\u77ed\u8bed\u97f3\u53cd\u9988\u3002\u8be5\u7cfb\u7edf\u8fd8\u5f15\u5165\u4e86\u201c\u5206\u5272\u540e\u5408\u5e76\u201d\uff08SWM\uff09\u6280\u672f\u6765\u4fee\u6b63\u591a\u8bf4\u8bdd\u4eba\u7247\u6bb5\u7684\u9519\u8bef\u5f52\u5c5e\uff0c\u5e76\u901a\u8fc7\u5728\u7ebf\u8bf4\u8bdd\u4eba\u6ce8\u518c\u6765\u6536\u96c6\u7528\u6237\u7ea0\u9519\u4fe1\u606f\uff0c\u4ee5\u9884\u9632\u672a\u6765\u9519\u8bef\u3002", "result": "\u5728AMI\u6d4b\u8bd5\u96c6\u4e0a\u7684LLM\u9a71\u52a8\u6a21\u62df\u663e\u793a\uff0c\u8be5\u7cfb\u7edf\u5c06\u8bf4\u8bdd\u4eba\u65e5\u5fd7\u9519\u8bef\u7387\uff08DER\uff09\u663e\u8457\u964d\u4f4e\u4e869.92%\uff0c\u5c06\u8bf4\u8bdd\u4eba\u6df7\u6dc6\u9519\u8bef\u964d\u4f4e\u4e8644.23%\u3002\u7814\u7a76\u8fd8\u5206\u6790\u4e86\u4e0d\u540c\u8bbe\u7f6e\uff08\u5982\u6458\u8981\u4e0e\u5b8c\u6574\u6587\u672c\u663e\u793a\u3001\u5728\u7ebf\u6ce8\u518c\u6570\u91cf\u9650\u5236\u3001\u7ea0\u9519\u9891\u7387\uff09\u5bf9\u7ea0\u9519\u6548\u679c\u7684\u5f71\u54cd\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684LLM\u8f85\u52a9\u7684\u8bf4\u8bdd\u4eba\u65e5\u5fd7\u7ea0\u9519\u7cfb\u7edf\u80fd\u6709\u6548\u63d0\u9ad8\u8bed\u97f3\u5904\u7406\u7684\u51c6\u786e\u6027\uff0c\u901a\u8fc7\u7528\u6237\u5b9e\u65f6\u53cd\u9988\u548c\u521b\u65b0\u7684\u6280\u672f\u624b\u6bb5\u89e3\u51b3\u4e86\u73b0\u6709\u7cfb\u7edf\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2509.19079", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.19079", "abs": "https://arxiv.org/abs/2509.19079", "authors": ["Samuel Chamoun", "Christian McDowell", "Robin Buchanan", "Kevin Chan", "Eric Graves", "Yin Sun"], "title": "MAPPO for Edge Server Monitoring", "comment": "6 pages, 4 figures. Accepted to IEEE MILCOM 2025", "summary": "In this paper, we consider a goal-oriented communication problem for edge\nserver monitoring, where jobs arrive intermittently at multiple dispatchers and\nmust be assigned to shared edge servers with finite queues and time-varying\navailability. Accurate knowledge of server status is critical for sustaining\nhigh throughput, yet remains challenging under dynamic workloads and partial\nobservability. To address this challenge, each dispatcher maintains server\nknowledge through two complementary mechanisms: (i) active status queries that\nprovide instantaneous updates at a communication cost, and (ii) job execution\nfeedback that reveals server conditions opportunistically. We formulate a\ncooperative multi-agent distributed decision-making problem in which\ndispatchers jointly optimize query scheduling to balance throughput against\ncommunication overhead. To solve this problem, we propose a Multi-Agent\nProximal Policy Optimization (MAPPO)-based algorithm that leverages centralized\ntraining with decentralized execution (CTDE) to learn distributed\nquery-and-dispatch policies under partial and stale observations. Numerical\nevaluations show that MAPPO achieves superior throughput-cost tradeoffs and\nsignificantly outperforms baseline strategies, achieving on average a 30%\nimprovement over the closest baseline.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMAPPO\u7684\u7b97\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u67e5\u8be2\u8c03\u5ea6\u6765\u89e3\u51b3\u8fb9\u7f18\u670d\u52a1\u5668\u76d1\u63a7\u4e2d\u7684\u901a\u4fe1\u95ee\u9898\uff0c\u4ee5\u5e73\u8861\u541e\u5410\u91cf\u548c\u901a\u4fe1\u5f00\u9500\u3002", "motivation": "\u5728\u8fb9\u7f18\u670d\u52a1\u5668\u76d1\u63a7\u4e2d\uff0c\u51c6\u786e\u4e86\u89e3\u670d\u52a1\u5668\u72b6\u6001\u5bf9\u4e8e\u7ef4\u6301\u9ad8\u541e\u5410\u91cf\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5728\u52a8\u6001\u5de5\u4f5c\u8d1f\u8f7d\u548c\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u4e0b\u96be\u4ee5\u5b9e\u73b0\u3002", "method": "\u901a\u8fc7\u4e3b\u52a8\u72b6\u6001\u67e5\u8be2\u548c\u4f5c\u4e1a\u6267\u884c\u53cd\u9988\u4e24\u79cd\u673a\u5236\u6765\u7ef4\u62a4\u670d\u52a1\u5668\u77e5\u8bc6\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08MAPPO\uff09\u7684\u7b97\u6cd5\uff0c\u91c7\u7528\u96c6\u4e2d\u8bad\u7ec3\u5206\u5e03\u5f0f\u6267\u884c\uff08CTDE\uff09\u7684\u65b9\u6cd5\u6765\u5b66\u4e60\u5206\u5e03\u5f0f\u67e5\u8be2\u548c\u8c03\u5ea6\u7b56\u7565\u3002", "result": "MAPPO\u7b97\u6cd5\u5728\u541e\u5410\u91cf-\u6210\u672c\u6743\u8861\u65b9\u9762\u8868\u73b0\u4f18\u8d8a\uff0c\u5e73\u5747\u6bd4\u6700\u8fd1\u7684\u57fa\u7ebf\u63d0\u9ad8\u4e8630%\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684MAPPO\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u8fb9\u7f18\u670d\u52a1\u5668\u76d1\u63a7\u4e2d\u7684\u901a\u4fe1\u95ee\u9898\uff0c\u5e76\u663e\u8457\u63d0\u9ad8\u7cfb\u7edf\u6027\u80fd\u3002"}}
{"id": "2509.18678", "categories": ["cond-mat.mtrl-sci", "math-ph", "math.MP"], "pdf": "https://arxiv.org/pdf/2509.18678", "abs": "https://arxiv.org/abs/2509.18678", "authors": ["Christian Tantardini"], "title": "A basis-free, octonionic criterion for Weyl points in solids", "comment": null, "summary": "Conventional diagnostics of Weyl points -- Berry flux on small spheres and $k\n\\cdot p$ linearization -- are topologically sound but depend on user choices\n(sphere center and radius, gauge smoothing, charting, and local frame\ntransport) that introduce algorithmic arbitrariness in first-principles\nworkflows. We propose a \\emph{local, basis-free} criterion built from the\noctonionic structure on $\\mathbb{R}^7$. From a smooth two-band projector we\nconstruct a unit octonion field and its octonionic connection; contracting\nthree directional derivatives with the $\\mathrm{G}_2$--invariant three-form\nyields a pseudoscalar density whose sign equals the Weyl chirality. A vanishing\noctonionic associator at leading order certifies that the local problem closes\ninside an associative (quaternionic) three-plane, while a nonzero density\nidentifies a Weyl point. The construction is invariant under $\\mathrm{SU}(2)$\ngauge changes of the two-band subspace and under $\\mathrm{G}_2$ rotations of\nits completion, and it eliminates enclosing surfaces and gauge-seam choices. In\nthe linear regime we prove equivalence with the conventional diagnostics (Chern\ncharge on a small sphere and $\\mathrm{sgn}\\det v$). We outline a practical\nalgorithm compatible with Wannier tight-binding Hamiltonians and provide\nself-consistency checks based on stencil refinement and the associator norm.\nThe \\emph{octonionic Weyl point criterion} streamlines chirality assignment in\nhigh-throughput searches and offers an intrinsic warning signal in the presence\nof band entanglement or proximity to multi-fold touchings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u516b\u5143\u6570\u7ed3\u6784\u7684\u5c40\u90e8\u3001\u65e0\u57fa\u5e95\u7684Weyl\u70b9\u8bca\u65ad\u65b0\u65b9\u6cd5\uff0c\u6d88\u9664\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e2d\u7684\u7b97\u6cd5\u4efb\u610f\u6027\uff0c\u5e76\u7b80\u5316\u4e86\u9ad8\u901a\u91cf\u641c\u7d22\u6d41\u7a0b\u3002", "motivation": "\u4f20\u7edfWeyl\u70b9\u8bca\u65ad\u65b9\u6cd5\uff08\u5982\u57fa\u4e8eBerry\u901a\u91cf\u548ck\u00b7p\u7ebf\u6027\u5316\uff09\u5b58\u5728\u7b97\u6cd5\u4efb\u610f\u6027\u95ee\u9898\uff0c\u4f9d\u8d56\u4e8e\u7528\u6237\u9009\u62e9\u7684\u53c2\u6570\uff0c\u4e0d\u9002\u7528\u4e8e\u7b2c\u4e00\u6027\u539f\u7406\u8ba1\u7b97\u5de5\u4f5c\u6d41\u3002\u9700\u8981\u4e00\u79cd\u66f4\u7a33\u5065\u3001\u5185\u5728\u5316\u7684\u8bca\u65ad\u65b9\u6cd5\u3002", "method": "\u5229\u7528$\\mathbb{R}^7$\u4e0a\u7684\u516b\u5143\u6570\u7ed3\u6784\uff0c\u4ece\u4e00\u4e2a\u5e73\u6ed1\u7684\u4e24\u5e26\u6295\u5f71\u7b97\u7b26\u6784\u5efa\u4e00\u4e2a\u5355\u4f4d\u516b\u5143\u6570\u573a\u53ca\u5176\u516b\u5143\u6570\u8054\u7edc\u3002\u901a\u8fc7\u5c06\u4e09\u4e2a\u65b9\u5411\u5bfc\u6570\u4e0eG2\u4e0d\u53d8\u7684\u4e09\u5f62\u5f0f\u6536\u7f29\uff0c\u5f97\u5230\u4e00\u4e2a\u8d5d\u6807\u91cf\u5bc6\u5ea6\uff0c\u5176\u7b26\u53f7\u5373\u4e3aWeyl\u7684\u091a\u093f\u0930\u0948\u0932\u093f\u091f\u0940\u3002\u901a\u8fc7\u68c0\u67e5\u516b\u5143\u6570\u5173\u8054\u5b50\u7684\u6d88\u5931\u6765\u786e\u5b9a\u5c40\u90e8\u95ee\u9898\u5728\u5173\u8054\uff08\u56db\u5143\u6570\uff09\u4e09\u5e73\u9762\u5185\u95ed\u5408\u3002", "result": "\u8be5\u65b9\u6cd5\u6d88\u9664\u4e86\u5bf9\u5c01\u95ed\u66f2\u9762\u548c\u89c4\u8303\u7f1d\u9009\u62e9\u7684\u4f9d\u8d56\uff0c\u5e76\u4e14\u5728\u4e0eChern\u7535\u8377\u548csgn(det v)\u7684\u8bca\u65ad\u65b9\u6cd5\u7b49\u4ef7\u3002\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u4e0e\u4f20\u7edf\u8bca\u65ad\u65b9\u6cd5\u5728\u67d0\u4e9b\u6761\u4ef6\u4e0b\u7b49\u4ef7\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u7b97\u6cd5\uff0c\u517c\u5bb9Wannier\u7d27\u675f\u7f1a\u54c8\u5bc6\u987f\u91cf\u3002", "conclusion": "\u63d0\u51fa\u7684\u516b\u5143\u6570Weyl\u70b9\u5224\u636e\u662f\u4e00\u79cd\u5c40\u90e8\u3001\u65e0\u57fa\u5e95\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u6d88\u9664\u4f20\u7edf\u65b9\u6cd5\u7684\u7b97\u6cd5\u4efb\u610f\u6027\uff0c\u7b80\u5316\u9ad8\u901a\u91cf\u641c\u7d22\uff0c\u5e76\u5728\u5b58\u5728\u80fd\u5e26\u7ea0\u7f20\u6216\u63a5\u8fd1\u591a\u91cd\u7b80\u5e76\u70b9\u65f6\u63d0\u4f9b\u5185\u5728\u7684\u8b66\u793a\u4fe1\u53f7\u3002"}}
{"id": "2509.19235", "categories": ["eess.SP", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2509.19235", "abs": "https://arxiv.org/abs/2509.19235", "authors": ["Wamberto J. L. Queiroz", "Hugerles S. Silva", "Higo T. P. Silva", "Alexandros-Apostolos A. Boulogeorgos"], "title": "On the Performance of THz Wireless Systems over $\u03b1$-$\\mathcal{F}$ Channels with Beam Misalignment and Mobility", "comment": null, "summary": "This paper investigates the performance of terahertz~(THz) wireless systems\nover the $\\alpha$-$\\mathcal{F}$ fading channels with beam misalignment and\nmobility. New expressions are derived for the probability density, cumulative\ndistribution, and moment generating functions, as well as higher-order moments\nof the instantaneous signal-to-noise ratio. Building upon the aforementioned\nexpressions, we extract novel formulas for the outage probability, symbol error\nprobability, and average channel capacity. Asymptotic metrics are also deduced,\nwhich provide useful insights. Monte Carlo simulations results are presented to\nsupport the derived analytical framework.", "AI": {"tldr": "\u672c\u8bba\u6587\u7814\u7a76\u4e86\u592a\u8d6b\u5179\uff08THz\uff09\u65e0\u7ebf\u7cfb\u7edf\u5728\u8003\u8651\u6ce2\u675f\u4e0d\u5bf9\u51c6\u548c\u79fb\u52a8\u6027\u56e0\u7d20\u4e0b\uff0c\u4e8e \u03b1-F \u8870\u843d\u4fe1\u9053\u4e0a\u7684\u6027\u80fd\u8868\u73b0\u3002", "motivation": "\u7814\u7a76\u592a\u8d6b\u5179\uff08THz\uff09\u65e0\u7ebf\u7cfb\u7edf\u5728\u5b58\u5728\u6ce2\u675f\u4e0d\u5bf9\u51c6\u548c\u79fb\u52a8\u6027\u56e0\u7d20\u4e0b\u7684\u6027\u80fd\uff0c\u8be5\u56e0\u7d20\u5728 \u03b1-F \u8870\u843d\u4fe1\u9053\u4e0a\u5c24\u4e3a\u91cd\u8981\u3002", "method": "\u63a8\u5bfc\u4e86\u77ac\u65f6\u4fe1\u566a\u6bd4\u7684\u6982\u7387\u5bc6\u5ea6\u3001\u7d2f\u79ef\u5206\u5e03\u3001\u77e9\u751f\u6210\u51fd\u6570\u4ee5\u53ca\u9ad8\u9636\u77e9\u7684\u65b0\u8868\u8fbe\u5f0f\u3002\u57fa\u4e8e\u8fd9\u4e9b\u8868\u8fbe\u5f0f\uff0c\u63a8\u5bfc\u4e86\u65b0\u7684\u4e2d\u65ad\u6982\u7387\u3001\u7b26\u53f7\u9519\u8bef\u6982\u7387\u548c\u5e73\u5747\u4fe1\u9053\u5bb9\u91cf\u7684\u8ba1\u7b97\u516c\u5f0f\uff0c\u5e76\u63a8\u5bfc\u51fa\u6e10\u8fd1\u5ea6\u91cf\u3002", "result": "\u63a8\u5bfc\u4e86\u77ac\u65f6\u4fe1\u566a\u6bd4\u7684\u6982\u7387\u5bc6\u5ea6\u3001\u7d2f\u79ef\u5206\u5e03\u3001\u77e9\u751f\u6210\u51fd\u6570\u4ee5\u53ca\u9ad8\u9636\u77e9\u7684\u65b0\u8868\u8fbe\u5f0f\uff0c\u5e76\u57fa\u4e8e\u8fd9\u4e9b\u8868\u8fbe\u5f0f\u63a8\u5bfc\u4e86\u65b0\u7684\u4e2d\u65ad\u6982\u7387\u3001\u7b26\u53f7\u9519\u8bef\u6982\u7387\u548c\u5e73\u5747\u4fe1\u9053\u5bb9\u91cf\u7684\u8ba1\u7b97\u516c\u5f0f\uff0c\u540c\u65f6\u63a8\u5bfc\u4e86\u6e10\u8fd1\u5ea6\u91cf\u3002", "conclusion": "\u901a\u8fc7\u8499\u7279\u5361\u6d1b\u6a21\u62df\u9a8c\u8bc1\u4e86\u6240\u63a8\u5bfc\u7684\u5206\u6790\u6846\u67b6\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2509.18506", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18506", "abs": "https://arxiv.org/abs/2509.18506", "authors": ["Siyuan Yu", "Congkai Shen", "Yufei Xi", "James Dallas", "Michael Thompson", "John Subosits", "Hiroshi Yasuda", "Tulga Ersal"], "title": "Spatial Envelope MPC: High Performance Driving without a Reference", "comment": null, "summary": "This paper presents a novel envelope based model predictive control (MPC)\nframework designed to enable autonomous vehicles to handle high performance\ndriving across a wide range of scenarios without a predefined reference. In\nhigh performance autonomous driving, safe operation at the vehicle's dynamic\nlimits requires a real time planning and control framework capable of\naccounting for key vehicle dynamics and environmental constraints when\nfollowing a predefined reference trajectory is suboptimal or even infeasible.\nState of the art planning and control frameworks, however, are predominantly\nreference based, which limits their performance in such situations. To address\nthis gap, this work first introduces a computationally efficient vehicle\ndynamics model tailored for optimization based control and a continuously\ndifferentiable mathematical formulation that accurately captures the entire\ndrivable envelope. This novel model and formulation allow for the direct\nintegration of dynamic feasibility and safety constraints into a unified\nplanning and control framework, thereby removing the necessity for predefined\nreferences. The challenge of envelope planning, which refers to maximally\napproximating the safe drivable area, is tackled by combining reinforcement\nlearning with optimization techniques. The framework is validated through both\nsimulations and real world experiments, demonstrating its high performance\nacross a variety of tasks, including racing, emergency collision avoidance and\noff road navigation. These results highlight the framework's scalability and\nbroad applicability across a diverse set of scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u5305\u7edc\u7ebf\u7684\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u6846\u67b6\uff0c\u4f7f\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u80fd\u591f\u5728\u6ca1\u6709\u9884\u5b9a\u4e49\u53c2\u8003\u7684\u60c5\u51b5\u4e0b\u5904\u7406\u5404\u79cd\u573a\u666f\u4e0b\u7684\u9ad8\u6027\u80fd\u9a7e\u9a76\u3002", "motivation": "\u5728\u9ad8\u6027\u80fd\u81ea\u52a8\u9a7e\u9a76\u4e2d\uff0c\u8f66\u8f86\u52a8\u6001\u6781\u9650\u4e0b\u7684\u5b89\u5168\u8fd0\u884c\u9700\u8981\u4e00\u4e2a\u5b9e\u65f6\u89c4\u5212\u548c\u63a7\u5236\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u5728\u9884\u5b9a\u4e49\u53c2\u8003\u8f68\u8ff9\u6700\u4f18\u6216\u4e0d\u53ef\u884c\u65f6\uff0c\u8003\u8651\u5173\u952e\u8f66\u8f86\u52a8\u529b\u5b66\u548c\u73af\u5883\u7ea6\u675f\u3002\u73b0\u6709\u7684\u6846\u67b6\u4e3b\u8981\u662f\u57fa\u4e8e\u53c2\u8003\u7684\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u5728\u6b64\u7c7b\u60c5\u51b5\u4e0b\u7684\u6027\u80fd\u3002", "method": "\u672c\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u4e2a\u8ba1\u7b97\u9ad8\u6548\u7684\u8f66\u8f86\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u8fde\u7eed\u53ef\u5fae\u7684\u6570\u5b66\u516c\u5f0f\uff0c\u7528\u4e8e\u7cbe\u786e\u6355\u6349\u6574\u4e2a\u53ef\u9a7e\u9a76\u5305\u7edc\u7ebf\u3002\u8be5\u6a21\u578b\u548c\u516c\u5f0f\u5141\u8bb8\u5c06\u52a8\u6001\u53ef\u884c\u6027\u548c\u5b89\u5168\u7ea6\u675f\u76f4\u63a5\u96c6\u6210\u5230\u7edf\u4e00\u7684\u89c4\u5212\u548c\u63a7\u5236\u6846\u67b6\u4e2d\uff0c\u4ece\u800c\u65e0\u9700\u9884\u5b9a\u4e49\u53c2\u8003\u3002\u901a\u8fc7\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u548c\u4f18\u5316\u6280\u672f\u6765\u89e3\u51b3\u5305\u7edc\u7ebf\u89c4\u5212\u95ee\u9898\uff08\u5373\u6700\u5927\u5316\u903c\u8fd1\u5b89\u5168\u53ef\u9a7e\u9a76\u533a\u57df\uff09\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u8d5b\u8f66\u3001\u7d27\u6025\u907f\u649e\u548c\u8d8a\u91ce\u5bfc\u822a\u7b49\u591a\u79cd\u4efb\u52a1\u4e2d\u7684\u9ad8\u6027\u80fd\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6846\u67b6\u5177\u6709\u9ad8\u52a8\u6001\u6027\u80fd\uff0c\u5e76\u4e14\u80fd\u591f\u5728\u6ca1\u6709\u9884\u5b9a\u4e49\u53c2\u8003\u7684\u60c5\u51b5\u4e0b\u5904\u7406\u5e7f\u6cdb\u7684\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u3002\u8be5\u6846\u67b6\u7684\u6709\u6548\u6027\u901a\u8fc7\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u5f97\u5230\u8bc1\u660e\uff0c\u5c55\u793a\u4e86\u5176\u8de8\u5404\u79cd\u4efb\u52a1\u7684\u53ef\u6269\u5c55\u6027\u548c\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2509.18189", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18189", "abs": "https://arxiv.org/abs/2509.18189", "authors": ["Daxiang Dong", "Mingming Zheng", "Dong Xu", "Bairong Zhuang", "Wenyu Zhang", "Chunhua Luo", "Haoran Wang", "Zijian Zhao", "Jie Li", "Yuxuan Li", "Hanjun Zhong", "Mengyue Liu", "Jieting Chen", "Shupeng Li", "Lun Tian", "Yaping Feng", "Xin Li", "Donggang Jiang", "Yong Chen", "Yehua Xu", "Duohao Qin", "Chen Feng", "Dan Wang", "Henghua Zhang", "Jingjing Ha", "Jinhui He", "Yanfeng Zhai", "Chengxin Zheng", "Jiayi Mao", "Jiacheng Chen", "Ruchang Yao", "Ziye Yuan", "Jianmin Wu", "Guangjun Xie", "Dou Shen"], "title": "Qianfan-VL: Domain-Enhanced Universal Vision-Language Models", "comment": "12 pages", "summary": "We present Qianfan-VL, a series of multimodal large language models ranging\nfrom 3B to 70B parameters, achieving state-of-the-art performance through\ninnovative domain enhancement techniques. Our approach employs multi-stage\nprogressive training and high-precision data synthesis pipelines, which prove\nto be critical technologies for enhancing domain-specific capabilities while\nmaintaining strong general performance. Qianfan-VL achieves comparable results\nto leading open-source models on general benchmarks, with state-of-the-art\nperformance on benchmarks such as CCBench, SEEDBench IMG, ScienceQA, and\nMMStar. The domain enhancement strategy delivers significant advantages in OCR\nand document understanding, validated on both public benchmarks (OCRBench 873,\nDocVQA 94.75%) and in-house evaluations. Notably, Qianfan-VL-8B and 70B\nvariants incorporate long chain-of-thought capabilities, demonstrating superior\nperformance on mathematical reasoning (MathVista 78.6%) and logical inference\ntasks. All models are trained entirely on Baidu's Kunlun P800 chips, validating\nthe capability of large-scale AI infrastructure to train SOTA-level multimodal\nmodels with over 90% scaling efficiency on 5000 chips for a single task. This\nwork establishes an effective methodology for developing domain-enhanced\nmultimodal models suitable for diverse enterprise deployment scenarios.", "AI": {"tldr": "Qianfan-VL\u662f\u4e00\u7cfb\u5217\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u53c2\u6570\u91cf\u4ece3B\u523070B\u4e0d\u7b49\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u9886\u57df\u589e\u5f3a\u6280\u672f\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728OCR\u548c\u6587\u6863\u7406\u89e3\u65b9\u9762\u8868\u73b0\u7a81\u51fa\uff0c\u5e76\u9a8c\u8bc1\u4e86\u767e\u5ea6AI\u57fa\u7840\u8bbe\u65bd\u7684\u6269\u5c55\u6548\u7387\u3002", "motivation": "\u5f00\u53d1\u80fd\u591f\u589e\u5f3a\u9886\u57df\u7279\u5b9a\u80fd\u529b\u5e76\u4fdd\u6301\u5f3a\u5927\u901a\u7528\u6027\u80fd\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u4ee5\u6ee1\u8db3\u591a\u6837\u5316\u7684\u4f01\u4e1a\u90e8\u7f72\u573a\u666f\u3002", "method": "\u91c7\u7528\u591a\u9636\u6bb5\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u548c\u9ad8\u7cbe\u5ea6\u6570\u636e\u5408\u6210\u6d41\u6c34\u7ebf\uff0c\u5e76\u5f15\u5165\u957f\u94fe\u601d\u7ef4\uff08long chain-of-thought\uff09\u80fd\u529b\uff0c\u5728\u767e\u5ea6\u7684\u6606\u4ed1800\u82af\u7247\u4e0a\u8fdb\u884c\u7aef\u5230\u7aef\u8bad\u7ec3\u3002", "result": "Qianfan-VL\u5728\u901a\u7528\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u53d6\u5f97\u4e86\u4e0e\u9886\u5148\u7684\u5f00\u6e90\u6a21\u578b\u76f8\u5f53\u7684\u7ed3\u679c\uff0c\u5728CCBench, SEEDBench IMG, ScienceQA\u548cMMStar\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u5728OCR\u548c\u6587\u6863\u7406\u89e3\u65b9\u9762\uff0cQianfan-VL\u5728OCRBench\u548cDocVQA\u4e0a\u83b7\u5f97\u4e86\u663e\u8457\u4f18\u52bf\u3002\u5176\u957f\u94fe\u601d\u7ef4\u53d8\u4f53\u5728\u6570\u5b66\u63a8\u7406\uff08MathVista 78.6%\uff09\u548c\u903b\u8f91\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\u3002\u540c\u65f6\uff0c\u57285000\u9897\u82af\u7247\u4e0a\u8bad\u7ec3\u5355\u4e2a\u4efb\u52a1\u5b9e\u73b0\u4e86\u8d85\u8fc790%\u7684\u6269\u5c55\u6548\u7387\u3002", "conclusion": "Qianfan-VL\u7cfb\u5217\u6a21\u578b\u5c55\u793a\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u6765\u5f00\u53d1\u9886\u57df\u589e\u5f3a\u7684\u591a\u6a21\u6001\u6a21\u578b\uff0c\u8fd9\u4e9b\u6a21\u578b\u5728\u901a\u7528\u548c\u7279\u5b9a\u9886\u57df\u4efb\u52a1\u4e0a\u5747\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u8bc1\u660e\u4e86\u5927\u89c4\u6a21AI\u57fa\u7840\u8bbe\u65bd\u5728\u8bad\u7ec3SOTA\u591a\u6a21\u6001\u6a21\u578b\u65b9\u9762\u7684\u80fd\u529b\u548c\u6548\u7387\u3002"}}
{"id": "2509.18423", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2509.18423", "abs": "https://arxiv.org/abs/2509.18423", "authors": ["Jiarui Liu", "Qiming Wu", "Joel E. Moore", "Hartmut Haeffner", "Christopher W. W\u00e4chtler"], "title": "Observation of synchronization between two quantum van der Pol oscillators in trapped ions", "comment": null, "summary": "Synchronization is a hallmark of nonlinear dynamics. It drives self-organized\nbehavior across systems ranging from astronomy to chemistry. Among the simplest\nsystems, the van der Pol oscillator captures the essence of limit-cycle\nbehavior and forms the basis for diverse physical and biological models. While\nmutual synchronization has long been established in classical systems, it has\nyet to be experimentally observed in quantum limit-cycle oscillators, despite a\ndecade of theoretical exploration. Here, we demonstrate synchronization between\ntwo quantum van der Pol oscillators using trapped ions. The synchronized state\nmanifests itself as a stable relative phase, while the individual oscillator\nphases remain inaccessible. In view of potential sensing applications, we\ninvestigate the response of the synchronized system to an external field. Our\nresults establish limit-cycle synchronization in the quantum regime and pave\nthe way toward exploring complex synchronized dynamics in larger networks,\nwhere persistence of quantum features remains an open question.", "AI": {"tldr": "\u672c\u7814\u7a76\u9996\u6b21\u5728\u5b9e\u9a8c\u4e2d\u5b9e\u73b0\u4e86\u4e24\u4e2a\u91cf\u5b50\u8303\u5fb7\u5821\u632f\u8361\u5668\u7684\u540c\u6b65\uff0c\u5e76\u63a2\u7d22\u4e86\u5176\u5bf9\u5916\u90e8\u573a\u7684\u54cd\u5e94\uff0c\u4e3a\u8fdb\u4e00\u6b65\u7814\u7a76\u91cf\u5b50\u540c\u6b65\u7f51\u7edc\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "motivation": "\u5728\u7ecf\u5178\u7cfb\u7edf\u4e2d\uff0c\u540c\u6b65\u73b0\u8c61\u5e7f\u6cdb\u5b58\u5728\u4e8e\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u4e2d\uff0c\u4f46\u5176\u5728\u91cf\u5b50\u6781\u9650\u73af\u632f\u8361\u5668\u4e2d\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4ecd\u6709\u5f85\u63a2\u7d22\u3002", "method": "\u5229\u7528\u56da\u7981\u79bb\u5b50\u5b9e\u73b0\u4e86\u4e24\u4e2a\u91cf\u5b50\u8303\u5fb7\u5821\u632f\u8361\u5668\u7684\u540c\u6b65\uff0c\u5e76\u89c2\u5bdf\u4e86\u540c\u6b65\u72b6\u6001\u4e0b\u7684\u7a33\u5b9a\u76f8\u5bf9\u76f8\u4f4d\uff0c\u540c\u65f6\u4e2a\u4f53\u76f8\u4f4d\u4fdd\u6301\u4e0d\u53ef\u53ca\u3002", "result": "\u6210\u529f\u5b9e\u73b0\u4e86\u4e24\u4e2a\u91cf\u5b50\u8303\u5fb7\u5821\u632f\u8361\u5668\u7684\u540c\u6b65\uff0c\u5e76\u89c2\u5bdf\u5230\u7a33\u5b9a\u7684\u76f8\u5bf9\u76f8\u4f4d\u3002\u7814\u7a76\u4e86\u540c\u6b65\u7cfb\u7edf\u5bf9\u5916\u90e8\u573a\u7684\u54cd\u5e94\u3002", "conclusion": "\u672c\u7814\u7a76\u9996\u6b21\u5728\u91cf\u5b50 regime \u4e0b\u5b9e\u73b0\u4e86\u6781\u9650\u73af\u540c\u6b65\uff0c\u5e76\u4e3a\u63a2\u7d22\u66f4\u5927\u89c4\u6a21\u91cf\u5b50\u540c\u6b65\u7f51\u7edc\u4e2d\u7684\u590d\u6742\u540c\u6b65\u52a8\u529b\u5b66\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u91cf\u5b50\u7279\u6027\u8fd9\u4e00\u5f00\u653e\u6027\u95ee\u9898\u3002"}}
{"id": "2509.18117", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18117", "abs": "https://arxiv.org/abs/2509.18117", "authors": ["Eric Petit", "Denis Ch\u00eane"], "title": "Robust and continuous machine learning of usage habits to adapt digital interfaces to user needs", "comment": "soumis {\\`a} la conf{\\'e}rence IHM 2025", "summary": "The paper presents a machine learning approach to design digital interfaces\nthat can dynamically adapt to different users and usage strategies. The\nalgorithm uses Bayesian statistics to model users' browsing behavior, focusing\non their habits rather than group preferences. It is distinguished by its\nonline incremental learning, allowing reliable predictions even with little\ndata and in the case of a changing environment. This inference method generates\na task model, providing a graphical representation of navigation with the usage\nstatistics of the current user. The algorithm learns new tasks while preserving\nprior knowledge. The theoretical framework is described, and simulations show\nthe effectiveness of the approach in stationary and non-stationary\nenvironments. In conclusion, this research paves the way for adaptive systems\nthat improve the user experience by helping them to better navigate and act on\ntheir interface.", "AI": {"tldr": "\u8be5\u65b9\u6cd5\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u8d1d\u53f6\u65af\u7edf\u8ba1\u548c\u5728\u7ebf\u589e\u91cf\u5b66\u4e60\u7684\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\uff0c\u7528\u4e8e\u8bbe\u8ba1\u80fd\u591f\u52a8\u6001\u9002\u5e94\u4e0d\u540c\u7528\u6237\u548c\u4f7f\u7528\u7b56\u7565\u7684\u6570\u5b57\u754c\u9762\u3002", "motivation": "\u8be5\u7814\u7a76\u7684\u52a8\u673a\u662f\u5f00\u53d1\u80fd\u591f\u6839\u636e\u7528\u6237\u7684\u4e2a\u4eba\u4e60\u60ef\u52a8\u6001\u8c03\u6574\u754c\u9762\u7684\u7cfb\u7edf\uff0c\u4ee5\u6539\u5584\u7528\u6237\u4f53\u9a8c\u3002", "method": "\u8be5\u7b97\u6cd5\u5229\u7528\u8d1d\u53f6\u65af\u7edf\u8ba1\u5bf9\u7528\u6237\u7684\u6d4f\u89c8\u884c\u4e3a\u8fdb\u884c\u5efa\u6a21\uff0c\u91cd\u70b9\u5173\u6ce8\u4e2a\u4eba\u4e60\u60ef\u800c\u975e\u7fa4\u4f53\u504f\u597d\u3002\u5b83\u91c7\u7528\u5728\u7ebf\u589e\u91cf\u5b66\u4e60\u65b9\u6cd5\uff0c\u5373\u4f7f\u6570\u636e\u91cf\u5c11\u6216\u73af\u5883\u53d8\u5316\u4e5f\u80fd\u8fdb\u884c\u53ef\u9760\u9884\u6d4b\u3002\u8be5\u65b9\u6cd5\u751f\u6210\u4e00\u4e2a\u4efb\u52a1\u6a21\u578b\uff0c\u4ee5\u56fe\u5f62\u65b9\u5f0f\u5c55\u793a\u5f53\u524d\u7528\u6237\u7684\u4f7f\u7528\u7edf\u8ba1\u6570\u636e\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u9759\u6001\u548c\u975e\u9759\u6001\u73af\u5883\u4e2d\u90fd\u6709\u6548\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5f00\u53d1\u80fd\u591f\u901a\u8fc7\u5e2e\u52a9\u7528\u6237\u66f4\u597d\u5730\u5bfc\u822a\u548c\u64cd\u4f5c\u754c\u9762\u6765\u6539\u5584\u7528\u6237\u4f53\u9a8c\u7684\u81ea\u9002\u5e94\u7cfb\u7edf\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2509.18226", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18226", "abs": "https://arxiv.org/abs/2509.18226", "authors": ["Yu Fu", "Linyue Cai", "Ruoyu Wu", "Yong Zhao"], "title": "From \"What to Eat?\" to Perfect Recipe: ChefMind's Chain-of-Exploration for Ambiguous User Intent in Recipe Recommendation", "comment": "5 pages, 3 figures, submitted to icassp 2026", "summary": "Personalized recipe recommendation faces challenges in handling fuzzy user\nintent, ensuring semantic accuracy, and providing sufficient detail coverage.\nWe propose ChefMind, a hybrid architecture combining Chain of Exploration\n(CoE), Knowledge Graph (KG), Retrieval-Augmented Generation (RAG), and a Large\nLanguage Model (LLM). CoE refines ambiguous queries into structured conditions,\nKG offers semantic reasoning and interpretability, RAG supplements contextual\nculinary details, and LLM integrates outputs into coherent recommendations. We\nevaluate ChefMind on the Xiachufang dataset and manually annotated queries,\ncomparing it with LLM-only, KG-only, and RAG-only baselines. Results show that\nChefMind achieves superior performance in accuracy, relevance, completeness,\nand clarity, with an average score of 8.7 versus 6.4-6.7 for ablation models.\nMoreover, it reduces unprocessed queries to 1.6%, demonstrating robustness in\nhandling fuzzy demands.", "AI": {"tldr": "ChefMind\u662f\u4e00\u4e2a\u7ed3\u5408\u4e86\u63a2\u7d22\u94fe\uff08CoE\uff09\u3001\u77e5\u8bc6\u56fe\uff08KG\uff09\u3001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6df7\u5408\u63a8\u8350\u7cfb\u7edf\uff0c\u7528\u4e8e\u89e3\u51b3\u4e2a\u6027\u5316\u83dc\u8c31\u63a8\u8350\u4e2d\u7684\u6a21\u7cca\u7528\u6237\u610f\u56fe\u3001\u8bed\u4e49\u51c6\u786e\u6027\u548c\u7ec6\u8282\u8986\u76d6\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u4e2a\u6027\u5316\u83dc\u8c31\u63a8\u8350\u7cfb\u7edf\u5728\u5904\u7406\u6a21\u7cca\u7684\u7528\u6237\u610f\u56fe\u3001\u786e\u4fdd\u8bed\u4e49\u51c6\u786e\u6027\u548c\u63d0\u4f9b\u5145\u8db3\u7684\u7ec6\u8282\u8986\u76d6\u65b9\u9762\u9762\u4e34\u6311\u6218\u3002", "method": "ChefMind\u91c7\u7528\u6df7\u5408\u67b6\u6784\uff0c\u7ed3\u5408\u4e86\u63a2\u7d22\u94fe\uff08CoE\uff09\u6765\u7ec6\u5316\u6a21\u7cca\u67e5\u8be2\u3001\u77e5\u8bc6\u56fe\uff08KG\uff09\u8fdb\u884c\u8bed\u4e49\u63a8\u7406\u548c\u89e3\u91ca\u3001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6765\u8865\u5145\u70f9\u996a\u7ec6\u8282\uff0c\u4ee5\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6765\u6574\u5408\u8f93\u51fa\u5e76\u751f\u6210\u8fde\u8d2f\u7684\u63a8\u8350\u3002", "result": "\u5728Xiachufang\u6570\u636e\u96c6\u548c\u624b\u52a8\u6807\u6ce8\u67e5\u8be2\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cChefMind\u7684\u5e73\u5747\u5f97\u5206\u4e3a8.7\uff0c\u4f18\u4e8e\u4ec5\u4f7f\u7528LLM\u3001KG\u6216RAG\u7684\u6a21\u578b\uff08\u5f97\u5206\u57286.4-6.7\u4e4b\u95f4\uff09\uff0c\u5728\u51c6\u786e\u6027\u3001\u76f8\u5173\u6027\u3001\u5b8c\u6574\u6027\u548c\u6e05\u6670\u5ea6\u65b9\u9762\u8868\u73b0\u66f4\u4f73\u3002\u6b64\u5916\uff0cChefMind\u5c06\u672a\u5904\u7406\u7684\u67e5\u8be2\u51cf\u5c11\u52301.6%\uff0c\u8bc1\u660e\u4e86\u5176\u5904\u7406\u6a21\u7cca\u9700\u6c42\u7684\u80fd\u529b\u3002", "conclusion": "ChefMind\u901a\u8fc7\u6574\u5408\u591a\u79cd\u6280\u672f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4e2a\u6027\u5316\u83dc\u8c31\u63a8\u8350\u4e2d\u7684\u6311\u6218\uff0c\u5e76\u5728\u51c6\u786e\u6027\u3001\u76f8\u5173\u6027\u3001\u5b8c\u6574\u6027\u548c\u6e05\u6670\u5ea6\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2509.18395", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18395", "abs": "https://arxiv.org/abs/2509.18395", "authors": ["Minki Hong", "Jangho Choi", "Jihie Kim"], "title": "NormGenesis: Multicultural Dialogue Generation via Exemplar-Guided Social Norm Modeling and Violation Recovery", "comment": "39 pages, 17 figures, EMNLP 2025 Main Conference", "summary": "Social norms govern culturally appropriate behavior in communication,\nenabling dialogue systems to produce responses that are not only coherent but\nalso socially acceptable. We present NormGenesis, a multicultural framework for\ngenerating and annotating socially grounded dialogues across English, Chinese,\nand Korean. To model the dynamics of social interaction beyond static norm\nclassification, we propose a novel dialogue type, Violation-to-Resolution\n(V2R), which models the progression of conversations following norm violations\nthrough recognition and socially appropriate repair. To improve pragmatic\nconsistency in underrepresented languages, we implement an exemplar-based\niterative refinement early in the dialogue synthesis process. This design\nintroduces alignment with linguistic, emotional, and sociocultural expectations\nbefore full dialogue generation begins. Using this framework, we construct a\ndataset of 10,800 multi-turn dialogues annotated at the turn level for norm\nadherence, speaker intent, and emotional response. Human and LLM-based\nevaluations demonstrate that NormGenesis significantly outperforms existing\ndatasets in refinement quality, dialogue naturalness, and generalization\nperformance. We show that models trained on our V2R-augmented data exhibit\nimproved pragmatic competence in ethically sensitive contexts. Our work\nestablishes a new benchmark for culturally adaptive dialogue modeling and\nprovides a scalable methodology for norm-aware generation across linguistically\nand culturally diverse languages.", "AI": {"tldr": "NormGenesis\u662f\u4e00\u4e2a\u591a\u6587\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u548c\u6807\u6ce8\u8de8\u82f1\u8bed\u3001\u4e2d\u6587\u548c\u97e9\u8bed\u7684\u793e\u4f1a\u5316\u5bf9\u8bdd\u3002\u5b83\u901a\u8fc7\u5f15\u5165\u65b0\u9896\u7684\u5bf9\u8bdd\u7c7b\u578b\u201c\u8fdd\u89c4\u5230\u89e3\u51b3\u201d\uff08V2R\uff09\u6765\u6a21\u62df\u793e\u4f1a\u4e92\u52a8\u52a8\u6001\uff0c\u5e76\u91c7\u7528\u57fa\u4e8e\u793a\u4f8b\u7684\u8fed\u4ee3\u7ec6\u5316\u65b9\u6cd5\u6765\u63d0\u9ad8\u975e\u4ee3\u8868\u6027\u8bed\u8a00\u7684\u8bed\u7528\u4e00\u81f4\u6027\u3002\u8be5\u6846\u67b6\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b10,800\u4e2a\u591a\u8f6e\u5bf9\u8bdd\u7684\u6570\u636e\u96c6\uff0c\u5e76\u5728\u5bf9\u8bdd\u5408\u89c4\u6027\u3001\u8bf4\u8bdd\u8005\u610f\u56fe\u548c\u60c5\u611f\u53cd\u5e94\u65b9\u9762\u8fdb\u884c\u4e86\u6807\u6ce8\u3002", "motivation": "\u4e3a\u4e86\u5728\u5bf9\u8bdd\u7cfb\u7edf\u4e2d\u751f\u6210\u4e0d\u4ec5\u8fde\u8d2f\u800c\u4e14\u5728\u793e\u4f1a\u53ef\u63a5\u53d7\u7684\u54cd\u5e94\uff0c\u9700\u8981\u5bf9\u793e\u4f1a\u89c4\u8303\u8fdb\u884c\u5efa\u6a21\uff0c\u4ee5\u6307\u5bfc\u7b26\u5408\u6587\u5316\u9002\u5f53\u884c\u4e3a\u7684\u4ea4\u6d41\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5bf9\u8bdd\u7c7b\u578b\u201c\u8fdd\u89c4\u5230\u89e3\u51b3\u201d\uff08V2R\uff09\uff0c\u4ee5\u6a21\u62df\u9075\u5faa\u89c4\u8303\u8fdd\u89c4\u7684\u5bf9\u8bdd\u8fdb\u5c55\uff0c\u901a\u8fc7\u8bc6\u522b\u548c\u9002\u5f53\u7684\u793e\u4f1a\u4fee\u590d\u3002\u5728\u5bf9\u8bdd\u5408\u6210\u8fc7\u7a0b\u65e9\u671f\u5b9e\u73b0\u57fa\u4e8e\u793a\u4f8b\u7684\u8fed\u4ee3\u7ec6\u5316\uff0c\u4ee5\u63d0\u9ad8\u975e\u4ee3\u8868\u6027\u8bed\u8a00\u7684\u8bed\u7528\u4e00\u81f4\u6027\u3002\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b10,800\u4e2a\u591a\u8f6e\u5bf9\u8bdd\u7684\u6570\u636e\u96c6\uff0c\u5e76\u5728\u6bcf\u4e2a\u56de\u5408\u4e2d\u5bf9\u89c4\u8303\u5408\u89c4\u6027\u3001\u8bf4\u8bdd\u8005\u610f\u56fe\u548c\u60c5\u611f\u53cd\u5e94\u8fdb\u884c\u6807\u6ce8\u3002", "result": "\u4f7f\u7528NormGenesis\u6846\u67b6\u6784\u5efa\u7684\u6570\u636e\u96c6\uff0c\u5728\u5bf9\u8bdd\u8d28\u91cf\u3001\u81ea\u7136\u5ea6\u548c\u6cdb\u5316\u6027\u80fd\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6570\u636e\u96c6\u3002\u5728V2R\u589e\u5f3a\u6570\u636e\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u6d89\u53ca\u4f26\u7406\u654f\u611f\u6027\u7684\u60c5\u5883\u4e2d\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u8bed\u7528\u80fd\u529b\u3002", "conclusion": "NormGenesis\u4e3a\u6587\u5316\u9002\u5e94\u6027\u5bf9\u8bdd\u5efa\u6a21\u8bbe\u7acb\u4e86\u65b0\u7684\u57fa\u51c6\uff0c\u5e76\u4e3a\u8de8\u8bed\u8a00\u548c\u6587\u5316\u591a\u6837\u6027\u7684\u89c4\u8303\u611f\u77e5\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\u3002"}}
{"id": "2509.19107", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.19107", "abs": "https://arxiv.org/abs/2509.19107", "authors": ["Khan Masood Parvez", "Sk Md Abidar Rahaman", "Ali Shiri Sichani", "Hadi AliAkbarpour"], "title": "AI-Enabled Smart Hygiene System for Real-Time Glucose Detection", "comment": null, "summary": "This research presents a smart urinary health monitoring system incorporating\na coplanar waveguide (CPW)-fed slot-loop antenna biosensor designed to analyse\nvarious urine samples. The antenna demonstrates distinct resonant frequency\nshifts when exposed to five specific urine conditions, deviating from its\nbaseline 1.42 GHz operation. These measurable frequency variations enable the\nantenna to function as an effective microwave sensor for urinary biomarker\ndetection. A potential artificial intelligence-based Convolutional Neural\nNetworks Long Short-Term Memory (CNN-LSTM) framework is also discussed to\novercome the limitations of overlapping frequency responses, aiming to improve\nthe accuracy of health condition detection. These components contribute to the\ndevelopment of a smart toilet system that displays real-time health information\non a wall-mounted urinal screen, without requiring any user effort or\nbehavioural change.", "AI": {"tldr": "\u57fa\u4e8eCPW\u9988\u7535\u7684\u5fae\u5e26\u73af\u5f62\u5929\u7ebf\u751f\u7269\u4f20\u611f\u5668\u53ef\u7528\u4e8e\u68c0\u6d4b\u5c3f\u6db2\u6837\u672c\u4e2d\u7684\u751f\u7269\u6807\u5fd7\u7269\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u5b9e\u65f6\u76d1\u6d4b\u7528\u6237\u6ccc\u5c3f\u5065\u5eb7\u72b6\u51b5\u7684\u667a\u80fd\u7cfb\u7edf\uff0c\u4ee5\u5b9e\u73b0\u65e9\u671f\u75be\u75c5\u68c0\u6d4b\u548c\u4e2a\u6027\u5316\u5065\u5eb7\u7ba1\u7406\u3002", "method": "\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u4e86\u4e00\u79cdCPW\u9988\u7535\u7684\u5fae\u5e26\u73af\u5f62\u5929\u7ebf\u751f\u7269\u4f20\u611f\u5668\uff0c\u80fd\u591f\u901a\u8fc7\u6d4b\u91cf\u8c10\u632f\u9891\u7387\u7684\u53d8\u5316\u6765\u68c0\u6d4b\u5c3f\u6db2\u6837\u672c\u7684\u4e94\u79cd\u4e0d\u540c\u72b6\u51b5\u3002\u540c\u65f6\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eCNN-LSTM\u7684AI\u6846\u67b6\uff0c\u7528\u4e8e\u5904\u7406\u91cd\u53e0\u7684\u9891\u7387\u54cd\u5e94\uff0c\u63d0\u9ad8\u5065\u5eb7\u72b6\u51b5\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u3002", "result": "\u5929\u7ebf\u5728\u66b4\u9732\u4e8e\u4e94\u79cd\u7279\u5b9a\u5c3f\u6db2\u72b6\u51b5\u65f6\uff0c\u5176\u8c10\u632f\u9891\u7387\u76f8\u5bf9\u4e8e\u57fa\u7ebf1.42 GHz\u53d1\u751f\u4e86\u660e\u663e\u504f\u79fb\uff0c\u8bc1\u660e\u4e86\u5176\u4f5c\u4e3a\u5fae\u6ce2\u4f20\u611f\u5668\u7684\u6709\u6548\u6027\u3002AI\u6846\u67b6\u6709\u671b\u63d0\u9ad8\u68c0\u6d4b\u51c6\u786e\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u667a\u80fd\u76d1\u6d4b\u7cfb\u7edf\uff0c\u7ed3\u5408\u4e86\u5148\u8fdb\u7684\u5929\u7ebf\u8bbe\u8ba1\u548cAI\u7b97\u6cd5\uff0c\u80fd\u591f\u5b9e\u65f6\u3001\u65e0\u611f\u5730\u76d1\u6d4b\u7528\u6237\u5065\u5eb7\u72b6\u51b5\uff0c\u4e3a\u667a\u80fd\u5bb6\u5c45\u548c\u8fdc\u7a0b\u533b\u7597\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.18745", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2509.18745", "abs": "https://arxiv.org/abs/2509.18745", "authors": ["Z. Mu", "Z. Zhang", "J. Frauni\u00e9", "C. Robert", "G. Seine", "B. Gil", "G. Cassabois", "V. Jacques"], "title": "Spin defects in hexagonal boron nitride as two-dimensional strain sensors", "comment": "6 pages, 4 figures", "summary": "Lattice deformation is a powerful way to engineer the properties of\ntwo-dimensional (2D) materials, making their precise measurement an important\nchallenge for both fundamental science and technological applications. Here, we\ndemonstrate that boron-vacancy (V$_\\text{B}^-$) color centers in hexagonal\nboron nitride (hBN) enable quantitative strain sensing with sub-micrometer\nspatial resolution. Using this approach, we precisely quantify the\nstrain-induced shift of the E$_{\\rm 2g}$ Raman mode in a hBN flake under\nuniaxial stress, establishing V$_\\text{B}^-$ centers as a new tool for strain\nmetrology in van der Waals heterostructures. Beyond strain sensing, our work\nalso highlights the unique multimodal sensing functionalities offered by\nV$_\\text{B}^-$ centers, which will be valuable for future studies of\nstrain-engineered 2D materials.", "AI": {"tldr": "V$_\text{B}^-$ \u8272\u5fc3\u53ef\u7528\u4e8e\u5728\u516d\u65b9\u6c2e\u5316\u787c (hBN) \u4e2d\u8fdb\u884c\u4e9a\u5fae\u7c73\u7ea7\u5e94\u53d8\u4f20\u611f\u3002", "motivation": "\u7cbe\u786e\u6d4b\u91cf\u4e8c\u7ef4\u6750\u6599\u7684\u6676\u683c\u53d8\u5f62\u5bf9\u4e8e\u57fa\u7840\u79d1\u5b66\u548c\u6280\u672f\u5e94\u7528\u90fd\u5f88\u91cd\u8981\u3002", "method": "\u5229\u7528\u516d\u65b9\u6c2e\u5316\u787c (hBN) \u4e2d\u7684\u787c-\u7a7a\u4f4d (V$_\text{B}^-$) \u8272\u5fc3\u8fdb\u884c\u5e94\u53d8\u4f20\u611f\u3002", "result": "\u7cbe\u786e\u91cf\u5316\u4e86\u5355\u8f74\u5e94\u529b\u4e0b hBN \u8584\u7247\u4e2d E$_{\rm 2g}$ \u62c9\u66fc\u6a21\u5f0f\u7684\u5e94\u53d8\u8bf1\u5bfc\u9891\u79fb\uff0c\u786e\u7acb\u4e86 V$_\text{B}^-$ \u8272\u5fc3\u4f5c\u4e3a\u8303\u5fb7\u534e\u5f02\u8d28\u7ed3\u6784\u4e2d\u5e94\u53d8\u8ba1\u91cf\u7684\u65b0\u5de5\u5177\u3002", "conclusion": "V$_\text{B}^-$ \u8272\u5fc3\u53ef\u7528\u4e8e\u7cbe\u786e\u5e94\u53d8\u6d4b\u91cf\uff0c\u5e76\u5177\u6709\u591a\u529f\u80fd\u4f20\u611f\u80fd\u529b\uff0c\u53ef\u7528\u4e8e\u7814\u7a76\u5e94\u53d8\u5de5\u7a0b\u4e8c\u7ef4\u6750\u6599\u3002"}}
{"id": "2509.19272", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.19272", "abs": "https://arxiv.org/abs/2509.19272", "authors": ["Sathwik Chadaga"], "title": "Faster-Than-Nyquist Signalling - Theoretical Limits on Capacity and Techniques to Approach Capacity", "comment": null, "summary": "Faster-Than-Nyquist (FTN) Signalling is a non-orthogonal transmission scheme\nthat violates the Nyquist zero-ISI criterion providing higher throughput and\nbetter spectral efficiency than a Nyquist transmission scheme. In this thesis,\nthe inter symbol interference (ISI) introduced by FTN signalling is studied,\nand conditions on pulse shapes and $\\tau$ (time acceleration factor) are\nderived so that the ISI can be avoided completely. Further, these conditions\nare reinforced by investigating the theoretical limits on the capacities of FTN\nsystems. Finally, the use of power allocation and adaptive loading techniques\nare explored in reducing the effect of ISI and increasing the throughput of\northogonal frequency division multiplexing (OFDM) FTN systems. The\nimplementation of these techniques and simulation results are also\ndemonstrated.", "AI": {"tldr": "FTN\u4fe1\u53f7\u662f\u4e00\u79cd\u975e\u6b63\u4ea4\u4f20\u8f93\u65b9\u6848\uff0c\u901a\u8fc7\u5f15\u5165\u7b26\u53f7\u95f4\u5e72\u6270\uff08ISI\uff09\u6765\u63d0\u9ad8\u541e\u5410\u91cf\u548c\u9891\u8c31\u6548\u7387\u3002\u672c\u8bba\u6587\u7814\u7a76\u4e86FTN\u4fe1\u53f7\u4e2d\u7684ISI\u95ee\u9898\uff0c\u5e76\u63a8\u5bfc\u4e86\u907f\u514dISI\u7684\u6761\u4ef6\u3002\u6b64\u5916\uff0c\u8fd8\u63a2\u8ba8\u4e86FTN\u7cfb\u7edf\u7684\u5bb9\u91cf\u7406\u8bba\u6781\u9650\uff0c\u5e76\u7814\u7a76\u4e86\u529f\u7387\u5206\u914d\u548c\u81ea\u9002\u5e94\u52a0\u8f7d\u6280\u672f\u5728\u51cf\u5c11ISI\u548c\u63d0\u9ad8OFDM-FTN\u7cfb\u7edf\u541e\u5410\u91cf\u65b9\u9762\u7684\u5e94\u7528\uff0c\u5e76\u8fdb\u884c\u4e86\u4eff\u771f\u9a8c\u8bc1\u3002", "motivation": "FTN\u4fe1\u53f7\u80fd\u591f\u63d0\u4f9b\u6bd4Nyquist\u4fe1\u53f7\u66f4\u9ad8\u7684\u541e\u5410\u91cf\u548c\u9891\u8c31\u6548\u7387\uff0c\u4f46\u5176\u5f15\u5165\u7684ISI\u95ee\u9898\u9700\u8981\u88ab\u7814\u7a76\u548c\u89e3\u51b3\u3002", "method": "\u63a8\u5bfc\u4e86\u907f\u514dFTN\u4fe1\u53f7\u4e2dISI\u7684\u8109\u51b2\u5f62\u72b6\u548c\u65f6\u95f4\u52a0\u901f\u56e0\u5b50\uff08\u03c4\uff09\u7684\u6761\u4ef6\uff1b\u7814\u7a76\u4e86FTN\u7cfb\u7edf\u7684\u5bb9\u91cf\u7406\u8bba\u6781\u9650\uff1b\u63a2\u7d22\u4e86\u529f\u7387\u5206\u914d\u548c\u81ea\u9002\u5e94\u52a0\u8f7d\u6280\u672f\u5728OFDM-FTN\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u3002", "result": "\u63a8\u5bfc\u4e86\u907f\u514dISI\u7684\u6761\u4ef6\uff1b\u8bc4\u4f30\u4e86FTN\u7cfb\u7edf\u7684\u5bb9\u91cf\uff1b\u5c55\u793a\u4e86\u529f\u7387\u5206\u914d\u548c\u81ea\u9002\u5e94\u52a0\u8f7d\u6280\u672f\u5728OFDM-FTN\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u548c\u4eff\u771f\u7ed3\u679c\u3002", "conclusion": "\u901a\u8fc7\u9009\u62e9\u5408\u9002\u7684\u8109\u51b2\u5f62\u72b6\u548c\u65f6\u95f4\u52a0\u901f\u56e0\u5b50\uff0c\u53ef\u4ee5\u5b8c\u5168\u907f\u514dFTN\u4fe1\u53f7\u4e2d\u7684ISI\u3002\u6b64\u5916\uff0c\u529f\u7387\u5206\u914d\u548c\u81ea\u9002\u5e94\u52a0\u8f7d\u6280\u672f\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63d0\u9ad8OFDM-FTN\u7cfb\u7edf\u7684\u6027\u80fd\u3002"}}
{"id": "2509.18576", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18576", "abs": "https://arxiv.org/abs/2509.18576", "authors": ["Zeyi Kang", "Liang He", "Yanxin Zhang", "Zuheng Ming", "Kaixing Zhao"], "title": "LCMF: Lightweight Cross-Modality Mambaformer for Embodied Robotics VQA", "comment": null, "summary": "Multimodal semantic learning plays a critical role in embodied intelligence,\nespecially when robots perceive their surroundings, understand human\ninstructions, and make intelligent decisions. However, the field faces\ntechnical challenges such as effective fusion of heterogeneous data and\ncomputational efficiency in resource-constrained environments. To address these\nchallenges, this study proposes the lightweight LCMF cascaded attention\nframework, introducing a multi-level cross-modal parameter sharing mechanism\ninto the Mamba module. By integrating the advantages of Cross-Attention and\nSelective parameter-sharing State Space Models (SSMs), the framework achieves\nefficient fusion of heterogeneous modalities and semantic complementary\nalignment. Experimental results show that LCMF surpasses existing multimodal\nbaselines with an accuracy of 74.29% in VQA tasks and achieves competitive\nmid-tier performance within the distribution cluster of Large Language Model\nAgents (LLM Agents) in EQA video tasks. Its lightweight design achieves a\n4.35-fold reduction in FLOPs relative to the average of comparable baselines\nwhile using only 166.51M parameters (image-text) and 219M parameters\n(video-text), providing an efficient solution for Human-Robot Interaction (HRI)\napplications in resource-constrained scenarios with strong multimodal decision\ngeneralization capabilities.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u8f7b\u91cf\u7ea7\u7684LCMF\u7ea7\u8054\u6ce8\u610f\u529b\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u7ea7\u8de8\u6a21\u6001\u53c2\u6570\u5171\u4eab\u673a\u5236\u548cMamba\u6a21\u5757\uff0c\u6709\u6548\u878d\u5408\u5f02\u6784\u6a21\u6001\u6570\u636e\uff0c\u5e76\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u5728VQA\u548cEQA\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u7684\u7ed3\u679c\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u673a\u5668\u4eba\u611f\u77e5\u3001\u7406\u89e3\u6307\u4ee4\u548c\u51b3\u7b56\u65f6\uff0c\u5f02\u6784\u6570\u636e\u878d\u5408\u56f0\u96be\u4ee5\u53ca\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u8ba1\u7b97\u6548\u7387\u95ee\u9898\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u4e86LCMF\u7ea7\u8054\u6ce8\u610f\u529b\u6846\u67b6\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684LCMF\u7ea7\u8054\u6ce8\u610f\u529b\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5c06\u591a\u7ea7\u8de8\u6a21\u6001\u53c2\u6570\u5171\u4eab\u673a\u5236\u5f15\u5165Mamba\u6a21\u5757\uff0c\u5e76\u7ed3\u5408\u4e86\u4ea4\u53c9\u6ce8\u610f\u529b\u548c\u9009\u62e9\u6027\u53c2\u6570\u5171\u4eab\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08SSMs\uff09\u7684\u4f18\u70b9\uff0c\u5b9e\u73b0\u4e86\u5f02\u6784\u6a21\u6001\u7684\u9ad8\u6548\u878d\u5408\u548c\u8bed\u4e49\u4e92\u8865\u5bf9\u9f50\u3002", "result": "LCMF\u5728VQA\u4efb\u52a1\u4e2d\u7684\u51c6\u786e\u7387\u4e3a74.29%\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u591a\u6a21\u6001\u57fa\u7ebf\u3002\u5728EQA\u89c6\u9891\u4efb\u52a1\u4e2d\uff0c\u5176\u8868\u73b0\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\uff08LLM Agents\uff09\u7684\u5206\u5e03\u96c6\u7fa4\u76f8\u5f53\u3002\u8be5\u6846\u67b6\u7684\u8ba1\u7b97\u91cf\uff08FLOPs\uff09\u6bd4\u53ef\u6bd4\u57fa\u7ebf\u5e73\u5747\u503c\u51cf\u5c11\u4e864.35\u500d\uff0c\u53c2\u6570\u91cf\uff08\u56fe\u50cf-\u6587\u672c\uff09\u4e3a1.6651\u4ebf\uff0c\uff08\u89c6\u9891-\u6587\u672c\uff09\u4e3a2.19\u4ebf\u3002", "conclusion": "LCMF\u6846\u67b6\u4e3a\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e0b\u7684\u4eba\u673a\u4ea4\u4e92\uff08HRI\uff09\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5b83\u5728\u5177\u6709\u5f3a\u5927\u8de8\u6a21\u6001\u51b3\u7b56\u6cdb\u5316\u80fd\u529b\u7684\u540c\u65f6\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u53c2\u6570\u91cf\u3002"}}
{"id": "2509.18190", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18190", "abs": "https://arxiv.org/abs/2509.18190", "authors": ["Junseong Shin", "Seungwoo Chung", "Yunjeong Yang", "Tae Hyun Kim"], "title": "HazeFlow: Revisit Haze Physical Model as ODE and Non-Homogeneous Haze Generation for Real-World Dehazing", "comment": null, "summary": "Dehazing involves removing haze or fog from images to restore clarity and\nimprove visibility by estimating atmospheric scattering effects. While deep\nlearning methods show promise, the lack of paired real-world training data and\nthe resulting domain gap hinder generalization to real-world scenarios. In this\ncontext, physics-grounded learning becomes crucial; however, traditional\nmethods based on the Atmospheric Scattering Model (ASM) often fall short in\nhandling real-world complexities and diverse haze patterns. To solve this\nproblem, we propose HazeFlow, a novel ODE-based framework that reformulates ASM\nas an ordinary differential equation (ODE). Inspired by Rectified Flow (RF),\nHazeFlow learns an optimal ODE trajectory to map hazy images to clean ones,\nenhancing real-world dehazing performance with only a single inference step.\nAdditionally, we introduce a non-homogeneous haze generation method using\nMarkov Chain Brownian Motion (MCBM) to address the scarcity of paired\nreal-world data. By simulating realistic haze patterns through MCBM, we enhance\nthe adaptability of HazeFlow to diverse real-world scenarios. Through extensive\nexperiments, we demonstrate that HazeFlow achieves state-of-the-art performance\nacross various real-world dehazing benchmark datasets.", "AI": {"tldr": "HazeFlow\u662f\u4e00\u4e2a\u57fa\u4e8eODE\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u62df\u5927\u6c14\u6563\u5c04\u6a21\u578b\u6765\u53bb\u9664\u56fe\u50cf\u4e2d\u7684\u96fe\u973e\uff0c\u5e76\u4f7f\u7528MCBM\u751f\u6210\u975e\u5747\u5300\u96fe\u973e\u6570\u636e\uff0c\u4ee5\u63d0\u9ad8\u5728\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u6df1\u5ea6\u5b66\u4e60\u53bb\u96fe\u65b9\u6cd5\u7f3a\u4e4f\u771f\u5b9e\u4e16\u754c\u7684\u914d\u5bf9\u8bad\u7ec3\u6570\u636e\uff0c\u5e76\u4e14\u5728\u5904\u7406\u771f\u5b9e\u4e16\u754c\u7684\u590d\u6742\u6027\u548c\u591a\u6837\u5316\u7684\u96fe\u973e\u6a21\u5f0f\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684ODE\uff08\u5e38\u5fae\u5206\u65b9\u7a0b\uff09\u65b9\u6cd5\uff0c\u5c06\u5927\u6c14\u6563\u5c04\u6a21\u578b\uff08ASM\uff09\u91cd\u65b0\u8868\u8ff0\u4e3aODE\u3002\u901a\u8fc7\u5b66\u4e60\u4e00\u4e2a\u6700\u4f18\u7684ODE\u8f68\u8ff9\uff0c\u5c06\u6709\u96fe\u56fe\u50cf\u6620\u5c04\u5230\u6e05\u6670\u56fe\u50cf\u3002\u5f15\u5165\u4e86\u4e00\u79cd\u4f7f\u7528\u9a6c\u5c14\u53ef\u592b\u94fe\u5e03\u6717\u8fd0\u52a8\uff08MCBM\uff09\u7684\u975e\u5747\u5300\u96fe\u973e\u751f\u6210\u65b9\u6cd5\u6765\u89e3\u51b3\u914d\u5bf9\u771f\u5b9e\u4e16\u754c\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\u3002", "result": "HazeFlow\u5728\u5404\u79cd\u771f\u5b9e\u4e16\u754c\u7684\u53bb\u96fe\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "HazeFlow\u901a\u8fc7\u5c06ASM\u91cd\u65b0\u8868\u8ff0\u4e3aODE\u5e76\u7ed3\u5408MCBM\u751f\u6210\u6280\u672f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u53bb\u96fe\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u771f\u5b9e\u4e16\u754c\u53bb\u96fe\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6210\u679c\u3002"}}
{"id": "2509.18474", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2509.18474", "abs": "https://arxiv.org/abs/2509.18474", "authors": ["Yuta Hirasaki", "Toshinari Itoko", "Naoki Kanazawa", "Eiji Saitoh"], "title": "Shift of quantum critical point of discrete time crystal on a noisy quantum simulator", "comment": "6 pages, 3 figures", "summary": "Recent advances in quantum technology have enabled the simulation of quantum\nmany-body systems on real quantum devices. However, such quantum simulators are\ninherently subject to decoherence, and its impact on system dynamics -\nparticularly near quantum phase transitions - remains insufficiently\nunderstood. In this work, we experimentally investigate how decoherence in\nquantum devices affects the dynamics of quantum time crystals, using a\n156-qubit IBM Quantum system. We find that decoherence shifts the location of\ncritical behavior associated with the phase transition, suggesting that noisy\nsimulations can lead to inaccurate identification of phase boundaries. Our\nresults underscore the importance of understanding and mitigating decoherence\nto reliably simulate quantum many-body systems on near-term quantum hardware.", "AI": {"tldr": "\u672c\u7814\u7a76\u4f7f\u7528156\u91cf\u5b50\u6bd4\u7279\u7684IBM\u91cf\u5b50\u7cfb\u7edf\uff0c\u7814\u7a76\u4e86\u91cf\u5b50\u6bd4\u7279\u9000\u76f8\u5e72\u5bf9\u91cf\u5b50\u65f6\u95f4\u6676\u4f53\u52a8\u529b\u5b66\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u9000\u76f8\u5e72\u4f1a\u79fb\u52a8\u91cf\u5b50\u76f8\u53d8\u76f8\u5173\u7684\u4e34\u754c\u884c\u4e3a\u7684\u4f4d\u7f6e\uff0c\u5e76\u5f3a\u8c03\u4e86\u5728\u8fd1\u671f\u91cf\u5b50\u786c\u4ef6\u4e0a\u7406\u89e3\u548c\u51cf\u8f7b\u9000\u76f8\u5e72\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u8fd1\u671f\u91cf\u5b50\u6280\u672f\u5728\u6a21\u62df\u91cf\u5b50\u591a\u4f53\u7cfb\u7edf\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u91cf\u5b50\u6a21\u62df\u5668\u56fa\u6709\u7684\u9000\u76f8\u5e72\u53ca\u5176\u5bf9\u7cfb\u7edf\u52a8\u529b\u5b66\uff08\u5c24\u5176\u662f\u5728\u91cf\u5b50\u76f8\u53d8\u9644\u8fd1\uff09\u7684\u5f71\u54cd\u4ecd\u672a\u5f97\u5230\u5145\u5206\u7406\u89e3\u3002", "method": "\u4f7f\u7528156\u91cf\u5b50\u6bd4\u7279\u7684IBM\u91cf\u5b50\u7cfb\u7edf\uff0c\u5b9e\u9a8c\u7814\u7a76\u4e86\u9000\u76f8\u5e72\u5bf9\u91cf\u5b50\u65f6\u95f4\u6676\u4f53\u52a8\u529b\u5b66\u7684\u5f71\u54cd\u3002", "result": "\u9000\u76f8\u5e72\u4f1a\u79fb\u52a8\u4e0e\u91cf\u5b50\u76f8\u53d8\u76f8\u5173\u7684\u4e34\u754c\u884c\u4e3a\u7684\u4f4d\u7f6e\uff0c\u8868\u660e\u6709\u566a\u58f0\u7684\u6a21\u62df\u53ef\u80fd\u5bfc\u81f4\u76f8\u8fb9\u754c\u8bc6\u522b\u4e0d\u51c6\u786e\u3002", "conclusion": "\u7ed3\u679c\u5f3a\u8c03\u4e86\u7406\u89e3\u548c\u51cf\u8f7b\u9000\u76f8\u5e72\u5bf9\u4e8e\u5728\u8fd1\u671f\u91cf\u5b50\u786c\u4ef6\u4e0a\u53ef\u9760\u5730\u6a21\u62df\u91cf\u5b50\u591a\u4f53\u7cfb\u7edf\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2509.18229", "categories": ["cs.AI", "70, 74, 76, 80"], "pdf": "https://arxiv.org/pdf/2509.18229", "abs": "https://arxiv.org/abs/2509.18229", "authors": ["Anthony Patera", "Rohan Abeyaratne"], "title": "An N-Plus-1 GPT Agency for Critical Solution of Mechanical Engineering Analysis Problems", "comment": null, "summary": "Generative AI, and specifically GPT, can produce a remarkable solution to a\nmechanical engineering analysis problem - but also, on occasion, a flawed\nsolution. For example, an elementary mechanics problem is solved flawlessly in\none GPT instance and incorrectly in a subsequent GPT instance, with a success\nprobability of only 85%. This unreliability renders \"out-of-the-box\" GPT\nunsuitable for deployment in education or engineering practice. We introduce an\n\"N-Plus-1\" GPT Agency for Initial (Low-Cost) Analysis of mechanical engineering\nProblem Statements. Agency first launches N instantiations of Agent Solve to\nyield N independent Proposed Problem Solution Realizations; Agency then invokes\nAgent Compare to summarize and compare the N Proposed Problem Solution\nRealizations and to provide a Recommended Problem Solution. We argue from\nCondorcet's Jury Theorem that, for a Problem Statement characterized by\nper-Solve success probability greater than 1/2 (and N sufficiently large), the\nPredominant (Agent Compare) Proposed Problem Solution will, with high\nprobability, correspond to a Correct Proposed Problem Solution. Furthermore,\nAgent Compare can also incorporate aspects of Secondary (Agent Compare)\nProposed Problem Solutions, in particular when the latter represent alternative\nProblem Statement interpretations - different Mathematical Models - or\nalternative Mathematical Solution Procedures. Comparisons to Grok Heavy, a\ncommercial multi-agent model, show similarities in design and performance, but\nalso important differences in emphasis: our Agency focuses on transparency and\npedagogical value.", "AI": {"tldr": "\u751f\u6210\u5f0fAI\uff08\u7279\u522b\u662fGPT\uff09\u867d\u7136\u80fd\u89e3\u51b3\u673a\u68b0\u5de5\u7a0b\u5206\u6790\u95ee\u9898\uff0c\u4f46\u5b58\u5728\u4e0d\u53ef\u9760\u6027\u3002\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u201cN-Plus-1\u201dGPT\u4ee3\u7406\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fd0\u884cN\u4e2aGPT\u5b9e\u4f8b\u5e76\u6bd4\u8f83\u5176\u7ed3\u679c\uff0c\u4ee5\u63d0\u9ad8\u89e3\u51b3\u65b9\u6848\u7684\u51c6\u786e\u6027\u3002\u8be5\u65b9\u6cd5\u501f\u9274\u4e86\u5b54\u591a\u585e\u966a\u5ba1\u5b9a\u7406\uff0c\u5728N\u8db3\u591f\u5927\u4e14\u5355\u4e2a\u5b9e\u4f8b\u6210\u529f\u7387\u9ad8\u4e8e1/2\u7684\u60c5\u51b5\u4e0b\uff0c\u80fd\u591f\u9ad8\u6982\u7387\u5730\u5f97\u5230\u6b63\u786e\u89e3\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u8fd8\u80fd\u5904\u7406\u4e0d\u540c\u7684\u95ee\u9898\u89e3\u91ca\u548c\u6c42\u89e3\u65b9\u6cd5\uff0c\u5e76\u5177\u6709\u900f\u660e\u5ea6\u548c\u6559\u5b66\u4ef7\u503c\u3002", "motivation": "GPT\u5728\u89e3\u51b3\u673a\u68b0\u5de5\u7a0b\u5206\u6790\u95ee\u9898\u65f6\u5b58\u5728\u4e0d\u53ef\u9760\u6027\uff0c\u6210\u529f\u7387\u4ec5\u4e3a85%\uff0c\u4e0d\u9002\u5408\u76f4\u63a5\u7528\u4e8e\u6559\u80b2\u6216\u5de5\u7a0b\u5b9e\u8df5\u3002", "method": "\u63d0\u51fa\u201cN-Plus-1\u201dGPT\u4ee3\u7406\u65b9\u6cd5\uff0c\u9996\u5148\u8fd0\u884cN\u4e2a\u72ec\u7acb\u7684GPT\u5b9e\u4f8b\uff08Agent Solve\uff09\u751f\u6210N\u4e2a\u89e3\u51b3\u65b9\u6848\uff0c\u7136\u540e\u7531Agent Compare\u8fdb\u884c\u603b\u7ed3\u548c\u6bd4\u8f83\uff0c\u6700\u7ec8\u63a8\u8350\u4e00\u4e2a\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u8be5\u65b9\u6cd5\u501f\u9274\u4e86\u5b54\u591a\u585e\u966a\u5ba1\u5b9a\u7406\uff0c\u5728\u5355\u4e2a\u5b9e\u4f8b\u6210\u529f\u7387\u5927\u4e8e1/2\u4e14N\u8db3\u591f\u5927\u7684\u60c5\u51b5\u4e0b\uff0c\u80fd\u591f\u9ad8\u6982\u7387\u5730\u5f97\u5230\u6b63\u786e\u89e3\u3002Agent Compare\u8fd8\u53ef\u4ee5\u6574\u5408\u6b21\u4f18\u89e3\u4e2d\u7684\u4fe1\u606f\uff0c\u4f8b\u5982\u4e0d\u540c\u7684\u6570\u5b66\u6a21\u578b\u6216\u6c42\u89e3\u65b9\u6cd5\u3002\u4e0eGrok Heavy\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5728\u8bbe\u8ba1\u548c\u6027\u80fd\u4e0a\u76f8\u4f3c\uff0c\u4f46\u5728\u900f\u660e\u5ea6\u548c\u6559\u5b66\u4ef7\u503c\u65b9\u9762\u66f4\u53d7\u91cd\u89c6\u3002", "conclusion": "\u201cN-Plus-1\u201dGPT\u4ee3\u7406\u65b9\u6cd5\u901a\u8fc7\u96c6\u6210\u591a\u4e2aGPT\u5b9e\u4f8b\u5e76\u8fdb\u884c\u6bd4\u8f83\uff0c\u89e3\u51b3\u4e86GPT\u5728\u673a\u68b0\u5de5\u7a0b\u5206\u6790\u4e2d\u7684\u53ef\u9760\u6027\u95ee\u9898\uff0c\u5e76\u5177\u6709\u6f5c\u5728\u7684\u6559\u5b66\u4ef7\u503c\u3002"}}
{"id": "2509.18401", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18401", "abs": "https://arxiv.org/abs/2509.18401", "authors": ["Armin Tourajmehr", "Mohammad Reza Modarres", "Yadollah Yaghoobzadeh"], "title": "Evaluating the Creativity of LLMs in Persian Literary Text Generation", "comment": null, "summary": "Large language models (LLMs) have demonstrated notable creative abilities in\ngenerating literary texts, including poetry and short stories. However, prior\nresearch has primarily centered on English, with limited exploration of\nnon-English literary traditions and without standardized methods for assessing\ncreativity. In this paper, we evaluate the capacity of LLMs to generate Persian\nliterary text enriched with culturally relevant expressions. We build a dataset\nof user-generated Persian literary spanning 20 diverse topics and assess model\noutputs along four creativity dimensions-originality, fluency, flexibility, and\nelaboration-by adapting the Torrance Tests of Creative Thinking. To reduce\nevaluation costs, we adopt an LLM as a judge for automated scoring and validate\nits reliability against human judgments using intraclass correlation\ncoefficients, observing strong agreement. In addition, we analyze the models'\nability to understand and employ four core literary devices: simile, metaphor,\nhyperbole, and antithesis. Our results highlight both the strengths and\nlimitations of LLMs in Persian literary text generation, underscoring the need\nfor further refinement.", "AI": {"tldr": "LLMs\u5728\u751f\u6210\u6ce2\u65af\u6587\u5b66\u6587\u672c\u65b9\u9762\u5c55\u73b0\u4e86\u6f5c\u529b\uff0c\u4f46\u4ecd\u9700\u6539\u8fdb\uff0c\u5e76\u4e14\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4eba\u7c7b\u5224\u65ad\u548cLLM\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "motivation": "\u8bc4\u4f30LLMs\u751f\u6210\u5177\u6709\u6587\u5316\u76f8\u5173\u8868\u8fbe\u7684\u6ce2\u65af\u6587\u5b66\u6587\u672c\u7684\u80fd\u529b\uff0c\u5e76\u89e3\u51b3\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u82f1\u8bed\u548c\u7f3a\u4e4f\u6807\u51c6\u5316\u521b\u9020\u529b\u8bc4\u4f30\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b20\u4e2a\u4e0d\u540c\u4e3b\u9898\u7684\u7528\u6237\u751f\u6210\u6ce2\u65af\u6587\u5b66\u6570\u636e\u96c6\u3002\u91c7\u7528\u6539\u7f16\u7684Torrance\u521b\u9020\u6027\u601d\u7ef4\u6d4b\u8bd5\u6765\u8bc4\u4f30\u6a21\u578b\u8f93\u51fa\u7684\u539f\u521b\u6027\u3001\u6d41\u7545\u6027\u3001\u7075\u6d3b\u6027\u548c\u7cbe\u7ec6\u5ea6\u3002\u4e3a\u4e86\u964d\u4f4e\u6210\u672c\uff0c\u4f7f\u7528LLM\u4f5c\u4e3a\u81ea\u52a8\u8bc4\u5206\u7684\u88c1\u5224\uff0c\u5e76\u901a\u8fc7\u7ec4\u5185\u76f8\u5173\u7cfb\u6570\u9a8c\u8bc1\u5176\u4e0e\u4eba\u7c7b\u5224\u65ad\u7684\u4e00\u81f4\u6027\u3002\u540c\u65f6\uff0c\u5206\u6790\u4e86\u6a21\u578b\u7406\u89e3\u548c\u8fd0\u7528\u56db\u79cd\u6838\u5fc3\u6587\u5b66\u624b\u6cd5\uff08\u660e\u55bb\u3001\u9690\u55bb\u3001\u5938\u5f20\u3001\u5bf9\u7acb\uff09\u7684\u80fd\u529b\u3002", "result": "LLMs\u5728\u6ce2\u65af\u6587\u5b66\u6587\u672c\u751f\u6210\u65b9\u9762\u65e2\u6709\u4f18\u52bf\u4e5f\u6709\u5c40\u9650\u6027\u3002LLM\u4f5c\u4e3a\u8bc4\u4f30\u88c1\u5224\u4e0e\u4eba\u7c7b\u5224\u65ad\u9ad8\u5ea6\u4e00\u81f4\uff0c\u8868\u660e\u5176\u4f5c\u4e3a\u8bc4\u4f30\u5de5\u5177\u7684\u53ef\u9760\u6027\u3002", "conclusion": "LLMs\u5728\u751f\u6210\u6ce2\u65af\u6587\u5b66\u6587\u672c\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u5176\u80fd\u529b\u4ecd\u6709\u5f85\u63d0\u9ad8\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7684\u4f18\u5316\u548c\u7814\u7a76\u3002"}}
{"id": "2509.19110", "categories": ["eess.SY", "cs.LG", "cs.RO", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.19110", "abs": "https://arxiv.org/abs/2509.19110", "authors": ["Chenxu Ke", "Congling Tian", "Kaichen Xu", "Ye Li", "Lingcong Bao"], "title": "A Fast Initialization Method for Neural Network Controllers: A Case Study of Image-based Visual Servoing Control for the multicopter Interception", "comment": null, "summary": "Reinforcement learning-based controller design methods often require\nsubstantial data in the initial training phase. Moreover, the training process\ntends to exhibit strong randomness and slow convergence. It often requires\nconsiderable time or high computational resources. Another class of\nlearning-based method incorporates Lyapunov stability theory to obtain a\ncontrol policy with stability guarantees. However, these methods generally\nrequire an initially stable neural network control policy at the beginning of\ntraining. Evidently, a stable neural network controller can not only serve as\nan initial policy for reinforcement learning, allowing the training to focus on\nimproving controller performance, but also act as an initial state for\nlearning-based Lyapunov control methods. Although stable controllers can be\ndesigned using traditional control theory, designers still need to have a great\ndeal of control design knowledge to address increasingly complicated control\nproblems. The proposed neural network rapid initialization method in this paper\nachieves the initial training of the neural network control policy by\nconstructing datasets that conform to the stability conditions based on the\nsystem model. Furthermore, using the image-based visual servoing control for\nmulticopter interception as a case study, simulations and experiments were\nconducted to validate the effectiveness and practical performance of the\nproposed method. In the experiment, the trained control policy attains a final\ninterception velocity of 15 m/s.", "AI": {"tldr": "\u8be5\u65b9\u6cd5\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u548c\u7a33\u5b9a\u6027\u6761\u4ef6\u7684\u5feb\u901f\u795e\u7ecf\u7f51\u7edc\u63a7\u5236\u5668\u521d\u59cb\u5316\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u6570\u636e\u9700\u6c42\u5927\u3001\u6536\u655b\u6162\u4ee5\u53ca\u57fa\u4e8e\u674e\u96c5\u666e\u8bfa\u592b\u7a33\u5b9a\u6027\u7684\u65b9\u6cd5\u9700\u8981\u521d\u59cb\u7a33\u5b9a\u7b56\u7565\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u63a7\u5236\u5668\u8bbe\u8ba1\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u521d\u59cb\u8bad\u7ec3\u6570\u636e\uff0c\u4e14\u5b58\u5728\u968f\u673a\u6027\u5f3a\u3001\u6536\u655b\u6162\u548c\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u9ad8\u7684\u95ee\u9898\u3002\u57fa\u4e8e\u674e\u96c5\u666e\u8bfa\u592b\u7a33\u5b9a\u6027\u7684\u65b9\u6cd5\u867d\u7136\u80fd\u63d0\u4f9b\u7a33\u5b9a\u6027\u4fdd\u8bc1\uff0c\u4f46\u901a\u5e38\u9700\u8981\u4e00\u4e2a\u521d\u59cb\u7a33\u5b9a\u7684\u795e\u7ecf\u7f51\u7edc\u63a7\u5236\u7b56\u7565\u3002\u7136\u800c\uff0c\u8bbe\u8ba1\u4e00\u4e2a\u521d\u59cb\u7a33\u5b9a\u7684\u63a7\u5236\u5668\u9700\u8981\u5927\u91cf\u7684\u63a7\u5236\u8bbe\u8ba1\u77e5\u8bc6\uff0c\u8fd9\u5bf9\u4e8e\u65e5\u76ca\u590d\u6742\u7684\u63a7\u5236\u95ee\u9898\u6765\u8bf4\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u795e\u7ecf\u7f51\u7edc\u5feb\u901f\u521d\u59cb\u5316\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u6784\u5efa\u7b26\u5408\u7a33\u5b9a\u6027\u6761\u4ef6\u7684\u6570\u636e\u96c6\uff0c\u5e76\u57fa\u4e8e\u7cfb\u7edf\u6a21\u578b\u6765\u521d\u59cb\u5316\u795e\u7ecf\u7f51\u7edc\u63a7\u5236\u7b56\u7565\u3002\u8fd9\u79cd\u65b9\u6cd5\u53ef\u4ee5\u4f5c\u4e3a\u5f3a\u5316\u5b66\u4e60\u7684\u521d\u59cb\u7b56\u7565\uff0c\u4ece\u800c\u5c06\u8bad\u7ec3\u91cd\u70b9\u653e\u5728\u63d0\u9ad8\u63a7\u5236\u5668\u6027\u80fd\u4e0a\uff0c\u4e5f\u53ef\u4ee5\u4f5c\u4e3a\u57fa\u4e8e\u5b66\u4e60\u7684\u674e\u96c5\u666e\u8bfa\u592b\u63a7\u5236\u65b9\u6cd5\u7684\u521d\u59cb\u72b6\u6001\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u901a\u8fc7\u4e00\u4e2a\u591a\u65cb\u7ffc\u98de\u673a\u62e6\u622a\u7684\u56fe\u50cf\u89c6\u89c9\u4f3a\u670d\u63a7\u5236\u7684\u6848\u4f8b\u8fdb\u884c\u4e86\u4eff\u771f\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5730\u8bad\u7ec3\u63a7\u5236\u7b56\u7565\uff0c\u5e76\u5b9e\u73b0\u4e86\u6700\u7ec815\u7c73/\u79d2\u7684\u62e6\u622a\u901f\u5ea6\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u795e\u7ecf\u7f51\u7edc\u5feb\u901f\u521d\u59cb\u5316\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5730\u89e3\u51b3\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u6570\u636e\u9700\u6c42\u5927\u3001\u6536\u655b\u6162\u4ee5\u53ca\u57fa\u4e8e\u674e\u96c5\u666e\u8bfa\u592b\u65b9\u6cd5\u9700\u8981\u521d\u59cb\u7a33\u5b9a\u7b56\u7565\u7684\u7f3a\u70b9\uff0c\u5e76\u901a\u8fc7\u591a\u65cb\u7ffc\u98de\u673a\u62e6\u622a\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2509.18818", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2509.18818", "abs": "https://arxiv.org/abs/2509.18818", "authors": ["Nabil. Daghbouj", "H. S. Sen", "Mohamed BenSalem", "Jan. Ducho\u0148c", "Bingsheng. Li", "Miroslav. Karl\u00edk", "F. Ge", "Vladimir. Krsjak", "Petr. B\u00e1borh", "M. O. Liedke", "M. Butterling", "Alexandre. Wagner", "Bora. Karasulub", "Tomas. Polcarak"], "title": "Asymmetrical Defect Sink Behaviour of HCP/BCC Zr/Nb Multilayer Interfaces: Bubble-Denuded Zones at Nb Layers", "comment": "18 figures, 48 pages", "summary": "Radiation induced helium bubble formation poses a major challenge to the\nstructural integrity of materials in nuclear energy systems. In this study, we\ninvestigate defect evolution and He behavior in ZrNb nanoscale metallic\nmultilayers with immiscible BCC and HCP interfaces, irradiated with 80 keV He\nions. For comparison, single crystal Nb and polycrystalline Zr were also\nirradiated under identical conditions to serve as reference materials. Using\ncross sectional TEM, SIMS, STEM EELS, nanoindentation, Doppler Broadening\nPositron Annihilation Spectroscopy, Positron Annihilation Lifetime\nSpectroscopy, and atomistic simulations, we reveal a highly asymmetric damage\nresponse across the multilayer interfaces. Zr layers exhibit larger He bubbles,\nhigher swelling, and greater helium retention while Nb layers develop\nbubble-denuded zones exclusively around the interfaces, where bubble nucleation\nis strongly suppressed and swelling is limited. This asymmetry arises from\ndifferences in atomic transport properties DFT calculations show lower\nmigration barriers for vacancies and He atoms in Nb, enabling efficient defect\nmigration and recombination at interfaces, whereas Zr retains defects due to\nhigher migration barriers. EELS and DBS PALS measurements confirm bubble\ndensities and the presence of sub-nanometer open volumes. Compared to\nmonolithic samples, the ZrNb multilayers exhibit lower irradiation induced\nhardening and reduced He retention. These findings highlight the role of\ninterfaces in driving asymmetric radiation damage and demonstrate the\neffectiveness of BCC Nb layers in mitigating defect growth. Overall, ZrNb\nmultilayers are established as a superior alternative to conventional single\nand polycrystalline materials for extreme irradiation environments.", "AI": {"tldr": "ZrNb\u7eb3\u7c73\u91d1\u5c5e\u591a\u5c42\u819c\u4e2d\u7684\u8f90\u5c04\u5f15\u8d77\u7684\u6c26\u6c14\u6ce1\u5f62\u6210\u548c\u7f3a\u9677\u6f14\u53d8\u3002Zr\u5c42\u6bd4Nb\u5c42\u8868\u73b0\u51fa\u66f4\u5927\u7684\u6c26\u6c14\u6ce1\u548c\u66f4\u9ad8\u7684\u80bf\u80c0\uff0c\u800cNb\u5c42\u5728\u754c\u9762\u5468\u56f4\u5f62\u6210\u65e0\u6c14\u6ce1\u533a\u3002\u8fd9\u662f\u7531\u4e8eZr\u548cNb\u4e4b\u95f4\u539f\u5b50\u4f20\u8f93\u6027\u8d28\u7684\u5dee\u5f02\u3002ZrNb\u591a\u5c42\u819c\u6bd4\u5355\u4e00\u6750\u6599\u5177\u6709\u66f4\u597d\u7684\u6297\u8f90\u7167\u6027\u80fd\u3002", "motivation": "\u6838\u80fd\u7cfb\u7edf\u4e2d\u6750\u6599\u7684\u7ed3\u6784\u5b8c\u6574\u6027\u9762\u4e34\u7740\u8f90\u5c04\u5f15\u8d77\u7684\u6c26\u6c14\u6ce1\u5f62\u6210\u7684\u6311\u6218\u3002", "method": "\u4f7f\u752880 keV\u6c26\u79bb\u5b50\u8f90\u7167ZrNb\u7eb3\u7c73\u91d1\u5c5e\u591a\u5c42\u819c\u3001\u5355\u6676\u94cc\u548c\u591a\u6676\u9506\u3002\u91c7\u7528\u900f\u5c04\u7535\u5b50\u663e\u5fae\u955c\u3001\u4e8c\u6b21\u79bb\u5b50\u8d28\u8c31\u3001\u626b\u63cf\u900f\u5c04\u7535\u5b50\u663e\u5fae\u955c\u7535\u5b50\u80fd\u91cf\u635f\u5931\u8c31\u3001\u7eb3\u7c73\u538b\u75d5\u3001\u591a\u666e\u52d2\u5c55\u5bbd\u6b63\u7535\u5b50\u6e6e\u6ca1\u5149\u8c31\u3001\u6b63\u7535\u5b50\u6e6e\u6ca1\u5bff\u547d\u5149\u8c31\u548c\u539f\u5b50\u6a21\u62df\u7b49\u65b9\u6cd5\u8fdb\u884c\u5206\u6790\u3002", "result": "Zr\u5c42\u663e\u793a\u51fa\u66f4\u5927\u7684\u6c26\u6c14\u6ce1\u3001\u66f4\u9ad8\u7684\u80bf\u80c0\u548c\u6c26\u4fdd\u7559\uff0c\u800cNb\u5c42\u5728\u754c\u9762\u9644\u8fd1\u5f62\u6210\u65e0\u6c14\u6ce1\u533a\u3002ZrNb\u591a\u5c42\u819c\u6bd4\u5355\u4e00\u6750\u6599\u8868\u73b0\u51fa\u8f83\u4f4e\u7684\u8f90\u7167\u786c\u5316\u548c\u6c26\u4fdd\u7559\u3002DFT\u8ba1\u7b97\u8868\u660eNb\u7684\u7a7a\u4f4d\u548c\u6c26\u539f\u5b50\u8fc1\u79fb\u52bf\u5792\u8f83\u4f4e\u3002", "conclusion": "ZrNb\u7eb3\u7c73\u591a\u5c42\u819c\u4e2d\u7684\u63a5\u53e3\u5728\u9a71\u52a8\u4e0d\u5bf9\u79f0\u8f90\u5c04\u635f\u4f24\u65b9\u9762\u8d77\u7740\u5173\u952e\u4f5c\u7528\uff0c\u5e76\u4e14Nb\u5c42\u5728\u6291\u5236\u7f3a\u9677\u751f\u957f\u65b9\u9762\u662f\u6709\u6548\u7684\u3002ZrNb\u591a\u5c42\u819c\u662f\u6bd4\u4f20\u7edf\u6750\u6599\u66f4\u80fd\u62b5\u6297\u6781\u7aef\u8f90\u7167\u73af\u5883\u7684\u4f18\u9009\u6750\u6599\u3002"}}
{"id": "2509.19275", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.19275", "abs": "https://arxiv.org/abs/2509.19275", "authors": ["Junzhe Song", "Ruisi He", "Mi Yang", "Zhengyu Zhang", "Xinwen Chen", "Xiaoying Zhang", "Bo Ai"], "title": "A Novel Site-Specific Inference Model for Urban Canyon Channels: From Measurements to Modeling", "comment": null, "summary": "With the rapid development of intelligent transportation and smart city\napplications, urban canyon has become a critical scenario for the design and\nevaluation of wireless communication systems. Due to its unique environmental\nlayout, the channel characteristics in urban canyon are strongly a street\ngeometry and building distribution, thereby exhibiting significant\nsite-specific channel condition. However, this feature has not been well\ncaptured in existing channel models. In this paper, we propose a site-specific\nchannel inference model based on environmental geometry, the model is\nparameterized using sub-6GHz channel measurements. Multipath components (MPCs)\nare extracted and clustered according to geometric propagation, which are\nexplicitly derived from the influence of canyon width, thereby establishing an\ninterpretable mapping between the physical environment and statistical\ncharacteristics of MPCs. A step-by-step implementation scheme is presented.\nSubsequently, the proposed site-specific channel inference model is validated\nby comparing second-order statistics of channels, derived from the model and\nmeasurements. The results show that the proposed model achieves high accuracy\nand robustness in different urban canyon scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u73af\u5883\u51e0\u4f55\u7684\u573a\u5730\u7279\u5b9a\u4fe1\u9053\u63a8\u7406\u6a21\u578b\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u4fe1\u9053\u6a21\u578b\u672a\u80fd\u5145\u5206\u6355\u6349\u57ce\u5e02\u5ce1\u8c37\u573a\u666f\u4e0b\u4fe1\u9053\u7279\u6027\u7684\u95ee\u9898\u3002\u8be5\u6a21\u578b\u5229\u7528\u4e9a 6GHz \u4fe1\u9053\u6d4b\u91cf\u6570\u636e\u8fdb\u884c\u53c2\u6570\u5316\uff0c\u901a\u8fc7\u805a\u7c7b\u5206\u6790\u4e0e\u51e0\u4f55\u4f20\u64ad\u76f8\u5173\u7684\u591a\u5f84\u5206\u91cf\uff08MPCs\uff09\uff0c\u5efa\u7acb\u4e86\u7269\u7406\u73af\u5883\u4e0e MPCs \u7edf\u8ba1\u7279\u6027\u4e4b\u95f4\u7684\u53ef\u89e3\u91ca\u6620\u5c04\u5173\u7cfb\u3002\u901a\u8fc7\u4e0e\u6d4b\u91cf\u6570\u636e\u7684\u4e8c\u9636\u7edf\u8ba1\u91cf\u5bf9\u6bd4\uff0c\u9a8c\u8bc1\u4e86\u8be5\u6a21\u578b\u5728\u4e0d\u540c\u57ce\u5e02\u5ce1\u8c37\u573a\u666f\u4e0b\u5177\u6709\u9ad8\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u4fe1\u9053\u6a21\u578b\u672a\u80fd\u5145\u5206\u6355\u6349\u57ce\u5e02\u5ce1\u8c37\u573a\u666f\u4e0b\u4fe1\u9053\u7279\u6027\u7684\u7279\u70b9\uff0c\u800c\u57ce\u5e02\u5ce1\u8c37\u573a\u666f\u5bf9\u4e8e\u667a\u80fd\u4ea4\u901a\u548c\u667a\u6167\u57ce\u5e02\u5e94\u7528\u4e2d\u7684\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\u8bbe\u8ba1\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u73af\u5883\u51e0\u4f55\u7684\u573a\u5730\u7279\u5b9a\u4fe1\u9053\u63a8\u7406\u6a21\u578b\u3002\u8be5\u6a21\u578b\u4f7f\u7528\u4e9a 6GHz \u4fe1\u9053\u6d4b\u91cf\u6570\u636e\u8fdb\u884c\u53c2\u6570\u5316\uff0c\u63d0\u53d6\u5e76\u6839\u636e\u51e0\u4f55\u4f20\u64ad\uff08\u7279\u522b\u662f\u5ce1\u8c37\u5bbd\u5ea6\uff09\u5f71\u54cd\u5bf9\u591a\u5f84\u5206\u91cf\uff08MPCs\uff09\u8fdb\u884c\u805a\u7c7b\uff0c\u4ece\u800c\u5efa\u7acb\u7269\u7406\u73af\u5883\u4e0e MPCs \u7edf\u8ba1\u7279\u6027\u4e4b\u95f4\u7684\u6620\u5c04\u5173\u7cfb\u3002", "result": "\u6240\u63d0\u51fa\u7684\u6a21\u578b\u5728\u4e0d\u540c\u57ce\u5e02\u5ce1\u8c37\u573a\u666f\u4e0b\uff0c\u901a\u8fc7\u4e0e\u6d4b\u91cf\u6570\u636e\u7684\u4e8c\u9636\u7edf\u8ba1\u91cf\u5bf9\u6bd4\uff0c\u8bc1\u660e\u4e86\u5176\u9ad8\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u57fa\u4e8e\u73af\u5883\u51e0\u4f55\u7684\u573a\u5730\u7279\u5b9a\u4fe1\u9053\u63a8\u7406\u6a21\u578b\u80fd\u591f\u51c6\u786e\u4e14\u7a33\u5065\u5730\u6355\u6349\u57ce\u5e02\u5ce1\u8c37\u573a\u666f\u4e0b\u7684\u4fe1\u9053\u7279\u6027\u3002"}}
{"id": "2509.18592", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.18592", "abs": "https://arxiv.org/abs/2509.18592", "authors": ["Neel P. Bhatt", "Yunhao Yang", "Rohan Siva", "Pranay Samineni", "Daniel Milan", "Zhangyang Wang", "Ufuk Topcu"], "title": "VLN-Zero: Rapid Exploration and Cache-Enabled Neurosymbolic Vision-Language Planning for Zero-Shot Transfer in Robot Navigation", "comment": "Codebase, datasets, and videos for VLN-Zero are available at:\n  https://vln-zero.github.io/", "summary": "Rapid adaptation in unseen environments is essential for scalable real-world\nautonomy, yet existing approaches rely on exhaustive exploration or rigid\nnavigation policies that fail to generalize. We present VLN-Zero, a two-phase\nvision-language navigation framework that leverages vision-language models to\nefficiently construct symbolic scene graphs and enable zero-shot neurosymbolic\nnavigation. In the exploration phase, structured prompts guide VLM-based search\ntoward informative and diverse trajectories, yielding compact scene graph\nrepresentations. In the deployment phase, a neurosymbolic planner reasons over\nthe scene graph and environmental observations to generate executable plans,\nwhile a cache-enabled execution module accelerates adaptation by reusing\npreviously computed task-location trajectories. By combining rapid exploration,\nsymbolic reasoning, and cache-enabled execution, the proposed framework\novercomes the computational inefficiency and poor generalization of prior\nvision-language navigation methods, enabling robust and scalable\ndecision-making in unseen environments. VLN-Zero achieves 2x higher success\nrate compared to state-of-the-art zero-shot models, outperforms most fine-tuned\nbaselines, and reaches goal locations in half the time with 55% fewer VLM calls\non average compared to state-of-the-art models across diverse environments.\nCodebase, datasets, and videos for VLN-Zero are available at:\nhttps://vln-zero.github.io/.", "AI": {"tldr": "VLN-Zero\u662f\u4e00\u4e2a\u521b\u65b0\u7684\u89c6\u89c9-\u8bed\u8a00\u5bfc\u822a\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u65b9\u6cd5\uff08\u63a2\u7d22\u548c\u90e8\u7f72\uff09\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u96f6\u6837\u672c\u5bfc\u822a\u3002\u5b83\u5229\u7528\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u6784\u5efa\u573a\u666f\u56fe\uff0c\u5e76\u7ed3\u5408\u795e\u7ecf\u7b26\u53f7\u89c4\u5212\u5668\u8fdb\u884c\u63a8\u7406\u548c\u6267\u884c\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5728\u672a\u77e5\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u6210\u529f\u7387\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u5bfc\u822a\u65b9\u6cd5\u5728\u63a2\u7d22\u6216\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u65e0\u6cd5\u6ee1\u8db3\u73b0\u5b9e\u4e16\u754c\u81ea\u4e3b\u6027\u7684\u9700\u6c42\u3002", "method": "VLN-Zero\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u9636\u6bb5\uff1a1. \u63a2\u7d22\u9636\u6bb5\uff1a\u5229\u7528\u7ed3\u6784\u5316\u63d0\u793a\u5f15\u5bfc\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u8fdb\u884c\u4fe1\u606f\u6027\u548c\u591a\u6837\u6027\u7684\u8f68\u8ff9\u641c\u7d22\uff0c\u751f\u6210\u573a\u666f\u56fe\u30022. \u90e8\u7f72\u9636\u6bb5\uff1a\u795e\u7ecf\u7b26\u53f7\u89c4\u5212\u5668\u57fa\u4e8e\u573a\u666f\u56fe\u548c\u73af\u5883\u89c2\u6d4b\u751f\u6210\u6267\u884c\u8ba1\u5212\uff0c\u540c\u65f6\u5229\u7528\u7f13\u5b58\u52a0\u901f\u6267\u884c\u8fc7\u7a0b\u3002", "result": "VLN-Zero\u7684\u6210\u529f\u7387\u662f\u73b0\u6709\u6700\u5148\u8fdb\u96f6\u6837\u672c\u6a21\u578b\u7684\u4e24\u500d\uff0c\u5e76\u4e14\u4f18\u4e8e\u5927\u591a\u6570\u7ecf\u8fc7\u5fae\u8c03\u7684\u6a21\u578b\u3002\u5b83\u80fd\u5728\u66f4\u77ed\u7684\u65f6\u95f4\u5185\u5230\u8fbe\u76ee\u6807\u5730\u70b9\uff0c\u5e76\u4e14VLM\u8c03\u7528\u6b21\u6570\u5e73\u5747\u51cf\u5c11\u4e8655%\u3002", "conclusion": "VLN-Zero\u901a\u8fc7\u7ed3\u5408\u5feb\u901f\u63a2\u7d22\u3001\u7b26\u53f7\u63a8\u7406\u548c\u7f13\u5b58\u6267\u884c\uff0c\u514b\u670d\u4e86\u5148\u524d\u89c6\u89c9-\u8bed\u8a00\u5bfc\u822a\u65b9\u6cd5\u7684\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\u548c\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5728\u672a\u77e5\u73af\u5883\u4e2d\u9c81\u68d2\u4e14\u53ef\u6269\u5c55\u7684\u51b3\u7b56\u80fd\u529b\u3002"}}
{"id": "2509.18193", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18193", "abs": "https://arxiv.org/abs/2509.18193", "authors": ["Omar H. Khater", "Abdul Jabbar Siddiqui", "Aiman El-Maleh", "M. Shamim Hossain"], "title": "TinyEcoWeedNet: Edge Efficient Real-Time Aerial Agricultural Weed Detection", "comment": null, "summary": "Deploying deep learning models in agriculture is difficult because edge\ndevices have limited resources, but this work presents a compressed version of\nEcoWeedNet using structured channel pruning, quantization-aware training (QAT),\nand acceleration with NVIDIA's TensorRT on the Jetson Orin Nano. Despite the\nchallenges of pruning complex architectures with residual shortcuts, attention\nmechanisms, concatenations, and CSP blocks, the model size was reduced by up to\n68.5% and computations by 3.2 GFLOPs, while inference speed reached 184 FPS at\nFP16, 28.7% faster than the baseline. On the CottonWeedDet12 dataset, the\npruned EcoWeedNet with a 39.5% pruning ratio outperformed YOLO11n and YOLO12n\n(with only 20% pruning), achieving 83.7% precision, 77.5% recall, and 85.9%\nmAP50, proving it to be both efficient and effective for precision agriculture.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u538b\u7f29\u7684 EcoWeedNet \u6a21\u578b\uff0c\u7528\u4e8e\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u8fdb\u884c\u519c\u4e1a\u5e94\u7528\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u901a\u9053\u526a\u679d\u3001\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\u548c TensorRT \u52a0\u901f\uff0c\u663e\u8457\u51cf\u5c0f\u4e86\u6a21\u578b\u5c3a\u5bf8\u548c\u8ba1\u7b97\u91cf\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u63a8\u7406\u901f\u5ea6\uff0c\u5e76\u5728 CottonWeedDet12 \u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u5728\u519c\u4e1a\u9886\u57df\u90e8\u7f72\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u9762\u4e34\u8fb9\u7f18\u8bbe\u5907\u8d44\u6e90\u6709\u9650\u7684\u6311\u6218\u3002", "method": "\u4f7f\u7528\u7ed3\u6784\u5316\u901a\u9053\u526a\u679d\u3001\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\uff08QAT\uff09\u548c NVIDIA \u7684 TensorRT \u5728 Jetson Orin Nano \u4e0a\u52a0\u901f\u538b\u7f29 EcoWeedNet \u6a21\u578b\u3002\u89e3\u51b3\u4e86\u526a\u679d\u590d\u6742\u7f51\u7edc\u7ed3\u6784\uff08\u5177\u6709\u6b8b\u5dee\u6377\u5f84\u3001\u6ce8\u610f\u529b\u673a\u5236\u3001\u8fde\u63a5\u548c CSP \u5757\uff09\u7684\u6311\u6218\u3002", "result": "\u6a21\u578b\u5c3a\u5bf8\u51cf\u5c0f\u9ad8\u8fbe 68.5%\uff0c\u8ba1\u7b97\u91cf\u51cf\u5c11 3.2 GFLOPs\uff0cFP16 \u63a8\u7406\u901f\u5ea6\u8fbe\u5230 184 FPS\uff0c\u6bd4\u57fa\u7ebf\u5feb 28.7%\u3002\u5728 CottonWeedDet12 \u6570\u636e\u96c6\u4e0a\uff0c\u526a\u679d\u540e\u7684 EcoWeedNet\uff08\u526a\u679d\u7387\u4e3a 39.5%\uff09\u5728\u7cbe\u5ea6\u3001\u53ec\u56de\u7387\u548c mAP50 \u65b9\u9762\u4f18\u4e8e YOLO11n \u548c YOLO12n\uff08\u526a\u679d\u7387\u4ec5\u4e3a 20%\uff09\uff0c\u5206\u522b\u8fbe\u5230 83.7%\u300177.5% \u548c 85.9%\u3002", "conclusion": "\u538b\u7f29\u540e\u7684 EcoWeedNet \u5728\u7cbe\u5ea6\u548c\u6548\u7387\u65b9\u9762\u90fd\u8868\u73b0\u51fa\u8272\uff0c\u9002\u7528\u4e8e\u7cbe\u51c6\u519c\u4e1a\u3002"}}
{"id": "2509.18479", "categories": ["quant-ph", "cs.CV", "physics.optics"], "pdf": "https://arxiv.org/pdf/2509.18479", "abs": "https://arxiv.org/abs/2509.18479", "authors": ["Louis Rossignol", "Tangui Aladjidi", "Myrann Baker-Rasooli", "Quentin Glorieux"], "title": "Machine learning approach to single-shot multiparameter estimation for the non-linear Schr\u00f6dinger equation", "comment": "10 pages, 4 figures", "summary": "The nonlinear Schr\\\"odinger equation (NLSE) is a fundamental model for wave\ndynamics in nonlinear media ranging from optical fibers to Bose-Einstein\ncondensates. Accurately estimating its parameters, which are often strongly\ncorrelated, from a single measurement remains a significant challenge. We\naddress this problem by treating parameter estimation as an inverse problem and\ntraining a neural network to invert the NLSE mapping. We combine a fast\nnumerical solver with a machine learning approach based on the ConvNeXt\narchitecture and a multivariate Gaussian negative log-likelihood loss function.\nFrom single-shot field (density and phase) images, our model estimates three\nkey parameters: the nonlinear coefficient $n_2$, the saturation intensity\n$I_{sat}$, and the linear absorption coefficient $\\alpha$. Trained on 100,000\nsimulated images, the model achieves a mean absolute error of $3.22\\%$ on\n12,500 unseen test samples, demonstrating strong generalization and close\nagreement with ground-truth values. This approach provides an efficient route\nfor characterizing nonlinear systems and has the potential to bridge\ntheoretical modeling and experimental data when realistic noise is\nincorporated.", "AI": {"tldr": "\u672c\u7814\u7a76\u4f7f\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08ConvNeXt\uff09\u548c\u591a\u5143\u9ad8\u65af\u8d1f\u5bf9\u6570\u4f3c\u7136\u635f\u5931\u51fd\u6570\uff0c\u5c06\u53c2\u6570\u4f30\u8ba1\u89c6\u4e3a\u9006\u95ee\u9898\uff0c\u901a\u8fc7\u5355\u6b21\u6d4b\u91cf\u56fe\u50cf\uff08\u5bc6\u5ea6\u548c\u76f8\u4f4d\uff09\u51c6\u786e\u4f30\u8ba1\u975e\u7ebf\u6027\u859b\u5b9a\u8c14\u65b9\u7a0b\uff08NLSE\uff09\u7684\u5173\u952e\u53c2\u6570\uff08\u975e\u7ebf\u6027\u7cfb\u6570 $n_2$\u3001\u9971\u548c\u5f3a\u5ea6 $I_{sat}$ \u548c\u7ebf\u6027\u5438\u6536\u7cfb\u6570 $\\alpha$\uff09\u3002", "motivation": "\u4ece\u5355\u6b21\u6d4b\u91cf\u4e2d\u51c6\u786e\u4f30\u8ba1\u975e\u7ebf\u6027\u859b\u5b9a\u8c14\u65b9\u7a0b\uff08NLSE\uff09\u7684\u53c2\u6570\uff0c\u8fd9\u4e9b\u53c2\u6570\u901a\u5e38\u662f\u5f3a\u76f8\u5173\u7684\uff0c\u4ecd\u7136\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002", "method": "\u5c06\u53c2\u6570\u4f30\u8ba1\u89c6\u4e3a\u9006\u95ee\u9898\uff0c\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u6765\u53cd\u6f14NLSE\u6620\u5c04\u3002\u7ed3\u5408\u4e86\u5feb\u901f\u6570\u503c\u6c42\u89e3\u5668\u3001\u57fa\u4e8eConvNeXt\u67b6\u6784\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u4ee5\u53ca\u591a\u5143\u9ad8\u65af\u8d1f\u5bf9\u6570\u4f3c\u7136\u635f\u5931\u51fd\u6570\u3002", "result": "\u5728100,000\u5f20\u6a21\u62df\u56fe\u50cf\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\uff0c\u572812,500\u4e2a\u672a\u89c1\u8fc7\u6d4b\u8bd5\u6837\u672c\u4e0a\u5b9e\u73b0\u4e863.22%\u7684\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\uff0c\u8bc1\u660e\u4e86\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u548c\u4e0e\u771f\u5b9e\u503c\u7684\u9ad8\u5ea6\u4e00\u81f4\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u8868\u5f81\u975e\u7ebf\u6027\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u6761\u6709\u6548\u7684\u9014\u5f84\uff0c\u5e76\u4e14\u5728\u8003\u8651\u5b9e\u9645\u566a\u58f0\u65f6\uff0c\u6709\u6f5c\u529b\u5728\u7406\u8bba\u6a21\u578b\u548c\u5b9e\u9a8c\u6570\u636e\u4e4b\u95f4\u67b6\u8d77\u6865\u6881\u3002"}}
{"id": "2509.18119", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18119", "abs": "https://arxiv.org/abs/2509.18119", "authors": ["Yifan Xu", "Xiao Liu", "Xinghan Liu", "Jiaqi Fu", "Hanchen Zhang", "Bohao Jing", "Shudan Zhang", "Yuting Wang", "Wenyi Zhao", "Yuxiao Dong"], "title": "MobileRL: Online Agentic Reinforcement Learning for Mobile GUI Agents", "comment": null, "summary": "Building general-purpose graphical user interface (GUI) agents has become\nincreasingly promising with the progress in vision language models. However,\ndeveloping effective mobile GUI agents with reinforcement learning (RL) remains\nchallenging due to the heavy-tailed distribution of task difficulty and the\ninefficiency of large-scale environment sampling. We present an online agentic\nreinforcement learning framework MOBILERL to enhance GUI agents in mobile\nenvironments. Its core component is the Difficulty-Adaptive GRPO (ADAGRPO)\nalgorithm. In ADAGRPO, we design difficulty-adaptive positive replay and\nfailure curriculum filtering to adapt the model to different task difficulties.\nWe introduce the shortest path reward adjustment strategy to reshape rewards\nconcerning the task length in multi-turn agentic tasks. Those strategies\njointly stabilize RL training, improve sample efficiency, and generate strong\nperformance across diverse mobile apps and tasks. We apply MOBILERL to two open\nmodels (Qwen2.5-VL-7B-Instruct and GLM-4.1V-9B-Base). The resultant MOBILERL-9B\nmodel achieves state-of-the-art results in terms of success rates on both\nAndroidWorld (75.8%) and AndroidLab (46.8%). The MOBILERL framework is adopted\nin the AutoGLM products, and also open-sourced at\nhttps://github.com/THUDM/MobileRL.", "AI": {"tldr": "MOBILERL\u662f\u4e00\u4e2a\u7528\u4e8e\u589e\u5f3a\u79fb\u52a8\u73af\u5883GUI\u667a\u80fd\u4f53\u7684\u5728\u7ebf\u667a\u80fd\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7ADAGRPO\u7b97\u6cd5\uff08\u5305\u62ec\u96be\u5ea6\u81ea\u9002\u5e94\u6b63\u5411\u56de\u653e\u548c\u5931\u8d25\u8bfe\u7a0b\u8fc7\u6ee4\uff09\u4ee5\u53ca\u6700\u77ed\u8def\u5f84\u5956\u52b1\u8c03\u6574\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4efb\u52a1\u96be\u5ea6\u5206\u5e03\u4e0d\u5747\u548c\u73af\u5883\u91c7\u6837\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u4ece\u800c\u7a33\u5b9a\u4e86\u8bad\u7ec3\uff0c\u63d0\u9ad8\u4e86\u6837\u672c\u6548\u7387\uff0c\u5e76\u5728AndroidWorld\u548cAndroidLab\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6210\u679c\u3002", "motivation": "\u79fb\u52a8GUI\u667a\u80fd\u4f53\u5728\u5f3a\u5316\u5b66\u4e60\u65b9\u9762\u4ecd\u9762\u4e34\u4e25\u5cfb\u6311\u6218\uff0c\u4e3b\u8981\u7531\u4e8e\u4efb\u52a1\u96be\u5ea6\u5206\u5e03\u4e0d\u5747\u548c\u5927\u89c4\u6a21\u73af\u5883\u91c7\u6837\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51faMOBILERL\u6846\u67b6\uff0c\u5176\u6838\u5fc3\u662fADAGRPO\u7b97\u6cd5\u3002ADAGRPO\u5305\u542b\u96be\u5ea6\u81ea\u9002\u5e94\u6b63\u5411\u56de\u653e\u548c\u5931\u8d25\u8bfe\u7a0b\u8fc7\u6ee4\uff0c\u5e76\u5f15\u5165\u4e86\u6700\u77ed\u8def\u5f84\u5956\u52b1\u8c03\u6574\u7b56\u7565\u3002", "result": "\u5c06MOBILERL\u5e94\u7528\u4e8eQwen2.5-VL-7B-Instruct\u548cGLM-4.1V-9B-Base\u4e24\u4e2a\u5f00\u6e90\u6a21\u578b\u3002MOBILERL-9B\u5728AndroidWorld\uff0875.8%\uff09\u548cAndroidLab\uff0846.8%\uff09\u4e0a\u5747\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6210\u529f\u7387\u3002", "conclusion": "MOBILERL\u6846\u67b6\u901a\u8fc7\u521b\u65b0\u7684\u7b97\u6cd5\u548c\u7b56\u7565\uff0c\u6709\u6548\u63d0\u5347\u4e86\u79fb\u52a8GUI\u667a\u80fd\u4f53\u7684\u6027\u80fd\uff0c\u5df2\u6210\u529f\u5e94\u7528\u4e8eAutoGLM\u4ea7\u54c1\u5e76\u5f00\u6e90\u3002"}}
{"id": "2509.18230", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18230", "abs": "https://arxiv.org/abs/2509.18230", "authors": ["Zihan Dong", "Xinyu Fan", "Zixiang Tang", "Yunqing Li"], "title": "Towards General Computer Control with Hierarchical Agents and Multi-Level Action Spaces", "comment": null, "summary": "Controlling desktop applications via software remains a fundamental yet\nunder-served problem. Existing multi-modal large language models (MLLMs) ingest\nscreenshots and task instructions to generate keystrokes and mouse events, but\nthey suffer from prohibitive inference latency, poor sample efficiency on\nlong-horizon sparse-reward tasks, and infeasible on-device deployment. We\nintroduce a lightweight hierarchical reinforcement learning framework,\nComputerAgent, that formulates OS control as a two-level option process\n(manager and subpolicy), employs a triple-modal state encoder (screenshot, task\nID, numeric state) to handle visual and contextual diversity, integrates\nmeta-actions with an early-stop mechanism to reduce wasted interactions, and\nuses a compact vision backbone plus small policy networks for on-device\ninference (15M parameters). On a suite of 135 real-world desktop tasks,\nComputerAgent attains 92.1% success on simple tasks (<8 steps) and 58.8% on\nhard tasks (>=8 steps), matching or exceeding 200B-parameter MLLM baselines on\nsimple scenarios while reducing model size by over four orders of magnitude and\nhalving inference time. These results demonstrate that hierarchical RL offers a\npractical, scalable alternative to monolithic MLLM-based automation for\ncomputer control.", "AI": {"tldr": "ComputerAgent\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u72b6\u6001\u7f16\u7801\u548c\u5143\u52a8\u4f5c\u4e0e\u65e9\u505c\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u684c\u9762\u5e94\u7528\u63a7\u5236\uff0c\u53c2\u6570\u91cf\u4ec5\u4e3a15M\uff0c\u5728\u771f\u5b9e\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u5927\u578b\u6a21\u578b\u3002", "motivation": "\u63a7\u5236\u684c\u9762\u5e94\u7528\u7a0b\u5e8f\u662f\u4e00\u4e2a\u57fa\u672c\u4f46\u670d\u52a1\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u73b0\u6709\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5b58\u5728\u63a8\u7406\u5ef6\u8fdf\u9ad8\u3001\u6837\u672c\u6548\u7387\u4f4e\u548c\u8bbe\u5907\u90e8\u7f72\u4e0d\u53ef\u884c\u7684\u95ee\u9898\u3002", "method": "ComputerAgent\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5b83\u5c06\u64cd\u4f5c\u7cfb\u7edf\u63a7\u5236\u8bbe\u8ba1\u4e3a\u4e00\u4e2a\u4e24\u7ea7\u9009\u9879\u8fc7\u7a0b\uff08\u7ba1\u7406\u5668\u548c\u5b50\u7b56\u7565\uff09\uff0c\u91c7\u7528\u4e09\u6a21\u6001\u72b6\u6001\u7f16\u7801\u5668\uff08\u5c4f\u5e55\u622a\u56fe\u3001\u4efb\u52a1ID\u3001\u6570\u503c\u72b6\u6001\uff09\u6765\u5904\u7406\u89c6\u89c9\u548c\u4e0a\u4e0b\u6587\u7684\u591a\u6837\u6027\uff0c\u96c6\u6210\u5143\u52a8\u4f5c\u548c\u65e9\u505c\u673a\u5236\u4ee5\u51cf\u5c11\u65e0\u6548\u4ea4\u4e92\uff0c\u5e76\u4f7f\u7528\u7d27\u51d1\u7684\u89c6\u89c9\u9aa8\u5e72\u7f51\u7edc\u548c\u5c0f\u578b\u7b56\u7565\u7f51\u7edc\u6765\u5b9e\u73b0\u8bbe\u5907\u7aef\u63a8\u7406\uff0815M\u53c2\u6570\uff09\u3002", "result": "\u5728135\u4e2a\u771f\u5b9e\u684c\u9762\u4efb\u52a1\u7684\u6d4b\u8bd5\u4e2d\uff0cComputerAgent\u5728\u7b80\u5355\u4efb\u52a1\uff08<8\u6b65\uff09\u4e0a\u5b9e\u73b0\u4e8692.1%\u7684\u6210\u529f\u7387\uff0c\u5728\u56f0\u96be\u4efb\u52a1\uff08>=8\u6b65\uff09\u4e0a\u5b9e\u73b0\u4e8658.8%\u7684\u6210\u529f\u7387\uff0c\u5728\u7b80\u5355\u573a\u666f\u4e0b\u5176\u8868\u73b0\u4e0e200B\u53c2\u6570\u7684MLLMs\u57fa\u7ebf\u76f8\u5f53\u6216\u66f4\u4f18\uff0c\u540c\u65f6\u6a21\u578b\u5c3a\u5bf8\u51cf\u5c0f\u4e86\u56db\u4e2a\u6570\u91cf\u7ea7\uff0c\u63a8\u7406\u65f6\u95f4\u7f29\u77ed\u4e86\u4e00\u534a\u3002", "conclusion": "\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u4e3a\u8ba1\u7b97\u673a\u63a7\u5236\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u3001\u53ef\u6269\u5c55\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u80fd\u591f\u53d6\u4ee3\u5355\u4f53MLLM\u9a71\u52a8\u7684\u81ea\u52a8\u5316\u3002"}}
{"id": "2509.18439", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18439", "abs": "https://arxiv.org/abs/2509.18439", "authors": ["Oscar J. Ponce-Ponte", "David Toro-Tobon", "Luis F. Figueroa", "Michael Gionfriddo", "Megan Branda", "Victor M. Montori", "Saturnino Luz", "Juan P. Brito"], "title": "Developing an AI framework to automatically detect shared decision-making in patient-doctor conversations", "comment": "53 pages, 1 figure, 4 tables, 5 supplementary figures, 13\n  supplementary tables", "summary": "Shared decision-making (SDM) is necessary to achieve patient-centred care.\nCurrently no methodology exists to automatically measure SDM at scale. This\nstudy aimed to develop an automated approach to measure SDM by using language\nmodelling and the conversational alignment (CA) score. A total of 157\nvideo-recorded patient-doctor conversations from a randomized multi-centre\ntrial evaluating SDM decision aids for anticoagulation in atrial fibrillations\nwere transcribed and segmented into 42,559 sentences. Context-response pairs\nand negative sampling were employed to train deep learning (DL) models and\nfine-tuned BERT models via the next sentence prediction (NSP) task. Each\ntop-performing model was used to calculate four types of CA scores. A\nrandom-effects analysis by clinician, adjusting for age, sex, race, and trial\narm, assessed the association between CA scores and SDM outcomes: the\nDecisional Conflict Scale (DCS) and the Observing Patient Involvement in\nDecision-Making 12 (OPTION12) scores. p-values were corrected for multiple\ncomparisons with the Benjamini-Hochberg method. Among 157 patients (34% female,\nmean age 70 SD 10.8), clinicians on average spoke more words than patients\n(1911 vs 773). The DL model without the stylebook strategy achieved a recall@1\nof 0.227, while the fine-tuned BERTbase (110M) achieved the highest recall@1\nwith 0.640. The AbsMax (18.36 SE7.74 p=0.025) and Max CA (21.02 SE7.63 p=0.012)\nscores generated with the DL without stylebook were associated with OPTION12.\nThe Max CA score generated with the fine-tuned BERTbase (110M) was associated\nwith the DCS score (-27.61 SE12.63 p=0.037). BERT model sizes did not have an\nimpact the association between CA scores and SDM. This study introduces an\nautomated, scalable methodology to measure SDM in patient-doctor conversations\nthrough explainable CA scores, with potential to evaluate SDM strategies at\nscale.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u6d4b\u91cf\u60a3\u8005-\u533b\u751f\u5171\u4eab\u51b3\u7b56\uff08SDM\uff09\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u8bed\u8a00\u6a21\u578b\u548c\u5bf9\u8bdd\u4e00\u81f4\u6027\uff08CA\uff09\u5f97\u5206\u6765\u8bc4\u4f30SDM\u3002", "motivation": "\u76ee\u524d\u7f3a\u4e4f\u81ea\u52a8\u6d4b\u91cfSDM\u7684\u65b9\u6cd5\uff0c\u800cSDM\u5bf9\u4e8e\u5b9e\u73b0\u4ee5\u60a3\u8005\u4e3a\u4e2d\u5fc3\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u5bf9157\u4e2a\u60a3\u8005-\u533b\u751f\u5bf9\u8bdd\u8fdb\u884c\u8f6c\u5f55\u548c\u5206\u6bb5\uff0c\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\uff08DL\uff09\u6a21\u578b\u548c\u5fae\u8c03\u7684BERT\u6a21\u578b\uff08\u901a\u8fc7\u4e0b\u4e00\u4e2a\u53e5\u5b50\u9884\u6d4b\u4efb\u52a1\uff09\u6765\u8bad\u7ec3\u6a21\u578b\uff0c\u5e76\u8ba1\u7b97\u56db\u79cdCA\u5f97\u5206\u3002\u7136\u540e\uff0c\u4f7f\u7528\u968f\u673a\u6548\u5e94\u6a21\u578b\u8bc4\u4f30CA\u5f97\u5206\u4e0eSDM\u7ed3\u679c\uff08DCS\u548cOPTION12\u5f97\u5206\uff09\u4e4b\u95f4\u7684\u5173\u8054\u3002", "result": "BERTbase\u6a21\u578b\u8fbe\u5230\u4e86\u6700\u9ad8\u7684\u53ec\u56de\u7387@1\uff080.640\uff09\u3002\u90e8\u5206CA\u5f97\u5206\u4e0eOPTION12\u548cDCS\u5f97\u5206\u76f8\u5173\uff0c\u8868\u660e\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u53ef\u4ee5\u8861\u91cfSDM\u3002", "conclusion": "\u672c\u7814\u7a76\u6210\u529f\u5f00\u53d1\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u3001\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u53ef\u89e3\u91ca\u7684CA\u5f97\u5206\u6765\u6d4b\u91cf\u60a3\u8005-\u533b\u751f\u5bf9\u8bdd\u4e2d\u7684SDM\uff0c\u6709\u6f5c\u529b\u5927\u89c4\u6a21\u8bc4\u4f30SDM\u7b56\u7565\u3002"}}
{"id": "2509.19111", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.19111", "abs": "https://arxiv.org/abs/2509.19111", "authors": ["Michael Ruderman", "Elia Brescia", "Paolo Roberto Massenio", "Giuseppe Leonardo Cascella", "David Naso"], "title": "Robust Synchronous Reference Frame Phase-Looked Loop (PLL) with Feed-Forward Frequency Estimation", "comment": "8 pages, 9 figures", "summary": "Synchronous reference frame phase-looked loop (SRF-PLL) techniques are widely\nused for interfacing and control applications in the power systems and energy\nconversion at large. Since a PLL system synchronizes its output with an\nexogenous harmonic signal, often 3-phases voltage or current, the locking of\nthe frequency and phase angle depends on the performance of the feedback loop\nwith at least two integrator terms, and on the distortions of the measured\ninput quantities. For the conventional SRF-PLL with a proportional-integral\n(PI) control in feedback, we are providing a robust design which maximizes the\nphase margin and uses the normalization scheme for yielding the loop\ninsensitive to the input amplitude variations. The main improvement in the\ntransient behavior and also in tracking of frequency ramps is achieved by using\nthe robust feed-forward frequency estimator, which is model-free and suitable\nfor the noisy and time-varying harmonic signals. The proposed\nfeed-forward-feedback SRF-PLL scheme is experimentally evaluated on the\n3-phases harmonic currents from standard PMSM drives with varying angular\nspeeds and loads. Both, the tracked angular frequency and locked phase angle\nare assessed as performance metrics of the robust SRF-PLL scheme with\nfeedforwarding.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u540c\u6b65\u53c2\u8003\u5e27\u9501\u76f8\u73af\uff08SRF-PLL\uff09\u6280\u672f\uff0c\u901a\u8fc7\u5f15\u5165\u9c81\u68d2\u8bbe\u8ba1\u548c\u524d\u9988\u9891\u7387\u4f30\u8ba1\u5668\uff0c\u63d0\u9ad8\u4e86\u5728\u7535\u529b\u7cfb\u7edf\u4e2d\u7684\u6027\u80fd\uff0c\u4f7f\u5176\u5bf9\u8f93\u5165\u4fe1\u53f7\u7684\u5e45\u503c\u53d8\u5316\u4e0d\u654f\u611f\uff0c\u5e76\u80fd\u66f4\u597d\u5730\u8ddf\u8e2a\u9891\u7387\u53d8\u5316\u3002", "motivation": "\u4f20\u7edf\u7684SRF-PLL\u5728\u5904\u7406\u542b\u6709\u566a\u58f0\u548c\u53d8\u5316\u7684\u8f93\u5165\u4fe1\u53f7\u65f6\uff0c\u5176\u9501\u76f8\u6027\u80fd\u4f1a\u53d7\u5230\u5f71\u54cd\u3002\u9700\u8981\u6539\u8fdbPLL\u8bbe\u8ba1\u4ee5\u63d0\u9ad8\u5176\u9c81\u68d2\u6027\u548c\u52a8\u6001\u54cd\u5e94\u3002", "method": "\u91c7\u7528\u6bd4\u4f8b-\u79ef\u5206\uff08PI\uff09\u63a7\u5236\u5668\uff0c\u5e76\u901a\u8fc7\u6700\u5927\u5316\u76f8\u89d2\u88d5\u5ea6\u548c\u91c7\u7528\u5f52\u4e00\u5316\u65b9\u6848\u6765\u8bbe\u8ba1\u9c81\u68d2\u7684\u53cd\u9988\u73af\u8def\uff0c\u4f7f\u5176\u5bf9\u8f93\u5165\u4fe1\u53f7\u7684\u5e45\u503c\u4e0d\u654f\u611f\u3002\u6b64\u5916\uff0c\u5f15\u5165\u4e00\u4e2a\u6a21\u578b\u65e0\u5173\u7684\u524d\u9988\u9891\u7387\u4f30\u8ba1\u5668\u6765\u6539\u5584\u77ac\u6001\u54cd\u5e94\u548c\u9891\u7387\u659c\u5761\u8ddf\u8e2a\u80fd\u529b\u3002", "result": "\u63d0\u51fa\u7684\u6539\u8fdb\u578bSRF-PLL\u65b9\u6848\u5728\u5305\u542b\u53d8\u5316\u89d2\u901f\u5ea6\u548c\u8d1f\u8f7d\u7684\u6c38\u78c1\u540c\u6b65\u7535\u673a\uff08PMSM\uff09\u9a71\u52a8\u5668\u7684\u4e09\u76f8\u8c10\u6ce2\u7535\u6d41\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8bc4\u4f30\u4e86\u5176\u8ddf\u8e2a\u89d2\u9891\u7387\u548c\u9501\u76f8\u89d2\u7684\u80fd\u529b\u3002", "conclusion": "\u63d0\u51fa\u7684\u524d\u9988-\u53cd\u9988SRF-PLL\u65b9\u6848\u80fd\u591f\u6709\u6548\u63d0\u9ad8\u9501\u76f8\u73af\u5728\u7535\u529b\u7cfb\u7edf\u5e94\u7528\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u52a8\u6001\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u566a\u58f0\u548c\u53d8\u5316\u4fe1\u53f7\u65f6\u3002"}}
{"id": "2509.18844", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2509.18844", "abs": "https://arxiv.org/abs/2509.18844", "authors": ["E. E. Torres-Miyares", "S. Miret-Art\u00e9s"], "title": "Quantum-to-classical transition and H-theorem in surface diffusion", "comment": "3 figures", "summary": "In this work, surface diffusion is studied with a different perspective by\nshowing how the corresponding open dynamics is transformed when passing, in a\ncontinuous and smooth way, from a pure quantum regime to a full classical\nregime; the so-called quantum-to-classical transition. This continuous process\nis carried out from the Liouville-von Neumann equation by scaling Planck's\nconstant. For this goal, the Brownian motion of an adsorbate on a flat surface\nis analyzed in order to show how this transition takes place. In particular,\nthis open dynamics is studied from the master equation for the reduced density\nmatrix within the Caldeira-Leggett formalism; in particular, the two extreme\ntime behaviors, the ballistic and diffusive motions. It is also shown that the\norigin of the ballistic motion is different for the quantum and classical\nregimes. In this scenario, the corresponding Gaussian function for the\nintermediate scattering function is governed by the thermal velocity in the\nclassical regime versus the initial spreading velocity of the wave packet for\nthe quantum regime, leading to speak of classical and quantum ideal gas,\nrespectively. Finally, in the diffusive regime, and starting from the\nChudley-Elliott model, the quantum-to-classical transition is also discussed in\nterms of the well-known H-function for three surface temperatures in the\ndiffusion of H and D on a Pt(111) surface. The main goal in this analysis is if\none can discriminate the irreversibility coming from tunneling and thermal\nactivation diffusion.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4ece\u91cf\u5b50\u5230\u7ecf\u5178\u52a8\u529b\u5b66\u7684\u8f6c\u53d8\uff0c\u91cd\u70b9\u5173\u6ce8\u8868\u9762\u6269\u6563\u7684\u5f00\u653e\u52a8\u529b\u5b66\u3002", "motivation": "\u7814\u7a76\u8868\u9762\u6269\u6563\u7684\u5f00\u653e\u52a8\u529b\u5b66\u5982\u4f55\u901a\u8fc7\u91cf\u5b50-\u7ecf\u5178\u8f6c\u53d8\uff0c\u4ece\u7eaf\u91cf\u5b50\u6001\u5e73\u6ed1\u8fc7\u6e21\u5230\u7ecf\u5178\u6001\u3002", "method": "\u4f7f\u7528 Liouville-von Neumann \u65b9\u7a0b\u548c Caldeira-Leggett \u5f62\u5f0f\u4e3b\u4e49\uff0c\u901a\u8fc7\u7f29\u653e\u666e\u6717\u514b\u5e38\u6570\u6765\u5206\u6790\u5438\u9644\u8d28\u5728\u5e73\u9762\u4e0a\u7684\u5e03\u6717\u8fd0\u52a8\uff0c\u7279\u522b\u5173\u6ce8\u4e86\u5f39\u9053\u548c\u6269\u6563\u8fd0\u52a8\u8fd9\u4e24\u79cd\u6781\u7aef\u7684\u65f6\u95f4\u884c\u4e3a\u3002", "result": "\u5728\u7ecf\u5178\u6a21\u578b\u4e2d\uff0c\u9ad8\u65af\u51fd\u6570\u7531\u70ed\u901f\u5ea6\u51b3\u5b9a\uff0c\u800c\u5728\u91cf\u5b50\u6a21\u578b\u4e2d\uff0c\u5219\u7531\u6ce2\u5305\u7684\u521d\u59cb\u6269\u6563\u901f\u5ea6\u51b3\u5b9a\u3002\u8fd9\u5bfc\u81f4\u4e86\u7ecf\u5178\u548c\u91cf\u5b50\u7406\u60f3\u6c14\u4f53\u7684\u6982\u5ff5\u3002\u6b64\u5916\uff0c\u8fd8\u8ba8\u8bba\u4e86\u5728 Pt(111) \u8868\u9762\u4e0a H \u548c D \u6269\u6563\u7684 H \u51fd\u6570\uff0c\u4ee5\u533a\u5206\u7531\u96a7\u7a7f\u548c\u70ed\u6fc0\u6d3b\u6269\u6563\u5f15\u8d77\u7684\u4e0d\u53ef\u9006\u6027\u3002", "conclusion": "\u672c\u6587\u63a2\u8ba8\u4e86\u91cf\u5b50-\u7ecf\u5178\u8f6c\u53d8\u5982\u4f55\u5f71\u54cd\u8868\u9762\u6269\u6563\u7684\u52a8\u529b\u5b66\uff0c\u5e76\u63d0\u51fa\u4e86\u533a\u5206\u91cf\u5b50\u548c\u7ecf\u5178\u6269\u6563\u673a\u5236\u7684\u65b9\u6cd5\u3002"}}
{"id": "2509.19281", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.19281", "abs": "https://arxiv.org/abs/2509.19281", "authors": ["Xiyang Lan", "Xin Li"], "title": "STFT-AECNN: An Attention-Enhanced CNN for Efficient \u03a6-OTDR Event Recognition in IoT-Enabled Distributed Acoustic Sensing", "comment": null, "summary": "Phase-sensitive optical time-domain reflectometry ({\\Phi}-OTDR) has emerged\nas a promising sensing technology in Internet of Things (IoT) infrastructures,\nenabling large-scale distributed acoustic sensing (DAS) for smart city\nsurveillance, industrial pipeline monitoring, and critical infrastructure\nprotection. However, accurately recognizing events from massive {\\Phi}-OTDR\ndata streams remains challenging, as existing deep learning methods either\ndisrupt the inherent spatiotemporal structure of signals or incur prohibitive\ncomputational costs, limiting their applicability in resource-constrained IoT\nscenarios. To overcome these challenges, we propose a novel STFT-based\nAttention-Enhanced Convolutional Neural Network (STFT-AECNN), which represents\nmulti-channel time-series data as stacked spectrograms to fully exploit their\nspatiotemporal characteristics while enabling efficient 2D CNN processing. A\nSpatial Efficient Attention Module (SEAM) is further introduced to adaptively\nemphasize the most informative channels, and a joint Cross-Entropy and Triplet\nloss is adopted to enhance the discriminability of the learned feature space.\nExtensive experiments on the public BJTU {\\Phi}-OTDR dataset demonstrate that\nSTFT-AECNN achieves a peak accuracy of 99.94% while maintaining high\ncomputational efficiency. These results highlight its potential for real-time,\nscalable, and robust event recognition in IoT-enabled DAS systems, paving the\nway for reliable and intelligent IoT sensing applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u77ed\u65f6\u5085\u91cc\u53f6\u53d8\u6362 (STFT) \u7684\u6ce8\u610f\u529b\u589e\u5f3a\u5377\u79ef\u795e\u7ecf\u7f51\u7edc (STFT-AECNN)\uff0c\u7528\u4e8e\u89e3\u51b3 {\\Phi}-OTDR \u6570\u636e\u5206\u6790\u4e2d\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u548c\u9ad8\u6548\u7387\u7684\u4e8b\u4ef6\u8bc6\u522b\uff0c\u9002\u7528\u4e8e\u7269\u8054\u7f51\u73af\u5883\u3002", "motivation": "\u73b0\u6709\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u5904\u7406 {\\Phi}-OTDR \u6d77\u91cf\u6570\u636e\u65f6\uff0c\u8981\u4e48\u7834\u574f\u4fe1\u53f7\u56fa\u6709\u7684\u65f6\u7a7a\u7ed3\u6784\uff0c\u8981\u4e48\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\uff0c\u9650\u5236\u4e86\u5176\u5728\u8d44\u6e90\u53d7\u9650\u7684\u7269\u8054\u7f51\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684 STFT-AECNN \u6a21\u578b\uff0c\u5c06\u591a\u901a\u9053\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u8868\u793a\u4e3a\u5806\u53e0\u7684\u9891\u8c31\u56fe\uff0c\u4ee5\u5145\u5206\u5229\u7528\u5176\u65f6\u7a7a\u7279\u5f81\uff0c\u5e76\u5b9e\u73b0\u9ad8\u6548\u7684\u4e8c\u7ef4\u5377\u79ef\u5904\u7406\u3002\u5f15\u5165\u4e86\u7a7a\u95f4\u9ad8\u6548\u6ce8\u610f\u529b\u6a21\u5757 (SEAM) \u6765\u81ea\u9002\u5e94\u5730\u5f3a\u8c03\u4fe1\u606f\u91cf\u6700\u5927\u7684\u901a\u9053\uff0c\u5e76\u91c7\u7528\u8054\u5408\u4ea4\u53c9\u71b5\u548c\u4e09\u5143\u7ec4\u635f\u5931\u6765\u589e\u5f3a\u5b66\u4e60\u5230\u7684\u7279\u5f81\u7a7a\u95f4\u7684\u5224\u522b\u529b\u3002", "result": "\u5728\u516c\u5f00\u7684 BJTU {\\Phi}-OTDR \u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cSTFT-AECNN \u5b9e\u73b0\u4e86 99.94% \u7684\u5cf0\u503c\u51c6\u786e\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "STFT-AECNN \u5728\u7269\u8054\u7f51 {\\Phi}-OTDR \u5e94\u7528\u4e2d\u5177\u6709\u5b9e\u65f6\u3001\u53ef\u6269\u5c55\u548c\u9c81\u68d2\u7684\u4e8b\u4ef6\u8bc6\u522b\u6f5c\u529b\uff0c\u4e3a\u53ef\u9760\u548c\u667a\u80fd\u7684\u7269\u8054\u7f51\u4f20\u611f\u5e94\u7528\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2509.18597", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18597", "abs": "https://arxiv.org/abs/2509.18597", "authors": ["Yuan Meng", "Zhenguo Sun", "Max Fest", "Xukun Li", "Zhenshan Bing", "Alois Knoll"], "title": "Growing with Your Embodied Agent: A Human-in-the-Loop Lifelong Code Generation Framework for Long-Horizon Manipulation Skills", "comment": "upload 9 main page - v1", "summary": "Large language models (LLMs)-based code generation for robotic manipulation\nhas recently shown promise by directly translating human instructions into\nexecutable code, but existing methods remain noisy, constrained by fixed\nprimitives and limited context windows, and struggle with long-horizon tasks.\nWhile closed-loop feedback has been explored, corrected knowledge is often\nstored in improper formats, restricting generalization and causing catastrophic\nforgetting, which highlights the need for learning reusable skills. Moreover,\napproaches that rely solely on LLM guidance frequently fail in extremely\nlong-horizon scenarios due to LLMs' limited reasoning capability in the robotic\ndomain, where such issues are often straightforward for humans to identify. To\naddress these challenges, we propose a human-in-the-loop framework that encodes\ncorrections into reusable skills, supported by external memory and\nRetrieval-Augmented Generation with a hint mechanism for dynamic reuse.\nExperiments on Ravens, Franka Kitchen, and MetaWorld, as well as real-world\nsettings, show that our framework achieves a 0.93 success rate (up to 27%\nhigher than baselines) and a 42% efficiency improvement in correction rounds.\nIt can robustly solve extremely long-horizon tasks such as \"build a house\",\nwhich requires planning over 20 primitives.", "AI": {"tldr": "\u4f7f\u7528\u5305\u542b\u5916\u90e8\u8bb0\u5fc6\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7684\u53ef\u91cd\u7528\u6280\u80fd\u6765\u6539\u8fdb\u57fa\u4e8eLLM\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u4ee3\u7801\u751f\u6210\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5982\u566a\u58f0\u3001\u56fa\u5b9a\u539f\u8bed\u3001\u6709\u9650\u4e0a\u4e0b\u6587\u548c\u957f\u65f6\u4efb\u52a1\u5904\u7406\u80fd\u529b\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u4ee3\u7801\u751f\u6210\u65b9\u6cd5\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u9886\u57df\u5b58\u5728\u566a\u58f0\u5927\u3001\u539f\u8bed\u56fa\u5b9a\u3001\u4e0a\u4e0b\u6587\u7a97\u53e3\u6709\u9650\u4ee5\u53ca\u96be\u4ee5\u5904\u7406\u957f\u65f6\u4efb\u52a1\u7684\u95ee\u9898\u3002\u6b64\u5916\uff0c\u95ed\u73af\u53cd\u9988\u4e2d\u7684\u77e5\u8bc6\u5b58\u50a8\u683c\u5f0f\u4e0d\u5f53\uff0c\u9650\u5236\u4e86\u6cdb\u5316\u80fd\u529b\u5e76\u53ef\u80fd\u5bfc\u81f4\u707e\u96be\u6027\u9057\u5fd8\uff0c\u56e0\u6b64\u9700\u8981\u5b66\u4e60\u53ef\u91cd\u7528\u7684\u6280\u80fd\u3002\u4ec5\u4f9d\u8d56LLM\u7684\u65b9\u6cd5\u5728\u6781\u957f\u65f6\u4efb\u52a1\u4e2d\u56e0\u5176\u6709\u9650\u7684\u63a8\u7406\u80fd\u529b\u800c\u5e38\u5e38\u5931\u8d25\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u5305\u542b\u5916\u90e8\u8bb0\u5fc6\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7684\u53ef\u91cd\u7528\u6280\u80fd\u6846\u67b6\uff0c\u5e76\u8f85\u4ee5\u63d0\u793a\u673a\u5236\u4ee5\u5b9e\u73b0\u52a8\u6001\u91cd\u7528\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u4eba\u7c7b\u5728\u5faa\u73af\u4e2d\u8fdb\u884c\u7ea0\u6b63\uff0c\u5e76\u5c06\u8fd9\u4e9b\u7ea0\u6b63\u7f16\u7801\u6210\u53ef\u91cd\u7528\u7684\u6280\u80fd\u3002", "result": "\u5728Ravens, Franka Kitchen\u548cMetaWorld\u6570\u636e\u96c6\u4ee5\u53ca\u771f\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u6846\u67b6\u7684\u6210\u529f\u7387\u4e3a0.93\uff0c\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u9ad8\u51fa27%\uff1b\u7ea0\u6b63\u8f6e\u6b21\u7684\u6548\u7387\u63d0\u9ad8\u4e8642%\uff1b\u80fd\u591f\u6210\u529f\u89e3\u51b3\u5982\u201c\u5efa\u9020\u623f\u5c4b\u201d\u8fd9\u7c7b\u9700\u8981\u8d85\u8fc720\u4e2a\u539f\u8bed\u89c4\u5212\u7684\u6781\u957f\u65f6\u4efb\u52a1\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6846\u67b6\u901a\u8fc7\u5c06\u7ea0\u6b63\u7f16\u7801\u4e3a\u53ef\u91cd\u7528\u7684\u6280\u80fd\uff0c\u5e76\u7ed3\u5408\u5916\u90e8\u8bb0\u5fc6\u548cRAG\uff0c\u80fd\u591f\u6709\u6548\u5730\u89e3\u51b3\u73b0\u6709LLM\u4ee3\u7801\u751f\u6210\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u590d\u6742\u548c\u957f\u65f6\u4efb\u52a1\u65b9\u9762\u3002"}}
{"id": "2509.18284", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18284", "abs": "https://arxiv.org/abs/2509.18284", "authors": ["Yi Gu", "Kuniaki Saito", "Jiaxin Ma"], "title": "Learning Contrastive Multimodal Fusion with Improved Modality Dropout for Disease Detection and Prediction", "comment": "MICCAI 2025", "summary": "As medical diagnoses increasingly leverage multimodal data, machine learning\nmodels are expected to effectively fuse heterogeneous information while\nremaining robust to missing modalities. In this work, we propose a novel\nmultimodal learning framework that integrates enhanced modalities dropout and\ncontrastive learning to address real-world limitations such as modality\nimbalance and missingness. Our approach introduces learnable modality tokens\nfor improving missingness-aware fusion of modalities and augments conventional\nunimodal contrastive objectives with fused multimodal representations. We\nvalidate our framework on large-scale clinical datasets for disease detection\nand prediction tasks, encompassing both visual and tabular modalities.\nExperimental results demonstrate that our method achieves state-of-the-art\nperformance, particularly in challenging and practical scenarios where only a\nsingle modality is available. Furthermore, we show its adaptability through\nsuccessful integration with a recent CT foundation model. Our findings\nhighlight the effectiveness, efficiency, and generalizability of our approach\nfor multimodal learning, offering a scalable, low-cost solution with\nsignificant potential for real-world clinical applications. The code is\navailable at https://github.com/omron-sinicx/medical-modality-dropout.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6a21\u6001\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u589e\u5f3a\u7684\u6a21\u6001\u4e22\u5f03\u548c\u5bf9\u6bd4\u5b66\u4e60\u6765\u89e3\u51b3\u6a21\u6001\u4e0d\u5e73\u8861\u548c\u7f3a\u5931\u7684\u95ee\u9898\uff0c\u5e76\u5728\u75be\u75c5\u68c0\u6d4b\u548c\u9884\u6d4b\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u533b\u5b66\u8bca\u65ad\u8d8a\u6765\u8d8a\u591a\u5730\u5229\u7528\u591a\u6a21\u6001\u6570\u636e\uff0c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u9700\u8981\u5728\u4fdd\u6301\u5bf9\u7f3a\u5931\u6a21\u6001\u7684\u9c81\u68d2\u6027\u7684\u540c\u65f6\uff0c\u6709\u6548\u5730\u878d\u5408\u5f02\u6784\u4fe1\u606f\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u6a21\u6001\u4e0d\u5e73\u8861\u548c\u7f3a\u5931\u7b49\u5b9e\u9645\u9650\u5236\u65f6\u9762\u4e34\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6a21\u6001\u5b66\u4e60\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u589e\u5f3a\u7684\u6a21\u6001\u4e22\u5f03\u548c\u5bf9\u6bd4\u5b66\u4e60\u3002\u5f15\u5165\u4e86\u53ef\u5b66\u4e60\u7684\u6a21\u6001\u4ee4\u724c\u4ee5\u6539\u8fdb\u7f3a\u5931\u611f\u77e5\u6a21\u6001\u878d\u5408\uff0c\u5e76\u901a\u8fc7\u878d\u5408\u7684\u591a\u6a21\u6001\u8868\u793a\u6765\u589e\u5f3a\u4f20\u7edf\u7684\u5355\u6a21\u6001\u5bf9\u6bd4\u76ee\u6807\u3002\u5bf9\u5305\u542b\u89c6\u89c9\u548c\u8868\u683c\u6a21\u6001\u7684\u5927\u89c4\u6a21\u4e34\u5e8a\u6570\u636e\u96c6\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "result": "\u5728\u75be\u75c5\u68c0\u6d4b\u548c\u9884\u6d4b\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u4ec5\u53ef\u7528\u5355\u4e00\u6a21\u6001\u7684\u6311\u6218\u6027\u548c\u5b9e\u9645\u573a\u666f\u4e2d\u3002\u8be5\u65b9\u6cd5\u8fd8\u6210\u529f\u96c6\u6210\u5230\u6700\u8fd1\u7684CT\u57fa\u7840\u6a21\u578b\u4e2d\uff0c\u8bc1\u660e\u4e86\u5176\u9002\u5e94\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u591a\u6a21\u6001\u5b66\u4e60\u65b9\u9762\u5177\u6709\u6709\u6548\u6027\u3001\u6548\u7387\u548c\u6cdb\u5316\u6027\uff0c\u4e3a\u73b0\u5b9e\u4e16\u754c\u7684\u4e34\u5e8a\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u3001\u4f4e\u6210\u672c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.18482", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2509.18482", "abs": "https://arxiv.org/abs/2509.18482", "authors": ["Yuanzheng Paul Tan", "Yung Szen Yap", "Long Hoang Nguyen", "Rangga P. Budoyo", "Patrick Bore", "Kun Hee Park", "Christoph Hufnagel", "Rainer Dumke"], "title": "Characterizing Noise in Controlling Superconducting Qubits", "comment": "5 pages, 4 figures", "summary": "Meaningful quantum computing is currently bottlenecked by the error rates of\ncurrent generation Noisy Intermediate Scale Quantum (NISQ) devices. To improve\nthe fidelity of the quantum logic gates, it is essential to recognize the\ncontributions of various sources of errors, including background noise. In this\nwork, we investigate the effects of noise when applied to superconducting qubit\ncontrol pulses to observe the dependency of the gate fidelity with the\nsignal-to-noise ratio (SNR). We propose a model on how the noise of the control\nelectronics interacts with the qubit system and demonstrate a method for\ncharacterizing the noise environment of the qubit control.", "AI": {"tldr": "\u91cf\u5b50\u8ba1\u7b97\u53d7\u9650\u4e8eNISQ\u8bbe\u5907\u7684\u9519\u8bef\u7387\uff0c\u63d0\u9ad8\u4fdd\u771f\u5ea6\u9700\u8981\u8bc6\u522b\u9519\u8bef\u6e90\u3002\u672c\u7814\u7a76\u8c03\u67e5\u4e86\u566a\u58f0\u5bf9\u8d85\u5bfc\u91cf\u5b50\u6bd4\u7279\u63a7\u5236\u8109\u51b2\u7684\u5f71\u54cd\uff0c\u5e76\u89c2\u5bdf\u5230\u95e8\u4fdd\u771f\u5ea6\u4e0e\u4fe1\u566a\u6bd4\u7684\u4f9d\u8d56\u5173\u7cfb\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u6a21\u578b\u6765\u63cf\u8ff0\u566a\u58f0\u4e0e\u91cf\u5b50\u6bd4\u7279\u7cfb\u7edf\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u5e76\u5c55\u793a\u4e86\u4e00\u79cd\u8868\u5f81\u566a\u58f0\u73af\u5883\u7684\u65b9\u6cd5\u3002", "motivation": "\u8bc6\u522bNISQ\u8bbe\u5907\u4e2d\u5bfc\u81f4\u91cf\u5b50\u903b\u8f91\u95e8\u9519\u8bef\u7684\u64cd\u4f5c\uff0c\u5305\u62ec\u80cc\u666f\u566a\u58f0\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u6a21\u578b\uff0c\u7814\u7a76\u63a7\u5236\u7535\u5b50\u5b66\u4e2d\u7684\u566a\u58f0\u4e0e\u91cf\u5b50\u6bd4\u7279\u7cfb\u7edf\u7684\u4ea4\u4e92\u4f5c\u7528\u3002\u901a\u8fc7\u8c03\u6574\u4fe1\u53f7\u4e0e\u566a\u58f0\u7684\u6bd4\u7387\u6765\u89c2\u5bdf\u95e8\u4fdd\u771f\u5ea6\u7684\u53d8\u5316\u3002", "result": "\u95e8\u4fdd\u771f\u5ea6\u4e0e\u4fe1\u566a\u6bd4\u7684\u4f9d\u8d56\u5173\u7cfb\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u566a\u58f0\u6a21\u578b\uff0c\u5e76\u5c55\u793a\u4e86\u4e00\u79cd\u5bf9\u91cf\u5b50\u6bd4\u7279\u63a7\u5236\u566a\u58f0\u73af\u5883\u8fdb\u884c\u8868\u5f81\u7684\u65b9\u6cd5\u3002"}}
{"id": "2509.18234", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18234", "abs": "https://arxiv.org/abs/2509.18234", "authors": ["Yu Gu", "Jingjing Fu", "Xiaodong Liu", "Jeya Maria Jose Valanarasu", "Noel Codella", "Reuben Tan", "Qianchu Liu", "Ying Jin", "Sheng Zhang", "Jinyu Wang", "Rui Wang", "Lei Song", "Guanghui Qin", "Naoto Usuyama", "Cliff Wong", "Cheng Hao", "Hohin Lee", "Praneeth Sanapathi", "Sarah Hilado", "Bian Jiang", "Javier Alvarez-Valle", "Mu Wei", "Jianfeng Gao", "Eric Horvitz", "Matt Lungren", "Hoifung Poon", "Paul Vozila"], "title": "The Illusion of Readiness: Stress Testing Large Frontier Models on Multimodal Medical Benchmarks", "comment": "35 pages", "summary": "Large frontier models like GPT-5 now achieve top scores on medical\nbenchmarks. But our stress tests tell a different story. Leading systems often\nguess correctly even when key inputs like images are removed, flip answers\nunder trivial prompt changes, and fabricate convincing yet flawed reasoning.\nThese aren't glitches; they expose how today's benchmarks reward test-taking\ntricks over medical understanding. We evaluate six flagship models across six\nwidely used benchmarks and find that high leaderboard scores hide brittleness\nand shortcut learning. Through clinician-guided rubric evaluation, we show that\nbenchmarks vary widely in what they truly measure yet are treated\ninterchangeably, masking failure modes. We caution that medical benchmark\nscores do not directly reflect real-world readiness. If we want AI to earn\ntrust in healthcare, we must demand more than leaderboard wins and must hold\nsystems accountable for robustness, sound reasoning, and alignment with real\nmedical demands.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u533b\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u538b\u529b\u6d4b\u8bd5\u4e0b\u66b4\u9732\u51fa\u8bb8\u591a\u95ee\u9898\uff0c\u4f8b\u5982\u5728\u7f3a\u5c11\u5173\u952e\u8f93\u5165\uff08\u5982\u56fe\u50cf\uff09\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u731c\u5bf9\uff0c\u5bf9\u5fae\u5c0f\u7684\u63d0\u793a\u66f4\u6539\u53cd\u5e94\u4e0d\u4e00\uff0c\u5e76\u4e14\u4f1a\u7f16\u9020\u770b\u4f3c\u5408\u7406\u4f46\u6709\u7f3a\u9677\u7684\u63a8\u7406\u8fc7\u7a0b\u3002\u8fd9\u8868\u660e\u5f53\u524d\u7684\u57fa\u51c6\u6d4b\u8bd5\u53ef\u80fd\u66f4\u4fa7\u91cd\u4e8e\u201c\u5e94\u8bd5\u6280\u5de7\u201d\u800c\u975e\u771f\u6b63\u7684\u533b\u5b66\u7406\u89e3\u3002", "motivation": "\u8bc4\u4f30\u5f53\u524d\u9886\u5148\u7684AI\u6a21\u578b\u5728\u533b\u5b66\u9886\u57df\u7684\u5b9e\u9645\u80fd\u529b\uff0c\u63ed\u793a\u5728\u4e25\u683c\u6d4b\u8bd5\u4e0b\u5b83\u4eec\u5b58\u5728\u7684\u8106\u5f31\u6027\u548c\u201c\u8d70\u6377\u5f84\u201d\u5b66\u4e60\u7684\u95ee\u9898\uff0c\u5e76\u6307\u51fa\u533b\u5b66\u57fa\u51c6\u6d4b\u8bd5\u5728\u8861\u91cf\u771f\u5b9e\u4e16\u754c\u80fd\u529b\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "method": "\u5bf9\u516d\u4e2a\u4e3b\u6d41\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u516d\u4e2a\u5e38\u7528\u7684\u533b\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fdb\u884c\u538b\u529b\u6d4b\u8bd5\u3002\u901a\u8fc7\u4e34\u5e8a\u533b\u751f\u6307\u5bfc\u7684\u8bc4\u5206\u6807\u51c6\u8bc4\u4f30\uff0c\u5206\u6790\u57fa\u51c6\u6d4b\u8bd5\u5b9e\u9645\u8861\u91cf\u7684\u5185\u5bb9\u4ee5\u53ca\u5b83\u4eec\u88ab\u4e0d\u5f53\u4e92\u6362\u4f7f\u7528\u7684\u73b0\u8c61\uff0c\u4ee5\u66b4\u9732\u6a21\u578b\u7684\u5931\u8d25\u6a21\u5f0f\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5c3d\u7ba1\u6a21\u578b\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5f97\u5206\u5f88\u9ad8\uff0c\u4f46\u5b83\u4eec\u8868\u73b0\u51fa\u8106\u5f31\u6027\u548c\u201c\u8d70\u6377\u5f84\u201d\u5b66\u4e60\u7684\u884c\u4e3a\u3002\u4e0d\u540c\u7684\u57fa\u51c6\u6d4b\u8bd5\u5b9e\u9645\u8861\u91cf\u7684\u5185\u5bb9\u5dee\u5f02\u5f88\u5927\uff0c\u4f46\u5374\u88ab\u5f53\u4f5c\u662f\u7b49\u6548\u7684\uff0c\u8fd9\u63a9\u76d6\u4e86\u6a21\u578b\u7684\u5931\u8d25\u6a21\u5f0f\u3002", "conclusion": "\u76ee\u524d\u7684\u533b\u5b66\u57fa\u51c6\u6d4b\u8bd5\u5206\u6570\u4e0d\u80fd\u76f4\u63a5\u53cd\u6620AI\u5728\u771f\u5b9e\u533b\u7597\u73af\u5883\u4e2d\u7684\u51c6\u5907\u7a0b\u5ea6\u3002\u4e3a\u4e86\u8ba9AI\u5728\u533b\u7597\u9886\u57df\u83b7\u5f97\u4fe1\u4efb\uff0c\u6211\u4eec\u9700\u8981\u5bf9\u5176\u9c81\u68d2\u6027\u3001\u63a8\u7406\u7684\u5408\u7406\u6027\u4ee5\u53ca\u662f\u5426\u7b26\u5408\u5b9e\u9645\u533b\u7597\u9700\u6c42\u6709\u66f4\u9ad8\u7684\u8981\u6c42\uff0c\u800c\u4e0d\u4ec5\u4ec5\u6ee1\u8db3\u4e8e\u57fa\u51c6\u6d4b\u8bd5\u7684\u9886\u5148\u5730\u4f4d\u3002"}}
{"id": "2509.18458", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50 (Primary) 68T07, 68T05, 68T20, 68T27 (Secondary)", "I.2.7; I.2.6; I.2.4; I.2.8"], "pdf": "https://arxiv.org/pdf/2509.18458", "abs": "https://arxiv.org/abs/2509.18458", "authors": ["Daniel Kaiser", "Arnoldo Frigessi", "Ali Ramezani-Kebrya", "Benjamin Ricaud"], "title": "CogniLoad: A Synthetic Natural Language Reasoning Benchmark With Tunable Length, Intrinsic Difficulty, and Distractor Density", "comment": "29 pages (main: 12 + supplemental material: 17), 6 figures, 4 tables,\n  Code: https://github.com/kaiserdan/cogniload, Data:\n  https://huggingface.co/datasets/cogniloadteam/cogniload", "summary": "Current benchmarks for long-context reasoning in Large Language Models (LLMs)\noften blur critical factors like intrinsic task complexity, distractor\ninterference, and task length. To enable more precise failure analysis, we\nintroduce CogniLoad, a novel synthetic benchmark grounded in Cognitive Load\nTheory (CLT). CogniLoad generates natural-language logic puzzles with\nindependently tunable parameters that reflect CLT's core dimensions: intrinsic\ndifficulty ($d$) controls intrinsic load; distractor-to-signal ratio ($\\rho$)\nregulates extraneous load; and task length ($N$) serves as an operational proxy\nfor conditions demanding germane load. Evaluating 22 SotA reasoning LLMs,\nCogniLoad reveals distinct performance sensitivities, identifying task length\nas a dominant constraint and uncovering varied tolerances to intrinsic\ncomplexity and U-shaped responses to distractor ratios. By offering systematic,\nfactorial control over these cognitive load dimensions, CogniLoad provides a\nreproducible, scalable, and diagnostically rich tool for dissecting LLM\nreasoning limitations and guiding future model development.", "AI": {"tldr": "CogniLoad\u662f\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5728\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u65b9\u9762\u7684\u80fd\u529b\uff0c\u901a\u8fc7\u63a7\u5236\u5185\u5728\u96be\u5ea6\u3001\u5e72\u6270\u548c\u4efb\u52a1\u957f\u5ea6\u6765\u66f4\u7cbe\u786e\u5730\u5206\u6790LLM\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5f53\u524d\u7684LLM\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u672a\u80fd\u6709\u6548\u5730\u533a\u5206\u4efb\u52a1\u590d\u6742\u6027\u3001\u5e72\u6270\u548c\u4efb\u52a1\u957f\u5ea6\u7b49\u5173\u952e\u56e0\u7d20\uff0c\u963b\u788d\u4e86\u7cbe\u786e\u7684\u5931\u8d25\u5206\u6790\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u4e2a\u80fd\u591f\u72ec\u7acb\u63a7\u5236\u8fd9\u4e9b\u56e0\u7d20\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "method": "CogniLoad\u4f7f\u7528\u8ba4\u77e5\u8d1f\u8377\u7406\u8bba\uff08CLT\uff09\u751f\u6210\u81ea\u7136\u8bed\u8a00\u903b\u8f91\u8c1c\u9898\uff0c\u5e76\u901a\u8fc7\u4e09\u4e2a\u53c2\u6570\u72ec\u7acb\u63a7\u5236\uff1a\u5185\u5728\u96be\u5ea6\uff08d\uff09\u63a7\u5236\u5185\u5728\u8d1f\u8377\uff0c\u5e72\u6270\u4e0e\u4fe1\u53f7\u4e4b\u6bd4\uff08\u03c1\uff09\u8c03\u8282\u5916\u5728\u8d1f\u8377\uff0c\u4efb\u52a1\u957f\u5ea6\uff08N\uff09\u4f5c\u4e3a\u5bf9\u9700\u8981\u5185\u5728\u8d1f\u8377\u7684\u6761\u4ef6\u7684\u4ee3\u7406\u3002\u5bf922\u4e2aSotA\u63a8\u7406LLM\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0c\u4e0d\u540c\u7684LLM\u5728CogniLoad\u4e0a\u7684\u8868\u73b0\u5b58\u5728\u5dee\u5f02\uff0c\u5176\u4e2d\u4efb\u52a1\u957f\u5ea6\u662f\u4e3b\u8981\u7684\u6027\u80fd\u9650\u5236\u56e0\u7d20\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u53d1\u73b0\u4e86LLM\u5bf9\u5185\u5728\u590d\u6742\u6027\u7684\u4e0d\u540c\u654f\u611f\u5ea6\u4ee5\u53ca\u5bf9\u5e72\u6270\u6bd4\u4f8b\u7684U\u578b\u53cd\u5e94\u6a21\u5f0f\u3002", "conclusion": "CogniLoad\u901a\u8fc7\u5bf9\u8ba4\u77e5\u8d1f\u8377\u7ef4\u5ea6\u8fdb\u884c\u7cfb\u7edf\u6027\u3001\u9636\u68af\u5f0f\u63a7\u5236\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u590d\u73b0\u3001\u53ef\u6269\u5c55\u4e14\u8bca\u65ad\u6027\u5f3a\u7684\u5de5\u5177\uff0c\u80fd\u591f\u6df1\u5165\u5256\u6790LLM\u7684\u63a8\u7406\u5c40\u9650\u6027\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u6a21\u578b\u5f00\u53d1\u63d0\u4f9b\u6307\u5bfc\u3002"}}
{"id": "2509.19243", "categories": ["eess.SY", "cs.SY", "econ.TH"], "pdf": "https://arxiv.org/pdf/2509.19243", "abs": "https://arxiv.org/abs/2509.19243", "authors": ["Ahmed S. Alahmed", "Audun Botterud", "Saurabh Amin", "Ali T. Al-Awami"], "title": "Watts and Drops: Co-Scheduling Power and Water in Desalination Plants", "comment": "5 pages, 6 figures. To appear in Proceedings of the 61st Allerton\n  Conference on Communication, Control, and Computing", "summary": "We develop a mathematical framework to jointly schedule water and electricity\nin a profit-maximizing renewable colocated water desalination plant that\nintegrates both thermal and membrane based technologies. The price-taking\ndesalination plant sells desalinated water to a water utility at a given price\nand engages in bidirectional electricity transactions with the grid, purchasing\nor selling power based on its net electricity demand. We show that the optimal\nscheduling policy depends on the plant's internal renewable generation and\nfollows a simple threshold structure. Under the optimal policy, thermal based\nwater output decreases monotonically with renewable output, while membrane\nbased water output increases monotonically. We characterize the structure and\nintuition behind the threshold policy and examine key special properties.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u6570\u5b66\u6846\u67b6\uff0c\u7528\u4e8e\u8054\u5408\u8c03\u5ea6\u4e00\u4e2a\u4ee5\u8425\u5229\u4e3a\u76ee\u7684\u7684\u3001\u7ed3\u5408\u4e86\u70ed\u80fd\u548c\u819c\u6280\u672f\u7684\u53ef\u518d\u751f\u80fd\u6e90\u534f\u540c\u6c34\u5904\u7406\u5382\u7684\u7528\u6c34\u548c\u7528\u7535\u3002\u8be5\u5de5\u5382\u5c06\u5904\u7406\u540e\u7684\u6c34\u4ee5\u56fa\u5b9a\u4ef7\u683c\u51fa\u552e\u7ed9\u6c34\u52a1\u516c\u53f8\uff0c\u5e76\u6839\u636e\u5176\u51c0\u7528\u7535\u9700\u6c42\u4e0e\u7535\u7f51\u8fdb\u884c\u53cc\u5411\u7535\u529b\u4ea4\u6613\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u6570\u5b66\u6846\u67b6\uff0c\u7528\u4e8e\u8054\u5408\u8c03\u5ea6\u53ef\u518d\u751f\u80fd\u6e90\u534f\u540c\u6c34\u5904\u7406\u5382\u7684\u7528\u6c34\u548c\u7528\u7535\uff0c\u4ee5\u5b9e\u73b0\u5229\u6da6\u6700\u5927\u5316\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u6570\u5b66\u6846\u67b6\uff0c\u8c03\u5ea6\u70ed\u80fd\u548c\u819c\u6280\u672f\u534f\u540c\u7684\u6c34\u5904\u7406\u5382\u7684\u7528\u6c34\u548c\u7528\u7535\u3002", "result": "\u6700\u4f18\u8c03\u5ea6\u7b56\u7565\u4f9d\u8d56\u4e8e\u5de5\u5382\u7684\u5185\u90e8\u53ef\u518d\u751f\u80fd\u6e90\u53d1\u7535\u91cf\uff0c\u5e76\u5448\u73b0\u51fa\u7b80\u5355\u7684\u9608\u503c\u7ed3\u6784\u3002\u5728\u8be5\u7b56\u7565\u4e0b\uff0c\u57fa\u4e8e\u70ed\u80fd\u7684\u4ea7\u6c34\u968f\u7740\u53ef\u518d\u751f\u80fd\u6e90\u53d1\u7535\u91cf\u7684\u589e\u52a0\u800c\u5355\u8c03\u9012\u51cf\uff0c\u800c\u57fa\u4e8e\u819c\u7684\u4ea7\u6c34\u5219\u5355\u8c03\u9012\u589e\u3002", "conclusion": "\u5f97\u51fa\u4e86\u6700\u4f18\u8c03\u5ea6\u7b56\u7565\u7684\u9608\u503c\u7ed3\u6784\u53ca\u5176\u80cc\u540e\u7684\u76f4\u89c2\u539f\u7406\uff0c\u5e76\u63a2\u8ba8\u4e86\u5176\u5173\u952e\u7279\u6027\u3002"}}
{"id": "2509.18855", "categories": ["cond-mat.mtrl-sci", "physics.optics"], "pdf": "https://arxiv.org/pdf/2509.18855", "abs": "https://arxiv.org/abs/2509.18855", "authors": ["M. Surynek", "A. Farkas", "J. Zubac", "P. Kubascik", "K. Olejnik", "F. Krizek", "L. Nadvornik", "T. Ostatnicky", "R. P. Campion", "V. Novak", "T. Jungwirth", "P. Nemec"], "title": "Sub-nanosecond heat-based logic, writing and reset in an antiferromagnetic magnetoresistive memory", "comment": "v1: preprint; licence: CC BY 4.0; Supplementary material is a part of\n  this submission", "summary": "Thermal logic aims to create thermal counterparts to electronic circuits. In\nthis work, we investigate experimentally the response of an analog memory\ndevice based on a thin film of an antiferromagnetic metal CuMnAs to bursts of\nheat pulses generated by the absorption of femtosecond laser pulses at room\nambient temperature. When a threshold temperature in the heat-based short-term\nmemory of the device is exceeded, the output of the in-memory logic operations\nis transferred within the same device to a long-term memory, where it can be\nretrieved at macroscopic times. The long-term memory is based on\nmagnetoresistive switching from a reference low-resistive uniform magnetic\nstate to high-resistive metastable nanofragmented magnetic states. The\nin-memory heat-based logic operations and the conversion of the outputs into\nthe electrically-readable long-term magnetoresistive memory were performed at\nsub-nanosecond time scales, making them compatible with the GHz frequencies of\nstandard electronics. Finally, we demonstrate the possibility of rapidly\nresetting the long-term memory to the reference low-resistive state by heat\npulses.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u57fa\u4e8eCuMnAs\u8584\u819c\u7684\u78c1\u70ed\u6a21\u62df\u8bb0\u5fc6\u5668\u4ef6\uff0c\u5b9e\u73b0\u4e86\u4e9a\u7eb3\u79d2\u7ea7\u7684\u70ed\u903b\u8f91\u8fd0\u7b97\u548c\u957f\u65f6\u8bb0\u5fc6\u5b58\u50a8\uff0c\u5e76\u80fd\u901a\u8fc7\u70ed\u8109\u51b2\u5feb\u901f\u91cd\u7f6e\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u63a2\u7d22\u57fa\u4e8e\u70ed\u8109\u51b2\u7684\u903b\u8f91\u8fd0\u7b97\u548c\u8bb0\u5fc6\u5b58\u50a8\u7684\u5b9e\u73b0\uff0c\u4e3a\u6784\u5efa\u70ed\u903b\u8f91\u7535\u8def\u63d0\u4f9b\u4e00\u79cd\u65b0\u7684\u601d\u8def\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u7814\u7a76\u4e86CuMnAs\u8584\u819c\u5668\u4ef6\u5bf9\u98de\u79d2\u6fc0\u5149\u8109\u51b2\u4ea7\u751f\u7684\u70ed\u8109\u51b2\u7684\u54cd\u5e94\u3002\u5f53\u70ed\u8109\u51b2\u8d85\u8fc7\u9608\u503c\u6e29\u5ea6\u65f6\uff0c\u5668\u4ef6\u5c06\u77ed\u65f6\u8bb0\u5fc6\u4e2d\u7684\u903b\u8f91\u8fd0\u7b97\u7ed3\u679c\u8f6c\u79fb\u5230\u57fa\u4e8e\u78c1\u963b\u6548\u5e94\u7684\u957f\u65f6\u8bb0\u5fc6\u4e2d\uff0c\u5e76\u901a\u8fc7\u70ed\u8109\u51b2\u5b9e\u73b0\u957f\u65f6\u8bb0\u5fc6\u7684\u91cd\u7f6e\u3002", "result": "\u5b9e\u73b0\u4e86\u4e9a\u7eb3\u79d2\u7ea7\u7684in-memory\u70ed\u903b\u8f91\u8fd0\u7b97\uff0c\u5e76\u5c06\u8fd0\u7b97\u7ed3\u679c\u4ee5\u78c1\u963b\u5f62\u5f0f\u5b58\u50a8\u5728\u957f\u65f6\u8bb0\u5fc6\u4e2d\uff0c\u8be5\u5b58\u50a8\u53ef\u7531\u7535\u4fe1\u53f7\u8bfb\u51fa\u3002\u540c\u65f6\uff0c\u8bc1\u660e\u4e86\u53ef\u4ee5\u901a\u8fc7\u70ed\u8109\u51b2\u5feb\u901f\u91cd\u7f6e\u957f\u65f6\u8bb0\u5fc6\u3002", "conclusion": "\u57fa\u4e8eCuMnAs\u8584\u819c\u7684\u78c1\u70ed\u6a21\u62df\u8bb0\u5fc6\u5668\u4ef6\u80fd\u591f\u5728\u4e9a\u7eb3\u79d2\u7ea7\u7684\u65f6\u95f4\u5c3a\u5ea6\u4e0a\u6267\u884c\u70ed\u903b\u8f91\u8fd0\u7b97\u548c\u5b58\u50a8\uff0c\u5e76\u80fd\u5feb\u901f\u91cd\u7f6e\uff0c\u8fd9\u4f7f\u5176\u4e0eGHz\u9891\u7387\u7684\u7535\u5b50\u5668\u4ef6\u517c\u5bb9\uff0c\u4e3a\u5f00\u53d1\u9ad8\u901f\u70ed\u903b\u8f91\u7535\u8def\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002"}}
{"id": "2509.18535", "categories": ["cs.CL", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.18535", "abs": "https://arxiv.org/abs/2509.18535", "authors": ["Mo Mu", "Dianqiao Lei", "Chang Li"], "title": "Trace Is In Sentences: Unbiased Lightweight ChatGPT-Generated Text Detector", "comment": null, "summary": "The widespread adoption of ChatGPT has raised concerns about its misuse,\nhighlighting the need for robust detection of AI-generated text. Current\nword-level detectors are vulnerable to paraphrasing or simple prompts (PSP),\nsuffer from biases induced by ChatGPT's word-level patterns (CWP) and training\ndata content, degrade on modified text, and often require large models or\nonline LLM interaction. To tackle these issues, we introduce a novel task to\ndetect both original and PSP-modified AI-generated texts, and propose a\nlightweight framework that classifies texts based on their internal structure,\nwhich remains invariant under word-level changes. Our approach encodes sentence\nembeddings from pre-trained language models and models their relationships via\nattention. We employ contrastive learning to mitigate embedding biases from\nautoregressive generation and incorporate a causal graph with counterfactual\nmethods to isolate structural features from topic-related biases. Experiments\non two curated datasets, including abstract comparisons and revised life FAQs,\nvalidate the effectiveness of our method.", "AI": {"tldr": "\u68c0\u6d4bAI\u751f\u6210\u6587\u672c\u7684\u6311\u6218\u5728\u4e8e\u5176\u6613\u88ab\u6539\u5199\u548c\u6a21\u578b\u672c\u8eab\u7684\u504f\u89c1\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6587\u672c\u5185\u90e8\u7ed3\u6784\u4e0d\u53d8\u6027\u7684\u8f7b\u91cf\u7ea7\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u7f16\u7801\u53e5\u5b50\u5d4c\u5165\u3001\u6ce8\u610f\u529b\u673a\u5236\u3001\u5bf9\u6bd4\u5b66\u4e60\u548c\u56e0\u679c\u56fe\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u5e76\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709AI\u751f\u6210\u6587\u672c\u68c0\u6d4b\u5668\u5bb9\u6613\u53d7\u5230\u6539\u5199\u6216\u7b80\u5355\u63d0\u793a\u7684\u5f71\u54cd\uff0c\u5b58\u5728\u6a21\u578b\u504f\u89c1\u3001\u8bad\u7ec3\u6570\u636e\u5185\u5bb9\u504f\u89c1\u3001\u5bf9\u4fee\u6539\u6587\u672c\u7684\u9c81\u68d2\u6027\u5dee\u4ee5\u53ca\u9700\u8981\u5927\u578b\u6a21\u578b\u6216\u5728\u7ebf\u4ea4\u4e92\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u68c0\u6d4b\u4efb\u52a1\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u57fa\u4e8e\u6587\u672c\u7684\u5185\u90e8\u7ed3\u6784\u8fdb\u884c\u5206\u7c7b\uff0c\u8fd9\u79cd\u7ed3\u6784\u5728\u8bcd\u8bed\u5c42\u9762\u4fee\u6539\u4e0b\u4fdd\u6301\u4e0d\u53d8\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\uff1a1. \u4f7f\u7528\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7f16\u7801\u53e5\u5b50\u5d4c\u5165\u30022. \u5229\u7528\u6ce8\u610f\u529b\u673a\u5236\u5bf9\u53e5\u5b50\u5d4c\u5165\u4e4b\u95f4\u7684\u5173\u7cfb\u8fdb\u884c\u5efa\u6a21\u30023. \u91c7\u7528\u5bf9\u6bd4\u5b66\u4e60\u51cf\u8f7b\u81ea\u56de\u5f52\u751f\u6210\u5f15\u8d77\u7684\u5d4c\u5165\u504f\u89c1\u30024. \u7ed3\u5408\u56e0\u679c\u56fe\u548c\u53cd\u4e8b\u5b9e\u65b9\u6cd5\uff0c\u5c06\u7ed3\u6784\u7279\u5f81\u4e0e\u4e3b\u9898\u504f\u89c1\u5206\u79bb\u5f00\u3002", "result": "\u5728\u4e24\u4e2a\u7cbe\u5fc3\u7b56\u5212\u7684\u6570\u636e\u96c6\uff08\u5305\u62ec\u6458\u8981\u6bd4\u8f83\u548c\u4fee\u6539\u540e\u7684\u751f\u6d3bFAQ\uff09\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u57fa\u4e8e\u5185\u90e8\u7ed3\u6784\u5206\u6790\u7684\u68c0\u6d4b\u6846\u67b6\u80fd\u591f\u6709\u6548\u68c0\u6d4b\u539f\u59cb\u548c\u7ecf\u8fc7\u6539\u5199\u7684AI\u751f\u6210\u6587\u672c\uff0c\u5e76\u80fd\u514b\u670d\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2509.18608", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18608", "abs": "https://arxiv.org/abs/2509.18608", "authors": ["Ana Luiza Mineiro", "Francisco Affonso", "Marcelo Becker"], "title": "End-to-End Crop Row Navigation via LiDAR-Based Deep Reinforcement Learning", "comment": "Accepted to the 22nd International Conference on Advanced Robotics\n  (ICAR 2025). 7 pages", "summary": "Reliable navigation in under-canopy agricultural environments remains a\nchallenge due to GNSS unreliability, cluttered rows, and variable lighting. To\naddress these limitations, we present an end-to-end learning-based navigation\nsystem that maps raw 3D LiDAR data directly to control commands using a deep\nreinforcement learning policy trained entirely in simulation. Our method\nincludes a voxel-based downsampling strategy that reduces LiDAR input size by\n95.83%, enabling efficient policy learning without relying on labeled datasets\nor manually designed control interfaces. The policy was validated in\nsimulation, achieving a 100% success rate in straight-row plantations and\nshowing a gradual decline in performance as row curvature increased, tested\nacross varying sinusoidal frequencies and amplitudes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u7aef\u5230\u7aef\u5bfc\u822a\u7cfb\u7edf\uff0c\u7528\u4e8e\u89e3\u51b3\u5728\u906e\u6321\u5bc6\u96c6\u3001GNSS\u4fe1\u53f7\u4e0d\u53ef\u9760\u7684\u519c\u4e1a\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u519c\u4e1a\u73af\u5883\u5bfc\u822a\u65b9\u6cd5\u5728GNSS\u4e0d\u53ef\u9760\u3001\u884c\u95f4\u6742\u4e71\u3001\u5149\u7167\u53d8\u5316\u7b49\u56e0\u7d20\u4e0b\u5b58\u5728\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4f53\u7d20\u4e0b\u91c7\u6837\u7b56\u7565\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5c063D LiDAR\u539f\u59cb\u6570\u636e\u76f4\u63a5\u6620\u5c04\u5230\u63a7\u5236\u6307\u4ee4\uff0c\u8f93\u5165\u7684LiDAR\u6570\u636e\u5c3a\u5bf8\u51cf\u5c11\u4e8695.83%\uff0c\u65e0\u9700\u6807\u7b7e\u6570\u636e\u96c6\u6216\u624b\u52a8\u8bbe\u8ba1\u7684\u63a7\u5236\u63a5\u53e3\uff0c\u5373\u53ef\u5728\u6a21\u62df\u73af\u5883\u4e2d\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728\u6a21\u62df\u73af\u5883\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u7b14\u76f4\u884c\u8d70\u7684\u79cd\u690d\u56ed\u4e2d\u5b9e\u73b0\u4e86100%\u7684\u6210\u529f\u7387\uff0c\u5728\u4e0d\u540c\u5f2f\u66f2\u5ea6\uff08\u4e0d\u540c\u6b63\u5f26\u9891\u7387\u548c\u5e45\u5ea6\uff09\u7684\u884c\u4e2d\u8868\u73b0\u51fa\u9010\u6b65\u4e0b\u964d\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5728\u6a21\u62df\u73af\u5883\u4e2d\u8bad\u7ec3\u7684\u7aef\u5230\u7aef\u5b66\u4e60\u5bfc\u822a\u7cfb\u7edf\uff0c\u80fd\u591f\u6709\u6548\u5730\u5904\u7406\u519c\u4e1a\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u6311\u6218\uff0c\u4f46\u6027\u80fd\u4f1a\u968f\u7740\u884c\u5f2f\u66f2\u5ea6\u7684\u589e\u52a0\u800c\u4e0b\u964d\u3002"}}
{"id": "2509.18308", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18308", "abs": "https://arxiv.org/abs/2509.18308", "authors": ["Yixin Zhang", "Ryan Chamberlain", "Lawrance Ngo", "Kevin Kramer", "Maciej A. Mazurowski"], "title": "Rethinking Pulmonary Embolism Segmentation: A Study of Current Approaches and Challenges with an Open Weight Model", "comment": "submitted to WACV 2026 application track, model weights available at:\n  https://github.com/mazurowski-lab/PulmonaryEmbolismSegmentation", "summary": "In this study, we curated a densely annotated in-house dataset comprising 490\nCTPA scans. Using this dataset, we systematically evaluated nine widely used\nsegmentation architectures from both the CNN and Vision Transformer (ViT)\nfamilies, initialized with either pretrained or random weights, under a unified\ntesting framework as a performance audit. Our study leads to several important\nobservations: (1) 3D U-Net with a ResNet encoder remains a highly effective\narchitecture for PE segmentation; (2) 3D models are particularly well-suited to\nthis task given the morphological characteristics of emboli; (3) CNN-based\nmodels generally yield superior performance compared to their ViT-based\ncounterparts in PE segmentation; (4) classification-based pretraining, even on\nlarge PE datasets, can adversely impact segmentation performance compared to\ntraining from scratch, suggesting that PE classification and segmentation may\nrely on different sets of discriminative features; (5) different model\narchitectures show a highly consistent pattern of segmentation performance when\ntrained on the same data; and (6) while central and large emboli can be\nsegmented with satisfactory accuracy, distal emboli remain challenging due to\nboth task complexity and the scarcity of high-quality datasets. Besides these\nfindings, our best-performing model achieves a mean Dice score of 0.7131 for\nsegmentation. It detects 181 emboli with 49 false positives and 28 false\nnegatives from 60 in-house testing scans. Its generalizability is further\nvalidated on public datasets.", "AI": {"tldr": "\u672c\u7814\u7a76\u5728\u4e00\u4e2a\u5305\u542b490\u4e2aCTPA\u626b\u63cf\u7684\u5185\u90e8\u6570\u636e\u96c6\u4e2d\uff0c\u8bc4\u4f30\u4e86\u4e5d\u79cd\u5e7f\u6cdb\u4f7f\u7528\u7684CNN\u548cViT\u5206\u5272\u67b6\u6784\u3002\u7814\u7a76\u53d1\u73b03D U-Net\u8868\u73b0\u6700\u4f73\uff0c3D\u6a21\u578b\u548cCNN\u6a21\u578b\u6bd4ViT\u6a21\u578b\u66f4\u9002\u5408\u6b64\u4efb\u52a1\u3002\u9884\u8bad\u7ec3\u53ef\u80fd\u635f\u5bb3\u5206\u5272\u6027\u80fd\uff0c\u800c\u8fdc\u7aef\u6813\u5b50\u5206\u5272\u4ecd\u5177\u6311\u6218\u6027\u3002\u6700\u4f73\u6a21\u578b\u8fbe\u5230\u4e860.7131\u7684Dice\u5206\u6570\uff0c\u5e76\u5728\u516c\u5171\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u7cfb\u7edf\u5730\u8bc4\u4f30\u548c\u6bd4\u8f83\u4e0d\u540c\u7684\u5206\u5272\u6a21\u578b\uff08CNN\u548cViT\u5bb6\u65cf\uff09\u5728\u80ba\u6813\u585e\uff08PE\uff09\u5206\u5272\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u4ee5\u627e\u51fa\u6700\u9002\u5408\u8be5\u4efb\u52a1\u7684\u6a21\u578b\uff0c\u5e76\u6df1\u5165\u4e86\u89e3\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u7684\u56e0\u7d20\u3002", "method": "\u7814\u7a76\u4eba\u5458\u5728\u4e00\u4e2a\u5305\u542b490\u4e2aCTPA\u626b\u63cf\u7684\u5185\u90e8\u6570\u636e\u96c6\u4e2d\uff0c\u5bf9\u4e5d\u79cd\u5e7f\u6cdb\u4f7f\u7528\u7684CNN\u548cViT\u5206\u5272\u67b6\u6784\uff08\u4f7f\u7528\u9884\u8bad\u7ec3\u6216\u968f\u673a\u6743\u91cd\u521d\u59cb\u5316\uff09\u8fdb\u884c\u4e86\u7edf\u4e00\u7684\u6d4b\u8bd5\u6846\u67b6\u4e0b\u7684\u6027\u80fd\u8bc4\u4f30\u3002", "result": "\u7814\u7a76\u53d1\u73b03D U-Net\uff08\u5e26\u6709ResNet\u7f16\u7801\u5668\uff09\u5728PE\u5206\u5272\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c3D\u6a21\u578b\u6bd4ViT\u6a21\u578b\u66f4\u9002\u5408\u8be5\u4efb\u52a1\u3002\u9884\u8bad\u7ec3\uff08\u5373\u4f7f\u5728\u5927\u578bPE\u6570\u636e\u96c6\u4e0a\uff09\u6709\u65f6\u4f1a\u635f\u5bb3\u5206\u5272\u6027\u80fd\uff0c\u800cCNN\u6a21\u578b\u901a\u5e38\u4f18\u4e8eViT\u6a21\u578b\u3002\u6a21\u578b\u6027\u80fd\u6a21\u5f0f\u5728\u76f8\u540c\u6570\u636e\u8bad\u7ec3\u4e0b\u8868\u73b0\u4e00\u81f4\u3002\u4e2d\u592e\u548c\u5927\u578b\u6813\u5b50\u5206\u5272\u51c6\u786e\u7387\u5c1a\u53ef\uff0c\u4f46\u8fdc\u7aef\u6813\u5b50\u5206\u5272\u4ecd\u5177\u6311\u6218\u6027\u3002\u6700\u4f73\u6a21\u578b\u5b9e\u73b0\u4e860.7131\u7684\u5e73\u5747Dice\u5206\u6570\uff0c\u5e76\u572860\u4e2a\u5185\u90e8\u6d4b\u8bd5\u626b\u63cf\u4e2d\u68c0\u6d4b\u5230181\u4e2a\u6813\u5b50\uff0c\u540c\u65f6\u5728\u516c\u5171\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "3D U-Net\u662fPE\u5206\u5272\u7684\u6709\u6548\u67b6\u6784\uff0c3D\u6a21\u578b\u548cCNN\u6a21\u578b\u5728\u8be5\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u4e8eViT\u6a21\u578b\u3002\u9884\u8bad\u7ec3\u7b56\u7565\u9700\u8981\u8c28\u614e\uff0c\u5e76\u4e14\u8fdc\u7aef\u6813\u5b50\u7684\u5206\u5272\u4ecd\u7136\u662f\u4e00\u4e2a\u96be\u9898\uff0c\u9700\u8981\u66f4\u9ad8\u8d28\u91cf\u548c\u66f4\u5927\u89c4\u6a21\u7684\u6570\u636e\u96c6\u3002"}}
{"id": "2509.18490", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2509.18490", "abs": "https://arxiv.org/abs/2509.18490", "authors": ["Amita Gnanapandithan", "Li Qian", "Hoi-Kwong Lo"], "title": "Mitigating Phase Correlations in Quantum Key Distribution Using Path-Selection Modulation", "comment": null, "summary": "Phase correlations are an under-explored vulnerability in QKD. Here, we\npresent an experimental and simulated characterization of correlations arising\nfrom electro-optic phase encoding, over repetition rates up to the GHz level.\nTo mitigate this vulnerability (and all side channels arising from active phase\nmodulators), we propose a \"path-selection modulation\" source that eliminates\nthe need for active phase modulation altogether. Encoding is achieved by\nrandomly selecting between multiple paths, each path corresponding to one of\nthe desired encoded states. Phase randomization is achieved using\ngain-switching. We characterize this source at a clock rate of 1 GHz.", "AI": {"tldr": "\u5229\u7528\u201c\u8def\u5f84\u9009\u62e9\u8c03\u5236\u201d\u5149\u6e90\uff0c\u6d88\u9664\u5bf9\u4e3b\u52a8\u76f8\u4f4d\u8c03\u5236\u5668\u548c\u8bf1\u5bfc\u4fa7\u4fe1\u9053\u7684\u9700\u6c42\uff0c\u4ee5\u589e\u5f3a\u91cf\u5b50\u5bc6\u94a5\u5206\u53d1\uff08QKD\uff09\u7684\u5b89\u5168\u6027\u3002", "motivation": "\u91cf\u5b50\u5bc6\u94a5\u5206\u53d1\uff08QKD\uff09\u4e2d\u5b58\u5728\u7684\u76f8\u4f4d\u76f8\u5173\u6027\u662f\u4e00\u4e2a\u672a\u88ab\u5145\u5206\u63a2\u7d22\u7684\u6f0f\u6d1e\uff0c\u53ef\u80fd\u5371\u53ca\u901a\u4fe1\u5b89\u5168\u3002", "method": "\u63d0\u51fa\u5e76\u5b9e\u73b0\u4e86\u4e00\u79cd\u201c\u8def\u5f84\u9009\u62e9\u8c03\u5236\u201d\u5149\u6e90\uff0c\u8be5\u5149\u6e90\u901a\u8fc7\u5728\u591a\u4e2a\u8def\u5f84\u4e4b\u95f4\u8fdb\u884c\u968f\u673a\u9009\u62e9\u6765\u7f16\u7801\u91cf\u5b50\u6001\uff0c\u907f\u514d\u4e86\u4f7f\u7528\u4e3b\u52a8\u76f8\u4f4d\u8c03\u5236\u5668\uff0c\u4ece\u800c\u6d88\u9664\u4e86\u4e0e\u4e4b\u76f8\u5173\u7684\u4fa7\u4fe1\u9053\u6f0f\u6d1e\u3002\u5229\u7528\u589e\u76ca\u5207\u6362\u6280\u672f\u5b9e\u73b0\u76f8\u4f4d\u968f\u673a\u5316\uff0c\u5e76\u57281 GHz\u7684\u65f6\u949f\u901f\u7387\u4e0b\u8fdb\u884c\u4e86\u8868\u5f81\u3002", "result": "\u8be5\u5149\u6e90\u6210\u529f\u5730\u5728\u9ad8\u8fbe1 GHz\u7684\u91cd\u590d\u7387\u4e0b\u8fdb\u884c\u4e86\u5b9e\u9a8c\u548c\u6a21\u62df\u8868\u5f81\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u6d88\u9664\u76f8\u4f4d\u76f8\u5173\u6027\u6f0f\u6d1e\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u201c\u8def\u5f84\u9009\u62e9\u8c03\u5236\u201d\u5149\u6e90\u4e3a\u89e3\u51b3QKD\u4e2d\u7684\u76f8\u4f4d\u76f8\u5173\u6027\u4fa7\u4fe1\u9053\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u671b\u63d0\u9ad8QKD\u7cfb\u7edf\u7684\u6574\u4f53\u5b89\u5168\u6027\u3002"}}
{"id": "2509.18124", "categories": ["cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2509.18124", "abs": "https://arxiv.org/abs/2509.18124", "authors": ["Edmund Agyemang", "Lawrence Agbota", "Vincent Agbenyeavu", "Peggy Akabuah", "Bismark Bimpong", "Christopher Attafuah"], "title": "Prediction of Coffee Ratings Based On Influential Attributes Using SelectKBest and Optimal Hyperparameters", "comment": "13 pages, 6 figures and 4 tables", "summary": "This study explores the application of supervised machine learning algorithms\nto predict coffee ratings based on a combination of influential textual and\nnumerical attributes extracted from user reviews. Through careful data\npreprocessing including text cleaning, feature extraction using TF-IDF, and\nselection with SelectKBest, the study identifies key factors contributing to\ncoffee quality assessments. Six models (Decision Tree, KNearest Neighbors,\nMulti-layer Perceptron, Random Forest, Extra Trees, and XGBoost) were trained\nand evaluated using optimized hyperparameters. Model performance was assessed\nprimarily using F1-score, Gmean, and AUC metrics. Results demonstrate that\nensemble methods (Extra Trees, Random Forest, and XGBoost), as well as\nMulti-layer Perceptron, consistently outperform simpler classifiers (Decision\nTrees and K-Nearest Neighbors) in terms of evaluation metrics such as F1\nscores, G-mean and AUC. The findings highlight the essence of rigorous feature\nselection and hyperparameter tuning in building robust predictive systems for\nsensory product evaluation, offering a data driven approach to complement\ntraditional coffee cupping by expertise of trained professionals.", "AI": {"tldr": "\u4f7f\u7528\u76d1\u7763\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u9884\u6d4b\u5496\u5561\u8bc4\u5206\u3002", "motivation": "\u63d0\u53d6\u7528\u6237\u8bc4\u8bba\u4e2d\u7684\u6587\u672c\u548c\u6570\u503c\u5c5e\u6027\uff0c\u627e\u51fa\u5f71\u54cd\u5496\u5561\u54c1\u8d28\u8bc4\u5206\u7684\u5173\u952e\u56e0\u7d20\u3002", "method": "\u4f7f\u7528TF-IDF\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\uff0cSelectKBest\u8fdb\u884c\u7279\u5f81\u9009\u62e9\u3002\u8bad\u7ec3\u5e76\u8bc4\u4f30\u4e86\u516d\u79cd\u6a21\u578b\uff08\u51b3\u7b56\u6811\u3001K\u8fd1\u90bb\u3001\u591a\u5c42\u611f\u77e5\u5668\u3001\u968f\u673a\u68ee\u6797\u3001Extra Trees\u548cXGBoost\uff09\uff0c\u5e76\u4f18\u5316\u4e86\u8d85\u53c2\u6570\u3002\u4e3b\u8981\u4f7f\u7528F1\u5206\u6570\u3001Gmean\u548cAUC\u6307\u6807\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u3002", "result": "\u96c6\u6210\u65b9\u6cd5\uff08Extra Trees\u3001\u968f\u673a\u68ee\u6797\u548cXGBoost\uff09\u4ee5\u53ca\u591a\u5c42\u611f\u77e5\u5668\u5728F1\u5206\u6570\u3001G-mean\u548cAUC\u7b49\u8bc4\u4f30\u6307\u6807\u4e0a\u6301\u7eed\u4f18\u4e8e\u7b80\u5355\u7684\u5206\u7c7b\u5668\uff08\u51b3\u7b56\u6811\u548cK\u8fd1\u90bb\uff09\u3002", "conclusion": "\u4e25\u683c\u7684\u7279\u5f81\u9009\u62e9\u548c\u8d85\u53c2\u6570\u8c03\u6574\u5bf9\u4e8e\u6784\u5efa\u7a33\u5065\u7684\u9884\u6d4b\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\uff0c\u53ef\u4ee5\u4e3a\u4f20\u7edf\u7684\u5496\u5561\u676f\u6d4b\u63d0\u4f9b\u6570\u636e\u9a71\u52a8\u7684\u8865\u5145\u3002"}}
{"id": "2509.18382", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18382", "abs": "https://arxiv.org/abs/2509.18382", "authors": ["Adarsha Balaji", "Le Chen", "Rajeev Thakur", "Franck Cappello", "Sandeep Madireddy"], "title": "Evaluating the Safety and Skill Reasoning of Large Reasoning Models Under Compute Constraints", "comment": null, "summary": "Test-time compute scaling has demonstrated the ability to improve the\nperformance of reasoning language models by generating longer chain-of-thought\n(CoT) sequences. However, this increase in performance comes with a significant\nincrease in computational cost. In this work, we investigate two compute\nconstraint strategies: (1) reasoning length constraint and (2) model\nquantization, as methods to reduce the compute demand of reasoning models and\nstudy their impact on their safety performance. Specifically, we explore two\napproaches to apply compute constraints to reasoning models: (1) fine-tuning\nreasoning models using a length controlled policy optimization (LCPO) based\nreinforcement learning method to satisfy a user-defined CoT reasoning length,\nand (2) applying quantization to maximize the generation of CoT sequences\nwithin a user-defined compute constraint. Furthermore, we study the trade-off\nbetween the computational efficiency and the safety of the model.", "AI": {"tldr": "\u901a\u8fc7\u957f\u5ea6\u7ea6\u675f\u548c\u6a21\u578b\u91cf\u5316\u6765\u51cf\u5c11\u63a8\u7406\u6a21\u578b\u7684\u8ba1\u7b97\u9700\u6c42\uff0c\u540c\u65f6\u7814\u7a76\u5176\u5bf9\u5b89\u5168\u6027\u7684\u5f71\u54cd\u3002", "motivation": "\u73b0\u6709\u7684\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u6269\u5c55\u65b9\u6cd5\u867d\u7136\u80fd\u901a\u8fc7\u751f\u6210\u66f4\u957f\u7684\u94fe\u5f0f\u601d\u8003\uff08CoT\uff09\u5e8f\u5217\u6765\u63d0\u5347\u63a8\u7406\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u663e\u8457\u589e\u52a0\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u6b64\u95ee\u9898\u3002", "method": "\u7814\u7a76\u4e86\u4e24\u79cd\u8ba1\u7b97\u7ea6\u675f\u7b56\u7565\uff1a\uff081\uff09\u63a8\u7406\u957f\u5ea6\u7ea6\u675f\uff1a\u4f7f\u7528\u57fa\u4e8e\u957f\u5ea6\u53d7\u63a7\u7b56\u7565\u4f18\u5316\uff08LCPO\uff09\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u8fdb\u884c\u5fae\u8c03\uff0c\u4ee5\u6ee1\u8db3\u7528\u6237\u5b9a\u4e49\u7684CoT\u63a8\u7406\u957f\u5ea6\uff1b\uff082\uff09\u6a21\u578b\u91cf\u5316\uff1a\u5728\u7528\u6237\u5b9a\u4e49\u7684\u8ba1\u7b97\u7ea6\u675f\u5185\uff0c\u6700\u5927\u5316CoT\u5e8f\u5217\u7684\u751f\u6210\u3002", "result": "\u63a2\u7a76\u4e86\u8ba1\u7b97\u6548\u7387\u4e0e\u6a21\u578b\u5b89\u5168\u6027\u7684\u6743\u8861\u3002", "conclusion": "\u8ba1\u7b97\u7ea6\u675f\u7b56\u7565\uff08\u957f\u5ea6\u7ea6\u675f\u548c\u6a21\u578b\u91cf\u5316\uff09\u53ef\u4ee5\u964d\u4f4e\u63a8\u7406\u6a21\u578b\u7684\u8ba1\u7b97\u9700\u6c42\uff0c\u5e76\u7814\u7a76\u4e86\u5176\u5bf9\u6a21\u578b\u5b89\u5168\u6027\u7684\u5f71\u54cd\u3002"}}
{"id": "2509.18467", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18467", "abs": "https://arxiv.org/abs/2509.18467", "authors": ["Zeyu Liu", "Souvik Kundu", "Lianghao Jiang", "Anni Li", "Srikanth Ronanki", "Sravan Bodapati", "Gourav Datta", "Peter A. Beerel"], "title": "LAWCAT: Efficient Distillation from Quadratic to Linear Attention with Convolution across Tokens for Long Context Modeling", "comment": "17 pages, 8 figures", "summary": "Although transformer architectures have achieved state-of-the-art performance\nacross diverse domains, their quadratic computational complexity with respect\nto sequence length remains a significant bottleneck, particularly for\nlatency-sensitive long-context applications. While recent linear-complexity\nalternatives are increasingly powerful, effectively training them from scratch\nis still resource-intensive. To overcome these limitations, we propose LAWCAT\n(Linear Attention with Convolution Across Time), a novel linearization\nframework designed to efficiently transfer the capabilities of pre-trained\ntransformers into a performant linear attention architecture. LAWCAT integrates\ncausal Conv1D layers to enhance local dependency modeling and employs\nnormalized gated linear attention to improve generalization across varying\ncontext lengths. Our comprehensive evaluations demonstrate that, distilling\nMistral-7B with only 1K-length sequences yields over 90\\% passkey retrieval\naccuracy up to 22K tokens, significantly extending its effective context\nwindow. Similarly, Llama3.2-1B LAWCAT variant achieves competitive performance\non S-NIAH 1\\&2\\&3 tasks (1K-8K context length) and BABILong benchmark\n(QA2\\&QA3, 0K-16K context length), requiring less than 0.1\\% pre-training\ntokens compared with pre-training models. Furthermore, LAWCAT exhibits faster\nprefill speeds than FlashAttention-2 for sequences exceeding 8K tokens. LAWCAT\nthus provides an efficient pathway to high-performance, long-context linear\nmodels suitable for edge deployment, reducing reliance on extensive\nlong-sequence training data and computational resources.", "AI": {"tldr": "LAWCAT\u662f\u4e00\u4e2a\u65b0\u7684\u6846\u67b6\uff0c\u53ef\u4ee5\u5c06\u9884\u8bad\u7ec3Transformer\u7684\u80fd\u529b\u8f6c\u79fb\u5230\u9ad8\u6548\u7684\u7ebf\u6027\u6ce8\u610f\u529b\u6a21\u578b\u4e2d\uff0c\u89e3\u51b3\u4e86Transformer\u7684\u957f\u5e8f\u5217\u8ba1\u7b97\u74f6\u9888\u95ee\u9898\u3002", "motivation": "Transformer\u6a21\u578b\u5728\u5904\u7406\u957f\u5e8f\u5217\u65f6\u5b58\u5728\u4e8c\u6b21\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5728\u957f\u4e0a\u4e0b\u6587\u5e94\u7528\u4e2d\u7684\u6027\u80fd\uff0c\u800c\u73b0\u6709\u7684\u7ebf\u6027\u590d\u6742\u5ea6\u6a21\u578b\u8bad\u7ec3\u6210\u672c\u9ad8\u3002", "method": "LAWCAT\u6846\u67b6\u7ed3\u5408\u4e86\u56e0\u679cConv1D\u5c42\u6765\u5efa\u6a21\u5c40\u90e8\u4f9d\u8d56\u5173\u7cfb\uff0c\u5e76\u4f7f\u7528\u5f52\u4e00\u5316\u7684\u95e8\u63a7\u7ebf\u6027\u6ce8\u610f\u529b\u6765\u63d0\u9ad8\u8de8\u4e0d\u540c\u4e0a\u4e0b\u6587\u957f\u5ea6\u7684\u6cdb\u5316\u80fd\u529b\u3002", "result": "LAWCAT\u5728\u957f\u4e0a\u4e0b\u6587\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f8b\u5982Mistral-7B\u6a21\u578b\u5728\u4ec5\u4f7f\u75281K\u5e8f\u5217\u8fdb\u884c\u84b8\u998f\u540e\uff0c\u572822K\u957f\u5ea6\u7684\u5e8f\u5217\u4e0a\u4ecd\u80fd\u8fbe\u523090%\u4ee5\u4e0a\u7684passkey\u68c0\u7d22\u51c6\u786e\u7387\u3002\u540c\u65f6\uff0cLlama3.2-1B LAWCAT\u53d8\u4f53\u5728S-NIAH\u548cBABILong\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u7ade\u4e89\u529b\uff0c\u4e14\u9884\u8bad\u7ec3\u8ba1\u7b97\u91cf\u8fdc\u4f4e\u4e8e\u73b0\u6709\u6a21\u578b\u3002\u6b64\u5916\uff0cLAWCAT\u5728\u957f\u5e8f\u5217\uff08\u8d85\u8fc78K\uff09\u4e0a\u7684\u9884\u586b\u5145\u901f\u5ea6\u4f18\u4e8eFlashAttention-2\u3002", "conclusion": "LAWCAT\u4e3a\u6784\u5efa\u9ad8\u6027\u80fd\u3001\u957f\u4e0a\u4e0b\u6587\u7684\u7ebf\u6027\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u6761\u6709\u6548\u9014\u5f84\uff0c\u9002\u7528\u4e8e\u8fb9\u7f18\u90e8\u7f72\uff0c\u5e76\u51cf\u5c11\u4e86\u5bf9\u957f\u5e8f\u5217\u8bad\u7ec3\u6570\u636e\u548c\u8ba1\u7b97\u8d44\u6e90\u7684\u9700\u6c42\u3002"}}
{"id": "2509.19266", "categories": ["eess.SY", "cs.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.19266", "abs": "https://arxiv.org/abs/2509.19266", "authors": ["Charis Stamouli", "Leonardo F. Toso", "Anastasios Tsiamis", "George J. Pappas", "James Anderson"], "title": "Policy Gradient Bounds in Multitask LQR", "comment": null, "summary": "We analyze the performance of policy gradient in multitask linear quadratic\nregulation (LQR), where the system and cost parameters differ across tasks. The\nmain goal of multitask LQR is to find a controller with satisfactory\nperformance on every task. Prior analyses on relevant contexts fail to capture\nclosed-loop task similarities, resulting in conservative performance\nguarantees. To account for such similarities, we propose bisimulation-based\nmeasures of task heterogeneity. Our measures employ new bisimulation functions\nto bound the cost gradient distance between a pair of tasks in closed loop with\na common stabilizing controller. Employing these measures, we derive\nsuboptimality bounds for both the multitask optimal controller and the\nasymptotic policy gradient controller with respect to each of the tasks. We\nfurther provide conditions under which the policy gradient iterates remain\nstabilizing for every system. For multiple random sets of certain tasks, we\nobserve that our bisimulation-based measures improve upon baseline measures of\ntask heterogeneity dramatically.", "AI": {"tldr": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53cc\u6a21\u4eff\u5ea6\u91cf\u7684\u4efb\u52a1\u5f02\u8d28\u6027\u65b9\u6cd5\uff0c\u7528\u4e8e\u591a\u4efb\u52a1\u7ebf\u6027\u4e8c\u6b21\u8c03\u8282\uff08LQR\uff09\u95ee\u9898\uff0c\u4ee5\u6539\u8fdb\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u7684\u6027\u80fd\u4fdd\u8bc1\u3002", "motivation": "\u73b0\u6709\u5173\u4e8e\u591a\u4efb\u52a1 LQR \u7684\u5206\u6790\u672a\u80fd\u6355\u6349\u5230\u95ed\u73af\u4efb\u52a1\u7684\u76f8\u4f3c\u6027\uff0c\u5bfc\u81f4\u6027\u80fd\u4fdd\u8bc1\u8fc7\u4e8e\u4fdd\u5b88\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u8861\u91cf\u4efb\u52a1\u5f02\u8d28\u6027\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u6539\u8fdb\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u53cc\u6a21\u4eff\u5ea6\u91cf\u7684\u4efb\u52a1\u5f02\u8d28\u6027\u65b9\u6cd5\uff0c\u4f7f\u7528\u65b0\u7684\u53cc\u6a21\u4eff\u51fd\u6570\u6765\u7ea6\u675f\u5728\u5171\u540c\u7a33\u5b9a\u63a7\u5236\u5668\u4e0b\uff0c\u4e00\u5bf9\u4efb\u52a1\u4e4b\u95f4\u7684\u95ed\u73af\u6210\u672c\u68af\u5ea6\u8ddd\u79bb\u3002\u5229\u7528\u8fd9\u4e9b\u5ea6\u91cf\uff0c\u63a8\u5bfc\u51fa\u591a\u4efb\u52a1\u6700\u4f18\u63a7\u5236\u5668\u548c\u6e10\u8fd1\u7b56\u7565\u68af\u5ea6\u63a7\u5236\u5668\u5728\u6bcf\u4e2a\u4efb\u52a1\u4e0a\u7684\u6b21\u4f18\u754c\u3002", "result": "\u5728\u591a\u4e2a\u968f\u673a\u4efb\u52a1\u96c6\u4e0a\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u57fa\u4e8e\u53cc\u6a21\u4eff\u5ea6\u91cf\u7684\u4efb\u52a1\u5f02\u8d28\u6027\u65b9\u6cd5\uff0c\u76f8\u6bd4\u4e8e\u57fa\u7ebf\u5ea6\u91cf\uff0c\u5728\u6539\u8fdb\u4efb\u52a1\u5f02\u8d28\u6027\u65b9\u9762\u8868\u73b0\u51fa\u663e\u8457\u7684\u63d0\u5347\u3002", "conclusion": "\u57fa\u4e8e\u53cc\u6a21\u4eff\u5ea6\u91cf\u7684\u4efb\u52a1\u5f02\u8d28\u6027\u65b9\u6cd5\u53ef\u4ee5\u66f4\u51c6\u786e\u5730\u6355\u6349\u591a\u4efb\u52a1 LQR \u4e2d\u7684\u4efb\u52a1\u76f8\u4f3c\u6027\uff0c\u4ece\u800c\u4e3a\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u63d0\u4f9b\u66f4\u4f18\u7684\u6027\u80fd\u4fdd\u8bc1\uff0c\u5e76\u5728\u5b9e\u8df5\u4e2d\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u6548\u679c\u3002"}}
{"id": "2509.18866", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2509.18866", "abs": "https://arxiv.org/abs/2509.18866", "authors": ["Georgy Ermolaev", "Tagir Mazitov", "Arslan Mazitov", "Adilet Toksumakov", "Dmitriy Grudinin", "Anton Minnekhanov", "Gleb Tselikov", "Dmitry Yakubovsky", "Gleb Tikhonowski", "Nikolay Pak", "Umer Ahsan", "Aleksandr Slavich", "Mikhail Mironov", "Alexey Tsapenko", "Andrey Vyshnevyy", "Ivan Kruglov", "Zdenek Sofer", "Aleksey Arsenin", "Kostya Novoselov", "Andrey Katanin", "Valentyn Volkov"], "title": "Giant optical anisotropy in CrSBr from giant exciton oscillator strength", "comment": "13 pages, 5 figures", "summary": "The interplay between dimensionality and electronic correlations in van der\nWaals (vdW) materials offers a powerful toolkit for engineering light-matter\ninteractions at the nanoscale. Excitons, bound electron-hole pairs, are central\nto this endeavor, yet maximizing their oscillator strength, which dictates the\ninteraction cross-section, remains a challenge. Conventional wisdom suggests a\ntrade-off, where the observable oscillator strength often decreases in strongly\nbound systems due to population dynamics. Here, we unveil a colossal oscillator\nstrength associated with the quasi-one-dimensional (quasi-1D) excitons in the\nlayered magnetic semiconductor CrSBr, which fundamentally defies this\nestablished scaling law. Through comprehensive optical characterization and ab\ninitio calculations, we establish that this anomalous enhancement originates\ndirectly from the reduced dimensionality, which enforces an increased\nelectron-hole wavefunction overlap. Moreover, we find a close connection\nbetween fundamental exciton and local spin fluctuations that contribute to the\nopening of the gap in the electronic spectrum. The resulting optical anisotropy\nshows a giant in-plane birefringence (Delta_n = 1.45) and profoundly\nanisotropic waveguiding, which we directly visualize using nano-optical\nimaging. Leveraging this extreme response, we realize a true zero-order\nquarter-wave plate with an unprecedented wavelength-to-thickness ratio\n(lambda/t) exceeding 3.4, surpassing the limits of current miniaturization\ntechnologies, including state-of-the-art metasurfaces. Our findings underscore\nthe profound impact of dimensionality engineering in magnetic vdW materials for\nrealizing novel regimes of light-matter coupling and developing next-generation\nultracompact photonic architectures.", "AI": {"tldr": "CrSBr\u6750\u6599\u4e2d\u7684\u51c6\u4e00\u7ef4\u6fc0\u5b50\u8868\u73b0\u51fa\u5de8\u5927\u7684\u632f\u8361\u5668\u5f3a\u5ea6\uff0c\u8fd9\u5f97\u76ca\u4e8e\u964d\u4f4e\u7684\u7ef4\u5ea6\u5bfc\u81f4\u7684\u7535\u5b50-\u7a7a\u7a74\u6ce2\u51fd\u6570\u91cd\u53e0\u589e\u52a0\uff0c\u5e76\u4e0e\u81ea\u65cb\u6da8\u843d\u76f8\u5173\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u8d85\u7d27\u51d1\u7684\u6ce2\u7247\u3002", "motivation": "\u7814\u7a76\u8303\u5fb7\u534e\u6750\u6599\u4e2d\u7ef4\u5ea6\u548c\u7535\u5b50\u5173\u8054\u5982\u4f55\u5f71\u54cd\u5149-\u7269\u8d28\u76f8\u4e92\u4f5c\u7528\uff0c\u7279\u522b\u662f\u5982\u4f55\u6700\u5927\u5316\u6fc0\u5b50\u632f\u8361\u5668\u5f3a\u5ea6\u3002", "method": "\u901a\u8fc7\u5149\u5b66\u6d4b\u91cf\u548c\u4ece\u5934\u8ba1\u7b97\u7814\u7a76CrSBr\u4e2d\u7684\u6fc0\u5b50\u7279\u6027\u3002", "result": "\u53d1\u73b0CrSBr\u4e2d\u7684\u51c6\u4e00\u7ef4\u6fc0\u5b50\u5177\u6709\u5f02\u5e38\u9ad8\u7684\u632f\u8361\u5668\u5f3a\u5ea6\uff0c\u5e76\u4e0e\u81ea\u65cb\u6da8\u843d\u6709\u5173\uff0c\u8868\u73b0\u51fa\u5de8\u5927\u7684\u53cc\u6298\u5c04\u548c\u5404\u5411\u5f02\u6027\u6ce2\u5bfc\u7279\u6027\uff0c\u5e76\u6210\u529f\u5b9e\u73b0\u4e86\u8d85\u7d27\u51d1\u7684\u56db\u5206\u4e4b\u4e00\u6ce2\u7247\u3002", "conclusion": "\u7ef4\u5ea6\u5de5\u7a0b\u5728\u78c1\u6027\u8303\u5fb7\u534e\u6750\u6599\u4e2d\u5bf9\u4e8e\u5b9e\u73b0\u65b0\u9896\u7684\u5149-\u7269\u8d28\u8026\u5408\u548c\u5f00\u53d1\u8d85\u7d27\u51d1\u5149\u5b50\u5668\u4ef6\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2509.18653", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.18653", "abs": "https://arxiv.org/abs/2509.18653", "authors": ["Paris A. Karakasis", "Nicholas D. Sidiropoulos"], "title": "Subspace Clustering of Subspaces: Unifying Canonical Correlation Analysis and Subspace Clustering", "comment": "13 pages, Submitted to IEEE Transactions on Signal Processing", "summary": "We introduce a novel framework for clustering a collection of tall matrices\nbased on their column spaces, a problem we term Subspace Clustering of\nSubspaces (SCoS). Unlike traditional subspace clustering methods that assume\nvectorized data, our formulation directly models each data sample as a matrix\nand clusters them according to their underlying subspaces. We establish\nconceptual links to Subspace Clustering and Generalized Canonical Correlation\nAnalysis (GCCA), and clarify key differences that arise in this more general\nsetting. Our approach is based on a Block Term Decomposition (BTD) of a\nthird-order tensor constructed from the input matrices, enabling joint\nestimation of cluster memberships and partially shared subspaces. We provide\nthe first identifiability results for this formulation and propose scalable\noptimization algorithms tailored to large datasets. Experiments on real-world\nhyperspectral imaging datasets demonstrate that our method achieves superior\nclustering accuracy and robustness, especially under high noise and\ninterference, compared to existing subspace clustering techniques. These\nresults highlight the potential of the proposed framework in challenging\nhigh-dimensional applications where structure exists beyond individual data\nvectors.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u79f0\u4e3a\u5b50\u7a7a\u95f4\u805a\u7c7b\uff08SCoS\uff09\uff0c\u7528\u4e8e\u57fa\u4e8e\u5217\u7a7a\u95f4\u5bf9\u7a00\u758f\u77e9\u9635\u96c6\u5408\u8fdb\u884c\u805a\u7c7b\u3002\u8be5\u65b9\u6cd5\u5c06\u6bcf\u4e2a\u6570\u636e\u6837\u672c\u5efa\u6a21\u4e3a\u77e9\u9635\uff0c\u5e76\u6839\u636e\u5176\u6f5c\u5728\u5b50\u7a7a\u95f4\u8fdb\u884c\u805a\u7c7b\uff0c\u4e0e\u4f20\u7edf\u7684\u5411\u91cf\u5316\u6570\u636e\u65b9\u6cd5\u4e0d\u540c\u3002", "motivation": "\u4f20\u7edf\u5b50\u7a7a\u95f4\u805a\u7c7b\u65b9\u6cd5\u5047\u8bbe\u6570\u636e\u88ab\u5411\u91cf\u5316\uff0c\u800c\u672c\u7814\u7a76\u63d0\u51fa\u7684\u65b9\u6cd5\u76f4\u63a5\u5c06\u6570\u636e\u6837\u672c\u5efa\u6a21\u4e3a\u77e9\u9635\uff0c\u5e76\u6839\u636e\u5176\u6f5c\u5728\u5b50\u7a7a\u95f4\u8fdb\u884c\u805a\u7c7b\uff0c\u4ee5\u89e3\u51b3\u66f4\u901a\u7528\u7684\u60c5\u51b5\u3002", "method": "\u8be5\u65b9\u6cd5\u5229\u7528\u4e09\u9636\u5f20\u91cf\u5206\u89e3\uff08Block Term Decomposition, BTD\uff09\u6765\u540c\u65f6\u4f30\u8ba1\u805a\u7c7b\u6210\u5458\u548c\u90e8\u5206\u5171\u4eab\u7684\u5b50\u7a7a\u95f4\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u771f\u5b9e\u4e16\u754c\u7684\u9ad8\u5149\u8c31\u6210\u50cf\u6570\u636e\u96c6\u4e0a\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u566a\u58f0\u548c\u5e72\u6270\u6761\u4ef6\u4e0b\uff0c\u6bd4\u73b0\u6709\u7684\u5b50\u7a7a\u95f4\u805a\u7c7b\u6280\u672f\u5177\u6709\u66f4\u9ad8\u7684\u805a\u7c7b\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u6311\u6218\u6027\u9ad8\u7ef4\u5e94\u7528\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u7ed3\u6784\u8d85\u8d8a\u5355\u4e2a\u6570\u636e\u5411\u91cf\u7684\u60c5\u51b5\u4e0b\u3002"}}
{"id": "2509.18609", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18609", "abs": "https://arxiv.org/abs/2509.18609", "authors": ["Chengran Yuan", "Zijian Lu", "Zhanqi Zhang", "Yimin Zhao", "Zefan Huang", "Shuo Sun", "Jiawei Sun", "Jiahui Li", "Christina Dao Wen Lee", "Dongen Li", "Marcelo H. Ang Jr"], "title": "PIE: Perception and Interaction Enhanced End-to-End Motion Planning for Autonomous Driving", "comment": null, "summary": "End-to-end motion planning is promising for simplifying complex autonomous\ndriving pipelines. However, challenges such as scene understanding and\neffective prediction for decision-making continue to present substantial\nobstacles to its large-scale deployment. In this paper, we present PIE, a\npioneering framework that integrates advanced perception, reasoning, and\nintention modeling to dynamically capture interactions between the ego vehicle\nand surrounding agents. It incorporates a bidirectional Mamba fusion that\naddresses data compression losses in multimodal fusion of camera and LiDAR\ninputs, alongside a novel reasoning-enhanced decoder integrating Mamba and\nMixture-of-Experts to facilitate scene-compliant anchor selection and optimize\nadaptive trajectory inference. PIE adopts an action-motion interaction module\nto effectively utilize state predictions of surrounding agents to refine ego\nplanning. The proposed framework is thoroughly validated on the NAVSIM\nbenchmark. PIE, without using any ensemble and data augmentation techniques,\nachieves an 88.9 PDM score and 85.6 EPDM score, surpassing the performance of\nprior state-of-the-art methods. Comprehensive quantitative and qualitative\nanalyses demonstrate that PIE is capable of reliably generating feasible and\nhigh-quality ego trajectories.", "AI": {"tldr": "PIE\u662f\u4e00\u4e2a\u521b\u65b0\u7684\u7aef\u5230\u7aef\u8fd0\u52a8\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u878d\u5408\u9ad8\u7ea7\u611f\u77e5\u3001\u63a8\u7406\u548c\u610f\u56fe\u5efa\u6a21\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u573a\u666f\u7406\u89e3\u548c\u9884\u6d4b\u96be\u9898\uff0c\u5e76\u5728NAVSIM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u9886\u5148\u7684\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u7aef\u5230\u7aef\u8fd0\u52a8\u89c4\u5212\u5f88\u6709\u524d\u666f\uff0c\u4f46\u573a\u666f\u7406\u89e3\u548c\u6709\u6548\u7684\u51b3\u7b56\u5236\u5b9a\u9884\u6d4b\u4ecd\u7136\u662f\u5176\u5927\u89c4\u6a21\u90e8\u7f72\u7684\u91cd\u5927\u969c\u788d\u3002", "method": "PIE\u6846\u67b6\u96c6\u6210\u4e86\u53cc\u5411Mamba\u878d\u5408\uff08\u5904\u7406\u591a\u6a21\u6001\u878d\u5408\u4e2d\u7684\u6570\u636e\u538b\u7f29\u635f\u5931\uff09\u548c\u4e00\u79cd\u65b0\u9896\u7684\u589e\u5f3a\u63a8\u7406\u89e3\u7801\u5668\uff08\u96c6\u6210Mamba\u548cMixture-of-Experts\uff0c\u7528\u4e8e\u573a\u666f\u517c\u5bb9\u7684\u951a\u70b9\u9009\u62e9\u548c\u81ea\u9002\u5e94\u8f68\u8ff9\u63a8\u7406\uff09\u3002\u6b64\u5916\uff0c\u5b83\u8fd8\u91c7\u7528\u52a8\u4f5c-\u8fd0\u52a8\u4ea4\u4e92\u6a21\u5757\u6765\u5229\u7528\u5468\u56f4\u4ee3\u7406\u7684\u72b6\u6001\u9884\u6d4b\u6765\u4f18\u5316\u81ea\u8eab\u89c4\u5212\u3002", "result": "PIE\u5728NAVSIM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u672a\u4f7f\u7528\u4efb\u4f55\u96c6\u6210\u6216\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u53d6\u5f97\u4e8688.9\u7684PDM\u5206\u6570\u548c85.6\u7684EPDM\u5206\u6570\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "\u5168\u9762\u7684\u5b9a\u91cf\u548c\u5b9a\u6027\u5206\u6790\u8868\u660e\uff0cPIE\u80fd\u591f\u53ef\u9760\u5730\u751f\u6210\u53ef\u884c\u4e14\u9ad8\u8d28\u91cf\u7684\u81ea\u6211\u8f68\u8ff9\u3002"}}
{"id": "2509.18309", "categories": ["cs.CV", "cs.LG", "I.2.10"], "pdf": "https://arxiv.org/pdf/2509.18309", "abs": "https://arxiv.org/abs/2509.18309", "authors": ["Alessa Carbo", "Eric Nalisnick"], "title": "Improving Handshape Representations for Sign Language Processing: A Graph Neural Network Approach", "comment": null, "summary": "Handshapes serve a fundamental phonological role in signed languages, with\nAmerican Sign Language employing approximately 50 distinct shapes.\nHowever,computational approaches rarely model handshapes explicitly, limiting\nboth recognition accuracy and linguistic analysis.We introduce a novel graph\nneural network that separates temporal dynamics from static handshape\nconfigurations. Our approach combines anatomically-informed graph structures\nwith contrastive learning to address key challenges in handshape recognition,\nincluding subtle interclass distinctions and temporal variations. We establish\nthe first benchmark for structured handshape recognition in signing sequences,\nachieving 46% accuracy across 37 handshape classes (with baseline methods\nachieving 25%).", "AI": {"tldr": "\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\uff0c\u901a\u8fc7\u5206\u79bb\u624b\u8bed\u4e2d\u7684\u65f6\u95f4\u52a8\u6001\u548c\u9759\u6001\u624b\u5f62\u914d\u7f6e\u6765\u89e3\u51b3\u624b\u5f62\u8bc6\u522b\u7684\u6311\u6218\uff0c\u5e76\u5efa\u7acb\u4e86\u7b2c\u4e00\u4e2a\u7ed3\u6784\u5316\u624b\u5f62\u8bc6\u522b\u7684\u57fa\u51c6\u3002", "motivation": "\u76ee\u524d\u5927\u591a\u6570\u8ba1\u7b97\u65b9\u6cd5\u5f88\u5c11\u660e\u786e\u5730\u5bf9\u6a21\u578b\u8fdb\u884c\u624b\u5f62\u5efa\u6a21\uff0c\u8fd9\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u9650\u5236\u4e86\u8bc6\u522b\u7cbe\u5ea6\u548c\u8bed\u8a00\u5b66\u5206\u6790\u7684\u51c6\u786e\u6027\u3002", "method": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\uff0c\u5c06\u89e3\u5256\u5b66\u77e5\u8bc6\u878d\u5165\u56fe\u7ed3\u6784\uff0c\u5e76\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\uff0c\u4ee5\u5e94\u5bf9\u624b\u5f62\u8bc6\u522b\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5305\u62ec\u7ec6\u5fae\u7684\u7c7b\u95f4\u533a\u5206\u548c\u65f6\u95f4\u53d8\u5316\u3002", "result": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u572837\u4e2a\u624b\u5f62\u7c7b\u522b\u4e0a\u5b9e\u73b0\u4e8646%\u7684\u51c6\u786e\u7387\uff0c\u800c\u57fa\u7ebf\u65b9\u6cd5\u7684\u51c6\u786e\u7387\u4e3a25%\uff0c\u5efa\u7acb\u4e86\u7b2c\u4e00\u4e2a\u7ed3\u6784\u5316\u624b\u5f62\u8bc6\u522b\u7684\u57fa\u51c6\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u624b\u5f62\u8bc6\u522b\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6210\u679c\uff0c\u5c55\u793a\u4e86\u5176\u5728\u5904\u7406\u7ec6\u5fae\u7684\u7c7b\u95f4\u533a\u5206\u548c\u65f6\u95f4\u53d8\u5316\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.18530", "categories": ["quant-ph", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18530", "abs": "https://arxiv.org/abs/2509.18530", "authors": ["Hyunho Cha", "Daniel K. Park", "Jungwoo Lee"], "title": "Re-uploading quantum data: A universal function approximator for quantum inputs", "comment": "24 pages, 11 figures", "summary": "Quantum data re-uploading has proved powerful for classical inputs, where\nrepeatedly encoding features into a small circuit yields universal function\napproximation. Extending this idea to quantum inputs remains underexplored, as\nthe information contained in a quantum state is not directly accessible in\nclassical form. We propose and analyze a quantum data re-uploading architecture\nin which a qubit interacts sequentially with fresh copies of an arbitrary input\nstate. The circuit can approximate any bounded continuous function using only\none ancilla qubit and single-qubit measurements. By alternating entangling\nunitaries with mid-circuit resets of the input register, the architecture\nrealizes a discrete cascade of completely positive and trace-preserving maps,\nanalogous to collision models in open quantum system dynamics. Our framework\nprovides a qubit-efficient and expressive approach to designing quantum machine\nlearning models that operate directly on quantum data.", "AI": {"tldr": "\u901a\u8fc7\u91cf\u5b50\u6570\u636e\u91cd\u7f16\u7801\u6280\u672f\uff0c\u53ef\u4ee5\u4ec5\u4f7f\u7528\u4e00\u4e2a\u8f85\u52a9\u91cf\u5b50\u6bd4\u7279\u548c\u5355\u91cf\u5b50\u6bd4\u7279\u6d4b\u91cf\u6765\u8fd1\u4f3c\u4efb\u4f55\u6709\u754c\u8fde\u7eed\u51fd\u6570\uff0c\u4e3a\u5904\u7406\u91cf\u5b50\u6570\u636e\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u5bcc\u6709\u8868\u73b0\u529b\u7684\u65b9\u6cd5\u3002", "motivation": "\u5728\u91cf\u5b50\u6570\u636e\u4e0a\u63a2\u7d22\u91cf\u5b50\u6570\u636e\u91cd\u7f16\u7801\u7684\u6f5c\u529b\uff0c\u56e0\u4e3a\u91cf\u5b50\u6001\u4e2d\u7684\u4fe1\u606f\u4e0d\u6613\u4ee5\u7ecf\u5178\u5f62\u5f0f\u8bbf\u95ee\u3002", "method": "\u63d0\u51fa\u5e76\u5206\u6790\u4e86\u4e00\u79cd\u91cf\u5b50\u6570\u636e\u91cd\u7f16\u7801\u67b6\u6784\uff0c\u5176\u4e2d\u4e00\u4e2a\u91cf\u5b50\u6bd4\u7279\u4e0e\u8f93\u5165\u72b6\u6001\u7684\u65b0\u7684\u526f\u672c\u8fdb\u884c\u987a\u5e8f\u4ea4\u4e92\u3002\u901a\u8fc7\u4ea4\u66ff\u4f7f\u7528\u7ea0\u7f20\u9149\u53d8\u6362\u548c\u8f93\u5165\u5bc4\u5b58\u5668\u7684\u4e2d\u7a0b\u91cd\u7f6e\uff0c\u53ef\u4ee5\u5b9e\u73b0\u5b8c\u5168\u6b63\u8ff9\u4fdd\u6301\u6620\u5c04\u7684\u79bb\u6563\u7ea7\u8054\u3002", "result": "\u8be5\u91cf\u5b50\u6570\u636e\u91cd\u7f16\u7801\u67b6\u6784\u53ef\u4ee5\u8fd1\u4f3c\u4efb\u4f55\u6709\u754c\u8fde\u7eed\u51fd\u6570\uff0c\u5e76\u4e14\u53ea\u9700\u8981\u4e00\u4e2a\u8f85\u52a9\u91cf\u5b50\u6bd4\u7279\u548c\u5355\u91cf\u5b50\u6bd4\u7279\u6d4b\u91cf\u3002", "conclusion": "\u8be5\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u79cd\u91cf\u5b50\u6bd4\u7279\u6548\u7387\u9ad8\u4e14\u5bcc\u6709\u8868\u73b0\u529b\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bbe\u8ba1\u76f4\u63a5\u5728\u91cf\u5b50\u6570\u636e\u4e0a\u64cd\u4f5c\u7684\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u6a21\u578b\u3002"}}
{"id": "2509.18125", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18125", "abs": "https://arxiv.org/abs/2509.18125", "authors": ["Harsha Koduri"], "title": "NurseSchedRL: Attention-Guided Reinforcement Learning for Nurse-Patient Assignment", "comment": null, "summary": "Healthcare systems face increasing pressure to allocate limited nursing\nresources efficiently while accounting for skill heterogeneity, patient acuity,\nstaff fatigue, and continuity of care. Traditional optimization and heuristic\nscheduling methods struggle to capture these dynamic, multi-constraint\nenvironments. I propose NurseSchedRL, a reinforcement learning framework for\nnurse-patient assignment that integrates structured state encoding, constrained\naction masking, and attention-based representations of skills, fatigue, and\ngeographical context. NurseSchedRL uses Proximal Policy Optimization (PPO) with\nfeasibility masks to ensure assignments respect real-world constraints, while\ndynamically adapting to patient arrivals and varying nurse availability. In\nsimulation with realistic nurse and patient data, NurseSchedRL achieves\nimproved scheduling efficiency, better alignment of skills to patient needs,\nand reduced fatigue compared to baseline heuristic and unconstrained RL\napproaches. These results highlight the potential of reinforcement learning for\ndecision support in complex, high-stakes healthcare workforce management.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.18383", "categories": ["cs.AI", "cs.DM", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18383", "abs": "https://arxiv.org/abs/2509.18383", "authors": ["Moran Feldman", "Amin Karbasi"], "title": "G\u00f6del Test: Can Large Language Models Solve Easy Conjectures?", "comment": null, "summary": "Recent announcements from frontier AI model labs have highlighted strong\nresults on high-school and undergraduate math competitions. Yet it remains\nunclear whether large language models can solve new, simple conjectures in more\nadvanced areas of mathematics. We propose the G\\\"odel Test: evaluating whether\na model can produce correct proofs for very simple, previously unsolved\nconjectures. To this end, we study the performance of GPT-5 on five conjectures\nin combinatorial optimization. For each problem, we provided one or two source\npapers from which the conjecture arose, withheld our own conjecture, and then\nassessed the model's reasoning in detail. On the three easier problems, GPT-5\nproduced nearly correct solutions; for Problem 2 it even derived a different\napproximation guarantee that, upon checking, refuted our conjecture while\nproviding a valid solution. The model failed on Problem 4, which required\ncombining results from two papers. On Problem 5, a harder case without a\nvalidated conjecture, GPT-5 proposed the same algorithm we had in mind but\nfailed in the analysis, suggesting the proof is more challenging than expected.\nAlthough our sample is small, the results point to meaningful progress on\nroutine reasoning, occasional flashes of originality, and clear limitations\nwhen cross-paper synthesis is required. GPT-5 may represent an early step\ntoward frontier models eventually passing the G\\\"odel Test.", "AI": {"tldr": "GPT-5\u5728\u7ec4\u5408\u4f18\u5316\u9886\u57df\u7684\u4e94\u4e2a\u731c\u60f3\u4e0a\u8868\u73b0\u51fa\u6709\u610f\u4e49\u7684\u8fdb\u5c55\uff0c\u4f46\u5728\u9700\u8981\u4ea4\u53c9\u8bba\u6587\u7efc\u5408\u7684\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002", "motivation": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u89e3\u51b3\u6570\u5b66\u7ade\u8d5b\u4e4b\u5916\u7684\u6570\u5b66\u95ee\u9898\uff0c\u7279\u522b\u662f\u4e4b\u524d\u672a\u89e3\u51b3\u7684\u7b80\u5355\u731c\u60f3\u65b9\u9762\u7684\u80fd\u529b\u3002", "method": "\u5bf9GPT-5\u5728\u4e94\u4e2a\u7ec4\u5408\u4f18\u5316\u731c\u60f3\u4e0a\u7684\u8868\u73b0\u8fdb\u884c\u7814\u7a76\uff0c\u4e3a\u6a21\u578b\u63d0\u4f9b\u731c\u60f3\u7684\u6765\u6e90\u8bba\u6587\uff0c\u5e76\u8bc4\u4f30\u5176\u63a8\u7406\u8fc7\u7a0b\u3002", "result": "GPT-5\u5728\u4e09\u4e2a\u8f83\u5bb9\u6613\u7684\u95ee\u9898\u4e0a\u7ed9\u51fa\u4e86\u8fd1\u4e4e\u6b63\u786e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5176\u4e2d\u4e00\u4e2a\u95ee\u9898\u751a\u81f3\u5f97\u51fa\u4e86\u4e00\u4e2a\u9519\u8bef\u7684\u731c\u60f3\u4f46\u7ed9\u51fa\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002\u5728\u9700\u8981\u7ed3\u5408\u4e24\u7bc7\u8bba\u6587\u4fe1\u606f\u7684\u95ee\u9898\u4e0a\uff0cGPT-5\u8868\u73b0\u4e0d\u4f73\u3002\u5728\u6700\u540e\u4e00\u4e2a\u66f4\u96be\u7684\u95ee\u9898\u4e0a\uff0cGPT-5\u63d0\u51fa\u4e86\u4e0e\u7814\u7a76\u4eba\u5458\u76f8\u540c\u7684\u7b97\u6cd5\uff0c\u4f46\u5728\u5206\u6790\u4e0a\u5931\u8d25\u4e86\u3002", "conclusion": "GPT-5\u5728\u5e38\u89c4\u63a8\u7406\u65b9\u9762\u53d6\u5f97\u4e86\u6709\u610f\u4e49\u7684\u8fdb\u5c55\uff0c\u5076\u5c14\u8868\u73b0\u51fa\u539f\u521b\u6027\uff0c\u4f46\u5728\u9700\u8981\u4ea4\u53c9\u8bba\u6587\u7efc\u5408\u65f6\u5b58\u5728\u660e\u663e\u9650\u5236\u3002GPT-5\u53ef\u80fd\u662f\u672a\u6765\u80fd\u591f\u901a\u8fc7\u201c\u54e5\u5fb7\u5c14\u6d4b\u8bd5\u201d\u7684\u524d\u6cbf\u6a21\u578b\u7684\u4e00\u4e2a\u65e9\u671f\u4ee3\u8868\u3002"}}
{"id": "2509.18487", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18487", "abs": "https://arxiv.org/abs/2509.18487", "authors": ["Ben Finkelshtein", "Silviu Cucerzan", "Sujay Kumar Jauhar", "Ryen White"], "title": "Actions Speak Louder than Prompts: A Large-Scale Study of LLMs for Graph Inference", "comment": null, "summary": "Large language models (LLMs) are increasingly used for text-rich graph\nmachine learning tasks such as node classification in high-impact domains like\nfraud detection and recommendation systems. Yet, despite a surge of interest,\nthe field lacks a principled understanding of the capabilities of LLMs in their\ninteraction with graph data. In this work, we conduct a large-scale, controlled\nevaluation across several key axes of variability to systematically assess the\nstrengths and weaknesses of LLM-based graph reasoning methods in text-based\napplications. The axes include the LLM-graph interaction mode, comparing\nprompting, tool-use, and code generation; dataset domains, spanning citation,\nweb-link, e-commerce, and social networks; structural regimes contrasting\nhomophilic and heterophilic graphs; feature characteristics involving both\nshort- and long-text node attributes; and model configurations with varying LLM\nsizes and reasoning capabilities. We further analyze dependencies by\nmethodically truncating features, deleting edges, and removing labels to\nquantify reliance on input types. Our findings provide practical and actionable\nguidance. (1) LLMs as code generators achieve the strongest overall performance\non graph data, with especially large gains on long-text or high-degree graphs\nwhere prompting quickly exceeds the token budget. (2) All interaction\nstrategies remain effective on heterophilic graphs, challenging the assumption\nthat LLM-based methods collapse under low homophily. (3) Code generation is\nable to flexibly adapt its reliance between structure, features, or labels to\nleverage the most informative input type. Together, these findings provide a\ncomprehensive view of the strengths and limitations of current LLM-graph\ninteraction modes and highlight key design principles for future approaches.", "AI": {"tldr": "LLM\u5728\u56fe\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u5bf9\u5176\u80fd\u529b\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7406\u89e3\u3002\u672c\u6587\u901a\u8fc7\u5927\u89c4\u6a21\u3001\u53d7\u63a7\u7684\u8bc4\u4f30\uff0c\u63a2\u7d22\u4e86LLM\u4e0e\u56fe\u6570\u636e\u4ea4\u4e92\u7684\u591a\u79cd\u6a21\u5f0f\uff08\u63d0\u793a\u3001\u5de5\u5177\u4f7f\u7528\u3001\u4ee3\u7801\u751f\u6210\uff09\u3001\u6570\u636e\u96c6\u7c7b\u578b\u3001\u7ed3\u6784\u6a21\u5f0f\u3001\u7279\u5f81\u7279\u5f81\u548c\u6a21\u578b\u914d\u7f6e\uff0c\u5e76\u5206\u6790\u4e86\u5176\u5bf9\u8f93\u5165\u7c7b\u578b\u7684\u4f9d\u8d56\u6027\u3002", "motivation": "\u8bc4\u4f30LLM\u5728\u6587\u672c\u56fe\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\uff0c\u7cfb\u7edf\u6027\u5730\u7406\u89e3\u5176\u4f18\u52bf\u548c\u52a3\u52bf\u3002", "method": "\u901a\u8fc7\u5927\u89c4\u6a21\u3001\u53d7\u63a7\u7684\u5b9e\u9a8c\uff0c\u8bc4\u4f30\u4e86LLM\u5728\u56fe\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u591a\u79cd\u4ea4\u4e92\u6a21\u5f0f\uff08\u63d0\u793a\u3001\u5de5\u5177\u4f7f\u7528\u3001\u4ee3\u7801\u751f\u6210\uff09\u3001\u6570\u636e\u96c6\u7c7b\u578b\u3001\u7ed3\u6784\u6a21\u5f0f\u3001\u7279\u5f81\u7279\u5f81\u548c\u6a21\u578b\u914d\u7f6e\uff0c\u5e76\u5206\u6790\u4e86\u5176\u5bf9\u8f93\u5165\u7c7b\u578b\u7684\u4f9d\u8d56\u6027\u3002", "result": "1. LLM\u4f5c\u4e3a\u4ee3\u7801\u751f\u6210\u5668\u5728\u56fe\u6570\u636e\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u5c24\u5176\u662f\u5728\u957f\u6587\u672c\u6216\u9ad8\u5ea6\u56fe\u4e0a\u30022. \u6240\u6709\u4ea4\u4e92\u7b56\u7565\u5728\u5f02\u8d28\u56fe\u4e0a\u5747\u6709\u6548\u30023. \u4ee3\u7801\u751f\u6210\u53ef\u4ee5\u7075\u6d3b\u5730\u5728\u7ed3\u6784\u3001\u7279\u5f81\u6216\u6807\u7b7e\u4e4b\u95f4\u5207\u6362\u4f9d\u8d56\u30024. \u8bc4\u4f30\u4e86LLM\u5bf9\u8f93\u5165\u7c7b\u578b\u7684\u4f9d\u8d56\u6027\u3002", "conclusion": "LLM\u5728\u56fe\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4ee3\u7801\u751f\u6210\u6a21\u5f0f\u8868\u73b0\u6700\u4f73\u3002\u7814\u7a76\u7ed3\u679c\u4e3a\u672a\u6765\u7684LLM-\u56fe\u4ea4\u4e92\u65b9\u6cd5\u63d0\u4f9b\u4e86\u5173\u952e\u7684\u8bbe\u8ba1\u539f\u5219\u3002"}}
{"id": "2509.18870", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2509.18870", "abs": "https://arxiv.org/abs/2509.18870", "authors": ["Nicola Kelly"], "title": "Chemistry and physics of layered oxychalcogenides containing an anti-cuprate type square lattice", "comment": "25 pages, 11 figures. Accepted in Solid State Sciences", "summary": "There has been significant recent interest in layered solid-state materials\ncontaining an [M2O] square lattice layer (M = transition metal), particularly\nbecause [M2O] is the anti-type of the [CuO2] planes in the layered cuprate\nsuperconductors. In addition to the superconducting titanium oxypnictides, the\n[M2O] anti-cuprate layer also occurs in a wide range of layered oxychalcogenide\ncompounds with M spanning early (Ti, V) to later transition metals (Mn, Co,\nFe). The chalcogenide in question - which sandwiches the anti-cuprate layer -\nmay be S, Se or Te, and in combination with a wide range of intervening\n\"spacer\" layers, many different structural families have been investigated.\nThis review surveys the structures and physical properties of all these\noxychalcogenide materials and relates these properties to their common\nanti-cuprate square lattice [M2O] layer. It is organised around the different\noxidation states of the metal ion M, in order to explore the effects of the\nelectronic configuration of M on the physical properties of each compound as a\nwhole. A key part of the review highlights the use of soft-chemical\nmodifications to alter physical properties of these materials, in the synthesis\nof novel van der Waals materials and other metastable compounds. Future avenues\nfor these materials in the bulk, few- and single-layer limits are discussed.", "AI": {"tldr": "\u8be5\u7efc\u8ff0\u63a2\u8ba8\u4e86\u542b\u6709[M2O]\u65b9\u5f62\u6676\u683c\u5c42\u7684\u5c42\u72b6\u56fa\u6001\u6750\u6599\uff0c\u91cd\u70b9\u5173\u6ce8\u4e86\u5b83\u4eec\u4e0e\u94dc\u9178\u76d0\u8d85\u5bfc\u4f53\u4e2d[CuO2]\u5e73\u9762\u7684\u53cd\u578b\u7ed3\u6784\u5173\u7cfb\u3002\u7814\u7a76\u4e86M\u4ece\u65e9\u671f\uff08Ti, V\uff09\u5230\u665a\u671f\uff08Mn, Co, Fe\uff09\u8fc7\u6e21\u91d1\u5c5e\u7684\u5404\u79cd\u6c27\u786b\u5c5e\u5316\u5408\u7269\uff0c\u5e76\u5206\u6790\u4e86\u8fd9\u4e9b\u5316\u5408\u7269\u7684\u7ed3\u6784\u3001\u7269\u7406\u6027\u8d28\u4ee5\u53ca[M2O]\u5c42\u5bf9\u5176\u6027\u8d28\u7684\u5f71\u54cd\u3002\u6587\u7ae0\u8fd8\u8ba8\u8bba\u4e86\u8f6f\u5316\u5b66\u6539\u6027\u5728\u5408\u6210\u65b0\u578b\u8303\u5fb7\u534e\u6750\u6599\u548c\u4e9a\u7a33\u6001\u5316\u5408\u7269\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u5c55\u671b\u4e86\u8fd9\u4e9b\u6750\u6599\u5728\u5757\u4f53\u3001\u5c11\u5c42\u548c\u5355\u5c42\u6781\u9650\u4e0b\u7684\u672a\u6765\u53d1\u5c55\u65b9\u5411\u3002", "motivation": "\u7531\u4e8e[M2O]\u5c42\u662f\u5c42\u72b6\u94dc\u9178\u76d0\u8d85\u5bfc\u4f53\u4e2d[CuO2]\u5e73\u9762\u7684\u53cd\u578b\u7ed3\u6784\uff0c\u56e0\u6b64\u8fd1\u5e74\u6765\u5bf9\u542b\u6709[M2O]\u65b9\u5f62\u6676\u683c\u5c42\u7684\u5c42\u72b6\u56fa\u6001\u6750\u6599\u5f15\u8d77\u4e86\u5e7f\u6cdb\u5173\u6ce8\u3002", "method": "\u8be5\u7efc\u8ff0\u7cfb\u7edf\u5730\u68b3\u7406\u4e86\u6240\u6709\u8fd9\u4e9b\u6c27\u786b\u5c5e\u5316\u5408\u7269\u7684\u7ed3\u6784\u548c\u7269\u7406\u6027\u8d28\uff0c\u5e76\u5c06\u8fd9\u4e9b\u6027\u8d28\u4e0e\u5176\u5171\u6709\u7684\u53cd\u578b\u94dc\u9178\u76d0\u65b9\u5f62\u6676\u683c[M2O]\u5c42\u8054\u7cfb\u8d77\u6765\u3002\u6587\u7ae0\u56f4\u7ed5\u91d1\u5c5e\u79bb\u5b50M\u7684\u4e0d\u540c\u6c27\u5316\u6001\u8fdb\u884c\u7ec4\u7ec7\uff0c\u4ee5\u63a2\u7d22M\u7684\u7535\u5b50\u6784\u578b\u5bf9\u6574\u4e2a\u5316\u5408\u7269\u7269\u7406\u6027\u8d28\u7684\u5f71\u54cd\u3002", "result": "\u603b\u7ed3\u4e86\u5177\u6709[M2O]\u53cd\u94dc\u9178\u76d0\u5c42\u7684\u5c42\u72b6\u6c27\u786b\u5c5e\u5316\u5408\u7269\u7684\u7ed3\u6784\u548c\u7269\u7406\u6027\u8d28\uff0c\u5e76\u5f3a\u8c03\u4e86\u8f6f\u5316\u5b66\u6539\u6027\u5728\u8c03\u8282\u8fd9\u4e9b\u6750\u6599\u6027\u8d28\u548c\u5408\u6210\u65b0\u578b\u8303\u5fb7\u534e\u6750\u6599\u53ca\u5176\u4ed6\u4e9a\u7a33\u6001\u5316\u5408\u7269\u4e2d\u7684\u4f5c\u7528\u3002", "conclusion": "\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u5305\u62ec\u5728\u5757\u4f53\u3001\u5c11\u5c42\u548c\u5355\u5c42\u6781\u9650\u4e0b\u63a2\u7d22\u8fd9\u4e9b\u6750\u6599\u7684\u53d1\u5c55\u6f5c\u529b\u3002"}}
{"id": "2509.18610", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18610", "abs": "https://arxiv.org/abs/2509.18610", "authors": ["Maximilian Adang", "JunEn Low", "Ola Shorinwa", "Mac Schwager"], "title": "SINGER: An Onboard Generalist Vision-Language Navigation Policy for Drones", "comment": null, "summary": "Large vision-language models have driven remarkable progress in\nopen-vocabulary robot policies, e.g., generalist robot manipulation policies,\nthat enable robots to complete complex tasks specified in natural language.\nDespite these successes, open-vocabulary autonomous drone navigation remains an\nunsolved challenge due to the scarcity of large-scale demonstrations, real-time\ncontrol demands of drones for stabilization, and lack of reliable external pose\nestimation modules. In this work, we present SINGER for language-guided\nautonomous drone navigation in the open world using only onboard sensing and\ncompute. To train robust, open-vocabulary navigation policies, SINGER leverages\nthree central components: (i) a photorealistic language-embedded flight\nsimulator with minimal sim-to-real gap using Gaussian Splatting for efficient\ndata generation, (ii) an RRT-inspired multi-trajectory generation expert for\ncollision-free navigation demonstrations, and these are used to train (iii) a\nlightweight end-to-end visuomotor policy for real-time closed-loop control.\nThrough extensive hardware flight experiments, we demonstrate superior\nzero-shot sim-to-real transfer of our policy to unseen environments and unseen\nlanguage-conditioned goal objects. When trained on ~700k-1M observation action\npairs of language conditioned visuomotor data and deployed on hardware, SINGER\noutperforms a velocity-controlled semantic guidance baseline by reaching the\nquery 23.33% more on average, and maintains the query in the field of view\n16.67% more on average, with 10% fewer collisions.", "AI": {"tldr": "SINGER\u662f\u4e00\u4e2a\u8bed\u8a00\u5f15\u5bfc\u7684\u81ea\u4e3b\u65e0\u4eba\u673a\u5bfc\u822a\u7cfb\u7edf\uff0c\u5229\u7528\u6a21\u62df\u5668\u3001\u4e13\u5bb6\u6f14\u793a\u548c\u8f7b\u91cf\u7ea7\u7b56\u7565\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u5b9e\u73b0\u4e86\u96f6\u6837\u672c\u6a21\u62df\u5230\u771f\u5b9e\u4e16\u754c\u7684\u8fc1\u79fb\uff0c\u5e76\u5728\u5230\u8fbe\u7387\u3001\u89c6\u91ce\u4fdd\u6301\u548c\u78b0\u649e\u51cf\u5c11\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u578b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u65b9\u9762\u53d6\u5f97\u4e86\u6210\u529f\uff0c\u4f46\u5728\u5f00\u653e\u8bcd\u6c47\u7684\u81ea\u4e3b\u65e0\u4eba\u673a\u5bfc\u822a\u65b9\u9762\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u539f\u56e0\u662f\u7f3a\u4e4f\u5927\u89c4\u6a21\u6f14\u793a\u3001\u5b9e\u65f6\u63a7\u5236\u9700\u6c42\u4ee5\u53ca\u5bf9\u5916\u90e8\u59ff\u6001\u4f30\u8ba1\u6a21\u5757\u7684\u4f9d\u8d56\u3002\u672c\u9879\u76ee\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "SINGER\u7cfb\u7edf\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\uff081\uff09\u4e00\u4e2a\u5229\u7528\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u751f\u6210\u903c\u771f\u4e14\u5d4c\u5165\u8bed\u8a00\u7684\u98de\u884c\u6a21\u62df\u6570\u636e\uff0c\u4ee5\u6700\u5c0f\u5316\u6a21\u62df\u5230\u771f\u5b9e\u7684\u5dee\u8ddd\uff1b\uff082\uff09\u4e00\u4e2a\u53d7RRT\u542f\u53d1\u7684\u3001\u7528\u4e8e\u751f\u6210\u65e0\u78b0\u649e\u5bfc\u822a\u6f14\u793a\u7684\u591a\u8f68\u8ff9\u751f\u6210\u4e13\u5bb6\uff1b\uff083\uff09\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u7aef\u5230\u7aef\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\uff0c\u7528\u4e8e\u5b9e\u65f6\u95ed\u73af\u63a7\u5236\u3002", "result": "\u901a\u8fc7\u5927\u91cf\u7684\u786c\u4ef6\u98de\u884c\u5b9e\u9a8c\uff0cSINGER\u5728\u96f6\u6837\u672c\u6a21\u62df\u5230\u771f\u5b9e\u4e16\u754c\u7684\u8fc1\u79fb\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u6210\u529f\u5bfc\u822a\u5230\u672a\u66fe\u89c1\u8fc7\u7684\u73af\u5883\u548c\u8bed\u8a00\u6307\u5b9a\u7684\u76ee\u6807\u3002\u4e0e\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0cSINGER\u7684\u5230\u8fbe\u7387\u63d0\u9ad8\u4e8623.33%\uff0c\u89c6\u91ce\u4fdd\u6301\u7387\u63d0\u9ad8\u4e8616.67%\uff0c\u78b0\u649e\u6b21\u6570\u51cf\u5c11\u4e8610%\u3002", "conclusion": "SINGER\u901a\u8fc7\u7ed3\u5408\u6a21\u62df\u5668\u3001\u4e13\u5bb6\u6f14\u793a\u548c\u8f7b\u91cf\u7ea7\u7b56\u7565\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u8bed\u8a00\u5f15\u5bfc\u7684\u81ea\u4e3b\u65e0\u4eba\u673a\u5bfc\u822a\uff0c\u5e76\u5728\u771f\u5b9e\u4e16\u754c\u7684\u98de\u884c\u5b9e\u9a8c\u4e2d\u8bc1\u660e\u4e86\u5176\u4f18\u8d8a\u6027\uff0c\u89e3\u51b3\u4e86\u5f00\u653e\u8bcd\u6c47\u65e0\u4eba\u673a\u5bfc\u822a\u7684\u6311\u6218\u3002"}}
{"id": "2509.18326", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18326", "abs": "https://arxiv.org/abs/2509.18326", "authors": ["Chun Kit Wong", "Anders N. Christensen", "Cosmin I. Bercea", "Julia A. Schnabel", "Martin G. Tolsgaard", "Aasa Feragen"], "title": "Influence of Classification Task and Distribution Shift Type on OOD Detection in Fetal Ultrasound", "comment": "MICCAI 2025", "summary": "Reliable out-of-distribution (OOD) detection is important for safe deployment\nof deep learning models in fetal ultrasound amidst heterogeneous image\ncharacteristics and clinical settings. OOD detection relies on estimating a\nclassification model's uncertainty, which should increase for OOD samples.\nWhile existing research has largely focused on uncertainty quantification\nmethods, this work investigates the impact of the classification task itself.\nThrough experiments with eight uncertainty quantification methods across four\nclassification tasks, we demonstrate that OOD detection performance\nsignificantly varies with the task, and that the best task depends on the\ndefined ID-OOD criteria; specifically, whether the OOD sample is due to: i) an\nimage characteristic shift or ii) an anatomical feature shift. Furthermore, we\nreveal that superior OOD detection does not guarantee optimal abstained\nprediction, underscoring the necessity to align task selection and uncertainty\nstrategies with the specific downstream application in medical image analysis.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u5b9e\u9a8c\u53d1\u73b0\uff0c\u5728\u80ce\u513f\u8d85\u58f0\u56fe\u50cf\u7684 OOD \u68c0\u6d4b\u4efb\u52a1\u4e2d\uff0c\u5206\u7c7b\u4efb\u52a1\u7684\u9009\u62e9\u5bf9\u6a21\u578b\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u548c OOD \u68c0\u6d4b\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\uff0c\u5e76\u4e14\u6700\u4f73\u4efb\u52a1\u9009\u62e9\u53d6\u51b3\u4e8e OOD \u6837\u672c\u7684\u5177\u4f53\u539f\u56e0\uff08\u56fe\u50cf\u7279\u5f81\u6216\u89e3\u5256\u7279\u5f81\u504f\u79fb\uff09\u3002\u7814\u7a76\u8fd8\u5f3a\u8c03\uff0c\u4f18\u79c0\u7684 OOD \u68c0\u6d4b\u80fd\u529b\u5e76\u4e0d\u76f4\u63a5\u7b49\u540c\u4e8e\u6700\u4f18\u7684\u5f03\u6743\u9884\u6d4b\uff0c\u56e0\u6b64\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u9700\u8981\u6839\u636e\u5177\u4f53\u9700\u6c42\u6765\u9009\u62e9\u5408\u9002\u7684\u5206\u7c7b\u4efb\u52a1\u548c\u4e0d\u786e\u5b9a\u6027\u7b56\u7565\u3002", "motivation": "\u5728\u80ce\u513f\u8d85\u58f0\u56fe\u50cf\u5206\u6790\u4e2d\uff0c\u53ef\u9760\u7684 OOD \u68c0\u6d4b\u5bf9\u4e8e\u6a21\u578b\u7684\u5b89\u5168\u90e8\u7f72\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u4e3a\u8be5\u9886\u57df\u9762\u4e34\u56fe\u50cf\u5f02\u8d28\u6027\u548c\u4e0d\u540c\u4e34\u5e8a\u73af\u5883\u7684\u6311\u6218\u3002OOD \u68c0\u6d4b\u4f9d\u8d56\u4e8e\u5bf9\u5206\u7c7b\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u7684\u4f30\u8ba1\uff0c\u800c\u6a21\u578b\u5e94\u5728\u9762\u5bf9 OOD \u6837\u672c\u65f6\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u672c\u7814\u7a76\u901a\u8fc7\u5728\u56db\u79cd\u4e0d\u540c\u7684\u5206\u7c7b\u4efb\u52a1\u4e0a\u5e94\u7528\u516b\u79cd\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\uff0c\u6765\u5b9e\u9a8c\u6027\u5730\u63a2\u7a76\u5206\u7c7b\u4efb\u52a1\u672c\u8eab\u5bf9 OOD \u68c0\u6d4b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cOOD \u68c0\u6d4b\u7684\u6027\u80fd\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u53d6\u51b3\u4e8e\u6240\u9009\u62e9\u7684\u5206\u7c7b\u4efb\u52a1\uff0c\u5e76\u4e14\u6700\u4f73\u4efb\u52a1\u7684\u9009\u62e9\u4e0e ID-OOD \u7684\u5b9a\u4e49\u6807\u51c6\uff08\u5373 OOD \u6837\u672c\u662f\u7531\u4e8e\u56fe\u50cf\u7279\u5f81\u504f\u79fb\u8fd8\u662f\u89e3\u5256\u7279\u5f81\u504f\u79fb\uff09\u76f8\u5173\u3002\u6b64\u5916\uff0c\u7814\u7a76\u53d1\u73b0\uff0c\u66f4\u597d\u7684 OOD \u68c0\u6d4b\u6027\u80fd\u5e76\u4e0d\u4e00\u5b9a\u80fd\u4fdd\u8bc1\u6700\u4f18\u7684\u5f03\u6743\u9884\u6d4b\u3002", "conclusion": "\u5206\u7c7b\u4efb\u52a1\u7684\u9009\u62e9\u5bf9\u80ce\u513f\u8d85\u58f0 OOD \u68c0\u6d4b\u7684\u6027\u80fd\u6709\u91cd\u8981\u5f71\u54cd\uff0c\u5e76\u4e14\u6700\u4f73\u4efb\u52a1\u7684\u9009\u62e9\u53d6\u51b3\u4e8e OOD \u6837\u672c\u7684\u7c7b\u578b\u3002\u6b64\u5916\uff0cOOD \u68c0\u6d4b\u6027\u80fd\u4e0e\u5f03\u6743\u9884\u6d4b\u6027\u80fd\u4e4b\u95f4\u53ef\u80fd\u5b58\u5728\u4e0d\u5339\u914d\uff0c\u56e0\u6b64\u5728\u533b\u5b66\u56fe\u50cf\u5206\u6790\u7b49\u4e0b\u6e38\u5e94\u7528\u4e2d\uff0c\u9700\u8981\u6839\u636e\u5177\u4f53\u9700\u6c42\u4ed4\u7ec6\u9009\u62e9\u5206\u7c7b\u4efb\u52a1\u548c\u4e0d\u786e\u5b9a\u6027\u7b56\u7565\uff0c\u4ee5\u5b9e\u73b0\u6700\u4f73\u7684\u6574\u4f53\u6a21\u578b\u8868\u73b0\u3002"}}
{"id": "2509.18533", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2509.18533", "abs": "https://arxiv.org/abs/2509.18533", "authors": ["Luis Arag\u00f3n-Mu\u00f1oz", "Chryssomalis Chryssomalakos", "Ana Gabriela Flores-Delgado", "John Martin", "Eduardo Serrano-Ens\u00e1stiga"], "title": "Universality in fidelity-based quantum metrology", "comment": "17 pages, 9 figures", "summary": "We consider the problem of identifying the quantum spin states that are the\noptimal sensors of a given transformation averaged over all possible\norientations of the spin system. Our geometric approach to the problem is based\non a fidelity criterion and is entirely general, encompassing both unitary\ntransformations (such as rotations and squeezing) and non-unitary\ntransformations (such as Lorentz boosts). This formalism leads to a\nuniversality result: There exists a zero-measure subset of states that will be\noptimal sensors for certain transformations and the worst sensors for others,\nand this set does not depend on the transformation under consideration. In\nother words, some spin states are simply the best (or worst) sensors,\nregardless of what they detect.", "AI": {"tldr": "\u67d0\u4e9b\u91cf\u5b50\u81ea\u65cb\u6001\u5728\u4f5c\u4e3a\u4f20\u611f\u5668\u65f6\u8868\u73b0\u51fa\u666e\u9002\u7684\u6700\u4f18\u6216\u6700\u5dee\u6027\u80fd\uff0c\u4e0d\u4f9d\u8d56\u4e8e\u6240\u63a2\u6d4b\u7684\u5177\u4f53\u53d8\u6362\u3002", "motivation": "\u786e\u5b9a\u54ea\u4e9b\u91cf\u5b50\u81ea\u65cb\u6001\u80fd\u591f\u6700\u4f18\u5730\u4f5c\u4e3a\u7ed9\u5b9a\u53d8\u6362\u7684\u4f20\u611f\u5668\uff0c\u5e76\u8003\u8651\u6240\u6709\u53ef\u80fd\u7684\u81ea\u65cb\u7cfb\u7edf\u5b9a\u5411\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u4fdd\u771f\u5ea6\u5224\u636e\u7684\u51e0\u4f55\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u5e7a\u6b63\u53d8\u6362\uff08\u5982\u65cb\u8f6c\u548c\u538b\u7f29\uff09\u548c\u975e\u5e7a\u6b63\u53d8\u6362\uff08\u5982\u6d1b\u4f26\u5179\u53d8\u6362\uff09\u3002", "result": "\u53d1\u73b0\u5b58\u5728\u4e00\u4e2a\u96f6\u6d4b\u5ea6\u5b50\u96c6\u7684\u72b6\u6001\uff0c\u8fd9\u4e9b\u72b6\u6001\u5bf9\u4e8e\u67d0\u4e9b\u53d8\u6362\u662f\u6700\u4f73\u4f20\u611f\u5668\uff0c\u800c\u5bf9\u4e8e\u5176\u4ed6\u53d8\u6362\u5219\u662f\u6700\u5dee\u4f20\u611f\u5668\uff0c\u5e76\u4e14\u8fd9\u4e2a\u96c6\u5408\u4e0e\u53d8\u6362\u672c\u8eab\u65e0\u5173\u3002", "conclusion": "\u5b58\u5728\u4e00\u4e9b\u81ea\u65cb\u6001\uff0c\u65e0\u8bba\u5b83\u4eec\u7528\u4e8e\u68c0\u6d4b\u4ec0\u4e48\u53d8\u6362\uff0c\u59cb\u7ec8\u662f\u6700\u4f73\u6216\u6700\u5dee\u7684\u4f20\u611f\u5668\u3002"}}
{"id": "2509.18126", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18126", "abs": "https://arxiv.org/abs/2509.18126", "authors": ["Bishal K C", "Amr Hilal", "Pawan Thapa"], "title": "Anomaly Detection in Electric Vehicle Charging Stations Using Federated Learning", "comment": null, "summary": "Federated Learning (FL) is a decentralized training framework widely used in\nIoT ecosystems that preserves privacy by keeping raw data local, making it\nideal for IoT-enabled cyber-physical systems with sensing and communication\nlike Smart Grids (SGs), Connected and Automated Vehicles (CAV), and Electric\nVehicle Charging Stations (EVCS). With the rapid expansion of electric vehicle\ninfrastructure, securing these IoT-based charging stations against cyber\nthreats has become critical. Centralized Intrusion Detection Systems (IDS)\nraise privacy concerns due to sensitive network and user data, making FL a\npromising alternative. However, current FL-based IDS evaluations overlook\npractical challenges such as system heterogeneity and non-IID data. To address\nthese challenges, we conducted experiments to evaluate the performance of\nfederated learning for anomaly detection in EV charging stations under system\nand data heterogeneity. We used FedAvg and FedAvgM, widely studied optimization\napproaches, to analyze their effectiveness in anomaly detection. Under IID\nsettings, FedAvg achieves superior performance to centralized models using the\nsame neural network. However, performance degrades with non-IID data and system\nheterogeneity. FedAvgM consistently outperforms FedAvg in heterogeneous\nsettings, showing better convergence and higher anomaly detection accuracy. Our\nresults demonstrate that FL can handle heterogeneity in IoT-based EVCS without\nsignificant performance loss, with FedAvgM as a promising solution for robust,\nprivacy-preserving EVCS security.", "AI": {"tldr": "\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u5728\u7535\u52a8\u6c7d\u8f66\u5145\u7535\u7ad9\uff08EVCS\uff09\u7684\u7269\u8054\u7f51\uff08IoT\uff09\u73af\u5883\u4e2d\u901a\u8fc7FedAvgM\u5728\u7cfb\u7edf\u548c\u6570\u636e\u5f02\u6784\u4e0b\u5b9e\u73b0\u9ad8\u6548\u3001\u6ce8\u91cd\u9690\u79c1\u7684\u5f02\u5e38\u68c0\u6d4b\u3002", "motivation": "\u968f\u7740\u7535\u52a8\u6c7d\u8f66\u57fa\u7840\u8bbe\u65bd\u7684\u5feb\u901f\u6269\u5f20\uff0c\u4fdd\u62a4\u8fd9\u4e9b\u57fa\u4e8e\u7269\u8054\u7f51\u7684\u5145\u7535\u7ad9\u514d\u53d7\u7f51\u7edc\u5a01\u80c1\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u7684\u96c6\u4e2d\u5f0f\u5165\u4fb5\u68c0\u6d4b\u7cfb\u7edf\uff08IDS\uff09\u56e0\u6d89\u53ca\u654f\u611f\u6570\u636e\u800c\u5f15\u53d1\u9690\u79c1\u62c5\u5fe7\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66ff\u4ee3\u65b9\u6848\uff0c\u5982\u8054\u90a6\u5b66\u4e60\u3002", "method": "\u8bc4\u4f30\u4e86FedAvg\u548cFedAvgM\u5728EVCS\u73af\u5883\u4e2d\u7684\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd\uff0c\u8003\u8651\u4e86\u7cfb\u7edf\u548c\u6570\u636e\u5f02\u6784\u6027\u3002", "result": "\u5728IID\u8bbe\u7f6e\u4e0b\uff0cFedAvg\u4f18\u4e8e\u96c6\u4e2d\u5f0f\u6a21\u578b\uff1b\u7136\u800c\uff0c\u5728\u975eIID\u6570\u636e\u548c\u7cfb\u7edf\u5f02\u6784\u4e0b\uff0c\u5176\u6027\u80fd\u4f1a\u4e0b\u964d\u3002FedAvgM\u5728\u5f02\u6784\u8bbe\u7f6e\u4e0b\u59cb\u7ec8\u4f18\u4e8eFedAvg\uff0c\u8868\u73b0\u51fa\u66f4\u597d\u7684\u6536\u655b\u6027\u548c\u66f4\u9ad8\u7684\u5f02\u5e38\u68c0\u6d4b\u51c6\u786e\u6027\u3002", "conclusion": "\u8054\u90a6\u5b66\u4e60\u53ef\u4ee5\u5904\u7406\u7269\u8054\u7f51EVCS\u4e2d\u7684\u5f02\u6784\u6027\uff0c\u800c\u4e0d\u4f1a\u9020\u6210\u663e\u8457\u7684\u6027\u80fd\u635f\u5931\uff0c\u5176\u4e2dFedAvgM\u662f\u5b9e\u73b0\u9c81\u68d2\u3001\u6ce8\u91cd\u9690\u79c1\u7684EVCS\u5b89\u5168\u7684\u4e00\u4e2a\u6709\u524d\u9014\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.18400", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18400", "abs": "https://arxiv.org/abs/2509.18400", "authors": ["Pritish Yuvraj", "Siva Devarakonda"], "title": "ATLAS: Benchmarking and Adapting LLMs for Global Trade via Harmonized Tariff Code Classification", "comment": null, "summary": "Accurate classification of products under the Harmonized Tariff Schedule\n(HTS) is a critical bottleneck in global trade, yet it has received little\nattention from the machine learning community. Misclassification can halt\nshipments entirely, with major postal operators suspending deliveries to the\nU.S. due to incomplete customs documentation. We introduce the first benchmark\nfor HTS code classification, derived from the U.S. Customs Rulings Online\nSearch System (CROSS). Evaluating leading LLMs, we find that our fine-tuned\nAtlas model (LLaMA-3.3-70B) achieves 40 percent fully correct 10-digit\nclassifications and 57.5 percent correct 6-digit classifications, improvements\nof 15 points over GPT-5-Thinking and 27.5 points over Gemini-2.5-Pro-Thinking.\nBeyond accuracy, Atlas is roughly five times cheaper than GPT-5-Thinking and\neight times cheaper than Gemini-2.5-Pro-Thinking, and can be self-hosted to\nguarantee data privacy in high-stakes trade and compliance workflows. While\nAtlas sets a strong baseline, the benchmark remains highly challenging, with\nonly 40 percent 10-digit accuracy. By releasing both dataset and model, we aim\nto position HTS classification as a new community benchmark task and invite\nfuture work in retrieval, reasoning, and alignment.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9996\u6b21\u63d0\u51fa\u4e86\u6d77\u5173\u7f16\u7801\u5206\u7c7b\u57fa\u51c6\uff0c\u5e76\u5f15\u5165\u4e86\u7ecf\u8fc7\u5fae\u8c03\u7684 Atlas \u6a21\u578b\uff08LLaMA-3.3-70B\uff09\uff0c\u572810\u4f4d\u548c6\u4f4d\u5206\u7c7b\u4e0a\u5747\u53d6\u5f97\u4e86\u663e\u8457\u7684\u51c6\u786e\u7387\u63d0\u5347\uff0c\u540c\u65f6\u6210\u672c\u66f4\u4f4e\u4e14\u652f\u6301\u81ea\u6258\u7ba1\u4ee5\u4fdd\u62a4\u6570\u636e\u9690\u79c1\u3002\u7814\u7a76\u65e8\u5728\u63a8\u52a8\u6d77\u5173\u7f16\u7801\u5206\u7c7b\u6210\u4e3a\u4e00\u4e2a\u65b0\u7684\u793e\u533a\u57fa\u51c6\u4efb\u52a1\u3002", "motivation": "\u51c6\u786e\u5bf9\u5546\u54c1\u8fdb\u884c\u6d77\u5173\u7f16\u7801\uff08HTS\uff09\u5206\u7c7b\u5bf9\u4e8e\u5168\u7403\u8d38\u6613\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6b64\u524d\u673a\u5668\u5b66\u4e60\u9886\u57df\u5bf9\u6b64\u5173\u6ce8\u4e0d\u8db3\uff0c\u9519\u8bef\u5206\u7c7b\u4f1a\u5bfc\u81f4\u8d27\u7269\u8fd0\u8f93\u4e2d\u65ad\u3002", "method": "\u521b\u5efa\u4e86\u9996\u4e2a\u6e90\u81ea\u7f8e\u56fd\u6d77\u5173\u88c1\u5b9a\u5728\u7ebf\u641c\u7d22\u7cfb\u7edf\uff08CROSS\uff09\u7684\u6d77\u5173\u7f16\u7801\uff08HTS\uff09\u5206\u7c7b\u57fa\u51c6\uff0c\u5e76\u8bc4\u4f30\u4e86\u5305\u62ec Atlas\uff08LLaMA-3.3-70B\uff09\u3001GPT-5-Thinking \u548c Gemini-2.5-Pro-Thinking \u5728\u5185\u7684\u9886\u5148\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u7ecf\u8fc7\u5fae\u8c03\u7684 Atlas \u6a21\u578b\u572810\u4f4d\u5206\u7c7b\u4e0a\u8fbe\u5230\u4e8640%\u7684\u51c6\u786e\u7387\uff0c6\u4f4d\u5206\u7c7b\u4e0a\u8fbe\u5230\u4e8657.5%\u7684\u51c6\u786e\u7387\uff0c\u5206\u522b\u6bd4 GPT-5-Thinking \u9ad815\u4e2a\u767e\u5206\u70b9\uff0c\u6bd4 Gemini-2.5-Pro-Thinking \u9ad827.5\u4e2a\u767e\u5206\u70b9\u3002\u6b64\u5916\uff0cAtlas \u7684\u6210\u672c\u6bd4 GPT-5-Thinking \u4f4e\u7ea6\u4e94\u500d\uff0c\u6bd4 Gemini-2.5-Pro-Thinking \u4f4e\u7ea6\u516b\u500d\u3002", "conclusion": "\u5c3d\u7ba1 Atlas \u6a21\u578b\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u6d77\u5173\u7f16\u7801\u5206\u7c7b\u4efb\u52a1\u4ecd\u7136\u5145\u6ee1\u6311\u6218\uff0c10\u4f4d\u5206\u7c7b\u51c6\u786e\u7387\u4ec5\u4e3a40%\u3002\u901a\u8fc7\u53d1\u5e03\u6570\u636e\u96c6\u548c\u6a21\u578b\uff0c\u7814\u7a76\u65e8\u5728\u5c06\u6d77\u5173\u7f16\u7801\u5206\u7c7b\u786e\u7acb\u4e3a\u4e00\u4e2a\u65b0\u7684\u793e\u533a\u57fa\u51c6\u4efb\u52a1\uff0c\u5e76\u9f13\u52b1\u5728\u68c0\u7d22\u3001\u63a8\u7406\u548c\u5bf9\u9f50\u65b9\u9762\u8fdb\u884c\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2509.18514", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18514", "abs": "https://arxiv.org/abs/2509.18514", "authors": ["Mohamad Elzohbi", "Richard Zhao"], "title": "A Rhythm-Aware Phrase Insertion for Classical Arabic Poetry Composition", "comment": "Accepted for the Third Arabic Natural Language Processing Conference\n  (ArabicNLP 2025)", "summary": "This paper presents a methodology for inserting phrases in Arabic poems to\nconform to a specific rhythm using ByT5, a byte-level multilingual\ntransformer-based model. Our work discusses a rule-based grapheme-to-beat\ntransformation tailored for extracting the rhythm from fully diacritized Arabic\nscript. Our approach employs a conditional denoising objective to fine-tune\nByT5, where the model reconstructs masked words to match a target rhythm. We\nadopt a curriculum learning strategy, pre-training on a general Arabic dataset\nbefore fine-tuning on poetic dataset, and explore cross-lingual transfer from\nEnglish to Arabic. Experimental results demonstrate that our models achieve\nhigh rhythmic alignment while maintaining semantic coherence. The proposed\nmodel has the potential to be used in co-creative applications in the process\nof composing classical Arabic poems.", "AI": {"tldr": "We present a method using ByT5 to insert Arabic poem phrases for specific rhythm, employing a rule-based rhythm extraction, conditional denoising, and curriculum learning. The model shows high rhythmic alignment and semantic coherence, with potential for co-creative poetry composition.", "motivation": "The motivation is to develop a method for inserting phrases into Arabic poems to ensure they conform to a specific rhythm, aiding in the composition of classical Arabic poetry.", "method": "The methodology involves using ByT5, a byte-level multilingual transformer, and a rule-based grapheme-to-beat transformation to extract rhythm from Arabic script. The ByT5 model is fine-tuned using a conditional denoising objective where it reconstructs masked words to match a target rhythm. A curriculum learning strategy is used, involving pre-training on a general Arabic dataset and then fine-tuning on a poetic dataset. Cross-lingual transfer from English to Arabic is also explored.", "result": "Experimental results show that the proposed models achieve high rhythmic alignment with the target rhythm while preserving the semantic coherence of the poem phrases.", "conclusion": "The proposed model demonstrates the capability to generate Arabic poem phrases that adhere to a specific rhythm and maintain semantic coherence, indicating its potential utility in co-creative applications for composing classical Arabic poetry."}}
{"id": "2509.18895", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2509.18895", "abs": "https://arxiv.org/abs/2509.18895", "authors": ["Nabil Daghbouj", "Ahmed. T. AlMotasem", "Jan. Ducho\u0148b", "Bingsheng. Li", "Mohamed. Bensalem", "Frans. Munnik", "Xin Ou", "Anna. Mackov\u00e1", "William. J. Weber", "Tomas. Polcara"], "title": "Nanoscale Strain Evolution and Grain Boundary-Mediated Defect Sink Behavior in Irradiated SiC: Insights from N-PED and DFT", "comment": "12 figurs, 38 pages", "summary": "Understanding irradiation-induced strain in silicon carbide (SiC) is\nessential for designing radiation-tolerant ceramic materials. However,\nconventional methods often fail to resolve nanoscale strain gradients,\nespecially in polycrystalline forms. In this study, we employ nano-beam\nprecession electron diffraction (N-PED) to perform high-resolution,\nmulti-directional strain mapping in both single-crystal 4H-SiC and\npolycrystalline {\\alpha}-SiC subjected to helium and hydrogen ion irradiation.\nThe high-resolution X-ray diffraction (HR-XRD) simulations of He + H irradiated\nsingle-crystal 4H-SiC closely match the strain profiles obtained from N-PED,\ndemonstrating the reliability and accuracy of the N-PED method. In\nHe-irradiated polycrystalline {\\alpha}-SiC at high temperatures, a\nbubble-depleted zone (BDZ) near the grain boundary (GB) reveals that GBs act as\nactive sinks for irradiation-induced defects. N-PED further shows strain\namplification localized at the GBs, reaching up to 2.5%, along with strain\nrelief within the BDZ. To explain this behavior, density functional theory\n(DFT) calculations of binding and migration energies indicate a strong tendency\nfor Si, C, and He atoms to segregate toward the GB core. This segregation\nreduces the availability of vacancies to accommodate He atoms and leads to\nlocal strain relaxation near the GB. Furthermore, first-principles tensile\nsimulations reveal that Si and C interstitials mitigate He-induced GB\nembrittlement. Charge density and DOS analyses link this effect to the bonding\ncharacteristics between point defects and neighboring atoms at GB. These\ninsights underscore the importance of grain boundary engineering in enhancing\nradiation tolerance of SiC for nuclear and space applications.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u7eb3\u7c73\u675f\u7cbe\u5bc6\u7535\u5b50\u884d\u5c04\uff08N-PED\uff09\u6280\u672f\uff0c\u7ed3\u5408\u9ad8\u5206\u8fa8\u7387X\u5c04\u7ebf\u884d\u5c04\uff08HR-XRD\uff09\u548c\u5bc6\u5ea6\u6cdb\u51fd\u7406\u8bba\uff08DFT\uff09\u8ba1\u7b97\uff0c\u5728\u9ad8\u7cbe\u5ea6\u5730\u8868\u5f81\u4e86\u8f90\u7167\u8bf1\u5bfc\u7684\u78b3\u5316\u7845\uff08SiC\uff09\u6750\u6599\u4e2d\u7684\u5e94\u53d8\u68af\u5ea6\uff0c\u7279\u522b\u662f\u5728\u591a\u6676SiC\u7684\u6676\u754c\uff08GB\uff09\u9644\u8fd1\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u6676\u754c\u5728\u8f90\u7167\u8fc7\u7a0b\u4e2d\u626e\u6f14\u7740\u7f3a\u9677\u6c47\u7684\u89d2\u8272\uff0c\u8868\u73b0\u51fa\u5e94\u53d8\u653e\u5927\u6548\u5e94\uff0c\u5e76\u4e14\u901a\u8fc7\u539f\u5b50 \u0924\u093f\u0925\u0947 segregation \u548c\u70b9\u7f3a\u9677\u7684\u7ed3\u5408\u673a\u5236\uff0c\u80fd\u591f\u7f13\u89e3\u6c26\uff08He\uff09\u8bf1\u5bfc\u7684\u6676\u754c\u8106\u5316\uff0c\u4e3a\u63d0\u5347SiC\u6750\u6599\u7684\u6297\u8f90\u7167\u6027\u80fd\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\u3002", "motivation": "\u8f90\u7167\u8bf1\u5bfc\u7684\u5e94\u53d8\u662f\u5f71\u54cdSiC\u6750\u6599\u6297\u8f90\u7167\u6027\u80fd\u7684\u5173\u952e\u56e0\u7d20\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u7cbe\u786e\u8868\u5f81\u591a\u6676SiC\u4e2d\u7684\u7eb3\u7c73\u5c3a\u5ea6\u5e94\u53d8\u68af\u5ea6\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u65b0\u7684\u8868\u5f81\u6280\u672f\u6765\u7406\u89e3\u8f90\u7167\u635f\u4f24\u673a\u5236\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528\u4e86\u7eb3\u7c73\u675f\u7cbe\u5bc6\u7535\u5b50\u884d\u5c04\uff08N-PED\uff09\u6280\u672f\uff0c\u5bf9\u5355\u6676\u548c\u591a\u6676SiC\u8fdb\u884c\u591a\u65b9\u5411\u5e94\u53d8\u6d4b\u91cf\uff0c\u5e76\u7ed3\u5408\u9ad8\u5206\u8fa8\u7387X\u5c04\u7ebf\u884d\u5c04\uff08HR-XRD\uff09\u6a21\u62df\u548c\u5bc6\u5ea6\u6cdb\u51fd\u7406\u8bba\uff08DFT\uff09\u8ba1\u7b97\uff0c\u6765\u5206\u6790\u8f90\u7167\u635f\u4f24\u548c\u6750\u6599\u7684\u5e94\u53d8\u884c\u4e3a\u3002", "result": "N-PED\u6280\u672f\u80fd\u591f\u7cbe\u786e\u6d4b\u91cfSiC\u6750\u6599\u4e2d\u7684\u5e94\u53d8\u68af\u5ea6\u3002\u7814\u7a76\u53d1\u73b0\u5728He+H\u8f90\u7167\u7684\u5355\u66764H-SiC\u4e2d\uff0cN-PED\u7ed3\u679c\u4e0eHR-XRD\u6a21\u62df\u7ed3\u679c\u543b\u5408\u826f\u597d\u3002\u5728He\u8f90\u7167\u7684\u591a\u6676\u03b1-SiC\u4e2d\uff0c\u89c2\u5bdf\u5230\u6676\u754c\u9644\u8fd1\u7684\u7a7a\u6ce1\u8d2b\u5316\u533a\uff08BDZ\uff09\uff0c\u8868\u660e\u6676\u754c\u662f\u7f3a\u9677\u7684\u6d3b\u6027\u6c47\u3002N-PED\u5728\u9ad8\u7c89\u672bSiC\u7684\u6676\u754c\u5904\u89c2\u6d4b\u5230\u9ad8\u8fbe2.5%\u7684\u5e94\u53d8\u653e\u5927\uff0c\u4ee5\u53caBDZ\u5185\u7684\u5e94\u53d8\u5f1b\u8c6b\u3002DFT\u8ba1\u7b97\u8868\u660e\uff0cSi\u3001C\u548cHe\u539f\u5b50\u6709\u5411\u6676\u754c\u6838\u5fc3\u533a\u8fc1\u79fb\u7684\u8d8b\u52bf\uff0c\u8fd9\u5bfc\u81f4\u7528\u4e8e\u5bb9\u7eb3He\u7684\u7a7a\u4f4d\u51cf\u5c11\uff0c\u5e76\u5728\u6676\u754c\u9644\u8fd1\u4ea7\u751f\u5c40\u90e8\u5e94\u53d8\u5f1b\u8c6b\u3002\u7b2c\u4e00\u6027\u539f\u7406\u62c9\u4f38\u6a21\u62df\u8868\u660e\uff0cSi\u548cC\u7684\u95f4\u9699\u539f\u5b50\u80fd\u591f\u7f13\u89e3He\u5f15\u8d77\u7684\u6676\u754c\u8106\u5316\u3002", "conclusion": "\u6676\u754c\u5de5\u7a0b\u5bf9\u4e8e\u63d0\u9ad8SiC\u6750\u6599\u7684\u6297\u8f90\u7167\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002\u901a\u8fc7\u7406\u89e3\u8f90\u7167\u8bf1\u5bfc\u7684\u5e94\u53d8\u673a\u5236\uff0c\u5e76\u5229\u7528Si\u3001C\u539f\u5b50\u95f4\u9699\u5bf9He\u5f15\u8d77\u7684\u6676\u754c\u8106\u5316\u7684\u7f13\u89e3\u4f5c\u7528\uff0c\u53ef\u4ee5\u4e3a\u6838\u80fd\u548c\u7a7a\u95f4\u5e94\u7528\u4e2d\u7684SiC\u6750\u6599\u8bbe\u8ba1\u63d0\u4f9b\u6307\u5bfc\u3002"}}
{"id": "2509.19073", "categories": ["cs.CV", "eess.IV", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.19073", "abs": "https://arxiv.org/abs/2509.19073", "authors": ["Hung Nguyen", "Runfa Li", "An Le", "Truong Nguyen"], "title": "WaveletGaussian: Wavelet-domain Diffusion for Sparse-view 3D Gaussian Object Reconstruction", "comment": null, "summary": "3D Gaussian Splatting (3DGS) has become a powerful representation for\nimage-based object reconstruction, yet its performance drops sharply in\nsparse-view settings. Prior works address this limitation by employing\ndiffusion models to repair corrupted renders, subsequently using them as pseudo\nground truths for later optimization. While effective, such approaches incur\nheavy computation from the diffusion fine-tuning and repair steps. We present\nWaveletGaussian, a framework for more efficient sparse-view 3D Gaussian object\nreconstruction. Our key idea is to shift diffusion into the wavelet domain:\ndiffusion is applied only to the low-resolution LL subband, while\nhigh-frequency subbands are refined with a lightweight network. We further\npropose an efficient online random masking strategy to curate training pairs\nfor diffusion fine-tuning, replacing the commonly used, but inefficient,\nleave-one-out strategy. Experiments across two benchmark datasets, Mip-NeRF 360\nand OmniObject3D, show WaveletGaussian achieves competitive rendering quality\nwhile substantially reducing training time.", "AI": {"tldr": "WaveletGaussian\u901a\u8fc7\u5728\u5c0f\u6ce2\u57df\u8fdb\u884c\u6269\u6563\uff0c\u5e76\u7ed3\u5408\u8f7b\u91cf\u7ea7\u7f51\u7edc\u548c\u5728\u7ebf\u968f\u673a\u906e\u853d\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u7a00\u758f\u89c6\u56fe3D\u9ad8\u65af\u91cd\u5efa\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u7ade\u4e89\u529b\u7684\u6e32\u67d3\u8d28\u91cf\u5e76\u663e\u8457\u51cf\u5c11\u4e86\u8bad\u7ec3\u65f6\u95f4\u3002", "motivation": "\u73b0\u6709\u76843D\u9ad8\u65af\u6cfc\u6e85\u65b9\u6cd5\u5728\u7a00\u758f\u89c6\u56fe\u4e0b\u6027\u80fd\u4e0b\u964d\uff0c\u800c\u901a\u8fc7\u6269\u6563\u6a21\u578b\u4fee\u590d\u6e32\u67d3\u56fe\u7684\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u3002", "method": "\u5c06\u6269\u6563\u8fc7\u7a0b\u8f6c\u79fb\u5230\u5c0f\u6ce2\u57df\uff0c\u4ec5\u5728\u4f4e\u9891LL\u5b50\u5e26\u4e0a\u8fdb\u884c\u6269\u6563\uff0c\u9ad8\u9891\u5b50\u5e26\u5219\u7528\u8f7b\u91cf\u7ea7\u7f51\u7edc\u4f18\u5316\uff1b\u63d0\u51fa\u5728\u7ebf\u968f\u673a\u906e\u853d\u7b56\u7565\u6765\u9009\u62e9\u6269\u6563\u5fae\u8c03\u7684\u8bad\u7ec3\u5bf9\u3002", "result": "WaveletGaussian\u5728Mip-NeRF 360\u548cOmniObject3D\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u6e32\u67d3\u8d28\u91cf\uff0c\u540c\u65f6\u5927\u5927\u7f29\u77ed\u4e86\u8bad\u7ec3\u65f6\u95f4\u3002", "conclusion": "WaveletGaussian\u662f\u4e00\u79cd\u66f4\u6709\u6548\u7684\u7a00\u758f\u89c6\u56fe3D\u9ad8\u65af\u5bf9\u8c61\u91cd\u5efa\u6846\u67b6\uff0c\u901a\u8fc7\u5c0f\u6ce2\u57df\u6269\u6563\u548c\u4f18\u5316\u7684\u8bad\u7ec3\u7b56\u7565\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2509.18626", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18626", "abs": "https://arxiv.org/abs/2509.18626", "authors": ["Jay Patrikar", "Apoorva Sharma", "Sushant Veer", "Boyi Li", "Sebastian Scherer", "Marco Pavone"], "title": "The Case for Negative Data: From Crash Reports to Counterfactuals for Reasonable Driving", "comment": "8 pages, 5 figures", "summary": "Learning-based autonomous driving systems are trained mostly on incident-free\ndata, offering little guidance near safety-performance boundaries. Real crash\nreports contain precisely the contrastive evidence needed, but they are hard to\nuse: narratives are unstructured, third-person, and poorly grounded to sensor\nviews. We address these challenges by normalizing crash narratives to\nego-centric language and converting both logs and crashes into a unified\nscene-action representation suitable for retrieval. At decision time, our\nsystem adjudicates proposed actions by retrieving relevant precedents from this\nunified index; an agentic counterfactual extension proposes plausible\nalternatives, retrieves for each, and reasons across outcomes before deciding.\nOn a nuScenes benchmark, precedent retrieval substantially improves\ncalibration, with recall on contextually preferred actions rising from 24% to\n53%. The counterfactual variant preserves these gains while sharpening\ndecisions near risk.", "AI": {"tldr": "\u5728\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e2d\uff0c\u901a\u8fc7\u5c06\u4e8b\u6545\u62a5\u544a\u8f6c\u5316\u4e3a\u4ee5\u81ea\u6211\u4e3a\u4e2d\u5fc3\u7684\u8bed\u8a00\uff0c\u5e76\u7edf\u4e00\u65e5\u5fd7\u548c\u4e8b\u6545\u573a\u666f\u5230\u573a\u666f-\u52a8\u4f5c\u8868\u793a\uff0c\u4ee5\u652f\u6301\u57fa\u4e8e\u68c0\u7d22\u7684\u51b3\u7b56\uff0c\u4ece\u800c\u63d0\u9ad8\u5b89\u5168\u6027\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e3b\u8981\u5728\u65e0\u4e8b\u6545\u6570\u636e\u4e0a\u8bad\u7ec3\uff0c\u7f3a\u4e4f\u5728\u5b89\u5168-\u6027\u80fd\u8fb9\u754c\u9644\u8fd1\u7684\u6307\u5bfc\u3002\u800c\u771f\u5b9e\u7684\u4e8b\u6545\u62a5\u544a\u5305\u542b\u4e86\u5173\u952e\u7684\u5bf9\u6bd4\u8bc1\u636e\uff0c\u4f46\u5176\u975e\u7ed3\u6784\u5316\u3001\u7b2c\u4e09\u4eba\u79f0\u7684\u53d9\u8ff0\u65b9\u5f0f\u96be\u4ee5\u76f4\u63a5\u5229\u7528\u3002", "method": "\u5c06\u4e8b\u6545\u62a5\u544a\u8fdb\u884c\u89c4\u8303\u5316\uff0c\u8f6c\u5316\u4e3a\u4ee5\u81ea\u6211\u4e3a\u4e2d\u5fc3\u7684\u8bed\u8a00\uff0c\u5e76\u5c06\u65e5\u5fd7\u548c\u4e8b\u6545\u62a5\u544a\u7edf\u4e00\u4e3a\u573a\u666f-\u52a8\u4f5c\u8868\u793a\u3002\u5728\u51b3\u7b56\u65f6\uff0c\u901a\u8fc7\u68c0\u7d22\u76f8\u5173\u5148\u4f8b\u6765\u4ef2\u88c1\u5efa\u8bae\u7684\u64cd\u4f5c\uff0c\u5e76\u5f15\u5165\u4ee3\u7406\u53cd\u4e8b\u5b9e\u6269\u5c55\u6765\u63d0\u51fa\u53ef\u80fd\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u68c0\u7d22\u5e76\u63a8\u7406\u4e0d\u540c\u7ed3\u679c\uff0c\u6700\u7ec8\u505a\u51fa\u51b3\u7b56\u3002", "result": "\u5728nuScenes\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5148\u4f8b\u68c0\u7d22\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u6821\u51c6\u5ea6\uff0c\u5c06\u4e0a\u4e0b\u6587\u9996\u9009\u64cd\u4f5c\u7684\u53ec\u56de\u7387\u4ece24%\u63d0\u9ad8\u523053%\u3002\u53cd\u4e8b\u5b9e\u53d8\u4f53\u5728\u4fdd\u6301\u8fd9\u4e9b\u63d0\u5347\u7684\u540c\u65f6\uff0c\u8fd8\u80fd\u4f18\u5316\u98ce\u9669\u8fb9\u7f18\u7684\u51b3\u7b56\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u901a\u8fc7\u5229\u7528\u4e8b\u6545\u62a5\u544a\u7684\u5148\u4f8b\u68c0\u7d22\u548c\u53cd\u4e8b\u5b9e\u63a8\u7406\uff0c\u80fd\u591f\u6709\u6548\u5730\u63d0\u9ad8\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u51b3\u7b56\u80fd\u529b\u3002"}}
{"id": "2509.18350", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18350", "abs": "https://arxiv.org/abs/2509.18350", "authors": ["Oussema Dhaouadi", "Riccardo Marin", "Johannes Meier", "Jacques Kaiser", "Daniel Cremers"], "title": "OrthoLoC: UAV 6-DoF Localization and Calibration Using Orthographic Geodata", "comment": "Accepted at NeurIPS 2025", "summary": "Accurate visual localization from aerial views is a fundamental problem with\napplications in mapping, large-area inspection, and search-and-rescue\noperations. In many scenarios, these systems require high-precision\nlocalization while operating with limited resources (e.g., no internet\nconnection or GNSS/GPS support), making large image databases or heavy 3D\nmodels impractical. Surprisingly, little attention has been given to leveraging\northographic geodata as an alternative paradigm, which is lightweight and\nincreasingly available through free releases by governmental authorities (e.g.,\nthe European Union). To fill this gap, we propose OrthoLoC, the first\nlarge-scale dataset comprising 16,425 UAV images from Germany and the United\nStates with multiple modalities. The dataset addresses domain shifts between\nUAV imagery and geospatial data. Its paired structure enables fair benchmarking\nof existing solutions by decoupling image retrieval from feature matching,\nallowing isolated evaluation of localization and calibration performance.\nThrough comprehensive evaluation, we examine the impact of domain shifts, data\nresolutions, and covisibility on localization accuracy. Finally, we introduce a\nrefinement technique called AdHoP, which can be integrated with any feature\nmatcher, improving matching by up to 95% and reducing translation error by up\nto 63%. The dataset and code are available at:\nhttps://deepscenario.github.io/OrthoLoC.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86OrthoLoC\u6570\u636e\u96c6\u548cAdHoP\u6539\u8fdb\u6280\u672f\uff0c\u7528\u4e8e\u5728\u6709\u9650\u8d44\u6e90\u4e0b\u8fdb\u884c\u9ad8\u7cbe\u5ea6\u89c6\u89c9\u5b9a\u4f4d\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9\u5b9a\u4f4d\u7cfb\u7edf\u5728\u8d44\u6e90\u53d7\u9650\uff08\u65e0\u7f51\u7edc\u3001\u65e0GPS\uff09\u4e14\u9700\u8981\u9ad8\u7cbe\u5ea6\u5b9a\u4f4d\u65f6\uff0c\u96be\u4ee5\u5904\u7406\u5927\u578b\u6570\u636e\u5e93\u62163D\u6a21\u578b\u3002\u73b0\u6709\u7684\u7814\u7a76\u5f88\u5c11\u5173\u6ce8\u5229\u7528\u8f7b\u91cf\u7ea7\u4e14\u6613\u4e8e\u83b7\u53d6\u7684\u6b63\u5c04\u5730\u7406\u6570\u636e\u3002", "method": "\u63d0\u51faOrthoLoC\u6570\u636e\u96c6\uff0c\u5305\u542b\u6765\u81ea\u5fb7\u56fd\u548c\u7f8e\u56fd\u768416,425\u5f20UAV\u56fe\u50cf\uff0c\u89e3\u51b3\u4e86UAV\u56fe\u50cf\u4e0e\u5730\u7406\u7a7a\u95f4\u6570\u636e\u4e4b\u95f4\u7684\u57df\u8f6c\u79fb\u95ee\u9898\uff0c\u5e76\u5141\u8bb8\u72ec\u7acb\u8bc4\u4f30\u5b9a\u4f4d\u548c\u6821\u51c6\u6027\u80fd\u3002\u63d0\u51faAdHoP\u6280\u672f\uff0c\u7528\u4e8e\u6539\u8fdb\u7279\u5f81\u5339\u914d\u3002", "result": "\u901a\u8fc7\u5168\u9762\u8bc4\u4f30\uff0c\u7814\u7a76\u4e86\u57df\u8f6c\u79fb\u3001\u6570\u636e\u5206\u8fa8\u7387\u548c\u5171\u540c\u53ef\u89c1\u6027\u5bf9\u5b9a\u4f4d\u7cbe\u5ea6\u7684\u5f71\u54cd\u3002AdHoP\u6280\u672f\u53ef\u5c06\u5339\u914d\u7387\u63d0\u9ad895%\uff0c\u5e76\u5c06\u5e73\u79fb\u8bef\u5dee\u964d\u4f4e63%\u3002", "conclusion": "OrthoLoC\u6570\u636e\u96c6\u548cAdHoP\u6280\u672f\u4e3a\u5728\u8d44\u6e90\u53d7\u9650\u6761\u4ef6\u4e0b\u8fdb\u884c\u9ad8\u7cbe\u5ea6\u89c6\u89c9\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u4e3a\u8be5\u9886\u57df\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u51c6\u3002"}}
{"id": "2509.18547", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2509.18547", "abs": "https://arxiv.org/abs/2509.18547", "authors": ["James D. Teoh", "Nathanael Cottet", "Patrick Winkel", "Luke D. Burkhart", "Luigi Frunzio", "Robert J. Schoelkopf"], "title": "Robust quantum communication through lossy microwave links", "comment": null, "summary": "Entanglement generation lies at the heart of many quantum networking\nprotocols as it enables distributed and modular quantum computing. For\nsuperconducting qubits, entanglement fidelity is typically limited by photon\nloss in the links that connect these qubits together. We propose and realize a\nnew scheme for heralded entanglement generation that almost entirely\ncircumvents this limit. We produce Bell states with $92\\pm1\\%$ state fidelity,\nincluding state preparation and measurement (SPAM) errors, between separated\nsuperconducting bosonic qubits in a high-loss regime where direct deterministic\nstate transfer fails. Our scheme exploits simple but fundamental physics found\nin microwave links, specifically the ability to treat our communication channel\nas a single standing wave mode. Combining this with local measurements on\nbosonically encoded qubits allows us to herald entanglement with success\nprobabilities approaching the scheme's upper limit of 50% per attempt. We then\nuse the heralded Bell state as a resource to deterministically teleport a qubit\nbetween modules with an average state transfer fidelity of $90\\pm1\\%$. This is\nachieved despite the link possessing a direct single photon transfer efficiency\nof 2%. Our work informs the design of future superconducting quantum networks,\nby demonstrating fast coupling rates and low loss links are no longer strict\nrequirements for high-fidelity quantum communication in the microwave regime.", "AI": {"tldr": "\u901a\u8fc7\u5229\u7528\u5fae\u6ce2\u94fe\u8def\u4e2d\u7684\u9a7b\u6ce2\u6a21\u5f0f\u548c\u5c40\u90e8\u6d4b\u91cf\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4fe1\u53f7\u8f85\u52a9\u7ea0\u7f20\u751f\u6210\u65b9\u6848\uff0c\u4ee5\u9ad8\u4fdd\u771f\u5ea6\uff0892\u00b11%\uff09\u5728\u5206\u79bb\u7684\u8d85\u5bfc\u6bd4\u7279\u4e4b\u95f4\u751f\u6210\u8d1d\u5c14\u6001\uff0c\u5373\u4f7f\u5728\u5149\u5b50\u635f\u8017\u4e25\u91cd\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u5b9e\u73b0\uff0c\u5e76\u6210\u529f\u5730\u5c06\u6b64\u4f5c\u4e3a\u8d44\u6e90\u8fdb\u884c\u4e86\u786e\u5b9a\u6027\u91cf\u5b50\u9690\u5f62\u4f20\u6001\uff0c\u5e73\u5747\u4fdd\u771f\u5ea6\u8fbe\u523090\u00b11%\u3002", "motivation": "\u4e3a\u4e86\u514b\u670d\u8d85\u5bfc\u91cf\u5b50\u7f51\u7edc\u4e2d\u5149\u5b50\u635f\u8017\u5bf9\u7ea0\u7f20\u4fdd\u771f\u5ea6\u7684\u9650\u5236\uff0c\u5e76\u4e3a\u5206\u5e03\u5f0f\u91cf\u5b50\u8ba1\u7b97\u63d0\u4f9b\u652f\u6301\u3002", "method": "\u5229\u7528\u5fae\u6ce2\u94fe\u8def\u4e2d\u7684\u9a7b\u6ce2\u6a21\u5f0f\u548c\u5bf9\u73bb\u8272\u5b50\u7f16\u7801\u91cf\u5b50\u8fdb\u884c\u5c40\u90e8\u6d4b\u91cf\uff0c\u5b9e\u73b0\u4fe1\u53f7\u8f85\u52a9\u7ea0\u7f20\u751f\u6210\uff0c\u5e76\u4ee5\u6b64\u4e3a\u8d44\u6e90\u8fdb\u884c\u786e\u5b9a\u6027\u91cf\u5b50\u9690\u5f62\u4f20\u6001\u3002", "result": "\u5728\u9ad8\u8fbe2%\u7684\u5355\u5149\u5b50\u4f20\u8f93\u6548\u7387\u7684\u6761\u4ef6\u4e0b\uff0c\u5b9e\u73b0\u4e8692\u00b11%\u7684\u8d1d\u5c14\u6001\u4fdd\u771f\u5ea6\u548c90\u00b11%\u7684\u91cf\u5b50\u6bd4\u7279\u4f20\u8f93\u4fdd\u771f\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6848\u8868\u660e\uff0c\u5728\u9ad8\u635f\u8017\u7684\u5fae\u6ce2\u901a\u4fe1\u4e2d\uff0c\u5feb\u901f\u7684\u8026\u5408\u901f\u7387\u548c\u4f4e\u635f\u8017\u94fe\u8def\u4e0d\u518d\u662f\u9ad8\u4fdd\u771f\u5ea6\u91cf\u5b50\u901a\u4fe1\u7684\u4e25\u683c\u8981\u6c42\uff0c\u4e3a\u672a\u6765\u8d85\u5bfc\u91cf\u5b50\u7f51\u7edc\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2509.18127", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18127", "abs": "https://arxiv.org/abs/2509.18127", "authors": ["Jiaqi Weng", "Han Zheng", "Hanyu Zhang", "Qinqin He", "Jialing Tao", "Hui Xue", "Zhixuan Chu", "Xiting Wang"], "title": "Safe-SAIL: Towards a Fine-grained Safety Landscape of Large Language Models via Sparse Autoencoder Interpretation Framework", "comment": null, "summary": "Increasing deployment of large language models (LLMs) in real-world\napplications raises significant safety concerns. Most existing safety research\nfocuses on evaluating LLM outputs or specific safety tasks, limiting their\nability to ad- dress broader, undefined risks. Sparse Autoencoders (SAEs)\nfacilitate interpretability research to clarify model behavior by explaining\nsingle-meaning atomic features decomposed from entangled signals. jHowever,\nprior applications on SAEs do not interpret features with fine-grained\nsafety-related con- cepts, thus inadequately addressing safety-critical\nbehaviors, such as generating toxic responses and violating safety regu-\nlations. For rigorous safety analysis, we must extract a rich and diverse set\nof safety-relevant features that effectively capture these high-risk behaviors,\nyet face two challenges: identifying SAEs with the greatest potential for\ngenerating safety concept-specific neurons, and the prohibitively high cost of\ndetailed feature explanation. In this paper, we pro- pose Safe-SAIL, a\nframework for interpreting SAE features within LLMs to advance mechanistic\nunderstanding in safety domains. Our approach systematically identifies SAE\nwith best concept-specific interpretability, explains safety-related neurons,\nand introduces efficient strategies to scale up the in- terpretation process.\nWe will release a comprehensive toolkit including SAE checkpoints and\nhuman-readable neuron ex- planations, which supports empirical analysis of\nsafety risks to promote research on LLM safety.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51faSafe-SAIL\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u91ca\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\u7684\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAE\uff09\u7279\u5f81\uff0c\u4ee5\u589e\u5f3a\u5bf9\u5b89\u5168\u9886\u57df\u7684\u673a\u5236\u7406\u89e3\u3002\u8be5\u6846\u67b6\u80fd\u8bc6\u522b\u5177\u6709\u6700\u4f73\u6982\u5ff5\u7279\u5f02\u6027\u53ef\u89e3\u91ca\u6027\u7684SAE\uff0c\u89e3\u91ca\u4e0e\u5b89\u5168\u76f8\u5173\u7684\u795e\u7ecf\u5143\uff0c\u5e76\u5f15\u5165\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u91ca\u7b56\u7565\uff0c\u4ee5\u5e94\u5bf9\u63d0\u53d6\u4e30\u5bcc\u3001\u591a\u6837\u5316\u7684\u5b89\u5168\u76f8\u5173\u7279\u5f81\u6240\u9762\u4e34\u7684\u6311\u6218\u3002\u7814\u7a76\u8fd8\u5c06\u53d1\u5e03\u4e00\u4e2a\u5305\u542bSAE\u68c0\u67e5\u70b9\u548c\u4eba\u7c7b\u53ef\u8bfb\u795e\u7ecf\u5143\u89e3\u91ca\u7684\u5de5\u5177\u5305\uff0c\u4ee5\u652f\u6301\u5bf9LLM\u5b89\u5168\u98ce\u9669\u7684\u5b9e\u8bc1\u5206\u6790\u3002", "motivation": "\u73b0\u6709LLM\u5b89\u5168\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u6a21\u578b\u8f93\u51fa\u6216\u7279\u5b9a\u5b89\u5168\u4efb\u52a1\uff0c\u96be\u4ee5\u5e94\u5bf9\u66f4\u5e7f\u6cdb\u3001\u672a\u5b9a\u4e49\u98ce\u9669\u3002SAE\u867d\u6709\u52a9\u4e8e\u6a21\u578b\u53ef\u89e3\u91ca\u6027\uff0c\u4f46\u4ee5\u5f80\u5e94\u7528\u672a\u80fd\u7cbe\u7ec6\u89e3\u91ca\u5b89\u5168\u6982\u5ff5\uff0c\u5bf9\u6bd2\u6027\u54cd\u5e94\u6216\u8fdd\u53cd\u5b89\u5168\u89c4\u5b9a\u7b49\u884c\u4e3a\u7684\u5e94\u5bf9\u4e0d\u8db3\u3002", "method": "\u63d0\u51faSafe-SAIL\u6846\u67b6\uff0c\u7cfb\u7edf\u5730\u8bc6\u522b\u5177\u6709\u6700\u4f73\u6982\u5ff5\u7279\u5f02\u6027\u53ef\u89e3\u91ca\u6027\u7684SAE\uff0c\u89e3\u91ca\u5b89\u5168\u76f8\u5173\u795e\u7ecf\u5143\uff0c\u5e76\u5f15\u5165\u4e86\u6709\u6548\u7684\u7b56\u7565\u6765\u6269\u5c55\u89e3\u91ca\u8fc7\u7a0b\u3002", "result": "\u7814\u7a76\u65e8\u5728\u63d0\u4f9b\u4e00\u79cd\u66f4\u6709\u6548\u7684SAE\u7279\u5f81\u89e3\u91ca\u65b9\u6cd5\uff0c\u4ee5\u5e94\u5bf9\u63d0\u53d6\u5b89\u5168\u76f8\u5173\u7279\u5f81\u7684\u6311\u6218\uff0c\u5e76\u6700\u7ec8\u901a\u8fc7\u53d1\u5e03\u7684\u5de5\u5177\u5305\u652f\u6301\u5bf9LLM\u5b89\u5168\u98ce\u9669\u7684\u5b9e\u8bc1\u5206\u6790\u3002", "conclusion": "Safe-SAIL\u6846\u67b6\u901a\u8fc7\u7cbe\u7ec6\u5316SAE\u7279\u5f81\u89e3\u91ca\uff0c\u80fd\u591f\u66f4\u6709\u6548\u5730\u6355\u6349\u548c\u7406\u89e3LLM\u4e2d\u7684\u5b89\u5168\u98ce\u9669\uff0c\u4e3aLLM\u5b89\u5168\u7814\u7a76\u63d0\u4f9b\u65b0\u7684\u673a\u5236\u5316\u89c6\u89d2\u548c\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2509.18420", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18420", "abs": "https://arxiv.org/abs/2509.18420", "authors": ["Nikolai Skripko"], "title": "Instruction-Following Evaluation in Function Calling for Large Language Models", "comment": null, "summary": "Function calling is a core capability of large language models, essential for\nAI agents. Existing benchmarks such as the Berkeley Function Calling\nLeaderboard (BFCL), tau^2-Bench (arXiv:2506.07982), and ACEBench\n(arXiv:2501.12851) evaluate argument correctness but do not test adherence to\nformat instructions embedded in parameter descriptions, such as enclosing\nvalues in double quotes or using ISO date formats.\n  We introduce IFEval-FC, a benchmark inspired by IFEval (arXiv:2311.07911)\nthat assesses precise instruction following in function calling. IFEval-FC\nencodes verifiable formats directly within JSON schema descriptions, for\nexample specifying that a value must not contain punctuation. It includes 750\ntest cases, each consisting of a function with an embedded format for one of\nits input parameters and a corresponding user query. Evaluation is fully\nalgorithmic, ensuring objectivity, reproducibility, and scalability.\n  Our results show that even state-of-the-art proprietary models, including\nGPT-5 and Claude 4.1 Opus, frequently fail to follow basic formatting rules,\nhighlighting a practical limitation for real-world agent systems. The complete\ncodebase and data are publicly available at\nhttps://github.com/Skripkon/IFEval-FC.", "AI": {"tldr": "IFEval-FC \u662f\u4e00\u4e2a\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u51fd\u6570\u8c03\u7528\u4e2d\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u7684\u65b0\u57fa\u51c6\u3002\u8be5\u57fa\u51c6\u901a\u8fc7\u5728 JSON schema \u4e2d\u7f16\u7801\u53ef\u9a8c\u8bc1\u7684\u683c\u5f0f\u6765\u6d4b\u8bd5\u6a21\u578b\u662f\u5426\u80fd\u9075\u5faa\u53c2\u6570\u63cf\u8ff0\u4e2d\u7684\u683c\u5f0f\u8bf4\u660e\u3002\u73b0\u6709\u6a21\u578b\uff08\u5305\u62ec GPT-5 \u548c Claude 4.1 Opus\uff09\u5728\u9075\u5faa\u57fa\u672c\u683c\u5f0f\u89c4\u5219\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u8fd9\u8868\u660e\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5b58\u5728\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u51fd\u6570\u8c03\u7528\u7684\u57fa\u51c6\uff08\u5982 BFCL\u3001tau^2-Bench\u3001ACEBench\uff09\u4e3b\u8981\u5173\u6ce8\u53c2\u6570\u7684\u6b63\u786e\u6027\uff0c\u4f46\u5ffd\u7565\u4e86\u6a21\u578b\u9075\u5faa\u53c2\u6570\u63cf\u8ff0\u4e2d\u5d4c\u5165\u7684\u683c\u5f0f\u6307\u4ee4\u7684\u80fd\u529b\uff0c\u4f8b\u5982\u503c\u662f\u5426\u7528\u53cc\u5f15\u53f7\u62ec\u8d77\u6765\u6216\u662f\u5426\u4f7f\u7528 ISO \u65e5\u671f\u683c\u5f0f\u3002\u8fd9\u5bf9\u4e8e\u6784\u5efa\u53ef\u9760\u7684 AI \u4ee3\u7406\u81f3\u5173\u91cd\u8981\u3002", "method": "IFEval-FC \u57fa\u51c6\u7684\u8bbe\u8ba1\u7075\u611f\u6765\u6e90\u4e8e IFEval\uff0c\u5b83\u5c06\u53ef\u9a8c\u8bc1\u7684\u683c\u5f0f\u76f4\u63a5\u5d4c\u5165\u5230 JSON schema \u63cf\u8ff0\u4e2d\uff0c\u4f8b\u5982\u6307\u5b9a\u67d0\u4e2a\u503c\u4e0d\u5e94\u5305\u542b\u6807\u70b9\u7b26\u53f7\u3002\u8be5\u57fa\u51c6\u5305\u542b 750 \u4e2a\u6d4b\u8bd5\u7528\u4f8b\uff0c\u6bcf\u4e2a\u7528\u4f8b\u90fd\u5305\u62ec\u4e00\u4e2a\u5e26\u6709\u683c\u5f0f\u8bf4\u660e\u7684\u51fd\u6570\u53ca\u5176\u5bf9\u5e94\u7684\u7528\u6237\u67e5\u8be2\u3002\u8bc4\u4f30\u8fc7\u7a0b\u5b8c\u5168\u7b97\u6cd5\u5316\uff0c\u4ee5\u786e\u4fdd\u5ba2\u89c2\u6027\u3001\u53ef\u590d\u73b0\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5373\u4f7f\u662f\u50cf GPT-5 \u548c Claude 4.1 Opus \u8fd9\u6837\u6700\u5148\u8fdb\u7684\u4e13\u6709\u6a21\u578b\uff0c\u4e5f\u7ecf\u5e38\u65e0\u6cd5\u9075\u5faa\u57fa\u672c\u7684\u683c\u5f0f\u89c4\u5219\u3002\u8fd9\u63ed\u793a\u4e86\u5728\u5b9e\u9645\u7684 AI \u4ee3\u7406\u7cfb\u7edf\u4e2d\u5b58\u5728\u4e00\u4e2a\u5b9e\u9645\u7684\u5c40\u9650\u6027\u3002", "conclusion": "IFEval-FC \u57fa\u51c6\u7684\u5f15\u5165\u7a81\u663e\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4e25\u683c\u9075\u5faa\u51fd\u6570\u8c03\u7528\u683c\u5f0f\u6307\u4ee4\u65b9\u9762\u5b58\u5728\u7684\u4e0d\u8db3\u3002\u73b0\u6709\u6a21\u578b\u5728\u5904\u7406\u8fd9\u4e9b\u683c\u5f0f\u7ea6\u675f\u65f6\u8868\u73b0\u51fa\u7684\u6323\u624e\uff0c\u5bf9\u9700\u8981\u7cbe\u786e\u683c\u5f0f\u5316\u8f93\u5165\u7684\u771f\u5b9e\u4e16\u754c AI \u4ee3\u7406\u7cfb\u7edf\u7684\u5f00\u53d1\u548c\u90e8\u7f72\u63d0\u51fa\u4e86\u6311\u6218\u3002"}}
{"id": "2509.18666", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.18666", "abs": "https://arxiv.org/abs/2509.18666", "authors": ["Kaizer Rahaman", "Simran Kumari", "Ashish R. Hota"], "title": "Distributionally Robust Safe Motion Planning with Contextual Information", "comment": null, "summary": "We present a distributionally robust approach for collision avoidance by\nincorporating contextual information. Specifically, we embed the conditional\ndistribution of future trajectory of the obstacle conditioned on the motion of\nthe ego agent in a reproducing kernel Hilbert space (RKHS) via the conditional\nkernel mean embedding operator. Then, we define an ambiguity set containing all\ndistributions whose embedding in the RKHS is within a certain distance from the\nempirical estimate of conditional mean embedding learnt from past data.\nConsequently, a distributionally robust collision avoidance constraint is\nformulated, and included in the receding horizon based motion planning\nformulation of the ego agent. Simulation results show that the proposed\napproach is more successful in avoiding collision compared to approaches that\ndo not include contextual information and/or distributional robustness in their\nformulation in several challenging scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684\u5206\u5e03\u9c81\u68d2\u78b0\u649e\u907f\u514d\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728RKHS\u4e2d\u5d4c\u5165\u6761\u4ef6\u8f68\u8ff9\u5206\u5e03\uff0c\u5b9a\u4e49\u6b67\u4e49\u96c6\uff0c\u5e76\u5c06\u5176\u7eb3\u5165\u8fd0\u52a8\u89c4\u5212\u3002", "motivation": "\u4e3a\u4e86\u5728\u907f\u514d\u78b0\u649e\u7684\u540c\u65f6\u8003\u8651\u73af\u5883\u7684\u52a8\u6001\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u63d0\u9ad8\u907f\u78b0\u7684\u6210\u529f\u7387\u3002", "method": "\u901a\u8fc7\u6761\u4ef6\u6838\u5747\u503c\u5d4c\u5165\u7b97\u5b50\u5728RKHS\u4e2d\u5d4c\u5165\u6761\u4ef6\u8f68\u8ff9\u5206\u5e03\uff0c\u5b9a\u4e49\u5305\u542b\u7ecf\u9a8c\u4f30\u8ba1\u503c\u4e00\u5b9a\u8ddd\u79bb\u5185\u7684\u5206\u5e03\u7684\u6b67\u4e49\u96c6\uff0c\u5e76\u5c06\u5176\u5f62\u5f0f\u5316\u4e3a\u5206\u5e03\u9c81\u68d2\u78b0\u649e\u907f\u514d\u7ea6\u675f\uff0c\u7eb3\u5165\u4ee5\u9012\u63a8\u9884\u6d4b\u4e3a\u57fa\u7840\u7684\u8fd0\u52a8\u89c4\u5212\u3002", "result": "\u76f8\u6bd4\u4e8e\u672a\u8003\u8651\u4e0a\u4e0b\u6587\u4fe1\u606f\u548c/\u6216\u5206\u5e03\u9c81\u68d2\u6027\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u573a\u666f\u4e2d\u66f4\u6210\u529f\u5730\u907f\u514d\u4e86\u78b0\u649e\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u5206\u5e03\u9c81\u68d2\u65b9\u6cd5\uff0c\u901a\u8fc7\u6574\u5408\u4e0a\u4e0b\u6587\u4fe1\u606f\u548c\u5206\u5e03\u9c81\u68d2\u6027\uff0c\u80fd\u591f\u66f4\u6709\u6548\u5730\u8fdb\u884c\u78b0\u649e\u907f\u514d\u3002"}}
{"id": "2509.18923", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2509.18923", "abs": "https://arxiv.org/abs/2509.18923", "authors": ["Christopher T. S. Cheung", "Valerio Vitale", "Lennart Klebl", "Ammon Fischer", "Dante M. Kennes", "Arash A. Mostofi", "Johannes Lischner", "Zachary A. H. Goodwin"], "title": "Magnetic Ordering in Moir\u00e9 Graphene Multilayers from a Continuum Hartree+U Approach", "comment": null, "summary": "Recently, symmetry-broken ground states, such as correlated insulating\nstates, magnetic order and superconductivity, have been discovered in twisted\nbilayer graphene (tBLG) and twisted trilayer graphene (tTLG) near the so-called\nmagic-angle. Understanding the magnetic order in these systems is challenging,\nas atomistic methods become extremely expensive near the magic angle and\ncontinuum approaches fail to capture important atomistic details. In this work,\nwe develop a self-consistent approach based on a continuum model that\nincorporates both short-ranged Hubbard interactions and long-ranged Coulomb\ninteractions, therefore allowing efficient exploration of magnetic order in\nmoir\\'e graphene multilayers. With this approach, we perform a systematic\nanalysis of the magnetic phase diagram of tBLG as a function of doping level\nand twist angle, near the magic angle. We find that the results are consistent\nwith previous perturbative atomistic Hartree+U calculations. Furthermore, we\npredict stable magnetic orders for the tTLG. We found that the magnetic orders\nare similar to those in tBLG for small values of on-site repulsion. In the\nfuture, the developed method can be utilized to investigate magnetic ordering\ntendencies from short-range exchange interactions in other moir\\'e graphene\nmultilayers as a function of doping, twist angle, screening environment, among\nother variables.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.19234", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.19234", "abs": "https://arxiv.org/abs/2509.19234", "authors": ["Hesam Hosseini", "Ying Cao", "Ali H. Sayed"], "title": "Stability and Generalization of Adversarial Diffusion Training", "comment": null, "summary": "Algorithmic stability is an established tool for analyzing generalization.\nWhile adversarial training enhances model robustness, it often suffers from\nrobust overfitting and an enlarged generalization gap. Although recent work has\nestablished the convergence of adversarial training in decentralized networks,\nits generalization properties remain unexplored. This work presents a\nstability-based generalization analysis of adversarial training under the\ndiffusion strategy for convex losses. We derive a bound showing that the\ngeneralization error grows with both the adversarial perturbation strength and\nthe number of training steps, a finding consistent with single-agent case but\nnovel for decentralized settings. Numerical experiments on logistic regression\nvalidate these theoretical predictions.", "AI": {"tldr": "\u672c\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u57fa\u4e8e\u7b97\u6cd5\u7a33\u5b9a\u6027\u7684\u65b9\u6cd5\u6765\u5206\u6790\u53bb\u4e2d\u5fc3\u5316\u8bbe\u7f6e\u4e0b\u5bf9\u6297\u6027\u8bad\u7ec3\u7684\u6cdb\u5316\u6027\u80fd\u3002", "motivation": "\u5bf9\u6297\u6027\u8bad\u7ec3\u867d\u7136\u80fd\u63d0\u9ad8\u6a21\u578b\u7684\u9c81\u68d2\u6027\uff0c\u4f46\u4f1a\u5e26\u6765\u9c81\u68d2\u6027\u8fc7\u62df\u5408\u548c\u6cdb\u5316\u95f4\u9699\u589e\u5927\u7684\u95ee\u9898\u3002\u5c3d\u7ba1\u5df2\u6709\u7814\u7a76\u8bc1\u660e\u4e86\u5bf9\u6297\u6027\u8bad\u7ec3\u5728\u53bb\u4e2d\u5fc3\u5316\u7f51\u7edc\u4e2d\u7684\u6536\u655b\u6027\uff0c\u4f46\u5176\u6cdb\u5316\u6027\u80fd\u4ecd\u672a\u5f97\u5230\u63a2\u7d22\u3002", "method": "\u5229\u7528\u57fa\u4e8e\u7a33\u5b9a\u6027\u7684\u65b9\u6cd5\uff0c\u9488\u5bf9\u5177\u6709\u51f8\u635f\u5931\u51fd\u6570\u7684\u6269\u6563\u7b56\u7565\uff0c\u5bf9\u53bb\u4e2d\u5fc3\u5316\u8bbe\u7f6e\u4e0b\u7684\u5bf9\u6297\u6027\u8bad\u7ec3\u8fdb\u884c\u6cdb\u5316\u6027\u80fd\u5206\u6790\u3002", "result": "\u63a8\u5bfc\u51fa\u7684\u754c\u9650\u8868\u660e\uff0c\u6cdb\u5316\u8bef\u5dee\u4f1a\u968f\u7740\u5bf9\u6297\u6027\u6270\u52a8\u5f3a\u5ea6\u548c\u8bad\u7ec3\u6b65\u6570\u7684\u589e\u52a0\u800c\u589e\u5927\uff0c\u8fd9\u4e0e\u5355\u667a\u80fd\u4f53\u60c5\u51b5\u4e00\u81f4\uff0c\u4f46\u5728\u53bb\u4e2d\u5fc3\u5316\u8bbe\u7f6e\u4e0b\u662f\u65b0\u7684\u53d1\u73b0\u3002", "conclusion": "\u7406\u8bba\u5206\u6790\u548c\u5728\u903b\u8f91\u56de\u5f52\u4e0a\u7684\u6570\u503c\u5b9e\u9a8c\u7ed3\u679c\u90fd\u8bc1\u5b9e\u4e86\u8be5\u6cdb\u5316\u8bef\u5dee\u754c\u9650\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2509.18631", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18631", "abs": "https://arxiv.org/abs/2509.18631", "authors": ["Shuo Cheng", "Liqian Ma", "Zhenyang Chen", "Ajay Mandlekar", "Caelan Garrett", "Danfei Xu"], "title": "Generalizable Domain Adaptation for Sim-and-Real Policy Co-Training", "comment": null, "summary": "Behavior cloning has shown promise for robot manipulation, but real-world\ndemonstrations are costly to acquire at scale. While simulated data offers a\nscalable alternative, particularly with advances in automated demonstration\ngeneration, transferring policies to the real world is hampered by various\nsimulation and real domain gaps. In this work, we propose a unified\nsim-and-real co-training framework for learning generalizable manipulation\npolicies that primarily leverages simulation and only requires a few real-world\ndemonstrations. Central to our approach is learning a domain-invariant,\ntask-relevant feature space. Our key insight is that aligning the joint\ndistributions of observations and their corresponding actions across domains\nprovides a richer signal than aligning observations (marginals) alone. We\nachieve this by embedding an Optimal Transport (OT)-inspired loss within the\nco-training framework, and extend this to an Unbalanced OT framework to handle\nthe imbalance between abundant simulation data and limited real-world examples.\nWe validate our method on challenging manipulation tasks, showing it can\nleverage abundant simulation data to achieve up to a 30% improvement in the\nreal-world success rate and even generalize to scenarios seen only in\nsimulation.", "AI": {"tldr": "\u4f7f\u7528\u7edf\u4e00\u7684sim-and-real\u534f\u540c\u8bad\u7ec3\u6846\u67b6\uff0c\u7ed3\u5408\u6700\u4f18\u4f20\u8f93\uff08OT\uff09\u635f\u5931\uff0c\u5229\u7528\u6a21\u62df\u6570\u636e\u548c\u5c11\u91cf\u771f\u5b9e\u4e16\u754c\u6f14\u793a\u6765\u5b66\u4e60\u53ef\u6cdb\u5316\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe30%\u7684\u771f\u5b9e\u4e16\u754c\u6210\u529f\u7387\u63d0\u5347\uff0c\u5e76\u80fd\u6cdb\u5316\u5230\u4ec5\u5728\u6a21\u62df\u4e2d seen \u7684\u573a\u666f\u3002", "motivation": "\u771f\u5b9e\u4e16\u754c\u6f14\u793a\u6570\u636e\u83b7\u53d6\u6210\u672c\u9ad8\uff0c\u800c\u6a21\u62df\u6570\u636e\u867d\u7136\u53ef\u6269\u5c55\u4f46\u5b58\u5728\u57df\u95f4\u5dee\u5f02\uff0c\u963b\u788d\u4e86\u7b56\u7565\u7684\u8fc1\u79fb\u3002\u672c\u7814\u7a76\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u80fd\u591f\u4e3b\u8981\u5229\u7528\u6a21\u62df\u6570\u636e\u5e76\u4ec5\u9700\u5c11\u91cf\u771f\u5b9e\u4e16\u754c\u6f14\u793a\u7684\u6cdb\u5316\u64cd\u4f5c\u7b56\u7565\u5b66\u4e60\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u7684sim-and-real\u534f\u540c\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u5b66\u4e60\u9886\u57df\u4e0d\u53d8\u3001\u4efb\u52a1\u76f8\u5173\u7684\u7279\u5f81\u7a7a\u95f4\u6765\u5f25\u5408\u6a21\u62df\u4e0e\u771f\u5b9e\u4e16\u754c\u7684\u5dee\u8ddd\u3002\u5173\u952e\u5728\u4e8e\u901a\u8fc7\u6700\u4f18\u4f20\u8f93\uff08OT\uff09\u635f\u5931\u6765\u5bf9\u9f50\u8de8\u9886\u57df\u7684\u89c2\u6d4b\u548c\u52a8\u4f5c\u7684\u8054\u5408\u5206\u5e03\uff0c\u5e76\u6269\u5c55\u5230\u975e\u5747\u8861OT\u4ee5\u5904\u7406\u6a21\u62df\u6570\u636e\u4e30\u5bcc\u800c\u771f\u5b9e\u6570\u636e\u6709\u9650\u7684\u95ee\u9898\u3002", "result": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\uff0c\u76f8\u6bd4\u4ec5\u4f7f\u7528\u6a21\u62df\u6570\u636e\u6216\u771f\u5b9e\u6570\u636e\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u5229\u7528\u4e30\u5bcc\u7684\u6a21\u62df\u6570\u636e\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u6210\u529f\u7387\u65b9\u9762\u63d0\u9ad8\u4e8630%\uff0c\u5e76\u4e14\u80fd\u591f\u6cdb\u5316\u5230\u4ec5\u5728\u6a21\u62df\u73af\u5883\u4e2d seen \u7684\u573a\u666f\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u7edf\u4e00sim-and-real\u534f\u540c\u8bad\u7ec3\u6846\u67b6\uff0c\u7ed3\u5408\u4e86OT\u635f\u5931\uff0c\u80fd\u591f\u6709\u6548\u5730\u5229\u7528\u6a21\u62df\u6570\u636e\u548c\u5c11\u91cf\u771f\u5b9e\u4e16\u754c\u6f14\u793a\u6765\u5b66\u4e60\u53ef\u6cdb\u5316\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2509.18354", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV", "62H35, 68T07, 62M40, 68T45", "I.2.6; I.2.10; I.4.6; I.4.8; I.5.1; I.5.4"], "pdf": "https://arxiv.org/pdf/2509.18354", "abs": "https://arxiv.org/abs/2509.18354", "authors": ["Mehrdad Moradi", "Shengzhe Chen", "Hao Yan", "Kamran Paynabar"], "title": "A Single Image Is All You Need: Zero-Shot Anomaly Localization Without Training Data", "comment": "12 pages, 10 figures, 1 table. Preprint submitted to a CVF conference", "summary": "Anomaly detection in images is typically addressed by learning from\ncollections of training data or relying on reference samples. In many\nreal-world scenarios, however, such training data may be unavailable, and only\nthe test image itself is provided. We address this zero-shot setting by\nproposing a single-image anomaly localization method that leverages the\ninductive bias of convolutional neural networks, inspired by Deep Image Prior\n(DIP). Our method is named Single Shot Decomposition Network (SSDnet). Our key\nassumption is that natural images often exhibit unified textures and patterns,\nand that anomalies manifest as localized deviations from these repetitive or\nstochastic patterns. To learn the deep image prior, we design a patch-based\ntraining framework where the input image is fed directly into the network for\nself-reconstruction, rather than mapping random noise to the image as done in\nDIP. To avoid the model simply learning an identity mapping, we apply masking,\npatch shuffling, and small Gaussian noise. In addition, we use a perceptual\nloss based on inner-product similarity to capture structure beyond pixel\nfidelity. Our approach needs no external training data, labels, or references,\nand remains robust in the presence of noise or missing pixels. SSDnet achieves\n0.99 AUROC and 0.60 AUPRC on MVTec-AD and 0.98 AUROC and 0.67 AUPRC on the\nfabric dataset, outperforming state-of-the-art methods. The implementation code\nwill be released at https://github.com/mehrdadmoradi124/SSDnet", "AI": {"tldr": "\u8be5\u65b9\u6cd5\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSSDnet\u7684\u5355\u5f20\u56fe\u50cf\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5229\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u5f52\u7eb3\u504f\u7f6e\uff0c\u5728\u65e0\u8bad\u7ec3\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u5f02\u5e38\u5b9a\u4f4d\u3002", "motivation": "\u5728\u8bb8\u591a\u771f\u5b9e\u573a\u666f\u4e2d\uff0c\u56fe\u50cf\u5f02\u5e38\u68c0\u6d4b\u7f3a\u4e4f\u8bad\u7ec3\u6570\u636e\u6216\u53c2\u8003\u6837\u672c\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u4ec5\u4f9d\u8d56\u6d4b\u8bd5\u56fe\u50cf\u672c\u8eab\u7684\u5355\u5f20\u56fe\u50cf\u5f02\u5e38\u5b9a\u4f4d\u65b9\u6cd5\u3002", "method": "SSDnet\u5229\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u5f52\u7eb3\u504f\u7f6e\uff0c\u901a\u8fc7\u57fa\u4e8e\u5757\u7684\u8bad\u7ec3\u6846\u67b6\u76f4\u63a5\u5c06\u8f93\u5165\u56fe\u50cf\u8f93\u5165\u7f51\u7edc\u8fdb\u884c\u81ea\u91cd\u6784\uff0c\u5e76\u91c7\u7528\u63a9\u7801\u3001\u5757\u6253\u4e71\u548c\u9ad8\u65af\u566a\u58f0\u6765\u907f\u514d\u6052\u7b49\u6620\u5c04\uff0c\u540c\u65f6\u4f7f\u7528\u57fa\u4e8e\u5185\u79ef\u76f8\u4f3c\u5ea6\u7684\u611f\u77e5\u635f\u5931\u6765\u6355\u6349\u7ed3\u6784\u4fe1\u606f\u3002", "result": "SSDnet\u5728MVTec-AD\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e860.99\u7684AUROC\u548c0.60\u7684AUPRC\uff0c\u5728fabric\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e860.98\u7684AUROC\u548c0.67\u7684AUPRC\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "SSDnet\u662f\u4e00\u79cd\u65e0\u9700\u5916\u90e8\u8bad\u7ec3\u6570\u636e\u3001\u6807\u7b7e\u6216\u53c2\u8003\uff0c\u4e14\u9c81\u68d2\u6027\u5f3a\u7684\u5355\u5f20\u56fe\u50cf\u5f02\u5e38\u5b9a\u4f4d\u65b9\u6cd5\u3002"}}
{"id": "2509.18586", "categories": ["quant-ph", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.18586", "abs": "https://arxiv.org/abs/2509.18586", "authors": ["Joseph Carolan"], "title": "Compressed Permutation Oracles", "comment": "80 pages, 3 figures", "summary": "The analysis of quantum algorithms which query random, invertible\npermutations has been a long-standing challenge in cryptography. Many\ntechniques which apply to random oracles fail, or are not known to generalize\nto this setting. As a result, foundational cryptographic constructions\ninvolving permutations often lack quantum security proofs. With the aim of\nclosing this gap, we develop and prove soundness of a compressed permutation\noracle. Our construction shares many of the attractive features of Zhandry's\noriginal compressed function oracle: the purification is a small list of\ninput-output pairs which meaningfully reflect an algorithm's knowledge of the\noracle.\n  We then apply this framework to show that the Feistel construction with seven\nrounds is a strong quantum PRP, resolving an open question of (Zhandry, 2012).\nWe further re-prove essentially all known quantum query lower bounds in the\nrandom permutation model, notably the collision and preimage resistance of both\nSponge and Davies-Meyer, hardness of double-sided zero search and sparse\npredicate search, and give new lower bounds for cycle finding and the one-more\nproblem.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5f00\u53d1\u4e86\u4e00\u79cd\u538b\u7f29\u7f6e\u6362\u9884\u8a00\u673a\uff0c\u7528\u4e8e\u5206\u6790\u91cf\u5b50\u7b97\u6cd5\u5728\u67e5\u8be2\u968f\u673a\u3001\u53ef\u9006\u7f6e\u6362\u65f6\u7684\u5bc6\u7801\u5b66\u95ee\u9898\uff0c\u5e76\u89e3\u51b3\u4e86 Feistel \u7f51\u7edc\u548c\u591a\u79cd\u5bc6\u7801\u5b66\u539f\u8bed\u7684\u91cf\u5b50\u5b89\u5168\u6027\u8bc1\u660e\u95ee\u9898\u3002", "motivation": "\u8bb8\u591a\u73b0\u6709\u7684\u5bc6\u7801\u5b66\u6280\u672f\u5728\u5e94\u7528\u4e8e\u968f\u673a\u9884\u8a00\u673a\u65f6\u4f1a\u5931\u6548\u6216\u65e0\u6cd5\u63a8\u5e7f\u5230\u968f\u673a\u7f6e\u6362\uff0c\u5bfc\u81f4\u57fa\u4e8e\u7f6e\u6362\u7684\u5bc6\u7801\u5b66\u7ed3\u6784\u7f3a\u4e4f\u91cf\u5b50\u5b89\u5168\u8bc1\u660e\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f25\u8865\u8fd9\u4e00\u5dee\u8ddd\u3002", "method": "\u63d0\u51fa\u5e76\u8bc1\u660e\u4e86\u4e00\u79cd\u538b\u7f29\u7f6e\u6362\u9884\u8a00\u673a\u7684\u5065\u5168\u6027\uff0c\u8be5\u9884\u8a00\u673a\u5177\u6709\u7c7b\u4f3c Zhandry \u538b\u7f29\u51fd\u6570\u9884\u8a00\u673a\u7684\u7279\u6027\uff0c\u80fd\u591f\u901a\u8fc7\u5c11\u91cf\u8f93\u5165-\u8f93\u51fa\u6765\u53cd\u6620\u7b97\u6cd5\u5bf9\u9884\u8a00\u673a\u7684\u4e86\u89e3\u7a0b\u5ea6\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u8bc1\u660e\u4e86\u4e03\u8f6e Feistel \u7ed3\u6784\u662f\u5f3a\u91cf\u5b50\u4f2a\u968f\u673a\u6392\u5217\uff08PRP\uff09\uff0c\u5e76\u91cd\u65b0\u8bc1\u660e\u4e86 Sponge \u548c Davies-Meyer \u7ed3\u6784\u5728\u78b0\u649e\u548c\u539f\u50cf\u62b5\u6297\u6027\u65b9\u9762\u7684\u91cf\u5b50\u67e5\u8be2\u4e0b\u754c\uff0c\u4ee5\u53ca\u53cc\u5411\u96f6\u641c\u7d22\u3001\u7a00\u758f\u8c13\u8bcd\u641c\u7d22\u7684\u56f0\u96be\u6027\uff0c\u5e76\u7ed9\u51fa\u4e86\u5faa\u73af\u67e5\u627e\u548c one-more \u95ee\u9898\u7684\u65b0\u4e0b\u754c\u3002", "result": "\u6210\u529f\u5f00\u53d1\u4e86\u538b\u7f29\u7f6e\u6362\u9884\u8a00\u673a\uff0c\u5e76\u8bc1\u660e\u4e86\u4e03\u8f6e Feistel \u7ed3\u6784\u662f\u5f3a\u91cf\u5b50 PRP\u3002\u6b64\u5916\uff0c\u8fd8\u91cd\u65b0\u8bc1\u660e\u4e86\u591a\u79cd\u5bc6\u7801\u5b66\u539f\u8bed\uff08\u5982 Sponge\u3001Davies-Meyer\uff09\u5728\u78b0\u649e\u548c\u539f\u50cf\u62b5\u6297\u6027\u65b9\u9762\u7684\u5df2\u77e5\u91cf\u5b50\u67e5\u8be2\u4e0b\u754c\uff0c\u5e76\u4e3a\u5faa\u73af\u67e5\u627e\u548c one-more \u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u4e0b\u754c\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u89e3\u51b3\u5bc6\u7801\u5b66\u4e2d\u91cf\u5b50\u7b97\u6cd5\u5206\u6790\u7684\u6311\u6218\u63d0\u4f9b\u4e86\u65b0\u7684\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u5e94\u7528\u8be5\u6846\u67b6\u8bc1\u660e\u4e86 Feistel \u7ed3\u6784\u548c\u591a\u79cd\u5bc6\u7801\u5b66\u539f\u8bed\u7684\u91cf\u5b50\u5b89\u5168\u6027\u3002"}}
{"id": "2509.18128", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18128", "abs": "https://arxiv.org/abs/2509.18128", "authors": ["Amirreza Tootchi", "Xiaoping Du"], "title": "Accounting for Uncertainty in Machine Learning Surrogates: A Gauss-Hermite Quadrature Approach to Reliability Analysis", "comment": null, "summary": "Machine learning surrogates are increasingly employed to replace expensive\ncomputational models for physics-based reliability analysis. However, their use\nintroduces epistemic uncertainty from model approximation errors, which couples\nwith aleatory uncertainty in model inputs, potentially compromising the\naccuracy of reliability predictions. This study proposes a Gauss-Hermite\nquadrature approach to decouple these nested uncertainties and enable more\naccurate reliability analysis. The method evaluates conditional failure\nprobabilities under aleatory uncertainty using First and Second Order\nReliability Methods and then integrates these probabilities across realizations\nof epistemic uncertainty. Three examples demonstrate that the proposed approach\nmaintains computational efficiency while yielding more trustworthy predictions\nthan traditional methods that ignore model uncertainty.", "AI": {"tldr": "\u673a\u5668\u5b66\u4e60\u66ff\u4ee3\u6a21\u578b\u4f1a\u5f15\u5165\u989d\u5916\u7684\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u65af-\u5384\u7c73\u7279\u6c42\u79ef\u65b9\u6cd5\u6765\u89e3\u8026\u8fd9\u79cd\u4e0d\u786e\u5b9a\u6027\uff0c\u4ee5\u63d0\u9ad8\u53ef\u9760\u6027\u5206\u6790\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u66ff\u4ee3\u6a21\u578b\u5728\u7269\u7406\u53ef\u9760\u6027\u5206\u6790\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5176\u5f15\u5165\u7684\u4f30\u7b97\u4e0d\u786e\u5b9a\u6027\u4f1a\u4e0e\u8f93\u5165\u53c2\u6570\u7684\u4e0d\u786e\u5b9a\u6027\u8026\u5408\uff0c\u5f71\u54cd\u9884\u6d4b\u7cbe\u5ea6\u3002", "method": "\u91c7\u7528\u9ad8\u65af-\u5384\u7c73\u7279\u6c42\u79ef\u65b9\u6cd5\uff0c\u9996\u5148\u4f7f\u7528\u4e00\u9636\u548c\u4e8c\u9636\u53ef\u9760\u6027\u65b9\u6cd5\u8bc4\u4f30\u6761\u4ef6\u5931\u6548\u6982\u7387\uff0c\u7136\u540e\u5bf9\u4f30\u7b97\u4e0d\u786e\u5b9a\u6027\u8fdb\u884c\u79ef\u5206\u3002", "result": "\u901a\u8fc7\u4e09\u4e2a\u7b97\u4f8b\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\uff0c\u6bd4\u5ffd\u7565\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u7684\u4f20\u7edf\u65b9\u6cd5\u80fd\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u9884\u6d4b\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u89e3\u8026\u4e0d\u786e\u5b9a\u6027\u7684\u65b9\u6cd5\u80fd\u591f\u66f4\u51c6\u786e\u5730\u8fdb\u884c\u53ef\u9760\u6027\u5206\u6790\u3002"}}
{"id": "2509.18436", "categories": ["cs.AI", "cs.CL", "cs.DB"], "pdf": "https://arxiv.org/pdf/2509.18436", "abs": "https://arxiv.org/abs/2509.18436", "authors": ["Hongda Jiang", "Xinyuan Zhang", "Siddhant Garg", "Rishab Arora", "Shiun-Zu Kuo", "Jiayang Xu", "Christopher Brossman", "Yue Liu", "Aaron Colak", "Ahmed Aly", "Anuj Kumar", "Xin Luna Dong"], "title": "Memory-QA: Answering Recall Questions Based on Multimodal Memories", "comment": null, "summary": "We introduce Memory-QA, a novel real-world task that involves answering\nrecall questions about visual content from previously stored multimodal\nmemories. This task poses unique challenges, including the creation of\ntask-oriented memories, the effective utilization of temporal and location\ninformation within memories, and the ability to draw upon multiple memories to\nanswer a recall question. To address these challenges, we propose a\ncomprehensive pipeline, Pensieve, integrating memory-specific augmentation,\ntime- and location-aware multi-signal retrieval, and multi-memory QA\nfine-tuning. We created a multimodal benchmark to illustrate various real\nchallenges in this task, and show the superior performance of Pensieve over\nstate-of-the-art solutions (up to 14% on QA accuracy).", "AI": {"tldr": "Memory-QA\u662f\u4e00\u4e2a\u65b0\u7684\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\uff0c\u9700\u8981\u4ece\u5148\u524d\u7684\u591a\u6a21\u6001\u8bb0\u5fc6\u4e2d\u56de\u7b54\u5173\u4e8e\u89c6\u89c9\u5185\u5bb9\u7684\u95ee\u9898\u3002Pensieve\u662f\u4e00\u4e2a\u96c6\u6210\u8bb0\u5fc6\u589e\u5f3a\u3001\u65f6\u7a7a\u611f\u77e5\u68c0\u7d22\u548c\u591a\u8bb0\u5fc6\u95ee\u7b54\u5fae\u8c03\u7684\u6d41\u6c34\u7ebf\uff0c\u65e8\u5728\u89e3\u51b3\u6b64\u4efb\u52a1\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u6280\u672f\u5728\u5904\u7406\u9700\u8981\u4ece\u5148\u524d\u5b58\u50a8\u7684\u591a\u6a21\u6001\u8bb0\u5fc6\u4e2d\u56de\u5fc6\u89c6\u89c9\u5185\u5bb9\u7684\u95ee\u9898\u65f6\u5b58\u5728\u4e0d\u8db3\uff0c\u8be5\u4efb\u52a1\u5177\u6709\u8bb0\u5fc6\u521b\u5efa\u3001\u65f6\u7a7a\u4fe1\u606f\u5229\u7528\u548c\u591a\u8bb0\u5fc6\u6574\u5408\u7b49\u72ec\u7279\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPensieve\u7684\u6d41\u6c34\u7ebf\uff0c\u5305\u62ec\u8bb0\u5fc6\u589e\u5f3a\u3001\u65f6\u7a7a\u611f\u77e5\u591a\u4fe1\u53f7\u68c0\u7d22\u548c\u591a\u8bb0\u5fc6\u95ee\u7b54\u5fae\u8c03\u3002", "result": "\u5728\u521b\u5efa\u7684\u591a\u6a21\u6001\u57fa\u51c6\u4e0a\uff0cPensieve\u7684\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0cQA\u51c6\u786e\u7387\u63d0\u9ad8\u4e8614%\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684Memory-QA\u4efb\u52a1\u548cPensieve\u6d41\u6c34\u7ebf\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u4ece\u591a\u6a21\u6001\u8bb0\u5fc6\u4e2d\u8fdb\u884c\u89c6\u89c9\u95ee\u7b54\u7684\u6311\u6218\uff0c\u5e76\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u7684\u6027\u80fd\u3002"}}
{"id": "2509.18536", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18536", "abs": "https://arxiv.org/abs/2509.18536", "authors": ["Jin Young Kim", "Ji Won Yoon"], "title": "CCQA: Generating Question from Solution Can Improve Inference-Time Reasoning in SLMs", "comment": "Published as a main conference paper at EMNLP 2025", "summary": "Recently, inference-time reasoning strategies have further improved the\naccuracy of large language models (LLMs), but their effectiveness on smaller\nmodels remains unclear. Based on the observation that conventional approaches\noften fail to improve performance in this context, we propose\n\\textbf{C}ycle-\\textbf{C}onsistency in \\textbf{Q}uestion \\textbf{A}nswering\n(CCQA), a novel reasoning method that can be effectively applied to SLMs.\nInspired by cycle consistency, CCQA generates a question from each reasoning\npath and answer, evaluates each by its similarity to the original question, and\nthen selects the candidate solution with the highest similarity score as the\nfinal response. Since conventional SLMs struggle to generate accurate questions\nfrom their own reasoning paths and answers, we employ a lightweight Flan-T5\nmodel specialized for question generation to support this process efficiently.\nFrom the experimental results, it is verified that CCQA consistently\noutperforms existing state-of-the-art (SOTA) methods across eight models on\nmathematical and commonsense reasoning benchmarks. Furthermore, our method\nestablishes a new practical baseline for efficient reasoning in SLMs. Source\ncode can be found at https://github.com/scai-research/ccqa_official.", "AI": {"tldr": "CCQA\u662f\u4e00\u79cd\u65b0\u9896\u7684\u63a8\u7406\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u95ee\u9898-\u7b54\u6848\u5bf9\u5e76\u8bc4\u4f30\u5176\u4e0e\u539f\u59cb\u95ee\u9898\u7684\u76f8\u4f3c\u5ea6\u6765\u63d0\u9ad8\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09\u7684\u51c6\u786e\u6027\u3002\u5b83\u4f7f\u7528\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684Flan-T5\u6a21\u578b\u6765\u751f\u6210\u95ee\u9898\uff0c\u5e76\u5728\u6570\u5b66\u548c\u5e38\u8bc6\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u4e86\u73b0\u6709\u7684SOTA\u65b9\u6cd5\uff0c\u4e3aSLMs\u5efa\u7acb\u4e86\u65b0\u7684\u9ad8\u6548\u63a8\u7406\u57fa\u7ebf\u3002", "motivation": "\u5c3d\u7ba1\u63a8\u7406\u65f6\u95f4\u63a8\u7406\u7b56\u7565\u63d0\u9ad8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u51c6\u786e\u6027\uff0c\u4f46\u5b83\u4eec\u5728\u5c0f\u578b\u6a21\u578b\u4e0a\u7684\u6548\u679c\u5c1a\u4e0d\u6e05\u695a\u3002\u4f20\u7edf\u65b9\u6cd5\u5728\u6b64\u80cc\u666f\u4e0b\u901a\u5e38\u65e0\u6cd5\u63d0\u9ad8\u6027\u80fd\u3002", "method": "CCQA\u65b9\u6cd5\u751f\u6210\u6bcf\u4e2a\u63a8\u7406\u8def\u5f84\u548c\u7b54\u6848\u7684\u95ee\u9898\uff0c\u8bc4\u4f30\u5176\u4e0e\u539f\u59cb\u95ee\u9898\u7684\u76f8\u4f3c\u5ea6\uff0c\u5e76\u9009\u62e9\u76f8\u4f3c\u5ea6\u5f97\u5206\u6700\u9ad8\u7684\u5019\u9009\u89e3\u51b3\u65b9\u6848\u4f5c\u4e3a\u6700\u7ec8\u54cd\u5e94\u3002\u4e3a\u652f\u6301\u95ee\u9898\u751f\u6210\uff0c\u91c7\u7528\u4e86\u8f7b\u91cf\u7ea7Flan-T5\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cCCQA\u5728\u6570\u5b66\u548c\u5e38\u8bc6\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u7684\u516b\u4e2a\u6a21\u578b\u4e0a\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u7684SOTA\u65b9\u6cd5\uff0c\u5e76\u4e3aSLMs\u5efa\u7acb\u4e86\u65b0\u7684\u5b9e\u7528\u9ad8\u6548\u63a8\u7406\u57fa\u7ebf\u3002", "conclusion": "CCQA\u662f\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u63d0\u9ad8\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u51c6\u786e\u6027\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u7814\u7a76\u8bbe\u5b9a\u4e86\u65b0\u7684\u57fa\u51c6\u3002"}}
{"id": "2509.18676", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.18676", "abs": "https://arxiv.org/abs/2509.18676", "authors": ["Sangjun Noh", "Dongwoo Nam", "Kangmin Kim", "Geonhyup Lee", "Yeonguk Yu", "Raeyoung Kang", "Kyoobin Lee"], "title": "3D Flow Diffusion Policy: Visuomotor Policy Learning via Generating Flow in 3D Space", "comment": "7 main scripts + 2 reference pages", "summary": "Learning robust visuomotor policies that generalize across diverse objects\nand interaction dynamics remains a central challenge in robotic manipulation.\nMost existing approaches rely on direct observation-to-action mappings or\ncompress perceptual inputs into global or object-centric features, which often\noverlook localized motion cues critical for precise and contact-rich\nmanipulation. We present 3D Flow Diffusion Policy (3D FDP), a novel framework\nthat leverages scene-level 3D flow as a structured intermediate representation\nto capture fine-grained local motion cues. Our approach predicts the temporal\ntrajectories of sampled query points and conditions action generation on these\ninteraction-aware flows, implemented jointly within a unified diffusion\narchitecture. This design grounds manipulation in localized dynamics while\nenabling the policy to reason about broader scene-level consequences of\nactions. Extensive experiments on the MetaWorld benchmark show that 3D FDP\nachieves state-of-the-art performance across 50 tasks, particularly excelling\non medium and hard settings. Beyond simulation, we validate our method on eight\nreal-robot tasks, where it consistently outperforms prior baselines in\ncontact-rich and non-prehensile scenarios. These results highlight 3D flow as a\npowerful structural prior for learning generalizable visuomotor policies,\nsupporting the development of more robust and versatile robotic manipulation.\nRobot demonstrations, additional results, and code can be found at\nhttps://sites.google.com/view/3dfdp/home.", "AI": {"tldr": "3D FDP \u4f7f\u7528 3D \u6d41\u4f5c\u4e3a\u4e2d\u95f4\u8868\u793a\u6765\u5b66\u4e60\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\uff0c\u5728\u6a21\u62df\u548c\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u5747\u53d6\u5f97\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u5b66\u4e60\u80fd\u591f\u8de8\u4e0d\u540c\u7269\u4f53\u548c\u4ea4\u4e92\u52a8\u529b\u5b66\u6cdb\u5316\u7684\u9c81\u68d2\u6027\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\uff0c\u56e0\u4e3a\u73b0\u6709\u65b9\u6cd5\u5e38\u5e38\u5ffd\u7565\u7cbe\u7ec6\u7684\u5c40\u90e8\u8fd0\u52a8\u7ebf\u7d22\u3002", "method": "\u63d0\u51fa 3D \u6d41\u6269\u6563\u7b56\u7565 (3D FDP) \u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u573a\u666f\u7ea7 3D \u6d41\u4f5c\u4e3a\u7ed3\u6784\u5316\u4e2d\u95f4\u8868\u793a\u6765\u6355\u6349\u7cbe\u7ec6\u7684\u5c40\u90e8\u8fd0\u52a8\u7ebf\u7d22\u3002\u8be5\u65b9\u6cd5\u9884\u6d4b\u91c7\u6837\u67e5\u8be2\u70b9\u7684\u65f6\u57df\u8f68\u8ff9\uff0c\u5e76\u5c06\u52a8\u4f5c\u751f\u6210\u4e0e\u8fd9\u4e9b\u4ea4\u4e92\u611f\u77e5\u6d41\u8fdb\u884c\u6761\u4ef6\u5316\uff0c\u8fd9\u4e9b\u6d41\u5728\u7edf\u4e00\u7684\u6269\u6563\u67b6\u6784\u4e2d\u8054\u5408\u5b9e\u73b0\u3002", "result": "\u5728 MetaWorld \u57fa\u51c6\u7684 50 \u4e2a\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u4e2d\u7b49\u548c\u56f0\u96be\u8bbe\u7f6e\u4e0b\u8868\u73b0\u51fa\u8272\u3002\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u7684\u516b\u4e2a\u4efb\u52a1\u4e2d\uff0c\u59cb\u7ec8\u4f18\u4e8e\u5148\u524d\u7684\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u5728\u63a5\u89e6\u4e30\u5bcc\u548c\u975e\u6293\u53d6\u573a\u666f\u4e2d\u3002", "conclusion": "3D \u6d41\u662f\u4e00\u79cd\u5f3a\u5927\u7684\u7ed3\u6784\u5316\u5148\u9a8c\uff0c\u53ef\u7528\u4e8e\u5b66\u4e60\u53ef\u6cdb\u5316\u7684\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\uff0c\u4ece\u800c\u4fc3\u8fdb\u66f4\u9c81\u68d2\u3001\u66f4\u591a\u529f\u80fd\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u53d1\u5c55\u3002"}}
{"id": "2509.18951", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2509.18951", "abs": "https://arxiv.org/abs/2509.18951", "authors": ["Amiya Chowdhury", "Acacio Rinc\u00f3n Romero", "Eduardo Aguilar-Bejarano", "Halar Memon", "Grazziela Figueredo", "Tanvir Hussain"], "title": "A Methodological Study on Data Representation for Machine Learning Modelling of Thermal Conductivity of Rare-Earth Oxides", "comment": null, "summary": "Quantitative structure-activity relationship (QSAR) modelling is widely\nemployed in materials sci- ence to predict properties of interest and extract\nuseful descriptors for measured properties. In thermal barrier coatings (TBC),\nQSAR can significantly shorten the experimental discovery cycle, which can take\nyears. Although machine learning methods are commonly employed for QSAR, their\nperformance depends on the data quality and how instances are represented.\nTraditional, hand-crafted descriptors based on known material properties are\nlimited to represent materials that share the same basic crystal structure,\nlimited the size of the dataset. By contrast, graph neural networks offer a\nmore expressive representation, encoding atomic positions and bonds in the\ncrystal lattice. In this study, we compare Random Forest (RF) and Gaussian\nProcess (GP) models trained on hand-crafted descriptors from the literature\nwith graph-based representations for high-entropy, rare-earth pyrochlore oxides\nusing the Crystal Graph Convolutional Neural Network (CGCNN). Two different\ntypes of augmentation methods are also explored to account for the limited data\nsize, one of which is only applicable to graph-based representations. Our\nfindings show that the CGCNN model substantially outperforms the RF and GP\nmodels, underscoring the potential of graph-based representations for enhanced\nQSAR modelling in TBC research.", "AI": {"tldr": "\u56fe\u795e\u7ecf\u7f51\u7edc\u5728\u70ed\u969c\u6d82\u5c42\u6750\u6599\u7684QSAR\u5efa\u6a21\u4e2d\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u70ed\u969c\u6d82\u5c42\uff08TBC\uff09\u6750\u6599\u7684QSAR\u5efa\u6a21\u53ef\u4ee5\u7f29\u77ed\u5b9e\u9a8c\u5468\u671f\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u53d7\u9650\u4e8e\u6570\u636e\u8d28\u91cf\u548c\u6750\u6599\u8868\u793a\u3002\u56fe\u795e\u7ecf\u7f51\u7edc\u53ef\u4ee5\u63d0\u4f9b\u66f4\u4e30\u5bcc\u7684\u6750\u6599\u8868\u793a\uff0c\u5e76\u53ef\u80fd\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u3002", "method": "\u6bd4\u8f83\u4e86\u57fa\u4e8e\u624b\u5de5\u63cf\u8ff0\u7b26\u7684\u968f\u673a\u68ee\u6797\uff08RF\uff09\u548c\u9ad8\u65af\u8fc7\u7a0b\uff08GP\uff09\u6a21\u578b\u4e0e\u57fa\u4e8e\u56fe\u8868\u793a\uff08\u4f7f\u7528\u6676\u4f53\u56fe\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff0cCGCNN\uff09\u7684\u6a21\u578b\u5728\u70ed\u969c\u6d82\u5c42\u6750\u6599\u4e0a\u7684\u6027\u80fd\u3002\u540c\u65f6\u63a2\u7d22\u4e86\u4e24\u79cd\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u3002", "result": "CGCNN\u6a21\u578b\u5728QSAR\u5efa\u6a21\u4e2d\u8868\u73b0\u663e\u8457\u4f18\u4e8eRF\u548cGP\u6a21\u578b\u3002", "conclusion": "\u57fa\u4e8e\u56fe\u8868\u793a\u7684CGCNN\u6a21\u578b\u5728\u70ed\u969c\u6d82\u5c42\u6750\u6599\u7684QSAR\u5efa\u6a21\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2509.18636", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18636", "abs": "https://arxiv.org/abs/2509.18636", "authors": ["Yuan Zhou", "Jialiang Hou", "Guangtong Xu", "Fei Gao"], "title": "Number Adaptive Formation Flight Planning via Affine Deformable Guidance in Narrow Environments", "comment": null, "summary": "Formation maintenance with varying number of drones in narrow environments\nhinders the convergence of planning to the desired configurations. To address\nthis challenge, this paper proposes a formation planning method guided by\nDeformable Virtual Structures (DVS) with continuous spatiotemporal\ntransformation. Firstly, to satisfy swarm safety distance and preserve\nformation shape filling integrity for irregular formation geometries, we employ\nLloyd algorithm for uniform $\\underline{PA}$rtitioning and Hungarian algorithm\nfor $\\underline{AS}$signment (PAAS) in DVS. Subsequently, a spatiotemporal\ntrajectory involving DVS is planned using primitive-based path search and\nnonlinear trajectory optimization. The DVS trajectory achieves adaptive\ntransitions with respect to a varying number of drones while ensuring\nadaptability to narrow environments through affine transformation. Finally,\neach agent conducts distributed trajectory planning guided by desired\nspatiotemporal positions within the DVS, while incorporating collision\navoidance and dynamic feasibility requirements. Our method enables up to 15\\%\nof swarm numbers to join or leave in cluttered environments while rapidly\nrestoring the desired formation shape in simulation. Compared to cutting-edge\nformation planning method, we demonstrate rapid formation recovery capacity and\nenvironmental adaptability. Real-world experiments validate the effectiveness\nand resilience of our formation planning method.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u53d8\u5f62\u865a\u62df\u7ed3\u6784\uff08DVS\uff09\u5e76\u7ed3\u5408\u8fde\u7eed\u65f6\u7a7a\u53d8\u6362\u7684\u7f16\u961f\u89c4\u5212\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u5728\u72ed\u7a84\u73af\u5883\u4e2d\u65e0\u4eba\u673a\u6570\u91cf\u53d8\u5316\u65f6\u7f16\u961f\u7ef4\u6301\u548c\u89c4\u5212\u6536\u655b\u6027\u95ee\u9898\u3002", "motivation": "\u5728\u72ed\u7a84\u73af\u5883\u4e2d\uff0c\u65e0\u4eba\u673a\u6570\u91cf\u7684\u53d8\u5316\u4f1a\u963b\u788d\u89c4\u5212\u6536\u655b\u5230\u671f\u671b\u7684\u914d\u7f6e\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u9002\u5e94\u8fd9\u79cd\u53d8\u5316\u7684\u7f16\u961f\u89c4\u5212\u65b9\u6cd5\u3002", "method": "\u9996\u5148\uff0c\u5229\u7528 Lloyd \u7b97\u6cd5\u8fdb\u884c\u5747\u5300\u5212\u5206\uff08Partitioning\uff09\u548c Hungarian \u7b97\u6cd5\u8fdb\u884c\u5206\u914d\uff08Assignment\uff09\uff0c\u5373 PAAS\uff0c\u6765\u6ee1\u8db3\u65e0\u4eba\u673a\u95f4\u7684\u5b89\u5168\u8ddd\u79bb\u548c\u4fdd\u6301\u7f16\u961f\u5f62\u72b6\u7684\u5b8c\u6574\u6027\uff0c\u7279\u522b\u9002\u7528\u4e8e\u4e0d\u89c4\u5219\u7f16\u961f\u51e0\u4f55\u3002\u7136\u540e\uff0c\u4f7f\u7528\u57fa\u4e8e\u539f\u8bed\u7684\u8def\u5f84\u641c\u7d22\u548c\u975e\u7ebf\u6027\u8f68\u8ff9\u4f18\u5316\u6765\u89c4\u5212\u5305\u542b DVS \u7684\u65f6\u7a7a\u8f68\u8ff9\u3002\u8be5 DVS \u8f68\u8ff9\u80fd\u591f\u81ea\u9002\u5e94\u5730\u5904\u7406\u65e0\u4eba\u673a\u6570\u91cf\u7684\u53d8\u5316\uff0c\u5e76\u901a\u8fc7\u4eff\u5c04\u53d8\u6362\u9002\u5e94\u72ed\u7a84\u73af\u5883\u3002\u6700\u540e\uff0c\u6bcf\u4e2a\u65e0\u4eba\u673a\u5728 DVS \u63d0\u4f9b\u7684\u671f\u671b\u65f6\u7a7a\u4f4d\u7f6e\u7684\u6307\u5bfc\u4e0b\uff0c\u8fdb\u884c\u5206\u5e03\u5f0f\u8f68\u8ff9\u89c4\u5212\uff0c\u540c\u65f6\u8003\u8651\u78b0\u649e\u907f\u514d\u548c\u52a8\u6001\u53ef\u884c\u6027\u8981\u6c42\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u5728\u6a21\u62df\u4e2d\u5b9e\u73b0\u9ad8\u8fbe 15% \u7684\u65e0\u4eba\u673a\u6570\u91cf\u52a0\u5165\u6216\u79bb\u5f00\uff0c\u5e76\u5728\u6df7\u4e71\u7684\u73af\u5883\u4e2d\u5feb\u901f\u6062\u590d\u671f\u671b\u7684\u7f16\u961f\u5f62\u72b6\u3002\u4e0e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u7f16\u961f\u89c4\u5212\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5c55\u73b0\u4e86\u66f4\u5feb\u7684\u7f16\u961f\u6062\u590d\u80fd\u529b\u548c\u73af\u5883\u9002\u5e94\u6027\u3002\u5b9e\u9645\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8e DVS \u7684\u7f16\u961f\u89c4\u5212\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5730\u5904\u7406\u65e0\u4eba\u673a\u6570\u91cf\u53d8\u5316\u548c\u72ed\u7a84\u73af\u5883\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u5feb\u901f\u7684\u7f16\u961f\u6062\u590d\u548c\u826f\u597d\u7684\u73af\u5883\u9002\u5e94\u6027\u3002"}}
{"id": "2509.18369", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18369", "abs": "https://arxiv.org/abs/2509.18369", "authors": ["Riad Ahmed Anonto", "Sardar Md. Saffat Zabin", "M. Saifur Rahman"], "title": "Align Where the Words Look: Cross-Attention-Guided Patch Alignment with Contrastive and Transport Regularization for Bengali Captioning", "comment": null, "summary": "Grounding vision--language models in low-resource languages remains\nchallenging, as they often produce fluent text about the wrong objects. This\nstems from scarce paired data, translation pivots that break alignment, and\nEnglish-centric pretraining that ignores target-language semantics. We address\nthis with a compute-aware Bengali captioning pipeline trained on LaBSE-verified\nEN--BN pairs and 110k bilingual-prompted synthetic images. A frozen MaxViT\nyields stable visual patches, a Bengali-native mBART-50 decodes, and a\nlightweight bridge links the modalities. Our core novelty is a tri-loss\nobjective: Patch-Alignment Loss (PAL) aligns real and synthetic patch\ndescriptors using decoder cross-attention, InfoNCE enforces global\nreal--synthetic separation, and Sinkhorn-based OT ensures balanced fine-grained\npatch correspondence. This PAL+InfoNCE+OT synergy improves grounding, reduces\nspurious matches, and drives strong gains on Flickr30k-1k (BLEU-4 12.29, METEOR\n27.98, BERTScore-F1 71.20) and MSCOCO-1k (BLEU-4 12.00, METEOR 28.14,\nBERTScore-F1 75.40), outperforming strong CE baselines and narrowing the\nreal--synthetic centroid gap by 41%.", "AI": {"tldr": "\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\uff0c\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5e38\u5e38\u4f1a\u751f\u6210\u4e0e\u56fe\u50cf\u5185\u5bb9\u4e0d\u7b26\u7684\u6587\u672c\uff0c\u8fd9\u662f\u7531\u4e8e\u6570\u636e\u7a00\u758f\u3001\u7ffb\u8bd1\u67a2\u7ebd\u7834\u574f\u5bf9\u9f50\u4ee5\u53ca\u4ee5\u82f1\u8bed\u4e3a\u4e2d\u5fc3\u7684\u9884\u8bad\u7ec3\u5ffd\u7565\u4e86\u76ee\u6807\u8bed\u8a00\u8bed\u4e49\u6240\u81f4\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u8ba1\u7b97\u611f\u77e5\u7684\u5b5f\u52a0\u62c9\u8bed\u56fe\u50cf\u63cf\u8ff0\u751f\u6210\u6d41\u7a0b\uff0c\u5728 LaBSE \u9a8c\u8bc1\u7684\u82f1\u5b5f\u914d\u5bf9\u6570\u636e\u548c 110k \u53cc\u8bed\u63d0\u793a\u5408\u6210\u56fe\u50cf\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002", "motivation": "\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u56fe\u50cf\u63cf\u8ff0\u65b9\u9762\u5b58\u5728\u5bf9\u9f50\u95ee\u9898\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5305\u542b Patch-Alignment Loss (PAL)\u3001InfoNCE \u548c Sinkhorn-based OT \u7684\u4e09\u91cd\u635f\u5931\u76ee\u6807\u51fd\u6570\uff0c\u4ee5\u5bf9\u9f50\u771f\u5b9e\u548c\u5408\u6210\u56fe\u50cf\u7684\u6591\u5757\u63cf\u8ff0\u7b26\uff0c\u5e76\u786e\u4fdd\u7ec6\u7c92\u5ea6\u7684\u6591\u5757\u5bf9\u5e94\u5173\u7cfb\u3002\u6a21\u578b\u4f7f\u7528\u51bb\u7ed3\u7684 MaxViT \u63d0\u53d6\u89c6\u89c9\u7279\u5f81\uff0c\u5b5f\u52a0\u62c9\u8bed\u7684 mBART-50 \u8fdb\u884c\u89e3\u7801\uff0c\u5e76\u7528\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u6865\u6881\u8fde\u63a5\u89c6\u89c9\u548c\u8bed\u8a00\u6a21\u6001\u3002", "result": "\u8be5\u6a21\u578b\u5728 Flickr30k-1k \u548c MSCOCO-1k \u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6539\u8fdb\uff0cBLEU-4 \u5206\u6570\u5206\u522b\u4e3a 12.29 \u548c 12.00\uff0cMETEOR \u5206\u6570\u5206\u522b\u4e3a 27.98 \u548c 28.14\uff0cBERTScore-F1 \u5206\u6570\u5206\u522b\u4e3a 71.20 \u548c 75.40\uff0c\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u6a21\u578b\uff0c\u5e76\u5c06\u771f\u5b9e-\u5408\u6210\u8d28\u5fc3\u8ddd\u79bb\u7f29\u5c0f\u4e86 41%\u3002", "conclusion": "\u6211\u4eec\u63d0\u51fa\u7684 PAL+InfoNCE+OT \u534f\u540c\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u56fe\u50cf\u63cf\u8ff0\u4e2d\u7684\u5bf9\u9f50\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u6027\u80fd\u3002"}}
{"id": "2509.18130", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18130", "abs": "https://arxiv.org/abs/2509.18130", "authors": ["Zijie Zhou", "Huichen Ma"], "title": "Research on Metro Transportation Flow Prediction Based on the STL-GRU Combined Model", "comment": null, "summary": "In the metro intelligent transportation system, accurate transfer passenger\nflow prediction is a key link in optimizing operation plans and improving\ntransportation efficiency. To further improve the theory of metro internal\ntransfer passenger flow prediction and provide more reliable support for\nintelligent operation decisions, this paper innovatively proposes a metro\ntransfer passenger flow prediction model that integrates the Seasonal and Trend\ndecomposition using Loess (STL) method and Gated Recurrent Unit (GRU).In\npractical application, the model first relies on the deep learning library\nKeras to complete the construction and training of the GRU model, laying the\nfoundation for subsequent prediction; then preprocesses the original metro card\nswiping data, uses the graph-based depth-first search algorithm to identify\npassengers' travel paths, and further constructs the transfer passenger flow\ntime series; subsequently adopts the STL time series decomposition algorithm to\ndecompose the constructed transfer passenger flow time series into trend\ncomponent, periodic component and residual component, and uses the 3{\\sigma}\nprinciple to eliminate and fill the outliers in the residual component, and\nfinally completes the transfer passenger flow prediction.Taking the transfer\npassenger flow data of a certain metro station as the research sample, the\nvalidity of the model is verified. The results show that compared with Long\nShort-Term Memory (LSTM), Gated Recurrent Unit (GRU), and the combined model of\nSTL time series decomposition method and Long Short-Term Memory (STL-LSTM), the\nSTL-GRU combined prediction model significantly improves the prediction\naccuracy of transfer passenger flow on weekdays (excluding Fridays), Fridays\nand rest days, with the mean absolute percentage error (MAPE) of the prediction\nresults reduced by at least 2.3, 1.36 and 6.42 percentage points respectively.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408STL\u548cGRU\u7684\u5730\u94c1\u6362\u4e58\u5ba2\u6d41\u9884\u6d4b\u6a21\u578b\uff0c\u901a\u8fc7\u5206\u89e3\u65f6\u95f4\u5e8f\u5217\u5e76\u5229\u7528GRU\u8fdb\u884c\u9884\u6d4b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u9884\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u4e3a\u4e86\u4f18\u5316\u5730\u94c1\u8fd0\u8425\u8ba1\u5212\u548c\u63d0\u9ad8\u4ea4\u901a\u6548\u7387\uff0c\u9700\u8981\u7cbe\u786e\u9884\u6d4b\u5730\u94c1\u5185\u90e8\u6362\u4e58\u5ba2\u6d41\u3002", "method": "1. \u4f7f\u7528Keras\u6784\u5efa\u548c\u8bad\u7ec3GRU\u6a21\u578b\u3002 2. \u9884\u5904\u7406\u5730\u94c1\u5237\u5361\u6570\u636e\uff0c\u5229\u7528\u6df1\u5ea6\u4f18\u5148\u641c\u7d22\u7b97\u6cd5\u8bc6\u522b\u4e58\u5ba2\u8def\u5f84\uff0c\u6784\u5efa\u6362\u4e58\u5ba2\u6d41\u65f6\u95f4\u5e8f\u5217\u3002 3. \u91c7\u7528STL\u65f6\u95f4\u5e8f\u5217\u5206\u89e3\u7b97\u6cd5\u5c06\u5e8f\u5217\u5206\u89e3\u4e3a\u8d8b\u52bf\u3001\u5468\u671f\u548c\u6b8b\u5dee\u5206\u91cf\u3002 4. \u5229\u75283\u03c3\u539f\u5219\u5904\u7406\u6b8b\u5dee\u5206\u91cf\u4e2d\u7684\u5f02\u5e38\u503c\u3002 5. \u8fdb\u884c\u6362\u4e58\u5ba2\u6d41\u9884\u6d4b\u3002", "result": "\u4e0eLSTM\u3001GRU\u548cSTL-LSTM\u6a21\u578b\u76f8\u6bd4\uff0cSTL-GRU\u6a21\u578b\u5728\u5de5\u4f5c\u65e5\uff08\u975e\u5468\u4e94\uff09\u3001\u5468\u4e94\u548c\u4f11\u606f\u65e5\u7684\u9884\u6d4b\u51c6\u786e\u6027\u5747\u6709\u663e\u8457\u63d0\u5347\uff0c\u5e73\u5747\u7edd\u5bf9\u767e\u5206\u6bd4\u8bef\u5dee\uff08MAPE\uff09\u5206\u522b\u81f3\u5c11\u964d\u4f4e\u4e862.3%\u30011.36%\u548c6.42%\u3002", "conclusion": "STL-GRU\u7ec4\u5408\u9884\u6d4b\u6a21\u578b\u80fd\u591f\u6709\u6548\u63d0\u9ad8\u5730\u94c1\u6362\u4e58\u5ba2\u6d41\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u4e3a\u667a\u80fd\u8fd0\u8425\u51b3\u7b56\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u652f\u6301\u3002"}}
{"id": "2509.18527", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18527", "abs": "https://arxiv.org/abs/2509.18527", "authors": ["Ziwen Chen", "Zhong Wang"], "title": "FERA: Foil Fencing Referee Assistant Using Pose-Based Multi-Label Move Recognition and Rule Reasoning", "comment": null, "summary": "The sport of fencing, like many other sports, faces challenges in refereeing:\nsubjective calls, human errors, bias, and limited availability in practice\nenvironments. We present FERA (Fencing Referee Assistant), a prototype AI\nreferee for foil fencing which integrates pose-based multi-label action\nrecognition and rule-based reasoning. FERA extracts 2D joint positions from\nvideo, normalizes them, computes a 101-dimensional kinematic feature set, and\napplies a Transformer for multi-label move and blade classification. To\ndetermine priority and scoring, FERA applies a distilled language model with\nencoded right-of-way rules, producing both a decision and an explanation for\neach exchange. With limited hand-labeled data, a 5-fold cross-validation\nachieves an average macro-F1 score of 0.549, outperforming multiple baselines,\nincluding a Temporal Convolutional Network (TCN), BiLSTM, and a vanilla\nTransformer. While not ready for deployment, these results demonstrate a\npromising path towards automated referee assistance in foil fencing and new\nopportunities for AI applications, such as coaching in the field of fencing.", "AI": {"tldr": "FERA\u662f\u4e00\u4e2a\u4e3a\u51fb\u5251\u6bd4\u8d5b\u8bbe\u8ba1\u7684AI\u88c1\u5224\u539f\u578b\uff0c\u4f7f\u7528\u57fa\u4e8e\u59ff\u6001\u7684\u591a\u6807\u7b7e\u52a8\u4f5c\u8bc6\u522b\u548c\u89c4\u5219\u63a8\u7406\u6765\u89e3\u51b3\u4e3b\u89c2\u5224\u7f5a\u3001\u9519\u8bef\u548c\u53ef\u7528\u6027\u95ee\u9898\u3002", "motivation": "\u4f53\u80b2\u6bd4\u8d5b\uff08\u5982\u51fb\u5251\uff09\u9762\u4e34\u88c1\u5224\u4e2d\u7684\u4e3b\u89c2\u5224\u7f5a\u3001\u4eba\u4e3a\u9519\u8bef\u3001\u504f\u89c1\u4ee5\u53ca\u7ec3\u4e60\u73af\u5883\u4e2d\u53ef\u7528\u6027\u6709\u9650\u7b49\u6311\u6218\u3002", "method": "FERA\u63d0\u53d6\u89c6\u9891\u4e2d\u76842D\u5173\u8282\u4f4d\u7f6e\uff0c\u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u8ba1\u7b97101\u7ef4\u8fd0\u52a8\u7279\u5f81\uff0c\u5e76\u4f7f\u7528Transformer\u8fdb\u884c\u591a\u6807\u7b7e\u52a8\u4f5c\u548c\u5251\u5203\u5206\u7c7b\u3002\u4e3a\u4e86\u786e\u5b9a\u4f18\u5148\u6743\u548c\u5f97\u5206\uff0cFERA\u5e94\u7528\u4e86\u5e26\u6709\u7f16\u7801\u7684\u53f3\u4fa7\u89c4\u5219\u7684\u7cbe\u70bc\u8bed\u8a00\u6a21\u578b\uff0c\u4e3a\u6bcf\u6b21\u653b\u9632\u63d0\u4f9b\u51b3\u7b56\u548c\u89e3\u91ca\u3002", "result": "\u5728\u6709\u9650\u7684\u6807\u6ce8\u6570\u636e\u4e0b\uff0c\u901a\u8fc75\u6298\u4ea4\u53c9\u9a8c\u8bc1\uff0c\u5e73\u5747\u5b8fF1\u5f97\u5206\u4e3a0.549\uff0c\u4f18\u4e8eTCN\u3001BiLSTM\u548cTransformer\u7b49\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "FERA\u5c55\u793a\u4e86\u5728\u51fb\u5251\u6bd4\u8d5b\u4e2d\u5b9e\u73b0\u81ea\u52a8\u5316\u88c1\u5224\u8f85\u52a9\u7684\u6f5c\u529b\uff0c\u5e76\u4e3aAI\u5728\u51fb\u5251\u9886\u57df\u7684\u5e94\u7528\uff08\u5982\u6307\u5bfc\uff09\u5f00\u8f9f\u4e86\u65b0\u7684\u673a\u4f1a\u3002"}}
{"id": "2509.18577", "categories": ["cs.CL", "68T50", "I.2.7"], "pdf": "https://arxiv.org/pdf/2509.18577", "abs": "https://arxiv.org/abs/2509.18577", "authors": ["Yeongbin Seo", "Gayoung Kim", "Jaehyung Kim", "Jinyoung Yeo"], "title": "Prior-based Noisy Text Data Filtering: Fast and Strong Alternative For Perplexity", "comment": null, "summary": "As large language models (LLMs) are pretrained on massive web corpora,\ncareful selection of data becomes essential to ensure effective and efficient\nlearning. While perplexity (PPL)-based filtering has shown strong performance,\nit suffers from drawbacks: substantial time costs and inherent unreliability of\nthe model when handling noisy or out-of-distribution samples. In this work, we\npropose a simple yet powerful alternative: a prior-based data filtering method\nthat estimates token priors using corpus-level term frequency statistics,\ninspired by linguistic insights on word roles and lexical density. Our approach\nfilters documents based on the mean and standard deviation of token priors,\nserving as a fast proxy to PPL while requiring no model inference. Despite its\nsimplicity, the prior-based filter achieves the highest average performance\nacross 20 downstream benchmarks, while reducing time cost by over 1000x\ncompared to PPL-based filtering. We further demonstrate its applicability to\nsymbolic languages such as code and math, and its dynamic adaptability to\nmultilingual corpora without supervision", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e token \u5148\u9a8c\u7684\u6587\u672c\u8fc7\u6ee4\u65b9\u6cd5\uff0c\u901a\u8fc7\u8ba1\u7b97\u8bcd\u9891\u7edf\u8ba1\u6765\u4f30\u8ba1 token \u5148\u9a8c\uff0c\u4ee5\u6b64\u66ff\u4ee3 PPL \u8fc7\u6ee4\uff0c\u80fd\u663e\u8457\u964d\u4f4e\u6210\u672c\u5e76\u63d0\u9ad8\u6027\u80fd\u3002", "motivation": "PPL \u8fc7\u6ee4\u65b9\u6cd5\u5b58\u5728\u8017\u65f6\u548c\u4e0d\u53ef\u9760\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u4f18\u7684\u8fc7\u6ee4\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u8ba1\u7b97 token \u7684\u5148\u9a8c\u6982\u7387\uff08\u57fa\u4e8e\u8bed\u6599\u5e93\u7684\u8bcd\u9891\u7edf\u8ba1\uff09\uff0c\u5e76\u6839\u636e token \u5148\u9a8c\u7684\u5747\u503c\u548c\u6807\u51c6\u5dee\u6765\u8fc7\u6ee4\u6587\u6863\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728 20 \u4e2a\u4e0b\u6e38\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u4f73\u7684\u5e73\u5747\u6027\u80fd\uff0c\u5e76\u5c06\u65f6\u95f4\u6210\u672c\u964d\u4f4e\u4e86 1000 \u500d\u4ee5\u4e0a\u3002\u8be5\u65b9\u6cd5\u8fd8\u9002\u7528\u4e8e\u4ee3\u7801\u548c\u6570\u5b66\u7b49\u7b26\u53f7\u8bed\u8a00\uff0c\u5e76\u80fd\u52a8\u6001\u9002\u5e94\u591a\u8bed\u8a00\u8bed\u6599\u5e93\u3002", "conclusion": "\u57fa\u4e8e token \u5148\u9a8c\u7684\u8fc7\u6ee4\u65b9\u6cd5\u662f\u4e00\u79cd\u7b80\u5355\u3001\u5feb\u901f\u4e14\u6709\u6548\u7684\u66ff\u4ee3 PPL \u8fc7\u6ee4\u7684\u65b9\u6cd5\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2509.19081", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2509.19081", "abs": "https://arxiv.org/abs/2509.19081", "authors": ["Xin Jin", "Qingqing Zhang", "Dengfeng Li", "Zhenxiang Cheng", "Jianli Wang", "Xuewei Lv", "Xiaoyuan Zhou", "Rui Wang", "Xianyong Ding", "Peng Yu", "Xiaolong Yang"], "title": "Anharmonicity-driven phonon avoided crossing and anomalous thermal transport in nodal-line semimetal ZrSiS", "comment": "12 pages,7 figures", "summary": "Understanding thermal and electrical transport in topological materials is\nessential for advancing their applications in quantum technologies and energy\nconversion. Herein, we employ first-principles calculations to systematically\ninvestigate phonon and charge transport in the prototypical nodal-line\nsemimetal ZrSiS. The results unveil that anharmonic phonon renormalization\nresults in the pronounced softening of heat-carrying phonons and suppressed\nlattice thermal conductivity ($\\kappa_{\\rm L}$). Crucially, anharmonic effects\nare found to noticeably weaken Zr-S interactions, triggering avoided-crossing\nbehavior of low-frequency optical phonons. The combination of phonon softening\nand avoided crossing synergistically reduces phonon group velocities, yielding\na 16\\% suppression in $\\kappa_{\\rm L}$ along the $c$-axis at room temperature.\nContrary to conventional metals, we discover that the lattice contribution to\nthermal conductivity in ZrSiS is abnormally large, even dominating heat\nconduction along the $c$-axis. This unusual behavior results in a substantial\ndeviation of the Lorenz number from the Sommerfeld value -- exceeding it by up\nto threefold -- thereby challenging the validation of standard Wiedemann-Franz\nlaw for thermal conductivity estimation. Moreover, our calculations demonstrate\nthat ZrSiS exhibits exceptional electrical conductivity, attributed to its\ntopological electronic Dirac states that accounts for both high Fermi\nvelocities and weak electron-phonon coupling. This study provides critical\ninsights into the electrical and thermal transport mechanisms in ZrSiS and\nhighlights the importance of anharmonic effects in the lattice dynamics and\nthermal transport of metallic materials.", "AI": {"tldr": "ZrSiS\u6750\u6599\u4e2d\uff0c\u975e\u8c10\u6548\u5e94\u663e\u8457\u964d\u4f4e\u4e86\u6676\u683c\u70ed\u5bfc\u7387\uff0c\u4e14\u5176\u6676\u683c\u70ed\u5bfc\u7387\u5f02\u5e38\u9ad8\uff0c\u4e0d\u9075\u5faa\u7ef4\u5fb7\u66fc-\u5f17\u5170\u5179\u5b9a\u5f8b\u3002\u540c\u65f6\uff0c\u5176\u62d3\u6251\u7535\u5b50\u72c4\u62c9\u514b\u6001\u8d4b\u4e88\u4e86\u4f18\u5f02\u7684\u5bfc\u7535\u6027\u3002", "motivation": "\u7406\u89e3\u62d3\u6251\u6750\u6599\u4e2d\u7684\u70ed\u7535\u8f93\u8fd0\u5bf9\u4e8e\u63a8\u52a8\u5176\u5728\u91cf\u5b50\u6280\u672f\u548c\u80fd\u6e90\u8f6c\u6362\u4e2d\u7684\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5229\u7528\u7b2c\u4e00\u6027\u539f\u7406\u8ba1\u7b97\u7cfb\u7edf\u7814\u7a76\u4e86ZrSiS\u7684\u58f0\u5b50\u548c\u7535\u8377\u8f93\u8fd0\u3002", "result": "\u975e\u8c10\u58f0\u5b50\u91cd\u6b63\u5316\u5bfc\u81f4\u8f7d\u70ed\u58f0\u5b50\u8f6f\u5316\uff0c\u6291\u5236\u4e86\u6676\u683c\u70ed\u5bfc\u7387\u3002\u975e\u8c10\u6548\u5e94\u8fd8\u524a\u5f31\u4e86Zr-S\u76f8\u4e92\u4f5c\u7528\uff0c\u5bfc\u81f4\u4f4e\u9891\u5149\u5b66\u58f0\u5b50\u51fa\u73b0\u907f\u514d\u4ea4\u53c9\u3002\u8fd9\u4e9b\u6548\u5e94\u534f\u540c\u4f5c\u7528\uff0c\u5728\u5ba4\u6e29\u4e0b\u6cbfc\u8f74\u4f7f\u6676\u683c\u70ed\u5bfc\u7387\u964d\u4f4e\u4e8616%\u3002ZrSiS\u7684\u6676\u683c\u70ed\u5bfc\u7387\u5f02\u5e38\u5927\uff0c\u751a\u81f3\u8d85\u8fc7\u4e86\u7535\u5b50\u70ed\u5bfc\u7387\uff0c\u5bfc\u81f4\u6d1b\u4f26\u5179\u6570\u663e\u8457\u504f\u79bbSommerfeld\u503c\u3002ZrSiS\u5177\u6709\u4f18\u5f02\u7684\u5bfc\u7535\u6027\uff0c\u5f52\u56e0\u4e8e\u5176\u62d3\u6251\u7535\u5b50\u72c4\u62c9\u514b\u6001\u3002", "conclusion": "\u8be5\u7814\u7a76\u6df1\u5165\u7406\u89e3\u4e86ZrSiS\u7684\u7535\u70ed\u8f93\u8fd0\u673a\u5236\uff0c\u5e76\u5f3a\u8c03\u4e86\u975e\u8c10\u6548\u5e94\u5bf9\u91d1\u5c5e\u6750\u6599\u6676\u683c\u52a8\u529b\u5b66\u548c\u70ed\u8f93\u8fd0\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2509.18644", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18644", "abs": "https://arxiv.org/abs/2509.18644", "authors": ["Juntu Zhao", "Wenbo Lu", "Di Zhang", "Yufeng Liu", "Yushen Liang", "Tianluo Zhang", "Yifeng Cao", "Junyuan Xie", "Yingdong Hu", "Shengjie Wang", "Junliang Guo", "Dequan Wang", "Yang Gao"], "title": "Do You Need Proprioceptive States in Visuomotor Policies?", "comment": "Project page: https://statefreepolicy.github.io", "summary": "Imitation-learning-based visuomotor policies have been widely used in robot\nmanipulation, where both visual observations and proprioceptive states are\ntypically adopted together for precise control. However, in this study, we find\nthat this common practice makes the policy overly reliant on the proprioceptive\nstate input, which causes overfitting to the training trajectories and results\nin poor spatial generalization. On the contrary, we propose the State-free\nPolicy, removing the proprioceptive state input and predicting actions only\nconditioned on visual observations. The State-free Policy is built in the\nrelative end-effector action space, and should ensure the full task-relevant\nvisual observations, here provided by dual wide-angle wrist cameras. Empirical\nresults demonstrate that the State-free policy achieves significantly stronger\nspatial generalization than the state-based policy: in real-world tasks such as\npick-and-place, challenging shirt-folding, and complex whole-body manipulation,\nspanning multiple robot embodiments, the average success rate improves from 0\\%\nto 85\\% in height generalization and from 6\\% to 64\\% in horizontal\ngeneralization. Furthermore, they also show advantages in data efficiency and\ncross-embodiment adaptation, enhancing their practicality for real-world\ndeployment.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u65e0\u72b6\u6001\u7b56\u7565\u201d\u7684\u65b9\u6cd5\uff0c\u4ec5\u901a\u8fc7\u89c6\u89c9\u8f93\u5165\u6765\u8bad\u7ec3\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\uff0c\u6452\u5f03\u4e86\u4f20\u7edf\u7684\u672c\u4f53\u611f\u89c9\u72b6\u6001\u8f93\u5165\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u7a7a\u95f4\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\uff0c\u7ed3\u5408\u89c6\u89c9\u548c\u672c\u4f53\u611f\u89c9\u72b6\u6001\u7684\u6a21\u4eff\u5b66\u4e60\u7b56\u7565\u5bb9\u6613\u8fc7\u5ea6\u4f9d\u8d56\u672c\u4f53\u611f\u89c9\u8f93\u5165\uff0c\u5bfc\u81f4\u5bf9\u8bad\u7ec3\u8f68\u8ff9\u7684\u8fc7\u62df\u5408\u548c\u8f83\u5dee\u7684\u7a7a\u95f4\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u201c\u65e0\u72b6\u6001\u7b56\u7565\u201d\uff0c\u4ec5\u4f7f\u7528\u89c6\u89c9\u89c2\u6d4b\u4f5c\u4e3a\u8f93\u5165\u6765\u9884\u6d4b\u52a8\u4f5c\uff0c\u6452\u5f03\u4e86\u672c\u4f53\u611f\u89c9\u72b6\u6001\u3002\u8be5\u7b56\u7565\u5728\u76f8\u5bf9\u672b\u7aef\u6267\u884c\u5668\u52a8\u4f5c\u7a7a\u95f4\u4e2d\u6784\u5efa\uff0c\u5e76\u5229\u7528\u53cc\u8155\u5e7f\u89d2\u6444\u50cf\u5934\u63d0\u4f9b\u7684\u89c6\u89c9\u4fe1\u606f\u3002", "result": "\u201c\u65e0\u72b6\u6001\u7b56\u7565\u201d\u5728\u7a7a\u95f4\u6cdb\u5316\u80fd\u529b\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u4e8e\u72b6\u6001\u7684\u7b56\u7565\u3002\u5728\u5b9e\u9645\u7684\u6293\u53d6\u3001\u6298\u53e0\u886c\u886b\u548c\u5168\u8eab\u64cd\u4f5c\u7b49\u4efb\u52a1\u4e2d\uff0c\u9ad8\u5ea6\u6cdb\u5316\u6210\u529f\u7387\u4ece0%\u63d0\u9ad8\u523085%\uff0c\u6c34\u5e73\u6cdb\u5316\u6210\u529f\u7387\u4ece6%\u63d0\u9ad8\u523064%\u3002\u6b64\u5916\uff0c\u8be5\u7b56\u7565\u5728\u6570\u636e\u6548\u7387\u548c\u8de8\u673a\u5668\u4eba\u9002\u5e94\u6027\u65b9\u9762\u4e5f\u8868\u73b0\u51fa\u4f18\u52bf\u3002", "conclusion": "\u201c\u65e0\u72b6\u6001\u7b56\u7565\u201d\u901a\u8fc7\u4ec5\u4f9d\u8d56\u89c6\u89c9\u8f93\u5165\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u5728\u5404\u79cd\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u7a7a\u95f4\u6cdb\u5316\u80fd\u529b\u3001\u6570\u636e\u6548\u7387\u548c\u9002\u5e94\u6027\uff0c\u4f7f\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u66f4\u5177\u5b9e\u7528\u6027\u3002"}}
{"id": "2509.18372", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18372", "abs": "https://arxiv.org/abs/2509.18372", "authors": ["Reeshad Khan", "John Gauch"], "title": "TinyBEV: Cross Modal Knowledge Distillation for Efficient Multi Task Bird's Eye View Perception and Planning", "comment": null, "summary": "We present TinyBEV, a unified, camera only Bird's Eye View (BEV) framework\nthat distills the full-stack capabilities of a large planning-oriented teacher\n(UniAD [19]) into a compact, real-time student model. Unlike prior efficient\ncamera only baselines such as VAD[23] and VADv2[7], TinyBEV supports the\ncomplete autonomy stack 3D detection, HD-map segmentation, motion forecasting,\noccupancy prediction, and goal-directed planning within a streamlined\n28M-parameter backbone, achieving a 78% reduction in parameters over UniAD\n[19]. Our model-agnostic, multi-stage distillation strategy combines\nfeature-level, output-level, and adaptive region-aware supervision to\neffectively transfer high-capacity multi-modal knowledge to a lightweight BEV\nrepresentation. On nuScenes[4], Tiny-BEV achieves 39.0 mAP for detection, 1.08\nminADE for motion forecasting, and a 0.32 collision rate, while running 5x\nfaster (11 FPS) and requiring only camera input. These results demonstrate that\nfull-stack driving intelligence can be retained in resource-constrained\nsettings, bridging the gap between large-scale, multi-modal perception-planning\nmodels and deployment-ready real-time autonomy.", "AI": {"tldr": "TinyBEV\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u3001\u4ec5\u6444\u50cf\u5934\u7684BEV\u6846\u67b6\uff0c\u80fd\u5c06\u5927\u578b\u6559\u5e08\u6a21\u578b\u7684\u5b8c\u6574\u529f\u80fd\u538b\u7f29\u5230\u4e00\u4e2a\u7d27\u51d1\u7684\u3001\u5b9e\u65f6\u7684\u5b66\u751f\u6a21\u578b\u4e2d\uff0c\u53c2\u6570\u91cf\u51cf\u5c1178%\uff0c\u901f\u5ea6\u63d0\u53475\u500d\uff0c\u540c\u65f6\u4ec5\u9700\u6444\u50cf\u5934\u8f93\u5165\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u5c06\u5927\u578b\u3001\u591a\u6a21\u6001\u7684\u611f\u77e5-\u89c4\u5212\u6a21\u578b\u7684\u5b8c\u6574\u81ea\u52a8\u9a7e\u9a76\u80fd\u529b\u8fc1\u79fb\u5230\u4e00\u4e2a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u8f7b\u91cf\u7ea7\u6a21\u578b\u4e2d\uff0c\u5b9e\u73b0\u5b9e\u65f6\u81ea\u4e3b\u8fd0\u884c\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u578b\u65e0\u5173\u7684\u591a\u9636\u6bb5\u84b8\u998f\u7b56\u7565\uff0c\u7ed3\u5408\u4e86\u7279\u5f81\u7ea7\u3001\u8f93\u51fa\u7ea7\u548c\u81ea\u9002\u5e94\u533a\u57df\u611f\u77e5\u76d1\u7763\uff0c\u5c06\u9ad8\u5bb9\u91cf\u7684\u591a\u6a21\u6001\u77e5\u8bc6\u6709\u6548\u8fc1\u79fb\u5230\u8f7b\u91cf\u7ea7BEV\u8868\u793a\u4e2d\u3002", "result": "\u5728nuScenes\u6570\u636e\u96c6\u4e0a\uff0cTiny-BEV\u5728\u68c0\u6d4b\u4efb\u52a1\u4e0a\u8fbe\u5230\u4e8639.0 mAP\uff0c\u5728\u8fd0\u52a8\u9884\u6d4b\u4efb\u52a1\u4e0a\u8fbe\u5230\u4e861.08 minADE\uff0c\u78b0\u649e\u7387\u4e3a0.32\uff0c\u540c\u65f6\u5b9e\u73b0\u4e8611 FPS\u7684\u5b9e\u65f6\u8fd0\u884c\u901f\u5ea6\uff0c\u6bd4UniAD\u5feb5\u500d\u3002", "conclusion": "TinyBEV\u8bc1\u660e\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u4ecd\u7136\u53ef\u4ee5\u4fdd\u7559\u5b8c\u6574\u7684\u5168\u6808\u9a7e\u9a76\u667a\u80fd\uff0c\u6709\u6548\u5f25\u5408\u4e86\u5927\u578b\u591a\u6a21\u6001\u611f\u77e5-\u89c4\u5212\u6a21\u578b\u4e0e\u53ef\u90e8\u7f72\u7684\u5b9e\u65f6\u81ea\u4e3b\u7cfb\u7edf\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2509.18674", "categories": ["quant-ph", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18674", "abs": "https://arxiv.org/abs/2509.18674", "authors": ["Hyunho Cha", "Wonjung Kim", "Jungwoo Lee"], "title": "Scalable bayesian shadow tomography for quantum property estimation with set transformers", "comment": "33 pages, 9 figures", "summary": "A scalable Bayesian machine learning framework is introduced for estimating\nscalar properties of an unknown quantum state from measurement data, which\nbypasses full density matrix reconstruction. This work is the first to\nintegrate the classical shadows protocol with a permutation-invariant set\ntransformer architecture, enabling the approach to predict and correct bias in\nexisting estimators to approximate the true Bayesian posterior mean.\nMeasurement outcomes are encoded as fixed-dimensional feature vectors, and the\nnetwork outputs a residual correction to a baseline estimator. Scalability to\nlarge quantum systems is ensured by the polynomial dependence of input size on\nsystem size and number of measurements. On Greenberger-Horne-Zeilinger state\nfidelity and second-order R\\'enyi entropy estimation tasks -- using random\nPauli and random Clifford measurements -- this Bayesian estimator always\nachieves lower mean squared error than classical shadows alone, with more than\na 99\\% reduction in the few copy regime.", "AI": {"tldr": "\u672c\u7814\u7a76\u4ecb\u7ecd\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u8d1d\u53f6\u65af\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u6d4b\u91cf\u6570\u636e\u4e2d\u4f30\u8ba1\u672a\u77e5\u91cf\u5b50\u6001\u7684\u6807\u91cf\u5c5e\u6027\uff0c\u65e0\u9700\u8fdb\u884c\u5b8c\u6574\u7684\u5bc6\u5ea6\u77e9\u9635\u91cd\u5efa\u3002", "motivation": "\u8be5\u6846\u67b6\u9996\u6b21\u5c06\u7ecf\u5178\u9634\u5f71\u534f\u8bae\u4e0e\u7f6e\u6362\u4e0d\u53d8\u96c6Transformer\u67b6\u6784\u76f8\u7ed3\u5408\uff0c\u4ee5\u9884\u6d4b\u548c\u7ea0\u6b63\u73b0\u6709\u4f30\u8ba1\u5668\u7684\u504f\u5dee\uff0c\u4ece\u800c\u903c\u8fd1\u771f\u5b9e\u7684\u8d1d\u53f6\u65af\u540e\u9a8c\u5747\u503c\u3002", "method": "\u6d4b\u91cf\u7ed3\u679c\u88ab\u7f16\u7801\u4e3a\u56fa\u5b9a\u7ef4\u5ea6\u7684\u7279\u5f81\u5411\u91cf\uff0c\u7f51\u7edc\u8f93\u51fa\u5bf9\u57fa\u7ebf\u4f30\u8ba1\u5668\u7684\u6b8b\u5dee\u6821\u6b63\u3002\u901a\u8fc7\u8f93\u5165\u5927\u5c0f\u4e0e\u7cfb\u7edf\u5927\u5c0f\u548c\u6d4b\u91cf\u6b21\u6570\u7684\u4f9d\u8d56\u6027\u4e3a\u591a\u9879\u5f0f\uff0c\u786e\u4fdd\u4e86\u5bf9\u5927\u578b\u91cf\u5b50\u7cfb\u7edf\u7684\u53ef\u6269\u5c55\u6027\u3002", "result": "\u5728\u91cf\u5b50\u6001\u4fdd\u771f\u5ea6\u548c\u4e8c\u9636R\u00e9nyi\u71b5\u4f30\u8ba1\u4efb\u52a1\u4e2d\uff0c\u4f7f\u7528\u968f\u673a\u6ce1\u5229\u548c\u968f\u673aClifford\u6d4b\u91cf\uff0c\u8be5\u8d1d\u53f6\u65af\u4f30\u8ba1\u5668\u5728\u526f\u672c\u6570\u91cf\u8f83\u5c11\u7684\u60c5\u51b5\u4e0b\uff0c\u5747\u65b9\u8bef\u5dee\u59cb\u7ec8\u4f4e\u4e8e\u5355\u72ec\u4f7f\u7528\u7ecf\u5178\u9634\u5f71\u7684\u65b9\u6cd5\uff0c\u964d\u4f4e\u5e45\u5ea6\u8d85\u8fc799%\u3002", "conclusion": "\u8be5\u8d1d\u53f6\u65af\u673a\u5668\u5b66\u4e60\u6846\u67b6\u5728\u4f30\u8ba1\u91cf\u5b50\u6001\u6807\u91cf\u5c5e\u6027\u65b9\u9762\uff0c\u76f8\u6bd4\u4e8e\u73b0\u6709\u65b9\u6cd5\u5177\u6709\u663e\u8457\u7684\u4f18\u52bf\uff0c\u5c24\u5176\u662f\u5728\u6570\u636e\u91cf\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u3002"}}
{"id": "2509.18131", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18131", "abs": "https://arxiv.org/abs/2509.18131", "authors": ["Jean-Michel Tucny", "Abhisek Ganguly", "Santosh Ansumali", "Sauro Succi"], "title": "Two ways to knowledge?", "comment": null, "summary": "It is shown that the weight matrices of transformer-based machine learning\napplications to the solution of two representative physical applications show a\nrandom-like character which bears no directly recognizable link to the physical\nand mathematical structure of the physical problem under study. This suggests\nthat machine learning and the scientific method may represent two distinct and\npotentially complementary paths to knowledge, even though a strict notion of\nexplainability in terms of direct correspondence between network parameters and\nphysical structures may remain out of reach. It is also observed that drawing a\nparallel between transformer operation and (generalized) path-integration\ntechniques may account for the random-like nature of the weights, but still\ndoes not resolve the tension with explainability. We conclude with some general\ncomments on the hazards of gleaning knowledge without the benefit of Insight.", "AI": {"tldr": "Transformer\u6a21\u578b\u7684\u6743\u91cd\u77e9\u9635\u4e0e\u7269\u7406\u95ee\u9898\u7ed3\u6784\u65e0\u76f4\u63a5\u5173\u8054\uff0c\u8868\u660e\u673a\u5668\u5b66\u4e60\u4e0e\u79d1\u5b66\u65b9\u6cd5\u53ef\u80fd\u662f\u72ec\u7acb\u4f46\u4e92\u8865\u7684\u77e5\u8bc6\u83b7\u53d6\u9014\u5f84\uff0c\u4f46\u76f4\u63a5\u7684\u53ef\u89e3\u91ca\u6027\u96be\u4ee5\u5b9e\u73b0\u3002", "motivation": "\u63a2\u8ba8Transformer\u6a21\u578b\u5728\u7269\u7406\u5e94\u7528\u4e2d\u6743\u91cd\u77e9\u9635\u7684\u7279\u6027\uff0c\u4ee5\u53ca\u5176\u4e0e\u7269\u7406\u95ee\u9898\u7ed3\u6784\u7684\u5173\u7cfb\uff0c\u5e76\u601d\u8003\u673a\u5668\u5b66\u4e60\u4e0e\u79d1\u5b66\u65b9\u6cd5\u7684\u5f02\u540c\u3002", "method": "\u5206\u6790Transformer\u6a21\u578b\u5728\u89e3\u51b3\u4e24\u4e2a\u7269\u7406\u95ee\u9898\u65f6\u6743\u91cd\u77e9\u9635\u7684\u968f\u673a\u6027\u7279\u5f81\uff0c\u5e76\u5c1d\u8bd5\u5c06\u5176\u4e0e\u8def\u5f84\u79ef\u5206\u6280\u672f\u8fdb\u884c\u7c7b\u6bd4\u3002", "result": "Transformer\u6a21\u578b\u7684\u6743\u91cd\u77e9\u9635\u8868\u73b0\u51fa\u968f\u673a\u6027\uff0c\u4e0e\u7269\u7406\u95ee\u9898\u7ed3\u6784\u65e0\u660e\u663e\u8054\u7cfb\uff0c\u4f46\u4e0e\u8def\u5f84\u79ef\u5206\u6280\u672f\u5b58\u5728\u6f5c\u5728\u5173\u8054\u3002", "conclusion": "\u673a\u5668\u5b66\u4e60\uff0c\u7279\u522b\u662fTransformer\u6a21\u578b\uff0c\u53ef\u80fd\u63d0\u4f9b\u4e00\u79cd\u4e0e\u4f20\u7edf\u79d1\u5b66\u65b9\u6cd5\u4e0d\u540c\u7684\u77e5\u8bc6\u83b7\u53d6\u9014\u5f84\uff0c\u4f46\u5176\u53ef\u89e3\u91ca\u6027\u4ecd\u662f\u6311\u6218\uff0c\u9700\u8981\u8b66\u60d5\u7f3a\u4e4f\u6d1e\u5bdf\u529b\u7684\u77e5\u8bc6\u83b7\u53d6\u65b9\u5f0f\u3002"}}
{"id": "2509.18557", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18557", "abs": "https://arxiv.org/abs/2509.18557", "authors": ["Tom Pawelek", "Raj Patel", "Charlotte Crowell", "Noorbakhsh Amiri", "Sudip Mittal", "Shahram Rahimi", "Andy Perkins"], "title": "LLMZ+: Contextual Prompt Whitelist Principles for Agentic LLMs", "comment": "7 pages, 5 figures, to be published and presented at ICMLA 2025", "summary": "Compared to traditional models, agentic AI represents a highly valuable\ntarget for potential attackers as they possess privileged access to data\nsources and API tools, which are traditionally not incorporated into classical\nagents. Unlike a typical software application residing in a Demilitarized Zone\n(DMZ), agentic LLMs consciously rely on nondeterministic behavior of the AI\n(only defining a final goal, leaving the path selection to LLM). This\ncharacteristic introduces substantial security risk to both operational\nsecurity and information security. Most common existing defense mechanism rely\non detection of malicious intent and preventing it from reaching the LLM agent,\nthus protecting against jailbreak attacks such as prompt injection. In this\npaper, we present an alternative approach, LLMZ+, which moves beyond\ntraditional detection-based approaches by implementing prompt whitelisting.\nThrough this method, only contextually appropriate and safe messages are\npermitted to interact with the agentic LLM. By leveraging the specificity of\ncontext, LLMZ+ guarantees that all exchanges between external users and the LLM\nconform to predefined use cases and operational boundaries. Our approach\nstreamlines the security framework, enhances its long-term resilience, and\nreduces the resources required for sustaining LLM information security. Our\nempirical evaluation demonstrates that LLMZ+ provides strong resilience against\nthe most common jailbreak prompts. At the same time, legitimate business\ncommunications are not disrupted, and authorized traffic flows seamlessly\nbetween users and the agentic LLM. We measure the effectiveness of approach\nusing false positive and false negative rates, both of which can be reduced to\n0 in our experimental setting.", "AI": {"tldr": "Agentic AI's privileged access and nondeterministic behavior pose significant security risks. LLMZ+ offers an alternative defense by using prompt whitelisting to ensure only safe and contextually appropriate messages interact with the LLM, thereby preventing jailbreak attacks without disrupting legitimate communications.", "motivation": "Traditional defense mechanisms for agentic AI focus on detecting malicious intent, but the unique characteristics of agentic LLMs (privileged access, reliance on nondeterministic behavior) introduce substantial security risks that these methods may not fully address. Thus, an alternative approach is needed.", "method": "LLMZ+ implements a prompt whitelisting strategy. This approach allows only messages that are contextually appropriate and safe to interact with the agentic LLM, ensuring that all communications adhere to predefined use cases and operational boundaries.", "result": "LLMZ+ demonstrates strong resilience against common jailbreak prompts, with no disruption to legitimate business communications. The effectiveness was measured by false positive and false negative rates, which were reduced to 0 in the experimental setting.", "conclusion": "LLMZ+ provides a more streamlined, resilient, and resource-efficient security framework for agentic LLMs by employing prompt whitelisting, which effectively mitigates jailbreak risks while preserving normal operations."}}
{"id": "2509.18585", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18585", "abs": "https://arxiv.org/abs/2509.18585", "authors": ["Yu Chen", "Yifei Han", "Long Zhang", "Yue Du", "Bin Li"], "title": "TsqLoRA: Towards Sensitivity and Quality Low-Rank Adaptation for Efficient Fine-Tuning", "comment": "5 pages, 4 figures, published to ICASSP2026", "summary": "Fine-tuning large pre-trained models for downstream tasks has become a\nfundamental approach in natural language processing. Fully fine-tuning all\nmodel parameters is computationally expensive and memory-intensive, especially\nin resource-constrained environments. Existing parameter-efficient fine-tuning\nmethods reduce the number of trainable parameters but typically overlook the\nvarying sensitivity of different model layers and the importance of training\ndata. In this work, we propose TsqLoRA, a novel method that integrates\ndata-quality-driven selection with sensitivity-aware low-rank adaptation,\nconsisted of two main components: a quality-aware sampling mechanism for\nselecting the most informative training data, and a dynamic rank allocation\nmodule that adjusts the rank of each layer based on its sensitivity to\nparameter updates. The experimental results demonstrate that TsqLoRA improves\nfine-tuning efficiency while maintaining or even improving performance on a\nvariety of NLP tasks. Our code will be available at\nhttps://github.com/Benjamin-Ricky/TsqLoRA.", "AI": {"tldr": "TsqLoRA\u901a\u8fc7\u6574\u5408\u6570\u636e\u8d28\u91cf\u9a71\u52a8\u7684\u9009\u62e9\u548c\u654f\u611f\u5ea6\u611f\u77e5\u7684\u4f4e\u79e9\u9002\u914d\uff0c\u5728\u4fdd\u6301\u751a\u81f3\u63d0\u9ad8\u6027\u80fd\u7684\u540c\u65f6\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u5fae\u8c03\u7684\u6548\u7387\u3002", "motivation": "\u5b8c\u5168\u5fae\u8c03\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u5185\u5b58\u5360\u7528\u5927\uff0c\u5c24\u5176\u662f\u5728\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u4e2d\u3002\u73b0\u6709\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\u5ffd\u7565\u4e86\u4e0d\u540c\u6a21\u578b\u5c42\u5bf9\u8bad\u7ec3\u6570\u636e\u7684\u654f\u611f\u6027\u5dee\u5f02\u548c\u91cd\u8981\u6027\u3002", "method": "TsqLoRA\u5305\u542b\u4e24\u4e2a\u4e3b\u8981\u7ec4\u4ef6\uff1a\u4e00\u4e2a\u8d28\u91cf\u611f\u77e5\u91c7\u6837\u673a\u5236\uff0c\u7528\u4e8e\u9009\u62e9\u4fe1\u606f\u91cf\u6700\u5927\u7684\u8bad\u7ec3\u6570\u636e\uff1b\u4e00\u4e2a\u52a8\u6001\u79e9\u5206\u914d\u6a21\u5757\uff0c\u6839\u636e\u6bcf\u5c42\u5bf9\u53c2\u6570\u66f4\u65b0\u7684\u654f\u611f\u5ea6\u6765\u8c03\u6574\u5176\u79e9\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cTsqLoRA\u5728\u591a\u79cdNLP\u4efb\u52a1\u4e0a\u63d0\u9ad8\u4e86\u5fae\u8c03\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u9ad8\u4e86\u6027\u80fd\u3002", "conclusion": "TsqLoRA\u662f\u4e00\u79cd\u65b0\u9896\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u8003\u8651\u6570\u636e\u8d28\u91cf\u548c\u6a21\u578b\u5c42\u654f\u611f\u5ea6\uff0c\u5728\u8ba1\u7b97\u548c\u6027\u80fd\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u7684\u5e73\u8861\u3002"}}
{"id": "2509.19106", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2509.19106", "abs": "https://arxiv.org/abs/2509.19106", "authors": ["Alam Osorio", "Lucia Reining", "Francesco Sottile"], "title": "Trends in the electronic structure of borophene polymorphs", "comment": null, "summary": "Borophene is a two-dimensional material made out of boron atoms only. It\nexhibits polymorphism and different allotropes can be studied in terms of a\nrigid electronic structure, where only the occupation of the states change with\nthe respect to the number of electrons available in the system (self-doping).\nIn this work we selected a set of representative borophene polymorphs\n($\\delta_3$, $\\delta_5$, $\\delta_6$, $\\beta_{12}$ $\\alpha_1$, $\\alpha'$,\n$\\alpha'$-Bilayer) and studied the shared features of their electronic\nstructures and the limitations of this model. Our work revealed the appearance\nof defect-like states in some polymorphs when related to a parent rigid\nelectronic structure, and bonding/antibonding monolayer-like states in the\n$\\alpha'$-Bilayer. Moreover, we show how the buckling of $\\delta_6$ and\n$\\alpha'$ can act as a tuning parameter, enabling semimetallicity, Dirac cones,\nand nesting of the Fermi surface. In light of their promises for exotic but\nalso useful behavior, we expect our work to foster the interest in larger and\nmore complex borophene structures.", "AI": {"tldr": "Borophene \u662f\u4e00\u79cd\u7531\u7eaf\u787c\u539f\u5b50\u7ec4\u6210\u7684\u4e8c\u7ef4\u6750\u6599\uff0c\u5177\u6709\u591a\u6676\u578b\u548c\u72ec\u7279\u7684\u7535\u5b50\u7ed3\u6784\u3002\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u4e0d\u540c\u787c\u70ef\u53d8\u4f53\u7684\u7535\u5b50\u7ed3\u6784\u5171\u4eab\u7279\u5f81\u548c\u6a21\u578b\u5c40\u9650\u6027\uff0c\u63ed\u793a\u4e86\u7f3a\u9677\u6837\u6001\u548c\u952e\u5408/\u53cd\u952e\u5408\u5355\u5c42\u6837\u6001\u7684\u51fa\u73b0\u3002\u7814\u7a76\u8fd8\u8868\u660e\uff0c\u787c\u70ef\u7684\u8936\u76b1\u53ef\u4ee5\u4f5c\u4e3a\u8c03\u8c10\u53c2\u6570\uff0c\u5b9e\u73b0\u534a\u91d1\u5c5e\u3001\u72c4\u62c9\u514b\u9525\u548c\u8d39\u7c73\u9762\u5d4c\u5957\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u4e0d\u540c\u787c\u70ef\u53d8\u4f53\u7684\u7535\u5b50\u7ed3\u6784\u5171\u4eab\u7279\u5f81\uff0c\u5e76\u7814\u7a76\u5176\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002", "method": "\u7814\u7a76\u9009\u53d6\u4e86\u4e00\u7ec4\u4ee3\u8868\u6027\u7684\u787c\u70ef\u53d8\u4f53\uff08$\rm \furthermore$, $\rm \furthermore$, $\rm \furthermore$, $\rm \furthermore$, $\rm \furthermore$, $\rm \furthermore$, $\rm \furthermore$-\u53cc\u5c42\uff09\u8fdb\u884c\u4e86\u7535\u5b50\u7ed3\u6784\u7814\u7a76\u3002", "result": "\u7814\u7a76\u63ed\u793a\u4e86\u5728\u67d0\u4e9b\u53d8\u4f53\u4e2d\u51fa\u73b0\u4e0e\u6bcd\u4f53\u521a\u6027\u7535\u5b50\u7ed3\u6784\u76f8\u5173\u7684\u7f3a\u9677\u6837\u6001\uff0c\u4ee5\u53ca\u5728$\rm \furthermore$-\u53cc\u5c42\u4e2d\u51fa\u73b0\u7684\u952e\u5408/\u53cd\u952e\u5408\u5355\u5c42\u6837\u6001\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u8868\u660e$\rm \furthermore$\u548c$\rm \furthermore$\u7684\u8936\u76b1\u53ef\u4ee5\u4f5c\u4e3a\u8c03\u8c10\u53c2\u6570\uff0c\u5b9e\u73b0\u534a\u91d1\u5c5e\u3001\u72c4\u62c9\u514b\u9525\u548c\u8d39\u7c73\u9762\u5d4c\u5957\u3002", "conclusion": "\u787c\u70ef\u7684\u72ec\u7279\u7535\u5b50\u7ed3\u6784\u548c\u53ef\u8c03\u8c10\u7279\u6027\u9884\u793a\u7740\u5176\u5728\u672a\u6765\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u524d\u666f\uff0c\u672c\u7814\u7a76\u671f\u671b\u80fd\u6fc0\u53d1\u5bf9\u66f4\u5927\u3001\u66f4\u590d\u6742\u787c\u70ef\u7ed3\u6784\u7684\u7814\u7a76\u5174\u8da3\u3002"}}
{"id": "2509.18648", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18648", "abs": "https://arxiv.org/abs/2509.18648", "authors": ["Yarden As", "Chengrui Qu", "Benjamin Unger", "Dongho Kang", "Max van der Hart", "Laixi Shi", "Stelian Coros", "Adam Wierman", "Andreas Krause"], "title": "SPiDR: A Simple Approach for Zero-Shot Safety in Sim-to-Real Transfer", "comment": null, "summary": "Safety remains a major concern for deploying reinforcement learning (RL) in\nreal-world applications. Simulators provide safe, scalable training\nenvironments, but the inevitable sim-to-real gap introduces additional safety\nconcerns, as policies must satisfy constraints in real-world conditions that\ndiffer from simulation. To address this challenge, robust safe RL techniques\noffer principled methods, but are often incompatible with standard scalable\ntraining pipelines. In contrast, domain randomization, a simple and popular\nsim-to-real technique, stands out as a promising alternative, although it often\nresults in unsafe behaviors in practice. We present SPiDR, short for\nSim-to-real via Pessimistic Domain Randomization -- a scalable algorithm with\nprovable guarantees for safe sim-to-real transfer. SPiDR uses domain\nrandomization to incorporate the uncertainty about the sim-to-real gap into the\nsafety constraints, making it versatile and highly compatible with existing\ntraining pipelines. Through extensive experiments on sim-to-sim benchmarks and\ntwo distinct real-world robotic platforms, we demonstrate that SPiDR\neffectively ensures safety despite the sim-to-real gap while maintaining strong\nperformance.", "AI": {"tldr": "SPiDR\u662f\u4e00\u79cd\u901a\u8fc7\u60b2\u89c2\u57df\u968f\u673a\u5316\u8fdb\u884c\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u5177\u6709\u53ef\u8bc1\u660e\u7684\u5b89\u5168\u4fdd\u8bc1\uff0c\u80fd\u591f\u6709\u6548\u5f25\u5408\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u5dee\u8ddd\uff0c\u540c\u65f6\u4fdd\u6301\u826f\u597d\u7684\u6027\u80fd\u3002", "motivation": "\u5728\u771f\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u90e8\u7f72\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u65f6\uff0c\u5b89\u5168\u6027\u4ecd\u7136\u662f\u4e00\u4e2a\u4e3b\u8981\u95ee\u9898\u3002\u867d\u7136\u6a21\u62df\u5668\u63d0\u4f9b\u4e86\u5b89\u5168\u7684\u8bad\u7ec3\u73af\u5883\uff0c\u4f46\u4e0d\u53ef\u907f\u514d\u7684\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u5dee\u8ddd\u4f1a\u5e26\u6765\u989d\u5916\u7684\u5b89\u5168\u95ee\u9898\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u8bc1\u5b89\u5168\u53c8\u80fd\u4e0e\u53ef\u6269\u5c55\u8bad\u7ec3\u6d41\u6c34\u7ebf\u517c\u5bb9\u7684\u65b9\u6cd5\u3002", "method": "SPiDR\u7b97\u6cd5\u901a\u8fc7\u57df\u968f\u673a\u5316\u5c06\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u5dee\u8ddd\u7684\u4e0d\u786e\u5b9a\u6027\u7eb3\u5165\u5b89\u5168\u7ea6\u675f\uff0c\u4f7f\u5176\u5177\u6709\u901a\u7528\u6027\u5e76\u4e0e\u73b0\u6709\u8bad\u7ec3\u6d41\u6c34\u7ebf\u9ad8\u5ea6\u517c\u5bb9\u3002", "result": "\u901a\u8fc7\u5728\u6a21\u62df\u5230\u6a21\u62df\u57fa\u51c6\u6d4b\u8bd5\u548c\u4e24\u4e2a\u4e0d\u540c\u7684\u771f\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\uff0cSPiDR\u88ab\u8bc1\u660e\u80fd\u591f\u6709\u6548\u786e\u4fdd\u5b89\u5168\uff0c\u540c\u65f6\u4fdd\u6301\u826f\u597d\u7684\u6027\u80fd\u3002", "conclusion": "SPiDR\u662f\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u7b97\u6cd5\uff0c\u5177\u6709\u53ef\u8bc1\u660e\u7684\u5b89\u5168\u4fdd\u8bc1\uff0c\u7528\u4e8e\u5b89\u5168\u7684\u6a21\u62df\u5230\u73b0\u5b9e\u8f6c\u79fb\u3002\u5b83\u901a\u8fc7\u57df\u968f\u673a\u5316\u5c06\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u5dee\u8ddd\u7684\u4e0d\u786e\u5b9a\u6027\u7eb3\u5165\u5b89\u5168\u7ea6\u675f\uff0c\u4ece\u800c\u89e3\u51b3\u4e86\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u4e00\u4e2a\u5173\u952e\u6311\u6218\u3002"}}
{"id": "2509.18387", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18387", "abs": "https://arxiv.org/abs/2509.18387", "authors": ["Thomas Gossard", "Filip Radovic", "Andreas Ziegler", "Andrea Zell"], "title": "BlurBall: Joint Ball and Motion Blur Estimation for Table Tennis Ball Tracking", "comment": null, "summary": "Motion blur reduces the clarity of fast-moving objects, posing challenges for\ndetection systems, especially in racket sports, where balls often appear as\nstreaks rather than distinct points. Existing labeling conventions mark the\nball at the leading edge of the blur, introducing asymmetry and ignoring\nvaluable motion cues correlated with velocity. This paper introduces a new\nlabeling strategy that places the ball at the center of the blur streak and\nexplicitly annotates blur attributes. Using this convention, we release a new\ntable tennis ball detection dataset. We demonstrate that this labeling approach\nconsistently enhances detection performance across various models. Furthermore,\nwe introduce BlurBall, a model that jointly estimates ball position and motion\nblur attributes. By incorporating attention mechanisms such as\nSqueeze-and-Excitation over multi-frame inputs, we achieve state-of-the-art\nresults in ball detection. Leveraging blur not only improves detection accuracy\nbut also enables more reliable trajectory prediction, benefiting real-time\nsports analytics.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8fd0\u52a8\u6a21\u7cca\u7403\u7c7b\u6807\u6ce8\u65b9\u6cd5\uff0c\u5e76\u5c06\u6807\u6ce8\u4e2d\u5fc3\u7f6e\u4e8e\u6a21\u7cca\u6761\u7eb9\u7684\u4e2d\u5fc3\uff0c\u540c\u65f6\u663e\u5f0f\u6807\u6ce8\u6a21\u7cca\u5c5e\u6027\uff0c\u5e76\u53d1\u5e03\u4e86\u4e00\u4e2a\u65b0\u7684\u4e52\u4e53\u7403\u68c0\u6d4b\u6570\u636e\u96c6\uff0c\u5728\u591a\u4e2a\u6a21\u578b\u4e0a\u9a8c\u8bc1\u4e86\u8be5\u6807\u6ce8\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aBlurBall\u7684\u6a21\u578b\uff0c\u901a\u8fc7\u5f15\u5165\u6ce8\u610f\u529b\u673a\u5236\u7b49\u65b9\u6cd5\uff0c\u5728\u7403\u7c7b\u68c0\u6d4b\u548c\u8f68\u8ff9\u9884\u6d4b\u65b9\u9762\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6210\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u8fd0\u52a8\u6a21\u7cca\u7403\u7c7b\u6807\u6ce8\u65b9\u6cd5\u5c06\u6807\u6ce8\u70b9\u7f6e\u4e8e\u6a21\u7cca\u6761\u7eb9\u7684\u524d\u7f18\uff0c\u5ffd\u7565\u4e86\u4e0e\u901f\u5ea6\u76f8\u5173\u7684\u8fd0\u52a8\u7ebf\u7d22\uff0c\u5e76\u5f15\u5165\u4e86\u4e0d\u5bf9\u79f0\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6807\u6ce8\u7b56\u7565\uff0c\u5c06\u7403\u7f6e\u4e8e\u6a21\u7cca\u6761\u7eb9\u7684\u4e2d\u5fc3\uff0c\u5e76\u663e\u5f0f\u6807\u6ce8\u6a21\u7cca\u5c5e\u6027\u3002\u57fa\u4e8e\u6b64\uff0c\u53d1\u5e03\u4e86\u4e00\u4e2a\u65b0\u7684\u4e52\u4e53\u7403\u68c0\u6d4b\u6570\u636e\u96c6\u3002\u63d0\u51fa\u4e86\u540d\u4e3aBlurBall\u7684\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u80fd\u591f\u8054\u5408\u4f30\u8ba1\u7403\u7684\u4f4d\u7f6e\u548c\u8fd0\u52a8\u6a21\u7cca\u5c5e\u6027\uff0c\u5e76\u5229\u7528\u4e86\u8de8\u5e27\u8f93\u5165\u7684Squeeze-and-Excitation\u7b49\u6ce8\u610f\u529b\u673a\u5236\u3002", "result": "\u6240\u63d0\u51fa\u7684\u6807\u6ce8\u65b9\u6cd5\u5728\u5404\u79cd\u6a21\u578b\u4e0a\u6301\u7eed\u63d0\u9ad8\u4e86\u68c0\u6d4b\u6027\u80fd\u3002BlurBall\u6a21\u578b\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7403\u7c7b\u68c0\u6d4b\u7ed3\u679c\uff0c\u5e76\u80fd\u5b9e\u73b0\u66f4\u53ef\u9760\u7684\u8f68\u8ff9\u9884\u6d4b\u3002", "conclusion": "\u5229\u7528\u8fd0\u52a8\u6a21\u7cca\u4fe1\u606f\u4e0d\u4ec5\u53ef\u4ee5\u63d0\u9ad8\u68c0\u6d4b\u7cbe\u5ea6\uff0c\u8fd8\u80fd\u5b9e\u73b0\u66f4\u53ef\u9760\u7684\u8f68\u8ff9\u9884\u6d4b\uff0c\u4ece\u800c\u4fc3\u8fdb\u5b9e\u65f6\u4f53\u80b2\u5206\u6790\u3002"}}
{"id": "2509.18725", "categories": ["quant-ph", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.18725", "abs": "https://arxiv.org/abs/2509.18725", "authors": ["Erkka Haapasalo"], "title": "Barycentric decompositions for extensive monotone divergences", "comment": "42 pages, 3 figures", "summary": "We study sets of divergences or dissimilarity measures in a generalized\nreal-algebraic setting which includes the cases of classical and quantum\nmultivariate divergences. We show that a special subset of divergences, the\nso-called test spectrum, characterizes the rest of the divergences through\nbarycentres and that the extreme points of relevant convex subsets of general\ndivergences are contained within the test spectrum. Only some special parts of\nthe test spectrum may contain non-extreme elements. We are able to fully\ncharacterize the test spectrum in the case of classical multivariate\ndivergences. The quantum case is much more varied, and we demonstrate that\nessentially all the bivariate and multivariate quantum divergences suggested\npreviously in literature are within the test spectrum and extreme within the\nset of all quantum (multivariate) divergences. This suggests that the\nvariability of quantum divergences is real since all the previously suggested\ndivergences are independent of each other.", "AI": {"tldr": "test spectrum characterizes divergences in a generalized real-algebraic setting, with implications for classical and quantum divergences.", "motivation": "To study sets of divergences/dissimilarity measures in a generalized real-algebraic setting, encompassing classical and quantum multivariate divergences.", "method": "Characterization of divergences using the 'test spectrum' through barycenters and extreme points. Full characterization of the test spectrum in the classical case and analysis of its composition in the quantum case.", "result": "The test spectrum characterizes all other divergences. Extreme points of relevant convex subsets are within the test spectrum. Classical multivariate divergences are fully characterized. In the quantum case, previously suggested divergences are within the test spectrum and are extreme points, suggesting their independence.", "conclusion": "The test spectrum is a key concept for understanding divergences. The findings highlight the real variability and independence of previously suggested quantum divergences."}}
{"id": "2509.18133", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18133", "abs": "https://arxiv.org/abs/2509.18133", "authors": ["Le Huang", "Jiazheng Kang", "Cheng Hou", "Zhe Zhao", "Zhenxiang Yan", "Chuan Shi", "Ting Bai"], "title": "Self-Evolving LLMs via Continual Instruction Tuning", "comment": null, "summary": "In real-world industrial settings, large language models (LLMs) must learn\ncontinually to keep pace with diverse and evolving tasks, requiring\nself-evolution to refine knowledge under dynamic data distributions. However,\nexisting continual learning (CL) approaches, such as replay and parameter\nisolation, often suffer from catastrophic forgetting: training on new tasks\ndegrades performance on earlier ones by overfitting to the new distribution and\nweakening generalization.We propose MoE-CL, a parameter-efficient adversarial\nmixture-of-experts framework for industrial-scale, self-evolving continual\ninstruction tuning of LLMs. MoE-CL uses a dual-expert design: (1) a dedicated\nLoRA expert per task to preserve task-specific knowledge via parameter\nindependence, mitigating forgetting; and (2) a shared LoRA expert to enable\ncross-task transfer. To prevent transferring task-irrelevant noise through the\nshared pathway, we integrate a task-aware discriminator within a GAN. The\ndiscriminator encourages the shared expert to pass only task-aligned\ninformation during sequential training. Through adversarial learning, the\nshared expert acquires generalized representations that mimic the\ndiscriminator, while dedicated experts retain task-specific details, balancing\nknowledge retention and cross-task generalization and thereby supporting\nself-evolution.Extensive experiments on the public MTL5 benchmark and an\nindustrial Tencent3 benchmark validate the effectiveness of MoE-CL for\ncontinual instruction tuning. In real-world A/B testing for content compliance\nreview on the Tencent Video platform, MoE-CL reduced manual review costs by\n15.3%. These results demonstrate that MoE-CL is practical for large-scale\nindustrial deployment where continual adaptation and stable transfer are\ncritical.", "AI": {"tldr": "MoE-CL\u662f\u4e00\u79cd\u53c2\u6570\u9ad8\u6548\u7684\u5bf9\u6297\u6027\u6df7\u5408\u4e13\u5bb6\u6846\u67b6\uff0c\u7528\u4e8eLLM\u7684\u5de5\u4e1a\u7ea7\u3001\u81ea\u6f14\u5316\u6301\u7eed\u6307\u4ee4\u8c03\u4f18\uff0c\u901a\u8fc7\u4e13\u7528\u7684LoRA\u4e13\u5bb6\u548c\u5171\u4eab\u7684LoRA\u4e13\u5bb6\u7ed3\u5408GAN\u4e2d\u7684\u5224\u522b\u5668\uff0c\u4ee5\u5e73\u8861\u77e5\u8bc6\u4fdd\u7559\u548c\u8de8\u4efb\u52a1\u6cdb\u5316\uff0c\u6210\u529f\u5e94\u7528\u4e8e\u5de5\u4e1a\u573a\u666f\u5e76\u964d\u4f4e\u4e86\u4eba\u5de5\u5ba1\u6838\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u7684\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u5728\u5904\u7406\u5de5\u4e1a\u7ea7LLM\u7684\u6301\u7eed\u6307\u4ee4\u8c03\u4f18\u65f6\uff0c\u7531\u4e8e\u707e\u96be\u6027\u9057\u5fd8\u7684\u95ee\u9898\uff0c\u5f80\u5f80\u5728\u65b0\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u65e7\u4efb\u52a1\u4e0a\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMoE-CL\u7684\u53c2\u6570\u9ad8\u6548\u7684\u5bf9\u6297\u6027\u6df7\u5408\u4e13\u5bb6\u6846\u67b6\u3002\u8be5\u6846\u67b6\u5305\u542b\u4e00\u4e2a\u4e13\u7528\u7684LoRA\u4e13\u5bb6\u7528\u4e8e\u4fdd\u7559\u4efb\u52a1\u7279\u5b9a\u77e5\u8bc6\uff0c\u4ee5\u53ca\u4e00\u4e2a\u5171\u4eab\u7684LoRA\u4e13\u5bb6\u7528\u4e8e\u8de8\u4efb\u52a1\u8fc1\u79fb\u3002\u901a\u8fc7\u7ed3\u5408\u4e00\u4e2a\u4efb\u52a1\u611f\u77e5\u7684\u5224\u522b\u5668\u548c\u4e00\u4e2aGAN\uff0c\u9632\u6b62\u566a\u58f0\u4fe1\u606f\u901a\u8fc7\u5171\u4eab\u8def\u5f84\u8fc1\u79fb\uff0c\u5e76\u4fc3\u4f7f\u5171\u4eab\u4e13\u5bb6\u5b66\u4e60\u6cdb\u5316\u8868\u793a\u3002", "result": "\u5728MTL5\u548cTencent3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMoE-CL\u5728\u6301\u7eed\u6307\u4ee4\u8c03\u4f18\u65b9\u9762\u8868\u73b0\u51fa\u6709\u6548\u6027\u3002\u5728\u817e\u8baf\u89c6\u9891\u5e73\u53f0\u7684\u5185\u5bb9\u5408\u89c4\u6027\u5ba1\u67e5A/B\u6d4b\u8bd5\u4e2d\uff0cMoE-CL\u5c06\u4eba\u5de5\u5ba1\u6838\u6210\u672c\u964d\u4f4e\u4e8615.3%\u3002", "conclusion": "MoE-CL\u5728\u9700\u8981\u6301\u7eed\u9002\u5e94\u548c\u7a33\u5b9a\u8fc1\u79fb\u7684\u5927\u89c4\u6a21\u5de5\u4e1a\u90e8\u7f72\u4e2d\u662f\u5b9e\u7528\u7684\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u5e76\u80fd\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5e26\u6765\u663e\u8457\u7684\u6210\u672c\u6548\u76ca\u3002"}}
{"id": "2509.18565", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18565", "abs": "https://arxiv.org/abs/2509.18565", "authors": ["Mitchell Piehl", "Dillon Wilson", "Ananya Kalita", "Jugal Kalita"], "title": "Solving Math Word Problems Using Estimation Verification and Equation Generation", "comment": "Accepted to IEEE ICMLA 2025", "summary": "Large Language Models (LLMs) excel at various tasks, including\nproblem-solving and question-answering. However, LLMs often find Math Word\nProblems (MWPs) challenging because solving them requires a range of reasoning\nand mathematical abilities with which LLMs seem to struggle. Recent efforts\nhave helped LLMs solve more complex MWPs with improved prompts. This study\nproposes a novel method that initially prompts an LLM to create equations from\na decomposition of the question, followed by using an external symbolic\nequation solver to produce an answer. To ensure the accuracy of the obtained\nanswer, inspired by an established recommendation of math teachers, the LLM is\ninstructed to solve the MWP a second time, but this time with the objective of\nestimating the correct answer instead of solving it exactly. The estimation is\nthen compared to the generated answer to verify. If verification fails, an\niterative rectification process is employed to ensure the correct answer is\neventually found. This approach achieves new state-of-the-art results on\ndatasets used by prior published research on numeric and algebraic MWPs,\nimproving the previous best results by nearly two percent on average. In\naddition, the approach obtains satisfactory results on trigonometric MWPs, a\ntask not previously attempted to the authors' best knowledge. This study also\nintroduces two new datasets, SVAMPClean and Trig300, to further advance the\ntesting of LLMs' reasoning abilities.", "AI": {"tldr": "LLM \u901a\u8fc7\u751f\u6210\u65b9\u7a0b\u5e76\u4f7f\u7528\u5916\u90e8\u6c42\u89e3\u5668\u6765\u89e3\u51b3\u6570\u5b66\u5e94\u7528\u9898\uff0c\u5e76\u901a\u8fc7\u4f30\u7b97\u6765\u9a8c\u8bc1\u7b54\u6848\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "motivation": "LLM \u5728\u89e3\u51b3\u6570\u5b66\u5e94\u7528\u9898\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u9700\u8981\u6539\u8fdb\u7684\u63d0\u793a\u65b9\u6cd5\u3002", "method": "\u9996\u5148\u63d0\u793a LLM \u4ece\u95ee\u9898\u5206\u89e3\u4e2d\u521b\u5efa\u65b9\u7a0b\uff0c\u7136\u540e\u4f7f\u7528\u5916\u90e8\u7b26\u53f7\u65b9\u7a0b\u6c42\u89e3\u5668\u5f97\u5230\u7b54\u6848\u3002\u7136\u540e\uff0cLLM \u4f1a\u7b2c\u4e8c\u6b21\u89e3\u51b3\u8be5\u95ee\u9898\u4ee5\u4f30\u7b97\u6b63\u786e\u7b54\u6848\uff0c\u5e76\u5c06\u5176\u4e0e\u751f\u6210\u7b54\u6848\u8fdb\u884c\u6bd4\u8f83\u3002\u5982\u679c\u9a8c\u8bc1\u5931\u8d25\uff0c\u5219\u91c7\u7528\u8fed\u4ee3\u6821\u6b63\u8fc7\u7a0b\u3002", "result": "\u5728\u6570\u503c\u548c\u4ee3\u6570 MWP \u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6210\u679c\uff0c\u5e73\u5747\u63d0\u9ad8\u4e86\u8fd1 2%\u3002\u5728\u4e09\u89d2\u51fd\u6570 MWP \u4e0a\u53d6\u5f97\u4e86\u4ee4\u4eba\u6ee1\u610f\u7684\u7ed3\u679c\u3002\u5f15\u5165\u4e86 SVAMPClean \u548c Trig300 \u4e24\u4e2a\u65b0\u6570\u636e\u96c6\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u89e3\u51b3\u6570\u5b66\u5e94\u7528\u9898\u65b9\u9762\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6210\u679c\uff0c\u5e76\u4e3a\u8be5\u9886\u57df\u505a\u51fa\u4e86\u8d21\u732e\u3002"}}
{"id": "2509.18588", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18588", "abs": "https://arxiv.org/abs/2509.18588", "authors": ["Jiarui Jin", "Haoyu Wang", "Xiang Lan", "Jun Li", "Gaofeng Cheng", "Hongyan Li", "Shenda Hong"], "title": "UniECG: Understanding and Generating ECG in One Unified Model", "comment": null, "summary": "Recent unified models such as GPT-5 have achieved encouraging progress on\nvision-language tasks. However, these unified models typically fail to\ncorrectly understand ECG signals and provide accurate medical diagnoses, nor\ncan they correctly generate ECG signals. To address these limitations, we\npropose UniECG, the first unified model for ECG capable of concurrently\nperforming evidence-based ECG interpretation and text-conditioned ECG\ngeneration tasks. Through a decoupled two-stage training approach, the model\nfirst learns evidence-based interpretation skills (ECG-to-Text), and then\ninjects ECG generation capabilities (Text-to-ECG) via latent space alignment.\nUniECG can autonomously choose to interpret or generate an ECG based on user\ninput, significantly extending the capability boundaries of current ECG models.\nOur code and checkpoints will be made publicly available at\nhttps://github.com/PKUDigitalHealth/UniECG upon acceptance.", "AI": {"tldr": "UniECG\u662f\u9996\u4e2a\u80fd\u591f\u540c\u65f6\u8fdb\u884c\u5faa\u8bc1\u5fc3\u7535\u56fe\u89e3\u91ca\u548c\u6587\u672c\u6761\u4ef6\u5fc3\u7535\u56fe\u751f\u6210\u7684\u7edf\u4e00\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u7684\u7edf\u4e00\u6a21\u578b\u5728\u7406\u89e3\u5fc3\u7535\u56fe\u4fe1\u53f7\u548c\u751f\u6210\u5fc3\u7535\u56fe\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u65e0\u6cd5\u63d0\u4f9b\u51c6\u786e\u7684\u533b\u5b66\u8bca\u65ad\u3002", "method": "UniECG\u91c7\u7528\u89e3\u8026\u7684\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6cd5\uff1a\u9996\u5148\u5b66\u4e60\u5faa\u8bc1\u89e3\u91ca\uff08\u5fc3\u7535\u56fe\u5230\u6587\u672c\uff09\uff0c\u7136\u540e\u901a\u8fc7\u6f5c\u5728\u7a7a\u95f4\u5bf9\u9f50\u6ce8\u5165\u5fc3\u7535\u56fe\u751f\u6210\u80fd\u529b\uff08\u6587\u672c\u5230\u5fc3\u7535\u56fe\uff09\u3002", "result": "UniECG\u80fd\u591f\u6839\u636e\u7528\u6237\u8f93\u5165\u81ea\u4e3b\u9009\u62e9\u89e3\u91ca\u6216\u751f\u6210\u5fc3\u7535\u56fe\uff0c\u6269\u5c55\u4e86\u5f53\u524d\u5fc3\u7535\u56fe\u6a21\u578b\u7684\u80fd\u529b\u8fb9\u754c\u3002", "conclusion": "UniECG\u5728\u5fc3\u7535\u56fe\u89e3\u91ca\u548c\u751f\u6210\u65b9\u9762\u53d6\u5f97\u4e86\u7a81\u7834\uff0c\u4e3a\u5fc3\u7535\u56fe\u5206\u6790\u548c\u5e94\u7528\u5f00\u8f9f\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2509.19121", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2509.19121", "abs": "https://arxiv.org/abs/2509.19121", "authors": ["Weiru Chen", "John C. Thomas", "Yihuang Xiong", "Zhuohang Yu", "Da Zhou", "Shalini Kumari", "Zhongwei Dai", "Joshua A. Robinson", "Mauricio Terrones", "Archana Raja", "Sin\u00e9ad Griffin", "Alexander Weber-Bargioni", "Geoffroy Hautier"], "title": "First principles and scanning tunneling spectroscopical evidences for thermodynamically stable \"on-top\" sulfur divacancy in monolayer WS$_{2}$", "comment": null, "summary": "Chalcogen vacancies in monolayer transition metal dichalcogenides (TMDs),\nsuch as WS$_{2}$, play a crucial role in various applications ranging from\noptoelectronics and catalysis to quantum information science (QIS), making\ntheir identification and control essential. This study focuses on WS$_{2}$\nsingle vacancy and vacancy pairs. Using first principles computations, we\ninvestigate their thermodynamic stabilities and electronic structures. We\nidentify an \"on-top\" divacancy configuration where two vacancies sit on top of\neach other to be the only energetically stable complex with a binding energy of\n160 meV. We compute a small difference in electronic structure with a shift of\nthe unoccupied state by 140 meV for the divacancy complex and observe\nelectronic state shift during Scanning Tunneling Spectroscopy of a series of\nvacancy in WS$_2$ providing spectroscopical evidence for the presence of this\ndefect.", "AI": {"tldr": "WS2\u5355\u5c42\u6750\u6599\u4e2d\u7684\u786b\u7a7a\u4f4d\u5bf9\u5149\u7535\u5b50\u5b66\u3001\u50ac\u5316\u548c\u91cf\u5b50\u4fe1\u606f\u79d1\u5b66\u7b49\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002\u672c\u7814\u7a76\u4f7f\u7528\u7b2c\u4e00\u6027\u539f\u7406\u8ba1\u7b97\uff0c\u91cd\u70b9\u7814\u7a76\u4e86WS2\u5355\u7a7a\u4f4d\u548c\u53cc\u7a7a\u4f4d\u7684\u70ed\u529b\u5b66\u7a33\u5b9a\u6027\u548c\u7535\u5b50\u7ed3\u6784\u3002", "motivation": "\u4e3a\u4e86\u8bc6\u522b\u548c\u63a7\u5236WS2\u5355\u5c42\u6750\u6599\u4e2d\u7684\u786b\u7a7a\u4f4d\uff0c\u672c\u7814\u7a76\u5bf9\u5176\u8fdb\u884c\u4e86\u6df1\u5165\u63a2\u7a76\u3002", "method": "\u5229\u7528\u7b2c\u4e00\u6027\u539f\u7406\u8ba1\u7b97\uff0c\u7814\u7a76\u4e86WS2\u5355\u7a7a\u4f4d\u548c\u53cc\u7a7a\u4f4d\u7684\u70ed\u529b\u5b66\u7a33\u5b9a\u6027\u548c\u7535\u5b50\u7ed3\u6784\u3002", "result": "\u53d1\u73b0\u4e86\u4e00\u4e2a\"on-top\"\u7684\u4e8c\u91cd\u7a7a\u4f4d\u7ed3\u6784\uff0c\u5176\u7ed3\u5408\u80fd\u4e3a160 meV\uff0c\u662f\u552f\u4e00\u80fd\u91cf\u4e0a\u7a33\u5b9a\u7684\u590d\u5408\u7269\u3002\u8be5\u4e8c\u91cd\u7a7a\u4f4d\u590d\u5408\u7269\u7684\u7535\u5b50\u7ed3\u6784\u4e0e\u5355\u7a7a\u4f4d\u76f8\u6bd4\uff0c\u672a\u5360\u636e\u6001\u79fb\u52a8\u4e86140 meV\u3002\u901a\u8fc7\u626b\u63cf\u96a7\u9053\u8c31\u6280\u672f\u89c2\u6d4b\u5230WS2\u4e2d\u4e00\u7cfb\u5217\u7a7a\u4f4d\u7684\u7535\u5b50\u6001\u79fb\u52a8\uff0c\u4e3a\u4e8c\u91cd\u7a7a\u4f4d\u7f3a\u9677\u7684\u5b58\u5728\u63d0\u4f9b\u4e86\u5149\u8c31\u5b66\u8bc1\u636e\u3002", "conclusion": "WS2\u5355\u5c42\u6750\u6599\u4e2d\u7684\"on-top\"\u4e8c\u91cd\u7a7a\u4f4d\u662f\u80fd\u91cf\u4e0a\u7a33\u5b9a\u7684\u7f3a\u9677\uff0c\u5176\u7535\u5b50\u7ed3\u6784\u7684\u6539\u53d8\u53ef\u4ee5\u901a\u8fc7\u626b\u63cf\u96a7\u9053\u8c31\u6280\u672f\u8fdb\u884c\u89c2\u6d4b\u3002"}}
{"id": "2509.18388", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18388", "abs": "https://arxiv.org/abs/2509.18388", "authors": ["Binhua Huang", "Ni Wang", "Wendong Yao", "Soumyabrata Dev"], "title": "MVP: Motion Vector Propagation for Zero-Shot Video Object Detection", "comment": "5 pages, 1 figure", "summary": "Running a large open-vocabulary (Open-vocab) detector on every video frame is\naccurate but expensive. We introduce a training-free pipeline that invokes\nOWLv2 only on fixed-interval keyframes and propagates detections to\nintermediate frames using compressed-domain motion vectors (MV). A simple 3x3\ngrid aggregation of motion vectors provides translation and uniform-scale\nupdates, augmented with an area-growth check and an optional single-class\nswitch. The method requires no labels, no fine-tuning, and uses the same prompt\nlist for all open-vocabulary methods. On ILSVRC2015-VID (validation dataset),\nour approach (MVP) attains mAP@0.5=0.609 and mAP@[0.5:0.95]=0.316. At loose\nintersection-over-union (IoU) thresholds it remains close to framewise\nOWLv2-Large (0.747/0.721 at 0.2/0.3 versus 0.784/0.780), reflecting that coarse\nlocalization is largely preserved. Under the same keyframe schedule, MVP\noutperforms tracker-based propagation (MOSSE, KCF, CSRT) at mAP@0.5. A\nsupervised reference (YOLOv12x) reaches 0.631 at mAP@0.5 but requires labeled\ntraining, whereas our method remains label-free and open-vocabulary. These\nresults indicate that compressed-domain propagation is a practical way to\nreduce detector invocations while keeping strong zero-shot coverage in videos.\nOur code and models are available at https://github.com/microa/MVP.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMVP\u7684\u8bad\u7ec3\u65e0\u5173\u578b\u89c6\u9891\u76ee\u6807\u68c0\u6d4b\u6d41\u6c34\u7ebf\uff0c\u901a\u8fc7\u5728\u5173\u952e\u5e27\u4e0a\u8c03\u7528OWLv2\uff0c\u5e76\u5229\u7528\u538b\u7f29\u57df\u8fd0\u52a8\u77e2\u91cf\u5728\u4e2d\u95f4\u5e27\u4f20\u64ad\u68c0\u6d4b\u6846\uff0c\u4ee5\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u5f00\u653e\u8bcd\u6c47\u68c0\u6d4b\u80fd\u529b\u3002", "motivation": "\u5728\u89c6\u9891\u4e0a\u8fd0\u884c\u5f00\u653e\u8bcd\u6c47\u68c0\u6d4b\u5668\u867d\u7136\u51c6\u786e\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "MVP\u6d41\u6c34\u7ebf\u5728\u56fa\u5b9a\u95f4\u9694\u7684\u5173\u952e\u5e27\u4e0a\u8c03\u7528OWLv2\uff0c\u5e76\u5229\u7528\u538b\u7f29\u57df\u8fd0\u52a8\u77e2\u91cf\uff08MV\uff09\u5c06\u68c0\u6d4b\u6846\u4f20\u64ad\u5230\u4e2d\u95f4\u5e27\u3002\u901a\u8fc73x3\u7f51\u683c\u805a\u5408\u8fd0\u52a8\u77e2\u91cf\u8fdb\u884c\u5e73\u79fb\u548c\u5c3a\u5ea6\u66f4\u65b0\uff0c\u5e76\u8f85\u4ee5\u9762\u79ef\u589e\u957f\u68c0\u67e5\u548c\u53ef\u9009\u7684\u5355\u7c7b\u522b\u5207\u6362\u3002\u8be5\u65b9\u6cd5\u4e0d\u9700\u8981\u6807\u7b7e\u6216\u5fae\u8c03\uff0c\u5e76\u53ef\u7528\u4e8e\u6240\u6709\u5f00\u653e\u8bcd\u6c47\u68c0\u6d4b\u5668\u3002", "result": "\u5728ILSVRC2015-VID\u9a8c\u8bc1\u96c6\u4e0a\uff0cMVP\u8fbe\u5230\u4e86mAP@0.5=0.609\u548cmAP@[0.5:0.95]=0.316\u3002\u5728\u8f83\u4f4e\u7684IoU\u9608\u503c\u4e0b\uff0c\u5176\u6027\u80fd\u63a5\u8fd1\u9010\u5e27OWLv2-Large\uff0c\u8868\u660e\u7c97\u7565\u5b9a\u4f4d\u5f97\u5230\u4fdd\u7559\u3002\u4e0e\u57fa\u4e8e\u8ddf\u8e2a\u5668\u7684\u4f20\u64ad\u65b9\u6cd5\uff08MOSSE, KCF, CSRT\uff09\u76f8\u6bd4\uff0cMVP\u5728mAP@0.5\u4e0a\u8868\u73b0\u66f4\u4f18\u3002\u4e0e\u9700\u8981\u6807\u7b7e\u8bad\u7ec3\u7684YOLOv12x\u76f8\u6bd4\uff0cMVP\u5728\u4fdd\u6301\u5f00\u653e\u8bcd\u6c47\u548c\u65e0\u6807\u7b7e\u7279\u6027\u7684\u540c\u65f6\uff0c\u53d6\u5f97\u4e86\u63a5\u8fd1\u7684\u6027\u80fd\u3002", "conclusion": "\u538b\u7f29\u57df\u4f20\u64ad\u662f\u4e00\u79cd\u5b9e\u7528\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u964d\u4f4e\u68c0\u6d4b\u5668\u8c03\u7528\u9891\u7387\u7684\u540c\u65f6\uff0c\u5728\u89c6\u9891\u4e2d\u4fdd\u6301\u5f3a\u5927\u7684\u96f6\u6837\u672c\u68c0\u6d4b\u80fd\u529b\u3002"}}
{"id": "2509.18134", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.18134", "abs": "https://arxiv.org/abs/2509.18134", "authors": ["Furan Xie", "Bing Liu", "Li Chai"], "title": "A Weighted Gradient Tracking Privacy-Preserving Method for Distributed Optimization", "comment": null, "summary": "This paper investigates the privacy-preserving distributed optimization\nproblem, aiming to protect agents' private information from potential attackers\nduring the optimization process. Gradient tracking, an advanced technique for\nimproving the convergence rate in distributed optimization, has been applied to\nmost first-order algorithms in recent years. We first reveal the inherent\nprivacy leakage risk associated with gradient tracking. Building upon this\ninsight, we propose a weighted gradient tracking distributed privacy-preserving\nalgorithm, eliminating the privacy leakage risk in gradient tracking using\ndecaying weight factors. Then, we characterize the convergence of the proposed\nalgorithm under time-varying heterogeneous step sizes. We prove the proposed\nalgorithm converges precisely to the optimal solution under mild assumptions.\nFinally, numerical simulations validate the algorithm's effectiveness through a\nclassical distributed estimation problem and the distributed training of a\nconvolutional neural network.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a0\u6743\u68af\u5ea6\u8ddf\u8e2a\u5206\u5e03\u5f0f\u9690\u79c1\u4fdd\u62a4\u7b97\u6cd5\uff0c\u901a\u8fc7\u4f7f\u7528\u8870\u51cf\u6743\u91cd\u56e0\u5b50\u6d88\u9664\u68af\u5ea6\u8ddf\u8e2a\u4e2d\u7684\u9690\u79c1\u6cc4\u9732\u98ce\u9669\uff0c\u5e76\u8bc1\u660e\u4e86\u8be5\u7b97\u6cd5\u5728\u65f6\u53d8\u5f02\u6784\u6b65\u957f\u4e0b\u7684\u6536\u655b\u6027\u3002", "motivation": "\u5728\u5206\u5e03\u5f0f\u4f18\u5316\u95ee\u9898\u4e2d\uff0c\u4fdd\u62a4\u4ee3\u7406\u7684\u79c1\u6709\u4fe1\u606f\u514d\u53d7\u6f5c\u5728\u653b\u51fb\u8005\u7684\u4fb5\u5bb3\u81f3\u5173\u91cd\u8981\u3002\u68af\u5ea6\u8ddf\u8e2a\u6280\u672f\u867d\u7136\u80fd\u63d0\u9ad8\u6536\u655b\u901f\u5ea6\uff0c\u4f46\u4e5f\u5b58\u5728\u56fa\u6709\u7684\u9690\u79c1\u6cc4\u9732\u98ce\u9669\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u52a0\u6743\u68af\u5ea6\u8ddf\u8e2a\u5206\u5e03\u5f0f\u9690\u79c1\u4fdd\u62a4\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u4f7f\u7528\u8870\u51cf\u6743\u91cd\u56e0\u5b50\u6765\u6d88\u9664\u68af\u5ea6\u8ddf\u8e2a\u4e2d\u7684\u9690\u79c1\u6cc4\u9732\u98ce\u9669\uff0c\u5e76\u5206\u6790\u4e86\u8be5\u7b97\u6cd5\u5728\u65f6\u53d8\u5f02\u6784\u6b65\u957f\u4e0b\u7684\u6536\u655b\u6027\u3002", "result": "\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u6e29\u548c\u5047\u8bbe\u4e0b\u80fd\u591f\u7cbe\u786e\u6536\u655b\u5230\u6700\u4f18\u89e3\u3002\u901a\u8fc7\u5728\u5206\u5e03\u5f0f\u4f30\u8ba1\u95ee\u9898\u548c\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u5206\u5e03\u5f0f\u8bad\u7ec3\u4e2d\u7684\u6570\u503c\u6a21\u62df\uff0c\u9a8c\u8bc1\u4e86\u8be5\u7b97\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u52a0\u6743\u68af\u5ea6\u8ddf\u8e2a\u5206\u5e03\u5f0f\u9690\u79c1\u4fdd\u62a4\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u6d88\u9664\u68af\u5ea6\u8ddf\u8e2a\u4e2d\u7684\u9690\u79c1\u6cc4\u9732\u98ce\u9669\uff0c\u5e76\u4fdd\u8bc1\u7b97\u6cd5\u5728\u65f6\u53d8\u5f02\u6784\u6b65\u957f\u4e0b\u7684\u7cbe\u786e\u6536\u655b\u6027\u3002\u6570\u503c\u6a21\u62df\u7ed3\u679c\u4e5f\u9a8c\u8bc1\u4e86\u8be5\u7b97\u6cd5\u5728\u5b9e\u9645\u95ee\u9898\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2509.18633", "categories": ["cs.AI", "q-fin.RM"], "pdf": "https://arxiv.org/pdf/2509.18633", "abs": "https://arxiv.org/abs/2509.18633", "authors": ["Yara Mohajerani"], "title": "Adaptive Learning in Spatial Agent-Based Models for Climate Risk Assessment: A Geospatial Framework with Evolutionary Economic Agents", "comment": "Submitted and accepted to Tackling Climate Change with Machine\n  Learning workshop at NeurIPS 2025. 5 pages, 1 figure. Source code and\n  documentation available at\n  https://github.com/yaramohajerani/spatial-climate-ABM", "summary": "Climate risk assessment requires modelling complex interactions between\nspatially heterogeneous hazards and adaptive economic systems. We present a\nnovel geospatial agent-based model that integrates climate hazard data with\nevolutionary learning for economic agents. Our framework combines Mesa-based\nspatial modelling with CLIMADA climate impact assessment, introducing adaptive\nlearning behaviours that allow firms to evolve strategies for budget\nallocation, pricing, wages, and risk adaptation through fitness-based selection\nand mutation. We demonstrate the framework using riverine flood projections\nunder RCP8.5 until 2100, showing that evolutionary adaptation enables firms to\nconverge with baseline (no hazard) production levels after decades of\ndisruption due to climate stress. Our results reveal systemic risks where even\nagents that are not directly exposed to floods face impacts through supply\nchain disruptions, with the end-of-century average price of goods 5.6% higher\nunder RCP8.5 compared to the baseline. This open-source framework provides\nfinancial institutions and companies with tools to quantify both direct and\ncascading climate risks while evaluating cost-effective adaptation strategies.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u5408\u4e86\u5730\u7406\u7a7a\u95f4\u3001\u57fa\u4e8e\u4ee3\u7406\u7684\u6a21\u578b\u548c\u8fdb\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u6c14\u5019\u98ce\u9669\u5bf9\u7ecf\u6d4e\u7cfb\u7edf\u7684\u5f71\u54cd\u3002", "motivation": "\u4e3a\u4e86\u6a21\u62df\u6c14\u5019\u98ce\u9669\u8bc4\u4f30\u4e2d\u590d\u6742\u7684\u7a7a\u95f4\u5f02\u8d28\u6027\u5371\u5bb3\u4e0e\u9002\u5e94\u6027\u7ecf\u6d4e\u7cfb\u7edf\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u3002", "method": "\u5229\u7528 Mesa \u6846\u67b6\u8fdb\u884c\u7a7a\u95f4\u5efa\u6a21\uff0c\u7ed3\u5408 CLIMADA \u6c14\u5019\u5f71\u54cd\u8bc4\u4f30\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u9002\u5e94\u6027\u5b66\u4e60\u548c\u8fdb\u5316\u9009\u62e9\u7684\u7ecf\u6d4e\u4ee3\u7406\u884c\u4e3a\uff0c\u4ee5\u6a21\u62df\u9884\u7b97\u5206\u914d\u3001\u5b9a\u4ef7\u3001\u5de5\u8d44\u548c\u98ce\u9669\u9002\u5e94\u7b56\u7565\u7684\u6f14\u53d8\u3002", "result": "\u5728 RCP8.5 \u6cb3\u6d41\u6d2a\u6c34\u60c5\u666f\u4e0b\uff0c\u6a21\u578b\u663e\u793a\u8fdb\u5316\u9002\u5e94\u4f7f\u4f01\u4e1a\u80fd\u5728\u6570\u5341\u5e74\u540e\u6062\u590d\u5230\u57fa\u7ebf\u751f\u4ea7\u6c34\u5e73\u3002\u4f46\u7814\u7a76\u4e5f\u63ed\u793a\u4e86\u7cfb\u7edf\u6027\u98ce\u9669\uff0c\u7531\u4e8e\u4f9b\u5e94\u94fe\u4e2d\u65ad\uff0c\u5373\u4f7f\u662f\u672a\u76f4\u63a5\u66b4\u9732\u4e8e\u6d2a\u6c34\u7684\u4ee3\u7406\uff0c\u5176\u5546\u54c1\u5e73\u5747\u4ef7\u683c\u4e5f\u6bd4\u57fa\u7ebf\u9ad8\u51fa 5.6%\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u5f00\u6e90\u6846\u67b6\u80fd\u591f\u91cf\u5316\u76f4\u63a5\u548c\u95f4\u63a5\u7684\u6c14\u5019\u98ce\u9669\uff0c\u5e76\u8bc4\u4f30\u5177\u6709\u6210\u672c\u6548\u76ca\u7684\u9002\u5e94\u7b56\u7565\uff0c\u4e3a\u91d1\u878d\u673a\u6784\u548c\u4f01\u4e1a\u63d0\u4f9b\u51b3\u7b56\u652f\u6301\u3002"}}
{"id": "2509.18632", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18632", "abs": "https://arxiv.org/abs/2509.18632", "authors": ["Nishant Balepur", "Matthew Shu", "Yoo Yeon Sung", "Seraphina Goldfarb-Tarrant", "Shi Feng", "Fumeng Yang", "Rachel Rudinger", "Jordan Lee Boyd-Graber"], "title": "A Good Plan is Hard to Find: Aligning Models with Preferences is Misaligned with What Helps Users", "comment": "EMNLP 2025", "summary": "To assist users in complex tasks, LLMs generate plans: step-by-step\ninstructions towards a goal. While alignment methods aim to ensure LLM plans\nare helpful, they train (RLHF) or evaluate (ChatbotArena) on what users prefer,\nassuming this reflects what helps them. We test this with Planorama: an\ninterface where 126 users answer 300 multi-step questions with LLM plans. We\nget 4388 plan executions and 5584 comparisons to measure plan helpfulness (QA\nsuccess) and user preferences on plans, and recreate the setup in agents and\nreward models to see if they simulate or prefer what helps users. We expose: 1)\nuser/model preferences and agent success do not accurately predict which plans\nhelp users, so common alignment feedback can misalign with helpfulness; 2) this\ngap is not due to user-specific preferences, as users are similarly successful\nwhen using plans they prefer/disprefer; 3) surface-level cues like brevity and\nquestion similarity strongly link to preferences, but such biases fail to\npredict helpfulness. In all, we argue aligning helpful LLMs needs feedback from\nreal user interactions, not just preferences of what looks helpful, so we\ndiscuss the plan NLP researchers can execute to solve this problem.", "AI": {"tldr": "\u7528\u6237\u504f\u597d\u4e0eLLM\u8ba1\u5212\u7684\u5b9e\u9645\u5e2e\u52a9\u6027\u4e0d\u7b26\uff0c\u9700\u8981\u771f\u5b9e\u7684\u7528\u6237\u4ea4\u4e92\u53cd\u9988\u6765\u5b9e\u73b0\u5bf9\u9f50\u3002", "motivation": "\u76ee\u524d\u7684LLM\u5bf9\u9f50\u65b9\u6cd5\uff08\u5982RLHF\u548cChatbot Arena\uff09\u5047\u8bbe\u7528\u6237\u504f\u597d\u80fd\u53cd\u6620\u8ba1\u5212\u7684\u6709\u6548\u6027\uff0c\u4f46\u8fd9\u79cd\u5047\u8bbe\u672a\u7ecf\u68c0\u9a8c\u3002", "method": "\u901a\u8fc7\u540d\u4e3aPlanorama\u7684\u754c\u9762\uff0c\u8ba9126\u540d\u7528\u6237\u56de\u7b54300\u4e2a\u591a\u6b65\u9aa4\u95ee\u9898\uff0c\u5e76\u4f7f\u7528LLM\u751f\u6210\u7684\u8ba1\u5212\u3002\u6536\u96c6\u4e864388\u4e2a\u8ba1\u5212\u6267\u884c\u548c5584\u4e2a\u6bd4\u8f83\u6570\u636e\uff0c\u4ee5\u8861\u91cf\u8ba1\u5212\u7684\u6709\u6548\u6027\uff08\u95ee\u7b54\u6210\u529f\u7387\uff09\u548c\u7528\u6237\u504f\u597d\u3002\u540c\u65f6\uff0c\u5728\u6a21\u62df\u73af\u5883\u4e2d\u590d\u73b0\u4e86\u8be5\u8bbe\u7f6e\uff0c\u4ee5\u8bc4\u4f30\u4ee3\u7406\u548c\u5956\u52b1\u6a21\u578b\u662f\u5426\u80fd\u6a21\u62df\u6216\u504f\u597d\u7528\u6237\u771f\u6b63\u9700\u8981\u7684\u8ba1\u5212\u3002", "result": "1. \u7528\u6237\u504f\u597d\u3001\u6a21\u578b\u504f\u597d\u548c\u4ee3\u7406\u6210\u529f\u7387\u4e0d\u80fd\u51c6\u786e\u9884\u6d4b\u54ea\u4e9b\u8ba1\u5212\u5bf9\u7528\u6237\u6709\u5e2e\u52a9\uff0c\u8868\u660e\u5e38\u89c1\u7684\u5bf9\u9f50\u53cd\u9988\u53ef\u80fd\u4e0e\u5b9e\u9645\u5e2e\u52a9\u6027\u4ea7\u751f\u504f\u5dee\u3002 2. \u8fd9\u79cd\u504f\u5dee\u5e76\u975e\u6e90\u4e8e\u7528\u6237\u4e2a\u4f53\u504f\u597d\uff0c\u56e0\u4e3a\u7528\u6237\u5728\u4f7f\u7528\u4ed6\u4eec\u504f\u597d\u6216\u4e0d\u504f\u597d\u7684\u8ba1\u5212\u65f6\uff0c\u6210\u529f\u7387\u6ca1\u6709\u663e\u8457\u5dee\u5f02\u3002 3. \u8868\u9762\u7ebf\u7d22\uff08\u5982\u7b80\u6d01\u6027\u3001\u95ee\u9898\u76f8\u4f3c\u6027\uff09\u4e0e\u7528\u6237\u504f\u597d\u9ad8\u5ea6\u76f8\u5173\uff0c\u4f46\u8fd9\u4e9b\u7ebf\u7d22\u4e0d\u80fd\u9884\u6d4b\u8ba1\u5212\u7684\u5b9e\u9645\u5e2e\u52a9\u6027\u3002", "conclusion": "\u4e3a\u4e86\u5b9e\u73b0\u6709\u5e2e\u52a9\u7684LLM\uff0c\u9700\u8981\u4f9d\u8d56\u771f\u5b9e\u7528\u6237\u4ea4\u4e92\u7684\u53cd\u9988\uff0c\u800c\u4e0d\u662f\u4ec5\u4ec5\u4f9d\u8d56\u90a3\u4e9b\u770b\u8d77\u6765\u6709\u5e2e\u52a9\u7684\u504f\u597d\u3002\u6587\u7ae0\u6700\u540e\u8ba8\u8bba\u4e86NLP\u7814\u7a76\u8005\u53ef\u4ee5\u91c7\u53d6\u7684\u89e3\u51b3\u6b64\u95ee\u9898\u7684\u65b9\u6cd5\u3002"}}
{"id": "2509.19132", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2509.19132", "abs": "https://arxiv.org/abs/2509.19132", "authors": ["Jacob T. Sivak", "R. Jackson Spurling", "Jon-Paul Maria", "Susan B. Sinnott"], "title": "Exploring Cation Selection and Disorder within Entropy-Driven $A_{6}B_{2}$O$_{17}$ ($A$=Zr/Hf, $B$=Nb/Ta) Oxides", "comment": null, "summary": "We investigate the local atomic and electronic structure, thermodynamic\nstability, and defect chemistry of $A_{6}B_{2}$O$_{17}$ ($A$ = Zr/Hf, $B$ =\nNb/Ta) oxides using first-principles density functional theory (DFT)\ncalculations. We examine both ordered unit cells as well as fully disordered\nspecial quasirandom structures to clearly discern the effects of cation\ndisorder. Structural predictions align closely with previous experimental\nresults and follow established ionic radii trends. The electronic structure is\nstrongly dependent on $B$-cation species: $A_{6}$Ta$_{2}$O$_{17}$ compositions\nhave ~30% larger band gaps than their $A_{6}$Nb$_{2}$O$_{17}$ counterparts.\nDefect chemistry is similar for all compositions, with anion vacancies being\nmore energetically favorable than corresponding cation defects. All explored\n$A_{6}B_{2}$O$_{17}$ compositions are enthalpically unstable with respect to\ntheir $A$O$_{2}$ and $B_{2}$O$_{5}$ competing oxides and are therefore\nclassified as entropy-stabilized materials, supporting prior experimental\nresults. The pronounced agreement between our disordered supercell predictions\nwith experimental measurements indicates all explored $A_{6}B_{2}$O$_{17}$\ncompositions contain substantial cation disorder across all 6-, 7-, and\n8-coordinated sites. Our findings collectively provide a fundamental\nunderstanding of the $A_{6}B_{2}$O$_{17}$ material family through DFT\ncalculations, establishing a framework for future compositional tuning to\nengineer targeted material properties.", "AI": {"tldr": "DFT\u8ba1\u7b97\u7814\u7a76\u4e86A6B2O17\uff08A=Zr/Hf\uff0cB=Nb/Ta\uff09\u6c27\u5316\u7269\u7684\u7ed3\u6784\u3001\u7a33\u5b9a\u6027\u548c\u7f3a\u9677\u5316\u5b66\uff0c\u53d1\u73b0Ta\u5316\u5408\u7269\u7684\u5e26\u9699\u6bd4Nb\u5316\u5408\u7269\u5927\uff0c\u5e76\u4e14\u6240\u6709\u6210\u5206\u90fd\u503e\u5411\u4e8e\u5f62\u6210\u6c27\u7a7a\u4f4d\uff0c\u4f46\u4e0eAO2\u548cB2O5\u76f8\u6bd4\uff0c\u5b83\u4eec\u5728\u7113\u4e0a\u4e0d\u7a33\u5b9a\uff0c\u5c5e\u4e8e\u71b5\u7a33\u5b9a\u6750\u6599\u3002", "motivation": "\u7814\u7a76A6B2O17\uff08A=Zr/Hf\uff0cB=Nb/Ta\uff09\u6c27\u5316\u7269\u7684\u5c40\u90e8\u539f\u5b50\u548c\u7535\u5b50\u7ed3\u6784\u3001\u70ed\u529b\u5b66\u7a33\u5b9a\u6027\u548c\u7f3a\u9677\u5316\u5b66\uff0c\u4ee5\u4e86\u89e3\u5176\u6027\u8d28\u5e76\u4e3a\u672a\u6765\u6210\u5206\u8c03\u6574\u63d0\u4f9b\u57fa\u7840\u3002", "method": "\u4f7f\u7528\u7b2c\u4e00\u6027\u539f\u7406\u5bc6\u5ea6\u6cdb\u51fd\u7406\u8bba\uff08DFT\uff09\u8ba1\u7b97\uff0c\u7814\u7a76\u6709\u5e8f\u548c\u5168\u65e0\u5e8f\u7684\u7279\u6b8a\u7c7b\u968f\u673a\u7ed3\u6784\uff0c\u5e76\u4e0e\u5b9e\u9a8c\u7ed3\u679c\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u7ed3\u6784\u9884\u6d4b\u4e0e\u5b9e\u9a8c\u7ed3\u679c\u4e00\u81f4\u3002\u7535\u5b50\u7ed3\u6784\u5f3a\u70c8\u4f9d\u8d56\u4e8eB\u9633\u79bb\u5b50\u79cd\u7c7b\uff1aA6Ta2O17\u7684\u5e26\u9699\u6bd4A6Nb2O17\u5927~30%\u3002\u7f3a\u9677\u5316\u5b66\u76f8\u4f3c\uff0c\u6c27\u7a7a\u4f4d\u6bd4\u9633\u79bb\u5b50\u7f3a\u9677\u66f4\u6709\u5229\u3002\u6240\u6709A6B2O17\u6210\u5206\u76f8\u5bf9\u4e8eAO2\u548cB2O5\u662f\u7113\u4e0d\u7a33\u5b9a\u7684\uff0c\u5c5e\u4e8e\u71b5\u7a33\u5b9a\u6750\u6599\u3002\u65e0\u5e8f\u8d85\u6676\u80de\u9884\u6d4b\u4e0e\u5b9e\u9a8c\u6d4b\u91cf\u7ed3\u679c\u543b\u5408\uff0c\u8868\u660e\u6240\u6709A6B2O17\u6210\u5206\u5728\u6240\u6709\u914d\u4f4d\u70b9\u4e0a\u90fd\u5b58\u5728\u663e\u8457\u7684\u9633\u79bb\u5b50\u65e0\u5e8f\u3002", "conclusion": "DFT\u8ba1\u7b97\u4e3aA6B2O17\u6750\u6599\u5bb6\u65cf\u63d0\u4f9b\u4e86\u57fa\u7840\u7406\u89e3\uff0c\u5e76\u5efa\u7acb\u4e86\u672a\u6765\u901a\u8fc7\u6210\u5206\u8c03\u6574\u6765\u8bbe\u8ba1\u76ee\u6807\u6750\u6599\u7279\u6027\u7684\u6846\u67b6\u3002"}}
{"id": "2509.18671", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18671", "abs": "https://arxiv.org/abs/2509.18671", "authors": ["Kaixin Chai", "Hyunjun Lee", "Joseph J. Lim"], "title": "N2M: Bridging Navigation and Manipulation by Learning Pose Preference from Rollout", "comment": null, "summary": "In mobile manipulation, the manipulation policy has strong preferences for\ninitial poses where it is executed. However, the navigation module focuses\nsolely on reaching the task area, without considering which initial pose is\npreferable for downstream manipulation. To address this misalignment, we\nintroduce N2M, a transition module that guides the robot to a preferable\ninitial pose after reaching the task area, thereby substantially improving task\nsuccess rates. N2M features five key advantages: (1) reliance solely on\nego-centric observation without requiring global or historical information; (2)\nreal-time adaptation to environmental changes; (3) reliable prediction with\nhigh viewpoint robustness; (4) broad applicability across diverse tasks,\nmanipulation policies, and robot hardware; and (5) remarkable data efficiency\nand generalizability. We demonstrate the effectiveness of N2M through extensive\nsimulation and real-world experiments. In the PnPCounterToCab task, N2M\nimproves the averaged success rate from 3% with the reachability-based baseline\nto 54%. Furthermore, in the Toybox Handover task, N2M provides reliable\npredictions even in unseen environments with only 15 data samples, showing\nremarkable data efficiency and generalizability.", "AI": {"tldr": "N2M\u662f\u4e00\u4e2a\u8fc7\u6e21\u6a21\u5757\uff0c\u901a\u8fc7\u5c06\u673a\u5668\u4eba\u5f15\u5bfc\u81f3\u4efb\u52a1\u533a\u57df\u5185\u66f4\u4f18\u7684\u521d\u59cb\u59ff\u6001\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u79fb\u52a8\u64cd\u4f5c\u4efb\u52a1\u7684\u6210\u529f\u7387\uff0c\u4ec5\u4f9d\u8d56\u4e8e\u4ee5\u81ea\u6211\u4e3a\u4e2d\u5fc3\u7684\u89c2\u5bdf\uff0c\u5e76\u4e14\u5177\u6709\u51fa\u8272\u7684\u6570\u636e\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u79fb\u52a8\u64cd\u4f5c\u4e2d\u7684\u6293\u53d6\u7b56\u7565\u901a\u5e38\u5bf9\u521d\u59cb\u59ff\u6001\u6709\u7279\u5b9a\u504f\u597d\uff0c\u4f46\u5bfc\u822a\u6a21\u5757\u53ea\u5173\u6ce8\u5230\u8fbe\u4efb\u52a1\u533a\u57df\uff0c\u5ffd\u7565\u4e86\u521d\u59cb\u59ff\u6001\u5bf9\u4e0b\u6e38\u64cd\u4f5c\u7684\u5f71\u54cd\uff0c\u5bfc\u81f4\u4efb\u52a1\u6210\u529f\u7387\u4e0d\u9ad8\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aN2M\u7684\u8fc7\u6e21\u6a21\u5757\uff0c\u8be5\u6a21\u5757\u5728\u673a\u5668\u4eba\u5230\u8fbe\u4efb\u52a1\u533a\u57df\u540e\uff0c\u80fd\u591f\u5f15\u5bfc\u5176\u8c03\u6574\u5230\u4e00\u4e2a\u66f4\u9002\u5408\u4e0b\u6e38\u6293\u53d6\u64cd\u4f5c\u7684\u521d\u59cb\u59ff\u6001\u3002N2M\u6a21\u5757\u7684\u7279\u70b9\u5305\u62ec\uff1a\u4ec5\u4f9d\u8d56\u4e8e\u4ee5\u81ea\u6211\u4e3a\u4e2d\u5fc3\u7684\u89c2\u5bdf\u3001\u5b9e\u65f6\u9002\u5e94\u73af\u5883\u53d8\u5316\u3001\u9ad8\u89c6\u70b9\u9c81\u68d2\u6027\u3001\u8de8\u4efb\u52a1/\u7b56\u7565/\u786c\u4ef6\u7684\u5e7f\u6cdb\u9002\u7528\u6027\u3001\u4ee5\u53ca\u51fa\u8272\u7684\u6570\u636e\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5728PnPCounterToCab\u4efb\u52a1\u4e2d\uff0cN2M\u5c06\u5e73\u5747\u6210\u529f\u7387\u4ece\u4ec5\u67093%\u7684\u57fa\u7ebf\u63d0\u9ad8\u523054%\u3002\u5728Toybox Handover\u4efb\u52a1\u4e2d\uff0cN2M\u5373\u4f7f\u5728\u4ec5\u670915\u4e2a\u6570\u636e\u6837\u672c\u7684\u672a\u77e5\u73af\u5883\u4e2d\u4e5f\u80fd\u63d0\u4f9b\u53ef\u9760\u7684\u9884\u6d4b\uff0c\u5c55\u793a\u4e86\u5176\u9ad8\u6548\u7684\u6570\u636e\u5229\u7528\u7387\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "N2M\u901a\u8fc7\u89e3\u51b3\u5bfc\u822a\u548c\u64cd\u4f5c\u4e4b\u95f4\u521d\u59cb\u59ff\u6001\u7684\u9519\u914d\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u79fb\u52a8\u64cd\u4f5c\u4efb\u52a1\u7684\u6210\u529f\u7387\uff0c\u5e76\u4e14\u5728\u6570\u636e\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2509.18390", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18390", "abs": "https://arxiv.org/abs/2509.18390", "authors": ["Zitian Zhang", "Joshua Urban Davis", "Jeanne Phuong Anh Vu", "Jiangtao Kuang", "Jean-Fran\u00e7ois Lalonde"], "title": "Improving the color accuracy of lighting estimation models", "comment": "Project page: https://lvsn.github.io/coloraccuracy", "summary": "Advances in high dynamic range (HDR) lighting estimation from a single image\nhave opened new possibilities for augmented reality (AR) applications.\nPredicting complex lighting environments from a single input image allows for\nthe realistic rendering and compositing of virtual objects. In this work, we\ninvestigate the color robustness of such methods -- an often overlooked yet\ncritical factor for achieving visual realism. While most evaluations conflate\ncolor with other lighting attributes (e.g., intensity, direction), we isolate\ncolor as the primary variable of interest. Rather than introducing a new\nlighting estimation algorithm, we explore whether simple adaptation techniques\ncan enhance the color accuracy of existing models. Using a novel HDR dataset\nfeaturing diverse lighting colors, we systematically evaluate several\nadaptation strategies. Our results show that preprocessing the input image with\na pre-trained white balance network improves color robustness, outperforming\nother strategies across all tested scenarios. Notably, this approach requires\nno retraining of the lighting estimation model. We further validate the\ngenerality of this finding by applying the technique to three state-of-the-art\nlighting estimation methods from recent literature.", "AI": {"tldr": "\u901a\u8fc7\u767d\u5e73\u8861\u9884\u5904\u7406\u63d0\u5347\u5355\u5f20\u56fe\u50cfHDR\u7167\u660e\u4f30\u8ba1\u7684\u989c\u8272\u9c81\u68d2\u6027\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u4e3a\u4e86\u5b9e\u73b0\u589e\u5f3a\u73b0\u5b9e\uff08AR\uff09\u5e94\u7528\u4e2d\u865a\u62df\u5bf9\u8c61\u7684\u903c\u771f\u6e32\u67d3\u548c\u5408\u6210\uff0c\u9700\u8981\u4ece\u5355\u5f20\u56fe\u50cf\u9884\u6d4b\u590d\u6742\u7684\u7167\u660e\u73af\u5883\u3002\u7136\u800c\uff0c\u989c\u8272\u9c81\u68d2\u6027\u662f\u5b9e\u73b0\u89c6\u89c9\u903c\u771f\u7684\u5173\u952e\u4e14\u5e38\u88ab\u5ffd\u89c6\u7684\u56e0\u7d20\u3002", "method": "\u672c\u7814\u7a76\u4e0d\u63d0\u51fa\u65b0\u7684\u7167\u660e\u4f30\u8ba1\u7b97\u6cd5\uff0c\u800c\u662f\u63a2\u7d22\u7b80\u5355\u7684\u9002\u5e94\u6280\u672f\u662f\u5426\u80fd\u589e\u5f3a\u73b0\u6709\u6a21\u578b\u7684\u989c\u8272\u51c6\u786e\u6027\u3002\u4f7f\u7528\u5305\u542b\u591a\u6837\u5316\u7167\u660e\u989c\u8272\u7684\u65b0\u578bHDR\u6570\u636e\u96c6\uff0c\u7cfb\u7edf\u5730\u8bc4\u4f30\u4e86\u51e0\u79cd\u9002\u5e94\u7b56\u7565\uff0c\u7279\u522b\u662f\u57fa\u4e8e\u9884\u8bad\u7ec3\u767d\u5e73\u8861\u7f51\u7edc\u7684\u8f93\u5165\u56fe\u50cf\u9884\u5904\u7406\u65b9\u6cd5\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u767d\u5e73\u8861\u7f51\u7edc\u5bf9\u8f93\u5165\u56fe\u50cf\u8fdb\u884c\u9884\u5904\u7406\uff0c\u80fd\u591f\u63d0\u9ad8\u989c\u8272\u9c81\u68d2\u6027\uff0c\u5e76\u4e14\u5728\u6240\u6709\u6d4b\u8bd5\u573a\u666f\u4e2d\u4f18\u4e8e\u5176\u4ed6\u7b56\u7565\u3002\u6b64\u65b9\u6cd5\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7167\u660e\u4f30\u8ba1\u6a21\u578b\u3002", "conclusion": "\u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u767d\u5e73\u8861\u7f51\u7edc\u5bf9\u8f93\u5165\u56fe\u50cf\u8fdb\u884c\u9884\u5904\u7406\u662f\u63d0\u5347\u5355\u5f20\u56fe\u50cfHDR\u7167\u660e\u4f30\u8ba1\u989c\u8272\u9c81\u68d2\u6027\u7684\u4e00\u79cd\u6709\u6548\u4e14\u901a\u7528\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u989c\u8272\u51c6\u786e\u6027\uff0c\u4e14\u4e0d\u5f71\u54cd\u73b0\u6709\u6a21\u578b\u7684\u6027\u80fd\u3002"}}
{"id": "2509.18756", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2509.18756", "abs": "https://arxiv.org/abs/2509.18756", "authors": ["Luca Bianchi", "Carlo Marconi", "Davide Bacco"], "title": "Bell state measurements in quantum optics: a review of recent progress and open challenges", "comment": "17 pages, 3 figures", "summary": "Bell state measurements, which project bipartite qubit systems onto the\nmaximally entangled Bell basis, are central to a wide range of quantum\ninformation processing tasks, including quantum teleportation, entanglement\nswapping, and fusion-gate quantum computation. In photonic quantum platforms,\nwhere information is encoded in optical degrees of freedom, the realization of\nefficient Bell state measurements is particularly challenging, especially when\nconstrained to linear optical elements. In this review, we provide a\ncomprehensive examination of existing proposals for implementing Bell state\nmeasurements, highlighting their fundamental limitations and the strategies\ndeveloped to overcome them. Additionally, we survey recent advances in Bell\nstate measurements for high-dimensional systems, an area of growing interest\ndue to its relevance in scalable quantum networks and high-capacity quantum\ncommunication.", "AI": {"tldr": "BMS are crucial for quantum information processing, but difficult to implement with linear optics in photonic platforms. This review examines existing proposals, their limitations, and strategies to overcome them, also surveying advances in high-dimensional systems for scalable quantum networks and high-capacity communication.", "motivation": "BMS are central to quantum information processing tasks like teleportation, entanglement swapping, and fusion-gate quantum computation. Efficient BMS are challenging in photonic platforms using linear optics.", "method": "The paper reviews existing proposals for implementing BMS in photonic platforms, discusses their fundamental limitations, and explores strategies to overcome them. It also surveys recent advances in BMS for high-dimensional systems.", "result": "The review provides a comprehensive examination of existing proposals for BMS, highlighting their limitations and strategies to overcome them. Recent advances in high-dimensional BMS are also surveyed.", "conclusion": "Efficient BMS, especially in high-dimensional systems, are crucial for advancing quantum networks and communication, despite the challenges posed by linear optics in photonic platforms."}}
{"id": "2509.18135", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18135", "abs": "https://arxiv.org/abs/2509.18135", "authors": ["Shaoxun Wang", "Xingjun Zhang", "Qianyang Li", "Jiawei Cao", "Zhendong Tan"], "title": "SDGF: Fusing Static and Multi-Scale Dynamic Correlations for Multivariate Time Series Forecasting", "comment": null, "summary": "Inter-series correlations are crucial for accurate multivariate time series\nforecasting, yet these relationships often exhibit complex dynamics across\ndifferent temporal scales. Existing methods are limited in modeling these\nmulti-scale dependencies and struggle to capture their intricate and evolving\nnature. To address this challenge, this paper proposes a novel Static-Dynamic\nGraph Fusion network (SDGF), whose core lies in capturing multi-scale\ninter-series correlations through a dual-path graph structure learning\napproach. Specifically, the model utilizes a static graph based on prior\nknowledge to anchor long-term, stable dependencies, while concurrently\nemploying Multi-level Wavelet Decomposition to extract multi-scale features for\nconstructing an adaptively learned dynamic graph to capture associations at\ndifferent scales. We design an attention-gated module to fuse these two\ncomplementary sources of information intelligently, and a multi-kernel dilated\nconvolutional network is then used to deepen the understanding of temporal\npatterns. Comprehensive experiments on multiple widely used real-world\nbenchmark datasets demonstrate the effectiveness of our proposed model.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9759\u6001-\u52a8\u6001\u56fe\u878d\u5408\u7f51\u7edc\uff08SDGF\uff09\uff0c\u901a\u8fc7\u53cc\u8def\u5f84\u56fe\u7ed3\u6784\u5b66\u4e60\u65b9\u6cd5\u6765\u6355\u6349\u591a\u5c3a\u5ea6\u65f6\u95f4\u5e8f\u5217\u95f4\u7684\u76f8\u5173\u6027\uff0c\u4ee5\u63d0\u9ad8\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5efa\u6a21\u591a\u5c3a\u5ea6\u4f9d\u8d56\u5173\u7cfb\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u96be\u4ee5\u6355\u6349\u5176\u590d\u6742\u548c\u4e0d\u65ad\u53d8\u5316\u7684\u6027\u8d28\u3002", "method": "SDGF\u6a21\u578b\u4f7f\u7528\u57fa\u4e8e\u5148\u9a8c\u77e5\u8bc6\u7684\u9759\u6001\u56fe\u6765\u951a\u5b9a\u957f\u671f\u7a33\u5b9a\u7684\u4f9d\u8d56\u5173\u7cfb\uff0c\u5e76\u5229\u7528\u591a\u7ea7\u5c0f\u6ce2\u5206\u89e3\u63d0\u53d6\u591a\u5c3a\u5ea6\u7279\u5f81\u6765\u6784\u5efa\u81ea\u9002\u5e94\u5b66\u4e60\u7684\u52a8\u6001\u56fe\uff0c\u4ee5\u6355\u6349\u4e0d\u540c\u5c3a\u5ea6\u7684\u5173\u8054\u6027\u3002\u901a\u8fc7\u6ce8\u610f\u529b\u95e8\u63a7\u6a21\u5757\u878d\u5408\u8fd9\u4e24\u79cd\u4fe1\u606f\uff0c\u5e76\u4f7f\u7528\u591a\u6838\u6269\u5f20\u5377\u79ef\u7f51\u7edc\u6765\u52a0\u6df1\u5bf9\u65f6\u95f4\u6a21\u5f0f\u7684\u7406\u89e3\u3002", "result": "\u5728\u591a\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u771f\u5b9e\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u6a21\u578b\u7684\u6709\u6548\u6027\u3002", "conclusion": "SDGF\u6a21\u578b\u80fd\u591f\u6709\u6548\u5730\u6355\u6349\u591a\u5c3a\u5ea6\u65f6\u95f4\u5e8f\u5217\u95f4\u7684\u76f8\u5173\u6027\uff0c\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u3002"}}
{"id": "2509.18667", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18667", "abs": "https://arxiv.org/abs/2509.18667", "authors": ["Qiao Xiao", "Hong Ting Tsang", "Jiaxin Bai"], "title": "TERAG: Token-Efficient Graph-Based Retrieval-Augmented Generation", "comment": "16 pages, 2 figures, 4 tables. Submitted to the 2026 18th\n  International Conference on Machine Learning and Computing (ICMLC 2026),\n  under review", "summary": "Graph-based Retrieval-augmented generation (RAG) has become a widely studied\napproach for improving the reasoning, accuracy, and factuality of Large\nLanguage Models. However, many existing graph-based RAG systems overlook the\nhigh cost associated with LLM token usage during graph construction, hindering\nlarge-scale adoption. To address this, we propose TERAG, a simple yet effective\nframework designed to build informative graphs at a significantly lower cost.\nInspired by HippoRAG, we incorporate Personalized PageRank (PPR) during the\nretrieval phase, and we achieve at least 80% of the accuracy of widely used\ngraph-based RAG methods while consuming only 3%-11% of the output tokens.", "AI": {"tldr": "TERAG\u662f\u4e00\u4e2a\u4f4e\u6210\u672c\u7684\u56fe\u6784\u5efa\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u4e2a\u6027\u5316PageRank\uff08PPR\uff09\u548c\u68c0\u7d22\uff0c\u5728\u4fdd\u8bc1\u9ad8\u51c6\u786e\u7387\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86LLM\u7684token\u6d88\u8017\u3002", "motivation": "\u73b0\u6709\u7684\u56fe\u57fa\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u65b9\u6cd5\u5728\u56fe\u6784\u5efa\u8fc7\u7a0b\u4e2dLLM\u7684token\u6d88\u8017\u6210\u672c\u9ad8\uff0c\u9650\u5236\u4e86\u5176\u5927\u89c4\u6a21\u5e94\u7528\u3002TERAG\u65e8\u5728\u4ee5\u66f4\u4f4e\u7684\u6210\u672c\u6784\u5efa\u4fe1\u606f\u4e30\u5bcc\u7684\u56fe\u3002", "method": "TERAG\u6846\u67b6\u7ed3\u5408\u4e86HippoRAG\u7684\u601d\u8def\uff0c\u5728\u68c0\u7d22\u9636\u6bb5\u5f15\u5165\u4e2a\u6027\u5316PageRank\uff08PPR\uff09\uff0c\u4ece\u800c\u5728\u6784\u5efa\u56fe\u7684\u8fc7\u7a0b\u4e2d\u964d\u4f4etoken\u6d88\u8017\u3002", "result": "TERAG\u80fd\u591f\u8fbe\u5230\u73b0\u6709\u56fe\u57faRAG\u65b9\u6cd5\u768480%\u4ee5\u4e0a\u7684\u51c6\u786e\u7387\uff0c\u540c\u65f6\u4ec5\u6d88\u80173%-11%\u7684\u8f93\u51fatoken\u3002", "conclusion": "TERAG\u662f\u4e00\u4e2a\u5728\u6210\u672c\u6548\u76ca\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u56fe\u57faRAG\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u9ad8token\u6d88\u8017\u7684\u95ee\u9898\u3002"}}
{"id": "2509.18655", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18655", "abs": "https://arxiv.org/abs/2509.18655", "authors": ["Lingwen Deng", "Yifei Han", "Long Zhang", "Yue Du", "Bin Li"], "title": "Consistency-Aware Parameter-Preserving Knowledge Editing Framework for Multi-Hop Question Answering", "comment": "Submitted to ICASSP 2026", "summary": "Parameter-Preserving Knowledge Editing (PPKE) enables updating models with\nnew or corrected information without retraining or parameter adjustment. Recent\nPPKE approaches based on knowledge graphs (KG) to extend knowledge editing (KE)\ncapabilities to multi-hop question answering (MHQA). However, these methods\noften lack consistency, leading to knowledge contamination, unstable updates,\nand retrieval behaviors that fail to reflect the intended edits. Such\ninconsistencies undermine the reliability of PPKE in multi- hop reasoning. We\npresent CAPE-KG, Consistency-Aware Parameter-Preserving Editing with Knowledge\nGraphs, a novel consistency-aware framework for PPKE on MHQA. CAPE-KG ensures\nKG construction, update, and retrieval are always aligned with the requirements\nof the MHQA task, maintaining coherent reasoning over both unedited and edited\nknowledge. Extensive experiments on the MQuAKE benchmark show accuracy\nimprovements in PPKE performance for MHQA, demonstrating the effectiveness of\naddressing consistency in PPKE.", "AI": {"tldr": "CAPE-KG\u662f\u4e00\u4e2a\u4e00\u81f4\u6027\u611f\u77e5\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u591a\u8df3\u95ee\u7b54\uff08MHQA\uff09\u4e0a\u8fdb\u884c\u53c2\u6570\u4fdd\u7559\u77e5\u8bc6\u7f16\u8f91\uff08PPKE\uff09\uff0c\u901a\u8fc7\u786e\u4fdd\u77e5\u8bc6\u56fe\u8c31\uff08KG\uff09\u7684\u6784\u5efa\u3001\u66f4\u65b0\u548c\u68c0\u7d22\u4e0eMHQA\u4efb\u52a1\u7684\u8981\u6c42\u4fdd\u6301\u4e00\u81f4\uff0c\u6765\u89e3\u51b3\u73b0\u6709PPKE\u65b9\u6cd5\u4e2d\u7684\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86PPKE\u5728MHQA\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\uff08KG\uff09\u7684\u53c2\u6570\u4fdd\u7559\u77e5\u8bc6\u7f16\u8f91\uff08PPKE\uff09\u65b9\u6cd5\u5728\u591a\u8df3\u95ee\u7b54\uff08MHQA\uff09\u4efb\u52a1\u4e2d\u5b58\u5728\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u5bfc\u81f4\u77e5\u8bc6\u6c61\u67d3\u3001\u66f4\u65b0\u4e0d\u7a33\u5b9a\u4ee5\u53ca\u68c0\u7d22\u884c\u4e3a\u4e0e\u9884\u671f\u4e0d\u7b26\uff0c\u4ece\u800c\u524a\u5f31\u4e86PPKE\u5728\u591a\u8df3\u63a8\u7406\u4e2d\u7684\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51faCAPE-KG\uff08Consistency-Aware Parameter-Preserving Editing with Knowledge Graphs\uff09\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u786e\u4fdd\u77e5\u8bc6\u56fe\u8c31\uff08KG\uff09\u7684\u6784\u5efa\u3001\u66f4\u65b0\u548c\u68c0\u7d22\u4e0eMHQA\u4efb\u52a1\u7684\u8981\u6c42\u4fdd\u6301\u4e00\u81f4\uff0c\u6765\u7ef4\u62a4\u7f16\u8f91\u540e\u77e5\u8bc6\u548c\u672a\u7f16\u8f91\u77e5\u8bc6\u7684\u4e00\u81f4\u6027\u63a8\u7406\u3002", "result": "\u5728MQuAKE\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cCAPE-KG\u5728MHQA\u7684PPKE\u6027\u80fd\u65b9\u9762\u6709\u6240\u63d0\u9ad8\uff0c\u8bc1\u660e\u4e86\u89e3\u51b3PPKE\u4e00\u81f4\u6027\u95ee\u9898\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u89e3\u51b3PPKE\u5728\u591a\u8df3\u95ee\u7b54\u4e2d\u7684\u4e00\u81f4\u6027\u95ee\u9898\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u5176\u6027\u80fd\u3002"}}
{"id": "2509.19155", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2509.19155", "abs": "https://arxiv.org/abs/2509.19155", "authors": ["Dean Kos", "Marta Mas-Torrent"], "title": "Optical probing of charge traps in organic field-effect transistors", "comment": "11 pages, 5 figures", "summary": "We report spatially resolved optical probing of charge traps in organic\nfield-effect transistors using focussed laser illumination. By scanning a 635\nnm laser across the transistor channel and simultaneously acquiring transfer\ncharacteristics, we observe persistent, localised shifts in transistor turn-on\nvoltage correlated with illumination dose and position, with negligible impact\non field-effect mobility. The effect is strongest 5-10 um from the source\nelectrode and requires a drain-to-source scan direction with sub-10 um step\nsize. Kelvin probe force microscopy confirms trapped negative charges along the\nscan path, consistent with exciton dissociation and electron trapping near the\nsemiconductor-dielectric interface. The phenomenon is reproducible across\nmultiple device geometries and organic semiconductors, including\nTMTES-pentacene, TIPS-pentacene, and diF-TES-ADT. These findings enable direct\nmapping of trap distributions and suggest new strategies for trap engineering,\nthreshold voltage tuning, and development of organic optoelectronic memories.", "AI": {"tldr": "\u5229\u7528\u805a\u7126\u6fc0\u5149\u7167\u660e\u5bf9\u6709\u673a\u573a\u6548\u5e94\u6676\u4f53\u7ba1\u4e2d\u7684\u7535\u8377\u9677\u9631\u8fdb\u884c\u7a7a\u95f4\u5206\u8fa8\u5149\u5b66\u63a2\u6d4b\uff0c\u53d1\u73b0\u9677\u9631\u4e0e\u5149\u7167\u5242\u91cf\u548c\u4f4d\u7f6e\u76f8\u5173\uff0c\u5e76\u8bc1\u5b9e\u4e86\u5176\u8d1f\u7535\u8377\u7279\u6027\uff0c\u4e3a\u9677\u9631\u5de5\u7a0b\u548c\u5668\u4ef6\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "motivation": "\u4e3a\u4e86\u7814\u7a76\u6709\u673a\u573a\u6548\u5e94\u6676\u4f53\u7ba1\u4e2d\u7684\u7535\u8377\u9677\u9631\uff0c\u5e76\u63a2\u7d22\u5176\u7a7a\u95f4\u5206\u5e03\u548c\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u626b\u63cf\u805a\u7126\u6fc0\u5149\uff0c\u540c\u65f6\u8bb0\u5f55\u5668\u4ef6\u7684\u4f20\u8f93\u7279\u6027\u66f2\u7ebf\uff0c\u5e76\u7ed3\u5408\u5f00\u5c14\u6587\u63a2\u9488\u663e\u5fae\u955c\u6280\u672f\u6765\u63a2\u6d4b\u7535\u8377\u9677\u9631\u3002", "result": "\u89c2\u5bdf\u5230\u5668\u4ef6\u7684\u5f00\u542f\u7535\u538b\u51fa\u73b0\u6301\u7eed\u7684\u3001\u5c40\u57df\u5316\u7684\u504f\u79fb\uff0c\u4e14\u8be5\u504f\u79fb\u4e0e\u5149\u7167\u5242\u91cf\u548c\u4f4d\u7f6e\u76f8\u5173\u3002\u901a\u8fc7\u5f00\u5c14\u6587\u63a2\u9488\u663e\u5fae\u955c\u8bc1\u5b9e\u4e86\u626b\u63cf\u8def\u5f84\u4e0a\u5b58\u5728\u8d1f\u7535\u8377\u9677\u9631\u3002\u8be5\u73b0\u8c61\u5728\u591a\u79cd\u5668\u4ef6\u51e0\u4f55\u548c\u6709\u673a\u534a\u5bfc\u4f53\u6750\u6599\u4e2d\u5747\u53ef\u91cd\u73b0\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u5b9e\u73b0\u5bf9\u7535\u8377\u9677\u9631\u5206\u5e03\u7684\u76f4\u63a5\u7ed8\u5236\uff0c\u5e76\u4e3a\u9677\u9631\u5de5\u7a0b\u3001\u9608\u503c\u7535\u538b\u8c03\u8c10\u4ee5\u53ca\u6709\u673a\u5149\u7535\u5668\u4ef6\u5b58\u50a8\u5668\u7684\u5f00\u53d1\u63d0\u4f9b\u4e86\u65b0\u7684\u7b56\u7565\u3002"}}
{"id": "2509.18405", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18405", "abs": "https://arxiv.org/abs/2509.18405", "authors": ["Sourav Halder", "Jinjun Tong", "Xinyu Wu"], "title": "Check Field Detection Agent (CFD-Agent) using Multimodal Large Language and Vision Language Models", "comment": "12 pages, 5 figures, 2 tables", "summary": "Checks remain a foundational instrument in the financial ecosystem,\nfacilitating substantial transaction volumes across institutions. However,\ntheir continued use also renders them a persistent target for fraud,\nunderscoring the importance of robust check fraud detection mechanisms. At the\ncore of such systems lies the accurate identification and localization of\ncritical fields, such as the signature, magnetic ink character recognition\n(MICR) line, courtesy amount, legal amount, payee, and payer, which are\nessential for subsequent verification against reference checks belonging to the\nsame customer. This field-level detection is traditionally dependent on object\ndetection models trained on large, diverse, and meticulously labeled datasets,\na resource that is scarce due to proprietary and privacy concerns. In this\npaper, we introduce a novel, training-free framework for automated check field\ndetection, leveraging the power of a vision language model (VLM) in conjunction\nwith a multimodal large language model (MLLM). Our approach enables zero-shot\ndetection of check components, significantly lowering the barrier to deployment\nin real-world financial settings. Quantitative evaluation of our model on a\nhand-curated dataset of 110 checks spanning multiple formats and layouts\ndemonstrates strong performance and generalization capability. Furthermore,\nthis framework can serve as a bootstrap mechanism for generating high-quality\nlabeled datasets, enabling the development of specialized real-time object\ndetection models tailored to institutional needs.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u3001\u65e0\u9700\u8bad\u7ec3\u7684\u652f\u7968\u5173\u952e\u5b57\u6bb5\u81ea\u52a8\u68c0\u6d4b\u6846\u67b6\uff0c\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u548c\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u5b9e\u73b0\u96f6\u6837\u672c\u68c0\u6d4b\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5bf9\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\u95ee\u9898\uff0c\u5e76\u4e3a\u751f\u6210\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002", "motivation": "\u4f20\u7edf\u652f\u7968\u6b3a\u8bc8\u68c0\u6d4b\u7cfb\u7edf\u4f9d\u8d56\u4e8e\u5bf9\u7b7e\u540d\u3001MICR\u884c\u3001\u91d1\u989d\u3001\u6536\u6b3e\u4eba\u3001\u4ed8\u6b3e\u4eba\u7b49\u5173\u952e\u5b57\u6bb5\u7684\u51c6\u786e\u8bc6\u522b\uff0c\u800c\u8fd9\u4e9b\u5b57\u6bb5\u7684\u63d0\u53d6\u901a\u5e38\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u4f46\u7531\u4e8e\u4e13\u6709\u548c\u9690\u79c1\u95ee\u9898\uff0c\u6b64\u7c7b\u6570\u636e\u96be\u4ee5\u83b7\u53d6\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65e0\u9700\u5927\u91cf\u6807\u6ce8\u6570\u636e\u5373\u53ef\u8fdb\u884c\u652f\u7968\u5173\u952e\u5b57\u6bb5\u68c0\u6d4b\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u3001\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u548c\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\uff0c\u5b9e\u73b0\u4e86\u5bf9\u652f\u7968\u5173\u952e\u5b57\u6bb5\u7684\u96f6\u6837\u672c\uff08zero-shot\uff09\u68c0\u6d4b\u3002", "result": "\u5728\u5305\u542b110\u5f20\u4e0d\u540c\u683c\u5f0f\u548c\u5e03\u5c40\u7684\u652f\u7968\u7684\u624b\u5de5\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5b9a\u91cf\u8bc4\u4f30\uff0c\u8bc1\u660e\u4e86\u8be5\u6a21\u578b\u5177\u6709\u5f3a\u5927\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002\u6b64\u5916\uff0c\u8be5\u6846\u67b6\u8fd8\u53ef\u4ee5\u4f5c\u4e3a\u751f\u6210\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\u96c6\u7684\u542f\u52a8\u673a\u5236\uff0c\u4ece\u800c\u4e3a\u5f00\u53d1\u6ee1\u8db3\u7279\u5b9a\u673a\u6784\u9700\u6c42\u7684\u4e13\u7528\u5b9e\u65f6\u5bf9\u8c61\u68c0\u6d4b\u6a21\u578b\u5960\u5b9a\u57fa\u7840\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6\u80fd\u591f\u6709\u6548\u5730\u5b9e\u73b0\u652f\u7968\u5173\u952e\u5b57\u6bb5\u7684\u96f6\u6837\u672c\u68c0\u6d4b\uff0c\u964d\u4f4e\u4e86\u5728\u91d1\u878d\u73af\u5883\u4e2d\u90e8\u7f72\u6b64\u7c7b\u7cfb\u7edf\u7684\u95e8\u69db\uff0c\u5e76\u4e3a\u540e\u7eed\u7684\u4e13\u4e1a\u6a21\u578b\u5f00\u53d1\u63d0\u4f9b\u4e86\u6570\u636e\u652f\u6301\u3002"}}
{"id": "2509.18803", "categories": ["quant-ph", "math.QA"], "pdf": "https://arxiv.org/pdf/2509.18803", "abs": "https://arxiv.org/abs/2509.18803", "authors": ["Zhixing Chen", "Lin Chen"], "title": "Virtual Quantum Markov Chain of four-qubit systems", "comment": null, "summary": "We extend the framework of virtual quantum Markov chains (VQMCs) from\ntripartite systems to the four-qubit setting. Structural criteria such as the\nkernel-inclusion condition are analyzed, showing that they are necessary but\nnot sufficient for the existence of a valid recovery map. By explicit examples,\nwe demonstrate that the four-qubit W state admits a recovery channel and thus\nqualifies as a VQMC, while the GHZ state does not. We further provide\nsemidefinite programming (SDP) formulations to test recoverability and quantify\nsampling overhead, and establish the non-convexity of the VQMC set through\nmixed-state counterexamples. These results supply both theoretical insights and\ncomputational tools for studying correlations and recoverability in\nmultipartite quantum systems.", "AI": {"tldr": "\u5c06\u91cf\u5b50\u9a6c\u5c14\u53ef\u592b\u94fe\u6846\u67b6\u6269\u5c55\u5230\u56db\u65b9\u5b50\u7cfb\u7edf\uff0c\u5e76\u5206\u6790\u5176\u7ed3\u6784\u6807\u51c6\uff0c\u8bc1\u660e\u4e86 W \u6001\u662f VQMC\uff0c\u800c GHZ \u6001\u4e0d\u662f\uff0c\u540c\u65f6\u63d0\u51fa\u4e86\u7528\u4e8e\u6d4b\u8bd5\u53ef\u6062\u590d\u6027\u548c\u91cf\u5316\u91c7\u6837\u5f00\u9500\u7684 SDP \u516c\u5f0f\u3002", "motivation": "\u5c06\u73b0\u6709\u7684\u4e09\u65b9\u91cf\u5b50\u9a6c\u5c14\u53ef\u592b\u94fe\uff08VQMC\uff09\u6846\u67b6\u6269\u5c55\u5230\u56db\u65b9\u5b50\u7cfb\u7edf\uff0c\u4ee5\u7814\u7a76\u591a\u65b9\u91cf\u5b50\u7cfb\u7edf\u4e2d\u7684\u76f8\u5173\u6027\u548c\u53ef\u6062\u590d\u6027\u3002", "method": "\u5206\u6790\u4e86 VQMC \u7684\u7ed3\u6784\u6807\u51c6\uff08\u5982\u6838\u5305\u542b\u6761\u4ef6\uff09\uff0c\u5e76\u4f7f\u7528\u56db\u65b9 W \u6001\u548c GHZ \u6001\u7684\u663e\u5f0f\u793a\u4f8b\u8fdb\u884c\u9a8c\u8bc1\u3002\u6b64\u5916\uff0c\u8fd8\u5f00\u53d1\u4e86\u7528\u4e8e\u6d4b\u8bd5\u53ef\u6062\u590d\u6027\u548c\u91cf\u5316\u91c7\u6837\u5f00\u9500\u7684\u534a\u5b9a\u89c4\u5212\uff08SDP\uff09\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u6df7\u5408\u72b6\u6001\u53cd\u4f8b\u5efa\u7acb\u4e86 VQMC \u96c6\u5408\u7684\u975e\u51f8\u6027\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u56db\u65b9 W \u6001\u6ee1\u8db3 VQMC \u7684\u6761\u4ef6\uff0c\u53ef\u4ee5\u6062\u590d\uff0c\u800c GHZ \u6001\u5219\u4e0d\u80fd\u3002\u63d0\u51fa\u4e86 SDP \u516c\u5f0f\u6765\u6d4b\u8bd5\u53ef\u6062\u590d\u6027\u5e76\u91cf\u5316\u91c7\u6837\u5f00\u9500\u3002\u901a\u8fc7\u6df7\u5408\u72b6\u6001\u53cd\u4f8b\u8bc1\u660e\u4e86 VQMC \u96c6\u5408\u7684\u975e\u51f8\u6027\u3002", "conclusion": "\u5bf9\u56db\u65b9 VQMC \u7684\u5206\u6790\u63d0\u4f9b\u4e86\u7406\u8bba\u89c1\u89e3\u548c\u8ba1\u7b97\u5de5\u5177\uff0c\u4ee5\u7814\u7a76\u591a\u65b9\u91cf\u5b50\u7cfb\u7edf\u4e2d\u7684\u76f8\u5173\u6027\u548c\u53ef\u6062\u590d\u6027\u3002 W \u6001\u662f VQMC\uff0c\u800c GHZ \u6001\u4e0d\u662f\uff0c\u5e76\u4e14 VQMC \u96c6\u5408\u662f\u975e\u51f8\u7684\u3002"}}
{"id": "2509.18136", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18136", "abs": "https://arxiv.org/abs/2509.18136", "authors": ["Suqing Wang", "Zuchao Li", "Luohe Shi", "Bo Du", "Hai Zhao", "Yun Li", "Qianren Wang"], "title": "From Parameters to Performance: A Data-Driven Study on LLM Structure and Development", "comment": "Accepted by EMNLP 2025", "summary": "Large language models (LLMs) have achieved remarkable success across various\ndomains, driving significant technological advancements and innovations.\nDespite the rapid growth in model scale and capability, systematic, data-driven\nresearch on how structural configurations affect performance remains scarce. To\naddress this gap, we present a large-scale dataset encompassing diverse\nopen-source LLM structures and their performance across multiple benchmarks.\nLeveraging this dataset, we conduct a systematic, data mining-driven analysis\nto validate and quantify the relationship between structural configurations and\nperformance. Our study begins with a review of the historical development of\nLLMs and an exploration of potential future trends. We then analyze how various\nstructural choices impact performance across benchmarks and further corroborate\nour findings using mechanistic interpretability techniques. By providing\ndata-driven insights into LLM optimization, our work aims to guide the targeted\ndevelopment and application of future models. We will release our dataset at\nhttps://huggingface.co/datasets/DX0369/LLM-Structure-Performance-Dataset", "AI": {"tldr": "LLM\u7ed3\u6784\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u5c1a\u4e0d\u6e05\u695a\uff0c\u672c\u7814\u7a76\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542bLLM\u7ed3\u6784\u53ca\u5176\u6027\u80fd\u7684\u5927\u578b\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc7\u6570\u636e\u6316\u6398\u548c\u53ef\u89e3\u91ca\u6027\u6280\u672f\u8fdb\u884c\u4e86\u5206\u6790\uff0c\u4e3a\u672a\u6765LLM\u7684\u5f00\u53d1\u63d0\u4f9b\u6307\u5bfc\u3002", "motivation": "LLM\u7684\u7ed3\u6784\u914d\u7f6e\u5982\u4f55\u5f71\u54cd\u5176\u6027\u80fd\u7684\u7814\u7a76\u4e0d\u8db3\uff0c\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542bLLM\u7ed3\u6784\u53ca\u5176\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u6027\u80fd\u7684\u5927\u578b\u6570\u636e\u96c6\uff0c\u5e76\u5229\u7528\u6570\u636e\u6316\u6398\u548c\u673a\u68b0\u53ef\u89e3\u91ca\u6027\u6280\u672f\u8fdb\u884c\u5206\u6790\u3002", "result": "\u53d1\u73b0\u4e86LLM\u7ed3\u6784\u914d\u7f6e\u4e0e\u6027\u80fd\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8fdb\u884c\u4e86\u91cf\u5316\u548c\u9a8c\u8bc1\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u4f9b\u4e86\u6570\u636e\u9a71\u52a8\u7684LLM\u4f18\u5316\u89c1\u89e3\uff0c\u65e8\u5728\u6307\u5bfc\u672a\u6765\u6a21\u578b\u7684\u8bbe\u8ba1\u548c\u5e94\u7528\u3002"}}
{"id": "2509.18681", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18681", "abs": "https://arxiv.org/abs/2509.18681", "authors": ["Nicolas Valot", "Louis Fabre", "Benjamin Lesage", "Ammar Mechouche", "Claire Pagetti"], "title": "Implementation of airborne ML models with semantics preservation", "comment": null, "summary": "Machine Learning (ML) may offer new capabilities in airborne systems.\nHowever, as any piece of airborne systems, ML-based systems will be required to\nguarantee their safe operation. Thus, their development will have to be\ndemonstrated to be compliant with the adequate guidance. So far, the European\nUnion Aviation Safety Agency (EASA) has published a concept paper and an\nEUROCAE/SAE group is preparing ED-324. Both approaches delineate high-level\nobjectives to confirm the ML model achieves its intended function and maintains\ntraining performance in the target environment. The paper aims to clarify the\ndifference between an ML model and its corresponding unambiguous description,\nreferred to as the Machine Learning Model Description (MLMD). It then refines\nthe essential notion of semantics preservation to ensure the accurate\nreplication of the model. We apply our contributions to several industrial use\ncases to build and compare several target models.", "AI": {"tldr": "The paper clarifies the difference between ML models and MLMDs and refines semantics preservation to ensure accurate model replication, applying it to industrial use cases.", "motivation": "The need to guarantee the safe operation of ML-based airborne systems and demonstrate compliance with safety regulations (EASA, ED-324).", "method": "Differentiating between ML models and MLMDs, and refining the concept of semantics preservation for accurate model replication.", "result": "Demonstration of the contributions applied to industrial use cases, involving the building and comparison of several target models.", "conclusion": "The paper's contributions provide a clearer understanding and methodology for ensuring the safety and reliability of ML models in airborne systems."}}
{"id": "2509.18658", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18658", "abs": "https://arxiv.org/abs/2509.18658", "authors": ["Huanxin Sheng", "Xinyi Liu", "Hangfeng He", "Jieyu Zhao", "Jian Kang"], "title": "Analyzing Uncertainty of LLM-as-a-Judge: Interval Evaluations with Conformal Prediction", "comment": "To appear in EMNLP 2025. Our code and data are available at\n  \\url{https://github.com/BruceSheng1202/Analyzing_Uncertainty_of_LLM-as-a-Judge", "summary": "LLM-as-a-judge has become a promising paradigm for using large language\nmodels (LLMs) to evaluate natural language generation (NLG), but the\nuncertainty of its evaluation remains underexplored. This lack of reliability\nmay limit its deployment in many applications. This work presents the first\nframework to analyze the uncertainty by offering a prediction interval of\nLLM-based scoring via conformal prediction. Conformal prediction constructs\ncontinuous prediction intervals from a single evaluation run, and we design an\nordinal boundary adjustment for discrete rating tasks. We also suggest a\nmidpoint-based score within the interval as a low-bias alternative to raw model\nscore and weighted average. We perform extensive experiments and analysis,\nwhich show that conformal prediction can provide valid prediction interval with\ncoverage guarantees. We also explore the usefulness of interval midpoint and\njudge reprompting for better judgment.", "AI": {"tldr": "LLM-as-a-judge \u8bc4\u4f30\u7684\u4e0d\u53ef\u9760\u6027\u9650\u5236\u4e86\u5176\u5e94\u7528\u3002\u672c\u7814\u7a76\u63d0\u51fa\u9996\u4e2a\u57fa\u4e8e\u5171\u5f62\u9884\u6d4b\u7684\u6846\u67b6\u6765\u5206\u6790 LLM \u8bc4\u4f30\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u901a\u8fc7\u63d0\u4f9b LLM \u8bc4\u5206\u7684\u9884\u6d4b\u533a\u95f4\u6765\u89e3\u51b3\u6b64\u95ee\u9898\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u533a\u95f4\u4e2d\u70b9\u4f5c\u4e3a\u4e00\u79cd\u4f4e\u504f\u5dee\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5e76\u63a2\u7d22\u4e86\u5176\u5728\u5224\u65ad\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "LLM-as-a-judge \u4f5c\u4e3a\u4e00\u79cd\u65b0\u5174\u7684 NLG \u8bc4\u4f30\u8303\u5f0f\uff0c\u5176\u8bc4\u4f30\u7ed3\u679c\u7684\u4e0d\u786e\u5b9a\u6027\u95ee\u9898\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u6027\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5229\u7528\u5171\u5f62\u9884\u6d4b\u6765\u5206\u6790 LLM \u8bc4\u4f30\u4e0d\u786e\u5b9a\u6027\u7684\u6846\u67b6\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u5b83\u4e3a LLM \u8bc4\u5206\u751f\u6210\u9884\u6d4b\u533a\u95f4\uff0c\u5e76\u4e3a\u79bb\u6563\u8bc4\u5206\u4efb\u52a1\u8bbe\u8ba1\u4e86\u8fb9\u754c\u8c03\u6574\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u533a\u95f4\u4e2d\u70b9\u7684\u8bc4\u5206\u4f5c\u4e3a\u4e00\u79cd\u4f4e\u504f\u5dee\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5171\u5f62\u9884\u6d4b\u80fd\u591f\u63d0\u4f9b\u5177\u6709\u8986\u76d6\u7387\u4fdd\u8bc1\u7684\u6709\u6548\u9884\u6d4b\u533a\u95f4\u3002\u540c\u65f6\uff0c\u7814\u7a76\u8fd8\u63a2\u8ba8\u4e86\u533a\u95f4\u4e2d\u70b9\u548c\u63d0\u793a\u8bcd\u8c03\u6574\u5bf9\u4e8e\u63d0\u5347\u8bc4\u4f30\u8d28\u91cf\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a LLM-as-a-judge \u8bc4\u4f30\u5f15\u5165\u4e86\u5171\u5f62\u9884\u6d4b\uff0c\u89e3\u51b3\u4e86\u5176\u4e0d\u786e\u5b9a\u6027\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u5177\u6709\u7406\u8bba\u4fdd\u969c\u7684\u9884\u6d4b\u533a\u95f4\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u8bc4\u4f30\u8d28\u91cf\u7684\u65b9\u6cd5\u3002"}}
{"id": "2509.19181", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2509.19181", "abs": "https://arxiv.org/abs/2509.19181", "authors": ["Jeffrey G. Ulbrandt", "Xiaozhi Zhang", "Randall L. Headrick"], "title": "Detachment limited interlayer transport processes during SrTiO3 pulsed laser epitaxy", "comment": "24 pages, 9 figures", "summary": "Pulsed laser epitaxial growth is characterized by high instantaneous\ndeposition rates that leads to the nucleation of transient islands, both on\nsurface terraces, and on top of stable islands formed during previous\ndeposition pulses. We report results from combined in-situ X-ray reflectivity\nand kinetic Monte Carlo (kMC) simulations. Specular reflectivity monitors\ninterlayer transport, while diffuse scattering reveals the evolution of\nin-plane length scales, both during the recovery time between individual laser\npulses and over multiple deposited layers. The initial stage after each laser\npulse is faster than the temporal resolution of the experiment, while\nsubsequent recovery occurs over seconds. The results suggest that transient\nislands on top of stable two-dimensional islands form immediately after the\ndeposition pulse, and then ripen via detachment and diffusion, leading to the\nslower component. kMC simulations show that the detachment energy barrier plays\na dominant role in determining the recovery time constant.", "AI": {"tldr": "\u8109\u51b2\u6fc0\u5149\u5916\u5ef6\u751f\u957f\u5bfc\u81f4\u8868\u9762\u5c9b\u7684\u6210\u6838\uff0c\u901a\u8fc7X\u5c04\u7ebf\u53cd\u5c04\u548c\u52a8\u529b\u5b66\u8499\u7279\u5361\u7f57\u6a21\u62df\u8fdb\u884c\u7814\u7a76\u3002", "motivation": "\u7814\u7a76\u8109\u51b2\u6fc0\u5149\u5916\u5ef6\u751f\u957f\u4e2d\u77ac\u65f6\u5c9b\u7684\u6210\u6838\u53ca\u5176\u6f14\u5316\u8fc7\u7a0b\u3002", "method": "\u7ed3\u5408\u4f7f\u7528\u539f\u4f4dX\u5c04\u7ebf\u53cd\u5c04\u548c\u52a8\u529b\u5b66\u8499\u7279\u5361\u7f57\uff08kMC\uff09\u6a21\u62df\u3002\u901a\u8fc7\u63a0\u5165\u5c04X\u5c04\u7ebf\u53cd\u5c04\u76d1\u6d4b\u5c42\u95f4\u4f20\u8f93\uff0c\u901a\u8fc7\u6269\u6563\u6563\u5c04\u63ed\u793a\u9762\u5185\u5c3a\u5ea6\u6f14\u53d8\u3002", "result": "\u5728\u6fc0\u5149\u8109\u51b2\u540e\u7684\u6062\u590d\u65f6\u95f4\u5185\uff0c\u521d\u59cb\u9636\u6bb5\u5feb\u4e8e\u5b9e\u9a8c\u65f6\u95f4\u5206\u8fa8\u7387\uff0c\u968f\u540e\u6062\u590d\u9700\u8981\u51e0\u79d2\u949f\u3002\u77ac\u65f6\u5c9b\u5728\u7a33\u5b9a\u5c9b\u4e0a\u5f62\u6210\uff0c\u7136\u540e\u901a\u8fc7\u8131\u9644\u548c\u6269\u6563\u6210\u719f\u3002kMC\u6a21\u62df\u8868\u660e\u8131\u9644\u80fd\u5792\u662f\u6062\u590d\u65f6\u95f4\u5e38\u6570\u7684\u4e3b\u8981\u51b3\u5b9a\u56e0\u7d20\u3002", "conclusion": "\u8109\u51b2\u6fc0\u5149\u5916\u5ef6\u751f\u957f\u8fc7\u7a0b\u4e2d\uff0c\u77ac\u65f6\u5c9b\u7684\u5f62\u6210\u548c\u6210\u719f\u662f\u5f71\u54cd\u8868\u9762\u5f62\u8c8c\u7684\u5173\u952e\u56e0\u7d20\uff0c\u5176\u52a8\u529b\u5b66\u8fc7\u7a0b\u53ef\u4ee5\u901a\u8fc7X\u5c04\u7ebf\u53cd\u5c04\u548ckMC\u6a21\u62df\u6709\u6548\u7814\u7a76\u3002"}}
{"id": "2509.18686", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18686", "abs": "https://arxiv.org/abs/2509.18686", "authors": ["Ziyi Xu", "Haohong Lin", "Shiqi Liu", "Ding Zhao"], "title": "Query-Centric Diffusion Policy for Generalizable Robotic Assembly", "comment": "8 pages, 7 figures", "summary": "The robotic assembly task poses a key challenge in building generalist robots\ndue to the intrinsic complexity of part interactions and the sensitivity to\nnoise perturbations in contact-rich settings. The assembly agent is typically\ndesigned in a hierarchical manner: high-level multi-part reasoning and\nlow-level precise control. However, implementing such a hierarchical policy is\nchallenging in practice due to the mismatch between high-level skill queries\nand low-level execution. To address this, we propose the Query-centric\nDiffusion Policy (QDP), a hierarchical framework that bridges high-level\nplanning and low-level control by utilizing queries comprising objects, contact\npoints, and skill information. QDP introduces a query-centric mechanism that\nidentifies task-relevant components and uses them to guide low-level policies,\nleveraging point cloud observations to improve the policy's robustness. We\nconduct comprehensive experiments on the FurnitureBench in both simulation and\nreal-world settings, demonstrating improved performance in skill precision and\nlong-horizon success rate. In the challenging insertion and screwing tasks, QDP\nimproves the skill-wise success rate by over 50% compared to baselines without\nstructured queries.", "AI": {"tldr": "QDP\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u673a\u5668\u4eba\u88c5\u914d\u6846\u67b6\uff0c\u901a\u8fc7\u67e5\u8be2\u673a\u5236\u8fde\u63a5\u9ad8\u5c42\u89c4\u5212\u548c\u5e95\u5c42\u63a7\u5236\uff0c\u63d0\u9ad8\u4e86\u88c5\u914d\u7cbe\u5ea6\u548c\u6210\u529f\u7387\u3002", "motivation": "\u673a\u5668\u4eba\u88c5\u914d\u4efb\u52a1\u56e0\u96f6\u4ef6\u4ea4\u4e92\u7684\u590d\u6742\u6027\u548c\u5bf9\u566a\u58f0\u7684\u654f\u611f\u6027\u800c\u5177\u6709\u6311\u6218\u6027\u3002\u73b0\u6709\u7684\u5206\u5c42\u7b56\u7565\u5728\u6280\u80fd\u67e5\u8be2\u548c\u6267\u884c\u4e4b\u95f4\u5b58\u5728\u4e0d\u5339\u914d\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ee5\u67e5\u8be2\u4e3a\u4e2d\u5fc3\u7684\u6269\u6563\u7b56\u7565\uff08QDP\uff09\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u5305\u542b\u5bf9\u8c61\u3001\u63a5\u89e6\u70b9\u548c\u6280\u80fd\u4fe1\u606f\u7684\u67e5\u8be2\u6765\u8fde\u63a5\u9ad8\u5c42\u89c4\u5212\u548c\u5e95\u5c42\u63a7\u5236\uff0c\u5e76\u5229\u7528\u70b9\u4e91\u89c2\u6d4b\u6765\u63d0\u9ad8\u7b56\u7565\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5728FurnitureBench\u7684\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0cQDP\u5728\u6280\u80fd\u7cbe\u5ea6\u548c\u957f\u65f6\u7a0b\u6210\u529f\u7387\u65b9\u9762\u5747\u6709\u6240\u63d0\u9ad8\uff0c\u5728\u63d2\u5165\u548c\u62e7\u7d27\u4efb\u52a1\u4e2d\u7684\u6280\u80fd\u6210\u529f\u7387\u6bd4\u57fa\u7ebf\u63d0\u9ad8\u4e8650%\u4ee5\u4e0a\u3002", "conclusion": "QDP\u6846\u67b6\u80fd\u591f\u6709\u6548\u89e3\u51b3\u9ad8\u5c42\u89c4\u5212\u548c\u5e95\u5c42\u63a7\u5236\u4e4b\u95f4\u7684\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u63d0\u9ad8\u673a\u5668\u4eba\u88c5\u914d\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2509.18425", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18425", "abs": "https://arxiv.org/abs/2509.18425", "authors": ["Philip Wootaek Shin", "Jack Sampson", "Vijaykrishnan Narayanan", "Andres Marquez", "Mahantesh Halappanavar"], "title": "Losing the Plot: How VLM responses degrade on imperfect charts", "comment": null, "summary": "Vision language models (VLMs) show strong results on chart understanding, yet\nexisting benchmarks assume clean figures and fact based queries. Real world\ncharts often contain distortions and demand reasoning beyond simple matching.\nWe evaluate ChatGPT 4o, Claude Sonnet 4, and Gemini 2.5 Pro, finding sharp\nperformance drops under corruption or occlusion, with hallucinations such as\nvalue fabrication, trend misinterpretation, and entity confusion becoming more\nfrequent. Models remain overconfident in degraded settings, generating\nplausible but unsupported explanations.\n  To address this gap, we introduce CHART NOISe(Chart Hallucinations, Answers,\nand Reasoning Testing on Noisy and Occluded Input Selections), a dataset\ncombining chart corruptions, occlusions, and exam style multiple choice\nquestions inspired by Korea's CSAT English section. A key innovation is prompt\nreverse inconsistency, where models contradict themselves when asked to confirm\nversus deny the same statement. Our contributions are threefold: (1)\nbenchmarking state of the art VLMs, exposing systematic vulnerabilities in\nchart reasoning; (2) releasing CHART NOISe, the first dataset unifying\ncorruption, occlusion, and reverse inconsistency; and (3) proposing baseline\nmitigation strategies such as quality filtering and occlusion detection.\nTogether, these efforts establish a rigorous testbed for advancing robustness\nand reliability in chart understanding.", "AI": {"tldr": "\u73b0\u6709\u56fe\u8868\u7406\u89e3\u57fa\u51c6\u5047\u8bbe\u56fe\u8868\u6e05\u6670\u4e14\u95ee\u9898\u57fa\u4e8e\u4e8b\u5b9e\uff0c\u4f46\u73b0\u5b9e\u4e16\u754c\u56fe\u8868\u5e38\u6709\u626d\u66f2\u4e14\u9700\u8981\u8d85\u8d8a\u7b80\u5355\u5339\u914d\u7684\u63a8\u7406\u3002\u672c\u7814\u7a76\u8bc4\u4f30\u4e86 ChatGPT 4o\u3001Claude Sonnet 4 \u548c Gemini 2.5 Pro \u5728\u53d7\u635f\u6216\u906e\u6321\u60c5\u51b5\u4e0b\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u5e76\u51fa\u73b0\u4ef7\u503c\u865a\u6784\u3001\u8d8b\u52bf\u8bef\u89e3\u548c\u5b9e\u4f53\u6df7\u6dc6\u7b49\u5e7b\u89c9\u3002\u6a21\u578b\u5728\u6076\u52a3\u73af\u5883\u4e0b\u4ecd\u8868\u73b0\u51fa\u8fc7\u5ea6\u81ea\u4fe1\u3002\u4e3a\u89e3\u51b3\u6b64\u95ee\u9898\uff0c\u7814\u7a76\u63d0\u51fa\u4e86 CHART NOISe \u6570\u636e\u96c6\uff0c\u5305\u542b\u56fe\u8868\u635f\u574f\u3001\u906e\u6321\u548c\u8003\u8bd5\u98ce\u683c\u7684\u591a\u9879\u9009\u62e9\u9898\uff0c\u5e76\u5f15\u5165\u4e86\u63d0\u793a\u53cd\u5411\u4e0d\u4e00\u81f4\u6027\u3002\u7814\u7a76\u8d21\u732e\u5305\u62ec\uff1a(1) \u57fa\u51c6\u6d4b\u8bd5\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u63ed\u793a\u5176\u5728\u56fe\u8868\u63a8\u7406\u65b9\u9762\u7684\u7cfb\u7edf\u6027\u6f0f\u6d1e\uff1b(2) \u53d1\u5e03 CHART NOISe \u6570\u636e\u96c6\uff1b(3) \u63d0\u51fa\u8d28\u91cf\u8fc7\u6ee4\u548c\u906e\u6321\u68c0\u6d4b\u7b49\u57fa\u7ebf\u7f13\u89e3\u7b56\u7565\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u56fe\u8868\u5e38\u5305\u542b\u626d\u66f2\u548c\u906e\u6321\uff0c\u5e76\u4e14\u9700\u8981\u8d85\u8d8a\u7b80\u5355\u5339\u914d\u7684\u63a8\u7406\u80fd\u529b\uff0c\u800c\u73b0\u6709\u57fa\u51c6\u672a\u80fd\u5145\u5206\u53cd\u6620\u8fd9\u4e9b\u6311\u6218\u3002\u7136\u800c\uff0c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u5904\u7406\u6b64\u7c7b\u6570\u636e\u65f6\u8868\u73b0\u51fa\u6027\u80fd\u4e0b\u964d\u548c\u5e7b\u89c9\u7b49\u95ee\u9898\u3002", "method": "\u5728\u5305\u542b\u56fe\u8868\u635f\u574f\u3001\u906e\u6321\u548c\u8003\u8bd5\u98ce\u683c\u9009\u62e9\u9898\u7684 CHART NOISe \u6570\u636e\u96c6\u4e0a\uff0c\u8bc4\u4f30\u4e86 ChatGPT 4o\u3001Claude Sonnet 4 \u548c Gemini 2.5 Pro\u3002\u5f15\u5165\u4e86\u63d0\u793a\u53cd\u5411\u4e0d\u4e00\u81f4\u6027\uff08prompt reverse inconsistency\uff09\u4f5c\u4e3a\u4e00\u79cd\u8bc4\u4f30\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u8d28\u91cf\u8fc7\u6ee4\u548c\u906e\u6321\u68c0\u6d4b\u7b49\u7f13\u89e3\u7b56\u7565\u3002", "result": "ChatGPT 4o\u3001Claude Sonnet 4 \u548c Gemini 2.5 Pro \u5728\u5904\u7406\u53d7\u635f\u6216\u906e\u6321\u7684\u56fe\u8868\u65f6\uff0c\u6027\u80fd\u51fa\u73b0\u663e\u8457\u4e0b\u964d\uff0c\u5e7b\u89c9\uff08\u5982\u4ef7\u503c\u865a\u6784\u3001\u8d8b\u52bf\u8bef\u89e3\u3001\u5b9e\u4f53\u6df7\u6dc6\uff09\u66f4\u52a0\u9891\u7e41\u3002\u6a21\u578b\u5728\u8fd9\u4e9b\u6076\u52a3\u8bbe\u7f6e\u4e0b\u4ecd\u7136\u8fc7\u5ea6\u81ea\u4fe1\uff0c\u751f\u6210\u770b\u4f3c\u5408\u7406\u4f46\u65e0\u4f9d\u636e\u7684\u89e3\u91ca\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u57fa\u51c6\u6d4b\u8bd5\u73b0\u6709 VLM\uff0c\u63ed\u793a\u4e86\u5176\u5728\u56fe\u8868\u63a8\u7406\u65b9\u9762\u7684\u7cfb\u7edf\u6027\u6f0f\u6d1e\uff0c\u5e76\u53d1\u5e03\u4e86\u9996\u4e2a\u5305\u542b\u635f\u574f\u3001\u906e\u6321\u548c\u53cd\u5411\u4e0d\u4e00\u81f4\u6027\u7684 CHART NOISe \u6570\u636e\u96c6\u3002\u63d0\u51fa\u7684\u7f13\u89e3\u7b56\u7565\u4e3a\u63d0\u9ad8\u56fe\u8868\u7406\u89e3\u7684\u9c81\u68d2\u6027\u548c\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4e25\u683c\u7684\u6d4b\u8bd5\u5e73\u53f0\u3002"}}
{"id": "2509.18834", "categories": ["quant-ph", "physics.atom-ph", "physics.optics"], "pdf": "https://arxiv.org/pdf/2509.18834", "abs": "https://arxiv.org/abs/2509.18834", "authors": ["Hai-Tao Tu", "Kai-Yu Liao", "Si-Yuan Qiu", "Xiao-Hong Liu", "Yi-Qi Guo", "Zheng-Qi Du", "Yang Xu", "Xin-Ding Zhang", "Hui Yan", "Shi-Liang Zhu"], "title": "Quantum-memory-assisted on-demand microwave-optical transduction", "comment": null, "summary": "Microwave-optical transducers and quantum memories are fundamental components\nof quantum repeaters, essential for developing a quantum internet in which\nsolid-state quantum computers serve as nodes interconnected by optical fibers\nfor data transmission. Although both technologies have made significant\nadvancements, the integration of microwave-optical conversion and quantum\nmemory functionalities remains a challenge. Here, we theoretically propose and\nexperimentally demonstrate a memoryenhanced quantum microwave-optical\ntransduction using a Rydberg ensemble. By utilizing a cascaded\nelectromagnetically induced transparency process, we store microwave photons in\na highly excited collective state and subsequently convert them into optical\nphotons during the retrieval process. Taking advantage of the optical depth\nwith order of millions for microwave photons in Rydberg ensemble, combined with\na minimal storage dephasing rate at the single-photon level, the transducer\nachieves an areanormalized storage efficiency greater than 90%, a bandwidth of\n2.1 MHz, and a noise equivalent temperature as low as 26 K, even in cavity-free\nconditions. Our findings pave the way for the practical implementation of\nquantum repeaters based on atomic ensembles in quantum information processing.", "AI": {"tldr": "\u4f7f\u7528Rydberg\u539f\u5b50\u7cfb\u7efc\u5b9e\u73b0\u4e86\u5185\u5b58\u589e\u5f3a\u7684\u5fae\u6ce2-\u5149\u5b66\u8f6c\u6362\uff0c\u5b58\u50a8\u6548\u7387>90%\uff0c\u5e26\u5bbd2.1MHz\uff0c\u566a\u58f0\u7b49\u6548\u6e29\u5ea626K\u3002", "motivation": "\u5fae\u6ce2-\u5149\u5b66\u8f6c\u6362\u5668\u548c\u91cf\u5b50\u5b58\u50a8\u5668\u662f\u91cf\u5b50\u4e2d\u7ee7\u5668\u7684\u5173\u952e\u7ec4\u6210\u90e8\u5206\uff0c\u4f46\u4e24\u8005\u7684\u96c6\u6210\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u5229\u7528\u7ea7\u8054\u7535\u78c1\u8bf1\u5bfc\u900f\u660e\u8fc7\u7a0b\uff0c\u5c06\u5fae\u6ce2\u5149\u5b50\u5b58\u50a8\u5728\u91cc\u5fb7\u5821\u539f\u5b50\u9ad8\u5ea6\u6fc0\u53d1\u6001\uff0c\u5e76\u5728\u68c0\u7d22\u8fc7\u7a0b\u4e2d\u5c06\u5176\u8f6c\u6362\u4e3a\u5149\u5b66\u5149\u5b50\u3002", "result": "\u5b9e\u73b0\u4e86\u5927\u4e8e90%\u7684\u9762\u79ef\u5f52\u4e00\u5316\u5b58\u50a8\u6548\u7387\uff0c2.1MHz\u7684\u5e26\u5bbd\uff0c\u4ee5\u53ca26K\u7684\u4f4e\u566a\u58f0\u7b49\u6548\u6e29\u5ea6\uff08\u65e0\u8154\u4f53\uff09\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u57fa\u4e8e\u539f\u5b50\u7cfb\u7efc\u7684\u91cf\u5b50\u4e2d\u7ee7\u5668\u5728\u91cf\u5b50\u4fe1\u606f\u5904\u7406\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2509.18137", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18137", "abs": "https://arxiv.org/abs/2509.18137", "authors": ["Shaoheng Wang", "Yao Lu", "Yuqi Li", "Yaxin Gao", "Jiaqi Nie", "Shanqing Yu", "Yingli Tian", "Qi Xuan"], "title": "LoRALib: A Standardized Benchmark for Evaluating LoRA-MoE Methods", "comment": null, "summary": "As a parameter efficient fine-tuning (PEFT) method, low-rank adaptation\n(LoRA) can save significant costs in storage and computing, but its strong\nadaptability to a single task is often accompanied by insufficient cross-task\ngeneralization capabilities. To improve this, existing work combines LoRA with\nmixture-of-experts (MoE) to enhance the model's adaptability through expert\nmodules and routing mechanisms. However, existing LoRA-MoE methods lack unified\nstandards in models, datasets, hyperparameters, and evaluation methods, making\nit difficult to conduct fair comparisons between different methods. To this\nend, we proposed a unified benchmark named LoRALib. Specifically, we\nstandardized datasets from $40$ downstream tasks into a unified format,\nfine-tuned them using the same hyperparameters and obtained $680$ LoRA modules\nacross $17$ model architectures. Based on this LoRA library, we conduct\nlarge-scale experiments on $3$ representative LoRA-MoE methods and different\nLoRA selection mechanisms using the open-sourced testing tool OpenCompass.\nExtensive experiments show that LoRAMoE performs best, and that prioritizing\nLoRAs relevant to the target task can further improve the performance of MoE.\nWe hope these findings will inspire future work. Our datasets and LoRA library\nare available at https://huggingface.co/datasets/YaoLuzjut/LoRAOcean_dataset\nand https://huggingface.co/YaoLuzjut/models.", "AI": {"tldr": "LoRA-MoE \u5728\u8de8\u4efb\u52a1\u6cdb\u5316\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff0c\u5e76\u4e14\u4f18\u5148\u9009\u62e9\u4e0e\u76ee\u6807\u4efb\u52a1\u76f8\u5173\u7684 LoRA \u53ef\u4ee5\u8fdb\u4e00\u6b65\u63d0\u9ad8 MoE \u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684 LoRA-MoE \u65b9\u6cd5\u5728\u6a21\u578b\u3001\u6570\u636e\u96c6\u3001\u8d85\u53c2\u6570\u548c\u8bc4\u4f30\u65b9\u6cd5\u65b9\u9762\u7f3a\u4e4f\u7edf\u4e00\u7684\u6807\u51c6\uff0c\u5bfc\u81f4\u96be\u4ee5\u8fdb\u884c\u516c\u5e73\u7684\u6bd4\u8f83\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a LoRALib \u7684\u7edf\u4e00\u57fa\u51c6\uff0c\u6807\u51c6\u5316\u4e86\u6765\u81ea 40 \u4e2a\u4e0b\u6e38\u4efb\u52a1\u7684\u6570\u636e\u96c6\uff0c\u5e76\u4f7f\u7528\u76f8\u540c\u7684\u8d85\u53c2\u6570\u8fdb\u884c\u4e86\u5fae\u8c03\uff0c\u83b7\u5f97\u4e86 680 \u4e2a LoRA \u6a21\u5757\uff0c\u6db5\u76d6\u4e86 17 \u79cd\u6a21\u578b\u67b6\u6784\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u4f7f\u7528 OpenCompass \u5bf9 3 \u79cd\u4ee3\u8868\u6027\u7684 LoRA-MoE \u65b9\u6cd5\u548c\u4e0d\u540c\u7684 LoRA \u9009\u62e9\u673a\u5236\u8fdb\u884c\u4e86\u5927\u89c4\u6a21\u5b9e\u9a8c\u3002", "result": "LoRA-MoE \u8868\u73b0\u6700\u4f73\uff0c\u5e76\u4e14\u4f18\u5148\u9009\u62e9\u4e0e\u76ee\u6807\u4efb\u52a1\u76f8\u5173\u7684 LoRA \u53ef\u4ee5\u8fdb\u4e00\u6b65\u63d0\u9ad8 MoE \u7684\u6027\u80fd\u3002", "conclusion": "LoRALib \u4e3a LoRA-MoE \u7684\u516c\u5e73\u6bd4\u8f83\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u6807\u51c6\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u53d1\u73b0\u548c\u65b9\u5411\u3002"}}
{"id": "2509.18690", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18690", "abs": "https://arxiv.org/abs/2509.18690", "authors": ["Zhiyu Kan", "Wensheng Gan", "Zhenlian Qi", "Philip S. Yu"], "title": "Advances in Large Language Models for Medicine", "comment": "Preprint. 5 figures, 4 tables", "summary": "Artificial intelligence (AI) technology has advanced rapidly in recent years,\nwith large language models (LLMs) emerging as a significant breakthrough. LLMs\nare increasingly making an impact across various industries, with the medical\nfield standing out as the most prominent application area. This paper\nsystematically reviews the up-to-date research progress of LLMs in the medical\nfield, providing an in-depth analysis of training techniques for large medical\nmodels, their adaptation in healthcare settings, related applications, as well\nas their strengths and limitations. Furthermore, it innovatively categorizes\nmedical LLMs into three distinct types based on their training methodologies\nand classifies their evaluation approaches into two categories. Finally, the\nstudy proposes solutions to existing challenges and outlines future research\ndirections based on identified issues in the field of medical LLMs. By\nsystematically reviewing previous and advanced research findings, we aim to\nhighlight the necessity of developing medical LLMs, provide a deeper\nunderstanding of their current state of development, and offer clear guidance\nfor subsequent research.", "AI": {"tldr": "AI\u6280\u672f\uff0c\u7279\u522b\u662f\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\uff0c\u5728\u533b\u7597\u9886\u57df\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u672c\u7efc\u8ff0\u5bf9LLM\u5728\u533b\u7597\u4e2d\u7684\u7814\u7a76\u8fdb\u5c55\u3001\u8bad\u7ec3\u6280\u672f\u3001\u5e94\u7528\u3001\u4f18\u7f3a\u70b9\u3001\u5206\u7c7b\u53ca\u672a\u6765\u65b9\u5411\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u5206\u6790\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u533b\u7597\u9886\u57df\u7684\u5e7f\u6cdb\u5e94\u7528\u548c\u91cd\u8981\u6027\u65e5\u76ca\u51f8\u663e\uff0c\u9700\u8981\u5bf9\u76f8\u5173\u7814\u7a76\u8fdb\u5c55\u8fdb\u884c\u7cfb\u7edf\u6027\u56de\u987e\u548c\u5206\u6790\u3002", "method": "\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u533b\u7597\u9886\u57df\u7684\u6700\u65b0\u7814\u7a76\u8fdb\u5c55\u8fdb\u884c\u7cfb\u7edf\u6027\u56de\u987e\uff0c\u5305\u62ec\u8bad\u7ec3\u6280\u672f\u3001\u5728\u533b\u7597\u73af\u5883\u4e2d\u7684\u9002\u5e94\u6027\u3001\u76f8\u5173\u5e94\u7528\u3001\u4f18\u7f3a\u70b9\u3002\u5bf9\u533b\u7597LLM\u8fdb\u884c\u5206\u7c7b\uff08\u57fa\u4e8e\u8bad\u7ec3\u65b9\u6cd5\u7684\u4e09\u79cd\u7c7b\u578b\u548c\u4e24\u79cd\u8bc4\u4f30\u65b9\u6cd5\uff09\u3002\u63d0\u51fa\u6311\u6218\u7684\u89e3\u51b3\u65b9\u6848\u5e76\u6982\u8ff0\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "result": "\u5bf9LLM\u5728\u533b\u7597\u9886\u57df\u7684\u8bad\u7ec3\u6280\u672f\u3001\u5e94\u7528\u3001\u4f18\u7f3a\u70b9\u8fdb\u884c\u4e86\u5206\u6790\uff0c\u5e76\u521b\u65b0\u6027\u5730\u5c06\u533b\u7597LLM\u5206\u4e3a\u4e09\u7c7b\uff08\u57fa\u4e8e\u8bad\u7ec3\u65b9\u6cd5\uff09\u548c\u4e24\u79cd\u8bc4\u4f30\u65b9\u6cd5\u3002\u6307\u51fa\u4e86\u73b0\u6709\u6311\u6218\u5e76\u63d0\u51fa\u4e86\u89e3\u51b3\u65b9\u6848\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u672c\u7814\u7a76\u7cfb\u7edf\u6027\u5730\u56de\u987e\u4e86LLM\u5728\u533b\u7597\u9886\u57df\u7684\u6700\u65b0\u7814\u7a76\uff0c\u5f3a\u8c03\u4e86\u53d1\u5c55\u533b\u7597LLM\u7684\u5fc5\u8981\u6027\uff0c\u52a0\u6df1\u4e86\u5bf9\u5176\u53d1\u5c55\u73b0\u72b6\u7684\u7406\u89e3\uff0c\u5e76\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u660e\u786e\u6307\u5bfc\u3002"}}
{"id": "2509.18713", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18713", "abs": "https://arxiv.org/abs/2509.18713", "authors": ["Yizhe Huang", "Yang Liu", "Ruiyu Zhao", "Xiaolong Zhong", "Xingming Yue", "Ling Jiang"], "title": "MemOrb: A Plug-and-Play Verbal-Reinforcement Memory Layer for E-Commerce Customer Service", "comment": null, "summary": "Large Language Model-based agents(LLM-based agents) are increasingly deployed\nin customer service, yet they often forget across sessions, repeat errors, and\nlack mechanisms for continual self-improvement. This makes them unreliable in\ndynamic settings where stability and consistency are critical. To better\nevaluate these properties, we emphasize two indicators: task success rate as a\nmeasure of overall effectiveness, and consistency metrics such as Pass$^k$ to\ncapture reliability across multiple trials. To address the limitations of\nexisting approaches, we propose MemOrb, a lightweight and plug-and-play verbal\nreinforcement memory layer that distills multi-turn interactions into compact\nstrategy reflections. These reflections are stored in a shared memory bank and\nretrieved to guide decision-making, without requiring any fine-tuning.\nExperiments show that MemOrb significantly improves both success rate and\nstability, achieving up to a 63 percentage-point gain in multi-turn success\nrate and delivering more consistent performance across repeated trials. Our\nresults demonstrate that structured reflection is a powerful mechanism for\nenhancing long-term reliability of frozen LLM agents in customer service\nscenarios.", "AI": {"tldr": "MemOrb\u662f\u4e00\u4e2a\u7528\u4e8eLLM\u5ba2\u670d\u4ee3\u7406\u7684\u8bb0\u5fc6\u5c42\uff0c\u901a\u8fc7\u63d0\u70bc\u591a\u8f6e\u4ea4\u4e92\u7684\u7b56\u7565\u53cd\u601d\u6765\u63d0\u9ad8\u4efb\u52a1\u6210\u529f\u7387\u548c\u4e00\u81f4\u6027\uff0c\u65e0\u9700\u5fae\u8c03\u3002", "motivation": "\u73b0\u6709\u7684LLM\u5ba2\u670d\u4ee3\u7406\u5728\u591a\u8f6e\u4ea4\u4e92\u4e2d\u5b58\u5728\u9057\u5fd8\u3001\u91cd\u590d\u9519\u8bef\u548c\u7f3a\u4e4f\u6301\u7eed\u81ea\u6211\u6539\u8fdb\u673a\u5236\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u5728\u52a8\u6001\u73af\u5883\u4e2d\u4e0d\u53ef\u9760\u3002", "method": "\u63d0\u51faMemOrb\uff0c\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u3001\u5373\u63d2\u5373\u7528\u7684\u53e3\u5934\u5f3a\u5316\u8bb0\u5fc6\u5c42\uff0c\u5c06\u591a\u8f6e\u4ea4\u4e92\u63d0\u70bc\u6210\u7d27\u51d1\u7684\u7b56\u7565\u53cd\u601d\uff0c\u5e76\u5b58\u50a8\u5728\u5171\u4eab\u8bb0\u5fc6\u5e93\u4e2d\u4ee5\u6307\u5bfc\u51b3\u7b56\uff0c\u65e0\u9700\u5fae\u8c03\u3002", "result": "MemOrb\u663e\u8457\u63d0\u9ad8\u4e86\u4efb\u52a1\u6210\u529f\u7387\u548c\u7a33\u5b9a\u6027\uff0c\u5728\u591a\u8f6e\u4efb\u52a1\u6210\u529f\u7387\u65b9\u9762\u63d0\u5347\u9ad8\u8fbe63\u4e2a\u767e\u5206\u70b9\uff0c\u5e76\u5728\u91cd\u590d\u8bd5\u9a8c\u4e2d\u8868\u73b0\u51fa\u66f4\u4e00\u81f4\u7684\u6027\u80fd\u3002", "conclusion": "\u7ed3\u6784\u5316\u53cd\u601d\u662f\u63d0\u5347\u5ba2\u670d\u573a\u666f\u4e2d\u51b7\u51bbLLM\u4ee3\u7406\u957f\u671f\u53ef\u9760\u6027\u7684\u6709\u6548\u673a\u5236\u3002"}}
{"id": "2509.18734", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18734", "abs": "https://arxiv.org/abs/2509.18734", "authors": ["Nishant Doshi", "Amey Sutvani", "Sanket Gujar"], "title": "Learning Obstacle Avoidance using Double DQN for Quadcopter Navigation", "comment": null, "summary": "One of the challenges faced by Autonomous Aerial Vehicles is reliable\nnavigation through urban environments. Factors like reduction in precision of\nGlobal Positioning System (GPS), narrow spaces and dynamically moving obstacles\nmake the path planning of an aerial robot a complicated task. One of the skills\nrequired for the agent to effectively navigate through such an environment is\nto develop an ability to avoid collisions using information from onboard depth\nsensors. In this paper, we propose Reinforcement Learning of a virtual\nquadcopter robot agent equipped with a Depth Camera to navigate through a\nsimulated urban environment.", "AI": {"tldr": "\u81ea\u4e3b\u98de\u884c\u5668\u5728\u57ce\u5e02\u73af\u5883\u4e2d\u901a\u8fc7\u6df1\u5ea6\u76f8\u673a\u8fdb\u884c\u5bfc\u822a\u7684\u5f3a\u5316\u5b66\u4e60\u3002", "motivation": "\u5e94\u5bf9\u57ce\u5e02\u73af\u5883\u4e2d\u81ea\u4e3b\u98de\u884c\u5668\u5bfc\u822a\u9762\u4e34\u7684\u6311\u6218\uff0c\u5982GPS\u7cbe\u5ea6\u4e0b\u964d\u3001\u72ed\u7a84\u7a7a\u95f4\u548c\u52a8\u6001\u969c\u788d\u7269\u3002", "method": "\u4f7f\u7528\u6df1\u5ea6\u76f8\u673a\u8bad\u7ec3\u4e00\u4e2a\u865a\u62df\u56db\u65cb\u7ffc\u673a\u5668\u4eba\u4ee3\u7406\u8fdb\u884c\u5bfc\u822a\u7684\u5f3a\u5316\u5b66\u4e60\u3002", "result": "\u6210\u529f\u5728\u6a21\u62df\u57ce\u5e02\u73af\u5883\u4e2d\u5b9e\u73b0\u5bfc\u822a\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u53ef\u7528\u4e8e\u8bad\u7ec3\u81ea\u4e3b\u98de\u884c\u5668\u5728\u590d\u6742\u57ce\u5e02\u73af\u5883\u4e2d\u8fdb\u884c\u5bfc\u822a\u3002"}}
{"id": "2509.18427", "categories": ["cs.CV", "physics.med-ph"], "pdf": "https://arxiv.org/pdf/2509.18427", "abs": "https://arxiv.org/abs/2509.18427", "authors": ["Xinyang Wu", "Muheng Li", "Xia Li", "Orso Pusterla", "Sairos Safai", "Philippe C. Cattin", "Antony J. Lomax", "Ye Zhang"], "title": "CPT-4DMR: Continuous sPatial-Temporal Representation for 4D-MRI Reconstruction", "comment": null, "summary": "Four-dimensional MRI (4D-MRI) is an promising technique for capturing\nrespiratory-induced motion in radiation therapy planning and delivery.\nConventional 4D reconstruction methods, which typically rely on phase binning\nor separate template scans, struggle to capture temporal variability,\ncomplicate workflows, and impose heavy computational loads. We introduce a\nneural representation framework that considers respiratory motion as a smooth,\ncontinuous deformation steered by a 1D surrogate signal, completely replacing\nthe conventional discrete sorting approach. The new method fuses motion\nmodeling with image reconstruction through two synergistic networks: the\nSpatial Anatomy Network (SAN) encodes a continuous 3D anatomical\nrepresentation, while a Temporal Motion Network (TMN), guided by\nTransformer-derived respiratory signals, produces temporally consistent\ndeformation fields. Evaluation using a free-breathing dataset of 19 volunteers\ndemonstrates that our template- and phase-free method accurately captures both\nregular and irregular respiratory patterns, while preserving vessel and\nbronchial continuity with high anatomical fidelity. The proposed method\nsignificantly improves efficiency, reducing the total processing time from\napproximately five hours required by conventional discrete sorting methods to\njust 15 minutes of training. Furthermore, it enables inference of each 3D\nvolume in under one second. The framework accurately reconstructs 3D images at\nany respiratory state, achieves superior performance compared to conventional\nmethods, and demonstrates strong potential for application in 4D radiation\ntherapy planning and real-time adaptive treatment.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u56db\u7ef4MRI\uff084D-MRI\uff09\u91cd\u5efa\u65b9\u6cd5\uff0c\u7528\u4e8e\u653e\u5c04\u6cbb\u7597\u8ba1\u5212\u3002\u8be5\u65b9\u6cd5\u5c06\u547c\u5438\u8fd0\u52a8\u89c6\u4e3a\u7531\u4e00\u7ef4\u4fe1\u53f7\u9a71\u52a8\u7684\u8fde\u7eed\u53d8\u5f62\uff0c\u53d6\u4ee3\u4e86\u4f20\u7edf\u7684\u79bb\u6563\u5206\u76f8\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u6548\u7387\u5e76\u80fd\u51c6\u786e\u6355\u6349\u4e0d\u89c4\u5219\u547c\u5438\u6a21\u5f0f\u3002", "motivation": "\u4f20\u7edf\u76844D-MRI\u91cd\u5efa\u65b9\u6cd5\u5728\u6355\u6349\u547c\u5438\u8fd0\u52a8\u7684\u65f6\u95f4\u53d8\u5f02\u6027\u3001\u5de5\u4f5c\u6d41\u7a0b\u590d\u6742\u6027\u548c\u8ba1\u7b97\u8d1f\u8377\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u3001\u66f4\u51c6\u786e\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\uff0c\u5305\u62ec\u7a7a\u95f4\u89e3\u5256\u7f51\u7edc\uff08SAN\uff09\u548c\u65f6\u95f4\u8fd0\u52a8\u7f51\u7edc\uff08TMN\uff09\u3002SAN\u7f16\u7801\u8fde\u7eed\u7684\u4e09\u7ef4\u89e3\u5256\u8868\u793a\uff0cTMN\u5728Transformer\u884d\u751f\u7684\u547c\u5438\u4fe1\u53f7\u7684\u6307\u5bfc\u4e0b\u4ea7\u751f\u65f6\u95f4\u4e00\u81f4\u7684\u5f62\u53d8\u573a\uff0c\u4ee5\u6355\u6349\u547c\u5438\u8fd0\u52a8\u3002", "result": "\u8be5\u65b9\u6cd5\u572819\u540d\u5fd7\u613f\u8005\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u8bc1\u660e\u4e86\u5176\u80fd\u591f\u51c6\u786e\u6355\u6349\u89c4\u5219\u548c\u4e0d\u89c4\u5219\u7684\u547c\u5438\u6a21\u5f0f\uff0c\u540c\u65f6\u4fdd\u6301\u8840\u7ba1\u548c\u652f\u6c14\u7ba1\u7684\u8fde\u7eed\u6027\uff0c\u5e76\u5177\u6709\u9ad8\u89e3\u5256\u4fdd\u771f\u5ea6\u3002\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u603b\u5904\u7406\u65f6\u95f4\u4ece\u7ea6\u4e94\u5c0f\u65f6\u7f29\u77ed\u523015\u5206\u949f\u7684\u8bad\u7ec3\u65f6\u95f4\uff0c\u5e76\u4e14\u6bcf\u4e2a\u4e09\u7ef4\u4f53\u79ef\u7684\u63a8\u7406\u65f6\u95f4\u4e0d\u5230\u4e00\u79d2\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u4ee5\u9ad8\u6548\u7387\u548c\u9ad8\u4fdd\u771f\u5ea6\u91cd\u5efa\u4efb\u610f\u547c\u5438\u72b6\u6001\u4e0b\u7684\u4e09\u7ef4\u56fe\u50cf\uff0c\u57284D\u653e\u5c04\u6cbb\u7597\u8ba1\u5212\u548c\u5b9e\u65f6\u81ea\u9002\u5e94\u6cbb\u7597\u65b9\u9762\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2509.18854", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2509.18854", "abs": "https://arxiv.org/abs/2509.18854", "authors": ["Lukas Brenner", "Beatriz Dias", "Robert Koenig"], "title": "Trading modes against energy", "comment": "61 pages, 5 figures", "summary": "We ask how much energy is required to weakly simulate an $n$-qubit quantum\ncircuit (i.e., produce samples from its output distribution) by a unitary\ncircuit in a hybrid qubit-oscillator model. The latter consists of a certain\nnumber of bosonic modes coupled to a constant number of qubits by a\nJaynes-Cummings Hamiltonian. We find that efficient approximate weak simulation\nof an $n$-qubit quantum circuit of polynomial size with inverse polynomial\nerror is possible with\n  (1) a linear number of bosonic modes and a polynomial amount of energy, or\n  (2) a sublinear (polynomial) number of modes and a subexponential amount of\nenergy, or\n  (3) a constant number of modes and an exponential amount of energy. Our\nconstruction encodes qubits into high-dimensional approximate\nGottesman-Kitaev-Preskill (GKP) codes. It provides new insight into the\ntrade-off between system size (i.e., number of modes) and the amount of energy\nrequired to perform quantum computation in the continuous-variable setting.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u6df7\u5408\u91cf\u5b50\u6bd4\u7279-\u632f\u8361\u5668\u6a21\u578b\u4e2d\uff0c\u4f7f\u7528\u9149\u7ebf\u8def\u8fdb\u884c\u5f31\u6a21\u62dfn\u91cf\u5b50\u6bd4\u7279\u91cf\u5b50\u7535\u8def\u6240\u9700\u7684\u80fd\u91cf\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u53ef\u4ee5\u901a\u8fc7\uff081\uff09\u7ebf\u6027\u6570\u91cf\u7684\u73bb\u8272\u5b50\u6a21\u5f0f\u548c\u591a\u9879\u5f0f\u80fd\u91cf\uff0c\uff082\uff09\u4e9a\u7ebf\u6027\u6570\u91cf\u7684\u6a21\u5f0f\u548c\u4e9a\u6307\u6570\u80fd\u91cf\uff0c\u6216\uff083\uff09\u6052\u5b9a\u6570\u91cf\u7684\u6a21\u5f0f\u548c\u6307\u6570\u80fd\u91cf\u6765\u5b9e\u73b0\u9ad8\u6548\u7684\u8fd1\u4f3c\u5f31\u6a21\u62df\u3002", "motivation": "\u63a2\u7a76\u5728\u6df7\u5408\u91cf\u5b50\u6bd4\u7279-\u632f\u8361\u5668\u6a21\u578b\u4e2d\uff0c\u4f7f\u7528\u9149\u7ebf\u8def\u8fdb\u884c\u5f31\u6a21\u62dfn\u91cf\u5b50\u6bd4\u7279\u91cf\u5b50\u7535\u8def\u6240\u9700\u7684\u80fd\u91cf\u3002", "method": "\u901a\u8fc7\u5c06\u91cf\u5b50\u6bd4\u7279\u7f16\u7801\u5230\u9ad8\u7ef4\u7684\u8fd1\u4f3cGKP\u7801\u4e2d\uff0c\u5e76\u5229\u7528Jaynes-Cummings\u54c8\u5bc6\u987f\u91cf\u5c06\u73bb\u8272\u5b50\u6a21\u5f0f\u8026\u5408\u5230\u91cf\u5b50\u6bd4\u7279\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u53ef\u4ee5\u901a\u8fc7\u4e09\u79cd\u4e0d\u540c\u7684\u8d44\u6e90\u6743\u8861\uff08\u6a21\u5f0f\u6570\u91cf\u4e0e\u80fd\u91cf\u6d88\u8017\uff09\u6765\u5b9e\u73b0\u9ad8\u6548\u7684\u8fd1\u4f3c\u5f31\u6a21\u62df\uff1a1. \u7ebf\u6027\u6570\u91cf\u7684\u6a21\u5f0f\u548c\u591a\u9879\u5f0f\u80fd\u91cf\uff1b2. \u4e9a\u7ebf\u6027\u6570\u91cf\u7684\u6a21\u5f0f\u548c\u4e9a\u6307\u6570\u80fd\u91cf\uff1b3. \u6052\u5b9a\u6570\u91cf\u7684\u6a21\u5f0f\u548c\u6307\u6570\u80fd\u91cf\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u5728\u8fde\u7eed\u53d8\u91cf\u8bbe\u7f6e\u4e2d\uff0c\u7cfb\u7edf\u89c4\u6a21\uff08\u6a21\u5f0f\u6570\u91cf\uff09\u4e0e\u6267\u884c\u91cf\u5b50\u8ba1\u7b97\u6240\u9700\u7684\u80fd\u91cf\u4e4b\u95f4\u5b58\u5728\u6743\u8861\u5173\u7cfb\u3002"}}
{"id": "2509.18138", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18138", "abs": "https://arxiv.org/abs/2509.18138", "authors": ["Tiantian Zhang"], "title": "Rank-Induced PL Mirror Descent: A Rank-Faithful Second-Order Algorithm for Sleeping Experts", "comment": null, "summary": "We introduce a new algorithm, \\emph{Rank-Induced Plackett--Luce Mirror\nDescent (RIPLM)}, which leverages the structural equivalence between the\n\\emph{rank benchmark} and the \\emph{distributional benchmark} established in\n\\citet{BergamOzcanHsu2022}. Unlike prior approaches that operate on expert\nidentities, RIPLM updates directly in the \\emph{rank-induced Plackett--Luce\n(PL)} parameterization. This ensures that the algorithm's played distributions\nremain within the class of rank-induced distributions at every round,\npreserving the equivalence with the rank benchmark. To our knowledge, RIPLM is\nthe first algorithm that is both (i) \\emph{rank-faithful} and (ii)\n\\emph{variance-adaptive} in the sleeping experts setting.", "AI": {"tldr": "RIPLM\u662f\u4e00\u79cd\u65b0\u7684\u7b97\u6cd5\uff0c\u76f4\u63a5\u5728\u79e9\u8bf1\u5bfcPlackett-Luce\u53c2\u6570\u5316\u4e2d\u8fdb\u884c\u66f4\u65b0\uff0c\u4fdd\u6301\u4e86\u4e0e\u79e9\u57fa\u51c6\u7684\u7b49\u4ef7\u6027\uff0c\u5e76\u4e14\u662f\u7b2c\u4e00\u4e2a\u5728\u7761\u7720\u4e13\u5bb6\u8bbe\u7f6e\u4e2d\u5177\u6709\u79e9\u5fe0\u5b9e\u6027\u548c\u65b9\u5dee\u9002\u5e94\u6027\u7684\u7b97\u6cd5\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u7b97\u6cd5RIPLM\uff0c\u5229\u7528\u79e9\u57fa\u51c6\u548c\u5206\u5e03\u57fa\u51c6\u4e4b\u95f4\u7684\u7ed3\u6784\u7b49\u4ef7\u6027\uff0c\u76f4\u63a5\u5728\u79e9\u8bf1\u5bfcPlackett-Luce\u53c2\u6570\u5316\u4e2d\u8fdb\u884c\u66f4\u65b0\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u4e0d\u76f4\u63a5\u5728\u53c2\u6570\u5316\u4e2d\u64cd\u4f5c\u7684\u95ee\u9898\u3002", "method": "RIPLM\u7b97\u6cd5\u76f4\u63a5\u5728\u79e9\u8bf1\u5bfcPlackett-Luce\u53c2\u6570\u5316\u4e2d\u8fdb\u884c\u66f4\u65b0\uff0c\u786e\u4fdd\u7b97\u6cd5\u5728\u6bcf\u4e00\u8f6e\u90fd\u80fd\u4fdd\u6301\u5728\u79e9\u8bf1\u5bfc\u5206\u5e03\u7c7b\u522b\u5185\uff0c\u4ece\u800c\u4fdd\u6301\u4e0e\u79e9\u57fa\u51c6\u7684\u7b49\u4ef7\u6027\u3002", "result": "RIPLM\u7b97\u6cd5\u662f\u7b2c\u4e00\u4e2a\u540c\u65f6\u6ee1\u8db3\u79e9\u5fe0\u5b9e\u6027\u548c\u65b9\u5dee\u9002\u5e94\u6027\u7684\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u5e94\u7528\u4e8e\u7761\u7720\u4e13\u5bb6\u8bbe\u7f6e\u3002", "conclusion": "RIPLM\u7b97\u6cd5\u901a\u8fc7\u76f4\u63a5\u5728\u79e9\u8bf1\u5bfcPlackett-Luce\u53c2\u6570\u5316\u4e2d\u66f4\u65b0\uff0c\u5b9e\u73b0\u4e86\u79e9\u5fe0\u5b9e\u6027\u548c\u65b9\u5dee\u9002\u5e94\u6027\uff0c\u4e3a\u7761\u7720\u4e13\u5bb6\u8bbe\u7f6e\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.18710", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18710", "abs": "https://arxiv.org/abs/2509.18710", "authors": ["Yanjie Fu", "Dongjie Wang", "Wangyang Ying", "Xiangliang Zhang", "Huan Liu", "Jian Pei"], "title": "Autonomous Data Agents: A New Opportunity for Smart Data", "comment": null, "summary": "As data continues to grow in scale and complexity, preparing, transforming,\nand analyzing it remains labor-intensive, repetitive, and difficult to scale.\nSince data contains knowledge and AI learns knowledge from it, the alignment\nbetween AI and data is essential. However, data is often not structured in ways\nthat are optimal for AI utilization. Moreover, an important question arises:\nhow much knowledge can we pack into data through intensive data operations?\nAutonomous data agents (DataAgents), which integrate LLM reasoning with task\ndecomposition, action reasoning and grounding, and tool calling, can\nautonomously interpret data task descriptions, decompose tasks into subtasks,\nreason over actions, ground actions into python code or tool calling, and\nexecute operations. Unlike traditional data management and engineering tools,\nDataAgents dynamically plan workflows, call powerful tools, and adapt to\ndiverse data tasks at scale. This report argues that DataAgents represent a\nparadigm shift toward autonomous data-to-knowledge systems. DataAgents are\ncapable of handling collection, integration, preprocessing, selection,\ntransformation, reweighing, augmentation, reprogramming, repairs, and\nretrieval. Through these capabilities, DataAgents transform complex and\nunstructured data into coherent and actionable knowledge. We first examine why\nthe convergence of agentic AI and data-to-knowledge systems has emerged as a\ncritical trend. We then define the concept of DataAgents and discuss their\narchitectural design, training strategies, as well as the new skills and\ncapabilities they enable. Finally, we call for concerted efforts to advance\naction workflow optimization, establish open datasets and benchmark ecosystems,\nsafeguard privacy, balance efficiency with scalability, and develop trustworthy\nDataAgent guardrails to prevent malicious actions.", "AI": {"tldr": "\u81ea\u4e3b\u6570\u636e\u4ee3\u7406\uff08DataAgents\uff09\u901a\u8fc7\u96c6\u6210LLM\u63a8\u7406\u548c\u4efb\u52a1\u5206\u89e3\u3001\u52a8\u4f5c\u63a8\u7406\u4e0e\u63a5\u5730\u3001\u5de5\u5177\u8c03\u7528\uff0c\u80fd\u591f\u81ea\u4e3b\u89e3\u91ca\u6570\u636e\u4efb\u52a1\u63cf\u8ff0\uff0c\u5c06\u5176\u5206\u89e3\u4e3a\u5b50\u4efb\u52a1\uff0c\u5e76\u6267\u884c\u76f8\u5e94\u64cd\u4f5c\uff0c\u5c06\u590d\u6742\u3001\u975e\u7ed3\u6784\u5316\u6570\u636e\u8f6c\u5316\u4e3a\u8fde\u8d2f\u3001\u53ef\u64cd\u4f5c\u7684\u77e5\u8bc6\uff0c\u4ee3\u8868\u4e86\u6570\u636e\u5230\u77e5\u8bc6\u7cfb\u7edf\u7684\u8303\u5f0f\u8f6c\u53d8\u3002", "motivation": "\u968f\u7740\u6570\u636e\u89c4\u6a21\u548c\u590d\u6742\u6027\u7684\u589e\u957f\uff0c\u6570\u636e\u51c6\u5907\u3001\u8f6c\u6362\u548c\u5206\u6790\u8fc7\u7a0b\u53d8\u5f97\u52b3\u52a8\u5bc6\u96c6\u3001\u91cd\u590d\u4e14\u96be\u4ee5\u6269\u5c55\u3002\u6570\u636e\u4e0eAI\u7684\u5bf9\u9f50\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6570\u636e\u5f80\u5f80\u672a\u9488\u5bf9AI\u5229\u7528\u8fdb\u884c\u4f18\u5316\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u96c6\u7ea6\u5316\u6570\u636e\u64cd\u4f5c\u5728\u6570\u636e\u4e2d\u5c01\u88c5\u66f4\u591a\u77e5\u8bc6\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u81ea\u4e3b\u6570\u636e\u4ee3\u7406\uff08DataAgents\uff09\u7684\u6982\u5ff5\uff0c\u8be5\u4ee3\u7406\u96c6\u6210\u4e86LLM\u63a8\u7406\u3001\u4efb\u52a1\u5206\u89e3\u3001\u52a8\u4f5c\u63a8\u7406\u4e0e\u63a5\u5730\u4ee5\u53ca\u5de5\u5177\u8c03\u7528\u3002DataAgents\u80fd\u591f\u81ea\u4e3b\u89e3\u91ca\u6570\u636e\u4efb\u52a1\u63cf\u8ff0\uff0c\u5c06\u4efb\u52a1\u5206\u89e3\u4e3a\u5b50\u4efb\u52a1\uff0c\u8fdb\u884c\u52a8\u4f5c\u63a8\u7406\uff0c\u5c06\u52a8\u4f5c\u63a5\u5730\u4e3aPython\u4ee3\u7801\u6216\u5de5\u5177\u8c03\u7528\uff0c\u5e76\u6267\u884c\u64cd\u4f5c\u3002\u5b83\u4eec\u80fd\u591f\u52a8\u6001\u89c4\u5212\u5de5\u4f5c\u6d41\uff0c\u8c03\u7528\u5f3a\u5927\u5de5\u5177\uff0c\u5e76\u9002\u5e94\u5404\u79cd\u89c4\u6a21\u7684\u6570\u636e\u4efb\u52a1\u3002", "result": "DataAgents\u80fd\u591f\u5904\u7406\u6570\u636e\u7684\u6536\u96c6\u3001\u96c6\u6210\u3001\u9884\u5904\u7406\u3001\u9009\u62e9\u3001\u8f6c\u6362\u3001\u91cd\u65b0\u52a0\u6743\u3001\u589e\u5f3a\u3001\u91cd\u7f16\u7a0b\u3001\u4fee\u590d\u548c\u68c0\u7d22\u3002\u901a\u8fc7\u8fd9\u4e9b\u80fd\u529b\uff0cDataAgents\u5c06\u590d\u6742\u3001\u975e\u7ed3\u6784\u5316\u7684\u6570\u636e\u8f6c\u5316\u4e3a\u8fde\u8d2f\u3001\u53ef\u64cd\u4f5c\u7684\u77e5\u8bc6\u3002", "conclusion": "DataAgents\u4ee3\u8868\u4e86\u8fc8\u5411\u81ea\u4e3b\u6570\u636e\u5230\u77e5\u8bc6\u7cfb\u7edf\u7684\u8303\u5f0f\u8f6c\u53d8\u3002\u7814\u7a76\u8005\u547c\u5401\u5171\u540c\u52aa\u529b\uff0c\u4ee5\u4f18\u5316\u52a8\u4f5c\u5de5\u4f5c\u6d41\uff0c\u5efa\u7acb\u5f00\u653e\u6570\u636e\u96c6\u548c\u57fa\u51c6\u751f\u6001\u7cfb\u7edf\uff0c\u4fdd\u62a4\u9690\u79c1\uff0c\u5e73\u8861\u6548\u7387\u4e0e\u53ef\u6269\u5c55\u6027\uff0c\u5e76\u5f00\u53d1\u53ef\u4fe1\u8d56\u7684DataAgents\u62a4\u680f\u4ee5\u9632\u6b62\u6076\u610f\u884c\u4e3a\u3002"}}
{"id": "2509.18722", "categories": ["cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.18722", "abs": "https://arxiv.org/abs/2509.18722", "authors": ["Pattara Tipaksorn", "Sumonmas Thatphithakkul", "Vataya Chunwijitra", "Kwanchiva Thangthai"], "title": "LOTUSDIS: A Thai far-field meeting corpus for robust conversational ASR", "comment": null, "summary": "We present LOTUSDIS, a publicly available Thai meeting corpus designed to\nadvance far-field conversational ASR. The dataset comprises 114 hours of\nspontaneous, unscripted dialogue collected in 15-20 minute sessions with three\nparticipants, where overlapping speech is frequent and natural. Speech was\nrecorded simultaneously by nine independent single-channel devices spanning six\nmicrophone types at distances from 0.12 m to 10 m, preserving the authentic\neffects of reverberation, noise, and device coloration without relying on\nmicrophone arrays. We provide standard train, dev, test splits and release a\nreproducible baseline system. We benchmarked several Whisper variants under\nzero-shot and fine-tuned conditions. Off-the-shelf models showed strong\ndegradation with distance, confirming a mismatch between pre-training data and\nThai far-field speech. Fine-tuning on LOTUSDIS dramatically improved\nrobustness: a Thai Whisper baseline reduced overall WER from 64.3 to 38.3 and\nfar-field WER from 81.6 to 49.5, with especially large gains on the most\ndistant microphones. These results underscore the importance of\ndistance-diverse training data for robust ASR. The corpus is available under\nCC-BY-SA 4.0. We also release training and evaluation scripts as a baseline\nsystem to promote reproducible research in this field.", "AI": {"tldr": "LOTUSDIS\u662f\u4e00\u4e2a\u5305\u542b114\u5c0f\u65f6\u6cf0\u8bed\u8fdc\u573a\u5bf9\u8bdd\u6570\u636e\u7684\u8bed\u6599\u5e93\uff0c\u65e8\u5728\u6539\u8fdb\u8fdc\u573a\u5bf9\u8bdd\u8bed\u97f3\u8bc6\u522b\u3002\u8be5\u6570\u636e\u96c6\u5305\u542b\u4e09\u4e2a\u53c2\u4e0e\u8005\u7684\u81ea\u53d1\u5bf9\u8bdd\uff0c\u8bb0\u5f55\u4e86\u771f\u5b9e\u6df7\u54cd\u3001\u566a\u58f0\u548c\u8bbe\u5907\u5f71\u54cd\uff0c\u65e0\u9700\u9ea6\u514b\u98ce\u9635\u5217\u3002\u7814\u7a76\u4eba\u5458\u5bf9\u591a\u79cdWhisper\u6a21\u578b\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u53d1\u73b0\u5728\u672a\u8fdb\u884c\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\uff0c\u6a21\u578b\u6027\u80fd\u968f\u8ddd\u79bb\u589e\u52a0\u800c\u663e\u8457\u4e0b\u964d\u3002\u901a\u8fc7\u5728LOTUSDIS\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5fae\u8c03\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\uff0c\u5176\u4e2d\u6cf0\u8bedWhisper\u57fa\u7ebf\u6a21\u578b\u7684\u603b\u4f53\u8bcd\u9519\u8bef\u7387\uff08WER\uff09\u4ece64.3%\u964d\u81f338.3%\uff0c\u8fdc\u573aWER\u4ece81.6%\u964d\u81f349.5%\uff0c\u5c24\u5176\u662f\u5728\u6700\u8fdc\u8ddd\u79bb\u7684\u9ea6\u514b\u98ce\u4e0a\u3002\u5b9e\u9a8c\u7ed3\u679c\u5f3a\u8c03\u4e86\u5305\u542b\u8ddd\u79bb\u591a\u6837\u5316\u8bad\u7ec3\u6570\u636e\u5bf9\u4e8e\u6784\u5efa\u9c81\u68d2\u7684ASR\u7cfb\u7edf\u7684\u91cd\u8981\u6027\u3002\u8be5\u8bed\u6599\u5e93\u9075\u5faaCC-BY-SA 4.0\u534f\u8bae\u53d1\u5e03\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u8bad\u7ec3\u548c\u8bc4\u4f30\u811a\u672c\u4ee5\u4fc3\u8fdb\u53ef\u590d\u73b0\u7684\u7814\u7a76\u3002", "motivation": "\u4e3a\u4e86\u63a8\u8fdb\u8fdc\u573a\u5bf9\u8bdd\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u6280\u672f\uff0c\u9700\u8981\u4e00\u4e2a\u80fd\u591f\u53cd\u6620\u771f\u5b9e\u4e16\u754c\u5f55\u97f3\u6761\u4ef6\u7684\u6cf0\u8bed\u6570\u636e\u96c6\u3002\u73b0\u6709\u7684\u6570\u636e\u96c6\u53ef\u80fd\u65e0\u6cd5\u5145\u5206\u6355\u6349\u8fdc\u573a\u5f55\u97f3\u4e2d\u5e38\u89c1\u7684\u6df7\u54cd\u3001\u566a\u58f0\u548c\u8bbe\u5907\u5dee\u5f02\u7b49\u6311\u6218\u3002", "method": "\u521b\u5efa\u4e86\u4e00\u4e2a\u5305\u542b114\u5c0f\u65f6\u6cf0\u8bed\u5bf9\u8bdd\u7684LOTUSDIS\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5305\u542b\u4e09\u4e2a\u53c2\u4e0e\u8005\u5728\u4e0d\u540c\u8ddd\u79bb\uff080.12\u7c73\u81f310\u7c73\uff09\u4f7f\u7528\u4e5d\u4e2a\u72ec\u7acb\u5355\u901a\u9053\u8bbe\u5907\u5f55\u5236\u7684\u81ea\u7136\u5bf9\u8bdd\u3002\u540c\u65f6\uff0c\u4ed6\u4eec\u8bc4\u4f30\u4e86\u4e0d\u540cWhisper\u6a21\u578b\u7684\u96f6\u6837\u672c\u548c\u5fae\u8c03\u6027\u80fd\uff0c\u5e76\u8ba1\u7b97\u4e86\u603b\u4f53\u8bcd\u9519\u8bef\u7387\uff08WER\uff09\u548c\u8fdc\u573aWER\u3002", "result": "\u5728LOTUSDIS\u6570\u636e\u96c6\u4e0a\uff0c\u672a\u5fae\u8c03\u7684Whisper\u6a21\u578b\u5728\u8fdc\u573a\u5f55\u97f3\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u7684\u6027\u80fd\u4e0b\u964d\u3002\u7136\u800c\uff0c\u901a\u8fc7\u5728LOTUSDIS\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5fae\u8c03\uff0c\u6a21\u578b\u7684\u9c81\u68d2\u6027\u5f97\u5230\u4e86\u663e\u8457\u63d0\u5347\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6cf0\u8bedWhisper\u57fa\u7ebf\u6a21\u578b\u7684\u603b\u4f53WER\u4ece64.3%\u964d\u4f4e\u523038.3%\uff0c\u8fdc\u573aWER\u4ece81.6%\u964d\u4f4e\u523049.5%\uff0c\u5728\u8ddd\u79bb\u6700\u8fdc\u7684\u9ea6\u514b\u98ce\u4e0a\u6539\u8fdb\u5c24\u4e3a\u660e\u663e\u3002", "conclusion": "\u8ddd\u79bb\u591a\u6837\u5316\u7684\u8bad\u7ec3\u6570\u636e\u5bf9\u4e8e\u63d0\u9ad8\u8fdc\u573a\u5bf9\u8bddASR\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u81f3\u5173\u91cd\u8981\u3002LOTUSDIS\u6570\u636e\u96c6\u548c\u57fa\u7ebf\u7cfb\u7edf\u7684\u53d1\u5e03\u5c06\u6709\u52a9\u4e8e\u63a8\u52a8\u8be5\u9886\u57df\u7684\u53ef\u590d\u73b0\u7814\u7a76\u3002"}}
{"id": "2509.18757", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18757", "abs": "https://arxiv.org/abs/2509.18757", "authors": ["Omar Rayyan", "John Abanes", "Mahmoud Hafez", "Anthony Tzes", "Fares Abu-Dakka"], "title": "MV-UMI: A Scalable Multi-View Interface for Cross-Embodiment Learning", "comment": "For project website and videos, see https https://mv-umi.github.io", "summary": "Recent advances in imitation learning have shown great promise for developing\nrobust robot manipulation policies from demonstrations. However, this promise\nis contingent on the availability of diverse, high-quality datasets, which are\nnot only challenging and costly to collect but are often constrained to a\nspecific robot embodiment. Portable handheld grippers have recently emerged as\nintuitive and scalable alternatives to traditional robotic teleoperation\nmethods for data collection. However, their reliance solely on first-person\nview wrist-mounted cameras often creates limitations in capturing sufficient\nscene contexts. In this paper, we present MV-UMI (Multi-View Universal\nManipulation Interface), a framework that integrates a third-person perspective\nwith the egocentric camera to overcome this limitation. This integration\nmitigates domain shifts between human demonstration and robot deployment,\npreserving the cross-embodiment advantages of handheld data-collection devices.\nOur experimental results, including an ablation study, demonstrate that our\nMV-UMI framework improves performance in sub-tasks requiring broad scene\nunderstanding by approximately 47% across 3 tasks, confirming the effectiveness\nof our approach in expanding the range of feasible manipulation tasks that can\nbe learned using handheld gripper systems, without compromising the\ncross-embodiment advantages inherent to such systems.", "AI": {"tldr": "MV-UMI\u6846\u67b6\u6574\u5408\u4e86\u7b2c\u4e09\u4eba\u79f0\u89c6\u89d2\u548c\u4ee5\u81ea\u6211\u4e3a\u4e2d\u5fc3\u7684\u6444\u50cf\u5934\uff0c\u4ee5\u514b\u670d\u624b\u6301\u6293\u53d6\u5668\u5728\u6536\u96c6\u6570\u636e\u65f6\u4ec5\u4f9d\u8d56\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u6240\u5e26\u6765\u7684\u573a\u666f\u4e0a\u4e0b\u6587\u4fe1\u606f\u4e0d\u8db3\u7684\u9650\u5236\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u7684\u6027\u80fd\u3002", "motivation": "\u624b\u6301\u6293\u53d6\u5668\u867d\u7136\u6613\u4e8e\u6269\u5c55\u4e14\u6210\u672c\u8f83\u4f4e\uff0c\u4f46\u4ec5\u4f9d\u8d56\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u4f1a\u9650\u5236\u5176\u6355\u6349\u8db3\u591f\u7684\u573a\u666f\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u4ece\u800c\u5f71\u54cd\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u7684\u9c81\u68d2\u6027\u3002", "method": "MV-UMI\u6846\u67b6\u6574\u5408\u4e86\u7b2c\u4e09\u4eba\u79f0\u89c6\u89d2\u548c\u4ee5\u81ea\u6211\u4e3a\u4e2d\u5fc3\u7684\u6444\u50cf\u5934\uff0c\u4ee5\u514b\u670d\u624b\u6301\u6293\u53d6\u5668\u5728\u6536\u96c6\u6570\u636e\u65f6\u4ec5\u4f9d\u8d56\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u6240\u5e26\u6765\u7684\u573a\u666f\u4e0a\u4e0b\u6587\u4fe1\u606f\u4e0d\u8db3\u7684\u9650\u5236\u3002", "result": "MV-UMI\u6846\u67b6\u5728\u9700\u8981\u5e7f\u6cdb\u573a\u666f\u7406\u89e3\u7684\u5b50\u4efb\u52a1\u4e2d\uff0c\u6027\u80fd\u5e73\u5747\u63d0\u9ad8\u4e86\u7ea647%\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u6269\u5c55\u624b\u6301\u6293\u53d6\u5668\u7cfb\u7edf\u53ef\u5b66\u4e60\u7684\u64cd\u4f5c\u4efb\u52a1\u8303\u56f4\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u5e76\u4e14\u6ca1\u6709\u727a\u7272\u5176\u56fa\u6709\u7684\u8de8\u8bbe\u5907\u517c\u5bb9\u6027\u3002", "conclusion": "MV-UMI\u6846\u67b6\u901a\u8fc7\u6574\u5408\u591a\u89c6\u89d2\u4fe1\u606f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u624b\u6301\u6293\u53d6\u5668\u5728\u6570\u636e\u6536\u96c6\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u6269\u5927\u4e86\u53ef\u5b66\u4e60\u7684\u64cd\u4f5c\u4efb\u52a1\u8303\u56f4\u3002"}}
{"id": "2509.18451", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18451", "abs": "https://arxiv.org/abs/2509.18451", "authors": ["Prithvi Raj Singh", "Raju Gottumukkala", "Anthony Maida"], "title": "An Analysis of Kalman Filter based Object Tracking Methods for Fast-Moving Tiny Objects", "comment": null, "summary": "Unpredictable movement patterns and small visual mark make precise tracking\nof fast-moving tiny objects like a racquetball one of the challenging problems\nin computer vision. This challenge is particularly relevant for sport robotics\napplications, where lightweight and accurate tracking systems can improve robot\nperception and planning capabilities. While Kalman filter-based tracking\nmethods have shown success in general object tracking scenarios, their\nperformance degrades substantially when dealing with rapidly moving objects\nthat exhibit irregular bouncing behavior. In this study, we evaluate the\nperformance of five state-of-the-art Kalman filter-based tracking\nmethods-OCSORT, DeepOCSORT, ByteTrack, BoTSORT, and StrongSORT-using a custom\ndataset containing 10,000 annotated racquetball frames captured at 720p-1280p\nresolution. We focus our analysis on two critical performance factors:\ninference speed and update frequency per image, examining how these parameters\naffect tracking accuracy and reliability for fast-moving tiny objects. Our\nexperimental evaluation across four distinct scenarios reveals that DeepOCSORT\nachieves the lowest tracking error with an average ADE of 31.15 pixels compared\nto ByteTrack's 114.3 pixels, while ByteTrack demonstrates the fastest\nprocessing at 26.6ms average inference time versus DeepOCSORT's 26.8ms.\nHowever, our results show that all Kalman filter-based trackers exhibit\nsignificant tracking drift with spatial errors ranging from 3-11cm (ADE values:\n31-114 pixels), indicating fundamental limitations in handling the\nunpredictable motion patterns of fast-moving tiny objects like racquetballs.\nOur analysis demonstrates that current tracking approaches require substantial\nimprovements, with error rates 3-4x higher than standard object tracking\nbenchmarks, highlighting the need for specialized methodologies for fast-moving\ntiny object tracking applications.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u4e94\u79cd\u5148\u8fdb\u7684\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u8ffd\u8e2a\u65b9\u6cd5\uff08OCSORT\u3001DeepOCSORT\u3001ByteTrack\u3001BoTSORT\u3001StrongSORT\uff09\u5728\u58c1\u7403\u8ffd\u8e2a\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u5feb\u901f\u79fb\u52a8\u3001\u8fd0\u52a8\u6a21\u5f0f\u4e0d\u53ef\u9884\u6d4b\u7684\u5c0f\u578b\u7269\u4f53\u65f6\u5b58\u5728\u663e\u8457\u5c40\u9650\u6027\u3002", "motivation": "\u7cbe\u786e\u8ffd\u8e2a\u58c1\u7403\u7b49\u5feb\u901f\u79fb\u52a8\u7684\u5c0f\u578b\u7269\u4f53\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u4f53\u80b2\u673a\u5668\u4eba\u9886\u57df\u7684\u4e00\u4e2a\u6311\u6218\uff0c\u73b0\u6709\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u65b9\u6cd5\u5728\u5904\u7406\u6b64\u7c7b\u573a\u666f\u65f6\u6027\u80fd\u4e0b\u964d\uff0c\u9700\u8981\u8bc4\u4f30\u548c\u6539\u8fdb\u3002", "method": "\u4f7f\u7528\u5305\u542b10,000\u5e27\u58c1\u7403\u56fe\u50cf\u7684\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e86\u4e94\u79cd\u5148\u8fdb\u7684\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u8ffd\u8e2a\u65b9\u6cd5\uff08OCSORT\u3001DeepOCSORT\u3001ByteTrack\u3001BoTSORT\u3001StrongSORT\uff09\u5728\u63a8\u7406\u901f\u5ea6\u548c\u6bcf\u56fe\u50cf\u66f4\u65b0\u9891\u7387\u65b9\u9762\u7684\u6027\u80fd\uff0c\u5e76\u5206\u6790\u4e86\u8fd9\u4e9b\u56e0\u7d20\u5bf9\u8ffd\u8e2a\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u7684\u5f71\u54cd\u3002", "result": "DeepOCSORT\u7684\u5e73\u5747\u8f68\u8ff9\u6f02\u79fb\u8ddd\u79bb\uff08ADE\uff09\u6700\u4f4e\uff0831.15\u50cf\u7d20\uff09\uff0c\u800cByteTrack\u7684\u5e73\u5747\u63a8\u7406\u65f6\u95f4\u6700\u77ed\uff0826.6\u6beb\u79d2\uff09\u3002\u7136\u800c\uff0c\u6240\u6709\u65b9\u6cd5\u90fd\u5b58\u5728\u663e\u8457\u7684\u8ffd\u8e2a\u6f02\u79fb\uff083-11\u5398\u7c73\uff09\uff0c\u8868\u660e\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u58c1\u7403\u8fd9\u7c7b\u5feb\u901f\u79fb\u52a8\u3001\u8fd0\u52a8\u6a21\u5f0f\u4e0d\u53ef\u9884\u6d4b\u7684\u7269\u4f53\u65f6\u5b58\u5728\u6839\u672c\u6027\u9650\u5236\uff0c\u9519\u8bef\u7387\u6bd4\u6807\u51c6\u7269\u4f53\u8ffd\u8e2a\u57fa\u51c6\u9ad83-4\u500d\u3002", "conclusion": "\u5f53\u524d\u7684\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u8ffd\u8e2a\u65b9\u6cd5\u5728\u5904\u7406\u5feb\u901f\u79fb\u52a8\u3001\u8fd0\u52a8\u6a21\u5f0f\u4e0d\u53ef\u9884\u6d4b\u7684\u5c0f\u578b\u7269\u4f53\uff08\u5982\u58c1\u7403\uff09\u65b9\u9762\u5b58\u5728\u663e\u8457\u5c40\u9650\u6027\uff0c\u9700\u8981\u5f00\u53d1\u4e13\u95e8\u7684\u65b9\u6cd5\u6765\u6539\u8fdb\u6027\u80fd\u3002"}}
{"id": "2509.18879", "categories": ["quant-ph", "physics.optics"], "pdf": "https://arxiv.org/pdf/2509.18879", "abs": "https://arxiv.org/abs/2509.18879", "authors": ["Heming Wang", "Shanhui Fan"], "title": "Braiding of dynamical eigenvalues of Hermitian bosonic Kitaev chains", "comment": "16 pages, 4 figures", "summary": "In quantum mechanics, observables correspond to Hermitian operators, and the\nspectra are restricted to be real. However, the dynamics of the underlying\nfields may allow complex eigenvalues and therefore create the possibility of\nbraiding structures. Here we study the braiding of dynamical eigenvalues in\nquantum systems by considering Hermitian bosonic Kitaev chains with multiple\nbands. The dynamics of the quantum fields in these systems are described by\ntheir dynamic matrices, which have complex eigenvalues. We show that there are\nsymmetry constraints imposed on these dynamic eigenvalues. Despite these\nconstraints, braiding is possible for frequencies within the effective gain and\nloss regions of the complex plane. We explicitly construct two- and\nthree-strand braidings using the exceptional points found in the system and\ndiscuss possible implementations.", "AI": {"tldr": "\u91cf\u5b50\u7cfb\u7edf\u4e2d\u7684\u52a8\u529b\u5b66\u7279\u5f81\u503c\u53ef\u4ee5\u5f62\u6210\u8fab\u5b50\u7ed3\u6784\uff0c\u5c24\u5176\u662f\u5728\u6709\u6548\u7684\u589e\u76ca\u548c\u635f\u8017\u533a\u57df\u3002", "motivation": "\u7814\u7a76\u5177\u6709\u591a\u4e2a\u80fd\u5e26\u7684\u5384\u7c73\u73bb\u8272\u5b50Kitaev\u94fe\u4e2d\u52a8\u529b\u5b66\u7279\u5f81\u503c\u7684\u8fab\u5b50\u7ed3\u6784\u3002", "method": "\u901a\u8fc7\u5206\u6790\u52a8\u529b\u5b66\u77e9\u9635\u7684\u590d\u6570\u7279\u5f81\u503c\u53ca\u5176\u5bf9\u79f0\u6027\u7ea6\u675f\uff0c\u5e76\u5229\u7528\u7cfb\u7edf\u4e2d\u7684\u5353\u8d8a\u70b9\u6765\u663e\u5f0f\u6784\u9020\u8fab\u5b50\u7ed3\u6784\u3002", "result": "\u8bc1\u660e\u4e86\u52a8\u529b\u5b66\u7279\u5f81\u503c\u7684\u5bf9\u79f0\u6027\u7ea6\u675f\u4ecd\u7136\u5141\u8bb8\u5728\u590d\u5e73\u9762\u7684\u6709\u6548\u589e\u76ca\u548c\u635f\u8017\u533a\u57df\u5185\u5f62\u6210\u8fab\u5b50\uff1b\u663e\u5f0f\u6784\u9020\u4e86\u4e24\u80a1\u548c\u4e09\u80a1\u8fab\u5b50\u3002", "conclusion": "\u5728\u5177\u6709\u591a\u4e2a\u80fd\u5e26\u7684\u5384\u7c73\u73bb\u8272\u5b50Kitaev\u94fe\u4e2d\uff0c\u52a8\u529b\u5b66\u7279\u5f81\u503c\u53ef\u4ee5\u901a\u8fc7\u5353\u8d8a\u70b9\u5f62\u6210\u8fab\u5b50\u7ed3\u6784\uff0c\u8fd9\u5728\u6709\u6548\u7684\u589e\u76ca\u548c\u635f\u8017\u533a\u57df\u662f\u53ef\u80fd\u7684\uff0c\u5e76\u8ba8\u8bba\u4e86\u5b9e\u73b0\u7684\u65b9\u6848\u3002"}}
{"id": "2509.18139", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18139", "abs": "https://arxiv.org/abs/2509.18139", "authors": ["Akshay Murthy", "Shawn Sebastian", "Manil Shangle", "Huaduo Wang", "Sopam Dasgupta", "Gopal Gupta"], "title": "Comparative Analysis of FOLD-SE vs. FOLD-R++ in Binary Classification and XGBoost in Multi-Category Classification", "comment": "7 pages", "summary": "Recently, the demand for Machine Learning (ML) models that can balance\naccuracy, efficiency, and interpreability has grown significantly.\nTraditionally, there has been a tradeoff between accuracy and explainability in\npredictive models, with models such as Neural Networks achieving high accuracy\non complex datasets while sacrificing internal transparency. As such, new\nrule-based algorithms such as FOLD-SE have been developed that provide tangible\njustification for predictions in the form of interpretable rule sets. The\nprimary objective of this study was to compare FOLD-SE and FOLD-R++, both\nrule-based classifiers, in binary classification and evaluate how FOLD-SE\nperforms against XGBoost, a widely used ensemble classifier, when applied to\nmulti-category classification. We hypothesized that because FOLD-SE can\ngenerate a condensed rule set in a more explainable manner, it would lose\nupwards of an average of 3 percent in accuracy and F1 score when compared with\nXGBoost and FOLD-R++ in multiclass and binary classification, respectively. The\nresearch used data collections for classification, with accuracy, F1 scores,\nand processing time as the primary performance measures. Outcomes show that\nFOLD-SE is superior to FOLD-R++ in terms of binary classification by offering\nfewer rules but losing a minor percentage of accuracy and efficiency in\nprocessing time; in tasks that involve multi-category classifications, FOLD-SE\nis more precise and far more efficient compared to XGBoost, in addition to\ngenerating a comprehensible rule set. The results point out that FOLD-SE is a\nbetter choice for both binary tasks and classifications with multiple\ncategories. Therefore, these results demonstrate that rule-based approaches\nlike FOLD-SE can bridge the gap between explainability and performance,\nhighlighting their potential as viable alternatives to black-box models in\ndiverse classification tasks.", "AI": {"tldr": "FOLD-SE\u5728\u4e8c\u5143\u5206\u7c7b\u548c\u591a\u7c7b\u522b\u5206\u7c7b\u4efb\u52a1\u4e2d\u90fd\u4f18\u4e8eFOLD-R++\u548cXGBoost\uff0c\u5e73\u8861\u4e86\u51c6\u786e\u6027\u3001\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u968f\u7740\u5bf9\u673a\u5668\u5b66\u4e60\u6a21\u578b\u51c6\u786e\u6027\u3001\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\u5e73\u8861\u7684\u9700\u6c42\u4e0d\u65ad\u589e\u957f\uff0c\u672c\u7814\u7a76\u65e8\u5728\u6bd4\u8f83FOLD-SE\u548cFOLD-R++\u5728\u4e8c\u5143\u5206\u7c7b\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u8bc4\u4f30FOLD-SE\u5728\u591a\u7c7b\u522b\u5206\u7c7b\u4e2d\u4e0eXGBoost\u7684\u6027\u80fd\u3002", "method": "\u7814\u7a76\u4f7f\u7528\u6570\u636e\u96c6\u5408\u8fdb\u884c\u5206\u7c7b\uff0c\u4e3b\u8981\u6027\u80fd\u6307\u6807\u5305\u62ec\u51c6\u786e\u6027\u3001F1\u5206\u6570\u548c\u5904\u7406\u65f6\u95f4\u3002", "result": "\u5728\u4e8c\u5143\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0cFOLD-SE\u751f\u6210\u7684\u89c4\u5219\u66f4\u5c11\uff0c\u867d\u7136\u51c6\u786e\u6027\u548c\u5904\u7406\u6548\u7387\u7565\u6709\u4e0b\u964d\uff0c\u4f46\u4f18\u4e8eFOLD-R++\u3002\u5728\u591a\u7c7b\u522b\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0cFOLD-SE\u6bd4XGBoost\u66f4\u7cbe\u786e\u3001\u66f4\u9ad8\u6548\uff0c\u5e76\u751f\u6210\u4e86\u53ef\u7406\u89e3\u7684\u89c4\u5219\u96c6\u3002", "conclusion": "FOLD-SE\u5728\u4e8c\u5143\u548c\u591a\u7c7b\u522b\u5206\u7c7b\u4efb\u52a1\u4e2d\u90fd\u662f\u66f4\u597d\u7684\u9009\u62e9\uff0c\u8868\u660e\u50cfFOLD-SE\u8fd9\u6837\u7684\u57fa\u4e8e\u89c4\u5219\u7684\u65b9\u6cd5\u53ef\u4ee5\u5f25\u5408\u53ef\u89e3\u91ca\u6027\u4e0e\u6027\u80fd\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5e76\u4f5c\u4e3a\u591a\u79cd\u5206\u7c7b\u4efb\u52a1\u4e2d\u9ed1\u76d2\u6a21\u578b\u7684\u53ef\u884c\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2509.18771", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18771", "abs": "https://arxiv.org/abs/2509.18771", "authors": ["Xingkun Yin", "Kaibin Huang", "Dong In Kim", "Hongyang Du"], "title": "Experience Scaling: Post-Deployment Evolution For Large Language Models", "comment": null, "summary": "Scaling model size, training data, and compute power have driven advances in\nlarge language models (LLMs), but these approaches are reaching saturation as\nhuman-generated text is exhausted and further gains diminish. We propose\nexperience scaling, a framework for continuous post-deployment evolution for\nLLMs through autonomous interaction with the environment and collaborative\nsharing of accumulated experience. The framework captures raw interactions,\ndistills them into compact, reusable knowledge, and periodically refines stored\ncontent to preserve relevance and efficiency. We validate the framework in\nsimulated real-world scenarios involving generalization to previously unseen\nbut related tasks, repetitive queries, and over-saturated knowledge stores.\nAcross all settings, experience scaling improves accuracy, sustains performance\nover time, and maintains gains when applied to novel situations. These results\ndemonstrate that structured post-deployment learning can extend LLM\ncapabilities beyond the limits of static human-generated data, offering a\nscalable path for continued intelligence progress.", "AI": {"tldr": "\u901a\u8fc7\u6301\u7eed\u7684\u4e0e\u73af\u5883\u4ea4\u4e92\u548c\u7ecf\u9a8c\u5171\u4eab\u6765\u6269\u5c55LLM\u80fd\u529b\uff0c\u4ee5\u514b\u670d\u9759\u6001\u6570\u636e\u7684\u9650\u5236\u3002", "motivation": "\u73b0\u6709\u7684LLM\u6269\u5c55\u65b9\u6cd5\uff08\u5982\u589e\u52a0\u6a21\u578b\u5927\u5c0f\u3001\u8bad\u7ec3\u6570\u636e\u548c\u8ba1\u7b97\u80fd\u529b\uff09\u6b63\u9762\u4e34\u74f6\u9888\uff0c\u56e0\u4e3a\u4eba\u7c7b\u751f\u6210\u6587\u672c\u5df2\u8d8b\u4e8e\u67af\u7aed\uff0c\u4e14\u6536\u76ca\u9012\u51cf\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u7ecf\u9a8c\u6269\u5c55\u201d\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u4e3b\u4e0e\u73af\u5883\u4ea4\u4e92\u548c\u5171\u4eab\u7ecf\u9a8c\u6765\u5b9e\u73b0LLM\u7684\u6301\u7eed\u540e\u90e8\u7f72\u6f14\u8fdb\u3002\u8be5\u6846\u67b6\u80fd\u591f\u6355\u83b7\u539f\u59cb\u4ea4\u4e92\u3001\u5c06\u5176\u63d0\u70bc\u6210\u7d27\u51d1\u7684\u3001\u53ef\u91cd\u7528\u7684\u77e5\u8bc6\uff0c\u5e76\u5b9a\u671f\u4f18\u5316\u5b58\u50a8\u7684\u5185\u5bb9\u4ee5\u4fdd\u6301\u76f8\u5173\u6027\u548c\u6548\u7387\u3002", "result": "\u5728\u6a21\u62df\u7684\u73b0\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\uff0c\u5305\u62ec\u6cdb\u5316\u5230\u4ee5\u524d\u672a\u89c1\u8fc7\u4f46\u76f8\u5173\u7684\u4efb\u52a1\u3001\u91cd\u590d\u67e5\u8be2\u548c\u8fc7\u9971\u548c\u77e5\u8bc6\u5e93\uff0c\u7ecf\u9a8c\u6269\u5c55\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\uff0c\u968f\u7740\u65f6\u95f4\u7684\u63a8\u79fb\u7ef4\u6301\u4e86\u6027\u80fd\uff0c\u5e76\u5728\u5e94\u7528\u4e8e\u65b0\u60c5\u51b5\u65f6\u4fdd\u6301\u4e86\u6536\u76ca\u3002", "conclusion": "\u7ed3\u6784\u5316\u7684\u540e\u90e8\u7f72\u5b66\u4e60\u53ef\u4ee5\u6269\u5c55LLM\u7684\u80fd\u529b\uff0c\u8d85\u8d8a\u9759\u6001\u4eba\u7c7b\u751f\u6210\u6570\u636e\u7684\u9650\u5236\uff0c\u4e3a\u6301\u7eed\u7684\u667a\u80fd\u8fdb\u6b65\u63d0\u4f9b\u4e86\u4e00\u6761\u53ef\u6269\u5c55\u7684\u8def\u5f84\u3002"}}
{"id": "2509.18742", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18742", "abs": "https://arxiv.org/abs/2509.18742", "authors": ["Yunan Wang", "Jianxin Li", "Ziwei Zhang"], "title": "Global-Recent Semantic Reasoning on Dynamic Text-Attributed Graphs with Large Language Models", "comment": null, "summary": "Dynamic Text-Attribute Graphs (DyTAGs), characterized by time-evolving graph\ninteractions and associated text attributes, are prevalent in real-world\napplications. Existing methods, such as Graph Neural Networks (GNNs) and Large\nLanguage Models (LLMs), mostly focus on static TAGs. Extending these existing\nmethods to DyTAGs is challenging as they largely neglect the recent-global\ntemporal semantics: the recent semantic dependencies among interaction texts\nand the global semantic evolution of nodes over time. Furthermore, applying\nLLMs to the abundant and evolving text in DyTAGs faces efficiency issues. To\ntackle these challenges, we propose Dynamic Global-Recent Adaptive Semantic\nProcessing (DyGRASP), a novel method that leverages LLMs and temporal GNNs to\nefficiently and effectively reason on DyTAGs. Specifically, we first design a\nnode-centric implicit reasoning method together with a sliding window mechanism\nto efficiently capture recent temporal semantics. In addition, to capture\nglobal semantic dynamics of nodes, we leverage explicit reasoning with tailored\nprompts and an RNN-like chain structure to infer long-term semantics. Lastly,\nwe intricately integrate the recent and global temporal semantics as well as\nthe dynamic graph structural information using updating and merging layers.\nExtensive experiments on DyTAG benchmarks demonstrate DyGRASP's superiority,\nachieving up to 34% improvement in Hit@10 for destination node retrieval task.\nBesides, DyGRASP exhibits strong generalization across different temporal GNNs\nand LLMs.", "AI": {"tldr": "DyTAGs, which have time-evolving interactions and text attributes, are challenging for existing GNNs and LLMs due to their neglect of recent-global temporal semantics and LLMs' efficiency issues. The proposed DyGRASP method uses LLMs and temporal GNNs to address these challenges by capturing recent semantics with implicit reasoning and a sliding window, and global semantics with explicit reasoning and an RNN-like structure, then integrating them with dynamic graph information.", "motivation": "Existing methods for Dynamic Text-Attribute Graphs (DyTAGs) struggle with capturing both recent and global temporal semantics, and applying LLMs to DyTAGs faces efficiency challenges.", "method": "DyGRASP utilizes LLMs and temporal GNNs. It captures recent semantics via node-centric implicit reasoning and a sliding window. Global semantics are captured through explicit reasoning with tailored prompts and an RNN-like structure. Recent and global semantics, along with graph structure, are integrated using updating and merging layers.", "result": "DyGRASP achieves superior performance on DyTAG benchmarks, with up to a 34% improvement in Hit@10 for destination node retrieval. It also shows strong generalization across different temporal GNNs and LLMs.", "conclusion": "DyGRASP effectively and efficiently processes DyTAGs by integrating recent and global temporal semantics with dynamic graph structures, outperforming existing methods."}}
{"id": "2509.19283", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2509.19283", "abs": "https://arxiv.org/abs/2509.19283", "authors": ["Mehdi Ghasemi", "Mohamad Ali Ghafari", "Masoud Babaei", "Valentina Erastova"], "title": "Molecular Insights into Caprock Integrity of Subsurface Hydrogen Storage: Perspective on Hydrogen-induced Swelling and Mechanical Response", "comment": null, "summary": "The geological storage of hydrogen (H_2) requires reliable long-term caprock\nsealing, yet the nanoscale interactions between H_2 and clay minerals remain\ncritically underexplored despite their importance for storage security. This\nlack of understanding has limited the ability to predict mechanical stability\nand leakage risks in H_2 storage formations. Using molecular simulations, this\nstudy investigates the swelling behavior and mechanical properties of sodium\nmontmorillonite (Mt), a common smectite clay, under varying hydration states\nand interlayer H_2 contents. Results show that H_2 accelerates hydration-state\ntransitions, narrows the stability window of crystalline swelling, and promotes\nasymmetric plume formation in confined interlayers. H_2 alters cation and water\ncoordination, thereby weakening Na^+--Mt electrostatic interactions and\nmodulating H-bond networks at the interface and in the bulk. Mechanical\nanalysis reveals pronounced anisotropy in Mt. In-plane stiffness is mainly\ngoverned by basal spacing expansion, whereas out-of-plane stiffness is highly\nsensitive to the initial presence of water or H_2, which weaken interlayer\ncohesion. Tensile and compressive strengths in the in-plane directions follow\nin-plane stiffness trends, while the out-of-plane tensile strength is governed\nby Mt--water H-bonds. The presence of H_2 further promotes Mt sheets separation\nby disrupting nanoscale liquid bridges. Collectively, these results provide the\nfirst atomistic-scale evidence that intercalated H_2 reshapes swelling\nenergetics, elastic anisotropy, and failure pathways in Mt, highlighting\ncritical nanoscale mechanisms that may compromise caprock integrity during\nunderground H_2 storage.", "AI": {"tldr": "\u6c22\u6c14\uff08H_2\uff09\u7684\u6ce8\u5165\u4f1a\u5f71\u54cd\u94a0\u57fa\u8499\u8131\u77f3\uff08Na+-Mt\uff09\u7684\u6eb6\u80c0\u884c\u4e3a\u548c\u529b\u5b66\u6027\u8d28\uff0c\u53ef\u80fd\u5f71\u54cd\u5730\u4e0b\u50a8\u6c22\u7684\u5b89\u5168\u3002", "motivation": "\u5730\u8d28\u50a8\u6c22\u9700\u8981\u53ef\u9760\u7684\u957f\u6548\u76d6\u5c42\u5bc6\u5c01\uff0c\u4f46H_2\u4e0e\u7c98\u571f\u77ff\u7269\u7684\u7eb3\u7c73\u7ea7\u76f8\u4e92\u4f5c\u7528\u5bf9\u4e8e\u50a8\u5b58\u5b89\u5168\u6027\u81f3\u5173\u91cd\u8981\uff0c\u76ee\u524d\u4ecd\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002", "method": "\u672c\u7814\u7a76\u4f7f\u7528\u5206\u5b50\u6a21\u62df\u7814\u7a76\u4e86\u4e0d\u540c\u6c34\u5408\u72b6\u6001\u548c\u5c42\u95f4H_2\u542b\u91cf\u7684\u94a0\u57fa\u8499\u8131\u77f3\uff08Mt\uff09\u7684\u6eb6\u80c0\u884c\u4e3a\u548c\u529b\u5b66\u6027\u8d28\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0cH_2\u4f1a\u52a0\u901f\u6c34\u5408\u72b6\u6001\u7684\u8f6c\u53d8\uff0c\u7f29\u5c0f\u6676\u4f53\u6eb6\u80c0\u7684\u7a33\u5b9a\u6027\u7a97\u53e3\uff0c\u5e76\u5728\u53d7\u9650\u5c42\u95f4\u4fc3\u8fdb\u975e\u5bf9\u79f0\u7fbd\u6d41\u7684\u5f62\u6210\u3002H_2\u6539\u53d8\u4e86\u9633\u79bb\u5b50\u548c\u6c34\u7684\u914d\u4f4d\uff0c\u4ece\u800c\u524a\u5f31\u4e86Na+-Mt\u7684\u9759\u7535\u76f8\u4e92\u4f5c\u7528\uff0c\u5e76\u8c03\u8282\u4e86\u754c\u9762\u548c\u672c\u4f53\u4e2d\u7684\u6c22\u952e\u7f51\u7edc\u3002\u529b\u5b66\u5206\u6790\u8868\u660e\uff0cMt\u5728\u9762\u5185\u521a\u5ea6\u65b9\u9762\u8868\u73b0\u51fa\u660e\u663e\u7684\u5404\u5411\u5f02\u6027\uff0c\u4e3b\u8981\u53d7\u57fa\u9762\u95f4\u8ddd\u81a8\u80c0\u63a7\u5236\uff0c\u800c\u9762\u5916\u521a\u5ea6\u5219\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u521d\u59cb\u6c34\u4e2d\u6216H_2\u7684\u5b58\u5728\uff0c\u8fd9\u4f1a\u524a\u5f31\u5c42\u95f4\u5185\u805a\u529b\u3002\u9762\u5185\u62c9\u4f38\u548c\u538b\u7f29\u5f3a\u5ea6\u9075\u5faa\u9762\u5185\u521a\u5ea6\u8d8b\u52bf\uff0c\u800c\u9762\u5916\u62c9\u4f38\u5f3a\u5ea6\u5219\u53d7Mt-\u6c34\u6c22\u952e\u63a7\u5236\u3002H_2\u7684\u5b58\u5728\u901a\u8fc7\u7834\u574f\u7eb3\u7c73\u5c3a\u5ea6\u7684\u6db2\u6865\u8fdb\u4e00\u6b65\u4fc3\u8fdb\u4e86Mt\u7247\u7684 \u0627\u0644\u0622\u062e\u0631\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u9996\u6b21\u63d0\u4f9b\u4e86\u539f\u5b50\u5c3a\u5ea6\u4e0a\u7684\u8bc1\u636e\uff0c\u8868\u660e\u63d2\u5165\u7684H_2\u4f1a\u6539\u53d8Mt\u7684\u6eb6\u80c0\u80fd\u91cf\u5b66\u3001\u5f39\u6027\u5404\u5411\u5f02\u6027\u548c\u65ad\u88c2\u8def\u5f84\uff0c\u8fd9\u51f8\u663e\u4e86\u5728\u5730\u4e0b\u50a8\u6c22\u8fc7\u7a0b\u4e2d\u53ef\u80fd\u635f\u5bb3\u76d6\u5c42\u5b8c\u6574\u6027\u7684\u5173\u952e\u7eb3\u7c73\u7ea7\u673a\u5236\u3002"}}
{"id": "2509.18778", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18778", "abs": "https://arxiv.org/abs/2509.18778", "authors": ["Shijia Ge", "Yinxin Zhang", "Shuzhao Xie", "Weixiang Zhang", "Mingcai Zhou", "Zhi Wang"], "title": "VGGT-DP: Generalizable Robot Control via Vision Foundation Models", "comment": "submitted to AAAI 2026", "summary": "Visual imitation learning frameworks allow robots to learn manipulation\nskills from expert demonstrations. While existing approaches mainly focus on\npolicy design, they often neglect the structure and capacity of visual\nencoders, limiting spatial understanding and generalization. Inspired by\nbiological vision systems, which rely on both visual and proprioceptive cues\nfor robust control, we propose VGGT-DP, a visuomotor policy framework that\nintegrates geometric priors from a pretrained 3D perception model with\nproprioceptive feedback. We adopt the Visual Geometry Grounded Transformer\n(VGGT) as the visual encoder and introduce a proprioception-guided visual\nlearning strategy to align perception with internal robot states, improving\nspatial grounding and closed-loop control. To reduce inference latency, we\ndesign a frame-wise token reuse mechanism that compacts multi-view tokens into\nan efficient spatial representation. We further apply random token pruning to\nenhance policy robustness and reduce overfitting. Experiments on challenging\nMetaWorld tasks show that VGGT-DP significantly outperforms strong baselines\nsuch as DP and DP3, particularly in precision-critical and long-horizon\nscenarios.", "AI": {"tldr": "VGGT-DP\u6846\u67b6\u6574\u5408\u4e863D\u611f\u77e5\u6a21\u578b\u7684\u51e0\u4f55\u5148\u9a8c\u548c\u672c\u4f53\u611f\u53d7\u53cd\u9988\uff0c\u7528\u4e8e\u89c6\u89c9\u6a21\u4eff\u5b66\u4e60\uff0c\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u64cd\u63a7\u6280\u80fd\u7684\u7a7a\u95f4\u7406\u89e3\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u6a21\u4eff\u5b66\u4e60\u6846\u67b6\u4e3b\u8981\u5173\u6ce8\u7b56\u7565\u8bbe\u8ba1\uff0c\u5ffd\u7565\u4e86\u89c6\u89c9\u7f16\u7801\u5668\u7684\u7ed3\u6784\u548c\u5bb9\u91cf\uff0c\u9650\u5236\u4e86\u7a7a\u95f4\u7406\u89e3\u548c\u6cdb\u5316\u80fd\u529b\u3002VGGT-DP\u6846\u67b6\u65e8\u5728\u89e3\u51b3\u6b64\u95ee\u9898\u3002", "method": "VGGT-DP\u6846\u67b6\u6574\u5408\u4e86\u89c6\u89c9\u51e0\u4f55\u5730\u9762\u5316Transformer\uff08VGGT\uff09\u4f5c\u4e3a\u89c6\u89c9\u7f16\u7801\u5668\uff0c\u5e76\u5f15\u5165\u4e86\u7531\u672c\u4f53\u611f\u53d7\u5f15\u5bfc\u7684\u89c6\u89c9\u5b66\u4e60\u7b56\u7565\uff0c\u5bf9\u611f\u77e5\u8fdb\u884c\u6821\u51c6\uff0c\u4ee5\u63d0\u9ad8\u7a7a\u95f4\u5b9a\u4f4d\u548c\u95ed\u73af\u63a7\u5236\u3002\u6b64\u5916\uff0c\u6846\u67b6\u8fd8\u8bbe\u8ba1\u4e86\u5e27\u95f4\u6807\u8bb0\u91cd\u7528\u673a\u5236\u4ee5\u964d\u4f4e\u63a8\u7406\u5ef6\u8fdf\uff0c\u5e76\u5e94\u7528\u4e86\u968f\u673a\u6807\u8bb0\u88c1\u526a\u6765\u589e\u5f3a\u7b56\u7565\u7684\u9c81\u68d2\u6027\u5e76\u51cf\u5c11\u8fc7\u62df\u5408\u3002", "result": "\u5728MetaWorld\u7684\u6311\u6218\u6027\u4efb\u52a1\u4e2d\uff0cVGGT-DP\u663e\u8457\u4f18\u4e8eDP\u548cDP3\u7b49\u5f3a\u57fa\u7ebf\u6a21\u578b\uff0c\u7279\u522b\u662f\u5728\u7cbe\u5ea6\u8981\u6c42\u9ad8\u548c\u957f\u65f6\u7a0b\u7684\u573a\u666f\u4e2d\u8868\u73b0\u66f4\u4f73\u3002", "conclusion": "VGGT-DP\u901a\u8fc7\u6574\u5408\u51e0\u4f55\u5148\u9a8c\u548c\u672c\u4f53\u611f\u53d7\u53cd\u9988\uff0c\u6709\u6548\u63d0\u5347\u4e86\u89c6\u89c9\u6a21\u4eff\u5b66\u4e60\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u590d\u6742\u64cd\u63a7\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u4f18\u8d8a\u6027\u3002"}}
{"id": "2509.18473", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18473", "abs": "https://arxiv.org/abs/2509.18473", "authors": ["Binhua Huang", "Wendong Yao", "Shaowu Chen", "Guoxin Wang", "Qingyuan Wang", "Soumyabrata Dev"], "title": "MoCrop: Training Free Motion Guided Cropping for Efficient Video Action Recognition", "comment": "5 pages, 2 figures", "summary": "We introduce MoCrop, a motion-aware adaptive cropping module for efficient\nvideo action recognition in the compressed domain. MoCrop uses motion vectors\nthat are available in H.264 video to locate motion-dense regions and produces a\nsingle clip-level crop that is applied to all I-frames at inference. The module\nis training free, adds no parameters, and can be plugged into diverse\nbackbones. A lightweight pipeline that includes denoising & merge (DM), Monte\nCarlo sampling (MCS), and adaptive cropping (AC) via a motion-density submatrix\nsearch yields robust crops with negligible overhead. On UCF101, MoCrop improves\naccuracy or reduces compute. With ResNet-50, it delivers +3.5% Top-1 accuracy\nat equal FLOPs (attention setting), or +2.4% Top-1 accuracy with 26.5% fewer\nFLOPs (efficiency setting). Applied to CoViAR, it reaches 89.2% Top-1 accuracy\nat the original cost and 88.5% Top-1 accuracy while reducing compute from 11.6\nto 8.5 GFLOPs. Consistent gains on MobileNet-V3, EfficientNet-B1, and Swin-B\nindicate strong generality and make MoCrop practical for real-time deployment\nin the compressed domain. Our code and models are available at\nhttps://github.com/microa/MoCrop.", "AI": {"tldr": "MoCrop\u662f\u4e00\u4e2a\u8fd0\u52a8\u611f\u77e5\u7684\u81ea\u9002\u5e94\u88c1\u526a\u6a21\u5757\uff0c\u7528\u4e8e\u5728\u538b\u7f29\u57df\u4e2d\u9ad8\u6548\u5730\u8fdb\u884c\u89c6\u9891\u52a8\u4f5c\u8bc6\u522b\u3002\u5b83\u5229\u7528H.264\u89c6\u9891\u4e2d\u7684\u8fd0\u52a8\u77e2\u91cf\u6765\u5b9a\u4f4d\u8fd0\u52a8\u5bc6\u96c6\u533a\u57df\uff0c\u5e76\u751f\u6210\u4e00\u4e2a\u5e94\u7528\u4e8e\u6240\u6709I\u5e27\u7684\u5355\u88c1\u526a\u3002\u8be5\u6a21\u5757\u65e0\u9700\u8bad\u7ec3\uff0c\u4e0d\u589e\u52a0\u53c2\u6570\uff0c\u53ef\u63d2\u5165\u5404\u79cd\u9aa8\u5e72\u7f51\u7edc\u3002", "motivation": "\u63d0\u9ad8\u89c6\u9891\u52a8\u4f5c\u8bc6\u522b\u5728\u538b\u7f29\u57df\u4e2d\u7684\u6548\u7387\uff0c\u51cf\u5c11\u8ba1\u7b97\u91cf\u5e76/\u6216\u63d0\u9ad8\u51c6\u786e\u7387\u3002", "method": "\u5229\u7528H.264\u89c6\u9891\u4e2d\u7684\u8fd0\u52a8\u77e2\u91cf\u6765\u5b9a\u4f4d\u8fd0\u52a8\u5bc6\u96c6\u533a\u57df\uff0c\u751f\u6210\u4e00\u4e2a\u5e94\u7528\u4e8e\u6240\u6709I\u5e27\u7684\u5355\u88c1\u526a\u3002\u8be5\u65b9\u6cd5\u5305\u62ec\u53bb\u566a\u4e0e\u5408\u5e76\uff08DM\uff09\u3001\u8499\u7279\u5361\u6d1b\u91c7\u6837\uff08MCS\uff09\u548c\u901a\u8fc7\u8fd0\u52a8\u5bc6\u5ea6\u5b50\u77e9\u9635\u641c\u7d22\u8fdb\u884c\u81ea\u9002\u5e94\u88c1\u526a\uff08AC\uff09\u3002", "result": "\u5728UCF101\u6570\u636e\u96c6\u4e0a\uff0cMoCrop\u4f7f\u7528ResNet-50\u9aa8\u5e72\u7f51\u7edc\uff0c\u5728FLOPs\u76f8\u540c\u7684\u60c5\u51b5\u4e0b\u63d0\u9ad8\u4e86+3.5%\u7684Top-1\u51c6\u786e\u7387\uff08\u6ce8\u610f\u529b\u8bbe\u7f6e\uff09\uff0c\u6216\u8005\u5728FLOPs\u51cf\u5c1126.5%\u7684\u60c5\u51b5\u4e0b\u63d0\u9ad8\u4e86+2.4%\u7684Top-1\u51c6\u786e\u7387\uff08\u6548\u7387\u8bbe\u7f6e\uff09\u3002\u5728CoViAR\u6570\u636e\u96c6\u4e0a\uff0cMoCrop\u8fbe\u5230\u4e8689.2%\u7684Top-1\u51c6\u786e\u7387\uff08\u539f\u59cb\u6210\u672c\uff09\uff0c\u6216\u5728\u8ba1\u7b97\u91cf\u4ece11.6 GFLOPs\u51cf\u5c11\u52308.5 GFLOPs\u7684\u60c5\u51b5\u4e0b\u8fbe\u5230\u4e8688.5%\u7684Top-1\u51c6\u786e\u7387\u3002\u5728MobileNet-V3\u3001EfficientNet-B1\u548cSwin-B\u4e0a\u4e5f\u53d6\u5f97\u4e86\u6301\u7eed\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "MoCrop\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u3001\u65e0\u53c2\u6570\u3001\u901a\u7528\u6027\u5f3a\u7684\u8fd0\u52a8\u611f\u77e5\u81ea\u9002\u5e94\u88c1\u526a\u6a21\u5757\uff0c\u80fd\u591f\u6709\u6548\u5e94\u7528\u4e8e\u538b\u7f29\u57df\u89c6\u9891\u52a8\u4f5c\u8bc6\u522b\uff0c\u5e76\u80fd\u5728\u63d0\u9ad8\u51c6\u786e\u7387\u6216\u51cf\u5c11\u8ba1\u7b97\u91cf\u65b9\u9762\u5e26\u6765\u663e\u8457\u6536\u76ca\uff0c\u9002\u5408\u5b9e\u65f6\u90e8\u7f72\u3002"}}
{"id": "2509.18925", "categories": ["quant-ph", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.18925", "abs": "https://arxiv.org/abs/2509.18925", "authors": ["Pierre Rouchon"], "title": "Diffusive Stochastic Master Equation (SME) with dispersive qubit/cavity coupling", "comment": "Submitted", "summary": "A detailed analysis of the diffusive Stochastic Master Equation (SME) for\nqubit/cavity systems with dispersive coupling is provided. This analysis\nincorporates classical input signals and output signals (measurement outcomes\nthrough homodyne detection). The dynamics of the qubit/cavity density operator\nis shown to converge exponentially towards a slow invariant manifold,\nparameterized via a time-varying deterministic Kraus map by the density\noperator of a fictitious qubit. This fictitious qubit is governed by a SME\nincorporating the classical input/output signals. Extension where the qubit is\nreplaced by any qudit dispersively coupled to an arbitrary set of modes with\ncollective input/output classical signals.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u4f9b\u4e86\u6269\u6563\u968f\u673a\u4e3b\u65b9\u7a0b\uff08SME\uff09\u5728\u63cf\u8ff0\u5177\u6709\u8272\u6563\u8026\u5408\u7684\u91cf\u5b50\u6bd4\u7279/\u8154\u7cfb\u7edf\u65b9\u9762\u7684\u8be6\u7ec6\u5206\u6790\uff0c\u8be5\u5206\u6790\u5305\u542b\u4e86\u7ecf\u5178\u8f93\u5165/\u8f93\u51fa\u4fe1\u53f7\u3002\u7814\u7a76\u8868\u660e\uff0c\u91cf\u5b50\u6bd4\u7279/\u8154\u5bc6\u5ea6\u7b97\u7b26\u7684\u52a8\u529b\u5b66\u6307\u6570\u6536\u655b\u4e8e\u4e00\u4e2a\u7f13\u6162\u4e0d\u53d8\u6d41\u5f62\uff0c\u8be5\u6d41\u5f62\u7531\u4e00\u4e2a\u865a\u6784\u91cf\u5b50\u6bd4\u7279\u7684\u5bc6\u5ea6\u7b97\u7b26\u901a\u8fc7\u4e00\u4e2a\u65f6\u53d8\u786e\u5b9a\u6027Kraus\u56fe\u53c2\u6570\u5316\u3002\u8be5\u865a\u6784\u91cf\u5b50\u6bd4\u7279\u53d7\u4e00\u4e2a\u5305\u542b\u7ecf\u5178\u8f93\u5165/\u8f93\u51fa\u4fe1\u53f7\u7684SME\u63a7\u5236\u3002\u8bba\u6587\u8fd8\u5c06\u5176\u6269\u5c55\u5230\u63cf\u8ff0\u7531\u4efb\u610f\u6a21\u5f0f\u548c\u96c6\u4f53\u8f93\u5165/\u8f93\u51fa\u4fe1\u53f7\u7ec4\u6210\u7684\u7cfb\u7edf\u7684\u52a8\u529b\u5b66\u3002", "motivation": "\u8be5\u7814\u7a76\u7684\u52a8\u673a\u5728\u4e8e\u63d0\u4f9b\u5bf9\u5177\u6709\u8272\u6563\u8026\u5408\u7684\u91cf\u5b50\u6bd4\u7279/\u8154\u7cfb\u7edf\u4f7f\u7528\u6269\u6563\u968f\u673a\u4e3b\u65b9\u7a0b\uff08SME\uff09\u7684\u6df1\u5165\u5206\u6790\uff0c\u5e76\u8003\u8651\u7ecf\u5178\u8f93\u5165/\u8f93\u51fa\u4fe1\u53f7\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u5206\u6790\u6269\u6563\u968f\u673a\u4e3b\u65b9\u7a0b\uff08SME\uff09\uff0c\u5c55\u793a\u91cf\u5b50\u6bd4\u7279/\u8154\u5bc6\u5ea6\u7b97\u7b26\u7684\u52a8\u529b\u5b66\u5982\u4f55\u6307\u6570\u6536\u655b\u5230\u4e00\u4e2a\u7531\u865a\u6784\u91cf\u5b50\u6bd4\u7279\u7684\u5bc6\u5ea6\u7b97\u7b26\u53c2\u6570\u5316\u7684\u7f13\u6162\u4e0d\u53d8\u6d41\u5f62\uff0c\u8be5\u6d41\u5f62\u7531\u4e00\u4e2a\u65f6\u53d8\u786e\u5b9a\u6027Kraus\u56fe\u8868\u793a\u3002\u865a\u6784\u91cf\u5b50\u6bd4\u7279\u7684\u52a8\u529b\u5b66\u7531\u4e00\u4e2a\u5305\u542b\u7ecf\u5178\u8f93\u5165/\u8f93\u51fa\u4fe1\u53f7\u7684SME\u63a7\u5236\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u91cf\u5b50\u6bd4\u7279/\u8154\u5bc6\u5ea6\u7b97\u7b26\u7684\u52a8\u529b\u5b66\u6307\u6570\u6536\u655b\u5230\u4e00\u4e2a\u7f13\u6162\u4e0d\u53d8\u6d41\u5f62\uff0c\u8be5\u6d41\u5f62\u53ef\u4ee5\u901a\u8fc7\u65f6\u53d8\u786e\u5b9a\u6027Kraus\u56fe\u8868\u793a\uff0c\u5e76\u53d7\u4e00\u4e2a\u865a\u6784\u91cf\u5b50\u6bd4\u7279\u7684SME\u63a7\u5236\u3002\u8be5\u65b9\u6cd5\u88ab\u6210\u529f\u6269\u5c55\u5230\u66f4\u4e00\u822c\u7684\u7cfb\u7edf\u3002", "conclusion": "\u8be5\u5206\u6790\u4e3a\u7406\u89e3\u548c\u6a21\u62df\u5177\u6709\u8272\u6563\u8026\u5408\u7684\u91cf\u5b50\u6bd4\u7279/\u8154\u7cfb\u7edf\uff08\u5305\u62ec\u7ecf\u5178\u8f93\u5165/\u8f93\u51fa\u4fe1\u53f7\uff09\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u6846\u67b6\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u7cfb\u7edf\u52a8\u529b\u5b66\u53ef\u4ee5\u88ab\u7b80\u5316\u4e3a\u5728\u7f13\u6162\u4e0d\u53d8\u6d41\u5f62\u4e0a\u7684\u6f14\u5316\uff0c\u5e76\u7531\u4e00\u4e2a\u865a\u6784\u91cf\u5b50\u6bd4\u7279\u7684SME\u63a7\u5236\u3002"}}
{"id": "2509.18140", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18140", "abs": "https://arxiv.org/abs/2509.18140", "authors": ["Iram Wajahat", "Amritpal Singh", "Fazel Keshtkar", "Syed Ahmad Chan Bukhari"], "title": "A Machine Learning Framework for Pathway-Driven Therapeutic Target Discovery in Metabolic Disorders", "comment": "6 pages, 6 figures", "summary": "Metabolic disorders, particularly type 2 diabetes mellitus (T2DM), represent\na significant global health burden, disproportionately impacting genetically\npredisposed populations such as the Pima Indians (a Native American tribe from\nsouth central Arizona). This study introduces a novel machine learning (ML)\nframework that integrates predictive modeling with gene-agnostic pathway\nmapping to identify high-risk individuals and uncover potential therapeutic\ntargets. Using the Pima Indian dataset, logistic regression and t-tests were\napplied to identify key predictors of T2DM, yielding an overall model accuracy\nof 78.43%. To bridge predictive analytics with biological relevance, we\ndeveloped a pathway mapping strategy that links identified predictors to\ncritical signaling networks, including insulin signaling, AMPK, and PPAR\npathways. This approach provides mechanistic insights without requiring direct\nmolecular data. Building upon these connections, we propose therapeutic\nstrategies such as dual GLP-1/GIP receptor agonists, AMPK activators, SIRT1\nmodulators, and phytochemical, further validated through pathway enrichment\nanalyses. Overall, this framework advances precision medicine by offering\ninterpretable and scalable solutions for early detection and targeted\nintervention in metabolic disorders. The key contributions of this work are:\n(1) development of an ML framework combining logistic regression and principal\ncomponent analysis (PCA) for T2DM risk prediction; (2) introduction of a\ngene-agnostic pathway mapping approach to generate mechanistic insights; and\n(3) identification of novel therapeutic strategies tailored for high-risk\npopulations.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.18787", "categories": ["cs.AI", "C.2.4"], "pdf": "https://arxiv.org/pdf/2509.18787", "abs": "https://arxiv.org/abs/2509.18787", "authors": ["Luca Muscariello", "Vijoy Pandey", "Ramiz Polic"], "title": "The AGNTCY Agent Directory Service: Architecture and Implementation", "comment": null, "summary": "The Agent Directory Service (ADS) is a distributed directory for the\ndiscovery of AI agent capabilities, metadata, and provenance. It leverages\ncontent-addressed storage, hierarchical taxonomies, and cryptographic signing\nto enable efficient, verifiable, and multi-dimensional discovery across\nheterogeneous Multi-Agent Systems (MAS). Built on the Open Agentic Schema\nFramework (OASF), ADS decouples capability indexing from content location\nthrough a two-level mapping realized over a Kademlia-based Distributed Hash\nTable (DHT). It reuses mature OCI / ORAS infrastructure for artifact\ndistribution, integrates Sigstore for provenance, and supports schema-driven\nextensibility for emerging agent modalities (LLM prompt agents, MCP servers,\nA2A-enabled components). This paper formalizes the architectural model,\ndescribes storage and discovery layers, explains security and performance\nproperties, and positions ADS within the broader landscape of emerging agent\nregistry and interoperability initiatives.", "AI": {"tldr": "ADS\u662f\u4e00\u4e2a\u5206\u5e03\u5f0fAI\u4ee3\u7406\u53d1\u73b0\u670d\u52a1\uff0c\u5229\u7528\u5185\u5bb9\u5bfb\u5740\u5b58\u50a8\u3001\u5206\u5c42\u5206\u7c7b\u6cd5\u548c\u52a0\u5bc6\u7b7e\u540d\uff0c\u5b9e\u73b0\u8de8\u5f02\u6784\u591a\u4ee3\u7406\u7cfb\u7edf\u7684\u591a\u7ef4\u5ea6\u53d1\u73b0\u3002\u5b83\u57fa\u4e8eOASF\u6784\u5efa\uff0c\u901a\u8fc7Kademlia DHT\u5b9e\u73b0\u7684\u4e24\u7ea7\u6620\u5c04\u6765\u89e3\u8026\u80fd\u529b\u7d22\u5f15\u548c\u5185\u5bb9\u4f4d\u7f6e\uff0c\u5e76\u91cd\u7528\u4e86OCI/ORAS\u57fa\u7840\u8bbe\u65bd\u8fdb\u884c artifact \u5206\u53d1\uff0c\u96c6\u6210\u4e86Sigstore\u8fdb\u884c\u6eaf\u6e90\uff0c\u652f\u6301\u9762\u5411LLM\u63d0\u793a\u4ee3\u7406\u3001MCP\u670d\u52a1\u5668\u548cA2A\u7ec4\u4ef6\u7b49\u65b0\u5174\u4ee3\u7406\u6a21\u5f0f\u7684\u6a21\u5f0f\u9a71\u52a8\u6269\u5c55\u3002", "motivation": "\u4e3aAI\u4ee3\u7406\u63d0\u4f9b\u4e00\u4e2a\u9ad8\u6548\u3001\u53ef\u9a8c\u8bc1\u3001\u591a\u7ef4\u5ea6\u7684\u53d1\u73b0\u670d\u52a1\uff0c\u4ee5\u5e94\u5bf9\u5f02\u6784\u591a\u4ee3\u7406\u7cfb\u7edf\u4e2d\u4ee3\u7406\u80fd\u529b\u7684\u7d22\u5f15\u548c\u53d1\u73b0\u6311\u6218\u3002", "method": "ADS\u57fa\u4e8eOASF\u6784\u5efa\uff0c\u5229\u7528\u5185\u5bb9\u5bfb\u5740\u5b58\u50a8\u3001\u5206\u5c42\u5206\u7c7b\u6cd5\u548c\u52a0\u5bc6\u7b7e\u540d\u3002\u901a\u8fc7Kademlia DHT\u5b9e\u73b0\u7684\u4e24\u7ea7\u6620\u5c04\u6765\u89e3\u8026\u80fd\u529b\u7d22\u5f15\u548c\u5185\u5bb9\u4f4d\u7f6e\u3002\u91cd\u7528OCI/ORAS\u8fdb\u884cartifact\u5206\u53d1\uff0c\u96c6\u6210Sigstore\u8fdb\u884c\u6eaf\u6e90\uff0c\u5e76\u652f\u6301\u6a21\u5f0f\u9a71\u52a8\u7684\u6269\u5c55\u3002", "result": "ADS\u80fd\u591f\u5b9e\u73b0\u9ad8\u6548\u3001\u53ef\u9a8c\u8bc1\u3001\u591a\u7ef4\u5ea6\u7684\u4ee3\u7406\u80fd\u529b\u3001\u5143\u6570\u636e\u548c\u6eaf\u6e90\u7684\u53d1\u73b0\uff0c\u5e76\u652f\u6301\u5bf9\u65b0\u5174\u4ee3\u7406\u6a21\u5f0f\u7684\u6269\u5c55\u3002", "conclusion": "ADS\u4e3aAI\u4ee3\u7406\u7684\u6ce8\u518c\u548c\u4e92\u64cd\u4f5c\u6027\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u57fa\u7840\u67b6\u6784\uff0c\u80fd\u591f\u9002\u5e94\u4e0d\u65ad\u53d1\u5c55\u7684\u4ee3\u7406\u6280\u672f\u3002"}}
{"id": "2509.18750", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18750", "abs": "https://arxiv.org/abs/2509.18750", "authors": ["Julie Kallini", "Dan Jurafsky", "Christopher Potts", "Martijn Bartelds"], "title": "False Friends Are Not Foes: Investigating Vocabulary Overlap in Multilingual Language Models", "comment": null, "summary": "Subword tokenizers trained on multilingual corpora naturally produce\noverlapping tokens across languages. Does token overlap facilitate\ncross-lingual transfer or instead introduce interference between languages?\nPrior work offers mixed evidence, partly due to varied setups and confounders,\nsuch as token frequency or subword segmentation granularity. To address this\nquestion, we devise a controlled experiment where we train bilingual\nautoregressive models on multiple language pairs under systematically varied\nvocabulary overlap settings. Crucially, we explore a new dimension to\nunderstanding how overlap affects transfer: the semantic similarity of tokens\nshared across languages. We first analyze our models' hidden representations\nand find that overlap of any kind creates embedding spaces that capture\ncross-lingual semantic relationships, while this effect is much weaker in\nmodels with disjoint vocabularies. On XNLI and XQuAD, we find that models with\noverlap outperform models with disjoint vocabularies, and that transfer\nperformance generally improves as overlap increases. Overall, our findings\nhighlight the advantages of token overlap in multilingual models and show that\nsubstantial shared vocabulary remains a beneficial design choice for\nmultilingual tokenizers.", "AI": {"tldr": "Subword tokenizers with overlapping tokens improve cross-lingual transfer in multilingual models, outperforming disjoint vocabularies and showing performance gains as overlap increases.", "motivation": "To investigate whether overlapping tokens in multilingual subword tokenizers facilitate cross-lingual transfer or cause interference, addressing mixed evidence from prior work due to varied experimental setups and confounders.", "method": "Devised a controlled experiment training bilingual autoregressive models on multiple language pairs with systematically varied vocabulary overlap settings, including a new dimension of semantic similarity of shared tokens.", "result": "Overlap, regardless of type, created embedding spaces capturing cross-lingual semantic relationships, a weaker effect in models with disjoint vocabularies. Models with overlap outperformed those with disjoint vocabularies on XNLI and XQuAD, with transfer performance generally improving as overlap increased.", "conclusion": "Token overlap offers advantages in multilingual models, and a substantial shared vocabulary is a beneficial design choice for multilingual tokenizers."}}
{"id": "2509.18786", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18786", "abs": "https://arxiv.org/abs/2509.18786", "authors": ["Johannes A. Gaus", "Loris Schneider", "Yitian Shi", "Jongseok Lee", "Rania Rayyes", "Rudolph Triebel"], "title": "Human-Interpretable Uncertainty Explanations for Point Cloud Registration", "comment": null, "summary": "In this paper, we address the point cloud registration problem, where\nwell-known methods like ICP fail under uncertainty arising from sensor noise,\npose-estimation errors, and partial overlap due to occlusion. We develop a\nnovel approach, Gaussian Process Concept Attribution (GP-CA), which not only\nquantifies registration uncertainty but also explains it by attributing\nuncertainty to well-known sources of errors in registration problems. Our\napproach leverages active learning to discover new uncertainty sources in the\nwild by querying informative instances. We validate GP-CA on three publicly\navailable datasets and in our real-world robot experiment. Extensive ablations\nsubstantiate our design choices. Our approach outperforms other\nstate-of-the-art methods in terms of runtime, high sample-efficiency with\nactive learning, and high accuracy. Our real-world experiment clearly\ndemonstrates its applicability. Our video also demonstrates that GP-CA enables\neffective failure-recovery behaviors, yielding more robust robotic perception.", "AI": {"tldr": "GP-CA \u662f\u4e00\u79cd\u65b0\u7684\u70b9\u4e91\u914d\u51c6\u65b9\u6cd5\uff0c\u53ef\u4ee5\u91cf\u5316\u548c\u89e3\u91ca\u914d\u51c6\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u5728\u516c\u5f00\u6570\u636e\u96c6\u548c\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u70b9\u4e91\u914d\u51c6\u65b9\u6cd5\uff08\u5982 ICP\uff09\u5728\u4f20\u611f\u5668\u566a\u58f0\u3001\u59ff\u6001\u4f30\u8ba1\u8bef\u5dee\u548c\u906e\u6321\u5bfc\u81f4\u7684\u90e8\u5206\u91cd\u53e0\u7b49\u4e0d\u786e\u5b9a\u6027\u60c5\u51b5\u4e0b\u4f1a\u5931\u8d25\u3002GP-CA \u65e8\u5728\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "method": "GP-CA \u5229\u7528\u9ad8\u65af\u8fc7\u7a0b\u6982\u5ff5\u5f52\u56e0\uff08GP-CA\uff09\u6765\u91cf\u5316\u548c\u89e3\u91ca\u914d\u51c6\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u901a\u8fc7\u67e5\u8be2\u4fe1\u606f\u5b9e\u4f8b\u6765\u53d1\u73b0\u65b0\u7684\u4e0d\u786e\u5b9a\u6027\u6765\u6e90\u3002\u5b83\u8fd8\u5229\u7528\u4e3b\u52a8\u5b66\u4e60\u6765\u63d0\u9ad8\u6837\u672c\u6548\u7387\u3002", "result": "GP-CA \u5728\u4e09\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u548c\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\u5f97\u5230\u4e86\u9a8c\u8bc1\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u5176\u4ed6\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u5728\u8fd0\u884c\u65f6\u3001\u6837\u672c\u6548\u7387\u548c\u51c6\u786e\u6027\u65b9\u9762\u90fd\u6709\u6240\u63d0\u9ad8\u3002", "conclusion": "GP-CA \u662f\u4e00\u79cd\u6709\u6548\u4e14\u9c81\u68d2\u7684\u70b9\u4e91\u914d\u51c6\u65b9\u6cd5\uff0c\u53ef\u91cf\u5316\u548c\u89e3\u91ca\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u901a\u8fc7\u4e3b\u52a8\u5b66\u4e60\u5b9e\u73b0\u9ad8\u6837\u672c\u6548\u7387\u3002\u5b83\u5728\u673a\u5668\u4eba\u611f\u77e5\u4e2d\u5177\u6709\u5b9e\u9645\u5e94\u7528\u524d\u666f\uff0c\u53ef\u4ee5\u5b9e\u73b0\u6709\u6548\u7684\u6545\u969c\u6062\u590d\u884c\u4e3a\u3002"}}
{"id": "2509.18481", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18481", "abs": "https://arxiv.org/abs/2509.18481", "authors": ["Xinyu Wang", "Zikun Zhou", "Yingjian Li", "Xin An", "Hongpeng Wang"], "title": "Codebook-Based Adaptive Feature Compression With Semantic Enhancement for Edge-Cloud Systems", "comment": null, "summary": "Coding images for machines with minimal bitrate and strong analysis\nperformance is key to effective edge-cloud systems. Several approaches deploy\nan image codec and perform analysis on the reconstructed image. Other methods\ncompress intermediate features using entropy models and subsequently perform\nanalysis on the decoded features. Nevertheless, these methods both perform\npoorly under low-bitrate conditions, as they retain many redundant details or\nlearn over-concentrated symbol distributions. In this paper, we propose a\nCodebook-based Adaptive Feature Compression framework with Semantic\nEnhancement, named CAFC-SE. It maps continuous visual features to discrete\nindices with a codebook at the edge via Vector Quantization (VQ) and\nselectively transmits them to the cloud. The VQ operation that projects feature\nvectors onto the nearest visual primitives enables us to preserve more\ninformative visual patterns under low-bitrate conditions. Hence, CAFC-SE is\nless vulnerable to low-bitrate conditions. Extensive experiments demonstrate\nthe superiority of our method in terms of rate and accuracy.", "AI": {"tldr": "CAFC-SE \u901a\u8fc7\u5411\u91cf\u91cf\u5316 (VQ) \u5c06\u8fde\u7eed\u89c6\u89c9\u7279\u5f81\u6620\u5c04\u5230\u79bb\u6563\u7d22\u5f15\uff0c\u5e76\u5728\u4f4e\u6bd4\u7279\u7387\u6761\u4ef6\u4e0b\u4fdd\u7559\u66f4\u591a\u4fe1\u606f\u6027\u89c6\u89c9\u6a21\u5f0f\uff0c\u4ece\u800c\u5728\u901f\u7387\u548c\u51c6\u786e\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u56fe\u50cf\u7f16\u7801\u65b9\u6cd5\u5728\u4f4e\u6bd4\u7279\u7387\u6761\u4ef6\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u4e3a\u5b83\u4eec\u4f1a\u4fdd\u7559\u5197\u4f59\u7ec6\u8282\u6216\u5b66\u4e60\u8fc7\u5ea6\u96c6\u4e2d\u7684\u7b26\u53f7\u5206\u5e03\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7801\u672c\u7684\u81ea\u9002\u5e94\u7279\u5f81\u538b\u7f29\u6846\u67b6\uff0c\u5e76\u8fdb\u884c\u4e86\u8bed\u4e49\u589e\u5f3a\uff08CAFC-SE\uff09\u3002\u8be5\u6846\u67b6\u5728\u8fb9\u7f18\u901a\u8fc7\u5411\u91cf\u91cf\u5316\uff08VQ\uff09\u5c06\u8fde\u7eed\u89c6\u89c9\u7279\u5f81\u6620\u5c04\u5230\u79bb\u6563\u7d22\u5f15\uff0c\u5e76\u9009\u62e9\u6027\u5730\u5c06\u5b83\u4eec\u4f20\u8f93\u5230\u4e91\u7aef\u3002VQ \u64cd\u4f5c\u5c06\u7279\u5f81\u5411\u91cf\u6295\u5f71\u5230\u6700\u8fd1\u7684\u89c6\u89c9\u539f\u8bed\u4e0a\u3002", "result": "CAFC-SE \u5728\u4f4e\u6bd4\u7279\u7387\u6761\u4ef6\u4e0b\u4e0d\u592a\u5bb9\u6613\u53d7\u5230\u5f71\u54cd\uff0c\u5e76\u5728\u901f\u7387\u548c\u51c6\u786e\u6027\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u3002", "conclusion": "CAFC-SE \u662f\u4e00\u79cd\u6709\u6548\u7684\u56fe\u50cf\u7f16\u7801\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u4f4e\u6bd4\u7279\u7387\u6761\u4ef6\u4e0b\uff0c\u80fd\u591f\u901a\u8fc7 VQ \u6620\u5c04\u4fdd\u7559\u66f4\u591a\u4fe1\u606f\u6027\u89c6\u89c9\u6a21\u5f0f\u3002"}}
{"id": "2509.18947", "categories": ["quant-ph", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18947", "abs": "https://arxiv.org/abs/2509.18947", "authors": ["Hillol Biswas"], "title": "Quantum Random Synthetic Skyrmion Texture Generation, a Qiskit Simulation", "comment": null, "summary": "An integer winding, i.e., topological charge, is a characteristic of\nskyrmions, which are topologically nontrivial spin patterns in magnets. They\nemerge when smooth two-dimensional spin configurations are stabilized by\nconflicting interactions such as exchange, anisotropy, the\nDzyaloshinskii-Moriya interaction, or geometric frustration. These nanoscale\ntextures, which are typically a few to tens of nanometers in size, are strong\n'particle-like' excitations because they are shielded by energy barriers\nconnected to their topology. By exploiting their helicity, i.e., spin rotation\nangle or associated internal modes, as a two-level system, skyrmions can\nfunction as quantum bits or qubits. Two quantized helicity states of a\nnanometer-scale skyrmion encode the logical value states in a 'skyrmion qubit.'\nInterestingly, skyrmion qubits are topologically protected and macroscopic,\ni.e., they involve a large number of spins; however, external influences can\nstill affect them. When the texture is tiny and disconnected, the helicity\nangle of the skyrmion becomes quantized. A qubit basis is made up of the lowest\ntwo energy eigenstates, i.e., symmetric or antisymmetric superpositions of\nopposite helicity, for example. Therefore, Skyrmion textures can provide\nvaluable insights for different purposes. However, is it possible to\nsynthetically generate skyrmion textures using quantum computing? This paper\ninvestigates the possibility and generates a few hundred different textures,\nproducing sample comparisons from various types, which indicate a novel\ndirection for skyrmion-based research based on quantum randomness and other\ncriteria.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5229\u7528\u91cf\u5b50\u8ba1\u7b97\u5408\u6210\u751f\u6210\u65af \u09aa\u09b0\u09bf\u09b8\u09cd\u09a5\u09bf\u09a4\u09bf\u09b0\u7eb9\u7406\u7684\u53ef\u80fd\u6027\u3002", "motivation": "\u65af \u09aa\u09b0\u09bf\u09b8\u09cd\u09a5\u09bf\u09a4\u09bf\u4f5c\u4e3a\u62d3\u6251\u81ea\u65cb\u7ed3\u6784\uff0c\u5177\u6709\u6574\u6570\u7ed5\u7ec4\u6570\uff08\u62d3\u6251\u8377\uff09\uff0c\u80fd\u591f\u88ab\u7528\u4f5c\u91cf\u5b50\u6bd4\u7279\uff08qubits\uff09\u3002\u672c\u7814\u7a76\u7684\u52a8\u673a\u5728\u4e8e\u63a2\u7d22\u5229\u7528\u91cf\u5b50\u8ba1\u7b97\u6765\u5408\u6210\u751f\u6210\u65af \u092a\u0930\u093f\u0938\u094d\u0925\u093f\u0924\u0940\u7eb9\u7406\uff0c\u4ee5\u671f\u5f00\u8f9f\u57fa\u4e8e\u91cf\u5b50\u968f\u673a\u6027\u548c\u5176\u4ed6\u6807\u51c6\u7684\u65af \u092a\u0930\u093f\u0938\u094d\u0925\u093f\u0924\u0940\u7814\u7a76\u65b0\u65b9\u5411\u3002", "method": "\u901a\u8fc7\u91cf\u5b50\u8ba1\u7b97\u751f\u6210\u4e86\u6570\u767e\u79cd\u4e0d\u540c\u7684\u65af \u092a\u0930\u093f\u0938\u094d\u0925\u093f\u0924\u0940\u7eb9\u7406\uff0c\u5e76\u8fdb\u884c\u4e86\u6837\u672c\u6bd4\u8f83\u3002", "result": "\u6210\u529f\u751f\u6210\u4e86\u591a\u79cd\u65af \u092a\u0930\u093f\u0938\u094d\u0925\u093f\u0924\u0940\u7eb9\u7406\uff0c\u5e76\u901a\u8fc7\u6837\u672c\u6bd4\u8f83\u5c55\u793a\u4e86\u5176\u591a\u6837\u6027\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5229\u7528\u91cf\u5b50\u8ba1\u7b97\u5408\u6210\u751f\u6210\u65af \u092a\u0930\u093f\u0938\u094d\u0925\u093f\u0924\u0940\u7eb9\u7406\u662f\u53ef\u884c\u7684\uff0c\u5e76\u4e3a\u76f8\u5173\u7814\u7a76\u6307\u660e\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2509.18141", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.AP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.18141", "abs": "https://arxiv.org/abs/2509.18141", "authors": ["Yao Zhao", "Haoyue Sun", "Yantian Ding", "Yanxun Xu"], "title": "KM-GPT: An Automated Pipeline for Reconstructing Individual Patient Data from Kaplan-Meier Plots", "comment": null, "summary": "Reconstructing individual patient data (IPD) from Kaplan-Meier (KM) plots\nprovides valuable insights for evidence synthesis in clinical research.\nHowever, existing approaches often rely on manual digitization, which is\nerror-prone and lacks scalability. To address these limitations, we develop\nKM-GPT, the first fully automated, AI-powered pipeline for reconstructing IPD\ndirectly from KM plots with high accuracy, robustness, and reproducibility.\nKM-GPT integrates advanced image preprocessing, multi-modal reasoning powered\nby GPT-5, and iterative reconstruction algorithms to generate high-quality IPD\nwithout manual input or intervention. Its hybrid reasoning architecture\nautomates the conversion of unstructured information into structured data flows\nand validates data extraction from complex KM plots. To improve accessibility,\nKM-GPT is equipped with a user-friendly web interface and an integrated AI\nassistant, enabling researchers to reconstruct IPD without requiring\nprogramming expertise. KM-GPT was rigorously evaluated on synthetic and\nreal-world datasets, consistently demonstrating superior accuracy. To\nillustrate its utility, we applied KM-GPT to a meta-analysis of gastric cancer\nimmunotherapy trials, reconstructing IPD to facilitate evidence synthesis and\nbiomarker-based subgroup analyses. By automating traditionally manual processes\nand providing a scalable, web-based solution, KM-GPT transforms clinical\nresearch by leveraging reconstructed IPD to enable more informed downstream\nanalyses, supporting evidence-based decision-making.", "AI": {"tldr": "KM-GPT\u662f\u4e00\u4e2a\u5168\u81ea\u52a8\u5316\u7684AI\u5de5\u5177\uff0c\u7528\u4e8e\u4eceKaplan-Meier\u56fe\u91cd\u5efa\u4e2a\u4f53\u60a3\u8005\u6570\u636e\uff08IPD\uff09\uff0c\u89e3\u51b3\u4e86\u624b\u52a8\u6570\u5b57\u5316\u6548\u7387\u4f4e\u4e14\u6613\u51fa\u9519\u7684\u95ee\u9898\u3002", "motivation": "\u624b\u52a8\u6570\u5b57\u5316Kaplan-Meier\u56fe\u91cd\u5efa\u4e2a\u4f53\u60a3\u8005\u6570\u636e\uff08IPD\uff09\u5b58\u5728\u6548\u7387\u4f4e\u3001\u6613\u51fa\u9519\u548c\u53ef\u6269\u5c55\u6027\u5dee\u7684\u95ee\u9898\uff0c\u963b\u788d\u4e86\u4e34\u5e8a\u7814\u7a76\u4e2d\u7684\u8bc1\u636e\u5408\u6210\u3002", "method": "KM-GPT\u5229\u7528\u5148\u8fdb\u7684\u56fe\u50cf\u9884\u5904\u7406\u3001GPT-5\u591a\u6a21\u6001\u63a8\u7406\u548c\u8fed\u4ee3\u91cd\u5efa\u7b97\u6cd5\uff0c\u5b9e\u73b0\u4e86\u4eceKM\u56fe\u5168\u81ea\u52a8\u3001\u9ad8\u7cbe\u5ea6\u5730\u91cd\u5efaIPD\uff0c\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u3002", "result": "KM-GPT\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u5747\u8868\u73b0\u51fa\u9ad8\u7cbe\u5ea6\u3001\u9c81\u68d2\u6027\u548c\u53ef\u590d\u73b0\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "KM-GPT\u901a\u8fc7\u81ea\u52a8\u5316IPD\u91cd\u5efa\u8fc7\u7a0b\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u3001\u7528\u6237\u53cb\u597d\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u52a0\u901f\u4e34\u5e8a\u7814\u7a76\u4e2d\u7684\u8bc1\u636e\u5408\u6210\u548c\u4e0b\u6e38\u5206\u6790\uff0c\u652f\u6301\u5faa\u8bc1\u51b3\u7b56\u3002"}}
{"id": "2509.18836", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18836", "abs": "https://arxiv.org/abs/2509.18836", "authors": ["Dennis Gross", "Helge Spieker", "Arnaud Gotlieb"], "title": "Bounded PCTL Model Checking of Large Language Model Outputs", "comment": "ICTAI 2025", "summary": "In this paper, we introduce LLMCHECKER, a model-checking-based verification\nmethod to verify the probabilistic computation tree logic (PCTL) properties of\nan LLM text generation process. We empirically show that only a limited number\nof tokens are typically chosen during text generation, which are not always the\nsame. This insight drives the creation of $\\alpha$-$k$-bounded text generation,\nnarrowing the focus to the $\\alpha$ maximal cumulative probability on the\ntop-$k$ tokens at every step of the text generation process. Our verification\nmethod considers an initial string and the subsequent top-$k$ tokens while\naccommodating diverse text quantification methods, such as evaluating text\nquality and biases. The threshold $\\alpha$ further reduces the selected tokens,\nonly choosing those that exceed or meet it in cumulative probability.\nLLMCHECKER then allows us to formally verify the PCTL properties of\n$\\alpha$-$k$-bounded LLMs. We demonstrate the applicability of our method in\nseveral LLMs, including Llama, Gemma, Mistral, Genstruct, and BERT. To our\nknowledge, this is the first time PCTL-based model checking has been used to\ncheck the consistency of the LLM text generation process.", "AI": {"tldr": "LLMCHECKER\u901a\u8fc7\u57fa\u4e8e\u6a21\u578b\u68c0\u67e5\u7684\u65b9\u6cd5\u9a8c\u8bc1LLM\u6587\u672c\u751f\u6210\u7684PCTL\u5c5e\u6027\uff0c\u901a\u8fc7\u5173\u6ce8\u7d2f\u79ef\u6982\u7387\u6700\u9ad8\u7684k\u4e2a\u6807\u8bb0\u6765\u63d0\u9ad8\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "LLM\u6587\u672c\u751f\u6210\u8fc7\u7a0b\u4e2d\uff0c\u6587\u672c\u8d28\u91cf\u548c\u504f\u89c1\u7b49\u56e0\u7d20\u7684\u91cf\u5316\u5bf9\u751f\u6210\u8fc7\u7a0b\u7684\u9a8c\u8bc1\u63d0\u51fa\u4e86\u6311\u6218\u3002\u4f20\u7edf\u7684\u9a8c\u8bc1\u65b9\u6cd5\u96be\u4ee5\u5904\u7406LLM\u7684\u6982\u7387\u6027\u548c\u590d\u6742\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLLMCHECKER\u7684\u57fa\u4e8e\u6a21\u578b\u68c0\u67e5\u7684\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u7528\u4e8e\u9a8c\u8bc1LLM\u6587\u672c\u751f\u6210\u8fc7\u7a0b\u7684\u6982\u7387\u8ba1\u7b97\u6811\u903b\u8f91\uff08PCTL\uff09\u5c5e\u6027\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5f15\u5165$\\\\alpha$-$k$-\u6709\u754c\u6587\u672c\u751f\u6210\u7b56\u7565\uff0c\u5c06\u9a8c\u8bc1\u8fc7\u7a0b\u805a\u7126\u4e8e\u6bcf\u4e00\u6b65\u751f\u6210\u8fc7\u7a0b\u4e2d\u7d2f\u79ef\u6982\u7387\u6700\u9ad8\u7684k\u4e2a\u6807\u8bb0\uff0c\u5e76\u6839\u636e\u9608\u503c$\\\\alpha$\u8fdb\u4e00\u6b65\u7b5b\u9009\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLLMCHECKER\u80fd\u591f\u6709\u6548\u5730\u9a8c\u8bc1Llama\u3001Gemma\u3001Mistral\u3001Genstruct\u548cBERT\u7b49\u591a\u79cdLLM\u7684PCTL\u5c5e\u6027\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u901a\u7528\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "LLMCHECKER\u662f\u9996\u6b21\u5c06PCTL\u6a21\u578b\u68c0\u67e5\u5e94\u7528\u4e8eLLM\u6587\u672c\u751f\u6210\u8fc7\u7a0b\u4e00\u81f4\u6027\u9a8c\u8bc1\u7684\u5c1d\u8bd5\uff0c\u4e3aLLM\u7684\u53ef\u9760\u6027\u548c\u5b89\u5168\u6027\u63d0\u4f9b\u4e86\u65b0\u7684\u9a8c\u8bc1\u9014\u5f84\u3002"}}
{"id": "2509.18762", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18762", "abs": "https://arxiv.org/abs/2509.18762", "authors": ["Yingming Zheng", "Hanqi Li", "Kai Yu", "Lu Chen"], "title": "When Long Helps Short: How Context Length in Supervised Fine-tuning Affects Behavior of Large Language Models", "comment": null, "summary": "Large language models (LLMs) have achieved impressive performance across\nnatural language processing (NLP) tasks. As real-world applications\nincreasingly demand longer context windows, continued pretraining and\nsupervised fine-tuning (SFT) on long-context data has become a common approach.\nWhile the effects of data length in continued pretraining have been extensively\nstudied, their implications for SFT remain unclear. In this work, we\nsystematically investigate how SFT data length influences LLM behavior on\nshort-context tasks. Counterintuitively, we find that long-context SFT improves\nshort-context performance, contrary to the commonly observed degradation from\nlong-context pretraining. To uncover the underlying mechanisms of this\nphenomenon, we first decouple and analyze two key components, Multi-Head\nAttention (MHA) and Feed-Forward Network (FFN), and show that both\nindependently benefit from long-context SFT. We further study their interaction\nand reveal a knowledge preference bias: long-context SFT promotes contextual\nknowledge, while short-context SFT favors parametric knowledge, making\nexclusive reliance on long-context SFT suboptimal. Finally, we demonstrate that\nhybrid training mitigates this bias, offering explainable guidance for\nfine-tuning LLMs.", "AI": {"tldr": "\u957f\u4e0a\u4e0b\u6587\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u53ef\u4ee5\u6539\u5584\u77ed\u4e0a\u4e0b\u6587\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u8fd9\u4e0e\u957f\u4e0a\u4e0b\u6587\u9884\u8bad\u7ec3\u7684\u666e\u904d\u89c2\u5bdf\u5230\u7684\u6027\u80fd\u4e0b\u964d\u76f8\u53cd\u3002", "motivation": "\u7814\u7a76\u957f\u4e0a\u4e0b\u6587SFT\u6570\u636e\u957f\u5ea6\u5bf9LLM\u77ed\u4e0a\u4e0b\u6587\u4efb\u52a1\u884c\u4e3a\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u5206\u6790\u591a\u5934\u6ce8\u610f\uff08MHA\uff09\u548c\u524d\u9988\u7f51\u7edc\uff08FFN\uff09\u8fd9\u4e24\u4e2a\u5173\u952e\u7ec4\u6210\u90e8\u5206\uff0c\u5e76\u7814\u7a76\u5b83\u4eec\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u63ed\u793a\u77e5\u8bc6\u504f\u597d\u504f\u5dee\u3002", "result": "\u957f\u4e0a\u4e0b\u6587SFT\u53ef\u4ee5\u72ec\u7acb\u5730\u6539\u5584MHA\u548cFFN\u7684\u6027\u80fd\u3002\u957f\u4e0a\u4e0b\u6587SFT\u503e\u5411\u4e8e\u4e0a\u4e0b\u6587\u77e5\u8bc6\uff0c\u800c\u77ed\u4e0a\u4e0b\u6587SFT\u503e\u5411\u4e8e\u53c2\u6570\u77e5\u8bc6\u3002\u6df7\u5408\u8bad\u7ec3\u53ef\u4ee5\u51cf\u8f7b\u8fd9\u79cd\u504f\u5dee\u3002", "conclusion": "\u6df7\u5408\u8bad\u7ec3\u53ef\u4ee5\u4f5c\u4e3a\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u6307\u5bfc\uff0c\u7528\u4e8e\u5fae\u8c03LLM\u3002"}}
{"id": "2509.18493", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18493", "abs": "https://arxiv.org/abs/2509.18493", "authors": ["Md Mostafijur Rahman", "Radu Marculescu"], "title": "MK-UNet: Multi-kernel Lightweight CNN for Medical Image Segmentation", "comment": "11 pages, 3 figures, Accepted at ICCV 2025 Workshop CVAMD", "summary": "In this paper, we introduce MK-UNet, a paradigm shift towards\nultra-lightweight, multi-kernel U-shaped CNNs tailored for medical image\nsegmentation. Central to MK-UNet is the multi-kernel depth-wise convolution\nblock (MKDC) we design to adeptly process images through multiple kernels,\nwhile capturing complex multi-resolution spatial relationships. MK-UNet also\nemphasizes the images salient features through sophisticated attention\nmechanisms, including channel, spatial, and grouped gated attention. Our\nMK-UNet network, with a modest computational footprint of only 0.316M\nparameters and 0.314G FLOPs, represents not only a remarkably lightweight, but\nalso significantly improved segmentation solution that provides higher accuracy\nover state-of-the-art (SOTA) methods across six binary medical imaging\nbenchmarks. Specifically, MK-UNet outperforms TransUNet in DICE score with\nnearly 333$\\times$ and 123$\\times$ fewer parameters and FLOPs, respectively.\nSimilarly, when compared against UNeXt, MK-UNet exhibits superior segmentation\nperformance, improving the DICE score up to 6.7% margins while operating with\n4.7$\\times$ fewer #Params. Our MK-UNet also outperforms other recent\nlightweight networks, such as MedT, CMUNeXt, EGE-UNet, and Rolling-UNet, with\nmuch lower computational resources. This leap in performance, coupled with\ndrastic computational gains, positions MK-UNet as an unparalleled solution for\nreal-time, high-fidelity medical diagnostics in resource-limited settings, such\nas point-of-care devices. Our implementation is available at\nhttps://github.com/SLDGroup/MK-UNet.", "AI": {"tldr": "MK-UNet\u662f\u4e00\u79cd\u8d85\u8f7b\u91cf\u7ea7\u3001\u591a\u6838U\u578bCNN\uff0c\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u5206\u5272\uff0c\u53c2\u6570\u91cf\u5c11\u3001\u8ba1\u7b97\u6548\u7387\u9ad8\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u4e86SOTA\u65b9\u6cd5\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u8d85\u8f7b\u91cf\u7ea7\u3001\u8ba1\u7b97\u6548\u7387\u9ad8\u4e14\u5206\u5272\u7cbe\u5ea6\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684CNN\u6a21\u578b\uff0c\u4ee5\u6ee1\u8db3\u8d44\u6e90\u53d7\u9650\u73af\u5883\uff08\u5982\u5373\u65f6\u8bca\u65ad\u8bbe\u5907\uff09\u4e0b\u7684\u533b\u5b66\u56fe\u50cf\u5206\u5272\u9700\u6c42\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u5305\u542b\u591a\u6838\u6df1\u5ea6\u5377\u79ef\uff08MKDC\uff09\u548c\u591a\u79cd\u6ce8\u610f\u529b\u673a\u5236\uff08\u901a\u9053\u3001\u7a7a\u95f4\u3001\u5206\u7ec4\u95e8\u63a7\u6ce8\u610f\u529b\uff09\u7684MK-UNet\u7f51\u7edc\uff0c\u8be5\u7f51\u7edc\u80fd\u591f\u5904\u7406\u591a\u5c3a\u5ea6\u7a7a\u95f4\u5173\u7cfb\u5e76\u63d0\u53d6\u56fe\u50cf\u663e\u8457\u7279\u5f81\u3002", "result": "MK-UNet\u5728\u516d\u4e2a\u4e8c\u5143\u533b\u5b66\u6210\u50cf\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4ee5\u6781\u5c11\u7684\u53c2\u6570\u91cf\uff080.316M\uff09\u548c\u8ba1\u7b97\u91cf\uff080.314G FLOPs\uff09\uff0c\u5728\u5206\u5272\u7cbe\u5ea6\u4e0a\u8d85\u8d8a\u4e86TransUNet\u3001UNeXt\u3001MedT\u3001CMUNeXt\u3001EGE-UNet\u548cRolling-UNet\u7b49SOTA\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u76f8\u5bf9\u4e8eTransUNet\uff0c\u53c2\u6570\u91cf\u548c\u8ba1\u7b97\u91cf\u5206\u522b\u51cf\u5c11\u4e86333\u500d\u548c123\u500d\uff1b\u76f8\u5bf9\u4e8eUNeXt\uff0c\u5206\u5272\u7cbe\u5ea6\uff08DICE\u5206\u6570\uff09\u63d0\u9ad8\u4e866.7%\uff0c\u53c2\u6570\u91cf\u51cf\u5c11\u4e864.7\u500d\u3002", "conclusion": "MK-UNet\u901a\u8fc7\u5176\u8f7b\u91cf\u5316\u7684\u8bbe\u8ba1\u548c\u4f18\u8d8a\u7684\u5206\u5272\u6027\u80fd\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u5b9e\u65f6\u3001\u9ad8\u4fdd\u771f\u533b\u5b66\u8bca\u65ad\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65e0\u4e0e\u4f26\u6bd4\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.19005", "categories": ["quant-ph", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19005", "abs": "https://arxiv.org/abs/2509.19005", "authors": ["Ren\u00e1ta Rusn\u00e1kov\u00e1", "Martin Chovanec", "Juraj Gazda"], "title": "Quantum Annealing for Minimum Bisection Problem: A Machine Learning-based Approach for Penalty Parameter Tuning", "comment": null, "summary": "The Minimum Bisection Problem is a well-known NP-hard problem in\ncombinatorial optimization, with practical applications in areas such as\nparallel computing, network design, and machine learning. In this paper, we\nexamine the potential of using D-Wave Systems' quantum annealing solvers to\nsolve the Minimum Bisection Problem, which we formulate as a Quadratic\nUnconstrained Binary Optimization model. A key challenge in this formulation\nlies in choosing an appropriate penalty parameter, as it plays a crucial role\nin ensuring both the quality of the solution and the satisfaction of the\nproblem's constraints. To address this, we introduce a novel machine\nlearning-based approach for adaptive tuning of the penalty parameter.\nSpecifically, we use a Gradient Boosting Regressor model trained to predict\nsuitable penalty parameter values based on structural properties of the input\ngraph, the number of nodes and the graph's density. This method enables the\npenalty parameter to be adjusted dynamically for each specific problem\ninstance, improving the solver's ability to balance the competing goals of\nminimizing the cut size and maintaining equally sized partitions. We test our\napproach on a large dataset of randomly generated Erd\\H{o}s-R\\'enyi graphs with\nup to 4,000 nodes, and we compare the results with classical partitioning\nalgorithms, Metis and Kernighan-Lin. Experimental findings demonstrate that our\nadaptive tuning strategy significantly improves the performance of the quantum\nannealing hybrid solver and consistently outperforms the classical methods\nused, indicating its potential as an alternative for the graph partitioning\nproblem.", "AI": {"tldr": "\u672c\u7814\u7a76\u4f7f\u7528\u91cf\u5b50\u9000\u706b\u548c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u89e3\u51b3\u6700\u5c0f\u4e8c\u5206\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u4f18\u4e8e\u7ecf\u5178\u7b97\u6cd5\u3002", "motivation": "\u6700\u5c0f\u4e8c\u5206\u95ee\u9898\u662f\u7ec4\u5408\u4f18\u5316\u4e2d\u7684\u4e00\u4e2a\u8457\u540dNP\u56f0\u96be\u95ee\u9898\uff0c\u5728\u5e76\u884c\u8ba1\u7b97\u3001\u7f51\u7edc\u8bbe\u8ba1\u548c\u673a\u5668\u5b66\u4e60\u7b49\u9886\u57df\u6709\u5b9e\u9645\u5e94\u7528\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u4f7f\u7528D-Wave\u91cf\u5b50\u9000\u706b\u6c42\u89e3\u5668\u89e3\u51b3\u6700\u5c0f\u4e8c\u5206\u95ee\u9898\u7684\u6f5c\u529b\u3002", "method": "\u5c06\u6700\u5c0f\u4e8c\u5206\u95ee\u9898\u5efa\u6a21\u4e3a\u4e8c\u6b21\u65e0\u7ea6\u675f\u4e8c\u5143\u4f18\u5316\uff08QUBO\uff09\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u68af\u5ea6\u589e\u5f3a\u56de\u5f52\u5668\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u81ea\u9002\u5e94\u8c03\u6574QUBO\u6a21\u578b\u4e2d\u7684\u60e9\u7f5a\u53c2\u6570\u3002\u8be5\u65b9\u6cd5\u6839\u636e\u8f93\u5165\u56fe\u7684\u7ed3\u6784\u5c5e\u6027\u3001\u8282\u70b9\u6570\u548c\u5bc6\u5ea6\u6765\u9884\u6d4b\u5408\u9002\u7684\u60e9\u7f5a\u53c2\u6570\u503c\u3002", "result": "\u5728\u5305\u542b\u591a\u8fbe4000\u4e2a\u8282\u70b9\u7684\u968f\u673aErdos-Renyi\u56fe\u7684\u5927\u578b\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\u4e86\u8be5\u65b9\u6cd5\uff0c\u5e76\u4e0e\u7ecf\u5178\u7684Metis\u548cKernighan-Lin\u7b97\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u81ea\u9002\u5e94\u8c03\u6574\u7b56\u7565\u663e\u8457\u63d0\u9ad8\u4e86\u91cf\u5b50\u9000\u706b\u6df7\u5408\u6c42\u89e3\u5668\u7684\u6027\u80fd\uff0c\u5e76\u59cb\u7ec8\u4f18\u4e8e\u6240\u4f7f\u7528\u7684\u7ecf\u5178\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u81ea\u9002\u5e94\u60e9\u7f5a\u53c2\u6570\u8c03\u6574\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u63d0\u9ad8\u91cf\u5b50\u9000\u706b\u6c42\u89e3\u5668\u5728\u6700\u5c0f\u4e8c\u5206\u95ee\u9898\u4e0a\u7684\u6027\u80fd\uff0c\u5e76\u4f18\u4e8eMetis\u548cKernighan-Lin\u7b49\u7ecf\u5178\u7b97\u6cd5\uff0c\u663e\u793a\u4e86\u5176\u5728\u56fe\u5212\u5206\u95ee\u9898\u4e0a\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.18144", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18144", "abs": "https://arxiv.org/abs/2509.18144", "authors": ["Yubo Yang", "Yichen Zhu", "Bo Jiang"], "title": "AdaSTI: Conditional Diffusion Models with Adaptive Dependency Modeling for Spatio-Temporal Imputation", "comment": "9 pages", "summary": "Spatio-temporal data abounds in domain like traffic and environmental\nmonitoring. However, it often suffers from missing values due to sensor\nmalfunctions, transmission failures, etc. Recent years have seen continued\nefforts to improve spatio-temporal data imputation performance. Recently\ndiffusion models have outperformed other approaches in various tasks, including\nspatio-temporal imputation, showing competitive performance. Extracting and\nutilizing spatio-temporal dependencies as conditional information is vital in\ndiffusion-based methods. However, previous methods introduce error accumulation\nin this process and ignore the variability of the dependencies in the noisy\ndata at different diffusion steps. In this paper, we propose AdaSTI (Adaptive\nDependency Model in Diffusion-based Spatio-Temporal Imputation), a novel\nspatio-temporal imputation approach based on conditional diffusion model.\nInside AdaSTI, we propose a BiS4PI network based on a bi-directional S4 model\nfor pre-imputation with the imputed result used to extract conditional\ninformation by our designed Spatio-Temporal Conditionalizer (STC)network. We\nalso propose a Noise-Aware Spatio-Temporal (NAST) network with a gated\nattention mechanism to capture the variant dependencies across diffusion steps.\nExtensive experiments on three real-world datasets show that AdaSTI outperforms\nexisting methods in all the settings, with up to 46.4% reduction in imputation\nerror.", "AI": {"tldr": "AdaSTI\u662f\u4e00\u79cd\u57fa\u4e8e\u6761\u4ef6\u6269\u6563\u6a21\u578b\u7684\u65b0\u578b\u65f6\u7a7a\u6570\u636e\u63d2\u8865\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cc\u5411S4\u6a21\u578b\u9884\u63d2\u8865\u63d0\u53d6\u6761\u4ef6\u4fe1\u606f\uff0c\u5e76\u5229\u7528\u566a\u58f0\u611f\u77e5\u65f6\u7a7a\u7f51\u7edc\u6355\u83b7\u4e0d\u540c\u6269\u6563\u6b65\u9aa4\u7684\u4f9d\u8d56\u6027\uff0c\u5728\u4e09\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u65f6\u7a7a\u6570\u636e\uff08\u5982\u4ea4\u901a\u548c\u73af\u5883\u76d1\u6d4b\uff09\u5e38\u56e0\u4f20\u611f\u5668\u6545\u969c\u7b49\u539f\u56e0\u51fa\u73b0\u7f3a\u5931\u503c\u3002\u867d\u7136\u6269\u6563\u6a21\u578b\u5728\u65f6\u7a7a\u63d2\u8865\u65b9\u9762\u8868\u73b0\u51fa\u7ade\u4e89\u529b\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u63d0\u53d6\u65f6\u7a7a\u4f9d\u8d56\u6027\u4f5c\u4e3a\u6761\u4ef6\u4fe1\u606f\u65f6\u5b58\u5728\u8bef\u5dee\u7d2f\u79ef\uff0c\u5e76\u5ffd\u7565\u4e86\u4e0d\u540c\u6269\u6563\u6b65\u9aa4\u4e0b\u4f9d\u8d56\u6027\u7684\u53d8\u5f02\u6027\u3002", "method": "\u63d0\u51faAdaSTI\uff08Adaptive Dependency Model in Diffusion-based Spatio-Temporal Imputation\uff09\uff0c\u5305\u62ec\uff1a1. \u57fa\u4e8e\u53cc\u5411S4\u6a21\u578b\u7684BiS4PI\u7f51\u7edc\u8fdb\u884c\u9884\u63d2\u8865\uff0c\u5e76\u5c06\u63d2\u8865\u7ed3\u679c\u7528\u4e8e\u63d0\u53d6\u6761\u4ef6\u4fe1\u606f\uff1b2. \u8bbe\u8ba1Spatio-Temporal Conditionalizer (STC)\u7f51\u7edc\u63d0\u53d6\u6761\u4ef6\u4fe1\u606f\uff1b3. \u63d0\u51fa\u5e26\u95e8\u63a7\u6ce8\u610f\u529b\u673a\u5236\u7684\u566a\u58f0\u611f\u77e5\u65f6\u7a7a\uff08NAST\uff09\u7f51\u7edc\uff0c\u4ee5\u6355\u6349\u8de8\u6269\u6563\u6b65\u9aa4\u7684\u53d8\u5f02\u4f9d\u8d56\u6027\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cAdaSTI\u5728\u6240\u6709\u8bbe\u7f6e\u4e0b\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u63d2\u8865\u8bef\u5dee\u6700\u591a\u53ef\u964d\u4f4e46.4%\u3002", "conclusion": "AdaSTI\u901a\u8fc7\u6709\u6548\u5904\u7406\u65f6\u7a7a\u4f9d\u8d56\u6027\u7684\u53d8\u5f02\u6027\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u65f6\u7a7a\u6570\u636e\u63d2\u8865\u7684\u6027\u80fd\u3002"}}
{"id": "2509.18846", "categories": ["cs.AI", "I.2.6; I.2.7; J.3"], "pdf": "https://arxiv.org/pdf/2509.18846", "abs": "https://arxiv.org/abs/2509.18846", "authors": ["Hong-Jie Dai", "Zheng-Hao Li", "An-Tai Lu", "Bo-Tsz Shain", "Ming-Ta Li", "Tatheer Hussain Mir", "Kuang-Te Wang", "Min-I Su", "Pei-Kang Liu", "Ming-Ju Tsai"], "title": "Model selection meets clinical semantics: Optimizing ICD-10-CM prediction via LLM-as-Judge evaluation, redundancy-aware sampling, and section-aware fine-tuning", "comment": "28 Pages, 4 Figures, 2 Tables", "summary": "Accurate International Classification of Diseases (ICD) coding is critical\nfor clinical documentation, billing, and healthcare analytics, yet it remains a\nlabour-intensive and error-prone task. Although large language models (LLMs)\nshow promise in automating ICD coding, their challenges in base model\nselection, input contextualization, and training data redundancy limit their\neffectiveness. We propose a modular framework for ICD-10 Clinical Modification\n(ICD-10-CM) code prediction that addresses these challenges through principled\nmodel selection, redundancy-aware data sampling, and structured input design.\nThe framework integrates an LLM-as-judge evaluation protocol with Plackett-Luce\naggregation to assess and rank open-source LLMs based on their intrinsic\ncomprehension of ICD-10-CM code definitions. We introduced embedding-based\nsimilarity measures, a redundancy-aware sampling strategy to remove\nsemantically duplicated discharge summaries. We leverage structured discharge\nsummaries from Taiwanese hospitals to evaluate contextual effects and examine\nsection-wise content inclusion under universal and section-specific modelling\nparadigms. Experiments across two institutional datasets demonstrate that the\nselected base model after fine-tuning consistently outperforms baseline LLMs in\ninternal and external evaluations. Incorporating more clinical sections\nconsistently improves prediction performance. This study uses open-source LLMs\nto establish a practical and principled approach to ICD-10-CM code prediction.\nThe proposed framework provides a scalable, institution-ready solution for\nreal-world deployment of automated medical coding systems by combining informed\nmodel selection, efficient data refinement, and context-aware prompting.", "AI": {"tldr": "\u4f7f\u7528\u6a21\u5757\u5316\u6846\u67b6\u548cLLM-as-judge\u8bc4\u4f30\u534f\u8bae\uff0c\u6539\u8fdbICD-10-CM\u7f16\u7801\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u89e3\u51b3\u6a21\u578b\u9009\u62e9\u3001\u8f93\u5165\u4e0a\u4e0b\u6587\u548c\u6570\u636e\u5197\u4f59\u7684\u6311\u6218\u3002", "motivation": "\u76ee\u524d\u7684ICD\u7f16\u7801\u4efb\u52a1\u52b3\u52a8\u5bc6\u96c6\u4e14\u6613\u51fa\u9519\uff0c\u5c3d\u7ba1LLM\u6709\u6f5c\u529b\u5b9e\u73b0\u81ea\u52a8\u5316\uff0c\u4f46\u5b58\u5728\u6a21\u578b\u9009\u62e9\u3001\u8f93\u5165\u4e0a\u4e0b\u6587\u548c\u6570\u636e\u5197\u4f59\u7b49\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u6a21\u5757\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u5408\u7406\u7684\u6a21\u578b\u9009\u62e9\u3001\u5197\u4f59\u611f\u77e5\u6570\u636e\u91c7\u6837\u548c\u7ed3\u6784\u5316\u8f93\u5165\u8bbe\u8ba1\u6765\u89e3\u51b3LLM\u5728ICD-10-CM\u7f16\u7801\u9884\u6d4b\u4e2d\u7684\u6311\u6218\u3002\u6846\u67b6\u6574\u5408\u4e86LLM-as-judge\u8bc4\u4f30\u534f\u8bae\u548cPlackett-Luce\u805a\u5408\uff0c\u4ee5\u8bc4\u4f30\u548c\u6392\u540d\u5f00\u6e90LLM\u3002\u5f15\u5165\u57fa\u4e8e\u5d4c\u5165\u7684\u76f8\u4f3c\u6027\u5ea6\u91cf\u548c\u5197\u4f59\u611f\u77e5\u91c7\u6837\u7b56\u7565\u6765\u53bb\u9664\u8bed\u4e49\u91cd\u590d\u7684\u51fa\u9662\u5c0f\u7ed3\u3002\u5229\u7528\u53f0\u6e7e\u533b\u9662\u7684\u7ed3\u6784\u5316\u51fa\u9662\u5c0f\u7ed3\u6765\u8bc4\u4f30\u4e0a\u4e0b\u6587\u6548\u5e94\uff0c\u5e76\u5728\u901a\u7528\u548c\u7279\u5b9a\u90e8\u5206\u7684\u5efa\u6a21\u8303\u5f0f\u4e0b\u68c0\u67e5\u5404\u90e8\u5206\u7684\u5305\u542b\u60c5\u51b5\u3002", "result": "\u5728\u4e24\u4e2a\u673a\u6784\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u9009\u7684\u57fa\u7840\u6a21\u578b\u5728\u7ecf\u8fc7\u5fae\u8c03\u540e\uff0c\u5728\u5185\u90e8\u548c\u5916\u90e8\u8bc4\u4f30\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u57fa\u7ebfLLM\u3002\u589e\u52a0\u4e34\u5e8a\u90e8\u5206\u80fd\u6301\u7eed\u63d0\u9ad8\u9884\u6d4b\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u4f7f\u7528\u5f00\u6e90LLM\uff0c\u5efa\u7acb\u4e86\u4e00\u79cd\u5b9e\u7528\u4e14\u6709\u539f\u5219\u7684ICD-10-CM\u7f16\u7801\u9884\u6d4b\u65b9\u6cd5\u3002\u6240\u63d0\u51fa\u7684\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u660e\u667a\u7684\u6a21\u578b\u9009\u62e9\u3001\u9ad8\u6548\u7684\u6570\u636e\u7cbe\u70bc\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u63d0\u793a\uff0c\u4e3a\u81ea\u52a8\u5316\u533b\u7597\u7f16\u7801\u7cfb\u7edf\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u90e8\u7f72\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u3001\u9002\u5408\u673a\u6784\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.18775", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18775", "abs": "https://arxiv.org/abs/2509.18775", "authors": ["Wei-Ning Chiu", "Yu-Hsiang Wang", "Andy Hsiao", "Yu-Shiang Huang", "Chuan-Ju Wang"], "title": "Financial Risk Relation Identification through Dual-view Adaptation", "comment": "11 pages, 3 figures, EMNLP 2025 Main Conference", "summary": "A multitude of interconnected risk events -- ranging from regulatory changes\nto geopolitical tensions -- can trigger ripple effects across firms.\nIdentifying inter-firm risk relations is thus crucial for applications like\nportfolio management and investment strategy. Traditionally, such assessments\nrely on expert judgment and manual analysis, which are, however, subjective,\nlabor-intensive, and difficult to scale. To address this, we propose a\nsystematic method for extracting inter-firm risk relations using Form 10-K\nfilings -- authoritative, standardized financial documents -- as our data\nsource. Leveraging recent advances in natural language processing, our approach\ncaptures implicit and abstract risk connections through unsupervised\nfine-tuning based on chronological and lexical patterns in the filings. This\nenables the development of a domain-specific financial encoder with a deeper\ncontextual understanding and introduces a quantitative risk relation score for\ntransparency, interpretable analysis. Extensive experiments demonstrate that\nour method outperforms strong baselines across multiple evaluation settings.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u5229\u752810-K\u6587\u4ef6\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\uff0c\u901a\u8fc7\u65e0\u76d1\u7763\u5fae\u8c03\u8bc6\u522b\u4f01\u4e1a\u95f4\u98ce\u9669\u5173\u8054\u7684\u7cfb\u7edf\u6027\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u91cf\u5316\u98ce\u9669\u5173\u8054\u5f97\u5206\uff0c\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u8bc6\u522b\u4f01\u4e1a\u95f4\u98ce\u9669\u5173\u8054\u5bf9\u4e8e\u6295\u8d44\u7ec4\u5408\u7ba1\u7406\u548c\u6295\u8d44\u7b56\u7565\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u4e3b\u89c2\u3001\u52b3\u52a8\u5bc6\u96c6\u4e14\u96be\u4ee5\u6269\u5c55\u3002", "method": "\u5229\u752810-K\u6587\u4ef6\u4f5c\u4e3a\u6570\u636e\u6e90\uff0c\u901a\u8fc7\u65e0\u76d1\u7763\u5fae\u8c03\uff0c\u57fa\u4e8e\u6587\u4ef6\u7684\u65f6\u95f4\u548c\u8bcd\u6c47\u6a21\u5f0f\uff0c\u6355\u6349\u9690\u542b\u7684\u62bd\u8c61\u98ce\u9669\u8054\u7cfb\uff0c\u5f00\u53d1\u9886\u57df\u7279\u5b9a\u7684\u91d1\u878d\u7f16\u7801\u5668\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u591a\u79cd\u8bc4\u4f30\u8bbe\u7f6e\u4e2d\u8868\u73b0\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u7cfb\u7edf\u5730\u63d0\u53d6\u4f01\u4e1a\u95f4\u98ce\u9669\u5173\u7cfb\uff0c\u5e76\u63d0\u4f9b\u91cf\u5316\u98ce\u9669\u5173\u7cfb\u5f97\u5206\uff0c\u5177\u6709\u900f\u660e\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2509.18573", "categories": ["cs.LG", "cond-mat.mtrl-sci", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18573", "abs": "https://arxiv.org/abs/2509.18573", "authors": ["Dong Chen", "Jian Liu", "Chun-Long Chen", "Guo-Wei Wei"], "title": "Interaction Topological Transformer for Multiscale Learning in Porous Materials", "comment": "4 figures, 2 tables", "summary": "Porous materials exhibit vast structural diversity and support critical\napplications in gas storage, separations, and catalysis. However, predictive\nmodeling remains challenging due to the multiscale nature of structure-property\nrelationships, where performance is governed by both local chemical\nenvironments and global pore-network topology. These complexities, combined\nwith sparse and unevenly distributed labeled data, hinder generalization across\nmaterial families. We propose the Interaction Topological Transformer (ITT), a\nunified data-efficient framework that leverages novel interaction topology to\ncapture materials information across multiple scales and multiple levels,\nincluding structural, elemental, atomic, and pairwise-elemental organization.\nITT extracts scale-aware features that reflect both compositional and\nrelational structure within complex porous frameworks, and integrates them\nthrough a built-in Transformer architecture that supports joint reasoning\nacross scales. Trained using a two-stage strategy, i.e., self-supervised\npretraining on 0.6 million unlabeled structures followed by supervised\nfine-tuning, ITT achieves state-of-the-art, accurate, and transferable\npredictions for adsorption, transport, and stability properties. This framework\nprovides a principled and scalable path for learning-guided discovery in\nstructurally and chemically diverse porous materials.", "AI": {"tldr": "ITT\u662f\u4e00\u4e2a\u6570\u636e\u9ad8\u6548\u7684\u6846\u67b6\uff0c\u5229\u7528\u65b0\u9896\u7684\u76f8\u4e92\u4f5c\u7528\u62d3\u6251\u6765\u6355\u6349\u591a\u5c3a\u5ea6\u548c\u591a\u5c42\u6b21\u7684\u6750\u6599\u4fe1\u606f\uff0c\u5305\u62ec\u7ed3\u6784\u3001\u5143\u7d20\u3001\u539f\u5b50\u548c\u6210\u5bf9\u5143\u7d20\u7ec4\u7ec7\u3002", "motivation": "\u9884\u6d4b\u591a\u5b54\u6750\u6599\u7684\u7ed3\u6784-\u6027\u8d28\u5173\u7cfb\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u5176\u591a\u5c3a\u5ea6\u6027\u8d28\u3001\u7a00\u758f\u4e14\u5206\u5e03\u4e0d\u5747\u7684\u6807\u8bb0\u6570\u636e\u9650\u5236\u4e86\u8de8\u6750\u6599\u65cf\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u76f8\u4e92\u4f5c\u7528\u62d3\u6251Transformer\uff08ITT\uff09\u6846\u67b6\uff0c\u63d0\u53d6\u53cd\u6620\u590d\u6742\u591a\u5b54\u9aa8\u67b6\u5185\u6210\u5206\u548c\u5173\u7cfb\u7ed3\u6784\u7684\u5c3a\u5ea6\u611f\u77e5\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u5185\u7f6eTransformer\u67b6\u6784\u8fdb\u884c\u96c6\u6210\uff0c\u652f\u6301\u8de8\u5c3a\u5ea6\u7684\u8054\u5408\u63a8\u7406\uff0c\u91c7\u7528\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u548c\u76d1\u7763\u5fae\u8c03\u7684\u4e24\u9636\u6bb5\u7b56\u7565\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "ITT\u5728\u5438\u9644\u3001\u4f20\u8f93\u548c\u7a33\u5b9a\u6027\u6027\u8d28\u65b9\u9762\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u3001\u51c6\u786e\u7684\u4e14\u53ef\u8f6c\u79fb\u7684\u9884\u6d4b\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u7ed3\u6784\u548c\u5316\u5b66\u6027\u8d28\u591a\u6837\u5316\u7684\u591a\u5b54\u6750\u6599\u63d0\u4f9b\u4e86\u6709\u539f\u5219\u7684\u3001\u53ef\u6269\u5c55\u7684\u5b66\u4e60\u5f15\u5bfc\u53d1\u73b0\u9014\u5f84\u3002"}}
{"id": "2509.18830", "categories": ["cs.RO", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18830", "abs": "https://arxiv.org/abs/2509.18830", "authors": ["Suzannah Wistreich", "Baiyu Shi", "Stephen Tian", "Samuel Clarke", "Michael Nath", "Chengyi Xu", "Zhenan Bao", "Jiajun Wu"], "title": "DexSkin: High-Coverage Conformable Robotic Skin for Learning Contact-Rich Manipulation", "comment": "Accepted to CoRL 2025", "summary": "Human skin provides a rich tactile sensing stream, localizing intentional and\nunintentional contact events over a large and contoured region. Replicating\nthese tactile sensing capabilities for dexterous robotic manipulation systems\nremains a longstanding challenge. In this work, we take a step towards this\ngoal by introducing DexSkin. DexSkin is a soft, conformable capacitive\nelectronic skin that enables sensitive, localized, and calibratable tactile\nsensing, and can be tailored to varying geometries. We demonstrate its efficacy\nfor learning downstream robotic manipulation by sensorizing a pair of parallel\njaw gripper fingers, providing tactile coverage across almost the entire finger\nsurfaces. We empirically evaluate DexSkin's capabilities in learning\nchallenging manipulation tasks that require sensing coverage across the entire\nsurface of the fingers, such as reorienting objects in hand and wrapping\nelastic bands around boxes, in a learning-from-demonstration framework. We then\nshow that, critically for data-driven approaches, DexSkin can be calibrated to\nenable model transfer across sensor instances, and demonstrate its\napplicability to online reinforcement learning on real robots. Our results\nhighlight DexSkin's suitability and practicality for learning real-world,\ncontact-rich manipulation. Please see our project webpage for videos and\nvisualizations: https://dex-skin.github.io/.", "AI": {"tldr": "DexSkin\u662f\u4e00\u79cd\u67d4\u8f6f\u3001\u53ef\u5b9a\u5236\u7684\u7535\u5bb9\u5f0f\u7535\u5b50\u76ae\u80a4\uff0c\u53ef\u7528\u4e8e\u673a\u5668\u4eba\u7684\u7cbe\u7ec6\u64cd\u4f5c\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u7684\u89e6\u89c9\u4f20\u611f\u3002", "motivation": "\u5b9e\u73b0\u7075\u5de7\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u6240\u9700\u7684\u89e6\u89c9\u4f20\u611f\u80fd\u529b\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u540d\u4e3aDexSkin\u7684\u67d4\u8f6f\u3001\u53ef\u5f62\u53d8\u3001\u7535\u5bb9\u5f0f\u7684\u7535\u5b50\u76ae\u80a4\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u5939\u722a\u624b\u6307\uff0c\u5b9e\u73b0\u4e86\u5927\u9762\u79ef\u3001\u9ad8\u7cbe\u5ea6\u7684\u89e6\u89c9\u611f\u77e5\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u5b66\u4e60\u4e2d\u7684\u6709\u6548\u6027\u3002", "result": "DexSkin\u80fd\u591f\u8fdb\u884c\u7075\u654f\u3001\u53ef\u5b9a\u4f4d\u3001\u53ef\u6821\u51c6\u7684\u89e6\u89c9\u4f20\u611f\uff0c\u5e76\u53ef\u9002\u5e94\u4e0d\u540c\u51e0\u4f55\u5f62\u72b6\u3002\u5728\u5939\u722a\u624b\u6307\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDexSkin\u80fd\u591f\u652f\u6301\u673a\u5668\u4eba\u8fdb\u884c\u6293\u53d6\u3001\u91cd\u5b9a\u5411\u7269\u4f53\u548c\u7f20\u7ed5\u6a61\u76ae\u7b4b\u7b49\u5177\u6709\u6311\u6218\u6027\u7684\u64cd\u4f5c\u3002\u6b64\u5916\uff0cDexSkin\u53ef\u4ee5\u6821\u51c6\u4ee5\u5b9e\u73b0\u8de8\u4f20\u611f\u5668\u5b9e\u4f8b\u7684\u6a21\u578b\u8fc1\u79fb\uff0c\u5e76\u9002\u7528\u4e8e\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u3002", "conclusion": "DexSkin\u5728\u5b66\u4e60\u73b0\u5b9e\u4e16\u754c\u4e2d\u63a5\u89e6\u5bc6\u96c6\u578b\u64cd\u4f5c\u65b9\u9762\u5177\u6709\u5b9e\u7528\u6027\u548c\u6709\u6548\u6027\u3002"}}
{"id": "2509.18501", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18501", "abs": "https://arxiv.org/abs/2509.18501", "authors": ["Maximilian Fehrentz", "Alexander Winkler", "Thomas Heiliger", "Nazim Haouchine", "Christian Heiliger", "Nassir Navab"], "title": "BridgeSplat: Bidirectionally Coupled CT and Non-Rigid Gaussian Splatting for Deformable Intraoperative Surgical Navigation", "comment": "Accepted at MICCAI 2025", "summary": "We introduce BridgeSplat, a novel approach for deformable surgical navigation\nthat couples intraoperative 3D reconstruction with preoperative CT data to\nbridge the gap between surgical video and volumetric patient data. Our method\nrigs 3D Gaussians to a CT mesh, enabling joint optimization of Gaussian\nparameters and mesh deformation through photometric supervision. By\nparametrizing each Gaussian relative to its parent mesh triangle, we enforce\nalignment between Gaussians and mesh and obtain deformations that can be\npropagated back to update the CT. We demonstrate BridgeSplat's effectiveness on\nvisceral pig surgeries and synthetic data of a human liver under simulation,\nshowing sensible deformations of the preoperative CT on monocular RGB data.\nCode, data, and additional resources can be found at\nhttps://maxfehrentz.github.io/ct-informed-splatting/ .", "AI": {"tldr": "BridgeSplat\u662f\u4e00\u79cd\u65b0\u9896\u7684\u53ef\u53d8\u5f62\u624b\u672f\u5bfc\u822a\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u672f\u4e2d3D\u91cd\u5efa\u4e0e\u672f\u524dCT\u6570\u636e\u76f8\u7ed3\u5408\uff0c\u4ee5\u8fde\u63a5\u624b\u672f\u89c6\u9891\u548c\u60a3\u8005\u4f53\u79ef\u6570\u636e\u3002", "motivation": "\u5c06\u672f\u4e2d3D\u91cd\u5efa\u4e0e\u672f\u524dCT\u6570\u636e\u76f8\u7ed3\u5408\uff0c\u4ee5\u5b9e\u73b0\u53ef\u53d8\u5f62\u624b\u672f\u5bfc\u822a\uff0c\u5f25\u5408\u624b\u672f\u89c6\u9891\u548c\u60a3\u8005\u4f53\u79ef\u6570\u636e\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "method": "\u901a\u8fc7\u5149\u5ea6\u76d1\u7763\u8054\u5408\u4f18\u53163D\u9ad8\u65af\u53c2\u6570\u548c\u7f51\u683c\u53d8\u5f62\u3002\u901a\u8fc7\u5c06\u6bcf\u4e2a\u9ad8\u65af\u76f8\u5bf9\u4e8e\u5176\u7236\u7f51\u683c\u4e09\u89d2\u5f62\u8fdb\u884c\u53c2\u6570\u5316\uff0c\u5b9e\u73b0\u9ad8\u65af\u4e0e\u7f51\u683c\u7684\u5bf9\u9f50\uff0c\u5e76\u5c06\u53d8\u5f62\u4f20\u64ad\u56de\u66f4\u65b0CT\u3002", "result": "\u5728\u5185\u810f\u732a\u624b\u672f\u548c\u6a21\u62df\u4eba\u7c7b\u809d\u810f\u7684\u5408\u6210\u6570\u636e\u4e0a\u8bc1\u660e\u4e86BridgeSplat\u7684\u6709\u6548\u6027\uff0c\u5e76\u5728\u5355\u76eeRGB\u6570\u636e\u4e0a\u5b9e\u73b0\u4e86\u672f\u524dCT\u7684\u5408\u7406\u53d8\u5f62\u3002", "conclusion": "BridgeSplat\u80fd\u591f\u6709\u6548\u5730\u5b9e\u73b0\u53ef\u53d8\u5f62\u624b\u672f\u5bfc\u822a\uff0c\u5e76\u901a\u8fc7\u5c06\u672f\u4e2d3D\u91cd\u5efa\u4e0e\u672f\u524dCT\u6570\u636e\u76f8\u7ed3\u5408\uff0c\u5bf9\u672f\u524dCT\u8fdb\u884c\u5408\u7406\u53d8\u5f62\u3002"}}
{"id": "2509.19022", "categories": ["quant-ph", "cond-mat.quant-gas", "physics.atom-ph", "physics.optics"], "pdf": "https://arxiv.org/pdf/2509.19022", "abs": "https://arxiv.org/abs/2509.19022", "authors": ["Philipp Stammer"], "title": "High harmonic generation from a Bose-Einstein condensate", "comment": "This paper was prepared for the occasion of the 70th birthday of\n  Maciej Lewenstein", "summary": "Lasers provide intense coherent radiation, essential to cool and trap atoms\ninto a Bose-Einstein condensate or can alternatively drive the non-linear\ndynamics of high-order harmonic generation. Yet, these two fundamental\nprocesses remained of independent consideration. Here, we connect matter waves\nat ultracold temperatures with radiation bursts on the ultrafast attosecond\ntimescale. We do this by exploring high harmonic generation from a\nBose-Einstein condensate.", "AI": {"tldr": "\u5229\u7528Bose-Einstein\u51dd\u805a\u4f53\u8fdb\u884c\u9ad8\u6b21\u8c10\u6ce2\u4ea7\u751f\uff0c\u8fde\u63a5\u4e86\u8d85\u51b7\u6e29\u5ea6\u4e0b\u7684\u7269\u8d28\u6ce2\u548c\u8d85\u5feb\u963f\u79d2\u65f6\u95f4\u5c3a\u5ea6\u7684\u8f90\u5c04\u7206\u53d1\u3002", "motivation": "\u672c\u6587\u7684\u52a8\u673a\u662f\u5c06\u8d85\u51b7\u539f\u5b50\u7814\u7a76\u4e2d\u7684Bose-Einstein\u51dd\u805a\u4f53\u4e0e\u9ad8\u6b21\u8c10\u6ce2\u4ea7\u751f\u9886\u57df\u8054\u7cfb\u8d77\u6765\uff0c\u4ee5\u524d\u8fd9\u4e24\u4e2a\u9886\u57df\u662f\u72ec\u7acb\u7814\u7a76\u7684\u3002", "method": "\u901a\u8fc7\u63a2\u7d22Bose-Einstein\u51dd\u805a\u4f53\u4ea7\u751f\u9ad8\u6b21\u8c10\u6ce2\u3002", "result": "\u5b9e\u73b0\u4e86\u8d85\u51b7\u6e29\u5ea6\u4e0b\u7684\u7269\u8d28\u6ce2\u4e0e\u8d85\u5feb\u963f\u79d2\u65f6\u95f4\u5c3a\u5ea6\u7684\u8f90\u5c04\u7206\u53d1\u7684\u8fde\u63a5\u3002", "conclusion": "\u672c\u6587\u6210\u529f\u5730\u5c06Bose-Einstein\u51dd\u805a\u4f53\u4f5c\u4e3a\u9ad8\u6b21\u8c10\u6ce2\u4ea7\u751f\u7684\u6e90\uff0c\u5b9e\u73b0\u4e86\u8d85\u51b7\u7269\u8d28\u6ce2\u4e0e\u8d85\u5feb\u8f90\u5c04\u7684\u8026\u5408\u3002"}}
{"id": "2509.18145", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18145", "abs": "https://arxiv.org/abs/2509.18145", "authors": ["Syed Ahmad Chan Bukhari", "Amritpal Singh", "Shifath Hossain", "Iram Wajahat"], "title": "Early Prediction of Multi-Label Care Escalation Triggers in the Intensive Care Unit Using Electronic Health Records", "comment": "7 pages, 3 Figure", "summary": "Intensive Care Unit (ICU) patients often present with complex, overlapping\nsigns of physiological deterioration that require timely escalation of care.\nTraditional early warning systems, such as SOFA or MEWS, are limited by their\nfocus on single outcomes and fail to capture the multi-dimensional nature of\nclinical decline. This study proposes a multi-label classification framework to\npredict Care Escalation Triggers (CETs), including respiratory failure,\nhemodynamic instability, renal compromise, and neurological deterioration,\nusing the first 24 hours of ICU data. Using the MIMIC-IV database, CETs are\ndefined through rule-based criteria applied to data from hours 24 to 72 (for\nexample, oxygen saturation below 90, mean arterial pressure below 65 mmHg,\ncreatinine increase greater than 0.3 mg/dL, or a drop in Glasgow Coma Scale\nscore greater than 2). Features are extracted from the first 24 hours and\ninclude vital sign aggregates, laboratory values, and static demographics. We\ntrain and evaluate multiple classification models on a cohort of 85,242 ICU\nstays (80 percent training: 68,193; 20 percent testing: 17,049). Evaluation\nmetrics include per-label precision, recall, F1-score, and Hamming loss.\nXGBoost, the best performing model, achieves F1-scores of 0.66 for respiratory,\n0.72 for hemodynamic, 0.76 for renal, and 0.62 for neurologic deterioration,\noutperforming baseline models. Feature analysis shows that clinically relevant\nparameters such as respiratory rate, blood pressure, and creatinine are the\nmost influential predictors, consistent with the clinical definitions of the\nCETs. The proposed framework demonstrates practical potential for early,\ninterpretable clinical alerts without requiring complex time-series modeling or\nnatural language processing.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.18849", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18849", "abs": "https://arxiv.org/abs/2509.18849", "authors": ["Wenke Huang", "Quan Zhang", "Yiyang Fang", "Jian Liang", "Xuankun Rong", "Huanjin Yao", "Guancheng Wan", "Ke Liang", "Wenwen He", "Mingjun Li", "Leszek Rutkowski", "Mang Ye", "Bo Du", "Dacheng Tao"], "title": "MAPO: Mixed Advantage Policy Optimization", "comment": null, "summary": "Recent advances in reinforcement learning for foundation models, such as\nGroup Relative Policy Optimization (GRPO), have significantly improved the\nperformance of foundation models on reasoning tasks. Notably, the advantage\nfunction serves as a central mechanism in GRPO for ranking the trajectory\nimportance. However, existing explorations encounter both advantage reversion\nand advantage mirror problems, which hinder the reasonable advantage allocation\nacross different query samples. In this work, we propose an easy but effective\nGRPO strategy, Mixed Advantage Policy Optimization (MAPO). We reveal that the\ntrajectory appears with different certainty and propose the advantage percent\ndeviation for samples with high-certainty trajectories. Furthermore, we\ndynamically reweight the advantage function for samples with varying trajectory\ncertainty, thereby adaptively configuring the advantage function to account for\nsample-specific characteristics. Comparison with related state-of-the-art\nmethods, along with ablation studies on different advantage variants, validates\nthe effectiveness of our approach.", "AI": {"tldr": "GRPO\u5728\u57fa\u7840\u6a21\u578b\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u4f18\u52bf\u51fd\u6570\u5b58\u5728\u4f18\u52bf\u53cd\u8f6c\u548c\u4f18\u52bf\u955c\u50cf\u95ee\u9898\u3002\u672c\u6587\u63d0\u51fa\u7684MAPO\u7b56\u7565\u901a\u8fc7\u5f15\u5165\u4f18\u52bf\u767e\u5206\u6bd4\u504f\u5dee\u548c\u52a8\u6001\u91cd\u52a0\u6743\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u5e76\u5df2\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "motivation": "GRPO\u5728\u57fa\u7840\u6a21\u578b\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u4f18\u52bf\u51fd\u6570\u5728\u5904\u7406\u4e0d\u540c\u67e5\u8be2\u6837\u672c\u65f6\u5b58\u5728\u4f18\u52bf\u53cd\u8f6c\u548c\u4f18\u52bf\u955c\u50cf\u95ee\u9898\uff0c\u963b\u788d\u4e86\u5408\u7406\u7684\u4f18\u52bf\u5206\u914d\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u7b56\u7565\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMAPO\uff08Mixed Advantage Policy Optimization\uff09\u7684\u7b56\u7565\uff0c\u901a\u8fc7\u5f15\u5165\u4f18\u52bf\u767e\u5206\u6bd4\u504f\u5dee\u6765\u5904\u7406\u9ad8\u786e\u5b9a\u6027\u8f68\u8ff9\u6837\u672c\uff0c\u5e76\u52a8\u6001\u91cd\u52a0\u6743\u4e0d\u540c\u8f68\u8ff9\u786e\u5b9a\u6027\u6837\u672c\u7684\u4f18\u52bf\u51fd\u6570\uff0c\u4ece\u800c\u81ea\u9002\u5e94\u5730\u8003\u8651\u6837\u672c\u7279\u6027\u3002", "result": "MAPO\u7b56\u7565\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u4f18\u52bf\u51fd\u6570\uff0c\u6709\u6548\u89e3\u51b3\u4e86GRPO\u7684\u4f18\u52bf\u53cd\u8f6c\u548c\u4f18\u52bf\u955c\u50cf\u95ee\u9898\uff0c\u5e76\u5df2\u901a\u8fc7\u4e0e\u73b0\u6709\u5148\u8fdb\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\u548c\u6d88\u878d\u7814\u7a76\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "conclusion": "MAPO\u662f\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684GRPO\u7b56\u7565\uff0c\u901a\u8fc7\u89e3\u51b3\u4f18\u52bf\u51fd\u6570\u4e2d\u7684\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u57fa\u7840\u6a21\u578b\u5728\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002"}}
{"id": "2509.18776", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18776", "abs": "https://arxiv.org/abs/2509.18776", "authors": ["Chen Liang", "Zhaoqi Huang", "Haofen Wang", "Fu Chai", "Chunying Yu", "Huanhuan Wei", "Zhengjie Liu", "Yanpeng Li", "Hongjun Wang", "Ruifeng Luo", "Xianzhong Zhao"], "title": "AECBench: A Hierarchical Benchmark for Knowledge Evaluation of Large Language Models in the AEC Field", "comment": null, "summary": "Large language models (LLMs), as a novel information technology, are seeing\nincreasing adoption in the Architecture, Engineering, and Construction (AEC)\nfield. They have shown their potential to streamline processes throughout the\nbuilding lifecycle. However, the robustness and reliability of LLMs in such a\nspecialized and safety-critical domain remain to be evaluated. To address this\nchallenge, this paper establishes AECBench, a comprehensive benchmark designed\nto quantify the strengths and limitations of current LLMs in the AEC domain.\nThe benchmark defines 23 representative tasks within a five-level\ncognition-oriented evaluation framework encompassing Knowledge Memorization,\nUnderstanding, Reasoning, Calculation, and Application. These tasks were\nderived from authentic AEC practice, with scope ranging from codes retrieval to\nspecialized documents generation. Subsequently, a 4,800-question dataset\nencompassing diverse formats, including open-ended questions, was crafted\nprimarily by engineers and validated through a two-round expert review.\nFurthermore, an LLM-as-a-Judge approach was introduced to provide a scalable\nand consistent methodology for evaluating complex, long-form responses\nleveraging expert-derived rubrics. Through the evaluation of nine LLMs, a clear\nperformance decline across five cognitive levels was revealed. Despite\ndemonstrating proficiency in foundational tasks at the Knowledge Memorization\nand Understanding levels, the models showed significant performance deficits,\nparticularly in interpreting knowledge from tables in building codes, executing\ncomplex reasoning and calculation, and generating domain-specific documents.\nConsequently, this study lays the groundwork for future research and\ndevelopment aimed at the robust and reliable integration of LLMs into\nsafety-critical engineering practices.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86AECBench\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLMs)\u5728\u5efa\u7b51\u3001\u5de5\u7a0b\u548c\u65bd\u5de5(AEC)\u9886\u57df\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "motivation": "\u8bc4\u4f30LLMs\u5728AEC\u9886\u57df\u7684\u9c81\u68d2\u6027\u548c\u53ef\u9760\u6027\uff0c\u56e0\u4e3a\u8be5\u9886\u57df\u4e13\u4e1a\u6027\u5f3a\u4e14\u6d89\u53ca\u5b89\u5168\u5173\u952e\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5305\u542b23\u4e2a\u4efb\u52a1\u7684\u4e94\u7ea7\u8ba4\u77e5\u8bc4\u4f30\u6846\u67b6\uff08\u77e5\u8bc6\u8bb0\u5fc6\u3001\u7406\u89e3\u3001\u63a8\u7406\u3001\u8ba1\u7b97\u3001\u5e94\u7528\uff09\uff0c\u521b\u5efa\u4e86\u5305\u542b4800\u4e2a\u95ee\u9898\u7684\u591a\u6837\u5316\u6570\u636e\u96c6\uff0c\u5e76\u91c7\u7528LLM-as-a-Judge\u65b9\u6cd5\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u8bc4\u4f30\u4e86\u4e5d\u4e2aLLMs\uff0c\u53d1\u73b0\u5b83\u4eec\u5728\u77e5\u8bc6\u8bb0\u5fc6\u548c\u7406\u89e3\u5c42\u9762\u8868\u73b0\u5c1a\u53ef\uff0c\u4f46\u5728\u4ece\u8868\u683c\u4e2d\u89e3\u8bfb\u5efa\u7b51\u89c4\u8303\u77e5\u8bc6\u3001\u6267\u884c\u590d\u6742\u63a8\u7406\u548c\u8ba1\u7b97\u4ee5\u53ca\u751f\u6210\u9886\u57df\u7279\u5b9a\u6587\u6863\u65b9\u9762\u5b58\u5728\u663e\u8457\u6027\u80fd\u7f3a\u9677\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u672a\u6765\u5c06LLMs\u53ef\u9760\u5730\u96c6\u6210\u5230\u5b89\u5168\u5173\u952e\u7684\u5de5\u7a0b\u5b9e\u8df5\u4e2d\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.18865", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18865", "abs": "https://arxiv.org/abs/2509.18865", "authors": ["Masato Kobayashi", "Thanpimon Buamanee"], "title": "Bi-VLA: Bilateral Control-Based Imitation Learning via Vision-Language Fusion for Action Generation", "comment": null, "summary": "We propose Bilateral Control-Based Imitation Learning via Vision-Language\nFusion for Action Generation (Bi-VLA), a novel framework that extends bilateral\ncontrol-based imitation learning to handle more than one task within a single\nmodel. Conventional bilateral control methods exploit joint angle, velocity,\ntorque, and vision for precise manipulation but require task-specific models,\nlimiting their generality. Bi-VLA overcomes this limitation by utilizing robot\njoint angle, velocity, and torque data from leader-follower bilateral control\nwith visual features and natural language instructions through SigLIP and\nFiLM-based fusion. We validated Bi-VLA on two task types: one requiring\nsupplementary language cues and another distinguishable solely by vision.\nReal-robot experiments showed that Bi-VLA successfully interprets\nvision-language combinations and improves task success rates compared to\nconventional bilateral control-based imitation learning. Our Bi-VLA addresses\nthe single-task limitation of prior bilateral approaches and provides empirical\nevidence that combining vision and language significantly enhances versatility.\nExperimental results validate the effectiveness of Bi-VLA in real-world tasks.\nFor additional material, please visit the website:\nhttps://mertcookimg.github.io/bi-vla/", "AI": {"tldr": "Bi-VLA\u662f\u4e00\u4e2a\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u548c\u8bed\u8a00\u6307\u4ee4\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u53cc\u8fb9\u63a7\u5236\u65b9\u6cd5\u6a21\u578b\u5355\u4e00\u4efb\u52a1\u7684\u5c40\u9650\u6027\uff0c\u63d0\u9ad8\u4e86\u4efb\u52a1\u6210\u529f\u7387\u548c\u901a\u7528\u6027\u3002", "motivation": "\u4f20\u7edf\u53cc\u8fb9\u63a7\u5236\u65b9\u6cd5\u9700\u8981\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\u8bbe\u8ba1\u6a21\u578b\uff0c\u9650\u5236\u4e86\u5176\u901a\u7528\u6027\u3002\u672c\u7814\u7a76\u65e8\u5728\u6269\u5c55\u53cc\u8fb9\u63a7\u5236\u65b9\u6cd5\uff0c\u4f7f\u5176\u80fd\u591f\u5728\u4e00\u4e2a\u6a21\u578b\u4e2d\u5904\u7406\u591a\u4e2a\u4efb\u52a1\u3002", "method": "Bi-VLA\u5229\u7528\u6765\u81ea\u9886\u5bfc-\u8ddf\u968f\u53cc\u8fb9\u63a7\u5236\u7684\u673a\u5668\u4eba\u5173\u8282\u89d2\u5ea6\u3001\u901f\u5ea6\u548c\u626d\u77e9\u6570\u636e\uff0c\u7ed3\u5408SigLIP\u548cFiLM\u8fdb\u884c\u89c6\u89c9\u7279\u5f81\u4e0e\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u7684\u878d\u5408\u3002", "result": "\u5728\u4e24\u79cd\u4efb\u52a1\u7c7b\u578b\uff08\u4e00\u79cd\u9700\u8981\u8bed\u8a00\u63d0\u793a\uff0c\u53e6\u4e00\u79cd\u4ec5\u51ed\u89c6\u89c9\u533a\u5206\uff09\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cBi-VLA\u80fd\u591f\u6210\u529f\u89e3\u91ca\u89c6\u89c9-\u8bed\u8a00\u7ec4\u5408\uff0c\u5e76\u63d0\u9ad8\u4e86\u4efb\u52a1\u6210\u529f\u7387\u3002", "conclusion": "Bi-VLA\u514b\u670d\u4e86\u5148\u524d\u53cc\u8fb9\u65b9\u6cd5\u5355\u4e00\u4efb\u52a1\u7684\u9650\u5236\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u89c6\u89c9\u548c\u8bed\u8a00\u7684\u7ed3\u5408\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u901a\u7528\u6027\u3002"}}
{"id": "2509.18502", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18502", "abs": "https://arxiv.org/abs/2509.18502", "authors": ["Wenjie Liu", "Hongmin Liu", "Lixin Zhang", "Bin Fan"], "title": "Source-Free Domain Adaptive Semantic Segmentation of Remote Sensing Images with Diffusion-Guided Label Enrichment", "comment": null, "summary": "Research on unsupervised domain adaptation (UDA) for semantic segmentation of\nremote sensing images has been extensively conducted. However, research on how\nto achieve domain adaptation in practical scenarios where source domain data is\ninaccessible namely, source-free domain adaptation (SFDA) remains limited.\nSelf-training has been widely used in SFDA, which requires obtaining as many\nhigh-quality pseudo-labels as possible to train models on target domain data.\nMost existing methods optimize the entire pseudo-label set to obtain more\nsupervisory information. However, as pseudo-label sets often contain\nsubstantial noise, simultaneously optimizing all labels is challenging. This\nlimitation undermines the effectiveness of optimization approaches and thus\nrestricts the performance of self-training. To address this, we propose a novel\npseudo-label optimization framework called Diffusion-Guided Label Enrichment\n(DGLE), which starts from a few easily obtained high-quality pseudo-labels and\npropagates them to a complete set of pseudo-labels while ensuring the quality\nof newly generated labels. Firstly, a pseudo-label fusion method based on\nconfidence filtering and super-resolution enhancement is proposed, which\nutilizes cross-validation of details and contextual information to obtain a\nsmall number of high-quality pseudo-labels as initial seeds. Then, we leverage\nthe diffusion model to propagate incomplete seed pseudo-labels with irregular\ndistributions due to its strong denoising capability for randomly distributed\nnoise and powerful modeling capacity for complex distributions, thereby\ngenerating complete and high-quality pseudo-labels. This method effectively\navoids the difficulty of directly optimizing the complete set of pseudo-labels,\nsignificantly improves the quality of pseudo-labels, and thus enhances the\nmodel's performance in the target domain.", "AI": {"tldr": "\u7814\u7a76\u4e86\u6e90\u57df\u4e0d\u53ef\u8bbf\u95ee\u7684\u56fe\u50cf\u8bed\u4e49\u5206\u5272\u4e2d\u7684\u65e0\u76d1\u7763\u57df\u9002\u5e94\uff08SFDA\uff09\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a DGLE \u7684\u4f2a\u6807\u7b7e\u4f18\u5316\u6846\u67b6\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u4ece\u5c11\u91cf\u9ad8\u8d28\u91cf\u4f2a\u6807\u7b7e\u51fa\u53d1\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u5b8c\u6574\u4f2a\u6807\u7b7e\u96c6\uff0c\u4ee5\u63d0\u5347\u6a21\u578b\u5728\u76ee\u6807\u57df\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6e90\u57df\u65e0\u76d1\u7763\u57df\u9002\u5e94\uff08SFDA\uff09\u7814\u7a76\u6709\u9650\uff0c\u7279\u522b\u662f\u5229\u7528\u81ea\u8bad\u7ec3\u65b9\u6cd5\u65f6\uff0c\u5982\u4f55\u5904\u7406\u5305\u542b\u5927\u91cf\u566a\u58f0\u7684\u4f2a\u6807\u7b7e\u96c6\u4ee5\u83b7\u5f97\u9ad8\u8d28\u91cf\u4f2a\u6807\u7b7e\u662f\u5173\u952e\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a Diffusion-Guided Label Enrichment (DGLE) \u7684\u4f2a\u6807\u7b7e\u4f18\u5316\u6846\u67b6\u3002\u9996\u5148\uff0c\u901a\u8fc7\u7f6e\u4fe1\u5ea6\u8fc7\u6ee4\u548c\u8d85\u5206\u8fa8\u7387\u589e\u5f3a\u5b9e\u73b0\u4f2a\u6807\u7b7e\u878d\u5408\uff0c\u83b7\u5f97\u5c11\u91cf\u9ad8\u8d28\u91cf\u7684\u521d\u59cb\u4f2a\u6807\u7b7e\u3002\u7136\u540e\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u7684\u53bb\u566a\u548c\u5206\u5e03\u5efa\u6a21\u80fd\u529b\uff0c\u5c06\u8fd9\u4e9b\u521d\u59cb\u4f2a\u6807\u7b7e\u4f20\u64ad\u4e3a\u5b8c\u6574\u9ad8\u8d28\u91cf\u7684\u4f2a\u6807\u7b7e\u96c6\u3002", "result": "DGLE \u6846\u67b6\u6709\u6548\u907f\u514d\u4e86\u76f4\u63a5\u4f18\u5316\u6574\u4e2a\u4f2a\u6807\u7b7e\u96c6\u7684\u56f0\u96be\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4f2a\u6807\u7b7e\u7684\u8d28\u91cf\uff0c\u4ece\u800c\u589e\u5f3a\u4e86\u6a21\u578b\u5728\u76ee\u6807\u57df\u7684\u6027\u80fd\u3002", "conclusion": "DGLE \u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u7f6e\u4fe1\u5ea6\u8fc7\u6ee4\u3001\u8d85\u5206\u8fa8\u7387\u589e\u5f3a\u548c\u6269\u6563\u6a21\u578b\uff0c\u6709\u6548\u5730\u89e3\u51b3\u4e86 SFDA \u4e2d\u7684\u4f2a\u6807\u7b7e\u8d28\u91cf\u95ee\u9898\uff0c\u4e3a\u81ea\u8bad\u7ec3\u65b9\u6cd5\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u4f2a\u6807\u7b7e\u751f\u6210\u9014\u5f84\uff0c\u5e76\u6700\u7ec8\u63d0\u5347\u4e86\u6a21\u578b\u5728\u76ee\u6807\u57df\u7684\u5206\u5272\u6027\u80fd\u3002"}}
{"id": "2509.19035", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2509.19035", "abs": "https://arxiv.org/abs/2509.19035", "authors": ["A. Yu. Chernyavskiy", "D. A. Kulikov", "B. I. Bantysh", "Yu. I. Bogdanov", "A. K. Fedorov", "E. O. Kiktenko"], "title": "Improving QAOA to find approximate QUBO solutions in O(1) shots", "comment": "9 pages, 6 figures", "summary": "Quantum Approximate Optimization Algorithm (QAOA) provides one of the most\npromising quantum frameworks for addressing discrete optimization problems with\nbroad real-world applications, particularly quadratic unconstrained binary\noptimization (QUBO) problems. However, the widely used hybrid\nquantum--classical implementation faces significant challenges due to\nstatistical noise and barren plateaus. A prominent approach to mitigate these\nissues is fixed-point QAOA (fpQAOA), where circuit parameters are trained on\nsmall problem instances and subsequently transferred to larger ones. In this\nwork, we develop a modified fpQAOA scheme that combines (i) considering the\nprobability of achieving a target approximation ratio (AR) rather than\nrequiring the exact optimum, (ii) setting the number of layers equal to the\nproblem size with the sine--cosine encoding of QAOA angles, and (iii) rescaling\nthe problem coefficient matrices to unit Frobenius norm. We demonstrate that\nthis combination leads to a decreasing median number of shots required to\nobtain approximate solutions as the problem size increases, with ARs being\nwithin a few percent from the optimum for the considered problem classes.\nExtrapolation of these results suggests an $O(1)$ shot complexity while\nretaining at most quadratic circuit depth, underscoring the potential of our\napproach to overcome key bottlenecks in QAOA implementations and scalability\nestimations. Remarkably, omitting even a single one of the modifications\n(i)--(iii) results in exponential growth of the number of shots required as the\nproblem size increases.", "AI": {"tldr": "QAOA\u5728\u89e3\u51b3\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u65b9\u9762\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u9762\u4e34\u566a\u58f0\u548c\u9000\u706b\u9000\u5316\u7b49\u6311\u6218\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u56fa\u5b9a\u70b9QAOA\uff08fpQAOA\uff09\u65b9\u6848\uff0c\u7ed3\u5408\u4e86\u6982\u7387\u76ee\u6807\u3001\u6b63\u5f26-\u4f59\u5f26\u7f16\u7801\u548c\u7cfb\u6570\u77e9\u9635\u91cd\u7f29\u653e\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u83b7\u5f97\u8fd1\u4f3c\u89e3\u6240\u9700\u7684\u91cf\u5b50\u6bd4\u7279\u6570\u91cf\uff0c\u5e76\u6709\u671b\u5b9e\u73b0O(1)\u91cf\u5b50\u6bd4\u7279\u590d\u6742\u5ea6\uff0c\u800c\u7535\u8def\u6df1\u5ea6\u4fdd\u6301\u4e8c\u6b21\u65b9\u589e\u957f\u3002\u5355\u72ec\u79fb\u9664\u8fd9\u4e9b\u6539\u8fdb\u4e2d\u7684\u4efb\u4f55\u4e00\u4e2a\u90fd\u4f1a\u5bfc\u81f4\u6240\u9700\u91cf\u5b50\u6bd4\u7279\u6570\u91cf\u968f\u7740\u95ee\u9898\u89c4\u6a21\u5448\u6307\u6570\u589e\u957f\u3002", "motivation": "QAOA\u4f5c\u4e3a\u89e3\u51b3\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u7684\u5f3a\u5927\u91cf\u5b50\u6846\u67b6\uff0c\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5c24\u5176\u662f\u5728\u4e8c\u6b21\u65e0\u7ea6\u675f\u4e8c\u5143\u4f18\u5316\uff08QUBO\uff09\u95ee\u9898\u4e0a\uff0c\u663e\u793a\u51fa\u5de8\u5927\u6f5c\u529b\u3002\u7136\u800c\uff0c\u5176\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u5b9e\u73b0\u9762\u4e34\u7edf\u8ba1\u566a\u58f0\u548c\u9000\u706b\u9000\u5316\u7b49\u4e25\u5cfb\u6311\u6218\u3002\u56fa\u5b9a\u70b9QAOA\uff08fpQAOA\uff09\u662f\u4e00\u79cd\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u5728\u5c0f\u95ee\u9898\u5b9e\u4f8b\u4e0a\u8bad\u7ec3\u53c2\u6570\u5e76\u5728\u5c06\u5176\u8f6c\u79fb\u5230\u5927\u95ee\u9898\u5b9e\u4f8b\u6765\u7f13\u89e3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684fpQAOA\u65b9\u6848\uff0c\u5176\u7279\u70b9\u5305\u62ec\uff1a(i) \u5173\u6ce8\u83b7\u5f97\u76ee\u6807\u8fd1\u4f3c\u6bd4\uff08AR\uff09\u7684\u6982\u7387\uff0c\u800c\u975e\u7cbe\u786e\u7684\u6700\u4f18\u89e3\uff1b(ii) \u5c06\u5c42\u6570\u8bbe\u7f6e\u4e3a\u95ee\u9898\u5927\u5c0f\uff0c\u5e76\u91c7\u7528QAOA\u89d2\u5ea6\u7684\u6b63\u5f26-\u4f59\u5f26\u7f16\u7801\uff1b(iii) \u5c06\u95ee\u9898\u7cfb\u6570\u77e9\u9635\u91cd\u7f29\u653e\u5230\u5355\u4f4d\u5f17\u7f57\u8d1d\u5c3c\u5384\u65af\u8303\u6570\u3002", "result": "\u8be5\u7ec4\u5408\u65b9\u6cd5\u4f7f\u5f97\u83b7\u5f97\u8fd1\u4f3c\u89e3\u6240\u9700\u7684\u4ecb\u6570\u91cf\uff08median number\uff09\u968f\u7740\u95ee\u9898\u89c4\u6a21\u7684\u589e\u52a0\u800c\u51cf\u5c11\uff0c\u5e76\u4e14\u5bf9\u4e8e\u6240\u8003\u8651\u7684\u95ee\u9898\u7c7b\u522b\uff0cAR\u503c\u4e0e\u6700\u4f18\u503c\u76f8\u5dee\u4ec5\u5728\u767e\u5206\u4e4b\u51e0\u4e4b\u5185\u3002\u7ed3\u679c\u5916\u63a8\u8868\u660e\uff0c\u5728\u4fdd\u6301\u4e8c\u6b21\u65b9\u7535\u8def\u6df1\u5ea6\u7684\u540c\u65f6\uff0c\u5177\u6709O(1)\u91cf\u5b50\u6bd4\u7279\u590d\u6742\u5ea6\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u6539\u8fdbfpQAOA\u65b9\u6848\u901a\u8fc7\u7ed3\u5408\u6982\u7387\u76ee\u6807\u3001\u6b63\u5f26-\u4f59\u5f26\u7f16\u7801\u548c\u7cfb\u6570\u77e9\u9635\u91cd\u7f29\u653e\uff0c\u6709\u6548\u89e3\u51b3\u4e86QAOA\u5b9e\u73b0\u548c\u53ef\u6269\u5c55\u6027\u4f30\u8ba1\u4e2d\u7684\u5173\u952e\u74f6\u9888\u3002\u8be5\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u83b7\u5f97\u8fd1\u4f3c\u89e3\u6240\u9700\u7684\u91cf\u5b50\u6bd4\u7279\u6570\u91cf\uff0c\u5e76\u6709\u671b\u5b9e\u73b0O(1)\u91cf\u5b50\u6bd4\u7279\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u4e8c\u6b21\u65b9\u7535\u8def\u6df1\u5ea6\u3002\u91cd\u8981\u7684\u662f\uff0c\u7701\u7565\u8be5\u65b9\u6848\u4e2d\u7684\u4efb\u4f55\u4e00\u4e2a\u6539\u8fdb\u90fd\u4f1a\u5bfc\u81f4\u6240\u9700\u91cf\u5b50\u6bd4\u7279\u6570\u91cf\u968f\u7740\u95ee\u9898\u89c4\u6a21\u7684\u589e\u52a0\u5448\u6307\u6570\u589e\u957f\u3002"}}
{"id": "2509.18147", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18147", "abs": "https://arxiv.org/abs/2509.18147", "authors": ["Xinyu Mu", "Hui Dou", "Furao Shen", "Jian Zhao"], "title": "ConceptFlow: Hierarchical and Fine-grained Concept-Based Explanation for Convolutional Neural Networks", "comment": null, "summary": "Concept-based interpretability for Convolutional Neural Networks (CNNs) aims\nto align internal model representations with high-level semantic concepts, but\nexisting approaches largely overlook the semantic roles of individual filters\nand the dynamic propagation of concepts across layers. To address these\nlimitations, we propose ConceptFlow, a concept-based interpretability framework\nthat simulates the internal \"thinking path\" of a model by tracing how concepts\nemerge and evolve across layers. ConceptFlow comprises two key components: (i)\nconcept attentions, which associate each filter with relevant high-level\nconcepts to enable localized semantic interpretation, and (ii) conceptual\npathways, derived from a concept transition matrix that quantifies how concepts\npropagate and transform between filters. Together, these components offer a\nunified and structured view of internal model reasoning. Experimental results\ndemonstrate that ConceptFlow yields semantically meaningful insights into model\nreasoning, validating the effectiveness of concept attentions and conceptual\npathways in explaining decision behavior. By modeling hierarchical conceptual\npathways, ConceptFlow provides deeper insight into the internal logic of CNNs\nand supports the generation of more faithful and human-aligned explanations.", "AI": {"tldr": "ConceptFlow \u901a\u8fc7\u5173\u6ce8\u6bcf\u4e2a\u6ee4\u6ce2\u5668\u4e0e\u9ad8\u5c42\u6982\u5ff5\u7684\u5173\u8054\u4ee5\u53ca\u6982\u5ff5\u5728\u5c42\u95f4\u7684\u4f20\u64ad\u8def\u5f84\uff0c\u6765\u6a21\u62df\u548c\u89e3\u91ca\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u7684\u5185\u90e8\u63a8\u7406\u8fc7\u7a0b\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6982\u5ff5\u7684\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u5ffd\u89c6\u4e86\u5355\u4e2a\u6ee4\u6ce2\u5668\u548c\u6982\u5ff5\u8de8\u5c42\u52a8\u6001\u4f20\u64ad\u7684\u8bed\u4e49\u89d2\u8272\uff0cConceptFlow \u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u5c40\u9650\u6027\u3002", "method": "ConceptFlow \u6846\u67b6\u5305\u542b\u4e24\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a(i) \u6982\u5ff5\u6ce8\u610f\u529b\uff08Concept attentions\uff09\uff0c\u5c06\u6bcf\u4e2a\u6ee4\u6ce2\u5668\u4e0e\u76f8\u5173\u7684\u9ad8\u5c42\u6982\u5ff5\u5173\u8054\u8d77\u6765\uff0c\u5b9e\u73b0\u5c40\u90e8\u8bed\u4e49\u89e3\u91ca\uff1b(ii) \u6982\u5ff5\u901a\u8def\uff08Conceptual pathways\uff09\uff0c\u6e90\u81ea\u91cf\u5316\u6982\u5ff5\u5982\u4f55\u5728\u6ee4\u6ce2\u5668\u4e4b\u95f4\u4f20\u64ad\u548c\u8f6c\u5316\u7684\u6982\u5ff5\u8f6c\u79fb\u77e9\u9635\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cConceptFlow \u63d0\u4f9b\u4e86\u6709\u610f\u4e49\u7684\u8bed\u4e49\u6d1e\u5bdf\uff0c\u9a8c\u8bc1\u4e86\u6982\u5ff5\u6ce8\u610f\u529b\u548c\u6982\u5ff5\u901a\u8def\u5728\u89e3\u91ca\u51b3\u7b56\u884c\u4e3a\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "ConceptFlow \u901a\u8fc7\u5bf9\u5206\u5c42\u6982\u5ff5\u901a\u8def\u8fdb\u884c\u5efa\u6a21\uff0c\u6df1\u5165\u4e86\u89e3 CNN \u7684\u5185\u90e8\u903b\u8f91\uff0c\u5e76\u652f\u6301\u751f\u6210\u66f4\u5fe0\u5b9e\u3001\u66f4\u7b26\u5408\u4eba\u7c7b\u7684\u89e3\u91ca\u3002"}}
{"id": "2509.18864", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18864", "abs": "https://arxiv.org/abs/2509.18864", "authors": ["Yingxin Li", "Jianbo Zhao", "Xueyu Ren", "Jie Tang", "Wangjie You", "Xu Chen", "Kan Zhou", "Chao Feng", "Jiao Ran", "Yuan Meng", "Zhi Wang"], "title": "Conf-Profile: A Confidence-Driven Reasoning Paradigm for Label-Free User Profiling", "comment": null, "summary": "User profiling, as a core technique for user understanding, aims to infer\nstructural attributes from user information. Large Language Models (LLMs)\nprovide a promising avenue for user profiling, yet the progress is hindered by\nthe lack of comprehensive benchmarks. To bridge this gap, we propose\nProfileBench, an industrial benchmark derived from a real-world video platform,\nencompassing heterogeneous user data and a well-structured profiling taxonomy.\nHowever, the profiling task remains challenging due to the difficulty of\ncollecting large-scale ground-truth labels, and the heterogeneous and noisy\nuser information can compromise the reliability of LLMs. To approach label-free\nand reliable user profiling, we propose a Confidence-driven Profile reasoning\nframework Conf-Profile, featuring a two-stage paradigm. We first synthesize\nhigh-quality labels by leveraging advanced LLMs with confidence hints, followed\nby confidence-weighted voting for accuracy improvement and confidence\ncalibration for a balanced distribution. The multiple profile results,\nrationales, and confidence scores are aggregated and distilled into a\nlightweight LLM. We further enhance the reasoning ability via confidence-guided\nunsupervised reinforcement learning, which exploits confidence for difficulty\nfiltering, quasi-ground truth voting, and reward weighting. Experimental\nresults demonstrate that Conf-Profile delivers substantial performance through\nthe two-stage training, improving F1 by 13.97 on Qwen3-8B.", "AI": {"tldr": "\u4e3a\u4e86\u89e3\u51b3\u7528\u6237\u753b\u50cf\u6280\u672f\u4e2d\u7f3a\u4e4f\u7efc\u5408\u57fa\u51c6\u548c\u6807\u7b7e\u7a00\u758f\u7684\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86ProfileBench\u57fa\u51c6\u548cConf-Profile\u6846\u67b6\u3002Conf-Profile\u91c7\u7528\u4e24\u9636\u6bb5\u8303\u5f0f\uff0c\u9996\u5148\u5229\u7528\u5e26\u6709\u7f6e\u4fe1\u5ea6\u63d0\u793a\u7684LLM\u5408\u6210\u9ad8\u8d28\u91cf\u6807\u7b7e\uff0c\u7136\u540e\u901a\u8fc7\u7f6e\u4fe1\u5ea6\u52a0\u6743\u6295\u7968\u548c\u6821\u51c6\u6765\u63d0\u9ad8\u51c6\u786e\u6027\u548c\u5e73\u8861\u5206\u5e03\u3002\u8be5\u6846\u67b6\u8fd8\u80fd\u901a\u8fc7\u7f6e\u4fe1\u5ea6\u5f15\u5bfc\u7684\u65e0\u76d1\u7763\u5f3a\u5316\u5b66\u4e60\u8fdb\u4e00\u6b65\u589e\u5f3a\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u5728Qwen3-8B\u6a21\u578b\u4e0a\u53d6\u5f97\u4e8613.97%\u7684F1\u63d0\u5347\u3002", "motivation": "\u5f53\u524d\u7528\u6237\u753b\u50cf\u6280\u672f\u7f3a\u4e4f\u7efc\u5408\u57fa\u51c6\uff0c\u4e14\u5b58\u5728\u6807\u7b7e\u7a00\u758f\u3001\u7528\u6237\u6570\u636e\u5f02\u6784\u4e14\u566a\u58f0\u5927\u7b49\u95ee\u9898\uff0c\u5f71\u54cd\u4e86LLM\u5728\u7528\u6237\u753b\u50cf\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "method": "\u63d0\u51faProfileBench\u57fa\u51c6\uff0c\u5e76\u8bbe\u8ba1\u4e86Conf-Profile\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5305\u542b\u4e24\u9636\u6bb5\u8303\u5f0f\uff1a1. \u5229\u7528LLM\u548c\u7f6e\u4fe1\u5ea6\u63d0\u793a\u5408\u6210\u9ad8\u8d28\u91cf\u6807\u7b7e\uff0c\u5e76\u8fdb\u884c\u7f6e\u4fe1\u5ea6\u52a0\u6743\u6295\u7968\u548c\u6821\u51c6\u30022. \u901a\u8fc7\u7f6e\u4fe1\u5ea6\u5f15\u5bfc\u7684\u65e0\u76d1\u7763\u5f3a\u5316\u5b66\u4e60\u589e\u5f3a\u63a8\u7406\u80fd\u529b\uff0c\u5305\u62ec\u5229\u7528\u7f6e\u4fe1\u5ea6\u8fdb\u884c\u96be\u5ea6\u8fc7\u6ee4\u3001\u51c6\u6807\u7b7e\u6295\u7968\u548c\u5956\u52b1\u52a0\u6743\u3002", "result": "Conf-Profile\u6846\u67b6\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\uff0c\u5728Qwen3-8B\u6a21\u578b\u4e0a\u5c06F1\u5206\u6570\u63d0\u9ad8\u4e8613.97%\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "Conf-Profile\u6846\u67b6\u80fd\u591f\u6709\u6548\u89e3\u51b3\u7528\u6237\u753b\u50cf\u4e2d\u7684\u6807\u7b7e\u7a00\u758f\u548c\u6570\u636e\u5f02\u6784\u566a\u58f0\u95ee\u9898\uff0c\u901a\u8fc7\u7f6e\u4fe1\u5ea6\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\u663e\u8457\u63d0\u5347\u4e86\u7528\u6237\u753b\u50cf\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2509.18792", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18792", "abs": "https://arxiv.org/abs/2509.18792", "authors": ["Sabri Boughorbel", "Fahim Dalvi", "Nadir Durrani", "Majd Hawasly"], "title": "Beyond the Leaderboard: Understanding Performance Disparities in Large Language Models via Model Diffing", "comment": "12 pages, accepted to the 2025 Conference on Empirical Methods in\n  Natural Language Processing (EMNLP 2025)", "summary": "As fine-tuning becomes the dominant paradigm for improving large language\nmodels (LLMs), understanding what changes during this process is increasingly\nimportant. Traditional benchmarking often fails to explain why one model\noutperforms another. In this work, we use model diffing, a mechanistic\ninterpretability approach, to analyze the specific capability differences\nbetween Gemma-2-9b-it and a SimPO-enhanced variant. Using crosscoders, we\nidentify and categorize latent representations that differentiate the two\nmodels. We find that SimPO acquired latent concepts predominantly enhance\nsafety mechanisms (+32.8%), multilingual capabilities (+43.8%), and\ninstruction-following (+151.7%), while its additional training also reduces\nemphasis on model self-reference (-44.1%) and hallucination management\n(-68.5%). Our analysis shows that model diffing can yield fine-grained insights\nbeyond leaderboard metrics, attributing performance gaps to concrete\nmechanistic capabilities. This approach offers a transparent and targeted\nframework for comparing LLMs.", "AI": {"tldr": "\u672c\u7814\u7a76\u4f7f\u7528\u6a21\u578b\u5dee\u5f02\u5316\u65b9\u6cd5\u5206\u6790\u4e86 Gemma-2-9b-it \u548c SimPO \u589e\u5f3a\u53d8\u4f53\u4e4b\u95f4\u7684\u80fd\u529b\u5dee\u5f02\uff0c\u53d1\u73b0\u4e86 SimPO \u4e3b\u8981\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u5b89\u5168\u6027\u3001\u591a\u8bed\u8a00\u80fd\u529b\u548c\u6307\u4ee4\u9075\u5faa\u80fd\u529b\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u6a21\u578b\u81ea\u6211\u6307\u6d89\u548c\u5e7b\u89c9\u7ba1\u7406\u65b9\u9762\u7684\u503e\u5411\u3002", "motivation": "\u968f\u7740\u5fae\u8c03\u6210\u4e3a\u4f18\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u7684\u4e3b\u6d41\u8303\u5f0f\uff0c\u7406\u89e3\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u7684\u5177\u4f53\u53d8\u5316\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u7684\u57fa\u51c6\u6d4b\u8bd5\u5f80\u5f80\u65e0\u6cd5\u89e3\u91ca\u6a21\u578b\u4e4b\u95f4\u6027\u80fd\u5dee\u5f02\u7684\u539f\u56e0\u3002", "method": "\u91c7\u7528\u6a21\u578b\u5dee\u5f02\u5316\uff08\u4e00\u79cd\u673a\u5236\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\uff09\u6765\u5206\u6790 Gemma-2-9b-it \u548c SimPO \u589e\u5f3a\u53d8\u4f53\u4e4b\u95f4\u7684\u5177\u4f53\u80fd\u529b\u5dee\u5f02\u3002\u5229\u7528\u4ea4\u53c9\u7f16\u7801\u5668\u8bc6\u522b\u548c\u5206\u7c7b\u533a\u5206\u8fd9\u4e24\u4e2a\u6a21\u578b\u7684\u6f5c\u5728\u8868\u5f81\u3002", "result": "SimPO \u83b7\u5f97\u7684\u6f5c\u5728\u6982\u5ff5\u4e3b\u8981\u589e\u5f3a\u4e86\u5b89\u5168\u673a\u5236\uff08+32.8%\uff09\u3001\u591a\u8bed\u8a00\u80fd\u529b\uff08+43.8%\uff09\u548c\u6307\u4ee4\u9075\u5faa\u80fd\u529b\uff08+151.7%\uff09\u3002\u540c\u65f6\uff0c\u5176\u989d\u5916\u7684\u8bad\u7ec3\u4e5f\u964d\u4f4e\u4e86\u6a21\u578b\u81ea\u6211\u6307\u6d89\uff08-44.1%\uff09\u548c\u5e7b\u89c9\u7ba1\u7406\uff08-68.5%\uff09\u7684\u4fa7\u91cd\u70b9\u3002", "conclusion": "\u6a21\u578b\u5dee\u5f02\u5316\u65b9\u6cd5\u80fd\u591f\u63d0\u4f9b\u8d85\u8d8a\u6392\u884c\u699c\u6307\u6807\u7684\u7ec6\u7c92\u5ea6\u89c1\u89e3\uff0c\u5c06\u6027\u80fd\u5dee\u8ddd\u5f52\u56e0\u4e8e\u5177\u4f53\u7684\u673a\u5236\u80fd\u529b\u3002\u8be5\u65b9\u6cd5\u4e3a\u6bd4\u8f83\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u900f\u660e\u4e14\u6709\u9488\u5bf9\u6027\u7684\u6846\u67b6\u3002"}}
{"id": "2509.18937", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18937", "abs": "https://arxiv.org/abs/2509.18937", "authors": ["Yanyuan Qiao", "Kieran Gilday", "Yutong Xie", "Josie Hughes"], "title": "Lang2Morph: Language-Driven Morphological Design of Robotic Hands", "comment": null, "summary": "Designing robotic hand morphologies for diverse manipulation tasks requires\nbalancing dexterity, manufacturability, and task-specific functionality. While\nopen-source frameworks and parametric tools support reproducible design, they\nstill rely on expert heuristics and manual tuning. Automated methods using\noptimization are often compute-intensive, simulation-dependent, and rarely\ntarget dexterous hands. Large language models (LLMs), with their broad\nknowledge of human-object interactions and strong generative capabilities,\noffer a promising alternative for zero-shot design reasoning. In this paper, we\npresent Lang2Morph, a language-driven pipeline for robotic hand design. It uses\nLLMs to translate natural-language task descriptions into symbolic structures\nand OPH-compatible parameters, enabling 3D-printable task-specific\nmorphologies. The pipeline consists of: (i) Morphology Design, which maps tasks\ninto semantic tags, structural grammars, and OPH-compatible parameters; and\n(ii) Selection and Refinement, which evaluates design candidates based on\nsemantic alignment and size compatibility, and optionally applies LLM-guided\nrefinement when needed. We evaluate Lang2Morph across varied tasks, and results\nshow that our approach can generate diverse, task-relevant morphologies. To our\nknowledge, this is the first attempt to develop an LLM-based framework for\ntask-conditioned robotic hand design.", "AI": {"tldr": "Lang2Morph\u662f\u4e00\u4e2a\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5c06\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u63cf\u8ff0\u8f6c\u5316\u4e3a\u673a\u5668\u4eba\u624b\u90e8\u51e0\u4f55\u7ed3\u6784\u548c\u53c2\u6570\u7684\u6d41\u6c34\u7ebf\uff0c\u5b9e\u73b0\u4e86\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\u76843D\u6253\u5370\u624b\u90e8\u8bbe\u8ba1\u3002", "motivation": "\u5f53\u524d\u673a\u5668\u4eba\u624b\u90e8\u8bbe\u8ba1\u4f9d\u8d56\u4e13\u5bb6\u7ecf\u9a8c\u548c\u624b\u52a8\u8c03\u6574\uff0c\u81ea\u52a8\u5316\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u4e0d\u9002\u7528\u4e8e\u7075\u5de7\u624b\u8bbe\u8ba1\u3002\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u7406\u89e3\u4eba\u4e0e\u7269\u4f53\u4ea4\u4e92\u548c\u751f\u6210\u80fd\u529b\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u53ef\u4ee5\u4f5c\u4e3a\u673a\u5668\u4eba\u624b\u90e8\u8bbe\u8ba1\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "Lang2Morph\u6d41\u6c34\u7ebf\u5305\u542b\u4e24\u4e2a\u4e3b\u8981\u90e8\u5206\uff1a1. \u5f62\u6001\u8bbe\u8ba1\uff1a\u5c06\u4efb\u52a1\u8f6c\u5316\u4e3a\u8bed\u4e49\u6807\u7b7e\u3001\u8bed\u6cd5\u7ed3\u6784\u548cOPHand\u517c\u5bb9\u53c2\u6570\u30022. \u9009\u62e9\u4e0e\u4f18\u5316\uff1a\u6839\u636e\u8bed\u4e49\u5339\u914d\u5ea6\u548c\u5c3a\u5bf8\u517c\u5bb9\u6027\u8bc4\u4f30\u8bbe\u8ba1\u5019\u9009\uff0c\u5e76\u5728\u9700\u8981\u65f6\u8fdb\u884cLLM\u5f15\u5bfc\u7684\u4f18\u5316\u3002", "result": "Lang2Morph\u5728\u591a\u79cd\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u591a\u6837\u5316\u4e14\u4e0e\u4efb\u52a1\u76f8\u5173\u7684\u673a\u5668\u4eba\u624b\u90e8\u5f62\u6001\u3002", "conclusion": "Lang2Morph\u662f\u9996\u4e2a\u5f00\u53d1\u7528\u4e8e\u4efb\u52a1\u9a71\u52a8\u673a\u5668\u4eba\u624b\u90e8\u8bbe\u8ba1\u7684\u57fa\u4e8eLLM\u7684\u6846\u67b6\uff0c\u5c55\u793a\u4e86\u5176\u5728\u751f\u6210\u4efb\u52a1\u76f8\u5173\u624b\u90e8\u5f62\u6001\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.18504", "categories": ["cs.CV", "cs.AI", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.18504", "abs": "https://arxiv.org/abs/2509.18504", "authors": ["Jiaxin Dai", "Xiang Xiang"], "title": "Hyperbolic Coarse-to-Fine Few-Shot Class-Incremental Learning", "comment": null, "summary": "In the field of machine learning, hyperbolic space demonstrates superior\nrepresentation capabilities for hierarchical data compared to conventional\nEuclidean space. This work focuses on the Coarse-To-Fine Few-Shot\nClass-Incremental Learning (C2FSCIL) task. Our study follows the Knowe\napproach, which contrastively learns coarse class labels and subsequently\nnormalizes and freezes the classifier weights of learned fine classes in the\nembedding space. To better interpret the \"coarse-to-fine\" paradigm, we propose\nembedding the feature extractor into hyperbolic space. Specifically, we employ\nthe Poincar\\'e ball model of hyperbolic space, enabling the feature extractor\nto transform input images into feature vectors within the Poincar\\'e ball\ninstead of Euclidean space. We further introduce hyperbolic contrastive loss\nand hyperbolic fully-connected layers to facilitate model optimization and\nclassification in hyperbolic space. Additionally, to enhance performance under\nfew-shot conditions, we implement maximum entropy distribution in hyperbolic\nspace to estimate the probability distribution of fine-class feature vectors.\nThis allows generation of augmented features from the distribution to mitigate\noverfitting during training with limited samples. Experiments on C2FSCIL\nbenchmarks show that our method effectively improves both coarse and fine class\naccuracies.", "AI": {"tldr": "\u63d0\u51fa\u5c06\u7279\u5f81\u63d0\u53d6\u5668\u5d4c\u5165\u53cc\u66f2\u7a7a\u95f4\u4ee5\u63d0\u5347\u5c11\u6837\u672c\u7c7b\u589e\u91cf\u5b66\u4e60\u7684\u6027\u80fd\u3002", "motivation": "\u53cc\u66f2\u7a7a\u95f4\u5728\u8868\u793a\u5c42\u7ea7\u6570\u636e\u65b9\u9762\u4f18\u4e8e\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u5728\u5c11\u6837\u672c\u7c7b\u589e\u91cf\u5b66\u4e60\uff08C2FSCIL\uff09\u4efb\u52a1\u4e2d\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u5728C2FSCIL\u4efb\u52a1\u4e2d\uff0c\u63d0\u51fa\u5c06\u7279\u5f81\u63d0\u53d6\u5668\u5d4c\u5165\u53cc\u66f2\u7a7a\u95f4\uff08\u7279\u522b\u662f\u5e9e\u52a0\u83b1\u7403\u6a21\u578b\uff09\uff0c\u5e76\u7ed3\u5408\u53cc\u66f2\u5bf9\u6bd4\u635f\u5931\u3001\u53cc\u66f2\u5168\u8fde\u63a5\u5c42\u4ee5\u53ca\u53cc\u66f2\u7a7a\u95f4\u4e2d\u7684\u6700\u5927\u71b5\u5206\u5e03\u6765\u751f\u6210\u589e\u5f3a\u7279\u5f81\uff0c\u4ee5\u89e3\u51b3\u6837\u672c\u91cf\u6709\u9650\u5bfc\u81f4\u7684\u8fc7\u62df\u5408\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u63d0\u5347\u7c97\u7c92\u5ea6\u548c\u7ec6\u7c92\u5ea6\u7c7b\u522b\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u5c06\u7279\u5f81\u63d0\u53d6\u5668\u5d4c\u5165\u53cc\u66f2\u7a7a\u95f4\uff0c\u5e76\u7ed3\u5408\u76f8\u5e94\u7684\u53cc\u66f2\u4f18\u5316\u548c\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347C2FSCIL\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2509.19062", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2509.19062", "abs": "https://arxiv.org/abs/2509.19062", "authors": ["Yoshiaki Teranishi", "Satoshi Morita", "Seiji Miyashita"], "title": "Nonadiabatic processes in dynamical controls", "comment": "13 pages, 10 figures", "summary": "In the real-time manipulation of quantum states, it is necessary to\ndynamically control the parameters of the system's Hamiltonian, which is a\nhighly nontrivial task. We have studied the survival probability during the\nconveyance of a particle by a trapping potential, where the particle may escape\nfrom the potential well due to quantum mechanical processes (Morita et al.,\nPhys. Rev. Research 6, 043329 (2024)). We pointed out that the escape\nmechanisms can be classified into two categories. One is an initial disturbance\ncaused by a non-analytic change of parameters at the starting point, the\nsignificance of which had been pointed out earlier by one of the authors\n(Morita, J. Phys. Soc. Jpn. 76, 104001 (2007)). The other is adiabatic\ntunneling, a phenomenon that occurs due to quantum tunneling during the\nacceleration process. We have proposed a formula for the survival probability\nas a function of acceleration protocols, taking both mechanisms into account.\nIn this paper, we quantitatively examine the survival probabilities in\nconveyance processes. Surprisingly, we find that the decay behaviors under\ndifferent acceleration protocols are almost perfectly explained by the combined\neffects of the initial and final disturbances, together with an integral form\nof adiabatic tunneling throughout the transport process. Therefore, once these\ndisturbance factors and the adiabatic tunneling contribution are determined,\nthe survival probability for any acceleration protocol can be estimated without\nperforming dynamical simulations for each individual case. We also analyze the\neffects of the disturbances at the initial and final points from the\nperspective of adiabatic theory.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u8ba1\u7b97\u91cf\u5b50\u6001\u8f93\u8fd0\u8fc7\u7a0b\u4e2d\u7c92\u5b50\u5b58\u6d3b\u6982\u7387\u7684\u7406\u8bba\u6a21\u578b\uff0c\u8003\u8651\u4e86\u521d\u59cb\u6270\u52a8\u3001\u6700\u7ec8\u6270\u52a8\u548c\u7edd\u70ed\u96a7\u7a7f\u4e09\u79cd\u673a\u5236\uff0c\u5e76\u53d1\u73b0\u8be5\u6a21\u578b\u80fd\u6709\u6548\u9884\u6d4b\u4e0d\u540c\u52a0\u901f\u534f\u8bae\u4e0b\u7684\u5b58\u6d3b\u6982\u7387\uff0c\u65e0\u9700\u8fdb\u884c\u590d\u6742\u7684\u52a8\u529b\u5b66\u6a21\u62df\u3002", "motivation": "\u5728\u91cf\u5b50\u6001\u7684\u5b9e\u65f6\u64cd\u63a7\u4e2d\uff0c\u52a8\u6001\u63a7\u5236\u54c8\u5bc6\u987f\u91cf\u53c2\u6570\u6781\u5177\u6311\u6218\u6027\u3002\u672c\u7814\u7a76\u65e8\u5728\u7814\u7a76\u7c92\u5b50\u5728\u52bf\u9631\u4e2d\u88ab\u8f93\u8fd0\u8fc7\u7a0b\u4e2d\uff0c\u7531\u4e8e\u91cf\u5b50\u529b\u5b66\u8fc7\u7a0b\uff08\u5982\u9003\u9038\uff09\u5bfc\u81f4\u7684\u5b58\u6d3b\u6982\u7387\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u8003\u8651\u4e86\u521d\u59cb\u6270\u52a8\u3001\u7edd\u70ed\u96a7\u7a7f\u548c\u6700\u7ec8\u6270\u52a8\u7684\u5b58\u6d3b\u6982\u7387\u8ba1\u7b97\u516c\u5f0f\uff0c\u5e76\u5bf9\u4e0d\u540c\u52a0\u901f\u534f\u8bae\u4e0b\u7684\u8f93\u8fd0\u8fc7\u7a0b\u8fdb\u884c\u5b9a\u91cf\u5206\u6790\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u8f93\u8fd0\u8fc7\u7a0b\u4e2d\uff0c\u7c92\u5b50\u7684\u5b58\u6d3b\u6982\u7387\u8870\u51cf\u884c\u4e3a\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u53ef\u4ee5\u7531\u521d\u59cb\u6270\u52a8\u3001\u6700\u7ec8\u6270\u52a8\u4ee5\u53ca\u8d2f\u7a7f\u6574\u4e2a\u8fc7\u7a0b\u7684\u7edd\u70ed\u96a7\u7a7f\u7684\u79ef\u5206\u5f62\u5f0f\u6240\u89e3\u91ca\u3002", "conclusion": "\u4e00\u65e6\u786e\u5b9a\u4e86\u6270\u52a8\u56e0\u7d20\u548c\u7edd\u70ed\u96a7\u7a7f\u7684\u8d21\u732e\uff0c\u5c31\u53ef\u4ee5\u5728\u4e0d\u8fdb\u884c\u5355\u72ec\u52a8\u529b\u5b66\u6a21\u62df\u7684\u60c5\u51b5\u4e0b\uff0c\u4f30\u7b97\u51fa\u4efb\u4f55\u52a0\u901f\u534f\u8bae\u4e0b\u7684\u5b58\u6d3b\u6982\u7387\u3002\u8be5\u7814\u7a76\u8fd8\u4ece\u7edd\u70ed\u7406\u8bba\u7684\u89d2\u5ea6\u5206\u6790\u4e86\u521d\u59cb\u548c\u6700\u7ec8\u70b9\u7684\u6270\u52a8\u6548\u5e94\u3002"}}
{"id": "2509.18150", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18150", "abs": "https://arxiv.org/abs/2509.18150", "authors": ["Kean Shi", "Liang Chen", "Haozhe Zhao", "Baobao Chang"], "title": "Sparse Training Scheme for Multimodal LLM", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have demonstrated outstanding\nperformance across a variety of domains. However, training MLLMs is often\ninefficient due to the significantly longer input sequences introduced by\nmultimodal data and the low utilization of inter-layer computations. To address\nthis challenge, we shift the focus to the training process itself and propose a\nnovel training-efficient framework based on sparse representations, termed the\nSparse Training Scheme (STS). This scheme consists of two key components: the\nVisual Token Compressor, which reduces the information load by compressing\nvisual tokens, and the Layer Dynamic Skipper, which mitigates the computational\noverhead by dynamically skipping unnecessary layers in the language model\nduring both forward and backward passes. Our approach is broadly applicable to\ndiverse MLLM architectures and has been extensively evaluated on multiple\nbenchmarks, demonstrating its effectiveness and efficiency.", "AI": {"tldr": "MLLMs\u8bad\u7ec3\u6548\u7387\u4f4e\u4e0b\uff0c\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u7a00\u758f\u8868\u793a\u7684STS\u6846\u67b6\uff0c\u5305\u62ec\u89c6\u89c9\u4ee4\u724c\u538b\u7f29\u5668\u548c\u5c42\u52a8\u6001\u8df3\u8fc7\u5668\uff0c\u4ee5\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "\u8bad\u7ec3MLLMs\u6548\u7387\u4f4e\u4e0b\uff0c\u56e0\u591a\u6a21\u6001\u6570\u636e\u5f15\u5165\u957f\u5e8f\u5217\u548c\u4f4e\u5c42\u95f4\u8ba1\u7b97\u5229\u7528\u7387\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u7a00\u758f\u8868\u793a\u7684STS\u6846\u67b6\uff0c\u5305\u62ec\u89c6\u89c9\u4ee4\u724c\u538b\u7f29\u5668\uff08\u538b\u7f29\u89c6\u89c9\u4ee4\u724c\uff09\u548c\u5c42\u52a8\u6001\u8df3\u8fc7\u5668\uff08\u52a8\u6001\u8df3\u8fc7\u4e0d\u5fc5\u8981\u7684\u5c42\uff09\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u8bc4\u4f30\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u6548\u7387\u3002", "conclusion": "STS\u6846\u67b6\u80fd\u6709\u6548\u63d0\u9ad8MLLMs\u7684\u8bad\u7ec3\u6548\u7387\u3002"}}
{"id": "2509.18868", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18868", "abs": "https://arxiv.org/abs/2509.18868", "authors": ["Dianxing Zhang", "Wendong Li", "Kani Song", "Jiaye Lu", "Gang Li", "Liuchun Yang", "Sheng Li"], "title": "Memory in Large Language Models: Mechanisms, Evaluation and Evolution", "comment": "50 pages, 1 figure, 8 tables This is a survey/framework paper on LLM\n  memory mechanisms and evaluation", "summary": "Under a unified operational definition, we define LLM memory as a persistent\nstate written during pretraining, finetuning, or inference that can later be\naddressed and that stably influences outputs. We propose a four-part taxonomy\n(parametric, contextual, external, procedural/episodic) and a memory quadruple\n(location, persistence, write/access path, controllability). We link mechanism,\nevaluation, and governance via the chain write -> read -> inhibit/update. To\navoid distorted comparisons across heterogeneous setups, we adopt a\nthree-setting protocol (parametric only, offline retrieval, online retrieval)\nthat decouples capability from information availability on the same data and\ntimeline. On this basis we build a layered evaluation: parametric (closed-book\nrecall, edit differential, memorization/privacy), contextual (position curves\nand the mid-sequence drop), external (answer correctness vs snippet\nattribution/faithfulness), and procedural/episodic (cross-session consistency\nand timeline replay, E MARS+). The framework integrates temporal governance and\nleakage auditing (freshness hits, outdated answers, refusal slices) and\nuncertainty reporting via inter-rater agreement plus paired tests with\nmultiple-comparison correction. For updating and forgetting, we present DMM\nGov: coordinating DAPT/TAPT, PEFT, model editing (ROME, MEND, MEMIT, SERAC),\nand RAG to form an auditable loop covering admission thresholds, rollout,\nmonitoring, rollback, and change audits, with specs for timeliness, conflict\nhandling, and long-horizon consistency. Finally, we give four testable\npropositions: minimum identifiability; a minimal evaluation card; causally\nconstrained editing with verifiable forgetting; and when retrieval with\nsmall-window replay outperforms ultra-long-context reading. This yields a\nreproducible, comparable, and governable coordinate system for research and\ndeployment.", "AI": {"tldr": "LLM\u8bb0\u5fc6\u88ab\u5b9a\u4e49\u4e3a\u4e00\u79cd\u6301\u4e45\u72b6\u6001\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u56db\u90e8\u5206\u5206\u7c7b\u6cd5\uff08\u53c2\u6570\u3001\u4e0a\u4e0b\u6587\u3001\u5916\u90e8\u3001\u7a0b\u5e8f/\u60c5\u666f\uff09\u548c\u4e00\u4e2a\u8bb0\u5fc6\u56db\u5143\u7ec4\uff08\u4f4d\u7f6e\u3001\u6301\u4e45\u6027\u3001\u8bfb\u5199\u8def\u5f84\u3001\u53ef\u63a7\u6027\uff09\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u201c\u5199\u5165->\u8bfb\u53d6->\u6291\u5236/\u66f4\u65b0\u201d\u94fe\u8fde\u63a5\u673a\u5236\u3001\u8bc4\u4f30\u548c\u6cbb\u7406\u3002\u4e3a\u4e86\u907f\u514d\u5f02\u6784\u8bbe\u7f6e\u4e2d\u7684\u5931\u771f\u6bd4\u8f83\uff0c\u91c7\u7528\u4e86\u4e09\u8bbe\u7f6e\u534f\u8bae\uff08\u4ec5\u53c2\u6570\u3001\u79bb\u7ebf\u68c0\u7d22\u3001\u5728\u7ebf\u68c0\u7d22\uff09\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u6784\u5efa\u4e86\u5206\u5c42\u8bc4\u4f30\uff1a\u53c2\u6570\uff08\u5c01\u95ed\u5f0f\u56de\u5fc6\u3001\u7f16\u8f91\u5dee\u5f02\u3001\u8bb0\u5fc6/\u9690\u79c1\uff09\u3001\u4e0a\u4e0b\u6587\uff08\u4f4d\u7f6e\u66f2\u7ebf\u548c\u4e2d\u95f4\u5e8f\u5217\u4e0b\u964d\uff09\u3001\u5916\u90e8\uff08\u7b54\u6848\u6b63\u786e\u6027\u4e0e\u7247\u6bb5\u5f52\u5c5e/\u5fe0\u5b9e\u5ea6\uff09\u4ee5\u53ca\u7a0b\u5e8f/\u60c5\u666f\uff08\u8de8\u4f1a\u8bdd\u4e00\u81f4\u6027\u548c\u65f6\u95f4\u7ebf\u56de\u653e\uff0cE MARS+\uff09\u3002\u8be5\u6846\u67b6\u6574\u5408\u4e86\u65f6\u95f4\u6cbb\u7406\u548c\u6cc4\u9732\u5ba1\u8ba1\uff08\u65b0\u9c9c\u5ea6\u547d\u4e2d\u3001\u8fc7\u65f6\u7b54\u6848\u3001\u62d2\u7edd\u5207\u7247\uff09\u4ee5\u53ca\u901a\u8fc7\u8bc4\u5206\u8005\u95f4\u4e00\u81f4\u6027\u52a0\u914d\u5bf9\u6d4b\u8bd5\uff08\u5e26\u6709\u591a\u6b21\u6bd4\u8f83\u6821\u6b63\uff09\u7684\u4e0d\u786e\u5b9a\u6027\u62a5\u544a\u3002\u4e3a\u4e86\u66f4\u65b0\u548c\u9057\u5fd8\uff0c\u63d0\u51fa\u4e86 DMM Gov\uff1a\u534f\u8c03 DAPT/TAPT\u3001PEFT\u3001\u6a21\u578b\u7f16\u8f91\uff08ROME\u3001MEND\u3001MEMIT\u3001SERAC\uff09\u548c RAG\uff0c\u5f62\u6210\u4e00\u4e2a\u53ef\u5ba1\u8ba1\u7684\u5faa\u73af\uff0c\u6db5\u76d6\u51c6\u5165\u9608\u503c\u3001\u63a8\u51fa\u3001\u76d1\u63a7\u3001\u56de\u6eda\u548c\u53d8\u66f4\u5ba1\u8ba1\uff0c\u5e76\u5bf9\u53ca\u65f6\u6027\u3001\u51b2\u7a81\u5904\u7406\u548c\u957f\u671f\u4e00\u81f4\u6027\u8fdb\u884c\u4e86\u89c4\u5b9a\u3002\u6700\u540e\uff0c\u63d0\u51fa\u4e86\u56db\u4e2a\u53ef\u68c0\u9a8c\u7684\u547d\u9898\uff1a\u6700\u5c0f\u53ef\u8bc6\u522b\u6027\uff1b\u6700\u5c0f\u8bc4\u4f30\u5361\uff1b\u56e0\u679c\u7ea6\u675f\u7f16\u8f91\u4e0e\u53ef\u9a8c\u8bc1\u9057\u5fd8\uff1b\u4ee5\u53ca\u68c0\u7d22\u4e0e\u5c0f\u7a97\u53e3\u56de\u653e\u4f55\u65f6\u4f18\u4e8e\u8d85\u957f\u4e0a\u4e0b\u6587\u9605\u8bfb\u3002\u8fd9\u4e3a\u7814\u7a76\u548c\u90e8\u7f72\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u91cd\u73b0\u3001\u53ef\u6bd4\u8f83\u548c\u53ef\u6cbb\u7406\u7684\u5750\u6807\u7cfb\u3002", "motivation": "\u5b9a\u4e49\u548c\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u8bb0\u5fc6\u80fd\u529b\uff0c\u5e76\u63d0\u51fa\u4e00\u4e2a\u6cbb\u7406\u6846\u67b6\uff0c\u4ee5\u5b9e\u73b0\u53ef\u91cd\u73b0\u3001\u53ef\u6bd4\u8f83\u548c\u53ef\u6cbb\u7406\u7684\u7814\u7a76\u548c\u90e8\u7f72\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u7684 LLM \u8bb0\u5fc6\u5b9a\u4e49\uff0c\u4e00\u4e2a\u56db\u90e8\u5206\u5206\u7c7b\u6cd5\u548c\u4e00\u4e2a\u8bb0\u5fc6\u56db\u5143\u7ec4\u3002\u91c7\u7528\u4e09\u8bbe\u7f6e\u534f\u8bae\u548c\u5206\u5c42\u8bc4\u4f30\u65b9\u6cd5\uff08\u53c2\u6570\u3001\u4e0a\u4e0b\u6587\u3001\u5916\u90e8\u3001\u7a0b\u5e8f/\u60c5\u666f\uff09\u3002\u63d0\u51fa DMM Gov \u6846\u67b6\u7528\u4e8e\u66f4\u65b0\u548c\u9057\u5fd8\uff0c\u6574\u5408\u4e86\u591a\u79cd\u6280\u672f\u3002\u63d0\u51fa\u56db\u4e2a\u53ef\u68c0\u9a8c\u7684\u547d\u9898\u3002", "result": "\u6784\u5efa\u4e86\u4e00\u4e2a LLM \u8bb0\u5fc6\u8bc4\u4f30\u548c\u6cbb\u7406\u6846\u67b6\uff0c\u5305\u62ec DMM Gov\uff0c\u5e76\u63d0\u51fa\u4e86\u56db\u4e2a\u53ef\u68c0\u9a8c\u7684\u547d\u9898\uff0c\u65e8\u5728\u5b9e\u73b0\u53ef\u91cd\u73b0\u3001\u53ef\u6bd4\u8f83\u548c\u53ef\u6cbb\u7406\u7684\u7814\u7a76\u548c\u90e8\u7f72\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684 LLM \u8bb0\u5fc6\u5b9a\u4e49\u3001\u8bc4\u4f30\u6846\u67b6\u548c\u6cbb\u7406\u673a\u5236\u4e3a LLM \u8bb0\u5fc6\u7684\u7814\u7a76\u548c\u90e8\u7f72\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7cfb\u7edf\u5316\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u5b9e\u73b0\u53ef\u91cd\u73b0\u3001\u53ef\u6bd4\u8f83\u548c\u53ef\u6cbb\u7406\u7684\u8bc4\u4f30\u4e0e\u8bb0\u5fc6\u76f8\u5173\u7684\u80fd\u529b\u3002"}}
{"id": "2509.18813", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18813", "abs": "https://arxiv.org/abs/2509.18813", "authors": ["Liting Zhang", "Shiwan Zhao", "Aobo Kong", "Qicheng Li"], "title": "MAPEX: A Multi-Agent Pipeline for Keyphrase Extraction", "comment": null, "summary": "Keyphrase extraction is a fundamental task in natural language processing.\nHowever, existing unsupervised prompt-based methods for Large Language Models\n(LLMs) often rely on single-stage inference pipelines with uniform prompting,\nregardless of document length or LLM backbone. Such one-size-fits-all designs\nhinder the full exploitation of LLMs' reasoning and generation capabilities,\nespecially given the complexity of keyphrase extraction across diverse\nscenarios. To address these challenges, we propose MAPEX, the first framework\nthat introduces multi-agent collaboration into keyphrase extraction. MAPEX\ncoordinates LLM-based agents through modules for expert recruitment, candidate\nextraction, topic guidance, knowledge augmentation, and post-processing. A\ndual-path strategy dynamically adapts to document length: knowledge-driven\nextraction for short texts and topic-guided extraction for long texts.\nExtensive experiments on six benchmark datasets across three different LLMs\ndemonstrate its strong generalization and universality, outperforming the\nstate-of-the-art unsupervised method by 2.44\\% and standard LLM baselines by\n4.01\\% in F1@5 on average. Code is available at\nhttps://github.com/NKU-LITI/MAPEX.", "AI": {"tldr": "MAPEX\u662f\u4e00\u4e2a\u65b0\u63d0\u51fa\u7684\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\uff0c\u7528\u4e8e\u65e0\u76d1\u7763\u5173\u952e\u8bcd\u63d0\u53d6\uff0c\u901a\u8fc7\u52a8\u6001\u9002\u5e94\u6587\u6863\u957f\u5ea6\u548c\u5229\u7528LLM\u80fd\u529b\u6765\u63d0\u9ad8\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u65e0\u76d1\u7763\u63d0\u793a\u8bcd\u65b9\u6cd5\u5728\u5904\u7406\u4e0d\u540c\u6587\u6863\u957f\u5ea6\u548cLLM\u9aa8\u5e72\u65f6\u5b58\u5728\u4e0d\u8db3\uff0c\u672a\u80fd\u5145\u5206\u53d1\u6325LLM\u7684\u6f5c\u529b\u3002", "method": "MAPEX\u5f15\u5165\u4e86\u591a\u667a\u80fd\u4f53\u534f\u4f5c\uff0c\u5305\u62ec\u4e13\u5bb6\u62db\u52df\u3001\u5019\u9009\u63d0\u53d6\u3001\u4e3b\u9898\u5f15\u5bfc\u3001\u77e5\u8bc6\u589e\u5f3a\u548c\u540e\u5904\u7406\u6a21\u5757\u3002\u91c7\u7528\u53cc\u8def\u5f84\u7b56\u7565\uff0c\u6839\u636e\u6587\u6863\u957f\u5ea6\u9009\u62e9\u77e5\u8bc6\u9a71\u52a8\u63d0\u53d6\uff08\u77ed\u6587\u672c\uff09\u6216\u4e3b\u9898\u5f15\u5bfc\u63d0\u53d6\uff08\u957f\u6587\u672c\uff09\u3002", "result": "\u5728\u516d\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u548c\u4e09\u79cd\u4e0d\u540c\u7684LLM\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0cMAPEX\u7684\u6cdb\u5316\u6027\u548c\u901a\u7528\u6027\u8868\u73b0\u51fa\u8272\uff0c\u5728F1@5\u6307\u6807\u4e0a\u5e73\u5747\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65e0\u76d1\u7763\u65b9\u6cd52.44%\uff0c\u4f18\u4e8e\u6807\u51c6\u7684LLM\u57fa\u7ebf4.01%\u3002", "conclusion": "MAPEX\u901a\u8fc7\u5f15\u5165\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u548c\u52a8\u6001\u9002\u5e94\u6587\u6863\u957f\u5ea6\u7684\u53cc\u8def\u5f84\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65e0\u76d1\u7763\u5173\u952e\u8bcd\u63d0\u53d6\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2509.18953", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18953", "abs": "https://arxiv.org/abs/2509.18953", "authors": ["Hanqing Liu", "Jiahuan Long", "Junqi Wu", "Jiacheng Hou", "Huili Tang", "Tingsong Jiang", "Weien Zhou", "Wen Yao"], "title": "Eva-VLA: Evaluating Vision-Language-Action Models' Robustness Under Real-World Physical Variations", "comment": null, "summary": "Vision-Language-Action (VLA) models have emerged as promising solutions for\nrobotic manipulation, yet their robustness to real-world physical variations\nremains critically underexplored. To bridge this gap, we propose Eva-VLA, the\nfirst unified framework that systematically evaluates the robustness of VLA\nmodels by transforming discrete physical variations into continuous\noptimization problems. However, comprehensively assessing VLA robustness\npresents two key challenges: (1) how to systematically characterize diverse\nphysical variations encountered in real-world deployments while maintaining\nevaluation reproducibility, and (2) how to discover worst-case scenarios\nwithout prohibitive real-world data collection costs efficiently. To address\nthe first challenge, we decompose real-world variations into three critical\ndomains: object 3D transformations that affect spatial reasoning, illumination\nvariations that challenge visual perception, and adversarial patches that\ndisrupt scene understanding. For the second challenge, we introduce a\ncontinuous black-box optimization framework that transforms discrete physical\nvariations into parameter optimization, enabling systematic exploration of\nworst-case scenarios. Extensive experiments on state-of-the-art OpenVLA models\nacross multiple benchmarks reveal alarming vulnerabilities: all variation types\ntrigger failure rates exceeding 60%, with object transformations causing up to\n97.8% failure in long-horizon tasks. Our findings expose critical gaps between\ncontrolled laboratory success and unpredictable deployment readiness, while the\nEva-VLA framework provides a practical pathway for hardening VLA-based robotic\nmanipulation models against real-world deployment challenges.", "AI": {"tldr": "Eva-VLA\u662f\u4e00\u4e2a\u8bc4\u4f30VLA\u6a21\u578b\u9c81\u68d2\u6027\u7684\u6846\u67b6\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u9762\u5bf9\u7269\u7406\u53d8\u5316\u65f6\u5b58\u5728\u4e25\u91cd\u6f0f\u6d1e\u3002", "motivation": "\u8bc4\u4f30\u548c\u6539\u8fdb\u73b0\u6709VLA\u6a21\u578b\u5728\u73b0\u5b9e\u4e16\u754c\u7269\u7406\u53d8\u5316\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u89e3\u51b3VLA\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u53ef\u80fd\u9047\u5230\u7684\u6311\u6218\u3002", "method": "\u5c06\u7269\u7406\u53d8\u5316\u8f6c\u5316\u4e3a\u8fde\u7eed\u4f18\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u4f18\u5316\u4e09\u4e2a\u5173\u952e\u9886\u57df\uff08\u7269\u4f533D\u53d8\u6362\u3001\u5149\u7167\u53d8\u5316\u3001\u5bf9\u6297\u6027\u8865\u4e01\uff09\u7684\u53c2\u6570\u6765\u63a2\u7d22\u6700\u574f\u60c5\u51b5\u3002", "result": "\u73b0\u6709VLA\u6a21\u578b\u5728\u6240\u6709\u7c7b\u578b\u7684\u53d8\u5316\u4e0b\u5931\u8d25\u7387\u5747\u8d85\u8fc760%\uff0c\u5176\u4e2d\u7269\u4f53\u53d8\u6362\u5728\u957f\u5e8f\u5217\u4efb\u52a1\u4e2d\u5931\u8d25\u7387\u9ad8\u8fbe97.8%\u3002", "conclusion": "Eva-VLA\u6846\u67b6\u53ef\u4ee5\u7cfb\u7edf\u5730\u8bc4\u4f30VLA\u6a21\u578b\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u4e3a\u63d0\u9ad8\u5176\u5728\u73b0\u5b9e\u4e16\u754c\u90e8\u7f72\u4e2d\u7684\u53ef\u9760\u6027\u63d0\u4f9b\u9014\u5f84\u3002Eva-VLA\u7684\u5b9e\u9a8c\u7ed3\u679c\u63ed\u793a\u4e86\u5f53\u524dVLA\u6a21\u578b\u5728\u9762\u5bf9\u771f\u5b9e\u4e16\u754c\u590d\u6742\u591a\u53d8\u7684\u73af\u5883\u65f6\u5b58\u5728\u663e\u8457\u7684\u8106\u5f31\u6027\u3002"}}
{"id": "2509.18538", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18538", "abs": "https://arxiv.org/abs/2509.18538", "authors": ["Zixin Zhu", "Haoxiang Li", "Xuelu Feng", "He Wu", "Chunming Qiao", "Junsong Yuan"], "title": "GeoRemover: Removing Objects and Their Causal Visual Artifacts", "comment": "Accepted as Spotlight at NeurIPS 2025", "summary": "Towards intelligent image editing, object removal should eliminate both the\ntarget object and its causal visual artifacts, such as shadows and reflections.\nHowever, existing image appearance-based methods either follow strictly\nmask-aligned training and fail to remove these causal effects which are not\nexplicitly masked, or adopt loosely mask-aligned strategies that lack\ncontrollability and may unintentionally over-erase other objects. We identify\nthat these limitations stem from ignoring the causal relationship between an\nobject's geometry presence and its visual effects. To address this limitation,\nwe propose a geometry-aware two-stage framework that decouples object removal\ninto (1) geometry removal and (2) appearance rendering. In the first stage, we\nremove the object directly from the geometry (e.g., depth) using strictly\nmask-aligned supervision, enabling structure-aware editing with strong\ngeometric constraints. In the second stage, we render a photorealistic RGB\nimage conditioned on the updated geometry, where causal visual effects are\nconsidered implicitly as a result of the modified 3D geometry. To guide\nlearning in the geometry removal stage, we introduce a preference-driven\nobjective based on positive and negative sample pairs, encouraging the model to\nremove objects as well as their causal visual artifacts while avoiding new\nstructural insertions. Extensive experiments demonstrate that our method\nachieves state-of-the-art performance in removing both objects and their\nassociated artifacts on two popular benchmarks. The code is available at\nhttps://github.com/buxiangzhiren/GeoRemover.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u51e0\u4f55\u611f\u77e5\u5bf9\u8c61\u79fb\u9664\u65b9\u6cd5\uff0c\u901a\u8fc7\u89e3\u8026\u51e0\u4f55\u79fb\u9664\u548c\u5916\u89c2\u6e32\u67d3\u4e24\u4e2a\u9636\u6bb5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u53bb\u9664\u9634\u5f71\u3001\u53cd\u5c04\u7b49\u89c6\u89c9\u7ebf\u7d22\uff0c\u4ee5\u53ca\u8fc7\u5ea6\u64e6\u9664\u7b49\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\u5728\u79fb\u9664\u5bf9\u8c61\u53ca\u5176\u89c6\u89c9\u7ebf\u7d22\uff08\u5982\u9634\u5f71\u3001\u53cd\u5c04\uff09\u65f6\u5b58\u5728\u4e0d\u8db3\uff0c\u8981\u4e48\u56e0\u4e25\u683c\u7684\u63a9\u7801\u5bf9\u9f50\u800c\u65e0\u6cd5\u53bb\u9664\u672a\u88ab\u63a9\u7801\u7684\u89c6\u89c9\u7ebf\u7d22\uff0c\u8981\u4e48\u56e0\u677e\u6563\u7684\u63a9\u7801\u5bf9\u9f50\u800c\u7f3a\u4e4f\u53ef\u63a7\u6027\u5e76\u53ef\u80fd\u8bef\u5220\u5176\u4ed6\u5bf9\u8c61\u3002\u8fd9\u4e3b\u8981\u662f\u7531\u4e8e\u5ffd\u7565\u4e86\u5bf9\u8c61\u51e0\u4f55\u4e0e\u5176\u89c6\u89c9\u7ebf\u7d22\u4e4b\u95f4\u7684\u56e0\u679c\u5173\u7cfb\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u51e0\u4f55\u611f\u77e5\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1. \u51e0\u4f55\u79fb\u9664\uff1a\u5229\u7528\u4e25\u683c\u63a9\u7801\u5bf9\u9f50\u7684\u76d1\u7763\uff0c\u76f4\u63a5\u4ece\u51e0\u4f55\u4fe1\u606f\uff08\u5982\u6df1\u5ea6\u56fe\uff09\u4e2d\u79fb\u9664\u5bf9\u8c61\uff0c\u5b9e\u73b0\u5177\u6709\u5f3a\u51e0\u4f55\u7ea6\u675f\u7684\u7ed3\u6784\u611f\u77e5\u7f16\u8f91\u30022. \u5916\u89c2\u6e32\u67d3\uff1a\u6839\u636e\u66f4\u65b0\u540e\u7684\u51e0\u4f55\u4fe1\u606f\u6e32\u67d3\u903c\u771f\u7684RGB\u56fe\u50cf\uff0c\u5c06\u89c6\u89c9\u7ebf\u7d22\u7684\u79fb\u9664\u4f5c\u4e3a\u4fee\u65393D\u51e0\u4f55\u7684\u5185\u9690\u7ed3\u679c\u3002\u5f15\u5165\u4e86\u57fa\u4e8e\u6b63\u8d1f\u6837\u672c\u5bf9\u7684\u504f\u597d\u9a71\u52a8\u76ee\u6807\uff0c\u4ee5\u6307\u5bfc\u51e0\u4f55\u79fb\u9664\u9636\u6bb5\u7684\u5b66\u4e60\u3002", "result": "\u5728\u4e24\u4e2a\u6d41\u884c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u79fb\u9664\u5bf9\u8c61\u53ca\u5176\u76f8\u5173\u89c6\u89c9\u7ebf\u7d22\u65b9\u9762\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u51e0\u4f55\u611f\u77e5\u4e24\u9636\u6bb5\u6846\u67b6\u80fd\u591f\u6709\u6548\u4e14\u53ef\u63a7\u5730\u79fb\u9664\u5bf9\u8c61\u53ca\u5176\u56e0\u679c\u89c6\u89c9\u7ebf\u7d22\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2509.19157", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2509.19157", "abs": "https://arxiv.org/abs/2509.19157", "authors": ["Shengxin Zhuang", "Yusen Wu", "Xavier F. Cadet", "Du Q. Huynh", "Wei Liu", "Philippe Charton", "Cedric Damour", "Frederic Cadet", "Jingbo B. Wang"], "title": "Quantum Autoencoder: An efficient approach to quantum feature map generation", "comment": null, "summary": "Quantum machine learning methods often rely on fixed, hand-crafted quantum\nencodings that may not capture optimal features for downstream tasks. In this\nwork, we study the power of quantum autoencoders in learning data-driven\nquantum representations. We first theoretically demonstrate that the quantum\nautoencoder method is efficient in terms of sample complexity throughout the\nentire training process. Then we numerically train the quantum autoencoder on 3\nmillion peptide sequences, and evaluate their effectiveness across multiple\npeptide classification problems including antihypertensive peptide prediction,\nblood-brain barrier-penetration, and cytotoxic activity detection. The learned\nrepresentations were compared against Hamiltonian-evolved baselines using a\nquantum kernel with support vector machines. Results show that quantum\nautoencoder learned representations achieve accuracy improvements ranging from\n0.4\\% to 8.1\\% over Hamiltonian baselines across seven datasets, demonstrating\neffective generalization to diverse downstream datasets with pre-training\nenabling effective transfer learning without task-specific fine-tuning. This\nwork establishes that quantum autoencoder architectures can effectively learn\nfrom large-scale datasets (3 million samples) with compact parameterizations\n($\\sim$900 parameters), demonstrating their viability for practical quantum\napplications.", "AI": {"tldr": "\u91cf\u5b50\u81ea\u7f16\u7801\u5668\u53ef\u4ee5\u4ece\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e2d\u5b66\u4e60\u6709\u6548\u7684\u91cf\u5b50\u8868\u5f81\uff0c\u5e76\u5728\u591a\u79cd\u80bd\u5206\u7c7b\u4efb\u52a1\u4e2d\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u56fa\u5b9a\u7684\u3001\u624b\u5de5\u8bbe\u8ba1\u7684\u91cf\u5b50\u7f16\u7801\uff0c\u53ef\u80fd\u65e0\u6cd5\u6355\u6349\u5230\u6700\u4f18\u7279\u5f81\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u91cf\u5b50\u81ea\u7f16\u7801\u5668\u5728\u5b66\u4e60\u6570\u636e\u9a71\u52a8\u7684\u91cf\u5b50\u8868\u5f81\u65b9\u9762\u7684\u80fd\u529b\u3002", "method": "\u5728\u7406\u8bba\u4e0a\u8bc1\u660e\u4e86\u91cf\u5b50\u81ea\u7f16\u7801\u5668\u5728\u6574\u4e2a\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5177\u6709\u6837\u672c\u590d\u6742\u5ea6\u4f18\u52bf\uff0c\u5e76\u5728\u6570\u503c\u4e0a\u5bf9300\u4e07\u4e2a\u80bd\u5e8f\u5217\u8fdb\u884c\u4e86\u8bad\u7ec3\uff0c\u7136\u540e\u5c06\u5b66\u4e60\u5230\u7684\u8868\u5f81\u4e0e\u57fa\u4e8e\u54c8\u5bc6\u987f\u6f14\u5316\u7684\u57fa\u7ebf\u8fdb\u884c\u6bd4\u8f83\uff0c\u4f7f\u7528\u4e86\u91cf\u5b50\u6838\u652f\u6301\u5411\u91cf\u673a\u3002", "result": "\u91cf\u5b50\u81ea\u7f16\u7801\u5668\u5b66\u4e60\u5230\u7684\u8868\u5f81\u5728\u4e03\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u51c6\u786e\u5ea6\u6bd4\u54c8\u5bc6\u987f\u57fa\u7ebf\u63d0\u9ad8\u4e860.4%\u52308.1%\uff0c\u8868\u660e\u5176\u80fd\u591f\u6709\u6548\u5730\u6cdb\u5316\u5230\u4e0d\u540c\u7684\u4e0b\u6e38\u6570\u636e\u96c6\uff0c\u5e76\u4e14\u9884\u8bad\u7ec3\u53ef\u4ee5\u5b9e\u73b0\u6709\u6548\u7684\u8fc1\u79fb\u5b66\u4e60\uff0c\u65e0\u9700\u8fdb\u884c\u7279\u5b9a\u4efb\u52a1\u7684\u5fae\u8c03\u3002\u6b64\u5916\uff0c\u91cf\u5b50\u81ea\u7f16\u7801\u5668\u67b6\u6784\u80fd\u591f\u4ece\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff08300\u4e07\u4e2a\u6837\u672c\uff09\u4e2d\u5b66\u4e60\uff0c\u5e76\u4e14\u53c2\u6570\u91cf\u7d27\u51d1\uff08\u7ea6900\u4e2a\u53c2\u6570\uff09\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5b9e\u9645\u91cf\u5b50\u5e94\u7528\u4e2d\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u91cf\u5b50\u81ea\u7f16\u7801\u5668\u662f\u4e00\u79cd\u6709\u6548\u7684\u5b66\u4e60\u6570\u636e\u9a71\u52a8\u91cf\u5b50\u8868\u5f81\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u5904\u7406\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u5e76\u5728\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\u4e2d\u53d6\u5f97\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u7684\u6027\u80fd\u3002"}}
{"id": "2509.18151", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18151", "abs": "https://arxiv.org/abs/2509.18151", "authors": ["Jindi Lv", "Yuhao Zhou", "Yuxin Tian", "Qing Ye", "Wentao Feng", "Jiancheng Lv"], "title": "HyperNAS: Enhancing Architecture Representation for NAS Predictor via Hypernetwork", "comment": null, "summary": "Time-intensive performance evaluations significantly impede progress in\nNeural Architecture Search (NAS). To address this, neural predictors leverage\nsurrogate models trained on proxy datasets, allowing for direct performance\npredictions for new architectures. However, these predictors often exhibit poor\ngeneralization due to their limited ability to capture intricate relationships\namong various architectures. In this paper, we propose HyperNAS, a novel neural\npredictor paradigm for enhancing architecture representation learning. HyperNAS\nconsists of two primary components: a global encoding scheme and a shared\nhypernetwork. The global encoding scheme is devised to capture the\ncomprehensive macro-structure information, while the shared hypernetwork serves\nas an auxiliary task to enhance the investigation of inter-architecture\npatterns. To ensure training stability, we further develop a dynamic adaptive\nmulti-task loss to facilitate personalized exploration on the Pareto front.\nExtensive experiments across five representative search spaces, including ViTs,\ndemonstrate the advantages of HyperNAS, particularly in few-shot scenarios. For\ninstance, HyperNAS strikes new state-of-the-art results, with 97.60\\% top-1\naccuracy on CIFAR-10 and 82.4\\% top-1 accuracy on ImageNet, using at least\n5.0$\\times$ fewer samples.", "AI": {"tldr": "HyperNAS\u901a\u8fc7\u5168\u5c40\u7f16\u7801\u65b9\u6848\u548c\u5171\u4eab\u8d85\u7f51\u7edc\u6765\u5b66\u4e60\u67b6\u6784\u8868\u793a\uff0c\u4ee5\u63d0\u9ad8\u795e\u7ecf\u67b6\u6784\u641c\u7d22\u4e2d\u7684\u6027\u80fd\u8bc4\u4f30\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5728\u5c11\u6837\u672c\u573a\u666f\u4e0b\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6210\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u795e\u7ecf\u67b6\u6784\u641c\u7d22\uff08NAS\uff09\u4e2d\u7684\u6027\u80fd\u8bc4\u4f30\u8017\u65f6\u8fc7\u957f\uff0c\u963b\u788d\u4e86\u7814\u7a76\u8fdb\u5c55\u3002\u867d\u7136\u795e\u7ecf\u9884\u6d4b\u5668\u53ef\u4ee5\u4f5c\u4e3a\u4ee3\u7406\u6a21\u578b\u52a0\u901f\u8bc4\u4f30\uff0c\u4f46\u5b83\u4eec\u7531\u4e8e\u96be\u4ee5\u6355\u6349\u67b6\u6784\u95f4\u7684\u590d\u6742\u5173\u7cfb\u800c\u6cdb\u5316\u80fd\u529b\u8f83\u5dee\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHyperNAS\u7684\u65b0\u578b\u795e\u7ecf\u9884\u6d4b\u5668\u8303\u5f0f\uff0c\u5305\u542b\u5168\u5c40\u7f16\u7801\u65b9\u6848\uff08\u7528\u4e8e\u6355\u6349\u5b8f\u89c2\u7ed3\u6784\u4fe1\u606f\uff09\u548c\u5171\u4eab\u8d85\u7f51\u7edc\uff08\u7528\u4e8e\u589e\u5f3a\u8de8\u67b6\u6784\u6a21\u5f0f\u7684\u5b66\u4e60\uff09\u3002\u6b64\u5916\uff0c\u8fd8\u5f00\u53d1\u4e86\u4e00\u79cd\u52a8\u6001\u81ea\u9002\u5e94\u591a\u4efb\u52a1\u635f\u5931\u6765\u7a33\u5b9a\u8bad\u7ec3\u5e76\u5b9e\u73b0Pareto\u524d\u6cbf\u7684\u4e2a\u6027\u5316\u63a2\u7d22\u3002", "result": "\u5728\u5305\u62ecViTs\u5728\u5185\u7684\u4e94\u4e2a\u4ee3\u8868\u6027\u641c\u7d22\u7a7a\u95f4\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u3002\u7ed3\u679c\u8868\u660e\uff0cHyperNAS\u5728\u5c11\u6837\u672c\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u663e\u8457\u4f18\u52bf\uff0c\u5728CIFAR-10\u4e0a\u8fbe\u5230\u4e8697.60%\u7684\u51c6\u786e\u7387\uff0c\u5728ImageNet\u4e0a\u8fbe\u5230\u4e8682.4%\u7684\u51c6\u786e\u7387\uff0c\u6837\u672c\u91cf\u51cf\u5c11\u4e86\u81f3\u5c115\u500d\u3002", "conclusion": "HyperNAS\u901a\u8fc7\u6539\u8fdb\u67b6\u6784\u8868\u793a\u5b66\u4e60\uff0c\u6709\u6548\u5730\u89e3\u51b3\u4e86NAS\u4e2d\u6027\u80fd\u8bc4\u4f30\u7684\u6311\u6218\uff0c\u5c24\u5176\u5728\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u80fd\u591f\u4ee5\u66f4\u5c11\u7684\u6837\u672c\u83b7\u5f97\u4f18\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2509.18883", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18883", "abs": "https://arxiv.org/abs/2509.18883", "authors": ["Meituan LongCat Team", "Anchun Gui", "Bei Li", "Bingyang Tao", "Bole Zhou", "Borun Chen", "Chao Zhang", "Chao Zhang", "Chengcheng Han", "Chenhui Yang", "Chi Zhang", "Chong Peng", "Chuyu Zhang", "Cong Chen", "Fengcun Li", "Gang Xu", "Guoyuan Lin", "Hao Jiang", "Hao Liang", "Haomin Fu", "Haoxiang Ma", "Hong Liu", "Hongyan Hao", "Hongyin Tang", "Hongyu Zang", "Hongzhi Ni", "Hui Su", "Jiahao Liu", "Jiahuan Li", "Jialin Liu", "Jianfei Zhang", "Jianhao Xu", "Jianing Wang", "Jiaqi Sun", "Jiaqi Zhang", "Jiarong Shi", "Jiawei Yang", "Jingang Wang", "Jinrui Ding", "Jun Kuang", "Jun Xu", "Ke He", "Kefeng Zhang", "Keheng Wang", "Keqing He", "Li Wei", "Liang Shi", "Lin Qiu", "Lingbin Kong", "Lingchuan Liu", "Linsen Guo", "Longfei An", "Mai Xia", "Meng Zhou", "Mengshen Zhu", "Peng Pei", "Pengcheng Jia", "Qi Gu", "Qi Guo", "Qiong Huang", "Quan Chen", "Quanchi Weng", "Rongxiang Weng", "Ruichen Shao", "Rumei Li", "Shanglin Lei", "Shuai Du", "Shuaikang Liu", "Shuang Zhou", "Shuhao Hu", "Siyu Xu", "Songshan Gong", "Tao Liang", "Tianhao Hu", "Wei He", "Wei Shi", "Wei Wang", "Wei Wu", "Wei Zhuo", "Weifeng Tang", "Wenjie Shi", "Wenlong Zhu", "Xi Su", "Xiangcheng Liu", "Xiangyu Xi", "Xiangzhou Huang", "Xiao Liu", "Xiaochen Jiang", "Xiaowei Shi", "Xiaowen Shi", "Xiaoyu Li", "Xin Chen", "Xinyue Zhao", "Xuan Huang", "Xuemiao Zhang", "Xuezhi Cao", "Xunliang Cai", "Yajie Zhang", "Yang Chen", "Yang Liu", "Yang Liu", "Yang Zheng", "Yaoming Wang", "Yaqi Huo", "Yerui Sun", "Yifan Lu", "Yiyang Li", "Youshao Xiao", "Yuanzhe Lei", "Yuchen Xie", "Yueqing Sun", "Yufei Zhang", "Yuhuai Wei", "Yulei Qian", "Yunke Zhao", "Yuqing Ding", "Yuwei Jiang", "Zhaohua Yang", "Zhengyu Chen", "Zhijian Liu", "Zhikang Xia", "Zhongda Su", "Ziran Li", "Ziwen Wang", "Ziyuan Zhuang", "Zongyu Wang", "Zunyuan Yang"], "title": "LongCat-Flash-Thinking Technical Report", "comment": null, "summary": "We present LongCat-Flash-Thinking, an efficient 560-billion-parameter\nopen-source Mixture-of-Experts (MoE) reasoning model. Its advanced capabilities\nare cultivated through a meticulously crafted training process, beginning with\nlong Chain-of-Thought (CoT) data cold-start and culminating in large-scale\nReinforcement Learning (RL). We first employ a well-designed cold-start\ntraining strategy, which significantly enhances the reasoning potential and\nequips the model with specialized skills in both formal and agentic reasoning.\nThen, a core innovation is our domain-parallel training scheme, which decouples\noptimization across distinct domains (e.g., STEM, Code, Agentic) and\nsubsequently fuses the resulting expert models into a single, nearly\nPareto-optimal model. This entire process is powered by our Dynamic\nORchestration for Asynchronous rollout (DORA) system, a large-scale RL\nframework that delivers a greater than threefold training speedup over\nsynchronous methods on tens of thousands of accelerators. As a result,\nLongCat-Flash-Thinking achieves state-of-the-art performance among open-source\nmodels on a suite of complex reasoning tasks. The model exhibits exceptional\nefficiency in agentic reasoning, reducing average token consumption by 64.5%\n(from 19, 653 to 6, 965) on AIME-25, without degrading task accuracy. We\nrelease LongCat-Flash-Thinking to promote further advances in reasoning systems\nand agentic AI research.", "AI": {"tldr": "LongCat-Flash-Thinking\u662f\u4e00\u4e2a\u9ad8\u6548\u76845600\u4ebf\u53c2\u6570\u5f00\u6e90MoE\u63a8\u7406\u6a21\u578b\uff0c\u901a\u8fc7\u957fCoT\u6570\u636e\u51b7\u542f\u52a8\u548c\u5927\u89c4\u6a21RL\u8bad\u7ec3\uff0c\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u8fbe\u5230SOTA\u3002", "motivation": "\u4e3a\u4e86\u5f00\u53d1\u4e00\u4e2a\u9ad8\u6548\u7684\u5927\u89c4\u6a21MoE\u63a8\u7406\u6a21\u578b\uff0c\u5e76\u63d0\u5347\u5176\u5728\u5f62\u5f0f\u5316\u548cagentic\u63a8\u7406\u65b9\u9762\u7684\u80fd\u529b\u3002", "method": "\u91c7\u7528\u957fCoT\u6570\u636e\u51b7\u542f\u52a8\uff0c\u7ed3\u5408\u9886\u57df\u5e76\u884c\u8bad\u7ec3\u65b9\u6848\uff0c\u5e76\u901a\u8fc7DORA\u7cfb\u7edf\u8fdb\u884c\u5927\u89c4\u6a21RL\u8bad\u7ec3\uff0c\u5b9e\u73b0\u5f02\u6b65\u8bad\u7ec3\u52a0\u901f\u3002", "result": "\u5728AIME-25\u7b49\u63a8\u7406\u4efb\u52a1\u4e0a\u53d6\u5f97SOTA\u6027\u80fd\uff0c\u5e76\u663e\u8457\u964d\u4f4e\u4e86agentic\u63a8\u7406\u7684token\u6d88\u8017\uff08\u5e73\u5747\u51cf\u5c1164.5%\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4efb\u52a1\u51c6\u786e\u6027\u3002", "conclusion": "LongCat-Flash-Thinking\u662f\u4e00\u4e2a\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u5f00\u6e90\u63a8\u7406\u6a21\u578b\uff0c\u5176\u8bad\u7ec3\u65b9\u6cd5\u548c\u7cfb\u7edf\u5177\u6709\u666e\u9002\u6027\uff0c\u65e8\u5728\u63a8\u52a8\u63a8\u7406\u7cfb\u7edf\u548cagentic AI\u7684\u7814\u7a76\u3002"}}
{"id": "2509.18843", "categories": ["cs.CL", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18843", "abs": "https://arxiv.org/abs/2509.18843", "authors": ["Damian Stachura", "Joanna Konieczna", "Artur Nowak"], "title": "Are Smaller Open-Weight LLMs Closing the Gap to Proprietary Models for Biomedical Question Answering?", "comment": "CLEF 2025 Working Notes, 9-12 September 2025, Madrid, Spain", "summary": "Open-weight versions of large language models (LLMs) are rapidly advancing,\nwith state-of-the-art models like DeepSeek-V3 now performing comparably to\nproprietary LLMs. This progression raises the question of whether small\nopen-weight LLMs are capable of effectively replacing larger closed-source\nmodels. We are particularly interested in the context of biomedical\nquestion-answering, a domain we explored by participating in Task 13B Phase B\nof the BioASQ challenge. In this work, we compare several open-weight models\nagainst top-performing systems such as GPT-4o, GPT-4.1, Claude 3.5 Sonnet, and\nClaude 3.7 Sonnet. To enhance question answering capabilities, we use various\ntechniques including retrieving the most relevant snippets based on embedding\ndistance, in-context learning, and structured outputs. For certain submissions,\nwe utilize ensemble approaches to leverage the diverse outputs generated by\ndifferent models for exact-answer questions. Our results demonstrate that\nopen-weight LLMs are comparable to proprietary ones. In some instances,\nopen-weight LLMs even surpassed their closed counterparts, particularly when\nensembling strategies were applied. All code is publicly available at\nhttps://github.com/evidenceprime/BioASQ-13b.", "AI": {"tldr": "\u5c0f\u578b\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u5728\u751f\u7269\u533b\u5b66\u95ee\u7b54\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u751a\u81f3\u4f18\u4e8e\u95ed\u6e90\u6a21\u578b\u3002", "motivation": "\u8bc4\u4f30\u5c0f\u578b\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u5728\u751f\u7269\u533b\u5b66\u95ee\u7b54\u9886\u57df\u7684\u66ff\u4ee3\u80fd\u529b\uff0c\u5e76\u4e0e\u95ed\u6e90\u6a21\u578b\u8fdb\u884c\u6bd4\u8f83\u3002", "method": "\u5728BioASQ\u6311\u6218\u8d5b\u4e2d\u4f7f\u7528\u68c0\u7d22\u589e\u5f3a\u3001\u4e0a\u4e0b\u6587\u5b66\u4e60\u548c\u96c6\u6210\u7b49\u6280\u672f\uff0c\u5bf9\u6bd4\u4e86\u591a\u4e2a\u5f00\u6e90\u6a21\u578b\u548cGPT-4o\u3001Claude 3.5/3.7\u7b49\u95ed\u6e90\u6a21\u578b\u3002", "result": "\u5f00\u6e90\u6a21\u578b\u5728\u751f\u7269\u533b\u5b66\u95ee\u7b54\u4efb\u52a1\u4e0a\u4e0e\u95ed\u6e90\u6a21\u578b\u76f8\u5f53\uff0c\u96c6\u6210\u7b56\u7565\u4e0b\u751a\u81f3\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u5c0f\u578b\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u8db3\u4ee5\u80dc\u4efb\u751f\u7269\u533b\u5b66\u95ee\u7b54\u4efb\u52a1\uff0c\u5e76\u4e14\u5728\u7279\u5b9a\u7b56\u7565\u4e0b\u80fd\u8d85\u8d8a\u95ed\u6e90\u6a21\u578b\u3002"}}
{"id": "2509.18954", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18954", "abs": "https://arxiv.org/abs/2509.18954", "authors": ["Minoo Dolatabadi", "Fardin Ayar", "Ehsan Javanmardi", "Manabu Tsukada", "Mahdi Javanmardi"], "title": "Towards Robust LiDAR Localization: Deep Learning-based Uncertainty Estimation", "comment": null, "summary": "LiDAR-based localization and SLAM often rely on iterative matching\nalgorithms, particularly the Iterative Closest Point (ICP) algorithm, to align\nsensor data with pre-existing maps or previous scans. However, ICP is prone to\nerrors in featureless environments and dynamic scenes, leading to inaccurate\npose estimation. Accurately predicting the uncertainty associated with ICP is\ncrucial for robust state estimation but remains challenging, as existing\napproaches often rely on handcrafted models or simplified assumptions.\nMoreover, a few deep learning-based methods for localizability estimation\neither depend on a pre-built map, which may not always be available, or provide\na binary classification of localizable versus non-localizable, which fails to\nproperly model uncertainty. In this work, we propose a data-driven framework\nthat leverages deep learning to estimate the registration error covariance of\nICP before matching, even in the absence of a reference map. By associating\neach LiDAR scan with a reliable 6-DoF error covariance estimate, our method\nenables seamless integration of ICP within Kalman filtering, enhancing\nlocalization accuracy and robustness. Extensive experiments on the KITTI\ndataset demonstrate the effectiveness of our approach, showing that it\naccurately predicts covariance and, when applied to localization using a\npre-built map or SLAM, reduces localization errors and improves robustness.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728ICP\u5339\u914d\u524d\u9884\u6d4b\u6fc0\u5149\u96f7\u8fbe\u626b\u63cf\u7684\u6ce8\u518c\u8bef\u5dee\u534f\u65b9\u5dee\uff0c\u5373\u4f7f\u5728\u6ca1\u6709\u53c2\u8003\u5730\u56fe\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u63d0\u9ad8\u5b9a\u4f4d\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u4f20\u7edf\u7684\u57fa\u4e8eICP\u7684\u6fc0\u5149\u96f7\u8fbe\u5b9a\u4f4d\u548cSLAM\u65b9\u6cd5\u5728\u7279\u5f81\u7a00\u758f\u6216\u52a8\u6001\u73af\u5883\u4e2d\u5bb9\u6613\u51fa\u73b0\u59ff\u6001\u4f30\u8ba1\u4e0d\u51c6\u786e\u7684\u95ee\u9898\uff0c\u5e76\u4e14\u51c6\u786e\u9884\u6d4bICP\u7684\u8bef\u5dee\u534f\u65b9\u5dee\u5177\u6709\u6311\u6218\u6027\u3002\u73b0\u6709\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u9884\u5efa\u5730\u56fe\uff0c\u8981\u4e48\u53ea\u63d0\u4f9b\u4e8c\u5143\u5206\u7c7b\uff0c\u65e0\u6cd5\u5145\u5206\u5efa\u6a21\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5728ICP\u5339\u914d\u4e4b\u524d\u4f30\u8ba1ICP\u7684\u6ce8\u518c\u8bef\u5dee\u534f\u65b9\u5dee\uff0c\u5373\u4f7f\u5728\u6ca1\u6709\u53c2\u8003\u5730\u56fe\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u8fdb\u884c\u4f30\u8ba1\u3002", "result": "\u5728KITTI\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u51c6\u786e\u9884\u6d4b\u534f\u65b9\u5dee\uff0c\u5e76\u5728\u4f7f\u7528\u9884\u5efa\u5730\u56fe\u8fdb\u884c\u5b9a\u4f4d\u6216SLAM\u65f6\uff0c\u80fd\u663e\u8457\u964d\u4f4e\u5b9a\u4f4d\u8bef\u5dee\u5e76\u63d0\u9ad8\u9c81\u68d2\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6846\u67b6\u901a\u8fc7\u63d0\u4f9b\u53ef\u9760\u76846-DoF\u8bef\u5dee\u534f\u65b9\u5dee\u4f30\u8ba1\uff0c\u80fd\u591f\u6709\u6548\u5730\u5c06ICP\u96c6\u6210\u5230\u5361\u5c14\u66fc\u6ee4\u6ce2\u4e2d\uff0c\u4ece\u800c\u63d0\u9ad8\u6fc0\u5149\u96f7\u8fbe\u5b9a\u4f4d\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2509.18546", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18546", "abs": "https://arxiv.org/abs/2509.18546", "authors": ["Yujia Liu", "Dingquan Li", "Tiejun Huang"], "title": "SEGA: A Transferable Signed Ensemble Gaussian Black-Box Attack against No-Reference Image Quality Assessment Models", "comment": null, "summary": "No-Reference Image Quality Assessment (NR-IQA) models play an important role\nin various real-world applications. Recently, adversarial attacks against\nNR-IQA models have attracted increasing attention, as they provide valuable\ninsights for revealing model vulnerabilities and guiding robust system design.\nSome effective attacks have been proposed against NR-IQA models in white-box\nsettings, where the attacker has full access to the target model. However,\nthese attacks often suffer from poor transferability to unknown target models\nin more realistic black-box scenarios, where the target model is inaccessible.\nThis work makes the first attempt to address the challenge of low\ntransferability in attacking NR-IQA models by proposing a transferable Signed\nEnsemble Gaussian black-box Attack (SEGA). The main idea is to approximate the\ngradient of the target model by applying Gaussian smoothing to source models\nand ensembling their smoothed gradients. To ensure the imperceptibility of\nadversarial perturbations, SEGA further removes inappropriate perturbations\nusing a specially designed perturbation filter mask. Experimental results on\nthe CLIVE dataset demonstrate the superior transferability of SEGA, validating\nits effectiveness in enabling successful transfer-based black-box attacks\nagainst NR-IQA models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSEGA\u7684\u53ef\u8fc1\u79fb\u9ad8\u65af\u9ed1\u76d2\u653b\u51fb\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u73b0\u6709\u9488\u5bf9\u56fe\u50cf\u8d28\u91cf\u8bc4\u4ef7\uff08NR-IQA\uff09\u6a21\u578b\u7684\u9ed1\u76d2\u653b\u51fb\u4e2d\u5b58\u5728\u7684\u8fc1\u79fb\u6027\u5dee\u7684\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u9488\u5bf9NR-IQA\u6a21\u578b\u7684\u9ed1\u76d2\u653b\u51fb\u65b9\u6cd5\u5b58\u5728\u8fc1\u79fb\u6027\u5dee\u7684\u95ee\u9898\uff0c\u96be\u4ee5\u653b\u51fb\u672a\u77e5\u7684\u76ee\u6807\u6a21\u578b\uff0c\u56e0\u6b64\u9700\u8981\u63d0\u51fa\u4e00\u79cd\u53ef\u8fc1\u79fb\u6027\u5f3a\u7684\u653b\u51fb\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u8fc1\u79fb\u7684\u7b26\u53f7\u96c6\u6210\u9ad8\u65af\u9ed1\u76d2\u653b\u51fb\uff08SEGA\uff09\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5bf9\u6e90\u6a21\u578b\u7684\u68af\u5ea6\u8fdb\u884c\u9ad8\u65af\u5e73\u6ed1\u5e76\u96c6\u6210\u5176\u5e73\u6ed1\u68af\u5ea6\u6765\u8fd1\u4f3c\u76ee\u6807\u6a21\u578b\u7684\u68af\u5ea6\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7279\u6b8a\u7684\u6270\u52a8\u6ee4\u6ce2\u5668\u6765\u53bb\u9664\u4e0d\u9002\u5b9c\u7684\u6270\u52a8\uff0c\u4ee5\u4fdd\u8bc1\u5bf9\u6297\u6027\u6270\u52a8\u7684\u4e0d\u53ef\u611f\u77e5\u6027\u3002", "result": "\u5728CLIVE\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSEGA\u5177\u6709\u4f18\u8d8a\u7684\u53ef\u8fc1\u79fb\u6027\uff0c\u80fd\u591f\u6210\u529f\u5b9e\u73b0\u57fa\u4e8e\u8fc1\u79fb\u7684\u9ed1\u76d2\u653b\u51fbNR-IQA\u6a21\u578b\u3002", "conclusion": "SEGA\u662f\u4e00\u79cd\u6709\u6548\u4e14\u53ef\u8fc1\u79fb\u7684\u9ed1\u76d2\u653b\u51fb\u65b9\u6cd5\uff0c\u80fd\u591f\u6210\u529f\u653b\u51fbNR-IQA\u6a21\u578b\uff0c\u5e76\u4e3a\u8bbe\u8ba1\u66f4\u9c81\u68d2\u7684NR-IQA\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2509.19179", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2509.19179", "abs": "https://arxiv.org/abs/2509.19179", "authors": ["Vincent Halde", "Olivier Bernard", "Mathieu Brochu", "Laurier Dufresne", "Nicolas Fleury", "Kayla Johnson", "Benjamin Moffet", "David Roy-Guay"], "title": "Who Let the Diamonds Out?", "comment": "8 pages, 16 figures", "summary": "Nitrogen-Vacancy (NV) center magnetometry is a highly promising quantum\nsensing technology, with early prototypes demonstrating impressive sensitivity\nin compact sensing heads. Yet, most existing implementations remain tied to\nlaboratory setups, lacking the portability and environmental robustness needed\nto unlock their full potential in real-world applications. In this work, we\nintroduce a fully portable, hand-held NV-based magnetometer that delivers a\nvector sensitivity of approximately 400 pT/sqrt(Hz), heading errors below 5 nT\nin Earth's field, and a wide signal bandwidth that supports on-field\nrecalibration and operation on moving platforms. We further demonstrate the\nsystem's technological maturity through environmental qualification such as\nthermal, vibration, radiation and other operational stresses related to\ndeployment in low Earth orbit, and through successful deployments in demanding\nscenarios, including northern Canadian weather conditions, drone-mounted\nsurveys and high-altitude balloon flights. Together, these achievements\nestablish this NV-based magnetometer as a robust, versatile tool ready to bring\nquantum sensing performance to a broad range of field and autonomous\napplications.", "AI": {"tldr": "\u8fd9\u662f\u4e00\u4e2a\u624b\u6301\u5f0f\u91d1\u521a\u77f3\u6c2e-\u7a7a\u4f4d\uff08NV\uff09\u8d28\u8c31\u4eea\uff0c\u53ef\u5728\u5730\u7403\u78c1\u573a\u548c\u6076\u52a3\u73af\u5883\u4e0b\u8fd0\u884c\uff0c\u5177\u6709\u51fa\u8272\u7684\u4f20\u611f\u5668\u6027\u80fd\u3002", "motivation": "\u5927\u591a\u6570\u73b0\u6709\u7684NV\u4e2d\u5fc3\u78c1\u529b\u8ba1\u90fd\u5c40\u9650\u4e8e\u5b9e\u9a8c\u5ba4\u8bbe\u7f6e\uff0c\u7f3a\u4e4f\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5b9e\u73b0\u5176\u5168\u90e8\u6f5c\u529b\u7684\u4fbf\u643a\u6027\u548c\u73af\u5883\u9c81\u68d2\u6027\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u5168\u4fbf\u643a\u3001\u624b\u6301\u5f0fNV\u78c1\u529b\u8ba1\uff0c\u53ef\u63d0\u4f9b\u7ea6400pT/sqrt(Hz)\u7684\u77e2\u91cf\u7075\u654f\u5ea6\u3001\u5730\u7403\u78c1\u573a\u4e2d\u4f4e\u4e8e5nT\u7684\u822a\u5411\u8bef\u5dee\u4ee5\u53ca\u652f\u6301\u73b0\u573a\u91cd\u65b0\u6821\u51c6\u548c\u5728\u79fb\u52a8\u5e73\u53f0\u4e0a\u8fd0\u884c\u7684\u5bbd\u4fe1\u53f7\u5e26\u5bbd\u3002\u8be5\u7cfb\u7edf\u5df2\u901a\u8fc7\u73af\u5883\u8d44\u683c\u8ba4\u8bc1\uff08\u70ed\u3001\u632f\u52a8\u3001\u8f90\u5c04\u548c\u4f4e\u5730\u7403\u8f68\u9053\u90e8\u7f72\u76f8\u5173\u7684\u5176\u4ed6\u8fd0\u884c\u538b\u529b\uff09\uff0c\u5e76\u6210\u529f\u90e8\u7f72\u5728\u5317\u52a0\u62ff\u5927\u3001\u65e0\u4eba\u673a\u548c\u9ad8\u7a7a\u5e73\u6d41\u5c42\u6c14\u7403\u7684\u6076\u52a3\u73af\u5883\u4e2d\u3002", "result": "\u8be5\u8bbe\u5907\u53ef\u5728\u5730\u7403\u78c1\u573a\u548c\u6076\u52a3\u73af\u5883\u4e0b\u8fd0\u884c\uff0c\u4f20\u611f\u5668\u6027\u80fd\u51fa\u8272\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4f7fNV\u78c1\u529b\u8ba1\u6210\u4e3a\u4e00\u79cd\u5f3a\u5927\u7684\u3001\u591a\u529f\u80fd\u7684\u5de5\u5177\uff0c\u53ef\u4e3a\u5404\u79cd\u73b0\u573a\u548c\u81ea\u4e3b\u5e94\u7528\u5e26\u6765\u91cf\u5b50\u4f20\u611f\u6027\u80fd\u3002"}}
{"id": "2509.18152", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18152", "abs": "https://arxiv.org/abs/2509.18152", "authors": ["Zhenyu Qi", "Qing Yu", "Jichen Wang", "Yun-Bo Zhao", "Zerui Li", "Wenjun Lv"], "title": "WLFM: A Well-Logs Foundation Model for Multi-Task and Cross-Well Geological Interpretation", "comment": null, "summary": "Well-log interpretation is fundamental for subsurface characterization but\nremains challenged by heterogeneous tool responses, noisy signals, and limited\nlabels. We propose WLFM, a foundation model pretrained on multi-curve logs from\n1200 wells, comprising three stages: tokenization of log patches into\ngeological tokens, self-supervised pretraining with masked-token modeling and\nstratigraphy-aware contrastive learning, and multi-task adaptation with\nfew-shot fine-tuning. WLFM consistently outperforms state-of-the-art baselines,\nachieving 0.0041 MSE in porosity estimation and 74.13\\% accuracy in lithology\nclassification, while WLFM-Finetune further improves to 0.0038 MSE and 78.10\\%\naccuracy. Beyond predictive accuracy, WLFM exhibits emergent layer-awareness,\nlearns a reusable geological vocabulary, and reconstructs masked curves with\nreasonable fidelity, though systematic offsets are observed in shallow and\nultra-deep intervals. Although boundary detection is not explicitly evaluated\nhere, clustering analyses suggest strong potential for future extension. These\nresults establish WLFM as a scalable, interpretable, and transferable backbone\nfor geological AI, with implications for multi-modal integration of logs,\nseismic, and textual data.", "AI": {"tldr": "WLFM\u662f\u4e00\u4e2a\u5728\u591a\u66f2\u7ebf\u6d4b\u4e95\u6570\u636e\u4e0a\u9884\u8bad\u7ec3\u7684\u57fa\u91d1\u6a21\u578b\uff0c\u7528\u4e8e\u6539\u8fdb\u5730\u4e0b\u7279\u5f81\u8868\u5f81\u3002\u5b83\u901a\u8fc7\u4e09\u4e2a\u9636\u6bb5\uff1a\u65e5\u5fd7\u5206\u5757\u3001\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u548c\u591a\u4efb\u52a1\u9002\u5e94\uff0c\u5b9e\u73b0\u4e86\u5728\u5b54\u9699\u5ea6\u4f30\u8ba1\u548c\u5ca9\u6027\u5206\u7c7b\u4e0a\u7684\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u5e76\u5c55\u73b0\u51fa\u5c42\u4f4d\u610f\u8bc6\u548c\u53ef\u590d\u7528\u8bcd\u6c47\u7b49\u65b0\u5174\u80fd\u529b\u3002", "motivation": "\u6d4b\u4e95\u89e3\u91ca\u5728\u5730\u4e0b\u8868\u5f81\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u9762\u4e34\u5de5\u5177\u54cd\u5e94\u5f02\u8d28\u6027\u3001\u4fe1\u53f7\u566a\u58f0\u548c\u6807\u7b7e\u6709\u9650\u7684\u6311\u6218\u3002", "method": "WLFM\u6a21\u578b\u5305\u542b\u4e09\u4e2a\u9636\u6bb5\uff1a\u5c06\u6d4b\u4e95\u6570\u636e\u5757\u8f6c\u6362\u4e3a\u5730\u8d28\u6807\u8bb0\uff08tokenization\uff09\u3001\u4f7f\u7528\u63a9\u7801\u6807\u8bb0\u5efa\u6a21\u548c\u5730\u5c42\u611f\u77e5\u5bf9\u6bd4\u5b66\u4e60\u8fdb\u884c\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\uff0c\u4ee5\u53ca\u901a\u8fc7\u5c11\u6837\u672c\u5fae\u8c03\u8fdb\u884c\u591a\u4efb\u52a1\u9002\u5e94\u3002", "result": "WLFM\u5728\u5b54\u9699\u5ea6\u4f30\u8ba1\u4e0a\u8fbe\u52300.0041\u7684\u5747\u65b9\u8bef\u5dee\uff08MSE\uff09\u548c\u5728\u5ca9\u6027\u5206\u7c7b\u4e0a\u8fbe\u523074.13%\u7684\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002WLFM-Finetune\u8fdb\u4e00\u6b65\u5c06MSE\u63d0\u9ad8\u52300.0038\uff0c\u51c6\u786e\u7387\u63d0\u9ad8\u523078.10%\u3002\u6b64\u5916\uff0cWLFM\u8fd8\u8868\u73b0\u51fa\u5c42\u4f4d\u610f\u8bc6\uff0c\u5b66\u4e60\u4e86\u53ef\u590d\u7528\u7684\u5730\u8d28\u8bcd\u6c47\uff0c\u5e76\u80fd\u4ee5\u5408\u7406\u7684\u4fdd\u771f\u5ea6\u91cd\u5efa\u88ab\u63a9\u7801\u7684\u66f2\u7ebf\uff0c\u5c3d\u7ba1\u5728\u6d45\u5c42\u548c\u8d85\u6df1\u5c42\u5b58\u5728\u7cfb\u7edf\u6027\u504f\u79fb\u3002", "conclusion": "WLFM\u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u3001\u53ef\u89e3\u91ca\u4e14\u53ef\u8fc1\u79fb\u7684\u5730\u8d28\u4eba\u5de5\u667a\u80fd\u9aa8\u5e72\u6a21\u578b\uff0c\u4e3a\u6574\u5408\u6d4b\u4e95\u3001\u5730\u9707\u548c\u6587\u672c\u7b49\u591a\u6a21\u6001\u6570\u636e\u63d0\u4f9b\u4e86\u6f5c\u529b\u3002"}}
{"id": "2509.18905", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18905", "abs": "https://arxiv.org/abs/2509.18905", "authors": ["Songsong Yu", "Yuxin Chen", "Hao Ju", "Lianjie Jia", "Fuxi Zhang", "Shaofei Huang", "Yuhan Wu", "Rundi Cui", "Binghao Ran", "Zaibin Zhang", "Zhedong Zheng", "Zhipeng Zhang", "Yifan Wang", "Lin Song", "Lijun Wang", "Yanwei Li", "Ying Shan", "Huchuan Lu"], "title": "How Far are VLMs from Visual Spatial Intelligence? A Benchmark-Driven Perspective", "comment": "a comprehensive visual spatial reasoning evaluation tool, 25 pages,\n  16 figures", "summary": "Visual Spatial Reasoning (VSR) is a core human cognitive ability and a\ncritical requirement for advancing embodied intelligence and autonomous\nsystems. Despite recent progress in Vision-Language Models (VLMs), achieving\nhuman-level VSR remains highly challenging due to the complexity of\nrepresenting and reasoning over three-dimensional space. In this paper, we\npresent a systematic investigation of VSR in VLMs, encompassing a review of\nexisting methodologies across input modalities, model architectures, training\nstrategies, and reasoning mechanisms. Furthermore, we categorize spatial\nintelligence into three levels of capability, ie, basic perception, spatial\nunderstanding, spatial planning, and curate SIBench, a spatial intelligence\nbenchmark encompassing nearly 20 open-source datasets across 23 task settings.\nExperiments with state-of-the-art VLMs reveal a pronounced gap between\nperception and reasoning, as models show competence in basic perceptual tasks\nbut consistently underperform in understanding and planning tasks, particularly\nin numerical estimation, multi-view reasoning, temporal dynamics, and spatial\nimagination. These findings underscore the substantial challenges that remain\nin achieving spatial intelligence, while providing both a systematic roadmap\nand a comprehensive benchmark to drive future research in the field. The\nrelated resources of this study are accessible at\nhttps://sibench.github.io/Awesome-Visual-Spatial-Reasoning/.", "AI": {"tldr": "\u5c3d\u7ba1\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5728\u89c6\u89c9\u7a7a\u95f4\u63a8\u7406\uff08VSR\uff09\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u7531\u4e8e\u4e09\u7ef4\u7a7a\u95f4\u8868\u793a\u548c\u63a8\u7406\u7684\u590d\u6742\u6027\uff0c\u5b9e\u73b0\u4eba\u7c7b\u6c34\u5e73\u7684VSR\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u672c\u7814\u7a76\u5bf9VSR\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u8c03\u67e5\uff0c\u6db5\u76d6\u4e86\u8f93\u5165\u6a21\u6001\u3001\u6a21\u578b\u67b6\u6784\u3001\u8bad\u7ec3\u7b56\u7565\u548c\u63a8\u7406\u673a\u5236\u3002\u7814\u7a76\u5c06\u7a7a\u95f4\u667a\u80fd\u5206\u4e3a\u4e09\u4e2a\u80fd\u529b\u7ea7\u522b\uff1a\u57fa\u672c\u611f\u77e5\u3001\u7a7a\u95f4\u7406\u89e3\u548c\u7a7a\u95f4\u89c4\u5212\uff0c\u5e76\u521b\u5efa\u4e86SIBench\u57fa\u51c6\uff0c\u5305\u542b\u4e8623\u4e2a\u4efb\u52a1\u8bbe\u7f6e\u7684\u8fd120\u4e2a\u5f00\u6e90\u6570\u636e\u96c6\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5f53\u524d\u6700\u5148\u8fdb\u7684VLM\u5728\u611f\u77e5\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u7406\u89e3\u548c\u89c4\u5212\u4efb\u52a1\u4e0a\uff0c\u5c24\u5176\u662f\u5728\u6570\u503c\u4f30\u8ba1\u3001\u591a\u89c6\u56fe\u63a8\u7406\u3001\u65f6\u95f4\u52a8\u6001\u548c\u7a7a\u95f4\u60f3\u8c61\u65b9\u9762\uff0c\u8868\u73b0\u4e0d\u4f73\u3002\u8fd9\u63ed\u793a\u4e86\u5b9e\u73b0\u7a7a\u95f4\u667a\u80fd\u7684\u91cd\u5927\u6311\u6218\uff0c\u5e76\u4e3a\u8be5\u9886\u57df\u672a\u6765\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u7684\u8def\u7ebf\u56fe\u548c\u5168\u9762\u7684\u57fa\u51c6\u3002", "motivation": "\u89c6\u89c9\u7a7a\u95f4\u63a8\u7406\uff08VSR\uff09\u662f\u4eba\u7c7b\u6838\u5fc3\u8ba4\u77e5\u80fd\u529b\uff0c\u5bf9\u4e8e\u53d1\u5c55\u5177\u8eab\u667a\u80fd\u548c\u81ea\u4e3b\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728VSR\u65b9\u9762\u4ecd\u6709\u5f88\u5927\u63d0\u5347\u7a7a\u95f4\u3002", "method": "\u5bf9VSR\u7684\u73b0\u6709\u65b9\u6cd5\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u8c03\u67e5\uff0c\u5305\u62ec\u8f93\u5165\u6a21\u6001\u3001\u6a21\u578b\u67b6\u6784\u3001\u8bad\u7ec3\u7b56\u7565\u548c\u63a8\u7406\u673a\u5236\u3002\u5c06\u7a7a\u95f4\u667a\u80fd\u5206\u4e3a\u4e09\u4e2a\u80fd\u529b\u7ea7\u522b\uff08\u57fa\u672c\u611f\u77e5\u3001\u7a7a\u95f4\u7406\u89e3\u3001\u7a7a\u95f4\u89c4\u5212\uff09\uff0c\u5e76\u521b\u5efa\u4e86SIBench\u57fa\u51c6\uff0c\u6574\u5408\u4e86\u8fd120\u4e2a\u5f00\u6e90\u6570\u636e\u96c6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cVLM\u5728\u57fa\u672c\u611f\u77e5\u4efb\u52a1\u4e0a\u8868\u73b0\u5c1a\u53ef\uff0c\u4f46\u5728\u7a7a\u95f4\u7406\u89e3\u548c\u7a7a\u95f4\u89c4\u5212\u4efb\u52a1\u4e0a\uff0c\u5c24\u5176\u662f\u5728\u6570\u503c\u4f30\u8ba1\u3001\u591a\u89c6\u56fe\u63a8\u7406\u3001\u65f6\u95f4\u52a8\u6001\u548c\u7a7a\u95f4\u60f3\u8c61\u65b9\u9762\uff0c\u8868\u73b0\u51fa\u663e\u8457\u7684\u4e0d\u8db3\uff0c\u5b58\u5728\u611f\u77e5\u4e0e\u63a8\u7406\u80fd\u529b\u7684\u663e\u8457\u5dee\u8ddd\u3002", "conclusion": "\u5b9e\u73b0\u7a7a\u95f4\u667a\u80fd\u4ecd\u7136\u9762\u4e34\u91cd\u5927\u6311\u6218\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u7684SIBench\u57fa\u51c6\u548c\u7cfb\u7edf\u6027\u8c03\u67e5\u4e3a\u672a\u6765VSR\u9886\u57df\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\u548c\u5de5\u5177\u3002"}}
{"id": "2509.18862", "categories": ["cs.CL", "I.2.7; I.2.1"], "pdf": "https://arxiv.org/pdf/2509.18862", "abs": "https://arxiv.org/abs/2509.18862", "authors": ["Luyan Zhang", "Xinyu Xie"], "title": "Multi-Hierarchical Feature Detection for Large Language Model Generated Text", "comment": "9 pages, 6 tables, empirical study on multi-feature AI text detection", "summary": "With the rapid advancement of large language model technology, there is\ngrowing interest in whether multi-feature approaches can significantly improve\nAI text detection beyond what single neural models achieve. While intuition\nsuggests that combining semantic, syntactic, and statistical features should\nprovide complementary signals, this assumption has not been rigorously tested\nwith modern LLM-generated text. This paper provides a systematic empirical\ninvestigation of multi-hierarchical feature integration for AI text detection,\nspecifically testing whether the computational overhead of combining multiple\nfeature types is justified by performance gains. We implement MHFD\n(Multi-Hierarchical Feature Detection), integrating DeBERTa-based semantic\nanalysis, syntactic parsing, and statistical probability features through\nadaptive fusion. Our investigation reveals important negative results: despite\ntheoretical expectations, multi-feature integration provides minimal benefits\n(0.4-0.5% improvement) while incurring substantial computational costs (4.2x\noverhead), suggesting that modern neural language models may already capture\nmost relevant detection signals efficiently. Experimental results on multiple\nbenchmark datasets demonstrate that the MHFD method achieves 89.7% accuracy in\nin-domain detection and maintains 84.2% stable performance in cross-domain\ndetection, showing modest improvements of 0.4-2.6% over existing methods.", "AI": {"tldr": "\u591a\u7279\u5f81\u878d\u5408\u5728AI\u6587\u672c\u68c0\u6d4b\u4e2d\u63d0\u5347\u6709\u9650\uff0c\u5e76\u589e\u52a0\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u63a2\u7a76\u591a\u7279\u5f81\u878d\u5408\u65b9\u6cd5\u80fd\u5426\u663e\u8457\u63d0\u5347AI\u6587\u672c\u68c0\u6d4b\u80fd\u529b\uff0c\u8d85\u8d8a\u5355\u4e00\u6a21\u578b\u3002", "method": "\u5b9e\u73b0\u4e86\u4e00\u4e2a\u540d\u4e3aMHFD\uff08Multi-Hierarchical Feature Detection\uff09\u7684\u65b9\u6cd5\uff0c\u878d\u5408\u4e86\u57fa\u4e8eDeBERTa\u7684\u8bed\u4e49\u5206\u6790\u3001\u53e5\u6cd5\u5206\u6790\u548c\u7edf\u8ba1\u6982\u7387\u7279\u5f81\uff0c\u5e76\u4f7f\u7528\u4e86\u81ea\u9002\u5e94\u878d\u5408\u6280\u672f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u591a\u7279\u5f81\u878d\u5408\u4ec5\u5e26\u67650.4-0.5%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u4f46\u8ba1\u7b97\u5f00\u9500\u5374\u589e\u52a0\u4e864.2\u500d\u3002MHFD\u5728\u9886\u57df\u5185\u68c0\u6d4b\u51c6\u786e\u7387\u4e3a89.7%\uff0c\u8de8\u9886\u57df\u68c0\u6d4b\u7a33\u5b9a\u6027\u80fd\u4e3a84.2%\uff0c\u76f8\u8f83\u4e8e\u73b0\u6709\u65b9\u6cd5\u4ec5\u63d0\u53470.4-2.6%\u3002", "conclusion": "\u591a\u7279\u5f81\u878d\u5408\u5bf9\u73b0\u4ee3LLM\u751f\u6210\u6587\u672c\u7684AI\u68c0\u6d4b\u63d0\u5347\u6709\u9650\uff0c\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u8868\u660e\u73b0\u6709\u7684\u795e\u7ecf\u8bed\u8a00\u6a21\u578b\u53ef\u80fd\u5df2\u7ecf\u9ad8\u6548\u5730\u6355\u6349\u4e86\u5927\u90e8\u5206\u76f8\u5173\u68c0\u6d4b\u4fe1\u53f7\u3002"}}
{"id": "2509.18979", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18979", "abs": "https://arxiv.org/abs/2509.18979", "authors": ["Lorenzo Shaikewitz", "Tim Nguyen", "Luca Carlone"], "title": "Category-Level Object Shape and Pose Estimation in Less Than a Millisecond", "comment": null, "summary": "Object shape and pose estimation is a foundational robotics problem,\nsupporting tasks from manipulation to scene understanding and navigation. We\npresent a fast local solver for shape and pose estimation which requires only\ncategory-level object priors and admits an efficient certificate of global\noptimality. Given an RGB-D image of an object, we use a learned front-end to\ndetect sparse, category-level semantic keypoints on the target object. We\nrepresent the target object's unknown shape using a linear active shape model\nand pose a maximum a posteriori optimization problem to solve for position,\norientation, and shape simultaneously. Expressed in unit quaternions, this\nproblem admits first-order optimality conditions in the form of an eigenvalue\nproblem with eigenvector nonlinearities. Our primary contribution is to solve\nthis problem efficiently with self-consistent field iteration, which only\nrequires computing a 4-by-4 matrix and finding its minimum eigenvalue-vector\npair at each iterate. Solving a linear system for the corresponding Lagrange\nmultipliers gives a simple global optimality certificate. One iteration of our\nsolver runs in about 100 microseconds, enabling fast outlier rejection. We test\nour method on synthetic data and a variety of real-world settings, including\ntwo public datasets and a drone tracking scenario. Code is released at\nhttps://github.com/MIT-SPARK/Fast-ShapeAndPose.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5feb\u901f\u7684\u3001\u53ef\u83b7\u5f97\u5168\u5c40\u6700\u4f18\u6027\u8bc1\u4e66\u7684\u7269\u4f53\u4e09\u7ef4\u5f62\u72b6\u548c\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\uff0c\u4ec5\u9700\u7c7b\u522b\u7ea7\u7269\u4f53\u5148\u9a8c\u77e5\u8bc6\u3002", "motivation": "\u7269\u4f53\u4e09\u7ef4\u5f62\u72b6\u548c\u59ff\u6001\u4f30\u8ba1\u662f\u673a\u5668\u4eba\u9886\u57df\u7684\u57fa\u7840\u95ee\u9898\uff0c\u652f\u6491\u7740\u6293\u53d6\u3001\u573a\u666f\u7406\u89e3\u548c\u5bfc\u822a\u7b49\u4efb\u52a1\u3002", "method": "\u8be5\u65b9\u6cd5\u5229\u7528\u5b66\u4e60\u5230\u7684\u524d\u7aef\u68c0\u6d4b\u7a00\u758f\u7684\u3001\u7c7b\u522b\u7ea7\u8bed\u4e49\u5173\u952e\u70b9\uff0c\u5e76\u4f7f\u7528\u7ebf\u6027\u6d3b\u52a8\u5f62\u72b6\u6a21\u578b\u8868\u793a\u7269\u4f53\u5f62\u72b6\u3002\u901a\u8fc7\u6700\u5927\u540e\u9a8c\u6982\u7387\u4f18\u5316\u95ee\u9898\u540c\u65f6\u6c42\u89e3\u4f4d\u7f6e\u3001\u65b9\u5411\u548c\u5f62\u72b6\u3002\u8be5\u4f18\u5316\u95ee\u9898\u53ef\u4ee5\u8f6c\u5316\u4e3a\u7279\u5f81\u503c\u95ee\u9898\uff0c\u5e76\u4f7f\u7528\u81ea\u6d3d\u573a\u8fed\u4ee3\uff08self-consistent field iteration\uff09\u8fdb\u884c\u9ad8\u6548\u6c42\u89e3\uff0c\u6bcf\u6b21\u8fed\u4ee3\u4ec5\u9700\u8ba1\u7b97\u4e00\u4e2a4x4\u77e9\u9635\u5e76\u627e\u5230\u5176\u6700\u5c0f\u7279\u5f81\u503c-\u5411\u91cf\u5bf9\u3002\u5bf9\u5e94\u7684\u62c9\u683c\u6717\u65e5\u4e58\u5b50\u53ef\u4ee5\u901a\u8fc7\u6c42\u89e3\u7ebf\u6027\u7cfb\u7edf\u5f97\u5230\uff0c\u4e3a\u5168\u5c40\u6700\u4f18\u6027\u63d0\u4f9b\u4e86\u7b80\u5355\u7684\u8bc1\u4e66\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u5408\u6210\u6570\u636e\u548c\u591a\u79cd\u771f\u5b9e\u4e16\u754c\u573a\u666f\uff08\u5305\u62ec\u4e24\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u548c\u4e00\u4e2a\u65e0\u4eba\u673a\u8ddf\u8e2a\u573a\u666f\uff09\u4e0a\u8fdb\u884c\u4e86\u6d4b\u8bd5\u3002\u5355\u6b21\u8fed\u4ee3\u8017\u65f6\u7ea6100\u5fae\u79d2\uff0c\u80fd\u591f\u5b9e\u73b0\u5feb\u901f\u7684\u5f02\u5e38\u503c\u5254\u9664\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u9ad8\u6548\u5730\u4f30\u8ba1\u7269\u4f53\u7684\u4e09\u7ef4\u5f62\u72b6\u548c\u59ff\u6001\uff0c\u5e76\u63d0\u4f9b\u5168\u5c40\u6700\u4f18\u6027\u8bc1\u4e66\uff0c\u9002\u7528\u4e8e\u673a\u5668\u4eba\u9886\u57df\u7684\u591a\u79cd\u4efb\u52a1\u3002"}}
{"id": "2509.18550", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18550", "abs": "https://arxiv.org/abs/2509.18550", "authors": ["Mohammad Junayed Hasan", "Nabeel Mohammed", "Shafin Rahman", "Philipp Koehn"], "title": "HadaSmileNet: Hadamard fusion of handcrafted and deep-learning features for enhancing facial emotion recognition of genuine smiles", "comment": "Accepted to IEEE International Conference on Data Mining (ICDM) 2025.\n  Final version to appear in the conference proceedings", "summary": "The distinction between genuine and posed emotions represents a fundamental\npattern recognition challenge with significant implications for data mining\napplications in social sciences, healthcare, and human-computer interaction.\nWhile recent multi-task learning frameworks have shown promise in combining\ndeep learning architectures with handcrafted D-Marker features for smile facial\nemotion recognition, these approaches exhibit computational inefficiencies due\nto auxiliary task supervision and complex loss balancing requirements. This\npaper introduces HadaSmileNet, a novel feature fusion framework that directly\nintegrates transformer-based representations with physiologically grounded\nD-Markers through parameter-free multiplicative interactions. Through\nsystematic evaluation of 15 fusion strategies, we demonstrate that Hadamard\nmultiplicative fusion achieves optimal performance by enabling direct feature\ninteractions while maintaining computational efficiency. The proposed approach\nestablishes new state-of-the-art results for deep learning methods across four\nbenchmark datasets: UvA-NEMO (88.7 percent, +0.8), MMI (99.7 percent), SPOS\n(98.5 percent, +0.7), and BBC (100 percent, +5.0). Comprehensive computational\nanalysis reveals 26 percent parameter reduction and simplified training\ncompared to multi-task alternatives, while feature visualization demonstrates\nenhanced discriminative power through direct domain knowledge integration. The\nframework's efficiency and effectiveness make it particularly suitable for\npractical deployment in multimedia data mining applications that require\nreal-time affective computing capabilities.", "AI": {"tldr": "HadaSmileNet\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u7279\u5f81\u878d\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u53c2\u6570\u65e0\u5173\u7684\u4e58\u6cd5\u4ea4\u4e92\u5c06\u57fa\u4e8eTransformer\u7684\u8868\u5f81\u4e0e\u751f\u7406\u5b66D-Markers\u76f4\u63a5\u96c6\u6210\uff0c\u5b9e\u73b0\u4e86\u8ba1\u7b97\u6548\u7387\u548c\u6027\u80fd\u7684\u6700\u4f18\u3002", "motivation": "\u533a\u5206\u771f\u5b9e\u548c\u5047\u7b11\u8868\u60c5\u5177\u6709\u91cd\u8981\u7684\u5e94\u7528\u4ef7\u503c\uff0c\u4f46\u73b0\u6709\u7684\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\u5b58\u5728\u8ba1\u7b97\u6548\u7387\u4f4e\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faHadaSmileNet\u6846\u67b6\uff0c\u901a\u8fc7\u53c2\u6570\u65e0\u5173\u7684\u4e58\u6cd5\u4ea4\u4e92\uff08\u7279\u522b\u662fHadamard\u4e58\u79ef\uff09\u76f4\u63a5\u878d\u5408Transformer\u8868\u5f81\u548cD-Markers\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u7ed3\u679c\uff08UvA-NEMO, MMI, SPOS, BBC\uff09\uff0c\u53c2\u6570\u51cf\u5c1126%\uff0c\u8bad\u7ec3\u7b80\u5316\uff0c\u5e76\u589e\u5f3a\u4e86\u5224\u522b\u80fd\u529b\u3002", "conclusion": "HadaSmileNet\u5728\u6548\u7387\u548c\u6709\u6548\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u9002\u5408\u9700\u8981\u5b9e\u65f6\u60c5\u611f\u8ba1\u7b97\u7684\u591a\u5a92\u4f53\u6570\u636e\u6316\u6398\u5e94\u7528\u3002"}}
{"id": "2509.19195", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2509.19195", "abs": "https://arxiv.org/abs/2509.19195", "authors": ["William Kirby", "Yizhi Shen", "Daan Camps", "Anirban Chowdhury", "Katherine Klymko", "Roel Van Beeumen"], "title": "Quantum Krylov Algorithm for Szeg\u00f6 Quadrature", "comment": "19 pages, 7 figures", "summary": "We present a quantum algorithm to evaluate matrix elements of functions of\nunitary operators. The method is based on calculating quadrature nodes and\nweights using data collected from a quantum processor. Given a unitary $U$ and\nquantum states $|\\psi_0\\rangle$, $|\\psi_1\\rangle$, the resulting quadrature\nrules form a functional that can then be used to classically approximate\n$\\langle\\psi_1|f(U)|\\psi_0\\rangle$ for any function $f$. In particular, the\nalgorithm calculates Szeg\\\"o quadrature rules, which, when $f$ is a Laurent\npolynomial, have the optimal relation between degree of $f$ and number of\ndistinct quantum circuits required. The unitary operator $U$ could approximate\na time evolution, opening the door to applications like estimating properties\nof Hamiltonian spectra and Gibbs states, but more generally could be any\noperator implementable via a quantum circuit. We expect this algorithm to be\nuseful as a subroutine in other quantum algorithms, much like quantum signal\nprocessing or the quantum eigenvalue transformation of unitaries. Key\nadvantages of our algorithm are that it does not require approximating $f$\ndirectly, via a series expansion or in any other way, and once the output\nfunctional has been constructed using the quantum algorithm, it can be applied\nto any $f$ classically after the fact.", "AI": {"tldr": "We present a quantum algorithm for evaluating matrix elements of functions of unitary operators using quantum processor data to construct quadrature rules. This method allows for classical approximation of $\\langle\\psi_1|f(U)|\\psi_0\\rangle$ and offers advantages like not requiring direct approximation of $f$ and post-computation flexibility.", "motivation": "The motivation is to develop a quantum algorithm to evaluate matrix elements of functions of unitary operators, enabling applications in estimating Hamiltonian spectra and Gibbs states, and serving as a subroutine in other quantum algorithms.", "method": "The algorithm calculates quadrature nodes and weights using data from a quantum processor. Specifically, it computes Szeg\u00f6 quadrature rules, which form a functional for classically approximating $\\langle\\psi_1|f(U)|\\psi_0\\rangle$ for any function $f$.", "result": "The algorithm computes Szeg\u00f6 quadrature rules that relate the degree of $f$ to the number of quantum circuits required. This functional can be classically applied to any $f$ after construction.", "conclusion": "The proposed quantum algorithm provides an efficient method for evaluating matrix elements of functions of unitary operators, with key advantages including avoiding direct approximation of $f$ and offering classical post-computation flexibility for arbitrary functions."}}
{"id": "2509.18153", "categories": ["cs.LG", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2509.18153", "abs": "https://arxiv.org/abs/2509.18153", "authors": ["Hanqun Cao", "Marcelo D. T. Torres", "Jingjie Zhang", "Zijun Gao", "Fang Wu", "Chunbin Gu", "Jure Leskovec", "Yejin Choi", "Cesar de la Fuente-Nunez", "Guangyong Chen", "Pheng-Ann Heng"], "title": "A deep reinforcement learning platform for antibiotic discovery", "comment": "42 pages, 16 figures", "summary": "Antimicrobial resistance (AMR) is projected to cause up to 10 million deaths\nannually by 2050, underscoring the urgent need for new antibiotics. Here we\npresent ApexAmphion, a deep-learning framework for de novo design of\nantibiotics that couples a 6.4-billion-parameter protein language model with\nreinforcement learning. The model is first fine-tuned on curated peptide data\nto capture antimicrobial sequence regularities, then optimised with proximal\npolicy optimization against a composite reward that combines predictions from a\nlearned minimum inhibitory concentration (MIC) classifier with differentiable\nphysicochemical objectives. In vitro evaluation of 100 designed peptides showed\nlow MIC values (nanomolar range in some cases) for all candidates (100% hit\nrate). Moreover, 99 our of 100 compounds exhibited broad-spectrum antimicrobial\nactivity against at least two clinically relevant bacteria. The lead molecules\nkilled bacteria primarily by potently targeting the cytoplasmic membrane. By\nunifying generation, scoring and multi-objective optimization with deep\nreinforcement learning in a single pipeline, our approach rapidly produces\ndiverse, potent candidates, offering a scalable route to peptide antibiotics\nand a platform for iterative steering toward potency and developability within\nhours.", "AI": {"tldr": "ApexAmphion\u662f\u4e00\u4e2a\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u5934\u8bbe\u8ba1\u6297\u751f\u7d20\uff0c\u901a\u8fc7\u7ed3\u5408\u5927\u578b\u86cb\u767d\u8d28\u8bed\u8a00\u6a21\u578b\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u5b9e\u73b0\u4e86\u5feb\u901f\u3001\u591a\u6837\u5316\u548c\u6709\u6548\u7684\u6297\u751f\u7d20\u53d1\u73b0\u3002", "motivation": "\u6297\u751f\u7d20\u8010\u836f\u6027\uff08AMR\uff09\u9884\u8ba1\u52302050\u5e74\u5c06\u5bfc\u81f4\u6bcf\u5e74\u9ad8\u8fbe1000\u4e07\u4eba\u6b7b\u4ea1\uff0c\u56e0\u6b64\u8feb\u5207\u9700\u8981\u65b0\u7684\u6297\u751f\u7d20\u3002", "method": "\u8be5\u6846\u67b6\u9996\u5148\u5728\u7cbe\u9009\u7684\u80bd\u6570\u636e\u4e0a\u8fdb\u884c\u5fae\u8c03\uff0c\u4ee5\u6355\u6349\u6297\u83cc\u5e8f\u5217\u7684\u89c4\u5f8b\u6027\uff0c\u7136\u540e\u901a\u8fc7\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\u8fdb\u884c\u4f18\u5316\uff0c\u7ed3\u5408\u4e86\u5b66\u4e60\u5230\u7684\u6700\u4f4e\u6291\u83cc\u6d53\u5ea6\uff08MIC\uff09\u5206\u7c7b\u5668\u7684\u9884\u6d4b\u548c\u53ef\u5fae\u5206\u7684\u7406\u5316\u76ee\u6807\u3002", "result": "\u5728\u4f53\u5916\u5bf9100\u79cd\u8bbe\u8ba1\u7684\u80bd\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u6240\u6709\u5019\u9009\u80bd\u5747\u663e\u793a\u51fa\u4f4eMIC\u503c\uff08\u67d0\u4e9b\u60c5\u51b5\u4e0b\u7684\u7eb3\u6469\u5c14\u8303\u56f4\uff09\uff0c\u547d\u4e2d\u7387\u4e3a100%\u3002\u5176\u4e2d99\u79cd\u80bd\u5bf9\u81f3\u5c11\u4e24\u79cd\u4e34\u5e8a\u76f8\u5173\u7ec6\u83cc\u8868\u73b0\u51fa\u5e7f\u8c31\u6297\u83cc\u6d3b\u6027\uff0c\u4e3b\u8981\u901a\u8fc7\u9776\u5411\u7ec6\u80de\u8d28\u819c\u6765\u6740\u706d\u7ec6\u83cc\u3002", "conclusion": "\u901a\u8fc7\u5c06\u751f\u6210\u3001\u8bc4\u5206\u548c\u591a\u76ee\u6807\u4f18\u5316\u4e0e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7edf\u4e00\u5728\u5355\u4e2a\u6d41\u7a0b\u4e2d\uff0c\u8be5\u65b9\u6cd5\u80fd\u5728\u6570\u5c0f\u65f6\u5185\u5feb\u901f\u751f\u6210\u591a\u6837\u5316\u3001\u6709\u6548\u7684\u5019\u9009\u836f\u7269\uff0c\u4e3a\u80bd\u7c7b\u6297\u751f\u7d20\u7684\u53d1\u73b0\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u9014\u5f84\uff0c\u5e76\u53ef\u7528\u4e8e\u8fed\u4ee3\u4f18\u5316\u4ee5\u63d0\u9ad8\u836f\u6548\u548c\u53ef\u5f00\u53d1\u6027\u3002"}}
{"id": "2509.18942", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18942", "abs": "https://arxiv.org/abs/2509.18942", "authors": ["Xiao Han", "Zimo Zhao", "Wanyu Wang", "Maolin Wang", "Zitao Liu", "Yi Chang", "Xiangyu Zhao"], "title": "Data Efficient Adaptation in Large Language Models via Continuous Low-Rank Fine-Tuning", "comment": null, "summary": "Recent advancements in Large Language Models (LLMs) have emphasized the\ncritical role of fine-tuning (FT) techniques in adapting LLMs to specific\ntasks, especially when retraining from scratch is computationally infeasible.\nFine-tuning enables LLMs to leverage task- or domain-specific data, producing\nmodels that more effectively meet the requirements of targeted applications.\nHowever, con- ventional FT approaches often suffer from catastrophic forgetting\nand suboptimal data efficiency, limiting their real-world applicability. To\naddress these challenges, this paper proposes DEAL, a novel framework that\nintegrates Low-Rank Adapta- tion (LoRA) with a continuous fine-tuning strategy.\nBy incorporating knowledge retention and adaptive parameter update modules, the\nframework mitigates the lim- itations of existing FT methods while maintaining\nefficiency in privacy-preserving settings. Experiments on 15 diverse datasets\nshow that DEAL consistently outper- forms baseline methods, yielding\nsubstantial gains in task accuracy and resource efficiency. These findings\ndemonstrate the potential of our approach to advance continual adaptation in\nLLMs by enhancing task performance while improving resource efficiency.", "AI": {"tldr": "DEAL\u6846\u67b6\u7ed3\u5408LoRA\u548c\u8fde\u7eed\u5fae\u8c03\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5fae\u8c03\u65b9\u6cd5\u7684\u707e\u96be\u6027\u9057\u5fd8\u548c\u6570\u636e\u6548\u7387\u4f4e\u4e0b\u95ee\u9898\uff0c\u572815\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u4efb\u52a1\u51c6\u786e\u6027\u548c\u8d44\u6e90\u6548\u7387\u3002", "motivation": "\u5fae\u8c03\uff08FT\uff09\u662f\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u9002\u914d\u5230\u7279\u5b9a\u4efb\u52a1\u7684\u5173\u952e\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u707e\u96be\u6027\u9057\u5fd8\u548c\u6570\u636e\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faDEAL\u6846\u67b6\uff0c\u7ed3\u5408\u4f4e\u79e9\u9002\u914d\uff08LoRA\uff09\u548c\u8fde\u7eed\u5fae\u8c03\u7b56\u7565\uff0c\u5e76\u52a0\u5165\u77e5\u8bc6\u4fdd\u7559\u548c\u81ea\u9002\u5e94\u53c2\u6570\u66f4\u65b0\u6a21\u5757\u3002", "result": "\u572815\u4e2a\u591a\u6837\u5316\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDEAL\u5728\u4efb\u52a1\u51c6\u786e\u6027\u548c\u8d44\u6e90\u6548\u7387\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "DEAL\u6846\u67b6\u80fd\u591f\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6301\u7eed\u9002\u5e94\u80fd\u529b\uff0c\u540c\u65f6\u63d0\u9ad8\u4efb\u52a1\u6027\u80fd\u548c\u8d44\u6e90\u6548\u7387\u3002"}}
{"id": "2509.18880", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18880", "abs": "https://arxiv.org/abs/2509.18880", "authors": ["Advik Raj Basani", "Pin-Yu Chen"], "title": "Diversity Boosts AI-Generated Text Detection", "comment": "Project Webpage: https://diveye.vercel.app/", "summary": "Detecting AI-generated text is an increasing necessity to combat misuse of\nLLMs in education, business compliance, journalism, and social media, where\nsynthetic fluency can mask misinformation or deception. While prior detectors\noften rely on token-level likelihoods or opaque black-box classifiers, these\napproaches struggle against high-quality generations and offer little\ninterpretability. In this work, we propose DivEye, a novel detection framework\nthat captures how unpredictability fluctuates across a text using\nsurprisal-based features. Motivated by the observation that human-authored text\nexhibits richer variability in lexical and structural unpredictability than LLM\noutputs, DivEye captures this signal through a set of interpretable statistical\nfeatures. Our method outperforms existing zero-shot detectors by up to 33.2%\nand achieves competitive performance with fine-tuned baselines across multiple\nbenchmarks. DivEye is robust to paraphrasing and adversarial attacks,\ngeneralizes well across domains and models, and improves the performance of\nexisting detectors by up to 18.7% when used as an auxiliary signal. Beyond\ndetection, DivEye provides interpretable insights into why a text is flagged,\npointing to rhythmic unpredictability as a powerful and underexplored signal\nfor LLM detection.", "AI": {"tldr": "DivEye\u662f\u4e00\u4e2a\u65b0\u9896\u7684AI\u751f\u6210\u6587\u672c\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790\u6587\u672c\u4e2d\u60ca\u5947\u5ea6\uff08surprisal\uff09\u7279\u5f81\u7684\u6ce2\u52a8\u6765\u8bc6\u522bAI\u751f\u6210\u5185\u5bb9\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\uff0c\u5b83\u5728\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u8868\u73b0\u66f4\u4f18\uff0c\u5e76\u4e14\u5bf9\u91ca\u4e49\u548c\u5bf9\u6297\u6027\u653b\u51fb\u5177\u6709\u9c81\u68d2\u6027\u3002", "motivation": "\u4e3a\u4e86\u5e94\u5bf9AI\u751f\u6210\u6587\u672c\u5728\u6559\u80b2\u3001\u5546\u4e1a\u3001\u65b0\u95fb\u548c\u793e\u4ea4\u5a92\u4f53\u7b49\u9886\u57df\u88ab\u6ee5\u7528\u7684\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u6bd4\u4ee5\u5f80\u66f4\u53ef\u9760\u3001\u66f4\u5177\u53ef\u89e3\u91ca\u6027\u7684\u68c0\u6d4b\u5668\uff0c\u4ee5\u5bf9\u6297\u9ad8\u8d28\u91cf\u7684AI\u751f\u6210\u5185\u5bb9\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDivEye\u7684\u65b0\u578b\u68c0\u6d4b\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u57fa\u4e8e\u60ca\u5947\u5ea6\uff08surprisal\uff09\u7684\u7279\u5f81\u6765\u6355\u6349\u6587\u672c\u4e2d\u4e0d\u53ef\u9884\u6d4b\u6027\u7684\u6ce2\u52a8\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u5b83\u901a\u8fc7\u4e00\u7cfb\u5217\u53ef\u89e3\u91ca\u7684\u7edf\u8ba1\u7279\u5f81\u6765\u6355\u6349\u4eba\u7c7b\u5199\u4f5c\u4e2d\u6bd4LLM\u8f93\u51fa\u66f4\u4e30\u5bcc\u7684\u8bcd\u6c47\u548c\u7ed3\u6784\u4e0a\u7684\u4e0d\u53ef\u9884\u6d4b\u6027\u53d8\u5316\u3002", "result": "DivEye\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5176\u96f6\u6837\u672c\u68c0\u6d4b\u6027\u80fd\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u9ad8\u4e8633.2%\uff0c\u5e76\u8fbe\u5230\u4e86\u4e0e\u5fae\u8c03\u57fa\u7ebf\u76f8\u5f53\u7684\u6027\u80fd\u3002\u8be5\u65b9\u6cd5\u5bf9\u91ca\u4e49\u548c\u5bf9\u6297\u6027\u653b\u51fb\u5177\u6709\u9c81\u68d2\u6027\uff0c\u80fd\u591f\u8de8\u9886\u57df\u548c\u8de8\u6a21\u578b\u6cdb\u5316\uff0c\u5e76\u4e14\u4f5c\u4e3a\u8f85\u52a9\u4fe1\u53f7\u65f6\uff0c\u80fd\u5c06\u73b0\u6709\u68c0\u6d4b\u5668\u7684\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe18.7%\u3002", "conclusion": "DivEye\u6846\u67b6\u901a\u8fc7\u5229\u7528\u6587\u672c\u4e2d\u8282\u594f\u6027\u4e0d\u53ef\u9884\u6d4b\u6027\u8fd9\u4e00\u5f3a\u5927\u4e14\u672a\u88ab\u5145\u5206\u63a2\u7d22\u7684\u4fe1\u53f7\uff0c\u4e0d\u4ec5\u5728\u68c0\u6d4bAI\u751f\u6210\u6587\u672c\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u8fd8\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u6d1e\u5bdf\uff0c\u6307\u51fa\u4e86\u6587\u672c\u88ab\u6807\u8bb0\u7684\u539f\u56e0\u3002"}}
{"id": "2509.19012", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19012", "abs": "https://arxiv.org/abs/2509.19012", "authors": ["Dapeng Zhang", "Jin Sun", "Chenghui Hu", "Xiaoyan Wu", "Zhenlong Yuan", "Rui Zhou", "Fei Shen", "Qingguo Zhou"], "title": "Pure Vision Language Action (VLA) Models: A Comprehensive Survey", "comment": null, "summary": "The emergence of Vision Language Action (VLA) models marks a paradigm shift\nfrom traditional policy-based control to generalized robotics, reframing Vision\nLanguage Models (VLMs) from passive sequence generators into active agents for\nmanipulation and decision-making in complex, dynamic environments. This survey\ndelves into advanced VLA methods, aiming to provide a clear taxonomy and a\nsystematic, comprehensive review of existing research. It presents a\ncomprehensive analysis of VLA applications across different scenarios and\nclassifies VLA approaches into several paradigms: autoregression-based,\ndiffusion-based, reinforcement-based, hybrid, and specialized methods; while\nexamining their motivations, core strategies, and implementations in detail. In\naddition, foundational datasets, benchmarks, and simulation platforms are\nintroduced. Building on the current VLA landscape, the review further proposes\nperspectives on key challenges and future directions to advance research in VLA\nmodels and generalizable robotics. By synthesizing insights from over three\nhundred recent studies, this survey maps the contours of this rapidly evolving\nfield and highlights the opportunities and challenges that will shape the\ndevelopment of scalable, general-purpose VLA methods.", "AI": {"tldr": "VLA\u6a21\u578b\u662f\u673a\u5668\u4eba\u9886\u57df\u7684\u4e00\u9879\u65b0\u8303\u5f0f\uff0c\u53ef\u4ee5\u4e3b\u52a8\u8fdb\u884c\u64cd\u4f5c\u548c\u51b3\u7b56\uff0c\u800c\u4e0d\u662f\u4f20\u7edf\u7684\u57fa\u4e8e\u7b56\u7565\u7684\u63a7\u5236\u3002\u672c\u7bc7\u7efc\u8ff0\u5bf9VLA\u65b9\u6cd5\u8fdb\u884c\u4e86\u5206\u7c7b\uff0c\u63a2\u8ba8\u4e86\u5176\u52a8\u673a\u3001\u7b56\u7565\u548c\u5b9e\u73b0\uff0c\u5e76\u4ecb\u7ecd\u4e86\u76f8\u5173\u6570\u636e\u96c6\u3001\u57fa\u51c6\u548c\u6a21\u62df\u5e73\u53f0\uff0c\u6700\u540e\u63d0\u51fa\u4e86\u672a\u6765\u7684\u6311\u6218\u548c\u53d1\u5c55\u65b9\u5411\u3002", "motivation": "VLA\u6a21\u578b\u4ee3\u8868\u4e86\u4ece\u4f20\u7edf\u57fa\u4e8e\u7b56\u7565\u7684\u63a7\u5236\u5230\u901a\u7528\u673a\u5668\u4eba\u7684\u8303\u5f0f\u8f6c\u53d8\uff0c\u5c06\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u4ece\u88ab\u52a8\u7684\u5e8f\u5217\u751f\u6210\u5668\u8f6c\u53d8\u4e3a\u590d\u6742\u3001\u52a8\u6001\u73af\u5883\u4e2d\u64cd\u4f5c\u548c\u51b3\u7b56\u7684\u4e3b\u52a8\u4ee3\u7406\u3002", "method": "\u672c\u7efc\u8ff0\u5bf9VLA\u65b9\u6cd5\u8fdb\u884c\u4e86\u5206\u7c7b\uff0c\u5305\u62ec\u81ea\u56de\u5f52\u3001\u6269\u6563\u3001\u5f3a\u5316\u5b66\u4e60\u3001\u6df7\u5408\u548c\u4e13\u7528\u65b9\u6cd5\uff0c\u5e76\u8be6\u7ec6\u68c0\u67e5\u4e86\u5b83\u4eec\u7684\u52a8\u673a\u3001\u6838\u5fc3\u7b56\u7565\u548c\u5b9e\u73b0\u3002\u6b64\u5916\uff0c\u8fd8\u4ecb\u7ecd\u4e86\u57fa\u7840\u6570\u636e\u96c6\u3001\u57fa\u51c6\u548c\u6a21\u62df\u5e73\u53f0\u3002", "result": "\u672c\u7efc\u8ff0\u5bf9VLA\u7684\u51fa\u73b0\u3001\u8de8\u4e0d\u540c\u573a\u666f\u7684\u5e94\u7528\u8fdb\u884c\u4e86\u5168\u9762\u7684\u5206\u6790\uff0c\u5e76\u5bf9\u57fa\u4e8e\u81ea\u56de\u5f52\u3001\u6269\u6563\u3001\u5f3a\u5316\u5b66\u4e60\u3001\u6df7\u5408\u548c\u4e13\u7528\u65b9\u6cd5\u8fdb\u884c\u4e86\u5206\u7c7b\u3002", "conclusion": "\u672c\u7efc\u8ff0\u5bf9VLA\u7684\u7814\u7a76\u73b0\u72b6\u8fdb\u884c\u4e86\u5168\u9762\u7684\u5206\u6790\uff0c\u5e76\u63d0\u51fa\u4e86\u5173\u952e\u6311\u6218\u548c\u672a\u6765\u65b9\u5411\u7684\u89c2\u70b9\uff0c\u4ee5\u63a8\u52a8VLA\u6a21\u578b\u548c\u53ef\u6cdb\u5316\u673a\u5668\u4eba\u7684\u7814\u7a76\u3002"}}
{"id": "2509.18566", "categories": ["cs.CV", "cs.RO", "eess.IV"], "pdf": "https://arxiv.org/pdf/2509.18566", "abs": "https://arxiv.org/abs/2509.18566", "authors": ["Xiaoting Yin", "Hao Shi", "Kailun Yang", "Jiajun Zhai", "Shangwei Guo", "Lin Wang", "Kaiwei Wang"], "title": "Event-guided 3D Gaussian Splatting for Dynamic Human and Scene Reconstruction", "comment": null, "summary": "Reconstructing dynamic humans together with static scenes from monocular\nvideos remains difficult, especially under fast motion, where RGB frames suffer\nfrom motion blur. Event cameras exhibit distinct advantages, e.g., microsecond\ntemporal resolution, making them a superior sensing choice for dynamic human\nreconstruction. Accordingly, we present a novel event-guided human-scene\nreconstruction framework that jointly models human and scene from a single\nmonocular event camera via 3D Gaussian Splatting. Specifically, a unified set\nof 3D Gaussians carries a learnable semantic attribute; only Gaussians\nclassified as human undergo deformation for animation, while scene Gaussians\nstay static. To combat blur, we propose an event-guided loss that matches\nsimulated brightness changes between consecutive renderings with the event\nstream, improving local fidelity in fast-moving regions. Our approach removes\nthe need for external human masks and simplifies managing separate Gaussian\nsets. On two benchmark datasets, ZJU-MoCap-Blur and MMHPSD-Blur, it delivers\nstate-of-the-art human-scene reconstruction, with notable gains over strong\nbaselines in PSNR/SSIM and reduced LPIPS, especially for high-speed subjects.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4e8b\u4ef6\u5f15\u5bfc\u5f0f\u4eba\u666f\u91cd\u5efa\u6846\u67b6\uff0c\u5229\u75283D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\uff0c\u4ec5\u7528\u4e00\u4e2a\u5355\u76ee\u4e8b\u4ef6\u76f8\u673a\u5373\u53ef\u540c\u65f6\u91cd\u5efa\u52a8\u6001\u4eba\u7c7b\u548c\u9759\u6001\u573a\u666f\u3002", "motivation": "\u73b0\u6709\u7684\u4ece\u5355\u76ee\u89c6\u9891\u4e2d\u91cd\u5efa\u52a8\u6001\u4eba\u7c7b\u548c\u9759\u6001\u573a\u666f\u7684\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u5feb\u901f\u8fd0\u52a8\u7684\u60c5\u51b5\u4e0b\uff0c\u7531\u4e8eRGB\u5e27\u7684\u8fd0\u52a8\u6a21\u7cca\u800c\u5b58\u5728\u56f0\u96be\u3002\u4e8b\u4ef6\u76f8\u673a\u5177\u6709\u5fae\u79d2\u7ea7\u7684\u65f6\u95f4\u5206\u8fa8\u7387\uff0c\u5728\u52a8\u6001\u4eba\u7c7b\u91cd\u5efa\u65b9\u9762\u5177\u6709\u4f18\u52bf\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u76843D\u9ad8\u65af\u96c6\u5408\uff0c\u8be5\u96c6\u5408\u5305\u542b\u53ef\u5b66\u4e60\u7684\u8bed\u4e49\u5c5e\u6027\u3002\u53ea\u6709\u88ab\u5f52\u7c7b\u4e3a\u4eba\u7c7b\u7684\u9ad8\u65af\u70b9\u624d\u8fdb\u884c\u53d8\u5f62\u4ee5\u5b9e\u73b0\u52a8\u753b\uff0c\u800c\u573a\u666f\u9ad8\u65af\u70b9\u4fdd\u6301\u9759\u6001\u3002\u4e3a\u4e86\u5bf9\u6297\u6a21\u7cca\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u4e8b\u4ef6\u5f15\u5bfc\u635f\u5931\uff0c\u5c06\u8fde\u7eed\u6e32\u67d3\u8fc7\u7a0b\u4e2d\u6a21\u62df\u7684\u4eae\u5ea6\u53d8\u5316\u4e0e\u4e8b\u4ef6\u6d41\u8fdb\u884c\u5339\u914d\uff0c\u4ee5\u63d0\u9ad8\u5feb\u901f\u79fb\u52a8\u533a\u57df\u7684\u5c40\u90e8\u4fdd\u771f\u5ea6\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u5916\u90e8\u4eba\u7c7b\u63a9\u7801\uff0c\u5e76\u7b80\u5316\u4e86\u5bf9\u72ec\u7acb\u9ad8\u65af\u96c6\u5408\u7684\u7ba1\u7406\u3002", "result": "\u5728ZJU-MoCap-Blur\u548cMMHPSD-Blur\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u4eba\u666f\u91cd\u5efa\u6548\u679c\uff0c\u5728PSNR/SSIM\u65b9\u9762\u76f8\u8f83\u4e8e\u5f3a\u5927\u7684\u57fa\u7ebf\u6a21\u578b\u6709\u663e\u8457\u63d0\u5347\uff0cLPIPS\u964d\u4f4e\uff0c\u5c24\u5176\u5728\u9ad8\u901f\u8fd0\u52a8\u4e3b\u4f53\u65b9\u9762\u6548\u679c\u66f4\u4f73\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u4e8b\u4ef6\u5f15\u5bfc\u5f0f\u4eba\u666f\u91cd\u5efa\u6846\u67b6\uff0c\u5229\u75283D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\uff0c\u80fd\u591f\u6709\u6548\u5730\u89e3\u51b3\u5355\u76ee\u4e8b\u4ef6\u76f8\u673a\u5728\u9ad8\u901f\u8fd0\u52a8\u4e0b\u7684\u4eba\u7c7b\u548c\u573a\u666f\u91cd\u5efa\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u8bc4\u4f30\u6307\u6807\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002"}}
{"id": "2509.18154", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18154", "abs": "https://arxiv.org/abs/2509.18154", "authors": ["Tianyu Yu", "Zefan Wang", "Chongyi Wang", "Fuwei Huang", "Wenshuo Ma", "Zhihui He", "Tianchi Cai", "Weize Chen", "Yuxiang Huang", "Yuanqian Zhao", "Bokai Xu", "Junbo Cui", "Yingjing Xu", "Liqing Ruan", "Luoyuan Zhang", "Hanyu Liu", "Jingkun Tang", "Hongyuan Liu", "Qining Guo", "Wenhao Hu", "Bingxiang He", "Jie Zhou", "Jie Cai", "Ji Qi", "Zonghao Guo", "Chi Chen", "Guoyang Zeng", "Yuxuan Li", "Ganqu Cui", "Ning Ding", "Xu Han", "Yuan Yao", "Zhiyuan Liu", "Maosong Sun"], "title": "MiniCPM-V 4.5: Cooking Efficient MLLMs via Architecture, Data, and Training Recipe", "comment": "Project Website: https://github.com/OpenBMB/MiniCPM-V", "summary": "Multimodal Large Language Models (MLLMs) are undergoing rapid progress and\nrepresent the frontier of AI development. However, their training and inference\nefficiency have emerged as a core bottleneck in making MLLMs more accessible\nand scalable. To address the challenges, we present MiniCPM-V 4.5, an 8B\nparameter model designed for high efficiency and strong performance. We\nintroduce three core improvements in model architecture, data strategy and\ntraining method: a unified 3D-Resampler model architecture for highly compact\nencoding over images and videos, a unified learning paradigm for document\nknowledge and text recognition without heavy data engineering, and a hybrid\nreinforcement learning strategy for proficiency in both short and long\nreasoning modes. Comprehensive experimental results in OpenCompass evaluation\nshow that MiniCPM-V 4.5 surpasses widely used proprietary models such as\nGPT-4o-latest, and significantly larger open-source models such as Qwen2.5-VL\n72B. Notably, the strong performance is achieved with remarkable efficiency.\nFor example, on the widely adopted VideoMME benchmark, MiniCPM-V 4.5 achieves\nstate-of-the-art performance among models under 30B size, using just 46.7\\% GPU\nmemory cost and 8.7\\% inference time of Qwen2.5-VL 7B.", "AI": {"tldr": "MiniCPM-V 4.5\u662f\u4e00\u4e2a\u9ad8\u6548\u4e14\u6027\u80fd\u5f3a\u5927\u768480\u4ebf\u53c2\u6570\u591a\u6a21\u6001\u5927\u6a21\u578b\uff0c\u901a\u8fc7\u6539\u8fdb\u6a21\u578b\u67b6\u6784\u3001\u6570\u636e\u7b56\u7565\u548c\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5728\u5404\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u4e86\u540c\u7c7b\u6a21\u578b\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u6a21\u578b\uff08MLLMs\uff09\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u6548\u7387\u4f4e\u4e0b\u662f\u5176\u53ef\u8bbf\u95ee\u6027\u548c\u53ef\u6269\u5c55\u6027\u7684\u4e3b\u8981\u74f6\u9888\u3002", "method": "\u63d0\u51faMiniCPM-V 4.5\uff0c\u4e00\u4e2a80\u4ebf\u53c2\u6570\u7684\u6a21\u578b\uff0c\u91c7\u7528\u4e86\u7edf\u4e00\u76843D-Resampler\u6a21\u578b\u67b6\u6784\u3001\u7edf\u4e00\u7684\u5b66\u4e60\u8303\u5f0f\u4ee5\u53ca\u6df7\u5408\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u3002", "result": "MiniCPM-V 4.5\u5728OpenCompass\u8bc4\u4f30\u4e2d\u8868\u73b0\u4f18\u4e8eGPT-4o-latest\u548cQwen2.5-VL 72B\u7b49\u6a21\u578b\u3002\u5728VideoMME\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5176GPU\u5185\u5b58\u6d88\u8017\u548c\u63a8\u7406\u65f6\u95f4\u4ec5\u5206\u522b\u4e3aQwen2.5-VL 7B\u768446.7%\u548c8.7%\u3002", "conclusion": "MiniCPM-V 4.5\u5728\u4fdd\u6301\u5f3a\u5927\u6027\u80fd\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6548\u7387\u63d0\u5347\uff0c\u4e3a\u591a\u6a21\u6001\u5927\u6a21\u578b\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.18970", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18970", "abs": "https://arxiv.org/abs/2509.18970", "authors": ["Xixun Lin", "Yucheng Ning", "Jingwen Zhang", "Yan Dong", "Yilong Liu", "Yongxuan Wu", "Xiaohua Qi", "Nan Sun", "Yanmin Shang", "Pengfei Cao", "Lixin Zou", "Xu Chen", "Chuan Zhou", "Jia Wu", "Shirui Pan", "Bin Wang", "Yanan Cao", "Kai Chen", "Songlin Hu", "Li Guo"], "title": "LLM-based Agents Suffer from Hallucinations: A Survey of Taxonomy, Methods, and Directions", "comment": null, "summary": "Driven by the rapid advancements of Large Language Models (LLMs), LLM-based\nagents have emerged as powerful intelligent systems capable of human-like\ncognition, reasoning, and interaction. These agents are increasingly being\ndeployed across diverse real-world applications, including student education,\nscientific research, and financial analysis. However, despite their remarkable\npotential, LLM-based agents remain vulnerable to hallucination issues, which\ncan result in erroneous task execution and undermine the reliability of the\noverall system design. Addressing this critical challenge requires a deep\nunderstanding and a systematic consolidation of recent advances on LLM-based\nagents. To this end, we present the first comprehensive survey of\nhallucinations in LLM-based agents. By carefully analyzing the complete\nworkflow of agents, we propose a new taxonomy that identifies different types\nof agent hallucinations occurring at different stages. Furthermore, we conduct\nan in-depth examination of eighteen triggering causes underlying the emergence\nof agent hallucinations. Through a detailed review of a large number of\nexisting studies, we summarize approaches for hallucination mitigation and\ndetection, and highlight promising directions for future research. We hope this\nsurvey will inspire further efforts toward addressing hallucinations in\nLLM-based agents, ultimately contributing to the development of more robust and\nreliable agent systems.", "AI": {"tldr": "LLM\u4ee3\u7406\u5728\u73b0\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u5e7b\u89c9\u95ee\u9898\u4e25\u91cd\u5f71\u54cd\u5176\u53ef\u9760\u6027\u3002\u672c\u6587\u9996\u6b21\u5168\u9762 survey \u4e86LLM\u4ee3\u7406\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u5206\u7c7b\u6cd5\uff0c\u5206\u6790\u4e86\u5e7b\u89c9\u7684\u89e6\u53d1\u539f\u56e0\uff0c\u5e76\u603b\u7ed3\u4e86\u73b0\u6709\u7684\u7f13\u89e3\u548c\u68c0\u6d4b\u65b9\u6cd5\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u6307\u660e\u4e86\u65b9\u5411\u3002", "motivation": "LLM\u4ee3\u7406\u867d\u7136\u5728\u6559\u80b2\u3001\u79d1\u7814\u3001\u91d1\u878d\u7b49\u9886\u57df\u6709\u5e7f\u6cdb\u5e94\u7528\u524d\u666f\uff0c\u4f46\u5e7b\u89c9\u95ee\u9898\u4f1a\u5bfc\u81f4\u4efb\u52a1\u9519\u8bef\uff0c\u964d\u4f4e\u7cfb\u7edf\u53ef\u9760\u6027\u3002\u56e0\u6b64\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u5730\u68b3\u7406\u548c\u7406\u89e3LLM\u4ee3\u7406\u7684\u5e7b\u89c9\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5206\u6790LLM\u4ee3\u7406\u7684\u5b8c\u6574\u5de5\u4f5c\u6d41\u7a0b\uff0c\u63d0\u51fa\u65b0\u7684\u5e7b\u89c9\u5206\u7c7b\u6cd5\uff0c\u8bc6\u522b\u4e0d\u540c\u9636\u6bb5\u7684\u5e7b\u89c9\u7c7b\u578b\uff1b\u6df1\u5165\u7814\u7a7618\u79cd\u5e7b\u89c9\u7684\u89e6\u53d1\u539f\u56e0\uff1b\u8be6\u7ec6\u56de\u987e\u73b0\u6709\u7814\u7a76\uff0c\u603b\u7ed3\u5e7b\u89c9\u7684\u7f13\u89e3\u548c\u68c0\u6d4b\u65b9\u6cd5\u3002", "result": "\u8bc6\u522b\u51faLLM\u4ee3\u7406\u5728\u4e0d\u540c\u9636\u6bb5\u53ef\u80fd\u51fa\u73b0\u7684\u5e7b\u89c9\u7c7b\u578b\uff0c\u5f52\u7eb3\u4e86\u5bfc\u81f4\u5e7b\u89c9\u51fa\u73b0\u768418\u79cd\u539f\u56e0\uff0c\u5e76\u603b\u7ed3\u4e86\u73b0\u6709\u7684\u5e7b\u89c9\u7f13\u89e3\u548c\u68c0\u6d4b\u6280\u672f\u3002", "conclusion": "\u672c\u6587\u9996\u6b21\u5168\u9762 survey \u4e86LLM\u4ee3\u7406\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u5206\u7c7b\u6cd5\uff0c\u5206\u6790\u4e86\u5e7b\u89c9\u7684\u89e6\u53d1\u539f\u56e0\uff0c\u5e76\u603b\u7ed3\u4e86\u73b0\u6709\u7684\u7f13\u89e3\u548c\u68c0\u6d4b\u65b9\u6cd5\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u6307\u660e\u4e86\u65b9\u5411\uff0c\u65e8\u5728\u4fc3\u8fdb\u66f4\u53ef\u9760\u7684LLM\u4ee3\u7406\u7cfb\u7edf\u7684\u53d1\u5c55\u3002"}}
{"id": "2509.18901", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18901", "abs": "https://arxiv.org/abs/2509.18901", "authors": ["Nicholas Popovi\u010d", "Michael F\u00e4rber"], "title": "Extractive Fact Decomposition for Interpretable Natural Language Inference in one Forward Pass", "comment": "EMNLP 2025", "summary": "Recent works in Natural Language Inference (NLI) and related tasks, such as\nautomated fact-checking, employ atomic fact decomposition to enhance\ninterpretability and robustness. For this, existing methods rely on\nresource-intensive generative large language models (LLMs) to perform\ndecomposition. We propose JEDI, an encoder-only architecture that jointly\nperforms extractive atomic fact decomposition and interpretable inference\nwithout requiring generative models during inference. To facilitate training,\nwe produce a large corpus of synthetic rationales covering multiple NLI\nbenchmarks. Experimental results demonstrate that JEDI achieves competitive\naccuracy in distribution and significantly improves robustness out of\ndistribution and in adversarial settings over models based solely on extractive\nrationale supervision. Our findings show that interpretability and robust\ngeneralization in NLI can be realized using encoder-only architectures and\nsynthetic rationales. Code and data available at https://jedi.nicpopovic.com", "AI": {"tldr": "JEDI\u662f\u4e00\u4e2a\u4ec5\u7f16\u7801\u5668\u6a21\u578b\uff0c\u53ef\u4ee5\u8fdb\u884c\u539f\u5b50\u4e8b\u5b9e\u5206\u89e3\u548c\u53ef\u89e3\u91ca\u63a8\u7406\uff0c\u65e0\u9700\u751f\u6210\u6a21\u578b\uff0c\u5e76\u4e14\u5728NLI\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u7684NLI\u548c\u4e8b\u5b9e\u6838\u67e5\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u8d44\u6e90\u5bc6\u96c6\u578b\u7684\u751f\u6210\u5f0fLLM\u6765\u8fdb\u884c\u4e8b\u5b9e\u5206\u89e3\uff0cJEDI\u65e8\u5728\u901a\u8fc7\u4ec5\u7f16\u7801\u5668\u6a21\u578b\u89e3\u51b3\u6b64\u95ee\u9898\u3002", "method": "JEDI\u6a21\u578b\u8fdb\u884c\u539f\u5b50\u4e8b\u5b9e\u5206\u89e3\u548c\u53ef\u89e3\u91ca\u63a8\u7406\uff0c\u65e0\u9700\u751f\u6210\u6a21\u578b\u3002\u4f7f\u7528\u5305\u542b\u591a\u4e2aNLI\u57fa\u51c6\u7684\u5408\u6210\u8bed\u6599\u5e93\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "JEDI\u5728\u5206\u5e03\u5185\u51c6\u786e\u7387\u65b9\u9762\u5177\u6709\u7ade\u4e89\u529b\uff0c\u5728\u5206\u5e03\u5916\u548c\u5bf9\u6297\u6027\u8bbe\u7f6e\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u9c81\u68d2\u6027\u3002", "conclusion": "JEDI\u8bc1\u660e\u4e86\u4ec5\u7f16\u7801\u5668\u6a21\u578b\u548c\u5408\u6210\u8bed\u6599\u5e93\u53ef\u4ee5\u5b9e\u73b0NLI\u7684\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\u6cdb\u5316\u3002"}}
{"id": "2509.19023", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19023", "abs": "https://arxiv.org/abs/2509.19023", "authors": ["Shuai Liu", "Meng Cheng Lau"], "title": "Reduced-Order Model-Guided Reinforcement Learning for Demonstration-Free Humanoid Locomotion", "comment": "11 pages, 5 figures, 1 table, Computational Science Graduate Project", "summary": "We introduce Reduced-Order Model-Guided Reinforcement Learning (ROM-GRL), a\ntwo-stage reinforcement learning framework for humanoid walking that requires\nno motion capture data or elaborate reward shaping. In the first stage, a\ncompact 4-DOF (four-degree-of-freedom) reduced-order model (ROM) is trained via\nProximal Policy Optimization. This generates energy-efficient gait templates.\nIn the second stage, those dynamically consistent trajectories guide a\nfull-body policy trained with Soft Actor--Critic augmented by an adversarial\ndiscriminator, ensuring the student's five-dimensional gait feature\ndistribution matches the ROM's demonstrations. Experiments at 1\nmeter-per-second and 4 meter-per-second show that ROM-GRL produces stable,\nsymmetric gaits with substantially lower tracking error than a pure-reward\nbaseline. By distilling lightweight ROM guidance into high-dimensional\npolicies, ROM-GRL bridges the gap between reward-only and imitation-based\nlocomotion methods, enabling versatile, naturalistic humanoid behaviors without\nany human demonstrations.", "AI": {"tldr": "ROM-GRL\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u4eba\u5f62\u673a\u5668\u4eba\u884c\u8d70\uff0c\u65e0\u9700\u52a8\u4f5c\u6355\u6349\u6570\u636e\u6216\u590d\u6742\u7684\u5956\u52b1\u5851\u9020\u3002\u7b2c\u4e00\u9636\u6bb5\u8bad\u7ec3\u4e00\u4e2a\u7d27\u51d1\u76844-DOF ROM\uff0c\u751f\u6210\u9ad8\u6548\u7684\u6b65\u6001\u6a21\u677f\u3002\u7b2c\u4e8c\u9636\u6bb5\u4f7f\u7528SAC\u548c\u5bf9\u6297\u6027\u5224\u522b\u5668\u8bad\u7ec3\u4e00\u4e2a\u5168\u8eab\u4f53\u7b56\u7565\uff0c\u786e\u4fdd\u5176\u6b65\u6001\u7279\u5f81\u5206\u5e03\u4e0eROM\u7684\u6f14\u793a\u5339\u914d\u3002\u5b9e\u9a8c\u8868\u660e\uff0cROM-GRL\u57281\u7c73/\u79d2\u548c4\u7c73/\u79d2\u7684\u901f\u5ea6\u4e0b\uff0c\u6bd4\u7eaf\u5956\u52b1\u57fa\u7ebf\u4ea7\u751f\u66f4\u7a33\u5b9a\u3001\u5bf9\u79f0\u7684\u6b65\u6001\uff0c\u5e76\u5177\u6709\u66f4\u4f4e\u7684\u8ddf\u8e2a\u8bef\u5dee\u3002\u901a\u8fc7\u5c06\u8f7b\u91cf\u7ea7ROM\u5f15\u5bfc\u63d0\u70bc\u5230\u9ad8\u7ef4\u7b56\u7565\u4e2d\uff0cROM-GRL\u5728\u4ec5\u5956\u52b1\u548c\u57fa\u4e8e\u6a21\u4eff\u7684\u8fd0\u52a8\u65b9\u6cd5\u4e4b\u95f4\u67b6\u8d77\u4e86\u6865\u6881\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u4eba\u7c7b\u6f14\u793a\u7684\u591a\u529f\u80fd\u3001\u81ea\u7136\u7684\u7c7b\u4eba\u884c\u4e3a\u3002", "motivation": "\u8be5\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u65e0\u9700\u52a8\u4f5c\u6355\u6349\u6570\u636e\u6216\u590d\u6742\u5956\u52b1\u5851\u9020\u5373\u53ef\u5b9e\u73b0\u4eba\u5f62\u673a\u5668\u4eba\u9ad8\u6548\u884c\u8d70\u7684\u65b9\u6cd5\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aROM-GRL\u7684\u4e24\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u3002\u7b2c\u4e00\u9636\u6bb5\uff0c\u901a\u8fc7\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08PPO\uff09\u8bad\u7ec3\u4e00\u4e2a4\u81ea\u7531\u5ea6\uff08DOF\uff09\u7684\u964d\u9636\u6a21\u578b\uff08ROM\uff09\uff0c\u4ee5\u751f\u6210\u9ad8\u6548\u7684\u6b65\u6001\u6a21\u677f\u3002\u7b2c\u4e8c\u9636\u6bb5\uff0c\u5229\u7528\u8f6fActor-Critic\uff08SAC\uff09\u7b97\u6cd5\u7ed3\u5408\u5bf9\u6297\u6027\u5224\u522b\u5668\uff0c\u8bad\u7ec3\u4e00\u4e2a\u5168\u8eab\u4f53\u7b56\u7565\uff0c\u786e\u4fdd\u5176\u4e94\u7ef4\u6b65\u6001\u7279\u5f81\u5206\u5e03\u4e0eROM\u7684\u6f14\u793a\u5339\u914d\u3002", "result": "\u57281\u7c73/\u79d2\u548c4\u7c73/\u79d2\u7684\u901f\u5ea6\u4e0b\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0cROM-GRL\u751f\u6210\u7684\u6b65\u6001\u7a33\u5b9a\u4e14\u5bf9\u79f0\uff0c\u4e0e\u7eaf\u5956\u52b1\u57fa\u7ebf\u76f8\u6bd4\uff0c\u8ddf\u8e2a\u8bef\u5dee\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "ROM-GRL\u901a\u8fc7\u5c06\u8f7b\u91cf\u7ea7ROM\u7684\u5f15\u5bfc\u63d0\u70bc\u5230\u9ad8\u7ef4\u7b56\u7565\u4e2d\uff0c\u5f25\u5408\u4e86\u4ec5\u5956\u52b1\u65b9\u6cd5\u548c\u57fa\u4e8e\u6a21\u4eff\u7684\u65b9\u6cd5\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u4eba\u7c7b\u6f14\u793a\u5373\u53ef\u5b9e\u73b0\u901a\u7528\u3001\u81ea\u7136\u7684\u7c7b\u4eba\u884c\u4e3a\u3002"}}
{"id": "2509.18571", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18571", "abs": "https://arxiv.org/abs/2509.18571", "authors": ["Yuhan Wang", "Cheng Liu", "Zihan Zhao", "Weichao Wu"], "title": "Live-E2T: Real-time Threat Monitoring in Video via Deduplicated Event Reasoning and Chain-of-Thought", "comment": null, "summary": "Real-time threat monitoring identifies threatening behaviors in video streams\nand provides reasoning and assessment of threat events through explanatory\ntext. However, prevailing methodologies, whether based on supervised learning\nor generative models, struggle to concurrently satisfy the demanding\nrequirements of real-time performance and decision explainability. To bridge\nthis gap, we introduce Live-E2T, a novel framework that unifies these two\nobjectives through three synergistic mechanisms. First, we deconstruct video\nframes into structured Human-Object-Interaction-Place semantic tuples. This\napproach creates a compact, semantically focused representation, circumventing\nthe information degradation common in conventional feature compression. Second,\nan efficient online event deduplication and updating mechanism is proposed to\nfilter spatio-temporal redundancies, ensuring the system's real time\nresponsiveness. Finally, we fine-tune a Large Language Model using a\nChain-of-Thought strategy, endow it with the capability for transparent and\nlogical reasoning over event sequences to produce coherent threat assessment\nreports. Extensive experiments on benchmark datasets, including XD-Violence and\nUCF-Crime, demonstrate that Live-E2T significantly outperforms state-of-the-art\nmethods in terms of threat detection accuracy, real-time efficiency, and the\ncrucial dimension of explainability.", "AI": {"tldr": "Live-E2T\u6846\u67b6\u901a\u8fc7\u7ed3\u6784\u5316\u8bed\u4e49\u5143\u7ec4\u3001\u5728\u7ebf\u4e8b\u4ef6\u53bb\u91cd\u548c\u4f18\u5316\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u5b9e\u65f6\u5a01\u80c1\u68c0\u6d4b\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u5a01\u80c1\u76d1\u63a7\u65b9\u6cd5\u5728\u5b9e\u65f6\u6027\u548c\u51b3\u7b56\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51faLive-E2T\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u673a\u5236\uff1a1. \u5c06\u89c6\u9891\u5e27\u5206\u89e3\u4e3a\u7ed3\u6784\u5316\u7684\u4eba-\u7269-\u4ea4\u4e92-\u5730\u70b9\u8bed\u4e49\u5143\u7ec4\uff1b2. \u8bbe\u8ba1\u9ad8\u6548\u7684\u5728\u7ebf\u4e8b\u4ef6\u53bb\u91cd\u548c\u66f4\u65b0\u673a\u5236\uff1b3. \u4f7f\u7528\u601d\u7ef4\u94fe\u7b56\u7565\u5fae\u8c03\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4ee5\u8fdb\u884c\u4e8b\u4ef6\u5e8f\u5217\u63a8\u7406\u548c\u751f\u6210\u5a01\u80c1\u8bc4\u4f30\u62a5\u544a\u3002", "result": "\u5728XD-Violence\u548cUCF-Crime\u6570\u636e\u96c6\u4e0a\uff0cLive-E2T\u5728\u5a01\u80c1\u68c0\u6d4b\u51c6\u786e\u6027\u3001\u5b9e\u65f6\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "Live-E2T\u6846\u67b6\u6210\u529f\u5730\u89e3\u51b3\u4e86\u5b9e\u65f6\u5a01\u80c1\u76d1\u63a7\u4e2d\u7684\u5b9e\u65f6\u6027\u548c\u53ef\u89e3\u91ca\u6027\u6311\u6218\u3002"}}
{"id": "2509.18161", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18161", "abs": "https://arxiv.org/abs/2509.18161", "authors": ["William H Patty"], "title": "Developing Training Procedures for Piecewise-linear Spline Activation Functions in Neural Networks", "comment": null, "summary": "Activation functions in neural networks are typically selected from a set of\nempirically validated, commonly used static functions such as ReLU, tanh, or\nsigmoid. However, by optimizing the shapes of a network's activation functions,\nwe can train models that are more parameter-efficient and accurate by assigning\nmore optimal activations to the neurons. In this paper, I present and compare 9\ntraining methodologies to explore dual-optimization dynamics in neural networks\nwith parameterized linear B-spline activation functions. The experiments\nrealize up to 94% lower end model error rates in FNNs and 51% lower rates in\nCNNs compared to traditional ReLU-based models. These gains come at the cost of\nadditional development and training complexity as well as end model latency.", "AI": {"tldr": "\u901a\u8fc7\u4f18\u5316\u795e\u7ecf\u7f51\u7edc\u7684\u6fc0\u6d3b\u51fd\u6570\u5f62\u72b6\uff0c\u53ef\u4ee5\u63d0\u9ad8\u6a21\u578b\u7684\u53c2\u6570\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u7684\u9759\u6001\u6fc0\u6d3b\u51fd\u6570\uff08\u5982ReLU\u3001tanh\u3001sigmoid\uff09\u662f\u7ecf\u9a8c\u6027\u9009\u62e9\u7684\uff0c\u4f46\u901a\u8fc7\u4f18\u5316\u6fc0\u6d3b\u51fd\u6570\u7684\u5f62\u72b6\uff0c\u53ef\u4ee5\u4e3a\u795e\u7ecf\u5143\u5206\u914d\u66f4\u4f18\u7684\u6fc0\u6d3b\u503c\uff0c\u4ece\u800c\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u5e76\u6bd4\u8f83\u4e869\u79cd\u8bad\u7ec3\u65b9\u6cd5\uff0c\u63a2\u7d22\u4e86\u53c2\u6570\u5316\u7ebf\u6027B\u6837\u6761\u6fc0\u6d3b\u51fd\u6570\u7684\u53cc\u91cd\u4f18\u5316\u52a8\u6001\u3002", "result": "\u4e0e\u4f20\u7edf\u7684ReLU\u6a21\u578b\u76f8\u6bd4\uff0c\u5728\u5168\u8fde\u63a5\u7f51\u7edc\uff08FNNs\uff09\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u8fbe94%\u7684\u6700\u7ec8\u6a21\u578b\u8bef\u5dee\u7387\u964d\u4f4e\uff0c\u5728\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNNs\uff09\u4e2d\u5b9e\u73b0\u4e8651%\u7684\u8bef\u5dee\u7387\u964d\u4f4e\u3002", "conclusion": "\u4f18\u5316\u6fc0\u6d3b\u51fd\u6570\u7684\u5f62\u72b6\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u6a21\u578b\u7684\u51c6\u786e\u6027\uff0c\u4f46\u4f1a\u589e\u52a0\u5f00\u53d1\u548c\u8bad\u7ec3\u7684\u590d\u6742\u6027\u4ee5\u53ca\u6700\u7ec8\u6a21\u578b\u7684\u5ef6\u8fdf\u3002"}}
{"id": "2509.18980", "categories": ["cs.AI", "cs.HC", "cs.IR", "H.3.3; H.5.2; I.2.7"], "pdf": "https://arxiv.org/pdf/2509.18980", "abs": "https://arxiv.org/abs/2509.18980", "authors": ["Maxime Manderlier", "Fabian Lecron", "Olivier Vu Thanh", "Nicolas Gillis"], "title": "From latent factors to language: a user study on LLM-generated explanations for an inherently interpretable matrix-based recommender system", "comment": null, "summary": "We investigate whether large language models (LLMs) can generate effective,\nuser-facing explanations from a mathematically interpretable recommendation\nmodel. The model is based on constrained matrix factorization, where user types\nare explicitly represented and predicted item scores share the same scale as\nobserved ratings, making the model's internal representations and predicted\nscores directly interpretable. This structure is translated into natural\nlanguage explanations using carefully designed LLM prompts. Many works in\nexplainable AI rely on automatic evaluation metrics, which often fail to\ncapture users' actual needs and perceptions. In contrast, we adopt a\nuser-centered approach: we conduct a study with 326 participants who assessed\nthe quality of the explanations across five key dimensions-transparency,\neffectiveness, persuasion, trust, and satisfaction-as well as the\nrecommendations themselves.To evaluate how different explanation strategies are\nperceived, we generate multiple explanation types from the same underlying\nmodel, varying the input information provided to the LLM. Our analysis reveals\nthat all explanation types are generally well received, with moderate\nstatistical differences between strategies. User comments further underscore\nhow participants react to each type of explanation, offering complementary\ninsights beyond the quantitative results.", "AI": {"tldr": "LLMs can generate interpretable recommendation explanations from a constrained matrix factorization model, evaluated through a user study.", "motivation": "To investigate if LLMs can create effective user-facing explanations from a mathematically interpretable recommendation model and to evaluate these explanations using a user-centered approach.", "method": "Used a constrained matrix factorization model with explicit user types and interpretable item scores. Translated model insights into natural language explanations via LLM prompts. Conducted a user study with 326 participants to assess explanation quality (transparency, effectiveness, persuasion, trust, satisfaction) and recommendation quality. Generated multiple explanation types by varying LLM inputs.", "result": "All generated explanation types were well-received by users, with minor statistical differences between strategies. User comments provided qualitative insights into their reactions to different explanation types.", "conclusion": "LLMs can generate effective, user-facing explanations from interpretable recommendation models. A user-centered evaluation approach is crucial for assessing explanation quality, complementing quantitative metrics."}}
{"id": "2509.18987", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18987", "abs": "https://arxiv.org/abs/2509.18987", "authors": ["Abderrahmane Issam", "Yusuf Can Semerci", "Jan Scholtes", "Gerasimos Spanakis"], "title": "DTW-Align: Bridging the Modality Gap in End-to-End Speech Translation with Dynamic Time Warping Alignment", "comment": "Accepted at WMT2025", "summary": "End-to-End Speech Translation (E2E-ST) is the task of translating source\nspeech directly into target text bypassing the intermediate transcription step.\nThe representation discrepancy between the speech and text modalities has\nmotivated research on what is known as bridging the modality gap.\nState-of-the-art methods addressed this by aligning speech and text\nrepresentations on the word or token level. Unfortunately, this requires an\nalignment tool that is not available for all languages. Although this issue has\nbeen addressed by aligning speech and text embeddings using nearest-neighbor\nsimilarity search, it does not lead to accurate alignments. In this work, we\nadapt Dynamic Time Warping (DTW) for aligning speech and text embeddings during\ntraining. Our experiments demonstrate the effectiveness of our method in\nbridging the modality gap in E2E-ST. Compared to previous work, our method\nproduces more accurate alignments and achieves comparable E2E-ST results while\nbeing significantly faster. Furthermore, our method outperforms previous work\nin low resource settings on 5 out of 6 language directions.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u7aef\u5230\u7aef\u8bed\u97f3\u7ffb\u8bd1\uff08E2E-ST\uff09\u65b9\u6cd5\uff0c\u4f7f\u7528\u52a8\u6001\u65f6\u95f4\u89c4\u6574\uff08DTW\uff09\u6765\u5bf9\u9f50\u8bed\u97f3\u548c\u6587\u672c\u5d4c\u5165\uff0c\u4ee5\u89e3\u51b3\u6a21\u6001\u9e3f\u6c9f\u95ee\u9898\u3002", "motivation": "\u7531\u4e8e\u8bed\u97f3\u548c\u6587\u672c\u6a21\u6001\u4e4b\u95f4\u7684\u8868\u793a\u5dee\u5f02\uff0c\u9700\u8981\u5f25\u5408\u6a21\u6001\u9e3f\u6c9f\u3002\u73b0\u6709\u7684\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u5355\u8bcd\u6216\u4ee4\u724c\u7ea7\u522b\u7684\u5bf9\u9f50\uff0c\u4f46\u8fd9\u9700\u8981\u4e00\u4e2a\u5e76\u975e\u6240\u6709\u8bed\u8a00\u90fd\u53ef\u7528\u7684\u5bf9\u9f50\u5de5\u5177\u3002\u4f7f\u7528\u6700\u8fd1\u90bb\u76f8\u4f3c\u6027\u641c\u7d22\u7684\u5bf9\u9f50\u65b9\u6cd5\u4e0d\u591f\u51c6\u786e\u3002", "method": "\u5c06\u52a8\u6001\u65f6\u95f4\u89c4\u6574\uff08DTW\uff09\u5e94\u7528\u4e8e\u8bad\u7ec3\u671f\u95f4\u7684\u8bed\u97f3\u548c\u6587\u672c\u5d4c\u5165\u5bf9\u9f50\u3002", "result": "\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0c\u672c\u7814\u7a76\u7684\u65b9\u6cd5\u80fd\u591f\u66f4\u51c6\u786e\u5730\u5bf9\u9f50\u8bed\u97f3\u548c\u6587\u672c\u5d4c\u5165\uff0c\u8fbe\u5230\u76f8\u5f53\u7684E2E-ST\u6027\u80fd\uff0c\u540c\u65f6\u901f\u5ea6\u66f4\u5feb\u3002\u57286\u4e2a\u8bed\u8a00\u65b9\u5411\u4e2d\u76845\u4e2a\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u4f4e\u8d44\u6e90\u73af\u5883\u4e0b\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684DTW\u5bf9\u9f50\u65b9\u6cd5\u6709\u6548\u5f25\u5408\u4e86E2E-ST\u4e2d\u7684\u6a21\u6001\u9e3f\u6c9f\uff0c\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u5728\u4f4e\u8d44\u6e90\u573a\u666f\u4e0b\u3002"}}
{"id": "2509.19037", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.19037", "abs": "https://arxiv.org/abs/2509.19037", "authors": ["Qingzheng Cong", "Steven Oh", "Wen Fan", "Shan Luo", "Kaspar Althoefer", "Dandan Zhang"], "title": "TacEva: A Performance Evaluation Framework For Vision-Based Tactile Sensors", "comment": "14 pages, 8 figures. Equal contribution: Qingzheng Cong, Steven Oh,\n  Wen Fan. Corresponding author: Dandan Zhang (d.zhang17@imperial.ac.uk).\n  Additional resources at http://stevenoh2003.github.io/TacEva/", "summary": "Vision-Based Tactile Sensors (VBTSs) are widely used in robotic tasks because\nof the high spatial resolution they offer and their relatively low\nmanufacturing costs. However, variations in their sensing mechanisms,\nstructural dimension, and other parameters lead to significant performance\ndisparities between existing VBTSs. This makes it challenging to optimize them\nfor specific tasks, as both the initial choice and subsequent fine-tuning are\nhindered by the lack of standardized metrics. To address this issue, TacEva is\nintroduced as a comprehensive evaluation framework for the quantitative\nanalysis of VBTS performance. The framework defines a set of performance\nmetrics that capture key characteristics in typical application scenarios. For\neach metric, a structured experimental pipeline is designed to ensure\nconsistent and repeatable quantification. The framework is applied to multiple\nVBTSs with distinct sensing mechanisms, and the results demonstrate its ability\nto provide a thorough evaluation of each design and quantitative indicators for\neach performance dimension. This enables researchers to pre-select the most\nappropriate VBTS on a task by task basis, while also offering\nperformance-guided insights into the optimization of VBTS design. A list of\nexisting VBTS evaluation methods and additional evaluations can be found on our\nwebsite: https://stevenoh2003.github.io/TacEva/", "AI": {"tldr": "TacEva\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u89e6\u89c9\u4f20\u611f\u5668(VBTS)\u6027\u80fd\u7684\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u4f20\u611f\u5668\u6027\u80fd\u4e0d\u4e00\u81f4\u548c\u7f3a\u4e4f\u6807\u51c6\u5316\u8bc4\u4f30\u6307\u6807\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u89e6\u89c9\u4f20\u611f\u5668(VBTS)\u7531\u4e8e\u4f20\u611f\u673a\u5236\u3001\u7ed3\u6784\u5c3a\u5bf8\u7b49\u53c2\u6570\u7684\u5dee\u5f02\uff0c\u5bfc\u81f4\u6027\u80fd\u5dee\u5f02\u663e\u8457\uff0c\u96be\u4ee5\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\u8fdb\u884c\u4f18\u5316\uff0c\u7f3a\u4e4f\u6807\u51c6\u5316\u8bc4\u4f30\u6307\u6807\u3002", "method": "\u63d0\u51faTacEva\u6846\u67b6\uff0c\u5b9a\u4e49\u4e86\u4e00\u5957\u6027\u80fd\u6307\u6807\uff0c\u5e76\u8bbe\u8ba1\u4e86\u7ed3\u6784\u5316\u7684\u5b9e\u9a8c\u6d41\u7a0b\uff0c\u4ee5\u5b9e\u73b0\u5bf9VBTS\u6027\u80fd\u7684\u4e00\u81f4\u6027\u548c\u53ef\u91cd\u590d\u6027\u91cf\u5316\u3002", "result": "\u5c06TacEva\u6846\u67b6\u5e94\u7528\u4e8e\u591a\u79cd\u4e0d\u540c\u4f20\u611f\u673a\u5236\u7684VBTS\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u6846\u67b6\u80fd\u591f\u63d0\u4f9b\u8be6\u5c3d\u7684\u8bbe\u8ba1\u8bc4\u4f30\u548c\u5404\u6027\u80fd\u7ef4\u5ea6\u7684\u91cf\u5316\u6307\u6807\u3002", "conclusion": "TacEva\u6846\u67b6\u80fd\u591f\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u6839\u636e\u5177\u4f53\u4efb\u52a1\u9884\u5148\u9009\u62e9\u6700\u5408\u9002\u7684VBTS\uff0c\u5e76\u4e3a\u4f18\u5316VBTS\u8bbe\u8ba1\u63d0\u4f9b\u6027\u80fd\u6307\u5bfc\u3002"}}
{"id": "2509.18582", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18582", "abs": "https://arxiv.org/abs/2509.18582", "authors": ["Daiqing Qi", "Handong Zhao", "Jing Shi", "Simon Jenni", "Yifei Fan", "Franck Dernoncourt", "Scott Cohen", "Sheng Li"], "title": "The Photographer Eye: Teaching Multimodal Large Language Models to See and Critique like Photographers", "comment": null, "summary": "While editing directly from life, photographers have found it too difficult\nto see simultaneously both the blue and the sky. Photographer and curator,\nSzarkowski insightfully revealed one of the notable gaps between general and\naesthetic visual understanding: while the former focuses on identifying the\nfactual element in an image (sky), the latter transcends such object\nidentification, viewing it instead as an aesthetic component--a pure color\nblock (blue). Such fundamental distinctions between general (detection,\nlocalization, etc.) and aesthetic (color, lighting, composition, etc.) visual\nunderstanding present a significant challenge for Multimodal Large Language\nModels (MLLMs). Although some recent works have made initial explorations, they\nare often limited to general and basic aesthetic commonsense. As a result, they\nfrequently fall short in real-world scenarios (Fig. 1), which require extensive\nexpertise--including photographic techniques, photo pre/post-processing\nknowledge, and more, to provide a detailed analysis and description. To\nfundamentally enhance the aesthetics understanding of MLLMs, we first introduce\na novel dataset, PhotoCritique, derived from extensive discussions among\nprofessional photographers and enthusiasts, and characterized by the large\nscale, expertise, and diversity. Then, to better learn visual aesthetics from\nPhotoCritique, we furthur propose a novel model, PhotoEye, featuring a\nlanguageguided multi-view vision fusion mechanism to understand image\naesthetics from multiple perspectives. Finally, we present a novel benchmark,\nPhotoBench, a comprehensive and professional benchmark for aesthetic visual\nunderstanding. On existing benchmarks and PhotoBench, our model demonstrates\nclear advantages over existing models.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86PhotoCritique\u6570\u636e\u96c6\u3001PhotoEye\u6a21\u578b\u548cPhotoBench\u57fa\u51c6\uff0c\u4ee5\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u56fe\u50cf\u7f8e\u5b66\u7684\u7406\u89e3\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u5728\u5904\u7406\u9700\u8981\u4e13\u4e1a\u6444\u5f71\u77e5\u8bc6\u7684\u771f\u5b9e\u4e16\u754c\u573a\u666f\u65f6\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u5728\u7406\u89e3\u56fe\u50cf\u7f8e\u5b66\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u6444\u5f71\u6280\u672f\u3001\u540e\u671f\u5904\u7406\u77e5\u8bc6\u7b49\u4e13\u4e1a\u77e5\u8bc6\u7684\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\uff0c\u5b83\u4eec\u5e38\u5e38\u65e0\u6cd5\u63d0\u4f9b\u6df1\u5165\u7684\u5206\u6790\u548c\u63cf\u8ff0\uff0c\u8fd9\u4e0e\u533a\u5206\u56fe\u50cf\u4e2d\u7684\u4e8b\u5b9e\u5143\u7d20\uff08\u5982\u5929\u7a7a\uff09\u548c\u7f8e\u5b66\u7ec4\u6210\u90e8\u5206\uff08\u5982\u84dd\u8272\u8272\u5757\uff09\u7684\u80fd\u529b\u6709\u5173\u3002", "method": "\u7814\u7a76\u8005\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aPhotoCritique\u7684\u65b0\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u6765\u6e90\u4e8e\u4e13\u4e1a\u6444\u5f71\u5e08\u548c\u7231\u597d\u8005\u7684\u5e7f\u6cdb\u8ba8\u8bba\uff0c\u5177\u6709\u89c4\u6a21\u5927\u3001\u4e13\u4e1a\u6027\u5f3a\u548c\u591a\u6837\u6027\u9ad8\u7684\u7279\u70b9\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u4ed6\u4eec\u8fd8\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aPhotoEye\u7684\u65b0\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u91c7\u7528\u8bed\u8a00\u5f15\u5bfc\u7684\u591a\u89c6\u56fe\u89c6\u89c9\u878d\u5408\u673a\u5236\u6765\u4ece\u591a\u4e2a\u89d2\u5ea6\u7406\u89e3\u56fe\u50cf\u7f8e\u5b66\u3002\u6700\u540e\uff0c\u4ed6\u4eec\u63d0\u51fa\u4e86PhotoBench\uff0c\u4e00\u4e2a\u5168\u9762\u4e14\u4e13\u4e1a\u7684\u89c6\u89c9\u7f8e\u5b66\u7406\u89e3\u57fa\u51c6\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5728\u73b0\u6709\u57fa\u51c6\u548c\u65b0\u63d0\u51fa\u7684PhotoBench\u57fa\u51c6\u4e0a\uff0cPhotoEye\u6a21\u578b\u76f8\u6bd4\u4e8e\u5176\u4ed6\u6a21\u578b\u8868\u73b0\u51fa\u4e86\u660e\u663e\u7684\u4f18\u52bf\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u56fe\u50cf\u7f8e\u5b66\u7406\u89e3\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u5f15\u5165\u65b0\u7684\u6570\u636e\u96c6\u3001\u6a21\u578b\u548c\u57fa\u51c6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u56fe\u50cf\u7f8e\u5b66\u7406\u89e3\u65b9\u9762\u7684\u80fd\u529b\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\u7684\u573a\u666f\u4e2d\uff0c\u4e3a\u8be5\u9886\u57df\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u548c\u5de5\u5177\u3002"}}
{"id": "2509.19299", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2509.19299", "abs": "https://arxiv.org/abs/2509.19299", "authors": ["Julia Jones", "Jason Pollack"], "title": "Uniqueness of Complementary Recovery in Holographic Error-Correcting Codes", "comment": "19 pages, 4 figures, 1 table: submitted to QIP 2026, comments welcome", "summary": "Holographic codes are a type of error-correcting code with extra geometric\nstructure ensured by a ``complementary recovery'' property: given a division of\nthe physical Hilbert space $\\mathcal{H}$ into $\\mathcal{H}_A$ and\n$\\mathcal{H}_{\\bar A}$, and an algebra of physical operators\n$\\mathcal{M}\\subseteq (\\mathcal{L}(\\mathcal{H}_A)\\otimes I_{\\mathcal{H}_{\\bar\nA}})$, the logical operators in $\\mathcal{L}(\\mathcal{H}_L)\\simeq\n\\mathcal{L}(P\\mathcal{H})$ which can be created by acting in $\\mathcal{M}$ are\nidentical to the logical operators whose expectation values cannot be altered\nby acting in the commutant $\\mathcal{M}^\\prime$, and vice versa. In\narXiv:2110.14691, a uniqueness theorem was stated: the only possible tuple of\n(code, bipartition, algebra) which can exhibit complementary recovery is the\nmaximal one $\\mathcal{M}=P(\\mathcal{L}(\\mathcal{H}_A)\\otimes\nI_{\\mathcal{H}_{\\bar A}})P$. We point out a counterexample to this result,\nusing a ``non-adjacent'' bipartition of a four-qubit code proposed in\narXiv:2110.14691. We show that the failure of uniqueness is due to a failure to\nenforce error correction against erasure of $\\mathcal{H}_{\\bar A}$, which\nrequires enforcing the algebraic Knill-Laflamme condition $[P E_i^\\dagger E_j\nP,\\mathcal{M}]=0$ for each pair of error operators. When we add the additional\nrequirement that $\\mathcal{M}$ be correctable with respect to this channel,\nuniqueness is restored, and we re-prove the theorem of arXiv:2110.14691 with\nthis added assumption. We present the list of bipartitions of the ``atomic''\nholographic codes in arXiv:2110.14691 in which the correctability assumption\ncan be violated.", "AI": {"tldr": "\u5168\u606f\u7801\u662f\u4e00\u79cd\u5177\u6709\u51e0\u4f55\u7ed3\u6784\u7684\u7ea0\u9519\u7801\uff0c\u5176\u5173\u952e\u7279\u6027\u662f\u201c\u4e92\u8865\u6062\u590d\u201d\u3002\u8be5\u8bba\u6587\u6307\u51fa\uff0c\u4e4b\u524d\u58f0\u79f0\u7684\u552f\u4e00\u6027\u5b9a\u7406\uff08\u5373\u53ea\u6709\u6700\u5927\u4ee3\u6570\u624d\u80fd\u6ee1\u8db3\u4e92\u8865\u6062\u590d\uff09\u5b58\u5728\u53cd\u4f8b\u3002\u901a\u8fc7\u4f7f\u7528\u4e00\u4e2a\u56db\u6bd4\u7279\u7801\u7684\u4e00\u4e2a\u201c\u975e\u76f8\u90bb\u201d\u5206\u533a\uff0c\u5e76\u5f15\u5165\u5bf9\u64e6\u9664\u7684\u7ea0\u9519\u8981\u6c42\uff08\u5373\u6ee1\u8db3\u4ee3\u6570Knill-Laflamme\u6761\u4ef6\uff09\uff0c\u8bba\u6587\u6062\u590d\u4e86\u552f\u4e00\u6027\uff0c\u5e76\u91cd\u65b0\u8bc1\u660e\u4e86\u8be5\u5b9a\u7406\u3002\u6700\u540e\uff0c\u8bba\u6587\u5217\u51fa\u4e86\u201c\u539f\u5b50\u201d\u5168\u606f\u7801\u4e2d\u53ef\u80fd\u8fdd\u53cd\u7ea0\u9519\u5047\u8bbe\u7684\u5206\u533a\u3002", "motivation": "\u63a2\u7a76\u5168\u606f\u7801\u7684\u4e92\u8865\u6062\u590d\u7279\u6027\uff0c\u5e76\u4fee\u6b63\u4e4b\u524d\u5173\u4e8e\u552f\u4e00\u6027\u5b9a\u7406\u7684\u9648\u8ff0\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u56db\u6bd4\u7279\u7801\u7684\u201c\u975e\u76f8\u90bb\u201d\u5206\u533a\u4f5c\u4e3a\u53cd\u4f8b\uff0c\u5e76\u5f15\u5165\u5bf9\u64e6\u9664\u7684\u7ea0\u9519\u8981\u6c42\uff08\u4ee3\u6570Knill-Laflamme\u6761\u4ef6\uff09\uff0c\u7136\u540e\u91cd\u65b0\u8bc1\u660e\u552f\u4e00\u6027\u5b9a\u7406\u3002", "result": "\u8bc1\u660e\u4e86\u4e4b\u524d\u5168\u606f\u7801\u552f\u4e00\u6027\u5b9a\u7406\u7684\u53cd\u4f8b\uff0c\u5e76\u901a\u8fc7\u6dfb\u52a0\u7ea0\u9519\u8981\u6c42\u6062\u590d\u4e86\u552f\u4e00\u6027\uff0c\u5e76\u91cd\u65b0\u8bc1\u660e\u4e86\u8be5\u5b9a\u7406\u3002\u6700\u540e\u5217\u51fa\u4e86\u539f\u5b50\u5168\u606f\u7801\u4e2d\u53ef\u80fd\u8fdd\u53cd\u7ea0\u9519\u5047\u8bbe\u7684\u5206\u533a\u3002", "conclusion": "\u5168\u606f\u7801\u7684\u4e92\u8865\u6062\u590d\u7279\u6027\u4f9d\u8d56\u4e8e\u7ea0\u9519\uff08\u7279\u522b\u662f\u5bf9\u64e6\u9664\u7684\u7ea0\u9519\uff09\u8981\u6c42\u3002\u5728\u6ee1\u8db3\u6b64\u8981\u6c42\u4e0b\uff0c\u5168\u606f\u7801\u7684\u4ee3\u6570\u7ed3\u6784\u5177\u6709\u552f\u4e00\u6027\u3002"}}
{"id": "2509.18162", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18162", "abs": "https://arxiv.org/abs/2509.18162", "authors": ["Meraryslan Meraliyev", "Cemil Turan", "Shirali Kadyrov"], "title": "A Simple and Reproducible Hybrid Solver for a Truck-Drone VRP with Recharge", "comment": null, "summary": "We study last-mile delivery with one truck and one drone under explicit\nbattery management: the drone flies at twice the truck speed; each sortie must\nsatisfy an endurance budget; after every delivery the drone recharges on the\ntruck before the next launch. We introduce a hybrid reinforcement learning (RL)\nsolver that couples an ALNS-based truck tour (with 2/3-opt and Or-opt) with a\nsmall pointer/attention policy that schedules drone sorties. The policy decodes\nlaunch--serve--rendezvous triplets with hard feasibility masks for endurance\nand post-delivery recharge; a fast, exact timeline simulator enforces\nlaunch/recovery handling and computes the true makespan used by masked\ngreedy/beam decoding. On Euclidean instances with $N{=}50$, $E{=}0.7$, and\n$R{=}0.1$, the method achieves an average makespan of \\textbf{5.203}$\\pm$0.093,\nversus \\textbf{5.349}$\\pm$0.038 for ALNS and \\textbf{5.208}$\\pm$0.124 for NN --\ni.e., \\textbf{2.73\\%} better than ALNS on average and within \\textbf{0.10\\%} of\nNN. Per-seed, the RL scheduler never underperforms ALNS on the same instance\nand ties or beats NN on two of three seeds. A decomposition of the makespan\nshows the expected truck--wait trade-off across heuristics; the learned\nscheduler balances both to minimize the total completion time. We provide a\nconfig-first implementation with plotting and significance-test utilities to\nsupport replication.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u6c42\u89e3\u5668\uff0c\u7528\u4e8e\u89e3\u51b3\u914d\u5907\u7535\u6c60\u7ba1\u7406\u7684\u6700\u540e\u4e00\u82f1\u91cc\u914d\u9001\u95ee\u9898\uff0c\u5176\u4e2d\u4e00\u8f86\u5361\u8f66\u548c\u4e00\u67b6\u65e0\u4eba\u673a\u534f\u540c\u5de5\u4f5c\u3002\u8be5\u6c42\u89e3\u5668\u7ed3\u5408\u4e86\u57fa\u4e8eALNS\u7684\u5361\u8f66\u8def\u5f84\u89c4\u5212\u548c\u57fa\u4e8e\u6307\u9488/\u6ce8\u610f\u529b\u673a\u5236\u7684\u65e0\u4eba\u673a\u51fa\u51fb\u8c03\u5ea6\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u89e3\u51b3\u6700\u540e\u4e00\u82f1\u91cc\u914d\u9001\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u8003\u8651\u7535\u6c60\u7ba1\u7406\u3001\u65e0\u4eba\u673a\u901f\u5ea6\u3001\u7eed\u822a\u91cc\u7a0b\u548c\u5145\u7535\u7b56\u7565\u7684\u60c5\u51b5\u4e0b\uff0c\u4f18\u5316\u5361\u8f66\u548c\u65e0\u4eba\u673a\u7684\u534f\u540c\u914d\u9001\u6548\u7387\u3002", "method": "\u91c7\u7528\u6df7\u5408\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u5e26\u67092/3-opt\u548cOr-opt\u7684ALNS\u7b97\u6cd5\u8fdb\u884c\u5361\u8f66\u8def\u5f84\u89c4\u5212\uff0c\u4ee5\u53ca\u4e00\u4e2a\u6307\u9488/\u6ce8\u610f\u529b\u7b56\u7565\u6765\u8c03\u5ea6\u65e0\u4eba\u673a\u51fa\u51fb\u3002\u8be5\u7b56\u7565\u80fd\u591f\u89e3\u7801\u53d1\u5c04-\u670d\u52a1-\u4f1a\u5408\u4e09\u5143\u7ec4\uff0c\u5e76\u8003\u8651\u7eed\u822a\u548c\u5145\u7535\u7684\u53ef\u884c\u6027\u7ea6\u675f\u3002\u4f7f\u7528\u7cbe\u786e\u7684\u65f6\u95f4\u7ebf\u6a21\u62df\u5668\u6765\u5f3a\u5236\u6267\u884c\u53d1\u5c04/\u56de\u6536\u5904\u7406\u5e76\u8ba1\u7b97\u603b\u65f6\u95f4\u3002", "result": "\u5728\u6b27\u51e0\u91cc\u5f97\u5b9e\u4f8b\uff08N=50, E=0.7, R=0.1\uff09\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u7684\u5e73\u5747\u603b\u65f6\u95f4\u4e3a5.203\u00b10.093\uff0c\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u4ec5\u4f7f\u7528ALNS\u7684\u4e3a5.349\u00b10.038\uff0c\u4ec5\u4f7f\u7528NN\u7684\u4e3a5.208\u00b10.124\u3002RL\u6c42\u89e3\u5668\u6bd4ALNS\u5e73\u5747\u5feb2.73%\uff0c\u5e76\u4e14\u5728\u63a5\u8fd1NN\u7684\u6027\u80fd\u3002", "conclusion": "RL\u6c42\u89e3\u5668\u5728\u6bcf\u6b21\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4e0d\u900a\u4e8eALNS\uff0c\u5e76\u4e14\u5728\u4e09\u5206\u4e4b\u4e8c\u7684\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8e\u6216\u7b49\u4e8eNN\u3002\u901a\u8fc7\u5bf9\u603b\u65f6\u95f4\u7684\u5206\u89e3\u5206\u6790\uff0c\u63ed\u793a\u4e86\u5361\u8f66\u7b49\u5f85\u65f6\u95f4\u548c\u65e0\u4eba\u673a\u4efb\u52a1\u65f6\u95f4\u4e4b\u95f4\u7684\u6743\u8861\u5173\u7cfb\uff0cRL\u8c03\u5ea6\u5668\u80fd\u591f\u6709\u6548\u5730\u5e73\u8861\u8fd9\u4e24\u8005\u4ee5\u6700\u5c0f\u5316\u603b\u5b8c\u6210\u65f6\u95f4\u3002"}}
{"id": "2509.18986", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18986", "abs": "https://arxiv.org/abs/2509.18986", "authors": ["Erik Penther", "Michael Grohs", "Jana-Rebecca Rehse"], "title": "Remaining Time Prediction in Outbound Warehouse Processes: A Case Study (Short Paper)", "comment": "Short paper at the ML4PM Workshop 2025, held in conjunction with the\n  ICPM 2025 in Montevideo, Uruguay", "summary": "Predictive process monitoring is a sub-domain of process mining which aims to\nforecast the future of ongoing process executions. One common prediction target\nis the remaining time, meaning the time that will elapse until a process\nexecution is completed. In this paper, we compare four different remaining time\nprediction approaches in a real-life outbound warehouse process of a logistics\ncompany in the aviation business. For this process, the company provided us\nwith a novel and original event log with 169,523 traces, which we can make\npublicly available. Unsurprisingly, we find that deep learning models achieve\nthe highest accuracy, but shallow methods like conventional boosting techniques\nachieve competitive accuracy and require significantly fewer computational\nresources.", "AI": {"tldr": "We compared four remaining time prediction methods on a real-life logistics dataset. Deep learning models were most accurate, but boosting techniques offered competitive accuracy with lower computational costs.", "motivation": "Predictive process monitoring aims to forecast future process executions, specifically predicting the remaining time until completion. This paper investigates different approaches for this task.", "method": "The study compares four different remaining time prediction methods using a novel event log from a real-life outbound warehouse process in the aviation logistics industry. The log contains 169,523 traces and will be made publicly available.", "result": "Deep learning models demonstrated the highest accuracy in predicting remaining time. However, shallow methods, such as conventional boosting techniques, achieved comparable accuracy while requiring substantially fewer computational resources.", "conclusion": "While deep learning models offer the best accuracy for remaining time prediction, traditional boosting techniques provide a viable alternative with significantly lower computational demands, making them a practical choice for certain applications."}}
{"id": "2509.19020", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.19020", "abs": "https://arxiv.org/abs/2509.19020", "authors": ["Shaomu Tan", "Ryosuke Mitani", "Ritvik Choudhary", "Toshiyuki Sekiya"], "title": "Investigating Test-Time Scaling with Reranking for Machine Translation", "comment": null, "summary": "Scaling model parameters has become the de facto strategy for improving NLP\nsystems, but it comes with substantial computational costs. Test-Time Scaling\n(TTS) offers an alternative by allocating more computation at inference:\ngenerating multiple candidates and selecting the best. While effective in tasks\nsuch as mathematical reasoning, TTS has not been systematically explored for\nmachine translation (MT). In this paper, we present the first systematic study\nof TTS for MT, investigating a simple but practical best-of-N framework on\nWMT24 benchmarks. Our experiments cover six high-resource and one low-resource\nlanguage pairs, five model sizes (3B-72B), and various TTS compute budget (N up\nto 1024). Our results show that a) For high-resource languages, TTS generally\nimproves translation quality according to multiple neural MT evaluation\nmetrics, and our human evaluation confirms these gains; b) Augmenting smaller\nmodels with large $N$ can match or surpass larger models at $N{=}1$ with more\ncompute cost; c) Under fixed compute budgets, larger models are typically more\nefficient, and TTS can degrade quality due to metric blind spots in\nlow-resource cases.", "AI": {"tldr": "\u901a\u8fc7\u5728\u63a8\u7406\u65f6\u751f\u6210\u591a\u4e2a\u5019\u9009\u5e76\u9009\u62e9\u6700\u4f73\u7684\u6d4b\u8bd5\u65f6\u6269\u5c55\uff08TTS\uff09\u65b9\u6cd5\uff0c\u53ef\u4ee5\u63d0\u9ad8\u673a\u5668\u7ffb\u8bd1\uff08MT\uff09\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u8d44\u6e90\u8bed\u8a00\u5bf9\u4e0a\u3002\u7136\u800c\uff0c\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u5bf9\u4e0a\uff0cTTS\u53ef\u80fd\u4f1a\u56e0\u6307\u6807\u7684\u5c40\u9650\u6027\u800c\u964d\u4f4e\u7ffb\u8bd1\u8d28\u91cf\u3002", "motivation": "\u63d0\u9ad8\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u540c\u65f6\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u7279\u522b\u662f\u5728\u673a\u5668\u7ffb\u8bd1\uff08MT\uff09\u9886\u57df\uff0c\u5e76\u7cfb\u7edf\u5730\u7814\u7a76\u6d4b\u8bd5\u65f6\u6269\u5c55\uff08TTS\uff09\u5728\u8be5\u9886\u57df\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u5b9e\u7528\u7684\u2018N\u4e2d\u9009\u4f18\u2019\uff08best-of-N\uff09\u6846\u67b6\uff0c\u5e76\u5728WMT24\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fdb\u884c\u4e86\u5b9e\u9a8c\u3002\u7814\u7a76\u8303\u56f4\u6db5\u76d6\u4e86\u516d\u79cd\u9ad8\u8d44\u6e90\u548c\u4e00\u79cd\u4f4e\u8d44\u6e90\u8bed\u8a00\u5bf9\uff0c\u4e94\u79cd\u4e0d\u540c\u6a21\u578b\u5c3a\u5bf8\uff083B-72B\uff09\uff0c\u4ee5\u53ca\u4e0d\u540c\u7684TTS\u8ba1\u7b97\u9884\u7b97\uff08N\u9ad8\u8fbe1024\uff09\u3002", "result": "\u5728\u9ad8\u8d44\u6e90\u8bed\u8a00\u5bf9\u4e0a\uff0cTTS\u666e\u904d\u63d0\u9ad8\u4e86\u7ffb\u8bd1\u8d28\u91cf\uff0c\u5e76\u901a\u8fc7\u4eba\u5de5\u8bc4\u4f30\u5f97\u5230\u8bc1\u5b9e\u3002\u901a\u8fc7\u589e\u52a0N\u503c\uff0c\u53ef\u4ee5\u4f7f\u8f83\u5c0f\u7684\u6a21\u578b\u8fbe\u5230\u751a\u81f3\u8d85\u8fc7\u8f83\u5927\u7684\u6a21\u578b\uff08N=1\uff09\u7684\u6027\u80fd\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u66f4\u9ad8\u3002\u5728\u56fa\u5b9a\u8ba1\u7b97\u9884\u7b97\u4e0b\uff0c\u8f83\u5927\u7684\u6a21\u578b\u901a\u5e38\u66f4\u6709\u6548\u7387\u3002\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u5bf9\u4e0a\uff0cTTS\u53ef\u80fd\u56e0\u6307\u6807\u7684\u5c40\u9650\u6027\u800c\u964d\u4f4e\u7ffb\u8bd1\u8d28\u91cf\u3002", "conclusion": "TTS\u662f\u4e00\u79cd\u6709\u524d\u666f\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u63d0\u9ad8\u673a\u5668\u7ffb\u8bd1\u7684\u8d28\u91cf\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u8d44\u6e90\u8bed\u8a00\u5bf9\u4e0a\u3002\u7136\u800c\uff0c\u5176\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u5bf9\u4e0a\u7684\u6709\u6548\u6027\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\uff0c\u5e76\u4e14\u9700\u8981\u5728\u6027\u80fd\u63d0\u5347\u548c\u8ba1\u7b97\u6210\u672c\u4e4b\u95f4\u8fdb\u884c\u6743\u8861\u3002"}}
{"id": "2509.19047", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.19047", "abs": "https://arxiv.org/abs/2509.19047", "authors": ["Geonhyup Lee", "Yeongjin Lee", "Kangmin Kim", "Seongju Lee", "Sangjun Noh", "Seunghyeok Back", "Kyoobin Lee"], "title": "ManipForce: Force-Guided Policy Learning with Frequency-Aware Representation for Contact-Rich Manipulation", "comment": "9 pages, 9 figures", "summary": "Contact-rich manipulation tasks such as precision assembly require precise\ncontrol of interaction forces, yet existing imitation learning methods rely\nmainly on vision-only demonstrations. We propose ManipForce, a handheld system\ndesigned to capture high-frequency force-torque (F/T) and RGB data during\nnatural human demonstrations for contact-rich manipulation. Building on these\ndemonstrations, we introduce the Frequency-Aware Multimodal Transformer (FMT).\nFMT encodes asynchronous RGB and F/T signals using frequency- and\nmodality-aware embeddings and fuses them via bi-directional cross-attention\nwithin a transformer diffusion policy. Through extensive experiments on six\nreal-world contact-rich manipulation tasks - such as gear assembly, box\nflipping, and battery insertion - FMT trained on ManipForce demonstrations\nachieves robust performance with an average success rate of 83% across all\ntasks, substantially outperforming RGB-only baselines. Ablation and\nsampling-frequency analyses further confirm that incorporating high-frequency\nF/T data and cross-modal integration improves policy performance, especially in\ntasks demanding high precision and stable contact.", "AI": {"tldr": "ManipForce\u662f\u4e00\u4e2a\u624b\u6301\u7cfb\u7edf\uff0c\u7528\u4e8e\u6355\u6349\u673a\u5668\u4eba\u64cd\u4f5c\u8fc7\u7a0b\u4e2d\u7684\u529b\u548c\u626d\u77e9\uff08F/T\uff09\u53caRGB\u6570\u636e\u3002\u7ed3\u5408\u8be5\u6570\u636e\uff0c\u7814\u7a76\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFMT\uff08Frequency-Aware Multimodal Transformer\uff09\u7684\u7b56\u7565\uff0c\u901a\u8fc7\u7f16\u7801\u548c\u878d\u5408\u5f02\u6b65\u7684RGB\u548cF/T\u4fe1\u53f7\uff0c\u4ee5Transformer\u6269\u6563\u7b56\u7565\u7684\u5f62\u5f0f\u6765\u5904\u7406\u63a5\u89e6\u5f0f\u64cd\u4f5c\u4efb\u52a1\u3002\u57286\u4e2a\u73b0\u5b9e\u4e16\u754c\u7684\u63a5\u89e6\u5f0f\u64cd\u4f5c\u4efb\u52a1\u4e2d\uff0cFMT\u7b56\u7565\u53d6\u5f97\u4e8683%\u7684\u5e73\u5747\u6210\u529f\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u4ec5\u4f7f\u7528RGB\u6570\u636e\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u9ad8\u9891F/T\u6570\u636e\u548c\u8de8\u6a21\u6001\u878d\u5408\u5bf9\u4e8e\u9700\u8981\u9ad8\u7cbe\u5ea6\u548c\u7a33\u5b9a\u63a5\u89e6\u7684\u4efb\u52a1\u5c24\u4e3a\u91cd\u8981\u3002", "motivation": "\u73b0\u6709\u7684\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u4e8e\u89c6\u89c9\u6f14\u793a\uff0c\u96be\u4ee5\u7cbe\u786e\u63a7\u5236\u63a5\u89e6\u5f0f\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u4ea4\u4e92\u529b\uff0c\u800c\u8fd9\u7c7b\u4efb\u52a1\uff08\u5982\u7cbe\u5bc6\u88c5\u914d\uff09\u9700\u8981\u7cbe\u786e\u7684\u529b\u63a7\u5236\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aManipForce\u7684\u624b\u6301\u7cfb\u7edf\uff0c\u7528\u4e8e\u6355\u6349\u81ea\u7136\u4eba\u6f14\u793a\u8fc7\u7a0b\u4e2d\u9ad8\u9891\u7684\u529b\u548c\u626d\u77e9\uff08F/T\uff09\u4ee5\u53caRGB\u6570\u636e\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFMT\uff08Frequency-Aware Multimodal Transformer\uff09\u7684\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u4f7f\u7528\u9891\u7387\u548c\u6a21\u6001\u611f\u77e5\u7684\u5d4c\u5165\u6765\u7f16\u7801\u5f02\u6b65\u7684RGB\u548cF/T\u4fe1\u53f7\uff0c\u5e76\u901a\u8fc7\u53cc\u5411\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u5728Transformer\u6269\u6563\u7b56\u7565\u4e2d\u878d\u5408\u8fd9\u4e9b\u4fe1\u606f\u3002", "result": "\u57286\u4e2a\u73b0\u5b9e\u4e16\u754c\u7684\u63a5\u89e6\u5f0f\u64cd\u4f5c\u4efb\u52a1\uff08\u5982\u9f7f\u8f6e\u7ec4\u88c5\u3001\u7ffb\u7bb1\u548c\u7535\u6c60\u63d2\u5165\uff09\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u3002FMT\u7b56\u7565\u5728ManipForce\u6f14\u793a\u4e0b\u8bad\u7ec3\uff0c\u5728\u6240\u6709\u4efb\u52a1\u4e0a\u7684\u5e73\u5747\u6210\u529f\u7387\u4e3a83%\uff0c\u663e\u8457\u4f18\u4e8e\u4ec5RGB\u6570\u636e\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002\u6d88\u878d\u7814\u7a76\u548c\u91c7\u6837\u9891\u7387\u5206\u6790\u8fdb\u4e00\u6b65\u8bc1\u5b9e\uff0c\u5305\u542b\u9ad8\u9891F/T\u6570\u636e\u548c\u8de8\u6a21\u6001\u96c6\u6210\u53ef\u4ee5\u63d0\u9ad8\u7b56\u7565\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u9ad8\u7cbe\u5ea6\u548c\u7a33\u5b9a\u63a5\u89e6\u7684\u4efb\u52a1\u4e2d\u3002", "conclusion": "\u9ad8\u9891\u7684\u529b\u548c\u626d\u77e9\u6570\u636e\u5bf9\u4e8e\u63d0\u9ad8\u63a5\u89e6\u5f0f\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u673a\u5668\u4eba\u7b56\u7565\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u9700\u8981\u9ad8\u7cbe\u5ea6\u548c\u7a33\u5b9a\u63a5\u89e6\u7684\u4efb\u52a1\u3002ManipForce\u7cfb\u7edf\u548cFMT\u7b56\u7565\u7684\u7ed3\u5408\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u3002"}}
{"id": "2509.18591", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18591", "abs": "https://arxiv.org/abs/2509.18591", "authors": ["Pengchao Deng", "Shengqi Chen"], "title": "Enhancing Video Object Segmentation in TrackRAD Using XMem Memory Network", "comment": null, "summary": "This paper presents an advanced tumor segmentation framework for real-time\nMRI-guided radiotherapy, designed for the TrackRAD2025 challenge. Our method\nleverages the XMem model, a memory-augmented architecture, to segment tumors\nacross long cine-MRI sequences. The proposed system efficiently integrates\nmemory mechanisms to track tumor motion in real-time, achieving high\nsegmentation accuracy even under challenging conditions with limited annotated\ndata. Unfortunately, the detailed experimental records have been lost,\npreventing us from reporting precise quantitative results at this stage.\nNevertheless, From our preliminary impressions during development, the\nXMem-based framework demonstrated reasonable segmentation performance and\nsatisfied the clinical real-time requirement. Our work contributes to improving\nthe precision of tumor tracking during MRI-guided radiotherapy, which is\ncrucial for enhancing the accuracy and safety of cancer treatments.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u5b9e\u65f6MRI\u5f15\u5bfc\u653e\u7597\u7684\u5148\u8fdb\u80bf\u7624\u5206\u5272\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528XMem\u6a21\u578b\u6765\u5904\u7406\u957f\u5e8f\u5217\u7684cine-MRI\u6570\u636e\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u548c\u5b9e\u65f6\u6027\u3002\u867d\u7136\u5177\u4f53\u7684\u5b9e\u9a8c\u6570\u636e\u4e22\u5931\uff0c\u4f46\u521d\u6b65\u7ed3\u679c\u663e\u793a\u5176\u5206\u5272\u6027\u80fd\u5408\u7406\uff0c\u6ee1\u8db3\u4e34\u5e8a\u5b9e\u65f6\u6027\u8981\u6c42\u3002", "motivation": "\u4e3aTrackRAD2025\u6311\u6218\u63d0\u4f9b\u4e00\u4e2a\u5148\u8fdb\u7684\u3001\u80fd\u591f\u5b9e\u65f6\u5206\u5272\u80bf\u7624\u7684MRI\u5f15\u5bfc\u653e\u7597\u6846\u67b6\uff0c\u4ee5\u63d0\u9ad8\u764c\u75c7\u6cbb\u7597\u7684\u7cbe\u786e\u6027\u548c\u5b89\u5168\u6027\u3002", "method": "\u5229\u7528XMem\u6a21\u578b\uff0c\u4e00\u4e2a\u5e26\u6709\u8bb0\u5fc6\u673a\u5236\u7684\u67b6\u6784\uff0c\u5bf9\u957f\u5e8f\u5217\u7684cine-MRI\u6570\u636e\u8fdb\u884c\u80bf\u7624\u5206\u5272\uff0c\u5e76\u5b9e\u65f6\u8ddf\u8e2a\u80bf\u7624\u8fd0\u52a8\u3002", "result": "\u7531\u4e8e\u5b9e\u9a8c\u8bb0\u5f55\u4e22\u5931\uff0c\u65e0\u6cd5\u63d0\u4f9b\u7cbe\u786e\u7684\u91cf\u5316\u7ed3\u679c\u3002\u4f46\u521d\u6b65\u5370\u8c61\u8868\u660e\uff0c\u57fa\u4e8eXMem\u7684\u6846\u67b6\u5206\u5272\u6027\u80fd\u5408\u7406\uff0c\u6ee1\u8db3\u5b9e\u65f6\u6027\u8981\u6c42\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u52a9\u4e8e\u63d0\u9ad8MRI\u5f15\u5bfc\u653e\u7597\u4e2d\u80bf\u7624\u8ddf\u8e2a\u7684\u7cbe\u5ea6\uff0c\u4e3a\u764c\u75c7\u6cbb\u7597\u7684\u7cbe\u786e\u6027\u548c\u5b89\u5168\u6027\u505a\u51fa\u8d21\u732e\u3002"}}
{"id": "2509.18483", "categories": ["cs.LG", "quant-ph"], "pdf": "https://arxiv.org/pdf/2509.18483", "abs": "https://arxiv.org/abs/2509.18483", "authors": ["Abhijit Sen", "Illya V. Lukin", "Kurt Jacobs", "Lev Kaplan", "Andrii G. Sotnikov", "Denys I. Bondar"], "title": "Physics-informed time series analysis with Kolmogorov-Arnold Networks under Ehrenfest constraints", "comment": null, "summary": "The prediction of quantum dynamical responses lies at the heart of modern\nphysics. Yet, modeling these time-dependent behaviors remains a formidable\nchallenge because quantum systems evolve in high-dimensional Hilbert spaces,\noften rendering traditional numerical methods computationally prohibitive.\nWhile large language models have achieved remarkable success in sequential\nprediction, quantum dynamics presents a fundamentally different challenge:\nforecasting the entire temporal evolution of quantum systems rather than merely\nthe next element in a sequence. Existing neural architectures such as recurrent\nand convolutional networks often require vast training datasets and suffer from\nspurious oscillations that compromise physical interpretability. In this work,\nwe introduce a fundamentally new approach: Kolmogorov Arnold Networks (KANs)\naugmented with physics-informed loss functions that enforce the Ehrenfest\ntheorems. Our method achieves superior accuracy with significantly less\ntraining data: it requires only 5.4 percent of the samples (200) compared to\nTemporal Convolution Networks (3,700). We further introduce the Chain of KANs,\na novel architecture that embeds temporal causality directly into the model\ndesign, making it particularly well-suited for time series modeling. Our\nresults demonstrate that physics-informed KANs offer a compelling advantage\nover conventional black-box models, maintaining both mathematical rigor and\nphysical consistency while dramatically reducing data requirements.", "AI": {"tldr": "KANs\u7ed3\u5408\u7269\u7406\u4fe1\u606f\u635f\u5931\u51fd\u6570\u5728\u91cf\u5b50\u52a8\u529b\u5b66\u9884\u6d4b\u65b9\u9762\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u6a21\u578b\uff0c\u6570\u636e\u9700\u6c42\u66f4\u5c11\u3002", "motivation": "\u91cf\u5b50\u7cfb\u7edf\u7684\u65f6\u53d8\u884c\u4e3a\u5efa\u6a21\u56e0\u9ad8\u7ef4\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u800c\u9762\u4e34\u8ba1\u7b97\u6311\u6218\uff0c\u73b0\u6709\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6570\u636e\u4e14\u6613\u51fa\u73b0\u4f2a\u632f\u8361\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528\u7ed3\u5408\u7269\u7406\u4fe1\u606f\u635f\u5931\u51fd\u6570\u7684Kolmogorov Arnold Networks (KANs)\uff0c\u5f3a\u5236\u6267\u884cEhrenfest\u5b9a\u7406\uff0c\u5e76\u5f15\u5165\u4e86Chain of KANs\u67b6\u6784\u3002", "result": "KANs\u4ec5\u97005.4%\u7684\u8bad\u7ec3\u6570\u636e\uff08200\u4e2a\u6837\u672c\uff09\u5373\u53ef\u8fbe\u5230\u6bd4Temporal Convolution Networks\uff083700\u4e2a\u6837\u672c\uff09\u66f4\u9ad8\u7684\u7cbe\u5ea6\u3002", "conclusion": "\u7269\u7406\u4fe1\u606fKANs\u5728\u4fdd\u6301\u6570\u5b66\u4e25\u8c28\u6027\u548c\u7269\u7406\u4e00\u81f4\u6027\u7684\u540c\u65f6\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u6570\u636e\u9700\u6c42\uff0c\u4e3a\u91cf\u5b50\u52a8\u529b\u5b66\u9884\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u529b\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2509.18164", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18164", "abs": "https://arxiv.org/abs/2509.18164", "authors": ["Ranfei Chen", "Ming Chen"], "title": "DSFT: Inspiring Diffusion Large Language Models to Comprehend Mathematical and Logical Patterns", "comment": null, "summary": "Diffusion large language models (dLLMs) have emerged as a new architecture\nfollowing auto regressive models. Their denoising process offers a powerful\ngenerative advantage, but they present significant challenges in learning and\nunderstanding numerically sensitive mathematical and order-sensitive logical\ntasks. Current training methods, including pre-training, fine-tuning, and\nreinforcement learning, focus primarily on improving general knowledge\nretention and reasoning abilities, but lack a comprehensive understanding of\nmathematical and logical patterns. We propose DSFT, a simple yet effective\nDiffusion SFT strategy, by adjusting the masking strategy and loss function,\nguiding models to understand mathematical and logical patterns. This strategy\ncan be flexibly combined with pre-training, reinforcement learning, and other\ntraining methods. Validated on models such as LLaDA and Dream series, we prove\nthat DSFT on small-scale data can achieve improvements of 5-10% and\napproximately 2% on mathematical and logical problems, respectively. This\ninspiring masking approach offers insights for future learning of specific\npatterns, which can be easily and efficiently combined with other training\nmethods and applied to various dLLMs. Our code is publicly available at\nhttps://anonymous.4open.science/r/DSFT-0FFB/", "AI": {"tldr": "DSFT\u662f\u4e00\u79cd\u901a\u8fc7\u8c03\u6574\u63a9\u7801\u7b56\u7565\u548c\u635f\u5931\u51fd\u6570\u6765\u589e\u5f3a\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\uff08dLLM\uff09\u5728\u6570\u5b66\u548c\u903b\u8f91\u4efb\u52a1\u4e0a\u8868\u73b0\u7684\u7b56\u7565\uff0c\u80fd\u5728\u5c0f\u89c4\u6a21\u6570\u636e\u4e0a\u5b9e\u73b05-10%\u7684\u6570\u5b66\u80fd\u529b\u548c\u7ea62%\u7684\u903b\u8f91\u80fd\u529b\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u7684dLLM\u8bad\u7ec3\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u901a\u7528\u77e5\u8bc6\u548c\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5728\u5904\u7406\u6570\u503c\u654f\u611f\u7684\u6570\u5b66\u548c\u987a\u5e8f\u654f\u611f\u7684\u903b\u8f91\u4efb\u52a1\u65b9\u9762\u5b58\u5728\u6311\u6218\u3002", "method": "\u63d0\u51faDSFT\uff08Diffusion SFT\uff09\u7b56\u7565\uff0c\u901a\u8fc7\u8c03\u6574\u63a9\u7801\u7b56\u7565\u548c\u635f\u5931\u51fd\u6570\u6765\u5f15\u5bfc\u6a21\u578b\u5b66\u4e60\u6570\u5b66\u548c\u903b\u8f91\u6a21\u5f0f\uff0c\u5e76\u53ef\u7075\u6d3b\u7ec4\u5408\u5176\u4ed6\u8bad\u7ec3\u65b9\u6cd5\u3002", "result": "\u5728LLaDA\u548cDream\u7cfb\u5217\u6a21\u578b\u4e0a\u9a8c\u8bc1\uff0cDSFT\u5728\u6570\u5b66\u548c\u903b\u8f91\u95ee\u9898\u4e0a\u5206\u522b\u5b9e\u73b0\u4e865-10%\u548c\u7ea62%\u7684\u63d0\u5347\u3002", "conclusion": "DSFT\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u7b56\u7565\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347dLLM\u5728\u6570\u5b66\u548c\u903b\u8f91\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u4e3a\u672a\u6765\u7279\u5b9a\u6a21\u5f0f\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002"}}
{"id": "2509.19030", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19030", "abs": "https://arxiv.org/abs/2509.19030", "authors": ["Victoire Herv\u00e9", "Henrik Warpefelt", "Christoph Salge"], "title": "Landmarks, Monuments, and Beacons: Understanding Generative Calls to Action", "comment": null, "summary": "Algorithmic evaluation of procedurally generated content struggles to find\nmetrics that align with human experience, particularly for composite artefacts.\nAutomatic decomposition as a possible solution requires concepts that meet a\nrange of properties. To this end, drawing on Games Studies and Game AI\nresearch, we introduce the nested concepts of \\textit{Landmarks},\n\\textit{Monuments}, and \\textit{Beacons}. These concepts are based on the\nartefact's perceivability, evocativeness, and Call to Action, all from a\nplayer-centric perspective. These terms are generic to games and usable across\ngenres. We argue that these entities can be found and evaluated with techniques\ncurrently used in both research and industry, opening a path towards a fully\nautomated decomposition of PCG, and evaluation of the salient sub-components.\nAlthough the work presented here emphasises mixed-initiative PCG and\ncompositional PCG, we believe it applies beyond those domains. With this\napproach, we intend to create a connection between humanities and technical\ngame research and allow for better computational PCG evaluation", "AI": {"tldr": "\u81ea\u52a8\u8bc4\u4f30\u7a0b\u5e8f\u751f\u6210\u5185\u5bb9\uff08PCG\uff09\u5b58\u5728\u56f0\u96be\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u590d\u5408\u4ea7\u7269\u65f6\u3002\u4e3a\u89e3\u51b3\u6b64\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u57fa\u4e8e\u53ef\u611f\u77e5\u6027\u3001\u5524\u8d77\u6027\u548c\u884c\u52a8\u53f7\u53ec\u7684\u201c\u5730\u6807\u201d\u3001\u201c\u7eaa\u5ff5\u7891\u201d\u548c\u201c\u4fe1\u6807\u201d\u7b49\u5d4c\u5957\u6982\u5ff5\uff0c\u4ee5\u73a9\u5bb6\u4e3a\u4e2d\u5fc3\uff0c\u53ef\u8de8\u6e38\u620f\u7c7b\u578b\u901a\u7528\u3002\u8fd9\u4e9b\u6982\u5ff5\u53ef\u4ee5\u901a\u8fc7\u73b0\u6709\u6280\u672f\u8fdb\u884c\u8bc6\u522b\u548c\u8bc4\u4f30\uff0c\u4ece\u800c\u5b9e\u73b0PCG\u7684\u5b8c\u5168\u81ea\u52a8\u5316\u5206\u89e3\u548c\u91cd\u8981\u5b50\u7ec4\u4ef6\u7684\u8bc4\u4f30\uff0c\u8fde\u63a5\u4eba\u6587\u5b66\u79d1\u4e0e\u6e38\u620f\u6280\u672f\u7814\u7a76\uff0c\u5e76\u6539\u8fdbPCG\u8bc4\u4f30\u3002", "motivation": "\u76ee\u524d\u7684\u7b97\u6cd5\u8bc4\u4f30\u65b9\u6cd5\u96be\u4ee5\u627e\u5230\u4e0e\u4eba\u7c7b\u4f53\u9a8c\u4e00\u81f4\u7684PCG\uff08\u7a0b\u5e8f\u751f\u6210\u5185\u5bb9\uff09\u8bc4\u4f30\u6307\u6807\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u590d\u5408\u4ea7\u7269\u65f6\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u81ea\u52a8\u5206\u89e3\u548c\u8bc4\u4f30PCG\u3002", "method": "\u672c\u6587\u501f\u9274\u6e38\u620f\u7814\u7a76\u548c\u6e38\u620f\u4eba\u5de5\u667a\u80fd\u7684\u7814\u7a76\uff0c\u5f15\u5165\u4e86\u201c\u5730\u6807\u201d\u3001\u201c\u7eaa\u5ff5\u7891\u201d\u548c\u201c\u4fe1\u6807\u201d\u8fd9\u4e09\u4e2a\u5d4c\u5957\u7684\u6982\u5ff5\u3002\u8fd9\u4e9b\u6982\u5ff5\u662f\u57fa\u4e8e\u4ea7\u7269\u7684\u53ef\u611f\u77e5\u6027\u3001\u5524\u8d77\u6027\u548c\u884c\u52a8\u53f7\u53ec\uff0c\u5e76\u4ece\u73a9\u5bb6\u7684\u89d2\u5ea6\u51fa\u53d1\u3002\u8fd9\u4e9b\u672f\u8bed\u5177\u6709\u901a\u7528\u6027\uff0c\u9002\u7528\u4e8e\u5404\u79cd\u6e38\u620f\u7c7b\u578b\u3002", "result": "\u8fd9\u4e9b\u6982\u5ff5\u53ef\u4ee5\u901a\u8fc7\u5f53\u524d\u7814\u7a76\u548c\u5de5\u4e1a\u754c\u4f7f\u7528\u7684\u6280\u672f\u6765\u53d1\u73b0\u548c\u8bc4\u4f30\u3002\u8fd9\u4e3aPCG\u7684\u5b8c\u5168\u81ea\u52a8\u5316\u5206\u89e3\u548c\u5173\u952e\u5b50\u7ec4\u4ef6\u7684\u8bc4\u4f30\u94fa\u5e73\u4e86\u9053\u8def\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u65e8\u5728\u8fde\u63a5\u4eba\u6587\u5b66\u79d1\u548c\u6280\u672f\u6e38\u620f\u7814\u7a76\uff0c\u5e76\u5b9e\u73b0\u66f4\u597d\u7684\u8ba1\u7b97PCG\u8bc4\u4f30\u3002\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u9002\u7528\u4e8e\u6df7\u5408\u5f0f\u548c\u7ec4\u5408\u5f0fPCG\uff0c\u800c\u4e14\u5177\u6709\u66f4\u5e7f\u6cdb\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2509.19033", "categories": ["cs.CL", "68T50", "I.2.7"], "pdf": "https://arxiv.org/pdf/2509.19033", "abs": "https://arxiv.org/abs/2509.19033", "authors": ["Chiara Alzetta", "Serena Auriemma", "Alessandro Bondielli", "Luca Dini", "Chiara Fazzone", "Alessio Miaschi", "Martina Miliani", "Marta Sartor"], "title": "Charting a Decade of Computational Linguistics in Italy: The CLiC-it Corpus", "comment": "Submitted to IJCoL", "summary": "Over the past decade, Computational Linguistics (CL) and Natural Language\nProcessing (NLP) have evolved rapidly, especially with the advent of\nTransformer-based Large Language Models (LLMs). This shift has transformed\nresearch goals and priorities, from Lexical and Semantic Resources to Language\nModelling and Multimodality. In this study, we track the research trends of the\nItalian CL and NLP community through an analysis of the contributions to\nCLiC-it, arguably the leading Italian conference in the field. We compile the\nproceedings from the first 10 editions of the CLiC-it conference (from 2014 to\n2024) into the CLiC-it Corpus, providing a comprehensive analysis of both its\nmetadata, including author provenance, gender, affiliations, and more, as well\nas the content of the papers themselves, which address various topics. Our goal\nis to provide the Italian and international research communities with valuable\ninsights into emerging trends and key developments over time, supporting\ninformed decisions and future directions in the field.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5206\u6790\u4e86\u610f\u5927\u5229\u8ba1\u7b97\u8bed\u8a00\u5b66\uff08CL\uff09\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u9886\u57df\u5728CLiC-it\u4f1a\u8bae\uff082014-2024\uff09\u4e0a\u7684\u7814\u7a76\u8d8b\u52bf\uff0c\u91cd\u70b9\u5173\u6ce8\u6280\u672f\u53d1\u5c55\u3001\u4f5c\u8005\u4fe1\u606f\u548c\u7814\u7a76\u4e3b\u9898\uff0c\u65e8\u5728\u4e3a\u793e\u533a\u63d0\u4f9b\u89c1\u89e3\u3002", "motivation": "\u8ffd\u8e2a\u610f\u5927\u5229\u8ba1\u7b97\u8bed\u8a00\u5b66\uff08CL\uff09\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u9886\u57df\u7684\u7814\u7a76\u8d8b\u52bf\uff0c\u7279\u522b\u662fTransformer \uae30\ubc18 \ub300\uaddc\ubaa8 \uc5b8\uc5b4 \ubaa8\ub378(LLM)\u51fa\u73b0\u540e\u7684\u53d8\u5316\uff0c\u4e3a\u610f\u5927\u5229\u548c\u56fd\u9645\u7814\u7a76\u754c\u63d0\u4f9b\u89c1\u89e3\uff0c\u652f\u6301\u672a\u6765\u5728\u8be5\u9886\u57df\u7684\u51b3\u7b56\u548c\u53d1\u5c55\u3002", "method": "\u5206\u6790\u4e862014\u5e74\u81f32024\u5e74CLiC-it\u4f1a\u8bae\uff08\u610f\u5927\u5229CL\u548cNLP\u9886\u57df\u7684\u4e3b\u8981\u4f1a\u8bae\uff09\u768410\u5e74\u8bba\u6587\u96c6\uff0c\u6784\u5efa\u4e86CLiC-it\u8bed\u6599\u5e93\uff0c\u5e76\u5bf9\u5176\u5143\u6570\u636e\uff08\u4f5c\u8005\u6765\u6e90\u3001\u6027\u522b\u3001\u96b6\u5c5e\u5173\u7cfb\u7b49\uff09\u548c\u8bba\u6587\u5185\u5bb9\uff08\u7814\u7a76\u4e3b\u9898\uff09\u8fdb\u884c\u4e86\u5168\u9762\u5206\u6790\u3002", "result": "\u5206\u6790\u4e86CLiC-it\u8bed\u6599\u5e93\u7684\u5143\u6570\u636e\u548c\u8bba\u6587\u5185\u5bb9\uff0c\u63ed\u793a\u4e86\u610f\u5927\u5229CL\u548cNLP\u9886\u57df\u968f\u65f6\u95f4\u53d8\u5316\u7684\u7814\u7a76\u8d8b\u52bf\u548c\u5173\u952e\u53d1\u5c55\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u610f\u5927\u5229\u548c\u56fd\u9645\u7814\u7a76\u754c\u63d0\u4f9b\u4e86\u5173\u4e8eCL\u548cNLP\u9886\u57df\u65b0\u5174\u8d8b\u52bf\u548c\u5173\u952e\u53d1\u5c55\u7684\u5b9d\u8d35\u89c1\u89e3\uff0c\u6709\u52a9\u4e8e\u652f\u6301\u8be5\u9886\u57df\u7684\u660e\u667a\u51b3\u7b56\u548c\u672a\u6765\u65b9\u5411\u3002"}}
{"id": "2509.19076", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.19076", "abs": "https://arxiv.org/abs/2509.19076", "authors": ["Laura Connolly", "Aravind S. Kumar", "Kapi Ketan Mehta", "Lidia Al-Zogbi", "Peter Kazanzides", "Parvin Mousavi", "Gabor Fichtinger", "Axel Krieger", "Junichi Tokuda", "Russell H. Taylor", "Simon Leonard", "Anton Deguet"], "title": "SlicerROS2: A Research and Development Module for Image-Guided Robotic Interventions", "comment": null, "summary": "Image-guided robotic interventions involve the use of medical imaging in\ntandem with robotics. SlicerROS2 is a software module that combines 3D Slicer\nand robot operating system (ROS) in pursuit of a standard integration approach\nfor medical robotics research. The first release of SlicerROS2 demonstrated the\nfeasibility of using the C++ API from 3D Slicer and ROS to load and visualize\nrobots in real time. Since this initial release, we've rewritten and redesigned\nthe module to offer greater modularity, access to low-level features, access to\n3D Slicer's Python API, and better data transfer protocols. In this paper, we\nintroduce this new design as well as four applications that leverage the core\nfunctionalities of SlicerROS2 in realistic image-guided robotics scenarios.", "AI": {"tldr": "SlicerROS2\u662f\u4e00\u4e2a\u7ed3\u5408\u4e863D Slicer\u548cROS\u7684\u8f6f\u4ef6\u6a21\u5757\uff0c\u7528\u4e8e\u6807\u51c6\u5316\u533b\u5b66\u673a\u5668\u4eba\u7814\u7a76\u7684\u96c6\u6210\u65b9\u6cd5\u3002\u65b0\u7248\u672c\u63d0\u4f9b\u4e86\u66f4\u5927\u7684\u6a21\u5757\u5316\u3001\u5bf9\u4f4e\u7ea7\u529f\u80fd\u548c3D Slicer Python API\u7684\u8bbf\u95ee\uff0c\u4ee5\u53ca\u66f4\u597d\u7684\u6570\u636e\u4f20\u8f93\u534f\u8bae\uff0c\u5e76\u901a\u8fc7\u56db\u4e2a\u5b9e\u9645\u5e94\u7528\u5c55\u793a\u4e86\u5176\u5728\u56fe\u50cf\u5f15\u5bfc\u673a\u5668\u4eba\u573a\u666f\u4e2d\u7684\u6838\u5fc3\u529f\u80fd\u3002", "motivation": "\u4e3a\u4e86\u5728\u533b\u5b66\u673a\u5668\u4eba\u7814\u7a76\u4e2d\u5b9e\u73b0\u6807\u51c6\u5316\u7684\u96c6\u6210\u65b9\u6cd5\uff0c\u9700\u8981\u4e00\u4e2a\u80fd\u591f\u7ed3\u54083D Slicer\u548cROS\u7684\u8f6f\u4ef6\u6a21\u5757\u3002", "method": "\u901a\u8fc7\u91cd\u5199\u548c\u91cd\u65b0\u8bbe\u8ba1SlicerROS2\u6a21\u5757\uff0c\u63d0\u4f9b\u4e86\u66f4\u5927\u7684\u6a21\u5757\u5316\u3001\u5bf9\u4f4e\u7ea7\u529f\u80fd\u548c3D Slicer Python API\u7684\u8bbf\u95ee\uff0c\u4ee5\u53ca\u66f4\u597d\u7684\u6570\u636e\u4f20\u8f93\u534f\u8bae\u3002", "result": "\u65b0\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u66f4\u5927\u7684\u6a21\u5757\u5316\u3001\u5bf9\u4f4e\u7ea7\u529f\u80fd\u548c3D Slicer Python API\u7684\u8bbf\u95ee\uff0c\u4ee5\u53ca\u66f4\u597d\u7684\u6570\u636e\u4f20\u8f93\u534f\u8bae\uff0c\u5e76\u901a\u8fc7\u56db\u4e2a\u5b9e\u9645\u5e94\u7528\u5c55\u793a\u4e86\u5176\u5728\u56fe\u50cf\u5f15\u5bfc\u673a\u5668\u4eba\u573a\u666f\u4e2d\u7684\u6838\u5fc3\u529f\u80fd\u3002", "conclusion": "\u65b0\u8bbe\u8ba1\u7684SlicerROS2\u6a21\u5757\u901a\u8fc7\u63d0\u4f9b\u589e\u5f3a\u7684\u529f\u80fd\u548c\u5b9e\u9645\u5e94\u7528\uff0c\u8fdb\u4e00\u6b65\u63a8\u52a8\u4e86\u533b\u5b66\u673a\u5668\u4eba\u7814\u7a76\u7684\u6807\u51c6\u96c6\u6210\u3002"}}
{"id": "2509.18593", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18593", "abs": "https://arxiv.org/abs/2509.18593", "authors": ["Xiaoman Wu", "Lubin Gan", "Siying Wu", "Jing Zhang", "Yunwei Ou", "Xiaoyan Sun"], "title": "SSCM: A Spatial-Semantic Consistent Model for Multi-Contrast MRI Super-Resolution", "comment": null, "summary": "Multi-contrast Magnetic Resonance Imaging super-resolution (MC-MRI SR) aims\nto enhance low-resolution (LR) contrasts leveraging high-resolution (HR)\nreferences, shortening acquisition time and improving imaging efficiency while\npreserving anatomical details. The main challenge lies in maintaining\nspatial-semantic consistency, ensuring anatomical structures remain\nwell-aligned and coherent despite structural discrepancies and motion between\nthe target and reference images. Conventional methods insufficiently model\nspatial-semantic consistency and underuse frequency-domain information, which\nleads to poor fine-grained alignment and inadequate recovery of high-frequency\ndetails. In this paper, we propose the Spatial-Semantic Consistent Model\n(SSCM), which integrates a Dynamic Spatial Warping Module for inter-contrast\nspatial alignment, a Semantic-Aware Token Aggregation Block for long-range\nsemantic consistency, and a Spatial-Frequency Fusion Block for fine structure\nrestoration. Experiments on public and private datasets show that SSCM achieves\nstate-of-the-art performance with fewer parameters while ensuring spatially and\nsemantically consistent reconstructions.", "AI": {"tldr": "MC-MRI SR\u65e8\u5728\u901a\u8fc7\u5229\u7528\u9ad8\u5206\u8fa8\u7387\u53c2\u8003\u56fe\u50cf\u6765\u589e\u5f3a\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\u7684\u5bf9\u6bd4\u5ea6\uff0c\u4ee5\u7f29\u77ed\u6210\u50cf\u65f6\u95f4\u5e76\u63d0\u9ad8\u6210\u50cf\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u7559\u89e3\u5256\u7ec6\u8282\u3002\u4e3b\u8981\u6311\u6218\u5728\u4e8e\u7ef4\u6301\u7a7a\u95f4-\u8bed\u4e49\u4e00\u81f4\u6027\uff0c\u786e\u4fdd\u89e3\u5256\u7ed3\u6784\u5728\u76ee\u6807\u56fe\u50cf\u548c\u53c2\u8003\u56fe\u50cf\u4e4b\u95f4\u5b58\u5728\u7ed3\u6784\u5dee\u5f02\u548c\u8fd0\u52a8\u7684\u60c5\u51b5\u4e0b\u4fdd\u6301\u5bf9\u9f50\u548c\u8fde\u8d2f\u3002\u4f20\u7edf\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u6a21\u62df\u7a7a\u95f4-\u8bed\u4e49\u4e00\u81f4\u6027\uff0c\u5e76\u4e14\u4f4e\u4f30\u4e86\u9891\u57df\u4fe1\u606f\u7684\u4f7f\u7528\uff0c\u5bfc\u81f4\u7c97\u7c92\u5ea6\u5bf9\u9f50\u4e0d\u4f73\u548c\u9ad8\u9891\u7ec6\u8282\u6062\u590d\u4e0d\u8db3\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7a7a\u95f4-\u8bed\u4e49\u4e00\u81f4\u6027\u6a21\u578b\uff08SSCM\uff09\uff0c\u5b83\u96c6\u6210\u4e86\u52a8\u6001\u7a7a\u95f4\u53d8\u5f62\u6a21\u5757\u7528\u4e8e\u5bf9\u6bd4\u5ea6\u95f4\u7a7a\u95f4\u5bf9\u9f50\uff0c\u8bed\u4e49\u611f\u77e5\u4ee4\u724c\u805a\u5408\u5757\u7528\u4e8e\u957f\u8ddd\u79bb\u8bed\u4e49\u4e00\u81f4\u6027\uff0c\u4ee5\u53ca\u7a7a\u95f4-\u9891\u7387\u878d\u5408\u5757\u7528\u4e8e\u7cbe\u7ec6\u7ed3\u6784\u6062\u590d\u3002\u5728\u516c\u5171\u548c\u79c1\u6709\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSSCM\u5728\u786e\u4fdd\u7a7a\u95f4\u548c\u8bed\u4e49\u4e0a\u7684\u4e00\u81f4\u6027\u91cd\u5efa\u7684\u540c\u65f6\uff0c\u7528\u66f4\u5c11\u7684\u53c2\u6570\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "MC-MRI SR \u65e8\u5728\u63d0\u9ad8\u6210\u50cf\u6548\u7387\u5e76\u4fdd\u7559\u89e3\u5256\u7ec6\u8282\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u5728\u7a7a\u95f4-\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u9891\u57df\u4fe1\u606f\u5229\u7528\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSSC\uff08Spatial-Semantic Consistent Model\uff09\u7684\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5305\u542b\u4e09\u4e2a\u5173\u952e\u6a21\u5757\uff1a\u52a8\u6001\u7a7a\u95f4\u53d8\u5f62\u6a21\u5757\uff08\u7528\u4e8e\u5bf9\u6bd4\u5ea6\u95f4\u7a7a\u95f4\u5bf9\u9f50\uff09\u3001\u8bed\u4e49\u611f\u77e5\u4ee4\u724c\u805a\u5408\u5757\uff08\u7528\u4e8e\u957f\u8ddd\u79bb\u8bed\u4e49\u4e00\u81f4\u6027\uff09\u548c\u7a7a\u95f4-\u9891\u7387\u878d\u5408\u5757\uff08\u7528\u4e8e\u7cbe\u7ec6\u7ed3\u6784\u6062\u590d\uff09\u3002", "result": "SSCM \u5728\u516c\u5171\u548c\u79c1\u6709\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u53c2\u6570\u66f4\u5c11\uff0c\u5e76\u786e\u4fdd\u4e86\u7a7a\u95f4\u548c\u8bed\u4e49\u4e0a\u7684\u4e00\u81f4\u6027\u91cd\u5efa\u3002", "conclusion": "SSCM \u80fd\u591f\u6709\u6548\u89e3\u51b3 MC-MRI SR \u4e2d\u7684\u7a7a\u95f4-\u8bed\u4e49\u4e00\u81f4\u6027\u6311\u6218\uff0c\u5e76\u6bd4\u4f20\u7edf\u65b9\u6cd5\u5177\u6709\u66f4\u597d\u7684\u6027\u80fd\u3002"}}
{"id": "2509.18166", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18166", "abs": "https://arxiv.org/abs/2509.18166", "authors": ["Xiaoqian Qi", "Haoye Chai", "Yong Li"], "title": "MobiGPT: A Foundation Model for Mobile Wireless Networks", "comment": null, "summary": "With the rapid development of mobile communication technologies, future\nmobile networks will offer vast services and resources for commuting,\nproduction, daily life, and entertainment. Accurate and efficient forecasting\nof mobile data (e.g., cell traffic, user behavior, channel quality) helps\noperators monitor network state changes, orchestrate wireless resources, and\nschedule infrastructure and users, thereby improving supply efficiency and\nservice quality. However, current forecasting paradigms rely on customized\ndesigns with tailored models for exclusive data types. Such approaches increase\ncomplexity and deployment costs under large-scale, heterogeneous networks\ninvolving base stations, users, and channels. In this paper, we design a\nfoundation model for mobile data forecasting, MobiGPT, with a unified structure\ncapable of forecasting three data types: base station traffic, user app usage,\nand channel quality. We propose a soft-prompt learning method to help the model\nunderstand features of different data types, and introduce a temporal masking\nmechanism to guide the model through three forecasting tasks: short-term\nprediction, long-term prediction, and distribution generation, supporting\ndiverse optimization scenarios. Evaluations on real-world datasets with over\n100,000 samples show that MobiGPT achieves accurate multi-type forecasting.\nCompared to existing models, it improves forecasting accuracy by 27.37%,\n20.08%, and 7.27%, reflecting strong generalization. Moreover, MobiGPT exhibits\nsuperior zero/few-shot performance in unseen scenarios, with over 21.51%\nimprovement, validating its strong transferability as a foundation model.", "AI": {"tldr": "MobiGPT\u662f\u4e00\u4e2a\u7528\u4e8e\u79fb\u52a8\u6570\u636e\u9884\u6d4b\u7684\u57fa\u7840\u6a21\u578b\uff0c\u80fd\u591f\u7edf\u4e00\u9884\u6d4b\u57fa\u7ad9\u6d41\u91cf\u3001\u7528\u6237\u5e94\u7528\u4f7f\u7528\u60c5\u51b5\u548c\u4fe1\u9053\u8d28\u91cf\u3002\u5b83\u901a\u8fc7\u8f6f\u63d0\u793a\u5b66\u4e60\u548c\u65f6\u95f4\u63a9\u7801\u673a\u5236\uff0c\u5728\u77ed\u671f\u9884\u6d4b\u3001\u957f\u671f\u9884\u6d4b\u548c\u5206\u5e03\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6bd4\u73b0\u6709\u6a21\u578b\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u672a\u6765\u7684\u79fb\u52a8\u7f51\u7edc\u9700\u8981\u51c6\u786e\u9ad8\u6548\u7684\u79fb\u52a8\u6570\u636e\u9884\u6d4b\u6765\u76d1\u63a7\u7f51\u7edc\u72b6\u6001\u3001\u534f\u8c03\u8d44\u6e90\u548c\u4f18\u5316\u670d\u52a1\u8d28\u91cf\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5b9a\u5236\u5316\u6a21\u578b\uff0c\u589e\u52a0\u4e86\u590d\u6742\u6027\u548c\u6210\u672c\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u540d\u4e3aMobiGPT\u7684\u57fa\u7840\u6a21\u578b\uff0c\u91c7\u7528\u7edf\u4e00\u7ed3\u6784\uff0c\u652f\u6301\u8f6f\u63d0\u793a\u5b66\u4e60\u4ee5\u7406\u89e3\u4e0d\u540c\u6570\u636e\u7c7b\u578b\u7279\u5f81\uff0c\u5e76\u5f15\u5165\u65f6\u95f4\u63a9\u7801\u673a\u5236\u6765\u5904\u7406\u77ed\u671f\u9884\u6d4b\u3001\u957f\u671f\u9884\u6d4b\u548c\u5206\u5e03\u751f\u6210\u4efb\u52a1\u3002", "result": "\u5728\u5305\u542b\u8d85\u8fc7100,000\u4e2a\u6837\u672c\u7684\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0cMobiGPT\u5728\u591a\u7c7b\u578b\u9884\u6d4b\u65b9\u9762\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\uff0c\u76f8\u8f83\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u5728\u4e09\u4e2a\u9884\u6d4b\u4efb\u52a1\u4e0a\u5206\u522b\u63d0\u9ad8\u4e8627.37%\u300120.08%\u548c7.27%\u7684\u51c6\u786e\u6027\u3002\u6b64\u5916\uff0c\u5728\u672a\u89c1\u8fc7\u7684\u573a\u666f\u4e0b\uff0cMobiGPT\u7684\u96f6/\u5c11\u6837\u672c\u6027\u80fd\u4e5f\u63d0\u9ad8\u4e8621.51%\u3002", "conclusion": "MobiGPT\u4f5c\u4e3a\u4e00\u4e2a\u57fa\u7840\u6a21\u578b\uff0c\u5c55\u73b0\u4e86\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u548c\u8fc1\u79fb\u80fd\u529b\uff0c\u80fd\u591f\u51c6\u786e\u9ad8\u6548\u5730\u8fdb\u884c\u591a\u7c7b\u578b\u79fb\u52a8\u6570\u636e\u9884\u6d4b\uff0c\u5e76\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2509.19058", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19058", "abs": "https://arxiv.org/abs/2509.19058", "authors": ["Kwonho Kim", "Heejeong Nam", "Inwoo Hwang", "Sanghack Lee"], "title": "Towards Causal Representation Learning with Observable Sources as Auxiliaries", "comment": null, "summary": "Causal representation learning seeks to recover latent factors that generate\nobservational data through a mixing function. Needing assumptions on latent\nstructures or relationships to achieve identifiability in general, prior works\noften build upon conditional independence given known auxiliary variables.\nHowever, prior frameworks limit the scope of auxiliary variables to be external\nto the mixing function. Yet, in some cases, system-driving latent factors can\nbe easily observed or extracted from data, possibly facilitating\nidentification. In this paper, we introduce a framework of observable sources\nbeing auxiliaries, serving as effective conditioning variables. Our main\nresults show that one can identify entire latent variables up to subspace-wise\ntransformations and permutations using volume-preserving encoders. Moreover,\nwhen multiple known auxiliary variables are available, we offer a\nvariable-selection scheme to choose those that maximize recoverability of the\nlatent factors given knowledge of the latent causal graph. Finally, we\ndemonstrate the effectiveness of our framework through experiments on synthetic\ngraph and image data, thereby extending the boundaries of current approaches.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5305\u542b\u53ef\u89c2\u6d4b\u6e90\u4f5c\u4e3a\u8f85\u52a9\u53d8\u91cf\u7684\u56e0\u679c\u8868\u5f81\u5b66\u4e60\u6846\u67b6\uff0c\u53ef\u4ee5\u8bc6\u522b\u9664\u5b50\u7a7a\u95f4\u53d8\u6362\u548c\u6392\u5217\u5916\u7684\u6574\u4e2a\u6f5c\u5728\u53d8\u91cf\u3002", "motivation": "\u73b0\u6709\u56e0\u679c\u8868\u5f81\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u5bf9\u6f5c\u5728\u7ed3\u6784\u6216\u5173\u7cfb\u505a\u51fa\u5047\u8bbe\uff0c\u5e76\u4e14\u5728\u8f85\u52a9\u53d8\u91cf\u7684\u9009\u62e9\u4e0a\u5b58\u5728\u5c40\u9650\u6027\uff0c\u4ec5\u9650\u4e8e\u6df7\u5408\u51fd\u6570\u4e4b\u5916\u7684\u53d8\u91cf\u3002\u672c\u7814\u7a76\u65e8\u5728\u6269\u5c55\u8f85\u52a9\u53d8\u91cf\u7684\u8303\u56f4\uff0c\u7eb3\u5165\u7cfb\u7edf\u4e2d\u6613\u4e8e\u89c2\u6d4b\u6216\u63d0\u53d6\u7684\u6f5c\u5728\u56e0\u7d20\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u5305\u542b\u53ef\u89c2\u6d4b\u6e90\u4f5c\u4e3a\u8f85\u52a9\u53d8\u91cf\u7684\u6846\u67b6\uff0c\u5229\u7528\u8fd9\u4e9b\u8f85\u52a9\u53d8\u91cf\u4f5c\u4e3a\u6709\u6548\u7684\u6761\u4ef6\u53d8\u91cf\u3002\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u4fdd\u4f53\u79ef\u7f16\u7801\u5668\u53ef\u4ee5\u8bc6\u522b\u51fa\u6f5c\u5728\u53d8\u91cf\uff08\u5728\u5b50\u7a7a\u95f4\u53d8\u6362\u548c\u7f6e\u6362\u4e0b\uff09\u3002\u5f53\u5b58\u5728\u591a\u4e2a\u5df2\u77e5\u8f85\u52a9\u53d8\u91cf\u65f6\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u53d8\u91cf\u9009\u62e9\u65b9\u6848\uff0c\u4ee5\u6700\u5927\u5316\u6f5c\u5728\u56e0\u7d20\u7684\u53ef\u6062\u590d\u6027\uff08\u5728\u5df2\u77e5\u6f5c\u5728\u56e0\u679c\u56fe\u7684\u60c5\u51b5\u4e0b\uff09\u3002", "result": "\u901a\u8fc7\u4fdd\u4f53\u79ef\u7f16\u7801\u5668\uff0c\u53ef\u4ee5\u8bc6\u522b\u6574\u4e2a\u6f5c\u5728\u53d8\u91cf\uff08\u5b50\u7a7a\u95f4\u53d8\u6362\u548c\u6392\u5217\uff09\u3002\u63d0\u51fa\u7684\u53d8\u91cf\u9009\u62e9\u65b9\u6848\u80fd\u5728\u5df2\u77e5\u6f5c\u5728\u56e0\u679c\u56fe\u7684\u60c5\u51b5\u4e0b\uff0c\u9009\u62e9\u6700\u4f18\u7684\u8f85\u52a9\u53d8\u91cf\u4ee5\u6700\u5927\u5316\u6f5c\u5728\u56e0\u7d20\u7684\u53ef\u6062\u590d\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u5305\u542b\u53ef\u89c2\u6d4b\u6e90\u4f5c\u4e3a\u8f85\u52a9\u53d8\u91cf\u7684\u56e0\u679c\u8868\u5f81\u5b66\u4e60\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u901a\u8fc7\u5408\u6210\u56fe\u548c\u56fe\u50cf\u6570\u636e\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\uff0c\u6269\u5c55\u4e86\u5f53\u524d\u65b9\u6cd5\u7684\u8fb9\u754c\u3002"}}
{"id": "2509.19094", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.19094", "abs": "https://arxiv.org/abs/2509.19094", "authors": ["Alireza Salemi", "Cheng Li", "Mingyang Zhang", "Qiaozhu Mei", "Zhuowan Li", "Spurthi Amba Hombaiah", "Weize Kong", "Tao Chen", "Hamed Zamani", "Michael Bendersky"], "title": "Pathways of Thoughts: Multi-Directional Thinking for Long-form Personalized Question Answering", "comment": null, "summary": "Personalization is essential for adapting question answering (QA) systems to\nuser-specific information needs, thereby improving both accuracy and user\nsatisfaction. However, personalized QA remains relatively underexplored due to\nchallenges such as inferring preferences from long, noisy, and implicit\ncontexts, and generating responses that are simultaneously correct,\ncontextually appropriate, and aligned with user expectations and background\nknowledge. To address these challenges, we propose Pathways of Thoughts (PoT),\nan inference-stage method that applies to any large language model (LLM)\nwithout requiring task-specific fine-tuning. The approach models the reasoning\nof an LLM as an iterative decision process, where the model dynamically selects\namong cognitive operations such as reasoning, revision, personalization, and\nclarification. This enables exploration of multiple reasoning trajectories,\nproducing diverse candidate responses that capture different perspectives. PoT\nthen aggregates and reweights these candidates according to inferred user\npreferences, yielding a final personalized response that benefits from the\ncomplementary strengths of diverse reasoning paths. Experiments on the LaMP-QA\nbenchmark for personalized QA show that PoT consistently outperforms\ncompetitive baselines, achieving up to a 13.1% relative improvement. Human\nevaluation corroborates these results, with annotators preferring outputs from\nPoT in 66% of cases and reporting ties in only 15% of cases.", "AI": {"tldr": "PoT\u662f\u4e00\u79cd\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u4e2a\u6027\u5316\u95ee\u7b54\u7cfb\u7edf\u7684\u63a8\u7406\u65b9\u6cd5\uff0c\u901a\u8fc7\u63a2\u7d22\u591a\u91cd\u63a8\u7406\u8def\u5f84\u5e76\u6839\u636e\u7528\u6237\u504f\u597d\u805a\u5408\u7b54\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u51c6\u786e\u6027\u548c\u7528\u6237\u6ee1\u610f\u5ea6\u3002", "motivation": "\u4e2a\u6027\u5316\u95ee\u7b54\u7cfb\u7edf\u5bf9\u4e8e\u6ee1\u8db3\u7528\u6237\u7279\u5b9a\u4fe1\u606f\u9700\u6c42\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u7cfb\u7edf\u5728\u5904\u7406\u957f\u3001\u5608\u6742\u3001\u9690\u5f0f\u4e0a\u4e0b\u6587\u4fe1\u606f\u4ee5\u53ca\u751f\u6210\u6ee1\u8db3\u7528\u6237\u671f\u671b\u7684\u7b54\u6848\u65b9\u9762\u9762\u4e34\u6311\u6218\u3002", "method": "PoT\u5c06LLM\u7684\u63a8\u7406\u8fc7\u7a0b\u5efa\u6a21\u4e3a\u8fed\u4ee3\u51b3\u7b56\u8fc7\u7a0b\uff0c\u52a8\u6001\u9009\u62e9\u63a8\u7406\u3001\u4fee\u6b63\u3001\u4e2a\u6027\u5316\u548c\u6f84\u6e05\u7b49\u8ba4\u77e5\u64cd\u4f5c\uff0c\u751f\u6210\u591a\u6837\u5316\u7684\u5019\u9009\u7b54\u6848\uff0c\u5e76\u6839\u636e\u7528\u6237\u504f\u597d\u8fdb\u884c\u805a\u5408\u548c\u52a0\u6743\u3002", "result": "\u5728LaMP-QA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPoT\u76f8\u6bd4\u7ade\u4e89\u57fa\u7ebf\u53d6\u5f97\u4e86\u9ad8\u8fbe13.1%\u7684\u76f8\u5bf9\u6539\u8fdb\u3002\u4eba\u7c7b\u8bc4\u4f30\u4e5f\u663e\u793a\uff0c66%\u7684\u60c5\u51b5\u4e0b\u8bc4\u4f30\u8005\u66f4\u559c\u6b22PoT\u7684\u8f93\u51fa\u3002", "conclusion": "PoT\u80fd\u591f\u6709\u6548\u89e3\u51b3\u4e2a\u6027\u5316\u95ee\u7b54\u4e2d\u7684\u6311\u6218\uff0c\u901a\u8fc7\u63a2\u7d22\u4e0d\u540c\u63a8\u7406\u8def\u5f84\u5e76\u7ed3\u5408\u7528\u6237\u504f\u597d\uff0c\u751f\u6210\u66f4\u4f18\u7684\u4e2a\u6027\u5316\u7b54\u6848\u3002"}}
{"id": "2509.19080", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19080", "abs": "https://arxiv.org/abs/2509.19080", "authors": ["Zhennan Jiang", "Kai Liu", "Yuxin Qin", "Shuai Tian", "Yupeng Zheng", "Mingcai Zhou", "Chao Yu", "Haoran Li", "Dongbin Zhao"], "title": "World4RL: Diffusion World Models for Policy Refinement with Reinforcement Learning for Robotic Manipulation", "comment": null, "summary": "Robotic manipulation policies are commonly initialized through imitation\nlearning, but their performance is limited by the scarcity and narrow coverage\nof expert data. Reinforcement learning can refine polices to alleviate this\nlimitation, yet real-robot training is costly and unsafe, while training in\nsimulators suffers from the sim-to-real gap. Recent advances in generative\nmodels have demonstrated remarkable capabilities in real-world simulation, with\ndiffusion models in particular excelling at generation. This raises the\nquestion of how diffusion model-based world models can be combined to enhance\npre-trained policies in robotic manipulation. In this work, we propose\nWorld4RL, a framework that employs diffusion-based world models as\nhigh-fidelity simulators to refine pre-trained policies entirely in imagined\nenvironments for robotic manipulation. Unlike prior works that primarily employ\nworld models for planning, our framework enables direct end-to-end policy\noptimization. World4RL is designed around two principles: pre-training a\ndiffusion world model that captures diverse dynamics on multi-task datasets and\nrefining policies entirely within a frozen world model to avoid online\nreal-world interactions. We further design a two-hot action encoding scheme\ntailored for robotic manipulation and adopt diffusion backbones to improve\nmodeling fidelity. Extensive simulation and real-world experiments demonstrate\nthat World4RL provides high-fidelity environment modeling and enables\nconsistent policy refinement, yielding significantly higher success rates\ncompared to imitation learning and other baselines. More visualization results\nare available at https://world4rl.github.io/.", "AI": {"tldr": "\u901a\u8fc7\u4f7f\u7528\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u4e16\u754c\u6a21\u578b\u6765\u4f18\u5316\u673a\u5668\u4eba\u64cd\u7eb5\u7b56\u7565\uff0c\u5b9e\u73b0\u5b8c\u5168\u5728\u6a21\u62df\u73af\u5883\u4e2d\u8fdb\u884c\uff0c\u4ece\u800c\u63d0\u9ad8\u6210\u529f\u7387\u3002", "motivation": "\u673a\u5668\u4eba\u64cd\u7eb5\u7b56\u7565\u7684\u8bad\u7ec3\u53d7\u9650\u4e8e\u4e13\u5bb6\u6570\u636e\u7a00\u758f\u548c\u8986\u76d6\u8303\u56f4\u7a84\u7684\u95ee\u9898\uff0c\u800c\u5f3a\u5316\u5b66\u4e60\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u8bad\u7ec3\u6210\u672c\u9ad8\u4e14\u4e0d\u5b89\u5168\uff0c\u5728\u6a21\u62df\u5668\u4e0a\u8bad\u7ec3\u5219\u5b58\u5728\u57df\u8fc1\u79fb\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aWorld4RL\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u4e16\u754c\u6a21\u578b\u4f5c\u4e3a\u9ad8\u4fdd\u771f\u6a21\u62df\u5668\uff0c\u5728\u5b8c\u5168\u865a\u6784\u7684\u73af\u5883\u4e2d\u5bf9\u9884\u8bad\u7ec3\u7684\u7b56\u7565\u8fdb\u884c\u4f18\u5316\u3002\u8be5\u6846\u67b6\u5305\u542b\u9884\u8bad\u7ec3\u6269\u6563\u4e16\u754c\u6a21\u578b\u548c\u5728\u51bb\u7ed3\u7684\u4e16\u754c\u6a21\u578b\u4e2d\u8fdb\u884c\u7b56\u7565\u7cbe\u70bc\u4e24\u4e2a\u5173\u952e\u90e8\u5206\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e24\u9879\u521b\u65b0\uff1a\u4e00\u79cd\u7528\u4e8e\u673a\u5668\u4eba\u64cd\u7eb5\u7684\u4e24\u70ed\u7f16\u7801\u65b9\u6848\u548c\u91c7\u7528\u6269\u6563\u4e3b\u5e72\u7f51\u7edc\u4ee5\u63d0\u9ad8\u5efa\u6a21\u4fdd\u771f\u5ea6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cWorld4RL\u80fd\u591f\u63d0\u4f9b\u9ad8\u4fdd\u771f\u5ea6\u7684\u73af\u5883\u5efa\u6a21\uff0c\u5e76\u5b9e\u73b0\u4e00\u81f4\u7684\u7b56\u7565\u7cbe\u70bc\uff0c\u4e0e\u4ec5\u6a21\u4eff\u5b66\u4e60\u548c\u5176\u4ed6\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6210\u529f\u7387\u663e\u8457\u63d0\u9ad8\u3002", "conclusion": "\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u4e16\u754c\u6a21\u578b\u53ef\u4ee5\u6709\u6548\u5730\u7528\u4e8e\u673a\u5668\u4eba\u64cd\u7eb5\u7b56\u7565\u7684\u4f18\u5316\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e86\u4f18\u4e8e\u5bf9\u6bd4\u65b9\u6cd5\u7684\u6210\u679c\u3002"}}
{"id": "2509.18600", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18600", "abs": "https://arxiv.org/abs/2509.18600", "authors": ["Zhuoxiao Chen", "Hongyang Yu", "Ying Xu", "Yadan Luo", "Long Duong", "Yuan-Fang Li"], "title": "OraPO: Oracle-educated Reinforcement Learning for Data-efficient and Factual Radiology Report Generation", "comment": null, "summary": "Radiology report generation (RRG) aims to automatically produce clinically\nfaithful reports from chest X-ray images. Prevailing work typically follows a\nscale-driven paradigm, by multi-stage training over large paired corpora and\noversized backbones, making pipelines highly data- and compute-intensive. In\nthis paper, we propose Oracle-educated GRPO {OraPO) with a FactScore-based\nreward (FactS) to tackle the RRG task under constrained budgets. OraPO enables\nsingle-stage, RL-only training by converting failed GRPO explorations on rare\nor difficult studies into direct preference supervision via a lightweight\noracle step. FactS grounds learning in diagnostic evidence by extracting atomic\nclinical facts and checking entailment against ground-truth labels, yielding\ndense, interpretable sentence-level rewards. Together, OraPO and FactS create a\ncompact and powerful framework that significantly improves learning efficiency\non clinically challenging cases, setting the new SOTA performance on the\nCheXpert Plus dataset (0.341 in F1) with 2--3 orders of magnitude less training\ndata using a small base VLM on modest hardware.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aOraPO\u7684\u6846\u67b6\uff0c\u7ed3\u5408FactS\u4f5c\u4e3a\u5956\u52b1\uff0c\u7528\u4e8e\u653e\u5c04\u79d1\u62a5\u544a\u751f\u6210\uff08RRG\uff09\uff0c\u5728\u6570\u636e\u548c\u8ba1\u7b97\u8d44\u6e90\u53d7\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86\u5355\u9636\u6bb5\u3001\u4ec5\u5f3a\u5316\u5b66\u4e60\u7684\u8bad\u7ec3\uff0c\u5e76\u53d6\u5f97\u4e86\u65b0\u7684SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u653e\u5c04\u79d1\u62a5\u544a\u751f\u6210\uff08RRG\uff09\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u591a\u9636\u6bb5\u8bad\u7ec3\u548c\u5e9e\u5927\u7684\u6570\u636e\u96c6\u53ca\u6a21\u578b\uff0c\u8bad\u7ec3\u6210\u672c\u9ad8\u6602\u3002\u672c\u7814\u7a76\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u5728\u9884\u7b97\u53d7\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u80fd\u591f\u9ad8\u6548\u89e3\u51b3RRG\u4efb\u52a1\u7684\u6846\u67b6\u3002", "method": "\u63d0\u51faOraPO\u6846\u67b6\uff0c\u5229\u7528\u57fa\u4e8eFactS\u7684\u5956\u52b1\u3002OraPO\u901a\u8fc7\u8f7b\u91cf\u7ea7\u7684oracle\u6b65\u9aa4\uff0c\u5c06\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u63a2\u7d22\u4e2d\u5931\u8d25\u7684\u6848\u4f8b\u8f6c\u5316\u4e3a\u76f4\u63a5\u504f\u597d\u76d1\u7763\uff0c\u5b9e\u73b0\u4e86\u5355\u9636\u6bb5\u3001\u4ec5RL\u7684\u8bad\u7ec3\u3002FactS\u901a\u8fc7\u63d0\u53d6\u539f\u5b50\u4e34\u5e8a\u4e8b\u5b9e\u5e76\u4e0e\u5730\u9762\u771f\u5b9e\u6807\u7b7e\u8fdb\u884c\u63a8\u7406\u68c0\u67e5\uff0c\u63d0\u4f9b\u5bc6\u96c6\u7684\u3001\u53ef\u89e3\u91ca\u7684\u53e5\u5b50\u7ea7\u5956\u52b1\u3002", "result": "OraPO\u548cFactS\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4e86\u5728\u4e34\u5e8a\u4e0a\u5177\u6709\u6311\u6218\u6027\u7684\u75c5\u4f8b\u7684\u5b66\u4e60\u6548\u7387\uff0c\u5728CheXpert Plus\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u65b0\u7684SOTA\u6027\u80fd\uff08F1\u5f97\u5206\u4e3a0.341\uff09\uff0c\u540c\u65f6\u8bad\u7ec3\u6570\u636e\u91cf\u51cf\u5c11\u4e862-3\u4e2a\u6570\u91cf\u7ea7\uff0c\u4f7f\u7528\u7684\u6a21\u578b\u4e3a\u5c0f\u578b\u57fa\u7840\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\uff0c\u786c\u4ef6\u8981\u6c42\u9002\u4e2d\u3002", "conclusion": "OraPO\u548cFactS\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7d27\u51d1\u4e14\u5f3a\u5927\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u663e\u8457\u63d0\u9ad8RRG\u4efb\u52a1\u7684\u5b66\u4e60\u6548\u7387\uff0c\u5c24\u5176\u662f\u5728\u6570\u636e\u548c\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u5e76\u5728CheXpert Plus\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u9886\u5148\u7684\u6027\u80fd\u3002"}}
{"id": "2509.18169", "categories": ["cs.LG", "cs.CE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18169", "abs": "https://arxiv.org/abs/2509.18169", "authors": ["Hengbo Xiao", "Jingyuan Fan", "Xin Tong", "Jingzhao Zhang", "Chao Lu", "Guannan He"], "title": "PiMoE: Token-Level Routing for Integrating High-Precision Computation and Reasoning", "comment": null, "summary": "Complex systems typically rely on high-precision numerical computation to\nsupport decisions, but current large language models (LLMs) cannot yet\nincorporate such computations as an intrinsic and interpretable capability with\nexisting architectures. Mainstream multi-agent approaches can leverage external\nexperts, but inevitably introduce communication overhead and suffer from\ninefficient multimodal emergent capability and limited scalability. To this\nend, we propose PiMoE (Physically-isolated Mixture of Experts), a training and\ninference architecture for integrating computation and reasoning. Instead of\nthe workflow paradigm of tool invocation, PiMoE endogenously integrates\ncomputational capabilities into neural networks after separately training\nexperts, a text-to-computation module, and a router. At inference, the router\ndirects computation and reasoning at the token level, thereby enabling\niterative alternation within a single chain of thought. We evaluate PiMoE on\ntwo reasoning-computation tasks against LLM finetuning and the multi-agent\nsystem approaches. Results show that the PiMoE architecture achieves not only\nhigher accuracy than directly finetuning LLMs but also significant improvements\nin response latency, token usage, and GPU energy consumption compared with\nmainstream multi-agent approaches. PiMoE offers an efficient, interpretable,\nand scalable paradigm for next-generation scientific or industrial intelligent\nsystems.", "AI": {"tldr": "PiMoE\u662f\u4e00\u79cd\u7ed3\u5408\u8ba1\u7b97\u548c\u63a8\u7406\u7684\u65b0\u578bLLM\u67b6\u6784\uff0c\u901a\u8fc7token\u7ea7\u522b\u7684\u8def\u7531\u5b9e\u73b0\u8fed\u4ee3\u5f0f\u8ba1\u7b97\uff0c\u5728\u51c6\u786e\u6027\u3001\u54cd\u5e94\u5ef6\u8fdf\u3001token\u4f7f\u7528\u91cf\u548cGPU\u80fd\u8017\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7f3a\u4e4f\u6574\u5408\u9ad8\u7cbe\u5ea6\u6570\u503c\u8ba1\u7b97\u7684\u80fd\u529b\uff0c\u800c\u591a\u667a\u80fd\u4f53\u65b9\u6cd5\u5b58\u5728\u901a\u4fe1\u5f00\u9500\u5927\u3001\u53ef\u6269\u5c55\u6027\u6709\u9650\u7b49\u95ee\u9898\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u67b6\u6784\u6765\u878d\u5408\u8ba1\u7b97\u548c\u63a8\u7406\u3002", "method": "\u63d0\u51faPiMoE\uff08Physically-isolated Mixture of Experts\uff09\u67b6\u6784\uff0c\u8be5\u67b6\u6784\u5728\u5355\u72ec\u8bad\u7ec3experts\u3001\u6587\u672c\u5230\u8ba1\u7b97\u6a21\u5757\u548c\u8def\u7531\u5668\u540e\uff0c\u5c06\u8ba1\u7b97\u80fd\u529b\u5185\u6e90\u6027\u5730\u96c6\u6210\u5230\u795e\u7ecf\u7f51\u7edc\u4e2d\u3002\u5728\u63a8\u7406\u65f6\uff0c\u8def\u7531\u5668\u5728token\u7ea7\u522b\u8fdb\u884c\u8ba1\u7b97\u548c\u63a8\u7406\u7684\u8def\u7531\uff0c\u5b9e\u73b0\u5355\u6b21\u601d\u7ef4\u94fe\u4e2d\u7684\u8fed\u4ee3\u4ea4\u66ff\u3002", "result": "\u5728\u4e24\u4e2a\u63a8\u7406-\u8ba1\u7b97\u4efb\u52a1\u4e0a\uff0cPiMoE\u67b6\u6784\u7684\u51c6\u786e\u6027\u4f18\u4e8e\u76f4\u63a5\u5fae\u8c03LLM\u7684\u65b9\u6cd5\uff0c\u5e76\u4e14\u5728\u54cd\u5e94\u5ef6\u8fdf\u3001token\u4f7f\u7528\u91cf\u548cGPU\u80fd\u8017\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u4e3b\u6d41\u7684\u591a\u667a\u80fd\u4f53\u65b9\u6cd5\u3002", "conclusion": "PiMoE\u4e3a\u4e0b\u4e00\u4ee3\u79d1\u5b66\u6216\u5de5\u4e1a\u667a\u80fd\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u4e14\u53ef\u6269\u5c55\u7684\u8303\u5f0f\u3002"}}
{"id": "2509.19077", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19077", "abs": "https://arxiv.org/abs/2509.19077", "authors": ["Zikang Tian", "Shaohui Peng", "Du Huang", "Jiaming Guo", "Ruizhi Chen", "Rui Zhang", "Xishan Zhang", "Yuxuan Guo", "Zidong Du", "Qi Guo", "Ling Li", "Yewen Pu", "Xing Hu", "Yunji Chen"], "title": "Code Driven Planning with Domain-Adaptive Critic", "comment": null, "summary": "Large Language Models (LLMs) have been widely adopted as task planners for AI\nagents in sequential decision-making problems, leveraging their extensive world\nknowledge. However, the gap between their general knowledge and\nenvironment-specific requirements often leads to inaccurate plans. To address\nthis, existing approaches rely on frequent LLM queries to iteratively refine\nplans based on immediate environmental feedback, which incurs substantial query\ncosts. However, this refinement is typically guided by short-term environmental\nfeedback, limiting LLMs from developing plans aligned with long-term rewards.\nWe propose Code Driven Planning with Domain-Adaptive Critic (CoPiC). Instead of\nrelying on frequent queries, CoPiC employs LLMs to generate a diverse set of\nhigh-level planning programs, which iteratively produce and refine candidate\nplans. A trained domain-adaptive critic then evaluates these candidates and\nselects the one most aligned with long-term rewards for execution. Using\nhigh-level planning programs as planner and domain-adaptive critic as\nestimator, CoPiC improves planning while significantly reducing query costs.\nResults in ALFWorld, NetHack, and StarCraft II Unit Building show that CoPiC\noutperforms advanced LLM-based baselines, AdaPlanner and Reflexion, achieving\nan average (1) 23.33% improvement in success rate and (2) 91.27% reduction in\nquery costs.", "AI": {"tldr": "CoPiC\u901a\u8fc7\u4f7f\u7528LLM\u751f\u6210\u53ef\u6267\u884c\u7684\u89c4\u5212\u7a0b\u5e8f\uff0c\u5e76\u5229\u7528\u9886\u57df\u81ea\u9002\u5e94\u7684Critic\u6765\u8bc4\u4f30\u548c\u9009\u62e9\u6700\u7b26\u5408\u957f\u671f\u5956\u52b1\u7684\u89c4\u5212\uff0c\u4ece\u800c\u5728\u964d\u4f4eLLM\u67e5\u8be2\u6210\u672c\u7684\u540c\u65f6\u63d0\u9ad8\u89c4\u5212\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684LLM\u4efb\u52a1\u89c4\u5212\u65b9\u6cd5\u5728\u5904\u7406\u9700\u8981\u73af\u5883\u7279\u5b9a\u77e5\u8bc6\u7684\u95ee\u9898\u65f6\uff0c\u7531\u4e8e\u901a\u7528\u77e5\u8bc6\u4e0e\u73af\u5883\u8981\u6c42\u7684\u5dee\u8ddd\u800c\u5bfc\u81f4\u8ba1\u5212\u4e0d\u51c6\u786e\u3002\u9891\u7e41\u67e5\u8be2LLM\u8fdb\u884c\u8fed\u4ee3\u4f18\u5316\u867d\u7136\u80fd\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u4f46\u6210\u672c\u9ad8\u6602\u4e14\u53d7\u9650\u4e8e\u77ed\u671f\u53cd\u9988\uff0c\u96be\u4ee5\u4f18\u5316\u957f\u671f\u5956\u52b1\u3002", "method": "CoPiC\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u4e0d\u4f9d\u8d56\u9891\u7e41\u67e5\u8be2\uff0c\u800c\u662f\u5229\u7528LLM\u751f\u6210\u591a\u6837\u5316\u7684\u3001\u53ef\u6267\u884c\u7684\u89c4\u5212\u7a0b\u5e8f\u3002\u7136\u540e\uff0c\u8bad\u7ec3\u4e00\u4e2a\u9886\u57df\u81ea\u9002\u5e94\u7684Critic\u6765\u8bc4\u4f30\u8fd9\u4e9b\u7a0b\u5e8f\u751f\u6210\u7684\u5019\u9009\u89c4\u5212\uff0c\u5e76\u9009\u62e9\u6700\u7b26\u5408\u957f\u671f\u5956\u52b1\u7684\u89c4\u5212\u6267\u884c\u3002", "result": "\u5728ALFWorld\u3001NetHack\u548cStarCraft II Unit Building\u7b49\u73af\u5883\u4e2d\uff0cCoPiC\u76f8\u6bd4\u4e8eAdaPlanner\u548cReflexion\u7b49\u73b0\u6709\u7684LLM\u57fa\u7ebf\u65b9\u6cd5\uff0c\u6210\u529f\u7387\u5e73\u5747\u63d0\u9ad8\u4e8623.33%\uff0c\u540c\u65f6\u67e5\u8be2\u6210\u672c\u964d\u4f4e\u4e8691.27%\u3002", "conclusion": "CoPiC\u901a\u8fc7\u4f7f\u7528LLM\u751f\u6210\u89c4\u5212\u7a0b\u5e8f\uff0c\u5e76\u7ed3\u5408\u9886\u57df\u81ea\u9002\u5e94Critic\u8fdb\u884c\u8bc4\u4f30\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709LLM\u89c4\u5212\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u5728\u964d\u4f4e\u6210\u672c\u7684\u540c\u65f6\u63d0\u9ad8\u89c4\u5212\u6548\u7387\u548c\u6548\u679c\u3002"}}
{"id": "2509.19108", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.19108", "abs": "https://arxiv.org/abs/2509.19108", "authors": ["Hiram Ring"], "title": "Are most sentences unique? An empirical examination of Chomskyan claims", "comment": null, "summary": "A repeated claim in linguistics is that the majority of linguistic utterances\nare unique. For example, Pinker (1994: 10), summarizing an argument by Noam\nChomsky, states that \"virtually every sentence that a person utters or\nunderstands is a brand-new combination of words, appearing for the first time\nin the history of the universe.\" With the increased availability of large\ncorpora, this is a claim that can be empirically investigated. The current\npaper addresses the question by using the NLTK Python library to parse corpora\nof different genres, providing counts of exact string matches in each. Results\nshow that while completely unique sentences are often the majority of corpora,\nthis is highly constrained by genre, and that duplicate sentences are not an\ninsignificant part of any individual corpus.", "AI": {"tldr": "\u5927\u591a\u6570\u8bed\u8a00\u90fd\u662f\u72ec\u4e00\u65e0\u4e8c\u7684\u3002", "motivation": "\u5927\u591a\u6570\u8bed\u8a00\u90fd\u662f\u72ec\u4e00\u65e0\u4e8c\u7684\uff0c\u8fd9\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528\u5927\u578b\u8bed\u6599\u5e93\u8fdb\u884c\u5b9e\u8bc1\u8c03\u67e5\u6765\u89e3\u51b3\u3002", "method": "\u4f7f\u7528 NLTK Python \u5e93\u89e3\u6790\u4e0d\u540c\u7c7b\u578b\u7684\u8bed\u6599\u5e93\uff0c\u5e76\u8ba1\u7b97\u6bcf\u4e2a\u8bed\u6599\u5e93\u4e2d\u7684\u7cbe\u786e\u5b57\u7b26\u4e32\u5339\u914d\u3002", "result": "\u867d\u7136\u5b8c\u5168\u72ec\u7279\u7684\u53e5\u5b50\u901a\u5e38\u662f\u8bed\u6599\u5e93\u7684\u5927\u591a\u6570\uff0c\u4f46\u8fd9\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u53d7\u8bed\u4f53\u9650\u5236\uff0c\u5e76\u4e14\u91cd\u590d\u7684\u53e5\u5b50\u5728\u4efb\u4f55\u5355\u4e2a\u8bed\u6599\u5e93\u4e2d\u90fd\u5360\u6709\u91cd\u8981\u7684\u4e00\u90e8\u5206\u3002", "conclusion": "\u5927\u591a\u6570\u8bed\u8a00\u90fd\u662f\u72ec\u4e00\u65e0\u4e8c\u7684\uff0c\u4f46\u8fd9\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u53d7\u8bed\u4f53\u9650\u5236\uff0c\u5e76\u4e14\u91cd\u590d\u7684\u53e5\u5b50\u5728\u4efb\u4f55\u5355\u4e2a\u8bed\u6599\u5e93\u4e2d\u90fd\u5360\u6709\u91cd\u8981\u7684\u4e00\u90e8\u5206\u3002"}}
{"id": "2509.19102", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19102", "abs": "https://arxiv.org/abs/2509.19102", "authors": ["Hongli Xu", "Lei Zhang", "Xiaoyue Hu", "Boyang Zhong", "Kaixin Bai", "Zolt\u00e1n-Csaba M\u00e1rton", "Zhenshan Bing", "Zhaopeng Chen", "Alois Christian Knoll", "Jianwei Zhang"], "title": "FUNCanon: Learning Pose-Aware Action Primitives via Functional Object Canonicalization for Generalizable Robotic Manipulation", "comment": "project website: https://sites.google.com/view/funcanon, 11 pages", "summary": "General-purpose robotic skills from end-to-end demonstrations often leads to\ntask-specific policies that fail to generalize beyond the training\ndistribution. Therefore, we introduce FunCanon, a framework that converts\nlong-horizon manipulation tasks into sequences of action chunks, each defined\nby an actor, verb, and object. These chunks focus policy learning on the\nactions themselves, rather than isolated tasks, enabling compositionality and\nreuse. To make policies pose-aware and category-general, we perform functional\nobject canonicalization for functional alignment and automatic manipulation\ntrajectory transfer, mapping objects into shared functional frames using\naffordance cues from large vision language models. An object centric and action\ncentric diffusion policy FuncDiffuser trained on this aligned data naturally\nrespects object affordances and poses, simplifying learning and improving\ngeneralization ability. Experiments on simulated and real-world benchmarks\ndemonstrate category-level generalization, cross-task behavior reuse, and\nrobust sim2real deployment, showing that functional canonicalization provides a\nstrong inductive bias for scalable imitation learning in complex manipulation\ndomains. Details of the demo and supplemental material are available on our\nproject website https://sites.google.com/view/funcanon.", "AI": {"tldr": "FunCanon\u6846\u67b6\u901a\u8fc7\u5c06\u957f\u65f6\u64cd\u4f5c\u4efb\u52a1\u5206\u89e3\u4e3a\u7531\u6267\u884c\u8005\u3001\u52a8\u8bcd\u548c\u5bf9\u8c61\u5b9a\u4e49\u7684\u52a8\u4f5c\u5757\u5e8f\u5217\uff0c\u5e76\u5c06\u7269\u4f53\u6620\u5c04\u5230\u5171\u4eab\u7684\u529f\u80fd\u5e27\u4e2d\uff0c\u6765\u63d0\u9ad8\u673a\u5668\u4eba\u6280\u80fd\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u901a\u7528\u673a\u5668\u4eba\u6280\u80fd\u901a\u5e38\u5c40\u9650\u4e8e\u7279\u5b9a\u4efb\u52a1\uff0c\u6cdb\u5316\u80fd\u529b\u5dee\u3002\u8be5\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u4e00\u79cd\u65b0\u6846\u67b6\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faFunCanon\u6846\u67b6\uff0c\u5c06\u957f\u65f6\u64cd\u4f5c\u4efb\u52a1\u5206\u89e3\u4e3a\u52a8\u4f5c\u5757\u5e8f\u5217\uff08\u6267\u884c\u8005\u3001\u52a8\u8bcd\u3001\u5bf9\u8c61\uff09\uff0c\u5e76\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u529f\u80fd\u7269\u4f53\u89c4\u8303\u5316\uff0c\u5b9e\u73b0\u59ff\u6001\u611f\u77e5\u548c\u7c7b\u522b\u6cdb\u5316\u3002\u5728\u6b64\u57fa\u7840\u4e0a\u8bad\u7ec3FuncDiffuser\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eFunCanon\u5728\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u7c7b\u522b\u6cdb\u5316\u3001\u8de8\u4efb\u52a1\u884c\u4e3a\u590d\u7528\u548c\u9c81\u68d2\u7684sim2real\u90e8\u7f72\u3002", "conclusion": "\u529f\u80fd\u89c4\u8303\u5316\u4e3a\u590d\u6742\u64cd\u4f5c\u9886\u57df\u7684\u53ef\u6269\u5c55\u6a21\u4eff\u5b66\u4e60\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u5f52\u7eb3\u504f\u89c1\uff0c\u63d0\u9ad8\u4e86\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2509.18602", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18602", "abs": "https://arxiv.org/abs/2509.18602", "authors": ["Xu Liu", "Yibo Lu", "Xinxian Wang", "Xinyu Wu"], "title": "Training-Free Multi-Style Fusion Through Reference-Based Adaptive Modulation", "comment": "Accepted at ACPR 2025 (oral)", "summary": "We propose Adaptive Multi-Style Fusion (AMSF), a reference-based\ntraining-free framework that enables controllable fusion of multiple reference\nstyles in diffusion models. Most of the existing reference-based methods are\nlimited by (a) acceptance of only one style image, thus prohibiting hybrid\naesthetics and scalability to more styles, and (b) lack of a principled\nmechanism to balance several stylistic influences. AMSF mitigates these\nchallenges by encoding all style images and textual hints with a semantic token\ndecomposition module that is adaptively injected into every cross-attention\nlayer of an frozen diffusion model. A similarity-aware re-weighting module then\nrecalibrates, at each denoising step, the attention allocated to every style\ncomponent, yielding balanced and user-controllable blends without any\nfine-tuning or external adapters. Both qualitative and quantitative evaluations\nshow that AMSF produces multi-style fusion results that consistently outperform\nthe state-of-the-art approaches, while its fusion design scales seamlessly to\ntwo or more styles. These capabilities position AMSF as a practical step toward\nexpressive multi-style generation in diffusion models.", "AI": {"tldr": "AMSF\u662f\u4e00\u4e2a\u521b\u65b0\u7684\u65e0\u8bad\u7ec3\u6846\u67b6\uff0c\u80fd\u591f\u878d\u5408\u591a\u79cd\u53c2\u8003\u98ce\u683c\u5230\u6269\u6563\u6a21\u578b\u4e2d\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u53ea\u80fd\u63a5\u53d7\u5355\u4e00\u98ce\u683c\u548c\u96be\u4ee5\u5e73\u8861\u591a\u79cd\u98ce\u683c\u5f71\u54cd\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u53c2\u8003\u56fe\u7684\u65b9\u6cd5\u5728\u5904\u7406\u5355\u4e00\u98ce\u683c\u65f6\u5b58\u5728\u5c40\u9650\uff0c\u65e0\u6cd5\u5b9e\u73b0\u6df7\u5408\u7f8e\u5b66\u548c\u6269\u5c55\u5230\u66f4\u591a\u98ce\u683c\uff0c\u5e76\u4e14\u7f3a\u4e4f\u5e73\u8861\u591a\u79cd\u98ce\u683c\u5f71\u54cd\u7684\u673a\u5236\u3002", "method": "AMSF\u901a\u8fc7\u4e00\u4e2a\u8bed\u4e49\u4ee4\u724c\u5206\u89e3\u6a21\u5757\u6765\u7f16\u7801\u6240\u6709\u98ce\u683c\u56fe\u50cf\u548c\u6587\u672c\u63d0\u793a\uff0c\u8be5\u6a21\u5757\u81ea\u9002\u5e94\u5730\u6ce8\u5165\u5230\u51bb\u7ed3\u7684\u6269\u6563\u6a21\u578b\u7684\u6bcf\u4e2a\u4ea4\u53c9\u6ce8\u610f\u529b\u5c42\u3002\u4e00\u4e2a\u76f8\u4f3c\u5ea6\u611f\u77e5\u91cd\u52a0\u6743\u6a21\u5757\u5728\u6bcf\u4e2a\u53bb\u566a\u6b65\u9aa4\u4e2d\u91cd\u65b0\u6821\u51c6\u5206\u914d\u7ed9\u6bcf\u4e2a\u98ce\u683c\u7ec4\u4ef6\u7684\u6ce8\u610f\u529b\uff0c\u4ece\u800c\u5b9e\u73b0\u65e0\u9700\u5fae\u8c03\u6216\u5916\u90e8\u9002\u914d\u5668\u7684\u3001\u5e73\u8861\u4e14\u7528\u6237\u53ef\u63a7\u7684\u98ce\u683c\u878d\u5408\u3002", "result": "AMSF\u5728\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u4f30\u4e2d\u5747\u8868\u73b0\u51fa\u8272\uff0c\u5176\u591a\u98ce\u683c\u878d\u5408\u7ed3\u679c\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5e76\u4e14\u5176\u878d\u5408\u8bbe\u8ba1\u80fd\u591f\u65e0\u7f1d\u6269\u5c55\u5230\u4e24\u79cd\u6216\u66f4\u591a\u98ce\u683c\u3002", "conclusion": "AMSF\u901a\u8fc7\u5176\u65b0\u9896\u7684\u878d\u5408\u8bbe\u8ba1\uff0c\u4e3a\u6269\u6563\u6a21\u578b\u4e2d\u7684\u8868\u8fbe\u6027\u591a\u98ce\u683c\u751f\u6210\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u751f\u6210\u5e73\u8861\u4e14\u7528\u6237\u53ef\u63a7\u7684\u591a\u98ce\u683c\u878d\u5408\u56fe\u50cf\u3002"}}
{"id": "2509.18171", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18171", "abs": "https://arxiv.org/abs/2509.18171", "authors": ["Zhanting Zhou", "KaHou Tam", "Zeqin Wu", "Pengzhao Sun", "Jinbo Wang", "Fengli Zhang"], "title": "FedIA: A Plug-and-Play Importance-Aware Gradient Pruning Aggregation Method for Domain-Robust Federated Graph Learning on Node Classification", "comment": null, "summary": "Federated Graph Learning (FGL) under domain skew -- as observed on platforms\nsuch as \\emph{Twitch Gamers} and multilingual \\emph{Wikipedia} networks --\ndrives client models toward incompatible representations, rendering naive\naggregation both unstable and ineffective. We find that the culprit is not the\nweighting scheme but the \\emph{noisy gradient signal}: empirical analysis of\nbaseline methods suggests that a vast majority of gradient dimensions can be\ndominated by domain-specific variance. We therefore shift focus from\n\"aggregation-first\" to a \\emph{projection-first} strategy that denoises client\nupdates \\emph{before} they are combined. The proposed FedIA framework realises\nthis \\underline{I}mportance-\\underline{A}ware idea through a two-stage,\nplug-and-play pipeline: (i) a server-side top-$\\rho$ mask keeps only the most\ninformative about 5% of coordinates, and (ii) a lightweight\ninfluence-regularised momentum weight suppresses outlier clients. FedIA adds\n\\emph{no extra uplink traffic and only negligible server memory}, making it\nreadily deployable. On both homogeneous (Twitch Gamers) and heterogeneous\n(Wikipedia) graphs, it yields smoother, more stable convergence and higher\nfinal accuracy than nine strong baselines. A convergence sketch further shows\nthat dynamic projection maintains the optimal\n$\\mathcal{O}(\\sigma^{2}/\\sqrt{T})$ rate.", "AI": {"tldr": "FedIA\u6846\u67b6\u901a\u8fc7\u5728\u805a\u5408\u524d\u8fdb\u884c\u53bb\u566a\u6765\u89e3\u51b3\u8054\u90a6\u56fe\u5b66\u4e60\u4e2d\u7684\u57df\u504f\u79fb\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u8868\u793a\u7684\u517c\u5bb9\u6027\u3001\u7a33\u5b9a\u6027\u548c\u6700\u7ec8\u51c6\u786e\u6027\u3002", "motivation": "\u5728Twitch Gamers\u548c\u591a\u8bed\u8a00Wikipedia\u7b49\u5e73\u53f0\u4e0a\u7684\u8054\u90a6\u56fe\u5b66\u4e60\uff08FGL\uff09\u4e2d\uff0c\u57df\u504f\u79fb\u4f1a\u5bfc\u81f4\u5ba2\u6237\u7aef\u6a21\u578b\u4ea7\u751f\u4e0d\u517c\u5bb9\u7684\u8868\u793a\uff0c\u4f7f\u5f97\u6734\u7d20\u805a\u5408\u4e0d\u7a33\u5b9a\u4e14\u65e0\u6548\u3002", "method": "\u63d0\u51faFedIA\u6846\u67b6\uff0c\u91c7\u7528\u201c\u5148\u6295\u5f71\uff0c\u540e\u805a\u5408\u201d\u7684\u7b56\u7565\u3002\u5177\u4f53\u5305\u62ec\uff1a1. \u670d\u52a1\u5668\u7aef\u4f7f\u7528Top-\u03c1\uff08\u7ea65%\uff09\u63a9\u7801\u4fdd\u7559\u6700\u5177\u4fe1\u606f\u91cf\u7684\u5750\u6807\uff1b2. \u4f7f\u7528\u8f7b\u91cf\u7ea7\u5f71\u54cd\u6b63\u5219\u5316\u52a8\u91cf\u6743\u91cd\u6291\u5236\u5f02\u5e38\u503c\u5ba2\u6237\u7aef\u3002\u8be5\u6846\u67b6\u4e0d\u589e\u52a0\u989d\u5916\u7684\u4e0a\u884c\u6d41\u91cf\uff0c\u670d\u52a1\u5668\u5185\u5b58\u5f00\u9500\u53ef\u5ffd\u7565\u3002", "result": "\u5728\u540c\u6784\uff08Twitch Gamers\uff09\u548c\u5f02\u6784\uff08Wikipedia\uff09\u56fe\u4e0a\uff0cFedIA\u7684\u6536\u655b\u66f4\u5e73\u7a33\u3001\u66f4\u7a33\u5b9a\uff0c\u5e76\u4e14\u6700\u7ec8\u51c6\u786e\u6027\u4f18\u4e8e\u4e5d\u79cd\u5f3a\u57fa\u7ebf\u65b9\u6cd5\u3002\u6536\u655b\u5206\u6790\u8868\u660e\uff0c\u52a8\u6001\u6295\u5f71\u53ef\u4ee5\u7ef4\u6301\u6700\u4f18\u7684O(\u03c3^2/\u221aT)\u6536\u655b\u7387\u3002", "conclusion": "FedIA\u901a\u8fc7\u5728\u805a\u5408\u524d\u5bf9\u5ba2\u6237\u7aef\u66f4\u65b0\u8fdb\u884c\u53bb\u566a\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8054\u90a6\u56fe\u5b66\u4e60\u4e2d\u7684\u57df\u504f\u79fb\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u6027\u80fd\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002"}}
{"id": "2509.19236", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19236", "abs": "https://arxiv.org/abs/2509.19236", "authors": ["Chunhao Tian", "Yutong Wang", "Xuebo Liu", "Zhexuan Wang", "Liang Ding", "Miao Zhang", "Min Zhang"], "title": "AgentInit: Initializing LLM-based Multi-Agent Systems via Diversity and Expertise Orchestration for Effective and Efficient Collaboration", "comment": "EMNLP 2025 Findings", "summary": "Proper initialization is crucial for any system, particularly in multi-agent\nsystems (MAS), where it plays a pivotal role in determining both the system's\nefficiency and effectiveness. However, existing MAS initialization methods do\nnot fully account for the collaborative needs of the generated agents in\nsubsequent stages. Inspired by the principles of effective team composition, we\npropose AgentInit, which aims to optimize the structure of agent teams.\nSpecifically, in addition to multi-round interactions and reflections between\nagents during agent generation, AgentInit incorporates a Natural Language to\nFormat mechanism to ensure consistency and standardization. Balanced team\nselection strategies using Pareto principles are subsequently applied to\njointly consider agent team diversity and task relevance to promote effective\nand efficient collaboration and enhance overall system performance. Experiments\nshow that AgentInit consistently outperforms state-of-the-art initialization\nmethods and pre-defined strategies across various frameworks and tasks,\nachieving an overall performance improvement of up to 1.2 and 1.6,\nrespectively, while also significantly reducing token consumption. Further\nanalysis confirms its strong transferability to similar tasks and verifies the\neffectiveness of its key components, demonstrating its capability and\nadaptability as a reliable MAS initialization method. Source code and models\nare available at https://github.com/1737423697/AgentInit.", "AI": {"tldr": "AgentInit\u662f\u4e00\u79cd\u7528\u4e8e\u4f18\u5316\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08MAS\uff09\u56e2\u961f\u7ed3\u6784\u7684\u65b0\u578b\u521d\u59cb\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u8f6e\u4ea4\u4e92\u3001\u81ea\u7136\u8bed\u8a00\u5230\u683c\u5f0f\u7684\u8f6c\u6362\u4ee5\u53ca\u57fa\u4e8ePareto\u539f\u5219\u7684\u56e2\u961f\u9009\u62e9\u7b56\u7565\uff0c\u63d0\u9ad8\u4e86\u56e2\u961f\u534f\u4f5c\u6548\u7387\u548c\u4efb\u52a1\u76f8\u5173\u6027\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5177\u6709\u826f\u597d\u7684\u53ef\u8fc1\u79fb\u6027\u3002", "motivation": "\u73b0\u6709MAS\u521d\u59cb\u5316\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u8003\u8651\u751f\u6210Agent\u5728\u540e\u7eed\u9636\u6bb5\u7684\u534f\u4f5c\u9700\u6c42\uff0c\u800cAgentInit\u65e8\u5728\u4f18\u5316Agent\u56e2\u961f\u7684\u7ed3\u6784\u4ee5\u6ee1\u8db3\u8fd9\u4e9b\u9700\u6c42\u3002", "method": "AgentInit\u7ed3\u5408\u4e86\u591a\u8f6eAgent\u4ea4\u4e92\u4e0e\u53cd\u601d\u3001\u81ea\u7136\u8bed\u8a00\u5230\u683c\u5f0f\u7684\u8f6c\u6362\u673a\u5236\u4ee5\u53ca\u57fa\u4e8ePareto\u539f\u5219\u7684\u56e2\u961f\u9009\u62e9\u7b56\u7565\uff0c\u4ee5\u5b9e\u73b0Agent\u56e2\u961f\u7ed3\u6784\u7684\u6700\u4f18\u5316\u3002", "result": "AgentInit\u5728\u5404\u79cd\u6846\u67b6\u548c\u4efb\u52a1\u4e2d\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u521d\u59cb\u5316\u65b9\u6cd5\u548c\u9884\u5b9a\u4e49\u7b56\u7565\uff0c\u6574\u4f53\u6027\u80fd\u5206\u522b\u63d0\u9ad8\u4e861.2\u548c1.6\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u4ee3\u5e01\u6d88\u8017\u3002\u6b64\u5916\uff0c\u5176\u5f3a\u5927\u7684\u53ef\u8fc1\u79fb\u6027\u548c\u5173\u952e\u7ec4\u6210\u90e8\u5206\u7684\u6709\u6548\u6027\u4e5f\u5f97\u5230\u4e86\u9a8c\u8bc1\u3002", "conclusion": "AgentInit\u4f5c\u4e3a\u4e00\u79cd\u53ef\u9760\u7684MAS\u521d\u59cb\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u56e2\u961f\u7ed3\u6784\u3001\u4fc3\u8fdb\u6709\u6548\u534f\u4f5c\u548c\u63d0\u9ad8\u6574\u4f53\u7cfb\u7edf\u6027\u80fd\uff0c\u5c55\u73b0\u4e86\u5176\u4f18\u8d8a\u6027\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2509.19109", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.19109", "abs": "https://arxiv.org/abs/2509.19109", "authors": ["Timur Turatali", "Anton Alekseev", "Gulira Jumalieva", "Gulnara Kabaeva", "Sergey Nikolenko"], "title": "Human-Annotated NER Dataset for the Kyrgyz Language", "comment": "Accepted to TurkLang-2025 conference, DOI and copyright will be added\n  upon confirmation of acceptance to publication in IEEE Xplore", "summary": "We introduce KyrgyzNER, the first manually annotated named entity recognition\ndataset for the Kyrgyz language. Comprising 1,499 news articles from the 24.KG\nnews portal, the dataset contains 10,900 sentences and 39,075 entity mentions\nacross 27 named entity classes. We show our annotation scheme, discuss the\nchallenges encountered in the annotation process, and present the descriptive\nstatistics. We also evaluate several named entity recognition models, including\ntraditional sequence labeling approaches based on conditional random fields and\nstate-of-the-art multilingual transformer-based models fine-tuned on our\ndataset. While all models show difficulties with rare entity categories, models\nsuch as the multilingual RoBERTa variant pretrained on a large corpus across\nmany languages achieve a promising balance between precision and recall. These\nfindings emphasize both the challenges and opportunities of using multilingual\npretrained models for processing languages with limited resources. Although the\nmultilingual RoBERTa model performed best, other multilingual models yielded\ncomparable results. This suggests that future work exploring more granular\nannotation schemes may offer deeper insights for Kyrgyz language processing\npipelines evaluation.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86 KyrgyzNER\uff0c\u8fd9\u662f\u9996\u4e2a\u4e3a\u5409\u5c14\u5409\u65af\u8bed\u624b\u52a8\u6807\u6ce8\u7684\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u6570\u636e\u96c6\u3002\u8be5\u6570\u636e\u96c6\u5305\u542b 1,499 \u7bc7\u65b0\u95fb\u6587\u7ae0\uff0c\u5171 10,900 \u53e5\uff0c39,075 \u4e2a\u5b9e\u4f53\u63d0\u53ca\uff0c\u6db5\u76d6 27 \u7c7b\u547d\u540d\u5b9e\u4f53\u3002\u7814\u7a76\u5c55\u793a\u4e86\u6807\u6ce8\u65b9\u6848\uff0c\u8ba8\u8bba\u4e86\u6807\u6ce8\u8fc7\u7a0b\u4e2d\u7684\u6311\u6218\uff0c\u5e76\u5448\u73b0\u4e86\u63cf\u8ff0\u6027\u7edf\u8ba1\u3002", "motivation": "\u73b0\u6709\u5409\u5c14\u5409\u65af\u8bed\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u8d44\u6e90\u532e\u4e4f\uff0c\u9700\u8981\u6784\u5efa\u6570\u636e\u96c6\u6765\u63a8\u52a8\u8be5\u9886\u57df\u7684\u7814\u7a76\u548c\u5e94\u7528\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b 1,499 \u7bc7\u65b0\u95fb\u6587\u7ae0\u7684\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u6570\u636e\u96c6\uff08KyrgyzNER\uff09\uff0c\u5305\u542b 10,900 \u53e5\u548c 39,075 \u4e2a\u5b9e\u4f53\u63d0\u53ca\uff0c\u6db5\u76d6 27 \u4e2a\u547d\u540d\u5b9e\u4f53\u7c7b\u522b\u3002\u8bc4\u4f30\u4e86\u6761\u4ef6\u968f\u673a\u573a\u548c\u591a\u8bed\u8a00 Transformer \u6a21\u578b\uff08\u5982 RoBERTa \u53d8\u4f53\uff09\u5728\u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\u3002", "result": "\u591a\u8bed\u8a00 RoBERTa \u6a21\u578b\u5728 precision \u548c recall \u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u7684\u5e73\u8861\uff0c\u5c3d\u7ba1\u6240\u6709\u6a21\u578b\u5728\u5904\u7406\u7a00\u6709\u5b9e\u4f53\u7c7b\u522b\u65f6\u90fd\u5b58\u5728\u56f0\u96be\u3002\u5176\u4ed6\u591a\u8bed\u8a00\u6a21\u578b\u4e5f\u53d6\u5f97\u4e86\u53ef\u6bd4\u7684\u7ed3\u679c\u3002", "conclusion": "\u591a\u8bed\u8a00\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u5904\u7406\u5409\u5c14\u5409\u65af\u8bed\u7b49\u8d44\u6e90\u6709\u9650\u7684\u8bed\u8a00\u65b9\u9762\u65e2\u5145\u6ee1\u6311\u6218\u4e5f\u5145\u6ee1\u673a\u9047\u3002\u672a\u6765\u7684\u5de5\u4f5c\u53ef\u4ee5\u63a2\u7d22\u66f4\u7ec6\u7c92\u5ea6\u7684\u6807\u6ce8\u65b9\u6848\uff0c\u4ee5\u63d0\u4f9b\u66f4\u6df1\u5165\u7684\u89c1\u89e3\u3002"}}
{"id": "2509.19105", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.19105", "abs": "https://arxiv.org/abs/2509.19105", "authors": ["Sarvesh Prajapati", "Ananya Trivedi", "Nathaniel Hanson", "Bruce Maxwell", "Taskin Padir"], "title": "Spectral Signature Mapping from RGB Imagery for Terrain-Aware Navigation", "comment": "8 pages, 10 figures, submitted to Robotic Computing & Communication", "summary": "Successful navigation in outdoor environments requires accurate prediction of\nthe physical interactions between the robot and the terrain. To this end,\nseveral methods rely on geometric or semantic labels to classify traversable\nsurfaces. However, such labels cannot distinguish visually similar surfaces\nthat differ in material properties. Spectral sensors enable inference of\nmaterial composition from surface reflectance measured across multiple\nwavelength bands. Although spectral sensing is gaining traction in robotics,\nwidespread deployment remains constrained by the need for custom hardware\nintegration, high sensor costs, and compute-intensive processing pipelines. In\nthis paper, we present RGB Image to Spectral Signature Neural Network (RS-Net),\na deep neural network designed to bridge the gap between the accessibility of\nRGB sensing and the rich material information provided by spectral data. RS-Net\npredicts spectral signatures from RGB patches, which we map to terrain labels\nand friction coefficients. The resulting terrain classifications are integrated\ninto a sampling-based motion planner for a wheeled robot operating in outdoor\nenvironments. Likewise, the friction estimates are incorporated into a\ncontact-force-based MPC for a quadruped robot navigating slippery surfaces.\nThus, we introduce a framework that learns the task-relevant physical property\nonce during training and thereafter relies solely on RGB sensing at test time.\nThe code is available at https://github.com/prajapatisarvesh/RS-Net.", "AI": {"tldr": "RS-Net\u5229\u7528RGB\u56fe\u50cf\u9884\u6d4b\u5149\u8c31\u7279\u5f81\uff0c\u4ece\u800c\u63a8\u65ad\u5730\u5f62\u548c\u6469\u64e6\u7cfb\u6570\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u8f6e\u5f0f\u548c\u56db\u8db3\u673a\u5668\u4eba\u7684\u8fd0\u52a8\u89c4\u5212\u4e2d\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u533a\u5206\u89c6\u89c9\u76f8\u4f3c\u4f46\u6750\u8d28\u4e0d\u540c\u7684\u5730\u8868\u65f6\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u673a\u5668\u4eba\u5bfc\u822a\u65b9\u6cd5\u4f9d\u8d56\u51e0\u4f55\u6216\u8bed\u4e49\u6807\u7b7e\u6765\u5206\u7c7b\u53ef\u901a\u884c\u8868\u9762\uff0c\u4f46\u65e0\u6cd5\u533a\u5206\u89c6\u89c9\u76f8\u4f3c\u4f46\u6750\u8d28\u4e0d\u540c\u7684\u8868\u9762\u3002\u5149\u8c31\u4f20\u611f\u53ef\u4ee5\u63a8\u65ad\u6750\u8d28\u7ec4\u6210\uff0c\u4f46\u5b58\u5728\u786c\u4ef6\u96c6\u6210\u3001\u6210\u672c\u548c\u8ba1\u7b97\u5904\u7406\u7684\u6311\u6218\u3002\u672c\u7814\u7a76\u65e8\u5728\u7ed3\u5408RGB\u4f20\u611f\u7684\u6613\u7528\u6027\u548c\u5149\u8c31\u6570\u636e\u7684\u4e30\u5bcc\u6750\u8d28\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRS-Net\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u7528\u4e8e\u4eceRGB\u56fe\u50cf\u5757\u9884\u6d4b\u5149\u8c31\u7279\u5f81\u3002\u7136\u540e\u5c06\u9884\u6d4b\u7684\u5149\u8c31\u7279\u5f81\u6620\u5c04\u5230\u5730\u5f62\u6807\u7b7e\u548c\u6469\u64e6\u7cfb\u6570\u3002\u6700\u540e\uff0c\u5c06\u5730\u5f62\u5206\u7c7b\u96c6\u6210\u5230\u91c7\u6837\u57fa\u8fd0\u52a8\u89c4\u5212\u5668\u4e2d\uff0c\u5c06\u6469\u64e6\u4f30\u8ba1\u96c6\u6210\u5230\u57fa\u4e8e\u63a5\u89e6\u529b-MPC\u7684\u63a7\u5236\u5668\u4e2d\uff0c\u7528\u4e8e\u8f6e\u5f0f\u548c\u56db\u8db3\u673a\u5668\u4eba\u5728\u6237\u5916\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u3002", "result": "RS-Net\u80fd\u591f\u4eceRGB\u56fe\u50cf\u9884\u6d4b\u5730\u5f62\u5206\u7c7b\u548c\u6469\u64e6\u7cfb\u6570\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8e\u8f6e\u5f0f\u548c\u56db\u8db3\u673a\u5668\u4eba\u7684\u5bfc\u822a\u4efb\u52a1\uff0c\u5b9e\u73b0\u4e86\u5728\u4ec5\u4f7f\u7528RGB\u4f20\u611f\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u6709\u6548\u7684\u5bfc\u822a\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6846\u67b6\uff0c\u901a\u8fc7RS-Net\u4eceRGB\u56fe\u50cf\u4e2d\u5b66\u4e60\u5e76\u9884\u6d4b\u7269\u7406\u5c5e\u6027\uff08\u5982\u5730\u5f62\u548c\u6469\u64e6\uff09\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u533a\u5206\u6750\u8d28\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u5e76\u4e3a\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u4ec5\u4f9d\u8d56RGB\u4f20\u611f\u5373\u53ef\u8fdb\u884c\u6237\u5916\u5bfc\u822a\u7684\u80fd\u529b\u3002"}}
{"id": "2509.18613", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18613", "abs": "https://arxiv.org/abs/2509.18613", "authors": ["Yuzhi Wu", "Li Xiao", "Jun Liu", "Guangfeng Jiang", "XiangGen Xia"], "title": "MLF-4DRCNet: Multi-Level Fusion with 4D Radar and Camera for 3D Object Detection in Autonomous Driving", "comment": null, "summary": "The emerging 4D millimeter-wave radar, measuring the range, azimuth,\nelevation, and Doppler velocity of objects, is recognized for its\ncost-effectiveness and robustness in autonomous driving. Nevertheless, its\npoint clouds exhibit significant sparsity and noise, restricting its standalone\napplication in 3D object detection. Recent 4D radar-camera fusion methods have\nprovided effective perception. Most existing approaches, however, adopt\nexplicit Bird's-Eye-View fusion paradigms originally designed for LiDAR-camera\nfusion, neglecting radar's inherent drawbacks. Specifically, they overlook the\nsparse and incomplete geometry of radar point clouds and restrict fusion to\ncoarse scene-level integration. To address these problems, we propose\nMLF-4DRCNet, a novel two-stage framework for 3D object detection via\nmulti-level fusion of 4D radar and camera images. Our model incorporates the\npoint-, scene-, and proposal-level multi-modal information, enabling\ncomprehensive feature representation. It comprises three crucial components:\nthe Enhanced Radar Point Encoder (ERPE) module, the Hierarchical Scene Fusion\nPooling (HSFP) module, and the Proposal-Level Fusion Enhancement (PLFE) module.\nOperating at the point-level, ERPE densities radar point clouds with 2D image\ninstances and encodes them into voxels via the proposed Triple-Attention Voxel\nFeature Encoder. HSFP dynamically integrates multi-scale voxel features with 2D\nimage features using deformable attention to capture scene context and adopts\npooling to the fused features. PLFE refines region proposals by fusing image\nfeatures, and further integrates with the pooled features from HSFP.\nExperimental results on the View-of-Delft (VoD) and TJ4DRadSet datasets\ndemonstrate that MLF-4DRCNet achieves the state-of-the-art performance.\nNotably, it attains performance comparable to LiDAR-based models on the VoD\ndataset.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86MLF-4DRCNet\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u591a\u5c42\u6b21\u878d\u54084D\u6beb\u7c73\u6ce2\u96f7\u8fbe\u548c\u6444\u50cf\u5934\u56fe\u50cf\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u7528\u4e8e3D\u76ee\u6807\u68c0\u6d4b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u96f7\u8fbe\u70b9\u4e91\u7a00\u758f\u6027\u548c\u51e0\u4f55\u4e0d\u5b8c\u6574\u6027\u7684\u95ee\u9898\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b34D\u6beb\u7c73\u6ce2\u96f7\u8fbe\u70b9\u4e91\u7a00\u758f\u3001\u566a\u58f0\u5927\u4ee5\u53ca\u73b0\u6709\u96f7\u8fbe-\u6444\u50cf\u5934\u878d\u5408\u65b9\u6cd5\u5ffd\u89c6\u96f7\u8fbe\u56fa\u6709\u7f3a\u9677\uff08\u5982\u7a00\u758f\u51e0\u4f55\u548c\u4ec5\u9650\u4e8e\u7c97\u7565\u7684\u573a\u666f\u7ea7\u96c6\u6210\uff09\u7684\u95ee\u9898\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u878d\u5408\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMLF-4DRCNet\u7684\u65b0\u578b\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u5c42\u6b21\u878d\u54084D\u96f7\u8fbe\u548c\u6444\u50cf\u5934\u56fe\u50cf\u6765\u8fdb\u884c3D\u76ee\u6807\u68c0\u6d4b\u3002\u8be5\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u589e\u5f3a\u96f7\u8fbe\u70b9\u7f16\u7801\u5668\uff08ERPE\uff09\u3001\u5206\u5c42\u573a\u666f\u878d\u5408\u6c60\u5316\uff08HSFP\uff09\u548c\u63d0\u6848\u7ea7\u878d\u5408\u589e\u5f3a\uff08PLFE\uff09\u3002ERPE\u5728\u70b9\u7ea7\u522b\u5904\u7406\uff0c\u901a\u8fc7\u63d0\u51fa\u7684\u4e09\u6ce8\u610f\u529b\u4f53\u7d20\u7279\u5f81\u7f16\u7801\u5668\u5c06\u96f7\u8fbe\u70b9\u4e91\u4e0e2D\u56fe\u50cf\u5b9e\u4f8b\u878d\u5408\u5e76\u7f16\u7801\u4e3a\u4f53\u7d20\u3002HSFP\u5728\u573a\u666f\u7ea7\u522b\u52a8\u6001\u878d\u5408\u591a\u5c3a\u5ea6\u4f53\u7d20\u7279\u5f81\u4e0e2D\u56fe\u50cf\u7279\u5f81\uff0c\u5e76\u5bf9\u878d\u5408\u540e\u7684\u7279\u5f81\u8fdb\u884c\u6c60\u5316\u3002PLFE\u5728\u63d0\u6848\u7ea7\u522b\u901a\u8fc7\u878d\u5408\u56fe\u50cf\u7279\u5f81\u6765\u4f18\u5316\u533a\u57df\u63d0\u6848\uff0c\u5e76\u4e0eHSFP\u7684\u6c60\u5316\u7279\u5f81\u8fdb\u884c\u96c6\u6210\u3002", "result": "\u5728View-of-Delft (VoD) \u548c TJ4DRadSet \u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMLF-4DRCNet \u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u5728VoD\u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\u4e0e\u57fa\u4e8eLiDAR\u7684\u6a21\u578b\u76f8\u5f53\u3002", "conclusion": "MLF-4DRCNet\u901a\u8fc7\u591a\u5c42\u6b21\u878d\u5408\uff08\u70b9\u3001\u573a\u666f\u3001\u63d0\u6848\u7ea7\u522b\uff09\u6709\u6548\u89e3\u51b3\u4e864D\u6beb\u7c73\u6ce2\u96f7\u8fbe\u57283D\u76ee\u6807\u68c0\u6d4b\u4e2d\u7684\u6311\u6218\uff0c\u5e76\u5728\u6807\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u9886\u57df\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.18172", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18172", "abs": "https://arxiv.org/abs/2509.18172", "authors": ["Wonjun Bang", "Jongseok Park", "Hongseung Yu", "Kyungmin Bin", "Kyunghan Lee"], "title": "SBVR: Summation of BitVector Representation for Efficient LLM Quantization", "comment": "9 pages, 4 figures", "summary": "With the advent of large language models (LLMs), numerous Post-Training\nQuantization (PTQ) strategies have been proposed to alleviate deployment\nbarriers created by their enormous parameter counts. Quantization achieves\ncompression by limiting the number of representable points in the data.\nTherefore, the key to achieving efficient quantization is selecting the optimal\ncombination of representation points, or codes, for the given data. Existing\nPTQ solutions adopt two major approaches to this problem: Round-To-Nearest\n(RTN)-based methods and codebook-based methods. RTN-based methods map LLM\nweights onto uniformly distributed integer grids, failing to account for the\nGaussian-like weight distribution of LLM weights. Codebook-based methods\nmitigate this issue by constructing distribution-aware codebooks; however, they\nsuffer from random and strided memory access patterns, resulting in degraded\ninference speed that is exacerbated by the limited size of GPU L1 cache. To\novercome these limitations, we propose a novel LLM quantization method, SBVR\n(Summation of BitVector Representation), that enables Gaussian-like code\nrepresentation in a hardware-friendly manner for fast inference. SBVR maps\nweight values to non-uniform representation points whose distribution follows\nthe actual distribution of LLM weights, enabling more accurate compression.\nAdditionally, we design a custom CUDA kernel that allows matrix-vector\nmultiplication directly in the SBVR format without decompression, thereby\nenabling high-performance execution of SBVR-compressed models. Our evaluations\nof SBVR on various models demonstrate state-of-the-art perplexity and accuracy\nbenchmark performance while delivering a 2.21x- 3.04x end-to-end\ntoken-generation speedup over naive FP16 models in the 4-bit quantization\nregime.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.19265", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.19265", "abs": "https://arxiv.org/abs/2509.19265", "authors": ["Saeed Almheiri", "Rania Hossam", "Mena Attia", "Chenxi Wang", "Preslav Nakov", "Timothy Baldwin", "Fajri Koto"], "title": "Cross-Cultural Transfer of Commonsense Reasoning in LLMs: Evidence from the Arab World", "comment": "EMNLP 2025 - Findings", "summary": "Large language models (LLMs) often reflect Western-centric biases, limiting\ntheir effectiveness in diverse cultural contexts. Although some work has\nexplored cultural alignment, the potential for cross-cultural transfer, using\nalignment in one culture to improve performance in others, remains\nunderexplored. This paper investigates cross-cultural transfer of commonsense\nreasoning in the Arab world, where linguistic and historical similarities\ncoexist with local cultural differences. Using a culturally grounded\ncommonsense reasoning dataset covering 13 Arab countries, we evaluate\nlightweight alignment methods such as in-context learning and\ndemonstration-based reinforcement (DITTO), alongside baselines like supervised\nfine-tuning and direct preference optimization. Our results show that merely 12\nculture-specific examples from one country can improve performance in others by\n10\\% on average, within multilingual models. In addition, we demonstrate that\nout-of-culture demonstrations from Indonesia and US contexts can match or\nsurpass in-culture alignment for MCQ reasoning, highlighting cultural\ncommonsense transferability beyond the Arab world. These findings demonstrate\nthat efficient cross-cultural alignment is possible and offer a promising\napproach to adapt LLMs to low-resource cultural settings.", "AI": {"tldr": "\u8de8\u6587\u5316\u8fc1\u79fb\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6587\u5316\u9002\u5e94\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u672c\u7814\u7a76\u901a\u8fc7\u963f\u62c9\u4f2f\u4e16\u754c\u7684\u8de8\u6587\u5316\u5e38\u8bc6\u63a8\u7406\u5b9e\u9a8c\u8fdb\u884c\u9a8c\u8bc1\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5e38\u5e26\u6709\u897f\u65b9\u4e2d\u5fc3\u504f\u89c1\uff0c\u9650\u5236\u4e86\u5176\u5728\u591a\u5143\u6587\u5316\u80cc\u666f\u4e0b\u7684\u6709\u6548\u6027\u3002\u867d\u7136\u5df2\u6709\u7814\u7a76\u63a2\u7d22\u6587\u5316\u9002\u5e94\uff0c\u4f46\u5229\u7528\u4e00\u79cd\u6587\u5316\u7684\u9002\u5e94\u6027\u6765\u6539\u5584\u53e6\u4e00\u79cd\u6587\u5316\u7684\u6027\u80fd\u7684\u8de8\u6587\u5316\u8fc1\u79fb\u6f5c\u529b\u4ecd\u6709\u5f85\u53d1\u6398\u3002", "method": "\u672c\u7814\u7a76\u4f7f\u7528\u6db5\u76d613\u4e2a\u963f\u62c9\u4f2f\u56fd\u5bb6\u7684\u3001\u57fa\u4e8e\u6587\u5316\u7684\u5e38\u8bc6\u63a8\u7406\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e86\u4e0a\u4e0b\u6587\u5b66\u4e60\u548c\u57fa\u4e8e\u6f14\u793a\u7684\u5f3a\u5316\uff08DITTO\uff09\u7b49\u8f7b\u91cf\u7ea7\u9002\u5e94\u65b9\u6cd5\uff0c\u4ee5\u53ca\u76d1\u7763\u5fae\u8c03\u548c\u76f4\u63a5\u504f\u597d\u4f18\u5316\u7b49\u57fa\u7ebf\u65b9\u6cd5\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u4ec5\u4f7f\u7528\u4e00\u4e2a\u56fd\u5bb6\u768412\u4e2a\u7279\u5b9a\u6587\u5316\u793a\u4f8b\uff0c\u5c31\u53ef\u4ee5\u5728\u591a\u8bed\u8a00\u6a21\u578b\u4e2d\u5c06\u5176\u4ed6\u56fd\u5bb6\u7684\u6027\u80fd\u5e73\u5747\u63d0\u9ad810%\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u53d1\u73b0\uff0c\u6765\u81ea\u5370\u5ea6\u5c3c\u897f\u4e9a\u548c\u7f8e\u56fd\u6587\u5316\u80cc\u666f\u7684\u793a\u4f8b\u5728\u9009\u62e9\u9898\u63a8\u7406\u65b9\u9762\u53ef\u4ee5\u5339\u914d\u751a\u81f3\u8d85\u8d8a\u672c\u56fd\u6587\u5316\u9002\u5e94\uff0c\u8868\u660e\u5e38\u8bc6\u63a8\u7406\u5177\u6709\u8de8\u8d8a\u963f\u62c9\u4f2f\u4e16\u754c\u7684\u6587\u5316\u8fc1\u79fb\u80fd\u529b\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660e\u4e86\u9ad8\u6548\u7684\u8de8\u6587\u5316\u9002\u5e94\u662f\u53ef\u884c\u7684\uff0c\u5e76\u4e3a\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5e94\u7528\u4e8e\u8d44\u6e90\u532e\u4e4f\u7684\u6587\u5316\u73af\u5883\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u666f\u7684\u65b9\u6cd5\u3002"}}
{"id": "2509.19125", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.19125", "abs": "https://arxiv.org/abs/2509.19125", "authors": ["Kun Zhu", "Lizi Liao", "Yuxuan Gu", "Lei Huang", "Xiaocheng Feng", "Bing Qin"], "title": "Context-Aware Hierarchical Taxonomy Generation for Scientific Papers via LLM-Guided Multi-Aspect Clustering", "comment": "Accepted to EMNLP 2025 Main", "summary": "The rapid growth of scientific literature demands efficient methods to\norganize and synthesize research findings. Existing taxonomy construction\nmethods, leveraging unsupervised clustering or direct prompting of large\nlanguage models (LLMs), often lack coherence and granularity. We propose a\nnovel context-aware hierarchical taxonomy generation framework that integrates\nLLM-guided multi-aspect encoding with dynamic clustering. Our method leverages\nLLMs to identify key aspects of each paper (e.g., methodology, dataset,\nevaluation) and generates aspect-specific paper summaries, which are then\nencoded and clustered along each aspect to form a coherent hierarchy. In\naddition, we introduce a new evaluation benchmark of 156 expert-crafted\ntaxonomies encompassing 11.6k papers, providing the first naturally annotated\ndataset for this task. Experimental results demonstrate that our method\nsignificantly outperforms prior approaches, achieving state-of-the-art\nperformance in taxonomy coherence, granularity, and interpretability.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7ed3\u5408LLM\u548c\u52a8\u6001\u805a\u7c7b\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u5206\u5c42\u5206\u7c7b\u6cd5\u751f\u6210\u6846\u67b6\uff0c\u7528\u4e8e\u7ec4\u7ec7\u548c\u7efc\u5408\u79d1\u5b66\u6587\u732e\uff0c\u5e76\u5728\u65b0\u7684\u8bc4\u4f30\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u79d1\u5b66\u6587\u732e\u7684\u5feb\u901f\u589e\u957f\u9700\u8981\u6709\u6548\u7684\u7ec4\u7ec7\u548c\u7efc\u5408\u7814\u7a76\u53d1\u73b0\u7684\u65b9\u6cd5\uff0c\u800c\u73b0\u6709\u7684\u65b9\u6cd5\u7f3a\u4e4f\u8fde\u8d2f\u6027\u548c\u7c92\u5ea6\u3002", "method": "\u5229\u7528LLM\u8bc6\u522b\u8bba\u6587\u7684\u5173\u952e\u65b9\u9762\uff08\u5982\u65b9\u6cd5\u3001\u6570\u636e\u96c6\u3001\u8bc4\u4f30\uff09\uff0c\u751f\u6210\u7279\u5b9a\u65b9\u9762\u7684\u8bba\u6587\u6458\u8981\uff0c\u7136\u540e\u5bf9\u8fd9\u4e9b\u6458\u8981\u8fdb\u884c\u7f16\u7801\u548c\u805a\u7c7b\uff0c\u4ee5\u5f62\u6210\u8fde\u8d2f\u7684\u5c42\u6b21\u7ed3\u6784\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u5206\u7c7b\u6cd5\u7684\u8fde\u8d2f\u6027\u3001\u7c92\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u6709\u6548\u5730\u751f\u6210\u8fde\u8d2f\u3001\u7ec6\u81f4\u4e14\u53ef\u89e3\u91ca\u7684\u79d1\u5b66\u6587\u732e\u5206\u7c7b\u6cd5\uff0c\u4e3a\u8be5\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u548c\u8bc4\u4f30\u6807\u51c6\u3002"}}
{"id": "2509.19142", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.19142", "abs": "https://arxiv.org/abs/2509.19142", "authors": ["Kangmin Kim", "Seunghyeok Back", "Geonhyup Lee", "Sangbeom Lee", "Sangjun Noh", "Kyoobin Lee"], "title": "BiGraspFormer: End-to-End Bimanual Grasp Transformer", "comment": "8 pages, 5 figures", "summary": "Bimanual grasping is essential for robots to handle large and complex\nobjects. However, existing methods either focus solely on single-arm grasping\nor employ separate grasp generation and bimanual evaluation stages, leading to\ncoordination problems including collision risks and unbalanced force\ndistribution. To address these limitations, we propose BiGraspFormer, a unified\nend-to-end transformer framework that directly generates coordinated bimanual\ngrasps from object point clouds. Our key idea is the Single-Guided Bimanual\n(SGB) strategy, which first generates diverse single grasp candidates using a\ntransformer decoder, then leverages their learned features through specialized\nattention mechanisms to jointly predict bimanual poses and quality scores. This\nconditioning strategy reduces the complexity of the 12-DoF search space while\nensuring coordinated bimanual manipulation. Comprehensive simulation\nexperiments and real-world validation demonstrate that BiGraspFormer\nconsistently outperforms existing methods while maintaining efficient inference\nspeed (<0.05s), confirming the effectiveness of our framework. Code and\nsupplementary materials are available at https://sites.google.com/bigraspformer", "AI": {"tldr": "BiGraspFormer\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684Transformer\u6846\u67b6\uff0c\u53ef\u4ee5\u76f4\u63a5\u4ece\u7269\u4f53\u70b9\u4e91\u751f\u6210\u534f\u8c03\u7684\u53cc\u81c2\u6293\u53d6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u534f\u8c03\u6293\u53d6\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u53cc\u81c2\u6293\u53d6\u65b9\u9762\u5b58\u5728\u534f\u8c03\u95ee\u9898\uff0c\u5982\u78b0\u649e\u98ce\u9669\u548c\u529b\u4e0d\u5e73\u8861\uff0c\u800cBiGraspFormer\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\u3002", "method": "BiGraspFormer\u91c7\u7528Single-Guided Bimanual (SGB)\u7b56\u7565\uff0c\u9996\u5148\u751f\u6210\u591a\u79cd\u5355\u81c2\u6293\u53d6\u5019\u9009\uff0c\u7136\u540e\u5229\u7528\u6ce8\u610f\u529b\u673a\u5236\u9884\u6d4b\u53cc\u81c2\u59ff\u6001\u548c\u8d28\u91cf\u5206\u6570\u3002", "result": "BiGraspFormer\u5728\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u4e14\u63a8\u7406\u901f\u5ea6\u5feb\uff08<0.05s\uff09\u3002", "conclusion": "BiGraspFormer\u80fd\u591f\u6709\u6548\u751f\u6210\u534f\u8c03\u7684\u53cc\u81c2\u6293\u53d6\uff0c\u5e76\u4e14\u6027\u80fd\u4f18\u8d8a\u3002"}}
{"id": "2509.18619", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18619", "abs": "https://arxiv.org/abs/2509.18619", "authors": ["Yichen Wu", "Xu Liu", "Chenxuan Zhao", "Xinyu Wu"], "title": "Prompt-Guided Dual Latent Steering for Inversion Problems", "comment": "Accepted at DICTA 2025 (oral)", "summary": "Inverting corrupted images into the latent space of diffusion models is\nchallenging. Current methods, which encode an image into a single latent\nvector, struggle to balance structural fidelity with semantic accuracy, leading\nto reconstructions with semantic drift, such as blurred details or incorrect\nattributes. To overcome this, we introduce Prompt-Guided Dual Latent Steering\n(PDLS), a novel, training-free framework built upon Rectified Flow models for\ntheir stable inversion paths. PDLS decomposes the inversion process into two\ncomplementary streams: a structural path to preserve source integrity and a\nsemantic path guided by a prompt. We formulate this dual guidance as an optimal\ncontrol problem and derive a closed-form solution via a Linear Quadratic\nRegulator (LQR). This controller dynamically steers the generative trajectory\nat each step, preventing semantic drift while ensuring the preservation of fine\ndetail without costly, per-image optimization. Extensive experiments on FFHQ-1K\nand ImageNet-1K under various inversion tasks, including Gaussian deblurring,\nmotion deblurring, super-resolution and freeform inpainting, demonstrate that\nPDLS produces reconstructions that are both more faithful to the original image\nand better aligned with the semantic information than single-latent baselines.", "AI": {"tldr": "PDLS\u901a\u8fc7\u5f15\u5165\u4e24\u4e2a\u4e92\u8865\u7684\u201c\u7ed3\u6784\u8def\u5f84\u201d\u548c\u201c\u8bed\u4e49\u8def\u5f84\u201d\u6765\u89e3\u51b3\u6269\u6563\u6a21\u578b\u56fe\u50cf\u6062\u590d\u4e2d\u7684\u7ed3\u6784\u4fdd\u771f\u5ea6\u548c\u8bed\u4e49\u51c6\u786e\u6027\u4e4b\u95f4\u7684\u5e73\u8861\u95ee\u9898\uff0c\u901a\u8fc7\u6700\u4f18\u63a7\u5236\u7406\u8bba\u548c\u7ebf\u6027\u4e8c\u6b21\u8c03\u8282\u5668\uff08LQR\uff09\u5b9e\u73b0\uff0c\u65e0\u9700\u6602\u8d35\u7684\u6bcf\u56fe\u50cf\u4f18\u5316\uff0c\u5e76\u5728\u5404\u79cd\u56fe\u50cf\u6062\u590d\u4efb\u52a1\u4e2d\u4f18\u4e8e\u5355\u9690\u53d8\u91cf\u57fa\u7ebf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5c06\u635f\u574f\u56fe\u50cf\u6620\u5c04\u5230\u6269\u6563\u6a21\u578b\u6f5c\u5728\u7a7a\u95f4\u65f6\uff0c\u96be\u4ee5\u5e73\u8861\u7ed3\u6784\u4fdd\u771f\u5ea6\u548c\u8bed\u4e49\u51c6\u786e\u6027\uff0c\u5bfc\u81f4\u91cd\u5efa\u56fe\u50cf\u51fa\u73b0\u8bed\u4e49\u6f02\u79fb\u3001\u7ec6\u8282\u6a21\u7cca\u6216\u5c5e\u6027\u9519\u8bef\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3a\u201c\u63d0\u793a\u5f15\u5bfc\u53cc\u91cd\u6f5c\u5728\u5411\u91cf\u5f15\u5bfc\u201d\uff08PDLS\uff09\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u57fa\u4e8e\u5177\u6709\u7a33\u5b9a\u9006\u8f6c\u8def\u5f84\u7684Rectified Flow\u6a21\u578b\uff0c\u5e76\u5c06\u9006\u8f6c\u8fc7\u7a0b\u5206\u89e3\u4e3a\u4e24\u6761\u4e92\u8865\u7684\u8def\u5f84\uff1a\u4e00\u6761\u7ed3\u6784\u8def\u5f84\u7528\u4e8e\u4fdd\u7559\u539f\u59cb\u56fe\u50cf\u7684\u5b8c\u6574\u6027\uff0c\u4e00\u6761\u7531\u63d0\u793a\u5f15\u5bfc\u7684\u8bed\u4e49\u8def\u5f84\u3002\u8be5\u6846\u67b6\u5c06\u53cc\u91cd\u5f15\u5bfc\u8868\u8ff0\u4e3a\u6700\u4f18\u63a7\u5236\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u7ebf\u6027\u4e8c\u6b21\u8c03\u8282\u5668\uff08LQR\uff09\u63a8\u5bfc\u51fa\u95ed\u5f0f\u89e3\uff0c\u8be5\u8c03\u8282\u5668\u5728\u6bcf\u4e00\u6b65\u52a8\u6001\u5730\u5f15\u5bfc\u751f\u6210\u8f68\u8ff9\uff0c\u4ee5\u9632\u6b62\u8bed\u4e49\u6f02\u79fb\u5e76\u786e\u4fdd\u7ec6\u8282\u7684\u4fdd\u7559\u3002", "result": "\u5728FFHQ-1K\u548cImageNet-1K\u6570\u636e\u96c6\u4e0a\uff0cPDLS\u5728\u5404\u79cd\u56fe\u50cf\u6062\u590d\u4efb\u52a1\uff08\u5305\u62ec\u9ad8\u65af\u53bb\u6a21\u7cca\u3001\u8fd0\u52a8\u53bb\u6a21\u7cca\u3001\u8d85\u5206\u8fa8\u7387\u548c\u81ea\u7531\u5f62\u5f0f\u4fee\u590d\uff09\u4e2d\uff0c\u4e0e\u5355\u9690\u53d8\u91cf\u57fa\u7ebf\u76f8\u6bd4\uff0c\u80fd\u591f\u751f\u6210\u66f4\u4fdd\u771f\u4e8e\u539f\u59cb\u56fe\u50cf\u4e14\u4e0e\u8bed\u4e49\u4fe1\u606f\u66f4\u4e00\u81f4\u7684\u91cd\u5efa\u56fe\u50cf\u3002", "conclusion": "PDLS\u662f\u4e00\u79cd\u65b0\u9896\u7684\u3001\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u7ed3\u6784\u548c\u8bed\u4e49\u5f15\u5bfc\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6269\u6563\u6a21\u578b\u4e2d\u56fe\u50cf\u9006\u8f6c\u7684\u6311\u6218\uff0c\u5728\u4fdd\u6301\u56fe\u50cf\u7ec6\u8282\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u8bed\u4e49\u51c6\u786e\u6027\uff0c\u5e76\u4e14\u65e0\u9700\u8fdb\u884c\u6602\u8d35\u7684\u6bcf\u56fe\u50cf\u4f18\u5316\u3002"}}
{"id": "2509.18173", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18173", "abs": "https://arxiv.org/abs/2509.18173", "authors": ["Hongyi Luo", "Qing Cheng", "Daniel Matos", "Hari Krishna Gadi", "Yanfeng Zhang", "Lu Liu", "Yongliang Wang", "Niclas Zeller", "Daniel Cremers", "Liqiu Meng"], "title": "TurnBack: A Geospatial Route Cognition Benchmark for Large Language Models through Reverse Route", "comment": "Accepted to EMNLP 2025 (Main). This is the camera-ready/author\n  version", "summary": "Humans can interpret geospatial information through natural language, while\nthe geospatial cognition capabilities of Large Language Models (LLMs) remain\nunderexplored. Prior research in this domain has been constrained by\nnon-quantifiable metrics, limited evaluation datasets and unclear research\nhierarchies. Therefore, we propose a large-scale benchmark and conduct a\ncomprehensive evaluation of the geospatial route cognition of LLMs. We create a\nlarge-scale evaluation dataset comprised of 36000 routes from 12 metropolises\nworldwide. Then, we introduce PathBuilder, a novel tool for converting natural\nlanguage instructions into navigation routes, and vice versa, bridging the gap\nbetween geospatial information and natural language. Finally, we propose a new\nevaluation framework and metrics to rigorously assess 11 state-of-the-art\n(SOTA) LLMs on the task of route reversal. The benchmark reveals that LLMs\nexhibit limitation to reverse routes: most reverse routes neither return to the\nstarting point nor are similar to the optimal route. Additionally, LLMs face\nchallenges such as low robustness in route generation and high confidence for\ntheir incorrect answers. Code\\ \\&\\ Data available here:\n\\href{https://github.com/bghjmn32/EMNLP2025_Turnback}{TurnBack.}", "AI": {"tldr": "LLMs\u5728\u5730\u7406\u7a7a\u95f4\u8def\u7ebf\u7406\u89e3\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5c24\u5176\u662f\u5728\u8def\u7ebf\u53cd\u8f6c\u4efb\u52a1\u4e0a\uff0c\u5e76\u4e14\u5728\u751f\u6210\u6b63\u786e\u8def\u7ebf\u65b9\u9762\u9c81\u68d2\u6027\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5bf9LLM\u7684\u5730\u7406\u7a7a\u95f4\u8ba4\u77e5\u80fd\u529b\u63a2\u7d22\u4e0d\u8db3\uff0c\u53d7\u9650\u4e8e\u4e0d\u53ef\u91cf\u5316\u7684\u6307\u6807\u3001\u6709\u9650\u7684\u8bc4\u4f30\u6570\u636e\u96c6\u548c\u4e0d\u660e\u786e\u7684\u7814\u7a76\u5c42\u6b21\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b36000\u6761\u8def\u7ebf\u7684\u5927\u578b\u8bc4\u4f30\u6570\u636e\u96c6\uff0c\u5f00\u53d1\u4e86PathBuilder\u5de5\u5177\u7528\u4e8e\u81ea\u7136\u8bed\u8a00\u4e0e\u5bfc\u822a\u8def\u7ebf\u4e4b\u95f4\u7684\u8f6c\u6362\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u8bc4\u4f30\u6846\u67b6\u548c\u6307\u6807\u6765\u8bc4\u4f3011\u79cd\u6700\u5148\u8fdb\u7684LLM\u5728\u8def\u7ebf\u53cd\u8f6c\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "result": "LLMs\u5728\u8def\u7ebf\u53cd\u8f6c\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u5c40\u9650\u6027\uff0c\u5927\u591a\u6570\u53cd\u8f6c\u540e\u7684\u8def\u7ebf\u672a\u80fd\u56de\u5230\u8d77\u70b9\u6216\u4e0e\u6700\u4f18\u8def\u7ebf\u76f8\u4f3c\u3002\u6b64\u5916\uff0cLLMs\u5728\u8def\u7ebf\u751f\u6210\u65b9\u9762\u9c81\u68d2\u6027\u4f4e\uff0c\u5e76\u4e14\u5bf9\u5176\u9519\u8bef\u7b54\u6848\u7684\u7f6e\u4fe1\u5ea6\u9ad8\u3002", "conclusion": "LLMs\u5728\u5904\u7406\u5730\u7406\u7a7a\u95f4\u8def\u7ebf\u4fe1\u606f\uff0c\u7279\u522b\u662f\u8def\u7ebf\u53cd\u8f6c\u4efb\u52a1\u65f6\uff0c\u80fd\u529b\u6709\u9650\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u6539\u8fdb\u3002"}}
{"id": "2509.19143", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.19143", "abs": "https://arxiv.org/abs/2509.19143", "authors": ["Alejandro Cuevas", "Saloni Dash", "Bharat Kumar Nayak", "Dan Vann", "Madeleine I. G. Daepp"], "title": "Anecdoctoring: Automated Red-Teaming Across Language and Place", "comment": "To be published in EMNLP 2025", "summary": "Disinformation is among the top risks of generative artificial intelligence\n(AI) misuse. Global adoption of generative AI necessitates red-teaming\nevaluations (i.e., systematic adversarial probing) that are robust across\ndiverse languages and cultures, but red-teaming datasets are commonly US- and\nEnglish-centric. To address this gap, we propose \"anecdoctoring\", a novel\nred-teaming approach that automatically generates adversarial prompts across\nlanguages and cultures. We collect misinformation claims from fact-checking\nwebsites in three languages (English, Spanish, and Hindi) and two geographies\n(US and India). We then cluster individual claims into broader narratives and\ncharacterize the resulting clusters with knowledge graphs, with which we\naugment an attacker LLM. Our method produces higher attack success rates and\noffers interpretability benefits relative to few-shot prompting. Results\nunderscore the need for disinformation mitigations that scale globally and are\ngrounded in real-world adversarial misuse.", "AI": {"tldr": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a anecdoctoring \u7684\u65b0\u578b\u7ea2\u961f\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u53ef\u4ee5\u8de8\u8bed\u8a00\u548c\u6587\u5316\u81ea\u52a8\u751f\u6210\u5bf9\u6297\u6027\u63d0\u793a\uff0c\u4ee5\u5e94\u5bf9\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u7684\u865a\u5047\u4fe1\u606f\u98ce\u9669\u3002", "motivation": "\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u7684\u5168\u7403\u666e\u53ca\u9700\u8981\u7a33\u5065\u7684\u7ea2\u961f\u6d4b\u8bd5\u8bc4\u4f30\uff0c\u4f46\u73b0\u6709\u7684\u7ea2\u961f\u6d4b\u8bd5\u6570\u636e\u96c6\u4e3b\u8981\u96c6\u4e2d\u5728\u7f8e\u56fd\u548c\u82f1\u8bed\uff0c\u672a\u80fd\u6ee1\u8db3\u5168\u7403\u591a\u6837\u5316\u7684\u9700\u6c42\u3002", "method": "\u6536\u96c6\u4e86\u4e09\u79cd\u8bed\u8a00\uff08\u82f1\u8bed\u3001\u897f\u73ed\u7259\u8bed\u548c\u5370\u5730\u8bed\uff09\u548c\u4e24\u4e2a\u5730\u533a\uff08\u7f8e\u56fd\u548c\u5370\u5ea6\uff09\u7684\u865a\u5047\u4fe1\u606f\u58f0\u660e\uff0c\u5c06\u5b83\u4eec\u805a\u7c7b\u6210\u66f4\u5e7f\u6cdb\u7684\u53d9\u8ff0\uff0c\u5e76\u4f7f\u7528\u77e5\u8bc6\u56fe\u8c31\u6765\u8868\u5f81\u8fd9\u4e9b\u53d9\u8ff0\uff0c\u7136\u540e\u5c06\u8fd9\u4e9b\u77e5\u8bc6\u56fe\u8c31\u589e\u5f3a\u7ed9\u653b\u51fb\u6027\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u3002", "result": "\u4e0e\u5c11\u6837\u672c\u63d0\u793a\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5728\u653b\u51fb\u6210\u529f\u7387\u65b9\u9762\u8868\u73b0\u66f4\u4f18\uff0c\u5e76\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u6027\u4f18\u52bf\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u5e94\u5bf9\u5168\u7403\u8303\u56f4\u5185\u865a\u5047\u4fe1\u606f\u98ce\u9669\u7684\u5fc5\u8981\u6027\uff0c\u9700\u8981\u80fd\u591f\u5927\u89c4\u6a21\u5e94\u5bf9\u5e76\u57fa\u4e8e\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u6076\u610f\u4f7f\u7528\u60c5\u51b5\u7684\u7f13\u89e3\u63aa\u65bd\u3002"}}
{"id": "2509.19168", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.19168", "abs": "https://arxiv.org/abs/2509.19168", "authors": ["Mark Gonzales", "Ethan Oh", "Joseph Moore"], "title": "A Multimodal Stochastic Planning Approach for Navigation and Multi-Robot Coordination", "comment": "8 Pages, 7 Figures", "summary": "In this paper, we present a receding-horizon, sampling-based planner capable\nof reasoning over multimodal policy distributions. By using the cross-entropy\nmethod to optimize a multimodal policy under a common cost function, our\napproach increases robustness against local minima and promotes effective\nexploration of the solution space. We show that our approach naturally extends\nto multi-robot collision-free planning, enables agents to share diverse\ncandidate policies to avoid deadlocks, and allows teams to minimize a global\nobjective without incurring the computational complexity of centralized\noptimization. Numerical simulations demonstrate that employing multiple modes\nsignificantly improves success rates in trap environments and in multi-robot\ncollision avoidance. Hardware experiments further validate the approach's\nreal-time feasibility and practical performance.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u4ea4\u53c9\u71b5\u7684\u591a\u6a21\u6001\u7b56\u7565\u89c4\u5212\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u907f\u969c\u548c\u9677\u9631\u73af\u5883\u4e0b\u7684\u89c4\u5212\u95ee\u9898\u3002", "motivation": "\u5728\u591a\u667a\u80fd\u4f53\u89c4\u5212\u4e2d\uff0c\u5c24\u5176\u662f\u5728\u5b58\u5728\u5c40\u90e8\u6700\u4f18\u89e3\u548c\u9700\u8981\u6709\u6548\u63a2\u7d22\u89e3\u7a7a\u95f4\u7684\u60c5\u51b5\u4e0b\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5904\u7406\u591a\u6a21\u6001\u7b56\u7565\u5206\u5e03\u7684\u65b9\u6cd5\u6765\u63d0\u9ad8\u9c81\u68d2\u6027\u3002", "method": "\u4f7f\u7528\u4ea4\u53c9\u71b5\u65b9\u6cd5\u4f18\u5316\u591a\u6a21\u6001\u7b56\u7565\u5206\u5e03\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u591a\u667a\u80fd\u4f53\u65e0\u78b0\u649e\u89c4\u5212\uff0c\u5141\u8bb8\u667a\u80fd\u4f53\u5171\u4eab\u591a\u6837\u5316\u7684\u5019\u9009\u7b56\u7565\u4ee5\u907f\u514d\u6b7b\u9501\uff0c\u5e76\u6700\u5c0f\u5316\u5168\u5c40\u76ee\u6807\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u9677\u9631\u73af\u5883\u548c\u591a\u667a\u80fd\u4f53\u907f\u969c\u7684\u6570\u503c\u6a21\u62df\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u6210\u529f\u7387\u3002\u786c\u4ef6\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u5b9e\u65f6\u53ef\u884c\u6027\u548c\u5b9e\u9645\u6027\u80fd\u3002", "conclusion": "\u57fa\u4e8e\u4ea4\u53c9\u71b5\u7684\u591a\u6a21\u6001\u7b56\u7565\u89c4\u5212\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u63d0\u9ad8\u591a\u667a\u80fd\u4f53\u89c4\u5212\u7684\u6210\u529f\u7387\u548c\u9c81\u68d2\u6027\uff0c\u5e76\u80fd\u5728\u4fdd\u6301\u8f83\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u540c\u65f6\u5b9e\u73b0\u5206\u5e03\u5f0f\u4f18\u5316\u3002"}}
{"id": "2509.18638", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18638", "abs": "https://arxiv.org/abs/2509.18638", "authors": ["Yiwei Lyu", "Samir Harake", "Asadur Chowdury", "Soumyanil Banerjee", "Rachel Gologorsky", "Shixuan Liu", "Anna-Katharina Meissner", "Akshay Rao", "Chenhui Zhao", "Akhil Kondepudi", "Cheng Jiang", "Xinhai Hou", "Rushikesh S. Joshi", "Volker Neuschmelting", "Ashok Srinivasan", "Dawn Kleindorfer", "Brian Athey", "Vikas Gulani", "Aditya Pandey", "Honglak Lee", "Todd Hollon"], "title": "Learning neuroimaging models from health system-scale data", "comment": null, "summary": "Neuroimaging is a ubiquitous tool for evaluating patients with neurological\ndiseases. The global demand for magnetic resonance imaging (MRI) studies has\nrisen steadily, placing significant strain on health systems, prolonging\nturnaround times, and intensifying physician burnout \\cite{Chen2017-bt,\nRula2024-qp-1}. These challenges disproportionately impact patients in\nlow-resource and rural settings. Here, we utilized a large academic health\nsystem as a data engine to develop Prima, the first vision language model (VLM)\nserving as an AI foundation for neuroimaging that supports real-world, clinical\nMRI studies as input. Trained on over 220,000 MRI studies, Prima uses a\nhierarchical vision architecture that provides general and transferable MRI\nfeatures. Prima was tested in a 1-year health system-wide study that included\n30K MRI studies. Across 52 radiologic diagnoses from the major neurologic\ndisorders, including neoplastic, inflammatory, infectious, and developmental\nlesions, Prima achieved a mean diagnostic area under the ROC curve of 92.0,\noutperforming other state-of-the-art general and medical AI models. Prima\noffers explainable differential diagnoses, worklist priority for radiologists,\nand clinical referral recommendations across diverse patient demographics and\nMRI systems. Prima demonstrates algorithmic fairness across sensitive groups\nand can help mitigate health system biases, such as prolonged turnaround times\nfor low-resource populations. These findings highlight the transformative\npotential of health system-scale VLMs and Prima's role in advancing AI-driven\nhealthcare.", "AI": {"tldr": "Prima\u662f\u9996\u4e2a\u7528\u4e8e\u795e\u7ecf\u5f71\u50cf\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\uff0c\u5728\u8d85\u8fc722\u4e07\u4efdMRI\u7814\u7a76\u7684\u8bad\u7ec3\u4e0b\uff0c\u5e73\u5747\u8bca\u65adROC\u66f2\u7ebf\u4e0b\u9762\u79ef\u8fbe\u523092.0\uff0c\u4f18\u4e8e\u5176\u4ed6\u6700\u5148\u8fdb\u7684AI\u6a21\u578b\uff0c\u5e76\u80fd\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u9274\u522b\u8bca\u65ad\u3001\u5de5\u4f5c\u5217\u8868\u4f18\u5148\u7ea7\u548c\u4e34\u5e8a\u8f6c\u8bca\u5efa\u8bae\uff0c\u540c\u65f6\u5c55\u73b0\u51fa\u7b97\u6cd5\u516c\u5e73\u6027\uff0c\u6709\u671b\u7f13\u89e3\u533b\u7597\u7cfb\u7edf\u4e2d\u7684\u6548\u7387\u4f4e\u4e0b\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u4f4e\u8d44\u6e90\u4eba\u7fa4\u4e2d\u3002", "motivation": "\u5168\u7403\u5bf9MRI\u7814\u7a76\u7684\u9700\u6c42\u4e0d\u65ad\u589e\u957f\uff0c\u7ed9\u533b\u7597\u7cfb\u7edf\u5e26\u6765\u5de8\u5927\u538b\u529b\uff0c\u5ef6\u957f\u4e86\u5468\u8f6c\u65f6\u95f4\uff0c\u52a0\u5267\u4e86\u533b\u751f\u7684\u804c\u4e1a\u5026\u6020\uff0c\u5c24\u5176\u5f71\u54cd\u4f4e\u8d44\u6e90\u548c\u519c\u6751\u5730\u533a\u7684\u60a3\u8005\u3002", "method": "\u5229\u7528\u4e00\u4e2a\u5927\u578b\u5b66\u672f\u533b\u7597\u7cfb\u7edf\u4f5c\u4e3a\u6570\u636e\u5f15\u64ce\uff0c\u5f00\u53d1\u4e86Prima\uff0c\u4e00\u4e2a\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684AI\u57fa\u7840\u6a21\u578b\uff0c\u7528\u4e8e\u4e34\u5e8aMRI\u7814\u7a76\uff0c\u8be5\u6a21\u578b\u5728\u4e00\u4e2a\u5305\u542b30,000\u4efdMRI\u7814\u7a76\u7684\u4e3a\u671f\u4e00\u5e74\u7684\u5065\u5eb7\u7cfb\u7edf\u8303\u56f4\u5185\u7814\u7a76\u4e2d\u8fdb\u884c\u4e86\u6d4b\u8bd5\u3002", "result": "Prima\u572852\u79cd\u653e\u5c04\u5b66\u8bca\u65ad\u4e2d\u53d6\u5f97\u4e8692.0\u7684\u5e73\u5747\u8bca\u65adROC\u66f2\u7ebf\u4e0b\u9762\u79ef\uff0c\u4f18\u4e8e\u5176\u4ed6\u6700\u5148\u8fdb\u7684\u901a\u7528\u548c\u533b\u5b66AI\u6a21\u578b\uff0c\u5e76\u80fd\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u9274\u522b\u8bca\u65ad\u3001\u5de5\u4f5c\u5217\u8868\u4f18\u5148\u7ea7\u548c\u4e34\u5e8a\u8f6c\u8bca\u5efa\u8bae\uff0c\u540c\u65f6\u5728\u4e0d\u540c\u4eba\u7fa4\u548cMRI\u7cfb\u7edf\u4e4b\u95f4\u5c55\u73b0\u51fa\u7b97\u6cd5\u516c\u5e73\u6027\u3002", "conclusion": "Prima\u5728\u533b\u7597\u7cfb\u7edf\u89c4\u6a21\u7684VLM\u65b9\u9762\u5c55\u73b0\u4e86\u53d8\u9769\u6f5c\u529b\uff0c\u5e76\u5728\u63a8\u8fdbAI\u9a71\u52a8\u7684\u533b\u7597\u4fdd\u5065\u65b9\u9762\u53d1\u6325\u7740\u5173\u952e\u4f5c\u7528\uff0c\u6709\u671b\u901a\u8fc7\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u8bca\u65ad\u3001\u5de5\u4f5c\u5217\u8868\u4f18\u5148\u7ea7\u548c\u4e34\u5e8a\u8f6c\u8bca\u5efa\u8bae\uff0c\u4ee5\u53ca\u901a\u8fc7\u7b97\u6cd5\u516c\u5e73\u6027\u6765\u7f13\u89e3\u533b\u7597\u7cfb\u7edf\u4e2d\u7684\u6548\u7387\u4f4e\u4e0b\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u4f4e\u8d44\u6e90\u4eba\u7fa4\u4e2d\u3002"}}
{"id": "2509.18200", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18200", "abs": "https://arxiv.org/abs/2509.18200", "authors": ["Yu Ti Huang"], "title": "Conversational Orientation Reasoning: Egocentric-to-Allocentric Navigation with Multimodal Chain-of-Thought", "comment": null, "summary": "Conversational agents must translate egocentric utterances (e.g., \"on my\nright\") into allocentric orientations (N/E/S/W). This challenge is particularly\ncritical in indoor or complex facilities where GPS signals are weak and\ndetailed maps are unavailable. While chain-of-thought (CoT) prompting has\nadvanced reasoning in language and vision tasks, its application to multimodal\nspatial orientation remains underexplored. We introduce Conversational\nOrientation Reasoning (COR), a new benchmark designed for Traditional Chinese\nconversational navigation projected from real-world environments, addressing\negocentric-to-allocentric reasoning in non-English and ASR-transcribed\nscenarios. We propose a multimodal chain-of-thought (MCoT) framework, which\nintegrates ASR-transcribed speech with landmark coordinates through a\nstructured three-step reasoning process: (1) extracting spatial relations, (2)\nmapping coordinates to absolute directions, and (3) inferring user orientation.\nA curriculum learning strategy progressively builds these capabilities on\nTaiwan-LLM-13B-v2.0-Chat, a mid-sized model representative of\nresource-constrained settings. Experiments show that MCoT achieves 100%\norientation accuracy on clean transcripts and 98.1% with ASR transcripts,\nsubstantially outperforming unimodal and non-structured baselines. Moreover,\nMCoT demonstrates robustness under noisy conversational conditions, including\nASR recognition errors and multilingual code-switching. The model also\nmaintains high accuracy in cross-domain evaluation and resilience to linguistic\nvariation, domain shift, and referential ambiguity. These findings highlight\nthe potential of structured MCoT spatial reasoning as a path toward\ninterpretable and resource-efficient embodied navigation.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCOR\u7684\u65b0\u57fa\u51c6\u548cMCoT\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u4e2d\u6587\u5bf9\u8bdd\u5bfc\u822a\u4e2d\u7684\u5355\u89d2\u5ea6\u5230\u7edd\u5bf9\u89d2\u5ea6\u7a7a\u95f4\u63a8\u7406\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728GPS\u4fe1\u53f7\u5f31\u548c\u7f3a\u4e4f\u8be6\u7ec6\u5730\u56fe\u7684\u5ba4\u5185\u73af\u5883\u4e2d\u3002", "motivation": "\u5728\u5ba4\u5185\u6216\u590d\u6742\u8bbe\u65bd\u4e2d\uff0c\u5bf9\u8bdd\u4ee3\u7406\u9700\u8981\u5c06\u5355\u89d2\u5ea6\u8868\u8ff0\uff08\u4f8b\u5982\u201c\u5728\u6211\u53f3\u8fb9\u201d\uff09\u8f6c\u6362\u4e3a\u7edd\u5bf9\u65b9\u5411\uff08\u4e1c/\u5357/\u897f/\u5317\uff09\u3002\u7136\u800c\uff0c\u5728GPS\u4fe1\u53f7\u5f31\u548c\u7f3a\u4e4f\u8be6\u7ec6\u5730\u56fe\u7684\u73af\u5883\u4e2d\uff0c\u8fd9\u79cd\u5355\u89d2\u5ea6\u5230\u7edd\u5bf9\u89d2\u5ea6\u7684\u7a7a\u95f4\u63a8\u7406\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002\u867d\u7136\u601d\u7ef4\u94fe\uff08CoT\uff09\u63d0\u793a\u5728\u8bed\u8a00\u548c\u89c6\u89c9\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u5176\u5728\u591a\u6a21\u6001\u7a7a\u95f4\u5b9a\u5411\u65b9\u9762\u7684\u5e94\u7528\u4ecd\u6709\u5f85\u63a2\u7d22\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCOR\uff08Conversational Orientation Reasoning\uff09\u7684\u65b0\u57fa\u51c6\uff0c\u7528\u4e8e\u5904\u7406\u4f20\u7edf\u4e2d\u6587\u5bf9\u8bdd\u5bfc\u822a\u7684\u5355\u89d2\u5ea6\u5230\u7edd\u5bf9\u89d2\u5ea6\u63a8\u7406\u95ee\u9898\u3002\u540c\u65f6\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u601d\u7ef4\u94fe\uff08MCoT\uff09\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u4e00\u4e2a\u7ed3\u6784\u5316\u7684\u4e09\u6b65\u63a8\u7406\u8fc7\u7a0b\u6574\u5408\u4e86\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u8f6c\u5f55\u7684\u8bed\u97f3\u548c\u5730\u6807\u5750\u6807\uff1a1. \u63d0\u53d6\u7a7a\u95f4\u5173\u7cfb\uff1b2. \u5c06\u5750\u6807\u6620\u5c04\u5230\u7edd\u5bf9\u65b9\u5411\uff1b3. \u63a8\u65ad\u7528\u6237\u65b9\u5411\u3002\u5e76\u91c7\u7528\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\u5728Taiwan-LLM-13B-v2.0-Chat\u6a21\u578b\u4e0a\u9010\u6b65\u6784\u5efa\u8fd9\u4e9b\u80fd\u529b\u3002", "result": "MCoT\u5728\u5e72\u51c0\u8f6c\u5f55\u4e0a\u5b9e\u73b0\u4e86100%\u7684\u5b9a\u5411\u51c6\u786e\u7387\uff0c\u5728ASR\u8f6c\u5f55\u4e0a\u8fbe\u5230\u4e8698.1%\uff0c\u663e\u8457\u4f18\u4e8e\u5355\u6a21\u6001\u548c\u975e\u7ed3\u6784\u5316\u57fa\u7ebf\u3002MCoT\u5728\u5608\u6742\u7684\u5bf9\u8bdd\u6761\u4ef6\u4e0b\uff08\u5305\u62ecASR\u8bc6\u522b\u9519\u8bef\u548c\u591a\u8bed\u8a00\u6df7\u5408\u5207\u6362\uff09\u8868\u73b0\u51fa\u9c81\u68d2\u6027\uff0c\u5e76\u5728\u8de8\u9886\u57df\u8bc4\u4f30\u4e2d\u4fdd\u6301\u4e86\u9ad8\u51c6\u786e\u7387\uff0c\u5bf9\u8bed\u8a00\u53d8\u5316\u3001\u9886\u57df\u8f6c\u79fb\u548c\u6307\u4ee3\u6b67\u4e49\u5177\u6709\u5f39\u6027\u3002", "conclusion": "\u7ed3\u6784\u5316\u7684\u591a\u6a21\u6001\u601d\u7ef4\u94fe\uff08MCoT\uff09\u5728\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4e3a\u5b9e\u73b0\u53ef\u89e3\u91ca\u4e14\u8d44\u6e90\u9ad8\u6548\u7684\u5177\u8eab\u5bfc\u822a\u63d0\u4f9b\u4e86\u9014\u5f84\u3002MCoT\u5728\u5904\u7406\u975e\u82f1\u8bed\u3001ASR\u8f6c\u5f55\u4ee5\u53ca\u8d44\u6e90\u53d7\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u80fd\u591f\u6709\u6548\u5730\u8fdb\u884c\u5355\u89d2\u5ea6\u5230\u7edd\u5bf9\u89d2\u5ea6\u7684\u7a7a\u95f4\u63a8\u7406\u3002"}}
{"id": "2509.19163", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.19163", "abs": "https://arxiv.org/abs/2509.19163", "authors": ["Chantal Shaib", "Tuhin Chakrabarty", "Diego Garcia-Olano", "Byron C. Wallace"], "title": "Measuring AI \"Slop\" in Text", "comment": null, "summary": "AI \"slop\" is an increasingly popular term used to describe low-quality\nAI-generated text, but there is currently no agreed upon definition of this\nterm nor a means to measure its occurrence. In this work, we develop a taxonomy\nof \"slop\" through interviews with experts in NLP, writing, and philosophy, and\npropose a set of interpretable dimensions for its assessment in text. Through\nspan-level annotation, we find that binary \"slop\" judgments are (somewhat)\nsubjective, but such determinations nonetheless correlate with latent\ndimensions such as coherence and relevance. Our framework can be used to\nevaluate AI-generated text in both detection and binary preference tasks,\npotentially offering new insights into the linguistic and stylistic factors\nthat contribute to quality judgments.", "AI": {"tldr": "AI\u751f\u6210\u5185\u5bb9\u7684\u201cslop\u201d\u7f3a\u4e4f\u660e\u786e\u5b9a\u4e49\u548c\u8861\u91cf\u6807\u51c6\uff0c\u672c\u7814\u7a76\u901a\u8fc7\u4e13\u5bb6\u8bbf\u8c08\u6784\u5efa\u4e86\u201cslop\u201d\u5206\u7c7b\u4f53\u7cfb\uff0c\u5e76\u63d0\u51fa\u53ef\u89e3\u91ca\u7684\u8bc4\u4f30\u7ef4\u5ea6\uff0c\u53d1\u73b0\u201cslop\u201d\u5224\u65ad\u5b58\u5728\u4e3b\u89c2\u6027\u4f46\u4e0e\u6f5c\u5728\u7ef4\u5ea6\u76f8\u5173\uff0c\u8be5\u6846\u67b6\u53ef\u7528\u4e8e\u8bc4\u4f30AI\u751f\u6210\u6587\u672c\u7684\u68c0\u6d4b\u548c\u504f\u597d\u4efb\u52a1\u3002", "motivation": "\u76ee\u524d\u7f3a\u4e4f\u5bf9AI\u751f\u6210\u201cslop\u201d\uff08\u4f4e\u8d28\u91cfAI\u6587\u672c\uff09\u7684\u660e\u786e\u5b9a\u4e49\u548c\u8861\u91cf\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u5bf9NLP\u3001\u5199\u4f5c\u548c\u54f2\u5b66\u4e13\u5bb6\u7684\u8bbf\u8c08\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u201cslop\u201d\u7684\u5206\u7c7b\u4f53\u7cfb\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u5957\u53ef\u89e3\u91ca\u7684\u7ef4\u5ea6\u6765\u8bc4\u4f30\u6587\u672c\u4e2d\u7684\u201cslop\u201d\u3002\u901a\u8fc7\u8de8\u5ea6\u7ea7\u522b\u7684\u6807\u6ce8\uff0c\u5206\u6790\u201cslop\u201d\u5224\u65ad\u7684\u4e3b\u89c2\u6027\u53ca\u5176\u4e0e\u6f5c\u5728\u7ef4\u5ea6\uff08\u5982\u8fde\u8d2f\u6027\u548c\u76f8\u5173\u6027\uff09\u7684\u5173\u7cfb\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u201cslop\u201d\u7684\u4e8c\u5143\u5224\u65ad\u5177\u6709\u4e00\u5b9a\u7684\u4e3b\u89c2\u6027\uff0c\u4f46\u8fd9\u79cd\u5224\u65ad\u4e0e\u8fde\u8d2f\u6027\u548c\u76f8\u5173\u6027\u7b49\u6f5c\u5728\u7ef4\u5ea6\u76f8\u5173\u3002\u6240\u63d0\u51fa\u7684\u6846\u67b6\u53ef\u7528\u4e8eAI\u751f\u6210\u6587\u672c\u7684\u68c0\u6d4b\u548c\u4e8c\u5143\u504f\u597d\u4efb\u52a1\u7684\u8bc4\u4f30\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u201cslop\u201d\u8bc4\u4f30\u6846\u67b6\u6709\u52a9\u4e8e\u91cf\u5316AI\u751f\u6210\u6587\u672c\u7684\u8d28\u91cf\uff0c\u5e76\u53ef\u80fd\u63ed\u793a\u5f71\u54cd\u8d28\u91cf\u5224\u65ad\u7684\u8bed\u8a00\u548c\u98ce\u683c\u56e0\u7d20\u3002"}}
{"id": "2509.19169", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.19169", "abs": "https://arxiv.org/abs/2509.19169", "authors": ["Tianyu Wu", "Xudong Han", "Haoran Sun", "Zishang Zhang", "Bangchao Huang", "Chaoyang Song", "Fang Wan"], "title": "MagiClaw: A Dual-Use, Vision-Based Soft Gripper for Bridging the Human Demonstration to Robotic Deployment Gap", "comment": "8 pages, 4 figures, accepted to Data@CoRL2025 Workshop", "summary": "The transfer of manipulation skills from human demonstration to robotic\nexecution is often hindered by a \"domain gap\" in sensing and morphology. This\npaper introduces MagiClaw, a versatile two-finger end-effector designed to\nbridge this gap. MagiClaw functions interchangeably as both a handheld tool for\nintuitive data collection and a robotic end-effector for policy deployment,\nensuring hardware consistency and reliability. Each finger incorporates a Soft\nPolyhedral Network (SPN) with an embedded camera, enabling vision-based\nestimation of 6-DoF forces and contact deformation. This proprioceptive data is\nfused with exteroceptive environmental sensing from an integrated iPhone, which\nprovides 6D pose, RGB video, and LiDAR-based depth maps. Through a custom iOS\napplication, MagiClaw streams synchronized, multi-modal data for real-time\nteleoperation, offline policy learning, and immersive control via mixed-reality\ninterfaces. We demonstrate how this unified system architecture lowers the\nbarrier to collecting high-fidelity, contact-rich datasets and accelerates the\ndevelopment of generalizable manipulation policies. Please refer to the iOS app\nat https://apps.apple.com/cn/app/magiclaw/id6661033548 for further details.", "AI": {"tldr": "MagiClaw\u662f\u4e00\u4e2a\u901a\u7528\u7684\u4e24\u6307\u672b\u7aef\u6267\u884c\u5668\uff0c\u5b83\u5145\u5f53\u4e86\u624b\u5de5\u5de5\u5177\u548c\u673a\u5668\u4eba\u672b\u7aef\u6267\u884c\u5668\u7684\u89d2\u8272\uff0c\u4ee5\u5f25\u5408\u64cd\u4f5c\u6280\u80fd\u8f6c\u79fb\u4e2d\u7684\u201c\u57df\u95f4\u9699\u201d\u3002\u5b83\u901a\u8fc7\u96c6\u6210\u7684SPN\uff08\u5e26\u6444\u50cf\u5934\uff09\u548ciPhone\uff08\u63d0\u4f9b\u59ff\u6001\u3001RGB\u89c6\u9891\u548c\u6df1\u5ea6\u56fe\uff09\u6765\u6536\u96c6\u591a\u6a21\u6001\u6570\u636e\uff0c\u4ee5\u5b9e\u73b0\u673a\u5668\u4eba\u64cd\u4f5c\u3002", "motivation": "\u89e3\u51b3\u5728\u4eba\u7c7b\u6f14\u793a\u5230\u673a\u5668\u4eba\u6267\u884c\u7684\u64cd\u4f5c\u6280\u80fd\u8f6c\u79fb\u4e2d\uff0c\u201c\u57df\u95f4\u9699\u201d\u5e26\u6765\u7684\u6311\u6218\uff0c\u4ee5\u5b9e\u73b0\u66f4\u901a\u7528\u7684\u64cd\u4f5c\u7b56\u7565\u3002", "method": "MagiClaw\u5229\u7528\u96c6\u6210\u7684SPN\uff08\u5e26\u6444\u50cf\u5934\uff09\u548ciPhone\uff08\u63d0\u4f9b6D\u59ff\u6001\u3001RGB\u89c6\u9891\u548c\u6df1\u5ea6\u56fe\uff09\u6765\u6536\u96c6\u591a\u6a21\u6001\u6570\u636e\u3002\u6570\u636e\u901a\u8fc7\u81ea\u5b9a\u4e49iOS\u5e94\u7528\u7a0b\u5e8f\u8fdb\u884c\u540c\u6b65\u548c\u6d41\u5f0f\u4f20\u8f93\uff0c\u7528\u4e8e\u5b9e\u65f6\u9065\u64cd\u4f5c\u3001\u79bb\u7ebf\u7b56\u7565\u5b66\u4e60\u548c\u6df7\u5408\u73b0\u5b9e\u63a7\u5236\u3002", "result": "MagiClaw\u80fd\u591f\u4f5c\u4e3a\u624b\u5de5\u5de5\u5177\u548c\u673a\u5668\u4eba\u672b\u7aef\u6267\u884c\u5668\uff0c\u6536\u96c6\u9ad8\u4fdd\u771f\u3001\u63a5\u89e6\u4e30\u5bcc\u7684\u64cd\u4f5c\u6570\u636e\u96c6\uff0c\u4ece\u800c\u52a0\u901f\u901a\u7528\u64cd\u4f5c\u7b56\u7565\u7684\u5f00\u53d1\u3002", "conclusion": "MagiClaw\u901a\u8fc7\u7edf\u4e00\u7684\u7cfb\u7edf\u67b6\u6784\uff0c\u964d\u4f4e\u4e86\u6536\u96c6\u6570\u636e\u96c6\u7684\u95e8\u69db\uff0c\u5e76\u52a0\u901f\u4e86\u901a\u7528\u64cd\u4f5c\u7b56\u7565\u7684\u5f00\u53d1\u3002"}}
{"id": "2509.18639", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18639", "abs": "https://arxiv.org/abs/2509.18639", "authors": ["Yuanhuiyi Lyu", "Chi Kit Wong", "Chenfei Liao", "Lutao Jiang", "Xu Zheng", "Zexin Lu", "Linfeng Zhang", "Xuming Hu"], "title": "Understanding-in-Generation: Reinforcing Generative Capability of Unified Model via Infusing Understanding into Generation", "comment": null, "summary": "Recent works have made notable advancements in enhancing unified models for\ntext-to-image generation through the Chain-of-Thought (CoT). However, these\nreasoning methods separate the processes of understanding and generation, which\nlimits their ability to guide the reasoning of unified models in addressing the\ndeficiencies of their generative capabilities. To this end, we propose a novel\nreasoning framework for unified models, Understanding-in-Generation (UiG),\nwhich harnesses the robust understanding capabilities of unified models to\nreinforce their performance in image generation. The core insight of our UiG is\nto integrate generative guidance by the strong understanding capabilities\nduring the reasoning process, thereby mitigating the limitations of generative\nabilities. To achieve this, we introduce \"Image Editing\" as a bridge to infuse\nunderstanding into the generation process. Initially, we verify the generated\nimage and incorporate the understanding of unified models into the editing\ninstructions. Subsequently, we enhance the generated image step by step,\ngradually infusing the understanding into the generation process. Our UiG\nframework demonstrates a significant performance improvement in text-to-image\ngeneration over existing text-to-image reasoning methods, e.g., a 3.92% gain on\nthe long prompt setting of the TIIF benchmark. The project code:\nhttps://github.com/QC-LY/UiG", "AI": {"tldr": "UiG\u6846\u67b6\u901a\u8fc7\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u6574\u5408\u5f3a\u5927\u7684\u7406\u89e3\u80fd\u529b\u6765\u589e\u5f3a\u7edf\u4e00\u6a21\u578b\u8fdb\u884c\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u7684\u80fd\u529b\uff0c\u5e76\u5728TIIF\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u7684CoT\u65b9\u6cd5\u5c06\u7406\u89e3\u548c\u751f\u6210\u8fc7\u7a0b\u5206\u5f00\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u5728\u5f25\u8865\u751f\u6210\u80fd\u529b\u65b9\u9762\u7684\u4e0d\u8db3\u3002UiG\u6846\u67b6\u65e8\u5728\u901a\u8fc7\u5229\u7528\u7edf\u4e00\u6a21\u578b\u7684\u5f3a\u5927\u7406\u89e3\u80fd\u529b\u6765\u589e\u5f3a\u5176\u56fe\u50cf\u751f\u6210\u6027\u80fd\u3002", "method": "UiG\u6846\u67b6\u901a\u8fc7\u201c\u56fe\u50cf\u7f16\u8f91\u201d\u4f5c\u4e3a\u6865\u6881\uff0c\u5c06\u7406\u89e3\u80fd\u529b\u878d\u5165\u751f\u6210\u8fc7\u7a0b\u3002\u9996\u5148\uff0c\u9a8c\u8bc1\u751f\u6210\u7684\u56fe\u50cf\uff0c\u5e76\u5c06\u7edf\u4e00\u6a21\u578b\u7684\u7406\u89e3\u878d\u5165\u7f16\u8f91\u6307\u4ee4\uff0c\u7136\u540e\u9010\u6b65\u589e\u5f3a\u56fe\u50cf\u3002", "result": "UiG\u6846\u67b6\u5728\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u6587\u672c\u5230\u56fe\u50cf\u63a8\u7406\u65b9\u6cd5\uff0c\u4f8b\u5982\uff0c\u5728TIIF\u57fa\u51c6\u7684\u957f\u63d0\u793a\u8bbe\u7f6e\u4e0a\u63d0\u9ad8\u4e863.92%\u3002", "conclusion": "UiG\u6846\u67b6\u901a\u8fc7\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u6574\u5408\u7406\u89e3\u80fd\u529b\uff0c\u6709\u6548\u63d0\u5347\u4e86\u7edf\u4e00\u6a21\u578b\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6027\u80fd\u3002"}}
{"id": "2509.18208", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18208", "abs": "https://arxiv.org/abs/2509.18208", "authors": ["Boyuan Zhang", "Yingjun Du", "Xiantong Zhen", "Ling Shao"], "title": "Variational Task Vector Composition", "comment": null, "summary": "Task vectors capture how a model changes during fine-tuning by recording the\ndifference between pre-trained and task-specific weights. The composition of\ntask vectors, a key operator in task arithmetic, enables models to integrate\nknowledge from multiple tasks without incurring additional inference costs. In\nthis paper, we propose variational task vector composition, where composition\ncoefficients are taken as latent variables and estimated in a Bayesian\ninference framework. Unlike previous methods that operate at the task level,\nour framework focuses on sample-specific composition. Motivated by the\nobservation of structural redundancy in task vectors, we introduce a\nSpike-and-Slab prior that promotes sparsity and preserves only the most\ninformative components. To further address the high variance and sampling\ninefficiency in sparse, high-dimensional spaces, we develop a gated sampling\nmechanism that constructs a controllable posterior by filtering the composition\ncoefficients based on both uncertainty and importance. This yields a more\nstable and interpretable variational framework by deterministically selecting\nreliable task components, reducing sampling variance while improving\ntransparency and generalization. Experimental results demonstrate that our\nmethod consistently outperforms existing approaches across all datasets by\nselectively leveraging the most reliable and informative components in task\nvectors. These findings highlight the practical value of our approach,\nestablishing a new standard for efficient and effective task vector\ncomposition.", "AI": {"tldr": "\u4efb\u52a1\u5411\u91cf\u7684\u8d1d\u53f6\u65af\u63a8\u65ad\u548c\u7a00\u758f\u5316\u65b9\u6cd5\u3002", "motivation": "\u5df2\u6709\u4efb\u52a1\u5411\u91cf\u7ec4\u5408\u65b9\u6cd5\u5728\u5904\u7406\u591a\u4efb\u52a1\u65f6\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u66f4\u6709\u6548\u3001\u53ef\u89e3\u91ca\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u53d8\u5206\u4efb\u52a1\u5411\u91cf\u7ec4\u5408\uff0c\u5c06\u7ec4\u5408\u7cfb\u6570\u4f5c\u4e3a\u6f5c\u5728\u53d8\u91cf\uff0c\u5e76\u4f7f\u7528Spike-and-Slab\u5148\u9a8c\u5b9e\u73b0\u7a00\u758f\u5316\uff0c\u540c\u65f6\u5f15\u5165\u95e8\u63a7\u91c7\u6837\u673a\u5236\u63d0\u9ad8\u7a33\u5b9a\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u5728\u6240\u6709\u6570\u636e\u96c6\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u5229\u7528\u4efb\u52a1\u5411\u91cf\u4e2d\u6700\u53ef\u9760\u3001\u6700\u5177\u4fe1\u606f\u91cf\u7684\u6210\u5206\u3002", "conclusion": "\u63d0\u51fa\u7684\u53d8\u5206\u4efb\u52a1\u5411\u91cf\u7ec4\u5408\u65b9\u6cd5\u5728\u6548\u7387\u548c\u6548\u679c\u4e0a\u5747\u80fd\u6ee1\u8db3\u5b9e\u9645\u9700\u6c42\uff0c\u5e76\u4e3a\u4efb\u52a1\u5411\u91cf\u7ec4\u5408\u8bbe\u5b9a\u4e86\u65b0\u7684\u6807\u51c6\u3002"}}
{"id": "2509.19170", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19170", "abs": "https://arxiv.org/abs/2509.19170", "authors": ["Natasha Butt", "Ariel Kwiatkowski", "Ismail Labiad", "Julia Kempe", "Yann Ollivier"], "title": "Soft Tokens, Hard Truths", "comment": null, "summary": "The use of continuous instead of discrete tokens during the Chain-of-Thought\n(CoT) phase of reasoning LLMs has garnered attention recently, based on the\nintuition that a continuous mixture of discrete tokens could simulate a\nsuperposition of several reasoning paths simultaneously. Theoretical results\nhave formally proven that continuous tokens have much greater expressivity and\ncan solve specific problems more efficiently. However, practical use of\ncontinuous tokens has been limited by strong training difficulties: previous\nworks either just use continuous tokens at inference time on a pre-trained\ndiscrete-token model, or must distill the continuous CoT from ground-truth\ndiscrete CoTs and face computational costs that limit the CoT to very few\ntokens.\n  This is the first work introducing a scalable method to learn continuous CoTs\nvia reinforcement learning (RL), without distilling from reference discrete\nCoTs. We use \"soft\" tokens: mixtures of tokens together with noise on the input\nembedding to provide RL exploration. Computational overhead is minimal,\nenabling us to learn continuous CoTs with hundreds of tokens. On math reasoning\nbenchmarks with Llama and Qwen models up to 8B, training with continuous CoTs\nmatch discrete-token CoTs for pass@1 and surpass them for pass@32, showing\ngreater CoT diversity. In systematic comparisons, the best-performing scenario\nis to train with continuous CoT tokens then use discrete tokens for inference,\nmeaning the \"soft\" models can be deployed in a standard way. Finally, we show\ncontinuous CoT RL training better preserves the predictions of the base model\non out-of-domain tasks, thus providing a softer touch to the base model.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u6765\u5b66\u4e60\u8fde\u7eed\u601d\u7ef4\u94fe\uff08CoT\uff09\uff0c\u89e3\u51b3\u4e86\u5148\u524d\u8fde\u7eed\u4ee4\u724c\u8bad\u7ec3\u56f0\u96be\u7684\u95ee\u9898\uff0c\u5e76\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u7ed3\u679c\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86 CoT \u7684\u591a\u6837\u6027\u5e76\u66f4\u597d\u5730\u4fdd\u7559\u4e86\u57fa\u7840\u6a21\u578b\u7684\u9884\u6d4b\u3002", "motivation": "\u5148\u524d\u5173\u4e8e\u8fde\u7eed\u4ee4\u724c\u7684\u7814\u7a76\u8868\u660e\u5176\u5177\u6709\u66f4\u9ad8\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u4f46\u5b9e\u9645\u5e94\u7528\u53d7\u9650\u4e8e\u8bad\u7ec3\u56f0\u96be\u3002\u672c\u7814\u7a76\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u3001\u65e0\u9700\u4ece\u79bb\u6563 CoT \u84b8\u998f\u7684\u8fde\u7eed CoT \u5b66\u4e60\u65b9\u6cd5\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u6765\u5b66\u4e60\u8fde\u7eed CoT \u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f7f\u7528\u201c\u8f6f\u201d\u4ee4\u724c\uff08\u4ee4\u724c\u7684\u6df7\u5408\uff09\u548c\u5728\u8f93\u5165\u5d4c\u5165\u4e0a\u6dfb\u52a0\u566a\u58f0\u6765\u5b9e\u73b0 RL \u63a2\u7d22\uff0c\u4ece\u800c\u89e3\u51b3\u4e86\u8bad\u7ec3\u56f0\u96be\u7684\u95ee\u9898\uff0c\u5e76\u4e14\u8ba1\u7b97\u5f00\u9500\u6781\u5c0f\uff0c\u53ef\u4ee5\u5b66\u4e60\u5305\u542b\u6570\u767e\u4e2a\u4ee4\u724c\u7684\u8fde\u7eed CoT\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u4e0a\uff0c\u4f7f\u7528 Llama \u548c Qwen \u6a21\u578b\uff08\u9ad8\u8fbe 8B\uff09\u8fdb\u884c\u8bad\u7ec3\uff0c\u7ed3\u679c\u663e\u793a\uff1a1. \u8fde\u7eed CoT \u5728 pass@1 \u6307\u6807\u4e0a\u4e0e\u79bb\u6563\u4ee4\u724c CoT \u76f8\u5f53\u30022. \u8fde\u7eed CoT \u5728 pass@32 \u6307\u6807\u4e0a\u8d85\u8d8a\u4e86\u79bb\u6563\u4ee4\u724c CoT\uff0c\u8868\u660e\u5176\u5177\u6709\u66f4\u9ad8\u7684 CoT \u591a\u6837\u6027\u30023. \u6700\u4f73\u8bad\u7ec3\u7b56\u7565\u662f\u4f7f\u7528\u8fde\u7eed CoT \u4ee4\u724c\u8fdb\u884c\u8bad\u7ec3\uff0c\u7136\u540e\u4f7f\u7528\u79bb\u6563\u4ee4\u724c\u8fdb\u884c\u63a8\u7406\u30024. \u8fde\u7eed CoT \u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u80fd\u66f4\u597d\u5730\u4fdd\u7559\u57fa\u7840\u6a21\u578b\u5728\u975e\u9886\u57df\u5916\u4efb\u52a1\u4e0a\u7684\u9884\u6d4b\u3002", "conclusion": "\u672c\u7814\u7a76\u9996\u6b21\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u3001\u65e0\u9700\u4ece\u53c2\u8003\u79bb\u6563 CoT \u84b8\u998f\u7684\u8fde\u7eed CoT \u5b66\u4e60\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u4e86\u8fd9\u4e00\u76ee\u6807\u3002\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\uff0c\u63d0\u9ad8\u4e86 CoT \u7684\u591a\u6837\u6027\uff0c\u5e76\u80fd\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\uff08\u4f7f\u7528\u79bb\u6563\u4ee4\u724c\u8fdb\u884c\u63a8\u7406\uff09\u53d6\u5f97\u6709\u7ade\u4e89\u529b\u7684\u7ed3\u679c\uff0c\u540c\u65f6\u8fd8\u80fd\u66f4\u597d\u5730\u4fdd\u7559\u57fa\u7840\u6a21\u578b\u7684\u9884\u6d4b\u80fd\u529b\u3002"}}
{"id": "2509.18642", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18642", "abs": "https://arxiv.org/abs/2509.18642", "authors": ["Nicolas Toussaint", "Emanuele Colleoni", "Ricardo Sanchez-Matilla", "Joshua Sutcliffe", "Vanessa Thompson", "Muhammad Asad", "Imanol Luengo", "Danail Stoyanov"], "title": "Zero-shot Monocular Metric Depth for Endoscopic Images", "comment": "Accepted at MICCAI 2025 DEMI Workshop", "summary": "Monocular relative and metric depth estimation has seen a tremendous boost in\nthe last few years due to the sharp advancements in foundation models and in\nparticular transformer based networks. As we start to see applications to the\ndomain of endoscopic images, there is still a lack of robust benchmarks and\nhigh-quality datasets in that area. This paper addresses these limitations by\npresenting a comprehensive benchmark of state-of-the-art (metric and relative)\ndepth estimation models evaluated on real, unseen endoscopic images, providing\ncritical insights into their generalisation and performance in clinical\nscenarios. Additionally, we introduce and publish a novel synthetic dataset\n(EndoSynth) of endoscopic surgical instruments paired with ground truth metric\ndepth and segmentation masks, designed to bridge the gap between synthetic and\nreal-world data. We demonstrate that fine-tuning depth foundation models using\nour synthetic dataset boosts accuracy on most unseen real data by a significant\nmargin. By providing both a benchmark and a synthetic dataset, this work\nadvances the field of depth estimation for endoscopic images and serves as an\nimportant resource for future research. Project page, EndoSynth dataset and\ntrained weights are available at https://github.com/TouchSurgery/EndoSynth.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u9488\u5bf9\u5185\u7aa5\u955c\u56fe\u50cf\u7684\u6df1\u5ea6\u4f30\u8ba1\u7684\u57fa\u51c6\u6d4b\u8bd5\u548c\u65b0\u7684\u5408\u6210\u6570\u636e\u96c6EndoSynth\uff0c\u5e76\u901a\u8fc7\u5728\u8be5\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u6a21\u578b\u6765\u63d0\u5347\u6a21\u578b\u5728\u771f\u5b9e\u672a\u89c1\u5185\u7aa5\u955c\u56fe\u50cf\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5185\u7aa5\u955c\u56fe\u50cf\u7684\u6df1\u5ea6\u4f30\u8ba1\u7f3a\u4e4f\u9c81\u68d2\u7684\u57fa\u51c6\u6d4b\u8bd5\u548c\u9ad8\u8d28\u91cf\u7684\u6570\u636e\u96c6\uff0c\u9650\u5236\u4e86\u8be5\u9886\u57df\u7684\u53d1\u5c55\u548c\u5e94\u7528\u3002", "method": "1.\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u591a\u79cd\u6700\u5148\u8fdb\u6df1\u5ea6\u4f30\u8ba1\u6a21\u578b\uff08\u5305\u62ec\u5ea6\u91cf\u548c\u76f8\u5bf9\u6df1\u5ea6\u4f30\u8ba1\uff09\u5728\u771f\u5b9e\u3001\u672a\u89c1\u5185\u7aa5\u955c\u56fe\u50cf\u4e0a\u7684\u8bc4\u4f30\u57fa\u51c6\u3002 2.\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aEndoSynth\u7684\u65b0\u578b\u5408\u6210\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5305\u542b\u5185\u7aa5\u955c\u624b\u672f\u5668\u68b0\u53ca\u5176\u5bf9\u5e94\u7684\u771f\u5b9e\u5ea6\u91cf\u6df1\u5ea6\u548c\u5206\u5272\u63a9\u7801\u3002 3.\u901a\u8fc7\u5728EndoSynth\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u73b0\u6709\u7684\u6df1\u5ea6\u4f30\u8ba1\u57fa\u7840\u6a21\u578b\uff0c\u5e76\u5728\u771f\u5b9e\u5185\u7aa5\u955c\u56fe\u50cf\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "result": "\u5728\u771f\u5b9e\u3001\u672a\u89c1\u7684\u5185\u7aa5\u955c\u56fe\u50cf\u4e0a\uff0c\u5bf9\u73b0\u6709\u6df1\u5ea6\u4f30\u8ba1\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6027\u80fd\u8fdb\u884c\u4e86\u5173\u952e\u7684\u5206\u6790\u3002\u901a\u8fc7\u5728EndoSynth\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u540e\uff0c\u6df1\u5ea6\u4f30\u8ba1\u57fa\u7840\u6a21\u578b\u5728\u5927\u591a\u6570\u672a\u89c1\u771f\u5b9e\u6570\u636e\u4e0a\u7684\u51c6\u786e\u6027\u6709\u4e86\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u63d0\u4f9b\u4e00\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c\u4e00\u4e2a\u5408\u6210\u6570\u636e\u96c6\uff0c\u63a8\u52a8\u4e86\u5185\u7aa5\u955c\u56fe\u50cf\u6df1\u5ea6\u4f30\u8ba1\u9886\u57df\u7684\u53d1\u5c55\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u8d44\u6e90\u3002"}}
{"id": "2509.18353", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18353", "abs": "https://arxiv.org/abs/2509.18353", "authors": ["Jakub Adamczyk", "Jakub Poziemski", "Franciszek Job", "Mateusz Kr\u00f3l", "Maciej Makowski"], "title": "MolPILE - large-scale, diverse dataset for molecular representation learning", "comment": null, "summary": "The size, diversity, and quality of pretraining datasets critically determine\nthe generalization ability of foundation models. Despite their growing\nimportance in chemoinformatics, the effectiveness of molecular representation\nlearning has been hindered by limitations in existing small molecule datasets.\nTo address this gap, we present MolPILE, large-scale, diverse, and rigorously\ncurated collection of 222 million compounds, constructed from 6 large-scale\ndatabases using an automated curation pipeline. We present a comprehensive\nanalysis of current pretraining datasets, highlighting considerable\nshortcomings for training ML models, and demonstrate how retraining existing\nmodels on MolPILE yields improvements in generalization performance. This work\nprovides a standardized resource for model training, addressing the pressing\nneed for an ImageNet-like dataset in molecular chemistry.", "AI": {"tldr": "MolPILE\u662f\u4e00\u4e2a\u5305\u542b2.22\u4ebf\u4e2a\u5316\u5408\u7269\u7684\u5927\u89c4\u6a21\u3001\u591a\u6837\u5316\u4e14\u7ecf\u8fc7\u4e25\u683c\u7b5b\u9009\u7684\u5206\u5b50\u6570\u636e\u96c6\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u5c0f\u578b\u5206\u5b50\u6570\u636e\u96c6\u5728\u5316\u5b66\u4fe1\u606f\u5b66\u4e2d\u8868\u793a\u5b66\u4e60\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u5c0f\u5206\u5b50\u6570\u636e\u96c6\u89c4\u6a21\u3001\u591a\u6837\u6027\u548c\u8d28\u91cf\u6709\u9650\uff0c\u963b\u788d\u4e86\u5206\u5b50\u8868\u793a\u5b66\u4e60\u5728\u5316\u5b66\u4fe1\u606f\u5b66\u4e2d\u7684\u5e94\u7528\u548c\u57fa\u7840\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u4e00\u4e2a\u81ea\u52a8\u5316\u7684\u5904\u7406\u6d41\u7a0b\uff0c\u6574\u5408\u4e866\u4e2a\u5927\u89c4\u6a21\u6570\u636e\u5e93\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b2.22\u4ebf\u4e2a\u5316\u5408\u7269\u7684\u6570\u636e\u96c6\uff0c\u5e76\u5bf9\u5176\u8fdb\u884c\u4e86\u5168\u9762\u7684\u5206\u6790\u3002", "result": "\u5728MolPILE\u6570\u636e\u96c6\u4e0a\u91cd\u65b0\u8bad\u7ec3\u73b0\u6709\u6a21\u578b\u53ef\u4ee5\u63d0\u9ad8\u6a21\u578b\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u8be5\u6570\u636e\u96c6\u7684\u6709\u6548\u6027\u3002", "conclusion": "MolPILE\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6807\u51c6\u5316\u7684\u8d44\u6e90\uff0c\u89e3\u51b3\u4e86\u5316\u5b66\u9886\u57df\u5bf9\u7c7b\u4f3cImageNet\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\u7684\u9700\u6c42\uff0c\u4e3a\u57fa\u7840\u6a21\u578b\u7684\u8bad\u7ec3\u63d0\u4f9b\u4e86\u652f\u6301\u3002"}}
{"id": "2509.19199", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.19199", "abs": "https://arxiv.org/abs/2509.19199", "authors": ["Xiaoqian Liu", "Ke Wang", "Yuchuan Wu", "Fei Huang", "Yongbin Li", "Junge Zhang", "Jianbin Jiao"], "title": "Online Process Reward Leanring for Agentic Reinforcement Learning", "comment": "preprint", "summary": "Large language models (LLMs) are increasingly trained with reinforcement\nlearning (RL) as autonomous agents that reason and act over long horizons in\ninteractive environments.\n  However, sparse and sometimes unverifiable rewards make temporal credit\nassignment extremely challenging.\n  Recent work attempts to integrate process supervision into agent learning but\nsuffers from biased annotation, reward hacking, high-variance from overly\nfine-grained signals or failtures when state overlap is rare.\n  We therefore introduce Online Process Reward Learning (OPRL), a general\ncredit-assignment strategy for agentic RL that integrates seamlessly with\nstandard on-policy algorithms without relying on additional rollouts or\nexplicit step labels.\n  In OPRL, we optimize an implicit process reward model (PRM) alternately with\nthe agent's policy to transform trajectory preferences into implicit step\nrewards through a trajectory-based DPO objective.\n  These step rewards are then used to compute step-level advantages, which are\ncombined with episode-level advantages from outcome rewards for policy update,\ncreating a self-reinforcing loop.\n  Theoretical findings guarantee that the learned step rewards are consistent\nwith trajectory preferences and act as potential-based shaping rewards,\nproviding bounded gradients to stabilize training.\n  Empirically, we evaluate OPRL on three distinct agent benmarks, including\nWebShop and VisualSokoban, as well as open-ended social interactions with\nunverfiable rewards in SOTOPIA.\n  Crucially, OPRL shows superior performance over frontier LLMs and strong RL\nbaselines across domains, achieving state-of-the-art results with higher\nsample-efficiency and lower variance during training.\n  Further analysis also demonstrates the efficient exploration by OPRL using\nfewer actions, underscoring its potential for agentic learning in real-world\nscenarios.", "AI": {"tldr": "LLM \u5728\u4ea4\u4e92\u73af\u5883\u4e2d\u8fdb\u884c\u957f\u671f\u63a8\u7406\u548c\u884c\u52a8\u65f6\uff0c\u7531\u4e8e\u5956\u52b1\u7a00\u758f\u4e14\u96be\u4ee5\u9a8c\u8bc1\uff0c\u65f6\u95f4\u4fe1\u7528\u5206\u914d\u9762\u4e34\u6311\u6218\u3002\u672c\u6587\u63d0\u51fa\u4e86\u5728\u7ebf\u8fc7\u7a0b\u5956\u52b1\u5b66\u4e60\uff08OPRL\uff09\uff0c\u4e00\u79cd\u96c6\u6210\u4e86\u6807\u51c6\u7b56\u7565\u5b66\u4e60\u7684\u4fe1\u7528\u5206\u914d\u7b56\u7565\uff0c\u901a\u8fc7\u9690\u5f0f\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff08PRM\uff09\u5c06\u8f68\u8ff9\u504f\u597d\u8f6c\u5316\u4e3a\u9690\u5f0f\u6b65\u5956\u52b1\uff0c\u5e76\u4e0e\u7ed3\u679c\u5956\u52b1\u76f8\u7ed3\u5408\u8fdb\u884c\u7b56\u7565\u66f4\u65b0\uff0c\u4ece\u800c\u5b9e\u73b0\u81ea\u6211\u5f3a\u5316\u5b66\u4e60\u3002", "motivation": "\u5728 LLM \u4f5c\u4e3a\u81ea\u4e3b\u667a\u80fd\u4f53\u8fdb\u884c\u957f\u671f\u63a8\u7406\u548c\u884c\u52a8\u65f6\uff0c\u7a00\u758f\u4e14\u96be\u4ee5\u9a8c\u8bc1\u7684\u5956\u52b1\u4f7f\u5f97\u65f6\u95f4\u4fe1\u7528\u5206\u914d\u53d8\u5f97\u6781\u5176\u56f0\u96be\u3002\u73b0\u6709\u7684\u8fc7\u7a0b\u76d1\u7763\u65b9\u6cd5\u5b58\u5728\u6807\u6ce8\u504f\u5dee\u3001\u5956\u52b1\u7834\u89e3\u3001\u4fe1\u53f7\u8fc7\u4e8e\u7cbe\u7ec6\u5bfc\u81f4\u9ad8\u65b9\u5dee\u4ee5\u53ca\u72b6\u6001\u91cd\u53e0\u7a00\u758f\u65f6\u5931\u6548\u7b49\u95ee\u9898\u3002", "method": "OPRL \u662f\u4e00\u79cd\u4fe1\u7528\u5206\u914d\u7b56\u7565\uff0c\u5b83\u5c06\u8f68\u8ff9\u504f\u597d\u8f6c\u5316\u4e3a\u9690\u5f0f\u6b65\u5956\u52b1\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u5b83\u4ea4\u66ff\u4f18\u5316\u4e00\u4e2a\u9690\u5f0f\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff08PRM\uff09\u548c\u667a\u80fd\u4f53\u7684\u7b56\u7565\uff0c\u901a\u8fc7\u57fa\u4e8e\u8f68\u8ff9\u7684 DPO \u76ee\u6807\u5c06\u8f68\u8ff9\u504f\u597d\u8f6c\u5316\u4e3a\u9690\u5f0f\u6b65\u5956\u52b1\u3002\u7136\u540e\uff0c\u8fd9\u4e9b\u6b65\u5956\u52b1\u4e0e\u6765\u81ea\u7ed3\u679c\u5956\u52b1\u7684\u7247\u6bb5\u7ea7\u4f18\u52bf\u76f8\u7ed3\u5408\uff0c\u7528\u4e8e\u7b56\u7565\u66f4\u65b0\uff0c\u5f62\u6210\u4e00\u4e2a\u81ea\u6211\u5f3a\u5316\u7684\u5faa\u73af\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u989d\u5916\u7684 rollout \u6216\u663e\u5f0f\u7684\u6b65\u6807\u7b7e\u3002", "result": "OPRL \u5728 WebShop\u3001VisualSokoban \u548c SOTOPIA \u7684\u5f00\u653e\u5f0f\u793e\u4ea4\u4e92\u52a8\u7b49\u4e09\u4e2a\u4e0d\u540c\u7684\u667a\u80fd\u4f53\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u524d\u6cbf LLM \u548c\u5f3a\u5927\u7684 RL \u57fa\u7ebf\uff0c\u5728\u6837\u672c\u6548\u7387\u548c\u8bad\u7ec3\u65b9\u5dee\u65b9\u9762\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002\u6b64\u5916\uff0cOPRL \u901a\u8fc7\u66f4\u5c11\u7684\u52a8\u4f5c\u5b9e\u73b0\u4e86\u66f4\u6709\u6548\u7684\u63a2\u7d22\u3002", "conclusion": "OPRL \u662f\u4e00\u79cd\u901a\u7528\u7684\u4fe1\u7528\u5206\u914d\u7b56\u7565\uff0c\u53ef\u4ee5\u4e0e\u6807\u51c6\u7b56\u7565\u5b66\u4e60\u7b97\u6cd5\u65e0\u7f1d\u96c6\u6210\uff0c\u89e3\u51b3\u4e86 LLM \u5728\u5177\u6709\u7a00\u758f\u6216\u4e0d\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u4ea4\u4e92\u73af\u5883\u4e2d\u5b66\u4e60\u65f6\u9762\u4e34\u7684\u65f6\u95f4\u4fe1\u7528\u5206\u914d\u6311\u6218\u3002\u7406\u8bba\u4e0a\uff0cOPRL \u4fdd\u8bc1\u4e86\u5b66\u4e60\u5230\u7684\u6b65\u5956\u52b1\u4e0e\u8f68\u8ff9\u504f\u597d\u4e00\u81f4\uff0c\u5e76\u4e14\u5145\u5f53\u4e86\u6f5c\u5728\u7684\u5851\u9020\u5956\u52b1\uff0c\u63d0\u4f9b\u4e86\u6709\u754c\u68af\u5ea6\u4ee5\u7a33\u5b9a\u8bad\u7ec3\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e OPRL \u5728\u6837\u672c\u6548\u7387\u3001\u8bad\u7ec3\u7a33\u5b9a\u6027\u4ee5\u53ca\u63a2\u7d22\u6548\u7387\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u73b0\u5b9e\u4e16\u754c\u7684\u667a\u80fd\u4f53\u5b66\u4e60\u4e2d\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2509.19261", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.19261", "abs": "https://arxiv.org/abs/2509.19261", "authors": ["Kuanqi Cai", "Chunfeng Wang", "Zeqi Li", "Haowen Yao", "Weinan Chen", "Luis Figueredo", "Aude Billard", "Arash Ajoudani"], "title": "Imitation-Guided Bimanual Planning for Stable Manipulation under Changing External Forces", "comment": null, "summary": "Robotic manipulation in dynamic environments often requires seamless\ntransitions between different grasp types to maintain stability and efficiency.\nHowever, achieving smooth and adaptive grasp transitions remains a challenge,\nparticularly when dealing with external forces and complex motion constraints.\nExisting grasp transition strategies often fail to account for varying external\nforces and do not optimize motion performance effectively. In this work, we\npropose an Imitation-Guided Bimanual Planning Framework that integrates\nefficient grasp transition strategies and motion performance optimization to\nenhance stability and dexterity in robotic manipulation. Our approach\nintroduces Strategies for Sampling Stable Intersections in Grasp Manifolds for\nseamless transitions between uni-manual and bi-manual grasps, reducing\ncomputational costs and regrasping inefficiencies. Additionally, a Hierarchical\nDual-Stage Motion Architecture combines an Imitation Learning-based Global Path\nGenerator with a Quadratic Programming-driven Local Planner to ensure real-time\nmotion feasibility, obstacle avoidance, and superior manipulability. The\nproposed method is evaluated through a series of force-intensive tasks,\ndemonstrating significant improvements in grasp transition efficiency and\nmotion performance. A video demonstrating our simulation results can be viewed\nat\n\\href{https://youtu.be/3DhbUsv4eDo}{\\textcolor{blue}{https://youtu.be/3DhbUsv4eDo}}.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u4eff\u5f15\u5bfc\u7684\u53cc\u81c2\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u96c6\u6210\u9ad8\u6548\u7684\u6293\u53d6\u8fc7\u6e21\u7b56\u7565\u548c\u8fd0\u52a8\u6027\u80fd\u4f18\u5316\uff0c\u6765\u63d0\u9ad8\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u7a33\u5b9a\u6027\u548c\u7075\u6d3b\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u6293\u53d6\u8fc7\u6e21\u7b56\u7565\u5e38\u5e38\u65e0\u6cd5\u8003\u8651\u4e0d\u65ad\u53d8\u5316\u7684\u5916\u90e8\u529b\uff0c\u4e5f\u4e0d\u80fd\u6709\u6548\u5730\u4f18\u5316\u8fd0\u52a8\u6027\u80fd\uff0c\u5bfc\u81f4\u5728\u52a8\u6001\u73af\u5883\u4e2d\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u7a33\u5b9a\u6027\u548c\u6548\u7387\u53d7\u5230\u9650\u5236\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u4eff\u5f15\u5bfc\u7684\u53cc\u81c2\u89c4\u5212\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5305\u542b\u7528\u4e8e\u5728\u5355\u81c2\u548c\u53cc\u81c2\u6293\u53d6\u4e4b\u95f4\u5b9e\u73b0\u65e0\u7f1d\u8fc7\u6e21\u7684\u7a33\u5b9a\u4ea4\u96c6\u91c7\u6837\u7b56\u7565\uff0c\u4ee5\u53ca\u4e00\u4e2a\u7ed3\u5408\u4e86\u6a21\u4eff\u5b66\u4e60\u7684\u5168\u5c40\u8def\u5f84\u751f\u6210\u5668\u548c\u4e8c\u6b21\u89c4\u5212\u9a71\u52a8\u7684\u5c40\u90e8\u89c4\u5212\u5668\u7684\u5206\u5c42\u53cc\u9636\u6bb5\u8fd0\u52a8\u67b6\u6784\u3002", "result": "\u901a\u8fc7\u4e00\u7cfb\u5217\u529b\u5bc6\u96c6\u578b\u4efb\u52a1\u7684\u8bc4\u4f30\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u6293\u53d6\u8fc7\u6e21\u6548\u7387\u548c\u8fd0\u52a8\u6027\u80fd\u65b9\u9762\u6709\u4e86\u663e\u8457\u7684\u6539\u8fdb\u3002", "conclusion": "\u63d0\u51fa\u7684\u6a21\u4eff\u5f15\u5bfc\u7684\u53cc\u81c2\u89c4\u5212\u6846\u67b6\u80fd\u591f\u6709\u6548\u5730\u5904\u7406\u52a8\u6001\u73af\u5883\u4e2d\u7684\u6293\u53d6\u8fc7\u6e21\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u7a33\u5b9a\u6027\u548c\u7075\u6d3b\u6027\u3002"}}
{"id": "2509.18683", "categories": ["cs.CV", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.18683", "abs": "https://arxiv.org/abs/2509.18683", "authors": ["Lanhu Wu", "Zilin Gao", "Hao Fei", "Mong-Li Lee", "Wynne Hsu"], "title": "LEAF-Mamba: Local Emphatic and Adaptive Fusion State Space Model for RGB-D Salient Object Detection", "comment": "Accepted to ACM MM 2025", "summary": "RGB-D salient object detection (SOD) aims to identify the most conspicuous\nobjects in a scene with the incorporation of depth cues. Existing methods\nmainly rely on CNNs, limited by the local receptive fields, or Vision\nTransformers that suffer from the cost of quadratic complexity, posing a\nchallenge in balancing performance and computational efficiency. Recently,\nstate space models (SSM), Mamba, have shown great potential for modeling\nlong-range dependency with linear complexity. However, directly applying SSM to\nRGB-D SOD may lead to deficient local semantics as well as the inadequate\ncross-modality fusion. To address these issues, we propose a Local Emphatic and\nAdaptive Fusion state space model (LEAF-Mamba) that contains two novel\ncomponents: 1) a local emphatic state space module (LE-SSM) to capture\nmulti-scale local dependencies for both modalities. 2) an SSM-based adaptive\nfusion module (AFM) for complementary cross-modality interaction and reliable\ncross-modality integration. Extensive experiments demonstrate that the\nLEAF-Mamba consistently outperforms 16 state-of-the-art RGB-D SOD methods in\nboth efficacy and efficiency. Moreover, our method can achieve excellent\nperformance on the RGB-T SOD task, proving a powerful generalization ability.", "AI": {"tldr": "\u63d0\u51faLEAF-Mamba\u6a21\u578b\uff0c\u7528\u4e8eRGB-D\u663e\u8457\u6027\u76ee\u6807\u68c0\u6d4b\uff0c\u901a\u8fc7\u5f15\u5165\u5c40\u90e8\u6e17\u6d41\u548c\u81ea\u9002\u5e94\u878d\u5408\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u73b0\u6709CNN\u548cTransformer\u6a21\u578b\u5728\u611f\u53d7\u91ce\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u4e0a\u7684\u9650\u5236\uff0c\u5e76\u5728RGB-D\u548cRGB-T SOD\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709RGB-D\u663e\u8457\u6027\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\uff08\u4e3b\u8981\u57fa\u4e8eCNN\u6216Transformer\uff09\u5728\u5904\u7406\u957f\u7a0b\u4f9d\u8d56\u3001\u5e73\u8861\u6027\u80fd\u4e0e\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u5b58\u5728\u6311\u6218\u3002", "method": "\u63d0\u51faLEAF-Mamba\u6a21\u578b\uff0c\u5305\u542b\u5c40\u90e8\u6e17\u6d41\u72b6\u6001\u7a7a\u95f4\u6a21\u5757\uff08LE-SSM\uff09\u6765\u6355\u6349\u591a\u5c3a\u5ea6\u5c40\u90e8\u4f9d\u8d56\uff0c\u4ee5\u53ca\u57fa\u4e8e\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u7684\u81ea\u9002\u5e94\u878d\u5408\u6a21\u5757\uff08AFM\uff09\u6765\u8fdb\u884c\u8de8\u6a21\u6001\u4ea4\u4e92\u548c\u878d\u5408\u3002", "result": "LEAF-Mamba\u5728RGB-D SOD\u4efb\u52a1\u4e0a\u8d85\u8d8a\u4e8616\u79cd\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5728RGB-T SOD\u4efb\u52a1\u4e0a\u4e5f\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "LEAF-Mamba\u5728RGB-D\u663e\u8457\u6027\u76ee\u6807\u68c0\u6d4b\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u548c\u6548\u7387\uff0c\u5e76\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2509.18362", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18362", "abs": "https://arxiv.org/abs/2509.18362", "authors": ["Yuxuan Cai", "Xiaozhuan Liang", "Xinghua Wang", "Jin Ma", "Haijin Liang", "Jinwen Luo", "Xinyu Zuo", "Lisheng Duan", "Yuyang Yin", "Xi Chen"], "title": "FastMTP: Accelerating LLM Inference with Enhanced Multi-Token Prediction", "comment": null, "summary": "As large language models (LLMs) become increasingly powerful, the sequential\nnature of autoregressive generation creates a fundamental throughput bottleneck\nthat limits the practical deployment. While Multi-Token Prediction (MTP) has\ndemonstrated remarkable benefits for model training efficiency and performance,\nits inherent potential for inference acceleration remains largely unexplored.\nThis paper introduces FastMTP, a simple yet effective method that improves\nmulti-step draft quality by aligning MTP training with its inference pattern,\nsignificantly enhancing speculative decoding performance. Our approach\nfine-tunes a single MTP head with position-shared weights on self-distilled\ndata, enabling it to capture dependencies among consecutive future tokens and\nmaintain high acceptance rates across multiple recursive draft steps. By\nintegrating language-aware dynamic vocabulary compression into the MTP head, we\nfurther reduce computational overhead in the drafting process. Experimental\nresults across seven diverse benchmarks demonstrate that FastMTP achieves an\naverage of 2.03x speedup compared to standard next token prediction with\nlossless output quality, outperforming vanilla MTP by 82%. FastMTP requires\nonly lightweight training and seamlessly integrates with existing inference\nframeworks, offering a practical and rapidly deployable solution for\naccelerating LLM inference.", "AI": {"tldr": "FastMTP\u901a\u8fc7\u5bf9MTP\u5934\u8fdb\u884c\u5fae\u8c03\uff0c\u4f7f\u5176\u4e0e\u63a8\u7406\u6a21\u5f0f\u5bf9\u9f50\uff0c\u4ece\u800c\u63d0\u9ad8\u591a\u6b65\u8349\u7a3f\u8d28\u91cf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6295\u673a\u89e3\u7801\u6027\u80fd\uff0c\u5b9e\u73b0\u4e862.03\u500d\u7684\u5e73\u5747\u52a0\u901f\uff0c\u4e14\u8f93\u51fa\u8d28\u91cf\u65e0\u635f\u3002", "motivation": "\u81ea\u56de\u5f52\u751f\u6210\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u63a8\u7406\u8fc7\u7a0b\u4e2d\u5b58\u5728\u541e\u5410\u91cf\u74f6\u9888\uff0c\u800c\u591a\u4ee4\u724c\u9884\u6d4b\uff08MTP\uff09\u5728\u63d0\u9ad8\u63a8\u7406\u901f\u5ea6\u65b9\u9762\u7684\u6f5c\u529b\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "FastMTP\u901a\u8fc7\u5bf9MTP\u5934\u8fdb\u884c\u5fae\u8c03\uff0c\u4f7f\u5176\u4e0e\u63a8\u7406\u6a21\u5f0f\u5bf9\u9f50\uff0c\u5e76\u91c7\u7528\u4f4d\u7f6e\u5171\u4eab\u6743\u91cd\u5728\u81ea\u84b8\u998f\u6570\u636e\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u4ee5\u6355\u6349\u8fde\u7eed\u672a\u6765\u4ee4\u724c\u4e4b\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\u3002\u6b64\u5916\uff0c\u8fd8\u96c6\u6210\u4e86\u8bed\u8a00\u611f\u77e5\u52a8\u6001\u8bcd\u6c47\u538b\u7f29\u6280\u672f\u6765\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u3002", "result": "FastMTP\u5728\u4e03\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u5e73\u57472.03\u500d\u7684\u52a0\u901f\uff0c\u8f93\u51fa\u8d28\u91cf\u65e0\u635f\uff0c\u6bd4\u6807\u51c6\u7684MTP\u65b9\u6cd5\u63d0\u9ad8\u4e8682%\u3002", "conclusion": "FastMTP\u662f\u4e00\u79cd\u5b9e\u7528\u4e14\u6613\u4e8e\u90e8\u7f72\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u8bad\u7ec3\u548c\u4e0e\u73b0\u6709\u63a8\u7406\u6846\u67b6\u7684\u65e0\u7f1d\u96c6\u6210\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLM\u63a8\u7406\u901f\u5ea6\u6162\u7684\u95ee\u9898\u3002"}}
{"id": "2509.19212", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19212", "abs": "https://arxiv.org/abs/2509.19212", "authors": ["Zheyuan Liu", "Zhangchen Xu", "Guangyao Dou", "Xiangchi Yuan", "Zhaoxuan Tan", "Radha Poovendran", "Meng Jiang"], "title": "Steering Multimodal Large Language Models Decoding for Context-Aware Safety", "comment": "A lightweight and model-agnostic decoding framework that dynamically\n  adjusts token generation based on multimodal context", "summary": "Multimodal Large Language Models (MLLMs) are increasingly deployed in\nreal-world applications, yet their ability to make context-aware safety\ndecisions remains limited. Existing methods often fail to balance\noversensitivity (unjustified refusals of benign queries) and undersensitivity\n(missed detection of visually grounded risks), leaving a persistent gap in\nsafety alignment. To address this issue, we introduce Safety-aware Contrastive\nDecoding (SafeCoDe), a lightweight and model-agnostic decoding framework that\ndynamically adjusts token generation based on multimodal context. SafeCoDe\noperates in two stages: (1) a contrastive decoding mechanism that highlights\ntokens sensitive to visual context by contrasting real and Gaussian-noised\nimages, and (2) a global-aware token modulation strategy that integrates\nscene-level reasoning with token-level adjustment to adapt refusals according\nto the predicted safety verdict. Extensive experiments across diverse MLLM\narchitectures and safety benchmarks, covering undersensitivity,\noversensitivity, and general safety evaluations, show that SafeCoDe\nconsistently improves context-sensitive refusal behaviors while preserving\nmodel helpfulness.", "AI": {"tldr": "MLLMs\u5728\u5b89\u5168\u51b3\u7b56\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0cSafeCoDe\u901a\u8fc7\u5bf9\u6bd4\u89e3\u7801\u548c\u5168\u5c40\u611f\u77e5\u6765\u89e3\u51b3\u8fc7/\u6b20\u654f\u611f\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709MLLMs\u5728\u8fdb\u884c\u5b89\u5168\u51b3\u7b56\u65f6\uff0c\u5728\u533a\u5206\u826f\u6027\u548c\u6709\u5bb3\u67e5\u8be2\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5bfc\u81f4\u4e86\u8fc7\u5ea6\u654f\u611f\u6216\u68c0\u6d4b\u9057\u6f0f\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSafeCoDe\u7684\u8f7b\u91cf\u7ea7\u3001\u6a21\u578b\u65e0\u5173\u7684\u89e3\u7801\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5305\u62ec\uff1a1.\u5bf9\u6bd4\u89e3\u7801\u673a\u5236\uff0c\u901a\u8fc7\u5bf9\u6bd4\u771f\u5b9e\u56fe\u50cf\u548c\u9ad8\u65af\u566a\u58f0\u56fe\u50cf\u6765\u8bc6\u522b\u5bf9\u89c6\u89c9\u4e0a\u4e0b\u6587\u654f\u611f\u7684token\uff1b2.\u5168\u5c40\u611f\u77e5token\u8c03\u5236\u7b56\u7565\uff0c\u7ed3\u5408\u573a\u666f\u7ea7\u63a8\u7406\u548ctoken\u7ea7\u8c03\u6574\uff0c\u4ee5\u9002\u5e94\u9884\u6d4b\u7684\u5b89\u5168\u5224\u51b3\u3002", "result": "SafeCoDe\u5728\u591a\u79cdMLLMs\u67b6\u6784\u548c\u5b89\u5168\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u80fd\u591f\u63d0\u5347\u6a21\u578b\u5bf9\u89c6\u89c9\u4e0a\u4e0b\u6587\u654f\u611f\u7684\u62d2\u7edd\u80fd\u529b\uff0c\u540c\u65f6\u4e0d\u635f\u5bb3\u6a21\u578b\u7684\u6574\u4f53\u6548\u7528\u3002", "conclusion": "SafeCoDe\u662f\u4e00\u79cd\u6709\u6548\u7684\u6846\u67b6\uff0c\u53ef\u4ee5\u63d0\u9ad8MLLMs\u5728\u5b89\u5168\u51b3\u7b56\u65b9\u9762\u7684\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u8fc7\u5ea6\u654f\u611f\u548c\u68c0\u6d4b\u9057\u6f0f\u65b9\u9762\u7684\u95ee\u9898\u3002"}}
{"id": "2509.19292", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19292", "abs": "https://arxiv.org/abs/2509.19292", "authors": ["Yang Jin", "Jun Lv", "Han Xue", "Wendi Chen", "Chuan Wen", "Cewu Lu"], "title": "SOE: Sample-Efficient Robot Policy Self-Improvement via On-Manifold Exploration", "comment": null, "summary": "Intelligent agents progress by continually refining their capabilities\nthrough actively exploring environments. Yet robot policies often lack\nsufficient exploration capability due to action mode collapse. Existing methods\nthat encourage exploration typically rely on random perturbations, which are\nunsafe and induce unstable, erratic behaviors, thereby limiting their\neffectiveness. We propose Self-Improvement via On-Manifold Exploration (SOE), a\nframework that enhances policy exploration and improvement in robotic\nmanipulation. SOE learns a compact latent representation of task-relevant\nfactors and constrains exploration to the manifold of valid actions, ensuring\nsafety, diversity, and effectiveness. It can be seamlessly integrated with\narbitrary policy models as a plug-in module, augmenting exploration without\ndegrading the base policy performance. Moreover, the structured latent space\nenables human-guided exploration, further improving efficiency and\ncontrollability. Extensive experiments in both simulation and real-world tasks\ndemonstrate that SOE consistently outperforms prior methods, achieving higher\ntask success rates, smoother and safer exploration, and superior sample\nefficiency. These results establish on-manifold exploration as a principled\napproach to sample-efficient policy self-improvement. Project website:\nhttps://ericjin2002.github.io/SOE", "AI": {"tldr": "SOE\u6846\u67b6\u901a\u8fc7\u5728\u53ef\u884c\u52a8\u4f5c\u6d41\u5f62\u4e0a\u8fdb\u884c\u63a2\u7d22\uff0c\u589e\u5f3a\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u7684\u63a2\u7d22\u548c\u6539\u8fdb\u80fd\u529b\uff0c\u63d0\u9ad8\u4e86\u4efb\u52a1\u6210\u529f\u7387\u548c\u6837\u672c\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u8bc1\u4e86\u5b89\u5168\u6027\u548c\u591a\u6837\u6027\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u4eba\u7b56\u7565\u7684\u63a2\u7d22\u80fd\u529b\u4e0d\u8db3\uff0c\u5e38\u56e0\u52a8\u4f5c\u6a21\u5f0f\u5d29\u6e83\u548c\u968f\u673a\u6270\u52a8\u5bfc\u81f4\u4e0d\u5b89\u5168\u3001\u4e0d\u7a33\u5b9a\u7684\u884c\u4e3a\u3002SOE\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u9ad8\u63a2\u7d22\u7684\u5b89\u5168\u6027\u3001\u591a\u6837\u6027\u548c\u6709\u6548\u6027\u3002", "method": "SOE\u5b66\u4e60\u4efb\u52a1\u76f8\u5173\u56e0\u7d20\u7684\u7d27\u51d1\u6f5c\u5728\u8868\u793a\uff0c\u5e76\u5c06\u63a2\u7d22\u9650\u5236\u5728\u53ef\u884c\u52a8\u4f5c\u7684\u6d41\u5f62\u4e0a\uff0c\u4ee5\u4fdd\u8bc1\u5b89\u5168\u3001\u591a\u6837\u548c\u6709\u6548\u3002\u5b83\u53ef\u4ee5\u4f5c\u4e3a\u63d2\u4ef6\u6a21\u5757\u96c6\u6210\u5230\u73b0\u6709\u7b56\u7565\u6a21\u578b\u4e2d\uff0c\u589e\u5f3a\u63a2\u7d22\u80fd\u529b\u800c\u4e0d\u5f71\u54cd\u57fa\u7840\u7b56\u7565\u6027\u80fd\u3002", "result": "SOE\u5728\u6a21\u62df\u548c\u73b0\u5b9e\u4e16\u754c\u7684\u4efb\u52a1\u4e2d\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u4efb\u52a1\u6210\u529f\u7387\u3001\u66f4\u5e73\u7a33\u548c\u66f4\u5b89\u5168\u7684\u63a2\u7d22\uff0c\u4ee5\u53ca\u66f4\u597d\u7684\u6837\u672c\u6548\u7387\u3002", "conclusion": "SOE\u5c06\u6d41\u5f62\u63a2\u7d22\u786e\u7acb\u4e3a\u4e00\u79cd\u539f\u5219\u6027\u7684\u3001\u6837\u672c\u9ad8\u6548\u7684\u7b56\u7565\u81ea\u6211\u6539\u8fdb\u65b9\u6cd5\u3002"}}
{"id": "2509.18692", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18692", "abs": "https://arxiv.org/abs/2509.18692", "authors": ["Xinle Gao", "Linghui Ye", "Zhiyong Xiao"], "title": "Lightweight Vision Transformer with Window and Spatial Attention for Food Image Classification", "comment": null, "summary": "With the rapid development of society and continuous advances in science and\ntechnology, the food industry increasingly demands higher production quality\nand efficiency. Food image classification plays a vital role in enabling\nautomated quality control on production lines, supporting food safety\nsupervision, and promoting intelligent agricultural production. However, this\ntask faces challenges due to the large number of parameters and high\ncomputational complexity of Vision Transformer models. To address these issues,\nwe propose a lightweight food image classification algorithm that integrates a\nWindow Multi-Head Attention Mechanism (WMHAM) and a Spatial Attention Mechanism\n(SAM). The WMHAM reduces computational cost by capturing local and global\ncontextual features through efficient window partitioning, while the SAM\nadaptively emphasizes key spatial regions to improve discriminative feature\nrepresentation. Experiments conducted on the Food-101 and Vireo Food-172\ndatasets demonstrate that our model achieves accuracies of 95.24% and 94.33%,\nrespectively, while significantly reducing parameters and FLOPs compared with\nbaseline methods. These results confirm that the proposed approach achieves an\neffective balance between computational efficiency and classification\nperformance, making it well-suited for deployment in resource-constrained\nenvironments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u98df\u7269\u56fe\u50cf\u5206\u7c7b\u7b97\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u7a97\u53e3\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\uff08WMHAM\uff09\u548c\u7a7a\u95f4\u6ce8\u610f\u529b\u673a\u5236\uff08SAM\uff09\uff0c\u5728\u4fdd\u8bc1\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u6a21\u578b\u53c2\u6570\u91cf\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u3002", "motivation": "\u98df\u7269\u56fe\u50cf\u5206\u7c7b\u5728\u81ea\u52a8\u5316\u8d28\u68c0\u3001\u98df\u54c1\u5b89\u5168\u76d1\u7ba1\u548c\u667a\u80fd\u519c\u4e1a\u751f\u4ea7\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u7684Vision Transformer\u6a21\u578b\u5b58\u5728\u53c2\u6570\u91cf\u5927\u3001\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u98df\u7269\u56fe\u50cf\u5206\u7c7b\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u96c6\u6210\u4e86\u7a97\u53e3\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\uff08WMHAM\uff09\u548c\u7a7a\u95f4\u6ce8\u610f\u529b\u673a\u5236\uff08SAM\uff09\u3002WMHAM\u901a\u8fc7\u7a97\u53e3\u5212\u5206\u6765\u6355\u6349\u5c40\u90e8\u548c\u5168\u5c40\u4e0a\u4e0b\u6587\u7279\u5f81\uff0c\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff1bSAM\u81ea\u9002\u5e94\u5730\u5f3a\u8c03\u5173\u952e\u7a7a\u95f4\u533a\u57df\uff0c\u4ee5\u589e\u5f3a\u533a\u5206\u6027\u7279\u5f81\u8868\u793a\u3002", "result": "\u5728Food-101\u548cVireo Food-172\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u6a21\u578b\u5206\u522b\u8fbe\u5230\u4e8695.24%\u548c94.33%\u7684\u51c6\u786e\u7387\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e86\u53c2\u6570\u91cf\u548cFLOPs\u3002", "conclusion": "\u8be5\u6a21\u578b\u5728\u8ba1\u7b97\u6548\u7387\u548c\u5206\u7c7b\u6027\u80fd\u4e4b\u95f4\u53d6\u5f97\u4e86\u6709\u6548\u7684\u5e73\u8861\uff0c\u9002\u5408\u5728\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u4e2d\u90e8\u7f72\u3002"}}
{"id": "2509.18367", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18367", "abs": "https://arxiv.org/abs/2509.18367", "authors": ["Zhuoyu Yao", "Yue Wang", "Songyang Zhang", "Yingshu Li", "Zhipeng Cai", "Zhi Tian"], "title": "Multi-Worker Selection based Distributed Swarm Learning for Edge IoT with Non-i.i.d. Data", "comment": null, "summary": "Recent advances in distributed swarm learning (DSL) offer a promising\nparadigm for edge Internet of Things. Such advancements enhance data privacy,\ncommunication efficiency, energy saving, and model scalability. However, the\npresence of non-independent and identically distributed (non-i.i.d.) data pose\na significant challenge for multi-access edge computing, degrading learning\nperformance and diverging training behavior of vanilla DSL. Further, there\nstill lacks theoretical guidance on how data heterogeneity affects model\ntraining accuracy, which requires thorough investigation. To fill the gap, this\npaper first study the data heterogeneity by measuring the impact of non-i.i.d.\ndatasets under the DSL framework. This then motivates a new multi-worker\nselection design for DSL, termed M-DSL algorithm, which works effectively with\ndistributed heterogeneous data. A new non-i.i.d. degree metric is introduced\nand defined in this work to formulate the statistical difference among local\ndatasets, which builds a connection between the measure of data heterogeneity\nand the evaluation of DSL performance. In this way, our M-DSL guides effective\nselection of multiple works who make prominent contributions for global model\nupdates. We also provide theoretical analysis on the convergence behavior of\nour M-DSL, followed by extensive experiments on different heterogeneous\ndatasets and non-i.i.d. data settings. Numerical results verify performance\nimprovement and network intelligence enhancement provided by our M-DSL beyond\nthe benchmarks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aM-DSL\u7684\u65b0\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u5206\u5e03\u5f0f\u7fa4\u4f53\u5b66\u4e60\uff08DSL\uff09\u4e2d\u7531\u975e\u72ec\u7acb\u540c\u5206\u5e03\uff08non-i.i.d.\uff09\u6570\u636e\u5f15\u8d77\u7684\u6570\u636e\u5f02\u8d28\u6027\u6311\u6218\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u5206\u5e03\u5f0f\u7fa4\u4f53\u5b66\u4e60\uff08DSL\uff09\u5728\u8fb9\u7f18\u7269\u8054\u7f51\u9886\u57df\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9762\u4e34\u975e\u72ec\u7acb\u540c\u5206\u5e03\uff08non-i.i.d.\uff09\u6570\u636e\u5e26\u6765\u7684\u6311\u6218\uff0c\u5bfc\u81f4\u5b66\u4e60\u6027\u80fd\u4e0b\u964d\u548c\u8bad\u7ec3\u884c\u4e3a\u4e0d\u4e00\u81f4\u3002\u6b64\u5916\uff0c\u7f3a\u4e4f\u5173\u4e8e\u6570\u636e\u5f02\u8d28\u6027\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u8bad\u7ec3\u51c6\u786e\u6027\u7684\u7406\u8bba\u6307\u5bfc\u3002", "method": "\u8bba\u6587\u9996\u5148\u7814\u7a76\u4e86\u6570\u636e\u5f02\u8d28\u6027\u5bf9DSL\u6846\u67b6\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u5de5\u4f5c\u8005\u9009\u62e9\u673a\u5236\uff0c\u79f0\u4e3aM-DSL\u7b97\u6cd5\u3002\u8be5\u7b97\u6cd5\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u7684\u975ei.i.d.\u7a0b\u5ea6\u5ea6\u91cf\uff0c\u7528\u4e8e\u91cf\u5316\u672c\u5730\u6570\u636e\u96c6\u4e4b\u95f4\u7684\u7edf\u8ba1\u5dee\u5f02\uff0c\u5e76\u5c06\u6570\u636e\u5f02\u8d28\u6027\u7684\u5ea6\u91cf\u4e0eDSL\u6027\u80fd\u8bc4\u4f30\u8054\u7cfb\u8d77\u6765\u3002M-DSL\u901a\u8fc7\u6709\u6548\u7684\u591a\u5de5\u4f5c\u8005\u9009\u62e9\u6765\u66f4\u65b0\u5168\u5c40\u6a21\u578b\uff0c\u5e76\u63d0\u4f9b\u4e86\u6536\u655b\u884c\u4e3a\u7684\u7406\u8bba\u5206\u6790\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cM-DSL\u7b97\u6cd5\u5728\u4e0d\u540c\u7684\u5f02\u8d28\u6570\u636e\u96c6\u548c\u975ei.i.d.\u6570\u636e\u8bbe\u7f6e\u4e0b\uff0c\u80fd\u591f\u6709\u6548\u63d0\u9ad8\u6027\u80fd\uff0c\u5e76\u589e\u5f3a\u7f51\u7edc\u667a\u80fd\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u51c6\u65b9\u6cd5\u3002", "conclusion": "M-DSL\u7b97\u6cd5\u901a\u8fc7\u6709\u6548\u9009\u62e9\u5bf9\u5168\u5c40\u6a21\u578b\u66f4\u65b0\u505a\u51fa\u8d21\u732e\u7684\u5de5\u4f5c\u8005\uff0c\u89e3\u51b3\u4e86DSL\u4e2d\u6570\u636e\u5f02\u8d28\u6027\u5e26\u6765\u7684\u6311\u6218\uff0c\u5e76\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u8bc1\u660e\u4e86\u5176\u5728\u63d0\u9ad8\u6a21\u578b\u51c6\u786e\u6027\u548c\u7f51\u7edc\u667a\u80fd\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2509.19224", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19224", "abs": "https://arxiv.org/abs/2509.19224", "authors": ["Tariq Abdul-Quddoos", "Xishuang Dong", "Lijun Qian"], "title": "Systematic Comparative Analysis of Large Pretrained Language Models on Contextualized Medication Event Extraction", "comment": null, "summary": "Attention-based models have become the leading approach in modeling medical\nlanguage for Natural Language Processing (NLP) in clinical notes. These models\noutperform traditional techniques by effectively capturing contextual rep-\nresentations of language. In this research a comparative analysis is done\namongst pre- trained attention based models namely Bert Base, BioBert, two\nvariations of Bio+Clinical Bert, RoBerta, and Clinical Long- former on task\nrelated to Electronic Health Record (EHR) information extraction. The tasks\nfrom Track 1 of Harvard Medical School's 2022 National Clinical NLP Challenges\n(n2c2) are considered for this comparison, with the Contextualized Medication\nEvent Dataset (CMED) given for these task. CMED is a dataset of unstructured\nEHRs and annotated notes that contain task relevant information about the EHRs.\nThe goal of the challenge is to develop effective solutions for extracting\ncontextual information related to patient medication events from EHRs using\ndata driven methods. Each pre-trained model is fine-tuned and applied on CMED\nto perform medication extraction, medical event detection, and\nmulti-dimensional medication event context classification. Pro- cessing methods\nare also detailed for breaking down EHRs for compatibility with the applied\nmodels. Performance analysis has been carried out using a script based on\nconstructing medical terms from the evaluation portion of CMED with metrics\nincluding recall, precision, and F1-Score. The results demonstrate that models\npre-trained on clinical data are more effective in detecting medication and\nmedication events, but Bert Base, pre- trained on general domain data showed to\nbe the most effective for classifying the context of events related to\nmedications.", "AI": {"tldr": "\u57fa\u4e8e BERT \u7684\u6a21\u578b\u5728\u4e34\u5e8a\u7b14\u8bb0\u7684 NLP \u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u9884\u8bad\u7ec3\u6570\u636e\u9886\u57df\u5f71\u54cd\u6a21\u578b\u6027\u80fd\uff1a\u4e34\u5e8a\u6570\u636e\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u4e8b\u4ef6\u68c0\u6d4b\u4e0a\u66f4\u4f18\uff0c\u800c\u901a\u7528\u6570\u636e\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u4e8b\u4ef6\u4e0a\u4e0b\u6587\u5206\u7c7b\u4e0a\u66f4\u4f18\u3002", "motivation": "\u6bd4\u8f83\u4e0d\u540c\u9884\u8bad\u7ec3\u7684\u6ce8\u610f\u529b\u6a21\u578b\u5728\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u4fe1\u606f\u63d0\u53d6\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u7279\u522b\u662f\u533a\u5206\u5728\u4e34\u5e8a\u6570\u636e\u548c\u901a\u7528\u6570\u636e\u4e0a\u9884\u8bad\u7ec3\u7684\u6a21\u578b\u3002", "method": "\u5c06 Bert Base\u3001BioBert\u3001\u4e24\u79cd Clinical Bert \u53d8\u4f53\u3001RoBerta \u548c Clinical Longformer \u5728 CMED \u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5fae\u8c03\uff0c\u4ee5\u6267\u884c\u836f\u7269\u63d0\u53d6\u3001\u533b\u7597\u4e8b\u4ef6\u68c0\u6d4b\u548c\u591a\u7ef4\u5ea6\u836f\u7269\u4e8b\u4ef6\u4e0a\u4e0b\u6587\u5206\u7c7b\u4efb\u52a1\uff0c\u5e76\u8be6\u7ec6\u8bf4\u660e\u4e86 EHR \u7684\u5904\u7406\u65b9\u6cd5\u3002", "result": "\u4e34\u5e8a\u6570\u636e\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u836f\u7269\u548c\u533b\u7597\u4e8b\u4ef6\u68c0\u6d4b\u65b9\u9762\u66f4\u6709\u6548\uff1b\u800c\u5728\u836f\u7269\u4e8b\u4ef6\u4e0a\u4e0b\u6587\u5206\u7c7b\u65b9\u9762\uff0c\u901a\u7528\u6570\u636e\u9884\u8bad\u7ec3\u7684 Bert Base \u6a21\u578b\u6548\u679c\u6700\u597d\u3002", "conclusion": "\u9884\u8bad\u7ec3\u6570\u636e\u7684\u9886\u57df\u5bf9\u4e8e\u6a21\u578b\u5728 EHR \u4fe1\u606f\u63d0\u53d6\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2509.19301", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19301", "abs": "https://arxiv.org/abs/2509.19301", "authors": ["Lars Ankile", "Zhenyu Jiang", "Rocky Duan", "Guanya Shi", "Pieter Abbeel", "Anusha Nagabandi"], "title": "Residual Off-Policy RL for Finetuning Behavior Cloning Policies", "comment": null, "summary": "Recent advances in behavior cloning (BC) have enabled impressive visuomotor\ncontrol policies. However, these approaches are limited by the quality of human\ndemonstrations, the manual effort required for data collection, and the\ndiminishing returns from increasing offline data. In comparison, reinforcement\nlearning (RL) trains an agent through autonomous interaction with the\nenvironment and has shown remarkable success in various domains. Still,\ntraining RL policies directly on real-world robots remains challenging due to\nsample inefficiency, safety concerns, and the difficulty of learning from\nsparse rewards for long-horizon tasks, especially for high-degree-of-freedom\n(DoF) systems. We present a recipe that combines the benefits of BC and RL\nthrough a residual learning framework. Our approach leverages BC policies as\nblack-box bases and learns lightweight per-step residual corrections via\nsample-efficient off-policy RL. We demonstrate that our method requires only\nsparse binary reward signals and can effectively improve manipulation policies\non high-degree-of-freedom (DoF) systems in both simulation and the real world.\nIn particular, we demonstrate, to the best of our knowledge, the first\nsuccessful real-world RL training on a humanoid robot with dexterous hands. Our\nresults demonstrate state-of-the-art performance in various vision-based tasks,\npointing towards a practical pathway for deploying RL in the real world.\nProject website: https://residual-offpolicy-rl.github.io", "AI": {"tldr": "BC and RL can be combined for better robot control, learning from human examples and then refining with reinforcement learning for efficiency and safety, even on complex robots like humanoids.", "motivation": "Traditional behavior cloning (BC) relies on human demonstrations, which are limited by data quality and collection effort. Reinforcement learning (RL) can train agents autonomously but faces challenges in sample efficiency, safety, and sparse rewards, especially for high-degree-of-freedom (DoF) systems in the real world.", "method": "A residual learning framework is proposed that uses BC policies as a base and learns lightweight per-step residual corrections through sample-efficient off-policy RL. This method only requires sparse binary rewards.", "result": "The approach improves manipulation policies on high-DoF systems in simulation and the real world, demonstrating state-of-the-art performance on vision-based tasks. It achieved the first successful real-world RL training on a humanoid robot with dexterous hands.", "conclusion": "The proposed method offers a practical way to deploy RL in the real world by combining the strengths of BC and RL, overcoming limitations of each individually, and achieving strong results on complex robotic manipulation tasks."}}
{"id": "2509.18693", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18693", "abs": "https://arxiv.org/abs/2509.18693", "authors": ["Siyi Chen", "Kai Wang", "Weicong Pang", "Ruiming Yang", "Ziru Chen", "Renjun Gao", "Alexis Kai Hon Lau", "Dasa Gu", "Chenchen Zhang", "Cheng Li"], "title": "OSDA: A Framework for Open-Set Discovery and Automatic Interpretation of Land-cover in Remote Sensing Imagery", "comment": "Project is available at\n  https://anonymous.4open.science/r/openset_remotesensing_tagging-2B5F/README.md", "summary": "Open-set land-cover analysis in remote sensing requires the ability to\nachieve fine-grained spatial localization and semantically open categorization.\nThis involves not only detecting and segmenting novel objects without\ncategorical supervision but also assigning them interpretable semantic labels\nthrough multimodal reasoning. In this study, we introduce OSDA, an integrated\nthree-stage framework for annotation-free open-set land-cover discovery,\nsegmentation, and description. The pipeline consists of: (1) precise discovery\nand mask extraction with a promptable fine-tuned segmentation model (SAM), (2)\nsemantic attribution and contextual description via a two-phase fine-tuned\nmultimodal large language model (MLLM), and (3) LLM-as-judge and manual scoring\nof the MLLMs evaluation. By combining pixel-level accuracy with high-level\nsemantic understanding, OSDA addresses key challenges in open-world remote\nsensing interpretation. Designed to be architecture-agnostic and label-free,\nthe framework supports robust evaluation across diverse satellite imagery\nwithout requiring manual annotation. Our work provides a scalable and\ninterpretable solution for dynamic land-cover monitoring, showing strong\npotential for automated cartographic updating and large-scale earth observation\nanalysis.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86OSDA\u6846\u67b6\uff0c\u7528\u4e8e\u65e0\u6807\u6ce8\u7684\u5f00\u653e\u96c6\u9646\u8868\u5206\u6790\uff0c\u7ed3\u5408\u4e86SAM\u8fdb\u884c\u7cbe\u786e\u5206\u5272\u548cMLLM\u8fdb\u884c\u8bed\u4e49\u63cf\u8ff0\uff0c\u5b9e\u73b0\u4e86\u50cf\u7d20\u7ea7\u7cbe\u5ea6\u548c\u9ad8\u5c42\u8bed\u4e49\u7406\u89e3\u7684\u7ed3\u5408\uff0c\u4e3a\u52a8\u6001\u9646\u8868\u76d1\u6d4b\u548c\u5730\u7403\u89c2\u6d4b\u5206\u6790\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u5f00\u653e\u96c6\u9065\u611f\u9646\u8868\u5206\u6790\u9700\u8981\u7ec6\u7c92\u5ea6\u7684\u7a7a\u95f4\u5b9a\u4f4d\u548c\u5f00\u653e\u7684\u8bed\u4e49\u5206\u7c7b\u80fd\u529b\uff0c\u5305\u62ec\u68c0\u6d4b\u3001\u5206\u5272\u65b0\u7269\u4f53\u5e76\u8d4b\u4e88\u5176\u53ef\u89e3\u91ca\u7684\u8bed\u4e49\u6807\u7b7e\u3002", "method": "\u63d0\u51faOSDA\u4e09\u9636\u6bb5\u6846\u67b6\uff1a(1) \u4f7f\u7528SAM\u8fdb\u884c\u7cbe\u786e\u53d1\u73b0\u548c\u63a9\u7801\u63d0\u53d6\uff1b(2) \u4f7f\u7528\u4e24\u9636\u6bb5\u5fae\u8c03\u7684MLLM\u8fdb\u884c\u8bed\u4e49\u5f52\u56e0\u548c\u4e0a\u4e0b\u6587\u63cf\u8ff0\uff1b(3) \u4f7f\u7528LLM-as-judge\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "OSDA\u7ed3\u5408\u4e86\u50cf\u7d20\u7ea7\u7cbe\u5ea6\u548c\u9ad8\u5c42\u8bed\u4e49\u7406\u89e3\uff0c\u89e3\u51b3\u4e86\u5f00\u653e\u4e16\u754c\u9065\u611f\u89e3\u91ca\u7684\u5173\u952e\u6311\u6218\uff0c\u80fd\u591f\u8fdb\u884c\u65e0\u6807\u6ce8\u7684\u5f00\u653e\u96c6\u9646\u8868\u53d1\u73b0\u3001\u5206\u5272\u548c\u63cf\u8ff0\u3002", "conclusion": "OSDA\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65e0\u9700\u624b\u52a8\u6807\u6ce8\u3001\u652f\u6301\u8de8\u591a\u79cd\u536b\u661f\u56fe\u50cf\u7684\u9c81\u68d2\u8bc4\u4f30\u7684\u6846\u67b6\uff0c\u4e3a\u52a8\u6001\u9646\u8868\u76d1\u6d4b\u3001\u81ea\u52a8\u5316\u5730\u56fe\u66f4\u65b0\u548c\u5927\u89c4\u6a21\u5730\u7403\u89c2\u6d4b\u5206\u6790\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.19228", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.19228", "abs": "https://arxiv.org/abs/2509.19228", "authors": ["Gabriele Berton", "Jayakrishnan Unnikrishnan", "Son Tran", "Mubarak Shah"], "title": "CompLLM: Compression for Long Context Q&A", "comment": null, "summary": "Large Language Models (LLMs) face significant computational challenges when\nprocessing long contexts due to the quadratic complexity of self-attention.\nWhile soft context compression methods, which map input text to smaller latent\nrepresentations, have shown promise, their real-world adoption is limited.\nExisting techniques typically compress the context as a single unit, which\nleads to quadratic compression complexity and an inability to reuse\ncomputations across queries with overlapping contexts. In this work, we\nintroduce CompLLM, a soft compression technique designed for practical\ndeployment. Instead of processing the context holistically, CompLLM divides it\ninto segments and compresses each one independently. This simple design choice\nyields three critical properties: efficiency, as the compression step scales\nlinearly with the context length; scalability, enabling models trained on short\nsequences (e.g., 1k tokens) to generalize to contexts of 100k tokens; and\nreusability, allowing compressed segments to be cached and reused across\ndifferent queries. Our experiments show that with a 2x compression rate, at\nhigh context lengths CompLLM speeds up Time To First Token (TTFT) by up to 4x\nand reduces the KV cache size by 50%. Furthermore, CompLLM achieves performance\ncomparable to that obtained with the uncompressed context, and even surpasses\nit on very long sequences, demonstrating its effectiveness and practical\nutility.", "AI": {"tldr": "LLMs\u5904\u7406\u957f\u6587\u672c\u65f6\u9762\u4e34\u4e8c\u6b21\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u95ee\u9898\u3002\u672c\u6587\u63d0\u51fa\u7684CompLLM\u901a\u8fc7\u5c06\u957f\u6587\u672c\u5206\u6bb5\u538b\u7f29\uff0c\u5b9e\u73b0\u4e86\u7ebf\u6027\u590d\u6742\u5ea6\u3001\u53ef\u6269\u5c55\u6027\u548c\u53ef\u91cd\u7528\u6027\uff0c\u663e\u8457\u63d0\u9ad8\u4e86TTFT\u901f\u5ea6\u5e76\u51cf\u5c0f\u4e86KV\u7f13\u5b58\u5927\u5c0f\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e0e\u672a\u538b\u7f29\u4e0a\u4e0b\u6587\u76f8\u5f53\u751a\u81f3\u66f4\u597d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u8f6f\u4e0a\u4e0b\u6587\u538b\u7f29\u65b9\u6cd5\u5b58\u5728\u4e8c\u6b21\u538b\u7f29\u590d\u6742\u5ea6\u3001\u65e0\u6cd5\u8de8\u67e5\u8be2\u91cd\u7528\u8ba1\u7b97\u7b49\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002", "method": "CompLLM\u5c06\u957f\u4e0a\u4e0b\u6587\u5212\u5206\u4e3a\u72ec\u7acb\u7684\u7247\u6bb5\u8fdb\u884c\u538b\u7f29\uff0c\u5b9e\u73b0\u4e86\u7ebf\u6027\u6269\u5c55\u3001\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u548c\u538b\u7f29\u7247\u6bb5\u7684\u7f13\u5b58\u91cd\u7528\u3002", "result": "\u57282\u500d\u538b\u7f29\u7387\u4e0b\uff0cCompLLM\u5728\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u4e0b\u5c06TTFT\u901f\u5ea6\u63d0\u5347\u9ad8\u8fbe4\u500d\uff0cKV\u7f13\u5b58\u5927\u5c0f\u51cf\u5c1150%\uff0c\u5e76\u4e14\u5728\u957f\u5e8f\u5217\u4e0a\u6027\u80fd\u8868\u73b0\u4f18\u4e8e\u672a\u538b\u7f29\u4e0a\u4e0b\u6587\u3002", "conclusion": "CompLLM\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u4e14\u53ef\u91cd\u7528\u7684\u8f6f\u4e0a\u4e0b\u6587\u538b\u7f29\u6280\u672f\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3LLMs\u5904\u7406\u957f\u6587\u672c\u7684\u8ba1\u7b97\u6311\u6218\uff0c\u5e76\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.18697", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18697", "abs": "https://arxiv.org/abs/2509.18697", "authors": ["Herve Goeau", "Pierre Bonnet", "Alexis Joly"], "title": "Overview of PlantCLEF 2021: cross-domain plant identification", "comment": "15 pages, 6 figures, CLEF 2021 Conference and Labs of the Evaluation\n  Forum, September 21 to 24, 2021, Bucharest, Romania", "summary": "Automated plant identification has improved considerably thanks to recent\nadvances in deep learning and the availability of training data with more and\nmore field photos. However, this profusion of data concerns only a few tens of\nthousands of species, mainly located in North America and Western Europe, much\nless in the richest regions in terms of biodiversity such as tropical\ncountries. On the other hand, for several centuries, botanists have\nsystematically collected, catalogued and stored plant specimens in herbaria,\nespecially in tropical regions, and recent efforts by the biodiversity\ninformatics community have made it possible to put millions of digitised\nrecords online. The LifeCLEF 2021 plant identification challenge (or \"PlantCLEF\n2021\") was designed to assess the extent to which automated identification of\nflora in data-poor regions can be improved by using herbarium collections. It\nis based on a dataset of about 1,000 species mainly focused on the Guiana\nShield of South America, a region known to have one of the highest plant\ndiversities in the world. The challenge was evaluated as a cross-domain\nclassification task where the training set consisted of several hundred\nthousand herbarium sheets and a few thousand photos to allow learning a\ncorrespondence between the two domains. In addition to the usual metadata\n(location, date, author, taxonomy), the training data also includes the values\nof 5 morphological and functional traits for each species. The test set\nconsisted exclusively of photos taken in the field. This article presents the\nresources and evaluations of the assessment carried out, summarises the\napproaches and systems used by the participating research groups and provides\nan analysis of the main results.", "AI": {"tldr": "\u5c3d\u7ba1\u6df1\u5ea6\u5b66\u4e60\u548c\u53ef\u7528\u6570\u636e\u6709\u6240\u6539\u8fdb\uff0c\u4f46\u690d\u7269\u8bc6\u522b\u4ecd\u9762\u4e34\u6570\u636e\u7a00\u758f\u533a\u57df\uff08\u5982\u70ed\u5e26\u5730\u533a\uff09\u7684\u6311\u6218\u3002LifeCLEF 2021 \u690d\u7269\u8bc6\u522b\u6311\u6218\u8d5b\u65e8\u5728\u5229\u7528\u6807\u672c\u9986\u85cf\u54c1\uff08\u4e3b\u8981\u6765\u81ea\u51e0\u5185\u4e9a\u6e7e\u5730\u533a\uff09\u6765\u6539\u5584\u6570\u636e\u7a00\u758f\u533a\u57df\u7684\u690d\u7269\u81ea\u52a8\u8bc6\u522b\u3002\u8be5\u6bd4\u8d5b\u4f7f\u7528\u5305\u542b\u6807\u672c\u7167\u7247\u548c\u5c11\u91cf\u91ce\u5916\u7167\u7247\u7684\u8bad\u7ec3\u96c6\uff0c\u4ee5\u53ca\u4ec5\u5305\u542b\u91ce\u5916\u7167\u7247\u7684\u6d4b\u8bd5\u96c6\uff0c\u4ee5\u89e3\u51b3\u8de8\u57df\u5206\u7c7b\u95ee\u9898\u3002", "motivation": "\u5229\u7528\u6807\u672c\u9986\u85cf\u54c1\u6765\u6539\u5584\u6570\u636e\u7a00\u758f\u533a\u57df\uff08\u5982\u70ed\u5e26\u5730\u533a\uff09\u7684\u690d\u7269\u81ea\u52a8\u8bc6\u522b\uff0c\u89e3\u51b3\u73b0\u6709\u6a21\u578b\u4e3b\u8981\u5173\u6ce8\u5317\u7f8e\u548c\u897f\u6b27\u800c\u5ffd\u89c6\u751f\u7269\u591a\u6837\u6027\u4e30\u5bcc\u7684\u5730\u533a\u7684\u95ee\u9898\u3002", "method": "\u5c06\u51e0\u79cd\u65b9\u6cd5\u7684\u7ec4\u5408\u5e94\u7528\u4e8e\u6311\u6218\u8d5b\u7684\u8bad\u7ec3\u96c6\uff0c\u8bad\u7ec3\u96c6\u5305\u542b\u6570\u4e07\u5f20\u6807\u672c\u7167\u7247\u548c\u51e0\u5343\u5f20\u91ce\u5916\u7167\u7247\uff0c\u4ee5\u5b66\u4e60\u4e24\u4e2a\u57df\u4e4b\u95f4\u7684\u5bf9\u5e94\u5173\u7cfb\u3002\u6a21\u578b\u8fd8\u4f1a\u5229\u7528\u6bcf\u4e2a\u7269\u79cd\u7684 5 \u79cd\u5f62\u6001\u548c\u529f\u80fd\u6027\u72b6\u503c\u4ee5\u53ca\u5730\u7406\u4f4d\u7f6e\u3001\u65e5\u671f\u3001\u4f5c\u8005\u548c\u5206\u7c7b\u5355\u5143\u7b49\u5143\u6570\u636e\u3002", "result": "\u8bc4\u4f30\u4e86\u53c2\u4e0e\u7814\u7a76\u5c0f\u7ec4\u7684\u65b9\u6cd5\u548c\u7cfb\u7edf\uff0c\u5e76\u5206\u6790\u4e86\u4e3b\u8981\u7ed3\u679c\u3002", "conclusion": "LifeCLEF 2021 \u690d\u7269\u8bc6\u522b\u6311\u6218\u8d5b\u6210\u529f\u8bc4\u4f30\u4e86\u5229\u7528\u6807\u672c\u9986\u85cf\u54c1\u6539\u5584\u70ed\u5e26\u5730\u533a\u690d\u7269\u81ea\u52a8\u8bc6\u522b\u7684\u53ef\u80fd\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u5404\u79cd\u65b9\u6cd5\u7684\u6027\u80fd\u3002"}}
{"id": "2509.18386", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18386", "abs": "https://arxiv.org/abs/2509.18386", "authors": ["Jonathan Kabala Mbuya", "Dieter Pfoser", "Antonios Anastasopoulos"], "title": "Graph Enhanced Trajectory Anomaly Detection", "comment": null, "summary": "Trajectory anomaly detection is essential for identifying unusual and\nunexpected movement patterns in applications ranging from intelligent\ntransportation systems to urban safety and fraud prevention.\n  Existing methods only consider limited aspects of the trajectory nature and\nits movement space by treating trajectories as sequences of sampled locations,\nwith sampling determined by positioning technology, e.g., GPS, or by high-level\nabstractions such as staypoints. Trajectories are analyzed in Euclidean space,\nneglecting the constraints and connectivity information of the underlying\nmovement network, e.g., road or transit networks.\n  The proposed Graph Enhanced Trajectory Anomaly Detection (GETAD) framework\ntightly integrates road network topology, segment semantics, and historical\ntravel patterns to model trajectory data. GETAD uses a Graph Attention Network\nto learn road-aware embeddings that capture both physical attributes and\ntransition behavior, and augments these with graph-based positional encodings\nthat reflect the spatial layout of the road network.\n  A Transformer-based decoder models sequential movement, while a\nmultiobjective loss function combining autoregressive prediction and supervised\nlink prediction ensures realistic and structurally coherent representations.\n  To improve the robustness of anomaly detection, we introduce Confidence\nWeighted Negative Log Likelihood (CW NLL), an anomaly scoring function that\nemphasizes high-confidence deviations.\n  Experiments on real-world and synthetic datasets demonstrate that GETAD\nachieves consistent improvements over existing methods, particularly in\ndetecting subtle anomalies in road-constrained environments. These results\nhighlight the benefits of incorporating graph structure and contextual\nsemantics into trajectory modeling, enabling more precise and context-aware\nanomaly detection.", "AI": {"tldr": "GETAD\u901a\u8fc7\u6574\u5408\u8def\u7f51\u62d3\u6251\u3001\u8def\u6bb5\u8bed\u4e49\u548c\u5386\u53f2\u51fa\u884c\u6a21\u5f0f\uff0c\u5229\u7528\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\u548cTransformer\u6a21\u578b\u8fdb\u884c\u8f68\u8ff9\u5f02\u5e38\u68c0\u6d4b\uff0c\u5e76\u5f15\u5165CWNLL\u63d0\u9ad8\u9c81\u68d2\u6027\uff0c\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u8f68\u8ff9\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u4ec5\u8003\u8651\u8f68\u8ff9\u7684\u6709\u9650\u65b9\u9762\uff0c\u5ffd\u7565\u4e86\u8def\u7f51\u7b49\u5e95\u5c42\u7f51\u7edc\u7ea6\u675f\u548c\u8fde\u901a\u6027\u4fe1\u606f\u3002", "method": "GETAD\u6846\u67b6\u6574\u5408\u4e86\u8def\u7f51\u62d3\u6251\u3001\u8def\u6bb5\u8bed\u4e49\u548c\u5386\u53f2\u51fa\u884c\u6a21\u5f0f\u3002\u5b83\u4f7f\u7528\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\u5b66\u4e60\u8def\u7f51\u611f\u77e5\u7684\u5d4c\u5165\uff0c\u5e76\u7ed3\u5408\u57fa\u4e8e\u56fe\u7684\u4f4d\u7f6e\u7f16\u7801\u3002Transformer\u6a21\u578b\u7528\u4e8e\u6a21\u62df\u5e8f\u5217\u8fd0\u52a8\uff0c\u7ed3\u5408\u81ea\u56de\u5f52\u9884\u6d4b\u548c\u76d1\u7763\u94fe\u63a5\u9884\u6d4b\u7684\u591a\u76ee\u6807\u635f\u5931\u51fd\u6570\u3002\u5f15\u5165\u4e86\u7f6e\u4fe1\u52a0\u6743\u8d1f\u5bf9\u6570\u4f3c\u7136\uff08CWNLL\uff09\u4f5c\u4e3a\u5f02\u5e38\u8bc4\u5206\u51fd\u6570\u3002", "result": "GETAD\u5728\u771f\u5b9e\u548c\u5408\u6210\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6bd4\u73b0\u6709\u65b9\u6cd5\u4e00\u81f4\u7684\u6539\u8fdb\uff0c\u5c24\u5176\u5728\u68c0\u6d4b\u9053\u8def\u7ea6\u675f\u73af\u5883\u4e2d\u7684\u7ec6\u5fae\u5f02\u5e38\u65b9\u9762\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u5c06\u56fe\u7ed3\u6784\u548c\u4e0a\u4e0b\u6587\u8bed\u4e49\u6574\u5408\u5230\u8f68\u8ff9\u5efa\u6a21\u4e2d\u6709\u52a9\u4e8e\u5b9e\u73b0\u66f4\u7cbe\u786e\u3001\u66f4\u5177\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u5f02\u5e38\u68c0\u6d4b\u3002"}}
{"id": "2509.19249", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19249", "abs": "https://arxiv.org/abs/2509.19249", "authors": ["Siheng Li", "Kejiao Li", "Zenan Xu", "Guanhua Huang", "Evander Yang", "Kun Li", "Haoyuan Wu", "Jiajia Wu", "Zihao Zheng", "Chenchen Zhang", "Kun Shi", "Kyrierl Deng", "Qi Yi", "Ruibin Xiong", "Tingqiang Xu", "Yuhao Jiang", "Jianfeng Yan", "Yuyuan Zeng", "Guanghui Xu", "Jinbao Xue", "Zhijiang Xu", "Zheng Fang", "Shuai Li", "Qibin Liu", "Xiaoxue Li", "Zhuoyu Li", "Yangyu Tao", "Fei Gao", "Cheng Jiang", "Bo Chao Wang", "Kai Liu", "Jianchen Zhu", "Wai Lam", "Wayyt Wang", "Bo Zhou", "Di Wang"], "title": "Reinforcement Learning on Pre-Training Data", "comment": "Work in progress", "summary": "The growing disparity between the exponential scaling of computational\nresources and the finite growth of high-quality text data now constrains\nconventional scaling approaches for large language models (LLMs). To address\nthis challenge, we introduce Reinforcement Learning on Pre-Training data\n(RLPT), a new training-time scaling paradigm for optimizing LLMs. In contrast\nto prior approaches that scale training primarily through supervised learning,\nRLPT enables the policy to autonomously explore meaningful trajectories to\nlearn from pre-training data and improve its capability through reinforcement\nlearning (RL). While existing RL strategies such as reinforcement learning from\nhuman feedback (RLHF) and reinforcement learning with verifiable rewards (RLVR)\nrely on human annotation for reward construction, RLPT eliminates this\ndependency by deriving reward signals directly from pre-training data.\nSpecifically, it adopts a next-segment reasoning objective, rewarding the\npolicy for accurately predicting subsequent text segments conditioned on the\npreceding context. This formulation allows RL to be scaled on pre-training\ndata, encouraging the exploration of richer trajectories across broader\ncontexts and thereby fostering more generalizable reasoning skills. Extensive\nexperiments on both general-domain and mathematical reasoning benchmarks across\nmultiple models validate the effectiveness of RLPT. For example, when applied\nto Qwen3-4B-Base, RLPT yields absolute improvements of $3.0$, $5.1$, $8.1$,\n$6.0$, $6.6$, and $5.3$ on MMLU, MMLU-Pro, GPQA-Diamond, KOR-Bench, AIME24, and\nAIME25, respectively. The results further demonstrate favorable scaling\nbehavior, suggesting strong potential for continued gains with more compute. In\naddition, RLPT provides a solid foundation, extending the reasoning boundaries\nof LLMs and enhancing RLVR performance.", "AI": {"tldr": "RLPT\u662f\u4e00\u79cd\u65b0\u7684LLM\u8bad\u7ec3\u8303\u5f0f\uff0c\u5b83\u901a\u8fc7\u5728\u9884\u8bad\u7ec3\u6570\u636e\u4e0a\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u6765\u4f18\u5316LLM\uff0c\u4ece\u800c\u514b\u670d\u4e86\u8ba1\u7b97\u8d44\u6e90\u548c\u9ad8\u8d28\u91cf\u6587\u672c\u6570\u636e\u589e\u957f\u4e0d\u5339\u914d\u7684\u95ee\u9898\u3002\u4e0e\u4f9d\u8d56\u4eba\u7c7b\u53cd\u9988\u7684RLHF\u548cRLVR\u4e0d\u540c\uff0cRLPT\u76f4\u63a5\u4ece\u9884\u8bad\u7ec3\u6570\u636e\u4e2d\u63d0\u53d6\u5956\u52b1\u4fe1\u53f7\uff0c\u5e76\u901a\u8fc7\u4e0b\u4e00\u6bb5\u9884\u6d4b\u4efb\u52a1\u6765\u8bad\u7ec3\u6a21\u578b\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u63a8\u7406\u80fd\u529b\u3002\u5b9e\u9a8c\u8bc1\u660eRLPT\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6709\u6548\uff0c\u5e76\u5177\u6709\u826f\u597d\u7684\u6269\u5c55\u6027\u3002", "motivation": "\u8ba1\u7b97\u8d44\u6e90\u5448\u6307\u6570\u7ea7\u589e\u957f\uff0c\u800c\u9ad8\u8d28\u91cf\u6587\u672c\u6570\u636e\u589e\u957f\u6709\u9650\uff0c\u8fd9\u9650\u5236\u4e86\u4f20\u7edfLLM\u7684\u6269\u5c55\u65b9\u6cd5\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u4f18\u5316LLM\u7684\u8bad\u7ec3\u3002", "method": "RLPT\uff08Reinforcement Learning on Pre-Training data\uff09\u662f\u4e00\u79cd\u65b0\u7684\u8bad\u7ec3\u8303\u5f0f\uff0c\u5b83\u5728\u9884\u8bad\u7ec3\u6570\u636e\u4e0a\u5e94\u7528\u5f3a\u5316\u5b66\u4e60\u3002\u901a\u8fc7\u91c7\u7528\u2018\u4e0b\u4e00\u6bb5\u9884\u6d4b\u2019\uff08next-segment prediction\uff09\u7684\u76ee\u6807\uff0c\u6a21\u578b\u53ef\u4ee5\u6839\u636e\u524d\u9762\u7684\u4e0a\u4e0b\u6587\u51c6\u786e\u5730\u9884\u6d4b\u540e\u7eed\u6587\u672c\u7247\u6bb5\uff0c\u5e76\u4ece\u4e2d\u83b7\u5f97\u5956\u52b1\u4fe1\u53f7\u3002\u8fd9\u79cd\u65b9\u6cd5\u4f7f\u5f3a\u5316\u5b66\u4e60\u80fd\u591f\u6269\u5c55\u5230\u9884\u8bad\u7ec3\u6570\u636e\u4e0a\uff0c\u9f13\u52b1\u6a21\u578b\u63a2\u7d22\u66f4\u4e30\u5bcc\u7684\u8f68\u8ff9\u548c\u66f4\u5e7f\u6cdb\u7684\u4e0a\u4e0b\u6587\uff0c\u4ece\u800c\u57f9\u517b\u66f4\u5f3a\u7684\u6cdb\u5316\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5728\u901a\u7528\u9886\u57df\u548c\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e86RLPT\u7684\u6709\u6548\u6027\u3002\u4f8b\u5982\uff0c\u5c06RLPT\u5e94\u7528\u4e8eQwen3-4B-Base\u6a21\u578b\u540e\uff0c\u5728MMLU\u3001MMLU-Pro\u3001GPQA-Diamond\u3001KOR-Bench\u3001AIME24\u548cAIME25\u4e0a\u7684\u63d0\u5347\u5206\u522b\u4e3a3.0\u30015.1\u30018.1\u30016.0\u30016.6\u548c5.3\u3002\u8fd9\u4e9b\u7ed3\u679c\u8868\u660eRLPT\u5177\u6709\u826f\u597d\u7684\u6269\u5c55\u6027\uff0c\u5e76\u6709\u8fdb\u4e00\u6b65\u63d0\u5347\u7684\u6f5c\u529b\u3002", "conclusion": "RLPT\u662f\u4e00\u79cd\u6709\u6548\u7684LLM\u8bad\u7ec3\u8303\u5f0f\uff0c\u5b83\u901a\u8fc7\u5728\u9884\u8bad\u7ec3\u6570\u636e\u4e0a\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u6765\u4f18\u5316\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u8ba1\u7b97\u8d44\u6e90\u4e0e\u6570\u636e\u589e\u957f\u4e0d\u5339\u914d\u7684\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u4e0b\u4e00\u6bb5\u9884\u6d4b\u4efb\u52a1\u76f4\u63a5\u4ece\u9884\u8bad\u7ec3\u6570\u636e\u4e2d\u63d0\u53d6\u5956\u52b1\u4fe1\u53f7\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u4e3aLLM\u7684\u63a8\u7406\u8fb9\u754c\u548cRLVR\u6027\u80fd\u7684\u63d0\u5347\u5960\u5b9a\u4e86\u57fa\u7840\u3002\u5176\u826f\u597d\u7684\u6269\u5c55\u6027\u9884\u793a\u7740\u672a\u6765\u8ba1\u7b97\u80fd\u529b\u7684\u63d0\u5347\u5c06\u5e26\u6765\u66f4\u5927\u7684\u6027\u80fd\u589e\u76ca\u3002"}}
{"id": "2509.18699", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18699", "abs": "https://arxiv.org/abs/2509.18699", "authors": ["Zedong Zhang", "Ying Tai", "Jianjun Qian", "Jian Yang", "Jun Li"], "title": "AGSwap: Overcoming Category Boundaries in Object Fusion via Adaptive Group Swapping", "comment": null, "summary": "Fusing cross-category objects to a single coherent object has gained\nincreasing attention in text-to-image (T2I) generation due to its broad\napplications in virtual reality, digital media, film, and gaming. However,\nexisting methods often produce biased, visually chaotic, or semantically\ninconsistent results due to overlapping artifacts and poor integration.\nMoreover, progress in this field has been limited by the absence of a\ncomprehensive benchmark dataset. To address these problems, we propose\n\\textbf{Adaptive Group Swapping (AGSwap)}, a simple yet highly effective\napproach comprising two key components: (1) Group-wise Embedding Swapping,\nwhich fuses semantic attributes from different concepts through feature\nmanipulation, and (2) Adaptive Group Updating, a dynamic optimization mechanism\nguided by a balance evaluation score to ensure coherent synthesis.\nAdditionally, we introduce \\textbf{Cross-category Object Fusion (COF)}, a\nlarge-scale, hierarchically structured dataset built upon ImageNet-1K and\nWordNet. COF includes 95 superclasses, each with 10 subclasses, enabling\n451,250 unique fusion pairs. Extensive experiments demonstrate that AGSwap\noutperforms state-of-the-art compositional T2I methods, including GPT-Image-1\nusing simple and complex prompts.", "AI": {"tldr": "AGSwap\u662f\u4e00\u79cd\u7528\u4e8e\u8de8\u7c7b\u522b\u5bf9\u8c61\u878d\u5408\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u7ec4\u5d4c\u5165\u4ea4\u6362\u548c\u81ea\u9002\u5e94\u5206\u7ec4\u66f4\u65b0\u5b9e\u73b0\uff0c\u5e76\u5f15\u5165\u4e86COF\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\u5728\u878d\u5408\u8de8\u7c7b\u522b\u5bf9\u8c61\u65f6\u5b58\u5728\u504f\u89c1\u3001\u6df7\u4e71\u6216\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u4e14\u7f3a\u4e4f\u7efc\u5408\u6027\u57fa\u51c6\u6570\u636e\u96c6\u3002", "method": "\u63d0\u51faAGSwap\u65b9\u6cd5\uff0c\u5305\u542b\u5206\u7ec4\u5d4c\u5165\u4ea4\u6362\uff08\u901a\u8fc7\u7279\u5f81\u64cd\u4f5c\u878d\u5408\u4e0d\u540c\u6982\u5ff5\u7684\u8bed\u4e49\u5c5e\u6027\uff09\u548c\u81ea\u9002\u5e94\u5206\u7ec4\u66f4\u65b0\uff08\u901a\u8fc7\u5e73\u8861\u8bc4\u4f30\u5206\u6570\u52a8\u6001\u4f18\u5316\u4ee5\u4fdd\u8bc1\u5408\u6210\u4e00\u81f4\u6027\uff09\u3002\u540c\u65f6\uff0c\u5f15\u5165COF\u6570\u636e\u96c6\uff08\u57fa\u4e8eImageNet-1K\u548cWordNet\u6784\u5efa\uff0c\u5305\u542b95\u4e2a\u8d85\u7c7b\u548c10\u4e2a\u5b50\u7c7b\uff0c\u63d0\u4f9b451,250\u4e2a\u878d\u5408\u5bf9\uff09\u3002", "result": "AGSwap\u5728\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u7684\u7ec4\u5408\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\uff0c\u5305\u62ec\u4f7f\u7528\u7b80\u5355\u548c\u590d\u6742\u63d0\u793a\u7684GPT-Image-1\u3002", "conclusion": "AGSwap\u5728\u8de8\u7c7b\u522b\u5bf9\u8c61\u878d\u5408\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6210\u529f\uff0c\u5e76\u4e14\u5f15\u5165\u7684COF\u6570\u636e\u96c6\u4e3a\u8be5\u9886\u57df\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\u3002"}}
{"id": "2509.18389", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18389", "abs": "https://arxiv.org/abs/2509.18389", "authors": ["Jiuqi Wang", "Rohan Chandra", "Shangtong Zhang"], "title": "Towards Provable Emergence of In-Context Reinforcement Learning", "comment": "NeurIPS 2025, 28 pages", "summary": "Typically, a modern reinforcement learning (RL) agent solves a task by\nupdating its neural network parameters to adapt its policy to the task.\nRecently, it has been observed that some RL agents can solve a wide range of\nnew out-of-distribution tasks without parameter updates after pretraining on\nsome task distribution. When evaluated in a new task, instead of making\nparameter updates, the pretrained agent conditions its policy on additional\ninput called the context, e.g., the agent's interaction history in the new\ntask. The agent's performance increases as the information in the context\nincreases, with the agent's parameters fixed. This phenomenon is typically\ncalled in-context RL (ICRL). The pretrained parameters of the agent network\nenable the remarkable ICRL phenomenon. However, many ICRL works perform the\npretraining with standard RL algorithms. This raises the central question this\npaper aims to address: Why can the RL pretraining algorithm generate network\nparameters that enable ICRL? We hypothesize that the parameters capable of ICRL\nare minimizers of the pretraining loss. This work provides initial support for\nthis hypothesis through a case study. In particular, we prove that when a\nTransformer is pretrained for policy evaluation, one of the global minimizers\nof the pretraining loss can enable in-context temporal difference learning.", "AI": {"tldr": "RL\u9884\u8bad\u7ec3\u53c2\u6570\u662fICRL\u73b0\u8c61\u7684\u5173\u952e\uff0c\u672c\u6587\u65e8\u5728\u63a2\u8ba8\u4e3a\u4f55RL\u9884\u8bad\u7ec3\u80fd\u4ea7\u751f\u652f\u6301ICRL\u7684\u53c2\u6570\uff0c\u5e76\u63d0\u51fa\u53c2\u6570\u662f\u9884\u8bad\u7ec3\u635f\u5931\u7684\u5168\u5c40\u6700\u5c0f\u5316\u5668\u7684\u5047\u8bbe\uff0c\u901a\u8fc7Transformer\u7684\u6848\u4f8b\u7814\u7a76\u8fdb\u884c\u521d\u6b65\u9a8c\u8bc1\u3002", "motivation": "\u8bb8\u591aICRL\u7814\u7a76\u4f7f\u7528\u6807\u51c6\u7684RL\u7b97\u6cd5\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u4f46\u5e76\u672a\u89e3\u91ca\u4e3a\u4f55RL\u9884\u8bad\u7ec3\u80fd\u4ea7\u751f\u652f\u6301ICRL\u7684\u53c2\u6570\uff0c\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u6838\u5fc3\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u4e00\u4e2a\u6848\u4f8b\u7814\u7a76\uff0c\u7279\u522b\u662f\u8bc1\u660e\u5f53Transformer\u4e3a\u7b56\u7565\u8bc4\u4f30\u8fdb\u884c\u9884\u8bad\u7ec3\u65f6\uff0c\u9884\u8bad\u7ec3\u635f\u5931\u7684\u5168\u5c40\u6700\u5c0f\u5316\u5668\u4e4b\u4e00\u80fd\u591f\u5b9e\u73b0\u4e0a\u4e0b\u6587\u4e2d\u7684\u65f6\u5e8f\u5dee\u5206\u5b66\u4e60\u3002", "result": "\u8bc1\u660e\u4e86\u5f53Transformer\u4e3a\u7b56\u7565\u8bc4\u4f30\u8fdb\u884c\u9884\u8bad\u7ec3\u65f6\uff0c\u9884\u8bad\u7ec3\u635f\u5931\u7684\u5168\u5c40\u6700\u5c0f\u5316\u5668\u4e4b\u4e00\u80fd\u591f\u5b9e\u73b0\u4e0a\u4e0b\u6587\u4e2d\u7684\u65f6\u5e8f\u5dee\u5206\u5b66\u4e60\u3002", "conclusion": "RL\u9884\u8bad\u7ec3\u53c2\u6570\u4e4b\u6240\u4ee5\u80fd\u652f\u6301ICRL\u73b0\u8c61\uff0c\u662f\u56e0\u4e3a\u8fd9\u4e9b\u53c2\u6570\u662f\u9884\u8bad\u7ec3\u635f\u5931\u7684\u5168\u5c40\u6700\u5c0f\u5316\u5668\u3002"}}
{"id": "2509.19269", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.19269", "abs": "https://arxiv.org/abs/2509.19269", "authors": ["Nitesh Kumar", "Usashi Chatterjee", "Steven Schockaert"], "title": "Extracting Conceptual Spaces from LLMs Using Prototype Embeddings", "comment": null, "summary": "Conceptual spaces represent entities and concepts using cognitively\nmeaningful dimensions, typically referring to perceptual features. Such\nrepresentations are widely used in cognitive science and have the potential to\nserve as a cornerstone for explainable AI. Unfortunately, they have proven\nnotoriously difficult to learn, although recent LLMs appear to capture the\nrequired perceptual features to a remarkable extent. Nonetheless, practical\nmethods for extracting the corresponding conceptual spaces are currently still\nlacking. While various methods exist for extracting embeddings from LLMs,\nextracting conceptual spaces also requires us to encode the underlying\nfeatures. In this paper, we propose a strategy in which features (e.g.\nsweetness) are encoded by embedding the description of a corresponding\nprototype (e.g. a very sweet food). To improve this strategy, we fine-tune the\nLLM to align the prototype embeddings with the corresponding conceptual space\ndimensions. Our empirical analysis finds this approach to be highly effective.", "AI": {"tldr": "LLMs\u53ef\u4ee5\u5b66\u4e60\u6982\u5ff5\u7a7a\u95f4\uff0c\u4f46\u9700\u8981\u4e13\u95e8\u7684\u65b9\u6cd5\u6765\u63d0\u53d6\u5b83\u4eec\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5d4c\u5165\u539f\u578b\u63cf\u8ff0\u5e76\u5fae\u8c03LLM\u6765\u63d0\u53d6\u6982\u5ff5\u7a7a\u95f4\u7684\u65b9\u6cd5\u3002", "motivation": "\u6982\u5ff5\u7a7a\u95f4\u5728\u8ba4\u77e5\u79d1\u5b66\u4e2d\u5f88\u6709\u7528\uff0c\u5e76\u4e14\u6709\u6f5c\u529b\u7528\u4e8e\u53ef\u89e3\u91ca\u7684AI\uff0c\u4f46\u5f88\u96be\u5b66\u4e60\u3002\u867d\u7136LLM\u53ef\u4ee5\u6355\u83b7\u5fc5\u8981\u7684\u611f\u77e5\u7279\u5f81\uff0c\u4f46\u7f3a\u4e4f\u63d0\u53d6\u6982\u5ff5\u7a7a\u95f4\u7684\u5b9e\u7528\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5d4c\u5165\u539f\u578b\uff08\u5982\u201c\u975e\u5e38\u751c\u7684\u98df\u7269\u201d\uff09\u7684\u63cf\u8ff0\u6765\u7f16\u7801\u7279\u5f81\uff08\u5982\u201c\u751c\u5ea6\u201d\uff09\u7684\u7b56\u7565\uff0c\u5e76\u901a\u8fc7\u5fae\u8c03LLM\u6765\u4f7f\u539f\u578b\u5d4c\u5165\u4e0e\u6982\u5ff5\u7a7a\u95f4\u7ef4\u5ea6\u5bf9\u9f50\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u7ecf\u9a8c\u4e0a\u88ab\u8bc1\u660e\u662f\u975e\u5e38\u6709\u6548\u7684\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u5730\u4eceLLM\u4e2d\u63d0\u53d6\u6982\u5ff5\u7a7a\u95f4\uff0c\u5e76\u6709\u671b\u5728\u53ef\u89e3\u91ca\u7684AI\u4e2d\u53d1\u6325\u4f5c\u7528\u3002"}}
{"id": "2509.18705", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18705", "abs": "https://arxiv.org/abs/2509.18705", "authors": ["Herve Goeau", "Pierre Bonnet", "Alexis Joly"], "title": "Overview of LifeCLEF Plant Identification task 2019: diving into data deficient tropical countries", "comment": "13 pages, 5 figures, CLEF 2019 Conference and Labs of the Evaluation\n  Forum, September 09 to 12, 2019, Lugano, Switzerland", "summary": "Automated identification of plants has improved considerably thanks to the\nrecent progress in deep learning and the availability of training data.\nHowever, this profusion of data only concerns a few tens of thousands of\nspecies, while the planet has nearly 369K. The LifeCLEF 2019 Plant\nIdentification challenge (or \"PlantCLEF 2019\") was designed to evaluate\nautomated identification on the flora of data deficient regions. It is based on\na dataset of 10K species mainly focused on the Guiana shield and the Northern\nAmazon rainforest, an area known to have one of the greatest diversity of\nplants and animals in the world. As in the previous edition, a comparison of\nthe performance of the systems evaluated with the best tropical flora experts\nwas carried out. This paper presents the resources and assessments of the\nchallenge, summarizes the approaches and systems employed by the participating\nresearch groups, and provides an analysis of the main outcomes.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86LifeCLEF 2019\u690d\u7269\u8bc6\u522b\u6311\u6218\u8d5b\uff0c\u8be5\u6bd4\u8d5b\u65e8\u5728\u8bc4\u4f30\u5728\u6570\u636e\u532e\u4e4f\u5730\u533a\u5bf9\u690d\u7269\u8fdb\u884c\u81ea\u52a8\u8bc6\u522b\u7684\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u690d\u7269\u81ea\u52a8\u8bc6\u522b\u6280\u672f\u867d\u7136\u5728\u6df1\u5ea6\u5b66\u4e60\u548c\u6570\u636e\u53ef\u7528\u6027\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u8bad\u7ec3\u6570\u636e\u4ec5\u6db5\u76d6\u4e86\u5c11\u91cf\u7269\u79cd\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5168\u7403\u8fd136.9\u4e07\u7269\u79cd\u7684\u8bc6\u522b\u9700\u6c42\u3002\u56e0\u6b64\uff0c\u672c\u6b21\u6311\u6218\u8d5b\u5173\u6ce8\u6570\u636e\u532e\u4e4f\u5730\u533a\uff08\u5982\u572d\u4e9a\u90a3\u5730\u76fe\u548c\u5317\u4e9a\u9a6c\u900a\u96e8\u6797\uff09\u7684\u690d\u7269\u8bc6\u522b\uff0c\u4ee5\u63a8\u52a8\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002", "method": "\u672c\u6b21\u6311\u6218\u8d5b\u57fa\u4e8e\u572d\u4e9a\u90a3\u5730\u76fe\u548c\u5317\u4e9a\u9a6c\u900a\u96e8\u6797\u768410K\u7269\u79cd\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u81ea\u52a8\u8bc6\u522b\u7cfb\u7edf\u7684\u6027\u80fd\u3002\u5e76\u4e0e\u9876\u5c16\u7684\u70ed\u5e26\u690d\u7269\u4e13\u5bb6\u8fdb\u884c\u4e86\u6027\u80fd\u6bd4\u8f83\u3002", "result": "\u6bd4\u8d5b\u8bc4\u4f30\u4e86\u53c2\u8d5b\u7814\u7a76\u5c0f\u7ec4\u91c7\u7528\u7684\u65b9\u6cd5\u548c\u7cfb\u7edf\uff0c\u5e76\u5bf9\u4e3b\u8981\u6210\u679c\u8fdb\u884c\u4e86\u5206\u6790\u3002", "conclusion": "\u672c\u6b21\u6311\u6218\u8d5b\u603b\u7ed3\u4e86\u8d44\u6e90\u3001\u8bc4\u4f30\u65b9\u6cd5\u3001\u53c2\u8d5b\u7cfb\u7edf\u7684\u7279\u70b9\u4ee5\u53ca\u4e3b\u8981\u7ed3\u679c\uff0c\u4e3a\u672a\u6765\u5728\u6570\u636e\u532e\u4e4f\u5730\u533a\u8fdb\u884c\u690d\u7269\u81ea\u52a8\u8bc6\u522b\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u53c2\u8003\u3002"}}
{"id": "2509.18396", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18396", "abs": "https://arxiv.org/abs/2509.18396", "authors": ["Do\u011fay Alt\u0131nel"], "title": "Development of Deep Learning Optimizers: Approaches, Concepts, and Update Rules", "comment": "24 pages", "summary": "Deep learning optimizers are optimization algorithms that enable deep neural\nnetworks to learn. The effectiveness of learning is highly dependent on the\noptimizer employed in the training process. Alongside the rapid advancement of\ndeep learning, a wide range of optimizers with different approaches have been\ndeveloped. This study aims to provide a review of various optimizers that have\nbeen proposed and received attention in the literature. From Stochastic\ngradient descent to the most recent ones such as Momentum, AdamW, Sophia, and\nMuon in chronological order, optimizers are examined individually, and their\ndistinctive features are highlighted in the study. The update rule of each\noptimizer is presented in detail, with an explanation of the associated\nconcepts and variables. The techniques applied by these optimizers, their\ncontributions to the optimization process, and their default hyperparameter\nsettings are also discussed. In addition, insights are offered into the open\nchallenges encountered in the optimization of deep learning models. Thus, a\ncomprehensive resource is provided both for understanding the current state of\noptimizers and for identifying potential areas of future development.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.19270", "categories": ["cs.CL", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.19270", "abs": "https://arxiv.org/abs/2509.19270", "authors": ["Erik Bo\u017e\u00edk", "Marek \u0160uppa"], "title": "SloPalSpeech: A 2,8000-Hour Slovak Speech Corpus from Parliamentary Data", "comment": null, "summary": "Automatic Speech Recognition (ASR) for low-resource languages like Slovak is\nhindered by the scarcity of training data. To address this, we introduce\nSloPalSpeech, a new, large-scale Slovak ASR dataset containing 2,806 hours of\nspeech from parliamentary proceedings. We developed a robust processing\npipeline to align and segment long-form recordings into clean, 30-second\naudio-transcript pairs suitable for model training. We use this dataset to\nfine-tune several OpenAI Whisper models (small, medium, large-v3, and\nlarge-v3-turbo), achieving significant Word Error Rate (WER) reductions on\nstandard Slovak benchmarks like Common Voice and FLEURS. For instance, the\nfine-tuned Whisper-small model's WER dropped by up to 70\\%, approaching the\nbaseline performance of the much larger Whisper-large-v3 model. To foster\nfuture research in low-resource speech recognition, we publicly release the\ncomplete SloPalSpeech dataset, the fully segmented transcripts (60 million\nwords), and all our fine-tuned models.", "AI": {"tldr": "\u4e3a\u65af\u6d1b\u4f10\u514b\u8bed\u7b49\u4f4e\u8d44\u6e90\u8bed\u8a00\u5f00\u53d1\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6SloPalSpeech\uff0c\u5e76\u5c55\u793a\u4e86\u4f7f\u7528\u8be5\u6570\u636e\u96c6\u5fae\u8c03Whisper\u6a21\u578b\u7684\u663e\u8457\u6548\u679c\u3002", "motivation": "\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u5982\u65af\u6d1b\u4f10\u514b\u8bed\uff09\u7684\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u9762\u4e34\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u7684\u6311\u6218\u3002", "method": "\u521b\u5efa\u4e86\u4e00\u4e2a\u5305\u542b2806\u5c0f\u65f6\u56fd\u4f1a\u6f14\u8bb2\u5f55\u97f3\u7684\u65af\u6d1b\u4f10\u514b\u8bedASR\u6570\u636e\u96c6SloPalSpeech\uff0c\u5e76\u5f00\u53d1\u4e86\u5904\u7406\u6d41\u7a0b\u4ee5\u751f\u6210\u9002\u5408\u6a21\u578b\u8bad\u7ec3\u768430\u79d2\u97f3\u9891-\u6587\u672c\u5bf9\u3002\u4f7f\u7528\u8be5\u6570\u636e\u96c6\u5fae\u8c03\u4e86OpenAI Whisper\u7684\u591a\u4e2a\u6a21\u578b\uff08small, medium, large-v3, large-v3-turbo\uff09\u3002", "result": "\u5728Common Voice\u548cFLEURS\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5fae\u8c03\u540e\u7684Whisper\u6a21\u578b\u9519\u8bef\u7387\uff08WER\uff09\u663e\u8457\u964d\u4f4e\uff0c\u5176\u4e2dWhisper-small\u6a21\u578b\u7684WER\u964d\u4f4e\u9ad8\u8fbe70%\uff0c\u63a5\u8fd1\u4e86\u66f4\u5927\u7684Whisper-large-v3\u6a21\u578b\u7684\u57fa\u7ebf\u6027\u80fd\u3002", "conclusion": "\u516c\u5f00SloPalSpeech\u6570\u636e\u96c6\u3001\u5206\u6bb5\u8f6c\u5f55\u6587\u672c\uff086000\u4e07\u8bcd\uff09\u548c\u5fae\u8c03\u540e\u7684\u6a21\u578b\uff0c\u4ee5\u4fc3\u8fdb\u4f4e\u8d44\u6e90\u8bed\u97f3\u8bc6\u522b\u9886\u57df\u7684\u7814\u7a76\u3002"}}
{"id": "2509.18711", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18711", "abs": "https://arxiv.org/abs/2509.18711", "authors": ["Ke Li", "Di Wang", "Ting Wang", "Fuyu Dong", "Yiming Zhang", "Luyao Zhang", "Xiangyu Wang", "Shaofeng Li", "Quan Wang"], "title": "RSVG-ZeroOV: Exploring a Training-Free Framework for Zero-Shot Open-Vocabulary Visual Grounding in Remote Sensing Images", "comment": null, "summary": "Remote sensing visual grounding (RSVG) aims to localize objects in remote\nsensing images based on free-form natural language expressions. Existing\napproaches are typically constrained to closed-set vocabularies, limiting their\napplicability in open-world scenarios. While recent attempts to leverage\ngeneric foundation models for open-vocabulary RSVG, they overly rely on\nexpensive high-quality datasets and time-consuming fine-tuning. To address\nthese limitations, we propose \\textbf{RSVG-ZeroOV}, a training-free framework\nthat aims to explore the potential of frozen generic foundation models for\nzero-shot open-vocabulary RSVG. Specifically, RSVG-ZeroOV comprises three key\nstages: (i) Overview: We utilize a vision-language model (VLM) to obtain\ncross-attention\\footnote[1]{In this paper, although decoder-only VLMs use\nself-attention over all tokens, we refer to the image-text interaction part as\ncross-attention to distinguish it from pure visual self-attention.}maps that\ncapture semantic correlations between text queries and visual regions. (ii)\nFocus: By leveraging the fine-grained modeling priors of a diffusion model\n(DM), we fill in gaps in structural and shape information of objects, which are\noften overlooked by VLM. (iii) Evolve: A simple yet effective attention\nevolution module is introduced to suppress irrelevant activations, yielding\npurified segmentation masks over the referred objects. Without cumbersome\ntask-specific training, RSVG-ZeroOV offers an efficient and scalable solution.\nExtensive experiments demonstrate that the proposed framework consistently\noutperforms existing weakly-supervised and zero-shot methods.", "AI": {"tldr": "\u63d0\u51faRSVG-ZeroOV\u6846\u67b6\uff0c\u901a\u8fc7\u51bb\u7ed3\u7684\u901a\u7528\u57fa\u7840\u6a21\u578b\u5b9e\u73b0\u96f6\u6837\u672c\u5f00\u653e\u8bcd\u6c47\u7684\u9065\u611f\u56fe\u50cf\u89c6\u89c9\u5b9a\u4f4d\uff0c\u65e0\u9700\u8bad\u7ec3\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u53d7\u9650\u4e8e\u95ed\u96c6\u8bcd\u6c47\uff0c\u6cdb\u5316\u80fd\u529b\u5dee\uff1b\u5229\u7528\u57fa\u7840\u6a21\u578b\u7684\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u6570\u636e\u96c6\u548c\u8017\u65f6\u5fae\u8c03\u3002\u672c\u7814\u7a76\u65e8\u5728\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u63a2\u7d22\u51bb\u7ed3\u901a\u7528\u57fa\u7840\u6a21\u578b\u5728\u96f6\u6837\u672c\u5f00\u653e\u8bcd\u6c47\u9065\u611f\u56fe\u50cf\u89c6\u89c9\u5b9a\u4f4d\u65b9\u9762\u7684\u6f5c\u529b\u3002", "method": "RSVG-ZeroOV\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u9636\u6bb5\uff1a(i) \u5229\u7528\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u83b7\u53d6\u6587\u672c\u67e5\u8be2\u548c\u89c6\u89c9\u533a\u57df\u4e4b\u95f4\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u56fe\uff1b(ii) \u5229\u7528\u6269\u6563\u6a21\u578b\uff08DM\uff09\u7684\u7ec6\u7c92\u5ea6\u5efa\u6a21\u5148\u9a8c\uff0c\u8865\u5145VLM\u5ffd\u7565\u7684\u7269\u4f53\u7ed3\u6784\u548c\u5f62\u72b6\u4fe1\u606f\uff1b(iii) \u5f15\u5165\u6ce8\u610f\u529b\u6f14\u5316\u6a21\u5757\uff0c\u6291\u5236\u4e0d\u76f8\u5173\u6fc0\u6d3b\uff0c\u751f\u6210\u7eaf\u51c0\u7684\u5206\u5272\u63a9\u7801\u3002", "result": "\u5728\u6ca1\u6709\u4efb\u4f55\u7e41\u7410\u7684\u7279\u5b9a\u4efb\u52a1\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0cRSVG-ZeroOV\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u7684\u6027\u80fd\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u7684\u5f31\u76d1\u7763\u548c\u96f6\u6837\u672c\u65b9\u6cd5\u3002", "conclusion": "RSVG-ZeroOV\u6846\u67b6\u80fd\u591f\u65e0\u9700\u8bad\u7ec3\uff0c\u5229\u7528\u51bb\u7ed3\u7684\u901a\u7528\u57fa\u7840\u6a21\u578b\uff0c\u5728\u96f6\u6837\u672c\u5f00\u653e\u8bcd\u6c47\u7684\u9065\u611f\u56fe\u50cf\u89c6\u89c9\u5b9a\u4f4d\u4efb\u52a1\u4e0a\u53d6\u5f97\u4f18\u5f02\u8868\u73b0\uff0c\u5e76\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2509.18408", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2509.18408", "abs": "https://arxiv.org/abs/2509.18408", "authors": ["Sarwan Ali"], "title": "Explicit Path CGR: Maintaining Sequence Fidelity in Geometric Representations", "comment": "Accepted to CIKM 2025 as Short paper", "summary": "We present a novel information-preserving Chaos Game Representation (CGR)\nmethod, also called Reverse-CGR (R-CGR), for biological sequence analysis that\naddresses the fundamental limitation of traditional CGR approaches - the loss\nof sequence information during geometric mapping. Our method introduces\ncomplete sequence recovery through explicit path encoding combined with\nrational arithmetic precision control, enabling perfect sequence reconstruction\nfrom stored geometric traces. Unlike purely geometric approaches, our\nreversibility is achieved through comprehensive path storage that maintains\nboth positional and character information at each step. We demonstrate the\neffectiveness of R-CGR on biological sequence classification tasks, achieving\ncompetitive performance compared to traditional sequence-based methods while\nproviding interpretable geometric visualizations. The approach generates\nfeature-rich images suitable for deep learning while maintaining complete\nsequence information through explicit encoding, opening new avenues for\ninterpretable bioinformatics analysis where both accuracy and sequence recovery\nare essential.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3a\u53cd\u5411\u6df7\u6c8c\u6e38\u620f\u8868\u793a\uff08R-CGR\uff09\u7684\u65b0\u4fe1\u606f\u4fdd\u6301\u65b9\u6cd5\uff0c\u7528\u4e8e\u751f\u7269\u5e8f\u5217\u5206\u6790\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfCGR\u4e22\u5931\u5e8f\u5217\u4fe1\u606f\u7684\u7f3a\u70b9\u3002", "motivation": "\u4f20\u7edfCGR\u65b9\u6cd5\u5728\u8fdb\u884c\u51e0\u4f55\u6620\u5c04\u65f6\u4f1a\u4e22\u5931\u5e8f\u5217\u4fe1\u606f\uff0c\u9650\u5236\u4e86\u5176\u5728\u751f\u7269\u5e8f\u5217\u5206\u6790\u4e2d\u7684\u5e94\u7528\u3002", "method": "R-CGR\u65b9\u6cd5\u901a\u8fc7\u663e\u5f0f\u8def\u5f84\u7f16\u7801\u548c\u6709\u7406\u6570\u7b97\u672f\u7cbe\u5ea6\u63a7\u5236\uff0c\u5b9e\u73b0\u4e86\u5b8c\u6574\u7684\u5e8f\u5217\u6062\u590d\uff0c\u80fd\u591f\u4ece\u5b58\u50a8\u7684\u51e0\u4f55\u8f68\u8ff9\u4e2d\u5b8c\u7f8e\u91cd\u5efa\u5e8f\u5217\u3002\u5b83\u901a\u8fc7\u5b58\u50a8\u5b8c\u6574\u7684\u8def\u5f84\u4fe1\u606f\uff08\u5305\u62ec\u4f4d\u7f6e\u548c\u5b57\u7b26\u4fe1\u606f\uff09\u6765\u5b9e\u73b0\u53ef\u9006\u6027\u3002", "result": "R-CGR\u5728\u751f\u7269\u5e8f\u5217\u5206\u7c7b\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u6709\u6548\u6027\uff0c\u4e0e\u4f20\u7edf\u7684\u57fa\u4e8e\u5e8f\u5217\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u53d6\u5f97\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u5e76\u80fd\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u51e0\u4f55\u53ef\u89c6\u5316\u3002", "conclusion": "R-CGR\u65b9\u6cd5\u901a\u8fc7\u663e\u5f0f\u7f16\u7801\u4fdd\u7559\u4e86\u5b8c\u6574\u7684\u5e8f\u5217\u4fe1\u606f\uff0c\u5e76\u751f\u6210\u4e86\u9002\u7528\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u7279\u5f81\u4e30\u5bcc\u7684\u56fe\u50cf\uff0c\u4e3a\u751f\u7269\u4fe1\u606f\u5b66\u5206\u6790\u5f00\u8f9f\u4e86\u65b0\u7684\u9014\u5f84\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u9700\u8981\u51c6\u786e\u6027\u548c\u5e8f\u5217\u53ef\u6062\u590d\u6027\u7684\u573a\u666f\u3002"}}
{"id": "2509.19271", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19271", "abs": "https://arxiv.org/abs/2509.19271", "authors": ["Abdou Karim Kandji", "Fr\u00e9d\u00e9ric Precioso", "Cheikh Ba", "Samba Ndiaye", "Augustin Ndione"], "title": "WolBanking77: Wolof Banking Speech Intent Classification Dataset", "comment": "10 pages, 7 figures", "summary": "Intent classification models have made a lot of progress in recent years.\nHowever, previous studies primarily focus on high-resource languages datasets,\nwhich results in a gap for low-resource languages and for regions with a high\nrate of illiterate people where languages are more spoken than read or written.\nThis is the case in Senegal, for example, where Wolof is spoken by around 90\\%\nof the population, with an illiteracy rate of 42\\% for the country. Wolof is\nactually spoken by more than 10 million people in West African region. To\ntackle such limitations, we release a Wolof Intent Classification Dataset\n(WolBanking77), for academic research in intent classification. WolBanking77\ncurrently contains 9,791 text sentences in the banking domain and more than 4\nhours of spoken sentences. Experiments on various baselines are conducted in\nthis work, including text and voice state-of-the-art models. The results are\nvery promising on this current dataset. This paper also provides detailed\nanalyses of the contents of the data. We report baseline f1-score and word\nerror rate metrics respectively on NLP and ASR models trained on WolBanking77\ndataset and also comparisons between models. We plan to share and conduct\ndataset maintenance, updates and to release open-source code.", "AI": {"tldr": "\u53d1\u5e03\u4e86\u4e00\u4e2a\u7528\u4e8e\u94f6\u884c\u9886\u57df\u7684\u6c83\u5c14\u8bed\u610f\u56fe\u5206\u7c7b\u6570\u636e\u96c6\uff08WolBanking77\uff09\uff0c\u5176\u4e2d\u5305\u542b\u6587\u672c\u548c\u8bed\u97f3\u6570\u636e\uff0c\u5e76\u5728\u5404\u79cd\u57fa\u7ebf\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u7ed3\u679c\u5f88\u6709\u5e0c\u671b\u3002", "motivation": "\u89e3\u51b3\u4f4e\u8d44\u6e90\u8bed\u8a00\u548c\u6587\u76f2\u7387\u9ad8\u5bfc\u81f4\u610f\u56fe\u5206\u7c7b\u6a21\u578b\u5728\u8fd9\u4e9b\u8bed\u8a00\u4e0a\u8868\u73b0\u4e0d\u4f73\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u50cf\u585e\u5185\u52a0\u5c14\u7684\u6c83\u5c14\u8bed\u8fd9\u6837\u7684\u8bed\u8a00\u3002", "method": "\u53d1\u5e03\u4e86\u4e00\u4e2a\u5305\u542b9,791\u4e2a\u6587\u672c\u53e5\u5b50\u548c4\u5c0f\u65f6\u4ee5\u4e0a\u53e3\u8bed\u6570\u636e\u7684\u6c83\u5c14\u8bed\u610f\u56fe\u5206\u7c7b\u6570\u636e\u96c6\uff08WolBanking77\uff09\uff0c\u5e76\u5728\u8be5\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5305\u62ec\u6587\u672c\u548c\u8bed\u97f3\u5728\u5185\u7684\u5404\u79cd\u57fa\u7ebf\u6a21\u578b\u5b9e\u9a8c\u3002", "result": "\u5728WolBanking77\u6570\u636e\u96c6\u4e0a\uff0cNLP\u548cASR\u6a21\u578b\u7684\u57fa\u7ebfF1\u5206\u6570\u548c\u8bcd\u9519\u8bef\u7387\u6307\u6807\u90fd\u5f88\u6709\u5e0c\u671b\uff0c\u5e76\u4e14\u5bf9\u6a21\u578b\u8fdb\u884c\u4e86\u6bd4\u8f83\u5206\u6790\u3002", "conclusion": "\u53d1\u5e03WolBanking77\u6570\u636e\u96c6\u4e3a\u6c83\u5c14\u8bed\u7684\u610f\u56fe\u5206\u7c7b\u7814\u7a76\u63d0\u4f9b\u4e86\u8d44\u6e90\uff0c\u5e76\u5728\u8be5\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u521d\u6b65\u5b9e\u9a8c\u7ed3\u679c\u4ee4\u4eba\u9f13\u821e\uff0c\u4e3a\u672a\u6765\u7684\u7814\u7a76\u548c\u5f00\u53d1\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.18715", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18715", "abs": "https://arxiv.org/abs/2509.18715", "authors": ["Yingquan Wang", "Pingping Zhang", "Chong Sun", "Dong Wang", "Huchuan Lu"], "title": "What Makes You Unique? Attribute Prompt Composition for Object Re-Identification", "comment": "Accepted by TCSVT2025", "summary": "Object Re-IDentification (ReID) aims to recognize individuals across\nnon-overlapping camera views. While recent advances have achieved remarkable\nprogress, most existing models are constrained to either single-domain or\ncross-domain scenarios, limiting their real-world applicability. Single-domain\nmodels tend to overfit to domain-specific features, whereas cross-domain models\noften rely on diverse normalization strategies that may inadvertently suppress\nidentity-specific discriminative cues. To address these limitations, we propose\nan Attribute Prompt Composition (APC) framework, which exploits textual\nsemantics to jointly enhance discrimination and generalization. Specifically,\nwe design an Attribute Prompt Generator (APG) consisting of a Semantic\nAttribute Dictionary (SAD) and a Prompt Composition Module (PCM). SAD is an\nover-complete attribute dictionary to provide rich semantic descriptions, while\nPCM adaptively composes relevant attributes from SAD to generate discriminative\nattribute-aware features. In addition, motivated by the strong generalization\nability of Vision-Language Models (VLM), we propose a Fast-Slow Training\nStrategy (FSTS) to balance ReID-specific discrimination and generalizable\nrepresentation learning. Specifically, FSTS adopts a Fast Update Stream (FUS)\nto rapidly acquire ReID-specific discriminative knowledge and a Slow Update\nStream (SUS) to retain the generalizable knowledge inherited from the\npre-trained VLM. Through a mutual interaction, the framework effectively\nfocuses on ReID-relevant features while mitigating overfitting. Extensive\nexperiments on both conventional and Domain Generalized (DG) ReID datasets\ndemonstrate that our framework surpasses state-of-the-art methods, exhibiting\nsuperior performances in terms of both discrimination and generalization. The\nsource code is available at https://github.com/AWangYQ/APC.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u5c5e\u6027\u63d0\u793a\u7ec4\u5408\uff08APC\uff09\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u76ee\u6807\u91cd\u8bc6\u522b\uff08ReID\uff09\u4e2d\u7684\u5355\u57df\u548c\u8de8\u57df\u9650\u5236\u95ee\u9898\uff0c\u901a\u8fc7\u5229\u7528\u6587\u672c\u8bed\u4e49\u589e\u5f3a\u8bc6\u522b\u80fd\u529b\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709ReID\u6a21\u578b\u5728\u5355\u57df\u6216\u8de8\u57df\u573a\u666f\u4e0b\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5355\u57df\u6a21\u578b\u6613\u8fc7\u62df\u5408\uff0c\u8de8\u57df\u6a21\u578b\u53ef\u80fd\u6291\u5236\u8fa8\u522b\u6027\u7ebf\u7d22\u3002", "method": "\u63d0\u51fa\u5c5e\u6027\u63d0\u793a\u7ec4\u5408\uff08APC\uff09\u6846\u67b6\uff0c\u5305\u62ec\u5c5e\u6027\u63d0\u793a\u751f\u6210\u5668\uff08APG\uff09\uff0c\u5176\u4e2d\u5305\u542b\u8bed\u4e49\u5c5e\u6027\u5b57\u5178\uff08SAD\uff09\u548c\u63d0\u793a\u7ec4\u5408\u6a21\u5757\uff08PCM\uff09\u3002SAD\u63d0\u4f9b\u4e30\u5bcc\u7684\u8bed\u4e49\u63cf\u8ff0\uff0cPCM\u81ea\u9002\u5e94\u5730\u7ec4\u5408\u76f8\u5173\u5c5e\u6027\u4ee5\u751f\u6210\u8fa8\u522b\u6027\u7279\u5f81\u3002\u6b64\u5916\uff0c\u91c7\u7528\u5feb\u6162\u8bad\u7ec3\u7b56\u7565\uff08FSTS\uff09\u6765\u5e73\u8861ReID\u7684\u8fa8\u522b\u6027\u548c\u6cdb\u5316\u6027\uff0c\u5305\u62ec\u5feb\u901f\u66f4\u65b0\u6d41\uff08FUS\uff09\u548c\u6162\u901f\u66f4\u65b0\u6d41\uff08SUS\uff09\u3002", "result": "\u5728\u6807\u51c6\u548c\u57df\u6cdb\u5316\uff08DG\uff09ReID\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cAPC\u6846\u67b6\u5728\u8fa8\u522b\u6027\u548c\u6cdb\u5316\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "APC\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u6587\u672c\u8bed\u4e49\u548c\u521b\u65b0\u7684\u8bad\u7ec3\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86ReID\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u5404\u79cd\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u9886\u5148\u7684\u6027\u80fd\u3002"}}
{"id": "2509.18433", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18433", "abs": "https://arxiv.org/abs/2509.18433", "authors": ["Chang Liu", "Ladda Thiamwong", "Yanjie Fu", "Rui Xie"], "title": "Diffusion Policies with Offline and Inverse Reinforcement Learning for Promoting Physical Activity in Older Adults Using Wearable Sensors", "comment": "Accepted at ICMLA 2025. 8 pages, 6 figures", "summary": "Utilizing offline reinforcement learning (RL) with real-world clinical data\nis getting increasing attention in AI for healthcare. However, implementation\nposes significant challenges. Defining direct rewards is difficult, and inverse\nRL (IRL) struggles to infer accurate reward functions from expert behavior in\ncomplex environments. Offline RL also encounters challenges in aligning learned\npolicies with observed human behavior in healthcare applications. To address\nchallenges in applying offline RL to physical activity promotion for older\nadults at high risk of falls, based on wearable sensor activity monitoring, we\nintroduce Kolmogorov-Arnold Networks and Diffusion Policies for Offline Inverse\nReinforcement Learning (KANDI). By leveraging the flexible function\napproximation in Kolmogorov-Arnold Networks, we estimate reward functions by\nlearning free-living environment behavior from low-fall-risk older adults\n(experts), while diffusion-based policies within an Actor-Critic framework\nprovide a generative approach for action refinement and efficiency in offline\nRL. We evaluate KANDI using wearable activity monitoring data in a two-arm\nclinical trial from our Physio-feedback Exercise Program (PEER) study,\nemphasizing its practical application in a fall-risk intervention program to\npromote physical activity among older adults. Additionally, KANDI outperforms\nstate-of-the-art methods on the D4RL benchmark. These results underscore\nKANDI's potential to address key challenges in offline RL for healthcare\napplications, offering an effective solution for activity promotion\nintervention strategies in healthcare.", "AI": {"tldr": "KANDI\u662f\u4e00\u79cd\u7528\u4e8e\u533b\u7597\u4fdd\u5065\u7684\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u5b83\u5229\u7528Kolmogorov-Arnold\u7f51\u7edc\u4f30\u8ba1\u5956\u52b1\u51fd\u6570\uff0c\u5e76\u7ed3\u5408\u6269\u6563\u7b56\u7565\u8fdb\u884c\u52a8\u4f5c\u4f18\u5316\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u6311\u6218\uff0c\u5e76\u5728\u4e34\u5e8a\u8bd5\u9a8c\u548cD4DR\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e34\u5e8a\u6570\u636e\u5728\u533b\u7597\u4fdd\u5065\u9886\u57df\u7684\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5e94\u7528\u9762\u4e34\u8bf8\u591a\u6311\u6218\uff0c\u5305\u62ec\u5956\u52b1\u5b9a\u4e49\u56f0\u96be\u3001\u9006\u5411\u5f3a\u5316\u5b66\u4e60\uff08IRL\uff09\u96be\u4ee5\u4ece\u4e13\u5bb6\u884c\u4e3a\u4e2d\u51c6\u786e\u63a8\u65ad\u5956\u52b1\u51fd\u6570\uff0c\u4ee5\u53ca\u5b66\u4e60\u5230\u7684\u7b56\u7565\u4e0e\u4eba\u7c7b\u884c\u4e3a\u4e0d\u4e00\u81f4\u7b49\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u5c06\u79bb\u7ebfRL\u5e94\u7528\u4e8e\u8001\u5e74\u4eba\u4fc3\u8fdb\u4f53\u80b2\u6d3b\u52a8\u4e2d\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aKANDI\uff08Kolmogorov-Arnold Networks and Diffusion Policies for Offline Inverse Reinforcement Learning\uff09\u7684\u65b0\u65b9\u6cd5\u3002KANDI\u5229\u7528Kolmogorov-Arnold\u7f51\u7edc\u7684\u51fd\u6570\u903c\u8fd1\u80fd\u529b\u6765\u4f30\u8ba1\u5956\u52b1\u51fd\u6570\uff0c\u901a\u8fc7\u5b66\u4e60\u4f4e\u8dcc\u5012\u98ce\u9669\u8001\u5e74\u4eba\u7684\u81ea\u7531\u751f\u6d3b\u73af\u5883\u884c\u4e3a\uff08\u4f5c\u4e3a\u4e13\u5bb6\uff09\u3002\u540c\u65f6\uff0c\u5728Actor-Critic\u6846\u67b6\u5185\u4f7f\u7528\u57fa\u4e8e\u6269\u6563\u7684\u7b56\u7565\uff0c\u4e3a\u79bb\u7ebfRL\u63d0\u4f9b\u4e86\u4e00\u79cd\u751f\u6210\u5f0f\u7684\u65b9\u6cd5\uff0c\u4ee5\u5b9e\u73b0\u52a8\u4f5c\u4f18\u5316\u548c\u63d0\u9ad8\u6548\u7387\u3002", "result": "KANDI\u65b9\u6cd5\u5728Physio-feedback Exercise Program (PEER)\u7814\u7a76\u7684\u53cc\u81c2\u4e34\u5e8a\u8bd5\u9a8c\u4e2d\uff0c\u4f7f\u7528\u53ef\u7a7f\u6234\u6d3b\u52a8\u76d1\u6d4b\u6570\u636e\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u4fc3\u8fdb\u8001\u5e74\u4eba\u4f53\u80b2\u6d3b\u52a8\u7684\u8dcc\u5012\u98ce\u9669\u5e72\u9884\u9879\u76ee\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002\u6b64\u5916\uff0cKANDI\u5728D4RL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "KANDI\u5728\u89e3\u51b3\u79bb\u7ebfRL\u5728\u533b\u7597\u4fdd\u5065\u5e94\u7528\u4e2d\u7684\u5173\u952e\u6311\u6218\u65b9\u9762\u663e\u793a\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u4e3a\u533b\u7597\u4fdd\u5065\u9886\u57df\u7684\u6d3b\u52a8\u4fc3\u8fdb\u5e72\u9884\u7b56\u7565\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.19274", "categories": ["cs.CL", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.19274", "abs": "https://arxiv.org/abs/2509.19274", "authors": ["Arijit Maji", "Raghvendra Kumar", "Akash Ghosh", "Anushka", "Nemil Shah", "Abhilekh Borah", "Vanshika Shah", "Nishant Mishra", "Sriparna Saha"], "title": "DRISHTIKON: A Multimodal Multilingual Benchmark for Testing Language Models' Understanding on Indian Culture", "comment": "EMNLP MAINS 2025", "summary": "We introduce DRISHTIKON, a first-of-its-kind multimodal and multilingual\nbenchmark centered exclusively on Indian culture, designed to evaluate the\ncultural understanding of generative AI systems. Unlike existing benchmarks\nwith a generic or global scope, DRISHTIKON offers deep, fine-grained coverage\nacross India's diverse regions, spanning 15 languages, covering all states and\nunion territories, and incorporating over 64,000 aligned text-image pairs. The\ndataset captures rich cultural themes including festivals, attire, cuisines,\nart forms, and historical heritage amongst many more. We evaluate a wide range\nof vision-language models (VLMs), including open-source small and large models,\nproprietary systems, reasoning-specialized VLMs, and Indic-focused models,\nacross zero-shot and chain-of-thought settings. Our results expose key\nlimitations in current models' ability to reason over culturally grounded,\nmultimodal inputs, particularly for low-resource languages and less-documented\ntraditions. DRISHTIKON fills a vital gap in inclusive AI research, offering a\nrobust testbed to advance culturally aware, multimodally competent language\ntechnologies.", "AI": {"tldr": "DRISHTIKON\u662f\u4e00\u4e2a\u9488\u5bf9\u5370\u5ea6\u6587\u5316\u7684\u3001\u591a\u6a21\u6001\u3001\u591a\u8bed\u8a00\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u751f\u6210\u5f0fAI\u7684\u6587\u5316\u7406\u89e3\u80fd\u529b\u3002\u5b83\u6db5\u76d6\u4e8615\u79cd\u8bed\u8a00\uff0c\u5305\u62ec\u4e86\u5370\u5ea6\u6240\u6709\u90a6\u548c\u8054\u90a6\u5c5e\u5730\uff0c\u62e5\u6709\u8d85\u8fc764,000\u4e2a\u6587\u672c-\u56fe\u50cf\u5bf9\uff0c\u805a\u7126\u4e8e\u8282\u65e5\u3001\u670d\u9970\u3001\u7f8e\u98df\u3001\u827a\u672f\u548c\u5386\u53f2\u9057\u4ea7\u7b49\u4e30\u5bcc\u7684\u6587\u5316\u4e3b\u9898\u3002\u7814\u7a76\u8bc4\u4f30\u4e86\u591a\u79cd\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u7406\u89e3\u6587\u5316\u76f8\u5173\u7684\u591a\u6a21\u6001\u8f93\u5165\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u548c\u9c9c\u4e3a\u4eba\u77e5\u7684\u4f20\u7edf\u65b9\u9762\u3002DRISHTIKON\u586b\u8865\u4e86\u5305\u5bb9\u6027AI\u7814\u7a76\u7684\u7a7a\u767d\uff0c\u4e3a\u63a8\u52a8\u6587\u5316\u610f\u8bc6\u548c\u591a\u6a21\u6001\u80fd\u529b\u7684\u8bed\u8a00\u6280\u672f\u63d0\u4f9b\u4e86\u6d4b\u8bd5\u5e73\u53f0\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u51c6\u6d4b\u8bd5\u901a\u5e38\u8303\u56f4\u901a\u7528\u6216\u5168\u7403\u5316\uff0c\u7f3a\u4e4f\u5bf9\u7279\u5b9a\u6587\u5316\uff08\u5c24\u5176\u662f\u5370\u5ea6\u6587\u5316\uff09\u7684\u6df1\u5165\u3001\u7ec6\u7c92\u5ea6\u8986\u76d6\u3002DRISHTIKON\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u901a\u8fc7\u521b\u5efa\u4e00\u4e2a\u4e13\u95e8\u9488\u5bf9\u5370\u5ea6\u6587\u5316\u7684\u3001\u591a\u6a21\u6001\u3001\u591a\u8bed\u8a00\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4ee5\u8bc4\u4f30\u548c\u63d0\u5347\u751f\u6210\u5f0fAI\u7cfb\u7edf\u5bf9\u5370\u5ea6\u6587\u5316\u7684\u7406\u89e3\u80fd\u529b\uff0c\u4fc3\u8fdb\u5305\u5bb9\u6027AI\u7814\u7a76\u3002", "method": "\u521b\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aDRISHTIKON\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8be5\u6d4b\u8bd5\u662f\u591a\u6a21\u6001\u548c\u591a\u8bed\u8a00\u7684\uff0c\u4e13\u95e8\u9488\u5bf9\u5370\u5ea6\u6587\u5316\u3002\u5b83\u5305\u542b\u4e8615\u79cd\u8bed\u8a00\uff0c\u8986\u76d6\u4e86\u5370\u5ea6\u6240\u6709\u5730\u533a\uff0c\u5e76\u6536\u96c6\u4e86\u8d85\u8fc764,000\u4e2a\u6587\u672c-\u56fe\u50cf\u5bf9\u3002\u8be5\u57fa\u51c6\u6d4b\u8bd5\u6db5\u76d6\u4e86\u8282\u65e5\u3001\u670d\u9970\u3001\u7f8e\u98df\u3001\u827a\u672f\u548c\u5386\u53f2\u9057\u4ea7\u7b49\u6587\u5316\u4e3b\u9898\u3002\u7814\u7a76\u4eba\u5458\u4f7f\u7528\u8be5\u57fa\u51c6\u6d4b\u8bd5\u8bc4\u4f30\u4e86\u591a\u79cd\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\uff0c\u5305\u62ec\u5f00\u6e90\u6a21\u578b\u3001\u4e13\u6709\u6a21\u578b\u3001\u63a8\u7406\u4e13\u7528\u6a21\u578b\u548c\u4e13\u6ce8\u4e8e\u5370\u5ea6\u8bed\u8a00\u7684\u6a21\u578b\uff0c\u5e76\u5728\u96f6\u6837\u672c\uff08zero-shot\uff09\u548c\u601d\u7ef4\u94fe\uff08chain-of-thought\uff09\u8bbe\u7f6e\u4e0b\u8fdb\u884c\u4e86\u6d4b\u8bd5\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0c\u5f53\u524d\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7406\u89e3\u548c\u63a8\u7406\u4e0e\u5370\u5ea6\u6587\u5316\u76f8\u5173\u7684\u591a\u6a21\u6001\u8f93\u5165\u65b9\u9762\u5b58\u5728\u663e\u8457\u5c40\u9650\u6027\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u4f4e\u8d44\u6e90\u8bed\u8a00\u548c\u4e0d\u592a\u4e3a\u4eba\u6240\u77e5\u7684\u6587\u5316\u4f20\u7edf\u65f6\u3002", "conclusion": "DRISHTIKON\u57fa\u51c6\u6d4b\u8bd5\u5bf9\u4e8e\u586b\u8865\u5305\u5bb9\u6027AI\u7814\u7a76\u4e2d\u7684\u5173\u952e\u7a7a\u767d\u81f3\u5173\u91cd\u8981\u3002\u5b83\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u6d4b\u8bd5\u5e73\u53f0\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u5f00\u53d1\u66f4\u5177\u6587\u5316\u610f\u8bc6\u3001\u5728\u591a\u6a21\u6001\u65b9\u9762\u66f4\u5177\u80fd\u529b\u548c\u66f4\u5305\u5bb9\u7684\u8bed\u8a00\u6280\u672f\u3002"}}
{"id": "2509.18717", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.18717", "abs": "https://arxiv.org/abs/2509.18717", "authors": ["Tong Zhang", "Kuofeng Gao", "Jiawang Bai", "Leo Yu Zhang", "Xin Yin", "Zonghui Wang", "Shouling Ji", "Wenzhi Chen"], "title": "Pre-training CLIP against Data Poisoning with Optimal Transport-based Matching and Alignment", "comment": null, "summary": "Recent studies have shown that Contrastive Language-Image Pre-training (CLIP)\nmodels are threatened by targeted data poisoning and backdoor attacks due to\nmassive training image-caption pairs crawled from the Internet. Previous\ndefense methods correct poisoned image-caption pairs by matching a new caption\nfor each image. However, the matching process relies solely on the global\nrepresentations of images and captions, overlooking fine-grained features of\nvisual and textual features. It may introduce incorrect image-caption pairs and\nharm the CLIP pre-training. To address their limitations, we propose an Optimal\nTransport-based framework to reconstruct image-caption pairs, named OTCCLIP. We\npropose a new optimal transport-based distance measure between fine-grained\nvisual and textual feature sets and re-assign new captions based on the\nproposed optimal transport distance. Additionally, to further reduce the\nnegative impact of mismatched pairs, we encourage the inter- and intra-modality\nfine-grained alignment by employing optimal transport-based objective\nfunctions. Our experiments demonstrate that OTCCLIP can successfully decrease\nthe attack success rates of poisoning attacks. Also, compared to previous\nmethods, OTCCLIP significantly improves CLIP's zero-shot and linear probing\nperformance trained on poisoned datasets.", "AI": {"tldr": "OTCCLIP\u901a\u8fc7\u4f7f\u7528\u57fa\u4e8e\u6700\u4f18\u4f20\u8f93\u7684\u8ddd\u79bb\u5ea6\u91cf\u6765\u91cd\u5efa\u56fe\u50cf-\u6807\u9898\u5bf9\uff0c\u4ee5\u9632\u5fa1\u9488\u5bf9CLIP\u6a21\u578b\u7684\u6570\u636e\u4e2d\u6bd2\u548c\u540e\u95e8\u653b\u51fb\u3002", "motivation": "\u73b0\u6709\u7684CLIP\u6a21\u578b\u6613\u53d7\u6570\u636e\u4e2d\u6bd2\u548c\u540e\u95e8\u653b\u51fb\uff0c\u800c\u73b0\u6709\u7684\u9632\u5fa1\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u4e8e\u5168\u5c40\u8868\u793a\uff0c\u53ef\u80fd\u5f15\u5165\u9519\u8bef\u7684\u56fe\u50cf-\u6807\u9898\u5bf9\u5e76\u635f\u5bb3CLIP\u9884\u8bad\u7ec3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6700\u4f18\u4f20\u8f93\u7684\u6846\u67b6OTCCLIP\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u8ba1\u7b97\u7ec6\u7c92\u5ea6\u89c6\u89c9\u548c\u6587\u672c\u7279\u5f81\u96c6\u4e4b\u95f4\u7684\u6700\u4f18\u4f20\u8f93\u8ddd\u79bb\u6765\u91cd\u5efa\u56fe\u50cf-\u6807\u9898\u5bf9\uff0c\u5e76\u91cd\u65b0\u5206\u914d\u6807\u9898\u3002\u6b64\u5916\uff0c\u8fd8\u91c7\u7528\u57fa\u4e8e\u6700\u4f18\u4f20\u8f93\u7684\u76ee\u6807\u51fd\u6570\u6765\u589e\u5f3a\u8de8\u6a21\u6001\u548c\u6a21\u6001\u5185\u7684\u7ec6\u7c92\u5ea6\u5bf9\u9f50\u3002", "result": "OTCCLIP\u80fd\u591f\u6210\u529f\u964d\u4f4e\u4e2d\u6bd2\u653b\u51fb\u7684\u6210\u529f\u7387\uff0c\u5e76\u663e\u8457\u63d0\u9ad8\u5728\u4e2d\u6bd2\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684CLIP\u6a21\u578b\u7684\u96f6\u6837\u672c\u548c\u7ebf\u6027\u63a2\u6d4b\u6027\u80fd\u3002", "conclusion": "OTCCLIP\u662f\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u9632\u5fa1\u9488\u5bf9CLIP\u6a21\u578b\u7684\u092f\u0940\u0921\u0947\u091f\u093e\u4e2d\u6bd2\u548c\u540e\u95e8\u653b\u51fb\uff0c\u540c\u65f6\u63d0\u9ad8\u6a21\u578b\u7684\u6027\u80fd\u3002"}}
{"id": "2509.18733", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18733", "abs": "https://arxiv.org/abs/2509.18733", "authors": ["Yilin Gao", "Kangyi Chen", "Zhongxing Peng", "Hengjie Lu", "Shugong Xu"], "title": "Knowledge Transfer from Interaction Learning", "comment": "Accepted by ICCV2025", "summary": "Current visual foundation models (VFMs) face a fundamental limitation in\ntransferring knowledge from vision language models (VLMs), while VLMs excel at\nmodeling cross-modal interactions through unified representation spaces,\nexisting VFMs predominantly adopt result-oriented paradigms that neglect the\nunderlying interaction processes. This representational discrepancy hinders\neffective knowledge transfer and limits generalization across diverse vision\ntasks. We propose Learning from Interactions (LFI), a cognitive-inspired\nframework that addresses this gap by explicitly modeling visual understanding\nas an interactive process. Our key insight is that capturing the dynamic\ninteraction patterns encoded in pre-trained VLMs enables more faithful and\nefficient knowledge transfer to VFMs. The approach centers on two technical\ninnovations, Interaction Queries, which maintain persistent relational\nstructures across network layers, and interaction-based supervision, derived\nfrom the cross-modal attention mechanisms of VLMs. Comprehensive experiments\ndemonstrate consistent improvements across multiple benchmarks, achieving 3.3\nand 1.6mAP/2.4AP absolute gains on TinyImageNet classification and COCO\ndetection/segmentation respectively, with minimal parameter overhead and faster\nconvergence. The framework particularly excels in cross-domain settings,\ndelivering 2.4 and 9.3 zero-shot improvements on PACS and VLCS. Human\nevaluations further confirm its cognitive alignment, outperforming\nresult-oriented methods by 2.7 times in semantic consistency metrics.", "AI": {"tldr": "\u73b0\u6709\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08VFMs\uff09\u5728\u77e5\u8bc6\u8fc1\u79fb\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u800c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u64c5\u957f\u8de8\u6a21\u6001\u4ea4\u4e92\u3002\u672c\u6587\u63d0\u51fa\u7684\u5b66\u4e60\u4ea4\u4e92\uff08LFI\uff09\u6846\u67b6\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u89c6\u89c9\u7406\u89e3\u4f5c\u4e3a\u4ea4\u4e92\u8fc7\u7a0b\uff0c\u5e76\u5229\u7528\u4ea4\u4e92\u67e5\u8be2\u548c\u57fa\u4e8e\u4ea4\u4e92\u7684\u76d1\u7763\uff0c\u5b9e\u73b0\u4e86\u66f4\u6709\u6548\u7684\u77e5\u8bc6\u8fc1\u79fb\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\uff0c\u5c24\u5176\u5728\u8de8\u57df\u8bbe\u7f6e\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u65b9\u9762\u8868\u73b0\u7a81\u51fa\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08VFMs\uff09\u5728\u4ece\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u8fc1\u79fb\u77e5\u8bc6\u65b9\u9762\u5b58\u5728\u6839\u672c\u6027\u9650\u5236\uff0c\u56e0\u4e3a\u73b0\u6709\u7684VFMs\u4e3b\u8981\u91c7\u7528\u5ffd\u89c6\u4ea4\u4e92\u8fc7\u7a0b\u7684\u9762\u5411\u7ed3\u679c\u7684\u8303\u5f0f\uff0c\u800cVLMs\u64c5\u957f\u901a\u8fc7\u7edf\u4e00\u7684\u8868\u793a\u7a7a\u95f4\u6765\u6a21\u62df\u8de8\u6a21\u6001\u4ea4\u4e92\u3002\u8fd9\u79cd\u8868\u793a\u4e0a\u7684\u5dee\u5f02\u963b\u788d\u4e86\u6709\u6548\u7684\u77e5\u8bc6\u8fc1\u79fb\uff0c\u5e76\u9650\u5236\u4e86\u5728\u4e0d\u540c\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u5b66\u4e60\u4ea4\u4e92\u201d\uff08LFI\uff09\u7684\u8ba4\u77e5\u542f\u53d1\u5f0f\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u663e\u5f0f\u5730\u5c06\u89c6\u89c9\u7406\u89e3\u5efa\u6a21\u4e3a\u4e00\u4e2a\u4ea4\u4e92\u8fc7\u7a0b\u6765\u89e3\u51b3\u4e0a\u8ff0\u5dee\u8ddd\u3002\u5176\u6838\u5fc3\u5728\u4e8e\uff0c\u6355\u6349\u9884\u8bad\u7ec3VLMs\u4e2d\u7f16\u7801\u7684\u52a8\u6001\u4ea4\u4e92\u6a21\u5f0f\uff0c\u53ef\u4ee5\u5b9e\u73b0\u66f4\u51c6\u786e\u3001\u66f4\u9ad8\u6548\u7684\u77e5\u8bc6\u8fc1\u79fb\u5230VFMs\u3002\u8be5\u65b9\u6cd5\u4fa7\u91cd\u4e8e\u4e24\u9879\u6280\u672f\u521b\u65b0\uff1a\u4ea4\u4e92\u67e5\u8be2\uff08Interaction Queries\uff09\uff0c\u5b83\u5728\u7f51\u7edc\u5c42\u4e4b\u95f4\u4fdd\u6301\u6301\u4e45\u7684\u5173\u7cfb\u7ed3\u6784\uff1b\u4ee5\u53ca\u57fa\u4e8e\u4ea4\u4e92\u7684\u76d1\u7763\uff08interaction-based supervision\uff09\uff0c\u5b83\u6e90\u81eaVLMs\u7684\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u673a\u5236\u3002", "result": "\u901a\u8fc7\u5168\u9762\u7684\u5b9e\u9a8c\u8bc1\u660e\uff0cLFI\u6846\u67b6\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6301\u7eed\u7684\u6539\u8fdb\uff0c\u5728TinyImageNet\u5206\u7c7b\u4efb\u52a1\u4e0a\u63d0\u5347\u4e863.3\u4e2a\u7edd\u5bf9mAP\u70b9\uff0c\u5728COCO\u68c0\u6d4b/\u5206\u5272\u4efb\u52a1\u4e0a\u63d0\u5347\u4e861.6\u4e2amAP\u70b9/2.4\u4e2aAP\u70b9\uff0c\u540c\u65f6\u53c2\u6570\u5f00\u9500\u548c\u6536\u655b\u901f\u5ea6\u4fdd\u6301\u5728\u8f83\u4f4e\u6c34\u5e73\u3002\u8be5\u6846\u67b6\u5728\u8de8\u57df\u8bbe\u7f6e\u4e0b\u8868\u73b0\u5c24\u4e3a\u51fa\u8272\uff0c\u5728PACS\u548cVLCS\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e862.4\u548c9.3\u4e2a\u767e\u5206\u70b9\u7684\u96f6\u6837\u672c\uff08zero-shot\uff09\u63d0\u5347\u3002\u6b64\u5916\uff0c\u4eba\u7c7b\u8bc4\u4f30\u8fdb\u4e00\u6b65\u8bc1\u5b9e\u4e86\u5176\u8ba4\u77e5\u5bf9\u9f50\u80fd\u529b\uff0c\u5728\u8bed\u4e49\u4e00\u81f4\u6027\u6307\u6807\u4e0a\uff0c\u5176\u8868\u73b0\u6bd4\u9762\u5411\u7ed3\u679c\u7684\u65b9\u6cd5\u9ad8\u51fa2.7\u500d\u3002", "conclusion": "LFI\u6846\u67b6\u901a\u8fc7\u6a21\u62df\u4ea4\u4e92\u8fc7\u7a0b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u5728\u77e5\u8bc6\u8fc1\u79fb\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u7684\u6311\u6218\uff0c\u5e76\u5728\u5404\u9879\u4efb\u52a1\u548c\u8bc4\u4f30\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u89c6\u89c9\u7406\u89e3\u548c\u8de8\u6a21\u6001\u5b66\u4e60\u9886\u57df\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.18738", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18738", "abs": "https://arxiv.org/abs/2509.18738", "authors": ["Ruichao Hou", "Xingyuan Li", "Tongwei Ren", "Dongming Zhou", "Gangshan Wu", "Jinde Cao"], "title": "HyPSAM: Hybrid Prompt-driven Segment Anything Model for RGB-Thermal Salient Object Detection", "comment": null, "summary": "RGB-thermal salient object detection (RGB-T SOD) aims to identify prominent\nobjects by integrating complementary information from RGB and thermal\nmodalities. However, learning the precise boundaries and complete objects\nremains challenging due to the intrinsic insufficient feature fusion and the\nextrinsic limitations of data scarcity. In this paper, we propose a novel\nhybrid prompt-driven segment anything model (HyPSAM), which leverages the\nzero-shot generalization capabilities of the segment anything model (SAM) for\nRGB-T SOD. Specifically, we first propose a dynamic fusion network (DFNet) that\ngenerates high-quality initial saliency maps as visual prompts. DFNet employs\ndynamic convolution and multi-branch decoding to facilitate adaptive\ncross-modality interaction, overcoming the limitations of fixed-parameter\nkernels and enhancing multi-modal feature representation. Moreover, we propose\na plug-and-play refinement network (P2RNet), which serves as a general\noptimization strategy to guide SAM in refining saliency maps by using hybrid\nprompts. The text prompt ensures reliable modality input, while the mask and\nbox prompts enable precise salient object localization. Extensive experiments\non three public datasets demonstrate that our method achieves state-of-the-art\nperformance. Notably, HyPSAM has remarkable versatility, seamlessly integrating\nwith different RGB-T SOD methods to achieve significant performance gains,\nthereby highlighting the potential of prompt engineering in this field. The\ncode and results of our method are available at:\nhttps://github.com/milotic233/HyPSAM.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHyPSAM\u7684\u65b0\u578b\u6df7\u5408\u63d0\u793a\u9a71\u52a8\u5206\u5272\u4efb\u4f55\u6a21\u578b\uff0c\u7528\u4e8eRGB-T\u663e\u7740\u76ee\u6807\u68c0\u6d4b\uff0c\u901a\u8fc7\u52a8\u6001\u878d\u5408\u7f51\u7edc\uff08DFNet\uff09\u751f\u6210\u521d\u59cb\u663e\u7740\u56fe\u4f5c\u4e3a\u63d0\u793a\uff0c\u5e76\u901a\u8fc7\u5373\u63d2\u5373\u7528\u7cbe\u70bc\u7f51\u7edc\uff08P2RNet\uff09\u5229\u7528\u6587\u672c\u3001\u63a9\u7801\u548c\u8fb9\u754c\u6846\u63d0\u793a\u6765\u4f18\u5316\u5206\u5272\u7ed3\u679c\uff0c\u5728\u4e09\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "RGB-T\u663e\u7740\u76ee\u6807\u68c0\u6d4b\uff08RGB-T SOD\uff09\u65e8\u5728\u901a\u8fc7\u878d\u5408RGB\u548c\u70ed\u529b\u5b66\u6a21\u6001\u6765\u8bc6\u522b\u663e\u8457\u7269\u4f53\uff0c\u4f46\u7531\u4e8e\u7279\u5f81\u878d\u5408\u4e0d\u8db3\u548c\u6570\u636e\u7a00\u7f3a\u6027\uff0c\u7cbe\u786e\u8fb9\u754c\u548c\u5b8c\u6574\u7269\u4f53\u7684\u5b66\u4e60\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHyPSAM\u7684\u65b0\u578b\u6df7\u5408\u63d0\u793a\u9a71\u52a8\u5206\u5272\u4efb\u4f55\u6a21\u578b\uff08HyPSAM\uff09\uff0c\u5229\u7528\u5206\u5272\u4efb\u4f55\u6a21\u578b\uff08SAM\uff09\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002\u9996\u5148\u63d0\u51fa\u4e00\u4e2a\u52a8\u6001\u878d\u5408\u7f51\u7edc\uff08DFNet\uff09\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u521d\u59cb\u663e\u7740\u56fe\u4f5c\u4e3a\u89c6\u89c9\u63d0\u793a\uff0cDFNet\u91c7\u7528\u52a8\u6001\u5377\u79ef\u548c\u591a\u5206\u652f\u89e3\u7801\u3002\u7136\u540e\u63d0\u51fa\u4e00\u4e2a\u5373\u63d2\u5373\u7528\u7cbe\u70bc\u7f51\u7edc\uff08P2RNet\uff09\uff0c\u5229\u7528\u6df7\u5408\u63d0\u793a\uff08\u6587\u672c\u63d0\u793a\u3001\u63a9\u7801\u63d0\u793a\u548c\u8fb9\u754c\u6846\u63d0\u793a\uff09\u6765\u6307\u5bfcSAM\u7cbe\u70bc\u663e\u7740\u56fe\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002HyPSAM\u5177\u6709\u51fa\u8272\u7684\u591a\u529f\u80fd\u6027\uff0c\u53ef\u4ee5\u4e0e\u4e0d\u540c\u7684RGB-T SOD\u65b9\u6cd5\u65e0\u7f1d\u96c6\u6210\uff0c\u5b9e\u73b0\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "HyPSAM\u901a\u8fc7\u6df7\u5408\u63d0\u793a\u5de5\u7a0b\u5b9e\u73b0\u4e86RGB-T\u663e\u7740\u76ee\u6807\u68c0\u6d4b\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u8a72\u9886\u57df\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.18457", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18457", "abs": "https://arxiv.org/abs/2509.18457", "authors": ["Ebrahim Farahmand", "Reza Rahimi Azghan", "Nooshin Taheri Chatrudi", "Velarie Yaa Ansu-Baidoo", "Eric Kim", "Gautham Krishna Gudur", "Mohit Malu", "Owen Krueger", "Edison Thomaz", "Giulia Pedrielli", "Pavan Turaga", "Hassan Ghasemzadeh"], "title": "GluMind: Multimodal Parallel Attention and Knowledge Retention for Robust Cross-Population Blood Glucose Forecasting", "comment": null, "summary": "This paper proposes GluMind, a transformer-based multimodal framework\ndesigned for continual and long-term blood glucose forecasting. GluMind devises\ntwo attention mechanisms, including cross-attention and multi-scale attention,\nwhich operate in parallel and deliver accurate predictive performance.\nCross-attention effectively integrates blood glucose data with other\nphysiological and behavioral signals such as activity, stress, and heart rate,\naddressing challenges associated with varying sampling rates and their adverse\nimpacts on robust prediction. Moreover, the multi-scale attention mechanism\ncaptures long-range temporal dependencies. To mitigate catastrophic forgetting,\nGluMind incorporates a knowledge retention technique into the transformer-based\nforecasting model. The knowledge retention module not only enhances the model's\nability to retain prior knowledge but also boosts its overall forecasting\nperformance. We evaluate GluMind on the recently released AIREADI dataset,\nwhich contains behavioral and physiological data collected from healthy people,\nindividuals with prediabetes, and those with type 2 diabetes. We examine the\nperformance stability and adaptability of GluMind in learning continuously as\nnew patient cohorts are introduced. Experimental results show that GluMind\nconsistently outperforms other state-of-the-art forecasting models, achieving\napproximately 15% and 9% improvements in root mean squared error (RMSE) and\nmean absolute error (MAE), respectively.", "AI": {"tldr": "GluMind\u662f\u4e00\u4e2a\u57fa\u4e8eTransformer\u7684\u591a\u6a21\u6001\u6846\u67b6\uff0c\u7528\u4e8e\u6301\u7eed\u548c\u957f\u671f\u7684\u8840\u7cd6\u9884\u6d4b\uff0c\u901a\u8fc7\u7ed3\u5408\u4ea4\u53c9\u6ce8\u610f\u529b\u548c\u591a\u5c3a\u5ea6\u6ce8\u610f\u529b\u673a\u5236\u6765\u6574\u5408\u751f\u7406\u548c\u884c\u4e3a\u4fe1\u53f7\uff0c\u5e76\u91c7\u7528\u77e5\u8bc6\u4fdd\u7559\u6280\u672f\u6765\u9632\u6b62\u707e\u96be\u6027\u9057\u5fd8\uff0c\u5728AIREADI\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u8840\u7cd6\u9884\u6d4b\u6a21\u578b\u5728\u5904\u7406\u591a\u6e90\u5f02\u6784\u6570\u636e\u3001\u6355\u6349\u957f\u671f\u4f9d\u8d56\u6027\u4ee5\u53ca\u6301\u7eed\u5b66\u4e60\u80fd\u529b\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u6574\u5408\u751f\u7406\u548c\u884c\u4e3a\u4fe1\u53f7\u8fdb\u884c\u957f\u671f\u9884\u6d4b\u7684\u573a\u666f\u4e0b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGluMind\u7684Transformer-based\u591a\u6a21\u6001\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7684\u6ce8\u610f\u529b\u673a\u5236\uff1a\u4ea4\u53c9\u6ce8\u610f\u673a\u5236\uff08\u7528\u4e8e\u6574\u5408\u4e0d\u540c\u91c7\u6837\u7387\u7684\u8840\u7cd6\u3001\u6d3b\u52a8\u3001\u538b\u529b\u3001\u5fc3\u7387\u7b49\u4fe1\u53f7\uff09\u548c\u591a\u5c3a\u5ea6\u6ce8\u610f\u529b\u673a\u5236\uff08\u7528\u4e8e\u6355\u6349\u957f\u671f\u65f6\u95f4\u4f9d\u8d56\u6027\uff09\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u4e00\u4e2a\u77e5\u8bc6\u4fdd\u7559\u6a21\u5757\u6765\u7f13\u89e3\u707e\u96be\u6027\u9057\u5fd8\uff0c\u5e76\u63d0\u5347\u6574\u4f53\u9884\u6d4b\u6027\u80fd\u3002", "result": "\u5728AIREADI\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cGluMind\u5728\u8840\u7cd6\u9884\u6d4b\u4efb\u52a1\u4e0a\uff0c\u4e0e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u6a21\u578b\u76f8\u6bd4\uff0c\u5747\u65b9\u6839\u8bef\u5dee\uff08RMSE\uff09\u548c\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\uff08MAE\uff09\u5206\u522b\u63d0\u9ad8\u4e86\u7ea615%\u548c9%\u3002\u6a21\u578b\u5728\u5f15\u5165\u65b0\u7684\u60a3\u8005\u961f\u5217\u65f6\uff0c\u5c55\u73b0\u4e86\u7a33\u5b9a\u7684\u6027\u80fd\u548c\u826f\u597d\u7684\u9002\u5e94\u6027\u3002", "conclusion": "GluMind\u5728\u6301\u7eed\u548c\u957f\u671f\u7684\u8840\u7cd6\u9884\u6d4b\u65b9\u9762\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6027\u80fd\uff0c\u5176\u591a\u6a21\u6001\u878d\u5408\u80fd\u529b\u548c\u77e5\u8bc6\u4fdd\u7559\u673a\u5236\u4f7f\u5176\u80fd\u591f\u6709\u6548\u5904\u7406\u590d\u6742\u6570\u636e\u5e76\u9002\u5e94\u6301\u7eed\u5b66\u4e60\u573a\u666f\uff0c\u4e3a\u4e2a\u6027\u5316\u7cd6\u5c3f\u75c5\u7ba1\u7406\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.18743", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18743", "abs": "https://arxiv.org/abs/2509.18743", "authors": ["Susmit Neogi"], "title": "TriFusion-AE: Language-Guided Depth and LiDAR Fusion for Robust Point Cloud Processing", "comment": "39th Conference on Neural Information Processing Systems (NeurIPS\n  2025) Workshop", "summary": "LiDAR-based perception is central to autonomous driving and robotics, yet raw\npoint clouds remain highly vulnerable to noise, occlusion, and adversarial\ncorruptions. Autoencoders offer a natural framework for denoising and\nreconstruction, but their performance degrades under challenging real-world\nconditions. In this work, we propose TriFusion-AE, a multimodal cross-attention\nautoencoder that integrates textual priors, monocular depth maps from\nmulti-view images, and LiDAR point clouds to improve robustness. By aligning\nsemantic cues from text, geometric (depth) features from images, and spatial\nstructure from LiDAR, TriFusion-AE learns representations that are resilient to\nstochastic noise and adversarial perturbations. Interestingly, while showing\nlimited gains under mild perturbations, our model achieves significantly more\nrobust reconstruction under strong adversarial attacks and heavy noise, where\nCNN-based autoencoders collapse. We evaluate on the nuScenes-mini dataset to\nreflect realistic low-data deployment scenarios. Our multimodal fusion\nframework is designed to be model-agnostic, enabling seamless integration with\nany CNN-based point cloud autoencoder for joint representation learning.", "AI": {"tldr": "TriFusion-AE\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u81ea\u7f16\u7801\u5668\uff0c\u7ed3\u5408\u4e86\u6587\u672c\u3001\u6df1\u5ea6\u56fe\u548cLiDAR\u70b9\u4e91\uff0c\u63d0\u9ad8\u4e86\u5bf9\u566a\u58f0\u548c\u5bf9\u6297\u6027\u6270\u52a8\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8eLiDAR\u7684\u611f\u77e5\u65b9\u6cd5\u5bb9\u6613\u53d7\u5230\u566a\u58f0\u3001\u906e\u6321\u548c\u5bf9\u6297\u6027\u7834\u574f\u7684\u5f71\u54cd\uff0c\u800c\u4f20\u7edf\u7684\u81ea\u7f16\u7801\u5668\u5728\u73b0\u5b9e\u4e16\u754c\u7684\u6311\u6218\u6027\u6761\u4ef6\u4e0b\u6027\u80fd\u4f1a\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTriFusion-AE\u7684\u591a\u6a21\u6001\u4ea4\u53c9\u6ce8\u610f\u529b\u81ea\u7f16\u7801\u5668\uff0c\u8be5\u6a21\u578b\u96c6\u6210\u4e86\u6587\u672c\u5148\u9a8c\u3001\u6765\u81ea\u591a\u89c6\u56fe\u56fe\u50cf\u7684\u5355\u76ee\u6df1\u5ea6\u56fe\u548cLiDAR\u70b9\u4e91\uff0c\u901a\u8fc7\u5bf9\u9f50\u8bed\u4e49\u7ebf\u7d22\u3001\u51e0\u4f55\u7279\u5f81\u548c\u7a7a\u95f4\u7ed3\u6784\u6765\u5b66\u4e60\u5bf9\u6270\u52a8\u5177\u6709\u5f39\u6027\u7684\u8868\u793a\u3002", "result": "\u5728\u5f3a\u5bf9\u6297\u6027\u653b\u51fb\u548c\u91cd\u566a\u58f0\u6761\u4ef6\u4e0b\uff0cTriFusion-AE\u5b9e\u73b0\u4e86\u6bd4\u57fa\u4e8eCNN\u7684\u81ea\u7f16\u7801\u5668\u66f4\u9c81\u68d2\u7684\u91cd\u5efa\uff0c\u4f46\u5728\u8f7b\u5ea6\u6270\u52a8\u4e0b\u6548\u679c\u63d0\u5347\u6709\u9650\u3002\u5728nuScenes-mini\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "conclusion": "TriFusion-AE\u901a\u8fc7\u878d\u5408\u591a\u6a21\u6001\u4fe1\u606f\u663e\u8457\u63d0\u9ad8\u4e86LiDAR\u611f\u77e5\u5bf9\u566a\u58f0\u548c\u5bf9\u6297\u6027\u653b\u51fb\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u4e14\u8be5\u6846\u67b6\u53ef\u4ee5\u4e0e\u4efb\u4f55\u57fa\u4e8eCNN\u7684\u70b9\u4e91\u81ea\u7f16\u7801\u5668\u96c6\u6210\u3002"}}
{"id": "2509.18469", "categories": ["cs.LG", "q-bio.NC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.18469", "abs": "https://arxiv.org/abs/2509.18469", "authors": ["Han-Lin Hsieh", "Maryam M. Shanechi"], "title": "Probabilistic Geometric Principal Component Analysis with application to neural data", "comment": "Published at the International Conference on Learning Representations\n  (ICLR) 2025. Code is available at GitHub\n  https://github.com/ShanechiLab/PGPCA.git", "summary": "Dimensionality reduction is critical across various domains of science\nincluding neuroscience. Probabilistic Principal Component Analysis (PPCA) is a\nprominent dimensionality reduction method that provides a probabilistic\napproach unlike the deterministic approach of PCA and serves as a connection\nbetween PCA and Factor Analysis (FA). Despite their power, PPCA and its\nextensions are mainly based on linear models and can only describe the data in\na Euclidean coordinate system. However, in many neuroscience applications, data\nmay be distributed around a nonlinear geometry (i.e., manifold) rather than\nlying in the Euclidean space. We develop Probabilistic Geometric Principal\nComponent Analysis (PGPCA) for such datasets as a new dimensionality reduction\nalgorithm that can explicitly incorporate knowledge about a given nonlinear\nmanifold that is first fitted from these data. Further, we show how in addition\nto the Euclidean coordinate system, a geometric coordinate system can be\nderived for the manifold to capture the deviations of data from the manifold\nand noise. We also derive a data-driven EM algorithm for learning the PGPCA\nmodel parameters. As such, PGPCA generalizes PPCA to better describe data\ndistributions by incorporating a nonlinear manifold geometry. In simulations\nand brain data analyses, we show that PGPCA can effectively model the data\ndistribution around various given manifolds and outperforms PPCA for such data.\nMoreover, PGPCA provides the capability to test whether the new geometric\ncoordinate system better describes the data than the Euclidean one. Finally,\nPGPCA can perform dimensionality reduction and learn the data distribution both\naround and on the manifold. These capabilities make PGPCA valuable for\nenhancing the efficacy of dimensionality reduction for analysis of\nhigh-dimensional data that exhibit noise and are distributed around a nonlinear\nmanifold.", "AI": {"tldr": "PGPCA\u662f\u4e00\u79cd\u65b0\u7684\u964d\u7ef4\u7b97\u6cd5\uff0c\u9002\u7528\u4e8e\u5b58\u5728\u975e\u7ebf\u6027\u6d41\u5f62\u7684\u6570\u636e\uff0c\u5e76\u4e14\u53ef\u4ee5\u7ed3\u5408\u6d41\u5f62\u77e5\u8bc6\uff0c\u540c\u65f6\u8003\u8651\u6d41\u5f62\u4e0a\u548c\u6d41\u5f62\u5916\u7684\u51e0\u4f55\u5750\u6807\u7cfb\u3002", "motivation": "\u73b0\u6709\u7684PPCA\u548c\u5176\u6269\u5c55\u4e3b\u8981\u57fa\u4e8e\u7ebf\u6027\u6a21\u578b\uff0c\u53ea\u80fd\u5728\u6b27\u51e0\u91cc\u5f97\u5750\u6807\u7cfb\u4e0b\u63cf\u8ff0\u6570\u636e\uff0c\u65e0\u6cd5\u5f88\u597d\u5730\u5904\u7406\u795e\u7ecf\u79d1\u5b66\u7b49\u9886\u57df\u4e2d\u5b58\u5728\u975e\u7ebf\u6027\u6d41\u5f62\u5206\u5e03\u7684\u6570\u636e\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6982\u7387\u51e0\u4f55\u4e3b\u6210\u5206\u5206\u6790\uff08PGPCA\uff09\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u53ef\u4ee5\u663e\u5f0f\u5730\u7ed3\u5408\u5df2\u77e5\u7684\u975e\u7ebf\u6027\u6d41\u5f62\u77e5\u8bc6\uff0c\u5e76\u4ece\u4e2d\u62df\u5408\u6d41\u5f62\u3002PGPCA\u53ef\u4ee5\u5bfc\u51fa\u51e0\u4f55\u5750\u6807\u7cfb\u6765\u6355\u6349\u6570\u636e\u504f\u79bb\u6d41\u5f62\u548c\u566a\u58f0\uff0c\u5e76\u63d0\u4f9b\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7684EM\u7b97\u6cd5\u6765\u5b66\u4e60\u6a21\u578b\u53c2\u6570\u3002", "result": "\u5728\u6a21\u62df\u548c\u5927\u8111\u6570\u636e\u5206\u6790\u4e2d\uff0cPGPCA\u80fd\u591f\u6709\u6548\u5730\u6a21\u62df\u6570\u636e\u5728\u5404\u79cd\u7ed9\u5b9a\u6d41\u5f62\u5468\u56f4\u7684\u5206\u5e03\uff0c\u5e76\u4e14\u5728\u8fd9\u4e9b\u6570\u636e\u4e0a\u4f18\u4e8ePPCA\u3002PGPCA\u8fd8\u53ef\u4ee5\u6d4b\u8bd5\u51e0\u4f55\u5750\u6807\u7cfb\u662f\u5426\u6bd4\u6b27\u51e0\u91cc\u5f97\u5750\u6807\u7cfb\u66f4\u80fd\u63cf\u8ff0\u6570\u636e\uff0c\u5e76\u540c\u65f6\u5728\u6d41\u5f62\u4e0a\u548c\u6d41\u5f62\u5468\u56f4\u8fdb\u884c\u964d\u7ef4\u548c\u6570\u636e\u5206\u5e03\u5b66\u4e60\u3002", "conclusion": "PGPCA\u901a\u8fc7\u7ed3\u5408\u975e\u7ebf\u6027\u6d41\u5f62\u51e0\u4f55\uff0c\u6cdb\u5316\u4e86PPCA\uff0c\u80fd\u591f\u66f4\u597d\u5730\u63cf\u8ff0\u6570\u636e\u5206\u5e03\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5b58\u5728\u566a\u58f0\u548c\u5206\u5e03\u5728\u975e\u7ebf\u6027\u6d41\u5f62\u5468\u56f4\u7684\u9ad8\u7ef4\u6570\u636e\u5206\u6790\u3002"}}
{"id": "2509.18754", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18754", "abs": "https://arxiv.org/abs/2509.18754", "authors": ["Yuyang Liu", "Xinyuan Shi", "Bang Yang", "Peilin Zhou", "Jiahua Dong", "Long Chen", "Ian Reid", "Xiaondan Liang"], "title": "COLT: Enhancing Video Large Language Models with Continual Tool Usage", "comment": "16 pages", "summary": "The success of Large Language Models (LLMs) has significantly propelled the\nresearch of video understanding. To harvest the benefits of well-trained expert\nmodels (i.e., tools), video LLMs prioritize the exploration of tool usage\ncapabilities. Existing methods either prompt closed-source LLMs or employ the\ninstruction tuning paradigm for tool-use fine-tuning. These methods, however,\nassume an established repository of fixed tools and struggle to generalize to\nreal-world environments where tool data is perpetually evolving and streaming\nin. To this end, we propose to enhance open-source video LLMs with COntinuaL\nTool usage (termed COLT), which automatically acquires tool-use ability in a\nsuccessive tool stream without suffering 'catastrophic forgetting' of the past\nlearned tools. Specifically, our COLT incorporates a learnable tool codebook as\na tool-specific memory system. Then relevant tools are dynamically selected\nbased on the similarity between user instruction and tool features within the\ncodebook. To unleash the tool usage potential of video LLMs, we collect a\nvideo-centric tool-use instruction tuning dataset VideoToolBench. Extensive\nexperiments on both previous video LLM benchmarks and the tool-use-specific\nVideoToolBench dataset demonstrate the state-of-the-art performance of our\nproposed COLT.", "AI": {"tldr": "COLT\u662f\u4e00\u4e2a\u6301\u7eed\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u589e\u5f3a\u5f00\u6e90\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5de5\u5177\u4f7f\u7528\u80fd\u529b\uff0c\u4f7f\u5176\u80fd\u591f\u5904\u7406\u4e0d\u65ad\u53d8\u5316\u7684\u5de5\u5177\u6d41\uff0c\u540c\u65f6\u907f\u514d\u9057\u5fd8\u5df2\u5b66\u77e5\u8bc6\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u7406\u89e3\u65b9\u6cd5\u5728\u5de5\u5177\u4f7f\u7528\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5b83\u4eec\u4f9d\u8d56\u56fa\u5b9a\u7684\u5de5\u5177\u5e93\uff0c\u96be\u4ee5\u9002\u5e94\u73b0\u5b9e\u4e16\u754c\u4e2d\u4e0d\u65ad\u53d8\u5316\u7684\u5de5\u5177\u6570\u636e\u6d41\u3002\u4e3a\u89e3\u51b3\u6b64\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6301\u7eed\u5b66\u4e60\u65b0\u5de5\u5177\u5e76\u4fdd\u7559\u65e7\u5de5\u5177\u80fd\u529b\u7684\u65b9\u6cd5\u3002", "method": "COLT\u6846\u67b6\u5f15\u5165\u4e86\u4e00\u4e2a\u53ef\u5b66\u4e60\u7684\u5de5\u5177\u4ee3\u7801\u5e93\u4f5c\u4e3a\u5185\u5b58\u7cfb\u7edf\uff0c\u7528\u4e8e\u5b58\u50a8\u548c\u68c0\u7d22\u5de5\u5177\u3002\u8be5\u7cfb\u7edf\u80fd\u591f\u6839\u636e\u7528\u6237\u6307\u4ee4\u4e0e\u5de5\u5177\u7279\u5f81\u7684\u76f8\u4f3c\u5ea6\u52a8\u6001\u9009\u62e9\u76f8\u5173\u5de5\u5177\uff0c\u5e76\u901a\u8fc7\u6301\u7eed\u5b66\u4e60\u673a\u5236\u4e0d\u65ad\u66f4\u65b0\u548c\u6269\u5c55\u5de5\u5177\u4f7f\u7528\u80fd\u529b\uff0c\u540c\u65f6\u9632\u6b62\u2018\u707e\u96be\u6027\u9057\u5fd8\u2019\u3002", "result": "\u5728\u73b0\u6709\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u57fa\u51c6\u548c\u4e13\u95e8\u7684\u89c6\u9891\u5de5\u5177\u4f7f\u7528\u6307\u4ee4\u8c03\u4f18\u6570\u636e\u96c6VideoToolBench\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cCOLT\u6846\u67b6\u5728\u89c6\u9891LLM\u7684\u5de5\u5177\u4f7f\u7528\u80fd\u529b\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "COLT\u901a\u8fc7\u5f15\u5165\u6301\u7eed\u5b66\u4e60\u673a\u5236\u548c\u5de5\u5177\u4ee3\u7801\u5e93\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u5728\u52a8\u6001\u5de5\u5177\u73af\u5883\u4e0b\u7684\u5de5\u5177\u4f7f\u7528\u6cdb\u5316\u80fd\u529b\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5176\u5904\u7406\u548c\u5229\u7528\u4e0d\u65ad\u66f4\u65b0\u7684\u5de5\u5177\u7684\u80fd\u529b\u3002"}}
{"id": "2509.18470", "categories": ["cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.18470", "abs": "https://arxiv.org/abs/2509.18470", "authors": ["Xiaozhou Tan", "Minghui Zhao", "Mattias Cross", "Anton Ragni"], "title": "Discrete-time diffusion-like models for speech synthesis", "comment": null, "summary": "Diffusion models have attracted a lot of attention in recent years. These\nmodels view speech generation as a continuous-time process. For efficient\ntraining, this process is typically restricted to additive Gaussian noising,\nwhich is limiting. For inference, the time is typically discretized, leading to\nthe mismatch between continuous training and discrete sampling conditions.\nRecently proposed discrete-time processes, on the other hand, usually do not\nhave these limitations, may require substantially fewer inference steps, and\nare fully consistent between training/inference conditions. This paper explores\nsome diffusion-like discrete-time processes and proposes some new variants.\nThese include processes applying additive Gaussian noise, multiplicative\nGaussian noise, blurring noise and a mixture of blurring and Gaussian noises.\nThe experimental results suggest that discrete-time processes offer comparable\nsubjective and objective speech quality to their widely popular continuous\ncounterpart, with more efficient and consistent training and inference schemas.", "AI": {"tldr": "\u79bb\u6563\u65f6\u95f4\u6269\u6563\u6a21\u578b\u5728\u8bed\u97f3\u751f\u6210\u65b9\u9762\u53ef\u4ee5\u8fbe\u5230\u4e0e\u8fde\u7eed\u65f6\u95f4\u6a21\u578b\u76f8\u5f53\u7684\u8d28\u91cf\uff0c\u540c\u65f6\u5177\u6709\u66f4\u9ad8\u6548\u548c\u4e00\u81f4\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u3002", "motivation": "\u73b0\u6709\u7684\u8fde\u7eed\u65f6\u95f4\u6269\u6563\u6a21\u578b\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u8fc7\u7a0b\u4e2d\u5b58\u5728\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u5e76\u4e14\u53ef\u80fd\u9700\u8981\u8f83\u591a\u7684\u63a8\u7406\u6b65\u6570\u3002", "method": "\u63d0\u51fa\u5e76\u63a2\u7d22\u4e86\u51e0\u79cd\u79bb\u6563\u65f6\u95f4\u6269\u6563\u8fc7\u7a0b\uff0c\u5305\u62ec\u52a0\u6027\u9ad8\u65af\u566a\u58f0\u3001\u4e58\u6027\u9ad8\u65af\u566a\u58f0\u3001\u6a21\u7cca\u566a\u58f0\u4ee5\u53ca\u6a21\u7cca\u548c\u9ad8\u65af\u566a\u58f0\u7684\u6df7\u5408\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u79bb\u6563\u65f6\u95f4\u8fc7\u7a0b\u5728\u8bed\u97f3\u751f\u6210\u65b9\u9762\u53ef\u4ee5\u8fbe\u5230\u4e0e\u8fde\u7eed\u65f6\u95f4\u6a21\u578b\u76f8\u5f53\u7684\u4e3b\u89c2\u548c\u5ba2\u89c2\u8d28\u91cf\uff0c\u5e76\u4e14\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u65b9\u9762\u66f4\u52a0\u9ad8\u6548\u548c\u4e00\u81f4\u3002", "conclusion": "\u79bb\u6563\u65f6\u95f4\u6269\u6563\u8fc7\u7a0b\u662f\u8bed\u97f3\u751f\u6210\u7684\u4e00\u4e2a\u6709\u524d\u666f\u7684\u65b9\u5411\uff0c\u53ef\u4ee5\u514b\u670d\u8fde\u7eed\u65f6\u95f4\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u4f9b\u66f4\u4f18\u7684\u6027\u80fd\u3002"}}
{"id": "2509.18759", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18759", "abs": "https://arxiv.org/abs/2509.18759", "authors": ["Zhaorui Wang", "Yi Gu", "Deming Zhou", "Renjing Xu"], "title": "FixingGS: Enhancing 3D Gaussian Splatting via Training-Free Score Distillation", "comment": null, "summary": "Recently, 3D Gaussian Splatting (3DGS) has demonstrated remarkable success in\n3D reconstruction and novel view synthesis. However, reconstructing 3D scenes\nfrom sparse viewpoints remains highly challenging due to insufficient visual\ninformation, which results in noticeable artifacts persisting across the 3D\nrepresentation. To address this limitation, recent methods have resorted to\ngenerative priors to remove artifacts and complete missing content in\nunder-constrained areas. Despite their effectiveness, these approaches struggle\nto ensure multi-view consistency, resulting in blurred structures and\nimplausible details. In this work, we propose FixingGS, a training-free method\nthat fully exploits the capabilities of the existing diffusion model for\nsparse-view 3DGS reconstruction enhancement. At the core of FixingGS is our\ndistillation approach, which delivers more accurate and cross-view coherent\ndiffusion priors, thereby enabling effective artifact removal and inpainting.\nIn addition, we propose an adaptive progressive enhancement scheme that further\nrefines reconstructions in under-constrained regions. Extensive experiments\ndemonstrate that FixingGS surpasses existing state-of-the-art methods with\nsuperior visual quality and reconstruction performance. Our code will be\nreleased publicly.", "AI": {"tldr": "FixingGS\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u589e\u5f3a\u7a00\u758f\u89c6\u89d2\u76843D\u9ad8\u65af\u6cfc\u6e85\u91cd\u5efa\uff0c\u901a\u8fc7\u84b8\u998f\u548c\u81ea\u9002\u5e94\u7684\u6e10\u8fdb\u5f0f\u589e\u5f3a\u6765\u6d88\u9664\u4f2a\u5f71\u548c\u4fee\u590d\u7f3a\u5931\u5185\u5bb9\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u8d28\u91cf\u7684\u91cd\u5efa\u3002", "motivation": "\u73b0\u6709\u76843D\u9ad8\u65af\u6cfc\u6e85\u65b9\u6cd5\u5728\u7a00\u758f\u89c6\u89d2\u91cd\u5efa\u65f6\u5b58\u5728\u4f2a\u5f71\uff0c\u800c\u5f15\u5165\u751f\u6210\u5f0f\u5148\u9a8c\u7684\u65b9\u6cd5\u53c8\u4f1a\u727a\u7272\u591a\u89c6\u89d2\u4e00\u81f4\u6027\uff0c\u5bfc\u81f4\u7ed3\u6784\u6a21\u7cca\u548c\u7ec6\u8282\u4e0d\u53ef\u9760\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFixingGS\u7684\u8bad\u7ec3\u65e0\u5173\u65b9\u6cd5\uff0c\u5176\u6838\u5fc3\u662f\u5229\u7528\u6269\u6563\u6a21\u578b\u8fdb\u884c\u7a00\u758f\u89c6\u89d23D\u9ad8\u65af\u6cfc\u6e85\u91cd\u5efa\u7684\u589e\u5f3a\u3002\u8be5\u65b9\u6cd5\u5305\u62ec\u4e00\u4e2a\u84b8\u998f\u65b9\u6cd5\uff0c\u4ee5\u63d0\u4f9b\u66f4\u51c6\u786e\u3001\u8de8\u89c6\u89d2\u4e00\u81f4\u7684\u6269\u6563\u5148\u9a8c\uff0c\u4ece\u800c\u6709\u6548\u6d88\u9664\u4f2a\u5f71\u548c\u8fdb\u884c\u4fee\u590d\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u4e2a\u81ea\u9002\u5e94\u7684\u6e10\u8fdb\u5f0f\u589e\u5f3a\u65b9\u6848\uff0c\u4ee5\u8fdb\u4e00\u6b65\u4f18\u5316\u5728\u7ea6\u675f\u4e0d\u8db3\u533a\u57df\u7684\u91cd\u5efa\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cFixingGS\u5728\u89c6\u89c9\u8d28\u91cf\u548c\u91cd\u5efa\u6027\u80fd\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "FixingGS\u901a\u8fc7\u5229\u7528\u6269\u6563\u6a21\u578b\u548c\u521b\u65b0\u7684\u84b8\u998f\u53ca\u589e\u5f3a\u7b56\u7565\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u7a00\u758f\u89c6\u89d23D\u9ad8\u65af\u6cfc\u6e85\u91cd\u5efa\u4e2d\u7684\u4f2a\u5f71\u548c\u591a\u89c6\u89d2\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6210\u679c\u3002"}}
{"id": "2509.18471", "categories": ["cs.LG", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.18471", "abs": "https://arxiv.org/abs/2509.18471", "authors": ["Mariano Tepper", "Ted Willke"], "title": "Individualized non-uniform quantization for vector search", "comment": null, "summary": "Embedding vectors are widely used for representing unstructured data and\nsearching through it for semantically similar items. However, the large size of\nthese vectors, due to their high-dimensionality, creates problems for modern\nvector search techniques: retrieving large vectors from memory/storage is\nexpensive and their footprint is costly. In this work, we present NVQ\n(non-uniform vector quantization), a new vector compression technique that is\ncomputationally and spatially efficient in the high-fidelity regime. The core\nin NVQ is to use novel parsimonious and computationally efficient\nnonlinearities for building non-uniform vector quantizers. Critically, these\nquantizers are \\emph{individually} learned for each indexed vector. Our\nexperimental results show that NVQ exhibits improved accuracy compared to the\nstate of the art with a minimal computational cost.", "AI": {"tldr": "NVQ\u662f\u4e00\u79cd\u65b0\u7684\u5411\u91cf\u538b\u7f29\u6280\u672f\uff0c\u901a\u8fc7\u5b66\u4e60\u5355\u72ec\u7684\u975e\u5747\u5300\u5411\u91cf\u91cf\u5316\u5668\u6765\u63d0\u9ad8\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u9ad8\u7ef4\u5411\u91cf\u7684\u5de8\u5927\u5c3a\u5bf8\u7ed9\u73b0\u4ee3\u5411\u91cf\u641c\u7d22\u6280\u672f\u5e26\u6765\u4e86\u5185\u5b58/\u5b58\u50a8\u68c0\u7d22\u6210\u672c\u9ad8\u548c\u5360\u7528\u7a7a\u95f4\u5927\u7684\u95ee\u9898\u3002", "method": "NVQ\u7684\u6838\u5fc3\u662f\u4f7f\u7528\u65b0\u9896\u7684\u3001\u8282\u7701\u8d44\u6e90\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u975e\u7ebf\u6027\u65b9\u6cd5\u6765\u6784\u5efa\u975e\u5747\u5300\u5411\u91cf\u91cf\u5316\u5668\uff0c\u5e76\u4e3a\u6bcf\u4e2a\u7d22\u5f15\u5411\u91cf\u5355\u72ec\u5b66\u4e60\u8fd9\u4e9b\u91cf\u5316\u5668\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cNVQ\u5728\u8ba1\u7b97\u6210\u672c\u5f88\u5c0f\u7684\u60c5\u51b5\u4e0b\uff0c\u6bd4\u73b0\u6709\u6280\u672f\u5177\u6709\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u3002", "conclusion": "NVQ\u662f\u4e00\u79cd\u5728\u8ba1\u7b97\u548c\u7a7a\u95f4\u4e0a\u90fd\u9ad8\u6548\u7684\u9ad8\u4fdd\u771f\u5ea6\u5411\u91cf\u538b\u7f29\u6280\u672f\u3002"}}
{"id": "2509.18763", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18763", "abs": "https://arxiv.org/abs/2509.18763", "authors": ["Xijun Wang", "Junyun Huang", "Rayyan Abdalla", "Chengyuan Zhang", "Ruiqi Xian", "Dinesh Manocha"], "title": "Bi-VLM: Pushing Ultra-Low Precision Post-Training Quantization Boundaries in Vision-Language Models", "comment": null, "summary": "We address the critical gap between the computational demands of\nvision-language models and the possible ultra-low-bit weight precision\n(bitwidth $\\leq2$ bits) we can use for higher efficiency. Our work is motivated\nby the substantial computational cost and memory requirements of VLMs, which\nrestrict their applicability in hardware-constrained environments. We propose\nBi-VLM, which separates model weights non-uniformly based on the Gaussian\nquantiles. Our formulation groups the model weights into outlier (salient) and\nmultiple inlier (unsalient) subsets, ensuring that each subset contains a\nproportion of weights corresponding to its quantile in the distribution. We\npropose a saliency-aware hybrid quantization algorithm and use it to quantize\nweights by imposing different constraints on the scaler and binary matrices\nbased on the saliency metric and compression objective. We have evaluated our\napproach on different VLMs. For the language model part of the VLM, our Bi-VLM\noutperforms the SOTA by 3%-47% on the visual question answering task in terms\nof four different benchmarks and three different models. For the overall VLM,\nour Bi-VLM outperforms the SOTA by 4%-45%. We also perform token pruning on the\nquantized models and observe that there is redundancy of image tokens 90% - 99%\nin the quantized models. This helps us to further prune the visual tokens to\nimprove efficiency.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.18480", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2509.18480", "abs": "https://arxiv.org/abs/2509.18480", "authors": ["Yuyang Wang", "Jiarui Lu", "Navdeep Jaitly", "Josh Susskind", "Miguel Angel Bautista"], "title": "SimpleFold: Folding Proteins is Simpler than You Think", "comment": "28 pages, 11 figures, 13 tables", "summary": "Protein folding models have achieved groundbreaking results typically via a\ncombination of integrating domain knowledge into the architectural blocks and\ntraining pipelines. Nonetheless, given the success of generative models across\ndifferent but related problems, it is natural to question whether these\narchitectural designs are a necessary condition to build performant models. In\nthis paper, we introduce SimpleFold, the first flow-matching based protein\nfolding model that solely uses general purpose transformer blocks. Protein\nfolding models typically employ computationally expensive modules involving\ntriangular updates, explicit pair representations or multiple training\nobjectives curated for this specific domain. Instead, SimpleFold employs\nstandard transformer blocks with adaptive layers and is trained via a\ngenerative flow-matching objective with an additional structural term. We scale\nSimpleFold to 3B parameters and train it on approximately 9M distilled protein\nstructures together with experimental PDB data. On standard folding benchmarks,\nSimpleFold-3B achieves competitive performance compared to state-of-the-art\nbaselines, in addition SimpleFold demonstrates strong performance in ensemble\nprediction which is typically difficult for models trained via deterministic\nreconstruction objectives. Due to its general-purpose architecture, SimpleFold\nshows efficiency in deployment and inference on consumer-level hardware.\nSimpleFold challenges the reliance on complex domain-specific architectures\ndesigns in protein folding, opening up an alternative design space for future\nprogress.", "AI": {"tldr": "SimpleFold\u662f\u4e00\u4e2a\u4ec5\u4f7f\u7528\u901a\u7528Transformer\u5757\u7684\u6d41\u5339\u914d\u86cb\u767d\u8d28\u6298\u53e0\u6a21\u578b\uff0c\u5b83\u5728\u4e0d\u4f9d\u8d56\u590d\u6742\u9886\u57df\u7279\u5b9a\u67b6\u6784\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u548c\u96c6\u6210\u9884\u6d4b\u65b9\u9762\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u6613\u4e8e\u90e8\u7f72\u3002", "motivation": "\u8bc4\u4f30\u4ec5\u4f7f\u7528\u901a\u7528Transformer\u5757\u548c\u6d41\u5339\u914d\u76ee\u6807\u662f\u5426\u80fd\u591f\u6784\u5efa\u9ad8\u6027\u80fd\u7684\u86cb\u767d\u8d28\u6298\u53e0\u6a21\u578b\uff0c\u6311\u6218\u5bf9\u590d\u6742\u9886\u57df\u7279\u5b9a\u67b6\u6784\u7684\u4f9d\u8d56\u3002", "method": "\u4f7f\u7528\u901a\u7528\u7684Transformer\u5757\uff0c\u5e76\u7ed3\u5408\u81ea\u9002\u5e94\u5c42\u548c\u6d41\u5339\u914d\u76ee\u6807\uff08\u9644\u52a0\u7ed3\u6784\u9879\uff09\u6765\u8bad\u7ec3SimpleFold\u6a21\u578b\uff0c\u5e76\u5c06\u5176\u6269\u5c55\u523030\u4ebf\u53c2\u6570\uff0c\u5728900\u4e07\u4e2a\u7cbe\u70bc\u7684\u86cb\u767d\u8d28\u7ed3\u6784\u548cPDB\u6570\u636e\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "SimpleFold-3B\u5728\u6807\u51c6\u6298\u53e0\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u4e0e\u6700\u5148\u8fdb\u57fa\u7ebf\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u5728\u96c6\u6210\u9884\u6d4b\u65b9\u9762\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u80fd\u529b\uff0c\u5e76\u4e14\u5728\u6d88\u8d39\u7ea7\u786c\u4ef6\u4e0a\u5177\u6709\u9ad8\u6548\u7684\u90e8\u7f72\u548c\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "SimpleFold\u7684\u6210\u529f\u8868\u660e\uff0c\u86cb\u767d\u8d28\u6298\u53e0\u6a21\u578b\u4e0d\u4e00\u5b9a\u9700\u8981\u590d\u6742\u7684\u9886\u57df\u7279\u5b9a\u67b6\u6784\uff0c\u4e3a\u672a\u6765\u7684\u7814\u7a76\u5f00\u8f9f\u4e86\u65b0\u7684\u8bbe\u8ba1\u65b9\u5411\u3002"}}
{"id": "2509.18765", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18765", "abs": "https://arxiv.org/abs/2509.18765", "authors": ["Azad Singh", "Deepak Mishra"], "title": "DiSSECT: Structuring Transfer-Ready Medical Image Representations through Discrete Self-Supervision", "comment": null, "summary": "Self-supervised learning (SSL) has emerged as a powerful paradigm for medical\nimage representation learning, particularly in settings with limited labeled\ndata. However, existing SSL methods often rely on complex architectures,\nanatomy-specific priors, or heavily tuned augmentations, which limit their\nscalability and generalizability. More critically, these models are prone to\nshortcut learning, especially in modalities like chest X-rays, where anatomical\nsimilarity is high and pathology is subtle. In this work, we introduce DiSSECT\n-- Discrete Self-Supervision for Efficient Clinical Transferable\nRepresentations, a framework that integrates multi-scale vector quantization\ninto the SSL pipeline to impose a discrete representational bottleneck. This\nconstrains the model to learn repeatable, structure-aware features while\nsuppressing view-specific or low-utility patterns, improving representation\ntransfer across tasks and domains. DiSSECT achieves strong performance on both\nclassification and segmentation tasks, requiring minimal or no fine-tuning, and\nshows particularly high label efficiency in low-label regimes. We validate\nDiSSECT across multiple public medical imaging datasets, demonstrating its\nrobustness and generalizability compared to existing state-of-the-art\napproaches.", "AI": {"tldr": "DiSSECT\u662f\u4e00\u79cd\u57fa\u4e8e\u79bb\u6563\u8868\u793a\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u5411\u91cf\u91cf\u5316\u6765\u5b66\u4e60\u533b\u5b66\u56fe\u50cf\u7684\u53ef\u590d\u73b0\u3001\u7ed3\u6784\u611f\u77e5\u7279\u5f81\uff0c\u4ee5\u63d0\u9ad8\u8868\u793a\u7684\u53ef\u8fc1\u79fb\u6027\uff0c\u5e76\u5728\u5404\u79cd\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u5728\u533b\u5b66\u56fe\u50cf\u5904\u7406\u4e2d\u867d\u7136\u5f3a\u5927\uff0c\u4f46\u5e38\u4f9d\u8d56\u590d\u6742\u67b6\u6784\u3001\u89e3\u5256\u5b66\u5148\u9a8c\u6216\u5927\u91cf\u8c03\u4f18\u7684\u589e\u5f3a\u65b9\u6cd5\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u53ef\u6269\u5c55\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\u66f4\u4e25\u91cd\u7684\u662f\uff0c\u8fd9\u4e9b\u6a21\u578b\u5bb9\u6613\u51fa\u73b0\u201c\u6377\u5f84\u5b66\u4e60\u201d\uff0c\u5c24\u5176\u662f\u5728\u89e3\u5256\u7ed3\u6784\u76f8\u4f3c\u800c\u75c5\u53d8\u7ec6\u5fae\u7684\u80f8\u90e8X\u5149\u7b49\u5f71\u50cf\u4e2d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDiSSECT\uff08\u79bb\u6563\u81ea\u76d1\u7763\u5b66\u4e60\u7528\u4e8e\u9ad8\u6548\u4e34\u5e8a\u53ef\u8fc1\u79fb\u8868\u793a\uff09\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5c06\u591a\u5c3a\u5ea6\u5411\u91cf\u91cf\u5316\u96c6\u6210\u5230\u81ea\u76d1\u7763\u5b66\u4e60\u6d41\u7a0b\u4e2d\uff0c\u4ee5\u5f3a\u5236\u5b9e\u73b0\u79bb\u6563\u8868\u793a\u74f6\u9888\u3002\u8fd9\u9650\u5236\u4e86\u6a21\u578b\u5b66\u4e60\u53ef\u91cd\u590d\u7684\u3001\u7ed3\u6784\u611f\u77e5\u7684\u7279\u5f81\uff0c\u540c\u65f6\u6291\u5236\u4e86\u7279\u5b9a\u4e8e\u89c6\u56fe\u6216\u4f4e\u6548\u7528\u7684\u6a21\u5f0f\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u8868\u793a\u5728\u4e0d\u540c\u4efb\u52a1\u548c\u9886\u57df\u4e4b\u95f4\u7684\u8fc1\u79fb\u80fd\u529b\u3002", "result": "DiSSECT\u5728\u5206\u7c7b\u548c\u5206\u5272\u4efb\u52a1\u4e0a\u5747\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6027\u80fd\uff0c\u4e14\u53ea\u9700\u8fdb\u884c\u5c11\u91cf\u6216\u65e0\u9700\u5fae\u8c03\u3002\u5728\u4f4e\u6807\u7b7e\u6570\u636e\u60c5\u51b5\u4e0b\uff0c\u5176\u6807\u7b7e\u6548\u7387\u5c24\u4e3a\u7a81\u51fa\u3002\u5728\u591a\u4e2a\u516c\u5171\u533b\u5b66\u5f71\u50cf\u6570\u636e\u96c6\u4e0a\u7684\u9a8c\u8bc1\u8868\u660e\uff0cDiSSECT\u6bd4\u73b0\u6709\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u5177\u6709\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "DiSSECT\u901a\u8fc7\u5f15\u5165\u79bb\u6563\u8868\u793a\u74f6\u9888\uff0c\u6210\u529f\u5730\u89e3\u51b3\u4e86\u73b0\u6709\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u5728\u533b\u5b66\u56fe\u50cf\u5904\u7406\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u63d0\u9ad8\u4e86\u7279\u5f81\u7684\u53ef\u91cd\u590d\u6027\u3001\u7ed3\u6784\u611f\u77e5\u80fd\u529b\u548c\u8de8\u4efb\u52a1/\u57df\u7684\u8fc1\u79fb\u80fd\u529b\uff0c\u5e76\u5728\u591a\u79cd\u533b\u5b66\u5f71\u50cf\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u5176\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2509.18779", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18779", "abs": "https://arxiv.org/abs/2509.18779", "authors": ["Hemanth Puppala", "Wayne Sarasua", "Srinivas Biyaguda", "Farhad Farzinpour", "Mashrur Chowdhury"], "title": "Real-time Deer Detection and Warning in Connected Vehicles via Thermal Sensing and Deep Learning", "comment": "Preprint under review in TRR, 20 pages, 9 figures, 4 tables", "summary": "Deer-vehicle collisions represent a critical safety challenge in the United\nStates, causing nearly 2.1 million incidents annually and resulting in\napproximately 440 fatalities, 59,000 injuries, and 10 billion USD in economic\ndamages. These collisions also contribute significantly to declining deer\npopulations. This paper presents a real-time detection and driver warning\nsystem that integrates thermal imaging, deep learning, and\nvehicle-to-everything communication to help mitigate deer-vehicle collisions.\nOur system was trained and validated on a custom dataset of over 12,000 thermal\ndeer images collected in Mars Hill, North Carolina. Experimental evaluation\ndemonstrates exceptional performance with 98.84 percent mean average precision,\n95.44 percent precision, and 95.96 percent recall. The system was field tested\nduring a follow-up visit to Mars Hill and readily sensed deer providing the\ndriver with advanced warning. Field testing validates robust operation across\ndiverse weather conditions, with thermal imaging maintaining between 88 and 92\npercent detection accuracy in challenging scenarios where conventional visible\nlight based cameras achieve less than 60 percent effectiveness. When a high\nprobability threshold is reached sensor data sharing messages are broadcast to\nsurrounding vehicles and roadside units via cellular vehicle to everything\n(CV2X) communication devices. Overall, our system achieves end to end latency\nconsistently under 100 milliseconds from detection to driver alert. This\nresearch establishes a viable technological pathway for reducing deer-vehicle\ncollisions through thermal imaging and connected vehicles.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u70ed\u6210\u50cf\u3001\u6df1\u5ea6\u5b66\u4e60\u548c\u8f66\u8054\u7f51\uff08CV2X\uff09\u6280\u672f\u7684\u5b9e\u65f6\u9e7f\u78b0\u649e\u9884\u8b66\u7cfb\u7edf\uff0c\u4ee5\u51cf\u5c11\u7f8e\u56fd\u6bcf\u5e74\u8fd1210\u4e07\u8d77\u9e7f\u8f66\u78b0\u649e\u4e8b\u4ef6\u9020\u6210\u7684\u7ecf\u6d4e\u635f\u5931\u548c\u4eba\u5458\u4f24\u4ea1\u3002", "motivation": "\u7531\u4e8e\u9e7f\u8f66\u78b0\u649e\u5728\u7f8e\u56fd\u9020\u6210\u4e25\u91cd\u7684\u7ecf\u6d4e\u635f\u5931\u3001\u4eba\u5458\u4f24\u4ea1\u5e76\u5a01\u80c1\u9e7f\u79cd\u7fa4\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u6709\u6548\u7684\u78b0\u649e\u7f13\u89e3\u7cfb\u7edf\u3002", "method": "\u8be5\u7cfb\u7edf\u4f7f\u7528\u70ed\u6210\u50cf\u6280\u672f\u6355\u6349\u9e7f\u7684\u70ed\u4fe1\u53f7\uff0c\u5e76\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u5b9e\u65f6\u68c0\u6d4b\u3002\u5f53\u68c0\u6d4b\u5230\u9ad8\u6982\u7387\u78b0\u649e\u98ce\u9669\u65f6\uff0c\u901a\u8fc7CV2X\u6280\u672f\u5411\u5468\u8fb9\u8f66\u8f86\u548c\u8def\u4fa7\u5355\u5143\u53d1\u9001\u9884\u8b66\u4fe1\u606f\uff0c\u6700\u7ec8\u5b9e\u73b0\u4ece\u68c0\u6d4b\u5230\u9884\u8b66\u7684\u7aef\u5230\u7aef\u5ef6\u8fdf\u4f4e\u4e8e100\u6beb\u79d2\u3002", "result": "\u8be5\u7cfb\u7edf\u5728\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e8698.84%\u7684\u5e73\u5747\u7cbe\u5ea6\uff0c95.44%\u7684\u7cbe\u786e\u7387\u548c95.96%\u7684\u53ec\u56de\u7387\u3002\u5728\u5b9e\u9645\u573a\u5730\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u7cfb\u7edf\u5728\u5404\u79cd\u5929\u6c14\u6761\u4ef6\u4e0b\u5747\u8868\u73b0\u7a33\u5065\uff0c\u70ed\u6210\u50cf\u6280\u672f\u7684\u68c0\u6d4b\u51c6\u786e\u7387\u572888%-92%\u4e4b\u95f4\uff0c\u8fdc\u9ad8\u4e8e\u53ef\u89c1\u5149\u76f8\u673a\uff08\u4f4e\u4e8e60%\uff09\u3002", "conclusion": "\u8be5\u7814\u7a76\u9a8c\u8bc1\u4e86\u7ed3\u5408\u70ed\u6210\u50cf\u548c\u8f66\u8054\u7f51\u6280\u672f\u662f\u51cf\u5c11\u9e7f\u8f66\u78b0\u649e\u7684\u53ef\u884c\u9014\u5f84\u3002"}}
{"id": "2509.18499", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18499", "abs": "https://arxiv.org/abs/2509.18499", "authors": ["Rachel Chung", "Pratyush Nidhi Sharma", "Mikko Siponen", "Rohit Vadodaria", "Luke Smith"], "title": "Hybrid Data can Enhance the Utility of Synthetic Data for Training Anti-Money Laundering Models", "comment": "Presented at the Association of Certified Fraud Examiners (ACFE)\n  Research Institute Annual Meeting, Las Vegas, NV, (2024)", "summary": "Money laundering is a critical global issue for financial institutions.\nAutomated Anti-money laundering (AML) models, like Graph Neural Networks (GNN),\ncan be trained to identify illicit transactions in real time. A major issue for\ndeveloping such models is the lack of access to training data due to privacy\nand confidentiality concerns. Synthetically generated data that mimics the\nstatistical properties of real data but preserves privacy and confidentiality\nhas been proposed as a solution. However, training AML models on purely\nsynthetic datasets presents its own set of challenges. This article proposes\nthe use of hybrid datasets to augment the utility of synthetic datasets by\nincorporating publicly available, easily accessible, and real-world features.\nThese additions demonstrate that hybrid datasets not only preserve privacy but\nalso improve model utility, offering a practical pathway for financial\ninstitutions to enhance AML systems.", "AI": {"tldr": "\u901a\u8fc7\u7ed3\u5408\u516c\u5f00\u6570\u636e\u548c\u5408\u6210\u6570\u636e\u6765\u589e\u5f3a\u53cd\u6d17\u94b1\u6a21\u578b", "motivation": "\u7f3a\u4e4f\u771f\u5b9e\u4e16\u754c\u7684\u53cd\u6d17\u94b1\uff08AML\uff09\u8bad\u7ec3\u6570\u636e\uff0c\u8fd9\u662f\u4e00\u4e2a\u5173\u952e\u7684\u5168\u7403\u6027\u95ee\u9898\uff0c\u56e0\u4e3a\u9690\u79c1\u548c\u4fdd\u5bc6\u95ee\u9898\u9650\u5236\u4e86\u8bbf\u95ee\u3002", "method": "\u63d0\u51fa\u4f7f\u7528\u6df7\u5408\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u7ed3\u5408\u6613\u4e8e\u8bbf\u95ee\u7684\u516c\u5f00\u6570\u636e\u548c\u5408\u6210\u751f\u6210\u7684\u6570\u636e\u6765\u589e\u5f3a AML \u6a21\u578b\u3002", "result": "\u6df7\u5408\u6570\u636e\u96c6\u5728\u4fdd\u6301\u9690\u79c1\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u6548\u7528\uff0c\u8bc1\u660e\u4e86\u5176\u4f5c\u4e3a\u4e00\u79cd\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u6df7\u5408\u6570\u636e\u96c6\u4e3a\u91d1\u878d\u673a\u6784\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u65b9\u6cd5\u6765\u6539\u8fdb\u5176\u53cd\u6d17\u94b1\u7cfb\u7edf\u3002"}}
{"id": "2509.18847", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18847", "abs": "https://arxiv.org/abs/2509.18847", "authors": ["Junhao Su", "Yuanliang Wan", "Junwei Yang", "Hengyu Shi", "Tianyang Han", "Junfeng Luo", "Yurui Qiu"], "title": "Failure Makes the Agent Stronger: Enhancing Accuracy through Structured Reflection for Reliable Tool Interactions", "comment": "9pages", "summary": "Tool-augmented large language models (LLMs) are usually trained with\nsupervised imitation or coarse-grained reinforcement learning that optimizes\nsingle tool calls. Current self-reflection practices rely on heuristic prompts\nor one-way reasoning: the model is urged to 'think more' instead of learning\nerror diagnosis and repair. This is fragile in multi-turn interactions; after a\nfailure the model often repeats the same mistake. We propose structured\nreflection, which turns the path from error to repair into an explicit,\ncontrollable, and trainable action. The agent produces a short yet precise\nreflection: it diagnoses the failure using evidence from the previous step and\nthen proposes a correct, executable follow-up call. For training we combine\nDAPO and GSPO objectives with a reward scheme tailored to tool use, optimizing\nthe stepwise strategy Reflect, then Call, then Final. To evaluate, we introduce\nTool-Reflection-Bench, a lightweight benchmark that programmatically checks\nstructural validity, executability, parameter correctness, and result\nconsistency. Tasks are built as mini trajectories of erroneous call,\nreflection, and corrected call, with disjoint train and test splits.\nExperiments on BFCL v3 and Tool-Reflection-Bench show large gains in multi-turn\ntool-call success and error recovery, and a reduction of redundant calls. These\nresults indicate that making reflection explicit and optimizing it directly\nimproves the reliability of tool interaction and offers a reproducible path for\nagents to learn from failure.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3a\u201c\u7ed3\u6784\u5316\u53cd\u601d\u201d\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u660e\u786e\u8bca\u65ad\u548c\u4fee\u590d\u9519\u8bef\u6765\u63d0\u9ad8\u5de5\u5177\u589e\u5f3a\u578b\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u9760\u6027\uff0c\u5e76\u5728 Tool-Reflection-Bench \u8fd9\u4e00\u65b0\u57fa\u51c6\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "motivation": "\u5f53\u524d\u5de5\u5177\u589e\u5f3a\u578b\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8f6e\u4ea4\u4e92\u4e2d\u5bb9\u6613\u91cd\u590d\u72af\u9519\uff0c\u56e0\u4e3a\u5b83\u4eec\u4f9d\u8d56\u4e8e\u542f\u53d1\u5f0f\u63d0\u793a\u6216\u5355\u5411\u63a8\u7406\uff0c\u7f3a\u4e4f\u6709\u6548\u7684\u9519\u8bef\u8bca\u65ad\u548c\u4fee\u590d\u673a\u5236\u3002", "method": "\u63d0\u51fa\u201c\u7ed3\u6784\u5316\u53cd\u601d\u201d\u65b9\u6cd5\uff0c\u5c06\u4ece\u9519\u8bef\u5230\u4fee\u590d\u7684\u8fc7\u7a0b\u89c6\u4e3a\u4e00\u4e2a\u660e\u786e\u3001\u53ef\u63a7\u4e14\u53ef\u8bad\u7ec3\u7684\u52a8\u4f5c\u3002\u8be5\u65b9\u6cd5\u5305\u62ec\u8bca\u65ad\u5931\u8d25\uff08\u4f7f\u7528\u5148\u524d\u6b65\u9aa4\u7684\u8bc1\u636e\uff09\u5e76\u63d0\u51fa\u6b63\u786e\u3001\u53ef\u6267\u884c\u7684\u540e\u7eed\u8c03\u7528\u3002\u8bad\u7ec3\u7ed3\u5408\u4e86 DAPO \u548c GSPO \u76ee\u6807\u4ee5\u53ca\u9488\u5bf9\u5de5\u5177\u4f7f\u7528\u7684\u5956\u52b1\u673a\u5236\uff0c\u4f18\u5316\u4e86\u201c\u53cd\u601d\uff0c\u7136\u540e\u8c03\u7528\uff0c\u7136\u540e\u6700\u7ec8\u786e\u5b9a\u201d\u7684\u6b65\u9aa4\u7b56\u7565\u3002\u521b\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3a Tool-Reflection-Bench \u7684\u57fa\u51c6\u6765\u8bc4\u4f30\u6a21\u578b\u3002", "result": "\u5728 BFCL v3 \u548c Tool-Reflection-Bench \u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u591a\u8f6e\u5de5\u5177\u8c03\u7528\u6210\u529f\u7387\u548c\u9519\u8bef\u6062\u590d\u80fd\u529b\uff0c\u5e76\u51cf\u5c11\u4e86\u5197\u4f59\u8c03\u7528\u3002", "conclusion": "\u4f7f\u53cd\u601d\u660e\u786e\u5316\u5e76\u76f4\u63a5\u5bf9\u5176\u8fdb\u884c\u4f18\u5316\uff0c\u53ef\u4ee5\u63d0\u9ad8\u5de5\u5177\u4ea4\u4e92\u7684\u53ef\u9760\u6027\uff0c\u5e76\u4e3a\u667a\u80fd\u4f53\u4ece\u5931\u8d25\u4e2d\u5b66\u4e60\u63d0\u4f9b\u53ef\u590d\u73b0\u7684\u9014\u5f84\u3002"}}
{"id": "2509.18796", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18796", "abs": "https://arxiv.org/abs/2509.18796", "authors": ["Danush Kumar Venkatesh", "Stefanie Speidel"], "title": "Towards Application Aligned Synthetic Surgical Image Synthesis", "comment": null, "summary": "The scarcity of annotated surgical data poses a significant challenge for\ndeveloping deep learning systems in computer-assisted interventions. While\ndiffusion models can synthesize realistic images, they often suffer from data\nmemorization, resulting in inconsistent or non-diverse samples that may fail to\nimprove, or even harm, downstream performance. We introduce \\emph{Surgical\nApplication-Aligned Diffusion} (SAADi), a new framework that aligns diffusion\nmodels with samples preferred by downstream models. Our method constructs pairs\nof \\emph{preferred} and \\emph{non-preferred} synthetic images and employs\nlightweight fine-tuning of diffusion models to align the image generation\nprocess with downstream objectives explicitly. Experiments on three surgical\ndatasets demonstrate consistent gains of $7$--$9\\%$ in classification and\n$2$--$10\\%$ in segmentation tasks, with the considerable improvements observed\nfor underrepresented classes. Iterative refinement of synthetic samples further\nboosts performance by $4$--$10\\%$. Unlike baseline approaches, our method\novercomes sample degradation and establishes task-aware alignment as a key\nprinciple for mitigating data scarcity and advancing surgical vision\napplications.", "AI": {"tldr": "SAADi\u6846\u67b6\u901a\u8fc7\u4f7f\u751f\u6210\u6a21\u578b\u4e0e\u4e0b\u6e38\u6a21\u578b\u7684\u504f\u597d\u4fdd\u6301\u4e00\u81f4\uff0c\u89e3\u51b3\u4e86\u5408\u6210\u6570\u636e\u7684\u8d28\u91cf\u95ee\u9898\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u624b\u672f\u5f71\u50cf\u5206\u6790\u7684\u6027\u80fd\u3002", "motivation": "\u6807\u6ce8\u624b\u672f\u6570\u636e\u7a00\u7f3a\uff0c\u963b\u788d\u4e86\u8ba1\u7b97\u673a\u8f85\u52a9\u4ecb\u5165\u624b\u672f\u7684\u6df1\u5ea6\u5b66\u4e60\u7cfb\u7edf\u53d1\u5c55\u3002\u73b0\u6709\u6269\u6563\u6a21\u578b\u867d\u7136\u80fd\u751f\u6210\u903c\u771f\u56fe\u50cf\uff0c\u4f46\u5b58\u5728\u6570\u636e\u8bb0\u5fc6\u3001\u6837\u672c\u4e0d\u4e00\u81f4\u6216\u591a\u6837\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u53ef\u80fd\u5f71\u54cd\u4e0b\u6e38\u4efb\u52a1\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51faSAADi\u6846\u67b6\uff0c\u4f7f\u6269\u6563\u6a21\u578b\u751f\u6210\u7684\u6837\u672c\u4e0e\u4e0b\u6e38\u6a21\u578b\u504f\u597d\u5bf9\u9f50\u3002\u901a\u8fc7\u6784\u5efa\u201c\u504f\u597d\u201d\u4e0e\u201c\u975e\u504f\u597d\u201d\u5408\u6210\u56fe\u50cf\u5bf9\uff0c\u5e76\u5bf9\u6269\u6563\u6a21\u578b\u8fdb\u884c\u8f7b\u91cf\u7ea7\u5fae\u8c03\uff0c\u4f7f\u56fe\u50cf\u751f\u6210\u8fc7\u7a0b\u663e\u5f0f\u5730\u4e0e\u4e0b\u6e38\u76ee\u6807\u5bf9\u9f50\u3002\u8fdb\u4e00\u6b65\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u5408\u6210\u6837\u672c\u6765\u63d0\u5347\u6027\u80fd\u3002", "result": "\u5728\u4e09\u4e2a\u624b\u672f\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSAADi\u5728\u5206\u7c7b\u4efb\u52a1\u4e0a\u63d0\u5347\u4e867-9%\uff0c\u5728\u5206\u5272\u4efb\u52a1\u4e0a\u63d0\u5347\u4e862-10%\uff0c\u5c24\u5176\u5728\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u7c7b\u522b\u4e0a\u6539\u5584\u663e\u8457\u3002\u8fed\u4ee3\u4f18\u5316\u4f7f\u6027\u80fd\u8fdb\u4e00\u6b65\u63d0\u53474-10%\u3002", "conclusion": "SAADi\u514b\u670d\u4e86\u6837\u672c\u9000\u5316\u95ee\u9898\uff0c\u786e\u7acb\u4e86\u4efb\u52a1\u611f\u77e5\u5bf9\u9f50\u539f\u5219\uff0c\u662f\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u548c\u63a8\u8fdb\u624b\u672f\u89c6\u89c9\u5e94\u7528\u7684\u5173\u952e\u65b9\u6cd5\u3002"}}
{"id": "2509.18521", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18521", "abs": "https://arxiv.org/abs/2509.18521", "authors": ["Yuzhen Zhou", "Jiajun Li", "Yusheng Su", "Gowtham Ramesh", "Zilin Zhu", "Xiang Long", "Chenyang Zhao", "Jin Pan", "Xiaodong Yu", "Ze Wang", "Kangrui Du", "Jialian Wu", "Ximeng Sun", "Jiang Liu", "Qiaolin Yu", "Hao Chen", "Zicheng Liu", "Emad Barsoum"], "title": "APRIL: Active Partial Rollouts in Reinforcement Learning to tame long-tail generation", "comment": null, "summary": "Reinforcement learning (RL) has become a cornerstone in advancing large-scale\npre-trained language models (LLMs). Successive generations, including GPT-o\nseries, DeepSeek-R1, Kimi-K1.5, Grok 4, and GLM-4.5, have relied on large-scale\nRL training to enhance reasoning and coding capabilities. To meet the\ncommunity's growing RL needs, numerous RL frameworks have been proposed. Most\nof these frameworks primarily rely on inference engines for rollout generation\nand training engines for policy updates. However, RL training remains\ncomputationally expensive, with rollout generation accounting for more than 90%\nof total runtime. In addition, its efficiency is often constrained by the\nlong-tail distribution of rollout response lengths, where a few lengthy\nresponses stall entire batches, leaving GPUs idle and underutilized. As model\nand rollout sizes continue to grow, this bottleneck increasingly limits\nscalability. To address this challenge, we propose Active Partial Rollouts in\nReinforcement Learning (APRIL), which mitigates long-tail inefficiency. In the\nrollout phase, APRIL over-provisions rollout requests, terminates once the\ntarget number of responses is reached, and recycles incomplete responses for\ncontinuation in future steps. This strategy ensures that no rollouts are\ndiscarded while substantially reducing GPU idle time. Experiments show that\nAPRIL improves rollout throughput by at most 44% across commonly used RL\nalgorithms (GRPO, DAPO, GSPO), accelerates convergence, and achieves at most 8%\nhigher final accuracy across tasks. Moreover, APRIL is both framework and\nhardware agnostic, already integrated into the slime RL framework, and\ndeployable on NVIDIA and AMD GPUs alike. Taken together, this work unifies\nsystem-level and algorithmic considerations in proposing APRIL, with the aim of\nadvancing RL training efficiency and inspiring further optimizations in RL\nsystems.", "AI": {"tldr": "APRIL\u901a\u8fc7\u5728\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u8bad\u7ec3\u4e2d\u91c7\u7528\u4e3b\u52a8\u90e8\u5206\u56de\u653e\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u56e0\u54cd\u5e94\u957f\u5ea6\u957f\u5c3e\u5206\u5e03\u5bfc\u81f4\u7684GPU\u5229\u7528\u7387\u4f4e\u4e0b\u548c\u6548\u7387\u74f6\u9888\u95ee\u9898\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u56de\u653e\u541e\u5410\u91cf\uff0c\u52a0\u901f\u4e86\u6536\u655b\uff0c\u5e76\u63d0\u5347\u4e86\u6700\u7ec8\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u6846\u67b6\u5728\u8bad\u7ec3\u5927\u578b\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u65f6\uff0c\u4e3b\u8981\u4f9d\u8d56\u63a8\u7406\u5f15\u64ce\u8fdb\u884c\u56de\u653e\u751f\u6210\u548c\u8bad\u7ec3\u5f15\u64ce\u8fdb\u884c\u7b56\u7565\u66f4\u65b0\u3002\u7136\u800c\uff0cRL\u8bad\u7ec3\u6210\u672c\u9ad8\u6602\uff0c\u5176\u4e2d\u56de\u653e\u751f\u6210\u5360\u603b\u8fd0\u884c\u65f6\u95f4\u768490%\u4ee5\u4e0a\u3002\u6b64\u5916\uff0c\u54cd\u5e94\u957f\u5ea6\u7684\u957f\u5c3e\u5206\u5e03\u5bfc\u81f4\u5c11\u6570\u957f\u54cd\u5e94\u62d6\u6162\u6574\u4e2a\u6279\u6b21\u7684\u5904\u7406\u901f\u5ea6\uff0c\u9020\u6210GPU\u7a7a\u95f2\u548c\u5229\u7528\u7387\u4f4e\u4e0b\uff0c\u9650\u5236\u4e86\u6a21\u578b\u7684\u6269\u5c55\u6027\u3002", "method": "APRIL\uff08Active Partial Rollouts in Reinforcement Learning\uff09\u63d0\u51fa\u4e86\u4e00\u79cd\u4e3b\u52a8\u90e8\u5206\u56de\u653e\u7b56\u7565\u3002\u5728\u56de\u653e\u9636\u6bb5\uff0cAPRIL\u4f1a\u8d85\u989d\u8bf7\u6c42\u56de\u653e\uff0c\u4e00\u65e6\u8fbe\u5230\u76ee\u6807\u54cd\u5e94\u6570\u91cf\u5c31\u7acb\u5373\u7ec8\u6b62\uff0c\u5e76\u5c06\u672a\u5b8c\u6210\u7684\u54cd\u5e94\u7528\u4e8e\u540e\u7eed\u6b65\u9aa4\u7684\u7ee7\u7eed\u5904\u7406\u3002\u8fd9\u79cd\u7b56\u7565\u786e\u4fdd\u4e86\u6240\u6709\u56de\u653e\u90fd\u80fd\u88ab\u6709\u6548\u5229\u7528\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e86GPU\u7684\u7a7a\u95f2\u65f6\u95f4\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cAPRIL\u5728\u5e38\u7528\u7684RL\u7b97\u6cd5\uff08GRPO, DAPO, GSPO\uff09\u4e2d\uff0c\u5c06\u56de\u653e\u541e\u5410\u91cf\u63d0\u9ad8\u4e8644%\uff0c\u52a0\u901f\u4e86\u6536\u655b\u8fc7\u7a0b\uff0c\u5e76\u5728\u5404\u9879\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u8fbe8%\u7684\u6700\u7ec8\u51c6\u786e\u7387\u63d0\u5347\u3002APRIL\u8fd8\u5177\u6709\u6846\u67b6\u548c\u786c\u4ef6\u65e0\u5173\u6027\uff0c\u5df2\u96c6\u6210\u5230slime RL\u6846\u67b6\u4e2d\uff0c\u5e76\u53ef\u5728NVIDIA\u548cAMD GPU\u4e0a\u8fd0\u884c\u3002", "conclusion": "APRIL\u901a\u8fc7\u7ed3\u5408\u7cfb\u7edf\u7ea7\u548c\u7b97\u6cd5\u7ea7\u7684\u8003\u91cf\uff0c\u6709\u6548\u5730\u63d0\u9ad8\u4e86RL\u8bad\u7ec3\u7684\u6548\u7387\uff0c\u89e3\u51b3\u4e86\u957f\u671f\u5b58\u5728\u7684\u957f\u5c3e\u54cd\u5e94\u95ee\u9898\uff0c\u5e76\u4e3aRL\u7cfb\u7edf\u7684\u8fdb\u4e00\u6b65\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u3002"}}
{"id": "2509.19002", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19002", "abs": "https://arxiv.org/abs/2509.19002", "authors": ["Hao Wang", "Eiki Murata", "Lingfang Zhang", "Ayako Sato", "So Fukuda", "Ziqi Yin", "Wentao Hu", "Keisuke Nakao", "Yusuke Nakamura", "Sebastian Zwirner", "Yi-Chia Chen", "Hiroyuki Otomo", "Hiroki Ouchi", "Daisuke Kawahara"], "title": "VIR-Bench: Evaluating Geospatial and Temporal Understanding of MLLMs via Travel Video Itinerary Reconstruction", "comment": null, "summary": "Recent advances in multimodal large language models (MLLMs) have\nsignificantly enhanced video understanding capabilities, opening new\npossibilities for practical applications. Yet current video benchmarks focus\nlargely on indoor scenes or short-range outdoor activities, leaving the\nchallenges associated with long-distance travel largely unexplored. Mastering\nextended geospatial-temporal trajectories is critical for next-generation\nMLLMs, underpinning real-world tasks such as embodied-AI planning and\nnavigation. To bridge this gap, we present VIR-Bench, a novel benchmark\nconsisting of 200 travel videos that frames itinerary reconstruction as a\nchallenging task designed to evaluate and push forward MLLMs'\ngeospatial-temporal intelligence. Experimental results reveal that\nstate-of-the-art MLLMs, including proprietary ones, struggle to achieve high\nscores, underscoring the difficulty of handling videos that span extended\nspatial and temporal scales. Moreover, we conduct an in-depth case study in\nwhich we develop a prototype travel-planning agent that leverages the insights\ngained from VIR-Bench. The agent's markedly improved itinerary recommendations\nverify that our evaluation protocol not only benchmarks models effectively but\nalso translates into concrete performance gains in user-facing applications.", "AI": {"tldr": "VIR-Bench\u662f\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u6539\u8fdb\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u957f\u8ddd\u79bb\u65c5\u884c\u89c6\u9891\u7406\u89e3\u65b9\u9762\u7684\u80fd\u529b\uff0c\u5b83\u901a\u8fc7\u8def\u7ebf\u91cd\u5efa\u4efb\u52a1\u6765\u8861\u91cf\u6a21\u578b\u5728\u5730\u7406\u7a7a\u95f4\u548c\u65f6\u95f4\u4e0a\u7684\u667a\u80fd\uff0c\u5e76\u5df2\u6210\u529f\u5e94\u7528\u4e8e\u6539\u8fdb\u65c5\u884c\u89c4\u5212\u4ee3\u7406\u3002 ", "motivation": "\u73b0\u6709\u7684\u89c6\u9891\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u5ba4\u5185\u573a\u666f\u6216\u77ed\u8ddd\u79bb\u6237\u5916\u6d3b\u52a8\uff0c\u672a\u80fd\u5145\u5206\u63a2\u7d22\u957f\u8ddd\u79bb\u65c5\u884c\u5e26\u6765\u7684\u6311\u6218\uff0c\u800c\u638c\u63e1\u957f\u671f\u7684\u5730\u7406\u7a7a\u95f4-\u65f6\u95f4\u8f68\u8ff9\u5bf9\u4e8e\u4e0b\u4e00\u4ee3MLLMs\u5728\u5177\u8eabAI\u89c4\u5212\u548c\u5bfc\u822a\u7b49\u5b9e\u9645\u4efb\u52a1\u4e2d\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51faVIR-Bench\uff0c\u4e00\u4e2a\u5305\u542b200\u4e2a\u65c5\u884c\u89c6\u9891\u7684\u65b0\u57fa\u51c6\uff0c\u5e76\u5c06\u8def\u7ebf\u91cd\u5efa\u4f5c\u4e3a\u4e00\u9879\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u65e8\u5728\u8bc4\u4f30\u548c\u63a8\u52a8MLLMs\u7684\u5730\u7406\u7a7a\u95f4-\u65f6\u95f4\u667a\u80fd\u3002", "result": "\u6700\u5148\u8fdb\u7684MLLMs\uff08\u5305\u62ec\u4e13\u6709\u6a21\u578b\uff09\u5728VIR-Bench\u4e0a\u5f97\u5206\u666e\u904d\u4e0d\u9ad8\uff0c\u8868\u660e\u5904\u7406\u8de8\u8d8a\u5e7f\u9614\u7a7a\u95f4\u548c\u65f6\u95f4\u5c3a\u5ea6\u7684\u89c6\u9891\u5177\u6709\u76f8\u5f53\u5927\u7684\u96be\u5ea6\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u4e00\u4e2a\u6848\u4f8b\u7814\u7a76\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u5229\u7528VIR-Bench\u89c1\u89e3\u7684\u65c5\u884c\u89c4\u5212\u4ee3\u7406\uff0c\u8be5\u4ee3\u7406\u5728\u8def\u7ebf\u63a8\u8350\u65b9\u9762\u8868\u73b0\u51fa\u663e\u8457\u7684\u6539\u8fdb\u3002", "conclusion": "VIR-Bench\u4e0d\u4ec5\u80fd\u6709\u6548\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\uff0c\u8fd8\u80fd\u5207\u5b9e\u63d0\u5347\u9762\u5411\u7528\u6237\u7684\u5e94\u7528\u7a0b\u5e8f\u7684\u8868\u73b0\uff0c\u5982\u65c5\u884c\u89c4\u5212\u4ee3\u7406\u3002"}}
{"id": "2509.18801", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18801", "abs": "https://arxiv.org/abs/2509.18801", "authors": ["Kuang Xiaodong", "Li Bingxuan", "Li Yuan", "Rao Fan", "Ma Gege", "Xie Qingguo", "Mok Greta S P", "Liu Huafeng", "Zhu Wentao"], "title": "A Kernel Space-based Multidimensional Sparse Model for Dynamic PET Image Denoising", "comment": null, "summary": "Achieving high image quality for temporal frames in dynamic positron emission\ntomography (PET) is challenging due to the limited statistic especially for the\nshort frames. Recent studies have shown that deep learning (DL) is useful in a\nwide range of medical image denoising tasks. In this paper, we propose a\nmodel-based neural network for dynamic PET image denoising. The inter-frame\nspatial correlation and intra-frame structural consistency in dynamic PET are\nused to establish the kernel space-based multidimensional sparse (KMDS) model.\nWe then substitute the inherent forms of the parameter estimation with neural\nnetworks to enable adaptive parameters optimization, forming the end-to-end\nneural KMDS-Net. Extensive experimental results from simulated and real data\ndemonstrate that the neural KMDS-Net exhibits strong denoising performance for\ndynamic PET, outperforming previous baseline methods. The proposed method may\nbe used to effectively achieve high temporal and spatial resolution for dynamic\nPET. Our source code is available at\nhttps://github.com/Kuangxd/Neural-KMDS-Net/tree/main.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u52a8\u6001PET\u56fe\u50cf\u53bb\u566a\u65b0\u65b9\u6cd5\uff0c\u540d\u4e3a neural KMDS-Net\u3002", "motivation": "\u52a8\u6001PET\u6210\u50cf\u5728\u5e27\u6570\u8f83\u5c11\u65f6\uff0c\u7531\u4e8e\u7edf\u8ba1\u91cf\u6709\u9650\uff0c\u56fe\u50cf\u8d28\u91cf\u96be\u4ee5\u4fdd\u8bc1\uff0c\u5c24\u5176\u662f\u5728\u63d0\u9ad8\u65f6\u95f4\u548c\u7a7a\u95f4\u5206\u8fa8\u7387\u65b9\u9762\u5b58\u5728\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u578b\u9a71\u52a8\u7684\u795e\u7ecf\u7f51\u7edc\uff0c\u7ed3\u5408\u4e86\u6838\u7a7a\u95f4\u7a00\u758f\u6a21\u578b\uff08KMDS\uff09\u548c\u7aef\u5230\u7aef\u7684\u795e\u7ecf\u7f51\u7edc\uff0c\u4ee5\u81ea\u9002\u5e94\u5730\u4f18\u5316\u53c2\u6570\u5e76\u5b9e\u73b0\u53bb\u566a\u3002", "result": "\u6a21\u62df\u548c\u771f\u5b9e\u6570\u636e\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cneural KMDS-Net\u5728\u52a8\u6001PET\u56fe\u50cf\u53bb\u566a\u65b9\u9762\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684neural KMDS-Net\u80fd\u591f\u6709\u6548\u63d0\u9ad8\u52a8\u6001PET\u56fe\u50cf\u7684\u65f6\u95f4\u548c\u7a7a\u95f4\u5206\u8fa8\u7387\uff0c\u5177\u6709\u826f\u597d\u7684\u53bb\u566a\u6027\u80fd\u3002"}}
{"id": "2509.18529", "categories": ["cs.LG", "q-bio.GN"], "pdf": "https://arxiv.org/pdf/2509.18529", "abs": "https://arxiv.org/abs/2509.18529", "authors": ["Mingqian Ma"], "title": "Reverse-Complement Consistency for DNA Language Models", "comment": null, "summary": "A fundamental property of DNA is that the reverse complement (RC) of a\nsequence often carries identical biological meaning. However, state-of-the-art\nDNA language models frequently fail to capture this symmetry, producing\ninconsistent predictions for a sequence and its RC counterpart, which\nundermines their reliability. In this work, we introduce Reverse-Complement\nConsistency Regularization (RCCR), a simple and model-agnostic fine-tuning\nobjective that directly penalizes the divergence between a model's prediction\non a sequence and the aligned prediction on its reverse complement. We evaluate\nRCCR across three diverse backbones (Nucleotide Transformer, HyenaDNA,\nDNABERT-2) on a wide range of genomic tasks, including sequence classification,\nscalar regression, and profile prediction. Our experiments show that RCCR\nsubstantially improves RC robustness by dramatically reducing prediction flips\nand errors, all while maintaining or improving task accuracy compared to\nbaselines such as RC data augmentation and test-time averaging. By integrating\na key biological prior directly into the learning process, RCCR produces a\nsingle, intrinsically robust, and computationally efficient model fine-tuning\nrecipe for diverse biology tasks.", "AI": {"tldr": "DNA\u8bed\u8a00\u6a21\u578b\u901a\u5e38\u65e0\u6cd5\u7406\u89e3\u5e8f\u5217\u4e0e\u5176\u53cd\u5411\u4e92\u8865\u5e8f\u5217\u7684\u751f\u7269\u5b66\u610f\u4e49\u76f8\u540c\u8fd9\u4e00\u57fa\u672c\u5c5e\u6027\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u53cd\u5411\u4e92\u8865\u4e00\u81f4\u6027\u6b63\u5219\u5316\uff08RCCR\uff09\u7684\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u76f4\u63a5\u60e9\u7f5a\u6a21\u578b\u5bf9\u5e8f\u5217\u53ca\u5176\u53cd\u5411\u4e92\u8865\u5e8f\u5217\u9884\u6d4b\u4e4b\u95f4\u7684\u5dee\u5f02\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684DNA\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u5e8f\u5217\u53ca\u5176\u53cd\u5411\u4e92\u8865\u5e8f\u5217\u65f6\u9884\u6d4b\u4e0d\u4e00\u81f4\uff0c\u8fd9\u964d\u4f4e\u4e86\u5176\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u53cd\u5411\u4e92\u8865\u4e00\u81f4\u6027\u6b63\u5219\u5316\uff08RCCR\uff09\u7684\u7b80\u5355\u3001\u6a21\u578b\u65e0\u5173\u7684\u5fae\u8c03\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u76f4\u63a5\u60e9\u7f5a\u6a21\u578b\u5728\u5e8f\u5217\u53ca\u5176\u53cd\u5411\u4e92\u8865\u5e8f\u5217\u4e0a\u7684\u9884\u6d4b\u5dee\u5f02\u3002", "result": "RCCR\u5728\u4e09\u79cd\u4e0d\u540c\u7684\u6a21\u578b\uff08Nucleotide Transformer, HyenaDNA, DNABERT-2\uff09\u548c\u591a\u79cd\u57fa\u56e0\u7ec4\u4efb\u52a1\uff08\u5e8f\u5217\u5206\u7c7b\u3001\u6807\u91cf\u56de\u5f52\u3001\u7279\u5f81\u9884\u6d4b\uff09\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u7ed3\u679c\u8868\u660eRCCR\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u53cd\u5411\u4e92\u8865\u9c81\u68d2\u6027\uff0c\u51cf\u5c11\u4e86\u9884\u6d4b\u9519\u8bef\uff0c\u5e76\u4e14\u5728\u4fdd\u6301\u6216\u63d0\u9ad8\u4efb\u52a1\u51c6\u786e\u6027\u7684\u540c\u65f6\uff0c\u4f18\u4e8e\u53cd\u5411\u4e92\u8865\u6570\u636e\u589e\u5f3a\u548c\u6d4b\u8bd5\u65f6\u5e73\u5747\u7b49\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "RCCR\u5c06\u5173\u952e\u7684\u751f\u7269\u5b66\u5148\u9a8c\u77e5\u8bc6\u76f4\u63a5\u878d\u5165\u5b66\u4e60\u8fc7\u7a0b\uff0c\u4e3a\u591a\u79cd\u751f\u7269\u5b66\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u5355\u4e00\u7684\u3001\u5185\u5728\u9c81\u68d2\u7684\u3001\u8ba1\u7b97\u9ad8\u6548\u7684\u6a21\u578b\u5fae\u8c03\u65b9\u6cd5\u3002"}}
{"id": "2509.19070", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.19070", "abs": "https://arxiv.org/abs/2509.19070", "authors": ["Zijian Ling", "Han Zhang", "Yazhuo Zhou", "Jiahao Cui"], "title": "ColorBlindnessEval: Can Vision-Language Models Pass Color Blindness Tests?", "comment": "Accepted at the Open Science for Foundation Models (SCI-FM) Workshop\n  at ICLR 2025", "summary": "This paper presents ColorBlindnessEval, a novel benchmark designed to\nevaluate the robustness of Vision-Language Models (VLMs) in visually\nadversarial scenarios inspired by the Ishihara color blindness test. Our\ndataset comprises 500 Ishihara-like images featuring numbers from 0 to 99 with\nvarying color combinations, challenging VLMs to accurately recognize numerical\ninformation embedded in complex visual patterns. We assess 9 VLMs using Yes/No\nand open-ended prompts and compare their performance with human participants.\nOur experiments reveal limitations in the models' ability to interpret numbers\nin adversarial contexts, highlighting prevalent hallucination issues. These\nfindings underscore the need to improve the robustness of VLMs in complex\nvisual environments. ColorBlindnessEval serves as a valuable tool for\nbenchmarking and improving the reliability of VLMs in real-world applications\nwhere accuracy is critical.", "AI": {"tldr": "ColorBlindnessEval\u662f\u4e00\u4e2a\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLMs)\u5728\u6a21\u62df\u8272\u76f2\u6d4b\u8bd5\u7684\u89c6\u89c9\u5bf9\u6297\u6027\u573a\u666f\u4e0b\u9c81\u68d2\u6027\u7684\u65b0\u57fa\u51c6\u3002\u8be5\u6570\u636e\u96c6\u5305\u542b500\u5f20\u6a21\u62df\u7684\u77f3\u539f\u6d4b\u8bd5\u56fe\u50cf\uff0c VLMs\u5728\u8bc6\u522b\u5d4c\u5165\u5728\u590d\u6742\u89c6\u89c9\u6a21\u5f0f\u4e2d\u7684\u6570\u5b57\u4fe1\u606f\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5e76\u4e14\u5b58\u5728\u666e\u904d\u7684\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLMs)\u5728\u6a21\u62df\u8272\u76f2\u6d4b\u8bd5\u7684\u89c6\u89c9\u5bf9\u6297\u6027\u573a\u666f\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u8bc6\u522b\u5176\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5e7b\u89c9\u95ee\u9898\u3002", "method": "\u521b\u5efa\u4e86\u4e00\u4e2a\u5305\u542b500\u5f20\u6a21\u62df\u77f3\u539f\u6d4b\u8bd5\u56fe\u50cf\u7684\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5d4c\u5165\u4e860\u523099\u7684\u6570\u5b57\uff0c\u5e76\u4f7f\u7528\u4e86\u662f/\u5426\u548c\u5f00\u653e\u5f0f\u63d0\u793a\u6765\u8bc4\u4f309\u4e2aVLMs\u7684\u8868\u73b0\uff0c\u5e76\u4e0e\u4eba\u7c7b\u53c2\u4e0e\u8005\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cVLMs\u5728\u7406\u89e3\u5bf9\u6297\u6027\u73af\u5883\u4e2d\u7684\u6570\u5b57\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5e76\u4e14\u5b58\u5728\u666e\u904d\u7684\u5e7b\u89c9\u95ee\u9898\u3002", "conclusion": "\u73b0\u6709\u7684VLMs\u5728\u590d\u6742\u89c6\u89c9\u73af\u5883\u4e2d\u9c81\u68d2\u6027\u4e0d\u8db3\uff0c\u9700\u8981\u6539\u8fdb\u3002ColorBlindnessEval\u6570\u636e\u96c6\u6709\u52a9\u4e8e\u57fa\u51c6\u6d4b\u8bd5\u548c\u63d0\u9ad8VLMs\u5728\u73b0\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2509.18802", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18802", "abs": "https://arxiv.org/abs/2509.18802", "authors": ["Garam Kim", "Tae Kyeong Jeong", "Juyoun Park"], "title": "Surgical Video Understanding with Label Interpolation", "comment": "8 pages, 10 figures", "summary": "Robot-assisted surgery (RAS) has become a critical paradigm in modern\nsurgery, promoting patient recovery and reducing the burden on surgeons through\nminimally invasive approaches. To fully realize its potential, however, a\nprecise understanding of the visual data generated during surgical procedures\nis essential. Previous studies have predominantly focused on single-task\napproaches, but real surgical scenes involve complex temporal dynamics and\ndiverse instrument interactions that limit comprehensive understanding.\nMoreover, the effective application of multi-task learning (MTL) requires\nsufficient pixel-level segmentation data, which are difficult to obtain due to\nthe high cost and expertise required for annotation. In particular, long-term\nannotations such as phases and steps are available for every frame, whereas\nshort-term annotations such as surgical instrument segmentation and action\ndetection are provided only for key frames, resulting in a significant\ntemporal-spatial imbalance. To address these challenges, we propose a novel\nframework that combines optical flow-based segmentation label interpolation\nwith multi-task learning. optical flow estimated from annotated key frames is\nused to propagate labels to adjacent unlabeled frames, thereby enriching sparse\nspatial supervision and balancing temporal and spatial information for\ntraining. This integration improves both the accuracy and efficiency of\nsurgical scene understanding and, in turn, enhances the utility of RAS.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5149\u6d41\u5f15\u5bfc\u7684\u5206\u5272\u6807\u7b7e\u63d2\u503c\u548c\u591a\u4efb\u52a1\u5b66\u4e60\u7684\u65b0\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u673a\u5668\u4eba\u8f85\u52a9\u624b\u672f\uff08RAS\uff09\u89c6\u89c9\u6570\u636e\u7406\u89e3\u4e2d\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u6570\u636e\u7a00\u758f\u6027\u548c\u65f6\u7a7a\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "motivation": "\u673a\u5668\u4eba\u8f85\u52a9\u624b\u672f\uff08RAS\uff09\u7684\u6f5c\u529b\u5c1a\u672a\u5b8c\u5168\u53d1\u6325\uff0c\u56e0\u4e3a\u9700\u8981\u7cbe\u786e\u7406\u89e3\u624b\u672f\u89c6\u89c9\u6570\u636e\uff0c\u800c\u4ee5\u5f80\u7684\u5355\u4efb\u52a1\u65b9\u6cd5\u548c\u6570\u636e\u6807\u6ce8\u7684\u4e0d\u8db3\uff08\u7279\u522b\u662f\u50cf\u7d20\u7ea7\u5206\u5272\u6570\u636e\u7f3a\u4e4f\u4ee5\u53ca\u65f6\u7a7a\u4fe1\u606f\u4e0d\u5e73\u8861\uff09\u9650\u5236\u4e86\u8fd9\u4e00\u70b9\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5149\u6d41\u5f15\u5bfc\u7684\u5206\u5272\u6807\u7b7e\u63d2\u503c\u548c\u591a\u4efb\u52a1\u5b66\u4e60\u7684\u65b0\u6846\u67b6\u3002\u5229\u7528\u4ece\u6807\u6ce8\u7684\u5173\u952e\u5e27\u4f30\u8ba1\u7684\u5149\u6d41\uff0c\u5c06\u6807\u7b7e\u4f20\u64ad\u5230\u672a\u6807\u6ce8\u7684\u76f8\u90bb\u5e27\uff0c\u4ece\u800c\u4e30\u5bcc\u7a00\u758f\u7684\u7a7a\u95f4\u76d1\u7763\uff0c\u5e73\u8861\u8bad\u7ec3\u7684\u65f6\u7a7a\u4fe1\u606f\u3002", "result": "\u8be5\u6846\u67b6\u901a\u8fc7\u4f20\u64ad\u6807\u7b7e\u4e30\u5bcc\u4e86\u7a00\u758f\u7684\u7a7a\u95f4\u76d1\u7763\uff0c\u5e73\u8861\u4e86\u65f6\u7a7a\u4fe1\u606f\uff0c\u63d0\u9ad8\u4e86\u624b\u672f\u573a\u666f\u7406\u89e3\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u4ece\u800c\u589e\u5f3a\u4e86RAS\u7684\u6548\u7528\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u5149\u6d41\u63d2\u503c\u548c\u591a\u4efb\u52a1\u5b66\u4e60\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u673a\u5668\u4eba\u8f85\u52a9\u624b\u672f\u89c6\u89c9\u6570\u636e\u7406\u89e3\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u63d0\u9ad8\u4e86\u624b\u672f\u573a\u666f\u7406\u89e3\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u6700\u7ec8\u5c06\u4fc3\u8fdbRAS\u7684\u5e94\u7528\u3002"}}
{"id": "2509.18542", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18542", "abs": "https://arxiv.org/abs/2509.18542", "authors": ["Qi Wang", "Hanyang Peng", "Yue Yu"], "title": "Symphony-MoE: Harmonizing Disparate Pre-trained Models into a Coherent Mixture-of-Experts", "comment": null, "summary": "Mixture-of-Experts (MoE) models enable scalable performance by activating\nlarge parameter sets sparsely, minimizing computational overhead. To circumvent\nthe prohibitive cost of training MoEs from scratch, recent work employs\nupcycling, reusing a single pre-trained dense model by replicating its\nfeed-forward network (FFN) layers into experts. However, this limits expert\ndiversity, as all experts originate from a single pre-trained dense model. This\npaper addresses this limitation by constructing powerful MoE models using\nexperts sourced from multiple identically-architected but disparate pre-trained\nmodels (e.g., Llama2-Chat and Code Llama). A key challenge lies in the fact\nthat these source models occupy disparate, dissonant regions of the parameter\nspace, making direct upcycling prone to severe performance degradation. To\novercome this, we propose Symphony-MoE, a novel two-stage framework designed to\nharmonize these models into a single, coherent expert mixture. First, we\nestablish this harmony in a training-free manner: we construct a shared\nbackbone via a layer-aware fusion strategy and, crucially, alleviate parameter\nmisalignment among experts using activation-based functional alignment.\nSubsequently, a single lightweight stage of router training coordinates the\nentire architecture. Experiments demonstrate that our method successfully\nintegrates experts from heterogeneous sources, achieving an MoE model that\nsignificantly surpasses baselines in multi-domain tasks and out-of-distribution\ngeneralization.", "AI": {"tldr": "\u901a\u8fc7\u878d\u5408\u6765\u81ea\u4e0d\u540c\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u4e13\u5bb6\u6765\u6784\u5efa\u5f3a\u5927\u7684MoE\u6a21\u578b\uff0c\u5e76\u4f7f\u7528\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\u6765\u89e3\u51b3\u53c2\u6570\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u4ece\u800c\u5728\u591a\u9886\u57df\u4efb\u52a1\u548c\u5206\u5e03\u5916\u6cdb\u5316\u65b9\u9762\u53d6\u5f97\u4f18\u4e8e\u57fa\u7ebf\u7684\u7ed3\u679c\u3002", "motivation": "\u73b0\u6709\u7684MoE\u6a21\u578b\u8bad\u7ec3\u6210\u672c\u9ad8\uff0c\u4e14\u901a\u8fc7\u5355\u4e00\u9884\u8bad\u7ec3\u6a21\u578b\u8fdb\u884c\u4e13\u5bb6\u590d\u7528\u4f1a\u5bfc\u81f4\u4e13\u5bb6\u591a\u6837\u6027\u53d7\u9650\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSymphony-MoE\u7684\u4e24\u9636\u6bb5\u6846\u67b6\u3002\u7b2c\u4e00\u9636\u6bb5\u5728\u65e0\u9700\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u5c42\u611f\u77e5\u878d\u5408\u7b56\u7565\u6784\u5efa\u5171\u4eab\u9aa8\u5e72\uff0c\u5e76\u4f7f\u7528\u57fa\u4e8e\u6fc0\u6d3b\u7684\u529f\u80fd\u5bf9\u9f50\u6765\u7f13\u89e3\u4e13\u5bb6\u4e4b\u95f4\u7684\u53c2\u6570\u4e0d\u5bf9\u9f50\u3002\u7b2c\u4e8c\u9636\u6bb5\u4ec5\u9700\u8f7b\u91cf\u7ea7\u7684\u8def\u7531\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u6210\u529f\u6574\u5408\u6765\u81ea\u5f02\u6784\u6e90\u7684\u4e13\u5bb6\uff0c\u6784\u5efa\u51fa\u5728\u591a\u9886\u57df\u4efb\u52a1\u548c\u5206\u5e03\u5916\u6cdb\u5316\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u7684MoE\u6a21\u578b\u3002", "conclusion": "Symphony-MoE\u6846\u67b6\u80fd\u591f\u6709\u6548\u5730\u878d\u5408\u6765\u81ea\u4e0d\u540c\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u4e13\u5bb6\uff0c\u514b\u670d\u53c2\u6570\u4e0d\u5bf9\u9f50\u7684\u6311\u6218\uff0c\u5e76\u63d0\u5347MoE\u6a21\u578b\u7684\u6027\u80fd\u3002"}}
{"id": "2509.19090", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.19090", "abs": "https://arxiv.org/abs/2509.19090", "authors": ["Guoxin Wang", "Jun Zhao", "Xinyi Liu", "Yanbo Liu", "Xuyang Cao", "Chao Li", "Zhuoyun Liu", "Qintian Sun", "Fangru Zhou", "Haoqiang Xing", "Zhenhong Yang"], "title": "Citrus-V: Advancing Medical Foundation Models with Unified Medical Image Grounding for Clinical Reasoning", "comment": null, "summary": "Medical imaging provides critical evidence for clinical diagnosis, treatment\nplanning, and surgical decisions, yet most existing imaging models are narrowly\nfocused and require multiple specialized networks, limiting their\ngeneralization. Although large-scale language and multimodal models exhibit\nstrong reasoning and multi-task capabilities, real-world clinical applications\ndemand precise visual grounding, multimodal integration, and chain-of-thought\nreasoning. We introduce Citrus-V, a multimodal medical foundation model that\ncombines image analysis with textual reasoning. The model integrates detection,\nsegmentation, and multimodal chain-of-thought reasoning, enabling pixel-level\nlesion localization, structured report generation, and physician-like\ndiagnostic inference in a single framework. We propose a novel multimodal\ntraining approach and release a curated open-source data suite covering\nreasoning, detection, segmentation, and document understanding tasks.\nEvaluations demonstrate that Citrus-V outperforms existing open-source medical\nmodels and expert-level imaging systems across multiple benchmarks, delivering\na unified pipeline from visual grounding to clinical reasoning and supporting\nprecise lesion quantification, automated reporting, and reliable second\nopinions.", "AI": {"tldr": "Citrus-V\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u533b\u5b66\u57fa\u7840\u6a21\u578b\uff0c\u6574\u5408\u4e86\u56fe\u50cf\u5206\u6790\u548c\u6587\u672c\u63a8\u7406\uff0c\u5b9e\u73b0\u4e86\u50cf\u7d20\u7ea7\u75c5\u7076\u5b9a\u4f4d\u3001\u7ed3\u6784\u5316\u62a5\u544a\u751f\u6210\u548c\u533b\u751f\u822c\u7684\u8bca\u65ad\u63a8\u7406\u3002", "motivation": "\u73b0\u6709\u533b\u5b66\u6210\u50cf\u6a21\u578b\u8fc7\u4e8e\u72ed\u7a84\uff0c\u9700\u8981\u591a\u4e2a\u4e13\u7528\u7f51\u7edc\uff0c\u9650\u5236\u4e86\u6cdb\u5316\u80fd\u529b\u3002\u73b0\u5b9e\u4e16\u754c\u7684\u4e34\u5e8a\u5e94\u7528\u9700\u8981\u7cbe\u786e\u7684\u89c6\u89c9\u57fa\u7840\u3001\u591a\u6a21\u6001\u96c6\u6210\u548c\u94fe\u5f0f\u601d\u8003\u63a8\u7406\u3002", "method": "Citrus-V\u6a21\u578b\u96c6\u6210\u4e86\u68c0\u6d4b\u3001\u5206\u5272\u548c\u591a\u6a21\u6001\u94fe\u5f0f\u601d\u8003\u63a8\u7406\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6a21\u6001\u8bad\u7ec3\u65b9\u6cd5\uff0c\u53d1\u5e03\u4e86\u4e00\u4e2a\u5305\u542b\u63a8\u7406\u3001\u68c0\u6d4b\u3001\u5206\u5272\u548c\u6587\u6863\u7406\u89e3\u4efb\u52a1\u7684\u5f00\u6e90\u6570\u636e\u96c6\u3002", "result": "Citrus-V\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u7684\u5f00\u6e90\u533b\u5b66\u6a21\u578b\u548c\u4e13\u5bb6\u7ea7\u6210\u50cf\u7cfb\u7edf\u3002", "conclusion": "Citrus-V\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4ece\u89c6\u89c9\u57fa\u7840\u5230\u4e34\u5e8a\u63a8\u7406\u7684\u7edf\u4e00\u6d41\u7a0b\uff0c\u652f\u6301\u7cbe\u786e\u7684\u75c5\u7076\u91cf\u5316\u3001\u81ea\u52a8\u5316\u62a5\u544a\u548c\u53ef\u9760\u7684\u7b2c\u4e8c\u610f\u89c1\u3002"}}
{"id": "2509.18824", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18824", "abs": "https://arxiv.org/abs/2509.18824", "authors": ["Yanzuo Lu", "Xin Xia", "Manlin Zhang", "Huafeng Kuang", "Jianbin Zheng", "Yuxi Ren", "Xuefeng Xiao"], "title": "Hyper-Bagel: A Unified Acceleration Framework for Multimodal Understanding and Generation", "comment": "Technical Report", "summary": "Unified multimodal models have recently attracted considerable attention for\ntheir remarkable abilities in jointly understanding and generating diverse\ncontent. However, as contexts integrate increasingly numerous interleaved\nmultimodal tokens, the iterative processes of diffusion denoising and\nautoregressive decoding impose significant computational overhead. To address\nthis, we propose Hyper-Bagel, a unified acceleration framework designed to\nsimultaneously speed up both multimodal understanding and generation tasks. Our\napproach uses a divide-and-conquer strategy, employing speculative decoding for\nnext-token prediction and a multi-stage distillation process for diffusion\ndenoising. The framework delivers substantial performance gains, achieving over\na 2x speedup in multimodal understanding. For generative tasks, our resulting\nlossless 6-NFE model yields a 16.67x speedup in text-to-image generation and a\n22x speedup in image editing, all while preserving the high-quality output of\nthe original model. We further develop a highly efficient 1-NFE model that\nenables near real-time interactive editing and generation. By combining\nadvanced adversarial distillation with human feedback learning, this model\nachieves ultimate cost-effectiveness and responsiveness, making complex\nmultimodal interactions seamless and instantaneous.", "AI": {"tldr": "Hyper-Bagel \u901a\u8fc7\u63a8\u6d4b\u6027\u89e3\u7801\u548c\u591a\u9636\u6bb5\u84b8\u998f\u52a0\u901f\u591a\u6a21\u6001\u7406\u89e3\u548c\u751f\u6210\u4efb\u52a1\uff0c\u5728\u591a\u6a21\u6001\u7406\u89e3\u3001\u6587\u751f\u56fe\u548c\u56fe\u7f16\u8f91\u65b9\u9762\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u8d28\u91cf\u7684\u8f93\u51fa\u3002", "motivation": "\u73b0\u6709\u7684\u7edf\u4e00\u591a\u6a21\u6001\u6a21\u578b\u5728\u5904\u7406\u5927\u91cf\u4ea4\u7ec7\u7684\u591a\u6a21\u6001\u4ee4\u724c\u65f6\uff0c\u5b58\u5728\u6269\u6563\u53bb\u566a\u548c\u81ea\u56de\u5f52\u89e3\u7801\u5e26\u6765\u7684\u5de8\u5927\u8ba1\u7b97\u5f00\u9500\u3002", "method": "\u63d0\u51fa Hyper-Bagel \u6846\u67b6\uff0c\u91c7\u7528\u201c\u5206\u800c\u6cbb\u4e4b\u201d\u7b56\u7565\uff0c\u7ed3\u5408\u63a8\u6d4b\u6027\u89e3\u7801\uff08\u7528\u4e8e\u9884\u6d4b\u4e0b\u4e00\u4e2a\u4ee4\u724c\uff09\u548c\u591a\u9636\u6bb5\u84b8\u998f\uff08\u7528\u4e8e\u6269\u6563\u53bb\u566a\uff09\uff0c\u4ee5\u52a0\u901f\u591a\u6a21\u6001\u7406\u89e3\u548c\u751f\u6210\u3002", "result": "Hyper-Bagel \u5728\u591a\u6a21\u6001\u7406\u89e3\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u8d85\u8fc7 2 \u500d\u7684\u52a0\u901f\u3002\u5728\u751f\u6210\u4efb\u52a1\u65b9\u9762\uff0c\u65e0\u635f 6-NFE \u6a21\u578b\u5728\u6587\u751f\u56fe\u65b9\u9762\u5b9e\u73b0\u4e86 16.67 \u500d\u7684\u52a0\u901f\uff0c\u5728\u56fe\u7f16\u8f91\u65b9\u9762\u5b9e\u73b0\u4e86 22 \u500d\u7684\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u539f\u59cb\u6a21\u578b\u7684\u8f93\u51fa\u8d28\u91cf\u3002\u6b64\u5916\uff0c\u8fd8\u5f00\u53d1\u4e86\u4e00\u4e2a\u9ad8\u6548\u7684 1-NFE \u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u8fd1\u4e4e\u5b9e\u65f6\u7684\u4ea4\u4e92\u5f0f\u7f16\u8f91\u548c\u751f\u6210\uff0c\u5e76\u901a\u8fc7\u5bf9\u6297\u6027\u84b8\u998f\u548c\u4eba\u7c7b\u53cd\u9988\u5b66\u4e60\u5b9e\u73b0\u4e86\u9ad8\u6210\u672c\u6548\u76ca\u548c\u54cd\u5e94\u901f\u5ea6\u3002", "conclusion": "Hyper-Bagel \u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u7edf\u4e00\u591a\u6a21\u6001\u6a21\u578b\u9762\u4e34\u7684\u8ba1\u7b97\u5f00\u9500\u95ee\u9898\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u52a0\u901f\u7b56\u7565\u5b9e\u73b0\u4e86\u5728\u7406\u89e3\u548c\u751f\u6210\u4efb\u52a1\u4e0a\u7684\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u80fd\u4fdd\u6301\u9ad8\u8d28\u91cf\u7684\u8f93\u51fa\uff0c\u4e3a\u5b9e\u73b0\u65e0\u7f1d\u3001\u5373\u65f6\u7684\u591a\u6a21\u6001\u4ea4\u4e92\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.18552", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18552", "abs": "https://arxiv.org/abs/2509.18552", "authors": ["Kiril Bangachev", "Guy Bresler", "Iliyas Noman", "Yury Polyanskiy"], "title": "Global Minimizers of Sigmoid Contrastive Loss", "comment": "Author names listed in alphabetical order. NeurIPS 2025", "summary": "The meta-task of obtaining and aligning representations through contrastive\npretraining is steadily gaining importance since its introduction in CLIP and\nALIGN. In this paper we theoretically explain the advantages of synchronizing\nwith trainable inverse temperature and bias under the sigmoid loss, as\nimplemented in the recent SigLIP and SigLIP2 models of Google DeepMind.\nTemperature and bias can drive the loss function to zero for a rich class of\nconfigurations that we call $(\\mathsf{m},\n\\mathsf{b}_{\\mathsf{rel}})$-Constellations. $(\\mathsf{m},\n\\mathsf{b}_{\\mathsf{rel}})$-Constellations are a novel combinatorial object\nrelated to spherical codes and are parametrized by a margin $\\mathsf{m}$ and\nrelative bias $\\mathsf{b}_{\\mathsf{rel}}$. We use our characterization of\nconstellations to theoretically justify the success of SigLIP on retrieval, to\nexplain the modality gap present in SigLIP, and to identify the necessary\ndimension for producing high-quality representations. Finally, we propose a\nreparameterization of the sigmoid loss with explicit relative bias, which\nimproves training dynamics in experiments with synthetic data.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u5f15\u5165\u53ef\u8bad\u7ec3\u7684\u9006\u6e29\u5ea6\u548c\u504f\u7f6e\uff0c\u5e76\u7ed3\u5408 Sigmoid \u635f\u5931\u51fd\u6570\uff0c\u4e3a\u5bf9\u6bd4\u9884\u8bad\u7ec3\u4e2d\u7684\u8868\u793a\u83b7\u53d6\u548c\u5bf9\u9f50\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u7684\u7406\u8bba\u89c6\u89d2\uff0c\u89e3\u91ca\u4e86 SigLIP \u548c SigLIP2 \u6a21\u578b\u7684\u4f18\u52bf\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u635f\u5931\u51fd\u6570\u3002", "motivation": "\u89e3\u91ca\u5bf9\u6bd4\u9884\u8bad\u7ec3\u4e2d\uff0c\u5728 Sigmoid \u635f\u5931\u4e0b\uff0c\u4f7f\u7528\u53ef\u8bad\u7ec3\u7684\u9006\u6e29\u5ea6\u548c\u504f\u7f6e\u7684\u4f18\u52bf\uff0c\u5e76\u5c06\u5176\u4e0e SigLIP \u548c SigLIP2 \u6a21\u578b\u8054\u7cfb\u8d77\u6765\u3002", "method": "\u7406\u8bba\u5206\u6790\u4e86\u53ef\u8bad\u7ec3\u7684\u9006\u6e29\u5ea6\u548c\u504f\u7f6e\u5728 Sigmoid \u635f\u5931\u4e0b\u7684\u884c\u4e3a\uff0c\u5f15\u5165\u4e86 $(\\mathsf{m}, \\mathsf{b}_{{\\mathsf{rel}}})$-Constellations \u6982\u5ff5\u6765\u8868\u5f81\u635f\u5931\u51fd\u6570\u53ef\u4ee5\u4e3a\u96f6\u7684\u914d\u7f6e\uff0c\u5e76\u5229\u7528\u8be5\u8868\u5f81\u6765\u89e3\u91ca SigLIP \u7684\u6210\u529f\u3001\u6a21\u6001\u95f4\u9699\u4ee5\u53ca\u786e\u5b9a\u9ad8\u8d28\u91cf\u8868\u793a\u6240\u9700\u7684\u7ef4\u5ea6\u3002", "result": "\u63d0\u51fa\u4e86 $(\\mathsf{m}, \\mathsf{b}_{{\\mathsf{rel}}})$-Constellations \u8fd9\u4e00\u65b0\u7ec4\u5408\u5bf9\u8c61\uff0c\u5e76\u7528\u5176\u7406\u8bba\u4e0a\u89e3\u91ca\u4e86 SigLIP \u5728\u68c0\u7d22\u4efb\u52a1\u4e2d\u7684\u6210\u529f\u3001\u6a21\u6001\u95f4\u9699\u73b0\u8c61\uff0c\u4ee5\u53ca\u786e\u5b9a\u4e86\u751f\u6210\u9ad8\u8d28\u91cf\u8868\u793a\u6240\u9700\u7684\u7ef4\u5ea6\u3002\u6b64\u5916\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5e26\u6709\u663e\u5f0f\u76f8\u5bf9\u504f\u7f6e\u7684 Sigmoid \u635f\u5931\u91cd\u53c2\u6570\u5316\u65b9\u6cd5\uff0c\u5e76\u5728\u5408\u6210\u6570\u636e\u5b9e\u9a8c\u4e2d\u8bc1\u660e\u4e86\u5176\u80fd\u6539\u5584\u8bad\u7ec3\u52a8\u6001\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165 $(\\mathsf{m}, \\mathsf{b}_{{\\mathsf{rel}}})$-Constellations \u548c\u5bf9 Sigmoid \u635f\u5931\u7684\u7406\u8bba\u5206\u6790\uff0c\u4e3a\u7406\u89e3\u548c\u6539\u8fdb\u5bf9\u6bd4\u9884\u8bad\u7ec3\u6a21\u578b\uff08\u5982 SigLIP\uff09\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9e\u9645\u53ef\u884c\u7684\u6539\u8fdb\u65b9\u6cd5\u3002"}}
{"id": "2509.18839", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18839", "abs": "https://arxiv.org/abs/2509.18839", "authors": ["Gianmarco Spinaci", "Lukas Klic", "Giovanni Colavizza"], "title": "Benchmarking Vision-Language and Multimodal Large Language Models in Zero-shot and Few-shot Scenarios: A study on Christian Iconography", "comment": "11 pages, 2 figures", "summary": "This study evaluates the capabilities of Multimodal Large Language Models\n(LLMs) and Vision Language Models (VLMs) in the task of single-label\nclassification of Christian Iconography. The goal was to assess whether\ngeneral-purpose VLMs (CLIP and SigLIP) and LLMs, such as GPT-4o and Gemini 2.5,\ncan interpret the Iconography, typically addressed by supervised classifiers,\nand evaluate their performance. Two research questions guided the analysis:\n(RQ1) How do multimodal LLMs perform on image classification of Christian\nsaints? And (RQ2), how does performance vary when enriching input with\ncontextual information or few-shot exemplars? We conducted a benchmarking study\nusing three datasets supporting Iconclass natively: ArtDL, ICONCLASS, and\nWikidata, filtered to include the top 10 most frequent classes. Models were\ntested under three conditions: (1) classification using class labels, (2)\nclassification with Iconclass descriptions, and (3) few-shot learning with five\nexemplars. Results were compared against ResNet50 baselines fine-tuned on the\nsame datasets. The findings show that Gemini-2.5 Pro and GPT-4o outperformed\nthe ResNet50 baselines. Accuracy dropped significantly on the Wikidata dataset,\nwhere Siglip reached the highest accuracy score, suggesting model sensitivity\nto image size and metadata alignment. Enriching prompts with class descriptions\ngenerally improved zero-shot performance, while few-shot learning produced\nlower results, with only occasional and minimal increments in accuracy. We\nconclude that general-purpose multimodal LLMs are capable of classification in\nvisually complex cultural heritage domains. These results support the\napplication of LLMs as metadata curation tools in digital humanities workflows,\nsuggesting future research on prompt optimization and the expansion of the\nstudy to other classification strategies and models.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5728\u57fa\u7763\u6559\u5723\u50cf\u5b66\u5355\u6807\u7b7e\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\uff0c\u5e76\u4e0e\u4f20\u7edf\u76d1\u7763\u5206\u7c7b\u5668\u8fdb\u884c\u6bd4\u8f83\u3002\u7814\u7a76\u53d1\u73b0\uff0cGemini-2.5 Pro\u548cGPT-4o\u5728\u90e8\u5206\u6570\u636e\u96c6\u4e0a\u4f18\u4e8eResNet50\u57fa\u7ebf\u6a21\u578b\uff0c\u4f46\u6a21\u578b\u5bf9\u6570\u636e\u96c6\uff08\u5982Wikidata\uff09\u548c\u8f93\u5165\u4fe1\u606f\uff08\u5982\u7c7b\u63cf\u8ff0\u6216\u5c11\u91cf\u6837\u672c\uff09\u654f\u611f\u3002\u6700\u7ec8\u7ed3\u8bba\u662f\uff0c\u901a\u7528\u591a\u6a21\u6001LLM\u53ef\u7528\u4e8e\u6587\u5316\u9057\u4ea7\u9886\u57df\u7684\u56fe\u50cf\u5206\u7c7b\uff0c\u5e76\u53ef\u4f5c\u4e3a\u6570\u5b57\u4eba\u6587\u5de5\u4f5c\u6d41\u4e2d\u7684\u5143\u6570\u636e\u7b56\u5c55\u5de5\u5177\u3002", "motivation": "\u8bc4\u4f30\u901a\u7528\u591a\u6a21\u6001LLM\uff08\u5982GPT-4o\u3001Gemini 2.5\uff09\u548cVLM\uff08\u5982CLIP\u3001SigLIP\uff09\u5728\u57fa\u7763\u6559\u5723\u50cf\u5b66\u5355\u6807\u7b7e\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e0a\u7684\u80fd\u529b\uff0c\u5e76\u4e0e\u4f20\u7edf\u76d1\u7763\u5206\u7c7b\u5668\uff08ResNet50\uff09\u8fdb\u884c\u6027\u80fd\u6bd4\u8f83\u3002", "method": "\u4f7f\u7528ArtDL\u3001ICONCLASS\u548cWikidata\u4e09\u4e2a\u6570\u636e\u96c6\uff0c\u5305\u542b\u524d10\u4e2a\u6700\u5e38\u89c1\u7c7b\u522b\u3002\u5728\u4e09\u79cd\u6761\u4ef6\u4e0b\u6d4b\u8bd5\u6a21\u578b\uff1a1\uff09\u4f7f\u7528\u7c7b\u522b\u6807\u7b7e\u8fdb\u884c\u5206\u7c7b\uff1b2\uff09\u63d0\u4f9bIconclass\u63cf\u8ff0\uff1b3\uff09\u8fdb\u884c\u5305\u542b\u4e94\u4e2a\u6837\u672c\u7684\u5c11\u6837\u672c\u5b66\u4e60\u3002\u5c06\u7ed3\u679c\u4e0e\u5728\u76f8\u540c\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u7684ResNet50\u57fa\u7ebf\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "Gemini-2.5 Pro\u548cGPT-4o\u5728\u90e8\u5206\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8eResNet50\u57fa\u7ebf\u3002Wikidata\u6570\u636e\u96c6\u4e0a\u7684\u51c6\u786e\u7387\u663e\u8457\u4e0b\u964d\uff0c\u4f46Siglip\u5728\u8be5\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u9ad8\u51c6\u786e\u7387\u3002\u5728\u63d0\u793a\u4e2d\u6dfb\u52a0\u7c7b\u522b\u63cf\u8ff0\u901a\u5e38\u80fd\u63d0\u9ad8\u96f6\u6837\u672c\u6027\u80fd\uff0c\u800c\u5c11\u6837\u672c\u5b66\u4e60\u6548\u679c\u4e0d\u4f73\u3002\u591a\u6a21\u6001LLM\u80fd\u591f\u5904\u7406\u89c6\u89c9\u590d\u6742\u7684\u6587\u5316\u9057\u4ea7\u9886\u57df\u7684\u5206\u7c7b\u4efb\u52a1\u3002", "conclusion": "\u901a\u7528\u591a\u6a21\u6001LLM\u80fd\u591f\u80dc\u4efb\u89c6\u89c9\u590d\u6742\u6587\u5316\u9057\u4ea7\u9886\u57df\u7684\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\uff0c\u5e76\u53ef\u4f5c\u4e3a\u6570\u5b57\u4eba\u6587\u5de5\u4f5c\u6d41\u4e2d\u7684\u5143\u6570\u636e\u7b56\u5c55\u5de5\u5177\u3002\u672a\u6765\u7684\u7814\u7a76\u5e94\u96c6\u4e2d\u4e8e\u4f18\u5316\u63d0\u793a\u548c\u63a2\u7d22\u5176\u4ed6\u5206\u7c7b\u7b56\u7565\u53ca\u6a21\u578b\u3002"}}
{"id": "2509.18568", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18568", "abs": "https://arxiv.org/abs/2509.18568", "authors": ["Niharika Tewari", "Nguyen Linh Dan Le", "Mujie Liu", "Jing Ren", "Ziqi Xu", "Tabinda Sarwar", "Veeky Baths", "Feng Xia"], "title": "Explainable Graph Neural Networks: Understanding Brain Connectivity and Biomarkers in Dementia", "comment": null, "summary": "Dementia is a progressive neurodegenerative disorder with multiple\netiologies, including Alzheimer's disease, Parkinson's disease, frontotemporal\ndementia, and vascular dementia. Its clinical and biological heterogeneity\nmakes diagnosis and subtype differentiation highly challenging. Graph Neural\nNetworks (GNNs) have recently shown strong potential in modeling brain\nconnectivity, but their limited robustness, data scarcity, and lack of\ninterpretability constrain clinical adoption. Explainable Graph Neural Networks\n(XGNNs) have emerged to address these barriers by combining graph-based\nlearning with interpretability, enabling the identification of disease-relevant\nbiomarkers, analysis of brain network disruptions, and provision of transparent\ninsights for clinicians. This paper presents the first comprehensive review\ndedicated to XGNNs in dementia research. We examine their applications across\nAlzheimer's disease, Parkinson's disease, mild cognitive impairment, and\nmulti-disease diagnosis. A taxonomy of explainability methods tailored for\ndementia-related tasks is introduced, alongside comparisons of existing models\nin clinical scenarios. We also highlight challenges such as limited\ngeneralizability, underexplored domains, and the integration of Large Language\nModels (LLMs) for early detection. By outlining both progress and open\nproblems, this review aims to guide future work toward trustworthy, clinically\nmeaningful, and scalable use of XGNNs in dementia research.", "AI": {"tldr": "\u672c\u7bc7\u8bba\u6587\u5168\u9762\u7efc\u8ff0\u4e86\u53ef\u89e3\u91ca\u56fe\u795e\u7ecf\u7f51\u7edc\uff08XGNNs\uff09\u5728\u75f4\u5446\u75c7\u7814\u7a76\u4e2d\u7684\u5e94\u7528\uff0c\u91cd\u70b9\u5173\u6ce8\u5176\u5728\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u3001\u5e15\u91d1\u68ee\u75c5\u7b49\u75be\u75c5\u8bca\u65ad\u548c\u751f\u7269\u6807\u5fd7\u7269\u8bc6\u522b\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u5e76\u63a2\u8ba8\u4e86\u8be5\u9886\u57df\u7684\u6311\u6218\u4e0e\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u75f4\u5446\u75c7\u7684\u5f02\u8d28\u6027\u7ed9\u8bca\u65ad\u548c\u4e9a\u578b\u533a\u5206\u5e26\u6765\u6311\u6218\u3002\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u5728\u5efa\u6a21\u5927\u8111\u8fde\u63a5\u65b9\u9762\u6709\u6f5c\u529b\uff0c\u4f46\u5176\u9c81\u68d2\u6027\u3001\u6570\u636e\u7a00\u758f\u6027\u548c\u53ef\u89e3\u91ca\u6027\u4e0d\u8db3\u9650\u5236\u4e86\u4e34\u5e8a\u5e94\u7528\u3002\u53ef\u89e3\u91ca\u56fe\u795e\u7ecf\u7f51\u7edc\uff08XGNNs\uff09\u7ed3\u5408\u4e86\u56fe\u5b66\u4e60\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u6709\u671b\u514b\u670d\u8fd9\u4e9b\u969c\u788d\u3002", "method": "\u672c\u6587\u5168\u9762\u56de\u987e\u4e86XGNNs\u5728\u75f4\u5446\u75c7\u7814\u7a76\u4e2d\u7684\u5e94\u7528\uff0c\u6db5\u76d6\u4e86\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u3001\u5e15\u91d1\u68ee\u75c5\u3001\u8f7b\u5ea6\u8ba4\u77e5\u969c\u788d\u548c\u591a\u75be\u75c5\u8bca\u65ad\u7b49\u9886\u57df\u3002\u6587\u7ae0\u8fd8\u63d0\u51fa\u4e86\u4e00\u4e2a\u9488\u5bf9\u75f4\u5446\u75c7\u76f8\u5173\u4efb\u52a1\u7684\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u5206\u7c7b\uff0c\u5e76\u5bf9\u73b0\u6709\u6a21\u578b\u5728\u4e34\u5e8a\u573a\u666f\u4e2d\u7684\u5e94\u7528\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "result": "XGNNs\u5728\u8bc6\u522b\u75be\u75c5\u76f8\u5173\u751f\u7269\u6807\u5fd7\u7269\u3001\u5206\u6790\u5927\u8111\u7f51\u7edc\u4e2d\u65ad\u4ee5\u53ca\u4e3a\u4e34\u5e8a\u533b\u751f\u63d0\u4f9b\u900f\u660e\u7684\u89c1\u89e3\u65b9\u9762\u5c55\u73b0\u4e86\u6f5c\u529b\u3002\u6587\u7ae0\u5bf9\u73b0\u6709\u6a21\u578b\u5728\u4e34\u5e8a\u573a\u666f\u4e2d\u7684\u5e94\u7528\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "conclusion": "XGNNs\u5728\u75f4\u5446\u75c7\u7814\u7a76\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u4ecd\u9762\u4e34\u6cdb\u5316\u6027\u6709\u9650\u3001\u672a\u5145\u5206\u63a2\u7d22\u7684\u9886\u57df\u4ee5\u53ca\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7ed3\u5408\u4ee5\u5b9e\u73b0\u65e9\u671f\u68c0\u6d4b\u7b49\u6311\u6218\u3002\u672c\u7efc\u8ff0\u65e8\u5728\u6307\u5bfc\u672a\u6765\u5de5\u4f5c\uff0c\u5b9e\u73b0XGNNs\u5728\u75f4\u5446\u75c7\u7814\u7a76\u4e2d\u503c\u5f97\u4fe1\u8d56\u3001\u5177\u6709\u4e34\u5e8a\u610f\u4e49\u548c\u53ef\u6269\u5c55\u7684\u5e94\u7528\u3002"}}
{"id": "2509.18840", "categories": ["cs.CV", "I.2.10"], "pdf": "https://arxiv.org/pdf/2509.18840", "abs": "https://arxiv.org/abs/2509.18840", "authors": ["Ismael Elsharkawi", "Hossam Sharara", "Ahmed Rafea"], "title": "ViG-LRGC: Vision Graph Neural Networks with Learnable Reparameterized Graph Construction", "comment": "Under Review", "summary": "Image Representation Learning is an important problem in Computer Vision.\nTraditionally, images were processed as grids, using Convolutional Neural\nNetworks or as a sequence of visual tokens, using Vision Transformers.\nRecently, Vision Graph Neural Networks (ViG) have proposed the treatment of\nimages as a graph of nodes; which provides a more intuitive image\nrepresentation. The challenge is to construct a graph of nodes in each layer\nthat best represents the relations between nodes and does not need a\nhyper-parameter search. ViG models in the literature depend on\nnon-parameterized and non-learnable statistical methods that operate on the\nlatent features of nodes to create a graph. This might not select the best\nneighborhood for each node. Starting from k-NN graph construction to HyperGraph\nConstruction and Similarity-Thresholded graph construction, these methods lack\nthe ability to provide a learnable hyper-parameter-free graph construction\nmethod. To overcome those challenges, we present the Learnable Reparameterized\nGraph Construction (LRGC) for Vision Graph Neural Networks. LRGC applies\nkey-query attention between every pair of nodes; then uses soft-threshold\nreparameterization for edge selection, which allows the use of a differentiable\nmathematical model for training. Using learnable parameters to select the\nneighborhood removes the bias that is induced by any clustering or thresholding\nmethods previously introduced in the literature. In addition, LRGC allows\ntuning the threshold in each layer to the training data since the thresholds\nare learnable through training and are not provided as hyper-parameters to the\nmodel. We demonstrate that the proposed ViG-LRGC approach outperforms\nstate-of-the-art ViG models of similar sizes on the ImageNet-1k benchmark\ndataset.", "AI": {"tldr": "ViG-LRGC\u662f\u4e00\u79cd\u7528\u4e8e\u56fe\u50cf\u7684\u65b0\u578b\u56fe\u795e\u7ecf\u7f51\u7edc\uff0c\u5b83\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u53c2\u6570\u81ea\u52a8\u6784\u5efa\u56fe\u7ed3\u6784\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728ImageNet-1k\u4e0a\u53d6\u5f97\u4e86\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9\u56fe\u795e\u7ecf\u7f51\u7edc\uff08ViG\uff09\u5728\u6784\u5efa\u8282\u70b9\u56fe\u65f6\u4f9d\u8d56\u4e8e\u975e\u53c2\u6570\u5316\u3001\u4e0d\u53ef\u5b66\u4e60\u7684\u7edf\u8ba1\u65b9\u6cd5\uff0c\u53ef\u80fd\u65e0\u6cd5\u9009\u62e9\u6700\u4f73\u90bb\u57df\uff0c\u5e76\u4e14\u9700\u8981\u8d85\u53c2\u6570\u641c\u7d22\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u5b66\u4e60\u7684\u91cd\u53c2\u6570\u5316\u56fe\u6784\u5efa\uff08LRGC\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u952e\u67e5\u8be2\u6ce8\u610f\u529b\u673a\u5236\u8ba1\u7b97\u8282\u70b9\u95f4\u7684\u5173\u7cfb\uff0c\u5e76\u4f7f\u7528\u8f6f\u9608\u503c\u91cd\u53c2\u6570\u5316\u8fdb\u884c\u8fb9\u9009\u62e9\uff0c\u5b9e\u73b0\u4e86\u53ef\u5fae\u5206\u7684\u3001\u65e0\u8d85\u53c2\u6570\u7684\u56fe\u6784\u5efa\u3002", "result": "\u5728ImageNet-1k\u6570\u636e\u96c6\u4e0a\uff0c\u63d0\u51fa\u7684ViG-LRGC\u6a21\u578b\u5728\u76f8\u4f3c\u6a21\u578b\u5c3a\u5bf8\u4e0b\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u7684ViG\u6a21\u578b\u3002", "conclusion": "LRGC\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u3001\u53ef\u5b66\u4e60\u7684\u3001\u65e0\u8d85\u53c2\u6570\u7684\u56fe\u6784\u5efa\u65b9\u6cd5\uff0c\u80fd\u591f\u4e3aViG\u6a21\u578b\u5e26\u6765\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2509.18584", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18584", "abs": "https://arxiv.org/abs/2509.18584", "authors": ["Mingchun Sun", "Rongqiang Zhao", "Jie Liu"], "title": "DS-Diffusion: Data Style-Guided Diffusion Model for Time-Series Generation", "comment": null, "summary": "Diffusion models are the mainstream approach for time series generation\ntasks. However, existing diffusion models for time series generation require\nretraining the entire framework to introduce specific conditional guidance.\nThere also exists a certain degree of distributional bias between the generated\ndata and the real data, which leads to potential model biases in downstream\ntasks. Additionally, the complexity of diffusion models and the latent spaces\nleads to an uninterpretable inference process. To address these issues, we\npropose the data style-guided diffusion model (DS-Diffusion). In the\nDS-Diffusion, a diffusion framework based on style-guided kernels is developed\nto avoid retraining for specific conditions. The time-information based\nhierarchical denoising mechanism (THD) is developed to reduce the\ndistributional bias between the generated data and the real data. Furthermore,\nthe generated samples can clearly indicate the data style from which they\noriginate. We conduct comprehensive evaluations using multiple public datasets\nto validate our approach. Experimental results show that, compared to the\nstate-of-the-art model such as ImagenTime, the predictive score and the\ndiscriminative score decrease by 5.56% and 61.55%, respectively. The\ndistributional bias between the generated data and the real data is further\nreduced, the inference process is also more interpretable. Moreover, by\neliminating the need to retrain the diffusion model, the flexibility and\nadaptability of the model to specific conditions are also enhanced.", "AI": {"tldr": "DS-Diffusion \u901a\u8fc7\u98ce\u683c\u5f15\u5bfc\u7684\u6838\u548c\u5206\u5c42\u53bb\u566a\u673a\u5236\u89e3\u51b3\u4e86\u73b0\u6709\u65f6\u95f4\u5e8f\u5217\u751f\u6210\u6269\u6563\u6a21\u578b\u5728\u6761\u4ef6\u5f15\u5165\u3001\u5206\u5e03\u504f\u5dee\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u7684\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u751f\u6210\u6570\u636e\u7684\u8d28\u91cf\u548c\u7075\u6d3b\u6027\u3002", "motivation": "\u73b0\u6709\u65f6\u95f4\u5e8f\u5217\u751f\u6210\u6269\u6563\u6a21\u578b\u5728\u5f15\u5165\u7279\u5b9a\u6761\u4ef6\u65f6\u9700\u8981\u91cd\u65b0\u8bad\u7ec3\u6574\u4e2a\u6846\u67b6\uff0c\u5b58\u5728\u751f\u6210\u6570\u636e\u4e0e\u771f\u5b9e\u6570\u636e\u4e4b\u95f4\u7684\u5206\u5e03\u504f\u5dee\uff0c\u5e76\u4e14\u63a8\u7406\u8fc7\u7a0b\u4e0d\u76f4\u89c2\uff0c\u53ef\u80fd\u5bfc\u81f4\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u6a21\u578b\u504f\u5dee\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u98ce\u683c\u5f15\u5bfc\u6838\u7684\u6269\u6563\u6a21\u578b\uff08DS-Diffusion\uff09\uff0c\u8be5\u6a21\u578b\u5305\u542b\u65f6\u95f4\u4fe1\u606f\u7684\u5206\u5c42\u53bb\u566a\u673a\u5236\uff08THD\uff09\uff0c\u4ee5\u907f\u514d\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u8fdb\u884c\u91cd\u65b0\u8bad\u7ec3\uff0c\u5e76\u51cf\u5c11\u751f\u6210\u6570\u636e\u4e0e\u771f\u5b9e\u6570\u636e\u4e4b\u95f4\u7684\u5206\u5e03\u504f\u5dee\u3002", "result": "\u4e0e ImagenTime \u7b49\u6700\u5148\u8fdb\u7684\u6a21\u578b\u76f8\u6bd4\uff0cDS-Diffusion \u7684\u9884\u6d4b\u5206\u6570\u548c\u5224\u522b\u5206\u6570\u5206\u522b\u964d\u4f4e\u4e86 5.56% \u548c 61.55%\uff0c\u8fdb\u4e00\u6b65\u51cf\u5c11\u4e86\u751f\u6210\u6570\u636e\u4e0e\u771f\u5b9e\u6570\u636e\u4e4b\u95f4\u7684\u5206\u5e03\u504f\u5dee\uff0c\u63a8\u7406\u8fc7\u7a0b\u4e5f\u66f4\u5177\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "DS-Diffusion \u901a\u8fc7\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u7684\u7075\u6d3b\u6027\u548c\u9002\u5e94\u6027\uff0c\u540c\u65f6\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2509.18891", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18891", "abs": "https://arxiv.org/abs/2509.18891", "authors": ["Xueyu Liu", "Xiaoyi Zhang", "Guangze Shi", "Meilin Liu", "Yexin Lai", "Yongfei Wu", "Mingqiang Wei"], "title": "Attack for Defense: Adversarial Agents for Point Prompt Optimization Empowering Segment Anything Model", "comment": null, "summary": "Prompt quality plays a critical role in the performance of the Segment\nAnything Model (SAM), yet existing approaches often rely on heuristic or\nmanually crafted prompts, limiting scalability and generalization. In this\npaper, we propose Point Prompt Defender, an adversarial reinforcement learning\nframework that adopts an attack-for-defense paradigm to automatically optimize\npoint prompts. We construct a task-agnostic point prompt environment by\nrepresenting image patches as nodes in a dual-space graph, where edges encode\nboth physical and semantic distances. Within this environment, an attacker\nagent learns to activate a subset of prompts that maximally degrade SAM's\nsegmentation performance, while a defender agent learns to suppress these\ndisruptive prompts and restore accuracy. Both agents are trained using Deep\nQ-Networks with a reward signal based on segmentation quality variation. During\ninference, only the defender is deployed to refine arbitrary coarse prompt\nsets, enabling enhanced SAM segmentation performance across diverse tasks\nwithout retraining. Extensive experiments show that Point Prompt Defender\neffectively improves SAM's robustness and generalization, establishing a\nflexible, interpretable, and plug-and-play framework for prompt-based\nsegmentation.", "AI": {"tldr": "Point Prompt Defender\u662f\u4e00\u4e2a\u5bf9\u6297\u6027\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u52a8\u4f18\u5316\u70b9\u63d0\u793a\u6765\u63d0\u9ad8SAM\u6a21\u578b\u7684\u5206\u5272\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709SAM\u6a21\u578b\u4f9d\u8d56\u4e8e\u542f\u53d1\u5f0f\u6216\u624b\u52a8\u8bbe\u8ba1\u7684\u63d0\u793a\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u8be5\u6846\u67b6\u6784\u5efa\u4e86\u4e00\u4e2a\u65e0\u4efb\u52a1\u7684\u70b9\u63d0\u793a\u73af\u5883\uff0c\u5c06\u56fe\u50cf\u5757\u8868\u793a\u4e3a\u53cc\u7a7a\u95f4\u56fe\u4e2d\u7684\u8282\u70b9\uff0c\u5e76\u4f7f\u7528\u5bf9\u6297\u6027\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u653b\u51fb\u8005\u548c\u9632\u5fa1\u8005\u4ee3\u7406\u6765\u4f18\u5316\u63d0\u793a\u3002\u653b\u51fb\u8005\u4ee3\u7406\u65e8\u5728\u6700\u5927\u5316\u964d\u4f4eSAM\u7684\u5206\u5272\u6027\u80fd\uff0c\u800c\u9632\u5fa1\u8005\u4ee3\u7406\u5219\u5b66\u4e60\u6291\u5236\u8fd9\u4e9b\u7834\u574f\u6027\u63d0\u793a\u5e76\u6062\u590d\u51c6\u786e\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cPoint Prompt Defender\u80fd\u6709\u6548\u63d0\u9ad8SAM\u5728\u5404\u79cd\u4efb\u52a1\u4e0a\u7684\u5206\u5272\u6027\u80fd\u3001\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4e14\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\u3002", "conclusion": "Point Prompt Defender\u662f\u4e00\u4e2a\u7075\u6d3b\u3001\u53ef\u89e3\u91ca\u4e14\u5373\u63d2\u5373\u7528\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u57fa\u4e8e\u63d0\u793a\u7684\u5206\u5272\u3002"}}
{"id": "2509.18607", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18607", "abs": "https://arxiv.org/abs/2509.18607", "authors": ["Qiuhai Zeng", "Sarvesh Rajkumar", "Di Wang", "Narendra Gyanchandani", "Wenbo Yan"], "title": "Reflect before Act: Proactive Error Correction in Language Models", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ninteractive decision-making tasks, but existing methods often struggle with\nerror accumulation and lack robust self-correction mechanisms. We introduce\n\"Reflect before Act\" (REBACT), a novel approach that enhances LLM-based\ndecision-making by introducing a critical reflect step prior to taking the next\naction. This approach allows for immediate error correction, ensuring smooth\naction path and adaptibity to environment feedback. We evaluate REBACT on three\ndiverse interactive environments: ALFWorld, WebShop, and TextCraft. Our results\ndemonstrate that REBACT significantly outperforms strong baselines, improving\nsuccess rates by up to 24% on WebShop (achieving 61%), 6.72% on ALFWorld\n(achieving 98.51%), and 0.5% on TextCraft (achieving 99.5%) using\nClaude3.5-sonnet as the underlying LLM. Further analysis reveals that REBACT's\nperformance improvements are achieved with only a few modification steps,\ndemonstrating its computational efficiency.", "AI": {"tldr": "REBACT\u901a\u8fc7\u5f15\u5165\u201c\u5148\u53cd\u601d\u540e\u884c\u52a8\u201d\u673a\u5236\uff0c\u5728\u4ea4\u4e92\u5f0f\u51b3\u7b56\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\uff0c\u63d0\u9ad8\u4e86\u6210\u529f\u7387\u5e76\u5177\u6709\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4ea4\u4e92\u5f0f\u51b3\u7b56\u4efb\u52a1\u4e2d\u5b58\u5728\u9519\u8bef\u7d2f\u79ef\u548c\u7f3a\u4e4f\u6709\u6548\u7684\u81ea\u6211\u7ea0\u6b63\u673a\u5236\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u5148\u53cd\u601d\u540e\u884c\u52a8\u201d\uff08REBACT\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u5728\u6267\u884c\u4e0b\u4e00\u6b65\u52a8\u4f5c\u4e4b\u524d\u589e\u52a0\u4e86\u4e00\u4e2a\u5173\u952e\u7684\u53cd\u601d\u6b65\u9aa4\uff0c\u4ee5\u5b9e\u73b0\u5373\u65f6\u7ea0\u9519\u3001\u786e\u4fdd\u52a8\u4f5c\u8def\u5f84\u7684\u987a\u7545\u4ee5\u53ca\u9002\u5e94\u73af\u5883\u53cd\u9988\u3002", "result": "REBACT\u5728ALFWorld\u3001WebShop\u548cTextCraft\u4e09\u4e2a\u73af\u5883\u4e2d\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u76f8\u6bd4\u5f3a\u57fa\u7ebf\u6a21\u578b\uff0c\u5728WebShop\u4e0a\u7684\u6210\u529f\u7387\u63d0\u9ad8\u4e8624%\uff08\u8fbe\u523061%\uff09\uff0c\u5728ALFWorld\u4e0a\u63d0\u9ad8\u4e866.72%\uff08\u8fbe\u523098.51%\uff09\uff0c\u5728TextCraft\u4e0a\u63d0\u9ad8\u4e860.5%\uff08\u8fbe\u523099.5%\uff09\uff0c\u540c\u65f6\u4ec5\u9700\u5c11\u91cf\u4fee\u6539\u6b65\u9aa4\uff0c\u8bc1\u660e\u4e86\u5176\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "REBACT\u662f\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4ea4\u4e92\u5f0f\u51b3\u7b56\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u901a\u8fc7\u5f15\u5165\u53cd\u601d\u6b65\u9aa4\u6765\u7ea0\u6b63\u9519\u8bef\u5e76\u9002\u5e94\u73af\u5883\u3002"}}
{"id": "2509.18894", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18894", "abs": "https://arxiv.org/abs/2509.18894", "authors": ["Jenna Kline", "Anirudh Potlapally", "Bharath Pillai", "Tanishka Wani", "Rugved Katole", "Vedant Patil", "Penelope Covey", "Hari Subramoni", "Tanya Berger-Wolf", "Christopher Stewart"], "title": "SmartWilds: Multimodal Wildlife Monitoring Dataset", "comment": "8 pages", "summary": "We present the first release of SmartWilds, a multimodal wildlife monitoring\ndataset. SmartWilds is a synchronized collection of drone imagery, camera trap\nphotographs and videos, and bioacoustic recordings collected during summer 2025\nat The Wilds safari park in Ohio. This dataset supports multimodal AI research\nfor comprehensive environmental monitoring, addressing critical needs in\nendangered species research, conservation ecology, and habitat management. Our\npilot deployment captured four days of synchronized monitoring across three\nmodalities in a 220-acre pasture containing Pere David's deer, Sichuan takin,\nPrzewalski's horses, as well as species native to Ohio, including bald eagles,\nwhite-tailed deer, and coyotes. We provide a comparative analysis of sensor\nmodality performance, demonstrating complementary strengths for landuse\npatterns, species detection, behavioral analysis, and habitat monitoring. This\nwork establishes reproducible protocols for multimodal wildlife monitoring\nwhile contributing open datasets to advance conservation computer vision\nresearch. Future releases will include synchronized GPS tracking data from\ntagged individuals, citizen science data, and expanded temporal coverage across\nmultiple seasons.", "AI": {"tldr": "SmartWilds \u662f\u4e00\u4e2a\u5305\u542b\u65e0\u4eba\u673a\u5f71\u50cf\u3001\u76f8\u673a\u9677\u9631\u7167\u7247\u548c\u89c6\u9891\u4ee5\u53ca\u751f\u7269\u58f0\u5b66\u8bb0\u5f55\u7684\u591a\u6a21\u6001\u91ce\u751f\u52a8\u7269\u76d1\u6d4b\u6570\u636e\u96c6\uff0c\u652f\u6301\u591a\u6a21\u6001\u4eba\u5de5\u667a\u80fd\u7814\u7a76\uff0c\u4ee5\u5e94\u5bf9\u6fd2\u5371\u7269\u79cd\u7814\u7a76\u3001\u4fdd\u62a4\u751f\u6001\u5b66\u548c\u6816\u606f\u5730\u7ba1\u7406\u7684\u5173\u952e\u9700\u6c42\u3002", "motivation": "\u652f\u6301\u591a\u6a21\u6001\u4eba\u5de5\u667a\u80fd\u7814\u7a76\uff0c\u4ee5\u5e94\u5bf9\u6fd2\u5371\u7269\u79cd\u7814\u7a76\u3001\u4fdd\u62a4\u751f\u6001\u5b66\u548c\u6816\u606f\u5730\u7ba1\u7406\u7684\u5173\u952e\u9700\u6c42\u3002", "method": "\u6536\u96c6\u4e862025\u5e74\u590f\u5b63\u5728\u4fc4\u4ea5\u4fc4\u5dde\u91ce\u751f\u52a8\u7269\u4fdd\u62a4\u533a\u7684\u540c\u6b65\u591a\u6a21\u6001\u6570\u636e\uff08\u65e0\u4eba\u673a\u5f71\u50cf\u3001\u76f8\u673a\u9677\u9631\u7167\u7247\u548c\u89c6\u9891\u3001\u751f\u7269\u58f0\u5b66\u8bb0\u5f55\uff09\uff0c\u5e76\u5bf9\u4f20\u611f\u5668\u6a21\u6001\u6027\u80fd\u8fdb\u884c\u4e86\u6bd4\u8f83\u5206\u6790\u3002", "result": "\u5c55\u793a\u4e86\u4f20\u611f\u5668\u6a21\u6001\u5728\u571f\u5730\u5229\u7528\u6a21\u5f0f\u3001\u7269\u79cd\u68c0\u6d4b\u3001\u884c\u4e3a\u5206\u6790\u548c\u6816\u606f\u5730\u76d1\u6d4b\u65b9\u9762\u7684\u4e92\u8865\u4f18\u52bf\u3002", "conclusion": "\u5efa\u7acb\u4e86\u53ef\u91cd\u590d\u7684\u591a\u6a21\u6001\u91ce\u751f\u52a8\u7269\u76d1\u6d4b\u534f\u8bae\uff0c\u5e76\u8d21\u732e\u4e86\u5f00\u653e\u6570\u636e\u96c6\uff0c\u4ee5\u63a8\u8fdb\u4fdd\u62a4\u8ba1\u7b97\u673a\u89c6\u89c9\u7814\u7a76\u3002"}}
{"id": "2509.18611", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18611", "abs": "https://arxiv.org/abs/2509.18611", "authors": ["Zituo Chen", "Sili Deng"], "title": "Flow marching for a generative PDE foundation model", "comment": null, "summary": "Pretraining on large-scale collections of PDE-governed spatiotemporal\ntrajectories has recently shown promise for building generalizable models of\ndynamical systems. Yet most existing PDE foundation models rely on\ndeterministic Transformer architectures, which lack generative flexibility for\nmany science and engineering applications. We propose Flow Marching, an\nalgorithm that bridges neural operator learning with flow matching motivated by\nan analysis of error accumulation in physical dynamical systems, and we build a\ngenerative PDE foundation model on top of it. By jointly sampling the noise\nlevel and the physical time step between adjacent states, the model learns a\nunified velocity field that transports a noisy current state toward its clean\nsuccessor, reducing long-term rollout drift while enabling uncertainty-aware\nensemble generations. Alongside this core algorithm, we introduce a\nPhysics-Pretrained Variational Autoencoder (P2VAE) to embed physical states\ninto a compact latent space, and an efficient Flow Marching Transformer (FMT)\nthat combines a diffusion-forcing scheme with latent temporal pyramids,\nachieving up to 15x greater computational efficiency than full-length video\ndiffusion models and thereby enabling large-scale pretraining at substantially\nreduced cost. We curate a corpus of ~2.5M trajectories across 12 distinct PDE\nfamilies and train suites of P2VAEs and FMTs at multiple scales. On downstream\nevaluation, we benchmark on unseen Kolmogorov turbulence with few-shot\nadaptation, demonstrate long-term rollout stability over deterministic\ncounterparts, and present uncertainty-stratified ensemble results, highlighting\nthe importance of generative PDE foundation models for real-world applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86Flow Marching\u7b97\u6cd5\uff0c\u7ed3\u5408\u4e86\u795e\u7ecf\u7b97\u5b50\u5b66\u4e60\u548c\u6d41\u5339\u914d\uff0c\u5e76\u6784\u5efa\u4e86\u57fa\u4e8e\u6b64\u7684\u751f\u6210\u5f0fPDE\u57fa\u7840\u6a21\u578b\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u786e\u5b9a\u6027Transformer\u67b6\u6784\u5728\u79d1\u5b66\u548c\u5de5\u7a0b\u5e94\u7528\u4e2d\u7684\u751f\u6210\u7075\u6d3b\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002\u8be5\u6a21\u578b\u901a\u8fc7\u8054\u5408\u91c7\u6837\u566a\u58f0\u6c34\u5e73\u548c\u7269\u7406\u65f6\u95f4\u6b65\u957f\uff0c\u5b66\u4e60\u7edf\u4e00\u7684\u901f\u5ea6\u573a\uff0c\u5c06\u5e26\u566a\u58f0\u7684\u5f53\u524d\u72b6\u6001\u8fc1\u79fb\u5230\u5176\u5e72\u51c0\u7684\u540e\u7ee7\u72b6\u6001\uff0c\u4ece\u800c\u51cf\u5c11\u957f\u671f\u9884\u6d4b\u7684\u6f02\u79fb\u5e76\u5b9e\u73b0\u53ef\u611f\u77e5\u4e0d\u786e\u5b9a\u6027\u7684\u96c6\u5408\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8ePDE\u7684\u57fa\u91d1\u6a21\u578b\u5927\u591a\u91c7\u7528\u786e\u5b9a\u6027Transformer\u67b6\u6784\uff0c\u7f3a\u4e4f\u751f\u6210\u7075\u6d3b\u6027\uff0c\u65e0\u6cd5\u6ee1\u8db3\u8bb8\u591a\u79d1\u5b66\u548c\u5de5\u7a0b\u5e94\u7528\u7684\u9700\u6c42\u3002", "method": "\u63d0\u51faFlow Matching\u7b97\u6cd5\uff0c\u7ed3\u5408\u795e\u7ecf\u7b97\u5b50\u5b66\u4e60\u548c\u6d41\u5339\u914d\u3002\u5f15\u5165\u7269\u7406\u9884\u8bad\u7ec3\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08P2VAE\uff09\u5c06\u7269\u7406\u72b6\u6001\u5d4c\u5165\u7d27\u51d1\u7684\u6f5c\u5728\u7a7a\u95f4\u3002\u8bbe\u8ba1\u4e86\u9ad8\u6548\u7684Flow Matching Transformer\uff08FMT\uff09\uff0c\u7ed3\u5408\u4e86\u6269\u6563-\u5f3a\u5236\u65b9\u6848\u548c\u6f5c\u5728\u65f6\u95f4\u91d1\u5b57\u5854\u3002", "result": "\u5728~2.5M\u4e2a\u8f68\u8ff9\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002\u5728\u672a\u89c1\u8fc7\u7684Kolmogorov\u6e4d\u6d41\u4e0a\u8fdb\u884c\u4e86\u5c11\u6837\u672c\u9002\u5e94\u6027\u6d4b\u8bd5\uff0c\u5c55\u793a\u4e86\u6bd4\u786e\u5b9a\u6027\u6a21\u578b\u66f4\u4f18\u8d8a\u7684\u957f\u671f\u9884\u6d4b\u7a33\u5b9a\u6027\uff0c\u5e76\u5448\u73b0\u4e86\u4e0d\u786e\u5b9a\u6027\u5206\u5c42\u7684\u96c6\u5408\u7ed3\u679c\u3002", "conclusion": "\u751f\u6210\u5f0fPDE\u57fa\u7840\u6a21\u578b\u5bf9\u4e8e\u5b9e\u9645\u5e94\u7528\u81f3\u5173\u91cd\u8981\uff0cFlow Matching\u7b97\u6cd5\u53ca\u5176\u76f8\u5173\u6a21\u578b\uff08P2VAE\u548cFMT\uff09\u5728\u89e3\u51b3\u957f\u671f\u9884\u6d4b\u6f02\u79fb\u3001\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u548c\u5b9e\u73b0\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u751f\u6210\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\u3002"}}
{"id": "2509.18897", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18897", "abs": "https://arxiv.org/abs/2509.18897", "authors": ["Jiayu Wang", "Ruizhi Wang", "Jie Song", "Haofei Zhang", "Mingli Song", "Zunlei Feng", "Li Sun"], "title": "RS3DBench: A Comprehensive Benchmark for 3D Spatial Perception in Remote Sensing", "comment": "26 pages, 4 figures", "summary": "In this paper, we introduce a novel benchmark designed to propel the\nadvancement of general-purpose, large-scale 3D vision models for remote sensing\nimagery. While several datasets have been proposed within the realm of remote\nsensing, many existing collections either lack comprehensive depth information\nor fail to establish precise alignment between depth data and remote sensing\nimages. To address this deficiency, we present a visual Benchmark for 3D\nunderstanding of Remotely Sensed images, dubbed RS3DBench. This dataset\nencompasses 54,951 pairs of remote sensing images and pixel-level aligned depth\nmaps, accompanied by corresponding textual descriptions, spanning a broad array\nof geographical contexts. It serves as a tool for training and assessing 3D\nvisual perception models within remote sensing image spatial understanding\ntasks. Furthermore, we introduce a remotely sensed depth estimation model\nderived from stable diffusion, harnessing its multimodal fusion capabilities,\nthereby delivering state-of-the-art performance on our dataset. Our endeavor\nseeks to make a profound contribution to the evolution of 3D visual perception\nmodels and the advancement of geographic artificial intelligence within the\nremote sensing domain. The dataset, models and code will be accessed on the\nhttps://rs3dbench.github.io.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aRS3DBench\u7684\u65b0\u578b\u57fa\u51c6\uff0c\u7528\u4e8e\u63a8\u52a8\u9065\u611f\u56fe\u50cf\u901a\u7528\u5927\u6a21\u578b3D\u89c6\u89c9\u7684\u53d1\u5c55\u3002\u8be5\u57fa\u51c6\u5305\u542b54,951\u5bf9\u9065\u611f\u56fe\u50cf\u53ca\u5176\u50cf\u7d20\u7ea7\u5bf9\u9f50\u7684\u6df1\u5ea6\u56fe\u548c\u6587\u672c\u63cf\u8ff0\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6570\u636e\u96c6\u6df1\u5ea6\u4fe1\u606f\u4e0d\u5168\u6216\u5bf9\u9f50\u4e0d\u7cbe\u786e\u7684\u95ee\u9898\u3002\u6b64\u5916\uff0c\u8bba\u6587\u8fd8\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u57fa\u4e8estable diffusion\u7684\u9065\u611f\u6df1\u5ea6\u4f30\u8ba1\u6a21\u578b\uff0c\u5e76\u5728\u8be5\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u9065\u611f\u6570\u636e\u96c6\u5728\u6df1\u5ea6\u4fe1\u606f\u6216\u6df1\u5ea6\u56fe\u4e0e\u9065\u611f\u56fe\u50cf\u7684\u7cbe\u786e\u5bf9\u9f50\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u963b\u788d\u4e86\u901a\u7528\u5927\u6a21\u578b3D\u89c6\u89c9\u7684\u53d1\u5c55\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b54,951\u5bf9\u9065\u611f\u56fe\u50cf\u3001\u50cf\u7d20\u7ea7\u5bf9\u9f50\u7684\u6df1\u5ea6\u56fe\u548c\u6587\u672c\u63cf\u8ff0\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6RS3DBench\u3002\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8estable diffusion\u7684\u9065\u611f\u6df1\u5ea6\u4f30\u8ba1\u6a21\u578b\u3002", "result": "\u6240\u63d0\u51fa\u7684RS3DBench\u6570\u636e\u96c6\u4e3a3D\u89c6\u89c9\u611f\u77e5\u6a21\u578b\u5728\u9065\u611f\u56fe\u50cf\u7a7a\u95f4\u7406\u89e3\u4efb\u52a1\u4e0a\u7684\u8bad\u7ec3\u548c\u8bc4\u4f30\u63d0\u4f9b\u4e86\u5de5\u5177\u3002\u57fa\u4e8estable diffusion\u7684\u6a21\u578b\u5728RS3DBench\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "RS3DBench\u6570\u636e\u96c6\u548c\u63d0\u51fa\u7684\u6a21\u578b\u5c06\u4e3a\u9065\u611f\u9886\u57df\u76843D\u89c6\u89c9\u611f\u77e5\u6a21\u578b\u53d1\u5c55\u548c\u5730\u7406\u667a\u80fd\u7684\u8fdb\u6b65\u505a\u51fa\u8d21\u732e\u3002"}}
{"id": "2509.18629", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18629", "abs": "https://arxiv.org/abs/2509.18629", "authors": ["Abel Gurung", "Joseph Campbell"], "title": "HyperAdapt: Simple High-Rank Adaptation", "comment": null, "summary": "Foundation models excel across diverse tasks, but adapting them to\nspecialized applications often requires fine-tuning, an approach that is memory\nand compute-intensive. Parameter-efficient fine-tuning (PEFT) methods mitigate\nthis by updating only a small subset of weights. In this paper, we introduce\nHyperAdapt, a parameter-efficient fine-tuning method that significantly reduces\nthe number of trainable parameters compared to state-of-the-art methods like\nLoRA. Specifically, HyperAdapt adapts a pre-trained weight matrix by applying\nrow- and column-wise scaling through diagonal matrices, thereby inducing a\nhigh-rank update while requiring only $n+m$ trainable parameters for an $n\n\\times m$ matrix. Theoretically, we establish an upper bound on the rank of\nHyperAdapt's updates, and empirically, we confirm that it consistently induces\nhigh-rank transformations across model layers. Experiments on GLUE, arithmetic\nreasoning, and commonsense reasoning benchmarks with models up to 14B\nparameters demonstrate that HyperAdapt matches or nearly matches the\nperformance of full fine-tuning and state-of-the-art PEFT methods while using\norders of magnitude fewer trainable parameters.", "AI": {"tldr": "HyperAdapt\u662f\u4e00\u79cd\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08PEFT\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u5e94\u7528\u5bf9\u89d2\u77e9\u9635\u8fdb\u884c\u884c\u548c\u5217\u7f29\u653e\u6765\u8c03\u6574\u9884\u8bad\u7ec3\u6743\u91cd\u77e9\u9635\uff0c\u4ece\u800c\u4ee5\u66f4\u5c11\u7684\u53c2\u6570\u5b9e\u73b0\u4e0e\u5168\u5fae\u8c03\u548c\u73b0\u6709PEFT\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u7684\u5fae\u8c03\u65b9\u6cd5\u5728\u9002\u5e94\u7279\u5b9a\u5e94\u7528\u65f6\u9700\u8981\u5927\u91cf\u7684\u5185\u5b58\u548c\u8ba1\u7b97\u8d44\u6e90\u3002\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08PEFT\uff09\u65b9\u6cd5\u901a\u8fc7\u53ea\u66f4\u65b0\u4e00\u5c0f\u90e8\u5206\u6743\u91cd\u6765\u7f13\u89e3\u8fd9\u4e2a\u95ee\u9898\u3002", "method": "HyperAdapt\u901a\u8fc7\u5e94\u7528\u884c\u548c\u5217\u7684\u7f29\u653e\uff08\u901a\u8fc7\u5bf9\u89d2\u77e9\u9635\uff09\u6765\u8c03\u6574\u9884\u8bad\u7ec3\u7684\u6743\u91cd\u77e9\u9635\uff0c\u4ece\u800c\u5728\u4ec5\u9700\u8981n+m\u4e2a\u53ef\u8bad\u7ec3\u53c2\u6570\u7684\u60c5\u51b5\u4e0b\uff0c\u4e3a\u4e00\u4e2an*m\u7684\u77e9\u9635\u5f15\u5165\u9ad8\u79e9\u66f4\u65b0\u3002", "result": "\u5728GLUE\u3001\u7b97\u672f\u63a8\u7406\u548c\u5e38\u8bc6\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4f7f\u7528\u9ad8\u8fbe14B\u53c2\u6570\u7684\u6a21\u578b\u8fdb\u884c\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660eHyperAdapt\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\uff0c\u6240\u9700\u7684\u53ef\u8bad\u7ec3\u53c2\u6570\u6bd4\u73b0\u6709\u65b9\u6cd5\u5c11\u51e0\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "HyperAdapt\u5728\u53c2\u6570\u6570\u91cf\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u51cf\u5c11\uff0c\u540c\u65f6\u5728\u591a\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e0e\u5168\u5fae\u8c03\u548c\u6700\u5148\u8fdb\u7684PEFT\u65b9\u6cd5\u76f8\u5f53\u6216\u63a5\u8fd1\u5176\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u4f5c\u4e3a\u4e00\u79cd\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2509.18898", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18898", "abs": "https://arxiv.org/abs/2509.18898", "authors": ["Pengteng Li", "Yunfan Lu", "Pinhao Song", "Weiyu Guo", "Huizai Yao", "F. Richard Yu", "Hui Xiong"], "title": "DeblurSplat: SfM-free 3D Gaussian Splatting with Event Camera for Robust Deblurring", "comment": null, "summary": "In this paper, we propose the first Structure-from-Motion (SfM)-free\ndeblurring 3D Gaussian Splatting method via event camera, dubbed DeblurSplat.\nWe address the motion-deblurring problem in two ways. First, we leverage the\npretrained capability of the dense stereo module (DUSt3R) to directly obtain\naccurate initial point clouds from blurred images. Without calculating camera\nposes as an intermediate result, we avoid the cumulative errors transfer from\ninaccurate camera poses to the initial point clouds' positions. Second, we\nintroduce the event stream into the deblur pipeline for its high sensitivity to\ndynamic change. By decoding the latent sharp images from the event stream and\nblurred images, we can provide a fine-grained supervision signal for scene\nreconstruction optimization. Extensive experiments across a range of scenes\ndemonstrate that DeblurSplat not only excels in generating high-fidelity novel\nviews but also achieves significant rendering efficiency compared to the SOTAs\nin deblur 3D-GS.", "AI": {"tldr": "DeblurSplat\u662f\u4e00\u79cd\u521b\u65b0\u7684\u3001\u65e0\u9700\u8fd0\u52a8\u6062\u590d\u7ed3\u6784\uff08SfM\uff09\u7684\u4e09\u7ef4\u9ad8\u65af\u6cfc\u6e85\uff083D Gaussian Splatting\uff09\u65b9\u6cd5\uff0c\u5b83\u5229\u7528\u4e8b\u4ef6\u76f8\u673a\u6765\u89e3\u51b3\u8fd0\u52a8\u6a21\u7cca\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u8fd0\u52a8\u6a21\u7cca\u95ee\u9898\uff0c\u5e76\u907f\u514d\u4f20\u7edfSfM\u65b9\u6cd5\u4e2d\u7d2f\u79ef\u7684\u8bef\u5dee\u3002", "method": "1. \u5229\u7528\u9884\u8bad\u7ec3\u7684\u5bc6\u96c6\u7acb\u4f53\u5339\u914d\u6a21\u5757\uff08DUSt3R\uff09\u76f4\u63a5\u4ece\u6a21\u7cca\u56fe\u50cf\u4e2d\u83b7\u53d6\u521d\u59cb\u70b9\u4e91\uff0c\u7ed5\u8fc7\u4e2d\u95f4\u7684\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u30022. \u5f15\u5165\u4e8b\u4ef6\u6d41\uff08event stream\uff09\u7684\u9ad8\u52a8\u6001\u654f\u611f\u6027\uff0c\u4ece\u4e8b\u4ef6\u6d41\u548c\u6a21\u7cca\u56fe\u50cf\u4e2d\u89e3\u7801\u51fa\u6f5c\u5728\u7684\u6e05\u6670\u56fe\u50cf\uff0c\u4e3a\u573a\u666f\u91cd\u5efa\u4f18\u5316\u63d0\u4f9b\u7cbe\u7ec6\u7684\u76d1\u7763\u4fe1\u53f7\u3002", "result": "DeblurSplat \u5728\u751f\u6210\u9ad8\u4fdd\u771f\u65b0\u89c6\u89d2\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u663e\u8457\u63d0\u9ad8\u4e86\u6e32\u67d3\u6548\u7387\u3002", "conclusion": "DeblurSplat \u662f\u4e00\u79cd\u6709\u6548\u7684\u65e0SfM\u8fd0\u52a8\u53bb\u6a21\u7cca\u4e09\u7ef4\u9ad8\u65af\u6cfc\u6e85\u65b9\u6cd5\uff0c\u5728\u751f\u6210\u8d28\u91cf\u548c\u6548\u7387\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002"}}
{"id": "2509.18910", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18910", "abs": "https://arxiv.org/abs/2509.18910", "authors": ["Shuwei Guo", "Simin Luan", "Yan Ke", "Zeyd Boukhers", "John See", "Cong Yang"], "title": "Moir\u00e9Net: A Compact Dual-Domain Network for Image Demoir\u00e9ing", "comment": null, "summary": "Moir\\'e patterns arise from spectral aliasing between display pixel lattices\nand camera sensor grids, manifesting as anisotropic, multi-scale artifacts that\npose significant challenges for digital image demoir\\'eing. We propose\nMoir\\'eNet, a convolutional neural U-Net-based framework that synergistically\nintegrates frequency and spatial domain features for effective artifact\nremoval. Moir\\'eNet introduces two key components: a Directional\nFrequency-Spatial Encoder (DFSE) that discerns moir\\'e orientation via\ndirectional difference convolution, and a Frequency-Spatial Adaptive Selector\n(FSAS) that enables precise, feature-adaptive suppression. Extensive\nexperiments demonstrate that Moir\\'eNet achieves state-of-the-art performance\non public and actively used datasets while being highly parameter-efficient.\nWith only 5.513M parameters, representing a 48% reduction compared to ESDNet-L,\nMoir\\'eNet combines superior restoration quality with parameter efficiency,\nmaking it well-suited for resource-constrained applications including\nsmartphone photography, industrial imaging, and augmented reality.", "AI": {"tldr": "Moir\u00e9Net\u662f\u4e00\u4e2a\u57fa\u4e8eU-Net\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u96c6\u6210\u9891\u7387\u548c\u7a7a\u95f4\u57df\u7279\u5f81\u6765\u53bb\u9664Moir\u00e9\u56fe\u6848\uff0c\u5e76\u5728\u53c2\u6570\u6548\u7387\u65b9\u9762\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "Moir\u00e9\u56fe\u6848\u7531\u663e\u793a\u50cf\u7d20\u70b9\u9635\u548c\u76f8\u673a\u4f20\u611f\u5668\u7f51\u683c\u4e4b\u95f4\u7684\u9891\u8c31\u6df7\u53e0\u5f15\u8d77\uff0c\u8868\u73b0\u4e3a\u5404\u5411\u5f02\u6027\u3001\u591a\u5c3a\u5ea6\u4f2a\u5f71\uff0c\u7ed9\u6570\u5b57\u56fe\u50cf\u53bbMoir\u00e9\u5e26\u6765\u5de8\u5927\u6311\u6218\u3002", "method": "Moir\u00e9Net\u6846\u67b6\u6574\u5408\u4e86\u65b9\u5411\u6027\u9891\u7387-\u7a7a\u95f4\u7f16\u7801\u5668\uff08DFSE\uff09\u548c\u9891\u7387-\u7a7a\u95f4\u81ea\u9002\u5e94\u9009\u62e9\u5668\uff08FSAS\uff09\uff0cDFSE\u901a\u8fc7\u65b9\u5411\u6027\u5dee\u5206\u5377\u79ef\u8bc6\u522bMoir\u00e9\u65b9\u5411\uff0cFSAS\u5b9e\u73b0\u7cbe\u786e\u3001\u7279\u5f81\u81ea\u9002\u5e94\u7684\u6291\u5236\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cMoir\u00e9Net\u5728\u516c\u5f00\u548c\u5b9e\u9645\u4f7f\u7528\u7684\u6570\u636e\u96c6\u4e0a\u90fd\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u53c2\u6570\u6548\u7387\u9ad8\uff0c\u62e5\u67095.513M\u53c2\u6570\uff0c\u6bd4ESDNet-L\u51cf\u5c11\u4e8648%\u3002", "conclusion": "Moir\u00e9Net\u7ed3\u5408\u4e86\u5353\u8d8a\u7684\u6062\u590d\u8d28\u91cf\u548c\u53c2\u6570\u6548\u7387\uff0c\u975e\u5e38\u9002\u5408\u667a\u80fd\u624b\u673a\u6444\u5f71\u3001\u5de5\u4e1a\u6210\u50cf\u548c\u589e\u5f3a\u73b0\u5b9e\u7b49\u8d44\u6e90\u53d7\u9650\u7684\u5e94\u7528\u3002"}}
{"id": "2509.18703", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18703", "abs": "https://arxiv.org/abs/2509.18703", "authors": ["Jakub Adamczyk"], "title": "Towards Rational Pesticide Design with Graph Machine Learning Models for Ecotoxicology", "comment": null, "summary": "This research focuses on rational pesticide design, using graph machine\nlearning to accelerate the development of safer, eco-friendly agrochemicals,\ninspired by in silico methods in drug discovery. With an emphasis on\necotoxicology, the initial contributions include the creation of ApisTox, the\nlargest curated dataset on pesticide toxicity to honey bees. We conducted a\nbroad evaluation of machine learning (ML) models for molecular graph\nclassification, including molecular fingerprints, graph kernels, GNNs, and\npretrained transformers. The results show that methods successful in medicinal\nchemistry often fail to generalize to agrochemicals, underscoring the need for\ndomain-specific models and benchmarks. Future work will focus on developing a\ncomprehensive benchmarking suite and designing ML models tailored to the unique\nchallenges of pesticide discovery.", "AI": {"tldr": "\u5229\u7528\u56fe\u673a\u5668\u5b66\u4e60\u52a0\u901f\u5f00\u53d1\u66f4\u5b89\u5168\u3001\u66f4\u73af\u4fdd\u7684\u519c\u836f\u3002", "motivation": "\u53d7\u5230\u836f\u7269\u53d1\u73b0\u4e2d in silico \u65b9\u6cd5\u7684\u542f\u53d1\uff0c\u672c\u7814\u7a76\u4e13\u6ce8\u4e8e\u5408\u7406\u8bbe\u8ba1\u519c\u836f\uff0c\u5229\u7528\u56fe\u673a\u5668\u5b66\u4e60\u52a0\u901f\u5f00\u53d1\u66f4\u5b89\u5168\u3001\u66f4\u73af\u4fdd\u7684\u519c\u836f\u3002", "method": "\u521b\u5efa\u4e86\u6700\u5927\u7684\u519c\u836f\u5bf9\u871c\u8702\u6bd2\u6027\u6570\u636e\u96c6ApisTox\uff0c\u5e76\u5e7f\u6cdb\u8bc4\u4f30\u4e86\u5206\u5b50\u6307\u7eb9\u3001\u56fe\u6838\u3001GNN\u548c\u9884\u8bad\u7ec3transformer\u7b49\u591a\u79cd\u673a\u5668\u5b66\u4e60\u6a21\u578b\u3002", "result": "\u4e0e\u836f\u7269\u5316\u5b66\u4e2d\u5e38\u7528\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u519c\u7528\u5316\u5b66\u54c1\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u8f83\u5dee\uff0c\u8fd9\u8868\u660e\u9700\u8981\u7279\u5b9a\u9886\u57df\u7684\u6a21\u578b\u548c\u57fa\u51c6\u3002", "conclusion": "\u672a\u6765\u7684\u5de5\u4f5c\u5c06\u96c6\u4e2d\u4e8e\u5f00\u53d1\u4e00\u4e2a\u5168\u9762\u7684\u57fa\u51c6\u5957\u4ef6\uff0c\u5e76\u8bbe\u8ba1\u9488\u5bf9\u519c\u836f\u53d1\u73b0\u72ec\u7279\u6311\u6218\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u3002"}}
{"id": "2509.18912", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18912", "abs": "https://arxiv.org/abs/2509.18912", "authors": ["Yunzhe Shen", "Kai Peng", "Leiye Liu", "Wei Ji", "Jingjing Li", "Miao Zhang", "Yongri Piao", "Huchuan Lu"], "title": "Frequency-Domain Decomposition and Recomposition for Robust Audio-Visual Segmentation", "comment": null, "summary": "Audio-visual segmentation (AVS) plays a critical role in multimodal machine\nlearning by effectively integrating audio and visual cues to precisely segment\nobjects or regions within visual scenes. Recent AVS methods have demonstrated\nsignificant improvements. However, they overlook the inherent frequency-domain\ncontradictions between audio and visual modalities--the pervasively interfering\nnoise in audio high-frequency signals vs. the structurally rich details in\nvisual high-frequency signals. Ignoring these differences can result in\nsuboptimal performance. In this paper, we rethink the AVS task from a deeper\nperspective by reformulating AVS task as a frequency-domain decomposition and\nrecomposition problem. To this end, we introduce a novel Frequency-Aware\nAudio-Visual Segmentation (FAVS) framework consisting of two key modules:\nFrequency-Domain Enhanced Decomposer (FDED) module and Synergistic Cross-Modal\nConsistency (SCMC) module. FDED module employs a residual-based iterative\nfrequency decomposition to discriminate modality-specific semantics and\nstructural features, and SCMC module leverages a mixture-of-experts\narchitecture to reinforce semantic consistency and modality-specific feature\npreservation through dynamic expert routing. Extensive experiments demonstrate\nthat our FAVS framework achieves state-of-the-art performance on three\nbenchmark datasets, and abundant qualitative visualizations further verify the\neffectiveness of the proposed FDED and SCMC modules. The code will be released\nas open source upon acceptance of the paper.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u97f3\u9891-\u89c6\u89c9\u5206\u5272\uff08AVS\uff09\u6846\u67b6FAVS\uff0c\u901a\u8fc7\u5728\u9891\u57df\u4e2d\u8fdb\u884c\u5206\u89e3\u548c\u91cd\u6784\u6765\u89e3\u51b3\u97f3\u9891\u548c\u89c6\u89c9\u6a21\u6001\u4e4b\u95f4\u56fa\u6709\u7684\u9891\u57df\u77db\u76fe\uff0c\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684AVS\u65b9\u6cd5\u5ffd\u7565\u4e86\u97f3\u9891\u9ad8\u9891\u4fe1\u53f7\u4e2d\u7684\u566a\u58f0\u548c\u89c6\u89c9\u9ad8\u9891\u4fe1\u53f7\u4e2d\u7684\u7ed3\u6784\u7ec6\u8282\u4e4b\u95f4\u7684\u56fa\u6709\u9891\u57df\u77db\u76fe\uff0c\u53ef\u80fd\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFAVS\u7684\u65b0\u9896\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u5173\u952e\u6a21\u5757\uff1a\u9891\u57df\u589e\u5f3a\u5206\u89e3\u5668\uff08FDED\uff09\u548c\u534f\u540c\u8de8\u6a21\u6001\u4e00\u81f4\u6027\uff08SCMC\uff09\u6a21\u5757\u3002FDED\u4f7f\u7528\u57fa\u4e8e\u6b8b\u5dee\u7684\u8fed\u4ee3\u9891\u57df\u5206\u89e3\u6765\u533a\u5206\u7279\u5b9a\u4e8e\u6a21\u6001\u7684\u8bed\u4e49\u548c\u7ed3\u6784\u7279\u5f81\uff0cSCMC\u5229\u7528\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\u901a\u8fc7\u52a8\u6001\u4e13\u5bb6\u8def\u7531\u6765\u589e\u5f3a\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u7279\u5b9a\u4e8e\u6a21\u6001\u7684\u7279\u5f81\u4fdd\u6301\u3002", "result": "FAVS\u6846\u67b6\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u5927\u91cf\u7684\u53ef\u89c6\u5316\u7ed3\u679c\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u7684FDED\u548cSCMC\u6a21\u5757\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684FAVS\u6846\u67b6\u901a\u8fc7\u5728\u9891\u57df\u4e2d\u8fdb\u884c\u5206\u89e3\u548c\u91cd\u6784\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u97f3\u9891-\u89c6\u89c9\u5206\u5272\u4e2d\u7684\u6a21\u6001\u77db\u76fe\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2509.18714", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18714", "abs": "https://arxiv.org/abs/2509.18714", "authors": ["Zhenyu Tao", "Wei Xu", "Xiaohu You"], "title": "A Generalized Bisimulation Metric of State Similarity between Markov Decision Processes: From Theoretical Propositions to Applications", "comment": "This paper is accepted by the 39th Conference on Neural Information\n  Processing Systems (NeurIPS 2025)", "summary": "The bisimulation metric (BSM) is a powerful tool for computing state\nsimilarities within a Markov decision process (MDP), revealing that states\ncloser in BSM have more similar optimal value functions. While BSM has been\nsuccessfully utilized in reinforcement learning (RL) for tasks like state\nrepresentation learning and policy exploration, its application to multiple-MDP\nscenarios, such as policy transfer, remains challenging. Prior work has\nattempted to generalize BSM to pairs of MDPs, but a lack of rigorous analysis\nof its mathematical properties has limited further theoretical progress. In\nthis work, we formally establish a generalized bisimulation metric (GBSM)\nbetween pairs of MDPs, which is rigorously proven with the three fundamental\nproperties: GBSM symmetry, inter-MDP triangle inequality, and the distance\nbound on identical state spaces. Leveraging these properties, we theoretically\nanalyse policy transfer, state aggregation, and sampling-based estimation in\nMDPs, obtaining explicit bounds that are strictly tighter than those derived\nfrom the standard BSM. Additionally, GBSM provides a closed-form sample\ncomplexity for estimation, improving upon existing asymptotic results based on\nBSM. Numerical results validate our theoretical findings and demonstrate the\neffectiveness of GBSM in multi-MDP scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u901a\u7528\u7684\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08MDP\uff09\u4e4b\u95f4\u7684\u5ea6\u91cf\uff08GBSM\uff09\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u4e09\u4e2a\u57fa\u672c\u6027\u8d28\uff1a\u5bf9\u79f0\u6027\u3001\u4e09\u89d2\u4e0d\u7b49\u5f0f\u548c\u76f8\u540c\u72b6\u6001\u7a7a\u95f4\u4e0a\u7684\u8ddd\u79bb\u754c\u9650\u3002\u8be5\u5ea6\u91cf\u5728\u7b56\u7565\u8fc1\u79fb\u3001\u72b6\u6001\u805a\u5408\u548c\u57fa\u4e8e\u91c7\u6837\u4f30\u8ba1\u7b49\u65b9\u9762\u63d0\u4f9b\u4e86\u4e25\u683c\u7684\u7406\u8bba\u5206\u6790\u548c\u6539\u8fdb\u7684\u754c\u9650\uff0c\u5e76\u5728\u6570\u503c\u5b9e\u9a8c\u4e2d\u5f97\u5230\u4e86\u9a8c\u8bc1\u3002", "motivation": "\u5728\u591aMDP\u573a\u666f\uff08\u5982\u7b56\u7565\u8fc1\u79fb\uff09\u4e2d\uff0c\u73b0\u6709\u7684\u53cc\u6a21\u62df\u5ea6\u91cf\uff08BSM\uff09\u5e94\u7528\u5b58\u5728\u6311\u6218\uff0c\u4e14\u7f3a\u4e4f\u4e25\u683c\u7684\u6570\u5b66\u6027\u8d28\u5206\u6790\uff0c\u963b\u788d\u4e86\u7406\u8bba\u8fdb\u5c55\u3002", "method": "\u5f62\u5f0f\u5316\u5b9a\u4e49\u4e86MDP\u5bf9\u4e4b\u95f4\u7684\u5e7f\u4e49\u53cc\u6a21\u62df\u5ea6\u91cf\uff08GBSM\uff09\uff0c\u5e76\u4e25\u683c\u8bc1\u660e\u4e86\u5176\u4e09\u4e2a\u57fa\u672c\u6027\u8d28\uff1a\u5bf9\u79f0\u6027\u3001\u4e09\u89d2\u4e0d\u7b49\u5f0f\u548c\u76f8\u540c\u72b6\u6001\u7a7a\u95f4\u4e0a\u7684\u8ddd\u79bb\u754c\u9650\u3002\u57fa\u4e8e\u8fd9\u4e9b\u6027\u8d28\uff0c\u5bf9\u7b56\u7565\u8fc1\u79fb\u3001\u72b6\u6001\u805a\u5408\u548c\u91c7\u6837\u4f30\u8ba1\u8fdb\u884c\u4e86\u7406\u8bba\u5206\u6790\u3002", "result": "GBSM\u5728\u7b56\u7565\u8fc1\u79fb\u3001\u72b6\u6001\u805a\u5408\u548c\u91c7\u6837\u4f30\u8ba1\u7b49\u65b9\u9762\u63d0\u4f9b\u4e86\u7406\u8bba\u5206\u6790\u548c\u663e\u5f0f\u754c\u9650\uff0c\u8fd9\u4e9b\u754c\u9650\u6bd4\u6807\u51c6BSM\u66f4\u4f18\u3002GBSM\u8fd8\u63d0\u4f9b\u4e86\u4f30\u8ba1\u7684\u5c01\u95ed\u5f0f\u6837\u672c\u590d\u6742\u5ea6\uff0c\u4f18\u4e8e\u57fa\u4e8eBSM\u7684\u6e10\u8fd1\u7ed3\u679c\u3002\u6570\u503c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u7406\u8bba\u53d1\u73b0\u3002", "conclusion": "GBSM\u4e3a\u591aMDP\u573a\u666f\u4e0b\u7684\u7406\u8bba\u5206\u6790\u63d0\u4f9b\u4e86\u575a\u5b9e\u7684\u57fa\u7840\uff0c\u5e76\u5728\u7b56\u7565\u8fc1\u79fb\u3001\u72b6\u6001\u805a\u5408\u548c\u91c7\u6837\u4f30\u8ba1\u7b49\u65b9\u9762\u53d6\u5f97\u4e86\u7406\u8bba\u548c\u5b9e\u8df5\u4e0a\u7684\u6539\u8fdb\u3002"}}
{"id": "2509.18913", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18913", "abs": "https://arxiv.org/abs/2509.18913", "authors": ["Nguyen Van Tu", "Pham Nguyen Hai Long", "Vo Hoai Viet"], "title": "xAI-CV: An Overview of Explainable Artificial Intelligence in Computer Vision", "comment": null, "summary": "Deep learning has become the de facto standard and dominant paradigm in image\nanalysis tasks, achieving state-of-the-art performance. However, this approach\noften results in \"black-box\" models, whose decision-making processes are\ndifficult to interpret, raising concerns about reliability in critical\napplications. To address this challenge and provide human a method to\nunderstand how AI model process and make decision, the field of xAI has\nemerged. This paper surveys four representative approaches in xAI for visual\nperception tasks: (i) Saliency Maps, (ii) Concept Bottleneck Models (CBM),\n(iii) Prototype-based methods, and (iv) Hybrid approaches. We analyze their\nunderlying mechanisms, strengths and limitations, as well as evaluation\nmetrics, thereby providing a comprehensive overview to guide future research\nand applications.", "AI": {"tldr": "\u6df1\u5ea6\u5b66\u4e60\u5728\u56fe\u50cf\u5206\u6790\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5e38\u88ab\u89c6\u4e3a\u201c\u9ed1\u7bb1\u201d\u6a21\u578b\uff0c\u5f15\u53d1\u53ef\u9760\u6027\u62c5\u5fe7\u3002\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\uff08xAI\uff09\u5e94\u8fd0\u800c\u751f\uff0c\u65e8\u5728\u63ed\u793aAI\u7684\u51b3\u7b56\u8fc7\u7a0b\u3002\u672c\u6587\u7efc\u8ff0\u4e86xAI\u5728\u89c6\u89c9\u611f\u77e5\u4efb\u52a1\u4e2d\u7684\u56db\u79cd\u4ee3\u8868\u6027\u65b9\u6cd5\uff1a\u663e\u8457\u56fe\u3001\u6982\u5ff5\u74f6\u9888\u6a21\u578b\uff08CBM\uff09\u3001\u539f\u578b\u65b9\u6cd5\u548c\u6df7\u5408\u65b9\u6cd5\u3002\u6587\u7ae0\u5206\u6790\u4e86\u5b83\u4eec\u7684\u4f5c\u7528\u673a\u5236\u3001\u4f18\u7f3a\u70b9\u53ca\u8bc4\u4f30\u6307\u6807\uff0c\u4e3a\u672a\u6765\u7684\u7814\u7a76\u548c\u5e94\u7528\u63d0\u4f9b\u6307\u5bfc\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u56fe\u50cf\u5206\u6790\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u201c\u9ed1\u7bb1\u201d\u6027\u8d28\u5f15\u53d1\u4e86\u5bf9\u53ef\u9760\u6027\u7684\u62c5\u5fe7\uff0c\u5c24\u5176\u662f\u5728\u5173\u952e\u5e94\u7528\u9886\u57df\u3002\u4e3a\u4e86\u8ba9\u4eba\u4eec\u80fd\u591f\u7406\u89e3AI\u6a21\u578b\u7684\u5904\u7406\u548c\u51b3\u7b56\u8fc7\u7a0b\uff0c\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\uff08xAI\uff09\u9886\u57df\u5e94\u8fd0\u800c\u751f\u3002", "method": "\u672c\u6587\u7efc\u8ff0\u4e86xAI\u5728\u89c6\u89c9\u611f\u77e5\u4efb\u52a1\u4e2d\u7684\u56db\u79cd\u4ee3\u8868\u6027\u65b9\u6cd5\uff0c\u5305\u62ec\uff1a(i) \u663e\u8457\u56fe\uff0c(ii) \u6982\u5ff5\u74f6\u9888\u6a21\u578b\uff08CBM\uff09\uff0c(iii) \u57fa\u4e8e\u539f\u578b\u7684\u6a21\u578b\uff0c\u4ee5\u53ca(iv) \u6df7\u5408\u65b9\u6cd5\u3002\u6587\u7ae0\u5c06\u5206\u6790\u8fd9\u4e9b\u65b9\u6cd5\u7684\u5185\u5728\u673a\u5236\u3001\u4f18\u70b9\u548c\u5c40\u9650\u6027\uff0c\u4ee5\u53ca\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u672c\u6587\u5bf9xAI\u5728\u89c6\u89c9\u611f\u77e5\u4efb\u52a1\u4e2d\u7684\u56db\u79cd\u4ee3\u8868\u6027\u65b9\u6cd5\u8fdb\u884c\u4e86\u7efc\u8ff0\uff0c\u5206\u6790\u4e86\u5b83\u4eec\u7684\u4f5c\u7528\u673a\u5236\u3001\u4f18\u7f3a\u70b9\u548c\u8bc4\u4f30\u6307\u6807\u3002", "conclusion": "\u672c\u6587\u65e8\u5728\u4e3axAI\u5728\u89c6\u89c9\u611f\u77e5\u9886\u57df\u7684\u672a\u6765\u7814\u7a76\u548c\u5e94\u7528\u63d0\u4f9b\u4e00\u4e2a\u5168\u9762\u7684\u6982\u8ff0\u548c\u6307\u5bfc\u3002"}}
{"id": "2509.18719", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18719", "abs": "https://arxiv.org/abs/2509.18719", "authors": ["Bo Qu", "Zhurong Wang", "Daisuke Yagi", "Zhen Xu", "Yang Zhao", "Yinan Shan", "Frank Zahradnik"], "title": "LLM-Enhanced Self-Evolving Reinforcement Learning for Multi-Step E-Commerce Payment Fraud Risk Detection", "comment": "12 pages, 12 figures, ACL 2025 industry track", "summary": "This paper presents a novel approach to e-commerce payment fraud detection by\nintegrating reinforcement learning (RL) with Large Language Models (LLMs). By\nframing transaction risk as a multi-step Markov Decision Process (MDP), RL\noptimizes risk detection across multiple payment stages. Crafting effective\nreward functions, essential for RL model success, typically requires\nsignificant human expertise due to the complexity and variability in design.\nLLMs, with their advanced reasoning and coding capabilities, are well-suited to\nrefine these functions, offering improvements over traditional methods. Our\napproach leverages LLMs to iteratively enhance reward functions, achieving\nbetter fraud detection accuracy and demonstrating zero-shot capability.\nExperiments with real-world data confirm the effectiveness, robustness, and\nresilience of our LLM-enhanced RL framework through long-term evaluations,\nunderscoring the potential of LLMs in advancing industrial RL applications.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u65b0\u578b\u7535\u5b50\u5546\u52a1\u652f\u4ed8\u6b3a\u8bc8\u68c0\u6d4b\u65b9\u6cd5\u3002\u901a\u8fc7\u5c06\u4ea4\u6613\u98ce\u9669\u5efa\u6a21\u4e3a\u591a\u6b65\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08MDP\uff09\uff0cRL\u80fd\u591f\u4f18\u5316\u8de8\u591a\u4e2a\u652f\u4ed8\u9636\u6bb5\u7684\u98ce\u9669\u68c0\u6d4b\u3002\u7531\u4e8e\u5956\u52b1\u51fd\u6570\u8bbe\u8ba1\u7684\u590d\u6742\u6027\u548c\u591a\u53d8\u6027\uff0c\u901a\u5e38\u9700\u8981\u5927\u91cf\u7684\u4eba\u7c7b\u4e13\u4e1a\u77e5\u8bc6\u6765\u6784\u5efa\u6709\u6548\u7684\u5956\u52b1\u51fd\u6570\u3002LLM\u51ed\u501f\u5176\u5f3a\u5927\u7684\u63a8\u7406\u548c\u7f16\u7801\u80fd\u529b\uff0c\u80fd\u591f\u4f18\u5316\u8fd9\u4e9b\u5956\u52b1\u51fd\u6570\uff0c\u4ece\u800c\u5728\u6b3a\u8bc8\u68c0\u6d4b\u65b9\u9762\u53d6\u5f97\u6bd4\u4f20\u7edf\u65b9\u6cd5\u66f4\u597d\u7684\u6548\u679c\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u6b3a\u8bc8\u68c0\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u8fd8\u5c55\u73b0\u4e86\u96f6\u6837\u672c\u5b66\u4e60\u80fd\u529b\u3002\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4e0a\u7684\u957f\u671f\u8bc4\u4f30\u8bc1\u5b9e\u4e86\u8be5LLM\u589e\u5f3aRL\u6846\u67b6\u7684\u6709\u6548\u6027\u3001\u9c81\u68d2\u6027\u548c\u97e7\u6027\uff0c\u8868\u660eLLM\u5728\u5de5\u4e1aRL\u5e94\u7528\u65b9\u9762\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002", "motivation": "\u7535\u5b50\u5546\u52a1\u652f\u4ed8\u6b3a\u8bc8\u68c0\u6d4b\u4e2d\u7684\u5956\u52b1\u51fd\u6570\u8bbe\u8ba1\u590d\u6742\u4e14\u9700\u8981\u5927\u91cf\u4e13\u4e1a\u77e5\u8bc6\uff0c\u800c\u4f20\u7edf\u7684\u5956\u52b1\u51fd\u6570\u8bbe\u8ba1\u65b9\u6cd5\u5728\u6548\u679c\u4e0a\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u5c06\u4ea4\u6613\u98ce\u9669\u5efa\u6a21\u4e3a\u591a\u6b65\u9a6c\u5c14\u53ef\u5728\u4ea4\u6613\u98ce\u9669\u5efa\u6a21\u4e3a\u591a\u6b65\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08MDP\uff09\uff0c\u5e76\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6765\u4f18\u5316\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u6a21\u578b\u7684\u5956\u52b1\u51fd\u6570\uff0c\u4ee5\u63d0\u9ad8\u6b3a\u8bc8\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "result": "\u901a\u8fc7LLM\u4f18\u5316\u5956\u52b1\u51fd\u6570\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4e0a\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u6b3a\u8bc8\u68c0\u6d4b\u51c6\u786e\u6027\uff0c\u5e76\u5c55\u73b0\u4e86\u96f6\u6837\u672c\u5b66\u4e60\u80fd\u529b\u3002\u957f\u671f\u8bc4\u4f30\u8bc1\u660e\u4e86\u8be5\u6846\u67b6\u7684\u6709\u6548\u6027\u3001\u9c81\u68d2\u6027\u548c\u97e7\u6027\u3002", "conclusion": "\u7ed3\u5408LLM\u548cRL\u7684\u652f\u4ed8\u6b3a\u8bc8\u68c0\u6d4b\u6846\u67b6\u80fd\u591f\u6709\u6548\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\uff0cLLM\u5728\u4f18\u5316\u5de5\u4e1aRL\u5e94\u7528\u65b9\u9762\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2509.18917", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18917", "abs": "https://arxiv.org/abs/2509.18917", "authors": ["Amirhesam Aghanouri", "Cristina Olaverri-Monreal"], "title": "LiDAR Point Cloud Image-based Generation Using Denoising Diffusion Probabilistic Models", "comment": null, "summary": "Autonomous vehicles (AVs) are expected to revolutionize transportation by\nimproving efficiency and safety. Their success relies on 3D vision systems that\neffectively sense the environment and detect traffic agents. Among sensors AVs\nuse to create a comprehensive view of surroundings, LiDAR provides\nhigh-resolution depth data enabling accurate object detection, safe navigation,\nand collision avoidance. However, collecting real-world LiDAR data is\ntime-consuming and often affected by noise and sparsity due to adverse weather\nor sensor limitations. This work applies a denoising diffusion probabilistic\nmodel (DDPM), enhanced with novel noise scheduling and time-step embedding\ntechniques to generate high-quality synthetic data for augmentation, thereby\nimproving performance across a range of computer vision tasks, particularly in\nAV perception. These modifications impact the denoising process and the model's\ntemporal awareness, allowing it to produce more realistic point clouds based on\nthe projection. The proposed method was extensively evaluated under various\nconfigurations using the IAMCV and KITTI-360 datasets, with four performance\nmetrics compared against state-of-the-art (SOTA) methods. The results\ndemonstrate the model's superior performance over most existing baselines and\nits effectiveness in mitigating the effects of noisy and sparse LiDAR data,\nproducing diverse point clouds with rich spatial relationships and structural\ndetail.", "AI": {"tldr": "DDPM\u88ab\u7528\u4e8e\u751f\u6210\u5408\u6210\u7684\u6fc0\u5149\u96f7\u8fbe\u70b9\u4e91\u6570\u636e\uff0c\u4ee5\u589e\u5f3a\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u7684\u611f\u77e5\u80fd\u529b\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u7684\u6210\u529f\u4f9d\u8d56\u4e8e\u6709\u6548\u76843D\u89c6\u89c9\u7cfb\u7edf\uff0c\u4f46\u771f\u5b9e\u7684\u6fc0\u5149\u96f7\u8fbe\u6570\u636e\u6536\u96c6\u8017\u65f6\u4e14\u5b58\u5728\u566a\u58f0\u548c\u7a00\u758f\u6027\u95ee\u9898\u3002\u56e0\u6b64\uff0c\u9700\u8981\u5408\u6210\u6570\u636e\u6765\u589e\u5f3a\u6a21\u578b\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684DDPM\u6a21\u578b\uff0c\u91c7\u7528\u65b0\u7684\u566a\u58f0\u8c03\u5ea6\u548c\u65f6\u95f4\u6b65\u5d4c\u5165\u6280\u672f\u6765\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u5408\u6210\u6fc0\u5149\u96f7\u8fbe\u70b9\u4e91\u6570\u636e\u3002", "result": "\u5728IAMCV\u548cKITTI-360\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u8bc4\u4f30\uff0c\u7ed3\u679c\u8868\u660e\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u70b9\u4e91\u751f\u6210\u8d28\u91cf\u548c\u4e0b\u6e38\u611f\u77e5\u4efb\u52a1\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u5e76\u80fd\u6709\u6548\u7f13\u89e3\u566a\u58f0\u548c\u7a00\u758f\u6570\u636e\u7684\u5f71\u54cd\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6539\u8fdbDDPM\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u591a\u6837\u5316\u7684\u5408\u6210\u6fc0\u5149\u96f7\u8fbe\u70b9\u4e91\u6570\u636e\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u7684\u611f\u77e5\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u566a\u58f0\u548c\u7a00\u758f\u6570\u636e\u65f6\u3002"}}
{"id": "2509.18744", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18744", "abs": "https://arxiv.org/abs/2509.18744", "authors": ["Yuqing Liu"], "title": "Theory of periodic convolutional neural network", "comment": null, "summary": "We introduce a novel convolutional neural network architecture, termed the\n\\emph{periodic CNN}, which incorporates periodic boundary conditions into the\nconvolutional layers. Our main theoretical contribution is a rigorous\napproximation theorem: periodic CNNs can approximate ridge functions depending\non $d-1$ linear variables in a $d$-dimensional input space, while such\napproximation is impossible in lower-dimensional ridge settings ($d-2$ or fewer\nvariables). This result establishes a sharp characterization of the expressive\npower of periodic CNNs. Beyond the theory, our findings suggest that periodic\nCNNs are particularly well-suited for problems where data naturally admits a\nridge-like structure of high intrinsic dimension, such as image analysis on\nwrapped domains, physics-informed learning, and materials science. The work\nthus both expands the mathematical foundation of CNN approximation theory and\nhighlights a class of architectures with surprising and practically relevant\napproximation capabilities.", "AI": {"tldr": "\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7ed3\u6784\uff0c\u79f0\u4e3a\u5468\u671f\u6027CNN\uff0c\u5b83\u5c06\u5468\u671f\u6027\u8fb9\u754c\u6761\u4ef6\u878d\u5165\u5377\u79ef\u5c42\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u903c\u8fd1d-1\u7ef4\u7ebf\u6027\u53d8\u91cf\u7684\u810a\u51fd\u6570\u7684\u7406\u8bba\u80fd\u529b\uff0c\u8fd9\u5728\u4f4e\u7ef4\u810a\u51fd\u6570\u8bbe\u7f6e\u4e2d\u662f\u4e0d\u53ef\u80fd\u7684\u3002", "motivation": "\u672c\u6587\u7684\u52a8\u673a\u662f\u63a2\u7d22\u548c\u8868\u5f81\u5468\u671f\u6027CNN\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u7279\u522b\u662f\u5176\u5728\u5904\u7406\u5177\u6709\u9ad8\u5185\u5728\u7ef4\u5ea6\u810a\u72b6\u7ed3\u6784\u7684\u6570\u636e\u65b9\u9762\u7684\u6f5c\u529b\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u79f0\u4e3a\u5468\u671f\u6027CNN\uff0c\u5e76\u5c06\u5468\u671f\u6027\u8fb9\u754c\u6761\u4ef6\u6574\u5408\u5230\u5377\u79ef\u5c42\u4e2d\u3002\u7406\u8bba\u8d21\u732e\u662f\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4e25\u683c\u7684\u903c\u8fd1\u5b9a\u7406\uff0c\u8bc1\u660e\u4e86\u5468\u671f\u6027CNN\u5728\u903c\u8fd1d-1\u7ef4\u7ebf\u6027\u53d8\u91cf\u7684\u810a\u51fd\u6570\u65b9\u9762\u7684\u80fd\u529b\u3002", "result": "\u8bc1\u660e\u4e86\u5468\u671f\u6027CNN\u53ef\u4ee5\u903c\u8fd1d-1\u7ef4\u7ebf\u6027\u53d8\u91cf\u7684\u810a\u51fd\u6570\uff0c\u800c\u5728\u7ef4\u5ea6\u8f83\u4f4e\u7684\u60c5\u51b5\u4e0b\u5219\u65e0\u6cd5\u505a\u5230\uff0c\u4ece\u800c\u4e3a\u5468\u671f\u6027CNN\u7684\u8868\u8fbe\u80fd\u529b\u63d0\u4f9b\u4e86\u7cbe\u786e\u7684\u523b\u753b\u3002", "conclusion": "\u5468\u671f\u6027CNN\u5728\u5904\u7406\u5177\u6709\u810a\u72b6\u7ed3\u6784\u7684\u6570\u636e\u65b9\u9762\u5177\u6709\u4f18\u52bf\uff0c\u9002\u7528\u4e8e\u56fe\u50cf\u5206\u6790\u3001\u7269\u7406\u4fe1\u606f\u5b66\u4e60\u548c\u6750\u6599\u79d1\u5b66\u7b49\u9886\u57df\uff0c\u6269\u5c55\u4e86CNN\u903c\u8fd1\u7406\u8bba\u7684\u6570\u5b66\u57fa\u7840\uff0c\u5e76\u5c55\u793a\u4e86\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u7684\u5468\u671f\u6027CNN\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.18919", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18919", "abs": "https://arxiv.org/abs/2509.18919", "authors": ["Chuni Liu", "Hongjie Li", "Jiaqi Du", "Yangyang Hou", "Qian Sun", "Lei Jin", "Ke Xu"], "title": "Advancing Metallic Surface Defect Detection via Anomaly-Guided Pretraining on a Large Industrial Dataset", "comment": null, "summary": "The pretraining-finetuning paradigm is a crucial strategy in metallic surface\ndefect detection for mitigating the challenges posed by data scarcity. However,\nits implementation presents a critical dilemma. Pretraining on natural image\ndatasets such as ImageNet, faces a significant domain gap. Meanwhile, naive\nself-supervised pretraining on in-domain industrial data is often ineffective\ndue to the inability of existing learning objectives to distinguish subtle\ndefect patterns from complex background noise and textures. To resolve this, we\nintroduce Anomaly-Guided Self-Supervised Pretraining (AGSSP), a novel paradigm\nthat explicitly guides representation learning through anomaly priors. AGSSP\nemploys a two-stage framework: (1) it first pretrains the model's backbone by\ndistilling knowledge from anomaly maps, encouraging the network to capture\ndefect-salient features; (2) it then pretrains the detector using pseudo-defect\nboxes derived from these maps, aligning it with localization tasks. To enable\nthis, we develop a knowledge-enhanced method to generate high-quality anomaly\nmaps and collect a large-scale industrial dataset of 120,000 images.\nAdditionally, we present two small-scale, pixel-level labeled metallic surface\ndefect datasets for validation. Extensive experiments demonstrate that AGSSP\nconsistently enhances performance across various settings, achieving up to a\n10\\% improvement in mAP@0.5 and 11.4\\% in mAP@0.5:0.95 compared to\nImageNet-based models. All code, pretrained models, and datasets are publicly\navailable at https://clovermini.github.io/AGSSP-Dev/.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.18751", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18751", "abs": "https://arxiv.org/abs/2509.18751", "authors": ["Samuel Yoon", "Jongwon Kim", "Juyoung Ha", "Young Myoung Ko"], "title": "MOMEMTO: Patch-based Memory Gate Model in Time Series Foundation Model", "comment": null, "summary": "Recently reconstruction-based deep models have been widely used for time\nseries anomaly detection, but as their capacity and representation capability\nincrease, these models tend to over-generalize, often reconstructing unseen\nanomalies accurately. Prior works have attempted to mitigate this by\nincorporating a memory architecture that stores prototypes of normal patterns.\nNevertheless, these approaches suffer from high training costs and have yet to\nbe effectively integrated with time series foundation models (TFMs). To address\nthese challenges, we propose \\textbf{MOMEMTO}, a TFM for anomaly detection,\nenhanced with a patch-based memory module to mitigate over-generalization. The\nmemory module is designed to capture representative normal patterns from\nmultiple domains and enables a single model to be jointly fine-tuned across\nmultiple datasets through a multi-domain training strategy. MOMEMTO initializes\nmemory items with latent representations from a pre-trained encoder, organizes\nthem into patch-level units, and updates them via an attention mechanism. We\nevaluate our method using 23 univariate benchmark datasets. Experimental\nresults demonstrate that MOMEMTO, as a single model, achieves higher scores on\nAUC and VUS metrics compared to baseline methods, and further enhances the\nperformance of its backbone TFM, particularly in few-shot learning scenarios.", "AI": {"tldr": "MOMEMTO\u662f\u4e00\u79cd\u7528\u4e8e\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u7684\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\uff08TFMs\uff09\uff0c\u5b83\u901a\u8fc7\u5f15\u5165\u57fa\u4e8e\u5757\u7684\u5185\u5b58\u6a21\u5757\u6765\u89e3\u51b3\u73b0\u6709\u6a21\u578b\u8fc7\u5ea6\u6cdb\u5316\u7684\u95ee\u9898\uff0c\u80fd\u591f\u5728\u4e00\u4e2a\u6a21\u578b\u4e2d\u8de8\u591a\u4e2a\u6570\u636e\u96c6\u8fdb\u884c\u8054\u5408\u5fae\u8c03\uff0c\u5e76\u572823\u4e2a\u6570\u636e\u96c6\u7684\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e86\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u91cd\u6784\u7684\u6df1\u5ea6\u6a21\u578b\u5728\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u4e2d\u5bb9\u6613\u8fc7\u5ea6\u6cdb\u5316\uff0c\u51c6\u786e\u5730\u91cd\u6784\u672a\u89c1\u8fc7\u7684\u5f02\u5e38\u3002\u867d\u7136\u5185\u5b58\u67b6\u6784\u53ef\u4ee5\u5b58\u50a8\u6b63\u5e38\u6a21\u5f0f\u7684\u539f\u578b\uff0c\u4f46\u8bad\u7ec3\u6210\u672c\u9ad8\uff0c\u4e14\u672a\u80fd\u4e0e\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\uff08TFMs\uff09\u6709\u6548\u7ed3\u5408\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMOMEMTO\u7684\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\uff08TFM\uff09\uff0c\u5e76\u589e\u5f3a\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5757\u7684\u5185\u5b58\u6a21\u5757\u6765\u7f13\u89e3\u8fc7\u5ea6\u6cdb\u5316\u3002\u8be5\u5185\u5b58\u6a21\u5757\u80fd\u591f\u6355\u83b7\u6765\u81ea\u591a\u4e2a\u57df\u7684\u4ee3\u8868\u6027\u6b63\u5e38\u6a21\u5f0f\uff0c\u5e76\u901a\u8fc7\u591a\u57df\u8bad\u7ec3\u7b56\u7565\u5141\u8bb8\u5355\u4e2a\u6a21\u578b\u8de8\u591a\u4e2a\u6570\u636e\u96c6\u8fdb\u884c\u8054\u5408\u5fae\u8c03\u3002MOMEMTO\u4f7f\u7528\u9884\u8bad\u7ec3\u7f16\u7801\u5668\u7684\u6f5c\u5728\u8868\u793a\u6765\u521d\u59cb\u5316\u5185\u5b58\u9879\uff0c\u5c06\u5b83\u4eec\u7ec4\u7ec7\u6210\u5757\u7ea7\u5355\u5143\uff0c\u5e76\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u8fdb\u884c\u66f4\u65b0\u3002", "result": "\u572823\u4e2a\u5355\u53d8\u91cf\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMOMEMTO\u4f5c\u4e3a\u5355\u4e2a\u6a21\u578b\uff0c\u5728AUC\u548cVUS\u6307\u6807\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u80fd\u8fdb\u4e00\u6b65\u63d0\u5347\u5176\u9aa8\u5e72TFM\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u5c11\u6837\u672c\u5b66\u4e60\u573a\u666f\u4e0b\u3002", "conclusion": "MOMEMTO\u901a\u8fc7\u5176\u521b\u65b0\u7684\u57fa\u4e8e\u5757\u7684\u5185\u5b58\u6a21\u5757\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u8fc7\u5ea6\u6cdb\u5316\u95ee\u9898\uff0c\u5e76\u5728\u591a\u57df\u5b66\u4e60\u548c\u5c11\u6837\u672c\u5b66\u4e60\u573a\u666f\u4e0b\u5c55\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2509.18924", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18924", "abs": "https://arxiv.org/abs/2509.18924", "authors": ["Kartik Teotia", "Helge Rhodin", "Mohit Mendiratta", "Hyeongwoo Kim", "Marc Habermann", "Christian Theobalt"], "title": "Audio-Driven Universal Gaussian Head Avatars", "comment": "(SIGGRAPH Asia 2025) Project page:\n  https://kartik-teotia.github.io/UniGAHA/", "summary": "We introduce the first method for audio-driven universal photorealistic\navatar synthesis, combining a person-agnostic speech model with our novel\nUniversal Head Avatar Prior (UHAP). UHAP is trained on cross-identity\nmulti-view videos. In particular, our UHAP is supervised with neutral scan\ndata, enabling it to capture the identity-specific details at high fidelity. In\ncontrast to previous approaches, which predominantly map audio features to\ngeometric deformations only while ignoring audio-dependent appearance\nvariations, our universal speech model directly maps raw audio inputs into the\nUHAP latent expression space. This expression space inherently encodes, both,\ngeometric and appearance variations. For efficient personalization to new\nsubjects, we employ a monocular encoder, which enables lightweight regression\nof dynamic expression variations across video frames. By accounting for these\nexpression-dependent changes, it enables the subsequent model fine-tuning stage\nto focus exclusively on capturing the subject's global appearance and geometry.\nDecoding these audio-driven expression codes via UHAP generates highly\nrealistic avatars with precise lip synchronization and nuanced expressive\ndetails, such as eyebrow movement, gaze shifts, and realistic mouth interior\nappearance as well as motion. Extensive evaluations demonstrate that our method\nis not only the first generalizable audio-driven avatar model that can account\nfor detailed appearance modeling and rendering, but it also outperforms\ncompeting (geometry-only) methods across metrics measuring lip-sync accuracy,\nquantitative image quality, and perceptual realism.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u9996\u4e2a\u80fd\u591f\u9a71\u52a8\u771f\u5b9e\u611f\u4eba\u50cf\u7684\u901a\u7528\u9762\u90e8\u6a21\u578b\uff0c\u80fd\u591f\u751f\u6210\u5305\u542b\u7cbe\u7ec6\u8868\u60c5\u548c\u52a8\u4f5c\u7684\u903c\u771f\u5934\u50cf\uff0c\u5e76\u4e14\u5728\u5507\u8bed\u540c\u6b65\u3001\u56fe\u50cf\u8d28\u91cf\u548c\u611f\u77e5\u771f\u5b9e\u5ea6\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4ee5\u5f80\u7684\u9762\u90e8\u751f\u6210\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u51e0\u4f55\u53d8\u5f62\uff0c\u5ffd\u7565\u4e86\u4e0e\u8bed\u97f3\u76f8\u5173\u7684\u5916\u89c2\u53d8\u5316\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u80fd\u591f\u540c\u65f6\u5904\u7406\u51e0\u4f55\u548c\u5916\u89c2\u53d8\u5316\u7684\u97f3\u9891\u9a71\u52a8\u4eba\u50cf\u5408\u6210\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u901a\u7528\u9762\u90e8\u6a21\u578b\uff08UHAP\uff09\uff0c\u8be5\u6a21\u578b\u80fd\u591f\u5c06\u8bed\u97f3\u7279\u5f81\u6620\u5c04\u5230\u5305\u542b\u51e0\u4f55\u548c\u5916\u89c2\u53d8\u5316\u7684\u6f5c\u5728\u7a7a\u95f4\u3002\u8be5\u6a21\u578b\u4f7f\u7528\u591a\u89c6\u89d2\u89c6\u9891\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u7ed3\u5408\u4e86\u4e2d\u6027\u626b\u63cf\u6570\u636e\u4ee5\u6355\u6349\u8eab\u4efd\u7ec6\u8282\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u91c7\u7528\u4e86\u4e00\u79cd\u5355\u76ee\u7f16\u7801\u5668\u6765\u5b9e\u73b0\u8f7b\u91cf\u7ea7\u7684\u4e2a\u6027\u5316\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u4e13\u6ce8\u4e8e\u6355\u6349\u76ee\u6807\u5bf9\u8c61\u7684\u5168\u5c40\u5916\u89c2\u548c\u51e0\u4f55\u7279\u5f81\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u5177\u6709\u7cbe\u786e\u5507\u8bed\u540c\u6b65\u3001\u7ec6\u5fae\u8868\u60c5\uff08\u5982\u7709\u6bdb\u8fd0\u52a8\u3001\u773c\u795e\u8f6c\u79fb\uff09\u4ee5\u53ca\u903c\u771f\u53e3\u90e8\u5185\u90e8\u5916\u89c2\u548c\u8fd0\u52a8\u7684\u903c\u771f\u5934\u50cf\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u65b9\u6cd5\u662f\u9996\u4e2a\u80fd\u591f\u5b9e\u73b0\u8be6\u7ec6\u5916\u89c2\u5efa\u6a21\u548c\u6e32\u67d3\u7684\u901a\u7528\u97f3\u9891\u9a71\u52a8\u5934\u50cf\u6a21\u578b\uff0c\u5e76\u5728\u5507\u8bed\u540c\u6b65\u3001\u56fe\u50cf\u8d28\u91cf\u548c\u611f\u77e5\u771f\u5b9e\u5ea6\u7b49\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2509.18766", "categories": ["cs.LG", "math.OC", "stat.ML", "62J07, 68T07", "G.3"], "pdf": "https://arxiv.org/pdf/2509.18766", "abs": "https://arxiv.org/abs/2509.18766", "authors": ["Rapha\u00ebl Berthier"], "title": "Diagonal Linear Networks and the Lasso Regularization Path", "comment": "29 pages, 1 figure", "summary": "Diagonal linear networks are neural networks with linear activation and\ndiagonal weight matrices. Their theoretical interest is that their implicit\nregularization can be rigorously analyzed: from a small initialization, the\ntraining of diagonal linear networks converges to the linear predictor with\nminimal 1-norm among minimizers of the training loss. In this paper, we deepen\nthis analysis showing that the full training trajectory of diagonal linear\nnetworks is closely related to the lasso regularization path. In this\nconnection, the training time plays the role of an inverse regularization\nparameter. Both rigorous results and simulations are provided to illustrate\nthis conclusion. Under a monotonicity assumption on the lasso regularization\npath, the connection is exact while in the general case, we show an approximate\nconnection.", "AI": {"tldr": "\u5bf9\u89d2\u7ebf\u6027\u7f51\u7edc\u8bad\u7ec3\u8f68\u8ff9\u4e0eLASSO\u6b63\u5219\u5316\u8def\u5f84\u7d27\u5bc6\u76f8\u5173\uff0c\u8bad\u7ec3\u65f6\u95f4\u626e\u6f14\u53cd\u5411\u6b63\u5219\u5316\u53c2\u6570\u7684\u89d2\u8272\u3002", "motivation": "\u6df1\u5165\u5206\u6790\u5bf9\u89d2\u7ebf\u6027\u7f51\u7edc\u7684\u8bad\u7ec3\u8f68\u8ff9\uff0c\u5e76\u63ed\u793a\u5176\u4e0eLASSO\u6b63\u5219\u5316\u8def\u5f84\u7684\u8054\u7cfb\u3002", "method": "\u5206\u6790\u5bf9\u89d2\u7ebf\u6027\u7f51\u7edc\u5728\u5c0f\u521d\u59cb\u5316\u4e0b\u7684\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u5e76\u5c06\u5176\u4e0eLASSO\u6b63\u5219\u5316\u8def\u5f84\u8fdb\u884c\u6bd4\u8f83\uff0c\u63d0\u4f9b\u7406\u8bba\u8bc1\u660e\u548c\u6a21\u62df\u3002", "result": "\u8bc1\u660e\u4e86\u5728\u5355\u8c03\u6027\u5047\u8bbe\u4e0b\uff0c\u5bf9\u89d2\u7ebf\u6027\u7f51\u7edc\u7684\u8bad\u7ec3\u8f68\u8ff9\u4e0eLASSO\u6b63\u5219\u5316\u8def\u5f84\u662f\u7cbe\u786e\u5bf9\u5e94\u7684\uff1b\u5728\u4e00\u822c\u60c5\u51b5\u4e0b\uff0c\u8bc1\u660e\u4e86\u8fd1\u4f3c\u5bf9\u5e94\u5173\u7cfb\u3002", "conclusion": "\u5bf9\u89d2\u7ebf\u6027\u7f51\u7edc\u7684\u8bad\u7ec3\u8fc7\u7a0b\u53ef\u4ee5\u88ab\u89c6\u4e3a\u4e00\u79cdLASSO\u6b63\u5219\u5316\uff0c\u8bad\u7ec3\u65f6\u95f4\u76f8\u5f53\u4e8e\u6b63\u5219\u5316\u53c2\u6570\uff0c\u8fd9\u4e3a\u7406\u89e3\u548c\u5206\u6790\u7ebf\u6027\u7f51\u7edc\u7684\u9690\u5f0f\u6b63\u5219\u5316\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002"}}
{"id": "2509.18926", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18926", "abs": "https://arxiv.org/abs/2509.18926", "authors": ["Pamela Osuna-Vargas", "Altug Kamacioglu", "Dominik F. Aschauer", "Petros E. Vlachos", "Sercan Alipek", "Jochen Triesch", "Simon Rumpel", "Matthias Kaschube"], "title": "SynapFlow: A Modular Framework Towards Large-Scale Analysis of Dendritic Spines", "comment": null, "summary": "Dendritic spines are key structural components of excitatory synapses in the\nbrain. Given the size of dendritic spines provides a proxy for synaptic\nefficacy, their detection and tracking across time is important for studies of\nthe neural basis of learning and memory. Despite their relevance, large-scale\nanalyses of the structural dynamics of dendritic spines in 3D+time microscopy\ndata remain challenging and labor-intense. Here, we present a modular machine\nlearning-based pipeline designed to automate the detection, time-tracking, and\nfeature extraction of dendritic spines in volumes chronically recorded with\ntwo-photon microscopy. Our approach tackles the challenges posed by biological\ndata by combining a transformer-based detection module, a depth-tracking\ncomponent that integrates spatial features, a time-tracking module to associate\n3D spines across time by leveraging spatial consistency, and a feature\nextraction unit that quantifies biologically relevant spine properties. We\nvalidate our method on open-source labeled spine data, and on two complementary\nannotated datasets that we publish alongside this work: one for detection and\ndepth-tracking, and one for time-tracking, which, to the best of our knowledge,\nis the first data of this kind. To encourage future research, we release our\ndata, code, and pre-trained weights at\nhttps://github.com/pamelaosuna/SynapFlow, establishing a baseline for scalable,\nend-to-end analysis of dendritic spine dynamics.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u81ea\u52a8\u5316\u5206\u6790\u6811\u7a81\u68d8\u7ed3\u6784\u52a8\u529b\u5b66\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u5206\u6790\u4e2d\u7684\u6311\u6218\u3002", "motivation": "\u6811\u7a81\u68d8\u7684\u5927\u5c0f\u4e0e\u7a81\u89e6\u6548\u7387\u76f8\u5173\uff0c\u56e0\u6b64\u5728\u5b66\u4e60\u548c\u8bb0\u5fc6\u7814\u7a76\u4e2d\uff0c\u5bf9\u5176\u8fdb\u884c\u68c0\u6d4b\u548c\u8ffd\u8e2a\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u76ee\u524d\u5bf93D+\u65f6\u95f4\u663e\u5fae\u955c\u6570\u636e\u8fdb\u884c\u5927\u89c4\u6a21\u6811\u7a81\u68d8\u7ed3\u6784\u52a8\u529b\u5b66\u5206\u6790\u4ecd\u7136\u56f0\u96be\u4e14\u8017\u65f6\u3002", "method": "\u7814\u7a76\u4eba\u5458\u5f00\u53d1\u4e86\u4e00\u4e2a\u6a21\u5757\u5316\u7684\u673a\u5668\u5b66\u4e60\u6d41\u7a0b\uff0c\u5305\u62ec\u57fa\u4e8eTransformer\u7684\u76ee\u6807\u68c0\u6d4b\u6a21\u5757\u3001\u6574\u5408\u4e86\u7a7a\u95f4\u7279\u5f81\u7684\u6df1\u5ea6\u8ffd\u8e2a\u7ec4\u4ef6\u3001\u5229\u7528\u7a7a\u95f4\u4e00\u81f4\u6027\u5173\u80543D\u68d8\u7a81\u7684\u65f6\u5e8f\u8ffd\u8e2a\u6a21\u5757\uff0c\u4ee5\u53ca\u91cf\u5316\u68d8\u7a81\u751f\u7269\u5b66\u7279\u6027\u7684\u7279\u5f81\u63d0\u53d6\u5355\u5143\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u5f00\u6e90\u6807\u8bb0\u68d8\u7a81\u6570\u636e\u4ee5\u53ca\u4e24\u9879\u8865\u5145\u6570\u636e\u96c6\uff08\u4e00\u9879\u7528\u4e8e\u68c0\u6d4b\u548c\u6df1\u5ea6\u8ffd\u8e2a\uff0c\u4e00\u9879\u7528\u4e8e\u65f6\u5e8f\u8ffd\u8e2a\uff09\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u5e76\u4e14\u662f\u7b2c\u4e00\u4e2a\u63d0\u4f9b\u65f6\u5e8f\u8ffd\u8e2a\u6570\u636e\u96c6\u7684\u7814\u7a76\u3002", "conclusion": "\u7814\u7a76\u4eba\u5458\u53d1\u5e03\u4e86\u6570\u636e\u3001\u4ee3\u7801\u548c\u9884\u8bad\u7ec3\u6743\u91cd\uff0c\u4e3a\u53ef\u6269\u5c55\u7684\u3001\u7aef\u5230\u7aef\u7684\u6811\u7a81\u68d8\u52a8\u529b\u5b66\u5206\u6790\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.18810", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18810", "abs": "https://arxiv.org/abs/2509.18810", "authors": ["Arman Mohammadi", "Mattias Krysander", "Daniel Jung", "Erik Frisk"], "title": "Probabilistic Machine Learning for Uncertainty-Aware Diagnosis of Industrial Systems", "comment": null, "summary": "Deep neural networks has been increasingly applied in fault diagnostics,\nwhere it uses historical data\n  to capture systems behavior, bypassing the need for high-fidelity physical\nmodels.\n  However, despite their competence in prediction tasks, these models often\nstruggle with\n  the evaluation of their confidence. This matter is particularly\n  important in consistency-based diagnosis where decision logic is highly\nsensitive to false alarms.\n  To address this challenge, this work presents a diagnostic framework that\nuses\n  ensemble probabilistic machine learning to\n  improve diagnostic characteristics of data driven consistency based diagnosis\n  by quantifying and automating the prediction uncertainty.\n  The proposed method is evaluated across several case studies using both\nablation\n  and comparative analyses, showing consistent improvements across a range of\ndiagnostic metrics.", "AI": {"tldr": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u6545\u969c\u8bca\u65ad\u4e2d\u6709\u5e94\u7528\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u5176\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u7684\u8bc4\u4f30\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u96c6\u6210\u6982\u7387\u673a\u5668\u5b66\u4e60\u7684\u8bca\u65ad\u6846\u67b6\uff0c\u901a\u8fc7\u91cf\u5316\u548c\u81ea\u52a8\u5316\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u6765\u6539\u8fdb\u6570\u636e\u9a71\u52a8\u7684\u3001\u57fa\u4e8e\u4e00\u81f4\u6027\u7684\u8bca\u65ad\u3002\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u6848\u4f8b\u7814\u7a76\u4e2d\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5e76\u5728\u8bca\u65ad\u6307\u6807\u65b9\u9762\u663e\u793a\u51fa\u4e00\u81f4\u7684\u6539\u8fdb\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u6545\u969c\u8bca\u65ad\u4e2d\u6709\u5e94\u7528\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u5176\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u7684\u8bc4\u4f30\uff0c\u8fd9\u5728\u5bf9\u8bef\u62a5\u9ad8\u5ea6\u654f\u611f\u7684\u4e00\u81f4\u6027\u8bca\u65ad\u4e2d\u5c24\u5176\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u96c6\u6210\u6982\u7387\u673a\u5668\u5b66\u4e60\u7684\u8bca\u65ad\u6846\u67b6\uff0c\u901a\u8fc7\u91cf\u5316\u548c\u81ea\u52a8\u5316\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u6765\u6539\u8fdb\u6570\u636e\u9a71\u52a8\u7684\u3001\u57fa\u4e8e\u4e00\u81f4\u6027\u7684\u8bca\u65ad\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u591a\u4e2a\u6848\u4f8b\u7814\u7a76\u4e2d\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5e76\u5728\u8bca\u65ad\u6307\u6807\u65b9\u9762\u663e\u793a\u51fa\u4e00\u81f4\u7684\u6539\u8fdb\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u91cf\u5316\u548c\u81ea\u52a8\u5316\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\uff0c\u6539\u8fdb\u4e86\u6570\u636e\u9a71\u52a8\u7684\u3001\u57fa\u4e8e\u4e00\u81f4\u6027\u7684\u8bca\u65ad\u3002"}}
{"id": "2509.18938", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18938", "abs": "https://arxiv.org/abs/2509.18938", "authors": ["Matheus Vin\u00edcius Todescato", "Joel Lu\u00eds Carbonera"], "title": "No Labels Needed: Zero-Shot Image Classification with Collaborative Self-Learning", "comment": "This paper was accepted at International Conference on Tools with\n  Artificial Intelligence (ICTAI) 2025", "summary": "While deep learning, including Convolutional Neural Networks (CNNs) and\nVision Transformers (ViTs), has significantly advanced classification\nperformance, its typical reliance on extensive annotated datasets presents a\nmajor obstacle in many practical scenarios where such data is scarce.\nVision-language models (VLMs) and transfer learning with pre-trained visual\nmodels appear as promising techniques to deal with this problem. This paper\nproposes a novel zero-shot image classification framework that combines a VLM\nand a pre-trained visual model within a self-learning cycle. Requiring only the\nset of class names and no labeled training data, our method utilizes a\nconfidence-based pseudo-labeling strategy to train a lightweight classifier\ndirectly on the test data, enabling dynamic adaptation. The VLM identifies\nhigh-confidence samples, and the pre-trained visual model enhances their visual\nrepresentations. These enhanced features then iteratively train the classifier,\nallowing the system to capture complementary semantic and visual cues without\nsupervision. Notably, our approach avoids VLM fine-tuning and the use of large\nlanguage models, relying on the visual-only model to reduce the dependence on\nsemantic representation. Experimental evaluations on ten diverse datasets\ndemonstrate that our approach outperforms the baseline zero-shot method.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u96f6\u6837\u672c\u56fe\u50cf\u5206\u7c7b\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u548c\u9884\u8bad\u7ec3\u89c6\u89c9\u6a21\u578b\uff0c\u5728\u6ca1\u6709\u6807\u6ce8\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u81ea\u5b66\u4e60\u5468\u671f\u548c\u7f6e\u4fe1\u5ea6\u4f2a\u6807\u7b7e\u7b56\u7565\u76f4\u63a5\u5728\u6d4b\u8bd5\u6570\u636e\u4e0a\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u5206\u7c7b\u5668\uff0c\u5b9e\u73b0\u4e86\u52a8\u6001\u9002\u5e94\uff0c\u5e76\u5728\u5341\u4e2a\u591a\u6837\u5316\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u57fa\u7ebf\u96f6\u6837\u672c\u65b9\u6cd5\u7684\u7ed3\u679c\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08\u5982CNNs\u548cViTs\uff09\u5728\u5206\u7c7b\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u901a\u5e38\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u8fd9\u5728\u6570\u636e\u7a00\u7f3a\u7684\u5b9e\u9645\u573a\u666f\u4e2d\u662f\u4e00\u4e2a\u4e3b\u8981\u969c\u788d\u3002\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u548c\u5e26\u6709\u9884\u8bad\u7ec3\u89c6\u89c9\u6a21\u578b\u7684\u8fc1\u79fb\u5b66\u4e60\u662f\u89e3\u51b3\u6b64\u95ee\u9898\u7684\u6709\u5e0c\u671b\u7684\u6280\u672f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408VLM\u548c\u9884\u8bad\u7ec3\u89c6\u89c9\u6a21\u578b\u7684\u65b0\u9896\u96f6\u6837\u672c\u56fe\u50cf\u5206\u7c7b\u6846\u67b6\u3002\u8be5\u6846\u67b6\u5229\u7528\u7f6e\u4fe1\u5ea6\u4f2a\u6807\u7b7e\u7b56\u7565\uff0c\u5728\u4ec5\u6709\u7c7b\u522b\u540d\u79f0\u4e14\u65e0\u6807\u6ce8\u8bad\u7ec3\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u76f4\u63a5\u5728\u6d4b\u8bd5\u6570\u636e\u4e0a\u8bad\u7ec3\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u5206\u7c7b\u5668\uff0c\u4ee5\u5b9e\u73b0\u52a8\u6001\u9002\u5e94\u3002VLM\u8bc6\u522b\u9ad8\u7f6e\u4fe1\u5ea6\u6837\u672c\uff0c\u9884\u8bad\u7ec3\u89c6\u89c9\u6a21\u578b\u589e\u5f3a\u5176\u89c6\u89c9\u8868\u793a\uff0c\u7136\u540e\u8fd9\u4e9b\u589e\u5f3a\u7684\u7279\u5f81\u8fed\u4ee3\u5730\u8bad\u7ec3\u5206\u7c7b\u5668\uff0c\u4ece\u800c\u5728\u65e0\u76d1\u7763\u7684\u60c5\u51b5\u4e0b\u6355\u83b7\u4e92\u8865\u7684\u8bed\u4e49\u548c\u89c6\u89c9\u7ebf\u7d22\u3002\u8be5\u65b9\u6cd5\u907f\u514d\u4e86VLM\u5fae\u8c03\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4f7f\u7528\uff0c\u901a\u8fc7\u4f9d\u8d56\u7eaf\u89c6\u89c9\u6a21\u578b\u6765\u51cf\u5c11\u5bf9\u8bed\u4e49\u8868\u793a\u7684\u4f9d\u8d56\u3002", "result": "\u5728\u5341\u4e2a\u591a\u6837\u5316\u7684\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u57fa\u7ebf\u7684\u96f6\u6837\u672c\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u96f6\u6837\u672c\u56fe\u50cf\u5206\u7c7b\u6846\u67b6\u80fd\u591f\u6709\u6548\u5730\u5728\u6ca1\u6709\u6807\u6ce8\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u5b66\u4e60\u548c\u52a8\u6001\u9002\u5e94\uff0c\u5e76\u4e14\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5c55\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2509.18811", "categories": ["cs.LG", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2509.18811", "abs": "https://arxiv.org/abs/2509.18811", "authors": ["Thomas Savary", "Fran\u00e7ois Rozet", "Gilles Louppe"], "title": "Training-Free Data Assimilation with GenCast", "comment": null, "summary": "Data assimilation is widely used in many disciplines such as meteorology,\noceanography, and robotics to estimate the state of a dynamical system from\nnoisy observations. In this work, we propose a lightweight and general method\nto perform data assimilation using diffusion models pre-trained for emulating\ndynamical systems. Our method builds on particle filters, a class of data\nassimilation algorithms, and does not require any further training. As a\nguiding example throughout this work, we illustrate our methodology on GenCast,\na diffusion-based model that generates global ensemble weather forecasts.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u6269\u6563\u6a21\u578b\u8fdb\u884c\u6570\u636e\u540c\u5316\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\uff0c\u5e76\u4ee5GenCast\u4e3a\u4f8b\u3002", "motivation": "\u6570\u636e\u540c\u5316\u5728\u6c14\u8c61\u3001\u6d77\u6d0b\u3001\u673a\u5668\u4eba\u7b49\u9886\u57df\u88ab\u5e7f\u6cdb\u5e94\u7528\u4e8e\u4ece\u566a\u58f0\u89c2\u6d4b\u4e2d\u4f30\u8ba1\u52a8\u529b\u7cfb\u7edf\u7684\u72b6\u6001\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u53ef\u80fd\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u901a\u7528\u7684\u6570\u636e\u540c\u5316\u65b9\u6cd5\uff0c\u5229\u7528\u4e3a\u6a21\u62df\u52a8\u529b\u7cfb\u7edf\u800c\u9884\u8bad\u7ec3\u7684\u6269\u6563\u6a21\u578b\u3002\u8be5\u65b9\u6cd5\u57fa\u4e8e\u7c92\u5b50\u6ee4\u6ce2\u5668\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "result": "\uff08\u6587\u4e2d\u672a\u660e\u786e\u8bf4\u660e\u5177\u4f53\u5b9e\u9a8c\u7ed3\u679c\uff0c\u4f46\u63d0\u5230\u4e86\u5c06\u8be5\u65b9\u6cd5\u5e94\u7528\u4e8eGenCast\uff0c\u4e00\u4e2a\u751f\u6210\u5f0f\u5929\u6c14\u9884\u62a5\u6a21\u578b\uff09", "conclusion": "\uff08\u6587\u4e2d\u672a\u660e\u786e\u7ed9\u51fa\u7ed3\u8bba\uff0c\u4f46\u8868\u660e\u8be5\u65b9\u6cd5\u662f\u4e00\u79cd\u6709\u524d\u666f\u7684\u6570\u636e\u540c\u5316\u89e3\u51b3\u65b9\u6848\uff09"}}
{"id": "2509.18956", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18956", "abs": "https://arxiv.org/abs/2509.18956", "authors": ["Zijing Guo", "Yunyang Zhao", "Lin Wang"], "title": "Seeing Through Reflections: Advancing 3D Scene Reconstruction in Mirror-Containing Environments with Gaussian Splatting", "comment": null, "summary": "Mirror-containing environments pose unique challenges for 3D reconstruction\nand novel view synthesis (NVS), as reflective surfaces introduce view-dependent\ndistortions and inconsistencies. While cutting-edge methods such as Neural\nRadiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) excel in typical\nscenes, their performance deteriorates in the presence of mirrors. Existing\nsolutions mainly focus on handling mirror surfaces through symmetry mapping but\noften overlook the rich information carried by mirror reflections. These\nreflections offer complementary perspectives that can fill in absent details\nand significantly enhance reconstruction quality. To advance 3D reconstruction\nin mirror-rich environments, we present MirrorScene3D, a comprehensive dataset\nfeaturing diverse indoor scenes, 1256 high-quality images, and annotated mirror\nmasks, providing a benchmark for evaluating reconstruction methods in\nreflective settings. Building on this, we propose ReflectiveGS, an extension of\n3D Gaussian Splatting that utilizes mirror reflections as complementary\nviewpoints rather than simple symmetry artifacts, enhancing scene geometry and\nrecovering absent details. Experiments on MirrorScene3D show that\nReflectiveGaussian outperforms existing methods in SSIM, PSNR, LPIPS, and\ntraining speed, setting a new benchmark for 3D reconstruction in mirror-rich\nenvironments.", "AI": {"tldr": "MirrorScene3D\u6570\u636e\u96c6\u548cReflectiveGS\u65b9\u6cd5\u63d0\u5347\u4e86\u5728\u5305\u542b\u955c\u5b50\u7684\u590d\u6742\u4e09\u7ef4\u573a\u666f\u91cd\u5efa\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u4e09\u7ef4\u91cd\u5efa\u65b9\u6cd5\u5728\u5904\u7406\u955c\u5b50\u8868\u9762\u65f6\u5b58\u5728\u4e0d\u8db3\uff0c\u672a\u80fd\u6709\u6548\u5229\u7528\u955c\u9762\u53cd\u5c04\u7684\u4e30\u5bcc\u4fe1\u606f\uff0c\u5bfc\u81f4\u91cd\u5efa\u8d28\u91cf\u4e0b\u964d\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u65b0\u7684\u6570\u636e\u96c6\u548c\u65b9\u6cd5\u6765\u6539\u8fdb\u955c\u5b50\u73af\u5883\u4e2d\u4e09\u7ef4\u91cd\u5efa\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u540d\u4e3aReflectiveGS\u7684\u6269\u5c55\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5c063D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u8fdb\u884c\u6269\u5c55\uff0c\u5c06\u955c\u9762\u53cd\u5c04\u89c6\u4e3a\u8865\u5145\u89c6\u89d2\u800c\u975e\u7b80\u5355\u7684\u5bf9\u79f0\u4f2a\u5f71\uff0c\u4ee5\u6b64\u6765\u589e\u5f3a\u573a\u666f\u51e0\u4f55\u548c\u6062\u590d\u7f3a\u5931\u7ec6\u8282\u3002\u540c\u65f6\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aMirrorScene3D\u7684\u6570\u636e\u96c6\uff0c\u5305\u542b\u591a\u6837\u5316\u7684\u5ba4\u5185\u573a\u666f\u3001\u9ad8\u8d28\u91cf\u56fe\u50cf\u548c\u6807\u6ce8\u7684\u955c\u5b50\u8499\u7248\uff0c\u7528\u4e8e\u8bc4\u4f30\u53cd\u5c04\u73af\u5883\u4e0b\u7684\u91cd\u5efa\u65b9\u6cd5\u3002", "result": "\u5728MirrorScene3D\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cReflectiveGaussian\u5728SSIM\u3001PSNR\u3001LPIPS\u548c\u8bad\u7ec3\u901f\u5ea6\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3a\u955c\u5b50\u4e30\u5bcc\u7684\u4e09\u7ef4\u91cd\u5efa\u8bbe\u5b9a\u4e86\u65b0\u7684\u57fa\u51c6\u3002", "conclusion": "ReflectiveGS\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5730\u5229\u7528\u955c\u9762\u53cd\u5c04\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5728\u5305\u542b\u955c\u5b50\u590d\u6742\u573a\u666f\u4e0b\u7684\u4e09\u7ef4\u91cd\u5efa\u8d28\u91cf\u548c\u6548\u7387\uff0c\u5e76\u5728MirrorScene3D\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u9886\u5148\u6027\u80fd\u3002"}}
{"id": "2509.18826", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18826", "abs": "https://arxiv.org/abs/2509.18826", "authors": ["Wenlong Lyu", "Yuheng Jia", "Hui Liu", "Junhui Hou"], "title": "Graph-based Clustering Revisited: A Relaxation of Kernel $k$-Means Perspective", "comment": "39 pages, 20 figures", "summary": "The well-known graph-based clustering methods, including spectral clustering,\nsymmetric non-negative matrix factorization, and doubly stochastic\nnormalization, can be viewed as relaxations of the kernel $k$-means approach.\nHowever, we posit that these methods excessively relax their inherent low-rank,\nnonnegative, doubly stochastic, and orthonormal constraints to ensure numerical\nfeasibility, potentially limiting their clustering efficacy. In this paper,\nguided by our theoretical analyses, we propose \\textbf{Lo}w-\\textbf{R}ank\n\\textbf{D}oubly stochastic clustering (\\textbf{LoRD}), a model that only\nrelaxes the orthonormal constraint to derive a probabilistic clustering\nresults. Furthermore, we theoretically establish the equivalence between\northogonality and block diagonality under the doubly stochastic constraint. By\nintegrating \\textbf{B}lock diagonal regularization into LoRD, expressed as the\nmaximization of the Frobenius norm, we propose \\textbf{B-LoRD}, which further\nenhances the clustering performance. To ensure numerical solvability, we\ntransform the non-convex doubly stochastic constraint into a linear convex\nconstraint through the introduction of a class probability parameter. We\nfurther theoretically demonstrate the gradient Lipschitz continuity of our LoRD\nand B-LoRD enables the proposal of a globally convergent projected gradient\ndescent algorithm for their optimization. Extensive experiments validate the\neffectiveness of our approaches. The code is publicly available at\nhttps://github.com/lwl-learning/LoRD.", "AI": {"tldr": "\u73b0\u6709\u7684\u56fe\u805a\u7c7b\u65b9\u6cd5\uff08\u8c31\u805a\u7c7b\u3001\u975e\u8d1f\u77e9\u9635\u5206\u89e3\u3001\u53cc\u968f\u673a\u5f52\u4e00\u5316\uff09\u88ab\u89c6\u4e3a\u6838 k-means \u7684\u677e\u5f1b\u5f62\u5f0f\uff0c\u4f46\u53ef\u80fd\u8fc7\u5ea6\u677e\u5f1b\u7ea6\u675f\u5bfc\u81f4\u805a\u7c7b\u6548\u679c\u4e0d\u4f73\u3002\u672c\u6587\u63d0\u51fa\u7684 LoRD \u6a21\u578b\u4ec5\u653e\u677e\u4e86\u6b63\u4ea4\u7ea6\u675f\uff0c\u5e76\u63a8\u5bfc\u4e86\u6982\u7387\u805a\u7c7b\u7ed3\u679c\u3002\u7406\u8bba\u4e0a\u8bc1\u660e\u4e86\u5728\u53cc\u968f\u673a\u7ea6\u675f\u4e0b\uff0c\u6b63\u4ea4\u6027\u7b49\u4ef7\u4e8e\u5757\u5bf9\u89d2\u6027\u3002\u901a\u8fc7\u5f15\u5165\u5757\u5bf9\u89d2\u6b63\u5219\u5316\uff08\u6700\u5927\u5316 Frobenius \u8303\u6570\uff09\uff0c\u63d0\u51fa\u4e86 B-LoRD \u6a21\u578b\uff0c\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4e86\u805a\u7c7b\u6027\u80fd\u3002\u901a\u8fc7\u5f15\u5165\u7c7b\u522b\u6982\u7387\u53c2\u6570\uff0c\u5c06\u975e\u51f8\u7684\u53cc\u968f\u673a\u7ea6\u675f\u8f6c\u5316\u4e3a\u7ebf\u6027\u51f8\u7ea6\u675f\u3002LoRD \u548c B-LoRD \u5177\u6709\u68af\u5ea6 Lipschitz \u8fde\u7eed\u6027\uff0c\u53ef\u91c7\u7528\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u4f18\u5316\u3002\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u56fe\u805a\u7c7b\u65b9\u6cd5\uff08\u5982\u8c31\u805a\u7c7b\u3001\u975e\u8d1f\u77e9\u9635\u5206\u89e3\u3001\u53cc\u968f\u673a\u5f52\u4e00\u5316\uff09\u53ef\u4ee5\u770b\u4f5c\u662f\u6838 k-means \u65b9\u6cd5\u7684\u677e\u5f1b\u5f62\u5f0f\uff0c\u4f46\u5b83\u4eec\u53ef\u80fd\u8fc7\u5ea6\u677e\u5f1b\u4e86\u56fa\u6709\u7684\u4f4e\u79e9\u3001\u975e\u8d1f\u3001\u53cc\u968f\u673a\u548c\u6b63\u4ea4\u7ea6\u675f\uff0c\u4ee5\u786e\u4fdd\u6570\u503c\u53ef\u884c\u6027\uff0c\u8fd9\u53ef\u80fd\u4f1a\u9650\u5236\u5b83\u4eec\u7684\u805a\u7c7b\u6548\u679c\u3002", "method": "\u672c\u6587\u63d0\u51fa\u7684 LoRD \u6a21\u578b\u4ec5\u653e\u677e\u6b63\u4ea4\u7ea6\u675f\uff0c\u5e76\u63a8\u5bfc\u51fa\u6982\u7387\u805a\u7c7b\u7ed3\u679c\u3002\u7406\u8bba\u4e0a\u8bc1\u660e\u4e86\u5728\u53cc\u968f\u673a\u7ea6\u675f\u4e0b\uff0c\u6b63\u4ea4\u6027\u7b49\u4ef7\u4e8e\u5757\u5bf9\u89d2\u6027\u3002\u901a\u8fc7\u5f15\u5165\u5757\u5bf9\u89d2\u6b63\u5219\u5316\uff08\u6700\u5927\u5316 Frobenius \u8303\u6570\uff09\uff0c\u63d0\u51fa\u4e86 B-LoRD \u6a21\u578b\u3002\u901a\u8fc7\u5f15\u5165\u7c7b\u522b\u6982\u7387\u53c2\u6570\uff0c\u5c06\u975e\u51f8\u7684\u53cc\u968f\u673a\u7ea6\u675f\u8f6c\u5316\u4e3a\u7ebf\u6027\u51f8\u7ea6\u675f\u3002\u7531\u4e8e LoRD \u548c B-LoRD \u5177\u6709\u68af\u5ea6 Lipschitz \u8fde\u7eed\u6027\uff0c\u56e0\u6b64\u53ef\u4ee5\u63d0\u51fa\u4e00\u79cd\u5168\u5c40\u6536\u655b\u7684\u6295\u5f71\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u6765\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u901a\u8fc7\u5927\u91cf\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684 LoRD \u548c B-LoRD \u6a21\u578b\uff0c\u4ee5\u53ca\u76f8\u5e94\u7684\u4f18\u5316\u7b97\u6cd5\uff0c\u5728\u56fe\u805a\u7c7b\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u6709\u6548\u6027\u3002"}}
{"id": "2509.18958", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18958", "abs": "https://arxiv.org/abs/2509.18958", "authors": ["Cristina Iacono", "Mariarosaria Meola", "Federica Conte", "Laura Mecozzi", "Umberto Bracale", "Pietro Falco", "Fanny Ficuciello"], "title": "Generative data augmentation for biliary tract detection on intraoperative images", "comment": null, "summary": "Cholecystectomy is one of the most frequently performed procedures in\ngastrointestinal surgery, and the laparoscopic approach is the gold standard\nfor symptomatic cholecystolithiasis and acute cholecystitis. In addition to the\nadvantages of a significantly faster recovery and better cosmetic results, the\nlaparoscopic approach bears a higher risk of bile duct injury, which has a\nsignificant impact on quality of life and survival. To avoid bile duct injury,\nit is essential to improve the intraoperative visualization of the bile duct.\nThis work aims to address this problem by leveraging a deep-learning approach\nfor the localization of the biliary tract from white-light images acquired\nduring the surgical procedures. To this end, the construction and annotation of\nan image database to train the Yolo detection algorithm has been employed.\nBesides classical data augmentation techniques, the paper proposes Generative\nAdversarial Network (GAN) for the generation of a synthetic portion of the\ntraining dataset. Experimental results have been discussed along with ethical\nconsiderations.", "AI": {"tldr": "\u6df1\u5ea6\u5b66\u4e60\u88ab\u7528\u4e8e\u8179\u8154\u955c\u80c6\u56ca\u5207\u9664\u672f\u4e2d\u80c6\u7ba1\u7684\u53ef\u89c6\u5316\uff0c\u4ee5\u964d\u4f4e\u80c6\u7ba1\u635f\u4f24\u7684\u98ce\u9669\u3002", "motivation": "\u4e3a\u4e86\u964d\u4f4e\u80c6\u56ca\u5207\u9664\u672f\u4e2d\u80c6\u7ba1\u635f\u4f24\u7684\u98ce\u9669\uff0c\u9700\u8981\u63d0\u9ad8\u672f\u4e2d\u80c6\u7ba1\u7684\u53ef\u89c6\u5316\u6c34\u5e73\u3002", "method": "\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\uff08Yolo\u68c0\u6d4b\u7b97\u6cd5\uff09\u548c\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GAN\uff09\u6765\u8bad\u7ec3\u6a21\u578b\uff0c\u4ee5\u4ece\u767d\u5149\u56fe\u50cf\u4e2d\u5b9a\u4f4d\u80c6\u9053\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u5df2\u8ba8\u8bba\u3002", "conclusion": "\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u63d0\u9ad8\u80c6\u7ba1\u53ef\u89c6\u5316\u6c34\u5e73\uff0c\u6709\u671b\u964d\u4f4e\u80c6\u7ba1\u635f\u4f24\u98ce\u9669\u3002"}}
{"id": "2509.18842", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18842", "abs": "https://arxiv.org/abs/2509.18842", "authors": ["Nikolas Chatzis", "Ioannis Kordonis", "Manos Theodosis", "Petros Maragos"], "title": "Shared-Weights Extender and Gradient Voting for Neural Network Expansion", "comment": "5 pages, 3 figures", "summary": "Expanding neural networks during training is a promising way to augment\ncapacity without retraining larger models from scratch. However, newly added\nneurons often fail to adjust to a trained network and become inactive,\nproviding no contribution to capacity growth. We propose the Shared-Weights\nExtender (SWE), a novel method explicitly designed to prevent inactivity of new\nneurons by coupling them with existing ones for smooth integration. In\nparallel, we introduce the Steepest Voting Distributor (SVoD), a gradient-based\nmethod for allocating neurons across layers during deep network expansion. Our\nextensive benchmarking on four datasets shows that our method can effectively\nsuppress neuron inactivity and achieve better performance compared to other\nexpanding methods and baselines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSWE\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u65b0\u795e\u7ecf\u5143\u4e0e\u73b0\u6709\u795e\u7ecf\u5143\u8026\u5408\uff0c\u9632\u6b62\u65b0\u795e\u7ecf\u5143\u5728\u8bad\u7ec3\u4e2d\u53d8\u5f97\u4e0d\u6d3b\u8dc3\uff0c\u4ece\u800c\u5b9e\u73b0\u795e\u7ecf\u7f51\u7edc\u7684\u6709\u6548\u6269\u5c55\u3002\u540c\u65f6\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSVoD\u7684\u68af\u5ea6\u4f18\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u7f51\u7edc\u6269\u5c55\u8fc7\u7a0b\u4e2d\u5408\u7406\u5206\u914d\u795e\u7ecf\u5143\u3002", "motivation": "\u795e\u7ecf\u7f51\u7edc\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u6269\u5c55\u5bb9\u91cf\u65f6\uff0c\u65b0\u6dfb\u52a0\u7684\u795e\u7ecf\u5143\u5f80\u5f80\u96be\u4ee5\u878d\u5165\u73b0\u6709\u7f51\u7edc\u800c\u53d8\u5f97\u4e0d\u6d3b\u8dc3\uff0c\u5bfc\u81f4\u5bb9\u91cf\u589e\u957f\u65e0\u6548\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u5171\u4eab\u6743\u91cd\u6269\u5c55\u5668\uff08SWE\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u65b0\u795e\u7ecf\u5143\u4e0e\u73b0\u6709\u795e\u7ecf\u5143\u8026\u5408\u6765\u5b9e\u73b0\u5e73\u6ed1\u96c6\u6210\uff0c\u4ee5\u9632\u6b62\u65b0\u795e\u7ecf\u5143\u4e0d\u6d3b\u8dc3\u3002\u540c\u65f6\uff0c\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u68af\u5ea6\u7684\u6700\u9661\u6295\u7968\u5206\u914d\u5668\uff08SVoD\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u6df1\u5ea6\u7f51\u7edc\u6269\u5c55\u671f\u95f4\u5206\u914d\u795e\u7ecf\u5143\u5230\u5404\u4e2a\u5c42\u3002", "result": "\u901a\u8fc7\u5728\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7ed3\u679c\u8868\u660e\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u5730\u6291\u5236\u795e\u7ecf\u5143\u4e0d\u6d3b\u8dc3\uff0c\u5e76\u4e14\u4e0e\u5176\u4ed6\u7684\u6269\u5c55\u65b9\u6cd5\u548c\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684SWE\u548cSVoD\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u795e\u7ecf\u7f51\u7edc\u6269\u5c55\u8fc7\u7a0b\u4e2d\u65b0\u795e\u7ecf\u5143\u4e0d\u6d3b\u8dc3\u7684\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2509.18973", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18973", "abs": "https://arxiv.org/abs/2509.18973", "authors": ["Jiabao Chen", "Shan Xiong", "Jialin Peng"], "title": "Prompt-DAS: Annotation-Efficient Prompt Learning for Domain Adaptive Semantic Segmentation of Electron Microscopy Images", "comment": "MICCAI2025", "summary": "Domain adaptive segmentation (DAS) of numerous organelle instances from\nlarge-scale electron microscopy (EM) is a promising way to enable\nannotation-efficient learning. Inspired by SAM, we propose a promptable\nmultitask framework, namely Prompt-DAS, which is flexible enough to utilize any\nnumber of point prompts during the adaptation training stage and testing stage.\nThus, with varying prompt configurations, Prompt-DAS can perform unsupervised\ndomain adaptation (UDA) and weakly supervised domain adaptation (WDA), as well\nas interactive segmentation during testing. Unlike the foundation model SAM,\nwhich necessitates a prompt for each individual object instance, Prompt-DAS is\nonly trained on a small dataset and can utilize full points on all instances,\nsparse points on partial instances, or even no points at all, facilitated by\nthe incorporation of an auxiliary center-point detection task. Moreover, a\nnovel prompt-guided contrastive learning is proposed to enhance discriminative\nfeature learning. Comprehensive experiments conducted on challenging benchmarks\ndemonstrate the effectiveness of the proposed approach over existing UDA, WDA,\nand SAM-based approaches.", "AI": {"tldr": "Prompt-DAS\u662f\u4e00\u4e2a\u53ef\u63d0\u793a\u7684\u591a\u4efb\u52a1\u6846\u67b6\uff0c\u7528\u4e8e\u5bf9\u5927\u89c4\u6a21\u7535\u5b50\u663e\u5fae\u955c\u56fe\u50cf\u4e2d\u7684\u7ec6\u80de\u5668\u5b9e\u4f8b\u8fdb\u884c\u57df\u81ea\u9002\u5e94\u5206\u5272\uff0c\u80fd\u591f\u5904\u7406\u4e0d\u540c\u6570\u91cf\u7684\u63d0\u793a\uff0c\u5e76\u652f\u6301\u65e0\u76d1\u7763\u57df\u81ea\u9002\u5e94\uff08UDA\uff09\u3001\u5f31\u76d1\u7763\u57df\u81ea\u9002\u5e94\uff08WDA\uff09\u548c\u4ea4\u4e92\u5f0f\u5206\u5272\u3002", "motivation": "\u5927\u89c4\u6a21\u7535\u5b50\u663e\u5fae\u955c\u56fe\u50cf\u7684\u7ec6\u80de\u5668\u5b9e\u4f8b\u5206\u5272\u7684\u57df\u81ea\u9002\u5e94\u5206\u5272\uff08DAS\uff09\u662f\u4e00\u79cd\u5b9e\u73b0\u9ad8\u6548\u6807\u6ce8\u5b66\u4e60\u7684\u6709\u524d\u666f\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPrompt-DAS\u7684\u53ef\u63d0\u793a\u591a\u4efb\u52a1\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5728\u81ea\u9002\u5e94\u8bad\u7ec3\u548c\u6d4b\u8bd5\u9636\u6bb5\u90fd\u53ef\u4ee5\u7075\u6d3b\u5730\u4f7f\u7528\u4efb\u610f\u6570\u91cf\u7684\u70b9\u63d0\u793a\u3002\u901a\u8fc7\u5f15\u5165\u8f85\u52a9\u4e2d\u5fc3\u70b9\u68c0\u6d4b\u4efb\u52a1\uff0cPrompt-DAS\u53ef\u4ee5\u5904\u7406\u5168\u70b9\u3001\u7a00\u758f\u70b9\u751a\u81f3\u65e0\u70b9\u7684\u60c5\u51b5\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u63d0\u793a\u5f15\u5bfc\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\u6765\u589e\u5f3a\u533a\u5206\u6027\u7279\u5f81\u5b66\u4e60\u3002", "result": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fdb\u884c\u4e86\u5168\u9762\u7684\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u73b0\u6709\u7684UDA\u3001WDA\u548c\u57fa\u4e8eSAM\u7684\u65b9\u6cd5\u4e0a\u90fd\u66f4\u6709\u6548\u3002", "conclusion": "Prompt-DAS\u901a\u8fc7\u5176\u7075\u6d3b\u6027\u548c\u65b0\u9896\u7684\u7ec4\u4ef6\uff0c\u5728\u7535\u5b50\u663e\u5fae\u955c\u56fe\u50cf\u7684\u7ec6\u80de\u5668\u5b9e\u4f8b\u5206\u5272\u65b9\u9762\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002"}}
{"id": "2509.18851", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18851", "abs": "https://arxiv.org/abs/2509.18851", "authors": ["Gongrui Nan", "Siye Chen", "Jing Huang", "Mengyu Lu", "Dexun Wang", "Chunmei Xie", "Weiqi Xiong", "Xianzhou Zeng", "Qixuan Zhou", "Yadong Li", "Xingzhong Xu"], "title": "NGRPO: Negative-enhanced Group Relative Policy Optimization", "comment": null, "summary": "RLVR has enhanced the reasoning capabilities of Large Language Models (LLMs)\nacross various tasks. However, GRPO, a representative RLVR algorithm, suffers\nfrom a critical limitation: when all responses within a group are either\nentirely correct or entirely incorrect, the model fails to learn from these\nhomogeneous responses. This is particularly problematic for homogeneously\nincorrect groups, where GRPO's advantage function yields a value of zero,\nleading to null gradients and the loss of valuable learning signals. To\novercome this issue, we propose NGRPO (Negative-enhanced Group Relative Policy\nOptimization), an algorithm designed to convert homogeneous errors into robust\nlearning signals. First, NGRPO introduces Advantage Calibration. This mechanism\nhypothesizes the existence of a virtual maximum-reward sample during advantage\ncalculation, thereby altering the mean and variance of rewards within a group\nand ensuring that the advantages for homogeneously incorrect samples are no\nlonger zero. Second, NGRPO employs Asymmetric Clipping, which relaxes the\nupdate magnitude for positive samples while imposing stricter constraints on\nthat of negative samples. This serves to stabilize the exploration pressure\nintroduced by the advantage calibration. Our experiments on Qwen2.5-Math-7B\ndemonstrate that NGRPO significantly outperforms baselines such as PPO, GRPO,\nDAPO, and PSR-NSR on mathematical benchmarks including MATH500, AMC23, and\nAIME2025. These results validate NGRPO's ability to learn from homogeneous\nerrors, leading to stable and substantial improvements in mathematical\nreasoning. Our code is available at https://github.com/nangongrui-ngr/NGRPO.", "AI": {"tldr": "GRPO\u7b97\u6cd5\u5728\u5904\u7406\u540c\u8d28\u5316\u54cd\u5e94\u65f6\u5b58\u5728\u5b66\u4e60\u4fe1\u53f7\u7f3a\u5931\u7684\u95ee\u9898\uff0cNGRPO\u901a\u8fc7\u5f15\u5165\u4f18\u52bf\u6821\u51c6\u548c\u975e\u5bf9\u79f0\u88c1\u526a\u6765\u89e3\u51b3\u6b64\u95ee\u9898\uff0c\u5e76\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "GRPO\u7b97\u6cd5\u5728\u5904\u7406\u5168\u5bf9\u6216\u5168\u9519\u7684\u540c\u8d28\u5316\u54cd\u5e94\u65f6\uff0c\u7531\u4e8e\u4f18\u52bf\u51fd\u6570\u503c\u4e3a\u96f6\uff0c\u5bfc\u81f4\u68af\u5ea6\u4e3a\u7a7a\uff0c\u65e0\u6cd5\u6709\u6548\u5b66\u4e60\u3002\u8fd9\u5728\u540c\u8d28\u5316\u9519\u8bef\u54cd\u5e94\u65f6\u5c24\u4e3a\u4e25\u91cd\uff0c\u4f1a\u4e22\u5931\u5b9d\u8d35\u7684\u5b66\u4e60\u4fe1\u53f7\u3002", "method": "NGRPO\u7b97\u6cd5\u901a\u8fc7\u5f15\u5165\u4e24\u4e2a\u5173\u952e\u673a\u5236\u6765\u89e3\u51b3GRPO\u7684\u5c40\u9650\u6027\uff1a1. \u4f18\u52bf\u6821\u51c6\uff08Advantage Calibration\uff09\uff1a\u5047\u8bbe\u5b58\u5728\u4e00\u4e2a\u865a\u62df\u7684\u6700\u5927\u5956\u52b1\u6837\u672c\uff0c\u5728\u8ba1\u7b97\u4f18\u52bf\u65f6\u6539\u53d8\u5956\u52b1\u7684\u5747\u503c\u548c\u65b9\u5dee\uff0c\u786e\u4fdd\u540c\u8d28\u5316\u9519\u8bef\u6837\u672c\u7684\u4f18\u52bf\u503c\u4e0d\u518d\u4e3a\u96f6\u30022. \u975e\u5bf9\u79f0\u88c1\u526a\uff08Asymmetric Clipping\uff09\uff1a\u5bf9\u6b63\u6837\u672c\u7684\u66f4\u65b0\u5e45\u5ea6\u8fdb\u884c\u653e\u5bbd\uff0c\u5bf9\u8d1f\u6837\u672c\u7684\u66f4\u65b0\u5e45\u5ea6\u8fdb\u884c\u66f4\u4e25\u683c\u7684\u7ea6\u675f\uff0c\u4ee5\u7a33\u5b9a\u4f18\u52bf\u6821\u51c6\u5e26\u6765\u7684\u63a2\u7d22\u538b\u529b\u3002", "result": "\u5728Qwen2.5-Math-7B\u6a21\u578b\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cNGRPO\u5728MATH500\u3001AMC23\u548cAIME2025\u7b49\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u663e\u8457\u4f18\u4e8ePPO\u3001GRPO\u3001DAPO\u548cPSR-NSR\u7b49\u57fa\u7ebf\u7b97\u6cd5\u3002", "conclusion": "NGRPO\u7b97\u6cd5\u6210\u529f\u5730\u4ece\u540c\u8d28\u5316\u9519\u8bef\u4e2d\u5b66\u4e60\uff0c\u5b9e\u73b0\u4e86\u6570\u5b66\u63a8\u7406\u80fd\u529b\u7684\u7a33\u5b9a\u4e14\u5927\u5e45\u5ea6\u7684\u63d0\u5347\u3002"}}
{"id": "2509.18893", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18893", "abs": "https://arxiv.org/abs/2509.18893", "authors": ["Qinhan Hou", "Yilun Zheng", "Xichun Zhang", "Sitao Luan", "Jing Tang"], "title": "Exploring Heterophily in Graph-level Tasks", "comment": "Accectped by NeurIPS 2025 Workshop, New Perspectives in Advancing\n  Graph Machine Learning (NPGML)", "summary": "While heterophily has been widely studied in node-level tasks, its impact on\ngraph-level tasks remains unclear. We present the first analysis of heterophily\nin graph-level learning, combining theoretical insights with empirical\nvalidation. We first introduce a taxonomy of graph-level labeling schemes, and\nfocus on motif-based tasks within local structure labeling, which is a popular\nlabeling scheme. Using energy-based gradient flow analysis, we reveal a key\ninsight: unlike frequency-dominated regimes in node-level tasks, motif\ndetection requires mixed-frequency dynamics to remain flexible across multiple\nspectral components. Our theory shows that motif objectives are inherently\nmisaligned with global frequency dominance, demanding distinct architectural\nconsiderations. Experiments on synthetic datasets with controlled heterophily\nand real-world molecular property prediction support our findings, showing that\nfrequency-adaptive model outperform frequency-dominated models. This work\nestablishes a new theoretical understanding of heterophily in graph-level\nlearning and offers guidance for designing effective GNN architectures.", "AI": {"tldr": "\u5728\u56fe\u7ea7\u5b66\u4e60\u4efb\u52a1\u4e2d\uff0c\u7814\u7a76\u5f02\u8d28\u6027\u5bf9\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u8bbe\u8ba1\u6709\u91cd\u8981\u610f\u4e49\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u8282\u70b9\u7ea7\u4efb\u52a1\u7684\u5f02\u8d28\u6027\uff0c\u800c\u5bf9\u56fe\u7ea7\u4efb\u52a1\u7684\u5f71\u54cd\u5c1a\u4e0d\u6e05\u695a\u3002", "method": "\u63d0\u51fa\u56fe\u7ea7\u6807\u8bb0\u65b9\u6848\u5206\u7c7b\u6cd5\uff0c\u5e76\u805a\u7126\u4e8e\u5c40\u90e8\u7ed3\u6784\u6807\u8bb0\u4e2d\u7684\u57fa\u4e8e\u56fe\u5143\u7684\u4efb\u52a1\u3002\u5229\u7528\u57fa\u4e8e\u80fd\u91cf\u7684\u68af\u5ea6\u6d41\u5206\u6790\uff0c\u63ed\u793a\u4e86\u4e0e\u8282\u70b9\u7ea7\u4efb\u52a1\u7684\u9891\u7387\u4e3b\u5bfc\u673a\u5236\u4e0d\u540c\uff0c\u56fe\u5143\u68c0\u6d4b\u9700\u8981\u6df7\u5408\u9891\u7387\u52a8\u6001\u6765\u9002\u5e94\u591a\u4e2a\u5149\u8c31\u6210\u5206\u3002\u5b9e\u9a8c\u5728\u5408\u6210\u6570\u636e\u96c6\u548c\u771f\u5b9e\u5206\u5b50\u5c5e\u6027\u9884\u6d4b\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u57fa\u4e8e\u56fe\u5143\u7684\u4efb\u52a1\u76ee\u6807\u4e0e\u5168\u5c40\u9891\u7387\u4e3b\u5bfc\u4e0d\u4e00\u81f4\uff0c\u9700\u8981\u4e0d\u540c\u7684\u67b6\u6784\u8003\u8651\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u9891\u7387\u81ea\u9002\u5e94\u6a21\u578b\u4f18\u4e8e\u9891\u7387\u4e3b\u5bfc\u6a21\u578b\u3002", "conclusion": "\u8be5\u7814\u7a76\u9996\u6b21\u5206\u6790\u4e86\u56fe\u7ea7\u5b66\u4e60\u4e2d\u7684\u5f02\u8d28\u6027\uff0c\u63ed\u793a\u4e86\u5176\u5728\u56fe\u5143\u68c0\u6d4b\u7b49\u4efb\u52a1\u4e2d\u7684\u91cd\u8981\u4f5c\u7528\uff0c\u5e76\u4e3a\u8bbe\u8ba1\u6709\u6548\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2509.19003", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19003", "abs": "https://arxiv.org/abs/2509.19003", "authors": ["Honghao Chen", "Xingzhou Lou", "Xiaokun Feng", "Kaiqi Huang", "Xinlong Wang"], "title": "Unveiling Chain of Step Reasoning for Vision-Language Models with Fine-grained Rewards", "comment": "Accepted by NeurIPS 2025", "summary": "Chain of thought reasoning has demonstrated remarkable success in large\nlanguage models, yet its adaptation to vision-language reasoning remains an\nopen challenge with unclear best practices. Existing attempts typically employ\nreasoning chains at a coarse-grained level, which struggles to perform\nfine-grained structured reasoning and, more importantly, are difficult to\nevaluate the reward and quality of intermediate reasoning. In this work, we\ndelve into chain of step reasoning for vision-language models, enabling\nassessing reasoning step quality accurately and leading to effective\nreinforcement learning and inference-time scaling with fine-grained rewards. We\npresent a simple, effective, and fully transparent framework, including the\nstep-level reasoning data, process reward model (PRM), and reinforcement\nlearning training. With the proposed approaches, our models set strong\nbaselines with consistent improvements on challenging vision-language\nbenchmarks. More importantly, we conduct a thorough empirical analysis and\nablation study, unveiling the impact of each component and several intriguing\nproperties of inference-time scaling. We believe this paper serves as a\nbaseline for vision-language models and offers insights into more complex\nmultimodal reasoning. Our dataset, PRM, and code will be available at\nhttps://github.com/baaivision/CoS.", "AI": {"tldr": "Chain of thought reasoning is adapted for vision-language models with a focus on step-by-step reasoning and fine-grained rewards, leading to improved performance and insights into scaling.", "motivation": "Existing coarse-grained chain of thought reasoning struggles with fine-grained vision-language tasks and evaluation. This work aims to enable accurate assessment of reasoning step quality for effective reinforcement learning and inference-time scaling.", "method": "The paper introduces a framework for step-level reasoning in vision-language models, including data, a process reward model (PRM), and reinforcement learning training, to facilitate fine-grained rewards and accurate step quality assessment.", "result": "The proposed methods establish strong baselines with consistent improvements on challenging vision-language benchmarks, supported by thorough empirical analysis and ablation studies.", "conclusion": "This work provides a baseline for vision-language models using step-level reasoning and offers insights into multimodal reasoning and inference-time scaling. The dataset, PRM, and code will be made available."}}
{"id": "2509.18904", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18904", "abs": "https://arxiv.org/abs/2509.18904", "authors": ["Zhaoxin Wang", "Handing Wang", "Cong Tian", "Yaochu Jin"], "title": "Enhancing the Effectiveness and Durability of Backdoor Attacks in Federated Learning through Maximizing Task Distinction", "comment": null, "summary": "Federated learning allows multiple participants to collaboratively train a\ncentral model without sharing their private data. However, this distributed\nnature also exposes new attack surfaces. In particular, backdoor attacks allow\nattackers to implant malicious behaviors into the global model while\nmaintaining high accuracy on benign inputs. Existing attacks usually rely on\nfixed patterns or adversarial perturbations as triggers, which tightly couple\nthe main and backdoor tasks. This coupling makes them vulnerable to dilution by\nhonest updates and limits their persistence under federated defenses. In this\nwork, we propose an approach to decouple the backdoor task from the main task\nby dynamically optimizing the backdoor trigger within a min-max framework. The\ninner layer maximizes the performance gap between poisoned and benign samples,\nensuring that the contributions of benign users have minimal impact on the\nbackdoor. The outer process injects the adaptive triggers into the local model.\nWe evaluate our method on both computer vision and natural language tasks, and\ncompare it with six backdoor attack methods under six defense algorithms.\nExperimental results show that our method achieves good attack performance and\ncan be easily integrated into existing backdoor attack techniques.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u8026\u62dc\u5360\u5ead\u653b\u51fb\u4e0e\u4e3b\u4efb\u52a1\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728min-max\u6846\u67b6\u5185\u52a8\u6001\u4f18\u5316\u62dc\u5360\u5ead\u89e6\u53d1\u5668\uff0c\u4ee5\u589e\u5f3a\u653b\u51fb\u7684\u6301\u4e45\u6027\u548c\u9690\u853d\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u62dc\u5360\u5ead\u653b\u51fb\u65b9\u6cd5\u5c06\u89e6\u53d1\u5668\u4e0e\u4e3b\u4efb\u52a1\u8026\u5408\uff0c\u5bb9\u6613\u88ab\u8bda\u5b9e\u66f4\u65b0\u7a00\u91ca\uff0c\u5e76\u4e14\u5728\u8054\u90a6\u9632\u5fa1\u4e0b\u6301\u4e45\u6027\u6709\u9650\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u89e3\u8026\u62dc\u5360\u5ead\u653b\u51fb\u4e0e\u4e3b\u4efb\u52a1\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728min-max\u6846\u67b6\u5185\u52a8\u6001\u4f18\u5316\u62dc\u5360\u5ead\u89e6\u53d1\u5668\u3002\u5185\u5c42\u6700\u5927\u5316\u4e2d\u6bd2\u548c\u826f\u6027\u6837\u672c\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u5916\u5c42\u5c06\u81ea\u9002\u5e94\u89e6\u53d1\u5668\u6ce8\u5165\u5c40\u90e8\u6a21\u578b\u3002", "result": "\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5e76\u5c06\u8be5\u65b9\u6cd5\u4e0e\u516d\u79cd\u62dc\u5360\u5ead\u653b\u51fb\u65b9\u6cd5\u548c\u516d\u79cd\u9632\u5fa1\u7b97\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u826f\u597d\u7684\u653b\u51fb\u6027\u80fd\uff0c\u5e76\u4e14\u6613\u4e8e\u96c6\u6210\u5230\u73b0\u6709\u7684\u62dc\u5360\u5ead\u653b\u51fb\u6280\u672f\u4e2d\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u52a8\u6001\u4f18\u5316\u89e6\u53d1\u5668\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u8026\u62dc\u5360\u5ead\u4efb\u52a1\u4e0e\u4e3b\u4efb\u52a1\uff0c\u589e\u5f3a\u4e86\u653b\u51fb\u7684\u6301\u4e45\u6027\u548c\u9690\u853d\u6027\uff0c\u5e76\u4e14\u6613\u4e8e\u96c6\u6210\u5230\u73b0\u6709\u653b\u51fb\u6280\u672f\u4e2d\uff0c\u5728\u591a\u79cd\u4efb\u52a1\u548c\u9632\u5fa1\u573a\u666f\u4e0b\u5747\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2509.19028", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19028", "abs": "https://arxiv.org/abs/2509.19028", "authors": ["Ioannis Sarafis", "Alexandros Papadopoulos", "Anastasios Delopoulos"], "title": "Weakly Supervised Food Image Segmentation using Vision Transformers and Segment Anything Model", "comment": "Submitted to the 20th International Workshop on Semantic and Social\n  Media Adaptation & Personalization", "summary": "In this paper, we propose a weakly supervised semantic segmentation approach\nfor food images which takes advantage of the zero-shot capabilities and\npromptability of the Segment Anything Model (SAM) along with the attention\nmechanisms of Vision Transformers (ViTs). Specifically, we use class activation\nmaps (CAMs) from ViTs to generate prompts for SAM, resulting in masks suitable\nfor food image segmentation. The ViT model, a Swin Transformer, is trained\nexclusively using image-level annotations, eliminating the need for pixel-level\nannotations during training. Additionally, to enhance the quality of the\nSAM-generated masks, we examine the use of image preprocessing techniques in\ncombination with single-mask and multi-mask SAM generation strategies. The\nmethodology is evaluated on the FoodSeg103 dataset, generating an average of\n2.4 masks per image (excluding background), and achieving an mIoU of 0.54 for\nthe multi-mask scenario. We envision the proposed approach as a tool to\naccelerate food image annotation tasks or as an integrated component in food\nand nutrition tracking applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528SAM\u548cViT\u7684\u5f31\u76d1\u7763\u98df\u7269\u56fe\u50cf\u8bed\u4e49\u5206\u5272\u65b9\u6cd5\uff0c\u4ec5\u4f7f\u7528\u56fe\u50cf\u7ea7\u6807\u6ce8\uff0c\u901a\u8fc7CAM\u751f\u6210SAM\u7684\u63d0\u793a\uff0c\u5728FoodSeg103\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e860.54\u7684mIoU\u3002", "motivation": "\u5229\u7528SAM\u7684\u96f6\u6837\u672c\u80fd\u529b\u548cViT\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u89e3\u51b3\u98df\u7269\u56fe\u50cf\u8bed\u4e49\u5206\u5272\u4e2d\u50cf\u7d20\u7ea7\u6807\u6ce8\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528ViT\uff08Swin Transformer\uff09\u7684\u7c7b\u522b\u6fc0\u6d3b\u56fe\uff08CAM\uff09\u4e3aSAM\u751f\u6210\u63d0\u793a\uff0c\u5e76\u7ed3\u5408\u56fe\u50cf\u9884\u5904\u7406\u3001\u5355\u63a9\u6a21\u548c\u591a\u63a9\u6a21\u7b56\u7565\u6765\u4f18\u5316SAM\u751f\u6210\u7684\u5206\u5272\u63a9\u6a21\u3002", "result": "\u5728FoodSeg103\u6570\u636e\u96c6\u4e0a\uff0c\u591a\u63a9\u6a21\u60c5\u51b5\u4e0b\u5e73\u5747\u6bcf\u4e2a\u56fe\u50cf\u751f\u62102.4\u4e2a\uff08\u4e0d\u5305\u62ec\u80cc\u666f\uff09\u63a9\u6a21\uff0cmIoU\u8fbe\u52300.54\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u4f5c\u4e3a\u52a0\u901f\u98df\u7269\u56fe\u50cf\u6807\u6ce8\u4efb\u52a1\u7684\u5de5\u5177\uff0c\u6216\u96c6\u6210\u5230\u98df\u7269\u548c\u8425\u517b\u8ffd\u8e2a\u5e94\u7528\u4e2d\u3002"}}
{"id": "2509.18930", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18930", "abs": "https://arxiv.org/abs/2509.18930", "authors": ["Alex Schutz", "Victor-Alexandru Darvariu", "Efimia Panagiotaki", "Bruno Lacerda", "Nick Hawes"], "title": "Tackling GNARLy Problems: Graph Neural Algorithmic Reasoning Reimagined through Reinforcement Learning", "comment": null, "summary": "Neural Algorithmic Reasoning (NAR) is a paradigm that trains neural networks\nto execute classic algorithms by supervised learning. Despite its successes,\nimportant limitations remain: inability to construct valid solutions without\npost-processing and to reason about multiple correct ones, poor performance on\ncombinatorial NP-hard problems, and inapplicability to problems for which\nstrong algorithms are not yet known. To address these limitations, we reframe\nthe problem of learning algorithm trajectories as a Markov Decision Process,\nwhich imposes structure on the solution construction procedure and unlocks the\npowerful tools of imitation and reinforcement learning (RL). We propose the\nGNARL framework, encompassing the methodology to translate problem formulations\nfrom NAR to RL and a learning architecture suitable for a wide range of\ngraph-based problems. We achieve very high graph accuracy results on several\nCLRS-30 problems, performance matching or exceeding much narrower NAR\napproaches for NP-hard problems and, remarkably, applicability even when\nlacking an expert algorithm.", "AI": {"tldr": "\u901a\u8fc7\u5c06\u5b66\u4e60\u7b97\u6cd5\u8f68\u8ff9\u7684\u95ee\u9898\u91cd\u5851\u4e3a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5e76\u5f15\u5165GNARL\u6846\u67b6\uff0c\u6211\u4eec\u6539\u8fdb\u4e86\u795e\u7ecf\u7b97\u6cd5\u63a8\u7406\uff08NAR\uff09\u7684\u5c40\u9650\u6027\uff0c\u4f7f\u5176\u80fd\u591f\u5904\u7406NP\u96be\u95ee\u9898\uff0c\u5e76\u4e14\u5728\u6ca1\u6709\u4e13\u5bb6\u7b97\u6cd5\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u9002\u7528\u3002", "motivation": "NAR\u8303\u5f0f\u5728\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u6267\u884c\u7ecf\u5178\u7b97\u6cd5\u65b9\u9762\u53d6\u5f97\u4e86\u6210\u529f\uff0c\u4f46\u5b58\u5728\u4e00\u4e9b\u5c40\u9650\u6027\uff0c\u5305\u62ec\u65e0\u6cd5\u72ec\u7acb\u6784\u5efa\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3001\u65e0\u6cd5\u5904\u7406\u591a\u4e2a\u6b63\u786e\u89e3\u51b3\u65b9\u6848\u3001\u5728\u7ec4\u5408NP\u96be\u95ee\u9898\u4e0a\u8868\u73b0\u4e0d\u4f73\u4ee5\u53ca\u65e0\u6cd5\u5e94\u7528\u4e8e\u5c1a\u65e0\u5df2\u77e5\u6709\u6548\u7b97\u6cd5\u7684\u95ee\u9898\u3002", "method": "\u5c06\u5b66\u4e60\u7b97\u6cd5\u8f68\u8ff9\u91cd\u5851\u4e3a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5e76\u63d0\u51faGNARL\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5305\u542b\u5c06NAR\u95ee\u9898\u8f6c\u5316\u4e3aRL\u95ee\u9898\u7684\u65b9\u6cd5\u4ee5\u53ca\u9002\u7528\u4e8e\u591a\u79cd\u57fa\u4e8e\u56fe\u7684\u95ee\u9898\u7684\u5b66\u4e60\u67b6\u6784\u3002", "result": "\u5728\u51e0\u4e2aCLRS-30\u95ee\u9898\u4e0a\u5b9e\u73b0\u4e86\u5f88\u9ad8\u7684\u56fe\u51c6\u786e\u6027\uff0c\u5176\u5728NP\u96be\u95ee\u9898\u4e0a\u7684\u6027\u80fd\u4e0e\u66f4\u7a84\u7684NAR\u65b9\u6cd5\u76f8\u5f53\u6216\u66f4\u4f18\uff0c\u5e76\u4e14\u5373\u4f7f\u5728\u6ca1\u6709\u4e13\u5bb6\u7b97\u6cd5\u7684\u60c5\u51b5\u4e0b\u4e5f\u5177\u6709\u9002\u7528\u6027\u3002", "conclusion": "GNARL\u6846\u67b6\u901a\u8fc7\u5c06NAR\u95ee\u9898\u8f6c\u5316\u4e3aRL\u95ee\u9898\uff0c\u6709\u6548\u5730\u89e3\u51b3\u4e86\u73b0\u6709NAR\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u5404\u79cd\u57fa\u4e8e\u56fe\u7684\u95ee\u9898\u4e0a\u5c55\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u5305\u62ecNP\u96be\u95ee\u9898\u548c\u7f3a\u4e4f\u4e13\u5bb6\u7b97\u6cd5\u7684\u95ee\u9898\u3002"}}
{"id": "2509.19052", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19052", "abs": "https://arxiv.org/abs/2509.19052", "authors": ["Jierui Qu", "Jianchun Zhao"], "title": "A DyL-Unet framework based on dynamic learning for Temporally Consistent Echocardiographic Segmentation", "comment": null, "summary": "Accurate segmentation of cardiac anatomy in echocardiography is essential for\ncardiovascular diagnosis and treatment. Yet echocardiography is prone to\ndeformation and speckle noise, causing frame-to-frame segmentation jitter. Even\nwith high accuracy in single-frame segmentation, temporal instability can\nweaken functional estimates and impair clinical interpretability. To address\nthese issues, we propose DyL-UNet, a dynamic learning-based temporal\nconsistency U-Net segmentation architecture designed to achieve temporally\nstable and precise echocardiographic segmentation. The framework constructs an\nEcho-Dynamics Graph (EDG) through dynamic learning to extract dynamic\ninformation from videos. DyL-UNet incorporates multiple Swin-Transformer-based\nencoder-decoder branches for processing single-frame images. It further\nintroduces Cardiac Phase-Dynamics Attention (CPDA) at the skip connections,\nwhich uses EDG-encoded dynamic features and cardiac-phase cues to enforce\ntemporal consistency during segmentation. Extensive experiments on the CAMUS\nand EchoNet-Dynamic datasets demonstrate that DyL-UNet maintains segmentation\naccuracy comparable to existing methods while achieving superior temporal\nconsistency, providing a reliable solution for automated clinical\nechocardiography.", "AI": {"tldr": "DyL-UNet\u662f\u4e00\u79cd\u57fa\u4e8e\u52a8\u6001\u5b66\u4e60\u7684U-Net\u5206\u5272\u6a21\u578b\uff0c\u901a\u8fc7\u6784\u5efa\u56de\u58f0\u52a8\u529b\u5b66\u56fe\uff08EDG\uff09\u5e76\u5f15\u5165\u5fc3\u8154\u76f8\u4f4d\u52a8\u529b\u5b66\u6ce8\u610f\u529b\uff08CPDA\uff09\u6765\u63d0\u9ad8\u8d85\u58f0\u5fc3\u52a8\u56fe\u5206\u5272\u7684\u65f6\u95f4\u7a33\u5b9a\u6027\u548c\u51c6\u786e\u6027\uff0c\u5728CAMUS\u548cEchoNet-Dynamic\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u8d85\u58f0\u5fc3\u52a8\u56fe\u7684\u53d8\u5f62\u548c\u6591\u70b9\u566a\u58f0\u4f1a\u5bfc\u81f4\u9010\u5e27\u5206\u5272\u4e0d\u51c6\u786e\uff0c\u5f71\u54cd\u529f\u80fd\u4f30\u8ba1\u548c\u4e34\u5e8a\u89e3\u91ca\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u5355\u5e27\u5206\u5272\u51c6\u786e\u6027\u9ad8\u7684\u60c5\u51b5\u4e0b\uff0c\u65f6\u95f4\u4e0d\u7a33\u5b9a\u6027\u4ecd\u7136\u662f\u4e00\u4e2a\u95ee\u9898\u3002", "method": "\u63d0\u51faDyL-UNet\u6a21\u578b\uff0c\u5305\u542b\u57fa\u4e8eSwin-Transformer\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u5206\u652f\uff0c\u5e76\u901a\u8fc7\u52a8\u6001\u5b66\u4e60\u6784\u5efa\u56de\u58f0\u52a8\u529b\u5b66\u56fe\uff08EDG\uff09\u63d0\u53d6\u52a8\u6001\u4fe1\u606f\u3002\u5728\u8df3\u8dc3\u8fde\u63a5\u5904\u5f15\u5165\u5fc3\u8154\u76f8\u4f4d\u52a8\u529b\u5b66\u6ce8\u610f\u529b\uff08CPDA\uff09\uff0c\u5229\u7528EDG\u548c\u5fc3\u8154\u76f8\u4f4d\u4fe1\u606f\u5f3a\u5236\u6267\u884c\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "result": "DyL-UNet\u5728CAMUS\u548cEchoNet-Dynamic\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u5f53\u7684\u5206\u5272\u51c6\u786e\u6027\uff0c\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u4e86\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "conclusion": "DyL-UNet\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u7528\u4e8e\u81ea\u52a8\u5316\u4e34\u5e8a\u8d85\u58f0\u5fc3\u52a8\u56fe\u5206\u6790\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u65f6\u95f4\u7a33\u5b9a\u6027\u548c\u51c6\u786e\u6027\u65b9\u9762\u5b58\u5728\u7684\u95ee\u9898\u3002"}}
{"id": "2509.18949", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18949", "abs": "https://arxiv.org/abs/2509.18949", "authors": ["Niccol\u00f2 Rocchi", "Fabio Stella", "Cassio de Campos"], "title": "Towards Privacy-Aware Bayesian Networks: A Credal Approach", "comment": "Accepted at ECAI2025 conference, 20 pages, 1 figure", "summary": "Bayesian networks (BN) are probabilistic graphical models that enable\nefficient knowledge representation and inference. These have proven effective\nacross diverse domains, including healthcare, bioinformatics and economics. The\nstructure and parameters of a BN can be obtained by domain experts or directly\nlearned from available data. However, as privacy concerns escalate, it becomes\nincreasingly critical for publicly released models to safeguard sensitive\ninformation in training data. Typically, released models do not prioritize\nprivacy by design. In particular, tracing attacks from adversaries can combine\nthe released BN with auxiliary data to determine whether specific individuals\nbelong to the data from which the BN was learned. State-of-the-art protection\ntecniques involve introducing noise into the learned parameters. While this\noffers robust protection against tracing attacks, it significantly impacts the\nmodel's utility, in terms of both the significance and accuracy of the\nresulting inferences. Hence, high privacy may be attained at the cost of\nreleasing a possibly ineffective model. This paper introduces credal networks\n(CN) as a novel solution for balancing the model's privacy and utility. After\nadapting the notion of tracing attacks, we demonstrate that a CN enables the\nmasking of the learned BN, thereby reducing the probability of successful\nattacks. As CNs are obfuscated but not noisy versions of BNs, they can achieve\nmeaningful inferences while safeguarding privacy. Moreover, we identify key\nlearning information that must be concealed to prevent attackers from\nrecovering the underlying BN. Finally, we conduct a set of numerical\nexperiments to analyze how privacy gains can be modulated by tuning the CN\nhyperparameters. Our results confirm that CNs provide a principled, practical,\nand effective approach towards the development of privacy-aware probabilistic\ngraphical models.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4f7f\u7528Credal Networks\uff08CN\uff09\u6765\u5e73\u8861\u8d1d\u53f6\u65af\u7f51\u7edc\uff08BN\uff09\u5728\u9690\u79c1\u4fdd\u62a4\u548c\u6a21\u578b\u6548\u7528\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u901a\u8fc7\u63a9\u7801\uff08masking\uff09\u6280\u672f\u964d\u4f4e \ucd94\uc801 \uacf5\uaca9 (tracing attacks) \u7684\u6210\u529f\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u8c03\u6574CN\u7684\u8d85\u53c2\u6570\u6765\u63a7\u5236\u9690\u79c1\u589e\u76ca\u3002", "motivation": "\u73b0\u6709\u7684\u8d1d\u53f6\u65af\u7f51\u7edc\uff08BN\uff09\u5728\u9690\u79c1\u4fdd\u62a4\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c \ucd94\uc801 \uacf5\uaca9 (tracing attacks) \u53ef\u4ee5\u7ed3\u5408\u516c\u5f00\u7684BN\u548c\u8f85\u52a9\u6570\u636e\u6765\u8bc6\u522b\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u4e2a\u4f53\uff0c\u800c\u73b0\u6709\u7684\u9690\u79c1\u4fdd\u62a4\u6280\u672f\uff08\u5982\u5411\u5b66\u4e60\u5230\u7684\u53c2\u6570\u4e2d\u6dfb\u52a0\u566a\u58f0\uff09\u4f1a\u663e\u8457\u5f71\u54cd\u6a21\u578b\u7684\u6548\u7528\uff08\u51c6\u786e\u6027\u548c\u663e\u8457\u6027\uff09\u3002", "method": "\u63d0\u51fa\u4f7f\u7528Credal Networks\uff08CN\uff09\u4f5c\u4e3a\u4e00\u79cd\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u6765\u5e73\u8861\u6a21\u578b\u7684\u9690\u79c1\u548c\u6548\u7528\u3002CN\u662f\u5bf9BN\u7684\u63a9\u7801\uff08masking\uff09\u7248\u672c\uff0c\u800c\u4e0d\u662f\u6dfb\u52a0\u566a\u58f0\u7684\u7248\u672c\u3002\u7814\u7a76\u4e86 \ucd94\uc801 \uacf5\uaca9 (tracing attacks) \u5728CN\u4e0a\u7684\u9002\u7528\u6027\uff0c\u5e76\u8bc1\u660eCN\u53ef\u4ee5\u964d\u4f4e\u653b\u51fb\u7684\u6210\u529f\u7387\u3002\u8bc6\u522b\u51fa\u9700\u8981\u9690\u85cf\u7684\u5173\u952e\u5b66\u4e60\u4fe1\u606f\u4ee5\u9632\u6b62\u653b\u51fb\u8005\u6062\u590d\u5e95\u5c42\u7684BN\u3002\u901a\u8fc7\u6570\u503c\u5b9e\u9a8c\u5206\u6790CN\u8d85\u53c2\u6570\u5bf9\u9690\u79c1\u589e\u76ca\u7684\u8c03\u8282\u4f5c\u7528\u3002", "result": "CN\u80fd\u591f\u63d0\u4f9b\u6709\u610f\u4e49\u7684\u63a8\u7406\uff0c\u540c\u65f6\u4fdd\u62a4\u9690\u79c1\u3002CN\u76f8\u6bd4\u4e8e\u6dfb\u52a0\u566a\u58f0\u7684\u65b9\u6cd5\uff0c\u5728\u9690\u79c1\u4fdd\u62a4\u548c\u6a21\u578b\u6548\u7528\u4e4b\u95f4\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u5e73\u8861\u3002", "conclusion": "CN\u4e3a\u5f00\u53d1\u9690\u79c1\u611f\u77e5\u7684\u6982\u7387\u56fe\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u539f\u5219\u6027\u3001\u5b9e\u7528\u4e14\u6709\u6548\u7684\u65b9\u6cd5\u3002"}}
{"id": "2509.18962", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18962", "abs": "https://arxiv.org/abs/2509.18962", "authors": ["Kirsten K\u00f6bschall", "Sebastian Buschj\u00e4ger", "Raphael Fischer", "Lisa Hartung", "Stefan Kramer"], "title": "Lift What You Can: Green Online Learning with Heterogeneous Ensembles", "comment": null, "summary": "Ensemble methods for stream mining necessitate managing multiple models and\nupdating them as data distributions evolve. Considering the calls for more\nsustainability, established methods are however not sufficiently considerate of\nensemble members' computational expenses and instead overly focus on predictive\ncapabilities. To address these challenges and enable green online learning, we\npropose heterogeneous online ensembles (HEROS). For every training step, HEROS\nchooses a subset of models from a pool of models initialized with diverse\nhyperparameter choices under resource constraints to train. We introduce a\nMarkov decision process to theoretically capture the trade-offs between\npredictive performance and sustainability constraints. Based on this framework,\nwe present different policies for choosing which models to train on incoming\ndata. Most notably, we propose the novel $\\zeta$-policy, which focuses on\ntraining near-optimal models at reduced costs. Using a stochastic model, we\ntheoretically prove that our $\\zeta$-policy achieves near optimal performance\nwhile using fewer resources compared to the best performing policy. In our\nexperiments across 11 benchmark datasets, we find empiric evidence that our\n$\\zeta$-policy is a strong contribution to the state-of-the-art, demonstrating\nhighly accurate performance, in some cases even outperforming competitors, and\nsimultaneously being much more resource-friendly.", "AI": {"tldr": "HEROS\u901a\u8fc7\u9009\u62e9\u8d44\u6e90\u53d7\u9650\u7684\u6a21\u578b\u5b50\u96c6\u8fdb\u884c\u8bad\u7ec3\uff0c\u4ee5\u5728\u9884\u6d4b\u80fd\u529b\u548c\u53ef\u6301\u7eed\u6027\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u03b6-\u7b56\u7565\uff0c\u53ef\u5728\u8f83\u4f4e\u6210\u672c\u4e0b\u5b9e\u73b0\u63a5\u8fd1\u6700\u4f18\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u6d41\u6316\u6398\u96c6\u6210\u65b9\u6cd5\u5728\u7ba1\u7406\u548c\u66f4\u65b0\u6a21\u578b\u4ee5\u9002\u5e94\u6570\u636e\u5206\u5e03\u53d8\u5316\u65f6\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u672a\u80fd\u5145\u5206\u8003\u8651\u53ef\u6301\u7eed\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u7eff\u8272\u7684\u5728\u7ebf\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHEROS\uff08\u5f02\u6784\u5728\u7ebf\u96c6\u6210\uff09\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u8bad\u7ec3\u671f\u95f4\u6839\u636e\u8d44\u6e90\u9650\u5236\u9009\u62e9\u6a21\u578b\u5b50\u96c6\u8fdb\u884c\u8bad\u7ec3\u3002\u4f7f\u7528\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u6765\u6355\u6349\u9884\u6d4b\u6027\u80fd\u548c\u53ef\u6301\u7eed\u6027\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u03b6-\u7b56\u7565\uff0c\u4ee5\u8f83\u4f4e\u7684\u6210\u672c\u8bad\u7ec3\u63a5\u8fd1\u6700\u4f18\u7684\u6a21\u578b\u3002", "result": "\u572811\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cHEROS\u7684\u03b6-\u7b56\u7565\u5728\u51c6\u786e\u6027\u65b9\u9762\u5177\u6709\u9ad8\u5ea6\u7ade\u4e89\u529b\uff0c\u6709\u65f6\u751a\u81f3\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8d44\u6e90\u6d88\u8017\u3002", "conclusion": "HEROS\u53ca\u5176\u03b6-\u7b56\u7565\u4e3a\u5b9e\u73b0\u7eff\u8272\u5728\u7ebf\u5b66\u4e60\u505a\u51fa\u4e86\u91cd\u8981\u8d21\u732e\uff0c\u80fd\u591f\u6709\u6548\u5730\u5e73\u8861\u9884\u6d4b\u6027\u80fd\u548c\u53ef\u6301\u7eed\u6027\uff0c\u4e3a\u6d41\u6316\u6398\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u7ecf\u6d4e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.18964", "categories": ["cs.LG", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.18964", "abs": "https://arxiv.org/abs/2509.18964", "authors": ["Xingtu Liu"], "title": "Central Limit Theorems for Asynchronous Averaged Q-Learning", "comment": null, "summary": "This paper establishes central limit theorems for Polyak-Ruppert averaged\nQ-learning under asynchronous updates. We present a non-asymptotic central\nlimit theorem, where the convergence rate in Wasserstein distance explicitly\nreflects the dependence on the number of iterations, state-action space size,\nthe discount factor, and the quality of exploration. In addition, we derive a\nfunctional central limit theorem, showing that the partial-sum process\nconverges weakly to a Brownian motion.", "AI": {"tldr": "\u672c\u8bba\u6587\u4e3a\u5f02\u6b65\u66f4\u65b0\u4e0b\u7684 Polak-Ruppert \u5e73\u5747 Q-learning \u5efa\u7acb\u4e86\u4e2d\u5fc3\u6781\u9650\u5b9a\u7406\u3002", "motivation": "\u7814\u7a76 Polak-Ruppert \u5e73\u5747 Q-learning \u5728\u5f02\u6b65\u66f4\u65b0\u4e0b\u7684\u7edf\u8ba1\u7279\u6027\uff0c\u7279\u522b\u662f\u5176\u6536\u655b\u6027\u548c\u7edf\u8ba1\u5206\u5e03\u3002", "method": "\u63a8\u5bfc\u4e86\u5173\u4e8e Polak-Ruppert \u5e73\u5747 Q-learning \u7684\u975e\u6e10\u8fd1\u4e2d\u5fc3\u6781\u9650\u5b9a\u7406\u548c\u6cdb\u51fd\u4e2d\u5fc3\u6781\u9650\u5b9a\u7406\u3002", "result": "\u5f97\u5230\u4e86\u4e00\u4e2a\u6536\u655b\u7387\u660e\u786e\u53cd\u6620\u8fed\u4ee3\u6b21\u6570\u3001\u72b6\u6001-\u52a8\u4f5c\u7a7a\u95f4\u5927\u5c0f\u3001\u6298\u6263\u56e0\u5b50\u548c\u63a2\u7d22\u8d28\u91cf\u7684 Wasserstein \u8ddd\u79bb\u7684\u975e\u6e10\u8fd1\u4e2d\u5fc3\u6781\u9650\u5b9a\u7406\u3002\u6b64\u5916\uff0c\u8fd8\u63a8\u5bfc\u4e86\u4e00\u4e2a\u6cdb\u51fd\u4e2d\u5fc3\u6781\u9650\u5b9a\u7406\uff0c\u8868\u660e\u90e8\u5206\u548c\u8fc7\u7a0b\u5f31\u6536\u655b\u4e8e\u5e03\u6717\u8fd0\u52a8\u3002", "conclusion": "\u8bc1\u660e\u4e86\u5728\u5f02\u6b65\u66f4\u65b0\u4e0b\uff0cPolak-Ruppert \u5e73\u5747 Q-learning \u5177\u6709\u826f\u597d\u7684\u7edf\u8ba1\u7279\u6027\uff0c\u5e76\u4e14\u5176\u6536\u655b\u8fc7\u7a0b\u53ef\u4ee5\u7528\u4e2d\u5fc3\u6781\u9650\u5b9a\u7406\u6765\u63cf\u8ff0\uff0c\u751a\u81f3\u53ef\u4ee5\u6536\u655b\u5230\u5e03\u6717\u8fd0\u52a8\u3002"}}
{"id": "2509.19082", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19082", "abs": "https://arxiv.org/abs/2509.19082", "authors": ["Alexey Nekrasov", "Ali Athar", "Daan de Geus", "Alexander Hermans", "Bastian Leibe"], "title": "3rd Place Report of LSVOS 2025 MeViS Track: Sa2VA-i: Improving Sa2VA Results with Consistent Training and Inference", "comment": null, "summary": "Sa2VA is a recent model for language-guided dense grounding in images and\nvideo that achieves state-of-the-art results on multiple segmentation\nbenchmarks and that has become widely popular. However, we found that Sa2VA\ndoes not perform according to its full potential for referring video object\nsegmentation tasks. We identify inconsistencies between training and inference\nprocedures as the key factor holding it back. To mitigate this issue, we\npropose an improved version of Sa2VA, Sa2VA-i, that rectifies these issues and\nimproves the results. In fact, Sa2VA-i sets a new state of the art for multiple\nvideo benchmarks and achieves improvements of up to +11.6 J&F on MeViS, +1.4 on\nRef-YT-VOS, +3.3 on Ref-DAVIS and +4.1 on ReVOS using the same Sa2VA\ncheckpoints. With our fixes, the Sa2VA-i-1B model even performs on par with the\noriginal Sa2VA-26B model on the MeViS benchmark. We hope that this work will\nshow the importance of seemingly trivial implementation details and that it\nwill provide valuable insights for the referring video segmentation field. We\nprovide the code and updated models at https://github.com/kumuji/sa2va-i", "AI": {"tldr": "Sa2VA-i\u901a\u8fc7\u89e3\u51b3Sa2VA\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u6539\u8fdb\u4e86\u5176\u5728\u89c6\u9891\u5bf9\u8c61\u5206\u5272\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6210\u679c\u3002", "motivation": "Sa2VA\u6a21\u578b\u5728\u8bed\u8a00\u5f15\u5bfc\u7684\u5bc6\u96c6\u56fe\u50cf\u548c\u89c6\u9891\u57fa\u7840\u65b9\u9762\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6210\u679c\uff0c\u4f46\u5728\u6307\u4ee3\u89c6\u9891\u5bf9\u8c61\u5206\u5272\u4efb\u52a1\u4e0a\u672a\u80fd\u53d1\u6325\u5176\u5168\u90e8\u6f5c\u529b\u3002", "method": "\u901a\u8fc7\u8bc6\u522b\u5e76\u89e3\u51b3Sa2VA\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u6539\u8fdb\u7248\u672cSa2VA-i\u3002", "result": "Sa2VA-i\u5728MeViS\u3001Ref-YT-VOS\u3001Ref-DAVIS\u548cReVOS\u7b49\u591a\u4e2a\u89c6\u9891\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u6700\u9ad8\u53ef\u8fbe+11.6 J&F\u3002Sa2VA-i-1B\u6a21\u578b\u5728MeViS\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u8868\u73b0\u4e0e\u539f\u59cbSa2VA-26B\u6a21\u578b\u76f8\u5f53\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5f3a\u8c03\u4e86\u770b\u4f3c\u5fae\u5c0f\u7684\u5b9e\u73b0\u7ec6\u8282\u7684\u91cd\u8981\u6027\uff0c\u5e76\u4e3a\u6307\u4ee3\u89c6\u9891\u5206\u5272\u9886\u57df\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002"}}
{"id": "2509.18968", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18968", "abs": "https://arxiv.org/abs/2509.18968", "authors": ["Zhanglu Yan", "Jiayi Mao", "Qianhui Liu", "Fanfan Li", "Gang Pan", "Tao Luo", "Bowen Zhu", "Weng-Fai Wong"], "title": "Otters: An Energy-Efficient SpikingTransformer via Optical Time-to-First-Spike Encoding", "comment": null, "summary": "Spiking neural networks (SNNs) promise high energy efficiency, particularly\nwith time-to-first-spike (TTFS) encoding, which maximizes sparsity by emitting\nat most one spike per neuron. However, such energy advantage is often\nunrealized because inference requires evaluating a temporal decay function and\nsubsequent multiplication with the synaptic weights. This paper challenges this\ncostly approach by repurposing a physical hardware `bug', namely, the natural\nsignal decay in optoelectronic devices, as the core computation of TTFS. We\nfabricated a custom indium oxide optoelectronic synapse, showing how its\nnatural physical decay directly implements the required temporal function. By\ntreating the device's analog output as the fused product of the synaptic weight\nand temporal decay, optoelectronic synaptic TTFS (named Otters) eliminates\nthese expensive digital operations. To use the Otters paradigm in complex\narchitectures like the transformer, which are challenging to train directly due\nto the sparsity issue, we introduce a novel quantized neural network-to-SNN\nconversion algorithm. This complete hardware-software co-design enables our\nmodel to achieve state-of-the-art accuracy across seven GLUE benchmark datasets\nand demonstrates a 1.77$\\times$ improvement in energy efficiency over previous\nleading SNNs, based on a comprehensive analysis of compute, data movement, and\nmemory access costs using energy measurements from a commercial 22nm process.\nOur work thus establishes a new paradigm for energy-efficient SNNs, translating\nfundamental device physics directly into powerful computational primitives. All\ncodes and data are open source.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aOtters\u7684\u65b0\u578b\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08SNN\uff09\u786c\u4ef6-\u8f6f\u4ef6\u534f\u540c\u8bbe\u8ba1\uff0c\u5229\u7528\u5149\u7535\u5668\u4ef6\u7684\u81ea\u7136\u4fe1\u53f7\u8870\u51cf\u6765\u4f18\u5316\u65f6\u95f4-\u9996\u6b21\u8109\u51b2\uff08TTFS\uff09\u7f16\u7801\uff0c\u4ece\u800c\u5b9e\u73b0\u9ad8\u80fd\u6548\u3002", "motivation": "\u73b0\u6709\u7684SNN\u5728\u5b9e\u73b0\u9ad8\u80fd\u6548\u65b9\u9762\u9762\u4e34\u6311\u6218\uff0c\u56e0\u4e3a\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08SNN\uff09\u4e2d\u7684\u65f6\u95f4\u8870\u51cf\u51fd\u6570\u548c\u7a81\u89e6\u6743\u91cd\u4e58\u6cd5\u7b49\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u3002", "method": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aOtters\u7684\u65b0\u578b\u786c\u4ef6-\u8f6f\u4ef6\u534f\u540c\u8bbe\u8ba1\uff0c\u5229\u7528\u5149\u7535\u5668\u4ef6\uff08\u7279\u522b\u662f\u6c27\u5316\u94df\u5149\u7535\u7a81\u89e6\uff09\u7684\u81ea\u7136\u4fe1\u53f7\u8870\u51cf\u6765\u76f4\u63a5\u5b9e\u73b0TTFS\u7f16\u7801\u6240\u9700\u7684\u65f6\u95f4\u8870\u51cf\u51fd\u6570\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u91cf\u5316\u795e\u7ecf\u7f51\u7edc\u5230SNN\u7684\u8f6c\u6362\u7b97\u6cd5\uff0c\u4ee5\u89e3\u51b3\u590d\u6742\u6a21\u578b\uff08\u5982Transformer\uff09\u7684\u8bad\u7ec3\u7a00\u758f\u6027\u95ee\u9898\u3002", "result": "Otters\u5728\u4e03\u4e2aGLUE\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u51c6\u786e\u6027\uff0c\u5e76\u4e14\u5728\u80fd\u6548\u65b9\u9762\u6bd4\u4ee5\u524d\u7684SNN\u9886\u5148\u6a21\u578b\u63d0\u9ad8\u4e861.77\u500d\u3002\u8be5\u7ed3\u679c\u662f\u901a\u8fc7\u5728\u5546\u752822nm\u5de5\u827a\u4e0a\u8fdb\u884c\u80fd\u8017\u6d4b\u91cf\u548c\u5168\u9762\u7684\u8ba1\u7b97\u3001\u6570\u636e\u79fb\u52a8\u548c\u5185\u5b58\u8bbf\u95ee\u6210\u672c\u5206\u6790\u5f97\u51fa\u7684\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u5c06\u5668\u4ef6\u7684\u7269\u7406\u7279\u6027\u76f4\u63a5\u8f6c\u5316\u4e3a\u8ba1\u7b97\u539f\u8bed\uff0c\u4e3a\u9ad8\u80fd\u6548SNN\u5f00\u521b\u4e86\u65b0\u8303\u5f0f\uff0c\u4ece\u800c\u663e\u8457\u63d0\u9ad8\u4e86\u6027\u80fd\u548c\u80fd\u6548\u3002"}}
{"id": "2509.19087", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19087", "abs": "https://arxiv.org/abs/2509.19087", "authors": ["Ganesh Mallya", "Yotam Gigi", "Dahun Kim", "Maxim Neumann", "Genady Beryozkin", "Tomer Shekel", "Anelia Angelova"], "title": "Zero-Shot Multi-Spectral Learning: Reimagining a Generalist Multimodal Gemini 2.5 Model for Remote Sensing Applications", "comment": null, "summary": "Multi-spectral imagery plays a crucial role in diverse Remote Sensing\napplications including land-use classification, environmental monitoring and\nurban planning. These images are widely adopted because their additional\nspectral bands correlate strongly with physical materials on the ground, such\nas ice, water, and vegetation. This allows for more accurate identification,\nand their public availability from missions, such as Sentinel-2 and Landsat,\nonly adds to their value. Currently, the automatic analysis of such data is\npredominantly managed through machine learning models specifically trained for\nmulti-spectral input, which are costly to train and support. Furthermore,\nalthough providing a lot of utility for Remote Sensing, such additional inputs\ncannot be used with powerful generalist large multimodal models, which are\ncapable of solving many visual problems, but are not able to understand\nspecialized multi-spectral signals.\n  To address this, we propose a training-free approach which introduces new\nmulti-spectral data in a Zero-Shot-only mode, as inputs to generalist\nmultimodal models, trained on RGB-only inputs. Our approach leverages the\nmultimodal models' understanding of the visual space, and proposes to adapt to\ninputs to that space, and to inject domain-specific information as instructions\ninto the model. We exemplify this idea with the Gemini2.5 model and observe\nstrong Zero-Shot performance gains of the approach on popular Remote Sensing\nbenchmarks for land cover and land use classification and demonstrate the easy\nadaptability of Gemini2.5 to new inputs. These results highlight the potential\nfor geospatial professionals, working with non-standard specialized inputs, to\neasily leverage powerful multimodal models, such as Gemini2.5, to accelerate\ntheir work, benefiting from their rich reasoning and contextual capabilities,\ngrounded in the specialized sensor data.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u8bad\u7ec3\u65e0\u5173\u7684\u65b9\u6cd5\uff0c\u5c06\u591a\u5149\u8c31\u6570\u636e\u4f5c\u4e3a\u96f6\u6837\u672c\u8f93\u5165\uff0c\u7528\u4e8e\u901a\u7528\u7684\u591a\u6a21\u6001\u6a21\u578b\uff0c\u4ee5\u89e3\u51b3\u9065\u611f\u9886\u57df\u4e2d\u591a\u5149\u8c31\u6570\u636e\u5206\u6790\u7684\u6311\u6218\u3002", "motivation": "\u76ee\u524d\uff0c\u591a\u5149\u8c31\u6570\u636e\u7684\u81ea\u52a8\u5206\u6790\u4e3b\u8981\u4f9d\u8d56\u4e8e\u4e13\u95e8\u4e3a\u591a\u5149\u8c31\u8f93\u5165\u8bad\u7ec3\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u8fd9\u4e9b\u6a21\u578b\u7684\u8bad\u7ec3\u548c\u652f\u6301\u6210\u672c\u9ad8\u6602\u3002\u6b64\u5916\uff0c\u5f3a\u5927\u7684\u901a\u7528\u591a\u6a21\u6001\u5927\u6a21\u578b\u65e0\u6cd5\u7406\u89e3\u4e13\u4e1a\u7684\u591a\u5149\u8c31\u4fe1\u53f7\uff0c\u9650\u5236\u4e86\u5176\u5728\u9065\u611f\u9886\u57df\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8bad\u7ec3\u65e0\u5173\u7684\u65b9\u6cd5\uff0c\u5c06\u591a\u5149\u8c31\u6570\u636e\u4ee5\u4ec5\u96f6\u6837\u672c\u6a21\u5f0f\u4f5c\u4e3a\u8f93\u5165\uff0c\u7528\u4e8e\u5728\u4ec5RGB\u8f93\u5165\u4e0a\u8bad\u7ec3\u7684\u901a\u7528\u591a\u6a21\u6001\u6a21\u578b\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u591a\u6a21\u6001\u6a21\u578b\u5bf9\u89c6\u89c9\u7a7a\u95f4\u7684\u7406\u89e3\uff0c\u5e76\u5c06\u9886\u57df\u7279\u5b9a\u7684\u4fe1\u606f\u4f5c\u4e3a\u6307\u4ee4\u6ce8\u5165\u6a21\u578b\u3002", "result": "\u5728\u9646\u5730\u8986\u76d6\u548c\u571f\u5730\u5229\u7528\u5206\u7c7b\u7b49\u6d41\u884c\u7684\u9065\u611f\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4f7f\u7528Gemini2.5\u6a21\u578b\u5728\u96f6\u6837\u672c\u6027\u80fd\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u63d0\u5347\uff0c\u5e76\u5c55\u793a\u4e86Gemini2.5\u6a21\u578b\u8f7b\u677e\u9002\u5e94\u65b0\u8f93\u5165\u7684\u7279\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u5904\u7406\u975e\u6807\u51c6\u4e13\u4e1a\u8f93\u5165\u7684\u5730\u7406\u7a7a\u95f4\u4e13\u4e1a\u4eba\u5458\uff0c\u80fd\u591f\u8f7b\u677e\u5229\u7528Gemini2.5\u7b49\u5f3a\u5927\u7684\u591a\u6a21\u6001\u6a21\u578b\u6765\u52a0\u901f\u5de5\u4f5c\uff0c\u5e76\u53d7\u76ca\u4e8e\u5176\u4e30\u5bcc\u7684\u63a8\u7406\u548c\u57fa\u4e8e\u4e13\u4e1a\u4f20\u611f\u5668\u6570\u636e\u7684\u4e0a\u4e0b\u6587\u80fd\u529b\u3002"}}
{"id": "2509.18990", "categories": ["cs.LG", "math.DS"], "pdf": "https://arxiv.org/pdf/2509.18990", "abs": "https://arxiv.org/abs/2509.18990", "authors": ["Carson Dudley", "Marisa Eisenberg"], "title": "Learning From Simulators: A Theory of Simulation-Grounded Learning", "comment": null, "summary": "Simulation-Grounded Neural Networks (SGNNs) are predictive models trained\nentirely on synthetic data from mechanistic simulations. They have achieved\nstate-of-the-art performance in domains where real-world labels are limited or\nunobserved, but lack a formal underpinning.\n  We present the foundational theory of simulation-grounded learning. We show\nthat SGNNs implement amortized Bayesian inference under a simulation prior and\nconverge to the Bayes-optimal predictor. We derive generalization bounds under\nmodel misspecification and prove that SGNNs can learn unobservable scientific\nquantities that empirical methods provably cannot. We also formalize a novel\nform of mechanistic interpretability uniquely enabled by SGNNs: by attributing\npredictions to the simulated mechanisms that generated them, SGNNs yield\nposterior-consistent, scientifically grounded explanations.\n  We provide numerical experiments to validate all theoretical predictions.\nSGNNs recover latent parameters, remain robust under mismatch, and outperform\nclassical tools: in a model selection task, SGNNs achieve half the error of AIC\nin distinguishing mechanistic dynamics. These results establish SGNNs as a\nprincipled and practical framework for scientific prediction in data-limited\nregimes.", "AI": {"tldr": "SGNNs \u662f\u5b8c\u5168\u5728\u6a21\u62df\u6570\u636e\u4e0a\u8bad\u7ec3\u7684\u9884\u6d4b\u6a21\u578b\uff0c\u5177\u6709\u7406\u8bba\u57fa\u7840\uff0c\u5e76\u80fd\u5728\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u79d1\u5b66\u9884\u6d4b\u3002", "motivation": "SGNNs \u5728\u73b0\u5b9e\u4e16\u754c\u6807\u7b7e\u6709\u9650\u6216\u672a\u88ab\u89c2\u5bdf\u5230\u7684\u9886\u57df\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4f46\u7f3a\u4e4f\u6b63\u5f0f\u7684\u57fa\u7840\u3002\u672c\u6587\u65e8\u5728\u4e3a\u6a21\u62df\u57fa\u7840\u5b66\u4e60\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u6a21\u62df\u57fa\u7840\u5b66\u4e60\u7684\u7406\u8bba\u57fa\u7840\uff0c\u8bc1\u660e SGNNs \u5b9e\u73b0\u4e86\u4e00\u4e2a\u6a21\u62df\u5148\u9a8c\u4e0b\u7684\u644a\u9500\u8d1d\u53f6\u65af\u63a8\u65ad\uff0c\u5e76\u6536\u655b\u5230\u8d1d\u53f6\u65af\u6700\u4f18\u9884\u6d4b\u5668\u3002\u63a8\u5bfc\u4e86\u6a21\u578b\u8bef\u8bbe\u4e0b\u7684\u6cdb\u5316\u754c\u9650\uff0c\u5e76\u8bc1\u660e\u4e86 SGNNs \u53ef\u4ee5\u5b66\u4e60\u7ecf\u9a8c\u65b9\u6cd5\u65e0\u6cd5\u5b66\u4e60\u7684\u4e0d\u53ef\u89c2\u5bdf\u7684\u79d1\u5b66\u91cf\u3002\u6b64\u5916\uff0c\u8fd8\u5f62\u5f0f\u5316\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u673a\u68b0\u53ef\u89e3\u91ca\u6027\uff0c\u901a\u8fc7\u5c06\u9884\u6d4b\u5f52\u56e0\u4e8e\u751f\u6210\u5b83\u4eec\u7684\u6a21\u62df\u673a\u5236\uff0c\u4ece\u800c\u83b7\u5f97\u540e\u9a8c\u4e00\u81f4\u7684\u3001\u6709\u79d1\u5b66\u4f9d\u636e\u7684\u89e3\u91ca\u3002", "result": "\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u6709\u7406\u8bba\u9884\u6d4b\u3002SGNNs \u6062\u590d\u4e86\u6f5c\u5728\u53c2\u6570\uff0c\u5728\u4e0d\u5339\u914d\u7684\u60c5\u51b5\u4e0b\u4fdd\u6301\u9c81\u68d2\u6027\uff0c\u5e76\u4e14\u4f18\u4e8e\u7ecf\u5178\u5de5\u5177\uff1a\u5728\u6a21\u578b\u9009\u62e9\u4efb\u52a1\u4e2d\uff0cSGNNs \u5728\u533a\u5206\u673a\u68b0\u52a8\u529b\u5b66\u65b9\u9762\u7684\u8bef\u5dee\u662f AIC \u7684\u4e00\u534a\u3002", "conclusion": "SGNNs \u5efa\u7acb\u4e86\u4e00\u4e2a\u6709\u539f\u5219\u4e14\u5b9e\u7528\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u79d1\u5b66\u9884\u6d4b\u3002"}}
{"id": "2509.18993", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18993", "abs": "https://arxiv.org/abs/2509.18993", "authors": ["Boao Kong", "Junzhu Liang", "Yuxi Liu", "Renjia Deng", "Kun Yuan"], "title": "CR-Net: Scaling Parameter-Efficient Training with Cross-Layer Low-Rank Structure", "comment": "32 pages", "summary": "Low-rank architectures have become increasingly important for efficient large\nlanguage model (LLM) pre-training, providing substantial reductions in both\nparameter complexity and memory/computational demands. Despite these\nadvantages, current low-rank methods face three critical shortcomings: (1)\ncompromised model performance, (2) considerable computational overhead, and (3)\nlimited activation memory savings. To address these limitations, we propose\nCross-layer Low-Rank residual Network (CR-Net), an innovative\nparameter-efficient framework inspired by our discovery that inter-layer\nactivation residuals possess low-rank properties. CR-Net implements this\ninsight through a dual-path architecture that efficiently reconstructs layer\nactivations by combining previous-layer outputs with their low-rank\ndifferences, thereby maintaining high-rank information with minimal parameters.\nWe further develop a specialized activation recomputation strategy tailored for\nCR-Net that dramatically reduces memory requirements. Extensive pre-training\nexperiments across model scales from 60M to 7B parameters demonstrate that\nCR-Net consistently outperforms state-of-the-art low-rank frameworks while\nrequiring fewer computational resources and less memory.", "AI": {"tldr": "CR-Net\u662f\u4e00\u79cd\u65b0\u7684\u53c2\u6570\u9ad8\u6548\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u8de8\u5c42\u6fc0\u6d3b\u6b8b\u5dee\u7684\u4f4e\u79e9\u7279\u6027\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\uff0c\u51cf\u5c11\u4e86\u53c2\u6570\u590d\u6742\u6027\u3001\u8ba1\u7b97\u5f00\u9500\u548c\u5185\u5b58\u9700\u6c42\uff0c\u5e76\u5728\u5404\u79cd\u6a21\u578b\u89c4\u6a21\u7684\u5b9e\u9a8c\u4e2d\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u4f4e\u79e9\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u4f4e\u79e9\u65b9\u6cd5\u5728\u6a21\u578b\u6027\u80fd\u3001\u8ba1\u7b97\u5f00\u9500\u548c\u6fc0\u6d3b\u5185\u5b58\u8282\u7701\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u4f18\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faCR-Net\u6846\u67b6\uff0c\u91c7\u7528\u5bf9\u5076\u8def\u5f84\u67b6\u6784\uff0c\u7ed3\u5408\u524d\u4e00\u5c42\u8f93\u51fa\u53ca\u5176\u4f4e\u79e9\u5dee\u503c\u6765\u91cd\u6784\u5c42\u6fc0\u6d3b\uff0c\u5e76\u5f00\u53d1\u4e86\u4e13\u95e8\u7684\u6fc0\u6d3b\u91cd\u8ba1\u7b97\u7b56\u7565\u4ee5\u964d\u4f4e\u5185\u5b58\u9700\u6c42\u3002", "result": "\u5728\u4ece60M\u52307B\u53c2\u6570\u7684\u6a21\u578b\u89c4\u6a21\u7684\u9884\u8bad\u7ec3\u5b9e\u9a8c\u4e2d\uff0cCR-Net\u5728\u6027\u80fd\u4e0a\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u4f4e\u79e9\u6846\u67b6\uff0c\u540c\u65f6\u9700\u8981\u66f4\u5c11\u7684\u8ba1\u7b97\u8d44\u6e90\u548c\u5185\u5b58\u3002", "conclusion": "CR-Net\u901a\u8fc7\u5229\u7528\u8de8\u5c42\u6fc0\u6d3b\u6b8b\u5dee\u7684\u4f4e\u79e9\u7279\u6027\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709\u4f4e\u79e9\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u6548\u7387\u548c\u6027\u80fd\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6539\u8fdb\u3002"}}
{"id": "2509.19096", "categories": ["cs.CV", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.19096", "abs": "https://arxiv.org/abs/2509.19096", "authors": ["Ilhan Skender", "Kailin Tong", "Selim Solmaz", "Daniel Watzenig"], "title": "Investigating Traffic Accident Detection Using Multimodal Large Language Models", "comment": "Accepted for presentation at the 2025 IEEE International Automated\n  Vehicle Validation Conference (IAVVC 2025). Final version to appear in IEEE\n  Xplore", "summary": "Traffic safety remains a critical global concern, with timely and accurate\naccident detection essential for hazard reduction and rapid emergency response.\nInfrastructure-based vision sensors offer scalable and efficient solutions for\ncontinuous real-time monitoring, facilitating automated detection of acci-\ndents directly from captured images. This research investigates the zero-shot\ncapabilities of multimodal large language models (MLLMs) for detecting and\ndescribing traffic accidents using images from infrastructure cameras, thus\nminimizing reliance on extensive labeled datasets. Main contributions include:\n(1) Evaluation of MLLMs using the simulated DeepAccident dataset from CARLA,\nexplicitly addressing the scarcity of diverse, realistic, infrastructure-based\naccident data through controlled simulations; (2) Comparative performance\nanalysis between Gemini 1.5 and 2.0, Gemma 3 and Pixtral models in acci- dent\nidentification and descriptive capabilities without prior fine-tuning; and (3)\nIntegration of advanced visual analytics, specifically YOLO for object\ndetection, Deep SORT for multi- object tracking, and Segment Anything (SAM) for\ninstance segmentation, into enhanced prompts to improve model accuracy and\nexplainability. Key numerical results show Pixtral as the top performer with an\nF1-score of 0.71 and 83% recall, while Gemini models gained precision with\nenhanced prompts (e.g., Gemini 1.5 rose to 90%) but suffered notable F1 and\nrecall losses. Gemma 3 offered the most balanced performance with minimal\nmetric fluctuation. These findings demonstrate the substantial potential of\nintegrating MLLMs with advanced visual analytics techniques, enhancing their\napplicability in real-world automated traffic monitoring systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u4ea4\u901a\u6444\u50cf\u5934\u56fe\u50cf\u4e2d\u96f6\u6837\u672c\u68c0\u6d4b\u548c\u63cf\u8ff0\u4ea4\u901a\u4e8b\u6545\u7684\u80fd\u529b\uff0c\u5e76\u7ed3\u5408\u4e86YOLO\u3001Deep SORT\u548cSAM\u7b49\u89c6\u89c9\u5206\u6790\u6280\u672f\u6765\u63d0\u9ad8\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u4ea4\u901a\u5b89\u5168\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u53ca\u65f6\u51c6\u786e\u7684\u4e8b\u6545\u68c0\u6d4b\u4ee5\u51cf\u5c11\u5371\u5bb3\u548c\u52a0\u5feb\u5e94\u6025\u54cd\u5e94\u3002\u57fa\u4e8e\u57fa\u7840\u8bbe\u65bd\u7684\u89c6\u89c9\u4f20\u611f\u5668\u80fd\u591f\u63d0\u4f9b\u53ef\u6269\u5c55\u3001\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7528\u4e8e\u6301\u7eed\u7684\u5b9e\u65f6\u76d1\u63a7\u548c\u81ea\u52a8\u5316\u7684\u4e8b\u6545\u68c0\u6d4b\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u96f6\u6837\u672c\u80fd\u529b\uff0c\u4ee5\u5e94\u5bf9\u771f\u5b9e\u4e16\u754c\u4e2d\u4ea4\u901a\u6444\u50cf\u5934\u6570\u636e\u7f3a\u4e4f\u591a\u6837\u6027\u548c\u6807\u6ce8\u7684\u6311\u6218\u3002", "method": "\u7814\u7a76\u4eba\u5458\u5728CARLA\u6a21\u62df\u7684DeepAccident\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e86MLLMs\u5728\u4ea4\u901a\u4e8b\u6545\u68c0\u6d4b\u548c\u63cf\u8ff0\u65b9\u9762\u7684\u96f6\u6837\u672c\u80fd\u529b\u3002\u4ed6\u4eec\u6bd4\u8f83\u4e86Gemini 1.5\u3001Gemini 2.0\u3001Gemma 3\u548cPixtral\u8fd9\u51e0\u4e2a\u6a21\u578b\u5728\u6ca1\u6709\u8fdb\u884c\u4efb\u4f55\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u4e8b\u6545\u8bc6\u522b\u548c\u63cf\u8ff0\u7684\u8868\u73b0\u3002\u6b64\u5916\uff0c\u4ed6\u4eec\u8fd8\u7ed3\u5408\u4e86YOLO\uff08\u5bf9\u8c61\u68c0\u6d4b\uff09\u3001Deep SORT\uff08\u591a\u5bf9\u8c61\u8ddf\u8e2a\uff09\u548cSAM\uff08\u5b9e\u4f8b\u5206\u5272\uff09\u7b49\u5148\u8fdb\u7684\u89c6\u89c9\u5206\u6790\u6280\u672f\uff0c\u901a\u8fc7\u589e\u5f3a\u63d0\u793a\u6765\u63d0\u9ad8\u6a21\u578b\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\uff0cPixtral\u8868\u73b0\u6700\u4f73\uff0cF1\u5206\u6570\u8fbe\u52300.71\uff0c\u53ec\u56de\u7387\u4e3a83%\u3002\u901a\u8fc7\u589e\u5f3a\u63d0\u793a\uff0cGemini\u6a21\u578b\u5728\u7cbe\u786e\u5ea6\u65b9\u9762\u6709\u6240\u63d0\u9ad8\uff08\u4f8b\u5982Gemini 1.5\u7cbe\u786e\u5ea6\u8fbe\u523090%\uff09\uff0c\u4f46F1\u5206\u6570\u548c\u53ec\u56de\u7387\u5374\u663e\u8457\u4e0b\u964d\u3002Gemma 3\u5728\u5404\u9879\u6307\u6807\u4e0a\u8868\u73b0\u5747\u8861\uff0c\u6ce2\u52a8\u6700\u5c0f\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5c06MLLMs\u4e0e\u5148\u8fdb\u7684\u89c6\u89c9\u5206\u6790\u6280\u672f\uff08\u5982YOLO\u3001Deep SORT\u548cSAM\uff09\u76f8\u7ed3\u5408\uff0c\u5728\u63d0\u5347\u4ea4\u901a\u6444\u50cf\u5934\u56fe\u50cf\u4e8b\u6545\u68c0\u6d4b\u548c\u63cf\u8ff0\u7684\u51c6\u786e\u6027\u3001\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u771f\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u7684\u6570\u636e\u7a00\u758f\u6027\u548c\u591a\u6837\u6027\u6311\u6218\u3002"}}
{"id": "2509.18997", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18997", "abs": "https://arxiv.org/abs/2509.18997", "authors": ["Pascal Esser", "Maximilian Fleissner", "Debarghya Ghoshdastidar"], "title": "Theoretical Foundations of Representation Learning using Unlabeled Data: Statistics and Optimization", "comment": null, "summary": "Representation learning from unlabeled data has been extensively studied in\nstatistics, data science and signal processing with a rich literature on\ntechniques for dimension reduction, compression, multi-dimensional scaling\namong others. However, current deep learning models use new principles for\nunsupervised representation learning that cannot be easily analyzed using\nclassical theories. For example, visual foundation models have found tremendous\nsuccess using self-supervision or denoising/masked autoencoders, which\neffectively learn representations from massive amounts of unlabeled data.\nHowever, it remains difficult to characterize the representations learned by\nthese models and to explain why they perform well for diverse prediction tasks\nor show emergent behavior. To answer these questions, one needs to combine\nmathematical tools from statistics and optimization. This paper provides an\noverview of recent theoretical advances in representation learning from\nunlabeled data and mentions our contributions in this direction.", "AI": {"tldr": "\u6df1\u5ea6\u5b66\u4e60\u5728\u65e0\u76d1\u7763\u8868\u5f81\u5b66\u4e60\u65b9\u9762\u53d6\u5f97\u4e86\u5de8\u5927\u6210\u529f\uff0c\u4f46\u5176\u80cc\u540e\u7684\u7406\u8bba\u57fa\u7840\u5c1a\u4e0d\u660e\u786e\u3002\u672c\u6587\u6982\u8ff0\u4e86\u8be5\u9886\u57df\u7684\u6700\u65b0\u7406\u8bba\u8fdb\u5c55\uff0c\u5e76\u7ed3\u5408\u4e86\u7edf\u8ba1\u5b66\u548c\u4f18\u5316\u9886\u57df\u7684\u6570\u5b66\u5de5\u5177\u3002", "motivation": "\u5f53\u524d\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u65e0\u76d1\u7763\u8868\u5f81\u5b66\u4e60\u65b9\u9762\u91c7\u7528\u4e86\u65b0\u7684\u539f\u7406\uff0c\u8fd9\u4e9b\u539f\u7406\u96be\u4ee5\u7528\u7ecf\u5178\u7684\u7edf\u8ba1\u5b66\u7406\u8bba\u8fdb\u884c\u5206\u6790\u3002\u5c24\u5176\u662f\u5728\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u9886\u57df\uff0c\u5c3d\u7ba1\u81ea\u76d1\u7763\u5b66\u4e60\u548c\u63a9\u7801\u81ea\u7f16\u7801\u5668\u7b49\u6280\u672f\u53d6\u5f97\u4e86\u5de8\u5927\u6210\u529f\uff0c\u4f46\u6211\u4eec\u4ecd\u7136\u96be\u4ee5\u51c6\u786e\u63cf\u8ff0\u5176\u5b66\u5230\u7684\u8868\u5f81\uff0c\u4e5f\u65e0\u6cd5\u89e3\u91ca\u5b83\u4eec\u4e3a\u4f55\u80fd\u9002\u7528\u4e8e\u5404\u79cd\u9884\u6d4b\u4efb\u52a1\u6216\u8868\u73b0\u51fa\u6d8c\u73b0\u884c\u4e3a\u3002", "method": "\u672c\u6587\u65e8\u5728\u7ed3\u5408\u7edf\u8ba1\u5b66\u548c\u4f18\u5316\u9886\u57df\u7684\u6570\u5b66\u5de5\u5177\uff0c\u4e3a\u65e0\u76d1\u7763\u8868\u5f81\u5b66\u4e60\u63d0\u4f9b\u7406\u8bba\u5206\u6790\u57fa\u7840\u3002\u8bba\u6587\u5c06\u6982\u8ff0\u8be5\u9886\u57df\u7684\u6700\u65b0\u7406\u8bba\u8fdb\u5c55\uff0c\u5e76\u4ecb\u7ecd\u4f5c\u8005\u5728\u8be5\u65b9\u5411\u4e0a\u7684\u8d21\u732e\u3002", "result": "\u672c\u6587\u5bf9\u65e0\u76d1\u7763\u8868\u5f81\u5b66\u4e60\u9886\u57df\u7684\u6700\u65b0\u7406\u8bba\u8fdb\u5c55\u8fdb\u884c\u4e86\u6982\u8ff0\u3002", "conclusion": "\u9700\u8981\u7ed3\u5408\u7edf\u8ba1\u5b66\u548c\u4f18\u5316\u9886\u57df\u7684\u6570\u5b66\u5de5\u5177\u6765\u6df1\u5165\u7406\u89e3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u65e0\u76d1\u7763\u8868\u5f81\u5b66\u4e60\u65b9\u9762\u7684\u8868\u73b0\u3002"}}
{"id": "2509.19115", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19115", "abs": "https://arxiv.org/abs/2509.19115", "authors": ["G\u00f6rkay Aydemir", "Weidi Xie", "Fatma G\u00fcney"], "title": "Track-On2: Enhancing Online Point Tracking with Memory", "comment": null, "summary": "In this paper, we consider the problem of long-term point tracking, which\nrequires consistent identification of points across video frames under\nsignificant appearance changes, motion, and occlusion. We target the online\nsetting, i.e. tracking points frame-by-frame, making it suitable for real-time\nand streaming applications. We extend our prior model Track-On into Track-On2,\na simple and efficient transformer-based model for online long-term tracking.\nTrack-On2 improves both performance and efficiency through architectural\nrefinements, more effective use of memory, and improved synthetic training\nstrategies. Unlike prior approaches that rely on full-sequence access or\niterative updates, our model processes frames causally and maintains temporal\ncoherence via a memory mechanism, which is key to handling drift and occlusions\nwithout requiring future frames. At inference, we perform coarse patch-level\nclassification followed by refinement. Beyond architecture, we systematically\nstudy synthetic training setups and their impact on memory behavior, showing\nhow they shape temporal robustness over long sequences. Through comprehensive\nexperiments, Track-On2 achieves state-of-the-art results across five synthetic\nand real-world benchmarks, surpassing prior online trackers and even strong\noffline methods that exploit bidirectional context. These results highlight the\neffectiveness of causal, memory-based architectures trained purely on synthetic\ndata as scalable solutions for real-world point tracking. Project page:\nhttps://kuis-ai.github.io/track_on2", "AI": {"tldr": "Track-On2\u662f\u4e00\u4e2a\u57fa\u4e8eTransformer\u7684\u5728\u7ebf\u957f\u65f6\u70b9\u8ddf\u8e2a\u6a21\u578b\uff0c\u901a\u8fc7\u67b6\u6784\u6539\u8fdb\u3001\u5185\u5b58\u4f18\u5316\u548c\u5408\u6210\u8bad\u7ec3\u7b56\u7565\u63d0\u5347\u4e86\u6027\u80fd\u548c\u6548\u7387\u3002\u5b83\u91c7\u7528\u56e0\u679c\u5904\u7406\u548c\u5185\u5b58\u673a\u5236\u6765\u5904\u7406\u6f02\u79fb\u548c\u906e\u6321\uff0c\u65e0\u9700\u672a\u6765\u5e27\u4fe1\u606f\u3002\u5728\u63a8\u7406\u65f6\uff0c\u6a21\u578b\u5148\u8fdb\u884c\u7c97\u7c92\u5ea6\u7684\u5757\u7ea7\u5206\u7c7b\uff0c\u518d\u8fdb\u884c\u7cbe\u70bc\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0cTrack-On2\u5728\u4e94\u4e2a\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6210\u679c\u3002", "motivation": "\u89e3\u51b3\u5728\u89c6\u9891\u5e27\u4e4b\u95f4\u8fdb\u884c\u70b9\u4f4d\u70b9\u7684\u4e00\u81f4\u6027\u8bc6\u522b\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u5916\u89c2\u53d8\u5316\u3001\u8fd0\u52a8\u548c\u906e\u6321\u663e\u8457\u7684\u60c5\u51b5\u4e0b\uff0c\u5e76\u9488\u5bf9\u5728\u7ebf\u8bbe\u7f6e\uff08\u9010\u5e27\u8ddf\u8e2a\uff09\u8fdb\u884c\u4e86\u4f18\u5316\uff0c\u4ee5\u9002\u5e94\u5b9e\u65f6\u548c\u6d41\u5f0f\u5e94\u7528\u3002", "method": "\u63d0\u51faTrack-On2\u6a21\u578b\uff0c\u5bf9Track-On\u6a21\u578b\u8fdb\u884c\u4e86\u6269\u5c55\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8eTransformer\u7684\u7b80\u5355\u800c\u9ad8\u6548\u7684\u5728\u7ebf\u957f\u65f6\u8ddf\u8e2a\u6a21\u578b\u3002\u8be5\u6a21\u578b\u901a\u8fc7\u67b6\u6784\u6539\u8fdb\u3001\u66f4\u6709\u6548\u7684\u5185\u5b58\u4f7f\u7528\u548c\u6539\u8fdb\u7684\u5408\u6210\u8bad\u7ec3\u7b56\u7565\u6765\u63d0\u9ad8\u6027\u80fd\u548c\u6548\u7387\u3002\u5b83\u91c7\u7528\u56e0\u679c\u5904\u7406\u548c\u5185\u5b58\u673a\u5236\u6765\u7ef4\u6301\u65f6\u95f4\u8fde\u8d2f\u6027\uff0c\u4ece\u800c\u5728\u4e0d\u4f7f\u7528\u672a\u6765\u5e27\u7684\u60c5\u51b5\u4e0b\u5904\u7406\u6f02\u79fb\u548c\u906e\u6321\u3002\u63a8\u7406\u65f6\uff0c\u5148\u8fdb\u884c\u7c97\u7c92\u5ea6\u7684\u5757\u7ea7\u5206\u7c7b\uff0c\u7136\u540e\u8fdb\u884c\u7cbe\u70bc\u3002", "result": "Track-On2\u5728\u4e94\u4e2a\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6210\u679c\uff0c\u5176\u6027\u80fd\u8d85\u8fc7\u4e86\u4e4b\u524d\u7684\u5728\u7ebf\u8ddf\u8e2a\u5668\uff0c\u751a\u81f3\u4f18\u4e8e\u5229\u7528\u53cc\u5411\u4e0a\u4e0b\u6587\u7684\u79bb\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u56e0\u679c\u3001\u57fa\u4e8e\u5185\u5b58\u7684\u67b6\u6784\uff0c\u5e76\u4e14\u4ec5\u4f7f\u7528\u5408\u6210\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u662f\u53ef\u6269\u5c55\u7684\u3001\u7528\u4e8e\u771f\u5b9e\u4e16\u754c\u70b9\u4f4d\u8ddf\u8e2a\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.19017", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19017", "abs": "https://arxiv.org/abs/2509.19017", "authors": ["Hazem Dewidar", "Elena Umili"], "title": "Fully Learnable Neural Reward Machines", "comment": null, "summary": "Non-Markovian Reinforcement Learning (RL) tasks present significant\nchallenges, as agents must reason over entire trajectories of state-action\npairs to make optimal decisions. A common strategy to address this is through\nsymbolic formalisms, such as Linear Temporal Logic (LTL) or automata, which\nprovide a structured way to express temporally extended objectives. However,\nthese approaches often rely on restrictive assumptions -- such as the\navailability of a predefined Symbol Grounding (SG) function mapping raw\nobservations to high-level symbolic representations, or prior knowledge of the\ntemporal task. In this work, we propose a fully learnable version of Neural\nReward Machines (NRM), which can learn both the SG function and the automaton\nend-to-end, removing any reliance on prior knowledge. Our approach is therefore\nas easily applicable as classic deep RL (DRL) approaches, while being far more\nexplainable, because of the finite and compact nature of automata. Furthermore,\nwe show that by integrating Fully Learnable Reward Machines (FLNRM) with DRL,\nour method outperforms previous approaches based on Recurrent Neural Networks\n(RNNs).", "AI": {"tldr": "\u53ef\u5b66\u4e60\u7684\u795e\u7ecf\u5956\u52b1\u673a\uff08NRM\uff09\u53ef\u4ee5\u7aef\u5230\u7aef\u5730\u5b66\u4e60\u7b26\u53f7\u6620\u5c04\uff08SG\uff09\u51fd\u6570\u548c\u81ea\u52a8\u673a\uff0c\u4ece\u800c\u65e0\u9700\u5148\u9a8c\u77e5\u8bc6\uff0c\u5e76\u4e14\u6bd4\u57fa\u4e8eRNN\u7684\u65b9\u6cd5\u66f4\u5177\u53ef\u89e3\u91ca\u6027\u4e14\u6027\u80fd\u66f4\u4f18\u3002", "motivation": "\u89e3\u51b3\u975e\u9a6c\u5c14\u53ef\u592b\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4efb\u52a1\u4e2d\u7684\u6311\u6218\uff0c\u8fd9\u4e9b\u4efb\u52a1\u9700\u8981\u667a\u80fd\u4f53\u5bf9\u6574\u4e2a\u72b6\u6001-\u52a8\u4f5c\u5bf9\u8f68\u8ff9\u8fdb\u884c\u63a8\u7406\u624d\u80fd\u505a\u51fa\u6700\u4f18\u51b3\u7b56\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u5b8c\u5168\u53ef\u5b66\u4e60\u7684\u795e\u7ecf\u5956\u52b1\u673a\uff08NRM\uff09\uff0c\u80fd\u591f\u7aef\u5230\u7aef\u5730\u5b66\u4e60\u7b26\u53f7\u6620\u5c04\uff08SG\uff09\u51fd\u6570\u548c\u81ea\u52a8\u673a\uff0c\u65e0\u9700\u4efb\u4f55\u5148\u9a8c\u77e5\u8bc6\u3002", "result": "\u4e0e\u5148\u524d\u57fa\u4e8e\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\uff08RNN\uff09\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684\u5b8c\u5168\u53ef\u5b66\u4e60\u5956\u52b1\u673a\uff08FLNRM\uff09\u4e0e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u7684\u7ed3\u5408\u53ef\u4ee5\u5e26\u6765\u66f4\u597d\u7684\u6027\u80fd\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u5b8c\u5168\u53ef\u5b66\u4e60\u5956\u52b1\u673a\uff08FLNRM\uff09\u80fd\u591f\u7aef\u5230\u7aef\u5730\u5b66\u4e60\u7b26\u53f7\u6620\u5c04\uff08SG\uff09\u51fd\u6570\u548c\u81ea\u52a8\u673a\uff0c\u65e0\u9700\u4efb\u4f55\u5148\u9a8c\u77e5\u8bc6\uff0c\u4e0e\u7ecf\u5178\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u65b9\u6cd5\u4e00\u6837\u6613\u4e8e\u5e94\u7528\uff0c\u5e76\u4e14\u7531\u4e8e\u81ea\u52a8\u673a\u7684\u6709\u9650\u548c\u7d27\u51d1\u7684\u6027\u8d28\u800c\u66f4\u5177\u53ef\u89e3\u91ca\u6027\u3002\u6b64\u5916\uff0c\u5b83\u5728\u4e0eDRL\u7ed3\u5408\u65f6\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u5148\u524d\u57fa\u4e8e\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\uff08RNN\uff09\u7684\u65b9\u6cd5\u3002"}}
{"id": "2509.19129", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19129", "abs": "https://arxiv.org/abs/2509.19129", "authors": ["Adam Romlein", "Benjamin X. Hou", "Yuval Boss", "Cynthia L. Christman", "Stacie Koslovsky", "Erin E. Moreland", "Jason Parham", "Anthony Hoogs"], "title": "KAMERA: Enhancing Aerial Surveys of Ice-associated Seals in Arctic Environments", "comment": "Accepted to the IEEE/CVF International Conference on Computer Vision\n  (ICCV 2025)", "summary": "We introduce KAMERA: a comprehensive system for multi-camera, multi-spectral\nsynchronization and real-time detection of seals and polar bears. Utilized in\naerial surveys for ice-associated seals in the Bering, Chukchi, and Beaufort\nseas around Alaska, KAMERA provides up to an 80% reduction in dataset\nprocessing time over previous methods. Our rigorous calibration and hardware\nsynchronization enable using multiple spectra for object detection. All\ncollected data are annotated with metadata so they can be easily referenced\nlater. All imagery and animal detections from a survey are mapped onto a world\nplane for accurate surveyed area estimates and quick assessment of survey\nresults. We hope KAMERA will inspire other mapping and detection efforts in the\nscientific community, with all software, models, and schematics fully\nopen-sourced.", "AI": {"tldr": "KAMERA\u7cfb\u7edf\u901a\u8fc7\u591a\u6444\u50cf\u5934\u3001\u591a\u5149\u8c31\u540c\u6b65\u548c\u5b9e\u65f6\u68c0\u6d4b\uff0c\u5c06\u6d77\u8c79\u548c\u5317\u6781\u718a\u7684\u7a7a\u4e2d\u8c03\u67e5\u6570\u636e\u96c6\u5904\u7406\u65f6\u95f4\u7f29\u77ed\u4e8680%\uff0c\u5e76\u5bf9\u6240\u6709\u6570\u636e\u8fdb\u884c\u4e86\u5143\u6570\u636e\u6807\u6ce8\uff0c\u65b9\u4fbf\u540e\u7eed\u5f15\u7528\u3002\u8be5\u7cfb\u7edf\u8fd8\u5c06\u6240\u6709\u56fe\u50cf\u548c\u68c0\u6d4b\u5230\u7684\u52a8\u7269\u6620\u5c04\u5230\u4e16\u754c\u5e73\u9762\u4e0a\uff0c\u4ee5\u51c6\u786e\u4f30\u7b97\u8c03\u67e5\u533a\u57df\u5e76\u5feb\u901f\u8bc4\u4f30\u8c03\u67e5\u7ed3\u679c\u3002", "motivation": "\u4ecb\u7ecdKAMERA\u7cfb\u7edf\uff0c\u4e00\u4e2a\u7528\u4e8e\u591a\u6444\u50cf\u5934\u3001\u591a\u5149\u8c31\u540c\u6b65\u4ee5\u53ca\u5bf9\u6d77\u8c79\u548c\u5317\u6781\u718a\u8fdb\u884c\u5b9e\u65f6\u68c0\u6d4b\u7684\u7efc\u5408\u7cfb\u7edf\uff0c\u65e8\u5728\u63d0\u9ad8\u51b0\u5c01\u6d77\u57df\u6d77\u6d0b\u54fa\u4e73\u52a8\u7269\u7a7a\u4e2d\u8c03\u67e5\u7684\u6548\u7387\u3002", "method": "KAMERA\u7cfb\u7edf\u5229\u7528\u591a\u6444\u50cf\u5934\u3001\u591a\u5149\u8c31\u540c\u6b65\u548c\u6807\u5b9a\uff0c\u7ed3\u5408\u4e13\u95e8\u7684\u8f6f\u4ef6\u548c\u6a21\u578b\uff0c\u5b9e\u73b0\u5bf9\u6d77\u8c79\u548c\u5317\u6781\u718a\u7684\u5b9e\u65f6\u68c0\u6d4b\u3002\u6240\u6709\u6536\u96c6\u7684\u6570\u636e\u90fd\u5e26\u6709\u5143\u6570\u636e\uff0c\u5e76\u88ab\u6620\u5c04\u5230\u4e16\u754c\u5e73\u9762\u4e0a\uff0c\u4ee5\u8fdb\u884c\u533a\u57df\u4f30\u7b97\u548c\u7ed3\u679c\u8bc4\u4f30\u3002", "result": "KAMERA\u7cfb\u7edf\u5728\u5e94\u7528\u4e8e\u963f\u62c9\u65af\u52a0\u9644\u8fd1\u6d77\u57df\u7684\u51b0\u5c01\u6d77\u8c79\u7a7a\u4e2d\u8c03\u67e5\u65f6\uff0c\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6570\u636e\u96c6\u5904\u7406\u65f6\u95f4\u6700\u591a\u53ef\u51cf\u5c1180%\u3002", "conclusion": "KAMERA\u7cfb\u7edf\u901a\u8fc7\u5176\u591a\u6444\u50cf\u5934\u3001\u591a\u5149\u8c31\u540c\u6b65\u548c\u5b9e\u65f6\u68c0\u6d4b\u80fd\u529b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7a7a\u4e2d\u8c03\u67e5\u7684\u6548\u7387\u548c\u6570\u636e\u5904\u7406\u901f\u5ea6\uff0c\u5e76\u4e14\u901a\u8fc7\u5c06\u6570\u636e\u6620\u5c04\u5230\u4e16\u754c\u5e73\u9762\u548c\u63d0\u4f9b\u5168\u9762\u7684\u5143\u6570\u636e\uff0c\u4fbf\u4e8e\u540e\u7eed\u7684\u5206\u6790\u548c\u8bc4\u4f30\u3002\u8be5\u7cfb\u7edf\u662f\u5b8c\u5168\u5f00\u6e90\u7684\uff0c\u6709\u671b\u63a8\u52a8\u79d1\u5b66\u754c\u7684\u7c7b\u4f3c\u9879\u76ee\u7684\u53d1\u5c55\u3002"}}
{"id": "2509.19018", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19018", "abs": "https://arxiv.org/abs/2509.19018", "authors": ["Teng Xiao", "Zuchao Li", "Lefei Zhang"], "title": "OmniBridge: Unified Multimodal Understanding, Generation, and Retrieval via Latent Space Alignment", "comment": null, "summary": "Recent advances in multimodal large language models (LLMs) have led to\nsignificant progress in understanding, generation, and retrieval tasks.\nHowever, current solutions often treat these tasks in isolation or require\ntraining LLMs from scratch, resulting in high computational costs and limited\ngeneralization across modalities. In this work, we present OmniBridge, a\nunified and modular multimodal framework that supports vision-language\nunderstanding, generation, and retrieval within a unified architecture.\nOmniBridge adopts a language-centric design that reuses pretrained LLMs and\nintroduces a lightweight bidirectional latent alignment module. To address the\nchallenge of task interference, we propose a two-stage decoupled training\nstrategy: supervised fine-tuning and latent space alignment for aligning LLM\nbehavior with multimodal reasoning, and semantic-guided diffusion training to\nalign cross-modal latent spaces via learnable query embeddings. Extensive\nexperiments across a wide range of benchmarks demonstrate that OmniBridge\nachieves competitive or state-of-the-art performance in all three tasks.\nMoreover, our results highlight the effectiveness of latent space alignment for\nunifying multimodal modeling under a shared representation space. Code and\nmodels are released at https://github.com/xiao-xt/OmniBridge.", "AI": {"tldr": "OmniBridge\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u3001\u6a21\u5757\u5316\u7684\u591a\u6a21\u6001\u6846\u67b6\uff0c\u652f\u6301\u89c6\u89c9-\u8bed\u8a00\u7406\u89e3\u3001\u751f\u6210\u548c\u68c0\u7d22\u4efb\u52a1\uff0c\u91c7\u7528\u8bed\u8a00\u4e2d\u5fc3\u8bbe\u8ba1\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u53cc\u5411\u6f5c\u5728\u5bf9\u9f50\u6a21\u5757\u548c\u89e3\u8026\u8bad\u7ec3\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u8de8\u6a21\u6001\u4efb\u52a1\u7684\u6700\u4f18\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u6a21\u578b\u5728\u5904\u7406\u7406\u89e3\u3001\u751f\u6210\u548c\u68c0\u7d22\u4efb\u52a1\u65f6\u5b58\u5728\u5404\u81ea\u4e3a\u6218\u3001\u8bad\u7ec3\u6210\u672c\u9ad8\u3001\u8de8\u6a21\u6001\u6cdb\u5316\u80fd\u529b\u6709\u9650\u7b49\u95ee\u9898\u3002", "method": "OmniBridge\u91c7\u7528\u8bed\u8a00\u4e2d\u5fc3\u8bbe\u8ba1\uff0c\u590d\u7528\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u5f15\u5165\u8f7b\u91cf\u7ea7\u53cc\u5411\u6f5c\u5728\u5bf9\u9f50\u6a21\u5757\u3002\u901a\u8fc7\u4e24\u9636\u6bb5\u89e3\u8026\u8bad\u7ec3\u7b56\u7565\uff1a\u76d1\u7763\u5fae\u8c03\u548c\u6f5c\u5728\u7a7a\u95f4\u5bf9\u9f50\uff0c\u4ee5\u53ca\u8bed\u4e49\u5f15\u5bfc\u7684\u6269\u6563\u8bad\u7ec3\uff0c\u6765\u7edf\u4e00\u591a\u6a21\u6001\u5efa\u6a21\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cOmniBridge\u5728\u7406\u89e3\u3001\u751f\u6210\u548c\u68c0\u7d22\u4e09\u4e2a\u4efb\u52a1\u4e0a\u5747\u53d6\u5f97\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u751a\u81f3\u662f\u6700\u4f18\u7684\u6027\u80fd\u3002", "conclusion": "\u6f5c\u5728\u7a7a\u95f4\u5bf9\u9f50\u662f\u7edf\u4e00\u591a\u6a21\u6001\u5efa\u6a21\u3001\u5b9e\u73b0\u5171\u4eab\u8868\u793a\u7a7a\u95f4\u7684\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2509.19156", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19156", "abs": "https://arxiv.org/abs/2509.19156", "authors": ["Maurf Hassan", "Steven Davy", "Muhammad Zawish", "Owais Bin Zuber", "Nouman Ashraf"], "title": "NeuCODEX: Edge-Cloud Co-Inference with Spike-Driven Compression and Dynamic Early-Exit", "comment": "This paper was accepted at ICMLA 2025. The official version will\n  appear in IEEE Xplore", "summary": "Spiking Neural Networks (SNNs) offer significant potential for enabling\nenergy-efficient intelligence at the edge. However, performing full SNN\ninference at the edge can be challenging due to the latency and energy\nconstraints arising from fixed and high timestep overheads. Edge-cloud\nco-inference systems present a promising solution, but their deployment is\noften hindered by high latency and feature transmission costs. To address these\nissues, we introduce NeuCODEX, a neuromorphic co-inference architecture that\njointly optimizes both spatial and temporal redundancy. NeuCODEX incorporates a\nlearned spike-driven compression module to reduce data transmission and employs\na dynamic early-exit mechanism to adaptively terminate inference based on\noutput confidence. We evaluated NeuCODEX on both static images (CIFAR10 and\nCaltech) and neuromorphic event streams (CIFAR10-DVS and N-Caltech). To\ndemonstrate practicality, we prototyped NeuCODEX on ResNet-18 and VGG-16\nbackbones in a real edge-to-cloud testbed. Our proposed system reduces data\ntransfer by up to 2048x and edge energy consumption by over 90%, while reducing\nend-to-end latency by up to 3x compared to edge-only inference, all with a\nnegligible accuracy drop of less than 2%. In doing so, NeuCODEX enables\npractical, high-performance SNN deployment in resource-constrained\nenvironments.", "AI": {"tldr": "NeuCODEX\u662f\u4e00\u79cd\u795e\u7ecf\u5f62\u6001\u534f\u540c\u63a8\u7406\u67b6\u6784\uff0c\u901a\u8fc7\u5b66\u4e60\u5230\u7684\u8109\u51b2\u9a71\u52a8\u7684\u538b\u7f29\u6a21\u5757\u548c\u52a8\u6001\u7684\u65e9\u671f\u9000\u51fa\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u8fb9\u7f18-\u4e91\u534f\u540c\u63a8\u7406\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u6570\u636e\u4f20\u8f93\u3001\u8fb9\u7f18\u80fd\u8017\u548c\u7aef\u5230\u7aef\u5ef6\u8fdf\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u53ef\u5ffd\u7565\u7684\u51c6\u786e\u6027\u635f\u5931\uff0c\u4f7f\u5f97\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u90e8\u7f72SNN\u6210\u4e3a\u53ef\u80fd\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u8fb9\u7f18SNN\u63a8\u7406\u7684\u5ef6\u8fdf\u548c\u80fd\u91cf\u9650\u5236\u4ee5\u53ca\u8fb9\u7f18-\u4e91\u534f\u540c\u63a8\u7406\u7684\u9ad8\u5ef6\u8fdf\u548c\u7279\u5f81\u4f20\u8f93\u6210\u672c\u95ee\u9898\u3002", "method": "\u63d0\u51faNeuCODEX\u795e\u7ecf\u5f62\u6001\u534f\u540c\u63a8\u7406\u67b6\u6784\uff0c\u91c7\u7528\u5b66\u4e60\u5230\u7684\u8109\u51b2\u9a71\u52a8\u538b\u7f29\u6a21\u5757\u51cf\u5c11\u6570\u636e\u4f20\u8f93\uff0c\u5e76\u4f7f\u7528\u52a8\u6001\u65e9\u671f\u9000\u51fa\u673a\u5236\u6839\u636e\u8f93\u51fa\u7f6e\u4fe1\u5ea6\u81ea\u9002\u5e94\u5730\u7ec8\u6b62\u63a8\u7406\u3002", "result": "\u5728CIFAR10\u3001Caltech\u3001CIFAR10-DVS\u548cN-Caltech\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u5728ResNet-18\u548cVGG-16\u9aa8\u5e72\u7f51\u7edc\u4e0a\u8fdb\u884c\u539f\u578b\u8bbe\u8ba1\u3002\u7ed3\u679c\u663e\u793a\uff0c\u4e0e\u4ec5\u5728\u8fb9\u7f18\u8fdb\u884c\u63a8\u7406\u76f8\u6bd4\uff0cNeuCODEX\u5c06\u6570\u636e\u4f20\u8f93\u51cf\u5c11\u4e86\u9ad8\u8fbe2048\u500d\uff0c\u8fb9\u7f18\u80fd\u8017\u964d\u4f4e\u4e8690%\u4ee5\u4e0a\uff0c\u7aef\u5230\u7aef\u5ef6\u8fdf\u51cf\u5c11\u4e86\u9ad8\u8fbe3\u500d\uff0c\u51c6\u786e\u6027\u635f\u5931\u4e0d\u52302%\u3002", "conclusion": "NeuCODEX\u4f7f\u5f97\u5728\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u4e2d\u5b9e\u9645\u90e8\u7f72\u9ad8\u6027\u80fdSNN\u6210\u4e3a\u53ef\u80fd\u3002"}}
{"id": "2509.19032", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19032", "abs": "https://arxiv.org/abs/2509.19032", "authors": ["Kashaf Ul Emaan"], "title": "Improving Credit Card Fraud Detection through Transformer-Enhanced GAN Oversampling", "comment": null, "summary": "Detection of credit card fraud is an acute issue of financial security\nbecause transaction datasets are highly lopsided, with fraud cases being only a\ndrop in the ocean. Balancing datasets using the most popular methods of\ntraditional oversampling such as the Synthetic Minority Oversampling Technique\n(SMOTE) generally create simplistic synthetic samples that are not readily\napplicable to complex fraud patterns. Recent industry advances that include\nConditional Tabular Generative Adversarial Networks (CTGAN) and Tabular\nVariational Autoencoders (TVAE) have demonstrated increased efficiency in\ntabular synthesis, yet all these models still exhibit issues with\nhigh-dimensional dependence modelling. Now we will present our hybrid approach\nwhere we use a Generative Adversarial Network (GAN) with a Transformer encoder\nblock to produce realistic fraudulent transactions samples. The GAN\narchitecture allows training realistic generators adversarial, and the\nTransformer allows the model to learn rich feature interactions by\nself-attention. Such a hybrid strategy overcomes the limitations of SMOTE,\nCTGAN, and TVAE by producing a variety of high-quality synthetic minority\nclasses samples. We test our algorithm on the publicly-available Credit Card\nFraud Detection dataset and compare it to conventional and generative\nresampling strategies with a variety of classifiers, such as Logistic\nRegression (LR), Random Forest (RF), Extreme Gradient Boosting (XGBoost), and\nSupport Vector Machine (SVM). Findings indicate that our Transformer-based GAN\nshows substantial gains in Recall, F1-score and Area Under the Receiver\nOperating Characteristic Curve (AUC), which indicates that it is effective in\novercoming the severe class imbalance inherent in the task of fraud detection.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7f16\u7801\u5668\u5757\u7684\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GAN\uff09\uff0c\u7528\u4e8e\u751f\u6210\u903c\u771f\u7684\u6b3a\u8bc8\u4ea4\u6613\u6837\u672c\uff0c\u4ee5\u89e3\u51b3\u4fe1\u7528\u5361\u6b3a\u8bc8\u68c0\u6d4b\u4e2d\u6570\u636e\u4e0d\u5e73\u8861\u7684\u95ee\u9898\uff0c\u5e76\u5728\u53ec\u56de\u7387\u3001F1\u5206\u6570\u548cAUC\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6539\u8fdb\u3002", "motivation": "\u4fe1\u7528\u5361\u6b3a\u8bc8\u68c0\u6d4b\u9762\u4e34\u7740\u4ea4\u6613\u6570\u636e\u96c6\u9ad8\u5ea6\u4e0d\u5e73\u8861\u7684\u95ee\u9898\uff0c\u4f20\u7edf\u7684\u8fc7\u91c7\u6837\u65b9\u6cd5\uff08\u5982SMOTE\uff09\u751f\u6210\u7684\u6837\u672c\u8fc7\u4e8e\u7b80\u5355\uff0c\u96be\u4ee5\u5e94\u5bf9\u590d\u6742\u7684\u6b3a\u8bc8\u6a21\u5f0f\u3002\u73b0\u6709\u7684CTGAN\u548cTVAE\u6a21\u578b\u5728\u8868\u683c\u5408\u6210\u65b9\u9762\u6548\u7387\u66f4\u9ad8\uff0c\u4f46\u5728\u9ad8\u7ef4\u4f9d\u8d56\u5efa\u6a21\u65b9\u9762\u4ecd\u5b58\u5728\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GAN\uff09\u548cTransformer\u7f16\u7801\u5668\u5757\uff0c\u4ee5\u751f\u6210\u903c\u771f\u7684\u6b3a\u8bc8\u4ea4\u6613\u6837\u672c\u3002GAN\u67b6\u6784\u5141\u8bb8\u8bad\u7ec3\u903c\u771f\u7684\u751f\u6210\u5668\uff0c\u800cTransformer\u901a\u8fc7\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5b66\u4e60\u4e30\u5bcc\u7684\u7279\u5f81\u4ea4\u4e92\u3002", "result": "\u5728\u516c\u5f00\u7684\u4fe1\u7528\u5361\u6b3a\u8bc8\u68c0\u6d4b\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u6d4b\u8bd5\uff0c\u5e76\u4e0e\u4f20\u7edf\u7684\u548c\u751f\u6210\u91cd\u91c7\u6837\u7b56\u7565\u4ee5\u53ca\u591a\u79cd\u5206\u7c7b\u5668\uff08\u5982\u903b\u8f91\u56de\u5f52\u3001\u968f\u673a\u68ee\u6797\u3001\u6781\u7aef\u68af\u5ea6\u63d0\u5347\u548c\u652f\u6301\u5411\u91cf\u673a\uff09\u8fdb\u884c\u6bd4\u8f83\u3002\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u57fa\u4e8eTransformer\u7684GAN\u5728\u53ec\u56de\u7387\u3001F1\u5206\u6570\u548cAUC\u65b9\u9762\u663e\u793a\u51fa\u663e\u8457\u7684\u4f18\u52bf\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8eTransformer\u7684GAN\u5728\u514b\u670d\u6b3a\u8bc8\u68c0\u6d4b\u4efb\u52a1\u4e2d\u56fa\u6709\u7684\u4e25\u91cd\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u65b9\u9762\u662f\u6709\u6548\u7684\u3002"}}
{"id": "2509.19165", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19165", "abs": "https://arxiv.org/abs/2509.19165", "authors": ["Yun Wang", "Junjie Hu", "Junhui Hou", "Chenghao Zhang", "Renwei Yang", "Dapeng Oliver Wu"], "title": "RoSe: Robust Self-supervised Stereo Matching under Adverse Weather Conditions", "comment": null, "summary": "Recent self-supervised stereo matching methods have made significant\nprogress, but their performance significantly degrades under adverse weather\nconditions such as night, rain, and fog. We identify two primary weaknesses\ncontributing to this performance degradation. First, adverse weather introduces\nnoise and reduces visibility, making CNN-based feature extractors struggle with\ndegraded regions like reflective and textureless areas. Second, these degraded\nregions can disrupt accurate pixel correspondences, leading to ineffective\nsupervision based on the photometric consistency assumption. To address these\nchallenges, we propose injecting robust priors derived from the visual\nfoundation model into the CNN-based feature extractor to improve feature\nrepresentation under adverse weather conditions. We then introduce scene\ncorrespondence priors to construct robust supervisory signals rather than\nrelying solely on the photometric consistency assumption. Specifically, we\ncreate synthetic stereo datasets with realistic weather degradations. These\ndatasets feature clear and adverse image pairs that maintain the same semantic\ncontext and disparity, preserving the scene correspondence property. With this\nknowledge, we propose a robust self-supervised training paradigm, consisting of\ntwo key steps: robust self-supervised scene correspondence learning and adverse\nweather distillation. Both steps aim to align underlying scene results from\nclean and adverse image pairs, thus improving model disparity estimation under\nadverse weather effects. Extensive experiments demonstrate the effectiveness\nand versatility of our proposed solution, which outperforms existing\nstate-of-the-art self-supervised methods. Codes are available at\n\\textcolor{blue}{https://github.com/cocowy1/RoSe-Robust-Self-supervised-Stereo-Matching-under-Adverse-Weather-Conditions}.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.19044", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19044", "abs": "https://arxiv.org/abs/2509.19044", "authors": ["Yang Li", "Chenyu Wang", "Tingrui Wang", "Yongwei Wang", "Haonan Li", "Zhunga Liu", "Quan Pan"], "title": "Latent Danger Zone: Distilling Unified Attention for Cross-Architecture Black-box Attacks", "comment": null, "summary": "Black-box adversarial attacks remain challenging due to limited access to\nmodel internals. Existing methods often depend on specific network\narchitectures or require numerous queries, resulting in limited\ncross-architecture transferability and high query costs. To address these\nlimitations, we propose JAD, a latent diffusion model framework for black-box\nadversarial attacks. JAD generates adversarial examples by leveraging a latent\ndiffusion model guided by attention maps distilled from both a convolutional\nneural network (CNN) and a Vision Transformer (ViT) models. By focusing on\nimage regions that are commonly sensitive across architectures, this approach\ncrafts adversarial perturbations that transfer effectively between different\nmodel types. This joint attention distillation strategy enables JAD to be\narchitecture-agnostic, achieving superior attack generalization across diverse\nmodels. Moreover, the generative nature of the diffusion framework yields high\nadversarial sample generation efficiency by reducing reliance on iterative\nqueries. Experiments demonstrate that JAD offers improved attack\ngeneralization, generation efficiency, and cross-architecture transferability\ncompared to existing methods, providing a promising and effective paradigm for\nblack-box adversarial attacks.", "AI": {"tldr": "JAD\u662f\u4e00\u4e2a\u5229\u7528\u6f5c\u5728\u6269\u6563\u6a21\u578b\u8fdb\u884c\u9ed1\u76d2\u5bf9\u6297\u6027\u653b\u51fb\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u878d\u5408CNN\u548cViT\u7684\u6ce8\u610f\u529b\u56fe\u6765\u751f\u6210\u80fd\u591f\u6709\u6548\u8de8\u4e0d\u540c\u6a21\u578b\u8fc1\u79fb\u7684\u5bf9\u6297\u6837\u672c\uff0c\u4ece\u800c\u63d0\u9ad8\u653b\u51fb\u7684\u6cdb\u5316\u6027\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u9ed1\u76d2\u5bf9\u6297\u6027\u653b\u51fb\u65b9\u6cd5\u5728\u6a21\u578b\u5185\u90e8\u4fe1\u606f\u83b7\u53d6\u6709\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u5f80\u5f80\u53d7\u9650\u4e8e\u7279\u5b9a\u7684\u7f51\u7edc\u67b6\u6784\u6216\u9700\u8981\u5927\u91cf\u7684\u67e5\u8be2\u6b21\u6570\uff0c\u5bfc\u81f4\u8de8\u67b6\u6784\u8fc1\u79fb\u80fd\u529b\u6709\u9650\u4e14\u67e5\u8be2\u6210\u672c\u9ad8\u3002", "method": "JAD\u5229\u7528\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff0c\u5e76\u7ed3\u5408\u4ece\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u548c\u89c6\u89c9Transformer\uff08ViT\uff09\u6a21\u578b\u4e2d\u63d0\u53d6\u7684\u6ce8\u610f\u529b\u56fe\u8fdb\u884c\u6307\u5bfc\uff0c\u751f\u6210\u5bf9\u6297\u6027\u6837\u672c\u3002\u901a\u8fc7\u5173\u6ce8\u8de8\u67b6\u6784\u666e\u904d\u654f\u611f\u7684\u56fe\u50cf\u533a\u57df\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u5728\u4e0d\u540c\u6a21\u578b\u7c7b\u578b\u4e4b\u95f4\u6709\u6548\u8fc1\u79fb\u7684\u5bf9\u6297\u6027\u6270\u52a8\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0cJAD\u5728\u653b\u51fb\u6cdb\u5316\u6027\u3001\u751f\u6210\u6548\u7387\u548c\u8de8\u67b6\u6784\u8fc1\u79fb\u80fd\u529b\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "JAD\u901a\u8fc7\u8054\u5408\u6ce8\u610f\u529b\u84b8\u998f\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u6a21\u578b\u65e0\u5173\u6027\uff0c\u5e76\u5728\u5404\u79cd\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u653b\u51fb\u6cdb\u5316\u80fd\u529b\u3002\u6b64\u5916\uff0c\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u7279\u6027\u901a\u8fc7\u51cf\u5c11\u5bf9\u8fed\u4ee3\u67e5\u8be2\u7684\u4f9d\u8d56\uff0c\u63d0\u9ad8\u4e86\u5bf9\u6297\u6837\u672c\u751f\u6210\u7684\u6548\u7387\uff0c\u4e3a\u9ed1\u76d2\u5bf9\u6297\u6027\u653b\u51fb\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u666f\u4e14\u6709\u6548\u7684\u65b9\u6cd5\u3002"}}
{"id": "2509.19166", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19166", "abs": "https://arxiv.org/abs/2509.19166", "authors": ["Siddharth Gupta", "Jitin Singla"], "title": "YOLO-LAN: Precise Polyp Detection via Optimized Loss, Augmentations and Negatives", "comment": null, "summary": "Colorectal cancer (CRC), a lethal disease, begins with the growth of abnormal\nmucosal cell proliferation called polyps in the inner wall of the colon. When\nleft undetected, polyps can become malignant tumors. Colonoscopy is the\nstandard procedure for detecting polyps, as it enables direct visualization and\nremoval of suspicious lesions. Manual detection by colonoscopy can be\ninconsistent and is subject to oversight. Therefore, object detection based on\ndeep learning offers a better solution for a more accurate and real-time\ndiagnosis during colonoscopy. In this work, we propose YOLO-LAN, a YOLO-based\npolyp detection pipeline, trained using M2IoU loss, versatile data\naugmentations and negative data to replicate real clinical situations. Our\npipeline outperformed existing methods for the Kvasir-seg and BKAI-IGH NeoPolyp\ndatasets, achieving mAP$_{50}$ of 0.9619, mAP$_{50:95}$ of 0.8599 with YOLOv12\nand mAP$_{50}$ of 0.9540, mAP$_{50:95}$ of 0.8487 with YOLOv8 on the Kvasir-seg\ndataset. The significant increase is achieved in mAP$_{50:95}$ score, showing\nthe precision of polyp detection. We show robustness based on polyp size and\nprecise location detection, making it clinically relevant in AI-assisted\ncolorectal screening.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eYOLO\u7684\u7ed3\u76f4\u80a0\u606f\u8089\u68c0\u6d4b\u6d41\u7a0bYOLO-LAN\uff0c\u901a\u8fc7\u4f7f\u7528M2IoU\u635f\u5931\u3001\u591a\u6837\u7684\u6570\u91c7\u589e\u5f3a\u548c\u8d1f\u6570\u91c7\u6765\u6a21\u62df\u771f\u5b9e\u4e34\u5e8a\u60c5\u51b5\uff0c\u5e76\u5728Kvasir-seg\u548cBKAI-IGH NeoPolyp\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u7ed3\u679c\uff0c\u5c24\u5176\u5728mAP$_{50:95}$\u5f97\u5206\u4e0a\u663e\u8457\u63d0\u9ad8\uff0c\u8bc1\u660e\u4e86\u5176\u5728AI\u8f85\u52a9\u7ed3\u76f4\u80a0\u764c\u7b5b\u67e5\u4e2d\u7684\u4e34\u5e8a\u76f8\u5173\u6027\u3002", "motivation": "\u7ed3\u76f4\u80a0\u764c\uff08CRC\uff09\u662f\u4e00\u79cd\u81f4\u547d\u6027\u75be\u75c5\uff0c\u5176\u65e9\u671f\u9636\u6bb5\u8868\u73b0\u4e3a\u7ed3\u76f4\u80a0\u606f\u8089\u3002\u624b\u52a8\u8fdb\u884c\u7ed3\u80a0\u955c\u68c0\u67e5\u4ee5\u68c0\u6d4b\u606f\u8089\u53ef\u80fd\u4e0d\u4e00\u81f4\u4e14\u5bb9\u6613\u51fa\u9519\u3002\u56e0\u6b64\uff0c\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u76ee\u6807\u68c0\u6d4b\u4e3a\u5728\u7ed3\u80a0\u955c\u68c0\u67e5\u671f\u95f4\u8fdb\u884c\u66f4\u51c6\u786e\u3001\u5b9e\u65f6\u7684\u8bca\u65ad\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aYOLO-LAN\u7684\u57fa\u4e8eYOLO\u7684\u606f\u8089\u68c0\u6d4b\u6d41\u7a0b\u3002\u8be5\u6d41\u7a0b\u91c7\u7528\u4e86M2IoU\u635f\u5931\u51fd\u6570\u3001\u591a\u6837\u5316\u7684\u6570\u636e\u589e\u5f3a\u6280\u672f\u4ee5\u53ca\u8d1f\u6837\u672c\u6570\u636e\uff0c\u4ee5\u6a21\u62df\u771f\u5b9e\u7684\u4e34\u5e8a\u73af\u5883\u3002\u7814\u7a76\u4eba\u5458\u5728Kvasir-seg\u548cBKAI-IGH NeoPolyp\u6570\u636e\u96c6\u4e0a\u5bf9\u8be5\u6d41\u7a0b\u8fdb\u884c\u4e86\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002", "result": "\u6240\u63d0\u51fa\u7684YOLO-LAN\u6d41\u7a0b\u5728Kvasir-seg\u548cBKAI-IGH NeoPolyp\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u5177\u4f53\u800c\u8a00\uff0c\u5728\u4f7f\u7528YOLOv12\u65f6\uff0c\u5728Kvasir-seg\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e860.9619\u7684mAP$_{50}$\u548c0.8599\u7684mAP$_{50:95}$\uff1b\u5728\u4f7f\u7528YOLOv8\u65f6\uff0c\u5219\u8fbe\u5230\u4e860.9540\u7684mAP$_{50}$\u548c0.8487\u7684mAP$_{50:95}$\u3002mAP$_{50:95}$\u5f97\u5206\u7684\u663e\u8457\u63d0\u9ad8\u8868\u660e\u4e86\u606f\u8089\u68c0\u6d4b\u7684\u7cbe\u786e\u5ea6\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u68c0\u6d4b\u4e0d\u540c\u5927\u5c0f\u606f\u8089\u548c\u7cbe\u786e\u5b9a\u4f4d\u65b9\u9762\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "YOLO-LAN\u5728\u7ed3\u76f4\u80a0\u606f\u8089\u68c0\u6d4b\u65b9\u9762\u8868\u73b0\u51fa\u9ad8\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\uff0c\u7279\u522b\u662f\u5728mAP$_{50:95}$\u5f97\u5206\u4e0a\u7684\u63d0\u5347\u5c24\u4e3a\u663e\u8457\u3002\u5176\u5728\u4e0d\u540c\u5927\u5c0f\u606f\u8089\u68c0\u6d4b\u548c\u7cbe\u786e\u5b9a\u4f4d\u65b9\u9762\u7684\u80fd\u529b\uff0c\u4f7f\u5176\u5728AI\u8f85\u52a9\u7ed3\u76f4\u80a0\u764c\u7b5b\u67e5\u4e2d\u5177\u6709\u91cd\u8981\u7684\u4e34\u5e8a\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.19063", "categories": ["cs.LG", "cs.AI", "68T07"], "pdf": "https://arxiv.org/pdf/2509.19063", "abs": "https://arxiv.org/abs/2509.19063", "authors": ["Przemys\u0142aw Spyra"], "title": "Beyond Backpropagation: Exploring Innovative Algorithms for Energy-Efficient Deep Neural Network Training", "comment": null, "summary": "The rising computational and energy demands of deep neural networks (DNNs),\ndriven largely by backpropagation (BP), challenge sustainable AI development.\nThis paper rigorously investigates three BP-free training methods: the\nForward-Forward (FF), Cascaded-Forward (CaFo), and Mono-Forward (MF)\nalgorithms, tracing their progression from foundational concepts to a\ndemonstrably superior solution.\n  A robust comparative framework was established: each algorithm was\nimplemented on its native architecture (MLPs for FF and MF, a CNN for CaFo) and\nbenchmarked against an equivalent BP-trained model. Hyperparameters were\noptimized with Optuna, and consistent early stopping criteria were applied\nbased on validation performance, ensuring all models were optimally tuned\nbefore comparison.\n  Results show that MF not only competes with but consistently surpasses BP in\nclassification accuracy on its native MLPs. Its superior generalization stems\nfrom converging to a more favorable minimum in the validation loss landscape,\nchallenging the assumption that global optimization is required for\nstate-of-the-art results. Measured at the hardware level using the NVIDIA\nManagement Library (NVML) API, MF reduces energy consumption by up to 41% and\nshortens training time by up to 34%, translating to a measurably smaller carbon\nfootprint as estimated by CodeCarbon.\n  Beyond this primary result, we present a hardware-level analysis that\nexplains the efficiency gains: exposing FF's architectural inefficiencies,\nvalidating MF's computationally lean design, and challenging the assumption\nthat all BP-free methods are inherently more memory-efficient. By documenting\nthe evolution from FF's conceptual groundwork to MF's synthesis of accuracy and\nsustainability, this work offers a clear, data-driven roadmap for future\nenergy-efficient deep learning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u4e09\u79cd\u53cd\u5411\u4f20\u64ad\uff08BP\uff09\u4e4b\u5916\u7684\u8bad\u7ec3\u65b9\u6cd5\uff08\u524d\u9988-\u524d\u9988 FF\u3001\u7ea7\u8054\u524d\u9988 CaFo \u548c\u5355\u524d\u9988 MF\uff09\uff0c\u5e76\u5c06\u5176\u4e0e BP \u8bad\u7ec3\u8fdb\u884c\u6bd4\u8f83\u3002\u7ed3\u679c\u663e\u793a MF \u5728 MLP \u4e0a\u4f18\u4e8e BP \u8bad\u7ec3\uff0c\u5e76\u4e14\u5728\u80fd\u6e90\u6d88\u8017\u548c\u8bad\u7ec3\u65f6\u95f4\u65b9\u9762\u66f4\u5177\u4f18\u52bf\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNN\uff09\u7684\u8ba1\u7b97\u548c\u80fd\u6e90\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u8fd9\u4e3b\u8981\u7531\u53cd\u5411\u4f20\u64ad\uff08BP\uff09\u5f15\u8d77\uff0c\u5bf9\u53ef\u6301\u7eed\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u7684\u53d1\u5c55\u6784\u6210\u4e86\u6311\u6218\u3002", "method": "\u8be5\u7814\u7a76\u5efa\u7acb\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u6bd4\u8f83\u6846\u67b6\uff0c\u5728\u5404\u81ea\u7684\u539f\u751f\u67b6\u6784\uff08MLP \u7528\u4e8e FF \u548c MF\uff0cCNN \u7528\u4e8e CaFo\uff09\u4e0a\u5b9e\u73b0\u4e86\u6bcf\u79cd\u7b97\u6cd5\uff0c\u5e76\u4e0e\u7b49\u6548\u7684 BP \u8bad\u7ec3\u6a21\u578b\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\u3002\u4f7f\u7528 Optuna \u4f18\u5316\u4e86\u8d85\u53c2\u6570\uff0c\u5e76\u6839\u636e\u9a8c\u8bc1\u6027\u80fd\u5e94\u7528\u4e86\u6301\u7eed\u7684\u63d0\u524d\u505c\u6b62\u6807\u51c6\uff0c\u4ee5\u786e\u4fdd\u6240\u6709\u6a21\u578b\u5728\u6bd4\u8f83\u524d\u90fd\u7ecf\u8fc7\u6700\u4f73\u8c03\u6574\u3002", "result": "MF \u4e0d\u4ec5\u5728 MLP \u4e0a\u53ef\u4ee5\u4e0e BP \u5ab2\u7f8e\uff0c\u800c\u4e14\u5728\u5206\u7c7b\u7cbe\u5ea6\u4e0a\u6301\u7eed\u8d85\u8d8a BP\u3002\u5176\u4f18\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\u6e90\u4e8e\u6536\u655b\u5230\u9a8c\u8bc1\u635f\u5931\u683c\u5c40\u4e2d\u66f4\u6709\u5229\u7684\u6700\u5c0f\u503c\uff0c\u8fd9\u6311\u6218\u4e86\u9700\u8981\u5168\u5c40\u4f18\u5316\u624d\u80fd\u83b7\u5f97\u6700\u5148\u8fdb\u7ed3\u679c\u7684\u5047\u8bbe\u3002MF \u5c06\u80fd\u6e90\u6d88\u8017\u548c\u8bad\u7ec3\u65f6\u95f4\u5206\u522b\u51cf\u5c11\u4e86\u9ad8\u8fbe 41% \u548c 34%\u3002", "conclusion": "\u901a\u8fc7\u8bb0\u5f55\u4ece FF \u7684\u6982\u5ff5\u57fa\u7840\u5230 MF \u7684\u51c6\u786e\u6027\u548c\u53ef\u6301\u7eed\u6027\u7684\u7efc\u5408\uff0c\u8fd9\u9879\u5de5\u4f5c\u4e3a\u672a\u6765\u8282\u80fd\u6df1\u5ea6\u5b66\u4e60\u63d0\u4f9b\u4e86\u6e05\u6670\u3001\u6570\u636e\u9a71\u52a8\u7684\u8def\u7ebf\u56fe\u3002"}}
{"id": "2509.19183", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19183", "abs": "https://arxiv.org/abs/2509.19183", "authors": ["Mingqi Gao", "Jingkun Chen", "Yunqi Miao", "Gengshen Wu", "Zhijin Qin", "Jungong Han"], "title": "The 1st Solution for MOSEv2 Challenge 2025: Long-term and Concept-aware Video Segmentation via SeC", "comment": null, "summary": "This technical report explores the MOSEv2 track of the LSVOS Challenge, which\ntargets complex semi-supervised video object segmentation. By analysing and\nadapting SeC, an enhanced SAM-2 framework, we conduct a detailed study of its\nlong-term memory and concept-aware memory, showing that long-term memory\npreserves temporal continuity under occlusion and reappearance, while\nconcept-aware memory supplies semantic priors that suppress distractors;\ntogether, these traits directly benefit several MOSEv2's core challenges. Our\nsolution achieves a JF score of 39.89% on the test set, ranking 1st in the\nMOSEv2 track of the LSVOS Challenge.", "AI": {"tldr": "\u672c\u9879\u76ee\u4e3aMOSEv2\u8d5b\u9053\u63d0\u4f9b\u4e86\u57fa\u4e8eSeC\u589e\u5f3aSAM-2\u6846\u67b6\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5b9e\u73b0\u4e8639.89%\u7684JF\u5206\u6570\uff0c\u5e76\u89e3\u51b3\u4e86\u906e\u6321\u3001\u91cd\u73b0\u548c\u5e72\u6270\u7269\u7b49\u6838\u5fc3\u6311\u6218\u3002", "motivation": "\u89e3\u51b3\u590d\u6742\u89c6\u9891\u5bf9\u8c61\u5206\u5272\u4e2d\u7684\u957f\u671f\u8bb0\u5fc6\u548c\u6982\u5ff5\u611f\u77e5\u8bb0\u5fc6\u95ee\u9898\uff0c\u4ee5\u5e94\u5bf9\u906e\u6321\u3001\u91cd\u73b0\u548c\u5e72\u6270\u7269\u7b49\u6311\u6218\u3002", "method": "\u5206\u6790\u5e76\u6539\u7f16\u4e86SeC\uff08\u4e00\u4e2a\u589e\u5f3a\u7684SAM-2\u6846\u67b6\uff09\uff0c\u5e76\u7814\u7a76\u4e86\u5176\u957f\u671f\u8bb0\u5fc6\u548c\u6982\u5ff5\u611f\u77e5\u8bb0\u5fc6\u7684\u7279\u6027\u3002", "result": "\u5728MOSEv2\u6d4b\u8bd5\u96c6\u4e0a\u53d6\u5f97\u4e8639.89%\u7684JF\u5206\u6570\uff0c\u5728LSVOS\u6311\u6218\u8d5b\u7684MOSEv2\u8d5b\u9053\u4e0a\u6392\u540d\u7b2c\u4e00\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u957f\u671f\u8bb0\u5fc6\u548c\u6982\u5ff5\u611f\u77e5\u8bb0\u5fc6\u5171\u540c\u4f5c\u7528\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u89c6\u9891\u5bf9\u8c61\u5206\u5272\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5e76\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u7ade\u8d5b\u6210\u7ee9\u3002"}}
{"id": "2509.19078", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19078", "abs": "https://arxiv.org/abs/2509.19078", "authors": ["Jian Xu", "Qibin Zhao", "John Paisley", "Delu Zeng"], "title": "Diffusion Bridge Variational Inference for Deep Gaussian Processes", "comment": null, "summary": "Deep Gaussian processes (DGPs) enable expressive hierarchical Bayesian\nmodeling but pose substantial challenges for posterior inference, especially\nover inducing variables. Denoising diffusion variational inference (DDVI)\naddresses this by modeling the posterior as a time-reversed diffusion from a\nsimple Gaussian prior. However, DDVI's fixed unconditional starting\ndistribution remains far from the complex true posterior, resulting in\ninefficient inference trajectories and slow convergence. In this work, we\npropose Diffusion Bridge Variational Inference (DBVI), a principled extension\nof DDVI that initiates the reverse diffusion from a learnable, data-dependent\ninitial distribution. This initialization is parameterized via an amortized\nneural network and progressively adapted using gradients from the ELBO\nobjective, reducing the posterior gap and improving sample efficiency. To\nenable scalable amortization, we design the network to operate on the inducing\ninputs, which serve as structured, low-dimensional summaries of the dataset and\nnaturally align with the inducing variables' shape. DBVI retains the\nmathematical elegance of DDVI, including Girsanov-based ELBOs and reverse-time\nSDEs,while reinterpreting the prior via a Doob-bridged diffusion process. We\nderive a tractable training objective under this formulation and implement DBVI\nfor scalable inference in large-scale DGPs. Across regression, classification,\nand image reconstruction tasks, DBVI consistently outperforms DDVI and other\nvariational baselines in predictive accuracy, convergence speed, and posterior\nquality.", "AI": {"tldr": "DDVI\u7684\u6539\u8fdb\u7248DBVI\uff0c\u901a\u8fc7\u5b66\u4e60\u6570\u636e\u4f9d\u8d56\u7684\u521d\u59cb\u5206\u5e03\u6765\u63d0\u9ad8\u6df1\u5ea6\u9ad8\u65af\u8fc7\u7a0b\u63a8\u7406\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u7684DDVI\u65b9\u6cd5\u5728\u6df1\u5ea6\u9ad8\u65af\u8fc7\u7a0b\u7684\u63a8\u7406\u4e2d\u5b58\u5728\u6536\u655b\u6162\u548c\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u56e0\u4e3a\u5176\u56fa\u5b9a\u7684\u5148\u9a8c\u5206\u5e03\u4e0e\u771f\u5b9e\u540e\u9a8c\u5206\u5e03\u5dee\u8ddd\u8fc7\u5927\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3a\u6269\u6563\u6865\u53d8\u5206\u63a8\u7406\uff08DBVI\uff09\u7684\u6539\u8fdb\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b66\u4e60\u4e00\u4e2a\u6570\u636e\u4f9d\u8d56\u7684\u521d\u59cb\u5206\u5e03\u6765\u66ff\u4ee3DDVI\u4e2d\u56fa\u5b9a\u7684\u5148\u9a8c\u5206\u5e03\u3002\u8be5\u521d\u59cb\u5206\u5e03\u7531\u4e00\u4e2a\u81ea\u9002\u5e94\u795e\u7ecf\u7f51\u7edc\u53c2\u6570\u5316\uff0c\u5e76\u5229\u7528ELBO\u76ee\u6807\u51fd\u6570\u7684\u68af\u5ea6\u8fdb\u884c\u4f18\u5316\uff0c\u4ece\u800c\u51cf\u5c0f\u4e86\u540e\u9a8c\u5dee\u8ddd\uff0c\u63d0\u9ad8\u4e86\u6837\u672c\u6548\u7387\u3002\u8be5\u795e\u7ecf\u7f51\u7edc\u88ab\u8bbe\u8ba1\u4e3a\u4f5c\u7528\u4e8e\u8bf1\u5bfc\u8f93\u5165\uff0c\u4ee5\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u644a\u9500\u3002", "result": "DBVI\u5728\u56de\u5f52\u3001\u5206\u7c7b\u548c\u56fe\u50cf\u91cd\u5efa\u4efb\u52a1\u4e2d\uff0c\u5728\u9884\u6d4b\u7cbe\u5ea6\u3001\u6536\u655b\u901f\u5ea6\u548c\u540e\u9a8c\u8d28\u91cf\u65b9\u9762\u5747\u4f18\u4e8eDDVI\u548c\u5176\u4ed6\u53d8\u5206\u57fa\u7ebf\u3002", "conclusion": "DBVI\u4f5c\u4e3aDDVI\u7684\u539f\u5219\u6027\u6269\u5c55\uff0c\u901a\u8fc7\u5f15\u5165\u53ef\u5b66\u4e60\u7684\u3001\u6570\u636e\u4f9d\u8d56\u7684\u521d\u59cb\u5206\u5e03\uff0c\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u6df1\u5ea6\u9ad8\u65af\u8fc7\u7a0b\u63a8\u7406\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u5e76\u4e14\u5728\u5404\u79cd\u4efb\u52a1\u4e2d\u90fd\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002"}}
{"id": "2509.19191", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19191", "abs": "https://arxiv.org/abs/2509.19191", "authors": ["Yueyan Li", "Chenggong Zhao", "Zeyuan Zang", "Caixia Yuan", "Xiaojie Wang"], "title": "Reading Images Like Texts: Sequential Image Understanding in Vision-Language Models", "comment": null, "summary": "Vision-Language Models (VLMs) have demonstrated remarkable performance across\na variety of real-world tasks. However, existing VLMs typically process visual\ninformation by serializing images, a method that diverges significantly from\nthe parallel nature of human vision. Moreover, their opaque internal mechanisms\nhinder both deeper understanding and architectural innovation. Inspired by the\ndual-stream hypothesis of human vision, which distinguishes the \"what\" and\n\"where\" pathways, we deconstruct the visual processing in VLMs into object\nrecognition and spatial perception for separate study. For object recognition,\nwe convert images into text token maps and find that the model's perception of\nimage content unfolds as a two-stage process from shallow to deep layers,\nbeginning with attribute recognition and culminating in semantic\ndisambiguation. For spatial perception, we theoretically derive and empirically\nverify the geometric structure underlying the positional representation in\nVLMs. Based on these findings, we introduce an instruction-agnostic token\ncompression algorithm based on a plug-and-play visual decoder to improve\ndecoding efficiency, and a RoPE scaling technique to enhance spatial reasoning.\nThrough rigorous experiments, our work validates these analyses, offering a\ndeeper understanding of VLM internals and providing clear principles for\ndesigning more capable future architectures.", "AI": {"tldr": "\u8be5\u7814\u7a76\u53d7\u5230\u4eba\u7c7b\u89c6\u89c9\u53cc\u6d41\u5047\u8bf4\u7684\u542f\u53d1\uff0c\u5c06\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u89c6\u89c9\u5904\u7406\u5206\u89e3\u4e3a\u7269\u4f53\u8bc6\u522b\u548c\u7a7a\u95f4\u611f\u77e5\uff0c\u4ee5\u5206\u522b\u8fdb\u884c\u7814\u7a76\u3002", "motivation": "\u73b0\u6709VLM\u7684\u56fe\u50cf\u5904\u7406\u65b9\u5f0f\uff08\u4e32\u884c\u5316\uff09\u4e0e\u4eba\u7c7b\u89c6\u89c9\uff08\u5e76\u884c\u5316\uff09\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u4e14\u5176\u4e0d\u900f\u660e\u7684\u5185\u90e8\u673a\u5236\u963b\u788d\u4e86\u6df1\u5165\u7406\u89e3\u548c\u67b6\u6784\u521b\u65b0\u3002", "method": "\u5c06\u56fe\u50cf\u8f6c\u6362\u4e3a\u6587\u672c\u6807\u8bb0\u56fe\uff0c\u5206\u6790\u7269\u4f53\u8bc6\u522b\u7684\u201c\u5185\u5bb9-\u5c5e\u6027-\u8bed\u4e49\u201d\u4e24\u9636\u6bb5\u8fc7\u7a0b\uff1b\u7406\u8bba\u63a8\u5bfc\u5e76\u9a8c\u8bc1\u4e86VLM\u4e2d\u4f4d\u7f6e\u8868\u793a\u7684\u51e0\u4f55\u7ed3\u6784\uff0c\u4ee5\u7814\u7a76\u7a7a\u95f4\u611f\u77e5\u3002\u57fa\u4e8e\u8fd9\u4e9b\u53d1\u73b0\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5373\u63d2\u5373\u7528\u89c6\u89c9\u89e3\u7801\u5668\u7684\u6307\u4ee4\u65e0\u5173\u6807\u8bb0\u538b\u7f29\u7b97\u6cd5\uff0c\u5e76\u91c7\u7528RoPE\u7f29\u653e\u6280\u672f\u6765\u63d0\u9ad8\u89e3\u7801\u6548\u7387\u548c\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u3002", "result": "\u901a\u8fc7\u4e25\u683c\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u4e0a\u8ff0\u5206\u6790\uff0c\u52a0\u6df1\u4e86\u5bf9VLM\u5185\u90e8\u673a\u5236\u7684\u7406\u89e3\uff0c\u5e76\u4e3a\u672a\u6765\u8bbe\u8ba1\u66f4\u5f3a\u5927\u7684VLM\u67b6\u6784\u63d0\u4f9b\u4e86\u660e\u786e\u7684\u539f\u5219\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u89e3\u6784VLM\u7684\u89c6\u89c9\u5904\u7406\u673a\u5236\uff0c\u63d0\u51fa\u4e86\u6539\u8fdb\u89e3\u7801\u6548\u7387\u548c\u7a7a\u95f4\u63a8\u7406\u7684\u65b9\u6cd5\uff0c\u5e76\u4e3aVLM\u7684\u672a\u6765\u53d1\u5c55\u63d0\u4f9b\u4e86\u7406\u8bba\u6307\u5bfc\u548c\u5b9e\u8df5\u4f9d\u636e\u3002"}}
{"id": "2509.19084", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19084", "abs": "https://arxiv.org/abs/2509.19084", "authors": ["Asela Hevapathige"], "title": "Graph Neural Networks with Similarity-Navigated Probabilistic Feature Copying", "comment": null, "summary": "Graph Neural Networks (GNNs) have demonstrated remarkable success across\nvarious graph-based tasks. However, they face some fundamental limitations:\nfeature oversmoothing can cause node representations to become\nindistinguishable in deeper networks, they struggle to effectively manage\nheterogeneous relationships where connected nodes differ significantly, and\nthey process entire feature vectors as indivisible units, which limits\nflexibility. We seek to address these limitations. We propose AxelGNN, a novel\nGNN architecture inspired by Axelrod's cultural dissemination model that\naddresses these limitations through a unified framework. AxelGNN incorporates\nsimilarity-gated probabilistic interactions that adaptively promote convergence\nor divergence based on node similarity, implements trait-level copying\nmechanisms for fine-grained feature aggregation at the segment level, and\nmaintains global polarization to preserve node distinctiveness across multiple\nrepresentation clusters. The model's bistable convergence dynamics naturally\nhandle both homophilic and heterophilic graphs within a single architecture.\nExtensive experiments on node classification and influence estimation\nbenchmarks demonstrate that AxelGNN consistently outperforms or matches\nstate-of-the-art GNN methods across diverse graph structures with varying\nhomophily-heterophily characteristics.", "AI": {"tldr": "AxelGNN\u662f\u4e00\u79cd\u53d7Axelrod\u6587\u5316\u4f20\u64ad\u6a21\u578b\u542f\u53d1\u7684\u65b0\u578b\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u67b6\u6784\uff0c\u901a\u8fc7\u5f15\u5165\u76f8\u4f3c\u6027\u95e8\u63a7\u6982\u7387\u4ea4\u4e92\u3001\u57fa\u4e8e\u7279\u5f81\u7684\u590d\u5236\u673a\u5236\u548c\u5168\u5c40\u6781\u5316\u6765\u89e3\u51b3GNN\u4e2d\u7684\u7279\u5f81\u5e73\u6ed1\u3001\u5f02\u6784\u5173\u7cfb\u5904\u7406\u548c\u7279\u5f81\u5411\u91cf\u5b8c\u6574\u6027\u95ee\u9898\uff0c\u5e76\u5728\u8282\u70b9\u5206\u7c7b\u548c\u5f71\u54cd\u4f30\u8ba1\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u6216\u6301\u5e73\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u5728\u7279\u5f81\u5e73\u6ed1\u3001\u5f02\u6784\u5173\u7cfb\u5904\u7406\u548c\u7279\u5f81\u5411\u91cf\u7684\u6574\u4f53\u6027\u5904\u7406\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAxelGNN\u7684\u65b0\u578bGNN\u67b6\u6784\uff0c\u8be5\u67b6\u6784\u53d7Axelrod\u6587\u5316\u4f20\u64ad\u6a21\u578b\u7684\u542f\u53d1\uff0c\u901a\u8fc7\u4ee5\u4e0b\u65b9\u5f0f\u89e3\u51b3\u73b0\u6709GNN\u7684\u5c40\u9650\u6027\uff1a1. \u5f15\u5165\u76f8\u4f3c\u6027\u95e8\u63a7\u6982\u7387\u4ea4\u4e92\uff1a\u8be5\u673a\u5236\u6839\u636e\u8282\u70b9\u76f8\u4f3c\u6027\u81ea\u9002\u5e94\u5730\u4fc3\u8fdb\u6536\u655b\u6216\u53d1\u6563\u30022. \u5b9e\u73b0\u57fa\u4e8e\u7279\u5f81\u7684\u590d\u5236\u673a\u5236\uff1a\u5728\u7247\u6bb5\u7ea7\u522b\u8fdb\u884c\u7ec6\u7c92\u5ea6\u7684\u7279\u5f81\u805a\u5408\u30023. \u4fdd\u6301\u5168\u5c40\u6781\u5316\uff1a\u5728\u591a\u4e2a\u8868\u793a\u7c07\u4e2d\u4fdd\u6301\u8282\u70b9\u7684\u72ec\u7279\u6027\u3002\u8be5\u6a21\u578b\u7684\u53cc\u7a33\u6001\u6536\u655b\u52a8\u529b\u5b66\u80fd\u591f\u5728\u4e00\u4e2a\u67b6\u6784\u4e2d\u540c\u65f6\u5904\u7406\u540c\u8d28\u548c\u5f02\u8d28\u56fe\u3002", "result": "\u5728\u8282\u70b9\u5206\u7c7b\u548c\u5f71\u54cd\u4f30\u8ba1\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAxelGNN\u5728\u5404\u79cd\u540c\u8d28/\u5f02\u8d28\u7279\u5f81\u7684\u56fe\u7ed3\u6784\u4e0a\uff0c\u6301\u7eed\u4f18\u4e8e\u6216\u6301\u5e73\u4e8e\u6700\u5148\u8fdb\u7684GNN\u65b9\u6cd5\u3002", "conclusion": "AxelGNN\u901a\u8fc7\u5176\u65b0\u9896\u7684\u67b6\u6784\u548c\u673a\u5236\uff0c\u6210\u529f\u5730\u89e3\u51b3\u4e86\u73b0\u6709GNN\u9762\u4e34\u7684\u5173\u952e\u6311\u6218\uff0c\u5e76\u5728\u591a\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2509.19203", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19203", "abs": "https://arxiv.org/abs/2509.19203", "authors": ["Ioanna Ntinou", "Alexandros Xenos", "Yassine Ouali", "Adrian Bulat", "Georgios Tzimiropoulos"], "title": "Vision-Free Retrieval: Rethinking Multimodal Search with Textual Scene Descriptions", "comment": "Accepted at EMNLP 2025", "summary": "Contrastively-trained Vision-Language Models (VLMs), such as CLIP, have\nbecome the standard approach for learning discriminative vision-language\nrepresentations. However, these models often exhibit shallow language\nunderstanding, manifesting bag-of-words behaviour. These limitations are\nreinforced by their dual-encoder design, which induces a modality gap.\nAdditionally, the reliance on vast web-collected data corpora for training\nmakes the process computationally expensive and introduces significant privacy\nconcerns. To address these limitations, in this work, we challenge the\nnecessity of vision encoders for retrieval tasks by introducing a vision-free,\nsingle-encoder retrieval pipeline. Departing from the traditional text-to-image\nretrieval paradigm, we migrate to a text-to-text paradigm with the assistance\nof VLLM-generated structured image descriptions. We demonstrate that this\nparadigm shift has significant advantages, including a substantial reduction of\nthe modality gap, improved compositionality, and better performance on short\nand long caption queries, all attainable with only a few hours of calibration\non two GPUs. Additionally, substituting raw images with textual descriptions\nintroduces a more privacy-friendly alternative for retrieval. To further assess\ngeneralisation and address some of the shortcomings of prior compositionality\nbenchmarks, we release two benchmarks derived from Flickr30k and COCO,\ncontaining diverse compositional queries made of short captions, which we coin\nsubFlickr and subCOCO. Our vision-free retriever matches and often surpasses\ntraditional multimodal models. Importantly, our approach achieves\nstate-of-the-art zero-shot performance on multiple retrieval and\ncompositionality benchmarks, with models as small as 0.3B parameters. Code is\navailable at: https://github.com/IoannaNti/LexiCLIP", "AI": {"tldr": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e0d\u9700\u8981\u89c6\u89c9\u7f16\u7801\u5668\u7684\u3001\u4ec5\u57fa\u4e8e\u6587\u672c\u7684\u56fe\u50cf\u68c0\u7d22\u65b9\u6cd5\uff0c\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u56fe\u50cf\u63cf\u8ff0\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7406\u89e3\u80fd\u529b\u6d45\u3001\u6a21\u6001\u95f4\u9699\u5927\u3001\u8ba1\u7b97\u6210\u672c\u9ad8\u548c\u9690\u79c1\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u5bf9\u6bd4\u5b66\u4e60\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u867d\u7136\u5728\u5b66\u4e60\u533a\u5206\u6027\u89c6\u89c9-\u8bed\u8a00\u8868\u793a\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b58\u5728\u8bed\u8a00\u7406\u89e3\u80a4\u6d45\u3001\u6a21\u6001\u95f4\u9699\u5927\u3001\u8bad\u7ec3\u6210\u672c\u9ad8\u548c\u9690\u79c1\u95ee\u9898\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u4ec5\u4f9d\u8d56\u6587\u672c\u7684\u3001\u5355\u4e00\u7f16\u7801\u5668\u7684\u68c0\u7d22\u6d41\u7a0b\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u7ed3\u6784\u5316\u56fe\u50cf\u63cf\u8ff0\uff0c\u5c06\u68c0\u7d22\u4efb\u52a1\u4ece\u4f20\u7edf\u7684\u201c\u6587\u672c\u5230\u56fe\u50cf\u201d\u8f6c\u53d8\u4e3a\u201c\u6587\u672c\u5230\u6587\u672c\u201d\u3002", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u51cf\u5c0f\u4e86\u6a21\u6001\u95f4\u9699\uff0c\u63d0\u9ad8\u4e86\u7ec4\u5408\u6027\uff0c\u5728\u77ed\u6587\u672c\u548c\u957f\u6587\u672c\u67e5\u8be2\u65b9\u9762\u5747\u8868\u73b0\u66f4\u4f18\uff0c\u4e14\u8bad\u7ec3\u6210\u672c\u4f4e\u3002\u5728 Flickr30k \u548c COCO \u6570\u636e\u96c6\u4e0a\u521b\u5efa\u7684 subFlickr \u548c subCOCO \u4e24\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u68c0\u7d22\u548c\u7ec4\u5408\u6027\u65b9\u9762\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u96f6\u6837\u672c\u6027\u80fd\uff0c\u5373\u4f7f\u662f\u53c2\u6570\u91cf\u4ec5\u4e3a 0.3B \u7684\u5c0f\u578b\u6a21\u578b\u4e5f\u80fd\u505a\u5230\u3002", "conclusion": "\u6211\u4eec\u8bc1\u660e\u4e86\u4ec5\u57fa\u4e8e\u6587\u672c\u7684\u68c0\u7d22\u65b9\u6cd5\u5728\u6548\u7387\u3001\u6027\u80fd\u548c\u9690\u79c1\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u7684\u591a\u6a21\u6001\u65b9\u6cd5\uff0c\u5e76\u4e14\u5728\u5404\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u96f6\u6837\u672c\u7ed3\u679c\u3002"}}
{"id": "2509.19098", "categories": ["cs.LG", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2509.19098", "abs": "https://arxiv.org/abs/2509.19098", "authors": ["Adrien Prevost", "Timothee Mathieu", "Odalric-Ambrym Maillard"], "title": "Asymptotically Optimal Problem-Dependent Bandit Policies for Transfer Learning", "comment": null, "summary": "We study the non-contextual multi-armed bandit problem in a transfer learning\nsetting: before any pulls, the learner is given N'_k i.i.d. samples from each\nsource distribution nu'_k, and the true target distributions nu_k lie within a\nknown distance bound d_k(nu_k, nu'_k) <= L_k. In this framework, we first\nderive a problem-dependent asymptotic lower bound on cumulative regret that\nextends the classical Lai-Robbins result to incorporate the transfer parameters\n(d_k, L_k, N'_k). We then propose KL-UCB-Transfer, a simple index policy that\nmatches this new bound in the Gaussian case. Finally, we validate our approach\nvia simulations, showing that KL-UCB-Transfer significantly outperforms the\nno-prior baseline when source and target distributions are sufficiently close.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86KL-UCB-Transfer\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u8fc1\u79fb\u5b66\u4e60\u573a\u666f\u4e0b\u7684\u975e\u4e0a\u4e0b\u6587\u591a\u81c2\u8001\u864e\u673a\u95ee\u9898\uff0c\u5e76\u5728\u7406\u8bba\u548c\u5b9e\u8df5\u4e0a\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5728\u8fc1\u79fb\u5b66\u4e60\u7684\u8bbe\u5b9a\u4e0b\uff0c\u7814\u7a76\u975e\u4e0a\u4e0b\u6587\u591a\u81c2\u8001\u864e\u673a\u95ee\u9898\uff0c\u5e76\u8003\u8651\u6e90\u6570\u636e\u548c\u76ee\u6807\u6570\u636e\u4e4b\u95f4\u7684\u8ddd\u79bb\u7ea6\u675f\u3002", "method": "Derive a problem-dependent asymptotic lower bound on cumulative regret that extends the classical Lai-Robbins result to incorporate the transfer parameters (d_k, L_k, N'_k). Propose KL-UCB-Transfer, a simple index policy that matches this new bound in the Gaussian case.", "result": "\u63a8\u5bfc\u4e86\u8003\u8651\u8fc1\u79fb\u53c2\u6570\u7684\u7d2f\u79ef\u9057\u61be\u7684\u6e10\u8fd1\u4e0b\u754c\uff0c\u5e76\u63d0\u51fa\u4e86KL-UCB-Transfer\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u5728Gaussian\u60c5\u51b5\u4e0b\u5339\u914d\u6b64\u65b0\u754c\u9650\u3002", "conclusion": "KL-UCB-Transfer\u7b97\u6cd5\u5728\u6e90\u6570\u636e\u548c\u76ee\u6807\u6570\u636e\u8db3\u591f\u63a5\u8fd1\u65f6\uff0c\u663e\u8457\u4f18\u4e8e\u65e0\u5148\u9a8c\u57fa\u7ebf\u3002"}}
{"id": "2509.19207", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19207", "abs": "https://arxiv.org/abs/2509.19207", "authors": ["Israfel Salazar", "Desmond Elliott", "Yova Kementchedjhieva"], "title": "Long Story Short: Disentangling Compositionality and Long-Caption Understanding in VLMs", "comment": null, "summary": "Contrastive vision-language models (VLMs) have made significant progress in\nbinding visual and textual information, but understanding long, dense captions\nremains an open challenge. We hypothesize that compositionality, the capacity\nto reason about object-attribute bindings and inter-object relationships, is\nkey to understanding longer captions. In this paper, we investigate the\ninteraction between compositionality and long-caption understanding, asking\nwhether training for one property enhances the other. We train and evaluate a\nrange of models that target each of these capabilities. Our results reveal a\nbidirectional relationship: compositional training improves performance on\nlong-caption retrieval, and training on long captions promotes\ncompositionality. However, these gains are sensitive to data quality and model\ndesign. We find that training on poorly structured captions, or with limited\nparameter updates, fails to support generalization. Likewise, strategies that\naim at retaining general alignment, such as freezing positional embeddings, do\nnot improve compositional understanding. Overall, we find that compositional\nunderstanding and long-caption understanding are intertwined capabilities that\ncan be jointly learned through training on dense, grounded descriptions.\nDespite these challenges, we show that models trained on high-quality,\nlong-caption data can achieve strong performance in both tasks, offering\npractical guidance for improving VLM generalization.", "AI": {"tldr": "\u5bf9\u6bd4\u5b66\u4e60\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5728\u7ed3\u5408\u89c6\u89c9\u548c\u6587\u672c\u4fe1\u606f\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u7406\u89e3\u957f\u800c\u5bc6\u96c6\u7684\u6807\u9898\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002\u672c\u6587\u7814\u7a76\u4e86\u7ec4\u5408\u6027\uff08\u7406\u89e3\u5bf9\u8c61-\u5c5e\u6027\u7ed1\u5b9a\u548c\u5bf9\u8c61\u95f4\u5173\u7cfb\u7684\u80fd\u529b\uff09\u4e0e\u957f\u6807\u9898\u7406\u89e3\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5e76\u63d0\u51fa\u7ec4\u5408\u6027\u662f\u7406\u89e3\u957f\u6807\u9898\u7684\u5173\u952e\u3002\u901a\u8fc7\u8bad\u7ec3\u548c\u8bc4\u4f30\u9488\u5bf9\u8fd9\u4e24\u9879\u80fd\u529b\u7684\u6a21\u578b\uff0c\u7814\u7a76\u53d1\u73b0\u7ec4\u5408\u6027\u8bad\u7ec3\u53ef\u4ee5\u63d0\u9ad8\u957f\u6807\u9898\u68c0\u7d22\u6027\u80fd\uff0c\u800c\u957f\u6807\u9898\u8bad\u7ec3\u4e5f\u80fd\u4fc3\u8fdb\u7ec4\u5408\u6027\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u63d0\u5347\u5bf9\u6570\u636e\u8d28\u91cf\u548c\u6a21\u578b\u8bbe\u8ba1\u5f88\u654f\u611f\u3002\u7814\u7a76\u8fd8\u53d1\u73b0\uff0c\u5728\u7ed3\u6784\u4e0d\u826f\u7684\u6807\u9898\u4e0a\u8bad\u7ec3\u6216\u53c2\u6570\u66f4\u65b0\u6709\u9650\uff0c\u65e0\u6cd5\u652f\u6301\u6cdb\u5316\u3002\u540c\u6837\uff0c\u65e8\u5728\u4fdd\u7559\u901a\u7528\u5bf9\u9f50\u7684\u7b56\u7565\uff08\u5982\u51bb\u7ed3\u4f4d\u7f6e\u5d4c\u5165\uff09\u4e5f\u65e0\u6cd5\u63d0\u9ad8\u7ec4\u5408\u6027\u7406\u89e3\u80fd\u529b\u3002\u603b\u7684\u6765\u8bf4\uff0c\u7ec4\u5408\u6027\u7406\u89e3\u548c\u957f\u6807\u9898\u7406\u89e3\u662f\u76f8\u4e92\u4ea4\u7ec7\u7684\u80fd\u529b\uff0c\u53ef\u4ee5\u901a\u8fc7\u5728\u5bc6\u96c6\u7684\u3001\u6709\u6839\u636e\u7684\u63cf\u8ff0\u4e0a\u8fdb\u884c\u8bad\u7ec3\u6765\u5171\u540c\u5b66\u4e60\u3002\u5c3d\u7ba1\u5b58\u5728\u8fd9\u4e9b\u6311\u6218\uff0c\u4f46\u901a\u8fc7\u5728\u9ad8\u3001\u957f\u6807\u9898\u6570\u636e\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u6a21\u578b\u53ef\u4ee5\u5728\u4e24\u9879\u4efb\u52a1\u4e0a\u90fd\u53d6\u5f97\u4f18\u5f02\u8868\u73b0\uff0c\u4e3a\u63d0\u9ad8 VLM \u6cdb\u5316\u80fd\u529b\u63d0\u4f9b\u4e86\u5b9e\u9645\u6307\u5bfc\u3002", "motivation": "\u5bf9\u6bd4\u5b66\u4e60\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5728\u7ed3\u5408\u89c6\u89c9\u548c\u6587\u672c\u4fe1\u606f\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u7406\u89e3\u957f\u800c\u5bc6\u96c6\u7684\u6807\u9898\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002\u672c\u6587\u7814\u7a76\u4e86\u7ec4\u5408\u6027\uff08\u7406\u89e3\u5bf9\u8c61-\u5c5e\u6027\u7ed1\u5b9a\u548c\u5bf9\u8c61\u95f4\u5173\u7cfb\u7684\u80fd\u529b\uff09\u4e0e\u957f\u6807\u9898\u7406\u89e3\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5e76\u63d0\u51fa\u7ec4\u5408\u6027\u662f\u7406\u89e3\u957f\u6807\u9898\u7684\u5173\u952e\u3002", "method": "\u901a\u8fc7\u8bad\u7ec3\u548c\u8bc4\u4f30\u9488\u5bf9\u7ec4\u5408\u6027\u548c\u957f\u6807\u9898\u7406\u89e3\u80fd\u529b\u7684\u4e00\u7cfb\u5217\u6a21\u578b\u3002", "result": "\u7ec4\u5408\u6027\u8bad\u7ec3\u53ef\u4ee5\u63d0\u9ad8\u957f\u6807\u9898\u68c0\u7d22\u6027\u80fd\uff0c\u800c\u957f\u6807\u9898\u8bad\u7ec3\u4e5f\u80fd\u4fc3\u8fdb\u7ec4\u5408\u6027\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u63d0\u5347\u5bf9\u6570\u636e\u8d28\u91cf\u548c\u6a21\u578b\u8bbe\u8ba1\u5f88\u654f\u611f\u3002\u5728\u7ed3\u6784\u4e0d\u826f\u7684\u6807\u9898\u4e0a\u8bad\u7ec3\u6216\u53c2\u6570\u66f4\u65b0\u6709\u9650\uff0c\u65e0\u6cd5\u652f\u6301\u6cdb\u5316\u3002\u65e8\u5728\u4fdd\u7559\u901a\u7528\u5bf9\u9f50\u7684\u7b56\u7565\uff08\u5982\u51bb\u7ed3\u4f4d\u7f6e\u5d4c\u5165\uff09\u4e5f\u65e0\u6cd5\u63d0\u9ad8\u7ec4\u5408\u6027\u7406\u89e3\u80fd\u529b\u3002", "conclusion": "\u7ec4\u5408\u6027\u7406\u89e3\u548c\u957f\u6807\u9898\u7406\u89e3\u662f\u76f8\u4e92\u4ea4\u7ec7\u7684\u80fd\u529b\uff0c\u53ef\u4ee5\u901a\u8fc7\u5728\u5bc6\u96c6\u7684\u3001\u6709\u6839\u636e\u7684\u63cf\u8ff0\u4e0a\u8fdb\u884c\u8bad\u7ec3\u6765\u5171\u540c\u5b66\u4e60\u3002\u901a\u8fc7\u5728\u9ad8\u3001\u957f\u6807\u9898\u6570\u636e\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u6a21\u578b\u53ef\u4ee5\u5728\u4e24\u9879\u4efb\u52a1\u4e0a\u90fd\u53d6\u5f97\u4f18\u5f02\u8868\u73b0\uff0c\u4e3a\u63d0\u9ad8 VLM \u6cdb\u5316\u80fd\u529b\u63d0\u4f9b\u4e86\u5b9e\u9645\u6307\u5bfc\u3002"}}
{"id": "2509.19100", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19100", "abs": "https://arxiv.org/abs/2509.19100", "authors": ["Alexander Robey"], "title": "Algorithms for Adversarially Robust Deep Learning", "comment": "PhD thesis", "summary": "Given the widespread use of deep learning models in safety-critical\napplications, ensuring that the decisions of such models are robust against\nadversarial exploitation is of fundamental importance. In this thesis, we\ndiscuss recent progress toward designing algorithms that exhibit desirable\nrobustness properties. First, we discuss the problem of adversarial examples in\ncomputer vision, for which we introduce new technical results, training\nparadigms, and certification algorithms. Next, we consider the problem of\ndomain generalization, wherein the task is to train neural networks to\ngeneralize from a family of training distributions to unseen test\ndistributions. We present new algorithms that achieve state-of-the-art\ngeneralization in medical imaging, molecular identification, and image\nclassification. Finally, we study the setting of jailbreaking large language\nmodels (LLMs), wherein an adversarial user attempts to design prompts that\nelicit objectionable content from an LLM. We propose new attacks and defenses,\nwhich represent the frontier of progress toward designing robust language-based\nagents.", "AI": {"tldr": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u8d8a\u6765\u8d8a\u666e\u53ca\uff0c\u56e0\u6b64\u786e\u4fdd\u5176\u51b3\u7b56\u80fd\u591f\u62b5\u5fa1\u5bf9\u6297\u6027\u653b\u51fb\u81f3\u5173\u91cd\u8981\u3002\u672c\u8bba\u6587\u63a2\u8ba8\u4e86\u65e8\u5728\u63d0\u9ad8\u6a21\u578b\u9c81\u68d2\u6027\u7684\u7b97\u6cd5\u7684\u6700\u65b0\u8fdb\u5c55\u3002\u6211\u4eec\u9996\u5148\u8ba8\u8bba\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u5bf9\u6297\u6027\u6837\u672c\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u65b0\u7684\u6280\u672f\u6210\u679c\u3001\u8bad\u7ec3\u8303\u5f0f\u548c\u8ba4\u8bc1\u7b97\u6cd5\u3002\u7136\u540e\uff0c\u6211\u4eec\u7814\u7a76\u9886\u57df\u6cdb\u5316\u95ee\u9898\uff0c\u5373\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u4ee5\u9002\u5e94\u672a\u89c1\u8fc7\u7684\u6d4b\u8bd5\u5206\u5e03\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u5728\u533b\u5b66\u6210\u50cf\u3001\u5206\u5b50\u8bc6\u522b\u548c\u56fe\u50cf\u5206\u7c7b\u7b49\u9886\u57df\u8fbe\u5230\u6700\u5148\u8fdb\u6cdb\u5316\u80fd\u529b\u7684\u65b0\u7b97\u6cd5\u3002\u6700\u540e\uff0c\u6211\u4eec\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u8d8a\u72f1\u95ee\u9898\uff0c\u5373\u653b\u51fb\u8005\u8bd5\u56fe\u8bf1\u5bfc LLM \u751f\u6210\u4e0d\u5f53\u5185\u5bb9\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u65b0\u7684\u653b\u51fb\u548c\u9632\u5fa1\u65b9\u6cd5\uff0c\u4ee5\u63a8\u52a8\u57fa\u4e8e\u8bed\u8a00\u7684\u667a\u80fd\u4f53\u5728\u9c81\u68d2\u6027\u65b9\u9762\u7684\u53d1\u5c55\u3002", "motivation": "\u786e\u4fdd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u7684\u51b3\u7b56\u80fd\u591f\u62b5\u5fa1\u5bf9\u6297\u6027\u653b\u51fb\uff0c\u5e76\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u3001\u9886\u57df\u6cdb\u5316\uff08\u7279\u522b\u662f\u5728\u533b\u5b66\u6210\u50cf\u3001\u5206\u5b50\u8bc6\u522b\u548c\u56fe\u50cf\u5206\u7c7b\u4e2d\uff09\u4ee5\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8d8a\u72f1\u95ee\u9898\u4e0a\u5b9e\u73b0\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u6280\u672f\u6210\u679c\u3001\u8bad\u7ec3\u8303\u5f0f\u548c\u8ba4\u8bc1\u7b97\u6cd5\u6765\u89e3\u51b3\u5bf9\u6297\u6027\u6837\u672c\u95ee\u9898\u3002\u5728\u9886\u57df\u6cdb\u5316\u65b9\u9762\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u7b97\u6cd5\u4ee5\u63d0\u9ad8\u5728\u533b\u5b66\u6210\u50cf\u3001\u5206\u5b50\u8bc6\u522b\u548c\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u65b9\u9762\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u653b\u51fb\u548c\u9632\u5fa1\u65b9\u6cd5\u6765\u5e94\u5bf9\u8d8a\u72f1\u95ee\u9898\u3002", "result": "\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u53d6\u5f97\u4e86\u5173\u4e8e\u5bf9\u6297\u6027\u6837\u672c\u7684\u65b0\u6280\u672f\u6210\u679c\u3001\u8bad\u7ec3\u8303\u5f0f\u548c\u8ba4\u8bc1\u7b97\u6cd5\u3002\u5728\u9886\u57df\u6cdb\u5316\u65b9\u9762\uff0c\u63d0\u51fa\u7684\u65b0\u7b97\u6cd5\u5728\u533b\u5b66\u6210\u50cf\u3001\u5206\u5b50\u8bc6\u522b\u548c\u56fe\u50cf\u5206\u7c7b\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6cdb\u5316\u80fd\u529b\u3002\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u65b9\u9762\uff0c\u63d0\u51fa\u7684\u65b0\u653b\u51fb\u548c\u9632\u5fa1\u65b9\u6cd5\u4ee3\u8868\u4e86\u5728\u8bbe\u8ba1\u9c81\u68d2\u8bed\u8a00\u667a\u80fd\u4f53\u65b9\u9762\u7684\u524d\u6cbf\u8fdb\u5c55\u3002", "conclusion": "\u672c\u8bba\u6587\u5728\u63d0\u9ad8\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08\u5305\u62ec\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff09\u7684\u9c81\u68d2\u6027\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u5e76\u5728\u9886\u57df\u6cdb\u5316\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2509.19208", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19208", "abs": "https://arxiv.org/abs/2509.19208", "authors": ["Earl Ranario", "Ismael Mayanja", "Heesup Yun", "Brian N. Bailey", "J. Mason Earles"], "title": "Enabling Plant Phenotyping in Weedy Environments using Multi-Modal Imagery via Synthetic and Generated Training Data", "comment": null, "summary": "Accurate plant segmentation in thermal imagery remains a significant\nchallenge for high throughput field phenotyping, particularly in outdoor\nenvironments where low contrast between plants and weeds and frequent\nocclusions hinder performance. To address this, we present a framework that\nleverages synthetic RGB imagery, a limited set of real annotations, and\nGAN-based cross-modality alignment to enhance semantic segmentation in thermal\nimages. We trained models on 1,128 synthetic images containing complex mixtures\nof crop and weed plants in order to generate image segmentation masks for crop\nand weed plants. We additionally evaluated the benefit of integrating as few as\nfive real, manually segmented field images within the training process using\nvarious sampling strategies. When combining all the synthetic images with a few\nlabeled real images, we observed a maximum relative improvement of 22% for the\nweed class and 17% for the plant class compared to the full real-data baseline.\nCross-modal alignment was enabled by translating RGB to thermal using\nCycleGAN-turbo, allowing robust template matching without calibration. Results\ndemonstrated that combining synthetic data with limited manual annotations and\ncross-domain translation via generative models can significantly boost\nsegmentation performance in complex field environments for multi-model imagery.", "AI": {"tldr": "\u5229\u7528\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GAN\uff09\u548c\u5408\u6210\u6570\u636e\u63d0\u9ad8\u70ed\u6210\u50cf\u4e2d\u690d\u7269\u5206\u5272\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u5728\u6237\u5916\u73af\u5883\u4e2d\uff0c\u4f4e\u5bf9\u6bd4\u5ea6\u548c\u9891\u7e41\u7684\u906e\u6321\u4f7f\u5f97\u5728\u70ed\u6210\u50cf\u4e2d\u51c6\u786e\u5206\u5272\u690d\u7269\u6210\u4e3a\u9ad8\u901a\u91cf\u7530\u95f4\u8868\u578b\u5206\u6790\u7684\u4e00\u5927\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5408\u6210RGB\u56fe\u50cf\u3001\u5c11\u91cf\u771f\u5b9e\u6807\u6ce8\u6570\u636e\u548c\u57fa\u4e8eGAN\u7684\u8de8\u6a21\u6001\u5bf9\u9f50\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u589e\u5f3a\u70ed\u6210\u50cf\u4e2d\u7684\u8bed\u4e49\u5206\u5272\u3002\u5229\u7528CycleGAN-turbo\u5c06RGB\u56fe\u50cf\u8f6c\u6362\u4e3a\u70ed\u6210\u50cf\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u6821\u51c6\u7684\u9c81\u68d2\u6a21\u677f\u5339\u914d\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u548c\u5c11\u91cf\u771f\u5b9e\u6807\u6ce8\u6570\u636e\uff08\u5305\u62ec5\u5f20\u771f\u5b9e\u56fe\u50cf\uff09\u7684\u7ed3\u5408\u4e0b\uff0c\u76f8\u6bd4\u4ec5\u4f7f\u7528\u771f\u5b9e\u6570\u636e\u7684\u57fa\u7ebf\u6a21\u578b\uff0c\u6742\u8349\u5206\u5272\u7cbe\u5ea6\u76f8\u5bf9\u63d0\u9ad8\u4e8622%\uff0c\u690d\u7269\u5206\u5272\u7cbe\u5ea6\u76f8\u5bf9\u63d0\u9ad8\u4e8617%\u3002", "conclusion": "\u5c06\u5408\u6210\u6570\u636e\u3001\u6709\u9650\u7684\u624b\u52a8\u6807\u6ce8\u548c\u751f\u6210\u6a21\u578b\u7684\u8de8\u57df\u7ffb\u8bd1\u76f8\u7ed3\u5408\uff0c\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u5728\u590d\u6742\u7530\u95f4\u73af\u5883\u4e2d\u8fdb\u884c\u591a\u6a21\u6001\u56fe\u50cf\u5206\u5272\u7684\u6027\u80fd\u3002"}}
{"id": "2509.19104", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.19104", "abs": "https://arxiv.org/abs/2509.19104", "authors": ["Sharan Sahu", "Martin T. Wells"], "title": "DRO-REBEL: Distributionally Robust Relative-Reward Regression for Fast and Efficient LLM Alignment", "comment": "70 pages, 9 figures, 3 tables", "summary": "Reinforcement learning with human feedback (RLHF) has become crucial for\naligning Large Language Models (LLMs) with human intent. However, existing\noffline RLHF approaches suffer from overoptimization, where models overfit to\nreward misspecification and drift from preferred behaviors observed during\ntraining. We introduce DRO-REBEL, a unified family of robust REBEL updates with\ntype-$p$ Wasserstein, KL, and $\\chi^2$ ambiguity sets. Using Fenchel duality,\neach update reduces to a simple relative-reward regression, preserving\nscalability and avoiding PPO-style clipping or auxiliary value networks. Under\nstandard linear-reward and log-linear policy classes with a data-coverage\ncondition, we establish $O(n^{-1/4})$ estimation bounds with tighter constants\nthan prior DRO-DPO approaches, and recover the minimax-optimal $O(n^{-1/2})$\nrate via a localized Rademacher complexity analysis. The same analysis closes\nthe gap for Wasserstein-DPO and KL-DPO, showing both also attain optimal\nparametric rates. We derive practical SGD algorithms for all three divergences:\ngradient regularization (Wasserstein), importance weighting (KL), and a fast\n1-D dual solve ($\\chi^2$). Experiments on Emotion Alignment, the large-scale\nArmoRM multi-objective benchmark, and HH-Alignment demonstrate strong\nworst-case robustness across unseen preference mixtures, model sizes, and data\nscales, with $\\chi^2$-REBEL showing consistently strong empirical performance.\nA controlled radius--coverage study validates a no-free-lunch trade-off: radii\nshrinking faster than empirical divergence concentration rates achieve\nminimax-optimal parametric rates but forfeit coverage, while\ncoverage-guaranteeing radii incur $O(n^{-1/4})$ rates.", "AI": {"tldr": "RLHF \u9886\u57df\u7684\u73b0\u6709\u79bb\u7ebf\u65b9\u6cd5\u5b58\u5728\u8fc7\u4f18\u5316\u95ee\u9898\uff0c\u4f1a\u5bfc\u81f4\u6a21\u578b\u8fc7\u62df\u5408\u5956\u52b1\u9519\u8bef\u6307\u5b9a\u5e76\u504f\u79bb\u8bad\u7ec3\u671f\u95f4\u89c2\u5bdf\u5230\u7684\u9996\u9009\u884c\u4e3a\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86 DRO-REBEL\uff0c\u8fd9\u662f\u4e00\u7c7b\u7edf\u4e00\u7684\u3001\u57fa\u4e8e Wasserstein\u3001KL \u548c \u03c72 \u6a21\u7cca\u96c6\u7684\u9c81\u68d2 REBEL \u66f4\u65b0\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u79bb\u7ebf RLHF \u65b9\u6cd5\u5728\u6a21\u578b\u5bf9\u5956\u52b1\u7684\u9519\u8bef\u6307\u5b9a\u4e0a\u8fc7\u62df\u5408\uff0c\u4ee5\u53ca\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u504f\u79bb\u9996\u9009\u884c\u4e3a\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a DRO-REBEL \u7684\u65b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5305\u542b\u57fa\u4e8e Wasserstein\u3001KL \u548c \u03c72 \u6a21\u7cca\u96c6\u7684\u9c81\u68d2 REBEL \u66f4\u65b0\u3002\u8be5\u65b9\u6cd5\u5229\u7528 Fenchel \u5bf9\u5076\u5c06\u66f4\u65b0\u7b80\u5316\u4e3a\u76f8\u5bf9\u5956\u52b1\u56de\u5f52\uff0c\u907f\u514d\u4e86 PPO \u98ce\u683c\u7684\u88c1\u526a\u6216\u8f85\u52a9\u503c\u7f51\u7edc\uff0c\u5e76\u5177\u6709\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u3002", "result": "\u5728\u6807\u51c6\u7ebf\u6027\u5956\u52b1\u548c\u5bf9\u6570\u7ebf\u6027\u7b56\u7565\u7c7b\u522b\u4ee5\u53ca\u6570\u636e\u8986\u76d6\u6761\u4ef6\u4e0b\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5177\u6709 O(n^{-1/4}) \u7684\u4f30\u8ba1\u754c\u9650\uff0c\u4f18\u4e8e\u73b0\u6709\u7684 DRO-DPO \u65b9\u6cd5\u3002\u901a\u8fc7\u5c40\u90e8 Rademacher \u590d\u6742\u6027\u5206\u6790\uff0c\u5b9e\u73b0\u4e86 O(n^{-1/2}) \u7684 minimax-optimal \u901f\u7387\u3002\u5728 Emotion Alignment\u3001ArmoRM \u548c HH-Alignment \u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u9762\u5bf9\u672a\u89c1\u7684\u504f\u597d\u6df7\u5408\u3001\u6a21\u578b\u5927\u5c0f\u548c\u6570\u636e\u5c3a\u5ea6\u65f6\uff0c\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6700\u574f\u60c5\u51b5\u9c81\u68d2\u6027\u3002", "conclusion": "DRO-REBEL \u63d0\u4f9b\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u3001\u9c81\u68d2\u7684 RLHF \u66f4\u65b0\u65b9\u6cd5\uff0c\u5728\u7406\u8bba\u548c\u5b9e\u8df5\u4e0a\u90fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u63ed\u793a\u4e86\u534a\u5f84-\u8986\u76d6\u7387\u6743\u8861\u4e2d\u7684\u201c\u6ca1\u6709\u514d\u8d39\u5348\u9910\u201d\u73b0\u8c61\u3002"}}
{"id": "2509.19218", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19218", "abs": "https://arxiv.org/abs/2509.19218", "authors": ["Yunzhi Xu", "Yushuang Ding", "Hu Sun", "Hongxi Zhang", "Li Zhao"], "title": "HyKid: An Open MRI Dataset with Expert-Annotated Multi-Structure and Choroid Plexus in Pediatric Hydrocephalus", "comment": "10 pages, 7 figures", "summary": "Evaluation of hydrocephalus in children is challenging, and the related\nresearch is limited by a lack of publicly available, expert-annotated datasets,\nparticularly those with segmentation of the choroid plexus. To address this, we\npresent HyKid, an open-source dataset from 48 pediatric patients with\nhydrocephalus. 3D MRIs were provided with 1mm isotropic resolution, which was\nreconstructed from routine low-resolution images using a slice-to-volume\nalgorithm. Manually corrected segmentations of brain tissues, including white\nmatter, grey matter, lateral ventricle, external CSF, and the choroid plexus,\nwere provided by an experienced neurologist. Additionally, structured data was\nextracted from clinical radiology reports using a Retrieval-Augmented\nGeneration framework. The strong correlation between choroid plexus volume and\ntotal CSF volume provided a potential biomarker for hydrocephalus evaluation,\nachieving excellent performance in a predictive model (AUC = 0.87). The\nproposed HyKid dataset provided a high-quality benchmark for neuroimaging\nalgorithms development, and it revealed the choroid plexus-related features in\nhydrocephalus assessments. Our datasets are publicly available at\nhttps://www.synapse.org/Synapse:syn68544889.", "AI": {"tldr": "HyKid\u662f\u4e00\u4e2a\u516c\u5f00\u7684\u6570\u636e\u96c6\uff0c\u5305\u542b48\u540d\u60a3\u6709\u8111\u79ef\u6c34\u7684\u513f\u7ae5\u76843D MRI\u6570\u636e\uff0c\u5e76\u63d0\u4f9b\u4e86\u8be6\u7ec6\u7684\u8111\u7ec4\u7ec7\u548c\u8109\u7edc\u4e1b\u5206\u5272\uff0c\u4ee5\u53ca\u4ece\u4e34\u5e8a\u62a5\u544a\u4e2d\u63d0\u53d6\u7684\u7ed3\u6784\u5316\u6570\u636e\u3002\u8be5\u6570\u636e\u96c6\u6709\u52a9\u4e8e\u8111\u79ef\u6c34\u8bc4\u4f30\uff0c\u7279\u522b\u662f\u8109\u7edc\u4e1b\u4f53\u79ef\u4e0e\u603b\u8111\u810a\u6db2\u4f53\u79ef\u7684\u76f8\u5173\u6027\uff0c\u53ef\u4f5c\u4e3a\u9884\u6d4b\u6a21\u578b\u4e2d\u7684\u6f5c\u5728\u751f\u7269\u6807\u5fd7\u7269\u3002", "motivation": "\u73b0\u6709\u7684\u8111\u79ef\u6c34\u7814\u7a76\u56e0\u7f3a\u4e4f\u516c\u5f00\u7684\u3001\u4e13\u5bb6\u6807\u6ce8\u7684\u6570\u636e\u96c6\uff08\u5c24\u5176\u662f\u5e26\u6709\u8109\u7edc\u4e1b\u5206\u5272\u7684\u6570\u636e\u96c6\uff09\u800c\u53d7\u5230\u9650\u5236\u3002", "method": "\u521b\u5efa\u4e86\u4e00\u4e2a\u5305\u542b48\u540d\u8111\u79ef\u6c34\u60a3\u513f\u76843D MRI\u6570\u636e\u96c6\uff08HyKid\uff09\uff0c\u5e76\u7531\u7ecf\u9a8c\u4e30\u5bcc\u7684\u795e\u7ecf\u79d1\u533b\u751f\u5bf9\u624b\u52a8\u6821\u6b63\u7684\u8111\u7ec4\u7ec7\uff08\u5305\u62ec\u767d\u8d28\u3001\u7070\u8d28\u3001\u4fa7\u8111\u5ba4\u3001\u5916\u90e8\u8111\u810a\u6db2\u548c\u8109\u7edc\u4e1b\uff09\u8fdb\u884c\u4e86\u5206\u5272\u3002\u4f7f\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6846\u67b6\u4ece\u4e34\u5e8a\u653e\u5c04\u5b66\u62a5\u544a\u4e2d\u63d0\u53d6\u7ed3\u6784\u5316\u6570\u636e\u3002", "result": "\u8109\u7edc\u4e1b\u4f53\u79ef\u4e0e\u603b\u8111\u810a\u6db2\u4f53\u79ef\u4e4b\u95f4\u5b58\u5728\u5f3a\u76f8\u5173\u6027\uff0c\u53ef\u4f5c\u4e3a\u8111\u79ef\u6c34\u8bc4\u4f30\u7684\u6f5c\u5728\u751f\u7269\u6807\u5fd7\u7269\uff0c\u9884\u6d4b\u6a21\u578b\u8868\u73b0\u4f18\u5f02\uff08AUC = 0.87\uff09\u3002", "conclusion": "\u63d0\u51fa\u7684HyKid\u6570\u636e\u96c6\u4e3a\u795e\u7ecf\u5f71\u50cf\u7b97\u6cd5\u5f00\u53d1\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u7684\u57fa\u51c6\uff0c\u5e76\u63ed\u793a\u4e86\u8109\u7edc\u4e1b\u76f8\u5173\u7684\u7279\u5f81\u5728\u8111\u79ef\u6c34\u8bc4\u4f30\u4e2d\u7684\u4f5c\u7528\u3002"}}
{"id": "2509.19112", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19112", "abs": "https://arxiv.org/abs/2509.19112", "authors": ["Hugo Math", "Rainer Lienhart"], "title": "Towards Practical Multi-label Causal Discovery in High-Dimensional Event Sequences via One-Shot Graph Aggregation", "comment": "Accepted at NeuRIPS2025 Workshop on Structured Probabilistic\n  Inference and Generative Modeling", "summary": "Understanding causality in event sequences where outcome labels such as\ndiseases or system failures arise from preceding events like symptoms or error\ncodes is critical. Yet remains an unsolved challenge across domains like\nhealthcare or vehicle diagnostics. We introduce CARGO, a scalable multi-label\ncausal discovery method for sparse, high-dimensional event sequences comprising\nof thousands of unique event types. Using two pretrained causal Transformers as\ndomain-specific foundation models for event sequences. CARGO infers in\nparallel, per sequence one-shot causal graphs and aggregates them using an\nadaptive frequency fusion to reconstruct the global Markov boundaries of\nlabels. This two-stage approach enables efficient probabilistic reasoning at\nscale while bypassing the intractable cost of full-dataset conditional\nindependence testing. Our results on a challenging real-world automotive fault\nprediction dataset with over 29,100 unique event types and 474 imbalanced\nlabels demonstrate CARGO's ability to perform structured reasoning.", "AI": {"tldr": "CARGO\u662f\u4e00\u79cd\u7528\u4e8e\u7a00\u758f\u3001\u9ad8\u7ef4\u4e8b\u4ef6\u5e8f\u5217\u7684\u591a\u6807\u7b7e\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\uff0c\u5b83\u5229\u7528\u9884\u8bad\u7ec3\u7684\u56e0\u679cTransformer\u6765\u63a8\u65ad\u56e0\u679c\u56fe\uff0c\u5e76\u901a\u8fc7\u81ea\u9002\u5e94\u9891\u7387\u878d\u5408\u8fdb\u884c\u805a\u5408\uff0c\u4ee5\u91cd\u5efa\u6807\u7b7e\u7684\u5168\u5c40\u9a6c\u5c14\u53ef\u592b\u8fb9\u754c\u3002", "motivation": "\u7406\u89e3\u4e8b\u4ef6\u5e8f\u5217\u4e2d\u7684\u56e0\u679c\u5173\u7cfb\uff0c\u7279\u522b\u662f\u50cf\u75be\u75c5\u6216\u7cfb\u7edf\u6545\u969c\u8fd9\u6837\u7684\u7ed3\u679c\u6807\u7b7e\uff0c\u4ee5\u53ca\u50cf\u75c7\u72b6\u6216\u9519\u8bef\u4ee3\u7801\u8fd9\u6837\u7684\u524d\u7f6e\u4e8b\u4ef6\uff0c\u5bf9\u4e8e\u533b\u7597\u4fdd\u5065\u6216\u8f66\u8f86\u8bca\u65ad\u7b49\u9886\u57df\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4ecd\u662f\u4e00\u4e2a\u672a\u89e3\u51b3\u7684\u6311\u6218\u3002", "method": "CARGO\u5229\u7528\u4e24\u4e2a\u9884\u8bad\u7ec3\u7684\u56e0\u679cTransformer\u4f5c\u4e3a\u4e8b\u4ef6\u5e8f\u5217\u7684\u9886\u57df\u7279\u5b9a\u57fa\u7840\u6a21\u578b\u3002\u5b83\u5e76\u884c\u63a8\u65ad\u6bcf\u4e2a\u5e8f\u5217\u7684\u5355\u6b21\u56e0\u679c\u56fe\uff0c\u5e76\u4f7f\u7528\u81ea\u9002\u5e94\u9891\u7387\u878d\u5408\u8fdb\u884c\u805a\u5408\uff0c\u4ee5\u91cd\u5efa\u6807\u7b7e\u7684\u5168\u5c40\u9a6c\u5c14\u53ef\u592b\u8fb9\u754c\u3002\u8fd9\u79cd\u4e24\u9636\u6bb5\u65b9\u6cd5\u80fd\u591f\u5728\u89c4\u6a21\u4e0a\u8fdb\u884c\u6709\u6548\u7684\u6982\u7387\u63a8\u7406\uff0c\u540c\u65f6\u907f\u514d\u4e86\u5bf9\u6574\u4e2a\u6570\u636e\u96c6\u8fdb\u884c\u6761\u4ef6\u72ec\u7acb\u6027\u6d4b\u8bd5\u7684\u4e0d\u53ef\u884c\u6210\u672c\u3002", "result": "\u5728\u5177\u6709\u8d85\u8fc729,100\u4e2a\u552f\u4e00\u4e8b\u4ef6\u7c7b\u578b\u548c474\u4e2a\u4e0d\u5e73\u8861\u6807\u7b7e\u7684\u5177\u6709\u6311\u6218\u6027\u7684\u771f\u5b9e\u4e16\u754c\u6c7d\u8f66\u6545\u969c\u9884\u6d4b\u6570\u636e\u96c6\u4e0a\uff0cCARGO\u5c55\u793a\u4e86\u5176\u6267\u884c\u7ed3\u6784\u5316\u63a8\u7406\u7684\u80fd\u529b\u3002", "conclusion": "CARGO\u662f\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u591a\u6807\u7b7e\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\uff0c\u80fd\u591f\u5904\u7406\u7a00\u758f\u3001\u9ad8\u7ef4\u4e8b\u4ef6\u5e8f\u5217\uff0c\u5e76\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4e0a\u6709\u6548\u5730\u8fdb\u884c\u56e0\u679c\u63a8\u7406\u3002"}}
{"id": "2509.19227", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19227", "abs": "https://arxiv.org/abs/2509.19227", "authors": ["Tongshuai Wu", "Chao Lu", "Ze Song", "Yunlong Lin", "Sizhe Fan", "Xuemei Chen"], "title": "MsFIN: Multi-scale Feature Interaction Network for Traffic Accident Anticipation", "comment": null, "summary": "With the widespread deployment of dashcams and advancements in computer\nvision, developing accident prediction models from the dashcam perspective has\nbecome critical for proactive safety interventions. However, two key challenges\npersist: modeling feature-level interactions among traffic participants (often\noccluded in dashcam views) and capturing complex, asynchronous multi-temporal\nbehavioral cues preceding accidents. To deal with these two challenges, a\nMulti-scale Feature Interaction Network (MsFIN) is proposed for early-stage\naccident anticipation from dashcam videos. MsFIN has three layers for\nmulti-scale feature aggregation, temporal feature processing and multi-scale\nfeature post fusion, respectively. For multi-scale feature aggregation, a\nMulti-scale Module is designed to extract scene representations at short-term,\nmid-term and long-term temporal scales. Meanwhile, the Transformer architecture\nis leveraged to facilitate comprehensive feature interactions. Temporal feature\nprocessing captures the sequential evolution of scene and object features under\ncausal constraints. In the multi-scale feature post fusion stage, the network\nfuses scene and object features across multiple temporal scales to generate a\ncomprehensive risk representation. Experiments on DAD and DADA datasets show\nthat MsFIN significantly outperforms state-of-the-art models with single-scale\nfeature extraction in both prediction correctness and earliness. Ablation\nstudies validate the effectiveness of each module in MsFIN, highlighting how\nthe network achieves superior performance through multi-scale feature fusion\nand contextual interaction modeling.", "AI": {"tldr": "\u672c\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMsFIN\u7684\u591a\u5c3a\u5ea6\u7279\u5f81\u4ea4\u4e92\u7f51\u7edc\uff0c\u7528\u4e8e\u4ece\u884c\u8f66\u8bb0\u5f55\u4eea\u89c6\u9891\u4e2d\u63d0\u524d\u9884\u6d4b\u4ea4\u901a\u4e8b\u6545\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u5728\u5904\u7406\u906e\u6321\u3001\u591a\u65f6\u95f4\u5c3a\u5ea6\u884c\u4e3a\u7ebf\u7d22\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u4e3a\u4e86\u5e94\u5bf9\u884c\u8f66\u8bb0\u5f55\u4eea\u5e7f\u6cdb\u90e8\u7f72\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u6280\u672f\u8fdb\u6b65\u7684\u80cc\u666f\u4e0b\uff0c\u4ece\u884c\u8f66\u8bb0\u5f55\u4eea\u89c6\u89d2\u9884\u6d4b\u4e8b\u6545\u4ee5\u5b9e\u73b0\u4e3b\u52a8\u5b89\u5168\u5e72\u9884\u7684\u5173\u952e\u9700\u6c42\uff0c\u540c\u65f6\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u5efa\u6a21\u4ea4\u901a\u53c2\u4e0e\u8005\u7279\u5f81\u4ea4\u4e92\uff08\u5e38\u88ab\u906e\u6321\uff09\u548c\u6355\u6349\u4e8b\u6545\u524d\u590d\u6742\u3001\u5f02\u6b65\u7684\u591a\u65f6\u5e8f\u884c\u4e3a\u7ebf\u7d22\u65b9\u9762\u7684\u4e24\u5927\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMsFIN\uff08Multi-scale Feature Interaction Network\uff09\u7684\u7f51\u7edc\uff0c\u8be5\u7f51\u7edc\u5305\u542b\u4e09\u4e2a\u5c42\u6b21\uff1a\u591a\u5c3a\u5ea6\u7279\u5f81\u805a\u5408\u3001\u65f6\u5e8f\u7279\u5f81\u5904\u7406\u548c\u591a\u5c3a\u5ea6\u7279\u5f81\u540e\u878d\u5408\u3002\u5176\u4e2d\uff0c\u591a\u5c3a\u5ea6\u6a21\u5757\u63d0\u53d6\u77ed\u671f\u3001\u4e2d\u671f\u548c\u957f\u671f\u65f6\u95f4\u5c3a\u5ea6\u7684\u573a\u666f\u8868\u793a\uff0c\u5e76\u5229\u7528Transformer\u4fc3\u8fdb\u7279\u5f81\u4ea4\u4e92\uff1b\u65f6\u5e8f\u7279\u5f81\u5904\u7406\u5728\u56e0\u679c\u7ea6\u675f\u4e0b\u6355\u6349\u573a\u666f\u548c\u7269\u4f53\u7279\u5f81\u7684\u5e8f\u5217\u6f14\u5316\uff1b\u6700\u540e\uff0c\u5728\u591a\u5c3a\u5ea6\u7279\u5f81\u540e\u878d\u5408\u9636\u6bb5\uff0c\u878d\u5408\u8de8\u591a\u4e2a\u65f6\u95f4\u5c3a\u5ea6\u7684\u573a\u666f\u548c\u7269\u4f53\u7279\u5f81\uff0c\u751f\u6210\u5168\u9762\u7684\u98ce\u9669\u8868\u793a\u3002", "result": "\u5728DAD\u548cDADA\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMsFIN\u5728\u9884\u6d4b\u6b63\u786e\u6027\u548c\u63d0\u524d\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u91c7\u7528\u5355\u4e00\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\u7684\u73b0\u6709\u6700\u5148\u8fdb\u6a21\u578b\u3002\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86MsFIN\u5404\u6a21\u5757\u7684\u6709\u6548\u6027\uff0c\u8bc1\u660e\u4e86\u8be5\u7f51\u7edc\u901a\u8fc7\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\u548c\u4e0a\u4e0b\u6587\u4ea4\u4e92\u5efa\u6a21\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u3002", "conclusion": "MsFIN\u901a\u8fc7\u5176\u521b\u65b0\u7684\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\u3001\u65f6\u5e8f\u4fe1\u606f\u5904\u7406\u548c\u8de8\u5c3a\u5ea6\u878d\u5408\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u884c\u8f66\u8bb0\u5f55\u4eea\u89c6\u9891\u4ea4\u901a\u4e8b\u6545\u9884\u6d4b\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u63d0\u524d\u6027\u3002"}}
{"id": "2509.19230", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19230", "abs": "https://arxiv.org/abs/2509.19230", "authors": ["Tianshuo Zhang", "Li Gao", "Siran Peng", "Xiangyu Zhu", "Zhen Lei"], "title": "DevFD: Developmental Face Forgery Detection by Learning Shared and Orthogonal LoRA Subspaces", "comment": "Accepted by NeurIPS 2025", "summary": "The rise of realistic digital face generation and manipulation poses\nsignificant social risks. The primary challenge lies in the rapid and diverse\nevolution of generation techniques, which often outstrip the detection\ncapabilities of existing models. To defend against the ever-evolving new types\nof forgery, we need to enable our model to quickly adapt to new domains with\nlimited computation and data while avoiding forgetting previously learned\nforgery types. In this work, we posit that genuine facial samples are abundant\nand relatively stable in acquisition methods, while forgery faces continuously\nevolve with the iteration of manipulation techniques. Given the practical\ninfeasibility of exhaustively collecting all forgery variants, we frame face\nforgery detection as a continual learning problem and allow the model to\ndevelop as new forgery types emerge. Specifically, we employ a Developmental\nMixture of Experts (MoE) architecture that uses LoRA models as its individual\nexperts. These experts are organized into two groups: a Real-LoRA to learn and\nrefine knowledge of real faces, and multiple Fake-LoRAs to capture incremental\ninformation from different forgery types. To prevent catastrophic forgetting,\nwe ensure that the learning direction of Fake-LoRAs is orthogonal to the\nestablished subspace. Moreover, we integrate orthogonal gradients into the\northogonal loss of Fake-LoRAs, preventing gradient interference throughout the\ntraining process of each task. Experimental results under both the datasets and\nmanipulation types incremental protocols demonstrate the effectiveness of our\nmethod.", "AI": {"tldr": "\u6570\u5b57\u4eba\u8138\u751f\u6210\u4e0e\u64cd\u7eb5\u7684\u793e\u4f1a\u98ce\u9669\u65e5\u76ca\u589e\u52a0\uff0c\u73b0\u6709\u68c0\u6d4b\u6a21\u578b\u96be\u4ee5\u8ddf\u4e0a\u6280\u672f\u6f14\u8fdb\u3002\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f00\u53d1\u6027\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u67b6\u6784\uff0c\u5229\u7528LoRA\u6a21\u578b\u4f5c\u4e3a\u4e13\u5bb6\uff0c\u7ed3\u5408Real-LoRA\u5b66\u4e60\u771f\u5b9e\u4eba\u8138\uff0c\u4ee5\u53ca\u591a\u4e2aFake-LoRA\u6355\u6349\u65b0\u578b\u4f2a\u9020\u4eba\u8138\uff0c\u5e76\u91c7\u7528\u6b63\u4ea4\u68af\u5ea6\u548c\u635f\u5931\u51fd\u6570\u9632\u6b62\u707e\u96be\u6027\u9057\u5fd8\u548c\u68af\u5ea6\u5e72\u6270\uff0c\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u4eba\u8138\u4f2a\u9020\u68c0\u6d4b\u6a21\u578b\u96be\u4ee5\u5e94\u5bf9\u5feb\u901f\u6f14\u8fdb\u7684\u751f\u6210\u6280\u672f\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5feb\u901f\u9002\u5e94\u65b0\u57df\u3001\u5c11\u91cf\u6570\u636e\u3001\u907f\u514d\u9057\u5fd8\u65e7\u7c7b\u578b\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u5f00\u53d1\u6027\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u67b6\u6784\uff0c\u4ee5LoRA\u6a21\u578b\u4f5c\u4e3a\u4e13\u5bb6\u3002\u5c06LoRA\u5206\u4e3a\u4e24\u7ec4\uff1aReal-LoRA\uff08\u5b66\u4e60\u771f\u5b9e\u4eba\u8138\uff09\u548c\u591a\u4e2aFake-LoRA\uff08\u5b66\u4e60\u4e0d\u540c\u4f2a\u9020\u7c7b\u578b\uff09\u3002\u4e3a\u9632\u6b62\u707e\u96be\u6027\u9057\u5fd8\uff0c\u786e\u4fddFake-LoRA\u7684\u5b66\u4e60\u65b9\u5411\u4e0e\u5df2\u6709\u5b50\u7a7a\u95f4\u6b63\u4ea4\uff0c\u5e76\u96c6\u6210\u6b63\u4ea4\u68af\u5ea6\u5230Fake-LoRA\u7684\u6b63\u4ea4\u635f\u5931\u4e2d\uff0c\u4ee5\u9632\u6b62\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u68af\u5ea6\u5e72\u6270\u3002", "result": "\u5728\u6570\u636e\u96c6\u548c\u64cd\u7eb5\u7c7b\u578b\u589e\u91cf\u534f\u8bae\u4e0b\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8e\u5f00\u53d1\u6027\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u548cLoRA\u7684\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u4e0d\u65ad\u53d8\u5316\u7684\u4eba\u8138\u4f2a\u9020\u6280\u672f\uff0c\u5e76\u5728\u9632\u6b62\u707e\u96be\u6027\u9057\u5fd8\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2509.19122", "categories": ["cs.LG", "cs.AI", "68T50", "I.2.7"], "pdf": "https://arxiv.org/pdf/2509.19122", "abs": "https://arxiv.org/abs/2509.19122", "authors": ["Chunming Ye", "Wenquan Tian", "Yalan Gao", "Songzhou Li"], "title": "Analysis on distribution and clustering of weight", "comment": "14page,16 figures", "summary": "The study on architecture and parameter characteristics remains the hot topic\nin the research of large language models. In this paper we concern with the\ncharacteristics of weight which are used to analyze the correlations and\ndifferences between models. Two kinds of vectors-standard deviation vector and\nclustering vector-are proposed to describe features of models. In the first\ncase, the weights are assumed to follow normal distribution. The standard\ndeviation values of projection matrices are normalized to form\nStandard-Deviation Vector, representing the distribution characteristics of\nmodels. In the second case, the singular values from each weight projection\nmatrix are extracted and grouped by K-Means algorithm. The grouped data with\nthe same type matrix are combined as Clustering Vector to represent the\ncorrelation characteristics of models' weights. The study reveals that these\ntwo vectors can effectively distinguish between different models and clearly\nshow the similarities among models of the same family. Moreover, after\nconducting LoRA fine-tuning with different datasets and models, it is found\nthat the distribution of weights represented by standard deviation vector is\ndirectly influenced by the dataset, but the correlations between different\nweights represented by clustering vector remain unaffected and maintain a high\nconsistency with the pre-trained model.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e24\u79cd\u5411\u91cf\uff08\u6807\u51c6\u5dee\u5411\u91cf\u548c\u805a\u7c7b\u5411\u91cf\uff09\u6765\u63cf\u8ff0\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6743\u91cd\u7279\u5f81\uff0c\u7528\u4e8e\u5206\u6790\u6a21\u578b\u95f4\u7684\u76f8\u5173\u6027\u548c\u5dee\u5f02\u6027\u3002", "motivation": "\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u67b6\u6784\u548c\u53c2\u6570\u7279\u5f81\uff0c\u7279\u522b\u662f\u6743\u91cd\u7279\u5f81\uff0c\u4ee5\u5206\u6790\u6a21\u578b\u95f4\u7684\u76f8\u5173\u6027\u548c\u5dee\u5f02\u6027\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u5411\u91cf\uff1a1. \u6807\u51c6\u5dee\u5411\u91cf\uff1a\u5047\u8bbe\u6743\u91cd\u670d\u4ece\u6b63\u6001\u5206\u5e03\uff0c\u5bf9\u6295\u5f71\u77e9\u9635\u7684\u6807\u51c6\u5dee\u503c\u8fdb\u884c\u5f52\u4e00\u5316\u5f97\u5230\u30022. \u805a\u7c7b\u5411\u91cf\uff1a\u63d0\u53d6\u6bcf\u4e2a\u6743\u91cd\u6295\u5f71\u77e9\u9635\u7684\u5947\u5f02\u503c\uff0c\u5e76\u4f7f\u7528K-Means\u7b97\u6cd5\u8fdb\u884c\u5206\u7ec4\uff0c\u5c06\u540c\u7c7b\u578b\u77e9\u9635\u7684\u5206\u7ec4\u6570\u636e\u5408\u5e76\u5f97\u5230\u3002", "result": "\u8fd9\u4e24\u79cd\u5411\u91cf\u80fd\u6709\u6548\u533a\u5206\u4e0d\u540c\u6a21\u578b\uff0c\u5e76\u5c55\u793a\u540c\u5bb6\u65cf\u6a21\u578b\u7684\u76f8\u4f3c\u6027\u3002LoRA\u5fae\u8c03\u5b9e\u9a8c\u8868\u660e\uff0c\u6807\u51c6\u5dee\u5411\u91cf\u53d7\u6570\u636e\u96c6\u5f71\u54cd\uff0c\u800c\u805a\u7c7b\u5411\u91cf\u57fa\u672c\u4e0d\u53d7\u5f71\u54cd\uff0c\u4e0e\u9884\u8bad\u7ec3\u6a21\u578b\u4fdd\u6301\u9ad8\u5ea6\u4e00\u81f4\u3002", "conclusion": "\u6807\u51c6\u5dee\u5411\u91cf\u80fd\u53cd\u6620\u6a21\u578b\u6743\u91cd\u5206\u5e03\u53d7\u6570\u636e\u96c6\u5f71\u54cd\u7684\u7279\u5f81\uff0c\u800c\u805a\u7c7b\u5411\u91cf\u5219\u80fd\u6355\u6349\u6a21\u578b\u6743\u91cd\u95f4\u4e0d\u53d7\u5fae\u8c03\u5f71\u54cd\u7684\u76f8\u5173\u6027\uff0c\u4e24\u8005\u7ed3\u5408\u53ef\u5168\u9762\u5206\u6790\u6a21\u578b\u7279\u5f81\u3002"}}
{"id": "2509.19244", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19244", "abs": "https://arxiv.org/abs/2509.19244", "authors": ["Shufan Li", "Jiuxiang Gu", "Kangning Liu", "Zhe Lin", "Zijun Wei", "Aditya Grover", "Jason Kuen"], "title": "Lavida-O: Elastic Masked Diffusion Models for Unified Multimodal Understanding and Generation", "comment": "32 pages, 15 figures", "summary": "We proposed Lavida-O, a unified multi-modal Masked Diffusion Model (MDM)\ncapable of image understanding and generation tasks. Unlike existing multimodal\ndiffsion language models such as MMaDa and Muddit which only support simple\nimage-level understanding tasks and low-resolution image generation, Lavida-O\nexhibits many new capabilities such as object grounding, image-editing, and\nhigh-resolution (1024px) image synthesis. It is also the first unified MDM that\nuses its understanding capabilities to improve image generation and editing\nresults through planning and iterative self-reflection. To allow effective and\nefficient training and sampling, Lavida-O ntroduces many novel techniques such\nas Elastic Mixture-of-Transformer architecture, universal text conditioning,\nand stratified sampling. \\ours~achieves state-of-the-art performance on a wide\nrange of benchmarks such as RefCOCO object grounding, GenEval text-to-image\ngeneration, and ImgEdit image editing, outperforming existing autoregressive\nand continuous diffusion models such as Qwen2.5-VL and FluxKontext-dev, while\noffering considerable speedup at inference.", "AI": {"tldr": "Lavida-O\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u6a21\u6001\u63a9\u7801\u6269\u6563\u6a21\u578b\uff08MDM\uff09\uff0c\u80fd\u591f\u6267\u884c\u56fe\u50cf\u7406\u89e3\u548c\u751f\u6210\u4efb\u52a1\uff0c\u5e76\u5728\u7269\u4f53\u8bc6\u522b\u3001\u56fe\u50cf\u7f16\u8f91\u548c\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u5408\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u5728\u591a\u6a21\u6001\u7406\u89e3\u548c\u56fe\u50cf\u751f\u6210\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0cLavida-O\u65e8\u5728\u901a\u8fc7\u7edf\u4e00\u7684MDM\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u5e76\u5b9e\u73b0\u66f4\u9ad8\u7ea7\u7684\u529f\u80fd\u3002", "method": "Lavida-O\u91c7\u7528\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5f39\u6027\u6df7\u5408Transformer\u67b6\u6784\uff0c\u5e76\u7ed3\u5408\u4e86\u901a\u7528\u6587\u672c\u6761\u4ef6\u548c\u5206\u5c42\u91c7\u6837\u7b49\u6280\u672f\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u6548\u7684\u8bad\u7ec3\u548c\u91c7\u6837\u3002", "result": "Lavida-O\u5728\u7269\u4f53\u8bc6\u522b\u3001\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u548c\u56fe\u50cf\u7f16\u8f91\u7b49\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u81ea\u56de\u5f52\u548c\u8fde\u7eed\u6269\u6563\u6a21\u578b\uff0c\u5e76\u4e14\u5728\u63a8\u7406\u901f\u5ea6\u4e0a\u4e5f\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "Lavida-O\u662f\u4e00\u4e2a\u5f3a\u5927\u7684\u7edf\u4e00\u591a\u6a21\u6001\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u5176\u72ec\u7279\u7684\u67b6\u6784\u548c\u6280\u672f\uff0c\u5728\u56fe\u50cf\u7406\u89e3\u548c\u751f\u6210\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u8fdb\u5c55\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2509.19128", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19128", "abs": "https://arxiv.org/abs/2509.19128", "authors": ["Alexandre Pich\u00e9", "Ehsan Kamaloo", "Rafael Pardinas", "Dzmitry Bahdanau"], "title": "PipelineRL: Faster On-policy Reinforcement Learning for Long Sequence Generatio", "comment": null, "summary": "Reinforcement Learning (RL) is increasingly utilized to enhance the reasoning\ncapabilities of Large Language Models (LLMs). However, effectively scaling\nthese RL methods presents significant challenges, primarily due to the\ndifficulty in maintaining high AI accelerator utilization without generating\nstale, off-policy data that harms common RL algorithms. This paper introduces\nPipelineRL, an approach designed to achieve a superior trade-off between\nhardware efficiency and data on-policyness for LLM training. PipelineRL employs\nconcurrent asynchronous data generation and model training, distinguished by\nthe novel in-flight weight updates. This mechanism allows the LLM generation\nengine to receive updated model weights with minimal interruption during the\ngeneration of token sequences, thereby maximizing both the accelerator\nutilization and the freshness of training data. Experiments conducted on\nlong-form reasoning tasks using 128 H100 GPUs demonstrate that PipelineRL\nachieves approximately $\\sim 2x$ faster learning compared to conventional RL\nbaselines while maintaining highly on-policy training data. A scalable and\nmodular open-source implementation of PipelineRL is also released as a key\ncontribution.", "AI": {"tldr": "PipelineRL\u901a\u8fc7\u5728LLM\u8bad\u7ec3\u4e2d\u5f15\u5165", "motivation": "RL\u65b9\u6cd5\u5728\u63d0\u5347LLM\u63a8\u7406\u80fd\u529b\u65b9\u9762\u6709\u6f5c\u529b\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u9762\u4e34\u6269\u5c55\u6027\u6311\u6218\uff0c\u96be\u4ee5\u5728\u9ad8AI\u52a0\u901f\u5668\u5229\u7528\u7387\u548c\u6570\u636e\u65b0\u9c9c\u5ea6\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "method": "PipelineRL\u63d0\u51fa\u4e86\u4e00\u79cd\u5e76\u53d1\u5f02\u6b65\u6570\u636e\u751f\u6210\u548c\u6a21\u578b\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u5176\u6838\u5fc3\u662f\u201cin-flight weight updates\u201d\u673a\u5236\uff0c\u5141\u8bb8\u5728\u751f\u6210\u5e8f\u5217\u7684\u540c\u65f6\u4ee5\u6700\u5c0f\u4e2d\u65ad\u66f4\u65b0\u6a21\u578b\u6743\u91cd\uff0c\u4ece\u800c\u63d0\u9ad8\u52a0\u901f\u5668\u5229\u7528\u7387\u548c\u6570\u636e\u65b0\u9c9c\u5ea6\u3002", "result": "\u5728\u957f\u7bc7\u63a8\u7406\u4efb\u52a1\u7684\u5b9e\u9a8c\u4e2d\uff0c\u4f7f\u7528128\u4e2aH100 GPU\uff0cPipelineRL\u7684\u5b66\u4e60\u901f\u5ea6\u7ea6\u662f\u4f20\u7edfRL\u57fa\u7ebf\u7684\u4e24\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u6bd4\u4f8b\u7684on-policy\u8bad\u7ec3\u6570\u636e\u3002", "conclusion": "PipelineRL\u5728LLM\u8bad\u7ec3\u4e2d\u5b9e\u73b0\u4e86\u786c\u4ef6\u6548\u7387\u548c\u6570\u636eon-policyness\u7684\u66f4\u4f18 trade-off\uff0c\u5e76\u4e14\u5f00\u6e90\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u5b9e\u73b0\u7248\u672c\u3002"}}
{"id": "2509.19245", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19245", "abs": "https://arxiv.org/abs/2509.19245", "authors": ["Benedetta Liberatori", "Alessandro Conti", "Lorenzo Vaquero", "Yiming Wang", "Elisa Ricci", "Paolo Rota"], "title": "ConViS-Bench: Estimating Video Similarity Through Semantic Concepts", "comment": "Accepted to NeurIPS 2025", "summary": "What does it mean for two videos to be similar? Videos may appear similar\nwhen judged by the actions they depict, yet entirely different if evaluated\nbased on the locations where they were filmed. While humans naturally compare\nvideos by taking different aspects into account, this ability has not been\nthoroughly studied and presents a challenge for models that often depend on\nbroad global similarity scores. Large Multimodal Models (LMMs) with video\nunderstanding capabilities open new opportunities for leveraging natural\nlanguage in comparative video tasks. We introduce Concept-based Video\nSimilarity estimation (ConViS), a novel task that compares pairs of videos by\ncomputing interpretable similarity scores across a predefined set of key\nsemantic concepts. ConViS allows for human-like reasoning about video\nsimilarity and enables new applications such as concept-conditioned video\nretrieval. To support this task, we also introduce ConViS-Bench, a new\nbenchmark comprising carefully annotated video pairs spanning multiple domains.\nEach pair comes with concept-level similarity scores and textual descriptions\nof both differences and similarities. Additionally, we benchmark several\nstate-of-the-art models on ConViS, providing insights into their alignment with\nhuman judgments. Our results reveal significant performance differences on\nConViS, indicating that some concepts present greater challenges for estimating\nvideo similarity. We believe that ConViS-Bench will serve as a valuable\nresource for advancing research in language-driven video understanding.", "AI": {"tldr": "ConViS\u662f\u4e00\u4e2a\u65b0\u7684\u4efb\u52a1\uff0c\u901a\u8fc7\u8ba1\u7b97\u9884\u5b9a\u4e49\u6982\u5ff5\u7684\u8bed\u4e49\u76f8\u4f3c\u5ea6\u5f97\u5206\u6765\u6bd4\u8f83\u89c6\u9891\u5bf9\uff0c\u5e76\u5f15\u5165\u4e86ConViS-Bench\u57fa\u51c6\u6765\u652f\u6301\u8be5\u4efb\u52a1\u3002", "motivation": "\u76ee\u524d\u7684\u89c6\u9891\u76f8\u4f3c\u5ea6\u8bc4\u4f30\u4f9d\u8d56\u4e8e\u5168\u5c40\u76f8\u4f3c\u5ea6\u5f97\u5206\uff0c\u5ffd\u7565\u4e86\u4eba\u7c7b\u5728\u6bd4\u8f83\u89c6\u9891\u65f6\u4f1a\u8003\u8651\u4e0d\u540c\u65b9\u9762\uff08\u5982\u52a8\u4f5c\u3001\u5730\u70b9\uff09\u7684\u80fd\u529b\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u8fdb\u884c\u7c7b\u4eba\u63a8\u7406\u7684\u89c6\u9891\u76f8\u4f3c\u5ea6\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aConViS\uff08Concept-based Video Similarity\uff09\u7684\u65b0\u4efb\u52a1\uff0c\u8be5\u4efb\u52a1\u901a\u8fc7\u8ba1\u7b97\u9884\u5b9a\u4e49\u5173\u952e\u8bed\u4e49\u6982\u5ff5\u7684\u89e3\u91ca\u6027\u76f8\u4f3c\u5ea6\u5f97\u5206\u6765\u6bd4\u8f83\u89c6\u9891\u5bf9\u3002\u540c\u65f6\uff0c\u53d1\u5e03\u4e86ConViS-Bench\u57fa\u51c6\uff0c\u5176\u4e2d\u5305\u542b\u7ecf\u8fc7\u4ed4\u7ec6\u6807\u6ce8\u7684\u89c6\u9891\u5bf9\uff0c\u6db5\u76d6\u591a\u4e2a\u9886\u57df\uff0c\u5e76\u63d0\u4f9b\u6982\u5ff5\u7ea7\u76f8\u4f3c\u5ea6\u5f97\u5206\u4ee5\u53ca\u5dee\u5f02\u548c\u76f8\u4f3c\u4e4b\u5904\u7684\u6587\u672c\u63cf\u8ff0\u3002\u5728ConViS-Bench\u4e0a\u5bf9\u51e0\u79cd\u6700\u5148\u8fdb\u7684\u6a21\u578b\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u5728ConViS\u4efb\u52a1\u4e0a\uff0c\u4e0d\u540c\u6a21\u578b\u7684\u8868\u73b0\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u8868\u660e\u67d0\u4e9b\u6982\u5ff5\u5728\u89c6\u9891\u76f8\u4f3c\u5ea6\u8bc4\u4f30\u65b9\u9762\u66f4\u5177\u6311\u6218\u6027\u3002", "conclusion": "ConViS\u4efb\u52a1\u548cConViS-Bench\u57fa\u51c6\u80fd\u591f\u4fc3\u8fdb\u8bed\u8a00\u9a71\u52a8\u7684\u89c6\u9891\u7406\u89e3\u7814\u7a76\uff0c\u5b9e\u73b0\u7c7b\u4eba\u63a8\u7406\u7684\u89c6\u9891\u76f8\u4f3c\u5ea6\u8bc4\u4f30\uff0c\u5e76\u652f\u6301\u65b0\u7684\u5e94\u7528\uff0c\u5982\u6982\u5ff5\u6761\u4ef6\u89c6\u9891\u68c0\u7d22\u3002"}}
{"id": "2509.19135", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19135", "abs": "https://arxiv.org/abs/2509.19135", "authors": ["Wenying Luo", "Zhiyuan Lin", "Wenhao Xu", "Minghao Liu", "Zhi Li"], "title": "GSTM-HMU: Generative Spatio-Temporal Modeling for Human Mobility Understanding", "comment": null, "summary": "Human mobility traces, often recorded as sequences of check-ins, provide a\nunique window into both short-term visiting patterns and persistent lifestyle\nregularities. In this work we introduce GSTM-HMU, a generative spatio-temporal\nframework designed to advance mobility analysis by explicitly modeling the\nsemantic and temporal complexity of human movement. The framework consists of\nfour key innovations. First, a Spatio-Temporal Concept Encoder (STCE)\nintegrates geographic location, POI category semantics, and periodic temporal\nrhythms into unified vector representations. Second, a Cognitive Trajectory\nMemory (CTM) adaptively filters historical visits, emphasizing recent and\nbehaviorally salient events in order to capture user intent more effectively.\nThird, a Lifestyle Concept Bank (LCB) contributes structured human preference\ncues, such as activity types and lifestyle patterns, to enhance\ninterpretability and personalization. Finally, task-oriented generative heads\ntransform the learned representations into predictions for multiple downstream\ntasks. We conduct extensive experiments on four widely used real-world\ndatasets, including Gowalla, WeePlace, Brightkite, and FourSquare, and evaluate\nperformance on three benchmark tasks: next-location prediction, trajectory-user\nidentification, and time estimation. The results demonstrate consistent and\nsubstantial improvements over strong baselines, confirming the effectiveness of\nGSTM-HMU in extracting semantic regularities from complex mobility data. Beyond\nraw performance gains, our findings also suggest that generative modeling\nprovides a promising foundation for building more robust, interpretable, and\ngeneralizable systems for human mobility intelligence.", "AI": {"tldr": "GSTM-HMU\u662f\u4e00\u4e2a\u751f\u6210\u5f0f\u65f6\u7a7a\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u5730\u7406\u4f4d\u7f6e\u3001POI\u7c7b\u522b\u8bed\u4e49\u548c\u5468\u671f\u6027\u65f6\u95f4\u8282\u594f\uff0c\u5e76\u8003\u8651\u7528\u6237\u610f\u56fe\u548c\u751f\u6d3b\u65b9\u5f0f\u6a21\u5f0f\uff0c\u6765\u5206\u6790\u4eba\u7c7b\u79fb\u52a8\u8f68\u8ff9\uff0c\u5e76\u5728\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6539\u8fdb\u3002", "motivation": "\u5206\u6790\u4eba\u7c7b\u79fb\u52a8\u8f68\u8ff9\uff0c\u7279\u522b\u662f\u77ed\u671f\u8bbf\u95ee\u6a21\u5f0f\u548c\u957f\u671f\u751f\u6d3b\u89c4\u5f8b\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u8fdb\u884c\u9884\u6d4b\u3002\u91cd\u70b9\u5728\u4e8e\u5904\u7406\u5730\u7406\u4f4d\u7f6e\u3001POI\u7c7b\u522b\u8bed\u4e49\u548c\u65f6\u95f4\u4fe1\u606f\u7684\u590d\u6742\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGSTM-HMU\u7684\u751f\u6210\u5f0f\u65f6\u7a7a\u6846\u67b6\uff0c\u5305\u542b\u56db\u4e2a\u5173\u952e\u521b\u65b0\uff1a1. \u65f6\u7a7a\u6982\u5ff5\u7f16\u7801\u5668\uff08STCE\uff09\u5c06\u5730\u7406\u4f4d\u7f6e\u3001POI\u7c7b\u522b\u8bed\u4e49\u548c\u5468\u671f\u6027\u65f6\u95f4\u8282\u594f\u6574\u5408\u4e3a\u7edf\u4e00\u7684\u5411\u91cf\u8868\u793a\u30022. \u8ba4\u77e5\u8f68\u8ff9\u8bb0\u5fc6\uff08CTM\uff09\u81ea\u9002\u5e94\u5730\u8fc7\u6ee4\u5386\u53f2\u8bbf\u95ee\u8bb0\u5f55\uff0c\u5f3a\u8c03\u8fd1\u671f\u548c\u884c\u4e3a\u4e0a\u91cd\u8981\u7684\u4e8b\u4ef6\uff0c\u4ee5\u66f4\u6709\u6548\u5730\u6355\u6349\u7528\u6237\u610f\u56fe\u30023. \u751f\u6d3b\u65b9\u5f0f\u6982\u5ff5\u5e93\uff08LCB\uff09\u63d0\u4f9b\u7ed3\u6784\u5316\u7684\u4eba\u7c7b\u504f\u597d\u7ebf\u7d22\uff0c\u5982\u6d3b\u52a8\u7c7b\u578b\u548c\u751f\u6d3b\u65b9\u5f0f\u6a21\u5f0f\uff0c\u4ee5\u589e\u5f3a\u53ef\u89e3\u91ca\u6027\u548c\u4e2a\u6027\u5316\u30024. \u9762\u5411\u4efb\u52a1\u7684\u751f\u6210\u5934\u5c06\u5b66\u4e60\u5230\u7684\u8868\u793a\u8f6c\u5316\u4e3a\u591a\u4e2a\u4e0b\u6e38\u4efb\u52a1\u7684\u9884\u6d4b\u3002", "result": "\u5728Gowalla, WeePlace, Brightkite\u548cFourSquare\u56db\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u5e76\u5728\u4e0b\u4e00\u4e2a\u4f4d\u7f6e\u9884\u6d4b\u3001\u8f68\u8ff9\u7528\u6237\u8bc6\u522b\u548c\u65f6\u95f4\u4f30\u8ba1\u4e09\u4e2a\u57fa\u51c6\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002\u7ed3\u679c\u663e\u793aGSTM-HMU\u5728\u8fd9\u4e9b\u4efb\u52a1\u4e0a\u76f8\u6bd4\u5f3a\u5927\u7684\u57fa\u7ebf\u6a21\u578b\u53d6\u5f97\u4e86\u6301\u7eed\u4e14\u663e\u8457\u7684\u6539\u8fdb\u3002", "conclusion": "GSTM-HMU\u5728\u4ece\u590d\u6742\u79fb\u52a8\u6570\u636e\u4e2d\u63d0\u53d6\u8bed\u4e49\u89c4\u5f8b\u65b9\u9762\u662f\u6709\u6548\u7684\u3002\u751f\u6210\u5f0f\u5efa\u6a21\u4e3a\u6784\u5efa\u66f4\u9c81\u68d2\u3001\u53ef\u89e3\u91ca\u548c\u53ef\u6cdb\u5316\u7684 \n\u4eba\u7c7b\u79fb\u52a8\u667a\u80fd\u7cfb\u7edf\u5960\u5b9a\u4e86\u6709\u5e0c\u671b\u7684\u57fa\u7840\u3002"}}
{"id": "2509.19252", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19252", "abs": "https://arxiv.org/abs/2509.19252", "authors": ["Gabriel Maldonado", "Narges Rashvand", "Armin Danesh Pazho", "Ghazal Alinezhad Noghre", "Vinit Katariya", "Hamed Tabkhi"], "title": "Adversarially-Refined VQ-GAN with Dense Motion Tokenization for Spatio-Temporal Heatmaps", "comment": null, "summary": "Continuous human motion understanding remains a core challenge in computer\nvision due to its high dimensionality and inherent redundancy. Efficient\ncompression and representation are crucial for analyzing complex motion\ndynamics. In this work, we introduce an adversarially-refined VQ-GAN framework\nwith dense motion tokenization for compressing spatio-temporal heatmaps while\npreserving the fine-grained traces of human motion. Our approach combines dense\nmotion tokenization with adversarial refinement, which eliminates\nreconstruction artifacts like motion smearing and temporal misalignment\nobserved in non-adversarial baselines. Our experiments on the CMU Panoptic\ndataset provide conclusive evidence of our method's superiority, outperforming\nthe dVAE baseline by 9.31% SSIM and reducing temporal instability by 37.1%.\nFurthermore, our dense tokenization strategy enables a novel analysis of motion\ncomplexity, revealing that 2D motion can be optimally represented with a\ncompact 128-token vocabulary, while 3D motion's complexity demands a much\nlarger 1024-token codebook for faithful reconstruction. These results establish\npractical deployment feasibility across diverse motion analysis applications.\nThe code base for this work is available at\nhttps://github.com/TeCSAR-UNCC/Pose-Quantization.", "AI": {"tldr": "\u4e00\u79cd\u7ed3\u5408\u4e86\u5bc6\u96c6\u8fd0\u52a8\u6807\u8bb0\u5316\u548c\u5bf9\u6297\u6027\u7cbe\u70bc\u7684 VQ-GAN \u6846\u67b6\uff0c\u7528\u4e8e\u538b\u7f29\u65f6\u7a7a\u70ed\u56fe\uff0c\u540c\u65f6\u4fdd\u7559\u7cbe\u7ec6\u7684\u4eba\u4f53\u8fd0\u52a8\u8f68\u8ff9\u3002", "motivation": "\u89e3\u51b3\u8fde\u7eed\u4eba\u7c7b\u8fd0\u52a8\u7406\u89e3\u4e2d\u7684\u9ad8\u7ef4\u5ea6\u548c\u56fa\u6709\u5197\u4f59\u6311\u6218\uff0c\u5b9e\u73b0\u9ad8\u6548\u538b\u7f29\u548c\u8868\u5f81\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u878d\u5408\u5bc6\u96c6\u8fd0\u52a8\u6807\u8bb0\u5316\u548c\u5bf9\u6297\u6027\u7cbe\u70bc\u7684 VQ-GAN \u6846\u67b6\uff0c\u4ee5\u538b\u7f29\u65f6\u7a7a\u70ed\u56fe\u5e76\u4fdd\u7559\u7cbe\u7ec6\u7684\u4eba\u4f53\u8fd0\u52a8\u8f68\u8ff9\u3002", "result": "\u5728 CMU Panoptic \u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5 SSIM \u6307\u6807\u4f18\u4e8e dVAE \u57fa\u7ebf 9.31%\uff0c\u5e76\u51cf\u5c11\u4e86 37.1% \u7684\u65f6\u95f4\u4e0d\u7a33\u5b9a\u6027\u3002\u6b64\u5916\uff0c\u7814\u7a76\u53d1\u73b0 2D \u8fd0\u52a8\u53ef\u4ee5\u7528 128 \u4e2a\u6807\u8bb0\u7684\u8bcd\u6c47\u8868\u8868\u793a\uff0c\u800c 3D \u8fd0\u52a8\u9700\u8981 1024 \u4e2a\u6807\u8bb0\u7684\u7801\u672c\u624d\u80fd\u8fdb\u884c\u4fdd\u771f\u91cd\u5efa\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u538b\u7f29\u65f6\u7a7a\u70ed\u56fe\u548c\u4fdd\u7559\u4eba\u4f53\u8fd0\u52a8\u7ec6\u8282\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u4e3a\u5206\u6790\u8fd0\u52a8\u590d\u6742\u5ea6\u63d0\u4f9b\u4e86\u65b0\u9896\u7684\u89c6\u89d2\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5404\u79cd\u8fd0\u52a8\u5206\u6790\u5e94\u7528\u4e2d\u7684\u5b9e\u9645\u90e8\u7f72\u53ef\u884c\u6027\u3002"}}
{"id": "2509.19159", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19159", "abs": "https://arxiv.org/abs/2509.19159", "authors": ["Qingfeng Lan", "Gautham Vasan", "A. Rupam Mahmood"], "title": "Efficient Reinforcement Learning by Reducing Forgetting with Elephant Activation Functions", "comment": "Code release: https://github.com/qlan3/ENN", "summary": "Catastrophic forgetting has remained a significant challenge for efficient\nreinforcement learning for decades (Ring 1994, Rivest and Precup 2003). While\nrecent works have proposed effective methods to mitigate this issue, they\nmainly focus on the algorithmic side. Meanwhile, we do not fully understand\nwhat architectural properties of neural networks lead to catastrophic\nforgetting. This study aims to fill this gap by studying the role of activation\nfunctions in the training dynamics of neural networks and their impact on\ncatastrophic forgetting in reinforcement learning setup. Our study reveals\nthat, besides sparse representations, the gradient sparsity of activation\nfunctions also plays an important role in reducing forgetting. Based on this\ninsight, we propose a new class of activation functions, elephant activation\nfunctions, that can generate both sparse outputs and sparse gradients. We show\nthat by simply replacing classical activation functions with elephant\nactivation functions in the neural networks of value-based algorithms, we can\nsignificantly improve the resilience of neural networks to catastrophic\nforgetting, thus making reinforcement learning more sample-efficient and\nmemory-efficient.", "AI": {"tldr": "Catastrophic forgetting in RL is a long-standing problem. This paper investigates the role of activation functions, proposing 'elephant activation functions' that induce sparse outputs and gradients to mitigate forgetting and improve RL efficiency.", "motivation": "Catastrophic forgetting is a major challenge in reinforcement learning (RL). While algorithmic solutions exist, the impact of neural network architecture, specifically activation functions, on this issue is not well understood.", "method": "The study analyzes how activation functions affect neural network training dynamics and catastrophic forgetting in RL. It identifies gradient sparsity as crucial and proposes 'elephant activation functions' that create both sparse outputs and gradients.", "result": "Replacing standard activation functions with elephant activation functions in value-based RL algorithms significantly reduces catastrophic forgetting, leading to more sample- and memory-efficient RL.", "conclusion": "Elephant activation functions, by promoting sparse outputs and gradients, offer a promising architectural solution to mitigate catastrophic forgetting in reinforcement learning, enhancing overall efficiency."}}
{"id": "2509.19258", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19258", "abs": "https://arxiv.org/abs/2509.19258", "authors": ["Dheerendranath Battalapalli", "Apoorva Safai", "Maria Jaramillo", "Hyemin Um", "Gustavo Adalfo Pineda Ortiz", "Ulas Bagci", "Manmeet Singh Ahluwalia", "Marwa Ismail", "Pallavi Tiwari"], "title": "Graph-Radiomic Learning (GrRAiL) Descriptor to Characterize Imaging Heterogeneity in Confounding Tumor Pathologies", "comment": "Under Review: npj Digital Medicine", "summary": "A significant challenge in solid tumors is reliably distinguishing\nconfounding pathologies from malignant neoplasms on routine imaging. While\nradiomics methods seek surrogate markers of lesion heterogeneity on CT/MRI,\nmany aggregate features across the region of interest (ROI) and miss complex\nspatial relationships among varying intensity compositions. We present a new\nGraph-Radiomic Learning (GrRAiL) descriptor for characterizing intralesional\nheterogeneity (ILH) on clinical MRI scans. GrRAiL (1) identifies clusters of\nsub-regions using per-voxel radiomic measurements, then (2) computes\ngraph-theoretic metrics to quantify spatial associations among clusters. The\nresulting weighted graphs encode higher-order spatial relationships within the\nROI, aiming to reliably capture ILH and disambiguate confounding pathologies\nfrom malignancy. To assess efficacy and clinical feasibility, GrRAiL was\nevaluated in n=947 subjects spanning three use cases: differentiating tumor\nrecurrence from radiation effects in glioblastoma (GBM; n=106) and brain\nmetastasis (n=233), and stratifying pancreatic intraductal papillary mucinous\nneoplasms (IPMNs) into no+low vs high risk (n=608). In a multi-institutional\nsetting, GrRAiL consistently outperformed state-of-the-art baselines - Graph\nNeural Networks (GNNs), textural radiomics, and intensity-graph analysis. In\nGBM, cross-validation (CV) and test accuracies for recurrence vs\npseudo-progression were 89% and 78% with >10% test-accuracy gains over\ncomparators. In brain metastasis, CV and test accuracies for recurrence vs\nradiation necrosis were 84% and 74% (>13% improvement). For IPMN risk\nstratification, CV and test accuracies were 84% and 75%, showing >10%\nimprovement.", "AI": {"tldr": "GrRAiL\u901a\u8fc7\u8bc6\u522b\u5b50\u533a\u57df\u96c6\u7fa4\u5e76\u91cf\u5316\u5b83\u4eec\u4e4b\u95f4\u7684\u7a7a\u95f4\u5173\u7cfb\u6765\u8868\u5f81\u80bf\u7624\u5185\u5f02\u8d28\u6027\uff08ILH\uff09\uff0c\u5728\u533a\u5206\u80bf\u7624\u590d\u53d1\u548c\u574f\u6b7b\u3001\u4ee5\u53caIPMN\u98ce\u9669\u5206\u5c42\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5728\u5e38\u89c4\u5f71\u50cf\u5b66\u4e2d\uff0c\u53ef\u9760\u5730\u533a\u5206\u5b9e\u4f53\u7624\u4e2d\u7684\u6df7\u6dc6\u75c5\u7406\u5b66\u548c\u6076\u6027\u80bf\u7624\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002\u73b0\u6709\u7684\u653e\u5c04\u7ec4\u5b66\u65b9\u6cd5\u5728\u805a\u5408\u7279\u5f81\u65f6\u4f1a\u5ffd\u7565\u533a\u57df\u5185\u590d\u6742\u7684\u7a7a\u95f4\u5173\u7cfb\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u56fe\u653e\u5c04\u7ec4\u5b66\u5b66\u4e60\uff08GrRAiL\uff09\u63cf\u8ff0\u7b26\uff0c\u901a\u8fc7\u8bc6\u522b\u50cf\u7d20\u7ea7\u653e\u5c04\u7ec4\u5b66\u6d4b\u91cf\u7684\u5b50\u533a\u57df\u96c6\u7fa4\uff0c\u7136\u540e\u8ba1\u7b97\u56fe\u8bba\u6307\u6807\u6765\u91cf\u5316\u96c6\u7fa4\u95f4\u7684\u7a7a\u95f4\u5173\u8054\uff0c\u4ece\u800c\u6355\u6349ILH\u5e76\u533a\u5206\u6df7\u6dc6\u75c5\u7406\u5b66\u548c\u6076\u6027\u80bf\u7624\u3002", "result": "\u5728\u80f6\u8d28\u6bcd\u7ec6\u80de\u7624\uff08GBM\uff09\u3001\u8111\u8f6c\u79fb\u7624\u548c\u80f0\u817a\u5bfc\u7ba1\u5185\u4e73\u5934\u72b6\u9ecf\u6db2\u7624\uff08IPMN\uff09\u7684\u4e09\u4e2a\u6848\u4f8b\u7814\u7a76\u4e2d\uff0cGrRAiL\u5728\u533a\u5206\u590d\u53d1/\u574f\u6b7b\u548cIPMN\u98ce\u9669\u5206\u5c42\u65b9\u9762\uff0c\u5728\u4ea4\u53c9\u9a8c\u8bc1\u548c\u6d4b\u8bd5\u51c6\u786e\u6027\u4e0a\u5747\u4f18\u4e8eGNN\u3001\u7eb9\u7406\u653e\u5c04\u7ec4\u5b66\u548c\u5f3a\u5ea6\u56fe\u5206\u6790\u7b49\u57fa\u7ebf\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e8610%\u4ee5\u4e0a\u3002", "conclusion": "GrRAiL\u5728\u591a\u673a\u6784\u8bbe\u7f6e\u4e2d\u80fd\u591f\u53ef\u9760\u5730\u6355\u6349ILH\uff0c\u5e76\u80fd\u533a\u5206\u6df7\u6dc6\u75c5\u7406\u5b66\u548c\u6076\u6027\u80bf\u7624\uff0c\u5728\u4e34\u5e8a\u53ef\u884c\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2509.19189", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.19189", "abs": "https://arxiv.org/abs/2509.19189", "authors": ["Binghui Li", "Fengling Chen", "Zixun Huang", "Lean Wang", "Lei Wu"], "title": "Unveiling the Role of Learning Rate Schedules via Functional Scaling Laws", "comment": "52 pages, accepted by NeurIPS 2025 as a spotlight paper", "summary": "Scaling laws have played a cornerstone role in guiding the training of large\nlanguage models (LLMs). However, most existing works on scaling laws primarily\nfocus on the final-step loss, overlooking the loss dynamics during the training\nprocess and, crucially, the impact of learning rate schedule (LRS). In this\npaper, we aim to bridge this gap by studying a teacher-student kernel\nregression setup trained via online stochastic gradient descent (SGD).\nLeveraging a novel intrinsic time viewpoint and stochastic differential\nequation (SDE) modeling of SGD, we introduce the Functional Scaling Law (FSL),\nwhich characterizes the evolution of population risk during the training\nprocess for general LRSs. Remarkably, the impact of the LRSs is captured\nthrough an explicit convolution-type functional term, making their effects\nfully tractable. To illustrate the utility of FSL, we analyze three widely used\nLRSs -- constant, exponential decay, and warmup-stable-decay (WSD) -- under\nboth data-limited and compute-limited regimes. We provide theoretical\njustification for widely adopted empirical practices in LLMs pre-training such\nas (i) higher-capacity models are more data- and compute-efficient; (ii)\nlearning rate decay can improve training efficiency; (iii) WSD-like schedules\ncan outperform direct-decay schedules. Lastly, we explore the practical\nrelevance of FSL as a surrogate model for fitting, predicting and optimizing\nthe loss curves in LLM pre-training, with experiments conducted across model\nsizes ranging from 0.1B to 1B parameters. We hope our FSL framework can deepen\nthe understanding of LLM pre-training dynamics and provide insights for\nimproving large-scale model training.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u51fd\u6570\u5f0f\u7f29\u653e\u5b9a\u5f8b\uff08FSL\uff09\uff0c\u7814\u7a76\u4e86\u5b66\u4e60\u7387\u7b56\u7565\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8bad\u7ec3\u52a8\u6001\u7684\u5f71\u54cd\uff0c\u5e76\u4e3aLLM\u9884\u8bad\u7ec3\u4e2d\u7684\u4e00\u4e9b\u7ecf\u9a8c\u5b9e\u8df5\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\u3002", "motivation": "\u73b0\u6709\u5173\u4e8eLLM\u7f29\u653e\u5b9a\u5f8b\u7684\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u6700\u7ec8\u635f\u5931\uff0c\u5ffd\u7565\u4e86\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u635f\u5931\u52a8\u6001\u4ee5\u53ca\u5b66\u4e60\u7387\u7b56\u7565\u7684\u5f71\u54cd\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u7814\u7a76\u4e00\u4e2a\u6559\u5e08-\u5b66\u751f\u6838\u56de\u5f52\u6a21\u578b\uff0c\u5e76\u5229\u7528\u5728\u7ebf\u968f\u673a\u68af\u5ea6\u4e0b\u964d\uff08SGD\uff09\u8fdb\u884c\u8bad\u7ec3\u3002\u501f\u52a9\u5185\u5728\u65f6\u95f4\u89c6\u89d2\u548cSGD\u7684\u968f\u673a\u5fae\u5206\u65b9\u7a0b\uff08SDE\uff09\u6a21\u578b\uff0c\u63d0\u51fa\u51fd\u6570\u5f0f\u7f29\u653e\u5b9a\u5f8b\uff08FSL\uff09\uff0c\u8be5\u5b9a\u5f8b\u80fd\u591f\u63cf\u8ff0\u5728\u4e00\u822c\u5b66\u4e60\u7387\u7b56\u7565\u4e0b\uff0c\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u603b\u4f53\u98ce\u9669\u7684\u6f14\u53d8\u3002\u5b66\u4e60\u7387\u7b56\u7565\u7684\u5f71\u54cd\u901a\u8fc7\u4e00\u4e2a\u663e\u5f0f\u7684\u5377\u79ef\u578b\u51fd\u6570\u9879\u6765\u4f53\u73b0\u3002", "result": "FSL\u80fd\u591f\u6355\u6349\u5b66\u4e60\u7387\u7b56\u7565\u7684\u5f71\u54cd\uff0c\u5e76\u88ab\u7528\u4e8e\u5206\u6790\u4e09\u79cd\u5e38\u7528\u7684\u5b66\u4e60\u7387\u7b56\u7565\uff08\u6052\u5b9a\u3001\u6307\u6570\u8870\u51cf\u3001\u9884\u70ed-\u7a33\u5b9a-\u8870\u51cf\uff09\u5728\u6570\u636e\u9650\u5236\u548c\u8ba1\u7b97\u9650\u5236\u4e24\u79cd\u60c5\u51b5\u4e0b\u7684\u8868\u73b0\u3002\u7814\u7a76\u7ed3\u679c\u4e3aLLM\u9884\u8bad\u7ec3\u4e2d\u7684\u4e00\u4e9b\u7ecf\u9a8c\u505a\u6cd5\uff08\u5982\u9ad8\u5bb9\u91cf\u6a21\u578b\u66f4\u9ad8\u6548\u3001\u5b66\u4e60\u7387\u8870\u51cf\u53ef\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\u3001\u9884\u70ed-\u7a33\u5b9a-\u8870\u51cf\u7b56\u7565\u4f18\u4e8e\u76f4\u63a5\u8870\u51cf\u7b56\u7565\uff09\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u3002\u6b64\u5916\uff0cFSL\u8fd8\u88ab\u7528\u4f5c\u4e00\u4e2a\u4ee3\u7406\u6a21\u578b\uff0c\u7528\u4e8e\u62df\u5408\u3001\u9884\u6d4b\u548c\u4f18\u5316LLM\u9884\u8bad\u7ec3\u4e2d\u7684\u635f\u5931\u66f2\u7ebf\uff0c\u5e76\u5728\u4e0d\u540c\u6a21\u578b\u89c4\u6a21\uff080.1B\u52301B\u53c2\u6570\uff09\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "conclusion": "FSL\u6846\u67b6\u80fd\u591f\u52a0\u6df1\u5bf9LLM\u9884\u8bad\u7ec3\u52a8\u6001\u7684\u7406\u89e3\uff0c\u5e76\u4e3a\u6539\u8fdb\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u63d0\u4f9b\u89c1\u89e3\u3002"}}
{"id": "2509.19259", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19259", "abs": "https://arxiv.org/abs/2509.19259", "authors": ["Markos Diomataris", "Berat Mert Albaba", "Giorgio Becherini", "Partha Ghosh", "Omid Taheri", "Michael J. Black"], "title": "Moving by Looking: Towards Vision-Driven Avatar Motion Generation", "comment": null, "summary": "The way we perceive the world fundamentally shapes how we move, whether it is\nhow we navigate in a room or how we interact with other humans. Current human\nmotion generation methods, neglect this interdependency and use task-specific\n``perception'' that differs radically from that of humans. We argue that the\ngeneration of human-like avatar behavior requires human-like perception.\nConsequently, in this work we present CLOPS, the first human avatar that solely\nuses egocentric vision to perceive its surroundings and navigate. Using vision\nas the primary driver of motion however, gives rise to a significant challenge\nfor training avatars: existing datasets have either isolated human motion,\nwithout the context of a scene, or lack scale. We overcome this challenge by\ndecoupling the learning of low-level motion skills from learning of high-level\ncontrol that maps visual input to motion. First, we train a motion prior model\non a large motion capture dataset. Then, a policy is trained using Q-learning\nto map egocentric visual inputs to high-level control commands for the motion\nprior. Our experiments empirically demonstrate that egocentric vision can give\nrise to human-like motion characteristics in our avatars. For example, the\navatars walk such that they avoid obstacles present in their visual field.\nThese findings suggest that equipping avatars with human-like sensors,\nparticularly egocentric vision, holds promise for training avatars that behave\nlike humans.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86CLOPS\uff0c\u4e00\u4e2a\u5229\u7528\u81ea\u6211\u4e2d\u5fc3\u89c6\u89c9\u8fdb\u884c\u5bfc\u822a\u548c\u611f\u77e5\u7684\u9996\u4e2a\u865a\u62df\u4eba\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u4eba\u7c7b\u611f\u77e5\u4e0e\u8fd0\u52a8\u76f8\u4e92\u4f9d\u8d56\u6027\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u4eba\u4f53\u8fd0\u52a8\u751f\u6210\u65b9\u6cd5\u5ffd\u89c6\u4e86\u4eba\u7c7b\u611f\u77e5\u4e0e\u8fd0\u52a8\u4e4b\u95f4\u7684\u76f8\u4e92\u4f9d\u8d56\u6027\uff0c\u5e76\u4e14\u4f7f\u7528\u4e86\u4e0e\u4eba\u7c7b\u622a\u7136\u4e0d\u540c\u4e14\u7279\u5b9a\u4e8e\u4efb\u52a1\u7684\u201c\u611f\u77e5\u201d\u65b9\u5f0f\u3002\u4e3a\u4e86\u751f\u6210\u7c7b\u4f3c\u4eba\u7c7b\u7684\u865a\u62df\u4eba\u884c\u4e3a\uff0c\u9700\u8981\u6a21\u62df\u4eba\u7c7b\u7684\u611f\u77e5\u65b9\u5f0f\u3002\u56e0\u6b64\uff0c\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5229\u7528\u81ea\u6211\u4e2d\u5fc3\u89c6\u89c9\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u8be5\u7814\u7a76\u9996\u5148\u5728\u4e00\u4e2a\u5927\u578b\u52a8\u4f5c\u6355\u6349\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u4e86\u4e00\u4e2a\u8fd0\u52a8\u5148\u9a8c\u6a21\u578b\uff0c\u7136\u540e\u4f7f\u7528Q\u5b66\u4e60\u8bad\u7ec3\u4e86\u4e00\u4e2a\u7b56\u7565\uff0c\u5c06\u81ea\u6211\u4e2d\u5fc3\u89c6\u89c9\u8f93\u5165\u6620\u5c04\u5230\u8fd0\u52a8\u5148\u9a8c\u7684\u9ad8\u5c42\u63a7\u5236\u6307\u4ee4\u3002\u8fd9\u79cd\u65b9\u6cd5\u5c06\u4f4e\u5c42\u8fd0\u52a8\u6280\u80fd\u7684\u5b66\u4e60\u4e0e\u9ad8\u5c42\u89c6\u89c9\u63a7\u5236\u7684\u5b66\u4e60\u5206\u79bb\u5f00\u6765\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u4f7f\u7528\u81ea\u6211\u4e2d\u5fc3\u89c6\u89c9\u53ef\u4ee5\u4f7f\u865a\u62df\u4eba\u7684\u8fd0\u52a8\u7279\u5f81\u66f4\u50cf\u4eba\u7c7b\uff0c\u4f8b\u5982\uff0c\u865a\u62df\u4eba\u80fd\u591f\u907f\u5f00\u89c6\u91ce\u4e2d\u7684\u969c\u788d\u7269\u3002", "conclusion": "\u5c06\u7c7b\u4f3c\u4eba\u7c7b\u7684\u4f20\u611f\u5668\uff0c\u7279\u522b\u662f\u81ea\u6211\u4e2d\u5fc3\u89c6\u89c9\uff0c\u5e94\u7528\u4e8e\u865a\u62df\u4eba\uff0c\u6709\u671b\u5b9e\u73b0\u66f4\u50cf\u4eba\u7c7b\u884c\u4e3a\u7684\u865a\u62df\u4eba\u8bad\u7ec3\u3002"}}
{"id": "2509.19197", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19197", "abs": "https://arxiv.org/abs/2509.19197", "authors": ["Abdul-Rauf Nuhu", "Parham Kebria", "Vahid Hemmati", "Benjamin Lartey", "Mahmoud Nabil Mahmoud", "Abdollah Homaifar", "Edward Tunstel"], "title": "A Validation Strategy for Deep Learning Models: Evaluating and Enhancing Robustness", "comment": null, "summary": "Data-driven models, especially deep learning classifiers often demonstrate\ngreat success on clean datasets. Yet, they remain vulnerable to common data\ndistortions such as adversarial and common corruption perturbations. These\nperturbations can significantly degrade performance, thereby challenging the\noverall reliability of the models. Traditional robustness validation typically\nrelies on perturbed test datasets to assess and improve model performance. In\nour framework, however, we propose a validation approach that extracts \"weak\nrobust\" samples directly from the training dataset via local robustness\nanalysis. These samples, being the most susceptible to perturbations, serve as\nan early and sensitive indicator of the model's vulnerabilities. By evaluating\nmodels on these challenging training instances, we gain a more nuanced\nunderstanding of its robustness, which informs targeted performance\nenhancement. We demonstrate the effectiveness of our approach on models trained\nwith CIFAR-10, CIFAR-100, and ImageNet, highlighting how robustness validation\nguided by weak robust samples can drive meaningful improvements in model\nreliability under adversarial and common corruption scenarios.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u4ece\u8bad\u7ec3\u6570\u636e\u4e2d\u63d0\u53d6\u201c\u5f31\u9c81\u68d2\u201d\u6837\u672c\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u63d0\u5347\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u9c81\u68d2\u6027\uff0c\u4ee5\u5e94\u5bf9\u5bf9\u6297\u6027\u548c\u5e38\u89c1\u6270\u52a8\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u5e72\u51c0\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u9762\u5bf9\u5bf9\u6297\u6027\u548c\u5e38\u89c1\u6270\u52a8\u65f6\u9c81\u68d2\u6027\u4e0d\u8db3\uff0c\u5f71\u54cd\u4e86\u6a21\u578b\u7684\u53ef\u9760\u6027\u3002\u4f20\u7edf\u7684\u9c81\u68d2\u6027\u9a8c\u8bc1\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u6270\u52a8\u6d4b\u8bd5\u6570\u636e\u96c6\uff0c\u800c\u672c\u7814\u7a76\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u9a8c\u8bc1\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u901a\u8fc7\u5c40\u90e8\u9c81\u68d2\u6027\u5206\u6790\u4ece\u8bad\u7ec3\u6570\u636e\u96c6\u4e2d\u63d0\u53d6\u201c\u5f31\u9c81\u68d2\u201d\u6837\u672c\u7684\u9a8c\u8bc1\u65b9\u6cd5\u3002\u8fd9\u4e9b\u6837\u672c\u6700\u5bb9\u6613\u53d7\u5230\u6270\u52a8\u7684\u5f71\u54cd\uff0c\u53ef\u4ee5\u4f5c\u4e3a\u6a21\u578b\u8106\u5f31\u6027\u7684\u65e9\u671f\u654f\u611f\u6307\u6807\u3002\u901a\u8fc7\u5728\u8fd9\u4e9b\u5177\u6709\u6311\u6218\u6027\u7684\u8bad\u7ec3\u5b9e\u4f8b\u4e0a\u8bc4\u4f30\u6a21\u578b\uff0c\u53ef\u4ee5\u66f4\u6df1\u5165\u5730\u7406\u89e3\u6a21\u578b\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u6307\u5bfc\u6709\u9488\u5bf9\u6027\u7684\u6027\u80fd\u63d0\u5347\u3002", "result": "\u5728 CIFAR-10\u3001CIFAR-100 \u548c ImageNet \u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\u4e0a\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u8868\u660e\u57fa\u4e8e\u5f31\u9c81\u68d2\u6837\u672c\u7684\u9c81\u68d2\u6027\u9a8c\u8bc1\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u6a21\u578b\u5728\u5bf9\u6297\u6027\u548c\u5e38\u89c1\u6270\u52a8\u4e0b\u7684\u53ef\u9760\u6027\u3002", "conclusion": "\u901a\u8fc7\u4ece\u8bad\u7ec3\u6570\u636e\u4e2d\u63d0\u53d6\u201c\u5f31\u9c81\u68d2\u201d\u6837\u672c\u8fdb\u884c\u5206\u6790\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u8bc4\u4f30\u548c\u6539\u8fdb\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u9c81\u68d2\u6027\uff0c\u4ece\u800c\u63d0\u9ad8\u6a21\u578b\u5728\u5404\u79cd\u6270\u52a8\u4e0b\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2509.19282", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19282", "abs": "https://arxiv.org/abs/2509.19282", "authors": ["Bingnan Li", "Chen-Yu Wang", "Haiyang Xu", "Xiang Zhang", "Ethan Armand", "Divyansh Srivastava", "Xiaojun Shan", "Zeyuan Chen", "Jianwen Xie", "Zhuowen Tu"], "title": "OverLayBench: A Benchmark for Layout-to-Image Generation with Dense Overlaps", "comment": "Accepted to NeurIPS 2025 Dataset&Benchmark Track", "summary": "Despite steady progress in layout-to-image generation, current methods still\nstruggle with layouts containing significant overlap between bounding boxes. We\nidentify two primary challenges: (1) large overlapping regions and (2)\noverlapping instances with minimal semantic distinction. Through both\nqualitative examples and quantitative analysis, we demonstrate how these\nfactors degrade generation quality. To systematically assess this issue, we\nintroduce OverLayScore, a novel metric that quantifies the complexity of\noverlapping bounding boxes. Our analysis reveals that existing benchmarks are\nbiased toward simpler cases with low OverLayScore values, limiting their\neffectiveness in evaluating model performance under more challenging\nconditions. To bridge this gap, we present OverLayBench, a new benchmark\nfeaturing high-quality annotations and a balanced distribution across different\nlevels of OverLayScore. As an initial step toward improving performance on\ncomplex overlaps, we also propose CreatiLayout-AM, a model fine-tuned on a\ncurated amodal mask dataset. Together, our contributions lay the groundwork for\nmore robust layout-to-image generation under realistic and challenging\nscenarios. Project link: https://mlpc-ucsd.github.io/OverLayBench.", "AI": {"tldr": "\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u5305\u542b\u5927\u91cf\u91cd\u53e0\u8fb9\u754c\u6846\u7684\u5e03\u5c40\u65f6\u5b58\u5728\u56f0\u96be\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 OverLayScore \u6765\u91cf\u5316\u91cd\u53e0\u7684\u590d\u6742\u6027\uff0c\u5e76\u5f15\u5165 OverLayBench \u57fa\u51c6\u6765\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\uff0c\u540c\u65f6\u63d0\u51fa CreatiLayout-AM \u6a21\u578b\u4ee5\u63d0\u9ad8\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u5e03\u5c40\u5230\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\u5728\u5904\u7406\u8fb9\u754c\u6846\u91cd\u53e0\u65f6\u6548\u679c\u4e0d\u4f73\uff0c\u7279\u522b\u662f\u5728\u91cd\u53e0\u533a\u57df\u5927\u6216\u91cd\u53e0\u5b9e\u4f8b\u8bed\u4e49\u533a\u5206\u5ea6\u4f4e\u7684\u60c5\u51b5\u4e0b\uff0c\u8fd9\u5bfc\u81f4\u4e86\u751f\u6210\u8d28\u91cf\u7684\u4e0b\u964d\u3002\u56e0\u6b64\uff0c\u9700\u8981\u65b0\u7684\u8bc4\u4f30\u65b9\u6cd5\u548c\u57fa\u51c6\u6765\u7cfb\u7edf\u5730\u8bc4\u4f30\u548c\u6539\u8fdb\u6a21\u578b\u5728\u590d\u6742\u91cd\u53e0\u573a\u666f\u4e0b\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86 OverLayScore \u6307\u6807\u6765\u91cf\u5316\u8fb9\u754c\u6846\u91cd\u53e0\u7684\u590d\u6742\u6027\uff0c\u5e76\u636e\u6b64\u6784\u5efa\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6 OverLayBench\uff0c\u8be5\u57fa\u51c6\u5305\u542b\u4e86\u4e0d\u540c\u91cd\u53e0\u590d\u6742\u5ea6\u7684\u6848\u4f8b\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86 CreatiLayout-AM \u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5728\u6c28\u63a9\u7801\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5fae\u8c03\uff0c\u4ee5\u5e94\u5bf9\u590d\u6742\u7684\u91cd\u53e0\u60c5\u51b5\u3002", "result": "\u901a\u8fc7 OverLayScore \u548c OverLayBench \u5206\u6790\u53d1\u73b0\uff0c\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u504f\u5411\u4e8e\u7b80\u5355\u7684\u91cd\u53e0\u60c5\u51b5\u3002CreatiLayout-AM \u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u91cd\u53e0\u65b9\u9762\u5c55\u73b0\u51fa\u6539\u8fdb\u7684\u6f5c\u529b\u3002", "conclusion": "\u73b0\u6709\u5e03\u5c40\u5230\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\u5728\u5904\u7406\u8fb9\u754c\u6846\u91cd\u53e0\u65b9\u9762\u4ecd\u6709\u5c40\u9650\u6027\u3002\u63d0\u51fa\u7684 OverLayScore \u548c OverLayBench \u4e3a\u8bc4\u4f30\u548c\u6539\u8fdb\u6a21\u578b\u5728\u590d\u6742\u91cd\u53e0\u573a\u666f\u4e0b\u7684\u6027\u80fd\u63d0\u4f9b\u4e86\u65b0\u7684\u5de5\u5177\u548c\u65b9\u5411\u3002CreatiLayout-AM \u662f\u671d\u7740\u63d0\u9ad8\u6a21\u578b\u5728\u73b0\u5b9e\u590d\u6742\u573a\u666f\u4e0b\u751f\u6210\u80fd\u529b\u8fc8\u51fa\u7684\u521d\u6b65\u5c1d\u8bd5\u3002"}}
{"id": "2509.19215", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19215", "abs": "https://arxiv.org/abs/2509.19215", "authors": ["Juntong Ni", "Saurabh Kataria", "Shengpu Tang", "Carl Yang", "Xiao Hu", "Wei Jin"], "title": "PPG-Distill: Efficient Photoplethysmography Signals Analysis via Foundation Model Distillation", "comment": "Accepted at NeurIPS 2025 Workshop on Learning from Time Series for\n  Health", "summary": "Photoplethysmography (PPG) is widely used in wearable health monitoring, yet\nlarge PPG foundation models remain difficult to deploy on resource-limited\ndevices. We present PPG-Distill, a knowledge distillation framework that\ntransfers both global and local knowledge through prediction-, feature-, and\npatch-level distillation. PPG-Distill incorporates morphology distillation to\npreserve local waveform patterns and rhythm distillation to capture inter-patch\ntemporal structures. On heart rate estimation and atrial fibrillation\ndetection, PPG-Distill improves student performance by up to 21.8% while\nachieving 7X faster inference and reducing memory usage by 19X, enabling\nefficient PPG analysis on wearables", "AI": {"tldr": "PPG-Distill\u662f\u4e00\u4e2a\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u5c42\u84b8\u998f\uff08\u9884\u6d4b\u3001\u7279\u5f81\u3001\u8865\u4e01\uff09\u548c\u4e13\u95e8\u7684\u5f62\u6001/\u8282\u5f8b\u84b8\u998f\uff0c\u5c06\u5927\u578bPPG\u6a21\u578b\u7684\u77e5\u8bc6\u8f6c\u79fb\u5230\u8d44\u6e90\u53d7\u9650\u7684\u8bbe\u5907\u4e0a\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684PPG\u5206\u6790\u3002", "motivation": "\u7531\u4e8e\u5927\u578bPPG\u57fa\u7840\u6a21\u578b\u96be\u4ee5\u90e8\u7f72\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8bbe\u5907\u4e0a\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u6765\u5728\u8fd9\u4e9b\u8bbe\u5907\u4e0a\u8fdb\u884cPPG\u5206\u6790\u3002", "method": "PPG-Distill\u6846\u67b6\u901a\u8fc7\u9884\u6d4b\u7ea7\u3001\u7279\u5f81\u7ea7\u548c\u8865\u4e01\u7ea7\u84b8\u998f\u6765\u4f20\u9012\u5168\u5c40\u548c\u5c40\u90e8\u77e5\u8bc6\u3002\u5b83\u8fd8\u7ed3\u5408\u4e86\u5f62\u6001\u84b8\u998f\u6765\u4fdd\u7559\u5c40\u90e8\u6ce2\u5f62\u6a21\u5f0f\uff0c\u4ee5\u53ca\u8282\u5f8b\u84b8\u998f\u6765\u6355\u83b7\u8865\u4e01\u95f4\u7684\u65f6\u95f4\u7ed3\u6784\u3002", "result": "\u5728\u5fc3\u7387\u4f30\u8ba1\u548c\u623f\u98a4\u68c0\u6d4b\u4efb\u52a1\u4e2d\uff0cPPG-Distill\u5c06\u5b66\u751f\u6a21\u578b\u7684\u6027\u80fd\u63d0\u9ad8\u4e8621.8%\uff0c\u540c\u65f6\u5b9e\u73b0\u4e867\u500d\u7684\u63a8\u7406\u901f\u5ea6\u63d0\u5347\u548c19\u500d\u7684\u5185\u5b58\u4f7f\u7528\u91cf\u51cf\u5c11\u3002", "conclusion": "PPG-Distill\u80fd\u591f\u6709\u6548\u5730\u5c06\u5927\u578bPPG\u6a21\u578b\u7684\u77e5\u8bc6\u8f6c\u79fb\u5230\u8d44\u6e90\u53d7\u9650\u7684\u8bbe\u5907\u4e0a\uff0c\u4ece\u800c\u5b9e\u73b0\u9ad8\u6548\u7684PPG\u5206\u6790\uff0c\u9002\u7528\u4e8e\u53ef\u7a7f\u6234\u8bbe\u5907\u3002"}}
{"id": "2509.19297", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19297", "abs": "https://arxiv.org/abs/2509.19297", "authors": ["Weijie Wang", "Yeqing Chen", "Zeyu Zhang", "Hengyu Liu", "Haoxiao Wang", "Zhiyuan Feng", "Wenkang Qin", "Zheng Zhu", "Donny Y. Chen", "Bohan Zhuang"], "title": "VolSplat: Rethinking Feed-Forward 3D Gaussian Splatting with Voxel-Aligned Prediction", "comment": "Project Page: https://lhmd.top/volsplat, Code:\n  https://github.com/ziplab/VolSplat", "summary": "Feed-forward 3D Gaussian Splatting (3DGS) has emerged as a highly effective\nsolution for novel view synthesis. Existing methods predominantly rely on a\npixel-aligned Gaussian prediction paradigm, where each 2D pixel is mapped to a\n3D Gaussian. We rethink this widely adopted formulation and identify several\ninherent limitations: it renders the reconstructed 3D models heavily dependent\non the number of input views, leads to view-biased density distributions, and\nintroduces alignment errors, particularly when source views contain occlusions\nor low texture. To address these challenges, we introduce VolSplat, a new\nmulti-view feed-forward paradigm that replaces pixel alignment with\nvoxel-aligned Gaussians. By directly predicting Gaussians from a predicted 3D\nvoxel grid, it overcomes pixel alignment's reliance on error-prone 2D feature\nmatching, ensuring robust multi-view consistency. Furthermore, it enables\nadaptive control over Gaussian density based on 3D scene complexity, yielding\nmore faithful Gaussian point clouds, improved geometric consistency, and\nenhanced novel-view rendering quality. Experiments on widely used benchmarks\nincluding RealEstate10K and ScanNet demonstrate that VolSplat achieves\nstate-of-the-art performance while producing more plausible and view-consistent\nGaussian reconstructions. In addition to superior results, our approach\nestablishes a more scalable framework for feed-forward 3D reconstruction with\ndenser and more robust representations, paving the way for further research in\nwider communities. The video results, code and trained models are available on\nour project page: https://lhmd.top/volsplat.", "AI": {"tldr": "3DGS\u7684\u50cf\u7d20\u5bf9\u9f50\u8303\u5f0f\u5b58\u5728\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u4f53\u7d20\u5bf9\u9f50\u7684\u9ad8\u65af \u0935\u093e\u092a\u0930\u093e\u65b9\u6cd5VolSplat\uff0c\u63d0\u9ad8\u4e86\u91cd\u5efa\u8d28\u91cf\u548c\u89c6\u56fe\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u67093DGS\u65b9\u6cd5\u4f9d\u8d56\u50cf\u7d20\u5bf9\u9f50\uff0c\u5b58\u5728\u89c6\u56fe\u4f9d\u8d56\u3001\u5bc6\u5ea6\u504f\u5dee\u548c\u5bf9\u906e\u6321/\u4f4e\u7eb9\u7406\u654f\u611f\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51faVolSplat\uff0c\u7528\u4f53\u7d20\u5bf9\u9f50\u7684\u9ad8\u65af\u4ee3\u66ff\u50cf\u7d20\u5bf9\u9f50\uff0c\u76f4\u63a5\u4ece\u9884\u6d4b\u76843D\u4f53\u7d20\u7f51\u683c\u9884\u6d4b\u9ad8\u65af\uff0c\u5b9e\u73b0\u81ea\u9002\u5e94\u5bc6\u5ea6\u63a7\u5236\u3002", "result": "VolSplat\u5728RealEstate10K\u548cScanNet\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u751f\u6210\u7684\u6a21\u578b\u66f4\u5408\u7406\u3001\u89c6\u56fe\u66f4\u4e00\u81f4\u3002", "conclusion": "VolSplat\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u63d0\u4f9b\u4e86\u66f4\u5177\u53ef\u6269\u5c55\u6027\u3001\u66f4\u5bc6\u96c6\u3001\u66f4\u9c81\u68d2\u76843D\u91cd\u5efa\u6846\u67b6\u3002"}}
{"id": "2509.19222", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19222", "abs": "https://arxiv.org/abs/2509.19222", "authors": ["Julien Delavande", "Regis Pierrard", "Sasha Luccioni"], "title": "Video Killed the Energy Budget: Characterizing the Latency and Power Regimes of Open Text-to-Video Models", "comment": "10 pages. Accepted as an oral presentation at the NeurIPS 2025\n  NextVid Workshop (San Diego, December 6, 2025)", "summary": "Recent advances in text-to-video (T2V) generation have enabled the creation\nof high-fidelity, temporally coherent clips from natural language prompts. Yet\nthese systems come with significant computational costs, and their energy\ndemands remain poorly understood. In this paper, we present a systematic study\nof the latency and energy consumption of state-of-the-art open-source T2V\nmodels. We first develop a compute-bound analytical model that predicts scaling\nlaws with respect to spatial resolution, temporal length, and denoising steps.\nWe then validate these predictions through fine-grained experiments on\nWAN2.1-T2V, showing quadratic growth with spatial and temporal dimensions, and\nlinear scaling with the number of denoising steps. Finally, we extend our\nanalysis to six diverse T2V models, comparing their runtime and energy profiles\nunder default settings. Our results provide both a benchmark reference and\npractical insights for designing and deploying more sustainable generative\nvideo systems.", "AI": {"tldr": "\u7814\u7a76\u4e86\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u6a21\u578b\uff08T2V\uff09\u7684\u8ba1\u7b97\u6210\u672c\u548c\u80fd\u8017\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5206\u6790\u6a21\u578b\u6765\u9884\u6d4b\u5176\u6027\u80fd\u3002", "motivation": "T2V\u6a21\u578b\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u4f46\u5176\u80fd\u6e90\u6d88\u8017\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u8ba1\u7b97\u5bc6\u96c6\u578b\u5206\u6790\u6a21\u578b\uff0c\u9884\u6d4b\u4e86\u6a21\u578b\u5728\u7a7a\u95f4\u5206\u8fa8\u7387\u3001\u65f6\u95f4\u957f\u5ea6\u548c\u53bb\u566a\u6b65\u6570\u65b9\u9762\u7684\u6269\u5c55\u89c4\u5f8b\uff0c\u5e76\u901a\u8fc7\u5728WAN2.1-T2V\u4e0a\u7684\u5b9e\u9a8c\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u968f\u540e\u5c06\u5206\u6790\u6269\u5c55\u5230\u5176\u4ed6\u516d\u4e2aT2V\u6a21\u578b\u3002", "result": "T2V\u6a21\u578b\u5728\u7a7a\u95f4\u5206\u8fa8\u7387\u548c\u65f6\u95f4\u957f\u5ea6\u4e0a\u5448\u73b0\u4e8c\u6b21\u65b9\u589e\u957f\uff0c\u5728\u53bb\u566a\u6b65\u6570\u4e0a\u5448\u73b0\u7ebf\u6027\u589e\u957f\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u8bbe\u8ba1\u548c\u90e8\u7f72\u66f4\u53ef\u6301\u7eed\u7684\u751f\u6210\u5f0f\u89c6\u9891\u7cfb\u7edf\u63d0\u4f9b\u4e86\u57fa\u51c6\u53c2\u8003\u548c\u5b9e\u8df5\u6307\u5bfc\u3002"}}
{"id": "2509.19300", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19300", "abs": "https://arxiv.org/abs/2509.19300", "authors": ["Chen Chen", "Pengsheng Guo", "Liangchen Song", "Jiasen Lu", "Rui Qian", "Xinze Wang", "Tsu-Jui Fu", "Wei Liu", "Yinfei Yang", "Alex Schwing"], "title": "CAR-Flow: Condition-Aware Reparameterization Aligns Source and Target for Better Flow Matching", "comment": null, "summary": "Conditional generative modeling aims to learn a conditional data distribution\nfrom samples containing data-condition pairs. For this, diffusion and\nflow-based methods have attained compelling results. These methods use a\nlearned (flow) model to transport an initial standard Gaussian noise that\nignores the condition to the conditional data distribution. The model is hence\nrequired to learn both mass transport and conditional injection. To ease the\ndemand on the model, we propose Condition-Aware Reparameterization for Flow\nMatching (CAR-Flow) -- a lightweight, learned shift that conditions the source,\nthe target, or both distributions. By relocating these distributions, CAR-Flow\nshortens the probability path the model must learn, leading to faster training\nin practice. On low-dimensional synthetic data, we visualize and quantify the\neffects of CAR. On higher-dimensional natural image data (ImageNet-256),\nequipping SiT-XL/2 with CAR-Flow reduces FID from 2.07 to 1.68, while\nintroducing less than 0.6% additional parameters.", "AI": {"tldr": "CAR-Flow \u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u53ef\u5b66\u4e60\u7684\u79fb\u4f4d\u65b9\u6cd5\uff0c\u7528\u4e8e\u6761\u4ef6\u751f\u6210\u5efa\u6a21\uff0c\u901a\u8fc7\u8c03\u6574\u6e90\u3001\u76ee\u6807\u6216\u4e24\u4e2a\u5206\u5e03\u6765\u7f29\u77ed\u6a21\u578b\u5fc5\u987b\u5b66\u4e60\u7684\u6982\u7387\u8def\u5f84\uff0c\u4ece\u800c\u52a0\u5feb\u8bad\u7ec3\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u6d41\u548c\u6269\u6563\u7684\u6761\u4ef6\u751f\u6210\u6a21\u578b\u9700\u8981\u5b66\u4e60\u4ece\u9ad8\u65af\u566a\u58f0\u5230\u6761\u4ef6\u6570\u636e\u5206\u5e03\u7684\u4f20\u8f93\uff0c\u8fd9\u65e2\u5305\u62ec\u8d28\u91cf\u4f20\u8f93\uff0c\u4e5f\u5305\u62ec\u6761\u4ef6\u6ce8\u5165\uff0c\u5bf9\u6a21\u578b\u63d0\u51fa\u4e86\u5f88\u9ad8\u7684\u8981\u6c42\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a CAR-Flow \u7684\u6761\u4ef6\u611f\u77e5\u91cd\u53c2\u6570\u5316\u65b9\u6cd5\uff0c\u5b83\u901a\u8fc7\u5b66\u4e60\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u79fb\u4f4d\u6765\u8c03\u6574\u6e90\u3001\u76ee\u6807\u6216\u4e24\u4e2a\u5206\u5e03\uff0c\u4ee5\u7f29\u77ed\u6a21\u578b\u5fc5\u987b\u5b66\u4e60\u7684\u6982\u7387\u8def\u5f84\u3002", "result": "\u5728\u4f4e\u7ef4\u5408\u6210\u6570\u636e\u4e0a\uff0cCAR \u7684\u6548\u679c\u5f97\u5230\u4e86\u53ef\u89c6\u5316\u548c\u91cf\u5316\u3002\u5728 ImageNet-256 \u6570\u636e\u96c6\u4e0a\uff0c\u4f7f\u7528 CAR-Flow \u6539\u8fdb\u7684 SiT-XL/2 \u6a21\u578b\u5c06 FID \u4ece 2.07 \u964d\u4f4e\u5230 1.68\uff0c\u540c\u65f6\u4ec5\u589e\u52a0\u4e86\u4e0d\u5230 0.6% \u7684\u53c2\u6570\u3002", "conclusion": "CAR-Flow \u662f\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u52a0\u5feb\u6761\u4ef6\u751f\u6210\u6a21\u578b\u7684\u8bad\u7ec3\u901f\u5ea6\uff0c\u5e76\u63d0\u9ad8\u5176\u6027\u80fd\uff0c\u540c\u65f6\u5bf9\u6a21\u578b\u53c2\u6570\u7684\u589e\u52a0\u91cf\u5f88\u5c0f\u3002"}}
{"id": "2509.19233", "categories": ["cs.LG", "I.2.0; I.2.4; I.2.6"], "pdf": "https://arxiv.org/pdf/2509.19233", "abs": "https://arxiv.org/abs/2509.19233", "authors": ["Milad Leyli-abadi", "Antoine Marot", "J\u00e9r\u00f4me Picault"], "title": "Study Design and Demystification of Physics Informed Neural Networks for Power Flow Simulation", "comment": "Accepted at ECML PKDD ML4SPS 2025 workshop", "summary": "In the context of the energy transition, with increasing integration of\nrenewable sources and cross-border electricity exchanges, power grids are\nencountering greater uncertainty and operational risk. Maintaining grid\nstability under varying conditions is a complex task, and power flow simulators\nare commonly used to support operators by evaluating potential actions before\nimplementation. However, traditional physical solvers, while accurate, are\noften too slow for near real-time use. Machine learning models have emerged as\nfast surrogates, and to improve their adherence to physical laws (e.g.,\nKirchhoff's laws), they are often trained with embedded constraints which are\nalso known as physics-informed or hybrid models. This paper presents an\nablation study to demystify hybridization strategies, ranging from\nincorporating physical constraints as regularization terms or unsupervised\nlosses, and exploring model architectures from simple multilayer perceptrons to\nadvanced graph-based networks enabling the direct optimization of physics\nequations. Using our custom benchmarking pipeline for hybrid models called\nLIPS, we evaluate these models across four dimensions: accuracy, physical\ncompliance, industrial readiness, and out-of-distribution generalization. The\nresults highlight how integrating physical knowledge impacts performance across\nthese criteria. All the implementations are reproducible and provided in the\ncorresponding Github page.", "AI": {"tldr": "\u53ef\u518d\u751f\u80fd\u6e90\u6574\u5408\u548c\u8de8\u754c\u7535\u529b\u4ea4\u6362\u589e\u52a0\uff0c\u5bfc\u81f4\u7535\u7f51\u9762\u4e34\u66f4\u5927\u4e0d\u786e\u5b9a\u6027\u548c\u8fd0\u884c\u98ce\u9669\u3002\u4f20\u7edf\u7269\u7406\u6c42\u89e3\u5668\u901f\u5ea6\u6162\uff0c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u53ef\u4f5c\u4e3a\u5feb\u901f\u66ff\u4ee3\uff0c\u4f46\u9700\u5d4c\u5165\u7269\u7406\u5b9a\u5f8b\uff08\u5982\u57fa\u5c14\u970d\u592b\u5b9a\u5f8b\uff09\u4ee5\u63d0\u9ad8\u5176\u51c6\u786e\u6027\u3002\u672c\u7814\u7a76\u901a\u8fc7\u6d88\u878d\u5b9e\u9a8c\uff0c\u7814\u7a76\u4e86\u4ece\u6b63\u5219\u5316\u9879\u6216\u65e0\u76d1\u7763\u635f\u5931\u4e2d\u5d4c\u5165\u7269\u7406\u7ea6\u675f\uff0c\u4ee5\u53ca\u4ece\u591a\u5c42\u611f\u77e5\u673a\u5230\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u6a21\u578b\u67b6\u6784\u7b49\u6df7\u5408\u7b56\u7565\u3002", "motivation": "\u5728\u80fd\u6e90\u8f6c\u578b\u80cc\u666f\u4e0b\uff0c\u9700\u8981\u89e3\u51b3\u65e5\u76ca\u589e\u957f\u7684\u53ef\u518d\u751f\u80fd\u6e90\u6574\u5408\u548c\u8de8\u754c\u7535\u529b\u4ea4\u6362\u5e26\u6765\u7684\u7535\u7f51\u4e0d\u786e\u5b9a\u6027\u548c\u8fd0\u884c\u98ce\u9669\u95ee\u9898\uff0c\u800c\u4f20\u7edf\u7269\u7406\u6c42\u89e3\u5668\u901f\u5ea6\u6162\uff0c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u53ef\u4ee5\u4f5c\u4e3a\u4e00\u79cd\u5feb\u901f\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u9700\u8981\u4fdd\u8bc1\u5176\u7269\u7406\u9075\u5faa\u6027\u3002", "method": "\u901a\u8fc7\u6d88\u878d\u5b9e\u9a8c\uff0c\u7814\u7a76\u4e0d\u540c\u6df7\u5408\u7b56\u7565\uff08\u5982\u5c06\u7269\u7406\u7ea6\u675f\u4f5c\u4e3a\u6b63\u5219\u5316\u9879\u6216\u65e0\u76d1\u7763\u635f\u5931\uff09\u4ee5\u53ca\u4e0d\u540c\u6a21\u578b\u67b6\u6784\uff08\u4ece\u591a\u5c42\u611f\u77e5\u673a\u5230\u56fe\u795e\u7ecf\u7f51\u7edc\uff09\u5bf9\u7269\u7406\u4fe1\u606f\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u5f71\u54cd\u3002\u4f7f\u7528\u540d\u4e3aLIPS\u7684\u57fa\u51c6\u6d4b\u8bd5\u6d41\u7a0b\uff0c\u4ece\u51c6\u786e\u6027\u3001\u7269\u7406\u5408\u89c4\u6027\u3001\u5de5\u4e1a\u51c6\u5907\u5ea6\u548c\u5206\u5e03\u5916\u6cdb\u5316\u80fd\u529b\u56db\u4e2a\u7ef4\u5ea6\u8bc4\u4f30\u6a21\u578b\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u6574\u5408\u7269\u7406\u77e5\u8bc6\u5bf9\u6a21\u578b\u7684\u51c6\u786e\u6027\u3001\u7269\u7406\u5408\u89c4\u6027\u3001\u5de5\u4e1a\u51c6\u5907\u5ea6\u548c\u5206\u5e03\u5916\u6cdb\u5316\u80fd\u529b\u7b49\u591a\u4e2a\u7ef4\u5ea6\u90fd\u4ea7\u751f\u4e86\u5f71\u54cd\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u6d88\u878d\u5b9e\u9a8c\u548c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6df1\u5165\u63a2\u8ba8\u4e86\u6df7\u5408\u7b56\u7565\u5bf9\u7269\u7406\u4fe1\u606f\u673a\u5668\u5b66\u4e60\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u4e3a\u63d0\u9ad8\u7535\u7f51\u7a33\u5b9a\u6027\u548c\u8fd0\u884c\u6548\u7387\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\u3002"}}
{"id": "2509.19284", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19284", "abs": "https://arxiv.org/abs/2509.19284", "authors": ["Yunzhen Feng", "Julia Kempe", "Cheng Zhang", "Parag Jain", "Anthony Hartshorn"], "title": "What Characterizes Effective Reasoning? Revisiting Length, Review, and Structure of CoT", "comment": null, "summary": "Large reasoning models (LRMs) spend substantial test-time compute on long\nchain-of-thought (CoT) traces, but what *characterizes* an effective CoT\nremains unclear. While prior work reports gains from lengthening CoTs and\nincreasing review (revisiting earlier steps) via appended *wait* tokens, recent\nstudies suggest that shorter thinking can outperform longer traces. We\ntherefore conduct a systematic evaluation across ten LRMs on math and\nscientific reasoning. Contrary to the \"longer-is-better\" narrative, we find\nthat both naive CoT lengthening and increased review are associated with\n*lower* accuracy.\n  As CoT unfolds step by step, token-level metrics can conflate verbosity with\nprocess quality. We introduce a graph view of CoT to extract structure and\nidentify a single statistic-the *Failed-Step Fraction (FSF)*, the fraction of\nsteps in abandoned branches-that consistently outpredicts length and review\nratio for correctness across models. To probe causality, we design two\ninterventions. First, we rank candidate CoTs by each metric at test time, where\nFSF yields the largest pass@1 gains; second, we edit CoTs to remove failed\nbranches, which significantly improves accuracy, indicating that failed\nbranches bias subsequent reasoning. Taken together, these results characterize\neffective CoTs as those that *fail less* and support *structure-aware*\ntest-time scaling over indiscriminately generating long CoT.", "AI": {"tldr": "\u6709\u6548\u6027 CoT \u7684\u7279\u70b9\u662f\u5931\u8d25\u7684\u6b65\u9aa4\u66f4\u5c11\uff0c\u5e76\u4e14\u652f\u6301\u7ed3\u6784\u611f\u77e5\u6d4b\u8bd5\u65f6\u95f4\u6269\u5c55\uff0c\u800c\u4e0d\u662f\u65e0\u5dee\u522b\u5730\u751f\u6210\u957f CoT\u3002", "motivation": " LRMs \u5728\u957f CoT \u8f68\u8ff9\u4e0a\u82b1\u8d39\u5927\u91cf\u7684\u6d4b\u8bd5\u65f6\u95f4\u8ba1\u7b97\uff0c\u4f46\u6709\u6548\u7684 CoT \u7684\u7279\u5f81\u4ecd\u7136\u4e0d\u6e05\u695a\u3002\u867d\u7136\u4e4b\u524d\u7684\u5de5\u4f5c\u62a5\u544a\u79f0\u901a\u8fc7\u9644\u52a0\u7684\u201c\u7b49\u5f85\u201d\u6807\u8bb0\u6765\u5ef6\u957f CoT \u548c\u589e\u52a0\u5ba1\u67e5\uff08\u91cd\u65b0\u8bbf\u95ee\u65e9\u671f\u6b65\u9aa4\uff09\u53ef\u4ee5\u5e26\u6765\u6536\u76ca\uff0c\u4f46\u6700\u8fd1\u7684\u7814\u7a76\u8868\u660e\uff0c\u8f83\u77ed\u7684\u601d\u8003\u53ef\u4ee5\u4f18\u4e8e\u8f83\u957f\u7684\u8f68\u8ff9\u3002", "method": "\u5f15\u5165 CoT \u7684\u56fe\u89c6\u56fe\u6765\u63d0\u53d6\u7ed3\u6784\uff0c\u5e76\u8bc6\u522b\u5355\u4e2a\u7edf\u8ba1\u6570\u636e\u2014\u2014\u5931\u8d25\u6b65\u9aa4\u5206\u6570 (FSF)\uff0c\u5373\u88ab\u653e\u5f03\u7684\u5206\u652f\u4e2d\u7684\u6b65\u9aa4\u7684\u6bd4\u4f8b\u2014\u2014\u8be5\u5206\u6570\u6301\u7eed\u4f18\u4e8e\u6a21\u578b\u4e4b\u95f4\u7684\u6b63\u786e\u6027\u957f\u5ea6\u548c\u5ba1\u67e5\u6bd4\u7387\u3002", "result": "\u4e0e\u201c\u8d8a\u957f\u8d8a\u597d\u201d\u7684\u8bf4\u6cd5\u76f8\u53cd\uff0c\u6211\u4eec\u53d1\u73b0\u5e7c\u7a1a\u7684 CoT \u5ef6\u957f\u548c\u5ba1\u67e5\u7684\u589e\u52a0\u90fd\u4e0e\u51c6\u786e\u6027\u964d\u4f4e\u6709\u5173\u3002FSF \u5728\u6a21\u578b\u4e4b\u95f4\u7684\u6b63\u786e\u6027\u957f\u5ea6\u548c\u5ba1\u67e5\u6bd4\u7387\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u3002", "conclusion": "\u6709\u6548 CoT \u7684\u7279\u70b9\u662f\u5931\u8d25\u7684\u6b21\u6570\u66f4\u5c11\uff0c\u5e76\u652f\u6301\u7ed3\u6784\u611f\u77e5\u6d4b\u8bd5\u65f6\u95f4\u6269\u5c55\uff0c\u800c\u4e0d\u662f\u65e0\u5dee\u522b\u5730\u751f\u6210\u957f CoT\u3002"}}
{"id": "2509.15156", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.15156", "abs": "https://arxiv.org/abs/2509.15156", "authors": ["Haobo Yang", "Minghao Guo", "Dequan Yang", "Wenyu Wang"], "title": "Leveraging Geometric Visual Illusions as Perceptual Inductive Biases for Vision Models", "comment": null, "summary": "Contemporary deep learning models have achieved impressive performance in\nimage classification by primarily leveraging statistical regularities within\nlarge datasets, but they rarely incorporate structured insights drawn directly\nfrom perceptual psychology. To explore the potential of perceptually motivated\ninductive biases, we propose integrating classic geometric visual illusions\nwell-studied phenomena from human perception into standard image-classification\ntraining pipelines. Specifically, we introduce a synthetic, parametric\ngeometric-illusion dataset and evaluate three multi-source learning strategies\nthat combine illusion recognition tasks with ImageNet classification\nobjectives. Our experiments reveal two key conceptual insights: (i)\nincorporating geometric illusions as auxiliary supervision systematically\nimproves generalization, especially in visually challenging cases involving\nintricate contours and fine textures; and (ii) perceptually driven inductive\nbiases, even when derived from synthetic stimuli traditionally considered\nunrelated to natural image recognition, can enhance the structural sensitivity\nof both CNN and transformer-based architectures. These results demonstrate a\nnovel integration of perceptual science and machine learning and suggest new\ndirections for embedding perceptual priors into vision model design.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
