{"id": "2510.23781", "categories": ["cs.NE"], "pdf": "https://arxiv.org/pdf/2510.23781", "abs": "https://arxiv.org/abs/2510.23781", "authors": ["Peilin He", "Tananun Songdechakraiwut"], "title": "Connectome-Guided Automatic Learning Rates for Deep Networks", "comment": "17 pages, 3 figures, 10 tables", "summary": "The human brain is highly adaptive: its functional connectivity reconfigures\non multiple timescales during cognition and learning, enabling flexible\ninformation processing. By contrast, artificial neural networks typically rely\non manually-tuned learning-rate schedules or generic adaptive optimizers whose\nhyperparameters remain largely agnostic to a model's internal dynamics. In this\npaper, we propose Connectome-Guided Automatic Learning Rate (CG-ALR) that\ndynamically constructs a functional connectome of the neural network from\nneuron co-activations at each training iteration and adjusts learning rates\nonline as this connectome reconfigures. This connectomics-inspired mechanism\nadapts step sizes to the network's dynamic functional organization, slowing\nlearning during unstable reconfiguration and accelerating it when stable\norganization emerges. Our results demonstrate that principles inspired by brain\nconnectomes can inform the design of adaptive learning rates in deep learning,\ngenerally outperforming traditional SGD-based schedules and recent methods.", "AI": {"tldr": "CG-ALR\u662f\u4e00\u79cd\u53d7\u5927\u8111\u8fde\u63a5\u7ec4\u542f\u53d1\u7684\u81ea\u9002\u5e94\u5b66\u4e60\u7387\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u7f51\u7edc\u5185\u90e8\u52a8\u6001\u6765\u4f18\u5316\u6df1\u5ea6\u5b66\u4e60\u8bad\u7ec3\u3002", "motivation": "\u4e0e\u901a\u5e38\u4f9d\u8d56\u624b\u52a8\u8c03\u6574\u6216\u901a\u7528\u4f18\u5316\u5668\u7684\u795e\u7ecf\u7f51\u7edc\u4e0d\u540c\uff0c\u4eba\u8111\u5177\u6709\u9ad8\u5ea6\u9002\u5e94\u6027\uff0c\u5176\u529f\u80fd\u8fde\u63a5\u5728\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u4f1a\u52a8\u6001\u91cd\u6784\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u5728\u8c03\u6574\u5b66\u4e60\u7387\u65b9\u9762\u6ede\u540e\u4e8e\u5176\u5185\u90e8\u52a8\u6001\u3002", "method": "CG-ALR\u5728\u6bcf\u6b21\u8bad\u7ec3\u8fed\u4ee3\u65f6\uff0c\u6839\u636e\u795e\u7ecf\u5143\u5171\u6fc0\u6d3b\u52a8\u6001\u6784\u5efa\u795e\u7ecf\u7f51\u7edc\u7684\u529f\u80fd\u8fde\u63a5\u7ec4\uff0c\u5e76\u5229\u7528\u8be5\u8fde\u63a5\u7ec4\u7684\u91cd\u6784\u4fe1\u606f\u5728\u7ebf\u8c03\u6574\u5b66\u4e60\u7387\u3002\u5f53\u8fde\u63a5\u7ec4\u4e0d\u7a33\u5b9a\u65f6\u51cf\u6162\u5b66\u4e60\uff0c\u5f53\u8fde\u63a5\u7ec4\u7a33\u5b9a\u65f6\u52a0\u5feb\u5b66\u4e60\u3002", "result": "CG-ALR\u65b9\u6cd5\u5728\u6df1\u5ea6\u5b66\u4e60\u4e2d\u80fd\u591f\u9002\u5e94\u6027\u5730\u8c03\u6574\u6b65\u957f\uff0c\u5176\u6027\u80fd\u666e\u904d\u4f18\u4e8e\u4f20\u7edf\u7684\u968f\u673a\u68af\u5ea6\u4e0b\u964d\uff08SGD\uff09\u5b66\u4e60\u7387\u8ba1\u5212\u548c\u8fd1\u671f\u65b9\u6cd5\u3002", "conclusion": "\u53d7\u5927\u8111\u8fde\u63a5\u7ec4\u539f\u7406\u7684\u542f\u53d1\u53ef\u4ee5\u4fc3\u8fdb\u6df1\u5ea6\u5b66\u4e60\u4e2d\u81ea\u9002\u5e94\u5b66\u4e60\u7387\u7684\u8bbe\u8ba1\uff0c\u4ece\u800c\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\u548c\u6027\u80fd\u3002"}}
{"id": "2510.24637", "categories": ["cs.NE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24637", "abs": "https://arxiv.org/abs/2510.24637", "authors": ["Andrea Castagnetti", "Alain Pegatoquet", "Beno\u00eet Miramond"], "title": "All in one timestep: Enhancing Sparsity and Energy efficiency in Multi-level Spiking Neural Networks", "comment": null, "summary": "Spiking Neural Networks (SNNs) are one of the most promising bio-inspired\nneural networks models and have drawn increasing attention in recent years. The\nevent-driven communication mechanism of SNNs allows for sparse and\ntheoretically low-power operations on dedicated neuromorphic hardware. However,\nthe binary nature of instantaneous spikes also leads to considerable\ninformation loss in SNNs, resulting in accuracy degradation. To address this\nissue, we propose a multi-level spiking neuron model able to provide both\nlow-quantization error and minimal inference latency while approaching the\nperformance of full precision Artificial Neural Networks (ANNs). Experimental\nresults with popular network architectures and datasets, show that multi-level\nspiking neurons provide better information compression, allowing therefore a\nreduction in latency without performance loss. When compared to binary SNNs on\nimage classification scenarios, multi-level SNNs indeed allow reducing by 2 to\n3 times the energy consumption depending on the number of quantization\nintervals. On neuromorphic data, our approach allows us to drastically reduce\nthe inference latency to 1 timestep, which corresponds to a compression factor\nof 10 compared to previously published results. At the architectural level, we\npropose a new residual architecture that we call Sparse-ResNet. Through a\ncareful analysis of the spikes propagation in residual connections we highlight\na spike avalanche effect, that affects most spiking residual architectures.\nUsing our Sparse-ResNet architecture, we can provide state-of-the-art accuracy\nresults in image classification while reducing by more than 20% the network\nactivity compared to the previous spiking ResNets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u5c42\u6b21\u8109\u51b2\u795e\u7ecf\u5143\u6a21\u578b\u548c\u7a00\u758f\u6b8b\u5dee\u7f51\u7edc\u67b6\u6784\uff0c\u4ee5\u89e3\u51b3\u4f20\u7edf\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08SNNs\uff09\u4fe1\u606f\u4e22\u5931\u548c\u7cbe\u5ea6\u4e0b\u964d\u7684\u95ee\u9898\u3002", "motivation": "\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08SNNs\uff09\u867d\u7136\u5177\u6709\u4f4e\u529f\u8017\u6f5c\u529b\uff0c\u4f46\u5176\u4e8c\u5143\u8109\u51b2\u7684\u7279\u6027\u5bfc\u81f4\u4fe1\u606f\u4e22\u5931\uff0c\u5f71\u54cd\u7cbe\u5ea6\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u51cf\u5c11\u91cf\u5316\u8bef\u5dee\u5e76\u4fdd\u6301\u4f4e\u63a8\u7406\u5ef6\u8fdf\u7684\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u5c42\u6b21\u8109\u51b2\u795e\u7ecf\u5143\u6a21\u578b\uff0c\u7528\u4e8e\u5728\u4f4e\u91cf\u5316\u8bef\u5dee\u548c\u4f4e\u63a8\u7406\u5ef6\u8fdf\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u7684\u7a00\u758f\u6b8b\u5dee\u7f51\u7edc\uff08Sparse-ResNet\uff09\u67b6\u6784\uff0c\u4ee5\u89e3\u51b3\u8109\u51b2\u6b8b\u5dee\u8fde\u63a5\u4e2d\u7684\u201c\u96ea\u5d29\u6548\u5e94\u201d\u3002", "result": "\u4e0e\u4e8c\u5143SNNs\u76f8\u6bd4\uff0c\u591a\u5c42\u6b21SNNs\u5728\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u53ef\u5c06\u80fd\u8017\u964d\u4f4e2\u52303\u500d\u3002\u5728\u795e\u7ecf\u5f62\u6001\u6570\u636e\u4e0a\uff0c\u63a8\u7406\u5ef6\u8fdf\u53ef\u964d\u4f4e\u52301\u4e2a\u65f6\u95f4\u6b65\u957f\uff0c\u538b\u7f29\u56e0\u5b50\u4e3a10\u3002\u7a00\u758f\u6b8b\u5dee\u7f51\u7edc\u5c06\u7f51\u7edc\u6d3b\u52a8\u51cf\u5c11\u4e8620%\u4ee5\u4e0a\uff0c\u540c\u65f6\u5728\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u591a\u5c42\u6b21\u8109\u51b2\u795e\u7ecf\u5143\u548c\u7a00\u758f\u6b8b\u5dee\u7f51\u7edc\u67b6\u6784\u80fd\u591f\u6709\u6548\u63d0\u9ad8SNNs\u7684\u6027\u80fd\uff0c\u51cf\u5c11\u80fd\u8017\u548c\u63a8\u7406\u5ef6\u8fdf\uff0c\u5e76\u5728\u56fe\u50cf\u5206\u7c7b\u7b49\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e0e\u5168\u7cbe\u5ea6\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\uff08ANNs\uff09\u76f8\u5f53\u7684\u6c34\u5e73\u3002"}}
{"id": "2510.23980", "categories": ["cs.LG", "cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2510.23980", "abs": "https://arxiv.org/abs/2510.23980", "authors": ["Guojing Cong", "Tom Potok", "Hamed Poursiami", "Maryam Parsa"], "title": "HyperGraphX: Graph Transductive Learning with Hyperdimensional Computing and Message Passing", "comment": null, "summary": "We present a novel algorithm, \\hdgc, that marries graph convolution with\nbinding and bundling operations in hyperdimensional computing for transductive\ngraph learning. For prediction accuracy \\hdgc outperforms major and popular\ngraph neural network implementations as well as state-of-the-art\nhyperdimensional computing implementations for a collection of homophilic\ngraphs and heterophilic graphs. Compared with the most accurate learning\nmethodologies we have tested, on the same target GPU platform, \\hdgc is on\naverage 9561.0 and 144.5 times faster than \\gcnii, a graph neural network\nimplementation and HDGL, a hyperdimensional computing implementation,\nrespectively. As the majority of the learning operates on binary vectors, we\nexpect outstanding energy performance of \\hdgc on neuromorphic and emerging\nprocess-in-memory devices.", "AI": {"tldr": "\u6211\u4eec\u63d0\u51fa\u4e86hdgc\uff0c\u4e00\u79cd\u7ed3\u5408\u4e86\u56fe\u5377\u79ef\u3001\u7ed1\u5b9a\u548c\u6346\u7ed1\u64cd\u4f5c\u7684\u8d85\u7ef4\u8ba1\u7b97\u7b97\u6cd5\uff0c\u7528\u4e8e\u5f52\u7eb3\u56fe\u5b66\u4e60\uff0c\u5728\u51c6\u786e\u6027\u548c\u901f\u5ea6\u4e0a\u90fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5c06\u56fe\u5377\u79ef\u4e0e\u8d85\u7ef4\u8ba1\u7b97\u4e2d\u7684\u7ed1\u5b9a\u548c\u6346\u7ed1\u64cd\u4f5c\u76f8\u7ed3\u5408\uff0c\u4ee5\u5b9e\u73b0\u66f4\u4f18\u8d8a\u7684\u5f52\u7eb3\u56fe\u5b66\u4e60\u3002", "method": "hdgc\u7b97\u6cd5\uff0c\u5b83\u7ed3\u5408\u4e86\u56fe\u5377\u79ef\u3001\u7ed1\u5b9a\u548c\u6346\u7ed1\u64cd\u4f5c\u3002", "result": "hdgc\u5728\u540c\u8d28\u548c\u5f02\u8d28\u56fe\u4e0a\u90fd\u4f18\u4e8e\u4e3b\u8981\u7684GNN\u548c\u8d85\u7ef4\u8ba1\u7b97\u65b9\u6cd5\uff0c\u5e76\u4e14\u6bd4GCNII\u548cHDGL\u5feb\u5f97\u591a\u3002", "conclusion": "hdgc\u5728\u51c6\u786e\u6027\u548c\u901f\u5ea6\u4e0a\u90fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u4e14\u7531\u4e8e\u5176\u5927\u90e8\u5206\u64cd\u4f5c\u57fa\u4e8e\u4e8c\u8fdb\u5236\u5411\u91cf\uff0c\u56e0\u6b64\u5728\u795e\u7ecf\u5f62\u6001\u548c\u5185\u5b58\u5904\u7406\u8bbe\u5907\u4e0a\u5177\u6709\u51fa\u8272\u7684\u80fd\u6548\u8868\u73b0\u3002"}}
{"id": "2510.24013", "categories": ["cs.AI", "cs.LG", "cs.NE", "math.CO", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.24013", "abs": "https://arxiv.org/abs/2510.24013", "authors": ["\u0130brahim O\u011fuz \u00c7etinkaya", "\u0130. Esra B\u00fcy\u00fcktahtak\u0131n", "Parshin Shojaee", "Chandan K. Reddy"], "title": "Discovering Heuristics with Large Language Models (LLMs) for Mixed-Integer Programs: Single-Machine Scheduling", "comment": null, "summary": "Our study contributes to the scheduling and combinatorial optimization\nliterature with new heuristics discovered by leveraging the power of Large\nLanguage Models (LLMs). We focus on the single-machine total tardiness (SMTT)\nproblem, which aims to minimize total tardiness by sequencing n jobs on a\nsingle processor without preemption, given processing times and due dates. We\ndevelop and benchmark two novel LLM-discovered heuristics, the EDD Challenger\n(EDDC) and MDD Challenger (MDDC), inspired by the well-known Earliest Due Date\n(EDD) and Modified Due Date (MDD) rules. In contrast to prior studies that\nemployed simpler rule-based heuristics, we evaluate our LLM-discovered\nalgorithms using rigorous criteria, including optimality gaps and solution time\nderived from a mixed-integer programming (MIP) formulation of SMTT. We compare\ntheir performance against state-of-the-art heuristics and exact methods across\nvarious job sizes (20, 100, 200, and 500 jobs). For instances with more than\n100 jobs, exact methods such as MIP and dynamic programming become\ncomputationally intractable. Up to 500 jobs, EDDC improves upon the classic EDD\nrule and another widely used algorithm in the literature. MDDC consistently\noutperforms traditional heuristics and remains competitive with exact\napproaches, particularly on larger and more complex instances. This study shows\nthat human-LLM collaboration can produce scalable, high-performing heuristics\nfor NP-hard constrained combinatorial optimization, even under limited\nresources when effectively configured.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u53d1\u73b0\u65b0\u7684\u542f\u53d1\u5f0f\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u5355\u673a\u603b\u62d6\u671f\u65f6\u95f4\uff08SMTT\uff09\u95ee\u9898\uff0c\u5e76\u53d6\u5f97\u4e86\u4f18\u4e8e\u4f20\u7edf\u7b97\u6cd5\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86LLM\u5728\u7ec4\u5408\u4f18\u5316\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u4e3a\u8c03\u5ea6\u548c\u7ec4\u5408\u4f18\u5316\u9886\u57df\u8d21\u732e\u65b0\u7684\u542f\u53d1\u5f0f\u7b97\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u80fd\u529b\u6765\u89e3\u51b3\u5355\u673a\u603b\u62d6\u671f\u65f6\u95f4\uff08SMTT\uff09\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u5e76\u6d4b\u8bd5\u4e86\u4e24\u79cd\u7531LLM\u542f\u53d1\u7684\u65b0\u578b\u542f\u53d1\u5f0f\u7b97\u6cd5\uff1aEDD Challenger (EDDC) \u548c MDD Challenger (MDDC)\uff0c\u5e76\u4f7f\u7528\u6df7\u5408\u6574\u6570\u89c4\u5212\uff08MIP\uff09\u6a21\u578b\u6765\u8bc4\u4f30\u5176\u6700\u4f18\u6027\u5dee\u8ddd\u548c\u6c42\u89e3\u65f6\u95f4\uff0c\u5c06\u5176\u4e0e\u73b0\u6709\u7b97\u6cd5\u548c\u7cbe\u786e\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "EDDC \u7b97\u6cd5\u5728\u8d85\u8fc7100\u4e2a\u4f5c\u4e1a\u7684\u5b9e\u4f8b\u4e2d\u4f18\u4e8e\u7ecf\u5178\u7684EDD\u89c4\u5219\u548c\u5176\u4ed6\u5e38\u7528\u7b97\u6cd5\u3002MDDC \u7b97\u6cd5\u5728\u4f20\u7edf\u542f\u53d1\u5f0f\u7b97\u6cd5\u57fa\u7840\u4e0a\u8868\u73b0\u66f4\u4f73\uff0c\u5e76\u4e14\u5728\u5927\u578b\u590d\u6742\u5b9e\u4f8b\u4e2d\u4e0e\u7cbe\u786e\u65b9\u6cd5\u76f8\u5f53\uff0c\u8bc1\u660e\u4e86LLM\u5728\u7ec4\u5408\u4f18\u5316\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u8868\u660e\uff0c\u5728\u8d44\u6e90\u6709\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u6709\u6548\u914d\u7f6e\uff0c\u4eba\u7c7b\u4e0eLLM\u7684\u534f\u4f5c\u80fd\u591f\u4e3aNP-hard\u7ea6\u675f\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u4ea7\u751f\u53ef\u6269\u5c55\u3001\u9ad8\u6027\u80fd\u7684\u542f\u53d1\u5f0f\u7b97\u6cd5\u3002"}}
{"id": "2510.23638", "categories": ["cs.ET", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23638", "abs": "https://arxiv.org/abs/2510.23638", "authors": ["Songyuan Li", "Teng Wang", "Jinrong Tang", "Ruiqi Liu", "Yuyao Lu", "Feng Xu", "Bin Gao", "Xiangwei Zhu"], "title": "Bridging Function Approximation and Device Physics via Negative Differential Resistance Networks", "comment": null, "summary": "Achieving fully analog neural computation requires hardware that can natively\nimplement both linear and nonlinear operations with high efficiency. While\nanalogue matrix-vector multiplication has advanced via compute-in-memory\narchitectures, nonlinear activation functions remain a bottleneck, often\nrequiring digital or hybrid solutions. Inspired by the Kolmogorov-Arnold\nframework, we propose KANalogue, a fully analogue implementation of\nKolmogorov-Arnold Networks (KANs) using negative differential resistance\ndevices as physical realizations of learnable univariate basis functions. By\nleveraging the intrinsic negative differential resistance characteristics of\ntunnel diodes fabricated from NbSi2N4/HfSi2N4 heterostructures, we construct\ncoordinate-wise nonlinearities with distinct curvature and support profiles. We\nextract I-V data from fabricated armchair and zigzag devices, fit high-order\npolynomials to emulate diode behavior in software, and train KANs on vision\nbenchmarks using these learned basis functions. Our results demonstrate that\nKANalogue can approximate complex functions with minimal parameters while\nmaintaining classification accuracy competitive with digital baselines. This\nwork bridges device-level physics and function approximation theory, charting a\npath toward scalable, energy-efficient analogue machine learning systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aKANalogue\u7684\u5168\u6a21\u62df\u524d\u9988\u795e\u7ecf\u7f51\u7edc\uff0c\u5229\u7528\u8d1f\u5fae\u5206\u7535\u963b\u5668\u4ef6\u5b9e\u73b0\u53ef\u5b66\u4e60\u7684\u975e\u7ebf\u6027\u6fc0\u6d3b\u51fd\u6570\uff0c\u5e76\u5728\u89c6\u89c9\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u4e0e\u6570\u5b57\u57fa\u7ebf\u76f8\u5f53\u7684\u5206\u7c7b\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u7684\u5168\u6a21\u62df\u795e\u7ecf\u7f51\u7edc\u5728\u5b9e\u73b0\u975e\u7ebf\u6027\u6fc0\u6d3b\u51fd\u6570\u65b9\u9762\u5b58\u5728\u74f6\u9888\uff0c\u901a\u5e38\u9700\u8981\u6570\u5b57\u6216\u6df7\u5408\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u53d7Kolmogorov-Arnold\u6846\u67b6\u7684\u542f\u53d1\uff0c\u63d0\u51faKANalogue\uff0c\u4e00\u79cd\u5229\u7528\u8d1f\u5fae\u5206\u7535\u963b\u5668\u4ef6\uff08\u5982NbSi2N4/HfSi2N4\u5f02\u8d28\u7ed3\u6784\u96a7\u7a7f\u4e8c\u6781\u7ba1\uff09\u4f5c\u4e3a\u53ef\u5b66\u4e60\u7684\u5355\u53d8\u91cf\u57fa\u51fd\u6570\u7684\u5168\u6a21\u62dfKolmogorov-Arnold\u7f51\u7edc\u5b9e\u73b0\u3002\u901a\u8fc7\u63d0\u53d6\u4e8c\u6781\u7ba1\u7684I-V\u6570\u636e\u5e76\u8fdb\u884c\u9ad8\u9636\u591a\u9879\u5f0f\u62df\u5408\u6765\u6a21\u62df\u4e8c\u6781\u7ba1\u884c\u4e3a\uff0c\u7136\u540e\u4f7f\u7528\u8fd9\u4e9b\u5b66\u4e60\u5230\u7684\u57fa\u51fd\u6570\u5728\u8f6f\u4ef6\u4e2d\u8bad\u7ec3KAN\u3002", "result": "KANalogue\u80fd\u591f\u4ee5\u6700\u5c11\u7684\u53c2\u6570\u903c\u8fd1\u590d\u6742\u51fd\u6570\uff0c\u5e76\u4e14\u5728\u89c6\u89c9\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4fdd\u6301\u4e86\u4e0e\u6570\u5b57\u57fa\u7ebf\u76f8\u5f53\u7684\u5206\u7c7b\u51c6\u786e\u6027\u3002", "conclusion": "KANalogue\u5f25\u5408\u4e86\u5668\u4ef6\u7ea7\u7269\u7406\u4e0e\u51fd\u6570\u903c\u8fd1\u7406\u8bba\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u6784\u5efa\u53ef\u6269\u5c55\u3001\u9ad8\u80fd\u6548\u7684\u5168\u6a21\u62df\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2510.23645", "categories": ["cs.SI"], "pdf": "https://arxiv.org/pdf/2510.23645", "abs": "https://arxiv.org/abs/2510.23645", "authors": ["Alexandre Goncalves", "Yee Man Margaret Ng"], "title": "Global YouTube Trending Dataset (2022-2025): Three Years of Platform-Curated, Cross-National Trends in Digital Culture", "comment": null, "summary": "On July 1, 2025, YouTube retired its decade-long public \"Trending\" pages,\nending platform-curated, non-personalized video discovery. The Trending list\nhad long served as a vital lens into algorithmic influence, cultural diffusion,\nand crisis communication globally, offering a rare \"ground-truth\" reference to\nstudy global attention and cultural salience. We present a three-year archival\ndataset of YouTube Trending videos, collected from July 1, 2022, to June 30,\n2025, with four daily snapshots for each of the 104 countries. The dataset\nincludes 446,971 snapshots, each capturing up to 200 trending videos,\nencompassing 78.4 million video entries (726,627 unique videos) and associated\nmetadata. Each record includes core identifiers (snapshot time, country, rank)\nand content metadata (video ID, channel ID, title, description, tags,\npublication date, category, channel name, language, live status, views, and\ncomments). Unlike previous datasets with limited geographic scope or short\ntimeframes, our non-personalized data provides exceptional cross-national and\nlongitudinal coverage for studying digital culture, platform governance, and\ntemporal dynamics in content popularity. We document the data collection\nmethodology, schema design, coverage, descriptive statistics for both global\nand U.S. trending videos, and the ethical safeguards implemented throughout.", "AI": {"tldr": "YouTube\u4e8e2025\u5e747\u67081\u65e5\u4e0b\u7ebf\u4e86\u5176\u6301\u7eed\u4e86\u5341\u5e74\u7684\u516c\u5f00\u201c\u70ed\u95e8\u201d\u9875\u9762\uff0c\u7ed3\u675f\u4e86\u5e73\u53f0\u7b56\u5c55\u7684\u3001\u975e\u4e2a\u6027\u5316\u7684\u89c6\u9891\u53d1\u73b0\u65b9\u5f0f\u3002\u7814\u7a76\u8005\u6536\u96c6\u4e862022\u5e747\u67081\u65e5\u81f32025\u5e746\u670830\u65e5\u4e3a\u671f\u4e09\u5e74\u7684YouTube\u70ed\u95e8\u89c6\u9891\u5b58\u6863\u6570\u636e\u96c6\uff0c\u5305\u542b104\u4e2a\u56fd\u5bb6\u7684\u6bcf\u65e5\u5feb\u7167\uff0c\u5171\u8ba1446,971\u4e2a\u5feb\u7167\uff0c\u6db5\u76d67840\u4e07\u4e2a\u89c6\u9891\u6761\u76ee\uff08726,627\u4e2a\u552f\u4e00\u89c6\u9891\uff09\u53ca\u5176\u5143\u6570\u636e\uff0c\u7528\u4e8e\u7814\u7a76\u6570\u5b57\u6587\u5316\u3001\u5e73\u53f0\u6cbb\u7406\u548c\u5185\u5bb9\u6d41\u884c\u5ea6\u7684\u65f6\u95f4\u52a8\u6001\u3002", "motivation": "YouTube\u7684\u70ed\u95e8\u9875\u9762\u662f\u7814\u7a76\u7b97\u6cd5\u5f71\u54cd\u3001\u6587\u5316\u4f20\u64ad\u548c\u5371\u673a\u6c9f\u901a\u7684\u91cd\u8981\u7a97\u53e3\uff0c\u4f46\u5df2\u4e8e2025\u5e747\u67081\u65e5\u505c\u6b62\u670d\u52a1\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u4e2a\u5b58\u6863\u6570\u636e\u96c6\u6765\u7ee7\u7eed\u8fdb\u884c\u76f8\u5173\u7814\u7a76\u3002", "method": "\u6536\u96c6\u4e862022\u5e747\u67081\u65e5\u81f32025\u5e746\u670830\u65e5\u671f\u95f4\uff0c104\u4e2a\u56fd\u5bb6/\u5730\u533a\uff0c\u6bcf\u65e5\u56db\u4e2a\u5feb\u7167\u7684YouTube\u70ed\u95e8\u89c6\u9891\u6570\u636e\uff0c\u5305\u62ec\u89c6\u9891ID\u3001\u9891\u9053ID\u3001\u6807\u9898\u3001\u63cf\u8ff0\u3001\u6807\u7b7e\u3001\u53d1\u5e03\u65e5\u671f\u3001\u7c7b\u522b\u3001\u9891\u9053\u540d\u79f0\u3001\u8bed\u8a00\u3001\u76f4\u64ad\u72b6\u6001\u3001\u89c2\u770b\u6b21\u6570\u548c\u8bc4\u8bba\u6570\u7b49\u5143\u6570\u636e\u3002", "result": "\u751f\u6210\u4e86\u4e00\u4e2a\u5305\u542b7840\u4e07\u4e2a\u89c6\u9891\u6761\u76ee\u7684\u5b58\u6863\u6570\u636e\u96c6\uff0c\u63d0\u4f9b\u4e86\u524d\u6240\u672a\u6709\u7684\u8de8\u56fd\u754c\u548c\u957f\u671f\u8986\u76d6\u8303\u56f4\uff0c\u53ef\u7528\u4e8e\u7814\u7a76\u6570\u5b57\u6587\u5316\u3001\u5e73\u53f0\u6cbb\u7406\u548c\u5185\u5bb9\u6d41\u884c\u5ea6\u7684\u65f6\u95f4\u52a8\u6001\u3002", "conclusion": "\u6240\u521b\u5efa\u7684\u6570\u636e\u96c6\u4e3a\u7814\u7a76\u6570\u5b57\u6587\u5316\u3001\u5e73\u53f0\u6cbb\u7406\u548c\u5185\u5bb9\u6d41\u884c\u5ea6\u7684\u65f6\u95f4\u52a8\u6001\u63d0\u4f9b\u4e86\u5b9d\u8d35\u8d44\u6e90\uff0c\u586b\u8865\u4e86YouTube\u70ed\u95e8\u9875\u9762\u4e0b\u7ebf\u540e\u7684\u7814\u7a76\u7a7a\u767d\u3002"}}
{"id": "2510.23615", "categories": ["cs.MA", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.23615", "abs": "https://arxiv.org/abs/2510.23615", "authors": ["Nishant Doshi"], "title": "Logic-based Task Representation and Reward Shaping in Multiagent Reinforcement Learning", "comment": null, "summary": "This paper presents an approach for accelerated learning of optimal plans for\na given task represented using Linear Temporal Logic (LTL) in multi-agent\nsystems. Given a set of options (temporally abstract actions) available to each\nagent, we convert the task specification into the corresponding Buchi Automaton\nand proceed with a model-free approach which collects transition samples and\nconstructs a product Semi Markov Decision Process (SMDP) on-the-fly.\nValue-based Reinforcement Learning algorithms can then be used to synthesize a\ncorrect-by-design controller without learning the underlying transition model\nof the multi-agent system. The exponential sample complexity due to multiple\nagents is dealt with using a novel reward shaping approach. We test the\nproposed algorithm in a deterministic gridworld simulation for different tasks\nand find that the reward shaping results in significant reduction in\nconvergence times. We also infer that using options becomes increasing more\nrelevant as the state and action space increases in multi-agent systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u52a0\u901f\u5b66\u4e60\u7ebf\u6027\u65f6\u5e8f\u903b\u8f91\uff08LTL\uff09\u4efb\u52a1\u6700\u4f18\u89c4\u5212\u7684\u65b9\u6cd5\u3002", "motivation": "\u5728\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\uff0c\u9488\u5bf9\u7ed9\u5b9a\u7684LTL\u4efb\u52a1\uff0c\u5b66\u4e60\u6700\u4f18\u89c4\u5212\u662f\u4e00\u4e2a\u6311\u6218\uff0c\u9700\u8981\u5904\u7406\u72b6\u6001\u548c\u52a8\u4f5c\u7a7a\u95f4\u7684\u589e\u957f\u4ee5\u53ca\u591a\u667a\u80fd\u4f53\u7684\u590d\u6742\u6027\u3002", "method": "\u5c06LTL\u4efb\u52a1\u8f6c\u5316\u4e3a\u76f8\u5e94\u7684B\u00fcchi\u81ea\u52a8\u673a\uff0c\u5e76\u91c7\u7528\u65e0\u6a21\u578b\u65b9\u6cd5\uff0c\u5b9e\u65f6\u6536\u96c6\u8f6c\u79fb\u6837\u672c\u5e76\u6784\u5efa\u79ef\u534a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08SMDP\uff09\u3002\u5229\u7528\u57fa\u4e8e\u4ef7\u503c\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u5408\u6210\u6ee1\u8db3\u8bbe\u8ba1\u8981\u6c42\u7684\u63a7\u5236\u5668\uff0c\u65e0\u9700\u5b66\u4e60\u5e95\u5c42\u8f6c\u79fb\u6a21\u578b\u3002\u91c7\u7528\u65b0\u9896\u7684\u5956\u52b1\u5851\u9020\u65b9\u6cd5\u5904\u7406\u591a\u667a\u80fd\u4f53\u5e26\u6765\u7684\u6307\u6570\u7ea7\u6837\u672c\u590d\u6742\u5ea6\u3002", "result": "\u5728\u786e\u5b9a\u6027\u7f51\u683c\u4e16\u754c\u6a21\u62df\u4e2d\uff0c\u9488\u5bf9\u4e0d\u540c\u4efb\u52a1\u6d4b\u8bd5\u4e86\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5956\u52b1\u5851\u9020\u663e\u8457\u964d\u4f4e\u4e86\u6536\u655b\u65f6\u95f4\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684LTL\u89c4\u5212\u95ee\u9898\uff0c\u5e76\u4e14\u5956\u52b1\u5851\u9020\u5728\u589e\u52a0\u72b6\u6001\u548c\u52a8\u4f5c\u7a7a\u95f4\u65f6\u5c24\u4e3a\u91cd\u8981\u3002"}}
{"id": "2510.23913", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2510.23913", "abs": "https://arxiv.org/abs/2510.23913", "authors": ["Daniel Agassy", "Dani Dorfman", "Haim Kaplan"], "title": "Expander Decomposition for Non-Uniform Vertex Measures", "comment": null, "summary": "A $(\\phi,\\epsilon)$-expander-decomposition of a graph $G$ (with $n$ vertices\nand $m$ edges) is a partition of $V$ into clusters $V_1,\\ldots,V_k$ with\nconductance $\\Phi(G[V_i]) \\ge \\phi$, such that there are at most $\\epsilon m$\ninter-cluster edges. Such a decomposition plays a crucial role in many graph\nalgorithms. [ADK23] gave a randomized $\\tilde{O}(m)$ time algorithm for\ncomputing a $(\\phi, \\phi\\log^2 {n})$-expander decomposition.\n  In this paper we generalize this result for a broader notion of expansion.\nLet $\\mu \\in {\\mathbb{R}}_{\\ge 0 }^n$ be a vertex measure. A standard\ngeneralization of conductance of a cut $(S,\\bar{S})$ is its $\\mu$-expansion\n$\\Phi^{\\mu}_G(S,\\bar{S}) = |E(S,\\bar{S})|/\\min \\mu(S)),\\mu(\\bar{S})\\}$, where\n$\\mu(S) = \\sum_{v\\in S} \\mu(v)$. We present a randomized $\\tilde{O}(m)$ time\nalgorithm for computing a $(\\phi, \\phi \\log^2\n{n}\\left(\\frac{\\mu(V)}{m}\\right))$-expander decomposition with respect to\n$\\mu$-expansion.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u56fe\u5206\u89e3\u7b97\u6cd5\uff0c\u53ef\u4ee5\u5728\u8fd1\u4f3c\u7ebf\u6027\u7684\u65f6\u95f4\u5185\u8ba1\u7b97\u51fa\u5177\u6709\u6307\u5b9a\u6269\u5f20\uff08expansion\uff09\u6027\u8d28\u7684\u56fe\u7c07\u5212\u5206\u3002", "motivation": "\u73b0\u6709\u7684\u56fe\u5206\u89e3\u7b97\u6cd5\u5728\u5904\u7406\u66f4\u5e7f\u6cdb\u7684\u6269\u5f20\u5b9a\u4e49\u65f6\u6548\u7387\u4e0d\u9ad8\uff0c\u672c\u6587\u65e8\u5728\u63a8\u5e7f\u73b0\u6709\u7684 $(\\phi,\\epsilon)$-expander-decomposition \u7b97\u6cd5\uff0c\u4ee5\u9002\u5e94\u66f4\u4e00\u822c\u7684 $\\mu$-expansion \u5b9a\u4e49\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u968f\u673a\u5316\u7b97\u6cd5\uff0c\u53ef\u4ee5\u5728\u8fd1\u4f3c\u7ebf\u6027\u7684\u65f6\u95f4\u5185\uff08$\\tilde{O}(m)$\uff09\u8ba1\u7b97\u51fa\u6ee1\u8db3 $(\\phi, \\phi \\log^2 {n}(\\frac{\\mu(V)}{m}))$-expansion \u6761\u4ef6\u7684\u56fe\u7c07\u5212\u5206\u3002", "result": "\u6210\u529f\u5b9e\u73b0\u4e86\u4e00\u79cd\u66f4\u901a\u7528\u7684\u56fe\u5206\u89e3\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u80fd\u591f\u5904\u7406\u57fa\u4e8e $\\mu$-expansion \u7684\u6269\u5f20\u5b9a\u4e49\uff0c\u5e76\u5728\u8fd1\u4f3c\u7ebf\u6027\u7684\u65f6\u95f4\u5185\u8fd0\u884c\u3002", "conclusion": "\u672c\u6587\u6210\u529f\u5730\u5c06 $(\\phi,\\epsilon)$-expander-decomposition \u7b97\u6cd5\u63a8\u5e7f\u5230\u4e86\u57fa\u4e8e $\\mu$-expansion \u7684\u60c5\u51b5\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u968f\u673a\u5316\u7b97\u6cd5\u6765\u5b9e\u73b0\u5b83\u3002"}}
{"id": "2510.23986", "categories": ["cs.LG", "cs.AI", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2510.23986", "abs": "https://arxiv.org/abs/2510.23986", "authors": ["Hong Wang", "Jiang Yixuan", "Jie Wang", "Xinyi Li", "Jian Luo", "Huanshuo Dong"], "title": "STNet: Spectral Transformation Network for Solving Operator Eigenvalue Problem", "comment": null, "summary": "Operator eigenvalue problems play a critical role in various scientific\nfields and engineering applications, yet numerical methods are hindered by the\ncurse of dimensionality. Recent deep learning methods provide an efficient\napproach to address this challenge by iteratively updating neural networks.\nThese methods' performance relies heavily on the spectral distribution of the\ngiven operator: larger gaps between the operator's eigenvalues will improve\nprecision, thus tailored spectral transformations that leverage the spectral\ndistribution can enhance their performance. Based on this observation, we\npropose the Spectral Transformation Network (STNet). During each iteration,\nSTNet uses approximate eigenvalues and eigenfunctions to perform spectral\ntransformations on the original operator, turning it into an equivalent but\neasier problem. Specifically, we employ deflation projection to exclude the\nsubspace corresponding to already solved eigenfunctions, thereby reducing the\nsearch space and avoiding converging to existing eigenfunctions. Additionally,\nour filter transform magnifies eigenvalues in the desired region and suppresses\nthose outside, further improving performance. Extensive experiments demonstrate\nthat STNet consistently outperforms existing learning-based methods, achieving\nstate-of-the-art performance in accuracy.", "AI": {"tldr": "STNet\u901a\u8fc7\u5728\u6bcf\u6b21\u8fed\u4ee3\u4e2d\u4f7f\u7528\u8fd1\u4f3c\u7279\u5f81\u503c\u548c\u7279\u5f81\u51fd\u6570\u6267\u884c\u7b97\u5b50\u4e0a\u7684\u8c31\u53d8\u6362\uff0c\u5c06\u539f\u95ee\u9898\u8f6c\u5316\u4e3a\u7b49\u4ef7\u4f46\u66f4\u6613\u4e8e\u89e3\u51b3\u7684\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u5c55\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u5b66\u4e60\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u89e3\u51b3\u7b97\u5b50\u7279\u5f81\u503c\u95ee\u9898\u65f6\uff0c\u5176\u6027\u80fd\u4e25\u91cd\u4f9d\u8d56\u4e8e\u7b97\u5b50\u7684\u8c31\u5206\u5e03\uff0c\u8f83\u5927\u7684\u7279\u5f81\u503c\u95f4\u9699\u53ef\u4ee5\u63d0\u9ad8\u7cbe\u5ea6\u3002\u56e0\u6b64\uff0c\u5229\u7528\u8c31\u5206\u5e03\u7684\u8c31\u53d8\u6362\u53ef\u4ee5\u63d0\u9ad8\u6027\u80fd\u3002", "method": "\u63d0\u51faSTNet\uff0c\u5728\u6bcf\u6b21\u8fed\u4ee3\u4e2d\u4f7f\u7528\u8fd1\u4f3c\u7279\u5f81\u503c\u548c\u7279\u5f81\u51fd\u6570\u5bf9\u539f\u7b97\u5b50\u8fdb\u884c\u8c31\u53d8\u6362\uff0c\u5c06\u5176\u8f6c\u5316\u4e3a\u7b49\u4ef7\u4f46\u66f4\u6613\u4e8e\u89e3\u51b3\u7684\u95ee\u9898\u3002\u5177\u4f53\u5730\uff0c\u91c7\u7528 deflation projection \u6392\u9664\u5df2\u6c42\u89e3\u7279\u5f81\u51fd\u6570\u5bf9\u5e94\u7684\u5b50\u7a7a\u95f4\uff0c\u4ee5\u51cf\u5c0f\u641c\u7d22\u7a7a\u95f4\u5e76\u907f\u514d\u6536\u655b\u5230\u73b0\u6709\u7279\u5f81\u51fd\u6570\u3002\u6b64\u5916\uff0c\u901a\u8fc7 filter transform \u653e\u5927\u671f\u671b\u533a\u57df\u7684\u7279\u5f81\u503c\u5e76\u6291\u5236\u5176\u4ed6\u533a\u57df\u7684\u7279\u5f81\u503c\uff0c\u4ee5\u8fdb\u4e00\u6b65\u63d0\u9ad8\u6027\u80fd\u3002", "result": "STNet \u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u5728\u51c6\u786e\u6027\u65b9\u9762\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "STNet \u662f\u4e00\u79cd\u6709\u6548\u7684\u7b97\u5b50\u7279\u5f81\u503c\u95ee\u9898\u89e3\u51b3\u65b9\u6cd5\uff0c\u901a\u8fc7\u8c31\u53d8\u6362\u63d0\u9ad8\u4e86\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u7684\u6027\u80fd\u3002"}}
{"id": "2510.23730", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23730", "abs": "https://arxiv.org/abs/2510.23730", "authors": ["Alessandra Terranova", "Bj\u00f6rn Ross", "Alexandra Birch"], "title": "Evaluating Long-Term Memory for Long-Context Question Answering", "comment": "14 pages including appendix, 3 figures. Submitted to October ARR and\n  to Metacognition in Generative AI EurIPS workshop (under review for both)", "summary": "In order for large language models to achieve true conversational continuity\nand benefit from experiential learning, they need memory. While research has\nfocused on the development of complex memory systems, it remains unclear which\ntypes of memory are most effective for long-context conversational tasks. We\npresent a systematic evaluation of memory-augmented methods using LoCoMo, a\nbenchmark of synthetic long-context dialogues annotated for question-answering\ntasks that require diverse reasoning strategies. We analyse full-context\nprompting, semantic memory through retrieval-augmented generation and agentic\nmemory, episodic memory through in-context learning, and procedural memory\nthrough prompt optimization. Our findings show that memory-augmented approaches\nreduce token usage by over 90% while maintaining competitive accuracy. Memory\narchitecture complexity should scale with model capability, with small\nfoundation models benefitting most from RAG, and strong instruction-tuned\nreasoning model gaining from episodic learning through reflections and more\ncomplex agentic semantic memory. In particular, episodic memory can help LLMs\nrecognise the limits of their own knowledge.", "AI": {"tldr": "\u5185\u5b58\u589e\u5f3a\u65b9\u6cd5\u53ef\u663e\u8457\u51cf\u5c11\u957f\u5bf9\u8bdd\u4e2d\u7684 token \u7528\u91cf\u5e76\u4fdd\u6301\u51c6\u786e\u6027\uff0c\u4e14\u5185\u5b58\u67b6\u6784\u7684\u590d\u6742\u6027\u5e94\u4e0e\u6a21\u578b\u80fd\u529b\u76f8\u5339\u914d\u3002", "motivation": "\u4e3a\u4e86\u5b9e\u73b0\u771f\u6b63\u7684\u4f1a\u8bdd\u8fde\u7eed\u6027\u548c\u5229\u7528\u7ecf\u9a8c\u5b66\u4e60\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9700\u8981\u8bb0\u5fc6\uff0c\u4f46\u5bf9\u4e8e\u957f\u4e0a\u4e0b\u6587\u4f1a\u8bdd\u4efb\u52a1\u6700\u6709\u6548\u7684\u8bb0\u5fc6\u7c7b\u578b\u5c1a\u4e0d\u6e05\u695a\u3002", "method": "\u901a\u8fc7 LoCoMo \u8fd9\u4e00\u5305\u542b\u9700\u8981\u591a\u79cd\u63a8\u7406\u7b56\u7565\u7684\u95ee\u7b54\u4efb\u52a1\u7684\u957f\u4e0a\u4e0b\u6587\u5bf9\u8bdd\u57fa\u51c6\uff0c\u7cfb\u7edf\u5730\u8bc4\u4f30\u4e86\u5185\u5b58\u589e\u5f3a\u65b9\u6cd5\uff0c\u5305\u62ec\u5168\u4e0a\u4e0b\u6587\u63d0\u793a\u3001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7684\u8bed\u4e49\u8bb0\u5fc6\u3001\u4ee3\u7406\u8bb0\u5fc6\u3001\u4e0a\u4e0b\u6587\u5b66\u4e60\u7684\u8868\u5f81\u8bb0\u5fc6\u548c\u63d0\u793a\u4f18\u5316\u7684\u7a0b\u5e8f\u8bb0\u5fc6\u3002", "result": "\u5185\u5b58\u589e\u5f3a\u65b9\u6cd5\u53ef\u5c06 token \u7528\u91cf\u51cf\u5c11 90% \u4ee5\u4e0a\uff0c\u540c\u65f6\u4fdd\u6301\u5177\u6709\u7ade\u4e89\u529b\u7684\u51c6\u786e\u6027\u3002\u5185\u5b58\u67b6\u6784\u7684\u590d\u6742\u6027\u5e94\u4e0e\u6a21\u578b\u80fd\u529b\u76f8\u5339\u914d\uff1a\u5c0f\u578b\u57fa\u7840\u6a21\u578b\u4ece RAG \u4e2d\u83b7\u76ca\u6700\u591a\uff0c\u800c\u5f3a\u5927\u7684\u6307\u4ee4\u8c03\u6574\u63a8\u7406\u6a21\u578b\u5219\u4ece\u901a\u8fc7\u53cd\u5c04\u8fdb\u884c\u7684\u8868\u5f81\u5b66\u4e60\u548c\u66f4\u590d\u6742\u7684\u4ee3\u7406\u8bed\u4e49\u8bb0\u5fc6\u4e2d\u83b7\u76ca\u3002\u7279\u522b\u662f\uff0c\u8868\u5f81\u8bb0\u5fc6\u53ef\u4ee5\u5e2e\u52a9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8ba4\u8bc6\u5230\u81ea\u8eab\u77e5\u8bc6\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u5185\u5b58\u589e\u5f3a\u65b9\u6cd5\u5728\u957f\u5bf9\u8bdd\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u5176\u6709\u6548\u6027\u53d6\u51b3\u4e8e\u6a21\u578b\u7684\u590d\u6742\u6027\u548c\u4efb\u52a1\u9700\u6c42\u3002"}}
{"id": "2510.23763", "categories": ["cs.RO", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23763", "abs": "https://arxiv.org/abs/2510.23763", "authors": ["Siyin Wang", "Jinlan Fu", "Feihong Liu", "Xinzhe He", "Huangxuan Wu", "Junhao Shi", "Kexin Huang", "Zhaoye Fei", "Jingjing Gong", "Zuxuan Wu", "Yugang Jiang", "See-Kiong Ng", "Tat-Seng Chua", "Xipeng Qiu"], "title": "RoboOmni: Proactive Robot Manipulation in Omni-modal Context", "comment": null, "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have driven rapid\nprogress in Vision-Language-Action (VLA) models for robotic manipulation.\nAlthough effective in many scenarios, current approaches largely rely on\nexplicit instructions, whereas in real-world interactions, humans rarely issue\ninstructions directly. Effective collaboration requires robots to infer user\nintentions proactively. In this work, we introduce cross-modal contextual\ninstructions, a new setting where intent is derived from spoken dialogue,\nenvironmental sounds, and visual cues rather than explicit commands. To address\nthis new setting, we present RoboOmni, a Perceiver-Thinker-Talker-Executor\nframework based on end-to-end omni-modal LLMs that unifies intention\nrecognition, interaction confirmation, and action execution. RoboOmni fuses\nauditory and visual signals spatiotemporally for robust intention recognition,\nwhile supporting direct speech interaction. To address the absence of training\ndata for proactive intention recognition in robotic manipulation, we build\nOmniAction, comprising 140k episodes, 5k+ speakers, 2.4k event sounds, 640\nbackgrounds, and six contextual instruction types. Experiments in simulation\nand real-world settings show that RoboOmni surpasses text- and ASR-based\nbaselines in success rate, inference speed, intention recognition, and\nproactive assistance.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8de8\u6a21\u6001\u60c5\u5883\u6307\u4ee4\u8bbe\u7f6e\uff0c\u5e76\u5f00\u53d1\u4e86RoboOmni\u6846\u67b6\uff0c\u4ee5\u5b9e\u73b0\u673a\u5668\u4eba\u4e3b\u52a8\u63a8\u7406\u7528\u6237\u610f\u56fe\u5e76\u6267\u884c\u4efb\u52a1\u3002", "motivation": "\u5f53\u524d\u673a\u5668\u4eba\u64cd\u4f5c\u6a21\u578b\u4e3b\u8981\u4f9d\u8d56\u663e\u5f0f\u6307\u4ee4\uff0c\u800c\u73b0\u5b9e\u4e16\u754c\u4e2d\u4eba\u7c7b\u534f\u4f5c\u9700\u8981\u673a\u5668\u4eba\u4e3b\u52a8\u63a8\u65ad\u7528\u6237\u610f\u56fe\u3002", "method": "\u63d0\u51fa\u8de8\u6a21\u6001\u60c5\u5883\u6307\u4ee4\u8bbe\u7f6e\uff0c\u5e76\u5f00\u53d1\u4e86\u57fa\u4e8e\u7aef\u5230\u7aef\u5168\u6a21\u6001LLM\u7684RoboOmni\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5305\u62ec\u611f\u77e5-\u601d\u8003-\u4ea4\u8c08-\u6267\u884c\u6a21\u5757\uff0c\u80fd\u591f\u878d\u5408\u65f6\u7a7a\u97f3\u89c6\u9891\u4fe1\u53f7\u8fdb\u884c\u610f\u56fe\u8bc6\u522b\uff0c\u5e76\u652f\u6301\u8bed\u97f3\u4ea4\u4e92\u3002\u540c\u65f6\uff0c\u6784\u5efa\u4e86\u5305\u542b140k\u4e2a\u8bad\u7ec3\u6837\u672c\u7684OmniAction\u6570\u636e\u96c6\u4ee5\u89e3\u51b3\u8bad\u7ec3\u6570\u636e\u7f3a\u4e4f\u7684\u95ee\u9898\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\uff0cRoboOmni\u5728\u6210\u529f\u7387\u3001\u63a8\u7406\u901f\u5ea6\u3001\u610f\u56fe\u8bc6\u522b\u548c\u4e3b\u52a8\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u57fa\u4e8e\u6587\u672c\u548c\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u7684\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "RoboOmni\u80fd\u591f\u6709\u6548\u5904\u7406\u8de8\u6a21\u6001\u60c5\u5883\u6307\u4ee4\uff0c\u5b9e\u73b0\u673a\u5668\u4eba\u4e3b\u52a8\u63a8\u7406\u7528\u6237\u610f\u56fe\u5e76\u5b8c\u6210\u64cd\u4f5c\u4efb\u52a1\uff0c\u4e3a\u673a\u5668\u4eba\u4e0e\u4eba\u7c7b\u7684\u534f\u4f5c\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.23911", "categories": ["cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2510.23911", "abs": "https://arxiv.org/abs/2510.23911", "authors": ["Arno Uhlig", "Iris Braun", "Matthias W\u00e4hlisch"], "title": "The SAP Cloud Infrastructure Dataset: A Reality Check of Scheduling and Placement of VMs in Cloud Computing", "comment": "15 pages", "summary": "Allocating resources in a distributed environment is a fundamental challenge.\nIn this paper, we analyze the scheduling and placement of virtual machines\n(VMs) in the cloud platform of SAP, the world's largest enterprise resource\nplanning software vendor. Based on data from roughly 1,800 hypervisors and\n48,000 VMs within a 30-day observation period, we highlight potential\nimprovements for workload management. The data was measured through\nobservability tooling that tracks resource usage and performance metrics across\nthe entire infrastructure. In contrast to existing datasets, ours uniquely\noffers fine-grained time-series telemetry data of fully virtualized\nenterprise-level workloads from both long-running and memory-intensive SAP\nS/4HANA and diverse, general-purpose applications. Our key findings include\nseveral suboptimal scheduling situations, such as CPU resource contention\nexceeding 40%, CPU ready times of up to 220 seconds, significantly imbalanced\ncompute hosts with a maximum CPU~utilization on intra-building block hosts of\nup to 99%, and overprovisioned CPU and memory resources resulting into over 80%\nof VMs using less than 70% of the provided resources. Bolstered by these\nfindings, we derive requirements for the design and implementation of novel\nplacement and scheduling algorithms and provide guidance to optimize resource\nallocations. We make the full dataset used in this study publicly available to\nenable data-driven evaluations of scheduling approaches for large-scale cloud\ninfrastructures in future research.", "AI": {"tldr": "SAP\u4e91\u5e73\u53f0\u865a\u62df\u673a\u8c03\u5ea6\u4e0e\u653e\u7f6e\u7684\u8d44\u6e90\u5206\u914d\u5206\u6790\uff0c\u53d1\u73b0CPU\u8d44\u6e90\u4e89\u7528\u3001CPU\u5c31\u7eea\u65f6\u95f4\u8fc7\u957f\u3001\u8ba1\u7b97\u4e3b\u673a\u8d1f\u8f7d\u4e0d\u5747\u4ee5\u53ca\u8d44\u6e90\u8fc7\u5ea6\u914d\u7f6e\u7b49\u95ee\u9898\uff0c\u5e76\u516c\u5f00\u6570\u636e\u96c6\u4ee5\u4fc3\u8fdb\u672a\u6765\u7814\u7a76\u3002", "motivation": "\u5728\u5206\u5e03\u5f0f\u73af\u5883\u4e2d\uff0c\u8d44\u6e90\u5206\u914d\u662f\u4e00\u4e2a\u57fa\u672c\u6311\u6218\u3002\u672c\u6587\u65e8\u5728\u5206\u6790SAP\u4e91\u5e73\u53f0\u4e2d\u865a\u62df\u673a\u7684\u8c03\u5ea6\u548c\u653e\u7f6e\u95ee\u9898\uff0c\u4ee5\u53d1\u73b0\u6f5c\u5728\u7684\u6539\u8fdb\u673a\u4f1a\u3002", "method": "\u901a\u8fc7\u5206\u6790SAP\u4e91\u5e73\u53f0\u4e2d\u7ea61,800\u4e2a\u865a\u62df\u673a\u76d1\u63a7\u7a0b\u5e8f\u548c48,000\u4e2a\u865a\u62df\u673a\u572830\u5929\u5185\u7684\u8fd0\u884c\u6570\u636e\uff0c\u5229\u7528\u53ef\u89c2\u6d4b\u6027\u5de5\u5177\u8ddf\u8e2a\u8d44\u6e90\u4f7f\u7528\u60c5\u51b5\u548c\u6027\u80fd\u6307\u6807\uff0c\u91cd\u70b9\u5206\u6790CPU\u8d44\u6e90\u4e89\u7528\u3001CPU\u5c31\u7eea\u65f6\u95f4\u3001\u8ba1\u7b97\u4e3b\u673a\u8d1f\u8f7d\u5747\u8861\u4ee5\u53ca\u8d44\u6e90\u8fc7\u5ea6\u914d\u7f6e\u7b49\u95ee\u9898\u3002", "result": "\u5206\u6790\u663e\u793a\uff0c\u5b58\u5728CPU\u8d44\u6e90\u4e89\u7528\uff08\u8d85\u8fc740%\uff09\u3001CPU\u5c31\u7eea\u65f6\u95f4\u957f\uff08\u9ad8\u8fbe220\u79d2\uff09\u3001\u8ba1\u7b97\u4e3b\u673a\u8d1f\u8f7d\u4e25\u91cd\u4e0d\u5747\uff08\u6700\u9ad8\u8fbe99%\u5229\u7528\u7387\uff09\u4ee5\u53ca\u5927\u91cfVM\uff08\u8d85\u8fc780%\uff09\u8d44\u6e90\u4f7f\u7528\u7387\u4f4e\uff08\u4f4e\u4e8e70%\uff09\u7b49\u591a\u79cd\u6b21\u4f18\u8c03\u5ea6\u60c5\u51b5\u3002", "conclusion": "\u57fa\u4e8e\u5206\u6790\u7ed3\u679c\uff0c\u8bba\u6587\u63d0\u51fa\u4e86\u5bf9\u65b0\u578b\u653e\u7f6e\u548c\u8c03\u5ea6\u7b97\u6cd5\u8bbe\u8ba1\u4e0e\u5b9e\u65bd\u7684\u8981\u6c42\uff0c\u5e76\u4e3a\u4f18\u5316\u8d44\u6e90\u5206\u914d\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002\u540c\u65f6\uff0c\u7814\u7a76\u5c06\u5168\u90e8\u6570\u636e\u96c6\u516c\u5f00\uff0c\u4ee5\u652f\u6301\u672a\u6765\u5bf9\u5927\u89c4\u6a21\u4e91\u57fa\u7840\u8bbe\u65bd\u8c03\u5ea6\u65b9\u6cd5\u8fdb\u884c\u6570\u636e\u9a71\u52a8\u7684\u8bc4\u4f30\u3002"}}
{"id": "2510.02685", "categories": ["quant-ph", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2510.02685", "abs": "https://arxiv.org/abs/2510.02685", "authors": ["Chi-Chuan Hwang"], "title": "Construction of the Complete Set of Maximally Entangled Basis Vectors for N-Qubit Systems", "comment": null, "summary": "In this study, we first use a three-qubit system as an example to demonstrate\nthe construction of quantum circuits for the eight maximally entangled basis\nvectors, subsequently extending the approach to N-qubit systems. We employ a\nrandom-number approach to generate maximally entangled basis vectors and their\ncorresponding circuits, while also detailing the required number of\nsingle-qubit and CNOT gates. This approach not only provides a solid\ntheoretical foundation but also establishes a practical technique for\ntechnological applications, bypassing the difficulty of storing large-scale\nencoding data.", "AI": {"tldr": "\u672c\u7814\u7a76\u5c55\u793a\u4e86\u5982\u4f55\u6784\u9020N\u91cf\u5b50\u6bd4\u7279\u7cfb\u7edf\u7684\u6700\u5927\u7ea0\u7f20\u57fa\u77e2\u91cf\u5b50\u7ebf\u8def\uff0c\u5e76\u4f7f\u7528\u968f\u673a\u6570\u65b9\u6cd5\u751f\u6210\u8fd9\u4e9b\u57fa\u77e2\u53ca\u5176\u7ebf\u8def\uff0c\u540c\u65f6\u7ed9\u51fa\u4e86\u6240\u9700\u7684\u5355\u6bd4\u7279\u548cCNOT\u95e8\u6570\u91cf\u3002", "motivation": "\u672c\u7814\u7a76\u7684\u52a8\u673a\u662f\u4e3aN\u91cf\u5b50\u6bd4\u7279\u7cfb\u7edf\u7684\u6700\u5927\u7ea0\u7f20\u57fa\u77e2\u63d0\u4f9b\u4e00\u79cd\u6784\u9020\u91cf\u5b50\u7ebf\u8def\u7684\u5b9e\u7528\u65b9\u6cd5\uff0c\u4ee5\u514b\u670d\u5b58\u50a8\u5927\u89c4\u6a21\u7f16\u7801\u6570\u636e\u7684\u56f0\u96be\uff0c\u5e76\u4e3a\u6280\u672f\u5e94\u7528\u5960\u5b9a\u7406\u8bba\u548c\u5b9e\u8df5\u57fa\u7840\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528\u968f\u673a\u6570\u65b9\u6cd5\u6765\u751f\u6210\u6700\u5927\u7ea0\u7f20\u57fa\u77e2\u53ca\u5176\u76f8\u5e94\u7684\u91cf\u5b50\u7ebf\u8def\uff0c\u5e76\u63a8\u5bfc\u4e86\u6240\u9700\u7684\u5355\u6bd4\u7279\u548cCNOT\u95e8\u6570\u91cf\u3002\u7814\u7a76\u9996\u5148\u4ee5\u4e09\u91cf\u5b50\u6bd4\u7279\u7cfb\u7edf\u4e3a\u4f8b\u8fdb\u884c\u6f14\u793a\uff0c\u7136\u540e\u63a8\u5e7f\u5230N\u91cf\u5b50\u6bd4\u7279\u7cfb\u7edf\u3002", "result": "\u7814\u7a76\u7ed9\u51fa\u4e86\u6784\u9020N\u91cf\u5b50\u6bd4\u7279\u6700\u5927\u7ea0\u7f20\u57fa\u77e2\u7684\u91cf\u5b50\u7ebf\u8def\uff0c\u5e76\u660e\u786e\u4e86\u5b9e\u73b0\u8fd9\u4e9b\u7ebf\u8def\u6240\u9700\u7684\u5355\u6bd4\u7279\u548cCNOT\u95e8\u6570\u91cf\u3002\u968f\u673a\u6570\u65b9\u6cd5\u88ab\u8bc1\u660e\u662f\u751f\u6210\u8fd9\u4e9b\u57fa\u77e2\u548c\u7ebf\u8def\u7684\u6709\u6548\u9014\u5f84\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3aN\u91cf\u5b50\u6bd4\u7279\u7cfb\u7edf\u7684\u6700\u5927\u7ea0\u7f20\u57fa\u77e2\u63d0\u4f9b\u4e86\u4e00\u79cd\u7406\u8bba\u4e0a\u53ef\u9760\u4e14\u5b9e\u8df5\u4e0a\u53ef\u884c\u7684\u91cf\u5b50\u7ebf\u8def\u6784\u9020\u6280\u672f\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u5927\u89c4\u6a21\u7f16\u7801\u6570\u636e\uff0c\u5177\u6709\u91cd\u8981\u7684\u6280\u672f\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.23686", "categories": ["cond-mat.mtrl-sci", "astro-ph.EP"], "pdf": "https://arxiv.org/pdf/2510.23686", "abs": "https://arxiv.org/abs/2510.23686", "authors": ["Amanda Ricketts", "Benjamin A. Clouter-Gergen", "Anastasis Georgiou", "Deborah Berhanu", "Liam S. Morrissey"], "title": "Surface Binding Energies for Amorphous Plagioclase Feldspar Calculated using Molecular Dynamics", "comment": null, "summary": "Despite the well-established presence of amorphous compounds on planetary\nbodies such as the Moon and Mercury due to space weathering, the direct effect\nof atomic arrangement on the surface binding energies (SBEs) of elements on\nthese bodies remains largely unexplored. Accurate SBE values are essential for\nreliably predicting sputtering yields and the energy distribution of ejecta.\nHere, we use molecular dynamics simulations to quantify SBEs for the different\nelements sputtered from amorphous atomic arrangements of the plagioclase\nfeldspar end members, albite and anorthite, and compare to their crystalline\ncounterparts. Results show that while the mean elemental SBEs from amorphous\nsurfaces are not significantly different from their crystalline counterparts,\nthe random orientation in amorphous structures gives rise to a spectrum of\nbonding configurations, resulting in a distribution of SBEs with a wider range.\nThis contrasts with the clearly discretized set of SBE values associated with\nthe ordered atomic structure of crystalline surfaces. We then consider\nsputtering by H, He, and a solar wind combination of 96% H and 4% He. For each\nof these cases, we demonstrate that there is minimal difference (<10% for\nalbite and <20% for anorthite) between the sputtering yields of amorphous and\ncrystalline surfaces. We attribute these results to the presence of the same\nelemental bonds across different atomic arrangements, which leads to similar\nmean SBEs and, consequently, comparable sputtering yields.", "AI": {"tldr": "\u884c\u661f\u8868\u9762\u7684\u539f\u5b50\u6392\u5217\uff08\u65e0\u5b9a\u5f62\u6216\u6676\u4f53\uff09\u5bf9\u8868\u9762\u7ed3\u5408\u80fd\uff08SBE\uff09\u548c\u6e85\u5c04\u4ea7\u7387\u7684\u5f71\u54cd\u5f88\u5c0f\u3002", "motivation": "\u867d\u7136\u7a7a\u95f4\u98ce\u5316\u4f5c\u7528\u5728\u6708\u7403\u548c\u6c34\u661f\u7b49\u884c\u661f\u4f53\u4e0a\u4ea7\u751f\u4e86\u975e\u6676\u6001\u5316\u5408\u7269\uff0c\u4f46\u539f\u5b50\u6392\u5217\u5bf9\u8868\u9762\u7ed3\u5408\u80fd\uff08SBE\uff09\u7684\u76f4\u63a5\u5f71\u54cd\u4ecd\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\uff0c\u800c\u51c6\u786e\u7684SBE\u503c\u5bf9\u4e8e\u9884\u6d4b\u6e85\u5c04\u4ea7\u7387\u548c\u788e\u5c51\u80fd\u91cf\u5206\u5e03\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4f7f\u7528\u5206\u5b50\u52a8\u529b\u5b66\u6a21\u62df\u6765\u91cf\u5316\u659c\u957f\u77f3\u7aef\u5458\uff08\u94a0\u957f\u77f3\u548c\u9499\u957f\u77f3\uff09\u7684\u65e0\u5b9a\u5f62\u539f\u5b50\u6392\u5217\u4ee5\u53ca\u5b83\u4eec\u7684\u6676\u4f53\u5bf9\u5e94\u7269\u7684\u5143\u7d20SBE\uff0c\u5e76\u8003\u8651\u4e86H\u3001He\u548c\u592a\u9633\u98ce\uff0896%H\u548c4%He\uff09\u7684\u6e85\u5c04\u3002", "result": "\u65e0\u5b9a\u5f62\u8868\u9762\u7684\u5e73\u5747\u5143\u7d20SBE\u4e0e\u5176\u6676\u4f53\u5bf9\u5e94\u7269\u6ca1\u6709\u663e\u8457\u5dee\u5f02\uff0c\u4f46\u65e0\u5b9a\u5f62\u7ed3\u6784\u4e2d\u7684\u968f\u673a\u53d6\u5411\u5bfc\u81f4SBE\u7684\u5206\u5e03\u8303\u56f4\u66f4\u5e7f\u3002\u5728\u6e85\u5c04\u65b9\u9762\uff0c\u65e0\u5b9a\u5f62\u548c\u6676\u4f53\u8868\u9762\u7684\u6e85\u5c04\u4ea7\u7387\u5dee\u5f02\u5f88\u5c0f\uff08\u5bf9\u4e8e\u94a0\u957f\u77f3<10%\uff0c\u5bf9\u4e8e\u9499\u957f\u77f3<20%\uff09\u3002", "conclusion": "\u884c\u661f\u8868\u9762\u7684\u539f\u5b50\u6392\u5217\uff08\u65e0\u5b9a\u5f62\u6216\u6676\u4f53\uff09\u5bf9\u8868\u9762\u7ed3\u5408\u80fd\uff08SBE\uff09\u548c\u6e85\u5c04\u4ea7\u7387\u7684\u5f71\u54cd\u5f88\u5c0f\uff0c\u56e0\u4e3a\u4e0d\u540c\u539f\u5b50\u6392\u5217\u4e2d\u7684\u5143\u7d20\u952e\u57fa\u672c\u76f8\u540c\uff0c\u5bfc\u81f4\u5e73\u5747SBE\u548c\u6e85\u5c04\u4ea7\u7387\u76f8\u4f3c\u3002"}}
{"id": "2510.23784", "categories": ["cond-mat.mes-hall", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2510.23784", "abs": "https://arxiv.org/abs/2510.23784", "authors": ["D. J. P. de Sousa", "N. Roldan-Levchenko", "C. O. Ascencio", "J. D. S. Forte", "Paul M. Haney", "Tony Low"], "title": "Metallic Electro-Optic Effect in Twisted Double-Bilayer Graphene", "comment": null, "summary": "Recent theoretical advances have highlighted the role of Bloch state\nintrinsic properties in enabling unconventional electro-optic (EO) phenomena in\nbulk metals, offering novel strategies for dynamic optical control in quantum\nmaterials. Here, we identify an alternative EO mechanism in bulk metallic\nsystems that arises from the interplay between Berry curvature and the orbital\nmagnetic moment of Bloch electrons. Focusing on twisted double-bilayer graphene\n(TDBG), we show that the enhanced intrinsic properties of moir\\'e Bloch bands\ngive rise to a sizable linear magnetoelectric EO response, a first-order,\nelectric-field-induced non-Hermitian correction to the gyrotropic magnetic\nsusceptibility. This mechanism dominates in $C_{3z}$-symmetric TDBG, where EO\ncontributions originating from the Berry curvature dipole (BCD) are\nsymmetry-forbidden. Our calculations reveal giant, gate-tunable linear and\ncircular dichroism in the terahertz regime, establishing a robust and tunable\nplatform for ultrafast EO modulation in two-dimensional materials beyond the\nBCD paradigm.", "AI": {"tldr": "\u5728\u626d\u66f2\u53cc\u5c42\u77f3\u58a8\u70ef (TDBG) \u4e2d\u53d1\u73b0\u4e86\u7531\u8d1d\u91cc\u66f2\u7387\u548c Bloch \u7535\u5b50\u8f68\u9053\u78c1\u77e9\u76f8\u4e92\u4f5c\u7528\u9a71\u52a8\u7684\u65b0\u578b\u7535\u5149\u6548\u5e94\uff0c\u8be5\u6548\u5e94\u4e0e\u4f20\u7edf\u7684\u8d1d\u91cc\u66f2\u7387\u5076\u6781\u5b50\u673a\u5236\u4e0d\u540c\uff0c\u5177\u6709\u5de8\u5927\u7684\u3001\u53ef\u95e8\u63a7\u7684\u7ebf\u6027\u548c\u5706\u4e8c\u5411\u8272\u6027\uff0c\u53ef\u5728\u592a\u8d6b\u5179\u8303\u56f4\u5185\u5b9e\u73b0\u8d85\u5feb\u7535\u5149\u8c03\u5236\u3002", "motivation": "\u73b0\u6709\u7406\u8bba\u7740\u91cd\u4e8e Bloch \u6001\u7684\u5185\u5728\u7279\u6027\u5728\u91d1\u5c5e\u4e2d\u5b9e\u73b0\u975e\u5e38\u89c4\u7535\u5149\u73b0\u8c61\u7684\u4f5c\u7528\uff0c\u4e3a\u91cf\u5b50\u6750\u6599\u4e2d\u7684\u52a8\u6001\u5149\u63a7\u63d0\u4f9b\u4e86\u65b0\u7b56\u7565\u3002\u672c\u7814\u7a76\u65e8\u5728\u53d1\u73b0\u4e00\u79cd\u65b0\u7684\u7535\u5149\u673a\u5236\u3002", "method": "\u901a\u8fc7\u805a\u7126\u626d\u66f2\u53cc\u5c42\u77f3\u58a8\u70ef (TDBG)\uff0c\u5e76\u7814\u7a76\u5176\u83ab\u5c14 Bloch \u5e26\u7684\u5185\u5728\u7279\u6027\uff0c\u5206\u6790\u8d1d\u91cc\u66f2\u7387\u4e0e Bloch \u7535\u5b50\u8f68\u9053\u78c1\u77e9\u7684\u76f8\u4e92\u4f5c\u7528\u5982\u4f55\u4ea7\u751f\u7535\u5149\u6548\u5e94\u3002", "result": "\u8ba1\u7b97\u8868\u660e\uff0c\u5728 $C_{3z}$ \u5bf9\u79f0\u7684 TDBG \u4e2d\uff0c\u6e90\u4e8e\u8d1d\u91cc\u66f2\u7387\u5076\u6781\u5b50 (BCD) \u7684\u7535\u5149\u8d21\u732e\u88ab\u5bf9\u79f0\u6027\u7981\u6b62\uff0c\u800c\u7531\u8d1d\u91cc\u66f2\u7387\u548c\u8f68\u9053\u78c1\u77e9\u76f8\u4e92\u4f5c\u7528\u9a71\u52a8\u7684\u7535\u5149\u673a\u5236\u5360\u4e3b\u5bfc\u5730\u4f4d\uff0c\u4ea7\u751f\u4e86\u5de8\u5927\u7684\u3001\u53ef\u95e8\u63a7\u7684\u7ebf\u6027\u548c\u5706\u4e8c\u5411\u8272\u6027\uff0c\u8be5\u6548\u5e94\u5728\u592a\u8d6b\u5179\u8303\u56f4\u5185\u8868\u73b0\u660e\u663e\u3002", "conclusion": "\u6240\u53d1\u73b0\u7684\u7535\u5149\u673a\u5236\u5728 $C_{3z}$ \u5bf9\u79f0\u7684 TDBG \u4e2d\u5360\u4e3b\u5bfc\u5730\u4f4d\uff0c\u5e76\u4ea7\u751f\u4e86\u5de8\u5927\u7684\u3001\u53ef\u95e8\u63a7\u7684\u7ebf\u6027\u548c\u5706\u4e8c\u5411\u8272\u6027\uff0c\u8fd9\u4e3a\u5728 BCD \u8303\u5f0f\u4e4b\u5916\u7684\u4e8c\u7ef4\u6750\u6599\u4e2d\u5b9e\u73b0\u8d85\u5feb\u7535\u5149\u8c03\u5236\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u4e14\u53ef\u8c03\u7684\u5e73\u53f0\u3002"}}
{"id": "2510.23880", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2510.23880", "abs": "https://arxiv.org/abs/2510.23880", "authors": ["Hanke Chen", "Yuan Liu", "Minchen Li"], "title": "TRELLISWorld: Training-Free World Generation from Object Generators", "comment": null, "summary": "Text-driven 3D scene generation holds promise for a wide range of\napplications, from virtual prototyping to AR/VR and simulation. However,\nexisting methods are often constrained to single-object generation, require\ndomain-specific training, or lack support for full 360-degree viewability. In\nthis work, we present a training-free approach to 3D scene synthesis by\nrepurposing general-purpose text-to-3D object diffusion models as modular tile\ngenerators. We reformulate scene generation as a multi-tile denoising problem,\nwhere overlapping 3D regions are independently generated and seamlessly blended\nvia weighted averaging. This enables scalable synthesis of large, coherent\nscenes while preserving local semantic control. Our method eliminates the need\nfor scene-level datasets or retraining, relies on minimal heuristics, and\ninherits the generalization capabilities of object-level priors. We demonstrate\nthat our approach supports diverse scene layouts, efficient generation, and\nflexible editing, establishing a simple yet powerful foundation for\ngeneral-purpose, language-driven 3D scene construction.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u76843D\u573a\u666f\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u901a\u7528\u7684\u6587\u672c\u52303D\u5bf9\u8c61\u6269\u6563\u6a21\u578b\u7528\u4f5c\u6a21\u5757\u5316\u74f7\u7816\u751f\u6210\u5668\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5355\u5bf9\u8c61\u751f\u6210\u3001\u9886\u57df\u7279\u5b9a\u8bad\u7ec3\u6216360\u5ea6\u89c6\u56fe\u652f\u6301\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u67093D\u573a\u666f\u751f\u6210\u65b9\u6cd5\u901a\u5e38\u4ec5\u9650\u4e8e\u5355\u4e2a\u5bf9\u8c61\uff0c\u9700\u8981\u9886\u57df\u7279\u5b9a\u7684\u8bad\u7ec3\uff0c\u6216\u8005\u7f3a\u4e4f\u5b8c\u6574\u7684360\u5ea6\u89c6\u56fe\u652f\u6301\u3002\u672c\u7814\u7a76\u65e8\u5728\u514b\u670d\u8fd9\u4e9b\u9650\u5236\uff0c\u5b9e\u73b0\u66f4\u5e7f\u6cdb\u7684\u5e94\u7528\u3002", "method": "\u5c06\u901a\u7528\u6587\u672c\u52303D\u5bf9\u8c61\u6269\u6563\u6a21\u578b\u91cd\u65b0\u7528\u4f5c\u6a21\u5757\u5316\u74f7\u7816\u751f\u6210\u5668\uff0c\u5e76\u5c06\u573a\u666f\u751f\u6210\u91cd\u65b0\u8868\u8ff0\u4e3a\u591a\u74f7\u7816\u53bb\u566a\u95ee\u9898\u3002\u91cd\u53e0\u76843D\u533a\u57df\u88ab\u72ec\u7acb\u751f\u6210\u5e76\u901a\u8fc7\u52a0\u6743\u5e73\u5747\u65e0\u7f1d\u878d\u5408\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u5927\u89c4\u6a21\u3001\u8fde\u8d2f\u76843D\u573a\u666f\uff0c\u540c\u65f6\u4fdd\u6301\u5c40\u90e8\u8bed\u4e49\u63a7\u5236\u3002\u5b83\u4e0d\u9700\u8981\u573a\u666f\u7ea7\u6570\u636e\u96c6\u6216\u91cd\u65b0\u8bad\u7ec3\uff0c\u5e76\u4e14\u53ef\u4ee5\u7075\u6d3b\u5730\u8fdb\u884c\u7f16\u8f91\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a\u901a\u7528\u3001\u7531\u8bed\u8a00\u9a71\u52a8\u76843D\u573a\u666f\u6784\u5efa\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7b80\u5355\u800c\u5f3a\u5927\u7684\u57fa\u7840\uff0c\u652f\u6301\u5404\u79cd\u573a\u666f\u5e03\u5c40\u548c\u9ad8\u6548\u751f\u6210\u3002"}}
{"id": "2510.23775", "categories": ["cs.CV", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.23775", "abs": "https://arxiv.org/abs/2510.23775", "authors": ["Aryan Mathur", "Asaduddin Ahmed", "Pushti Amit Vasoya", "Simeon Kandan Sonar", "Yasir Z", "Madesh Kuppusamy"], "title": "Explainable Detection of AI-Generated Images with Artifact Localization Using Faster-Than-Lies and Vision-Language Models for Edge Devices", "comment": null, "summary": "The increasing realism of AI-generated imagery poses challenges for verifying\nvisual authenticity. We present an explainable image authenticity detection\nsystem that combines a lightweight convolutional classifier\n(\"Faster-Than-Lies\") with a Vision-Language Model (Qwen2-VL-7B) to classify,\nlocalize, and explain artifacts in 32x32 images. Our model achieves 96.5%\naccuracy on the extended CiFAKE dataset augmented with adversarial\nperturbations and maintains an inference time of 175ms on 8-core CPUs, enabling\ndeployment on local or edge devices. Using autoencoder-based reconstruction\nerror maps, we generate artifact localization heatmaps, which enhance\ninterpretability for both humans and the VLM. We further categorize 70 visual\nartifact types into eight semantic groups and demonstrate explainable text\ngeneration for each detected anomaly. This work highlights the feasibility of\ncombining visual and linguistic reasoning for interpretable authenticity\ndetection in low-resolution imagery and outlines potential cross-domain\napplications in forensics, industrial inspection, and social media moderation.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2510.23610", "categories": ["quant-ph", "cs.ET", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.23610", "abs": "https://arxiv.org/abs/2510.23610", "authors": ["Shakil Ahmed"], "title": "Dynamic RIS-Assisted THz Quantum Networks: Joint Optimization of Entanglement Generation and Fidelity under Channel Impairments", "comment": null, "summary": "Quantum networks (QNs) supported by terahertz (THz) wireless links present a\ntransformative alternative to fiber-based infrastructures, particularly in\nmobile and infrastructure-scarce environments. However, signal attenuation,\nmolecular absorption, and severe propagation losses in THz channels pose\nsignificant challenges to reliable quantum state transmission and entanglement\ndistribution. To overcome these limitations, we propose a dynamic\nreconfigurable intelligent surface (RIS)-assisted wireless QN architecture that\nleverages adaptive RIS elements capable of switching between active and passive\nmodes based on the incident signal-to-noise ratio (SNR). These dynamic RIS\nelements enhance beamforming control over amplitude and phase, enabling robust\nredirection and compensation for THz-specific impairments. We develop a\ndetailed analytical model that incorporates key physical layer phenomena in THz\nquantum links, including path loss, fading, thermal noise, and alignment\nvariations. A secure optimization framework is formulated to jointly determine\nRIS placement and entanglement generation rate (EGR) allocation, while\nsatisfying fidelity, security, and fairness constraints under diverse quality\nof service (QoS) demands. The model also includes an exploration of\nside-channel vulnerabilities arising from dynamic RIS switching patterns.\nSimulation results demonstrate that the proposed architecture yields up to 87\\%\nfidelity enhancement and 65\\% fairness improvement compared to static RIS\nbaselines, while maintaining robustness under realistic THz channel conditions.\nThese results underscore the promise of dynamic RIS technology in enabling\nscalable and adaptive quantum communications over wireless THz links.", "AI": {"tldr": "\u592a\u8d6b\u5179\u65e0\u7ebf\u91cf\u5b50\u7f51\u7edc\u5229\u7528\u52a8\u6001\u53ef\u91cd\u6784\u667a\u80fd\u8868\u9762\uff08RIS\uff09\u514b\u670d\u4fe1\u53f7\u8870\u51cf\u548c\u635f\u8017\uff0c\u901a\u8fc7\u81ea\u9002\u5e94RIS\u5143\u4ef6\u589e\u5f3a\u6ce2\u675f\u5f62\u6210\u548c\u4fe1\u53f7\u8865\u507f\uff0c\u5e76\u901a\u8fc7\u4f18\u5316RIS\u5e03\u5c40\u548c\u7ea0\u7f20\u751f\u6210\u7387\uff08EGR\uff09\u6765\u63d0\u9ad8\u4fdd\u771f\u5ea6\u3001\u5b89\u5168\u6027\u548c\u516c\u5e73\u6027\u3002", "motivation": "\u592a\u8d6b\u5179\uff08THz\uff09\u65e0\u7ebf\u94fe\u8def\u5728\u79fb\u52a8\u548c\u57fa\u7840\u8bbe\u65bd\u7a00\u7f3a\u7684\u73af\u5883\u4e2d\u4e3a\u91cf\u5b50\u7f51\u7edc\uff08QN\uff09\u63d0\u4f9b\u4e86\u66ff\u4ee3\u5149\u7ea4\u7684\u65b9\u6848\uff0c\u4f46THz\u4fe1\u9053\u7684\u4fe1\u53f7\u8870\u51cf\u3001\u5206\u5b50\u5438\u6536\u548c\u4e25\u91cd\u7684\u4f20\u64ad\u635f\u8017\u5bf9\u91cf\u5b50\u6001\u4f20\u8f93\u548c\u7ea0\u7f20\u5206\u53d1\u6784\u6210\u4e86\u91cd\u5927\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u53ef\u91cd\u6784\u667a\u80fd\u8868\u9762\uff08RIS\uff09\u8f85\u52a9\u7684\u65e0\u7ebf\u91cf\u5b50\u7f51\u7edc\u67b6\u6784\uff0c\u5176\u4e2d\u81ea\u9002\u5e94RIS\u5143\u4ef6\u53ef\u6839\u636e\u5165\u5c04\u4fe1\u566a\u6bd4\uff08SNR\uff09\u5728\u4e3b\u52a8\u548c\u88ab\u52a8\u6a21\u5f0f\u4e4b\u95f4\u5207\u6362\u3002\u5f00\u53d1\u4e86\u4e00\u4e2a\u8be6\u7ec6\u7684\u5206\u6790\u6a21\u578b\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u5b89\u5168\u7684\u4f18\u5316\u6846\u67b6\uff0c\u4ee5\u8054\u5408\u786e\u5b9aRIS\u7684\u653e\u7f6e\u548c\u7ea0\u7f20\u751f\u6210\u7387\uff08EGR\uff09\u7684\u5206\u914d\uff0c\u540c\u65f6\u6ee1\u8db3\u4fdd\u771f\u5ea6\u3001\u5b89\u5168\u6027\u548c\u516c\u5e73\u6027\u7ea6\u675f\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u9759\u6001RIS\u57fa\u7ebf\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684\u67b6\u6784\u5728\u4fdd\u771f\u5ea6\u65b9\u9762\u63d0\u9ad8\u4e8687%\uff0c\u5728\u516c\u5e73\u6027\u65b9\u9762\u63d0\u9ad8\u4e8665%\uff0c\u540c\u65f6\u5728\u73b0\u5b9e\u7684THz\u4fe1\u9053\u6761\u4ef6\u4e0b\u4fdd\u6301\u4e86\u9c81\u68d2\u6027\u3002", "conclusion": "\u52a8\u6001RIS\u6280\u672f\u6709\u671b\u5b9e\u73b0\u53ef\u6269\u5c55\u548c\u81ea\u9002\u5e94\u7684\u65e0\u7ebfTHz\u94fe\u8def\u91cf\u5b50\u901a\u4fe1\u3002"}}
{"id": "2510.24060", "categories": ["cs.LO", "math.AP"], "pdf": "https://arxiv.org/pdf/2510.24060", "abs": "https://arxiv.org/abs/2510.24060", "authors": ["Moritz Doll"], "title": "Formalizing Schwartz functions and tempered distributions", "comment": "13 pages", "summary": "Distribution theory is a cornerstone of the theory of partial differential\nequations. We report on the progress of formalizing the theory of tempered\ndistributions in the interactive proof assistant Lean, which is the first\nformalization in any proof assistant. We give an overview of the mathematical\ntheory and highlight key aspects of the formalization that differ from the\nclassical presentation. As an application, we prove that the Fourier transform\nextends to a linear isometry on $L^2$ and we define Sobolev spaces via the\nFourier transform on tempered distributions.", "AI": {"tldr": "\u5f62\u5f0f\u5316\u4e86\u52d2\u8ba9\u5fb7\u5206\u5e03\u7406\u8bba\uff0c\u5e76\u5e94\u7528\u5176\u8bc1\u660e\u4e86\u5085\u91cc\u53f6\u53d8\u6362\u662fL^2\u4e0a\u7684\u7ebf\u6027\u7b49\u8ddd\u53d8\u6362\uff0c\u4ee5\u53ca\u5b9a\u4e49\u4e86\u5085\u91cc\u53f6\u53d8\u6362\u4e0b\u7684\u7d22\u4f2f\u5207\u592b\u7a7a\u95f4\u3002", "motivation": "\u5bf9\u52d2\u8ba9\u5fb7\u5206\u5e03\u7406\u8bba\u8fdb\u884c\u5f62\u5f0f\u5316\uff0c\u5e76\u8bc1\u660e\u5085\u91cc\u53f6\u53d8\u6362\u7684\u6027\u8d28\u3002", "method": "\u5728Lean\u8bc1\u660e\u52a9\u624b\u4e0a\u5f62\u5f0f\u5316\u52d2\u8ba9\u5fb7\u5206\u5e03\u7406\u8bba\uff0c\u5e76\u5e94\u7528\u8be5\u7406\u8bba\u3002", "result": "\u6210\u529f\u5728Lean\u4e0a\u5f62\u5f0f\u5316\u4e86\u52d2\u8ba9\u5fb7\u5206\u5e03\u7406\u8bba\uff0c\u5e76\u8bc1\u660e\u4e86\u5085\u91cc\u53f6\u53d8\u6362\u5728L^2\u4e0a\u7684\u7ebf\u6027\u7b49\u8ddd\u6027\u8d28\uff0c\u540c\u65f6\u5b9a\u4e49\u4e86\u5085\u91cc\u53f6\u53d8\u6362\u4e0b\u7684\u7d22\u4f2f\u5207\u592b\u7a7a\u95f4\u3002", "conclusion": "\u5bf9\u52d2\u8ba9\u5fb7\u5206\u5e03\u7406\u8bba\u7684\u6210\u529f\u5f62\u5f0f\u5316\u4ee5\u53ca\u5085\u91cc\u53f6\u53d8\u6362\u6027\u8d28\u7684\u8bc1\u660e\u548c\u7d22\u4f2f\u5207\u592b\u7a7a\u95f4\u7684\u5b9a\u4e49\u3002"}}
{"id": "2510.24112", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.24112", "abs": "https://arxiv.org/abs/2510.24112", "authors": ["Junchi Wu", "Xinfei Wan", "Zhuoran Li", "Yuyang Jin", "Guangyu Sun", "Yun Liang", "Diyu Zhou", "Youwei Zhuo"], "title": "SlowPoke: Understanding and Detecting On-Chip Fail-Slow Failures in Many-Core Systems", "comment": "15 pages, 15 figures", "summary": "Many-core architectures are essential for high-performance computing, but\ntheir performance is undermined by widespread fail-slow failures. Detecting\nsuch failures on-chip is challenging, as prior methods from distributed systems\nare unsuitable due to strict memory limits and their inability to track\nfailures across the hardware topology. This paper introduces SlowPoke, a\nlightweight, hardware-aware framework for practical on-chip fail-slow\ndetection. SlowPoke combines compiler-based instrumentation for low-overhead\nmonitoring, on-the-fly trace compression to operate within kilobytes of memory,\nand a novel topology-aware ranking algorithm to pinpoint a failure's root\ncause. We evaluate SlowPoke on a wide range of representative many-core\nworkloads, and the results demonstrate that SlowPoke reduces the storage\noverhead of detection traces by an average of 115.9$\\times$, while achieving an\naverage fail-slow detection accuracy of 86.77% and a false positive rate (FPR)\nof 12.11%. More importantly, SlowPoke scales effectively across different\nmany-core architectures, making it practical for large-scale deployments.", "AI": {"tldr": "SlowPoke\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u786c\u4ef6\u611f\u77e5\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u7247\u4e0a\u68c0\u6d4b\u6162\u901f\u6545\u969c\uff0c\u901a\u8fc7\u7f16\u8bd1\u5668\u63d2\u6869\u3001\u8f68\u8ff9\u538b\u7f29\u548c\u62d3\u6251\u611f\u77e5\u6392\u5e8f\u6765\u51cf\u5c11\u5b58\u50a8\u5f00\u9500\u5e76\u63d0\u9ad8\u68c0\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u8bb8\u591a\u6838\u5fc3\u67b6\u6784\u5bf9\u9ad8\u6027\u80fd\u8ba1\u7b97\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u6027\u80fd\u53d7\u5230\u666e\u904d\u5b58\u5728\u7684\u6162\u901f\u6545\u969c\u7684\u635f\u5bb3\u3002\u7531\u4e8e\u5185\u5b58\u9650\u5236\u548c\u65e0\u6cd5\u8ddf\u8e2a\u786c\u4ef6\u62d3\u6251\u4e2d\u7684\u6545\u969c\uff0c\u5728\u82af\u7247\u4e0a\u68c0\u6d4b\u6b64\u7c7b\u6545\u969c\u5177\u6709\u6311\u6218\u6027\u3002", "method": "SlowPoke\u7ed3\u5408\u4e86\u57fa\u4e8e\u7f16\u8bd1\u5668\u7684\u63d2\u6869\u8fdb\u884c\u4f4e\u5f00\u9500\u76d1\u63a7\u3001\u52a8\u6001\u8f68\u8ff9\u538b\u7f29\u4ee5\u5728\u51e0KB\u5185\u5b58\u5185\u8fd0\u884c\uff0c\u4ee5\u53ca\u4e00\u79cd\u65b0\u9896\u7684\u62d3\u6251\u611f\u77e5\u6392\u5e8f\u7b97\u6cd5\u6765\u67e5\u660e\u6545\u969c\u7684\u6839\u672c\u539f\u56e0\u3002", "result": "SlowPoke\u5c06\u68c0\u6d4b\u8f68\u8ff9\u7684\u5b58\u50a8\u5f00\u9500\u5e73\u5747\u51cf\u5c11\u4e86115.9\u500d\uff0c\u540c\u65f6\u5b9e\u73b0\u4e8686.77%\u7684\u5e73\u5747\u6162\u901f\u6545\u969c\u68c0\u6d4b\u51c6\u786e\u7387\u548c12.11%\u7684\u8bef\u62a5\u7387\uff08FPR\uff09\u3002", "conclusion": "SlowPoke\u80fd\u591f\u6709\u6548\u5730\u6269\u5c55\u5230\u4e0d\u540c\u7684\u591a\u6838\u67b6\u6784\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u90e8\u7f72\u3002"}}
{"id": "2510.23806", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.23806", "abs": "https://arxiv.org/abs/2510.23806", "authors": ["Samuel Chevalier", "Duncan Starkenburg", "Robert Parker", "Noah Rhodes"], "title": "Maximal Load Shedding Verification for Neural Network Models of AC Line Switching", "comment": null, "summary": "Solving for globally optimal line switching decisions in AC transmission\ngrids can be intractability slow. Machine learning (ML) models, meanwhile, can\nbe trained to predict near-optimal decisions at a fraction of the speed.\nVerifying the performance and impact of these ML models on network operation,\nhowever, is a critically important step prior to their actual deployment. In\nthis paper, we train a Neural Network (NN) to solve the optimal power shutoff\nline switching problem. To assess the worst-case load shedding induced by this\nmodel, we propose a bilevel attacker-defender verification approach that finds\nthe NN line switching decisions that cause the highest quantity of network load\nshedding. Solving this problem to global optimality is challenging (due to AC\npower flow and NN nonconvexities), so our approach exploits a convex relaxation\nof the AC physics, combined with a local NN search, to find a guaranteed lower\nbound on worst--case load shedding. These under-approximation bounds are solved\nvia MathOptAI.jl. We benchmark against a random sampling approach, and we find\nthat our optimization-based approach always finds larger load shedding. Test\nresults are collected on multiple PGLib test cases and on trained NN models\nwhich contain more than 10 million model parameters.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4f18\u5316\u7684\u65b9\u6cd5\u6765\u8bc4\u4f30\u795e\u7ecf\u2f79\u7edc\u5728\u4ea4\u6d41\u7535\u2f79\u7edc\u7ebf\u2f79\u7edc\u5207\u6362\u4e2d\u7684\u6700\u574f\u60c5\u51b5\u2f79\u7edc\u5378\u8f7d\uff0c\u5728 PGLib \u6d4b\u8bd5\u2f64\u4f8b\u4e0a\u53d1\u73b0\u4e86\u2f50\u968f\u673a\u91c7\u6837\u7684\u66f4\u2f24\u5378\u8f7d\u3002", "motivation": "\u2f46\u6cd5\u5feb\u901f\u6c42\u89e3\u6700\u4f18\u7684\u4ea4\u6d41\u7535\u2f79\u7edc\u7ebf\u5207\u6362\u51b3\u7b56\uff1b\u9700\u8981\u9a8c\u8bc1\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5bf9\u2f79\u7edc\u8fd0\u2f8f\u7684\u5f71\u54cd\u3002", "method": "\u8bad\u7ec3\u795e\u7ecf\u2f79\u7edc\u9884\u6d4b\u7ebf\u5207\u6362\uff0c\u5e76\u4f7f\u2f64\u4e0a\u4e0b\u2f40\u653b\u51fb-\u9632\u5fa1\u6846\u67b6\u6765\u8bc4\u4f30\u6700\u574f\u60c5\u51b5\u4e0b\u7684\u5378\u8f7d\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u4ea4\u6d41\u7535\u7269\u7406\u7684\u51f8\u677e\u5f1b\u548c\u5c40\u90e8\u795e\u7ecf\u2f79\u7edc\u641c\u7d22\u3002", "result": "\u5728 PGLib \u6d4b\u8bd5\u2f64\u4f8b\u548c\u5177\u6709\u8d85\u8fc7 1000 \u4e07\u4e2a\u6a21\u578b\u53c2\u6570\u7684\u8bad\u7ec3\u795e\u7ecf\u2f79\u7edc\u6a21\u578b\u4e0a\uff0c\u4e0e\u968f\u673a\u91c7\u6837\u2f45\u6cd5\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u4f18\u5316\u2f45\u6cd5\u603b\u662f\u80fd\u53d1\u73b0\u66f4\u2f24\u7684\u5378\u8f7d\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u4e0a\u4e0b\u2f40\u653b\u51fb-\u9632\u5fa1\u2f45\u6cd5\u80fd\u591f\u6709\u6548\u5730\u627e\u5230\u795e\u7ecf\u2f79\u7edc\u7ebf\u5207\u6362\u51b3\u7b56\u5bfc\u81f4\u7684\u6700\u2f24\u5378\u8f7d\u7684\u4e0b\u754c\uff0c\u63d0\u4f9b\u4e86\u5bf9\u6a21\u578b\u5728\u2f79\u7edc\u5b89\u5168\u4e2d\u7684\u6700\u574f\u60c5\u51b5\u7684\u4fdd\u8bc1\u3002"}}
{"id": "2510.23780", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.23780", "abs": "https://arxiv.org/abs/2510.23780", "authors": ["Omran Abbas", "Abdullah Zayat", "Lo\u0131c Markley", "Anas Chaaban"], "title": "Nonlinear Stacked Intelligent Surfaces for Wireless Systems", "comment": "9 pages, 4 figures", "summary": "Stacked intelligent surfaces (SIS) are a promising technology for\nnext-generation wireless systems, offering an opportunity to enhance\ncommunication performance with low power consumption. Typically, an SIS is\nmodelled as a surface that imparts phase shifts on impinging electromagnetic\nsignals to achieve desired communication objectives. However, this mode of\noperation results in a linear SIS, which limits its applicability to linear\noperations. To unlock further SIS potential, we propose a nonlinear SIS that\ncan mimic the behaviour of nonlinear neural networks. We discuss the\nfeasibility and potential of this idea and propose a nonlinear SIS unit cell\nwith a step-like response. To evaluate the system-level performance of\nnonlinear SIS, we present a case study where SIS structures are optimized to\nminimize the symbol error rate (SER) in an MIMO system with SIS deployed at\nboth the transmitter and receiver sides using only statistical channel\ninformation. We demonstrate that a nonlinear SIS can improve communication\nreliability compared to a linear SIS by forming complex signal patterns across\nthe SIS surface, which provide higher diversity against noise disturbances,\nwhile still allowing the receiver to discern these patterns. Finally, we\noutline several potential applications of nonlinear SIS in wireless\ncommunication scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u975e\u7ebf\u6027\u667a\u80fd\u8d85\u8868\u9762\uff08SIS\uff09\uff0c\u53ef\u6a21\u62df\u795e\u7ecf\u7f51\u7edc\u884c\u4e3a\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728MIMO\u7cfb\u7edf\u4e2d\u4f18\u4e8e\u7ebf\u6027SIS\uff0c\u80fd\u63d0\u9ad8\u901a\u4fe1\u53ef\u9760\u6027\u3002", "motivation": "SIS\u6280\u672f\u5728\u63d0\u9ad8\u901a\u4fe1\u6027\u80fd\u548c\u964d\u4f4e\u529f\u8017\u65b9\u9762\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u73b0\u6709\u7ebf\u6027SIS\u6a21\u578b\u9650\u5236\u4e86\u5176\u5e94\u7528\u8303\u56f4\u3002\u4e3a\u8fdb\u4e00\u6b65\u6316\u6398SIS\u6f5c\u529b\uff0c\u9700\u63a2\u7d22\u5176\u975e\u7ebf\u6027\u7279\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u975e\u7ebf\u6027SIS\u6a21\u578b\uff0c\u5e76\u8bbe\u8ba1\u5177\u6709\u9636\u8dc3\u54cd\u5e94\u7684\u5355\u5143\u7ed3\u6784\u3002\u5728MIMO\u7cfb\u7edf\u4e2d\uff0c\u5229\u7528\u7edf\u8ba1\u4fe1\u9053\u4fe1\u606f\u4f18\u5316SIS\u7ed3\u6784\uff0c\u4ee5\u6700\u5c0f\u5316\u7b26\u53f7\u9519\u8bef\u7387\uff08SER\uff09\u3002", "result": "\u4e0e\u7ebf\u6027SIS\u76f8\u6bd4\uff0c\u975e\u7ebf\u6027SIS\u901a\u8fc7\u5f62\u6210\u590d\u6742\u7684\u4fe1\u53f7\u6a21\u5f0f\uff0c\u5728MIMO\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u4fe1\u566a\u6bd4\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u901a\u4fe1\u53ef\u9760\u6027\uff0c\u540c\u65f6\u4ecd\u80fd\u88ab\u63a5\u6536\u7aef\u8bc6\u522b\u3002", "conclusion": "\u975e\u7ebf\u6027SIS\u6280\u672f\u53ef\u884c\u4e14\u6f5c\u529b\u5de8\u5927\uff0c\u80fd\u591f\u901a\u8fc7\u6a21\u62df\u795e\u7ecf\u7f51\u7edc\u884c\u4e3a\u663e\u8457\u63d0\u5347\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u4e3a\u672a\u6765\u65e0\u7ebf\u901a\u4fe1\u63d0\u4f9b\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.23617", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23617", "abs": "https://arxiv.org/abs/2510.23617", "authors": ["Phuong Q. Dao", "Mark Roantree", "Vuong M. Ngo"], "title": "An Enhanced Dual Transformer Contrastive Network for Multimodal Sentiment Analysis", "comment": "The paper has been accepted for presentation at the MEDES 2025\n  conference", "summary": "Multimodal Sentiment Analysis (MSA) seeks to understand human emotions by\njointly analyzing data from multiple modalities typically text and images\noffering a richer and more accurate interpretation than unimodal approaches. In\nthis paper, we first propose BERT-ViT-EF, a novel model that combines powerful\nTransformer-based encoders BERT for textual input and ViT for visual input\nthrough an early fusion strategy. This approach facilitates deeper cross-modal\ninteractions and more effective joint representation learning. To further\nenhance the model's capability, we propose an extension called the Dual\nTransformer Contrastive Network (DTCN), which builds upon BERT-ViT-EF. DTCN\nincorporates an additional Transformer encoder layer after BERT to refine\ntextual context (before fusion) and employs contrastive learning to align text\nand image representations, fostering robust multimodal feature learning.\nEmpirical results on two widely used MSA benchmarks MVSA-Single and TumEmo\ndemonstrate the effectiveness of our approach. DTCN achieves best accuracy\n(78.4%) and F1-score (78.3%) on TumEmo, and delivers competitive performance on\nMVSA-Single, with 76.6% accuracy and 75.9% F1-score. These improvements\nhighlight the benefits of early fusion and deeper contextual modeling in\nTransformer-based multimodal sentiment analysis.", "AI": {"tldr": "BERT-ViT-EF and DTCN\u6a21\u578b\u5728\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u826f\u597d\u6548\u679c\u3002", "motivation": "\u4e3a\u4e86\u66f4\u4e30\u5bcc\u3001\u66f4\u51c6\u786e\u5730\u7406\u89e3\u4eba\u7c7b\u60c5\u611f\uff0c\u63d0\u51faBERT-ViT-EF\u6a21\u578b\uff0c\u901a\u8fc7\u65e9\u671f\u878d\u5408\u7b56\u7565\u7ed3\u5408BERT\u548cViT\uff0c\u4fc3\u8fdb\u8de8\u6a21\u6001\u4ea4\u4e92\u548c\u8054\u5408\u8868\u793a\u5b66\u4e60\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u63d0\u51faDTCN\u6a21\u578b\uff0c\u901a\u8fc7\u589e\u52a0Transformer\u7f16\u7801\u5668\u5c42\u548c\u5bf9\u6bd4\u5b66\u4e60\u6765\u8fdb\u4e00\u6b65\u63d0\u5347\u6587\u672c\u548c\u56fe\u50cf\u8868\u793a\u7684\u5bf9\u9f50\u548c\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51faBERT-ViT-EF\u6a21\u578b\uff0c\u91c7\u7528\u65e9\u671f\u878d\u5408\u7b56\u7565\uff0c\u7ed3\u5408BERT\u5904\u7406\u6587\u672c\uff0cViT\u5904\u7406\u56fe\u50cf\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u63d0\u51faDTCN\u6a21\u578b\uff0c\u5728BERT\u4e4b\u540e\u589e\u52a0Transformer\u7f16\u7801\u5668\u5c42\u6765\u7ec6\u5316\u6587\u672c\u4e0a\u4e0b\u6587\uff0c\u5e76\u4f7f\u7528\u5bf9\u6bd4\u5b66\u4e60\u6765\u5bf9\u9f50\u6587\u672c\u548c\u56fe\u50cf\u8868\u793a\u3002", "result": "\u5728MVSA-Single\u548cTumEmo\u4e24\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\u3002DTCN\u5728TumEmo\u4e0a\u8fbe\u5230\u4e86\u6700\u4f73\u51c6\u786e\u7387\uff0878.4%\uff09\u548cF1\u5206\u6570\uff0878.3%\uff09\uff0c\u5728MVSA-Single\u4e0a\u4e5f\u53d6\u5f97\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u51c6\u786e\u7387\u4e3a76.6%\uff0cF1\u5206\u6570\u4e3a75.9%\u3002", "conclusion": "\u65e9\u671f\u878d\u5408\u548c\u6df1\u5ea6\u4e0a\u4e0b\u6587\u5efa\u6a21\u7684Transformer-based\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u65b9\u6cd5\u662f\u6709\u6548\u7684\u3002"}}
{"id": "2510.23691", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23691", "abs": "https://arxiv.org/abs/2510.23691", "authors": ["Zihao Wang", "Xujing Li", "Yining Ye", "Junjie Fang", "Haoming Wang", "Longxiang Liu", "Shihao Liang", "Junting Lu", "Zhiyong Wu", "Jiazhan Feng", "Wanjun Zhong", "Zili Li", "Yu Wang", "Yu Miao", "Bo Zhou", "Yuanfan Li", "Hao Wang", "Zhongkai Zhao", "Faming Wu", "Zhengxuan Jiang", "Weihao Tan", "Heyuan Yao", "Shi Yan", "Xiangyang Li", "Yitao Liang", "Yujia Qin", "Guang Shi"], "title": "Game-TARS: Pretrained Foundation Models for Scalable Generalist Multimodal Game Agents", "comment": null, "summary": "We present Game-TARS, a generalist game agent trained with a unified,\nscalable action space anchored to human-aligned native keyboard-mouse inputs.\nUnlike API- or GUI-based approaches, this paradigm enables large-scale\ncontinual pre-training across heterogeneous domains, including OS, web, and\nsimulation games. Game-TARS is pre-trained on over 500B tokens with diverse\ntrajectories and multimodal data. Key techniques include a decaying continual\nloss to reduce causal confusion and an efficient Sparse-Thinking strategy that\nbalances reasoning depth and inference cost. Experiments show that Game-TARS\nachieves about 2 times the success rate over the previous sota model on\nopen-world Minecraft tasks, is close to the generality of fresh humans in\nunseen web 3d games, and outperforms GPT-5, Gemini-2.5-Pro, and Claude-4-Sonnet\nin FPS benchmarks. Scaling results on training-time and test-time confirm that\nthe unified action space sustains improvements when scaled to cross-game and\nmultimodal data. Our results demonstrate that simple, scalable action\nrepresentations combined with large-scale pre-training provide a promising path\ntoward generalist agents with broad computer-use abilities.", "AI": {"tldr": "Game-TARS\u662f\u4e00\u4e2a\u901a\u7528\u7684\u6e38\u620f\u4ee3\u7406\uff0c\u901a\u8fc7\u7edf\u4e00\u3001\u53ef\u6269\u5c55\u7684\u3001\u4ee5\u4eba\u7c7b\u4e3a\u4e2d\u5fc3\u7684\u952e\u76d8\u9f20\u6807\u8f93\u5165\u52a8\u4f5c\u7a7a\u95f4\u8fdb\u884c\u8bad\u7ec3\uff0c\u53ef\u4ee5\u8de8\u8d8a\u64cd\u4f5c\u7cfb\u7edf\u3001\u7f51\u9875\u548c\u6a21\u62df\u6e38\u620f\u7b49\u5f02\u6784\u9886\u57df\u8fdb\u884c\u5927\u89c4\u6a21\u6301\u7eed\u9884\u8bad\u7ec3\u3002\u5b83\u5728500B\u4ee5\u4e0atoken\u4e0a\u8fdb\u884c\u4e86\u9884\u8bad\u7ec3\uff0c\u5e76\u91c7\u7528\u4e86\u8870\u51cf\u6301\u7eed\u635f\u5931\u548c\u7a00\u758f\u601d\u8003\u7b56\u7565\u3002", "motivation": "\u5728\u901a\u7528\u4ee3\u7406\u548c\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u65b9\u9762\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u8de8\u8d8a\u4e0d\u540c\u9886\u57df\uff08\u5982\u64cd\u4f5c\u7cfb\u7edf\u3001\u7f51\u9875\u548c\u6e38\u620f\uff09\u5e76\u80fd\u5904\u7406\u4eba\u7c7b\u539f\u751f\u8f93\u5165\uff08\u952e\u76d8\u9f20\u6807\uff09\u7684\u7edf\u4e00\u65b9\u6cd5\u3002", "method": "Game-TARS \u4f7f\u7528\u7edf\u4e00\u3001\u53ef\u6269\u5c55\u7684\u52a8\u4f5c\u7a7a\u95f4\uff0c\u8be5\u7a7a\u95f4\u951a\u5b9a\u4e8e\u4eba\u7c7b\u539f\u751f\u7684\u952e\u76d8\u9f20\u6807\u8f93\u5165\u3002\u5b83\u91c7\u7528\u4e86\u8870\u51cf\u6301\u7eed\u635f\u5931\u6765\u51cf\u5c11\u56e0\u679c\u6df7\u6dc6\uff0c\u5e76\u7ed3\u5408\u4e86\u7a00\u758f\u601d\u8003\u7b56\u7565\u6765\u5e73\u8861\u63a8\u7406\u6df1\u5ea6\u548c\u63a8\u7406\u6210\u672c\u3002\u5728\u5927\u89c4\u6a21\u5f02\u6784\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\u3002", "result": "Game-TARS \u5728\u5f00\u653e\u4e16\u754c\u300a\u6211\u7684\u4e16\u754c\u300b\u4efb\u52a1\u4e0a\u7684\u6210\u529f\u7387\u662f\u4e4b\u524d\u6700\u4f18\u6a21\u578b\u7684\u7ea62\u500d\uff0c\u5728\u672a\u89c1\u7684\u7f51\u98753D\u6e38\u620f\u4e2d\u8868\u73b0\u63a5\u8fd1\u4eba\u7c7b\u7684\u901a\u7528\u6027\uff0c\u5e76\u5728FPS\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u4e86GPT-5\u3001Gemini-2.5-Pro\u548cClaude-4-Sonnet\u3002", "conclusion": "\u7b80\u5355\u7684\u3001\u53ef\u6269\u5c55\u7684\u52a8\u4f5c\u8868\u793a\u4e0e\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u76f8\u7ed3\u5408\uff0c\u4e3a\u5177\u6709\u5e7f\u6cdb\u8ba1\u7b97\u673a\u4f7f\u7528\u80fd\u529b\u7684\u901a\u7528\u4ee3\u7406\u63d0\u4f9b\u4e86\u4e00\u6761\u6709\u524d\u666f\u7684\u9053\u8def\u3002"}}
{"id": "2510.24510", "categories": ["cs.ET"], "pdf": "https://arxiv.org/pdf/2510.24510", "abs": "https://arxiv.org/abs/2510.24510", "authors": ["Hugo Alcaraz-Herrera", "Michail-Antisthenis Tsompanas", "Igor Balaz", "Andrew Adamatzky"], "title": "Evaluating Fitness Averaging Strategies in Cooperative NeuroCoEvolution for Automated Soft Actuator Design", "comment": null, "summary": "Soft robotics are increasingly favoured in specific applications such as\nhealthcare, due to their adaptability, which stems from the non-linear\nproperties of their building materials. However, these properties also pose\nsignificant challenges in designing the morphologies and controllers of soft\nrobots. The relatively short history of this field has not yet produced\nsufficient knowledge to consistently derive optimal solutions. Consequently, an\nautomated process for the design of soft robot morphologies can be extremely\nhelpful. This study focusses on the cooperative NeuroCoEvolution of networks\nthat are indirect representations of soft robot actuators. Both the\nmorphologies and controllers represented by Compositional Pattern Producing\nNetworks are evolved using the well-established method NeuroEvolution of\nAugmented Topologies (CPPN-NEAT). The CoEvolution of controllers and\nmorphologies is implemented using the top n individuals from the cooperating\npopulation, with various averaging methods tested to determine the fitness of\nthe evaluated individuals. The test-case application for this research is the\noptimisation of a soft actuator for a drug delivery system. The primary metric\nused is the maximum displacement of one end of the actuator in a specified\ndirection. Additionally, the robustness of the evolved morphologies is assessed\nagainst a range of randomly generated controllers to simulate potential noise\nin real-world applications. The results of this investigation indicate that\nCPPN-NEAT produces superior morphologies compared to previously published\nresults from multi-objective optimisation, with reduced computational effort\nand time. Moreover, the best configuration is found to be CoEvolution with the\ntwo best individuals from the cooperative population and the averaging of their\nfitness using the weighted mean method.", "AI": {"tldr": "CPPN-NEAT\u901a\u8fc7\u534f\u540c\u8fdb\u5316\u514b\u670d\u4e86\u8f6f\u4f53\u673a\u5668\u4eba\u8bbe\u8ba1\u7684\u6311\u6218\uff0c\u5e76\u5728\u836f\u7269\u8f93\u9001\u5e94\u7528\u4e2d\u4ea7\u751f\u4e86\u4f18\u8d8a\u7684\u5f62\u6001\uff0c\u4f18\u4e8e\u591a\u76ee\u6807\u4f18\u5316\u65b9\u6cd5\u3002", "motivation": "\u8f6f\u4f53\u673a\u5668\u4eba\u56e0\u5176\u9002\u5e94\u6027\u800c\u53d7\u5230\u9752\u7750\uff0c\u4f46\u5176\u975e\u7ebf\u6027\u6750\u6599\u7279\u6027\u7ed9\u8bbe\u8ba1\u5e26\u6765\u4e86\u6311\u6218\uff0c\u9700\u8981\u81ea\u52a8\u5316\u8bbe\u8ba1\u6d41\u7a0b\u3002", "method": "\u4f7f\u7528CPPN-NEAT\uff08\u4e00\u79cd\u7ec4\u5408\u6a21\u5f0f\u751f\u6210\u7f51\u7edc\uff09\u5bf9\u8f6f\u4f53\u673a\u5668\u4eba\u7684\u5f62\u6001\u548c\u63a7\u5236\u5668\u8fdb\u884c\u534f\u540c\u8fdb\u5316\uff0c\u5e76\u6d4b\u8bd5\u4e86\u4e0d\u540c\u7684\u5e73\u5747\u65b9\u6cd5\u6765\u786e\u5b9a\u4e2a\u4f53\u9002\u5e94\u5ea6\u3002", "result": "CPPN-NEAT\u4ea7\u751f\u7684\u5f62\u6001\u4f18\u4e8e\u591a\u76ee\u6807\u4f18\u5316\u65b9\u6cd5\uff0c\u5e76\u4e14\u8ba1\u7b97\u6210\u672c\u66f4\u4f4e\u3002\u534f\u540c\u8fdb\u5316\u4e0e\u4e24\u4e2a\u6700\u4f73\u4e2a\u4f53\u4ee5\u53ca\u52a0\u6743\u5e73\u5747\u9002\u5e94\u5ea6\u7684\u65b9\u6cd5\u6548\u679c\u6700\u4f73\u3002\u6b64\u5916\uff0c\u8fdb\u5316\u51fa\u7684\u5f62\u6001\u5728\u9762\u5bf9\u968f\u673a\u63a7\u5236\u5668\u65f6\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u3002", "conclusion": "CPPN-NEAT\u662f\u4e00\u79cd\u6709\u6548\u7684\u8f6f\u4f53\u673a\u5668\u4eba\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u836f\u7269\u8f93\u9001\u7b49\u5e94\u7528\u4e2d\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5e76\u4e14\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u3002"}}
{"id": "2510.23646", "categories": ["cs.SI", "05C12 (primary), 68R10 (secondary)"], "pdf": "https://arxiv.org/pdf/2510.23646", "abs": "https://arxiv.org/abs/2510.23646", "authors": ["R. Scott Johnson"], "title": "Hamming Graph Metrics: A Multi-Scale Framework for Structural Redundancy and Uniqueness in Graphs", "comment": "57 pages, 3 tables, two appendices,", "summary": "Traditional graph centrality measures effectively quantify node importance\nbut fail to capture the structural uniqueness of multi-scale connectivity\npatterns -- critical for understanding network resilience and function. This\npaper introduces \\emph{Hamming Graph Metrics (HGM)}, a framework that\nrepresents a graph by its exact-$k$ reachability tensor\n$\\mathcal{B}G\\in{0,1}^{N\\times N\\times D}$ with slices\n$(\\mathcal{B}G){:,:,1}=A$ and, for $k\\ge 2$,\n$(\\mathcal{B}G){:,:,k}=\\mathbf{1}!\\left[\\sum{t=1}^{k}\nA^t>0\\right]-\\mathbf{1}!\\left[\\sum_{t=1}^{k-1} A^t>0\\right]$ (shortest-path\ndistance exactly $k$). Guarantees. (i) \\emph{Permutation invariance}:\n$d_{\\mathrm{HGM}}(\\pi(G),\\pi(H))=d_{\\mathrm{HGM}}(G,H)$ for all vertex\nrelabelings $\\pi$; (ii) the \\emph{tensor Hamming distance}\n$d_{\\mathrm{HGM}}(G,H):=|,\\mathcal{B}G-\\mathcal{B}H,|{1}=\\sum{i,j,k}\\mathbf{1}!\\big[(\\mathcal{B}G){ijk}\\neq(\\mathcal{B}H){ijk}\\big]$\nis a \\emph{true metric} on labeled graphs; and (iii) \\emph{Lipschitz stability}\nto edge perturbations with explicit degree-dependent constants (see\nGraph-to-Graph Comparison'' $\\to$ Tensor Hamming metric''; ``Stability to edge\nperturbations''; Appendix A). We develop: (1) \\emph{per-scale spectral\nanalysis} via classical MDS on double-centered Hamming matrices $D^{(k)}$,\nyielding spectral coordinates and explained variances; (2) \\emph{summary\nstatistics} for node-wise and graph-level structural dissimilarity; (3)\n\\emph{graph-to-graph comparison} via the metric above; and (4) \\emph{analytic\nproperties} including extremal characterizations, multi-scale limits, and\nstability bounds.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHamming Graph Metrics (HGM)\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u6790\u56fe\u7ed3\u6784\uff0c\u7279\u522b\u5173\u6ce8\u8282\u70b9\u7684\u53ef\u8fbe\u6027\u6a21\u5f0f\uff0c\u4ee5\u514b\u670d\u4f20\u7edf\u4e2d\u5fc3\u6027\u5ea6\u91cf\u5728\u6355\u6349\u591a\u5c3a\u5ea6\u8fde\u901a\u6027\u6a21\u5f0f\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u4f20\u7edf\u7684\u56fe\u4e2d\u5fc3\u6027\u5ea6\u91cf\u867d\u7136\u80fd\u6709\u6548\u91cf\u5316\u8282\u70b9\u91cd\u8981\u6027\uff0c\u4f46\u5728\u6355\u6349\u591a\u5c3a\u5ea6\u8fde\u901a\u6027\u6a21\u5f0f\u4ee5\u7406\u89e3\u7f51\u7edc\u97e7\u6027\u548c\u529f\u80fd\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002\u672c\u7814\u7a76\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u6846\u67b6\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51faHamming Graph Metrics (HGM)\u6846\u67b6\uff0c\u5c06\u56fe\u8868\u793a\u4e3a\u5176\u7cbe\u786ek\u53ef\u8fbe\u6027\u5f20\u91cf $\\mathcal{B}G$\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u5b9a\u4e49\u4e86\u5f20\u91cf\u6c49\u660e\u8ddd\u79bb $d_{\\mathrm{HGM}}$ \u4f5c\u4e3a\u8861\u91cf\u56fe\u4e4b\u95f4\u7ed3\u6784\u5dee\u5f02\u7684\u5ea6\u91cf\uff0c\u5e76\u63a8\u5bfc\u4e86\u8be5\u5ea6\u91cf\u7684\u6027\u8d28\uff0c\u5305\u62ec\u6392\u5217\u4e0d\u53d8\u6027\u3001\u771f\u6b63\u7684\u5ea6\u91cf\u6027\u8d28\u548c\u5bf9\u8fb9\u6270\u52a8\u7684Lipschitz\u7a33\u5b9a\u6027\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u5f00\u53d1\u4e86\u7528\u4e8e\u8282\u70b9\u548c\u56fe\u7ea7\u522b\u7ed3\u6784\u5dee\u5f02\u7684\u6458\u8981\u7edf\u8ba1\u91cf\uff0c\u4ee5\u53ca\u7528\u4e8e\u56fe\u5230\u56fe\u6bd4\u8f83\u7684\u65b9\u6cd5\u3002", "result": "\u7814\u7a76\u63a8\u5bfc\u4e86Hamming Graph Metrics (HGM)\u6846\u67b6\u7684\u7406\u8bba\u6027\u8d28\uff0c\u5305\u62ec\u6392\u5217\u4e0d\u53d8\u6027\u3001\u5ea6\u91cf\u6027\u8d28\u548cLipschitz\u7a33\u5b9a\u6027\u3002\u540c\u65f6\uff0c\u8fd8\u5f00\u53d1\u4e86\u7528\u4e8e\u5206\u6790\u8282\u70b9\u548c\u56fe\u7ed3\u6784\u5dee\u5f02\u7684\u6458\u8981\u7edf\u8ba1\u91cf\uff0c\u5e76\u8fdb\u884c\u4e86\u56fe\u5230\u56fe\u7684\u6bd4\u8f83\u3002", "conclusion": "Hamming Graph Metrics (HGM)\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u3001\u66f4\u5168\u9762\u7684\u56fe\u7ed3\u6784\u5206\u6790\u65b9\u6cd5\uff0c\u80fd\u591f\u6355\u6349\u591a\u5c3a\u5ea6\u8fde\u901a\u6027\u6a21\u5f0f\uff0c\u5e76\u5177\u6709\u826f\u597d\u7684\u7406\u8bba\u6027\u8d28\u548c\u7a33\u5b9a\u6027\uff0c\u4e3a\u7406\u89e3\u7f51\u7edc\u97e7\u6027\u548c\u529f\u80fd\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002"}}
{"id": "2510.23899", "categories": ["cs.MA", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.23899", "abs": "https://arxiv.org/abs/2510.23899", "authors": ["Maria G. Mendoza", "Addison Kalanther", "Daniel Bostwick", "Emma Stephan", "Chinmay Maheshwari", "Shankar Sastry"], "title": "Coordinated Autonomous Drones for Human-Centered Fire Evacuation in Partially Observable Urban Environments", "comment": "Accepted to IEEE Global Humanitarian Technology Conference (GHTC\n  2025). 8 pages, 4 figures", "summary": "Autonomous drone technology holds significant promise for enhancing search\nand rescue operations during evacuations by guiding humans toward safety and\nsupporting broader emergency response efforts. However, their application in\ndynamic, real-time evacuation support remains limited. Existing models often\noverlook the psychological and emotional complexity of human behavior under\nextreme stress. In real-world fire scenarios, evacuees frequently deviate from\ndesignated safe routes due to panic and uncertainty. To address these\nchallenges, this paper presents a multi-agent coordination framework in which\nautonomous Unmanned Aerial Vehicles (UAVs) assist human evacuees in real-time\nby locating, intercepting, and guiding them to safety under uncertain\nconditions. We model the problem as a Partially Observable Markov Decision\nProcess (POMDP), where two heterogeneous UAV agents, a high-level rescuer (HLR)\nand a low-level rescuer (LLR), coordinate through shared observations and\ncomplementary capabilities. Human behavior is captured using an agent-based\nmodel grounded in empirical psychology, where panic dynamically affects\ndecision-making and movement in response to environmental stimuli. The\nenvironment features stochastic fire spread, unknown evacuee locations, and\nlimited visibility, requiring UAVs to plan over long horizons to search for\nhumans and adapt in real-time. Our framework employs the Proximal Policy\nOptimization (PPO) algorithm with recurrent policies to enable robust\ndecision-making in partially observable settings. Simulation results\ndemonstrate that the UAV team can rapidly locate and intercept evacuees,\nsignificantly reducing the time required for them to reach safety compared to\nscenarios without UAV assistance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u65e0\u4eba\u673a\u534f\u540c\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u758f\u6563\u8fc7\u7a0b\u4e2d\u5b9e\u65f6\u534f\u52a9\u4eba\u7c7b\u64a4\u79bb\uff0c\u4ee5\u5e94\u5bf9\u706b\u707e\u7b49\u7d27\u6025\u60c5\u51b5\u3002", "motivation": "\u73b0\u6709\u7684\u7814\u7a76\u672a\u80fd\u5145\u5206\u8003\u8651\u4eba\u7c7b\u5728\u6781\u7aef\u538b\u529b\u4e0b\u7684\u5fc3\u7406\u548c\u884c\u4e3a\u590d\u6742\u6027\uff0c\u5bfc\u81f4\u65e0\u4eba\u673a\u5728\u5b9e\u9645\u758f\u6563\u652f\u6301\u4e2d\u7684\u5e94\u7528\u53d7\u9650\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u667a\u80fd\u4f53\u534f\u8c03\u6846\u67b6\uff0c\u5176\u4e2d\u5305\u542b\u4e00\u4e2a\u9ad8\u7ea7\u522b\u6551\u63f4\u8005\uff08HLR\uff09\u548c\u4e00\u4e2a\u4f4e\u7ea7\u522b\u6551\u63f4\u8005\uff08LLR\uff09\u7684\u5f02\u6784\u65e0\u4eba\u673a\u4ee3\u7406\u3002\u8be5\u6846\u67b6\u5229\u7528\u90e8\u5206\u53ef\u89c2\u5bdf\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08POMDP\uff09\u5bf9\u95ee\u9898\u8fdb\u884c\u5efa\u6a21\uff0c\u5e76\u7ed3\u5408\u57fa\u4e8e\u7ecf\u9a8c\u5fc3\u7406\u5b66\u7684\u591a\u667a\u80fd\u4f53\u6a21\u578b\u6765\u6a21\u62df\u4eba\u7c7b\u5728\u6050\u614c\u5f71\u54cd\u4e0b\u7684\u51b3\u7b56\u548c\u79fb\u52a8\u3002\u4f7f\u7528\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08PPO\uff09\u7b97\u6cd5\u548c\u5faa\u73af\u7b56\u7565\u6765\u5b9e\u73b0\u90e8\u5206\u53ef\u89c2\u5bdf\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u51b3\u7b56\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65e0\u4eba\u673a\u534f\u540c\u7cfb\u7edf\u80fd\u591f\u5feb\u901f\u5b9a\u4f4d\u5e76\u62e6\u622a\u758f\u6563\u4eba\u5458\uff0c\u4e0e\u65e0\u4eba\u673a\u8f85\u52a9\u573a\u666f\u76f8\u6bd4\uff0c\u663e\u8457\u7f29\u77ed\u4e86\u4eba\u5458\u5230\u8fbe\u5b89\u5168\u533a\u57df\u7684\u65f6\u95f4\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u591a\u65e0\u4eba\u673a\u534f\u540c\u6846\u67b6\u80fd\u591f\u6709\u6548\u89e3\u51b3\u73b0\u6709\u65e0\u4eba\u673a\u5728\u5b9e\u65f6\u758f\u6563\u652f\u6301\u4e2d\u9762\u4e34\u7684\u6311\u6218\uff0c\u901a\u8fc7\u8003\u8651\u4eba\u7c7b\u884c\u4e3a\u7684\u590d\u6742\u6027\u5e76\u91c7\u7528\u5148\u8fdb\u7684\u534f\u8c03\u548c\u5b66\u4e60\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u758f\u6563\u6548\u7387\u3002"}}
{"id": "2510.24098", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2510.24098", "abs": "https://arxiv.org/abs/2510.24098", "authors": ["Tianyu Zuo", "Xueyan Tang", "Bu Sung Lee", "Jianfei Cai"], "title": "On Competitiveness of Dynamic Replication for Distributed Data Access", "comment": "Extended version of a paper that will appear in ICDCN 2026 conference", "summary": "This paper studies an online cost optimization problem for distributed\nstorage and access. The goal is to dynamically create and delete copies of data\nobjects over time at geo-distributed servers to serve access requests and\nminimize the total storage and network cost. We revisit a recent algorithm in\nthe literature and show that it does not have a competitive ratio of $2$ as\nclaimed by constructing a counterexample. We further prove that no\ndeterministic online algorithm can achieve a competitive ratio bounded by $2$\nfor the general cost optimization problem. We develop an online algorithm and\nprove that it achieves a competitive ratio of $\\max\\{2, \\min\\{\\gamma, 3\\}\\}$,\nwhere $\\gamma$ is the max/min storage cost ratio among all servers. Examples\nare given to confirm the tightness of competitive analysis. We also empirically\nevaluate algorithms using real object access traces.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4e00\u4e2a\u5728\u7ebf\u6210\u672c\u4f18\u5316\u95ee\u9898\uff0c\u65e8\u5728\u6700\u5c0f\u5316\u5206\u5e03\u5f0f\u5b58\u50a8\u548c\u8bbf\u95ee\u4e2d\u7684\u603b\u5b58\u50a8\u548c\u7f51\u7edc\u6210\u672c\u3002", "motivation": "\u9488\u5bf9\u5206\u5e03\u5f0f\u5b58\u50a8\u548c\u8bbf\u95ee\u4e2d\u7684\u5728\u7ebf\u6210\u672c\u4f18\u5316\u95ee\u9898\uff0c\u7814\u7a76\u5982\u4f55\u52a8\u6001\u7ba1\u7406\u6570\u636e\u526f\u672c\u4ee5\u6700\u5c0f\u5316\u6210\u672c\u3002", "method": "\u9996\u5148\uff0c\u901a\u8fc7\u6784\u9020\u53cd\u4f8b\uff0c\u8bc1\u660e\u4e86\u6587\u732e\u4e2d\u5df2\u6709\u7b97\u6cd5\u672a\u80fd\u8fbe\u5230\u58f0\u79f0\u7684\u7ade\u4e89\u6bd42\uff0c\u5e76\u8bc1\u660e\u4e86\u6ca1\u6709\u786e\u5b9a\u6027\u5728\u7ebf\u7b97\u6cd5\u80fd\u8fbe\u52302\u7684\u7ade\u4e89\u6bd4\u3002\u7136\u540e\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5728\u7ebf\u7b97\u6cd5\uff0c\u5e76\u8bc1\u660e\u5176\u7ade\u4e89\u6bd4\u4e3amax{2, min{\u03b3, 3}}\uff0c\u5176\u4e2d\u03b3\u662f\u670d\u52a1\u5668\u95f4\u7684\u5b58\u50a8\u6210\u672c\u6bd4\u3002\u6700\u540e\uff0c\u901a\u8fc7\u5b9e\u4f8b\u548c\u5b9e\u9645\u8bbf\u95ee\u8f68\u8ff9\u8fdb\u884c\u4e86\u7ecf\u9a8c\u8bc4\u4f30\u3002", "result": "1. \u53d1\u73b0\u5e76\u6784\u9020\u4e86\u73b0\u6709\u7b97\u6cd5\u7684}(\text{counterexample})\uff0c\u8bc1\u660e\u4e86\u5176\u7ade\u4e89\u6bd4\u5e76\u975e2\u3002 2. \u8bc1\u660e\u4e86\u4e0d\u5b58\u5728\u7ade\u4e89\u6bd4\u5c0f\u4e8e2\u7684\u786e\u5b9a\u6027\u5728\u7ebf\u7b97\u6cd5\u3002 3. \u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5728\u7ebf\u7b97\u6cd5\uff0c\u5176\u7ade\u4e89\u6bd4\u4e3amax{2, min{\u03b3, 3}}\u3002 4. \u901a\u8fc7\u5b9e\u4f8b\u5206\u6790\u548c\u5b9e\u9645\u6570\u636e\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u5bf9\u4e8e\u5206\u5e03\u5f0f\u5b58\u50a8\u548c\u8bbf\u95ee\u7684\u5728\u7ebf\u6210\u672c\u4f18\u5316\u95ee\u9898\uff0c\u4e0d\u5b58\u5728\u7ade\u4e89\u6bd4\u5c0f\u4e8e2\u7684\u786e\u5b9a\u6027\u7b97\u6cd5\u3002\u672c\u6587\u63d0\u51fa\u7684\u65b0\u7b97\u6cd5\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5177\u6709\u826f\u597d\u7684\u6027\u80fd\uff0c\u5176\u7ade\u4e89\u6bd4\u4e0e\u670d\u52a1\u5668\u6210\u672c\u6bd4\u6709\u5173\u3002"}}
{"id": "2510.24173", "categories": ["cs.LG", "cs.NA", "math.DS", "math.NA", "physics.flu-dyn"], "pdf": "https://arxiv.org/pdf/2510.24173", "abs": "https://arxiv.org/abs/2510.24173", "authors": ["Yiheng Du", "Aditi S. Krishnapriyan"], "title": "EddyFormer: Accelerated Neural Simulations of Three-Dimensional Turbulence at Scale", "comment": "NeurIPS 2025", "summary": "Computationally resolving turbulence remains a central challenge in fluid\ndynamics due to its multi-scale interactions. Fully resolving large-scale\nturbulence through direct numerical simulation (DNS) is computationally\nprohibitive, motivating data-driven machine learning alternatives. In this\nwork, we propose EddyFormer, a Transformer-based spectral-element (SEM)\narchitecture for large-scale turbulence simulation that combines the accuracy\nof spectral methods with the scalability of the attention mechanism. We\nintroduce an SEM tokenization that decomposes the flow into grid-scale and\nsubgrid-scale components, enabling capture of both local and global features.\nWe create a new three-dimensional isotropic turbulence dataset and train\nEddyFormer to achieves DNS-level accuracy at 256^3 resolution, providing a 30x\nspeedup over DNS. When applied to unseen domains up to 4x larger than in\ntraining, EddyFormer preserves accuracy on physics-invariant metrics-energy\nspectra, correlation functions, and structure functions-showing domain\ngeneralization. On The Well benchmark suite of diverse turbulent flows,\nEddyFormer resolves cases where prior ML models fail to converge, accurately\nreproducing complex dynamics across a wide range of physical conditions.", "AI": {"tldr": "EddyFormer\u662f\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u8c31\u5143\uff08SEM\uff09\u65b9\u6cd5\uff0c\u53ef\u5728\u5927\u89c4\u6a21\u6e4d\u6d41\u6a21\u62df\u4e2d\u5b9e\u73b0\u4e0e\u76f4\u63a5\u6570\u503c\u6a21\u62df\uff08DNS\uff09\u540c\u7b49\u7cbe\u5ea6\uff0c\u540c\u65f6\u901f\u5ea6\u63d0\u9ad830\u500d\uff0c\u5e76\u80fd\u5728\u672a\u89c1\u8fc7\u7684\u66f4\u5927\u5c3a\u5ea6\u4e0a\u4fdd\u6301\u51c6\u786e\u6027\u3002", "motivation": "\u7531\u4e8e\u6e4d\u6d41\u7684\u591a\u5c3a\u5ea6\u76f8\u4e92\u4f5c\u7528\uff0c\u5728\u8ba1\u7b97\u4e0a\u89e3\u51b3\u6e4d\u6d41\u662f\u6d41\u4f53\u52a8\u529b\u5b66\u4e2d\u7684\u4e00\u4e2a\u6838\u5fc3\u6311\u6218\u3002\u901a\u8fc7\u76f4\u63a5\u6570\u503c\u6a21\u62df\uff08DNS\uff09\u5b8c\u5168\u89e3\u6790\u5927\u5c3a\u5ea6\u6e4d\u6d41\u5728\u8ba1\u7b97\u4e0a\u662f\u96be\u4ee5\u627f\u53d7\u7684\uff0c\u8fd9\u4fc3\u4f7f\u4e86\u6570\u636e\u9a71\u52a8\u7684\u673a\u5668\u5b66\u4e60\u66ff\u4ee3\u65b9\u6848\u7684\u53d1\u5c55\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEddyFormer\u7684Transformer-based spectral-element\uff08SEM\uff09\u67b6\u6784\uff0c\u5b83\u7ed3\u5408\u4e86\u8c31\u65b9\u6cd5\u7684\u7cbe\u5ea6\u548c\u6ce8\u610f\u529b\u673a\u5236\u7684\u53ef\u6269\u5c55\u6027\u3002\u901a\u8fc7SEM\u5206\u8bcd\u5c06\u6d41\u4f53\u5206\u89e3\u4e3a\u7f51\u683c\u5c3a\u5ea6\u548c\u5b50\u7f51\u683c\u5c3a\u5ea6\u5206\u91cf\uff0c\u4ee5\u6355\u6349\u5c40\u90e8\u548c\u5168\u5c40\u7279\u5f81\u3002", "result": "EddyFormer\u5728256^3\u5206\u8fa8\u7387\u4e0b\u5b9e\u73b0\u4e86DNS\u7ea7\u522b\u7684\u51c6\u786e\u6027\uff0c\u6bd4DNS\u5feb\u4e8630\u500d\u3002\u5728\u9ad8\u8fbe\u8bad\u7ec3\u6570\u636e4\u500d\u7684\u672a\u89c1\u8fc7\u7684\u533a\u57df\u4e0a\uff0cEddyFormer\u5728\u7269\u7406\u4e0d\u53d8\u6307\u6807\uff08\u80fd\u91cf\u8c31\u3001\u76f8\u5173\u51fd\u6570\u548c\u7ed3\u6784\u51fd\u6570\uff09\u4e0a\u4fdd\u6301\u4e86\u51c6\u786e\u6027\uff0c\u663e\u793a\u4e86\u5176\u6cdb\u5316\u80fd\u529b\u3002\u5728\u591a\u6837\u5316\u7684\u6e4d\u6d41\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cEddyFormer\u80fd\u591f\u89e3\u51b3\u5148\u524d\u673a\u5668\u5b66\u4e60\u6a21\u578b\u65e0\u6cd5\u6536\u655b\u7684\u95ee\u9898\uff0c\u5e76\u51c6\u786e\u518d\u73b0\u4e86\u5404\u79cd\u7269\u7406\u6761\u4ef6\u4e0b\u7684\u590d\u6742\u52a8\u529b\u5b66\u3002", "conclusion": "EddyFormer\u5728\u5927\u89c4\u6a21\u6e4d\u6d41\u6a21\u62df\u4e2d\u5c55\u73b0\u4e86\u5353\u8d8a\u7684\u6027\u80fd\uff0c\u80fd\u591f\u4ee5\u66f4\u9ad8\u7684\u6548\u7387\u5b9e\u73b0\u4e0eDNS\u76f8\u5f53\u7684\u7cbe\u5ea6\uff0c\u5e76\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u80fd\u591f\u5904\u7406\u6bd4\u8bad\u7ec3\u6570\u636e\u66f4\u590d\u6742\u7684\u6d41\u52a8\u60c5\u51b5\u3002"}}
{"id": "2510.23766", "categories": ["cs.CL", "68T05", "I.2.6; I.2.7"], "pdf": "https://arxiv.org/pdf/2510.23766", "abs": "https://arxiv.org/abs/2510.23766", "authors": ["Ramshankar Bhuvaneswaran", "Handan Liu"], "title": "BitSkip: An Empirical Analysis of Quantization and Early Exit Composition", "comment": "Submitted to JMLR", "summary": "The pursuit of efficient Large Language Models (LLMs) has led to increasingly\ncomplex techniques like extreme quantization and dynamic routing. While\nindividual benefits of these methods are well-documented, their compositional\neffects remain poorly understood. This paper introduces BitSkip, a hybrid\narchitectural framework for systematically exploring these interactions.\nCounter-intuitively, our findings reveal that a simple 8-bit quantized model\nwithout Hadamard transform (BitSkip-V1) not only outperforms its more complex\n4-bit and Hadamard-enhanced counterparts but also competes the full-precision\nbaseline in quality (perplexity of 1.13 vs 1.19) . The introduction of Hadamard\ntransforms, even at 8-bit precision, catastrophically degraded performance by\nover 37,000%, tracing fundamental training instability. Our BitSkip-V1 recipe\ndemonstrates superior early-exit characteristics, with layer 18 providing\noptimal 32.5% speed gain for minimal 4% quality loss.", "AI": {"tldr": "BitSkip\u6846\u67b6\u901a\u8fc7\u7cfb\u7edf\u6027\u5730\u63a2\u7d22\u91cf\u5316\u548c\u52a8\u6001\u8def\u7531\u7b49\u6280\u672f\u7ec4\u5408\u6548\u5e94\uff0c\u53d1\u73b0\u7b80\u5355\u76848\u4f4d\u91cf\u5316\u6a21\u578b\uff08BitSkip-V1\uff09\u4f18\u4e8e\u66f4\u590d\u6742\u76844\u4f4d\u548cHadamard\u53d8\u6362\u6a21\u578b\uff0c\u5e76\u4e14\u5728\u65e9\u671f\u9000\u51fa\u7b56\u7565\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u5f53\u524d\u7814\u7a76\u7f3a\u4e4f\u5bf9LLM\u6548\u7387\u6280\u672f\uff08\u5982\u6781\u7aef\u91cf\u5316\u3001\u52a8\u6001\u8def\u7531\uff09\u7ec4\u5408\u6548\u5e94\u7684\u7406\u89e3\u3002", "method": "\u63d0\u51faBitSkip\u6df7\u5408\u67b6\u6784\u6846\u67b6\uff0c\u7528\u4e8e\u7cfb\u7edf\u6027\u5730\u63a2\u7d22\u8fd9\u4e9b\u6280\u672f\u7ec4\u5408\u3002", "result": "\u53d1\u73b0\u7b80\u5355\u76848\u4f4d\u91cf\u5316\u6a21\u578b\uff08BitSkip-V1\uff09\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u66f4\u590d\u6742\u76844\u4f4d\u548cHadamard\u53d8\u6362\u6a21\u578b\uff0c\u4e14\u63a5\u8fd1\u5168\u7cbe\u5ea6\u57fa\u7ebf\u3002Hadamard\u53d8\u6362\u5f15\u5165\u4e8637000%\u4ee5\u4e0a\u7684\u6027\u80fd\u4e0b\u964d\uff0c\u8868\u660e\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u3002BitSkip-V1\u5728\u65e9\u671f\u9000\u51fa\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u52bf\uff0c\u7b2c18\u5c42\u5728\u4ec5\u635f\u59314%\u8d28\u91cf\u7684\u60c5\u51b5\u4e0b\u63d0\u4f9b\u4e8632.5%\u7684\u901f\u5ea6\u63d0\u5347\u3002", "conclusion": "\u7b80\u5355\u76848\u4f4d\u91cf\u5316\u6a21\u578b\uff08BitSkip-V1\uff09\u5177\u6709\u7ade\u4e89\u529b\uff0c\u5e76\u4e14\u901a\u8fc7\u65e9\u671f\u9000\u51fa\u673a\u5236\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63d0\u9ad8\u6548\u7387\u3002Hadamard\u53d8\u6362\u5728\u6240\u7814\u7a76\u7684\u914d\u7f6e\u4e2d\u5177\u6709\u7834\u574f\u6027\u3002"}}
{"id": "2510.23860", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.23860", "abs": "https://arxiv.org/abs/2510.23860", "authors": ["Hyung Chan Cho", "Go-Eum Cha", "Yanfu Liu", "Sooyeon Jeong"], "title": "Motivating Students' Self-study with Goal Reminder and Emotional Support", "comment": "RO-MAN 2025 accepted paper", "summary": "While the efficacy of social robots in supporting people in learning tasks\nhas been extensively investigated, their potential impact in assisting students\nin self-studying contexts has not been investigated much. This study explores\nhow a social robot can act as a peer study companion for college students\nduring self-study tasks by delivering task-oriented goal reminder and positive\nemotional support. We conducted an exploratory Wizard-of-Oz study to explore\nhow these robotic support behaviors impacted students' perceived focus,\nproductivity, and engagement in comparison to a robot that only provided\nphysical presence (control). Our study results suggest that participants in the\ngoal reminder and the emotional support conditions reported greater ease of\nuse, with the goal reminder condition additionally showing a higher willingness\nto use the robot in future study sessions. Participants' satisfaction with the\nrobot was correlated with their perception of the robot as a social other, and\nthis perception was found to be a predictor for their level of goal achievement\nin the self-study task. These findings highlight the potential of socially\nassistive robots to support self-study through both functional and emotional\nengagement.", "AI": {"tldr": "While social robots are effective in learning tasks, their use in self-study is under-explored. This study investigated a social robot's impact on college students' self-study by providing goal reminders and emotional support. An exploratory Wizard-of-Oz study compared this to a robot with only physical presence. Results showed participants in the goal reminder and emotional support conditions reported greater ease of use. The goal reminder condition also showed higher willingness for future use. Satisfaction correlated with perceiving the robot as a social entity, which predicted goal achievement. The findings suggest social robots can support self-study through functional and emotional engagement.", "motivation": "The efficacy of social robots in learning tasks is well-documented, but their potential in assisting students with self-study remains largely unexplored. This study aims to investigate the impact of a social robot acting as a peer study companion, providing task-oriented goal reminders and positive emotional support, to aid college students in self-study contexts.", "method": "An exploratory Wizard-of-Oz study was conducted to investigate the impact of a social robot's support behaviors (task-oriented goal reminders and positive emotional support) on students' perceived focus, productivity, and engagement. This was compared to a control condition where the robot only provided physical presence.", "result": "Participants in the goal reminder and emotional support conditions reported greater ease of use. The goal reminder condition showed a higher willingness to use the robot in future study sessions. Student satisfaction with the robot was correlated with their perception of the robot as a social other, which in turn predicted their level of goal achievement in the self-study task.", "conclusion": "Socially assistive robots have the potential to effectively support self-study by engaging students both functionally (through goal reminders) and emotionally. Perceiving the robot as a social entity is linked to both satisfaction and successful goal achievement, highlighting the importance of social interaction in robotic assistance for self-study."}}
{"id": "2510.23993", "categories": ["cs.DC", "cs.CE"], "pdf": "https://arxiv.org/pdf/2510.23993", "abs": "https://arxiv.org/abs/2510.23993", "authors": ["Anthony Carreon", "Jagmohan Singh", "Shivank Sharma", "Shuzhi Zhang", "Venkat Raman"], "title": "A GPU-based Compressible Combustion Solver for Applications Exhibiting Disparate Space and Time Scales", "comment": "32 pages, 12 figures", "summary": "High-speed chemically active flows present significant computational\nchallenges due to their disparate space and time scales, where stiff chemistry\noften dominates simulation time. While modern supercomputing scientific codes\nachieve exascale performance by leveraging graphics processing units (GPUs),\nexisting GPU-based compressible combustion solvers face critical limitations in\nmemory management, load balancing, and handling the highly localized nature of\nchemical reactions. To this end, we present a high-performance compressible\nreacting flow solver built on the AMReX framework and optimized for multi-GPU\nsettings. Our approach addresses three GPU performance bottlenecks: memory\naccess patterns through column-major storage optimization, computational\nworkload variability via a bulk-sparse integration strategy for chemical\nkinetics, and multi-GPU load distribution for adaptive mesh refinement\napplications. The solver adapts existing matrix-based chemical kinetics\nformulations to multigrid contexts. Using representative combustion\napplications including hydrogen-air detonations and jet in supersonic crossflow\nconfigurations, we demonstrate $2-5\\times$ performance improvements over\ninitial GPU implementations with near-ideal weak scaling across $1-96$ NVIDIA\nH100 GPUs. Roofline analysis reveals substantial improvements in arithmetic\nintensity for both convection ($\\sim 10 \\times$) and chemistry ($\\sim 4\n\\times$) routines, confirming efficient utilization of GPU memory bandwidth and\ncomputational resources.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u591aGPU\u73af\u5883\u4f18\u5316\u7684\u3001\u9ad8\u6027\u80fd\u7684\u53ef\u538b\u7f29\u53cd\u5e94\u6d41\u6c42\u89e3\u5668\uff0c\u89e3\u51b3\u4e86\u73b0\u6709GPU\u71c3\u70e7\u6c42\u89e3\u5668\u5728\u5185\u5b58\u7ba1\u7406\u3001\u8d1f\u8f7d\u5747\u8861\u548c\u5316\u5b66\u53cd\u5e94\u5c40\u90e8\u6027\u5904\u7406\u65b9\u9762\u7684\u74f6\u9888\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u548c\u9ad8\u6548\u7684GPU\u8d44\u6e90\u5229\u7528\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eGPU\u7684\u53ef\u538b\u7f29\u71c3\u70e7\u6c42\u89e3\u5668\u5728\u5185\u5b58\u7ba1\u7406\u3001\u8d1f\u8f7d\u5747\u8861\u548c\u5904\u7406\u9ad8\u5ea6\u5c40\u90e8\u5316\u7684\u5316\u5b66\u53cd\u5e94\u65b9\u9762\u5b58\u5728\u5173\u952e\u9650\u5236\uff0c\u5bfc\u81f4\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\uff0c\u800c\u9ad8\u8d85\u58f0\u901f\u5316\u5b66\u6d3b\u6027\u6d41\u8ba1\u7b97\u53c8\u9762\u4e34\u7740\u5de8\u5927\u7684\u8ba1\u7b97\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eAMReX\u6846\u67b6\u5e76\u9488\u5bf9\u591aGPU\u73af\u5883\u8fdb\u884c\u4f18\u5316\u7684\u53ef\u538b\u7f29\u53cd\u5e94\u6d41\u6c42\u89e3\u5668\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u4ee5\u4e0b\u65b9\u5f0f\u89e3\u51b3\u4e86\u4e09\u4e2aGPU\u6027\u80fd\u74f6\u9888\uff1a1. \u4f18\u5316\u5217\u4e3b\u5b58\u50a8\u4ee5\u6539\u5584\u5185\u5b58\u8bbf\u95ee\u6a21\u5f0f\uff1b2. \u91c7\u7528\u6279\u91cf\u7a00\u758f\u79ef\u5206\u7b56\u7565\u5904\u7406\u5316\u5b66\u52a8\u529b\u5b66\u4ee5\u5e94\u5bf9\u8ba1\u7b97\u5de5\u4f5c\u8d1f\u8f7d\u53d8\u5316\uff1b3. \u9488\u5bf9\u81ea\u9002\u5e94\u7f51\u683c\u52a0\u5bc6\u5e94\u7528\u4f18\u5316\u591aGPU\u8d1f\u8f7d\u5206\u914d\u3002\u8be5\u6c42\u89e3\u5668\u8fd8\u5c06\u73b0\u6709\u7684\u57fa\u4e8e\u77e9\u9635\u7684\u5316\u5b66\u52a8\u529b\u5b66\u8868\u8ff0\u9002\u5e94\u4e8e\u591a\u91cd\u7f51\u683c\u4e0a\u4e0b\u6587\u3002", "result": "\u5728\u4f7f\u7528\u6c22\u6c14-\u7a7a\u6c14\u7206\u8f70\u548c\u8d85\u97f3\u901f\u6a2a\u6d41\u4e2d\u7684\u5c04\u6d41\u7b49\u4ee3\u8868\u6027\u71c3\u70e7\u5e94\u7528\u4e2d\uff0c\u6027\u80fd\u6bd4\u521d\u59cbGPU\u5b9e\u73b0\u63d0\u9ad8\u4e862-5\u500d\uff0c\u5e76\u4e14\u57281-96\u4e2aNVIDIA H100 GPU\u4e0a\u5b9e\u73b0\u4e86\u8fd1\u4e4e\u7406\u60f3\u7684\u5f31\u6269\u5c55\u6027\u3002Roofline\u5206\u6790\u663e\u793a\uff0c\u5bf9\u6d41\uff08\u7ea610\u500d\uff09\u548c\u5316\u5b66\uff08\u7ea64\u500d\uff09\u4f8b\u7a0b\u7684\u7b97\u672f\u5f3a\u5ea6\u5747\u5f97\u5230\u663e\u8457\u6539\u5584\uff0c\u8bc1\u5b9e\u4e86GPU\u5185\u5b58\u5e26\u5bbd\u548c\u8ba1\u7b97\u8d44\u6e90\u7684\u6709\u6548\u5229\u7528\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u591aGPU\u53ef\u538b\u7f29\u53cd\u5e94\u6d41\u6c42\u89e3\u5668\u5728\u8ba1\u7b97\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u6b65\uff0c\u4e3a\u89e3\u51b3\u9ad8\u8d85\u58f0\u901f\u5316\u5b66\u6d3b\u6027\u6d41\u7684\u8ba1\u7b97\u6311\u6218\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2510.23801", "categories": ["cond-mat.mtrl-sci", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2510.23801", "abs": "https://arxiv.org/abs/2510.23801", "authors": ["M. D. Medina", "H. I. Giron", "K. Paucar", "A. Talledo", "B. R. Pujada"], "title": "Stress in chromium thin films deposited by DC magnetron sputtering on grounded cupper and stainless-steel substrate holders", "comment": "8 pages, 4 figures, Meeting of Physics (Peru)", "summary": "Chromium thin films deposited on silicon substrates by DC magnetron\nsputtering were systematically investigated as a function of film thickness,\nusing a DC power of 50 W and a post-deposition annealing temperature of 200 C.\nTwo types of grounded substrate holders, copper and stainless steel, were\nemployed to assess substrate-dependent effects. The intrinsic stress,\ndetermined by the wafer curvature method, decreases with increasing film\nthickness but increases with the annealing temperature. It is observed that for\nthinner as-deposited chromium films, the stress showed a pronounced\nirreversible increase when measured immediately after deposition and after\nseveral days of aging. Films deposited on copper holders consistently exhibited\nhigher stress values than those grown on stainless steel holders. These\nobservations suggest that the intrinsic stress in as-deposited films is linked\nto the growth mechanism, while the stress increase after annealing may be\nrelated to thermally active diffusion and structural relaxation. The higher\nstress in films grown on copper substrate holder can likely be associated with\nenhanced ion bombardment due to the higher electrical conductivity of copper.", "AI": {"tldr": "\u94ec\u8584\u819c\u7684\u5e94\u529b\u968f\u539a\u5ea6\u548c\u9000\u706b\u6e29\u5ea6\u53d8\u5316\uff0c\u5e76\u53d7\u57fa\u677f\u7c7b\u578b\u5f71\u54cd\u3002", "motivation": "\u7814\u7a76\u76f4\u6d41\u78c1\u63a7\u6e85\u5c04\u6c89\u79ef\u7684\u94ec\u8584\u819c\u7684\u5185\u5e94\u529b\uff0c\u4ee5\u53ca\u8584\u819c\u539a\u5ea6\u3001\u9000\u706b\u6e29\u5ea6\u548c\u57fa\u677f\u7c7b\u578b\u5bf9\u5176\u7684\u5f71\u54cd\u3002", "method": "\u91c7\u7528\u76f4\u6d41\u78c1\u63a7\u6e85\u5c04\u6c89\u79ef\u94ec\u8584\u819c\uff0c\u901a\u8fc7\u76f4\u6d41\u529f\u738750W\u548c200\u00b0C\u7684\u9000\u706b\u6e29\u5ea6\u8fdb\u884c\u5b9e\u9a8c\u3002\u4f7f\u7528\u94dc\u548c\u4e0d\u9508\u94a2\u57fa\u677f\u652f\u67b6\uff0c\u5e76\u91c7\u7528\u6676\u5706\u66f2\u7387\u6cd5\u6d4b\u91cf\u5185\u5e94\u529b\u3002", "result": "\u5185\u5e94\u529b\u968f\u8584\u819c\u539a\u5ea6\u589e\u52a0\u800c\u51cf\u5c0f\uff0c\u968f\u9000\u706b\u6e29\u5ea6\u5347\u9ad8\u800c\u589e\u5927\u3002\u8584\u819c\u539a\u5ea6\u8f83\u5c0f\u65f6\uff0c\u5e94\u529b\u5728\u6d4b\u91cf\u540e\u7acb\u5373\u548c\u51e0\u5929\u540e\u5747\u51fa\u73b0\u660e\u663e\u4e0d\u53ef\u9006\u589e\u52a0\u3002\u5728\u94dc\u652f\u67b6\u4e0a\u751f\u957f\u7684\u8584\u819c\u5e94\u529b\u503c\u9ad8\u4e8e\u5728\u4e0d\u9508\u94a2\u652f\u67b6\u4e0a\u751f\u957f\u7684\u8584\u819c\u3002\u9000\u706b\u540e\u7684\u5e94\u529b\u589e\u52a0\u53ef\u80fd\u4e0e\u70ed\u6fc0\u6d3b\u6269\u6563\u548c\u7ed3\u6784\u5f1b\u8c6b\u6709\u5173\u3002\u94dc\u57fa\u677f\u652f\u67b6\u4e0a\u751f\u957f\u7684\u8584\u819c\u5e94\u529b\u8f83\u9ad8\u53ef\u80fd\u4e0e\u94dc\u7684\u8f83\u9ad8\u5bfc\u7535\u6027\u589e\u5f3a\u4e86\u79bb\u5b50\u8f70\u51fb\u6709\u5173\u3002", "conclusion": "\u5185\u5e94\u529b\u4e0e\u751f\u957f\u673a\u5236\u3001\u70ed\u6fc0\u6d3b\u6269\u6563\u3001\u7ed3\u6784\u5f1b\u8c6b\u548c\u79bb\u5b50\u8f70\u51fb\u6709\u5173\u3002"}}
{"id": "2510.23961", "categories": ["cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2510.23961", "abs": "https://arxiv.org/abs/2510.23961", "authors": ["Haining Pan", "Jacob R. Taylor", "Jay D. Sau", "Sankar Das Sarma"], "title": "Ballistic transport in 1D Rashba systems in the context of Majorana nanowires", "comment": "13 pages, 5 figures", "summary": "Recent work on Majorana-bound states in semiconductor-superconductor hybrid\nstructures has elucidated the key role of unintentional (and unknown) disorder\n(producing low-energy Andreev-bound states) in the system, which is detrimental\nto the emergence of Majorana-carrying topological superconductivity\nartificially engineered through the combination of superconductivity, Zeeman\nspin splitting, and Rashba spin-orbit coupling. In particular, the disorder\nmust be smaller than the superconducting gap for the appearance of Majorana\nmodes, but the disorder-induced appearance of subgap Andreev-bound states\nsuppresses the Majorana modes. We theoretically investigate, as a function of\ndisorder, the normal state ballistic transport properties of nanowires with and\nwithout superconductors in order to provide guidance on how to experimentally\nestimate the level of disorder. Experimentally, the superconductivity is\nsuppressed simply by rotating the magnetic field appropriately, so both physics\ncan be studied in the same set-up. In particular, the presence of spin-orbit\ncoupling and Zeeman splitting produces a helical gap in the 1D electronic band\nstructure, which should have clear signatures in ballistic transport unless\nthese signatures are suppressed by disorder and/or Fabry-P\\'erot resonances\nassociated with the finite wire sizes. Our work provides a benchmarking of when\nand what signatures of the putative helical gap (which is essential for the\nemergence of Majorana modes by leading to a single Fermi surface) could\nmanifest in realistic nanowires.", "AI": {"tldr": " Majorana \u7ed3\u5408\u7684\u62d3\u6251\u8d85\u5bfc\u7535\u6027\u9700\u8981\u4f4e\u4e8e\u8d85\u5bfc\u9699\u7684\u65e0\u5e8f\u6c34\u5e73\uff0c\u4f46\u65e0\u5e8f\u5f15\u8d77\u7684\u4e9a\u5e26\u9699\u5b89\u5fb7\u70c8\u592b\u675f\u7f1a\u6001\u4f1a\u6291\u5236 Majorana \u6a21\u5f0f\u3002\u672c\u7814\u7a76\u901a\u8fc7\u7406\u8bba\u7814\u7a76\u65e0\u5e8f\u6c34\u5e73\u5bf9\u6b63\u5e38\u72b6\u6001\u5f39\u9053\u8f93\u8fd0\u7279\u6027\u7684\u5f71\u54cd\uff0c\u4e3a\u5b9e\u9a8c\u8bc4\u4f30\u65e0\u5e8f\u6c34\u5e73\u63d0\u4f9b\u6307\u5bfc\u3002 ", "motivation": " Majorana \u96f6\u6a21\u7684\u51fa\u73b0\u5bf9\u65e0\u5e8f\u6c34\u5e73\u975e\u5e38\u654f\u611f\uff0c\u65e0\u5e8f\u5bfc\u81f4\u7684\u4e9a\u5e26\u9699\u5b89\u5fb7\u70c8\u592b\u675f\u7f1a\u6001\u4f1a\u6291\u5236 Majorana \u96f6\u6a21\u3002\u56e0\u6b64\uff0c\u9700\u8981\u8bc4\u4f30\u548c\u63a7\u5236\u65e0\u5e8f\u6c34\u5e73\u4ee5\u5b9e\u73b0 Majorana \u96f6\u6a21\u3002 ", "method": " \u7406\u8bba\u7814\u7a76\u4e86\u6709\u8d85\u5bfc\u4f53\u548c\u65e0\u8d85\u5bfc\u4f53\u7684\u7eb3\u7c73\u7ebf\u5728\u4e0d\u540c\u65e0\u5e8f\u6c34\u5e73\u4e0b\u7684\u6b63\u5e38\u72b6\u6001\u5f39\u9053\u8f93\u8fd0\u7279\u6027\u3002 ", "result": " \u786e\u5b9a\u4e86\u65e0\u5e8f\u6c34\u5e73\u5bf9\u7eb3\u7c73\u7ebf\u5f39\u9053\u8f93\u8fd0\u7279\u6027\u7684\u5f71\u54cd\uff0c\u5e76\u8bc6\u522b\u4e86\u53ef\u80fd\u51fa\u73b0\u7684\u87ba\u65cb\u95f4\u9699\u7279\u5f81\u3002\u7814\u7a76\u7ed3\u679c\u4e3a\u5b9e\u9a8c\u63d0\u4f9b\u4e86\u6307\u5bfc\uff0c\u4ee5\u533a\u5206\u548c\u91cf\u5316\u65e0\u5e8f\u548c\u6cd5\u5e03\u91cc-\u73c0\u7f57\u5171\u632f\u7684\u5f71\u54cd\u3002 ", "conclusion": " \u6211\u4eec\u7684\u5de5\u4f5c\u5bf9\u5b9e\u9645\u7eb3\u7c73\u7ebf\u4e2d\u6f5c\u5728\u7684\u87ba\u65cb\u95f4\u9699\u7684\u7279\u5f81\u4f55\u65f6\u4ee5\u53ca\u4f55\u79cd\u7a0b\u5ea6\u4e0a\u51fa\u73b0\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u87ba\u65cb\u95f4\u9699\u5bf9\u4e8e\u901a\u8fc7\u5b9e\u73b0\u5355\u4e2a\u8d39\u7c73\u9762\u800c\u51fa\u73b0\u7684 Majorana \u6a21\u5f0f\u81f3\u5173\u91cd\u8981\u3002 "}}
{"id": "2510.24486", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2510.24486", "abs": "https://arxiv.org/abs/2510.24486", "authors": ["Tinsae G. Dulecha", "Leonardo Righetto", "Ruggero Pintus", "Enrico Gobbetti", "Andrea Giachetti"], "title": "Fast and accurate neural reflectance transformation imaging through knowledge distillation", "comment": "18 pages", "summary": "Reflectance Transformation Imaging (RTI) is very popular for its ability to\nvisually analyze surfaces by enhancing surface details through interactive\nrelighting, starting from only a few tens of photographs taken with a fixed\ncamera and variable illumination. Traditional methods like Polynomial Texture\nMaps (PTM) and Hemispherical Harmonics (HSH) are compact and fast, but struggle\nto accurately capture complex reflectance fields using few per-pixel\ncoefficients and fixed bases, leading to artifacts, especially in highly\nreflective or shadowed areas. The NeuralRTI approach, which exploits a neural\nautoencoder to learn a compact function that better approximates the local\nreflectance as a function of light directions, has been shown to produce\nsuperior quality at comparable storage cost. However, as it performs\ninteractive relighting with custom decoder networks with many parameters, the\nrendering step is computationally expensive and not feasible at full resolution\nfor large images on limited hardware. Earlier attempts to reduce costs by\ndirectly training smaller networks have failed to produce valid results. For\nthis reason, we propose to reduce its computational cost through a novel\nsolution based on Knowledge Distillation (DisK-NeuralRTI). ...", "AI": {"tldr": "NeuralRTI\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u6765\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u4ee5\u89e3\u51b3\u5176\u6e32\u67d3\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u7684RTI\u65b9\u6cd5\uff08\u5982PTM\u548cHSH\uff09\u5728\u5904\u7406\u590d\u6742\u53cd\u5c04\u573a\u65f6\u5b58\u5728\u4f2a\u5f71\uff0c\u800cNeuralRTI\u867d\u7136\u8d28\u91cf\u66f4\u9ad8\uff0c\u4f46\u6e32\u67d3\u6210\u672c\u9ad8\uff0c\u4e0d\u9002\u7528\u4e8e\u786c\u4ef6\u53d7\u9650\u7684\u5927\u578b\u56fe\u50cf\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u77e5\u8bc6\u84b8\u998f\u7684\u65b0\u65b9\u6cd5\uff08DisK-NeuralRTI\uff09\uff0c\u4ee5\u964d\u4f4eNeuralRTI\u7684\u8ba1\u7b97\u6210\u672c\u3002", "result": "\uff08\u8bba\u6587\u6458\u8981\u672a\u63d0\u4f9b\u5177\u4f53\u7ed3\u679c\uff09", "conclusion": "\uff08\u8bba\u6587\u6458\u8981\u672a\u63d0\u4f9b\u5177\u4f53\u7ed3\u8bba\uff09"}}
{"id": "2510.23785", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23785", "abs": "https://arxiv.org/abs/2510.23785", "authors": ["Md Tanvir Hossain", "Akif Islam", "Mohd Ruhul Ameen"], "title": "CountFormer: A Transformer Framework for Learning Visual Repetition and Structure in Class-Agnostic Object Counting", "comment": "6 pages, 2 tables, 6 figures. Submitted to IEEE 5th International\n  Conference on Electrical, Computer and Telecommunication Engineering (ICECTE\n  2025)", "summary": "Humans can effortlessly count diverse objects by perceiving visual repetition\nand structural relationships rather than relying on class identity. However,\nmost existing counting models fail to replicate this ability; they often\nmiscount when objects exhibit complex shapes, internal symmetry, or overlapping\ncomponents. In this work, we introduce CountFormer, a transformer-based\nframework that learns to recognize repetition and structural coherence for\nclass-agnostic object counting. Built upon the CounTR architecture, our model\nreplaces its visual encoder with the self-supervised foundation model DINOv2,\nwhich produces richer and spatially consistent feature representations. We\nfurther incorporate positional embedding fusion to preserve geometric\nrelationships before decoding these features into density maps through a\nlightweight convolutional decoder. Evaluated on the FSC-147 dataset, our model\nachieves performance comparable to current state-of-the-art methods while\ndemonstrating superior accuracy on structurally intricate or densely packed\nscenes. Our findings indicate that integrating foundation models such as DINOv2\nenables counting systems to approach human-like structural perception,\nadvancing toward a truly general and exemplar-free counting paradigm.", "AI": {"tldr": "CountFormer\u662f\u4e00\u4e2a\u57fa\u4e8etransformer\u7684\u8ba1\u6570\u6846\u67b6\uff0c\u5229\u7528DINOv2\u66ff\u4ee3\u5176\u89c6\u89c9\u7f16\u7801\u5668\uff0c\u5e76\u7ed3\u5408\u4f4d\u7f6e\u5d4c\u5165\u878d\u5408\uff0c\u4ee5\u5b9e\u73b0\u7c7b\u65e0\u5173\u7684\u7269\u4f53\u8ba1\u6570\uff0c\u5728FSC-147\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u590d\u6742\u7ed3\u6784\u6216\u5bc6\u96c6\u573a\u666f\u65f6\u3002", "motivation": "\u73b0\u6709\u8ba1\u6570\u6a21\u578b\u5728\u5904\u7406\u5177\u6709\u590d\u6742\u5f62\u72b6\u3001\u5bf9\u79f0\u6027\u6216\u91cd\u53e0\u90e8\u4ef6\u7684\u5bf9\u8c61\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u65e0\u6cd5\u50cf\u4eba\u7c7b\u4e00\u6837\u901a\u8fc7\u611f\u77e5\u91cd\u590d\u548c\u7ed3\u6784\u5173\u7cfb\u8fdb\u884c\u8ba1\u6570\u3002\u56e0\u6b64\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u6a21\u62df\u4eba\u7c7b\u8ba1\u6570\u80fd\u529b\u7684\u6a21\u578b\u3002", "method": "CountFormer\u6846\u67b6\u57fa\u4e8eCounTR\u67b6\u6784\uff0c\u5c06\u89c6\u89c9\u7f16\u7801\u5668\u66ff\u6362\u4e3aDINOv2\uff0c\u4ee5\u83b7\u53d6\u66f4\u4e30\u5bcc\u3001\u7a7a\u95f4\u4e00\u81f4\u7684\u7279\u5f81\u8868\u793a\u3002\u901a\u8fc7\u52a0\u5165\u4f4d\u7f6e\u5d4c\u5165\u878d\u5408\u6765\u4fdd\u7559\u51e0\u4f55\u5173\u7cfb\uff0c\u7136\u540e\u901a\u8fc7\u8f7b\u91cf\u7ea7\u5377\u79ef\u89e3\u7801\u5668\u5c06\u7279\u5f81\u89e3\u7801\u4e3a\u5bc6\u5ea6\u56fe\u3002", "result": "\u5728FSC-147\u6570\u636e\u96c6\u4e0a\uff0cCountFormer\u7684\u6027\u80fd\u4e0e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u76f8\u5f53\uff0c\u4f46\u5728\u5904\u7406\u7ed3\u6784\u590d\u6742\u7684\u573a\u666f\u6216\u5bc6\u96c6\u4eba\u7fa4\u65b9\u9762\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u5c06DINOv2\u7b49\u57fa\u7840\u6a21\u578b\u96c6\u6210\u5230\u8ba1\u6570\u7cfb\u7edf\u4e2d\uff0c\u53ef\u4ee5\u4f7f\u8ba1\u6570\u7cfb\u7edf\u63a5\u8fd1\u4eba\u7c7b\u7684\u7ed3\u6784\u611f\u77e5\u80fd\u529b\uff0c\u63a8\u52a8\u8ba1\u6570\u8303\u5f0f\u5411\u771f\u6b63\u901a\u7528\u548c\u65e0\u9700\u793a\u4f8b\u7684\u65b9\u5411\u53d1\u5c55\u3002"}}
{"id": "2510.23654", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.23654", "abs": "https://arxiv.org/abs/2510.23654", "authors": ["Yurang", "Kuang"], "title": "Quantum Mechanics of Stochastic Systems", "comment": null, "summary": "We develop a fundamental framework for the quantum mechanics of stochastic\nsystems (QMSS), showing that classical discrete stochastic processes emerge\nnaturally as perturbations of the quantum harmonic oscillator (QHO). By\nconstructing exact perturbation potentials that transform QHO eigenstates into\nstochastic representations, we demonstrate that canonical probability\ndistributions, including Binomial, Negative Binomial, and Poisson, arise from\nspecific modifications of the harmonic potential. Each stochastic system is\ngoverned by a Count Operator (N), with probabilities determined by squared\namplitudes in a Born-rule-like manner.\n  The framework introduces a complete operator algebra for moment generation\nand information-theoretic analysis, together with modular projection operators\n(R_M) that enable finite-dimensional approximations supported by rigorous\nuniform convergence theorems. This mathematical structure underpins True\nUniform Random Number Generation (TURNG) [Kuang, Sci. Rep., 2025], eliminating\nthe need for external whitening processes.\n  Beyond randomness generation, the QMSS framework enables quantum probability\nengineering: the physical realization of classical distributions through\ndesigned quantum perturbations. These results demonstrate that stochastic\nsystems are inherently quantum-mechanical in structure, bridging quantum\ndynamics, statistical physics, and experimental probability realization.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u91cf\u5b50\u529b\u5b66\u968f\u673a\u7cfb\u7edf(QMSS)\u7684\u57fa\u672c\u6846\u67b6\uff0c\u8868\u660e\u7ecf\u5178\u79bb\u6563\u968f\u673a\u8fc7\u7a0b\u662f\u91cf\u5b50\u8c10\u632f\u5b50(QHO)\u7684\u81ea\u7136\u6270\u52a8\u3002\u901a\u8fc7\u6784\u5efa\u7cbe\u786e\u7684\u6270\u52a8\u52bf\uff0c\u5c06QHO\u672c\u5f81\u6001\u8f6c\u5316\u4e3a\u968f\u673a\u8868\u793a\uff0c\u8bc1\u660e\u4e86\u4e8c\u9879\u5206\u5e03\u3001\u8d1f\u4e8c\u9879\u5206\u5e03\u548c\u6cca\u677e\u5206\u5e03\u7b49\u6807\u51c6\u6982\u7387\u5206\u5e03\u53ef\u7531\u8c10\u632f\u5b50\u52bf\u7684\u7279\u5b9a\u4fee\u6539\u4ea7\u751f\u3002\u6bcf\u4e2a\u968f\u673a\u7cfb\u7edf\u7531\u4e00\u4e2a\u8ba1\u6570\u7b97\u7b26(N)\u63a7\u5236\uff0c\u5176\u6982\u7387\u6839\u636e\u7c7b\u4f3c\u73bb\u6069\u6cd5\u5219\u7684\u5e73\u65b9\u5e45\u5ea6\u786e\u5b9a\u3002", "motivation": "\u5efa\u7acb\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\u6765\u89e3\u91ca\u7ecf\u5178\u968f\u673a\u8fc7\u7a0b\u7684\u91cf\u5b50\u529b\u5b66\u8d77\u6e90\uff0c\u5e76\u5c55\u793a\u5982\u4f55\u5229\u7528\u91cf\u5b50\u529b\u5b66\u539f\u7406\u6765\u751f\u6210\u548c\u63a7\u5236\u968f\u673a\u6027\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u7cbe\u786e\u7684\u6270\u52a8\u52bf\uff0c\u5c06\u91cf\u5b50\u8c10\u632f\u5b50(QHO)\u7684\u672c\u5f81\u6001\u8f6c\u5316\u4e3a\u968f\u673a\u8868\u793a\uff0c\u4ece\u800c\u63a8\u5bfc\u51fa\u5404\u79cd\u7ecf\u5178\u6982\u7387\u5206\u5e03\u3002\u5f15\u5165\u4e86\u7528\u4e8e\u77e9\u751f\u6210\u548c\u4fe1\u606f\u8bba\u5206\u6790\u7684\u7b97\u7b26\u4ee3\u6570\uff0c\u4ee5\u53ca\u7528\u4e8e\u6709\u9650\u7ef4\u8fd1\u4f3c\u7684\u6a21\u6001\u6295\u5f71\u7b97\u7b26(R_M)\u3002", "result": "\u6210\u529f\u5730\u4ece\u91cf\u5b50\u8c10\u632f\u5b50\u6a21\u578b\u4e2d\u63a8\u5bfc\u51fa\u4e86\u4e8c\u9879\u5206\u5e03\u3001\u8d1f\u4e8c\u9879\u5206\u5e03\u548c\u6cca\u677e\u5206\u5e03\u7b49\u7ecf\u5178\u6982\u7387\u5206\u5e03\u3002\u8be5\u6846\u67b6\u652f\u6301\u771f\u6b63\u7684\u7edf\u4e00\u968f\u673a\u6570\u751f\u6210(TURNG)\uff0c\u65e0\u9700\u989d\u5916\u7684\u767d\u5316\u8fc7\u7a0b\u3002\u6b64\u5916\uff0c\u8be5\u6846\u67b6\u8fd8\u5b9e\u73b0\u4e86\u91cf\u5b50\u6982\u7387\u5de5\u7a0b\uff0c\u5373\u901a\u8fc7\u8bbe\u8ba1\u7684\u91cf\u5b50\u6270\u52a8\u7269\u7406\u5b9e\u73b0\u7ecf\u5178\u6982\u7387\u5206\u5e03\u3002", "conclusion": "\u968f\u673a\u7cfb\u7edf\u672c\u8d28\u4e0a\u5177\u6709\u91cf\u5b50\u529b\u5b66\u7684\u7ed3\u6784\uff0c\u8be5\u7814\u7a76\u6210\u529f\u5730\u8fde\u63a5\u4e86\u91cf\u5b50\u52a8\u529b\u5b66\u3001\u7edf\u8ba1\u7269\u7406\u5b66\u548c\u5b9e\u9a8c\u6982\u7387\u5b9e\u73b0\u3002QMSS\u6846\u67b6\u4e3a\u7406\u89e3\u968f\u673a\u6027\u7684\u91cf\u5b50\u8d77\u6e90\u548c\u5229\u7528\u91cf\u5b50\u73b0\u8c61\u8fdb\u884c\u6982\u7387\u5de5\u7a0b\u63d0\u4f9b\u4e86\u65b0\u7684\u9014\u5f84\u3002"}}
{"id": "2510.24165", "categories": ["cs.LO"], "pdf": "https://arxiv.org/pdf/2510.24165", "abs": "https://arxiv.org/abs/2510.24165", "authors": ["Clara Lerouvillois", "Francesca Poggiolesi"], "title": "Dynamic Hypersequents for Public Announcement Logic", "comment": null, "summary": "Dynamic Epistemic Logic extends classical epistemic logic by modeling not\nonly static knowledge but also its evolution through information updates. Among\nits various systems, Public Announcement Logic (PAL) provides one of the\nsimplest and most studied frameworks for representing epistemic change. While\nthe semantics of PAL is well understood as transformation of Kripke models, the\nproof theory so far developed fails to represent this dynamism in purely\nsyntactical terms. The aim of this paper is to repair this lack. In particular,\nbuilding on a hypersequent calculus for S5, we extend it with a mechanism that\nmodels the transition between epistemic models induced by public announcements.\nWe call these structures dynamic hypersequents. Using dynamic hypersequents, we\nconstruct a calculus for PAL and we show that it enjoys several desirable\nproperties: admissibility of all structural rules (including contraction),\ninvertibility of logical rules, as well as syntactic cut-elimination.", "AI": {"tldr": "\u672c\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u52a8\u6001\u8d85\u5e8f\u5217\u6f14\u7b97\uff0c\u7528\u4e8e\u89e3\u51b3\u516c\u5171\u516c\u544a\u903b\u8f91\uff08PAL\uff09\u7684\u53e5\u6cd5\u8868\u793a\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684PAL\u8bc1\u660e\u7406\u8bba\u65e0\u6cd5\u7528\u7eaf\u53e5\u6cd5\u65b9\u5f0f\u8868\u8fbe\u6a21\u578b\u66f4\u65b0\u7684\u52a8\u6001\u6027\u3002", "method": "\u5728S5\u7684\u8d85\u5e8f\u5217\u6f14\u7b97\u57fa\u7840\u4e0a\uff0c\u5f15\u5165\u52a8\u6001\u8d85\u5e8f\u5217\u673a\u5236\u6765\u6a21\u62df\u516c\u5171\u516c\u544a\u5f15\u53d1\u7684\u77e5\u8bc6\u6a21\u578b\u8f6c\u6362\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5177\u6709\u7ed3\u6784\u89c4\u5219\u53ef\u5bb9\u6027\u3001\u903b\u8f91\u89c4\u5219\u53ef\u9006\u6027\u4ee5\u53ca\u53e5\u6cd5\u6d88\u51cf\u6027\u7684PAL\u6f14\u7b97\u3002", "conclusion": "\u5f00\u53d1\u7684\u52a8\u6001\u8d85\u5e8f\u5217\u6f14\u7b97\u80fd\u591f\u4ee5\u53e5\u6cd5\u65b9\u5f0f\u6210\u529f\u5730\u8868\u793aPAL\u7684\u52a8\u6001\u6027\uff0c\u5e76\u5177\u6709\u826f\u597d\u7684\u6027\u8d28\u3002"}}
{"id": "2510.24113", "categories": ["cs.AR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24113", "abs": "https://arxiv.org/abs/2510.24113", "authors": ["Arnav Shukla", "Harsh Sharma", "Srikant Bharadwaj", "Vinayak Abrol", "Sujay Deb"], "title": "Taming the Tail: NoI Topology Synthesis for Mixed DL Workloads on Chiplet-Based Accelerators", "comment": null, "summary": "Heterogeneous chiplet-based systems improve scaling by disag-gregating\nCPUs/GPUs and emerging technologies (HBM/DRAM).However this on-package\ndisaggregation introduces a latency inNetwork-on-Interposer(NoI). We observe\nthat in modern large-modelinference, parameters and activations routinely move\nbackand forth from HBM/DRAM, injecting large, bursty flows into theinterposer.\nThese memory-driven transfers inflate tail latency andviolate Service Level\nAgreements (SLAs) across k-ary n-cube base-line NoI topologies. To address this\ngap we introduce an InterferenceScore (IS) that quantifies worst-case slowdown\nunder contention.We then formulate NoI synthesis as a multi-objective\noptimization(MOO) problem. We develop PARL (Partition-Aware\nReinforcementLearner), a topology generator that balances throughput,\nlatency,and power. PARL-generated topologies reduce contention at the memory\ncut, meet SLAs, and cut worst-case slowdown to 1.2 times while maintaining\ncompetitive mean throughput relative to link-rich meshes. Overall, this\nreframes NoI design for heterogeneouschiplet accelerators with workload-aware\nobjectives.", "AI": {"tldr": "\u5f02\u6784 chiplet \u7cfb\u7edf\u901a\u8fc7\u5206\u79bb CPU/GPU \u548c HBM/DRAM \u7b49\u65b0\u5174\u6280\u672f\u6765\u6539\u5584\u6269\u5c55\u6027\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u5c01\u88c5\u5206\u79bb\u4f1a\u5728\u7247\u4e0a\u7f51\u7edc\uff08NoI\uff09\u4e2d\u5f15\u5165\u5ef6\u8fdf\u3002\u5728\u73b0\u4ee3\u5927\u578b\u6a21\u578b\u63a8\u7406\u4e2d\uff0c\u53c2\u6570\u548c\u6fc0\u6d3b\u4f1a\u9891\u7e41\u5730\u5728 HBM/DRAM \u4e4b\u95f4\u4f20\u8f93\uff0c\u5bfc\u81f4\u5927\u800c\u7a81\u53d1\u7684\u6570\u636e\u6d41\u8fdb\u5165\u7247\u4e0a\u7f51\u7edc\u3002\u8fd9\u4e9b\u7531\u5185\u5b58\u9a71\u52a8\u7684\u4f20\u8f93\u4f1a\u589e\u52a0\u5c3e\u90e8\u5ef6\u8fdf\uff0c\u5e76\u8fdd\u53cd k-ary n-cube \u57fa\u7ebf NoI \u62d3\u6251\u7684\u670d\u52a1\u6c34\u5e73\u534f\u8bae\uff08SLA\uff09\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u5e72\u6270\u8bc4\u5206\uff08IS\uff09\uff0c\u7528\u4e8e\u91cf\u5316\u4e89\u7528\u4e0b\u7684\u6700\u574f\u60c5\u51b5\u51cf\u901f\u3002\u6211\u4eec\u5c06 NoI \u5408\u6210\u8868\u8ff0\u4e3a\u591a\u76ee\u6807\u4f18\u5316\uff08MOO\uff09\u95ee\u9898\u3002\u6211\u4eec\u5f00\u53d1\u4e86 PARL\uff08Partition-Aware Reinforcement Learner\uff09\uff0c\u4e00\u79cd\u62d3\u6251\u751f\u6210\u5668\uff0c\u53ef\u4ee5\u5e73\u8861\u541e\u5410\u91cf\u3001\u5ef6\u8fdf\u548c\u529f\u8017\u3002PARL \u751f\u6210\u7684\u62d3\u6251\u53ef\u51cf\u5c11\u5185\u5b58\u8fde\u63a5\u5904\u7684\u4e89\u7528\uff0c\u6ee1\u8db3 SLA\uff0c\u5e76\u5c06\u6700\u574f\u60c5\u51b5\u51cf\u901f\u964d\u4f4e\u5230 1.2 \u500d\uff0c\u540c\u65f6\u76f8\u5bf9\u4e8e\u5bcc\u8fde\u63a5\u7f51\u683c\u4fdd\u6301\u6709\u7ade\u4e89\u529b\u7684\u5e73\u5747\u541e\u5410\u91cf\u3002\u603b\u4e4b\uff0c\u8fd9\u9879\u5de5\u4f5c\u4e3a\u5177\u6709\u5de5\u4f5c\u8d1f\u8f7d\u611f\u77e5\u76ee\u6807\u7684\u5f02\u6784 chiplet \u52a0\u901f\u5668\u91cd\u65b0\u5b9a\u4e49\u4e86 NoI \u8bbe\u8ba1\u3002", "motivation": "\u73b0\u4ee3\u5927\u578b\u6a21\u578b\u63a8\u7406\u4e2d\uff0c\u5185\u5b58\u8bbf\u95ee\u5bfc\u81f4\u7684\u7247\u4e0a\u7f51\u7edc\uff08NoI\uff09\u5ef6\u8fdf\u548c\u5c3e\u90e8\u5ef6\u8fdf\u589e\u52a0\uff0c\u8fdd\u53cd\u4e86\u670d\u52a1\u6c34\u5e73\u534f\u8bae\uff08SLA\uff09\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5e72\u6270\u8bc4\u5206\uff08IS\uff09\u6765\u91cf\u5316\u4e89\u7528\u4e0b\u7684\u6700\u574f\u60c5\u51b5\u51cf\u901f\u3002\u5c06 NoI \u5408\u6210\u8868\u8ff0\u4e3a\u591a\u76ee\u6807\u4f18\u5316\uff08MOO\uff09\u95ee\u9898\uff0c\u5e76\u5f00\u53d1\u4e86 PARL\uff08Partition-Aware Reinforcement Learner\uff09\u62d3\u6251\u751f\u6210\u5668\u6765\u5e73\u8861\u541e\u5410\u91cf\u3001\u5ef6\u8fdf\u548c\u529f\u8017\u3002", "result": "PARL \u751f\u6210\u7684\u62d3\u6251\u53ef\u51cf\u5c11\u5185\u5b58\u8fde\u63a5\u5904\u7684\u4e89\u7528\uff0c\u6ee1\u8db3 SLA\uff0c\u5e76\u5c06\u6700\u574f\u60c5\u51b5\u51cf\u901f\u964d\u4f4e\u5230 1.2 \u500d\uff0c\u540c\u65f6\u76f8\u5bf9\u4e8e\u5bcc\u8fde\u63a5\u7f51\u683c\u4fdd\u6301\u6709\u7ade\u4e89\u529b\u7684\u5e73\u5747\u541e\u5410\u91cf\u3002", "conclusion": "\u5bf9\u4e8e\u5177\u6709\u5de5\u4f5c\u8d1f\u8f7d\u611f\u77e5\u76ee\u6807\u7684\u5f02\u6784 chiplet \u52a0\u901f\u5668\uff0cPARL \u80fd\u591f\u6709\u6548\u5730\u8bbe\u8ba1 NoI \u62d3\u6251\uff0c\u4ece\u800c\u51cf\u5c11\u5ef6\u8fdf\u548c\u4e89\u7528\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u541e\u5410\u91cf\u3002"}}
{"id": "2510.23819", "categories": ["eess.SY", "cs.SY", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.23819", "abs": "https://arxiv.org/abs/2510.23819", "authors": ["Avishka Herath", "Malith Jayalath", "Kumudu Kaushalya", "Sanjana Kapukotuwa", "Chathuni Wijegunawardena", "Pahan Mendis", "Kithmin Wickremasinghe", "Duminda Samarasinghe", "Wageesha N. Manamperi", "Chamira U. S. Edussooriya"], "title": "A Simultaneous ECG-PCG Acquisition System with Real-Time Burst-Adaptive Noise Cancellation", "comment": "Paper submitted to IEEE International Symposium on Circuits and\n  Systems (ISCAS) 2026", "summary": "Cardiac auscultation is an essential clinical skill, requiring excellent\nhearing to distinguish subtle differences in timing and pitch of heart sounds.\nHowever, diagnosing solely from these sounds is often challenging due to\ninterference from surrounding noise, and the information may be limited.\nExisting solutions that adaptively cancel external noise are either not\nreal-time or are computationally intensive, making them unsuitable for\nimplementation in a portable system. This work proposes an end-to-end system\nwith a real-time adaptive noise cancellation pipeline integrated into a device\nthat simultaneously acquires electrocardiogram (ECG) and phonocardiogram (PCG)\nsignals. The performance of the system is validated using real-world hospital\nnoise datasets and recordings captured with the dual-modality device. For PCG\nand ECG signals recorded from the device in noisy hospital settings, the\nproposed algorithms achieved signal-to-noise ratio improvements of 37.01 dB and\n30.32 dB, respectively. These results demonstrate the systems effectiveness in\nenabling reliable and accessible cardiac screening, including noisy hospital\nenvironments typical of resource-constrained settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u96c6\u6210\u4e86\u5b9e\u65f6\u81ea\u9002\u5e94\u566a\u58f0\u6d88\u9664\u7ba1\u9053\u7684\u7aef\u5230\u7aef\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u53ef\u540c\u65f6\u91c7\u96c6\u5fc3\u7535\u56fe\uff08ECG\uff09\u548c\u5fc3\u97f3\u56fe\uff08PCG\uff09\u4fe1\u53f7\uff0c\u4ee5\u514b\u670d\u4f20\u7edf\u5fc3\u810f\u542c\u8bca\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edf\u7684\u5fc3\u810f\u542c\u8bca\u5728\u533a\u5206\u5fc3\u97f3\u7684\u7ec6\u5fae\u5dee\u522b\u65f6\u9700\u8981\u51fa\u8272\u7684\u542c\u529b\uff0c\u5e76\u4e14\u7531\u4e8e\u5468\u56f4\u566a\u58f0\u7684\u5e72\u6270\uff0c\u4ec5\u51ed\u58f0\u97f3\u8fdb\u884c\u8bca\u65ad\u901a\u5e38\u5177\u6709\u6311\u6218\u6027\u3002\u73b0\u6709\u7684\u566a\u58f0\u6d88\u9664\u89e3\u51b3\u65b9\u6848\u8981\u4e48\u4e0d\u662f\u5b9e\u65f6\u7684\uff0c\u8981\u4e48\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u4e0d\u9002\u7528\u4e8e\u4fbf\u643a\u5f0f\u8bbe\u5907\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7aef\u5230\u7aef\u7cfb\u7edf\uff0c\u5176\u4e2d\u5305\u542b\u4e00\u4e2a\u5b9e\u65f6\u81ea\u9002\u5e94\u566a\u58f0\u6d88\u9664\u7ba1\u9053\uff0c\u5e76\u96c6\u6210\u5230\u4e00\u4e2a\u53ef\u4ee5\u540c\u65f6\u91c7\u96c6\u5fc3\u7535\u56fe\uff08ECG\uff09\u548c\u5fc3\u97f3\u56fe\uff08PCG\uff09\u4fe1\u53f7\u7684\u8bbe\u5907\u4e2d\u3002", "result": "\u5728\u5608\u6742\u7684\u533b\u9662\u73af\u5883\u4e2d\uff0c\u4f7f\u7528\u8be5\u8bbe\u5907\u8bb0\u5f55\u7684\u5fc3\u97f3\u56fe\uff08PCG\uff09\u548c\u5fc3\u7535\u56fe\uff08ECG\uff09\u4fe1\u53f7\uff0c\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u5206\u522b\u5b9e\u73b0\u4e86 37.01 dB \u548c 30.32 dB \u7684\u4fe1\u566a\u6bd4\u6539\u8fdb\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u5730\u5728\u5305\u62ec\u5608\u6742\u7684\u533b\u9662\u73af\u5883\u5728\u5185\u7684\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\uff0c\u5b9e\u73b0\u53ef\u9760\u4e14\u6613\u4e8e\u8bbf\u95ee\u7684\u5fc3\u810f\u7b5b\u67e5\u3002"}}
{"id": "2510.23832", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.23832", "abs": "https://arxiv.org/abs/2510.23832", "authors": ["Evan Allen", "Karim Said", "Robert Calderbank", "Lingjia Liu"], "title": "Communication in a Fractional World: MIMO MC-OTFS Precoder Prediction", "comment": null, "summary": "As 6G technologies advance, international bodies and regulatory agencies are\nintensifying efforts to extend seamless connectivity especially for\nhigh-mobility scenarios such as Mobile Ad-Hoc Networks (\\textit{MANETs}) types\nsuch as Vehicular Ad-Hoc Networks (\\textit{VANETs}) and Flying Ad-Hoc Networks\n(\\textit{FANETs}). For these environments to be considered for long term\nadoption and use they must support Multiple-Input-Multiple- (MIMO) technology,\nrapidly fluctuating channel conditions in these environments place a heavy\nburden on traditional time-frequency CSI feedback schemes required for MIMO\nprecoding. This motivates a shift toward delay-Doppler representations like\nthose employed by Orthogonal Time-Frequency Space(OTFS) modulation, which\noffers greater stability under mobility. We derive an expression for the\nvariation over time in the OTFS I/O relationship. We then use this to create a\nphysics informed complex exponential basis expansion model prediction framework\nthat maximizes the usefulness of outdated Channel State Information (CSI) in\nthe presence of integer and fractional delay-Doppler channels and facilitates\nhigh mobility MIMO communication.", "AI": {"tldr": "As 6G advances, the stability of MIMO communication in high-mobility scenarios like VANETs and FANETs is a challenge due to fluctuating channel conditions. Traditional CSI feedback is insufficient, motivating a shift towards delay-Doppler representations like OTFS modulation for more stable communication.", "motivation": "Traditional time-frequency CSI feedback schemes struggle with the rapidly fluctuating channel conditions in high-mobility MANETs (VANETs, FANETs), hindering MIMO communication in 6G environments. This necessitates a move towards more stable representations.", "method": "The paper derives an expression for the time-varying OTFS I/O relationship and uses it to create a physics-informed complex exponential basis expansion model prediction framework that leverages outdated CSI for high-mobility MIMO communication.", "result": "A novel prediction framework is developed that effectively utilizes outdated CSI in the presence of integer and fractional delay-Doppler channels, enabling high-mobility MIMO communication.", "conclusion": "The proposed OTFS-based framework addresses the limitations of traditional CSI feedback in high-mobility scenarios, offering a promising solution for 6G connectivity."}}
{"id": "2510.23621", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23621", "abs": "https://arxiv.org/abs/2510.23621", "authors": ["Alexandre Benoit"], "title": "Speeding Up MACE: Low-Precision Tricks for Equivarient Force Fields", "comment": "78 pages, 21 figures", "summary": "Machine-learning force fields can deliver accurate molecular dynamics (MD) at\nhigh computational cost. For SO(3)-equivariant models such as MACE, there is\nlittle systematic evidence on whether reduced-precision arithmetic and\nGPU-optimized kernels can cut this cost without harming physical fidelity. This\nthesis aims to make MACE cheaper and faster while preserving accuracy by\nidentifying computational bottlenecks and evaluating low-precision execution\npolicies. We profile MACE end-to-end and per block, compare the e3nn and NVIDIA\ncuEquivariance backends, and assess FP64/FP32/BF16/FP16 settings (with FP32\naccumulation) for inference, short NVT and long NPT water simulations, and toy\ntraining runs under reproducible, steady-state timing. cuEquivariance reduces\ninference latency by about $3\\times$. Casting only linear layers to BF16/FP16\nwithin an FP32 model yields roughly 4x additional speedups, while energies and\nthermodynamic observables in NVT/NPT MD remain within run-to-run variability.\nHalf-precision weights during training degrade force RMSE. Mixing e3nn and cuEq\nmodules without explicit adapters causes representation mismatches. Fused\nequivariant kernels and mixed-precision inference can substantially accelerate\nstate-of-the-art force fields with negligible impact on downstream MD. A\npractical policy is to use cuEquivariance with FP32 by default and enable\nBF16/FP16 for linear layers (keeping FP32 accumulations) for maximum\nthroughput, while training remains in FP32. Further gains are expected on\nAmpere/Hopper GPUs (TF32/BF16) and from kernel-level FP16/BF16 paths and\npipeline fusion.", "AI": {"tldr": "SO(3)-equivariant MACE\u6a21\u578b\u7684\u4f4e\u7cbe\u5ea6\u8ba1\u7b97\u548cGPU\u4f18\u5316\u53ef\u4ee5\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u7269\u7406\u4fdd\u771f\u5ea6\u3002\u901a\u8fc7\u5206\u6790\u8ba1\u7b97\u74f6\u9888\u5e76\u8bc4\u4f30\u4f4e\u7cbe\u5ea6\u6267\u884c\u7b56\u7565\uff0c\u8be5\u7814\u7a76\u8868\u660e\u4f7f\u7528cuEquivariance\u540e\u7aef\u548c\u6df7\u5408\u7cbe\u5ea6\uff08\u7ebf\u6027\u5c42\u4f7f\u7528BF16/FP16\uff0c\u7d2f\u52a0\u4f7f\u7528FP32\uff09\u53ef\u4ee5\u5728\u5206\u5b50\u52a8\u529b\u5b66\u6a21\u62df\u4e2d\u5b9e\u73b0\u7ea64\u500d\u7684\u52a0\u901f\uff0c\u800c\u5bf9\u6a21\u62df\u7ed3\u679c\u7684\u5f71\u54cd\u53ef\u5ffd\u7565\u4e0d\u8ba1\u3002\u8bad\u7ec3\u4ecd\u5efa\u8bae\u4f7f\u7528FP32\uff0c\u4ee5\u907f\u514d\u7cbe\u5ea6\u635f\u5931\u3002", "motivation": "\u8bc4\u4f30\u4f4e\u7cbe\u5ea6\u7b97\u672f\u548cGPU\u4f18\u5316\u7684\u8ba1\u7b97\u65b9\u6cd5\u5728\u4e0d\u635f\u5bb3\u7269\u7406\u4fdd\u771f\u5ea6\u7684\u60c5\u51b5\u4e0b\uff0c\u964d\u4f4eSO(3)-\u7b49\u53d8\u6a21\u578b\uff08\u5982MACE\uff09\u7684\u8ba1\u7b97\u6210\u672c\u3002", "method": "\u5bf9MACE\u6a21\u578b\u8fdb\u884c\u7aef\u5230\u7aef\u548c\u6309\u6a21\u5757\u6027\u80fd\u5206\u6790\uff0c\u6bd4\u8f83e3nn\u548cNVIDIA cuEquivariance\u540e\u7aef\uff0c\u5e76\u8bc4\u4f30\u4e0d\u540c\u7cbe\u5ea6\u8bbe\u7f6e\uff08FP64/FP32/BF16/FP16\uff0cFP32\u7d2f\u52a0\uff09\u5728\u63a8\u7406\u3001\u77edNVT\u548c\u957fNPT\u6c34\u6a21\u62df\u4ee5\u53ca\u73a9\u5177\u8bad\u7ec3\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "cuEquivariance\u5c06\u63a8\u7406\u5ef6\u8fdf\u964d\u4f4e\u4e86\u7ea63\u500d\u3002\u4ec5\u5c06\u7ebf\u6027\u5c42\u8f6c\u6362\u4e3aBF16/FP16\uff08\u4fdd\u6301FP32\u7d2f\u52a0\uff09\u53ef\u5b9e\u73b0\u7ea64\u500d\u7684\u989d\u5916\u52a0\u901f\uff0c\u540c\u65f6NVT/NPT\u5206\u5b50\u52a8\u529b\u5b66\u4e2d\u7684\u80fd\u91cf\u548c\u70ed\u529b\u5b66\u89c2\u6d4b\u503c\u4fdd\u6301\u5728\u8fd0\u884c\u53ef\u53d8\u6027\u8303\u56f4\u5185\u3002\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4f7f\u7528\u534a\u7cbe\u5ea6\u6743\u91cd\u4f1a\u964d\u4f4e\u529b\u5747\u65b9\u6839\u8bef\u5dee\uff08RMSE\uff09\u3002\u6df7\u5408\u4f7f\u7528e3nn\u548ccuEq\u6a21\u5757\u800c\u4e0d\u663e\u5f0f\u9002\u914d\u5668\u4f1a\u5bfc\u81f4\u8868\u793a\u4e0d\u5339\u914d\u3002", "conclusion": "\u878d\u5408\u7684\u7b49\u53d8\u5185\u6838\u548c\u6df7\u5408\u7cbe\u5ea6\u63a8\u7406\u53ef\u4ee5\u663e\u8457\u52a0\u901f\u6700\u5148\u8fdb\u7684\u529b\u573a\uff0c\u540c\u65f6\u5bf9\u4e0b\u6e38\u5206\u5b50\u52a8\u529b\u5b66\u7684\u5f71\u54cd\u53ef\u5ffd\u7565\u4e0d\u8ba1\u3002\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u5efa\u8bae\u9ed8\u8ba4\u4f7f\u7528cuEquivariance\u548cFP32\uff0c\u5e76\u4e3a\u7ebf\u6027\u5c42\u542f\u7528BF16/FP16\uff08\u4fdd\u6301FP32\u7d2f\u52a0\uff09\u4ee5\u83b7\u5f97\u6700\u5927\u541e\u5410\u91cf\uff0c\u800c\u8bad\u7ec3\u4ecd\u4fdd\u6301\u5728FP32\u3002\u9884\u8ba1\u5728Ampere/Hopper GPU\u4e0a\uff08TF32/BF16\uff09\u4ee5\u53ca\u901a\u8fc7\u5185\u6838\u7ea7FP16/BF16\u8def\u5f84\u548c\u6d41\u6c34\u7ebf\u878d\u5408\uff0c\u53ef\u4ee5\u5b9e\u73b0\u8fdb\u4e00\u6b65\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2510.23734", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23734", "abs": "https://arxiv.org/abs/2510.23734", "authors": ["Eamon Duede"], "title": "AI and the Decentering of Disciplinary Creativity", "comment": null, "summary": "This paper examines the role of artificial intelligence in scientific\nproblem-solving, with a focus on its implications for disciplinary creativity.\nDrawing on recent work in the philosophy of creativity, I distinguish between\ncreative approaches and creative products, and introduce the concept of\ndisciplinary creativity -the creative application of discipline-specific\nexpertise to a valued problem within that field. Through two cases in\nmathematics, I show that while computation can extend disciplinary creativity,\ncertain approaches involving AI can serve to displace it. This displacement has\nthe potential to alter (and, perhaps, diminish) the value of scientific\npursuit.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u4eba\u5de5\u667a\u80fd\u5728\u79d1\u5b66\u95ee\u9898\u89e3\u51b3\u4e2d\u7684\u4f5c\u7528\uff0c\u7279\u522b\u662f\u5bf9\u5b66\u79d1\u521b\u9020\u529b\u7684\u5f71\u54cd\u3002\u6587\u7ae0\u533a\u5206\u4e86\u521b\u9020\u6027\u65b9\u6cd5\u548c\u521b\u9020\u6027\u4ea7\u54c1\uff0c\u5e76\u63d0\u51fa\u4e86\u201c\u5b66\u79d1\u521b\u9020\u529b\u201d\u7684\u6982\u5ff5\u2014\u2014\u5373\u5728\u7279\u5b9a\u9886\u57df\u5185\uff0c\u8fd0\u7528\u8be5\u9886\u57df\u7684\u4e13\u4e1a\u77e5\u8bc6\u6765\u89e3\u51b3\u6709\u4ef7\u503c\u7684\u95ee\u9898\u3002\u901a\u8fc7\u4e24\u4e2a\u6570\u5b66\u6848\u4f8b\uff0c\u7814\u7a76\u8868\u660e\u8ba1\u7b97\u53ef\u4ee5\u589e\u5f3a\u5b66\u79d1\u521b\u9020\u529b\uff0c\u4f46\u67d0\u4e9b\u6d89\u53ca\u4eba\u5de5\u667a\u80fd\u7684\u65b9\u6cd5\u53ef\u80fd\u4f1a\u53d6\u4ee3\u5b83\uff0c\u4ece\u800c\u53ef\u80fd\u964d\u4f4e\u79d1\u5b66\u63a2\u7d22\u7684\u4ef7\u503c\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u63a2\u8ba8\u4eba\u5de5\u667a\u80fd\u5bf9\u79d1\u5b66\u95ee\u9898\u89e3\u51b3\u548c\u5b66\u79d1\u521b\u9020\u529b\u7684\u5f71\u54cd\uff0c\u7279\u522b\u662f\u533a\u5206\u4e86\u521b\u9020\u6027\u65b9\u6cd5\u4e0e\u521b\u9020\u6027\u4ea7\u54c1\uff0c\u5e76\u63d0\u51fa\u4e86\u201c\u5b66\u79d1\u521b\u9020\u529b\u201d\u7684\u6982\u5ff5\uff0c\u4ee5\u8bc4\u4f30\u4eba\u5de5\u667a\u80fd\u5728\u79d1\u5b66\u7814\u7a76\u4e2d\u7684\u6f5c\u5728\u4ef7\u503c\u548c\u98ce\u9669\u3002", "method": "\u672c\u6587\u901a\u8fc7\u533a\u5206\u521b\u9020\u6027\u65b9\u6cd5\u548c\u521b\u9020\u6027\u4ea7\u54c1\uff0c\u5e76\u63d0\u51fa\u201c\u5b66\u79d1\u521b\u9020\u529b\u201d\u7684\u6982\u5ff5\uff0c\u7136\u540e\u901a\u8fc7\u4e24\u4e2a\u6570\u5b66\u6848\u4f8b\u7814\u7a76\uff0c\u5206\u6790\u4e86\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u5728\u589e\u5f3a\u6216\u53d6\u4ee3\u5b66\u79d1\u521b\u9020\u529b\u65b9\u9762\u7684\u4f5c\u7528\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u8ba1\u7b97\u53ef\u4ee5\u6269\u5c55\u5b66\u79d1\u521b\u9020\u529b\uff0c\u4f46\u67d0\u4e9b\u6d89\u53ca\u4eba\u5de5\u667a\u80fd\u7684\u65b9\u6cd5\u53ef\u80fd\u4f1a\u53d6\u4ee3\u5b83\uff0c\u4ece\u800c\u53ef\u80fd\u964d\u4f4e\u79d1\u5b66\u63a2\u7d22\u7684\u4ef7\u503c\u3002", "conclusion": "\u867d\u7136\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u5728\u79d1\u5b66\u95ee\u9898\u89e3\u51b3\u65b9\u9762\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u8981\u8b66\u60d5\u5176\u53ef\u80fd\u53d6\u4ee3\u4eba\u7c7b\u5b66\u79d1\u521b\u9020\u529b\uff0c\u4ece\u800c\u5f71\u54cd\u79d1\u5b66\u7814\u7a76\u4ef7\u503c\u7684\u98ce\u9669\u3002"}}
{"id": "2510.23648", "categories": ["cs.SI", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23648", "abs": "https://arxiv.org/abs/2510.23648", "authors": ["Ashutosh Anshul", "Mohammad Zia Ur Rehman", "Sri Akash Kadali", "Nagendra Kumar"], "title": "RoGBot: Relationship-Oblivious Graph-based Neural Network with Contextual Knowledge for Bot Detection", "comment": "Submitted to IEEE", "summary": "Detecting automated accounts (bots) among genuine users on platforms like\nTwitter remains a challenging task due to the evolving behaviors and adaptive\nstrategies of such accounts. While recent methods have achieved strong\ndetection performance by combining text, metadata, and user relationship\ninformation within graph-based frameworks, many of these models heavily depend\non explicit user-user relationship data. This reliance limits their\napplicability in scenarios where such information is unavailable. To address\nthis limitation, we propose a novel multimodal framework that integrates\ndetailed textual features with enriched user metadata while employing\ngraph-based reasoning without requiring follower-following data. Our method\nuses transformer-based models (e.g., BERT) to extract deep semantic embeddings\nfrom tweets, which are aggregated using max pooling to form comprehensive\nuser-level representations. These are further combined with auxiliary\nbehavioral features and passed through a GraphSAGE model to capture both local\nand global patterns in user behavior. Experimental results on the Cresci-15,\nCresci-17, and PAN 2019 datasets demonstrate the robustness of our approach,\nachieving accuracies of 99.8%, 99.1%, and 96.8%, respectively, and highlighting\nits effectiveness against increasingly sophisticated bot strategies.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u6a21\u6001\u6846\u67b6\uff0c\u7ed3\u5408\u6587\u672c\u548c\u7528\u6237\u5143\u6570\u636e\uff0c\u65e0\u9700\u8ddf\u968f\u8005-\u88ab\u5173\u6ce8\u8005\u6570\u636e\u5373\u53ef\u8fdb\u884c\u57fa\u4e8e\u56fe\u7684\u63a8\u7406\uff0c\u4ee5\u68c0\u6d4b\u673a\u5668\u4eba\u8d26\u6237\u3002", "motivation": "\u7531\u4e8e\u673a\u5668\u4eba\u8d26\u6237\u884c\u4e3a\u7684\u4e0d\u65ad\u6f14\u53d8\u548c\u9002\u5e94\u6027\u7b56\u7565\uff0c\u5728Twitter\u7b49\u5e73\u53f0\u4e0a\u68c0\u6d4b\u673a\u5668\u4eba\u8d26\u6237\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u867d\u7136\u5728\u56fe\u6846\u67b6\u4e2d\u7ed3\u5408\u4e86\u6587\u672c\u3001\u5143\u6570\u636e\u548c\u7528\u6237\u5173\u7cfb\u4fe1\u606f\uff0c\u4f46\u8fc7\u5ea6\u4f9d\u8d56\u663e\u5f0f\u7528\u6237-\u7528\u6237\u5173\u7cfb\u6570\u636e\uff0c\u9650\u5236\u4e86\u5176\u5e94\u7528\u8303\u56f4\u3002", "method": "\u4f7f\u7528\u57fa\u4e8eTransformer\u7684\u6a21\u578b\uff08\u5982BERT\uff09\u63d0\u53d6\u63a8\u6587\u7684\u6df1\u5ea6\u8bed\u4e49\u5d4c\u5165\uff0c\u5e76\u901a\u8fc7\u6700\u5927\u6c60\u5316\u805a\u5408\u5f62\u6210\u7528\u6237\u7ea7\u8868\u793a\u3002\u5c06\u8fd9\u4e9b\u8868\u793a\u4e0e\u8f85\u52a9\u884c\u4e3a\u7279\u5f81\u7ed3\u5408\uff0c\u5e76\u4f7f\u7528GraphSAGE\u6a21\u578b\u6765\u6355\u6349\u7528\u6237\u884c\u4e3a\u7684\u5c40\u90e8\u548c\u5168\u5c40\u6a21\u5f0f\u3002", "result": "\u5728Cresci-15\u3001Cresci-17\u548cPAN 2019\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5206\u522b\u5b9e\u73b0\u4e8699.8%\u300199.1%\u548c96.8%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u68c0\u6d4b\u590d\u6742\u7684\u673a\u5668\u4eba\u7b56\u7565\u65b9\u9762\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u548c\u6709\u6548\u6027\uff0c\u5e76\u4e14\u65e0\u9700\u4f7f\u7528\u5173\u6ce8\u8005-\u88ab\u5173\u6ce8\u8005\u6570\u636e\u3002"}}
{"id": "2510.24030", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2510.24030", "abs": "https://arxiv.org/abs/2510.24030", "authors": ["Ahmet Akkaya Melih", "Yamuna Singh", "Kunal L. Agarwal", "Priya Mukherjee", "Kiran Pattnaik", "Hanuman Bhatia"], "title": "Human Machine Social Hybrid Intelligence:A Collaborative Decision Making Framework for Large Model Agent Groups and Human Experts", "comment": null, "summary": "The rapid advancements in large foundation models and multi-agent systems\noffer unprecedented capabilities, yet current Human-in-the-Loop (HiTL)\nparadigms inadequately integrate human expertise, often leading to cognitive\noverload and decision-making bottlenecks in complex, high-stakes environments.\nWe propose the \"Human-Machine Social Hybrid Intelligence\" (HMS-HI) framework, a\nnovel architecture designed for deep, collaborative decision-making between\ngroups of human experts and LLM-powered AI agents. HMS-HI is built upon three\ncore pillars: (1) a \\textbf{Shared Cognitive Space (SCS)} for unified,\nmulti-modal situational awareness and structured world modeling; (2) a\n\\textbf{Dynamic Role and Task Allocation (DRTA)} module that adaptively assigns\ntasks to the most suitable agent (human or AI) based on capabilities and\nworkload; and (3) a \\textbf{Cross-Species Trust Calibration (CSTC)} protocol\nthat fosters transparency, accountability, and mutual adaptation through\nexplainable declarations and structured feedback. Validated in a high-fidelity\nurban emergency response simulation, HMS-HI significantly reduced civilian\ncasualties by 72\\% and cognitive load by 70\\% compared to traditional HiTL\napproaches, demonstrating superior decision quality, efficiency, and human-AI\ntrust. An ablation study confirms the critical contribution of each module,\nhighlighting that engineered trust and shared context are foundational for\nscalable, synergistic human-AI collaboration.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd", "motivation": "\u73b0\u6709\u7684\u4eba\u673a\u534f\u540c\uff08HiTL\uff09\u8303\u5f0f\u672a\u80fd\u5145\u5206\u6574\u5408\u4eba\u7c7b\u4e13\u4e1a\u77e5\u8bc6\uff0c\u5bfc\u81f4\u5728\u590d\u6742\u3001\u9ad8\u98ce\u9669\u73af\u5883\u4e2d\u51fa\u73b0\u8ba4\u77e5\u8fc7\u8f7d\u548c\u51b3\u7b56\u74f6\u9888\u3002", "method": "\u63d0\u51fa\u201c\u4eba\u673a\u793e\u4f1a\u6df7\u5408\u667a\u80fd\u201d\uff08HMS-HI\uff09\u6846\u67b6\uff0c\u4e00\u4e2a\u7528\u4e8e\u4eba\u7c7b\u4e13\u5bb6\u548cLLM\u9a71\u52a8\u7684AI\u4ee3\u7406\u7fa4\u4f53\u4e4b\u95f4\u6df1\u5ea6\u534f\u4f5c\u51b3\u7b56\u7684\u65b0\u9896\u67b6\u6784\u3002HMS-HI\u5efa\u7acb\u5728\u4e09\u4e2a\u6838\u5fc3\u652f\u67f1\u4e4b\u4e0a\uff1a(1) \u7528\u4e8e\u7edf\u4e00\u3001\u591a\u6a21\u6001\u6001\u52bf\u611f\u77e5\u548c\u7ed3\u6784\u5316\u4e16\u754c\u5efa\u6a21\u7684\u5171\u4eab\u8ba4\u77e5\u7a7a\u95f4\uff08SCS\uff09\uff1b(2) \u4e00\u4e2a\u52a8\u6001\u89d2\u8272\u548c\u4efb\u52a1\u5206\u914d\uff08DRTA\uff09\u6a21\u5757\uff0c\u6839\u636e\u80fd\u529b\u548c\u5de5\u4f5c\u91cf\u81ea\u9002\u5e94\u5730\u5c06\u4efb\u52a1\u5206\u914d\u7ed9\u6700\u5408\u9002\u7684\u4ee3\u7406\uff08\u4eba\u7c7b\u6216AI\uff09\uff1b(3) \u4e00\u4e2a\u8de8\u7269\u79cd\u4fe1\u4efb\u6821\u51c6\uff08CSTC\uff09\u534f\u8bae\uff0c\u901a\u8fc7\u53ef\u89e3\u91ca\u7684\u58f0\u660e\u548c\u7ed3\u6784\u5316\u53cd\u9988\u4fc3\u8fdb\u900f\u660e\u5ea6\u3001\u95ee\u8d23\u5236\u548c\u76f8\u4e92\u9002\u5e94\u3002", "result": "\u5728\u57ce\u5e02\u5e94\u6025\u54cd\u5e94\u6a21\u62df\u4e2d\uff0cHMS-HI\u4e0e\u4f20\u7edf\u7684HiTL\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5c06\u5e73\u6c11\u4f24\u4ea1\u51cf\u5c11\u4e8672%\uff0c\u8ba4\u77e5\u8d1f\u8377\u964d\u4f4e\u4e8670%\uff0c\u5728\u51b3\u7b56\u8d28\u91cf\u3001\u6548\u7387\u548c\u4eba\u673a\u4fe1\u4efb\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u4e86\u6bcf\u4e2a\u6a21\u5757\u7684\u5173\u952e\u8d21\u732e\uff0c\u5e76\u5f3a\u8c03\u5de5\u7a0b\u5316\u4fe1\u4efb\u548c\u5171\u4eab\u4e0a\u4e0b\u6587\u662f\u53ef\u6269\u5c55\u3001\u534f\u540c\u7684\u4eba\u673a\u534f\u4f5c\u7684\u57fa\u7840\u3002", "conclusion": "HMS-HI\u6846\u67b6\u901a\u8fc7\u5171\u4eab\u8ba4\u77e5\u7a7a\u95f4\u3001\u52a8\u6001\u4efb\u52a1\u5206\u914d\u548c\u4fe1\u4efb\u6821\u51c6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u590d\u6742\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u8d28\u91cf\u548c\u6548\u7387\uff0c\u964d\u4f4e\u4e86\u8ba4\u77e5\u8d1f\u8377\uff0c\u5e76\u589e\u5f3a\u4e86\u4eba\u673a\u4fe1\u4efb\u3002"}}
{"id": "2510.24621", "categories": ["cs.DS", "cs.CG", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.24621", "abs": "https://arxiv.org/abs/2510.24621", "authors": ["Ziyi Fang", "Lingxiao Huang", "Runkai Yang"], "title": "Coreset for Robust Geometric Median: Eliminating Size Dependency on Outliers", "comment": "This paper has been accepted by NeurIPS 2025", "summary": "We study the robust geometric median problem in Euclidean space\n$\\mathbb{R}^d$, with a focus on coreset construction.A coreset is a compact\nsummary of a dataset $P$ of size $n$ that approximates the robust cost for all\ncenters $c$ within a multiplicative error $\\varepsilon$. Given an outlier count\n$m$, we construct a coreset of size $\\tilde{O}(\\varepsilon^{-2} \\cdot\n\\min\\{\\varepsilon^{-2}, d\\})$ when $n \\geq 4m$, eliminating the $O(m)$\ndependency present in prior work [Huang et al., 2022 & 2023]. For the special\ncase of $d = 1$, we achieve an optimal coreset size of\n$\\tilde{\\Theta}(\\varepsilon^{-1/2} + \\frac{m}{n} \\varepsilon^{-1})$, revealing\na clear separation from the vanilla case studied in [Huang et al., 2023;\nAfshani and Chris, 2024]. Our results further extend to robust\n$(k,z)$-clustering in various metric spaces, eliminating the $m$-dependence\nunder mild data assumptions. The key technical contribution is a novel\nnon-component-wise error analysis, enabling substantial reduction of outlier\ninfluence, unlike prior methods that retain them.Empirically, our algorithms\nconsistently outperform existing baselines in terms of size-accuracy tradeoffs\nand runtime, even when data assumptions are violated across a wide range of\ndatasets.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u6b27\u6c0f\u7a7a\u95f4\u4e2d\u7684\u9c81\u68d2\u51e0\u4f55\u4e2d\u503c\u95ee\u9898\uff0c\u91cd\u70b9\u662f\u5171\u6838\uff08coreset\uff09\u6784\u9020\u3002\u5f53\u6570\u636e\u70b9\u6570\u91cf n \u5927\u4e8e\u7b49\u4e8e 4m \u65f6\uff0c\u6211\u4eec\u6784\u9020\u4e86\u4e00\u4e2a\u5927\u5c0f\u4e3a O(\u03b5^{-2} * min{\u03b5^{-2}, d}) \u7684\u5171\u6838\uff0c\u6d88\u9664\u4e86\u5148\u524d\u5de5\u4f5c\u4e2d\u5b58\u5728\u7684 O(m) \u4f9d\u8d56\u6027\u3002\u5bf9\u4e8e d=1 \u7684\u7279\u4f8b\uff0c\u6211\u4eec\u5b9e\u73b0\u4e86 O(\u03b5^{-1/2} + (m/n)\u03b5^{-1}) \u7684\u6700\u4f18\u5171\u6838\u5927\u5c0f\u3002\u8fd9\u4e9b\u7ed3\u679c\u4e5f\u6269\u5c55\u5230\u9c81\u68d2 (k,z)-\u805a\u7c7b\u95ee\u9898\u3002\u5173\u952e\u6280\u672f\u8d21\u732e\u662f\u4e00\u79cd\u65b0\u9896\u7684\u975e\u5206\u91cf\u8bef\u5dee\u5206\u6790\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u51cf\u5c11\u79bb\u7fa4\u70b9\u7684\u5f71\u54cd\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u9c81\u68d2\u51e0\u4f55\u4e2d\u503c\u95ee\u9898\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u8fdb\u884c\u5171\u6838\uff08coreset\uff09\u6784\u9020\uff0c\u4ee5\u671f\u5728\u8fd1\u4f3c\u6210\u672c\u8ba1\u7b97\u4e2d\u83b7\u5f97\u66f4\u4f18\u7684\u8bef\u5dee\u754c\u9650\uff0c\u5e76\u51cf\u5c11\u5bf9\u79bb\u7fa4\u70b9\u6570\u91cf\u7684\u4f9d\u8d56\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u975e\u5206\u91cf\u8bef\u5dee\u5206\u6790\u65b9\u6cd5\uff0c\u7528\u4e8e\u6784\u9020\u9c81\u68d2\u51e0\u4f55\u4e2d\u503c\u95ee\u9898\u7684\u5171\u6838\u3002\u5bf9\u4e8e\u4e00\u822c\u60c5\u51b5\u548c\u4e00\u7ef4\u60c5\u51b5\uff0c\u5206\u522b\u5f97\u5230\u4e86\u4e0d\u540c\u5927\u5c0f\u7684\u5171\u6838\uff0c\u5e76\u5c06\u5176\u6269\u5c55\u5230\u9c81\u68d2 (k,z)-\u805a\u7c7b\u95ee\u9898\u3002", "result": "\u5728 n \u2265 4m \u7684\u60c5\u51b5\u4e0b\uff0c\u5f97\u5230\u4e86 O(\u03b5^{-2} * min{\u03b5^{-2}, d}) \u5927\u5c0f\u7684\u5171\u6838\uff0c\u6d88\u9664\u4e86\u5bf9 m \u7684\u4f9d\u8d56\u3002\u5728\u4e00\u7ef4\u60c5\u51b5\u4e0b\uff0c\u5f97\u5230 O(\u03b5^{-1/2} + (m/n)\u03b5^{-1}) \u7684\u6700\u4f18\u5171\u6838\u5927\u5c0f\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u7b97\u6cd5\u5728\u5927\u5c0f-\u7cbe\u5ea6\u6743\u8861\u548c\u8fd0\u884c\u65f6\u95f4\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u5171\u6838\u6784\u9020\u65b9\u6cd5\u5728\u9c81\u68d2\u51e0\u4f55\u4e2d\u503c\u95ee\u9898\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u5c24\u5176\u662f\u5728\u51cf\u5c11\u5171\u6838\u5927\u5c0f\u548c\u6d88\u9664\u5bf9\u79bb\u7fa4\u70b9\u6570\u91cf\u7684\u4f9d\u8d56\u65b9\u9762\u3002\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u5728\u7406\u8bba\u4e0a\u5177\u6709\u4f18\u52bf\uff0c\u5728\u5b9e\u8df5\u4e2d\u4e5f\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2510.24228", "categories": ["eess.SY", "cs.LG", "cs.NA", "cs.SY", "math.NA"], "pdf": "https://arxiv.org/pdf/2510.24228", "abs": "https://arxiv.org/abs/2510.24228", "authors": ["Luis Romero-Ben", "Paul Irofti", "Florin Stoican", "Vicen\u00e7 Puig"], "title": "A comparison between joint and dual UKF implementations for state estimation and leak localization in water distribution networks", "comment": "This work has been submitted to ECC2026 for review. It has 7 pages\n  and 2 figures", "summary": "The sustainability of modern cities highly depends on efficient water\ndistribution management, including effective pressure control and leak\ndetection and localization. Accurate information about the network hydraulic\nstate is therefore essential. This article presents a comparison between two\ndata-driven state estimation methods based on the Unscented Kalman Filter\n(UKF), fusing pressure, demand and flow data for head and flow estimation. One\napproach uses a joint state vector with a single estimator, while the other\nuses a dual-estimator scheme. We analyse their main characteristics, discussing\ndifferences, advantages and limitations, and compare them theoretically in\nterms of accuracy and complexity. Finally, we show several estimation results\nfor the L-TOWN benchmark, allowing to discuss their properties in a real\nimplementation.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6bd4\u8f83\u4e86\u4e24\u79cd\u57fa\u4e8e\u65e0\u8ff9\u5361\u5c14\u66fc\u6ee4\u6ce2\uff08UKF\uff09\u7684\u6570\u636e\u9a71\u52a8\u72b6\u6001\u4f30\u8ba1\u65b9\u6cd5\uff0c\u7528\u4e8e\u57ce\u5e02\u4f9b\u6c34\u7f51\u7edc\u7684\u6c34\u538b\u548c\u6d41\u91cf\u4f30\u8ba1\uff0c\u5e76\u4ee5L-TOWN\u4e3a\u57fa\u51c6\u8fdb\u884c\u4e86\u5b9e\u8bc1\u5206\u6790\u3002", "motivation": "\u73b0\u4ee3\u57ce\u5e02\u7684\u53ef\u6301\u7eed\u6027\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u9ad8\u6548\u7684\u4f9b\u6c34\u7ba1\u7406\uff0c\u5305\u62ec\u6709\u6548\u7684\u538b\u529b\u63a7\u5236\u3001\u6cc4\u6f0f\u68c0\u6d4b\u548c\u5b9a\u4f4d\uff0c\u56e0\u6b64\u51c6\u786e\u7684\u7f51\u7edc\u6c34\u529b\u72b6\u6001\u4fe1\u606f\u81f3\u5173\u91cd\u8981\u3002", "method": "\u672c\u6587\u63d0\u51fa\u5e76\u6bd4\u8f83\u4e86\u4e24\u79cd\u57fa\u4e8e\u65e0\u8ff9\u5361\u5c14\u66fc\u6ee4\u6ce2\uff08UKF\uff09\u7684\u6570\u636e\u9a71\u52a8\u72b6\u6001\u4f30\u8ba1\u65b9\u6cd5\uff0c\u878d\u5408\u4e86\u538b\u529b\u3001\u9700\u6c42\u548c\u6d41\u91cf\u6570\u636e\u6765\u4f30\u8ba1\u6c34\u538b\u548c\u6d41\u91cf\u3002\u4e00\u79cd\u65b9\u6cd5\u4f7f\u7528\u8054\u5408\u72b6\u6001\u5411\u91cf\u548c\u5355\u4e00\u4f30\u8ba1\u5668\uff0c\u53e6\u4e00\u79cd\u65b9\u6cd5\u4f7f\u7528\u53cc\u4f30\u8ba1\u5668\u65b9\u6848\u3002", "result": "\u901a\u8fc7\u5bf9L-TOWN\u57fa\u51c6\u8fdb\u884c\u7684\u591a\u9879\u4f30\u8ba1\u7ed3\u679c\u5c55\u793a\uff0c\u5206\u6790\u4e86\u4e24\u79cd\u65b9\u6cd5\u7684\u7279\u6027\u3001\u4f18\u7f3a\u70b9\uff0c\u5e76\u4ece\u7406\u8bba\u4e0a\u6bd4\u8f83\u4e86\u5b83\u4eec\u7684\u51c6\u786e\u6027\u548c\u590d\u6742\u6027\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u8fd9\u4e24\u79cdUKF\u65b9\u6cd5\u5728L-TOWN\u57fa\u51c6\u7684\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u4e0d\u540c\u7684\u7279\u6027\uff0c\u4e3a\u4f9b\u6c34\u7f51\u7edc\u72b6\u6001\u4f30\u8ba1\u63d0\u4f9b\u4e86\u53c2\u8003\u3002"}}
{"id": "2510.23828", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23828", "abs": "https://arxiv.org/abs/2510.23828", "authors": ["Mena Attia", "Aashiq Muhamed", "Mai Alkhamissi", "Thamar Solorio", "Mona Diab"], "title": "Beyond Understanding: Evaluating the Pragmatic Gap in LLMs' Cultural Processing of Figurative Language", "comment": null, "summary": "We present a comprehensive evaluation of the ability of large language models\n(LLMs) to process culturally grounded language, specifically to understand and\npragmatically use figurative expressions that encode local knowledge and\ncultural nuance. Using figurative language as a proxy for cultural nuance and\nlocal knowledge, we design evaluation tasks for contextual understanding,\npragmatic use, and connotation interpretation in Arabic and English. We\nevaluate 22 open- and closed-source LLMs on Egyptian Arabic idioms,\nmultidialectal Arabic proverbs, and English proverbs. Our results show a\nconsistent hierarchy: the average accuracy for Arabic proverbs is 4.29% lower\nthan for English proverbs, and performance for Egyptian idioms is 10.28% lower\nthan for Arabic proverbs. For the pragmatic use task, accuracy drops by 14.07%\nrelative to understanding, though providing contextual idiomatic sentences\nimproves accuracy by 10.66%. Models also struggle with connotative meaning,\nreaching at most 85.58% agreement with human annotators on idioms with 100%\ninter-annotator agreement. These findings demonstrate that figurative language\nserves as an effective diagnostic for cultural reasoning: while LLMs can often\ninterpret figurative meaning, they face challenges in using it appropriately.\nTo support future research, we release Kinayat, the first dataset of Egyptian\nArabic idioms designed for both figurative understanding and pragmatic use\nevaluation.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u7406\u89e3\u548c\u8fd0\u7528\u963f\u62c9\u4f2f\u8bed\u548c\u82f1\u8bed\u7684\u4e60\u8bed\u548c\u8c1a\u8bed\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u6587\u5316\u7ec6\u5fae\u5dee\u522b\u548c\u5f53\u5730\u77e5\u8bc6\u65b9\u9762\u3002\u6a21\u578b\u5728\u7406\u89e3\u65b9\u9762\u4f18\u4e8e\u5728\u5b9e\u9645\u5e94\u7528\u65b9\u9762\uff0c\u5e76\u4e14\u5728\u5904\u7406\u82f1\u8bed\u8c1a\u8bed\u65b9\u9762\u4f18\u4e8e\u5904\u7406\u963f\u62c9\u4f2f\u8bed\u4e60\u8bed\u548c\u8c1a\u8bed\u3002\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5728\u7406\u89e3\u4e60\u8bed\u7684\u5bd3\u610f\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002", "motivation": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5904\u7406\u6587\u5316\u7279\u5b9a\u8bed\u8a00\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u7406\u89e3\u548c\u5b9e\u7528\u6027\u5730\u4f7f\u7528\u5305\u542b\u5f53\u5730\u77e5\u8bc6\u548c\u6587\u5316\u7ec6\u5fae\u5dee\u522b\u7684\u6bd4\u55bb\u6027\u8868\u8fbe\u3002", "method": "\u8bbe\u8ba1\u4e86\u963f\u62c9\u4f2f\u8bed\u548c\u82f1\u8bed\u7684\u8bc4\u4f30\u4efb\u52a1\uff0c\u7528\u4e8e\u6d4b\u8bd5\u4e0a\u4e0b\u6587\u7406\u89e3\u3001\u5b9e\u7528\u6027\u4f7f\u7528\u548c\u5185\u6db5\u89e3\u91ca\u3002\u8bc4\u4f30\u4e86 22 \u4e2a\u5f00\u6e90\u548c\u95ed\u6e90 LLMs \u5728\u57c3\u53ca\u963f\u62c9\u4f2f\u8bed\u4e60\u8bed\u3001\u591a\u65b9\u8a00\u963f\u62c9\u4f2f\u8bed\u8c1a\u8bed\u548c\u82f1\u8bed\u8c1a\u8bed\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u963f\u62c9\u4f2f\u8bed\u8c1a\u8bed\u7684\u5e73\u5747\u51c6\u786e\u7387\u6bd4\u82f1\u8bed\u8c1a\u8bed\u4f4e 4.29%\uff0c\u57c3\u53ca\u4e60\u8bed\u7684\u51c6\u786e\u7387\u6bd4\u963f\u62c9\u4f2f\u8bed\u8c1a\u8bed\u4f4e 10.28%\u3002\u5728\u5b9e\u7528\u6027\u4efb\u52a1\u4e2d\uff0c\u51c6\u786e\u7387\u6bd4\u7406\u89e3\u4efb\u52a1\u4f4e 14.07%\uff0c\u4f46\u63d0\u4f9b\u4e0a\u4e0b\u6587\u4e60\u8bed\u53e5\u5b50\u53ef\u5c06\u51c6\u786e\u7387\u63d0\u9ad8 10.66%\u3002\u6a21\u578b\u5728\u5904\u7406\u5185\u6db5\u610f\u4e49\u65b9\u9762\u4e5f\u5b58\u5728\u56f0\u96be\uff0c\u5176\u7ed3\u679c\u4e0e\u4eba\u7c7b\u6ce8\u91ca\u8005\u7684\u4e00\u81f4\u6027\u6700\u9ad8\u8fbe\u5230 85.58%\u3002", "conclusion": "\u6bd4\u55bb\u6027\u8bed\u8a00\u662f\u8861\u91cf\u6587\u5316\u63a8\u7406\u7684\u6709\u6548\u8bca\u65ad\u5de5\u5177\uff1a\u867d\u7136 LLMs \u901a\u5e38\u80fd\u591f\u89e3\u91ca\u6bd4\u55bb\u610f\u4e49\uff0c\u4f46\u5b83\u4eec\u5728\u6070\u5f53\u4f7f\u7528\u65b9\u9762\u9762\u4e34\u6311\u6218\u3002\u4e3a\u652f\u6301\u672a\u6765\u7814\u7a76\uff0c\u53d1\u5e03\u4e86 Kinayat \u6570\u636e\u96c6\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u57c3\u53ca\u963f\u62c9\u4f2f\u8bed\u4e60\u8bed\u6bd4\u55bb\u7406\u89e3\u548c\u5b9e\u7528\u6027\u4f7f\u7528\u7684 Egyptian Arabic idioms \u6570\u636e\u96c6\u3002"}}
{"id": "2510.23902", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.23902", "abs": "https://arxiv.org/abs/2510.23902", "authors": ["Jans Solano", "Diego Quiroz"], "title": "Stand, Walk, Navigate: Recovery-Aware Visual Navigation on a Low-Cost Wheeled Quadruped", "comment": "Accepted at the IROS 2025 Workshop on Wheeled-Legged Robots", "summary": "Wheeled-legged robots combine the efficiency of wheels with the obstacle\nnegotiation of legs, yet many state-of-the-art systems rely on costly actuators\nand sensors, and fall-recovery is seldom integrated, especially for\nwheeled-legged morphologies. This work presents a recovery-aware\nvisual-inertial navigation system on a low-cost wheeled quadruped. The proposed\nsystem leverages vision-based perception from a depth camera and deep\nreinforcement learning policies for robust locomotion and autonomous recovery\nfrom falls across diverse terrains. Simulation experiments show agile mobility\nwith low-torque actuators over irregular terrain and reliably recover from\nexternal perturbations and self-induced failures. We further show goal directed\nnavigation in structured indoor spaces with low-cost perception. Overall, this\napproach lowers the barrier to deploying autonomous navigation and robust\nlocomotion policies in budget-constrained robotic platforms.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u4f4e\u6210\u672c\u8f6e\u817f\u5f0f\u56db\u8db3\u673a\u5668\u4eba\uff0c\u96c6\u6210\u4e86\u89c6\u89c9\u60ef\u6027\u5bfc\u822a\u548c\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u5728\u590d\u6742\u5730\u5f62\u4e0a\u7684\u7a33\u5b9a\u8fd0\u52a8\u548c\u8dcc\u5012\u81ea\u6062\u590d\u80fd\u529b\uff0c\u5e76\u80fd\u5728\u5ba4\u5185\u73af\u5883\u4e2d\u8fdb\u884c\u76ee\u6807\u5bfc\u5411\u5bfc\u822a\uff0c\u964d\u4f4e\u4e86\u9884\u7b97\u53d7\u9650\u673a\u5668\u4eba\u7684\u90e8\u7f72\u95e8\u69db\u3002", "motivation": "\u73b0\u6709\u7684\u8f6e\u817f\u5f0f\u673a\u5668\u4eba\u867d\u7136\u7ed3\u5408\u4e86\u8f6e\u5f0f\u548c\u817f\u5f0f\u673a\u5668\u4eba\u7684\u4f18\u70b9\uff0c\u4f46\u901a\u5e38\u6210\u672c\u9ad8\u6602\u4e14\u7f3a\u4e4f\u8dcc\u5012\u6062\u590d\u80fd\u529b\uff0c\u5c24\u5176\u662f\u5728\u8f6e\u817f\u5f0f\u5f62\u6001\u4e0b\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u4f4e\u6210\u672c\u3001\u5177\u5907\u8dcc\u5012\u6062\u590d\u80fd\u529b\u7684\u8f6e\u817f\u5f0f\u673a\u5668\u4eba\u5bfc\u822a\u7cfb\u7edf\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4f4e\u6210\u672c\u8f6e\u817f\u5f0f\u56db\u8db3\u673a\u5668\u4eba\uff0c\u5e76\u96c6\u6210\u4e86\u4e00\u4e2a\u89c6\u89c9\u60ef\u6027\u5bfc\u822a\u7cfb\u7edf\u3002\u8be5\u7cfb\u7edf\u5229\u7528\u6df1\u5ea6\u76f8\u673a\u8fdb\u884c\u89c6\u89c9\u611f\u77e5\uff0c\u5e76\u7ed3\u5408\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u6765\u5b9e\u73b0\u9c81\u68d2\u7684\u8fd0\u52a8\u548c\u8dcc\u5012\u81ea\u6062\u590d\u3002\u901a\u8fc7\u6a21\u62df\u5b9e\u9a8c\u548c\u5728\u5ba4\u5185\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u6d4b\u8bd5\u6765\u9a8c\u8bc1\u5176\u6027\u80fd\u3002", "result": "\u6a21\u62df\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u80fd\u591f\u5728\u5d0e\u5c96\u5730\u5f62\u4e0a\u5b9e\u73b0\u4f4e\u626d\u77e9\u9a71\u52a8\u7684\u654f\u6377\u8fd0\u52a8\uff0c\u5e76\u80fd\u53ef\u9760\u5730\u4ece\u5916\u90e8\u5e72\u6270\u548c\u81ea\u8eab\u6545\u969c\u4e2d\u6062\u590d\u3002\u5728\u5ba4\u5185\u73af\u5883\u4e2d\uff0c\u8be5\u673a\u5668\u4eba\u80fd\u591f\u6267\u884c\u76ee\u6807\u5bfc\u5411\u5bfc\u822a\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u65b9\u6cd5\u901a\u8fc7\u96c6\u6210\u4f4e\u6210\u672c\u7684\u611f\u77e5\u548c\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u8f6e\u817f\u5f0f\u673a\u5668\u4eba\u7684\u9c81\u68d2\u8fd0\u52a8\u548c\u8dcc\u5012\u6062\u590d\u80fd\u529b\uff0c\u4e3a\u9884\u7b97\u53d7\u9650\u7684\u673a\u5668\u4eba\u5e73\u53f0\u63d0\u4f9b\u4e86\u7ecf\u6d4e\u9ad8\u6548\u7684\u81ea\u4e3b\u5bfc\u822a\u548c\u8fd0\u52a8\u7b56\u7565\u90e8\u7f72\u65b9\u6848\u3002"}}
{"id": "2510.24175", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.24175", "abs": "https://arxiv.org/abs/2510.24175", "authors": ["Nitin Shukla", "Alessandro Romeo", "Caterina Caravita", "Michael Redenti", "Radim Vavrik", "Lubomir Riha", "Andrea Mignone", "Marco Rossazza", "Stefano Truzzi", "Luca Tornatore", "Antonio Ragagnin", "Tiago Castro", "Geray S. Karademir", "Klaus Dolag", "Pranab J. Deka", "Fabio Bacchini", "Rostislav-Paul Wilhelm", "Daniele Gregori", "Elisabetta Boella"], "title": "Towards Exascale Computing for Astrophysical Simulation Leveraging the Leonardo EuroHPC System", "comment": null, "summary": "Developing and redesigning astrophysical, cosmological, and space plasma\nnumerical codes for existing and next-generation accelerators is critical for\nenabling large-scale simulations. To address these challenges, the SPACE Center\nof Excellence (SPACE-CoE) fosters collaboration between scientists, code\ndevelopers, and high-performance computing experts to optimize applications for\nthe exascale era. This paper presents our strategy and initial results on the\nLeonardo system at CINECA for three flagship codes, namely gPLUTO, OpenGadget3\nand iPIC3D, using profiling tools to analyze performance on single and multiple\nnodes. Preliminary tests show all three codes scale efficiently, reaching 80%\nscalability up to 1,024 GPUs.", "AI": {"tldr": "SPACE-CoE\u901a\u8fc7\u4f18\u5316gPLUTO\u3001OpenGadget3\u548ciPIC3D\u8fd9\u4e09\u4e2a\u65d7\u8230\u4ee3\u7801\uff0c\u4e3aexascale\u8ba1\u7b97\u505a\u51c6\u5907\uff0c\u5e76\u5728Leonardo\u7cfb\u7edf\u4e0a\u53d6\u5f97\u4e86\u826f\u597d\u7684\u6269\u5c55\u6027\u3002", "motivation": "\u4e3a\u4e86\u5728\u73b0\u6709\u548c\u4e0b\u4e00\u4ee3\u52a0\u901f\u5668\u4e0a\u5b9e\u73b0\u5927\u89c4\u6a21\u6a21\u62df\uff0c\u5fc5\u987b\u5f00\u53d1\u548c\u91cd\u65b0\u8bbe\u8ba1\u5929\u4f53\u7269\u7406\u5b66\u3001\u5b87\u5b99\u5b66\u548c\u7a7a\u95f4\u7b49\u79bb\u5b50\u4f53\u6570\u503c\u4ee3\u7801\u3002", "method": "\u5728CINECA\u7684Leonardo\u7cfb\u7edf\u4e0a\uff0c\u4f7f\u7528\u5206\u6790\u5de5\u5177\u5bf9gPLUTO\u3001OpenGadget3\u548ciPIC3D\u8fd9\u4e09\u4e2a\u65d7\u8230\u4ee3\u7801\u5728\u5355\u8282\u70b9\u548c\u591a\u8282\u70b9\u4e0a\u7684\u6027\u80fd\u8fdb\u884c\u4e86\u5206\u6790\u3002", "result": "\u521d\u6b65\u6d4b\u8bd5\u8868\u660e\uff0c\u6240\u6709\u4e09\u4e2a\u4ee3\u7801\u90fd\u5177\u6709\u9ad8\u6548\u7684\u6269\u5c55\u6027\uff0c\u5728\u9ad8\u8fbe1,024\u4e2aGPU\u4e0a\u5b9e\u73b0\u4e8680%\u7684\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "SPACE-CoE\u5728\u4f18\u5316\u65d7\u8230\u4ee3\u7801\u4ee5\u9002\u5e94exascale\u65f6\u4ee3\u65b9\u9762\u53d6\u5f97\u4e86\u521d\u6b65\u6210\u529f\uff0cgPLUTO\u3001OpenGadget3\u548ciPIC3D\u5728Leonardo\u7cfb\u7edf\u4e0a\u8868\u73b0\u51fa\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2510.23852", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2510.23852", "abs": "https://arxiv.org/abs/2510.23852", "authors": ["J. D\u00edaz", "J. Rodr\u00edguez-Fern\u00e1ndez", "J. Rubio-Zuazo"], "title": "Thickness dependent rare earth segregation in magnetron deposited NdCo$_{4.6}$ thin films studied by Xray reflectivity and Hard Xray photoemission", "comment": "12 pages, 14 figures, regular paper", "summary": "The magnetic anisotropy of amorphous NdCo$_{4.6}$ compounds deposited by\nmagnetron sputtering change with film thickness from in plane to out of plane\nanisotropy at thickness above 40 nm. Xray reflectivity measurements shows the\nprogressive formation of an additional layer in between the 3 nm thick Si\ncapping layer and the NdCo compound film. Hard Xray Photoemission spectroscoy\n(HAXPES) was used to analyze the composition and distribution of cobalt and\nneodymium at the top layers region of NdCo$_{4.6}$ films of thickness ranging\nfrom 5 nm to 65 nm using 7 keV, 10 keV and 13 keV incident photon energies,\nwith inelastic electron mean free paths ranging from 7.2 nm to 12.3 nm in\ncobalt. The atomic cobalt concentration of the alloy deduced from HAXPES\nmeasurements at the Nd 3d and Co 2p excitations results to be bellow the\nnominal value, changing with thickness and incident photon energy. This proves\na segregation of the rare earth at the surface of the NdCo$_{4.6}$ thin film\nwhich increases with thickness. The analysis of the background of the Co 2p and\nNd 3d peaks was consistent with this conclusion. This demonstrates that\nneodymium incorporation in the cobalt lattice have a cost in energy which can\nbe associated to strain due to the difference in volume between the two\nelements. The lowering of this strain energy will favor atomic anisotropic\nenvironments for neodymium that explains the perpendicular anisotropy and its\nthickness dependence of these thin film compounds.", "AI": {"tldr": "", "motivation": "\u975e\u6676\u6001NdCo4.6\u5316\u5408\u7269\u7684\u78c1\u5404\u5411\u5f02\u6027\u968f\u8584\u819c\u539a\u5ea6\u53d8\u5316\uff0c\u572840 nm\u4ee5\u4e0a\u4ece\u9762\u5185\u5404\u5411\u5f02\u6027\u8f6c\u53d8\u4e3a\u9762\u5916\u5404\u5411\u5f02\u6027\u3002", "method": "\u4f7f\u7528X\u5c04\u7ebf\u53cd\u5c04\u7387\u6d4b\u91cf\u548c\u786cX\u5c04\u7ebf\u5149\u7535\u5b50\u80fd\u8c31\uff08HAXPES\uff09\u5206\u6790\u4e86\u4e0d\u540c\u539a\u5ea6\uff085 nm\u81f365 nm\uff09\u7684NdCo4.6\u8584\u819c\u7684\u6210\u5206\u548c\u94b4\u3001\u9495\u7684\u5206\u5e03\u3002HAXPES\u5206\u6790\u4f7f\u75287 keV\u300110 keV\u548c13 keV\u7684\u5165\u5c04\u5149\u5b50\u80fd\u91cf\uff0c\u7535\u5b50\u5e73\u5747\u81ea\u7531\u7a0b\u4e3a7.2 nm\u81f312.3 nm\u3002", "result": "HAXPES\u6d4b\u91cf\u7ed3\u679c\u663e\u793a\uff0c\u94b4\u7684\u539f\u5b50\u6d53\u5ea6\u4f4e\u4e8e\u6807\u79f0\u503c\uff0c\u5e76\u968f\u539a\u5ea6\u548c\u5165\u5c04\u5149\u5b50\u80fd\u91cf\u53d8\u5316\uff0c\u8868\u660e\u7a00\u571f\u5143\u7d20\u5728NdCo4.6\u8584\u819c\u8868\u9762\u53d1\u751f\u504f\u6790\uff0c\u4e14\u8fd9\u79cd\u504f\u6790\u968f\u539a\u5ea6\u589e\u52a0\u800c\u589e\u5f3a\u3002\u80cc\u666f\u5206\u6790\u4e5f\u652f\u6301\u6b64\u7ed3\u8bba\u3002", "conclusion": "\u9495\u5728\u94b4\u6676\u683c\u4e2d\u7684\u63ba\u6742\u4f1a\u4ea7\u751f\u80fd\u91cf\u4ee3\u4ef7\uff0c\u8fd9\u53ef\u80fd\u4e0e\u4e24\u79cd\u5143\u7d20\u4f53\u79ef\u5dee\u5f02\u5f15\u8d77\u7684\u5e94\u53d8\u6709\u5173\u3002\u5e94\u53d8\u80fd\u7684\u964d\u4f4e\u6709\u5229\u4e8e\u9495\u7684\u539f\u5b50\u5404\u5411\u5f02\u6027\u73af\u5883\uff0c\u4ece\u800c\u89e3\u91ca\u4e86\u8584\u819c\u5316\u5408\u7269\u7684\u5782\u76f4\u5404\u5411\u5f02\u6027\u53ca\u5176\u539a\u5ea6\u4f9d\u8d56\u6027\u3002"}}
{"id": "2510.23971", "categories": ["cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2510.23971", "abs": "https://arxiv.org/abs/2510.23971", "authors": ["Zhuo-Hua Chen", "Hou-Jian Duan", "Ming-Xun Deng", "Rui-Qiang Wang"], "title": "Nonlinear Layer Hall Effect and Detection of the Hidden Berry Curvature Dipole in $\\mathcal{PT}$-Symmetric Antiferromagnetic Insulators", "comment": "4", "summary": "Recent experimental and theoretical studies have revealed the emergence of a\nlinear layer Hall effect (LHE) induced by hidden Berry curvature in\n\\textrm{MnBi}$_{2}$\\textrm{Te}$_{4}$ thin films. This phenomenon underscores\nthe layer degree of freedom as a novel mechanism for generating Hall transport\nin layered materials, providing a new pathway to probe and manipulate the\ninternal structure of fully compensated topological antiferromagnets (AFMs). In\nthis work, we predict a nonlinear LHE in $\\mathcal{PT}$-symmetric layered AFMs,\nwhich manifests as a detectable nonlinear Hall conductivity even with respect\nto the AFM order and odd with respect to the vertical electric field, in\ncontrast to the linear LHE. Furthermore, we demonstrate that the nonlinear Hall\ncurrents induced by the hidden BCD and quantum metric dipole (QMD) obey\ndistinct symmetries and flow in different directions. Our proposed nonlinear\nLHE establishes an experimentally advantageous framework for exclusively\nprobing the hidden BCD quantum geometry.", "AI": {"tldr": "MnBi2Te4\u8584\u819c\u4e2d\u53d1\u73b0\u4e86\u7531\u9690\u85cfBerry\u66f2\u7387\u5f15\u8d77\u7684\u7ebf\u6027\u970d\u5c14\u6548\u5e94\uff08LHE\uff09\u3002\u672c\u7814\u7a76\u9884\u6d4b\u4e86PT\u5bf9\u79f0\u5c42\u72b6\u53cd\u94c1\u78c1\u4f53\u4e2d\u7684\u975e\u7ebf\u6027LHE\uff0c\u5b83\u4e0e\u7ebf\u6027LHE\u4e0d\u540c\uff0c\u5373\u4f7f\u76f8\u5bf9\u4e8e\u53cd\u94c1\u78c1\u5e8f\u4e5f\u662f\u53ef\u68c0\u6d4b\u7684\uff0c\u5e76\u4e14\u76f8\u5bf9\u4e8e\u5782\u76f4\u7535\u573a\u662f\u5947\u51fd\u6570\u3002\u6211\u4eec\u8fd8\u8bc1\u660e\u4e86\u7531\u9690\u85cfBCD\u548c\u91cf\u5b50\u5ea6\u91cf\u5076\u6781\u5b50\uff08QMD\uff09\u5f15\u8d77\u7684\u975e\u7ebf\u6027\u970d\u5c14\u7535\u6d41\u9075\u5faa\u4e0d\u540c\u7684\u5bf9\u79f0\u6027\u5e76\u4e14\u6d41\u5411\u4e0d\u540c\u3002\u6240\u63d0\u51fa\u7684\u975e\u7ebf\u6027LHE\u4e3a\u4e13\u95e8\u63a2\u6d4b\u9690\u85cfBCD\u91cf\u5b50\u51e0\u4f55\u63d0\u4f9b\u4e86\u6709\u5229\u7684\u5b9e\u9a8c\u6846\u67b6\u3002", "motivation": "\u63a2\u7d22\u7531\u9690\u85cfBerry\u66f2\u7387\u5f15\u8d77\u7684\u5c42\u72b6\u970d\u5c14\u6548\u5e94\uff08LHE\uff09\u5728MnBi2Te4\u8584\u819c\u4e2d\u7684\u51fa\u73b0\uff0c\u5e76\u5c06\u5176\u4f5c\u4e3a\u4e00\u79cd\u63a2\u6d4b\u548c\u64cd\u63a7\u5168\u8865\u507f\u62d3\u6251\u53cd\u94c1\u78c1\u4f53\uff08AFM\uff09\u5185\u90e8\u7ed3\u6784\u7684\u65b0\u9014\u5f84\u3002\u672c\u7814\u7a76\u65e8\u5728\u9884\u6d4b\u5e76\u63ed\u793aPT\u5bf9\u79f0\u5c42\u72b6\u53cd\u94c1\u78c1\u4f53\u4e2d\u7684\u975e\u7ebf\u6027LHE\u73b0\u8c61\uff0c\u5e76\u63a2\u7d22\u5176\u4e0e\u7ebf\u6027LHE\u7684\u533a\u522b\u4ee5\u53ca\u5176\u4f5c\u4e3a\u63a2\u6d4b\u9690\u85cfBerry\u66f2\u7387\uff08BCD\uff09\u548c\u91cf\u5b50\u5ea6\u91cf\u5076\u6781\u5b50\uff08QMD\uff09\u7684\u6f5c\u529b\u3002", "method": "\u7406\u8bba\u9884\u6d4bPT\u5bf9\u79f0\u5c42\u72b6\u53cd\u94c1\u78c1\u4f53\u4e2d\u7684\u975e\u7ebf\u6027\u970d\u5c14\u6548\u5e94\uff08LHE\uff09\uff0c\u5e76\u5206\u6790\u5176\u76f8\u5bf9\u4e8e\u53cd\u94c1\u78c1\u5e8f\u548c\u5782\u76f4\u7535\u573a\u7684\u5bf9\u79f0\u6027\u3002\u901a\u8fc7\u5bf9\u6bd4\u5206\u6790\u975e\u7ebf\u6027\u970d\u5c14\u7535\u6d41\u7531\u9690\u85cfBCD\u548cQMD\u8bf1\u5bfc\u65f6\u7684\u4e0d\u540c\u5bf9\u79f0\u6027\u548c\u6d41\u52a8\u65b9\u5411\uff0c\u63d0\u51fa\u4e00\u79cd\u7528\u4e8e\u63a2\u6d4b\u9690\u85cfBCD\u91cf\u5b50\u51e0\u4f55\u7684\u5b9e\u9a8c\u6846\u67b6\u3002", "result": "\u9884\u6d4b\u5728PT\u5bf9\u79f0\u5c42\u72b6\u53cd\u94c1\u78c1\u4f53\u4e2d\u5b58\u5728\u975e\u7ebf\u6027LHE\uff0c\u8be5\u6548\u5e94\u76f8\u5bf9\u4e8e\u53cd\u94c1\u78c1\u5e8f\u662f\u53ef\u68c0\u6d4b\u7684\uff0c\u76f8\u5bf9\u4e8e\u5782\u76f4\u7535\u573a\u662f\u5947\u51fd\u6570\u3002\u8bc1\u660e\u4e86\u7531\u9690\u85cfBCD\u548cQMD\u8bf1\u5bfc\u7684\u975e\u7ebf\u6027\u970d\u5c14\u7535\u6d41\u9075\u5faa\u4e0d\u540c\u7684\u5bf9\u79f0\u6027\u5e76\u4e14\u6d41\u5411\u4e0d\u540c\u3002", "conclusion": "\u63d0\u51fa\u7684\u975e\u7ebf\u6027LHE\u4e3a\u63a2\u6d4b\u9690\u85cfBerry\u66f2\u7387\uff08BCD\uff09\u91cf\u5b50\u51e0\u4f55\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u9a8c\u4e0a\u6709\u5229\u7684\u6846\u67b6\uff0c\u5e76\u63ed\u793a\u4e86\u975e\u7ebf\u6027\u970d\u5c14\u7535\u6d41\u7531BCD\u548cQMD\u8bf1\u5bfc\u65f6\u7684\u533a\u522b\u3002"}}
{"id": "2510.23798", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23798", "abs": "https://arxiv.org/abs/2510.23798", "authors": ["Gauthier Grimmer", "Romain Wenger", "Cl\u00e9ment Flint", "Germain Forestier", "Gilles Rixhon", "Valentin Chardon"], "title": "A geometric and deep learning reproducible pipeline for monitoring floating anthropogenic debris in urban rivers using in situ cameras", "comment": null, "summary": "The proliferation of floating anthropogenic debris in rivers has emerged as a\npressing environmental concern, exerting a detrimental influence on\nbiodiversity, water quality, and human activities such as navigation and\nrecreation. The present study proposes a novel methodological framework for the\nmonitoring the aforementioned waste, utilising fixed, in-situ cameras. This\nstudy provides two key contributions: (i) the continuous quantification and\nmonitoring of floating debris using deep learning and (ii) the identification\nof the most suitable deep learning model in terms of accuracy and inference\nspeed under complex environmental conditions. These models are tested in a\nrange of environmental conditions and learning configurations, including\nexperiments on biases related to data leakage. Furthermore, a geometric model\nis implemented to estimate the actual size of detected objects from a 2D image.\nThis model takes advantage of both intrinsic and extrinsic characteristics of\nthe camera. The findings of this study underscore the significance of the\ndataset constitution protocol, particularly with respect to the integration of\nnegative images and the consideration of temporal leakage. In conclusion, the\nfeasibility of metric object estimation using projective geometry coupled with\nregression corrections is demonstrated. This approach paves the way for the\ndevelopment of robust, low-cost, automated monitoring systems for urban aquatic\nenvironments.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u56fa\u5b9a\u6444\u50cf\u5934\u548c\u6df1\u5ea6\u5b66\u4e60\u76d1\u6d4b\u6cb3\u6d41\u4e2d\u6f02\u6d6e\u5783\u573e\u7684\u65b0\u65b9\u6cd5\uff0c\u5e76\u80fd\u4f30\u8ba1\u5783\u573e\u7684\u5927\u5c0f\uff0c\u65e8\u5728\u4e3a\u57ce\u5e02\u6c34\u4f53\u73af\u5883\u63d0\u4f9b\u4f4e\u6210\u672c\u3001\u81ea\u52a8\u5316\u7684\u76d1\u6d4b\u7cfb\u7edf\u3002", "motivation": "\u6cb3\u6d41\u4e2d\u6f02\u6d6e\u5783\u573e\u7684\u589e\u591a\u5bf9\u751f\u7269\u591a\u6837\u6027\u3001\u6c34\u8d28\u4ee5\u53ca\u822a\u884c\u548c\u5a31\u4e50\u7b49\u4eba\u6587\u6d3b\u52a8\u9020\u6210\u4e86\u8d1f\u9762\u5f71\u54cd\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u76d1\u6d4b\u8fd9\u4e9b\u5783\u573e\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u56fa\u5b9a\u3001\u73b0\u573a\u6444\u50cf\u5934\u8fdb\u884c\u5783\u573e\u76d1\u6d4b\u7684\u65b0\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u5305\u62ec\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u8fdb\u884c\u8fde\u7eed\u91cf\u5316\u548c\u76d1\u6d4b\uff0c\u5e76\u8bc6\u522b\u5728\u590d\u6742\u73af\u5883\u6761\u4ef6\u4e0b\u51c6\u786e\u6027\u548c\u63a8\u7406\u901f\u5ea6\u6700\u4f73\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3002\u6b64\u5916\uff0c\u8fd8\u5b9e\u73b0\u4e86\u4e00\u4e2a\u51e0\u4f55\u6a21\u578b\uff0c\u5229\u7528\u6444\u50cf\u5934\u7684\u5185\u5728\u548c\u5916\u5728\u7279\u6027\u6765\u4f30\u8ba12D\u56fe\u50cf\u4e2d\u68c0\u6d4b\u5230\u7684\u7269\u4f53\u7684\u5b9e\u9645\u5c3a\u5bf8\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u6570\u636e\u96c6\u6784\u6210\u534f\u8bae\u7684\u91cd\u8981\u6027\uff0c\u7279\u522b\u662f\u8d1f\u7247\u56fe\u50cf\u7684\u6574\u5408\u548c\u65f6\u95f4\u6cc4\u6f0f\u7684\u8003\u8651\u3002\u7814\u7a76\u8bc1\u660e\u4e86\u4f7f\u7528\u6295\u5f71\u51e0\u4f55\u7ed3\u5408\u56de\u5f52\u6821\u6b63\u8fdb\u884c\u5ea6\u91cf\u5bf9\u8c61\u4f30\u8ba1\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u8bc1\u660e\u4e86\u4f7f\u7528\u6295\u5f71\u51e0\u4f55\u548c\u56de\u5f52\u6821\u6b63\u8fdb\u884c\u5ea6\u91cf\u5bf9\u8c61\u4f30\u8ba1\u662f\u53ef\u884c\u7684\uff0c\u5e76\u4e3a\u5f00\u53d1\u7a33\u5065\u3001\u4f4e\u6210\u672c\u3001\u81ea\u52a8\u5316\u7684\u57ce\u5e02\u6c34\u751f\u73af\u5883\u76d1\u6d4b\u7cfb\u7edf\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2510.23719", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.23719", "abs": "https://arxiv.org/abs/2510.23719", "authors": ["Markus Heinrich", "Jonas Haferkamp", "Ingo Roth", "Jonas Helsen"], "title": "Anti-concentration is (almost) all you need", "comment": "4+2 pages. Comments welcome", "summary": "Until very recently, it was generally believed that the (approximate)\n2-design property is strictly stronger than anti-concentration of random\nquantum circuits, mainly because it was shown that the latter anti-concentrate\nin logarithmic depth, while the former generally need linear depth circuits.\nThis belief was disproven by recent results which show that so-called\nrelative-error approximate unitary designs can in fact be generated in\nlogarithmic depth, implying anti-concentration. Their result does however not\napply to ordinary local random circuits, a gap which we close in this paper, at\nleast for 2-designs. More precisely, we show that anti-concentration of local\nrandom quantum circuits already implies that they form relative-error\napproximate state 2-designs, making them equivalent properties for these\nensembles. Our result holds more generally for any random circuit which is\ninvariant under local (single-qubit) unitaries, independent of the\narchitecture.", "AI": {"tldr": "\u968f\u673a\u91cf\u5b50\u7535\u8def\u7684 anti-concentration \u6027\u8d28\u548c 2-design \u6027\u8d28\u88ab\u8bc1\u660e\u662f\u7b49\u4ef7\u7684\uff0c\u5c24\u5176\u662f\u5728\u5177\u6709\u5c40\u90e8\u9149\u4e0d\u53d8\u6027\u7684\u968f\u673a\u7535\u8def\u4e0b\u3002", "motivation": "\u7ea0\u6b63\u5148\u524d\u8ba4\u4e3a 2-design \u6027\u8d28\u4e25\u683c\u5f3a\u4e8e anti-concentration \u6027\u8d28\u7684\u666e\u904d\u770b\u6cd5\uff0c\u5e76\u586b\u8865\u5148\u524d\u7814\u7a76\u5728\u5c40\u90e8\u968f\u673a\u7535\u8def\u65b9\u9762\u7684\u7a7a\u767d\u3002", "method": "\u8bc1\u660e anti-concentration \u6027\u8d28\u8574\u542b\u7740\u76f8\u5bf9\u8bef\u5dee\u8fd1\u4f3c\u72b6\u6001 2-design \u6027\u8d28\uff0c\u8bc1\u660e\u4e86\u4e24\u8005\u5bf9\u4e8e\u5177\u6709\u5c40\u90e8\u9149\u4e0d\u53d8\u6027\u7684\u968f\u673a\u7535\u8def\u662f\u7b49\u4ef7\u7684\u3002", "result": "\u8bc1\u660e\u4e86\u5c40\u90e8\u968f\u673a\u91cf\u5b50\u7535\u8def\u7684 anti-concentration \u6027\u8d28\u5df2\u7ecf\u8574\u542b\u4e86\u5b83\u4eec\u5f62\u6210\u76f8\u5bf9\u8bef\u5dee\u8fd1\u4f3c\u72b6\u6001 2-design \u6027\u8d28\uff0c\u8bc1\u660e\u4e86\u8fd9\u4e24\u8005\u5bf9\u4e8e\u8fd9\u4e9b\u96c6\u5408\u662f\u7b49\u4ef7\u7684\u3002", "conclusion": "\u5bf9\u4e8e\u5177\u6709\u5c40\u90e8\u9149\u4e0d\u53d8\u6027\u7684\u968f\u673a\u7535\u8def\uff0canti-concentration \u6027\u8d28\u548c\uff08\u76f8\u5bf9\u8bef\u5dee\u8fd1\u4f3c\uff092-design \u6027\u8d28\u662f\u7b49\u4ef7\u7684\u3002"}}
{"id": "2510.24203", "categories": ["cs.LO", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.24203", "abs": "https://arxiv.org/abs/2510.24203", "authors": ["Lukas Bartl", "Julian Linne", "Kirstin Peters"], "title": "Fault-Tolerant Multiparty Session Types with Global Escape Loops", "comment": "In Proceedings EXPRESS/SOS 2025, arXiv:2510.23211", "summary": "Multiparty session types are designed to abstractly capture the structure of\ncommunication protocols and verify behavioural properties. One important such\nproperty is progress, i.e., the absence of deadlock. Distributed algorithms\noften resemble multiparty communication protocols. But proving their\nproperties, in particular termination that is closely related to progress, can\nbe elaborate. Since distributed algorithms are often designed to cope with\nfaults, a first step towards using session types to verify distributed\nalgorithms is to integrate fault-tolerance.\n  We extend FTMPST (a version of fault-tolerant multiparty session types with\nfailure patterns to represent system requirements for system failures such as\nunreliable communication and process crashes) by a novel, fault-tolerant loop\nconstruct with global escapes that does not require global coordination. Each\nprocess runs its own local version of the loop. If a process finds a solution\nto the considered problem, it does not only terminate its own loop but also\ninforms the other participants via exit-messages. Upon receiving an\nexit-message, a process immediately terminates its algorithm. To increase\nefficiency and model standard fault-tolerant algorithms, these messages are\nnon-blocking, i.e., a process may continue until a possibly delayed\nexit-message is received. To illustrate our approach, we analyse a variant of\nthe well-known rotating coordinator algorithm by Chandra and Toueg.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u65e0\u5168\u5c40\u534f\u8c03\u7684\u5bb9\u9519\u5faa\u73af\u7ed3\u6784\uff0c\u7528\u4e8e\u5728\u591a\u65b9\u4f1a\u8bdd\u7c7b\u578b\u4e2d\u5904\u7406\u5206\u5e03\u5f0f\u7b97\u6cd5\u4e2d\u7684\u6545\u969c\u548c\u8fdb\u7a0b\u5d29\u6e83\uff0c\u5e76\u4ee5\u4e00\u4e2a\u5177\u4f53\u7684\u7b97\u6cd5\u5206\u6790\u4e3a\u4f8b\u8fdb\u884c\u4e86\u8bf4\u660e\u3002", "motivation": "\u591a\u65b9\u4f1a\u8bdd\u7c7b\u578b\u5728\u901a\u4fe1\u534f\u8bae\u62bd\u8c61\u548c\u884c\u4e3a\u9a8c\u8bc1\u65b9\u9762\u53d1\u6325\u7740\u91cd\u8981\u4f5c\u7528\uff0c\u4f46\u5c06\u8fd9\u4e9b\u7c7b\u578b\u5e94\u7528\u4e8e\u9700\u8981\u5bb9\u9519\u7684\u5206\u5e03\u5f0f\u7b97\u6cd5\uff0c\u7279\u522b\u662f\u8bc1\u660e\u5176\u7ec8\u6b62\u6027\uff0c\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002\u56e0\u6b64\uff0c\u6709\u5fc5\u8981\u5c06\u5bb9\u9519\u673a\u5236\u6574\u5408\u5230\u4f1a\u8bdd\u7c7b\u578b\u4e2d\uff0c\u4ee5\u5904\u7406\u5206\u5e03\u5f0f\u7b97\u6cd5\u4e2d\u7684\u6545\u969c\u3002", "method": "\u5728\u73b0\u6709\u7684\u5bb9\u9519\u591a\u65b9\u4f1a\u8bdd\u7c7b\u578b\uff08FTMPST\uff09\u57fa\u7840\u4e0a\uff0c\u63d0\u51fa\u5e76\u5b9e\u73b0\u4e86\u4e00\u79cd\u65b0\u7684\u5bb9\u9519\u5faa\u73af\u7ed3\u6784\uff0c\u8be5\u7ed3\u6784\u652f\u6301\u5168\u5c40\u51fa\u53e3\uff0c\u4e14\u4e0d\u9700\u8981\u5168\u5c40\u534f\u8c03\u3002\u6bcf\u4e2a\u8fdb\u7a0b\u72ec\u7acb\u6267\u884c\u5c40\u90e8\u5faa\u73af\uff0c\u5e76\u5728\u627e\u5230\u89e3\u51b3\u65b9\u6848\u65f6\u901a\u8fc7\u975e\u963b\u585e\u7684\u9000\u51fa\u6d88\u606f\u901a\u77e5\u5176\u4ed6\u53c2\u4e0e\u8005\uff0c\u5176\u4ed6\u53c2\u4e0e\u8005\u6536\u5230\u6d88\u606f\u540e\u7acb\u5373\u7ec8\u6b62\u3002\u5bf9 Chandra \u548c Toueg \u7684\u65cb\u8f6c\u534f\u8c03\u5668\u7b97\u6cd5\u8fdb\u884c\u4e86\u5206\u6790\uff0c\u4ee5\u5c55\u793a\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u65e0\u9700\u5168\u5c40\u534f\u8c03\u7684\u5bb9\u9519\u5faa\u73af\u7ed3\u6784\uff0c\u53ef\u4ee5\u5904\u7406\u8fdb\u7a0b\u5d29\u6e83\u548c\u4e0d\u53ef\u9760\u901a\u4fe1\u7b49\u6545\u969c\u3002\u901a\u8fc7\u975e\u963b\u585e\u7684\u9000\u51fa\u6d88\u606f\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u7b97\u6cd5\u7684\u6709\u6548\u7ec8\u6b62\u3002\u901a\u8fc7\u5bf9\u65cb\u8f6c\u534f\u8c03\u5668\u7b97\u6cd5\u7684\u5206\u6790\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u6210\u529f\u5730\u5c06\u5bb9\u9519\u80fd\u529b\u6269\u5c55\u5230\u4e86\u591a\u65b9\u4f1a\u8bdd\u7c7b\u578b\u4e2d\uff0c\u901a\u8fc7\u5f15\u5165\u4e00\u79cd\u65b0\u7684\u5bb9\u9519\u5faa\u73af\u7ed3\u6784\uff0c\u89e3\u51b3\u4e86\u5206\u5e03\u5f0f\u7b97\u6cd5\u4e2d\u7684\u6b7b\u9501\u548c\u6545\u969c\u95ee\u9898\uff0c\u4e3a\u4f7f\u7528\u4f1a\u8bdd\u7c7b\u578b\u9a8c\u8bc1\u5206\u5e03\u5f0f\u7b97\u6cd5\u63d0\u4f9b\u4e86\u65b0\u7684\u9014\u5f84\u3002"}}
{"id": "2510.23820", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.23820", "abs": "https://arxiv.org/abs/2510.23820", "authors": ["Shahab Jahanbazi", "Mateen Ashraf", "Onel L. A. L\u00f3pez"], "title": "MDP-based Energy-aware Task Scheduling for Battery-less IoT", "comment": "13 pages, 11 figures", "summary": "Realizing high long-term task completion rates represents a fundamental\nchallenge in battery-less Internet of Things (IoT) devices powered by ambient\nenergy harvesting. This difficulty is primarily due to the stochastic and\ntime-varying characteristics of the available energy, which significantly\ncomplicate the design of optimal task scheduling policies. In this paper, we\nconsider a battery-less IoT device that must periodically report sensing\nmeasurements to a monitoring center. We adopt the Markov decision process (MDP)\nframework to handle energy variability while aiming to maximize the long-term\ntask completion rate. For this, we first identify its components and then\ndefine two appropriate reward functions. We demonstrate the inherent properties\nassociated with the MDP formulation and the related optimal policy.\nSubsequently, we solve the resulting optimization problem, leading to the\noptimal stationary threshold-based (OSTB) scheduling. Simulation results\ndemonstrate that OSTB outperforms the well-known ``as late as possible'' (ALAP)\nscheduling strategy. For instance, an $8.6\\%$ increase in the task completion\nrate, along with a $65\\%$ reduction in power failures and a $86.29\\%$ decrease\nin execution delays during task execution are registered assuming a $4.7$ mF\ncapacitor.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u7535\u6c60\u5bff\u547d\u6709\u9650\u7684\u7269\u8054\u7f51\u8bbe\u5907\u7684\u6700\u4f73\u56fa\u5b9a\u9608\u503c\uff08OSTB\uff09\u8c03\u5ea6\u7b56\u7565\uff0c\u4ee5\u6700\u5927\u9650\u5ea6\u5730\u63d0\u9ad8\u4efb\u52a1\u5b8c\u6210\u7387\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u4f18\u4e8e\u201c\u5c3d\u53ef\u80fd\u665a\u201d\uff08ALAP\uff09\u7684\u8c03\u5ea6\u7b56\u7565\u3002", "motivation": "\u7535\u6c60\u5bff\u547d\u6709\u9650\u7684\u7269\u8054\u7f51\u8bbe\u5907\u5728\u957f\u671f\u4efb\u52a1\u5b8c\u6210\u7387\u65b9\u9762\u9762\u4e34\u6311\u6218\uff0c\u4e3b\u8981\u662f\u7531\u4e8e\u80fd\u91cf\u6536\u96c6\u7684\u968f\u673a\u6027\u548c\u65f6\u53d8\u6027\uff0c\u8fd9\u4f7f\u5f97\u8bbe\u8ba1\u6700\u4f18\u4efb\u52a1\u8c03\u5ea6\u7b56\u7565\u53d8\u5f97\u590d\u6742\u3002", "method": "\u8bba\u6587\u91c7\u7528\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08MDP\uff09\u6846\u67b6\u6765\u5904\u7406\u80fd\u91cf\u53d8\u5f02\u6027\uff0c\u4ee5\u6700\u5927\u9650\u5ea6\u5730\u63d0\u9ad8\u957f\u671f\u4efb\u52a1\u5b8c\u6210\u7387\u3002\u7814\u7a76\u4eba\u5458\u786e\u5b9a\u4e86MDP\u7684\u7ec4\u6210\u90e8\u5206\uff0c\u5b9a\u4e49\u4e86\u4e24\u4e2a\u5408\u9002\u7684\u5956\u52b1\u51fd\u6570\uff0c\u5e76\u89e3\u51b3\u4e86\u7531\u6b64\u4ea7\u751f\u7684\u4f18\u5316\u95ee\u9898\uff0c\u4ece\u800c\u5f97\u51fa\u4e86OSTB\u8c03\u5ea6\u3002", "result": "\u6a21\u62df\u7ed3\u679c\u663e\u793a\uff0cOSTB\u7b56\u7565\u5728\u4efb\u52a1\u5b8c\u6210\u7387\u65b9\u9762\u6bd4ALAP\u7b56\u7565\u63d0\u9ad8\u4e868.6%\uff0c\u5728\u65ad\u7535\u65b9\u9762\u51cf\u5c11\u4e8665%\uff0c\u5728\u4efb\u52a1\u6267\u884c\u5ef6\u8fdf\u65b9\u9762\u51cf\u5c11\u4e8686.29%\uff0c\u7279\u522b\u662f\u5728\u4f7f\u7528\u4e864.7 mF\u7684\u7535\u5bb9\u5668\u7684\u60c5\u51b5\u4e0b\u3002", "conclusion": "OSTB\u8c03\u5ea6\u7b56\u7565\u80fd\u591f\u6709\u6548\u89e3\u51b3\u7535\u6c60\u5bff\u547d\u6709\u9650\u7684\u7269\u8054\u7f51\u8bbe\u5907\u7684\u4efb\u52a1\u8c03\u5ea6\u95ee\u9898\uff0c\u5e76\u80fd\u63d0\u9ad8\u4efb\u52a1\u5b8c\u6210\u7387\uff0c\u51cf\u5c11\u65ad\u7535\u548c\u6267\u884c\u5ef6\u8fdf\u3002"}}
{"id": "2510.23837", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.23837", "abs": "https://arxiv.org/abs/2510.23837", "authors": ["Ali Amhaz", "Shreya Khisa", "Mohamed Elhattab", "Chadi Assi", "Sanaa Sharafeddine"], "title": "Coordinated Multipoint Transmission in Pinching Antenna Systems", "comment": null, "summary": "We study a coordinated multi-point (CoMP) transmission where two base\nstations (BSs), each supported by a pinching antenna system (PASS), are\ndeployed to jointly serve communication users under spatial division multiple\naccess (SDMA) technology. Pinching Antenna technology was introduced as a\npromising solution to overcome the large-scale fading that has been shown to be\nan impediment in multiple-input multiple-output (MIMO) systems. To realize the\nadvantages of this technology in CoMP systems, which suffer from an upperbound\nrate limitation when traditional uniform linear arrays (ULAs) are adopted, we\nformulate an optimization problem with the aim of maximizing the achievable sum\nrate by jointly determining the transmit beamforming vectors and pinching\nlocations on the waveguides while respecting the quality of service (QoS)\nrequirements of users. This problem is inherently non-convex due to the strong\ncoupling among its decision parameters, making it challenging to solve using\ntraditional optimization methods. Thus, we utilize a gradient-based\nmeta-learning (GML) strategy specifically designed for large-scale optimization\ntasks. Finally, numerical analysis demonstrates the effectiveness of the\nproposed GML approach, achieving 92 percent of the optimal solution, and the\nsuperiority of the solution presented compared to other benchmarks. In\naddition, it achieves a higher upper bound on the achievable rate compared to\nconventional CoMP systems.", "AI": {"tldr": "\u901a\u8fc7\u7ed3\u5408\u591a\u70b9\u534f\u540c\u4f20\u8f93\uff08CoMP\uff09\u6280\u672f\u548c the Pinching Antenna System (PASS)\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4f18\u5316\u65b9\u6cd5\u6765\u63d0\u9ad8\u901a\u4fe1\u901f\u7387\uff0c\u5e76\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728CoMP\u7cfb\u7edf\u4e2d\u7684\u901f\u7387\u9650\u5236\u95ee\u9898\u3002", "motivation": "\u4e3a\u4e86\u514b\u670dMIMO\u7cfb\u7edf\u4e2d\u7684\u5927\u89c4\u6a21\u8870\u843d\u95ee\u9898\uff0c\u5e76\u63d0\u9ad8\u5728CoMP\u7cfb\u7edf\u4e2d\u91c7\u7528\u4f20\u7edf\u5747\u5300\u7ebf\u6027\u9635\u5217\uff08ULA\uff09\u65f6\u901f\u7387\u4e0a\u9650\u53d7\u9650\u7684\u95ee\u9898\uff0c\u672c\u7814\u7a76\u65e8\u5728\u5229\u7528Pinching Antenna\u6280\u672f\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u4f18\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u8054\u5408\u786e\u5b9a\u4f20\u8f93\u6ce2\u675f\u6210\u5f62\u5411\u91cf\u548c\u6ce2\u5bfc\u4e0a\u7684 the pinching \u4f4d\u7f6e\u6765\u6700\u5927\u5316\u53ef\u5b9e\u73b0\u7684\u548c\u901f\u7387\uff0c\u540c\u65f6\u6ee1\u8db3\u7528\u6237\u7684\u670d\u52a1\u8d28\u91cf\uff08QoS\uff09\u8981\u6c42\u3002\u7531\u4e8e\u51b3\u7b56\u53c2\u6570\u4e4b\u95f4\u5b58\u5728\u8026\u5408\uff0c\u8be5\u95ee\u9898\u662f\u975e\u51f8\u7684\u3002\u56e0\u6b64\uff0c\u91c7\u7528\u68af\u5ea6\u5143\u5b66\u4e60\uff08GML\uff09\u7b56\u7565\u6765\u89e3\u51b3\u3002", "result": "\u63d0\u51fa\u7684GML\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6700\u4f18\u89e3\u768492%\uff0c\u4f18\u4e8e\u5176\u4ed6\u57fa\u51c6\u65b9\u6cd5\uff0c\u5e76\u5b9e\u73b0\u4e86\u6bd4\u4f20\u7edfCoMP\u7cfb\u7edf\u66f4\u9ad8\u7684\u53ef\u5b9e\u73b0\u901f\u7387\u4e0a\u9650\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684GML\u65b9\u6cd5\u5728CoMP\u7cfb\u7edf\u4e2d\u662f\u6709\u6548\u4e14\u4f18\u8d8a\u7684\uff0c\u80fd\u591f\u63d0\u9ad8\u901a\u4fe1\u901f\u7387\u5e76\u514b\u670d\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2510.23622", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.23622", "abs": "https://arxiv.org/abs/2510.23622", "authors": ["Alyssa Gerhart", "Balaji Iyangar"], "title": "Adversarially-Aware Architecture Design for Robust Medical AI Systems", "comment": null, "summary": "Adversarial attacks pose a severe risk to AI systems used in healthcare,\ncapable of misleading models into dangerous misclassifications that can delay\ntreatments or cause misdiagnoses. These attacks, often imperceptible to human\nperception, threaten patient safety, particularly in underserved populations.\nOur study explores these vulnerabilities through empirical experimentation on a\ndermatological dataset, where adversarial methods significantly reduce\nclassification accuracy. Through detailed threat modeling, experimental\nbenchmarking, and model evaluation, we demonstrate both the severity of the\nthreat and the partial success of defenses like adversarial training and\ndistillation. Our results show that while defenses reduce attack success rates,\nthey must be balanced against model performance on clean data. We conclude with\na call for integrated technical, ethical, and policy-based approaches to build\nmore resilient, equitable AI in healthcare.", "AI": {"tldr": "\u5bf9\u6297\u6027\u653b\u51fb\u5bf9\u533b\u7597AI\u6784\u6210\u4e25\u91cd\u5a01\u80c1\uff0c\u53ef\u80fd\u5bfc\u81f4\u8bef\u8bca\u548c\u5ef6\u8bef\u6cbb\u7597\uff0c\u5c24\u5176\u5f71\u54cd\u5f31\u52bf\u7fa4\u4f53\u3002\u672c\u7814\u7a76\u901a\u8fc7\u76ae\u80a4\u75c5\u5b66\u6570\u636e\u96c6\u7684\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86\u8fd9\u4e9b\u653b\u51fb\u7684\u5371\u5bb3\u6027\uff0c\u5e76\u8bc4\u4f30\u4e86\u5bf9\u6297\u6027\u8bad\u7ec3\u548c\u84b8\u998f\u7b49\u9632\u5fa1\u63aa\u65bd\u7684\u6548\u679c\u3002\u7ed3\u679c\u8868\u660e\uff0c\u9632\u5fa1\u63aa\u65bd\u867d\u7136\u80fd\u964d\u4f4e\u653b\u51fb\u6210\u529f\u7387\uff0c\u4f46\u9700\u6743\u8861\u5bf9\u6a21\u578b\u5728\u5e72\u51c0\u6570\u636e\u4e0a\u6027\u80fd\u7684\u5f71\u54cd\u3002\u7814\u7a76\u547c\u5401\u7ed3\u5408\u6280\u672f\u3001\u4f26\u7406\u548c\u653f\u7b56\u624b\u6bb5\uff0c\u6784\u5efa\u66f4\u5177\u97e7\u6027\u548c\u516c\u5e73\u6027\u7684\u533b\u7597AI\u3002", "motivation": "\u533b\u7597AI\u7cfb\u7edf\u9762\u4e34\u4e25\u5cfb\u7684\u5bf9\u6297\u6027\u653b\u51fb\u98ce\u9669\uff0c\u53ef\u80fd\u5bfc\u81f4\u5371\u9669\u7684\u8bef\u5206\u7c7b\uff0c\u5ef6\u8bef\u6cbb\u7597\u6216\u9020\u6210\u8bef\u8bca\uff0c\u5a01\u80c1\u60a3\u8005\u5b89\u5168\uff0c\u5c24\u5176\u662f\u5728\u670d\u52a1\u4e0d\u8db3\u7684\u7fa4\u4f53\u4e2d\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u8ba8\u8fd9\u4e9b\u6f0f\u6d1e\u3002", "method": "\u901a\u8fc7\u5728\u76ae\u80a4\u75c5\u5b66\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u8bc1\u5b9e\u9a8c\uff0c\u8fd0\u7528\u8be6\u7ec6\u7684\u5a01\u80c1\u5efa\u6a21\u3001\u5b9e\u9a8c\u57fa\u51c6\u6d4b\u8bd5\u548c\u6a21\u578b\u8bc4\u4f30\u65b9\u6cd5\uff0c\u6765\u5206\u6790\u548c\u5e94\u5bf9\u5bf9\u6297\u6027\u653b\u51fb\u3002", "result": "\u5bf9\u6297\u6027\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u5206\u7c7b\u51c6\u786e\u6027\uff0c\u9632\u5fa1\u63aa\u65bd\uff08\u5982\u5bf9\u6297\u6027\u8bad\u7ec3\u548c\u84b8\u998f\uff09\u5728\u964d\u4f4e\u653b\u51fb\u6210\u529f\u7387\u65b9\u9762\u53d6\u5f97\u90e8\u5206\u6210\u529f\uff0c\u4f46\u9700\u8981\u5728\u6a21\u578b\u5728\u5e72\u51c0\u6570\u636e\u4e0a\u7684\u6027\u80fd\u548c\u9632\u5fa1\u80fd\u529b\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "conclusion": "\u5c3d\u7ba1\u9632\u5fa1\u63aa\u65bd\u80fd\u964d\u4f4e\u653b\u51fb\u6210\u529f\u7387\uff0c\u4f46\u5fc5\u987b\u5728\u6a21\u578b\u5728\u5e72\u51c0\u6570\u636e\u4e0a\u7684\u6027\u80fd\u4e0e\u9632\u5fa1\u80fd\u529b\u4e4b\u95f4\u8fdb\u884c\u6743\u8861\u3002\u7814\u7a76\u6700\u540e\u547c\u5401\u91c7\u7528\u6280\u672f\u3001\u4f26\u7406\u548c\u653f\u7b56\u76f8\u7ed3\u5408\u7684\u7efc\u5408\u65b9\u6cd5\uff0c\u4ee5\u6784\u5efa\u66f4\u5177\u97e7\u6027\u3001\u66f4\u52a0\u516c\u5e73\u7684\u533b\u7597AI\u3002"}}
{"id": "2510.23744", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23744", "abs": "https://arxiv.org/abs/2510.23744", "authors": ["Eline M. Bovy", "Caleb Probine", "Marnix Suilen", "Ufuk Topcu", "Nils Jansen"], "title": "Multi-Environment POMDPs: Discrete Model Uncertainty Under Partial Observability", "comment": "Accepted at NeurIPS 2025", "summary": "Multi-environment POMDPs (ME-POMDPs) extend standard POMDPs with discrete\nmodel uncertainty. ME-POMDPs represent a finite set of POMDPs that share the\nsame state, action, and observation spaces, but may arbitrarily vary in their\ntransition, observation, and reward models. Such models arise, for instance,\nwhen multiple domain experts disagree on how to model a problem. The goal is to\nfind a single policy that is robust against any choice of POMDP within the set,\ni.e., a policy that maximizes the worst-case reward across all POMDPs. We\ngeneralize and expand on existing work in the following way. First, we show\nthat ME-POMDPs can be generalized to POMDPs with sets of initial beliefs, which\nwe call adversarial-belief POMDPs (AB-POMDPs). Second, we show that any\narbitrary ME-POMDP can be reduced to a ME-POMDP that only varies in its\ntransition and reward functions or only in its observation and reward\nfunctions, while preserving (optimal) policies. We then devise exact and\napproximate (point-based) algorithms to compute robust policies for AB-POMDPs,\nand thus ME-POMDPs. We demonstrate that we can compute policies for standard\nPOMDP benchmarks extended to the multi-environment setting.", "AI": {"tldr": "ME-POMDPs \u63d0\u51fa\u4e86\u4e00\u79cd\u5904\u7406\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bfb\u627e\u4e00\u4e2a\u5728\u6240\u6709\u53ef\u80fd\u6a21\u578b\u4e2d\u90fd\u80fd\u6700\u5927\u5316\u6700\u574f\u60c5\u51b5\u5956\u52b1\u7684\u5355\u4e00\u7b56\u7565\uff0c\u5e76\u63d0\u51fa\u4e86\u89e3\u51b3\u6b64\u7c7b\u95ee\u9898\u7684\u7b97\u6cd5\u3002", "motivation": "\u89e3\u51b3\u6807\u51c6POMDPs\u5728\u5904\u7406\u591a\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u65f6\u7684\u5c40\u9650\u6027\uff0c\u5c24\u5176\u662f\u5728\u4e0d\u540c\u9886\u57df\u4e13\u5bb6\u5bf9\u6a21\u578b\u5b58\u5728\u5206\u6b67\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u5c06ME-POMDPs\u63a8\u5e7f\u5230AB-POMDPs\uff0c\u5e76\u63d0\u51fa\u5c06\u4efb\u610fME-POMDPs\u7b80\u5316\u4e3a\u4ec5\u5728\u8f6c\u79fb\u548c\u5956\u52b1\u51fd\u6570\u6216\u4ec5\u5728\u89c2\u6d4b\u548c\u5956\u52b1\u51fd\u6570\u4e0a\u6709\u6240\u4e0d\u540c\u7684ME-POMDPs\u3002\u5f00\u53d1\u4e86\u7528\u4e8e\u8ba1\u7b97\u9c81\u68d2\u7b56\u7565\u7684\u7cbe\u786e\u548c\u8fd1\u4f3c\u7b97\u6cd5\u3002", "result": "\u6210\u529f\u4e3a\u6269\u5c55\u5230\u591a\u73af\u5883\u8bbe\u7f6e\u7684\u6807\u51c6POMDPs\u57fa\u51c6\u8ba1\u7b97\u4e86\u7b56\u7565\u3002", "conclusion": "ME-POMDPs\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u6765\u5904\u7406\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u4e14\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u80fd\u591f\u8ba1\u7b97\u51fa\u9c81\u68d2\u7684\u7b56\u7565\u3002"}}
{"id": "2510.23659", "categories": ["cs.LG", "cs.CV", "cs.ET"], "pdf": "https://arxiv.org/pdf/2510.23659", "abs": "https://arxiv.org/abs/2510.23659", "authors": ["Md. Farhan Shahriyar", "Gazi Tanbhir", "Abdullah Md Raihan Chy"], "title": "Quantum Machine Learning for Image Classification: A Hybrid Model of Residual Network with Quantum Support Vector Machine", "comment": null, "summary": "Recently, there has been growing attention on combining quantum machine\nlearning (QML) with classical deep learning approaches, as computational\ntechniques are key to improving the performance of image classification tasks.\nThis study presents a hybrid approach that uses ResNet-50 (Residual Network)\nfor feature extraction and Quantum Support Vector Machines (QSVM) for\nclassification in the context of potato disease detection. Classical machine\nlearning as well as deep learning models often struggle with high-dimensional\nand complex datasets, necessitating advanced techniques like quantum computing\nto improve classification efficiency. In our research, we use ResNet-50 to\nextract deep feature representations from RGB images of potato diseases. These\nfeatures are then subjected to dimensionality reduction using Principal\nComponent Analysis (PCA). The resulting features are processed through QSVM\nmodels which apply various quantum feature maps such as ZZ, Z, and Pauli-X to\ntransform classical data into quantum states. To assess the model performance,\nwe compared it with classical machine learning algorithms such as Support\nVector Machine (SVM) and Random Forest (RF) using five-fold stratified\ncross-validation for comprehensive evaluation. The experimental results\ndemonstrate that the Z-feature map-based QSVM outperforms classical models,\nachieving an accuracy of 99.23 percent, surpassing both SVM and RF models. This\nresearch highlights the advantages of integrating quantum computing into image\nclassification and provides a potential disease detection solution through\nhybrid quantum-classical modeling.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408ResNet-50\u548c\u91cf\u5b50\u652f\u6301\u5411\u91cf\u673a\uff08QSVM\uff09\u7684\u6df7\u5408\u65b9\u6cd5\uff0c\u7528\u4e8e\u9a6c\u94c3\u85af\u75c5\u5bb3\u56fe\u50cf\u5206\u7c7b\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u96be\u4ee5\u5904\u7406\u9ad8\u7ef4\u590d\u6742\u6570\u636e\u96c6\uff0c\u56e0\u6b64\u9700\u8981\u91cf\u5b50\u8ba1\u7b97\u7b49\u5148\u8fdb\u6280\u672f\u6765\u63d0\u9ad8\u5206\u7c7b\u6548\u7387\u3002", "method": "\u4f7f\u7528ResNet-50\u63d0\u53d6\u56fe\u50cf\u7279\u5f81\uff0cPCA\u964d\u7ef4\uff0c\u7136\u540e\u4f7f\u7528\u91cf\u5b50\u7279\u5f81\u6620\u5c04\uff08ZZ, Z, Pauli-X\uff09\u7684QSVM\u8fdb\u884c\u5206\u7c7b\uff0c\u5e76\u4e0eSVM\u548cRF\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u57fa\u4e8eZ\u7279\u5f81\u56fe\u7684QSVM\u5728\u9a6c\u94c3\u85af\u75c5\u5bb3\u68c0\u6d4b\u4e2d\u8fbe\u523099.23%\u7684\u51c6\u786e\u7387\uff0c\u4f18\u4e8eSVM\u548cRF\u3002", "conclusion": "\u5c06\u91cf\u5b50\u8ba1\u7b97\u4e0e\u56fe\u50cf\u5206\u7c7b\u76f8\u7ed3\u5408\u7684\u6df7\u5408\u65b9\u6cd5\u5728\u9a6c\u94c3\u85af\u75c5\u5bb3\u68c0\u6d4b\u65b9\u9762\u5177\u6709\u4f18\u52bf\u3002"}}
{"id": "2510.23662", "categories": ["cs.SI", "cs.IT", "cs.LG", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.23662", "abs": "https://arxiv.org/abs/2510.23662", "authors": ["Liangzhe Han", "Leilei Sun", "Tongyu Zhu", "Tao Tao", "Jibin Wang", "Weifeng Lv"], "title": "JiuTian Chuanliu: A Large Spatiotemporal Model for General-purpose Dynamic Urban Sensing", "comment": null, "summary": "As a window for urban sensing, human mobility contains rich spatiotemporal\ninformation that reflects both residents' behavior preferences and the\nfunctions of urban areas. The analysis of human mobility has attracted the\nattention of many researchers. However, existing methods often address specific\ntasks from a particular perspective, leading to insufficient modeling of human\nmobility and limited applicability of the learned knowledge in various\ndownstream applications. To address these challenges, this paper proposes to\npush massive amounts of human mobility data into a spatiotemporal model,\ndiscover latent semantics behind mobility behavior and support various urban\nsensing tasks. Specifically, a large-scale and widely covering human mobility\ndata is collected through the ubiquitous base station system and a framework\nnamed General-purpose and Dynamic Human Mobility Embedding (GDHME) for urban\nsensing is introduced. The framework follows the self-supervised learning idea\nand contains two major stages. In stage 1, GDHME treats people and regions as\nnodes within a dynamic graph, unifying human mobility data as\npeople-region-time interactions. An encoder operating in continuous-time\ndynamically computes evolving node representations, capturing dynamic states\nfor both people and regions. Moreover, an autoregressive self-supervised task\nis specially designed to guide the learning of the general-purpose node\nembeddings. In stage 2, these representations are utilized to support various\ntasks. To evaluate the effectiveness of our GDHME framework, we further\nconstruct a multi-task urban sensing benchmark. Offline experiments demonstrate\nGDHME's ability to automatically learn valuable node features from vast amounts\nof data. Furthermore, our framework is used to deploy the JiuTian ChuanLiu Big\nModel, a system that has been presented at the 2023 China Mobile Worldwide\nPartner Conference.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGDHME\u7684\u901a\u7528\u52a8\u6001\u4eba\u7c7b\u6d3b\u52a8\u5d4c\u5165\u6846\u67b6\uff0c\u5229\u7528\u57fa\u7ad9\u6570\u636e\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u6316\u6398\u4eba\u7c7b\u6d3b\u52a8\u7684\u65f6\u7a7a\u8bed\u4e49\uff0c\u4ee5\u652f\u6301\u57ce\u5e02\u611f\u77e5\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u4eba\u7c7b\u6d3b\u52a8\u5efa\u6a21\u548c\u77e5\u8bc6\u5e94\u7528\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u4eba\u7c7b\u6d3b\u52a8\u8574\u542b\u7684\u65f6\u7a7a\u4fe1\u606f\u3002", "method": "GDHME\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u4e3b\u8981\u9636\u6bb5\uff1a1. \u5c06\u4eba\u7c7b\u6d3b\u52a8\u6570\u636e\u7edf\u4e00\u4e3a\u201c\u4eba-\u533a\u57df-\u65f6\u95f4\u201d\u4ea4\u4e92\uff0c\u6784\u5efa\u52a8\u6001\u56fe\uff0c\u5229\u7528\u8fde\u7eed\u65f6\u95f4\u7f16\u7801\u5668\u548c\u81ea\u76d1\u7763\u4efb\u52a1\u5b66\u4e60\u901a\u7528\u8282\u70b9\u5d4c\u5165\u30022. \u5229\u7528\u5b66\u4e60\u5230\u7684\u8868\u793a\u652f\u6301\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\u3002", "result": "GDHME\u80fd\u591f\u4ece\u6d77\u91cf\u6570\u636e\u4e2d\u81ea\u52a8\u5b66\u4e60\u6709\u4ef7\u503c\u7684\u8282\u70b9\u7279\u5f81\uff0c\u5e76\u5728\u591a\u4efb\u52a1\u57ce\u5e02\u611f\u77e5\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "GDHME\u6846\u67b6\u80fd\u591f\u6709\u6548\u5730\u4ece\u5927\u89c4\u6a21\u4eba\u7c7b\u6d3b\u52a8\u6570\u636e\u4e2d\u5b66\u4e60\u65f6\u7a7a\u8bed\u4e49\uff0c\u5e76\u652f\u6301\u591a\u79cd\u57ce\u5e02\u611f\u77e5\u4efb\u52a1\uff0c\u5c55\u793a\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u5df2\u6210\u529f\u90e8\u7f72\u4e3a\u201c\u4e5d\u5929\u5ddd\u6d41\u5927\u6570\u636e\u6a21\u578b\u201d\u3002"}}
{"id": "2510.24383", "categories": ["cs.AI", "cs.CY", "cs.MA", "I.2.11; I.2.1; I.2.4; K.4.1; K.4.3"], "pdf": "https://arxiv.org/pdf/2510.24383", "abs": "https://arxiv.org/abs/2510.24383", "authors": ["Juraj Mavra\u010di\u0107"], "title": "Policy Cards: Machine-Readable Runtime Governance for Autonomous AI Agents", "comment": "First published on 19/10/2025. Canonical archived record and DOI:\n  10.5281/zenodo.17391796", "summary": "Policy Cards are introduced as a machine-readable, deployment-layer standard\nfor expressing operational, regulatory, and ethical constraints for AI agents.\nThe Policy Card sits with the agent and enables it to follow required\nconstraints at runtime. It tells the agent what it must and must not do. As\nsuch, it becomes an integral part of the deployed agent. Policy Cards extend\nexisting transparency artifacts such as Model, Data, and System Cards by\ndefining a normative layer that encodes allow/deny rules, obligations,\nevidentiary requirements, and crosswalk mappings to assurance frameworks\nincluding NIST AI RMF, ISO/IEC 42001, and the EU AI Act. Each Policy Card can\nbe validated automatically, version-controlled, and linked to runtime\nenforcement or continuous-audit pipelines. The framework enables verifiable\ncompliance for autonomous agents, forming a foundation for distributed\nassurance in multi-agent ecosystems. Policy Cards provide a practical mechanism\nfor integrating high-level governance with hands-on engineering practice and\nenabling accountable autonomy at scale.", "AI": {"tldr": "Policy Cards are a new standard for AI agents that define operational, regulatory, and ethical constraints, enabling verifiable compliance and accountability in AI systems.", "motivation": "The need for a machine-readable standard to express and enforce AI agent constraints at runtime, extending existing transparency artifacts and enabling verifiable compliance.", "method": "Policy Cards are introduced as a deployment-layer standard that sits with the agent, encoding allow/deny rules, obligations, evidentiary requirements, and mappings to assurance frameworks. They support automatic validation, version control, and integration with runtime enforcement or auditing.", "result": "Policy Cards provide a practical mechanism for integrating governance with engineering, enabling accountable autonomy at scale and forming a foundation for distributed assurance in multi-agent ecosystems.", "conclusion": "Policy Cards offer a novel solution for ensuring AI agents operate within defined constraints, facilitating transparency, accountability, and verifiable compliance in complex AI deployments."}}
{"id": "2510.23842", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23842", "abs": "https://arxiv.org/abs/2510.23842", "authors": ["Saki Imai", "Lee Kezar", "Laurel Aichler", "Mert Inan", "Erin Walker", "Alicia Wooten", "Lorna Quandt", "Malihe Alikhani"], "title": "How Pragmatics Shape Articulation: A Computational Case Study in STEM ASL Discourse", "comment": null, "summary": "Most state-of-the-art sign language models are trained on interpreter or\nisolated vocabulary data, which overlooks the variability that characterizes\nnatural dialogue. However, human communication dynamically adapts to contexts\nand interlocutors through spatiotemporal changes and articulation style. This\nspecifically manifests itself in educational settings, where novel vocabularies\nare used by teachers, and students. To address this gap, we collect a motion\ncapture dataset of American Sign Language (ASL) STEM (Science, Technology,\nEngineering, and Mathematics) dialogue that enables quantitative comparison\nbetween dyadic interactive signing, solo signed lecture, and interpreted\narticles. Using continuous kinematic features, we disentangle dialogue-specific\nentrainment from individual effort reduction and show spatiotemporal changes\nacross repeated mentions of STEM terms. On average, dialogue signs are\n24.6%-44.6% shorter in duration than the isolated signs, and show significant\nreductions absent in monologue contexts. Finally, we evaluate sign embedding\nmodels on their ability to recognize STEM signs and approximate how entrained\nthe participants become over time. Our study bridges linguistic analysis and\ncomputational modeling to understand how pragmatics shape sign articulation and\nits representation in sign language technologies.", "AI": {"tldr": "\u6700\u5148\u8fdb\u7684\u624b\u8bed\u6a21\u578b\u901a\u5e38\u5728\u53e3\u8bd1\u5458\u6216\u5b64\u7acb\u8bcd\u6c47\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u5ffd\u7565\u4e86\u81ea\u7136\u5bf9\u8bdd\u7684\u7279\u70b9\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u5dee\u8ddd\uff0c\u6211\u4eec\u6536\u96c6\u4e86\u4e00\u4e2a\u5305\u542b\u7f8e\u56fd\u624b\u8bed\uff08ASL\uff09STEM\uff08\u79d1\u5b66\u3001\u6280\u672f\u3001\u5de5\u7a0b\u548c\u6570\u5b66\uff09\u5bf9\u8bdd\u7684\u8fd0\u52a8\u6355\u6349\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u80fd\u591f\u5bf9\u53cc\u5411\u4e92\u52a8\u624b\u8bed\u3001\u72ec\u767d\u5f0f\u8bb2\u5ea7\u548c\u53e3\u8bd1\u6587\u7ae0\u8fdb\u884c\u91cf\u5316\u6bd4\u8f83\u3002\u901a\u8fc7\u5206\u6790\u8fd0\u52a8\u5b66\u7279\u5f81\uff0c\u6211\u4eec\u53d1\u73b0\u5bf9\u8bdd\u4e2d\u7684\u624b\u8bed\u6bd4\u5b64\u7acb\u7684\u624b\u8bed\u5728\u6301\u7eed\u65f6\u95f4\u4e0a\u5e73\u5747\u7f29\u77ed\u4e86 24.6%-44.6%\uff0c\u5e76\u4e14\u8fd9\u79cd\u7f29\u77ed\u5728\u72ec\u767d\u573a\u666f\u4e2d\u5e76\u4e0d\u663e\u8457\u3002\u6700\u540e\uff0c\u6211\u4eec\u8bc4\u4f30\u4e86\u624b\u8bed\u5d4c\u5165\u6a21\u578b\u8bc6\u522b STEM \u624b\u8bed\u7684\u80fd\u529b\uff0c\u5e76\u8fd1\u4f3c\u8ba1\u7b97\u4e86\u53c2\u4e0e\u8005\u968f\u65f6\u95f4\u63a8\u79fb\u7684\u4e92\u52a8\u7a0b\u5ea6\u3002", "motivation": "\u5927\u591a\u6570\u6700\u5148\u8fdb\u7684\u624b\u8bed\u6a21\u578b\u90fd\u5728\u53e3\u8bd1\u5458\u6216\u5b64\u7acb\u8bcd\u6c47\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u8fd9\u5ffd\u7565\u4e86\u81ea\u7136\u5bf9\u8bdd\u7684\u7279\u70b9\uff0c\u7279\u522b\u662f\u5728\u6559\u80b2\u73af\u5883\u4e2d\uff0c\u6559\u5e08\u548c\u5b66\u751f\u4f1a\u4f7f\u7528\u65b0\u7684\u8bcd\u6c47\u3002\u56e0\u6b64\uff0c\u9700\u8981\u6536\u96c6\u80fd\u591f\u6355\u6349\u8fd9\u4e9b\u52a8\u6001\u53d8\u5316\u7684\u6570\u636e\u96c6\u3002", "method": "\u6536\u96c6\u4e86\u4e00\u4e2a\u5305\u542b\u7f8e\u56fd\u624b\u8bed STEM \u5bf9\u8bdd\u7684\u8fd0\u52a8\u6355\u6349\u6570\u636e\u96c6\uff0c\u5e76\u4f7f\u7528\u8fde\u7eed\u8fd0\u52a8\u5b66\u7279\u5f81\u6765\u5206\u6790\u53cc\u5411\u4e92\u52a8\u624b\u8bed\u3001\u72ec\u767d\u5f0f\u8bb2\u5ea7\u548c\u53e3\u8bd1\u6587\u7ae0\u4e4b\u95f4\u7684\u5dee\u5f02\u3002\u901a\u8fc7\u5206\u6790\u53d1\u73b0\uff0c\u5bf9\u8bdd\u624b\u8bed\u7684\u6301\u7eed\u65f6\u95f4\u6bd4\u5b64\u7acb\u624b\u8bed\u77ed\uff0c\u5e76\u4e14\u8fd9\u79cd\u73b0\u8c61\u5728\u72ec\u767d\u573a\u666f\u4e2d\u4e0d\u663e\u8457\u3002\u6700\u540e\uff0c\u8bc4\u4f30\u4e86\u624b\u8bed\u5d4c\u5165\u6a21\u578b\u5728\u8bc6\u522b STEM \u624b\u8bed\u548c\u53c2\u4e0e\u8005\u4e92\u52a8\u7a0b\u5ea6\u65b9\u9762\u7684\u80fd\u529b\u3002", "result": "\u5728\u53cc\u5411\u4e92\u52a8\u624b\u8bed\u573a\u666f\u4e2d\uff0c\u624b\u8bed\u7684\u6301\u7eed\u65f6\u95f4\u5e73\u5747\u6bd4\u5b64\u7acb\u624b\u8bed\u7f29\u77ed\u4e86 24.6%-44.6%\uff0c\u5e76\u4e14\u8fd9\u79cd\u7f29\u77ed\u5728\u72ec\u767d\u573a\u666f\u4e2d\u5e76\u4e0d\u663e\u8457\u3002\u7814\u7a76\u8fd8\u8bc4\u4f30\u4e86\u624b\u8bed\u5d4c\u5165\u6a21\u578b\u8bc6\u522b STEM \u624b\u8bed\u548c\u53c2\u4e0e\u8005\u4e92\u52a8\u7a0b\u5ea6\u7684\u80fd\u529b\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u7ed3\u5408\u4e86\u8bed\u8a00\u5b66\u5206\u6790\u548c\u8ba1\u7b97\u5efa\u6a21\uff0c\u4ee5\u7406\u89e3\u8bed\u7528\u5b66\u5982\u4f55\u5f71\u54cd\u624b\u8bed\u7684\u53d1\u97f3\u53ca\u5176\u5728\u624b\u8bed\u6280\u672f\u4e2d\u7684\u8868\u793a\uff0c\u7279\u522b\u662f STEM \u9886\u57df\u4e2d\u7684\u8868\u793a\u3002"}}
{"id": "2510.23928", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23928", "abs": "https://arxiv.org/abs/2510.23928", "authors": ["Raman Jha", "Yang Zhou", "Giuseppe Loianno"], "title": "Adaptive Keyframe Selection for Scalable 3D Scene Reconstruction in Dynamic Environments", "comment": "Under Review for ROBOVIS 2026", "summary": "In this paper, we propose an adaptive keyframe selection method for improved\n3D scene reconstruction in dynamic environments. The proposed method integrates\ntwo complementary modules: an error-based selection module utilizing\nphotometric and structural similarity (SSIM) errors, and a momentum-based\nupdate module that dynamically adjusts keyframe selection thresholds according\nto scene motion dynamics. By dynamically curating the most informative frames,\nour approach addresses a key data bottleneck in real-time perception. This\nallows for the creation of high-quality 3D world representations from a\ncompressed data stream, a critical step towards scalable robot learning and\ndeployment in complex, dynamic environments. Experimental results demonstrate\nsignificant improvements over traditional static keyframe selection strategies,\nsuch as fixed temporal intervals or uniform frame skipping. These findings\nhighlight a meaningful advancement toward adaptive perception systems that can\ndynamically respond to complex and evolving visual scenes. We evaluate our\nproposed adaptive keyframe selection module on two recent state-of-the-art 3D\nreconstruction networks, Spann3r and CUT3R, and observe consistent improvements\nin reconstruction quality across both frameworks. Furthermore, an extensive\nablation study confirms the effectiveness of each individual component in our\nmethod, underlining their contribution to the overall performance gains.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u5173\u952e\u5e27\u9009\u62e9\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u52a8\u6001\u73af\u5883\u4e2d\u6539\u8fdb\u4e09\u7ef4\u573a\u666f\u91cd\u5efa\u3002", "motivation": "\u5728\u5b9e\u65f6\u611f\u77e5\u4e2d\uff0c\u5173\u952e\u5e27\u9009\u62e9\u662f\u4e00\u4e2a\u5173\u952e\u7684\u6570\u636e\u74f6\u9888\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u52a8\u6001\u9002\u5e94\u573a\u666f\u8fd0\u52a8\u7684\u65b9\u6cd5\uff0c\u4ee5\u521b\u5efa\u9ad8\u8d28\u91cf\u7684\u4e09\u7ef4\u4e16\u754c\u8868\u793a\u3002", "method": "\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u57fa\u4e8e\u8bef\u5dee\uff08\u5149\u5ea6\u548cSSIM\uff09\u7684\u9009\u62e9\u6a21\u5757\u548c\u57fa\u4e8e\u52a8\u91cf\u7684\u66f4\u65b0\u6a21\u5757\uff0c\u8be5\u6a21\u5757\u6839\u636e\u573a\u666f\u8fd0\u52a8\u52a8\u6001\u8c03\u6574\u5173\u952e\u5e27\u9009\u62e9\u9608\u503c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u4f20\u7edf\u7684\u9759\u6001\u5173\u952e\u5e27\u9009\u62e9\u7b56\u7565\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5728\u91cd\u5efa\u8d28\u91cf\u65b9\u9762\u6709\u663e\u8457\u7684\u6539\u8fdb\u3002\u5728Spann3r\u548cCUT3R\u4e24\u4e2a\u7f51\u7edc\u4e0a\u90fd\u89c2\u5bdf\u5230\u4e86\u6301\u7eed\u7684\u6539\u8fdb\u3002\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u4e86\u6bcf\u4e2a\u7ec4\u4ef6\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u81ea\u9002\u5e94\u5173\u952e\u5e27\u9009\u62e9\u65b9\u6cd5\u80fd\u591f\u52a8\u6001\u5730\u54cd\u5e94\u590d\u6742\u548c\u4e0d\u65ad\u53d8\u5316\u7684\u53ef\u89c6\u5316\u573a\u666f\uff0c\u4e3a\u5b9e\u73b0\u81ea\u9002\u5e94\u611f\u77e5\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u610f\u4e49\u7684\u8fdb\u5c55\u3002"}}
{"id": "2510.24205", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.24205", "abs": "https://arxiv.org/abs/2510.24205", "authors": ["Telmo Ribeiro", "Jos\u00e9 Proen\u00e7a", "M\u00e1rio Florido"], "title": "CoMPSeT: A Framework for Comparing Multiparty Session Types", "comment": "In Proceedings EXPRESS/SOS 2025, arXiv:2510.23211", "summary": "Concurrent systems are often complex and difficult to design. Choreographic\nlanguages, such as Multiparty Session Types (MPST), allow the description of\nglobal protocols of interactions by capturing valid patterns of interactions\nbetween participants. Many variations of MPST exist, each one with its rather\nspecific features and idiosyncrasies. Here we propose a tool (CoMPSeT) that\nprovides clearer insights over different features in existing MPST. We select a\nrepresentative set of MPST examples and provide mechanisms to combine different\nfeatures and to animate and compare the semantics of concrete examples. CoMPSeT\nis open-source, compiled into JavaScript, and can be directly executed from any\nbrowser, becoming useful both for researchers who want to better understand the\nlandscape of MPST and for teachers who want to explain global choreographies.", "AI": {"tldr": "CoMPSeT\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u3001\u57fa\u4e8e\u6d4f\u89c8\u5668\u7684\u5de5\u5177\uff0c\u7528\u4e8e\u53ef\u89c6\u5316\u3001\u6bd4\u8f83\u548c\u7406\u89e3\u591a\u4eba\u4f1a\u8bdd\u7c7b\u578b\uff08MPST\uff09\u53ca\u5176\u53d8\u4f53\u3002\u5b83\u901a\u8fc7\u63d0\u4f9b\u5bf9\u4e0d\u540cMPST\u7279\u6027\u7684\u6e05\u6670\u89c1\u89e3\uff0c\u5e76\u5141\u8bb8\u7ec4\u5408\u7279\u6027\u3001\u52a8\u753b\u548c\u6bd4\u8f83\u5177\u4f53\u793a\u4f8b\u7684\u8bed\u4e49\uff0c\u4ece\u800c\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u548c\u6559\u5e08\u66f4\u597d\u5730\u7406\u89e3MPST\u3002", "motivation": " concurrent systems \u590d\u6742\u96be\u8bbe\u8ba1\uff0cMPST \u63d0\u4f9b\u4e86\u63cf\u8ff0\u4ea4\u4e92\u5168\u5c40\u534f\u8bae\u7684\u80fd\u529b\uff0c\u4f46 MPST \u5b58\u5728\u591a\u79cd\u53d8\u4f53\uff0c\u5404\u6709\u7279\u70b9\uff0c\u9700\u8981\u66f4\u597d\u7684\u5de5\u5177\u6765\u7406\u89e3\u548c\u6bd4\u8f83\u8fd9\u4e9b\u53d8\u4f53\u53ca\u5176\u7279\u6027\u3002", "method": "\u9009\u62e9\u4e86\u4e00\u7ec4\u4ee3\u8868\u6027\u7684 MPST \u793a\u4f8b\uff0c\u63d0\u4f9b\u4e86\u7ec4\u5408\u4e0d\u540c\u7279\u6027\u3001\u52a8\u753b\u548c\u6bd4\u8f83\u5177\u4f53\u793a\u4f8b\u8bed\u4e49\u7684\u673a\u5236\u3002CoMPSeT \u662f\u4e00\u4e2a\u5f00\u6e90\u5de5\u5177\uff0c\u7f16\u8bd1\u4e3a JavaScript\uff0c\u53ef\u5728\u6d4f\u89c8\u5668\u4e2d\u8fd0\u884c\u3002", "result": "CoMPSeT \u80fd\u591f\u6e05\u6670\u5730\u5c55\u793a MPST \u7684\u4e0d\u540c\u7279\u6027\uff0c\u5e76\u5141\u8bb8\u7528\u6237\u901a\u8fc7\u7ec4\u5408\u7279\u6027\u3001\u52a8\u753b\u548c\u6bd4\u8f83\u8bed\u4e49\u6765\u6df1\u5165\u7406\u89e3\u5b83\u4eec\u3002\u8be5\u5de5\u5177\u53ef\u7528\u4e8e\u7814\u7a76 MPST \u7684\u591a\u6837\u6027\uff0c\u5e76\u7528\u4e8e\u6559\u5b66\u76ee\u7684\u3002", "conclusion": "CoMPSeT \u4e3a\u7814\u7a76\u4eba\u5458\u548c\u6559\u5e08\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u7528\u7684\u5de5\u5177\uff0c\u4ee5\u66f4\u597d\u5730\u7406\u89e3\u548c\u4f20\u8fbe MPST \u8bed\u8a00\u7684\u590d\u6742\u6027\u3002"}}
{"id": "2510.23855", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2510.23855", "abs": "https://arxiv.org/abs/2510.23855", "authors": ["Carmine Autieri", "Amar Fakhredine"], "title": "Relativistic Spin-momentum locking in altermagnets", "comment": "7 pages, 4 figures", "summary": "Spin-momentum locking in altermagnets has been deeply explored in the\nnon-relativistic limit. Including spin-orbit coupling, altermagnets exhibit\nantisymmetric exchange interactions, leading to spin cantings. Therefore, the\nspin-momentum locking differs among the three spin components Sx, Sy, and Sz,\nforming the relativistic spin-momentum locking. We consider orthorhombic YVO3\nand hexagonal MnTe. For YVO3, the relativistic locking comprises s-, dxy -, and\ndxz-wave. In MnTe, the dominant component Sy of MnTe inherits the polarized\ncharge distribution and the non-relativistic spin-momentum locking bulk g-wave,\nbut the breaking of the C6z rotational symmetry by the Neel vector lowers the\nsymmetry from g-wave to d-wave. The relativistic spin-momentum locking for MnTe\nis composed of dxz-, dyz- and s-wave. Despite small magnitudes in real space,\nthe canted spin components contribute significant spectral weight in k-space,\nimpacting k-space properties such as the spin-Hall conductivity.", "AI": {"tldr": "\u7814\u7a76\u4e86\u975e\u76f8\u5bf9\u8bba\u6781\u9650\u4e0b\u53cd\u94c1\u78c1\u4f53\u4e2d\u81ea\u65cb-\u52a8\u91cf\u9501\u5b9a\u7684\u6027\u8d28\uff0c\u5e76\u8003\u8651\u4e86\u76f8\u5bf9\u8bba\u6548\u5e94\uff0c\u53d1\u73b0\u5176\u5177\u6709\u4e0d\u540c\u7684\u81ea\u65cb\u5206\u91cf\u8026\u5408\u65b9\u5f0f\uff0c\u5e76\u4ee5 YVO3 \u548c MnTe \u4e3a\u4f8b\u8fdb\u884c\u4e86\u5177\u4f53\u5206\u6790\u3002", "motivation": "\u5728\u975e\u76f8\u5bf9\u8bba\u6781\u9650\u4e0b\u6df1\u5165\u7814\u7a76\u4e86\u53cd\u94c1\u78c1\u4f53\u4e2d\u7684\u81ea\u65cb-\u52a8\u91cf\u9501\u5b9a\u73b0\u8c61\uff0c\u5e76\u8003\u8651\u4e86\u81ea\u65cb-\u8f68\u9053\u8026\u5408\u5bfc\u81f4\u7684\u53cd\u94c1\u78c1\u4f53\u4e2d\u7684\u53cd\u5bf9\u79f0\u4ea4\u6362\u76f8\u4e92\u4f5c\u7528\u53ca\u5176\u5f15\u8d77\u7684\u81ea\u65cb\u503e\u659c\uff0c\u8fdb\u800c\u7814\u7a76\u4e86\u4e0d\u540c\u81ea\u65cb\u5206\u91cf\uff08Sx, Sy, Sz\uff09\u7684\u81ea\u65cb-\u52a8\u91cf\u9501\u5b9a\u5dee\u5f02\uff0c\u5373\u76f8\u5bf9\u8bba\u81ea\u65cb-\u52a8\u91cf\u9501\u5b9a\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\uff0c\u7814\u7a76\u4e86\u6b63\u4ea4\u6676 YVO3 \u548c\u516d\u65b9\u6676 MnTe \u7684\u76f8\u5bf9\u8bba\u81ea\u65cb-\u52a8\u91cf\u9501\u5b9a\u6027\u8d28\u3002\u5bf9\u4e8e YVO3\uff0c\u5206\u6790\u4e86\u5176\u76f8\u5bf9\u8bba\u9501\u5b9a\u7684 s-, dxy-, \u548c dxz-\u6ce2\u5206\u91cf\u3002\u5bf9\u4e8e MnTe\uff0c\u5206\u6790\u4e86\u5176\u4e3b\u8981\u7684 Sy \u5206\u91cf\u5982\u4f55\u7ee7\u627f\u975e\u76f8\u5bf9\u8bba\u7684\u81ea\u65cb-\u52a8\u91cf\u9501\u5b9a\u6027\u8d28\uff0c\u4ee5\u53ca Neel \u77e2\u91cf\u5982\u4f55\u7834\u574f C6z \u65cb\u8f6c\u5bf9\u79f0\u6027\u5bfc\u81f4\u4ece g \u6ce2\u53d8\u4e3a d \u6ce2\u3002\u6700\u540e\uff0c\u8ba1\u7b97\u4e86\u503e\u659c\u81ea\u65cb\u5206\u91cf\u5bf9 k \u7a7a\u95f4\u6027\u8d28\uff08\u5982\u81ea\u65cb\u970d\u5c14\u7535\u5bfc\u7387\uff09\u7684\u5f71\u54cd\u3002", "result": "\u5bf9\u4e8e YVO3\uff0c\u76f8\u5bf9\u8bba\u9501\u5b9a\u5305\u542b s-, dxy-, \u548c dxz-\u6ce2\u3002\u5bf9\u4e8e MnTe\uff0c\u5176\u4e3b\u8981\u7684 Sy \u5206\u91cf\u7ee7\u627f\u4e86\u975e\u76f8\u5bf9\u8bba\u81ea\u65cb-\u52a8\u91cf\u9501\u5b9a g-\u6ce2\u6027\u8d28\uff0c\u4f46\u7531\u4e8e Neel \u77e2\u91cf\u7834\u574f\u4e86 C6z \u65cb\u8f6c\u5bf9\u79f0\u6027\uff0c\u5bf9\u79f0\u6027\u4ece g \u6ce2\u964d\u4f4e\u5230 d \u6ce2\u3002MnTe \u7684\u76f8\u5bf9\u8bba\u81ea\u65cb-\u52a8\u91cf\u9501\u5b9a\u7531 dxz-, dyz- \u548c s-\u6ce2\u7ec4\u6210\u3002\u5c3d\u7ba1\u503e\u659c\u7684\u81ea\u65cb\u5206\u91cf\u5728\u5b9e\u7a7a\u95f4\u4e2d\u5e45\u5ea6\u5f88\u5c0f\uff0c\u4f46\u5b83\u4eec\u5728 k \u7a7a\u95f4\u4e2d\u8d21\u732e\u4e86\u663e\u8457\u7684\u8c31\u6743\u91cd\uff0c\u5f71\u54cd\u4e86\u81ea\u65cb\u970d\u5c14\u7535\u5bfc\u7387\u7b49 k \u7a7a\u95f4\u6027\u8d28\u3002", "conclusion": "\u76f8\u5bf9\u8bba\u6548\u5e94\u5728\u53cd\u94c1\u78c1\u4f53\u4e2d\u5f15\u5165\u4e86\u4e0d\u540c\u4e8e\u975e\u76f8\u5bf9\u8bba\u6781\u9650\u7684\u81ea\u65cb-\u52a8\u91cf\u9501\u5b9a\u884c\u4e3a\u3002\u5373\u4f7f\u5728\u5b9e\u7a7a\u95f4\u4e2d\u5e45\u5ea6\u5f88\u5c0f\u7684\u503e\u659c\u81ea\u65cb\u5206\u91cf\uff0c\u5728 k \u7a7a\u95f4\u4e2d\u4e5f\u53ef\u80fd\u4ea7\u751f\u91cd\u8981\u7684\u5f71\u54cd\uff0c\u4f8b\u5982\u6539\u53d8\u81ea\u65cb\u970d\u5c14\u7535\u5bfc\u7387\u3002"}}
{"id": "2510.24062", "categories": ["cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2510.24062", "abs": "https://arxiv.org/abs/2510.24062", "authors": ["Erin S. Grant", "Joseph F. Olorunyomi", "Sam C. Scholten", "Islay O. Robertson", "Amanda N. Abraham", "Nandish H. Srikantamurthy", "Billy J. Murdoch", "Edwin L. H. Maye", "Blanca del Rosal Rabes", "Alexander J. Healey", "Cara M. Doherty", "Philipp Reineck", "Xavier Mulet", "Jean-Philippe Tetienne", "David A. Broadway"], "title": "Spin-dependent photoluminescence in carbon-based quantum dots", "comment": "26 pages, 27 figures", "summary": "The ability to modulate the photoluminescence (PL) of nanomaterials via\nspin-related effects is vital for many emerging quantum technologies, with\nnanoscale quantum sensing and imaging being particular areas of focus.\nCarbon-based quantum dots (CQDs) are among the most common forms of luminescent\nnanomaterials, appealing due to their ease of synthesis, tunability through\norganic chemistry, high brightness, and natural biocompatibility. However, the\nobservation of room temperature, spin-dependent PL has remained elusive. Here\nwe report on the observation of PL modulation of CQDs by magnetic fields ($\\sim\n10$ mT) under ambient conditions. We synthesize a series of CQDs using 19\ndifferent amino acids, which have a range of PL emission spectra and exhibit a\nclear magneto-PL effect (up to $\\sim 1$% change). Furthermore, an electron spin\nresonance is detected in the PL with a g-factor of g $\\approx$ 2, suggesting a\nprocess similar to the radical pair mechanism is responsible. Finally, we show\nthat the magneto-PL contrast decreases in the presence of paramagnetic species,\nwhich we attribute to an increase in magnetic noise-induced spin relaxation in\nthe CQDs. Our work brings new functionalities to these commonly used and\nbiocompatible luminescent nanoparticles, opening new opportunities for in situ\nquantum sensing and imaging of biological samples.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2510.23816", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23816", "abs": "https://arxiv.org/abs/2510.23816", "authors": ["Forouzan Fallah", "Wenwen Li", "Chia-Yu Hsu", "Hyunho Lee", "Yezhou Yang"], "title": "RareFlow: Physics-Aware Flow-Matching for Cross-Sensor Super-Resolution of Rare-Earth Features", "comment": null, "summary": "Super-resolution (SR) for remote sensing imagery often fails under\nout-of-distribution (OOD) conditions, such as rare geomorphic features captured\nby diverse sensors, producing visually plausible but physically inaccurate\nresults. We present RareFlow, a physics-aware SR framework designed for OOD\nrobustness. RareFlow's core is a dual-conditioning architecture. A Gated\nControlNet preserves fine-grained geometric fidelity from the low-resolution\ninput, while textual prompts provide semantic guidance for synthesizing complex\nfeatures. To ensure physically sound outputs, we introduce a multifaceted loss\nfunction that enforces both spectral and radiometric consistency with sensor\nproperties. Furthermore, the framework quantifies its own predictive\nuncertainty by employing a stochastic forward pass approach; the resulting\noutput variance directly identifies unfamiliar inputs, mitigating feature\nhallucination. We validate RareFlow on a new, curated benchmark of multi-sensor\nsatellite imagery. In blind evaluations, geophysical experts rated our model's\noutputs as approaching the fidelity of ground truth imagery, significantly\noutperforming state-of-the-art baselines. This qualitative superiority is\ncorroborated by quantitative gains in perceptual metrics, including a nearly\n40\\% reduction in FID. RareFlow provides a robust framework for high-fidelity\nsynthesis in data-scarce scientific domains and offers a new paradigm for\ncontrolled generation under severe domain shift.", "AI": {"tldr": "RareFlow\u662f\u4e00\u4e2a\u7528\u4e8e\u9065\u611f\u56fe\u50cf\u7684\u7269\u7406\u611f\u77e5\u8d85\u5206\u8fa8\u7387\u6846\u67b6\uff0c\u65e8\u5728\u63d0\u9ad8\u5728\u5206\u5e03\u5916\uff08OOD\uff09\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u901a\u8fc7\u7ed3\u5408\u51e0\u4f55\u4fdd\u771f\u5ea6\u548c\u8bed\u4e49\u6307\u5bfc\uff0c\u5e76\u5f3a\u5236\u6267\u884c\u5149\u8c31\u548c\u8f90\u5c04\u4e00\u81f4\u6027\uff0c\u4ee5\u751f\u6210\u7269\u7406\u4e0a\u51c6\u786e\u7684\u7ed3\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u8d85\u5206\u8fa8\u7387\uff08SR\uff09\u65b9\u6cd5\u5728\u5904\u7406\u5206\u5e03\u5916\uff08OOD\uff09\u6761\u4ef6\uff08\u5982\u7f55\u89c1\u7684\u5730\u7406\u5730\u8c8c\u7279\u5f81\u3001\u4e0d\u540c\u4f20\u611f\u5668\u6355\u83b7\u7684\u56fe\u50cf\uff09\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u5bb9\u6613\u751f\u6210\u89c6\u89c9\u4e0a\u5408\u7406\u4f46\u7269\u7406\u4e0a\u4e0d\u51c6\u786e\u7684\u7ed3\u679c\u3002", "method": "RareFlow\u91c7\u7528\u4e00\u79cd\u53cc\u91cd\u6761\u4ef6\u67b6\u6784\uff1aGated ControlNet\u4fdd\u7559\u4f4e\u5206\u8fa8\u7387\u8f93\u5165\u7684\u7ec6\u7c92\u5ea6\u51e0\u4f55\u4fdd\u771f\u5ea6\uff0c\u800c\u6587\u672c\u63d0\u793a\u63d0\u4f9b\u8bed\u4e49\u6307\u5bfc\uff1b\u5f15\u5165\u591a\u65b9\u9762\u635f\u5931\u51fd\u6570\u4ee5\u5f3a\u5236\u6267\u884c\u5149\u8c31\u548c\u8f90\u5c04\u4e00\u81f4\u6027\uff1b\u91c7\u7528\u968f\u673a\u524d\u5411\u4f20\u9012\u91cf\u5316\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\uff0c\u8bc6\u522b\u4e0d\u719f\u6089\u8f93\u5165\u3002", "result": "\u5728\u591a\u4f20\u611f\u5668\u536b\u661f\u56fe\u50cf\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRareFlow\u5728\u76f2\u8bc4\u4f30\u4e2d\u63a5\u8fd1\u771f\u5b9e\u56fe\u50cf\u7684\u4fdd\u771f\u5ea6\uff0c\u5e76\u5728FID\u7b49\u611f\u77e5\u6307\u6807\u4e0a\u53d6\u5f97\u4e86\u8fd140%\u7684\u63d0\u5347\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "RareFlow\u4e3a\u6570\u636e\u7a00\u758f\u7684\u79d1\u5b66\u9886\u57df\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u4fdd\u771f\u5ea6\u5408\u6210\u7684\u9c81\u68d2\u6846\u67b6\uff0c\u5e76\u5728\u4e25\u91cd\u7684\u9886\u57df\u8f6c\u79fb\u4e0b\u5b9e\u73b0\u4e86\u53d7\u63a7\u751f\u6210\u7684\u65b0\u8303\u5f0f\u3002"}}
{"id": "2510.23726", "categories": ["quant-ph", "math-ph", "math.MP"], "pdf": "https://arxiv.org/pdf/2510.23726", "abs": "https://arxiv.org/abs/2510.23726", "authors": ["Daniel Belkin", "James Allen", "Bryan K. Clark"], "title": "Apparent Universal Behavior in Second Moments of Random Quantum Circuits", "comment": "29 pages, 17 figures", "summary": "Just how fast does the brickwork circuit form an approximate 2-design?\n  Is there any difference between anticoncentration and being a 2-design?\n  Does geometry matter?\n  How deep a circuit will I need in practice?\n  We tell you everything you always wanted to know about second moments of\nrandom quantum circuits, but were too afraid to compute. Our answers generally\ntake the form of numerical results for up to 50 qubits.\n  Our first contribution is a strategy to determine explicitly the optimal\nexperiment which distinguishes any given ensemble from the Haar measure. With\nthis formula and some computational tricks, we are able to compute $t = 2$\nmultiplicative errors exactly out to modest system sizes. As expected, we see\nthat most families of circuits form $\\epsilon$-approximate $2$-designs in depth\nproportional to $\\log n$. For the 1D brickwork, we work out the leading-order\nconstants explicitly.\n  For graphs, we find some exceptions which are much slower, proving that they\nrequire at least $\\Omega(n^2)$ gates. This answers a question asked by ref. 1\nin the negative. We explain these exceptional architectures in terms of\nconnectedness. Based on this intuition we conjecture universal upper and lower\nbounds for graph-sampled circuit ensembles.\n  For many architectures, the optimal experiment which determines the\nmultiplicative error corresponds exactly to the collision probability (i.e.\nanticoncentration). However, we find that the star graph anticoncentrates much\nfaster than it forms an $\\epsilon$-approximate $2$-design. Finally, we show\nthat one needs only ten to twenty layers to construct an approximate $2$-design\nfor realistic parameter ranges. This is a large constant-factor improvement\nover previous constructions. The parallel complete-graph architecture is not\nquite the fastest scrambler, partially resolving a question raised by ref. 2.", "AI": {"tldr": "\u672c\u8bba\u6587\u7814\u7a76\u4e86\u968f\u673a\u91cf\u5b50\u7535\u8def\u7684\u4e8c\u9636\u77e9\uff0c\u5e76\u7ed9\u51fa\u4e86\u5728\u4e0d\u540c\u7535\u8def\u7ed3\u6784\u4e0b\uff0c\u7535\u8def\u5f62\u6210\u8fd1\u4f3c\u4e8c\u9636\u8bbe\u8ba1\u7684\u6df1\u5ea6\u548c\u590d\u6742\u5ea6\u7684\u6570\u503c\u7ed3\u679c\u3002", "motivation": "\u7814\u7a76\u968f\u673a\u91cf\u5b50\u7535\u8def\u5f62\u6210\u8fd1\u4f3c\u4e8c\u9636\u8bbe\u8ba1\u7684\u6761\u4ef6\u548c\u901f\u5ea6\uff0c\u7279\u522b\u662f\u7816\u780c\u7535\u8def\u548c\u56fe\u91c7\u6837\u7535\u8def\u3002", "method": "\u901a\u8fc7\u8ba1\u7b97\u4e8c\u9636\u77e9\uff0c\u786e\u5b9a\u533a\u5206\u7ed9\u5b9a\u7cfb\u7efc\u4e0eHaar\u6d4b\u5ea6\u7684\u6700\u4f18\u5b9e\u9a8c\uff0c\u5e76\u8fdb\u884c\u6570\u503c\u8ba1\u7b97\u3002", "result": "\u5927\u591a\u6570\u7535\u8def\u5728\u6df1\u5ea6\u4e0elog n\u6210\u6b63\u6bd4\u65f6\u5f62\u6210\u8fd1\u4f3c\u4e8c\u9636\u8bbe\u8ba1\u3002\u7816\u780c\u7535\u8def\u7684\u5e38\u6570\u9879\u5df2\u88ab\u660e\u786e\u8ba1\u7b97\u3002\u56fe\u91c7\u6837\u7535\u8def\u4e2d\u5b58\u5728\u4e00\u4e9b\u4f8b\u5916\uff0c\u9700\u8981\u81f3\u5c11\u03a9(n^2)\u4e2a\u95e8\u3002\u5bf9\u4e8e\u8bb8\u591a\u67b6\u6784\uff0c\u6700\u4f18\u5b9e\u9a8c\u7b49\u540c\u4e8e\u78b0\u649e\u6982\u7387\u3002\u4f46\u661f\u5f62\u56fe\u7684\u5feb\u7167\u901f\u5ea6\u5feb\u4e8e\u5176\u5f62\u6210\u8fd1\u4f3c\u4e8c\u9636\u8bbe\u8ba1\u3002\u5bf9\u4e8e\u5b9e\u9645\u53c2\u6570\u8303\u56f4\uff0c10\u523020\u5c42\u5373\u53ef\u6784\u5efa\u8fd1\u4f3c\u4e8c\u9636\u8bbe\u8ba1\u3002", "conclusion": "\u7816\u780c\u7535\u8def\u5f62\u6210\u8fd1\u4f3c\u4e8c\u9636\u8bbe\u8ba1\u9700\u8981\u4e0elog n\u6210\u6b63\u6bd4\u7684\u6df1\u5ea6\u3002\u5b58\u5728\u4e00\u4e9b\u56fe\u91c7\u6837\u7535\u8def\u9700\u8981\u66f4\u9ad8\u7684\u6df1\u5ea6\u3002\u6700\u4f18\u5b9e\u9a8c\u901a\u5e38\u4e0e\u78b0\u649e\u6982\u7387\u4e00\u81f4\uff0c\u4f46\u5b58\u5728\u4f8b\u5916\u3002\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u8f83\u5c11\u7684\u5c42\u6570\u5373\u53ef\u6784\u5efa\u8fd1\u4f3c\u4e8c\u9636\u8bbe\u8ba1\u3002"}}
{"id": "2510.24204", "categories": ["cs.LO", "F.3.2"], "pdf": "https://arxiv.org/pdf/2510.24204", "abs": "https://arxiv.org/abs/2510.24204", "authors": ["Renato Neves"], "title": "An Adequacy Theorem Between Mixed Powerdomains and Probabilistic Concurrency", "comment": "In Proceedings EXPRESS/SOS 2025, arXiv:2510.23211", "summary": "We present an adequacy theorem for a concurrent extension of probabilistic\nGCL. The underlying denotational semantics is based on the so-called mixed\npowerdomains, which combine non-determinism with probabilistic behaviour. The\ntheorem itself is formulated via M. Smyth's idea of treating observable\nproperties as open sets of a topological space. The proof hinges on a\n'topological generalisation' of K\\\"onig's lemma in the setting of probabilistic\nprogramming (a result that is proved in the paper as well). One application of\nthe theorem is that it entails semi-decidability w.r.t. whether a concurrent\nprogram satisfies an observable property (written in a certain form). This is\nrelated to M. Escardo's conjecture about semi-decidability w.r.t. may and must\nprobabilistic testing.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u6982\u7387\u6027GCL\u7684\u5e76\u53d1\u6269\u5c55\u7684\u5145\u5206\u6027\u5b9a\u7406\u3002", "motivation": "\u4e3a\u6982\u7387\u6027GCL\u7684\u5e76\u53d1\u6269\u5c55\u63d0\u4f9b\u4e00\u4e2a\u5145\u5206\u6027\u5b9a\u7406\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u89e3\u51b3\u5e76\u53d1\u7a0b\u5e8f\u53ef\u89c2\u5bdf\u5c5e\u6027\u7684\u534a\u53ef\u5224\u5b9a\u6027\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u6df7\u5408\u5e42\u57df\u4f5c\u4e3a\u57fa\u7840\u7684\u6307\u793a\u8bed\u4e49\uff0c\u8be5\u5e42\u57df\u7ed3\u5408\u4e86\u4e0d\u786e\u5b9a\u6027\u548c\u6982\u7387\u884c\u4e3a\u3002\u5229\u7528M. Smyth\u63d0\u51fa\u7684\u5c06\u53ef\u89c2\u5bdf\u5c5e\u6027\u89c6\u4e3a\u62d3\u6251\u7a7a\u95f4\u4e2d\u7684\u5f00\u96c6\u7684\u65b9\u6cd5\u6765\u8868\u8ff0\u5b9a\u7406\u3002\u8bc1\u660e\u7684\u5173\u952e\u5728\u4e8e\u6982\u7387\u7f16\u7a0b\u73af\u5883\u4e0bK\u00f6nig\u5f15\u7406\u7684\u62d3\u6251\u63a8\u5e7f\u3002", "result": "\u8bc1\u660e\u4e86\u4e00\u4e2a\u9002\u7528\u4e8e\u6982\u7387\u6027GCL\u5e76\u53d1\u6269\u5c55\u7684\u5145\u5206\u6027\u5b9a\u7406\uff0c\u5e76\u8bc1\u660e\u4e86K\u00f6nig\u5f15\u7406\u7684\u62d3\u6251\u63a8\u5e7f\u3002\u8be5\u5b9a\u7406\u53ef\u4ee5\u63a8\u5bfc\u51fa\u7279\u5b9a\u5f62\u5f0f\u7684\u53ef\u89c2\u5bdf\u5c5e\u6027\u7684\u534a\u53ef\u5224\u5b9a\u6027\u3002", "conclusion": "\u8be5\u8bba\u6587\u7684\u5145\u5206\u6027\u5b9a\u7406\u4e3a\u7406\u89e3\u548c\u5206\u6790\u6982\u7387\u6027GCL\u5e76\u53d1\u7a0b\u5e8f\u7684\u884c\u4e3a\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u5e76\u6697\u793a\u4e86\u5176\u5728\u53ef\u89c2\u5bdf\u5c5e\u6027\u6d4b\u8bd5\u65b9\u9762\u7684\u534a\u53ef\u5224\u5b9a\u6027\uff0c\u8fd9\u4e0eM. Escardo\u5173\u4e8e\u6982\u7387\u6d4b\u8bd5\u7684\u731c\u60f3\u76f8\u5173\u3002"}}
{"id": "2510.23867", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.23867", "abs": "https://arxiv.org/abs/2510.23867", "authors": ["Zhentong Shao", "Jingtao Qin", "Nanpeng Yu"], "title": "Neural Two-Stage Stochastic Volt-VAR Optimization for Three-Phase Unbalanced Distribution Systems with Network Reconfiguration", "comment": null, "summary": "The increasing integration of intermittent distributed energy resources\n(DERs) has introduced significant variability in distribution networks, posing\nchallenges to voltage regulation and reactive power management. This paper\npresents a novel neural two-stage stochastic Volt-VAR optimization (2S-VVO)\nmethod for three-phase unbalanced distribution systems considering network\nreconfiguration under uncertainty. To address the computational intractability\nassociated with solving large-scale scenario-based 2S-VVO problems, a\nlearning-based acceleration strategy is introduced, wherein the second-stage\nrecourse model is approximated by a neural network. This neural approximation\nis embedded into the optimization model as a mixed-integer linear program\n(MILP), enabling effective enforcement of operational constraints related to\nthe first-stage decisions. Numerical simulations on a 123-bus unbalanced\ndistribution system demonstrate that the proposed approach achieves over 50\ntimes speedup compared to conventional solvers and decomposition methods, while\nmaintaining a typical optimality gap below 0.30%. These results underscore the\nmethod's efficacy and scalability in addressing large-scale stochastic VVO\nproblems under practical operating conditions.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684\u4e24\u9636\u6bb5\u968f\u673a\u538b\u7535\u4f18\u5316\uff082S-VVO\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u542b\u6709\u95f4\u6b47\u6027\u80fd\u6e90\u7684\u4e09\u76f8\u4e0d\u5e73\u8861\u914d\u7535\u7f51\u7684\u7535\u538b\u548c\u65e0\u529f\u529f\u7387\u7ba1\u7406\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u52a0\u901f\u8ba1\u7b97\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u8fd1\u4f3c\u6700\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u5206\u5e03\u5f0f\u80fd\u6e90\uff08DER\uff09\u7684\u589e\u52a0\u5bfc\u81f4\u914d\u7535\u7f51\u7684\u7535\u538b\u6ce2\u52a8\u548c\u65e0\u529f\u529f\u7387\u7ba1\u7406\u56f0\u96be\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e24\u9636\u6bb5\u968f\u673a\u538b\u7535\u4f18\u5316\uff082S-VVO\uff09\u65b9\u6cd5\uff0c\u5e76\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u6765\u52a0\u901f\u6c42\u89e3\uff0c\u5c06\u5176\u5d4c\u5165\u5230\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212\uff08MILP\uff09\u6a21\u578b\u4e2d\u3002", "result": "\u8be5\u65b9\u6cd5\u5728123\u8282\u70b9\u7cfb\u7edf\u4e0a\u5b9e\u73b0\u4e86\u8d85\u8fc750\u500d\u7684\u52a0\u901f\uff0c\u4e14\u6700\u4f18\u6027\u5dee\u8ddd\u4f4e\u4e8e0.30%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5730\u89e3\u51b3\u5927\u89c4\u6a21\u968f\u673a\u538b\u7535\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u4e14\u5177\u6709\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2510.23844", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.23844", "abs": "https://arxiv.org/abs/2510.23844", "authors": ["Cameron M. Pike", "Brad Oney", "Gabriel Hepner", "Animesh Yadav"], "title": "Accurate Prediction of Nonlinear Distortion of Multi-Carrier Signals", "comment": "7 Pages, 7 figures, 6 pages, 6 figures, accepted for publication in\n  IEEE TCAS-II", "summary": "Nonlinearities in power amplifiers adversely affect multi-carrier modulation\ntechniques. Accurate prediction of nonlinear distortion is essential for making\ndesign trade-offs between output power and network throughput. We use the\nseries form of the characteristic function (ch.f.) method to predict distortion\nspectra for sparse multi-carrier transmissions. This method results in\nefficient calculations of individual signal and distortion components. The\nmethod is validated both theoretically and practically. Theoretical validation\nis performed by modeling the signal as a bandpass Gaussian process that is hard\nlimited, and it is shown that the series ch.f. method produces results that are\nidentical with the classical Price's theorem. Practical validation is shown by\nconsidering an orthogonal frequency division multiplexing (OFDM) signal with a\nfragmented spectrum which is then applied to an amplifier driven into\ncompression for which application of Price's theorem is difficult, and the\npredicted output spectrum corroborates laboratory measurements. Part of the\ncomputational efficiency is realized in that the nonlinearity can be expressed\nas the fast Fourier transform (FFT) of samples of its forward scattering\nparameter (i.e., S21) or transconductance function (including AM-PM effects),\nand distortion contributions of the signal can be expressed as numerical\nautoconvolutions of the clean spectrum. Signal-to-distortion ratio (SDR) can be\neasily computed and parameterized across variables of interest, such as\noverdrive level.", "AI": {"tldr": "\u4f7f\u7528\u7ea7\u6570\u5f62\u5f0f\u7684\u7279\u5f81\u51fd\u6570\uff08ch.f.\uff09\u65b9\u6cd5\u6765\u9884\u6d4b\u7a00\u758f\u591a\u8f7d\u6ce2\u4f20\u8f93\u7684\u5931\u771f\u9891\u8c31\uff0c\u8be5\u65b9\u6cd5\u5728\u7406\u8bba\u548c\u5b9e\u9645\u4e2d\u90fd\u5f97\u5230\u4e86\u9a8c\u8bc1\u3002", "motivation": "\u51c6\u786e\u9884\u6d4b\u975e\u7ebf\u6027\u5931\u771f\u5bf9\u4e8e\u5728\u8f93\u51fa\u529f\u7387\u548c\u7f51\u7edc\u541e\u5410\u91cf\u4e4b\u95f4\u8fdb\u884c\u8bbe\u8ba1\u6743\u8861\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4f7f\u7528\u7ea7\u6570\u5f62\u5f0f\u7684\u7279\u5f81\u51fd\u6570\uff08ch.f.\uff09\u65b9\u6cd5\u6765\u9884\u6d4b\u7a00\u758f\u591a\u8f7d\u6ce2\u4f20\u8f93\u7684\u5931\u771f\u9891\u8c31\u3002\u5c06\u975e\u7ebf\u6027\u8868\u793a\u4e3a\u524d\u5411\u6563\u5c04\u53c2\u6570\uff08S21\uff09\u6216\u8de8\u5bfc\u51fd\u6570\u7684\u5feb\u901f\u5085\u91cc\u53f6\u53d8\u6362\uff08FFT\uff09\uff0c\u5e76\u5c06\u5931\u771f\u8d21\u732e\u8868\u793a\u4e3a\u5e72\u51c0\u9891\u8c31\u7684\u6570\u503c\u81ea\u5377\u79ef\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u7406\u8bba\u4e0a\u4e0e\u7ecf\u5178Price\u5b9a\u7406\u5f97\u5230\u76f8\u540c\u7ed3\u679c\uff0c\u5728\u5b9e\u9645\u4e2d\u4e0eOFDM\u4fe1\u53f7\u5728\u5b9e\u9a8c\u5ba4\u6d4b\u91cf\u7ed3\u679c\u4e00\u81f4\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u7ea7\u6570ch.f.\u65b9\u6cd5\u80fd\u591f\u9ad8\u6548\u5730\u8ba1\u7b97\u5355\u4e2a\u4fe1\u53f7\u548c\u5931\u771f\u5206\u91cf\uff0c\u5e76\u80fd\u8f7b\u677e\u8ba1\u7b97\u4fe1\u566a\u6bd4\uff08SDR\uff09\u548c\u53c2\u6570\u5316"}}
{"id": "2510.23624", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.23624", "abs": "https://arxiv.org/abs/2510.23624", "authors": ["Tiago Mendon\u00e7a dos Santos", "Rafael Izbicki", "Lu\u00eds Gustavo Esteves"], "title": "DiNo and RanBu: Lightweight Predictions from Shallow Random Forests", "comment": null, "summary": "Random Forest ensembles are a strong baseline for tabular prediction tasks,\nbut their reliance on hundreds of deep trees often results in high inference\nlatency and memory demands, limiting deployment in latency-sensitive or\nresource-constrained environments. We introduce DiNo (Distance with Nodes) and\nRanBu (Random Bushes), two shallow-forest methods that convert a small set of\ndepth-limited trees into efficient, distance-weighted predictors. DiNo measures\ncophenetic distances via the most recent common ancestor of observation pairs,\nwhile RanBu applies kernel smoothing to Breiman's classical proximity measure.\nBoth approaches operate entirely after forest training: no additional trees are\ngrown, and tuning of the single bandwidth parameter $h$ requires only\nlightweight matrix-vector operations. Across three synthetic benchmarks and 25\npublic datasets, RanBu matches or exceeds the accuracy of full-depth random\nforests-particularly in high-noise settings-while reducing training plus\ninference time by up to 95\\%. DiNo achieves the best bias-variance trade-off in\nlow-noise regimes at a modest computational cost. Both methods extend directly\nto quantile regression, maintaining accuracy with substantial speed gains. The\nimplementation is available as an open-source R/C++ package at\nhttps://github.com/tiagomendonca/dirf. We focus on structured tabular random\nsamples (i.i.d.), leaving extensions to other modalities for future work.", "AI": {"tldr": "DiNo and RanBu\u662f\u4e24\u79cd\u6d45\u5c42\u68ee\u6797\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5c06\u6df1\u5ea6\u53d7\u9650\u7684\u6811\u6728\u8f6c\u5316\u4e3a\u9ad8\u6548\u7684\u8ddd\u79bb\u52a0\u6743\u9884\u6d4b\u5668\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e86\u8bad\u7ec3\u548c\u63a8\u7406\u65f6\u95f4\u3002", "motivation": "\u968f\u673a\u68ee\u6797\u6a21\u578b\u5728\u8868\u683c\u9884\u6d4b\u4efb\u52a1\u4e2d\u867d\u7136\u8868\u73b0\u5f3a\u52b2\uff0c\u4f46\u5176\u6df1\u5ea6\u548c\u6570\u91cf\u4f17\u591a\u7684\u6811\u6728\u5bfc\u81f4\u4e86\u9ad8\u63a8\u7406\u5ef6\u8fdf\u548c\u5185\u5b58\u9700\u6c42\uff0c\u9650\u5236\u4e86\u5176\u5728\u5bf9\u5ef6\u8fdf\u654f\u611f\u6216\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u90e8\u7f72\u3002", "method": "DiNo\u901a\u8fc7\u6700\u8fd1\u516c\u5171\u7956\u5148\u6d4b\u91cf\u5171\u73b0\u8ddd\u79bb\uff0cRanBu\u5c06\u6838\u5e73\u6ed1\u5e94\u7528\u4e8eBreiman\u7684\u7ecf\u5178\u90bb\u8fd1\u5ea6\u91cf\u3002\u8fd9\u4e24\u79cd\u65b9\u6cd5\u90fd\u5728\u68ee\u6797\u8bad\u7ec3\u540e\u8fdb\u884c\uff0c\u65e0\u9700\u989d\u5916\u751f\u957f\u6811\u6728\uff0c\u5e76\u4e14\u4ec5\u9700\u8981\u8f7b\u91cf\u7ea7\u7684\u77e9\u9635\u5411\u91cf\u8fd0\u7b97\u5373\u53ef\u8c03\u6574\u5355\u4e2a\u5e26\u5bbd\u53c2\u6570h\u3002", "result": "\u5728\u4e09\u4e2a\u5408\u6210\u57fa\u51c6\u548c25\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\uff0cRanBu\u7684\u51c6\u786e\u6027\u4e0e\u5168\u6df1\u5ea6\u968f\u673a\u68ee\u6797\u76f8\u5f53\u751a\u81f3\u66f4\u9ad8\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u566a\u58f0\u73af\u5883\u4e0b\uff0c\u540c\u65f6\u5c06\u8bad\u7ec3\u548c\u63a8\u7406\u65f6\u95f4\u7f29\u77ed\u4e86\u9ad8\u8fbe95%\u3002DiNo\u5728\u4f4e\u566a\u58f0\u73af\u5883\u4e0b\u5b9e\u73b0\u4e86\u6700\u4f73\u7684\u504f\u5dee-\u65b9\u5dee\u6743\u8861\uff0c\u8ba1\u7b97\u6210\u672c\u9002\u4e2d\u3002", "conclusion": "DiNo\u548cRanBu\u4f5c\u4e3a\u4e24\u79cd\u9ad8\u6548\u7684\u6d45\u5c42\u68ee\u6797\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u968f\u673a\u68ee\u6797\u7684\u90e8\u7f72\u9650\u5236\u95ee\u9898\uff0c\u5e76\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u7684\u5e73\u8861\uff0c\u540c\u65f6\u8fd8\u652f\u6301\u5206\u4f4d\u6570\u56de\u5f52\u4efb\u52a1\u3002"}}
{"id": "2510.23746", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23746", "abs": "https://arxiv.org/abs/2510.23746", "authors": ["Laura Mismetti", "Marvin Alberts", "Andreas Krause", "Mara Graziani"], "title": "Test-Time Tuned Language Models Enable End-to-end De Novo Molecular Structure Generation from MS/MS Spectra", "comment": null, "summary": "Tandem Mass Spectrometry enables the identification of unknown compounds in\ncrucial fields such as metabolomics, natural product discovery and\nenvironmental analysis. However, current methods rely on database matching from\npreviously observed molecules, or on multi-step pipelines that require\nintermediate fragment or fingerprint prediction. This makes finding the correct\nmolecule highly challenging, particularly for compounds absent from reference\ndatabases. We introduce a framework that, by leveraging test-time tuning,\nenhances the learning of a pre-trained transformer model to address this gap,\nenabling end-to-end de novo molecular structure generation directly from the\ntandem mass spectra and molecular formulae, bypassing manual annotations and\nintermediate steps. We surpass the de-facto state-of-the-art approach DiffMS on\ntwo popular benchmarks NPLIB1 and MassSpecGym by 100% and 20%, respectively.\nTest-time tuning on experimental spectra allows the model to dynamically adapt\nto novel spectra, and the relative performance gain over conventional\nfine-tuning is of 62% on MassSpecGym. When predictions deviate from the ground\ntruth, the generated molecular candidates remain structurally accurate,\nproviding valuable guidance for human interpretation and more reliable\nidentification.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5229\u7528\u6d4b\u8bd5\u65f6\u8c03\u4f18\u589e\u5f3a\u9884\u8bad\u7ec3 Transformer \u6a21\u578b\u7684\u65b0\u6846\u67b6\uff0c\u53ef\u76f4\u63a5\u4ece\u4e32\u8054\u8d28\u8c31\u548c\u5206\u5b50\u5f0f\u751f\u6210\u5206\u5b50\u7ed3\u6784\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6570\u636e\u5e93\u5339\u914d\u6216\u591a\u6b65\u9884\u6d4b\u7684\u5c40\u9650\u6027\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u6570\u636e\u5e93\u4e2d\u4e0d\u5b58\u5728\u7684\u5316\u5408\u7269\u3002\u8be5\u65b9\u6cd5\u5728\u4e24\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u5e76\u80fd\u5728\u9884\u6d4b\u504f\u5dee\u65f6\u63d0\u4f9b\u7ed3\u6784\u51c6\u786e\u7684\u5019\u9009\u5206\u5b50\uff0c\u8f85\u52a9\u4eba\u5de5\u5224\u8bfb\u3002", "motivation": "\u73b0\u6709\u8d28\u8c31\u5206\u6790\u65b9\u6cd5\u4f9d\u8d56\u6570\u636e\u5e93\u5339\u914d\u6216\u591a\u6b65\u9884\u6d4b\uff0c\u96be\u4ee5\u8bc6\u522b\u6570\u636e\u5e93\u5916\u5316\u5408\u7269\uff1b\u9700\u8981\u4e00\u79cd\u80fd\u76f4\u63a5\u4ece\u8d28\u8c31\u548c\u5206\u5b50\u5f0f\u751f\u6210\u5206\u5b50\u7ed3\u6784\u7684\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u6d4b\u8bd5\u65f6\u8c03\u4f18\uff08test-time tuning\uff09\u589e\u5f3a\u9884\u8bad\u7ec3 Transformer \u6a21\u578b\uff0c\u5b9e\u73b0\u4ece\u4e32\u8054\u8d28\u8c31\u548c\u5206\u5b50\u5f0f\u5230\u5206\u5b50\u7ed3\u6784\u7684\u7aef\u5230\u7aef\u751f\u6210\u3002", "result": "\u5728 NPLIB1 \u548c MassSpecGym \u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5206\u522b\u8d85\u8d8a\u4e86 DiffMS \u65b9\u6cd5 100% \u548c 20%\uff1b\u5728 MassSpecGym \u4e0a\uff0c\u4e0e\u5e38\u89c4\u5fae\u8c03\u76f8\u6bd4\uff0c\u6d4b\u8bd5\u65f6\u8c03\u4f18\u5e26\u6765\u4e86 62% \u7684\u76f8\u5bf9\u6027\u80fd\u63d0\u5347\uff1b\u751f\u6210\u7684\u5206\u5b50\u5019\u9009\u5728\u504f\u79bb\u771f\u503c\u65f6\u4ecd\u4fdd\u6301\u7ed3\u6784\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u6d4b\u8bd5\u65f6\u8c03\u4f18\uff0c\u5b9e\u73b0\u4e86\u5bf9\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u6709\u6548\u589e\u5f3a\uff0c\u80fd\u591f\u76f4\u63a5\u4ece\u4e32\u8054\u8d28\u8c31\u548c\u5206\u5b50\u5f0f\u751f\u6210\u5206\u5b50\u7ed3\u6784\uff0c\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u4e0d\u8db3\uff0c\u5c24\u5176\u5728\u5904\u7406\u672a\u77e5\u5316\u5408\u7269\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u80fd\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u7ed3\u6784\u4fe1\u606f\u8f85\u52a9\u4eba\u5de5\u5224\u8bfb\u3002"}}
{"id": "2510.23908", "categories": ["eess.SP", "cs.ET"], "pdf": "https://arxiv.org/pdf/2510.23908", "abs": "https://arxiv.org/abs/2510.23908", "authors": ["M. T. Hassan", "D. Zelenchuk", "M. A. B. Abbasi"], "title": "Machine Learning-Driven User Localization in RIS-Assisted Wireless Systems", "comment": null, "summary": "The sixth generation (6G) targets ultra reliable, low latency (URLLC) gigabit\nconnectivity in mmWave bands, where directional channels require precise beam\nalignment. Reconfigurable intelligent surfaces (RIS) reshape wave propagation\nand extend coverage, but they enlarge the beam search space at the base\nstation, making exhaustive sweeps inefficient due to control overhead and\nlatency. We propose an ML based user localization framework for RIS assisted\ncommunication at 27 GHz. A 20x20 RIS reflects signals from a core network\nconnected base station and sweeps beams across the 0-90 degree elevation plane,\ndivided into four angular sectors. We build a dataset by recording received\nsignal power (Pr in dBm) across user locations and train multiple regressors,\nincluding decision tree (DT), support vector regressor (SVR), k nearest\nneighbor (KNN), XGBoost, gradient boosting, and random forest. In operation, an\nunknown user in the same plane measures four received power values (one per\nsector) and reports them to the pretrained RIS controller, which predicts the\nuser's angular position in real time. Evaluation using mean absolute error\n(MAE), root mean squared error (RMSE), and R squared (R2) shows high accuracy.\nThe DT model achieves an MAE of 4.8 degrees with R2 = 0.96, while other models\nreach 70 to 86 percent. Predicted radiation patterns, including main lobe\nalignment between 52 and 55 degrees, closely track ground truth. The framework\nreduces beam probing, enables faster alignment, and lowers latency for RIS\nassisted 6G networks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684RIS\u8f85\u52a9\u901a\u4fe1\u7528\u6237\u5b9a\u4f4d\u6846\u67b6\uff0c\u7528\u4e8e6G\u6beb\u7c73\u6ce2\u9891\u6bb5\uff0c\u4ee5\u89e3\u51b3\u4f20\u7edf\u6ce2\u675f\u641c\u7d22\u6548\u7387\u4f4e\u7684\u95ee\u9898\u3002", "motivation": "6G URLLC\u8981\u6c42\u4f4e\u5ef6\u8fdf\u548c\u9ad8\u53ef\u9760\u6027\uff0c\u4f46\u6beb\u7c73\u6ce2\u9891\u6bb5\u7684\u5b9a\u5411\u4fe1\u9053\u548cRIS\u7684\u4f7f\u7528\u589e\u5927\u4e86\u6ce2\u675f\u641c\u7d22\u7a7a\u95f4\uff0c\u5bfc\u81f4\u4f20\u7edf\u65b9\u6cd5\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u7528\u6237\u5b9a\u4f4d\u6846\u67b6\u3002\u4f7f\u752820x20 RIS\u53cd\u5c04\u57fa\u7ad9\u4fe1\u53f7\uff0c\u5c060-90\u5ea6\u4ef0\u89d2\u5e73\u9762\u5212\u5206\u4e3a\u56db\u4e2a\u6247\u533a\u3002\u901a\u8fc7\u8bb0\u5f55\u4e0d\u540c\u7528\u6237\u4f4d\u7f6e\u7684\u63a5\u6536\u4fe1\u53f7\u529f\u7387\uff08Pr in dBm\uff09\u6765\u6784\u5efa\u6570\u636e\u96c6\uff0c\u5e76\u8bad\u7ec3\u4e86\u5305\u62ec\u51b3\u7b56\u6811\uff08DT\uff09\u3001\u652f\u6301\u5411\u91cf\u56de\u5f52\uff08SVR\uff09\u3001K\u8fd1\u90bb\uff08KNN\uff09\u3001XGBoost\u3001\u68af\u5ea6\u63d0\u5347\u548c\u968f\u673a\u68ee\u6797\u5728\u5185\u7684\u591a\u79cd\u56de\u5f52\u6a21\u578b\u3002\u5728\u5b9e\u9645\u64cd\u4f5c\u4e2d\uff0c\u672a\u77e5\u7528\u6237\u6d4b\u91cf\u56db\u4e2a\u6247\u533a\u7684\u63a5\u6536\u529f\u7387\u503c\uff0c\u5e76\u5c06\u5176\u62a5\u544a\u7ed9\u9884\u8bad\u7ec3\u7684RIS\u63a7\u5236\u5668\uff0c\u63a7\u5236\u5668\u5b9e\u65f6\u9884\u6d4b\u7528\u6237\u7684\u89d2\u5ea6\u4f4d\u7f6e\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0cDT\u6a21\u578b\u5b9e\u73b0\u4e864.8\u5ea6\u7684\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\uff08MAE\uff09\u548c0.96\u7684R\u65b9\u503c\uff08R2\uff09\u3002\u5176\u4ed6\u6a21\u578b\u4e5f\u8fbe\u5230\u4e8670%\u523086%\u7684\u51c6\u786e\u7387\u3002\u9884\u6d4b\u7684\u8f90\u5c04\u65b9\u5411\u56fe\u4e0e\u771f\u5b9e\u503c\u9ad8\u5ea6\u543b\u5408\uff0c\u4e3b\u74e3\u5bf9\u9f50\u89d2\u5ea6\u572852\u523055\u5ea6\u4e4b\u95f4\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u663e\u8457\u51cf\u5c11\u4e86\u6ce2\u675f\u63a2\u6d4b\uff0c\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u6ce2\u675f\u5bf9\u9f50\uff0c\u964d\u4f4e\u4e86RIS\u8f85\u52a96G\u7f51\u7edc\u7684\u5ef6\u8fdf\uff0c\u63d0\u9ad8\u4e86\u901a\u4fe1\u6548\u7387\u3002"}}
{"id": "2510.23692", "categories": ["cs.SI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.23692", "abs": "https://arxiv.org/abs/2510.23692", "authors": ["Calla Beauregard", "Parisa Suchdev", "Ashley M. A. Fehr", "Isabelle T. Smith", "Tabia Tanzin Prama", "Julia Witte Zimmerman", "Carter Ward", "Juniper Lovato", "Christopher M. Danforth", "Peter Sheridan Dodd"], "title": "Detecting sub-populations in online health communities: A mixed-methods exploration of breastfeeding messages in BabyCenter Birth Clubs", "comment": null, "summary": "Parental stress is a nationwide health crisis according to the U.S. Surgeon\nGeneral's 2024 advisory. To allay stress, expecting parents seek advice and\nshare experiences in a variety of venues, from in-person birth education\nclasses and parenting groups to virtual communities, for example, BabyCenter, a\nmoderated online forum community with over 4 million members in the United\nStates alone. In this study, we aim to understand how parents talk about\npregnancy, birth, and parenting by analyzing 5.43M posts and comments from the\nApril 2017--January 2024 cohort of 331,843 BabyCenter \"birth club\" users (that\nis, users who participate in due date forums or \"birth clubs\" based on their\nbabies' due dates). Using BERTopic to locate breastfeeding threads and LDA to\nsummarize themes, we compare documents in breastfeeding threads to all other\nbirth-club content. Analyzing time series of word rank, we find that posts and\ncomments containing anxiety-related terms increased steadily from April 2017 to\nJanuary 2024. We used an ensemble of topic models to identify dominant\nbreastfeeding topics within birth clubs, and then explored trends among all\nuser content versus those who posted in threads related to breastfeeding\ntopics. We conducted Latent Dirichlet Allocation (LDA) topic modeling to\nidentify the most common topics in the full population, as well as within the\nsubset breastfeeding population. We find that the topic of sleep dominates in\ncontent generated by the breastfeeding population, as well anxiety-related and\nwork/daycare topics that are not predominant in the full BabyCenter birth club\ndataset.", "AI": {"tldr": "\u7f8e\u56fd\u4e00\u9879\u7814\u7a76\u901a\u8fc7\u5206\u6790BabyCenter\u5728\u7ebf\u8bba\u575b\u7684\u7528\u6237\u53d1\u5e16\uff0c\u53d1\u73b0\u4ece2017\u5e744\u6708\u52302024\u5e741\u6708\uff0c\u4e0e\u6bcd\u4e73\u5582\u517b\u76f8\u5173\u5e16\u5b50\u4e2d\u4e0e\u7126\u8651\u3001\u7761\u7720\u548c\u65e5\u6258\u76f8\u5173\u7684\u8bdd\u9898\u663e\u8457\u589e\u591a\uff0c\u800c\u7126\u8651\u76f8\u5173\u8bcd\u6c47\u5728\u6240\u6709\u5e16\u5b50\u4e2d\u4e5f\u5448\u7a33\u5b9a\u589e\u957f\u8d8b\u52bf\u3002", "motivation": "\u4e3a\u4e86\u7f13\u89e3\u7236\u6bcd\u7684\u538b\u529b\uff0c\u4e86\u89e3\u4ed6\u4eec\u5728\u6000\u5b55\u3001\u5206\u5a29\u548c\u80b2\u513f\u8fc7\u7a0b\u4e2d\u5982\u4f55\u4ea4\u6d41\u548c\u5bfb\u6c42\u5efa\u8bae\uff0c\u7279\u522b\u5173\u6ce8\u6bcd\u4e73\u5582\u517b\u8bdd\u9898\u3002", "method": "\u5229\u7528BERTopic\u8bc6\u522b\u6bcd\u4e73\u5582\u517b\u76f8\u5173\u5e16\u5b50\uff0cLDA\u6a21\u578b\u603b\u7ed3\u4e3b\u9898\uff0c\u5e76\u5bf9\u6bcd\u4e73\u5582\u517b\u5e16\u5b50\u4e0e\u6240\u6709\u5176\u4ed6\u5e16\u5b50\u8fdb\u884c\u6bd4\u8f83\uff0c\u540c\u65f6\u5206\u6790\u8bcd\u6c47\u6392\u540d\u7684\u65f6\u95f4\u5e8f\u5217\u3002", "result": "\u53d1\u73b0\u5728\u6bcd\u4e73\u5582\u517b\u76f8\u5173\u5e16\u5b50\u4e2d\uff0c\u7761\u7720\u3001\u7126\u8651\u4ee5\u53ca\u5de5\u4f5c/\u65e5\u6258\u662f\u4e3b\u5bfc\u8bdd\u9898\uff0c\u8fd9\u5728\u6240\u6709\u5e16\u5b50\u4e2d\u5e76\u4e0d\u7a81\u51fa\u3002\u540c\u65f6\uff0c\u7126\u8651\u76f8\u5173\u8bcd\u6c47\u57282017\u5e744\u6708\u81f32024\u5e741\u6708\u671f\u95f4\u5448\u7a33\u6b65\u589e\u957f\u3002", "conclusion": "\u6bcd\u4e73\u5582\u517b\u793e\u7fa4\u9762\u4e34\u7740\u72ec\u7279\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u7761\u7720\u548c\u7126\u8651\u65b9\u9762\uff0c\u8fd9\u53ef\u80fd\u9700\u8981\u66f4\u6709\u9488\u5bf9\u6027\u7684\u652f\u6301\u548c\u8d44\u6e90\u3002"}}
{"id": "2510.24438", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.24438", "abs": "https://arxiv.org/abs/2510.24438", "authors": ["Abdullah Mushtaq", "Rafay Naeem", "Ezieddin Elmahjub", "Ibrahim Ghaznavi", "Shawqi Al-Maliki", "Mohamed Abdallah", "Ala Al-Fuqaha", "Junaid Qadir"], "title": "Can LLMs Write Faithfully? An Agent-Based Evaluation of LLM-generated Islamic Content", "comment": "Accepted at 39th Conference on Neural Information Processing Systems\n  (NeurIPS 2025) Workshop: 5th Muslims in Machine Learning (MusIML) Workshop", "summary": "Large language models are increasingly used for Islamic guidance, but risk\nmisquoting texts, misapplying jurisprudence, or producing culturally\ninconsistent responses. We pilot an evaluation of GPT-4o, Ansari AI, and Fanar\non prompts from authentic Islamic blogs. Our dual-agent framework uses a\nquantitative agent for citation verification and six-dimensional scoring (e.g.,\nStructure, Islamic Consistency, Citations) and a qualitative agent for\nfive-dimensional side-by-side comparison (e.g., Tone, Depth, Originality).\nGPT-4o scored highest in Islamic Accuracy (3.93) and Citation (3.38), Ansari AI\nfollowed (3.68, 3.32), and Fanar lagged (2.76, 1.82). Despite relatively strong\nperformance, models still fall short in reliably producing accurate Islamic\ncontent and citations -- a paramount requirement in faith-sensitive writing.\nGPT-4o had the highest mean quantitative score (3.90/5), while Ansari AI led\nqualitative pairwise wins (116/200). Fanar, though trailing, introduces\ninnovations for Islamic and Arabic contexts. This study underscores the need\nfor community-driven benchmarks centering Muslim perspectives, offering an\nearly step toward more reliable AI in Islamic knowledge and other high-stakes\ndomains such as medicine, law, and journalism.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4f0a\u65af\u5170\u6307\u5bfc\u65b9\u9762\u5b58\u5728\u6587\u672c\u5f15\u7528\u4e0d\u51c6\u786e\u3001\u6559\u6cd5\u5e94\u7528\u9519\u8bef\u6216\u6587\u5316\u4e0d\u4e00\u81f4\u7684\u98ce\u9669\u3002\u672c\u7814\u7a76\u8bc4\u4f30\u4e86 GPT-4o\u3001Ansari AI \u548c Fanar \u5728\u5904\u7406\u6e90\u81ea\u771f\u5b9e\u4f0a\u65af\u5170\u535a\u5ba2\u7684\u63d0\u793a\u65b9\u9762\u7684\u8868\u73b0\u3002", "motivation": "\u8bc4\u4f30 GPT-4o\u3001Ansari AI \u548c Fanar \u5728\u5904\u7406\u4f0a\u65af\u5170\u6307\u5bfc\u65b9\u9762\u7684\u80fd\u529b\uff0c\u5e76\u8bc6\u522b\u5b83\u4eec\u5728\u51c6\u786e\u6027\u3001\u6559\u6cd5\u5e94\u7528\u548c\u6587\u5316\u4e00\u81f4\u6027\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "method": "\u4f7f\u7528\u4e00\u4e2a\u53cc\u4ee3\u7406\u6846\u67b6\uff0c\u5305\u62ec\u4e00\u4e2a\u91cf\u5316\u4ee3\u7406\uff08\u8d1f\u8d23\u5f15\u7528\u9a8c\u8bc1\u548c\u516d\u4e2a\u7ef4\u5ea6\u7684\u8bc4\u5206\uff0c\u5982\u7ed3\u6784\u3001\u4f0a\u65af\u5170\u4e00\u81f4\u6027\u548c\u5f15\u7528\uff09\u548c\u4e00\u4e2a\u8d28\u5316\u4ee3\u7406\uff08\u8d1f\u8d23\u4e94\u4e2a\u7ef4\u5ea6\u7684\u5e76\u6392\u6bd4\u8f83\uff0c\u5982\u8bed\u6c14\u3001\u6df1\u5ea6\u548c\u539f\u521b\u6027\uff09\u3002", "result": "GPT-4o \u5728\u4f0a\u65af\u5170\u51c6\u786e\u6027\u548c\u5f15\u7528\u65b9\u9762\u5f97\u5206\u6700\u9ad8\uff08\u5206\u522b\u4e3a 3.93 \u548c 3.38\uff09\uff0cAnsari AI \u7d27\u968f\u5176\u540e\uff08\u5206\u522b\u4e3a 3.68 \u548c 3.32\uff09\uff0cFanar \u843d\u540e\uff08\u5206\u522b\u4e3a 2.76 \u548c 1.82\uff09\u3002GPT-4o \u7684\u5e73\u5747\u91cf\u5316\u5f97\u5206\u6700\u9ad8\uff083.90/5\uff09\uff0c\u800c Ansari AI \u5728\u8d28\u5316\u4e24\u4e24\u6bd4\u8f83\u4e2d\u83b7\u80dc\u6b21\u6570\u6700\u591a\uff08116/200\uff09\u3002Fanar \u4e5f\u5728\u4e3a\u4f0a\u65af\u5170\u548c\u963f\u62c9\u4f2f\u8bed\u5883\u5e26\u6765\u521b\u65b0\u3002", "conclusion": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4f0a\u65af\u5170\u6307\u5bfc\u65b9\u9762\u53d6\u5f97\u4e86\u76f8\u5bf9\u8f83\u5f3a\u7684\u6027\u80fd\uff0c\u4f46\u5b83\u4eec\u5728\u53ef\u9760\u5730\u751f\u6210\u51c6\u786e\u7684\u4f0a\u65af\u5170\u5185\u5bb9\u548c\u5f15\u7528\u65b9\u9762\u4ecd\u6709\u4e0d\u8db3\uff0c\u800c\u8fd9\u5728\u4fe1\u4ef0\u654f\u611f\u7684\u5199\u4f5c\u4e2d\u662f\u81f3\u5173\u91cd\u8981\u7684\u3002\u7814\u7a76\u5f3a\u8c03\u4e86\u5efa\u7acb\u4ee5\u7a46\u65af\u6797\u89c6\u89d2\u4e3a\u4e2d\u5fc3\u7684\u793e\u533a\u9a71\u52a8\u57fa\u51c6\u7684\u5fc5\u8981\u6027\uff0c\u4e3a\u63d0\u9ad8 AI \u5728\u4f0a\u65af\u5170\u77e5\u8bc6\u4ee5\u53ca\u533b\u5b66\u3001\u6cd5\u5f8b\u548c\u65b0\u95fb\u7b49\u9ad8\u98ce\u9669\u9886\u57df\u7684\u53ef\u9760\u6027\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2510.23845", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23845", "abs": "https://arxiv.org/abs/2510.23845", "authors": ["Grace Byun", "Rebecca Lipschutz", "Sean T. Minton", "Abigail Lott", "Jinho D. Choi"], "title": "CRADLE Bench: A Clinician-Annotated Benchmark for Multi-Faceted Mental Health Crisis and Safety Risk Detection", "comment": null, "summary": "Detecting mental health crisis situations such as suicide ideation, rape,\ndomestic violence, child abuse, and sexual harassment is a critical yet\nunderexplored challenge for language models. When such situations arise during\nuser--model interactions, models must reliably flag them, as failure to do so\ncan have serious consequences. In this work, we introduce CRADLE BENCH, a\nbenchmark for multi-faceted crisis detection. Unlike previous efforts that\nfocus on a limited set of crisis types, our benchmark covers seven types\ndefined in line with clinical standards and is the first to incorporate\ntemporal labels. Our benchmark provides 600 clinician-annotated evaluation\nexamples and 420 development examples, together with a training corpus of\naround 4K examples automatically labeled using a majority-vote ensemble of\nmultiple language models, which significantly outperforms single-model\nannotation. We further fine-tune six crisis detection models on subsets defined\nby consensus and unanimous ensemble agreement, providing complementary models\ntrained under different agreement criteria.", "AI": {"tldr": "CRADLE BENCH\u662f\u4e00\u4e2a\u7528\u4e8e\u591a\u65b9\u9762\u5371\u673a\u68c0\u6d4b\u7684\u57fa\u51c6\uff0c\u6db5\u76d6\u4e03\u79cd\u4e34\u5e8a\u6807\u51c6\u5b9a\u4e49\u7684\u5371\u673a\u7c7b\u578b\uff0c\u5e76\u9996\u6b21\u5305\u542b\u65f6\u95f4\u6807\u7b7e\uff0c\u65e8\u5728\u89e3\u51b3\u8bed\u8a00\u6a21\u578b\u5728\u68c0\u6d4b\u81ea\u6740\u610f\u5ff5\u3001\u5f3a\u5978\u3001\u5bb6\u5ead\u66b4\u529b\u3001\u8650\u5f85\u513f\u7ae5\u548c\u6027\u9a9a\u6270\u7b49\u5fc3\u7406\u5065\u5eb7\u5371\u673a\u65b9\u9762\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u7684\u8bed\u8a00\u6a21\u578b\u5728\u68c0\u6d4b\u5fc3\u7406\u5065\u5eb7\u5371\u673a\uff08\u5982\u81ea\u6740\u610f\u5ff5\u3001\u5f3a\u5978\u3001\u5bb6\u5ead\u66b4\u529b\u3001\u8650\u5f85\u513f\u7ae5\u548c\u6027\u9a9a\u6270\uff09\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u800c\u6a21\u578b\u5728\u7528\u6237-\u6a21\u578b\u4ea4\u4e92\u4e2d\u53ef\u9760\u5730\u8bc6\u522b\u8fd9\u4e9b\u60c5\u51b5\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u4e3a\u5931\u8bef\u53ef\u80fd\u5bfc\u81f4\u4e25\u91cd\u540e\u679c\u3002", "method": "CRADLE BENCH\u662f\u4e00\u4e2a\u5305\u542b600\u4e2a\u4e34\u5e8a\u533b\u751f\u6807\u6ce8\u7684\u8bc4\u4f30\u6837\u672c\u548c420\u4e2a\u5f00\u53d1\u6837\u672c\u7684\u57fa\u51c6\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7ea64K\u4e2a\u6837\u672c\u7684\u8bad\u7ec3\u8bed\u6599\u5e93\uff0c\u8fd9\u4e9b\u6837\u672c\u662f\u4f7f\u7528\u591a\u79cd\u8bed\u8a00\u6a21\u578b\u591a\u6570\u6295\u7968\u96c6\u6210\u81ea\u52a8\u6807\u6ce8\u7684\u3002\u6b64\u5916\uff0c\u8fd8\u5bf9\u516d\u4e2a\u5371\u673a\u68c0\u6d4b\u6a21\u578b\u8fdb\u884c\u4e86\u5fae\u8c03\uff0c\u8fd9\u4e9b\u6a21\u578b\u5728\u4e0d\u540c\u4e00\u81f4\u6027\u6807\u51c6\u4e0b\u8bad\u7ec3\uff0c\u4ee5\u63d0\u4f9b\u4e92\u8865\u7684\u6a21\u578b\u3002", "result": "\u81ea\u52a8\u6807\u6ce8\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u5355\u6a21\u578b\u6807\u6ce8\uff0c\u5e76\u4e14\u5bf9\u516d\u4e2a\u5371\u673a\u68c0\u6d4b\u6a21\u578b\u8fdb\u884c\u4e86\u5fae\u8c03\uff0c\u8fd9\u4e9b\u6a21\u578b\u5728\u4e0d\u540c\u4e00\u81f4\u6027\u6807\u51c6\u4e0b\u8bad\u7ec3\u3002", "conclusion": "CRADLE BENCH\u901a\u8fc7\u63d0\u4f9b\u4e00\u4e2a\u5168\u9762\u3001\u5305\u542b\u65f6\u95f4\u6807\u7b7e\u7684\u57fa\u51c6\uff0c\u5e76\u91c7\u7528\u5148\u8fdb\u7684\u6807\u6ce8\u548c\u6a21\u578b\u8bad\u7ec3\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u8bed\u8a00\u6a21\u578b\u5728\u5371\u673a\u68c0\u6d4b\u65b9\u9762\u7684\u6311\u6218\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2510.23954", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.23954", "abs": "https://arxiv.org/abs/2510.23954", "authors": ["Pejman Kheradmand", "Behnam Moradkhani", "Raghavasimhan Sankaranarayanan", "Kent K. Yamamoto", "Tanner J. Zachem", "Patrick J. Codd", "Yash Chitalia", "Pierre E. Dupont"], "title": "A Comprehensive General Model of Tendon-Actuated Concentric Tube Robots with Multiple Tubes and Tendons", "comment": null, "summary": "Tendon-actuated concentric tube mechanisms combine the advantages of\ntendon-driven continuum robots and concentric tube robots while addressing\ntheir respective limitations. They overcome the restricted degrees of freedom\noften seen in tendon-driven designs, and mitigate issues such as snapping\ninstability associated with concentric tube robots. However, a complete and\ngeneral mechanical model for these systems remains an open problem. In this\nwork, we propose a Cosserat rod-based framework for modeling the general case\nof $n$ concentric tubes, each actuated by $m_i$ tendons, where $i = \\{1,\n\\ldots, n\\}$. The model allows each tube to twist and elongate while enforcing\na shared centerline for bending. We validate the proposed framework through\nexperiments with two-tube and three tube assemblies under various tendon\nrouting configurations, achieving tip prediction errors $<4\\%$ of the robot's\ntotal length. We further demonstrate the model's generality by applying it to\nexisting robots in the field, where maximum tip deviations remain around $5\\%$\nof the total length. This model provides a foundation for accurate shape\nestimation and control of advanced tendon-actuated concentric tube robots.", "AI": {"tldr": "\u6587\u7ae0\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e Cosserat \u6881\u7684\u901a\u7528\u529b\u5b66\u6a21\u578b\uff0c\u7528\u4e8e\u6a21\u62df\u8171\u9a71\u52a8\u540c\u5fc3\u7ba1\u673a\u5668\u4eba\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u8bbe\u8ba1\u4e2d\u81ea\u7531\u5ea6\u53d7\u9650\u548c\u6613\u53d1\u751f\u5f39\u8df3\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u8171\u9a71\u52a8\u8fde\u7eed\u4f53\u673a\u5668\u4eba\u548c\u540c\u5fc3\u7ba1\u673a\u5668\u4eba\u5728\u81ea\u7531\u5ea6\u548c\u7a33\u5b9a\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u4e00\u4e2a\u66f4\u901a\u7528\u7684\u6a21\u578b\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e Cosserat \u6881\u7684\u529b\u5b66\u5efa\u6a21\u6846\u67b6\uff0c\u80fd\u591f\u6a21\u62df\u5177\u6709 $n$ \u6839\u540c\u5fc3\u7ba1\u548c $m_i$ \u6839\u9a71\u52a8\u8171\u7684\u7cfb\u7edf\uff0c\u5141\u8bb8\u6bcf\u6839\u7ba1\u5b50\u72ec\u7acb\u626d\u8f6c\u548c\u4f38\u957f\uff0c\u4f46\u5171\u4eab\u5f2f\u66f2\u4e2d\u5fc3\u7ebf\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8be5\u6a21\u578b\u5728\u4e24\u7ba1\u548c\u4e09\u7ba1\u673a\u5668\u4eba\u4e0a\u7684\u5c16\u7aef\u9884\u6d4b\u8bef\u5dee\u5c0f\u4e8e\u673a\u5668\u4eba\u603b\u957f\u5ea6\u7684 4%\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8e\u73b0\u6709\u673a\u5668\u4eba\uff0c\u6700\u5927\u5c16\u7aef\u504f\u5dee\u7ea6\u4e3a\u603b\u957f\u5ea6\u7684 5%\u3002", "conclusion": "\u8be5\u6a21\u578b\u4e3a\u7cbe\u786e\u7684\u8171\u9a71\u52a8\u540c\u5fc3\u7ba1\u673a\u5668\u4eba\u7684\u5f62\u72b6\u4f30\u8ba1\u548c\u63a7\u5236\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2510.24452", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24452", "abs": "https://arxiv.org/abs/2510.24452", "authors": ["Xi Cheng", "Weijie Shen", "Haoming Chen", "Chaoyi Shen", "Jean Ortega", "Jiashang Liu", "Steve Thomas", "Honglin Zheng", "Haoyun Wu", "Yuxiang Li", "Casey Lichtendahl", "Jenny Ortiz", "Gang Liu", "Haiyang Qi", "Omid Fatemieh", "Chris Fry", "Jing Jing Long"], "title": "ARIMA_PLUS: Large-scale, Accurate, Automatic and Interpretable In-Database Time Series Forecasting and Anomaly Detection in Google BigQuery", "comment": null, "summary": "Time series forecasting and anomaly detection are common tasks for\npractitioners in industries such as retail, manufacturing, advertising and\nenergy. Two unique challenges stand out: (1) efficiently and accurately\nforecasting time series or detecting anomalies in large volumes automatically;\nand (2) ensuring interpretability of results to effectively incorporate\nbusiness insights. We present ARIMA_PLUS, a novel framework to overcome these\ntwo challenges by a unique combination of (a) accurate and interpretable time\nseries models and (b) scalable and fully managed system infrastructure. The\nmodel has a sequential and modular structure to handle different components of\nthe time series, including holiday effects, seasonality, trend, and anomalies,\nwhich enables high interpretability of the results. Novel enhancements are made\nto each module, and a unified framework is established to address both\nforecasting and anomaly detection tasks simultaneously. In terms of accuracy,\nits comprehensive benchmark on the 42 public datasets in the Monash forecasting\nrepository shows superior performance over not only well-established\nstatistical alternatives (such as ETS, ARIMA, TBATS, Prophet) but also newer\nneural network models (such as DeepAR, N-BEATS, PatchTST, TimeMixer). In terms\nof infrastructure, it is directly built into the query engine of BigQuery in\nGoogle Cloud. It uses a simple SQL interface and automates tedious\ntechnicalities such as data cleaning and model selection. It automatically\nscales with managed cloud computational and storage resources, making it\npossible to forecast 100 million time series using only 1.5 hours with a\nthroughput of more than 18000 time series per second. In terms of\ninterpretability, we present several case studies to demonstrate time series\ninsights it generates and customizability it offers.", "AI": {"tldr": "ARIMA_PLUS\u662f\u4e00\u4e2a\u7ed3\u5408\u4e86\u7cbe\u786e\u53ef\u89e3\u91ca\u7684\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\u548c\u53ef\u6269\u5c55\u7684\u5b8c\u5168\u6258\u7ba1\u7cfb\u7edf\u57fa\u7840\u8bbe\u65bd\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u5927\u89c4\u6a21\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u548c\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u6548\u7387\u3001\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u6311\u6218\u3002", "motivation": "\u5728\u96f6\u552e\u3001\u5236\u9020\u3001\u5e7f\u544a\u548c\u80fd\u6e90\u7b49\u884c\u4e1a\u4e2d\uff0c\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u548c\u5f02\u5e38\u68c0\u6d4b\u9762\u4e34\u7740\u5728\u6d77\u91cf\u6570\u636e\u4e2d\u9ad8\u6548\u51c6\u786e\u5730\u8fdb\u884c\u9884\u6d4b\u6216\u68c0\u6d4b\u7684\u6311\u6218\uff0c\u4ee5\u53ca\u4fdd\u8bc1\u7ed3\u679c\u53ef\u89e3\u91ca\u6027\u4ee5\u6709\u6548\u7ed3\u5408\u4e1a\u52a1\u6d1e\u5bdf\u7684\u9700\u6c42\u3002", "method": "ARIMA_PLUS\u6846\u67b6\u7ed3\u5408\u4e86\u7cbe\u786e\u53ef\u89e3\u91ca\u7684\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\u548c\u53ef\u6269\u5c55\u7684\u5b8c\u5168\u6258\u7ba1\u7cfb\u7edf\u57fa\u7840\u8bbe\u65bd\u3002\u8be5\u6a21\u578b\u91c7\u7528\u987a\u5e8f\u548c\u6a21\u5757\u5316\u7ed3\u6784\u6765\u5904\u7406\u65f6\u95f4\u5e8f\u5217\u7684\u4e0d\u540c\u7ec4\u6210\u90e8\u5206\uff08\u5982\u8282\u5047\u65e5\u6548\u5e94\u3001\u5b63\u8282\u6027\u3001\u8d8b\u52bf\u548c\u5f02\u5e38\uff09\uff0c\u4ece\u800c\u5b9e\u73b0\u9ad8\u5ea6\u53ef\u89e3\u91ca\u6027\u3002\u5b83\u5bf9\u6bcf\u4e2a\u6a21\u5757\u8fdb\u884c\u4e86\u6539\u8fdb\uff0c\u5e76\u5efa\u7acb\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\u6765\u540c\u65f6\u5904\u7406\u9884\u6d4b\u548c\u5f02\u5e38\u68c0\u6d4b\u4efb\u52a1\u3002", "result": "\u5728\u51c6\u786e\u6027\u65b9\u9762\uff0cARIMA_PLUS\u5728Monash\u9884\u6d4b\u5b58\u50a8\u5e93\u768442\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u7684\u7efc\u5408\u57fa\u51c6\u6d4b\u8bd5\u8868\u660e\uff0c\u5176\u6027\u80fd\u4f18\u4e8eETS\u3001ARIMA\u3001TBATS\u3001Prophet\u7b49\u6210\u719f\u7684\u7edf\u8ba1\u6a21\u578b\uff0c\u4ee5\u53caDeepAR\u3001N-BEATS\u3001PatchTST\u3001TimeMixer\u7b49\u8f83\u65b0\u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u3002\u5728\u57fa\u7840\u8bbe\u65bd\u65b9\u9762\uff0c\u5b83\u5185\u7f6e\u4e8eGoogle Cloud BigQuery\u7684\u67e5\u8be2\u5f15\u64ce\u4e2d\uff0c\u4f7f\u7528\u7b80\u5355\u7684SQL\u63a5\u53e3\uff0c\u81ea\u52a8\u5316\u4e86\u6570\u636e\u6e05\u7406\u548c\u6a21\u578b\u9009\u62e9\u7b49\u6280\u672f\u7ec6\u8282\uff0c\u5e76\u80fd\u81ea\u52a8\u6269\u5c55\uff0c\u80fd\u57281.5\u5c0f\u65f6\u5185\u5904\u74061\u4ebf\u4e2a\u65f6\u95f4\u5e8f\u5217\uff0c\u541e\u5410\u91cf\u8d85\u8fc718000\u4e2a\u65f6\u95f4\u5e8f\u5217/\u79d2\u3002\u5728\u53ef\u89e3\u91ca\u6027\u65b9\u9762\uff0c\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u4e86\u5176\u751f\u6210\u7684\u65f6\u95f4\u5e8f\u5217\u6d1e\u5bdf\u548c\u63d0\u4f9b\u7684\u53ef\u5b9a\u5236\u6027\u3002", "conclusion": "ARIMA_PLUS\u901a\u8fc7\u5176\u521b\u65b0\u7684\u6a21\u578b\u7ed3\u6784\u548c\u5f3a\u5927\u7684\u7cfb\u7edf\u57fa\u7840\u8bbe\u65bd\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u548c\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5e76\u5728\u51c6\u786e\u6027\u3001\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6210\u679c\u3002"}}
{"id": "2510.23983", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2510.23983", "abs": "https://arxiv.org/abs/2510.23983", "authors": ["Zhonghao Xia", "Zhilong Yang", "Yali Yang", "Kaile Ren", "Jiangang He"], "title": "Strong Intra- and Interchain Orbital Coupling Leads to Multiband and High Thermoelectric Performance in Na$_2$Au$X$ ($X$ = P, As, Sb, and Bi)", "comment": "11 pages, 7 figures", "summary": "The intrinsic coupling among electrical conductivity ($\\sigma$), Seebeck\ncoefficient ($S$), and lattice thermal conductivity ($\\kappa_{\\mathrm{L}}$)\nimposes a fundamental limit on the dimensionless figure of merit $ZT$ in\nthermoelectric (TE) materials. Increasing band degeneracy can effectively\nbalance $\\sigma$ and $S$, enabling a high power factor (PF, $S^{2}\\sigma$).\nHowever, compounds with intrinsically large band degeneracy are scarce. Here,\nwe present an unconventional strategy to realize elevated band degeneracy in\nzigzag-chain Na$_2$Au$X$ ($X$ = P, As, Sb, Bi) compounds by harnessing strong\nintra- and interchain orbital coupling. Pronounced hybridization between\nAu-$d_{z^{2}}$ and $X$-$p_{z}$ orbitals along the Au--$X$ zigzag chains,\ntogether with unexpectedly strong interchain $X$-$p_{x}/p_{y}$ coupling,\nproduces a highly dispersive, multivalley valence band structure that supports\nan exceptional PF. Concurrently, the intrinsically weak interchain interactions\narising from the quasi-one-dimensional framework, together with the weakened\nAu--$X$ and Au--Au bonds within the chains due to filling of $p$-$d^{*}$\nantibonding states, result in an ultralow $\\kappa_{\\mathrm{L}}$.\nFirst-principles calculations combined with Boltzmann transport theory predict\nthat $p$-type Na$_2$AuBi achieves a PF of\n$63.9\\,\\mu\\mathrm{W}\\,\\mathrm{cm}^{-1}\\,\\mathrm{K}^{-2}$, an ultralow\n$\\kappa_{\\mathrm{L}}$ of $0.49\\,\\mathrm{W}\\,\\mathrm{m}^{-1}\\,\\mathrm{K}^{-1}$,\nand a maximum $ZT$ of $4.7$ along the zigzag-chain direction at\n$800\\,\\mathrm{K}$. This work establishes a new design paradigm for\nhigh-efficiency TE materials by exploiting substantial orbital overlap in\nstructurally weakly bonded, quasi-one-dimensional systems, opening promising\navenues for the discovery and engineering of next-generation high-performance\nTE materials.", "AI": {"tldr": "\u901a\u8fc7\u5229\u7528\u94fe\u5185\u548c\u94fe\u95f4\u7684\u8f68\u9053\u8026\u5408\uff0c\u5728zigzag\u94feNa2AuX\uff08X=P, As, Sb, Bi\uff09\u5316\u5408\u7269\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u7684\u80fd\u5e26\u7b80\u5e76\u5ea6\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u529f\u7387\u56e0\u5b50\uff08PF\uff09\u3002\u540c\u65f6\uff0c\u5f31\u7684\u94fe\u95f4\u76f8\u4e92\u4f5c\u7528\u548c\u5f31\u7684Au-X\u3001Au-Au\u952e\u5bfc\u81f4\u4e86\u6781\u4f4e\u7684\u6676\u683c\u70ed\u5bfc\u7387\uff08\u03baL\uff09\u3002\u7406\u8bba\u8ba1\u7b97\u9884\u6d4bp\u578bNa2AuBi\u5728800K\u65f6ZT\u53ef\u8fbe4.7\u3002", "motivation": "\u70ed\u7535\u6750\u6599\u7684\u7535\u5bfc\u7387\uff08\u03c3\uff09\u3001\u585e\u8d1d\u514b\u7cfb\u6570\uff08S\uff09\u548c\u6676\u683c\u70ed\u5bfc\u7387\uff08\u03baL\uff09\u4e4b\u95f4\u7684\u5185\u5728\u8026\u5408\u9650\u5236\u4e86\u5176\u6027\u80fd\u3002\u63d0\u9ad8\u80fd\u5e26\u7b80\u5e76\u5ea6\u662f\u5e73\u8861\u03c3\u548cS\u3001\u63d0\u9ad8\u529f\u7387\u56e0\u5b50\uff08PF\uff09\u7684\u6709\u6548\u9014\u5f84\uff0c\u4f46\u5177\u6709\u9ad8\u672c\u5f81\u80fd\u5e26\u7b80\u5e76\u5ea6\u7684\u5316\u5408\u7269\u5f88\u5c11\u89c1\u3002", "method": "\u5229\u7528\u94fe\u5185\u548c\u94fe\u95f4\u7684\u8f68\u9053\u8026\u5408\uff08\u7279\u522b\u662fAu-d\u548cX-p\u8f68\u9053\u7684\u6742\u5316\uff09\u6765\u63d0\u9ad8Na2AuX\uff08X=P, As, Sb, Bi\uff09\u5316\u5408\u7269\u7684\u80fd\u5e26\u7b80\u5e76\u5ea6\uff0c\u5e76\u7ed3\u5408\u7b2c\u4e00\u6027\u539f\u7406\u8ba1\u7b97\u548c\u73bb\u5c14\u5179\u66fc\u8f93\u8fd0\u7406\u8bba\u9884\u6d4b\u5176\u70ed\u7535\u6027\u80fd\u3002", "result": "p\u578bNa2AuBi\u5b9e\u73b0\u4e8663.9 \u03bcW cm\u207b\u00b9 K\u207b\u00b2\u7684\u529f\u7387\u56e0\u5b50\uff0c0.49 W m\u207b\u00b9 K\u207b\u00b9\u7684\u6781\u4f4e\u6676\u683c\u70ed\u5bfc\u7387\uff0c\u4ee5\u53ca\u5728800K\u65f6\u6cbfzigzag\u94fe\u65b9\u5411\u9ad8\u8fbe4.7\u7684ZT\u3002", "conclusion": "\u901a\u8fc7\u5229\u7528\u51c6\u4e00\u7ef4\u4f53\u7cfb\u4e2d\u663e\u8457\u7684\u8f68\u9053\u91cd\u53e0\u548c\u5f31\u7684\u7ed3\u6784\u952e\u5408\uff0c\u53ef\u4ee5\u8bbe\u8ba1\u51fa\u9ad8\u6027\u80fd\u70ed\u7535\u6750\u6599\u3002\u8fd9\u79cd\u7b56\u7565\u4e3a\u53d1\u73b0\u548c\u5de5\u7a0b\u5316\u4e0b\u4e00\u4ee3\u9ad8\u6027\u80fd\u70ed\u7535\u6750\u6599\u63d0\u4f9b\u4e86\u65b0\u7684\u9014\u5f84\u3002"}}
{"id": "2510.24239", "categories": ["cond-mat.mes-hall", "cond-mat.dis-nn", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2510.24239", "abs": "https://arxiv.org/abs/2510.24239", "authors": ["Zhen-Hao Gong", "Zhi-Hao Wei", "Hai-Zhou Lu", "X. C. Xie"], "title": "Identifying geometric third-order nonlinear transport in disordered materials", "comment": "8 pages, 3 figures, 1 table", "summary": "In third-order nonlinear transport, a voltage can be measured in response to\nthe cube of a driving current as a result of the quantum geometric effects,\nwhich has attracted tremendous attention. However, in realistic materials where\ndisorder scattering also contributes to nonlinear transport, identifying the\ngeometric mechanisms remains a challenge. We find a total of 20 mechanisms of\nthird-order nonlinear transport by developing a comprehensive theory that\ntreats the geometric effects and disorder scattering on an equal footing. More\nimportantly, we find that 12 of these mechanisms can be unambiguously\nidentified, by deriving a scaling law that expresses the third-order nonlinear\nHall conductivity as a polynomial in the linear longitudinal conductivity. We\napply this theory to identify the geometric mechanisms of third-order nonlinear\ntransport in materials both with and without time-reversal symmetry, such as 2D\nmaterials, topological materials, and altermagnets. This theory further\npromotes nonlinear transport as a probe of geometric effects and phase\ntransitions in quantum materials.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u53d1\u5c55\u4e00\u79cd\u5c06\u51e0\u4f55\u6548\u5e94\u548c\u65e0\u5e8f\u6563\u5c04\u7f6e\u4e8e\u540c\u7b49\u5730\u4f4d\u7684\u7efc\u5408\u7406\u8bba\uff0c\u53d1\u73b0\u4e8620\u79cd\u4e09\u9636\u975e\u7ebf\u6027\u8f93\u8fd0\u673a\u5236\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u6807\u5ea6\u5f8b\uff0c\u53ef\u4ee5\u5c06\u5176\u4e2d12\u79cd\u673a\u5236\u660e\u786e\u533a\u5206\u51fa\u6765\uff0c\u4ece\u800c\u63a8\u52a8\u4e86\u975e\u7ebf\u6027\u8f93\u8fd0\u5728\u91cf\u5b50\u6750\u6599\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u5728\u4e09\u9636\u975e\u7ebf\u6027\u8f93\u8fd0\u4e2d\uff0c\u7531\u4e8e\u91cf\u5b50\u51e0\u4f55\u6548\u5e94\uff0c\u53ef\u4ee5\u6d4b\u91cf\u4e0e\u9a71\u52a8\u7535\u6d41\u7acb\u65b9\u6210\u6bd4\u4f8b\u7684\u7535\u538b\uff0c\u4f46\u8fd9\u5728\u8003\u8651\u4e86\u65e0\u5e8f\u6563\u5c04\u7684\u5b9e\u9645\u6750\u6599\u4e2d\u96be\u4ee5\u533a\u5206\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u7efc\u5408\u7406\u8bba\uff0c\u5c06\u51e0\u4f55\u6548\u5e94\u548c\u65e0\u5e8f\u6563\u5c04\u540c\u7b49\u5bf9\u5f85\uff0c\u4ee5\u8bc6\u522b\u4e09\u9636\u975e\u7ebf\u6027\u8f93\u8fd0\u673a\u5236\u3002\u63a8\u5bfc\u4e86\u4e00\u4e2a\u6807\u5ea6\u5f8b\uff0c\u5c06\u4e09\u9636\u975e\u7ebf\u6027\u970d\u5c14\u7535\u5bfc\u8868\u793a\u4e3a\u7ebf\u6027\u7eb5\u5411\u7535\u5bfc\u7684\u591a\u9879\u5f0f\uff0c\u4ee5\u533a\u5206\u4e0d\u540c\u7684\u673a\u5236\u3002", "result": "\u53d1\u73b0\u4e8620\u79cd\u4e09\u9636\u975e\u7ebf\u6027\u8f93\u8fd0\u673a\u5236\uff0c\u5176\u4e2d12\u79cd\u53ef\u4ee5\u901a\u8fc7\u63a8\u5bfc\u51fa\u7684\u6807\u5ea6\u5f8b\u660e\u786e\u8bc6\u522b\u3002\u8be5\u7406\u8bba\u5df2\u5e94\u7528\u4e8e\u8bc6\u522b\u6709\u65e0\u65f6\u95f4\u53cd\u8f6c\u5bf9\u79f0\u6027\u7684\u6750\u6599\u4e2d\u7684\u51e0\u4f55\u673a\u5236\uff0c\u5982\u4e8c\u7ef4\u6750\u6599\u3001\u62d3\u6251\u6750\u6599\u548c\u4ea4\u53d8\u78c1\u4f53\u3002", "conclusion": "\u672c\u7406\u8bba\u5c06\u975e\u7ebf\u6027\u8f93\u8fd0\u53d1\u5c55\u4e3a\u63a2\u6d4b\u91cf\u5b50\u6750\u6599\u4e2d\u51e0\u4f55\u6548\u5e94\u548c\u76f8\u53d8\u7684\u65b0\u9014\u5f84\u3002"}}
{"id": "2510.23731", "categories": ["quant-ph", "math-ph", "math.MP"], "pdf": "https://arxiv.org/pdf/2510.23731", "abs": "https://arxiv.org/abs/2510.23731", "authors": ["Himanshu Badhani", "Dhanuja G S", "Siddhartha Das"], "title": "Thermodynamic work capacity of quantum information processing", "comment": "1 table, 6 pages, see companion work arXiv:2510.12790", "summary": "We introduce the resource-theoretic free energy of a quantum channel as the\nmaximal work extractable from the channel as its output equilibrates to a\nthermal state and its reference system remains locally intact. It is\nproportional to the relative entropy between the given channel and the\nabsolutely thermal channel. It attains a clear operational meaning as twice the\nasymptotic rates of athermality distillation and formation under Gibbs\npreserving superchannels, which map one absolutely thermal channel to another\nfor a given bath, thereby revealing the asymptotic reversibility of the\nresource theory of athermality for quantum channels. Consequently, we establish\nthat the optimal extractable work in converting one channel to another through\nthe asymptotic athermality distillation and formation tasks equals the\ndifference in their free energies. We call this optimal work the thermodynamic\nwork capacity of channel conversion. Quantum information processing and\ncomputing fundamentally concern the manipulation and transformation of quantum\nchannels, which encompass quantum states, their transformations, and\nmeasurements. A quantitative characterization of the optimal thermodynamic work\ngain or expenditure in quantum information processing constitutes a key step\ntoward formulating thermodynamics of quantum processes.", "AI": {"tldr": "\u672c\u6587\u5c06\u91cf\u5b50\u4fe1\u9053\u7684\u8d44\u6e90\u7406\u8bba\u81ea\u7531\u80fd\u5b9a\u4e49\u4e3a\u4fe1\u9053\u8f93\u51fa\u8fbe\u5230\u70ed\u5e73\u8861\u72b6\u6001\u4e14\u53c2\u8003\u7cfb\u7edf\u4fdd\u6301\u5c40\u90e8\u5b8c\u6574\u65f6\u53ef\u63d0\u53d6\u7684\u6700\u5927\u529f\uff0c\u8be5\u81ea\u7531\u80fd\u6b63\u6bd4\u4e8e\u4fe1\u9053\u4e0e\u7edd\u5bf9\u70ed\u4fe1\u9053\u4e4b\u95f4\u7684\u76f8\u5bf9\u71b5\u3002", "motivation": "\u63d0\u51fa\u91cf\u5b50\u4fe1\u9053\u7684\u8d44\u6e90\u7406\u8bba\u81ea\u7531\u80fd\uff0c\u5e76\u9610\u8ff0\u5176\u5728\u91cf\u5b50\u4fe1\u606f\u5904\u7406\u548c\u8ba1\u7b97\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u65e8\u5728\u91cf\u5316\u91cf\u5b50\u8fc7\u7a0b\u4e2d\u7684\u6700\u4f18\u70ed\u529b\u5b66\u529f\u589e\u76ca\u6216\u652f\u51fa\uff0c\u4e3a\u91cf\u5b50\u8fc7\u7a0b\u70ed\u529b\u5b66\u5960\u5b9a\u57fa\u7840\u3002", "method": "\u5c06\u91cf\u5b50\u4fe1\u9053\u7684\u8d44\u6e90\u7406\u8bba\u81ea\u7531\u80fd\u5b9a\u4e49\u4e3a\u4fe1\u9053\u8f93\u51fa\u8fbe\u5230\u70ed\u5e73\u8861\u72b6\u6001\u4e14\u53c2\u8003\u7cfb\u7edf\u4fdd\u6301\u5c40\u90e8\u5b8c\u6574\u65f6\u53ef\u63d0\u53d6\u7684\u6700\u5927\u529f\uff0c\u5e76\u63a8\u5bfc\u51fa\u5176\u4e0e\u4fe1\u9053\u548c\u7edd\u5bf9\u70ed\u4fe1\u9053\u4e4b\u95f4\u76f8\u5bf9\u71b5\u7684\u5173\u7cfb\uff0c\u540c\u65f6\u63ed\u793a\u5176\u4f5c\u4e3a\u70ed\u5ea6\u84b8\u998f\u548c\u751f\u6210\u6e10\u8fd1\u901f\u7387\u4e24\u500d\u7684\u660e\u786e\u64cd\u4f5c\u610f\u4e49\u3002", "result": "\u8bc1\u660e\u4e86\u6700\u4f18\u53ef\u63d0\u53d6\u529f\u7b49\u4e8e\u81ea\u7531\u80fd\u4e4b\u5dee\uff0c\u5e76\u5c06\u6b64\u5b9a\u4e49\u4e3a\u4fe1\u9053\u8f6c\u6362\u7684\u70ed\u529b\u5b66\u529f\u5bb9\u91cf\u3002", "conclusion": "\u6700\u4f18\u53ef\u63d0\u53d6\u529f\u7b49\u4e8e\u81ea\u7531\u80fd\u4e4b\u5dee\uff0c\u5373\u4fe1\u9053\u8f6c\u6362\u7684\u70ed\u529b\u5b66\u529f\u5bb9\u91cf\uff0c\u4e3a\u91cf\u5b50\u8fc7\u7a0b\u70ed\u529b\u5b66\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u5173\u952e\u4e00\u6b65\u3002"}}
{"id": "2510.24206", "categories": ["cs.LO", "F.3.2"], "pdf": "https://arxiv.org/pdf/2510.24206", "abs": "https://arxiv.org/abs/2510.24206", "authors": ["Rob van Glabbeek"], "title": "Unique Solutions of Guarded Recursive Equations", "comment": "In Proceedings EXPRESS/SOS 2025, arXiv:2510.23211", "summary": "This paper shows that guarded systems of recursive equations have unique\nsolutions up to strong bisimilarity for any process algebra with a structural\noperation semantics in the ready simulation format. A similar result holds for\nsimulation equivalence, for ready simulation equivalence and for the (ready)\nsimulation preorder. As a consequence, these equivalences and preorders are\nfull (pre)congruences for guarded recursion. Moreover, the unique-solutions\nresult yields a sound and ground-complete axiomatisation of strong bisimilarity\nfor any finitary GSOS language.", "AI": {"tldr": "\u5177\u6709\u7ed3\u6784\u64cd\u4f5c\u8bed\u4e49\u7684\u53d7\u4fdd\u62a4\u9012\u5f52\u65b9\u7a0b\u7ec4\u5728\u5c31\u7eea\u6a21\u62df\u683c\u5f0f\u4e0b\u5177\u6709\u552f\u4e00\u7684\u5f3a\u53cc\u6a21\u62df\u89e3\uff0c\u5e76\u4e3a\u4efb\u4f55\u6709\u9650 GSOS \u8bed\u8a00\u63d0\u4f9b\u4e86\u5f3a\u53cc\u6a21\u62df\u7684\u516c\u7406\u5316\u3002", "motivation": "\u7814\u7a76\u53d7\u4fdd\u62a4\u9012\u5f52\u65b9\u7a0b\u7ec4\u7684\u552f\u4e00\u89e3\u95ee\u9898\uff0c\u7279\u522b\u662f\u4e0e\u5f3a\u53cc\u6a21\u62df\u3001\u6a21\u62df\u7b49\u4ef7\u3001\u5c31\u7eea\u6a21\u62df\u7b49\u4ef7\u53ca\uff08\u5c31\u7eea\uff09\u6a21\u62df\u524d\u5e8f\u7684\u5173\u7cfb\u3002", "method": "\u5229\u7528\u5177\u6709\u7ed3\u6784\u64cd\u4f5c\u8bed\u4e49\u7684\u8fc7\u7a0b\u4ee3\u6570\u548c\u5c31\u7eea\u6a21\u62df\u683c\u5f0f\uff0c\u8bc1\u660e\u53d7\u4fdd\u62a4\u9012\u5f52\u65b9\u7a0b\u7ec4\u7684\u552f\u4e00\u89e3\u6027\u8d28\u3002", "result": "\u8bc1\u660e\u4e86\u53d7\u4fdd\u62a4\u9012\u5f52\u65b9\u7a0b\u7ec4\u5728\u5f3a\u53cc\u6a21\u62df\u3001\u6a21\u62df\u7b49\u4ef7\u3001\u5c31\u7eea\u6a21\u62df\u7b49\u4ef7\u53ca\uff08\u5c31\u7eea\uff09\u6a21\u62df\u524d\u5e8f\u4e0b\u5177\u6709\u552f\u4e00\u89e3\u3002\u8fd9\u4e9b\u7b49\u4ef7\u5173\u7cfb\u548c\u524d\u5e8f\u662f\u53d7\u4fdd\u62a4\u9012\u5f52\u7684\u5168\uff08\u524d\uff09\u540c\u8d28\u3002\u6b64\u5916\uff0c\u552f\u4e00\u89e3\u7684\u7ed3\u679c\u4e3a\u6709\u9650 GSOS \u8bed\u8a00\u7684\u5f3a\u53cc\u6a21\u62df\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5065\u5168\u4e14\u5730\u9762\u5b8c\u5907\u7684\u516c\u7406\u5316\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u53d7\u4fdd\u62a4\u9012\u5f52\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u4e2a\u575a\u5b9e\u7684\u7406\u8bba\u57fa\u7840\uff0c\u5e76\u4e3a\u5f3a\u53cc\u6a21\u62df\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u516c\u7406\u5316\u65b9\u6cd5\u3002"}}
{"id": "2510.23873", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.23873", "abs": "https://arxiv.org/abs/2510.23873", "authors": ["Zhentong Shao", "Jingtao Qin", "Xianbang Chen", "Nanpeng Yu"], "title": "A Spatio-Temporal Graph Learning Approach to Real-Time Economic Dispatch with Multi-Transmission-Node DER Aggregation", "comment": null, "summary": "The integration of distributed energy resources (DERs) into wholesale\nelectricity markets, as mandated by FERC Order 2222, imposes new challenges on\nsystem operations. To remain consistent with existing market structures,\nregional transmission organizations (RTOs) have advanced the aggregation of\ntransmission-node-level DERs (T-DERs), where a nodal virtual power plant (VPP)\nrepresents the mapping of all distribution-level DERs to their respective\ntransmission nodes. This paper develops a real-time economic dispatch (RTED)\nframework that enables multi-transmission-node DER aggregation while addressing\ncomputational efficiency. To this end, we introduce a spatio-temporal graph\nconvolutional network (ST-GCN) for adaptive prediction of distribution factors\n(DFs), thereby capturing the dynamic influence of individual T-DERs across the\ntransmission system. Furthermore, an iterative constraint identification\nstrategy is incorporated to alleviate transmission security constraints without\ncompromising system reliability. Together, these innovations accelerate the\nmarket clearing process and support the effective participation of T-DER\naggregators under current market paradigms. The proposed approach is validated\non large-scale test systems, including modified 118-, 2383-, and 3012-bus\nnetworks under a rolling RTED setting with real demand data. Numerical results\ndemonstrate significant improvements in reducing operational costs and\nmaintaining transmission network feasibility, underscoring the scalability and\npracticality of the proposed framework.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9e\u65f6\u7ecf\u6d4e\u8c03\u5ea6\uff08RTED\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u805a\u5408\u4f20\u8f93\u8282\u70b9\u7ea7\u7684\u5206\u5e03\u5f0f\u80fd\u6e90\uff08T-DER\uff09\uff0c\u4ee5\u5e94\u5bf9 FERC Order 2222 \u5e26\u6765\u7684\u6311\u6218\u3002", "motivation": "FERC Order 2222 \u8981\u6c42\u5c06\u5206\u5e03\u5f0f\u80fd\u6e90\uff08DERs\uff09\u6574\u5408\u5230\u6279\u53d1\u7535\u529b\u5e02\u573a\uff0c\u8fd9\u7ed9\u7cfb\u7edf\u8fd0\u8425\u5e26\u6765\u4e86\u65b0\u7684\u6311\u6218\u3002\u4e3a\u4e86\u4e0e\u73b0\u6709\u7684\u5e02\u573a\u7ed3\u6784\u4fdd\u6301\u4e00\u81f4\uff0c\u533a\u57df\u8f93\u7535\u7ec4\u7ec7\uff08RTOs\uff09\u63a8\u52a8\u4e86\u4f20\u8f93\u8282\u70b9\u7ea7 DERs\uff08T-DERs\uff09\u7684\u805a\u5408\uff0c\u5176\u4e2d\u8282\u70b9\u865a\u62df\u7535\u5382\uff08VPP\uff09\u4ee3\u8868\u4e86\u6240\u6709\u5206\u5e03\u7ea7 DERs \u6620\u5c04\u5230\u5176\u76f8\u5e94\u7684\u4f20\u8f93\u8282\u70b9\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9e\u65f6\u7ecf\u6d4e\u8c03\u5ea6\uff08RTED\uff09\u6846\u67b6\uff0c\u80fd\u591f\u805a\u5408\u591a\u4e2a\u4f20\u8f93\u8282\u70b9\u7684 DERs\uff0c\u5e76\u89e3\u51b3\u4e86\u8ba1\u7b97\u6548\u7387\u95ee\u9898\u3002\u5f15\u5165\u4e86\u65f6\u7a7a\u56fe\u5377\u79ef\u7f51\u7edc\uff08ST-GCN\uff09\u6765\u9002\u5e94\u6027\u5730\u9884\u6d4b\u5206\u5e03\u56e0\u5b50\uff08DFs\uff09\uff0c\u4ece\u800c\u6355\u6349\u5355\u4e2a T-DERs \u5728\u4f20\u8f93\u7cfb\u7edf\u4e2d\u7684\u52a8\u6001\u5f71\u54cd\u3002\u6b64\u5916\uff0c\u8fd8\u91c7\u7528\u4e86\u4e00\u79cd\u8fed\u4ee3\u7ea6\u675f\u8bc6\u522b\u7b56\u7565\u6765\u7f13\u89e3\u4f20\u8f93\u5b89\u5168\u7ea6\u675f\uff0c\u540c\u65f6\u4e0d\u635f\u5bb3\u7cfb\u7edf\u53ef\u9760\u6027\u3002", "result": "\u901a\u8fc7\u5728\u4fee\u6539\u540e\u7684 118\u30012383 \u548c 3012 \u6bcd\u7ebf\u7f51\u7edc\u4e0a\u8fdb\u884c\u9a8c\u8bc1\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u6846\u67b6\u5728\u964d\u4f4e\u8fd0\u8425\u6210\u672c\u548c\u7ef4\u6301\u4f20\u8f93\u7f51\u7edc\u53ef\u884c\u6027\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\uff0c\u5e76\u5f3a\u8c03\u4e86\u8be5\u6846\u67b6\u7684\u53ef\u6269\u5c55\u6027\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7 ST-GCN \u548c\u8fed\u4ee3\u7ea6\u675f\u8bc6\u522b\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86 T-DERs \u805a\u5408\u5e26\u6765\u7684\u6311\u6218\uff0c\u52a0\u901f\u4e86\u5e02\u573a\u6e05\u7b97\u8fc7\u7a0b\uff0c\u5e76\u652f\u6301\u4e86 T-DERs \u805a\u5408\u5668\u5728\u5f53\u524d\u5e02\u573a\u6a21\u5f0f\u4e0b\u7684\u6709\u6548\u53c2\u4e0e\u3002"}}
{"id": "2510.23892", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.23892", "abs": "https://arxiv.org/abs/2510.23892", "authors": ["Kebin Contreras", "Emmanuel Martinez", "Brayan Monroy", "Sebastian Ardila", "Cristian Ramirez", "Mariana Caicedo", "Hans Garcia", "Tatiana Gelvez-Barrera", "Juan Poveda-Jaramillo", "Henry Arguello", "Jorge Bacca"], "title": "Learning-based Spectral Regression for Cocoa Bean Physicochemical Property Prediction", "comment": null, "summary": "Cocoa bean quality assessment is essential for ensuring compliance with\ncommercial standards, protecting consumer health, and increasing the market\nvalue of the cocoa product. The quality assessment estimates key\nphysicochemical properties, such as fermentation level, moisture content,\npolyphenol concentration, and cadmium content, among others. This assessment\nhas traditionally relied on the accurate estimation of these properties via\nvisual or sensory evaluation, jointly with laboratory-based physicochemical\nanalyses, which are often time-consuming, destructive, and difficult to scale.\nThis creates the need for rapid, reliable, and noninvasive alternatives.\nSpectroscopy, particularly in the visible and near-infrared ranges, offers a\nnon-invasive alternative by capturing the molecular signatures associated with\nthese properties. Therefore, this work introduces a scalable methodology for\nevaluating the quality of cocoa beans by predicting key physicochemical\nproperties from the spectral signatures of cocoa beans. This approach utilizes\na conveyor belt system integrated with a VIS-NIR spectrometer, coupled with\nlearning-based regression models. Furthermore, a dataset is built using cocoa\nbean batches from Santander, Colombia. Ground-truth reference values were\nobtained through standardized laboratory analyses and following commercial\ncocoa quality regulations. To further evaluate the proposed methodology's\ngeneralization, performance is tested on samples collected from other Colombian\nregions and from Cusco, Peru. Experimental results show that the proposed\nmodels achieved R2 scores exceeding 0.98 across all physicochemical properties,\nand reached 0.96 accuracy on geographically independent samples. This\nnon-destructive approach represents a suitable and scalable alternative to\nconventional laboratory methods for quality assessment across the cocoa\nproduction chain.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u53ef\u89c1\u5149-\u8fd1\u7ea2\u5916\u5149\u8c31\u548c\u673a\u5668\u5b66\u4e60\u7684\u975e\u4fb5\u5165\u5f0f\u53ef\u6269\u5c55\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u53ef\u53ef\u8c46\u7684\u8d28\u91cf\uff0c\u80fd\u591f\u5feb\u901f\u9884\u6d4b\u5173\u952e\u7684\u7406\u5316\u6027\u8d28\uff0c\u4e14\u51c6\u786e\u7387\u9ad8\uff0c\u53ef\u4f5c\u4e3a\u4f20\u7edf\u5b9e\u9a8c\u5ba4\u65b9\u6cd5\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "motivation": "\u4f20\u7edf\u53ef\u53ef\u8c46\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\u8017\u65f6\u3001\u6709\u635f\u4e14\u96be\u4ee5\u89c4\u6a21\u5316\uff0c\u9700\u8981\u5feb\u901f\u3001\u53ef\u9760\u3001\u975e\u4fb5\u5165\u6027\u7684\u66ff\u4ee3\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u96c6\u6210\u53ef\u89c1\u5149-\u8fd1\u7ea2\u5916\u5149\u8c31\u4eea\u7684\u4f20\u9001\u5e26\u7cfb\u7edf\uff0c\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u56de\u5f52\u6a21\u578b\uff0c\u4ece\u53ef\u53ef\u8c46\u7684\u5149\u8c31\u7279\u5f81\u9884\u6d4b\u5176\u7406\u5316\u6027\u8d28\u3002", "result": "\u6240\u63d0\u51fa\u7684\u6a21\u578b\u5728\u6240\u6709\u7406\u5316\u6027\u8d28\u4e0a\u5747\u5b9e\u73b0\u4e86\u8d85\u8fc70.98\u7684R2\u5206\u6570\uff0c\u5728\u5730\u7406\u4e0a\u72ec\u7acb\u7684\u6837\u672c\u4e0a\u8fbe\u5230\u4e860.96\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u8be5\u975e\u7834\u574f\u6027\u65b9\u6cd5\u4e3a\u53ef\u53ef\u751f\u4ea7\u94fe\u4e2d\u8d28\u91cf\u8bc4\u4f30\u63d0\u4f9b\u4e86\u4e00\u79cd\u5408\u9002\u7684\u3001\u53ef\u6269\u5c55\u7684\u66ff\u4ee3\u4f20\u7edf\u5b9e\u9a8c\u5ba4\u65b9\u6cd5\u7684\u6280\u672f\u3002"}}
{"id": "2510.23626", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23626", "abs": "https://arxiv.org/abs/2510.23626", "authors": ["Shuang Geng", "Wenli Zhang", "Jiaheng Xie", "Rui Wang", "Sudha Ram"], "title": "From Detection to Discovery: A Closed-Loop Approach for Simultaneous and Continuous Medical Knowledge Expansion and Depression Detection on Social Media", "comment": "Presented at SWAIB2025 and HICSS2026", "summary": "Social media user-generated content (UGC) provides real-time, self-reported\nindicators of mental health conditions such as depression, offering a valuable\nsource for predictive analytics. While prior studies integrate medical\nknowledge to improve prediction accuracy, they overlook the opportunity to\nsimultaneously expand such knowledge through predictive processes. We develop a\nClosed-Loop Large Language Model (LLM)-Knowledge Graph framework that\nintegrates prediction and knowledge expansion in an iterative learning cycle.\nIn the knowledge-aware depression detection phase, the LLM jointly performs\ndepression detection and entity extraction, while the knowledge graph\nrepresents and weights these entities to refine prediction performance. In the\nknowledge refinement and expansion phase, new entities, relationships, and\nentity types extracted by the LLM are incorporated into the knowledge graph\nunder expert supervision, enabling continual knowledge evolution. Using\nlarge-scale UGC, the framework enhances both predictive accuracy and medical\nunderstanding. Expert evaluations confirmed the discovery of clinically\nmeaningful symptoms, comorbidities, and social triggers complementary to\nexisting literature. We conceptualize and operationalize\nprediction-through-learning and learning-through-prediction as mutually\nreinforcing processes, advancing both methodological and theoretical\nunderstanding in predictive analytics. The framework demonstrates the\nco-evolution of computational models and domain knowledge, offering a\nfoundation for adaptive, data-driven knowledge systems applicable to other\ndynamic risk monitoring contexts.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u95ed\u73af\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09-\u77e5\u8bc6\u56fe\u8c31\u6846\u67b6\uff0c\u901a\u8fc7\u8fed\u4ee3\u5b66\u4e60\u5468\u671f\u6574\u5408\u9884\u6d4b\u548c\u77e5\u8bc6\u6269\u5c55\uff0c\u4ee5\u63d0\u9ad8\u5fc3\u7406\u5065\u5eb7\uff08\u7279\u522b\u662f\u6291\u90c1\u75c7\uff09\u7684\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u5e76\u540c\u65f6\u6269\u5c55\u533b\u5b66\u77e5\u8bc6\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5728\u5229\u7528\u793e\u4ea4\u5a92\u4f53\u7528\u6237\u751f\u6210\u5185\u5bb9\uff08UGC\uff09\u9884\u6d4b\u5fc3\u7406\u5065\u5eb7\u72b6\u51b5\u65f6\uff0c\u867d\u7136\u6574\u5408\u4e86\u533b\u5b66\u77e5\u8bc6\uff0c\u4f46\u5ffd\u7565\u4e86\u901a\u8fc7\u9884\u6d4b\u8fc7\u7a0b\u540c\u65f6\u6269\u5c55\u8fd9\u4e9b\u77e5\u8bc6\u7684\u673a\u4f1a\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u95ed\u73afLLM-\u77e5\u8bc6\u56fe\u8c31\u6846\u67b6\uff0c\u5728\u77e5\u8bc6\u611f\u77e5\u6291\u90c1\u68c0\u6d4b\u9636\u6bb5\uff0cLLM\u540c\u65f6\u8fdb\u884c\u6291\u90c1\u68c0\u6d4b\u548c\u5b9e\u4f53\u63d0\u53d6\uff0c\u77e5\u8bc6\u56fe\u8c31\u8868\u793a\u548c\u52a0\u6743\u8fd9\u4e9b\u5b9e\u4f53\u4ee5\u63d0\u9ad8\u9884\u6d4b\u6027\u80fd\u3002\u5728\u77e5\u8bc6\u7cbe\u70bc\u548c\u6269\u5c55\u9636\u6bb5\uff0cLLM\u63d0\u53d6\u7684\u65b0\u5b9e\u4f53\u3001\u5173\u7cfb\u548c\u5b9e\u4f53\u7c7b\u578b\u5728\u4e13\u5bb6\u76d1\u7763\u4e0b\u88ab\u6574\u5408\u5230\u77e5\u8bc6\u56fe\u8c31\u4e2d\uff0c\u5b9e\u73b0\u77e5\u8bc6\u7684\u6301\u7eed\u6f14\u5316\u3002", "result": "\u4f7f\u7528\u5927\u89c4\u6a21UGC\uff0c\u8be5\u6846\u67b6\u63d0\u9ad8\u4e86\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u5e76\u52a0\u6df1\u4e86\u533b\u5b66\u7406\u89e3\u3002\u4e13\u5bb6\u8bc4\u4f30\u8bc1\u5b9e\u53d1\u73b0\u4e86\u4e0e\u73b0\u6709\u6587\u732e\u4e92\u8865\u7684\u3001\u5177\u6709\u4e34\u5e8a\u610f\u4e49\u7684\u75c7\u72b6\u3001\u5171\u75c5\u548c\u793e\u4f1a\u89e6\u53d1\u56e0\u7d20\u3002", "conclusion": "\u8be5\u7814\u7a76\u6982\u5ff5\u5316\u5e76\u64cd\u4f5c\u5316\u4e86\u201c\u901a\u8fc7\u5b66\u4e60\u8fdb\u884c\u9884\u6d4b\u201d\u548c\u201c\u901a\u8fc7\u9884\u6d4b\u8fdb\u884c\u5b66\u4e60\u201d\u8fd9\u4e24\u4e2a\u76f8\u4e92\u5f3a\u5316\u7684\u8fc7\u7a0b\uff0c\u5728\u9884\u6d4b\u5206\u6790\u9886\u57df\u63a8\u52a8\u4e86\u65b9\u6cd5\u8bba\u548c\u7406\u8bba\u7406\u89e3\u3002\u8be5\u6846\u67b6\u5c55\u793a\u4e86\u8ba1\u7b97\u6a21\u578b\u548c\u9886\u57df\u77e5\u8bc6\u7684\u534f\u540c\u6f14\u5316\uff0c\u4e3a\u9002\u7528\u4e8e\u5176\u4ed6\u52a8\u6001\u98ce\u9669\u76d1\u6d4b\u73af\u5883\u7684\u81ea\u9002\u5e94\u3001\u6570\u636e\u9a71\u52a8\u7684\u77e5\u8bc6\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2510.23772", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23772", "abs": "https://arxiv.org/abs/2510.23772", "authors": ["Vivek Veeriah", "Federico Barbero", "Marcus Chiam", "Xidong Feng", "Michael Dennis", "Ryan Pachauri", "Thomas Tumiel", "Johan Obando-Ceron", "Jiaxin Shi", "Shaobo Hou", "Satinder Singh", "Nenad Toma\u0161ev", "Tom Zahavy"], "title": "Evaluating In Silico Creativity: An Expert Review of AI Chess Compositions", "comment": "Accepted at the Creative AI Track, NeurIPS 2025", "summary": "The rapid advancement of Generative AI has raised significant questions\nregarding its ability to produce creative and novel outputs. Our recent work\ninvestigates this question within the domain of chess puzzles and presents an\nAI system designed to generate puzzles characterized by aesthetic appeal,\nnovelty, counter-intuitive and unique solutions. We briefly discuss our method\nbelow and refer the reader to the technical paper for more details. To assess\nour system's creativity, we presented a curated booklet of AI-generated puzzles\nto three world-renowned experts: International Master for chess compositions\nAmatzia Avni, Grandmaster Jonathan Levitt, and Grandmaster Matthew Sadler. All\nthree are noted authors on chess aesthetics and the evolving role of computers\nin the game. They were asked to select their favorites and explain what made\nthem appealing, considering qualities such as their creativity, level of\nchallenge, or aesthetic design.", "AI": {"tldr": "\u751f\u6210\u5f0fAI\u5728\u56fd\u9645\u8c61\u68cb\u8c1c\u9898\u751f\u6210\u65b9\u9762\u5177\u6709\u521b\u9020\u529b\uff0c\u4e13\u5bb6\u8bc4\u5ba1\u9a8c\u8bc1\u4e86\u5176\u65b0\u9896\u6027\u548c\u7f8e\u5b66\u5438\u5f15\u529b\u3002", "motivation": "\u8bc4\u4f30\u751f\u6210\u5f0fAI\u5728\u56fd\u9645\u8c61\u68cb\u8c1c\u9898\u751f\u6210\u65b9\u9762\u7684\u521b\u9020\u529b\uff0c\u7279\u522b\u662f\u751f\u6210\u5177\u6709\u7f8e\u5b66\u5438\u5f15\u529b\u3001\u65b0\u9896\u6027\u3001\u53cd\u76f4\u89c9\u548c\u72ec\u7279\u89e3\u51b3\u65b9\u6848\u7684\u8c1c\u9898\u3002", "method": "\u5f00\u53d1\u4e00\u4e2aAI\u7cfb\u7edf\u6765\u751f\u6210\u56fd\u9645\u8c61\u68cb\u8c1c\u9898\uff0c\u5e76\u9080\u8bf7\u56fd\u9645\u5927\u5e08\u548c\u7279\u7ea7\u5927\u5e08\u5bf9\u751f\u6210\u7684\u8c1c\u9898\u8fdb\u884c\u8bc4\u5ba1\uff0c\u8ba9\u4ed6\u4eec\u6311\u9009\u559c\u6b22\u7684\u8c1c\u9898\u5e76\u89e3\u91ca\u5176\u5438\u5f15\u529b\u3002", "result": "AI\u751f\u6210\u7684\u56fd\u9645\u8c61\u68cb\u8c1c\u9898\u5f97\u5230\u4e86\u4e09\u4f4d\u56fd\u9645\u8c61\u68cb\u4e13\u5bb6\u7684\u9ad8\u5ea6\u8bc4\u4ef7\uff0c\u4ed6\u4eec\u8ba4\u53ef\u4e86\u5176\u521b\u9020\u529b\u3001\u6311\u6218\u6027\u548c\u7f8e\u5b66\u8bbe\u8ba1\u3002", "conclusion": "\u751f\u6210\u5f0fAI\u80fd\u591f\u751f\u6210\u5177\u6709\u521b\u9020\u6027\u3001\u65b0\u9896\u6027\u548c\u7f8e\u5b66\u5438\u5f15\u529b\u7684\u56fd\u9645\u8c61\u68cb\u8c1c\u9898\uff0c\u5176\u8bc4\u4f30\u7ed3\u679c\u5f97\u5230\u4e86\u56fd\u9645\u8c61\u68cb\u4e13\u5bb6\u7684\u8ba4\u53ef\u3002"}}
{"id": "2510.24077", "categories": ["cs.SI", "stat.AP", "62"], "pdf": "https://arxiv.org/pdf/2510.24077", "abs": "https://arxiv.org/abs/2510.24077", "authors": ["Sayantan Mukherjee", "Pritam Ranjan", "Joysankar Bhattacharya"], "title": "Assessing the influence of social media feedback on traveler's future trip-planning behavior: A multi-model machine learning approach", "comment": "38 pages, 10 tables, 6 figures", "summary": "With the surge of domestic tourism in India and the influence of social media\non young tourists, this paper aims to address the research question on how\n\"social return\" - responses received on social media sharing - of recent trip\ndetails can influence decision-making for short-term future travels. The paper\ndevelops a multi-model framework to build a predictive machine learning model\nthat establishes a relationship between a traveler's social return, various\nsocial media usage, trip-related factors, and her future trip-planning\nbehavior. The primary data was collected via a survey from Indian tourists.\nAfter data cleaning, the imbalance in the data was addressed using a robust\noversampling method, and the reliability of the predictive model was ensured by\napplying a Monte Carlo cross-validation technique. The results suggest at least\n75% overall accuracy in predicting the influence of social return on changing\nthe future trip plan. Moreover, the model fit results provide crucial practical\nimplications for the domestic tourism sector in India with future research\ndirections concerning social media, destination marketing, smart tourism,\nheritage tourism, etc.", "AI": {"tldr": "\u793e\u4ea4\u5a92\u4f53\u4e0a\u7684\u70b9\u8d5e\u548c\u8bc4\u8bba\u7b49\u201c\u793e\u4ea4\u56de\u62a5\u201d\u4f1a\u5f71\u54cd\u5370\u5ea6\u5e74\u8f7b\u6e38\u5ba2\u7684\u672a\u6765\u77ed\u671f\u65c5\u884c\u51b3\u7b56\u3002", "motivation": "\u5206\u6790\u56fd\u5185\u65c5\u6e38\u6fc0\u589e\u548c\u793e\u4ea4\u5a92\u4f53\u5bf9\u5e74\u8f7b\u6e38\u5ba2\u7684\u5f71\u54cd\uff0c\u63a2\u8ba8\u793e\u4ea4\u5a92\u4f53\u5206\u4eab\u7684\u201c\u793e\u4ea4\u56de\u62a5\u201d\u5982\u4f55\u5f71\u54cd\u77ed\u671f\u672a\u6765\u65c5\u884c\u7684\u51b3\u7b56\u3002", "method": "\u5f00\u53d1\u591a\u6a21\u578b\u6846\u67b6\uff0c\u5efa\u7acb\u65c5\u884c\u8005\u7684\u793e\u4ea4\u56de\u62a5\u3001\u793e\u4ea4\u5a92\u4f53\u4f7f\u7528\u3001\u65c5\u884c\u76f8\u5173\u56e0\u7d20\u4e0e\u672a\u6765\u65c5\u884c\u8ba1\u5212\u884c\u4e3a\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5e76\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u9884\u6d4b\u3002\u901a\u8fc7\u5bf9\u5370\u5ea6\u6e38\u5ba2\u8fdb\u884c\u8c03\u67e5\u6536\u96c6\u539f\u59cb\u6570\u636e\uff0c\u5e76\u8fdb\u884c\u6570\u636e\u6e05\u6d17\u3001\u89e3\u51b3\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\uff08\u91c7\u7528\u9c81\u68d2\u8fc7\u91c7\u6837\u65b9\u6cd5\uff09\u4ee5\u53ca\u5e94\u7528\u8499\u7279\u5361\u6d1b\u4ea4\u53c9\u9a8c\u8bc1\u6280\u672f\u786e\u4fdd\u9884\u6d4b\u6a21\u578b\u7684\u53ef\u9760\u6027\u3002", "result": "\u9884\u6d4b\u6a21\u578b\u663e\u793a\uff0c\u5728\u6539\u53d8\u672a\u6765\u65c5\u884c\u8ba1\u5212\u65b9\u9762\uff0c\u201c\u793e\u4ea4\u56de\u62a5\u201d\u7684\u5f71\u54cd\u5177\u6709\u81f3\u5c1175%\u7684\u6574\u4f53\u51c6\u786e\u6027\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u5370\u5ea6\u7684\u56fd\u5185\u65c5\u6e38\u4e1a\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u5b9e\u9645\u6307\u5bfc\u610f\u4e49\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u5728\u793e\u4ea4\u5a92\u4f53\u3001\u76ee\u7684\u5730\u8425\u9500\u3001\u667a\u6167\u65c5\u6e38\u3001\u9057\u4ea7\u65c5\u6e38\u7b49\u65b9\u9762\u53ef\u4ee5\u8fdb\u4e00\u6b65\u7814\u7a76\u7684\u65b9\u5411\u3002"}}
{"id": "2510.24442", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.24442", "abs": "https://arxiv.org/abs/2510.24442", "authors": ["Yiding Wang", "Yuxuan Chen", "Fanxu Meng", "Xifan Chen", "Xiaolei Yang", "Muhan Zhang"], "title": "Law in Silico: Simulating Legal Society with LLM-Based Agents", "comment": null, "summary": "Since real-world legal experiments are often costly or infeasible, simulating\nlegal societies with Artificial Intelligence (AI) systems provides an effective\nalternative for verifying and developing legal theory, as well as supporting\nlegal administration. Large Language Models (LLMs), with their world knowledge\nand role-playing capabilities, are strong candidates to serve as the foundation\nfor legal society simulation. However, the application of LLMs to simulate\nlegal systems remains underexplored. In this work, we introduce Law in Silico,\nan LLM-based agent framework for simulating legal scenarios with individual\ndecision-making and institutional mechanisms of legislation, adjudication, and\nenforcement. Our experiments, which compare simulated crime rates with\nreal-world data, demonstrate that LLM-based agents can largely reproduce\nmacro-level crime trends and provide insights that align with real-world\nobservations. At the same time, micro-level simulations reveal that a\nwell-functioning, transparent, and adaptive legal system offers better\nprotection of the rights of vulnerable individuals.", "AI": {"tldr": "LLM\u9a71\u52a8\u7684\u6cd5\u5f8b\u6a21\u62df\u6846\u67b6\uff0c\u80fd\u590d\u73b0\u5b8f\u89c2\u72af\u7f6a\u8d8b\u52bf\uff0c\u5e76\u63ed\u793a\u5fae\u89c2\u6cd5\u5f8b\u7cfb\u7edf\u5bf9\u5f31\u52bf\u7fa4\u4f53\u7684\u4fdd\u62a4\u4f5c\u7528\u3002", "motivation": "LLM\u5728\u6cd5\u5f8b\u6a21\u62df\u65b9\u9762\u7684\u5e94\u7528\u5c1a\u5f85\u63a2\u7d22\uff0c\u800cLLM\u56e0\u5176\u4e16\u754c\u77e5\u8bc6\u548c\u89d2\u8272\u626e\u6f14\u80fd\u529b\uff0c\u662f\u6cd5\u5f8b\u793e\u4f1a\u6a21\u62df\u7684\u6709\u529b\u5019\u9009\u8005\u3002", "method": "\u63d0\u51faLaw in Silico\u6846\u67b6\uff0c\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u4ee3\u7406\u6846\u67b6\uff0c\u7528\u4e8e\u6a21\u62df\u5177\u6709\u4e2a\u4f53\u51b3\u7b56\u4ee5\u53ca\u7acb\u6cd5\u3001\u5ba1\u5224\u548c\u6267\u6cd5\u673a\u6784\u673a\u5236\u7684\u6cd5\u5f8b\u573a\u666f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8eLLM\u7684\u4ee3\u7406\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u53ef\u4ee5\u590d\u73b0\u5b8f\u89c2\u72af\u7f6a\u8d8b\u52bf\uff0c\u5e76\u63d0\u4f9b\u4e0e\u73b0\u5b9e\u89c2\u5bdf\u4e00\u81f4\u7684\u89c1\u89e3\u3002\u5fae\u89c2\u6a21\u62df\u663e\u793a\uff0c\u5065\u5168\u3001\u900f\u660e\u548c\u9002\u5e94\u6027\u5f3a\u7684\u6cd5\u5f8b\u4f53\u7cfb\u80fd\u66f4\u597d\u5730\u4fdd\u62a4\u5f31\u52bf\u4e2a\u4f53\u7684\u6743\u5229\u3002", "conclusion": "LLM\u53ef\u4ee5\u4f5c\u4e3a\u6a21\u62df\u6cd5\u5f8b\u573a\u666f\u7684\u6709\u6548\u5de5\u5177\uff0c\u5728\u5b8f\u89c2\u548c\u5fae\u89c2\u5c42\u9762\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u89c1\u89e3\uff0c\u5e76\u53ef\u7528\u4e8e\u9a8c\u8bc1\u548c\u53d1\u5c55\u6cd5\u5f8b\u7406\u8bba\u4ee5\u53ca\u652f\u6301\u6cd5\u5f8b\u7ba1\u7406\u3002"}}
{"id": "2510.23853", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23853", "abs": "https://arxiv.org/abs/2510.23853", "authors": ["Yize Cheng", "Arshia Soltani Moakhar", "Chenrui Fan", "Kazem Faghih", "Parsa Hosseini", "Wenxiao Wang", "Soheil Feizi"], "title": "Temporal Blindness in Multi-Turn LLM Agents: Misaligned Tool Use vs. Human Time Perception", "comment": "preliminary work in progress", "summary": "Large language model agents are increasingly used in multi-turn\nconversational settings to interact with and execute tasks in dynamic\nenvironments. However, a key limitation is their temporal blindness: they, by\ndefault, operate with a stationary context, failing to account for the\nreal-world time elapsed between messages. This becomes a critical liability\nwhen an agent must decide whether to invoke a tool based on how much time has\npassed since the last observation. Without temporal awareness, agents often\neither over-rely on previous context (skipping necessary tool calls), or\nunder-rely on it (unnecessarily repeating tool calls). To study this challenge,\nwe introduce TicToc-v1, a test set of multi-turn user-agent trajectories across\n34 scenarios with varying time sensitivity. Each trajectory ends with a user\nquestion, where the need for a tool call depends on the amount of time elapsed\nsince the last message. To give LLMs temporal context, we augment dialogue\nmessages with explicit timestamps, bridging the gap between static dialogue and\nevolving environments. We then collected human preferences for these samples,\ncreating two subsets: one where humans preferred relying on the previous\nobservation (prefer-noTool), and another where they preferred a new tool call\n(prefer-Tool). We evaluated how well LLM tool-calling decisions align with\nhuman preferences under varying time intervals on TicToc-v1. Our analysis show\nthat without time information, most models perform only slightly better than\nrandom, with the top alignment rate being just over 60%. While adding\ntimestamps leads to a slight improvement, particularly for larger models, the\nimprovement is modest, peaking at around 65%. We also show that naive,\nprompt-based alignment have limited effectiveness. Our findings highlight the\nneed for specific post-training alignment to align multi-turn LLM tool use with\nhuman temporal perception.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u5b58\u5728\u65f6\u95f4\u611f\u77e5\u7f3a\u5931\u95ee\u9898\uff0c\u5bfc\u81f4\u5de5\u5177\u8c03\u7528\u51b3\u7b56\u4e0d\u5f53\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86TicToc-v1\u6570\u636e\u96c6\u548c\u65f6\u95f4\u6233\u589e\u5f3a\u65b9\u6cd5\uff0c\u5e76\u5206\u6790\u4e86\u5f53\u524d\u6a21\u578b\u5728\u65f6\u95f4\u654f\u611f\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u5f3a\u8c03\u4e86\u8fdb\u884c\u4e13\u95e8\u7684\u5bf9\u9f50\u8bad\u7ec3\u7684\u5fc5\u8981\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u5728\u4e0e\u52a8\u6001\u73af\u5883\u4ea4\u4e92\u548c\u6267\u884c\u4efb\u52a1\u7684\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u5b58\u5728\u65f6\u95f4\u611f\u77e5\u7f3a\u5931\uff08temporal blindness\uff09\u95ee\u9898\uff0c\u5373\u65e0\u6cd5\u8003\u8651\u6d88\u606f\u4e4b\u95f4\u5b9e\u9645\u7ecf\u8fc7\u7684\u65f6\u95f4\uff0c\u5bfc\u81f4\u5de5\u5177\u8c03\u7528\u51b3\u7b56\u4e0d\u51c6\u786e\uff0c\u8981\u4e48\u8fc7\u5ea6\u4f9d\u8d56\u65e7\u4fe1\u606f\uff0c\u8981\u4e48\u91cd\u590d\u8c03\u7528\u5de5\u5177\u3002", "method": "\u63d0\u51faTicToc-v1\u6570\u636e\u96c6\uff0c\u5305\u542b34\u4e2a\u4e0d\u540c\u65f6\u95f4\u654f\u611f\u5ea6\u7684\u591a\u8f6e\u5bf9\u8bdd\u573a\u666f\uff0c\u5e76\u5728\u5bf9\u8bdd\u6d88\u606f\u4e2d\u6dfb\u52a0\u663e\u5f0f\u65f6\u95f4\u6233\u6765\u589e\u5f3a\u65f6\u95f4\u4fe1\u606f\u3002\u6536\u96c6\u4e86\u4eba\u7c7b\u504f\u597d\u6570\u636e\uff0c\u5206\u4e3a\u503e\u5411\u4e8e\u4f9d\u8d56\u5148\u524d\u89c2\u5bdf\uff08prefer-noTool\uff09\u548c\u503e\u5411\u4e8e\u8c03\u7528\u65b0\u5de5\u5177\uff08prefer-Tool\uff09\u4e24\u7c7b\u3002\u8bc4\u4f30\u4e86\u5728TicToc-v1\u4e0a\uff0c\u6dfb\u52a0\u65f6\u95f4\u6233\u540eLLM\u7684\u5de5\u5177\u8c03\u7528\u51b3\u7b56\u4e0e\u4eba\u7c7b\u504f\u597d\u7684\u5bf9\u9f50\u7387\u3002", "result": "\u5728\u6ca1\u6709\u65f6\u95f4\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b\uff0c\u5927\u591a\u6570\u6a21\u578b\u8868\u73b0\u4ec5\u7565\u4f18\u4e8e\u968f\u673a\u731c\u6d4b\uff0c\u6700\u9ad8\u5bf9\u9f50\u7387\u7ea6\u4e3a60%\u3002\u6dfb\u52a0\u65f6\u95f4\u6233\u540e\uff0c\u6a21\u578b\u8868\u73b0\u6709\u6240\u6539\u5584\uff0c\u7279\u522b\u662f\u5927\u578b\u6a21\u578b\uff0c\u4f46\u6539\u8fdb\u6709\u9650\uff0c\u6700\u9ad8\u5bf9\u9f50\u7387\u7ea6\u4e3a65%\u3002\u57fa\u4e8e\u63d0\u793a\u7684\u65b9\u6cd5\u5bf9\u9f50\u6548\u679c\u6709\u9650\u3002", "conclusion": "\u5f53\u524d\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7684\u5de5\u5177\u8c03\u7528\u51b3\u7b56\u4e0e\u4eba\u7c7b\u7684\u65f6\u95f4\u611f\u77e5\u5b58\u5728\u5dee\u8ddd\uff0c\u9700\u8981\u8fdb\u884c\u4e13\u95e8\u7684\u540e\u8bad\u7ec3\uff08post-training\uff09\u5bf9\u9f50\uff0c\u4ee5\u89e3\u51b3\u5176\u65f6\u95f4\u611f\u77e5\u7f3a\u5931\u95ee\u9898\u3002"}}
{"id": "2510.23963", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.23963", "abs": "https://arxiv.org/abs/2510.23963", "authors": ["Hiroki Ishikawa", "Kyosuke Ishibashi", "Ko Yamamoto"], "title": "Adaptive-twist Soft Finger Mechanism for Grasping by Wrapping", "comment": null, "summary": "This paper presents a soft robot finger capable of adaptive-twist deformation\nto grasp objects by wrapping them. For a soft hand to grasp and pick-up one\nobject from densely contained multiple objects, a soft finger requires the\nadaptive-twist deformation function in both in-plane and out-of-plane\ndirections. The function allows the finger to be inserted deeply into a limited\ngap among objects. Once inserted, the soft finger requires appropriate control\nof grasping force normal to contact surface, thereby maintaining the twisted\ndeformation. In this paper, we refer to this type of grasping as grasping by\nwrapping. To achieve these two functions by a single actuation source, we\npropose a variable stiffness mechanism that can adaptively change the stiffness\nas the pressure is higher. We conduct a finite element analysis (FEA) on the\nproposed mechanism and determine its design parameter based on the FEA result.\nUsing the developed soft finger, we report basic experimental results and\ndemonstrations on grasping various objects.", "AI": {"tldr": "\u8f6f\u4f53\u673a\u5668\u4eba\u624b\u6307\u80fd\u81ea\u9002\u5e94\u626d\u8f6c\u53d8\u5f62\u4ee5\u5305\u88f9\u6293\u53d6\u7269\u4f53\uff0c\u901a\u8fc7\u5355\u4e00\u9a71\u52a8\u6e90\u7ed3\u5408\u53ef\u53d8\u521a\u5ea6\u673a\u5236\u5b9e\u73b0\u3002", "motivation": "\u4e3a\u4e86\u8ba9\u8f6f\u4f53\u624b\u80fd\u591f\u4ece\u5bc6\u96c6\u5806\u53e0\u7684\u591a\u4e2a\u7269\u4f53\u4e2d\u6293\u53d6\u5355\u4e2a\u7269\u4f53\uff0c\u8f6f\u4f53\u624b\u6307\u9700\u8981\u5728\u9762\u5185\u548c\u9762\u5916\u4e24\u4e2a\u65b9\u5411\u4e0a\u5b9e\u73b0\u81ea\u9002\u5e94\u626d\u8f6c\u53d8\u5f62\uff0c\u4ee5\u4fbf\u80fd\u6df1\u5165\u63d2\u5165\u7269\u4f53\u95f4\u7684\u72ed\u7a84\u95f4\u9699\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u53d8\u521a\u5ea6\u673a\u5236\uff0c\u80fd\u591f\u968f\u7740\u538b\u529b\u7684\u5347\u9ad8\u81ea\u9002\u5e94\u5730\u6539\u53d8\u624b\u6307\u7684\u521a\u5ea6\uff0c\u4ece\u800c\u5728\u4fdd\u6301\u626d\u8f6c\u53d8\u5f62\u7684\u540c\u65f6\u63a7\u5236\u6cd5\u5411\u6293\u53d6\u529b\u3002\u901a\u8fc7\u6709\u9650\u5143\u5206\u6790\uff08FEA\uff09\u786e\u5b9a\u4e86\u8bbe\u8ba1\u53c2\u6570\u3002", "result": "\u5b8c\u6210\u4e86\u8f6f\u4f53\u624b\u6307\u7684\u5f00\u53d1\uff0c\u5e76\u8fdb\u884c\u4e86\u6293\u53d6\u5404\u79cd\u7269\u4f53\u7684\u57fa\u672c\u5b9e\u9a8c\u548c\u6f14\u793a\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u8f6f\u4f53\u624b\u6307\u53ca\u5176\u53ef\u53d8\u521a\u5ea6\u673a\u5236\u80fd\u591f\u5b9e\u73b0\u81ea\u9002\u5e94\u626d\u8f6c\u53d8\u5f62\uff0c\u5e76\u80fd\u6709\u6548\u5730\u6293\u53d6\u7269\u4f53\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5bc6\u96c6\u7269\u4f53\u6293\u53d6\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.23931", "categories": ["cs.LG", "cs.CR", "cs.DC", "68T07 (Primary) 68M14, 68P27, 68Q32, 94A16, 62H35 (Secondary)", "I.2.11; I.2.6; C.2.4; D.4.6; K.4.1"], "pdf": "https://arxiv.org/pdf/2510.23931", "abs": "https://arxiv.org/abs/2510.23931", "authors": ["Miguel Fernandez-de-Retana", "Unai Zulaika", "Rub\u00e9n S\u00e1nchez-Corcuera", "Aitor Almeida"], "title": "Differential Privacy: Gradient Leakage Attacks in Federated Learning Environments", "comment": "17 pages, 12 figures", "summary": "Federated Learning (FL) allows for the training of Machine Learning models in\na collaborative manner without the need to share sensitive data. However, it\nremains vulnerable to Gradient Leakage Attacks (GLAs), which can reveal private\ninformation from the shared model updates. In this work, we investigate the\neffectiveness of Differential Privacy (DP) mechanisms - specifically, DP-SGD\nand a variant based on explicit regularization (PDP-SGD) - as defenses against\nGLAs. To this end, we evaluate the performance of several computer vision\nmodels trained under varying privacy levels on a simple classification task,\nand then analyze the quality of private data reconstructions obtained from the\nintercepted gradients in a simulated FL environment. Our results demonstrate\nthat DP-SGD significantly mitigates the risk of gradient leakage attacks,\nalbeit with a moderate trade-off in model utility. In contrast, PDP-SGD\nmaintains strong classification performance but proves ineffective as a\npractical defense against reconstruction attacks. These findings highlight the\nimportance of empirically evaluating privacy mechanisms beyond their\ntheoretical guarantees, particularly in distributed learning scenarios where\ninformation leakage may represent an unassumable critical threat to data\nsecurity and privacy.", "AI": {"tldr": "\u5dee\u5206\u9690\u79c1\uff08DP\uff09\u673a\u5236\uff0c\u7279\u522b\u662fDP-SGD\uff0c\u53ef\u4ee5\u6709\u6548\u9632\u5fa1\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u4e2d\u7684\u68af\u5ea6\u6cc4\u9732\u653b\u51fb\uff08GLAs\uff09\uff0c\u4f46\u53ef\u80fd\u4f1a\u727a\u7272\u4e00\u5b9a\u7684\u6a21\u578b\u6548\u7528\u3002PDP-SGD\u867d\u7136\u80fd\u4fdd\u6301\u6a21\u578b\u6027\u80fd\uff0c\u4f46\u4e0d\u80fd\u6709\u6548\u9632\u5fa1\u91cd\u5efa\u653b\u51fb\u3002", "motivation": "\u7814\u7a76\u5dee\u5206\u9690\u79c1\uff08DP\uff09\u673a\u5236\uff08DP-SGD\u548cPDP-SGD\uff09\u5728\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u4e2d\u9632\u5fa1\u68af\u5ea6\u6cc4\u9732\u653b\u51fb\uff08GLAs\uff09\u7684\u6709\u6548\u6027\uff0c\u5e76\u8bc4\u4f30\u5176\u5bf9\u6a21\u578b\u6548\u7528\u548c\u6570\u636e\u91cd\u5efa\u7684\u5f71\u54cd\u3002", "method": "\u5728\u6a21\u62df\u7684FL\u73af\u5883\u4e2d\uff0c\u8bc4\u4f30\u4e0d\u540c\u9690\u79c1\u7ea7\u522b\u4e0b\u8bad\u7ec3\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5e76\u5206\u6790\u4ece\u622a\u83b7\u7684\u68af\u5ea6\u4e2d\u91cd\u5efa\u79c1\u4eba\u6570\u636e\u7684\u8d28\u91cf\u3002", "result": "DP-SGD\u663e\u8457\u964d\u4f4e\u4e86\u68af\u5ea6\u6cc4\u9732\u7684\u98ce\u9669\uff0c\u4f46\u6a21\u578b\u6548\u7528\u6709\u6240\u964d\u4f4e\u3002PDP-SGD\u5728\u5206\u7c7b\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u4e0d\u80fd\u6709\u6548\u9632\u5fa1\u91cd\u5efa\u653b\u51fb\u3002", "conclusion": "\u5728\u8bc4\u4f30DP\u673a\u5236\u65f6\uff0c\u5fc5\u987b\u8fdb\u884c\u5b9e\u9645\u8bc4\u4f30\uff0c\u7279\u522b\u662f\u5728FL\u73af\u5883\u4e2d\uff0c\u4fe1\u606f\u6cc4\u9732\u53ef\u80fd\u5bf9\u6570\u636e\u5b89\u5168\u548c\u9690\u79c1\u6784\u6210\u4e25\u91cd\u5a01\u80c1\u3002"}}
{"id": "2510.24144", "categories": ["cond-mat.mtrl-sci", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2510.24144", "abs": "https://arxiv.org/abs/2510.24144", "authors": ["Lei Sun", "Elvar \u00d6rn J\u00f3nsson", "Aleksei Ivanov", "Ji Chen", "Hannes J\u00f3nsson"], "title": "Variational Calculations of the Excited States of the Charged NV-center in Diamond Using a Hybrid Functional", "comment": null, "summary": "The excited electronic states involved in the optical cycle preparation of a\npure spin state of the negatively charged NV-defect in diamond are calculated\nusing the HSE06 hybrid density functional and variational optimization of the\norbitals. This includes the energy of the excited triplet as well as the two\nlowest singlet states with respect to the ground triplet state. In addition to\nthe vertical excitation, the effect of structural relaxation is also estimated\nusing analytical atomic forces. The lowering of the energy in the triplet\nexcited state and the resulting zero-phonon line triplet excitation energy are\nboth within 0.1 eV of the experimental estimates. An analogous relaxation in\nthe lower energy singlet state using spin purified atomic forces is estimated\nto be 0.06 eV. These results, obtained with a hybrid density functional,\nimprove on previously published results using local and semi-local functionals,\nwhich are known to underestimate the band gap. The good agreement with\nexperimental estimates demonstrates how time-independent variational\ncalculations of excited states using density functionals can give accurate\nresults and, thereby, provide a powerful screening tool for identifying other\ndefect systems as candidates for quantum technologies.", "AI": {"tldr": "\u4f7f\u7528 HSE06 \u6df7\u5408\u5bc6\u5ea6\u6cdb\u51fd\u8ba1\u7b97\u4e86\u91d1\u521a\u77f3\u4e2d\u5e26\u8d1f\u7535\u7684 NV \u7f3a\u9677\u7eaf\u81ea\u65cb\u6001\u7684\u5149\u5b66\u5faa\u73af\u5236\u5907\u6240\u6d89\u53ca\u7684\u6fc0\u53d1\u7535\u5b50\u6001\u3002", "motivation": "\u8ba1\u7b97 NV \u7f3a\u9677\u7eaf\u81ea\u65cb\u6001\u5149\u5b66\u5faa\u73af\u5236\u5907\u6240\u6d89\u53ca\u7684\u6fc0\u53d1\u7535\u5b50\u6001\uff0c\u5e76\u4e0e\u5b9e\u9a8c\u7ed3\u679c\u8fdb\u884c\u6bd4\u8f83\uff0c\u4ee5\u9a8c\u8bc1\u8be5\u8ba1\u7b97\u65b9\u6cd5\u7684\u51c6\u786e\u6027\uff0c\u5e76\u4e3a\u91cf\u5b50\u6280\u672f\u7b5b\u9009\u5176\u4ed6\u7f3a\u9677\u7cfb\u7edf\u3002", "method": "\u4f7f\u7528 HSE06 \u6df7\u5408\u5bc6\u5ea6\u6cdb\u51fd\u548c\u8f68\u9053\u7684\u53d8\u5206\u4f18\u5316\u8ba1\u7b97\u4e86\u6fc0\u53d1\u4e09\u7ebf\u6001\u4ee5\u53ca\u76f8\u5bf9\u4e8e\u57fa\u6001\u4e09\u7ebf\u6001\u7684\u6700\u4f4e\u4e24\u4e2a\u5355\u7ebf\u6001\u7684\u80fd\u91cf\u3002\u4f30\u8ba1\u4e86\u7ed3\u6784\u5f1b\u8c6b\u5bf9\u5782\u76f4\u6fc0\u53d1\u7684\u5f71\u54cd\uff0c\u5e76\u8ba1\u7b97\u4e86\u4e09\u7ebf\u6001\u6fc0\u53d1\u6001\u548c\u96f6\u58f0\u5b50\u7ebf\u4e09\u7ebf\u6001\u6fc0\u53d1\u80fd\u3002", "result": "\u4e09\u7ebf\u6001\u6fc0\u53d1\u6001\u548c\u96f6\u58f0\u5b50\u7ebf\u4e09\u7ebf\u6001\u6fc0\u53d1\u80fd\u7684\u80fd\u91cf\u964d\u4f4e\u503c\u4e0e\u5b9e\u9a8c\u4f30\u8ba1\u503c\u5728 0.1 eV \u4ee5\u5185\u3002\u4f7f\u7528\u81ea\u65cb\u7eaf\u5316\u539f\u5b50\u529b\u4f30\u8ba1\u7684\u8f83\u4f4e\u80fd\u91cf\u5355\u7ebf\u6001\u7684\u7c7b\u4f3c\u5f1b\u8c6b\u4e3a 0.06 eV\u3002", "conclusion": "\u901a\u8fc7\u65f6\u65e0\u5173\u7684\u5bc6\u5ea6\u6cdb\u51fd\u6fc0\u53d1\u6001\u53d8\u5206\u8ba1\u7b97\u53ef\u4ee5\u83b7\u5f97\u51c6\u786e\u7684\u7ed3\u679c\uff0c\u53ef\u4ee5\u4f5c\u4e3a\u8bc6\u522b\u5176\u4ed6\u91cf\u5b50\u6280\u672f\u5019\u9009\u7f3a\u9677\u7cfb\u7edf\u7684\u7b5b\u9009\u5de5\u5177\u3002"}}
{"id": "2510.24291", "categories": ["cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2510.24291", "abs": "https://arxiv.org/abs/2510.24291", "authors": ["Vladimir U. Nazarov", "Tchavdar N. Todorov", "E. K. U. Gross"], "title": "Viscous AC current-driven nanomotors", "comment": "19 pages, 7 figures", "summary": "The recent discovery that electrons in nano-scale conductors can act like a\nhighly viscous liquid has triggered a surge of research activities\ninvestigating consequences of this surprising fact. Here we demonstrate that\nthe electronic viscosity has an enormous influence on the operation of a\nprototypical AC-current-driven nano-motor. The design of this prototype\nconsists of a diatomic molecule immersed in an otherwise homogeneous electron\nliquid which carries an AC current. The motion of the diatomic is determined by\na subtle balance between the current-induced forces and electronic friction. By\nab-initio time-dependent density-functional simulations we demonstrate that the\ndiatomic performs a continuous rotation provided the amplitude and frequency of\nthe imposed AC current lie within certain islands of stability. Outside these\nislands the nuclear motion is either chaotic or comes to a stand-still. The\nproposed design of the nano-motor is the conceptually simplest realization of\nthe idea of an molecular waterwheel sandwiched between conducting leads", "AI": {"tldr": "\u7535\u5b50\u7c98\u5ea6\u663e\u8457\u5f71\u54cd\u7eb3\u7c73\u9a6c\u8fbe\u7684\u8fd0\u884c\uff0c\u7279\u5b9a\u7684\u4ea4\u6d41\u7535\u6d41\u53c2\u6570\u53ef\u5b9e\u73b0\u8fde\u7eed\u65cb\u8f6c\uff0c\u5426\u5219\u8fd0\u52a8\u5c06\u53d8\u5f97\u6df7\u4e71\u6216\u505c\u6b62\u3002", "motivation": "\u7535\u5b50\u5728\u7eb3\u7c73\u5bfc\u4f53\u4e2d\u8868\u73b0\u51fa\u9ad8\u7c98\u5ea6\u6db2\u4f53\u7684\u7279\u6027\uff0c\u5f15\u53d1\u4e86\u5bf9\u5176\u5f71\u54cd\u7684\u7814\u7a76\uff0c\u672c\u6587\u65e8\u5728\u63a2\u8ba8\u8fd9\u79cd\u7535\u5b50\u7c98\u5ea6\u5bf9\u4ea4\u6d41\u9a71\u52a8\u7eb3\u7c73\u9a6c\u8fbe\u8fd0\u884c\u7684\u5de8\u5927\u5f71\u54cd\u3002", "method": "\u5229\u7528\u4ece\u5934\u7b97\u65f6\u95f4\u4f9d\u8d56\u5bc6\u5ea6\u6cdb\u51fd\u6a21\u62df\uff0c\u7814\u7a76\u4e86\u6d78\u5165\u7535\u5b50\u6db2\u4f53\u4e2d\u7684\u53cc\u539f\u5b50\u5206\u5b50\u5728\u4ea4\u6d41\u7535\u6d41\u9a71\u52a8\u4e0b\u7684\u8fd0\u52a8\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u5f53\u4ea4\u6d41\u7535\u6d41\u7684\u5e45\u5ea6\u548c\u9891\u7387\u5728\u7279\u5b9a\u7684\u7a33\u5b9a\u5c9b\u5185\u65f6\uff0c\u53cc\u539f\u5b50\u5206\u5b50\u53ef\u4ee5\u6301\u7eed\u65cb\u8f6c\u3002\u5728\u8fd9\u4e9b\u5c9b\u4e4b\u5916\uff0c\u5206\u5b50\u8fd0\u52a8\u8868\u73b0\u4e3a\u6df7\u6c8c\u6216\u505c\u6b62\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u7eb3\u7c73\u9a6c\u8fbe\u8bbe\u8ba1\u662f\u5206\u5b50\u6c34\u8f66\u6982\u5ff5\u6700\u7b80\u5355\u7684\u5b9e\u73b0\u65b9\u5f0f\uff0c\u7535\u5b50\u7c98\u5ea6\u662f\u5176\u8fd0\u884c\u7684\u5173\u952e\u56e0\u7d20\u3002"}}
{"id": "2510.23894", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23894", "abs": "https://arxiv.org/abs/2510.23894", "authors": ["Jinxin Zhou", "Jiachen Jiang", "Zhihui Zhu"], "title": "Improving Visual Discriminability of CLIP for Training-Free Open-Vocabulary Semantic Segmentation", "comment": "23 pages, 10 figures, 14 tables", "summary": "Extending CLIP models to semantic segmentation remains challenging due to the\nmisalignment between their image-level pre-training objectives and the\npixel-level visual understanding required for dense prediction. While prior\nefforts have achieved encouraging results by reorganizing the final layer and\nfeatures, they often inherit the global alignment bias of preceding layers,\nleading to suboptimal segmentation performance. In this work, we propose\nLHT-CLIP, a novel training-free framework that systematically exploits the\nvisual discriminability of CLIP across layer, head, and token levels. Through\ncomprehensive analysis, we reveal three key insights: (i) the final layers\nprimarily strengthen image-text alignment with sacrifice of visual\ndiscriminability (e.g., last 3 layers in ViT-B/16 and 8 layers in ViT-L/14),\npartly due to the emergence of anomalous tokens; (ii) a subset of attention\nheads (e.g., 10 out of 144 in ViT-B/16) display consistently strong visual\ndiscriminability across datasets; (iii) abnormal tokens display sparse and\nconsistent activation pattern compared to normal tokens. Based on these\nfindings, we propose three complementary techniques: semantic-spatial\nreweighting, selective head enhancement, and abnormal token replacement to\neffectively restore visual discriminability and improve segmentation\nperformance without any additional training, auxiliary pre-trained networks, or\nextensive hyperparameter tuning. Extensive experiments on 8 common semantic\nsegmentation benchmarks demonstrate that LHT-CLIP achieves state-of-the-art\nperformance across diverse scenarios, highlighting its effectiveness and\npracticality for real-world deployment.", "AI": {"tldr": "LHT-CLIP\u662f\u4e00\u4e2a\u65b0\u7684\u8bad\u7ec3\u514d\u8d39\u6846\u67b6\uff0c\u5b83\u5229\u7528CLIP\u7684\u89c6\u89c9\u5224\u522b\u529b\u6765\u63d0\u9ad8\u8bed\u4e49\u5206\u5272\u6027\u80fd\u3002", "motivation": "CLIP\u6a21\u578b\u5728\u56fe\u50cf\u7ea7\u9884\u8bad\u7ec3\u548c\u50cf\u7d20\u7ea7\u8bed\u4e49\u5206\u5272\u4e4b\u95f4\u7684\u76ee\u6807\u4e0d\u4e00\u81f4\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\u3002", "method": "\u901a\u8fc7\u5206\u6790CLIP\u6a21\u578b\uff0c\u53d1\u73b0\u6700\u540e\u4e00\u5c42\u548c\u67d0\u4e9b\u6ce8\u610f\u529b\u5934\u4f1a\u524a\u5f31\u89c6\u89c9\u5224\u522b\u529b\uff0c\u5e76\u63d0\u51fa\u8bed\u4e49\u7a7a\u95f4\u91cd\u52a0\u6743\u3001\u9009\u62e9\u6027\u6ce8\u610f\u529b\u5934\u589e\u5f3a\u548c\u5f02\u5e38Token\u66ff\u6362\u6280\u672f\u6765\u6062\u590d\u89c6\u89c9\u5224\u522b\u529b\u3002", "result": "\u57288\u4e2a\u8bed\u4e49\u5206\u5272\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLHT-CLIP\u5728\u5404\u79cd\u573a\u666f\u4e0b\u90fd\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "LHT-CLIP\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u6709\u6548\u63d0\u9ad8\u8bed\u4e49\u5206\u5272\u6027\u80fd\uff0c\u5e76\u4e14\u5177\u6709\u5f88\u5f3a\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2510.23736", "categories": ["quant-ph", "math-ph", "math.CO", "math.MP", "81P42, 81P45, 81P73, 05B35"], "pdf": "https://arxiv.org/pdf/2510.23736", "abs": "https://arxiv.org/abs/2510.23736", "authors": ["Stephane Dartois", "Gilles Z\u00e9mor"], "title": "The injective norm of CSS quantum error-correcting codes", "comment": "11 pages", "summary": "In this paper, we compute the injective norm - a.k.a. geometric entanglement\n- of standard basis states of CSS quantum error-correcting codes. The injective\nnorm of a quantum state is a measure of genuine multipartite entanglement.\nComputing this measure is generically NP-hard. However, it has been computed\nexactly in condensed-matter theory - notably in the context of topological\nphases - for the Kitaev code and its extensions, in works by Or\\'us and\ncollaborators. We extend these results to all CSS codes and thereby obtain the\ninjective norm for a nontrivial, infinite family of quantum states. In doing\nso, we uncover an interesting connection to matroid theory and Edmonds'\nintersection theorem.", "AI": {"tldr": "\u672c\u6587\u8ba1\u7b97\u4e86CSS\u91cf\u5b50\u7ea0\u9519\u7801\u7684\u6807\u51c6\u57fa\u6001\u7684\u5185\u5c04\u8303\u6570\uff08\u4e5f\u79f0\u4e3a\u51e0\u4f55\u7ea0\u7f20\uff09\u3002", "motivation": "\u8ba1\u7b97\u5185\u5c04\u8303\u6570\uff08\u4e00\u79cd\u8861\u91cf\u591a\u65b9\u7ea0\u7f20\u7684\u5ea6\u91cf\uff09\u901a\u5e38\u662fNP\u96be\u7684\uff0c\u4f46\u672c\u6587\u5c06\u8be5\u8ba1\u7b97\u6269\u5c55\u5230\u6240\u6709CSS\u7801\uff0c\u5e76\u53d1\u73b0\u4e86\u4e0e\u62df\u9635\u8bba\u548cEdmonds\u4ea4\u70b9\u5b9a\u7406\u7684\u8054\u7cfb\u3002", "method": "\u5c06\u4e4b\u524d\u5bf9Kitaev\u7801\u53ca\u5176\u6269\u5c55\u7684\u7814\u7a76\u63a8\u5e7f\u5230\u6240\u6709CSS\u7801\uff0c\u5e76\u5229\u7528\u62df\u9635\u8bba\u548cEdmonds\u4ea4\u70b9\u5b9a\u7406\u3002", "result": "\u5f97\u5230\u4e86\u4e00\u4e2a\u975e\u5e73\u51e1\u7684\u3001\u65e0\u9650\u7684\u91cf\u5b50\u6001\u5bb6\u65cf\u7684\u5185\u5c04\u8303\u6570\u3002", "conclusion": "\u8ba1\u7b97CSS\u91cf\u5b50\u7ea0\u9519\u7801\u7684\u6807\u51c6\u57fa\u6001\u7684\u5185\u5c04\u8303\u6570\uff0c\u63ed\u793a\u4e86\u4e0e\u62df\u9635\u8bba\u7684\u8054\u7cfb\u3002"}}
{"id": "2510.24252", "categories": ["cs.LO"], "pdf": "https://arxiv.org/pdf/2510.24252", "abs": "https://arxiv.org/abs/2510.24252", "authors": ["Benjamin Plummer", "Corina Cirstea"], "title": "Traces via Strategies in Two-Player Games", "comment": null, "summary": "Traces form a coarse notion of semantic equivalence between states of a\nprocess, and have been studied coalgebraically for various types of system. We\ninstantiate the finitary coalgebraic trace semantics framework of Hasuo et al.\nfor controller-versus-environment games, encompassing both nondeterministic and\nprobabilistic environments. Although our choice of monads is guided by the\nconstraints of this abstract framework, they enable us to recover familiar\ngame-theoretic concepts. Concretely, we show that in these games, each element\nin the trace map corresponds to a collection (a subset or distribution) of\nplays the controller can force. Furthermore, each element can be seen as the\noutcome of following a controller strategy. Our results are parametrised by a\nweak distributive law, which computes what the controller can force in a single\nstep.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5c06Hasuo\u7b49\u4eba\u7684\u6709\u9650\u53f6\u5b50\u4ee3\u6570\u8ffd\u8e2a\u8bed\u4e49\u6846\u67b6\u5e94\u7528\u4e8e\u63a7\u5236\u5668\u4e0e\u73af\u5883\u535a\u5f08\uff0c\u5e76\u6210\u529f\u5730\u878d\u5408\u4e86\u4e0d\u786e\u5b9a\u6027\u548c\u6982\u7387\u6027\u73af\u5883\u3002", "motivation": "\u9700\u8981\u4e3a\u63a7\u5236\u5668-\u73af\u5883\u535a\u5f08\u5efa\u7acb\u4e00\u4e2a\u53f6\u5b50\u8ffd\u8e2a\u8bed\u4e49\u6846\u67b6\uff0c\u4ee5\u878d\u5408\u4e0d\u786e\u5b9a\u6027\u548c\u6982\u7387\u6027\u73af\u5883\u3002", "method": "\u5c06Hasuo\u7b49\u4eba\u7684\u6709\u9650\u53f6\u5b50\u4ee3\u6570\u8ffd\u8e2a\u8bed\u4e49\u6846\u67b6\u5b9e\u4f8b\u5316\uff0c\u4ee5\u5904\u7406\u63a7\u5236\u5668-\u73af\u5883\u535a\u5f08\uff0c\u5e76\u7ed3\u5408\u4e0d\u786e\u5b9a\u6027\u548c\u6982\u7387\u6027\u73af\u5883\u3002", "result": "\u5728\u8fd9\u4e9b\u535a\u5f08\u4e2d\uff0c\u8ffd\u8e2a\u56fe\u7684\u6bcf\u4e2a\u5143\u7d20\u5bf9\u5e94\u63a7\u5236\u5668\u53ef\u4ee5\u5f3a\u8feb\u7684\u4e00\u7ec4\u535a\u5f08\uff08\u5b50\u96c6\u6216\u5206\u5e03\uff09\uff0c\u5e76\u4e14\u53ef\u4ee5\u88ab\u89c6\u4e3a\u9075\u5faa\u63a7\u5236\u5668\u7b56\u7565\u7684\u7ed3\u679c\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u878d\u5408\u4e0d\u786e\u5b9a\u6027\u548c\u6982\u7387\u6027\u73af\u5883\uff0c\u5e76\u4e14\u5176\u7ed3\u679c\u4e0e\u719f\u6089\u7684\u535a\u5f08\u8bba\u6982\u5ff5\u76f8\u5173\u3002"}}
{"id": "2510.23877", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.23877", "abs": "https://arxiv.org/abs/2510.23877", "authors": ["Zhentong Shao", "Nanpeng Yu"], "title": "Carbon-Aware Optimal Power Flow with Data-Driven Carbon Emission Tracing", "comment": null, "summary": "Quantifying locational carbon emissions in power grids is crucial for\nimplementing effective carbon reduction strategies for customers relying on\nelectricity. This paper presents a carbon-aware optimal power flow (OPF)\nframework that incorporates data-driven carbon tracing, enabling rapid\nestimation of nodal carbon emissions from electric loads. By developing\ngenerator-to-load carbon emission distribution factors through data-driven\ntechnique, the analytical formulas for both average and marginal carbon\nemissions can be derived and integrated seamlessly into DC OPF models as linear\nconstraints. The proposed carbon-aware OPF model enables market operators to\noptimize energy dispatch while reducing greenhouse gas emissions. Simulations\non IEEE test systems confirm the accuracy and computational efficiency of the\nproposed approach, highlighting its applicability for real-time carbon-aware\nsystem operations.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7684\u78b3\u6392\u653e\u91cf\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f18\u5316\u7535\u7f51\u8c03\u5ea6\u4ee5\u51cf\u5c11\u78b3\u6392\u653e\u3002", "motivation": "\u91cf\u5316\u7535\u7f51\u4e2d\u7684\u533a\u57df\u78b3\u6392\u653e\u5bf9\u4e8e\u5236\u5b9a\u6709\u6548\u7684\u51cf\u6392\u7b56\u7565\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5f00\u53d1\u4e86\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u6280\u672f\u751f\u6210\u4ece\u53d1\u7535\u673a\u5230\u8d1f\u8377\u7684\u78b3\u6392\u653e\u56e0\u5b50\uff0c\u5e76\u5c06\u5176\u96c6\u6210\u5230\u76f4\u6d41\u6700\u4f18\u6f6e\u6d41\uff08OPF\uff09\u6a21\u578b\u4e2d\uff0c\u63a8\u5bfc\u51fa\u5e73\u5747\u548c\u8fb9\u9645\u78b3\u6392\u653e\u7684\u89e3\u6790\u516c\u5f0f\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u51c6\u786e\u4e14\u9ad8\u6548\u5730\u4f30\u7b97\u8282\u70b9\u78b3\u6392\u653e\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8eIEEE\u6d4b\u8bd5\u7cfb\u7edf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u7528\u4e8e\u5b9e\u65f6\u78b3\u611f\u77e5\u7cfb\u7edf\u8fd0\u884c\uff0c\u4f18\u5316\u53d1\u7535\u8c03\u5ea6\u4ee5\u51cf\u5c11\u6e29\u5ba4\u6c14\u4f53\u6392\u653e\u3002"}}
{"id": "2510.23900", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.23900", "abs": "https://arxiv.org/abs/2510.23900", "authors": ["Kuan-Po Chiu", "Sumit Roy"], "title": "LEO Downlink Channel Model Revisited: Scattering Geometry-Inspired Derivation", "comment": "Accepted to Globecom 2025", "summary": "This paper presents a new derivation of LEO-to-ground receiver channel model\nto address a clear gap in the prior art: the lack of an appropriate geometry\naware characterization of non LOS (NLOS) link model represented by the power\nspectral density (PSD). Specifically, the main contribution is a coherent\nderivation of the PSD from 1st principles that is able to reproduce results in\nprior art and explain the causal relationship of main PSD features to the\npropagation geometry parameters.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2510.23629", "categories": ["cs.LG", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2510.23629", "abs": "https://arxiv.org/abs/2510.23629", "authors": ["Nuo Chen", "Zehua Li", "Keqin Bao", "Junyang Lin", "Dayiheng Liu"], "title": "Chain of Execution Supervision Promotes General Reasoning in Large Language Models", "comment": null, "summary": "Building robust and general reasoning ability is a central goal in the\ndevelopment of large language models (LLMs). Recent efforts increasingly turn\nto code as a rich training source, given its inherent logical structure and\ndiverse reasoning paradigms such as divide-and-conquer, topological ordering,\nand enumeration. However, reasoning in code is often expressed implicitly and\nentangled with syntactic or implementation noise, making direct training on raw\ncode suboptimal.To address this, we introduce TracePile, a large-scale corpus\nof 2.6 million samples that transforms code execution into explicit,\nstep-by-step chain-of-thought-style rationales, which we call Chain of\nExecution (CoE). The corpus spans domains including mathematics, classical\nalgorithms and algorithmic competition, and is enriched with variable-tracing\nquestions and code rewritings to enhance logical granularity and code\ndiversity. We evaluate TracePile using three training setups:\ncontinue-pretraining, instruction tuning after pretraining, and two-stage\nfinetuning. Experiments across four base models (LLaMA 3, LLaMA 3.1, Qwen-2.5,\nand Qwen-2.5 Coder) and 20 benchmarks covering math, code, logic, and\nalgorithms demonstrate consistent improvements. Notably, TracePile boosts\nLLaMA3.1-8B by 7.1\\% on average across nine math datasets and delivers clear\ngains on LiveCodeBench, CRUX, and MMLU under two-stage fine-tuning.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86TracePile\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u5c06\u4ee3\u7801\u6267\u884c\u8f6c\u5316\u4e3a\u660e\u786e\u7684\u94fe\u5f0f\u601d\u8003\uff08Chain of Execution, CoE\uff09\uff0c\u4ee5\u589e\u5f3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u5728\u6570\u5b66\u3001\u4ee3\u7801\u3001\u903b\u8f91\u548c\u7b97\u6cd5\u7b49\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6539\u8fdb\u3002", "motivation": "\u4ee3\u7801\u63a8\u7406\u80fd\u529b\u662f\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u53d1\u5c55\u4e2d\u7684\u4e00\u4e2a\u6838\u5fc3\u76ee\u6807\u3002\u7136\u800c\uff0c\u539f\u59cb\u4ee3\u7801\u4e2d\u7684\u63a8\u7406\u8fc7\u7a0b\u5f80\u5f80\u662f\u9690\u6666\u7684\uff0c\u5e76\u5939\u6742\u7740\u8bed\u6cd5\u6216\u5b9e\u73b0\u4e0a\u7684\u566a\u97f3\uff0c\u8fd9\u4f7f\u5f97\u76f4\u63a5\u5728\u539f\u59cb\u4ee3\u7801\u4e0a\u8fdb\u884c\u8bad\u7ec3\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86TracePile\u6570\u636e\u96c6\uff0c\u5305\u542b260\u4e07\u4e2a\u6837\u672c\uff0c\u5c06\u4ee3\u7801\u6267\u884c\u8f6c\u5316\u4e3a\u660e\u786e\u7684\u3001\u9010\u6b65\u7684\u94fe\u5f0f\u601d\u8003\uff08Chain of Execution, CoE\uff09\u3002\u8be5\u6570\u636e\u96c6\u6db5\u76d6\u6570\u5b66\u3001\u7ecf\u5178\u7b97\u6cd5\u548c\u7b97\u6cd5\u7ade\u8d5b\u7b49\u9886\u57df\uff0c\u5e76\u589e\u52a0\u4e86\u53d8\u91cf\u8ffd\u8e2a\u95ee\u9898\u548c\u4ee3\u7801\u91cd\u5199\uff0c\u4ee5\u589e\u5f3a\u903b\u8f91\u7c92\u5ea6\u548c\u4ee3\u7801\u591a\u6837\u6027\u3002\u7814\u7a76\u91c7\u7528\u4e86\u7ee7\u7eed\u9884\u8bad\u7ec3\u3001\u9884\u8bad\u7ec3\u540e\u6307\u4ee4\u8c03\u4f18\u548c\u4e24\u9636\u6bb5\u5fae\u8c03\u7b49\u4e09\u79cd\u8bad\u7ec3\u65b9\u6cd5\u3002", "result": "\u5728LLaMA 3, LLaMA 3.1, Qwen-2.5, \u548c Qwen-2.5 Coder\u8fd9\u56db\u79cd\u57fa\u7840\u6a21\u578b\u4e0a\uff0c\u4ee5\u53ca\u8986\u76d6\u6570\u5b66\u3001\u4ee3\u7801\u3001\u903b\u8f91\u548c\u7b97\u6cd5\u768420\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u7684\u5b9e\u9a8c\u4e2d\uff0cTracePile\u90fd\u5c55\u73b0\u4e86\u6301\u7eed\u7684\u6539\u8fdb\u3002\u7279\u522b\u5730\uff0cTracePile\u5728\u4e5d\u4e2a\u6570\u5b66\u6570\u636e\u96c6\u4e0a\u5e73\u5747\u63d0\u5347\u4e86LLaMA3.1-8B\u7684\u6027\u80fd7.1%\uff0c\u5e76\u5728\u4e24\u9636\u6bb5\u5fae\u8c03\u4e0b\uff0c\u5728LiveCodeBench\u3001CRUX\u548cMMLU\u4e0a\u53d6\u5f97\u4e86\u660e\u663e\u7684\u63d0\u5347\u3002", "conclusion": "TracePile\u6570\u636e\u96c6\u901a\u8fc7\u5c06\u4ee3\u7801\u6267\u884c\u663e\u5f0f\u5316\u4e3a\u94fe\u5f0f\u601d\u8003\uff0c\u80fd\u591f\u6709\u6548\u589e\u5f3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u5728\u591a\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2510.23807", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23807", "abs": "https://arxiv.org/abs/2510.23807", "authors": ["Hamid R. Tizhoosh"], "title": "Why Foundation Models in Pathology Are Failing", "comment": null, "summary": "In non-medical domains, foundation models (FMs) have revolutionized computer\nvision and language processing through large-scale self-supervised and\nmultimodal learning. Consequently, their rapid adoption in computational\npathology was expected to deliver comparable breakthroughs in cancer diagnosis,\nprognostication, and multimodal retrieval. However, recent systematic\nevaluations reveal fundamental weaknesses: low diagnostic accuracy, poor\nrobustness, geometric instability, heavy computational demands, and concerning\nsafety vulnerabilities. This short paper examines these shortcomings and argues\nthat they stem from deeper conceptual mismatches between the assumptions\nunderlying generic foundation modeling in mainstream AI and the intrinsic\ncomplexity of human tissue. Seven interrelated causes are identified:\nbiological complexity, ineffective self-supervision, overgeneralization,\nexcessive architectural complexity, lack of domain-specific innovation,\ninsufficient data, and a fundamental design flaw related to tissue patch size.\nThese findings suggest that current pathology foundation models remain\nconceptually misaligned with the nature of tissue morphology and call for a\nfundamental rethinking of the paradigm itself.", "AI": {"tldr": "\u5f53\u524d\u5728\u533b\u5b66\u9886\u57df\uff0c\u5c24\u5176\u662f\u5728\u8ba1\u7b97\u75c5\u7406\u5b66\u4e2d\uff0c\u57fa\u7840\u6a21\u578b\uff08FMs\uff09\u5728\u764c\u75c7\u8bca\u65ad\u3001\u9884\u540e\u548c\u591a\u6a21\u6001\u68c0\u7d22\u65b9\u9762\u5e76\u672a\u8fbe\u5230\u9884\u671f\u6548\u679c\uff0c\u53cd\u800c\u66b4\u9732\u51fa\u4e00\u7cfb\u5217\u6839\u672c\u6027\u7f3a\u9677\uff0c\u5305\u62ec\u8bca\u65ad\u51c6\u786e\u6027\u4f4e\u3001\u9c81\u68d2\u6027\u5dee\u3001\u51e0\u4f55\u4e0d\u7a33\u5b9a\u6027\u3001\u8ba1\u7b97\u9700\u6c42\u9ad8\u4ee5\u53ca\u5b89\u5168\u6f0f\u6d1e\u7b49\u3002", "motivation": "\u8bc4\u4f30\u57fa\u7840\u6a21\u578b\u5728\u8ba1\u7b97\u75c5\u7406\u5b66\u4e2d\u7684\u5e94\u7528\u53ca\u5176\u5728\u764c\u75c7\u8bca\u65ad\u3001\u9884\u540e\u548c\u591a\u6a21\u6001\u68c0\u7d22\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u5e76\u63a2\u8ba8\u5176\u672a\u80fd\u53d6\u5f97\u9884\u671f\u7a81\u7834\u7684\u539f\u56e0\u3002", "method": "\u5ba1\u89c6\u4e86\u57fa\u7840\u6a21\u578b\u5728\u8ba1\u7b97\u75c5\u7406\u5b66\u4e2d\u5b58\u5728\u7684\u7f3a\u9677\uff0c\u5e76\u5206\u6790\u4e86\u4e03\u4e2a\u76f8\u4e92\u5173\u8054\u7684\u539f\u56e0\uff0c\u5305\u62ec\u751f\u7269\u590d\u6742\u6027\u3001\u65e0\u6548\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u3001\u8fc7\u5ea6\u6cdb\u5316\u3001\u8fc7\u5ea6\u7684\u67b6\u6784\u590d\u6742\u6027\u3001\u7f3a\u4e4f\u9886\u57df\u7279\u5b9a\u521b\u65b0\u3001\u6570\u636e\u4e0d\u8db3\u4ee5\u53ca\u4e0e\u7ec4\u7ec7\u5757\u5927\u5c0f\u76f8\u5173\u7684\u6839\u672c\u6027\u8bbe\u8ba1\u7f3a\u9677\u3002", "result": "\u53d1\u73b0\u5f53\u524d\u57fa\u7840\u6a21\u578b\u5728\u8ba1\u7b97\u75c5\u7406\u5b66\u4e2d\u5b58\u5728\u8bca\u65ad\u51c6\u786e\u6027\u4f4e\u3001\u9c81\u68d2\u6027\u5dee\u3001\u51e0\u4f55\u4e0d\u7a33\u5b9a\u6027\u3001\u8ba1\u7b97\u9700\u6c42\u9ad8\u548c\u5b89\u5168\u6f0f\u6d1e\u7b49\u95ee\u9898\u3002", "conclusion": "\u5f53\u524d\u75c5\u7406\u5b66\u9886\u57df\u7684\u57fa\u7840\u6a21\u578b\u5728\u6982\u5ff5\u4e0a\u4e0e\u5176\u6240\u5904\u7406\u7684\u4eba\u7c7b\u7ec4\u7ec7\u5f62\u6001\u7684\u590d\u6742\u6027\u548c\u7279\u6027\u4e0d\u5339\u914d\uff0c\u9700\u8981\u5bf9\u8be5\u8303\u5f0f\u8fdb\u884c\u6839\u672c\u6027\u7684\u53cd\u601d\u548c\u91cd\u65b0\u6784\u60f3\u3002"}}
{"id": "2510.24251", "categories": ["cs.SI"], "pdf": "https://arxiv.org/pdf/2510.24251", "abs": "https://arxiv.org/abs/2510.24251", "authors": ["Jiarui Ji", "Zehua Zhang", "Zhewei Wei", "Bin Tong", "Guan Wang", "Bo Zheng"], "title": "GRAPHIA: Harnessing Social Graph Data to Enhance LLM-Based Social Simulation", "comment": null, "summary": "Large language models (LLMs) have shown promise in simulating human-like\nsocial behaviors. Social graphs provide high-quality supervision signals that\nencode both local interactions and global network structure, yet they remain\nunderutilized for LLM training. To address this gap, we propose Graphia, the\nfirst general LLM-based social graph simulation framework that leverages graph\ndata as supervision for LLM post-training via reinforcement learning. With\nGNN-based structural rewards, Graphia trains specialized agents to predict whom\nto interact with (destination selection) and how to interact (edge generation),\nfollowed by designed graph generation pipelines. We evaluate Graphia under two\nsettings: Transductive Dynamic Graph Generation (TDGG), a micro-level task with\nour proposed node-wise interaction alignment metrics; and Inductive Dynamic\nGraph Generation (IDGG), a macro-level task with our proposed metrics for\naligning emergent network properties. On three real-world networks, Graphia\nimproves micro-level alignment by 6.1% in the composite destination selection\nscore, 12% in edge classification accuracy, and 27.9% in edge content BERTScore\nover the strongest baseline. For macro-level alignment, it achieves 41.11%\nhigher structural similarity and 32.98% better replication of social phenomena\nsuch as power laws and echo chambers. Graphia also supports counterfactual\nsimulation, generating plausible behavioral shifts under platform incentives.\nOur results show that social graphs can serve as high-quality supervision\nsignals for LLM post-training, closing the gap between agent behaviors and\nnetwork dynamics for LLM-based simulation. Code is available at\nhttps://github.com/Ji-Cather/Graphia.git.", "AI": {"tldr": "Graphia\u662f\u4e00\u4e2a\u5229\u7528\u56fe\u6570\u636e\u4f5c\u4e3aLLM\uff08\u5927\u8bed\u8a00\u6a21\u578b\uff09\u7684\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u76d1\u7763\u7684\u901a\u7528LLM\u793e\u4ea4\u56fe\u6a21\u62df\u6846\u67b6\uff0c\u901a\u8fc7GNN\uff08\u56fe\u795e\u7ecf\u7f51\u7edc\uff09\u5956\u52b1\u6765\u8bad\u7ec3\u6a21\u578b\u9884\u6d4b\u4ea4\u4e92\u5bf9\u8c61\u548c\u4ea4\u4e92\u65b9\u5f0f\uff0c\u4ece\u800c\u5b9e\u73b0\u52a8\u6001\u56fe\u751f\u6210\u548c\u5b8f\u89c2\u7f51\u7edc\u5c5e\u6027\u5bf9\u9f50\uff0c\u5e76\u5728\u771f\u5b9e\u4e16\u754c\u7f51\u7edc\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u7684LLM\u5728\u6a21\u62df\u4eba\u7c7b\u793e\u4ea4\u884c\u4e3a\u65b9\u9762\u6709\u6f5c\u529b\uff0c\u4f46\u5229\u7528\u793e\u4ea4\u56fe\u6570\u636e\u8fdb\u884cLLM\u8bad\u7ec3\u4ecd\u672a\u88ab\u5145\u5206\u5229\u7528\u3002", "method": "\u63d0\u51faGraphia\u6846\u67b6\uff0c\u5229\u7528GNN-based\u7ed3\u6784\u5956\u52b1\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8fdb\u884cLLM\u540e\u8bad\u7ec3\uff0c\u8bad\u7ec3\u4e13\u95e8\u7684Agent\u6765\u9884\u6d4b\u4ea4\u4e92\u5bf9\u8c61\uff08\u76ee\u6807\u9009\u62e9\uff09\u548c\u4ea4\u4e92\u65b9\u5f0f\uff08\u8fb9\u751f\u6210\uff09\uff0c\u7136\u540e\u901a\u8fc7\u8bbe\u8ba1\u7684\u56fe\u751f\u6210\u6d41\u7a0b\u8fdb\u884c\u56fe\u7684\u751f\u6210\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u7f51\u7edc\u4e0a\uff0cGraphia\u5728\u5fae\u89c2\u5c42\u9762\uff08\u76ee\u6807\u9009\u62e9\u5f97\u5206\u3001\u8fb9\u5206\u7c7b\u51c6\u786e\u7387\u3001\u8fb9\u5185\u5bb9BERTScore\uff09\u548c\u5b8f\u89c2\u5c42\u9762\uff08\u7ed3\u6784\u76f8\u4f3c\u6027\u3001\u5e42\u5f8b\u548c\u56de\u58f0\u5ba4\u7b49\u793e\u4f1a\u73b0\u8c61\u590d\u5236\uff09\u5747\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u652f\u6301\u53cd\u4e8b\u5b9e\u6a21\u62df\u3002", "conclusion": "\u793e\u4ea4\u56fe\u53ef\u4ee5\u4f5c\u4e3aLLM\u540e\u8bad\u7ec3\u7684\u9ad8\u8d28\u91cf\u76d1\u7763\u4fe1\u53f7\uff0c\u7f29\u5c0f\u4e86Agent\u884c\u4e3a\u4e0e\u7f51\u7edc\u52a8\u529b\u5b66\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5b9e\u73b0\u4e86\u66f4\u771f\u5b9e\u7684LLM\u793e\u4ea4\u56fe\u6a21\u62df\u3002"}}
{"id": "2510.24459", "categories": ["cs.AI", "cs.MA", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.24459", "abs": "https://arxiv.org/abs/2510.24459", "authors": ["Habtom Kahsay Gidey", "Niklas Huber", "Alexander Lenz", "Alois Knoll"], "title": "Affordance Representation and Recognition for Autonomous Agents", "comment": null, "summary": "The autonomy of software agents is fundamentally dependent on their ability\nto construct an actionable internal world model from the structured data that\ndefines their digital environment, such as the Document Object Model (DOM) of\nweb pages and the semantic descriptions of web services. However, constructing\nthis world model from raw structured data presents two critical challenges: the\nverbosity of raw HTML makes it computationally intractable for direct use by\nfoundation models, while the static nature of hardcoded API integrations\nprevents agents from adapting to evolving services.\n  This paper introduces a pattern language for world modeling from structured\ndata, presenting two complementary architectural patterns. The DOM Transduction\nPattern addresses the challenge of web page complexity by distilling} a\nverbose, raw DOM into a compact, task-relevant representation or world model\noptimized for an agent's reasoning core. Concurrently, the Hypermedia\nAffordances Recognition Pattern enables the agent to dynamically enrich its\nworld model by parsing standardized semantic descriptions to discover and\nintegrate the capabilities of unknown web services at runtime. Together, these\npatterns provide a robust framework for engineering agents that can efficiently\nconstruct and maintain an accurate world model, enabling scalable, adaptive,\nand interoperable automation across the web and its extended resources.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u7ed3\u6784\u5316\u6570\u636e\u6784\u5efa\u4e16\u754c\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u4ee5\u5e94\u5bf9 HTML \u7684\u5197\u957f\u548c Web \u670d\u52a1\u7684\u9759\u6001\u96c6\u6210\u95ee\u9898\u3002", "motivation": "\u8f6f\u4ef6\u4ee3\u7406\u7684\u81ea\u4e3b\u6027\u4f9d\u8d56\u4e8e\u5b83\u4eec\u4ece\u6570\u5b57\u73af\u5883\uff08\u5982\u7f51\u9875\u7684 DOM \u548c Web \u670d\u52a1\u7684\u8bed\u4e49\u63cf\u8ff0\uff09\u7684\u7ed3\u6784\u5316\u6570\u636e\u6784\u5efa\u53ef\u64cd\u4f5c\u7684\u5185\u90e8\u4e16\u754c\u6a21\u578b\u7684\u80fd\u529b\u3002\u7136\u800c\uff0c\u4ece\u539f\u59cb\u7ed3\u6784\u5316\u6570\u636e\u6784\u5efa\u6b64\u4e16\u754c\u6a21\u578b\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a\u539f\u59cb HTML \u7684\u5197\u957f\u4f7f\u5176\u96be\u4ee5\u88ab\u57fa\u7840\u6a21\u578b\u76f4\u63a5\u4f7f\u7528\uff0c\u800c\u786c\u7f16\u7801\u7684 API \u96c6\u6210\u7684\u9759\u6001\u6027\u8d28\u963b\u788d\u4e86\u4ee3\u7406\u9002\u5e94\u4e0d\u65ad\u53d1\u5c55\u7684\u670d\u52a1\u3002", "method": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u7528\u4e8e\u4ece\u7ed3\u6784\u5316\u6570\u636e\u6784\u5efa\u4e16\u754c\u6a21\u578b\u7684\u6a21\u5f0f\u8bed\u8a00\uff0c\u5e76\u63d0\u51fa\u4e86\u4e24\u79cd\u4e92\u8865\u7684\u67b6\u6784\u6a21\u5f0f\u3002DOM \u8f6c\u6362\u6a21\u5f0f\u901a\u8fc7\u5c06\u5197\u957f\u7684\u539f\u59cb DOM \u63d0\u70bc\u6210\u9488\u5bf9\u4ee3\u7406\u63a8\u7406\u6838\u5fc3\u8fdb\u884c\u4f18\u5316\u7684\u7d27\u51d1\u3001\u4e0e\u4efb\u52a1\u76f8\u5173\u7684\u8868\u793a\u6216\u4e16\u754c\u6a21\u578b\u6765\u89e3\u51b3\u7f51\u9875\u590d\u6742\u6027\u7684\u6311\u6218\u3002\u540c\u65f6\uff0c\u8d85\u5a92\u4f53\u53ef\u7528\u6027\u8bc6\u522b\u6a21\u5f0f\u4f7f\u4ee3\u7406\u80fd\u591f\u901a\u8fc7\u89e3\u6790\u6807\u51c6\u5316\u7684\u8bed\u4e49\u63cf\u8ff0\u6765\u52a8\u6001\u4e30\u5bcc\u5176\u4e16\u754c\u6a21\u578b\uff0c\u4ece\u800c\u5728\u8fd0\u884c\u65f6\u53d1\u73b0\u548c\u96c6\u6210\u672a\u77e5 Web \u670d\u52a1\u7684.\u80fd\u529b\u3002", "result": "\u8fd9\u4e24\u79cd\u6a21\u5f0f\u5171\u540c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u6784\u5efa\u80fd\u591f\u6709\u6548\u6784\u5efa\u548c\u7ef4\u62a4\u51c6\u786e\u4e16\u754c\u6a21\u578b\u7684\u4ee3\u7406\uff0c\u4ece\u800c\u5b9e\u73b0\u53ef\u6269\u5c55\u3001\u81ea\u9002\u5e94\u548c\u53ef\u4e92\u64cd\u4f5c\u7684 Web \u53ca\u5176\u6269\u5c55\u8d44\u6e90\u7684\u81ea\u52a8\u5316\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6a21\u5f0f\u8bed\u8a00\u548c\u4e24\u79cd\u67b6\u6784\u6a21\u5f0f\uff08DOM \u8f6c\u6362\u6a21\u5f0f\u548c\u8d85\u5a92\u4f53\u53ef\u7528\u6027\u8bc6\u522b\u6a21\u5f0f\uff09\u4e3a\u4ece\u7ed3\u6784\u5316\u6570\u636e\u6784\u5efa\u4e16\u754c\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u63d0\u9ad8\u4e86\u4ee3\u7406\u7684\u9002\u5e94\u6027\u548c\u4e92\u64cd\u4f5c\u6027\u3002"}}
{"id": "2510.23854", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23854", "abs": "https://arxiv.org/abs/2510.23854", "authors": ["Jyotika Singh", "Weiyi Sun", "Amit Agarwal", "Viji Krishnamurthy", "Yassine Benajiba", "Sujith Ravi", "Dan Roth"], "title": "Can LLMs Narrate Tabular Data? An Evaluation Framework for Natural Language Representations of Text-to-SQL System Outputs", "comment": "Accepted at EMNLP 2025", "summary": "In modern industry systems like multi-turn chat agents, Text-to-SQL\ntechnology bridges natural language (NL) questions and database (DB) querying.\nThe conversion of tabular DB results into NL representations (NLRs) enables the\nchat-based interaction. Currently, NLR generation is typically handled by large\nlanguage models (LLMs), but information loss or errors in presenting tabular\nresults in NL remains largely unexplored. This paper introduces a novel\nevaluation method - Combo-Eval - for judgment of LLM-generated NLRs that\ncombines the benefits of multiple existing methods, optimizing evaluation\nfidelity and achieving a significant reduction in LLM calls by 25-61%.\nAccompanying our method is NLR-BIRD, the first dedicated dataset for NLR\nbenchmarking. Through human evaluations, we demonstrate the superior alignment\nof Combo-Eval with human judgments, applicable across scenarios with and\nwithout ground truth references.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a Combo-Eval \u7684\u65b0\u9896\u8bc4\u4f30\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u81ea\u7136\u8bed\u8a00\u8868\u793a\uff08NLR\uff09\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u591a\u79cd\u73b0\u6709\u65b9\u6cd5\u7684\u4f18\u70b9\uff0c\u63d0\u9ad8\u4e86\u8bc4\u4f30\u7684\u51c6\u786e\u6027\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u5bf9 LLM \u7684\u8c03\u7528\u6b21\u6570\uff08\u51cf\u5c11 25-61%\uff09\u3002\u6b64\u5916\uff0c\u8fd8\u53d1\u5e03\u4e86 NLR-BIRD \u6570\u636e\u96c6\uff0c\u8fd9\u662f\u9996\u4e2a\u4e13\u95e8\u7528\u4e8e NLR \u57fa\u51c6\u6d4b\u8bd5\u7684\u6570\u636e\u96c6\u3002\u4eba\u7c7b\u8bc4\u4f30\u8868\u660e\uff0cCombo-Eval \u4e0e\u4eba\u7c7b\u5224\u65ad\u9ad8\u5ea6\u4e00\u81f4\uff0c\u9002\u7528\u4e8e\u6709\u65e0\u771f\u5b9e\u53c2\u8003\u4e24\u79cd\u573a\u666f\u3002", "motivation": "\u76ee\u524d\uff0c\u867d\u7136\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u88ab\u5e7f\u6cdb\u7528\u4e8e\u5c06\u6570\u636e\u5e93\uff08DB\uff09\u7684\u67e5\u8be2\u7ed3\u679c\u8f6c\u6362\u4e3a\u81ea\u7136\u8bed\u8a00\u8868\u793a\uff08NLR\uff09\uff0c\u4f46\u4fe1\u606f\u4e22\u5931\u6216\u9519\u8bef\u7684\u95ee\u9898\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u9ad8 NLR \u751f\u6210\u7684\u8d28\u91cf\u548c\u8bc4\u4f30\u7684\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a Combo-Eval \u7684\u65b0\u9896\u8bc4\u4f30\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u591a\u79cd\u73b0\u6709\u65b9\u6cd5\u7684\u4f18\u70b9\uff0c\u4ee5\u63d0\u9ad8\u8bc4\u4f30\u7684\u51c6\u786e\u6027\u5e76\u51cf\u5c11\u5bf9 LLM \u7684\u8c03\u7528\u3002\u540c\u65f6\uff0c\u521b\u5efa\u4e86 NLR-BIRD \u6570\u636e\u96c6\uff0c\u8fd9\u662f\u9996\u4e2a\u4e13\u95e8\u7528\u4e8e NLR \u57fa\u51c6\u6d4b\u8bd5\u7684\u6570\u636e\u96c6\u3002", "result": "Combo-Eval \u8bc4\u4f30\u65b9\u6cd5\u5728\u51cf\u5c11 LLM \u8c03\u7528\u6b21\u6570\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6210\u6548\uff0825-61%\uff09\u3002\u901a\u8fc7\u4eba\u7c7b\u8bc4\u4f30\u8bc1\u660e\uff0cCombo-Eval \u4e0e\u4eba\u7c7b\u5224\u65ad\u9ad8\u5ea6\u4e00\u81f4\uff0c\u5e76\u4e14\u9002\u7528\u4e8e\u6709\u65e0\u771f\u5b9e\u53c2\u8003\u7684\u573a\u666f\u3002", "conclusion": "Combo-Eval \u662f\u4e00\u79cd\u6709\u6548\u4e14\u9ad8\u6548\u7684 NLR \u8bc4\u4f30\u65b9\u6cd5\uff0c\u80fd\u591f\u66f4\u597d\u5730\u4e0e\u4eba\u7c7b\u5224\u65ad\u4fdd\u6301\u4e00\u81f4\uff0c\u5e76\u4e14 NLR-BIRD \u6570\u636e\u96c6\u4e3a NLR \u7684\u57fa\u51c6\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u8d44\u6e90\u3002"}}
{"id": "2510.23988", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.23988", "abs": "https://arxiv.org/abs/2510.23988", "authors": ["Phuc Nguyen Xuan", "Thanh Nguyen Canh", "Huu-Hung Nguyen", "Nak Young Chong", "Xiem HoangVan"], "title": "A Survey on Collaborative SLAM with 3D Gaussian Splatting", "comment": null, "summary": "This survey comprehensively reviews the evolving field of multi-robot\ncollaborative Simultaneous Localization and Mapping (SLAM) using 3D Gaussian\nSplatting (3DGS). As an explicit scene representation, 3DGS has enabled\nunprecedented real-time, high-fidelity rendering, ideal for robotics. However,\nits use in multi-robot systems introduces significant challenges in maintaining\nglobal consistency, managing communication, and fusing data from heterogeneous\nsources. We systematically categorize approaches by their architecture --\ncentralized, distributed -- and analyze core components like multi-agent\nconsistency and alignment, communication-efficient, Gaussian representation,\nsemantic distillation, fusion and pose optimization, and real-time scalability.\nIn addition, a summary of critical datasets and evaluation metrics is provided\nto contextualize performance. Finally, we identify key open challenges and\nchart future research directions, including lifelong mapping, semantic\nassociation and mapping, multi-model for robustness, and bridging the Sim2Real\ngap.", "AI": {"tldr": "\u8be5\u8c03\u67e5\u5168\u9762\u56de\u987e\u4e86\u4f7f\u7528 3D \u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u7684\u591a\u673a\u5668\u4eba\u534f\u540c\u540c\u6b65\u5b9a\u4f4d\u4e0e\u5730\u56fe\u6784\u5efa\uff08SLAM\uff09\u9886\u57df\u3002", "motivation": "3DGS \u4f5c\u4e3a\u4e00\u79cd\u663e\u5f0f\u573a\u666f\u8868\u793a\uff0c\u5b9e\u73b0\u4e86\u524d\u6240\u672a\u6709\u3001\u5b9e\u65f6\u3001\u9ad8\u4fdd\u771f\u7684\u6e32\u67d3\uff0c\u975e\u5e38\u9002\u5408\u673a\u5668\u4eba\u6280\u672f\u3002\u7136\u800c\uff0c\u5728\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u4f7f\u7528 3DGS \u4f1a\u5728\u4fdd\u6301\u5168\u5c40\u4e00\u81f4\u6027\u3001\u901a\u4fe1\u7ba1\u7406\u548c\u878d\u5408\u5f02\u6784\u6570\u636e\u65b9\u9762\u5e26\u6765\u91cd\u5927\u6311\u6218\u3002", "method": "\u7cfb\u7edf\u5730\u5c06\u65b9\u6cd5\u6309\u67b6\u6784\uff08\u96c6\u4e2d\u5f0f\u3001\u5206\u5e03\u5f0f\uff09\u8fdb\u884c\u5206\u7c7b\uff0c\u5e76\u5206\u6790\u4e86\u591a\u667a\u80fd\u4f53\u4e00\u81f4\u6027\u4e0e\u5bf9\u9f50\u3001\u901a\u4fe1\u6548\u7387\u3001\u9ad8\u65af\u8868\u793a\u3001\u8bed\u4e49\u84b8\u998f\u3001\u878d\u5408\u4e0e\u59ff\u6001\u4f18\u5316\u4ee5\u53ca\u5b9e\u65f6\u53ef\u6269\u5c55\u6027\u7b49\u6838\u5fc3\u7ec4\u4ef6\u3002\u6b64\u5916\uff0c\u8fd8\u603b\u7ed3\u4e86\u5173\u952e\u7684\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u5bf9\u591a\u673a\u5668\u4eba\u534f\u540c SLAM \u7684\u5404\u79cd\u65b9\u6cd5\u8fdb\u884c\u4e86\u5206\u6790\uff0c\u5e76\u8003\u8651\u4e86 3DGS \u7684\u6027\u80fd\u3002", "conclusion": "\u786e\u5b9a\u4e86\u5173\u952e\u7684\u5f00\u653e\u6027\u6311\u6218\uff0c\u5e76\u89c4\u5212\u4e86\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\uff0c\u5305\u62ec\u7ec8\u8eab\u6620\u5c04\u3001\u8bed\u4e49\u5173\u8054\u4e0e\u6620\u5c04\u3001\u591a\u6a21\u578b\u9c81\u68d2\u6027\u4ee5\u53ca\u5f25\u5408\u4eff\u771f\u5230\u771f\u5b9e\uff08Sim2Real\uff09\u7684\u5dee\u8ddd\u3002"}}
{"id": "2510.24200", "categories": ["cs.LG", "cs.CR", "cs.DC", "I.2.11"], "pdf": "https://arxiv.org/pdf/2510.24200", "abs": "https://arxiv.org/abs/2510.24200", "authors": ["Alexander Bakarsky", "Dimitar I. Dimitrov", "Maximilian Baader", "Martin Vechev"], "title": "SPEAR++: Scaling Gradient Inversion via Sparsely-Used Dictionary Learning", "comment": "Published at the Workshop on Regulatable ML at the 39th Conference on\n  Neural Information Processing Systems (NeurIPS 2025)", "summary": "Federated Learning has seen an increased deployment in real-world scenarios\nrecently, as it enables the distributed training of machine learning models\nwithout explicit data sharing between individual clients. Yet, the introduction\nof the so-called gradient inversion attacks has fundamentally challenged its\nprivacy-preserving properties. Unfortunately, as these attacks mostly rely on\ndirect data optimization without any formal guarantees, the vulnerability of\nreal-world systems remains in dispute and requires tedious testing for each new\nfederated deployment. To overcome these issues, recently the SPEAR attack was\nintroduced, which is based on a theoretical analysis of the gradients of linear\nlayers with ReLU activations. While SPEAR is an important theoretical\nbreakthrough, the attack's practicality was severely limited by its exponential\nruntime in the batch size b. In this work, we fill this gap by applying\nState-of-the-Art techniques from Sparsely-Used Dictionary Learning to make the\nproblem of gradient inversion on linear layers with ReLU activations tractable.\nOur experiments demonstrate that our new attack, SPEAR++, retains all desirable\nproperties of SPEAR, such as robustness to DP noise and FedAvg aggregation,\nwhile being applicable to 10x bigger batch sizes.", "AI": {"tldr": "SPEAR++ \u662f\u4e00\u79cd\u57fa\u4e8e\u5b57\u5178\u5b66\u4e60\u7684\u68af\u5ea6\u53cd\u6f14\u653b\u51fb\uff0c\u89e3\u51b3\u4e86 SPEAR \u653b\u51fb\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u56e0\u6279\u5904\u7406\u5927\u5c0f\u9650\u5236\u800c\u65e0\u6cd5\u6269\u5c55\u7684\u95ee\u9898\u3002", "motivation": "\u68af\u5ea6\u53cd\u6f14\u653b\u51fb\u5bf9\u8054\u90a6\u5b66\u4e60\u7684\u9690\u79c1\u4fdd\u62a4\u63d0\u51fa\u4e86\u6311\u6218\uff0c\u4f46\u73b0\u6709\u653b\u51fb\u7f3a\u4e4f\u7406\u8bba\u4fdd\u8bc1\u4e14\u96be\u4ee5\u5e94\u7528\u4e8e\u5b9e\u9645\u90e8\u7f72\u3002SPEAR \u653b\u51fb\u867d\u7136\u6709\u7406\u8bba\u7a81\u7834\uff0c\u4f46\u8fd0\u884c\u65f6\u95f4\u4e0e\u6279\u5904\u7406\u5927\u5c0f\u5448\u6307\u6570\u5173\u7cfb\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u7528\u6027\u3002", "method": "\u5229\u7528\u5b57\u5178\u5b66\u4e60\u6280\u672f\u6765\u89e3\u51b3\u7ebf\u6027\u5c42 ReLU \u6fc0\u6d3b\u7684\u68af\u5ea6\u53cd\u6f14\u95ee\u9898\uff0c\u4f7f\u653b\u51fb\u5728\u5b9e\u8df5\u4e2d\u66f4\u5177\u53ef\u884c\u6027\u3002", "result": "SPEAR++ \u653b\u51fb\u5728\u4fdd\u6301 SPEAR \u653b\u51fb\u7684\u4f18\u52bf\uff08\u5982\u5bf9\u5dee\u5206\u9690\u79c1\u566a\u58f0\u548c FedAvg \u805a\u5408\u7684\u9c81\u68d2\u6027\uff09\u7684\u540c\u65f6\uff0c\u80fd\u591f\u5904\u7406\u7684\u6279\u5904\u7406\u5927\u5c0f\u662f SPEAR \u653b\u51fb\u7684 10 \u500d\u3002", "conclusion": "SPEAR++ \u653b\u51fb\u901a\u8fc7\u5f15\u5165\u5b57\u5178\u5b66\u4e60\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u68af\u5ea6\u53cd\u6f14\u653b\u51fb\u7684\u6548\u7387\u95ee\u9898\uff0c\u4f7f\u5176\u5728\u5b9e\u8df5\u4e2d\u66f4\u5177\u5a01\u80c1\u6027\uff0c\u80fd\u591f\u5904\u7406\u66f4\u5927\u7684\u6279\u5904\u7406\u89c4\u6a21\u3002"}}
{"id": "2510.24147", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2510.24147", "abs": "https://arxiv.org/abs/2510.24147", "authors": ["Manivannan Saminathan", "Prakash Govindaraj", "Hern Kim", "Kowsalya Murugan", "Kathirvel Venugopal"], "title": "Interplay between Cu diffusion and bonding anisotropy on the thermoelectric performance of double cation chalcohalides $CuBiSeX_{2} (X = Cl, Br)$", "comment": "31 Pages, 9 Figures", "summary": "Double cation chalcohalide have recently been emerged as the interesting\ncandidates for sustainable energy conversion applications, owing to their\nintrinsic chemical tunability, suitable band gap, and low thermal conductivity.\nWith this motivation, the current study is designed to explore the structural,\nelectron and phonon transport mechanism, and thermoelectric properties of\n$CuBiSeX_{2} (X = Cl, Br)$ through density functional theory-based\ncomputations. The experimental feasibility of the compounds is ensured, and\nthey are predicted to be thermally, dynamically, and mechanically stable. The\ndistinct structural attributes coupled with suitable electronic band structure\npromotes the electron transport properties. Comprehensively, the delocalized Cu\natom enhancing the phonon scattering process and the off-centred displacement\nof cations leading to bonding anharmonicity results ultra-low lattice thermal\nconductivity $(\\kappa_L)$. Among these systems, $CuBiSeCl_2$ exhibits low\n$\\kappa_L$ (0.24 $W m^{-1} K^{-1}$ at 300 K) and superior thermoelectric\nperformance (zT = 1.18 at 600 K), whereas $CuBiSeBr_2$ ($\\kappa_L$ = 0.65 $W\nm^{-1} K^{-1}$ at 300 K, zT = 0.68 at 600 K) demands further optimization.\nOverall, the study sheds light into the interplay between the Cu diffusion and\nbonding anisotropy in phonon propagation and establishes the potential of\ndouble-cation chalcohalides for mid-temperature thermoelectric applications.", "AI": {"tldr": "CuBiSeX\u2082(X=Cl,Br)\u5177\u6709\u6f5c\u5728\u7684\u06a9\u0647\u0631\u0628\u0627\u5e94\u7528\u524d\u666f\uff0c\u5176\u4e2dCuBiSeCl\u2082\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u6027\u80fd\u3002", "motivation": "\u63a2\u7d22CuBiSeX\u2082(X=Cl,Br)\u7684\u7ed3\u6784\u3001\u7535\u5b50\u548c\u58f0\u5b50\u8f93\u8fd0\u673a\u5236\u53ca\u5176\u06a9\u0647\u0631\u0628\u0627\u6027\u80fd\u3002", "method": "\u57fa\u4e8e\u5bc6\u5ea6\u6cdb\u51fd\u7406\u8bba\u7684\u8ba1\u7b97\u3002", "result": "CuBiSeCl\u2082\u548cCuBiSeBr\u2082\u5728\u70ed\u529b\u5b66\u3001\u52a8\u529b\u5b66\u548c\u673a\u68b0\u4e0a\u662f\u7a33\u5b9a\u7684\u3002CuBiSeCl\u2082\u5728600K\u65f6\u5177\u6709zT=1.18\uff0c\u800cCuBiSeBr\u2082\u5728600K\u65f6\u5177\u6709zT=0.68\u3002", "conclusion": "CuBiSeX\u2082\u662f\u4e00\u79cd\u6709\u524d\u9014\u7684\u06a9\u0647\u0631\u0628\u0627\u6750\u6599\uff0c\u5176\u4e2dCuBiSeCl\u2082\u5728\u4e2d\u6e29\u4e0b\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u6027\u80fd\uff0c\u8fd9\u5f52\u56e0\u4e8e\u5176\u72ec\u7279\u7684\u7ed3\u6784\u7279\u6027\u548c\u58f0\u5b50\u6563\u5c04\u673a\u5236\u3002"}}
{"id": "2510.24294", "categories": ["cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2510.24294", "abs": "https://arxiv.org/abs/2510.24294", "authors": ["Carlos Caro", "Francisco Gamez"], "title": "Phase-Rotated Altermagnets as Chern Valves for Topological Transport", "comment": "13 pages, 1 Figure, 1 TOC", "summary": "Motivated by the emerging control of Berry-curvature textures in\naltermagnets, we explore a two-terminal configuration where a\ntopological-insulator film is interfaced with two altermagnetic electrodes\nwhose crystalline phases can be rotated independently. The proximity coupling\nimprints each momentum-dependent of the altermagnet spin texture onto the Dirac\nsurface states, giving rise to an angular mass whose sign follows the lattice\norientation. Adjusting the phase of one electrode redefines this mass pattern,\nthereby tuning the number and spatial distribution of chiral edge channels.\nThis results in discrete conductance steps and a reversible inversion of the\nthermoelectric coefficient-achieved without external magnetic fields or net\nmagnetization. A compact Dirac model captures both the quantized switching and\nits resilience to moderate disorder. Overall, this symmetry-driven mechanism\nprovides a practical and low-dissipation route to programmable topological\ntransport via lattice rotation.", "AI": {"tldr": "\u901a\u8fc7\u65cb\u8f6c\u4e0d\u540c\u76f8\u4f4d\u7684\u53cd\u94c1\u78c1\u7535\u6781\u6765\u8c03\u63a7\u62d3\u6251\u7edd\u7f18\u4f53\u8584\u819c\u4e2d\u7684\u72c4\u62c9\u514b\u8868\u9762\u6001\uff0c\u5b9e\u73b0\u65e0\u9700\u5916\u52a0\u78c1\u573a\u5373\u53ef\u5b9e\u73b0\u53ef\u7f16\u7a0b\u7684\u62d3\u6251\u8f93\u8fd0\u3002", "motivation": "\u5229\u7528\u65b0\u5174\u7684Berry\u66f2\u7387\u7eb9\u7406\u8c03\u63a7\u53cd\u94c1\u78c1\u4f53\uff0c\u63a2\u7d22\u5728\u53cc\u7aef\u914d\u7f6e\u4e0b\u8c03\u63a7\u62d3\u6251\u7edd\u7f18\u4f53\u8584\u819c\u7684\u72c4\u62c9\u514b\u8868\u9762\u6001\u3002", "method": "\u5c06\u62d3\u6251\u7edd\u7f18\u4f53\u8584\u819c\u4e0e\u4e24\u4e2a\u53cd\u94c1\u78c1\u7535\u6781\u76f8\u63a5\u89e6\uff0c\u901a\u8fc7\u65cb\u8f6c\u7535\u6781\u7684\u6676\u4f53\u76f8\u4f4d\u6765\u8c03\u63a7\u8fd1\u90bb\u8026\u5408\u4ea7\u751f\u7684\u89d2\u8d28\u91cf\uff0c\u8fdb\u800c\u6539\u53d8\u624b\u5f81\u8fb9\u7f18\u901a\u9053\u7684\u6570\u91cf\u548c\u5206\u5e03\u3002", "result": "\u89c2\u5bdf\u5230\u79bb\u6563\u7684\u7535\u5bfc\u53f0\u9636\u548c\u53ef\u9006\u7684\u70ed\u7535\u7cfb\u6570\u53cd\u8f6c\uff0c\u4e14\u6b64\u73b0\u8c61\u4e0d\u4f9d\u8d56\u4e8e\u5916\u52a0\u78c1\u573a\u6216\u51c0\u78c1\u5316\u5f3a\u5ea6\u3002\u6784\u5efa\u7684\u7d27\u51d1\u72c4\u62c9\u514b\u6a21\u578b\u80fd\u591f\u89e3\u91ca\u91cf\u5316\u5f00\u5173\u884c\u4e3a\u53ca\u5176\u5bf9\u9002\u5ea6\u65e0\u5e8f\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u5bf9\u79f0\u6027\u9a71\u52a8\u673a\u5236\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u4e14\u4f4e\u8017\u635f\u7684\u9014\u5f84\uff0c\u901a\u8fc7\u6676\u683c\u65cb\u8f6c\u5b9e\u73b0\u53ef\u7f16\u7a0b\u7684\u62d3\u6251\u8f93\u8fd0\u3002"}}
{"id": "2510.23907", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23907", "abs": "https://arxiv.org/abs/2510.23907", "authors": ["Eddison Pham", "Prisha Priyadarshini", "Adrian Maliackel", "Kanishk Bandi", "Cristian Meo", "Kevin Zhu"], "title": "DynaStride: Dynamic Stride Windowing with MMCoT for Instructional Multi-Scene Captioning", "comment": "16 pages, 15 figures, 5 Tables, submitted to AAAI AI4ED Workshop 2026", "summary": "Scene-level captioning in instructional videos can enhance learning by\nrequiring an understanding of both visual cues and temporal structure. By\naligning visual cues with textual guidance, this understanding supports\nprocedural learning and multimodal reasoning, providing a richer context for\nskill acquisition. However, captions that fail to capture this structure may\nlack coherence and quality, which can create confusion and undermine the\nvideo's educational intent. To address this gap, we introduce DynaStride, a\npipeline to generate coherent, scene-level captions without requiring manual\nscene segmentation. Using the YouCookII dataset's scene annotations, DynaStride\nperforms adaptive frame sampling and multimodal windowing to capture key\ntransitions within each scene. It then employs a multimodal chain-of-thought\nprocess to produce multiple action-object pairs, which are refined and fused\nusing a dynamic stride window selection algorithm that adaptively balances\ntemporal context and redundancy. The final scene-level caption integrates\nvisual semantics and temporal reasoning in a single instructional caption.\nEmpirical evaluations against strong baselines, including VLLaMA3 and GPT-4o,\ndemonstrate consistent gains on both N-gram-based metrics (BLEU, METEOR) and\nsemantic similarity measures (BERTScore, CLIPScore). Qualitative analyses\nfurther show that DynaStride produces captions that are more temporally\ncoherent and informative, suggesting a promising direction for improving\nAI-powered instructional content generation.", "AI": {"tldr": "DynaStride\u662f\u4e00\u4e2a\u7528\u4e8e\u751f\u6210\u8fde\u8d2f\u7684\u3001\u573a\u666f\u7ea7\u5b57\u5e55\u7684\u7ba1\u9053\uff0c\u65e0\u9700\u624b\u52a8\u573a\u666f\u5206\u5272\u3002\u5b83\u901a\u8fc7\u81ea\u9002\u5e94\u5e27\u91c7\u6837\u3001\u591a\u6a21\u6001\u7a97\u53e3\u548c\u94fe\u5f0f\u601d\u8003\u8fc7\u7a0b\u6765\u751f\u6210\u548c\u4f18\u5316\u5b57\u5e55\uff0c\u5e76\u5728YouCookII\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u4e0e\u57fa\u7ebf\u76f8\u6bd4\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u573a\u666f\u7ea7\u5b57\u5e55\u751f\u6210\u65b9\u6cd5\u5728\u5904\u7406\u89c6\u89c9\u7ebf\u7d22\u548c\u65f6\u95f4\u7ed3\u6784\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u53ef\u80fd\u5bfc\u81f4\u5b57\u5e55\u7f3a\u4e4f\u8fde\u8d2f\u6027\u548c\u8d28\u91cf\uff0c\u4ece\u800c\u5f71\u54cd\u6559\u5b66\u6548\u679c\u3002", "method": "DynaStride\u91c7\u7528\u81ea\u9002\u5e94\u5e27\u91c7\u6837\u548c\u591a\u6a21\u6001\u7a97\u53e3\u6765\u6355\u6349\u573a\u666f\u5185\u7684\u5173\u952e\u8f6c\u6298\u70b9\u3002\u7136\u540e\uff0c\u5b83\u5229\u7528\u591a\u6a21\u6001\u94fe\u5f0f\u601d\u8003\u8fc7\u7a0b\u751f\u6210\u52a8\u4f5c-\u5bf9\u8c61\u5bf9\uff0c\u5e76\u901a\u8fc7\u52a8\u6001\u6b65\u5e45\u7a97\u53e3\u9009\u62e9\u7b97\u6cd5\u8fdb\u884c\u4f18\u5316\u548c\u878d\u5408\uff0c\u4ee5\u5e73\u8861\u65f6\u95f4\u4e0a\u4e0b\u6587\u548c\u5197\u4f59\u5ea6\u3002\u6700\u540e\uff0c\u5c06\u89c6\u89c9\u8bed\u4e49\u548c\u65f6\u95f4\u63a8\u7406\u6574\u5408\u5230\u6700\u7ec8\u7684\u573a\u666f\u7ea7\u5b57\u5e55\u4e2d\u3002", "result": "\u5728YouCookII\u6570\u636e\u96c6\u4e0a\uff0cDynaStride\u5728N-gram\u6307\u6807\uff08BLEU\u3001METEOR\uff09\u548c\u8bed\u4e49\u76f8\u4f3c\u5ea6\u6307\u6807\uff08BERTScore\u3001CLIPScore\uff09\u4e0a\u5747\u53d6\u5f97\u4e86\u663e\u8457\u7684\u63d0\u5347\uff0c\u4f18\u4e8eVLLaMA3\u548cGPT-4o\u7b49\u57fa\u7ebf\u6a21\u578b\u3002\u5b9a\u6027\u5206\u6790\u8868\u660e\uff0cDynaStride\u751f\u6210\u7684\u5b57\u5e55\u5728\u65f6\u95f4\u8fde\u8d2f\u6027\u548c\u4fe1\u606f\u91cf\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "DynaStride\u6210\u529f\u5730\u751f\u6210\u4e86\u8fde\u8d2f\u4e14\u4fe1\u606f\u4e30\u5bcc\u7684\u573a\u666f\u7ea7\u5b57\u5e55\uff0c\u65e0\u9700\u624b\u52a8\u573a\u666f\u5206\u5272\uff0c\u4e3a\u6539\u8fdbAI\u9a71\u52a8\u7684\u6559\u5b66\u5185\u5bb9\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\u3002"}}
{"id": "2510.23796", "categories": ["quant-ph", "cond-mat.mes-hall", "physics.optics"], "pdf": "https://arxiv.org/pdf/2510.23796", "abs": "https://arxiv.org/abs/2510.23796", "authors": ["A. Zecchetto", "J. -R. Coudevylle", "M. Morassi", "A. Lema\u00eetre", "M. I. Amanti", "S. Ducci", "F. Baboux"], "title": "Topological protection of photon-pair generation in nonlinear waveguide arrays", "comment": null, "summary": "Harnessing topological effects offers a promising route to protect quantum\nstates of light from imperfections, potentially enabling more robust platforms\nfor quantum information processing. This capability is particularly relevant\nfor active photonic circuits that generate quantum light directly on-chip.\nHere, we explore topological effects on photon-pair generation via spontaneous\nparametric down-conversion (SPDC) in nonlinear waveguide arrays, both\ntheoretically and experimentally. A systematic comparison of homogeneous,\ntrivial, and topological Su-Schrieffer-Heeger arrays reveals that only the\ntopological configuration preserves a stable SPDC resonance spectrum under\ndisorder in the tunnel couplings, with fluctuations in the resonance position\nreduced by more than one order of magnitude. An analytical model supports our\nexperimental observations by linking this robustness to the band-structure\nproperties of the interacting modes. These findings establish quadratic\nnonlinear waveguide arrays as a promising platform to explore the interplay of\nnonlinearity, topology, and disorder in quantum photonic circuits.", "AI": {"tldr": "\u6587\u7ae0\u5229\u7528\u975e\u7ebf\u6027\u6ce2\u5bfc\u9635\u5217\u4e2d\u7684\u81ea\u53d1\u53c2\u91cf\u4e0b\u8f6c\u6362\uff08SPDC\uff09\u6765\u63a2\u7d22\u62d3\u6251\u6548\u5e94\u5bf9\u5149\u5b50\u5bf9\u751f\u6210\u7684\u5f71\u54cd\uff0c\u8bc1\u660e\u4e86\u62d3\u6251\u7ed3\u6784\u5728\u6291\u5236\u7531\u4e0d\u89c4\u5219\u6027\u5f15\u8d77\u7684\u4e0d\u786e\u5b9a\u6027\u65b9\u9762\u5177\u6709\u4f18\u52bf\uff0c\u53ef\u7528\u4e8e\u6784\u5efa\u66f4\u7a33\u5b9a\u7684\u91cf\u5b50\u4fe1\u606f\u5904\u7406\u5e73\u53f0\u3002", "motivation": "\u5229\u7528\u62d3\u6251\u6548\u5e94\u4fdd\u62a4\u91cf\u5b50\u6001\u514d\u53d7\u566a\u58f0\u5e72\u6270\uff0c\u7279\u522b\u662f\u5728\u4e3b\u52a8\u5149\u5b50\u7535\u8def\u4e2d\uff0c\u4ee5\u5b9e\u73b0\u66f4\u9c81\u68d2\u7684\u91cf\u5b50\u4fe1\u606f\u5904\u7406\u3002", "method": "\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u63a2\u7d22\u4e86\u5728\u975e\u7ebf\u6027\u6ce2\u5bfc\u9635\u5217\uff08\u5305\u62ec\u5747\u5300\u3001\u5e73\u51e1\u548c\u62d3\u6251SSu-Schrieffer-Heeger\u9635\u5217\uff09\u4e2d\u901a\u8fc7SPDC\u4ea7\u751f\u5149\u5b50\u5bf9\u7684\u62d3\u6251\u6548\u5e94\u3002", "result": "\u62d3\u6251\u914d\u7f6e\u80fd\u591f\u4fdd\u6301\u7a33\u5b9a\u7684SPDC\u5171\u632f\u5149\u8c31\uff0c\u5373\u4f7f\u5728\u4f20\u8f93\u8026\u5408\u4e0d\u89c4\u5219\u7684\u60c5\u51b5\u4e0b\uff0c\u5171\u632f\u4f4d\u7f6e\u7684\u6ce2\u52a8\u4e5f\u51cf\u5c0f\u4e86\u4e00\u4e2a\u6570\u91cf\u7ea7\u4ee5\u4e0a\u3002\u7406\u8bba\u6a21\u578b\u652f\u6301\u4e86\u5b9e\u9a8c\u89c2\u5bdf\u7ed3\u679c\uff0c\u5e76\u5c06\u8fd9\u79cd\u9c81\u68d2\u6027\u4e0e\u76f8\u4e92\u4f5c\u7528\u6a21\u5f0f\u7684\u80fd\u5e26\u7ed3\u6784\u7279\u6027\u8054\u7cfb\u8d77\u6765\u3002", "conclusion": "\u4e8c\u6b21\u975e\u7ebf\u6027\u6ce2\u5bfc\u9635\u5217\u662f\u63a2\u7d22\u91cf\u5b50\u5149\u5b50\u7535\u8def\u4e2d\u975e\u7ebf\u6027\u3001\u62d3\u6251\u548c\u4e0d\u89c4\u5219\u6027\u76f8\u4e92\u4f5c\u7528\u7684\u6709\u524d\u666f\u7684\u5e73\u53f0\u3002"}}
{"id": "2510.24353", "categories": ["cs.LO", "03C05, 08B05, 68Q85", "F.1.1; F.4.3"], "pdf": "https://arxiv.org/pdf/2510.24353", "abs": "https://arxiv.org/abs/2510.24353", "authors": ["Hannes Schulze", "Lutz Schr\u00f6der", "\u00dcsame Cengiz"], "title": "Graded Monads in the Semantics of Nominal Automata", "comment": null, "summary": "Nominal automata models serve as a formalism for data languages, and in fact\noften relate closely to classical register models. The paradigm of name\nallocation in nominal automata helps alleviate the pervasive computational\nhardness of register models in a tradeoff between expressiveness and\ncomputational tractability. For instance, regular nondeterministic nominal\nautomata (RNNAs) correspond, under their local freshness semantics, to a form\nof lossy register automata, and unlike the full register automaton model allow\nfor inclusion checking in elementary complexity. The semantic framework of\ngraded monads provides a unified algebraic treatment of spectra of behavioural\nequivalences in the setting of universal coalgebra. In the present work, we\nextend the associated notion of graded algebraic theory to the nominal setting.\nIn the arising framework of graded nominal algebra, we give an algebraic theory\ncapturing the local freshness semantics of RNNAs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u540d\u4e49\u81ea\u52a8\u673a\u6a21\u578b\uff0c\u5e76\u5c06\u5176\u4e0e\u7ecf\u5178\u5bc4\u5b58\u5668\u6a21\u578b\u8054\u7cfb\u8d77\u6765\uff0c\u901a\u8fc7\u5f15\u5165\u540d\u79f0\u5206\u914d\u673a\u5236\u6765\u63d0\u9ad8\u53ef\u8ba1\u7b97\u6027\u3002", "motivation": "\u540d\u4e49\u81ea\u52a8\u673a\u6a21\u578b\u5728\u6570\u636e\u8bed\u8a00\u548c\u5bc4\u5b58\u5668\u6a21\u578b\u4e4b\u95f4\u67b6\u8d77\u4e86\u6865\u6881\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u8ba1\u7b97\u590d\u6742\u6027\u65b9\u9762\u3002", "method": "\u5c06\u6e10\u8fdb\u5e7a\u534a\u7fa4\u7684\u8bed\u4e49\u6846\u67b6\u6269\u5c55\u5230\u540d\u4e49\u8bbe\u7f6e\uff0c\u5e76\u63d0\u51fa\u4e86\u6e10\u8fdb\u540d\u4e49\u4ee3\u6570\u6846\u67b6\uff0c\u4e3a RNNAs \u7684\u5c40\u90e8\u65b0\u9c9c\u5ea6\u8bed\u4e49\u63d0\u4f9b\u4ee3\u6570\u7406\u8bba\u3002", "result": "\u5728\u6e10\u8fdb\u540d\u4e49\u4ee3\u6570\u6846\u67b6\u4e0b\uff0c\u8bba\u6587\u4e3a RNNAs \u7684\u5c40\u90e8\u65b0\u9c9c\u5ea6\u8bed\u4e49\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4ee3\u6570\u7406\u8bba\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u7406\u89e3\u548c\u5904\u7406\u540d\u4e49\u81ea\u52a8\u673a\u53ca\u5176\u4e0e\u5bc4\u5b58\u5668\u6a21\u578b\u7684\u5173\u7cfb\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u7684\u4ee3\u6570\u89c6\u89d2\u3002"}}
{"id": "2510.23895", "categories": ["eess.SY", "cs.OS", "cs.RO", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.23895", "abs": "https://arxiv.org/abs/2510.23895", "authors": ["Hoora Sobhani", "Hyoseung Kim"], "title": "Modeling and Scheduling of Fusion Patterns in Autonomous Driving Systems (Extended Version)", "comment": null, "summary": "In Autonomous Driving Systems (ADS), Directed Acyclic Graphs (DAGs) are\nwidely used to model complex data dependencies and inter-task communication.\nHowever, existing DAG scheduling approaches oversimplify data fusion tasks by\nassuming fixed triggering mechanisms, failing to capture the diverse fusion\npatterns found in real-world ADS software stacks. In this paper, we propose a\nsystematic framework for analyzing various fusion patterns and their\nperformance implications in ADS. Our framework models three distinct fusion\ntask types: timer-triggered, wait-for-all, and immediate fusion, which\ncomprehensively represent real-world fusion behaviors. Our Integer Linear\nProgramming (ILP)-based approach enables an optimization of multiple real-time\nperformance metrics, including reaction time, time disparity, age of\ninformation, and response time, while generating deterministic offline\nschedules directly applicable to real platforms. Evaluation using real-world\nADS case studies, Raspberry Pi implementation, and randomly generated DAGs\ndemonstrates that our framework handles diverse fusion patterns beyond the\nscope of existing work, and achieves substantial performance improvements in\ncomparable scenarios.", "AI": {"tldr": "\u73b0\u6709\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\uff08ADS\uff09\u4e2d\u7684\u6709\u5411\u65e0\u73af\u56fe\uff08DAG\uff09\u8c03\u5ea6\u65b9\u6cd5\u8fc7\u4e8e\u7b80\u5316\u4e86\u6570\u636e\u878d\u5408\u4efb\u52a1\uff0c\u672a\u80fd\u6355\u6349\u771f\u5b9e\u4e16\u754cADS\u8f6f\u4ef6\u6808\u4e2d\u591a\u6837\u5316\u7684\u878d\u5408\u6a21\u5f0f\u3002\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u7cfb\u7edf\u6027\u6846\u67b6\u6765\u5206\u6790\u4e0d\u540c\u7684\u878d\u5408\u6a21\u5f0f\u53ca\u5176\u6027\u80fd\u5f71\u54cd\uff0c\u5bf9\u878d\u5408\u4efb\u52a1\u8fdb\u884c\u4e86\u4e09\u79cd\u7c7b\u578b\uff08\u5b9a\u65f6\u89e6\u53d1\u3001\u5168\u7b49\u5f85\u3001\u7acb\u5373\u878d\u5408\uff09\u7684\u5efa\u6a21\uff0c\u5e76\u4f7f\u7528\u6574\u6570\u7ebf\u6027\u89c4\u5212\uff08ILP\uff09\u65b9\u6cd5\u4f18\u5316\u4e86\u591a\u4e2a\u5b9e\u65f6\u6027\u80fd\u6307\u6807\uff08\u53cd\u5e94\u65f6\u95f4\u3001\u65f6\u95f4\u5dee\u5f02\u3001\u4fe1\u606f\u5e74\u9f84\u3001\u54cd\u5e94\u65f6\u95f4\uff09\uff0c\u751f\u6210\u4e86\u53ef\u76f4\u63a5\u7528\u4e8e\u5b9e\u9645\u5e73\u53f0\u7684\u786e\u5b9a\u6027\u79bb\u7ebf\u8c03\u5ea6\u3002\u5b9e\u9645\u6848\u4f8b\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u6846\u67b6\u80fd\u5904\u7406\u6bd4\u73b0\u6709\u5de5\u4f5c\u66f4\u5e7f\u6cdb\u7684\u878d\u5408\u6a21\u5f0f\uff0c\u5e76\u5728\u53ef\u6bd4\u573a\u666f\u4e2d\u5b9e\u73b0\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709DAG\u8c03\u5ea6\u65b9\u6cd5\u672a\u80fd\u6355\u6349\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e2d\u591a\u6837\u5316\u7684\u6570\u636e\u878d\u5408\u6a21\u5f0f\uff0c\u9700\u8981\u4e00\u4e2a\u66f4\u7cfb\u7edf\u7684\u6846\u67b6\u6765\u5206\u6790\u548c\u4f18\u5316\u8fd9\u4e9b\u6a21\u5f0f\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u7cfb\u7edf\u6027\u6846\u67b6\uff0c\u5bf9\u5b9a\u65f6\u89e6\u53d1\u3001\u5168\u7b49\u5f85\u548c\u7acb\u5373\u878d\u5408\u4e09\u79cd\u6570\u636e\u878d\u5408\u4efb\u52a1\u7c7b\u578b\u8fdb\u884c\u5efa\u6a21\u3002\u91c7\u7528\u57fa\u4e8e\u6574\u6570\u7ebf\u6027\u89c4\u5212\uff08ILP\uff09\u7684\u65b9\u6cd5\u8fdb\u884c\u4f18\u5316\uff0c\u4ee5\u63d0\u9ad8\u53cd\u5e94\u65f6\u95f4\u3001\u65f6\u95f4\u5dee\u5f02\u3001\u4fe1\u606f\u5e74\u9f84\u548c\u54cd\u5e94\u65f6\u95f4\u7b49\u591a\u4e2a\u5b9e\u65f6\u6027\u80fd\u6307\u6807\uff0c\u5e76\u751f\u6210\u786e\u5b9a\u6027\u79bb\u7ebf\u8c03\u5ea6\u3002", "result": "\u4f7f\u7528\u771f\u5b9eADS\u6848\u4f8b\u3001Raspberry Pi\u5b9e\u73b0\u548c\u968f\u673a\u751f\u6210\u7684DAG\u8fdb\u884c\u8bc4\u4f30\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u6846\u67b6\u80fd\u5904\u7406\u6bd4\u73b0\u6709\u5de5\u4f5c\u66f4\u5e7f\u6cdb\u7684\u878d\u5408\u6a21\u5f0f\uff0c\u5e76\u5728\u53ef\u6bd4\u573a\u666f\u4e2d\u5b9e\u73b0\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u7cfb\u7edf\u6027\u6846\u67b6\u80fd\u591f\u6709\u6548\u5904\u7406\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e2d\u591a\u6837\u5316\u7684\u6570\u636e\u878d\u5408\u6a21\u5f0f\uff0c\u5e76\u901a\u8fc7ILP\u4f18\u5316\u663e\u8457\u63d0\u5347\u4e86\u591a\u4e2a\u5b9e\u65f6\u6027\u80fd\u6307\u6807\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2510.23905", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23905", "abs": "https://arxiv.org/abs/2510.23905", "authors": ["Yiming Zhang", "Vikram Krishnamurthy", "Shashwat Jain"], "title": "Inferring Group Intent as a Cooperative Game. An NLP-based Framework for Trajectory Analysis using Graph Transformer Neural Network", "comment": null, "summary": "This paper studies group target trajectory intent as the outcome of a\ncooperative game where the complex-spatio trajectories are modeled using an\nNLP-based generative model. In our framework, the group intent is specified by\nthe characteristic function of a cooperative game, and allocations for players\nin the cooperative game are specified by either the core, the Shapley value, or\nthe nucleolus. The resulting allocations induce probability distributions that\ngovern the coordinated spatio-temporal trajectories of the targets that reflect\nthe group's underlying intent. We address two key questions: (1) How can the\nintent of a group trajectory be optimally formalized as the characteristic\nfunction of a cooperative game? (2) How can such intent be inferred from noisy\nobservations of the targets? To answer the first question, we introduce a\nFisher-information-based characteristic function of the cooperative game, which\nyields probability distributions that generate coordinated spatio-temporal\npatterns. As a generative model for these patterns, we develop an NLP-based\ngenerative model built on formal grammar, enabling the creation of realistic\nmulti-target trajectory data. To answer the second question, we train a Graph\nTransformer Neural Network (GTNN) to infer group trajectory intent-expressed as\nthe characteristic function of the cooperative game-from observational data\nwith high accuracy. The self-attention function of the GTNN depends on the\ntrack estimates. Thus, the formulation and algorithms provide a multi-layer\napproach that spans target tracking (Bayesian signal processing) and the GTNN\n(for group intent inference).", "AI": {"tldr": "\u672c\u7814\u7a76\u5c06\u7fa4\u4f53\u76ee\u6807\u8f68\u8ff9\u610f\u56fe\u5efa\u6a21\u4e3a\u5408\u4f5c\u535a\u5f08\u7684\u7ed3\u679c\uff0c\u5e76\u4f7f\u7528\u57fa\u4e8eNLP\u7684\u751f\u6210\u6a21\u578b\u6765\u5904\u7406\u590d\u6742\u65f6\u7a7a\u8f68\u8ff9\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u6700\u4f18\u5730\u5c06\u7fa4\u4f53\u8f68\u8ff9\u610f\u56fe\u5f62\u5f0f\u5316\u4e3a\u5408\u4f5c\u535a\u5f08\u7684\u7279\u5f81\u51fd\u6570\uff0c\u5e76\u4ece\u566a\u58f0\u89c2\u6d4b\u4e2d\u63a8\u65ad\u8fd9\u79cd\u610f\u56fe\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eFisher\u4fe1\u606f\u7684\u5408\u4f5c\u535a\u5f08\u7279\u5f81\u51fd\u6570\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8eNLP\u548c\u5f62\u5f0f\u8bed\u6cd5\u7684\u751f\u6210\u6a21\u578b\u6765\u751f\u6210\u534f\u8c03\u7684\u65f6\u7a7a\u8f68\u8ff9\u3002\u6b64\u5916\uff0c\u8fd8\u8bad\u7ec3\u4e86\u4e00\u4e2a\u56feTransformer\u795e\u7ecf\u7f51\u7edc\uff08GTNN\uff09\u6765\u4ece\u89c2\u6d4b\u6570\u636e\u4e2d\u63a8\u65ad\u7fa4\u4f53\u8f68\u8ff9\u610f\u56fe\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u534f\u8c03\u7684\u65f6\u7a7a\u8f68\u8ff9\u6a21\u5f0f\uff0c\u5e76\u80fd\u9ad8\u7cbe\u5ea6\u5730\u4ece\u89c2\u6d4b\u6570\u636e\u4e2d\u63a8\u65ad\u7fa4\u4f53\u8f68\u8ff9\u610f\u56fe\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u5c42\u6b21\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u76ee\u6807\u8ddf\u8e2a\uff08\u8d1d\u53f6\u65af\u4fe1\u53f7\u5904\u7406\uff09\u548cGTNN\uff08\u7528\u4e8e\u7fa4\u4f53\u610f\u56fe\u63a8\u65ad\uff09\uff0c\u4e3a\u7fa4\u4f53\u76ee\u6807\u8f68\u8ff9\u610f\u56fe\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002"}}
{"id": "2510.23630", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23630", "abs": "https://arxiv.org/abs/2510.23630", "authors": ["Ninghui Feng", "Yiyan Qi"], "title": "NUM2EVENT: Interpretable Event Reasoning from Numerical time-series", "comment": null, "summary": "Large language models (LLMs) have recently demonstrated impressive multimodal\nreasoning capabilities, yet their understanding of purely numerical time-series\nsignals remains limited. Existing approaches mainly focus on forecasting or\ntrend description, without uncovering the latent events that drive numerical\nchanges or explaining the reasoning process behind them. In this work, we\nintroduce the task of number-to-event reasoning and decoding, which aims to\ninfer interpretable structured events from numerical inputs, even when current\ntext is unavailable. To address the data scarcity and semantic alignment\nchallenges, we propose a reasoning-aware framework that integrates an\nagent-guided event extractor (AGE), a marked multivariate Hawkes-based\nsynthetic generator (EveDTS), and a two-stage fine-tuning pipeline combining a\ntime-series encoder with a structured decoder. Our model explicitly reasons\nover numerical changes, generates intermediate explanations, and outputs\nstructured event hypotheses. Experiments on multi-domain datasets show that our\nmethod substantially outperforms strong LLM baselines in event-level precision\nand recall. These results suggest a new direction for bridging quantitative\nreasoning and semantic understanding, enabling LLMs to explain and predict\nevents directly from numerical dynamics.", "AI": {"tldr": "LLMs\u5728\u7406\u89e3\u7eaf\u6570\u503c\u65f6\u95f4\u5e8f\u5217\u4fe1\u53f7\u65b9\u9762\u80fd\u529b\u6709\u9650\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u6570\u503c\u4fe1\u53f7\u63a8\u65ad\u4e8b\u4ef6\u7684\u65b0\u4efb\u52a1\uff08number-to-event reasoning and decoding\uff09\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u540d\u4e3aAGE+EveDTS\u7684\u6846\u67b6\u6765\u89e3\u51b3\u6570\u636e\u7a00\u758f\u548c\u8bed\u4e49\u5bf9\u9f50\u95ee\u9898\uff0c\u901a\u8fc7\u663e\u5f0f\u63a8\u7406\u6570\u503c\u53d8\u5316\u3001\u751f\u6210\u4e2d\u95f4\u89e3\u91ca\u548c\u8f93\u51fa\u7ed3\u6784\u5316\u4e8b\u4ef6\u5047\u8bbe\uff0c\u5728\u591a\u9886\u57df\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6bd4\u73b0\u6709LLM\u57fa\u7ebf\u66f4\u597d\u7684\u4e8b\u4ef6\u63d0\u53d6\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7684LLM\u5728\u5904\u7406\u7eaf\u6570\u503c\u65f6\u95f4\u5e8f\u5217\u4fe1\u53f7\u65b9\u9762\u80fd\u529b\u6709\u9650\uff0c\u4e3b\u8981\u96c6\u4e2d\u4e8e\u9884\u6d4b\u6216\u8d8b\u52bf\u63cf\u8ff0\uff0c\u672a\u80fd\u63ed\u793a\u9a71\u52a8\u6570\u503c\u53d8\u5316\u7684\u6f5c\u5728\u4e8b\u4ef6\u6216\u89e3\u91ca\u5176\u539f\u56e0\u3002", "method": "\u63d0\u51fanumber-to-event reasoning and decoding\u4efb\u52a1\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u540d\u4e3aAGE+EveDTS\u7684\u63a8\u7406\u611f\u77e5\u6846\u67b6\u3002\u8be5\u6846\u67b6\u6574\u5408\u4e86\u4ee3\u7406\u5f15\u5bfc\u4e8b\u4ef6\u63d0\u53d6\u5668\uff08AGE\uff09\u548c\u57fa\u4e8e\u6807\u8bb0\u591a\u5143 Hawkes\u8fc7\u7a0b\u7684\u5408\u6210\u6570\u636e\u751f\u6210\u5668\uff08EveDTS\uff09\uff0c\u5e76\u91c7\u7528\u7ed3\u5408\u65f6\u95f4\u5e8f\u5217\u7f16\u7801\u5668\u548c\u7ed3\u6784\u5316\u89e3\u7801\u5668\u7684\u4e24\u9636\u6bb5\u5fae\u8c03\u6d41\u7a0b\u3002\u6a21\u578b\u80fd\u663e\u5f0f\u63a8\u7406\u6570\u503c\u53d8\u5316\u3001\u751f\u6210\u4e2d\u95f4\u89e3\u91ca\uff0c\u5e76\u8f93\u51fa\u7ed3\u6784\u5316\u4e8b\u4ef6\u5047\u8bbe\u3002", "result": "\u5728\u591a\u9886\u57df\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4e8b\u4ef6\u7ea7\u522b\u7684\u7cbe\u786e\u7387\u548c\u53ec\u56de\u7387\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u5f3a\u5927\u7684LLM\u57fa\u7ebf\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u8fde\u63a5\u91cf\u5316\u63a8\u7406\u548c\u8bed\u4e49\u7406\u89e3\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\uff0c\u4f7fLLM\u80fd\u591f\u76f4\u63a5\u4ece\u6570\u503c\u52a8\u6001\u4e2d\u89e3\u91ca\u548c\u9884\u6d4b\u4e8b\u4ef6\u3002"}}
{"id": "2510.23822", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23822", "abs": "https://arxiv.org/abs/2510.23822", "authors": ["Zhenyu Zhang", "Tianyi Chen", "Weiran Xu", "Alex Pentland", "Jiaxin Pei"], "title": "ReCAP: Recursive Context-Aware Reasoning and Planning for Large Language Model Agents", "comment": null, "summary": "Long-horizon tasks requiring multi-step reasoning and dynamic re-planning\nremain challenging for large language models (LLMs). Sequential prompting\nmethods are prone to context drift, loss of goal information, and recurrent\nfailure cycles, while hierarchical prompting methods often weaken cross-level\ncontinuity or incur substantial runtime overhead. We introduce ReCAP (Recursive\nContext-Aware Reasoning and Planning), a hierarchical framework with shared\ncontext for reasoning and planning in LLMs. ReCAP combines three key\nmechanisms: (i) plan-ahead decomposition, in which the model generates a full\nsubtask list, executes the first item, and refines the remainder; (ii)\nstructured re-injection of parent plans, maintaining consistent multi-level\ncontext during recursive return; and (iii) memory-efficient execution, bounding\nthe active prompt so costs scale linearly with task depth. Together these\nmechanisms align high-level goals with low-level actions, reduce redundant\nprompting, and preserve coherent context updates across recursion. Experiments\ndemonstrate that ReCAP substantially improves subgoal alignment and success\nrates on various long-horizon reasoning benchmarks, achieving a 32% gain on\nsynchronous Robotouille and a 29% improvement on asynchronous Robotouille under\nthe strict pass@1 protocol.", "AI": {"tldr": "ReCAP\u662f\u4e00\u4e2a\u5206\u5c42\u6846\u67b6\uff0c\u901a\u8fc7\u8ba1\u5212\u63d0\u524d\u5206\u89e3\u3001\u7ed3\u6784\u5316\u7236\u8ba1\u5212\u91cd\u6ce8\u5165\u548c\u8bb0\u5fc6\u9ad8\u6548\u6267\u884c\u6765\u89e3\u51b3LLM\u5728\u957f\u65f6\u4efb\u52a1\u4e2d\u7684\u591a\u6b65\u63a8\u7406\u548c\u52a8\u6001\u91cd\u65b0\u89c4\u5212\u6311\u6218\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5b50\u76ee\u6807\u5bf9\u9f50\u548c\u6210\u529f\u7387\u3002", "motivation": "\u957f\u65f6\u4efb\u52a1\u9700\u8981\u591a\u6b65\u63a8\u7406\u548c\u52a8\u6001\u91cd\u89c4\u5212\uff0c\u8fd9\u5bf9LLM\u6765\u8bf4\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002\u73b0\u6709\u7684\u987a\u5e8f\u63d0\u793a\u65b9\u6cd5\u5bb9\u6613\u51fa\u73b0\u4e0a\u4e0b\u6587\u6f02\u79fb\u3001\u76ee\u6807\u4fe1\u606f\u4e22\u5931\u548c\u91cd\u590d\u5931\u8d25\u5faa\u73af\uff0c\u800c\u5206\u5c42\u63d0\u793a\u65b9\u6cd5\u5f80\u5f80\u4f1a\u524a\u5f31\u8de8\u5c42\u8fde\u7eed\u6027\u6216\u5e26\u6765\u663e\u8457\u7684\u8fd0\u884c\u65f6\u5f00\u9500\u3002", "method": "ReCAP\u662f\u4e00\u4e2a\u5206\u5c42\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u673a\u5236\uff1a1. \u8ba1\u5212\u63d0\u524d\u5206\u89e3\uff1a\u6a21\u578b\u751f\u6210\u5b8c\u6574\u7684\u5b50\u4efb\u52a1\u5217\u8868\uff0c\u6267\u884c\u7b2c\u4e00\u9879\uff0c\u7136\u540e\u7ec6\u5316\u5269\u4f59\u4efb\u52a1\u30022. \u7ed3\u6784\u5316\u7236\u8ba1\u5212\u91cd\u6ce8\u5165\uff1a\u5728\u9012\u5f52\u8fd4\u56de\u8fc7\u7a0b\u4e2d\u4fdd\u6301\u4e00\u81f4\u7684\u591a\u5c42\u4e0a\u4e0b\u6587\u30023. \u8bb0\u5fc6\u9ad8\u6548\u6267\u884c\uff1a\u9650\u5236\u6d3b\u52a8\u63d0\u793a\uff0c\u4f7f\u6210\u672c\u4e0e\u4efb\u52a1\u6df1\u5ea6\u5448\u7ebf\u6027\u5173\u7cfb\u3002", "result": "ReCAP\u5728\u5404\u79cd\u957f\u65f6\u63a8\u7406\u57fa\u51c6\u4e0a\u663e\u8457\u63d0\u9ad8\u4e86\u5b50\u76ee\u6807\u5bf9\u9f50\u548c\u6210\u529f\u7387\uff0c\u5728\u540c\u6b65Robotouille\u4e0a\u63d0\u9ad8\u4e8632%\uff0c\u5728\u5f02\u6b65Robotouille\u4e0a\u63d0\u9ad8\u4e8629%\uff0c\u4e14\u9075\u5faa\u4e25\u683c\u7684pass@1\u534f\u8bae\u3002", "conclusion": "ReCAP\u901a\u8fc7\u5bf9\u9f50\u9ad8\u5c42\u76ee\u6807\u548c\u4f4e\u5c42\u52a8\u4f5c\u3001\u51cf\u5c11\u5197\u4f59\u63d0\u793a\u4ee5\u53ca\u5728\u9012\u5f52\u4e2d\u4fdd\u6301\u8fde\u8d2f\u7684\u4e0a\u4e0b\u6587\u66f4\u65b0\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLM\u5728\u957f\u65f6\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u548c\u89c4\u5212\u95ee\u9898\u3002"}}
{"id": "2510.24354", "categories": ["cs.SI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.24354", "abs": "https://arxiv.org/abs/2510.24354", "authors": ["Jacopo D'Ignazi", "Andreas Kaltenbrunner", "Ga\u00ebl Le Mens", "Fabrizio Germano", "Vicen\u00e7 G\u00f3mez"], "title": "Rewarding Engagement and Personalization in Popularity-Based Rankings Amplifies Extremism and Polarization", "comment": null, "summary": "Despite extensive research, the mechanisms through which online platforms\nshape extremism and polarization remain poorly understood. We identify and test\na mechanism, grounded in empirical evidence, that explains how ranking\nalgorithms can amplify both phenomena. This mechanism is based on\nwell-documented assumptions: (i) users exhibit position bias and tend to prefer\nitems displayed higher in the ranking, (ii) users prefer like-minded content,\n(iii) users with more extreme views are more likely to engage actively, and\n(iv) ranking algorithms are popularity-based, assigning higher positions to\nitems that attract more clicks. Under these conditions, when platforms\nadditionally reward \\emph{active} engagement and implement \\emph{personalized}\nrankings, users are inevitably driven toward more extremist and polarized news\nconsumption. We formalize this mechanism in a dynamical model, which we\nevaluate by means of simulations and interactive experiments with hundreds of\nhuman participants, where the rankings are updated dynamically in response to\nuser activity.", "AI": {"tldr": "\u5728\u7ebf\u5e73\u53f0\u7684\u6392\u540d\u7b97\u6cd5\u53ef\u80fd\u52a0\u5267\u6781\u7aef\u4e3b\u4e49\u548c\u4e24\u6781\u5206\u5316\uff0c\u56e0\u4e3a\u5b83\u4eec\u4f18\u5148\u8003\u8651\u5438\u5f15\u70b9\u51fb\u7684\u201c\u6d3b\u8dc3\u201d\u53c2\u4e0e\u548c\u4e2a\u6027\u5316\u5185\u5bb9\u3002", "motivation": "\u5728\u7ebf\u5e73\u53f0\u5982\u4f55\u52a0\u5267\u6781\u7aef\u4e3b\u4e49\u548c\u4e24\u6781\u5206\u5316\u4ecd\u7136\u77e5\u4e4b\u751a\u5c11\uff0c\u9700\u8981\u5bf9\u5176\u6f5c\u5728\u673a\u5236\u8fdb\u884c\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u4e00\u4e2a\u8003\u8651\u4e86\u7528\u6237\u4f4d\u7f6e\u504f\u89c1\u3001\u5bf9\u76f8\u4f3c\u5185\u5bb9\u7684\u504f\u597d\u3001\u6781\u7aef\u7528\u6237\u66f4\u6d3b\u8dc3\u4ee5\u53ca\u7b97\u6cd5\u57fa\u4e8e\u53d7\u6b22\u8fce\u7a0b\u5ea6\u8fdb\u884c\u6392\u540d\u7684\u5047\u8bbe\u7684\u52a8\u6001\u6a21\u578b\uff0c\u5bf9\u8fd9\u4e00\u673a\u5236\u8fdb\u884c\u4e86\u5f62\u5f0f\u5316\u3002\u8be5\u6a21\u578b\u901a\u8fc7\u6a21\u62df\u548c\u5305\u542b\u6570\u767e\u540d\u53c2\u4e0e\u8005\u7684\u4ea4\u4e92\u5b9e\u9a8c\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5176\u4e2d\u6392\u540d\u4f1a\u6839\u636e\u7528\u6237\u6d3b\u52a8\u52a8\u6001\u66f4\u65b0\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5f53\u5e73\u53f0\u5956\u52b1\u201c\u6d3b\u8dc3\u201d\u53c2\u4e0e\u5e76\u5b9e\u65bd\u201c\u4e2a\u6027\u5316\u201d\u6392\u540d\u65f6\uff0c\u7528\u6237\u4f1a\u88ab\u4e0d\u53ef\u907f\u514d\u5730\u5f15\u5bfc\u81f3\u66f4\u6781\u7aef\u3001\u66f4\u4e24\u6781\u5316\u7684\u65b0\u95fb\u6d88\u8d39\u3002", "conclusion": "\u5728\u7ebf\u5e73\u53f0\u7684\u6392\u540d\u7b97\u6cd5\uff0c\u7279\u522b\u662f\u90a3\u4e9b\u5956\u52b1\u6d3b\u8dc3\u53c2\u4e0e\u548c\u4f7f\u7528\u4e2a\u6027\u5316\u6392\u540d\u7684\u7b97\u6cd5\uff0c\u662f\u52a0\u5267\u6781\u7aef\u4e3b\u4e49\u548c\u4e24\u6781\u5206\u5316\u73b0\u8c61\u7684\u5173\u952e\u9a71\u52a8\u56e0\u7d20\u3002"}}
{"id": "2510.24503", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.DC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.24503", "abs": "https://arxiv.org/abs/2510.24503", "authors": ["Mortesa Hussaini", "Jan Thei\u00df", "Anthony Stein"], "title": "Local Performance vs. Out-of-Distribution Generalization: An Empirical Analysis of Personalized Federated Learning in Heterogeneous Data Environments", "comment": null, "summary": "In the context of Federated Learning with heterogeneous data environments,\nlocal models tend to converge to their own local model optima during local\ntraining steps, deviating from the overall data distributions. Aggregation of\nthese local updates, e.g., with FedAvg, often does not align with the global\nmodel optimum (client drift), resulting in an update that is suboptimal for\nmost clients. Personalized Federated Learning approaches address this challenge\nby exclusively focusing on the average local performances of clients' models on\ntheir own data distribution. Generalization to out-of-distribution samples,\nwhich is a substantial benefit of FedAvg and represents a significant component\nof robustness, appears to be inadequately incorporated into the assessment and\nevaluation processes. This study involves a thorough evaluation of Federated\nLearning approaches, encompassing both their local performance and their\ngeneralization capabilities. Therefore, we examine different stages within a\nsingle communication round to enable a more nuanced understanding of the\nconsidered metrics. Furthermore, we propose and incorporate a modified approach\nof FedAvg, designated as Federated Learning with Individualized Updates (FLIU),\nextending the algorithm by a straightforward individualization step with an\nadaptive personalization factor. We evaluate and compare the approaches\nempirically using MNIST and CIFAR-10 under various distributional conditions,\nincluding benchmark IID and pathological non-IID, as well as additional novel\ntest environments with Dirichlet distribution specifically developed to stress\nthe algorithms on complex data heterogeneity.", "AI": {"tldr": "\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\uff0c\u672c\u5730\u6a21\u578b\u53ef\u80fd\u56e0\u6570\u636e\u5f02\u8d28\u6027\u800c\u504f\u79bb\u5168\u5c40\u6700\u4f18\uff08\u5ba2\u6237\u7aef\u6f02\u79fb\uff09\u3002\u73b0\u6709\u4e2a\u6027\u5316\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u4fa7\u91cd\u4e8e\u5e73\u5747\u672c\u5730\u6027\u80fd\uff0c\u5ffd\u89c6\u4e86\u6cdb\u5316\u80fd\u529b\u3002\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u7684\u672c\u5730\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684 FedAvg \u65b9\u6cd5\uff08FLIU\uff09\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u4e2a\u6027\u5316\u56e0\u5b50\u589e\u5f3a\u3002\u5b9e\u9a8c\u5728 MNIST \u548c CIFAR-10 \u6570\u636e\u96c6\u4e0a\u8fdb\u884c\uff0c\u5305\u62ec IID\u3001\u975e IID \u548c\u65b0\u7684 Dirichlet \u5206\u5e03\u6570\u636e\u96c6\uff0c\u4ee5\u8bc4\u4f30\u7b97\u6cd5\u5728\u590d\u6742\u6570\u636e\u5f02\u8d28\u6027\u4e0b\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u5728\u5904\u7406\u6570\u636e\u5f02\u8d28\u6027\u65f6\uff0c\u672c\u5730\u6a21\u578b\u5bb9\u6613\u504f\u79bb\u5168\u5c40\u6700\u4f18\uff0c\u5bfc\u81f4\u805a\u5408\u540e\u7684\u6a21\u578b\u6027\u80fd\u4e0d\u4f73\u3002\u540c\u65f6\uff0c\u5bf9\u6cdb\u5316\u80fd\u529b\u7684\u8bc4\u4f30\u4e0d\u8db3\uff0c\u8fd9\u5bf9\u4e8e FedAvg \u7b49\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\u81f3\u5173\u91cd\u8981\u3002", "method": "\u672c\u7814\u7a76\u901a\u8fc7\u8bc4\u4f30\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u5728\u672c\u5730\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u4e24\u65b9\u9762\u7684\u8868\u73b0\uff0c\u5e76\u6df1\u5165\u5206\u6790\u4e86\u901a\u4fe1\u8f6e\u6b21\u4e2d\u4e0d\u540c\u9636\u6bb5\u7684\u6307\u6807\u3002\u6b64\u5916\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a FLIU\uff08Federated Learning with Individualized Updates\uff09\u7684\u6539\u8fdb FedAvg \u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u81ea\u9002\u5e94\u4e2a\u6027\u5316\u56e0\u5b50\u6765\u589e\u5f3a\u6a21\u578b\u7684\u4e2a\u6027\u5316\u66f4\u65b0\u80fd\u529b\u3002", "result": "\u5728 MNIST \u548c CIFAR-10 \u6570\u636e\u96c6\u4e0a\uff0c\u9488\u5bf9 IID\u3001\u975e IID \u4ee5\u53ca\u4e13\u95e8\u8bbe\u8ba1\u7684 Dirichlet \u5206\u5e03\u6570\u636e\u96c6\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cFLIU \u5728\u5904\u7406\u590d\u6742\u6570\u636e\u5f02\u8d28\u6027\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u52bf\u3002", "conclusion": "FLIU \u901a\u8fc7\u5f15\u5165\u81ea\u9002\u5e94\u4e2a\u6027\u5316\u56e0\u5b50\uff0c\u5728\u4fdd\u6301\u6216\u63d0\u5347\u672c\u5730\u6027\u80fd\u7684\u540c\u65f6\uff0c\u6539\u5584\u4e86\u8054\u90a6\u5b66\u4e60\u6a21\u578b\u5728\u6570\u636e\u5f02\u8d28\u73af\u5883\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.23870", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23870", "abs": "https://arxiv.org/abs/2510.23870", "authors": ["Marianne Menglin Liu", "Sai Ashish Somayajula", "Syed Fahad Allam Shah", "Sujith Ravi", "Dan Roth"], "title": "OraPlan-SQL: A Planning-Centric Framework for Complex Bilingual NL2SQL Reasoning", "comment": null, "summary": "We present OraPlan-SQL, our system for the Archer NL2SQL Evaluation Challenge\n2025, a bilingual benchmark requiring complex reasoning such as arithmetic,\ncommonsense, and hypothetical inference. OraPlan-SQL ranked first, exceeding\nthe second-best system by more than 6% in execution accuracy (EX), with 55.0%\nin English and 56.7% in Chinese, while maintaining over 99% SQL validity (VA).\nOur system follows an agentic framework with two components: Planner agent that\ngenerates stepwise natural language plans, and SQL agent that converts these\nplans into executable SQL. Since SQL agent reliably adheres to the plan, our\nrefinements focus on the planner. Unlike prior methods that rely on multiple\nsub-agents for planning and suffer from orchestration overhead, we introduce a\nfeedback-guided meta-prompting strategy to refine a single planner. Failure\ncases from a held-out set are clustered with human input, and an LLM distills\nthem into corrective guidelines that are integrated into the planner's system\nprompt, improving generalization without added complexity. For the multilingual\nscenario, to address transliteration and entity mismatch issues, we incorporate\nentity-linking guidelines that generate alternative surface forms for entities\nand explicitly include them in the plan. Finally, we enhance reliability\nthrough plan diversification: multiple candidate plans are generated for each\nquery, with the SQL agent producing a query for each plan, and final output\nselected via majority voting over their executions.", "AI": {"tldr": "OraPlan-SQL \u5728 Archer NL2SQL \u8bc4\u4f30\u6311\u6218\u8d5b\u4e2d\u53d6\u5f97\u7b2c\u4e00\u540d\uff0c\u901a\u8fc7\u4e00\u79cd\u65b0\u7684\u53cd\u9988\u5f15\u5bfc\u7684\u5143\u63d0\u793a\u7b56\u7565\u548c\u5b9e\u4f53\u94fe\u63a5\u6307\u5357\u6765\u63d0\u9ad8\u89c4\u5212\u5668\u7684\u6027\u80fd\uff0c\u5e76\u91c7\u7528\u8ba1\u5212\u591a\u6837\u6027\u6765\u63d0\u9ad8\u53ef\u9760\u6027\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u5728 Archer NL2SQL \u8bc4\u4f30\u6311\u6218\u8d5b 2025 \u4e2d\u8868\u73b0\u4f18\u5f02\u7684\u7cfb\u7edf\uff0c\u8be5\u6311\u6218\u8d5b\u9700\u8981\u590d\u6742\u7684\u53cc\u8bed\u63a8\u7406\u80fd\u529b\u3002", "method": "\u91c7\u7528\u4e00\u79cd\u5305\u542b\u89c4\u5212\u4ee3\u7406\u548c SQL \u4ee3\u7406\u7684\u4ee3\u7406\u6846\u67b6\u3002\u89c4\u5212\u4ee3\u7406\u4f7f\u7528\u53cd\u9988\u5f15\u5bfc\u7684\u5143\u63d0\u793a\u7b56\u7565\u6765\u751f\u6210\u9010\u6b65\u7684\u81ea\u7136\u8bed\u8a00\u8ba1\u5212\uff0c\u5e76\u7ed3\u5408\u5b9e\u4f53\u94fe\u63a5\u6307\u5357\u6765\u5904\u7406\u591a\u8bed\u8a00\u95ee\u9898\u3002SQL \u4ee3\u7406\u5c06\u8ba1\u5212\u8f6c\u6362\u4e3a SQL\u3002\u901a\u8fc7\u8ba1\u5212\u591a\u6837\u6027\u6765\u63d0\u9ad8\u53ef\u9760\u6027\u3002", "result": "OraPlan-SQL \u5728 Archer NL2SQL \u8bc4\u4f30\u6311\u6218\u8d5b 2025 \u4e2d\u83b7\u5f97\u7b2c\u4e00\u540d\uff0c\u82f1\u8bed\u6267\u884c\u51c6\u786e\u7387 (EX) \u4e3a 55.0%\uff0c\u4e2d\u6587\u4e3a 56.7%\uff0cSQL \u6709\u6548\u6027 (VA) \u8d85\u8fc7 99%\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u53cd\u9988\u5f15\u5bfc\u7684\u5143\u63d0\u793a\u7b56\u7565\u3001\u5b9e\u4f53\u94fe\u63a5\u6307\u5357\u548c\u8ba1\u5212\u591a\u6837\u6027\u663e\u8457\u63d0\u9ad8\u4e86 NL2SQL \u7cfb\u7edf\u7684\u6027\u80fd\u548c\u53ef\u9760\u6027\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u590d\u6742\u7684\u591a\u8bed\u8a00\u63a8\u7406\u4efb\u52a1\u65f6\u3002"}}
{"id": "2510.23997", "categories": ["cs.RO", "I.2.9"], "pdf": "https://arxiv.org/pdf/2510.23997", "abs": "https://arxiv.org/abs/2510.23997", "authors": ["Stanley Wu", "Mohamad H. Danesh", "Simon Li", "Hanna Yurchyk", "Amin Abyaneh", "Anas El Houssaini", "David Meger", "Hsiu-Chin Lin"], "title": "VOCALoco: Viability-Optimized Cost-aware Adaptive Locomotion", "comment": "Accepted in IEEE Robotics and Automation Letters (RAL), 2025. 8\n  pages, 9 figures", "summary": "Recent advancements in legged robot locomotion have facilitated traversal\nover increasingly complex terrains. Despite this progress, many existing\napproaches rely on end-to-end deep reinforcement learning (DRL), which poses\nlimitations in terms of safety and interpretability, especially when\ngeneralizing to novel terrains. To overcome these challenges, we introduce\nVOCALoco, a modular skill-selection framework that dynamically adapts\nlocomotion strategies based on perceptual input. Given a set of pre-trained\nlocomotion policies, VOCALoco evaluates their viability and energy-consumption\nby predicting both the safety of execution and the anticipated cost of\ntransport over a fixed planning horizon. This joint assessment enables the\nselection of policies that are both safe and energy-efficient, given the\nobserved local terrain. We evaluate our approach on staircase locomotion tasks,\ndemonstrating its performance in both simulated and real-world scenarios using\na quadrupedal robot. Empirical results show that VOCALoco achieves improved\nrobustness and safety during stair ascent and descent compared to a\nconventional end-to-end DRL policy", "AI": {"tldr": "VOCALoco\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u6280\u80fd\u9009\u62e9\u6846\u67b6\uff0c\u901a\u8fc7\u8bc4\u4f30\u9884\u8bad\u7ec3\u7684\u8fd0\u52a8\u7b56\u7565\u7684\u5b89\u5168\u6027\u548c\u80fd\u8017\uff0c\u4ece\u800c\u5728\u590d\u6742\u5730\u5f62\u4e0a\u5b9e\u73b0\u7a33\u5065\u4e14\u8282\u80fd\u7684\u56db\u8db3\u673a\u5668\u4eba\u8fd0\u52a8\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u7684\u673a\u5668\u4eba\u8fd0\u52a8\u65b9\u6cd5\u5728\u6cdb\u5316\u5230\u65b0\u5730\u5f62\u65f6\u5b58\u5728\u5b89\u5168\u6027\u548c\u53ef\u89e3\u91ca\u6027\u9650\u5236\u3002VOCALoco\u65e8\u5728\u514b\u670d\u8fd9\u4e9b\u6311\u6218\u3002", "method": "VOCALoco\u6846\u67b6\u9996\u5148\u9884\u8bad\u7ec3\u4e00\u7ec4\u8fd0\u52a8\u7b56\u7565\uff0c\u7136\u540e\u6839\u636e\u611f\u77e5\u8f93\u5165\u52a8\u6001\u8bc4\u4f30\u8fd9\u4e9b\u7b56\u7565\u7684\u6267\u884c\u5b89\u5168\u6027\u548c\u9884\u671f\u80fd\u8017\uff08\u5728\u56fa\u5b9a\u7684\u89c4\u5212\u8303\u56f4\u5185\uff09\uff0c\u6700\u7ec8\u9009\u62e9\u5b89\u5168\u4e14\u80fd\u8017\u4f4e\u7684\u7b56\u7565\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u7684\u697c\u68af\u8fd0\u52a8\u4efb\u52a1\u4e2d\uff0cVOCALoco\u76f8\u6bd4\u4e8e\u4f20\u7edf\u7684\u7aef\u5230\u7aefDRL\u7b56\u7565\uff0c\u5728\u697c\u68af\u4e0a\u4e0b\u8fd0\u52a8\u65f6\u5c55\u73b0\u4e86\u66f4\u9ad8\u7684\u9c81\u68d2\u6027\u548c\u5b89\u5168\u6027\u3002", "conclusion": "VOCALoco\u901a\u8fc7\u8054\u5408\u8bc4\u4f30\u8fd0\u52a8\u7b56\u7565\u7684\u5b89\u5168\u6027\u548c\u80fd\u8017\uff0c\u80fd\u591f\u6709\u6548\u9009\u62e9\u9002\u5408\u5f53\u524d\u5730\u5f62\u7684\u7b56\u7565\uff0c\u4ece\u800c\u63d0\u9ad8\u673a\u5668\u4eba\u5728\u590d\u6742\u5730\u5f62\uff08\u5982\u697c\u68af\uff09\u4e0a\u7684\u8fd0\u52a8\u9c81\u68d2\u6027\u548c\u5b89\u5168\u6027\u3002"}}
{"id": "2510.24158", "categories": ["cond-mat.mtrl-sci", "physics.optics"], "pdf": "https://arxiv.org/pdf/2510.24158", "abs": "https://arxiv.org/abs/2510.24158", "authors": ["Jisong Gao", "Qiaoxiao Zhao", "Wenbo Liu", "Dong Li", "Zhicheng Gao", "Yudian Zhou", "Xuegao Hu", "Zhihao Cai", "Zhilin Li", "Youguo Shi", "Peng Cheng", "Zhaojun Liu", "Lan Chen", "Kehui Wu", "Zhigang Zhao", "Baojie Feng"], "title": "Development of a 10.8-eV Tabletop Femtosecond Laser with Tunable Polarization for High-Resolution Angle-Resolved Photoemission Spectroscopy", "comment": null, "summary": "The development of extreme ultraviolet sources is critical for advancing\nangleresolved photoemission spectroscopy (ARPES), a powerful technique for\nprobing the electronic structure of materials. Here, we report the construction\nof a tabletop 10.8-eV femtosecond laser through cascaded third-harmonic\ngeneration, which operates at a repetition rate of 1 MHz and delivers a photon\nflux of approximately 1012 photons/s. The system achieves a high energy\nresolution of approximately 11.8 meV and tunable polarization. This flexibility\nenables detailed studies of orbitaland (pseudo)spin characteristics in quantum\nmaterials. We demonstrate the capabilities of this laser-ARPES system by\ninvestigating several prototypical materials, showcasing its potential for\nelucidating complex phenomena in quantum materials.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u578b tabletop 10.8 eV \u98de\u79d2\u6fc0\u5149\u5668\uff0c\u7528\u4e8e\u9ad8\u5206\u8fa8\u7387 ARPES \u5b9e\u9a8c\uff0c\u53ef\u7814\u7a76\u91cf\u5b50\u6750\u6599\u7684\u7535\u5b50\u7ed3\u6784\u3002", "motivation": "\u4e3a\u4e86\u63a8\u8fdb\u89d2\u5206\u8fa8\u5149\u7535\u5b50\u80fd\u8c31\u5b66 (ARPES) \u6280\u672f\uff0c\u9700\u8981\u5f00\u53d1\u6781\u7d2b\u5916\u5149\u6e90\uff0c\u4ee5\u63a2\u6d4b\u6750\u6599\u7684\u7535\u5b50\u7ed3\u6784\u3002", "method": "\u901a\u8fc7\u7ea7\u8054\u4e09\u6b21\u8c10\u6ce2\u4ea7\u751f\u6784\u5efa\u4e86\u4e00\u79cd tabletop 10.8 eV \u98de\u79d2\u6fc0\u5149\u5668\uff0c\u8be5\u6fc0\u5149\u5668\u5de5\u4f5c\u5728 1 MHz \u91cd\u590d\u9891\u7387\u4e0b\uff0c\u5149\u5b50\u901a\u91cf\u7ea6\u4e3a 10^12 photons/s\uff0c\u5b9e\u73b0\u4e86\u7ea6 11.8 meV \u7684\u9ad8\u80fd\u91cf\u5206\u8fa8\u7387\u548c\u53ef\u8c03\u8c10\u504f\u632f\u3002", "result": "\u6240\u5f00\u53d1\u7684\u6fc0\u5149\u5668\u5b9e\u73b0\u4e86\u9ad8\u80fd\u91cf\u5206\u8fa8\u7387\u548c\u53ef\u8c03\u8c10\u504f\u632f\uff0c\u53ef\u7528\u4e8e\u8be6\u7ec6\u7814\u7a76\u91cf\u5b50\u6750\u6599\u4e2d\u7684\u8f68\u9053\u548c\uff08\u8d5d\uff09\u81ea\u65cb\u7279\u6027\u3002", "conclusion": "\u8be5\u6fc0\u5149\u5668-ARPES \u7cfb\u7edf\u80fd\u591f\u9610\u660e\u91cf\u5b50\u6750\u6599\u4e2d\u7684\u590d\u6742\u73b0\u8c61\uff0c\u5c55\u793a\u4e86\u5176\u5728\u91cf\u5b50\u6750\u6599\u7814\u7a76\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.24301", "categories": ["cond-mat.mes-hall", "quant-ph"], "pdf": "https://arxiv.org/pdf/2510.24301", "abs": "https://arxiv.org/abs/2510.24301", "authors": ["Edilberto O. Silva"], "title": "Bounds on Lorentz-violating parameters in magnetically confined 2D systems: A phenomenological approach", "comment": "12 pages, 4 figures, 6 tables", "summary": "We present a unified, SI-consistent framework to constrain minimal SME\ncoefficients $a_\\mu$ and $b_\\mu$ using magnetically confined two-dimensional\nelectron systems under a uniform magnetic field. Working in the nonrelativistic\n(Schr\\\"odinger--Pauli) limit with effective mass, we derive the radial problem\nfor cylindrical geometries and identify how spatial components ($\\mathbf\na,\\mathbf b$) reshape the effective potential, via $1/r$ and $r$ terms or\nspin-selective offsets, while scalar components ($a_0,b_0$) act through a\nglobal energy shift and a spin-momentum coupling. Phenomenological upper bounds\nfollow from requiring LV-induced shifts to lie below typical spectroscopic\nresolutions: $|a_0|\\lesssim\\delta E$, $|b_z|\\lesssim\\delta E/\\hbar$, and\ncompact expressions for $|a_\\varphi|$ and $|b_0|$ that expose their dependence\non device scales ($r_0$, $B_0$, $\\mu$, $m$). Dimensional analysis clarifies\nthat, in this regime, spatial $a_i$ carry momentum dimension and $b_i$ carry\ninverse-time/length dimensions, ensuring gauge-independent, unit-consistent\nreporting. Finite-difference eigenvalue calculations validate the scaling laws\nand illustrate spectral signatures across realistic parameter sets. The results\nshow that scalar sectors (notably $a_0$) are tightly constrained by\nstate-of-the-art $\\mu$eV-resolution probes, while spatial and axial sectors\nbenefit from spin- and $m$-resolved spectroscopy and geometric leverage,\nproviding a reproducible pathway to test Lorentz symmetry in condensed-matter\nplatforms.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u7684\u3001SI\u4e00\u81f4\u7684\u6846\u67b6\uff0c\u5229\u7528\u78c1\u7ea6\u675f\u4e8c\u7ef4\u7535\u5b50\u7cfb\u7edf\u548c\u5747\u5300\u78c1\u573a\u6765\u7ea6\u675f\u6700\u5c0fSME\u7cfb\u6570$a_\\mu$\u548c$b_\\mu$\u3002", "motivation": "\u5229\u7528\u78c1\u7ea6\u675f\u4e8c\u7ef4\u7535\u5b50\u7cfb\u7edf\u548c\u5747\u5300\u78c1\u573a\u6765\u7ea6\u675f\u6700\u5c0fSME\u7cfb\u6570$a_\\mu$\u548c$b_\\mu$\u3002", "method": "\u5728\u975e\u76f8\u5bf9\u8bba\uff08Schr\"odinger-Pauli\uff09\u6781\u9650\u4e0b\uff0c\u4f7f\u7528\u6709\u6548\u8d28\u91cf\uff0c\u63a8\u5bfc\u4e86\u5706\u67f1\u51e0\u4f55\u7684\u5f84\u5411\u95ee\u9898\uff0c\u5e76\u901a\u8fc7$1/r$\u548c$r$\u9879\u6216\u81ea\u65cb\u9009\u62e9\u6027\u504f\u79fb\u6765\u8bc6\u522b\u7a7a\u95f4\u5206\u91cf($\\mathbf a,\\mathbf b$)\u5982\u4f55\u91cd\u5851\u6709\u6548\u52bf\uff0c\u800c\u6807\u91cf\u5206\u91cf($a_0,b_0$)\u5219\u901a\u8fc7\u5168\u5c40\u80fd\u91cf\u79fb\u52a8\u548c\u81ea\u65cb-\u52a8\u91cf\u8026\u5408\u8d77\u4f5c\u7528\u3002", "result": "\u901a\u8fc7\u8981\u6c42LV\u5f15\u8d77\u7684\u4f4d\u79fb\u4f4e\u4e8e\u5178\u578b\u7684\u5149\u8c31\u5206\u8fa8\u7387\u6765\u83b7\u5f97\u73b0\u8c61\u5b66\u4e0a\u9650\uff1a$|a_0|\\lesssim\\delta E$, $|b_z|\\lesssim\\delta E/\\hbar$\uff0c\u4ee5\u53ca$|a_\\varphi|$\u548c$|b_0|$\u7684\u7d27\u51d1\u8868\u8fbe\u5f0f\uff0c\u63ed\u793a\u4e86\u5b83\u4eec\u5bf9\u8bbe\u5907\u5c3a\u5ea6\u7684\u4f9d\u8d56\u6027($r_0$, $B_0$, $\\mu$, $m$)\u3002\u91cf\u7eb2\u5206\u6790\u8868\u660e\uff0c\u5728\u8be5\u673a\u5236\u4e0b\uff0c\u7a7a\u95f4$a_i$\u5177\u6709\u52a8\u91cf\u91cf\u7eb2\uff0c\u800c$b_i$\u5177\u6709\u53cd\u65f6\u95f4/\u957f\u5ea6\u91cf\u7eb2\uff0c\u786e\u4fdd\u4e86\u89c4\u8303\u65e0\u5173\u3001\u5355\u4f4d\u4e00\u81f4\u7684\u62a5\u544a\u3002\u6709\u9650\u5dee\u5206\u7279\u5f81\u503c\u8ba1\u7b97\u9a8c\u8bc1\u4e86\u7f29\u653e\u5b9a\u5f8b\uff0c\u5e76\u8bf4\u660e\u4e86\u5728\u5b9e\u9645\u53c2\u6570\u96c6\u4e0a\u7684\u8c31\u7279\u5f81\u3002", "conclusion": "\u6807\u91cf\u6247\uff08\u5c24\u5176\u662f$a_0$\uff09\u53d7\u5230\u6700\u5148\u8fdb\u7684$\\\\mu$eV\u5206\u8fa8\u7387\u63a2\u6d4b\u5668\u7684\u4e25\u683c\u7ea6\u675f\uff0c\u800c\u7a7a\u95f4\u548c\u8f74\u5411\u6247\u5219\u53d7\u76ca\u4e8e\u81ea\u65cb\u548cm\u5206\u8fa8\u5149\u8c31\u4ee5\u53ca\u51e0\u4f55\u6760\u6746\u4f5c\u7528\uff0c\u4e3a\u5728\u51dd\u805a\u6001\u5e73\u53f0\u4e2d\u6d4b\u8bd5\u6d1b\u4f26\u5179\u5bf9\u79f0\u6027\u63d0\u4f9b\u4e86\u53ef\u91cd\u590d\u7684\u9014\u5f84\u3002"}}
{"id": "2510.23929", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23929", "abs": "https://arxiv.org/abs/2510.23929", "authors": ["Emily Kim", "Julieta Martinez", "Timur Bagautdinov", "Jessica Hodgins"], "title": "TurboPortrait3D: Single-step diffusion-based fast portrait novel-view synthesis", "comment": null, "summary": "We introduce TurboPortrait3D: a method for low-latency novel-view synthesis\nof human portraits. Our approach builds on the observation that existing\nimage-to-3D models for portrait generation, while capable of producing\nrenderable 3D representations, are prone to visual artifacts, often lack of\ndetail, and tend to fail at fully preserving the identity of the subject. On\nthe other hand, image diffusion models excel at generating high-quality images,\nbut besides being computationally expensive, are not grounded in 3D and thus\nare not directly capable of producing multi-view consistent outputs. In this\nwork, we demonstrate that image-space diffusion models can be used to\nsignificantly enhance the quality of existing image-to-avatar methods, while\nmaintaining 3D-awareness and running with low-latency. Our method takes a\nsingle frontal image of a subject as input, and applies a feedforward\nimage-to-avatar generation pipeline to obtain an initial 3D representation and\ncorresponding noisy renders. These noisy renders are then fed to a single-step\ndiffusion model which is conditioned on input image(s), and is specifically\ntrained to refine the renders in a multi-view consistent way. Moreover, we\nintroduce a novel effective training strategy that includes pre-training on a\nlarge corpus of synthetic multi-view data, followed by fine-tuning on\nhigh-quality real images. We demonstrate that our approach both qualitatively\nand quantitatively outperforms current state-of-the-art for portrait novel-view\nsynthesis, while being efficient in time.", "AI": {"tldr": "TurboPortrait3D\u5229\u7528\u56fe\u50cf\u7a7a\u95f4\u6269\u6563\u6a21\u578b\u63d0\u5347\u73b0\u6709\u56fe\u50cf\u52303D\u6a21\u578b\u7684\u8096\u50cf\u751f\u6210\u8d28\u91cf\uff0c\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u3001\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u7684\u65b0\u89c6\u56fe\u5408\u6210\uff0c\u5e76\u5728\u5b9a\u6027\u548c\u5b9a\u91cf\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u52303D\u6a21\u578b\u5728\u8096\u50cf\u751f\u6210\u4e2d\u5b58\u5728\u89c6\u89c9\u7455\u75b5\u3001\u7ec6\u8282\u4e0d\u8db3\u548c\u8eab\u4efd\u4fdd\u6301\u4e0d\u4f73\u7684\u95ee\u9898\uff0c\u800c\u56fe\u50cf\u6269\u6563\u6a21\u578b\u867d\u7136\u751f\u6210\u8d28\u91cf\u9ad8\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u975e3D\u611f\u77e5\u3002", "method": "\u63d0\u51faTurboPortrait3D\u65b9\u6cd5\uff0c\u7ed3\u5408\u56fe\u50cf\u52303D\u6a21\u578b\u548c\u5355\u6b65\u6269\u6563\u6a21\u578b\u3002\u9996\u5148\uff0c\u901a\u8fc7\u524d\u9988\u56fe\u50cf\u52303D\u7ba1\u9053\u751f\u6210\u521d\u59cb3D\u8868\u793a\u548c\u5e26\u566a\u6e32\u67d3\u56fe\uff1b\u7136\u540e\uff0c\u5229\u7528\u4ee5\u8f93\u5165\u56fe\u50cf\u4e3a\u6761\u4ef6\u7684\u5355\u6b65\u6269\u6563\u6a21\u578b\uff0c\u5bf9\u5e26\u566a\u6e32\u67d3\u56fe\u8fdb\u884c\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u4f18\u5316\u3002\u91c7\u7528\u9884\u8bad\u7ec3\uff08\u5927\u91cf\u5408\u6210\u591a\u89c6\u89d2\u6570\u636e\uff09\u548c\u5fae\u8c03\uff08\u9ad8\u8d28\u91cf\u771f\u5b9e\u56fe\u50cf\uff09\u7684\u8bad\u7ec3\u7b56\u7565\u3002", "result": "TurboPortrait3D\u5728\u8096\u50cf\u65b0\u89c6\u56fe\u5408\u6210\u65b9\u9762\uff0c\u65e0\u8bba\u5728\u5b9a\u6027\u8fd8\u662f\u5b9a\u91cf\u4e0a\u90fd\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u5e76\u4e14\u5728\u65f6\u95f4\u6548\u7387\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "TurboPortrait3D\u6210\u529f\u5730\u5229\u7528\u56fe\u50cf\u7a7a\u95f4\u6269\u6563\u6a21\u578b\u589e\u5f3a\u4e86\u73b0\u6709\u56fe\u50cf\u52303D\u8096\u50cf\u751f\u6210\u65b9\u6cd5\u7684\u8d28\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u4e863D\u611f\u77e5\u80fd\u529b\u548c\u4f4e\u5ef6\u8fdf\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u3001\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u7684\u65b0\u89c6\u56fe\u5408\u6210\u3002"}}
{"id": "2510.23797", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.23797", "abs": "https://arxiv.org/abs/2510.23797", "authors": ["Evangelia Takou", "Kenneth R. Brown"], "title": "Estimating and decoding coherent errors of QEC experiments with detector error models", "comment": "9 pages, 7 figures", "summary": "Decoders of quantum error correction (QEC) experiments make decisions based\non detected errors and the expected rates of error events, which together\ncomprise a detector error model. Here we show that the syndrome history of QEC\nexperiments is sufficient to detect and estimate coherent errors, removing the\nneed for prior device benchmarking experiments. Importantly, our method shows\nthat experimentally determined detector error models work equally well for both\nstochastic and coherent noise regimes. We model fully-coherent or\nfully-stochastic noise for repetition and surface codes and for various\nphenomenological and circuit-level noise scenarios, by employing Majorana and\nMonte Carlo simulators. We capture the interference of coherent errors, which\nappears as enhanced or suppressed physical error rates compared to the\nstochastic case, and also observe hyperedges that do not appear in the\ncorresponding Pauli-twirled models. Finally, we decode the detector error\nmodels undergoing coherent noise and find different thresholds compared to\ndetector error models built based on the stochastic noise assumption.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u91cf\u5b50\u7ea0\u9519\u89e3\u7801\u65b9\u6cd5\uff0c\u65e0\u9700\u9884\u5148\u8fdb\u884c\u8bbe\u5907\u57fa\u51c6\u6d4b\u8bd5\u5373\u53ef\u68c0\u6d4b\u548c\u4f30\u8ba1\u76f8\u5e72\u9519\u8bef\uff0c\u5e76\u4e14\u5bf9\u968f\u673a\u548c\u76f8\u5e72\u566a\u58f0\u5747\u6709\u6548\u3002", "motivation": "\u4ee5\u5f80\u7684\u91cf\u5b50\u7ea0\u9519\u89e3\u7801\u4f9d\u8d56\u4e8e\u5df2\u77e5\u7684\u9519\u8bef\u7387\uff0c\u9700\u8981\u8bbe\u5907\u57fa\u51c6\u6d4b\u8bd5\u3002\u672c\u7814\u7a76\u65e8\u5728\u53bb\u9664\u6b64\u4f9d\u8d56\uff0c\u5e76\u5904\u7406\u66f4\u5e7f\u6cdb\u7684\u566a\u58f0\u6a21\u578b\u3002", "method": "\u901a\u8fc7\u5206\u6790\u540c\u6b65\u5386\u53f2\u6765\u68c0\u6d4b\u548c\u4f30\u8ba1\u76f8\u5e72\u9519\u8bef\uff0c\u5e76\u4f7f\u7528\u6a21\u62df\u5668\uff08Majorana\u548cMonte Carlo\uff09\u5bf9\u4e0d\u540c\u566a\u58f0\u573a\u666f\uff08\u968f\u673a\u548c\u76f8\u5e72\uff09\u4e0b\u7684\u91cd\u590d\u7801\u548c\u8868\u9762\u7801\u8fdb\u884c\u5efa\u6a21\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u6355\u6349\u76f8\u5e72\u9519\u8bef\u7684\u5e72\u6270\u6548\u5e94\uff0c\u5e76\u53d1\u73b0\u4e0e\u57fa\u4e8e\u968f\u673a\u566a\u58f0\u7684\u5047\u8bbe\u4e0d\u540c\u7684\u89e3\u7801\u9608\u503c\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5730\u5904\u7406\u76f8\u5e72\u566a\u58f0\uff0c\u5e76\u5728\u91cf\u5b50\u7ea0\u9519\u4e2d\u63d0\u4f9b\u66f4\u51c6\u786e\u7684\u89e3\u7801\uff0c\u65e0\u9700\u9884\u5148\u7684\u8bbe\u5907\u57fa\u51c6\u6d4b\u8bd5\u3002"}}
{"id": "2510.23637", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2510.23637", "abs": "https://arxiv.org/abs/2510.23637", "authors": ["Job Petrov\u010di\u010d", "David Eliecer Narvaez Denis", "Ljup\u010do Todorovski"], "title": "Combining Textual and Structural Information for Premise Selection in Lean", "comment": null, "summary": "Premise selection is a key bottleneck for scaling theorem proving in large\nformal libraries. Yet existing language-based methods often treat premises in\nisolation, ignoring the web of dependencies that connects them. We present a\ngraph-augmented approach that combines dense text embeddings of Lean\nformalizations with graph neural networks over a heterogeneous dependency graph\ncapturing both state--premise and premise--premise relations. On the LeanDojo\nBenchmark, our method outperforms the ReProver language-based baseline by over\n25% across standard retrieval metrics. These results demonstrate the power of\nrelational information for more effective premise selection.", "AI": {"tldr": "\u901a\u8fc7\u7ed3\u5408\u6587\u672c\u5d4c\u5165\u548c\u56fe\u795e\u7ecf\u7f51\u7edc\u6765\u6539\u8fdb\u5b9a\u7406\u8bc1\u660e\u4e2d\u7684\u524d\u63d0\u9009\u62e9\uff0c\u5728 LeanDojo Benchmark \u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6210\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u8bed\u8a00\u7684\u65b9\u6cd5\u5728\u5904\u7406\u524d\u63d0\u9009\u62e9\u65f6\uff0c\u5f80\u5f80\u5b64\u7acb\u5730\u770b\u5f85\u524d\u63d0\uff0c\u5ffd\u7565\u4e86\u5b83\u4eec\u4e4b\u95f4\u5b58\u5728\u7684\u4f9d\u8d56\u5173\u7cfb\uff0c\u8fd9\u9650\u5236\u4e86\u5b9a\u7406\u8bc1\u660e\u5728\u5927\u578b\u5f62\u5f0f\u5316\u5e93\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u56fe\u589e\u5f3a\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86 LeanFormalizations \u7684\u5bc6\u96c6\u6587\u672c\u5d4c\u5165\u548c\u5f02\u6784\u4f9d\u8d56\u56fe\u4e0a\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\u3002\u8be5\u56fe\u6355\u6349\u4e86\u72b6\u6001-\u524d\u63d0\u548c\u524d\u63d0-\u524d\u63d0\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "result": "\u5728 LeanDojo Benchmark \u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u6807\u51c6\u68c0\u7d22\u6307\u6807\u4e0a\u6bd4 ReProver \u8bed\u8a00\u6a21\u578b\u57fa\u7ebf\u63d0\u9ad8\u4e86 25% \u4ee5\u4e0a\u3002", "conclusion": "\u5173\u7cfb\u4fe1\u606f\u5bf9\u4e8e\u66f4\u6709\u6548\u7684\u524d\u63d0\u9009\u62e9\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2510.23910", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.23910", "abs": "https://arxiv.org/abs/2510.23910", "authors": ["Khadija Omar Said", "Yukta Pareek", "Satadru Dey", "Ashish Ranjan Kumar"], "title": "Dynamical Modeling of Temperature and Smoke Evolution in a Thermal-Runaway Event of a Large-Format Lithium-ion Battery in a Mine Tunnel", "comment": null, "summary": "Large-format lithium-ion batteries (LIBs) provide effective energy storage\nsolutions for high-power equipment used in underground mining operations. They\nhave high Columbic efficiency and minimal heat and emission footprints.\nHowever, improper use of LIBs, accidents, or other factors may increase the\nprobability of thermal runaway (TR), a rapid combustion reaction that\ndischarges toxic and flammable substances. Several such incidents have been\ndocumented in mines. Since repeatable TR experiments to uncover the\ntransient-state propagation of TR are expensive and hazardous, high-fidelity\nmodels are usually developed to mimic the impact of these events. They are\nresource-intensive and are impractical to develop for many scenarios that could\nbe observed in a mine. Therefore, dynamic models within a reduced-order\nframework were constructed to represent the transient-state combustion event.\nReduced order models (ROMs) reasonably replicate trends in temperature and\nsmoke, showing strong alignment with the ground-truth dataset.", "AI": {"tldr": "\u5927\u578b\u9502\u79bb\u5b50\u7535\u6c60\uff08LIBs\uff09\u53ef\u4e3a\u5730\u4e0b\u77ff\u5c71\u7684\u9ad8\u529f\u7387\u8bbe\u5907\u63d0\u4f9b\u6709\u6548\u7684\u50a8\u80fd\u89e3\u51b3\u65b9\u6848\u3002\u7136\u800c\uff0c\u4e0d\u5f53\u4f7f\u7528\u3001\u4e8b\u6545\u6216\u5176\u4ed6\u56e0\u7d20\u53ef\u80fd\u589e\u52a0\u70ed\u5931\u63a7\uff08TR\uff09\u7684\u6982\u7387\u3002\u7531\u4e8e\u53ef\u91cd\u590d\u7684\u70ed\u5931\u63a7\u5b9e\u9a8c\u6210\u672c\u9ad8\u6602\u4e14\u5371\u9669\uff0c\u56e0\u6b64\u901a\u5e38\u4f1a\u5f00\u53d1\u9ad8\u4fdd\u771f\u6a21\u578b\u6765\u6a21\u62df\u8fd9\u4e9b\u4e8b\u4ef6\u7684\u5f71\u54cd\uff0c\u4f46\u8fd9\u4e9b\u6a21\u578b\u5bf9\u4e8e\u77ff\u5c71\u4e2d\u7684\u8bb8\u591a\u60c5\u51b5\u6765\u8bf4\u4e0d\u5207\u5b9e\u9645\u3002\u56e0\u6b64\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u964d\u9636\u6846\u67b6\u5185\u7684\u52a8\u6001\u6a21\u578b\u6765\u8868\u793a\u77ac\u6001\u71c3\u70e7\u4e8b\u4ef6\u3002\u964d\u9636\u6a21\u578b\uff08ROMs\uff09\u53ef\u4ee5\u5408\u7406\u5730\u590d\u5236\u6e29\u5ea6\u548c\u70df\u96fe\u7684\u53d8\u5316\u8d8b\u52bf\uff0c\u5e76\u4e0e\u5b9e\u9645\u6570\u636e\u96c6\u8868\u73b0\u51fa\u9ad8\u5ea6\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u578b\u9502\u79bb\u5b50\u7535\u6c60\uff08LIBs\uff09\u6a21\u578b\u5728\u6a21\u62df\u5730\u4e0b\u77ff\u5c71\u73af\u5883\u4e2d\u70ed\u5931\u63a7\uff08TR\uff09\u4e8b\u4ef6\u65f6\uff0c\u7531\u4e8e\u8d44\u6e90\u5bc6\u96c6\u4e14\u4e0d\u5207\u5b9e\u9645\uff0c\u96be\u4ee5\u6ee1\u8db3\u591a\u79cd\u573a\u666f\u7684\u9700\u6c42\u3002", "method": "\u91c7\u7528\u964d\u9636\u6a21\u578b\uff08ROMs\uff09\u6846\u67b6\u5185\u7684\u52a8\u6001\u6a21\u578b\u6765\u6a21\u62df\u77ac\u6001\u71c3\u70e7\u4e8b\u4ef6\u3002", "result": "\u964d\u9636\u6a21\u578b\uff08ROMs\uff09\u80fd\u591f\u5408\u7406\u5730\u590d\u5236\u6e29\u5ea6\u548c\u70df\u96fe\u7684\u53d8\u5316\u8d8b\u52bf\uff0c\u5e76\u4e0e\u5b9e\u9645\u6570\u636e\u96c6\u8868\u73b0\u51fa\u9ad8\u5ea6\u4e00\u81f4\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u964d\u9636\u6a21\u578b\uff08ROMs\uff09\u4e3a\u6a21\u62df\u5730\u4e0b\u77ff\u5c71\u73af\u5883\u4e2d\u9502\u79bb\u5b50\u7535\u6c60\u70ed\u5931\u63a7\uff08TR\uff09\u4e8b\u4ef6\u63d0\u4f9b\u4e86\u4e00\u79cd\u7ecf\u6d4e\u9ad8\u6548\u4e14\u5b9e\u7528\u7684\u65b9\u6cd5\u3002"}}
{"id": "2510.23631", "categories": ["cs.LG", "cs.AI", "stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.23631", "abs": "https://arxiv.org/abs/2510.23631", "authors": ["Yuxuan Tang", "Yifan Feng"], "title": "Beyond Pairwise: Empowering LLM Alignment With Ranked Choice Modeling", "comment": null, "summary": "Alignment of large language models (LLMs) has predominantly relied on\npairwise preference optimization, where annotators select the better of two\nresponses to a prompt. While simple, this approach overlooks the opportunity to\nlearn from richer forms of human feedback, such as multiwise comparisons and\ntop-$k$ rankings. We propose Ranked Choice Preference Optimization (RCPO), a\nunified framework that bridges preference optimization with (ranked) choice\nmodeling via maximum likelihood estimation. The framework is flexible,\nsupporting both utility-based and rank-based choice models. It subsumes several\nexisting pairwise methods (e.g., DPO, SimPO), while providing principled\ntraining objectives for richer feedback formats. We instantiate this framework\nwith two representative ranked choice models (Multinomial Logit and\nMallows-RMJ). Empirical studies on Llama-3-8B-Instruct and Gemma-2-9B-it across\nAlpacaEval 2 and Arena-Hard benchmarks show that RCPO consistently outperforms\ncompetitive baselines. RCPO shows how directly leveraging ranked preference\ndata, combined with the right choice models, yields more effective alignment.\nIt offers a versatile and extensible foundation for incorporating (ranked)\nchoice modeling into LLM training.", "AI": {"tldr": "RCPO\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u5c06\u504f\u597d\u4f18\u5316\u4e0e\uff08\u6392\u5e8f\uff09\u9009\u62e9\u6a21\u578b\u76f8\u7ed3\u5408\uff0c\u652f\u6301\u57fa\u4e8e\u6548\u7528\u548c\u57fa\u4e8e\u6392\u540d\u7684\u9009\u62e9\u6a21\u578b\uff0c\u5e76\u4e3a\u66f4\u4e30\u5bcc\u7684\u53cd\u9988\u683c\u5f0f\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u7684\u8bad\u7ec3\u76ee\u6807\u3002", "motivation": "\u73b0\u6709\u7684LLM\u5bf9\u9f50\u4e3b\u8981\u4f9d\u8d56\u4e8e\u6210\u5bf9\u504f\u597d\u4f18\u5316\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u5ffd\u7565\u4e86\u4ece\u591a\u9879\u9009\u62e9\u548ctop-k\u6392\u540d\u7b49\u66f4\u4e30\u5bcc\u7684\u53cd\u9988\u5f62\u5f0f\u4e2d\u5b66\u4e60\u7684\u673a\u4f1a\u3002", "method": "RCPO\u901a\u8fc7\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u5c06\u504f\u597d\u4f18\u5316\u4e0e\uff08\u6392\u5e8f\uff09\u9009\u62e9\u6a21\u578b\u76f8\u7ed3\u5408\uff0c\u652f\u6301\u57fa\u4e8e\u6548\u7528\u548c\u57fa\u4e8e\u6392\u540d\u7684\u9009\u62e9\u6a21\u578b\u3002", "result": "\u5728Llama-3-8B-Instruct\u548cGemma-2-9B-it\u6a21\u578b\u4e0a\uff0cRCPO\u5728AlpacaEval 2\u548cArena-Hard\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u7ade\u4e89\u57fa\u7ebf\u3002", "conclusion": "RCPO\u8bc1\u660e\u4e86\u76f4\u63a5\u5229\u7528\u6392\u5e8f\u504f\u597d\u6570\u636e\u5e76\u7ed3\u5408\u9002\u5f53\u7684\u9009\u62e9\u6a21\u578b\u53ef\u4ee5\u5b9e\u73b0\u66f4\u6709\u6548\u7684LLM\u5bf9\u9f50\uff0c\u5e76\u4e3a\u5c06\uff08\u6392\u5e8f\uff09\u9009\u62e9\u5efa\u6a21\u7eb3\u5165LLM\u8bad\u7ec3\u63d0\u4f9b\u4e86\u4e00\u4e2a\u901a\u7528\u4e14\u53ef\u6269\u5c55\u7684\u57fa\u7840\u3002"}}
{"id": "2510.23824", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23824", "abs": "https://arxiv.org/abs/2510.23824", "authors": ["Murad Ismayilov", "Edwin Meriaux", "Shuo Wen", "Gregory Dudek"], "title": "Decentralized Multi-Agent Goal Assignment for Path Planning using Large Language Models", "comment": "Accepted at MIT URTC 2025", "summary": "Coordinating multiple autonomous agents in shared environments under\ndecentralized conditions is a long-standing challenge in robotics and\nartificial intelligence. This work addresses the problem of decentralized goal\nassignment for multi-agent path planning, where agents independently generate\nranked preferences over goals based on structured representations of the\nenvironment, including grid visualizations and scenario data. After this\nreasoning phase, agents exchange their goal rankings, and assignments are\ndetermined by a fixed, deterministic conflict-resolution rule (e.g., agent\nindex ordering), without negotiation or iterative coordination. We\nsystematically compare greedy heuristics, optimal assignment, and large\nlanguage model (LLM)-based agents in fully observable grid-world settings. Our\nresults show that LLM-based agents, when provided with well-designed prompts\nand relevant quantitative information, can achieve near-optimal makespans and\nconsistently outperform traditional heuristics. These findings underscore the\npotential of language models for decentralized goal assignment in multi-agent\npath planning and highlight the importance of information structure in such\nsystems.", "AI": {"tldr": "\u5728\u53bb\u4e2d\u5fc3\u5316\u6761\u4ef6\u4e0b\u534f\u8c03\u5171\u4eab\u73af\u5883\u4e2d\u7684\u591a\u4e2a\u81ea\u4e3b\u4ee3\u7406\u662f\u673a\u5668\u4eba\u548c\u4eba\u5de5\u667a\u80fd\u4e2d\u7684\u4e00\u4e2a\u957f\u671f\u6311\u6218\u3002\u672c\u7814\u7a76\u89e3\u51b3\u4e86\u591a\u4ee3\u7406\u8def\u5f84\u89c4\u5212\u4e2d\u7684\u53bb\u4e2d\u5fc3\u5316\u76ee\u6807\u5206\u914d\u95ee\u9898\uff0c\u5176\u4e2d\u4ee3\u7406\u6839\u636e\u73af\u5883\u7684\u7ed3\u6784\u5316\u8868\u793a\uff08\u5305\u62ec\u7f51\u683c\u53ef\u89c6\u5316\u548c\u573a\u666f\u6570\u636e\uff09\u72ec\u7acb\u751f\u6210\u76ee\u6807\u6392\u5e8f\u504f\u597d\u3002\u5728\u6b64\u63a8\u7406\u9636\u6bb5\u4e4b\u540e\uff0c\u4ee3\u7406\u4ea4\u6362\u5176\u76ee\u6807\u6392\u540d\uff0c\u5e76\u901a\u8fc7\u56fa\u5b9a\u7684\u3001\u786e\u5b9a\u7684\u51b2\u7a81\u89e3\u51b3\u89c4\u5219\uff08\u4f8b\u5982\uff0c\u4ee3\u7406\u7d22\u5f15\u6392\u5e8f\uff09\u786e\u5b9a\u5206\u914d\uff0c\u800c\u65e0\u9700\u534f\u5546\u6216\u8fed\u4ee3\u534f\u8c03\u3002\u6211\u4eec\u5728\u5b8c\u5168\u53ef\u89c2\u5bdf\u7684\u7f51\u683c\u4e16\u754c\u73af\u5883\u4e2d\u7cfb\u7edf\u5730\u6bd4\u8f83\u4e86\u8d2a\u5a6a\u542f\u53d1\u5f0f\u3001\u6700\u4f18\u5206\u914d\u548c\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u4ee3\u7406\u3002\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8e LLM \u7684\u4ee3\u7406\u5728\u63d0\u4f9b\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63d0\u793a\u548c\u76f8\u5173\u5b9a\u91cf\u4fe1\u606f\u65f6\uff0c\u53ef\u4ee5\u5b9e\u73b0\u63a5\u8fd1\u6700\u4f18\u7684\u8de8\u65f6\u5b8c\u6210\u65f6\u95f4\uff0c\u5e76\u6301\u7eed\u4f18\u4e8e\u4f20\u7edf\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002\u8fd9\u4e9b\u53d1\u73b0\u5f3a\u8c03\u4e86\u8bed\u8a00\u6a21\u578b\u5728\u591a\u4ee3\u7406\u8def\u5f84\u89c4\u5212\u4e2d\u8fdb\u884c\u53bb\u4e2d\u5fc3\u5316\u76ee\u6807\u5206\u914d\u7684\u6f5c\u529b\uff0c\u5e76\u7a81\u663e\u4e86\u6b64\u7c7b\u7cfb\u7edf\u4e2d\u4fe1\u606f\u7ed3\u6784\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u5728\u53bb\u4e2d\u5fc3\u5316\u6761\u4ef6\u4e0b\u534f\u8c03\u5171\u4eab\u73af\u5883\u4e2d\u7684\u591a\u4e2a\u81ea\u4e3b\u4ee3\u7406\u662f\u673a\u5668\u4eba\u548c\u4eba\u5de5\u667a\u80fd\u4e2d\u7684\u4e00\u4e2a\u957f\u671f\u6311\u6218\u3002", "method": "\u4ee3\u7406\u6839\u636e\u73af\u5883\u7684\u7ed3\u6784\u5316\u8868\u793a\uff08\u5305\u62ec\u7f51\u683c\u53ef\u89c6\u5316\u548c\u573a\u666f\u6570\u636e\uff09\u72ec\u7acb\u751f\u6210\u76ee\u6807\u6392\u5e8f\u504f\u597d\u3002\u4e4b\u540e\uff0c\u4ee3\u7406\u4ea4\u6362\u5176\u76ee\u6807\u6392\u540d\uff0c\u5e76\u901a\u8fc7\u56fa\u5b9a\u7684\u3001\u786e\u5b9a\u7684\u51b2\u7a81\u89e3\u51b3\u89c4\u5219\uff08\u4f8b\u5982\uff0c\u4ee3\u7406\u7d22\u5f15\u6392\u5e8f\uff09\u786e\u5b9a\u5206\u914d\uff0c\u800c\u65e0\u9700\u534f\u5546\u6216\u8fed\u4ee3\u534f\u8c03\u3002\u6211\u4eec\u5728\u5b8c\u5168\u53ef\u89c2\u5bdf\u7684\u7f51\u683c\u4e16\u754c\u73af\u5883\u4e2d\u7cfb\u7edf\u5730\u6bd4\u8f83\u4e86\u8d2a\u5a6a\u542f\u53d1\u5f0f\u3001\u6700\u4f18\u5206\u914d\u548c\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u4ee3\u7406\u3002", "result": "\u57fa\u4e8e LLM \u7684\u4ee3\u7406\u5728\u63d0\u4f9b\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63d0\u793a\u548c\u76f8\u5173\u5b9a\u91cf\u4fe1\u606f\u65f6\uff0c\u53ef\u4ee5\u5b9e\u73b0\u63a5\u8fd1\u6700\u4f18\u7684\u8de8\u65f6\u5b8c\u6210\u65f6\u95f4\uff0c\u5e76\u6301\u7eed\u4f18\u4e8e\u4f20\u7edf\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u5f3a\u8c03\u4e86\u8bed\u8a00\u6a21\u578b\u5728\u591a\u4ee3\u7406\u8def\u5f84\u89c4\u5212\u4e2d\u8fdb\u884c\u53bb\u4e2d\u5fc3\u5316\u76ee\u6807\u5206\u914d\u7684\u6f5c\u529b\uff0c\u5e76\u7a81\u663e\u4e86\u6b64\u7c7b\u7cfb\u7edf\u4e2d\u4fe1\u606f\u7ed3\u6784\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2510.24360", "categories": ["cs.SI"], "pdf": "https://arxiv.org/pdf/2510.24360", "abs": "https://arxiv.org/abs/2510.24360", "authors": ["Kosti Koistinen", "Vesa Kuikka", "Kimmo Kaski"], "title": "Importance of Overlapping Network Nodes in Influence Spreading", "comment": "17 pages, 11 figures", "summary": "In complex networks there are overlapping substructures or \"circles\" that\nconsist of nodes belonging to multiple cohesive subgroups. Yet the role of\nthese overlapping nodes in influence spreading processes remains underexplored.\nIn the present study, we analyse networks with circle structures using a\nprobabilistic influence spreading model for processes of simple and complex\ncontagion. We quantify the roles of nodes using three metrics, i.e.,\nIn-Centrality, Out-Centrality, and Betweenness Centrality that represent the\nsusceptibility, spreading power, and mediatory role of nodes, respectively, and\nfind that at each stage of the spreading process the overlapping nodes\nconsistently exhibit greater influence than the non-overlapping ones.\nFurthermore, we observe that the criteria to define circles shape the\noverlapping effects. When we restrict our analysis to only largest circles, we\nfind that circles reflect not only node-level attributes but also of\ntopological importance. These findings clarify the distinction between local\nattribute-driven circles and global community structures, thus highlighting the\nstrategic importanc of overlapping nodes in spreading dynamics. This provides\nfoundation for future research on overlapping nodes in both circles and\ncommunities.", "AI": {"tldr": "\u91cd\u53e0\u8282\u70b9\u5728\u590d\u6742\u7f51\u7edc\u4e2d\u7684\u5f71\u54cd\u4f20\u64ad\u4e2d\u8d77\u7740\u5173\u952e\u4f5c\u7528\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u672a\u80fd\u5145\u5206\u63a2\u8ba8\u91cd\u53e0\u8282\u70b9\u5728\u5f71\u54cd\u4f20\u64ad\u8fc7\u7a0b\u4e2d\u7684\u4f5c\u7528\u3002", "method": "\u4f7f\u7528\u6982\u7387\u6027\u5f71\u54cd\u4f20\u64ad\u6a21\u578b\u5206\u6790\u5177\u6709\u793e\u56e2\u7ed3\u6784\u7684\u590d\u6742\u7f51\u7edc\uff0c\u5e76\u4f7f\u7528\u4e09\u79cd\u4e2d\u5fc3\u6027\u5ea6\u91cf\uff08\u5165\u4e2d\u5fc3\u6027\u3001\u51fa\u4e2d\u5fc3\u6027\u548c\u4e2d\u4ecb\u4e2d\u5fc3\u6027\uff09\u6765\u91cf\u5316\u8282\u70b9\u7684\u4f5c\u7528\u3002", "result": "\u91cd\u53e0\u8282\u70b9\u5728\u5f71\u54cd\u4f20\u64ad\u7684\u6bcf\u4e2a\u9636\u6bb5\u90fd\u6bd4\u975e\u91cd\u53e0\u8282\u70b9\u5177\u6709\u66f4\u5927\u7684\u5f71\u54cd\u529b\u3002\u6b64\u5916\uff0c\u793e\u56e2\u7684\u5b9a\u4e49\u65b9\u5f0f\u4f1a\u5f71\u54cd\u91cd\u53e0\u6548\u5e94\u3002\u5206\u6790\u6700\u5927\u7684\u793e\u56e2\u65f6\uff0c\u793e\u56e2\u4e0d\u4ec5\u53cd\u6620\u4e86\u8282\u70b9\u5c5e\u6027\uff0c\u8fd8\u53cd\u6620\u4e86\u62d3\u6251\u91cd\u8981\u6027\u3002", "conclusion": "\u91cd\u53e0\u8282\u70b9\u5728\u4f20\u64ad\u52a8\u6001\u4e2d\u5177\u6709\u6218\u7565\u91cd\u8981\u6027\uff0c\u533a\u5206\u4e86\u7531\u5c40\u90e8\u5c5e\u6027\u9a71\u52a8\u7684\u793e\u56e2\u548c\u5168\u5c40\u793e\u56e2\u7ed3\u6784\u3002"}}
{"id": "2510.24701", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.24701", "abs": "https://arxiv.org/abs/2510.24701", "authors": ["Tongyi DeepResearch Team", "Baixuan Li", "Bo Zhang", "Dingchu Zhang", "Fei Huang", "Guangyu Li", "Guoxin Chen", "Huifeng Yin", "Jialong Wu", "Jingren Zhou", "Kuan Li", "Liangcai Su", "Litu Ou", "Liwen Zhang", "Pengjun Xie", "Rui Ye", "Wenbiao Yin", "Xinmiao Yu", "Xinyu Wang", "Xixi Wu", "Xuanzhong Chen", "Yida Zhao", "Zhen Zhang", "Zhengwei Tao", "Zhongwang Zhang", "Zile Qiao", "Chenxi Wang", "Donglei Yu", "Gang Fu", "Haiyang Shen", "Jiayin Yang", "Jun Lin", "Junkai Zhang", "Kui Zeng", "Li Yang", "Hailong Yin", "Maojia Song", "Ming Yan", "Peng Xia", "Qian Xiao", "Rui Min", "Ruixue Ding", "Runnan Fang", "Shaowei Chen", "Shen Huang", "Shihang Wang", "Shihao Cai", "Weizhou Shen", "Xiaobin Wang", "Xin Guan", "Xinyu Geng", "Yingcheng Shi", "Yuning Wu", "Zhuo Chen", "Zijian Li", "Yong Jiang"], "title": "Tongyi DeepResearch Technical Report", "comment": "https://tongyi-agent.github.io/blog", "summary": "We present Tongyi DeepResearch, an agentic large language model, which is\nspecifically designed for long-horizon, deep information-seeking research\ntasks. To incentivize autonomous deep research agency, Tongyi DeepResearch is\ndeveloped through an end-to-end training framework that combines agentic\nmid-training and agentic post-training, enabling scalable reasoning and\ninformation seeking across complex tasks. We design a highly scalable data\nsynthesis pipeline that is fully automatic, without relying on costly human\nannotation, and empowers all training stages. By constructing customized\nenvironments for each stage, our system enables stable and consistent\ninteractions throughout. Tongyi DeepResearch, featuring 30.5 billion total\nparameters, with only 3.3 billion activated per token, achieves\nstate-of-the-art performance across a range of agentic deep research\nbenchmarks, including Humanity's Last Exam, BrowseComp, BrowseComp-ZH,\nWebWalkerQA, xbench-DeepSearch, FRAMES and xbench-DeepSearch-2510. We\nopen-source the model, framework, and complete solutions to empower the\ncommunity.", "AI": {"tldr": "Tongyi DeepResearch\u662f\u4e00\u4e2a305\u4ebf\u53c2\u6570\u7684\u81ea\u4e3b\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u7aef\u5230\u7aef\u8bad\u7ec3\u6846\u67b6\uff08\u5305\u62ec\u4e2d\u8bad\u7ec3\u548c\u540e\u8bad\u7ec3\uff09\u5b9e\u73b0\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\uff0c\u5728\u591a\u9879\u6df1\u5ea6\u7814\u7a76\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "motivation": "\u672c\u7814\u7a76\u7684\u52a8\u673a\u662f\u4e3a\u957f\u65f6\u3001\u6df1\u5ea6\u4fe1\u606f\u68c0\u7d22\u7814\u7a76\u4efb\u52a1\u8bbe\u8ba1\u4e00\u4e2a\u4e13\u95e8\u7684\u4ee3\u7406\u5f0f\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4ee5\u6fc0\u52b1\u81ea\u4e3b\u7684\u6df1\u5ea6\u7814\u7a76\u80fd\u529b\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528\u7aef\u5230\u7aef\u8bad\u7ec3\u6846\u67b6\uff0c\u7ed3\u5408\u4ee3\u7406\u5f0f\u4e2d\u8bad\u7ec3\u548c\u4ee3\u7406\u5f0f\u540e\u8bad\u7ec3\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5168\u81ea\u52a8\u3001\u53ef\u6269\u5c55\u7684\u6570\u636e\u5408\u6210\u6d41\u7a0b\uff0c\u4ee5\u652f\u6301\u6240\u6709\u8bad\u7ec3\u9636\u6bb5\u3002\u901a\u8fc7\u4e3a\u6bcf\u4e2a\u9636\u6bb5\u6784\u5efa\u5b9a\u5236\u5316\u73af\u5883\uff0c\u5b9e\u73b0\u4e86\u7a33\u5b9a\u4e00\u81f4\u7684\u4ea4\u4e92\u3002", "result": "Tongyi DeepResearch\u6a21\u578b\uff08305\u4ebf\u53c2\u6570\uff0c\u6bcf\u4e2atoken\u6fc0\u6d3b33\u4ebf\uff09\u5728\u5305\u62ecHumanity's Last Exam\u3001BrowseComp\u3001BrowseComp-ZH\u3001WebWalkerQA\u3001xbench-DeepSearch\u3001FRAMES\u548cxbench-DeepSearch-2510\u5728\u5185\u7684\u591a\u9879\u4ee3\u7406\u5f0f\u6df1\u5ea6\u7814\u7a76\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "Tongyi DeepResearch\u6a21\u578b\u901a\u8fc7\u5176\u521b\u65b0\u7684\u8bad\u7ec3\u6846\u67b6\u548c\u6570\u636e\u5408\u6210\u65b9\u6cd5\uff0c\u5728\u4ee3\u7406\u5f0f\u6df1\u5ea6\u7814\u7a76\u4efb\u52a1\u4e2d\u5c55\u73b0\u4e86\u5353\u8d8a\u7684\u6027\u80fd\uff0c\u5e76\u5df2\u5f00\u6e90\u4ee5\u4f9b\u793e\u533a\u4f7f\u7528\u3002"}}
{"id": "2510.23884", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23884", "abs": "https://arxiv.org/abs/2510.23884", "authors": ["Tananun Songdechakraiwut", "Michael Lutz"], "title": "Language Models for Longitudinal Clinical Prediction", "comment": null, "summary": "We explore a lightweight framework that adapts frozen large language models\nto analyze longitudinal clinical data. The approach integrates patient history\nand context within the language model space to generate accurate forecasts\nwithout model fine-tuning. Applied to neuropsychological assessments, it\nachieves accurate and reliable performance even with minimal training data,\nshowing promise for early-stage Alzheimer's monitoring.", "AI": {"tldr": "\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u53ef\u5728\u65e0\u9700\u6a21\u578b\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u6574\u5408\u60a3\u8005\u75c5\u53f2\u548c\u4e0a\u4e0b\u6587\uff0c\u5229\u7528\u51bb\u7ed3\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u7eb5\u5411\u4e34\u5e8a\u6570\u636e\u8fdb\u884c\u51c6\u786e\u9884\u6d4b\uff0c\u7279\u522b\u662f\u5728\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u65e9\u671f\u76d1\u6d4b\u65b9\u9762\u8868\u73b0\u51fa\u6f5c\u529b\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u9002\u5e94\u5df2\u51bb\u7ed3\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4ee5\u5206\u6790\u7eb5\u5411\u4e34\u5e8a\u6570\u636e\u3002", "method": "\u8be5\u65b9\u6cd5\u5c06\u60a3\u8005\u75c5\u53f2\u548c\u4e0a\u4e0b\u6587\u6574\u5408\u5230\u8bed\u8a00\u6a21\u578b\u7a7a\u95f4\u4e2d\uff0c\u4ee5\u751f\u6210\u51c6\u786e\u7684\u9884\u6d4b\uff0c\u800c\u65e0\u9700\u8fdb\u884c\u6a21\u578b\u5fae\u8c03\u3002", "result": "\u5728\u795e\u7ecf\u5fc3\u7406\u5b66\u8bc4\u4f30\u4e2d\u7684\u5e94\u7528\uff0c\u5373\u4f7f\u5728\u8bad\u7ec3\u6570\u636e\u5f88\u5c11\u7684\u60c5\u51b5\u4e0b\uff0c\u4e5f\u5b9e\u73b0\u4e86\u51c6\u786e\u53ef\u9760\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u65e9\u671f\u76d1\u6d4b\u65b9\u9762\u663e\u793a\u51fa\u6f5c\u529b\u3002"}}
{"id": "2510.24029", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.SY", "eess.SY", "q-bio.NC", "I.2.9; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.24029", "abs": "https://arxiv.org/abs/2510.24029", "authors": ["Andrew Gerstenslager", "Bekarys Dukenbaev", "Ali A. Minai"], "title": "Improved Accuracy of Robot Localization Using 3-D LiDAR in a Hippocampus-Inspired Model", "comment": "8 pages, 9 figures, Presented at the 2025 International Joint\n  Conference on Neural Networks, Rome, July 2025", "summary": "Boundary Vector Cells (BVCs) are a class of neurons in the brains of\nvertebrates that encode environmental boundaries at specific distances and\nallocentric directions, playing a central role in forming place fields in the\nhippocampus. Most computational BVC models are restricted to two-dimensional\n(2D) environments, making them prone to spatial ambiguities in the presence of\nhorizontal symmetries in the environment. To address this limitation, we\nincorporate vertical angular sensitivity into the BVC framework, thereby\nenabling robust boundary detection in three dimensions, and leading to\nsignificantly more accurate spatial localization in a biologically-inspired\nrobot model.\n  The proposed model processes LiDAR data to capture vertical contours, thereby\ndisambiguating locations that would be indistinguishable under a purely 2D\nrepresentation. Experimental results show that in environments with minimal\nvertical variation, the proposed 3D model matches the performance of a 2D\nbaseline; yet, as 3D complexity increases, it yields substantially more\ndistinct place fields and markedly reduces spatial aliasing. These findings\nshow that adding a vertical dimension to BVC-based localization can\nsignificantly enhance navigation and mapping in real-world 3D spaces while\nretaining performance parity in simpler, near-planar scenarios.", "AI": {"tldr": "BVC\u6a21\u578b\u5df2\u4ece2D\u6269\u5c55\u52303D\uff0c\u4ee5\u63d0\u9ad8\u7a7a\u95f4\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u5c24\u5176\u662f\u5728\u5177\u6709\u5782\u76f4\u53d8\u5316\u7684\u73af\u5883\u4e2d\u3002", "motivation": "\u73b0\u6709\u7684\u8ba1\u7b97BVC\u6a21\u578b\u4e3b\u8981\u9650\u4e8e2D\u73af\u5883\uff0c\u5728\u9762\u5bf9\u6c34\u5e73\u5bf9\u79f0\u6027\u65f6\u5bb9\u6613\u51fa\u73b0\u7a7a\u95f4\u6b67\u4e49\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5f15\u5165\u5782\u76f4\u89d2\u5ea6\u654f\u611f\u6027\u6765\u89e3\u51b3\u6b64\u9650\u5236\u3002", "method": "\u8be5\u6a21\u578b\u901a\u8fc7\u5904\u7406\u6fc0\u5149\u96f7\u8fbe\u6570\u636e\u6765\u6355\u83b7\u5782\u76f4\u8f6e\u5ed3\uff0c\u4ece\u800c\u5b9e\u73b03D\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u8fb9\u754c\u68c0\u6d4b\uff0c\u5e76\u88ab\u6574\u5408\u5230\u4e00\u4e2a\u53d7\u751f\u7269\u542f\u53d1\u7684\u673a\u5668\u4eba\u6a21\u578b\u4e2d\u3002", "result": "\u5728\u5177\u6709\u6700\u5c0f\u5782\u76f4\u53d8\u5316\u7684\u73af\u5883\u4e2d\uff0c3D\u6a21\u578b\u4e0e2D\u6a21\u578b\u6027\u80fd\u76f8\u5f53\u3002\u7136\u800c\uff0c\u5728\u66f4\u590d\u6742\u76843D\u73af\u5883\u4e2d\uff0c3D\u6a21\u578b\u4ea7\u751f\u4e86\u66f4\u6e05\u6670\u7684\u5730\u70b9\u573a\uff0c\u5e76\u663e\u8457\u51cf\u5c11\u4e86\u7a7a\u95f4\u6df7\u6dc6\u3002", "conclusion": "\u5c06\u5782\u76f4\u7ef4\u5ea6\u6dfb\u52a0\u5230\u57fa\u4e8eBVC\u7684\u5b9a\u4f4d\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u5728\u771f\u5b9e\u76843D\u7a7a\u95f4\u4e2d\u7684\u5bfc\u822a\u548c\u7ed8\u56fe\u80fd\u529b\uff0c\u540c\u65f6\u5728\u7b80\u5355\u7684\u3001\u8fd1\u4e4e\u5e73\u9762\u7684\u573a\u666f\u4e2d\u4fdd\u6301\u540c\u7b49\u7684\u6027\u80fd\u3002"}}
{"id": "2510.24229", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2510.24229", "abs": "https://arxiv.org/abs/2510.24229", "authors": ["Surender Kumar", "Stefan Velja", "Muhammad Sufyan Ramzan", "Caterina Cocchi"], "title": "Edge Magnetism in Colloidal MoS2 Triangular Nanoflakes", "comment": null, "summary": "The control of localized magnetic domains at the nanoscale holds great\npromise for next-generation spintronic applications. Colloidal transition metal\ndichalcogenides nanostructures are experimentally accessible and chemically\ntunable platforms for spintronics, deserving dedicated research to assess their\npotential. Here, we investigate from first principles free-standing triangular\nMoS2 nanoflakes with sulfur-terminated, hydrogen-passivated edges, to probe\nintrinsic spin behavior at varying side lengths. We find a critical edge length\nof approximately 1.5 nm separating nonmagnetic nanoflakes from larger ones with\na magnetic ground state emerging from several, energetically competing spin\nconfigurations. In these systems, the magnetic activity is not uniformly\ndistributed along the edges but localized on specific \"magnetic islands\" around\nmolybdenum edge atoms. The localization of magnetic moments is robust even in\nnon-equilateral nanoflake geometries, highlighting their intrinsic stability\nregardless of the (high) symmetry of the hosting structure. These findings\nestablish that the S-terminated, H-passivated triangular MoS2 nanoflakes are a\nstable and experimentally accessible platform via colloidal synthesis for\nlow-dimensional, next-generation spintronic devices.", "AI": {"tldr": "MoS2\u7eb3\u7c73\u7247\u5728\u7ea61.5nm\u7684\u4e34\u754c\u8fb9\u957f\u4e0b\u8868\u73b0\u51fa\u78c1\u6027\uff0c\u78c1\u77e9\u5c40\u57df\u5728\u94bc\u8fb9\u7f18\u539f\u5b50\u5468\u56f4\u7684\u201c\u78c1\u5c9b\u201d\u4e0a\uff0c\u9002\u7528\u4e8e\u7814\u53d1\u4e0b\u4e00\u4ee3\u81ea\u65cb\u7535\u5b50\u5668\u4ef6\u3002", "motivation": "\u7814\u7a76\u80f6\u4f53\u8fc7\u6e21\u91d1\u5c5e\u4e8c\u5364\u5316\u7269\u7684\u7eb3\u7c73\u7ed3\u6784\u5728\u81ea\u65cb\u7535\u5b50\u5b66\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u4f7f\u7528\u7b2c\u4e00\u6027\u539f\u7406\u8ba1\u7b97\u7814\u7a76\u4e86\u4e0d\u540c\u8fb9\u957f\u7684\u81ea\u7531\u60ac\u6d6e\u4e09\u89d2\u5f62MoS2\u7eb3\u7c73\u7247\uff08\u786b\u7ec8\u6b62\u3001\u6c22\u949d\u5316\u8fb9\u7f18\uff09\u7684\u672c\u5f81\u81ea\u65cb\u884c\u4e3a\u3002", "result": "\u53d1\u73b0\u4e34\u754c\u8fb9\u957f\u7ea6\u4e3a1.5nm\uff0c\u5c0f\u4e8e\u8be5\u8fb9\u957f\u7684\u7eb3\u7c73\u7247\u662f\u975e\u78c1\u6027\u7684\uff0c\u5927\u4e8e\u8be5\u8fb9\u957f\u7684\u7eb3\u7c73\u7247\u5177\u6709\u7531\u591a\u4e2a\u80fd\u91cf\u4e0a\u7ade\u4e89\u7684\u81ea\u65cb\u6784\u578b\u4ea7\u751f\u7684\u78c1\u57fa\u6001\u3002\u78c1\u6d3b\u6027\u5c40\u57df\u5728\u94bc\u8fb9\u7f18\u539f\u5b50\u7684\u201c\u78c1\u5c9b\u201d\u4e0a\uff0c\u4e14\u5728\u975e\u7b49\u8fb9\u7eb3\u7c73\u7247\u4e2d\u4e5f\u8868\u73b0\u51fa\u7a33\u5b9a\u6027\u3002", "conclusion": "S\u7ec8\u6b62\u3001H\u949d\u5316\u7684\u4e09\u89d2\u5f62MoS2\u7eb3\u7c73\u7247\u662f\u7a33\u5b9a\u4e14\u53ef\u901a\u8fc7\u80f6\u4f53\u5408\u6210\u8fdb\u884c\u5b9e\u9a8c\u7684\u5e73\u53f0\uff0c\u53ef\u7528\u4e8e\u4f4e\u7ef4\u7684\u4e0b\u4e00\u4ee3\u81ea\u65cb\u7535\u5b50\u5668\u4ef6\u3002"}}
{"id": "2510.24641", "categories": ["cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2510.24641", "abs": "https://arxiv.org/abs/2510.24641", "authors": ["Lucky Donald Lyngdoh Kynshi", "Umang Soni", "Chithra H Sharma", "Yu Cheng", "Kristian Deneke", "Robert Zierold", "Shengqiang Zhou", "Robert H Blick", "Anil Shaji", "Madhu Thalakulam"], "title": "Density-driven scattering and valley splitting in undoped Si/SiGe two-dimensional electron system", "comment": "22 pages", "summary": "Undoped Si-SiGe two-dimensional electron gas (2DEG) provide an ideal platform\nfor hosting quantum-dot spin-qubits owing enhanced spin dephasing times and\ncompatibility with standard CMOS technology. The strained Si quantum well\nreduces the valley degeneracy into two closely spaced ones. The existence of a\nnear-degenerate valley state act as a leakage channel and compromises gate\nfidelity. A robust and uniform valley splitting across the entire chip is\ncrucial for achieving scalability in the architecture and reliability in\noperation. Imperfections such as broadened interfaces, alloy disorders and\natomic steps significantly compromise the valley splitting. The associated\nscattering mechanisms play detrimental roles in the performance of the qubits.\nIn this manuscript, exploiting low-temperature magnetotransport measurements,\nwe investigate the scattering mechanisms and valley splitting in a\nhigh-mobility undoped Si-SiGe 2DEG. At lower carrier densities, transport is\nlimited by remote impurity scattering, whereas at higher densities, background\nimpurity scattering near the quantum well dominates. Both the transport and\nquantum lifetimes of the charge carriers increase with carrier concentration,\ndue to the enhancement in the impurity screening. Magnetic-field-induced\nconfinement effect also is found to improve the valley splitting.\nCurrent-biasing measurements reveals the role of carrier heating in the\nvisibility of valley splitting and reveal a temperature limited valley\nsplitting of approximately 100 micro-eV. These results provide critical insight\ninto scattering-dominated regimes and valley splitting in undoped Si-SiGe,\nadvancing its potential for silicon-based quantum devices.", "AI": {"tldr": "Si-SiGe\u4e8c\u7ef4\u7535\u5b50\u6c14\u4e2d\u7684\u91cf\u5b50\u70b9\u5728\u91cf\u5b50\u8ba1\u7b97\u4e2d\u6709\u5e94\u7528\u524d\u666f\uff0c\u4f46\u9700\u8981\u89e3\u51b3\u8f7d\u6d41\u5b50\u6563\u5c04\u548c\u91cf\u5b50\u6bd4\u7279\u64cd\u63a7\u4fdd\u771f\u5ea6\u95ee\u9898\u3002\u8be5\u7814\u7a76\u901a\u8fc7\u4f4e\u6e29\u78c1\u8f93\u8fd0\u6d4b\u91cf\uff0c\u6df1\u5165\u5206\u6790\u4e86\u6563\u5c04\u673a\u5236\u548c\u5288\u88c2\uff0c\u5e76\u63d0\u51fa\u4e86\u63d0\u9ad8\u91cf\u5b50\u6bd4\u7279\u6027\u80fd\u7684\u65b9\u6cd5\u3002", "motivation": "Si-SiGe\u4e8c\u7ef4\u7535\u5b50\u6c14\uff082DEG\uff09\u662f\u5b9e\u73b0\u91cf\u5b50\u70b9\u81ea\u65cb\u91cf\u5b50\u6bd4\u7279\u7684\u7406\u60f3\u5e73\u53f0\uff0c\u4f46\u5176\u91cf\u5b50\u6bd4\u7279\u7684\u6027\u80fd\u53d7\u5230\u8fd1\u7b80\u5e76\u7684\u8c37\u6001\u4f5c\u4e3a\u6cc4\u6f0f\u901a\u9053\u7684\u5f71\u54cd\uff0c\u8fdb\u800c\u5f71\u54cd\u95e8\u4fdd\u771f\u5ea6\u3002\u56e0\u6b64\uff0c\u5b9e\u73b0\u5668\u4ef6\u8303\u56f4\u5185\u5747\u5300\u7684\u8c37\u5288\u88c2\u5bf9\u4e8e\u63d0\u9ad8\u53ef\u6269\u5c55\u6027\u548c\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5229\u7528\u4f4e\u6e29\u78c1\u8f93\u8fd0\u6d4b\u91cf\uff0c\u7814\u7a76\u4e86\u9ad8\u8fc1\u79fb\u7387\u672a\u63ba\u6742Si-SiGe 2DEG\u4e2d\u7684\u6563\u5c04\u673a\u5236\u548c\u8c37\u5288\u88c2\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u8f83\u4f4e\u8f7d\u6d41\u5b50\u5bc6\u5ea6\u4e0b\uff0c\u8fdc\u7a0b\u6742\u8d28\u6563\u5c04\u662f\u4e3b\u8981\u7684\u9650\u5236\u56e0\u7d20\uff1b\u5728\u8f83\u9ad8\u5bc6\u5ea6\u4e0b\uff0c\u91cf\u5b50\u9631\u9644\u8fd1\u7684\u80cc\u666f\u6742\u8d28\u6563\u5c04\u8d77\u4e3b\u5bfc\u4f5c\u7528\u3002\u8f7d\u6d41\u5b50\u6d53\u5ea6\u589e\u52a0\u53ef\u63d0\u9ad8\u4f20\u8f93\u548c\u91cf\u5b50\u5bff\u547d\uff0c\u8fd9\u5f52\u56e0\u4e8e\u6742\u8d28\u5c4f\u853d\u7684\u589e\u5f3a\u3002\u78c1\u573a\u8bf1\u5bfc\u7684\u7ea6\u675f\u6548\u5e94\u4e5f\u80fd\u6539\u5584\u8c37\u5288\u88c2\u3002\u901a\u8fc7\u7535\u6d41\u504f\u7f6e\u6d4b\u91cf\uff0c\u63ed\u793a\u4e86\u8f7d\u6d41\u5b50\u52a0\u70ed\u5bf9\u8c37\u5288\u88c2\u53ef\u89c1\u6027\u7684\u5f71\u54cd\uff0c\u5e76\u6d4b\u5f97\u7ea6100\u5fae\u7535\u5b50\u4f0f\u7279\u7684\u6e29\u5ea6\u9650\u5236\u8c37\u5288\u88c2\u3002", "conclusion": "\u8be5\u7814\u7a76\u6df1\u5165\u9610\u660e\u4e86\u672a\u63ba\u6742Si-SiGe 2DEG\u4e2d\u7684\u6563\u5c04\u673a\u5236\u548c\u8c37\u5288\u88c2\uff0c\u4e3a\u57fa\u4e8e\u7845\u7684\u91cf\u5b50\u5668\u4ef6\u7684\u5f00\u53d1\u63d0\u4f9b\u4e86\u5173\u952e\u89c1\u89e3\u3002"}}
{"id": "2510.23930", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23930", "abs": "https://arxiv.org/abs/2510.23930", "authors": ["Xirui Jin", "Renbiao Jin", "Boying Li", "Danping Zou", "Wenxian Yu"], "title": "PlanarGS: High-Fidelity Indoor 3D Gaussian Splatting Guided by Vision-Language Planar Priors", "comment": "Accepted by NeurIPS 2025. Project page: https://planargs.github.io", "summary": "Three-dimensional Gaussian Splatting (3DGS) has recently emerged as an\nefficient representation for novel-view synthesis, achieving impressive visual\nquality. However, in scenes dominated by large and low-texture regions, common\nin indoor environments, the photometric loss used to optimize 3DGS yields\nambiguous geometry and fails to recover high-fidelity 3D surfaces. To overcome\nthis limitation, we introduce PlanarGS, a 3DGS-based framework tailored for\nindoor scene reconstruction. Specifically, we design a pipeline for\nLanguage-Prompted Planar Priors (LP3) that employs a pretrained vision-language\nsegmentation model and refines its region proposals via cross-view fusion and\ninspection with geometric priors. 3D Gaussians in our framework are optimized\nwith two additional terms: a planar prior supervision term that enforces planar\nconsistency, and a geometric prior supervision term that steers the Gaussians\ntoward the depth and normal cues. We have conducted extensive experiments on\nstandard indoor benchmarks. The results show that PlanarGS reconstructs\naccurate and detailed 3D surfaces, consistently outperforming state-of-the-art\nmethods by a large margin. Project page: https://planargs.github.io", "AI": {"tldr": "PlanarGS\u662f\u4e00\u79cd\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\u7684\u5ba4\u5185\u573a\u666f\u91cd\u5efa\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u8bed\u8a00\u63d0\u793a\u7684\u5e73\u9762\u5148\u9a8c\uff08LP3\uff09\u6765\u89e3\u51b3\u4f20\u7edf3DGS\u5728\u4f4e\u7eb9\u7406\u533a\u57df\u7684\u51e0\u4f55\u6a21\u7cca\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5e73\u9762\u4e00\u81f4\u6027\u548c\u51e0\u4f55\u7ebf\u7d22\u8fdb\u884c\u4f18\u5316\uff0c\u5728\u5ba4\u5185\u573a\u666f\u91cd\u5efa\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6210\u679c\u3002", "motivation": "\u73b0\u6709\u4e09\u7ef4\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u65b9\u6cd5\u5728\u5904\u7406\u5927\u7247\u4f4e\u7eb9\u7406\u533a\u57df\uff08\u5e38\u89c1\u4e8e\u5ba4\u5185\u73af\u5883\uff09\u65f6\uff0c\u7531\u4e8e\u4f9d\u8d56\u7684\u5149\u5ea6\u635f\u5931\u4f1a\u4ea7\u751f\u6a21\u7cca\u7684\u51e0\u4f55\u5f62\u72b6\uff0c\u96be\u4ee5\u6062\u590d\u9ad8\u4fdd\u771f\u5ea6\u7684\u4e09\u7ef4\u8868\u9762\u3002", "method": "PlanarGS\u6846\u67b6\u5229\u7528\u8bed\u8a00\u63d0\u793a\u7684\u5e73\u9762\u5148\u9a8c\uff08LP3\uff09\u6280\u672f\uff0c\u7ed3\u5408\u9884\u8bad\u7ec3\u7684\u89c6\u89c9-\u8bed\u8a00\u5206\u5272\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u8de8\u89c6\u56fe\u878d\u5408\u548c\u51e0\u4f55\u5148\u9a8c\u8fdb\u884c\u7ec6\u5316\u3002\u540c\u65f6\uff0c\u5728\u4f18\u53163D\u9ad8\u65af\u65f6\u5f15\u5165\u4e86\u5e73\u9762\u5148\u9a8c\u76d1\u7763\u9879\u548c\u51e0\u4f55\u5148\u9a8c\u76d1\u7763\u9879\uff0c\u4ee5\u589e\u5f3a\u5e73\u9762\u7684\u8fde\u7eed\u6027\u548c\u5f15\u5bfc\u9ad8\u65af\u5411\u6df1\u5ea6\u53ca\u6cd5\u7ebf\u7ebf\u7d22\u9760\u62e2\u3002", "result": "\u5728\u6807\u51c6\u5ba4\u5185\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cPlanarGS\u80fd\u591f\u91cd\u5efa\u7cbe\u786e\u4e14\u7ec6\u8282\u4e30\u5bcc\u7684\u4e09\u7ef4\u8868\u9762\uff0c\u5176\u6027\u80fd\u6301\u7eed\u5927\u5e45\u8d85\u8d8a\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "PlanarGS\u901a\u8fc7\u5f15\u5165LP3\u548c\u51e0\u4f55\u5148\u9a8c\uff0c\u6709\u6548\u89e3\u51b3\u4e863DGS\u5728\u5ba4\u5185\u4f4e\u7eb9\u7406\u573a\u666f\u4e0b\u7684\u91cd\u5efa\u96be\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u4e09\u7ef4\u8868\u9762\u91cd\u5efa\u3002"}}
{"id": "2510.23815", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.23815", "abs": "https://arxiv.org/abs/2510.23815", "authors": ["Iagoba Apellaniz", "Manuel Gessner", "G\u00e9za T\u00f3th"], "title": "Differential magnetometry with partially flipped Dicke states", "comment": "15 page, 2 figures, revtex 4.2", "summary": "We study magnetometry of gradients and homogeneous background fields along\nall three spatial axes using two spatially separated spin ensembles. We derive\ntrade-off relations for the achievable estimation precision of these\nparameters. Dicke states, optimal for homogeneous field estimation, can be\nlocally rotated into states sensitive to magnetic gradients by rotating the\nspins in one subensemble. We determine bounds for the precision for gradient\nmetrology in the three orthogonal directions as a function of the sensitivities\nof the homogenous field in those directions. The resulting partially flipped\nDicke state saturates the bounds above, showing similar sensitivity in two\ndirections but significantly reduced sensitivity in the third. Exploiting\nentanglement between the two ensembles, this state achieves roughly twice the\nprecision attainable by the best bipartite separable state, which is a product\nof local Dicke states. For small ensembles, we explicitly identify measurement\noperators saturating the quantum Cram\\'er-Rao bound, while for larger\nensembles, we propose simpler but suboptimal schemes. In both cases, the\ngradient is estimated from second moments and correlations of angular momentum\noperators. Our results demonstrate how the metrological properties of Dicke\nstates can be exploited for quantum-enhanced multiparameter estimation.", "AI": {"tldr": "\u5229\u7528\u4e24\u4e2a\u7a7a\u95f4\u4e0a\u5206\u79bb\u7684\u81ea\u65cb\u7cfb\u7efc\uff0c\u7814\u7a76\u4e86\u6240\u6709\u4e09\u4e2a\u7a7a\u95f4\u8f74\u4e0a\u7684\u78c1\u573a\u68af\u5ea6\u548c\u5747\u5300\u80cc\u666f\u573a\u7684\u78c1\u589e\u5f3a\u6d4b\u91cf\u6280\u672f\uff0c\u5e76\u63a8\u5bfc\u4e86\u53ef\u5b9e\u73b0\u53c2\u6570\u4f30\u8ba1\u7cbe\u5ea6\u4e4b\u95f4\u7684\u6743\u8861\u5173\u7cfb\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u7814\u7a76\u5982\u4f55\u5229\u7528\u81ea\u65cb\u7cfb\u7efc\u8fdb\u884c\u78c1\u573a\u6d4b\u91cf\uff0c\u7279\u522b\u662f\u78c1\u573a\u68af\u5ea6\u548c\u80cc\u666f\u573a\uff0c\u5e76\u63a2\u7d22\u63d0\u9ad8\u6d4b\u91cf\u7cbe\u5ea6\u7684\u91cf\u5b50\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u4e24\u4e2a\u7a7a\u95f4\u4e0a\u5206\u79bb\u7684\u81ea\u65cb\u7cfb\u7efc\uff0c\u63a8\u5bfc\u4e86\u78c1\u573a\u68af\u5ea6\u548c\u80cc\u666f\u573a\u4f30\u8ba1\u7cbe\u5ea6\u4e4b\u95f4\u7684\u6743\u8861\u5173\u7cfb\u3002\u901a\u8fc7\u65cb\u8f6c\u4e00\u4e2a\u5b50\u7cfb\u7efc\u4e2d\u7684\u81ea\u65cb\uff0c\u5c06\u6700\u4f18\u7684\u5747\u5300\u573a\u4f30\u8ba1\u6001\uff08Dicke\u6001\uff09\u8f6c\u53d8\u4e3a\u5bf9\u78c1\u573a\u68af\u5ea6\u654f\u611f\u7684\u6001\u3002\u786e\u5b9a\u4e86\u68af\u5ea6\u6d4b\u91cf\u7cbe\u5ea6\u5728\u4e09\u4e2a\u6b63\u4ea4\u65b9\u5411\u4e0a\u7684\u7cbe\u5ea6\u8fb9\u754c\uff0c\u5e76\u63d0\u51fa\u4e86\u90e8\u5206\u7ffb\u8f6c\u7684Dicke\u6001\uff0c\u8be5\u6001\u5728\u4e24\u4e2a\u65b9\u5411\u4e0a\u5b9e\u73b0\u4e86\u4e0e\u68af\u5ea6\u6d4b\u91cf\u7cbe\u5ea6\u8fb9\u754c\u76f8\u5f53\u7684\u7075\u654f\u5ea6\uff0c\u4f46\u5728\u7b2c\u4e09\u4e2a\u65b9\u5411\u4e0a\u7075\u654f\u5ea6\u663e\u8457\u964d\u4f4e\u3002", "result": "\u90e8\u5206\u7ffb\u8f6c\u7684Dicke\u6001\u5728\u4e24\u4e2a\u65b9\u5411\u4e0a\u5b9e\u73b0\u4e86\u4e0e\u7cbe\u5ea6\u8fb9\u754c\u76f8\u5f53\u7684\u7075\u654f\u5ea6\uff0c\u4f46\u5728\u7b2c\u4e09\u4e2a\u65b9\u5411\u4e0a\u663e\u8457\u964d\u4f4e\u3002\u4e0e\u6700\u4f73\u53ef\u5206\u6001\u76f8\u6bd4\uff0c\u8be5\u72b6\u6001\u7684\u7cbe\u5ea6\u5927\u7ea6\u63d0\u9ad8\u4e86\u4e24\u500d\u3002\u5bf9\u4e8e\u5c0f\u7cfb\u7efc\uff0c\u663e\u5f0f\u786e\u5b9a\u4e86\u9971\u548c\u91cf\u5b50Cram\u00e9r-Rao\u754c\u7684\u6d4b\u91cf\u7b97\u7b26\uff1b\u5bf9\u4e8e\u5927\u7cfb\u7efc\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u7b80\u5355\u4f46\u6b21\u4f18\u7684\u65b9\u6848\u3002\u4e24\u79cd\u60c5\u51b5\u4e0b\uff0c\u68af\u5ea6\u5747\u7531\u89d2\u52a8\u91cf\u7b97\u7b26\u7684\u4e8c\u9636\u77e9\u548c\u76f8\u5173\u6027\u4f30\u8ba1\u5f97\u5230\u3002", "conclusion": "\u672c\u6587\u5c55\u793a\u4e86\u5982\u4f55\u5229\u7528Dicke\u6001\u7684\u6d4b\u91cf\u6027\u8d28\uff0c\u901a\u8fc7\u91cf\u5b50\u589e\u5f3a\u7684\u591a\u53c2\u6570\u4f30\u8ba1\u6280\u672f\uff0c\u5b9e\u73b0\u5bf9\u78c1\u573a\u68af\u5ea6\u7684\u9ad8\u7cbe\u5ea6\u6d4b\u91cf\u3002"}}
{"id": "2510.23682", "categories": ["cs.LG", "cs.AI", "cs.LO", "cs.SE", "I.2.11; I.2.6; I.2.8"], "pdf": "https://arxiv.org/pdf/2510.23682", "abs": "https://arxiv.org/abs/2510.23682", "authors": ["Gokturk Aytug Akarlar"], "title": "Beyond Prompt Engineering: Neuro-Symbolic-Causal Architecture for Robust Multi-Objective AI Agents", "comment": "35 pages, 15 figures, 2 tables. Keywords: Large Language Models,\n  Autonomous Agents, Neuro-Symbolic AI, Causal Inference, Formal Verification,\n  Multi-Objective Optimization. Open-source code and interactive demo available", "summary": "Large language models show promise as autonomous decision-making agents, yet\ntheir deployment in high-stakes domains remains fraught with risk. Without\narchitectural safeguards, LLM agents exhibit catastrophic brittleness:\nidentical capabilities produce wildly different outcomes depending solely on\nprompt framing. We present Chimera, a neuro-symbolic-causal architecture that\nintegrates three complementary components - an LLM strategist, a formally\nverified symbolic constraint engine, and a causal inference module for\ncounterfactual reasoning. We benchmark Chimera against baseline architectures\n(LLM-only, LLM with symbolic constraints) across 52-week simulations in a\nrealistic e-commerce environment featuring price elasticity, trust dynamics,\nand seasonal demand. Under organizational biases toward either volume or margin\noptimization, LLM-only agents fail catastrophically (total loss of \\$99K in\nvolume scenarios) or destroy brand trust (-48.6% in margin scenarios). Adding\nsymbolic constraints prevents disasters but achieves only 43-87% of Chimera's\nprofit. Chimera consistently delivers the highest returns (\\$1.52M and \\$1.96M\nrespectively, some cases +\\$2.2M) while improving brand trust (+1.8% and\n+10.8%, some cases +20.86%), demonstrating prompt-agnostic robustness. Our TLA+\nformal verification proves zero constraint violations across all scenarios.\nThese results establish that architectural design not prompt engineering\ndetermines the reliability of autonomous agents in production environments. We\nprovide open-source implementations and interactive demonstrations for\nreproducibility.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u4f5c\u4e3a\u81ea\u4e3b\u51b3\u7b56\u4ee3\u7406\u65f6\u8868\u73b0\u51fa\u201c\u707e\u96be\u6027\u8106\u5f31\u6027\u201d\uff0c\u5373\u4f7f\u662f\u76f8\u540c\u7684\u80fd\u529b\u4e5f\u4f1a\u56e0\u63d0\u793a\u63aa\u8f9e\u7684\u4e0d\u540c\u800c\u4ea7\u751f\u622a\u7136\u4e0d\u540c\u7684\u7ed3\u679c\u3002\u672c\u6587\u63d0\u51fa\u7684 Chimera \u67b6\u6784\uff0c\u4e00\u79cd\u7ed3\u5408\u4e86 LLM \u7b56\u7565\u3001\u7b26\u53f7\u7ea6\u675f\u5f15\u64ce\u548c\u56e0\u679c\u63a8\u7406\u6a21\u5757\u7684\u795e\u7ecf-\u7b26\u53f7-\u56e0\u679c\u67b6\u6784\uff0c\u89e3\u51b3\u4e86\u8fd9\u4e2a\u95ee\u9898\u3002\u5728\u6a21\u62df\u7684\u7535\u5b50\u5546\u52a1\u73af\u5883\u4e2d\uff0cChimera \u5728\u5404\u79cd\u7ec4\u7ec7\u504f\u89c1\u4e0b\uff0c\u4e0e\u4ec5\u4f7f\u7528 LLM \u6216 LLM \u52a0\u7b26\u53f7\u7ea6\u675f\u7684\u57fa\u7ebf\u67b6\u6784\u76f8\u6bd4\uff0c\u59cb\u7ec8\u80fd\u5e26\u6765\u66f4\u9ad8\u7684\u56de\u62a5\u5e76\u63d0\u9ad8\u54c1\u724c\u4fe1\u4efb\u5ea6\uff0c\u540c\u65f6\u4fdd\u8bc1\u96f6\u7ea6\u675f\u8fdd\u53cd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u4f5c\u4e3a\u81ea\u4e3b\u51b3\u7b56\u4ee3\u7406\u65f6\u5b58\u5728\u201c\u707e\u96be\u6027\u8106\u5f31\u6027\u201d\u95ee\u9898\uff0c\u5373\u76f8\u540c\u7684\u80fd\u529b\u53ef\u80fd\u56e0\u63d0\u793a\u63aa\u8f9e\u7684\u4e0d\u540c\u800c\u4ea7\u751f\u622a\u7136\u4e0d\u540c\u7684\u7ed3\u679c\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u5728\u9ad8\u98ce\u9669\u9886\u57df\u7684\u5e94\u7528\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u89e3\u51b3\u8fd9\u79cd\u8106\u5f31\u6027\u5e76\u63d0\u9ad8 LLM \u4ee3\u7406\u53ef\u9760\u6027\u7684\u67b6\u6784\u3002", "method": "\u63d0\u51fa\u5e76\u5b9e\u73b0\u4e86\u4e00\u4e2a\u540d\u4e3a Chimera \u7684\u795e\u7ecf-\u7b26\u53f7-\u56e0\u679c\u67b6\u6784\uff0c\u8be5\u67b6\u6784\u96c6\u6210\u4e86\u4e09\u4e2a\u7ec4\u4ef6\uff1a\u4e00\u4e2a LLM \u7b56\u7565\u6a21\u5757\u3001\u4e00\u4e2a\u7ecf\u8fc7\u5f62\u5f0f\u9a8c\u8bc1\u7684\u7b26\u53f7\u7ea6\u675f\u5f15\u64ce\u548c\u4e00\u4e2a\u7528\u4e8e\u53cd\u4e8b\u5b9e\u63a8\u7406\u7684\u56e0\u679c\u63a8\u7406\u6a21\u5757\u3002\u5728\u5305\u542b\u4ef7\u683c\u5f39\u6027\u3001\u4fe1\u4efb\u52a8\u6001\u548c\u5b63\u8282\u6027\u9700\u6c42\u7684\u73b0\u5b9e\u7535\u5b50\u5546\u52a1\u73af\u5883\u4e2d\uff0c\u8fdb\u884c\u4e86\u957f\u8fbe 52 \u5468\u7684\u6a21\u62df\uff0c\u5e76\u5c06 Chimera \u4e0e\u4ec5\u4f7f\u7528 LLM \u548c\u7ed3\u5408\u7b26\u53f7\u7ea6\u675f\u7684 LLM \u57fa\u7ebf\u67b6\u6784\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u5728\u7ec4\u7ec7\u504f\u5411\u4e8e\u9500\u91cf\u6216\u5229\u6da6\u6700\u5927\u5316\u7684\u504f\u89c1\u4e0b\uff0c\u4ec5\u4f7f\u7528 LLM \u7684\u4ee3\u7406\u5728\u9500\u91cf\u573a\u666f\u4e2d\u635f\u5931\u4e86 99,000 \u7f8e\u5143\u7684\u9500\u91cf\uff0c\u5728\u5229\u6da6\u573a\u666f\u4e2d\u54c1\u724c\u4fe1\u4efb\u5ea6\u4e0b\u964d\u4e86 48.6%\u3002\u52a0\u5165\u7b26\u53f7\u7ea6\u675f\u53ef\u4ee5\u9632\u6b62\u707e\u96be\uff0c\u4f46\u5229\u6da6\u4ec5\u8fbe\u5230 Chimera \u7684 43-87%\u3002Chimera \u6301\u7eed\u83b7\u5f97\u6700\u9ad8\u56de\u62a5\uff08\u5206\u522b\u4e3a 152 \u4e07\u7f8e\u5143\u548c 196 \u4e07\u7f8e\u5143\uff0c\u90e8\u5206\u6848\u4f8b\u8d85\u8fc7 220 \u4e07\u7f8e\u5143\uff09\uff0c\u540c\u65f6\u63d0\u9ad8\u54c1\u724c\u4fe1\u4efb\u5ea6\uff08\u5206\u522b\u4e3a 1.8% \u548c 10.8%\uff0c\u90e8\u5206\u6848\u4f8b\u8fbe\u5230 20.86%\uff09\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u5728\u5404\u79cd\u63d0\u793a\u4e0b\u90fd\u5177\u6709\u9c81\u68d2\u6027\u3002TLA+ \u5f62\u5f0f\u9a8c\u8bc1\u8bc1\u660e\u4e86\u5728\u6240\u6709\u573a\u666f\u4e2d\u96f6\u7ea6\u675f\u8fdd\u53cd\u3002", "conclusion": "\u67b6\u6784\u8bbe\u8ba1\uff08\u800c\u975e\u63d0\u793a\u5de5\u7a0b\uff09\u51b3\u5b9a\u4e86\u751f\u4ea7\u73af\u5883\u4e2d\u81ea\u4e3b\u4ee3\u7406\u7684\u53ef\u9760\u6027\u3002Chimera \u67b6\u6784\u901a\u8fc7\u7ed3\u5408 LLM\u3001\u7b26\u53f7\u7ea6\u675f\u548c\u56e0\u679c\u63a8\u7406\uff0c\u5b9e\u73b0\u4e86\u6bd4\u4f20\u7edf\u65b9\u6cd5\u66f4\u9ad8\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\uff0c\u5e76\u4fdd\u8bc1\u4e86\u7ea6\u675f\u7684\u9075\u5b88\u3002"}}
{"id": "2510.23922", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.23922", "abs": "https://arxiv.org/abs/2510.23922", "authors": ["Shashank Dhananjay Vyas", "Satadru Dey"], "title": "Secure Control of Connected and Autonomous Electrified Vehicles Under Adversarial Cyber-Attacks", "comment": null, "summary": "Connected and Autonomous Electrified Vehicles (CAEV) is the solution to the\nfuture smart mobility having benefits of efficient traffic flow and cleaner\nenvironmental impact. Although CAEV has advantages they are still susceptible\nto adversarial cyber attacks due to their autonomous electric operation and the\ninvolved connectivity. To alleviate this issue, we propose a secure control\narchitecture of CAEV. Particularly, we design an additional control input using\nReinforcement Learning (RL) to be applied to the vehicle powertrain along with\nthe input commanded by the battery. We present simulation case studies to\ndemonstrate the potential of the proposed approach in keeping the CAEV platoon\noperating safely without collisions by curbing the effect of adversarial\nattacks.", "AI": {"tldr": "CAEVs\u9762\u4e34\u7f51\u7edc\u653b\u51fb\u98ce\u9669\uff0c\u63d0\u51fa\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u5b89\u5168\u63a7\u5236\u67b6\u6784\u4ee5\u4fdd\u969c\u884c\u8f66\u5b89\u5168\u3002", "motivation": "CAEVs\u867d\u7136\u80fd\u63d0\u9ad8\u4ea4\u901a\u6548\u7387\u548c\u6539\u5584\u73af\u5883\uff0c\u4f46\u6613\u53d7\u7f51\u7edc\u653b\u51fb\u3002\u9700\u8981\u63d0\u51fa\u5b89\u5168\u63a7\u5236\u67b6\u6784\u3002 ", "method": "\u8bbe\u8ba1\u4e00\u4e2a\u989d\u5916\u7684\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u63a7\u5236\u8f93\u5165\uff0c\u4e0e\u7535\u6c60\u63a7\u5236\u8f93\u5165\u4e00\u8d77\u4f5c\u7528\u4e8e\u8f66\u8f86\u52a8\u529b\u7cfb\u7edf\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u6291\u5236\u653b\u51fb\u6548\u679c\uff0c\u4fdd\u969cCAEV\u8f66\u961f\u5b89\u5168\u8fd0\u884c\uff0c\u907f\u514d\u78b0\u649e\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u5b89\u5168\u63a7\u5236\u67b6\u6784\u80fd\u6709\u6548\u62b5\u5fa1\u7f51\u7edc\u653b\u51fb\uff0c\u4fdd\u969cCAEV\u7684\u5b89\u5168\u8fd0\u884c\u3002"}}
{"id": "2510.24017", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.24017", "abs": "https://arxiv.org/abs/2510.24017", "authors": ["Brian Skoglind", "Travis Roberts", "Sourabh Karmakar", "Cameron Turner", "Laine Mears"], "title": "Localized Acoustic-Event Measurement Probe: Connector Confirmation Utilizing Acoustic Signatures", "comment": null, "summary": "Modern consumer products are full of interconnected electrical and electronic\nmodules to fulfill direct and indirect needs. In an automated assembly line\nstill, most of these interconnections are required to be done manually due to\nthe large variety of connector types, connector positions, and the soft,\nflexible nature of their structures. The manual connection points are the\nsource of partial or completely loose connections. Sometimes connections are\nmissed due to the application of unequal mating forces and natural human\nfatigue. Subsequently, these defects can lead to unexpected downtime and\nexpensive rework. For successful connection detection, past approaches such as\nvision verification, Augmented Reality, or circuit parameter-based measurements\nhave shown limited ability to detect the correct connection state. Though most\nconnections emit a specific noise for successful mating, the acoustic-based\nverification system for electrical connection confirmation has not been\nextensively researched. The main discouraging reason for such research is the\ntypically low signal-to-noise ratio (SNR) between the sound of a pair of\nelectrical connector mating and the diverse soundscape of the plant. In this\nstudy, the authors investigated increasing the SNR between the electrical\nconnector mating sound and the plant soundscape to improve connection success\ndetection by employing a physical system for background noise mitigation and\nthe successful met noise signature amplification algorithm. The solution is\nover 75% effective at detecting and classifying connection state. The solution\nhas been constructed without any modification to the existing manual\ninterconnection process.", "AI": {"tldr": "\u624b\u52a8\u8fde\u63a5\u7684\u7535\u5b50\u4ea7\u54c1\u5b58\u5728\u8fde\u63a5\u95ee\u9898\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u58f0\u5b66\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u63d0\u9ad8\u68c0\u6d4b\u6210\u529f\u7387\u3002", "motivation": "\u624b\u52a8\u8fde\u63a5\u7535\u5b50\u4ea7\u54c1\u5728\u81ea\u52a8\u5316\u751f\u4ea7\u7ebf\u4e2d\u4ecd\u666e\u904d\u5b58\u5728\uff0c\u4f46\u6613\u56e0\u64cd\u4f5c\u4e0d\u5f53\u5bfc\u81f4\u8fde\u63a5\u677e\u52a8\u6216\u7f3a\u5931\uff0c\u8fdb\u800c\u5f15\u8d77\u505c\u673a\u548c\u8fd4\u5de5\u3002\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\uff08\u5982\u89c6\u89c9\u3001AR\u3001\u7535\u8def\u53c2\u6570\u6d4b\u91cf\uff09\u6548\u679c\u6709\u9650\u3002\u8fde\u63a5\u6210\u529f\u65f6\u901a\u5e38\u4f1a\u53d1\u51fa\u7279\u5b9a\u58f0\u97f3\uff0c\u4f46\u8be5\u58f0\u97f3\u5728\u5de5\u5382\u73af\u5883\u4e2d\u4fe1\u566a\u6bd4\u8f83\u4f4e\uff0c\u963b\u788d\u4e86\u58f0\u5b66\u68c0\u6d4b\u65b9\u6cd5\u7684\u7814\u7a76\u3002", "method": "\u672c\u7814\u7a76\u901a\u8fc7\u4e00\u4e2a\u7269\u7406\u7cfb\u7edf\u6765\u964d\u4f4e\u80cc\u666f\u566a\u97f3\uff0c\u5e76\u91c7\u7528\u4e00\u79cd\u7b97\u6cd5\u6765\u653e\u5927\u8fde\u63a5\u6210\u529f\u65f6\u7684\u58f0\u97f3\u7279\u5f81\uff0c\u4ee5\u63d0\u9ad8\u4fe1\u566a\u6bd4\uff0c\u4ece\u800c\u5b9e\u73b0\u8fde\u63a5\u72b6\u6001\u7684\u68c0\u6d4b\u3002", "result": "\u63d0\u51fa\u7684\u89e3\u51b3\u65b9\u6848\u5728\u68c0\u6d4b\u548c\u5206\u7c7b\u8fde\u63a5\u72b6\u6001\u65b9\u9762\uff0c\u6210\u529f\u7387\u8d85\u8fc775%\uff0c\u4e14\u65e0\u9700\u5bf9\u73b0\u6709\u624b\u52a8\u8fde\u63a5\u6d41\u7a0b\u8fdb\u884c\u4efb\u4f55\u4fee\u6539\u3002", "conclusion": "\u672c\u7814\u7a76\u6210\u529f\u5f00\u53d1\u4e86\u4e00\u79cd\u65e0\u9700\u6539\u9020\u73b0\u6709\u6d41\u7a0b\u7684\u58f0\u5b66\u68c0\u6d4b\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u624b\u52a8\u8fde\u63a5\u7535\u5b50\u4ea7\u54c1\u65f6\u7684\u8fde\u63a5\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2510.23632", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23632", "abs": "https://arxiv.org/abs/2510.23632", "authors": ["Guozhong Li", "Muhannad Alhumaidi", "Spiros Skiadopoulos", "Panos Kalnis"], "title": "LLMComp: A Language Modeling Paradigm for Error-Bounded Scientific Data Compression", "comment": null, "summary": "The rapid growth of high-resolution scientific simulations and observation\nsystems is generating massive spatiotemporal datasets, making efficient,\nerror-bounded compression increasingly important. Meanwhile, decoder-only large\nlanguage models (LLMs) have demonstrated remarkable capabilities in modeling\ncomplex sequential data. In this paper, we propose LLMCOMP, a novel lossy\ncompression paradigm that leverages decoder-only large LLMs to model scientific\ndata. LLMCOMP first quantizes 3D fields into discrete tokens, arranges them via\nZ-order curves to preserve locality, and applies coverage-guided sampling to\nenhance training efficiency. An autoregressive transformer is then trained with\nspatial-temporal embeddings to model token transitions. During compression, the\nmodel performs top-k prediction, storing only rank indices and fallback\ncorrections to ensure strict error bounds. Experiments on multiple reanalysis\ndatasets show that LLMCOMP consistently outperforms state-of-the-art\ncompressors, achieving up to 30% higher compression ratios under strict error\nbounds. These results highlight the potential of LLMs as general-purpose\ncompressors for high-fidelity scientific data.", "AI": {"tldr": "LLMCOMP\u662f\u4e00\u79cd\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8fdb\u884c\u79d1\u5b66\u6570\u636e\u6709\u635f\u538b\u7f29\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cf\u5316\u3001Z\u9636\u66f2\u7ebf\u7f16\u7801\u3001\u8986\u76d6\u5f15\u5bfc\u91c7\u6837\u548c\u81ea\u56de\u5f52Transformer\u5efa\u6a21\uff0c\u5728\u4e25\u683c\u7684\u8bef\u5dee\u8fb9\u754c\u4e0b\u5b9e\u73b0\u4e86\u6bd4\u73b0\u6709\u6280\u672f\u66f4\u9ad8\u7684\u538b\u7f29\u7387\u3002", "motivation": "\u79d1\u5b66\u6a21\u62df\u548c\u89c2\u6d4b\u7cfb\u7edf\u4ea7\u751f\u6d77\u91cf\u65f6\u7a7a\u6570\u636e\uff0c\u9700\u8981\u9ad8\u6548\u3001\u6709\u8bef\u5dee\u8fb9\u754c\u7684\u6570\u636e\u538b\u7f29\u65b9\u6cd5\u3002", "method": "LLMCOMP\u5c063D\u573a\u91cf\u5316\u4e3a\u79bb\u6563\u4ee4\u724c\uff0c\u901a\u8fc7Z\u9636\u66f2\u7ebf\u6392\u5217\u4ee5\u4fdd\u7559\u5c40\u90e8\u6027\uff0c\u5e76\u4f7f\u7528\u8986\u76d6\u5f15\u5bfc\u91c7\u6837\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\u3002\u7136\u540e\uff0c\u8bad\u7ec3\u4e00\u4e2a\u5177\u6709\u65f6\u7a7a\u5d4c\u5165\u7684\u81ea\u56de\u5f52Transformer\u6765\u6a21\u62df\u4ee4\u724c\u8f6c\u6362\u3002\u538b\u7f29\u65f6\uff0c\u6a21\u578b\u8fdb\u884ctop-k\u9884\u6d4b\uff0c\u4ec5\u5b58\u50a8\u6392\u540d\u7d22\u5f15\u548c\u56de\u9000\u6821\u6b63\uff0c\u4ee5\u786e\u4fdd\u4e25\u683c\u7684\u8bef\u5dee\u8fb9\u754c\u3002", "result": "\u5728\u591a\u4e2a\u518d\u5206\u6790\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cLLMCOMP\u5728\u4e25\u683c\u7684\u8bef\u5dee\u8fb9\u754c\u4e0b\uff0c\u538b\u7f29\u7387\u6bd4\u6700\u5148\u8fdb\u7684\u538b\u7f29\u5668\u9ad8\u51fa30%\u3002", "conclusion": "LLM\uff08\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff09\u6709\u6f5c\u529b\u6210\u4e3a\u9ad8\u4fdd\u771f\u79d1\u5b66\u6570\u636e\u901a\u7528\u538b\u7f29\u5668\u3002"}}
{"id": "2510.23856", "categories": ["cs.AI", "68Txx"], "pdf": "https://arxiv.org/pdf/2510.23856", "abs": "https://arxiv.org/abs/2510.23856", "authors": ["Segev Shlomov", "Alon Oved", "Sami Marreed", "Ido Levy", "Offer Akrabi", "Avi Yaeli", "\u0141ukasz Str\u0105k", "Elizabeth Koumpan", "Yinon Goldshtein", "Eilam Shapira", "Nir Mashkif", "Asaf Adi"], "title": "From Benchmarks to Business Impact: Deploying IBM Generalist Agent in Enterprise Production", "comment": "AAAI Conference on Artificial Intelligence", "summary": "Agents are rapidly advancing in automating digital work, but enterprises face\na harder challenge: moving beyond prototypes to deployed systems that deliver\nmeasurable business value. This path is complicated by fragmented frameworks,\nslow development, and the absence of standardized evaluation practices.\nGeneralist agents have emerged as a promising direction, excelling on academic\nbenchmarks and offering flexibility across task types, applications, and\nmodalities. Yet, evidence of their use in production enterprise settings\nremains limited. This paper reports IBM's experience developing and piloting\nthe Computer Using Generalist Agent (CUGA), which has been open-sourced for the\ncommunity (https://github.com/cuga-project/cuga-agent). CUGA adopts a\nhierarchical planner--executor architecture with strong analytical foundations,\nachieving state-of-the-art performance on AppWorld and WebArena. Beyond\nbenchmarks, it was evaluated in a pilot within the Business-Process-Outsourcing\ntalent acquisition domain, addressing enterprise requirements for scalability,\nauditability, safety, and governance. To support assessment, we introduce\nBPO-TA, a 26-task benchmark spanning 13 analytics endpoints. In preliminary\nevaluations, CUGA approached the accuracy of specialized agents while\nindicating potential for reducing development time and cost. Our contribution\nis twofold: presenting early evidence of generalist agents operating at\nenterprise scale, and distilling technical and organizational lessons from this\ninitial pilot. We outline requirements and next steps for advancing\nresearch-grade architectures like CUGA into robust, enterprise-ready systems.", "AI": {"tldr": "IBM \u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a CUGA \u7684\u901a\u7528\u667a\u80fd\u4f53\uff0c\u8be5\u667a\u80fd\u4f53\u91c7\u7528\u5206\u5c42\u89c4\u5212-\u6267\u884c\u5668\u67b6\u6784\uff0c\u5728 AppWorld \u548c WebArena \u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u8be5\u667a\u80fd\u4f53\u8fd8\u5728\u4e1a\u52a1\u6d41\u7a0b\u5916\u5305\u4eba\u624d\u62db\u8058\u9886\u57df\u8fdb\u884c\u4e86\u8bd5\u70b9\uff0c\u5e76\u5728\u53ef\u6269\u5c55\u6027\u3001\u53ef\u5ba1\u8ba1\u6027\u3001\u5b89\u5168\u6027\u548c\u6cbb\u7406\u65b9\u9762\u6ee1\u8db3\u4e86\u4f01\u4e1a\u9700\u6c42\u3002CUGA \u7684\u76ee\u6807\u662f\u4e3a\u901a\u7528\u667a\u80fd\u4f53\u5728\u4f01\u4e1a\u7ea7\u5e94\u7528\u4e2d\u63d0\u4f9b\u65e9\u671f\u8bc1\u636e\uff0c\u5e76\u603b\u7ed3\u4ece\u4e2d\u83b7\u5f97\u7684\u7ecf\u9a8c\u6559\u8bad\u3002", "motivation": "\u5f53\u524d\u4f01\u4e1a\u5728\u81ea\u52a8\u5316\u6570\u5b57\u5de5\u4f5c\u65b9\u9762\u9762\u4e34\u4ece\u539f\u578b\u5230\u5b9e\u9645\u5e94\u7528\u4ea4\u4ed8\u53ef\u8861\u91cf\u4e1a\u52a1\u4ef7\u503c\u7684\u6311\u6218\uff0c\u4e3b\u8981\u53d7\u9650\u4e8e\u788e\u7247\u5316\u7684\u6846\u67b6\u3001\u7f13\u6162\u7684\u5f00\u53d1\u901f\u5ea6\u548c\u7f3a\u4e4f\u6807\u51c6\u5316\u7684\u8bc4\u4f30\u5b9e\u8df5\u3002\u901a\u7528\u667a\u80fd\u4f53\u867d\u7136\u5728\u5b66\u672f\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\u4e14\u5177\u6709\u7075\u6d3b\u6027\uff0c\u4f46\u5728\u5b9e\u9645\u4f01\u4e1a\u73af\u5883\u4e2d\u5e94\u7528\u6709\u9650\u3002", "method": "CUGA \u91c7\u7528\u5206\u5c42\u89c4\u5212-\u6267\u884c\u5668\u67b6\u6784\uff0c\u8be5\u67b6\u6784\u5177\u6709\u624e\u5b9e\u7684\u5206\u6790\u57fa\u7840\u3002\u8be5\u667a\u80fd\u4f53\u5728\u4e1a\u52a1\u6d41\u7a0b\u5916\u5305\u4eba\u624d\u62db\u8058\u9886\u57df\u8fdb\u884c\u4e86\u8bd5\u70b9\u8bc4\u4f30\uff0c\u4ee5\u6ee1\u8db3\u4f01\u4e1a\u5bf9\u53ef\u6269\u5c55\u6027\u3001\u53ef\u5ba1\u8ba1\u6027\u3001\u5b89\u5168\u6027\u548c\u6cbb\u7406\u7684\u8981\u6c42\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86 BPO-TA \u8fd9\u4e00\u5305\u542b 26 \u4e2a\u4efb\u52a1\u548c 13 \u4e2a\u5206\u6790\u7aef\u70b9\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002IBM \u8fd8\u5c06 CUGA \u5f00\u6e90\u3002", "result": "CUGA \u5728 AppWorld \u548c WebArena \u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u5728\u4e1a\u52a1\u6d41\u7a0b\u5916\u5305\u4eba\u624d\u62db\u8058\u9886\u57df\u7684\u8bd5\u70b9\u8bc4\u4f30\u4e2d\uff0cCUGA \u7684\u51c6\u786e\u6027\u63a5\u8fd1\u4e13\u4e1a\u667a\u80fd\u4f53\uff0c\u5e76\u663e\u793a\u51fa\u7f29\u77ed\u5f00\u53d1\u65f6\u95f4\u548c\u964d\u4f4e\u6210\u672c\u7684\u6f5c\u529b\u3002BPO-TA \u57fa\u51c6\u6d4b\u8bd5\u88ab\u7528\u4e8e\u652f\u6301\u8bc4\u4f30\u3002", "conclusion": "\u901a\u7528\u667a\u80fd\u4f53\uff08\u5982 CUGA\uff09\u6709\u6f5c\u529b\u5728\u4f01\u4e1a\u89c4\u6a21\u4e0a\u8fd0\u884c\uff0c\u5e76\u4e3a\u4f01\u4e1a\u5e26\u6765\u5b9e\u9645\u4ef7\u503c\u3002\u7136\u800c\uff0c\u5c06\u7814\u7a76\u7ea7\u67b6\u6784\uff08\u5982 CUGA\uff09\u8f6c\u5316\u4e3a\u5065\u58ee\u3001\u9002\u5408\u4f01\u4e1a\u4f7f\u7528\u7684\u7cfb\u7edf\uff0c\u8fd8\u9700\u8981\u5728\u6280\u672f\u548c\u7ec4\u7ec7\u5c42\u9762\u8fdb\u884c\u8fdb\u4e00\u6b65\u7684\u52aa\u529b\uff0c\u5305\u62ec\u660e\u786e\u7cfb\u7edf\u9700\u6c42\u548c\u89c4\u5212\u540e\u7eed\u6b65\u9aa4\u3002"}}
{"id": "2510.24337", "categories": ["cs.AI", "cs.SI"], "pdf": "https://arxiv.org/pdf/2510.24337", "abs": "https://arxiv.org/abs/2510.24337", "authors": ["Daria Kravets-Meinke", "Hannah Schmid-Petri", "Sonja Niemann", "Ute Schmid"], "title": "Generative Large Language Models (gLLMs) in Content Analysis: A Practical Guide for Communication Research", "comment": null, "summary": "Generative Large Language Models (gLLMs), such as ChatGPT, are increasingly\nbeing used in communication research for content analysis. Studies show that\ngLLMs can outperform both crowd workers and trained coders, such as research\nassistants, on various coding tasks relevant to communication science, often at\na fraction of the time and cost. Additionally, gLLMs can decode implicit\nmeanings and contextual information, be instructed using natural language,\ndeployed with only basic programming skills, and require little to no annotated\ndata beyond a validation dataset - constituting a paradigm shift in automated\ncontent analysis. Despite their potential, the integration of gLLMs into the\nmethodological toolkit of communication research remains underdeveloped. In\ngLLM-assisted quantitative content analysis, researchers must address at least\nseven critical challenges that impact result quality: (1) codebook development,\n(2) prompt engineering, (3) model selection, (4) parameter tuning, (5)\niterative refinement, (6) validation of the model's reliability, and\noptionally, (7) performance enhancement. This paper synthesizes emerging\nresearch on gLLM-assisted quantitative content analysis and proposes a\ncomprehensive best-practice guide to navigate these challenges. Our goal is to\nmake gLLM-based content analysis more accessible to a broader range of\ncommunication researchers and ensure adherence to established disciplinary\nquality standards of validity, reliability, reproducibility, and research\nethics.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08gLLMs\uff09\u5728\u4f20\u64ad\u5b66\u7814\u7a76\u7684\u5185\u5bb9\u5206\u6790\u4e2d\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u80fd\u591f\u8d85\u8d8a\u4eba\u7c7b\u7f16\u7801\u5458\uff0c\u5e76\u63d0\u4f9b\u66f4\u5feb\u7684\u901f\u5ea6\u548c\u66f4\u4f4e\u7684\u6210\u672c\u3002\u7136\u800c\uff0c\u5176\u5728\u7814\u7a76\u4e2d\u7684\u5e94\u7528\u4ecd\u5904\u4e8e\u521d\u6b65\u9636\u6bb5\uff0c\u7814\u7a76\u8005\u9700\u8981\u514b\u670d\u4ee3\u7801\u4e66\u5f00\u53d1\u3001\u63d0\u793a\u5de5\u7a0b\u3001\u6a21\u578b\u9009\u62e9\u3001\u53c2\u6570\u8c03\u6574\u3001\u8fed\u4ee3\u4f18\u5316\u3001\u6a21\u578b\u53ef\u9760\u6027\u9a8c\u8bc1\u4ee5\u53ca\u6027\u80fd\u589e\u5f3a\u7b49\u4e03\u4e2a\u5173\u952e\u6311\u6218\u3002\u672c\u6587\u65e8\u5728\u603b\u7ed3\u73b0\u6709\u7814\u7a76\uff0c\u5e76\u63d0\u4f9b\u4e00\u4e2a\u5168\u9762\u7684\u6700\u4f73\u5b9e\u8df5\u6307\u5357\uff0c\u4ee5\u5e2e\u52a9\u4f20\u64ad\u5b66\u7814\u7a76\u8005\u66f4\u6709\u6548\u5730\u5229\u7528gLLMs\u8fdb\u884c\u5185\u5bb9\u5206\u6790\uff0c\u5e76\u786e\u4fdd\u7814\u7a76\u7684\u6709\u6548\u6027\u3001\u53ef\u9760\u6027\u3001\u53ef\u590d\u73b0\u6027\u548c\u7814\u7a76\u4f26\u7406\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08gLLMs\uff09\u5728\u4f20\u64ad\u5b66\u7814\u7a76\u7684\u5185\u5bb9\u5206\u6790\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u5176\u5e94\u7528\u4ecd\u4e0d\u6210\u719f\uff0c\u7814\u7a76\u8005\u5728\u6574\u5408\u8fc7\u7a0b\u4e2d\u9762\u4e34\u8bf8\u591a\u6311\u6218\u3002", "method": "\u672c\u6587\u603b\u7ed3\u4e86gLLM\u8f85\u52a9\u5b9a\u91cf\u5185\u5bb9\u5206\u6790\u7684\u65b0\u5174\u7814\u7a76\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u6700\u4f73\u5b9e\u8df5\u6307\u5357\uff0c\u4ee5\u5e94\u5bf9\u4ee3\u7801\u4e66\u5f00\u53d1\u3001\u63d0\u793a\u5de5\u7a0b\u3001\u6a21\u578b\u9009\u62e9\u3001\u53c2\u6570\u8c03\u6574\u3001\u8fed\u4ee3\u4f18\u5316\u3001\u6a21\u578b\u53ef\u9760\u6027\u9a8c\u8bc1\u548c\u6027\u80fd\u589e\u5f3a\u7b49\u6311\u6218\u3002", "result": "gLLMs\u5728\u5185\u5bb9\u5206\u6790\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u4eba\u7c7b\u7f16\u7801\u5458\uff0c\u901f\u5ea6\u66f4\u5feb\u3001\u6210\u672c\u66f4\u4f4e\uff0c\u5e76\u80fd\u89e3\u7801\u9690\u6666\u610f\u4e49\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002\u7136\u800c\uff0c\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u7814\u7a76\u8005\u9700\u8981\u89e3\u51b3\u4e03\u4e2a\u5173\u952e\u6311\u6218\u4ee5\u4fdd\u8bc1\u7ed3\u679c\u8d28\u91cf\u3002", "conclusion": "\u672c\u6587\u65e8\u5728\u4f7fgLLM-\u9a71\u52a8\u7684\u5185\u5bb9\u5206\u6790\u66f4\u6613\u4e8e\u4f20\u64ad\u5b66\u7814\u7a76\u8005\u4f7f\u7528\uff0c\u5e76\u786e\u4fdd\u5176\u7b26\u5408\u5b66\u79d1\u65e2\u5b9a\u7684\u6709\u6548\u6027\u3001\u53ef\u9760\u6027\u3001\u53ef\u590d\u73b0\u6027\u548c\u7814\u7a76\u4f26\u7406\u7b49\u8d28\u91cf\u6807\u51c6\u3002"}}
{"id": "2510.23896", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23896", "abs": "https://arxiv.org/abs/2510.23896", "authors": ["Kosei Uemura", "Miaoran Zhang", "David Ifeoluwa Adelani"], "title": "AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages", "comment": null, "summary": "Text embeddings are an essential building component of several NLP tasks such\nas retrieval-augmented generation which is crucial for preventing\nhallucinations in LLMs. Despite the recent release of massively multilingual\nMTEB (MMTEB), African languages remain underrepresented, with existing tasks\noften repurposed from translation benchmarks such as FLORES clustering or\nSIB-200. In this paper, we introduce AfriMTEB -- a regional expansion of MMTEB\ncovering 59 languages, 14 tasks, and 38 datasets, including six newly added\ndatasets. Unlike many MMTEB datasets that include fewer than five languages,\nthe new additions span 14 to 56 African languages and introduce entirely new\ntasks, such as hate speech detection, intent detection, and emotion\nclassification, which were not previously covered. Complementing this, we\npresent AfriE5, an adaptation of the instruction-tuned mE5 model to African\nlanguages through cross-lingual contrastive distillation. Our evaluation shows\nthat AfriE5 achieves state-of-the-art performance, outperforming strong\nbaselines such as Gemini-Embeddings and mE5.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86AfriMTEB\uff0c\u4e00\u4e2a\u5305\u542b59\u79cd\u975e\u6d32\u8bed\u8a00\u300114\u4e2a\u4efb\u52a1\u548c38\u4e2a\u6570\u636e\u96c6\u7684\u6587\u672c\u5d4c\u5165\u57fa\u51c6\uff0c\u5e76\u4ecb\u7ecd\u4e86AfriE5\u6a21\u578b\uff0c\u65e8\u5728\u6539\u5584\u975e\u6d32\u8bed\u8a00\u5728NLP\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u975e\u6d32\u8bed\u8a00\u5728\u73b0\u6709\u7684\u591a\u8bed\u8a00\u6587\u672c\u5d4c\u5165\u57fa\u51c6\uff08\u5982MMTEB\uff09\u4e2d\u4ee3\u8868\u6027\u4e0d\u8db3\uff0c\u73b0\u6709\u4efb\u52a1\u591a\u4e3a\u7ffb\u8bd1\u6539\u7f16\uff0c\u672a\u80fd\u5145\u5206\u8986\u76d6\u975e\u6d32\u8bed\u8a00\u7684\u5b9e\u9645\u9700\u6c42\uff0c\u5982\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u3001\u610f\u56fe\u68c0\u6d4b\u548c\u60c5\u611f\u5206\u7c7b\u7b49\u3002", "method": "\u5f15\u5165AfriMTEB\u57fa\u51c6\uff0c\u5305\u542b59\u79cd\u975e\u6d32\u8bed\u8a00\u300114\u4e2a\u4efb\u52a1\u548c38\u4e2a\u6570\u636e\u96c6\uff0c\u5176\u4e2d6\u4e2a\u4e3a\u65b0\u589e\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u51fa\u4e86AfriE5\u6a21\u578b\uff0c\u901a\u8fc7\u8de8\u8bed\u8a00\u5bf9\u6bd4\u84b8\u998f\u5bf9mE5\u6a21\u578b\u8fdb\u884c\u4e86\u6539\u8fdb\uff0c\u4f7f\u5176\u9002\u5e94\u975e\u6d32\u8bed\u8a00\u3002", "result": "AfriE5\u6a21\u578b\u5728\u6587\u672c\u5d4c\u5165\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4f18\u4e8eGemini-Embeddings\u548cmE5\u7b49\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "AfriMTEB\u57fa\u51c6\u548cAfriE5\u6a21\u578b\u7684\u63d0\u51fa\uff0c\u663e\u8457\u6539\u5584\u4e86\u975e\u6d32\u8bed\u8a00\u5728NLP\u4efb\u52a1\u4e2d\u7684\u8868\u793a\u548c\u6027\u80fd\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002"}}
{"id": "2510.24052", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24052", "abs": "https://arxiv.org/abs/2510.24052", "authors": ["Jongsuk Kim", "Jaeyoung Lee", "Gyojin Han", "Dongjae Lee", "Minki Jeong", "Junmo Kim"], "title": "SynAD: Enhancing Real-World End-to-End Autonomous Driving Models through Synthetic Data Integration", "comment": null, "summary": "Recent advancements in deep learning and the availability of high-quality\nreal-world driving datasets have propelled end-to-end autonomous driving.\nDespite this progress, relying solely on real-world data limits the variety of\ndriving scenarios for training. Synthetic scenario generation has emerged as a\npromising solution to enrich the diversity of training data; however, its\napplication within E2E AD models remains largely unexplored. This is primarily\ndue to the absence of a designated ego vehicle and the associated sensor\ninputs, such as camera or LiDAR, typically provided in real-world scenarios. To\naddress this gap, we introduce SynAD, the first framework designed to enhance\nreal-world E2E AD models using synthetic data. Our method designates the agent\nwith the most comprehensive driving information as the ego vehicle in a\nmulti-agent synthetic scenario. We further project path-level scenarios onto\nmaps and employ a newly developed Map-to-BEV Network to derive bird's-eye-view\nfeatures without relying on sensor inputs. Finally, we devise a training\nstrategy that effectively integrates these map-based synthetic data with real\ndriving data. Experimental results demonstrate that SynAD effectively\nintegrates all components and notably enhances safety performance. By bridging\nsynthetic scenario generation and E2E AD, SynAD paves the way for more\ncomprehensive and robust autonomous driving models.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86SynAD\u6846\u67b6\uff0c\u9996\u6b21\u5229\u7528\u5408\u6210\u6570\u636e\u6765\u589e\u5f3a\u7aef\u5230\u7aef\u7684\u81ea\u52a8\u9a7e\u9a76\uff08E2E AD\uff09\u6a21\u578b\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5c06\u591a\u667a\u80fd\u4f53\u5408\u6210\u573a\u666f\u4e2d\u7684\u667a\u80fd\u4f53\u6307\u5b9a\u4e3a\u201c\u81ea\u8f66\u201d\uff0c\u5e76\u5c06\u8def\u5f84\u7ea7\u573a\u666f\u6295\u5f71\u5230\u5730\u56fe\u4e0a\uff0c\u5229\u7528\u65b0\u63d0\u51fa\u7684Map-to-BEV\u7f51\u7edc\u5728\u65e0\u4f20\u611f\u5668\u8f93\u5165\u7684\u60c5\u51b5\u4e0b\u63d0\u53d6\u9e1f\u77b0\u56fe\u7279\u5f81\u3002\u6700\u7ec8\uff0c\u901a\u8fc7\u6574\u5408\u8fd9\u4e9b\u57fa\u4e8e\u5730\u56fe\u7684\u5408\u6210\u6570\u636e\u548c\u771f\u5b9e\u9a7e\u9a76\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u5b89\u5168\u6027\u3002", "motivation": "\u5c3d\u7ba1\u6df1\u5ea6\u5b66\u4e60\u548c\u771f\u5b9e\u9a7e\u9a76\u6570\u636e\u96c6\u5728\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\uff08E2E AD\uff09\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u4ec5\u4f9d\u8d56\u771f\u5b9e\u6570\u636e\u4f1a\u9650\u5236\u8bad\u7ec3\u573a\u666f\u7684\u591a\u6837\u6027\u3002\u5408\u6210\u573a\u666f\u751f\u6210\u662f\u4e00\u4e2a\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u7531\u4e8e\u7f3a\u4e4f\u6307\u5b9a\u7684\u81ea\u8f66\u53ca\u5176\u4f20\u611f\u5668\u8f93\u5165\uff0c\u5176\u5728E2E AD\u6a21\u578b\u4e2d\u7684\u5e94\u7528\u4ecd\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "SynAD\u6846\u67b6\u5c06\u591a\u667a\u80fd\u4f53\u5408\u6210\u573a\u666f\u4e2d\u7684\u667a\u80fd\u4f53\u6307\u5b9a\u4e3a\u81ea\u8f66\uff0c\u5e76\u5c06\u8def\u5f84\u7ea7\u573a\u666f\u6295\u5f71\u5230\u5730\u56fe\u4e0a\u3002\u5229\u7528\u65b0\u63d0\u51fa\u7684Map-to-BEV\u7f51\u7edc\uff0c\u5728\u6ca1\u6709\u4f20\u611f\u5668\u8f93\u5165\u7684\u60c5\u51b5\u4e0b\uff0c\u4ece\u5730\u56fe\u4fe1\u606f\u4e2d\u63d0\u53d6\u9e1f\u77b0\u56fe\uff08BEV\uff09\u7279\u5f81\u3002\u6700\u540e\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u8bad\u7ec3\u7b56\u7565\uff0c\u5c06\u8fd9\u4e9b\u57fa\u4e8e\u5730\u56fe\u7684\u5408\u6210\u6570\u636e\u4e0e\u771f\u5b9e\u9a7e\u9a76\u6570\u636e\u8fdb\u884c\u6574\u5408\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSynAD\u80fd\u591f\u6709\u6548\u5730\u6574\u5408\u6240\u6709\u7ec4\u4ef6\uff0c\u5e76\u663e\u8457\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u6a21\u578b\u7684\u5b89\u5168\u6027\u80fd\u3002", "conclusion": "SynAD\u6846\u67b6\u6210\u529f\u5730\u5f25\u5408\u4e86\u5408\u6210\u573a\u666f\u751f\u6210\u4e0e\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u5f00\u53d1\u66f4\u5168\u9762\u3001\u66f4\u9c81\u68d2\u7684\u81ea\u52a8\u9a7e\u9a76\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u7684\u9014\u5f84\u3002"}}
{"id": "2510.24238", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2510.24238", "abs": "https://arxiv.org/abs/2510.24238", "authors": ["Maja Szymczak", "Jan Ho\u010devar", "Jernej Iskra", "Darja Lisjak", "Jelena Papan Djani\u0161", "Lukasz Marciniak", "Karolina Elzbieciak-Piecka"], "title": "Unlocking Dynamic Luminescent Mapping of pH with Sustainable Lignin-Derived Carbon Dots with Multimodal Readout Capacity", "comment": null, "summary": "In this work, we demonstrate the use of CQDs synthesized from lignin -\ncurrently one of the most abundant and underutilized by-products of paper and\npulp production - for advanced pH monitoring applications. The presented\napproach integrates green chemistry principles with an operator-friendly,\nlow-cost, and practical solution for spatial and temporal pH measurement. CQDs\nfunctionalized with m-aminophenylboronic acid enable highly sensitive and\nreversible pH readouts through two complementary mechanisms: ratiometric\nmonitoring of emission band intensities, and direct visual observation of\ncolorimetric changes reflected in the CIE1931 chromaticity coordinates. The\nsystem achieves maximal sensitivities of 137 percent per pH unit and 49.5\npercent per pH unit, respectively, while simultaneously maintaining high\nmeasurement resolution and full reproducibility of the readouts, placing it\namong the most effective CQD-based pH sensors reported to date. Here, we\ndemonstrate the capability of 2D luminescent imaging of pH distributions,\nallowing for both spatially resolved and time-resolved monitoring. Employing\njust an excitation source, a digital camera or smartphone, and RGB channel\nanalysis, the setup eliminates the necessity for specialized filters or\nsophisticated instrumentation. The combination of multimodal readout strategies\nwith the capacity for large-area visualization establishes lignin-derived CQDs\nas a sustainable and practical platform for pH sensing. By simultaneously\naddressing the challenges of waste valorization and the demand for innovative\nsensing technologies, this solution fulfills the requirements of both\nenvironmentally responsible material design and next-generation pH sensor\ndevelopment.", "AI": {"tldr": "\u5229\u7528\u4ece\u6728\u8d28\u7d20\u5408\u6210\u7684\u78b3\u91cf\u5b50\u70b9\uff08CQDs\uff09\u5f00\u53d1\u4e86\u4e00\u79cd\u73af\u4fdd\u3001\u4f4e\u6210\u672c\u4e14\u6613\u4e8e\u4f7f\u7528\u7684pH\u76d1\u6d4b\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u8367\u5149\u548c\u6bd4\u8272\u4e24\u79cd\u8bfb\u51fa\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7075\u654f\u5ea6\u3001\u9ad8\u5206\u8fa8\u7387\u548c\u53ef\u91cd\u590d\u6027\u7684pH\u6d4b\u91cf\uff0c\u5e76\u80fd\u8fdb\u884c\u4e8c\u7ef4\u8367\u5149\u6210\u50cf\uff0c\u6709\u671b\u7528\u4e8e\u4e0b\u4e00\u4ee3pH\u4f20\u611f\u5668\u7684\u5f00\u53d1\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u5229\u7528\u5ec9\u4ef7\u6613\u5f97\u4e14\u672a\u88ab\u5145\u5206\u5229\u7528\u7684\u6728\u8d28\u7d20\uff08\u7eb8\u6d46\u751f\u4ea7\u7684\u526f\u4ea7\u54c1\uff09\u6765\u5408\u6210\u78b3\u91cf\u5b50\u70b9\uff08CQDs\uff09\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u5148\u8fdb\u7684pH\u76d1\u6d4b\u9886\u57df\uff0c\u4ee5\u5b9e\u73b0\u7eff\u8272\u5316\u5b66\u3001\u4f4e\u6210\u672c\u548c\u6613\u4e8e\u64cd\u4f5c\u7684pH\u6d4b\u91cf\u3002", "method": "\u901a\u8fc7\u5728\u6728\u8d28\u7d20\u884d\u751f\u7684CQDs\u4e0a\u529f\u80fd\u5316\u95f4\u6c28\u57fa\u82ef\u787c\u9178\uff0c\u5229\u7528\u8367\u5149\u53d1\u5c04\u5f3a\u5ea6\u6bd4\u7387\u548c\u6bd4\u8272\u53d8\u5316\u4e24\u79cd\u673a\u5236\u6765\u5b9e\u73b0pH\u76d1\u6d4b\u3002\u7ed3\u5408\u4e8c\u7ef4\u53d1\u5149\u6210\u50cf\u6280\u672f\uff0c\u4f7f\u7528\u6fc0\u53d1\u5149\u6e90\u3001\u6570\u7801\u76f8\u673a\u6216\u667a\u80fd\u624b\u673a\u8fdb\u884c\u6d4b\u91cf\uff0c\u65e0\u9700\u7279\u6b8a\u4eea\u5668\u3002", "result": "\u6240\u5f00\u53d1\u7684CQDs\u57fapH\u4f20\u611f\u5668\u5b9e\u73b0\u4e86137%/pH\u548c49.5%/pH\u7684\u6700\u5927\u7075\u654f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u5206\u8fa8\u7387\u548c\u53ef\u91cd\u590d\u6027\u3002\u8be5\u4f20\u611f\u5668\u80fd\u591f\u8fdb\u884c\u4e8c\u7ef4pH\u5206\u5e03\u7684\u8367\u5149\u6210\u50cf\uff0c\u5b9e\u73b0\u7a7a\u95f4\u548c\u65f6\u95f4\u5206\u8fa8\u7684\u76d1\u6d4b\u3002", "conclusion": "\u6728\u8d28\u7d20\u884d\u751f\u7684CQDs\u4f5c\u4e3a\u4e00\u79cd\u53ef\u6301\u7eed\u4e14\u5b9e\u7528\u7684\u5e73\u53f0\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u8bfb\u51fa\u7b56\u7565\u548c\u53ef\u89c6\u5316\u80fd\u529b\uff0c\u80fd\u591f\u6ee1\u8db3\u5bf9\u73af\u5883\u53cb\u597d\u6750\u6599\u8bbe\u8ba1\u548c\u4e0b\u4e00\u4ee3pH\u4f20\u611f\u5668\u7684\u9700\u6c42\u3002"}}
{"id": "2510.24656", "categories": ["cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2510.24656", "abs": "https://arxiv.org/abs/2510.24656", "authors": ["Alexander Lidiak", "Jacob Swain", "David L. Craig", "Joseph Hickie", "Yikai Yang", "Federico Fedele", "Jaime Saez-Mollejo", "Andrea Ballabio", "Daniel Chrastina", "Giovanni Isella", "Georgios Katsaros", "Dominic T. Lennon", "Vincent P. Michal", "Erik M. Gauger", "Natalia Ares"], "title": "Virtual Gates Enabled by Digital Surrogate of Quantum Dot Devices", "comment": null, "summary": "Advances in quantum technologies are often limited by slow device\ncharacterization, complex tuning requirements, and scalability challenges. Spin\nqubits in electrostatically defined quantum dots provide a promising platform\nbut are not exempt from these limitations. Simulations enhance our\nunderstanding of such devices, and in many cases, rapid feedback between\nmeasurements and simulations can guide the development of optimal design and\ncontrol strategies. Here, we introduce a modular, graph-based simulator that\nacts as a digital surrogate for a semiconductor quantum dot device, where\ncomputationally expensive processes are accelerated using deep learning. We\ndemonstrate its potential by estimating crosstalk effects between gate\nelectrodes and applying these estimates to construct virtual gates in a quantum\ndot device. We validate our approach through comparison with experiments on a\ndouble quantum dot defined in a Ge/SiGe heterostructure. We envision that this\nsimulation framework will advance semiconductor-based quantum technologies by\nenabling more efficient design, characterization, and control of complex\ndevices.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6a21\u5757\u5316\u3001\u56fe \uae30\ubc18\u91cf\u5b50\u70b9\u8bbe\u5907\u6a21\u62df\u5668\uff0c\u4ee5\u52a0\u901f\u5668\u4ef6\u8868\u5f81\u5e76\u6307\u5bfc\u8bbe\u8ba1\u548c\u63a7\u5236\u7b56\u7565\u3002", "motivation": "\u91cf\u5b50\u70b9\u81ea\u65cb\u91cf\u5b50\u6bd4\u7279\u5728\u91cf\u5b50\u6280\u672f\u4e2d\u6709\u524d\u666f\uff0c\u4f46\u9762\u4e34\u5668\u4ef6\u8868\u5f81\u7f13\u6162\u3001\u8c03\u8c10\u590d\u6742\u548c\u53ef\u6269\u5c55\u6027\u6311\u6218\u3002\u9700\u8981\u901a\u8fc7\u6a21\u62df\u6765\u52a0\u901f\u7406\u89e3\u548c\u4f18\u5316\u8bbe\u8ba1\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u6a21\u5757\u5316\u3001\u56fe \uae30\ubc18\u7684\u6a21\u62df\u5668\uff0c\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u52a0\u901f\u8ba1\u7b97\u5bc6\u96c6\u578b\u8fc7\u7a0b\uff0c\u5e76\u4f30\u7b97\u4e86\u95e8\u7535\u6781\u95f4\u7684\u4e32\u6270\u6548\u5e94\uff0c\u7528\u4e8e\u6784\u5efa\u865a\u62df\u95e8\u3002", "result": "\u901a\u8fc7\u5728 Ge/SiGe \u5f02\u8d28\u7ed3\u6784\u53cc\u91cf\u5b50\u70b9\u5668\u4ef6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8bc1\u660e\u4e86\u8be5\u6a21\u62df\u5668\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6a21\u62df\u6846\u67b6\u80fd\u591f\u901a\u8fc7\u66f4\u9ad8\u6548\u7684\u5668\u4ef6\u8bbe\u8ba1\u3001\u8868\u5f81\u548c\u63a7\u5236\uff0c\u63a8\u52a8\u534a\u5bfc\u4f53\u91cf\u5b50\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2510.23943", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23943", "abs": "https://arxiv.org/abs/2510.23943", "authors": ["Diana Aldana", "Jo\u00e3o Paulo Lima", "Daniel Csillag", "Daniel Perazzo", "Haoan Feng", "Luiz Velho", "Tiago Novello"], "title": "Adaptive Training of INRs via Pruning and Densification", "comment": null, "summary": "Encoding input coordinates with sinusoidal functions into multilayer\nperceptrons (MLPs) has proven effective for implicit neural representations\n(INRs) of low-dimensional signals, enabling the modeling of high-frequency\ndetails. However, selecting appropriate input frequencies and architectures\nwhile managing parameter redundancy remains an open challenge, often addressed\nthrough heuristics and heavy hyperparameter optimization schemes. In this\npaper, we introduce AIRe ($\\textbf{A}$daptive $\\textbf{I}$mplicit neural\n$\\textbf{Re}$presentation), an adaptive training scheme that refines the INR\narchitecture over the course of optimization. Our method uses a neuron pruning\nmechanism to avoid redundancy and input frequency densification to improve\nrepresentation capacity, leading to an improved trade-off between network size\nand reconstruction quality. For pruning, we first identify less-contributory\nneurons and apply a targeted weight decay to transfer their information to the\nremaining neurons, followed by structured pruning. Next, the densification\nstage adds input frequencies to spectrum regions where the signal underfits,\nexpanding the representational basis. Through experiments on images and SDFs,\nwe show that AIRe reduces model size while preserving, or even improving,\nreconstruction quality. Code and pretrained models will be released for public\nuse.", "AI": {"tldr": "AIRe\u662f\u4e00\u79cd\u81ea\u9002\u5e94\u8bad\u7ec3\u65b9\u6848\uff0c\u901a\u8fc7\u795e\u7ecf\u5143\u4fee\u526a\u548c\u8f93\u5165\u9891\u7387\u81f4\u5bc6\u5316\u6765\u4f18\u5316\u9690\u5f0f\u795e\u7ecf\u8868\u793a\uff08INR\uff09\u7684\u7f51\u7edc\u7ed3\u6784\uff0c\u80fd\u5728\u51cf\u5c0f\u6a21\u578b\u5c3a\u5bf8\u7684\u540c\u65f6\u4fdd\u6301\u751a\u81f3\u63d0\u9ad8\u91cd\u5efa\u8d28\u91cf\u3002", "motivation": "\u5728\u9690\u5f0f\u795e\u7ecf\u8868\u793a\uff08INR\uff09\u4e2d\uff0c\u4f7f\u7528\u6b63\u5f26\u51fd\u6570\u5bf9\u8f93\u5165\u5750\u6807\u8fdb\u884c\u7f16\u7801\u5df2\u8bc1\u660e\u5bf9\u4f4e\u7ef4\u4fe1\u53f7\u6709\u6548\uff0c\u4f46\u8f93\u5165\u9891\u7387\u548c\u7f51\u7edc\u7ed3\u6784\u7684\u9009\u62e9\u4ee5\u53ca\u53c2\u6570\u5197\u4f59\u7684\u7ba1\u7406\u4ecd\u662f\u6311\u6218\u3002", "method": "AIRe\u91c7\u7528\u4e24\u9636\u6bb5\u81ea\u9002\u5e94\u8bad\u7ec3\uff1a\u9996\u5148\u901a\u8fc7\u6743\u91cd\u8870\u51cf\u548c\u7ed3\u6784\u5316\u4fee\u526a\u6765\u8bc6\u522b\u5e76\u79fb\u9664\u8d21\u732e\u5ea6\u4f4e\u7684\u795e\u7ecf\u5143\uff0c\u4ee5\u907f\u514d\u5197\u4f59\uff1b\u7136\u540e\uff0c\u5728\u4fe1\u53f7\u6b20\u62df\u5408\u7684\u5149\u8c31\u533a\u57df\u589e\u52a0\u8f93\u5165\u9891\u7387\uff0c\u4ee5\u6269\u5c55\u8868\u793a\u57fa\u7840\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cAIRe\u5728\u56fe\u50cf\u548cSDFs\uff08Signed Distance Functions\uff09\u4e0a\u7684\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u591f\u51cf\u5c0f\u6a21\u578b\u5c3a\u5bf8\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u9ad8\u91cd\u5efa\u8d28\u91cf\u3002", "conclusion": "AIRe\u901a\u8fc7\u81ea\u9002\u5e94\u5730\u4f18\u5316INR\u7ed3\u6784\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u5728\u6a21\u578b\u5927\u5c0f\u548c\u91cd\u5efa\u8d28\u91cf\u4e4b\u95f4\u53d6\u5f97\u66f4\u597d\u6743\u8861\u7684\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2510.23827", "categories": ["quant-ph", "cond-mat.mes-hall", "cond-mat.other", "math-ph", "math.MP"], "pdf": "https://arxiv.org/pdf/2510.23827", "abs": "https://arxiv.org/abs/2510.23827", "authors": ["Xicheng Xu", "Ahmed Adel Mahmoud", "Noah Gorgichuk", "Ronny Thomale", "Steven Rayan", "Matteo Mariantoni"], "title": "A Scalable Superconducting Circuit Framework for Emulating Physics in Hyperbolic Space", "comment": "18 pages, 12 figures (5 figures in main body and 7 figures in\n  extended material)", "summary": "Theoretical studies and experiments in the last six years have revealed the\npotential for novel behaviours and functionalities in device physics through\nthe synthetic engineering of negatively-curved spaces. For instance, recent\ndevelopments in hyperbolic band theory have unveiled the emergence of\nhigher-dimensional eigenstates -- features fundamentally absent in conventional\nEuclidean systems. At the same time, superconducting quantum circuits have\nemerged as a leading platform for quantum analogue emulations and digital\nsimulations in scalable architectures. Here, we introduce a scalable\nsuperconducting circuit framework for the analogue quantum emulation of\ntight-binding models on hyperbolic and kagome-like lattices. Using this\napproach, we experimentally realize three distinct lattices, including, for the\nfirst time to our knowledge, a hyperbolic lattice whose unit cell resides on a\ngenus-3 Riemann surface. Our method encodes the hyperbolic metric directly into\ncapacitive couplings between high-quality superconducting resonators, enabling\ntenable reproduction of spectral and localization properties while overcoming\nmajor scalability and spectral resolution limitations of previous designs.\nThese results set the stage for large-scale experimental studies of hyperbolic\nmaterials in condensed matter physics and lay the groundwork for realizing\nhyperbolic quantum processors, with potential implications for both fundamental\nphysics and quantum computing", "AI": {"tldr": "\u5229\u7528\u8d85\u5c0e\u91cf\u5b50\u96fb\u8def\u6846\u67b6\uff0c\u5728\u96d9\u66f2\u548c kagome-like \u683c\u9ede\u4e0a\u9032\u884c\u91cf\u5b50\u985e\u6bd4\u6a21\u64ec\uff0c\u5be6\u9a57\u4e0a\u5be6\u73fe\u4e86\u5305\u542b\u524d\u6240\u672a\u898b\u7684\u4e09\u8449\u9762\u96d9\u66f2\u683c\u9ede\u5728\u5167\u7684\u4e09\u7a2e\u4e0d\u540c\u683c\u9ede\uff0c\u4e26\u5c55\u793a\u4e86\u5176\u983b\u8b5c\u548c\u5c40\u57df\u5316\u6027\u8cea\u3002", "motivation": "\u5229\u7528\u8ca0\u66f2\u7387\u7a7a\u9593\u7684\u5408\u6210\u5de5\u7a0b\u4f86\u63a2\u7d22\u65b0\u7a4e\u7684\u5668\u4ef6\u7269\u7406\u884c\u70ba\u548c\u529f\u80fd\uff0c\u7279\u5225\u662f\u96d9\u66f2\u80fd\u5e36\u7406\u8ad6\u63ed\u793a\u7684\u9ad8\u7dad\u7d50\u69cb \u0648\u0627\u0644\u062a\u064a\u537b\u5728\u50b3\u7d71\u6b50\u5e7e\u91cc\u5f97\u7cfb\u7d71\u4e2d\u7f3a\u5931\u3002", "method": "\u63d0\u51fa\u4e00\u500b\u53ef\u64f4\u5c55\u7684\u8d85\u5c0e\u91cf\u5b50\u96fb\u8def\u6846\u67b6\uff0c\u7528\u65bc\u96d9\u66f2\u548c kagome-like \u683c\u9ede\u7684\u7dca\u675f\u7e1b\u6a21\u578b\u7684\u91cf\u5b50\u985e\u6bd4\u6a21\u64ec\uff0c\u5176\u4e2d\u96d9\u66f2\u5ea6\u91cf\u76f4\u63a5\u7de8\u78bc\u5728\u9ad8\u54c1\u8cea\u8d85\u5c0e\u8ae7\u632f\u5668\u4e4b\u9593\u7684\u96fb\u5bb9\u8026\u5408\u4e2d\u3002", "result": "\u6210\u529f\u5be6\u73fe\u4e86\u4e09\u7a2e\u4e0d\u540c\u7684\u683c\u9ede\uff0c\u5305\u62ec\u4e00\u500b\u5177\u6709 genus-3 \u9ece\u66fc\u66f2\u9762\u55ae\u5143\u683c\u9ede\u7684\u96d9\u66f2\u683c\u9ede\uff0c\u4e26\u5c55\u793a\u4e86\u8207\u9810\u671f\u4e00\u81f4\u7684\u983b\u8b5c\u548c\u5c40\u57df\u5316\u6027\u8cea\uff0c\u514b\u670d\u4e86\u5148\u524d\u8a2d\u8a08\u7684\u64f4\u5c55\u6027\u548c\u983b\u8b5c\u89e3\u6790\u5ea6\u7684\u9650\u5236\u3002", "conclusion": "\u8a72\u7814\u7a76\u70ba\u51dd\u805a\u614b\u7269\u7406\u4e2d\u96d9\u66f2\u6750\u6599\u7684\u5927\u898f\u6a21\u5be6\u9a57\u7814\u7a76\u4ee5\u53ca\u5be6\u73fe\u5177\u6709\u6f5b\u5728\u57fa\u672c\u7269\u7406\u548c\u91cf\u5b50\u8a08\u7b97\u610f\u7fa9\u7684\u96d9\u66f2\u91cf\u5b50\u8655\u7406\u5668\u5960\u5b9a\u4e86\u57fa\u790e\u3002"}}
{"id": "2510.24633", "categories": ["cs.LG", "cs.LO"], "pdf": "https://arxiv.org/pdf/2510.24633", "abs": "https://arxiv.org/abs/2510.24633", "authors": ["Mingyue Liu", "Andrew Cropper"], "title": "Symbolic Snapshot Ensembles", "comment": null, "summary": "Inductive logic programming (ILP) is a form of logical machine learning. Most\nILP algorithms learn a single hypothesis from a single training run. Ensemble\nmethods train an ILP algorithm multiple times to learn multiple hypotheses. In\nthis paper, we train an ILP algorithm only once and save intermediate\nhypotheses. We then combine the hypotheses using a minimum description length\nweighting scheme. Our experiments on multiple benchmarks, including game\nplaying and visual reasoning, show that our approach improves predictive\naccuracy by 4% with less than 1% computational overhead.", "AI": {"tldr": "ILP \u7b97\u6cd5\u901a\u8fc7\u4e00\u6b21\u8bad\u7ec3\u548c\u4e2d\u95f4\u5047\u8bbe\u7ec4\u5408\u6765\u63d0\u9ad8\u51c6\u786e\u6027\uff0c\u8ba1\u7b97\u5f00\u9500\u5374\u5f88\u5c0f\u3002", "motivation": "\u5927\u591a\u6570 ILP \u7b97\u6cd5\u4e00\u6b21\u8bad\u7ec3\u53ea\u5b66\u4e60\u4e00\u4e2a\u5047\u8bbe\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u4e00\u6b21\u8bad\u7ec3\u5b66\u4e60\u591a\u4e2a\u5047\u8bbe\u5e76\u7ec4\u5408\u5b83\u4eec\uff0c\u4ee5\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u3002", "method": "\u901a\u8fc7\u4e00\u6b21 ILP \u8bad\u7ec3\u8fc7\u7a0b\u4fdd\u5b58\u4e2d\u95f4\u5047\u8bbe\uff0c\u5e76\u4f7f\u7528\u6700\u5c0f\u63cf\u8ff0\u957f\u5ea6\u52a0\u6743\u65b9\u6848\u7ec4\u5408\u8fd9\u4e9b\u5047\u8bbe\u3002", "result": "\u4e0e\u4f20\u7edf ILP \u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\uff08\u5305\u62ec\u6e38\u620f\u548c\u89c6\u89c9\u63a8\u7406\uff09\u4e0a\uff0c\u9884\u6d4b\u51c6\u786e\u6027\u63d0\u9ad8\u4e86 4%\uff0c\u8ba1\u7b97\u5f00\u9500\u4ec5\u589e\u52a0\u4e86\u4e0d\u5230 1%\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u65b9\u6cd5\u901a\u8fc7\u4e00\u6b21\u8bad\u7ec3\u548c\u4e2d\u95f4\u5047\u8bbe\u7ec4\u5408\uff0c\u5728\u4fdd\u6301\u8f83\u4f4e\u8ba1\u7b97\u5f00\u9500\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86 ILP \u7684\u9884\u6d4b\u51c6\u786e\u6027\u3002"}}
{"id": "2510.24191", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.24191", "abs": "https://arxiv.org/abs/2510.24191", "authors": ["Isabelle Krauss", "Victor G. Lopez", "Matthias A. M\u00fcller"], "title": "Sample-based Moving Horizon Estimation", "comment": null, "summary": "In this paper, we propose a sample-based moving horizon estimation (MHE)\nscheme for general nonlinear systems to estimate the current system state using\nirregularly and/or infrequently available measurements. The cost function of\nthe MHE optimization problem is suitably designed to accommodate these\nirregular output sequences. We also establish that, under a suitable\nsample-based detectability condition known as sample-based incremental\ninput/output-to-state stability (i-IOSS), the proposed sample-based MHE\nachieves robust global exponential stability (RGES). Additionally, for the case\nof linear systems, we draw connections between sample-based observability and\nsample-based i-IOSS. This demonstrates that previously established conditions\nfor linear systems to be sample-based observable can be utilized to verify or\ndesign sampling strategies that satisfy the conditions to guarantee RGES of the\nsample-based MHE. Finally, the effectiveness of the proposed sample-based MHE\nis illustrated through a simulation example.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u901a\u7528\u975e\u7ebf\u6027\u7cfb\u7edf\u6837\u672c\u79fb\u52a8\u89c6\u754c\u4f30\u8ba1\uff08MHE\uff09\u65b9\u6848\uff0c\u4ee5\u4f7f\u7528\u4e0d\u89c4\u5219/\u4e0d\u9891\u7e41\u7684\u6d4b\u91cf\u6765\u4f30\u8ba1\u5f53\u524d\u7cfb\u7edf\u72b6\u6001\u3002", "motivation": "\u4e3a\u901a\u7528\u975e\u7ebf\u6027\u7cfb\u7edf\u63d0\u51fa\u4e00\u79cd\u6837\u672cMHE\u65b9\u6848\uff0c\u4ee5\u5904\u7406\u4e0d\u89c4\u5219\u548c/\u6216\u4e0d\u9891\u7e41\u7684\u6d4b\u91cf\u3002", "method": "\u8bbe\u8ba1\u4e86MHE\u4f18\u5316\u95ee\u9898\u7684\u6210\u672c\u51fd\u6570\u6765\u9002\u5e94\u4e0d\u89c4\u5219\u8f93\u51fa\u5e8f\u5217\uff0c\u5e76\u5efa\u7acb\u4e86\u6837\u672ci-IOSS\u6761\u4ef6\uff0c\u4ee5\u5b9e\u73b0\u9c81\u68d2\u5168\u5c40\u6307\u6570\u7a33\u5b9a\u6027\uff08RGES\uff09\u3002\u5bf9\u4e8e\u7ebf\u6027\u7cfb\u7edf\uff0c\u5c06\u6837\u672c\u53ef\u89c2\u6d4b\u6027\u4e0e\u6837\u672ci-IOSS\u8054\u7cfb\u8d77\u6765\u3002", "result": "\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7684\u6837\u672cMHE\u5728\u6837\u672ci-IOSS\u6761\u4ef6\u4e0b\u53ef\u5b9e\u73b0RGES\u3002\u5bf9\u4e8e\u7ebf\u6027\u7cfb\u7edf\uff0c\u5c55\u793a\u4e86\u5982\u4f55\u5229\u7528\u6837\u672c\u53ef\u89c2\u6d4b\u6027\u6761\u4ef6\u6765\u4fdd\u8bc1RGES\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6837\u672cMHE\u65b9\u6848\u80fd\u591f\u6709\u6548\u5730\u5904\u7406\u4e0d\u89c4\u5219\u548c/\u6216\u4e0d\u9891\u7e41\u7684\u6d4b\u91cf\uff0c\u5e76\u80fd\u5b9e\u73b0\u9c81\u68d2\u5168\u5c40\u6307\u6570\u7a33\u5b9a\u6027\uff0c\u901a\u8fc7\u4eff\u771f\u793a\u4f8b\u5f97\u5230\u4e86\u9a8c\u8bc1\u3002"}}
{"id": "2510.24058", "categories": ["eess.SP", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24058", "abs": "https://arxiv.org/abs/2510.24058", "authors": ["Zihan Zhao", "Masood Mortazavi", "Ning Yan"], "title": "PULSE: Privileged Knowledge Transfer from Electrodermal Activity to Low-Cost Sensors for Stress Monitoring", "comment": "Accepted as a finders paper at ML4H 2025", "summary": "Electrodermal activity (EDA), the primary signal for stress detection,\nrequires costly hardware often unavailable in real-world wearables. In this\npaper, we propose PULSE, a framework that utilizes EDA exclusively during\nself-supervised pretraining, while enabling inference without EDA but with more\nreadily available modalities such as ECG, BVP, ACC, and TEMP. Our approach\nseparates encoder outputs into shared and private embeddings. We align shared\nembeddings across modalities and fuse them into a modality-invariant\nrepresentation. The private embeddings carry modality-specific information to\nsupport the reconstruction objective. Pretraining is followed by knowledge\ntransfer where a frozen EDA teacher transfers sympathetic-arousal\nrepresentations into student encoders. On WESAD, our method achieves strong\nstress-detection performance, showing that representations of privileged EDA\ncan be transferred to low-cost sensors to improve accuracy while reducing\nhardware cost.", "AI": {"tldr": "EDA \u4fe1\u53f7\u5728\u538b\u529b\u68c0\u6d4b\u4e2d\u662f\u4e3b\u8981\u7684\uff0c\u4f46\u6210\u672c\u9ad8\u6602\u3002PULSE \u6846\u67b6\u5728\u9884\u8bad\u7ec3\u65f6\u4f7f\u7528 EDA\uff0c\u5728\u63a8\u7406\u65f6\u5219\u4e0d\u9700\u8981 EDA\uff0c\u800c\u662f\u4f7f\u7528\u66f4\u6613\u83b7\u5f97\u7684\u6a21\u6001\uff0c\u5982 ECG\u3001BVP\u3001ACC \u548c TEMP\u3002\u8be5\u65b9\u6cd5\u5c06\u7f16\u7801\u5668\u8f93\u51fa\u5206\u4e3a\u5171\u4eab\u548c\u79c1\u6709\u5d4c\u5165\uff0c\u5bf9\u5171\u4eab\u5d4c\u5165\u8fdb\u884c\u6a21\u6001\u4e0d\u53d8\u8868\u793a\u7684\u878d\u5408\uff0c\u79c1\u6709\u5d4c\u5165\u5219\u7528\u4e8e\u91cd\u5efa\u3002\u9884\u8bad\u7ec3\u540e\uff0c\u901a\u8fc7\u77e5\u8bc6\u8f6c\u79fb\uff0cEDA \u6559\u5e08\u5c06\u4ea4\u611f\u795e\u7ecf\u5524\u9192\u8868\u5f81\u8f6c\u79fb\u5230\u5b66\u751f\u7f16\u7801\u5668\u4e2d\u3002\u5728 WESAD \u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u538b\u529b\u68c0\u6d4b\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u8868\u660e EDA \u7684\u8868\u5f81\u53ef\u4ee5\u8f6c\u79fb\u5230\u4f4e\u6210\u672c\u4f20\u611f\u5668\u4ee5\u63d0\u9ad8\u51c6\u786e\u6027\u5e76\u964d\u4f4e\u786c\u4ef6\u6210\u672c\u3002", "motivation": "EDA \u4fe1\u53f7\u662f\u538b\u529b\u68c0\u6d4b\u7684\u4e3b\u8981\u4fe1\u53f7\uff0c\u4f46\u5176\u6210\u672c\u9ad8\u6602\uff0c\u5728\u5b9e\u9645\u53ef\u7a7f\u6234\u8bbe\u5907\u4e2d\u96be\u4ee5\u83b7\u5f97\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5728\u4e0d\u4f9d\u8d56 EDA \u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u538b\u529b\u68c0\u6d4b\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a PULSE \u7684\u6846\u67b6\u3002\u8be5\u6846\u67b6\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\u4f7f\u7528 EDA \u4fe1\u53f7\uff0c\u800c\u5728\u63a8\u7406\u9636\u6bb5\u5219\u5229\u7528 ECG\u3001BVP\u3001ACC \u548c TEMP \u7b49\u66f4\u6613\u83b7\u5f97\u7684\u6a21\u6001\u3002\u8be5\u65b9\u6cd5\u5c06\u7f16\u7801\u5668\u8f93\u51fa\u5206\u4e3a\u5171\u4eab\u548c\u79c1\u6709\u5d4c\u5165\uff0c\u5e76\u901a\u8fc7\u5bf9\u5171\u4eab\u5d4c\u5165\u8fdb\u884c\u5bf9\u9f50\u548c\u878d\u5408\uff0c\u751f\u6210\u6a21\u6001\u4e0d\u53d8\u7684\u8868\u793a\u3002\u79c1\u6709\u5d4c\u5165\u5219\u7528\u4e8e\u652f\u6301\u91cd\u5efa\u76ee\u6807\u3002\u9884\u8bad\u7ec3\u540e\uff0c\u901a\u8fc7\u77e5\u8bc6\u8f6c\u79fb\uff0c\u5229\u7528\u4e00\u4e2a\u56fa\u5b9a\u7684 EDA \u6559\u5e08\u6a21\u578b\u5c06\u4ea4\u611f\u795e\u7ecf\u5524\u9192\u7684\u8868\u5f81\u8f6c\u79fb\u5230\u5b66\u751f\u7f16\u7801\u5668\u4e2d\u3002", "result": "\u5728 WESAD \u6570\u636e\u96c6\u4e0a\uff0cPULSE \u6846\u67b6\u5b9e\u73b0\u4e86\u4f18\u5f02\u7684\u538b\u529b\u68c0\u6d4b\u6027\u80fd\u3002", "conclusion": "PULSE \u6846\u67b6\u80fd\u591f\u5c06 EDA \u4fe1\u53f7\u7684\u8868\u5f81\u8f6c\u79fb\u5230\u4f4e\u6210\u672c\u4f20\u611f\u5668\u4e2d\uff0c\u4ece\u800c\u5728\u63d0\u9ad8\u51c6\u786e\u6027\u7684\u540c\u65f6\u964d\u4f4e\u786c\u4ef6\u6210\u672c\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u538b\u529b\u68c0\u6d4b\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2510.23633", "categories": ["cs.LG", "cs.AI", "cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.23633", "abs": "https://arxiv.org/abs/2510.23633", "authors": ["Xun Su", "Hiroyuki Kasai"], "title": "Noise is All You Need: Solving Linear Inverse Problems by Noise Combination Sampling with Diffusion Models", "comment": "9 pages", "summary": "Pretrained diffusion models have demonstrated strong capabilities in\nzero-shot inverse problem solving by incorporating observation information into\nthe generation process of the diffusion models. However, this presents an\ninherent dilemma: excessive integration can disrupt the generative process,\nwhile insufficient integration fails to emphasize the constraints imposed by\nthe inverse problem. To address this, we propose \\emph{Noise Combination\nSampling}, a novel method that synthesizes an optimal noise vector from a noise\nsubspace to approximate the measurement score, replacing the noise term in the\nstandard Denoising Diffusion Probabilistic Models process. This enables\nconditional information to be naturally embedded into the generation process\nwithout reliance on step-wise hyperparameter tuning. Our method can be applied\nto a wide range of inverse problem solvers, including image compression, and,\nparticularly when the number of generation steps $T$ is small, achieves\nsuperior performance with negligible computational overhead, significantly\nimproving robustness and stability.", "AI": {"tldr": "\u901a\u8fc7\u7ed3\u5408\u566a\u58f0\u5b50\u7a7a\u95f4\u4e2d\u7684\u566a\u58f0\u5411\u91cf\u6765\u89e3\u51b3\u9006\u95ee\u9898\uff0c\u5728\u4e0d\u727a\u7272\u751f\u6210\u8fc7\u7a0b\u7684\u60c5\u51b5\u4e0b\u6ee1\u8db3\u7ea6\u675f\uff0c\u5e76\u4e14\u51e0\u4e4e\u6ca1\u6709\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u73b0\u6709\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u5728\u89e3\u51b3\u9006\u95ee\u9898\u65f6\uff0c\u5728\u6574\u5408\u89c2\u6d4b\u4fe1\u606f\u548c\u4fdd\u6301\u751f\u6210\u8fc7\u7a0b\u4e4b\u95f4\u5b58\u5728\u4e24\u96be\uff1a\u6574\u5408\u8fc7\u591a\u4f1a\u7834\u574f\u751f\u6210\u8fc7\u7a0b\uff0c\u6574\u5408\u4e0d\u8db3\u5219\u65e0\u6cd5\u6ee1\u8db3\u9006\u95ee\u9898\u7684\u7ea6\u675f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u566a\u58f0\u7ec4\u5408\u91c7\u6837\u201d\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ece\u566a\u58f0\u5b50\u7a7a\u95f4\u5408\u6210\u6700\u4f18\u566a\u58f0\u5411\u91cf\u6765\u8fd1\u4f3c\u6d4b\u91cf\u5206\u6570\uff0c\u4ece\u800c\u53d6\u4ee3\u6807\u51c6\u53bb\u566a\u6269\u6563\u6982\u7387\u6a21\u578b\u8fc7\u7a0b\u4e2d\u7684\u566a\u58f0\u9879\uff0c\u4f7f\u6761\u4ef6\u4fe1\u606f\u80fd\u591f\u81ea\u7136\u5730\u5d4c\u5165\u751f\u6210\u8fc7\u7a0b\uff0c\u65e0\u9700\u8fdb\u884c\u5206\u6b65\u8d85\u53c2\u6570\u8c03\u6574\u3002", "result": "\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u5404\u79cd\u9006\u95ee\u9898\u6c42\u89e3\u5668\uff0c\u7279\u522b\u662f\u5f53\u751f\u6210\u6b65\u6570 T \u8f83\u5c0f\u65f6\uff0c\u80fd\u4ee5\u53ef\u5ffd\u7565\u7684\u8ba1\u7b97\u5f00\u9500\u5b9e\u73b0\u5353\u8d8a\u7684\u6027\u80fd\uff0c\u663e\u8457\u63d0\u9ad8\u9c81\u68d2\u6027\u548c\u7a33\u5b9a\u6027\u3002", "conclusion": "\u566a\u58f0\u7ec4\u5408\u91c7\u6837\u662f\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u89e3\u51b3\u9006\u95ee\u9898\u7684\u540c\u65f6\u4fdd\u6301\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u80fd\u529b\uff0c\u5e76\u4e14\u5177\u6709\u8ba1\u7b97\u6548\u7387\u9ad8\u3001\u9c81\u68d2\u6027\u548c\u7a33\u5b9a\u6027\u9ad8\u7b49\u4f18\u70b9\u3002"}}
{"id": "2510.23881", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23881", "abs": "https://arxiv.org/abs/2510.23881", "authors": ["Xidong Feng", "Vivek Veeriah", "Marcus Chiam", "Michael Dennis", "Ryan Pachauri", "Thomas Tumiel", "Federico Barbero", "Johan Obando-Ceron", "Jiaxin Shi", "Satinder Singh", "Shaobo Hou", "Nenad Toma\u0161ev", "Tom Zahavy"], "title": "Generating Creative Chess Puzzles", "comment": null, "summary": "While Generative AI rapidly advances in various domains, generating truly\ncreative, aesthetic, and counter-intuitive outputs remains a challenge. This\npaper presents an approach to tackle these difficulties in the domain of chess\npuzzles. We start by benchmarking Generative AI architectures, and then\nintroduce an RL framework with novel rewards based on chess engine search\nstatistics to overcome some of those shortcomings. The rewards are designed to\nenhance a puzzle's uniqueness, counter-intuitiveness, diversity, and realism.\nOur RL approach dramatically increases counter-intuitive puzzle generation by\n10x, from 0.22\\% (supervised) to 2.5\\%, surpassing existing dataset rates\n(2.1\\%) and the best Lichess-trained model (0.4\\%). Our puzzles meet novelty\nand diversity benchmarks, retain aesthetic themes, and are rated by human\nexperts as more creative, enjoyable, and counter-intuitive than composed book\npuzzles, even approaching classic compositions. Our final outcome is a curated\nbooklet of these AI-generated puzzles, which is acknowledged for creativity by\nthree world-renowned experts.", "AI": {"tldr": "\u751f\u6210\u5f0fAI\u5728\u56fd\u9645\u8c61\u68cb\u8c1c\u9898\u751f\u6210\u65b9\u9762\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u65b0\u9896\u7684\u5956\u52b1\u673a\u5236\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u53cd\u76f4\u89c9\u8c1c\u9898\u7684\u751f\u6210\u6bd4\u4f8b\uff0c\u5e76\u751f\u6210\u4e86\u88ab\u4e13\u5bb6\u8ba4\u53ef\u7684\u3001\u5bcc\u6709\u521b\u9020\u529b\u7684\u8c1c\u9898\u3002", "motivation": "\u751f\u6210\u5f0fAI\u5728\u751f\u6210\u771f\u6b63\u5177\u6709\u521b\u9020\u6027\u3001\u7f8e\u89c2\u6027\u548c\u53cd\u76f4\u89c9\u7684\u8f93\u51fa\u65b9\u9762\u4ecd\u5b58\u5728\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u56fd\u9645\u8c61\u68cb\u8c1c\u9898\u751f\u6210\u9886\u57df\u3002", "method": "\u9996\u5148\u5bf9\u751f\u6210\u5f0fAI\u67b6\u6784\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7136\u540e\u5f15\u5165\u4e00\u4e2a\u57fa\u4e8e\u56fd\u9645\u8c61\u68cb\u5f15\u64ce\u641c\u7d22\u7edf\u8ba1\u4fe1\u606f\u7684\u65b0\u9896\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u4ee5\u589e\u5f3a\u8c1c\u9898\u7684\u72ec\u7279\u6027\u3001\u53cd\u76f4\u89c9\u6027\u3001\u591a\u6837\u6027\u548c\u771f\u5b9e\u6027\u3002", "result": "\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5c06\u53cd\u76f4\u89c9\u8c1c\u9898\u7684\u751f\u6210\u6bd4\u4f8b\u4ece\u76d1\u7763\u5b66\u4e60\u76840.22%\u63d0\u9ad8\u52302.5%\uff0c\u8d85\u8fc7\u4e86\u73b0\u6709\u6570\u636e\u96c6\uff082.1%\uff09\u548cLichess\u8bad\u7ec3\u6a21\u578b\u7684\uff080.4%\uff09\u3002\u751f\u6210\u8c1c\u9898\u5728\u65b0\u9896\u6027\u548c\u591a\u6837\u6027\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u5e76\u88ab\u4eba\u7c7b\u4e13\u5bb6\u8bc4\u4e3a\u6bd4\u4f20\u7edf\u8c1c\u9898\u66f4\u5177\u521b\u9020\u6027\u3001\u8da3\u5473\u6027\u548c\u53cd\u76f4\u89c9\u6027\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u5bcc\u6709\u521b\u9020\u6027\u548c\u53cd\u76f4\u89c9\u7684\u56fd\u9645\u8c61\u68cb\u8c1c\u9898\uff0c\u5e76\u901a\u8fc7\u4e13\u5bb6\u8bc4\u5ba1\u548c\u51fa\u7248\u8c1c\u9898\u96c6\u8bc1\u660e\u4e86\u5176\u6210\u529f\u3002"}}
{"id": "2510.23921", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23921", "abs": "https://arxiv.org/abs/2510.23921", "authors": ["Kaveh Eskandari Miandoab", "Mahammed Kamruzzaman", "Arshia Gharooni", "Gene Louis Kim", "Vasanth Sarathy", "Ninareh Mehrabi"], "title": "Breaking the Benchmark: Revealing LLM Bias via Minimal Contextual Augmentation", "comment": "9 pages, 3 figures, 3 tables", "summary": "Large Language Models have been shown to demonstrate stereotypical biases in\ntheir representations and behavior due to the discriminative nature of the data\nthat they have been trained on. Despite significant progress in the development\nof methods and models that refrain from using stereotypical information in\ntheir decision-making, recent work has shown that approaches used for bias\nalignment are brittle. In this work, we introduce a novel and general\naugmentation framework that involves three plug-and-play steps and is\napplicable to a number of fairness evaluation benchmarks. Through application\nof augmentation to a fairness evaluation dataset (Bias Benchmark for Question\nAnswering (BBQ)), we find that Large Language Models (LLMs), including\nstate-of-the-art open and closed weight models, are susceptible to\nperturbations to their inputs, showcasing a higher likelihood to behave\nstereotypically. Furthermore, we find that such models are more likely to have\nbiased behavior in cases where the target demographic belongs to a community\nless studied by the literature, underlining the need to expand the fairness and\nsafety research to include more diverse communities.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u56e0\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u6b67\u89c6\u6027\u800c\u8868\u73b0\u51fa\u523b\u677f\u5370\u8c61\u504f\u89c1\uff1b\u73b0\u6709\u53bb\u504f\u89c1\u65b9\u6cd5\u5b58\u5728\u7f3a\u9677\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u901a\u7528\u589e\u5f3a\u6846\u67b6\uff0c\u5e76\u5e94\u7528\u4e8e\u504f\u89c1\u8bc4\u4f30\u57fa\u51c6\uff08BBQ\uff09\uff0c\u53d1\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8f93\u5165\u6270\u52a8\u4e0b\u66f4\u6613\u8868\u73b0\u51fa\u523b\u677f\u5370\u8c61\u504f\u89c1\uff0c\u5c24\u5176\u662f\u5728\u7814\u7a76\u8f83\u5c11\u7684\u7fa4\u4f53\u4e2d\uff0c\u51f8\u663e\u4e86\u6269\u5927\u516c\u5e73\u6027\u7814\u7a76\u8303\u56f4\u7684\u5fc5\u8981\u6027\u3002", "motivation": "\u73b0\u6709\u7528\u4e8e\u6d88\u9664\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u523b\u677f\u5370\u8c61\u504f\u89c1\u7684\u65b9\u6cd5\u5b58\u5728\u6613\u53d7\u653b\u51fb\u7684\u7f3a\u9677\uff0c\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u6b64\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u5305\u542b\u4e09\u4e2a\u5373\u63d2\u5373\u7528\u6b65\u9aa4\u7684\u65b0\u9896\u901a\u7528\u589e\u5f3a\u6846\u67b6\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u504f\u89c1\u8bc4\u4f30\u6570\u636e\u96c6\uff08BBQ\uff09\u3002", "result": "\u901a\u8fc7\u5e94\u7528\u589e\u5f3a\u6846\u67b6\u4e8eBBQ\u6570\u636e\u96c6\uff0c\u53d1\u73b0\u5305\u62ec\u6700\u5148\u8fdb\u7684\u5f00\u653e\u548c\u5c01\u95ed\u6743\u91cd\u6a21\u578b\u5728\u5185\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u8f93\u5165\u6270\u52a8\u4e0b\u66f4\u5bb9\u6613\u8868\u73b0\u51fa\u523b\u677f\u5370\u8c61\u504f\u89c1\u3002\u6b64\u5916\uff0c\u5728\u76ee\u6807\u4eba\u7fa4\u662f\u7814\u7a76\u8f83\u5c11\u7684\u7fa4\u4f53\u65f6\uff0c\u6a21\u578b\u8868\u73b0\u51fa\u504f\u89c1\u884c\u4e3a\u7684\u53ef\u80fd\u6027\u66f4\u5927\u3002", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bb9\u6613\u53d7\u5230\u8f93\u5165\u6270\u52a8\u7684\u5f71\u54cd\uff0c\u8868\u73b0\u51fa\u523b\u677f\u5370\u8c61\u504f\u89c1\uff0c\u5c24\u5176\u662f\u5728\u7814\u7a76\u8f83\u5c11\u7684\u7fa4\u4f53\u4e2d\u3002\u8fd9\u8868\u660e\u9700\u8981\u6269\u5927\u516c\u5e73\u6027\u548c\u5b89\u5168\u7814\u7a76\u7684\u8303\u56f4\uff0c\u4ee5\u5305\u542b\u66f4\u591a\u6837\u5316\u7684\u793e\u7fa4\u3002"}}
{"id": "2510.24055", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24055", "abs": "https://arxiv.org/abs/2510.24055", "authors": ["Xiucheng Zhang", "Yang Jiang", "Hongwei Qing", "Jiashuo Bai"], "title": "Language-Conditioned Representations and Mixture-of-Experts Policy for Robust Multi-Task Robotic Manipulation", "comment": "8 pages", "summary": "Perceptual ambiguity and task conflict limit multitask robotic manipulation\nvia imitation learning. We propose a framework combining a Language-Conditioned\nVisual Representation (LCVR) module and a Language-conditioned\nMixture-ofExperts Density Policy (LMoE-DP). LCVR resolves perceptual\nambiguities by grounding visual features with language instructions, enabling\ndifferentiation between visually similar tasks. To mitigate task conflict,\nLMoE-DP uses a sparse expert architecture to specialize in distinct, multimodal\naction distributions, stabilized by gradient modulation. On real-robot\nbenchmarks, LCVR boosts Action Chunking with Transformers (ACT) and Diffusion\nPolicy (DP) success rates by 33.75% and 25%, respectively. The full framework\nachieves a 79% average success, outperforming the advanced baseline by 21%. Our\nwork shows that combining semantic grounding and expert specialization enables\nrobust, efficient multi-task manipulation", "AI": {"tldr": "\u901a\u8fc7\u7ed3\u5408\u8bed\u8a00\u6761\u4ef6\u89c6\u89c9\u8868\u793a\uff08LCVR\uff09\u548c\u8bed\u8a00\u6761\u4ef6\u4e13\u5bb6\u6df7\u5408\u5bc6\u5ea6\u7b56\u7565\uff08LMoE-DP\uff09\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u591a\u4efb\u52a1\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u611f\u77e5\u6b67\u4e49\u548c\u4efb\u52a1\u51b2\u7a81\u95ee\u9898\u3002", "motivation": "\u591a\u4efb\u52a1\u673a\u5668\u4eba\u64cd\u4f5c\u53d7\u9650\u4e8e\u611f\u77e5\u6b67\u4e49\uff08\u4f8b\u5982\uff0c\u89c6\u89c9\u4e0a\u76f8\u4f3c\u7684\u4efb\u52a1\u96be\u4ee5\u533a\u5206\uff09\u548c\u4efb\u52a1\u51b2\u7a81\uff08\u4f8b\u5982\uff0c\u4e0d\u540c\u7684\u4efb\u52a1\u53ef\u80fd\u9700\u8981\u77db\u76fe\u7684\u52a8\u4f5c\uff09\u3002", "method": "LCVR\u6a21\u5757\u901a\u8fc7\u8bed\u8a00\u6307\u4ee4\u6765\u7ea6\u675f\u89c6\u89c9\u7279\u5f81\uff0c\u89e3\u51b3\u611f\u77e5\u6b67\u4e49\u3002LMoE-DP\u6a21\u5757\u5229\u7528\u7a00\u758f\u4e13\u5bb6\u67b6\u6784\u6765\u4e13\u95e8\u5904\u7406\u4e0d\u540c\u7684\u3001\u591a\u6a21\u6001\u7684\u52a8\u4f5c\u5206\u5e03\uff0c\u5e76\u901a\u8fc7\u68af\u5ea6\u8c03\u5236\u6765\u7a33\u5b9a\u8bad\u7ec3\uff0c\u4ee5\u7f13\u89e3\u4efb\u52a1\u51b2\u7a81\u3002", "result": "\u5728\u771f\u5b9e\u673a\u5668\u4eba\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLCVR\u5355\u72ec\u4f7f\u7528\u65f6\uff0c\u80fd\u591f\u4f7fACT\u548cDP\u7684\u6210\u529f\u7387\u5206\u522b\u63d0\u9ad833.75%\u548c25%\u3002\u5b8c\u6574\u7684\u6846\u67b6\u5b9e\u73b0\u4e8679%\u7684\u5e73\u5747\u6210\u529f\u7387\uff0c\u6bd4\u5148\u8fdb\u7684\u57fa\u7ebf\u63d0\u9ad8\u4e8621%\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u7ed3\u5408\u8bed\u4e49\u7406\u89e3\u548c\u4e13\u5bb6\u4e13\u95e8\u5316\u80fd\u591f\u5b9e\u73b0\u9c81\u68d2\u4e14\u9ad8\u6548\u7684\u591a\u4efb\u52a1\u673a\u5668\u4eba\u64cd\u4f5c\u3002"}}
{"id": "2510.24305", "categories": ["cond-mat.mtrl-sci", "cond-mat.other"], "pdf": "https://arxiv.org/pdf/2510.24305", "abs": "https://arxiv.org/abs/2510.24305", "authors": ["Alexis Forestier", "Gunnar Weck", "Sandra Ninet", "Gaston Garbarino", "Mohamed Mezouar", "Fr\u00e9d\u00e9ric Datchi", "Paul Loubeyre"], "title": "Observation of Hexagonal Close-Packed Water Ice at Extreme Pressures and Temperatures", "comment": null, "summary": "The determination of the phase diagram of water ice under extreme conditions\nremains a fundamental challenge in high-pressure physics. While theoretical\npredictions have long suggested the existence of compact phases, such as\nface-centered cubic (fcc) and hexagonal close-packed (hcp) structures, yet only\nthe fcc phase has been experimentally confirmed. Here, we report the first\ndirect observation of a hcp ice phase using synchrotron x-ray diffraction in\nlaser-heated diamond anvil cells. Between 80 and 200 GPa, we observe the\ncoexistence of fcc and hcp ice, arising from stacking disorder in the fcc\noxygen layers, similar to phenomena seen in compressed noble gases. Above 200\nGPa, the hcp phase becomes dominant and is recovered at 300 K, indicating its\nincreased thermodynamic stability at ultrahigh pressures. These findings not\nonly expand our understanding of water's complex behavior under extreme\nconditions but also may play a crucial role in the interiors of giant icy\nplanets.", "AI": {"tldr": "\u572880-200 GPa\u4e0b\uff0c\u89c2\u5bdf\u5230\u4e86fcc\u548chcp\u51b0\u7684\u5171\u5b58\uff0chcp\u51b0\u5728200 GPa\u4ee5\u4e0a\u6210\u4e3a\u4e3b\u5bfc\u76f8\u3002", "motivation": "\u786e\u5b9a\u6781\u7aef\u6761\u4ef6\u4e0b\u6c34\u51b0\u76f8\u56fe\u662f\u9ad8\u538b\u7269\u7406\u5b66\u7684\u57fa\u672c\u6311\u6218\u3002", "method": "\u4f7f\u7528\u540c\u6b65X\u5c04\u7ebf\u884d\u5c04\u548c\u6fc0\u5149\u52a0\u70ed\u91d1\u521a\u77f3\u538b\u8154\u3002", "result": "\u9996\u6b21\u76f4\u63a5\u89c2\u5bdf\u5230hcp\u51b0\u76f8\uff0c\u5e76\u786e\u5b9a\u5176\u5728\u8d85\u9ad8\u538b\u4e0b\u6bd4fcc\u51b0\u66f4\u7a33\u5b9a\u3002", "conclusion": "hcp\u51b0\u76f8\u7684\u53d1\u73b0\u6269\u5c55\u4e86\u6211\u4eec\u5bf9\u6c34\u5728\u6781\u7aef\u6761\u4ef6\u4e0b\u884c\u4e3a\u7684\u7406\u89e3\uff0c\u5e76\u53ef\u80fd\u5bf9\u7406\u89e3\u5de8\u578b\u51b0\u884c\u661f\u7684\u5185\u90e8\u7ed3\u6784\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2510.24689", "categories": ["cond-mat.mes-hall", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2510.24689", "abs": "https://arxiv.org/abs/2510.24689", "authors": ["Fatimah Habis", "Yuanxi Wang"], "title": "Bonding Character as a Descriptor for Huang-Rhys Factors in Optically Active Defects", "comment": null, "summary": "The electron phonon coupling of a defect characterized by its Huang Rhys (HR)\nfactor is a crucial metric determining its excited-state dynamics, relevant to\ndefect applications as qubits and quantum emitters. However, HR factors remain\nchallenging to calculate from first principles, complicated by convergence\nissues in excited-state relaxation and time consuming phonon calculations. Even\nwhen calculated, HR factors lack a rational design principle. Here we show that\nan orbital-based descriptor can be used to rationalize and efficiently estimate\nHR factors. Combining this descriptor with a ground state deformation technique\nallows circumventing both excited state relaxation and full phonon\ncalculations. Specifically, our descriptor for HR factors is constructed using\nbonding character differences obtained from ground state density functional\ntheory, measured using crystal orbital Hamilton populations. We demonstrate\nthis descriptor for prototypical hBN defects and the diamond NV center. This\norbital-based descriptor can be potentially used in high throughput\ncomputational screening to identify ideal candidates of spin qubits and SPEs.", "AI": {"tldr": "\u7535\u5b50-\u58f0\u5b50\u8026\u5408\u662f\u51b3\u5b9a\u7f3a\u9677\u6fc0\u53d1\u6001\u52a8\u529b\u5b66\u7684\u5173\u952e\u6307\u6807\uff0c\u4f46\u4ece\u5934\u7b97\u8d77\u8ba1\u7b97\u9ec4-\u9510\u56e0\u5b50\uff08HR\u56e0\u5b50\uff09\u5b58\u5728\u6536\u655b\u6027\u95ee\u9898\u548c\u8017\u65f6\u7684\u58f0\u5b50\u8ba1\u7b97\u7b49\u6311\u6218\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8f68\u9053\u7684\u63cf\u8ff0\u7b26\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u4f30\u8ba1HR\u56e0\u5b50\uff0c\u5e76\u7ed3\u5408\u4e86\u57fa\u6001\u5f62\u53d8\u6280\u672f\uff0c\u907f\u514d\u4e86\u6fc0\u53d1\u6001\u5f1b\u8c6b\u548c\u5168\u58f0\u5b50\u8ba1\u7b97\u3002\u8be5\u63cf\u8ff0\u7b26\u57fa\u4e8e\u57fa\u6001\u5bc6\u5ea6\u6cdb\u51fd\u7406\u8bba\u8ba1\u7b97\u7684\u6676\u4f53\u8f68\u9053\u54c8\u5bc6\u987f\u91cf\u5e03\u5c45\u6570\uff08COHP\uff09\u6765\u8861\u91cf\u6210\u952e\u7279\u5f81\u7684\u5dee\u5f02\u3002\u5728hBN\u7f3a\u9677\u548c\u91d1\u521a\u77f3NV\u8272\u5fc3\u7b49\u539f\u578b\u4f53\u7cfb\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u63cf\u8ff0\u7b26\u7684\u6709\u6548\u6027\uff0c\u5e76\u6709\u671b\u7528\u4e8e\u9ad8\u901a\u91cf\u8ba1\u7b97\u7b5b\u9009\uff0c\u4ee5\u8bc6\u522b\u7406\u60f3\u7684\u81ea\u65cb\u91cf\u5b50\u6bd4\u7279\u548c\u5355\u5149\u5b50\u53d1\u5c04\u5668\u3002", "motivation": "\u9ec4-\u9510\u56e0\u5b50\uff08HR\u56e0\u5b50\uff09\u662f\u51b3\u5b9a\u7f3a\u9677\u6fc0\u53d1\u6001\u52a8\u529b\u5b66\u7684\u5173\u952e\u6307\u6807\uff0c\u5bf9\u4e8e\u7f3a\u9677\u5728\u91cf\u5b50\u6bd4\u7279\u548c\u91cf\u5b50\u53d1\u5149\u5668\u4e2d\u7684\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u4ece\u7b2c\u4e00\u6027\u539f\u7406\u8ba1\u7b97HR\u56e0\u5b50\u5b58\u5728\u6536\u655b\u6027\u95ee\u9898\u4ee5\u53ca\u8017\u65f6\u7684\u58f0\u5b50\u8ba1\u7b97\u7b49\u6311\u6218\uff0c\u5e76\u4e14\u7f3a\u4e4f\u5408\u7406\u7684\u8bbef\u8ba1\u539f\u5219\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8f68\u9053\u7684\u63cf\u8ff0\u7b26\uff0c\u8be5\u63cf\u8ff0\u7b26\u4f7f\u7528\u57fa\u6001\u5bc6\u5ea6\u6cdb\u51fd\u7406\u8bba\u8ba1\u7b97\u7684\u6210\u952e\u7279\u5f81\u5dee\u5f02\uff08\u901a\u8fc7\u6676\u4f53\u8f68\u9053\u54c8\u5bc6\u987f\u91cf\u5e03\u5c45\u6570\u6d4b\u91cf\uff09\u6765\u6784\u5efa\uff0c\u5e76\u7ed3\u5408\u4e86\u57fa\u6001\u5f62\u53d8\u6280\u672f\uff0c\u4ece\u800c\u53ef\u4ee5\u5408\u7406\u5316\u548c\u6709\u6548\u4f30\u8ba1HR\u56e0\u5b50\uff0c\u540c\u65f6\u907f\u514d\u4e86\u6fc0\u53d1\u6001\u5f1b\u8c6b\u548c\u5168\u58f0\u5b50\u8ba1\u7b97\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u8f68\u9053\u7684\u63cf\u8ff0\u7b26\uff0c\u7528\u4e8e\u8ba1\u7b97\u548c\u4f30\u8ba1HR\u56e0\u5b50\uff0c\u5e76\u901a\u8fc7hBN\u7f3a\u9677\u548c\u91d1\u521a\u77f3NV\u8272\u5fc3\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002\u8be5\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u907f\u514d\u6602\u8d35\u7684\u6fc0\u53d1\u6001\u8ba1\u7b97\u548c\u58f0\u5b50\u8ba1\u7b97\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8e\u8f68\u9053\u7684\u63cf\u8ff0\u7b26\u7ed3\u5408\u57fa\u6001\u5f62\u53d8\u6280\u672f\uff0c\u4e3a\u8ba1\u7b97\u548c\u8bbe\u8ba1HR\u56e0\u5b50\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u884c\u7684\u65b9\u6848\uff0c\u6709\u671b\u7528\u4e8e\u9ad8\u901a\u91cf\u8ba1\u7b97\u7b5b\u9009\uff0c\u4ee5\u8bc6\u522b\u7406\u60f3\u7684\u91cf\u5b50\u6bd4\u7279\u548c\u5355\u5149\u5b50\u53d1\u5c04\u5668\u6750\u6599\u3002"}}
{"id": "2510.23956", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23956", "abs": "https://arxiv.org/abs/2510.23956", "authors": ["Alejandro Escontrela", "Shrinu Kushagra", "Sjoerd van Steenkiste", "Yulia Rubanova", "Aleksander Holynski", "Kelsey Allen", "Kevin Murphy", "Thomas Kipf"], "title": "Neural USD: An object-centric framework for iterative editing and control", "comment": "22 pages, 16 figures, 1 table", "summary": "Amazing progress has been made in controllable generative modeling,\nespecially over the last few years. However, some challenges remain. One of\nthem is precise and iterative object editing. In many of the current methods,\ntrying to edit the generated image (for example, changing the color of a\nparticular object in the scene or changing the background while keeping other\nelements unchanged) by changing the conditioning signals often leads to\nunintended global changes in the scene. In this work, we take the first steps\nto address the above challenges. Taking inspiration from the Universal Scene\nDescriptor (USD) standard developed in the computer graphics community, we\nintroduce the \"Neural Universal Scene Descriptor\" or Neural USD. In this\nframework, we represent scenes and objects in a structured, hierarchical\nmanner. This accommodates diverse signals, minimizes model-specific\nconstraints, and enables per-object control over appearance, geometry, and\npose. We further apply a fine-tuning approach which ensures that the above\ncontrol signals are disentangled from one another. We evaluate several design\nconsiderations for our framework, demonstrating how Neural USD enables\niterative and incremental workflows. More information at:\nhttps://escontrela.me/neural_usd .", "AI": {"tldr": "Neural USD \u6846\u67b6\u901a\u8fc7\u7ed3\u6784\u5316\u573a\u666f\u8868\u793a\u5b9e\u73b0\u7cbe\u786e\u3001\u8fed\u4ee3\u7684\u5bf9\u8c61\u7f16\u8f91\uff0c\u89e3\u51b3\u4e86\u5f53\u524d\u751f\u6210\u6a21\u578b\u4e2d\u5b58\u5728\u7684\u5168\u5c40\u7f16\u8f91\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u53ef\u63a7\u751f\u6210\u6a21\u578b\u5728\u7cbe\u786e\u3001\u8fed\u4ee3\u7684\u5bf9\u8c61\u7f16\u8f91\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u5982\u989c\u8272\u6216\u80cc\u666f\u66f4\u6539\u5e38\u5bfc\u81f4\u975e\u9884\u671f\u7684\u5168\u5c40\u53d8\u5316\u3002", "method": "\u63d0\u51fa Neural USD \u6846\u67b6\uff0c\u501f\u9274\u8ba1\u7b97\u673a\u56fe\u5f62\u5b66\u4e2d\u7684 USD \u6807\u51c6\uff0c\u4ee5\u7ed3\u6784\u5316\u3001\u5206\u5c42\u7684\u65b9\u5f0f\u8868\u793a\u573a\u666f\u548c\u5bf9\u8c61\uff0c\u5b9e\u73b0\u5bf9\u5916\u89c2\u3001\u51e0\u4f55\u548c\u59ff\u6001\u7684\u9010\u5bf9\u8c61\u63a7\u5236\uff0c\u5e76\u901a\u8fc7\u5fae\u8c03\u786e\u4fdd\u63a7\u5236\u4fe1\u53f7\u89e3\u8026\u3002", "result": "Neural USD \u6846\u67b6\u80fd\u591f\u5b9e\u73b0\u8fed\u4ee3\u5f0f\u3001\u589e\u91cf\u5f0f\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5e76\u80fd\u8fdb\u884c\u7cbe\u786e\u7684\u5bf9\u8c61\u7f16\u8f91\u3002", "conclusion": "Neural USD \u6846\u67b6\u4e3a\u7cbe\u786e\u3001\u8fed\u4ee3\u7684\u5bf9\u8c61\u7f16\u8f91\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.23862", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.23862", "abs": "https://arxiv.org/abs/2510.23862", "authors": ["Moein Kazemi", "Mehdi Keshavarz", "Mark E. Turiansky", "John L. Lyons", "Nikolay V. Abrosimov", "Stephanie Simmons", "Daniel B. Higginbottom", "Mike L. W. Thewalt"], "title": "Giant Isotope Effect on the Excited-State Lifetime and Emission Efficiency of the Silicon T Centre", "comment": "7 pages, 4 figures", "summary": "Efficient single-photon emitters are desirable for quantum technologies\nincluding quantum networks and photonic quantum computers. We investigate the T\ncentre, a telecommunications-band emitter in silicon, and find a strong isotope\ndependence of its excited-state lifetime. In particular, the lifetime of the\ndeuterium T centre is over five times longer than the common protium variant.\nThrough explicit first-principles calculations, we demonstrate that this\ndramatic difference is due to a reduction in the carbon-hydrogen local\nvibrational mode energy, which suppresses non-radiative decay. Our results\nimply that the deuterium T centre approaches unit quantum efficiency, enabling\nmore efficient single-photon sources, quantum memories, and entanglement\ngeneration.", "AI": {"tldr": "T\u4e2d\u5fc3\uff08\u4e00\u79cd\u7845\u57fa\u7535\u4fe1\u6ce2\u6bb5\u53d1\u5149\u5668\uff09\u7684\u5bff\u547d\u5bf9\u5176\u540c\u4f4d\u7d20\u6709\u663e\u8457\u7684\u4f9d\u8d56\u6027\uff0c\u5176\u4e2d\u6c18T\u4e2d\u5fc3\u7684\u5bff\u547d\u6bd4\u5e38\u89c1\u7684\u6c15\u53d8\u4f53\u957f\u4e94\u500d\u4ee5\u4e0a\u3002", "motivation": "\u4e3a\u4e86\u5b9e\u73b0\u5305\u62ec\u91cf\u5b50\u7f51\u7edc\u548c\u5149\u91cf\u5b50\u8ba1\u7b97\u673a\u5728\u5185\u7684\u91cf\u5b50\u6280\u672f\uff0c\u9700\u8981\u9ad8\u6548\u7684\u5355\u5149\u5b50\u53d1\u5149\u5668\u3002", "method": "\u901a\u8fc7\u660e\u786e\u7684\u7b2c\u4e00\u6027\u539f\u7406\u8ba1\u7b97\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u8fd9\u79cd\u663e\u8457\u5dee\u5f02\u662f\u7531\u4e8e\u78b3-\u6c22\u5c40\u90e8\u632f\u52a8\u6a21\u5f0f\u80fd\u91cf\u7684\u964d\u4f4e\uff0c\u4ece\u800c\u6291\u5236\u4e86\u975e\u8f90\u5c04\u8870\u51cf\u3002", "result": "\u6c18T\u4e2d\u5fc3\u7684\u5bff\u547d\u6bd4\u5e38\u89c1\u7684\u6c15\u53d8\u4f53\u957f\u4e94\u500d\u4ee5\u4e0a\uff0c\u5e76\u4e14\u63a5\u8fd1\u5355\u4f4d\u91cf\u5b50\u6548\u7387\u3002", "conclusion": "\u6c18T\u4e2d\u5fc3\u63a5\u8fd1\u5355\u4f4d\u91cf\u5b50\u6548\u7387\uff0c\u80fd\u591f\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u5355\u5149\u5b50\u6e90\u3001\u91cf\u5b50\u5b58\u50a8\u5668\u548c\u7ea0\u7f20\u751f\u6210\u3002"}}
{"id": "2510.24185", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.24185", "abs": "https://arxiv.org/abs/2510.24185", "authors": ["Kwadwo Mensah Obeng Afrane", "Yang Miao", "Andr\u00e9 B. J. Kokkeler"], "title": "Performance Analysis of Sub-band Full-duplex Cell-free Massive MIMO JCAS Systems", "comment": null, "summary": "In-band Full-duplex joint communication and sensing systems require self\ninterference cancellation as well as decoupling of the mutual interference\nbetween UL communication signals and radar echoes. We present sub-band\nfull-duplex as an alternative duplexing scheme to achieve simultaneous uplink\ncommunication and target parameter estimation in a cell-free massive MIMO\nsystem. Sub-band full-duplex allows uplink and downlink transmissions\nsimultaneously on non-overlapping frequency resources via explicitly defined\nuplink and downlink sub-bands in each timeslot. Thus, we propose a sub-band\nfull-duplex cell-free massive MIMO system with active downlink sensing on\ndownlink sub-bands and uplink communication on uplink sub-band. In the proposed\nsystem, the target illumination signal is transmitted on the downlink (radar)\nsub-band whereas uplink users transmit on the uplink (communication) sub-band.\nBy assuming efficient suppression of inter-sub-band interference between radar\nand communication sub-bands, uplink communication and radar signals can be\nefficiently processed without mutual interference. We show that each AP can\nestimate sensing parameters with high accuracy in SBFD cell-free massive MIMO\nJCAS systems.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5b50\u5e26\u5168\u53cc\u5de5\uff08SBFD\uff09\u6280\u672f\uff0c\u7528\u4e8e\u5728\u65e0\u57fa\u7ad9\u6d77\u91cfMIMO\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u901a\u4fe1\u548c\u611f\u77e5\u529f\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u5168\u53cc\u5de5\u901a\u4fe1\u548c\u611f\u77e5\u7cfb\u7edf\u9700\u8981\u590d\u6742\u7684\u81ea\u5e72\u6270\u6d88\u9664\u548c\u76f8\u4e92\u5e72\u6270\u89e3\u8026\uff0c\u800cSBFD\u63d0\u4f9b\u4e86\u4e00\u79cd\u66ff\u4ee3\u65b9\u6848\u3002", "method": "SBFD\u6280\u672f\u5141\u8bb8\u5728\u540c\u4e00\u65f6\u9699\u5185\uff0c\u5728\u4e0d\u540c\u7684\u3001\u4e0d\u91cd\u53e0\u7684\u4e0a\u4e0b\u884c\u5b50\u5e26\u4e2d\u540c\u65f6\u8fdb\u884c\u4e0a\u884c\u901a\u4fe1\u548c\u4e0b\u884c\u611f\u77e5\uff08\u96f7\u8fbe\uff09\u4f20\u8f93\u3002\u5728\u6240\u63d0\u51fa\u7684\u7cfb\u7edf\u4e2d\uff0c\u4e0b\u884c\uff08\u96f7\u8fbe\uff09\u5b50\u5e26\u7528\u4e8e\u4f20\u8f93\u76ee\u6807\u7167\u5c04\u4fe1\u53f7\uff0c\u800c\u4e0a\u884c\uff08\u901a\u4fe1\uff09\u5b50\u5e26\u7528\u4e8e\u7528\u6237\u4e0a\u884c\u901a\u4fe1\u3002\u5047\u8bbe\u5b50\u5e26\u95f4\u7684\u5e72\u6270\u5f97\u5230\u6709\u6548\u6291\u5236\u3002", "result": "\u8bc1\u660e\u4e86\u5728SBFD\u65e0\u57fa\u7ad9\u6d77\u91cfMIMO\u8054\u5408\u901a\u4fe1\u611f\u77e5\u7cfb\u7edf\u4e2d\uff0c\u6bcf\u4e2a\u63a5\u5165\u70b9\uff08AP\uff09\u90fd\u80fd\u9ad8\u7cbe\u5ea6\u5730\u4f30\u8ba1\u611f\u77e5\u53c2\u6570\u3002", "conclusion": "SBFD\u6280\u672f\u80fd\u591f\u6709\u6548\u5730\u89e3\u51b3\u5168\u53cc\u5de5\u901a\u4fe1\u4e0e\u611f\u77e5\u5171\u5b58\u7684\u5e72\u6270\u95ee\u9898\uff0c\u5e76\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u7684\u611f\u77e5\u53c2\u6570\u4f30\u8ba1\u3002"}}
{"id": "2510.23634", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23634", "abs": "https://arxiv.org/abs/2510.23634", "authors": ["Soutrik Sarangi", "Yonatan Sverdlov", "Nadav Dym", "Abir De"], "title": "Monotone and Separable Set Functions: Characterizations and Neural Models", "comment": null, "summary": "Motivated by applications for set containment problems, we consider the\nfollowing fundamental problem: can we design set-to-vector functions so that\nthe natural partial order on sets is preserved, namely $S\\subseteq T \\text{ if\nand only if } F(S)\\leq F(T) $. We call functions satisfying this property\nMonotone and Separating (MAS) set functions. % We establish lower and upper\nbounds for the vector dimension necessary to obtain MAS functions, as a\nfunction of the cardinality of the multisets and the underlying ground set. In\nthe important case of an infinite ground set, we show that MAS functions do not\nexist, but provide a model called our which provably enjoys a relaxed MAS\nproperty we name \"weakly MAS\" and is stable in the sense of Holder continuity.\nWe also show that MAS functions can be used to construct universal models that\nare monotone by construction and can approximate all monotone set functions.\nExperimentally, we consider a variety of set containment tasks. The experiments\nshow the benefit of using our our model, in comparison with standard set models\nwhich do not incorporate set containment as an inductive bias. Our code is\navailable in https://github.com/yonatansverdlov/Monotone-Embedding.", "AI": {"tldr": "MAS\u51fd\u6570\u5728\u96c6\u5408\u5305\u542b\u95ee\u9898\u4e2d\u5177\u6709\u5e94\u7528\uff0c\u4f46\u5bf9\u4e8e\u65e0\u9650\u96c6\u5408\uff0cMAS\u51fd\u6570\u4e0d\u5b58\u5728\u3002\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u4e2a\u201c\u5f31MAS\u201d\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5177\u6709Holder\u8fde\u7eed\u6027\uff0c\u5e76\u4e14\u53ef\u4ee5\u7528\u4e8e\u6784\u5efa\u901a\u7528\u6a21\u578b\u6765\u8fd1\u4f3c\u6240\u6709\u5355\u8c03\u96c6\u5408\u51fd\u6570\u3002", "motivation": "\u8bbe\u8ba1\u80fd\u591f\u4fdd\u6301\u96c6\u5408\u95f4\u81ea\u7136\u504f\u5e8f\u5173\u7cfb\u7684\u96c6\u5408\u5230\u5411\u91cf\u51fd\u6570\uff0c\u5373\u6ee1\u8db3 $S\filepath T \text{ \u5f53\u4e14\u4ec5\u5f53 } F(S)\filepath F(T)$\u3002", "method": "\u63a8\u5bfc\u4e86MAS\u51fd\u6570\u6240\u9700\u7684\u5411\u91cf\u7ef4\u5ea6\u4e0b\u754c\u548c\u4e0a\u754c\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a\u201c\u5f31MAS\u201d\u7684}(\filepath{inf})-MAS}(inf)\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5177\u6709Holder\u8fde\u7eed\u6027\u3002", "result": "MAS\u51fd\u6570\u53ef\u4ee5\u7528\u4e8e\u6784\u5efa\u5355\u8c03\u7684\u901a\u7528\u6a21\u578b\uff0c\u5e76\u80fd\u8fd1\u4f3c\u6240\u6709\u5355\u8c03\u96c6\u5408\u51fd\u6570\u3002\u5728\u5b9e\u9a8c\u4e2d\uff0c\u201c\u5f31MAS\u201d\u6a21\u578b\u5728\u96c6\u5408\u5305\u542b\u4efb\u52a1\u4e0a\u4f18\u4e8e\u4e0d\u8003\u8651\u96c6\u5408\u5305\u542b\u4f5c\u4e3a\u5f52\u7eb3\u504f\u7f6e\u7684\u6807\u51c6\u96c6\u5408\u6a21\u578b\u3002", "conclusion": "MAS\u51fd\u6570\u5728\u6709\u9650\u96c6\u5408\u4e0a\u662f\u5b58\u5728\u7684\uff0c\u4f46\u5bf9\u4e8e\u65e0\u9650\u96c6\u5408\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u4e2a\u201c\u5f31MAS\u201d\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5177\u6709\u826f\u597d\u7684\u6027\u8d28\u5e76\u4e14\u5728\u5b9e\u8df5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2510.23882", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23882", "abs": "https://arxiv.org/abs/2510.23882", "authors": ["Adil Rasheed", "Oscar Ravik", "Omer San"], "title": "Hybrid Modeling, Sim-to-Real Reinforcement Learning, and Large Language Model Driven Control for Digital Twins", "comment": null, "summary": "This work investigates the use of digital twins for dynamical system modeling\nand control, integrating physics-based, data-driven, and hybrid approaches with\nboth traditional and AI-driven controllers. Using a miniature greenhouse as a\ntest platform, four predictive models Linear, Physics-Based Modeling (PBM),\nLong Short Term Memory (LSTM), and Hybrid Analysis and Modeling (HAM) are\ndeveloped and compared under interpolation and extrapolation scenarios. Three\ncontrol strategies Model Predictive Control (MPC), Reinforcement Learning (RL),\nand Large Language Model (LLM) based control are also implemented to assess\ntrade-offs in precision, adaptability, and implementation effort. Results show\nthat in modeling HAM provides the most balanced performance across accuracy,\ngeneralization, and computational efficiency, while LSTM achieves high\nprecision at greater resource cost. Among controllers, MPC delivers robust and\npredictable performance, RL demonstrates strong adaptability, and LLM-based\ncontrollers offer flexible human-AI interaction when coupled with predictive\ntools.", "AI": {"tldr": "\u6570\u5b57\u5b6a\u751f\u53ef\u7528\u4e8e\u52a8\u6001\u7cfb\u7edf\u5efa\u6a21\u4e0e\u63a7\u5236\uff0c\u6df7\u5408\u65b9\u6cd5\uff08HAM\uff09\u5728\u5efa\u6a21\u65b9\u9762\u8868\u73b0\u5747\u8861\uff0cLSTM\u7cbe\u5ea6\u9ad8\u4f46\u8d44\u6e90\u6d88\u8017\u5927\uff1bMPC\u63a7\u5236\u7a33\u5b9a\uff0cRL\u9002\u5e94\u6027\u5f3a\uff0cLLM\u6613\u4e8e\u4eba\u673a\u4ea4\u4e92\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u5229\u7528\u6570\u5b57\u5b6a\u751f\u6280\u672f\u5bf9\u52a8\u6001\u7cfb\u7edf\u8fdb\u884c\u5efa\u6a21\u4e0e\u63a7\u5236\uff0c\u6574\u5408\u4e86\u591a\u79cd\u5efa\u6a21\u65b9\u6cd5\uff08\u57fa\u4e8e\u7269\u7406\u3001\u6570\u636e\u9a71\u52a8\u3001\u6df7\u5408\uff09\u548c\u63a7\u5236\u7b56\u7565\uff08\u4f20\u7edf\u53caAI\u9a71\u52a8\uff09\u3002", "method": "\u5728\u7f29\u5fae\u6e29\u5ba4\u5b9e\u9a8c\u5e73\u53f0\u4e0a\uff0c\u5f00\u53d1\u5e76\u6bd4\u8f83\u4e86\u56db\u79cd\u9884\u6d4b\u6a21\u578b\uff08\u7ebf\u6027\u3001\u57fa\u4e8e\u7269\u7406\u5efa\u6a21PBM\u3001\u957f\u77ed\u671f\u8bb0\u5fc6LSTM\u3001\u6df7\u5408\u5206\u6790\u5efa\u6a21HAM\uff09\u5728\u5185\u63d2\u548c\u5916\u63d2\u573a\u666f\u4e0b\u7684\u8868\u73b0\u3002\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u4e09\u79cd\u63a7\u5236\u7b56\u7565\uff08\u6a21\u578b\u9884\u6d4b\u63a7\u5236MPC\u3001\u5f3a\u5316\u5b66\u4e60RL\u3001\u5927\u8bed\u8a00\u6a21\u578bLLM\uff09\u7684\u8bc4\u4f30\uff0c\u4ee5\u8861\u91cf\u7cbe\u5ea6\u3001\u9002\u5e94\u6027\u548c\u5b9e\u73b0\u6210\u672c\u3002", "result": "\u5728\u5efa\u6a21\u65b9\u9762\uff0cHAM\u6a21\u578b\u5728\u7cbe\u5ea6\u3001\u6cdb\u5316\u6027\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u53d6\u5f97\u4e86\u6700\u5747\u8861\u7684\u8868\u73b0\uff0c\u800cLSTM\u6a21\u578b\u5219\u4ee5\u66f4\u9ad8\u7684\u8d44\u6e90\u6210\u672c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u3002\u5728\u63a7\u5236\u65b9\u9762\uff0cMPC\u8868\u73b0\u51fa\u7a33\u5065\u4e14\u53ef\u9884\u6d4b\u7684\u6027\u80fd\uff0cRL\u5c55\u73b0\u4e86\u5f3a\u5927\u7684\u9002\u5e94\u80fd\u529b\uff0c\u800c\u57fa\u4e8eLLM\u7684\u63a7\u5236\u5668\u5728\u4e0e\u9884\u6d4b\u5de5\u5177\u7ed3\u5408\u65f6\uff0c\u63d0\u4f9b\u4e86\u7075\u6d3b\u7684\u4eba\u673a\u4ea4\u4e92\u3002", "conclusion": "\u6df7\u5408\u5206\u6790\u5efa\u6a21\uff08HAM\uff09\u662f\u52a8\u6001\u7cfb\u7edf\u5efa\u6a21\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u800cMPC\u3001RL\u548cLLM\u7b49\u63a7\u5236\u7b56\u7565\u5404\u6709\u4f18\u52a3\uff0c\u53ef\u6839\u636e\u5177\u4f53\u9700\u6c42\u9009\u62e9\u3002\u6570\u5b57\u5b6a\u751f\u6280\u672f\u7ed3\u5408\u8fd9\u4e9b\u65b9\u6cd5\uff0c\u4e3a\u52a8\u6001\u7cfb\u7edf\u7684\u5efa\u6a21\u4e0e\u63a7\u5236\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.23924", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23924", "abs": "https://arxiv.org/abs/2510.23924", "authors": ["Dina Pisarevskaya", "Arkaitz Zubiaga"], "title": "Agent-based Automated Claim Matching with Instruction-following LLMs", "comment": "Accepted for the International Joint Conference on Natural Language\n  Processing & Asia-Pacific Chapter of the Association for Computational\n  Linguistics (2025) Findings", "summary": "We present a novel agent-based approach for the automated claim matching task\nwith instruction-following LLMs. We propose a two-step pipeline that first\ngenerates prompts with LLMs, to then perform claim matching as a binary\nclassification task with LLMs. We demonstrate that LLM-generated prompts can\noutperform SOTA with human-generated prompts, and that smaller LLMs can do as\nwell as larger ones in the generation process, allowing to save computational\nresources. We also demonstrate the effectiveness of using different LLMs for\neach step of the pipeline, i.e. using an LLM for prompt generation, and another\nfor claim matching. Our investigation into the prompt generation process in\nturn reveals insights into the LLMs' understanding of claim matching.", "AI": {"tldr": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u4ee3\u7406\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u6307\u4ee4\u9075\u5faa\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8fdb\u884c\u81ea\u52a8\u7d22\u8d54\u5339\u914d\u3002\u8be5\u65b9\u6cd5\u5305\u62ec\u4e00\u4e2a\u4e24\u6b65\u6d41\u7a0b\uff1a\u9996\u5148\u4f7f\u7528LLM\u751f\u6210\u63d0\u793a\uff0c\u7136\u540e\u5c06\u7d22\u8d54\u5339\u914d\u89c6\u4e3a\u4e00\u4e2a\u4e8c\u5143\u5206\u7c7b\u4efb\u52a1\uff0c\u5e76\u4f7f\u7528LLM\u8fdb\u884c\u6267\u884c\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0cLLM\u751f\u6210\u7684\u63d0\u793a\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u4eba\u5de5\u751f\u6210\u63d0\u793a\uff0c\u5e76\u4e14\u8f83\u5c0f\u7684LLM\u5728\u63d0\u793a\u751f\u6210\u8fc7\u7a0b\u4e2d\u4e5f\u80fd\u53d6\u5f97\u4e0e\u5927\u578bLLM\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u4ece\u800c\u8282\u7701\u4e86\u8ba1\u7b97\u8d44\u6e90\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u8bc1\u660e\u4e86\u5728\u7ba1\u9053\u7684\u6bcf\u4e2a\u6b65\u9aa4\u4e2d\u4f7f\u7528\u4e0d\u540c\u7684LLM\uff08\u5373\u4e00\u4e2a\u7528\u4e8e\u63d0\u793a\u751f\u6210\uff0c\u53e6\u4e00\u4e2a\u7528\u4e8e\u7d22\u8d54\u5339\u914d\uff09\u7684\u6709\u6548\u6027\u3002\u901a\u8fc7\u5bf9\u63d0\u793a\u751f\u6210\u8fc7\u7a0b\u7684\u6df1\u5165\u7814\u7a76\uff0c\u6211\u4eec\u63ed\u793a\u4e86LLM\u5bf9\u7d22\u8d54\u5339\u914d\u4efb\u52a1\u7684\u7406\u89e3\u3002", "motivation": "\u672c\u7814\u7a76\u7684\u52a8\u673a\u662f\u6539\u8fdb\u81ea\u52a8\u5316\u7d22\u8d54\u5339\u914d\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u5e76\u63a2\u7d22\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u8fd9\u4e00\u8fc7\u7a0b\u4e2d\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u901a\u8fc7LLM\u751f\u6210\u63d0\u793a\u6765\u6307\u5bfc\u7d22\u8d54\u5339\u914d\u4efb\u52a1\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u6b65\u7684\u57fa\u4e8e\u4ee3\u7406\u7684\u65b9\u6cd5\uff0c\u9996\u5148\u4f7f\u7528LLM\u751f\u6210\u7528\u4e8e\u7d22\u8d54\u5339\u914d\u7684\u63d0\u793a\uff0c\u7136\u540e\u4f7f\u7528LLM\u5c06\u7d22\u8d54\u5339\u914d\u4f5c\u4e3a\u4e8c\u5143\u5206\u7c7b\u4efb\u52a1\u8fdb\u884c\u6267\u884c\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0cLLM\u751f\u6210\u7684\u63d0\u793a\u5728\u7d22\u8d54\u5339\u914d\u4efb\u52a1\u4e0a\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u4eba\u5de5\u751f\u6210\u63d0\u793a\u3002\u6b64\u5916\uff0c\u8f83\u5c0f\u7684LLM\u5728\u63d0\u793a\u751f\u6210\u65b9\u9762\u4e0e\u5927\u578bLLM\u76f8\u5f53\uff0c\u5e76\u4e14\u5728\u7ba1\u9053\u4e2d\u4f7f\u7528\u4e0d\u540c\u7684LLM\uff08\u4e00\u4e2a\u7528\u4e8e\u751f\u6210\uff0c\u4e00\u4e2a\u7528\u4e8e\u5339\u914d\uff09\u662f\u6709\u6548\u7684\u3002", "conclusion": "\u672c\u7814\u7a76\u5f97\u51fa\u7ed3\u8bba\uff0cLLM\u80fd\u591f\u6709\u6548\u5730\u751f\u6210\u7528\u4e8e\u7d22\u8d54\u5339\u914d\u4efb\u52a1\u7684\u63d0\u793a\uff0c\u5e76\u4e14\u53ef\u4ee5\u5229\u7528LLM\u672c\u8eab\u6765\u6267\u884c\u7d22\u8d54\u5339\u914d\u4efb\u52a1\uff0c\u4ece\u800c\u5728\u6027\u80fd\u548c\u8ba1\u7b97\u8d44\u6e90\u5229\u7528\u65b9\u9762\u90fd\u53d6\u5f97\u4e86\u6539\u8fdb\u3002\u7814\u7a76\u8fd8\u4e3a\u7406\u89e3LLM\u5728\u7d22\u8d54\u5339\u914d\u65b9\u9762\u7684\u80fd\u529b\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002"}}
{"id": "2510.24067", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24067", "abs": "https://arxiv.org/abs/2510.24067", "authors": ["Tianyi Ding", "Ronghao Zheng", "Senlin Zhang", "Meiqin Liu"], "title": "Balanced Collaborative Exploration via Distributed Topological Graph Voronoi Partition", "comment": null, "summary": "This work addresses the collaborative multi-robot autonomous online\nexploration problem, particularly focusing on distributed exploration planning\nfor dynamically balanced exploration area partition and task allocation among a\nteam of mobile robots operating in obstacle-dense non-convex environments.\n  We present a novel topological map structure that simultaneously\ncharacterizes both spatial connectivity and global exploration completeness of\nthe environment. The topological map is updated incrementally to utilize known\nspatial information for updating reachable spaces, while exploration targets\nare planned in a receding horizon fashion under global coverage guidance.\n  A distributed weighted topological graph Voronoi algorithm is introduced\nimplementing balanced graph space partitions of the fused topological maps.\nTheoretical guarantees are provided for distributed consensus convergence and\nequitable graph space partitions with constant bounds.\n  A local planner optimizes the visitation sequence of exploration targets\nwithin the balanced partitioned graph space to minimize travel distance, while\ngenerating safe, smooth, and dynamically feasible motion trajectories.\n  Comprehensive benchmarking against state-of-the-art methods demonstrates\nsignificant improvements in exploration efficiency, completeness, and workload\nbalance across the robot team.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u62d3\u6251\u5730\u56fe\u7ed3\u6784\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u673a\u5668\u4eba\u534f\u540c\u81ea\u4e3b\u5728\u7ebf\u63a2\u7d22\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u5b58\u5728\u969c\u788d\u7269\u7684\u975e\u51f8\u73af\u5883\u4e2d\u8fdb\u884c\u5206\u5e03\u5f0f\u63a2\u7d22\u89c4\u5212\u3002", "motivation": "\u89e3\u51b3\u591a\u673a\u5668\u4eba\u534f\u540c\u81ea\u4e3b\u5728\u7ebf\u63a2\u7d22\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u52a8\u6001\u5e73\u8861\u7684\u533a\u57df\u5212\u5206\u548c\u4efb\u52a1\u5206\u914d\u65b9\u9762\uff0c\u4ee5\u5e94\u5bf9\u5b58\u5728\u5927\u91cf\u969c\u788d\u7269\u7684\u975e\u51f8\u73af\u5883\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u62d3\u6251\u5730\u56fe\u7ed3\u6784\uff0c\u8be5\u7ed3\u6784\u80fd\u591f\u540c\u65f6\u8868\u5f81\u73af\u5883\u7684\u7a7a\u95f4\u8fde\u901a\u6027\u548c\u5168\u5c40\u63a2\u7d22\u5b8c\u6574\u6027\u3002\u901a\u8fc7\u589e\u91cf\u66f4\u65b0\u62d3\u6251\u5730\u56fe\u6765\u5229\u7528\u5df2\u77e5\u7a7a\u95f4\u4fe1\u606f\u66f4\u65b0\u53ef\u8fbe\u7a7a\u95f4\uff0c\u5e76\u5728\u5168\u5c40\u8986\u76d6\u7684\u6307\u5bfc\u4e0b\uff0c\u4ee5\u9012\u63a8\u89c6\u754c\u7684\u65b9\u5f0f\u89c4\u5212\u63a2\u7d22\u76ee\u6807\u3002\u5f15\u5165\u4e86\u5206\u5e03\u5f0f\u52a0\u6743\u62d3\u6251\u56fe Voronoi \u7b97\u6cd5\uff0c\u5b9e\u73b0\u4e86\u5bf9\u878d\u5408\u62d3\u6251\u5730\u56fe\u7684\u5747\u8861\u56fe\u7a7a\u95f4\u5212\u5206\u3002\u4e3a\u5206\u5e03\u5f0f\u5171\u8bc6\u6536\u655b\u548c\u5177\u6709\u6052\u5b9a\u754c\u9650\u7684\u516c\u5e73\u56fe\u7a7a\u95f4\u5212\u5206\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u3002\u5c40\u90e8\u89c4\u5212\u5668\u5219\u5728\u5747\u8861\u5212\u5206\u7684\u56fe\u7a7a\u95f4\u5185\u4f18\u5316\u63a2\u7d22\u76ee\u6807\u7684\u8bbf\u95ee\u987a\u5e8f\uff0c\u4ee5\u6700\u5c0f\u5316\u884c\u9a76\u8ddd\u79bb\uff0c\u5e76\u751f\u6210\u5b89\u5168\u3001\u5e73\u6ed1\u3001\u52a8\u6001\u53ef\u884c\u7684\u8fd0\u52a8\u8f68\u8ff9\u3002", "result": "\u4e0e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u8fdb\u884c\u4e86\u5168\u9762\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7ed3\u679c\u8868\u660e\u5728\u63a2\u7d22\u6548\u7387\u3001\u5b8c\u6574\u6027\u548c\u673a\u5668\u4eba\u56e2\u961f\u7684\u5de5\u4f5c\u91cf\u5e73\u8861\u65b9\u9762\u90fd\u6709\u663e\u8457\u63d0\u9ad8\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u5206\u5e03\u5f0f\u63a2\u7d22\u89c4\u5212\u65b9\u6cd5\u5728\u590d\u6742\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u5b8c\u6574\u4e14\u5747\u8861\u7684\u591a\u673a\u5668\u4eba\u534f\u540c\u63a2\u7d22\u3002"}}
{"id": "2510.24324", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2510.24324", "abs": "https://arxiv.org/abs/2510.24324", "authors": ["Ivan Tolkachev", "Daniel R. Mason", "Max Boleininger", "Pui-Wai Ma", "Felix Hofmann"], "title": "Energy evolution in nanocrystalline iron driven by collision cascades", "comment": null, "summary": "Nanocrystalline materials are promising candidates for future fusion reactor\napplications, due to their high density of grain boundaries which may serve as\nsinks for irradiation induced defects. We use molecular dynamics to simulate\ncollision cascades in nanocrystalline iron and compare these to collision\ncascades in initially defect free single crystals. We create nanocrystalline\nsamples via Voronoi tessellation of initially randomly placed grain seeds and\nvia severe plastic shearing. An irradiation induced annealing is observed\nwhereby after ~ 2 displacements per atom (dpa), irradiation drives all\nsimulation cells to a single crystalline state. Irradiation-induced defects\nthat distort the lattice generate elastic strain, so we use excess potential\nenergy as a measure of defect content. At low doses, the Voronoi samples\nfeature a few large, low energy grains, whereas the sheared samples show many\nsmall, high energy grains due to the high defect and grain boundary content\ncaused by severe deformation. As dose increases beyond 1 dpa however, all\nnanocrystalline samples converge to a similar behaviour. Excess potential\nenergy mirrors this trend, plateauing above ~ 4 dpa. We hypothesise that the\ninitially pristine cells will also reach a similar plateau after 5 dpa, which\nis seemingly confirmed by running a single instance of each cell type to 10\ndpa. A model is developed to explain the energy evolution.", "AI": {"tldr": "\u7eb3\u7c73\u6676\u6750\u6599\u56e0\u5176\u9ad8\u5bc6\u5ea6\u7684\u6676\u754c\u53ef\u4f5c\u4e3a\u8f90\u7167\u7f3a\u9677\u7684\u63a5\u6536\u4f53\uff0c\u5728\u672a\u6765\u805a\u53d8\u53cd\u5e94\u5806\u5e94\u7528\u4e2d\u663e\u793a\u51fa\u6f5c\u529b\u3002\u672c\u7814\u7a76\u4f7f\u7528\u5206\u5b50\u52a8\u529b\u5b66\u6a21\u62df\u4e86\u7eb3\u7c73\u6676\u94c1\u4e2d\u7684\u78b0\u649e\u7ea7\u8054\uff0c\u5e76\u4e0e\u5355\u6676\u94c1\u8fdb\u884c\u6bd4\u8f83\u3002\u6a21\u62df\u53d1\u73b0\uff0c\u5728\u7ea62 dpa\u7684\u5242\u91cf\u4e0b\uff0c\u8f90\u7167\u4f1a\u5bfc\u81f4\u6240\u6709\u6a21\u62df\u5355\u5143\u6f14\u53d8\u4e3a\u5355\u4e00\u6676\u4f53\u72b6\u6001\u3002\u7814\u7a76\u8fd8\u53d1\u73b0\uff0c\u4f4e\u5242\u91cf\u4e0b\uff0cVoronoi\u751f\u6210\u7684\u7eb3\u7c73\u6676\u6837\u54c1\u5177\u6709\u5c11\u91cf\u5927\u6676\u7c92\uff0c\u800c\u4e25\u91cd\u5851\u6027\u526a\u5207\u751f\u6210\u7684\u6837\u54c1\u5219\u5177\u6709\u5927\u91cf\u5c0f\u6676\u7c92\u3002\u968f\u7740\u5242\u91cf\u7684\u589e\u52a0\uff0c\u6240\u6709\u7eb3\u7c73\u6676\u6837\u54c1\u7684\u884c\u4e3a\u8d8b\u4e8e\u4e00\u81f4\uff0c\u80fd\u91cf\u6f14\u5316\u4e5f\u8d8b\u4e8e\u5e73\u7f13\u3002\u6700\u7ec8\uff0c\u4e00\u4e2a\u89e3\u91ca\u80fd\u91cf\u6f14\u5316\u7684\u6a21\u578b\u88ab\u63d0\u51fa\u3002", "motivation": "\u7eb3\u7c73\u6676\u6750\u6599\u56e0\u5176\u9ad8\u5bc6\u5ea6\u7684\u6676\u754c\u53ef\u4f5c\u4e3a\u8f90\u7167\u7f3a\u9677\u7684\u63a5\u6536\u4f53\uff0c\u5728\u672a\u6765\u805a\u53d8\u53cd\u5e94\u5806\u5e94\u7528\u4e2d\u5177\u6709\u6f5c\u529b\u3002", "method": "\u4f7f\u7528\u5206\u5b50\u52a8\u529b\u5b66\u6a21\u62df\u7eb3\u7c73\u6676\u94c1\u4e2d\u7684\u78b0\u649e\u7ea7\u8054\uff0c\u5e76\u4e0e\u5355\u6676\u94c1\u8fdb\u884c\u6bd4\u8f83\u3002\u901a\u8fc7Voronoi\u9576\u5d4c\u548c\u4e25\u91cd\u5851\u6027\u526a\u5207\u751f\u6210\u7eb3\u7c73\u6676\u6837\u54c1\u3002", "result": "\u5728\u7ea62 dpa\u7684\u5242\u91cf\u4e0b\uff0c\u8f90\u7167\u5bfc\u81f4\u6240\u6709\u6a21\u62df\u5355\u5143\u6f14\u53d8\u4e3a\u5355\u4e00\u6676\u4f53\u72b6\u6001\u3002\u4f4e\u5242\u91cf\u4e0b\uff0cVoronoi\u6837\u54c1\u5177\u6709\u5c11\u91cf\u5927\u6676\u7c92\uff0c\u800c\u526a\u5207\u6837\u54c1\u5177\u6709\u5927\u91cf\u5c0f\u6676\u7c92\u3002\u968f\u7740\u5242\u91cf\u7684\u589e\u52a0\uff0c\u6240\u6709\u6837\u54c1\u884c\u4e3a\u8d8b\u4e8e\u4e00\u81f4\uff0c\u80fd\u91cf\u6f14\u5316\u8d8b\u4e8e\u5e73\u7f13\uff08\u7ea64 dpa\uff09\u3002", "conclusion": "\u8f90\u7167\u5bf9\u7eb3\u7c73\u6676\u6750\u6599\u5177\u6709\u663e\u8457\u5f71\u54cd\uff0c\u53ef\u80fd\u5bfc\u81f4\u5176\u7ed3\u6784\u6f14\u5316\u548c\u6700\u7ec8\u7684\u5355\u6676\u5316\u3002\u80fd\u91cf\u6f14\u5316\u6a21\u578b\u53ef\u4ee5\u89e3\u91ca\u8fd9\u79cd\u73b0\u8c61\u3002"}}
{"id": "2510.23960", "categories": ["cs.CV", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.23960", "abs": "https://arxiv.org/abs/2510.23960", "authors": ["Peiyang Xu", "Minzhou Pan", "Zhaorun Chen", "Shuang Yang", "Chaowei Xiao", "Bo Li"], "title": "SafeVision: Efficient Image Guardrail with Robust Policy Adherence and Explainability", "comment": "42 pages, 9 figures", "summary": "With the rapid proliferation of digital media, the need for efficient and\ntransparent safeguards against unsafe content is more critical than ever.\nTraditional image guardrail models, constrained by predefined categories, often\nmisclassify content due to their pure feature-based learning without semantic\nreasoning. Moreover, these models struggle to adapt to emerging threats,\nrequiring costly retraining for new threats. To address these limitations, we\nintroduce SafeVision, a novel image guardrail that integrates human-like\nreasoning to enhance adaptability and transparency. Our approach incorporates\nan effective data collection and generation framework, a policy-following\ntraining pipeline, and a customized loss function. We also propose a diverse QA\ngeneration and training strategy to enhance learning effectiveness. SafeVision\ndynamically aligns with evolving safety policies at inference time, eliminating\nthe need for retraining while ensuring precise risk assessments and\nexplanations. Recognizing the limitations of existing unsafe image benchmarks,\nwhich either lack granularity or cover limited risks, we introduce VisionHarm,\na high-quality dataset comprising two subsets: VisionHarm Third-party\n(VisionHarm-T) and VisionHarm Comprehensive(VisionHarm-C), spanning diverse\nharmful categories. Through extensive experiments, we show that SafeVision\nachieves state-of-the-art performance on different benchmarks. SafeVision\noutperforms GPT-4o by 8.6% on VisionHarm-T and by 15.5% on VisionHarm-C, while\nbeing over 16x faster. SafeVision sets a comprehensive, policy-following, and\nexplainable image guardrail with dynamic adaptation to emerging threats.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2510.23923", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.23923", "abs": "https://arxiv.org/abs/2510.23923", "authors": ["Ilias Magoulas", "Francesco A. Evangelista"], "title": "Clifford Transformations for Fermionic Quantum Systems: From Paulis to Majoranas to Fermions", "comment": null, "summary": "Clifford gates and transformations, which map products of elementary Pauli or\nMajorana operators to other such products, are foundational in quantum\ncomputing, underpinning the stabilizer formalism, error-correcting codes, magic\nstate distillation, quantum communication and cryptography, and qubit tapering.\nMoreover, circuits composed entirely of Clifford gates are classically\nsimulatable, highlighting their computational significance. In this work, we\nextend the concept of Clifford transformations to fermionic systems. We\ndemonstrate that fermionic Clifford transformations are generated by half-body\nand pair operators, providing a systematic framework for their\ncharacterization. Additionally, we establish connections with fermionic\nmean-field theories and applications in qubit tapering, offering insights into\ntheir broader implications in quantum computing.", "AI": {"tldr": "Clifford transformations have been extended to fermionic systems, generated by half-body and pair operators, with connections to fermionic mean-field theories and applications in qubit tapering.", "motivation": "The paper aims to extend the concept of Clifford transformations, foundational in quantum computing, to fermionic systems.", "method": "The study demonstrates that fermionic Clifford transformations are generated by half-body and pair operators, establishing connections with fermionic mean-field theories.", "result": "The research provides a systematic framework for characterizing fermionic Clifford transformations and reveals their implications in quantum computing, particularly in qubit tapering.", "conclusion": "The extension of Clifford transformations to fermionic systems offers a valuable tool for quantum computing research, bridging concepts in fermionic systems with existing quantum computing frameworks."}}
{"id": "2510.24272", "categories": ["eess.SY", "cs.AI", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.24272", "abs": "https://arxiv.org/abs/2510.24272", "authors": ["Maximilian Bloor", "Max Mowbray", "Ehecatl Antonio Del Rio Chanona", "Calvin Tsay"], "title": "Survey and Tutorial of Reinforcement Learning Methods in Process Systems Engineering", "comment": null, "summary": "Sequential decision making under uncertainty is central to many Process\nSystems Engineering (PSE) challenges, where traditional methods often face\nlimitations related to controlling and optimizing complex and stochastic\nsystems. Reinforcement Learning (RL) offers a data-driven approach to derive\ncontrol policies for such challenges. This paper presents a survey and tutorial\non RL methods, tailored for the PSE community. We deliver a tutorial on RL,\ncovering fundamental concepts and key algorithmic families including\nvalue-based, policy-based and actor-critic methods. Subsequently, we survey\nexisting applications of these RL techniques across various PSE domains, such\nas in fed-batch and continuous process control, process optimization, and\nsupply chains. We conclude with PSE focused discussion of specialized\ntechniques and emerging directions. By synthesizing the current state of RL\nalgorithm development and implications for PSE this work identifies successes,\nchallenges, trends, and outlines avenues for future research at the interface\nof these fields.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4e3a\u8fc7\u7a0b\u7cfb\u7edf\u5de5\u7a0b\uff08PSE\uff09\u9886\u57df\u7684RL\u65b9\u6cd5\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u8c03\u67e5\u548c\u6559\u7a0b\uff0c\u91cd\u70b9\u4ecb\u7ecd\u4e86RL\u6982\u5ff5\u3001\u7b97\u6cd5\u548cPSE\u5e94\u7528\uff0c\u5e76\u8ba8\u8bba\u4e86\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u987a\u5e8f\u51b3\u7b56\u5236\u5b9a\u5728\u8bb8\u591a\u8fc7\u7a0b\u7cfb\u7edf\u5de5\u7a0b\uff08PSE\uff09\u6311\u6218\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u5728\u63a7\u5236\u548c\u4f18\u5316\u590d\u6742\u548c\u968f\u673a\u7cfb\u7edf\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u63d0\u4f9b\u4e86\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\u3002", "method": "\u8be5\u8bba\u6587\u9996\u5148\u5bf9RL\u8fdb\u884c\u4e86\u6559\u7a0b\uff0c\u6db5\u76d6\u4e86\u57fa\u672c\u6982\u5ff5\u548c\u5173\u952e\u7b97\u6cd5\u5bb6\u65cf\uff08\u5305\u62ec\u57fa\u4e8e\u503c\u3001\u57fa\u4e8e\u7b56\u7565\u548cActor-Critic\u65b9\u6cd5\uff09\u3002\u7136\u540e\uff0c\u8c03\u67e5\u4e86RL\u6280\u672f\u5728PSE\u9886\u57df\u7684\u5404\u79cd\u5e94\u7528\uff0c\u5982\u6279\u6b21\u548c\u8fde\u7eed\u8fc7\u7a0b\u63a7\u5236\u3001\u8fc7\u7a0b\u4f18\u5316\u548c\u4f9b\u5e94\u94fe\u3002", "result": "\u8be5\u8c03\u67e5\u548c\u6559\u7a0b\u65e8\u5728\u4e3aPSE\u793e\u533a\u63d0\u4f9bRL\u65b9\u9762\u7684\u77e5\u8bc6\uff0c\u6db5\u76d6\u57fa\u672c\u6982\u5ff5\u3001\u7b97\u6cd5\u548c\u5b9e\u9645\u5e94\u7528\u3002", "conclusion": "\u901a\u8fc7\u7efc\u5408RL\u7b97\u6cd5\u5f00\u53d1\u548c\u5bf9PSE\u7684\u5f71\u54cd\u7684\u73b0\u72b6\uff0c\u8be5\u5de5\u4f5c\u786e\u5b9a\u4e86\u6210\u529f\u3001\u6311\u6218\u3001\u8d8b\u52bf\uff0c\u5e76\u4e3a\u8fd9\u4e9b\u9886\u57df\u7684\u4ea4\u53c9\u9886\u57df\u672a\u6765\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2510.24193", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.24193", "abs": "https://arxiv.org/abs/2510.24193", "authors": ["Tailai Wen", "Da Ke", "Xiang Wang", "Zhitao Huang"], "title": "Dual-Domain Constraints: Designing Covert and Efficient Adversarial Examples for Secure Communication", "comment": null, "summary": "The advancements in Automatic Modulation Classification (AMC) have propelled\nthe development of signal sensing and identification technologies in\nnon-cooperative communication scenarios but also enable eavesdroppers to\neffectively intercept user signals in wireless communication environments. To\nprotect user privacy in communication links, we have optimized the adversarial\nexample generation model and introduced a novel framework for generating\nadversarial perturbations for transmitted signals. This framework implements\ndual-domain constraints in both the time and frequency domains, ensuring that\nthe adversarial perturbation cannot be filtered out. Comparative experiments\nconfirm the superiority of the proposed method and the concealment of the\nadversarial examples it generates.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u5bf9\u6297\u6027\u6270\u52a8\u4ee5\u4fdd\u62a4\u65e0\u7ebf\u901a\u4fe1\u7528\u6237\u9690\u79c1\uff0c\u8be5\u6846\u67b6\u5728\u65f6\u57df\u548c\u9891\u57df\u4e2d\u5b9e\u65bd\u4e86\u53cc\u57df\u7ea6\u675f\uff0c\u4ee5\u9632\u6b62\u5bf9\u6297\u6027\u6270\u52a8\u88ab\u8fc7\u6ee4\u6389\u3002", "motivation": "\u4e3a\u4e86\u4fdd\u62a4\u901a\u4fe1\u94fe\u8def\u4e2d\u7684\u7528\u6237\u9690\u79c1\uff0c\u5e76\u5e94\u5bf9\u81ea\u52a8\u8c03\u5236\u5206\u7c7b\uff08AMC\uff09\u6280\u672f\u5728\u4fe1\u53f7\u4f20\u611f\u548c\u8bc6\u522b\u65b9\u9762\u7684\u8fdb\u5c55\u53ef\u80fd\u88ab\u7a83\u542c\u8005\u5229\u7528\u7684\u98ce\u9669\u3002", "method": "\u901a\u8fc7\u4f18\u5316\u5bf9\u6297\u6027\u793a\u4f8b\u751f\u6210\u6a21\u578b\uff0c\u5e76\u5f15\u5165\u4e00\u79cd\u5728\u65f6\u57df\u548c\u9891\u57df\u4e2d\u5b9e\u65bd\u53cc\u57df\u7ea6\u675f\u7684\u65b0\u9896\u6846\u67b6\uff0c\u4ee5\u751f\u6210\u65e0\u6cd5\u88ab\u8fc7\u6ee4\u6389\u7684\u5bf9\u6297\u6027\u6270\u52a8\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u5b9e\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u4e14\u751f\u6210\u7684\u5bf9\u6297\u6027\u793a\u4f8b\u5177\u6709\u826f\u597d\u7684\u9690\u853d\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u591f\u6709\u6548\u5730\u751f\u6210\u5bf9\u6297\u6027\u6270\u52a8\uff0c\u4ee5\u4fdd\u62a4\u65e0\u7ebf\u901a\u4fe1\u7528\u6237\u9690\u79c1\uff0c\u5e76\u4e14\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5177\u6709\u4f18\u8d8a\u6027\u548c\u9690\u853d\u6027\u3002"}}
{"id": "2510.23635", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23635", "abs": "https://arxiv.org/abs/2510.23635", "authors": ["Andrea Bontempelli", "Matteo Busso", "Leonardo Javier Malcotti", "Fausto Giunchiglia"], "title": "Help the machine to help you: an evaluation in the wild of egocentric data cleaning via skeptical learning", "comment": null, "summary": "Any digital personal assistant, whether used to support task performance,\nanswer questions, or manage work and daily life, including fitness schedules,\nrequires high-quality annotations to function properly. However, user\nannotations, whether actively produced or inferred from context (e.g., data\nfrom smartphone sensors), are often subject to errors and noise. Previous\nresearch on Skeptical Learning (SKEL) addressed the issue of noisy labels by\ncomparing offline active annotations with passive data, allowing for an\nevaluation of annotation accuracy. However, this evaluation did not include\nconfirmation from end-users, the best judges of their own context. In this\nstudy, we evaluate SKEL's performance in real-world conditions with actual\nusers who can refine the input labels based on their current perspectives and\nneeds. The study involves university students using the iLog mobile application\non their devices over a period of four weeks. The results highlight the\nchallenges of finding the right balance between user effort and data quality,\nas well as the potential benefits of using SKEL, which include reduced\nannotation effort and improved quality of collected data.", "AI": {"tldr": "\u7528\u6237\u9700\u8981\u9ad8\u8d28\u91cf\u7684\u6807\u6ce8\u6765\u652f\u6301\u6570\u5b57\u4e2a\u4eba\u52a9\u7406\uff0c\u4f46\u73b0\u6709\u6807\u6ce8\u5e38\u6709\u9519\u8bef\u3002\u672c\u7814\u7a76\u8bc4\u4f30\u4e86Skeptical Learning\uff08SKEL\uff09\u5728\u771f\u5b9e\u7528\u6237\u573a\u666f\u4e0b\u7684\u8868\u73b0\uff0c\u4ee5\u671f\u5728\u7528\u6237\u7cbe\u529b\u4e0e\u6570\u636e\u8d28\u91cf\u95f4\u627e\u5230\u5e73\u8861\u70b9\uff0c\u5e76\u964d\u4f4e\u6807\u6ce8\u7cbe\u529b\u3001\u63d0\u9ad8\u6570\u636e\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u7528\u6237\u6807\u6ce8\u5e38\u6709\u9519\u8bef\uff0c\u800cSkeptical Learning\uff08SKEL\uff09\u867d\u80fd\u5904\u7406\u566a\u58f0\u6807\u7b7e\uff0c\u4f46\u7f3a\u4e4f\u6700\u7ec8\u7528\u6237\u5bf9\u5176\u6807\u6ce8\u51c6\u786e\u6027\u7684\u786e\u8ba4\u3002\u672c\u7814\u7a76\u65e8\u5728\u771f\u5b9e\u7528\u6237\u573a\u666f\u4e0b\u8bc4\u4f30SKEL\u7684\u8868\u73b0\u3002", "method": "\u5728\u771f\u5b9e\u7528\u6237\u573a\u666f\u4e0b\uff0c\u8ba9\u5927\u5b66\u751f\u4f7f\u7528iLog\u79fb\u52a8\u5e94\u7528\u56db\u5468\uff0c\u8bc4\u4f30SKEL\u7684\u8868\u73b0\uff0c\u5e76\u5141\u8bb8\u7528\u6237\u6839\u636e\u81ea\u8eab\u9700\u6c42\u548c\u89c6\u89d2\u4fee\u6b63\u6807\u7b7e\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u663e\u793a\uff0c\u5728\u7528\u6237\u7cbe\u529b\u4e0e\u6570\u636e\u8d28\u91cf\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u5b58\u5728\u6311\u6218\uff0c\u4f46SKEL\u5728\u964d\u4f4e\u6807\u6ce8\u7cbe\u529b\u65b9\u9762\u6709\u6f5c\u529b\uff0c\u5e76\u80fd\u63d0\u9ad8\u6536\u96c6\u5230\u7684\u6570\u636e\u7684\u8d28\u91cf\u3002", "conclusion": "SKEL\u5728\u771f\u5b9e\u7528\u6237\u573a\u666f\u4e0b\u5177\u6709\u964d\u4f4e\u6807\u6ce8\u7cbe\u529b\u4e0e\u63d0\u9ad8\u6570\u636e\u8d28\u91cf\u7684\u6f5c\u529b\uff0c\u4f46\u9700\u8981\u5728\u7528\u6237\u7cbe\u529b\u4e0e\u6570\u636e\u8d28\u91cf\u4e4b\u95f4\u627e\u5230\u5408\u9002\u7684\u5e73\u8861\u70b9\u3002"}}
{"id": "2510.23883", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23883", "abs": "https://arxiv.org/abs/2510.23883", "authors": ["Shrestha Datta", "Shahriar Kabir Nahin", "Anshuman Chhabra", "Prasant Mohapatra"], "title": "Agentic AI Security: Threats, Defenses, Evaluation, and Open Challenges", "comment": null, "summary": "Agentic AI systems powered by large language models (LLMs) and endowed with\nplanning, tool use, memory, and autonomy, are emerging as powerful, flexible\nplatforms for automation. Their ability to autonomously execute tasks across\nweb, software, and physical environments creates new and amplified security\nrisks, distinct from both traditional AI safety and conventional software\nsecurity. This survey outlines a taxonomy of threats specific to agentic AI,\nreviews recent benchmarks and evaluation methodologies, and discusses defense\nstrategies from both technical and governance perspectives. We synthesize\ncurrent research and highlight open challenges, aiming to support the\ndevelopment of secure-by-design agent systems.", "AI": {"tldr": "Agentic AI systems pose new security risks due to their autonomy and ability to interact with various environments. This survey categorizes these threats, reviews evaluation methods, and discusses defenses to promote secure design.", "motivation": "Agentic AI systems, powered by LLMs and capable of planning, tool use, memory, and autonomy, present amplified security risks distinct from traditional AI safety and software security. There is a need to understand and address these specific risks.", "method": "This survey outlines a taxonomy of threats specific to agentic AI, reviews recent benchmarks and evaluation methodologies, and discusses defense strategies from both technical and governance perspectives.", "result": "The survey synthesizes current research on agentic AI security, identifies a taxonomy of threats, reviews evaluation benchmarks, and discusses defense strategies.", "conclusion": "The paper aims to support the development of secure-by-design agent systems by synthesizing research, highlighting challenges, and providing a framework for understanding and mitigating agentic AI security risks."}}
{"id": "2510.23941", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23941", "abs": "https://arxiv.org/abs/2510.23941", "authors": ["Soham Satyadharma", "Fatemeh Sheikholeslami", "Swati Kaul", "Aziz Umit Batur", "Suleiman A. Khan"], "title": "Auto prompting without training labels: An LLM cascade for product quality assessment in e-commerce catalogs", "comment": null, "summary": "We introduce a novel, training free cascade for auto-prompting Large Language\nModels (LLMs) to assess product quality in e-commerce. Our system requires no\ntraining labels or model fine-tuning, instead automatically generating and\nrefining prompts for evaluating attribute quality across tens of thousands of\nproduct category-attribute pairs. Starting from a seed of human-crafted\nprompts, the cascade progressively optimizes instructions to meet\ncatalog-specific requirements. This approach bridges the gap between general\nlanguage understanding and domain-specific knowledge at scale in complex\nindustrial catalogs. Our extensive empirical evaluations shows the auto-prompt\ncascade improves precision and recall by $8-10\\%$ over traditional\nchain-of-thought prompting. Notably, it achieves these gains while reducing\ndomain expert effort from 5.1 hours to 3 minutes per attribute - a $99\\%$\nreduction. Additionally, the cascade generalizes effectively across five\nlanguages and multiple quality assessment tasks, consistently maintaining\nperformance gains.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u81ea\u52a8\u63d0\u793a\u7ea7\u8054\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u7535\u5546\u4ea7\u54c1\u8d28\u91cf\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8bc4\u4f30\u7684\u7cbe\u5ea6\u548c\u53ec\u56de\u7387\uff0c\u5e76\u5927\u5e45\u51cf\u5c11\u4e86\u9886\u57df\u4e13\u5bb6\u7684\u5de5\u4f5c\u91cf\u3002", "motivation": "\u4e3a\u4e86\u5728\u7535\u5546\u9886\u57df\u5927\u89c4\u6a21\u3001\u81ea\u52a8\u5316\u5730\u8bc4\u4f30\u4ea7\u54c1\u8d28\u91cf\uff0c\u5c24\u5176\u662f\u5728\u7f3a\u4e4f\u8bad\u7ec3\u6807\u7b7e\u548c\u9700\u8981\u9886\u57df\u7279\u5b9a\u77e5\u8bc6\u7684\u573a\u666f\u4e0b\uff0c\u5f25\u5408\u901a\u7528\u8bed\u8a00\u7406\u89e3\u4e0e\u9886\u57df\u77e5\u8bc6\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "method": "\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u4e86\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u81ea\u52a8\u63d0\u793a\u7ea7\u8054\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u4ece\u5c11\u91cf\u4eba\u5de5\u8bbe\u8ba1\u7684\u63d0\u793a\u5f00\u59cb\uff0c\u901a\u8fc7\u9010\u6b65\u4f18\u5316\u63d0\u793a\u6765\u6ee1\u8db3\u7279\u5b9a\u4ea7\u54c1\u76ee\u5f55\u7684\u8981\u6c42\uff0c\u6700\u7ec8\u5b9e\u73b0\u5927\u89c4\u6a21\u7684\u4ea7\u54c1\u5c5e\u6027\u8d28\u91cf\u8bc4\u4f30\u3002", "result": "\u4e0e\u4f20\u7edf\u7684\u94fe\u5f0f\u601d\u8003\u63d0\u793a\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u81ea\u52a8\u63d0\u793a\u7ea7\u8054\u5728\u7cbe\u5ea6\u548c\u53ec\u56de\u7387\u65b9\u9762\u63d0\u9ad8\u4e86 8-10%\u3002\u5c06\u9886\u57df\u4e13\u5bb6\u5bf9\u6bcf\u4e2a\u5c5e\u6027\u7684\u8bc4\u4f30\u65f6\u95f4\u4ece 5.1 \u5c0f\u65f6\u51cf\u5c11\u5230 3 \u5206\u949f\uff08\u51cf\u5c11 99%\uff09\u3002\u8be5\u7cfb\u7edf\u5728\u4e94\u79cd\u8bed\u8a00\u548c\u591a\u4e2a\u8d28\u91cf\u8bc4\u4f30\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u81ea\u52a8\u63d0\u793a\u7ea7\u8054\u65b9\u6cd5\u662f\u4e00\u79cd\u6709\u6548\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5927\u89c4\u6a21\u5730\u81ea\u52a8\u5316\u8bc4\u4f30\u7535\u5546\u4ea7\u54c1\u8d28\u91cf\uff0c\u65e0\u9700\u8bad\u7ec3\u6570\u636e\uff0c\u5e76\u4e14\u663e\u8457\u51cf\u5c11\u4e86\u4eba\u529b\u6210\u672c\u3002"}}
{"id": "2510.24069", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24069", "abs": "https://arxiv.org/abs/2510.24069", "authors": ["Sangmin Kim", "Hajun Kim", "Gijeong Kim", "Min-Gyu Kim", "Hae-Won Park"], "title": "Dynamically-Consistent Trajectory Optimization for Legged Robots via Contact Point Decomposition", "comment": "8 pages, 4 figures, IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT\n  VERSION. ACCEPTED OCTOBER, 2025", "summary": "To generate reliable motion for legged robots through trajectory\noptimization, it is crucial to simultaneously compute the robot's path and\ncontact sequence, as well as accurately consider the dynamics in the problem\nformulation. In this paper, we present a phase-based trajectory optimization\nthat ensures the feasibility of translational dynamics and friction cone\nconstraints throughout the entire trajectory. Specifically, our approach\nleverages the superposition properties of linear differential equations to\ndecouple the translational dynamics for each contact point, which operates\nunder different phase sequences. Furthermore, we utilize the differentiation\nmatrix of B{\\'e}zier polynomials to derive an analytical relationship between\nthe robot's position and force, thereby ensuring the consistent satisfaction of\ntranslational dynamics. Additionally, by exploiting the convex closure property\nof B{\\'e}zier polynomials, our method ensures compliance with friction cone\nconstraints. Using the aforementioned approach, the proposed trajectory\noptimization framework can generate dynamically reliable motions with various\ngait sequences for legged robots. We validate our framework using a quadruped\nrobot model, focusing on the feasibility of dynamics and motion generation.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u76f8\u4f4d\u5206\u6bb5\u7684\u8f68\u8ff9\u4f18\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u751f\u6210\u5177\u6709\u53ef\u9760\u52a8\u529b\u5b66\u548c\u6469\u64e6\u9525\u7ea6\u675f\u7684\u53cc\u8db3\u673a\u5668\u4eba\u8fd0\u52a8\u3002", "motivation": "\u4e3a\u4e86\u5728\u8f68\u8ff9\u4f18\u5316\u4e2d\u4e3a\u673a\u5668\u4eba\u751f\u6210\u53ef\u9760\u7684\u8fd0\u52a8\uff0c\u9700\u8981\u540c\u65f6\u8ba1\u7b97\u673a\u5668\u4eba\u7684\u8def\u5f84\u3001\u63a5\u89e6\u987a\u5e8f\uff0c\u5e76\u51c6\u786e\u8003\u8651\u52a8\u529b\u5b66\u3002", "method": "\u8be5\u65b9\u6cd5\u5229\u7528\u7ebf\u6027\u5fae\u5206\u65b9\u7a0b\u7684\u53e0\u52a0\u6027\u8d28\u6765\u89e3\u8026\u6bcf\u4e2a\u63a5\u89e6\u70b9\u7684\u5e73\u79fb\u52a8\u529b\u5b66\uff0c\u5e76\u5229\u7528\u8d1d\u585e\u5c14\u66f2\u7ebf\u7684\u5fae\u5206\u77e9\u9635\u6765\u63a8\u5bfc\u4f4d\u7f6e\u548c\u529b\u4e4b\u95f4\u7684\u89e3\u6790\u5173\u7cfb\uff0c\u786e\u4fdd\u5e73\u79fb\u52a8\u529b\u5b66\u7684\u4e00\u81f4\u6027\u3002\u6b64\u5916\uff0c\u5229\u7528\u8d1d\u585e\u5c14\u66f2\u7ebf\u7684\u51f8\u5305\u6027\u8d28\u6765\u786e\u4fdd\u6469\u64e6\u9525\u7ea6\u675f\u7684\u6ee1\u8db3\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u6ee1\u8db3\u52a8\u529b\u5b66\u548c\u6469\u64e6\u9525\u7ea6\u675f\u7684\u3001\u5177\u6709\u4e0d\u540c\u6b65\u6001\u5e8f\u5217\u7684\u53cc\u8db3\u673a\u5668\u4eba\u8fd0\u52a8\u3002\u901a\u8fc7\u5728\u56db\u8db3\u673a\u5668\u4eba\u6a21\u578b\u4e0a\u8fdb\u884c\u9a8c\u8bc1\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u57fa\u4e8e\u76f8\u4f4d\u5206\u6bb5\u7684\u8f68\u8ff9\u4f18\u5316\u6846\u67b6\u80fd\u591f\u4e3a\u53cc\u8db3\u673a\u5668\u4eba\u751f\u6210\u52a8\u529b\u5b66\u53ef\u9760\u7684\u8fd0\u52a8\uff0c\u5e76\u6ee1\u8db3\u6469\u64e6\u9525\u7ea6\u675f\u3002"}}
{"id": "2510.24343", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2510.24343", "abs": "https://arxiv.org/abs/2510.24343", "authors": ["Maciej Wilczynski", "Mark Fedorov", "Tymofii Khvan", "F. Javier Dominguez-Gutierrez", "and Jacek Jagielski"], "title": "Molecular Dynamics Study of Irradiation-Induced Defect and Dislocation Evolution in Strained Nickel", "comment": null, "summary": "Molecular dynamics (MD) simulations were performed to investigate the\ninfluence of mechanical strain on irradiation-induced defect and dislocation\nevolution in nickel single crystals subjected to cumulative overlapping 5 keV\ncollision cascades at 300 K. The simulations reveal that tensile strain\nmodifies the dynamics of defect generation and recovery, promoting\nstress-assisted defect mobility and enhancing defect survival compared to the\nunstrained case. The heat spike duration and intensity decrease systematically\nwith increasing strain, indicating faster energy dissipation and altered defect\nrecombination behavior under applied stress. Analysis of the dislocation\nstructure shows that Shockley-type partial dislocations dominate the\nmicrostructural response, while Hirth and other dislocation types remain\ncomparatively minor. Both the total and Shockley dislocation densities reach a\nsaturation value of $~10^{16}m^{-2}$ , marking the establishment of a\nsteady-state microstructure governed by the balance between dislocation\naccumulation and recovery. The evolution of the total dislocation density with\nstrain is successfully described by the Kocks-Mecking model, demonstrating its\napplicability to strain-dependent irradiation effects in metallic systems", "AI": {"tldr": "\u62c9\u4f38\u5e94\u53d8\u4f1a\u5f71\u54cd\u8f90\u7167\u635f\u4f24\u548c\u4f4d\u9519\u6f14\u53d8\uff0c\u52a0\u901f\u80fd\u91cf\u8017\u6563\uff0c\u589e\u52a0\u7f3a\u9677\u5b58\u6d3b\u7387\uff0c\u5e76\u4f7f\u4f4d\u9519\u5bc6\u5ea6\u8fbe\u5230\u7a33\u6001\u3002", "motivation": "\u7814\u7a76\u673a\u68b0\u5e94\u53d8\u5bf9\u954d\u5355\u6676\u4e2d\u8f90\u7167\u5f15\u8d77\u7684\u7f3a\u9677\u548c\u4f4d\u9519\u6f14\u53d8\u7684\u5f71\u54cd\u3002", "method": "\u4f7f\u7528\u5206\u5b50\u52a8\u529b\u5b66\u6a21\u62df\u7814\u7a76\u4e86\u5728300K\u4e0b\uff0c5keV\u78b0\u649e\u7ea7\u8054\u8f90\u7167\u5bf9\u53d7\u62c9\u4f38\u5e94\u53d8\u7684\u954d\u5355\u6676\u7684\u5f71\u54cd\u3002", "result": "\u62c9\u4f38\u5e94\u53d8\u4f1a\u52a0\u901f\u80fd\u91cf\u8017\u6563\uff0c\u589e\u52a0\u7f3a\u9677\u5b58\u6d3b\u7387\uff0c\u5e76\u4e14\u4f4d\u9519\u5bc6\u5ea6\uff08\u5305\u62ecShockley\u4e0d\u5168\u4f4d\u9519\uff09\u4f1a\u8fbe\u5230\u4e00\u4e2a\u9971\u548c\u503c\u3002Kocks-Mecking\u6a21\u578b\u53ef\u4ee5\u63cf\u8ff0\u8fd9\u79cd\u5e94\u53d8\u4f9d\u8d56\u7684\u6548\u5e94\u3002", "conclusion": "\u673a\u68b0\u5e94\u53d8\u5728\u8f90\u7167\u635f\u4f24\u548c\u4f4d\u9519\u6f14\u53d8\u4e2d\u8d77\u7740\u91cd\u8981\u4f5c\u7528\uff0c\u5e76\u4e14Kocks-Mecking\u6a21\u578b\u80fd\u591f\u6709\u6548\u63cf\u8ff0\u8fd9\u79cd\u5e94\u53d8\u4f9d\u8d56\u7684\u6548\u5e94\u3002"}}
{"id": "2510.23968", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23968", "abs": "https://arxiv.org/abs/2510.23968", "authors": ["Andriy Myronenko", "Dong Yang", "Baris Turkbey", "Mariam Aboian", "Sena Azamat", "Esra Akcicek", "Hongxu Yin", "Pavlo Molchanov", "Marc Edgar", "Yufan He", "Pengfei Guo", "Yucheng Tang", "Daguang Xu"], "title": "Reasoning Visual Language Model for Chest X-Ray Analysis", "comment": "NV-Reason-CXR-3B", "summary": "Vision-language models (VLMs) have shown strong promise for medical image\nanalysis, but most remain opaque, offering predictions without the transparent,\nstepwise reasoning clinicians rely on. We present a framework that brings\nchain-of-thought (CoT) reasoning to chest X-ray interpretation. Inspired by\nreasoning-first training paradigms, our approach is designed to learn how\nexperts reason, not just what they conclude, by aligning intermediate steps\nwith observable image evidence and radiology workflow. Beyond accuracy, the\nexplicit reasoning traces support clinical auditability: they reveal why a\nconclusion was reached, which alternatives were considered, and where\nuncertainty remains, enabling quality assurance, error analysis, and safer\nhuman-AI collaboration.\n  Our model couples high-fidelity visual encoding with a two-stage training\nrecipe: a reasoning-style supervised fine-tuning (SFT) followed by\nreinforcement learning (RL) that uses verifiable rewards over a list of X-ray\nabnormalities. The model outputs reasoning that mirrors radiologists systematic\nthought process, uncertainty, and differential diagnosis. In\nout-of-distribution evaluation, the approach achieves competitive multi-label\nclassification while improving interpretability. In a reader study with expert\nradiologists, full reasoning traces increased confidence, supported error\nauditing, and reduced time to finalize reports. We release code and the model\nNV-Reason-CXR-3B to support community progress toward trustworthy, explainable\nAI in chest radiography and other medical imaging tasks where reasoning quality\nis as critical as prediction quality.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u601d\u7ef4\u94fe\uff08CoT\uff09\u63a8\u7406\u5e94\u7528\u4e8e\u80f8\u90e8X\u5149\u7247\u89e3\u8bfb\u7684\u6846\u67b6\uff0c\u65e8\u5728\u63d0\u9ad8\u533b\u5b66\u5f71\u50cf\u5206\u6790\u7684\u900f\u660e\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u76ee\u524d\u7684\u533b\u5b66\u5f71\u50cf\u5206\u6790\u6a21\u578b\uff08VLMs\uff09\u5927\u591a\u4e0d\u900f\u660e\uff0c\u65e0\u6cd5\u63d0\u4f9b\u4e34\u5e8a\u533b\u751f\u4f9d\u8d56\u7684\u3001\u5faa\u5e8f\u6e10\u8fdb\u7684\u63a8\u7406\u8fc7\u7a0b\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u8ba9\u6a21\u578b\u4e0d\u4ec5\u5b66\u4f1a\u7ed3\u8bba\uff0c\u66f4\u80fd\u5b66\u4f1a\u4e13\u5bb6\u5982\u4f55\u63a8\u7406\u3002", "method": "\u8be5\u6a21\u578b\u7ed3\u5408\u4e86\u9ad8\u4fdd\u771f\u89c6\u89c9\u7f16\u7801\u548c\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6cd5\uff1a\u9996\u5148\u662f\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\uff0c\u7136\u540e\u662f\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\uff0cRL\u9636\u6bb5\u4f7f\u7528\u53ef\u9a8c\u8bc1\u7684\u5956\u52b1\u6765\u8bc4\u4f30X\u5149\u7247\u5f02\u5e38\u5217\u8868\u3002\u8be5\u6a21\u578b\u751f\u6210\u7684\u63a8\u7406\u8fc7\u7a0b\u6a21\u4eff\u4e86\u653e\u5c04\u79d1\u533b\u751f\u7cfb\u7edf\u7684\u601d\u8003\u8fc7\u7a0b\u3001\u4e0d\u786e\u5b9a\u6027\u548c\u9274\u522b\u8bca\u65ad\u3002", "result": "\u5728\u5206\u5e03\u5916\u8bc4\u4f30\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u6807\u7b7e\u5206\u7c7b\u65b9\u9762\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u7ed3\u679c\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u53ef\u89e3\u91ca\u6027\u3002\u5728\u4e00\u9879\u9488\u5bf9\u4e13\u5bb6\u653e\u5c04\u79d1\u533b\u751f\u7684\u8bfb\u8005\u7814\u7a76\u4e2d\uff0c\u5b8c\u6574\u7684\u63a8\u7406\u8fc7\u7a0b\u63d0\u9ad8\u4e86\u4ed6\u4eec\u7684\u4fe1\u5fc3\uff0c\u652f\u6301\u4e86\u9519\u8bef\u5ba1\u8ba1\uff0c\u5e76\u51cf\u5c11\u4e86\u62a5\u544a\u7684\u6700\u7ec8\u786e\u5b9a\u65f6\u95f4\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u6846\u67b6\u901a\u8fc7\u5c06CoT\u63a8\u7406\u5e94\u7528\u4e8e\u80f8\u90e8X\u5149\u7247\u89e3\u8bfb\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u533b\u5b66\u5f71\u50cf\u5206\u6790\u7684\u900f\u660e\u5ea6\u3001\u53ef\u5ba1\u8ba1\u6027\u548c\u5b89\u5168\u6027\u3002\u7814\u7a76\u6210\u679c\uff08\u4ee3\u7801\u548c\u6a21\u578bNV-Reason-CXR-3B\uff09\u7684\u53d1\u5e03\u5c06\u6709\u52a9\u4e8e\u63a8\u52a8\u53ef\u4fe1\u3001\u53ef\u89e3\u91ca\u7684AI\u5728\u80f8\u90e8\u653e\u5c04\u5b66\u53ca\u5176\u4ed6\u533b\u5b66\u5f71\u50cf\u4efb\u52a1\u4e2d\u7684\u53d1\u5c55\u3002"}}
{"id": "2510.23996", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.23996", "abs": "https://arxiv.org/abs/2510.23996", "authors": ["Y. T. Zhu", "Shibei Xue", "Fangfang Ju", "Haidong Yuan"], "title": "Nonreciprocity enhanced Quantum Gyroscopes based on Surface Acoustic Waves", "comment": "Submitted to Physical Review Applied", "summary": "Surface acoustic waves (SAWs), as Rayleigh waves generated by elastic media,\nhave been used in gyroscopes for over 40 years due to their unique propagation\ncharacteristics. However, their working principle, based on Coriolis effects,\nhas become increasingly ineffective for addressing modern sensing challenges in\ncomplex scenarios. Fortunately, recent advancements in quantized SAWs offer a\npromising solution: SAWs operating at extremely low pump powers (approximately\nat the single-phonon level) can exhibit substantial quantum coherence, enabling\ninvestigations into the fundamental limits of SAW gyroscopes as constrained by\nthe Heisenberg uncertainty relation. In particular, when multiple SAWs couple\nto a common waveguide at distinct locations, the nonlocality arising from the\nspatial separation among coupling points induces directional coupling between\nthe SAWs. To elucidate this directionality, we propose a quantum gyroscope\ncharacterized by multiplepoint couplings. Unlike traditional single-point\ncoupling designs, our gyroscope exhibits distinctive time-delayed dynamics that\ndepend on the system's topologies. We emphasize that these dynamics invalidate\nthe Markovian approximation, even when the time delay is relatively small.\nThrough a comprehensive analysis of all possible topologies, we observe that\nthe directional coupling implies an inherent nonreciprocal transfer. This\nnonreciprocity confers signiffcant advantages to our gyroscope compared to\ntraditional designs, notably enhancing both the signal-to-noise ratio and\nsensitivity. Speciffcally, it enables the extraction of output signals that\nwould otherwise be obscured by noise. Consequently, our ffndings suggest that\nsystems with multiple-point couplings and the associated nonreciprocity can\nserve as valuable resources for advancing quantum sensing technologies.", "AI": {"tldr": "\u57fa\u4e8e\u591a\u70b9\u8026\u5408\u548c\u975e\u4e92\u6613\u6027\u7684\u91cf\u5b50\u9640\u87ba\u4eea", "motivation": "\u4f20\u7edf SAW \u9640\u87ba\u4eea\u57fa\u4e8e\u79d1\u91cc\u5965\u5229\u6548\u5e94\uff0c\u5728\u590d\u6742\u573a\u666f\u4e0b\u96be\u4ee5\u6ee1\u8db3\u73b0\u4ee3\u4f20\u611f\u9700\u6c42\u3002\u91cf\u5316 SAW \u7684\u51fa\u73b0\u4e3a\u89e3\u51b3\u6b64\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\uff0c\u5c24\u5176\u662f\u5728\u91cf\u5b50\u9640\u87ba\u4eea\u9886\u57df\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u70b9\u8026\u5408\u7684\u91cf\u5b50\u9640\u87ba\u4eea\uff0c\u5229\u7528\u591a\u91cd SAW \u5728\u516c\u5171\u6ce2\u5bfc\u4e0a\u7684\u8026\u5408\u3002\u5206\u6790\u4e86\u7531\u7a7a\u95f4\u5206\u79bb\u5f15\u8d77\u7684\u591a\u70b9\u8026\u5408\u7684\u975e\u5c40\u57df\u6027\uff0c\u63ed\u793a\u4e86\u5176\u56fa\u6709\u7684\u975e\u4e92\u6613\u6027\u65f6\u95f4\u5ef6\u8fdf\u52a8\u529b\u5b66\uff0c\u5e76\u8bc1\u660e\u4e86\u8be5\u52a8\u529b\u5b66\u8fdd\u53cd\u4e86\u9a6c\u5c14\u53ef\u592b\u8fd1\u4f3c\u3002\u5bf9\u6240\u6709\u53ef\u80fd\u62d3\u6251\u8fdb\u884c\u4e86\u5168\u9762\u5206\u6790\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u591a\u70b9\u8026\u5408\u7684\u91cf\u5b50\u9640\u87ba\u4eea\u5c55\u73b0\u51fa\u65f6\u95f4\u5ef6\u8fdf\u52a8\u529b\u5b66\uff0c\u4e14\u5177\u6709\u975e\u4e92\u6613\u6027\u3002\u4e0e\u4f20\u7edf\u5355\u70b9\u8026\u5408\u8bbe\u8ba1\u76f8\u6bd4\uff0c\u8be5\u9640\u87ba\u4eea\u663e\u8457\u63d0\u9ad8\u4e86\u4fe1\u566a\u6bd4\u548c\u7075\u654f\u5ea6\uff0c\u80fd\u591f\u63d0\u53d6\u88ab\u566a\u58f0\u63a9\u76d6\u7684\u4fe1\u53f7\u3002", "conclusion": "\u5177\u6709\u591a\u70b9\u8026\u5408\u548c\u7531\u6b64\u4ea7\u751f\u7684\u975e\u4e92\u6613\u6027\u7684\u7cfb\u7edf\u53ef\u4ee5\u4f5c\u4e3a\u63a8\u8fdb\u91cf\u5b50\u4f20\u611f\u6280\u672f\u7684\u5b9d\u8d35\u8d44\u6e90\uff0c\u4e3a SAW \u9640\u87ba\u4eea\u7684\u672a\u6765\u53d1\u5c55\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2510.24370", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.24370", "abs": "https://arxiv.org/abs/2510.24370", "authors": ["Yue Wu"], "title": "Mechanism-Guided Residual Lifting and Control Consistent Modeling for Pneumatic Drying Processes", "comment": "6figs,4tables", "summary": "Pneumatic drying processes in industries such as agriculture, chemicals,and\npharmaceuticals are notoriously difficult to model and control due to\nmulti-source disturbances,coupled stage dynamics, and significant measurement\ndelays. Traditional modeling paradigms often fail to simultaneously deliver\naccuracy, interpretability, and closed-loop applicability. To address this\nchallenge, this paper introduces a unified hybrid modeling framework, termed\nPhysics-Guided Residual Lifting with Control-Consistent Correction,which\nintegrates a transient mechanistic model with a stability-constrained\ndata-driven component. The framework covers the complete process chain of\ndrying, transport, and winnowing. On the mechanistic level, the model unifies\nmass transfer dynamics using the partial pressure difference of water vapor,\nincorporates water activity clamping and latent heat corrections for bound\nwater, and ensures energy closure with moisture-dependent specific heat. On the\ndata-driven level,we propose an orthogonal residual learning scheme. It\nleverages intermediate states from the mechanistic model as proxy variables to\nconstruct a physics-inspired dictionary, preventing parameter compensation and\noverfitting during ridge regression. Furthermore, to ensure suitability for\npredictive control, a Control-Consistent Extended Dynamic Mode Decomposition\nwith stability constraints is employed to learn the residual dynamics, for\nwhich we provide boundedness proofs and stability guarantees. The framework was\nvalidated on 10 industrial batches, comprising 63,000 samples. On unseen test\ndata, the hybrid model achieved a Mean Absolute Error of 0.016% for outlet\nmoisture and 0.015 {\\deg}C for outlet temperature, with values improving to\n0.986 and 0.995, respectively. The resulting prediction residuals exhibit\nwhite-noise characteristics, with significantly reduced spectral energy at low\nfrequencies.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6df7\u5408\u5efa\u6a21\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u6c14\u529b\u5e72\u71e5\u8fc7\u7a0b\u7684\u5efa\u6a21\u548c\u63a7\u5236\u96be\u9898\u3002", "motivation": "\u6c14\u529b\u5e72\u71e5\u8fc7\u7a0b\u7531\u4e8e\u591a\u6e90\u5e72\u6270\u3001\u8026\u5408\u52a8\u529b\u5b66\u548c\u663e\u8457\u7684\u6d4b\u91cf\u5ef6\u8fdf\uff0c\u96be\u4ee5\u8fdb\u884c\u7cbe\u786e\u5efa\u6a21\u548c\u63a7\u5236\uff0c\u4f20\u7edf\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u95ed\u73af\u5e94\u7528\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u77ac\u6001\u673a\u7406\u6a21\u578b\u548c\u53d7\u7a33\u5b9a\u6027\u7ea6\u675f\u7684\u6570\u636e\u9a71\u52a8\u7ec4\u4ef6\u3002\u673a\u7406\u6a21\u578b\u7edf\u4e00\u4e86\u4f20\u8d28\u52a8\u529b\u5b66\uff0c\u5e76\u8003\u8651\u4e86\u6c34\u7684\u7279\u6027\uff1b\u6570\u636e\u9a71\u52a8\u90e8\u5206\u91c7\u7528\u6b63\u4ea4\u6b8b\u5dee\u5b66\u4e60\u548c\u5177\u6709\u7a33\u5b9a\u6027\u7ea6\u675f\u7684\u6269\u5c55\u52a8\u6001\u6a21\u5f0f\u5206\u89e3\uff08EDMD\uff09\u6765\u5b66\u4e60\u6b8b\u5dee\u52a8\u529b\u5b66\uff0c\u4ee5\u786e\u4fdd\u9884\u6d4b\u63a7\u5236\u7684\u9002\u7528\u6027\u3002", "result": "\u572810\u4e2a\u5de5\u4e1a\u6279\u6b21\uff0863,000\u4e2a\u6837\u672c\uff09\u7684\u9a8c\u8bc1\u4e2d\uff0c\u8be5\u6df7\u5408\u6a21\u578b\u5728\u6d4b\u8bd5\u6570\u636e\u4e0a\u5b9e\u73b0\u4e860.016%\u7684\u51fa\u53e3\u6c34\u5206\u548c0.015\u00b0C\u7684\u51fa\u53e3\u6e29\u5ea6\u7684\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\uff0c\u9884\u6d4b\u6b8b\u5dee\u8868\u73b0\u51fa\u767d\u566a\u58f0\u7279\u6027\uff0c\u4f4e\u9891\u80fd\u91cf\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "\u63d0\u51fa\u7684Physics-Guided Residual Lifting with Control-Consistent Correction\u6846\u67b6\u80fd\u591f\u51c6\u786e\u3001\u53ef\u89e3\u91ca\u5730\u5bf9\u6c14\u529b\u5e72\u71e5\u8fc7\u7a0b\u8fdb\u884c\u5efa\u6a21\uff0c\u5e76\u4fdd\u8bc1\u95ed\u73af\u63a7\u5236\u7684\u7a33\u5b9a\u6027\u3002"}}
{"id": "2510.24223", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.24223", "abs": "https://arxiv.org/abs/2510.24223", "authors": ["Mahmut Kemal Ercan", "Alireza Pourafzal", "Musa Furkan Keskin", "Sinan Gezici", "Henk Wymeersch"], "title": "Pilot Distortion Design for ToA Obfuscation in Uplink OFDM Communication", "comment": "The document consists of 6 pages and features 4 figures. Submitted to\n  IEEE WCNC 2026", "summary": "We study uplink orthogonal frequency-division multiplexing (OFDM) pilot\ndistortion to deliberately obfuscate time-of-arrival (ToA) estimation at a\nsingle base station while preserving communication performance. We design a\ncomplex per-subcarrier distortion vector that increases sidelobes of the\nmismatched ambiguity function (MAF) relative to its mainlobe, using two\nobjectives: the sidelobe-to-peak level ratio and the integrated sidelobe level.\nThe design is subject to a transmit-power budget and a proximity\n(dissimilarity) constraint around the communication-optimal pilot.\nCommunication impact is quantied by a capacity-motivated lower bound obtained\nfrom the linear minimum mean-squared error error covariance with a mismatched\nchannel estimate. The resulting generalized fractional program is solved with\nDinkelbach's transform and a difference-of-convex update that yields a\nclosed-form Karush-Kuhn-Tucker step. Simulations on a single-input\nsingle-output OFDM link show that the optimized distortions raise MAF sidelobes\nand degrade delay estimation, as validated by a mismatched maximum-likelihood\nToA estimator, while incurring only marginal capacity loss over a broad\nsignal-to-noise ratio range. The method requires no protocol changes or\nartificial path injection and provides a signal-level mechanism to control ToA\nobservability under communication constraints.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u5728OFDM\u4e0a\u884c\u94fe\u8def\u4e2d\u5f15\u5165\u5b50\u8f7d\u6ce2\u5931\u771f\uff0c\u65e8\u5728\u63d0\u9ad8\u5230\u8fbe\u65f6\u95f4(ToA)\u4f30\u8ba1\u7684\u6a21\u7cca\u5ea6\uff0c\u540c\u65f6\u6700\u5c0f\u5316\u5bf9\u901a\u4fe1\u6027\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "\u4e3a\u4e86\u5728\u4e0d\u6539\u53d8\u901a\u4fe1\u534f\u8bae\u6216\u5f15\u5165\u4eba\u5de5\u8def\u5f84\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u4fe1\u53f7\u5c42\u9762\u63a7\u5236ToA\u53ef\u89c2\u6d4b\u6027\uff0c\u7279\u522b\u662f\u5728\u901a\u4fe1\u7ea6\u675f\u4e0b\u3002", "method": "\u8bbe\u8ba1\u4e00\u79cd\u590d\u6742\u7684\u3001\u6bcf\u5b50\u8f7d\u6ce2\u7684\u5931\u771f\u5411\u91cf\uff0c\u4ee5\u589e\u52a0\u5931\u914d\u6a21\u7cca\u51fd\u6570(MAF)\u7684\u65c1\u74e3\u76f8\u5bf9\u4e8e\u4e3b\u74e3\uff0c\u540c\u65f6\u6ee1\u8db3\u53d1\u5c04\u529f\u7387\u9884\u7b97\u548c\u901a\u4fe1\u6700\u4f18\u5bfc\u9891\u7684\u90bb\u8fd1\u6027\u7ea6\u675f\u3002\u901a\u4fe1\u5f71\u54cd\u901a\u8fc7\u57fa\u4e8e\u5bb9\u91cf\u7684\u4e0b\u754c\u6765\u91cf\u5316\uff0c\u8be5\u4e0b\u754c\u6e90\u4e8e\u5177\u6709\u5931\u914d\u4fe1\u9053\u4f30\u8ba1\u7684\u6700\u5c0f\u5747\u65b9\u8bef\u5dee\u534f\u65b9\u5dee\u3002\u901a\u8fc7Dinkelbach\u53d8\u6362\u548c\u5dee\u5206\u51f8\u66f4\u65b0\uff08\u5177\u6709\u95ed\u5f0fKKT\u6b65\u9aa4\uff09\u6c42\u89e3\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0c\u4f18\u5316\u7684\u5931\u771f\u80fd\u591f\u6709\u6548\u63d0\u9ad8MAF\u65c1\u74e3\uff0c\u964d\u4f4e\u5ef6\u8fdf\u4f30\u8ba1\u7cbe\u5ea6\uff0c\u540c\u65f6\u5728\u5e7f\u6cdb\u7684\u4fe1\u566a\u6bd4\u8303\u56f4\u5185\u4ec5\u5e26\u6765\u8fb9\u9645\u5bb9\u91cf\u635f\u5931\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u53ef\u4ee5\u5728\u4e0d\u6539\u53d8\u534f\u8bae\u6216\u5f15\u5165\u4eba\u5de5\u8def\u5f84\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u4fe1\u53f7\u7ea7\u522b\u7684\u673a\u5236\u5728\u901a\u4fe1\u7ea6\u675f\u4e0b\u63a7\u5236ToA\u7684\u53ef\u89c2\u6d4b\u6027\u3002"}}
{"id": "2510.23636", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23636", "abs": "https://arxiv.org/abs/2510.23636", "authors": ["Thaweerath Phisannupawong", "Joshua Julian Damanik", "Han-Lim Choi"], "title": "Flight Delay Prediction via Cross-Modality Adaptation of Large Language Models and Aircraft Trajectory Representation", "comment": "Preprint submitted to Aerospace Science and Technology (Elsevier) for\n  possible publication", "summary": "Flight delay prediction has become a key focus in air traffic management, as\ndelays highlight inefficiencies that impact overall network performance. This\npaper presents a lightweight large language model-based multimodal flight delay\nprediction, formulated from the perspective of air traffic controllers\nmonitoring aircraft delay after entering the terminal area. The approach\nintegrates trajectory representations with textual aeronautical information,\nincluding flight information, weather reports, and aerodrome notices, by\nadapting trajectory data into the language modality to capture airspace\nconditions. Experimental results show that the model consistently achieves\nsub-minute prediction error by effectively leveraging contextual information\nrelated to the sources of delay. The framework demonstrates that linguistic\nunderstanding, when combined with cross-modality adaptation of trajectory\ninformation, enhances delay prediction. Moreover, the approach shows\npracticality and scalability for real-world operations, supporting real-time\nupdates that refine predictions upon receiving new operational information.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u6a21\u6001\u822a\u73ed\u5ef6\u8bef\u9884\u6d4b\u65b9\u6cd5\uff0c\u4ece\u7a7a\u7ba1\u89c6\u89d2\u51fa\u53d1\uff0c\u878d\u5408\u4e86\u98de\u884c\u8f68\u8ff9\u548c\u6587\u672c\u4fe1\u606f\uff08\u822a\u73ed\u4fe1\u606f\u3001\u5929\u6c14\u62a5\u544a\u3001\u673a\u573a\u901a\u77e5\uff09\uff0c\u5b9e\u73b0\u4e86\u4e9a\u5206\u949f\u7ea7\u7684\u9884\u6d4b\u8bef\u5dee\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002", "motivation": "\u822a\u73ed\u5ef6\u8bef\u5f71\u54cd\u822a\u7a7a\u7f51\u7edc\u8fd0\u884c\u6548\u7387\uff0c\u56e0\u6b64\u5bf9\u5ef6\u8bef\u8fdb\u884c\u51c6\u786e\u9884\u6d4b\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u7ec8\u7aef\u533a\u76d1\u63a7\u4e0b\u3002", "method": "\u5c06\u8f68\u8ff9\u6570\u636e\u8f6c\u5316\u4e3a\u8bed\u8a00\u6a21\u5f0f\uff0c\u5e76\u4e0e\u6587\u672c\u5316\u7684\u822a\u7a7a\u4fe1\u606f\uff08\u822a\u73ed\u4fe1\u606f\u3001\u5929\u6c14\u62a5\u544a\u3001\u673a\u573a\u901a\u77e5\uff09\u76f8\u7ed3\u5408\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u591a\u6a21\u6001\u9884\u6d4b\u3002", "result": "\u8be5\u6a21\u578b\u80fd\u591f\u6709\u6548\u5229\u7528\u5ef6\u8fdf\u6e90\u76f8\u5173\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u5b9e\u73b0\u4e9a\u5206\u949f\u7ea7\u7684\u9884\u6d4b\u8bef\u5dee\uff0c\u5e76\u8bc1\u660e\u4e86\u8bed\u8a00\u7406\u89e3\u548c\u8de8\u6a21\u6001\u8f68\u8ff9\u4fe1\u606f\u9002\u914d\u80fd\u591f\u63d0\u5347\u9884\u6d4b\u7cbe\u5ea6\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6846\u67b6\u5177\u6709\u5b9e\u9645\u64cd\u4f5c\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u80fd\u591f\u652f\u6301\u5b9e\u65f6\u66f4\u65b0\uff0c\u5e76\u4e14\u5728\u7ed3\u5408\u8bed\u8a00\u7406\u89e3\u548c\u8f68\u8ff9\u4fe1\u606f\u8de8\u6a21\u6001\u9002\u914d\u540e\uff0c\u80fd\u6709\u6548\u63d0\u5347\u822a\u73ed\u5ef6\u8bef\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2510.23925", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23925", "abs": "https://arxiv.org/abs/2510.23925", "authors": ["Guohao Sun", "Hang Hua", "Jian Wang", "Jiebo Luo", "Sohail Dianat", "Majid Rabbani", "Raghuveer Rao", "Zhiqiang Tao"], "title": "Latent Chain-of-Thought for Visual Reasoning", "comment": "NeurIPS 2025", "summary": "Chain-of-thought (CoT) reasoning is critical for improving the\ninterpretability and reliability of Large Vision-Language Models (LVLMs).\nHowever, existing training algorithms such as SFT, PPO, and GRPO may not\ngeneralize well across unseen reasoning tasks and heavily rely on a biased\nreward model. To address this challenge, we reformulate reasoning in LVLMs as\nposterior inference and propose a scalable training algorithm based on\namortized variational inference. By leveraging diversity-seeking reinforcement\nlearning algorithms, we introduce a novel sparse reward function for\ntoken-level learning signals that encourage diverse, high-likelihood latent\nCoT, overcoming deterministic sampling limitations and avoiding reward hacking.\nAdditionally, we implement a Bayesian inference-scaling strategy that replaces\ncostly Best-of-N and Beam Search with a marginal likelihood to efficiently rank\noptimal rationales and answers. We empirically demonstrate that the proposed\nmethod enhances the state-of-the-art LVLMs on seven reasoning benchmarks, in\nterms of effectiveness, generalization, and interpretability.", "AI": {"tldr": "Chain-of-thought (CoT) reasoning in LVLMs is improved with a new training algorithm based on amortized variational inference and diversity-seeking reinforcement learning, addressing generalization and reward model biases. The method enhances state-of-the-art LVLMs on seven reasoning benchmarks.", "motivation": "Existing training algorithms for CoT reasoning in LVLMs (SFT, PPO, GRPO) have poor generalization to unseen tasks and rely on biased reward models. The goal is to improve the interpretability and reliability of LVLMs through better CoT reasoning.", "method": "The paper reformulates reasoning in LVLMs as posterior inference and proposes a training algorithm based on amortized variational inference. It uses diversity-seeking reinforcement learning with a novel sparse reward function for token-level learning signals to encourage diverse, high-likelihood latent CoT, avoiding deterministic sampling limitations and reward hacking. A Bayesian inference-scaling strategy replaces costly methods like Best-of-N and Beam Search with marginal likelihood for efficient ranking of rationales and answers.", "result": "The proposed method enhances state-of-the-art LVLMs on seven reasoning benchmarks, improving effectiveness, generalization, and interpretability.", "conclusion": "The proposed method effectively addresses the limitations of existing training algorithms for CoT reasoning in LVLMs, leading to improved performance, generalization, and interpretability across various reasoning tasks."}}
{"id": "2510.23946", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23946", "abs": "https://arxiv.org/abs/2510.23946", "authors": ["Tananun Songdechakraiwut"], "title": "Leveraging LLMs for Early Alzheimer's Prediction", "comment": null, "summary": "We present a connectome-informed LLM framework that encodes dynamic fMRI\nconnectivity as temporal sequences, applies robust normalization, and maps\nthese data into a representation suitable for a frozen pre-trained LLM for\nclinical prediction. Applied to early Alzheimer's detection, our method\nachieves sensitive prediction with error rates well below clinically recognized\nmargins, with implications for timely Alzheimer's intervention.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7ed3\u5408\u52a8\u6001fMRI\u8fde\u63a5\u7ec4\u5b66\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4e34\u5e8a\u9884\u6d4b\uff0c\u7279\u522b\u662f\u65e9\u671f\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u68c0\u6d4b\u3002", "motivation": "\u4e3a\u4e86\u5b9e\u73b0\u5bf9\u65e9\u671f\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u7684\u654f\u611f\u9884\u6d4b\uff0c\u4ee5\u63d0\u4f9b\u53ca\u65f6\u7684\u5e72\u9884\u3002", "method": "\u5c06\u52a8\u6001fMRI\u8fde\u63a5\u7ec4\u5b66\u6570\u636e\u7f16\u7801\u4e3a\u65f6\u95f4\u5e8f\u5217\uff0c\u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u5e76\u6620\u5c04\u5230\u9002\u5408\u51bb\u7ed3\u7684\u9884\u8bad\u7ec3LLM\u7684\u8868\u793a\uff0c\u7528\u4e8e\u4e34\u5e8a\u9884\u6d4b\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u65e9\u671f\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u68c0\u6d4b\u4e2d\u53d6\u5f97\u4e86\u654f\u611f\u7684\u9884\u6d4b\u6548\u679c\uff0c\u9519\u8bef\u7387\u8fdc\u4f4e\u4e8e\u4e34\u5e8a\u516c\u8ba4\u7684\u9608\u503c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5bf9\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u7684\u53ca\u65f6\u5e72\u9884\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2510.24108", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24108", "abs": "https://arxiv.org/abs/2510.24108", "authors": ["Zhenxin Li", "Wenhao Yao", "Zi Wang", "Xinglong Sun", "Jingde Chen", "Nadine Chang", "Maying Shen", "Jingyu Song", "Zuxuan Wu", "Shiyi Lan", "Jose M. Alvarez"], "title": "ZTRS: Zero-Imitation End-to-end Autonomous Driving with Trajectory Scoring", "comment": null, "summary": "End-to-end autonomous driving maps raw sensor inputs directly into\nego-vehicle trajectories to avoid cascading errors from perception modules and\nto leverage rich semantic cues. Existing frameworks largely rely on Imitation\nLearning (IL), which can be limited by sub-optimal expert demonstrations and\ncovariate shift during deployment. On the other hand, Reinforcement Learning\n(RL) has recently shown potential in scaling up with simulations, but is\ntypically confined to low-dimensional symbolic inputs (e.g. 3D objects and\nmaps), falling short of full end-to-end learning from raw sensor data. We\nintroduce ZTRS (Zero-Imitation End-to-End Autonomous Driving with Trajectory\nScoring), a framework that combines the strengths of both worlds: sensor inputs\nwithout losing information and RL training for robust planning. To the best of\nour knowledge, ZTRS is the first framework that eliminates IL entirely by only\nlearning from rewards while operating directly on high-dimensional sensor data.\nZTRS utilizes offline reinforcement learning with our proposed Exhaustive\nPolicy Optimization (EPO), a variant of policy gradient tailored for enumerable\nactions and rewards. ZTRS demonstrates strong performance across three\nbenchmarks: Navtest (generic real-world open-loop planning), Navhard (open-loop\nplanning in challenging real-world and synthetic scenarios), and HUGSIM\n(simulated closed-loop driving). Specifically, ZTRS achieves the\nstate-of-the-art result on Navhard and outperforms IL-based baselines on\nHUGSIM. Code will be available at https://github.com/woxihuanjiangguo/ZTRS.", "AI": {"tldr": "ZTRS\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u81ea\u52a8\u9a7e\u9a76\u6846\u67b6\uff0c\u5b83\u7ed3\u5408\u4e86\u6a21\u4eff\u5b66\u4e60\uff08IL\uff09\u548c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684\u4f18\u70b9\uff0c\u53ef\u4ee5\u76f4\u63a5\u5904\u7406\u9ad8\u7ef4\u4f20\u611f\u5668\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u5956\u52b1\u8fdb\u884c\u5b66\u4e60\uff0c\u65e0\u9700IL\u3002", "motivation": "\u73b0\u6709\u7684\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u6846\u67b6\u4e3b\u8981\u4f9d\u8d56\u6a21\u4eff\u5b66\u4e60\uff08IL\uff09\uff0c\u4f46IL\u53ef\u80fd\u53d7\u5230\u4e13\u5bb6\u6f14\u793a\u7684\u6b21\u4f18\u6027\u548c\u90e8\u7f72\u65f6\u7684\u534f\u53d8\u91cf\u504f\u79fb\u7684\u9650\u5236\u3002\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u867d\u7136\u6709\u6f5c\u529b\uff0c\u4f46\u901a\u5e38\u4ec5\u9650\u4e8e\u4f4e\u7ef4\u5ea6\u7684\u7b26\u53f7\u8f93\u5165\uff0c\u65e0\u6cd5\u5b9e\u73b0\u4ece\u539f\u59cb\u4f20\u611f\u5668\u6570\u636e\u8fdb\u884c\u7aef\u5230\u7aef\u5b66\u4e60\u3002\u672c\u7814\u7a76\u65e8\u5728\u7ed3\u5408\u4e24\u8005\u7684\u4f18\u70b9\uff0c\u63d0\u51fa\u4e00\u4e2a\u80fd\u591f\u76f4\u63a5\u5904\u7406\u9ad8\u7ef4\u4f20\u611f\u5668\u6570\u636e\u7684RL\u6846\u67b6\u3002", "method": "ZTRS\uff08Zero-Imitation End-to-End Autonomous Driving with Trajectory Scoring\uff09\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u4f20\u611f\u5668\u8f93\u5165\u548cRL\u8bad\u7ec3\u3002\u5b83\u4f7f\u7528\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u548c\u63d0\u51fa\u7684\u8be6\u5c3d\u7b56\u7565\u4f18\u5316\uff08EPO\uff09\u65b9\u6cd5\uff0cEPO\u662f\u7b56\u7565\u68af\u5ea6\u7684\u53d8\u4f53\uff0c\u9002\u7528\u4e8e\u53ef\u679a\u4e3e\u7684\u52a8\u4f5c\u548c\u5956\u52b1\u3002", "result": "ZTRS\u5728Navtest\uff08\u901a\u7528\u771f\u5b9e\u4e16\u754c\u5f00\u73af\u89c4\u5212\uff09\u3001Navhard\uff08\u771f\u5b9e\u4e16\u754c\u548c\u5408\u6210\u573a\u666f\u4e0b\u7684\u5f00\u73af\u89c4\u5212\uff09\u548cHUGSIM\uff08\u6a21\u62df\u95ed\u73af\u9a7e\u9a76\uff09\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u5f3a\u52b2\u3002\u5728Navhard\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6210\u679c\uff0c\u5e76\u5728HUGSIM\u4e0a\u8d85\u8d8a\u4e86\u57fa\u4e8eIL\u7684\u57fa\u7ebf\u3002", "conclusion": "ZTRS\u662f\u7b2c\u4e00\u4e2a\u5b8c\u5168\u6d88\u9664IL\uff0c\u4ec5\u901a\u8fc7\u5956\u52b1\u5b66\u4e60\uff0c\u5e76\u76f4\u63a5\u5728 \u0410\u9ad8\u7ef4\u4f20\u611f\u5668\u6570\u636e\u4e0a\u8fd0\u884c\u7684\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u6846\u67b6\u3002"}}
{"id": "2510.24409", "categories": ["cond-mat.mtrl-sci", "cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2510.24409", "abs": "https://arxiv.org/abs/2510.24409", "authors": ["Yanzhen Cai", "Mingtai Xie", "Jing Kang", "Weizhen Zhuo", "Wei Ren", "Xijing Dai", "Anmin Zhang", "Jianting Ji", "Feng Jin", "Zheng Zhang", "Qingming Zhang"], "title": "Anomalous enhancement of magnetism by nonmagnetic doping in the honeycomb-lattice antiferromagnet ErOCl", "comment": "12 pages, 4 figures", "summary": "Tuning magnetic anisotropy through chemical doping is a powerful strategy for\ndesigning functional materials with enhanced magnetic properties. Here, we\nreport an enhanced Er^3+ magnetic moment resulting from nonmagnetic Lu^3+\nsubstitution in the honeycomb-lattice antiferromagnet ErOCl. Unlike the\nCurie-Weiss type divergence typically observed in diluted magnetic systems, our\nfindings reveal a distinct enhancement of magnetization per Er^3+ ion under\nhigh magnetic fields, suggesting an unconventional mechanism. Structural\nanalysis reveals that Lu^3+ doping leads to a pronounced contraction of the c\naxis, which is attributed to chemical pressure effects, while preserving the\nlayered SmSI-type crystal structure with space group R-3m. High-resolution\nRaman spectroscopy reveals a systematic blueshift of the first and seventh\ncrystalline electric field (CEF) excitations, indicating an increase in the\naxial CEF parameter B_2^0. This modification enhances the magnetic anisotropy\nalong the c axis, leading to a significant increase in magnetization at low\ntemperatures and under high magnetic fields, contrary to conventional\nexpectations for magnetic dilution. Our work not only clarifies the intimate\nconnection between magnetism and CEF in rare-earth compounds, but more\nimportantly, it reveals a physical pathway to effectively tune magnetic\nanisotropy via anisotropic lattice distortion induced by chemical pressure.", "AI": {"tldr": "\u901a\u8fc7\u5316\u5b66\u63ba\u6742\u8c03\u63a7\u78c1\u5404\u5411\u5f02\u6027\u662f\u8bbe\u8ba1\u529f\u80fd\u6750\u6599\u7684\u6709\u6548\u7b56\u7565\u3002\u672c\u7814\u7a76\u901a\u8fc7\u5728\u8702\u7a9d\u72b6\u53cd\u94c1\u78c1\u4f53ErOCl\u4e2d\u63ba\u6742\u975e\u78c1\u6027Lu^3+\uff0c\u5b9e\u73b0\u4e86Er^3+\u78c1\u77e9\u7684\u589e\u5f3a\uff0c\u5e76\u63ed\u793a\u4e86\u5176\u5728\u5316\u5b66\u538b\u529b\u5f15\u8d77\u7684\u6676\u683c\u7578\u53d8\u548c\u6676\u4f53\u7535\u573a\u6548\u5e94\u4e0b\u7684\u975e\u5e38\u89c4\u78c1\u5316\u673a\u5236\u3002", "motivation": "Tuning magnetic anisotropy through chemical doping is a powerful strategy for designing functional materials with enhanced magnetic properties. This paper investigates the effect of Lu^3+ substitution on the magnetic properties of the ErOCl antiferromagnet.", "method": "The study employed structural analysis, high-resolution Raman spectroscopy, and magnetic measurements to investigate the effects of Lu^3+ doping. Structural analysis characterized the lattice changes, Raman spectroscopy probed the crystalline electric field excitations, and magnetic measurements quantified the resulting magnetic properties.", "result": "Lu^3+ doping led to a contraction of the c axis due to chemical pressure, enhancing the axial CEF parameter B_2^0. This resulted in enhanced magnetic anisotropy along the c axis, leading to a distinct enhancement of magnetization per Er^3+ ion under high magnetic fields, contrary to typical magnetic dilution effects.", "conclusion": "The research clarifies the connection between magnetism and CEF in rare-earth compounds and demonstrates a method to tune magnetic anisotropy via anisotropic lattice distortion induced by chemical pressure."}}
{"id": "2510.24162", "categories": ["quant-ph", "cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2510.24162", "abs": "https://arxiv.org/abs/2510.24162", "authors": ["F. Cavaliere", "D. Ferraro", "M. Carrega", "G. Benenti", "M. Sassetti"], "title": "Quantum advantage bounds for a multipartite Gaussian battery", "comment": "Main text: 9 pages, 4 figures. Supplemental material: 10 pages, 1\n  figure", "summary": "We demonstrate the possibility of a genuine quantum advantage in the\nefficiency of quantum batteries by analyzing a model that enables a consistent\ncomparison between quantum and classical regimes. Our system consists of $N$\nharmonic oscillator cells coupled to a common thermal reservoir, evolving\nthrough Gaussian states. We define the global efficiency as the ratio of\nextractable work (ergotropy) to stored energy, and derive analytical bounds\nthat distinguish, in order of increasing efficiency, regimes characterized by\nclassical squeezing, quantum squeezing without entanglement, and genuine\nentanglement. Moreover, numerical simulations support the emergence of a\nsimilar hierarchy for the thermodynamic efficiency, defined as the ratio\nbetween ergotropy and the total thermodynamic cost of the charging process.", "AI": {"tldr": "\u901a\u8fc7\u5206\u6790\u4e00\u4e2a\u5141\u8bb8\u5728\u91cf\u5b50\u548c\u7ecf\u5178\u673a\u5236\u4e4b\u95f4\u8fdb\u884c\u4e00\u81f4\u6bd4\u8f83\u7684\u6a21\u578b\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u91cf\u5b50\u7535\u6c60\u5728\u6548\u7387\u65b9\u9762\u5177\u6709\u771f\u6b63\u7684\u91cf\u5b50\u4f18\u52bf\u3002", "motivation": "\u672c\u6587\u7684\u52a8\u673a\u5728\u4e8e\u63a2\u7d22\u91cf\u5b50\u7535\u6c60\u5728\u6548\u7387\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u5e76\u901a\u8fc7\u5efa\u7acb\u4e00\u4e2a\u80fd\u591f\u533a\u5206\u91cf\u5b50\u548c\u7ecf\u5178\u673a\u5236\u7684\u6a21\u578b\u6765\u8fdb\u884c\u4e00\u81f4\u6027\u6bd4\u8f83\u3002", "method": "\u7814\u7a76\u91c7\u7528\u4e86\u4e00\u4e2a\u7531N\u4e2a\u8c10\u632f\u5b50\u5355\u5143\u8026\u5408\u5230\u4e00\u4e2a\u516c\u5171\u70ed\u5e93\u5e76\u6f14\u5316\u4e3a\u9ad8\u65af\u6001\u7684\u7cfb\u7edf\u3002\u901a\u8fc7\u5b9a\u4e49\u53ef\u63d0\u53d6\u529f\uff08\u529f\u91cf\uff09\u4e0e\u5b58\u50a8\u80fd\u91cf\u4e4b\u6bd4\u4f5c\u4e3a\u5168\u5c40\u6548\u7387\uff0c\u5e76\u63a8\u5bfc\u51fa\u533a\u5206\u4e0d\u540c\u673a\u5236\uff08\u7ecf\u5178\u538b\u7f29\u3001\u65e0\u7ea0\u7f20\u91cf\u5b50\u538b\u7f29\u3001\u771f\u5b9e\u7ea0\u7f20\uff09\u7684\u89e3\u6790\u754c\u9650\u3002\u6b64\u5916\uff0c\u6570\u503c\u6a21\u62df\u4e5f\u652f\u6301\u4e86\u70ed\u529b\u5b66\u6548\u7387\uff08\u529f\u91cf\u4e0e\u5145\u7535\u8fc7\u7a0b\u603b\u70ed\u529b\u5b66\u6210\u672c\u4e4b\u6bd4\uff09\u7684\u7c7b\u4f3c\u5c42\u7ea7\u7ed3\u6784\u3002", "result": "\u7814\u7a76\u63a8\u5bfc\u51fa\u4e86\u533a\u5206\u7ecf\u5178\u538b\u7f29\u3001\u65e0\u7ea0\u7f20\u91cf\u5b50\u538b\u7f29\u548c\u771f\u5b9e\u7ea0\u7f20\u7684\u5168\u5c40\u6548\u7387\u89e3\u6790\u754c\u9650\u3002\u6570\u503c\u6a21\u62df\u7ed3\u679c\u652f\u6301\u70ed\u529b\u5b66\u6548\u7387\u4e5f\u5448\u73b0\u7c7b\u4f3c\u7684\u5c42\u7ea7\u7ed3\u6784\u3002", "conclusion": "\u672c\u6587\u8bc1\u660e\u4e86\u5728\u91cf\u5b50\u7535\u6c60\u6548\u7387\u65b9\u9762\u5b58\u5728\u771f\u6b63\u7684\u91cf\u5b50\u4f18\u52bf\uff0c\u5e76\u63ed\u793a\u4e86\u4e0d\u540c\u7684\u91cf\u5b50\u5173\u8054\uff08\u538b\u7f29\u3001\u7ea0\u7f20\uff09\u5728\u6548\u7387\u63d0\u5347\u4e2d\u626e\u6f14\u7684\u89d2\u8272\u3002"}}
{"id": "2510.23978", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23978", "abs": "https://arxiv.org/abs/2510.23978", "authors": ["Kazutoshi Akita", "Norimichi Ukita"], "title": "Efficient Cost-and-Quality Controllable Arbitrary-scale Super-resolution with Fourier Constraints", "comment": "9 pages", "summary": "Cost-and-Quality (CQ) controllability in arbitrary-scale super-resolution is\ncrucial. Existing methods predict Fourier components one by one using a\nrecurrent neural network. However, this approach leads to performance\ndegradation and inefficiency due to independent prediction. This paper proposes\npredicting multiple components jointly to improve both quality and efficiency.", "AI": {"tldr": "\u8054\u5408\u9884\u6d4b\u5085\u91cc\u53f6\u5206\u91cf\u4ee5\u63d0\u9ad8\u4efb\u610f\u5c3a\u5ea6\u8d85\u5206\u8fa8\u7387\u7684\u6210\u672c\u548c\u8d28\u91cf\u53ef\u63a7\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u9010\u4e2a\u9884\u6d4b\u5085\u91cc\u53f6\u5206\u91cf\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u548c\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u8054\u5408\u9884\u6d4b\u591a\u4e2a\u5085\u91cc\u53f6\u5206\u91cf\u3002", "result": "\u63d0\u9ad8\u4e86\u8d28\u91cf\u548c\u6548\u7387\u3002", "conclusion": "\u8054\u5408\u9884\u6d4b\u662f\u63d0\u9ad8\u8d85\u5206\u8fa8\u7387\u6027\u80fd\u548c\u6548\u7387\u7684\u5173\u952e\u3002"}}
{"id": "2510.24047", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.24047", "abs": "https://arxiv.org/abs/2510.24047", "authors": ["B. M. Rodriguez-Lara", "H. Ghaemi-Dizicheh", "S. Dehdashti", "A. Hanke", "A. Touhami", "J. N\u00f6tzel"], "title": "Non-Hermitian $\\mathrm{sl}(3, \\mathbb{C})$ three-mode couplers", "comment": "33 pages, 9 figures", "summary": "Photonic systems with exceptional points, where eigenvalues and corresponding\neigenstates coalesce, have attracted interest due to their topological features\nand enhanced sensitivity to external perturbations. Non-Hermitian mode-coupling\nmatrices provide a tractable analytic framework to model gain, loss, and\nchirality across optical, electronic, and mechanical platforms without the\ncomplexity of full open-system dynamics. Exceptional points define their\nspectral topology, and enable applications in mode control, amplification, and\nsensing. Yet $N$-mode couplers, the minimal setting for $N$th-order exceptional\npoints, are often studied in specific designs that overlook their algebraic\nstructure. We introduce a general $\\mathrm{sl}(N,\\mathbb{C})$ framework for\narbitrary $N$-mode couplers in classical and quantum regimes, and develop it\nexplicitly for $N=3$. This case admits algebraic diagonalization, where a\npropagation-dependent gauge aligns local and dynamical spectra and reveals the\ngeometric phase connecting adiabatic and exact propagation. An exact\nWei--Norman propagator captures the full dynamics and makes crossing\nexceptional points explicit. Our framework enables classification of coupler\nfamilies. We study the family spanning $\\mathcal{PT}$-symmetric and\nnon-Hermitian cyclic couplers, where two exceptional points of order three lie\nwithin a continuum of exceptional points of order two, ruling out pure\nencircling. As an application, we study these exceptional points for a lossy\nthree-leg beam splitter and reveal its propagation dynamics as a function of\ninitial states, such as Fock and NOON states. Our approach provides a\nsystematic route to analyze non-Hermitian mode couplers and guide design in\nclassical and quantum platforms.", "AI": {"tldr": "exceptional points in N-mode couplers can be modeled using a general sl(N,C) framework, with explicit analysis for N=3, revealing algebraic diagonalization and dynamics via a Wei-Norman propagator. This framework allows for coupler classification and analysis of applications like beam splitters with different initial states.", "motivation": "exceptional points in photonic systems offer topological features and enhanced sensitivity, but N-mode couplers, minimal for Nth-order exceptional points, are often studied in specific designs neglecting their algebraic structure.", "method": "Introduced a general sl(N,C) framework for N-mode couplers, developing it explicitly for N=3. Used algebraic diagonalization and a propagation-dependent gauge to align spectra and reveal geometric phase. Employed an exact Wei-Norman propagator to capture full dynamics and make crossing exceptional points explicit. Classified coupler families, studying the PT-symmetric and non-Hermitian cyclic couplers.", "result": "The N=3 case admits algebraic diagonalization, revealing the geometric phase. An exact Wei-Norman propagator captures the full dynamics. Two exceptional points of order three were found within a continuum of exceptional points of order two for PT-symmetric and non-Hermitian cyclic couplers. The propagation dynamics of a lossy three-leg beam splitter with Fock and NOON states were studied.", "conclusion": "The sl(N,C) framework provides a systematic route to analyze non-Hermitian mode couplers and guide their design in classical and quantum platforms."}}
{"id": "2510.24389", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.24389", "abs": "https://arxiv.org/abs/2510.24389", "authors": ["Lamine Chalal", "Ahmed Rachid"], "title": "Development of a Digital Twin for an Electric Vehicle Emulator Modeling, Control, and Experimental Validation", "comment": "6 pages, Accepted at CODIT 2025 (Conference on Decision and Control\n  in Intelligent Technology)", "summary": "This paper presents the development and validation of a digital twin for a\nscaled-down electric vehicle (EV) emulator, designed to replicate longitudinal\nvehicle dynamics under diverse operating conditions. The emulator integrates a\nseparately excited DC motor (SEDCM), a four-quadrant DC-DC converter, a battery\nemulator, and a mechanical load emulator. The system models tractive effort,\naerodynamic drag, and gradient resistance using Newton's second law. In\ncontrast to conventional graphical modeling tools (e.g., block diagrams and\nbond graphs), the adopted Energetic Macroscopic Representation (EMR) framework\noffers clear advantages by explicitly representing energy interactions and\nfacilitating the systematic derivation of control structures. A control\nstrategy developed within this framework governs energy flow across the\npowertrain, enabling accurate speed control via armature voltage regulation.\nExperimental tests conducted on a Lucas-Nulle test bench show strong\ncorrelation with simulation results. The study also introduces a methodology to\ncompute the maximum admissible vehicle mass - determined to be 13.5 kg for a\n180 W motor operating at 1900 rpm - based on acceleration and slope\nconstraints. Furthermore, a switching algorithm for the bidirectional converter\nensures reliable four quadrant operation. Overall, the proposed framework\nprovides a scalable and effective approach for EV emulation, control design,\nand energy management validation.", "AI": {"tldr": "\u672c\u6587\u5f00\u53d1\u5e76\u9a8c\u8bc1\u4e86\u4e00\u4e2a\u7528\u4e8e\u6a21\u62df\u7535\u52a8\u6c7d\u8f66\uff08EV\uff09\u7eb5\u5411\u52a8\u529b\u5b66\u7684\u6570\u5b57\u5b6a\u751f\u6a21\u578b\uff0c\u91c7\u7528Energetic Macroscopic Representation (EMR)\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u51c6\u786e\u6027\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u8ba1\u7b97\u6700\u5927\u5141\u8bb8\u8f66\u8f86\u8d28\u91cf\u7684\u65b9\u6cd5\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u7cbe\u786e\u6a21\u62df\u7535\u52a8\u6c7d\u8f66\uff08EV\uff09\u7eb5\u5411\u52a8\u529b\u5b66\u884c\u4e3a\u7684\u6570\u5b57\u5b6a\u751f\u6a21\u578b\uff0c\u4ee5\u4fbf\u4e8e\u7814\u7a76\u548c\u9a8c\u8bc1\u63a7\u5236\u7b56\u7565\u53ca\u80fd\u6e90\u7ba1\u7406\u3002EMR\u6846\u67b6\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u5177\u6709\u66f4\u6e05\u6670\u7684\u80fd\u91cf\u4ea4\u4e92\u8868\u793a\u548c\u7cfb\u7edf\u5316\u7684\u63a7\u5236\u7ed3\u6784\u63a8\u5bfc\u4f18\u52bf\u3002", "method": "\u4f7f\u7528Energetic Macroscopic Representation (EMR)\u6846\u67b6\u5efa\u7acb\u4e86\u4e00\u4e2a\u7535\u52a8\u6c7d\u8f66\uff08EV\uff09\u6570\u5b57\u5b6a\u751f\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u96c6\u6210\u4e86\u76f4\u6d41\u7535\u673a\u3001DC-DC\u53d8\u6362\u5668\u3001\u7535\u6c60\u6a21\u62df\u5668\u548c\u673a\u68b0\u8d1f\u8f7d\u6a21\u62df\u5668\u3002\u901a\u8fc7\u725b\u987f\u7b2c\u4e8c\u5b9a\u5f8b\u6a21\u62df\u7275\u5f15\u529b\u3001\u7a7a\u6c14\u963b\u529b\u548c\u5761\u9053\u963b\u529b\u3002\u63a7\u5236\u7b56\u7565\u57fa\u4e8eEMR\u6846\u67b6\uff0c\u901a\u8fc7\u8c03\u8282\u70b9\u706b\u7535\u538b\u6765\u8c03\u8282\u80fd\u91cf\u6d41\uff0c\u5b9e\u73b0\u7cbe\u786e\u7684\u901f\u5ea6\u63a7\u5236\u3002\u8bbe\u8ba1\u4e86\u53cc\u5411\u53d8\u6362\u5668\u7684\u5207\u6362\u7b97\u6cd5\u4ee5\u786e\u4fdd\u56db\u8c61\u9650\u8fd0\u884c\u3002", "result": "\u5b9e\u9a8c\u6d4b\u8bd5\u7ed3\u679c\u4e0e\u4eff\u771f\u7ed3\u679c\u9ad8\u5ea6\u543b\u5408\uff0c\u8bc1\u660e\u4e86\u6a21\u578b\u7684\u51c6\u786e\u6027\u3002\u8ba1\u7b97\u5f97\u51fa\uff0c\u5bf9\u4e8e\u4e00\u4e2a\u57281900\u8f6c/\u5206\u949f\u4e0b\u8fd0\u884c\u7684180\u74e6\u7535\u673a\uff0c\u6700\u5927\u5141\u8bb8\u8f66\u8f86\u8d28\u91cf\u4e3a13.5\u516c\u65a4\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6570\u5b57\u5b6a\u751f\u6846\u67b6\u4e3a\u7535\u52a8\u6c7d\u8f66\uff08EV\uff09\u6a21\u62df\u3001\u63a7\u5236\u8bbe\u8ba1\u548c\u80fd\u6e90\u7ba1\u7406\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u6709\u6548\u7684\u65b9\u6cd5\u3002EMR\u6846\u67b6\u5728\u8868\u793a\u80fd\u91cf\u4ea4\u4e92\u548c\u63a8\u5bfc\u63a7\u5236\u7ed3\u6784\u65b9\u9762\u663e\u793a\u51fa\u5176\u4f18\u8d8a\u6027\u3002"}}
{"id": "2510.24243", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.24243", "abs": "https://arxiv.org/abs/2510.24243", "authors": ["Duc Nguyen Dao", "Haibin Zhang", "Andre B. J. Kokkeler", "Yang Miao"], "title": "Joint Beamforming for Multi-user Multi-target FD ISAC System: A Hybrid GRQ-GA Approach", "comment": "6 pages, 4 figures", "summary": "In this paper, we consider a full-duplex (FD) Integrated Sensing and\nCommunication (ISAC) system, in which the base station (BS) performs downlink\nand uplink communications with multiple users while simultaneously sensing\nmultiple targets. In the scope of this work, we assume a narrowband and static\nscenario, aiming to focus on the beamforming and power allocation strategies.\nWe propose a joint beamforming strategy for designing transmit and receive\nbeamformer vectors at the BS. The optimization problem aims to maximize the\ncommunication sum-rate, which is critical for ensuring high-quality service to\nusers, while also maintaining accurate sensing performance for detection tasks\nand adhering to maximum power constraints for efficient resource usage. The\noptimal receive beamformers are first derived using a closed-form Generalized\nRayleigh Quotient (GRQ) solution, reducing the variables to be optimized. Then,\nthe remaining problem is solved using floating-point Genetic Algorithms (GA).\nThe numerical results show that the proposed GA-based solution demonstrates up\nto a 98% enhancement in sum-rate compared to a baseline half-duplex ISAC system\nand provides better performance than a benchmark algorithm from the literature.\nAdditionally, it offers insights into sensing performance effects on beam\npatterns as well as communicationsensing trade-offs in multi-target scenarios.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5168\u53cc\u5de5\uff08FD\uff09\u96c6\u6210\u4f20\u611f\u4e0e\u901a\u4fe1\uff08ISAC\uff09\u7cfb\u7edf\u7684\u8054\u5408\u6ce2\u675f\u5f62\u6210\u7b56\u7565\uff0c\u4ee5\u6700\u5927\u5316\u901a\u4fe1\u548c\u4f20\u611f\u6027\u80fd\u3002", "motivation": "\u5728\u5168\u53cc\u5de5\uff08FD\uff09ISAC\u7cfb\u7edf\u4e2d\uff0c\u540c\u65f6\u8fdb\u884c\u901a\u4fe1\u548c\u4f20\u611f\u4efb\u52a1\uff0c\u9700\u8981\u4f18\u5316\u6ce2\u675f\u5f62\u6210\u548c\u529f\u7387\u5206\u914d\u7b56\u7565\uff0c\u4ee5\u5728\u6ee1\u8db3\u901a\u4fe1\u901f\u7387\u8981\u6c42\u7684\u540c\u65f6\uff0c\u4fdd\u8bc1\u4f20\u611f\u7cbe\u5ea6\u548c\u529f\u7387\u7ea6\u675f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u5408\u6ce2\u675f\u5f62\u6210\u7b56\u7565\uff0c\u9996\u5148\u5229\u7528\u5e7f\u4e49\u745e\u5229\u5546\uff08GRQ\uff09\u5f97\u5230\u6700\u4f18\u63a5\u6536\u6ce2\u675f\u5f62\u6210\u5668\u7684\u95ed\u5f0f\u89e3\uff0c\u7136\u540e\u4f7f\u7528\u6d6e\u70b9\u9057\u4f20\u7b97\u6cd5\uff08GA\uff09\u89e3\u51b3\u5269\u4f59\u95ee\u9898\u3002", "result": "\u4e0e\u57fa\u51c6\u534a\u53cc\u5de5ISAC\u7cfb\u7edf\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684GA\u65b9\u6cd5\u53ef\u5c06\u548c\u901f\u7387\u63d0\u9ad898%\uff0c\u5e76\u4e14\u4f18\u4e8e\u6587\u732e\u4e2d\u7684\u57fa\u51c6\u7b97\u6cd5\u3002\u8be5\u65b9\u6cd5\u8fd8\u63ed\u793a\u4e86\u4f20\u611f\u6027\u80fd\u5bf9\u6ce2\u675f\u6a21\u5f0f\u7684\u5f71\u54cd\u4ee5\u53ca\u591a\u76ee\u6807\u573a\u666f\u4e0b\u7684\u901a\u4fe1-\u4f20\u611f\u6743\u8861\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u8054\u5408\u6ce2\u675f\u5f62\u6210\u7b56\u7565\u80fd\u591f\u6709\u6548\u63d0\u5347\u5168\u53cc\u5de5ISAC\u7cfb\u7edf\u7684\u901a\u4fe1\u548c\u4f20\u611f\u6027\u80fd\uff0c\u5e76\u5728\u901a\u4fe1\u901f\u7387\u548c\u4f20\u611f\u7cbe\u5ea6\u4e4b\u95f4\u53d6\u5f97\u826f\u597d\u7684\u6743\u8861\u3002"}}
{"id": "2510.23942", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23942", "abs": "https://arxiv.org/abs/2510.23942", "authors": ["Sridhar Mahadevan"], "title": "Decentralized Causal Discovery using Judo Calculus", "comment": "54 pages", "summary": "We describe a theory and implementation of an intuitionistic decentralized\nframework for causal discovery using judo calculus, which is formally defined\nas j-stable causal inference using j-do-calculus in a topos of sheaves. In\nreal-world applications -- from biology to medicine and social science --\ncausal effects depend on regime (age, country, dose, genotype, or lab\nprotocol). Our proposed judo calculus formalizes this context dependence\nformally as local truth: a causal claim is proven true on a cover of regimes,\nnot everywhere at once. The Lawvere-Tierney modal operator j chooses which\nregimes are relevant; j-stability means the claim holds constructively and\nconsistently across that family. We describe an algorithmic and implementation\nframework for judo calculus, combining it with standard score-based,\nconstraint-based, and gradient-based causal discovery methods. We describe\nexperimental results on a range of domains, from synthetic to real-world\ndatasets from biology and economics. Our experimental results show the\ncomputational efficiency gained by the decentralized nature of sheaf-theoretic\ncausal discovery, as well as improved performance over classical causal\ndiscovery methods.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e j-do-calculus \u7684\u76f4\u89c9\u4e3b\u4e49\u53bb\u4e2d\u5fc3\u5316\u56e0\u679c\u53d1\u73b0\u6846\u67b6\uff0c\u5e76\u63d0\u4f9b\u4e86\u5b9e\u73b0\u3002\u8be5\u6846\u67b6\u5c06\u56e0\u679c\u6548\u5e94\u7684\u4f9d\u8d56\u6027\u5f62\u5f0f\u5316\u4e3a\u5c40\u90e8\u771f\u7406\uff0c\u5373\u56e0\u679c\u5173\u7cfb\u5728\u4e00\u4e2a\u653f\u6743\uff08\u5982\u5e74\u9f84\u3001\u56fd\u5bb6\u3001\u5242\u91cf\u7b49\uff09\u7684\u8986\u76d6\u8303\u56f4\u5185\u5f97\u5230\u8bc1\u660e\uff0c\u800c\u4e0d\u662f\u5728\u6240\u6709\u5730\u65b9\u540c\u65f6\u8bc1\u660e\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u7684\u56e0\u679c\u6548\u5e94\u4f9d\u8d56\u4e8e\u7279\u5b9a\u60c5\u5883\uff08\u5982\u5e74\u9f84\u3001\u56fd\u5bb6\u3001\u5242\u91cf\u3001\u57fa\u56e0\u578b\u6216\u5b9e\u9a8c\u5ba4\u65b9\u6848\uff09\uff0c\u672c\u7814\u7a76\u65e8\u5728\u5f62\u5f0f\u5316\u8fd9\u79cd\u60c5\u5883\u4f9d\u8d56\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201cjudo calculus\u201d\u7684\u7406\u8bba\u548c\u5b9e\u73b0\uff0c\u5b83\u5728\u4e00\u79cd\u79f0\u4e3a\u201csheaves\u201d\u7684\u6570\u5b66\u7ed3\u6784\u4e2d\uff0c\u4f7f\u7528 j-do-calculus \u8fdb\u884c\u5f62\u5f0f\u5316\u5b9a\u4e49\u3002\u8be5\u65b9\u6cd5\u5c06\u56e0\u679c\u5173\u7cfb\u8868\u793a\u4e3a\u201c\u5c40\u90e8\u771f\u7406\u201d\uff0c\u5e76\u901a\u8fc7 Lawvere-Tierney \u6a21\u6001\u7b97\u5b50 j \u9009\u62e9\u76f8\u5173\u7684\u653f\u6743\u3002j-stability \u4fdd\u8bc1\u4e86\u56e0\u679c\u5173\u7cfb\u5728\u76f8\u5173\u653f\u6743\u5bb6\u65cf\u4e2d\u7684\u5efa\u8bbe\u6027\u548c\u4e00\u81f4\u6027\u3002\u7814\u7a76\u5c06 judo calculus \u4e0e\u6807\u51c6\u7684\u57fa\u4e8e\u5206\u6570\u3001\u57fa\u4e8e\u7ea6\u675f\u548c\u57fa\u4e8e\u68af\u5ea6\u7684\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\u76f8\u7ed3\u5408\uff0c\u5e76\u5f00\u53d1\u4e86\u76f8\u5e94\u7684\u7b97\u6cd5\u548c\u5b9e\u73b0\u6846\u67b6\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u548c\u6765\u81ea\u751f\u7269\u5b66\u3001\u7ecf\u6d4e\u5b66\u7684\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\u3002\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8e\u5c42\u8bba\u7684\u53bb\u4e2d\u5fc3\u5316\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\u5728\u8ba1\u7b97\u6548\u7387\u4e0a\u6709\u6240\u63d0\u9ad8\uff0c\u5e76\u4e14\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u7ecf\u5178\u7684\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\u3002", "conclusion": "judo calculus \u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u8ba1\u7b97\u6548\u7387\u9ad8\u7684\u65b9\u5f0f\u6765\u5904\u7406\u56e0\u679c\u53d1\u73b0\u4e2d\u7684\u60c5\u5883\u4f9d\u8d56\u6027\uff0c\u5e76\u5728\u5404\u79cd\u5e94\u7528\u9886\u57df\u8868\u73b0\u51fa\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u7684\u6027\u80fd\u3002"}}
{"id": "2510.23949", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23949", "abs": "https://arxiv.org/abs/2510.23949", "authors": ["Kyomin Hwang", "Hyeonjin Kim", "Seungyeon Kim", "Sunghyun Wee", "Nojun Kwak"], "title": "Uncovering the Potential Risks in Unlearning: Danger of English-only Unlearning in Multilingual LLMs", "comment": null, "summary": "There have been a couple of studies showing that attempting to erase\nmultilingual knowledge using only English data is insufficient for multilingual\nLLMs. However, their analyses remain highly performance-oriented. In this\npaper, we switch the point of view to evaluation, and address an additional\nblind spot which reveals itself when the multilingual LLM is fully finetuned\nwith parallel multilingual dataset before unlearning. Here, language confusion\noccurs whereby a model responds in language different from that of the input\nprompt. Language confusion is a problematic phenomenon in unlearning, causing\nthe standard reference-based metrics to fail. We tackle this phenomenon in\nthree steps: (1) introduce N-gram-based Language-Mix (N-Mix) score to\nquantitatively show the language confusion is pervasive and consistent in\nmultilingual LLMs, (2) demonstrate that reference-based metrics result in false\nnegatives when N-Mix score is high, and(3) suggest the need of new type of\nunlearning evaluation that can directly assess the content of the generated\nsentences. We call this type of metrics as semantic-based metric.", "AI": {"tldr": "\u82f1\u6587\u6570\u636e\u4e0d\u8db3\u4ee5\u64e6\u9664\u591a\u8bed\u8a00LLM\u7684\u591a\u8bed\u8a00\u77e5\u8bc6\uff0c\u4e14\u5728\u5b8c\u5168\u5fae\u8c03\u540e\u8fdb\u884c\u64e6\u9664\u4f1a\u5bfc\u81f4\u8bed\u8a00\u6df7\u6dc6\uff0c\u73b0\u6709\u6307\u6807\u5931\u6548\uff0c\u9700\u8981\u65b0\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u8868\u660e\u4ec5\u7528\u82f1\u6587\u6570\u636e\u64e6\u9664\u591a\u8bed\u8a00LLM\u7684\u591a\u8bed\u8a00\u77e5\u8bc6\u662f\u4e0d\u591f\u7684\uff0c\u4f46\u8fd9\u4e9b\u7814\u7a76\u8fc7\u4e8e\u5173\u6ce8\u6a21\u578b\u6027\u80fd\u3002\u672c\u6587\u4ece\u8bc4\u4f30\u89d2\u5ea6\u5207\u5165\uff0c\u5e76\u6307\u51fa\u5728\u591a\u8bed\u8a00LLM\u5b8c\u5168\u5fae\u8c03\u540e\u518d\u8fdb\u884c\u64e6\u9664\u4f1a\u4ea7\u751f\u8bed\u8a00\u6df7\u6dc6\u95ee\u9898\u3002", "method": "1. \u63d0\u51fa\u57fa\u4e8eN-gram\u7684\u8bed\u8a00\u6df7\u5408\uff08N-Mix\uff09\u5f97\u5206\uff0c\u91cf\u5316\u8bed\u8a00\u6df7\u6dc6\u7684\u666e\u904d\u6027\u548c\u4e00\u81f4\u6027\u30022. \u8bc1\u660e\u5f53N-Mix\u5f97\u5206\u9ad8\u65f6\uff0c\u57fa\u4e8e\u53c2\u8003\u7684\u6307\u6807\u4f1a\u5bfc\u81f4\u5047\u9634\u6027\u30023. \u63d0\u51fa\u9700\u8981\u4e00\u79cd\u65b0\u7684\u64e6\u9664\u8bc4\u4f30\u65b9\u6cd5\uff0c\u80fd\u591f\u76f4\u63a5\u8bc4\u4f30\u751f\u6210\u53e5\u5b50\u7684\u5185\u5bb9\uff0c\u5373\u8bed\u4e49\u578b\u6307\u6807\u3002", "result": "\u8bed\u8a00\u6df7\u6dc6\u73b0\u8c61\u666e\u904d\u5b58\u5728\u4e8e\u591a\u8bed\u8a00LLM\u4e2d\uff0c\u5e76\u5bfc\u81f4\u73b0\u6709\u57fa\u4e8e\u53c2\u8003\u7684\u8bc4\u4f30\u6307\u6807\u5931\u6548\u3002", "conclusion": "\u73b0\u6709\u7684\u57fa\u4e8e\u53c2\u8003\u7684\u8bc4\u4f30\u65b9\u6cd5\u5728\u5904\u7406\u8bed\u8a00\u6df7\u6dc6\u65f6\u5b58\u5728\u7f3a\u9677\uff0c\u9700\u8981\u5f00\u53d1\u65b0\u7684\u8bed\u4e49\u578b\u8bc4\u4f30\u6307\u6807\u6765\u66f4\u51c6\u786e\u5730\u8bc4\u4f30\u64e6\u9664\u6548\u679c\u3002"}}
{"id": "2510.24109", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24109", "abs": "https://arxiv.org/abs/2510.24109", "authors": ["Wenbin Ding", "Jun Chen", "Mingjia Chen", "Fei Xie", "Qi Mao", "Philip Dames"], "title": "PFEA: An LLM-based High-Level Natural Language Planning and Feedback Embodied Agent for Human-Centered AI", "comment": null, "summary": "The rapid advancement of Large Language Models (LLMs) has marked a\nsignificant breakthrough in Artificial Intelligence (AI), ushering in a new era\nof Human-centered Artificial Intelligence (HAI). HAI aims to better serve human\nwelfare and needs, thereby placing higher demands on the intelligence level of\nrobots, particularly in aspects such as natural language interaction, complex\ntask planning, and execution. Intelligent agents powered by LLMs have opened up\nnew pathways for realizing HAI. However, existing LLM-based embodied agents\noften lack the ability to plan and execute complex natural language control\ntasks online. This paper explores the implementation of intelligent robotic\nmanipulating agents based on Vision-Language Models (VLMs) in the physical\nworld. We propose a novel embodied agent framework for robots, which comprises\na human-robot voice interaction module, a vision-language agent module and an\naction execution module. The vision-language agent itself includes a\nvision-based task planner, a natural language instruction converter, and a task\nperformance feedback evaluator. Experimental results demonstrate that our agent\nachieves a 28\\% higher average task success rate in both simulated and real\nenvironments compared to approaches relying solely on LLM+CLIP, significantly\nimproving the execution success rate of high-level natural language instruction\ntasks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u673a\u5668\u4eba\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7528\u4e8e\u589e\u5f3a\u673a\u5668\u4eba\u5728\u7269\u7406\u4e16\u754c\u4e2d\u6267\u884c\u590d\u6742\u81ea\u7136\u8bed\u8a00\u63a7\u5236\u4efb\u52a1\u7684\u80fd\u529b\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5176\u4efb\u52a1\u6210\u529f\u7387\u663e\u8457\u4f18\u4e8e\u4ec5\u4f9d\u8d56LLM+CLIP\u7684\u65b9\u6cd5\u3002", "motivation": "\u4e3a\u4e86\u5b9e\u73b0\u4ee5\u4eba\u4e3a\u672c\u7684\u4eba\u5de5\u667a\u80fd\uff08HAI\uff09\uff0c\u7279\u522b\u662f\u63d0\u5347\u673a\u5668\u4eba\u5728\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u3001\u590d\u6742\u4efb\u52a1\u89c4\u5212\u548c\u6267\u884c\u65b9\u9762\u7684\u667a\u80fd\u6c34\u5e73\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5904\u7406\u590d\u6742\u81ea\u7136\u8bed\u8a00\u63a7\u5236\u4efb\u52a1\u7684\u667a\u80fd\u4f53\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5305\u542b\u8bed\u97f3\u4ea4\u4e92\u3001\u89c6\u89c9\u8bed\u8a00\u667a\u80fd\u4f53\u548c\u52a8\u4f5c\u6267\u884c\u7684\u673a\u5668\u4eba\u667a\u80fd\u4f53\u6846\u67b6\u3002\u8be5\u89c6\u89c9\u8bed\u8a00\u667a\u80fd\u4f53\u8fdb\u4e00\u6b65\u5305\u542b\u57fa\u4e8e\u89c6\u89c9\u7684\u4efb\u52a1\u89c4\u5212\u5668\u3001\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u8f6c\u6362\u5668\u548c\u4efb\u52a1\u6267\u884c\u53cd\u9988\u8bc4\u4f30\u5668\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u73af\u5883\u4e2d\uff0c\u6240\u63d0\u51fa\u7684\u667a\u80fd\u4f53\u6bd4\u4ec5\u4f9d\u8d56LLM+CLIP\u7684\u65b9\u6cd5\u7684\u5e73\u5747\u4efb\u52a1\u6210\u529f\u7387\u9ad8\u51fa28%\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u9ad8\u7ea7\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u4efb\u52a1\u7684\u6267\u884c\u6210\u529f\u7387\u3002", "conclusion": "\u57fa\u4e8eVLM\u7684\u673a\u5668\u4eba\u667a\u80fd\u4f53\u6846\u67b6\u80fd\u591f\u6709\u6548\u63d0\u5347\u673a\u5668\u4eba\u5728\u7269\u7406\u4e16\u754c\u4e2d\u6267\u884c\u590d\u6742\u81ea\u7136\u8bed\u8a00\u63a7\u5236\u4efb\u52a1\u7684\u80fd\u529b\u3002"}}
{"id": "2510.24445", "categories": ["cond-mat.mtrl-sci", "physics.ins-det"], "pdf": "https://arxiv.org/pdf/2510.24445", "abs": "https://arxiv.org/abs/2510.24445", "authors": ["Hitoshi Makino", "Bernd Rellinghaus", "Sebastian Schneider", "Axel Lubk", "Darius Pohl"], "title": "Noise Estimation and Suppression in Quantitative EMCD Measurements", "comment": null, "summary": "Quantitative electron magnetic circular dichroism (EMCD) in transmission\nelectron microscopy (TEM) enables the measurement of magnetic moments with\nelemental and atomic site sensitivity, but its practical application is\nfundamentally limited by noise. This study presents a comprehensive methodology\nfor noise estimation and suppression in EMCD measurements, demonstrated on\nTi-doped barium hexaferrite lamellae. By employing a classical three-beam\ngeometry and long-term acquisition of electron energy-loss spectra, we\nsystematically analyze the signal-to-noise ratio (SNR) across individual energy\nchannels using bootstrap statistics. A robust energy alignment procedure based\non the neighboring Ba-M4,5 edges with an adequate energy upsampling is\nintroduced to minimize systematic errors from energy misalignment. The impact\nof detector noise, particularly from CMOS-based EELS cameras, is evaluated\nthrough variance-to-mean analysis and described by the noise amplification\ncoefficients, revealing that detector-amplified shot noise is the dominant\nnoise source. We recommend a stricter SNR threshold for reliable EMCD detection\nand quantification, ensuring that critical spectral features such as the\nFe-L2,3 peaks meet the requirements for quantitative analysis. The approach\nalso provides a framework for determining the minimum electron dose necessary\nfor valid measurements and can be generalized to scintillator-based or direct\nelectron detectors. This work advances the reliability of EMCD as a\nquantitative tool for magnetic characterization at the nanoscale with unknown\nmagnetic structures. The proposed procedures lay the groundwork for improved\nerror handling and SNR optimization in future EMCD studies.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u900f\u5c04\u7535\u5b50\u663e\u5fae\u955c\uff08TEM\uff09\u4e2d\u8fdb\u884c\u5b9a\u91cf\u7535\u5b50\u78c1\u5706\u4e8c\u8272\u6027\uff08EMCD\uff09\u6d4b\u91cf\u7684\u964d\u566a\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u78c1\u77e9\u6d4b\u91cf\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u5b9a\u91cfEMCD\u5728TEM\u4e2d\u53ef\u4ee5\u5b9e\u73b0\u5bf9\u5143\u7d20\u548c\u539f\u5b50\u4f4d\u70b9\u7684\u78c1\u77e9\u8fdb\u884c\u9ad8\u7075\u654f\u5ea6\u6d4b\u91cf\uff0c\u4f46\u5b9e\u9645\u5e94\u7528\u53d7\u5230\u566a\u58f0\u7684\u4e25\u91cd\u9650\u5236\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u9ad8EMCD\u6d4b\u91cf\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528\u7ecf\u5178\u7684\u201c\u4e09\u675f\u201d\u51e0\u4f55\u6784\u578b\u548c\u957f\u65f6\u95f4\u91c7\u96c6\u7535\u5b50\u80fd\u91cf\u635f\u5931\u8c31\uff08EELS\uff09\u7684\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7bootstrap\u7edf\u8ba1\u5206\u6790\u4e86\u4e0d\u540c\u80fd\u91cf\u901a\u9053\u7684\u4fe1\u566a\u6bd4\uff08SNR\uff09\u3002\u540c\u65f6\uff0c\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u90bb\u8fd1Ba-M4,5\u8fb9\u7f18\u7684\u80fd\u91cf\u5bf9\u9f50\u65b9\u6cd5\uff0c\u5e76\u7ed3\u5408\u80fd\u91cf\u4e0a\u91c7\u6837\u6765\u51cf\u5c0f\u7cfb\u7edf\u8bef\u5dee\u3002\u901a\u8fc7\u65b9\u5dee-\u5747\u503c\u5206\u6790\u8bc4\u4f30\u4e86\u63a2\u6d4b\u5668\u566a\u58f0\uff08\u7279\u522b\u662fCMOS\u76f8\u673a\u7684\u566a\u58f0\uff09\u7684\u5f71\u54cd\uff0c\u5e76\u786e\u5b9a\u4e86\u566a\u58f0\u653e\u5927\u7cfb\u6570\uff0c\u6307\u51fa\u63a2\u6d4b\u5668\u653e\u5927\u7684\u6563\u5f39\u566a\u58f0\u662f\u4e3b\u8981\u7684\u566a\u58f0\u6e90\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u63a2\u6d4b\u5668\u566a\u58f0\uff0c\u7279\u522b\u662fCMOS\u76f8\u673a\u7684\u6563\u5f39\u566a\u58f0\u662f\u4e3b\u8981\u7684\u566a\u58f0\u6e90\u3002\u901a\u8fc7\u91c7\u7528\u66f4\u4e25\u683c\u7684SNR\u9608\u503c\uff0c\u53ef\u4ee5\u786e\u4fddFe-L2,3\u5cf0\u7b49\u5173\u952e\u5149\u8c31\u7279\u5f81\u6ee1\u8db3\u5b9a\u91cf\u5206\u6790\u7684\u8981\u6c42\u3002\u8be5\u65b9\u6cd5\u8fd8\u53ef\u7528\u4e8e\u786e\u5b9a\u6709\u6548\u6d4b\u91cf\u7684\u6700\u4f4e\u7535\u5b50\u5242\u91cf\uff0c\u5e76\u9002\u7528\u4e8e\u4e0d\u540c\u7c7b\u578b\u7684\u7535\u5b50\u63a2\u6d4b\u5668\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684EMCD\u964d\u566a\u65b9\u6cd5\u63d0\u9ad8\u4e86\u5b9a\u91cf\u6d4b\u91cf\u7684\u53ef\u9760\u6027\uff0c\u4e3a\u672a\u6765EMCD\u7814\u7a76\u7684\u8bef\u5dee\u5904\u7406\u548cSNR\u4f18\u5316\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u6709\u671b\u5728\u672a\u77e5\u78c1\u7ed3\u6784\u6837\u54c1\u7684\u7eb3\u7c73\u5c3a\u5ea6\u78c1\u6027\u8868\u5f81\u4e2d\u5f97\u5230\u5e7f\u6cdb\u5e94\u7528\u3002"}}
{"id": "2510.24685", "categories": ["cond-mat.mtrl-sci", "cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2510.24685", "abs": "https://arxiv.org/abs/2510.24685", "authors": ["Rafael Franco Ribeiro Reis", "Gabriel Elyas Gama Araujo", "Danilo Kuritza", "Alexandre Cavalheiro Dias", "Andreia Luisa da Rosa", "Renato Borges Pontes"], "title": "Flat bands in ultra-wide gap two-dimensional germanium dioxide", "comment": null, "summary": "We employ first principles density-functional theory (DFT) and the\nBethe-Salpeter equation (BSE) in the framework of tight-binding based maximally\nlocalized Wannier functions (MLWF-TB) model to investigate the electronic and\noptical properties of free-standing two-dimensional (2D) germanium dioxide\nphases. All investigated 2D GeO2 polymorphs exhibit ultra-wide band gaps and\nstrong excitonic effects, with flat O-p-derived valence bands tunable under\nstrain. These features allow the design of flat band materials with ultra large\nelectronic gaps in low-dimensional systems, making these materials promising\nfor devices operation at higher voltages and temperatures than conventional\nsemiconductor materials.", "AI": {"tldr": "\u4e8c\u7ef4GeO2\u5177\u6709\u8d85\u5bbd\u5e26\u9699\u548c\u5f3a\u6fc0\u5b50\u6548\u5e94\uff0c\u9002\u7528\u4e8e\u9ad8\u6e29\u9ad8\u538b\u5668\u4ef6\u3002", "motivation": "\u7814\u7a76\u81ea\u7531\u6001\u4e8c\u7ef4\uff082D\uff09\u4e8c\u6c27\u5316\u9517\uff08GeO2\uff09\u76f8\u7684\u7535\u5b50\u548c\u5149\u5b66\u6027\u8d28\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u7d27\u675f\u7f1a\u7684\u6781\u5927\u5c40\u57df\u5316\u74e6\u5c3c\u5c14\u51fd\u6570\uff08MLWF-TB\uff09\u6a21\u578b\u7684\u4ece\u5934\u7b97\u5bc6\u5ea6\u6cdb\u51fd\u7406\u8bba\uff08DFT\uff09\u548cBethe-Salpeter\u65b9\u7a0b\uff08BSE\uff09\u3002", "result": "\u6240\u6709\u7814\u7a76\u7684\u4e8c\u7ef4GeO2\u591a\u6676\u578b\u7269\u90fd\u8868\u73b0\u51fa\u8d85\u5bbd\u5e26\u9699\u548c\u5f3a\u70c8\u7684\u6fc0\u5b50\u6548\u5e94\uff0c\u5e76\u4e14\u5e73\u5766\u7684O-p\u5bfc\u51fa\u7684\u4ef7\u5e26\u53ef\u4ee5\u5728\u5e94\u53d8\u4e0b\u8c03\u8282\u3002", "conclusion": "\u8fd9\u4e9b\u7279\u6027\u4f7f\u5f97\u8bbe\u8ba1\u5177\u6709\u8d85\u5927\u7535\u5b50\u5e26\u9699\u7684\u5e73\u5e26\u6750\u6599\u6210\u4e3a\u53ef\u80fd\uff0c\u8fd9\u4e9b\u6750\u6599\u5728\u6bd4\u4f20\u7edf\u534a\u5bfc\u4f53\u6750\u6599\u66f4\u9ad8\u7684\u7535\u538b\u548c\u6e29\u5ea6\u4e0b\u8fd0\u884c\u7684\u5668\u4ef6\u4e2d\u5177\u6709\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2510.23981", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23981", "abs": "https://arxiv.org/abs/2510.23981", "authors": ["Jiaqi Yan", "Ruilong Ren", "Jingren Liu", "Shuning Xu", "Ling Wang", "Yiheng Wang", "Yun Wang", "Long Zhang", "Xiangyu Chen", "Changzhi Sun", "Jixiang Luo", "Dell Zhang", "Hao Sun", "Chi Zhang", "Xuelong Li"], "title": "TeleEgo: Benchmarking Egocentric AI Assistants in the Wild", "comment": null, "summary": "Egocentric AI assistants in real-world settings must process multi-modal\ninputs (video, audio, text), respond in real time, and retain evolving\nlong-term memory. However, existing benchmarks typically evaluate these\nabilities in isolation, lack realistic streaming scenarios, or support only\nshort-term tasks. We introduce \\textbf{TeleEgo}, a long-duration, streaming,\nomni-modal benchmark for evaluating egocentric AI assistants in realistic daily\ncontexts. The dataset features over 14 hours per participant of synchronized\negocentric video, audio, and text across four domains: work \\& study, lifestyle\n\\& routines, social activities, and outings \\& culture. All data is aligned on\na unified global timeline and includes high-quality visual narrations and\nspeech transcripts, curated through human refinement.TeleEgo defines 12\ndiagnostic subtasks across three core capabilities: Memory (recalling past\nevents), Understanding (interpreting the current moment), and Cross-Memory\nReasoning (linking distant events). It contains 3,291 human-verified QA items\nspanning multiple question formats (single-choice, binary, multi-choice, and\nopen-ended), evaluated strictly in a streaming setting. We propose two key\nmetrics -- Real-Time Accuracy and Memory Persistence Time -- to jointly assess\ncorrectness, temporal responsiveness, and long-term retention. TeleEgo provides\na realistic and comprehensive evaluation to advance the development of\npractical AI assistants.", "AI": {"tldr": "TeleEgo\u662f\u4e00\u4e2a\u957f\u65f6\u3001\u6d41\u5f0f\u3001\u5168\u6a21\u6001\u7684\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u771f\u5b9e\u65e5\u5e38\u573a\u666f\u4e2d\u7684\u5355\u76eeAI\u52a9\u624b\uff0c\u5305\u542b14\u5c0f\u65f6\u4ee5\u4e0a\u7684\u6570\u636e\uff0c\u5e76\u5b9a\u4e49\u4e8612\u4e2a\u8de8\u8bb0\u5fc6\u63a8\u7406\u3001\u7406\u89e3\u548c\u8bb0\u5fc6\u56de\u5fc6\u7684\u5b50\u4efb\u52a1\uff0c\u4ee5\u53ca\u65b0\u7684\u8bc4\u4f30\u6307\u6807\u3002", "motivation": "\u73b0\u6709\u7684\u5355\u76eeAI\u52a9\u624b\u57fa\u51c6\u8bc4\u4f30\u80fd\u529b\u5b64\u7acb\uff0c\u7f3a\u4e4f\u771f\u5b9e\u7684\u6d41\u5f0f\u573a\u666f\uff0c\u6216\u4ec5\u652f\u6301\u77ed\u671f\u4efb\u52a1\uff0c\u65e0\u6cd5\u6ee1\u8db3\u73b0\u5b9e\u4e16\u754c\u591a\u6a21\u6001\u8f93\u5165\u3001\u5b9e\u65f6\u54cd\u5e94\u548c\u957f\u671f\u8bb0\u5fc6\u7684\u9700\u6c42\u3002", "method": "TeleEgo\u5305\u542b\u6bcf\u4f4d\u53c2\u4e0e\u8005\u8d85\u8fc714\u5c0f\u65f6\u7684\u540c\u6b65\u5355\u76ee\u89c6\u9891\u3001\u97f3\u9891\u548c\u6587\u672c\u6570\u636e\uff0c\u6db5\u76d6\u5de5\u4f5c\u5b66\u4e60\u3001\u751f\u6d3b\u4e60\u60ef\u3001\u793e\u4ea4\u6d3b\u52a8\u548c\u5916\u51fa\u6587\u5316\u56db\u4e2a\u9886\u57df\u3002\u6570\u636e\u5bf9\u9f50\u4e8e\u7edf\u4e00\u7684\u5168\u5c40\u65f6\u95f4\u7ebf\uff0c\u5e76\u5305\u542b\u9ad8\u8d28\u91cf\u7684\u89c6\u89c9\u53d9\u8ff0\u548c\u8bed\u97f3\u8f6c\u5f55\u3002\u5b83\u5b9a\u4e49\u4e8612\u4e2a\u8bca\u65ad\u5b50\u4efb\u52a1\uff0c\u8de8\u8d8a\u8bb0\u5fc6\u56de\u5fc6\u3001\u77ac\u95f4\u7406\u89e3\u548c\u8fdc\u8ddd\u79bb\u4e8b\u4ef6\u5173\u8054\u4e09\u4e2a\u6838\u5fc3\u80fd\u529b\uff0c\u5305\u542b3,291\u4e2a\u7ecf\u4eba\u7c7b\u9a8c\u8bc1\u7684\u95ee\u7b54\u9879\uff0c\u5e76\u5728\u4e25\u683c\u7684\u6d41\u5f0f\u8bbe\u7f6e\u4e0b\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "TeleEgo\u5305\u542b\u8de8\u8d8a\u56db\u4e2a\u9886\u57df\u3001\u603b\u8ba1\u8d85\u8fc714\u5c0f\u65f6\u7684\u540c\u6b65\u5355\u76ee\u89c6\u9891\u3001\u97f3\u9891\u548c\u6587\u672c\u6570\u636e\uff0c\u4ee5\u53ca3,291\u4e2a\u4eba\u7c7b\u9a8c\u8bc1\u7684\u95ee\u7b54\u9879\u3002\u5b83\u5b9a\u4e49\u4e8612\u4e2a\u5b50\u4efb\u52a1\uff0c\u4ee5\u8bc4\u4f30\u8bb0\u5fc6\u56de\u5fc6\u3001\u77ac\u95f4\u7406\u89e3\u548c\u8de8\u8bb0\u5fc6\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u5f15\u5165\u4e86\u5b9e\u65f6\u51c6\u786e\u6027\u548c\u8bb0\u5fc6\u6301\u4e45\u6027\u65f6\u95f4\u4e24\u4e2a\u65b0\u6307\u6807\u3002", "conclusion": "TeleEgo\u63d0\u4f9b\u4e86\u4e00\u4e2a\u771f\u5b9e\u4e14\u5168\u9762\u7684\u8bc4\u4f30\u57fa\u51c6\uff0c\u901a\u8fc7\u5176\u957f\u65f6\u3001\u6d41\u5f0f\u3001\u5168\u6a21\u6001\u7684\u6570\u636e\u548c\u591a\u6837\u7684\u5b50\u4efb\u52a1\u53ca\u8bc4\u4f30\u6307\u6807\uff0c\u63a8\u52a8\u4e86\u5b9e\u7528AI\u52a9\u624b\u7684\u5f00\u53d1\u3002"}}
{"id": "2510.24050", "categories": ["quant-ph", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2510.24050", "abs": "https://arxiv.org/abs/2510.24050", "authors": ["Connor van Rossum", "Sally Shrapnel", "Riddhi Gupta"], "title": "Exploiting biased noise in variational quantum models", "comment": "17 pages, 7 figures", "summary": "Variational quantum algorithms (VQAs) are promising tools for demonstrating\nquantum utility on near-term quantum hardware, with applications in\noptimisation, quantum simulation, and machine learning. While researchers have\nstudied how easy VQAs are to train, the effect of quantum noise on the\nclassical optimisation process is still not well understood. Contrary to\nexpectations, we find that twirling, which is commonly used in standard\nerror-mitigation strategies to symmetrise noise, actually degrades performance\nin the variational setting, whereas preserving biased or non-unital noise can\nhelp classical optimisers find better solutions. Analytically, we study a\nuniversal quantum regression model and demonstrate that relatively uniform\nPauli channels suppress gradient magnitudes and reduce expressivity, making\noptimisation more difficult. Conversely, asymmetric noise such as amplitude\ndamping or biased Pauli channels introduces directional bias that can be\nexploited during optimisation. Numerical experiments on a variational\neigensolver for the transverse-field Ising model confirm that non-unital noise\nyields lower-energy states compared to twirled noise. Finally, we show that\ncoherent errors are fully mitigated by re-parameterisation. These findings\nchallenge conventional noise-mitigation strategies and suggest that preserving\nnoise biases may enhance VQA performance.", "AI": {"tldr": "\u91cf\u5b50\u566a\u58f0\u5b9e\u9645\u4e0a\u53ef\u4ee5\u63d0\u9ad8\u53d8\u5206\u91cf\u5b50\u7b97\u6cd5\u7684\u6027\u80fd\uff0c\u8fd9\u4e0e\u901a\u5e38\u7684\u964d\u566a\u7b56\u7565\u76f8\u53cd\u3002", "motivation": "\u7814\u7a76\u91cf\u5b50\u566a\u58f0\u5bf9\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u53d8\u5206\u91cf\u5b50\u7b97\u6cd5\u7684\u7ecf\u5178\u4f18\u5316\u8fc7\u7a0b\u7684\u5f71\u54cd\u3002", "method": "\u5206\u6790\u4e86\u5177\u6709\u4ee3\u8868\u6027\u7684\u91cf\u5b50\u901a\u9053\uff08\u4f8b\u5982\uff0c\u7edf\u4e00\u7684 Pauli \u901a\u9053\u3001\u5e45\u5ea6\u963b\u5c3c\u548c\u6709\u504f\u7684 Pauli \u901a\u9053\uff09\u5982\u4f55\u5f71\u54cd\u68af\u5ea6\u7684\u5927\u5c0f\u548c\u8868\u8fbe\u80fd\u529b\u3002\u901a\u8fc7\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8fd9\u4e9b\u53d1\u73b0\uff0c\u5e76\u7814\u7a76\u4e86\u91cd\u53c2\u6570\u5316\u5bf9\u76f8\u5e72\u9519\u8bef\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u566a\u58f0\uff08\u7279\u522b\u662f\u504f\u7f6e\u566a\u58f0\uff09\u53ef\u4ee5\u5e2e\u52a9\u7ecf\u5178\u4f18\u5316\u5668\u627e\u5230\u66f4\u597d\u7684\u89e3\u51b3\u65b9\u6848\u3002\u7edf\u4e00\u7684 Pauli \u901a\u9053\u4f1a\u6291\u5236\u68af\u5ea6\u7684\u5927\u5c0f\u5e76\u964d\u4f4e\u8868\u8fbe\u80fd\u529b\uff0c\u4ece\u800c\u4f7f\u4f18\u5316\u66f4\u52a0\u56f0\u96be\u3002\u5e45\u5ea6\u963b\u5c3c\u6216\u6709\u504f\u7684 Pauli \u901a\u9053\u4f1a\u5f15\u5165\u53ef\u5229\u7528\u7684\u5b9a\u5411\u504f\u5dee\u3002\u91cd\u53c2\u6570\u5316\u53ef\u4ee5\u5b8c\u5168\u7f13\u89e3\u76f8\u5e72\u9519\u8bef\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5e94\u91cd\u65b0\u8003\u8651\u6807\u51c6\u7684\u566a\u58f0\u7f13\u89e3\u7b56\u7565\uff0c\u5e76\u4e14\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0c\u4fdd\u7559\u566a\u58f0\u504f\u5dee\u53ef\u80fd\u6bd4\u4f7f\u7528\u4f20\u7edf\u65b9\u6cd5\uff08\u4f8b\u5982\uff0c\u626d\u66f2\uff09\u53ef\u4ee5\u66f4\u597d\u5730\u63d0\u9ad8\u53d8\u5206\u91cf\u5b50\u7b97\u6cd5\u7684\u6027\u80fd\u3002"}}
{"id": "2510.24391", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.24391", "abs": "https://arxiv.org/abs/2510.24391", "authors": ["Alvaro Detailleur", "Dalim Wahby", "Guillaume Ducard", "Christopher Onder"], "title": "Contributions to Semialgebraic-Set-Based Stability Verification of Dynamical Systems with Neural-Network-Based Controllers", "comment": "Submitted to the IEEE for possible publication, 16 pages, 6 figures", "summary": "Neural-network-based controllers (NNCs) can represent complex, highly\nnonlinear control laws, but verifying the closed-loop stability of dynamical\nsystems using them remains challenging. This work presents contributions to a\nstate-of-the-art stability verification procedure for NNC-controlled systems\nwhich relies on semialgebraic-set-based input-output modeling to pose the\nsearch for a Lyapunov function as an optimization problem. Specifically, this\nprocedure's conservatism when analyzing NNCs using transcendental activation\nfunctions and the restriction to feedforward NNCs are addressed by a)\nintroducing novel semialgebraic activation functions that preserve key\nproperties of common transcendental activations and b) proving compatibility of\nNNCs from the broader class of recurrent equilibrium networks (RENs) with this\nprocedure. Furthermore, the indirect optimization of a local region of\nattraction (RoA) estimate using a restricted set of candidate Lyapunov\nfunctions is greatly improved via c) the introduction of a richer\nparameterization of candidate Lyapunov functions than previously reported and\nd) the formulation of novel semidefinite programs (SDPs) that directly optimize\nthe resulting RoA estimate. The value of these contributions is highlighted in\ntwo numerical examples.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u9a8c\u8bc1\u795e\u7ecf\u7f51\u200b\u200b\u7edc\u63a7\u5236\u5668\uff08NNC\uff09\u63a7\u5236\u7cfb\u7edf\u7684\u7a33\u5b9a\u6027\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u65b0\u7684\u534a\u4ee3\u6570\u6fc0\u6d3b\u51fd\u6570\u548c\u652f\u6301\u5faa\u73af\u5747\u8861\u7f51\u7edc\uff08RENs\uff09\uff0c\u5e76\u6539\u8fdb\u4e86\u5438\u5f15\u57df\uff08RoA\uff09\u4f30\u8ba1\u7684\u4f18\u5316\u3002", "motivation": "\u9a8c\u8bc1\u57fa\u4e8e\u795e\u7ecf\u7f51\u200b\u200b\u7edc\u63a7\u5236\u5668\uff08NNC\uff09\u7684\u95ed\u73af\u7a33\u5b9a\u6027\u662f\u4e00\u4e2a\u6311\u6218\uff0c\u5c24\u5176\u662f\u5bf9\u4e8e\u5177\u6709\u8d85\u8d8a\u6fc0\u6d3b\u51fd\u6570\u7684NNC\u4ee5\u53ca\u53ea\u652f\u6301\u524d\u9988NNC\u7684\u73b0\u6709\u65b9\u6cd5\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e24\u79cd\u6539\u8fdb\uff1a1. \u5f15\u5165\u65b0\u7684\u534a\u4ee3\u6570\u6fc0\u6d3b\u51fd\u6570\u6765\u5904\u7406\u8d85\u8d8a\u6fc0\u6d3b\u51fd\u6570\uff0c\u5e76\u8bc1\u660e\u5faa\u73af\u5747\u8861\u7f51\u7edc\uff08RENs\uff09\u4e0e\u8be5\u9a8c\u8bc1\u7a0b\u5e8f\u517c\u5bb9\u3002 2. \u901a\u8fc7\u66f4\u4e30\u5bcc\u7684\u5019\u9009\u674e\u96c5\u666e\u8bfa\u592b\u51fd\u6570\u53c2\u6570\u5316\u548c\u65b0\u7684\u534a\u5b9a\u7a0b\u5e8f\uff08SDPs\uff09\u6765\u6539\u8fdb\u5438\u5f15\u57df\uff08RoA\uff09\u4f30\u8ba1\u7684\u4f18\u5316\u3002", "result": "\u901a\u8fc7\u4e24\u4e2a\u6570\u503c\u793a\u4f8b\u7a81\u51fa\u4e86\u8fd9\u4e9b\u8d21\u732e\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u63d0\u9ad8NNC\u63a7\u5236\u7cfb\u7edf\u7684\u7a33\u5b9a\u6027\u9a8c\u8bc1\u51c6\u786e\u6027\u548c\u9002\u7528\u6027\u505a\u51fa\u4e86\u8d21\u732e\u3002"}}
{"id": "2510.24255", "categories": ["eess.SP", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24255", "abs": "https://arxiv.org/abs/2510.24255", "authors": ["Jihao Luo", "Zesong Fei", "Xinyi Wang", "Le Zhao", "Yuanhao Cui", "Guangxu Zhu", "Dusit Niyato"], "title": "Trajectory Design for UAV-Based Low-Altitude Wireless Networks in Unknown Environments: A Digital Twin-Assisted TD3 Approach", "comment": "13 pages, 11 figures", "summary": "Unmanned aerial vehicles (UAVs) are emerging as key enablers for low-altitude\nwireless network (LAWN), particularly when terrestrial networks are\nunavailable. In such scenarios, the environmental topology is typically\nunknown; hence, designing efficient and safe UAV trajectories is essential yet\nchallenging. To address this, we propose a digital twin (DT)-assisted training\nand deployment framework. In this framework, the UAV transmits integrated\nsensing and communication signals to provide communication services to ground\nusers, while simultaneously collecting echoes that are uploaded to the DT\nserver to progressively construct virtual environments (VEs). These VEs\naccelerate model training and are continuously updated with real-time UAV\nsensing data during deployment, supporting decision-making and enhancing flight\nsafety. Based on this framework, we further develop a trajectory design scheme\nthat integrates simulated annealing for efficient user scheduling with the\ntwin-delayed deep deterministic policy gradient algorithm for continuous\ntrajectory design, aiming to minimize mission completion time while ensuring\nobstacle avoidance. Simulation results demonstrate that the proposed approach\nachieves faster convergence, higher flight safety, and shorter mission\ncompletion time compared with baseline methods, providing a robust and\nefficient solution for LAWN deployment in unknown environments.", "AI": {"tldr": "\u5229\u7528\u6570\u5b57\u5b6a\u751f\uff08DT\uff09\u6280\u672f\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u4e3a\u65e0\u4eba\u673a\uff08UAV\uff09\u5728\u672a\u77e5\u73af\u5883\u4e2d\u63d0\u4f9b\u65e0\u7ebf\u7f51\u7edc\u8986\u76d6\uff08LAWN\uff09\u8bbe\u8ba1\u4e86\u65b0\u7684\u8f68\u8ff9\u89c4\u5212\u65b9\u6848\uff0c\u63d0\u9ad8\u4e86\u4efb\u52a1\u6548\u7387\u548c\u98de\u884c\u5b89\u5168\u3002", "motivation": "\u5728\u901a\u4fe1\u57fa\u7840\u8bbe\u65bd\u53d7\u635f\u6216\u7f3a\u5931\u7684\u573a\u666f\u4e0b\uff0c\u65e0\u4eba\u673a\uff08UAV\uff09\u4f5c\u4e3a\u4f4e\u7a7a\u65e0\u7ebf\u7f51\u7edc\uff08LAWN\uff09\u7684\u5173\u952e\u7ec4\u6210\u90e8\u5206\uff0c\u5176\u9ad8\u6548\u548c\u5b89\u5168\u7684\u8f68\u8ff9\u8bbe\u8ba1\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u73af\u5883\u62d3\u6251\u672a\u77e5\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6570\u5b57\u5b6a\u751f\uff08DT\uff09\u7684\u8bad\u7ec3\u548c\u90e8\u7f72\u6846\u67b6\u3002\u5728\u8be5\u6846\u67b6\u4e2d\uff0c\u65e0\u4eba\u673a\u5229\u7528\u96c6\u6210\u4f20\u611f\u548c\u901a\u4fe1\u4fe1\u53f7\u4e3a\u5730\u9762\u7528\u6237\u63d0\u4f9b\u670d\u52a1\uff0c\u540c\u65f6\u6536\u96c6\u56de\u6ce2\u6570\u636e\u7528\u4e8e\u6784\u5efa\u548c\u66f4\u65b0\u6570\u5b57\u5b6a\u751f\u4e2d\u7684\u865a\u62df\u73af\u5883\uff08VE\uff09\u3002\u7ed3\u5408\u6a21\u62df\u9000\u706b\u7b97\u6cd5\u8fdb\u884c\u7528\u6237\u8c03\u5ea6\uff0c\u5e76\u5229\u7528\u53cc\u5ef6\u8fdf\u6df1\u5ea6\u786e\u5b9a\u6027\u7b56\u7565\u68af\u5ea6\uff08TD3\uff09\u7b97\u6cd5\u8fdb\u884c\u8fde\u7eed\u8f68\u8ff9\u8bbe\u8ba1\uff0c\u4ee5\u6700\u5c0f\u5316\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u548c\u786e\u4fdd\u907f\u969c\u3002", "result": "\u4e0e\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u3001\u66f4\u9ad8\u7684\u98de\u884c\u5b89\u5168\u6027\u548c\u66f4\u77ed\u7684\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6570\u5b57\u5b6a\u751f\u8f85\u52a9\u6846\u67b6\u548c\u8f68\u8ff9\u8bbe\u8ba1\u65b9\u6848\u4e3a\u5728\u672a\u77e5\u73af\u5883\u4e2d\u90e8\u7f72\u4f4e\u7a7a\u65e0\u7ebf\u7f51\u7edc\uff08LAWN\uff09\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7a33\u5065\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.23639", "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2510.23639", "abs": "https://arxiv.org/abs/2510.23639", "authors": ["Jonathan Amar", "Edward Liu", "Alessandra Breschi", "Liangliang Zhang", "Pouya Kheradpour", "Sylvia Li", "Lisa Soleymani Lehmann", "Alessandro Giulianelli", "Matt Edwards", "Yugang Jia", "David Nola", "Raghav Mani", "Pankaj Vats", "Jesse Tetreault", "T. J. Chen", "Cory Y. McLean"], "title": "Integrating Genomics into Multimodal EHR Foundation Models", "comment": null, "summary": "This paper introduces an innovative Electronic Health Record (EHR) foundation\nmodel that integrates Polygenic Risk Scores (PRS) as a foundational data\nmodality, moving beyond traditional EHR-only approaches to build more holistic\nhealth profiles. Leveraging the extensive and diverse data from the All of Us\n(AoU) Research Program, this multimodal framework aims to learn complex\nrelationships between clinical data and genetic predispositions. The\nmethodology extends advancements in generative AI to the EHR foundation model\nspace, enhancing predictive capabilities and interpretability. Evaluation on\nAoU data demonstrates the model's predictive value for the onset of various\nconditions, particularly Type 2 Diabetes (T2D), and illustrates the interplay\nbetween PRS and EHR data. The work also explores transfer learning for custom\nclassification tasks, showcasing the architecture's versatility and efficiency.\nThis approach is pivotal for unlocking new insights into disease prediction,\nproactive health management, risk stratification, and personalized treatment\nstrategies, laying the groundwork for more personalized, equitable, and\nactionable real-world evidence generation in healthcare.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u6574\u5408\u4e86\u591a\u57fa\u56e0\u98ce\u9669\u8bc4\u5206\uff08PRS\uff09\u7684\u521b\u65b0\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u57fa\u7840\u6a21\u578b\uff0c\u4ee5\u66f4\u5168\u9762\u5730\u4e86\u89e3\u5065\u5eb7\u72b6\u51b5\u3002", "motivation": "\u4e3a\u4e86\u6784\u5efa\u66f4\u5168\u9762\u7684\u5065\u5eb7\u6863\u6848\uff0c\u8d85\u8d8a\u4f20\u7edf\u4ec5\u4f7f\u7528EHR\u6570\u636e\u7684\u5c40\u9650\u6027\uff0c\u5e76\u6574\u5408\u9057\u4f20\u6613\u611f\u6027\u4fe1\u606f\u3002", "method": "\u5229\u7528All of Us\uff08AoU\uff09\u7814\u7a76\u9879\u76ee\u7684\u6570\u636e\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u591a\u6a21\u6001\u6846\u67b6\uff0c\u5c06PRS\u4f5c\u4e3a\u57fa\u7840\u6570\u636e\u6a21\u5f0f\uff0c\u5e76\u8fd0\u7528\u4e86\u751f\u6210\u5f0fAI\u7684\u6700\u65b0\u8fdb\u5c55\u3002", "result": "\u5728AoU\u6570\u636e\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u6a21\u578b\u80fd\u591f\u9884\u6d4b\u5305\u62ec2\u578b\u7cd6\u5c3f\u75c5\uff08T2D\uff09\u5728\u5185\u7684\u591a\u79cd\u75be\u75c5\u7684\u53d1\u75c5\uff0c\u5e76\u63ed\u793a\u4e86PRS\u4e0eEHR\u6570\u636e\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u3002\u6b64\u5916\uff0c\u6a21\u578b\u5728\u81ea\u5b9a\u4e49\u5206\u7c7b\u4efb\u52a1\u4e0a\u7684\u8fc1\u79fb\u5b66\u4e60\u8868\u73b0\u4e5f\u8bc1\u660e\u4e86\u5176\u901a\u7528\u6027\u548c\u6548\u7387\u3002", "conclusion": "\u8be5\u591a\u6a21\u6001EHR\u57fa\u7840\u6a21\u578b\u5728\u75be\u75c5\u9884\u6d4b\u3001\u4e3b\u52a8\u5065\u5eb7\u7ba1\u7406\u3001\u98ce\u9669\u5206\u5c42\u548c\u4e2a\u6027\u5316\u6cbb\u7597\u65b9\u9762\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u4e3a\u751f\u6210\u66f4\u4e2a\u6027\u5316\u3001\u516c\u5e73\u548c\u53ef\u64cd\u4f5c\u7684\u533b\u7597\u4fdd\u5065\u9886\u57df\u771f\u5b9e\u4e16\u754c\u8bc1\u636e\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2510.23965", "categories": ["cs.AI", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.23965", "abs": "https://arxiv.org/abs/2510.23965", "authors": ["Aymane El Gadarri", "Ali Aouad", "Vivek F. Farias"], "title": "The Sign Estimator: LLM Alignment in the Face of Choice Heterogeneity", "comment": null, "summary": "Traditional LLM alignment methods are vulnerable to heterogeneity in human\npreferences. Fitting a na\\\"ive probabilistic model to pairwise comparison data\n(say over prompt-completion pairs) yields an inconsistent estimate of the\npopulation-average utility -a canonical measure of social welfare. We propose a\nnew method, dubbed the sign estimator, that provides a simple, provably\nconsistent, and efficient estimator by replacing cross-entropy with binary\nclassification loss in the aggregation step. This simple modification recovers\nconsistent ordinal alignment under mild assumptions and achieves the first\npolynomial finite-sample error bounds in this setting. In realistic simulations\nof LLM alignment using digital twins, the sign estimator substantially reduces\npreference distortion over a panel of simulated personas, cutting (angular)\nestimation error by nearly 35% and decreasing disagreement with true population\npreferences from 12% to 8% compared to standard RLHF. Our method also compares\nfavorably to panel data heuristics that explicitly model user heterogeneity and\nrequire tracking individual-level preference data-all while maintaining the\nimplementation simplicity of existing LLM alignment pipelines.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2510.23995", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23995", "abs": "https://arxiv.org/abs/2510.23995", "authors": ["Mengzhou Sun", "Sendong Zhao", "Jianyu Chen", "Haochun Wang", "Bin Qin"], "title": "M-Eval: A Heterogeneity-Based Framework for Multi-evidence Validation in Medical RAG Systems", "comment": null, "summary": "Retrieval-augmented Generation (RAG) has demonstrated potential in enhancing\nmedical question-answering systems through the integration of large language\nmodels (LLMs) with external medical literature. LLMs can retrieve relevant\nmedical articles to generate more professional responses efficiently. However,\ncurrent RAG applications still face problems. They generate incorrect\ninformation, such as hallucinations, and they fail to use external knowledge\ncorrectly. To solve these issues, we propose a new method named M-Eval. This\nmethod is inspired by the heterogeneity analysis approach used in\nEvidence-Based Medicine (EBM). Our approach can check for factual errors in RAG\nresponses using evidence from multiple sources. First, we extract additional\nmedical literature from external knowledge bases. Then, we retrieve the\nevidence documents generated by the RAG system. We use heterogeneity analysis\nto check whether the evidence supports different viewpoints in the response. In\naddition to verifying the accuracy of the response, we also assess the\nreliability of the evidence provided by the RAG system. Our method shows an\nimprovement of up to 23.31% accuracy across various LLMs. This work can help\ndetect errors in current RAG-based medical systems. It also makes the\napplications of LLMs more reliable and reduces diagnostic errors.", "AI": {"tldr": "M-Eval\u901a\u8fc7\u8bc1\u636e\u5f02\u6784\u6027\u5206\u6790\u89e3\u51b3RAG\u5728\u533b\u5b66\u95ee\u7b54\u4e2d\u7684\u5e7b\u89c9\u548c\u77e5\u8bc6\u4f7f\u7528\u4e0d\u5f53\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u56de\u7b54\u51c6\u786e\u6027\u3002", "motivation": "\u5f53\u524d\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u5728\u533b\u5b66\u95ee\u7b54\u4e2d\u5b58\u5728\u751f\u6210\u4e0d\u6b63\u786e\u4fe1\u606f\uff08\u5982\u5e7b\u89c9\uff09\u548c\u672a\u80fd\u6b63\u786e\u4f7f\u7528\u5916\u90e8\u77e5\u8bc6\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faM-Eval\u65b9\u6cd5\uff0c\u501f\u9274\u5faa\u8bc1\u533b\u5b66\uff08EBM\uff09\u7684\u5f02\u6784\u6027\u5206\u6790\u65b9\u6cd5\uff0c\u901a\u8fc7\u63d0\u53d6\u989d\u5916\u533b\u5b66\u6587\u732e\u3001\u68c0\u7d22RAG\u7cfb\u7edf\u751f\u6210\u7684\u8bc1\u636e\uff0c\u5e76\u5206\u6790\u8bc1\u636e\u662f\u5426\u652f\u6301\u56de\u7b54\u4e2d\u7684\u4e0d\u540c\u89c2\u70b9\uff0c\u6765\u68c0\u67e5RAG\u56de\u7b54\u7684\u4e8b\u5b9e\u9519\u8bef\u548c\u8bc4\u4f30\u8bc1\u636e\u7684\u53ef\u9760\u6027\u3002", "result": "M-Eval\u65b9\u6cd5\u5728\u591a\u79cd\u8bed\u8a00\u6a21\u578b\u4e0a\u663e\u793a\u51fa\u9ad8\u8fbe23.31%\u7684\u51c6\u786e\u6027\u63d0\u5347\u3002", "conclusion": "M-Eval\u6709\u52a9\u4e8e\u68c0\u6d4b\u5f53\u524d\u57fa\u4e8eRAG\u7684\u533b\u5b66\u7cfb\u7edf\u4e2d\u7684\u9519\u8bef\uff0c\u63d0\u9ad8LLM\u5e94\u7528\u7684\u53ef\u9760\u6027\uff0c\u5e76\u51cf\u5c11\u8bca\u65ad\u9519\u8bef\u3002"}}
{"id": "2510.24118", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24118", "abs": "https://arxiv.org/abs/2510.24118", "authors": ["Haotian Zhou", "Xiaole Wang", "He Li", "Fusheng Sun", "Shengyu Guo", "Guolei Qi", "Jianghuan Xu", "Huijing Zhao"], "title": "LagMemo: Language 3D Gaussian Splatting Memory for Multi-modal Open-vocabulary Multi-goal Visual Navigation", "comment": null, "summary": "Navigating to a designated goal using visual information is a fundamental\ncapability for intelligent robots. Most classical visual navigation methods are\nrestricted to single-goal, single-modality, and closed set goal settings. To\naddress the practical demands of multi-modal, open-vocabulary goal queries and\nmulti-goal visual navigation, we propose LagMemo, a navigation system that\nleverages a language 3D Gaussian Splatting memory. During exploration, LagMemo\nconstructs a unified 3D language memory. With incoming task goals, the system\nqueries the memory, predicts candidate goal locations, and integrates a local\nperception-based verification mechanism to dynamically match and validate goals\nduring navigation. For fair and rigorous evaluation, we curate GOAT-Core, a\nhigh-quality core split distilled from GOAT-Bench tailored to multi-modal\nopen-vocabulary multi-goal visual navigation. Experimental results show that\nLagMemo's memory module enables effective multi-modal open-vocabulary goal\nlocalization, and that LagMemo outperforms state-of-the-art methods in\nmulti-goal visual navigation. Project page:\nhttps://weekgoodday.github.io/lagmemo", "AI": {"tldr": "LagMemo\u662f\u4e00\u4e2a\u5229\u7528\u8bed\u8a003D\u9ad8\u65af\u6cfc\u6e85\u8bb0\u5fc6\u8fdb\u884c\u591a\u6a21\u6001\u3001\u5f00\u653e\u8bcd\u6c47\u3001\u591a\u76ee\u6807\u89c6\u89c9\u5bfc\u822a\u7684\u7cfb\u7edf\uff0c\u5728GOAT-Core\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u89c6\u89c9\u5bfc\u822a\u65b9\u6cd5\u5728\u591a\u6a21\u6001\u3001\u5f00\u653e\u8bcd\u6c47\u76ee\u6807\u67e5\u8be2\u548c\u591a\u76ee\u6807\u5bfc\u822a\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "method": "LagMemo\u5728\u63a2\u7d22\u8fc7\u7a0b\u4e2d\u6784\u5efa\u7edf\u4e00\u76843D\u8bed\u8a00\u8bb0\u5fc6\uff0c\u901a\u8fc7\u67e5\u8be2\u8bb0\u5fc6\u6765\u9884\u6d4b\u5019\u9009\u76ee\u6807\u4f4d\u7f6e\uff0c\u5e76\u7ed3\u5408\u57fa\u4e8e\u5c40\u90e8\u611f\u77e5\u7684\u9a8c\u8bc1\u673a\u5236\u6765\u52a8\u6001\u5339\u914d\u548c\u9a8c\u8bc1\u5bfc\u822a\u8fc7\u7a0b\u4e2d\u7684\u76ee\u6807\u3002", "result": "LagMemo\u7684\u8bb0\u5fc6\u6a21\u5757\u80fd\u591f\u6709\u6548\u5730\u8fdb\u884c\u591a\u6a21\u6001\u5f00\u653e\u8bcd\u6c47\u76ee\u6807\u5b9a\u4f4d\uff0c\u5e76\u4e14\u5728\u591a\u76ee\u6807\u89c6\u89c9\u5bfc\u822a\u4efb\u52a1\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "LagMemo\u5728\u591a\u6a21\u6001\u5f00\u653e\u8bcd\u6c47\u591a\u76ee\u6807\u89c6\u89c9\u5bfc\u822a\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5176\u8bb0\u5fc6\u6a21\u5757\u5728\u76ee\u6807\u5b9a\u4f4d\u548c\u5bfc\u822a\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2510.24465", "categories": ["cond-mat.mtrl-sci", "cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2510.24465", "abs": "https://arxiv.org/abs/2510.24465", "authors": ["Seung Gyo Jeong", "Minjae Kim", "Jin Young Oh", "Youngeun Ham", "In Hyeok Choi", "Seong Won Cho", "Jihyun Kim", "Huimin Jeong", "Byungmin Sohn", "Tuson Park", "Suyoun Lee", "Jong Seok Lee", "Deok-Yong Cho", "Bongjae Kim", "Woo Seok Choi"], "title": "Strain Engineering of van Hove Singularity and Coupled Itinerant Ferromagnetism in Quasi-2D Oxide Superlattices", "comment": "29 pages, 3 figures", "summary": "Engineering van Hove singularities (vHss) near the Fermi level, if feasible,\noffers a powerful route to control exotic quantum phases in electronic and\nmagnetic behaviors. However, conventional approaches, which rely primarily on\nchemical and electrical doping, focus mainly on local electrical or optical\nmeasurements, limiting their applicability to coupled functionalities. In this\nstudy, a vHs-induced insulator-metal transition coupled with a ferromagnetic\nphase transition was empirically achieved in atomically designed quasi-2D\nSrRuO3 (SRO) superlattices via epitaxial strain engineering, which has not been\nobserved in conventional 3D SRO systems. Theoretical calculations revealed that\nepitaxial strain effectively modulates the strength and energy positions of vHs\nof specific Ru orbitals, driving correlated phase transitions in the electronic\nand magnetic ground states. X-ray absorption spectroscopy confirmed the\nanisotropic electronic structure of quasi-2D SRO modulated by epitaxial strain.\nMagneto-optic Kerr effect and electrical transport measurements demonstrated\nmodulated magnetic and electronic phases. Furthermore, magneto-electrical\nmeasurements detected significant anomalous Hall effect signals and\nferromagnetic magnetoresistance, indicating the presence of magnetically\ncoupled charge carriers in the 2D metallic regime. This study establishes\nstrain engineering as a promising platform for tuning vHss and resultant\nitinerant ferromagnetism of low-dimensional correlated quantum systems.", "AI": {"tldr": "\u901a\u8fc7\u5916\u5ef6\u5e94\u53d8\u5de5\u7a0b\u5728\u51c6\u4e8c\u7ef4SrRuO3\u8d85\u6676\u683c\u4e2d\u5b9e\u73b0\u4e86\u8303\u970d\u592b\u5947\u70b9\u8bf1\u5bfc\u7684\u7edd\u7f18\u4f53-\u91d1\u5c5e\u76f8\u53d8\u548c\u94c1\u78c1\u76f8\u53d8\u3002", "motivation": "\u5de5\u7a0b\u8303\u970d\u592b\u5947\u70b9\uff08vHs\uff09\u4ee5\u63a7\u5236\u7535\u5b50\u548c\u78c1\u884c\u4e3a\u4e2d\u7684\u5916\u6765\u91cf\u5b50\u76f8\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\uff08\u5316\u5b66\u548c\u7535\u6c14\u63ba\u6742\uff09\u4e3b\u8981\u4f9d\u8d56\u5c40\u90e8\u6d4b\u91cf\uff0c\u9650\u5236\u4e86\u5176\u5728\u8026\u5408\u529f\u80fd\u4e0a\u7684\u5e94\u7528\u3002", "method": "\u901a\u8fc7\u5916\u5ef6\u5e94\u53d8\u5de5\u7a0b\u5728\u539f\u5b50\u8bbe\u8ba1\u7684\u51c6\u4e8c\u7ef4SrRuO3\uff08SRO\uff09\u8d85\u6676\u683c\u4e2d\uff0c\u7ed3\u5408\u7406\u8bba\u8ba1\u7b97\u3001X\u5c04\u7ebf\u5438\u6536\u5149\u8c31\u3001\u78c1\u5149\u514b\u5c14\u6548\u5e94\u548c\u7535\u963b\u6d4b\u91cf\u3002", "result": "\u5916\u5ef6\u5e94\u53d8\u6709\u6548\u8c03\u8282\u4e86\u7279\u5b9aRu\u8f68\u9053\u7684vHs\u7684\u5f3a\u5ea6\u548c\u80fd\u91cf\u4f4d\u7f6e\uff0c\u9a71\u52a8\u4e86\u7535\u5b50\u548c\u78c1\u6027\u57fa\u6001\u7684\u5173\u8054\u76f8\u53d8\u3002\u6d4b\u91cf\u7ed3\u679c\u8bc1\u5b9e\u4e86\u5e94\u53d8\u8c03\u5236\u7684\u51c6\u4e8c\u7ef4SRO\u7684\u5404\u5411\u5f02\u6027\u7535\u5b50\u7ed3\u6784\u3001\u8c03\u5236\u7684\u78c1\u76f8\u548c\u7535\u5b50\u76f8\u3002\u6b64\u5916\uff0c\u8fd8\u63a2\u6d4b\u5230\u4e86\u663e\u8457\u7684\u970d\u5c14\u6548\u5e94\u4fe1\u53f7\u548c\u94c1\u78c1\u78c1\u963b\u3002", "conclusion": "\u5916\u5ef6\u5e94\u53d8\u5de5\u7a0b\u4e3a\u8c03\u8c10\u4f4e\u7ef4\u5173\u8054\u91cf\u5b50\u7cfb\u7edf\u7684vHs\u548c\u7531\u6b64\u4ea7\u751f\u7684\u8fc1\u5ef6\u94c1\u78c1\u6027\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u524d\u9014\u7684\u5e73\u53f0\u3002"}}
{"id": "2510.24000", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24000", "abs": "https://arxiv.org/abs/2510.24000", "authors": ["Heethanjan Kanagalingam", "Thenukan Pathmanathan", "Mokeeshan Vathanakumar", "Tharmakulasingam Mukunthan"], "title": "AdvBlur: Adversarial Blur for Robust Diabetic Retinopathy Classification and Cross-Domain Generalization", "comment": null, "summary": "Diabetic retinopathy (DR) is a leading cause of vision loss worldwide, yet\nearly and accurate detection can significantly improve treatment outcomes.\nWhile numerous Deep learning (DL) models have been developed to predict DR from\nfundus images, many face challenges in maintaining robustness due to\ndistributional variations caused by differences in acquisition devices,\ndemographic disparities, and imaging conditions. This paper addresses this\ncritical limitation by proposing a novel DR classification approach, a method\ncalled AdvBlur. Our method integrates adversarial blurred images into the\ndataset and employs a dual-loss function framework to address domain\ngeneralization. This approach effectively mitigates the impact of unseen\ndistributional variations, as evidenced by comprehensive evaluations across\nmultiple datasets. Additionally, we conduct extensive experiments to explore\nthe effects of factors such as camera type, low-quality images, and dataset\nsize. Furthermore, we perform ablation studies on blurred images and the loss\nfunction to ensure the validity of our choices. The experimental results\ndemonstrate the effectiveness of our proposed method, achieving competitive\nperformance compared to state-of-the-art domain generalization DR models on\nunseen external datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAdvBlur\u7684\u65b0\u578b\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\u5206\u7c7b\u65b9\u6cd5\uff0c\u901a\u8fc7\u96c6\u6210\u5bf9\u6297\u6027\u6a21\u7cca\u56fe\u50cf\u548c\u53cc\u91cd\u635f\u5931\u51fd\u6570\u6765\u89e3\u51b3\u9886\u57df\u6cdb\u5316\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u7684\u8bc4\u4f30\u4e2d\u663e\u793a\u51fa\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u9884\u6d4b\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\uff08DR\uff09\u65f6\uff0c\u7531\u4e8e\u8bbe\u5907\u3001\u4eba\u7fa4\u548c\u6210\u50cf\u6761\u4ef6\u5f15\u8d77\u7684\u5206\u5e03\u53d8\u5316\uff0c\u5728\u9c81\u68d2\u6027\u65b9\u9762\u9762\u4e34\u6311\u6218\u3002", "method": "AdvBlur\u65b9\u6cd5\u6574\u5408\u4e86\u5bf9\u6297\u6027\u6a21\u7cca\u56fe\u50cf\uff0c\u5e76\u91c7\u7528\u53cc\u91cd\u635f\u5931\u51fd\u6570\u6846\u67b6\u6765\u89e3\u51b3\u9886\u57df\u6cdb\u5316\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u770b\u4e0d\u89c1\u7684\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4e0e\u6700\u5148\u8fdb\u7684\u9886\u57df\u6cdb\u5316DR\u6a21\u578b\u76f8\u5ab2\u7f8e\u7684\u5f71\u54cd\uff0c\u5e76\u4e14\u6709\u6548\u51cf\u8f7b\u4e86\u672a\u89c1\u5206\u5e03\u53d8\u5316\u7684\u5f71\u54cd\u3002", "conclusion": "AdvBlur\u65b9\u6cd5\u901a\u8fc7\u96c6\u6210\u5bf9\u6297\u6027\u6a21\u7cca\u56fe\u50cf\u548c\u53cc\u91cd\u635f\u5931\u51fd\u6570\uff0c\u80fd\u591f\u6709\u6548\u63d0\u9ad8\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\u68c0\u6d4b\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u80fd\u5f88\u597d\u5730\u6cdb\u5316\u5230\u672a\u77e5\u7684\u5206\u5e03\u3002"}}
{"id": "2510.24059", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.24059", "abs": "https://arxiv.org/abs/2510.24059", "authors": ["Zehang Bao", "Zitian Zhu", "Yang-Ren Liu", "Zixuan Song", "Feitong Jin", "Xuhao Zhu", "Yu Gao", "Chuanyu Zhang", "Ning Wang", "Yiren Zou", "Ziqi Tan", "Aosai Zhang", "Zhengyi Cui", "Fanhao Shen", "Jiarun Zhong", "Yiyang He", "Han Wang", "Jia-Nan Yang", "Yanzhe Wang", "Jiayuan Shen", "Gongyu Liu", "Yihang Han", "Yaozu Wu", "Jinfeng Deng", "Hang Dong", "Pengfei Zhang", "Hekang Li", "Zhen Wang", "Chao Song", "Chen Cheng", "Rubem Mondaini", "Qiujiang Guo", "Biao Huang", "H. Wang"], "title": "Fock space prethermalization and time-crystalline order on a quantum processor", "comment": "8 pages, 4 figures + supplementary information", "summary": "Periodically driven quantum many-body systems exhibit a wide variety of\nexotic nonequilibrium phenomena and provide a promising pathway for quantum\napplications. A fundamental challenge for stabilizing and harnessing these\nhighly entangled states of matter is system heating by energy absorption from\nthe drive. Here, we propose and demonstrate a disorder-free mechanism, dubbed\nFock space prethermalization (FSP), to suppress heating. This mechanism divides\nthe Fock-space network into linearly many sparse sub-networks, thereby\nprolonging the thermalization timescale even for initial states at high energy\ndensities. Using 72 superconducting qubits, we observe an FSP-based\ntime-crystalline order that persists over 120 cycles for generic initial Fock\nstates. The underlying kinetic constraint of approximately conserved domain\nwall (DW) numbers is identified by measuring site-resolved correlators.\nFurther, we perform finite-size scaling analysis for DW and Fock-space dynamics\nby varying system sizes, which reveals size-independent regimes for\nFSP-thermalization crossover and links the dynamical behaviors to the\neigenstructure of the Floquet unitary. Our work establishes FSP as a robust\nmechanism for breaking ergodicity, and paves the way for exploring novel\nnonequilibrium quantum matter and its applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFock\u7a7a\u95f4\u9884\u70ed\uff08FSP\uff09\u7684\u65e0\u5931\u771f\u673a\u5236\uff0c\u7528\u4e8e\u6291\u5236\u5468\u671f\u9a71\u52a8\u91cf\u5b50\u591a\u4f53\u7cfb\u7edf\u4e2d\u7531\u80fd\u91cf\u5438\u6536\u5f15\u8d77\u7684\u7cfb\u7edf\u52a0\u70ed\uff0c\u901a\u8fc7\u5c06Fock\u7a7a\u95f4\u7f51\u7edc\u5212\u5206\u4e3a\u7a00\u758f\u5b50\u7f51\u7edc\u6765\u5ef6\u957f\u70ed\u5316\u65f6\u95f4\u5c3a\u5ea6\u3002", "motivation": "\u5468\u671f\u9a71\u52a8\u91cf\u5b50\u591a\u4f53\u7cfb\u7edf\u867d\u80fd\u5c55\u73b0\u65b0\u5947\u7684\u975e\u5e73\u8861\u73b0\u8c61\u4e14\u5728\u91cf\u5b50\u5e94\u7528\u65b9\u9762\u524d\u666f\u5e7f\u9614\uff0c\u4f46\u7cfb\u7edf\u52a0\u70ed\u662f\u7a33\u5b9a\u548c\u5229\u7528\u8fd9\u4e9b\u9ad8\u5ea6\u7ea0\u7f20\u7269\u8d28\u72b6\u6001\u9762\u4e34\u7684\u57fa\u672c\u6311\u6218\u3002", "method": "\u63d0\u51fa\u5e76\u6f14\u793a\u4e86\u4e00\u79cd\u65e0\u5931\u771f\u7684Fock\u7a7a\u95f4\u9884\u70ed\uff08FSP\uff09\u673a\u5236\uff0c\u8be5\u673a\u5236\u5c06Fock\u7a7a\u95f4\u7f51\u7edc\u5212\u5206\u4e3a\u7ebf\u6027\u6570\u91cf\u7684\u7a00\u758f\u5b50\u7f51\u7edc\uff0c\u4ece\u800c\u5ef6\u957f\u4e86\u5373\u4f7f\u5bf9\u4e8e\u9ad8\u80fd\u91cf\u5bc6\u5ea6\u521d\u59cb\u72b6\u6001\u7684\u70ed\u5316\u65f6\u95f4\u5c3a\u5ea6\u3002\u4f7f\u752872\u4e2a\u8d85\u5bfc\u91cf\u5b50\u6bd4\u7279\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5e76\u901a\u8fc7\u6d4b\u91cf\u7ad9\u70b9\u89e3\u6790\u7684\u76f8\u5173\u5668\u6765\u8bc6\u522b\u8fd1\u4f3c\u5b88\u6052\u7684\u7574\u58c1\uff08DW\uff09\u6570\u7684\u52a8\u529b\u5b66\u7ea6\u675f\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u6539\u53d8\u7cfb\u7edf\u5927\u5c0f\u8fdb\u884c\u6709\u9650\u5c3a\u5bf8\u6807\u5ea6\u5206\u6790\uff0c\u4ee5\u63ed\u793aFSP-\u70ed\u5316\u4ea4\u53c9\u7684\u72ec\u7acb\u4e8e\u5927\u5c0f\u7684\u533a\u57df\uff0c\u5e76\u5c06\u52a8\u529b\u5b66\u884c\u4e3a\u4e0eFloquet\u9149\u7b97\u7b26\u7684\u672c\u5f81\u7ed3\u6784\u8054\u7cfb\u8d77\u6765\u3002", "result": "\u572872\u4e2a\u8d85\u5bfc\u91cf\u5b50\u6bd4\u7279\u7cfb\u7edf\u4e2d\u89c2\u5bdf\u5230\u4e86\u57fa\u4e8eFSP\u7684\u65f6\u95f4\u6676\u4f53\u5e8f\uff0c\u8be5\u5e8f\u5728\u901a\u7528\u7684\u521d\u59cbFock\u6001\u4e0b\u6301\u7eed\u4e86120\u4e2a\u5468\u671f\u3002\u8bc6\u522b\u51fa\u8fd1\u4f3c\u5b88\u6052\u7684\u7574\u58c1\uff08DW\uff09\u6570\u4f5c\u4e3a\u5e95\u5c42\u52a8\u529b\u5b66\u7ea6\u675f\u3002\u6709\u9650\u5c3a\u5bf8\u6807\u5ea6\u5206\u6790\u63ed\u793a\u4e86FSP-\u70ed\u5316\u4ea4\u53c9\u7684\u72ec\u7acb\u4e8e\u5927\u5c0f\u7684\u533a\u57df\uff0c\u5e76\u5c06\u52a8\u529b\u5b66\u884c\u4e3a\u4e0eFloquet\u9149\u7b97\u7b26\u7684\u672c\u5f81\u7ed3\u6784\u8054\u7cfb\u8d77\u6765\u3002", "conclusion": "FSP\u88ab\u786e\u7acb\u4e3a\u4e00\u79cd\u6253\u7834\u904d\u5386\u6027\u7684\u9c81\u68d2\u673a\u5236\uff0c\u4e3a\u63a2\u7d22\u65b0\u9896\u7684\u975e\u5e73\u8861\u91cf\u5b50\u7269\u8d28\u53ca\u5176\u5e94\u7528\u5f00\u8f9f\u4e86\u9053\u8def\u3002"}}
{"id": "2510.24416", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.24416", "abs": "https://arxiv.org/abs/2510.24416", "authors": ["Nikhat Khan", "E. M. H. E. B. Ekanayake", "Nicolas Casilli", "Cristian Cassella", "Luke Theogarajan", "Nikhil Shukla"], "title": "Analyzing Parametric Oscillator Ising Machines through the Kuramoto Lens", "comment": null, "summary": "Networks of coupled nonlinear oscillators are emerging as powerful physical\nplatforms for implementing Ising machines. Yet the relationship between\nparametric-oscillator implementations and traditional oscillator-based Ising\nmachines remains underexplored. In this work, we develop a Kuramoto-style,\ncanonical phase description of parametric oscillator Ising machines by starting\nfrom the Stuart-Landau oscillator model -- the canonical normal form near a\nHopf bifurcation, and a natural reduced description for many parametric\noscillator implementations such as the degenerate optical parametric oscillator\n(DOPO) among others. The resulting phase dynamics combine the usual\nphase-difference coupling observed in the standard Kuramoto model along with an\nintrinsic phase sum term that is generated when conjugate coupling is\nconsidered. Moreover, our formulation helps explain why explicit\nsecond-harmonic driving is unnecessary in parametric oscillators and also\nreveals how quasi-steady amplitude heterogeneity scales the original strength\nof the spin interaction with potentially adverse impacts on the solution\nquality. Our work helps develop a unifying view of the oscillator-based\napproach to designing Ising machines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53c2\u6570\u8026\u5408\u632f\u8361\u5668\u7684Ising\u673a\u7684\u65b0\u578b\u76f8\u63cf\u8ff0\uff0c\u8be5\u63cf\u8ff0\u5c06\u4f20\u7edf\u7684\u76f8\u4f4d\u5dee\u8026\u5408\u4e0e\u56fa\u6709\u7684\u76f8\u4f4d\u548c\u9879\u76f8\u7ed3\u5408\uff0c\u4ece\u800c\u7edf\u4e00\u4e86\u57fa\u4e8e\u632f\u8361\u5668\u7684Ising\u673a\u8bbe\u8ba1\u65b9\u6cd5\u3002", "motivation": "\u63a2\u7d22\u53c2\u6570\u632f\u8361\u5668\u5b9e\u73b0\u4e0e\u4f20\u7edf\u632f\u8361\u5668Ising\u673a\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5e76\u4e3a\u57fa\u4e8e\u632f\u8361\u5668\u7684Ising\u673a\u8bbe\u8ba1\u63d0\u4f9b\u7edf\u4e00\u7684\u89c6\u89d2\u3002", "method": "\u4eceStuart-Landau\u632f\u8361\u5668\u6a21\u578b\u51fa\u53d1\uff0c\u63a8\u5bfc\u4e86\u53c2\u6570\u632f\u8361\u5668Ising\u673a\u7684Kuramoto\u98ce\u683c\u7684\u3001\u89c4\u8303\u7684\u76f8\u4f4d\u63cf\u8ff0\uff0c\u8be5\u63cf\u8ff0\u7ed3\u5408\u4e86\u6807\u51c6\u7684Kuramoto\u6a21\u578b\u4e2d\u7684\u76f8\u4f4d\u5dee\u8026\u5408\u4ee5\u53ca\u8003\u8651\u5171\u8f6d\u8026\u5408\u65f6\u4ea7\u751f\u7684\u5185\u7980\u76f8\u4f4d\u548c\u9879\u3002", "result": "\u5f97\u5230\u4e86\u53c2\u6570\u632f\u8361\u5668Ising\u673a\u7684\u76f8\u52a8\u529b\u5b66\u63cf\u8ff0\uff0c\u89e3\u91ca\u4e86\u4e3a\u4f55\u53c2\u6570\u632f\u8361\u5668\u4e0d\u9700\u8981\u663e\u5f0f\u7684\u4e8c\u6b21\u8c10\u6ce2\u9a71\u52a8\uff0c\u5e76\u63ed\u793a\u4e86\u632f\u5e45\u5f02\u8d28\u6027\u5982\u4f55\u5f71\u54cd\u81ea\u65cb\u76f8\u4e92\u4f5c\u7528\u7684\u5f3a\u5ea6\u548c\u89e3\u7684\u8d28\u91cf\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u76f8\u4f4d\u63cf\u8ff0\u6709\u52a9\u4e8e\u7edf\u4e00\u548c\u7406\u89e3\u57fa\u4e8e\u632f\u8361\u5668\u7684Ising\u673a\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u5e76\u4e3a\u8fdb\u4e00\u6b65\u7814\u7a76\u53c2\u6570\u632f\u8361\u5668\u5728Ising\u673a\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2510.24287", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24287", "abs": "https://arxiv.org/abs/2510.24287", "authors": ["Richard Koebe", "Noah Saibel", "Juan Miguel Lopez Alcaraz", "Simon Sch\u00e4fer", "Nils Strodthoff"], "title": "Towards actionable hypotension prediction -- predicting catecholamine therapy initiation in the intensive care unit", "comment": "27 pages, 8 figures, source code under\n  https://github.com/AI4HealthUOL/actionable-hypotension", "summary": "Hypotension in critically ill ICU patients is common and life-threatening.\nEscalation to catecholamine therapy marks a key management step, with both\nundertreatment and overtreatment posing risks. Most machine learning (ML)\nmodels predict hypotension using fixed MAP thresholds or MAP forecasting,\noverlooking the clinical decision behind treatment escalation. Predicting\ncatecholamine initiation, the start of vasoactive or inotropic agent\nadministration offers a more clinically actionable target reflecting real\ndecision-making. Using the MIMIC-III database, we modeled catecholamine\ninitiation as a binary event within a 15-minute prediction window. Input\nfeatures included statistical descriptors from a two-hour sliding MAP context\nwindow, along with demographics, biometrics, comorbidities, and ongoing\ntreatments. An Extreme Gradient Boosting (XGBoost) model was trained and\ninterpreted via SHapley Additive exPlanations (SHAP). The model achieved an\nAUROC of 0.822 (0.813-0.830), outperforming the hypotension baseline (MAP < 65,\nAUROC 0.686 [0.675-0.699]). SHAP analysis highlighted recent MAP values, MAP\ntrends, and ongoing treatments (e.g., sedatives, electrolytes) as dominant\npredictors. Subgroup analysis showed higher performance in males, younger\npatients (<53 years), those with higher BMI (>32), and patients without\ncomorbidities or concurrent medications. Predicting catecholamine initiation\nbased on MAP dynamics, treatment context, and patient characteristics supports\nthe critical decision of when to escalate therapy, shifting focus from\nthreshold-based alarms to actionable decision support. This approach is\nfeasible across a broad ICU cohort under natural event imbalance. Future work\nshould enrich temporal and physiological context, extend label definitions to\ninclude therapy escalation, and benchmark against existing hypotension\nprediction systems.", "AI": {"tldr": "\u6587\u7ae0\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9884\u6d4bICU\u60a3\u8005\u52a0\u538b\u7d20\u8d77\u59cb\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u57fa\u4e8eMAP\u52a8\u6001\u3001\u6cbb\u7597\u80cc\u666f\u548c\u60a3\u8005\u7279\u5f81\uff0c\u800c\u975e\u4f20\u7edf\u7684\u57fa\u4e8e\u56fa\u5b9aMAP\u9608\u503c\u7684\u65b9\u6cd5\uff0c\u5e76\u5728MIMIC-III\u6570\u636e\u5e93\u4e0a\u53d6\u5f97\u4e86\u826f\u597d\u7684\u9884\u6d4b\u6548\u679c\u3002", "motivation": "ICU\u60a3\u8005\u4f4e\u8840\u538b\u5e38\u89c1\u4e14\u5371\u53ca\u751f\u547d\uff0c\u4f46\u73b0\u6709\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u9884\u6d4b\u4f4e\u8840\u538b\u65f6\uff0c\u5f80\u5f80\u5ffd\u7565\u4e86\u6cbb\u7597\u5347\u7ea7\u8fd9\u4e00\u5173\u952e\u4e34\u5e8a\u51b3\u7b56\u3002\u9884\u6d4b\u52a0\u538b\u7d20\uff08\u8840\u7ba1\u6d3b\u6027\u6216\u6b63\u6027\u808c\u529b\u836f\u7269\uff09\u7684\u8d77\u59cb\uff0c\u66f4\u80fd\u53cd\u6620\u771f\u5b9e\u7684\u4e34\u5e8a\u51b3\u7b56\u8fc7\u7a0b\u3002", "method": "\u4f7f\u7528MIMIC-III\u6570\u636e\u5e93\uff0c\u5c06\u52a0\u538b\u7d20\u8d77\u59cb\u5efa\u6a21\u4e3a\u4e00\u4e2a15\u5206\u949f\u5185\u7684\u4e8c\u5143\u4e8b\u4ef6\u3002\u8f93\u5165\u7279\u5f81\u5305\u62ec\u8fc7\u53bb\u4e24\u5c0f\u65f6MAP\u7684\u7edf\u8ba1\u63cf\u8ff0\u3001\u4eba\u53e3\u7edf\u8ba1\u5b66\u4fe1\u606f\u3001\u751f\u7269\u8bc6\u522b\u6570\u636e\u3001\u5408\u5e76\u75c7\u548c\u6b63\u5728\u8fdb\u884c\u7684\u6cbb\u7597\u3002\u8bad\u7ec3\u4e86\u4e00\u4e2aExtreme Gradient Boosting (XGBoost)\u6a21\u578b\uff0c\u5e76\u4f7f\u7528SHapley Additive exPlanations (SHAP)\u8fdb\u884c\u4e86\u89e3\u91ca\u3002", "result": "XGBoost\u6a21\u578b\u5728\u9884\u6d4b\u52a0\u538b\u7d20\u8d77\u59cb\u65b9\u9762\u7684AUROC\u4e3a0.822\uff0c\u4f18\u4e8e\u9884\u6d4b\u4f4e\u8840\u538b\uff08MAP < 65\uff09\u7684\u57fa\u7ebf\u6a21\u578b\uff08AUROC 0.686\uff09\u3002SHAP\u5206\u6790\u663e\u793a\uff0c\u6700\u8fd1\u7684MAP\u503c\u3001MAP\u8d8b\u52bf\u4ee5\u53ca\u6b63\u5728\u8fdb\u884c\u7684\u6cbb\u7597\uff08\u5982\u9547\u9759\u5242\u3001\u7535\u89e3\u8d28\uff09\u662f\u4e3b\u8981\u7684\u9884\u6d4b\u56e0\u5b50\u3002\u6a21\u578b\u5728\u7537\u6027\u3001\u5e74\u8f7b\u60a3\u8005\uff08<53\u5c81\uff09\u3001\u9ad8BMI\uff08>32\uff09\u4ee5\u53ca\u65e0\u5408\u5e76\u75c7\u6216\u5408\u5e76\u7528\u836f\u7684\u60a3\u8005\u4e9a\u7ec4\u4e2d\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "\u57fa\u4e8eMAP\u52a8\u6001\u3001\u6cbb\u7597\u80cc\u666f\u548c\u60a3\u8005\u7279\u5f81\u9884\u6d4b\u52a0\u538b\u7d20\u8d77\u59cb\uff0c\u80fd\u591f\u4e3a\u6cbb\u7597\u5347\u7ea7\u7684\u5173\u952e\u51b3\u7b56\u63d0\u4f9b\u652f\u6301\uff0c\u5c06\u7126\u70b9\u4ece\u57fa\u4e8e\u9608\u503c\u7684\u8b66\u62a5\u8f6c\u79fb\u5230\u53ef\u64cd\u4f5c\u7684\u51b3\u7b56\u652f\u6301\u3002\u8be5\u65b9\u6cd5\u5728ICU\u4eba\u7fa4\u4e2d\u5177\u6709\u53ef\u884c\u6027\uff0c\u5e76\u4e14\u80fd\u591f\u5904\u7406\u4e8b\u4ef6\u4e0d\u5e73\u8861\u7684\u60c5\u51b5\u3002\u672a\u6765\u7684\u5de5\u4f5c\u53ef\u4ee5\u8003\u8651\u4e30\u5bcc\u65f6\u95f4\u548c\u751f\u7406\u80cc\u666f\u4fe1\u606f\uff0c\u6269\u5c55\u6807\u7b7e\u5b9a\u4e49\u4ee5\u5305\u542b\u6cbb\u7597\u5347\u7ea7\uff0c\u5e76\u4e0e\u73b0\u6709\u7684\u4f4e\u8840\u538b\u9884\u6d4b\u7cfb\u7edf\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002"}}
{"id": "2510.23640", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23640", "abs": "https://arxiv.org/abs/2510.23640", "authors": ["Zihao Jing", "Yan Sun", "Yan Yi Li", "Sugitha Janarthanan", "Alana Deng", "Pingzhao Hu"], "title": "Structure-Aware Fusion with Progressive Injection for Multimodal Molecular Representation Learning", "comment": "Accepted by NeurIPS 2025", "summary": "Multimodal molecular models often suffer from 3D conformer unreliability and\nmodality collapse, limiting their robustness and generalization. We propose\nMuMo, a structured multimodal fusion framework that addresses these challenges\nin molecular representation through two key strategies. To reduce the\ninstability of conformer-dependent fusion, we design a Structured Fusion\nPipeline (SFP) that combines 2D topology and 3D geometry into a unified and\nstable structural prior. To mitigate modality collapse caused by naive fusion,\nwe introduce a Progressive Injection (PI) mechanism that asymmetrically\nintegrates this prior into the sequence stream, preserving modality-specific\nmodeling while enabling cross-modal enrichment. Built on a state space\nbackbone, MuMo supports long-range dependency modeling and robust information\npropagation. Across 29 benchmark tasks from Therapeutics Data Commons (TDC) and\nMoleculeNet, MuMo achieves an average improvement of 2.7% over the\nbest-performing baseline on each task, ranking first on 22 of them, including a\n27% improvement on the LD50 task. These results validate its robustness to 3D\nconformer noise and the effectiveness of multimodal fusion in molecular\nrepresentation. The code is available at: github.com/selmiss/MuMo.", "AI": {"tldr": "MuMo\u662f\u4e00\u4e2a\u7ed3\u6784\u5316\u591a\u6a21\u6001\u5206\u5b50\u8868\u793a\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u878d\u5408\u6d41\u7a0b\uff08SFP\uff09\u548c\u6e10\u8fdb\u6ce8\u5165\uff08PI\uff09\u673a\u5236\u89e3\u51b33D\u6784\u8c61\u4e0d\u786e\u5b9a\u6027\u548c\u6a21\u6001\u574d\u584c\u95ee\u9898\uff0c\u572829\u4e2aTDC\u548cMoleculeNet\u57fa\u51c6\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u591a\u6a21\u6001\u5206\u5b50\u6a21\u578b\u57283D\u6784\u8c61\u4e0d\u786e\u5b9a\u6027\u548c\u6a21\u6001\u574d\u584c\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5f71\u54cd\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51faMuMo\u6846\u67b6\uff0c\u5305\u62ec\u7ed3\u6784\u5316\u878d\u5408\u6d41\u7a0b\uff08SFP\uff09\u7528\u4e8e\u7ed3\u54082D\u62d3\u6251\u548c3D\u51e0\u4f55\uff0c\u4ee5\u53ca\u6e10\u8fdb\u6ce8\u5165\uff08PI\uff09\u673a\u5236\u7528\u4e8e\u4e0d\u5bf9\u79f0\u5730\u6574\u5408\u5148\u9a8c\u4fe1\u606f\uff0c\u4ee5\u51cf\u5c11\u6a21\u6001\u4e0d\u786e\u5b9a\u6027\u548c\u6a21\u6001\u574d\u584c\u3002", "result": "MuMo\u572829\u4e2aTDC\u548cMoleculeNet\u57fa\u51c6\u4efb\u52a1\u4e0a\uff0c\u5e73\u5747\u6bd4\u6700\u4f73\u57fa\u7ebf\u63d0\u9ad8\u4e862.7%\uff0c\u572822\u4e2a\u4efb\u52a1\u4e2d\u6392\u540d\u7b2c\u4e00\uff0c\u5e76\u5728LD50\u4efb\u52a1\u4e0a\u63d0\u9ad8\u4e8627%\u3002", "conclusion": "MuMo\u5728\u5904\u74063D\u6784\u8c61\u566a\u58f0\u65b9\u9762\u8868\u73b0\u51fa\u9c81\u68d2\u6027\uff0c\u5e76\u6709\u6548\u5730\u5b9e\u73b0\u4e86\u591a\u6a21\u6001\u878d\u5408\u5728\u5206\u5b50\u8868\u793a\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2510.23989", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23989", "abs": "https://arxiv.org/abs/2510.23989", "authors": ["Shangde Gao", "Zelin Xu", "Zhe Jiang"], "title": "Learning Individual Movement Shifts After Urban Disruptions with Social Infrastructure Reliance", "comment": null, "summary": "Shifts in individual movement patterns following disruptive events can reveal\nchanging demands for community resources. However, predicting such shifts\nbefore disruptive events remains challenging for several reasons. First,\nmeasures are lacking for individuals' heterogeneous social infrastructure\nresilience (SIR), which directly influences their movement patterns, and\ncommonly used features are often limited or unavailable at scale, e.g.,\nsociodemographic characteristics. Second, the complex interactions between\nindividual movement patterns and spatial contexts have not been sufficiently\ncaptured. Third, individual-level movement may be spatially sparse and not\nwell-suited to traditional decision-making methods for movement predictions.\nThis study incorporates individuals' SIR into a conditioned deep learning model\nto capture the complex relationships between individual movement patterns and\nlocal spatial context using large-scale, sparse individual-level data. Our\nexperiments demonstrate that incorporating individuals' SIR and spatial context\ncan enhance the model's ability to predict post-event individual movement\npatterns. The conditioned model can capture the divergent shifts in movement\npatterns among individuals who exhibit similar pre-event patterns but differ in\nSIR.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4e2a\u4f53\u793e\u4f1a\u57fa\u7840\u8bbe\u65bd\u97e7\u6027(SIR)\u548c\u7a7a\u95f4\u80cc\u666f\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u4ee5\u9884\u6d4b\u5e72\u6270\u4e8b\u4ef6\u540e\u4e2a\u4f53\u6d3b\u52a8\u6a21\u5f0f\u7684\u53d8\u5316\uff0c\u5e76\u8bc1\u660e\u8be5\u6a21\u578b\u80fd\u6709\u6548\u63d0\u5347\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u4e2a\u4f53\u6d3b\u52a8\u6a21\u5f0f\u5728\u5e72\u6270\u4e8b\u4ef6\u540e\u7684\u53d8\u5316\u80fd\u53cd\u6620\u793e\u533a\u8d44\u6e90\u9700\u6c42\u7684\u53d8\u52a8\uff0c\u4f46\u9884\u6d4b\u8fd9\u4e9b\u53d8\u5316\u9762\u4e34\u4e2a\u4f53SIR\u5ea6\u91cf\u7f3a\u5931\u3001\u4e2a\u4f53\u6d3b\u52a8\u4e0e\u7a7a\u95f4\u80cc\u666f\u7684\u590d\u6742\u4ea4\u4e92\u672a\u88ab\u5145\u5206\u6355\u6349\u3001\u4ee5\u53ca\u4e2a\u4f53\u6d3b\u52a8\u6570\u636e\u7a00\u758f\u7b49\u6311\u6218\u3002", "method": "\u7814\u7a76\u5c06\u4e2a\u4f53\u7684SIR\u7eb3\u5165\u6761\u4ef6\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u4ee5\u5904\u7406\u5927\u89c4\u6a21\u3001\u7a00\u758f\u7684\u4e2a\u4f53\u6d3b\u52a8\u6570\u636e\uff0c\u5e76\u6355\u6349\u4e2a\u4f53\u6d3b\u52a8\u6a21\u5f0f\u4e0e\u5c40\u90e8\u7a7a\u95f4\u80cc\u666f\u4e4b\u95f4\u590d\u6742\u7684\u5173\u8054\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u7eb3\u5165SIR\u548c\u7a7a\u95f4\u80cc\u666f\u4fe1\u606f\u80fd\u63d0\u5347\u6a21\u578b\u9884\u6d4b\u4e8b\u4ef6\u540e\u4e2a\u4f53\u6d3b\u52a8\u6a21\u5f0f\u7684\u80fd\u529b\uff0c\u5c24\u5176\u80fd\u533a\u5206\u5177\u6709\u76f8\u4f3c\u4e8b\u4ef6\u524d\u6d3b\u52a8\u6a21\u5f0f\u4f46SIR\u4e0d\u540c\u7684\u4e2a\u4f53\u6240\u8868\u73b0\u51fa\u7684\u5dee\u5f02\u6027\u6d3b\u52a8\u6a21\u5f0f\u8f6c\u53d8\u3002", "conclusion": "\u4e2a\u4f53SIR\u548c\u793e\u4f1a\u7a7a\u95f4\u80cc\u666f\u662f\u9884\u6d4b\u5e72\u6270\u4e8b\u4ef6\u540e\u4e2a\u4f53\u6d3b\u52a8\u6a21\u5f0f\u53d8\u5316\u7684\u5173\u952e\u56e0\u7d20\uff0c\u5c06\u5176\u7eb3\u5165\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u80fd\u663e\u8457\u63d0\u9ad8\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2510.23998", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23998", "abs": "https://arxiv.org/abs/2510.23998", "authors": ["Mengzhou Sun", "Sendong Zhao", "Jianyu Chen", "Bin Qin"], "title": "PICOs-RAG: PICO-supported Query Rewriting for Retrieval-Augmented Generation in Evidence-Based Medicine", "comment": null, "summary": "Evidence-based medicine (EBM) research has always been of paramount\nimportance. It is important to find appropriate medical theoretical support for\nthe needs from physicians or patients to reduce the occurrence of medical\naccidents. This process is often carried out by human querying relevant\nliterature databases, which lacks objectivity and efficiency. Therefore,\nresearchers utilize retrieval-augmented generation (RAG) to search for evidence\nand generate responses automatically. However, current RAG methods struggle to\nhandle complex queries in real-world clinical scenarios. For example, when\nqueries lack certain information or use imprecise language, the model may\nretrieve irrelevant evidence and generate unhelpful answers. To address this\nissue, we present the PICOs-RAG to expand the user queries into a better\nformat. Our method can expand and normalize the queries into professional ones\nand use the PICO format, a search strategy tool present in EBM, to extract the\nmost important information used for retrieval. This approach significantly\nenhances retrieval efficiency and relevance, resulting in up to an 8.8\\%\nimprovement compared to the baseline evaluated by our method. Thereby the\nPICOs-RAG improves the performance of the large language models into a helpful\nand reliable medical assistant in EBM.", "AI": {"tldr": "\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u5728\u5faa\u8bc1\u533b\u5b66\uff08EBM\uff09\u4e2d\u7528\u4e8e\u6587\u732e\u68c0\u7d22\u548c\u81ea\u52a8\u5e94\u7b54\uff0c\u4f46\u5f53\u524d\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u590d\u6742\u6216\u4e0d\u7cbe\u786e\u7684\u67e5\u8be2\u3002\u4e3a\u89e3\u51b3\u6b64\u95ee\u9898\uff0c\u63d0\u51faPICOs-RAG\u65b9\u6cd5\uff0c\u5c06\u7528\u6237\u67e5\u8be2\u6269\u5c55\u5e76\u89c4\u8303\u5316\u4e3aPICO\u683c\u5f0f\uff0c\u4ee5\u63d0\u9ad8\u68c0\u7d22\u6548\u7387\u548c\u76f8\u5173\u6027\uff0c\u5e76\u5c06LLM\u5e94\u7528\u4e8eEBM\u3002", "motivation": "\u4e3a\u4e86\u51cf\u5c11\u533b\u7597\u4e8b\u6545\u7684\u53d1\u751f\uff0c\u9700\u8981\u4e3a\u533b\u751f\u6216\u60a3\u8005\u7684\u9700\u6c42\u627e\u5230\u5408\u9002\u7684\u533b\u5b66\u7406\u8bba\u652f\u6301\uff0c\u800c\u4f20\u7edf\u7684\u4eba\u5de5\u6587\u732e\u68c0\u7d22\u65b9\u6cd5\u7f3a\u4e4f\u5ba2\u89c2\u6027\u548c\u6548\u7387\u3002\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u53ef\u4ee5\u81ea\u52a8\u68c0\u7d22\u8bc1\u636e\u5e76\u751f\u6210\u54cd\u5e94\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u73b0\u5b9e\u4e34\u5e8a\u573a\u666f\u4e2d\u7684\u590d\u6742\u67e5\u8be2\u3002", "method": "\u63d0\u51faPICOs-RAG\u65b9\u6cd5\uff0c\u5c06\u7528\u6237\u67e5\u8be2\u6269\u5c55\u5e76\u89c4\u8303\u5316\u4e3aPICO\u683c\u5f0f\uff08\u4e00\u79cdEBM\u4e2d\u7684\u641c\u7d22\u7b56\u7565\u5de5\u5177\uff09\uff0c\u4ee5\u63d0\u53d6\u6700\u5173\u952e\u7684\u7528\u4e8e\u68c0\u7d22\u7684\u4fe1\u606f\u3002", "result": "PICOs-RAG\u65b9\u6cd5\u5728\u68c0\u7d22\u6548\u7387\u548c\u76f8\u5173\u6027\u65b9\u9762\u663e\u8457\u63d0\u9ad8\uff0c\u4e0e\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8bc4\u4f30\u7ed3\u679c\u6700\u591a\u63d0\u9ad8\u4e868.8%\u3002", "conclusion": "PICOs-RAG\u65b9\u6cd5\u80fd\u591f\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63d0\u5347\u4e3aEBM\u4e2d\u6709\u7528\u4e14\u53ef\u9760\u7684\u533b\u5b66\u52a9\u624b\u3002"}}
{"id": "2510.24194", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24194", "abs": "https://arxiv.org/abs/2510.24194", "authors": ["Ev Zisselman", "Mirco Mutti", "Shelly Francis-Meretzki", "Elisei Shafer", "Aviv Tamar"], "title": "Blindfolded Experts Generalize Better: Insights from Robotic Manipulation and Videogames", "comment": null, "summary": "Behavioral cloning is a simple yet effective technique for learning\nsequential decision-making from demonstrations. Recently, it has gained\nprominence as the core of foundation models for the physical world, where\nachieving generalization requires countless demonstrations of a multitude of\ntasks. Typically, a human expert with full information on the task demonstrates\na (nearly) optimal behavior. In this paper, we propose to hide some of the\ntask's information from the demonstrator. This ``blindfolded'' expert is\ncompelled to employ non-trivial exploration to solve the task. We show that\ncloning the blindfolded expert generalizes better to unseen tasks than its\nfully-informed counterpart. We conduct experiments of real-world robot peg\ninsertion tasks with (limited) human demonstrations, alongside videogames from\nthe Procgen benchmark. Additionally, we support our findings with theoretical\nanalysis, which confirms that the generalization error scales with\n$\\sqrt{I/m}$, where $I$ measures the amount of task information available to\nthe demonstrator, and $m$ is the number of demonstrated tasks. Both theory and\npractice indicate that cloning blindfolded experts generalizes better with\nfewer demonstrated tasks. Project page with videos and code:\nhttps://sites.google.com/view/blindfoldedexperts/home", "AI": {"tldr": "\u901a\u8fc7\u6a21\u4eff\u201c\u8499\u773c\u201d\u4e13\u5bb6\u7684\u884c\u4e3a\uff0c\u53ef\u4ee5\u6bd4\u6a21\u4eff\u5b8c\u5168\u77e5\u60c5\u7684\u4e13\u5bb6\u66f4\u80fd\u6cdb\u5316\u5230\u65b0\u7684\u4efb\u52a1\u3002", "motivation": "\u4e3a\u4e86\u5728\u7269\u7406\u4e16\u754c\u4e2d\u5b9e\u73b0\u884c\u4e3a\u514b\u9686\u7684\u6cdb\u5316\uff0c\u9700\u8981\u5927\u91cf\u7684\u4efb\u52a1\u6f14\u793a\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u65b9\u6cd5\u6210\u672c\u9ad8\u6602\u3002\u672c\u7814\u7a76\u65e8\u5728\u627e\u5230\u4e00\u79cd\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u6765\u83b7\u5f97\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u201c\u8499\u773c\u201d\u4e13\u5bb6\u65b9\u6cd5\uff0c\u5728\u8fd9\u79cd\u65b9\u6cd5\u4e2d\uff0c\u6f14\u793a\u8005\u5728\u6f14\u793a\u8fc7\u7a0b\u4e2d\u65e0\u6cd5\u83b7\u5f97\u6240\u6709\u4efb\u52a1\u4fe1\u606f\uff0c\u4ece\u800c\u8feb\u4f7f\u4ed6\u4eec\u8fdb\u884c\u63a2\u7d22\u3002\u901a\u8fc7\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u548c\u89c6\u9891\u6e38\u620f\u7b49\u4efb\u52a1\u4e2d\u8fdb\u884c\u5b9e\u9a8c\u6765\u9a8c\u8bc1\u8be5\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6a21\u4eff\u201c\u8499\u773c\u201d\u4e13\u5bb6\u6bd4\u6a21\u4eff\u5b8c\u5168\u77e5\u60c5\u7684\u4e13\u5bb6\u5728\u6cdb\u5316\u5230\u65b0\u4efb\u52a1\u65b9\u9762\u8868\u73b0\u66f4\u597d\u3002\u7406\u8bba\u5206\u6790\u652f\u6301\u4e86\u8fd9\u4e00\u53d1\u73b0\uff0c\u8868\u660e\u6cdb\u5316\u8bef\u5dee\u4e0e\u6f14\u793a\u8005\u53ef\u83b7\u5f97\u7684\u4efb\u52a1\u4fe1\u606f\u91cf\u6210\u53cd\u6bd4\uff0c\u4e0e\u6f14\u793a\u4efb\u52a1\u7684\u6570\u91cf\u6210\u6b63\u6bd4\u3002", "conclusion": "\u201c\u8499\u773c\u201d\u4e13\u5bb6\u65b9\u6cd5\u662f\u4e00\u79cd\u66f4\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u4ec5\u901a\u8fc7\u5c11\u91cf\u6f14\u793a\u5c31\u80fd\u5728\u884c\u4e3a\u514b\u9686\u4e2d\u5b9e\u73b0\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2510.24523", "categories": ["cond-mat.mtrl-sci", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24523", "abs": "https://arxiv.org/abs/2510.24523", "authors": ["Samuel Del Fr\u00e9", "Andr\u00e9e de Backer", "Christophe Domain", "Ludovic Thuinet", "Charlotte S. Becquart"], "title": "Unsupervised Machine-Learning Pipeline for Data-Driven Defect Detection and Characterisation: Application to Displacement Cascades", "comment": "22 pages, 1 graphical abstract, 7 figures, 4 tables", "summary": "Neutron irradiation produces, within a few picoseconds, displacement cascades\nthat are sequences of atomic collisions generating point and extended defects\nwhich subsequently affects the long-term evolution of materials. The diversity\nof these defects, characterized morphologically and statistically, defines what\nis called the \"primary damage\". In this work, we present a fully unsupervised\nmachine learning (ML) workflow that detects and classifies these defects\ndirectly from molecular dynamics data. Local environments are encoded by the\nSmooth Overlap of Atomic Positions (SOAP) vector, anomalous atoms are isolated\nwith autoencoder neural networks (AE), embedded with Uniform Manifold\nApproximation and Projection (UMAP) and clustered using Hierarchical\nDensity-Based Spatial Clustering of Applications with Noise (HDBSCAN). Applied\nto 80 keV displacement cascades in Ni, Fe$_7$0Ni$_{10}$Cr$_{20}$, and Zr, the\nAE successfully identify the small fraction of outlier atoms that participate\nin defect formation. HDBSCAN then partitions the UMAP latent space of\nAE-flagged SOAP descriptors into well defined groups representing vacancy- and\ninterstitial-dominated regions and, within each, separates small from large\naggregates, assigning 99.7 % of outliers to compact physical motifs. A signed\ncluster-identification score confirms this separation, and cluster size scales\nwith net defect counts (R2 > 0.89). Statistical cross analyses between the ML\noutlier map and several conventional detectors (centrosymmetry, dislocation\nextraction, etc.) reveal strong overlap and complementary coverage, all\nachieved without template or threshold tuning. This ML workflow thus provides\nan efficient tool for the quantitative mapping of structural anomalies in\nmaterials, particularly those arising from irradiation damage in displacement\ncascades.", "AI": {"tldr": "\u672c\u5de5\u4f5c\u63d0\u51fa\u4e86\u4e00\u79cd\u5168\u65e0\u76d1\u7763\u7684\u673a\u5668\u5b66\u4e60\uff08ML\uff09\u5de5\u4f5c\u6d41\u7a0b\uff0c\u53ef\u4ee5\u76f4\u63a5\u4ece\u5206\u5b50\u52a8\u529b\u5b66\u6570\u636e\u4e2d\u68c0\u6d4b\u548c\u5206\u7c7b\u4e2d\u5b50\u8f90\u7167\u4ea7\u751f\u7684\u70b9\u548c\u6269\u5c55\u7f3a\u9677\uff08\u5373\u201c\u521d\u7ea7\u635f\u4f24\u201d\uff09\u3002", "motivation": "\u4e2d\u5b50\u8f90\u7167\u4ea7\u751f\u7684\u521d\u7ea7\u635f\u4f24\uff0c\u7279\u522b\u662f\u5176\u591a\u6837\u6027\uff0c\u76f4\u63a5\u5f71\u54cd\u6750\u6599\u7684\u957f\u671f\u6f14\u53d8\uff0c\u7136\u800c\uff0c\u5bf9\u8fd9\u4e9b\u7f3a\u9677\u7684\u8868\u5f81\u548c\u5206\u7c7b\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u8be5\u5de5\u4f5c\u6d41\u7a0b\u5229\u7528\u539f\u5b50\u73af\u5883\u7684\u5e73\u6ed1\u91cd\u53e0\uff08SOAP\uff09\u5411\u91cf\u7f16\u7801\uff0c\u901a\u8fc7\u81ea\u52a8\u7f16\u7801\u5668\u795e\u7ecf\u7f51\u7edc\uff08AE\uff09\u5206\u79bb\u5f02\u5e38\u539f\u5b50\uff0c\u5e76\u4f7f\u7528\u5747\u5300\u6d41\u5f62\u8fd1\u4f3c\u4e0e\u6295\u5f71\uff08UMAP\uff09\u8fdb\u884c\u5d4c\u5165\uff0c\u6700\u540e\u7528\u5c42\u6b21\u5bc6\u5ea6\u805a\u7c7b\uff08HDBSCAN\uff09\u5bf9\u5d4c\u5165\u7684\u7279\u5f81\u5411\u91cf\u8fdb\u884c\u805a\u7c7b\uff0c\u4ee5\u8bc6\u522b\u548c\u5206\u7c7b\u7f3a\u9677\u3002", "result": "\u8be5\u65b9\u6cd5\u6210\u529f\u5e94\u7528\u4e8eNi\u3001Fe70Ni10Cr20\u548cZr\u4e2d\u768480 keV\u4f4d\u79fb\u7ea7\u8054\uff0c\u80fd\u591f\u8bc6\u522b\u7f3a\u9677\u539f\u5b50\uff0c\u5e76\u5c0699.7%\u7684\u5f02\u5e38\u539f\u5b50\u5212\u5206\u5230\u4ee3\u8868\u7a7a\u4f4d\u548c\u95f4\u9699\u7f3a\u9677\u7684\u7c07\u4e2d\uff0c\u8fdb\u4e00\u6b65\u533a\u5206\u4e86\u5c0f\u805a\u96c6\u4f53\u548c\u5927\u805a\u96c6\u4f53\u3002\u805a\u7c7b\u7ed3\u679c\u4e0e\u4f20\u7edf\u63a2\u6d4b\u5668\uff08\u5982\u8d28\u5fc3\u504f\u5dee\u3001\u4f4d\u9519\u63d0\u53d6\u7b49\uff09\u7684\u5206\u6790\u663e\u793a\u51fa\u9ad8\u5ea6\u91cd\u53e0\u548c\u4e92\u8865\u7684\u8986\u76d6\uff0c\u4e14\u65e0\u9700\u8c03\u6574\u53c2\u6570\u3002", "conclusion": "\u8be5\u673a\u5668\u5b66\u4e60\u5de5\u4f5c\u6d41\u7a0b\u4e3a\u5b9a\u91cf\u8868\u5f81\u6750\u6599\u7ed3\u6784\u5f02\u5e38\uff08\u5c24\u5176\u662f\u5728\u4f4d\u79fb\u7ea7\u8054\u8f90\u7167\u635f\u4f24\u60c5\u51b5\u4e0b\uff09\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u7684\u5de5\u5177\uff0c\u80fd\u591f\u81ea\u52a8\u3001\u51c6\u786e\u5730\u8bc6\u522b\u548c\u5206\u7c7b\u5404\u79cd\u7f3a\u9677\u3002"}}
{"id": "2510.24009", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24009", "abs": "https://arxiv.org/abs/2510.24009", "authors": ["Yuan Jin", "Antonio Pepe", "Gian Marco Melito", "Yuxuan Chen", "Yunsu Byeon", "Hyeseong Kim", "Kyungwon Kim", "Doohyun Park", "Euijoon Choi", "Dosik Hwang", "Andriy Myronenko", "Dong Yang", "Yufan He", "Daguang Xu", "Ayman El-Ghotni", "Mohamed Nabil", "Hossam El-Kady", "Ahmed Ayyad", "Amr Nasr", "Marek Wodzinski", "Henning M\u00fcller", "Hyeongyu Kim", "Yejee Shin", "Abbas Khan", "Muhammad Asad", "Alexander Zolotarev", "Caroline Roney", "Anthony Mathur", "Martin Benning", "Gregory Slabaugh", "Theodoros Panagiotis Vagenas", "Konstantinos Georgas", "George K. Matsopoulos", "Jihan Zhang", "Zhen Zhang", "Liqin Huang", "Christian Mayer", "Heinrich M\u00e4chler", "Jan Egger"], "title": "Towards the Automatic Segmentation, Modeling and Meshing of the Aortic Vessel Tree from Multicenter Acquisitions: An Overview of the SEG.A. 2023 Segmentation of the Aorta Challenge", "comment": null, "summary": "The automated analysis of the aortic vessel tree (AVT) from computed\ntomography angiography (CTA) holds immense clinical potential, but its\ndevelopment has been impeded by a lack of shared, high-quality data. We\nlaunched the SEG.A. challenge to catalyze progress in this field by introducing\na large, publicly available, multi-institutional dataset for AVT segmentation.\nThe challenge benchmarked automated algorithms on a hidden test set, with\nsubsequent optional tasks in surface meshing for computational simulations. Our\nfindings reveal a clear convergence on deep learning methodologies, with 3D\nU-Net architectures dominating the top submissions. A key result was that an\nensemble of the highest-ranking algorithms significantly outperformed\nindividual models, highlighting the benefits of model fusion. Performance was\nstrongly linked to algorithmic design, particularly the use of customized\npost-processing steps, and the characteristics of the training data. This\ninitiative not only establishes a new performance benchmark but also provides a\nlasting resource to drive future innovation toward robust, clinically\ntranslatable tools.", "AI": {"tldr": "SEG.A\u6311\u6218\u901a\u8fc7\u63d0\u4f9b\u5927\u578b\u516c\u5f00\u6570\u636e\u96c6\uff0c\u4fc3\u8fdb\u4e86\u81ea\u52a8\u5316\u7684\u4e3b\u52a8\u8109\u8840\u7ba1\u6811\uff08AVT\uff09\u5206\u5272\u7814\u7a76\uff0c\u5e76\u5c55\u793a\u4e86\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff08\u5c24\u5176\u662f3D U-Net\uff09\u7684\u6709\u6548\u6027\uff0c\u540c\u65f6\u5f3a\u8c03\u4e86\u6a21\u578b\u96c6\u6210\u548c\u5b9a\u5236\u5316\u540e\u5904\u7406\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u7f3a\u4e4f\u5171\u4eab\u7684\u9ad8\u8d28\u91cf\u6570\u636e\u963b\u788d\u4e86\u8ba1\u7b97\u673a\u65ad\u5c42\u626b\u63cf\u8840\u7ba1\u9020\u5f71\uff08CTA\uff09\u7684\u4e3b\u52a8\u8109\u8840\u7ba1\u6811\uff08AVT\uff09\u7684\u81ea\u52a8\u5316\u5206\u6790\u7684\u53d1\u5c55\u3002", "method": "\u53d1\u8d77SEG.A\u6311\u6218\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5927\u578b\u3001\u516c\u5f00\u3001\u591a\u673a\u6784\u7684AVT\u5206\u5272\u6570\u636e\u96c6\uff0c\u5e76\u5bf9\u9690\u85cf\u6d4b\u8bd5\u96c6\u4e0a\u7684\u81ea\u52a8\u5316\u7b97\u6cd5\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8fd8\u5305\u62ec\u4e86\u8ba1\u7b97\u6a21\u62df\u7684\u8868\u9762\u7f51\u683c\u5212\u5206\u7684\u53ef\u9009\u4efb\u52a1\u3002", "result": "\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u7279\u522b\u662f3D U-Net\u67b6\u6784\uff0c\u5728\u7b97\u6cd5\u4e2d\u5360\u4e3b\u5bfc\u5730\u4f4d\u3002\u6700\u9ad8\u6392\u540d\u7684\u7b97\u6cd5\u96c6\u5408\u663e\u8457\u4f18\u4e8e\u5355\u4e2a\u6a21\u578b\u3002\u6027\u80fd\u4e0e\u7b97\u6cd5\u8bbe\u8ba1\uff08\u7279\u522b\u662f\u5b9a\u5236\u5316\u540e\u5904\u7406\uff09\u548c\u8bad\u7ec3\u6570\u636e\u7279\u5f81\u5bc6\u5207\u76f8\u5173\u3002", "conclusion": "SEG.A\u6311\u6218\u4e0d\u4ec5\u5efa\u7acb\u4e86\u65b0\u7684\u6027\u80fd\u57fa\u51c6\uff0c\u8fd8\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6301\u4e45\u7684\u8d44\u6e90\uff0c\u4ee5\u63a8\u52a8\u672a\u6765\u5f00\u53d1\u7a33\u5065\u3001\u4e34\u5e8a\u53ef\u8f6c\u79fb\u7684\u5de5\u5177\u3002"}}
{"id": "2510.24082", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.24082", "abs": "https://arxiv.org/abs/2510.24082", "authors": ["Yanjun Ji", "Susanna Kirchhoff", "Frank K. Wilhelm"], "title": "Exploring the Fidelity of Flux Qubit Measurement in Different Bases via Quantum Flux Parametron", "comment": null, "summary": "High-fidelity qubit readout is a fundamental requirement for practical\nquantum computing systems. In this work, we investigate methods to enhance the\nmeasurement fidelity of flux qubits via a quantum flux parametron-mediated\nreadout scheme. Through theoretical modeling and numerical simulations, we\nanalyze the impact of different measurement bases on fidelity in single-qubit\nand coupled two-qubit systems. For single-qubit systems, we show that energy\nbases consistently outperform flux bases in achieving higher fidelity. In\ncoupled two-qubit systems, we explore two measurement models: sequential and\nsimultaneous measurements, both aimed at reading out a single target qubit. Our\nresults indicate that the highest fidelity can be achieved either by performing\nsequential measurement in a dressed basis over a longer duration or by\nconducting simultaneous measurement in a bare basis over a shorter duration.\nImportantly, the sequential measurement model consistently yields more robust\nand higher fidelity readouts compared to the simultaneous approach. These\nfindings quantify achievable fidelities and provide valuable guidance for\noptimizing measurement protocols in emerging quantum computing architectures.", "AI": {"tldr": "\u91cf\u5b50\u901a\u91cf\u53c2\u91cf\u8ba1\u53ef\u63d0\u9ad8\u901a\u91cf\u91cf\u5b50\u6bd4\u7279\u8bfb\u51fa\u4fdd\u771f\u5ea6\uff0c\u80fd\u91cf\u57fa\u4f18\u4e8e\u901a\u91cf\u57fa\uff0c\u4e14\u4e32\u884c\u6d4b\u91cf\u6bd4\u5e76\u884c\u6d4b\u91cf\u66f4\u4f18\u3002", "motivation": "\u9ad8\u4fdd\u771f\u5ea6\u91cf\u5b50\u6bd4\u7279\u8bfb\u51fa\u662f\u91cf\u5b50\u8ba1\u7b97\u7684\u57fa\u7840\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63d0\u9ad8\u901a\u91cf\u91cf\u5b50\u6bd4\u7279\u7684\u6d4b\u91cf\u4fdd\u771f\u5ea6\u3002", "method": "\u7814\u7a76\u4e86\u901a\u8fc7\u91cf\u5b50\u901a\u91cf\u53c2\u91cf\u8ba1\u4ecb\u5bfc\u7684\u8bfb\u51fa\u65b9\u6848\uff0c\u5e76\u8fdb\u884c\u4e86\u7406\u8bba\u5efa\u6a21\u548c\u6570\u503c\u6a21\u62df\uff0c\u5206\u6790\u4e86\u4e0d\u540c\u6d4b\u91cf\u57fa\u5728\u5355\u91cf\u5b50\u6bd4\u7279\u548c\u8026\u5408\u53cc\u91cf\u5b50\u6bd4\u7279\u7cfb\u7edf\u4e2d\u7684\u4fdd\u771f\u5ea6\u5f71\u54cd\uff0c\u5e76\u6bd4\u8f83\u4e86\u4e32\u884c\u548c\u5e76\u884c\u4e24\u79cd\u6d4b\u91cf\u6a21\u578b\u3002", "result": "\u5728\u5355\u91cf\u5b50\u6bd4\u7279\u7cfb\u7edf\u4e2d\uff0c\u80fd\u91cf\u57fa\u7684\u4fdd\u771f\u5ea6\u9ad8\u4e8e\u901a\u91cf\u57fa\u3002\u5728\u8026\u5408\u53cc\u91cf\u5b50\u6bd4\u7279\u7cfb\u7edf\u4e2d\uff0c\u4e32\u884c\u6d4b\u91cf\u6216\u5e76\u884c\u6d4b\u91cf\u5747\u53ef\u8bfb\u51fa\u5355\u4e2a\u76ee\u6807\u91cf\u5b50\u6bd4\u7279\uff0c\u5176\u4e2d\u4e32\u884c\u6d4b\u91cf\u5728\u8f83\u957f\u65f6\u95f4\u548c\u8026\u5408\u57fa\u4e0b\u4fdd\u771f\u5ea6\u66f4\u9ad8\uff0c\u800c\u5e76\u884c\u6d4b\u91cf\u5728\u8f83\u77ed\u65f6\u95f4\u548c\u88f8\u57fa\u4e0b\u4fdd\u771f\u5ea6\u66f4\u9ad8\u3002\u4f46\u603b\u4f53\u800c\u8a00\uff0c\u4e32\u884c\u6d4b\u91cf\u6bd4\u5e76\u884c\u6d4b\u91cf\u5177\u6709\u66f4\u4f18\u8d8a\u3001\u66f4\u7a33\u5b9a\u7684\u4fdd\u771f\u5ea6\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u91cf\u5316\u4e86\u53ef\u5b9e\u73b0\u7684\u4fdd\u771f\u5ea6\uff0c\u4e3a\u4f18\u5316\u91cf\u5b50\u8ba1\u7b97\u67b6\u6784\u4e2d\u7684\u6d4b\u91cf\u534f\u8bae\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2510.24350", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.24350", "abs": "https://arxiv.org/abs/2510.24350", "authors": ["Yiming Zhu", "Zhuhong Zhu", "Xiaodong Xu", "Hongwei Hou", "Wenjin Wang", "Rui Ding"], "title": "Achieving Constant-Envelope Waveform in CP-OFDMA Framework", "comment": "This work will be submitted to the IEEE for possible publication", "summary": "OFDM is widely adopted in modern wireless communication systems, but its\npower efficiency is limited by high envelope fluctuations. Although various\nhigh power-efficiency waveforms have been proposed, most are incompatible with\nthe CP-OFDMA framework and remain ineffective in multi-user downlink\ntransmissions. To address this issue, we propose a constant-envelope (CE)\nwaveform design, which enables low-complexity transceiver architectures while\nmaintaining full compatibility with the prevailing CP-OFDMA framework.\nSpecifically, we start from a general CE FDMA signal model and develop a\nCP-OFDMA-compatible waveform implementation structure, followed by the design\nof an optimized CE-constrained pulse-shaping filter to suppress out-of-band\nemissions. To tackle channel estimation challenge under non-flat\nfrequency-domain pilots induced by CE modulation, we optimize the time-domain\nbinary pilot sequence to achieve frequency-domain CE properties, and then\npropose a multi-stage method combining delay-domain denoising with power delay\nprofile estimation to facilitate reduced-dimension LMMSE estimation.\nSubsequently, we design a low-complexity maximum ratio combining-aided LMMSE\nequalizer by exploiting the periodicity and conjugate symmetry of the CE\nreceived signals. To mitigate the downlink peak-to-average power ratio increase\ncaused by FDMA, we further develop a multi-user downlink CE transmission scheme\nincluding multiple access mechanism, downlink control information design, and\ncorresponding system-level implementation, which ensures compatibility with the\nNew Radio standard. Numerical results demonstrate that the proposed scheme\nachieves bit error rate performance close to the ideal case while significantly\nreducing transceiver complexity compared to existing CE waveform solutions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6052\u5305\u7edc\uff08CE\uff09\u6ce2\u5f62\u8bbe\u8ba1\uff0c\u4e0eCP-OFDMA\u6846\u67b6\u517c\u5bb9\uff0c\u9002\u7528\u4e8e\u591a\u7528\u6237\u4e0b\u884c\u4f20\u8f93\uff0c\u5b9e\u73b0\u4e86\u4f4e\u590d\u6742\u5ea6\u8bbe\u8ba1\u548c\u63a5\u8fd1\u7406\u60f3\u7684\u6bd4\u7279\u8bef\u5dee\u7387\u6027\u80fd\u3002", "motivation": "OFDM\u7cfb\u7edf\u867d\u7136\u88ab\u5e7f\u6cdb\u91c7\u7528\uff0c\u4f46\u5176\u529f\u7387\u6548\u7387\u53d7\u5230\u9ad8\u5305\u7edc\u6ce2\u52a8\u7684\u9650\u5236\u3002\u73b0\u6709\u7684\u9ad8\u529f\u7387\u6548\u7387\u6ce2\u5f62\u5927\u591a\u4e0eCP-OFDMA\u6846\u67b6\u4e0d\u517c\u5bb9\uff0c\u5e76\u4e14\u5728\u591a\u7528\u6237\u4e0b\u884c\u4f20\u8f93\u4e2d\u6548\u679c\u4e0d\u4f73\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u6052\u5305\u7edc\uff08CE\uff09\u6ce2\u5f62\uff0c\u4fdd\u6301\u4e86\u4e0eCP-OFDMA\u6846\u67b6\u7684\u5b8c\u5168\u517c\u5bb9\u6027\uff0c\u5e76\u5b9e\u73b0\u4e86\u4f4e\u590d\u6742\u5ea6\u7684\u6536\u53d1\u5668\u67b6\u6784\u3002\u5177\u4f53\u5305\u62ec\uff1a1. \u8bbe\u8ba1\u4f18\u5316\u7684CE\u7ea6\u675f\u8109\u51b2\u6574\u5f62\u6ee4\u6ce2\u5668\u4ee5\u6291\u5236\u5e26\u5916\u53d1\u5c04\u30022. \u4f18\u5316\u65f6\u57df\u4e8c\u5143\u5bfc\u9891\u5e8f\u5217\u4ee5\u5b9e\u73b0\u9891\u57dfCE\u7279\u6027\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u5ef6\u8fdf\u57df\u53bb\u566a\u548c\u529f\u7387\u5ef6\u8fdf\u5206\u5e03\u4f30\u8ba1\u7684\u591a\u9636\u6bb5\u65b9\u6cd5\uff0c\u4ee5\u7b80\u5316LMMSE\u4f30\u8ba1\u30023. \u8bbe\u8ba1\u4e00\u79cd\u4f4e\u590d\u6742\u5ea6\u6700\u5927\u6bd4\u5408\u5e76\u8f85\u52a9LMMSE\u5747\u8861\u5668\u30024. \u63d0\u51fa\u4e00\u79cd\u591a\u7528\u6237\u4e0b\u884cCE\u4f20\u8f93\u65b9\u6848\uff0c\u5305\u62ec\u591a\u5740\u63a5\u5165\u673a\u5236\u3001\u4e0b\u884c\u63a7\u5236\u4fe1\u606f\u8bbe\u8ba1\u548c\u7cfb\u7edf\u7ea7\u5b9e\u73b0\uff0c\u786e\u4fdd\u4e0eNew Radio\u6807\u51c6\u7684\u517c\u5bb9\u6027\u3002", "result": "\u63d0\u51fa\u7684CE\u6ce2\u5f62\u8bbe\u8ba1\u4e0eCP-OFDMA\u6846\u67b6\u517c\u5bb9\uff0c\u5b9e\u73b0\u4e86\u4f4e\u590d\u6742\u5ea6\u7684\u6536\u53d1\u5668\u67b6\u6784\u3002\u5728\u4fe1\u9053\u4f30\u8ba1\u65b9\u9762\uff0c\u901a\u8fc7\u4f18\u5316\u5bfc\u9891\u5e8f\u5217\u548c\u591a\u9636\u6bb5\u65b9\u6cd5\uff0c\u7b80\u5316\u4e86LMMSE\u4f30\u8ba1\u3002\u63d0\u51fa\u7684\u5747\u8861\u5668\u4e5f\u964d\u4f4e\u4e86\u590d\u6742\u5ea6\u3002\u591a\u7528\u6237\u4e0b\u884c\u4f20\u8f93\u65b9\u6848\u786e\u4fdd\u4e86\u4e0eNew Radio\u6807\u51c6\u7684\u517c\u5bb9\u6027\u3002\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6848\u7684\u6bd4\u7279\u8bef\u5dee\u7387\u6027\u80fd\u63a5\u8fd1\u7406\u60f3\u60c5\u51b5\uff0c\u4e14\u4e0e\u73b0\u6709\u7684CE\u6ce2\u5f62\u89e3\u51b3\u65b9\u6848\u76f8\u6bd4\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u6536\u53d1\u5668\u590d\u6742\u5ea6\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u6052\u5305\u7edc\uff08CE\uff09\u6ce2\u5f62\u8bbe\u8ba1\uff0c\u5728\u4fdd\u6301\u4e0eCP-OFDMA\u6846\u67b6\u517c\u5bb9\u7684\u540c\u65f6\uff0c\u6709\u6548\u89e3\u51b3\u4e86OFDM\u7cfb\u7edf\u529f\u7387\u6548\u7387\u548c\u591a\u7528\u6237\u4e0b\u884c\u4f20\u8f93\u7684\u6311\u6218\u3002\u8be5\u65b9\u6848\u901a\u8fc7\u4f18\u5316\u7684\u6ee4\u6ce2\u5668\u8bbe\u8ba1\u3001\u4fe1\u9053\u4f30\u8ba1\u7b97\u6cd5\u3001\u5747\u8861\u5668\u4ee5\u53ca\u7cfb\u7edf\u7ea7\u4f20\u8f93\u65b9\u6848\uff0c\u5b9e\u73b0\u4e86\u4f4e\u590d\u6742\u5ea6\u6536\u53d1\u5668\u548c\u4f18\u8d8a\u7684\u6bd4\u7279\u8bef\u5dee\u7387\u6027\u80fd\uff0c\u4e3a\u73b0\u4ee3\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.23641", "categories": ["cs.LG", "cs.AI", "hep-ex", "physics.ins-det"], "pdf": "https://arxiv.org/pdf/2510.23641", "abs": "https://arxiv.org/abs/2510.23641", "authors": ["Aaron Wang", "Zihan Zhao", "Subash Katel", "Vivekanand Gyanchand Sahu", "Elham E Khoda", "Abhijith Gandrakota", "Jennifer Ngadiuba", "Richard Cavanaugh", "Javier Duarte"], "title": "Spatially Aware Linear Transformer (SAL-T) for Particle Jet Tagging", "comment": null, "summary": "Transformers are very effective in capturing both global and local\ncorrelations within high-energy particle collisions, but they present\ndeployment challenges in high-data-throughput environments, such as the CERN\nLHC. The quadratic complexity of transformer models demands substantial\nresources and increases latency during inference. In order to address these\nissues, we introduce the Spatially Aware Linear Transformer (SAL-T), a\nphysics-inspired enhancement of the linformer architecture that maintains\nlinear attention. Our method incorporates spatially aware partitioning of\nparticles based on kinematic features, thereby computing attention between\nregions of physical significance. Additionally, we employ convolutional layers\nto capture local correlations, informed by insights from jet physics. In\naddition to outperforming the standard linformer in jet classification tasks,\nSAL-T also achieves classification results comparable to full-attention\ntransformers, while using considerably fewer resources with lower latency\nduring inference. Experiments on a generic point cloud classification dataset\n(ModelNet10) further confirm this trend. Our code is available at\nhttps://github.com/aaronw5/SAL-T4HEP.", "AI": {"tldr": "SAL-T\u662f\u4e00\u79cd\u6539\u8fdb\u7684\u7ebf\u6027Transformer\u6a21\u578b\uff0c\u901a\u8fc7\u7a7a\u95f4\u5206\u533a\u548c\u5377\u79ef\u5c42\u6765\u5904\u7406\u9ad8\u80fd\u7c92\u5b50\u78b0\u649e\u6570\u636e\uff0c\u63d0\u9ad8\u4e86\u6548\u7387\u5e76\u4fdd\u6301\u4e86\u51c6\u786e\u6027\u3002", "motivation": "\u6807\u51c6\u7684Transformer\u6a21\u578b\u5728\u5904\u7406\u9ad8\u80fd\u7c92\u5b50\u78b0\u649e\u6570\u636e\u65f6\u5b58\u5728\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u5ef6\u8fdf\u95ee\u9898\uff0c\u96be\u4ee5\u5728\u6570\u636e\u541e\u5410\u91cf\u5927\u7684\u73af\u5883\u4e2d\u90e8\u7f72\u3002", "method": "\u63d0\u51faSpatially Aware Linear Transformer (SAL-T)\u6a21\u578b\uff0c\u7ed3\u5408\u4e86\u7269\u7406\u542f\u53d1\u7684\u7a7a\u95f4\u611f\u77e5\u5206\u533a\u548c\u5377\u79ef\u5c42\u6765\u6355\u6349\u5c40\u90e8\u548c\u5168\u5c40\u76f8\u5173\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u7ebf\u6027\u6ce8\u610f\u529b\u673a\u5236\u3002", "result": "SAL-T\u5728\u7c92\u5b50\u78b0\u649e\u7684\u55b7\u6ce8\u5206\u7c7b\u4efb\u52a1\u4e0a\u4f18\u4e8e\u6807\u51c6\u7684\u7ebf\u6027Transformer\uff0c\u5e76\u4e14\u5728\u5206\u7c7b\u51c6\u786e\u6027\u4e0a\u4e0e\u5168\u6ce8\u610f\u529bTransformer\u76f8\u5f53\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8d44\u6e90\u6d88\u8017\u548c\u63a8\u7406\u5ef6\u8fdf\u3002\u5728ModelNet10\u6570\u636e\u96c6\u4e0a\u4e5f\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "SAL-T\u6210\u529f\u89e3\u51b3\u4e86Transformer\u5728\u9ad8\u541e\u5410\u91cf\u73af\u5883\u4e0b\u7684\u90e8\u7f72\u6311\u6218\uff0c\u4e3a\u9ad8\u80fd\u7269\u7406\u5b66\u4e2d\u7684\u6570\u636e\u5206\u6790\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.24003", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24003", "abs": "https://arxiv.org/abs/2510.24003", "authors": ["Mengzhou Sun", "Sendong Zhao", "Jianyu Chen", "Haochun Wang", "Bin Qin"], "title": "META-RAG: Meta-Analysis-Inspired Evidence-Re-Ranking Method for Retrieval-Augmented Generation in Evidence-Based Medicine", "comment": null, "summary": "Evidence-based medicine (EBM) holds a crucial role in clinical application.\nGiven suitable medical articles, doctors effectively reduce the incidence of\nmisdiagnoses. Researchers find it efficient to use large language models (LLMs)\ntechniques like RAG for EBM tasks. However, the EBM maintains stringent\nrequirements for evidence, and RAG applications in EBM struggle to efficiently\ndistinguish high-quality evidence. Therefore, inspired by the meta-analysis\nused in EBM, we provide a new method to re-rank and filter the medical\nevidence. This method presents multiple principles to filter the best evidence\nfor LLMs to diagnose. We employ a combination of several EBM methods to emulate\nthe meta-analysis, which includes reliability analysis, heterogeneity analysis,\nand extrapolation analysis. These processes allow the users to retrieve the\nbest medical evidence for the LLMs. Ultimately, we evaluate these high-quality\narticles and show an accuracy improvement of up to 11.4% in our experiments and\nresults. Our method successfully enables RAG to extract higher-quality and more\nreliable evidence from the PubMed dataset. This work can reduce the infusion of\nincorrect knowledge into responses and help users receive more effective\nreplies.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u6539\u8fdb\u57fa\u4e8e\u5faa\u8bc1\u533b\u5b66\uff08EBM\uff09\u7684\u4efb\u52a1\u4e2d\u68c0\u7d22\u5230\u7684\u8bc1\u636e\u8d28\u91cf\uff0c\u901a\u8fc7\u6a21\u62df\u5143\u5206\u6790\u6765\u91cd\u65b0\u6392\u5e8f\u548c\u8fc7\u6ee4\u533b\u7597\u8bc1\u636e\uff0c\u63d0\u9ad8\u4e86LLM\u7684\u8bca\u65ad\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u5faa\u8bc1\u533b\u5b66\uff08EBM\uff09\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6280\u672f\u5728\u533a\u5206\u9ad8\u8d28\u91cf\u8bc1\u636e\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5bfc\u81f4LLM\u5728EBM\u4efb\u52a1\u4e2d\u53ef\u80fd\u56e0\u8bc1\u636e\u8d28\u91cf\u4e0d\u9ad8\u800c\u8bef\u8bca\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u62df\u5143\u5206\u6790\u7684\u65b0\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u53ef\u9760\u6027\u5206\u6790\u3001\u5f02\u8d28\u6027\u5206\u6790\u548c\u5916\u63a8\u5206\u6790\uff0c\u5bf9\u533b\u7597\u8bc1\u636e\u8fdb\u884c\u91cd\u65b0\u6392\u5e8f\u548c\u8fc7\u6ee4\uff0c\u4ee5\u63d0\u9ad8\u8bc1\u636e\u8d28\u91cf\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u663e\u8457\u63d0\u9ad8LLM\u5728EBM\u4efb\u52a1\u4e2d\u7684\u51c6\u786e\u6027\uff0c\u6700\u9ad8\u53ef\u8fbe11.4%\uff0c\u5e76\u6210\u529f\u4ecePubMed\u6570\u636e\u96c6\u4e2d\u63d0\u53d6\u4e86\u66f4\u9ad8\u8d28\u91cf\u3001\u66f4\u53ef\u9760\u7684\u8bc1\u636e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u63d0\u9ad8RAG\u4ecePubMed\u6570\u636e\u96c6\u4e2d\u63d0\u53d6\u8bc1\u636e\u7684\u8d28\u91cf\u548c\u53ef\u9760\u6027\uff0c\u51cf\u5c11\u9519\u8bef\u4fe1\u606f\u6ce8\u5165\uff0c\u4ece\u800c\u5e2e\u52a9\u7528\u6237\u83b7\u5f97\u66f4\u6709\u6548\u7684\u56de\u590d\uff0c\u5e76\u63d0\u9ad8LLM\u5728EBM\u4efb\u52a1\u4e2d\u7684\u8bca\u65ad\u51c6\u786e\u6027\u3002"}}
{"id": "2510.24257", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24257", "abs": "https://arxiv.org/abs/2510.24257", "authors": ["Ziqi Ma", "Changda Tian", "Yue Gao"], "title": "Manipulate as Human: Learning Task-oriented Manipulation Skills by Adversarial Motion Priors", "comment": null, "summary": "In recent years, there has been growing interest in developing robots and\nautonomous systems that can interact with human in a more natural and intuitive\nway. One of the key challenges in achieving this goal is to enable these\nsystems to manipulate objects and tools in a manner that is similar to that of\nhumans. In this paper, we propose a novel approach for learning human-style\nmanipulation skills by using adversarial motion priors, which we name HMAMP.\nThe approach leverages adversarial networks to model the complex dynamics of\ntool and object manipulation, as well as the aim of the manipulation task. The\ndiscriminator is trained using a combination of real-world data and simulation\ndata executed by the agent, which is designed to train a policy that generates\nrealistic motion trajectories that match the statistical properties of human\nmotion. We evaluated HMAMP on one challenging manipulation task: hammering, and\nthe results indicate that HMAMP is capable of learning human-style manipulation\nskills that outperform current baseline methods. Additionally, we demonstrate\nthat HMAMP has potential for real-world applications by performing real robot\narm hammering tasks. In general, HMAMP represents a significant step towards\ndeveloping robots and autonomous systems that can interact with humans in a\nmore natural and intuitive way, by learning to manipulate tools and objects in\na manner similar to how humans do.", "AI": {"tldr": "HMAMP\u662f\u4e00\u4e2a\u901a\u8fc7\u5bf9\u6297\u8fd0\u52a8\u5148\u9a8c\u5b66\u4e60\u4eba\u7c7b\u64cd\u7eb5\u6280\u80fd\u7684\u65b0\u65b9\u6cd5\uff0c\u5728\u9524\u51fb\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u5df2\u6210\u529f\u5e94\u7528\u4e8e\u673a\u5668\u4eba\u3002", "motivation": "\u8ba9\u673a\u5668\u4eba\u80fd\u591f\u50cf\u4eba\u7c7b\u4e00\u6837\u81ea\u7136\u76f4\u89c2\u5730\u4e0e\u73af\u5883\u4ea4\u4e92\uff0c\u7279\u522b\u662f\u901a\u8fc7\u5b66\u4e60\u4eba\u7c7b\u98ce\u683c\u7684\u7269\u4f53\u64cd\u7eb5\u6280\u5de7\u3002", "method": "\u5229\u7528\u5bf9\u6297\u7f51\u7edc\u6765\u6a21\u62df\u5de5\u5177\u548c\u7269\u4f53\u64cd\u7eb5\u7684\u590d\u6742\u52a8\u529b\u5b66\u4ee5\u53ca\u64cd\u7eb5\u4efb\u52a1\u7684\u76ee\u6807\u3002\u5224\u522b\u5668\u4f7f\u7528\u771f\u5b9e\u4e16\u754c\u548c\u6a21\u62df\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u4ee5\u751f\u6210\u4e0e\u4eba\u7c7b\u8fd0\u52a8\u7edf\u8ba1\u7279\u6027\u76f8\u5339\u914d\u7684\u903c\u771f\u8fd0\u52a8\u8f68\u8ff9\u3002", "result": "HMAMP \u5728\u9524\u51fb\u4efb\u52a1\u4e0a\u5b66\u4f1a\u4e86\u4eba\u7c7b\u98ce\u683c\u7684\u64cd\u7eb5\u6280\u80fd\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u4e14\u6210\u529f\u5730\u5728\u673a\u5668\u4eba\u624b\u81c2\u4e0a\u8fdb\u884c\u4e86\u771f\u5b9e\u4e16\u754c\u7684\u9524\u51fb\u4efb\u52a1\u3002", "conclusion": "HMAMP \u901a\u8fc7\u5b66\u4e60\u4eba\u7c7b\u64cd\u7eb5\u5de5\u5177\u548c\u7269\u4f53\u7684\u65b9\u5f0f\uff0c\u671d\u7740\u5f00\u53d1\u66f4\u81ea\u7136\u3001\u66f4\u76f4\u89c2\u7684\u4e0e\u4eba\u4ea4\u4e92\u7684\u673a\u5668\u4eba\u548c\u81ea\u4e3b\u7cfb\u7edf\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2510.24564", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2510.24564", "abs": "https://arxiv.org/abs/2510.24564", "authors": ["Prajwal M. Laxmeesha", "Rajesh Dutta", "Rajeev Kumar Rai", "Sharup Sheikh", "Michael F. DiScala", "Uditha M. Jayathilake", "Alexander Veli\u010d", "Tarush Tandon", "Christoph Klewe", "Haile Ambaye", "Timothy Charlton", "Tien-Lin Lee", "Eric A. Stach", "Kemp W. Plumb", "Alexander X. Gray", "Steven J. May"], "title": "Evolution of electronic and magnetic properties in Mn- and Co-alloyed ferromagnetic kagome metal Fe3Sn2", "comment": "23 pages, 4 figures, submitted", "summary": "Kagome metals are an intriguing class of quantum materials as the presence of\nboth flat bands and Dirac points provides access to functional properties\npresent in strongly correlated and topological materials. To fully harness\nthese electronic features, the ability to tune the Fermi level relative to the\nband positions is needed. Here we explore the structural, electronic and\nmagnetic impacts of substitutional alloying within ferromagnetic kagome metal\nFe3Sn2 in thin films grown by molecular beam epitaxy. Transition metals Mn and\nCo are chosen as substitutes for Fe to reduce or increase the d-band electron\ncount, thereby moving the Fermi level accordingly. We find that Co is not\nincorporated into the Fe3Sn2 structure but instead results in a two-phase Fe-Co\nand (Fe,Co)Sn composite. In contrast, Fe3-xMnxSn2 films are realized with x up\nto 1.0, retaining crystalline quality comparable to the parent phase. The\nincorporation of Mn repositions the flat bands relative to the Fermi level in a\nmanner consistent with hole-doping, as revealed by hard x-ray photoemission and\ndensity functional theory. The Fe3-xMnxSn2 films retain room temperature\nferromagnetism, with x-ray magnetic circular dichroism measurements confirming\nthat the Fe and Mn moments are ferromagnetically aligned. The ability to\nhole-dope this magnetic kagome metal provides a platform for tuning properties\nsuch as anomalous Hall and Nernst responses.", "AI": {"tldr": "\u901a\u8fc7\u5728Fe3Sn2\u4e2d\u52a0\u5165Mn\u6765\u8c03\u63a7\u5176\u7535\u5b50\u7ed3\u6784\u548c\u78c1\u6027\uff0c\u5b9e\u73b0\u4e86\u5411\u4e0b\u63ba\u6742\uff0c\u5e76\u4fdd\u6301\u4e86\u5ba4\u6e29\u94c1\u78c1\u6027\u3002", "motivation": "\u4e3a\u4e86\u5145\u5206\u5229\u7528kagome \u91d1\u5c5e\uff08\u7279\u522b\u662fFe3Sn2\uff09\u7684\u5e73\u5e26\u548c\u72c4\u62c9\u514b\u70b9\u7279\u6027\uff0c\u9700\u8981\u8c03\u63a7\u5176\u8d39\u7c73\u80fd\u7ea7\u7684\u4f4d\u7f6e\u3002", "method": "\u91c7\u7528\u5206\u5b50\u675f\u5916\u5ef6\u6280\u672f\u751f\u957f\u4e86Fe3Sn2\u8584\u819c\uff0c\u5e76\u5c1d\u8bd5\u7528Mn\u548cCo\u8fdb\u884c\u8fc7\u6e21\u91d1\u5c5e\u53d6\u4ee3\u3002\u901a\u8fc7\u786cX\u5c04\u7ebf\u5149\u7535\u5b50\u80fd\u8c31\u548c\u5bc6\u5ea6\u6cdb\u51fd\u7406\u8bba\u5206\u6790\u4e86\u63ba\u6742Mn\u5bf9\u6676\u4f53\u7ed3\u6784\u3001\u7535\u5b50\u7ed3\u6784\u548c\u78c1\u6027\u7684\u5f71\u54cd\u3002", "result": "Co\u7684\u63ba\u5165\u672a\u80fd\u5f62\u6210Fe3Sn2\u7ed3\u6784\uff0c\u800c\u662f\u5f62\u6210\u4e86Fe-Co\u548c(Fe,Co)Sn\u7684\u590d\u5408\u76f8\u3002Fe3-xMnxSn2\u8584\u819c\u53ef\u4ee5\u5b9e\u73b0x\u9ad8\u8fbe1.0\uff0c\u4e14\u4fdd\u6301\u4e86\u4e0e\u6bcd\u4f53\u76f8\u76f8\u5f53\u7684\u6676\u4f53\u8d28\u91cf\u3002Mn\u7684\u63ba\u5165\u5bfc\u81f4\u4e86\u5e73\u5e26\u76f8\u5bf9\u4e8e\u8d39\u7c73\u80fd\u7ea7\u7684\u79fb\u52a8\uff0c\u5448\u73b0\u51fa\u7a7a\u7a74\u63ba\u6742\u7684\u7279\u5f81\u3002Fe3-xMnxSn2\u8584\u819c\u4fdd\u6301\u4e86\u5ba4\u6e29\u94c1\u78c1\u6027\uff0c\u5e76\u4e14Fe\u548cMn\u7684\u78c1\u77e9\u662f\u94c1\u78c1\u6709\u5e8f\u7684\u3002", "conclusion": "Fe3-xMnxSn2\u8584\u819c\u7684\u5236\u5907\u6210\u529f\u4e3a\u8c03\u63a7kagome \u91d1\u5c5e\u7684\u6027\u8d28\uff08\u5982\u53cd\u5e38\u970d\u5c14\u6548\u5e94\u548c\u80fd\u65af\u7279\u6548\u5e94\uff09\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5e73\u53f0\u3002"}}
{"id": "2510.24010", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24010", "abs": "https://arxiv.org/abs/2510.24010", "authors": ["Mirali Purohit", "Bimal Gajera", "Vatsal Malaviya", "Irish Mehta", "Kunal Kasodekar", "Jacob Adler", "Steven Lu", "Umaa Rebbapragada", "Hannah Kerner"], "title": "Mars-Bench: A Benchmark for Evaluating Foundation Models for Mars Science Tasks", "comment": "Accepted at NeurIPS 2025", "summary": "Foundation models have enabled rapid progress across many specialized domains\nby leveraging large-scale pre-training on unlabeled data, demonstrating strong\ngeneralization to a variety of downstream tasks. While such models have gained\nsignificant attention in fields like Earth Observation, their application to\nMars science remains limited. A key enabler of progress in other domains has\nbeen the availability of standardized benchmarks that support systematic\nevaluation. In contrast, Mars science lacks such benchmarks and standardized\nevaluation frameworks, which have limited progress toward developing foundation\nmodels for Martian tasks. To address this gap, we introduce Mars-Bench, the\nfirst benchmark designed to systematically evaluate models across a broad range\nof Mars-related tasks using both orbital and surface imagery. Mars-Bench\ncomprises 20 datasets spanning classification, segmentation, and object\ndetection, focused on key geologic features such as craters, cones, boulders,\nand frost. We provide standardized, ready-to-use datasets and baseline\nevaluations using models pre-trained on natural images, Earth satellite data,\nand state-of-the-art vision-language models. Results from all analyses suggest\nthat Mars-specific foundation models may offer advantages over general-domain\ncounterparts, motivating further exploration of domain-adapted pre-training.\nMars-Bench aims to establish a standardized foundation for developing and\ncomparing machine learning models for Mars science. Our data, models, and code\nare available at: https://mars-bench.github.io/.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86Mars-Bench\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u706b\u661f\u79d1\u5b66\u9886\u57df\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u57fa\u51c6\uff0c\u4ee5\u89e3\u51b3\u7f3a\u4e4f\u6807\u51c6\u5316\u8bc4\u4f30\u6846\u67b6\u7684\u95ee\u9898\u3002", "motivation": "\u706b\u661f\u79d1\u5b66\u9886\u57df\u7f3a\u4e4f\u6807\u51c6\u5316\u57fa\u51c6\u548c\u8bc4\u4f30\u6846\u67b6\uff0c\u8fd9\u963b\u788d\u4e86\u57fa\u7840\u6a21\u578b\u5728\u8be5\u9886\u57df\u7684\u8fdb\u5c55\u3002", "method": "\u63d0\u51faMars-Bench\uff0c\u5305\u542b20\u4e2a\u6db5\u76d6\u5206\u7c7b\u3001\u5206\u5272\u548c\u76ee\u6807\u68c0\u6d4b\u7684\u6570\u636e\u96c6\uff0c\u4f7f\u7528\u706b\u661f\u8f68\u9053\u548c\u5730\u8868\u56fe\u50cf\uff0c\u5e76\u63d0\u4f9b\u4e86\u4f7f\u7528\u81ea\u7136\u56fe\u50cf\u3001\u5730\u7403\u536b\u661f\u6570\u636e\u548c\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\u7684\u6a21\u578b\u7684\u57fa\u7ebf\u8bc4\u4f30\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u706b\u661f\u7279\u5b9a\u57fa\u7840\u6a21\u578b\u53ef\u80fd\u4f18\u4e8e\u901a\u7528\u9886\u57df\u6a21\u578b\uff0c\u8868\u660e\u9886\u57df\u9002\u5e94\u6027\u9884\u8bad\u7ec3\u503c\u5f97\u8fdb\u4e00\u6b65\u7814\u7a76\u3002", "conclusion": "Mars-Bench\u65e8\u5728\u4e3a\u5f00\u53d1\u548c\u6bd4\u8f83\u706b\u661f\u79d1\u5b66\u9886\u57df\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5960\u5b9a\u6807\u51c6\u5316\u57fa\u7840\u3002"}}
{"id": "2510.24099", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.24099", "abs": "https://arxiv.org/abs/2510.24099", "authors": ["S. McKay", "S. R. Parnell", "R. M. Dalgliesh", "N. V. Lavrik", "I. I. Kravchenko", "Q. Le Thien", "D. V. Baxter", "G. Ortiz", "R. Pynn"], "title": "Topological shaping of vortex neutron beams using forked phase gratings", "comment": null, "summary": "Beams of light or matter that carry well-defined states of orbital angular\nmomentum (OAM) are promising probes of topological and textured condensed\nmatter systems such as magnetic skyrmions. Using spin-echo small-angle neutron\nscattering (SESANS), we demonstrate the production of vortex neutron beams from\nforked phase gratings of various topological charges. In contrast to some\nprevious techniques used to verify OAM production, SESANS is a more precise\nmeasurement of the neutron's OAM as it is a phase-sensitive, interferometric\ntechnique that directly measures the phase between the scattered neutron spin\nstates.", "AI": {"tldr": "\u5229\u7528\u81ea\u65cb\u56de\u6ce2\u5c0f\u89d2\u4e2d\u5b50\u6563\u5c04\uff08SESANS\uff09\u6280\u672f\uff0c\u901a\u8fc7\u5206\u53c9\u76f8\u5149\u6805\u4ea7\u751f\u5177\u6709\u62d3\u6251\u8377\u7684\u6da1\u65cb\u4e2d\u5b50\u675f\uff0c\u5e76\u7cbe\u786e\u6d4b\u91cf\u4e2d\u5b50\u7684\u8f68\u9053\u89d2\u52a8\u91cf\uff08OAM\uff09\u3002", "motivation": "OAM\u5149\u675f\u53ef\u7528\u4e8e\u63a2\u6d4b\u78c1\u65af\u683c\u660e\u5b50\u7b49\u62d3\u6251\u548c\u7eb9\u7406\u51dd\u805a\u6001\u7cfb\u7edf\u3002", "method": "\u4f7f\u7528\u5177\u6709\u4e0d\u540c\u62d3\u6251\u8377\u7684\u53c9\u5f62\u76f8\u5149\u6805\uff0c\u901a\u8fc7SESANS\u4ea7\u751f\u6da1\u65cb\u4e2d\u5b50\u675f\uff0c\u5e76\u8fdb\u884c\u76f8\u4f4d\u654f\u611f\u7684\u5e72\u6d89\u6d4b\u91cf\uff0c\u4ee5\u7cbe\u786e\u6d4b\u91cf\u4e2d\u5b50\u7684OAM\u3002", "result": "\u6210\u529f\u6f14\u793a\u4e86\u5229\u7528SESANS\u4ea7\u751f\u6da1\u65cb\u4e2d\u5b50\u675f\uff0c\u5e76\u7cbe\u786e\u6d4b\u91cf\u4e86\u5176\u4e2d\u5b50\u7684OAM\u3002", "conclusion": "SESANS\u662f\u4e00\u79cd\u7cbe\u786e\u6d4b\u91cf\u4e2d\u5b50OAM\u7684\u76f8\u4f4d\u654f\u611f\u5e72\u6d89\u6280\u672f\uff0c\u9002\u7528\u4e8e\u63a2\u6d4b\u62d3\u6251\u548c\u7eb9\u7406\u51dd\u805a\u6001\u7cfb\u7edf\u3002"}}
{"id": "2510.24359", "categories": ["cs.AI", "cs.SY", "eess.SY", "q-bio.QM", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.24359", "abs": "https://arxiv.org/abs/2510.24359", "authors": ["Pedram Fard", "Alaleh Azhir", "Neguine Rezaii", "Jiazi Tian", "Hossein Estiri"], "title": "An N-of-1 Artificial Intelligence Ecosystem for Precision Medicine", "comment": "This study has been supported by grants from the National Institutes\n  of Health: The National Institute on Aging R01AG074372 and The National\n  Institute of Allergy and Infectious Diseases R01AI165535", "summary": "Artificial intelligence in medicine is built to serve the average patient. By\nminimizing error across large datasets, most systems deliver strong aggregate\naccuracy yet falter at the margins: patients with rare variants,\nmultimorbidity, or underrepresented demographics. This average patient fallacy\nerodes both equity and trust. We propose a different design: a multi-agent\necosystem for N-of-1 decision support. In this environment, agents clustered by\norgan systems, patient populations, and analytic modalities draw on a shared\nlibrary of models and evidence synthesis tools. Their results converge in a\ncoordination layer that weighs reliability, uncertainty, and data density\nbefore presenting the clinician with a decision-support packet: risk estimates\nbounded by confidence ranges, outlier flags, and linked evidence. Validation\nshifts from population averages to individual reliability, measured by error in\nlow-density regions, calibration in the small, and risk--coverage trade-offs.\nAnticipated challenges include computational demands, automation bias, and\nregulatory fit, addressed through caching strategies, consensus checks, and\nadaptive trial frameworks. By moving from monolithic models to orchestrated\nintelligence, this approach seeks to align medical AI with the first principle\nof medicine: care that is transparent, equitable, and centered on the\nindividual.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aN-of-1\u51b3\u7b56\u652f\u6301\u7684\u591a\u4ee3\u7406\u7cfb\u7edf\uff0c\u65e8\u5728\u89e3\u51b3\u5f53\u524d\u533b\u7597AI\u5728\u670d\u52a1\u5e73\u5747\u60a3\u8005\u65f6\u5ffd\u7565\u8fb9\u7f18\u5316\u4eba\u7fa4\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u6574\u5408\u4e0d\u540c\u9886\u57df\u7684\u4ee3\u7406\u548c\u534f\u8c03\u5c42\uff0c\u4e3a\u533b\u751f\u63d0\u4f9b\u66f4\u4e2a\u4f53\u5316\u3001\u66f4\u53ef\u9760\u7684\u533b\u7597\u51b3\u7b56\u652f\u6301\uff0c\u5e76\u8ba8\u8bba\u4e86\u76f8\u5173\u7684\u6311\u6218\u548c\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u73b0\u6709\u533b\u7597AI\u4e3b\u8981\u670d\u52a1\u4e8e\u5e73\u5747\u60a3\u8005\uff0c\u5728\u5904\u7406\u7f55\u89c1\u75c5\u3001\u591a\u91cd\u75be\u75c5\u6216\u4ee3\u8868\u6027\u4e0d\u8db3\u4eba\u7fa4\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u5bfc\u81f4\u533b\u7597\u516c\u5e73\u6027\u548c\u4fe1\u4efb\u5ea6\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u6784\u5efa\u4e00\u4e2a\u591a\u4ee3\u7406\u751f\u6001\u7cfb\u7edf\uff0c\u5176\u4e2d\u5305\u542b\u6309\u5668\u5b98\u7cfb\u7edf\u3001\u60a3\u8005\u4eba\u7fa4\u548c\u5206\u6790\u6a21\u5f0f\u5212\u5206\u7684\u4ee3\u7406\u3002\u8fd9\u4e9b\u4ee3\u7406\u5171\u4eab\u6a21\u578b\u548c\u8bc1\u636e\u5408\u6210\u5de5\u5177\u5e93\uff0c\u5e76\u901a\u8fc7\u4e00\u4e2a\u534f\u8c03\u5c42\u6574\u5408\u7ed3\u679c\uff0c\u8be5\u5c42\u4f1a\u8bc4\u4f30\u53ef\u9760\u6027\u3001\u4e0d\u786e\u5b9a\u6027\u548c\u6570\u636e\u5bc6\u5ea6\uff0c\u6700\u7ec8\u4e3a\u533b\u751f\u63d0\u4f9b\u5305\u542b\u98ce\u9669\u4f30\u8ba1\u3001\u7f6e\u4fe1\u533a\u95f4\u3001\u5f02\u5e38\u503c\u6807\u5fd7\u548c\u76f8\u5173\u8bc1\u636e\u7684\u51b3\u7b56\u652f\u6301\u5305\u3002", "result": "\u5c06\u9a8c\u8bc1\u65b9\u6cd5\u4ece\u57fa\u4e8e\u603b\u4f53\u5e73\u5747\u503c\u8f6c\u5411\u57fa\u4e8e\u4e2a\u4f53\u53ef\u9760\u6027\uff0c\u901a\u8fc7\u5728\u4f4e\u6570\u636e\u5bc6\u5ea6\u533a\u57df\u7684\u9519\u8bef\u7387\u3001\u5c0f\u6837\u672c\u6821\u51c6\u4ee5\u53ca\u98ce\u9669-\u8986\u76d6\u6743\u8861\u6765\u8861\u91cf\u3002", "conclusion": "\u901a\u8fc7\u5c06\u5355\u4e00\u6a21\u578b\u8f6c\u53d8\u4e3a\u534f\u540c\u667a\u80fd\uff0c\u8be5\u65b9\u6cd5\u65e8\u5728\u4f7f\u533b\u7597AI\u7b26\u5408\u533b\u5b66\u7684\u9996\u8981\u539f\u5219\uff1a\u63d0\u4f9b\u900f\u660e\u3001\u516c\u5e73\u4e14\u4ee5\u4e2a\u4f53\u4e3a\u4e2d\u5fc3\u7684\u533b\u7597\u670d\u52a1\u3002\u540c\u65f6\uff0c\u7814\u7a76\u4e5f\u9884\u89c1\u4e86\u8ba1\u7b97\u9700\u6c42\u3001\u81ea\u52a8\u5316\u504f\u89c1\u548c\u76d1\u7ba1\u9002\u5e94\u6027\u7b49\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u7f13\u5b58\u7b56\u7565\u3001\u5171\u8bc6\u68c0\u67e5\u548c\u9002\u5e94\u6027\u8bd5\u9a8c\u6846\u67b6\u7b49\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.24400", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.24400", "abs": "https://arxiv.org/abs/2510.24400", "authors": ["Francisco D\u00edaz-Ruiz", "Francisco J. Mart\u00edn-Vega", "Jos\u00e9 Antonio Cort\u00e9s", "Gerardo G\u00f3mez", "Mari Carmen Aguayo-Torres"], "title": "Deep Learning-Based CSI Prediction Framework for Channel Aging Mitigation in TDD 5G Systems", "comment": null, "summary": "Time division duplexing (TDD) has become the dominant duplexing mode in 5G\nand beyond due to its ability to exploit channel reciprocity for efficient\ndownlink channel state information (CSI) acquisition. However, channel aging\ncaused by user mobility and processing delays degrades the accuracy of CSI,\nleading to suboptimal link adaptation and loss of performance. To address this\nissue, we propose a learning-based CSI prediction framework that leverages\ntemporal correlations in wireless channels to forecast future signal to\ninterference plus noise ratio (SINR) values. The prediction operates in the\neffective SINR domain, obtained via exponential effective SINR mapping (EESM),\nensuring full compatibility with existing 5G standards without requiring\ncontinuous pilot signaling. Two models are considered: a fully connected deep\nneural network (DNN) and an long short-term memory (LSTM)-based network. The\nsimulation results show that the LSTM predictor achieves an improvement of up\nto 2 dB in normalized mean squared error (NMSE) and a gain of up to 1.2 Mbps\nthroughput over a baseline without prediction under moderate Doppler\nconditions. These results confirm the potential of lightweight AI-based CSI\nprediction to effectively mitigate channel aging and enhance link adaptation in\nTDD 5G systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684CSI\u9884\u6d4b\u6846\u67b6\uff0c\u4ee5\u89e3\u51b35G TDD\u7cfb\u7edf\u4e2d\u7531\u4e8e\u4fe1\u9053\u8001\u5316\u5bfc\u81f4\u7684CSI\u7cbe\u5ea6\u4e0b\u964d\u95ee\u9898\uff0c\u5e76\u53d6\u5f97\u4e86\u826f\u597d\u7684\u4eff\u771f\u7ed3\u679c\u3002", "motivation": "5G TDD\u7cfb\u7edf\u5229\u7528\u4fe1\u9053\u4e92\u6613\u6027\u8fdb\u884cCSI\u83b7\u53d6\uff0c\u4f46\u7528\u6237\u79fb\u52a8\u6027\u548c\u5904\u7406\u5ef6\u8fdf\u4f1a\u5bfc\u81f4\u4fe1\u9053\u8001\u5316\uff0c\u964d\u4f4eCSI\u7cbe\u5ea6\uff0c\u4ece\u800c\u5f71\u54cd\u94fe\u8def\u81ea\u9002\u5e94\u548c\u7cfb\u7edf\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684CSI\u9884\u6d4b\u6846\u67b6\uff0c\u5229\u7528\u65e0\u7ebf\u4fe1\u9053\u7684\u65f6\u95f4\u76f8\u5173\u6027\u6765\u9884\u6d4b\u672a\u6765\u7684SINR\u503c\u3002\u9884\u6d4b\u5728\u6709\u6548\u7684SINR\u57df\u5185\u8fdb\u884c\uff0c\u8be5\u57df\u901a\u8fc7\u6307\u6570\u6709\u6548SINR\u6620\u5c04\uff08EESM\uff09\u83b7\u5f97\uff0c\u786e\u4fdd\u4e0e\u73b0\u67095G\u6807\u51c6\u517c\u5bb9\uff0c\u4e14\u65e0\u9700\u6301\u7eed\u7684\u5bfc\u9891\u4fe1\u4ee4\u3002\u8003\u8651\u4e86\u4e24\u79cd\u6a21\u578b\uff1a\u5168\u8fde\u63a5\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNN\uff09\u548c\u57fa\u4e8e\u957f\u77ed\u671f\u8bb0\u5fc6\uff08LSTM\uff09\u7684\u7f51\u7edc\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u5728\u9002\u5ea6\u7684\u591a\u666e\u52d2\u6761\u4ef6\u4e0b\uff0cLSTM\u9884\u6d4b\u5668\u5728\u5f52\u4e00\u5316\u5747\u65b9\u8bef\u5dee\uff08NMSE\uff09\u65b9\u9762\u63d0\u9ad8\u4e86\u9ad8\u8fbe2 dB\uff0c\u5728\u541e\u5410\u91cf\u65b9\u9762\u6bd4\u57fa\u7ebf\u63d0\u9ad8\u4e861.2 Mbps\u3002", "conclusion": "\u8f7b\u91cf\u7ea7\u7684\u57fa\u4e8eAI\u7684CSI\u9884\u6d4b\u53ef\u4ee5\u6709\u6548\u5730\u7f13\u89e3\u4fe1\u9053\u8001\u5316\uff0c\u5e76\u589e\u5f3aTDD 5G\u7cfb\u7edf\u4e2d\u7684\u94fe\u8def\u81ea\u9002\u5e94\u3002"}}
{"id": "2510.23649", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23649", "abs": "https://arxiv.org/abs/2510.23649", "authors": ["Tenghui Li", "Guoxu Zhou", "Xuyang Zhao", "Yuning Qiu", "Qibin Zhao"], "title": "Efficient Low Rank Attention for Long-Context Inference in Large Language Models", "comment": null, "summary": "As the length of input text grows, the key-value (KV) cache in LLMs imposes\nprohibitive GPU memory costs and limits long-context inference on resource\nconstrained devices. Existing approaches, such as KV quantization and pruning,\nreduce memory usage but suffer from numerical precision loss or suboptimal\nretention of key-value pairs. We introduce Low Rank Query and Key attention\n(LRQK), a two-stage framework that jointly decomposes the full-precision query\nand key matrices into compact rank-\\(r\\) factors during the prefill stage, and\nthen uses these low-dimensional projections to compute proxy attention scores\nin \\(\\mathcal{O}(lr)\\) time at each decode step. By selecting only the\ntop-\\(k\\) tokens and a small fixed set of recent tokens, LRQK employs a mixed\nGPU-CPU cache with a hit-and-miss mechanism that transfers only missing\nfull-precision KV pairs, thereby preserving exact attention outputs while\nreducing CPU-GPU data movement. Extensive experiments on the RULER and\nLongBench benchmarks with LLaMA-3-8B and Qwen2.5-7B demonstrate that LRQK\nmatches or surpasses leading sparse-attention methods in long context settings,\nwhile delivering significant memory savings with minimal loss in accuracy. Our\ncode is available at https://github.com/tenghuilee/LRQK.", "AI": {"tldr": "KV\u7f13\u5b58\u7684\u5185\u5b58\u6210\u672c\u9650\u5236\u4e86\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\uff0cLRQK\u901a\u8fc7\u4f4e\u79e9\u5206\u89e3\u548c\u6df7\u5408GPU-CPU\u7f13\u5b58\u6765\u89e3\u51b3\u6b64\u95ee\u9898\u3002", "motivation": "KV\u7f13\u5b58\u7684\u5185\u5b58\u6210\u672c\u9650\u5236\u4e86\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\uff0c\u5c24\u5176\u662f\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8bbe\u5907\u4e0a\u3002\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u7cbe\u5ea6\u635f\u5931\u6216\u6b21\u4f18\u4fdd\u7559\u95ee\u9898\u3002", "method": "LRQK\u662f\u4e00\u79cd\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u5728\u9884\u586b\u5145\u9636\u6bb5\u5c06\u67e5\u8be2\u548c\u952e\u77e9\u9635\u5206\u89e3\u4e3a\u4f4e\u79e9\u56e0\u5b50\uff0c\u5e76\u4f7f\u7528\u8fd9\u4e9b\u4f4e\u7ef4\u6295\u5f71\u5728\u89e3\u7801\u9636\u6bb5\u4ee5O(lr)\u65f6\u95f4\u8ba1\u7b97\u6ce8\u610f\u529b\u5206\u6570\u3002\u5b83\u8fd8\u91c7\u7528\u6df7\u5408GPU-CPU\u7f13\u5b58\uff0c\u53ea\u4f20\u8f93\u7f3a\u5931\u7684KV\u5bf9\uff0c\u4ee5\u4fdd\u6301\u7cbe\u786e\u7684\u6ce8\u610f\u529b\u8f93\u51fa\u5e76\u51cf\u5c11CPU-GPU\u6570\u636e\u79fb\u52a8\u3002", "result": "\u5728RULER\u548cLongBench\u57fa\u51c6\u6d4b\u8bd5\u4e0a\uff0cLLaMA-3-8B\u548cQwen2.5-7B\u6a21\u578b\u7684\u5b9e\u9a8c\u8868\u660e\uff0cLRQK\u5728\u957f\u4e0a\u4e0b\u6587\u8bbe\u7f6e\u4e0b\u80fd\u4e0e\u9886\u5148\u7684\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6cd5\u76f8\u5ab2\u7f8e\u6216\u8d85\u8d8a\uff0c\u540c\u65f6\u663e\u8457\u8282\u7701\u5185\u5b58\u4e14\u7cbe\u5ea6\u635f\u5931\u6700\u5c0f\u3002", "conclusion": "LRQK\u901a\u8fc7\u4f4e\u79e9\u5206\u89e3\u548c\u6df7\u5408GPU-CPU\u7f13\u5b58\u6709\u6548\u89e3\u51b3\u4e86\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u4e2d\u7684KV\u7f13\u5b58\u5185\u5b58\u6210\u672c\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5185\u5b58\u8282\u7701\u548c\u9ad8\u7cbe\u5ea6\u7684\u5e73\u8861\u3002"}}
{"id": "2510.24028", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24028", "abs": "https://arxiv.org/abs/2510.24028", "authors": ["Tingyue Pan", "Mingyue Cheng", "Shilong Zhang", "Zhiding Liu", "Xiaoyu Tao", "Yucong Luo", "Jintao Zhang", "Qi Liu"], "title": "OneCast: Structured Decomposition and Modular Generation for Cross-Domain Time Series Forecasting", "comment": null, "summary": "Cross-domain time series forecasting is a valuable task in various web\napplications. Despite its rapid advancement, achieving effective generalization\nacross heterogeneous time series data remains a significant challenge. Existing\nmethods have made progress by extending single-domain models, yet often fall\nshort when facing domain-specific trend shifts and inconsistent periodic\npatterns. We argue that a key limitation lies in treating temporal series as\nundifferentiated sequence, without explicitly decoupling their inherent\nstructural components. To address this, we propose OneCast, a structured and\nmodular forecasting framework that decomposes time series into seasonal and\ntrend components, each modeled through tailored generative pathways.\nSpecifically, the seasonal component is captured by a lightweight projection\nmodule that reconstructs periodic patterns via interpretable basis functions.\nIn parallel, the trend component is encoded into discrete tokens at segment\nlevel via a semantic-aware tokenizer, and subsequently inferred through a\nmasked discrete diffusion mechanism. The outputs from both branches are\ncombined to produce a final forecast that captures seasonal patterns while\ntracking domain-specific trends. Extensive experiments across eight domains\ndemonstrate that OneCast mostly outperforms state-of-the-art baselines.", "AI": {"tldr": "OneCast\u662f\u4e00\u4e2a\u521b\u65b0\u7684\u8de8\u57df\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u65f6\u95f4\u5e8f\u5217\u5206\u89e3\u4e3a\u5b63\u8282\u6027\u548c\u8d8b\u52bf\u6027\u5206\u91cf\uff0c\u5e76\u4e3a\u6bcf\u4e2a\u5206\u91cf\u8bbe\u8ba1\u4e86\u4e13\u95e8\u7684\u751f\u6210\u8def\u5f84\uff0c\u4ece\u800c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u57df\u7279\u5b9a\u8d8b\u52bf\u53d8\u5316\u548c\u4e0d\u4e00\u81f4\u5468\u671f\u6a21\u5f0f\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u5c55\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u8de8\u57df\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u6cd5\u5728\u5904\u7406\u5f02\u6784\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u65f6\uff0c\u7531\u4e8e\u672a\u80fd\u663e\u5f0f\u5206\u79bb\u5176\u56fa\u6709\u7684\u7ed3\u6784\u6210\u5206\uff0c\u5e38\u5728\u5e94\u5bf9\u57df\u7279\u5b9a\u8d8b\u52bf\u53d8\u5316\u548c\u4e0d\u4e00\u81f4\u5468\u671f\u6a21\u5f0f\u65f6\u8868\u73b0\u4e0d\u4f73\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u5173\u952e\u9650\u5236\u3002", "method": "\u63d0\u51fa\u540d\u4e3aOneCast\u7684\u7ed3\u6784\u5316\u3001\u6a21\u5757\u5316\u9884\u6d4b\u6846\u67b6\u3002\u8be5\u6846\u67b6\u5c06\u65f6\u95f4\u5e8f\u5217\u5206\u89e3\u4e3a\u5b63\u8282\u6027\u548c\u8d8b\u52bf\u6027\u5206\u91cf\u3002\u5b63\u8282\u6027\u5206\u91cf\u4f7f\u7528\u8f7b\u91cf\u7ea7\u6295\u5f71\u6a21\u5757\uff0c\u901a\u8fc7\u53ef\u89e3\u91ca\u57fa\u51fd\u6570\u91cd\u6784\u5468\u671f\u6a21\u5f0f\u3002\u8d8b\u52bf\u6027\u5206\u91cf\u901a\u8fc7\u8bed\u4e49\u611f\u77e5\u5206\u8bcd\u5668\u5728\u7247\u6bb5\u7ea7\u522b\u7f16\u7801\u4e3a\u79bb\u6563\u6807\u8bb0\uff0c\u5e76\u5229\u7528\u63a9\u7801\u79bb\u6563\u6269\u6563\u673a\u5236\u8fdb\u884c\u63a8\u7406\u3002\u6700\u540e\u7ed3\u5408\u4e24\u4e2a\u5206\u652f\u7684\u8f93\u51fa\u6765\u751f\u6210\u6700\u7ec8\u9884\u6d4b\u3002", "result": "\u5728\u4e00\u9879\u6d89\u53ca\u516b\u4e2a\u9886\u57df\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u4e2d\uff0cOneCast \u7684\u8868\u73b0\u666e\u904d\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "OneCast\u901a\u8fc7\u663e\u5f0f\u5206\u79bb\u548c\u5206\u522b\u5efa\u6a21\u65f6\u95f4\u5e8f\u5217\u7684\u5b63\u8282\u6027\u548c\u8d8b\u52bf\u6027\u5206\u91cf\uff0c\u6709\u6548\u63d0\u5347\u4e86\u8de8\u57df\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5c24\u5176\u5728\u5904\u7406\u57df\u7279\u5b9a\u7684\u8d8b\u52bf\u548c\u5468\u671f\u6a21\u5f0f\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2510.24014", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24014", "abs": "https://arxiv.org/abs/2510.24014", "authors": ["Yizhu Jiao", "Sha Li", "Sizhe Zhou", "Heng Ji", "Jiawei Han"], "title": "TEXT2DB: Integration-Aware Information Extraction with Large Language Model Agents", "comment": "ACL 2025. Source code: https://github.com/yzjiao/Text2DB", "summary": "The task of information extraction (IE) is to extract structured knowledge\nfrom text. However, it is often not straightforward to utilize IE output due to\nthe mismatch between the IE ontology and the downstream application needs. We\npropose a new formulation of IE TEXT2DB that emphasizes the integration of IE\noutput and the target database (or knowledge base). Given a user instruction, a\ndocument set, and a database, our task requires the model to update the\ndatabase with values from the document set to satisfy the user instruction.\nThis task requires understanding user instructions for what to extract and\nadapting to the given DB/KB schema for how to extract on the fly. To evaluate\nthis new task, we introduce a new benchmark featuring common demands such as\ndata infilling, row population, and column addition. In addition, we propose an\nLLM agent framework OPAL (Observe-PlanAnalyze LLM) which includes an Observer\ncomponent that interacts with the database, the Planner component that\ngenerates a code-based plan with calls to IE models, and the Analyzer component\nthat provides feedback regarding code quality before execution. Experiments\nshow that OPAL can successfully adapt to diverse database schemas by generating\ndifferent code plans and calling the required IE models. We also highlight\ndifficult cases such as dealing with large databases with complex dependencies\nand extraction hallucination, which we believe deserve further investigation.\nSource code: https://github.com/yzjiao/Text2DB", "AI": {"tldr": "TEXT2DB \u662f\u4e00\u4e2a\u65b0\u63d0\u51fa\u7684\u4fe1\u606f\u62bd\u53d6\u4efb\u52a1\uff0c\u65e8\u5728\u5c06\u6587\u672c\u4fe1\u606f\u76f4\u63a5\u66f4\u65b0\u5230\u76ee\u6807\u6570\u636e\u5e93\uff0c\u4ee5\u6ee1\u8db3\u7528\u6237\u6307\u4ee4\u3002\u8be5\u4efb\u52a1\u9700\u8981\u6a21\u578b\u7406\u89e3\u7528\u6237\u6307\u4ee4\u5e76\u6839\u636e\u6570\u636e\u5e93 schema \u52a8\u6001\u8c03\u6574\u62bd\u53d6\u7b56\u7565\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u540d\u4e3a OPAL \u7684 LLM Agent \u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5305\u542b Observer\u3001Planner \u548c Analyzer \u4e09\u4e2a\u7ec4\u4ef6\uff0c\u80fd\u591f\u751f\u6210\u4ee3\u7801\u8ba1\u5212\u5e76\u8c03\u7528\u4fe1\u606f\u62bd\u53d6\u6a21\u578b\u6765\u66f4\u65b0\u6570\u636e\u5e93\u3002\u5b9e\u9a8c\u8bc1\u660e OPAL \u80fd\u591f\u9002\u5e94\u4e0d\u540c\u7684\u6570\u636e\u5e93 schema\uff0c\u4f46\u5904\u7406\u5927\u578b\u590d\u6742\u6570\u636e\u5e93\u548c\u89e3\u51b3\u62bd\u53d6\u5e7b\u89c9\u7b49\u95ee\u9898\u4ecd\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u3002", "motivation": "\u4fe1\u606f\u62bd\u53d6\uff08IE\uff09\u7684\u8f93\u51fa\u5f80\u5f80\u96be\u4ee5\u76f4\u63a5\u7528\u4e8e\u4e0b\u6e38\u5e94\u7528\uff0c\u56e0\u4e3a\u5176\u672c\u4f53\u4e0e\u5e94\u7528\u9700\u6c42\u4e4b\u95f4\u5b58\u5728\u4e0d\u5339\u914d\u3002TEXT2DB \u4efb\u52a1\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u901a\u8fc7\u5c06 IE \u8f93\u51fa\u4e0e\u76ee\u6807\u6570\u636e\u5e93\uff08\u6216\u77e5\u8bc6\u5e93\uff09\u96c6\u6210\u3002", "method": "\u63d0\u51fa TEXT2DB \u4efb\u52a1\uff0c\u4e00\u4e2a\u7528\u6237\u6307\u4ee4\u3001\u6587\u6863\u96c6\u548c\u6570\u636e\u5e93\u8f93\u5165\uff0c\u6a21\u578b\u9700\u8981\u66f4\u65b0\u6570\u636e\u5e93\u4ee5\u6ee1\u8db3\u7528\u6237\u6307\u4ee4\u3002\u5f15\u5165 OPAL (Observe-Plan-Analyze LLM) Agent \u6846\u67b6\uff0c\u5305\u542b Observer\u3001Planner \u548c Analyzer \u7ec4\u4ef6\uff0cPlanner \u7ec4\u4ef6\u751f\u6210\u4ee3\u7801\u8ba1\u5212\u5e76\u8c03\u7528 IE \u6a21\u578b\uff0cAnalyzer \u7ec4\u4ef6\u63d0\u4f9b\u4ee3\u7801\u8d28\u91cf\u53cd\u9988\u3002", "result": "OPAL \u80fd\u591f\u6210\u529f\u9002\u5e94\u4e0d\u540c\u7684\u6570\u636e\u5e93 schema\uff0c\u5e76\u751f\u6210\u4e0d\u540c\u7684\u4ee3\u7801\u8ba1\u5212\u6765\u8c03\u7528\u6240\u9700\u7684 IE \u6a21\u578b\u3002\u5b9e\u9a8c\u8fd8\u7a81\u51fa\u4e86\u5904\u7406\u5927\u578b\u6570\u636e\u5e93\u3001\u590d\u6742\u4f9d\u8d56\u5173\u7cfb\u4ee5\u53ca\u62bd\u53d6\u5e7b\u89c9\u7b49\u65b9\u9762\u7684\u6311\u6218\u3002", "conclusion": "TEXT2DB \u662f\u4e00\u4e2a\u6709\u524d\u666f\u7684\u65b0\u4efb\u52a1\uff0cOPAL \u6846\u67b6\u5728\u9002\u5e94\u6570\u636e\u5e93 schema \u548c\u4fe1\u606f\u62bd\u53d6\u65b9\u9762\u8868\u73b0\u51fa\u6709\u6548\u6027\uff0c\u4f46\u4ecd\u6709\u6311\u6218\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2510.24261", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24261", "abs": "https://arxiv.org/abs/2510.24261", "authors": ["Jingyi Tian", "Le Wang", "Sanping Zhou", "Sen Wang", "Jiayi Li", "Gang Hua"], "title": "DynaRend: Learning 3D Dynamics via Masked Future Rendering for Robotic Manipulation", "comment": "Accepted to NeurIPS 2025", "summary": "Learning generalizable robotic manipulation policies remains a key challenge\ndue to the scarcity of diverse real-world training data. While recent\napproaches have attempted to mitigate this through self-supervised\nrepresentation learning, most either rely on 2D vision pretraining paradigms\nsuch as masked image modeling, which primarily focus on static semantics or\nscene geometry, or utilize large-scale video prediction models that emphasize\n2D dynamics, thus failing to jointly learn the geometry, semantics, and\ndynamics required for effective manipulation. In this paper, we present\nDynaRend, a representation learning framework that learns 3D-aware and\ndynamics-informed triplane features via masked reconstruction and future\nprediction using differentiable volumetric rendering. By pretraining on\nmulti-view RGB-D video data, DynaRend jointly captures spatial geometry, future\ndynamics, and task semantics in a unified triplane representation. The learned\nrepresentations can be effectively transferred to downstream robotic\nmanipulation tasks via action value map prediction. We evaluate DynaRend on two\nchallenging benchmarks, RLBench and Colosseum, as well as in real-world robotic\nexperiments, demonstrating substantial improvements in policy success rate,\ngeneralization to environmental perturbations, and real-world applicability\nacross diverse manipulation tasks.", "AI": {"tldr": "DynaRend\u662f\u4e00\u4e2a\u4e09\u7ef4\u611f\u77e5\u3001\u52a8\u6001\u611f\u77e5\u7684\u8868\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u63a9\u7801\u91cd\u6784\u548c\u672a\u6765\u9884\u6d4b\u6765\u5b66\u4e60\u4e09\u5e73\u9762\u7279\u5f81\uff0c\u4ee5\u5e94\u5bf9\u673a\u5668\u4eba\u64cd\u4f5c\u6570\u636e\u7a00\u758f\u7684\u6311\u6218\u3002", "motivation": "\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u7684\u6cdb\u5316\u6027\u5b66\u4e60\u9762\u4e34\u6570\u636e\u7a00\u758f\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u6709\u6548\u878d\u5408\u51e0\u4f55\u3001\u8bed\u4e49\u548c\u52a8\u6001\u4fe1\u606f\u3002", "method": "\u63d0\u51faDynaRend\u6846\u67b6\uff0c\u5229\u7528\u53ef\u5fae\u5206\u4f53\u79ef\u6e32\u67d3\u8fdb\u884c\u63a9\u7801\u91cd\u6784\u548c\u672a\u6765\u9884\u6d4b\uff0c\u5b66\u4e60\u4e09\u5e73\u9762\u8868\u793a\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u4e0b\u6e38\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u3002", "result": "\u5728RLBench\u3001Colosseum\u548c\u771f\u5b9e\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\uff0cDynaRend\u5728\u7b56\u7565\u6210\u529f\u7387\u3001\u5bf9\u73af\u5883\u6270\u52a8\u7684\u6cdb\u5316\u80fd\u529b\u4ee5\u53ca\u73b0\u5b9e\u4e16\u754c\u9002\u7528\u6027\u65b9\u9762\u5747\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "DynaRend\u80fd\u591f\u6709\u6548\u5730\u5b66\u4e60\u4e09\u7ef4\u611f\u77e5\u548c\u52a8\u6001\u611f\u77e5\u7684\u8868\u793a\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8e\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.24613", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2510.24613", "abs": "https://arxiv.org/abs/2510.24613", "authors": ["Daniel Tezze", "Jose M. Pereira", "Dogukan Tutar", "Maria Ramos", "Jakub Regner", "Pierluigi Gargiani", "Frederik Schiller", "Felix Casanova", "Angel Alegria", "Beatriz Martin-Garcia", "Hasan Sahin", "Zdenek Sofer", "Maider Ormaza", "Luis Hueso", "Marco Gobbi"], "title": "Tunable magnetism in 2D organic-ion-intercalated MnPS3 via molecule-dependent vacancy generation", "comment": null, "summary": "The magnetic properties of van der Waals materials are profoundly influenced\nby structural defects. The layered antiferromagnet MnPS3 offers a unique\nopportunity to explore defect-related magnetism, as Mn2+ vacancies can be\ngenerated by the intercalation of specific guest molecules. However, the\neffectiveness of this process in atomically thin flakes and the extent of the\nmagnetic tunability remain unclear. Here, we show that the magnetic properties\nof MnPS3 can be tailored through the intercalation of different guest\nmolecules. Notably, the insertion of four alkylammonium ions introduces\ndifferent populations of Mn2+ vacancies, leading to a transition from the\npristine antiferromagnetic state to more complex magnetic textures, including a\nferrimagnetic state displaying a magnetic saturation of 1 uB/atom. Moreover, we\nshow that the intercalation of few-nm-thick flakes also leads to the emergence\nof a ferrimagnetic response. This in-flake intercalation, which can be\nmonitored in real time using optical microscopy, can be interrupted before\ncompletion, generating lateral heterostructures between pristine and\nintercalated areas. This approach opens the way to the use of partial\nintercalation to define regions with distinct magnetic properties within a\nsingle flake.", "AI": {"tldr": "\u901a\u8fc7\u63d2\u5165\u4e0d\u540c\u7684\u5ba2\u4f53\u5206\u5b50\uff0c\u53ef\u4ee5\u8c03\u63a7MnPS3\u7684\u78c1\u6027\uff0c\u5b9e\u73b0\u4ece\u53cd\u94c1\u78c1\u5230\u4e9a\u94c1\u78c1\u7684\u8f6c\u53d8\uff0c\u5373\u4f7f\u662f\u8584\u5c42\u6750\u6599\u4e5f\u6709\u6548\uff0c\u8fd8\u53ef\u4ee5\u901a\u8fc7\u63a7\u5236\u63d2\u5c42\u8fc7\u7a0b\u5236\u9020\u5177\u6709\u4e0d\u540c\u78c1\u6027\u7684\u533a\u57df\u3002", "motivation": "\u4e86\u89e3\u7ed3\u6784\u7f3a\u9677\u5982\u4f55\u5f71\u54cd\u8303\u5fb7\u534e\u6750\u6599\u7684\u78c1\u6027\uff0c\u7279\u522b\u662fMnPS3\u4e2dMn2+\u7a7a\u4f4d\u5bf9\u78c1\u6027\u7684\u5f71\u54cd\uff0c\u4ee5\u53ca\u5728\u539f\u5b50\u7ea7\u8584\u7247\u4e2d\u7684\u78c1\u6027\u8c03\u63a7\u6f5c\u529b\u3002", "method": "\u901a\u8fc7\u63d2\u5c42\u4e0d\u540c\u7684\u70f7\u57fa\u94f5\u79bb\u5b50\u5230MnPS3\u4e2d\uff0c\u5e76\u5229\u7528\u5149\u5b66\u663e\u5fae\u955c\u5b9e\u65f6\u76d1\u6d4b\u63d2\u5c42\u8fc7\u7a0b\uff0c\u7814\u7a76\u5176\u5bf9\u78c1\u6027\u7684\u5f71\u54cd\u3002", "result": "\u63d2\u5165\u4e0d\u540c\u7684\u70f7\u57fa\u94f5\u79bb\u5b50\u4f1a\u4ea7\u751f\u4e0d\u540c\u6570\u91cf\u7684Mn2+\u7a7a\u4f4d\uff0c\u5bfc\u81f4\u78c1\u6027\u4ece\u53cd\u94c1\u78c1\u8f6c\u53d8\u4e3a\u4e9a\u94c1\u78c1\uff08\u78c1\u9971\u548c\u5ea6\u4e3a1 \u03bcB/\u539f\u5b50\uff09\u3002\u5373\u4f7f\u662f\u8584\u5c42MnPS3\uff0c\u63d2\u5c42\u4e5f\u80fd\u8bf1\u5bfc\u4e9a\u94c1\u78c1\u54cd\u5e94\u3002\u53ef\u4ee5\u901a\u8fc7\u4e2d\u65ad\u63d2\u5c42\u8fc7\u7a0b\u5236\u9020\u5305\u542b\u4e0d\u540c\u78c1\u6027\u533a\u57df\u7684\u5f02\u8d28\u7ed3\u6784\u3002", "conclusion": "MnPS3\u7684\u78c1\u6027\u53ef\u4ee5\u901a\u8fc7\u5ba2\u4f53\u5206\u5b50\u7684\u63d2\u5c42\u8fdb\u884c\u8c03\u63a7\uff0c\u8584\u5c42\u6750\u6599\u540c\u6837\u9002\u7528\u3002\u90e8\u5206\u63d2\u5c42\u6280\u672f\u53ef\u4ee5\u5b9e\u73b0\u5355\u7247\u6750\u6599\u5185\u4e0d\u540c\u78c1\u6027\u533a\u57df\u7684\u5b9a\u4e49\uff0c\u4e3a\u78c1\u6027\u5668\u4ef6\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2510.24034", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24034", "abs": "https://arxiv.org/abs/2510.24034", "authors": ["Yufan Liu", "Wanqian Zhang", "Huashan Chen", "Lin Wang", "Xiaojun Jia", "Zheng Lin", "Weiping Wang"], "title": "AutoPrompt: Automated Red-Teaming of Text-to-Image Models via LLM-Driven Adversarial Prompts", "comment": "Accepted by ICCV 2025", "summary": "Despite rapid advancements in text-to-image (T2I) models, their safety\nmechanisms are vulnerable to adversarial prompts, which maliciously generate\nunsafe images. Current red-teaming methods for proactively assessing such\nvulnerabilities usually require white-box access to T2I models, and rely on\ninefficient per-prompt optimization, as well as inevitably generate\nsemantically meaningless prompts easily blocked by filters. In this paper, we\npropose APT (AutoPrompT), a black-box framework that leverages large language\nmodels (LLMs) to automatically generate human-readable adversarial suffixes for\nbenign prompts. We first introduce an alternating optimization-finetuning\npipeline between adversarial suffix optimization and fine-tuning the LLM\nutilizing the optimized suffix. Furthermore, we integrates a dual-evasion\nstrategy in optimization phase, enabling the bypass of both perplexity-based\nfilter and blacklist word filter: (1) we constrain the LLM generating\nhuman-readable prompts through an auxiliary LLM perplexity scoring, which\nstarkly contrasts with prior token-level gibberish, and (2) we also introduce\nbanned-token penalties to suppress the explicit generation of banned-tokens in\nblacklist. Extensive experiments demonstrate the excellent red-teaming\nperformance of our human-readable, filter-resistant adversarial prompts, as\nwell as superior zero-shot transferability which enables instant adaptation to\nunseen prompts and exposes critical vulnerabilities even in commercial APIs\n(e.g., Leonardo.Ai.).", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAPT\u7684\u9ed1\u76d2\u6846\u67b6\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u81ea\u52a8\u751f\u6210\u53ef\u8bfb\u7684\u5bf9\u6297\u6027\u540e\u7f00\uff0c\u4ee5\u6d4b\u8bd5\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u6a21\u578b\u7684\u5b89\u5168\u6027\uff0c\u6709\u6548\u7ed5\u8fc7\u8fc7\u6ee4\u5668\u5e76\u751f\u6210\u4e0d\u5b89\u5168\u56fe\u50cf\u3002", "motivation": "\u73b0\u6709\u7684T2I\u6a21\u578b\u5b89\u5168\u673a\u5236\u5bb9\u6613\u53d7\u5230\u6076\u610f\u5bf9\u6297\u6027\u63d0\u793a\u7684\u653b\u51fb\uff0c\u4f46\u73b0\u6709\u7684\u7ea2\u961f\u6d4b\u8bd5\u65b9\u6cd5\u9700\u8981\u767d\u76d2\u8bbf\u95ee\uff0c\u6548\u7387\u4f4e\u4e0b\u4e14\u751f\u6210\u7684\u63d0\u793a\u65e0\u610f\u4e49\u3002", "method": "APT\u6846\u67b6\u91c7\u7528\u4ea4\u66ff\u4f18\u5316\u5fae\u8c03\u6d41\u7a0b\uff0c\u7ed3\u5408\u4e86\u5bf9\u6297\u6027\u540e\u7f00\u4f18\u5316\u548c\u5229\u7528\u4f18\u5316\u540e\u540e\u7f00\u5bf9LLM\u8fdb\u884c\u5fae\u8c03\u3002\u6b64\u5916\uff0c\u8fd8\u96c6\u6210\u4e86\u4e00\u79cd\u53cc\u91cd\u89c4\u907f\u7b56\u7565\uff0c\u901a\u8fc7\u8f85\u52a9LLM\u56f0\u60d1\u5ea6\u8bc4\u5206\u548c\u7981\u6b62\u6807\u8bb0\u60e9\u7f5a\u6765\u7ed5\u8fc7\u56f0\u60d1\u5ea6\u8fc7\u6ee4\u5668\u548c\u9ed1\u540d\u5355\u8fc7\u6ee4\u5668\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cAPT\u751f\u6210\u7684\u5177\u6709\u4eba\u7c7b\u53ef\u8bfb\u6027\u3001\u80fd\u62b5\u6297\u8fc7\u6ee4\u5668\u7684\u5bf9\u6297\u6027\u63d0\u793a\u5728\u7ea2\u961f\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u5177\u6709\u4f18\u8d8a\u7684\u96f6\u6837\u672c\u8fc1\u79fb\u80fd\u529b\uff0c\u80fd\u591f\u5373\u65f6\u9002\u5e94\u672a\u89c1\u7684\u63d0\u793a\uff0c\u751a\u81f3\u80fd\u66b4\u9732\u5546\u4e1aAPI\uff08\u5982Leonardo.Ai.\uff09\u7684\u5173\u952e\u6f0f\u6d1e\u3002", "conclusion": "APT\u6846\u67b6\u80fd\u591f\u81ea\u52a8\u751f\u6210\u4eba\u7c7b\u53ef\u8bfb\u4e14\u80fd\u62b5\u6297\u8fc7\u6ee4\u5668\u7684\u5bf9\u6297\u6027\u540e\u7f00\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5bf9T2I\u6a21\u578b\u5b89\u5168\u6027\u7684\u7ea2\u961f\u6d4b\u8bd5\u80fd\u529b\uff0c\u5e76\u80fd\u8bc6\u522b\u51fa\u5546\u4e1aAPI\u7684\u6f5c\u5728\u5b89\u5168\u98ce\u9669\u3002"}}
{"id": "2510.24100", "categories": ["quant-ph", "math-ph", "math.MP", "nlin.CD", "physics.chem-ph"], "pdf": "https://arxiv.org/pdf/2510.24100", "abs": "https://arxiv.org/abs/2510.24100", "authors": ["Swetamber Das", "Arghya Dutta"], "title": "Dynamical system analysis of quantum tunneling in an asymmetric double-well potential", "comment": "14 pages, 6 figures; Comments are welcome", "summary": "We study quantum tunneling in an asymmetric double-well potential using a\ndynamical systems-based approach rooted in the Ehrenfest formalism. In this\nframework, the time evolution of a Gaussian wave packet is governed by a\nhierarchy of coupled equations linking lower- and higher-order position\nmoments. An approximate closure, required to render the system tractable,\nyields a reduced dynamical system for the mean and variance, with skewness\nentering explicitly due to the potential's asymmetry. Stability analysis of\nthis system identifies energy thresholds for detectable tunneling across the\nbarrier and reveals regimes where tunneling, though theoretically allowed,\nremains practically undetectable. Comparison with full numerical solutions of\nthe time-dependent Schr\\\"odinger equation shows that, beyond reproducing key\ntunneling features, the dynamical systems approach provides an interpretable\ndescription of quantum transport through tunneling in an effective asymmetric\ntwo-level system.", "AI": {"tldr": "\u8be5\u7814\u7a76\u4f7f\u7528\u52a8\u529b\u5b66\u7cfb\u7edf\u65b9\u6cd5\u7814\u7a76\u4e86\u4e0d\u5bf9\u79f0\u53cc\u52bf\u9631\u4e2d\u7684\u91cf\u5b50\u96a7\u7a7f\u73b0\u8c61\u3002", "motivation": "\u7814\u7a76\u4e0d\u5bf9\u79f0\u53cc\u52bf\u9631\u4e2d\u7684\u91cf\u5b50\u96a7\u7a7f\u73b0\u8c61\uff0c\u5e76\u63d0\u4f9b\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u63cf\u8ff0\u3002", "method": "\u91c7\u7528\u57fa\u4e8eEhrenfest\u5f62\u5f0f\u4e3b\u4e49\u7684\u52a8\u529b\u5b66\u7cfb\u7edf\u65b9\u6cd5\uff0c\u901a\u8fc7\u9ad8\u65af\u6ce2\u5305\u7684\u65f6\u95f4\u6f14\u5316\u6765\u7814\u7a76\uff0c\u5e76\u4f7f\u7528\u8fd1\u4f3c\u95ed\u5408\u65b9\u6cd5\u5c06\u7cfb\u7edf\u7b80\u5316\u4e3a\u5e73\u5747\u503c\u3001\u65b9\u5dee\u548c\u504f\u5ea6\u65b9\u7a0b\u3002", "result": "\u8bc6\u522b\u4e86\u53ef\u68c0\u6d4b\u96a7\u7a7f\u7684\u80fd\u91cf\u9608\u503c\uff0c\u5e76\u63ed\u793a\u4e86\u7406\u8bba\u4e0a\u5141\u8bb8\u4f46\u5b9e\u9645\u4e0a\u4e0d\u53ef\u68c0\u6d4b\u7684\u96a7\u7a7f\u673a\u5236\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u91cd\u73b0\u96a7\u7a7f\u7684\u5173\u952e\u7279\u5f81\uff0c\u5e76\u63d0\u4f9b\u4e86\u5bf9\u91cf\u5b50\u8f93\u8fd0\u7684\u6709\u6548\u53cc\u80fd\u7ea7\u7cfb\u7edf\u63cf\u8ff0\u3002", "conclusion": "\u57fa\u4e8e\u52a8\u529b\u5b66\u7cfb\u7edf\u7684\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u5730\u63cf\u8ff0\u4e0d\u5bf9\u79f0\u53cc\u52bf\u9631\u4e2d\u7684\u91cf\u5b50\u96a7\u7a7f\u73b0\u8c61\uff0c\u5e76\u63d0\u4f9b\u6bd4\u5b8c\u5168\u6570\u503c\u89e3\u66f4\u5177\u53ef\u89e3\u91ca\u6027\u7684\u7ed3\u679c\u3002"}}
{"id": "2510.24457", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.24457", "abs": "https://arxiv.org/abs/2510.24457", "authors": ["Jorge Vicente-Martinez", "Edgar Ramirez-Laboreo"], "title": "Flatness-based trajectory planning for 3D overhead cranes with friction compensation and collision avoidance", "comment": "8 pages, 11 figures", "summary": "This paper presents an optimal trajectory generation method for 3D overhead\ncranes by leveraging differential flatness. This framework enables the direct\ninclusion of complex physical and dynamic constraints, such as nonlinear\nfriction and collision avoidance for both payload and rope. Our approach allows\nfor aggressive movements by constraining payload swing only at the final point.\nA comparative simulation study validates our approach, demonstrating that\nneglecting dry friction leads to actuator saturation and collisions. The\nresults show that friction modeling is a fundamental requirement for fast and\nsafe crane trajectories.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5fae\u5206\u5e73\u5766\u6027\u751f\u6210\u4e09\u7ef4\u6865\u5f0f\u8d77\u91cd\u673a\u6700\u4f18\u8f68\u8ff9\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u53ef\u4ee5\u76f4\u63a5\u5305\u542b\u975e\u7ebf\u6027\u6469\u64e6\u548c\u8f7d\u8377/\u7ef3\u7d22\u78b0\u649e\u907f\u514d\u7b49\u590d\u6742\u7684\u7269\u7406\u548c\u52a8\u6001\u7ea6\u675f\uff0c\u5e76\u5141\u8bb8\u5728\u4ec5\u5728\u7ec8\u70b9\u7ea6\u675f\u8f7d\u8377\u6446\u52a8\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u5feb\u901f\u79fb\u52a8\u3002", "motivation": "\u672c\u7814\u7a76\u7684\u52a8\u673a\u662f\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u5904\u7406\u590d\u6742\u7269\u7406\u548c\u52a8\u6001\u7ea6\u675f\uff08\u5982\u975e\u7ebf\u6027\u6469\u64e6\u548c\u78b0\u649e\u907f\u514d\uff09\u7684\u6700\u4f18\u4e09\u7ef4\u6865\u5f0f\u8d77\u91cd\u673a\u8f68\u8ff9\u751f\u6210\u65b9\u6cd5\uff0c\u5e76\u5141\u8bb8\u5728\u7ec8\u70b9\u7ea6\u675f\u8f7d\u8377\u6446\u52a8\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u5feb\u901f\u79fb\u52a8\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5fae\u5206\u5e73\u5766\u6027\u751f\u6210\u4e09\u7ef4\u6865\u5f0f\u8d77\u91cd\u673a\u6700\u4f18\u8f68\u8ff9\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u53ef\u4ee5\u76f4\u63a5\u5305\u542b\u975e\u7ebf\u6027\u6469\u64e6\u548c\u8f7d\u8377/\u7ef3\u7d22\u78b0\u649e\u907f\u514d\u7b49\u590d\u6742\u7684\u7269\u7406\u548c\u52a8\u6001\u7ea6\u675f\u3002", "result": "\u901a\u8fc7\u5bf9\u6bd4\u4eff\u771f\u7814\u7a76\uff0c\u9a8c\u8bc1\u4e86\u672c\u7814\u7a76\u63d0\u51fa\u7684\u65b9\u6cd5\uff0c\u7ed3\u679c\u8868\u660e\u5ffd\u7565\u5e72\u6469\u64e6\u4f1a\u5bfc\u81f4\u6267\u884c\u5668\u9971\u548c\u548c\u78b0\u649e\uff0c\u8bc1\u660e\u4e86\u6469\u64e6\u5efa\u6a21\u5bf9\u4e8e\u5feb\u901f\u5b89\u5168\u7684\u8d77\u91cd\u673a\u8f68\u8ff9\u662f\u57fa\u672c\u8981\u6c42\u3002", "conclusion": "\u6469\u64e6\u5efa\u6a21\u5bf9\u4e8e\u751f\u6210\u5feb\u901f\u4e14\u5b89\u5168\u7684\u6865\u5f0f\u8d77\u91cd\u673a\u8f68\u8ff9\u81f3\u5173\u91cd\u8981\uff0c\u5ffd\u7565\u5b83\u4f1a\u5bfc\u81f4\u6267\u884c\u5668\u9971\u548c\u548c\u6f5c\u5728\u7684\u78b0\u649e\u3002"}}
{"id": "2510.24495", "categories": ["eess.SP", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24495", "abs": "https://arxiv.org/abs/2510.24495", "authors": ["Yuzhi Yang", "Sen Yan", "Weijie Zhou", "Brahim Mefgouda", "Ridong Li", "Zhaoyang Zhang", "M\u00e9rouane Debbah"], "title": "Diffusion Models for Wireless Transceivers: From Pilot-Efficient Channel Estimation to AI-Native 6G Receivers", "comment": "Submitted for potential publication in IEEE Wireless Communications", "summary": "With the development of artificial intelligence (AI) techniques, implementing\nAI-based techniques to improve wireless transceivers becomes an emerging\nresearch topic. Within this context, AI-based channel characterization and\nestimation become the focus since these methods have not been solved by\ntraditional methods very well and have become the bottleneck of transceiver\nefficiency in large-scale orthogonal frequency division multiplexing (OFDM)\nsystems. Specifically, by formulating channel estimation as a generative AI\nproblem, generative AI methods such as diffusion models (DMs) can efficiently\ndeal with rough initial estimations and have great potential to cooperate with\ntraditional signal processing methods. This paper focuses on the transceiver\ndesign of OFDM systems based on DMs, provides an illustration of the potential\nof DMs in wireless transceivers, and points out the related research directions\nbrought by DMs. We also provide a proof-of-concept case study of further\nadapting DMs for better wireless receiver performance.", "AI": {"tldr": "\u672c\u7bc7\u8bba\u6587\u63d0\u51fa\u5c06\u751f\u6210\u5f0fAI\uff0c\u7279\u522b\u662f\u6269\u6563\u6a21\u578b\uff08DM\uff09\uff0c\u5e94\u7528\u4e8e\u5927\u89c4\u6a21\u6b63\u4ea4\u9891\u5206\u590d\u7528\uff08OFDM\uff09\u7cfb\u7edf\u7684\u65e0\u7ebf\u6536\u53d1\u5668\u8bbe\u8ba1\uff0c\u4ee5\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5728\u4fe1\u9053\u8868\u5f81\u548c\u4f30\u8ba1\u65b9\u9762\u7684\u74f6\u9888\u3002", "motivation": "AI\u6280\u672f\u7684\u53d1\u5c55\u4fc3\u4f7f\u7814\u7a76\u8005\u63a2\u7d22\u5c06\u5176\u5e94\u7528\u4e8e\u6539\u8fdb\u65e0\u7ebf\u6536\u53d1\u5668\uff0c\u5c24\u5176\u662f\u5728\u5927\u89c4\u6a21OFDM\u7cfb\u7edf\u4e2d\uff0c\u4fe1\u9053\u8868\u5f81\u548c\u4f30\u8ba1\u5df2\u6210\u4e3a\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u89e3\u51b3\u7684\u74f6\u9888\uff0c\u5f71\u54cd\u4e86\u6536\u53d1\u6548\u7387\u3002", "method": "\u5c06\u4fe1\u9053\u4f30\u8ba1\u95ee\u9898\u5f62\u5f0f\u5316\u4e3a\u751f\u6210\u5f0fAI\u95ee\u9898\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\uff08DM\uff09\u5904\u7406\u7c97\u7565\u7684\u521d\u59cb\u4f30\u8ba1\uff0c\u5e76\u63a2\u7d22\u5176\u4e0e\u4f20\u7edf\u4fe1\u53f7\u5904\u7406\u65b9\u6cd5\u7ed3\u5408\u7684\u53ef\u80fd\u6027\u3002", "result": "\u901a\u8fc7\u4e00\u4e2a\u6982\u5ff5\u9a8c\u8bc1\u7684\u6848\u4f8b\u7814\u7a76\uff0c\u5c55\u793a\u4e86DM\u5728\u6539\u8fdb\u65e0\u7ebf\u63a5\u6536\u5668\u6027\u80fd\u65b9\u9762\u7684\u6f5c\u529b\u3002", "conclusion": "\u751f\u6210\u5f0fAI\uff0c\u7279\u522b\u662f\u6269\u6563\u6a21\u578b\uff0c\u5728OFDM\u7cfb\u7edf\u65e0\u7ebf\u6536\u53d1\u5668\u8bbe\u8ba1\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u5e76\u4e3a\u76f8\u5173\u7814\u7a76\u6307\u660e\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2510.23650", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23650", "abs": "https://arxiv.org/abs/2510.23650", "authors": ["Wei Xia"], "title": "Beyond Hidden-Layer Manipulation: Semantically-Aware Logit Interventions for Debiasing LLMs", "comment": null, "summary": "We proposed Static and Dynamic -- two zero-shot logits-layer debiasing\nmethods. Dynamic reduces bias by up to 70% with minimal fluency loss. Logits\nintervention outperforms hidden-layer approaches. We show semantic-aware logits\nintervention is stable and effective for debiasing aligned LLMs.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2510.24031", "categories": ["cs.AI", "cs.CR", "H.3.3, I.2.7, I.5.3, I.2.5,"], "pdf": "https://arxiv.org/pdf/2510.24031", "abs": "https://arxiv.org/abs/2510.24031", "authors": ["Peng Cai", "Reza Ryan", "Nickson M. Karie"], "title": "LLMLogAnalyzer: A Clustering-Based Log Analysis Chatbot using Large Language Models", "comment": "33 pages, 10 figures", "summary": "System logs are a cornerstone of cybersecurity, supporting proactive breach\nprevention and post-incident investigations. However, analyzing vast amounts of\ndiverse log data remains significantly challenging, as high costs, lack of\nin-house expertise, and time constraints make even basic analysis difficult for\nmany organizations. This study introduces LLMLogAnalyzer, a clustering-based\nlog analysis chatbot that leverages Large Language Models (LLMs) and Machine\nLearning (ML) algorithms to simplify and streamline log analysis processes.\nThis innovative approach addresses key LLM limitations, including context\nwindow constraints and poor structured text handling capabilities, enabling\nmore effective summarization, pattern extraction, and anomaly detection tasks.\nLLMLogAnalyzer is evaluated across four distinct domain logs and various tasks.\nResults demonstrate significant performance improvements over state-of-the-art\nLLM-based chatbots, including ChatGPT, ChatPDF, and NotebookLM, with consistent\ngains ranging from 39% to 68% across different tasks. The system also exhibits\nstrong robustness, achieving a 93% reduction in interquartile range (IQR) when\nusing ROUGE-1 scores, indicating significantly lower result variability. The\nframework's effectiveness stems from its modular architecture comprising a\nrouter, log recognizer, log parser, and search tools. This design enhances LLM\ncapabilities for structured text analysis while improving accuracy and\nrobustness, making it a valuable resource for both cybersecurity experts and\nnon-technical users.", "AI": {"tldr": "LLMLogAnalyzer\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u548cML\u7684\u65e5\u5fd7\u5206\u6790\u804a\u5929\u673a\u5668\u4eba\uff0c\u901a\u8fc7\u89e3\u51b3LLM\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u65e5\u5fd7\u5206\u6790\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u5728\u5404\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u804a\u5929\u673a\u5668\u4eba\u3002", "motivation": "\u73b0\u6709\u65e5\u5fd7\u5206\u6790\u65b9\u6cd5\u6210\u672c\u9ad8\u3001\u7f3a\u4e4f\u4e13\u4e1a\u77e5\u8bc6\u4e14\u8017\u65f6\uff0cLLMLogAnalyzer\u65e8\u5728\u7b80\u5316\u548c\u4f18\u5316\u8fd9\u4e00\u8fc7\u7a0b\u3002", "method": "LLMLogAnalyzer\u5229\u7528LLM\u548cML\u7b97\u6cd5\uff0c\u901a\u8fc7\u805a\u7c7b\u65b9\u6cd5\u5904\u7406\u65e5\u5fd7\u6570\u636e\uff0c\u5e76\u89e3\u51b3\u4e86LLM\u5728\u5904\u7406\u7ed3\u6784\u5316\u6587\u672c\u548c\u4e0a\u4e0b\u6587\u7a97\u53e3\u65b9\u9762\u7684\u9650\u5236\u3002", "result": "LLMLogAnalyzer\u5728\u56db\u4e2a\u4e0d\u540c\u9886\u57df\u7684\u65e5\u5fd7\u548c\u5404\u79cd\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5728\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u5747\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u4e0e\u73b0\u6709\u804a\u5929\u673a\u5668\u4eba\u76f8\u6bd4\uff0c\u6027\u80fd\u63d0\u5347\u4e8639%\u81f368%\uff0c\u5e76\u964d\u4f4e\u4e86\u7ed3\u679c\u53d8\u5f02\u6027\u3002", "conclusion": "LLMLogAnalyzer\u901a\u8fc7\u5176\u6a21\u5757\u5316\u67b6\u6784\uff0c\u6709\u6548\u589e\u5f3a\u4e86LLM\u5728\u7ed3\u6784\u5316\u6587\u672c\u5206\u6790\u65b9\u9762\u7684\u80fd\u529b\uff0c\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u7f51\u7edc\u5b89\u5168\u4e13\u5bb6\u548c\u975e\u6280\u672f\u7528\u6237\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.24020", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24020", "abs": "https://arxiv.org/abs/2510.24020", "authors": ["Hao An", "Yang Xu"], "title": "Teaching LLMs to Abstain via Fine-Grained Semantic Confidence Reward", "comment": "23pages, 4figures", "summary": "Mitigating hallucinations in Large Language Models (LLMs) is critical for\ntheir reliable deployment. Existing methods typically fine-tune LLMs to abstain\nfrom answering questions beyond their knowledge scope. However, these methods\noften rely on coarse-grained signals to guide LLMs to abstain, such as overall\nconfidence or uncertainty scores on multiple sampled answers, which may result\nin an imprecise awareness of the model's own knowledge boundaries. To this end,\nwe propose a novel reinforcement learning framework built on\n$\\textbf{\\underline{Fi}ne-grained \\underline{S}emantic \\underline{Co}nfidence\n\\underline{Re}ward (\\Ours)}$, which guides LLMs to abstain via sample-specific\nconfidence. Specifically, our method operates by sampling multiple candidate\nanswers and conducting semantic clustering, then training the LLM to retain\nanswers within high-confidence clusters and discard those within low-confidence\nones, thereby promoting accurate post-hoc abstention. Additionally, we propose\na new metric for evaluating the reliability of abstention fine-tuning tasks\nmore comprehensively. Our method significantly enhances reliability in both\nin-domain and out-of-distribution benchmarks.", "AI": {"tldr": "\u901a\u8fc7\u57fa\u4e8e\u7ec6\u7c92\u5ea6\u8bed\u4e49\u7f6e\u4fe1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u77e5\u8bc6\u8fb9\u754c\u4e0a\u7684\u81ea\u6211\u8ba4\u77e5\u548c\u89c4\u907f\u4e0d\u786e\u5b9a\u7b54\u6848\u7684\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7f13\u89e3\u5e7b\u89c9\u7684\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u7c97\u7c92\u5ea6\u7684\u4fe1\u53f7\u6765\u6307\u5bfc\u6a21\u578b\u5728\u77e5\u8bc6\u8303\u56f4\u4e4b\u5916\u65f6\u5f03\u7b54\uff0c\u8fd9\u79cd\u65b9\u5f0f\u53ef\u80fd\u5bfc\u81f4\u6a21\u578b\u5bf9\u5176\u77e5\u8bc6\u8fb9\u754c\u7684\u8ba4\u77e5\u4e0d\u7cbe\u786e\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u79f0\u4e3a\u7ec6\u7c92\u5ea6\u8bed\u4e49\u7f6e\u4fe1\u5956\u52b1\uff08\\", "result": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u8bad\u7ec3LLM\u4fdd\u7559\u9ad8\u7f6e\u4fe1\u5ea6\u805a\u7c7b\u4e2d\u7684\u7b54\u6848\u5e76\u4e22\u5f03\u4f4e\u7f6e\u4fe1\u5ea6\u805a\u7c7b\u4e2d\u7684\u7b54\u6848\uff0c\u4ece\u800c\u4fc3\u8fdb\u4e86\u7cbe\u786e\u7684\u540e\u9a8c\u5f03\u7b54\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u66f4\u5168\u9762\u5730\u8bc4\u4f30\u5f03\u7b54\u5fae\u8c03\u4efb\u52a1\u7684\u53ef\u9760\u6027\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u9886\u57df\u5185\u548c\u57df\u5916\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u90fd\u663e\u8457\u63d0\u9ad8\u4e86\u53ef\u9760\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u7ec6\u7c92\u5ea6\u8bed\u4e49\u7f6e\u4fe1\u5956\u52b1\u6846\u67b6\u80fd\u6709\u6548\u63d0\u5347LLM\u5728\u77e5\u8bc6\u8fb9\u754c\u4e0a\u7684\u81ea\u6211\u8ba4\u77e5\u80fd\u529b\uff0c\u663e\u8457\u589e\u5f3a\u5176\u89c4\u907f\u4e0d\u786e\u5b9a\u7b54\u6848\u7684\u53ef\u9760\u6027\uff0c\u5e76\u4e14\u63d0\u51fa\u7684\u65b0\u8bc4\u4f30\u6307\u6807\u80fd\u66f4\u5168\u9762\u5730\u8861\u91cf\u5f03\u7b54\u5fae\u8c03\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2510.24315", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24315", "abs": "https://arxiv.org/abs/2510.24315", "authors": ["Baozhe Zhang", "Xinwei Chen", "Qingcheng Chen", "Chao Xu", "Fei Gao", "Yanjun Cao"], "title": "Global-State-Free Obstacle Avoidance for Quadrotor Control in Air-Ground Cooperation", "comment": null, "summary": "CoNi-MPC provides an efficient framework for UAV control in air-ground\ncooperative tasks by relying exclusively on relative states, eliminating the\nneed for global state estimation. However, its lack of environmental\ninformation poses significant challenges for obstacle avoidance. To address\nthis issue, we propose a novel obstacle avoidance algorithm, Cooperative\nNon-inertial frame-based Obstacle Avoidance (CoNi-OA), designed explicitly for\nUAV-UGV cooperative scenarios without reliance on global state estimation or\nobstacle prediction. CoNi-OA uniquely utilizes a single frame of raw LiDAR data\nfrom the UAV to generate a modulation matrix, which directly adjusts the\nquadrotor's velocity to achieve obstacle avoidance. This modulation-based\nmethod enables real-time generation of collision-free trajectories within the\nUGV's non-inertial frame, significantly reducing computational demands (less\nthan 5 ms per iteration) while maintaining safety in dynamic and unpredictable\nenvironments. The key contributions of this work include: (1) a\nmodulation-based obstacle avoidance algorithm specifically tailored for UAV-UGV\ncooperation in non-inertial frames without global states; (2) rapid, real-time\ntrajectory generation based solely on single-frame LiDAR data, removing the\nneed for obstacle modeling or prediction; and (3) adaptability to both static\nand dynamic environments, thus extending applicability to featureless or\nunknown scenarios.", "AI": {"tldr": "CoNi-OA\u662f\u4e00\u79cd\u7528\u4e8e\u65e0\u4eba\u673a-\u65e0\u4eba\u8f66\u534f\u540c\u4efb\u52a1\u7684\u65b0\u578b\u969c\u788d\u7269\u89c4\u907f\u7b97\u6cd5\uff0c\u5b83\u5229\u7528\u5355\u5e27\u6fc0\u5149\u96f7\u8fbe\u6570\u636e\u751f\u6210\u8c03\u5236\u77e9\u9635\uff0c\u76f4\u63a5\u8c03\u6574\u65e0\u4eba\u673a\u901f\u5ea6\u4ee5\u89c4\u907f\u969c\u788d\u7269\uff0c\u65e0\u9700\u5168\u5c40\u72b6\u6001\u4f30\u8ba1\u6216\u969c\u788d\u7269\u9884\u6d4b\uff0c\u80fd\u5728\u975e\u60ef\u6027\u5750\u6807\u7cfb\u4e0b\u5b9e\u65f6\u751f\u6210\u65e0\u78b0\u649e\u8f68\u8ff9\uff0c\u8ba1\u7b97\u91cf\u4f4e\uff0c\u5e76\u80fd\u9002\u5e94\u52a8\u6001\u548c\u9759\u6001\u73af\u5883\u3002", "motivation": "\u73b0\u6709\u7684CoNi-MPC\u6846\u67b6\u867d\u7136\u80fd\u9ad8\u6548\u5730\u8fdb\u884c\u65e0\u4eba\u673a\u63a7\u5236\uff0c\u4f46\u7f3a\u4e4f\u73af\u5883\u4fe1\u606f\uff0c\u96be\u4ee5\u5b9e\u73b0\u969c\u788d\u7269\u89c4\u907f\u3002\u672c\u7814\u7a76\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u969c\u788d\u7269\u89c4\u907f\u7b97\u6cd5\uff0c\u4ee5\u89e3\u51b3\u6b64\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCoNi-OA\u7684\u65b0\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u5229\u7528\u65e0\u4eba\u673a\u5355\u5e27\u539f\u59cb\u6fc0\u5149\u96f7\u8fbe\u6570\u636e\u751f\u6210\u4e00\u4e2a\u8c03\u5236\u77e9\u9635\uff0c\u76f4\u63a5\u8c03\u6574\u65e0\u4eba\u673a\u7684\u901f\u5ea6\u4ee5\u5b9e\u73b0\u969c\u788d\u7269\u89c4\u907f\u3002\u8be5\u7b97\u6cd5\u5728\u65e0\u4eba\u673a-\u65e0\u4eba\u8f66\u534f\u540c\u573a\u666f\u4e0b\uff0c\u65e0\u9700\u5168\u5c40\u72b6\u6001\u4f30\u8ba1\u6216\u969c\u788d\u7269\u9884\u6d4b\uff0c\u5373\u53ef\u5728\u65e0\u4eba\u8f66\u7684\u975e\u60ef\u6027\u5750\u6807\u7cfb\u4e0b\u5b9e\u65f6\u751f\u6210\u65e0\u78b0\u649e\u8f68\u8ff9\u3002", "result": "CoNi-OA\u7b97\u6cd5\u80fd\u57285\u6beb\u79d2\u5185\u5b8c\u6210\u6bcf\u6b21\u8fed\u4ee3\u8ba1\u7b97\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u9700\u6c42\u3002\u540c\u65f6\uff0c\u8be5\u7b97\u6cd5\u80fd\u591f\u5b9e\u65f6\u751f\u6210\u65e0\u78b0\u649e\u8f68\u8ff9\uff0c\u5e76\u80fd\u9002\u5e94\u9759\u6001\u548c\u52a8\u6001\u73af\u5883\uff0c\u751a\u81f3\u5728\u7279\u5f81\u7f3a\u5931\u6216\u672a\u77e5\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u6709\u6548\u5de5\u4f5c\u3002", "conclusion": "CoNi-OA\u7b97\u6cd5\u662f\u4e00\u79cd\u6709\u6548\u7684\u3001\u57fa\u4e8e\u8c03\u5236\u7684\u969c\u788d\u7269\u89c4\u907f\u7b97\u6cd5\uff0c\u7279\u522b\u9002\u7528\u4e8e\u65e0\u4eba\u673a-\u65e0\u4eba\u8f66\u534f\u540c\u4efb\u52a1\u3002\u5b83\u901a\u8fc7\u4ec5\u4f7f\u7528\u5355\u5e27\u6fc0\u5149\u96f7\u8fbe\u6570\u636e\u548c\u5728\u975e\u60ef\u6027\u5750\u6807\u7cfb\u4e0b\u64cd\u4f5c\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u5b9e\u65f6\u7684\u65e0\u78b0\u649e\u8f68\u8ff9\u751f\u6210\uff0c\u65e0\u9700\u5168\u5c40\u72b6\u6001\u4f30\u8ba1\u6216\u969c\u788d\u7269\u9884\u6d4b\uff0c\u5e76\u80fd\u9002\u5e94\u5404\u79cd\u73af\u5883\u3002"}}
{"id": "2510.24646", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2510.24646", "abs": "https://arxiv.org/abs/2510.24646", "authors": ["Takeru Omiya", "Enrico Pedretti", "Pooja Sharma", "Albano Cavaleiro", "Arm\u00e9nio C. Serra", "Jorge F. J. Coelho", "Maria Clelia Righi", "F\u00e1bio Ferreira"], "title": "Comparative analysis of the lubrication performance of functionalized copolymers interacting with silicon, cobalt, and silver doped diamond-like carbon", "comment": null, "summary": "This study examines the tribological behavior of diamond-like carbon (DLC)\ncoatings doped with silicon (Si), cobalt (Co), or silver (Ag) in the presence\nof an amine-functionalized block copolymer lubricant. Under boundary\nlubrication, Si-doped DLC (Si-DLC) exhibited the lowest coefficient of friction\n($\\approx$0.045) and nearly 45% lower wear than undoped DLC. Co-DLC showed\nmoderate improvement, while Ag-DLC provided no significant benefit.\nCross-sectional FIB-TEM revealed thin tribofilms, 12-17 nm in thickness, on Si-\nand Co-doped surfaces. As reported for Si-DLC, these films incorporate\ncopolymer-derived fragments, suggesting a similar composition for Co-DLC. These\nresults indicate that dopant-polymer interactions are key to the development of\nself-organized boundary layers. To gain atomic-level insight, first-principles\ncalculations were carried out on the adsorption of the dimethylaminoethyl\nmethacrylate (DMAEMA) unit, the copolymer's functional group. The calculated\nadsorption energies were $-$2.27 to $-$0.57 eV for Si-DLC, $-$1.73 to $-$1.49\neV for Co(0001), and $-$1.21 to $-$1.08 eV for Ag(111). The order of stability\n(Si $>$ Co $>$ Ag) was consistent with the experimental tribological ranking.\nChemical bonding dominated for Si-DLC, while Ag showed mainly weak\nphysisorption. Simulated pull-off forces further reflected this hierarchy, with\nN-Si bonds requiring about twice the force of N-Co and nearly five times that\nof N-Ag. The correspondence between adsorption strength and tribological\nresponse highlights the decisive role of dopant species in tribofilm formation.\nThese findings provide guidance for designing durable low-friction surfaces in\napplications such as electric drivetrains and precision mechanical systems.", "AI": {"tldr": "Si\u63ba\u6742DLC\u6d82\u5c42\u5728\u80fa\u529f\u80fd\u5316\u5d4c\u6bb5\u5171\u805a\u7269\u6da6\u6ed1\u5242\u4e2d\u8868\u73b0\u51fa\u6700\u4f73\u7684\u6469\u64e6\u5b66\u6027\u80fd\uff0c\u5177\u6709\u4f4e\u6469\u64e6\u7cfb\u6570\u548c\u663e\u8457\u964d\u4f4e\u7684\u78e8\u635f\uff0c\u8fd9\u5f52\u56e0\u4e8e\u63ba\u6742\u5242\u4e0e\u805a\u5408\u7269\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u5f62\u6210\u4e86\u7a33\u5b9a\u7684\u81ea\u7ec4\u7ec7\u8fb9\u754c\u819c\u3002", "motivation": "\u7814\u7a76\u63ba\u6742\u5242\uff08\u7845\u3001\u94b4\u3001\u94f6\uff09\u5bf9\u7c7b\u91d1\u521a\u77f3\u78b3\uff08DLC\uff09\u6d82\u5c42\u5728\u80fa\u529f\u80fd\u5316\u5d4c\u6bb5\u5171\u805a\u7269\u6da6\u6ed1\u5242\u5b58\u5728\u4e0b\u7684\u6469\u64e6\u5b66\u884c\u4e3a\u7684\u5f71\u54cd\uff0c\u5e76\u63a2\u7d22\u63ba\u6742\u5242-\u805a\u5408\u7269\u76f8\u4e92\u4f5c\u7528\u5728\u8fb9\u754c\u819c\u5f62\u6210\u4e2d\u7684\u4f5c\u7528\u3002", "method": "\u5728\u8fb9\u754c\u6da6\u6ed1\u6761\u4ef6\u4e0b\uff0c\u901a\u8fc7\u5b9e\u9a8c\u6d4b\u8bd5\u4e0d\u540c\u63ba\u6742DLC\u6d82\u5c42\u7684\u6469\u64e6\u7cfb\u6570\u548c\u78e8\u635f\u7387\u3002\u5229\u7528\u805a\u7126\u79bb\u5b50\u675f\u900f\u5c04\u7535\u5b50\u663e\u5fae\u955c\uff08FIB-TEM\uff09\u5206\u6790\u4e86Si-DLC\u548cCo-DLC\u8868\u9762\u7684\u6469\u64e6\u819c\u3002\u901a\u8fc7\u7b2c\u4e00\u6027\u539f\u7406\u8ba1\u7b97\u4e86\u5171\u805a\u7269\u7684\u529f\u80fd\u57fa\u56e2\uff08DMAEMA\uff09\u5728\u4e0d\u540c\u63ba\u6742DLC\u8868\u9762\u7684\u5438\u9644\u80fd\u548c\u62c9\u8131\u529b\uff0c\u4ee5\u63d0\u4f9b\u539f\u5b50\u5c42\u9762\u7684\u89c1\u89e3\u3002", "result": "Si-DLC\u6d82\u5c42\u7684\u6469\u64e6\u7cfb\u6570\u6700\u4f4e\uff08\u7ea60.045\uff09\uff0c\u78e8\u635f\u6bd4\u672a\u63ba\u6742DLC\u964d\u4f4e\u4e86\u8fd145%\u3002Co-DLC\u8868\u73b0\u51fa\u9002\u5ea6\u7684\u6539\u5584\uff0c\u800cAg-DLC\u7684\u76ca\u5904\u4e0d\u660e\u663e\u3002Si-DLC\u548cCo-DLC\u8868\u9762\u5f62\u6210\u4e8612-17\u7eb3\u7c73\u539a\u7684\u6469\u64e6\u819c\uff0c\u5176\u4e2d\u5305\u542b\u5171\u805a\u7269\u884d\u751f\u7684\u7247\u6bb5\u3002\u8ba1\u7b97\u5438\u9644\u80fd\u548c\u62c9\u8131\u529b\u7ed3\u679c\u4e0e\u5b9e\u9a8c\u7ed3\u679c\u4e00\u81f4\uff0c\u8868\u660eSi-DLC\u7684\u5438\u9644\u6700\u7a33\u5b9a\uff0c\u5176\u6b21\u662fCo-DLC\uff0c\u7136\u540e\u662fAg-DLC\u3002", "conclusion": "\u63ba\u6742\u5242-\u805a\u5408\u7269\u76f8\u4e92\u4f5c\u7528\u5728Si-DLC\u6d82\u5c42\u7684\u4f4e\u6469\u64e6\u548c\u8010\u78e8\u6027\u4e2d\u8d77\u7740\u5173\u952e\u4f5c\u7528\uff0c\u8fd9\u5f97\u76ca\u4e8e\u5b83\u4eec\u4e0e\u5171\u805a\u7269\u5f62\u6210\u7a33\u5b9a\u7684\u81ea\u7ec4\u7ec7\u8fb9\u754c\u819c\u3002\u5438\u9644\u7a33\u5b9a\u6027\u548c\u5316\u5b66\u952e\u5408\u7684\u5f3a\u5ea6\uff08Si > Co > Ag\uff09\u4e0e\u5b9e\u9a8c\u89c2\u5bdf\u5230\u7684\u6469\u64e6\u5b66\u6027\u80fd\u6392\u540d\u4e00\u81f4\u3002\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u8bbe\u8ba1\u5728\u7535\u52a8\u4f20\u52a8\u7cfb\u7edf\u548c\u7cbe\u5bc6\u673a\u68b0\u7cfb\u7edf\u7b49\u5e94\u7528\u4e2d\u5177\u6709\u4f4e\u6469\u64e6\u548c\u9ad8\u8010\u7528\u6027\u7684\u8868\u9762\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2510.24036", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24036", "abs": "https://arxiv.org/abs/2510.24036", "authors": ["Xingyu Liu", "Kun Ming Goh"], "title": "ResNet: Enabling Deep Convolutional Neural Networks through Residual Learning", "comment": "3 pages, 5 figures, 1 table", "summary": "Convolutional Neural Networks (CNNs) has revolutionized computer vision, but\ntraining very deep networks has been challenging due to the vanishing gradient\nproblem. This paper explores Residual Networks (ResNet), introduced by He et\nal. (2015), which overcomes this limitation by using skip connections. ResNet\nenables the training of networks with hundreds of layers by allowing gradients\nto flow directly through shortcut connections that bypass intermediate layers.\nIn our implementation on the CIFAR-10 dataset, ResNet-18 achieves 89.9%\naccuracy compared to 84.1% for a traditional deep CNN of similar depth, while\nalso converging faster and training more stably.", "AI": {"tldr": "\u6b8b\u5dee\u7f51\u7edc\uff08ResNet\uff09\u901a\u8fc7\u8df3\u8dc3\u8fde\u63a5\u89e3\u51b3\u4e86\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u8bad\u7ec3\u4e2d\u7684\u68af\u5ea6\u6d88\u5931\u95ee\u9898\uff0c\u5728CIFAR-10\u6570\u636e\u96c6\u4e0a\uff0cResNet-18\u7684\u51c6\u786e\u7387\u8fbe\u5230\u4e8689.9%\uff0c\u4f18\u4e8e\u4f20\u7edfCNN\u768484.1%\uff0c\u5e76\u4e14\u6536\u655b\u66f4\u5feb\uff0c\u8bad\u7ec3\u66f4\u7a33\u5b9a\u3002", "motivation": "\u89e3\u51b3\u8bad\u7ec3\u975e\u5e38\u6df1\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u65f6\u9047\u5230\u7684\u68af\u5ea6\u6d88\u5931\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u6b8b\u5dee\u7f51\u7edc\uff08ResNet\uff09\uff0c\u5176\u7279\u70b9\u662f\u5f15\u5165\u4e86\u8df3\u8dc3\u8fde\u63a5\uff0c\u5141\u8bb8\u68af\u5ea6\u76f4\u63a5\u6d41\u8fc7\u5feb\u6377\u8fde\u63a5\uff0c\u7ed5\u8fc7\u4e2d\u95f4\u5c42\u3002", "result": "\u5728CIFAR-10\u6570\u636e\u96c6\u4e0a\uff0cResNet-18\u5b9e\u73b0\u4e8689.9%\u7684\u51c6\u786e\u7387\uff0c\u800c\u6df1\u5ea6\u76f8\u4f3c\u7684\u4f20\u7edfCNN\u4e3a84.1%\u3002ResNet\u8fd8\u8868\u73b0\u51fa\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u548c\u66f4\u7a33\u5b9a\u7684\u8bad\u7ec3\u8fc7\u7a0b\u3002", "conclusion": "\u6b8b\u5dee\u7f51\u7edc\uff08ResNet\uff09\u6709\u6548\u5730\u89e3\u51b3\u4e86\u68af\u5ea6\u6d88\u5931\u95ee\u9898\uff0c\u80fd\u591f\u8bad\u7ec3\u66f4\u6df1\u7684\u7f51\u7edc\uff0c\u5e76\u5728\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u4f18\u4e8e\u4f20\u7edfCNN\u7684\u6027\u80fd\u3002"}}
{"id": "2510.24110", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.24110", "abs": "https://arxiv.org/abs/2510.24110", "authors": ["Linwei Li", "Hongmei Yao", "Chunlin Yang", "Shaoming Fei"], "title": "Separability Criteria of Quantum States based on Generalized Bloch Representation", "comment": "31 pages, 6 figures", "summary": "Quantum entanglement serves as a fundamental resource in quantum information\ntheory. This paper presents a comprehensive framework of separability criteria\nfor detecting entanglement across quantum systems, from bipartite to\nmultipartite states. We propose a novel unified parameterized extended\ncorrelation tensor, constructed via the generalized Bloch representation under\nan arbitrary orthogonal basis, which bridges our bipartite criterion with\nseveral existing ones. Moreover, we develop a specialized tensor unfolding\ntechnique -- termed mixed mode matrix unfolding -- that naturally generalizes\nthe conventional $k$-mode matrix unfolding and enables the generalization of\nthe extended correlation tensor construction to multipartite systems. And we\nderive several separability criteria for multipartite states. Numerical\nexamples demonstrate that our separability criteria exhibit enhanced capability\nin detecting entanglement.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u7edf\u4e00\u53c2\u6570\u5316\u6269\u5c55\u76f8\u5173\u5f20\u91cf\uff0c\u7528\u4e8e\u68c0\u6d4b\u91cf\u5b50\u7cfb\u7edf\u4e2d\u7684\u7ea0\u7f20\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u6a21\u5f0f\u77e9\u9635\u5c55\u5f00\u6280\u672f\uff0c\u7528\u4e8e\u6784\u5efa\u591a\u4f53\u7cfb\u7edf\u4e2d\u7684\u5f20\u91cf\u3002", "motivation": "\u91cf\u5b50\u7ea0\u7f20\u662f\u91cf\u5b50\u4fe1\u606f\u8bba\u4e2d\u7684\u4e00\u79cd\u57fa\u672c\u8d44\u6e90\uff0c\u9700\u8981\u6709\u6548\u7684\u7ea0\u7f20\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u7edf\u4e00\u53c2\u6570\u5316\u6269\u5c55\u76f8\u5173\u5f20\u91cf\uff0c\u901a\u8fc7\u5e7f\u4e49Bloch\u8868\u793a\u548c\u4efb\u610f\u6b63\u4ea4\u57fa\u6784\u5efa\u3002\u63d0\u51fa\u4e00\u79cd\u6df7\u5408\u6a21\u5f0f\u77e9\u9635\u5c55\u5f00\u6280\u672f\uff0c\u5c06\u53cc\u4f53\u6807\u51c6\u63a8\u5e7f\u5230\u591a\u4f53\u7cfb\u7edf\u3002", "result": "\u63a8\u5bfc\u4e86\u591a\u4f53\u7cfb\u7edf\u7684\u82e5\u5e72\u53ef\u5206\u6027\u5224\u636e\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u4f8b\u5b50\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7684\u53ef\u5206\u6027\u5224\u636e\u5728\u68c0\u6d4b\u7ea0\u7f20\u65b9\u9762\u5177\u6709\u589e\u5f3a\u7684\u80fd\u529b\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6846\u67b6\u548c\u6280\u672f\u80fd\u591f\u6709\u6548\u5730\u68c0\u6d4b\u91cf\u5b50\u7cfb\u7edf\u4e2d\u7684\u7ea0\u7f20\uff0c\u5e76\u5728\u68c0\u6d4b\u80fd\u529b\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2510.24674", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.24674", "abs": "https://arxiv.org/abs/2510.24674", "authors": ["Bram De Cooman", "Johan Suykens"], "title": "Learning to Drive Safely with Hybrid Options", "comment": null, "summary": "Out of the many deep reinforcement learning approaches for autonomous\ndriving, only few make use of the options (or skills) framework. That is\nsurprising, as this framework is naturally suited for hierarchical control\napplications in general, and autonomous driving tasks in specific. Therefore,\nin this work the options framework is applied and tailored to autonomous\ndriving tasks on highways. More specifically, we define dedicated options for\nlongitudinal and lateral manoeuvres with embedded safety and comfort\nconstraints. This way, prior domain knowledge can be incorporated into the\nlearning process and the learned driving behaviour can be constrained more\neasily. We propose several setups for hierarchical control with options and\nderive practical algorithms following state-of-the-art reinforcement learning\ntechniques. By separately selecting actions for longitudinal and lateral\ncontrol, the introduced policies over combined and hybrid options obtain the\nsame expressiveness and flexibility that human drivers have, while being easier\nto interpret than classical policies over continuous actions. Of all the\ninvestigated approaches, these flexible policies over hybrid options perform\nthe best under varying traffic conditions, outperforming the baseline policies\nover actions.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5c06\u9009\u9879\u6846\u67b6\u5e94\u7528\u4e8e\u9ad8\u901f\u516c\u8def\u81ea\u52a8\u9a7e\u9a76\u4efb\u52a1\uff0c\u901a\u8fc7\u5b9a\u4e49\u7eb5\u5411\u548c\u6a2a\u5411\u673a\u52a8\u9009\u9879\u6765\u6574\u5408\u9886\u57df\u77e5\u8bc6\u548c\u7ea6\u675f\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u7075\u6d3b\u7684\u6df7\u5408\u9009\u9879\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u5728\u4e0d\u540c\u4ea4\u901a\u6761\u4ef6\u4e0b\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u7b56\u7565\u3002", "motivation": "\u5c3d\u7ba1\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u6709\u591a\u79cd\u5e94\u7528\uff0c\u4f46\u9009\u9879\uff08\u6216\u6280\u80fd\uff09\u6846\u67b6\u7684\u5e94\u7528\u8f83\u5c11\uff0c\u7136\u800c\u8be5\u6846\u67b6\u975e\u5e38\u9002\u5408\u81ea\u52a8\u9a7e\u9a76\u8fd9\u79cd\u5c42\u6b21\u5316\u63a7\u5236\u4efb\u52a1\u3002", "method": "\u5c06\u9009\u9879\u6846\u67b6\u5e94\u7528\u4e8e\u9ad8\u901f\u516c\u8def\u81ea\u52a8\u9a7e\u9a76\uff0c\u5b9a\u4e49\u4e86\u5305\u542b\u5b89\u5168\u548c\u8212\u9002\u7ea6\u675f\u7684\u7eb5\u5411\u548c\u6a2a\u5411\u673a\u52a8\u9009\u9879\uff0c\u5e76\u63d0\u51fa\u51e0\u79cd\u5c42\u6b21\u5316\u63a7\u5236\u8bbe\u7f6e\u548c\u76f8\u5e94\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u3002", "result": "\u63d0\u51fa\u7684\u7075\u6d3b\u7684\u6df7\u5408\u9009\u9879\u7b56\u7565\u5728\u4e0d\u540c\u4ea4\u901a\u6761\u4ef6\u4e0b\u8868\u73b0\u6700\u4f73\uff0c\u4f18\u4e8e\u57fa\u7ebf\u7b56\u7565\u3002", "conclusion": "\u901a\u8fc7\u5c06\u9009\u9879\u6846\u67b6\u5e94\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\uff0c\u5e76\u91c7\u7528\u7075\u6d3b\u7684\u6df7\u5408\u9009\u9879\u7b56\u7565\uff0c\u53ef\u4ee5\u5728\u4fdd\u6301\u6613\u4e8e\u89e3\u91ca\u6027\u7684\u540c\u65f6\uff0c\u83b7\u5f97\u4e0e\u4eba\u7c7b\u9a7e\u9a76\u5458\u76f8\u5f53\u7684\u8868\u8fbe\u80fd\u529b\u548c\u7075\u6d3b\u6027\u3002"}}
{"id": "2510.24512", "categories": ["eess.SP", "physics.geo-ph", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.24512", "abs": "https://arxiv.org/abs/2510.24512", "authors": ["Magnus Heimpel", "Irena Hajnsek", "Othmar Frey"], "title": "Quality Coefficients for Interferometric Phase Linking", "comment": null, "summary": "In multi-temporal InSAR, phase linking refers to the estimation of a\nsingle-reference interferometric phase history from the information contained\nin the coherence matrix of a distributed scatterer. Since the phase information\nin the coherence matrix is typically inconsistent, the extent to which the\nestimated phase history captures it must be assessed to exclude unreliable\npixels from further processing. We introduce three quality criteria in the form\nof coefficients, for threshold-based pixel selection: a coefficient based on\nclosure phase that quantifies the internal consistency of the phase information\nin the coherence matrix; a goodness-of-fit coefficient that quantifies how well\na resulting phase history estimate approximates the phase information according\nto the characteristic optimization model of a given phase linking method; and\nan ambiguity coefficient that compares the goodness of fit of the original\nestimate with that of an orthogonal alternative. We formulate the phase linking\nmethods and these criteria within a unified mathematical framework and discuss\ncomputational and algorithmic aspects. Unlike existing goodness-of-fit\nindicators, the proposed coefficients are normalized to the unit interval with\nexplicit noise-floor correction, improving interpretability across stacks of\ndifferent size. Experiments on TerraSAR-X data over Visp, Switzerland, indicate\nthat the closure phase coefficient effectively pre-screens stable areas, the\ngoodness-of-fit coefficient aligns with and systematically generalizes\nestablished quality indicators, and the ambiguity coefficient flags solutions\nthat fit well but are unstable. Together, the coefficients enable systematic\npixel selection and quality control in the interferometric processing of\ndistributed scatterers.", "AI": {"tldr": "\u63d0\u51fa\u4e09\u79cd\u65b0\u7684\u8d28\u91cf\u7cfb\u6570\uff0c\u7528\u4e8e\u5728\u591a\u65f6\u76f8InSAR\u4e2d\u5bf9\u5206\u5e03\u5f0f\u6563\u5e03\u4f53\u8fdb\u884c\u50cf\u7d20\u9009\u62e9\u548c\u8d28\u91cf\u63a7\u5236\u3002", "motivation": "\u5728\u591a\u65f6\u76f8InSAR\u4e2d\uff0c\u7531\u4e8e\u76f8\u5e72\u77e9\u9635\u4e2d\u5305\u542b\u7684\u4e0d\u4e00\u81f4\u76f8\u4f4d\u4fe1\u606f\uff0c\u9700\u8981\u8bc4\u4f30\u76f8\u4f4d\u4f30\u8ba1\u7684\u53ef\u9760\u6027\uff0c\u4ee5\u6392\u9664\u4e0d\u53ef\u9760\u7684\u50cf\u7d20\u3002", "method": "\u63d0\u51fa\u4e09\u79cd\u8d28\u91cf\u7cfb\u6570\uff1a\u57fa\u4e8e\u95ed\u5408\u76f8\u4f4d\uff08\u91cf\u5316\u76f8\u4f4d\u4fe1\u606f\u5185\u90e8\u4e00\u81f4\u6027\uff09\u3001\u57fa\u4e8e\u62df\u5408\u4f18\u5ea6\uff08\u91cf\u5316\u76f8\u4f4d\u4f30\u8ba1\u4e0e\u4f18\u5316\u6a21\u578b\u7684\u62df\u5408\u7a0b\u5ea6\uff09\u548c\u57fa\u4e8e\u6a21\u7cca\u6027\uff08\u6bd4\u8f83\u539f\u59cb\u4f30\u8ba1\u4e0e\u6b63\u4ea4\u4f30\u8ba1\u7684\u62df\u5408\u4f18\u5ea6\uff09\u3002\u5c06\u8fd9\u4e9b\u7cfb\u6570\u4e0e\u76f8\u4f4d\u8fde\u63a5\u65b9\u6cd5\u7edf\u4e00\u5728\u6570\u5b66\u6846\u67b6\u5185\u8fdb\u884c\u8ba8\u8bba\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u95ed\u5408\u76f8\u4f4d\u7cfb\u6570\u80fd\u6709\u6548\u7b5b\u9009\u7a33\u5b9a\u533a\u57df\uff1b\u62df\u5408\u4f18\u5ea6\u7cfb\u6570\u4e0e\u73b0\u6709\u8d28\u91cf\u6307\u6807\u4e00\u81f4\u5e76\u7cfb\u7edf\u6027\u5730\u63a8\u5e7f\uff1b\u6a21\u7cca\u6027\u7cfb\u6570\u80fd\u6807\u8bb0\u62df\u5408\u826f\u597d\u4f46\u4e0d\u7a33\u5b9a\u7684\u89e3\u3002", "conclusion": "\u8fd9\u4e09\u79cd\u7cfb\u6570\u7684\u63d0\u51fa\uff0c\u80fd\u591f\u5bf9\u5e72\u6d89\u5904\u7406\u4e2d\u7684\u5206\u5e03\u5f0f\u6563\u5e03\u4f53\u8fdb\u884c\u7cfb\u7edf\u7684\u50cf\u7d20\u9009\u62e9\u548c\u8d28\u91cf\u63a7\u5236\u3002"}}
{"id": "2510.23652", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23652", "abs": "https://arxiv.org/abs/2510.23652", "authors": ["Yao Lu", "Yuqi Li", "Wenbin Xie", "Shanqing Yu", "Qi Xuan", "Zhaowei Zhu", "Shiping Wen"], "title": "The Structural Scalpel: Automated Contiguous Layer Pruning for Large Language Models", "comment": null, "summary": "Although large language models (LLMs) have achieved revolutionary\nbreakthroughs in many fields, their large model size and high computational\ncost pose significant challenges for practical deployment on\nresource-constrained edge devices. To this end, layer pruning has been proposed\nto reduce the computational overhead by directly removing redundant layers.\nHowever, existing layer pruning methods typically rely on hand-crafted metrics\nto evaluate and remove individual layers, while ignoring the dependencies\nbetween layers. This can disrupt the model's information flow and severely\ndegrade performance. To address these issues, we propose CLP, a novel\ncontinuous layer pruning framework that introduces two key innovations: a\ndifferentiable concave gate algorithm that automatically identifies the best\ncontinuous layer segments for pruning via gradient-based optimization; and a\ncutoff endpoint tuning strategy that effectively restores model performance by\nfine-tuning only the layers adjacent to the pruned segments. Extensive\nexperiments across multiple model architectures (including LLaMA2, LLaMA3 and\nQwen) and sizes (from $7$B to $70$B parameters) show that CLP significantly\noutperforms existing state-of-the-art baselines. For example, at a pruning rate\nof $20\\%$, CLP achieves an average performance retention of $95.34\\%$ on\nLLaMA3-70B, outperforming baselines by $4.29\\%$-$30.52\\%$. Furthermore, CLP can\nbe seamlessly combined with quantization to further compress the model with\nonly a slight performance loss.", "AI": {"tldr": "CLP\u662f\u4e00\u4e2a\u521b\u65b0\u7684\u8fde\u7eed\u5c42\u526a\u679d\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u5fae\u51f9\u95e8\u7b97\u6cd5\u81ea\u52a8\u8bc6\u522b\u5e76\u526a\u679d\u5197\u4f59\u5c42\uff0c\u5e76\u5229\u7528\u622a\u6b62\u70b9\u8c03\u6574\u7b56\u7565\u6062\u590d\u6a21\u578b\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u53ef\u4e0e\u91cf\u5316\u7ed3\u5408\u4ee5\u8fdb\u4e00\u6b65\u538b\u7f29\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u56e0\u6a21\u578b\u5e9e\u5927\u548c\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u96be\u4ee5\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72\u3002\u5c42\u526a\u679d\u867d\u7136\u80fd\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u4e86\u5c42\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\uff0c\u53ef\u80fd\u7834\u574f\u4fe1\u606f\u6d41\u5e76\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCLP\u7684\u8fde\u7eed\u5c42\u526a\u679d\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u4e3b\u8981\u521b\u65b0\uff1a1. \u53ef\u5fae\u51f9\u95e8\u7b97\u6cd5\uff1a\u81ea\u52a8\u8bc6\u522b\u5e76\u526a\u679d\u6700\u4f73\u8fde\u7eed\u5c42\u6bb5\u30022. \u622a\u6b62\u70b9\u8c03\u6574\u7b56\u7565\uff1a\u901a\u8fc7\u5fae\u8c03\u526a\u679d\u5c42\u6bb5\u7684\u76f8\u90bb\u5c42\u6765\u6062\u590d\u6a21\u578b\u6027\u80fd\u3002", "result": "\u5728LLaMA2\u3001LLaMA3\u548cQwen\u7b49\u591a\u79cd\u6a21\u578b\u67b6\u6784\u548c\u5927\u5c0f\uff087B\u81f370B\u53c2\u6570\uff09\u4e0a\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cCLP\u7684\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002\u4f8b\u5982\uff0c\u572820%\u7684\u526a\u679d\u7387\u4e0b\uff0cCLP\u5728LLaMA3-70B\u4e0a\u5b9e\u73b0\u4e8695.34%\u7684\u5e73\u5747\u6027\u80fd\u4fdd\u6301\u7387\uff0c\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u9ad8\u51fa4.29%-30.52%\u3002\u6b64\u5916\uff0cCLP\u53ef\u4e0e\u91cf\u5316\u6280\u672f\u7ed3\u5408\uff0c\u4ee5\u8f83\u5c0f\u7684\u6027\u80fd\u635f\u5931\u8fdb\u4e00\u6b65\u538b\u7f29\u6a21\u578b\u3002", "conclusion": "CLP\u6846\u67b6\u901a\u8fc7\u5f15\u5165\u53ef\u5fae\u51f9\u95e8\u7b97\u6cd5\u548c\u622a\u6b62\u70b9\u8c03\u6574\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u5c42\u526a\u679d\u65b9\u6cd5\u5ffd\u7565\u5c42\u95f4\u4f9d\u8d56\u6027\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u6a21\u578b\u538b\u7f29\uff0c\u5e76\u4e14\u53ef\u4ee5\u4e0e\u91cf\u5316\u6280\u672f\u534f\u540c\u5de5\u4f5c\u3002"}}
{"id": "2510.24085", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24085", "abs": "https://arxiv.org/abs/2510.24085", "authors": ["Md. Shihab Uddin", "Md Nazmus Shakib", "Rahul Bhadani"], "title": "Modeling Electric Vehicle Car-Following Behavior: Classical vs Machine Learning Approach", "comment": null, "summary": "The increasing adoption of electric vehicles (EVs) necessitates an\nunderstanding of their driving behavior to enhance traffic safety and develop\nsmart driving systems. This study compares classical and machine learning\nmodels for EV car following behavior. Classical models include the Intelligent\nDriver Model (IDM), Optimum Velocity Model (OVM), Optimal Velocity Relative\nVelocity (OVRV), and a simplified CACC model, while the machine learning\napproach employs a Random Forest Regressor. Using a real world dataset of an EV\nfollowing an internal combustion engine (ICE) vehicle under varied driving\nconditions, we calibrated classical model parameters by minimizing the RMSE\nbetween predictions and real data. The Random Forest model predicts\nacceleration using spacing, speed, and gap type as inputs. Results demonstrate\nthe Random Forest's superior accuracy, achieving RMSEs of 0.0046 (medium gap),\n0.0016 (long gap), and 0.0025 (extra long gap). Among physics based models,\nCACC performed best, with an RMSE of 2.67 for long gaps. These findings\nhighlight the machine learning model's performance across all scenarios. Such\nmodels are valuable for simulating EV behavior and analyzing mixed autonomy\ntraffic dynamics in EV integrated environments.", "AI": {"tldr": "\u672c\u7814\u7a76\u6bd4\u8f83\u4e86\u7ecf\u5178\u6a21\u578b\u548c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u7535\u52a8\u6c7d\u8f66\uff08EV\uff09\u8ddf\u8f66\u884c\u4e3a\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff08\u968f\u673a\u68ee\u6797\u56de\u5f52\u5668\uff09\u7684\u9884\u6d4b\u7cbe\u5ea6\u8fdc\u9ad8\u4e8e\u7ecf\u5178\u6a21\u578b\u3002", "motivation": "\u4e3a\u4e86\u89e3\u7535\u52a8\u6c7d\u8f66\u7684\u9a7e\u9a76\u884c\u4e3a\uff0c\u4ee5\u63d0\u9ad8\u4ea4\u901a\u5b89\u5168\u548c\u5f00\u53d1\u667a\u80fd\u9a7e\u9a76\u7cfb\u7edf\u3002", "method": "\u4f7f\u7528\u771f\u5b9e\u6570\u636e\u96c6\uff0c\u5bf9\u7ecf\u5178\u6a21\u578b\uff08IDM\u3001OVM\u3001OVRV\u3001CACC\uff09\u7684\u53c2\u6570\u8fdb\u884c\u6821\u51c6\uff0c\u5e76\u4f7f\u7528\u968f\u673a\u68ee\u6797\u56de\u5f52\u5668\u8fdb\u884c\u9884\u6d4b\u3002", "result": "\u968f\u673a\u68ee\u6797\u56de\u5f52\u5668\u5728\u5404\u79cd\u95f4\u9699\u7c7b\u578b\u4e0b\u90fd\u53d6\u5f97\u4e86\u66f4\u9ad8\u7684\u51c6\u786e\u7387\uff08RMSE \u5206\u522b\u4e3a 0.0046\u30010.0016 \u548c 0.0025\uff09\u3002\u5728\u57fa\u4e8e\u7269\u7406\u7684\u6a21\u578b\u4e2d\uff0cCACC \u5728\u957f\u95f4\u9699\u4e0b\u7684\u8868\u73b0\u6700\u4f73\uff08RMSE \u4e3a 2.67\uff09\u3002", "conclusion": "\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u6a21\u62df\u7535\u52a8\u6c7d\u8f66\u884c\u4e3a\u548c\u5206\u6790\u6df7\u5408\u81ea\u52a8\u9a7e\u9a76\u4ea4\u901a\u52a8\u6001\u65b9\u9762\u5177\u6709\u4f18\u52bf\u3002"}}
{"id": "2510.24021", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24021", "abs": "https://arxiv.org/abs/2510.24021", "authors": ["Haiduo Huang", "Jiangcheng Song", "Yadong Zhang", "Pengju Ren"], "title": "SpecKD: Speculative Decoding for Effective Knowledge Distillation of LLMs", "comment": null, "summary": "Knowledge Distillation (KD) has become a cornerstone technique for\ncompressing Large Language Models (LLMs) into smaller, more efficient student\nmodels. However, conventional KD approaches typically apply the distillation\nloss uniformly across all tokens, regardless of the teacher's confidence. This\nindiscriminate mimicry can introduce noise, as the student is forced to learn\nfrom the teacher's uncertain or high-entropy predictions, which may ultimately\nharm student performance-especially when the teacher is much larger and more\npowerful. To address this, we propose Speculative Knowledge Distillation\n(SpecKD), a novel, plug-and-play framework that introduces a dynamic,\ntoken-level gating mechanism inspired by the \"propose-and-verify\" paradigm of\nspeculative decoding. At each step, the student's token proposal is verified\nagainst the teacher's distribution; the distillation loss is selectively\napplied only to \"accepted\" tokens, while \"rejected\" tokens are masked out.\nExtensive experiments on diverse text generation tasks show that SpecKD\nconsistently and significantly outperforms strong KD baselines, leading to more\nstable training and more capable student models, and achieving state-of-the-art\nresults.", "AI": {"tldr": "SpecKD \u662f\u4e00\u79cd\u65b0\u9896\u7684\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u52a8\u6001\u7684\u3001token\u7ea7\u522b\u7684\u95e8\u63a7\u673a\u5236\uff0c\u4ec5\u5728teacher\u6a21\u578b\u7f6e\u4fe1\u5ea6\u9ad8\u65f6\u5e94\u7528\u84b8\u998f\u635f\u5931\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u5b66\u751f\u6a21\u578b\u7684\u6027\u80fd\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u4f20\u7edf\u7684\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u5c06\u84b8\u998f\u635f\u5931\u5e94\u7528\u4e8e\u6240\u6709token\uff0c\u5373\u4f7f\u5728teacher\u6a21\u578b\u4e0d\u786e\u5b9a\u65f6\u4e5f\u662f\u5982\u6b64\uff0c\u8fd9\u53ef\u80fd\u5f15\u5165\u566a\u58f0\u5e76\u635f\u5bb3\u5b66\u751f\u6a21\u578b\u7684\u6027\u80fd\u3002\u5c24\u5176\u662f\u5728teacher\u6a21\u578b\u8fdc\u5927\u4e8estudent\u6a21\u578b\u65f6\uff0c\u8fd9\u79cd\u5f71\u54cd\u66f4\u4e3a\u663e\u8457\u3002", "method": "SpecKD \u6846\u67b6\u5f15\u5165\u4e86\u4e00\u79cd\u52a8\u6001\u7684\u3001token\u7ea7\u522b\u7684\u95e8\u63a7\u673a\u5236\uff0c\u8be5\u673a\u5236\u501f\u9274\u4e86\u63a8\u6d4b\u89e3\u7801\u7684\u201c\u63d0\u51fa-\u9a8c\u8bc1\u201d\u8303\u5f0f\u3002\u5728\u6bcf\u4e00\u6b65\uff0c\u5b66\u751f\u6a21\u578b\u7684token\u5efa\u8bae\u4f1a\u6839\u636eteacher\u6a21\u578b\u7684\u5206\u5e03\u8fdb\u884c\u9a8c\u8bc1\uff1b\u53ea\u6709\u88ab\u201c\u63a5\u53d7\u201d\u7684token\u624d\u4f1a\u5e94\u7528\u84b8\u998f\u635f\u5931\uff0c\u800c\u201c\u62d2\u7edd\u201d\u7684token\u5219\u88ab\u5c4f\u853d\u3002", "result": "\u5728\u5404\u79cd\u6587\u672c\u751f\u6210\u4efb\u52a1\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cSpecKD \u6301\u7eed\u4e14\u663e\u8457\u5730\u4f18\u4e8e\u5f3a\u5927\u7684\u77e5\u8bc6\u84b8\u998f\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u66f4\u7a33\u5b9a\u7684\u8bad\u7ec3\u548c\u66f4\u5f3a\u5927\u7684\u5b66\u751f\u6a21\u578b\uff0c\u5e76\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "conclusion": "SpecKD \u901a\u8fc7\u9009\u62e9\u6027\u5730\u5e94\u7528\u84b8\u998f\u635f\u5931\uff0c\u6709\u6548\u5730\u89e3\u51b3\u4e86\u4f20\u7edf\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u4e2d\u7684\u566a\u58f0\u95ee\u9898\uff0c\u4e3a\u8bad\u7ec3\u66f4\u9ad8\u6548\u3001\u6027\u80fd\u66f4\u4f18\u7684\u5b66\u751f\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u9896\u4e14\u6709\u6548\u7684\u65b9\u6cd5\u3002"}}
{"id": "2510.24335", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24335", "abs": "https://arxiv.org/abs/2510.24335", "authors": ["Mingyu Jeong", "Eunsung Kim", "Sehun Park", "Andrew Jaeyong Choi"], "title": "NVSim: Novel View Synthesis Simulator for Large Scale Indoor Navigation", "comment": "9 pages, 10 figures", "summary": "We present NVSim, a framework that automatically constructs large-scale,\nnavigable indoor simulators from only common image sequences, overcoming the\ncost and scalability limitations of traditional 3D scanning. Our approach\nadapts 3D Gaussian Splatting to address visual artifacts on sparsely observed\nfloors a common issue in robotic traversal data. We introduce Floor-Aware\nGaussian Splatting to ensure a clean, navigable ground plane, and a novel\nmesh-free traversability checking algorithm that constructs a topological graph\nby directly analyzing rendered views. We demonstrate our system's ability to\ngenerate valid, large-scale navigation graphs from real-world data. A video\ndemonstration is avilable at https://youtu.be/tTiIQt6nXC8", "AI": {"tldr": "NVSim\u5229\u75283D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\uff0c\u901a\u8fc7\u5206\u6790\u56fe\u50cf\u5e8f\u5217\u81ea\u52a8\u6784\u5efa\u53ef\u5bfc\u822a\u7684\u5ba4\u5185\u6a21\u62df\u5668\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf3D\u626b\u63cf\u7684\u6210\u672c\u548c\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65e0\u7f51\u683c\u53ef\u901a\u884c\u6027\u68c0\u67e5\u7b97\u6cd5\u3002", "motivation": "\u514b\u670d\u4f20\u7edf3D\u626b\u63cf\u65b9\u6cd5\u7684\u6210\u672c\u548c\u53ef\u6269\u5c55\u6027\u9650\u5236\uff0c\u81ea\u52a8\u4ece\u56fe\u50cf\u5e8f\u5217\u6784\u5efa\u5927\u89c4\u6a21\u3001\u53ef\u5bfc\u822a\u7684\u5ba4\u5185\u6a21\u62df\u5668\u3002", "method": "1. \u91c7\u75283D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u5904\u7406\u673a\u5668\u4eba\u904d\u5386\u6570\u636e\u4e2d\u5e38\u89c1\u4e8e\u7a00\u758f\u89c2\u5bdf\u7684\u697c\u5c42\u89c6\u89c9\u4f2a\u5f71\u3002 2. \u63d0\u51fa\u201c\u5730\u6bef\u611f\u77e5\u9ad8\u65af\u6cfc\u6e85\u201d\uff08Floor-Aware Gaussian Splatting\uff09\u4ee5\u786e\u4fdd\u5e72\u51c0\u3001\u53ef\u5bfc\u822a\u7684\u5730\u9762\u3002 3. \u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u65e0\u7f51\u683c\u53ef\u901a\u884c\u6027\u68c0\u67e5\u7b97\u6cd5\uff0c\u901a\u8fc7\u76f4\u63a5\u5206\u6790\u6e32\u67d3\u89c6\u56fe\u6765\u6784\u5efa\u62d3\u6251\u56fe\u3002", "result": "\u6210\u529f\u751f\u6210\u4e86\u53ef\u5bfc\u822a\u7684\u5ba4\u5185\u6a21\u62df\u5668\uff0c\u5e76\u80fd\u4ece\u771f\u5b9e\u4e16\u754c\u6570\u636e\u751f\u6210\u6709\u6548\u7684\u3001\u5927\u89c4\u6a21\u7684\u5bfc\u822a\u56fe\u3002", "conclusion": "NVSim \u6846\u67b6\u80fd\u591f\u6709\u6548\u5730\u4ece\u5e38\u89c1\u7684\u56fe\u50cf\u5e8f\u5217\u81ea\u52a8\u6784\u5efa\u5927\u89c4\u6a21\u3001\u53ef\u5bfc\u822a\u7684\u5ba4\u5185\u6a21\u62df\u5668\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2510.24037", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24037", "abs": "https://arxiv.org/abs/2510.24037", "authors": ["Shufan Shen", "Junshu Sun", "Shuhui Wang", "Qingming Huang"], "title": "Kernelized Sparse Fine-Tuning with Bi-level Parameter Competition for Vision Models", "comment": null, "summary": "Parameter-efficient fine-tuning (PEFT) aims to adapt pre-trained vision\nmodels to downstream tasks. Among PEFT paradigms, sparse tuning achieves\nremarkable performance by adjusting only the weights most relevant to\ndownstream tasks, rather than densely tuning the entire weight matrix. Current\nmethods follow a two-stage paradigm. First, it locates task-relevant weights by\ngradient information, which overlooks the parameter adjustments during\nfine-tuning and limits the performance. Second, it updates only the located\nweights by applying a sparse mask to the gradient of the weight matrix, which\nresults in high memory usage due to the storage of all weight matrices in the\noptimizer. In this paper, we propose a one-stage method named SNELLA to\novercome the above limitations. For memory usage, SNELLA selectively updates\nthe weight matrix by adding it to another sparse matrix that is merged by two\nlow-rank learnable matrices. We extend the low-rank decomposition by\nintroducing nonlinear kernel functions, thereby increasing the rank of the\nresulting merged matrix to prevent the interdependency among weight updates,\nenabling better adaptation to downstream tasks. For locating task-relevant\nweights, we propose an adaptive bi-level sparsity allocation mechanism that\nencourages weights to compete across and inside layers based on their\nimportance scores in an end-to-end manner. Extensive experiments are conducted\non classification, segmentation, and generation tasks using different\npre-trained vision models. The results show that SNELLA achieves SOTA\nperformance with low memory usage. Notably, SNELLA obtains 1.8% (91.9% v.s.\n90.1%) higher Top-1 accuracy on the FGVC benchmark compared to SPT-LoRA.\nCompared to previous methods, SNELLA achieves a memory reduction of 31.1%-39.9%\nacross models with parameter scales from 86M to 632M. Our source codes are\navailable at https://github.com/ssfgunner/SNELL.", "AI": {"tldr": "SNELLA\u662f\u4e00\u79cd\u65b0\u7684\u4e00\u9636\u6bb5\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08PEFT\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u66f4\u65b0\u6743\u91cd\u77e9\u9635\u5e76\u5f15\u5165\u81ea\u9002\u5e94\u53cc\u5c42\u7a00\u758f\u5206\u914d\u673a\u5236\uff0c\u5728\u4e0d\u663e\u8457\u589e\u52a0\u5185\u5b58\u5360\u7528\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u5404\u79cd\u4e0b\u6e38\u89c6\u89c9\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u663e\u8457\u51cf\u5c11\u4e86\u5185\u5b58\u4f7f\u7528\u91cf\u3002", "motivation": "\u73b0\u6709\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08PEFT\uff09\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u7a00\u758f\u5fae\u8c03\uff0c\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u4e24\u9636\u6bb5\u65b9\u6cd5\u5728\u5b9a\u4f4d\u4efb\u52a1\u76f8\u5173\u6743\u91cd\u65f6\uff0c\u5ffd\u7565\u4e86\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u7684\u53c2\u6570\u8c03\u6574\uff0c\u9650\u5236\u4e86\u6027\u80fd\uff1b2\uff09\u5728\u66f4\u65b0\u6743\u91cd\u65f6\uff0c\u9700\u8981\u5b58\u50a8\u6240\u6709\u6743\u91cd\u77e9\u9635\uff0c\u5bfc\u81f4\u5185\u5b58\u5360\u7528\u8fc7\u9ad8\u3002SNELLA\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u5c40\u9650\u6027\u3002", "method": "SNELLA\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e00\u9636\u6bb5\u65b9\u6cd5\u3002\u4e3a\u4e86\u89e3\u51b3\u5185\u5b58\u5360\u7528\u95ee\u9898\uff0cSNELLA\u901a\u8fc7\u5c06\u6743\u91cd\u77e9\u9635\u4e0e\u7531\u4e24\u4e2a\u4f4e\u79e9\u53ef\u5b66\u4e60\u77e9\u9635\u5408\u5e76\u800c\u6210\u7684\u7a00\u758f\u77e9\u9635\u76f8\u52a0\u6765\u9009\u62e9\u6027\u5730\u66f4\u65b0\u6743\u91cd\u77e9\u9635\u3002\u901a\u8fc7\u5f15\u5165\u975e\u7ebf\u6027\u6838\u51fd\u6570\u6269\u5c55\u4f4e\u79e9\u5206\u89e3\uff0c\u589e\u52a0\u4e86\u5408\u5e76\u77e9\u9635\u7684\u79e9\uff0c\u4ee5\u9632\u6b62\u6743\u91cd\u66f4\u65b0\u4e4b\u95f4\u7684\u76f8\u4e92\u4f9d\u8d56\u6027\u3002\u4e3a\u4e86\u5b9a\u4f4d\u4efb\u52a1\u76f8\u5173\u6743\u91cd\uff0cSNELLA\u63d0\u51fa\u4e86\u4e00\u4e2a\u81ea\u9002\u5e94\u53cc\u5c42\u7a00\u758f\u5206\u914d\u673a\u5236\uff0c\u8be5\u673a\u5236\u4ee5\u7aef\u5230\u7aef\u7684\u65b9\u5f0f\u6839\u636e\u91cd\u8981\u6027\u5206\u6570\u5bf9\u5c42\u5185\u548c\u5c42\u95f4\u7684\u6743\u91cd\u8fdb\u884c\u7ade\u4e89\u3002", "result": "SNELLA\u5728\u5206\u7c7b\u3001\u5206\u5272\u548c\u751f\u6210\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u4f7f\u7528\u4e86\u4e0d\u540c\u7684\u9884\u8bad\u7ec3\u89c6\u89c9\u6a21\u578b\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSNELLA\u5728\u5185\u5b58\u5360\u7528\u4f4e\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u5177\u4f53\u800c\u8a00\uff0cSNELLA\u5728FGVC\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u6bd4SPT-LoRA\u63d0\u9ad8\u4e861.8%\u7684Top-1\u51c6\u786e\u7387\uff0891.9% vs. 90.1%\uff09\u3002\u4e0e\u4e4b\u524d\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0cSNELLA\u572886M\u5230632M\u53c2\u6570\u89c4\u6a21\u7684\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e8631.1%-39.9%\u7684\u5185\u5b58\u51cf\u5c11\u3002", "conclusion": "SNELLA\u901a\u8fc7\u5176\u521b\u65b0\u7684\u65b9\u6cd5\uff0c\u5728\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u9886\u57df\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u5c24\u5176\u662f\u5728\u89e3\u51b3\u5185\u5b58\u5360\u7528\u548c\u63d0\u9ad8\u9002\u5e94\u6027\u65b9\u9762\u3002\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u548c\u663e\u8457\u7684\u5185\u5b58\u8282\u7701\uff0c\u4f7f\u5176\u6210\u4e3a\u4e00\u9879\u6709\u524d\u9014\u7684\u6280\u672f\uff0c\u9002\u7528\u4e8e\u5404\u79cd\u4e0b\u6e38\u89c6\u89c9\u4efb\u52a1\u3002"}}
{"id": "2510.24137", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.24137", "abs": "https://arxiv.org/abs/2510.24137", "authors": ["Sojeong Park", "Changhun Oh"], "title": "Matrix product state approach to lossy boson sampling and noisy IQP sampling", "comment": "19 pages, 10 figures", "summary": "Sampling problems have emerged as a central avenue for demonstrating quantum\nadvantage on noisy intermediate-scale quantum devices. However, physical noise\ncan fundamentally alter their computational complexity, often making them\nclassically tractable. Motivated by the recent success of matrix product state\n(MPS)-based classical simulation of Gaussian boson sampling (Oh et al., 2024),\nwe extend this framework to investigate the classical simulability of other\nnoisy quantum sampling models. We develop MPS-based classical algorithms for\nlossy boson sampling and noisy instantaneous quantum polynomial-time (IQP)\nsampling, both of which retain the tunable accuracy characteristic of the MPS\napproach through the bond dimension. Our approach constructs pure-state\ndecompositions of noisy or lossy input states whose components remain weakly\nentangled after circuit evolution, thereby providing a means to systematically\nexplore the boundary between quantum-hard and classically-simulable regimes.\nFor boson sampling, we analyze single-photon, Fock, and cat-state inputs,\nshowing that classical simulability emerges at transmission rates scaling as\n$O(1/\\sqrt{N})$, reaching the known boundary of quantum advantage with a\ntunable and scalable method. Beyond reproducing previous thresholds, our\nalgorithm offers significantly improved control over the accuracy-efficiency\ntrade-off. It further extends the applicability of MPS-based simulation to\nbroader classes of noisy quantum sampling models, including IQP circuits.", "AI": {"tldr": "MPS\u65b9\u6cd5\u53ef\u7528\u4e8e\u6a21\u62df\u6709\u635f\u548c\u566a\u58f0IQP\u91c7\u6837\uff0c\u5e76\u5206\u6790\u4e86\u5176\u7ecf\u5178\u53ef\u6a21\u62df\u6027\u7684\u8fb9\u754c\u3002", "motivation": "\u7814\u7a76\u6700\u8fd1\u6210\u529f\u7684\u57fa\u4e8eMPS\u7684\u91cf\u5b50\u91c7\u6837\u6a21\u62df\uff0c\u5e76\u5c06\u5176\u6269\u5c55\u5230\u5176\u4ed6\u6709\u635f\u548c\u566a\u58f0\u91cf\u5b50\u91c7\u6837\u6a21\u578b\uff0c\u4ee5\u63a2\u7d22\u91cf\u5b50\u4f18\u52bf\u548c\u7ecf\u5178\u53ef\u6a21\u62df\u6027\u4e4b\u95f4\u7684\u754c\u9650\u3002", "method": "\u5f00\u53d1\u57fa\u4e8eMPS\u7684\u7ecf\u5178\u7b97\u6cd5\u6765\u6a21\u62df\u6709\u635f\u73bb\u8272\u5b50\u91c7\u6837\u548c\u566a\u58f0IQP\u91c7\u6837\uff0c\u901a\u8fc7\u77e9\u9635\u7ef4\u5ea6\u6765\u8c03\u6574\u7cbe\u5ea6\uff0c\u5e76\u7814\u7a76\u4e86\u4e0d\u540c\u8f93\u5165\u72b6\u6001\uff08\u5355\u5149\u5b50\u3001Fock\u3001cat\uff09\u4e0b\u7684\u6a21\u62df\u6548\u679c\u3002", "result": "\u8868\u660e\u5728\u4f20\u8f93\u7387\u4e0e1/sqrt(N)\u6210\u6bd4\u4f8b\u65f6\uff0c\u7ecf\u5178\u6a21\u62df\u6210\u4e3a\u53ef\u80fd\uff0c\u8fd9\u4e0e\u5df2\u77e5\u7684\u91cf\u5b50\u4f18\u52bf\u8fb9\u754c\u4e00\u81f4\u3002\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u5bf9\u7cbe\u5ea6-\u6548\u7387\u6743\u8861\u7684\u66f4\u597d\u63a7\u5236\uff0c\u5e76\u6269\u5c55\u4e86MPS\u6a21\u62df\u7684\u5e94\u7528\u8303\u56f4\u3002", "conclusion": "MPS\u65b9\u6cd5\u4e3a\u6a21\u62df\u66f4\u5e7f\u6cdb\u7684\u6709\u635f\u548c\u566a\u58f0\u91cf\u5b50\u91c7\u6837\u6a21\u578b\uff08\u5305\u62ecIQP\u7535\u8def\uff09\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u63a7\u4e14\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\uff0c\u6709\u52a9\u4e8e\u7cfb\u7edf\u5730\u63a2\u7d22\u91cf\u5b50\u786c\u5ea6\u548c\u7ecf\u5178\u53ef\u6a21\u62df\u6027\u7684\u8fb9\u754c\u3002"}}
{"id": "2510.24676", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.24676", "abs": "https://arxiv.org/abs/2510.24676", "authors": ["Jiaxuan Zhang", "Yuquan Leng", "Yixuan Guo", "Chenglong Fu"], "title": "Feature Matching-Based Gait Phase Prediction for Obstacle Crossing Control of Powered Transfemoral Prosthesis", "comment": "6 pages, conference", "summary": "For amputees with powered transfemoral prosthetics, navigating obstacles or\ncomplex terrain remains challenging. This study addresses this issue by using\nan inertial sensor on the sound ankle to guide obstacle-crossing movements. A\ngenetic algorithm computes the optimal neural network structure to predict the\nrequired angles of the thigh and knee joints. A gait progression prediction\nalgorithm determines the actuation angle index for the prosthetic knee motor,\nultimately defining the necessary thigh and knee angles and gait progression.\nResults show that when the standard deviation of Gaussian noise added to the\nthigh angle data is less than 1, the method can effectively eliminate noise\ninterference, achieving 100\\% accuracy in gait phase estimation under 150 Hz,\nwith thigh angle prediction error being 8.71\\% and knee angle prediction error\nbeing 6.78\\%. These findings demonstrate the method's ability to accurately\npredict gait progression and joint angles, offering significant practical value\nfor obstacle negotiation in powered transfemoral prosthetics.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5229\u7528\u60ef\u6027\u4f20\u611f\u5668\u548c\u9057\u4f20\u7b97\u6cd5\u4f18\u5316\u795e\u7ecf\u7f51\u7edc\uff0c\u9884\u6d4b\u548c\u63a7\u5236\u4eff\u751f\u817f\u90e8\u5173\u8282\u89d2\u5ea6\uff0c\u4ee5\u5e2e\u52a9\u622a\u80a2\u8005\u8de8\u8d8a\u969c\u788d\u7269\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u6b65\u6001\u9884\u6d4b\u548c\u5173\u8282\u89d2\u5ea6\u9884\u6d4b\u3002", "motivation": "\u4e3a\u89e3\u51b3\u52a8\u529b\u5047\u80a2\u7528\u6237\u5728\u590d\u6742\u5730\u5f62\u548c\u8de8\u8d8a\u969c\u788d\u7269\u65f6\u9047\u5230\u7684\u6311\u6218\u3002", "method": "\u5229\u7528\u5b89\u88c5\u5728\u5065\u4fa7\u811a\u8e1d\u4e0a\u7684\u60ef\u6027\u4f20\u611f\u5668\uff0c\u901a\u8fc7\u9057\u4f20\u7b97\u6cd5\u4f18\u5316\u795e\u7ecf\u7f51\u7edc\uff0c\u9884\u6d4b\u6240\u9700\u7684\u5927\u817f\u548c\u819d\u76d6\u5173\u8282\u89d2\u5ea6\uff0c\u5e76\u7ed3\u5408\u6b65\u6001\u8fdb\u5c55\u9884\u6d4b\u7b97\u6cd5\u6765\u786e\u5b9a\u5047\u80a2\u819d\u76d6\u9a6c\u8fbe\u7684\u9a71\u52a8\u89d2\u5ea6\u7d22\u5f15\uff0c\u6700\u7ec8\u5b9e\u73b0\u5bf9\u5927\u817f\u548c\u819d\u76d6\u89d2\u5ea6\u4ee5\u53ca\u6b65\u6001\u8fdb\u5c55\u7684\u63a7\u5236\u3002", "result": "\u5728\u6807\u51c6\u5dee\u5c0f\u4e8e1\u7684\u60c5\u51b5\u4e0b\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u6d88\u9664\u566a\u58f0\u5e72\u6270\uff0c\u5728150Hz\u4e0b\u5b9e\u73b0\u4e86100%\u7684\u6b65\u6001\u76f8\u4f4d\u4f30\u8ba1\u51c6\u786e\u7387\uff0c\u5927\u817f\u89d2\u5ea6\u9884\u6d4b\u8bef\u5dee\u4e3a8.71%\uff0c\u819d\u76d6\u89d2\u5ea6\u9884\u6d4b\u8bef\u5dee\u4e3a6.78%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u51c6\u786e\u9884\u6d4b\u6b65\u6001\u8fdb\u5c55\u548c\u5173\u8282\u89d2\u5ea6\uff0c\u5728\u52a8\u529b\u5047\u80a2\u8de8\u8d8a\u969c\u788d\u7269\u65b9\u9762\u5177\u6709\u91cd\u8981\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.24569", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.24569", "abs": "https://arxiv.org/abs/2510.24569", "authors": ["Ashkan Jafari Fesharaki", "Yasser Mestrah", "Yi Ma", "Rahim Tafazolli", "Ibrahim Hemadeh", "Mohammad Heggo", "Arman Shojaeifard", "Javier Lorca Hernando", "Alain Mourad"], "title": "Advanced Closed-Loop Method with Limited Feedback for ISAC", "comment": null, "summary": "6G wireless networks are poised to seamlessly integrate communication,\ncomputing, localization, and sensing functionalities, ensuring high reliability\nand trustworthiness. This paper introduces Smart Sensing Feedback (SSF), a\nlimited-feedback framework designed to enhance sensing capabilities while\nmaintaining communication performance. SSF adapts the concept of retransmission\nfrom communication to sensing. Specifically, we focus on downlink (DL) bistatic\nsensing, where the User Equipment (UE) performs measurements from reflected\nsensing signals and provides feedback to the network (NW). In sensing services,\nUE reporting can vary significantly due to dynamic factors such as target\ncharacteristics, environmental conditions, and UE status. Our results\ndemonstrate that SSF significantly improves sensing quality while preserving\ncommunication efficiency. Additionally, it enhances key performance metrics\nsuch as probability of detection, latency, and power consumption. These\nimprovements underscore SSF's ability to deliver robust, low-overhead feedback\nand adaptability to support a wide range of ISAC applications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u667a\u80fd\u4f20\u611f\u53cd\u9988\uff08SSF\uff09\u7684\u6709\u9650\u53cd\u9988\u6846\u67b6\uff0c\u7528\u4e8e\u589e\u5f3a6G\u65e0\u7ebf\u7f51\u7edc\u4e2d\u7684\u4f20\u611f\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u901a\u4fe1\u6027\u80fd\u3002SSF\u5c06\u901a\u4fe1\u4e2d\u7684\u91cd\u4f20\u6982\u5ff5\u5e94\u7528\u4e8e\u4f20\u611f\uff0c\u7279\u522b\u662f\u5728\u4ee5\u4e0b\u884c\uff08DL\uff09\u53cc\u7ad9\u4f20\u611f\u573a\u666f\u4e2d\uff0c\u7528\u6237\u8bbe\u5907\uff08UE\uff09\u6d4b\u91cf\u53cd\u5c04\u7684\u4f20\u611f\u4fe1\u53f7\u5e76\u5411\u7f51\u7edc\uff08NW\uff09\u63d0\u4f9b\u53cd\u9988\u3002", "motivation": "6G\u65e0\u7ebf\u7f51\u7edc\u65e8\u5728\u65e0\u7f1d\u96c6\u6210\u901a\u4fe1\u3001\u8ba1\u7b97\u3001\u5b9a\u4f4d\u548c\u4f20\u611f\u529f\u80fd\uff0c\u5e76\u786e\u4fdd\u9ad8\u53ef\u9760\u6027\u548c\u53ef\u4fe1\u5ea6\u3002\u8be5\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u5728\u52a8\u6001\u53d8\u5316\u7684\u6761\u4ef6\u4e0b\uff0cUE\u62a5\u544a\u7684\u4f20\u611f\u4fe1\u606f\u53ef\u80fd\u5b58\u5728\u663e\u8457\u5dee\u5f02\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u589e\u5f3a\u4f20\u611f\u8d28\u91cf\u548c\u901a\u4fe1\u6548\u7387\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "SSF\u6846\u67b6\u501f\u9274\u4e86\u901a\u4fe1\u4e2d\u7684\u91cd\u4f20\u673a\u5236\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8eDL\u53cc\u7ad9\u4f20\u611f\u3002UE\u6d4b\u91cf\u53cd\u5c04\u7684\u4f20\u611f\u4fe1\u53f7\uff0c\u5e76\u5411\u7f51\u7edc\u63d0\u4f9b\u53cd\u9988\u3002\u8be5\u65b9\u6cd5\u4fa7\u91cd\u4e8eUE\u62a5\u544a\u7684\u9002\u5e94\u6027\u548c\u4f4e\u5f00\u9500\u3002", "result": "SSF\u663e\u8457\u63d0\u9ad8\u4e86\u4f20\u611f\u8d28\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u901a\u4fe1\u6548\u7387\u3002\u6b64\u5916\uff0c\u5b83\u8fd8\u63d0\u9ad8\u4e86\u68c0\u6d4b\u6982\u7387\u3001\u5ef6\u8fdf\u548c\u529f\u8017\u7b49\u5173\u952e\u6027\u80fd\u6307\u6807\u3002", "conclusion": "SSF\u80fd\u591f\u63d0\u4f9b\u5f3a\u5927\u3001\u4f4e\u5f00\u9500\u4e14\u9002\u5e94\u6027\u5f3a\u7684\u53cd\u9988\uff0c\u4ee5\u652f\u6301\u5e7f\u6cdb\u7684\u96c6\u6210\u4f20\u611f\u4e0e\u901a\u4fe1\uff08ISAC\uff09\u5e94\u7528\u3002"}}
{"id": "2510.23656", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23656", "abs": "https://arxiv.org/abs/2510.23656", "authors": ["Fuqiang Liu", "Weiping Ding", "Luis Miranda-Moreno", "Lijun Sun"], "title": "Error Adjustment Based on Spatiotemporal Correlation Fusion for Traffic Forecasting", "comment": "12 pages, 7 figures, 3 tables", "summary": "Deep neural networks (DNNs) play a significant role in an increasing body of\nresearch on traffic forecasting due to their effectively capturing\nspatiotemporal patterns embedded in traffic data. A general assumption of\ntraining the said forecasting models via mean squared error estimation is that\nthe errors across time steps and spatial positions are uncorrelated. However,\nthis assumption does not really hold because of the autocorrelation caused by\nboth the temporality and spatiality of traffic data. This gap limits the\nperformance of DNN-based forecasting models and is overlooked by current\nstudies. To fill up this gap, this paper proposes Spatiotemporally\nAutocorrelated Error Adjustment (SAEA), a novel and general framework designed\nto systematically adjust autocorrelated prediction errors in traffic\nforecasting. Unlike existing approaches that assume prediction errors follow a\nrandom Gaussian noise distribution, SAEA models these errors as a\nspatiotemporal vector autoregressive (VAR) process to capture their intrinsic\ndependencies. First, it explicitly captures both spatial and temporal error\ncorrelations by a coefficient matrix, which is then embedded into a newly\nformulated cost function. Second, a structurally sparse regularization is\nintroduced to incorporate prior spatial information, ensuring that the learned\ncoefficient matrix aligns with the inherent road network structure. Finally, an\ninference process with test-time error adjustment is designed to dynamically\nrefine predictions, mitigating the impact of autocorrelated errors in real-time\nforecasting. The effectiveness of the proposed approach is verified on\ndifferent traffic datasets. Results across a wide range of traffic forecasting\nmodels show that our method enhances performance in almost all cases.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSAEA\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u8c03\u6574\u4ea4\u901a\u9884\u6d4b\u4e2d\u7684\u65f6\u7a7a\u81ea\u76f8\u5173\u8bef\u5dee\uff0c\u901a\u8fc7\u5c06\u8bef\u5dee\u5efa\u6a21\u4e3a\u5411\u91cf\u81ea\u56de\u5f52\u8fc7\u7a0b\u5e76\u7ed3\u5408\u7a00\u758f\u6b63\u5219\u5316\u548c\u6d4b\u8bd5\u65f6\u8bef\u5dee\u8c03\u6574\uff0c\u4ece\u800c\u63d0\u9ad8\u9884\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709DNN\u4ea4\u901a\u9884\u6d4b\u6a21\u578b\u901a\u5e38\u5047\u8bbe\u8bef\u5dee\u5728\u65f6\u7a7a\u4e0a\u4e0d\u76f8\u5173\uff0c\u4f46\u8fd9\u4e0e\u4ea4\u901a\u6570\u636e\u7684\u5b9e\u9645\u65f6\u7a7a\u81ea\u76f8\u5173\u6027\u4e0d\u7b26\uff0c\u9650\u5236\u4e86\u6a21\u578b\u6027\u80fd\u3002", "method": "\u63d0\u51faSAEA\u6846\u67b6\uff0c\u5c06\u9884\u6d4b\u8bef\u5dee\u5efa\u6a21\u4e3a\u65f6\u7a7a\u5411\u91cf\u81ea\u56de\u5f52\uff08VAR\uff09\u8fc7\u7a0b\uff0c\u901a\u8fc7\u7cfb\u6570\u77e9\u9635\u6355\u6349\u65f6\u7a7a\u4f9d\u8d56\u6027\u3002\u5f15\u5165\u7ed3\u6784\u7a00\u758f\u6b63\u5219\u5316\u4ee5\u6574\u5408\u8def\u7f51\u7ed3\u6784\u4fe1\u606f\u3002\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5305\u542b\u6d4b\u8bd5\u65f6\u8bef\u5dee\u8c03\u6574\u7684\u63a8\u7406\u8fc7\u7a0b\u6765\u52a8\u6001\u4f18\u5316\u9884\u6d4b\u3002", "result": "\u5728\u591a\u4e2a\u4ea4\u901a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86SAEA\u7684\u6709\u6548\u6027\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u7edd\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u90fd\u80fd\u63d0\u5347\u73b0\u6709\u4ea4\u901a\u9884\u6d4b\u6a21\u578b\u7684\u6027\u80fd\u3002", "conclusion": "SAEA\u6846\u67b6\u80fd\u591f\u6709\u6548\u5904\u7406\u4ea4\u901a\u9884\u6d4b\u4e2d\u7684\u65f6\u7a7a\u81ea\u76f8\u5173\u8bef\u5dee\uff0c\u663e\u8457\u63d0\u5347\u9884\u6d4b\u6a21\u578b\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2510.24115", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24115", "abs": "https://arxiv.org/abs/2510.24115", "authors": ["Sandeep Vissapragada", "Vikrant Sahu", "Gagan Raj Gupta", "Vandita Singh"], "title": "HistoLens: An Interactive XAI Toolkit for Verifying and Mitigating Flaws in Vision-Language Models for Histopathology", "comment": null, "summary": "For doctors to truly trust artificial intelligence, it can't be a black box.\nThey need to understand its reasoning, almost as if they were consulting a\ncolleague. We created HistoLens1 to be that transparent, collaborative partner.\nIt allows a pathologist to simply ask a question in plain English about a\ntissue slide--just as they would ask a trainee. Our system intelligently\ntranslates this question into a precise query for its AI engine, which then\nprovides a clear, structured report. But it doesn't stop there. If a doctor\never asks, \"Why?\", HistoLens can instantly provide a 'visual proof' for any\nfinding--a heatmap that points to the exact cells and regions the AI used for\nits analysis. We've also ensured the AI focuses only on the patient's tissue,\njust like a trained pathologist would, by teaching it to ignore distracting\nbackground noise. The result is a workflow where the pathologist remains the\nexpert in charge, using a trustworthy AI assistant to verify their insights and\nmake faster, more confident diagnoses.", "AI": {"tldr": "HistoLens\u662f\u4e00\u4e2a\u900f\u660e\u7684AI\u52a9\u624b\uff0c\u53ef\u4ee5\u5e2e\u52a9\u75c5\u7406\u5b66\u5bb6\u66f4\u5feb\u3001\u66f4\u81ea\u4fe1\u5730\u505a\u51fa\u8bca\u65ad\u3002", "motivation": "\u4e3a\u4e86\u8ba9\u533b\u751f\u771f\u6b63\u4fe1\u4efb\u4eba\u5de5\u667a\u80fd\uff0c\u5b83\u4e0d\u80fd\u662f\u4e00\u4e2a\u9ed1\u76d2\u5b50\u3002\u533b\u751f\u9700\u8981\u7406\u89e3\u5b83\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u5c31\u50cf\u54a8\u8be2\u540c\u4e8b\u4e00\u6837\u3002HistoLens\u88ab\u521b\u5efa\u4e3a\u8fd9\u79cd\u900f\u660e\u7684\u3001\u534f\u4f5c\u7684\u4f19\u4f34\u3002", "method": "HistoLens\u5141\u8bb8\u75c5\u7406\u5b66\u5bb6\u7528\u7b80\u5355\u7684\u82f1\u8bed\u8be2\u95ee\u6709\u5173\u7ec4\u7ec7\u5207\u7247\u7684\u95ee\u9898\uff0c\u7136\u540e\u7cfb\u7edf\u4f1a\u5c06\u95ee\u9898\u7ffb\u8bd1\u6210\u7cbe\u786e\u7684\u67e5\u8be2\uff0cAI\u5f15\u64ce\u4f1a\u63d0\u4f9b\u6e05\u6670\u3001\u7ed3\u6784\u5316\u7684\u62a5\u544a\u3002\u5982\u679c\u533b\u751f\u95ee\u201c\u4e3a\u4ec0\u4e48\uff1f\u201d\uff0cHistoLens\u53ef\u4ee5\u4e3a\u4efb\u4f55\u53d1\u73b0\u63d0\u4f9b\u201c\u89c6\u89c9\u8bc1\u660e\u201d\u2014\u2014\u4e00\u4e2a\u6307\u5411AI\u7528\u4e8e\u5206\u6790\u7684\u786e\u5207\u7ec6\u80de\u548c\u533a\u57df\u7684\u70ed\u56fe\u3002\u901a\u8fc7\u6559\u4f1aAI\u5ffd\u7565\u5206\u6563\u6ce8\u610f\u529b\u7684\u80cc\u666f\u566a\u97f3\uff0c\u786e\u4fddAI\u53ea\u5173\u6ce8\u60a3\u8005\u7684\u7ec4\u7ec7\u3002", "result": "HistoLens\u63d0\u4f9b\u4e86\u4e00\u4e2a\u75c5\u7406\u5b66\u5bb6\u4ecd\u7136\u662f\u8d1f\u8d23\u4e13\u5bb6\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u4f7f\u7528\u503c\u5f97\u4fe1\u8d56\u7684AI\u52a9\u624b\u6765\u9a8c\u8bc1\u4ed6\u4eec\u7684\u89c1\u89e3\uff0c\u5e76\u505a\u51fa\u66f4\u5feb\u3001\u66f4\u81ea\u4fe1\u7684\u8bca\u65ad\u3002", "conclusion": "HistoLens\u4f7f\u75c5\u7406\u5b66\u5bb6\u80fd\u591f\u901a\u8fc7\u63d0\u4f9b\u900f\u660e\u7684AI\u63a8\u7406\u548c\u89c6\u89c9\u8bc1\u636e\u6765\u66f4\u5feb\u3001\u66f4\u81ea\u4fe1\u5730\u505a\u51fa\u8bca\u65ad\u3002"}}
{"id": "2510.24023", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24023", "abs": "https://arxiv.org/abs/2510.24023", "authors": ["Saujas Vaduguru", "Yilun Hua", "Yoav Artzi", "Daniel Fried"], "title": "Success and Cost Elicit Convention Formation for Efficient Communication", "comment": null, "summary": "Humans leverage shared conversational context to become increasingly\nsuccessful and efficient at communicating over time. One manifestation of this\nis the formation of ad hoc linguistic conventions, which allow people to\ncoordinate on short, less costly utterances that are understood using shared\nconversational context. We present a method to train large multimodal models to\nform conventions, enabling efficient communication. Our approach uses simulated\nreference games between models, and requires no additional human-produced data.\nIn repeated reference games involving photographs and tangram images, our\nmethod enables models to communicate efficiently with people: reducing the\nmessage length by up to 41% while increasing success by 15% over the course of\nthe interaction. Human listeners respond faster when interacting with our model\nthat forms conventions. We also show that training based on success or cost\nalone is insufficient - both are necessary to elicit convention formation.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u8bad\u7ec3\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u4ee5\u5f62\u6210\u8bed\u8a00\u7ea6\u5b9a\uff08\u5373\u201c\u7ea6\u5b9a\u201d\uff09\u7684\u65b9\u6cd5\uff0c\u8fd9\u79cd\u7ea6\u5b9a\u80fd\u591f\u63d0\u9ad8\u901a\u4fe1\u6548\u7387\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u6a21\u62df\u53c2\u8003\u6e38\u620f\u8fdb\u884c\uff0c\u65e0\u9700\u989d\u5916\u7684\u4eba\u5de5\u6570\u636e\u3002", "motivation": "\u4eba\u7c7b\u901a\u8fc7\u5171\u4eab\u5bf9\u8bdd\u80cc\u666f\u63d0\u9ad8\u6c9f\u901a\u6548\u7387\uff0c\u8868\u73b0\u4e3a\u7ea6\u5b9a\u5f62\u6210\uff0c\u5373\u4f7f\u7528\u7b80\u77ed\u3001\u4f4e\u6210\u672c\u7684\u8868\u8fbe\u65b9\u5f0f\uff0c\u5e76\u4f9d\u8d56\u5171\u4eab\u7684\u5bf9\u8bdd\u80cc\u666f\u8fdb\u884c\u7406\u89e3\u3002", "method": "\u4f7f\u7528\u6a21\u62df\u7684\u53c2\u8003\u6e38\u620f\uff08\u5305\u542b\u7167\u7247\u548c tangram \u56fe\u50cf\uff09\u6765\u8bad\u7ec3\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u5f62\u6210\u7ea6\u5b9a\u3002", "result": "\u5728\u6a21\u62df\u53c2\u8003\u6e38\u620f\u4e2d\uff0c\u8be5\u65b9\u6cd5\u4f7f\u6a21\u578b\u80fd\u591f\u4e0e\u4eba\u7c7b\u9ad8\u6548\u6c9f\u901a\uff0c\u6d88\u606f\u957f\u5ea6\u51cf\u5c11\u9ad8\u8fbe 41%\uff0c\u6210\u529f\u7387\u63d0\u9ad8 15%\uff1b\u4eba\u7c7b\u542c\u4f17\u4e0e\u5f62\u6210\u7ea6\u5b9a\u7684\u6a21\u578b\u4e92\u52a8\u65f6\u53cd\u5e94\u66f4\u5feb\u3002", "conclusion": "\u8bad\u7ec3\u6a21\u578b\u5f62\u6210\u7ea6\u5b9a\u9700\u8981\u540c\u65f6\u8003\u8651\u6210\u529f\u7387\u548c\u6210\u672c\uff0c\u4ec5\u5355\u72ec\u8003\u8651\u5176\u4e2d\u4e00\u4e2a\u56e0\u7d20\u662f\u4e0d\u591f\u7684\u3002"}}
{"id": "2510.24038", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24038", "abs": "https://arxiv.org/abs/2510.24038", "authors": ["Xingyu Zhu", "Beier Zhu", "Shuo Wang", "Kesen Zhao", "Hanwang Zhang"], "title": "Enhancing CLIP Robustness via Cross-Modality Alignment", "comment": "NeurIPS 2025 Spotlight", "summary": "Vision-language models (VLMs) such as CLIP demonstrate strong generalization\nin zero-shot classification but remain highly vulnerable to adversarial\nperturbations. Existing methods primarily focus on adversarial fine-tuning or\nprompt optimization; they often overlook the gaps in CLIP's encoded features,\nwhich is shown as the text and image features lie far apart from each other.\nThis misalignment is significantly amplified under adversarial perturbations,\nleading to severe degradation in classification performance. To address this\nproblem, we propose Cross-modality Alignment, dubbed COLA, an optimal\ntransport-based framework that explicitly addresses adversarial misalignment by\nrestoring both global image-text alignment and local structural consistency in\nthe feature space. (1) COLA first projects adversarial image embeddings onto a\nsubspace spanned by class text features, effectively filtering out non-semantic\ndistortions while preserving discriminative information. (2) It then models\nimages and texts as discrete distributions over multiple augmented views and\nrefines their alignment via OT, with the subspace projection seamlessly\nintegrated into the cost computation. This design ensures stable cross-modal\nalignment even under adversarial conditions. COLA is training-free and\ncompatible with existing fine-tuned models. Extensive evaluations across 14\nzero-shot classification benchmarks demonstrate the effectiveness of COLA,\nespecially with an average improvement of 6.7% on ImageNet and its variants\nunder PGD adversarial attacks, while maintaining high accuracy on clean\nsamples.", "AI": {"tldr": "COLA\u901a\u8fc7\u6700\u4f18\u4f20\u8f93\u6846\u67b6\u89e3\u51b3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5bf9\u6297\u6027\u653b\u51fb\u4e0b\u7684\u7279\u5f81\u5bf9\u9f50\u95ee\u9898\uff0c\u63d0\u5347\u96f6\u6837\u672c\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u96f6\u6837\u672c\u5206\u7c7b\u65b9\u9762\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u5bb9\u6613\u53d7\u5230\u5bf9\u6297\u6027\u6270\u52a8\u7684\u653b\u51fb\u3002\u73b0\u6709\u7684\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5bf9\u6297\u6027\u5fae\u8c03\u6216\u63d0\u793a\u4f18\u5316\uff0c\u5ffd\u89c6\u4e86CLIP\u7f16\u7801\u7279\u5f81\u4e2d\u7684\u6587\u672c\u548c\u56fe\u50cf\u7279\u5f81\u8ddd\u79bb\u8f83\u8fdc\u7684\u95ee\u9898\uff0c\u8fd9\u79cd\u4e0d\u5339\u914d\u5728\u5bf9\u6297\u6027\u6270\u52a8\u4e0b\u4f1a\u88ab\u653e\u5927\uff0c\u5bfc\u81f4\u5206\u7c7b\u6027\u80fd\u4e25\u91cd\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCOLA\u7684\u6700\u4f18\u4f20\u8f93\uff08OT\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u6062\u590d\u7279\u5f81\u7a7a\u95f4\u4e2d\u7684\u5168\u5c40\u56fe\u50cf-\u6587\u672c\u5bf9\u9f50\u548c\u5c40\u90e8\u7ed3\u6784\u4e00\u81f4\u6027\u6765\u89e3\u51b3\u5bf9\u6297\u6027\u4e0d\u5339\u914d\u95ee\u9898\u3002\u5177\u4f53\u6765\u8bf4\uff0cCOLA\u9996\u5148\u5c06\u5bf9\u6297\u6027\u56fe\u50cf\u5d4c\u5165\u6295\u5f71\u5230\u7531\u7c7b\u522b\u6587\u672c\u7279\u5f81\u8de8\u8d8a\u7684\u5b50\u7a7a\u95f4\uff0c\u4ee5\u8fc7\u6ee4\u975e\u8bed\u4e49\u5931\u771f\u5e76\u4fdd\u7559\u5224\u522b\u4fe1\u606f\u3002\u7136\u540e\uff0c\u901a\u8fc7OT\u5bf9\u9f50\u56fe\u50cf\u548c\u6587\u672c\uff0c\u5e76\u5c06\u5b50\u7a7a\u95f4\u6295\u5f71\u6574\u5408\u5230\u6210\u672c\u8ba1\u7b97\u4e2d\uff0c\u4ee5\u5b9e\u73b0\u7a33\u5b9a\u7684\u8de8\u6a21\u6001\u5bf9\u9f50\u3002", "result": "\u572814\u4e2a\u96f6\u6837\u672c\u5206\u7c7b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCOLA\u5728ImageNet\u53ca\u5176\u53d8\u4f53\u4e0a\u7684PGD\u5bf9\u6297\u653b\u51fb\u4e0b\u5e73\u5747\u63d0\u9ad8\u4e866.7%\u7684\u51c6\u786e\u7387\uff0c\u540c\u65f6\u5728\u5e72\u51c0\u6837\u672c\u4e0a\u4fdd\u6301\u4e86\u9ad8\u7cbe\u5ea6\u3002COLA\u65e0\u9700\u8bad\u7ec3\uff0c\u5e76\u4e14\u53ef\u4ee5\u4e0e\u73b0\u6709\u7684\u5fae\u8c03\u6a21\u578b\u517c\u5bb9\u3002", "conclusion": "COLA\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u6700\u4f18\u4f20\u8f93\u89e3\u51b3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5bf9\u6297\u6027\u653b\u51fb\u4e0b\u7684\u7279\u5f81\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2510.24597", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.24597", "abs": "https://arxiv.org/abs/2510.24597", "authors": ["Longpan Wang", "Zhuoran Zhang", "Zhenyuan Li", "Xuetao Gan", "Xudong Bai", "Wen Chen", "Qingqing Wu"], "title": "Multifunctional Wideband Digital Metasurface for Secure Electromagnetic Manipulation in S-Band", "comment": null, "summary": "Digital metasurfaces have attracted significant attention in recent years due\nto their ability to manipulate electromagnetic (EM) waves for secure sensing\nand communication. However, most reported metasurfaces operate at relatively\nhigh frequencies, primarily due to the constraints imposed by the physical\nscale of the dielectric substrate, thus limiting their full-wave system\napplications. In this work, a wideband digital reflective metasurface is\npresented for capable of dynamically controlling EM waves, with multifunctional\napplications in the lower-frequency S-band. The metasurface is composed of\nelectronically reconfigurable meta-atoms with wideband characteristics, and\ndesigned by using trapezoidal and M-shaped patches connected by a pin diode.\nSimulation results show that the proposed digital metasurface could achieve\nwideband 1-bit phase quantization with a stable phase difference within 180\ndegree +/- 25 degree and small reflection loss below 0.6 dB from 2.72 to 3.25\nGHz. To validate the proposed design, a 20x20-unit metasurface array was\ndesigned, simulated and fabricated. By dynamically adjusting the coding\nsequence, the metasurface could enable multi-mode orbital angular momentum\n(OAM) beam generation, dynamic beam scanning, and precise direction finding.\nThese capabilities support secure sensing and secure communications through\nhigh-resolution target detection and anti-jamming beam steering, as well as\nphysical-layer security. The proposed wideband metasurface may serve as an\neffective candidate for enhancing spectral efficiency and security performance\nin radar and wireless systems.", "AI": {"tldr": "A wideband digital reflective metasurface for the S-band is presented, utilizing reconfigurable meta-atoms to dynamically control EM waves for secure sensing and communication, enabling multi-mode OAM beam generation, dynamic beam scanning, and precise direction finding.", "motivation": "Most reported metasurfaces operate at high frequencies due to physical scale limitations, restricting their use in full-wave system applications. This work addresses this by presenting a metasurface for lower-frequency S-band applications.", "method": "The metasurface is composed of electronically reconfigurable meta-atoms with wideband characteristics, designed using trapezoidal and M-shaped patches connected by a pin diode. A 20x20-unit array was designed, simulated, and fabricated.", "result": "The proposed digital metasurface achieves wideband 1-bit phase quantization with a stable phase difference (within 180\u00b0 \u00b1 25\u00b0) and low reflection loss (below 0.6 dB) from 2.72 to 3.25 GHz. The fabricated array demonstrates multi-mode OAM beam generation, dynamic beam scanning, and precise direction finding.", "conclusion": "The proposed wideband metasurface is a viable candidate for enhancing spectral efficiency and security in radar and wireless systems, supporting secure sensing and communication through high-resolution target detection, anti-jamming beam steering, and physical-layer security."}}
{"id": "2510.23657", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23657", "abs": "https://arxiv.org/abs/2510.23657", "authors": ["Saklain Niam", "Tashfiqur Rahman", "Md. Amjad Patwary", "Mukarram Hossain"], "title": "A machine learning framework integrating seed traits and plasma parameters for predicting germination uplift in crops", "comment": null, "summary": "Cold plasma (CP) is an eco-friendly method to enhance seed germination, yet\noutcomes remain difficult to predict due to complex seed--plasma--environment\ninteractions. This study introduces the first machine learning framework to\nforecast germination uplift in soybean, barley, sunflower, radish, and tomato\nunder dielectric barrier discharge (DBD) plasma. Among the models tested (GB,\nXGB, ET, and hybrids), Extra Trees (ET) performed best (R\\textsuperscript{2} =\n0.919; RMSE = 3.21; MAE = 2.62), improving to R\\textsuperscript{2} = 0.925\nafter feature reduction. Engineering analysis revealed a hormetic response:\nnegligible effects at $<$7 kV or $<$200 s, maximum germination at 7--15 kV for\n200--500 s, and reduced germination beyond 20 kV or prolonged exposures.\nDischarge power was also a dominant factor, with germination rate maximizing at\n$\\geq$100 W with low exposure time. Species and cultivar-level predictions\nshowed radish (MAE = 1.46) and soybean (MAE = 2.05) were modeled with high\nconsistency, while sunflower remained slightly higher variable (MAE = 3.80).\nAmong cultivars, Williams (MAE = 1.23) and Sari (1.33) were well predicted,\nwhile Arian (2.86) and Ny\\'{\\i}rs\\'{e}gi fekete (3.74) were comparatively\npoorly captured. This framework was also embedded into MLflow, providing a\ndecision-support tool for optimizing CP seed germination in precision\nagriculture.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5229\u7528\u673a\u5668\u5b66\u4e60\u9884\u6d4b\u51b7\u7b49\u79bb\u5b50\u4f53\u5904\u7406\u5bf9\u5927\u8c46\u3001\u5927\u9ea6\u3001\u5411\u65e5\u8475\u3001\u841d\u535c\u548c\u756a\u8304\u79cd\u5b50\u840c\u53d1\u5f71\u54cd\u7684\u6846\u67b6\uff0c\u5176\u4e2dExtra Trees\u6a21\u578b\u6548\u679c\u6700\u4f73\uff0c\u5e76\u63ed\u793a\u4e86\u51b7\u7b49\u79bb\u5b50\u4f53\u5904\u7406\u4e0e\u79cd\u5b50\u840c\u53d1\u4e4b\u95f4\u7684\u5242\u91cf-\u6548\u5e94\u5173\u7cfb\uff0c\u4e3a\u7cbe\u51c6\u519c\u4e1a\u63d0\u4f9b\u4e86\u51b3\u7b56\u652f\u6301\u5de5\u5177\u3002", "motivation": "\u51b7\u7b49\u79bb\u5b50\u4f53\uff08CP\uff09\u4f5c\u4e3a\u4e00\u79cd\u73af\u4fdd\u65b9\u6cd5\u53ef\u4ee5\u4fc3\u8fdb\u79cd\u5b50\u840c\u53d1\uff0c\u4f46\u7531\u4e8e\u79cd\u5b50-\u7b49\u79bb\u5b50\u4f53-\u73af\u5883\u4e4b\u95f4\u590d\u6742\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u5176\u6548\u679c\u96be\u4ee5\u9884\u6d4b\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u673a\u5668\u5b66\u4e60\u6846\u67b6\u6765\u9884\u6d4bCP\u5bf9\u591a\u79cd\u79cd\u5b50\u840c\u53d1\u7684\u5f71\u54cd\u3002", "method": "\u7814\u7a76\u4eba\u5458\u6d4b\u8bd5\u4e86\u51e0\u79cd\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff08GB\u3001XGB\u3001ET\u548c\u6df7\u5408\u6a21\u578b\uff09\uff0c\u5e76\u9009\u62e9\u4e86Extra Trees\uff08ET\uff09\u6a21\u578b\uff0c\u5bf9\u5176\u8fdb\u884c\u4e86\u7279\u5f81\u964d\u7ef4\u3002\u6b64\u5916\uff0c\u8fd8\u8fdb\u884c\u4e86\u5de5\u7a0b\u5206\u6790\u4ee5\u786e\u5b9a\u6700\u4f73\u5904\u7406\u53c2\u6570\uff0c\u5e76\u8bc4\u4f30\u4e86\u6a21\u578b\u5728\u4e0d\u540c\u7269\u79cd\u548c\u54c1\u79cd\u4e0a\u7684\u9884\u6d4b\u80fd\u529b\u3002\u6700\u540e\uff0c\u5c06\u8be5\u6846\u67b6\u96c6\u6210\u5230MLflow\u4e2d\u3002", "result": "Extra Trees\uff08ET\uff09\u6a21\u578b\u5728\u9884\u6d4b\u79cd\u5b50\u840c\u53d1\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff08R\u00b2 = 0.919\uff1bRMSE = 3.21\uff1bMAE = 2.62\uff09\uff0c\u7279\u5f81\u964d\u7ef4\u540e\u6027\u80fd\u8fdb\u4e00\u6b65\u63d0\u5347\uff08R\u00b2 = 0.925\uff09\u3002\u7814\u7a76\u53d1\u73b0\u4e86\u51b7\u7b49\u79bb\u5b50\u4f53\u5904\u7406\u7684 hormetic \u6548\u5e94\uff0c\u6700\u4f73\u5904\u7406\u8303\u56f4\u4e3a7-15 kV\u548c200-500\u79d2\u3002\u653e\u7535\u529f\u7387\u4e5f\u662f\u4e00\u4e2a\u5173\u952e\u56e0\u7d20\uff0c\u9ad8\u529f\u7387\uff08\u2265100 W\uff09\u548c\u77ed\u65f6\u95f4\u5904\u7406\u53ef\u6700\u5927\u5316\u840c\u53d1\u7387\u3002\u841d\u535c\u548c\u9ec4\u8c46\u7684\u9884\u6d4b\u7cbe\u5ea6\u8f83\u9ad8\uff08MAE\u5206\u522b\u4e3a1.46\u548c2.05\uff09\uff0c\u800c\u5411\u65e5\u8475\u7684\u9884\u6d4b\u53d8\u5f02\u6027\u7a0d\u5927\uff08MAE = 3.80\uff09\u3002\u5728\u54c1\u79cd\u65b9\u9762\uff0cWilliams\u548cSari\u7684\u9884\u6d4b\u6548\u679c\u8f83\u597d\uff08MAE\u5206\u522b\u4e3a1.23\u548c1.33\uff09\uff0c\u800cArian\u548cNy\u00edrs\u00e9gi fekete\u7684\u9884\u6d4b\u6548\u679c\u76f8\u5bf9\u8f83\u5dee\uff08MAE\u5206\u522b\u4e3a2.86\u548c3.74\uff09\u3002", "conclusion": "\u672c\u7814\u7a76\u6210\u529f\u5f00\u53d1\u4e86\u4e00\u4e2a\u5229\u7528\u673a\u5668\u5b66\u4e60\u9884\u6d4b\u51b7\u7b49\u79bb\u5b50\u4f53\u5bf9\u79cd\u5b50\u840c\u53d1\u5f71\u54cd\u7684\u6846\u67b6\uff0c\u5e76\u8bc6\u522b\u4e86\u5173\u952e\u7684\u4f18\u5316\u53c2\u6570\u548c\u6f5c\u5728\u7684 hormetic \u6548\u5e94\u3002\u8be5\u6846\u67b6\u80fd\u591f\u4e3a\u4e0d\u540c\u7269\u79cd\u548c\u54c1\u79cd\u63d0\u4f9b\u9884\u6d4b\uff0c\u5e76\u5df2\u96c6\u6210\u5230MLflow\u4e2d\uff0c\u4e3a\u7cbe\u51c6\u519c\u4e1a\u4e2d\u7684\u51b7\u7b49\u79bb\u5b50\u4f53\u79cd\u5b50\u5904\u7406\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u51b3\u7b56\u652f\u6301\u3002"}}
{"id": "2510.24145", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24145", "abs": "https://arxiv.org/abs/2510.24145", "authors": ["Yu Luo", "Jiamin Jiang", "Jingfei Feng", "Lei Tao", "Qingliang Zhang", "Xidao Wen", "Yongqian Sun", "Shenglin Zhang", "Jielong Huang", "Nan Qi", "Dan Pei"], "title": "From Observability Data to Diagnosis: An Evolving Multi-agent System for Incident Management in Cloud Systems", "comment": null, "summary": "Incident management (IM) is central to the reliability of large-scale cloud\nsystems. Yet manual IM, where on-call engineers examine metrics, logs, and\ntraces is labor-intensive and error-prone in the face of massive and\nheterogeneous observability data. Existing automated IM approaches often\nstruggle to generalize across systems, provide limited interpretability, and\nincur high deployment costs, which hinders adoption in practice. In this paper,\nwe present OpsAgent, a lightweight, self-evolving multi-agent system for IM\nthat employs a training-free data processor to convert heterogeneous\nobservability data into structured textual descriptions, along with a\nmulti-agent collaboration framework that makes diagnostic inference transparent\nand auditable. To support continual capability growth, OpsAgent also introduces\na dual self-evolution mechanism that integrates internal model updates with\nexternal experience accumulation, thereby closing the deployment loop.\nComprehensive experiments on the OPENRCA benchmark demonstrate state-of-the-art\nperformance and show that OpsAgent is generalizable, interpretable,\ncost-efficient, and self-evolving, making it a practically deployable and\nsustainable solution for long-term operation in real-world cloud systems.", "AI": {"tldr": "OpsAgent\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u81ea\u8fdb\u5316\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u7528\u4e8e\u4e8b\u4ef6\u7ba1\u7406\uff08IM\uff09\uff0c\u5b83\u901a\u8fc7\u8bad\u7ec3\u65e0\u5173\u7684\u6570\u636e\u5904\u7406\u5668\u5c06\u5f02\u6784\u7684\u53ef\u89c2\u6d4b\u6027\u6570\u636e\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u6587\u672c\u63cf\u8ff0\uff0c\u5e76\u91c7\u7528\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\u8fdb\u884c\u900f\u660e\u548c\u53ef\u5ba1\u8ba1\u7684\u8bca\u65ad\u63a8\u7406\u3002\u5b83\u8fd8\u5f15\u5165\u4e86\u53cc\u91cd\u81ea\u8fdb\u5316\u673a\u5236\uff0c\u4ee5\u652f\u6301\u6301\u7eed\u7684\u80fd\u529b\u589e\u957f\u3002", "motivation": "\u624b\u52a8\u4e8b\u4ef6\u7ba1\u7406\uff08IM\uff09\u5728\u5904\u7406\u5927\u89c4\u6a21\u4e91\u7cfb\u7edf\u7684\u6d77\u91cf\u3001\u5f02\u6784\u53ef\u89c2\u6d4b\u6570\u636e\u65f6\uff0c\u52b3\u52a8\u5bc6\u96c6\u4e14\u6613\u51fa\u9519\u3002\u73b0\u6709\u7684\u81ea\u52a8\u5316IM\u65b9\u6cd5\u6cdb\u5316\u6027\u6709\u9650\uff0c\u53ef\u89e3\u91ca\u6027\u5dee\uff0c\u4e14\u90e8\u7f72\u6210\u672c\u9ad8\uff0c\u963b\u788d\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "OpsAgent\u4f7f\u7528\u8bad\u7ec3\u65e0\u5173\u7684\u6570\u636e\u5904\u7406\u5668\u5c06\u5f02\u6784\u53ef\u89c2\u6d4b\u6027\u6570\u636e\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u6587\u672c\u63cf\u8ff0\uff0c\u5e76\u7ed3\u5408\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\u8fdb\u884c\u8bca\u65ad\u63a8\u7406\u3002\u901a\u8fc7\u5185\u90e8\u6a21\u578b\u66f4\u65b0\u548c\u5916\u90e8\u7ecf\u9a8c\u79ef\u7d2f\u7684\u53cc\u91cd\u81ea\u8fdb\u5316\u673a\u5236\uff0c\u5b9e\u73b0\u80fd\u529b\u7684\u6301\u7eed\u589e\u957f\u3002", "result": "\u5728OPENRCA\u57fa\u51c6\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0cOpsAgent\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u6027\u3001\u53ef\u89e3\u91ca\u6027\u3001\u6210\u672c\u6548\u76ca\u548c\u81ea\u8fdb\u5316\u80fd\u529b\u3002", "conclusion": "OpsAgent\u662f\u4e00\u4e2a\u5b9e\u7528\u4e14\u53ef\u6301\u7eed\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u73b0\u5b9e\u4e16\u754c\u4e91\u7cfb\u7edf\u7684\u957f\u671f\u8fd0\u7ef4\u3002"}}
{"id": "2510.24051", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24051", "abs": "https://arxiv.org/abs/2510.24051", "authors": ["In Gim", "Zhiyao Ma", "Seung-seob Lee", "Lin Zhong"], "title": "Pie: A Programmable Serving System for Emerging LLM Applications", "comment": "SOSP 2025. Source code available at\n  https://github.com/pie-project/pie", "summary": "Emerging large language model (LLM) applications involve diverse reasoning\nstrategies and agentic workflows, straining the capabilities of existing\nserving systems built on a monolithic token generation loop. This paper\nintroduces Pie, a programmable LLM serving system designed for flexibility and\nefficiency. Pie decomposes the traditional generation loop into fine-grained\nservice handlers exposed via an API and delegates control of the generation\nprocess to user-provided programs, called inferlets. This enables applications\nto implement new KV cache strategies, bespoke generation logic, and seamlessly\nintegrate computation and I/O-entirely within the application, without\nrequiring modifications to the serving system. Pie executes inferlets using\nWebAssembly, benefiting from its lightweight sandboxing. Our evaluation shows\nPie matches state-of-the-art performance on standard tasks (3-12% latency\noverhead) while significantly improving latency and throughput (1.3x-3.4x\nhigher) on agentic workflows by enabling application-specific optimizations.", "AI": {"tldr": "LLM\u670d\u52a1\u7cfb\u7edfPie\u901a\u8fc7\u5c06\u751f\u6210\u8fc7\u7a0b\u5206\u89e3\u4e3a\u53ef\u7f16\u7a0b\u7684API\u548c\u670d\u52a1\u5904\u7406\u7a0b\u5e8f\uff0c\u5e76\u4f7f\u7528WebAssembly\u6267\u884c\u7528\u6237\u63d0\u4f9b\u7684inferlets\uff0c\u4ee5\u63d0\u9ad8\u7075\u6d3b\u6027\u548c\u6548\u7387\uff0c\u5c24\u5176\u5728\u4ee3\u7406\u5de5\u4f5c\u6d41\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u7684LLM\u670d\u52a1\u7cfb\u7edf\u5728\u5904\u7406\u591a\u6837\u5316\u7684\u63a8\u7406\u7b56\u7565\u548c\u4ee3\u7406\u5de5\u4f5c\u6d41\u65b9\u9762\u80fd\u529b\u53d7\u9650\uff0c\u56e0\u4e3a\u5b83\u4eec\u4f9d\u8d56\u4e8e\u5355\u4e00\u7684token\u751f\u6210\u5faa\u73af\u3002", "method": "Pie\u5c06\u4f20\u7edf\u7684\u751f\u6210\u5faa\u73af\u5206\u89e3\u4e3a\u7ec6\u7c92\u5ea6\u7684\u670d\u52a1\u5904\u7406\u7a0b\u5e8f\uff0c\u901a\u8fc7API\u66b4\u9732\uff0c\u5e76\u5c06\u751f\u6210\u8fc7\u7a0b\u7684\u63a7\u5236\u6743\u4ea4\u7ed9\u7528\u6237\u63d0\u4f9b\u7684\u540d\u4e3ainferlets\u7684\u7a0b\u5e8f\u3002Pie\u4f7f\u7528WebAssembly\u6267\u884cinferlets\uff0c\u5b9e\u73b0\u4e86\u8f7b\u91cf\u7ea7\u6c99\u7bb1\u3002", "result": "\u5728\u6807\u51c6\u4efb\u52a1\u4e0a\uff0cPie\u7684\u6027\u80fd\u4e0e\u73b0\u6709\u6280\u672f\u76f8\u5f53\uff08\u5ef6\u8fdf\u5f00\u95003-12%\uff09\u3002\u5728\u4ee3\u7406\u5de5\u4f5c\u6d41\u65b9\u9762\uff0c\u901a\u8fc7\u5b9e\u73b0\u7279\u5b9a\u4e8e\u5e94\u7528\u7a0b\u5e8f\u7684\u4f18\u5316\uff0cPie\u5c06\u5ef6\u8fdf\u548c\u541e\u5410\u91cf\u663e\u8457\u63d0\u9ad8\u4e861.3-3.4\u500d\u3002", "conclusion": "Pie\u662f\u4e00\u4e2a\u7075\u6d3b\u4e14\u9ad8\u6548\u7684\u53ef\u7f16\u7a0bLLM\u670d\u52a1\u7cfb\u7edf\uff0c\u901a\u8fc7\u5c06LLM\u670d\u52a1\u5206\u89e3\u4e3a\u53ef\u7f16\u7a0b\u7684API\u548c\u670d\u52a1\u5904\u7406\u7a0b\u5e8f\uff0c\u5e76\u4f7f\u7528WebAssembly\u6267\u884c\u7528\u6237\u63d0\u4f9b\u7684inferlets\uff0c\u80fd\u591f\u5728\u4ee3\u7406\u5de5\u4f5c\u6d41\u7b49\u573a\u666f\u4e0b\u5b9e\u73b0\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2510.24508", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24508", "abs": "https://arxiv.org/abs/2510.24508", "authors": ["Haoying Li", "Yifan Peng", "Junfeng Wu"], "title": "Supervisory Measurement-Guided Noise Covariance Estimation", "comment": null, "summary": "Reliable state estimation hinges on accurate specification of sensor noise\ncovariances, which weigh heterogeneous measurements. In practice, these\ncovariances are difficult to identify due to environmental variability,\nfront-end preprocessing, and other reasons. We address this by formulating\nnoise covariance estimation as a bilevel optimization that, from a Bayesian\nperspective, factorizes the joint likelihood of so-called odometry and\nsupervisory measurements, thereby balancing information utilization with\ncomputational efficiency. The factorization converts the nested Bayesian\ndependency into a chain structure, enabling efficient parallel computation: at\nthe lower level, an invariant extended Kalman filter with state augmentation\nestimates trajectories, while a derivative filter computes analytical gradients\nin parallel for upper-level gradient updates. The upper level refines the\ncovariance to guide the lower-level estimation. Experiments on synthetic and\nreal-world datasets show that our method achieves higher efficiency over\nexisting baselines.", "AI": {"tldr": "\u901a\u8fc7\u5c06\u566a\u58f0\u534f\u65b9\u5dee\u4f30\u8ba1\u8868\u8ff0\u4e3a\u53cc\u5c42\u4f18\u5316\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u63d0\u9ad8\u72b6\u6001\u4f30\u8ba1\u7cbe\u5ea6\u548c\u6548\u7387\u7684\u65b9\u6cd5\u3002", "motivation": "\u4f20\u611f\u5668\u566a\u58f0\u534f\u65b9\u5dee\u7684\u51c6\u786e\u89c4\u8303\u5bf9\u4e8e\u6743\u8861\u5f02\u6784\u6d4b\u91cf\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u73af\u5883\u53d8\u5316\u3001\u524d\u7aef\u9884\u5904\u7406\u7b49\u539f\u56e0\uff0c\u5b9e\u9645\u4e2d\u96be\u4ee5\u786e\u5b9a\u3002", "method": "\u5c06\u566a\u58f0\u534f\u65b9\u5dee\u4f30\u8ba1\u6784\u5efa\u4e3a\u53cc\u5c42\u4f18\u5316\u95ee\u9898\uff0c\u4ece\u8d1d\u53f6\u65af\u89d2\u5ea6\u5206\u89e3\u4e86\u91cc\u7a0b\u8ba1\u548c\u76d1\u7763\u6d4b\u91cf\u503c\u7684\u8054\u5408\u4f3c\u7136\uff0c\u4ece\u800c\u5e73\u8861\u4e86\u4fe1\u606f\u5229\u7528\u548c\u8ba1\u7b97\u6548\u7387\u3002\u8fd9\u79cd\u5206\u89e3\u5c06\u5d4c\u5957\u7684\u8d1d\u53f6\u65af\u4f9d\u8d56\u5173\u7cfb\u8f6c\u6362\u4e3a\u94fe\u5f0f\u7ed3\u6784\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5e76\u884c\u8ba1\u7b97\uff1a\u5e95\u5c42\u4f7f\u7528\u72b6\u6001\u589e\u5f3a\u7684\u4e0d\u53d8\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u6765\u4f30\u8ba1\u8f68\u8ff9\uff0c\u800c\u5bfc\u6570\u6ee4\u6ce2\u5668\u5219\u5e76\u884c\u8ba1\u7b97\u4e0a\u5c42\u68af\u5ea6\u66f4\u65b0\u7684\u89e3\u6790\u68af\u5ea6\u3002\u4e0a\u5c42\u5219\u7cbe\u70bc\u534f\u65b9\u5dee\u4ee5\u6307\u5bfc\u5e95\u5c42\u4f30\u8ba1\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6bd4\u73b0\u6709\u57fa\u7ebf\u5177\u6709\u66f4\u9ad8\u7684\u6548\u7387\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8e\u53cc\u5c42\u4f18\u5316\u548c\u5e76\u884c\u8ba1\u7b97\u7684\u65b9\u6cd5\u80fd\u591f\u66f4\u6709\u6548\u5730\u4f30\u8ba1\u4f20\u611f\u5668\u566a\u58f0\u534f\u65b9\u5dee\uff0c\u4ece\u800c\u63d0\u9ad8\u72b6\u6001\u4f30\u8ba1\u7684\u6027\u80fd\u3002"}}
{"id": "2510.24078", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24078", "abs": "https://arxiv.org/abs/2510.24078", "authors": ["William Yang", "Xindi Wu", "Zhiwei Deng", "Esin Tureci", "Olga Russakovsky"], "title": "Beyond Objects: Contextual Synthetic Data Generation for Fine-Grained Classification", "comment": null, "summary": "Text-to-image (T2I) models are increasingly used for synthetic dataset\ngeneration, but generating effective synthetic training data for classification\nremains challenging. Fine-tuning a T2I model with a few real examples can help\nimprove the quality of synthetic training data; however, it may also cause\noverfitting and reduce diversity in the generated samples. We propose a\nfine-tuning strategy BOB (BeyondOBjects) to mitigate these concerns for\nfine-grained classification. Given a small set of real examples, we first\nextract class-agnostic attributes such as scene background and object pose. We\nthen explicitly condition on these attributes during fine-tuning of the T2I\nmodel and marginalize them out during generation. This design mitigates\noverfitting, preserves the T2I model's generative prior, reduces estimation\nerrors, and further minimizes unintended inter-class associations. Extensive\nexperiments across multiple T2I models, backbones, and datasets show that our\nmethod achieves state-of-the-art performance in low-shot fine-grained\nclassification when augmented with synthetic data. Concretely, BOB outperforms\nDataDream by 7.4% on the Aircraft dataset (from 50.0% to 57.4% when fine-tuning\na CLIP classifier with five real images augmented with 100 synthetic images).\nIn three of the four benchmarks, fine-tuning downstream models with 5 real\nimages augmented with BOB achieves better performance than fine-tuning with 10\nreal images. Collectively, BOB outperforms prior art in 18 of 24 experimental\nsettings, with 2+% accuracy improvements in 14 of these settings.", "AI": {"tldr": "\u901a\u8fc7\u63d0\u53d6\u5e76\u663e\u5f0f\u5229\u7528\u7c7b\u522b\u65e0\u5173\u7684\u5c5e\u6027\uff08\u5982\u573a\u666f\u80cc\u666f\u548c\u7269\u4f53\u59ff\u6001\uff09\u6765\u5fae\u8c03\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\uff0c\u4ee5\u63d0\u9ad8\u4f4e\u6837\u672c\u7ec6\u7c92\u5ea6\u5206\u7c7b\u4efb\u52a1\u7684\u5408\u6210\u6570\u636e\u8d28\u91cf\uff0c\u540c\u65f6\u907f\u514d\u8fc7\u62df\u5408\u548c\u4fdd\u6301\u591a\u6837\u6027\u3002", "motivation": "\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u6a21\u578b\u5728\u751f\u6210\u5408\u6210\u8bad\u7ec3\u6570\u636e\u65b9\u9762\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u751f\u6210\u6709\u6548\u7684\u5408\u6210\u8bad\u7ec3\u6570\u636e\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002\u867d\u7136\u4f7f\u7528\u5c11\u91cf\u771f\u5b9e\u6837\u672c\u5fae\u8c03T2I\u6a21\u578b\u53ef\u4ee5\u63d0\u9ad8\u5408\u6210\u6570\u636e\u7684\u8d28\u91cf\uff0c\u4f46\u53ef\u80fd\u5bfc\u81f4\u8fc7\u62df\u5408\u5e76\u964d\u4f4e\u751f\u6210\u6837\u672c\u7684\u591a\u6837\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aBOB\uff08Beyond Objects\uff09\u7684\u5fae\u8c03\u7b56\u7565\u3002\u8be5\u7b56\u7565\u9996\u5148\u4ece\u5c11\u91cf\u771f\u5b9e\u6837\u672c\u4e2d\u63d0\u53d6\u7c7b\u522b\u65e0\u5173\u7684\u5c5e\u6027\uff08\u5982\u573a\u666f\u80cc\u666f\u548c\u7269\u4f53\u59ff\u6001\uff09\uff0c\u7136\u540e\u5728\u5fae\u8c03T2I\u6a21\u578b\u65f6\u663e\u5f0f\u5730\u4ee5\u8fd9\u4e9b\u5c5e\u6027\u4f5c\u4e3a\u6761\u4ef6\uff0c\u5728\u751f\u6210\u65f6\u518d\u5c06\u5b83\u4eec\u8fb9\u7f18\u5316\u6389\u3002", "result": "\u901a\u8fc7\u5728\u591a\u79cdT2I\u6a21\u578b\u3001\u9aa8\u5e72\u7f51\u7edc\u548c\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86BOB\u65b9\u6cd5\u5728\u4f4e\u6837\u672c\u7ec6\u7c92\u5ea6\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u901a\u8fc7\u589e\u52a0\u5408\u6210\u6570\u636e\u8fdb\u884c\u589e\u5f3a\u65f6\uff0c\u80fd\u591f\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u5177\u4f53\u800c\u8a00\uff0c\u5728\u98de\u673a\u6570\u636e\u96c6\u4e0a\uff0cBOB\u76f8\u5bf9\u4e8eDataDream\u63d0\u5347\u4e867.4%\u7684\u6027\u80fd\uff08\u4f7f\u75285\u5f20\u771f\u5b9e\u56fe\u50cf\u548c100\u5f20\u5408\u6210\u56fe\u50cf\u5fae\u8c03CLIP\u5206\u7c7b\u5668\u65f6\uff0c\u51c6\u786e\u7387\u4ece50.0%\u63d0\u5347\u523057.4%\uff09\u3002\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u4e09\u4e2a\u4e2d\uff0c\u4f7f\u75285\u5f20\u771f\u5b9e\u56fe\u50cf\u5e76\u7ed3\u5408BOB\u751f\u6210\u7684\u5408\u6210\u56fe\u50cf\u8fdb\u884c\u4e0b\u6e38\u6a21\u578b\u5fae\u8c03\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u4ec5\u4f7f\u752810\u5f20\u771f\u5b9e\u56fe\u50cf\u8fdb\u884c\u5fae\u8c03\u3002\u603b\u7684\u6765\u8bf4\uff0cBOB\u572824\u79cd\u5b9e\u9a8c\u8bbe\u7f6e\u4e2d\u768418\u79cd\u4e2d\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5728\u5176\u4e2d14\u79cd\u8bbe\u7f6e\u4e2d\u51c6\u786e\u7387\u63d0\u9ad8\u4e862%\u4ee5\u4e0a\u3002", "conclusion": "BOB\u901a\u8fc7\u89e3\u8026\u5e76\u663e\u5f0f\u5229\u7528\u7c7b\u522b\u65e0\u5173\u5c5e\u6027\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5728\u4f4e\u6837\u672c\u7ec6\u7c92\u5ea6\u5206\u7c7b\u4efb\u52a1\u4e2d\u5fae\u8c03T2I\u6a21\u578b\u65f6\u53ef\u80fd\u51fa\u73b0\u7684\u8fc7\u62df\u5408\u548c\u591a\u6837\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5408\u6210\u6570\u636e\u7684\u8bad\u7ec3\u6548\u679c\uff0c\u8fbe\u5230\u4e86\u5f53\u524d\u6700\u4f18\u6c34\u5e73\u3002"}}
{"id": "2510.24163", "categories": ["quant-ph", "gr-qc", "hep-th"], "pdf": "https://arxiv.org/pdf/2510.24163", "abs": "https://arxiv.org/abs/2510.24163", "authors": ["Zhenghao Luo", "Yi Li", "Xingyu Zhao", "Zihan Xie", "Zehua Tian", "Yiheng Lin"], "title": "Experimental Demonstration of the Timelike Unruh Effect with a Trapped-Ion System", "comment": "11 pages, 6 figures", "summary": "The Unruh effect predicts that an accelerated observer perceives the\nMinkowski vacuum as a thermal bath, but its direct observation requires extreme\naccelerations beyond current experimental reach. Foundational theory [Olson &\nRalph, Phys. Rev. Lett. 106, 110404 (2011)] shows that an equivalent thermal\nresponse, known as the timelike Unruh effect, can occur for detectors following\nspecific timelike trajectories without acceleration, enabling laboratory tests\nwith stationary yet time-dependent detectors. Here, we report a\nproof-of-principle demonstration of the timelike Unruh effect in a quantum\nsystem of trapped ion, where a two-level spin serves as the detector and is\ntemporally coupled to the ambient field encoded in the ion's vibrational\nmotion. Specifically, we study both excitation and emission dynamics of the\ndetector moving along a spacetime trajectory in the future/past light cone, and\ndemonstrate the thermal response of the detector to the Minkowski vacuum that\nresembles the Unruh effect. This work establishes a controllable tabletop\nplatform for exploring relativistic quantum physics under accessible laboratory\nconditions.", "AI": {"tldr": "\u52a0\u901f\u89c2\u5bdf\u8005\u4f1a\u611f\u77e5\u5230\u7c73\u6c0f\u771f\u7a7a\u662f\u4e00\u4e2a\u70ed\u6d74\uff0c\u4f46\u5176\u76f4\u63a5\u89c2\u6d4b\u9700\u8981\u6781\u9ad8\u7684\u52a0\u901f\u5ea6\u3002\u672c\u6587\u5c55\u793a\u4e86\u4e00\u79cd\u53ef\u5728\u5b9e\u9a8c\u5ba4\u6761\u4ef6\u4e0b\u901a\u8fc7\u7c7b\u65f6\u8f68\u8ff9\u5b9e\u73b0\u7684\u7b49\u6548\u70ed\u54cd\u5e94\uff0c\u5373\u7c7b\u65f6\u6602\u9c81\u6548\u5e94\uff0c\u5e76\u4f7f\u7528\u79bb\u5b50\u9631\u91cf\u5b50\u7cfb\u7edf\u8fdb\u884c\u4e86\u539f\u7406\u6027\u6f14\u793a\u3002", "motivation": "\u6f14\u793a\u5728\u5b9e\u9a8c\u5ba4\u6761\u4ef6\u4e0b\u53ef\u884c\u7684\u7c7b\u65f6\u6602\u9c81\u6548\u5e94\uff0c\u4ee5\u66ff\u4ee3\u9700\u8981\u6781\u9ad8\u52a0\u901f\u5ea6\u7684\u76f4\u63a5\u6602\u9c81\u6548\u5e94\u89c2\u6d4b\u3002", "method": "\u4f7f\u7528\u79bb\u5b50\u9631\u4e2d\u7684\u4e24\u80fd\u7ea7\u81ea\u65cb\u4f5c\u4e3a\u63a2\u6d4b\u5668\uff0c\u4f7f\u5176\u6cbf\u7740\u7279\u5b9a\u7684\u7c7b\u65f6\u65f6\u7a7a\u8f68\u8ff9\u6f14\u5316\uff0c\u5e76\u4e0e\u7f16\u7801\u5728\u79bb\u5b50\u632f\u52a8\u4e2d\u7684\u73af\u5883\u573a\u8fdb\u884c\u65f6\u95f4\u8026\u5408\u3002", "result": "\u89c2\u5bdf\u5230\u63a2\u6d4b\u5668\u7684\u6fc0\u53d1\u548c\u53d1\u5c04\u52a8\u529b\u5b66\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u5bf9\u7c73\u6c0f\u771f\u7a7a\u7684\u70ed\u54cd\u5e94\uff0c\u8be5\u54cd\u5e94\u7c7b\u4f3c\u4e8e\u6602\u9c81\u6548\u5e94\u3002", "conclusion": "\u6210\u529f\u5728\u53ef\u63a7\u7684\u684c\u9762\u5e73\u53f0\u4e0a\u6f14\u793a\u4e86\u7c7b\u65f6\u6602\u9c81\u6548\u5e94\uff0c\u4e3a\u5728\u53ef\u53ca\u7684\u5b9e\u9a8c\u5ba4\u6761\u4ef6\u4e0b\u63a2\u7d22\u76f8\u5bf9\u8bba\u91cf\u5b50\u7269\u7406\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2510.23658", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23658", "abs": "https://arxiv.org/abs/2510.23658", "authors": ["Vaibhav Jindal", "Hejian Sang", "Chun-Mao Lai", "Yanning Chen", "Zhipeng Wang"], "title": "Aligning Diffusion Language Models via Unpaired Preference Optimization", "comment": null, "summary": "Diffusion language models (dLLMs) are an emerging alternative to\nautoregressive (AR) generators, but aligning them to human preferences is\nchallenging because sequence log-likelihoods are intractable and pairwise\npreference data are costly to collect. We introduce ELBO-KTO, which combines an\nELBO surrogate for diffusion log-likelihoods with a prospect-theoretic,\nunpaired preference objective (Kahneman Tversky Optimization, KTO). We analyze\nthe bias and variance induced by the ELBO substitution and employ\nvariance-reduction practices that stabilize gradients during training. Applied\nto LLaDA-8B-Instruct, ELBO-KTO yields \\textbf{65.9\\%} and \\textbf{62.3\\%}\nadjusted win rates on kto-mix-14k and UltraFeedback-Binary, respectively,\nversus the base model under an automatic LLM judge. Across downstream tasks,\nincluding GSM8K, MMLU, and additional reasoning/knowledge benchmarks, ELBO-KTO\ntrained on UltraFeedback-Binary performs on par with or better than the base\nmodel under identical decoding. This establishes unpaired preference\noptimization as a viable alternative to pairwise alignment in diffusion LLMs.", "AI": {"tldr": "ELBO-KTO\u662f\u4e00\u79cd\u7ed3\u5408\u4e86ELBO\u548cKTO\u7684\u65b0\u578b\u65b9\u6cd5\uff0c\u7528\u4e8e\u5bf9\u6269\u6563\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\uff0c\u5728\u4e0d\u914d\u5bf9\u504f\u597d\u4f18\u5316\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\u3002", "motivation": "\u5bf9\u9f50\u6269\u6563\u8bed\u8a00\u6a21\u578b\uff08dLLMs\uff09\u4ee5\u9002\u5e94\u4eba\u7c7b\u504f\u597d\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u5e8f\u5217\u5bf9\u6570\u4f3c\u7136\u662f\u96be\u4ee5\u5904\u7406\u7684\uff0c\u5e76\u4e14\u914d\u5bf9\u504f\u597d\u6570\u636e\u6536\u96c6\u6210\u672c\u9ad8\u6602\u3002", "method": "ELBO-KTO\u7ed3\u5408\u4e86\u7528\u4e8e\u6269\u6563\u5bf9\u6570\u4f3c\u7136\u7684ELBO\u4ee3\u7406\u548c\u57fa\u4e8e\u524d\u666f\u7406\u8bba\u7684\u3001\u672a\u914d\u5bf9\u7684\u504f\u597d\u76ee\u6807\uff08KTO\uff09\u3002\u5206\u6790\u4e86ELBO\u66ff\u4ee3\u5f15\u5165\u7684\u504f\u5dee\u548c\u65b9\u5dee\uff0c\u5e76\u91c7\u7528\u4e86\u51cf\u5c11\u65b9\u5dee\u7684\u505a\u6cd5\u6765\u7a33\u5b9a\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u68af\u5ea6\u3002", "result": "\u5c06ELBO-KTO\u5e94\u7528\u4e8eLLaDA-8B-Instruct\uff0c\u5728kto-mix-14k\u548cUltraFeedback-Binary\u6570\u636e\u96c6\u4e0a\uff0c\u76f8\u5bf9\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u5728\u81ea\u52a8LLM\u8bc4\u5224\u4e0b\u5206\u522b\u83b7\u5f97\u4e8665.9%\u548c62.3%\u7684\u8c03\u6574\u540e\u80dc\u7387\u3002\u5728GSM8K\u3001MMLU\u4ee5\u53ca\u5176\u4ed6\u63a8\u7406/\u77e5\u8bc6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cELBO-KTO\u5728UltraFeedback-Binary\u4e0a\u8bad\u7ec3\u540e\uff0c\u5728\u76f8\u540c\u89e3\u7801\u6761\u4ef6\u4e0b\u8868\u73b0\u4e0e\u57fa\u7ebf\u6a21\u578b\u76f8\u5f53\u6216\u66f4\u4f18\u3002", "conclusion": "ELBO-KTO\u8bc1\u660e\u4e86\u672a\u914d\u5bf9\u504f\u597d\u4f18\u5316\u53ef\u4ee5\u4f5c\u4e3a\u6269\u6563\u8bed\u8a00\u6a21\u578b\u4e2d\u914d\u5bf9\u5bf9\u9f50\u7684\u53ef\u884c\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2510.24151", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24151", "abs": "https://arxiv.org/abs/2510.24151", "authors": ["Bingsen Qiu", "Zijian Liu", "Xiao Liu", "Haoshen Yang", "Zeren Gao", "Bingjie Wang", "Feier Zhang", "Yixuan Qin", "Chunyan Li"], "title": "BMGQ: A Bottom-up Method for Generating Complex Multi-hop Reasoning Questions from Semi-structured Data", "comment": null, "summary": "Building training-ready multi-hop question answering (QA) datasets that truly\nstress a model's retrieval and reasoning abilities remains highly challenging\nrecently. While there have been a few recent evaluation datasets that capture\nthe characteristics of hard-to-search but easy-to-verify problems -- requiring\nthe integration of ambiguous, indirect, and cross-domain cues -- these data\nresources remain scarce and are mostly designed for evaluation, making them\nunsuitable for supervised fine-tuning (SFT) or reinforcement learning (RL).\nMeanwhile, manually curating non-trivially retrievable questions -- where\nanswers cannot be found through a single direct query but instead require\nmulti-hop reasoning over oblique and loosely connected evidence -- incurs\nprohibitive human costs and fails to scale, creating a critical data bottleneck\nfor training high-capability retrieval-and-reasoning agents.\n  To address this, we present an automated framework for generating\nhigh-difficulty, training-ready multi-hop questions from semi-structured\nknowledge sources. The system (i) grows diverse, logically labeled evidence\nclusters through Natural Language Inference (NLI)-based relation typing and\ndiversity-aware expansion; (ii) applies reverse question construction to\ncompose oblique cues so that isolated signals are underinformative but their\ncombination uniquely identifies the target entity; and (iii) enforces quality\nwith a two-step evaluation pipeline that combines multi-model consensus\nfiltering with structured constraint decomposition and evidence-based matching.\nThe result is a scalable process that yields complex, retrieval-resistant yet\nverifiable questions suitable for SFT/RL training as well as challenging\nevaluation, substantially reducing human curation effort while preserving the\ndifficulty profile of strong evaluation benchmarks.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u534a\u7ed3\u6784\u5316\u77e5\u8bc6\u6e90\u751f\u6210\u9ad8\u96be\u5ea6\u3001\u53ef\u7528\u4e8e\u8bad\u7ec3\u7684\u591a\u8df3\u95ee\u7b54\uff08QA\uff09\u95ee\u9898\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u6570\u636e\u96c6\u7a00\u7f3a\u4e14\u4e0d\u9002\u5408\u6a21\u578b\u8bad\u7ec3\u7684\u74f6\u9888\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u89c4\u6a21\u591a\u8df3QA\u6570\u636e\u96c6\u96be\u4ee5\u6ee1\u8db3\u6a21\u578b\u8bad\u7ec3\u9700\u6c42\uff0c\u56e0\u5176\u8fc7\u4e8e\u4fa7\u91cd\u6613\u4e8e\u9a8c\u8bc1\u4f46\u96be\u4e8e\u68c0\u7d22\u7684\u95ee\u9898\uff0c\u4e14\u4e0d\u9002\u5408\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u6216\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u3002\u624b\u52a8\u521b\u5efa\u590d\u6742\u7684\u591a\u8df3\u95ee\u9898\u6210\u672c\u9ad8\u6602\u4e14\u96be\u4ee5\u6269\u5c55\u3002", "method": "\u8be5\u6846\u67b6\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63a8\u7406\uff08NLI\uff09\u5173\u7cfb\u7c7b\u578b\u548c\u591a\u6837\u6027\u611f\u77e5\u6269\u5c55\u6765\u751f\u6210\u591a\u6837\u7684\u3001\u5e26\u6709\u903b\u8f91\u6807\u7b7e\u7684\u8bc1\u636e\u96c6\uff1b\u5229\u7528\u53cd\u5411\u95ee\u9898\u751f\u6210\u6765\u7ec4\u5408\u6a21\u7cca\u7ebf\u7d22\uff0c\u4f7f\u5f97\u5355\u4e00\u7ebf\u7d22\u4fe1\u606f\u4e0d\u8db3\u4f46\u7ec4\u5408\u8d77\u6765\u80fd\u552f\u4e00\u786e\u5b9a\u76ee\u6807\u5b9e\u4f53\uff1b\u6700\u540e\u901a\u8fc7\u591a\u6a21\u578b\u5171\u8bc6\u8fc7\u6ee4\u548c\u7ed3\u6784\u5316\u7ea6\u675f\u5206\u89e3\u4e0e\u57fa\u4e8e\u8bc1\u636e\u7684\u5339\u914d\u8fdb\u884c\u4e24\u6b65\u8bc4\u4f30\u4ee5\u4fdd\u8bc1\u95ee\u9898\u8d28\u91cf\u3002", "result": "\u8be5\u6846\u67b6\u80fd\u591f\u5927\u89c4\u6a21\u751f\u6210\u590d\u6742\u3001\u68c0\u7d22\u56f0\u96be\u4f46\u53ef\u9a8c\u8bc1\u7684\u95ee\u9898\uff0c\u9002\u7528\u4e8eSFT/RL\u8bad\u7ec3\u548c\u8bc4\u4f30\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u4eba\u5de5\u6210\u672c\uff0c\u5e76\u4fdd\u6301\u4e86\u4e0e\u73b0\u6709\u8bc4\u4f30\u57fa\u51c6\u7684\u96be\u5ea6\u6c34\u5e73\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u81ea\u52a8\u5316\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u591a\u8df3QA\u6570\u636e\u751f\u6210\u4e2d\u7684\u6570\u636e\u74f6\u9888\u95ee\u9898\uff0c\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u63a8\u52a8\u4e86\u80fd\u591f\u8fdb\u884c\u68c0\u7d22\u548c\u63a8\u7406\u7684AI\u4ee3\u7406\u7684\u53d1\u5c55\u3002"}}
{"id": "2510.24073", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24073", "abs": "https://arxiv.org/abs/2510.24073", "authors": ["Xinwei Wu", "Heng Liu", "Jiang Zhou", "Xiaohu Zhao", "Linlong Xu", "Longyue Wang", "Weihua Luo", "Kaifu Zhang"], "title": "Challenging Multilingual LLMs: A New Taxonomy and Benchmark for Unraveling Hallucination in Translation", "comment": null, "summary": "Large Language Models (LLMs) have advanced machine translation but remain\nvulnerable to hallucinations. Unfortunately, existing MT benchmarks are not\ncapable of exposing failures in multilingual LLMs. To disclose hallucination in\nmultilingual LLMs, we introduce a diagnostic framework with a taxonomy that\nseparates Instruction Detachment from Source Detachment. Guided by this\ntaxonomy, we create HalloMTBench, a multilingual, human-verified benchmark\nacross 11 English-to-X directions. We employed 4 frontier LLMs to generate\ncandidates and scrutinize these candidates with an ensemble of LLM judges, and\nexpert validation. In this way, we curate 5,435 high-quality instances. We have\nevaluated 17 LLMs on HalloMTBench. Results reveal distinct ``hallucination\ntriggers'' -- unique failure patterns reflecting model scale, source length\nsensitivity, linguistic biases, and Reinforcement-Learning (RL) amplified\nlanguage mixing. HalloMTBench offers a forward-looking testbed for diagnosing\nLLM translation failures. HalloMTBench is available in\nhttps://huggingface.co/collections/AIDC-AI/marco-mt.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aHalloMTBench\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u673a\u5668\u7ffb\u8bd1\uff08MT\uff09\u4e2d\u51fa\u73b0\u7684\u5e7b\u89c9\u95ee\u9898\u3002\u901a\u8fc7\u533a\u5206\u201c\u6307\u4ee4\u5206\u79bb\u201d\u548c\u201c\u6e90\u5206\u79bb\u201d\u4e24\u4e2a\u7ef4\u5ea6\uff0c\u5e76\u7ed3\u5408\u4eba\u7c7b\u4e13\u5bb6\u7684\u9a8c\u8bc1\uff0c\u8be5\u6846\u67b6\u751f\u6210\u4e86\u5305\u542b5435\u4e2a\u5b9e\u4f8b\u7684\u6d4b\u8bd5\u96c6\uff0c\u5e76\u8bc4\u4f30\u4e8617\u4e2aLLMs\uff0c\u63ed\u793a\u4e86\u4e0e\u6a21\u578b\u89c4\u6a21\u3001\u6e90\u6587\u672c\u957f\u5ea6\u3001\u8bed\u8a00\u504f\u89c1\u548cRL\u589e\u5f3a\u7684\u8bed\u8a00\u6df7\u5408\u76f8\u5173\u7684\u201c\u5e7b\u89c9\u89e6\u53d1\u5668\u201d\u3002", "motivation": "\u73b0\u6709\u7684\u673a\u5668\u7ffb\u8bd1\u57fa\u51c6\u65e0\u6cd5\u6709\u6548\u63ed\u793a\u591a\u8bed\u8a00\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u4e13\u95e8\u7684\u8bca\u65ad\u6846\u67b6\u6765\u89e3\u51b3\u8fd9\u4e00\u7f3a\u9677\u3002", "method": "\u7814\u7a76\u4eba\u5458\u521b\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u201c\u6307\u4ee4\u5206\u79bb\u201d\u548c\u201c\u6e90\u5206\u79bb\u201d\u4e24\u4e2a\u7ef4\u5ea6\u7684\u5206\u7c7b\u6cd5\uff0c\u5e76\u57fa\u4e8e\u6b64\u5f00\u53d1\u4e86HalloMTBench\u591a\u8bed\u8a00\u57fa\u51c6\u3002\u8be5\u57fa\u51c6\u5305\u542b11\u4e2a\u82f1\u8bed\u5230\u5176\u4ed6\u8bed\u8a00\u7684\u7ffb\u8bd1\u65b9\u5411\uff0c\u5229\u75284\u4e2a\u524d\u6cbfLLMs\u751f\u6210\u7ffb\u8bd1\u5019\u9009\uff0c\u5e76\u7ed3\u5408LLM\u88c1\u5224\u548c\u4e13\u5bb6\u9a8c\u8bc1\u6765\u7b5b\u9009\u51fa5435\u4e2a\u9ad8\u8d28\u91cf\u7684\u5b9e\u4f8b\u3002", "result": "\u5728HalloMTBench\u4e0a\u5bf917\u4e2aLLMs\u7684\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0c\u6a21\u578b\u89c4\u6a21\u3001\u6e90\u6587\u672c\u957f\u5ea6\u654f\u611f\u6027\u3001\u8bed\u8a00\u504f\u89c1\u4ee5\u53ca\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u589e\u5f3a\u7684\u8bed\u8a00\u6df7\u5408\u662f\u5bfc\u81f4\u5e7b\u89c9\u7684\u72ec\u7279\u201c\u5e7b\u89c9\u89e6\u53d1\u5668\u201d\u3002", "conclusion": "HalloMTBench\u63d0\u4f9b\u4e86\u4e00\u4e2a\u524d\u77bb\u6027\u7684\u6d4b\u8bd5\u5e73\u53f0\uff0c\u7528\u4e8e\u8bca\u65adLLMs\u5728\u7ffb\u8bd1\u4e2d\u51fa\u73b0\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u6709\u52a9\u4e8e\u672a\u6765\u591a\u8bed\u8a00LLMs\u7684\u6539\u8fdb\u3002"}}
{"id": "2510.24515", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24515", "abs": "https://arxiv.org/abs/2510.24515", "authors": ["Malintha Fernando", "Petter \u00d6gren", "Silun Zhang"], "title": "Stochastic Prize-Collecting Games: Strategic Planning in Multi-Robot Systems", "comment": "Submitted to IEEE Robotics and Automation Letters", "summary": "The Team Orienteering Problem (TOP) generalizes many real-world multi-robot\nscheduling and routing tasks that occur in autonomous mobility, aerial\nlogistics, and surveillance applications. While many flavors of the TOP exist\nfor planning in multi-robot systems, they assume that all the robots cooperate\ntoward a single objective; thus, they do not extend to settings where the\nrobots compete in reward-scarce environments. We propose Stochastic\nPrize-Collecting Games (SPCG) as an extension of the TOP to plan in the\npresence of self-interested robots operating on a graph, under energy\nconstraints and stochastic transitions. A theoretical study on complete and\nstar graphs establishes that there is a unique pure Nash equilibrium in SPCGs\nthat coincides with the optimal routing solution of an equivalent TOP given a\nrank-based conflict resolution rule. This work proposes two algorithms: Ordinal\nRank Search (ORS) to obtain the ''ordinal rank'' --one's effective rank in\ntemporarily-formed local neighborhoods during the games' stages, and Fictitious\nOrdinal Response Learning (FORL) to obtain best-response policies against one's\nsenior-rank opponents. Empirical evaluations conducted on road networks and\nsynthetic graphs under both dynamic and stationary prize distributions show\nthat 1) the state-aliasing induced by OR-conditioning enables learning policies\nthat scale more efficiently to large team sizes than those trained with the\nglobal index, and 2) Policies trained with FORL generalize better to imbalanced\nprize distributions than those with other multi-agent training methods.\nFinally, the learned policies in the SPCG achieved between 87% and 95%\noptimality compared to an equivalent TOP solution obtained by mixed-integer\nlinear programming.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u968f\u673a\u5956\u8d4f\u6536\u96c6\u535a\u5f08\uff08SPCG\uff09\u6765\u89e3\u51b3\u591a\u673a\u5668\u4eba\u89c4\u5212\u95ee\u9898\uff0c\u5e76\u5f00\u53d1\u4e86\u5e8f\u6570\u6392\u540d\u641c\u7d22\uff08ORS\uff09\u548c\u865a\u6784\u5e8f\u6570\u54cd\u5e94\u5b66\u4e60\uff08FORL\uff09\u7b97\u6cd5\uff0c\u5728\u5956\u52b1\u7a00\u7f3a\u7684\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u63a5\u8fd1\u6700\u4f18\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u56e2\u961f\u5bfc\u5411\u95ee\u9898\uff08TOP\uff09\u7684\u7814\u7a76\u5047\u8bbe\u673a\u5668\u4eba\u534f\u540c\u5de5\u4f5c\uff0c\u65e0\u6cd5\u89e3\u51b3\u673a\u5668\u4eba\u4e4b\u95f4\u5b58\u5728\u7ade\u4e89\u3001\u5956\u52b1\u6709\u9650\u7684\u73af\u5883\u4e0b\u7684\u89c4\u5212\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u968f\u673a\u5956\u8d4f\u6536\u96c6\u535a\u5f08\uff08SPCG\uff09\u4f5c\u4e3aTOP\u7684\u6269\u5c55\uff0c\u5e76\u5f00\u53d1\u4e86\u5e8f\u6570\u6392\u540d\u641c\u7d22\uff08ORS\uff09\u548c\u865a\u6784\u5e8f\u6570\u54cd\u5e94\u5b66\u4e60\uff08FORL\uff09\u7b97\u6cd5\u6765\u5b66\u4e60\u673a\u5668\u4eba\u7b56\u7565\u3002", "result": "\u7814\u7a76\u8bc1\u660e\u4e86\u5728\u7279\u5b9a\u56fe\u7ed3\u6784\u4e2d\u5b58\u5728\u552f\u4e00\u7684\u7eaf\u7eb3\u5747\u8861\uff0c\u5e76\u8868\u660eSPCG\u80fd\u6709\u6548\u5904\u7406\u81ea\u6211\u5229\u76ca\u51b2\u7a81\u7684\u673a\u5668\u4eba\u3002\u7b97\u6cd5\u5728\u5b9e\u9645\u9053\u8def\u7f51\u7edc\u548c\u5408\u6210\u56fe\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u57fa\u4e8e\u5e8f\u6570\u6392\u540d\u7684\u8bad\u7ec3\u7b56\u7565\u6bd4\u57fa\u4e8e\u5168\u5c40\u7d22\u5f15\u7684\u7b56\u7565\u6269\u5c55\u6027\u66f4\u597d\uff0c\u5e76\u4e14FORL\u7b97\u6cd5\u5728\u4e0d\u5e73\u8861\u5956\u8d4f\u5206\u5e03\u4e0b\u5177\u6709\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002\u6700\u7ec8\u5b66\u4e60\u5230\u7684\u7b56\u7565\u8fbe\u5230\u4e86\u7b49\u6548TOP\u89e3\u51b3\u65b9\u6848\u768487%\u81f395%\u7684\u6700\u4f18\u6027\u3002", "conclusion": "SPCG\u53ca\u5176\u76f8\u5173\u7b97\u6cd5\u4e3a\u89e3\u51b3\u591a\u673a\u5668\u4eba\u534f\u4f5c\u4e0e\u7ade\u4e89\u5e76\u5b58\u7684\u590d\u6742\u89c4\u5212\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u5956\u52b1\u6709\u9650\u4e14\u673a\u5668\u4eba\u5177\u6709\u81ea\u6211\u5229\u76ca\u7684\u73af\u5883\u4e0b\u3002"}}
{"id": "2510.24093", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24093", "abs": "https://arxiv.org/abs/2510.24093", "authors": ["Agus Gunawan", "Samuel Teodoro", "Yun Chen", "Soo Ye Kim", "Jihyong Oh", "Munchurl Kim"], "title": "OmniText: A Training-Free Generalist for Controllable Text-Image Manipulation", "comment": "The first two authors contributed equally to this work. The last two\n  authors are co-corresponding authors", "summary": "Recent advancements in diffusion-based text synthesis have demonstrated\nsignificant performance in inserting and editing text within images via\ninpainting. However, despite the potential of text inpainting methods, three\nkey limitations hinder their applicability to broader Text Image Manipulation\n(TIM) tasks: (i) the inability to remove text, (ii) the lack of control over\nthe style of rendered text, and (iii) a tendency to generate duplicated\nletters. To address these challenges, we propose OmniText, a training-free\ngeneralist capable of performing a wide range of TIM tasks. Specifically, we\ninvestigate two key properties of cross- and self-attention mechanisms to\nenable text removal and to provide control over both text styles and content.\nOur findings reveal that text removal can be achieved by applying\nself-attention inversion, which mitigates the model's tendency to focus on\nsurrounding text, thus reducing text hallucinations. Additionally, we\nredistribute cross-attention, as increasing the probability of certain text\ntokens reduces text hallucination. For controllable inpainting, we introduce\nnovel loss functions in a latent optimization framework: a cross-attention\ncontent loss to improve text rendering accuracy and a self-attention style loss\nto facilitate style customization. Furthermore, we present OmniText-Bench, a\nbenchmark dataset for evaluating diverse TIM tasks. It includes input images,\ntarget text with masks, and style references, covering diverse applications\nsuch as text removal, rescaling, repositioning, and insertion and editing with\nvarious styles. Our OmniText framework is the first generalist method capable\nof performing diverse TIM tasks. It achieves state-of-the-art performance\nacross multiple tasks and metrics compared to other text inpainting methods and\nis comparable with specialist methods.", "AI": {"tldr": "OmniText\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u901a\u7528\u6587\u672c\u56fe\u50cf\u5904\u7406\u65b9\u6cd5\uff0c\u80fd\u591f\u89e3\u51b3\u73b0\u6709\u6587\u672c\u4fee\u590d\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u652f\u6301\u6587\u672c\u79fb\u9664\u3001\u98ce\u683c\u63a7\u5236\u548c\u5185\u5bb9\u7f16\u8f91\u7b49\u591a\u79cd\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u4fee\u590d\u65b9\u6cd5\u5728\u6587\u672c\u79fb\u9664\u3001\u98ce\u683c\u63a7\u5236\u548c\u91cd\u590d\u5b57\u7b26\u751f\u6210\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9650\u5236\u4e86\u5176\u5728\u66f4\u5e7f\u6cdb\u7684\u6587\u672c\u56fe\u50cf\u5904\u7406\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u901a\u8fc7\u7814\u7a76\u8de8\u6ce8\u610f\u529b\u548c\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u63d0\u51faOmniText\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\uff1a1. \u4f7f\u7528\u81ea\u6ce8\u610f\u529b\u53cd\u8f6c\u6765\u5b9e\u73b0\u6587\u672c\u79fb\u9664\uff0c\u51cf\u5c11\u5bf9\u5468\u56f4\u6587\u672c\u7684\u5173\u6ce8\uff0c\u964d\u4f4e\u5e7b\u89c9\u30022. \u91cd\u65b0\u5206\u914d\u8de8\u6ce8\u610f\u529b\uff0c\u589e\u52a0\u67d0\u4e9b\u6587\u672c\u6807\u8bb0\u7684\u6982\u7387\uff0c\u51cf\u5c11\u5e7b\u89c9\u30023. \u5f15\u5165\u65b0\u7684\u635f\u5931\u51fd\u6570\uff08\u8de8\u6ce8\u610f\u529b\u5185\u5bb9\u635f\u5931\u548c\u81ea\u6ce8\u610f\u529b\u98ce\u683c\u635f\u5931\uff09\u5230\u6f5c\u5728\u4f18\u5316\u6846\u67b6\u4e2d\uff0c\u4ee5\u5b9e\u73b0\u53ef\u63a7\u7684\u4fee\u590d\uff0c\u7528\u4e8e\u6587\u672c\u5185\u5bb9\u6e32\u67d3\u548c\u98ce\u683c\u5b9a\u5236\u30024. \u6784\u5efaOmniText-Bench\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6837\u5316\u7684\u6587\u672c\u56fe\u50cf\u5904\u7406\u4efb\u52a1\u3002", "result": "OmniText\u5728\u6587\u672c\u79fb\u9664\u3001\u91cd\u7f29\u653e\u3001\u91cd\u5b9a\u4f4d\u3001\u63d2\u5165\u548c\u7f16\u8f91\u7b49\u591a\u79cd\u4efb\u52a1\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u5728\u4e0e\u4e13\u95e8\u65b9\u6cd5\u76f8\u6bd4\u65f6\u5177\u6709\u53ef\u6bd4\u6027\u3002", "conclusion": "OmniText\u662f\u7b2c\u4e00\u4e2a\u80fd\u591f\u6267\u884c\u591a\u6837\u5316\u6587\u672c\u56fe\u50cf\u5904\u7406\u4efb\u52a1\u7684\u901a\u7528\u65b9\u6cd5\uff0c\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u548c\u6307\u6807\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u5176\u4ed6\u6587\u672c\u4fee\u590d\u65b9\u6cd5\u548c\u53ef\u4e0e\u4e13\u95e8\u65b9\u6cd5\u76f8\u5ab2\u7f8e\u7684\u6027\u80fd\u3002"}}
{"id": "2510.24181", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.24181", "abs": "https://arxiv.org/abs/2510.24181", "authors": ["SiYing Wang", "ZhiXin Xia", "Yue Yan", "Xiang-Bin Wang"], "title": "An exact Error Threshold of Surface Code under Correlated Nearest-Neighbor Errors: A Statistical Mechanical Analysis", "comment": null, "summary": "The surface code represents a promising candidate for fault-tolerant quantum\ncomputation due to its high error threshold and experimental accessibility with\nnearest-neighbor interactions. However, current exact surface code threshold\nanalyses are based on the assumption of independent and identically distributed\n(i.i.d.) errors. Though there are numerical studieds for threshold with\ncorrelated error, they are only the lower bond ranther than exact value, this\noffers potential for higher error thresholds.Here, we establish an error-edge\nmap, which allows for the mapping of quantum error correction to a\nsquare-octagonal random bond Ising model. We then present the exact threshold\nunder a realistic noise model that combines independent single-qubit errors\nwith correlated errors between nearest-neighbor data qubits. Our method is\napplicable for any ratio of nearest-neighbor correlated errors to i.i.d.\nerrors. We investigate the error correction threshold of surface codes and we\npresent analytical constraints giving exact value of error threshold. This\nmeans that our error threshold is both upper bound and achievable and hence on\nthe one hand the existing numerical threshold values can all be improved to our\nthreshold value, on the other hand, our threshold value is highest achievable\nvalue in principle.", "AI": {"tldr": "\u8868\u9762\u7801\u662f\u4e00\u79cd\u6709\u524d\u9014\u7684\u5bb9\u9519\u91cf\u5b50\u8ba1\u7b97\u65b9\u6cd5\uff0c\u4f46\u76ee\u524d\u7684\u9608\u503c\u5206\u6790\u57fa\u4e8e\u72ec\u7acb\u540c\u5206\u5e03\uff08i.i.d.\uff09\u9519\u8bef\u7684\u5047\u8bbe\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u91cf\u5b50\u8bef\u5dee\u6821\u6b63\u6620\u5c04\u5230\u65b9\u5f62-\u516b\u8fb9\u5f62\u968f\u673a\u952e\u4f0a\u8f9b\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u5e76\u5728\u8003\u8651\u4e86\u72ec\u7acb\u5355\u91cf\u5b50\u6bd4\u7279\u9519\u8bef\u548c\u6700\u8fd1\u90bb\u6570\u636e\u91cf\u5b50\u6bd4\u7279\u4e4b\u95f4\u76f8\u5173\u9519\u8bef\u7684\u60c5\u51b5\u4e0b\uff0c\u5f97\u51fa\u4e86\u7cbe\u786e\u7684\u9519\u8bef\u9608\u503c\u3002", "motivation": "\u5f53\u524d\u7684\u8868\u9762\u7801\u5bb9\u9519\u91cf\u5b50\u8ba1\u7b97\u7814\u7a76\u4e2d\u7684\u9519\u8bef\u9608\u503c\u5206\u6790\u57fa\u4e8e\u72ec\u7acb\u540c\u5206\u5e03\uff08i.i.d.\uff09\u9519\u8bef\u7684\u5047\u8bbe\uff0c\u800c\u5ffd\u7565\u4e86\u5b9e\u9645\u566a\u58f0\u6a21\u578b\u4e2d\u53ef\u80fd\u5b58\u5728\u7684\u76f8\u5173\u9519\u8bef\u3002\u5c3d\u7ba1\u5df2\u6709\u6570\u503c\u7814\u7a76\u4e86\u76f8\u5173\u9519\u8bef\u7684\u9608\u503c\uff0c\u4f46\u4ec5\u80fd\u63d0\u4f9b\u4e0b\u754c\u3002\u56e0\u6b64\uff0c\u6709\u5fc5\u8981\u7814\u7a76\u5305\u542b\u76f8\u5173\u9519\u8bef\u5728\u5185\u7684\u66f4\u73b0\u5b9e\u7684\u566a\u58f0\u6a21\u578b\uff0c\u4ee5\u5bfb\u6c42\u66f4\u9ad8\u7684\u9519\u8bef\u9608\u503c\u3002", "method": "\u672c\u7814\u7a76\u5efa\u7acb\u4e86\u4e00\u4e2a\u9519\u8bef-\u8fb9\u6620\u5c04\uff0c\u5c06\u91cf\u5b50\u8bef\u5dee\u6821\u6b63\u95ee\u9898\u8f6c\u5316\u4e3a\u4e00\u4e2a\u65b9\u5f62-\u516b\u8fb9\u5f62\u968f\u673a\u952e\u4f0a\u8f9b\u6a21\u578b\u3002\u5229\u7528\u8be5\u6620\u5c04\uff0c\u7814\u7a76\u4eba\u5458\u63a8\u5bfc\u51fa\u4e86\u5728\u7ed3\u5408\u4e86\u72ec\u7acb\u5355\u91cf\u5b50\u6bd4\u7279\u9519\u8bef\u548c\u6700\u8fd1\u90bb\u6570\u636e\u91cf\u5b50\u6bd4\u7279\u4e4b\u95f4\u76f8\u5173\u9519\u8bef\u7684\u566a\u58f0\u6a21\u578b\u4e0b\u7684\u7cbe\u786e\u9519\u8bef\u9608\u503c\u3002\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u4efb\u610f\u6bd4\u4f8b\u7684\u76f8\u5173\u9519\u8bef\u4e0ei.i.d.\u9519\u8bef\u7684\u60c5\u51b5\u3002", "result": "\u901a\u8fc7\u9519\u8bef-\u8fb9\u6620\u5c04\u548c\u4f0a\u8f9b\u6a21\u578b\u5206\u6790\uff0c\u672c\u7814\u7a76\u5f97\u51fa\u4e86\u8868\u9762\u7801\u5728\u7ed3\u5408\u4e86\u72ec\u7acb\u5355\u91cf\u5b50\u6bd4\u7279\u9519\u8bef\u548c\u6700\u8fd1\u90bb\u6570\u636e\u91cf\u5b50\u6bd4\u7279\u76f8\u5173\u9519\u8bef\u7684\u566a\u58f0\u6a21\u578b\u4e0b\u7684\u7cbe\u786e\u9519\u8bef\u9608\u503c\u3002\u8be5\u9608\u503c\u540c\u65f6\u662f\u7406\u8bba\u4e0a\u7684\u4e0a\u754c\u548c\u53ef\u5b9e\u73b0\u7684\u503c\uff0c\u8fd9\u610f\u5473\u7740\u5b83\u6bd4\u73b0\u6709\u7684\u6570\u503c\u7814\u7a76\u7ed3\u679c\u66f4\u9ad8\uff0c\u5e76\u4e14\u662f\u539f\u5219\u4e0a\u53ef\u8fbe\u5230\u7684\u6700\u9ad8\u503c\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u9519\u8bef-\u8fb9\u6620\u5c04\u548c\u5206\u6790\u65b9\u6cd5\uff0c\u6210\u529f\u5730\u5728\u8003\u8651\u4e86\u76f8\u5173\u9519\u8bef\u7684\u60c5\u51b5\u4e0b\uff0c\u5f97\u51fa\u4e86\u8868\u9762\u7801\u7684\u7cbe\u786e\u9519\u8bef\u9608\u503c\u3002\u8fd9\u4e00\u7ed3\u679c\u4e0d\u4ec5\u6539\u8fdb\u4e86\u73b0\u6709\u7684\u7406\u8bba\u548c\u6570\u503c\u7ed3\u679c\uff0c\u800c\u4e14\u63d0\u4f9b\u4e86\u8fc4\u4eca\u4e3a\u6b62\u8868\u9762\u7801\u53ef\u5b9e\u73b0\u7684\u6700\u9ad8\u9519\u8bef\u9608\u503c\uff0c\u4e3a\u6784\u5efa\u66f4\u5f3a\u5927\u7684\u5bb9\u9519\u91cf\u5b50\u8ba1\u7b97\u673a\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u3002"}}
{"id": "2510.24614", "categories": ["cs.LG", "cs.CE", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.24614", "abs": "https://arxiv.org/abs/2510.24614", "authors": ["James Josep Perry", "Pablo Garcia-Conde Ortiz", "George Konstantinou", "Cornelie Vergouwen", "Edlyn Santha Kumaran", "Morteza Moradi"], "title": "Semi-supervised and unsupervised learning for health indicator extraction from guided waves in aerospace composite structures", "comment": null, "summary": "Health indicators (HIs) are central to diagnosing and prognosing the\ncondition of aerospace composite structures, enabling efficient maintenance and\noperational safety. However, extracting reliable HIs remains challenging due to\nvariability in material properties, stochastic damage evolution, and diverse\ndamage modes. Manufacturing defects (e.g., disbonds) and in-service incidents\n(e.g., bird strikes) further complicate this process. This study presents a\ncomprehensive data-driven framework that learns HIs via two learning approaches\nintegrated with multi-domain signal processing. Because ground-truth HIs are\nunavailable, a semi-supervised and an unsupervised approach are proposed: (i) a\ndiversity deep semi-supervised anomaly detection (Diversity-DeepSAD) approach\naugmented with continuous auxiliary labels used as hypothetical damage proxies,\nwhich overcomes the limitation of prior binary labels that only distinguish\nhealthy and failed states while neglecting intermediate degradation, and (ii) a\ndegradation-trend-constrained variational autoencoder (DTC-VAE), in which the\nmonotonicity criterion is embedded via an explicit trend constraint. Guided\nwaves with multiple excitation frequencies are used to monitor single-stiffener\ncomposite structures under fatigue loading. Time, frequency, and time-frequency\nrepresentations are explored, and per-frequency HIs are fused via unsupervised\nensemble learning to mitigate frequency dependence and reduce variance. Using\nfast Fourier transform features, the augmented Diversity-DeepSAD model achieved\n81.6% performance, while DTC-VAE delivered the most consistent HIs with 92.3%\nperformance, outperforming existing baselines.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2510.24161", "categories": ["cs.AI", "cs.MM", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24161", "abs": "https://arxiv.org/abs/2510.24161", "authors": ["Wentao Tan", "Bowen Wang", "Heng Zhi", "Chenyu Liu", "Zhe Li", "Jian Liu", "Zengrong Lin", "Yukun Dai", "Yipeng Chen", "Wenjie Yang", "Enci Xie", "Hao Xue", "Baixu Ji", "Chen Xu", "Zhibin Wang", "Tianshi Wang", "Lei Zhu", "Heng Tao Shen"], "title": "BLM$_1$: A Boundless Large Model for Cross-Space, Cross-Task, and Cross-Embodiment Learning", "comment": null, "summary": "Multimodal large language models (MLLMs) have advanced vision-language\nreasoning and are increasingly deployed in embodied agents. However,\nsignificant limitations remain: MLLMs generalize poorly across digital-physical\nspaces and embodiments; vision-language-action models (VLAs) produce low-level\nactions yet lack robust high-level embodied reasoning; and most embodied large\nlanguage models (ELLMs) are constrained to digital-space with poor\ngeneralization to the physical world. Thus, unified models that operate\nseamlessly across digital and physical spaces while generalizing across\nembodiments and tasks remain absent. We introduce the \\textbf{Boundless Large\nModel (BLM$_1$)}, a multimodal spatial foundation model that preserves\ninstruction following and reasoning, incorporates embodied knowledge, and\nsupports robust cross-embodiment control. BLM$_1$ integrates three key\ncapabilities -- \\textit{cross-space transfer, cross-task learning, and\ncross-embodiment generalization} -- via a two-stage training paradigm. Stage I\ninjects embodied knowledge into the MLLM through curated digital corpora while\nmaintaining language competence. Stage II trains a policy module through an\nintent-bridging interface that extracts high-level semantics from the MLLM to\nguide control, without fine-tuning the MLLM backbone. This process is supported\nby a self-collected cross-embodiment demonstration suite spanning four robot\nembodiments and six progressively challenging tasks. Evaluations across digital\nand physical benchmarks show that a single BLM$_1$ instance outperforms four\nmodel families -- MLLMs, ELLMs, VLAs, and GMLMs -- achieving\n$\\sim\\!\\textbf{6%}$ gains in digital tasks and $\\sim\\!\\textbf{3%}$ in physical\ntasks.", "AI": {"tldr": "Boundless Large Model (BLM1) \u662f\u4e00\u79cd\u7edf\u4e00\u7684\u591a\u6a21\u6001\u7a7a\u95f4\u57fa\u7840\u6a21\u578b\uff0c\u53ef\u5728\u6570\u5b57\u548c\u7269\u7406\u7a7a\u95f4\u4e2d\u65e0\u7f1d\u64cd\u4f5c\uff0c\u5e76\u5b9e\u73b0\u8de8\u5177\u8eab\u3001\u8de8\u4efb\u52a1\u7684\u6cdb\u5316\u3002", "motivation": "\u73b0\u6709\u7684 MLLMs\u3001VLAs \u548c ELLMs \u5728\u6570\u5b57-\u7269\u7406\u7a7a\u95f4\u6cdb\u5316\u3001\u9ad8\u5c42\u7ea7\u5177\u8eab\u63a8\u7406\u4ee5\u53ca\u8de8\u5177\u8eab\u63a7\u5236\u65b9\u9762\u5b58\u5728\u663e\u8457\u5c40\u9650\u6027\u3002\u56e0\u6b64\uff0c\u9700\u8981\u80fd\u591f\u7edf\u4e00\u5904\u7406\u6570\u5b57\u548c\u7269\u7406\u7a7a\u95f4\uff0c\u5e76\u5b9e\u73b0\u8de8\u5177\u8eab\u548c\u8de8\u4efb\u52a1\u6cdb\u5316\u7684\u6a21\u578b\u3002", "method": "BLM1 \u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u8303\u5f0f\uff1a\u7b2c\u4e00\u9636\u6bb5\uff0c\u901a\u8fc7\u7cbe\u9009\u7684\u6570\u5b57\u8bed\u6599\u5e93\u5c06\u5177\u8eab\u77e5\u8bc6\u6ce8\u5165 MLLM\uff0c\u540c\u65f6\u4fdd\u6301\u8bed\u8a00\u80fd\u529b\uff1b\u7b2c\u4e8c\u9636\u6bb5\uff0c\u901a\u8fc7\u610f\u56fe\u6865\u63a5\u63a5\u53e3\u8bad\u7ec3\u7b56\u7565\u6a21\u5757\uff0c\u4ece MLLM \u63d0\u53d6\u9ad8\u5c42\u7ea7\u8bed\u4e49\u4ee5\u6307\u5bfc\u63a7\u5236\uff0c\u800c\u4e0d\u5bf9 MLLM \u4e3b\u5e72\u8fdb\u884c\u5fae\u8c03\u3002\u8be5\u8fc7\u7a0b\u5f97\u5230\u4e86\u4e00\u4e2a\u5305\u542b\u56db\u79cd\u673a\u5668\u4eba\u5177\u8eab\u548c\u516d\u4e2a\u4e0d\u540c\u96be\u5ea6\u4efb\u52a1\u7684\u8de8\u5177\u8eab\u6f14\u793a\u5957\u4ef6\u7684\u652f\u6301\u3002", "result": "\u5728\u6570\u5b57\u548c\u7269\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u5355\u4e2a BLM1 \u5b9e\u4f8b\u7684\u6027\u80fd\u4f18\u4e8e MLLMs\u3001ELLMs\u3001VLAs \u548c GMLMs \u56db\u79cd\u6a21\u578b\u7cfb\u5217\uff0c\u5728\u6570\u5b57\u4efb\u52a1\u4e0a\u63d0\u5347\u7ea6 6%\uff0c\u5728\u7269\u7406\u4efb\u52a1\u4e0a\u63d0\u5347\u7ea6 3%\u3002", "conclusion": "BLM1 \u6210\u529f\u5730\u5b9e\u73b0\u4e86\u8de8\u7a7a\u95f4\u8fc1\u79fb\u3001\u8de8\u4efb\u52a1\u5b66\u4e60\u548c\u8de8\u5177\u8eab\u6cdb\u5316\uff0c\u514b\u670d\u4e86\u73b0\u6709\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u6570\u5b57\u548c\u7269\u7406\u73af\u5883\u4e2d\u90fd\u53d6\u5f97\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2510.24081", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24081", "abs": "https://arxiv.org/abs/2510.24081", "authors": ["Tyler A. Chang", "Catherine Arnett", "Abdelrahman Eldesokey", "Abdelrahman Sadallah", "Abeer Kashar", "Abolade Daud", "Abosede Grace Olanihun", "Adamu Labaran Mohammed", "Adeyemi Praise", "Adhikarinayum Meerajita Sharma", "Aditi Gupta", "Afitab Iyigun", "Afonso Simpl\u00edcio", "Ahmed Essouaied", "Aicha Chorana", "Akhil Eppa", "Akintunde Oladipo", "Akshay Ramesh", "Aleksei Dorkin", "Alfred Malengo Kondoro", "Alham Fikri Aji", "Ali Eren \u00c7etinta\u015f", "Allan Hanbury", "Alou Dembele", "Alp Niksarli", "\u00c1lvaro Arroyo", "Amin Bajand", "Amol Khanna", "Ana Chkhaidze", "Ana Condez", "Andiswa Mkhonto", "Andrew Hoblitzell", "Andrew Tran", "Angelos Poulis", "Anirban Majumder", "Anna Vacalopoulou", "Annette Kuuipolani Kanahele Wong", "Annika Simonsen", "Anton Kovalev", "Ashvanth. S", "Ayodeji Joseph Lana", "Barkin Kinay", "Bashar Alhafni", "Benedict Cibalinda Busole", "Bernard Ghanem", "Bharti Nathani", "Biljana Stojanovska \u0110uri\u0107", "Bola Agbonile", "Bragi Bergsson", "Bruce Torres Fischer", "Burak Tutar", "Burcu Alaku\u015f \u00c7\u0131nar", "Cade J. Kanoniakapueo Kane", "Can Udomcharoenchaikit", "Catherine Arnett", "Chadi Helwe", "Chaithra Reddy Nerella", "Chen Cecilia Liu", "Chiamaka Glory Nwokolo", "Cristina Espa\u00f1a-Bonet", "Cynthia Amol", "DaeYeop Lee", "Dana Arad", "Daniil Dzenhaliou", "Daria Pugacheva", "Dasol Choi", "Daud Abolade", "David Liu", "David Semedo", "Deborah Popoola", "Deividas Mataciunas", "Delphine Nyaboke", "Dhyuthy Krishna Kumar", "Diogo Gl\u00f3ria-Silva", "Diogo Tavares", "Divyanshu Goyal", "DongGeon Lee", "Ebele Nwamaka Anajemba", "Egonu Ngozi Grace", "Elena Mickel", "Elena Tutubalina", "Elias Herranen", "Emile Anand", "Emmanuel Habumuremyi", "Emuobonuvie Maria Ajiboye", "Eryawan Presma Yulianrifat", "Esther Adenuga", "Ewa Rudnicka", "Faith Olabisi Itiola", "Faran Taimoor Butt", "Fathima Thekkekara", "Fatima Haouari", "Filbert Aurelian Tjiaranata", "Firas Laakom", "Francesca Grasso", "Francesco Orabona", "Francesco Periti", "Gbenga Kayode Solomon", "Gia Nghia Ngo", "Gloria Udhehdhe-oze", "Gon\u00e7alo Martins", "Gopi Naga Sai Ram Challagolla", "Guijin Son", "Gulnaz Abdykadyrova", "Hafsteinn Einarsson", "Hai Hu", "Hamidreza Saffari", "Hamza Zaidi", "Haopeng Zhang", "Harethah Abu Shairah", "Harry Vuong", "Hele-Andra Kuulmets", "Houda Bouamor", "Hwanjo Yu", "Iben Nyholm Debess", "\u0130brahim Ethem Deveci", "Ikhlasul Akmal Hanif", "Ikhyun Cho", "In\u00eas Calvo", "In\u00eas Vieira", "Isaac Manzi", "Ismail Daud", "Itay Itzhak", "Iuliia", "Alekseenko", "Ivan Belashkin", "Ivan Spada", "Ivan Zhelyazkov", "Jacob Brinton", "Jafar Isbarov", "Jaka \u010cibej", "Jan \u010cuhel", "Jan Koco\u0144", "Jauza Akbar Krito", "Jebish Purbey", "Jennifer Mickel", "Jennifer Za", "Jenny Kunz", "Jihae Jeong", "Jimena Tena D\u00e1valos", "Jinu Lee", "Jo\u00e3o Magalh\u00e3es", "John Yi", "Jongin Kim", "Joseph Chataignon", "Joseph Marvin Imperial", "Jubeerathan Thevakumar", "Judith Land", "Junchen Jiang", "Jungwhan Kim", "Kairit Sirts", "Kamesh R", "Kamesh V", "Kanda Patrick Tshinu", "K\u00e4triin Kukk", "Kaustubh Ponkshe", "Kavsar Huseynova", "Ke He", "Kelly Buchanan", "Kengatharaiyer Sarveswaran", "Kerem Zaman", "Khalil Mrini", "Kian Kyars", "Krister Kruusmaa", "Kusum Chouhan", "Lainitha Krishnakumar", "Laura Castro S\u00e1nchez", "Laura Porrino Moscoso", "Leshem Choshen", "Levent Sencan", "Lilja \u00d8vrelid", "Lisa Alazraki", "Lovina Ehimen-Ugbede", "Luheerathan Thevakumar", "Luxshan Thavarasa", "Mahnoor Malik", "Mamadou K. Keita", "Mansi Jangid", "Marco De Santis", "Marcos Garc\u00eda", "Marek Suppa", "Mariam D'Ciofalo", "Marii Ojastu", "Maryam Sikander", "Mausami Narayan", "Maximos Skandalis", "Mehak Mehak", "Mehmet \u0130lteri\u015f Bozkurt", "Melaku Bayu Workie", "Menan Velayuthan", "Michael Leventhal", "Micha\u0142 Marci\u0144czuk", "Mirna Poto\u010dnjak", "Mohammadamin Shafiei", "Mridul Sharma", "Mrityunjaya Indoria", "Muhammad Ravi Shulthan Habibi", "Murat Koli\u0107", "Nada Galant", "Naphat Permpredanun", "Narada Maugin", "Nicholas Kluge Corr\u00eaa", "Nikola Ljube\u0161i\u0107", "Nirmal Thomas", "Nisansa de Silva", "Nisheeth Joshi", "Nitish Ponkshe", "Nizar Habash", "Nneoma C. Udeze", "Noel Thomas", "No\u00e9mi Ligeti-Nagy", "Nouhoum Coulibaly", "Nsengiyumva Faustin", "Odunayo Kareemat Buliaminu", "Odunayo Ogundepo", "Oghojafor Godswill Fejiro", "Ogundipe Blessing Funmilola", "Okechukwu God'spraise", "Olanrewaju Samuel", "Olaoye Deborah Oluwaseun", "Olasoji Akindejoye", "Olga Popova", "Olga Snissarenko", "Onyinye Anulika Chiemezie", "Orkun Kinay", "Osman Tursun", "Owoeye Tobiloba Moses", "Oyelade Oluwafemi Joshua", "Oyesanmi Fiyinfoluwa", "Pablo Gamallo", "Pablo Rodr\u00edguez Fern\u00e1ndez", "Palak Arora", "Pedro Valente", "Peter Rupnik", "Philip Oghenesuowho Ekiugbo", "Pramit Sahoo", "Prokopis Prokopidis", "Pua Niau-Puhipau", "Quadri Yahya", "Rachele Mignone", "Raghav Singhal", "Ram Mohan Rao Kadiyala", "Raphael Merx", "Rapheal Afolayan", "Ratnavel Rajalakshmi", "Rishav Ghosh", "Romina Oji", "Ron Kekeha Solis", "Rui Guerra", "Rushikesh Zawar", "Sa'ad Nasir Bashir", "Saeed Alzaabi", "Sahil Sandeep", "Sai Pavan Batchu", "SaiSandeep Kantareddy", "Salsabila Zahirah Pranida", "Sam Buchanan", "Samuel Rutunda", "Sander Land", "Sarah Sulollari", "Sardar Ali", "Saroj Sapkota", "Saulius Tautvaisas", "Sayambhu Sen", "Sayantani Banerjee", "Sebastien Diarra", "SenthilNathan. M", "Sewoong Lee", "Shaan Shah", "Shankar Venkitachalam", "Sharifa Djurabaeva", "Sharon Ibejih", "Shivanya Shomir Dutta", "Siddhant Gupta", "Silvia Paniagua Su\u00e1rez", "Sina Ahmadi", "Sivasuthan Sukumar", "Siyuan Song", "Snegha A.", "Sokratis Sofianopoulos", "Sona Elza Simon", "Sonja Ben\u010dina", "Sophie Gvasalia", "Sphurti Kirit More", "Spyros Dragazis", "Stephan P. Kaufhold", "Suba. S", "Sultan AlRashed", "Surangika Ranathunga", "Taiga Someya", "Taja Kuzman Punger\u0161ek", "Tal Haklay", "Tasi'u Jibril", "Tatsuya Aoyama", "Tea Abashidze", "Terenz Jomar Dela Cruz", "Terra Blevins", "Themistoklis Nikas", "Theresa Dora Idoko", "Thu Mai Do", "Tilek Chubakov", "Tommaso Gargiani", "Uma Rathore", "Uni Johannesen", "Uwuma Doris Ugwu", "Vallerie Alexandra Putra", "Vanya Bannihatti Kumar", "Varsha Jeyarajalingam", "Varvara Arzt", "Vasudevan Nedumpozhimana", "Viktoria Ondrejova", "Viktoryia Horbik", "Vishnu Vardhan Reddy Kummitha", "Vuk Dini\u0107", "Walelign Tewabe Sewunetie", "Winston Wu", "Xiaojing Zhao", "Yacouba Diarra", "Yaniv Nikankin", "Yash Mathur", "Yixi Chen", "Yiyuan Li", "Yolanda Xavier", "Yonatan Belinkov", "Yusuf Ismail Abayomi", "Zaid Alyafeai", "Zhengyang Shan", "Zhi Rui Tam", "Zilu Tang", "Zuzana Nadova", "Baber Abbasi", "Stella Biderman", "David Stap", "Duygu Ataman", "Fabian Schmidt", "Hila Gonen", "Jiayi Wang", "David Ifeoluwa Adelani"], "title": "Global PIQA: Evaluating Physical Commonsense Reasoning Across 100+ Languages and Cultures", "comment": "Preprint", "summary": "To date, there exist almost no culturally-specific evaluation benchmarks for\nlarge language models (LLMs) that cover a large number of languages and\ncultures. In this paper, we present Global PIQA, a participatory commonsense\nreasoning benchmark for over 100 languages, constructed by hand by 335\nresearchers from 65 countries around the world. The 116 language varieties in\nGlobal PIQA cover five continents, 14 language families, and 23 writing\nsystems. In the non-parallel split of Global PIQA, over 50% of examples\nreference local foods, customs, traditions, or other culturally-specific\nelements. We find that state-of-the-art LLMs perform well on Global PIQA in\naggregate, but they exhibit weaker performance in lower-resource languages (up\nto a 37% accuracy gap, despite random chance at 50%). Open models generally\nperform worse than proprietary models. Global PIQA highlights that in many\nlanguages and cultures, everyday knowledge remains an area for improvement,\nalongside more widely-discussed capabilities such as complex reasoning and\nexpert knowledge. Beyond its uses for LLM evaluation, we hope that Global PIQA\nprovides a glimpse into the wide diversity of cultures in which human language\nis embedded.", "AI": {"tldr": "\u5168\u5c40PIQA\u662f\u4e00\u4e2a\u5305\u542b100\u591a\u79cd\u8bed\u8a00\u7684\u53c2\u4e0e\u5f0f\u5e38\u8bc6\u63a8\u7406\u57fa\u51c6\uff0c\u7531\u6765\u81ea65\u4e2a\u56fd\u5bb6\u7684335\u540d\u7814\u7a76\u4eba\u5458\u624b\u5de5\u6784\u5efa\uff0c\u65e8\u5728\u89e3\u51b3\u5f53\u524d\u7f3a\u4e4f\u8de8\u6587\u5316LLM\u8bc4\u4f30\u57fa\u51c6\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u9488\u5bf9\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u3001\u6db5\u76d6\u5e7f\u6cdb\u8bed\u8a00\u548c\u6587\u5316\u7684\u3001\u5177\u6709\u6587\u5316\u7279\u5f02\u6027\u7684\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "\u901a\u8fc7\u5168\u7403335\u540d\u7814\u7a76\u4eba\u5458\u624b\u5de5\u6784\u5efa\uff0c\u8986\u76d6100\u591a\u79cd\u8bed\u8a00\uff08116\u4e2a\u8bed\u8a00\u53d8\u4f53\uff09\uff0c\u6d89\u53ca\u4e94\u5927\u6d32\u300114\u4e2a\u8bed\u7cfb\u548c23\u79cd\u4e66\u5199\u7cfb\u7edf\u3002\u5176\u4e2d\uff0c\u975e\u5e76\u884c\u6570\u636e\u96c6\u4e2d\u8d85\u8fc750%\u7684\u793a\u4f8b\u5305\u542b\u672c\u5730\u98df\u7269\u3001\u4e60\u4fd7\u3001\u4f20\u7edf\u6216\u5176\u4ed6\u6587\u5316\u7279\u5f02\u6027\u5143\u7d20\u3002", "result": "\u5c3d\u7ba1\u6700\u5148\u8fdb\u7684LLMs\u5728\u5168\u5c40PIQA\u4e0a\u603b\u4f53\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e0a\u7684\u8868\u73b0\u8f83\u5f31\uff0c\u51c6\u786e\u7387\u5dee\u8ddd\u53ef\u8fbe37%\uff08\u800c\u968f\u673a\u731c\u6d4b\u4e3a50%\uff09\u3002\u5f00\u6e90\u6a21\u578b\u901a\u5e38\u8868\u73b0\u4e0d\u5982\u4e13\u6709\u6a21\u578b\u3002", "conclusion": "\u5168\u5c40PIQA\u63ed\u793a\u4e86\u5728\u8bb8\u591a\u8bed\u8a00\u548c\u6587\u5316\u4e2d\uff0c\u65e5\u5e38\u77e5\u8bc6\u4ecd\u7136\u662fLLMs\u9700\u8981\u6539\u8fdb\u7684\u9886\u57df\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u590d\u6742\u7684\u63a8\u7406\u548c\u4e13\u5bb6\u77e5\u8bc6\u3002\u8be5\u57fa\u51c6\u4e0d\u4ec5\u53ef\u7528\u4e8eLLM\u8bc4\u4f30\uff0c\u4e5f\u5c55\u793a\u4e86\u4eba\u7c7b\u8bed\u8a00\u6240\u5d4c\u5165\u7684\u5e7f\u6cdb\u591a\u6837\u7684\u6587\u5316\u3002"}}
{"id": "2510.24533", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24533", "abs": "https://arxiv.org/abs/2510.24533", "authors": ["Yuan Shen", "Yuze Hong", "Guangyang Zeng", "Tengfei Zhang", "Pui Yi Chui", "Ziyang Hong", "Junfeng Wu"], "title": "GeVI-SLAM: Gravity-Enhanced Stereo Visua Inertial SLAM for Underwater Robots", "comment": null, "summary": "Accurate visual inertial simultaneous localization and mapping (VI SLAM) for\nunderwater robots remains a significant challenge due to frequent visual\ndegeneracy and insufficient inertial measurement unit (IMU) motion excitation.\nIn this paper, we present GeVI-SLAM, a gravity-enhanced stereo VI SLAM system\ndesigned to address these issues. By leveraging the stereo camera's direct\ndepth estimation ability, we eliminate the need to estimate scale during IMU\ninitialization, enabling stable operation even under low acceleration dynamics.\nWith precise gravity initialization, we decouple the pitch and roll from the\npose estimation and solve a 4 degrees of freedom (DOF) Perspective-n-Point\n(PnP) problem for pose tracking. This allows the use of a minimal 3-point\nsolver, which significantly reduces computational time to reject outliers\nwithin a Random Sample Consensus framework. We further propose a\nbias-eliminated 4-DOF PnP estimator with provable consistency, ensuring the\nrelative pose converges to the true value as the feature number increases. To\nhandle dynamic motion, we refine the full 6-DOF pose while jointly estimating\nthe IMU covariance, enabling adaptive weighting of the gravity prior. Extensive\nexperiments on simulated and real-world data demonstrate that GeVI-SLAM\nachieves higher accuracy and greater stability compared to state-of-the-art\nmethods.", "AI": {"tldr": "GeVI-SLAM\u662f\u4e00\u4e2a\u5229\u7528\u7acb\u4f53\u76f8\u673a\u548cIMU\u8fdb\u884c\u89c6\u89c9\u60ef\u6027SLAM\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u5229\u7528\u91cd\u529b\u4fe1\u606f\u548c\u6539\u8fdb\u7684PnP\u6c42\u89e3\u5668\u6765\u63d0\u9ad8\u6c34\u4e0b\u673a\u5668\u4eba\u7684\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u6c34\u4e0b\u673a\u5668\u4eba\u89c6\u89c9\u60ef\u6027SLAM\uff08VI SLAM\uff09\u9762\u4e34\u89c6\u89c9\u9000\u5316\u548cIMU\u8fd0\u52a8\u6fc0\u52b1\u4e0d\u8db3\u7684\u6311\u6218\uff0c\u5bfc\u81f4\u5b9a\u4f4d\u7cbe\u5ea6\u4e0d\u9ad8\u3002", "method": "GeVI-SLAM\u7cfb\u7edf\u5229\u7528\u7acb\u4f53\u76f8\u673a\u7684\u6df1\u5ea6\u4f30\u8ba1\u80fd\u529b\uff0c\u65e0\u9700IMU\u521d\u59cb\u5316\u65f6\u7684\u5c3a\u5ea6\u4f30\u8ba1\uff0c\u5b9e\u73b0\u4f4e\u52a0\u901f\u5ea6\u4e0b\u7684\u7a33\u5b9a\u8fd0\u884c\u3002\u901a\u8fc7\u7cbe\u786e\u7684\u91cd\u529b\u521d\u59cb\u5316\uff0c\u89e3\u8026\u4e86\u4fef\u4ef0\u89d2\u548c\u7ffb\u6eda\u89d2\uff0c\u5e76\u89e3\u51b3\u4e864\u81ea\u7531\u5ea6PnP\u95ee\u9898\uff0c\u4f7f\u75283\u70b9\u6c42\u89e3\u5668\u51cf\u5c11\u8ba1\u7b97\u91cf\uff0c\u5e76\u63d0\u51fa\u504f\u5dee\u6d88\u9664\u76844\u81ea\u7531\u5ea6PnP\u4f30\u8ba1\u5668\u3002\u6b64\u5916\uff0c\u8fd8\u901a\u8fc7\u8054\u5408\u4f30\u8ba1IMU\u534f\u65b9\u5dee\u6765\u4f18\u53166\u81ea\u7531\u5ea6\u59ff\u6001\uff0c\u5e76\u81ea\u9002\u5e94\u5730\u8c03\u6574\u91cd\u529b\u5148\u9a8c\u6743\u91cd\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cGeVI-SLAM\u7684\u7cbe\u5ea6\u548c\u7a33\u5b9a\u6027\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "GeVI-SLAM\u901a\u8fc7\u91cd\u529b\u589e\u5f3a\u7684\u7acb\u4f53\u89c6\u89c9\u60ef\u6027SLAM\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6c34\u4e0b\u673a\u5668\u4ebaVI SLAM\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u63d0\u9ad8\u4e86\u5b9a\u4f4d\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.24105", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24105", "abs": "https://arxiv.org/abs/2510.24105", "authors": ["Shufan Shen", "Zhaobo Qi", "Junshu Sun", "Qingming Huang", "Qi Tian", "Shuhui Wang"], "title": "Enhancing Pre-trained Representation Classifiability can Boost its Interpretability", "comment": "ICLR 2025 (Spotlight)", "summary": "The visual representation of a pre-trained model prioritizes the\nclassifiability on downstream tasks, while the widespread applications for\npre-trained visual models have posed new requirements for representation\ninterpretability. However, it remains unclear whether the pre-trained\nrepresentations can achieve high interpretability and classifiability\nsimultaneously. To answer this question, we quantify the representation\ninterpretability by leveraging its correlation with the ratio of interpretable\nsemantics within the representations. Given the pre-trained representations,\nonly the interpretable semantics can be captured by interpretations, whereas\nthe uninterpretable part leads to information loss. Based on this fact, we\npropose the Inherent Interpretability Score (IIS) that evaluates the\ninformation loss, measures the ratio of interpretable semantics, and quantifies\nthe representation interpretability. In the evaluation of the representation\ninterpretability with different classifiability, we surprisingly discover that\nthe interpretability and classifiability are positively correlated, i.e.,\nrepresentations with higher classifiability provide more interpretable\nsemantics that can be captured in the interpretations. This observation further\nsupports two benefits to the pre-trained representations. First, the\nclassifiability of representations can be further improved by fine-tuning with\ninterpretability maximization. Second, with the classifiability improvement for\nthe representations, we obtain predictions based on their interpretations with\nless accuracy degradation. The discovered positive correlation and\ncorresponding applications show that practitioners can unify the improvements\nin interpretability and classifiability for pre-trained vision models. Codes\nare available at https://github.com/ssfgunner/IIS.", "AI": {"tldr": "\u9884\u8bad\u7ec3\u89c6\u89c9\u6a21\u578b\u7684\u8868\u793a\u80fd\u529b\u548c\u53ef\u89e3\u91ca\u6027\u662f\u6b63\u76f8\u5173\u7684\uff0c\u53ef\u4ee5\u901a\u8fc7\u6700\u5927\u5316\u53ef\u89e3\u91ca\u6027\u6765\u540c\u65f6\u63d0\u9ad8\u4e24\u8005\u3002", "motivation": "\u8bc4\u4f30\u9884\u8bad\u7ec3\u89c6\u89c9\u6a21\u578b\u5728\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u5206\u7c7b\u6027\u4e4b\u95f4\u662f\u5426\u5b58\u5728\u6743\u8861\uff0c\u5e76\u63a2\u7d22\u5982\u4f55\u540c\u65f6\u4f18\u5316\u4e24\u8005\u3002", "method": "\u63d0\u51fa\u4e0d\u4f9d\u8d56\u4e8e\u4e0b\u6e38\u4efb\u52a1\u7684\u56fa\u6709\u53ef\u89e3\u91ca\u6027\u5f97\u5206\uff08IIS\uff09\u6765\u91cf\u5316\u8868\u793a\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u901a\u8fc7\u5206\u6790IIS\u4e0e\u53ef\u5206\u7c7b\u6027\u7684\u76f8\u5173\u6027\u6765\u8bc4\u4f30\u4e24\u8005\u5173\u7cfb\u3002", "result": "\u53d1\u73b0\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u8868\u793a\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u5206\u7c7b\u6027\u4e4b\u95f4\u5b58\u5728\u6b63\u76f8\u5173\u5173\u7cfb\u3002IIS\u5f97\u5206\u8d8a\u9ad8\u7684\u8868\u793a\uff0c\u5176\u53ef\u89e3\u91ca\u6027\u8d8a\u5f3a\uff0c\u540c\u65f6\u53ef\u5206\u7c7b\u6027\u4e5f\u8d8a\u597d\u3002\u901a\u8fc7\u6700\u5927\u5316\u53ef\u89e3\u91ca\u6027\u8fdb\u884c\u5fae\u8c03\uff0c\u53ef\u4ee5\u540c\u65f6\u63d0\u9ad8\u8868\u793a\u7684\u53ef\u5206\u7c7b\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u51cf\u5c11\u57fa\u4e8e\u89e3\u91ca\u8fdb\u884c\u9884\u6d4b\u65f6\u7684\u51c6\u786e\u6027\u635f\u5931\u3002", "conclusion": "\u9884\u8bad\u7ec3\u89c6\u89c9\u6a21\u578b\u7684\u8868\u793a\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u5206\u7c7b\u6027\u53ef\u4ee5\u534f\u540c\u4f18\u5316\uff0cIIS\u63d0\u4f9b\u4e86\u4e00\u79cd\u91cf\u5316\u548c\u63d0\u5347\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u7684\u65b9\u6cd5\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2510.24199", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.24199", "abs": "https://arxiv.org/abs/2510.24199", "authors": ["Loek van Everdingen", "Jaimy Plugge", "Tim Fuchs", "Guido van de Stolpe", "Dalal Benali", "Thijmen de Jong", "Jasper Bijl", "Wim Bosch", "Tjerk Oosterkamp"], "title": "A Sub-kHz Mechanical Resonator Passively Cooled to 6 mK", "comment": "8 pages, 3 figures", "summary": "Fundamental tests of quantum mechanics, such as the generation of\nnon-classical states and tests of wavefunction collapse models, are performed\non increasingly larger size and mass scales. Highly coherent mechanical\nresonators, which also prove invaluable in ultrasensitive microscopy methods,\nare essential tools towards these efforts. Studying these resonators in a\nthermal equilibrium state at millikelvin temperatures provides a promising path\nto increase their coherence time. Here, we passively cool a 700 Hz, massive\n(1.5 ng) mechanical cantilever down to 6.1(4)mK by means of nuclear\ndemagnetization, as confirmed by detecting its thermal motion via a lock-in\nbased detection scheme. At the lowest temperatures the thermal motion of the\nresonator is still clearly distinguishable from the background noise. Our data\nanalysis confirms that at these temperatures the motion is still thermally\ndistributed. These results pave the way for passiveof cooling low-frequency\nresonators to the sub-milllikelvin regime, which would enable new tests of\nquantum mechanics and advances in ultrasensitive force detection.", "AI": {"tldr": "\u4f7f\u7528\u6838\u84b8\u53d1\u51b7\u5374\u673a\u68b0\u8c10\u632f\u5668\u81f3\u6beb\u5f00\u5c14\u6587\u7ea7\u522b\uff0c\u4ee5\u8fdb\u884c\u91cf\u5b50\u529b\u5b66\u548c\u7cbe\u5bc6\u6d4b\u529b\u7684\u5b9e\u9a8c\u3002", "motivation": "\u5728\u8d8a\u6765\u8d8a\u5927\u7684\u5c3a\u5bf8\u548c\u8d28\u91cf\u5c3a\u5ea6\u4e0a\u8fdb\u884c\u91cf\u5b50\u529b\u5b66\u57fa\u672c\u68c0\u9a8c\uff08\u4f8b\u5982\u975e\u7ecf\u5178\u6001\u7684\u4ea7\u751f\u548c\u6ce2\u51fd\u6570\u574d\u7f29\u6a21\u578b\u7684\u68c0\u9a8c\uff09\uff0c\u9700\u8981\u9ad8\u76f8\u5e72\u6027\u7684\u673a\u68b0\u8c10\u632f\u5668\uff0c\u5176\u76f8\u5e72\u65f6\u95f4\u662f\u8fd9\u4e9b\u5b9e\u9a8c\u7684\u5173\u952e\u3002\u5728\u6beb\u5f00\u5c14\u6587\u6e29\u5ea6\u4e0b\u7814\u7a76\u8fd9\u4e9b\u8c10\u632f\u5668\u63d0\u4f9b\u4e86\u4e00\u79cd\u5ef6\u957f\u5176\u76f8\u5e72\u65f6\u95f4\u7684\u53ef\u884c\u9014\u5f84\u3002", "method": "\u4f7f\u7528\u6838\u84b8\u53d1\u5236\u51b7\u6280\u672f\uff0c\u5c06\u4e00\u4e2a700 Hz\u30011.5 ng\u7684\u673a\u68b0\u60ac\u81c2\u6881\u88ab\u52a8\u51b7\u5374\u81f36.1(4) mK\uff0c\u5e76\u901a\u8fc7\u57fa\u4e8e\u9501\u5b9a\u653e\u5927\u5668\u7684\u68c0\u6d4b\u65b9\u6848\u63a2\u6d4b\u5176\u70ed\u8fd0\u52a8\u3002", "result": "\u5728\u6700\u4f4e\u6e29\u5ea6\u4e0b\uff0c\u8c10\u632f\u5668\u7684\u70ed\u8fd0\u52a8\u4ecd\u7136\u6e05\u6670\u53ef\u8fa8\uff0c\u5e76\u4e14\u6570\u636e\u5206\u6790\u8bc1\u5b9e\u5176\u8fd0\u52a8\u4ecd\u7136\u662f\u70ed\u529b\u5b66\u5206\u5e03\u7684\u3002", "conclusion": "\u8be5\u5b9e\u9a8c\u5c55\u793a\u4e86\u5c06\u4f4e\u9891\u8c10\u632f\u5668\u88ab\u52a8\u51b7\u5374\u81f3\u4e9a\u6beb\u5f00\u5c14\u6587\u533a\u57df\u7684\u53ef\u80fd\u6027\uff0c\u8fd9\u5c06\u4e3a\u68c0\u9a8c\u91cf\u5b50\u529b\u5b66\u548c\u63a8\u8fdb\u8d85\u7075\u654f\u529b\u63a2\u6d4b\u5f00\u8f9f\u65b0\u7684\u9014\u5f84\u3002"}}
{"id": "2510.23660", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23660", "abs": "https://arxiv.org/abs/2510.23660", "authors": ["Gazi Tanbhir", "Md. Farhan Shahriyar", "Abdullah Md Raihan Chy"], "title": "Quanvolutional Neural Networks for Pneumonia Detection: An Efficient Quantum-Assisted Feature Extraction Paradigm", "comment": null, "summary": "Pneumonia poses a significant global health challenge, demanding accurate and\ntimely diagnosis. While deep learning, particularly Convolutional Neural\nNetworks (CNNs), has shown promise in medical image analysis for pneumonia\ndetection, CNNs often suffer from high computational costs, limitations in\nfeature representation, and challenges in generalizing from smaller datasets.\nTo address these limitations, we explore the application of Quanvolutional\nNeural Networks (QNNs), leveraging quantum computing for enhanced feature\nextraction. This paper introduces a novel hybrid quantum-classical model for\npneumonia detection using the PneumoniaMNIST dataset. Our approach utilizes a\nquanvolutional layer with a parameterized quantum circuit (PQC) to process 2x2\nimage patches, employing rotational Y-gates for data encoding and entangling\nlayers to generate non-classical feature representations. These\nquantum-extracted features are then fed into a classical neural network for\nclassification. Experimental results demonstrate that the proposed QNN achieves\na higher validation accuracy of 83.33 percent compared to a comparable\nclassical CNN which achieves 73.33 percent. This enhanced convergence and\nsample efficiency highlight the potential of QNNs for medical image analysis,\nparticularly in scenarios with limited labeled data. This research lays the\nfoundation for integrating quantum computing into deep-learning-driven medical\ndiagnostic systems, offering a computationally efficient alternative to\ntraditional approaches.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u91cf\u5b50\u8ba1\u7b97\u548c\u7ecf\u5178\u6df1\u5ea6\u5b66\u4e60\u7684\u6df7\u5408\u6a21\u578b\uff0c\u7528\u4e8e\u80ba\u708e\u68c0\u6d4b\uff0c\u76f8\u6bd4\u4f20\u7edfCNN\uff0c\u5728PneumoniaMNIST\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u66f4\u9ad8\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u4f20\u7edfCNN\u5728\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u7279\u5f81\u8868\u793a\u80fd\u529b\u6709\u9650\u4ee5\u53ca\u5728\u5c0f\u6570\u636e\u96c6\u4e0a\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u672c\u7814\u7a76\u63a2\u7d22\u4e86\u5229\u7528\u91cf\u5b50\u8ba1\u7b97\u7684QNN\u6765\u589e\u5f3a\u7279\u5f81\u63d0\u53d6\u3002", "method": "\u672c\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u6a21\u578b\uff0c\u5229\u7528QNN\u4e2d\u7684quanvolutional\u5c42\u548c\u53c2\u6570\u5316\u91cf\u5b50\u7535\u8def\uff08PQC\uff09\u6765\u5904\u7406\u56fe\u50cf\u5757\uff0c\u5e76\u4f7f\u7528\u65cb\u8f6cY\u95e8\u8fdb\u884c\u6570\u636e\u7f16\u7801\u548c\u7ea0\u7f20\u5c42\u751f\u6210\u975e\u7ecf\u5178\u7279\u5f81\u8868\u793a\uff0c\u7136\u540e\u5c06\u8fd9\u4e9b\u91cf\u5b50\u63d0\u53d6\u7684\u7279\u5f81\u8f93\u5165\u5230\u7ecf\u5178\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u4e0e\u8fbe\u523073.33%\u51c6\u786e\u7387\u7684\u7ecf\u5178CNN\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684QNN\u5728PneumoniaMNIST\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e8683.33%\u7684\u66f4\u9ad8\u9a8c\u8bc1\u51c6\u786e\u7387\uff0c\u5e76\u4e14\u5177\u6709\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u548c\u66f4\u9ad8\u7684\u6837\u672c\u6548\u7387\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0cQNN\u5728\u533b\u5b66\u56fe\u50cf\u5206\u6790\uff0c\u7279\u522b\u662f\u5728\u6807\u8bb0\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u5177\u6709\u8ba1\u7b97\u6548\u7387\u9ad8\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u6f5c\u529b\uff0c\u4e3a\u5c06\u91cf\u5b50\u8ba1\u7b97\u96c6\u6210\u5230\u6df1\u5ea6\u5b66\u4e60\u9a71\u52a8\u7684\u533b\u5b66\u8bca\u65ad\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2510.24166", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24166", "abs": "https://arxiv.org/abs/2510.24166", "authors": ["Xin Yang", "Yuhang Zhang", "Wei Li", "Xin Lin", "Wenbin Zou", "Chen Xu"], "title": "UniPlanner: A Unified Motion Planning Framework for Autonomous Vehicle Decision-Making Systems via Multi-Dataset Integration", "comment": null, "summary": "Motion planning is a critical component of autonomous vehicle decision-making\nsystems, directly determining trajectory safety and driving efficiency. While\ndeep learning approaches have advanced planning capabilities, existing methods\nremain confined to single-dataset training, limiting their robustness in\nplanning.\n  Through systematic analysis, we discover that vehicular trajectory\ndistributions and history-future correlations demonstrate remarkable\nconsistency across different datasets. Based on these findings, we propose\nUniPlanner, the first planning framework designed for multi-dataset integration\nin autonomous vehicle decision-making. UniPlanner achieves unified\ncross-dataset learning through three synergistic innovations.\n  First, the History-Future Trajectory Dictionary Network (HFTDN) aggregates\nhistory-future trajectory pairs from multiple datasets, using historical\ntrajectory similarity to retrieve relevant futures and generate cross-dataset\nplanning guidance.\n  Second, the Gradient-Free Trajectory Mapper (GFTM) learns robust\nhistory-future correlations from multiple datasets, transforming historical\ntrajectories into universal planning priors. Its gradient-free design ensures\nthe introduction of valuable priors while preventing shortcut learning, making\nthe planning knowledge safely transferable. Third, the Sparse-to-Dense (S2D)\nparadigm implements adaptive dropout to selectively suppress planning priors\nduring training for robust learning, while enabling full prior utilization\nduring inference to maximize planning performance.", "AI": {"tldr": "UniPlanner \u662f\u9996\u4e2a\u652f\u6301\u591a\u6570\u636e\u96c6\u96c6\u6210\u3001\u5b9e\u73b0\u8de8\u6570\u636e\u96c6\u5b66\u4e60\u7684\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u6846\u67b6\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u8fd0\u52a8\u89c4\u5212\u65b9\u6cd5\u53d7\u9650\u4e8e\u5355\u4e00\u6570\u636e\u96c6\u8bad\u7ec3\uff0c\u9c81\u68d2\u6027\u4e0d\u8db3\u3002\u901a\u8fc7\u5206\u6790\u53d1\u73b0\uff0c\u4e0d\u540c\u6570\u636e\u96c6\u4e0b\u7684\u8f66\u8f86\u8f68\u8ff9\u5206\u5e03\u548c\u5386\u53f2-\u672a\u6765\u76f8\u5173\u6027\u5177\u6709\u9ad8\u5ea6\u4e00\u81f4\u6027\u3002", "method": "\u63d0\u51fa UniPlanner \u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u521b\u65b0\u70b9\uff1a1. \u5386\u53f2-\u672a\u6765\u8f68\u8ff9\u5b57\u5178\u7f51\u7edc\uff08HFTDN\uff09\u805a\u5408\u591a\u6570\u636e\u96c6\u8f68\u8ff9\u5bf9\uff0c\u5229\u7528\u5386\u53f2\u8f68\u8ff9\u76f8\u4f3c\u6027\u68c0\u7d22\u76f8\u5173\u672a\u6765\u30022. \u65e0\u68af\u5ea6\u8f68\u8ff9\u6620\u5c04\u5668\uff08GFTM\uff09\u4ece\u591a\u6570\u636e\u96c6\u4e2d\u5b66\u4e60\u5386\u53f2-\u672a\u6765\u76f8\u5173\u6027\uff0c\u751f\u6210\u901a\u7528\u7684\u89c4\u5212\u5148\u9a8c\uff0c\u5e76\u9632\u6b62\u6377\u5f84\u5b66\u4e60\u30023. \u7a00\u758f\u5230\u5bc6\u96c6\uff08S2D\uff09\u8303\u5f0f\u5728\u8bad\u7ec3\u65f6\u81ea\u9002\u5e94\u4e22\u5f03\uff0c\u5728\u63a8\u7406\u65f6\u5145\u5206\u5229\u7528\u5148\u9a8c\u3002 ", "result": "UniPlanner \u5b9e\u73b0\u4e86\u7edf\u4e00\u7684\u8de8\u6570\u636e\u96c6\u5b66\u4e60\u3002", "conclusion": "UniPlanner \u901a\u8fc7\u591a\u6570\u636e\u96c6\u96c6\u6210\u548c\u521b\u65b0\u7684\u5b66\u4e60\u673a\u5236\uff0c\u63d0\u9ad8\u4e86\u8fd0\u52a8\u89c4\u5212\u7684\u9c81\u68d2\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2510.24096", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24096", "abs": "https://arxiv.org/abs/2510.24096", "authors": ["Md. Rezuwan Hassan", "Azmol Hossain", "Kanij Fatema", "Rubayet Sabbir Faruque", "Tanmoy Shome", "Ruwad Naswan", "Trina Chakraborty", "Md. Foriduzzaman Zihad", "Tawsif Tashwar Dipto", "Nazia Tasnim", "Nazmuddoha Ansary", "Md. Mehedi Hasan Shawon", "Ahmed Imtiaz Humayun", "Md. Golam Rabiul Alam", "Farig Sadeque", "Asif Sushmit"], "title": "RegSpeech12: A Regional Corpus of Bengali Spontaneous Speech Across Dialects", "comment": "26 pages", "summary": "The Bengali language, spoken extensively across South Asia and among\ndiasporic communities, exhibits considerable dialectal diversity shaped by\ngeography, culture, and history. Phonological and pronunciation-based\nclassifications broadly identify five principal dialect groups: Eastern\nBengali, Manbhumi, Rangpuri, Varendri, and Rarhi. Within Bangladesh, further\ndistinctions emerge through variation in vocabulary, syntax, and morphology, as\nobserved in regions such as Chittagong, Sylhet, Rangpur, Rajshahi, Noakhali,\nand Barishal. Despite this linguistic richness, systematic research on the\ncomputational processing of Bengali dialects remains limited. This study seeks\nto document and analyze the phonetic and morphological properties of these\ndialects while exploring the feasibility of building computational models\nparticularly Automatic Speech Recognition (ASR) systems tailored to regional\nvarieties. Such efforts hold potential for applications in virtual assistants\nand broader language technologies, contributing to both the preservation of\ndialectal diversity and the advancement of inclusive digital tools for\nBengali-speaking communities. The dataset created for this study is released\nfor public use.", "AI": {"tldr": "\u8be5\u7814\u7a76\u65e8\u5728\u5206\u6790\u5b5f\u52a0\u62c9\u8bed\u65b9\u8a00\u7684\u8bed\u97f3\u548c\u5f62\u6001\u7279\u5f81\uff0c\u5e76\u63a2\u7d22\u6784\u5efa\u7279\u5b9a\u4e8e\u533a\u57df\u7684\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u7cfb\u7edf\u7684\u53ef\u884c\u6027\u3002", "motivation": "\u5b5f\u52a0\u62c9\u8bed\u65b9\u8a00\u591a\u6837\u6027\u4e30\u5bcc\uff0c\u4f46\u8ba1\u7b97\u5904\u7406\u7814\u7a76\u6709\u9650\uff0c\u672c\u7814\u7a76\u65e8\u5728\u5f25\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u5e76\u63a8\u52a8\u5305\u5bb9\u6027\u6570\u5b57\u5de5\u5177\u7684\u53d1\u5c55\u3002", "method": "\u7814\u7a76\u5206\u6790\u4e86\u5b5f\u52a0\u62c9\u8bed\u5404\u65b9\u8a00\u7684\u8bed\u97f3\u548c\u5f62\u6001\u7279\u5f81\uff0c\u5e76\u63a2\u7d22\u4e86\u6784\u5efa\u7279\u5b9a\u4e8e\u533a\u57df\u7684\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u7cfb\u7edf\u7684\u53ef\u884c\u6027\u3002", "result": "\u672c\u7814\u7a76\u521b\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u5b5f\u52a0\u62c9\u8bed\u5404\u65b9\u8a00\u6570\u636e\u7684\u516c\u5f00\u6570\u636e\u96c6\uff0c\u5e76\u4e3a\u6784\u5efa\u7279\u5b9a\u65b9\u8a00\u7684ASR\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "conclusion": "\u672c\u7814\u7a76\u5f3a\u8c03\u4e86\u5bf9\u5b5f\u52a0\u62c9\u8bed\u65b9\u8a00\u8fdb\u884c\u8ba1\u7b97\u5904\u7406\u7684\u91cd\u8981\u6027\uff0c\u5e76\u4e3a\u76f8\u5173\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u548c\u5e94\u7528\u63d0\u4f9b\u4e86\u8d44\u6e90\u3002"}}
{"id": "2510.24554", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24554", "abs": "https://arxiv.org/abs/2510.24554", "authors": ["Vignesh Kottayam Viswanathan", "Yifan Bai", "Scott Fredriksson", "Sumeet Satpute", "Christoforos Kanellakis", "George Nikolakopoulos"], "title": "An Adaptive Inspection Planning Approach Towards Routine Monitoring in Uncertain Environments", "comment": "Submitted for ICRA 2026", "summary": "In this work, we present a hierarchical framework designed to support robotic\ninspection under environment uncertainty. By leveraging a known environment\nmodel, existing methods plan and safely track inspection routes to visit points\nof interest. However, discrepancies between the model and actual site\nconditions, caused by either natural or human activities, can alter the surface\nmorphology or introduce path obstructions. To address this challenge, the\nproposed framework divides the inspection task into: (a) generating the initial\nglobal view-plan for region of interests based on a historical map and (b)\nlocal view replanning to adapt to the current morphology of the inspection\nscene. The proposed hierarchy preserves global coverage objectives while\nenabling reactive adaptation to the local surface morphology. This enables the\nlocal autonomy to remain robust against environment uncertainty and complete\nthe inspection tasks. We validate the approach through deployments in\nreal-world subterranean mines using quadrupedal robot.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u73af\u5883\u4e0d\u786e\u5b9a\u6027\u4e0b\u652f\u6301\u673a\u5668\u4eba\u68c0\u67e5\uff0c\u901a\u8fc7\u7ed3\u5408\u5168\u5c40\u89c4\u5212\u548c\u5c40\u90e8\u91cd\u89c4\u5212\u6765\u9002\u5e94\u5b9e\u9645\u5730\u5f62\u53d8\u5316\uff0c\u5e76\u5728\u771f\u5b9e\u7684\u5730\u4e0b\u77ff\u4e95\u73af\u5883\u4e2d\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u4eba\u68c0\u67e5\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u5df2\u77e5\u73af\u5883\u6a21\u578b\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u7531\u4e8e\u81ea\u7136\u6216\u4eba\u4e3a\u6d3b\u52a8\uff0c\u6a21\u578b\u4e0e\u5b9e\u9645\u60c5\u51b5\u53ef\u80fd\u5b58\u5728\u5dee\u5f02\uff0c\u5bfc\u81f4\u8868\u9762\u5f62\u6001\u53d8\u5316\u6216\u8def\u5f84\u963b\u585e\uff0c\u5f71\u54cd\u68c0\u67e5\u4efb\u52a1\u7684\u5b89\u5168\u6027\u4e0e\u5b8c\u6574\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u5206\u5c42\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u4e3b\u8981\u90e8\u5206\uff1a(a) \u57fa\u4e8e\u5386\u53f2\u5730\u56fe\u751f\u6210\u521d\u59cb\u5168\u5c40\u89c6\u70b9\u89c4\u5212\uff0c\u4ee5\u8986\u76d6\u611f\u5174\u8da3\u7684\u533a\u57df\uff1b(b) \u5c40\u90e8\u89c6\u70b9\u91cd\u89c4\u5212\uff0c\u4ee5\u9002\u5e94\u5f53\u524d\u68c0\u67e5\u573a\u666f\u7684\u5c40\u90e8\u5f62\u6001\u3002\u8fd9\u79cd\u5206\u5c42\u7ed3\u6784\u65e2\u80fd\u4fdd\u8bc1\u5168\u5c40\u8986\u76d6\u76ee\u6807\uff0c\u53c8\u80fd\u5bf9\u5c40\u90e8\u5730\u5f62\u53d8\u5316\u505a\u51fa\u5b9e\u65f6\u54cd\u5e94\u3002", "result": "\u901a\u8fc7\u5728\u771f\u5b9e\u7684\u5730\u4e0b\u77ff\u4e95\u73af\u5883\u4e2d\u4f7f\u7528\u56db\u8db3\u673a\u5668\u4eba\u8fdb\u884c\u90e8\u7f72\u548c\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u5728\u673a\u5668\u4eba\u68c0\u67e5\u4efb\u52a1\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u5206\u5c42\u6846\u67b6\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u73af\u5883\u4e0d\u786e\u5b9a\u6027\uff0c\u786e\u4fdd\u673a\u5668\u4eba\u68c0\u67e5\u4efb\u52a1\u5728\u5168\u5c40\u8986\u76d6\u76ee\u6807\u4e0d\u53d8\u7684\u60c5\u51b5\u4e0b\uff0c\u80fd\u591f\u6839\u636e\u5c40\u90e8\u5730\u5f62\u53d8\u5316\u8fdb\u884c\u81ea\u4e3b\u9002\u5e94\uff0c\u4ece\u800c\u6210\u529f\u5b8c\u6210\u68c0\u67e5\u4efb\u52a1\u3002"}}
{"id": "2510.24116", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24116", "abs": "https://arxiv.org/abs/2510.24116", "authors": ["Fengming Yu", "Haiwei Pan", "Kejia Zhang", "Jian Guan", "Haiying Jiang"], "title": "UHKD: A Unified Framework for Heterogeneous Knowledge Distillation via Frequency-Domain Representations", "comment": "14 pages, 4 figures", "summary": "Knowledge distillation (KD) is an effective model compression technique that\ntransfers knowledge from a high-performance teacher to a lightweight student,\nreducing cost while maintaining accuracy. In visual applications, where\nlarge-scale image models are widely used, KD enables efficient deployment.\nHowever, architectural diversity introduces semantic discrepancies that hinder\nthe use of intermediate representations. Most existing KD methods are designed\nfor homogeneous models and degrade in heterogeneous scenarios, especially when\nintermediate features are involved. Prior studies mainly focus on the logits\nspace, making limited use of the semantic information in intermediate layers.\nTo address this limitation, Unified Heterogeneous Knowledge Distillation (UHKD)\nis proposed as a framework that leverages intermediate features in the\nfrequency domain for cross-architecture transfer. Fourier transform is applied\nto capture global feature information, alleviating representational\ndiscrepancies between heterogeneous teacher-student pairs. A Feature\nTransformation Module (FTM) produces compact frequency-domain representations\nof teacher features, while a learnable Feature Alignment Module (FAM) projects\nstudent features and aligns them via multi-level matching. Training is guided\nby a joint objective combining mean squared error on intermediate features with\nKullback-Leibler divergence on logits. Experiments on CIFAR-100 and ImageNet-1K\ndemonstrate gains of 5.59% and 0.83% over the latest method, highlighting UHKD\nas an effective approach for unifying heterogeneous representations and\nenabling efficient utilization of visual knowledge", "AI": {"tldr": "UHKD\u6846\u67b6\u901a\u8fc7\u5728\u9891\u57df\u4e2d\u5229\u7528\u4e2d\u95f4\u7279\u5f81\uff0c\u5b9e\u73b0\u4e86\u8de8\u67b6\u6784\u7684\u77e5\u8bc6\u84b8\u998f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5f02\u6784\u6a21\u578b\u95f4\u7684\u8bed\u4e49\u5dee\u5f02\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8logits\u7a7a\u95f4\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u4e2d\u95f4\u5c42\u7684\u8bed\u4e49\u4fe1\u606f\uff0c\u4e14\u5728\u5f02\u6784\u6a21\u578b\u573a\u666f\u4e0b\u8868\u73b0\u4e0d\u4f73\u3002", "method": "UHKD\u6846\u67b6\u5229\u7528\u5085\u91cc\u53f6\u53d8\u6362\u6355\u6349\u5168\u5c40\u7279\u5f81\u4fe1\u606f\uff0c\u5e76\u901a\u8fc7\u7279\u5f81\u53d8\u6362\u6a21\u5757\uff08FTM\uff09\u548c\u7279\u5f81\u5bf9\u9f50\u6a21\u5757\uff08FAM\uff09\u5b9e\u73b0\u5f02\u6784\u6a21\u578b\u95f4\u4e2d\u95f4\u7279\u5f81\u7684\u8de8\u57df\u8fc1\u79fb\u548c\u5bf9\u9f50\uff0c\u6700\u7ec8\u7ed3\u5408MSE\u548cKL\u6563\u5ea6\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728CIFAR-100\u548cImageNet-1K\u6570\u636e\u96c6\u4e0a\uff0cUHKD\u76f8\u6bd4\u6700\u65b0\u65b9\u6cd5\u5206\u522b\u53d6\u5f97\u4e865.59%\u548c0.83%\u7684\u51c6\u786e\u7387\u63d0\u5347\u3002", "conclusion": "UHKD\u662f\u4e00\u79cd\u6709\u6548\u7684\u5f02\u6784\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\uff0c\u80fd\u591f\u7edf\u4e00\u5f02\u6784\u8868\u793a\u5e76\u5b9e\u73b0\u89c6\u89c9\u77e5\u8bc6\u7684\u9ad8\u6548\u5229\u7528\u3002"}}
{"id": "2510.24218", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.24218", "abs": "https://arxiv.org/abs/2510.24218", "authors": ["Gleb Kalachev", "Pavel Mosharev", "Zuoheng Zou", "Pavel Panteleev", "Man-Hong Yung"], "title": "Pilot-Wave Simulator: Exact Classical Sampling from Ideal and Noisy Quantum Circuits up to Hundreds of Qubits", "comment": null, "summary": "Quantum circuit simulators running on classical computers offer a vital\nplatform for designing, testing, and optimizing quantum algorithms, driving\ninnovation despite limited access to real quantum hardware. However, their\nscalability is inherently constrained by exponential memory and computational\noverhead, which restricts accurate simulation of large-scale quantum circuits\nand often results in approximate output distributions. Here, we propose an\nexact sampling algorithm that integrates tensor network contraction techniques\nwith a Markov process, wherein a classical state evolves according to the local\nstructure of the quantum circuit. As a demonstration, we target the challenge\nof generating samples from ideal and noisy QAOA circuits with up to 476 qubits,\nincorporating both depolarizing and amplitude damping noise models. These\nresults enable further validation of several assumptions and conjectures at a\nscale previously out of reach, significantly expanding the scope of classical\nsimulation in quantum algorithm research.", "AI": {"tldr": "\u7ecf\u5178\u8ba1\u7b97\u673a\u4e0a\u7684\u91cf\u5b50\u7535\u8def\u6a21\u62df\u5668\u867d\u7136\u5728\u91cf\u5b50\u7b97\u6cd5\u8bbe\u8ba1\u3001\u6d4b\u8bd5\u548c\u4f18\u5316\u65b9\u9762\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u9762\u4e34\u5185\u5b58\u548c\u8ba1\u7b97\u5f00\u9500\u7684\u6307\u6570\u7ea7\u589e\u957f\u9650\u5236\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5f20\u91cf\u7f51\u7edc\u6536\u7f29\u548c\u9a6c\u5c14\u53ef\u592b\u8fc7\u7a0b\u7684\u7cbe\u786e\u91c7\u6837\u7b97\u6cd5\uff0c\u80fd\u591f\u6a21\u62df\u591a\u8fbe476\u4e2a\u91cf\u5b50\u6bd4\u7279\u7684QAOA\u7535\u8def\uff08\u5305\u62ec\u566a\u58f0\uff09\uff0c\u89e3\u51b3\u4e86\u4ee5\u5f80\u56e0\u89c4\u6a21\u9650\u5236\u800c\u65e0\u6cd5\u8fdb\u884c\u7684\u9a8c\u8bc1\u95ee\u9898\uff0c\u6269\u5c55\u4e86\u7ecf\u5178\u6a21\u62df\u5728\u91cf\u5b50\u7b97\u6cd5\u7814\u7a76\u4e2d\u7684\u5e94\u7528\u8303\u56f4\u3002", "motivation": "\u91cf\u5b50\u7535\u8def\u6a21\u62df\u5668\u5728\u91cf\u5b50\u7b97\u6cd5\u7814\u7a76\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7ecf\u5178\u8ba1\u7b97\u673a\u4e0a\u7684\u6a21\u62df\u5668\u9762\u4e34\u53ef\u6269\u5c55\u6027\u9650\u5236\uff0c\u5bfc\u81f4\u65e0\u6cd5\u7cbe\u786e\u6a21\u62df\u5927\u89c4\u6a21\u91cf\u5b50\u7535\u8def\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5f20\u91cf\u7f51\u7edc\u6536\u7f29\u6280\u672f\u548c\u9a6c\u5c14\u53ef\u592b\u8fc7\u7a0b\u7684\u7cbe\u786e\u91c7\u6837\u7b97\u6cd5\uff0c\u901a\u8fc7\u5c40\u90e8\u91cf\u5b50\u7535\u8def\u7ed3\u6784\u6f14\u5316\u7ecf\u5178\u72b6\u6001\u3002", "result": "\u6210\u529f\u6a21\u62df\u4e86\u5177\u6709\u591a\u8fbe476\u4e2a\u91cf\u5b50\u6bd4\u7279\u7684\u7406\u60f3\u548c\u5e26\u566a\u58f0\uff08\u5305\u62ec\u53bb\u6781\u5316\u548c\u5e45\u5ea6\u8870\u51cf\u566a\u58f0\uff09\u7684QAOA\u7535\u8def\u7684\u91c7\u6837\uff0c\u8fbe\u5230\u4e86\u524d\u6240\u672a\u6709\u7684\u89c4\u6a21\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u663e\u8457\u6269\u5c55\u4e86\u7ecf\u5178\u6a21\u62df\u5728\u91cf\u5b50\u7b97\u6cd5\u7814\u7a76\u4e2d\u7684\u8303\u56f4\uff0c\u4f7f\u5f97\u5728\u66f4\u5927\u89c4\u6a21\u4e0a\u9a8c\u8bc1\u4e4b\u524d\u7684\u5047\u8bbe\u548c\u731c\u60f3\u6210\u4e3a\u53ef\u80fd\u3002"}}
{"id": "2510.23663", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23663", "abs": "https://arxiv.org/abs/2510.23663", "authors": ["Padmanabhan Jagannathan Prajesh", "Kaliaperumal Ragunath", "Miriam Gordon", "Bruce Rathgeber", "Suresh Neethirajan"], "title": "AI-Driven Carbon Monitoring: Transformer-Based Reconstruction of Atmospheric CO2 in Canadian Poultry Regions", "comment": null, "summary": "Accurate mapping of column-averaged CO2 (XCO2) over agricultural landscapes\nis essential for guiding emission mitigation strategies. We present a\nSpatiotemporal Vision Transformer with Wavelets (ST-ViWT) framework that\nreconstructs continuous, uncertainty-quantified XCO2 fields from OCO-2 across\nsouthern Canada, emphasizing poultry-intensive regions. The model fuses wavelet\ntime-frequency representations with transformer attention over meteorology,\nvegetation indices, topography, and land cover. On 2024 OCO-2 data, ST-ViWT\nattains R2 = 0.984 and RMSE = 0.468 ppm; 92.3 percent of gap-filled predictions\nlie within +/-1 ppm. Independent validation with TCCON shows robust\ngeneralization (bias = -0.14 ppm; r = 0.928), including faithful reproduction\nof the late-summer drawdown. Spatial analysis across 14 poultry regions reveals\na moderate positive association between facility density and XCO2 (r = 0.43);\nhigh-density areas exhibit larger seasonal amplitudes (9.57 ppm) and enhanced\nsummer variability. Compared with conventional interpolation and standard\nmachine-learning baselines, ST-ViWT yields seamless 0.25 degree CO2 surfaces\nwith explicit uncertainties, enabling year-round coverage despite sparse\nobservations. The approach supports integration of satellite constraints with\nnational inventories and precision livestock platforms to benchmark emissions,\nrefine region-specific factors, and verify interventions. Importantly,\ntransformer-based Earth observation enables scalable, transparent, spatially\nexplicit carbon accounting, hotspot prioritization, and policy-relevant\nmitigation assessment.", "AI": {"tldr": "ST-ViWT\u6846\u67b6\u5229\u7528\u5c0f\u6ce2\u548cTransformer\u878d\u5408\u591a\u6e90\u6570\u636e\uff0c\u5b9e\u73b0\u4e86\u5bf9\u519c\u4e1a\u5730\u533aCO2\u6d53\u5ea6\u7684\u7cbe\u786e\u65f6\u7a7a\u91cd\u5efa\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff0c\u9a8c\u8bc1\u4e86\u5bb6\u79bd\u517b\u6b96\u5bc6\u5ea6\u4e0eCO2\u6d53\u5ea6\u4e4b\u95f4\u7684\u5173\u8054\uff0c\u5e76\u4e3a\u78b3\u6838\u7b97\u548c\u51cf\u6392\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002", "motivation": "\u51c6\u786e\u7ed8\u5236\u519c\u4e1a\u5730\u533aXCO2\u56fe\u5bf9\u4e8e\u6307\u5bfc\u51cf\u6392\u7b56\u7565\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65f6\u7a7a\u5c0f\u6ce2Transformer\uff08ST-ViWT\uff09\u6846\u67b6\uff0c\u878d\u5408\u4e86\u5c0f\u6ce2\u65f6\u9891\u8868\u793a\u3001Transformer\u6ce8\u610f\u529b\u548c\u6c14\u8c61\u3001\u690d\u88ab\u6307\u6570\u3001\u5730\u5f62\u548c\u571f\u5730\u8986\u76d6\u7b49\u591a\u79cd\u6570\u636e\uff0c\u7528\u4e8e\u4eceOCO-2\u6570\u636e\u91cd\u5efaXCO2\u573a\u3002", "result": "ST-ViWT\u57282024\u5e74OCO-2\u6570\u636e\u4e0a\u8fbe\u5230\u4e86R2=0.984\u548cRMSE=0.468 ppm\u7684\u6027\u80fd\uff0c92.3%\u7684\u586b\u8865\u9884\u6d4b\u503c\u5728+/-1 ppm\u8303\u56f4\u5185\u3002\u72ec\u7acb\u9a8c\u8bc1\u663e\u793a\u4e86\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff08\u504f\u5dee=-0.14 ppm\uff1br=0.928\uff09\uff0c\u5e76\u80fd\u51c6\u786e\u6a21\u62df\u590f\u5b63CO2\u4e0b\u964d\u3002\u7a7a\u95f4\u5206\u6790\u663e\u793a\u5bb6\u79bd\u517b\u6b96\u8bbe\u65bd\u5bc6\u5ea6\u4e0eXCO2\u5448\u4e2d\u7b49\u6b63\u76f8\u5173\uff08r=0.43\uff09\uff0c\u9ad8\u5bc6\u5ea6\u533a\u57df\u5b63\u8282\u6027\u53d8\u5316\u5e45\u5ea6\u66f4\u5927\u3002", "conclusion": "ST-ViWT\u80fd\u591f\u751f\u6210\u65e0\u7f1d\u76840.25\u5ea6CO2\u8868\u9762\u56fe\uff0c\u5e76\u63d0\u4f9b\u660e\u786e\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5373\u4f7f\u5728\u89c2\u6d4b\u7a00\u758f\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u5b9e\u73b0\u5168\u5e74\u8986\u76d6\u3002\u8be5\u65b9\u6cd5\u652f\u6301\u536b\u661f\u6570\u636e\u4e0e\u56fd\u5bb6\u6e05\u5355\u548c\u7cbe\u51c6\u755c\u7267\u5e73\u53f0\u7684\u96c6\u6210\uff0c\u53ef\u7528\u4e8e\u8bc4\u4f30\u6392\u653e\u3001\u5b8c\u5584\u533a\u57df\u56e0\u5b50\u548c\u9a8c\u8bc1\u51cf\u6392\u63aa\u65bd\u3002\u57fa\u4e8eTransformer\u7684\u5730\u7403\u89c2\u6d4b\u6280\u672f\u4e3a\u53ef\u6269\u5c55\u3001\u900f\u660e\u3001\u7a7a\u95f4\u660e\u786e\u7684\u78b3\u6838\u7b97\u3001\u70ed\u70b9\u8bc6\u522b\u548c\u653f\u7b56\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002"}}
{"id": "2510.24168", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24168", "abs": "https://arxiv.org/abs/2510.24168", "authors": ["Weihua Cheng", "Ersheng Ni", "Wenlong Wang", "Yifei Sun", "Junming Liu", "Wangyu Shen", "Yirong Chen", "Botian Shi", "Ding Wang"], "title": "MGA: Memory-Driven GUI Agent for Observation-Centric Interaction", "comment": "Submitted to WWW2025", "summary": "The rapid progress of Large Language Models (LLMs) and their multimodal\nextensions (MLLMs) has enabled agentic systems capable of perceiving and acting\nacross diverse environments. A challenging yet impactful frontier is the\ndevelopment of GUI agents, which must navigate complex desktop and web\ninterfaces while maintaining robustness and generalization. Existing paradigms\ntypically model tasks as long-chain executions, concatenating historical\ntrajectories into the context. While approaches such as Mirage and GTA1 refine\nplanning or introduce multi-branch action selection, they remain constrained by\ntwo persistent issues: Dependence on historical trajectories, which amplifies\nerror propagation. And Local exploration bias, where \"decision-first,\nobservation-later\" mechanisms overlook critical interface cues. We introduce\nthe Memory-Driven GUI Agent (MGA), which reframes GUI interaction around the\nprinciple of observe first, then decide. MGA models each step as an\nindependent, context-rich environment state represented by a triad: current\nscreenshot, task-agnostic spatial information, and a dynamically updated\nstructured memory. Experiments on OSworld benchmarks, real desktop applications\n(Chrome, VSCode, VLC), and cross-task transfer demonstrate that MGA achieves\nsubstantial gains in robustness, generalization, and efficiency compared to\nstate-of-the-art baselines. The code is publicly available at:\n{https://anonymous.4open.science/r/MGA-3571}.", "AI": {"tldr": "MGA\u662f\u4e00\u79cd\u65b0\u7684GUI\u4ee3\u7406\uff0c\u5b83\u901a\u8fc7\u201c\u5148\u89c2\u5bdf\uff0c\u540e\u51b3\u5b9a\u201d\u7684\u4ea4\u4e92\u65b9\u5f0f\uff0c\u89e3\u51b3\u4e86\u73b0\u6709GUI\u4ee3\u7406\u5728\u9519\u8bef\u4f20\u64ad\u548c\u5c40\u90e8\u63a2\u7d22\u504f\u5dee\u65b9\u9762\u7684\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u9c81\u68d2\u6027\u3001\u6cdb\u5316\u6027\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709GUI\u4ee3\u7406\u5728\u5904\u7406\u590d\u6742\u754c\u9762\u65f6\u5b58\u5728\u9519\u8bef\u4f20\u64ad\u548c\u5c40\u90e8\u63a2\u7d22\u504f\u5dee\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMGA\uff08Memory-Driven GUI Agent\uff09\u7684\u65b0\u8303\u5f0f\uff0c\u8be5\u8303\u5f0f\u5c06GUI\u4ea4\u4e92\u91cd\u6784\u4e3a\u201c\u5148\u89c2\u5bdf\uff0c\u540e\u51b3\u5b9a\u201d\u7684\u6a21\u5f0f\u3002MGA\u5c06\u6bcf\u4e00\u6b65\u5efa\u6a21\u4e3a\u4e00\u4e2a\u72ec\u7acb\u7684\u3001\u5bcc\u542b\u4e0a\u4e0b\u6587\u7684\u73af\u5883\u72b6\u6001\uff0c\u8be5\u72b6\u6001\u7531\u5f53\u524d\u5c4f\u5e55\u622a\u56fe\u3001\u4efb\u52a1\u65e0\u5173\u7684\u7a7a\u95f4\u4fe1\u606f\u4ee5\u53ca\u52a8\u6001\u66f4\u65b0\u7684\u7ed3\u6784\u5316\u8bb0\u5fc6\u7ec4\u6210\u3002", "result": "\u5728OSworld\u57fa\u51c6\u6d4b\u8bd5\u3001\u771f\u5b9e\u684c\u9762\u5e94\u7528\u7a0b\u5e8f\uff08Chrome\u3001VSCode\u3001VLC\uff09\u4ee5\u53ca\u8de8\u4efb\u52a1\u8fc1\u79fb\u5b9e\u9a8c\u4e2d\uff0cMGA\u76f8\u6bd4\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u6a21\u578b\uff0c\u5728\u9c81\u68d2\u6027\u3001\u6cdb\u5316\u6027\u548c\u6548\u7387\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u7684\u63d0\u5347\u3002", "conclusion": "MGA\u901a\u8fc7\u201c\u5148\u89c2\u5bdf\uff0c\u540e\u51b3\u5b9a\u201d\u7684\u4ea4\u4e92\u8303\u5f0f\u548c\u7ed3\u6784\u5316\u8bb0\u5fc6\uff0c\u6709\u6548\u5730\u89e3\u51b3\u4e86\u73b0\u6709GUI\u4ee3\u7406\u7684\u75db\u70b9\uff0c\u5e76\u5728\u591a\u4e2a\u573a\u666f\u4e0b\u5c55\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2510.24102", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24102", "abs": "https://arxiv.org/abs/2510.24102", "authors": ["Yihan Wang", "Peiyu Liu", "Runyu Chen", "Jiaxing Pu", "Wei Xu"], "title": "Squrve: A Unified and Modular Framework for Complex Real-World Text-to-SQL Tasks", "comment": null, "summary": "Text-to-SQL technology has evolved rapidly, with diverse academic methods\nachieving impressive results. However, deploying these techniques in real-world\nsystems remains challenging due to limited integration tools. Despite these\nadvances, we introduce Squrve, a unified, modular, and extensive Text-to-SQL\nframework designed to bring together research advances and real-world\napplications. Squrve first establishes a universal execution paradigm that\nstandardizes invocation interfaces, then proposes a multi-actor collaboration\nmechanism based on seven abstracted effective atomic actor components.\nExperiments on widely adopted benchmarks demonstrate that the collaborative\nworkflows consistently outperform the original individual methods, thereby\nopening up a new effective avenue for tackling complex real-world queries. The\ncodes are available at https://github.com/Satissss/Squrve.", "AI": {"tldr": "Squrve\u662f\u4e00\u4e2a\u7edf\u4e00\u3001\u6a21\u5757\u5316\u548c\u5e7f\u6cdb\u7684Text-to-SQL\u6846\u67b6\uff0c\u65e8\u5728\u6574\u5408\u5b66\u672f\u7814\u7a76\u548c\u5b9e\u9645\u5e94\u7528\uff0c\u901a\u8fc7\u901a\u7528\u7684\u6267\u884c\u8303\u5f0f\u548c\u57fa\u4e8e\u4e03\u4e2a\u62bd\u8c61\u539f\u5b50\u7ec4\u4ef6\u7684\u591a\u53c2\u4e0e\u8005\u534f\u4f5c\u673a\u5236\uff0c\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u4f18\u4e8e\u5355\u72ec\u65b9\u6cd5\u7684\u6210\u679c\u3002", "motivation": "\u90e8\u7f72\u73b0\u6709\u7684Text-to-SQL\u6280\u672f\u5230\u5b9e\u9645\u7cfb\u7edf\u9762\u4e34\u5de5\u5177\u96c6\u6210\u4e0d\u8db3\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u6267\u884c\u8303\u5f0f\uff0c\u6807\u51c6\u5316\u8c03\u7528\u63a5\u53e3\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u4e03\u4e2a\u62bd\u8c61\u539f\u5b50\u7ec4\u4ef6\u7684\u591a\u53c2\u4e0e\u8005\u534f\u4f5c\u673a\u5236\u3002", "result": "\u5728\u5e7f\u6cdb\u91c7\u7528\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u534f\u4f5c\u6d41\u7a0b\u7684\u6027\u80fd\u6301\u7eed\u4f18\u4e8e\u6700\u521d\u7684\u5355\u72ec\u65b9\u6cd5\u3002", "conclusion": "Squrve\u7684\u534f\u4f5c\u65b9\u6cd5\u4e3a\u89e3\u51b3\u590d\u6742\u7684\u5b9e\u9645\u67e5\u8be2\u5f00\u8f9f\u4e86\u4e00\u6761\u65b0\u7684\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2510.24571", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24571", "abs": "https://arxiv.org/abs/2510.24571", "authors": ["Hongxu Zhao", "Guangyang Zeng", "Yunling Shao", "Tengfei Zhang", "Junfeng Wu"], "title": "Spatiotemporal Calibration of Doppler Velocity Logs for Underwater Robots", "comment": null, "summary": "The calibration of extrinsic parameters and clock offsets between sensors for\nhigh-accuracy performance in underwater SLAM systems remains insufficiently\nexplored. Existing methods for Doppler Velocity Log (DVL) calibration are\neither constrained to specific sensor configurations or rely on oversimplified\nassumptions, and none jointly estimate translational extrinsics and time\noffsets. We propose a Unified Iterative Calibration (UIC) framework for general\nDVL sensor setups, formulated as a Maximum A Posteriori (MAP) estimation with a\nGaussian Process (GP) motion prior for high-fidelity motion interpolation. UIC\nalternates between efficient GP-based motion state updates and gradient-based\ncalibration variable updates, supported by a provably statistically consistent\nsequential initialization scheme. The proposed UIC can be applied to IMU,\ncameras and other modalities as co-sensors. We release an open-source\nDVL-camera calibration toolbox. Beyond underwater applications, several aspects\nof UIC-such as the integration of GP priors for MAP-based calibration and the\ndesign of provably reliable initialization procedures-are broadly applicable to\nother multi-sensor calibration problems. Finally, simulations and real-world\ntests validate our approach.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u8fed\u4ee3\u6821\u51c6\uff08UIC\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u6c34\u4e0bSLAM\u7cfb\u7edf\u4e2d\u4f20\u611f\u5668\u5916\u53c2\u548c\u65f6\u949f\u504f\u79fb\u7684\u6821\u51c6\u95ee\u9898\uff0c\u7279\u522b\u5173\u6ce8DVL\u4f20\u611f\u5668\u3002", "motivation": "\u73b0\u6709\u6c34\u4e0bSLAM\u7cfb\u7edf\u4e2dDVL\u4f20\u611f\u5668\u7684\u5916\u53c2\u548c\u65f6\u949f\u504f\u79fb\u6821\u51c6\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u65e0\u6cd5\u6ee1\u8db3\u9ad8\u7cbe\u5ea6\u8981\u6c42\uff0c\u4e14\u901a\u5e38\u65e0\u6cd5\u540c\u65f6\u4f30\u8ba1\u5e73\u79fb\u5916\u53c2\u548c\u65f6\u95f4\u504f\u79fb\u3002", "method": " UIC\u6846\u67b6\u5c06\u6821\u51c6\u95ee\u9898\u6784\u5efa\u4e3a\u9ad8\u65af\u8fc7\u7a0b\uff08GP\uff09\u8fd0\u52a8\u5148\u9a8c\u4e0b\u7684\u6700\u5927\u540e\u9a8c\uff08MAP\uff09\u4f30\u8ba1\u3002\u8be5\u65b9\u6cd5\u4ea4\u66ff\u8fdb\u884c\u57fa\u4e8eGP\u7684\u9ad8\u6548\u8fd0\u52a8\u72b6\u6001\u66f4\u65b0\u548c\u57fa\u4e8e\u68af\u5ea6\u7684\u6821\u51c6\u53d8\u91cf\u66f4\u65b0\uff0c\u5e76\u91c7\u7528\u6e10\u8fdb\u5f0f\u521d\u59cb\u5316\u65b9\u6848\u3002UIC\u6846\u67b6\u9002\u7528\u4e8eIMU\u3001\u6444\u50cf\u5934\u7b49\u591a\u79cd\u4f20\u611f\u5668\u3002", "result": "\u901a\u8fc7\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u6d4b\u8bd5\u9a8c\u8bc1\u4e86UIC\u6846\u67b6\u7684\u6709\u6548\u6027\u3002\u7814\u7a76\u8fd8\u53d1\u5e03\u4e86\u4e00\u4e2a\u5f00\u6e90\u7684DVL-\u6444\u50cf\u5934\u6821\u51c6\u5de5\u5177\u7bb1\u3002", "conclusion": "UIC\u6846\u67b6\u4e3a\u6c34\u4e0b\u591a\u4f20\u611f\u5668\uff08\u5982DVL\u3001IMU\u3001\u6444\u50cf\u5934\uff09\u7684\u5916\u53c2\u548c\u65f6\u949f\u504f\u79fb\u6821\u51c6\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u7528\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5176GP\u5148\u9a8c\u548c\u521d\u59cb\u5316\u65b9\u6848\u4e5f\u53ef\u5e94\u7528\u4e8e\u5176\u4ed6\u591a\u4f20\u611f\u5668\u6821\u51c6\u95ee\u9898\u3002"}}
{"id": "2510.24117", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24117", "abs": "https://arxiv.org/abs/2510.24117", "authors": ["Zan Wang", "Siyu Chen", "Luya Mo", "Xinfeng Gao", "Yuxin Shen", "Lebin Ding", "Wei Liang"], "title": "DogMo: A Large-Scale Multi-View RGB-D Dataset for 4D Canine Motion Recovery", "comment": "19 pages", "summary": "We present DogMo, a large-scale multi-view RGB-D video dataset capturing\ndiverse canine movements for the task of motion recovery from images. DogMo\ncomprises 1.2k motion sequences collected from 10 unique dogs, offering rich\nvariation in both motion and breed. It addresses key limitations of existing\ndog motion datasets, including the lack of multi-view and real 3D data, as well\nas limited scale and diversity. Leveraging DogMo, we establish four motion\nrecovery benchmark settings that support systematic evaluation across monocular\nand multi-view, RGB and RGB-D inputs. To facilitate accurate motion recovery,\nwe further introduce a three-stage, instance-specific optimization pipeline\nthat fits the SMAL model to the motion sequences. Our method progressively\nrefines body shape and pose through coarse alignment, dense correspondence\nsupervision, and temporal regularization. Our dataset and method provide a\nprincipled foundation for advancing research in dog motion recovery and open up\nnew directions at the intersection of computer vision, computer graphics, and\nanimal behavior modeling.", "AI": {"tldr": "DogMo\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u591a\u89c6\u89d2RGB-D\u89c6\u9891\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u4ece\u56fe\u50cf\u4e2d\u6062\u590d\u72ac\u7c7b\u8fd0\u52a8\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4e09\u9636\u6bb5\u7684\u4f18\u5316\u6d41\u7a0b\u6765\u62df\u5408SMAL\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u72ac\u7c7b\u8fd0\u52a8\u6570\u636e\u96c6\u7f3a\u4e4f\u591a\u89c6\u89d2\u3001\u771f\u5b9e3D\u6570\u636e\uff0c\u4e14\u89c4\u6a21\u548c\u591a\u6837\u6027\u6709\u9650\uff0c\u56e0\u6b64\u9700\u8981DogMo\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51faDogMo\u6570\u636e\u96c6\uff0c\u5e76\u5f15\u5165\u4e00\u4e2a\u4e09\u9636\u6bb5\u7684\u5b9e\u4f8b\u7279\u5b9a\u4f18\u5316\u6d41\u7a0b\uff0c\u901a\u8fc7\u7c97\u7565\u5bf9\u9f50\u3001\u5bc6\u96c6\u5bf9\u5e94\u76d1\u7763\u548c\u65f6\u95f4\u6b63\u5219\u5316\u6765\u9010\u6b65\u4f18\u5316\u8eab\u4f53\u5f62\u72b6\u548c\u59ff\u52bf\uff0c\u4ee5\u62df\u5408SMAL\u6a21\u578b\u3002", "result": "DogMo\u6570\u636e\u96c6\u5305\u542b1.2k\u4e2a\u8fd0\u52a8\u5e8f\u5217\uff0c\u6db5\u76d610\u53ea\u4e0d\u540c\u7684\u72d7\uff0c\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u8fd0\u52a8\u548c\u54c1\u79cd\u53d8\u5316\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u51c6\u786e\u5730\u4ece\u89c6\u9891\u4e2d\u6062\u590d\u72ac\u7c7b\u8fd0\u52a8\u3002", "conclusion": "DogMo\u6570\u636e\u96c6\u548c\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a\u63a8\u52a8\u72ac\u7c7b\u8fd0\u52a8\u6062\u590d\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5e76\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u3001\u8ba1\u7b97\u673a\u56fe\u5f62\u5b66\u548c\u52a8\u7269\u884c\u4e3a\u5efa\u6a21\u4ea4\u53c9\u9886\u57df\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2510.24253", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.24253", "abs": "https://arxiv.org/abs/2510.24253", "authors": ["Marcin \u0141obejko", "Tanmoy Biswas", "Micha\u0142 Horodecki"], "title": "Equivalence of Discrete and Continuous Otto-Like Engines assisted by Catalysts: Mapping Catalytic Advantages from the Discrete to the Continuous Framework", "comment": null, "summary": "The catalytic extension of a discrete two-stroke engine employs a cyclic\nauxiliary system - the catalyst - that remains decoupled from the baths and\nperforms no work, yet enhances power and efficiency beyond the corresponding\nnon-catalytic counterpart. Theoretical models of discrete engines are\nrelatively easy to analyze but remain challenging for experimental\nimplementation due to the required control over individual strokes. In\ncontrast, externally driven engines that are simultaneously coupled to both\nheat baths - the so-called continuous engines - are more experimentally\nfeasible. Here, we establish an equivalence between discrete and continuous\nmachines, both with and without a catalyst, by mapping the discrete unitary\nprocesses and thermalization steps onto an interaction Hamiltonian and a\nMarkovian model of dissipation. As a result, by replacing probability flows\nwith probability currents, we construct an analogous continuous machine\ncorresponding to previously demonstrated catalytic schemes that generalize Otto\nengines. We illustrate this mapping for the simplest catalytic extension of the\nOtto engine, demonstrating catalytic enhancement in the continuous regime.", "AI": {"tldr": "\u901a\u8fc7\u5c06\u6982\u7387\u6d41\u66ff\u6362\u4e3a\u6982\u7387\u6d41\uff0c\u53ef\u4ee5\u5b9e\u73b0\u79bb\u6563\u50ac\u5316\u53d1\u52a8\u673a\u5230\u8fde\u7eed\u50ac\u5316\u53d1\u52a8\u673a\u7684\u6620\u5c04\uff0c\u4ece\u800c\u5728\u8fde\u7eed\u4f53\u7cfb\u4e2d\u5b9e\u73b0\u50ac\u5316\u589e\u5f3a\u3002", "motivation": "\u7406\u8bba\u6a21\u578b\u6613\u4e8e\u5206\u6790\u4f46\u96be\u4ee5\u5b9e\u73b0\uff0c\u800c\u8fde\u7eed\u53d1\u52a8\u673a\u66f4\u6613\u4e8e\u5b9e\u9a8c\u5b9e\u73b0\u3002\u672c\u6587\u65e8\u5728\u5efa\u7acb\u79bb\u6563\u548c\u8fde\u7eed\u53d1\u52a8\u673a\u4e4b\u95f4\u7684\u7b49\u4ef7\u6027\uff0c\u4ee5\u4fbf\u4e8e\u5b9e\u9a8c\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u5c06\u79bb\u6563\u7684\u5355\u5143\u8fc7\u7a0b\u548c\u70ed\u5316\u6b65\u9aa4\u6620\u5c04\u5230\u76f8\u4e92\u4f5c\u7528\u54c8\u5bc6\u987f\u91cf\u548c\u9a6c\u5c14\u53ef\u592b\u8017\u6563\u6a21\u578b\uff0c\u5efa\u7acb\u79bb\u6563\u53d1\u52a8\u673a\u4e0e\u8fde\u7eed\u53d1\u52a8\u673a\u4e4b\u95f4\u7684\u7b49\u4ef7\u6027\u3002", "result": "\u6210\u529f\u5730\u5c06\u79bb\u6563\u50ac\u5316\u53d1\u52a8\u673a\uff08\u5305\u62ecOtto\u53d1\u52a8\u673a\u7684\u50ac\u5316\u6269\u5c55\uff09\u6620\u5c04\u5230\u8fde\u7eed\u53d1\u52a8\u673a\uff0c\u5e76\u5c55\u793a\u4e86\u8fde\u7eed\u4f53\u7cfb\u4e2d\u7684\u50ac\u5316\u589e\u5f3a\u6548\u5e94\u3002", "conclusion": "\u8bc1\u660e\u4e86\u79bb\u6563\u548c\u8fde\u7eed\u53d1\u52a8\u673a\u4e4b\u95f4\u7684\u7b49\u4ef7\u6027\uff0c\u4e3a\u5728\u8fde\u7eed\u4f53\u7cfb\u4e2d\u7814\u7a76\u548c\u5b9e\u73b0\u50ac\u5316\u53d1\u52a8\u673a\u63d0\u4f9b\u4e86\u65b0\u7684\u9014\u5f84\u3002"}}
{"id": "2510.23665", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23665", "abs": "https://arxiv.org/abs/2510.23665", "authors": ["Juan C. Leon Alcazar", "Mattia Soldan", "Mohammad Saatialsoruji", "Alejandro Pardo", "Hani Itani", "Juan Camilo Perez", "Bernard Ghanem"], "title": "Transformers from Compressed Representations", "comment": null, "summary": "Compressed file formats are the corner stone of efficient data storage and\ntransmission, yet their potential for representation learning remains largely\nunderexplored. We introduce TEMPEST (TransformErs froM comPressed\nrEpreSenTations), a method that exploits the inherent byte-stream structure of\ncompressed files to design an effective tokenization and encoding strategy. By\nleveraging this compact encoding, a standard transformer can directly learn\nsemantic representations from compressed data streams, bypassing the need for\nraw byte-level processing or full media decoding. Our proposal substantially\nreduces the number of tokens required for semantic classification, thereby\nlowering both computational complexity and memory usage. Through extensive\nexperiments across diverse datasets, coding schemes, and modalities, we show\nthat TEMPEST achieves accuracy competitive wit the state-of-the-art while\ndelivering efficiency gains in memory and compute.", "AI": {"tldr": "TEMPEST\u662f\u4e00\u79cd\u5229\u7528\u538b\u7f29\u6587\u4ef6\u5185\u5728\u5b57\u8282\u6d41\u7ed3\u6784\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6709\u6548\u7684\u6807\u8bb0\u5316\u548c\u7f16\u7801\u7b56\u7565\uff0c\u76f4\u63a5\u4ece\u538b\u7f29\u6570\u636e\u6d41\u4e2d\u5b66\u4e60\u8bed\u4e49\u8868\u793a\uff0c\u4ece\u800c\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u5185\u5b58\u4f7f\u7528\u91cf\uff0c\u5e76\u5728\u51c6\u786e\u6027\u4e0a\u8fbe\u5230\u4e0e\u6700\u5148\u8fdb\u6280\u672f\u76f8\u5f53\u7684\u6c34\u5e73\u3002", "motivation": "\u5df2\u538b\u7f29\u6587\u4ef6\u683c\u5f0f\u5728\u6570\u636e\u5b58\u50a8\u548c\u4f20\u8f93\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u5728\u8868\u793a\u5b66\u4e60\u65b9\u9762\u7684\u6f5c\u529b\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "TEMPEST\uff08TransformErs froM comPressed rEpreSenTations\uff09\u5229\u7528\u538b\u7f29\u6587\u4ef6\u7684\u5185\u5728\u5b57\u8282\u6d41\u7ed3\u6784\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u6709\u6548\u7684\u6807\u8bb0\u5316\u548c\u7f16\u7801\u7b56\u7565\uff0c\u4f7f\u6807\u51c6Transformer\u53ef\u4ee5\u76f4\u63a5\u4ece\u538b\u7f29\u6570\u636e\u6d41\u4e2d\u5b66\u4e60\u8bed\u4e49\u8868\u793a\uff0c\u65e0\u9700\u8fdb\u884c\u539f\u59cb\u5b57\u8282\u7ea7\u5904\u7406\u6216\u5b8c\u6574\u7684\u5a92\u4f53\u89e3\u7801\u3002", "result": "TEMPEST\u5927\u5927\u51cf\u5c11\u4e86\u8bed\u4e49\u5206\u7c7b\u6240\u9700\u7684\u6807\u8bb0\u6570\u91cf\uff0c\u4ece\u800c\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u5185\u5b58\u4f7f\u7528\u91cf\u3002\u5728\u8de8\u8d8a\u4e0d\u540c\u6570\u636e\u96c6\u3001\u7f16\u7801\u65b9\u6848\u548c\u6a21\u6001\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u4e2d\uff0cTEMPEST\u5728\u51c6\u786e\u6027\u4e0a\u8fbe\u5230\u4e86\u4e0e\u73b0\u6709\u6280\u672f\u76f8\u5f53\u7684\u6c34\u5e73\uff0c\u540c\u65f6\u5728\u5185\u5b58\u548c\u8ba1\u7b97\u65b9\u9762\u5b9e\u73b0\u4e86\u6548\u7387\u63d0\u5347\u3002", "conclusion": "TEMPEST\u901a\u8fc7\u5229\u7528\u538b\u7f29\u6587\u4ef6\u683c\u5f0f\u7684\u5185\u5728\u7ed3\u6784\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u80fd\u591f\u76f4\u63a5\u4ece\u538b\u7f29\u6570\u636e\u4e2d\u63d0\u53d6\u8bed\u4e49\u4fe1\u606f\u3002"}}
{"id": "2510.24284", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24284", "abs": "https://arxiv.org/abs/2510.24284", "authors": ["Wenhao Wang", "Peizhi Niu", "Zhao Xu", "Zhaoyu Chen", "Jian Du", "Yaxin Du", "Xianghe Pang", "Keduan Huang", "Yanfeng Wang", "Qiang Yan", "Siheng Chen"], "title": "MCP-Flow: Facilitating LLM Agents to Master Real-World, Diverse and Scaling MCP Tools", "comment": null, "summary": "Large Language Models (LLMs) increasingly rely on external tools to perform\ncomplex, realistic tasks, yet their ability to utilize the rapidly expanding\nModel Contextual Protocol (MCP) ecosystem remains limited. Existing MCP\nresearch covers few servers, depends on costly manual curation, and lacks\ntraining support, hindering progress toward real-world deployment. To overcome\nthese limitations, we introduce MCP-Flow, an automated web-agent-driven\npipeline for large-scale server discovery, data synthesis, and model training.\nMCP-Flow collects and filters data from 1166 servers and 11536 tools, producing\n68733 high-quality instruction-function call pairs and 6439 trajectories, far\nexceeding prior work in scale and diversity. Extensive experiments demonstrate\nMCP-Flow's effectiveness in driving superior MCP tool selection, function-call\ngeneration, and enhanced agentic task performance. MCP-Flow thus provides a\nscalable foundation for advancing LLM agents' proficiency in real-world MCP\nenvironments. MCP-Flow is publicly available at\n\\href{https://github.com/wwh0411/MCP-Flow}{https://github.com/wwh0411/MCP-Flow}.", "AI": {"tldr": "MCP-Flow\u662f\u4e00\u4e2a\u81ea\u52a8\u5316\u7684Web\u4ee3\u7406\u9a71\u52a8\u6d41\u7a0b\uff0c\u7528\u4e8e\u5927\u89c4\u6a21\u670d\u52a1\u5668\u53d1\u73b0\u3001\u6570\u636e\u5408\u6210\u548c\u6a21\u578b\u8bad\u7ec3\uff0c\u4ee5\u514b\u670d\u73b0\u6709MCP\u7814\u7a76\u7684\u5c40\u9650\u6027\uff0c\u4fc3\u8fdbLLM\u4ee3\u7406\u5728\u73b0\u5b9e\u4e16\u754cMCP\u73af\u5883\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u73b0\u6709MCP\u7814\u7a76\u8986\u76d6\u7684\u670d\u52a1\u5668\u5c11\uff0c\u4f9d\u8d56\u624b\u52a8\u6574\u7406\uff0c\u7f3a\u4e4f\u8bad\u7ec3\u652f\u6301\uff0c\u963b\u788d\u4e86\u73b0\u5b9e\u4e16\u754c\u90e8\u7f72\u3002MCP-Flow\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\u3002", "method": "MCP-Flow\u662f\u4e00\u4e2a\u81ea\u52a8\u5316\u7684Web\u4ee3\u7406\u9a71\u52a8\u6d41\u7a0b\uff0c\u7528\u4e8e\u6536\u96c6\u548c\u8fc7\u6ee4\u6765\u81ea1166\u4e2a\u670d\u52a1\u5668\u548c11536\u4e2a\u5de5\u5177\u7684\u6570\u636e\uff0c\u751f\u6210\u6307\u4ee4-\u51fd\u6570\u8c03\u7528\u5bf9\u548c\u8f68\u8ff9\uff0c\u5e76\u7528\u4e8e\u6a21\u578b\u8bad\u7ec3\u3002", "result": "MCP-Flow\u6536\u96c6\u548c\u8fc7\u6ee4\u4e861166\u4e2a\u670d\u52a1\u5668\u548c11536\u4e2a\u5de5\u5177\u7684\u6570\u636e\uff0c\u751f\u6210\u4e8668733\u4e2a\u9ad8\u8d28\u91cf\u7684\u6307\u4ee4-\u51fd\u6570\u8c03\u7528\u5bf9\u548c6439\u4e2a\u8f68\u8ff9\uff0c\u89c4\u6a21\u548c\u591a\u6837\u6027\u8fdc\u8d85\u4ee5\u5f80\u5de5\u4f5c\u3002\u5b9e\u9a8c\u8bc1\u660eMCP-Flow\u5728MCP\u5de5\u5177\u9009\u62e9\u3001\u51fd\u6570\u8c03\u7528\u751f\u6210\u548c\u4ee3\u7406\u4efb\u52a1\u6027\u80fd\u65b9\u9762\u8868\u73b0\u4f18\u8d8a\u3002", "conclusion": "MCP-Flow\u4e3a\u63d0\u5347LLM\u4ee3\u7406\u5728\u73b0\u5b9e\u4e16\u754cMCP\u73af\u5883\u4e2d\u7684\u719f\u7ec3\u5ea6\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u57fa\u7840\u3002"}}
{"id": "2510.24126", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24126", "abs": "https://arxiv.org/abs/2510.24126", "authors": ["Vivek Kalyan", "Martin Andrews"], "title": "Reinforcement Learning for Long-Horizon Multi-Turn Search Agents", "comment": "4 pages plus references and appendices. Accepted into the First\n  Workshop on Multi-Turn Interactions in Large Language Models at NeurIPS 2025", "summary": "Large Language Model (LLM) agents can leverage multiple turns and tools to\nsolve complex tasks, with prompt-based approaches achieving strong performance.\nThis work demonstrates that Reinforcement Learning (RL) can push capabilities\nsignificantly further by learning from experience. Through experiments on a\nlegal document search benchmark, we show that our RL-trained 14 Billion\nparameter model outperforms frontier class models (85% vs 78% accuracy). In\naddition, we explore turn-restricted regimes, during training and at test-time,\nthat show these agents achieve better results if allowed to operate over longer\nmulti-turn horizons.", "AI": {"tldr": "LLM agent\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5728\u6cd5\u5f8b\u6587\u4ef6\u641c\u7d22\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6bd4\u524d\u6cbf\u6a21\u578b\u66f4\u597d\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u5728\u66f4\u957f\u7684\u591a\u8f6e\u4ea4\u4e92\u4e2d\u8868\u73b0\u66f4\u4f73\u3002", "motivation": "LLM agent\u53ef\u4ee5\u901a\u8fc7\u5b66\u4e60\u7ecf\u9a8c\u6765\u63d0\u9ad8\u89e3\u51b3\u590d\u6742\u4efb\u52a1\u7684\u80fd\u529b\uff0c\u800c\u4e0d\u662f\u4ec5\u4ec5\u4f9d\u8d56\u4e8e\u63d0\u793a\u3002", "method": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u6765\u8bad\u7ec3\u4e00\u4e2a140\u4ebf\u53c2\u6570\u7684LLM agent\uff0c\u5e76\u5728\u6cd5\u5f8b\u6587\u4ef6\u641c\u7d22\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fdb\u884c\u8bc4\u4f30\uff0c\u540c\u65f6\u63a2\u7d22\u4e86\u9650\u5236\u4ea4\u4e92\u8f6e\u6570\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u5728\u6cd5\u5f8b\u6587\u4ef6\u641c\u7d22\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u6a21\u578b\u51c6\u786e\u7387\u8fbe\u523085%\uff0c\u4f18\u4e8e\u524d\u6cbf\u6a21\u578b\uff0878%\uff09\u3002\u5728\u9650\u5236\u4ea4\u4e92\u8f6e\u6570\u7684\u60c5\u51b5\u4e0b\uff0c\u6a21\u578b\u5728\u66f4\u957f\u7684\u591a\u8f6e\u4ea4\u4e92\u4e2d\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u80fd\u591f\u663e\u8457\u63d0\u5347LLM agent\u5728\u590d\u6742\u4efb\u52a1\u4e0a\u7684\u80fd\u529b\uff0c\u5e76\u4e14\u5141\u8bb8\u6a21\u578b\u8fdb\u884c\u66f4\u957f\u7684\u591a\u8f6e\u4ea4\u4e92\u80fd\u591f\u8fdb\u4e00\u6b65\u63d0\u9ad8\u5176\u6027\u80fd\u3002"}}
{"id": "2510.24584", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24584", "abs": "https://arxiv.org/abs/2510.24584", "authors": ["J\u00f8rgen Anker Olsen", "Lars R\u00f8nhaug Pettersen", "Kostas Alexis"], "title": "Towards Quadrupedal Jumping and Walking for Dynamic Locomotion using Reinforcement Learning", "comment": "8 pages", "summary": "This paper presents a curriculum-based reinforcement learning framework for\ntraining precise and high-performance jumping policies for the robot `Olympus'.\nSeparate policies are developed for vertical and horizontal jumps, leveraging a\nsimple yet effective strategy. First, we densify the inherently sparse jumping\nreward using the laws of projectile motion. Next, a reference state\ninitialization scheme is employed to accelerate the exploration of dynamic\njumping behaviors without reliance on reference trajectories. We also present a\nwalking policy that, when combined with the jumping policies, unlocks versatile\nand dynamic locomotion capabilities. Comprehensive testing validates walking on\nvaried terrain surfaces and jumping performance that exceeds previous works,\neffectively crossing the Sim2Real gap. Experimental validation demonstrates\nhorizontal jumps up to 1.25 m with centimeter accuracy and vertical jumps up to\n1.0 m. Additionally, we show that with only minor modifications, the proposed\nmethod can be used to learn omnidirectional jumping.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bfe\u7a0b\u5b66\u4e60\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u8bad\u7ec3\u673a\u5668\u4eba\u201cOlympus\u201d\u7684\u7cbe\u786e\u3001\u9ad8\u6027\u80fd\u8df3\u8dc3\u7b56\u7565\u3002", "motivation": "\u4e3a\u4e86\u5b9e\u73b0\u673a\u5668\u4eba\u201cOlympus\u201d\u7684\u7cbe\u786e\u3001\u9ad8\u6027\u80fd\u8df3\u8dc3\uff0c\u5e76\u89e3\u9501\u5176\u5728\u5404\u79cd\u5730\u5f62\u4e0a\u7684\u591a\u529f\u80fd\u3001\u52a8\u6001\u8fd0\u52a8\u80fd\u529b\u3002", "method": "1. \u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bfe\u7a0b\u5b66\u4e60\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u3002\n2. \u5206\u522b\u4e3a\u5782\u76f4\u8df3\u8dc3\u548c\u6c34\u5e73\u8df3\u8dc3\u5f00\u53d1\u4e86\u72ec\u7acb\u7684\u7b56\u7565\u3002\n3. \u5229\u7528\u5f39\u9053\u8fd0\u52a8\u5b9a\u5f8b\u5bf9\u7a00\u758f\u7684\u8df3\u8dc3\u5956\u52b1\u8fdb\u884c\u7a20\u5bc6\u5316\u3002\n4. \u91c7\u7528\u53c2\u8003\u72b6\u6001\u521d\u59cb\u5316\u65b9\u6848\u6765\u52a0\u901f\u52a8\u6001\u8df3\u8dc3\u884c\u4e3a\u7684\u63a2\u7d22\uff0c\u65e0\u9700\u53c2\u8003\u8f68\u8ff9\u3002\n5. \u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u5408\u8df3\u8dc3\u7b56\u7565\u7684\u884c\u8d70\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u591a\u529f\u80fd\u8fd0\u52a8\u80fd\u529b\u3002\n6. \u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u53ef\u4ee5\u5b66\u4e60\u5168\u5411\u8df3\u8dc3\u3002", "result": "1. \u673a\u5668\u4eba\u53ef\u4ee5\u5728\u5404\u79cd\u5730\u5f62\u4e0a\u884c\u8d70\u3002\n2. \u8df3\u8dc3\u6027\u80fd\u8d85\u8fc7\u4e86\u4ee5\u5f80\u7684\u7814\u7a76\uff0c\u6709\u6548\u7f29\u5c0f\u4e86\u4eff\u771f\u5230\u73b0\u5b9e\uff08Sim2Real\uff09\u7684\u5dee\u8ddd\u3002\n3. \u5b9e\u73b0\u4e86\u6c34\u5e73\u8df3\u8dc3\u8ddd\u79bb\u8fbe 1.25 \u7c73\uff0c\u7cbe\u5ea6\u8fbe\u5398\u7c73\u7ea7\u3002\n4. \u5b9e\u73b0\u4e86\u5782\u76f4\u8df3\u8dc3\u9ad8\u5ea6\u8fbe 1.0 \u7c73\u3002\n5. \u63d0\u51fa\u7684\u65b9\u6cd5\u7ecf\u8fc7\u5fae\u5c0f\u4fee\u6539\u5373\u53ef\u5b66\u4e60\u5168\u5411\u8df3\u8dc3\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684\u57fa\u4e8e\u8bfe\u7a0b\u5b66\u4e60\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u80fd\u591f\u6709\u6548\u5730\u8bad\u7ec3\u673a\u5668\u4eba\u6267\u884c\u7cbe\u786e\u3001\u9ad8\u6027\u80fd\u7684\u8df3\u8dc3\uff0c\u5e76\u4e14\u901a\u8fc7\u7ed3\u5408\u884c\u8d70\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u8fd0\u52a8\u80fd\u529b\uff0c\u514b\u670d\u4e86Sim2Real\u7684\u6311\u6218\u3002"}}
{"id": "2510.24129", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24129", "abs": "https://arxiv.org/abs/2510.24129", "authors": ["Jiajian Xie", "Hubery Yin", "Chen Li", "Zhou Zhao", "Shengyu Zhang"], "title": "ETC: training-free diffusion models acceleration with Error-aware Trend Consistency", "comment": "17 pages, 10 figures", "summary": "Diffusion models have achieved remarkable generative quality but remain\nbottlenecked by costly iterative sampling. Recent training-free methods\naccelerate diffusion process by reusing model outputs. However, these methods\nignore denoising trends and lack error control for model-specific tolerance,\nleading to trajectory deviations under multi-step reuse and exacerbating\ninconsistencies in the generated results. To address these issues, we introduce\nError-aware Trend Consistency (ETC), a framework that (1) introduces a\nconsistent trend predictor that leverages the smooth continuity of diffusion\ntrajectories, projecting historical denoising patterns into stable future\ndirections and progressively distributing them across multiple approximation\nsteps to achieve acceleration without deviating; (2) proposes a model-specific\nerror tolerance search mechanism that derives corrective thresholds by\nidentifying transition points from volatile semantic planning to stable quality\nrefinement. Experiments show that ETC achieves a 2.65x acceleration over FLUX\nwith negligible (-0.074 SSIM score) degradation of consistency.", "AI": {"tldr": "ETC\u901a\u8fc7\u5f15\u5165\u8d8b\u52bf\u9884\u6d4b\u548c\u8bef\u5dee\u5bb9\u5fcd\u5ea6\u641c\u7d22\uff0c\u5728\u52a0\u901f\u6269\u6563\u6a21\u578b\u91c7\u6837\u540c\u65f6\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u548c\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u8bad\u7ec3\u514d\u8d39\u52a0\u901f\u65b9\u6cd5\u5ffd\u7565\u4e86\u53bb\u566a\u8d8b\u52bf\u548c\u9519\u8bef\u63a7\u5236\uff0c\u5bfc\u81f4\u591a\u6b65\u590d\u7528\u4e0b\u7684\u8f68\u8ff9\u504f\u5dee\u548c\u751f\u6210\u7ed3\u679c\u4e0d\u4e00\u81f4\u3002", "method": "ETC\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u4e3b\u8981\u90e8\u5206\uff1a1. \u8d8b\u52bf\u9884\u6d4b\u5668\uff0c\u5229\u7528\u6269\u6563\u8f68\u8ff9\u7684\u5e73\u6ed1\u8fde\u7eed\u6027\uff0c\u5c06\u5386\u53f2\u53bb\u566a\u6a21\u5f0f\u6295\u5f71\u5230\u7a33\u5b9a\u7684\u672a\u6765\u65b9\u5411\uff0c\u5e76\u9010\u6b65\u5206\u5e03\u5728\u591a\u4e2a\u8fd1\u4f3c\u6b65\u9aa4\u4e2d\u4ee5\u5b9e\u73b0\u52a0\u901f\u800c\u4e0d\u4ea7\u751f\u504f\u5dee\u30022. \u7279\u5b9a\u4e8e\u6a21\u578b\u7684\u8bef\u5dee\u5bb9\u5dee\u641c\u7d22\u673a\u5236\uff0c\u901a\u8fc7\u8bc6\u522b\u4ece\u4e0d\u7a33\u5b9a\u7684\u8bed\u4e49\u89c4\u5212\u5230\u7a33\u5b9a\u7684\u8d28\u91cf\u7ec6\u5316\u7684\u8fc7\u6e21\u70b9\u6765\u63a8\u5bfc\u6821\u6b63\u9608\u503c\u3002", "result": "ETC\u5b9e\u73b0\u4e86\u6bd4FLUX\u9ad82.65\u500d\u7684\u52a0\u901f\uff0c\u540c\u65f6\u5728\u4e00\u81f4\u6027\u65b9\u9762\u4ec5\u6709\u5fae\u5c0f\u7684(-0.074 SSIM\u5f97\u5206)\u4e0b\u964d\u3002", "conclusion": "ETC\u6846\u67b6\u901a\u8fc7\u5f15\u5165\u8d8b\u52bf\u9884\u6d4b\u548c\u8bef\u5dee\u5bb9\u5fcd\u5ea6\u641c\u7d22\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709\u52a0\u901f\u65b9\u6cd5\u5b58\u5728\u7684\u95ee\u9898\uff0c\u5728\u52a0\u901f\u6269\u6563\u6a21\u578b\u91c7\u6837\u7684\u540c\u65f6\uff0c\u4fdd\u6301\u4e86\u751f\u6210\u8d28\u91cf\u548c\u4e00\u81f4\u6027\u3002"}}
{"id": "2510.24275", "categories": ["quant-ph", "physics.optics"], "pdf": "https://arxiv.org/pdf/2510.24275", "abs": "https://arxiv.org/abs/2510.24275", "authors": ["Christof Wetterich"], "title": "Quantum evolution with classical fields", "comment": "9 pages", "summary": "Wave guides for classical electromagnetic fields can realize the quantum\nevolution of the wave function for a system of qubits.\n  Phase shifts, switches and beam splits allow for the construction of\narbitrary quantum gates.\n  They can act at once on a large number of qubits.\n  For this correlation based photonic quantum computer the channels of the wave\nguides represent basis states of a multi-qubit system rather than individual\nqubits.\n  The classical probabilistic implementation of a quantum evolution sheds new\nlight on the foundations of quantum mechanics.", "AI": {"tldr": "\u5bfc\u6ce2\u53ef\u4ee5\u5b9e\u73b0\u91cf\u5b50\u8ba1\u7b97\u673a\uff0c\u5e76\u4e14\u5b83\u4eec\u53ef\u4ee5\u4e00\u6b21\u6027\u4f5c\u7528\u4e8e\u591a\u4e2a\u91cf\u5b50\u6bd4\u7279\u3002\u8fd9\u4e3a\u91cf\u5b50\u529b\u5b66\u5960\u5b9a\u4e86\u65b0\u7684\u57fa\u7840\u3002", "motivation": "\u5229\u7528\u7ecf\u5178\u7535\u78c1\u573a\u4e2d\u7684\u6ce2\u5bfc\u6765\u5b9e\u73b0\u91cf\u5b50\u6bd4\u7279\u7684\u91cf\u5b50\u6f14\u5316\uff0c\u5e76\u63a2\u8ba8\u5176\u5728\u6784\u5efa\u91cf\u5b50\u8ba1\u7b97\u548c\u91cf\u5b50\u529b\u5b66\u57fa\u7840\u7814\u7a76\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u901a\u8fc7\u6ce2\u5bfc\u4e2d\u7684\u76f8\u4f4d\u79fb\u52a8\u3001\u5f00\u5173\u548c\u6ce2\u675f\u5206\u88c2\u6765\u5b9e\u73b0\u4efb\u610f\u91cf\u5b50\u95e8\uff0c\u5c06\u6ce2\u5bfc\u901a\u9053\u4f5c\u4e3a\u591a\u91cf\u5b50\u6bd4\u7279\u7cfb\u7edf\u7684\u57fa\u6001\u3002", "result": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u5173\u8054\u7684\u5149\u91cf\u5b50\u8ba1\u7b97\u673a\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u80fd\u591f\u4e00\u6b21\u6027\u4f5c\u7528\u4e8e\u5927\u91cf\u91cf\u5b50\u6bd4\u7279\u7684\u6f5c\u529b\u3002", "conclusion": "\u7ecf\u5178\u6982\u7387\u5b9e\u73b0\u91cf\u5b50\u6f14\u5316\u4e3a\u4e86\u89e3\u91cf\u5b50\u529b\u5b66\u57fa\u7840\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002"}}
{"id": "2510.23667", "categories": ["cs.LG", "cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2510.23667", "abs": "https://arxiv.org/abs/2510.23667", "authors": ["Amin Heyrani Nobari", "Lyle Regenwetter", "Cyril Picard", "Ligong Han", "Faez Ahmed"], "title": "Optimize Any Topology: A Foundation Model for Shape- and Resolution-Free Structural Topology Optimization", "comment": null, "summary": "Structural topology optimization (TO) is central to engineering design but\nremains computationally intensive due to complex physics and hard constraints.\nExisting deep-learning methods are limited to fixed square grids, a few\nhand-coded boundary conditions, and post-hoc optimization, preventing general\ndeployment. We introduce Optimize Any Topology (OAT), a foundation-model\nframework that directly predicts minimum-compliance layouts for arbitrary\naspect ratios, resolutions, volume fractions, loads, and fixtures. OAT combines\na resolution- and shape-agnostic autoencoder with an implicit neural-field\ndecoder and a conditional latent-diffusion model trained on OpenTO, a new\ncorpus of 2.2 million optimized structures covering 2 million unique\nboundary-condition configurations. On four public benchmarks and two\nchallenging unseen tests, OAT lowers mean compliance up to 90% relative to the\nbest prior models and delivers sub-1 second inference on a single GPU across\nresolutions from 64 x 64 to 256 x 256 and aspect ratios as high as 10:1. These\nresults establish OAT as a general, fast, and resolution-free framework for\nphysics-aware topology optimization and provide a large-scale dataset to spur\nfurther research in generative modeling for inverse design. Code & data can be\nfound at https://github.com/ahnobari/OptimizeAnyTopology.", "AI": {"tldr": "OAT\u662f\u4e00\u4e2a\u4e3a\u5de5\u7a0b\u8bbe\u8ba1\u63d0\u4f9b\u9ad8\u8ba1\u7b97\u6548\u7387\u548c\u901a\u7528\u6027\u7684\u57fa\u7840\u6a21\u578b\u6846\u67b6\uff0c\u76f4\u63a5\u9884\u6d4b\u4efb\u610f\u957f\u5bbd\u6bd4\u3001\u5206\u8fa8\u7387\u3001\u4f53\u79ef\u5206\u6570\u3001\u8f7d\u8377\u548c\u5939\u5177\u7684\u6700\u5c0f\u5408\u89c4\u6027\u5e03\u5c40\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u62d3\u6251\u4f18\u5316\u4e2d\u53d7\u5230\u56fa\u5b9a\u7f51\u683c\u3001\u6709\u9650\u8fb9\u754c\u6761\u4ef6\u548c\u4e8b\u540e\u4f18\u5316\u7684\u9650\u5236\uff0c\u963b\u788d\u4e86\u5176\u5e7f\u6cdb\u5e94\u7528\u3002", "method": "OAT\u7ed3\u5408\u4e86\u5206\u8fa8\u7387\u548c\u5f62\u72b6\u65e0\u5173\u7684\u81ea\u52a8\u7f16\u7801\u5668\u3001\u9690\u5f0f\u795e\u7ecf\u573a\u89e3\u7801\u5668\u548c\u6761\u4ef6\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff0c\u5e76\u5728\u5305\u542b220\u4e07\u4e2a\u4f18\u5316\u7ed3\u6784\u548c200\u4e07\u4e2a\u72ec\u7279\u8fb9\u754c\u6761\u4ef6\u914d\u7f6e\u7684OpenTO\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bad\u7ec3\u3002", "result": "OAT\u5728\u56db\u4e2a\u516c\u5f00\u57fa\u51c6\u548c\u4e24\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u672a\u89c1\u6d4b\u8bd5\u4e2d\uff0c\u5c06\u5e73\u5747\u5408\u89c4\u6027\u964d\u4f4e\u4e86\u9ad8\u8fbe90%\uff0c\u5e76\u4e14\u572864x64\u5230256x256\u7684\u5206\u8fa8\u7387\u548c\u9ad8\u8fbe10:1\u7684\u957f\u5bbd\u6bd4\u4e0b\uff0c\u5728\u5355\u4e2aGPU\u4e0a\u5b9e\u73b0\u4e86\u4f4e\u4e8e1\u79d2\u7684\u63a8\u7406\u901f\u5ea6\u3002", "conclusion": "OAT\u662f\u4e00\u4e2a\u901a\u7528\u3001\u5feb\u901f\u4e14\u4e0d\u53d7\u5206\u8fa8\u7387\u9650\u5236\u7684\u7269\u7406\u611f\u77e5\u62d3\u6251\u4f18\u5316\u6846\u67b6\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u4ee5\u4fc3\u8fdb\u751f\u6210\u6a21\u578b\u5728\u9006\u8bbe\u8ba1\u65b9\u9762\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2510.24297", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24297", "abs": "https://arxiv.org/abs/2510.24297", "authors": ["Robin Schm\u00f6cker", "Alexander Dockhorn", "Bodo Rosenhahn"], "title": "Investigating Intra-Abstraction Policies For Non-exact Abstraction Algorithms", "comment": null, "summary": "One weakness of Monte Carlo Tree Search (MCTS) is its sample efficiency which\ncan be addressed by building and using state and/or action abstractions in\nparallel to the tree search such that information can be shared among nodes of\nthe same layer. The primary usage of abstractions for MCTS is to enhance the\nUpper Confidence Bound (UCB) value during the tree policy by aggregating visits\nand returns of an abstract node. However, this direct usage of abstractions\ndoes not take the case into account where multiple actions with the same parent\nmight be in the same abstract node, as these would then all have the same UCB\nvalue, thus requiring a tiebreak rule. In state-of-the-art abstraction\nalgorithms such as pruned On the Go Abstractions (pruned OGA), this case has\nnot been noticed, and a random tiebreak rule was implicitly chosen. In this\npaper, we propose and empirically evaluate several alternative\nintra-abstraction policies, several of which outperform the random policy\nacross a majority of environments and parameter settings.", "AI": {"tldr": "MCTS\u7684\u6837\u672c\u6548\u7387\u4f4e\u4e0b\uff0c\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528\u62bd\u8c61\u6765\u89e3\u51b3\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u540c\u4e00\u62bd\u8c61\u8282\u70b9\u4e2d\u7684\u591a\u4e2a\u52a8\u4f5c\u65f6\u5b58\u5728\u95ee\u9898\u3002\u672c\u6587\u63d0\u51fa\u5e76\u8bc4\u4f30\u4e86\u51e0\u79cd\u6539\u8fdb\u7684\u62bd\u8c61\u7b56\u7565\uff0c\u5176\u4e2d\u4e00\u4e9b\u5728\u5927\u591a\u6570\u73af\u5883\u4e0b\u4f18\u4e8e\u968f\u673a\u7b56\u7565\u3002", "motivation": "\u73b0\u6709MCTS\u62bd\u8c61\u65b9\u6cd5\u5728\u5904\u7406\u540c\u4e00\u62bd\u8c61\u8282\u70b9\u5185\u7684\u591a\u4e2a\u52a8\u4f5c\u65f6\uff0c\u672a\u80fd\u5145\u5206\u8003\u8651\u8be5\u60c5\u51b5\uff0c\u5bfc\u81f4\u9700\u8981\u4e00\u4e2a\u4e0d\u660e\u786e\u7684\u968f\u673a\u9009\u62e9\u7b56\u7565\u3002", "method": "\u63d0\u51fa\u5e76\u5b9e\u8bc1\u8bc4\u4f30\u4e86\u51e0\u79cd\u66ff\u4ee3\u7684\u62bd\u8c61\u5185\u90e8\u7b56\u7565\uff0c\u4ee5\u6539\u8fdbMCTS\u7684\u62bd\u8c61\u65b9\u6cd5\u3002", "result": "\u5728\u5927\u591a\u6570\u73af\u5883\u548c\u53c2\u6570\u8bbe\u7f6e\u4e0b\uff0c\u6240\u63d0\u51fa\u7684\u51e0\u79cd\u7b56\u7565\u7684\u6027\u80fd\u4f18\u4e8e\u968f\u673a\u7b56\u7565\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u62bd\u8c61\u5185\u90e8\u7b56\u7565\u5728MCTS\u7684\u62bd\u8c61\u65b9\u6cd5\u65b9\u9762\u53d6\u5f97\u4e86\u6539\u8fdb\uff0c\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2510.24139", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24139", "abs": "https://arxiv.org/abs/2510.24139", "authors": ["Chanwoo Park", "Suyoung Park", "Yelim Ahn", "Jongmin Kim", "Jongyeon Park", "Jaejin Lee"], "title": "Beyond Line-Level Filtering for the Pretraining Corpora of LLMs", "comment": "submitted to ACL ARR Rolling Review", "summary": "While traditional line-level filtering techniques, such as line-level\ndeduplication and trailing-punctuation filters, are commonly used, these basic\nmethods can sometimes discard valuable content, negatively affecting downstream\nperformance. In this paper, we introduce two methods-pattern-aware line-level\ndeduplication (PLD) and pattern-aware trailing punctuation filtering (PTF)-by\nenhancing the conventional filtering techniques. Our approach not only\nconsiders line-level signals but also takes into account their sequential\ndistribution across documents, enabling us to retain structurally important\ncontent that might otherwise be removed. We evaluate these proposed methods by\ntraining small language models (1 B parameters) in both English and Korean. The\nresults demonstrate that our methods consistently improve performance on\nmultiple-choice benchmarks and significantly enhance generative\nquestion-answering accuracy on both SQuAD v1 and KorQuAD v1.", "AI": {"tldr": "\u5f15\u5165\u4e86\u6a21\u5f0f\u611f\u77e5\u65b9\u6cd5\u6765\u6539\u8fdb\u6587\u672c\u8fc7\u6ee4\uff0c\u63d0\u9ad8\u4e86\u4e0b\u6e38\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7684\u884c\u7ea7\u8fc7\u6ee4\u6280\u672f\uff08\u5982\u53bb\u91cd\u548c\u6807\u70b9\u8fc7\u6ee4\uff09\u4f1a\u4e22\u5f03\u6709\u4ef7\u503c\u7684\u5185\u5bb9\uff0c\u5f71\u54cd\u4e0b\u6e38\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u6a21\u5f0f\u611f\u77e5\u884c\u7ea7\u53bb\u91cd\uff08PLD\uff09\u548c\u6a21\u5f0f\u611f\u77e5\u5c3e\u90e8\u6807\u70b9\u8fc7\u6ee4\uff08PTF\uff09\uff0c\u5728\u8003\u8651\u884c\u7ea7\u4fe1\u53f7\u7684\u540c\u65f6\u8003\u8651\u5e8f\u5217\u5206\u5e03\u3002", "result": "\u5728\u82f1\u8bed\u548c\u97e9\u8bed\u7684\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff0810\u4ebf\u53c2\u6570\uff09\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u53d1\u73b0\u5728\u591a\u9879\u9009\u62e9\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6027\u80fd\u6709\u6240\u63d0\u9ad8\uff0c\u5e76\u5728SQuAD v1\u548cKorQuAD v1\u4e0a\u663e\u8457\u63d0\u9ad8\u4e86\u751f\u6210\u5f0f\u95ee\u7b54\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6a21\u5f0f\u611f\u77e5\u8fc7\u6ee4\u65b9\u6cd5\u80fd\u591f\u4fdd\u7559\u91cd\u8981\u7684\u7ed3\u6784\u5316\u5185\u5bb9\uff0c\u6709\u6548\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u3002"}}
{"id": "2510.24623", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24623", "abs": "https://arxiv.org/abs/2510.24623", "authors": ["Nicolai Steinke", "Daniel Goehring"], "title": "GroundLoc: Efficient Large-Scale Outdoor LiDAR-Only Localization", "comment": null, "summary": "In this letter, we introduce GroundLoc, a LiDAR-only localization pipeline\ndesigned to localize a mobile robot in large-scale outdoor environments using\nprior maps. GroundLoc employs a Bird's-Eye View (BEV) image projection focusing\non the perceived ground area and utilizes the place recognition network R2D2,\nor alternatively, the non-learning approach Scale-Invariant Feature Transform\n(SIFT), to identify and select keypoints for BEV image map registration. Our\nresults demonstrate that GroundLoc outperforms state-of-the-art methods on the\nSemanticKITTI and HeLiPR datasets across various sensors. In the multi-session\nlocalization evaluation, GroundLoc reaches an Average Trajectory Error (ATE)\nwell below 50 cm on all Ouster OS2 128 sequences while meeting online runtime\nrequirements. The system supports various sensor models, as evidenced by\nevaluations conducted with Velodyne HDL-64E, Ouster OS2 128, Aeva Aeries II,\nand Livox Avia sensors. The prior maps are stored as 2D raster image maps,\nwhich can be created from a single drive and require only 4 MB of storage per\nsquare kilometer. The source code is available at\nhttps://github.com/dcmlr/groundloc.", "AI": {"tldr": "GroundLoc\u662f\u4e00\u4e2a\u4ec5\u4f7f\u7528\u6fc0\u5149\u96f7\u8fbe\u7684\u5b9a\u4f4d\u6d41\u7a0b\uff0c\u7528\u4e8e\u5728\u5927\u89c4\u6a21\u5ba4\u5916\u73af\u5883\u4e2d\u4f7f\u7528\u5148\u9a8c\u5730\u56fe\u8fdb\u884c\u673a\u5668\u4eba\u5b9a\u4f4d\u3002\u5b83\u4f7f\u7528\u9e1f\u77b0\u56fe\uff08BEV\uff09\u56fe\u50cf\u6295\u5f71\uff0c\u5e76\u7ed3\u5408R2D2\u6216SIFT\u7b97\u6cd5\u8fdb\u884c\u5730\u56fe\u914d\u51c6\u3002", "motivation": "\u5728\u5927\u89c4\u6a21\u5ba4\u5916\u73af\u5883\u4e2d\uff0c\u4f7f\u7528\u6fc0\u5149\u96f7\u8fbe\u8fdb\u884c\u673a\u5668\u4eba\u5b9a\u4f4d\u3002\u5728\u591a\u4f1a\u8bdd\u672c\u5730\u5316\u8bc4\u4f30\u4e2d\uff0cGroundLoc\u5728Ouster OS2 128\u5e8f\u5217\u4e0a\u5b9e\u73b0\u4e86\u4f4e\u4e8e50\u5398\u7c73\u7684\u5e73\u5747\u8f68\u8ff9\u8bef\u5dee\uff08ATE\uff09\uff0c\u540c\u65f6\u6ee1\u8db3\u5728\u7ebf\u8fd0\u884c\u65f6\u8981\u6c42\u3002", "method": "GroundLoc\u91c7\u7528\u9e1f\u77b0\u56fe\uff08BEV\uff09\u56fe\u50cf\u6295\u5f71\uff0c\u805a\u7126\u4e8e\u611f\u77e5\u7684\u5730\u9762\u533a\u57df\uff0c\u5e76\u5229\u7528R2D2\u7f51\u7edc\u6216SIFT\u7b97\u6cd5\u8bc6\u522b\u548c\u9009\u62e9\u5173\u952e\u70b9\u8fdb\u884cBEV\u56fe\u50cf\u5730\u56fe\u914d\u51c6\u3002\u5148\u9a8c\u5730\u56fe\u4ee52D\u6805\u683c\u56fe\u50cf\u5730\u56fe\u7684\u5f62\u5f0f\u5b58\u50a8\uff0c\u6bcf\u5e73\u65b9\u516c\u91cc\u4ec5\u97004MB\u5b58\u50a8\u7a7a\u95f4\u3002", "result": "GroundLoc\u5728SemanticKITTI\u548cHeLiPR\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5e76\u4e14\u652f\u6301\u591a\u79cd\u4f20\u611f\u5668\u6a21\u578b\uff0c\u5305\u62ecVelodyne HDL-64E\u3001Ouster OS2 128\u3001Aeva Aeries II\u548cLivox Avia\u3002", "conclusion": "GroundLoc\u662f\u4e00\u4e2a\u9ad8\u6548\u7684\u6fc0\u5149\u96f7\u8fbe\u672c\u5730\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u5ba4\u5916\u73af\u5883\uff0c\u5e76\u5728\u51c6\u786e\u6027\u548c\u8fd0\u884c\u65f6\u6027\u80fd\u65b9\u9762\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6210\u679c\u3002"}}
{"id": "2510.24133", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24133", "abs": "https://arxiv.org/abs/2510.24133", "authors": ["Minsuk Ji", "Sanghyeok Lee", "Namhyuk Ahn"], "title": "Compositional Image Synthesis with Inference-Time Scaling", "comment": "projcet page: https://github.com/gcl-inha/ReFocus", "summary": "Despite their impressive realism, modern text-to-image models still struggle\nwith compositionality, often failing to render accurate object counts,\nattributes, and spatial relations. To address this challenge, we present a\ntraining-free framework that combines an object-centric approach with\nself-refinement to improve layout faithfulness while preserving aesthetic\nquality. Specifically, we leverage large language models (LLMs) to synthesize\nexplicit layouts from input prompts, and we inject these layouts into the image\ngeneration process, where a object-centric vision-language model (VLM) judge\nreranks multiple candidates to select the most prompt-aligned outcome\niteratively. By unifying explicit layout-grounding with self-refine-based\ninference-time scaling, our framework achieves stronger scene alignment with\nprompts compared to recent text-to-image models. The code are available at\nhttps://github.com/gcl-inha/ReFocus.", "AI": {"tldr": "\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u4ee5\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u7684\u65b9\u6cd5\u548c\u81ea\u6211\u5b8c\u5584\u6280\u672f\uff0c\u4ee5\u63d0\u9ad8\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u5e03\u5c40\u51c6\u786e\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u56fe\u50cf\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u7684\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5728\u7ec4\u5408\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u96be\u4ee5\u51c6\u786e\u6e32\u67d3\u7269\u4f53\u6570\u91cf\u3001\u5c5e\u6027\u548c\u7a7a\u95f4\u5173\u7cfb\u3002", "method": "\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ece\u8f93\u5165\u63d0\u793a\u4e2d\u5408\u6210\u663e\u5f0f\u5e03\u5c40\uff0c\u5e76\u5c06\u8fd9\u4e9b\u5e03\u5c40\u6ce8\u5165\u56fe\u50cf\u751f\u6210\u8fc7\u7a0b\u3002\u4f7f\u7528\u4ee5\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5bf9\u591a\u4e2a\u5019\u9009\u56fe\u50cf\u8fdb\u884c\u8bc4\u5206\u548c\u8fed\u4ee3\u9009\u62e9\uff0c\u4ee5\u9009\u62e9\u6700\u7b26\u5408\u63d0\u793a\u7684\u56fe\u50cf\u3002", "result": "\u4e0e\u6700\u8fd1\u7684\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u76f8\u6bd4\uff0c\u8be5\u6846\u67b6\u5728\u573a\u666f\u4e0e\u63d0\u793a\u7684\u5bf9\u9f50\u65b9\u9762\u8868\u73b0\u66f4\u5f3a\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u663e\u5f0f\u5e03\u5c40\u5f15\u5bfc\u548c\u57fa\u4e8e\u81ea\u6211\u5b8c\u5584\u7684\u63a8\u7406\u65f6\u6269\u5c55\uff0c\u8be5\u6846\u67b6\u80fd\u6709\u6548\u63d0\u9ad8\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u5e03\u5c40\u51c6\u786e\u6027\u548c\u573a\u666f\u5bf9\u9f50\u6027\u3002"}}
{"id": "2510.24316", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.24316", "abs": "https://arxiv.org/abs/2510.24316", "authors": ["Kyeongan Park", "Gwonhak Lee", "Minhyeok Kang", "Youngjun Park", "Joonsuk Huh"], "title": "Jacobi-Anger Density Estimation for Energy Distribution of Quantum States", "comment": null, "summary": "The energy distribution of a quantum state is essential for accurately\nestimating a molecule's ground state energy in quantum computing. Directly\nobtaining this distribution requires full Hamiltonian diagonalization, which is\ncomputationally prohibitive for large-scale systems. A more practical strategy\nis to approximate the distribution from a finite set of Hamiltonian moments.\nHowever, reconstructing an accurate distribution from only a limited number of\nmoments remains a significant challenge. In this work, we introduce\nJacobi-Anger Density Estimation (JADE), a non-parametric, quantum-inspired\nmethod designed to overcome this difficulty. JADE reconstructs the\ncharacteristic function from a finite set of moments using the Jacobi-Anger\nexpansion and then estimates the underlying distribution via an inverse Fourier\ntransform. We demonstrate that JADE can accurately recover the energy\ndistribution of a quantum state for a molecular system. Beyond quantum\nchemistry, we also show that JADE is broadly applicable to the estimation of\ncomplicated probability density functions in various other scientific and\nengineering fields. Our results highlight JADE as a powerful and versatile tool\nfor practical quantum systems, with the potential to significantly enhance\nground state energy estimation and related applications.", "AI": {"tldr": "JADE\u662f\u4e00\u79cd\u65b0\u7684\u91cf\u5b50\u7b97\u6cd5\uff0c\u7528\u4e8e\u4ece\u6709\u9650\u7684\u54c8\u5bc6\u987f\u91cf\u77e9\u4e2d\u7cbe\u786e\u4f30\u8ba1\u91cf\u5b50\u6001\u7684\u80fd\u91cf\u5206\u5e03\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u5e76\u6709\u6f5c\u529b\u5e94\u7528\u4e8e\u91cf\u5b50\u5316\u5b66\u53ca\u5176\u4ed6\u79d1\u5b66\u9886\u57df\u3002", "motivation": "\u7cbe\u786e\u4f30\u8ba1\u91cf\u5b50\u6001\u7684\u57fa\u6001\u80fd\u91cf\u5bf9\u4e8e\u91cf\u5b50\u8ba1\u7b97\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76f4\u63a5\u83b7\u53d6\u80fd\u91cf\u5206\u5e03\u9700\u8981\u8ba1\u7b97\u4e0a\u96be\u4ee5\u627f\u53d7\u7684\u5b8c\u5168\u54c8\u5bc6\u987f\u91cf\u5bf9\u89d2\u5316\u3002\u4ece\u6709\u9650\u7684\u54c8\u5bc6\u987f\u91cf\u77e9\u4e2d\u91cd\u5efa\u7cbe\u786e\u5206\u5e03\u4ecd\u7136\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aJacobi-Anger\u5bc6\u5ea6\u4f30\u8ba1\uff08JADE\uff09\u7684\u975e\u53c2\u6570\u3001\u53d7\u91cf\u5b50\u542f\u53d1\u7684\u65b9\u200b\u200b\u6cd5\u3002JADE\u4f7f\u7528Jacobi-Anger\u5c55\u5f00\u5f0f\u4ece\u6709\u9650\u7684\u77e9\u96c6\u5408\u4e2d\u91cd\u5efa\u7279\u5f81\u51fd\u6570\uff0c\u7136\u540e\u901a\u8fc7\u5085\u7acb\u53f6\u9006\u53d8\u6362\u4f30\u8ba1\u57fa\u7840\u5206\u5e03\u3002", "result": "JADE\u80fd\u591f\u51c6\u786e\u5730\u6062\u590d\u5206\u5b50\u7cfb\u7edf\u7684\u91cf\u5b50\u6001\u80fd\u91cf\u5206\u5e03\u3002\u6b64\u5916\uff0cJADE\u8fd8\u53ef\u4ee5\u5e94\u7528\u4e8e\u5176\u4ed6\u79d1\u5b66\u548c\u5de5\u7a0b\u9886\u57df\u4e2d\u590d\u6742\u6982\u7387\u5bc6\u5ea6\u51fd\u6570\u7684\u4f30\u8ba1\u3002", "conclusion": "JADE\u662f\u4e00\u79cd\u5f3a\u5927\u800c\u901a\u7528\u7684\u5de5\u5177\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u7684\u91cf\u5b50\u7cfb\u7edf\uff0c\u6709\u6f5c\u529b\u663e\u8457\u63d0\u9ad8\u57fa\u6001\u80fd\u91cf\u4f30\u8ba1\u53ca\u76f8\u5173\u5e94\u7528\u3002"}}
{"id": "2510.23668", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23668", "abs": "https://arxiv.org/abs/2510.23668", "authors": ["Fujiang Yuan", "Yangrui Fan", "Xiaohuan Bing", "Zhen Tian", "Chunhong Yuan", "Yankang Li"], "title": "Traffic flow forecasting, STL decomposition, Hybrid model, LSTM, ARIMA, XGBoost, Intelligent transportation systems", "comment": null, "summary": "Accurate traffic flow forecasting is essential for intelligent transportation\nsystems and urban traffic management. However, single model approaches often\nfail to capture the complex, nonlinear, and multi scale temporal patterns in\ntraffic flow data. This study proposes a decomposition driven hybrid framework\nthat integrates Seasonal Trend decomposition using Loess (STL) with three\ncomplementary predictive models. STL first decomposes the original time series\ninto trend, seasonal, and residual components. Then, a Long Short Term Memory\n(LSTM) network models long term trends, an Autoregressive Integrated Moving\nAverage (ARIMA) model captures seasonal periodicity, and an Extreme Gradient\nBoosting (XGBoost) algorithm predicts nonlinear residual fluctuations. The\nfinal forecast is obtained through multiplicative integration of the sub model\npredictions. Using 998 traffic flow records from a New York City intersection\nbetween November and December 2015, results show that the LSTM ARIMA XGBoost\nhybrid model significantly outperforms standalone models including LSTM, ARIMA,\nand XGBoost across MAE, RMSE, and R squared metrics. The decomposition strategy\neffectively isolates temporal characteristics, allowing each model to\nspecialize, thereby improving prediction accuracy, interpretability, and\nrobustness.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408STL\u5206\u89e3\u548cLSTM\u3001ARIMA\u3001XGBoost\u7684\u6df7\u5408\u6a21\u578b\uff0c\u7528\u4e8e\u66f4\u51c6\u786e\u5730\u9884\u6d4b\u4ea4\u901a\u6d41\u91cf\u3002", "motivation": "\u5355\u4e00\u6a21\u578b\u96be\u4ee5\u6355\u6349\u4ea4\u901a\u6d41\u91cf\u6570\u636e\u4e2d\u590d\u6742\u7684\u975e\u7ebf\u6027\u3001\u591a\u65f6\u95f4\u5c3a\u5ea6\u7684\u65f6\u5e8f\u6a21\u5f0f\u3002", "method": "\u9996\u5148\u4f7f\u7528STL\u5206\u89e3\u65f6\u95f4\u5e8f\u5217\u4e3a\u8d8b\u52bf\u3001\u5b63\u8282\u548c\u6b8b\u5dee\u5206\u91cf\uff0c\u7136\u540e\u5206\u522b\u4f7f\u7528LSTM\uff08\u957f\u671f\u8d8b\u52bf\uff09\u3001ARIMA\uff08\u5b63\u8282\u6027\uff09\u548cXGBoost\uff08\u975e\u7ebf\u6027\u6b8b\u5dee\uff09\u8fdb\u884c\u9884\u6d4b\uff0c\u6700\u540e\u901a\u8fc7\u4e58\u6cd5\u96c6\u6210\u5b50\u6a21\u578b\u9884\u6d4b\u3002", "result": "\u5728\u7ebd\u7ea6\u5e02\u4ea4\u901a\u6d41\u91cf\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u6df7\u5408\u6a21\u578b\u5728MAE\u3001RMSE\u548cR\u00b2\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u5355\u72ec\u7684LSTM\u3001ARIMA\u548cXGBoost\u6a21\u578b\u3002", "conclusion": "STL\u5206\u89e3\u7b56\u7565\u6709\u6548\u5206\u79bb\u4e86\u65f6\u95f4\u7279\u5f81\uff0c\u4f7f\u6bcf\u4e2a\u6a21\u578b\u80fd\u591f\u4e13\u4e1a\u5316\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.24299", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24299", "abs": "https://arxiv.org/abs/2510.24299", "authors": ["Jiayu Liu", "Wei Dai", "Zhenya Huang", "Ning Miao", "Enhong Chen"], "title": "Verifying Large Language Models' Reasoning Paths via Correlation Matrix Rank", "comment": null, "summary": "Despite the strong reasoning ability of large language models~(LLMs), they\nare prone to errors and hallucinations. As a result, how to check their outputs\neffectively and efficiently has become a critical problem in their\napplications. Existing checking methods heavily rely on external resources,\nsuch as trained verifiers (e.g., process/outcome reward models) or elaborate\nprompts, which lead to high computational overhead and are only applicable to\nspecific domains. In this paper, we investigate whether the internal behaviors\nof LLMs have already implied the credibility of their reasoning paths.\nSpecifically, we find that the rank of the correlation matrix between the input\nproblem and the output reasoning path is a robust indicator of reasoning\ncorrectness. Different from other correctness indicators for LLMs, the\ncalculation of the correlation matrix only relies on the LLM itself, which\navoids the hassle of training a separate model or designing complicated\nprompts. Based on it, we design a simple, plug-and-play Self-Indicator method\nto reweight candidate reasoning paths, which achieves significant performance\nimprovements than other voting and verification methods with very few\ncomputational overhead. Our experiments across multiple LLMs of varying scales\nand model families have further shown the effectiveness of Self-Indicator. It\nachieves over 75% accuracy in distinguishing correct reasoning paths from\nincorrect ones, and, in turn, improves the accuracies on three reasoning\nbenchmarks by more than 8%.", "AI": {"tldr": "LLM\u8f93\u51fa\u7684\u6b63\u786e\u6027\u53ef\u4ee5\u901a\u8fc7\u5206\u6790\u8f93\u5165\u95ee\u9898\u548c\u8f93\u51fa\u63a8\u7406\u8def\u5f84\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u77e9\u9635\u6765\u8bc4\u4f30\uff0c\u8fd9\u662f\u4e00\u79cd\u65e0\u9700\u5916\u90e8\u8d44\u6e90\u6216\u590d\u6742\u63d0\u793a\u7684\u81ea\u6211\u6307\u793a\u65b9\u6cd5\u3002", "motivation": "\u8bc4\u4f30LLM\u8f93\u51fa\u7684\u53ef\u4fe1\u5ea6\uff0c\u514b\u670d\u73b0\u6709\u65b9\u6cd5\u8ba1\u7b97\u5f00\u9500\u5927\u548c\u9886\u57df\u9650\u5236\u7684\u95ee\u9898\u3002", "method": "\u8ba1\u7b97\u8f93\u5165\u95ee\u9898\u548c\u8f93\u51fa\u63a8\u7406\u8def\u5f84\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u77e9\u9635\uff0c\u5e76\u5229\u7528\u5176\u79e9\u4f5c\u4e3a\u6b63\u786e\u6027\u6307\u6807\uff0c\u8bbe\u8ba1\u4e86Self-Indicator\u65b9\u6cd5\u6765\u91cd\u65b0\u52a0\u6743\u63a8\u7406\u8def\u5f84\u3002", "result": "Self-Indicator\u65b9\u6cd5\u80fd\u591f\u533a\u5206\u6b63\u786e\u548c\u4e0d\u6b63\u786e\u7684\u63a8\u7406\u8def\u5f84\uff0c\u51c6\u786e\u7387\u8d85\u8fc775%\uff0c\u5e76\u5728\u4e09\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c06\u51c6\u786e\u7387\u63d0\u9ad8\u4e868%\u4ee5\u4e0a\u3002", "conclusion": "\u57fa\u4e8eLLM\u5185\u90e8\u884c\u4e3a\u7684Self-Indicator\u65b9\u6cd5\u662f\u4e00\u79cd\u6709\u6548\u4e14\u8ba1\u7b97\u5f00\u9500\u5c0f\u7684LLM\u8f93\u51fa\u9a8c\u8bc1\u65b9\u6cd5\u3002"}}
{"id": "2510.24150", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24150", "abs": "https://arxiv.org/abs/2510.24150", "authors": ["Chanwoo Park", "Suyoung Park", "JiA Kang", "Jongyeon Park", "Sangho Kim", "Hyunji M. Park", "Sumin Bae", "Mingyu Kang", "Jaejin Lee"], "title": "Ko-MuSR: A Multistep Soft Reasoning Benchmark for LLMs Capable of Understanding Korean", "comment": "submitted to ACL ARR Rolling Review", "summary": "We present Ko-MuSR, the first benchmark to comprehensively evaluate\nmultistep, soft reasoning in long Korean narratives while minimizing data\ncontamination. Built following MuSR, Ko-MuSR features fully Korean narratives,\nreasoning chains, and multiple-choice questions verified by human annotators\nfor logical consistency and answerability. Evaluations of four large language\nmodels -- two multilingual and two Korean-specialized -- show that multilingual\nmodels outperform Korean-focused ones even in Korean reasoning tasks,\nindicating cross-lingual generalization of reasoning ability. Carefully\ndesigned prompting strategies, which combine few-shot examples, reasoning\ntraces, and task-specific hints, further boost accuracy, approaching\nhuman-level performance. Ko-MuSR offers a solid foundation for advancing Korean\nNLP by enabling systematic evaluation of long-context reasoning and prompting\nstrategies.", "AI": {"tldr": "Ko-MuSR\u662f\u9996\u4e2a\u7528\u4e8e\u8bc4\u4f30\u957f\u7bc7\u97e9\u8bed\u53d9\u4e8b\u4e2d\u591a\u6b65\u3001\u8f6f\u63a8\u7406\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u5e76\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u4e86\u6570\u636e\u6c61\u67d3\u3002", "motivation": "\u8bc4\u4f30\u957f\u7bc7\u97e9\u8bed\u53d9\u4e8b\u4e2d\u7684\u591a\u6b65\u3001\u8f6f\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u7cfb\u7edf\u5730\u8bc4\u4f30\u63d0\u793a\u7b56\u7565\u3002", "method": "\u6784\u5efa\u4e86Ko-MuSR\u57fa\u51c6\uff0c\u5305\u542b\u5168\u97e9\u8bed\u53d9\u4e8b\u3001\u63a8\u7406\u94fe\u548c\u591a\u9879\u9009\u62e9\u9898\uff0c\u5e76\u7531\u4eba\u5de5\u6807\u6ce8\u8005\u9a8c\u8bc1\u3002\u5bf9\u56db\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "result": "\u591a\u8bed\u8a00\u6a21\u578b\u5728\u97e9\u8bed\u63a8\u7406\u4efb\u52a1\u4e0a\u4f18\u4e8e\u97e9\u8bed\u4e13\u7528\u6a21\u578b\uff0c\u8868\u660e\u63a8\u7406\u80fd\u529b\u7684\u8de8\u8bed\u8a00\u6cdb\u5316\u3002\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63d0\u793a\u7b56\u7565\uff08\u7ed3\u5408\u5c11\u6837\u672c\u793a\u4f8b\u3001\u63a8\u7406\u75d5\u8ff9\u548c\u7279\u5b9a\u4efb\u52a1\u63d0\u793a\uff09\u53ef\u63d0\u9ad8\u51c6\u786e\u6027\uff0c\u63a5\u8fd1\u4eba\u7c7b\u6c34\u5e73\u3002", "conclusion": "Ko-MuSR\u4e3a\u63a8\u8fdb\u97e9\u8bedNLP\u63d0\u4f9b\u4e86\u575a\u5b9e\u7684\u57fa\u7840\uff0c\u80fd\u591f\u7cfb\u7edf\u5730\u8bc4\u4f30\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u548c\u63d0\u793a\u7b56\u7565\u3002"}}
{"id": "2510.24671", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24671", "abs": "https://arxiv.org/abs/2510.24671", "authors": ["Li Li", "Tobias Brinkmann", "Till Temmen", "Markus Eisenbarth", "Jakob Andert"], "title": "Multi-Agent Scenario Generation in Roundabouts with a Transformer-enhanced Conditional Variational Autoencoder", "comment": null, "summary": "With the increasing integration of intelligent driving functions into\nserial-produced vehicles, ensuring their functionality and robustness poses\ngreater challenges. Compared to traditional road testing, scenario-based\nvirtual testing offers significant advantages in terms of time and cost\nefficiency, reproducibility, and exploration of edge cases. We propose a\nTransformer-enhanced Conditional Variational Autoencoder (CVAE-T) model for\ngenerating multi-agent traffic scenarios in roundabouts, which are\ncharacterized by high vehicle dynamics and complex layouts, yet remain\nrelatively underexplored in current research. The results show that the\nproposed model can accurately reconstruct original scenarios and generate\nrealistic, diverse synthetic scenarios. Besides, two Key-Performance-Indicators\n(KPIs) are employed to evaluate the interactive behavior in the generated\nscenarios. Analysis of the latent space reveals partial disentanglement, with\nseveral latent dimensions exhibiting distinct and interpretable effects on\nscenario attributes such as vehicle entry timing, exit timing, and velocity\nprofiles. The results demonstrate the model's capability to generate scenarios\nfor the validation of intelligent driving functions involving multi-agent\ninteractions, as well as to augment data for their development and iterative\nimprovement.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2510.24134", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24134", "abs": "https://arxiv.org/abs/2510.24134", "authors": ["Yang Du", "Zhuoran Lin", "Kaiqiang Song", "Biao Wang", "Zhicheng Zheng", "Tiezheng Ge", "Bo Zheng", "Qin Jin"], "title": "VC4VG: Optimizing Video Captions for Text-to-Video Generation", "comment": "Accepted by EMNLP 2025", "summary": "Recent advances in text-to-video (T2V) generation highlight the critical role\nof high-quality video-text pairs in training models capable of producing\ncoherent and instruction-aligned videos. However, strategies for optimizing\nvideo captions specifically for T2V training remain underexplored. In this\npaper, we introduce VC4VG (Video Captioning for Video Generation), a\ncomprehensive caption optimization framework tailored to the needs of T2V\nmodels.We begin by analyzing caption content from a T2V perspective,\ndecomposing the essential elements required for video reconstruction into\nmultiple dimensions, and proposing a principled caption design methodology. To\nsupport evaluation, we construct VC4VG-Bench, a new benchmark featuring\nfine-grained, multi-dimensional, and necessity-graded metrics aligned with\nT2V-specific requirements.Extensive T2V fine-tuning experiments demonstrate a\nstrong correlation between improved caption quality and video generation\nperformance, validating the effectiveness of our approach. We release all\nbenchmark tools and code at https://github.com/qyr0403/VC4VG to support further\nresearch.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aVC4VG\u7684\u89c6\u9891\u5b57\u5e55\u4f18\u5316\u6846\u67b6\uff0c\u65e8\u5728\u63d0\u5347\u6587\u672c\u5230\u89c6\u9891\uff08T2V\uff09\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5e76\u901a\u8fc7VC4VG-Bench\u57fa\u51c6\u6d4b\u8bd5\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u6587\u672c\u5230\u89c6\u9891\uff08T2V\uff09\u751f\u6210\u6a21\u578b\u5728\u8bad\u7ec3\u65f6\u9ad8\u5ea6\u4f9d\u8d56\u9ad8\u8d28\u91cf\u7684\u89c6\u9891-\u6587\u672c\u5bf9\uff0c\u4f46\u9488\u5bf9T2V\u8bad\u7ec3\u4f18\u5316\u89c6\u9891\u5b57\u5e55\u7684\u7b56\u7565\u7814\u7a76\u5c1a\u4e0d\u5145\u5206\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aVC4VG\uff08Video Captioning for Video Generation\uff09\u7684\u89c6\u9891\u5b57\u5e55\u4f18\u5316\u6846\u67b6\u3002\u8be5\u6846\u67b6\u9996\u5148\u4eceT2V\u7684\u89d2\u5ea6\u5206\u6790\u5b57\u5e55\u5185\u5bb9\uff0c\u5c06\u89c6\u9891\u91cd\u5efa\u6240\u9700\u7684\u5173\u952e\u8981\u7d20\u5206\u89e3\u4e3a\u591a\u4e2a\u7ef4\u5ea6\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u539f\u5219\u6027\u7684\u5b57\u5e55\u8bbe\u8ba1\u65b9\u6cd5\u3002\u4e3a\u4e86\u652f\u6301\u8bc4\u4f30\uff0c\u7814\u7a76\u4eba\u5458\u6784\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aVC4VG-Bench\u7684\u65b0\u57fa\u51c6\uff0c\u8be5\u57fa\u51c6\u5305\u542b\u4e0eT2V\u7279\u5b9a\u9700\u6c42\u5bf9\u9f50\u7684\u7ec6\u7c92\u5ea6\u3001\u591a\u7ef4\u5ea6\u548c\u5fc5\u8981\u6027\u5206\u7ea7\u7684\u5ea6\u91cf\u3002", "result": "\u5728T2V\u6a21\u578b\u7684\u5fae\u8c03\u5b9e\u9a8c\u4e2d\uff0c\u53d1\u73b0\u6539\u8fdb\u5b57\u5e55\u8d28\u91cf\u4e0e\u89c6\u9891\u751f\u6210\u6027\u80fd\u4e4b\u95f4\u5b58\u5728\u5f88\u5f3a\u7684\u76f8\u5173\u6027\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "VC4VG\u6846\u67b6\u80fd\u591f\u6709\u6548\u63d0\u5347\u89c6\u9891\u5b57\u5e55\u8d28\u91cf\uff0c\u8fdb\u800c\u63d0\u9ad8T2V\u6a21\u578b\u7684\u751f\u6210\u6027\u80fd\u3002\u7814\u7a76\u8005\u516c\u5f00\u53d1\u5e03\u4e86\u6240\u6709\u57fa\u51c6\u5de5\u5177\u548c\u4ee3\u7801\u4ee5\u652f\u6301\u540e\u7eed\u7814\u7a76\u3002"}}
{"id": "2510.24323", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.24323", "abs": "https://arxiv.org/abs/2510.24323", "authors": ["Evandro C. R. Rosa", "Jerusa Marchi", "Eduardo I. Duzzioni", "Rafael de Santiago"], "title": "Optimizing Quantum Compilation via High-Level Quantum Instructions", "comment": null, "summary": "Current quantum programming is dominated by low-level, circuit-centric\napproaches that limit the potential for compiler optimization. This work\npresents how a high-level programming construct provides compilers with the\nsemantic information needed for advanced optimizations. We introduce a novel\noptimization that leverages a quantum-specific instruction to automatically\nsubstitute quantum gates with more efficient, approximate decompositions, a\nprocess that is transparent to the programmer and significantly reduces quantum\nresource requirements. Furthermore, we show how this instruction guarantees the\ncorrect uncomputation of auxiliary qubits, enabling safe, dynamic quantum\nmemory management. We illustrate these concepts by implementing a V-chain\ndecomposition of the multi-controlled NOT gate, showing that our high-level\napproach not only simplifies the code but also enables the compiler to generate\na circuit with up to a 50% reduction in CNOT gates. Our results suggest that\nhigh-level abstractions are crucial for unlocking a new class of powerful\ncompiler optimizations, paving the way for more efficient quantum computation.", "AI": {"tldr": "\u901a\u8fc7\u5f15\u5165\u4e00\u79cd\u65b0\u7684\u9ad8\u7ea7\u7f16\u7a0b\u7ed3\u6784\u548c\u91cf\u5b50\u6307\u4ee4\uff0c\u53ef\u4ee5\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u91cf\u5b50\u8ba1\u7b97\uff0c\u51cf\u5c11CNOT\u95e8\u7684\u4f7f\u7528\u9ad8\u8fbe50%\uff0c\u5e76\u5b9e\u73b0\u5b89\u5168\u7684\u91cf\u5b50\u5185\u5b58\u7ba1\u7406\u3002", "motivation": "\u5f53\u524d\u91cf\u5b50\u7f16\u7a0b\u7684\u4f4e\u7ea7\u3001\u4ee5\u7535\u8def\u4e3a\u4e2d\u5fc3\u7684\u65b9\u6cd5\u9650\u5236\u4e86\u7f16\u8bd1\u5668\u7684\u4f18\u5316\u6f5c\u529b\u3002", "method": "\u5f15\u5165\u4e00\u79cd\u65b0\u7684\u9ad8\u7ea7\u7f16\u7a0b\u7ed3\u6784\u548c\u91cf\u5b50\u6307\u4ee4\uff0c\u8be5\u6307\u4ee4\u80fd\u591f\u81ea\u52a8\u7528\u66f4\u9ad8\u6548\u3001\u8fd1\u4f3c\u7684\u5206\u89e3\u66ff\u6362\u91cf\u5b50\u95e8\uff0c\u5e76\u4fdd\u8bc1\u8f85\u52a9\u91cf\u5b50\u4f4d\u7684\u6b63\u786e\u975e\u8ba1\u7b97\uff0c\u4ece\u800c\u5b9e\u73b0\u52a8\u6001\u91cf\u5b50\u5185\u5b58\u7ba1\u7406\u3002\u901a\u8fc7\u5b9e\u73b0\u591a\u63a7\u5236NOT\u95e8\u7684V\u94fe\u5206\u89e3\u6765\u8bf4\u660e\u8fd9\u4e9b\u6982\u5ff5\u3002", "result": "\u5b9e\u73b0V\u94fe\u5206\u89e3\u7684\u591a\u63a7\u5236NOT\u95e8\uff0c\u751f\u6210\u4e86\u4e00\u4e2aCNOT\u95e8\u6570\u91cf\u51cf\u5c11\u9ad8\u8fbe50%\u7684\u7535\u8def\uff0c\u540c\u65f6\u7b80\u5316\u4e86\u4ee3\u7801\u3002", "conclusion": "\u9ad8\u7ea7\u62bd\u8c61\u5bf9\u4e8e\u89e3\u9501\u4e00\u7c7b\u5f3a\u5927\u7684\u7f16\u8bd1\u5668\u4f18\u5316\u81f3\u5173\u91cd\u8981\uff0c\u4e3a\u66f4\u9ad8\u6548\u7684\u91cf\u5b50\u8ba1\u7b97\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2510.23671", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23671", "abs": "https://arxiv.org/abs/2510.23671", "authors": ["Marmik Chaudhari", "Jeremi Nuer", "Rome Thorstenson"], "title": "Sparsity and Superposition in Mixture of Experts", "comment": null, "summary": "Mixture of Experts (MoE) models have become central to scaling large language\nmodels, yet their mechanistic differences from dense networks remain poorly\nunderstood. Previous work has explored how dense models use\n\\textit{superposition} to represent more features than dimensions, and how\nsuperposition is a function of feature sparsity and feature importance. MoE\nmodels cannot be explained mechanistically through the same lens. We find that\nneither feature sparsity nor feature importance cause discontinuous phase\nchanges, and that network sparsity (the ratio of active to total experts)\nbetter characterizes MoEs. We develop new metrics for measuring superposition\nacross experts. Our findings demonstrate that models with greater network\nsparsity exhibit greater \\emph{monosemanticity}. We propose a new definition of\nexpert specialization based on monosemantic feature representation rather than\nload balancing, showing that experts naturally organize around coherent feature\ncombinations when initialized appropriately. These results suggest that network\nsparsity in MoEs may enable more interpretable models without sacrificing\nperformance, challenging the common assumption that interpretability and\ncapability are fundamentally at odds.", "AI": {"tldr": "MoE \u6a21\u578b\u5728\u6269\u5c55\u5927\u578b\u8bed\u8a00\u6a21\u578b\u65b9\u9762\u53d1\u6325\u7740\u6838\u5fc3\u4f5c\u7528\uff0c\u4f46\u5176\u4e0e\u5bc6\u96c6\u7f51\u7edc\u7684\u673a\u5236\u5dee\u5f02\u4ecd\u672a\u5f97\u5230\u5145\u5206\u7406\u89e3\u3002\u672c\u6587\u63a2\u8ba8\u4e86\u5bc6\u96c6\u6a21\u578b\u5982\u4f55\u5229\u7528\u201c\u53e0\u52a0\u201d\u6765\u8868\u793a\u6bd4\u7ef4\u5ea6\u66f4\u591a\u7684\u7279\u5f81\uff0c\u4ee5\u53ca\u53e0\u52a0\u5982\u4f55\u6210\u4e3a\u7279\u5f81\u7a00\u758f\u6027\u548c\u91cd\u8981\u6027\u7684\u51fd\u6570\u3002\u7814\u7a76\u53d1\u73b0\uff0cMoE \u6a21\u578b\u65e0\u6cd5\u901a\u8fc7\u540c\u4e00\u89c6\u89d2\u6765\u89e3\u91ca\u5176\u673a\u5236\u3002\u7279\u5f81\u7a00\u758f\u6027\u6216\u91cd\u8981\u6027\u5747\u4e0d\u4f1a\u5bfc\u81f4\u4e0d\u8fde\u7eed\u7684\u76f8\u53d8\uff0c\u800c\u7f51\u7edc\u7a00\u758f\u6027\uff08\u6fc0\u6d3b\u4e13\u5bb6\u4e0e\u603b\u4e13\u5bb6\u4e4b\u6bd4\uff09\u66f4\u80fd\u8868\u5f81 MoE \u6a21\u578b\u3002\u4f5c\u8005\u5f00\u53d1\u4e86\u8861\u91cf\u8de8\u4e13\u5bb6\u53e0\u52a0\u7684\u65b0\u6307\u6807\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5355\u4e49\u7279\u5f81\u8868\u793a\u800c\u975e\u8d1f\u8f7d\u5747\u8861\u7684\u4e13\u5bb6\u4e13\u4e1a\u5316\u65b0\u5b9a\u4e49\u3002\u7ed3\u679c\u8868\u660e\uff0c\u7f51\u7edc\u7a00\u758f\u6027\u66f4\u5f3a\u7684\u6a21\u578b\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u5355\u4e49\u6027\uff0c\u5e76\u4e14\u4e13\u5bb6\u5728\u9002\u5f53\u521d\u59cb\u5316\u65f6\u4f1a\u56f4\u7ed5\u8fde\u8d2f\u7684\u7279\u5f81\u7ec4\u5408\u8fdb\u884c\u81ea\u7136\u7ec4\u7ec7\u3002\u8fd9\u4e9b\u53d1\u73b0\u8868\u660e\uff0cMoE \u4e2d\u7684\u7f51\u7edc\u7a00\u758f\u6027\u53ef\u80fd\u6709\u52a9\u4e8e\u63d0\u9ad8\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u800c\u4e0d\u4f1a\u727a\u7272\u6027\u80fd\uff0c\u8fd9\u6311\u6218\u4e86\u53ef\u89e3\u91ca\u6027\u4e0e\u80fd\u529b\u4e4b\u95f4\u5b58\u5728\u6839\u672c\u51b2\u7a81\u7684\u666e\u904d\u5047\u8bbe\u3002", "motivation": "\u4e86\u89e3 MoE \u6a21\u578b\u4e0e\u5bc6\u96c6\u7f51\u7edc\u7684\u673a\u5236\u5dee\u5f02\uff0c\u4ee5\u53ca\u7f51\u7edc\u7a00\u758f\u6027\u5bf9 MoE \u6a21\u578b\u53ef\u89e3\u91ca\u6027\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u5f00\u53d1\u65b0\u6307\u6807\u8861\u91cf\u53e0\u52a0\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u5355\u4e49\u7279\u5f81\u8868\u793a\u7684\u4e13\u5bb6\u4e13\u4e1a\u5316\u65b0\u5b9a\u4e49\uff0c\u5206\u6790\u7f51\u7edc\u7a00\u758f\u6027\u4e0e\u5355\u4e49\u6027\u7684\u5173\u7cfb\u3002", "result": "\u7f51\u7edc\u7a00\u758f\u6027\u8d8a\u5f3a\u7684\u6a21\u578b\uff0c\u5355\u4e49\u6027\u8d8a\u5f3a\u3002\u4e13\u5bb6\u5728\u9002\u5f53\u521d\u59cb\u5316\u65f6\u4f1a\u56f4\u7ed5\u8fde\u8d2f\u7684\u7279\u5f81\u7ec4\u5408\u8fdb\u884c\u81ea\u7136\u7ec4\u7ec7\u3002", "conclusion": "\u7f51\u7edc\u7a00\u758f\u6027\u53ef\u80fd\u6709\u52a9\u4e8e\u63d0\u9ad8 MoE \u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u800c\u4e0d\u4f1a\u727a\u7272\u6027\u80fd\u3002"}}
{"id": "2510.24303", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24303", "abs": "https://arxiv.org/abs/2510.24303", "authors": ["Deniz Gorur", "Antoni Rago", "Francesca Toni"], "title": "Retrieval and Argumentation Enhanced Multi-Agent LLMs for Judgmental Forecasting", "comment": null, "summary": "Judgmental forecasting is the task of making predictions about future events\nbased on human judgment. This task can be seen as a form of claim verification,\nwhere the claim corresponds to a future event and the task is to assess the\nplausibility of that event. In this paper, we propose a novel multi-agent\nframework for claim verification, whereby different agents may disagree on\nclaim veracity and bring specific evidence for and against the claims,\nrepresented as quantitative bipolar argumentation frameworks (QBAFs). We then\ninstantiate the framework for supporting claim verification, with a variety of\nagents realised with Large Language Models (LLMs): (1) ArgLLM agents, an\nexisting approach for claim verification that generates and evaluates QBAFs;\n(2) RbAM agents, whereby LLM-empowered Relation-based Argument Mining (RbAM)\nfrom external sources is used to generate QBAFs; (3) RAG-ArgLLM agents,\nextending ArgLLM agents with a form of Retrieval-Augmented Generation (RAG) of\narguments from external sources. Finally, we conduct experiments with two\nstandard judgmental forecasting datasets, with instances of our framework with\ntwo or three agents, empowered by six different base LLMs. We observe that\ncombining evidence from agents can improve forecasting accuracy, especially in\nthe case of three agents, while providing an explainable combination of\nevidence for claim verification.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u6846\u67b6\u7684\u9884\u6d4b\u6a21\u578b\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8fdb\u884c\u8bba\u8bc1\u548c\u8bc1\u636e\u6316\u6398\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u8be5\u6a21\u578b\u80fd\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u5e76\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u8bc1\u636e\u7ec4\u5408\u3002", "motivation": "\u5c06\u5224\u65ad\u6027\u9884\u6d4b\u89c6\u4e3a\u4e00\u79cd\u4e3b\u5f20\u9a8c\u8bc1\u4efb\u52a1\uff0c\u65e8\u5728\u8bc4\u4f30\u672a\u6765\u4e8b\u4ef6\u7684\u5408\u7406\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5176\u4e2d\u4e0d\u540c\u7684\u667a\u80fd\u4f53\u53ef\u80fd\u5bf9\u4e3b\u5f20\u7684\u771f\u5b9e\u6027\u6709\u4e0d\u540c\u610f\u89c1\uff0c\u5e76\u5229\u7528\u5b9a\u91cf\u53cc\u6781\u8bba\u8bc1\u6846\u67b6\uff08QBAFs\uff09\u6765\u652f\u6301\u548c\u53cd\u5bf9\u4e3b\u5f20\u3002\u6846\u67b6\u7684\u667a\u80fd\u4f53\u5305\u62ec\uff1a1. ArgLLM \u667a\u80fd\u4f53\uff08\u751f\u6210\u548c\u8bc4\u4f30 QBAFs\uff09\uff1b2. RbAM \u667a\u80fd\u4f53\uff08\u4f7f\u7528 LLM \u9a71\u52a8\u7684\u5173\u7cfb\u578b\u8bba\u8bc1\u6316\u6398\u4ece\u5916\u90e8\u6765\u6e90\u751f\u6210 QBAFs\uff09\uff1b3. RAG-ArgLLM \u667a\u80fd\u4f53\uff08\u6269\u5c55 ArgLLM\uff0c\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u4ece\u5916\u90e8\u6765\u6e90\u83b7\u53d6\u8bba\u8bc1\uff09\u3002", "result": "\u901a\u8fc7\u5728\u4e24\u4e2a\u6807\u51c6\u7684\u5224\u65ad\u6027\u9884\u6d4b\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\uff0c\u7ed3\u5408\u6765\u81ea\u4e0d\u540c\u667a\u80fd\u4f53\u7684\u8bc1\u636e\u53ef\u4ee5\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u5c24\u5176\u662f\u5728\u4f7f\u7528\u4e09\u4e2a\u667a\u80fd\u4f53\u7684\u60c5\u51b5\u4e0b\uff0c\u540c\u65f6\u8fd8\u80fd\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u8bc1\u636e\u7ec4\u5408\u4ee5\u652f\u6301\u4e3b\u5f20\u9a8c\u8bc1\u3002", "conclusion": "\u7ed3\u5408\u6765\u81ea\u4e0d\u540c\u667a\u80fd\u4f53\u7684\u8bc1\u636e\u53ef\u4ee5\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u5c24\u5176\u662f\u5728\u4f7f\u7528\u4e09\u4e2a\u667a\u80fd\u4f53\u7684\u60c5\u51b5\u4e0b\uff0c\u540c\u65f6\u8fd8\u80fd\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u8bc1\u636e\u7ec4\u5408\u4ee5\u652f\u6301\u4e3b\u5f20\u9a8c\u8bc1\u3002"}}
{"id": "2510.24178", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24178", "abs": "https://arxiv.org/abs/2510.24178", "authors": ["Aaron Scott", "Maike Z\u00fcfle", "Jan Niehues"], "title": "MuSaG: A Multimodal German Sarcasm Dataset with Full-Modal Annotations", "comment": null, "summary": "Sarcasm is a complex form of figurative language in which the intended\nmeaning contradicts the literal one. Its prevalence in social media and popular\nculture poses persistent challenges for natural language understanding,\nsentiment analysis, and content moderation. With the emergence of multimodal\nlarge language models, sarcasm detection extends beyond text and requires\nintegrating cues from audio and vision. We present MuSaG, the first German\nmultimodal sarcasm detection dataset, consisting of 33 minutes of manually\nselected and human-annotated statements from German television shows. Each\ninstance provides aligned text, audio, and video modalities, annotated\nseparately by humans, enabling evaluation in unimodal and multimodal settings.\nWe benchmark nine open-source and commercial models, spanning text, audio,\nvision, and multimodal architectures, and compare their performance to human\nannotations. Our results show that while humans rely heavily on audio in\nconversational settings, models perform best on text. This highlights a gap in\ncurrent multimodal models and motivates the use of MuSaG for developing models\nbetter suited to realistic scenarios. We release MuSaG publicly to support\nfuture research on multimodal sarcasm detection and human-model alignment.", "AI": {"tldr": "MuSaG \u662f\u9996\u4e2a\u5fb7\u8bed\u591a\u6a21\u6001\u8bbd\u523a\u68c0\u6d4b\u6570\u636e\u96c6\uff0c\u5305\u542b33\u5206\u949f\u7684\u7535\u89c6\u8282\u76ee\u7247\u6bb5\uff0c\u63d0\u4f9b\u6587\u672c\u3001\u97f3\u9891\u548c\u89c6\u9891\u6a21\u6001\u3002\u8be5\u6570\u636e\u96c6\u7528\u4e8e\u8bc4\u4f30\u548c\u5f00\u53d1\u80fd\u66f4\u597d\u5904\u7406\u73b0\u5b9e\u573a\u666f\u7684\u591a\u6a21\u6001\u8bbd\u523a\u68c0\u6d4b\u6a21\u578b\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u548c\u6d41\u884c\u6587\u5316\u4e2d\u8bbd\u523a\u7684\u666e\u904d\u6027\u5bf9\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u3001\u60c5\u611f\u5206\u6790\u548c\u5185\u5bb9\u5ba1\u6838\u63d0\u51fa\u4e86\u6301\u7eed\u7684\u6311\u6218\uff0c\u800c\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\u4f7f\u5f97\u8bbd\u523a\u68c0\u6d4b\u9700\u8981\u6574\u5408\u6765\u81ea\u97f3\u9891\u548c\u89c6\u89c9\u7684\u7ebf\u7d22\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3a MuSaG \u7684\u5fb7\u8bed\u591a\u6a21\u6001\u8bbd\u523a\u68c0\u6d4b\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5305\u542b33\u5206\u949f\u7684\u624b\u52a8\u9009\u62e9\u548c\u4eba\u5de5\u6807\u6ce8\u7684\u5fb7\u56fd\u7535\u89c6\u8282\u76ee\u7247\u6bb5\u3002\u6bcf\u4e2a\u5b9e\u4f8b\u90fd\u63d0\u4f9b\u4e86\u5bf9\u9f50\u7684\u6587\u672c\u3001\u97f3\u9891\u548c\u89c6\u9891\u6a21\u6001\uff0c\u5e76\u7531\u4eba\u7c7b\u5355\u72ec\u6807\u6ce8\uff0c\u4ee5\u4fbf\u5728\u5355\u6a21\u6001\u548c\u591a\u6a21\u6001\u8bbe\u7f6e\u4e0b\u8fdb\u884c\u8bc4\u4f30\u3002\u5bf9\u4e5d\u4e2a\u5f00\u6e90\u548c\u5546\u4e1a\u6a21\u578b\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u4e0e\u4eba\u7c7b\u6807\u6ce8\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u867d\u7136\u4eba\u7c7b\u5728\u5bf9\u8bdd\u73af\u5883\u4e2d\u4e25\u91cd\u4f9d\u8d56\u97f3\u9891\uff0c\u4f46\u6a21\u578b\u5728\u6587\u672c\u4e0a\u7684\u8868\u73b0\u6700\u4f73\u3002\u8fd9\u63ed\u793a\u4e86\u5f53\u524d\u591a\u6a21\u6001\u6a21\u578b\u5b58\u5728\u5dee\u8ddd\uff0c\u5e76\u4fc3\u4f7f\u4f7f\u7528 MuSaG \u6765\u5f00\u53d1\u66f4\u9002\u5408\u73b0\u5b9e\u573a\u666f\u7684\u6a21\u578b\u3002", "conclusion": "MuSaG \u662f\u9996\u4e2a\u5fb7\u8bed\u591a\u6a21\u6001\u8bbd\u523a\u68c0\u6d4b\u6570\u636e\u96c6\uff0c\u5b83\u4e3a\u5f00\u53d1\u80fd\u66f4\u597d\u5904\u7406\u73b0\u5b9e\u573a\u666f\u7684\u591a\u6a21\u6001\u8bbd\u523a\u68c0\u6d4b\u6a21\u578b\u63d0\u4f9b\u4e86\u652f\u6301\uff0c\u5e76\u6709\u52a9\u4e8e\u5b9e\u73b0\u4eba\u7c7b\u4e0e\u6a21\u578b\u7684\u66f4\u597d\u5bf9\u9f50\u3002MuSaG \u5df2\u516c\u5f00\u53d1\u5e03\uff0c\u4ee5\u652f\u6301\u672a\u6765\u7684\u7814\u7a76\u3002"}}
{"id": "2510.24136", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24136", "abs": "https://arxiv.org/abs/2510.24136", "authors": ["Ovi Sarkar", "Md Shafiuzzaman", "Md. Faysal Ahamed", "Golam Mahmud", "Muhammad E. H. Chowdhury"], "title": "MSRANetV2: An Explainable Deep Learning Architecture for Multi-class Classification of Colorectal Histopathological Images", "comment": null, "summary": "Colorectal cancer (CRC) is a leading worldwide cause of cancer-related\nmortality, and the role of prompt precise detection is of paramount interest in\nimproving patient outcomes. Conventional diagnostic methods such as colonoscopy\nand histological examination routinely exhibit subjectivity, are extremely\ntime-consuming, and are susceptible to variation. Through the development of\ndigital pathology, deep learning algorithms have become a powerful approach in\nenhancing diagnostic precision and efficiency. In our work, we proposed a\nconvolutional neural network architecture named MSRANetV2, specially optimized\nfor the classification of colorectal tissue images. The model employs a\nResNet50V2 backbone, extended with residual attention mechanisms and\nsqueeze-and-excitation (SE) blocks, to extract deep semantic and fine-grained\nspatial features. With channel alignment and upsampling operations, MSRANetV2\neffectively fuses multi-scale representations, thereby enhancing the robustness\nof the classification. We evaluated our model on a five-fold stratified\ncross-validation strategy on two publicly available datasets: CRC-VAL-HE-7K and\nNCT-CRC-HE-100K. The proposed model achieved remarkable average Precision,\nrecall, F1-score, AUC, and test accuracy were 0.9884 plus-minus 0.0151, 0.9900\nplus-minus 0.0151, 0.9900 plus-minus 0.0145, 0.9999 plus-minus 0.00006, and\n0.9905 plus-minus 0.0025 on the 7K dataset. On the 100K dataset, they were\n0.9904 plus-minus 0.0091, 0.9900 plus-minus 0.0071, 0.9900 plus-minus 0.0071,\n0.9997 plus-minus 0.00016, and 0.9902 plus-minus 0.0006. Additionally, Grad-CAM\nvisualizations were incorporated to enhance model interpretability by\nhighlighting tissue areas that are medically relevant. These findings validate\nthat MSRANetV2 is a reliable, interpretable, and high-performing architectural\nmodel for classifying CRC tissues.", "AI": {"tldr": "MSRANetV2\u662f\u4e00\u79cd\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u901a\u8fc7\u878d\u5408\u591a\u5c3a\u5ea6\u7279\u5f81\u548c\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5728\u7ed3\u76f4\u80a0\u764c\u7ec4\u7ec7\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u9ad8\u7cbe\u5ea6\u548c\u9ad8\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u7ed3\u76f4\u80a0\u764c\uff08CRC\uff09\u7684\u65e9\u671f\u7cbe\u786e\u8bca\u65ad\u5bf9\u6539\u5584\u60a3\u8005\u9884\u540e\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u8bca\u65ad\u65b9\u6cd5\u5b58\u5728\u4e3b\u89c2\u6027\u548c\u8017\u65f6\u6027\u3002\u6df1\u5ea6\u5b66\u4e60\u5728\u63d0\u9ad8\u8bca\u65ad\u7cbe\u5ea6\u548c\u6548\u7387\u65b9\u9762\u5c55\u73b0\u51fa\u6f5c\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMSRANetV2\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u91c7\u7528ResNet50V2\u4f5c\u4e3a\u9aa8\u5e72\uff0c\u5e76\u7ed3\u5408\u6b8b\u5dee\u6ce8\u610f\u529b\u673a\u5236\u548cSE\u6a21\u5757\u6765\u63d0\u53d6\u591a\u5c3a\u5ea6\u7279\u5f81\u3002\u901a\u8fc7\u901a\u9053\u5bf9\u9f50\u548c\u4e0a\u91c7\u6837\u64cd\u4f5c\u878d\u5408\u591a\u5c3a\u5ea6\u8868\u793a\uff0c\u5e76\u4f7f\u7528Grad-CAM\u53ef\u89c6\u5316\u589e\u5f3a\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u5728\u4e24\u4e2a\u516c\u5171\u6570\u636e\u96c6\uff08CRC-VAL-HE-7K\u548cNCT-CRC-HE-100K\uff09\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0cMSRANetV2\u57287K\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e860.9905\u7684\u51c6\u786e\u7387\uff0c\u5728100K\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e860.9902\u7684\u51c6\u786e\u7387\uff0c\u5404\u9879\u6027\u80fd\u6307\u6807\uff08\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u3001F1\u5206\u6570\u3001AUC\uff09\u5747\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "MSRANetV2\u662f\u4e00\u4e2a\u53ef\u9760\u3001\u53ef\u89e3\u91ca\u4e14\u9ad8\u6027\u80fd\u7684\u67b6\u6784\u6a21\u578b\uff0c\u9002\u7528\u4e8e\u7ed3\u76f4\u80a0\u764c\u7ec4\u7ec7\u5206\u7c7b\u3002"}}
{"id": "2510.24327", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.24327", "abs": "https://arxiv.org/abs/2510.24327", "authors": ["Valia Allori"], "title": "Mind, Matter, and Freedom in Quantum Mechanics and the de Broglie-Bohm Theory", "comment": "Conference BOHM IN BRAZIL (BiB) UNIVERSITY OF SAO PAULO (USP) July\n  2025", "summary": "There are several important philosophical problems to which quantum mechanics\nis often said to have made significant contributions:\n  - Determinism: quantum theory has been taken to refute determinism;\n  -Free Will: in turn, this is thought to open the door to free will;\n  - The mind-body problem: relatedly, it is sometimes said to shed light on\nconsciousness;\n  - Idealism: more radically, quantum theory is assumed to have refuted realism\nand to have placed the observer at the center of the world;\n  - Reductionism: even granting realism, it has been claimed that quantum\ntheory undermines reductionism.\n  Our main thesis in this paper is that none of this is either necessary or\ndesirable. By adopting the de Broglie--Bohm theory (or Bohmian mechanics), one\ncan straightforwardly account for quantum phenomena without endorsing any of\nthese claims.", "AI": {"tldr": "\u91cf\u5b50\u529b\u5b66\u5728\u54f2\u5b66\u4e0a\u7684\u8d21\u732e\uff08\u51b3\u5b9a\u8bba\u3001\u81ea\u7531\u610f\u5fd7\u3001\u8eab\u5fc3\u95ee\u9898\u3001\u552f\u5fc3\u4e3b\u4e49\u3001\u8fd8\u539f\u8bba\uff09\u90fd\u53ef\u4ee5\u901a\u8fc7\u91c7\u7528\u5fb7\u5e03\u7f57\u610f-\u73bb\u59c6\u7406\u8bba\u6765\u89e3\u91ca\uff0c\u800c\u65e0\u9700\u8ba4\u53ef\u8fd9\u4e9b\u8bf4\u6cd5\u3002", "motivation": "\u89e3\u91ca\u91cf\u5b50\u529b\u5b66\u5728\u54f2\u5b66\u4e0a\u7684\u8d21\u732e\uff0c\u5e76\u63d0\u51fa\u5fb7\u5e03\u7f57\u610f-\u73bb\u59c6\u7406\u8bba\u53ef\u4ee5\u4f5c\u4e3a\u4e00\u79cd\u66ff\u4ee3\u89e3\u91ca\uff0c\u65e0\u9700\u8ba4\u53ef\u8fd9\u4e9b\u54f2\u5b66\u4e3b\u5f20\u3002", "method": "\u91c7\u7528\u5fb7\u5e03\u7f57\u610f-\u73bb\u59c6\u7406\u8bba\u6765\u89e3\u91ca\u91cf\u5b50\u529b\u5b66\u73b0\u8c61\u3002", "result": "\u5fb7\u5e03\u7f57\u610f-\u73bb\u59c6\u7406\u8bba\u53ef\u4ee5\u89e3\u91ca\u91cf\u5b50\u529b\u5b66\u73b0\u8c61\uff0c\u5e76\u4e14\u53ef\u4ee5\u4e0d\u8ba4\u53ef\u5176\u54f2\u5b66\u4e0a\u7684\u4e3b\u5f20\u3002", "conclusion": "\u91cf\u5b50\u529b\u5b66\u5728\u54f2\u5b66\u4e0a\u7684\u8d21\u732e\uff08\u51b3\u5b9a\u8bba\u3001\u81ea\u7531\u610f\u5fd7\u3001\u8eab\u5fc3\u95ee\u9898\u3001\u552f\u5fc3\u4e3b\u4e49\u3001\u8fd8\u539f\u8bba\uff09\u90fd\u4e0d\u662f\u5fc5\u9700\u7684\uff0c\u901a\u8fc7\u5fb7\u5e03\u7f57\u610f-\u73bb\u59c6\u7406\u8bba\u53ef\u4ee5\u907f\u514d\u8fd9\u4e9b\u54f2\u5b66\u4e0a\u7684\u4e89\u8bae\u3002"}}
{"id": "2510.23672", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23672", "abs": "https://arxiv.org/abs/2510.23672", "authors": ["Xiangfei Qiu", "Xingjian Wu", "Hanyin Cheng", "Xvyuan Liu", "Chenjuan Guo", "Jilin Hu", "Bin Yang"], "title": "DBLoss: Decomposition-based Loss Function for Time Series Forecasting", "comment": "Accepted by NeurIPS 2025", "summary": "Time series forecasting holds significant value in various domains such as\neconomics, traffic, energy, and AIOps, as accurate predictions facilitate\ninformed decision-making. However, the existing Mean Squared Error (MSE) loss\nfunction sometimes fails to accurately capture the seasonality or trend within\nthe forecasting horizon, even when decomposition modules are used in the\nforward propagation to model the trend and seasonality separately. To address\nthese challenges, we propose a simple yet effective Decomposition-Based Loss\nfunction called DBLoss. This method uses exponential moving averages to\ndecompose the time series into seasonal and trend components within the\nforecasting horizon, and then calculates the loss for each of these components\nseparately, followed by weighting them. As a general loss function, DBLoss can\nbe combined with any deep learning forecasting model. Extensive experiments\ndemonstrate that DBLoss significantly improves the performance of\nstate-of-the-art models across diverse real-world datasets and provides a new\nperspective on the design of time series loss functions.", "AI": {"tldr": "DBLoss\u901a\u8fc7\u5206\u522b\u8ba1\u7b97\u5e76\u52a0\u6743\u65f6\u95f4\u5e8f\u5217\u7684\u5b63\u8282\u6027\u548c\u8d8b\u52bf\u5206\u91cf\u7684\u635f\u5931\u6765\u63d0\u9ad8\u9884\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u5747\u65b9\u8bef\u5dee\uff08MSE\uff09\u635f\u5931\u51fd\u6570\u6709\u65f6\u65e0\u6cd5\u51c6\u786e\u6355\u6349\u9884\u6d4b\u8303\u56f4\u5185\u7684\u5b63\u8282\u6027\u6216\u8d8b\u52bf\uff0c\u5373\u4f7f\u4f7f\u7528\u4e86\u5206\u89e3\u6a21\u5757\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDBLoss\u7684\u57fa\u4e8e\u5206\u89e3\u7684\u635f\u5931\u51fd\u6570\uff0c\u8be5\u51fd\u6570\u4f7f\u7528\u6307\u6570\u79fb\u52a8\u5e73\u5747\u6765\u5206\u89e3\u65f6\u95f4\u5e8f\u5217\u7684\u5b63\u8282\u6027\u548c\u8d8b\u52bf\u5206\u91cf\uff0c\u7136\u540e\u5206\u522b\u8ba1\u7b97\u8fd9\u4e9b\u5206\u91cf\u7684\u635f\u5931\u5e76\u8fdb\u884c\u52a0\u6743\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cDBLoss\u5728\u5404\u79cd\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u9ad8\u4e86\u6700\u5148\u8fdb\u6a21\u578b\u7684\u6027\u80fd\u3002", "conclusion": "DBLoss\u4f5c\u4e3a\u4e00\u79cd\u901a\u7528\u7684\u635f\u5931\u51fd\u6570\uff0c\u53ef\u4ee5\u4e0e\u4efb\u4f55\u6df1\u5ea6\u5b66\u4e60\u9884\u6d4b\u6a21\u578b\u7ed3\u5408\u4f7f\u7528\uff0c\u4e3a\u8bbe\u8ba1\u65f6\u95f4\u5e8f\u5217\u635f\u5931\u51fd\u6570\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002"}}
{"id": "2510.24179", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24179", "abs": "https://arxiv.org/abs/2510.24179", "authors": ["Iv\u00e1n Mart\u00ednez-Murillo", "Paloma Moreda", "Elena Lloret"], "title": "Exploring the Influence of Relevant Knowledge for Natural Language Generation Interpretability", "comment": null, "summary": "This paper explores the influence of external knowledge integration in\nNatural Language Generation (NLG), focusing on a commonsense generation task.\nWe extend the CommonGen dataset by creating KITGI, a benchmark that pairs input\nconcept sets with retrieved semantic relations from ConceptNet and includes\nmanually annotated outputs. Using the T5-Large model, we compare sentence\ngeneration under two conditions: with full external knowledge and with filtered\nknowledge where highly relevant relations were deliberately removed. Our\ninterpretability benchmark follows a three-stage method: (1) identifying and\nremoving key knowledge, (2) regenerating sentences, and (3) manually assessing\noutputs for commonsense plausibility and concept coverage. Results show that\nsentences generated with full knowledge achieved 91\\% correctness across both\ncriteria, while filtering reduced performance drastically to 6\\%. These\nfindings demonstrate that relevant external knowledge is critical for\nmaintaining both coherence and concept coverage in NLG. This work highlights\nthe importance of designing interpretable, knowledge-enhanced NLG systems and\ncalls for evaluation frameworks that capture the underlying reasoning beyond\nsurface-level metrics.", "AI": {"tldr": "\u5916\u90e8\u77e5\u8bc6\u5bf9\u4e8e\u81ea\u7136\u8bed\u8a00\u751f\u6210\u81f3\u5173\u91cd\u8981\uff0c\u7279\u522b\u662f\u5728\u5e38\u8bc6\u751f\u6210\u4efb\u52a1\u4e2d\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5728\u81ea\u7136\u8bed\u8a00\u751f\u6210\uff08NLG\uff09\u4e2d\u6574\u5408\u5916\u90e8\u77e5\u8bc6\u7684\u5f71\u54cd\uff0c\u7279\u522b\u5173\u6ce8\u5e38\u8bc6\u751f\u6210\u4efb\u52a1\u3002", "method": "\u7814\u7a76\u6269\u5c55\u4e86CommonGen\u6570\u636e\u96c6\uff0c\u521b\u5efa\u4e86KITGI\u57fa\u51c6\uff0c\u5e76\u4f7f\u7528T5-Large\u6a21\u578b\u5728\u5168\u77e5\u8bc6\u548c\u8fc7\u6ee4\u77e5\u8bc6\u6761\u4ef6\u4e0b\u8fdb\u884c\u53e5\u5b50\u751f\u6210\u5bf9\u6bd4\u3002\u901a\u8fc7\u8bc6\u522b\u548c\u79fb\u9664\u5173\u952e\u77e5\u8bc6\u3001\u91cd\u65b0\u751f\u6210\u53e5\u5b50\u4ee5\u53ca\u4eba\u5de5\u8bc4\u4f30\u8f93\u51fa\u6765\u8fdb\u884c\u5206\u6790\u3002", "result": "\u5728\u5168\u77e5\u8bc6\u6761\u4ef6\u4e0b\uff0c\u53e5\u5b50\u751f\u6210\u5728\u6b63\u786e\u6027\u548c\u6982\u5ff5\u8986\u76d6\u7387\u65b9\u9762\u8fbe\u5230\u4e8691%\u7684\u51c6\u786e\u7387\uff1b\u800c\u8fc7\u6ee4\u77e5\u8bc6\u540e\uff0c\u6027\u80fd\u6025\u5267\u4e0b\u964d\u81f36%\u3002", "conclusion": "\u76f8\u5173\u5916\u90e8\u77e5\u8bc6\u5bf9\u4e8e\u7ef4\u6301NLG\u7684\u8fde\u8d2f\u6027\u548c\u6982\u5ff5\u8986\u76d6\u7387\u81f3\u5173\u91cd\u8981\u3002\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u8bbe\u8ba1\u53ef\u89e3\u91ca\u3001\u77e5\u8bc6\u589e\u5f3a\u7684NLG\u7cfb\u7edf\u7684\u91cd\u8981\u6027\uff0c\u5e76\u547c\u5401\u5efa\u7acb\u8d85\u8d8a\u8868\u9762\u6307\u6807\u7684\u3001\u80fd\u591f\u6355\u6349\u6f5c\u5728\u63a8\u7406\u7684\u8bc4\u4f30\u6846\u67b6\u3002"}}
{"id": "2510.24680", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24680", "abs": "https://arxiv.org/abs/2510.24680", "authors": ["Zishuo Wang", "Joel Loo", "David Hsu"], "title": "Fare: Failure Resilience in Learned Visual Navigation Control", "comment": null, "summary": "While imitation learning (IL) enables effective visual navigation, IL\npolicies are prone to unpredictable failures in out-of-distribution (OOD)\nscenarios. We advance the notion of failure-resilient policies, which not only\ndetect failures but also recover from them automatically. Failure recognition\nthat identifies the factors causing failure is key to informing recovery: e.g.\npinpointing image regions triggering failure detections can provide cues to\nguide recovery. We present Fare, a framework to construct failure-resilient IL\npolicies, embedding OOD-detection and recognition in them without using\nexplicit failure data, and pairing them with recovery heuristics. Real-world\nexperiments show that Fare enables failure recovery across two different policy\narchitectures, enabling robust long-range navigation in complex environments.", "AI": {"tldr": "Fare\u6846\u67b6\u901a\u8fc7\u5d4c\u5165OOD\u68c0\u6d4b\u548c\u8bc6\u522b\u529f\u80fd\uff0c\u5e76\u7ed3\u5408\u6062\u590d\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u5bf9\u89c6\u89c9\u5bfc\u822a\u4e2d\u4e0d\u53ef\u9884\u6d4b\u7684OOD\u5931\u8d25\u7684\u68c0\u6d4b\u548c\u6062\u590d\u3002", "motivation": "\u4f7f\u6a21\u4eff\u5b66\u4e60\uff08IL\uff09\u7b56\u7565\u5728\u5206\u5e03\u5916\uff08OOD\uff09\u573a\u666f\u4e2d\u4e5f\u80fd\u8fdb\u884c\u6709\u6548\u7684\u89c6\u89c9\u5bfc\u822a\uff0c\u5e76\u80fd\u81ea\u52a8\u68c0\u6d4b\u548c\u4ece\u5931\u8d25\u4e2d\u6062\u590d\u3002", "method": "Fare\u6846\u67b6\u5d4c\u5165\u4e86OOD\u68c0\u6d4b\u548c\u8bc6\u522b\u529f\u80fd\uff0c\u4e0d\u4f9d\u8d56\u663e\u5f0f\u7684\u5931\u8d25\u6570\u636e\uff0c\u5e76\u7ed3\u5408\u4e86\u6062\u590d\u7b56\u7565\u3002", "result": "Fare\u6846\u67b6\u80fd\u591f\u5b9e\u73b0\u8de8\u4e0d\u540c\u7b56\u7565\u67b6\u6784\u7684\u5931\u8d25\u6062\u590d\uff0c\u5728\u590d\u6742\u73af\u5883\u4e2d\u5b9e\u73b0\u9c81\u68d2\u7684\u957f\u8ddd\u79bb\u5bfc\u822a\u3002", "conclusion": "Fare\u6846\u67b6\u80fd\u591f\u5b9e\u73b0\u5bf9\u89c6\u89c9\u5bfc\u822a\u4e2d\u4e0d\u53ef\u9884\u6d4b\u7684OOD\u5931\u8d25\u7684\u68c0\u6d4b\u548c\u6062\u590d\uff0c\u4ece\u800c\u63d0\u9ad8\u5bfc\u822a\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.24152", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24152", "abs": "https://arxiv.org/abs/2510.24152", "authors": ["Aodi Wu", "Xubo Luo"], "title": "Enhancing Vision-Language Models for Autonomous Driving through Task-Specific Prompting and Spatial Reasoning", "comment": "RoboSense Challenge with IROS 2025", "summary": "This technical report presents our solution for the RoboSense Challenge at\nIROS 2025, which evaluates Vision-Language Models (VLMs) on autonomous driving\nscene understanding across perception, prediction, planning, and corruption\ndetection tasks. We propose a systematic framework built on four core\ncomponents. First, a Mixture-of-Prompts router classifies questions and\ndispatches them to task-specific expert prompts, eliminating interference\nacross diverse question types. Second, task-specific prompts embed explicit\ncoordinate systems, spatial reasoning rules, role-playing,\nChain-of-Thought/Tree-of-Thought reasoning, and few-shot examples tailored to\neach task. Third, a visual assembly module composes multi-view images with\nobject crops, magenta markers, and adaptive historical frames based on question\nrequirements. Fourth, we configure model inference parameters (temperature,\ntop-p, message roles) per task to optimize output quality. Implemented on\nQwen2.5-VL-72B, our approach achieves 70.87% average accuracy on Phase-1 (clean\ndata) and 72.85% on Phase-2 (corrupted data), demonstrating that structured\nprompting and spatial grounding substantially enhance VLM performance on\nsafety-critical autonomous driving tasks. Code and prompt are available at\nhttps://github.com/wuaodi/UCAS-CSU-phase2.", "AI": {"tldr": "\u672c\u6280\u672f\u62a5\u544a\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u7406\u89e3\u7684VLM\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u63d0\u793a\u548c\u7a7a\u95f4\u5b9a\u4f4d\u663e\u8457\u63d0\u9ad8\u4e86\u6027\u80fd\uff0c\u5728RoboSense\u6311\u6218\u8d5b\u4e2d\u53d6\u5f97\u4e86\u4f18\u5f02\u6210\u7ee9\u3002", "motivation": "\u8bc4\u4f30\u548c\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5728\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u7406\u89e3\u4e2d\u7684\u80fd\u529b\uff0c\u6db5\u76d6\u611f\u77e5\u3001\u9884\u6d4b\u3001\u89c4\u5212\u548c\u8150\u8d25\u68c0\u6d4b\u7b49\u4efb\u52a1\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u5305\u542b\u56db\u4e2a\u6838\u5fc3\u7ec4\u4ef6\u7684\u7cfb\u7edf\u6846\u67b6\uff1a1. \u6df7\u5408\u63d0\u793a\u8def\u7531\u5668\uff0c\u7528\u4e8e\u5206\u7c7b\u95ee\u9898\u5e76\u5206\u914d\u7ed9\u7279\u5b9a\u4efb\u52a1\u7684\u4e13\u5bb6\u63d0\u793a\uff1b2. \u7279\u5b9a\u4efb\u52a1\u7684\u63d0\u793a\uff0c\u5d4c\u5165\u5750\u6807\u7cfb\u3001\u7a7a\u95f4\u63a8\u7406\u89c4\u5219\u3001\u89d2\u8272\u626e\u6f14\u3001\u601d\u7ef4\u94fe/\u601d\u7ef4\u6811\u63a8\u7406\u548c\u5c11\u6837\u672c\u793a\u4f8b\uff1b3. \u89c6\u89c9\u7ec4\u88c5\u6a21\u5757\uff0c\u6839\u636e\u95ee\u9898\u8981\u6c42\u7ec4\u5408\u591a\u89c6\u56fe\u56fe\u50cf\u3001\u7269\u4f53\u88c1\u526a\u3001\u989c\u8272\u6807\u8bb0\u548c\u5386\u53f2\u5e27\uff1b4. \u6a21\u578b\u63a8\u7406\u53c2\u6570\u914d\u7f6e\uff0c\u9488\u5bf9\u4e0d\u540c\u4efb\u52a1\u4f18\u5316\u8f93\u51fa\u8d28\u91cf\u3002\u6a21\u578b\u57fa\u4e8eQwen2.5-VL-72B\u3002", "result": "\u5728RoboSense\u6311\u6218\u8d5b\u7684Phase-1\uff08\u5e72\u51c0\u6570\u636e\uff09\u548cPhase-2\uff08\u8150\u8d25\u6570\u636e\uff09\u4e0a\uff0c\u5e73\u5747\u51c6\u786e\u7387\u5206\u522b\u4e3a70.87%\u548c72.85%\u3002", "conclusion": "\u7ed3\u6784\u5316\u63d0\u793a\u548c\u7a7a\u95f4\u5b9a\u4f4d\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8VLM\u5728\u5b89\u5168\u5173\u952e\u7684\u81ea\u52a8\u9a7e\u9a76\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2510.24348", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.24348", "abs": "https://arxiv.org/abs/2510.24348", "authors": ["Xin Wang", "Rebing Wu"], "title": "Tight Generalization Bound for Supervised Quantum Machine Learning", "comment": "21 pages, 14 figures", "summary": "We derive a tight generalization bound for quantum machine learning that is\napplicable to a wide range of supervised tasks, data, and models. Our bound is\nboth efficiently computable and free of big-O notation. Furthermore, we point\nout that previous bounds relying on big-O notation may provide misleading\nsuggestions regarding the generalization error. Our generalization bound\ndemonstrates that for quantum machine learning models of arbitrary size and\ndepth, the sample size is the most dominant factor governing the generalization\nerror. Additionally, the spectral norm of the measurement observable, the bound\nand Lipschitz constant of the selected risk function also influence the\ngeneralization upper bound. However, the number of quantum gates, the number of\nqubits, data encoding methods, and hyperparameters chosen during the learning\nprocess such as batch size, epochs, learning rate, and optimizer do not\nsignificantly impact the generalization capability of quantum machine learning.\nWe experimentally demonstrate the tightness of our generalization bound across\nclassification and regression tasks. Furthermore, we show that our tight\ngeneralization upper bound holds even when labels are completely randomized. We\nthus bring clarity to the fundamental question of generalization in quantum\nmachine learning.", "AI": {"tldr": "\u672c\u7814\u7a76\u4e3a\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u63a8\u5bfc\u51fa\u4e86\u4e00\u4e2a\u7d27\u5b9e\u7684\u6cdb\u5316\u754c\u9650\uff0c\u9002\u7528\u4e8e\u5e7f\u6cdb\u7684\u76d1\u7763\u4efb\u52a1\u3001\u6570\u636e\u548c\u6a21\u578b\u3002\u8be5\u754c\u9650\u53ef\u9ad8\u6548\u8ba1\u7b97\u4e14\u4e0d\u542b\u5927O\u7b26\u53f7\uff0c\u5e76\u6307\u51fa\u5148\u524d\u4f9d\u8d56\u5927O\u7b26\u53f7\u7684\u754c\u9650\u53ef\u80fd\u8bef\u5bfc\u6cdb\u5316\u8bef\u5dee\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u6837\u672c\u91cf\u662f\u5f71\u54cd\u6cdb\u5316\u8bef\u5dee\u7684\u6700\u4e3b\u8981\u56e0\u7d20\uff0c\u800c\u91cf\u5b50\u95e8\u6570\u91cf\u3001\u91cf\u5b50\u6bd4\u7279\u6570\u3001\u6570\u636e\u7f16\u7801\u65b9\u6cd5\u53ca\u5b66\u4e60\u8d85\u53c2\u6570\uff08\u5982\u6279\u5927\u5c0f\u3001\u5468\u671f\u3001\u5b66\u4e60\u7387\u548c\u4f18\u5316\u5668\uff09\u5bf9\u6cdb\u5316\u80fd\u529b\u5f71\u54cd\u4e0d\u5927\u3002", "motivation": "\u4e4b\u524d\u7684\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u6cdb\u5316\u754c\u9650\u53ef\u80fd\u5b58\u5728\u8bef\u5bfc\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63d0\u4f9b\u4e00\u4e2a\u66f4\u7cbe\u786e\u3001\u66f4\u6613\u4e8e\u8ba1\u7b97\u7684\u6cdb\u5316\u754c\u9650\uff0c\u5e76\u6f84\u6e05\u5f71\u54cd\u6cdb\u5316\u80fd\u529b\u7684\u5173\u952e\u56e0\u7d20\u3002", "method": "\u63a8\u5bfc\u4e86\u4e00\u4e2a\u7d27\u5b9e\u7684\u6cdb\u5316\u754c\u9650\uff0c\u8be5\u754c\u9650\u4e0d\u542b\u5927O\u7b26\u53f7\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u5728\u5206\u7c7b\u548c\u56de\u5f52\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5373\u4f7f\u5728\u6807\u7b7e\u5b8c\u5168\u968f\u673a\u7684\u60c5\u51b5\u4e0b\u4e5f\u6210\u7acb\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7d27\u5b9e\u7684\u6cdb\u5316\u754c\u9650\uff0c\u660e\u786e\u4e86\u6837\u672c\u91cf\u3001\u6d4b\u91cf\u7b97\u5b50\u7684\u8c31\u8303\u6570\u3001\u98ce\u9669\u51fd\u6570\u7684\u754c\u9650\u548c\u5229\u666e\u5e0c\u8328\u5e38\u6570\u662f\u5f71\u54cd\u6cdb\u5316\u8bef\u5dee\u7684\u4e3b\u8981\u56e0\u7d20\uff0c\u800c\u91cf\u5b50\u95e8\u7684\u6570\u91cf\u3001\u91cf\u5b50\u6bd4\u7279\u6570\u3001\u6570\u636e\u7f16\u7801\u548c\u90e8\u5206\u8d85\u53c2\u6570\u5219\u5f71\u54cd\u4e0d\u5927\u3002\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u754c\u9650\u7684\u7d27\u5bc6\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u7684\u6cdb\u5316\u95ee\u9898\u5e26\u6765\u4e86\u6e05\u6670\u7684\u8ba4\u8bc6\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u66f4\u51c6\u786e\u3001\u5b9e\u7528\u7684\u6cdb\u5316\u754c\u9650\uff0c\u5e76\u7ea0\u6b63\u4e86\u4ee5\u5f80\u7814\u7a76\u4e2d\u53ef\u80fd\u5b58\u5728\u7684\u8bef\u89e3\u3002"}}
{"id": "2510.23681", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23681", "abs": "https://arxiv.org/abs/2510.23681", "authors": ["Carl Hvarfner", "David Eriksson", "Eytan Bakshy", "Max Balandat"], "title": "Informed Initialization for Bayesian Optimization and Active Learning", "comment": "28 pages", "summary": "Bayesian Optimization is a widely used method for optimizing expensive\nblack-box functions, relying on probabilistic surrogate models such as Gaussian\nProcesses. The quality of the surrogate model is crucial for good optimization\nperformance, especially in the few-shot setting where only a small number of\nbatches of points can be evaluated. In this setting, the initialization plays a\ncritical role in shaping the surrogate's predictive quality and guiding\nsubsequent optimization. Despite this, practitioners typically rely on\n(quasi-)random designs to cover the input space. However, such approaches\nneglect two key factors: (a) space-filling designs may not be desirable to\nreduce predictive uncertainty, and (b) efficient hyperparameter learning during\ninitialization is essential for high-quality prediction, which may conflict\nwith space-filling designs. To address these limitations, we propose\nHyperparameter-Informed Predictive Exploration (HIPE), a novel acquisition\nstrategy that balances predictive uncertainty reduction with hyperparameter\nlearning using information-theoretic principles. We derive a closed-form\nexpression for HIPE in the Gaussian Process setting and demonstrate its\neffectiveness through extensive experiments in active learning and few-shot BO.\nOur results show that HIPE outperforms standard initialization strategies in\nterms of predictive accuracy, hyperparameter identification, and subsequent\noptimization performance, particularly in large-batch, few-shot settings\nrelevant to many real-world Bayesian Optimization applications.", "AI": {"tldr": "\u5728\u6570\u636e\u91cf\u6709\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHIPE\u7684\u65b0\u578b\u8d1d\u53f6\u65af\u4f18\u5316\u521d\u59cb\u5316\u7b56\u7565\uff0c\u901a\u8fc7\u5e73\u8861\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u548c\u8d85\u53c2\u6570\u5b66\u4e60\u6765\u63d0\u9ad8\u4f18\u5316\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u8d1d\u53f6\u65af\u4f18\u5316\u521d\u59cb\u5316\u65b9\u6cd5\uff08\u5982\u968f\u673a\u8bbe\u8ba1\uff09\u5728\u6570\u636e\u91cf\u6709\u9650\u65f6\u6548\u679c\u4e0d\u4f73\uff0c\u56e0\u4e3a\u5b83\u4eec\u5ffd\u7565\u4e86\u7a7a\u95f4\u586b\u5145\u548c\u9ad8\u6548\u8d85\u53c2\u6570\u5b66\u4e60\u7684\u91cd\u8981\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHIPE\uff08Hyperparameter-Informed Predictive Exploration\uff09\u7684\u65b0\u578b\u91c7\u96c6\u7b56\u7565\uff0c\u5b83\u5229\u7528\u4fe1\u606f\u8bba\u539f\u7406\u6765\u5e73\u8861\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u964d\u4f4e\u548c\u8d85\u53c2\u6570\u5b66\u4e60\u3002\u5728Gaussian Process\uff08GP\uff09\u8bbe\u7f6e\u4e0b\u63a8\u5bfc\u4e86HIPE\u7684\u95ed\u5f0f\u8868\u8fbe\u5f0f\u3002", "result": "\u901a\u8fc7\u5728\u4e3b\u52a8\u5b66\u4e60\u548c\u5c11\u6837\u672c\u8d1d\u53f6\u65af\u4f18\u5316\uff08BO\uff09\u4e2d\u7684\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\uff0cHIPE\u5728\u9884\u6d4b\u51c6\u786e\u6027\u3001\u8d85\u53c2\u6570\u8bc6\u522b\u548c\u540e\u7eed\u4f18\u5316\u6027\u80fd\u65b9\u9762\u4f18\u4e8e\u6807\u51c6\u7684\u521d\u59cb\u5316\u7b56\u7565\uff0c\u5c24\u5176\u662f\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5e38\u89c1\u7684\u5927\u6279\u91cf\u3001\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\u3002", "conclusion": "HIPE\u662f\u4e00\u79cd\u6709\u6548\u7684\u65b0\u578b\u521d\u59cb\u5316\u7b56\u7565\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u8d1d\u53f6\u65af\u4f18\u5316\u5728\u6570\u636e\u91cf\u6709\u9650\u573a\u666f\u4e0b\u7684\u6027\u80fd\u3002"}}
{"id": "2510.24339", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24339", "abs": "https://arxiv.org/abs/2510.24339", "authors": ["Yunxuan Jiang", "Silan Hu", "Xiaoning Wang", "Yuanyuan Zhang", "Xiangyu Chang"], "title": "VDSAgents: A PCS-Guided Multi-Agent System for Veridical Data Science Automation", "comment": "29 pages, 6 figures. Yunxuan Jiang and Silan Hu contributed equally.\n  Code available at https://github.com/fengzer/VDSAgents", "summary": "Large language models (LLMs) become increasingly integrated into data science\nworkflows for automated system design. However, these LLM-driven data science\nsystems rely solely on the internal reasoning of LLMs, lacking guidance from\nscientific and theoretical principles. This limits their trustworthiness and\nrobustness, especially when dealing with noisy and complex real-world datasets.\nThis paper provides VDSAgents, a multi-agent system grounded in the\nPredictability-Computability-Stability (PCS) principles proposed in the\nVeridical Data Science (VDS) framework. Guided by PCS principles, the system\nimplements a modular workflow for data cleaning, feature engineering, modeling,\nand evaluation. Each phase is handled by an elegant agent, incorporating\nperturbation analysis, unit testing, and model validation to ensure both\nfunctionality and scientific auditability. We evaluate VDSAgents on nine\ndatasets with diverse characteristics, comparing it with state-of-the-art\nend-to-end data science systems, such as AutoKaggle and DataInterpreter, using\nDeepSeek-V3 and GPT-4o as backends. VDSAgents consistently outperforms the\nresults of AutoKaggle and DataInterpreter, which validates the feasibility of\nembedding PCS principles into LLM-driven data science automation.", "AI": {"tldr": "VDSAgents\u662f\u4e00\u4e2a\u57fa\u4e8e\u53ef\u9884\u6d4b\u6027-\u53ef\u8ba1\u7b97\u6027-\u7a33\u5b9a\u6027\uff08PCS\uff09\u539f\u5219\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u7528\u4e8e\u6570\u636e\u79d1\u5b66\u81ea\u52a8\u5316\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u5de5\u4f5c\u6d41\u548c\u79d1\u5b66\u5ba1\u8ba1\u63d0\u9ad8\u4e86LLM\u9a71\u52a8\u7684\u6570\u636e\u79d1\u5b66\u7cfb\u7edf\u7684\u53ef\u4fe1\u5ea6\u548c\u9c81\u68d2\u6027\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524dLLM\u9a71\u52a8\u7684\u6570\u636e\u79d1\u5b66\u7cfb\u7edf\u7f3a\u4e4f\u79d1\u5b66\u548c\u7406\u8bba\u539f\u5219\u7684\u6307\u5bfc\uff0c\u9650\u5236\u4e86\u5176\u5728\u5904\u7406\u590d\u6742\u73b0\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u65f6\u7684\u53ef\u4fe1\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "method": "VDSAgents\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u57fa\u4e8eVeridical Data Science (VDS)\u6846\u67b6\u4e2d\u7684PCS\u539f\u5219\u3002\u8be5\u7cfb\u7edf\u91c7\u7528\u6a21\u5757\u5316\u5de5\u4f5c\u6d41\uff0c\u5305\u62ec\u6570\u636e\u6e05\u6d17\u3001\u7279\u5f81\u5de5\u7a0b\u3001\u5efa\u6a21\u548c\u8bc4\u4f30\uff0c\u6bcf\u4e2a\u9636\u6bb5\u7531\u4e00\u4e2a\u72ec\u7acb\u7684\u667a\u80fd\u4f53\u5904\u7406\uff0c\u5e76\u7ed3\u5408\u6270\u52a8\u5206\u6790\u3001\u5355\u5143\u6d4b\u8bd5\u548c\u6a21\u578b\u9a8c\u8bc1\u3002", "result": "\u5728\u4e5d\u4e2a\u4e0d\u540c\u7279\u6027\u7684\u6570\u636e\u96c6\u4e0a\uff0cVDSAgents\u7684\u6027\u80fd\u4f18\u4e8eAutoKaggle\u548cDataInterpreter\u7b49\u73b0\u6709\u7aef\u5230\u7aef\u6570\u636e\u79d1\u5b66\u7cfb\u7edf\u3002", "conclusion": "\u5c06PCS\u539f\u5219\u5d4c\u5165LLM\u9a71\u52a8\u7684\u6570\u636e\u79d1\u5b66\u81ea\u52a8\u5316\u662f\u53ef\u884c\u7684\uff0cVDSAgents\u7684\u6027\u80fd\u9a8c\u8bc1\u4e86\u8fd9\u4e00\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.24208", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24208", "abs": "https://arxiv.org/abs/2510.24208", "authors": ["Jian Gu", "Aldeida Aleti", "Chunyang Chen", "Hongyu Zhang"], "title": "Beyond Neural Incompatibility: Easing Cross-Scale Knowledge Transfer in Large Language Models through Latent Semantic Alignment", "comment": "an early-stage version", "summary": "Large Language Models (LLMs) encode vast amounts of knowledge in their\nmassive parameters, which is accessible to locate, trace, and analyze. Despite\nadvances in neural interpretability, it is still not clear how to transfer\nknowledge in a fine-grained manner, namely parametric knowledge transfer (PKT).\nA key problem is enabling effective and efficient knowledge transfer across\nLLMs of different scales, which is essential for achieving greater flexibility\nand broader applicability in transferring knowledge between LLMs. Due to neural\nincompatibility, referring to the architectural and parametric differences\nbetween LLMs of varying scales, existing methods that directly reuse layer\nparameters are severely limited. In this paper, we identify the semantic\nalignment in latent space as the fundamental prerequisite for LLM cross-scale\nknowledge transfer. Instead of directly using the layer parameters, our\napproach takes activations as the medium of layer-wise knowledge transfer.\nLeveraging the semantics in latent space, our approach is simple and\noutperforms prior work, better aligning model behaviors across varying scales.\nEvaluations on four benchmarks demonstrate the efficacy of our method. Further\nanalysis reveals the key factors easing cross-scale knowledge transfer and\nprovides insights into the nature of latent semantic alignment.", "AI": {"tldr": "LLM\u77e5\u8bc6\u8fc1\u79fb\u5b58\u5728\u8de8\u5c3a\u5ea6\u96be\u9898\uff0c\u63d0\u51fa\u5229\u7528\u6f5c\u5728\u7a7a\u95f4\u8bed\u4e49\u5bf9\u9f50\uff0c\u901a\u8fc7\u6fc0\u6d3b\u503c\u8fdb\u884c\u77e5\u8bc6\u8fc1\u79fb\uff0c\u5b9e\u9a8c\u8bc1\u660e\u6709\u6548\u3002", "motivation": "LLM\u77e5\u8bc6\u8fc1\u79fb\u7684\u8de8\u5c3a\u5ea6\u6027\u4e0e\u7075\u6d3b\u6027\u9700\u6c42\uff0c\u4ee5\u53ca\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u4e0d\u540c\u5c3a\u5ea6LLM\u65f6\u5b58\u5728\u7684\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u5229\u7528\u6f5c\u5728\u7a7a\u95f4\u8bed\u4e49\u5bf9\u9f50\u4f5c\u4e3a\u8de8\u5c3a\u5ea6\u77e5\u8bc6\u8fc1\u79fb\u7684\u524d\u63d0\uff0c\u901a\u8fc7\u6fc0\u6d3b\u503c\u8fdb\u884c\u5c42\u7ea7\u77e5\u8bc6\u8fc1\u79fb\uff0c\u800c\u975e\u76f4\u63a5\u590d\u7528\u53c2\u6570\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u7b80\u5355\u4e14\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u591f\u66f4\u597d\u5730\u5728\u4e0d\u540c\u5c3a\u5ea6\u6a21\u578b\u95f4\u5bf9\u9f50\u884c\u4e3a\uff0c\u5e76\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u6f5c\u5728\u7a7a\u95f4\u8bed\u4e49\u5bf9\u9f50\u662fLLM\u8de8\u5c3a\u5ea6\u77e5\u8bc6\u8fc1\u79fb\u7684\u5173\u952e\uff0c\u6240\u63d0\u51fa\u7684\u57fa\u4e8e\u6fc0\u6d3b\u503c\u7684\u8fc1\u79fb\u65b9\u6cd5\u6709\u6548\u4e14\u5177\u6709\u6d1e\u5bdf\u529b\u3002"}}
{"id": "2510.24683", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24683", "abs": "https://arxiv.org/abs/2510.24683", "authors": ["Caleb Escobedo", "Nataliya Nechyporenko", "Shreyas Kadekodi", "Alessandro Roncone"], "title": "A Framework for the Systematic Evaluation of Obstacle Avoidance and Object-Aware Controllers", "comment": null, "summary": "Real-time control is an essential aspect of safe robot operation in the real\nworld with dynamic objects. We present a framework for the analysis of\nobject-aware controllers, methods for altering a robot's motion to anticipate\nand avoid possible collisions. This framework is focused on three design\nconsiderations: kinematics, motion profiles, and virtual constraints.\nAdditionally, the analysis in this work relies on verification of robot\nbehaviors using fundamental robot-obstacle experimental scenarios. To showcase\nthe effectiveness of our method we compare three representative object-aware\ncontrollers. The comparison uses metrics originating from the design\nconsiderations. From the analysis, we find that the design of object-aware\ncontrollers often lacks kinematic considerations, continuity of control points,\nand stability in movement profiles. We conclude that this framework can be used\nin the future to design, compare, and benchmark obstacle avoidance methods.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7528\u4e8e\u5206\u6790\u7269\u4f53\u611f\u77e5\u63a7\u5236\u5668\uff08object-aware controllers\uff09\u7684\u6846\u67b6\uff0c\u8be5\u63a7\u5236\u5668\u80fd\u9884\u6d4b\u5e76\u907f\u5f00\u53ef\u80fd\u7684\u78b0\u649e\u3002", "motivation": "\u673a\u5668\u4eba\u5fc5\u987b\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u5b89\u5168\u8fd0\u884c\uff0c\u5c24\u5176\u662f\u5728\u6709\u52a8\u6001\u7269\u4f53\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u65f6\u63a7\u5236\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u5305\u542b\u8fd0\u52a8\u5b66\u3001\u8fd0\u52a8\u8f6e\u5ed3\u548c\u865a\u62df\u7ea6\u675f\u4e09\u4e2a\u8bbe\u8ba1\u8981\u7d20\u7684\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u57fa\u7840\u7684\u673a\u5668\u4eba-\u969c\u788d\u7269\u5b9e\u9a8c\u573a\u666f\u9a8c\u8bc1\u673a\u5668\u4eba\u884c\u4e3a\u3002", "result": "\u5206\u6790\u53d1\u73b0\uff0c\u5f53\u524d\u7269\u4f53\u611f\u77e5\u63a7\u5236\u5668\u5728\u8fd0\u52a8\u5b66\u8003\u91cf\u3001\u63a7\u5236\u70b9\u8fde\u7eed\u6027\u548c\u8fd0\u52a8\u8f6e\u5ed3\u7a33\u5b9a\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "conclusion": "\u8be5\u6846\u67b6\u53ef\u7528\u4e8e\u672a\u6765\u8bbe\u8ba1\u3001\u6bd4\u8f83\u548c\u57fa\u51c6\u5316\u969c\u788d\u7269\u907f\u5f00\u65b9\u6cd5\u3002"}}
{"id": "2510.24195", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24195", "abs": "https://arxiv.org/abs/2510.24195", "authors": ["Ziqi Zhou", "Yifan Hu", "Yufei Song", "Zijing Li", "Shengshan Hu", "Leo Yu Zhang", "Dezhong Yao", "Long Zheng", "Hai Jin"], "title": "Vanish into Thin Air: Cross-prompt Universal Adversarial Attacks for SAM2", "comment": "Accepted by NeurIPS 2025", "summary": "Recent studies reveal the vulnerability of the image segmentation foundation\nmodel SAM to adversarial examples. Its successor, SAM2, has attracted\nsignificant attention due to its strong generalization capability in video\nsegmentation. However, its robustness remains unexplored, and it is unclear\nwhether existing attacks on SAM can be directly transferred to SAM2. In this\npaper, we first analyze the performance gap of existing attacks between SAM and\nSAM2 and highlight two key challenges arising from their architectural\ndifferences: directional guidance from the prompt and semantic entanglement\nacross consecutive frames. To address these issues, we propose UAP-SAM2, the\nfirst cross-prompt universal adversarial attack against SAM2 driven by dual\nsemantic deviation. For cross-prompt transferability, we begin by designing a\ntarget-scanning strategy that divides each frame into k regions, each randomly\nassigned a prompt, to reduce prompt dependency during optimization. For\neffectiveness, we design a dual semantic deviation framework that optimizes a\nUAP by distorting the semantics within the current frame and disrupting the\nsemantic consistency across consecutive frames. Extensive experiments on six\ndatasets across two segmentation tasks demonstrate the effectiveness of the\nproposed method for SAM2. The comparative results show that UAP-SAM2\nsignificantly outperforms state-of-the-art (SOTA) attacks by a large margin.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2510.24439", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.24439", "abs": "https://arxiv.org/abs/2510.24439", "authors": ["Tse-Yu Lin", "Wei-Kai Huang", "Pei-Yu Tu", "Yong-Fan Chen", "Ite A. Yu"], "title": "Fundamental limit on the heralded single photons' spectral brightness", "comment": "5 figures, 3 tables", "summary": "The heralded single photons' (HSPs) spectral brightness (SB) is defined as\nthe generation rate per linewidth. As the generation rate of HSPs gets larger\nor the photons' linewidth becomes narrower, both of which are desirable in\nquantum information processing using HSPs, does the SB have a limit? We\nsystematically studied the SB and the cross-correlation function, or\nequivalently, the signal-to-background ratio. The results in this study provide\nan answer applicable to all types of HSP sources. The answer relies on a newly\ndefined quantity, the quality factor, which reveals how a HSP source approaches\nthe ideal noise-free one. Furthermore, employing the HSP source based on hot\natomic vapor, we achieved an SB of (7.0$\\pm$0.3)$\\times10^5$ pairs/s/MHz and a\nquality factor of 0.68$\\pm$0.02 under the single-photon criterion. Both values\nare the highest records to date among all kinds of HSP sources.", "AI": {"tldr": "\u5355\u5149\u5b50\u6e90\u7684\u4f18\u503c\u56e0\u5b50\u88ab\u5b9a\u4e49\uff0c\u4e14\u5176\u503c\u8d8a\u9ad8\uff0c\u4ee3\u8868\u8d8a\u63a5\u8fd1\u7406\u60f3\u7684\u65e0\u566a\u58f0\u6e90\u3002\u7814\u7a76\u8868\u660e\uff0c\u4f18\u503c\u56e0\u5b50\u63ed\u793a\u4e86\u5355\u5149\u5b50\u6e90\u7684\u8c31\u4eae\u5ea6\u5b58\u5728\u4e00\u4e2a\u6781\u9650\uff0c\u5e76\u9996\u6b21\u5728\u5b9e\u9a8c\u4e2d\u8fbe\u5230\u66f4\u9ad8\u7684\u8c31\u4eae\u5ea6\u4e0e\u4f18\u503c\u56e0\u5b50\u3002", "motivation": "\u7814\u7a76\u5355\u5149\u5b50\u6e90\u7684\u8c31\u4eae\u5ea6\uff08SB\uff09\u6781\u9650\uff0c\u5e76\u63a2\u7d22\u63d0\u9ad8\u5176\u6027\u80fd\u7684\u65b9\u6cd5\u3002", "method": "\u7cfb\u7edf\u7814\u7a76\u4e86\u8c31\u4eae\u5ea6\uff08SB\uff09\u548c\u4ea4\u53c9\u76f8\u5173\u51fd\u6570\uff08\u7b49\u6548\u4e8e\u4fe1\u566a\u6bd4\uff09\uff0c\u5e76\u5f15\u5165\u4e86\u65b0\u7684\u91cf\u201c\u4f18\u503c\u56e0\u5b50\u201d\u6765\u8bc4\u4f30\u5355\u5149\u5b50\u6e90\u7684\u6027\u80fd\u3002", "result": "\u9996\u6b21\u5728\u57fa\u4e8e\u70ed\u539f\u5b50\u84b8\u6c14\u7684\u5355\u5149\u5b50\u6e90\u5b9e\u9a8c\u4e2d\uff0c\u8fbe\u5230\u4e86$7.0 \\pm 0.3 \\times 10^5$ \u5bf9/\u79d2/\u5146\u8d6b\u5179\u7684\u8c31\u4eae\u5ea6\uff08SB\uff09\u548c$0.68 \\pm 0.02$ \u7684\u4f18\u503c\u56e0\u5b50\uff0c\u5747\u4e3a\u5f53\u524d\u6240\u6709\u5355\u5149\u5b50\u6e90\u7c7b\u578b\u7684\u6700\u9ad8\u8bb0\u5f55\u3002", "conclusion": "\u8c31\u4eae\u5ea6\uff08SB\uff09\u5b58\u5728\u4e00\u4e2a\u6781\u9650\uff0c\u8be5\u6781\u9650\u53ef\u4ee5\u901a\u8fc7\u4f18\u503c\u56e0\u5b50\u6765\u8861\u91cf\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8e\u70ed\u539f\u5b50\u84b8\u6c14\u7684\u5355\u5149\u5b50\u6e90\u5728\u73b0\u6709\u6280\u672f\u4e0b\u8fbe\u5230\u4e86\u6700\u4f18\u6027\u80fd\u3002"}}
{"id": "2510.24342", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24342", "abs": "https://arxiv.org/abs/2510.24342", "authors": ["Silin Chen", "Yuzhong Chen", "Zifan Wang", "Junhao Wang", "Zifeng Jia", "Keith M Kendrick", "Tuo Zhang", "Lin Zhao", "Dezhong Yao", "Tianming Liu", "Xi Jiang"], "title": "A Unified Geometric Space Bridging AI Models and the Human Brain", "comment": null, "summary": "For decades, neuroscientists and computer scientists have pursued a shared\nambition: to understand intelligence and build it. Modern artificial neural\nnetworks now rival humans in language, perception, and reasoning, yet it is\nstill largely unknown whether these artificial systems organize information as\nthe brain does. Existing brain-AI alignment studies have shown the striking\ncorrespondence between the two systems, but such comparisons remain bound to\nspecific inputs and tasks, offering no common ground for comparing how AI\nmodels with different kinds of modalities-vision, language, or multimodal-are\nintrinsically organized. Here we introduce a groundbreaking concept of\nBrain-like Space: a unified geometric space in which every AI model can be\nprecisely situated and compared by mapping its intrinsic spatial attention\ntopological organization onto canonical human functional brain networks,\nregardless of input modality, task, or sensory domain. Our extensive analysis\nof 151 Transformer-based models spanning state-of-the-art large vision models,\nlarge language models, and large multimodal models uncovers a continuous\narc-shaped geometry within this space, reflecting a gradual increase of\nbrain-likeness; different models exhibit distinct distribution patterns within\nthis geometry associated with different degrees of brain-likeness, shaped not\nmerely by their modality but by whether the pretraining paradigm emphasizes\nglobal semantic abstraction and whether the positional encoding scheme\nfacilitates deep fusion across different modalities. Moreover, the degree of\nbrain-likeness for a model and its downstream task performance are not\n\"identical twins\". The Brain-like Space provides the first unified framework\nfor situating, quantifying, and comparing intelligence across domains,\nrevealing the deep organizational principles that bridge machines and the\nbrain.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a\u201c\u7c7b\u8111\u7a7a\u95f4\u201d\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u7528\u4e8e\u6bd4\u8f83\u4e0d\u540c\u6a21\u6001\uff08\u89c6\u89c9\u3001\u8bed\u8a00\u3001\u591a\u6a21\u6001\uff09\u7684AI\u6a21\u578b\u4e0e\u4eba\u8111\u7684\u7ec4\u7ec7\u65b9\u5f0f\uff0c\u53d1\u73b0AI\u6a21\u578b\u5728\u5927\u8111\u76f8\u4f3c\u6027\u4e0a\u5448\u73b0\u8fde\u7eed\u7684\u5f27\u5f62\u51e0\u4f55\u5206\u5e03\uff0c\u5e76\u4e0e\u9884\u8bad\u7ec3\u8303\u5f0f\u548c\u4f4d\u7f6e\u7f16\u7801\u65b9\u6848\u76f8\u5173\uff0c\u540c\u65f6\u5927\u8111\u76f8\u4f3c\u6027\u4e0e\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u5e76\u975e\u5b8c\u5168\u4e00\u81f4\u3002", "motivation": "\u7406\u89e3\u667a\u80fd\u7684\u672c\u8d28\u5e76\u8fdb\u884c\u6784\u5efa\uff0c\u63a2\u7a76\u73b0\u6709AI\u6a21\u578b\uff08\u5c24\u5176\u662fTransformer\uff09\u5728\u4fe1\u606f\u7ec4\u7ec7\u65b9\u5f0f\u4e0a\u662f\u5426\u4e0e\u4eba\u8111\u4e00\u81f4\uff0c\u5e76\u63d0\u4f9b\u4e00\u4e2a\u7edf\u4e00\u7684\u8de8\u6a21\u6001\u6bd4\u8f83\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u201c\u7c7b\u8111\u7a7a\u95f4\u201d\u6982\u5ff5\uff0c\u5c06AI\u6a21\u578b\u7684\u5185\u5728\u7a7a\u95f4\u6ce8\u610f\u529b\u62d3\u6251\u7ed3\u6784\u6620\u5c04\u5230\u4eba\u7c7b\u5927\u8111\u529f\u80fd\u7f51\u7edc\u4e0a\uff0c\u5e76\u5206\u6790\u4e86151\u4e2aTransformer\u6a21\u578b\uff08\u5305\u62ec\u89c6\u89c9\u3001\u8bed\u8a00\u548c\u591a\u6a21\u6001\u6a21\u578b\uff09\u5728\u8be5\u7a7a\u95f4\u4e2d\u7684\u5206\u5e03\u548c\u51e0\u4f55\u7279\u5f81\u3002", "result": "\u5728\u201c\u7c7b\u8111\u7a7a\u95f4\u201d\u4e2d\uff0cAI\u6a21\u578b\u5448\u73b0\u51fa\u8fde\u7eed\u7684\u5f27\u5f62\u51e0\u4f55\u5206\u5e03\uff0c\u8868\u660e\u5927\u8111\u76f8\u4f3c\u6027\u9010\u6e10\u589e\u52a0\u3002\u6a21\u578b\u7684\u5206\u5e03\u6a21\u5f0f\u4e0e\u5176\u5927\u8111\u76f8\u4f3c\u5ea6\u76f8\u5173\uff0c\u53d7\u9884\u8bad\u7ec3\u8303\u5f0f\uff08\u662f\u5426\u5f3a\u8c03\u5168\u5c40\u8bed\u4e49\u62bd\u8c61\uff09\u548c\u4f4d\u7f6e\u7f16\u7801\u65b9\u6848\uff08\u662f\u5426\u4fc3\u8fdb\u8de8\u6a21\u6001\u6df1\u5ea6\u878d\u5408\uff09\u7684\u5f71\u54cd\u3002\u5927\u8111\u76f8\u4f3c\u5ea6\u4e0e\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u5e76\u975e\u5b8c\u5168\u7b49\u540c\u3002", "conclusion": "\u201c\u7c7b\u8111\u7a7a\u95f4\u201d\u4e3a\u8de8\u9886\u57df\u667a\u80fd\u7684\u5b9a\u4f4d\u3001\u91cf\u5316\u548c\u6bd4\u8f83\u63d0\u4f9b\u4e86\u9996\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u8fde\u63a5\u673a\u5668\u548c\u5927\u8111\u7684\u6df1\u5c42\u7ec4\u7ec7\u539f\u7406\u3002AI\u6a21\u578b\u7684\u7ec4\u7ec7\u65b9\u5f0f\u4e0e\u5176\u5927\u8111\u76f8\u4f3c\u5ea6\u53d7\u8bad\u7ec3\u65b9\u5f0f\u548c\u7f16\u7801\u65b9\u6848\u5f71\u54cd\uff0c\u4e14\u5927\u8111\u76f8\u4f3c\u5ea6\u4e0d\u76f4\u63a5\u7b49\u540c\u4e8e\u4efb\u52a1\u6027\u80fd\u3002"}}
{"id": "2510.24222", "categories": ["cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.24222", "abs": "https://arxiv.org/abs/2510.24222", "authors": ["Adi Simhi", "Jonathan Herzig", "Itay Itzhak", "Dana Arad", "Zorik Gekhman", "Roi Reichart", "Fazl Barez", "Gabriel Stanovsky", "Idan Szpektor", "Yonatan Belinkov"], "title": "HACK: Hallucinations Along Certainty and Knowledge Axes", "comment": "The code is available at\n  https://github.com/technion-cs-nlp/HACK_Hallucinations_Along_Certainty_and_Knowledge_axes", "summary": "Hallucinations in LLMs present a critical barrier to their reliable usage.\nExisting research usually categorizes hallucination by their external\nproperties rather than by the LLMs' underlying internal properties. This\nexternal focus overlooks that hallucinations may require tailored mitigation\nstrategies based on their underlying mechanism. We propose a framework for\ncategorizing hallucinations along two axes: knowledge and certainty. Since\nparametric knowledge and certainty may vary across models, our categorization\nmethod involves a model-specific dataset construction process that\ndifferentiates between those types of hallucinations. Along the knowledge axis,\nwe distinguish between hallucinations caused by a lack of knowledge and those\noccurring despite the model having the knowledge of the correct response. To\nvalidate our framework along the knowledge axis, we apply steering mitigation,\nwhich relies on the existence of parametric knowledge to manipulate model\nactivations. This addresses the lack of existing methods to validate knowledge\ncategorization by showing a significant difference between the two\nhallucination types. We further analyze the distinct knowledge and\nhallucination patterns between models, showing that different hallucinations do\noccur despite shared parametric knowledge. Turning to the certainty axis, we\nidentify a particularly concerning subset of hallucinations where models\nhallucinate with certainty despite having the correct knowledge internally. We\nintroduce a new evaluation metric to measure the effectiveness of mitigation\nmethods on this subset, revealing that while some methods perform well on\naverage, they fail disproportionately on these critical cases. Our findings\nhighlight the importance of considering both knowledge and certainty in\nhallucination analysis and call for targeted mitigation approaches that\nconsider the hallucination underlying factors.", "AI": {"tldr": "LLM\u5e7b\u89c9\u5206\u4e3a\u77e5\u8bc6\u548c\u786e\u5b9a\u6027\u4e24\u4e2a\u7ef4\u5ea6\uff0c\u9700\u8981\u9488\u5bf9\u6027\u5730\u89e3\u51b3\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u4ece\u5916\u90e8\u7279\u6027\u800c\u975e\u5185\u90e8\u673a\u5236\u5bf9LLM\u5e7b\u89c9\u8fdb\u884c\u5206\u7c7b\uff0c\u5bfc\u81f4\u65e0\u6cd5\u8fdb\u884c\u9488\u5bf9\u6027\u89e3\u51b3\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5206\u7c7b\u6846\u67b6\uff0c\u8003\u8651\u4e86\u5185\u90e8\u77e5\u8bc6\u548c\u786e\u5b9a\u6027\u4e24\u4e2a\u7ef4\u5ea6\uff0c\u5e76\u5bf9\u4e0d\u540c\u7c7b\u578b\u7684\u5e7b\u89c9\u8fdb\u884c\u4e86\u9a8c\u8bc1\u548c\u5206\u6790\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u77e5\u8bc6\u548c\u786e\u5b9a\u6027\u4e24\u4e2a\u7ef4\u5ea6\u7684LLM\u5e7b\u89c9\u5206\u7c7b\u6846\u67b6\u3002\u5728\u77e5\u8bc6\u7ef4\u5ea6\u4e0a\uff0c\u533a\u5206\u4e86\u56e0\u7f3a\u4e4f\u77e5\u8bc6\u548c\u62e5\u6709\u77e5\u8bc6\u4f46\u4ecd\u4ea7\u751f\u5e7b\u89c9\u4e24\u79cd\u60c5\u51b5\uff0c\u5e76\u4f7f\u7528\u201csteering mitigation\u201d\u65b9\u6cd5\u8fdb\u884c\u9a8c\u8bc1\u3002\u5728\u786e\u5b9a\u6027\u7ef4\u5ea6\u4e0a\uff0c\u8bc6\u522b\u4e86\u201c\u6709\u77e5\u8bc6\u4f46\u786e\u4fe1\u5730\u4ea7\u751f\u5e7b\u89c9\u201d\u7684\u5b50\u96c6\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u901a\u8fc7\u201csteering mitigation\u201d\u65b9\u6cd5\u9a8c\u8bc1\u4e86\u77e5\u8bc6\u7ef4\u5ea6\u7684\u5206\u7c7b\u6709\u6548\u6027\uff0c\u8bc1\u660e\u4e86\u4e0d\u540c\u6a21\u578b\u5373\u4f7f\u62e5\u6709\u76f8\u540c\u77e5\u8bc6\u4e5f\u5b58\u5728\u4e0d\u540c\u7684\u5e7b\u89c9\u6a21\u5f0f\u3002\u65b0\u7684\u8bc4\u4f30\u6307\u6807\u63ed\u793a\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u201c\u6709\u77e5\u8bc6\u4f46\u786e\u4fe1\u5730\u4ea7\u751f\u5e7b\u89c9\u201d\u7684\u5b50\u96c6\u65f6\u5b58\u5728\u4e0d\u8db3\u3002", "conclusion": "LLM\u5e7b\u89c9\u7684\u5206\u6790\u5e94\u540c\u65f6\u8003\u8651\u77e5\u8bc6\u548c\u786e\u5b9a\u6027\u4e24\u4e2a\u7ef4\u5ea6\uff0c\u5e76\u5f00\u53d1\u9488\u5bf9\u6027\u7684\u7f13\u89e3\u7b56\u7565\u3002"}}
{"id": "2510.24692", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24692", "abs": "https://arxiv.org/abs/2510.24692", "authors": ["Jun Wang", "Ziyang Zhou", "Ardalan Kahak", "Suyi Li"], "title": "Embodying Physical Computing into Soft Robots", "comment": null, "summary": "Softening and onboarding computers and controllers is one of the final\nfrontiers in soft robotics towards their robustness and intelligence for\neveryday use. In this regard, embodying soft and physical computing presents\nexciting potential. Physical computing seeks to encode inputs into a mechanical\ncomputing kernel and leverage the internal interactions among this kernel's\nconstituent elements to compute the output. Moreover, such input-to-output\nevolution can be re-programmable. This perspective paper proposes a framework\nfor embodying physical computing into soft robots and discusses three unique\nstrategies in the literature: analog oscillators, physical reservoir computing,\nand physical algorithmic computing. These embodied computers enable the soft\nrobot to perform complex behaviors that would otherwise require CMOS-based\nelectronics -- including coordinated locomotion with obstacle avoidance,\npayload weight and orientation classification, and programmable operation based\non logical rules. This paper will detail the working principles of these\nembodied physical computing methods, survey the current state-of-the-art, and\npresent a perspective for future development.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u7269\u7406\u8ba1\u7b97\u5d4c\u5165\u8f6f\u673a\u5668\u4eba\u7684\u6846\u67b6\uff0c\u4ee5\u63d0\u9ad8\u5176\u9c81\u68d2\u6027\u548c\u667a\u80fd\u6027\uff0c\u5e76\u63a2\u8ba8\u4e86\u4e09\u79cd\u5b9e\u73b0\u7b56\u7565\uff1a\u6a21\u62df\u632f\u8361\u5668\u3001\u7269\u7406\u84c4\u6c34\u6c60\u8ba1\u7b97\u548c\u7269\u7406\u7b97\u6cd5\u8ba1\u7b97\u3002", "motivation": "\u8f6f\u4f53\u673a\u5668\u4eba\u9700\u8981\u5728\u65e5\u5e38\u5e94\u7528\u4e2d\u5b9e\u73b0\u66f4\u9ad8\u7684\u9c81\u68d2\u6027\u548c\u667a\u80fd\u6027\uff0c\u800c\u5c06\u8f6f\u4f53\u7269\u7406\u8ba1\u7b97\u5d4c\u5165\u673a\u5668\u4eba\u662f\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\u7684\u5173\u952e\u9014\u5f84\u4e4b\u4e00\u3002", "method": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u7269\u7406\u8ba1\u7b97\u5d4c\u5165\u8f6f\u673a\u5668\u4eba\u7684\u6846\u67b6\uff0c\u5e76\u91cd\u70b9\u4ecb\u7ecd\u4e86\u6a21\u62df\u632f\u8361\u5668\u3001\u7269\u7406\u84c4\u6c34\u6c60\u8ba1\u7b97\u548c\u7269\u7406\u7b97\u6cd5\u8ba1\u7b97\u8fd9\u4e09\u79cd\u5b9e\u73b0\u7b56\u7565\u3002", "result": "\u901a\u8fc7\u5d4c\u5165\u7269\u7406\u8ba1\u7b97\uff0c\u8f6f\u673a\u5668\u4eba\u80fd\u591f\u6267\u884c\u539f\u672c\u9700\u8981\u57fa\u4e8eCMOS\u7684\u7535\u5b50\u5668\u4ef6\u624d\u80fd\u5b8c\u6210\u7684\u590d\u6742\u884c\u4e3a\uff0c\u4f8b\u5982\u5177\u6709\u969c\u788d\u7269\u89c4\u907f\u80fd\u529b\u7684\u534f\u8c03\u8fd0\u52a8\u3001\u6709\u6548\u8f7d\u8377\u7684\u91cd\u91cf\u548c\u65b9\u5411\u5206\u7c7b\uff0c\u4ee5\u53ca\u57fa\u4e8e\u903b\u8f91\u89c4\u5219\u7684\u53ef\u7f16\u7a0b\u64cd\u4f5c\u3002", "conclusion": "\u8bba\u6587\u8be6\u7ec6\u4ecb\u7ecd\u4e86\u8fd9\u4e9b\u5d4c\u5165\u5f0f\u7269\u7406\u8ba1\u7b97\u7684\u5de5\u4f5c\u539f\u7406\uff0c\u56de\u987e\u4e86\u5f53\u524d\u7684\u7814\u7a76\u73b0\u72b6\uff0c\u5e76\u5bf9\u672a\u6765\u7684\u53d1\u5c55\u65b9\u5411\u8fdb\u884c\u4e86\u5c55\u671b\u3002"}}
{"id": "2510.24202", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24202", "abs": "https://arxiv.org/abs/2510.24202", "authors": ["Anshul Kaushal", "Kunal Jangid", "Vinod K. Kurmi"], "title": "CLFSeg: A Fuzzy-Logic based Solution for Boundary Clarity and Uncertainty Reduction in Medical Image Segmentation", "comment": "The 36th British Machine Vision Conference (BMVC) 2025", "summary": "Accurate polyp and cardiac segmentation for early detection and treatment is\nessential for the diagnosis and treatment planning of cancer-like diseases.\nTraditional convolutional neural network (CNN) based models have represented\nlimited generalizability, robustness, and inability to handle uncertainty,\nwhich affects the segmentation performance. To solve these problems, this paper\nintroduces CLFSeg, an encoder-decoder based framework that aggregates the\nFuzzy-Convolutional (FC) module leveraging convolutional layers and fuzzy\nlogic. This module enhances the segmentation performance by identifying local\nand global features while minimizing the uncertainty, noise, and ambiguity in\nboundary regions, ensuring computing efficiency. In order to handle class\nimbalance problem while focusing on the areas of interest with tiny and\nboundary regions, binary cross-entropy (BCE) with dice loss is incorporated.\nOur proposed model exhibits exceptional performance on four publicly available\ndatasets, including CVC-ColonDB, CVC-ClinicDB, EtisLaribPolypDB, and ACDC.\nExtensive experiments and visual studies show CLFSeg surpasses the existing\nSOTA performance and focuses on relevant regions of interest in anatomical\nstructures. The proposed CLFSeg improves performance while ensuring computing\nefficiency, which makes it a potential solution for real-world medical\ndiagnostic scenarios. Project page is available at\nhttps://visdomlab.github.io/CLFSeg/", "AI": {"tldr": "CLFSeg\u662f\u4e00\u4e2a\u57fa\u4e8e\u6a21\u7cca\u5377\u79ef\uff08FC\uff09\u6a21\u5757\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u9ad8\u606f\u8089\u548c\u5fc3\u810f\u5206\u5272\u7684\u51c6\u786e\u6027\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfCNN\u6cdb\u5316\u6027\u3001\u9c81\u68d2\u6027\u548c\u4e0d\u786e\u5b9a\u6027\u5904\u7406\u80fd\u529b\u6709\u9650\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8eCNN\u7684\u6a21\u578b\u5728\u6cdb\u5316\u6027\u3001\u9c81\u68d2\u6027\u548c\u4e0d\u786e\u5b9a\u6027\u5904\u7406\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u5f71\u54cd\u4e86\u5206\u5272\u6027\u80fd\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u63d0\u9ad8\u533b\u7597\u5f71\u50cf\u5206\u5272\u7684\u51c6\u786e\u6027\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u764c\u75c7\u7b49\u75be\u75c5\u7684\u65e9\u671f\u68c0\u6d4b\u548c\u6cbb\u7597\u89c4\u5212\u4e2d\u3002", "method": "\u63d0\u51faCLFSeg\u6846\u67b6\uff0c\u96c6\u6210\u4e86\u6a21\u7cca\u5377\u79ef\uff08FC\uff09\u6a21\u5757\uff0c\u8be5\u6a21\u5757\u5229\u7528\u5377\u79ef\u5c42\u548c\u6a21\u7cca\u903b\u8f91\u6765\u8bc6\u522b\u5c40\u90e8\u548c\u5168\u5c40\u7279\u5f81\uff0c\u540c\u65f6\u6700\u5c0f\u5316\u8fb9\u754c\u533a\u57df\u7684\u4e0d\u786e\u5b9a\u6027\u3001\u566a\u58f0\u548c\u6a21\u7cca\u6027\uff0c\u5e76\u91c7\u7528\u4e8c\u5143\u4ea4\u53c9\u71b5\uff08BCE\uff09\u7ed3\u5408Dice loss\u6765\u5904\u7406\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "result": "CLFSeg\u5728CVC-ColonDB\u3001CVC-ClinicDB\u3001EtisLaribPolypDB\u548cACDC\u56db\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u7684SOTA\u6027\u80fd\uff0c\u5e76\u4e14\u80fd\u591f\u5173\u6ce8\u89e3\u5256\u7ed3\u6784\u7684\u76f8\u5173\u533a\u57df\u3002", "conclusion": "CLFSeg\u6846\u67b6\u901a\u8fc7\u63d0\u9ad8\u5206\u5272\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u4e3a\u5b9e\u9645\u533b\u7597\u8bca\u65ad\u573a\u666f\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6f5c\u529b\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.24484", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.24484", "abs": "https://arxiv.org/abs/2510.24484", "authors": ["Anindita Sarkar", "Paranjoy Chaki", "Priya Ghosh", "Ujjwal Sen"], "title": "Comparing physical quantities with finite-precision: beyond standard metrology and an illustration for cooling in quantum processes", "comment": "10 pages, 2 figures", "summary": "We propose a general framework to compare the values of a physical quantity\npertaining to two - or more - physical setups, in the finite-precision\nscenario. Such a situation requires us to compare between two \"patches\" on the\nreal line instead of two numbers. Identification of extent of the patches is\ntypically done via standard deviation, as obtained within usual quantum\nmetrological considerations, but can not be always applied, especially for\nasymmetric error distributions. The extent can however be universally\ndetermined by utilizing the concept of percentiles of the probability\ndistribution of the corresponding estimator. As an application, we introduce\nthe concept of finite-precision cooling in a generic quantum system. We use\nthis approach in the working of a three-qubit quantum refrigerator governed by\nMarkovian dynamics, and demonstrate the occurrence of cooling within finite\nprecision for both transient and steady-state regimes, across strong- and\nweak-coupling limits of the inter-qubit interaction.", "AI": {"tldr": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u901a\u7528\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u6709\u9650\u7cbe\u5ea6\u4e0b\u6bd4\u8f83\u4e24\u4e2a\u6216\u591a\u4e2a\u7269\u7406\u7cfb\u7edf\u4e2d\u7684\u7269\u7406\u91cf\u7684\u53d6\u503c\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u767e\u5206\u4f4d\u6570\u7684\u6982\u5ff5\u6765\u786e\u5b9a\u6982\u7387\u5206\u5e03\u7684\u8303\u56f4\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6807\u51c6\u5dee\u65b9\u6cd5\u5728\u975e\u5bf9\u79f0\u8bef\u5dee\u5206\u5e03\u4e0b\u7684\u5c40\u9650\u6027\u3002\u4f5c\u4e3a\u5e94\u7528\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u6709\u9650\u7cbe\u5ea6\u51b7\u5374\u7684\u6982\u5ff5\uff0c\u5e76\u4ee5\u4e00\u4e2a\u4e09\u6bd4\u7279\u91cf\u5b50\u51b0\u7bb1\u4e3a\u4f8b\uff0c\u6f14\u793a\u4e86\u5728\u4e0d\u540c\u8026\u5408\u5f3a\u5ea6\u4e0b\uff0c\u7cfb\u7edf\u5728\u77ac\u6001\u548c\u7a33\u6001\u8fc7\u7a0b\u4e2d\u5747\u53ef\u5b9e\u73b0\u6709\u9650\u7cbe\u5ea6\u51b7\u5374\u3002", "motivation": "\u5728\u6709\u9650\u7cbe\u5ea6\u6d4b\u91cf\u573a\u666f\u4e0b\uff0c\u6bd4\u8f83\u7269\u7406\u91cf\u5728\u4e0d\u540c\u7269\u7406\u7cfb\u7edf\u4e2d\u7684\u53d6\u503c\u662f\u4e00\u4e2a\u666e\u904d\u5b58\u5728\u7684\u95ee\u9898\u3002\u4f20\u7edf\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u6807\u51c6\u5dee\u6765\u786e\u5b9a\u6d4b\u91cf\u503c\u7684\u8303\u56f4\uff0c\u4f46\u5728\u8bef\u5dee\u5206\u5e03\u4e0d\u5bf9\u79f0\u7684\u60c5\u51b5\u4e0b\u5b58\u5728\u5c40\u9650\u6027\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u901a\u7528\u7684\u65b9\u6cd5\u6765\u5904\u7406\u8fd9\u79cd\u60c5\u51b5\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u901a\u7528\u6846\u67b6\uff0c\u5229\u7528\u6982\u7387\u5206\u5e03\u7684\u767e\u5206\u4f4d\u6570\u6982\u5ff5\u6765\u786e\u5b9a\u7269\u7406\u91cf\u53d6\u503c\u7684\u8303\u56f4\uff0c\u4ee5\u5904\u7406\u6709\u9650\u7cbe\u5ea6\u6d4b\u91cf\u95ee\u9898\u3002\u5c06\u6b64\u6846\u67b6\u5e94\u7528\u4e8e\u91cf\u5b50\u7cfb\u7edf\u4e2d\uff0c\u7279\u522b\u662f\u5206\u6790\u4e09\u6bd4\u7279\u91cf\u5b50\u51b0\u7bb1\u5728\u9a6c\u5c14\u53ef\u592b\u52a8\u529b\u5b66\u4e0b\u7684\u6709\u9650\u7cbe\u5ea6\u51b7\u5374\u73b0\u8c61\uff0c\u5e76\u8003\u8651\u4e86\u5f3a\u8026\u5408\u548c\u5f31\u8026\u5408\u4e24\u79cd\u60c5\u51b5\u3002", "result": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u6570\u503c\u6a21\u62df\uff0c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7684\u6709\u9650\u7cbe\u5ea6\u51b7\u5374\u65b9\u6cd5\u5728\u4e09\u6bd4\u7279\u91cf\u5b50\u51b0\u7bb1\u4e2d\u662f\u6709\u6548\u7684\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u65e0\u8bba\u662f\u5728\u77ac\u6001\u8fd8\u662f\u7a33\u6001\u8fc7\u7a0b\u4e2d\uff0c\u65e0\u8bba\u662f\u5728\u5f3a\u8026\u5408\u8fd8\u662f\u5f31\u8026\u5408\u6781\u9650\u4e0b\uff0c\u7cfb\u7edf\u5747\u80fd\u5b9e\u73b0\u6709\u9650\u7cbe\u5ea6\u7684\u51b7\u5374\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8e\u767e\u5206\u4f4d\u6570\u7684\u6846\u67b6\u80fd\u591f\u6709\u6548\u5730\u5904\u7406\u6709\u9650\u7cbe\u5ea6\u6d4b\u91cf\u95ee\u9898\uff0c\u5e76\u4e3a\u91cf\u5b50\u7cfb\u7edf\u7684\u6709\u9650\u7cbe\u5ea6\u51b7\u5374\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u9014\u5f84\u3002\u8be5\u65b9\u6cd5\u5728\u5b9e\u9645\u7684\u91cf\u5b50\u70ed\u673a\u8bbe\u8ba1\u548c\u4f18\u5316\u4e2d\u5177\u6709\u6f5c\u5728\u7684\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.23685", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23685", "abs": "https://arxiv.org/abs/2510.23685", "authors": ["Junwen Ma", "Mingyu Ge", "Yisen Wang", "Yong Zhang", "Weicheng Fu"], "title": "Parallel BiLSTM-Transformer networks for forecasting chaotic dynamics", "comment": "9 pages,7 figures", "summary": "The nonlinear nature of chaotic systems results in extreme sensitivity to\ninitial conditions and highly intricate dynamical behaviors, posing fundamental\nchallenges for accurately predicting their evolution. To overcome the\nlimitation that conventional approaches fail to capture both local features and\nglobal dependencies in chaotic time series simultaneously, this study proposes\na parallel predictive framework integrating Transformer and Bidirectional Long\nShort-Term Memory (BiLSTM) networks. The hybrid model employs a dual-branch\narchitecture, where the Transformer branch mainly captures long-range\ndependencies while the BiLSTM branch focuses on extracting local temporal\nfeatures. The complementary representations from the two branches are fused in\na dedicated feature-fusion layer to enhance predictive accuracy. As\nillustrating examples, the model's performance is systematically evaluated on\ntwo representative tasks in the Lorenz system. The first is autonomous\nevolution prediction, in which the model recursively extrapolates system\ntrajectories from the time-delay embeddings of the state vector to evaluate\nlong-term tracking accuracy and stability. The second is inference of\nunmeasured variable, where the model reconstructs the unobserved states from\nthe time-delay embeddings of partial observations to assess its\nstate-completion capability. The results consistently indicate that the\nproposed hybrid framework outperforms both single-branch architectures across\ntasks, demonstrating its robustness and effectiveness in chaotic system\nprediction.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408Transformer\u548cBiLSTM\u7684\u6df7\u5408\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4b\u6df7\u6c8c\u7cfb\u7edf\u7684\u65f6\u95f4\u5e8f\u5217\uff0c\u5e76\u5728Lorenz\u7cfb\u7edf\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u6355\u6349\u6df7\u6c8c\u65f6\u95f4\u5e8f\u5217\u7684\u5c40\u90e8\u7279\u5f81\u548c\u5168\u5c40\u4f9d\u8d56\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u5e76\u884c\u9884\u6d4b\u6846\u67b6\uff0c\u878d\u5408Transformer\uff08\u6355\u6349\u957f\u7a0b\u4f9d\u8d56\uff09\u548cBiLSTM\uff08\u63d0\u53d6\u5c40\u90e8\u65f6\u5e8f\u7279\u5f81\uff09\u7684\u53cc\u5206\u652f\u67b6\u6784\uff0c\u5e76\u901a\u8fc7\u7279\u5f81\u878d\u5408\u5c42\u589e\u5f3a\u9884\u6d4b\u7cbe\u5ea6\u3002", "result": "\u5728Lorenz\u7cfb\u7edf\u4e0a\uff0c\u5bf9\u81ea\u4e3b\u6f14\u5316\u9884\u6d4b\u548c\u672a\u6d4b\u91cf\u53d8\u91cf\u63a8\u65ad\u4e24\u9879\u4efb\u52a1\u7684\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u6df7\u5408\u6a21\u578b\u4f18\u4e8e\u5355\u4e00\u5206\u652f\u6a21\u578b\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u6df7\u6c8c\u7cfb\u7edf\u9884\u6d4b\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6df7\u5408\u6a21\u578b\u80fd\u591f\u6709\u6548\u6355\u6349\u6df7\u6c8c\u65f6\u95f4\u5e8f\u5217\u7684\u5c40\u90e8\u548c\u5168\u5c40\u4f9d\u8d56\u6027\uff0c\u5728\u9884\u6d4b\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u4e8e\u5355\u4e00\u6a21\u578b\u3002"}}
{"id": "2510.24236", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24236", "abs": "https://arxiv.org/abs/2510.24236", "authors": ["Teague McMillan", "Gabriele Dominici", "Martin Gjoreski", "Marc Langheinrich"], "title": "Towards Transparent Reasoning: What Drives Faithfulness in Large Language Models?", "comment": "39th Conference on Neural Information Processing Systems (NeurIPS\n  2025) Workshop: NeurIPS 2025 Workshop on Evaluating the Evolving LLM\n  Lifecycle: Benchmarks, Emergent Abilities, and Scaling", "summary": "Large Language Models (LLMs) often produce explanations that do not\nfaithfully reflect the factors driving their predictions. In healthcare\nsettings, such unfaithfulness is especially problematic: explanations that omit\nsalient clinical cues or mask spurious shortcuts can undermine clinician trust\nand lead to unsafe decision support. We study how inference and training-time\nchoices shape explanation faithfulness, focusing on factors practitioners can\ncontrol at deployment. We evaluate three LLMs (GPT-4.1-mini, LLaMA 70B, LLaMA\n8B) on two datasets-BBQ (social bias) and MedQA (medical licensing questions),\nand manipulate the number and type of few-shot examples, prompting strategies,\nand training procedure. Our results show: (i) both the quantity and quality of\nfew-shot examples significantly impact model faithfulness; (ii) faithfulness is\nsensitive to prompting design; (iii) the instruction-tuning phase improves\nmeasured faithfulness on MedQA. These findings offer insights into strategies\nfor enhancing the interpretability and trustworthiness of LLMs in sensitive\ndomains.", "AI": {"tldr": "LLM\u5728\u533b\u7597\u7b49\u654f\u611f\u9886\u57df\u7f3a\u4e4f\u53ef\u4fe1\u5ea6\u7684\u89e3\u91ca\uff0c\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u63a8\u7406\u548c\u8bad\u7ec3\u65f6\u7684\u9009\u62e9\u5982\u4f55\u5f71\u54cd\u89e3\u91ca\u7684\u53ef\u4fe1\u5ea6\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u7b56\u7565\u3002", "motivation": "LLM\u5728\u533b\u7597\u573a\u666f\u4e0b\u63d0\u4f9b\u7684\u89e3\u91ca\u53ef\u80fd\u65e0\u6cd5\u51c6\u786e\u53cd\u6620\u5176\u9884\u6d4b\u4f9d\u636e\uff0c\u53ef\u80fd\u5305\u542b\u4e0d\u76f8\u5173\u7684\u7ebf\u7d22\u6216\u5229\u7528\u9519\u8bef\u7684\u5173\u8054\uff0c\u8fd9\u4f1a\u524a\u5f31\u4e34\u5e8a\u533b\u751f\u5bf9AI\u7684\u4fe1\u4efb\u5e76\u5bfc\u81f4\u4e0d\u5b89\u5168\u7684\u51b3\u7b56\u3002\u56e0\u6b64\uff0c\u7814\u7a76LLM\u89e3\u91ca\u53ef\u4fe1\u5ea6\u81f3\u5173\u91cd\u8981\u3002", "method": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u4e09\u79cdLLM\uff08GPT-4.1-mini, LLaMA 70B, LLaMA 8B\uff09\u5728\u4e24\u4e2a\u6570\u636e\u96c6\uff08BBQ\u793e\u4ea4\u504f\u89c1\u6570\u636e\u96c6\u548cMedQA\u533b\u7597\u8003\u8bd5\u6570\u636e\u96c6\uff09\u4e0a\u7684\u8868\u73b0\u3002\u7814\u7a76\u4eba\u5458\u901a\u8fc7\u8c03\u6574\u5c11\u6570\u793a\u4f8b\u7684\u6570\u91cf\u548c\u7c7b\u578b\u3001\u63d0\u793a\u7b56\u7565\u4ee5\u53ca\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u6765\u63a2\u7a76\u8fd9\u4e9b\u56e0\u7d20\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u89e3\u91ca\u7684\u53ef\u4fe1\u5ea6\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff1a(i) \u5c11\u6570\u793a\u4f8b\u7684\u6570\u91cf\u548c\u8d28\u91cf\u90fd\u4f1a\u663e\u8457\u5f71\u54cd\u6a21\u578b\u89e3\u91ca\u7684\u53ef\u4fe1\u5ea6\uff1b(ii) \u89e3\u91ca\u7684\u53ef\u4fe1\u5ea6\u5bf9\u63d0\u793a\u8bbe\u8ba1\u654f\u611f\uff1b(iii) \u6307\u4ee4\u5fae\u8c03\u9636\u6bb5\u63d0\u9ad8\u4e86MedQA\u6570\u636e\u96c6\u4e0a\u7684\u6a21\u578b\u89e3\u91ca\u53ef\u4fe1\u5ea6\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u63d0\u9ad8LLM\u5728\u654f\u611f\u9886\u57df\u7684\u89e3\u91ca\u6027\u548c\u53ef\u4fe1\u5ea6\u63d0\u4f9b\u4e86\u7b56\u7565\u6027\u89c1\u89e3\u3002"}}
{"id": "2510.24211", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24211", "abs": "https://arxiv.org/abs/2510.24211", "authors": ["Junhyuk So", "Hyunho Kook", "Chaeyeon Jang", "Eunhyeok Park"], "title": "MC-SJD : Maximal Coupling Speculative Jacobi Decoding for Autoregressive Visual Generation Acceleration", "comment": null, "summary": "While autoregressive (AR) modeling has recently emerged as a new paradigm in\nvisual generation, its practical adoption is severely constrained by the slow\ninference speed of per-token generation, which often requires thousands of\nsteps to produce a single sample. To address this challenge, we propose MC-SJD,\na training-free, lossless parallel decoding framework designed to accelerate AR\nvisual generation by extending the recently introduced Speculative Jacobi\nDecoding (SJD). Although SJD shows strong potential for accelerating AR\ngeneration, we demonstrate that token instability across iterations\nsignificantly reduces the acceptance rate, a limitation that primarily arises\nfrom the independent sampling process used during draft token generation. To\novercome this, we introduce MC-SJD, an information-theoretic approach based on\ncoupling, which substantially accelerates standard SJD by maximizing the\nprobability of sampling identical draft tokens across consecutive iterations,\nall while preserving its lossless property. Remarkably, this method requires\nonly a single-line modification to the existing algorithm, yet achieves\nsubstantial performance gains, delivering up to a ~4.2x acceleration in image\ngeneration and ~13.3x acceleration in video generation compared to standard AR\ndecoding, without any degradation in output quality.", "AI": {"tldr": "MC-SJD\u662f\u4e00\u79cd\u8bad\u7ec3\u514d\u8d39\u3001\u65e0\u635f\u7684\u5e76\u884c\u89e3\u7801\u6846\u67b6\uff0c\u901a\u8fc7\u6269\u5c55Speculative Jacobi Decoding\uff08SJD\uff09\u6765\u52a0\u901f\u81ea\u56de\u5f52\uff08AR\uff09\u89c6\u89c9\u751f\u6210\uff0c\u89e3\u51b3\u4e86AR\u6a21\u578b\u63a8\u7406\u901f\u5ea6\u6162\u7684\u95ee\u9898\u3002\u901a\u8fc7\u5f15\u5165\u57fa\u4e8e\u8026\u5408\u7684\u4fe1\u606f\u8bba\u65b9\u6cd5\uff0cMC-SJD\u6700\u5927\u5316\u8fde\u7eed\u8fed\u4ee3\u4e4b\u95f4\u751f\u6210\u76f8\u540c\u8349\u7a3f\u4ee4\u724c\u7684\u6982\u7387\uff0c\u4ece\u800c\u663e\u8457\u63d0\u9ad8\u4e86SJD\u7684\u63a5\u53d7\u7387\uff0c\u5728\u56fe\u50cf\u751f\u6210\u65b9\u9762\u5b9e\u73b0\u4e86\u9ad8\u8fbe\u7ea64.2\u500d\u7684\u52a0\u901f\uff0c\u5728\u89c6\u9891\u751f\u6210\u65b9\u9762\u5b9e\u73b0\u4e86\u9ad8\u8fbe\u7ea613.3\u500d\u7684\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8f93\u51fa\u8d28\u91cf\u3002", "motivation": "\u81ea\u56de\u5f52\uff08AR\uff09\u6a21\u578b\u5728\u89c6\u89c9\u751f\u6210\u9886\u57df\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u5176\u9010\u4e2a\u4ee4\u724c\u751f\u6210\u7684\u7f13\u6162\u63a8\u7406\u901f\u5ea6\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u63d0\u51faMC-SJD\uff0c\u4e00\u79cd\u8bad\u7ec3\u514d\u8d39\u3001\u65e0\u635f\u7684\u5e76\u884c\u89e3\u7801\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u57fa\u4e8e\u8026\u5408\u7684\u4fe1\u606f\u8bba\u65b9\u6cd5\uff0c\u6700\u5927\u5316\u8fde\u7eed\u8fed\u4ee3\u4e4b\u95f4\u751f\u6210\u76f8\u540c\u8349\u7a3f\u4ee4\u724c\u7684\u6982\u7387\uff0c\u4ece\u800c\u52a0\u901fSJD\u3002", "result": "MC-SJD\u5728\u56fe\u50cf\u751f\u6210\u65b9\u9762\u5b9e\u73b0\u4e86\u9ad8\u8fbe\u7ea64.2\u500d\u7684\u52a0\u901f\uff0c\u5728\u89c6\u9891\u751f\u6210\u65b9\u9762\u5b9e\u73b0\u4e86\u9ad8\u8fbe\u7ea613.3\u500d\u7684\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8f93\u51fa\u8d28\u91cf\u3002", "conclusion": "MC-SJD\u901a\u8fc7\u4e00\u79cd\u7b80\u5355\u7684\u4fee\u6539\uff0c\u663e\u8457\u63d0\u9ad8\u4e86SJD\u7684\u6027\u80fd\uff0c\u89e3\u51b3\u4e86AR\u89c6\u89c9\u751f\u6210\u4e2d\u7684\u63a8\u7406\u901f\u5ea6\u74f6\u9888\uff0c\u800c\u4e0d\u4f1a\u5f71\u54cd\u751f\u6210\u8d28\u91cf\u3002"}}
{"id": "2510.24509", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.24509", "abs": "https://arxiv.org/abs/2510.24509", "authors": ["Carlos Flores-Garrigos", "Gaurav Dev", "Michael Falkenthal", "Alejandro Gomez Cadavid", "Anton Simen", "Shubham Kumar", "Enrique Solano", "Narendra N. Hegade"], "title": "Quantum Combinatorial Reasoning for Large Language Models", "comment": null, "summary": "We design and implement a quantum combinatorial reasoning framework for large\nlanguage models (QCR-LLM), integrating a real quantum computer in the hybrid\nworkflow. QCR-LLM reformulates reasoning aggregation as a higher-order\nunconstrained binary optimization (HUBO) problem. In this sense, reasoning\nfragments are represented as binary variables and their interactions encode\nstatistical relevance, logical coherence, and semantic redundancy. We tackle\nthe resulting high-order optimization problem both classically, via simulated\nannealing, and quantumly through the bias-field digitized counterdiabatic\nquantum optimizer (BF-DCQO) executed on IBM's superconducting digital quantum\nprocessors. Experiments on BIG-Bench Extra Hard (BBEH) benchmarks demonstrate\nthat our QCR-LLM consistently improves reasoning accuracy across multiple LLM\nbackbones, surpassing reasoning-native systems such as o3-high and DeepSeek R1\nby up to $+9\\,$pp. Despite requiring multiple reasoning samples per query, our\nQCR-LLM remains approximately five times more energy-efficient than o3-high,\nowing to the low per-token energy footprint of its GPT-4o backbone. These\nresults constitute the first experimental evidence of quantum-assisted\nreasoning, showing that hybrid quantum-classical optimization can efficiently\nenhance reasoning coherence, interpretability, and sustainability in\nlarge-scale language models. We have opened the doors to the emergence of\nquantum intelligence, where harder prompts require quantum optimizers at\nquantum-advantage level.", "AI": {"tldr": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a QCR-LLM \u7684\u91cf\u5b50\u7ec4\u5408\u63a8\u7406\u6846\u67b6\uff0c\u5b83\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0e\u91cf\u5b50\u8ba1\u7b97\u673a\u76f8\u7ed3\u5408\u3002\u8be5\u6846\u67b6\u5c06\u63a8\u7406\u8fc7\u7a0b\u8f6c\u5316\u4e3a\u4e00\u4e2a\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u4f7f\u7528\u7ecf\u5178\u548c\u91cf\u5b50\u65b9\u6cd5\u6765\u89e3\u51b3\u3002\u5b9e\u9a8c\u8868\u660e\uff0cQCR-LLM \u63d0\u9ad8\u4e86\u63a8\u7406\u51c6\u786e\u6027\uff0c\u5e76\u4e14\u6bd4\u73b0\u6709\u7684\u7cfb\u7edf\u66f4\u8282\u80fd\u3002", "motivation": "\u5c06\u91cf\u5b50\u8ba1\u7b97\u80fd\u529b\u6574\u5408\u5230\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\uff0c\u4ee5\u589e\u5f3a\u5176\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u63a2\u7d22\u91cf\u5b50\u8f85\u52a9\u63a8\u7406\u7684\u53ef\u884c\u6027\u3002", "method": "\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u8fc7\u7a0b\u91cd\u65b0\u8868\u8ff0\u4e3a\u9ad8\u9636\u65e0\u7ea6\u675f\u4e8c\u5143\u4f18\u5316\uff08HUBO\uff09\u95ee\u9898\uff0c\u5e76\u4f7f\u7528\u7ecf\u5178\u6a21\u62df\u9000\u706b\u548c\u91cf\u5b50\u8ba1\u7b97\uff08BF-DCQO\uff09\u8fdb\u884c\u6c42\u89e3\u3002", "result": "QCR-LLM \u5728 BIG-Bench Extra Hard (BBEH) \u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5728\u591a\u4e2a LLM \u540e\u7aef\u4e0a\u4e00\u81f4\u5730\u63d0\u9ad8\u4e86\u63a8\u7406\u51c6\u786e\u6027\uff0c\u8d85\u8d8a\u4e86 o3-high \u548c DeepSeek R1 \u7b49\u7cfb\u7edf\uff0c\u51c6\u786e\u7387\u63d0\u5347\u9ad8\u8fbe +9pp\u3002\u6b64\u5916\uff0cQCR-LLM \u7684\u80fd\u6548\u6bd4 o3-high \u9ad8\u7ea6\u4e94\u500d\u3002", "conclusion": "\u5b9e\u9a8c\u8bc1\u660e\u4e86\u91cf\u5b50\u8f85\u52a9\u63a8\u7406\u7684\u6709\u6548\u6027\uff0c\u8868\u660e\u91cf\u5b50-\u7ecf\u5178\u6df7\u5408\u4f18\u5316\u80fd\u591f\u63d0\u9ad8\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u8fde\u8d2f\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u6301\u7eed\u6027\u65b9\u9762\u7684\u8868\u73b0\u3002\u8fd9\u4e3a\u91cf\u5b50\u667a\u80fd\u7684\u51fa\u73b0\u5f00\u8f9f\u4e86\u9053\u8def\uff0c\u672a\u6765\u66f4\u590d\u6742\u7684\u63d0\u793a\u5c06\u9700\u8981\u91cf\u5b50\u4f18\u5316\u5668\u5b9e\u73b0\u91cf\u5b50\u4f18\u52bf\u3002"}}
{"id": "2510.23693", "categories": ["cs.LG", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.23693", "abs": "https://arxiv.org/abs/2510.23693", "authors": ["Joachim Baumann"], "title": "On the Societal Impact of Machine Learning", "comment": "PhD thesis", "summary": "This PhD thesis investigates the societal impact of machine learning (ML). ML\nincreasingly informs consequential decisions and recommendations, significantly\naffecting many aspects of our lives. As these data-driven systems are often\ndeveloped without explicit fairness considerations, they carry the risk of\ndiscriminatory effects. The contributions in this thesis enable more\nappropriate measurement of fairness in ML systems, systematic decomposition of\nML systems to anticipate bias dynamics, and effective interventions that reduce\nalgorithmic discrimination while maintaining system utility. I conclude by\ndiscussing ongoing challenges and future research directions as ML systems,\nincluding generative artificial intelligence, become increasingly integrated\ninto society. This work offers a foundation for ensuring that ML's societal\nimpact aligns with broader social values.", "AI": {"tldr": "\u673a\u5668\u5b66\u4e60\u65e5\u76ca\u5f71\u54cd\u6211\u4eec\u751f\u6d3b\u7684\u65b9\u65b9\u9762\u9762\uff0c\u4f46\u5176\u7f3a\u4e4f\u660e\u786e\u7684\u516c\u5e73\u6027\u8003\u91cf\u53ef\u80fd\u5bfc\u81f4\u6b67\u89c6\u3002\u672c\u8bba\u6587\u65e8\u5728\u6539\u8fdb\u673a\u5668\u5b66\u4e60\u516c\u5e73\u6027\u5ea6\u91cf\uff0c\u5206\u89e3\u7cfb\u7edf\u4ee5\u9884\u6d4b\u504f\u89c1\uff0c\u5e76\u63d0\u51fa\u51cf\u5c11\u7b97\u6cd5\u6b67\u89c6\u4e14\u4e0d\u635f\u5bb3\u6548\u7528\u7684\u65b9\u6cd5\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u65e5\u76ca\u5f71\u54cd\u6211\u4eec\u751f\u6d3b\u7684\u65b9\u65b9\u9762\u9762\uff0c\u4f46\u5176\u7f3a\u4e4f\u660e\u786e\u7684\u516c\u5e73\u6027\u8003\u91cf\u53ef\u80fd\u5bfc\u81f4\u6b67\u89c6\u3002\u672c\u8bba\u6587\u65e8\u5728\u6539\u8fdb\u673a\u5668\u5b66\u4e60\u516c\u5e73\u6027\u5ea6\u91cf\uff0c\u5206\u89e3\u7cfb\u7edf\u4ee5\u9884\u6d4b\u504f\u89c1\uff0c\u5e76\u63d0\u51fa\u51cf\u5c11\u7b97\u6cd5\u6b67\u89c6\u4e14\u4e0d\u635f\u5bb3\u6548\u7528\u7684\u65b9\u6cd5\u3002", "method": "\u672c\u8bba\u6587\u63d0\u51fa\u66f4\u6070\u5f53\u7684\u516c\u5e73\u6027\u5ea6\u91cf\u65b9\u6cd5\uff0c\u7cfb\u7edf\u6027\u5206\u89e3\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u4ee5\u9884\u6d4b\u504f\u89c1\u52a8\u6001\uff0c\u5e76\u8fdb\u884c\u6709\u6548\u5e72\u9884\u4ee5\u51cf\u5c11\u7b97\u6cd5\u6b67\u89c6\uff0c\u540c\u65f6\u4fdd\u6301\u7cfb\u7edf\u6548\u7528\u3002", "result": "\u672c\u8bba\u6587\u63d0\u51fa\u66f4\u6070\u5f53\u7684\u516c\u5e73\u6027\u5ea6\u91cf\u65b9\u6cd5\uff0c\u7cfb\u7edf\u6027\u5206\u89e3\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u4ee5\u9884\u6d4b\u504f\u89c1\u52a8\u6001\uff0c\u5e76\u8fdb\u884c\u6709\u6548\u5e72\u9884\u4ee5\u51cf\u5c11\u7b97\u6cd5\u6b67\u89c6\uff0c\u540c\u65f6\u4fdd\u6301\u7cfb\u7edf\u6548\u7528\u3002", "conclusion": "\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\uff08\u5305\u62ec\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\uff09\u65e5\u76ca\u878d\u5165\u793e\u4f1a\uff0c\u5e26\u6765\u4e86\u6301\u7eed\u7684\u6311\u6218\u548c\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u3002\u672c\u7814\u7a76\u4e3a\u786e\u4fdd\u673a\u5668\u5b66\u4e60\u7684\u793e\u4f1a\u5f71\u54cd\u4e0e\u66f4\u5e7f\u6cdb\u7684\u793e\u4f1a\u4ef7\u503c\u89c2\u4fdd\u6301\u4e00\u81f4\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2510.24247", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24247", "abs": "https://arxiv.org/abs/2510.24247", "authors": ["Ahmad Ghannam", "Naif Alharthi", "Faris Alasmary", "Kholood Al Tabash", "Shouq Sadah", "Lahouari Ghouti"], "title": "Abjad AI at NADI 2025: CATT-Whisper: Multimodal Diacritic Restoration Using Text and Speech Representations", "comment": null, "summary": "In this work, we tackle the Diacritic Restoration (DR) task for Arabic\ndialectal sentences using a multimodal approach that combines both textual and\nspeech information. We propose a model that represents the text modality using\nan encoder extracted from our own pre-trained model named CATT. The speech\ncomponent is handled by the encoder module of the OpenAI Whisper base model.\nOur solution is designed following two integration strategies. The former\nconsists of fusing the speech tokens with the input at an early stage, where\nthe 1500 frames of the audio segment are averaged over 10 consecutive frames,\nresulting in 150 speech tokens. To ensure embedding compatibility, these\naveraged tokens are processed through a linear projection layer prior to\nmerging them with the text tokens. Contextual encoding is guaranteed by the\nCATT encoder module. The latter strategy relies on cross-attention, where text\nand speech embeddings are fused. The cross-attention output is then fed to the\nCATT classification head for token-level diacritic prediction. To further\nimprove model robustness, we randomly deactivate the speech input during\ntraining, allowing the model to perform well with or without speech. Our\nexperiments show that the proposed approach achieves a word error rate (WER) of\n0.25 and a character error rate (CER) of 0.9 on the development set. On the\ntest set, our model achieved WER and CER scores of 0.55 and 0.13, respectively.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6587\u672c\u548c\u8bed\u97f3\u4fe1\u606f\u7684\u963f\u62c9\u4f2f\u8bed\u65b9\u8a00\u8bed\u8c03\u6062\u590d\uff08DR\uff09\u7684\u591a\u6a21\u6001\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u963f\u62c9\u4f2f\u8bed\u65b9\u8a00\u7684\u8bed\u8c03\u6062\u590d\uff08DR\uff09\u4efb\u52a1\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6587\u672c\u548c\u8bed\u97f3\u4fe1\u606f\u7684\u591a\u6a21\u6001\u6a21\u578b\u3002\u6587\u672c\u90e8\u5206\u4f7f\u7528CATT\u9884\u8bad\u7ec3\u6a21\u578b\u63d0\u53d6\u7f16\u7801\u5668\uff0c\u8bed\u97f3\u90e8\u5206\u4f7f\u7528OpenAI Whisper\u57fa\u7840\u6a21\u578b\u7684\u7f16\u7801\u5668\u3002\u6a21\u578b\u91c7\u7528\u4e86\u4e24\u79cd\u96c6\u6210\u7b56\u7565\uff1a\u4e00\u79cd\u662f\u65e9\u671f\u878d\u5408\uff0c\u5c06\u8bed\u97f3\u4fe1\u53f7\u5e73\u5747\u6210150\u4e2a\u8bed\u97f3\u6807\u8bb0\uff0c\u5e76\u901a\u8fc7\u7ebf\u6027\u6295\u5f71\u5c42\u4e0e\u6587\u672c\u6807\u8bb0\u5408\u5e76\uff1b\u53e6\u4e00\u79cd\u662f\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\uff0c\u878d\u5408\u6587\u672c\u548c\u8bed\u97f3\u5d4c\u5165\uff0c\u7136\u540e\u5c06\u7ed3\u679c\u8f93\u5165CATT\u5206\u7c7b\u5934\u8fdb\u884c\u6807\u8bb0\u7ea7\u8bed\u8c03\u9884\u6d4b\u3002\u4e3a\u63d0\u9ad8\u6a21\u578b\u9c81\u68d2\u6027\uff0c\u8bad\u7ec3\u65f6\u968f\u673a\u7981\u7528\u8bed\u97f3\u8f93\u5165\u3002", "result": "\u5728\u5f00\u53d1\u96c6\u4e0a\uff0c\u6a21\u578b\u7684\u8bcd\u9519\u8bef\u7387\uff08WER\uff09\u4e3a0.25\uff0c\u5b57\u7b26\u9519\u8bef\u7387\uff08CER\uff09\u4e3a0.9\u3002\u5728\u6d4b\u8bd5\u96c6\u4e0a\uff0cWER\u4e3a0.55\uff0cCER\u4e3a0.13\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u591a\u6a21\u6001\u65b9\u6cd5\u5728\u963f\u62c9\u4f2f\u8bed\u65b9\u8a00\u7684\u8bed\u8c03\u6062\u590d\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6548\u679c\u3002"}}
{"id": "2510.24213", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24213", "abs": "https://arxiv.org/abs/2510.24213", "authors": ["Haoxin Yang", "Yihong Lin", "Jingdan Kang", "Xuemiao Xu", "Yue Li", "Cheng Xu", "Shengfeng He"], "title": "Beyond Inference Intervention: Identity-Decoupled Diffusion for Face Anonymization", "comment": null, "summary": "Face anonymization aims to conceal identity information while preserving\nnon-identity attributes. Mainstream diffusion models rely on inference-time\ninterventions such as negative guidance or energy-based optimization, which are\napplied post-training to suppress identity features. These interventions often\nintroduce distribution shifts and entangle identity with non-identity\nattributes, degrading visual fidelity and data utility. To address this, we\npropose \\textbf{ID\\textsuperscript{2}Face}, a training-centric anonymization\nframework that removes the need for inference-time optimization. The rationale\nof our method is to learn a structured latent space where identity and\nnon-identity information are explicitly disentangled, enabling direct and\ncontrollable anonymization at inference. To this end, we design a conditional\ndiffusion model with an identity-masked learning scheme. An Identity-Decoupled\nLatent Recomposer uses an Identity Variational Autoencoder to model identity\nfeatures, while non-identity attributes are extracted from same-identity pairs\nand aligned through bidirectional latent alignment. An Identity-Guided Latent\nHarmonizer then fuses these representations via soft-gating conditioned on\nnoisy feature prediction. The model is trained with a recomposition-based\nreconstruction loss to enforce disentanglement. At inference, anonymization is\nachieved by sampling a random identity vector from the learned identity space.\nTo further suppress identity leakage, we introduce an Orthogonal Identity\nMapping strategy that enforces orthogonality between sampled and source\nidentity vectors. Experiments demonstrate that ID\\textsuperscript{2}Face\noutperforms existing methods in visual quality, identity suppression, and\nutility preservation.", "AI": {"tldr": "ID\textsuperscript{2}Face\u662f\u4e00\u79cd\u8bad\u7ec3\u4e2d\u5fc3\u7684\u975e\u8eab\u4efd\u8bc6\u522b\u6846\u67b6\uff0c\u901a\u8fc7\u5b66\u4e60\u7ed3\u6784\u5316\u6f5c\u5728\u7a7a\u95f4\u6765\u89e3\u8026\u8eab\u4efd\u548c\u975e\u8eab\u4efd\u4fe1\u606f\uff0c\u4ece\u800c\u5728\u63a8\u7406\u65f6\u8fdb\u884c\u76f4\u63a5\u53ef\u63a7\u7684\u975e\u8eab\u4efd\u8bc6\u522b\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u4e3b\u6d41\u9762\u90e8\u975e\u8eab\u4efd\u8bc6\u522b\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u63a8\u7406\u65f6\u5e72\u9884\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u5206\u5e03\u504f\u79fb\uff0c\u5e76\u524a\u5f31\u8eab\u4efd\u4e0e\u975e\u8eab\u4efd\u5c5e\u6027\u7684\u8054\u7cfb\uff0c\u4ece\u800c\u964d\u4f4e\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u6570\u636e\u6548\u7528\u3002", "method": "ID\textsuperscript{2}Face\u901a\u8fc7\u8bbe\u8ba1\u4e00\u4e2a\u6761\u4ef6\u6269\u6563\u6a21\u578b\u548c\u4e00\u4e2a\u8eab\u4efd\u5c4f\u853d\u5b66\u4e60\u65b9\u6848\u6765\u5b9e\u73b0\u3002\u5b83\u5305\u542b\u4e00\u4e2a\u8eab\u4efd\u89e3\u8026\u6f5c\u5728\u91cd\u7ec4\u5668\uff0c\u4f7f\u7528\u8eab\u4efd\u53d8\u5206\u81ea\u7f16\u7801\u5668\u6765\u5efa\u6a21\u8eab\u4efd\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u53cc\u5411\u6f5c\u5728\u5bf9\u9f50\u6765\u63d0\u53d6\u548c\u5bf9\u9f50\u540c\u4e00\u8eab\u4efd\u5bf9\u7684\u975e\u8eab\u4efd\u5c5e\u6027\u3002\u4e00\u4e2a\u8eab\u4efd\u5f15\u5bfc\u6f5c\u5728\u534f\u8c03\u5668\u901a\u8fc7\u57fa\u4e8e\u8f6f\u95e8\u63a7\u7684\u566a\u58f0\u7279\u5f81\u9884\u6d4b\u6765\u878d\u5408\u8fd9\u4e9b\u8868\u793a\u3002\u8be5\u6a21\u578b\u4f7f\u7528\u57fa\u4e8e\u91cd\u7ec4\u7684\u91cd\u5efa\u635f\u5931\u8fdb\u884c\u8bad\u7ec3\uff0c\u4ee5\u5f3a\u5236\u6267\u884c\u89e3\u8026\u3002\u5728\u63a8\u7406\u65f6\uff0c\u901a\u8fc7\u4ece\u5b66\u4e60\u5230\u7684\u8eab\u4efd\u7a7a\u95f4\u4e2d\u91c7\u6837\u4e00\u4e2a\u968f\u673a\u8eab\u4efd\u5411\u91cf\u6765\u5b9e\u73b0\u975e\u8eab\u4efd\u8bc6\u522b\uff0c\u5e76\u91c7\u7528\u6b63\u4ea4\u8eab\u4efd\u6620\u5c04\u7b56\u7565\u6765\u8fdb\u4e00\u6b65\u6291\u5236\u8eab\u4efd\u6cc4\u9732\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cID\textsuperscript{2}Face\u5728\u89c6\u89c9\u8d28\u91cf\u3001\u8eab\u4efd\u6291\u5236\u548c\u6548\u7528\u4fdd\u7559\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "ID\textsuperscript{2}Face\u901a\u8fc7\u5176\u8bad\u7ec3\u4e2d\u5fc3\u7684\u65b9\u6cd5\uff0c\u6210\u529f\u5730\u89e3\u8026\u4e86\u8eab\u4efd\u548c\u975e\u8eab\u4efd\u4fe1\u606f\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u9762\u90e8\u975e\u8eab\u4efd\u8bc6\u522b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6570\u636e\u7684\u6548\u7528\u3002"}}
{"id": "2510.24534", "categories": ["quant-ph", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.24534", "abs": "https://arxiv.org/abs/2510.24534", "authors": ["Xin Jin", "Nitish Kumar Chandra", "Mohadeseh Azari", "Kaushik P. Seshadreesan", "Junyu Liu"], "title": "Quantum-Resistant Networks Using Post-Quantum Cryptography", "comment": "Submission for 2025 IEEE Workshop on Quantum IntelLigence, Learning &\n  Security (QUILLS), https://sites.google.com/view/quills2025/home", "summary": "Quantum networks rely on both quantum and classical channels for coordinated\noperation. Current architectures employ entanglement distribution and key\nexchange over quantum channels but often assume that classical communication is\nsufficiently secure. In practice, classical channels protected by traditional\ncryptography remain vulnerable to quantum adversaries, since large-scale\nquantum computers could break widely used public-key schemes and reduce the\neffective security of symmetric cryptography. This perspective presents a\nquantum-resistant network architecture that secures classical communication\nwith post-quantum cryptographic techniques while supporting entanglement-based\ncommunication over quantum channels. Beyond cryptographic protection, the\nframework incorporates continuous monitoring of both quantum and classical\nlayers, together with orchestration across heterogeneous infrastructures, to\nensure end-to-end security. Collectively, these mechanisms provide a pathway\ntoward scalable, robust, and secure quantum networks that remain dependable\nagainst both classical and quantum-era threats.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6297\u91cf\u5b50\u7f51\u7edc\u67b6\u6784\uff0c\u5229\u7528\u540e\u91cf\u5b50\u5bc6\u7801\u5b66\u4fdd\u62a4\u7ecf\u5178\u901a\u4fe1\uff0c\u5e76\u652f\u6301\u57fa\u4e8e\u7ea0\u7f20\u7684\u901a\u4fe1\uff0c\u540c\u65f6\u901a\u8fc7\u6301\u7eed\u76d1\u63a7\u548c\u8de8\u5f02\u6784\u57fa\u7840\u8bbe\u65bd\u7684\u534f\u8c03\u6765\u786e\u4fdd\u7aef\u5230\u7aef\u5b89\u5168\u3002", "motivation": "\u5f53\u524d\u91cf\u5b50\u7f51\u7edc\u4f9d\u8d56\u7684\u7ecf\u5178\u901a\u4fe1\u901a\u9053\u5bb9\u6613\u53d7\u5230\u672a\u6765\u91cf\u5b50\u8ba1\u7b97\u673a\u7684\u653b\u51fb\uff0c\u56e0\u4e3a\u5b83\u4eec\u53ef\u80fd\u7834\u574f\u73b0\u6709\u7684\u516c\u94a5\u52a0\u5bc6\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4e86\u540e\u91cf\u5b50\u5bc6\u7801\u5b66\u3001\u91cf\u5b50\u4fe1\u9053\u4e0a\u7684\u7ea0\u7f20\u5206\u53d1\u4ee5\u53ca\u5bf9\u91cf\u5b50\u548c\u7ecf\u5178\u5c42\u8fdb\u884c\u6301\u7eed\u76d1\u63a7\u7684\u7f51\u7edc\u67b6\u6784\u3002", "result": "\u8be5\u67b6\u6784\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u7a33\u5065\u4e14\u5b89\u5168\u7684\u91cf\u5b50\u7f51\u7edc\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u62b5\u5fa1\u5f53\u524d\u548c\u672a\u6765\u7684\u91cf\u5b50\u5a01\u80c1\u3002", "conclusion": "\u4e3a\u4e86\u5b9e\u73b0\u53ef\u6269\u5c55\u3001\u7a33\u5065\u4e14\u5b89\u5168\u7684\u91cf\u5b50\u7f51\u7edc\uff0c\u5fc5\u987b\u5728\u7ecf\u5178\u901a\u4fe1\u4e2d\u4f7f\u7528\u540e\u91cf\u5b50\u5bc6\u7801\u5b66\uff0c\u5e76\u7ed3\u5408\u5bf9\u7f51\u7edc\u5404\u4e2a\u5c42\u9762\u7684\u6301\u7eed\u76d1\u63a7\u3002"}}
{"id": "2510.23727", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23727", "abs": "https://arxiv.org/abs/2510.23727", "authors": ["Anisha Saha", "Varsha Suresh", "Timothy Hospedales", "Vera Demberg"], "title": "MUStReason: A Benchmark for Diagnosing Pragmatic Reasoning in Video-LMs for Multimodal Sarcasm Detection", "comment": null, "summary": "Sarcasm is a specific type of irony which involves discerning what is said\nfrom what is meant. Detecting sarcasm depends not only on the literal content\nof an utterance but also on non-verbal cues such as speaker's tonality, facial\nexpressions and conversational context. However, current multimodal models\nstruggle with complex tasks like sarcasm detection, which require identifying\nrelevant cues across modalities and pragmatically reasoning over them to infer\nthe speaker's intention. To explore these limitations in VideoLMs, we introduce\nMUStReason, a diagnostic benchmark enriched with annotations of\nmodality-specific relevant cues and underlying reasoning steps to identify\nsarcastic intent. In addition to benchmarking sarcasm classification\nperformance in VideoLMs, using MUStReason we quantitatively and qualitatively\nevaluate the generated reasoning by disentangling the problem into perception\nand reasoning, we propose PragCoT, a framework that steers VideoLMs to focus on\nimplied intentions over literal meaning, a property core to detecting sarcasm.", "AI": {"tldr": "Sarcasm detection is challenging for current multimodal models, so we introduce MUStReason, a benchmark for evaluating VideoLMs on this task. We also propose PragCoT, a framework to improve their ability to detect sarcasm by focusing on implied intentions.", "motivation": "Current multimodal models struggle with complex tasks like sarcasm detection, which require identifying relevant cues across modalities and pragmatically reasoning over them to infer the speaker's intention.", "method": "We introduce MUStReason, a diagnostic benchmark enriched with annotations of modality-specific relevant cues and underlying reasoning steps to identify sarcastic intent. We also propose PragCoT, a framework that steers VideoLMs to focus on implied intentions over literal meaning.", "result": "MUStReason allows for quantitative and qualitative evaluation of sarcasm classification performance and generated reasoning in VideoLMs. PragCoT is shown to steer VideoLMs towards focusing on implied intentions.", "conclusion": "MUStReason and PragCoT provide a way to diagnose and improve the performance of VideoLMs on sarcasm detection by addressing the limitations in cross-modal cue identification and pragmatic reasoning."}}
{"id": "2510.24390", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24390", "abs": "https://arxiv.org/abs/2510.24390", "authors": ["Xianjun Gao", "Jianchun Liu", "Hongli Xu", "Liusheng Huang"], "title": "Improving LLM Reasoning via Dependency-Aware Query Decomposition and Logic-Parallel Content Expansion", "comment": null, "summary": "The integration of Large Language Models (LLMs) into real-time Web\napplications, such as AI-powered search and conversational agents, presents a\nfundamental Web infrastructure challenge: reconciling the demand for\nhigh-quality, complex reasoning with the stringent low-latency and\nhigh-throughput requirements of interactive services. Current LLM reasoning,\nhindered by computationally inefficient sequential generation and rigid\nreasoning strategies, creates a critical bottleneck for the Web services.\nExisting approaches typically optimize the LLM reasoning for either efficiency\nor quality but struggle to achieve both, and thus fail to meet the dual\nrequirements of modern Web platforms. To overcome these limitations, we propose\nOrion, a novel and efficient reasoning framework that enables dependency-aware\nquery decomposition and logic-parallel content expansion. Concretely, Orion\ndecomposes a single query reasoning process into two synergistic phases: (1)\n\\textit{key point generation}, which distills logically structured key points\nthrough retrieval-augmented few-shot prompting, and (2) \\textit{content\nparallel expansion}, which concurrently elaborates on these points based on a\ndependency graph to ensure logical consistency. Furthermore, Orion introduces a\npipeline scheduling mechanism that exploits the complementary computational\ncharacteristics of the two phases (generation imposes pressure on GPU computing\nand expansion stresses on GPU memory) across multiple queries, enabling\ncross-query parallelism and dramatically improving reasoning performance (\\ie,\nefficiency and quality). Experiments on diverse benchmarks show that Orion not\nonly delivers up to 4.33x higher token generation speed and 3.42x lower answer\nlatency over the baselines but also improves reasoning quality by up to 18.75%\nthrough explicitly modeling inter-point dependencies.", "AI": {"tldr": "Orion\u662f\u4e00\u4e2a\u9ad8\u6548\u7684\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u4f9d\u8d56\u611f\u77e5\u67e5\u8be2\u5206\u89e3\u548c\u903b\u8f91\u5e76\u884c\u5185\u5bb9\u6269\u5c55\uff0c\u89e3\u51b3\u4e86LLM\u5728\u5b9e\u65f6Web\u5e94\u7528\u4e2d\u7684\u5ef6\u8fdf\u548c\u541e\u5410\u91cf\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u6548\u7387\u548c\u8d28\u91cf\u7684\u53cc\u91cd\u63d0\u5347\u3002", "motivation": "\u5f53\u524dLLM\u63a8\u7406\u5728\u5b9e\u65f6Web\u5e94\u7528\u4e2d\u5b58\u5728\u5ef6\u8fdf\u9ad8\u3001\u541e\u5410\u91cf\u4f4e\u7684\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u517c\u987e\u6548\u7387\u548c\u8d28\u91cf\u3002", "method": "Orion\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u63a8\u7406\u6846\u67b6\uff0c\u5305\u62ec\u4e24\u4e2a\u9636\u6bb5\uff1a1) \u5173\u952e\u70b9\u751f\u6210\uff0c\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u7684\u5c11\u6837\u672c\u63d0\u793a\u63d0\u70bc\u903b\u8f91\u7ed3\u6784\u5316\u7684\u5173\u952e\u70b9\uff1b2) \u5185\u5bb9\u5e76\u884c\u6269\u5c55\uff0c\u57fa\u4e8e\u4f9d\u8d56\u56fe\u5e76\u53d1\u5730\u9610\u8ff0\u8fd9\u4e9b\u5173\u952e\u70b9\u4ee5\u786e\u4fdd\u903b\u8f91\u4e00\u81f4\u6027\u3002\u6b64\u5916\uff0cOrion\u5f15\u5165\u4e86\u7ba1\u9053\u8c03\u5ea6\u673a\u5236\uff0c\u8de8\u67e5\u8be2\u5229\u7528\u4e24\u4e2a\u9636\u6bb5\u7684\u8ba1\u7b97\u7279\u6027\uff0c\u5b9e\u73b0\u8de8\u67e5\u8be2\u5e76\u884c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cOrion\u7684\u4ee4\u724c\u751f\u6210\u901f\u5ea6\u6700\u9ad8\u53ef\u63d0\u9ad84.33\u500d\uff0c\u7b54\u6848\u5ef6\u8fdf\u6700\u4f4e\u53ef\u964d\u4f4e3.42\u500d\uff0c\u540c\u65f6\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u70b9\u95f4\u4f9d\u8d56\u6027\uff0c\u63a8\u7406\u8d28\u91cf\u63d0\u9ad8\u4e8618.75%\u3002", "conclusion": "Orion\u901a\u8fc7\u5176\u521b\u65b0\u7684\u65b9\u6cd5\uff0c\u6210\u529f\u5730\u89e3\u51b3\u4e86LLM\u5728\u5b9e\u65f6Web\u5e94\u7528\u4e2d\u7684\u6548\u7387\u548c\u8d28\u91cf\u74f6\u9888\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6027\u80fd\u3002"}}
{"id": "2510.24250", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24250", "abs": "https://arxiv.org/abs/2510.24250", "authors": ["Syed Zohaib Hassan", "P\u00e5l Halvorsen", "Miriam S. Johnson", "Pierre Lison"], "title": "Evaluating LLMs on Generating Age-Appropriate Child-Like Conversations", "comment": "11 pages excluding references and appendix. 3 figures and 6 tables", "summary": "Large Language Models (LLMs), predominantly trained on adult conversational\ndata, face significant challenges when generating authentic, child-like\ndialogue for specialized applications. We present a comparative study\nevaluating five different LLMs (GPT-4, RUTER-LLAMA-2-13b, GPTSW, NorMistral-7b,\nand NorBloom-7b) to generate age-appropriate Norwegian conversations for\nchildren aged 5 and 9 years. Through a blind evaluation by eleven education\nprofessionals using both real child interview data and LLM-generated text\nsamples, we assessed authenticity and developmental appropriateness. Our\nresults show that evaluators achieved strong inter-rater reliability (ICC=0.75)\nand demonstrated higher accuracy in age prediction for younger children\n(5-year-olds) compared to older children (9-year-olds). While GPT-4 and\nNorBloom-7b performed relatively well, most models generated language perceived\nas more linguistically advanced than the target age groups. These findings\nhighlight critical data-related challenges in developing LLM systems for\nspecialized applications involving children, particularly in low-resource\nlanguages where comprehensive age-appropriate lexical resources are scarce.", "AI": {"tldr": "LLM\u5728\u751f\u6210\u513f\u7ae5\u5bf9\u8bdd\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u4e94\u79cdLLM\u751f\u6210\u632a\u5a01\u8bed\u513f\u7ae5\u5bf9\u8bdd\u7684\u6548\u679c\uff0c\u53d1\u73b0\u5927\u591a\u6570\u6a21\u578b\u751f\u6210\u7684\u8bed\u8a00\u8fc7\u4e8e\u6210\u4eba\u5316\uff0cGPT-4\u548cNorBloom-7b\u8868\u73b0\u76f8\u5bf9\u8f83\u597d\uff0c\u6570\u636e\u548c\u8d44\u6e90\u7a00\u7f3a\u662f\u4e3b\u8981\u6311\u6218\u3002", "motivation": "\u73b0\u6709LLM\u4e3b\u8981\u57fa\u4e8e\u6210\u4eba\u5bf9\u8bdd\u6570\u636e\u8bad\u7ec3\uff0c\u96be\u4ee5\u751f\u6210\u9002\u5408\u513f\u7ae5\u7684\u5bf9\u8bdd\uff0c\u5c24\u5176\u662f\u5728\u632a\u5a01\u8bed\u7b49\u8d44\u6e90\u532e\u4e4f\u7684\u8bed\u8a00\u73af\u5883\u4e2d\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528\u76f2\u8bc4\u65b9\u6cd5\uff0c\u9080\u8bf711\u4f4d\u6559\u80b2\u4e13\u4e1a\u4eba\u58eb\u8bc4\u4f30\u4e86\u4e94\u79cdLLM\uff08GPT-4\u3001RUTER-LLAMA-2-13b\u3001GPTSW\u3001NorMistral-7b\u548cNorBloom-7b\uff09\u751f\u6210\u7684\u9488\u5bf95\u5c81\u548c9\u5c81\u513f\u7ae5\u7684\u632a\u5a01\u8bed\u5bf9\u8bdd\u6837\u672c\uff0c\u5e76\u4e0e\u771f\u5b9e\u513f\u7ae5\u8bbf\u8c08\u6570\u636e\u8fdb\u884c\u6bd4\u8f83\uff0c\u8bc4\u4f30\u4e86\u5bf9\u8bdd\u7684\u771f\u5b9e\u6027\u548c\u53d1\u5c55\u9002\u5b9c\u6027\u3002", "result": "\u8bc4\u4f30\u8005\u4e4b\u95f4\u5177\u6709\u8f83\u9ad8\u7684\u4e00\u81f4\u6027\uff08ICC=0.75\uff09\uff0c\u5e76\u4e14\u80fd\u66f4\u51c6\u786e\u5730\u9884\u6d4b5\u5c81\u513f\u7ae5\u7684\u5e74\u9f84\u3002GPT-4\u548cNorBloom-7b\u8868\u73b0\u8f83\u597d\uff0c\u4f46\u5927\u591a\u6570\u6a21\u578b\u751f\u6210\u7684\u8bed\u8a00\u6bd4\u76ee\u6807\u5e74\u9f84\u7ec4\u66f4\u6210\u719f\u3002", "conclusion": "LLM\u5728\u4e3a\u513f\u7ae5\uff08\u5c24\u5176\u662f\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\uff09\u751f\u6210\u5bf9\u8bdd\u65f6\u9762\u4e34\u6570\u636e\u6311\u6218\uff0c\u73b0\u6709\u6a21\u578b\u751f\u6210\u7684\u8bed\u8a00\u901a\u5e38\u8fc7\u4e8e\u6210\u719f\uff0c\u96be\u4ee5\u6ee1\u8db3\u7279\u5b9a\u5e74\u9f84\u6bb5\u513f\u7ae5\u7684\u9700\u6c42\u3002"}}
{"id": "2510.24214", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24214", "abs": "https://arxiv.org/abs/2510.24214", "authors": ["Jinhong Deng", "Wen Li", "Joey Tianyi Zhou", "Yang He"], "title": "SCOPE: Saliency-Coverage Oriented Token Pruning for Efficient Multimodel LLMs", "comment": "NeurIPS 2025", "summary": "Multimodal Large Language Models (MLLMs) typically process a large number of\nvisual tokens, leading to considerable computational overhead, even though many\nof these tokens are redundant. Existing visual token pruning methods primarily\nfocus on selecting the most salient tokens based on attention scores, resulting\nin the semantic incompleteness of the selected tokens. In this paper, we\npropose a novel visual token pruning strategy, called\n\\textbf{S}aliency-\\textbf{C}overage \\textbf{O}riented token \\textbf{P}runing\nfor \\textbf{E}fficient MLLMs (SCOPE), to jointly model both the saliency and\ncoverage of the selected visual tokens to better preserve semantic\ncompleteness. Specifically, we introduce a set-coverage for a given set of\nselected tokens, computed based on the token relationships. We then define a\ntoken-coverage gain for each unselected token, quantifying how much additional\ncoverage would be obtained by including it. By integrating the saliency score\ninto the token-coverage gain, we propose our SCOPE score and iteratively select\nthe token with the highest SCOPE score. We conduct extensive experiments on\nmultiple vision-language understanding benchmarks using the LLaVA-1.5 and\nLLaVA-Next models. Experimental results demonstrate that our method\nconsistently outperforms prior approaches. Our code is available at\n\\href{https://github.com/kinredon/SCOPE}{https://github.com/kinredon/SCOPE}.", "AI": {"tldr": "\u4e3a\u4e86\u63d0\u9ad8\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b(MLLMs)\u7684\u5904\u7406\u6548\u7387\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89c6\u89c9\u6807\u8bb0\u526a\u679d\u7b56\u7565SCOPE\uff0c\u8be5\u7b56\u7565\u901a\u8fc7\u8054\u5408\u8003\u8651\u6807\u8bb0\u7684\u663e\u8457\u6027\u548c\u8986\u76d6\u7387\u6765\u6700\u5927\u9650\u5ea6\u5730\u4fdd\u7559\u8bed\u4e49\u5b8c\u6574\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9\u6807\u8bb0\u526a\u679d\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u57fa\u4e8e\u6ce8\u610f\u5206\u6570\u7684\u663e\u8457\u6027\u6807\u8bb0\u9009\u62e9\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u8bed\u4e49\u4e0d\u5b8c\u6574\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u540c\u65f6\u8003\u8651\u663e\u8457\u6027\u548c\u8986\u76d6\u7387\u7684\u65b0\u7b56\u7565\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSCOPE\uff08Saliency-Coverage Oriented token Pruning for Efficient MLLMs\uff09\u7684\u89c6\u89c9\u6807\u8bb0\u526a\u679d\u7b56\u7565\u3002\u8be5\u7b56\u7565\u9996\u5148\u8ba1\u7b97\u6807\u8bb0\u96c6\u8986\u76d6\u7387\uff0c\u7136\u540e\u5b9a\u4e49\u6807\u8bb0\u8986\u76d6\u589e\u76ca\uff0c\u5e76\u7ed3\u5408\u663e\u8457\u6027\u5206\u6570\uff0c\u901a\u8fc7\u8fed\u4ee3\u9009\u62e9\u5177\u6709\u6700\u9ad8SCOPE\u5206\u6570\uff08\u663e\u8457\u6027\u5206\u6570\u4e0e\u8986\u76d6\u589e\u76ca\u7684\u7ed3\u5408\uff09\u7684\u6807\u8bb0\u3002", "result": "\u5728LLaVA-1.5\u548cLLaVA-Next\u6a21\u578b\u4ee5\u53ca\u591a\u4e2a\u89c6\u89c9-\u8bed\u8a00\u7406\u89e3\u57fa\u51c6\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cSCOPE\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "SCOPE\u662f\u4e00\u79cd\u65b0\u9896\u4e14\u6709\u6548\u7684\u89c6\u89c9\u6807\u8bb0\u526a\u679d\u7b56\u7565\uff0c\u80fd\u591f\u901a\u8fc7\u8054\u5408\u5efa\u6a21\u663e\u8457\u6027\u548c\u8986\u76d6\u7387\u6765\u63d0\u9ad8MLLMs\u7684\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u8bed\u4e49\u5b8c\u6574\u6027\u3002"}}
{"id": "2510.24572", "categories": ["quant-ph", "math-ph", "math.MP"], "pdf": "https://arxiv.org/pdf/2510.24572", "abs": "https://arxiv.org/abs/2510.24572", "authors": ["Samuel Alperin"], "title": "A No-Go Theorem for Shaping Quantum Resources", "comment": "5 pages", "summary": "The ability to independently control higher-order statistical moments of\ncontinuous-variable quantum states would allow the direct ``shaping'' of\nnon-Gaussian resources, with wide implications for quantum communication,\ncomputation, and metrology. Here we prove that such control is fundamentally\nimpossible under any smooth Hamiltonian dynamics. Within the full\ninfinite-dimensional algebra of Hamiltonian vector fields on phase space, the\nquadratic (symplectic) subalgebra $\\mathfrak{sp}(2N,\\mathbb R)$ -- and, in the\nsingle-mode case, its $\\mathrm{SU}(1,1)$ representation -- is the unique\nhierarchy-preserving structure: only quadratic generators produce differential\noperators that terminate at second order and thereby decouple first and second\nmoments from higher cumulants. Any smooth non-quadratic Hamiltonian introduces\nthird- and higher-order derivatives in the phase-space generator, enforcing a\nuniversal coupling between the Gaussian and non-Gaussian sectors. This\n\\emph{rigidity of the moment hierarchy} generalizes the Gaussian no-go theorems\nand identifies the analytic boundary between symplectic (Clifford) dynamics and\nthe non-simulable regime beyond the Gottesman--Knill limit.", "AI": {"tldr": "\u65e0\u6cd5\u72ec\u7acb\u63a7\u5236\u8fde\u7eed\u53d8\u91cf\u91cf\u5b50\u6001\u7684\u9ad8\u9636\u7edf\u8ba1\u77e9", "motivation": "\u72ec\u7acb\u63a7\u5236\u9ad8\u9636\u7edf\u8ba1\u77e9\u53ef\u4ee5\u5851\u9020\u975e\u9ad8\u65af\u8d44\u6e90\uff0c\u5728\u91cf\u5b50\u901a\u4fe1\u3001\u8ba1\u7b97\u548c\u8ba1\u91cf\u5b66\u4e2d\u6709\u5e7f\u6cdb\u5e94\u7528\u3002", "method": "\u8bc1\u660e\u4e86\u5728\u4efb\u4f55\u5e73\u6ed1\u7684\u54c8\u5bc6\u987f\u52a8\u529b\u5b66\u4e0b\uff0c\u8fd9\u79cd\u63a7\u5236\u5728\u6839\u672c\u4e0a\u662f\u4e0d\u53ef\u80fd\u7684\u3002\u8bc1\u660e\u4e86\u53ea\u6709\u4e8c\u6b21\u751f\u6210\u5668\u80fd\u591f\u751f\u6210\u5728\u4e8c\u9636\u7ec8\u6b62\u7684\u5fae\u5206\u7b97\u5b50\uff0c\u4ece\u800c\u5c06\u4e00\u9636\u548c\u4e8c\u9636\u77e9\u4e0e\u66f4\u9ad8\u9636\u7d2f\u79ef\u91cf\u5206\u79bb\u5f00\u3002", "result": "\u4efb\u4f55\u5e73\u6ed1\u7684\u975e\u4e8c\u6b21\u54c8\u5bc6\u987f\u91cf\u90fd\u4f1a\u5728\u76f8\u7a7a\u95f4\u751f\u6210\u5668\u4e2d\u5f15\u5165\u4e09\u9636\u53ca\u66f4\u9ad8\u9636\u5bfc\u6570\uff0c\u5f3a\u5236\u5b9e\u73b0\u9ad8\u65af\u548c\u975e\u9ad8\u65af\u90e8\u95e8\u4e4b\u95f4\u7684\u666e\u904d\u8026\u5408\u3002", "conclusion": "\u77e9\u5c42\u7ea7\u7684\u521a\u6027\u63a8\u5e7f\u4e86\u9ad8\u65af\u4e0d\u53ef\u884c\u5b9a\u7406\uff0c\u5e76\u786e\u5b9a\u4e86\u8f9b\uff08\u514b\u5229\u798f\u5fb7\uff09\u52a8\u529b\u5b66\u4e0e\u6208\u7279\u65af\u66fc-\u514b\u5c3c\u5c14\u6781\u9650\u4e4b\u5916\u7684\u4e0d\u53ef\u6a21\u62df\u533a\u57df\u4e4b\u95f4\u7684\u5206\u6790\u8fb9\u754c\u3002"}}
{"id": "2510.23751", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.23751", "abs": "https://arxiv.org/abs/2510.23751", "authors": ["Ignavier Ng", "Patrick Bl\u00f6baum", "Siddharth Bhandari", "Kun Zhang", "Shiva Kasiviswanathan"], "title": "Debiasing Reward Models by Representation Learning with Guarantees", "comment": null, "summary": "Recent alignment techniques, such as reinforcement learning from human\nfeedback, have been widely adopted to align large language models with human\npreferences by learning and leveraging reward models. In practice, these models\noften exploit spurious correlations, involving, e.g., response length,\ndiscrimination, sycophancy, and conceptual bias, which is a problem that has\nreceived increasing attention. In this work, we propose a principled framework\nthat mitigates these biases in reward models while preserving the underlying\nfactors that reflect intended preferences. We first provide a formulation of\nthe data-generating process, assuming that the observed data (e.g., text) is\ngenerated from both spurious and non-spurious latent variables. We show that,\ninterestingly, these non-spurious latent variables can be theoretically\nidentified from data, regardless of whether a surrogate for the spurious latent\nvariables is available. This further inspires a practical method that uses\nvariational inference to recover these variables and leverages them to train\nreward models. Experiments on synthetic and real-world datasets demonstrate\nthat our method effectively mitigates spurious correlation issues and yields\nmore robust reward models.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u53d8\u5206\u63a8\u65ad\u6765\u7f13\u89e3\u5956\u52b1\u6a21\u578b\u4e2d\u865a\u5047\u76f8\u5173\u6027\u95ee\u9898\u7684\u6846\u67b6\uff0c\u540c\u65f6\u4fdd\u7559\u53cd\u6620\u9884\u671f\u504f\u597d\u7684\u6f5c\u5728\u56e0\u7d20\uff0c\u4ece\u800c\u8bad\u7ec3\u51fa\u66f4\u9c81\u68d2\u7684\u5956\u52b1\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u7684\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u6280\u672f\uff08\u5982\u4eba\u7c7b\u53cd\u9988\u5f3a\u5316\u5b66\u4e60\uff09\u5728\u5b9e\u8df5\u4e2d\u5e38\u5e38\u5229\u7528\u54cd\u5e94\u957f\u5ea6\u3001\u6b67\u89c6\u3001\u8c04\u5a9a\u548c\u6982\u5ff5\u504f\u5dee\u7b49\u865a\u5047\u76f8\u5173\u6027\uff0c\u8fd9\u662f\u4e00\u4e2a\u65e5\u76ca\u53d7\u5230\u5173\u6ce8\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u539f\u5219\u6027\u6846\u67b6\uff0c\u9996\u5148\u5bf9\u6570\u636e\u751f\u6210\u8fc7\u7a0b\u8fdb\u884c\u516c\u5f0f\u5316\uff0c\u5047\u8bbe\u89c2\u6d4b\u6570\u636e\u7531\u865a\u5047\u548c\u975e\u865a\u5047\u6f5c\u5728\u53d8\u91cf\u751f\u6210\u3002\u7136\u540e\uff0c\u63d0\u51fa\u4e00\u79cd\u4f7f\u7528\u53d8\u5206\u63a8\u65ad\u6765\u6062\u590d\u8fd9\u4e9b\u6f5c\u5728\u53d8\u91cf\u5e76\u5229\u7528\u5b83\u4eec\u6765\u8bad\u7ec3\u5956\u52b1\u6a21\u578b\u7684\u65b9\u6cd5\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u7f13\u89e3\u865a\u5047\u76f8\u5173\u6027\u95ee\u9898\uff0c\u5e76\u4ea7\u751f\u66f4\u9c81\u68d2\u7684\u5956\u52b1\u6a21\u578b\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u591f\u4ece\u6570\u636e\u4e2d\u8bc6\u522b\u51fa\u975e\u865a\u5047\u6f5c\u5728\u53d8\u91cf\uff0c\u5e76\u5229\u7528\u8fd9\u4e9b\u53d8\u91cf\u6765\u8bad\u7ec3\u80fd\u591f\u66f4\u597d\u53cd\u6620\u4eba\u7c7b\u504f\u597d\u7684\u5956\u52b1\u6a21\u578b\uff0c\u4ece\u800c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u5b58\u5728\u7684\u865a\u5047\u76f8\u5173\u6027\u95ee\u9898\u3002"}}
{"id": "2510.24397", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24397", "abs": "https://arxiv.org/abs/2510.24397", "authors": ["Jiarui Qin", "Yunjia Xi", "Junjie Huang", "Renting Rui", "Di Yin", "Weiwen Liu", "Yong Yu", "Weinan Zhang", "Xing Sun"], "title": "APTBench: Benchmarking Agentic Potential of Base LLMs During Pre-Training", "comment": "46 pages", "summary": "With the rapid development of LLM-based agents, there is a growing trend to\nincorporate agent-specific data into the pre-training stage of LLMs, aiming to\nbetter align LLMs with real-world autonomous task execution. However, current\npre-training benchmarks primarily focus on isolated and static skills, e.g.,\ncommon knowledge or mathematical/code reasoning, and fail to reflect model's\nagentic capabilities. On the other hand, agent benchmarks are typically\ndesigned for post-trained models, requiring multi-turn task execution abilities\nthat base models struggle to support. Thus, there is a compelling need for a\nbenchmark that can evaluate agentic potentials during pre-training and guide\nthe model training more effectively. To address this gap, we propose APTBench,\na framework that converts real-world agent tasks and successful trajectories\ninto multiple-choice or text completion questions tailored for base models. It\nfocuses on core agentic abilities, e.g., planning and action, and covers key\nagent scenarios, software engineering and deep research. Compared to existing\ngeneral-purpose benchmarks, APTBench offers a more predictive signal of a\nmodel's downstream performance as an agent, while remaining significantly more\nlightweight and cost-effective than full-scale, end-to-end agent evaluations\nafter post-training.", "AI": {"tldr": "APTBench\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u57fa\u7840\u6a21\u578b\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\u7684\u4ee3\u7406\u6f5c\u529b\u7684\u57fa\u51c6\uff0c\u5b83\u5c06\u771f\u5b9e\u4e16\u754c\u7684\u4ee3\u7406\u4efb\u52a1\u548c\u6210\u529f\u8f68\u8ff9\u8f6c\u6362\u4e3a\u9002\u5408\u57fa\u7840\u6a21\u578b\u7684\u591a\u9879\u9009\u62e9\u6216\u6587\u672c\u5b8c\u6210\u95ee\u9898\uff0c\u4ee5\u89e3\u51b3\u5f53\u524d\u9884\u8bad\u7ec3\u57fa\u51c6\u7f3a\u4e4f\u4ee3\u7406\u80fd\u529b\u8bc4\u4f30\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u7684\u9884\u8bad\u7ec3\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u5b64\u7acb\u7684\u3001\u9759\u6001\u7684\u6280\u80fd\uff0c\u672a\u80fd\u53cd\u6620\u6a21\u578b\u7684\u4ee3\u7406\u80fd\u529b\uff0c\u800c\u73b0\u6709\u7684\u4ee3\u7406\u57fa\u51c6\u5219\u9700\u8981\u7ecf\u8fc7\u540e\u8bad\u7ec3\u7684\u6a21\u578b\uff0c\u5e76\u8981\u6c42\u591a\u8f6e\u4efb\u52a1\u6267\u884c\u80fd\u529b\uff0c\u8fd9\u662f\u57fa\u7840\u6a21\u578b\u96be\u4ee5\u652f\u6301\u7684\u3002\u56e0\u6b64\uff0c\u8feb\u5207\u9700\u8981\u4e00\u4e2a\u80fd\u591f\u8bc4\u4f30\u9884\u8bad\u7ec3\u9636\u6bb5\u4ee3\u7406\u6f5c\u529b\u7684\u57fa\u51c6\u3002", "method": "APTBench\u6846\u67b6\u5c06\u771f\u5b9e\u4e16\u754c\u7684\u4ee3\u7406\u4efb\u52a1\u548c\u6210\u529f\u8f68\u8ff9\u8f6c\u6362\u4e3a\u9002\u5408\u57fa\u7840\u6a21\u578b\u7684\u591a\u9879\u9009\u62e9\u6216\u6587\u672c\u5b8c\u6210\u95ee\u9898\uff0c\u4e13\u6ce8\u4e8e\u6838\u5fc3\u4ee3\u7406\u80fd\u529b\uff0c\u5982\u89c4\u5212\u548c\u884c\u52a8\uff0c\u5e76\u6db5\u76d6\u8f6f\u4ef6\u5de5\u7a0b\u548c\u6df1\u5ea6\u7814\u7a76\u7b49\u5173\u952e\u4ee3\u7406\u573a\u666f\u3002", "result": "APTBench\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6bd4\u73b0\u6709\u901a\u7528\u57fa\u51c6\u66f4\u80fd\u9884\u6d4b\u6a21\u578b\u4f5c\u4e3a\u4ee3\u7406\u7684\u4e0b\u6e38\u6027\u80fd\u7684\u4fe1\u53f7\uff0c\u540c\u65f6\u6bd4\u5b8c\u5168\u7aef\u5230\u7aef\u7684\u4ee3\u7406\u8bc4\u4f30\u66f4\u8f7b\u91cf\u3001\u6210\u672c\u6548\u76ca\u66f4\u9ad8\u3002", "conclusion": "APTBench\u586b\u8865\u4e86\u8bc4\u4f30\u9884\u8bad\u7ec3\u9636\u6bb5\u4ee3\u7406\u6f5c\u529b\u7684\u57fa\u51c6\u7a7a\u767d\uff0c\u80fd\u591f\u66f4\u6709\u6548\u5730\u6307\u5bfc\u6a21\u578b\u8bad\u7ec3\uff0c\u5e76\u4e3a\u6a21\u578b\u4f5c\u4e3a\u4ee3\u7406\u7684\u4e0b\u6e38\u6027\u80fd\u63d0\u4f9b\u66f4\u5177\u9884\u6d4b\u6027\u7684\u4fe1\u53f7\u3002"}}
{"id": "2510.24256", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24256", "abs": "https://arxiv.org/abs/2510.24256", "authors": ["Jack Merullo", "Srihita Vatsavaya", "Lucius Bushnaq", "Owen Lewis"], "title": "From Memorization to Reasoning in the Spectrum of Loss Curvature", "comment": null, "summary": "We characterize how memorization is represented in transformer models and\nshow that it can be disentangled in the weights of both language models (LMs)\nand vision transformers (ViTs) using a decomposition based on the loss\nlandscape curvature. This insight is based on prior theoretical and empirical\nwork showing that the curvature for memorized training points is much sharper\nthan non memorized, meaning ordering weight components from high to low\ncurvature can reveal a distinction without explicit labels. This motivates a\nweight editing procedure that suppresses far more recitation of untargeted\nmemorized data more effectively than a recent unlearning method\n(BalancedSubnet), while maintaining lower perplexity. Since the basis of\ncurvature has a natural interpretation for shared structure in model weights,\nwe analyze the editing procedure extensively on its effect on downstream tasks\nin LMs, and find that fact retrieval and arithmetic are specifically and\nconsistently negatively affected, even though open book fact retrieval and\ngeneral logical reasoning is conserved. We posit these tasks rely heavily on\nspecialized directions in weight space rather than general purpose mechanisms,\nregardless of whether those individual datapoints are memorized. We support\nthis by showing a correspondence between task data's activation strength with\nlow curvature components that we edit out, and the drop in task performance\nafter the edit. Our work enhances the understanding of memorization in neural\nnetworks with practical applications towards removing it, and provides evidence\nfor idiosyncratic, narrowly-used structures involved in solving tasks like math\nand fact retrieval.", "AI": {"tldr": "Transformer\u6a21\u578b\u4e2d\u7684\u8bb0\u5fc6\u53ef\u4ee5\u901a\u8fc7\u57fa\u4e8e\u635f\u5931\u666f\u89c2\u66f2\u7387\u7684\u5206\u89e3\u6765\u89e3\u8026\uff0c\u4ece\u800c\u5b9e\u73b0\u6709\u6548\u7684\u8bb0\u5fc6\u6291\u5236\uff0c\u4f46\u53ef\u80fd\u4f1a\u5f71\u54cd\u7279\u5b9a\u7684\u4e0b\u6e38\u4efb\u52a1\u3002", "motivation": "\u7406\u89e3Transformer\u6a21\u578b\u4e2d\u7684\u8bb0\u5fc6\u8868\u793a\uff0c\u5e76\u5f00\u53d1\u4e00\u79cd\u6709\u6548\u6291\u5236\u8bb0\u5fc6\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u635f\u5931\u666f\u89c2\u66f2\u7387\u5206\u89e3\u6765\u8bc6\u522b\u548c\u7f16\u8f91\u6a21\u578b\u6743\u91cd\uff0c\u4ee5\u51cf\u5c11\u8bb0\u5fc6\u3002", "result": "\u6240\u63d0\u51fa\u7684\u6743\u91cd\u7f16\u8f91\u65b9\u6cd5\u6bd4\u73b0\u6709\u7684\u65b9\u6cd5\u66f4\u6709\u6548\u5730\u6291\u5236\u4e86\u8bb0\u5fc6\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8f83\u4f4e\u7684\u56f0\u60d1\u5ea6\u3002\u7136\u800c\uff0c\u8be5\u65b9\u6cd5\u5bf9\u7279\u5b9a\u4e0b\u6e38\u4efb\u52a1\uff08\u5982\u4e8b\u5b9e\u68c0\u7d22\u548c\u7b97\u672f\uff09\u4ea7\u751f\u4e86\u8d1f\u9762\u5f71\u54cd\uff0c\u5c3d\u7ba1\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u53ef\u4ee5\u4fdd\u7559\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "\u8bb0\u5fc6\u5728Transformer\u6a21\u578b\u4e2d\u53ef\u4ee5\u901a\u8fc7\u66f2\u7387\u5206\u89e3\u6765\u89e3\u8026\u3002\u6240\u63d0\u51fa\u7684\u7f16\u8f91\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u5730\u51cf\u5c11\u8bb0\u5fc6\uff0c\u4f46\u9700\u8981\u6ce8\u610f\u5176\u5bf9\u7279\u5b9a\u4efb\u52a1\u7684\u6f5c\u5728\u5f71\u54cd\u3002\u67d0\u4e9b\u4e0b\u6e38\u4efb\u52a1\uff08\u5982\u6570\u5b66\u548c\u4e8b\u5b9e\u68c0\u7d22\uff09\u53ef\u80fd\u4f9d\u8d56\u4e8e\u6a21\u578b\u6743\u91cd\u4e2d\u7279\u5b9a\u7684\u3001\u72ed\u7a84\u4f7f\u7528\u7684\u7ed3\u6784\u3002"}}
{"id": "2510.24095", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24095", "abs": "https://arxiv.org/abs/2510.24095", "authors": ["Vedant Gupta", "Haotian Fu", "Calvin Luo", "Yiding Jiang", "George Konidaris"], "title": "Learning Parameterized Skills from Demonstrations", "comment": "Neurips 2025", "summary": "We present DEPS, an end-to-end algorithm for discovering parameterized skills\nfrom expert demonstrations. Our method learns parameterized skill policies\njointly with a meta-policy that selects the appropriate discrete skill and\ncontinuous parameters at each timestep. Using a combination of temporal\nvariational inference and information-theoretic regularization methods, we\naddress the challenge of degeneracy common in latent variable models, ensuring\nthat the learned skills are temporally extended, semantically meaningful, and\nadaptable. We empirically show that learning parameterized skills from\nmultitask expert demonstrations significantly improves generalization to unseen\ntasks. Our method outperforms multitask as well as skill learning baselines on\nboth LIBERO and MetaWorld benchmarks. We also demonstrate that DEPS discovers\ninterpretable parameterized skills, such as an object grasping skill whose\ncontinuous arguments define the grasp location.", "AI": {"tldr": "DEPS\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u7b97\u6cd5\uff0c\u7528\u4e8e\u4ece\u4e13\u5bb6\u6f14\u793a\u4e2d\u53d1\u73b0\u53c2\u6570\u5316\u6280\u80fd\u3002\u8be5\u65b9\u6cd5\u5b66\u4e60\u53c2\u6570\u5316\u6280\u80fd\u7b56\u7565\uff0c\u5e76\u7ed3\u5408\u4e00\u4e2a\u5143\u7b56\u7565\uff0c\u5728\u6bcf\u4e2a\u65f6\u95f4\u6b65\u9009\u62e9\u5408\u9002\u7684\u79bb\u6563\u6280\u80fd\u548c\u8fde\u7eed\u53c2\u6570\u3002\u901a\u8fc7\u7ed3\u5408\u65f6\u95f4\u53d8\u5206\u63a8\u7406\u548c\u4fe1\u606f\u8bba\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u6f5c\u5728\u53d8\u91cf\u6a21\u578b\u4e2d\u5e38\u89c1\u7684\u9000\u5316\u95ee\u9898\uff0c\u786e\u4fdd\u5b66\u4e60\u5230\u7684\u6280\u80fd\u5177\u6709\u65f6\u95f4\u6269\u5c55\u6027\u3001\u8bed\u4e49\u610f\u4e49\u548c\u9002\u5e94\u6027\u3002", "motivation": "\u4ece\u4e13\u5bb6\u6f14\u793a\u4e2d\u5b66\u4e60\u53c2\u6570\u5316\u6280\u80fd\uff0c\u4ee5\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\u5e76\u514b\u670d\u6f5c\u5728\u53d8\u91cf\u6a21\u578b\u4e2d\u7684\u9000\u5316\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u65f6\u95f4\u53d8\u5206\u63a8\u7406\u548c\u4fe1\u606f\u8bba\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u8054\u5408\u5b66\u4e60\u53c2\u6570\u5316\u6280\u80fd\u7b56\u7565\u548c\u9009\u62e9\u79bb\u6563\u6280\u80fd\u53ca\u8fde\u7eed\u53c2\u6570\u7684\u5143\u7b56\u7565\u3002", "result": "DEPS\u5728LIBERO\u548cMetaWorld\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u76f8\u6bd4\u591a\u4efb\u52a1\u548c\u6280\u80fd\u5b66\u4e60\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u672a\u89c1\u8fc7\u7684\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u663e\u8457\u7684\u6cdb\u5316\u80fd\u529b\u63d0\u5347\uff0c\u5e76\u53d1\u73b0\u4e86\u53ef\u89e3\u91ca\u7684\u53c2\u6570\u5316\u6280\u80fd\u3002", "conclusion": "DEPS\u80fd\u591f\u6709\u6548\u5730\u4ece\u4e13\u5bb6\u6f14\u793a\u4e2d\u5b66\u4e60\u53ef\u89e3\u91ca\u7684\u3001\u53ef\u6cdb\u5316\u7684\u53c2\u6570\u5316\u6280\u80fd\u3002"}}
{"id": "2510.24231", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24231", "abs": "https://arxiv.org/abs/2510.24231", "authors": ["Waseem Shariff", "Timothy Hanley", "Maciej Stec", "Hossein Javidnia", "Peter Corcoran"], "title": "Benchmarking Microsaccade Recognition with Event Cameras: A Novel Dataset and Evaluation", "comment": "Accepted in British Machine Vision Conference (BMVC) 2025, Main\n  Conference", "summary": "Microsaccades are small, involuntary eye movements vital for visual\nperception and neural processing. Traditional microsaccade studies typically\nuse eye trackers or frame-based analysis, which, while precise, are costly and\nlimited in scalability and temporal resolution. Event-based sensing offers a\nhigh-speed, low-latency alternative by capturing fine-grained spatiotemporal\nchanges efficiently. This work introduces a pioneering event-based microsaccade\ndataset to support research on small eye movement dynamics in cognitive\ncomputing. Using Blender, we render high-fidelity eye movement scenarios and\nsimulate microsaccades with angular displacements from 0.5 to 2.0 degrees,\ndivided into seven distinct classes. These are converted to event streams using\nv2e, preserving the natural temporal dynamics of microsaccades, with durations\nranging from 0.25 ms to 2.25 ms. We evaluate the dataset using Spiking-VGG11,\nSpiking-VGG13, and Spiking-VGG16, and propose Spiking-VGG16Flow, an\noptical-flow-enhanced variant implemented in SpikingJelly. The models achieve\naround 90 percent average accuracy, successfully classifying microsaccades by\nangular displacement, independent of event count or duration. These results\ndemonstrate the potential of spiking neural networks for fine motion\nrecognition and establish a benchmark for event-based vision research. The\ndataset, code, and trained models will be publicly available at\nhttps://waseemshariff126.github.io/microsaccades/ .", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5f00\u521b\u6027\u7684\u4e8b\u4ef6\u9a71\u52a8\u578b\u5fae\u773c\u8df3\u6570\u636e\u96c6\uff0c\u5e76\u8bc4\u4f30\u4e86\u57fa\u4e8e\u5c16\u5cf0\u7684\u795e\u7ecf\u7f51\u7edc\uff08SNN\uff09\u5728\u5fae\u773c\u8df3\u5206\u7c7b\u65b9\u9762\u7684\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u7684\u5fae\u773c\u8df3\u7814\u7a76\u65b9\u6cd5\u6210\u672c\u9ad8\u3001\u53ef\u6269\u5c55\u6027\u548c\u65f6\u95f4\u5206\u8fa8\u7387\u6709\u9650\u3002\u4e8b\u4ef6\u9a71\u52a8\u4f20\u611f\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u901f\u3001\u4f4e\u5ef6\u8fdf\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u56e0\u6b64\u672c\u7814\u7a76\u65e8\u5728\u521b\u5efa\u4e00\u4e2a\u4e8b\u4ef6\u9a71\u52a8\u7684\u5fae\u773c\u8df3\u6570\u636e\u96c6\uff0c\u4ee5\u4fc3\u8fdb\u5bf9\u8ba4\u77e5\u8ba1\u7b97\u4e2d\u5c0f\u773c\u7403\u8fd0\u52a8\u52a8\u529b\u5b66\u7684\u7814\u7a76\u3002", "method": "\u4f7f\u7528 Blender \u6e32\u67d3\u9ad8\u4fdd\u771f\u773c\u52a8\u573a\u666f\uff0c\u5e76\u6a21\u62df\u5177\u6709 0.5 \u81f3 2.0 \u5ea6\u89d2\u4f4d\u79fb\u7684\u5fae\u773c\u8df3\uff0c\u5206\u4e3a\u4e03\u4e2a\u7c7b\u522b\u3002\u4f7f\u7528 v2e \u5c06\u5176\u8f6c\u6362\u4e3a\u4e8b\u4ef6\u6d41\uff0c\u5e76\u4f7f\u7528 Spiking-VGG11\u3001Spiking-VGG13 \u548c Spiking-VGG16 \u8bc4\u4f30\u6570\u636e\u96c6\uff0c\u540c\u65f6\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a Spiking-VGG16Flow \u7684\u5149\u5b66\u6d41\u589e\u5f3a\u53d8\u4f53\u3002", "result": "\u6240\u63d0\u51fa\u7684 SNN \u6a21\u578b\u5728\u5fae\u773c\u8df3\u5206\u7c7b\u65b9\u9762\u8fbe\u5230\u4e86\u7ea6 90% \u7684\u5e73\u5747\u51c6\u786e\u7387\uff0c\u80fd\u591f\u6839\u636e\u89d2\u4f4d\u79fb\u72ec\u7acb\u5730\u5bf9\u5fae\u773c\u8df3\u8fdb\u884c\u5206\u7c7b\uff0c\u800c\u4e0e\u4e8b\u4ef6\u6570\u91cf\u6216\u6301\u7eed\u65f6\u95f4\u65e0\u5173\u3002", "conclusion": "\u672c\u7814\u7a76\u8bc1\u660e\u4e86 SNN \u5728\u7cbe\u7ec6\u8fd0\u52a8\u8bc6\u522b\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u5e76\u4e3a\u4e8b\u4ef6\u9a71\u52a8\u89c6\u89c9\u7814\u7a76\u8bbe\u7acb\u4e86\u57fa\u51c6\u3002\u6570\u636e\u96c6\u3001\u4ee3\u7801\u548c\u8bad\u7ec3\u6a21\u578b\u5c06\u5728\u6307\u5b9a\u7f51\u5740\u516c\u5f00\u63d0\u4f9b\u3002"}}
{"id": "2510.24615", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.24615", "abs": "https://arxiv.org/abs/2510.24615", "authors": ["Yutaka Hirano", "Riki Toshio", "Tomohiro Itogawa", "Keisuke Fujii"], "title": "Efficient magic state cultivation with lattice surgery", "comment": "12 pages, 14 figures", "summary": "Magic state distillation plays a crucial role in fault-tolerant quantum\ncomputation and represents a major bottleneck. In contrast to traditional\nlogical-level distillation, physical-level distillation offers significant\noverhead reduction by enabling direct implementation with physical gates. Magic\nstate cultivation is a state-of-the-art physical-level distillation protocol\nthat is compatible with the square-grid connectivity and yields high-fidelity\nmagic states. However, it relies on the complex grafted code, which incurs\nsubstantial spacetime overhead and complicates practical implementation. In\nthis work, we propose an efficient cultivation-based protocol compatible with\nthe square-grid connectivity. We reduce the spatial overhead by avoiding the\ngrafted code and further reduce the average spacetime overhead by utilizing\ncode expansion and enabling early rejection. Numerical simulations show that,\nwith a color code distance of 3 and a physical error probability of $10^{-3}$,\nour protocol achieves a logical error probability for the resulting magic state\ncomparable to that of magic state cultivation ($\\approx 3 \\times 10^{-6}$),\nwhile requiring about half the spacetime overhead. Our work provides an\nefficient and simple distillation protocol suitable for megaquop use cases and\nearly fault-tolerant devices.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u9ad8\u6548\u7684\u3001\u57fa\u4e8e\u9b54\u6001\u57f9\u517b\u7684\u3001\u517c\u5bb9\u65b9\u5f62\u7f51\u683c\u8fde\u63a5\u7684\u534f\u8bae\uff0c\u901a\u8fc7\u907f\u514d\u5ac1\u63a5\u7801\u6765\u51cf\u5c11\u7a7a\u95f4\u5f00\u9500\uff0c\u5e76\u901a\u8fc7\u7801\u6269\u5c55\u548c\u65e9\u671f\u62d2\u7edd\u6765\u51cf\u5c11\u5e73\u5747\u65f6\u7a7a\u5f00\u9500\u3002\u8be5\u534f\u8bae\u5728\u989c\u8272\u7801\u8ddd\u79bb\u4e3a3\u3001\u7269\u7406\u9519\u8bef\u6982\u7387\u4e3a10^-3\u65f6\uff0c\u5b9e\u73b0\u7684\u9b54\u6001\u903b\u8f91\u9519\u8bef\u6982\u7387\u4e0e\u9b54\u6001\u57f9\u517b\u76f8\u5f53\uff08\u7ea63x10^-6\uff09\uff0c\u4f46\u65f6\u7a7a\u5f00\u9500\u4ec5\u4e3a\u5176\u4e00\u534a\u3002", "motivation": "\u9b54\u6001\u84b8\u998f\u662f\u5bb9\u9519\u91cf\u5b50\u8ba1\u7b97\u7684\u5173\u952e\u74f6\u9888\uff0c\u800c\u4f20\u7edf\u7684\u903b\u8f91\u7ea7\u84b8\u998f\u5f00\u9500\u5de8\u5927\u3002\u7269\u7406\u7ea7\u84b8\u998f\uff08\u5982\u9b54\u6001\u57f9\u517b\uff09\u867d\u7136\u53ef\u4ee5\u964d\u4f4e\u5f00\u9500\uff0c\u4f46\u4f9d\u8d56\u590d\u6742\u7684\u5ac1\u63a5\u7801\uff0c\u589e\u52a0\u4e86\u65f6\u7a7a\u5f00\u9500\u548c\u5b9e\u73b0\u96be\u5ea6\u3002\u56e0\u6b64\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u3001\u66f4\u7b80\u5355\u7684\u7269\u7406\u7ea7\u84b8\u998f\u534f\u8bae\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9b54\u6001\u57f9\u517b\u7684\u3001\u517c\u5bb9\u65b9\u5f62\u7f51\u683c\u8fde\u63a5\u7684\u534f\u8bae\u3002\u8be5\u534f\u8bae\u901a\u8fc7\u4ee5\u4e0b\u65b9\u5f0f\u51cf\u5c11\u5f00\u9500\uff1a1. \u907f\u514d\u4f7f\u7528\u5ac1\u63a5\u7801\uff0c\u51cf\u5c11\u7a7a\u95f4\u5f00\u9500\u30022. \u5229\u7528\u7801\u6269\u5c55\u6280\u672f\u30023. \u542f\u7528\u65e9\u671f\u62d2\u7edd\u673a\u5236\u3002\u901a\u8fc7\u6570\u503c\u6a21\u62df\u9a8c\u8bc1\u4e86\u8be5\u534f\u8bae\u7684\u6709\u6548\u6027\u3002", "result": "\u5728\u989c\u8272\u7801\u8ddd\u79bb\u4e3a3\u3001\u7269\u7406\u9519\u8bef\u6982\u7387\u4e3a10^-3\u7684\u6761\u4ef6\u4e0b\uff0c\u6240\u63d0\u51fa\u7684\u534f\u8bae\u5b9e\u73b0\u7684\u9b54\u6001\u903b\u8f91\u9519\u8bef\u6982\u7387\u7ea6\u4e3a3x10^-6\uff0c\u4e0e\u9b54\u6001\u57f9\u517b\u76f8\u5f53\u3002\u540c\u65f6\uff0c\u8be5\u534f\u8bae\u6240\u9700\u7684\u65f6\u7a7a\u5f00\u9500\u4ec5\u4e3a\u9b54\u6001\u57f9\u517b\u7684\u4e00\u534a\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u6bd4\u73b0\u6709\u9b54\u6001\u57f9\u517b\u534f\u8bae\u66f4\u9ad8\u6548\u3001\u66f4\u7b80\u5355\u7684\u84b8\u998f\u65b9\u6848\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u65f6\u7a7a\u5f00\u9500\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u91cf\u5b50\u8ba1\u7b97\u548c\u65e9\u671f\u5bb9\u9519\u8bbe\u5907\u3002"}}
{"id": "2510.23756", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23756", "abs": "https://arxiv.org/abs/2510.23756", "authors": ["Nicki Barari", "Edward Kim", "Christopher MacLellan"], "title": "Explaining Robustness to Catastrophic Forgetting Through Incremental Concept Formation", "comment": "18 pages, 5 figures, Advances in Cognitive Systems 2025", "summary": "Catastrophic forgetting remains a central challenge in continual learning,\nwhere models are required to integrate new knowledge over time without losing\nwhat they have previously learned. In prior work, we introduced Cobweb/4V, a\nhierarchical concept formation model that exhibited robustness to catastrophic\nforgetting in visual domains. Motivated by this robustness, we examine three\nhypotheses regarding the factors that contribute to such stability: (1)\nadaptive structural reorganization enhances knowledge retention, (2) sparse and\nselective updates reduce interference, and (3) information-theoretic learning\nbased on sufficiency statistics provides advantages over gradient-based\nbackpropagation. To test these hypotheses, we compare Cobweb/4V with neural\nbaselines, including CobwebNN, a neural implementation of the Cobweb framework\nintroduced in this work. Experiments on datasets of varying complexity (MNIST,\nFashion-MNIST, MedMNIST, and CIFAR-10) show that adaptive restructuring\nenhances learning plasticity, sparse updates help mitigate interference, and\nthe information-theoretic learning process preserves prior knowledge without\nrevisiting past data. Together, these findings provide insight into mechanisms\nthat can mitigate catastrophic forgetting and highlight the potential of\nconcept-based, information-theoretic approaches for building stable and\nadaptive continual learning systems.", "AI": {"tldr": "Cobweb/4V, a hierarchical concept formation model, mitigates catastrophic forgetting in continual learning by employing adaptive restructuring, sparse updates, and information-theoretic learning, outperforming neural baselines across various datasets.", "motivation": "The paper addresses the challenge of catastrophic forgetting in continual learning, aiming to understand the factors contributing to the robustness of the Cobweb/4V model.", "method": "The study tests three hypotheses: adaptive structural reorganization, sparse/selective updates, and information-theoretic learning. These are evaluated by comparing Cobweb/4V with neural baselines (including CobwebNN) on MNIST, Fashion-MNIST, MedMNIST, and CIFAR-10 datasets.", "result": "Experiments demonstrate that adaptive restructuring improves plasticity, sparse updates reduce interference, and information-theoretic learning preserves prior knowledge without data replay. Cobweb/4V shows robustness against catastrophic forgetting.", "conclusion": "The findings suggest that concept-based, information-theoretic approaches with adaptive restructuring and sparse updates are promising for developing stable and adaptive continual learning systems that avoid catastrophic forgetting."}}
{"id": "2510.24411", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.24411", "abs": "https://arxiv.org/abs/2510.24411", "authors": ["Qiushi Sun", "Mukai Li", "Zhoumianze Liu", "Zhihui Xie", "Fangzhi Xu", "Zhangyue Yin", "Kanzhi Cheng", "Zehao Li", "Zichen Ding", "Qi Liu", "Zhiyong Wu", "Zhuosheng Zhang", "Ben Kao", "Lingpeng Kong"], "title": "OS-Sentinel: Towards Safety-Enhanced Mobile GUI Agents via Hybrid Validation in Realistic Workflows", "comment": "work in progress", "summary": "Computer-using agents powered by Vision-Language Models (VLMs) have\ndemonstrated human-like capabilities in operating digital environments like\nmobile platforms. While these agents hold great promise for advancing digital\nautomation, their potential for unsafe operations, such as system compromise\nand privacy leakage, is raising significant concerns. Detecting these safety\nconcerns across the vast and complex operational space of mobile environments\npresents a formidable challenge that remains critically underexplored. To\nestablish a foundation for mobile agent safety research, we introduce\nMobileRisk-Live, a dynamic sandbox environment accompanied by a safety\ndetection benchmark comprising realistic trajectories with fine-grained\nannotations. Built upon this, we propose OS-Sentinel, a novel hybrid safety\ndetection framework that synergistically combines a Formal Verifier for\ndetecting explicit system-level violations with a VLM-based Contextual Judge\nfor assessing contextual risks and agent actions. Experiments show that\nOS-Sentinel achieves 10%-30% improvements over existing approaches across\nmultiple metrics. Further analysis provides critical insights that foster the\ndevelopment of safer and more reliable autonomous mobile agents.", "AI": {"tldr": "VLMs\u9a71\u52a8\u7684\u7535\u8111\u4ee3\u7406\u5728\u6570\u5b57\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u7c7b\u4eba\u80fd\u529b\uff0c\u4f46\u5b58\u5728\u5b89\u5168\u98ce\u9669\u3002\u672c\u6587\u63d0\u51fa\u4e86MobileRisk-Live\uff08\u4e00\u4e2a\u52a8\u6001\u6c99\u76d2\u73af\u5883\u548c\u5b89\u5168\u68c0\u6d4b\u57fa\u51c6\uff09\u548cOS-Sentinel\uff08\u4e00\u4e2a\u7ed3\u5408\u5f62\u5f0f\u5316\u9a8c\u8bc1\u5668\u548c\u57fa\u4e8eVLM\u7684\u4e0a\u4e0b\u6587\u5224\u65ad\u5668\u7684\u6df7\u5408\u5b89\u5168\u68c0\u6d4b\u6846\u67b6\uff09\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002OS-Sentinel\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u9ad8\u4e8610%-30%\uff0c\u5e76\u4e3a\u5f00\u53d1\u66f4\u5b89\u5168\u53ef\u9760\u7684\u79fb\u52a8\u4ee3\u7406\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002", "motivation": "VLMs\u9a71\u52a8\u7684\u7535\u8111\u4ee3\u7406\u5728\u64cd\u4f5c\u6570\u5b57\u73af\u5883\uff08\u5982\u79fb\u52a8\u5e73\u53f0\uff09\u65f6\uff0c\u53ef\u80fd\u5b58\u5728\u4e0d\u5b89\u5168\u7684\u64cd\u4f5c\uff08\u5982\u7cfb\u7edf\u6cc4\u9732\u548c\u9690\u79c1\u6cc4\u9732\uff09\uff0c\u5728\u79fb\u52a8\u73af\u5883\u4e2d\u68c0\u6d4b\u8fd9\u4e9b\u5b89\u5168\u95ee\u9898\u662f\u4e00\u4e2a\u4e25\u5cfb\u7684\u6311\u6218\uff0c\u5e76\u4e14\u4ecd\u7136\u662f\u4e00\u4e2a\u672a\u88ab\u5145\u5206\u63a2\u7d22\u7684\u9886\u57df\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86MobileRisk-Live\uff0c\u4e00\u4e2a\u52a8\u6001\u6c99\u76d2\u73af\u5883\uff0c\u5e76\u9644\u5e26\u4e00\u4e2a\u5305\u542b\u771f\u5b9e\u8f68\u8ff9\u548c\u7ec6\u7c92\u5ea6\u6ce8\u91ca\u7684\u5b89\u5168\u68c0\u6d4b\u57fa\u51c6\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aOS-Sentinel\u7684\u65b0\u578b\u6df7\u5408\u5b89\u5168\u68c0\u6d4b\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5c06\u5f62\u5f0f\u5316\u9a8c\u8bc1\u5668\uff08\u7528\u4e8e\u68c0\u6d4b\u660e\u786e\u7684\u7cfb\u7edf\u7ea7\u8fdd\u89c4\uff09\u4e0e\u57fa\u4e8eVLM\u7684\u4e0a\u4e0b\u6587\u5224\u65ad\u5668\uff08\u7528\u4e8e\u8bc4\u4f30\u4e0a\u4e0b\u6587\u98ce\u9669\u548c\u4ee3\u7406\u884c\u4e3a\uff09\u76f8\u7ed3\u5408\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cOS-Sentinel\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u9ad8\u4e8610%-30%\u3002", "conclusion": "OS-Sentinel\u5728\u68c0\u6d4b\u79fb\u52a8\u4ee3\u7406\u4e2d\u7684\u5b89\u5168\u98ce\u9669\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u4e3a\u5f00\u53d1\u66f4\u5b89\u5168\u3001\u66f4\u53ef\u9760\u7684\u81ea\u4e3b\u79fb\u52a8\u4ee3\u7406\u63d0\u4f9b\u4e86\u5173\u952e\u89c1\u89e3\u3002"}}
{"id": "2510.24259", "categories": ["cs.CL", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24259", "abs": "https://arxiv.org/abs/2510.24259", "authors": ["Ziqi Ma", "Sao Mai Nguyen", "Philippe Xu"], "title": "Can LLMs Translate Human Instructions into a Reinforcement Learning Agent's Internal Emergent Symbolic Representation?", "comment": null, "summary": "Emergent symbolic representations are critical for enabling developmental\nlearning agents to plan and generalize across tasks. In this work, we\ninvestigate whether large language models (LLMs) can translate human natural\nlanguage instructions into the internal symbolic representations that emerge\nduring hierarchical reinforcement learning. We apply a structured evaluation\nframework to measure the translation performance of commonly seen LLMs -- GPT,\nClaude, Deepseek and Grok -- across different internal symbolic partitions\ngenerated by a hierarchical reinforcement learning algorithm in the Ant Maze\nand Ant Fall environments. Our findings reveal that although LLMs demonstrate\nsome ability to translate natural language into a symbolic representation of\nthe environment dynamics, their performance is highly sensitive to partition\ngranularity and task complexity. The results expose limitations in current LLMs\ncapacity for representation alignment, highlighting the need for further\nresearch on robust alignment between language and internal agent\nrepresentations.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5c06\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u7ffb\u8bd1\u6210\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u4e2d\u6d8c\u73b0\u7684\u5185\u90e8\u7b26\u53f7\u8868\u793a\u65b9\u9762\u8868\u73b0\u51fa\u6709\u9650\u7684\u80fd\u529b\uff0c\u5e76\u4e14\u5bf9\u5206\u533a\u7c92\u5ea6\u548c\u4efb\u52a1\u590d\u6742\u6027\u9ad8\u5ea6\u654f\u611f\u3002", "motivation": "\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u662f\u5426\u80fd\u5c06\u4eba\u7c7b\u7684\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u7ffb\u8bd1\u6210\u5728\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u4e2d\u51fa\u73b0\u7684\u5185\u90e8\u7b26\u53f7\u8868\u793a\u3002", "method": "\u5728Ant Maze\u548cAnt Fall\u73af\u5883\u4e2d\uff0c\u4f7f\u7528\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u751f\u6210\u7684\u4e0d\u540c\u5185\u90e8\u7b26\u53f7\u5206\u533a\uff0c\u5e94\u7528\u7ed3\u6784\u5316\u8bc4\u4f30\u6846\u67b6\u6765\u8861\u91cfLLMs\uff08GPT\u3001Claude\u3001Deepseek\u548cGrok\uff09\u7684\u7ffb\u8bd1\u6027\u80fd\u3002", "result": "LLMs\u5728\u7ffb\u8bd1\u81ea\u7136\u8bed\u8a00\u5230\u73af\u5883\u52a8\u6001\u7684\u7b26\u53f7\u8868\u793a\u65b9\u9762\u8868\u73b0\u51fa\u4e00\u5b9a\u80fd\u529b\uff0c\u4f46\u6027\u80fd\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u5206\u533a\u7c92\u5ea6\u548c\u4efb\u52a1\u590d\u6742\u6027\u3002", "conclusion": "\u76ee\u524dLLMs\u5728\u8868\u793a\u5bf9\u9f50\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u8bed\u8a00\u4e0e\u4ee3\u7406\u5185\u90e8\u8868\u793a\u4e4b\u95f4\u7684\u9c81\u68d2\u5bf9\u9f50\u3002"}}
{"id": "2510.24232", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24232", "abs": "https://arxiv.org/abs/2510.24232", "authors": ["Qing Zhao", "Weijian Deng", "Pengxu Wei", "ZiYi Dong", "Hannan Lu", "Xiangyang Ji", "Liang Lin"], "title": "Delving into Cascaded Instability: A Lipschitz Continuity View on Image Restoration and Object Detection Synergy", "comment": "NeurIPS 2025", "summary": "To improve detection robustness in adverse conditions (e.g., haze and low\nlight), image restoration is commonly applied as a pre-processing step to\nenhance image quality for the detector. However, the functional mismatch\nbetween restoration and detection networks can introduce instability and hinder\neffective integration -- an issue that remains underexplored. We revisit this\nlimitation through the lens of Lipschitz continuity, analyzing the functional\ndifferences between restoration and detection networks in both the input space\nand the parameter space. Our analysis shows that restoration networks perform\nsmooth, continuous transformations, while object detectors operate with\ndiscontinuous decision boundaries, making them highly sensitive to minor\nperturbations. This mismatch introduces instability in traditional cascade\nframeworks, where even imperceptible noise from restoration is amplified during\ndetection, disrupting gradient flow and hindering optimization. To address\nthis, we propose Lipschitz-regularized object detection (LROD), a simple yet\neffective framework that integrates image restoration directly into the\ndetector's feature learning, harmonizing the Lipschitz continuity of both tasks\nduring training. We implement this framework as Lipschitz-regularized YOLO\n(LR-YOLO), extending seamlessly to existing YOLO detectors. Extensive\nexperiments on haze and low-light benchmarks demonstrate that LR-YOLO\nconsistently improves detection stability, optimization smoothness, and overall\naccuracy.", "AI": {"tldr": "\u56fe\u50cf\u6062\u590d\u548c\u68c0\u6d4b\u7f51\u7edc\u7684\u529f\u80fd\u4e0d\u5339\u914d\u4f1a\u964d\u4f4e\u68c0\u6d4b\u7684\u9c81\u68d2\u6027\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a LR-YOLO \u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u8bad\u7ec3\u671f\u95f4\u534f\u8c03\u4e24\u4e2a\u4efb\u52a1\u7684 Lipschitz \u8fde\u7eed\u6027\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u4ece\u800c\u63d0\u9ad8\u68c0\u6d4b\u7a33\u5b9a\u6027\u3001\u4f18\u5316\u5e73\u6ed1\u5ea6\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u7684\u5c06\u56fe\u50cf\u6062\u590d\u4f5c\u4e3a\u68c0\u6d4b\u524d\u7f6e\u5904\u7406\u7684\u65b9\u6cd5\u5b58\u5728\u6062\u590d\u548c\u68c0\u6d4b\u7f51\u7edc\u4e4b\u95f4\u529f\u80fd\u4e0d\u5339\u914d\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u6a21\u578b\u4e0d\u7a33\u5b9a\uff0c\u96be\u4ee5\u6709\u6548\u96c6\u6210\u3002\u8fd9\u4e2a\u95ee\u9898\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a Lipschitz \u8c03\u8282\u5bf9\u8c61\u68c0\u6d4b\uff08LROD\uff09\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5c06\u56fe\u50cf\u6062\u590d\u76f4\u63a5\u96c6\u6210\u5230\u68c0\u6d4b\u5668\u7684\u7279\u5f81\u5b66\u4e60\u4e2d\uff0c\u5e76\u5728\u8bad\u7ec3\u671f\u95f4\u534f\u8c03\u6062\u590d\u548c\u68c0\u6d4b\u4efb\u52a1\u7684 Lipschitz \u8fde\u7eed\u6027\u3002\u5177\u4f53\u5b9e\u73b0\u4e3a LR-YOLO\uff0c\u53ef\u65e0\u7f1d\u6269\u5c55\u5230\u73b0\u6709\u7684 YOLO \u68c0\u6d4b\u5668\u3002", "result": "\u5728\u6709\u96fe\u548c\u5f31\u5149\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cLR-YOLO \u6301\u7eed\u63d0\u9ad8\u4e86\u68c0\u6d4b\u7a33\u5b9a\u6027\u3001\u4f18\u5316\u5e73\u6ed1\u5ea6\u548c\u6574\u4f53\u51c6\u786e\u6027\u3002", "conclusion": "LR-YOLO \u6846\u67b6\u901a\u8fc7\u89e3\u51b3\u56fe\u50cf\u6062\u590d\u548c\u68c0\u6d4b\u7f51\u7edc\u4e4b\u95f4\u7684\u529f\u80fd\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u6709\u6548\u63d0\u9ad8\u4e86\u5728\u6076\u52a3\u6761\u4ef6\u4e0b\u7684\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\u3002"}}
{"id": "2510.24681", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.24681", "abs": "https://arxiv.org/abs/2510.24681", "authors": ["Moritz Scheer", "Alberto Baiardi", "Elisa B\u00e4umer Marty", "Zhi-Yuan Wei", "Daniel Malz"], "title": "Renormalization-group-based preparation of matrix product states on up to 80 qubits", "comment": "8 pages, 6 figures", "summary": "A key challenge for quantum computers is the efficient preparation of\nmany-body entangled states across many qubits. In this work, we demonstrate the\npreparation of matrix product states (MPS) using a\nrenormalization-group(RG)-based quantum algorithm on superconducting quantum\nhardware. Compared to sequential generation, it has been shown that the\nRG-based protocol asymptotically prepares short-range correlated MPS with an\nexponentially shallower circuit depth (when scaling system size), but it is not\nyet clear for which system sizes it starts to convey an advantage. We thus\napply this algorithm to prepare a class of MPS exhibiting a phase transition\nbetween a symmetry-protected topological (SPT) and a trivial phase for systems\nof up to 80 qubits. We find that the reduced depth of the RG-based circuits\nmakes them more resilient to noise, and that they generally outperform the\nsequential circuits for large systems, as we showcase by measuring\nstring-order-like local expectation values and energy densities. We thus\ndemonstrate that the RG-based protocol enables large-scale preparation of MPS\nand, in particular, SPT-ordered states beyond the fixed point.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u5e76\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u4e00\u79cd\u57fa\u4e8e\u91cd\u6b63\u5316\u7fa4\uff08RG\uff09\u7684\u91cf\u5b50\u7b97\u6cd5\uff0c\u7528\u4e8e\u5728\u8d85\u5bfc\u91cf\u5b50\u786c\u4ef6\u4e0a\u5236\u5907\u591a\u4f53\u7ea0\u7f20\u7684\u77e9\u9635\u4e58\u79ef\u6001\uff08MPS\uff09\u3002", "motivation": "\u91cf\u5b50\u8ba1\u7b97\u673a\u5728\u9ad8\u6548\u5236\u5907\u591a\u4f53\u7ea0\u7f20\u6001\u65b9\u9762\u9762\u4e34\u6311\u6218\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u5927\u89c4\u6a21\u7cfb\u7edf\u3002\u9700\u8981\u660e\u786eRG-based\u534f\u8bae\u5728\u591a\u5927\u7cfb\u7edf\u89c4\u6a21\u4e0b\u76f8\u5bf9\u4e8e\u987a\u5e8f\u751f\u6210\u65b9\u6848\u5177\u6709\u4f18\u52bf\u3002", "method": "\u63d0\u51fa\u5e76\u4f7f\u7528\u4e00\u79cd\u57fa\u4e8e\u91cd\u6b63\u5316\u7fa4\uff08RG\uff09\u7684\u91cf\u5b50\u7b97\u6cd5\uff0c\u5728\u8d85\u5bfc\u91cf\u5b50\u786c\u4ef6\u4e0a\u5236\u5907\u77e9\u9635\u4e58\u79ef\u6001\uff08MPS\uff09\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0cRG-based\u534f\u8bae\u5236\u5907\u7684MPS\u5bf9\u4e8e\u5927\u89c4\u6a21\u7cfb\u7edf\uff08\u9ad8\u8fbe80\u4e2a\u91cf\u5b50\u6bd4\u7279\uff09\u6bd4\u987a\u5e8f\u751f\u6210\u65b9\u6848\u66f4\u5177\u4f18\u52bf\uff0c\u5176\u51cf\u5c11\u7684\u7535\u8def\u6df1\u5ea6\u4f7f\u5176\u5bf9\u566a\u58f0\u7684\u9c81\u68d2\u6027\u66f4\u5f3a\u3002\u901a\u8fc7\u6d4b\u91cf\u7279\u5b9a\u89c2\u6d4b\u91cf\uff0c\u9a8c\u8bc1\u4e86RG-based\u534f\u8bae\u5728\u5236\u5907MPS\uff0c\u7279\u522b\u662fSPT\u5e8f\u6001\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "RG-based\u534f\u8bae\u80fd\u591f\u5b9e\u73b0\u5927\u89c4\u6a21MPS\u7684\u5236\u5907\uff0c\u5e76\u4e14\u5728\u8d85\u5bfc\u91cf\u5b50\u786c\u4ef6\u4e0a\u4f18\u4e8e\u987a\u5e8f\u751f\u6210\u65b9\u6848\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u5177\u6709\u590d\u6742\u91cf\u5b50\u5173\u8054\uff08\u5982SPT\u5e8f\u6001\uff09\u7684\u7cfb\u7edf\u65f6\u3002"}}
{"id": "2510.23786", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23786", "abs": "https://arxiv.org/abs/2510.23786", "authors": ["Joohwan Ko", "Aristofanis Rontogiannis", "Yih-En Andrew Ban", "Axel Elaldi", "Nicholas Franklin"], "title": "Relaxed Sequence Sampling for Diverse Protein Design", "comment": null, "summary": "Protein design using structure prediction models such as AlphaFold2 has shown\nremarkable success, but existing approaches like relaxed sequence optimization\n(RSO) rely on single-path gradient descent and ignore sequence-space\nconstraints, limiting diversity and designability. We introduce Relaxed\nSequence Sampling (RSS), a Markov chain Monte Carlo (MCMC) framework that\nintegrates structural and evolutionary information for protein design. RSS\noperates in continuous logit space, combining gradient-guided exploration with\nprotein language model-informed jumps. Its energy function couples\nAlphaFold2-derived structural objectives with ESM2-derived sequence priors,\nbalancing accuracy and biological plausibility. In an in silico protein binder\ndesign task, RSS produces 5$\\times$ more designable structures and 2-3$\\times$\ngreater structural diversity than RSO baselines, at equal computational cost.\nThese results highlight RSS as a principled approach for efficiently exploring\nthe protein design landscape.", "AI": {"tldr": "RSS\u662f\u4e00\u79cd\u57fa\u4e8eMCMC\u7684\u86cb\u767d\u8d28\u8bbe\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u7ed3\u6784\u548c\u8fdb\u5316\u4fe1\u606f\uff0c\u5728\u4fdd\u6301\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\uff0c\u63d0\u9ad8\u4e86\u8bbe\u8ba1\u7684\u591a\u6837\u6027\u548c\u53ef\u884c\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u86cb\u767d\u8d28\u8bbe\u8ba1\u65b9\u6cd5\uff08\u5982RSO\uff09\u4f9d\u8d56\u4e8e\u5355\u8def\u5f84\u68af\u5ea6\u4e0b\u964d\uff0c\u5ffd\u7565\u4e86\u5e8f\u5217\u7a7a\u95f4\u7684\u7ea6\u675f\uff0c\u5bfc\u81f4\u591a\u6837\u6027\u548c\u53ef\u8bbe\u8ba1\u6027\u53d7\u9650\u3002", "method": "RSS\u662f\u4e00\u4e2a\u9a6c\u5c14\u53ef\u592b\u94fe\u8499\u7279\u5361\u6d1b\uff08MCMC\uff09\u6846\u67b6\uff0c\u5728\u8fde\u7eed\u7684logit\u7a7a\u95f4\u4e2d\u8fdb\u884c\u64cd\u4f5c\uff0c\u5c06\u68af\u5ea6\u5f15\u5bfc\u7684\u63a2\u7d22\u4e0e\u86cb\u767d\u8d28\u8bed\u8a00\u6a21\u578b\uff08ESM2\uff09\u63d0\u4f9b\u7684\u4fe1\u606f\u76f8\u7ed3\u5408\u3002\u5176\u80fd\u91cf\u51fd\u6570\u7ed3\u5408\u4e86AlphaFold2\u884d\u751f\u7684\u7ed3\u6784\u76ee\u6807\u548cESM2\u884d\u751f\u7684\u5e8f\u5217\u5148\u9a8c\u3002", "result": "\u4e0eRSO\u57fa\u7ebf\u76f8\u6bd4\uff0cRSS\u5728\u8ba1\u7b97\u673a\u86cb\u767d\u8d28\u7ed3\u5408\u5242\u8bbe\u8ba1\u4efb\u52a1\u4e2d\u4ea7\u751f\u4e86\u591a5\u500d\u7684\u53ef\u8bbe\u8ba1\u7ed3\u6784\u548c2-3\u500d\u7684\u7ed3\u6784\u591a\u6837\u6027\uff0c\u800c\u8ba1\u7b97\u6210\u672c\u76f8\u5f53\u3002", "conclusion": "RSS\u662f\u4e00\u79cd\u6709\u6548\u63a2\u7d22\u86cb\u767d\u8d28\u8bbe\u8ba1\u7a7a\u95f4\u7684\u539f\u5219\u6027\u65b9\u6cd5\u3002"}}
{"id": "2510.24435", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24435", "abs": "https://arxiv.org/abs/2510.24435", "authors": ["Benjamin Grando Moreira"], "title": "Human-Level Reasoning: A Comparative Study of Large Language Models on Logical and Abstract Reasoning", "comment": "12 pages", "summary": "Evaluating reasoning ability in Large Language Models (LLMs) is important for\nadvancing artificial intelligence, as it transcends mere linguistic task\nperformance. It involves understanding whether these models truly understand\ninformation, perform inferences, and are able to draw conclusions in a logical\nand valid way. This study compare logical and abstract reasoning skills of\nseveral LLMs - including GPT, Claude, DeepSeek, Gemini, Grok, Llama, Mistral,\nPerplexity, and Sabi\\'a - using a set of eight custom-designed reasoning\nquestions. The LLM results are benchmarked against human performance on the\nsame tasks, revealing significant differences and indicating areas where LLMs\nstruggle with deduction.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u516b\u4e2a\u81ea\u5b9a\u4e49\u63a8\u7406\u95ee\u9898\uff0c\u5bf9\u6bd4\u4e86\u5305\u62ecGPT\u3001Claude\u3001DeepSeek\u3001Gemini\u3001Grok\u3001Llama\u3001Mistral\u3001Perplexity\u548cSabi'a\u5728\u5185\u7684\u591a\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u903b\u8f91\u548c\u62bd\u8c61\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u4e0e\u4eba\u7c7b\u8868\u73b0\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63ed\u793a\u4e86LLM\u5728\u6f14\u7ece\u63a8\u7406\u65b9\u9762\u5b58\u5728\u7684\u663e\u8457\u5dee\u5f02\u548c\u6311\u6218\u3002", "motivation": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u63a8\u7406\u80fd\u529b\u5bf9\u4e8e\u63a8\u52a8\u4eba\u5de5\u667a\u80fd\u53d1\u5c55\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u4e3a\u5b83\u8d85\u8d8a\u4e86\u5355\u7eaf\u7684\u8bed\u8a00\u4efb\u52a1\u8868\u73b0\uff0c\u6d89\u53ca\u5230\u6a21\u578b\u662f\u5426\u771f\u6b63\u7406\u89e3\u4fe1\u606f\u3001\u8fdb\u884c\u63a8\u7406\u4ee5\u53ca\u80fd\u5426\u4ee5\u5408\u4e4e\u903b\u8f91\u4e14\u6709\u6548\u7684\u65b9\u5f0f\u5f97\u51fa\u7ed3\u8bba\u3002", "method": "\u4f7f\u7528\u4e00\u7ec4\u81ea\u5b9a\u4e49\u8bbe\u8ba1\u7684\u63a8\u7406\u95ee\u9898\uff0c\u5bf9\u5305\u62ecGPT\u3001Claude\u3001DeepSeek\u3001Gemini\u3001Grok\u3001Llama\u3001Mistral\u3001Perplexity\u548cSabi'a\u5728\u5185\u7684\u591a\u4e2aLLM\u7684\u903b\u8f91\u548c\u62bd\u8c61\u63a8\u7406\u80fd\u529b\u8fdb\u884c\u5bf9\u6bd4\u8bc4\u4f30\uff0c\u5e76\u5c06LLM\u7684\u7ed3\u679c\u4e0e\u540c\u4e00\u4efb\u52a1\u4e0a\u7684\u4eba\u7c7b\u8868\u73b0\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u63ed\u793a\u4e86LLM\u5728\u63a8\u7406\u80fd\u529b\u65b9\u9762\u4e0e\u4eba\u7c7b\u8868\u73b0\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u5e76\u6307\u51fa\u4e86LLM\u5728\u6f14\u7ece\u63a8\u7406\u65b9\u9762\u9047\u5230\u7684\u56f0\u96be\u3002", "conclusion": "LLM\u5728\u63a8\u7406\u80fd\u529b\u65b9\u9762\u4e0e\u4eba\u7c7b\u8868\u73b0\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u8868\u660e\u5b83\u4eec\u5728\u6f14\u7ece\u63a8\u7406\u65b9\u9762\u5b58\u5728\u6311\u6218\u3002"}}
{"id": "2510.24295", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24295", "abs": "https://arxiv.org/abs/2510.24295", "authors": ["M\u0103d\u0103lina Zgreab\u0103n", "Tejaswini Deoskar", "Lasha Abzianidze"], "title": "MERGE: Minimal Expression-Replacement GEneralization Test for Natural Language Inference", "comment": "Pre-print", "summary": "In recent years, many generalization benchmarks have shown language models'\nlack of robustness in natural language inference (NLI). However, manually\ncreating new benchmarks is costly, while automatically generating high-quality\nones, even by modifying existing benchmarks, is extremely difficult. In this\npaper, we propose a methodology for automatically generating high-quality\nvariants of original NLI problems by replacing open-class words, while\ncrucially preserving their underlying reasoning. We dub our generalization test\nas MERGE (Minimal Expression-Replacements GEneralization), which evaluates the\ncorrectness of models' predictions across reasoning-preserving variants of the\noriginal problem. Our results show that NLI models' perform 4-20% worse on\nvariants, suggesting low generalizability even on such minimally altered\nproblems. We also analyse how word class of the replacements, word probability,\nand plausibility influence NLI models' performance.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2510.24260", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24260", "abs": "https://arxiv.org/abs/2510.24260", "authors": ["Zhaotong Yang", "Yi Chen", "Yanying Li", "Shengfeng He", "Yangyang Xu", "Junyu Dong", "Jian Yang", "Yong Du"], "title": "DeshadowMamba: Deshadowing as 1D Sequential Similarity", "comment": null, "summary": "Recent deep models for image shadow removal often rely on attention-based\narchitectures to capture long-range dependencies. However, their fixed\nattention patterns tend to mix illumination cues from irrelevant regions,\nleading to distorted structures and inconsistent colors. In this work, we\nrevisit shadow removal from a sequence modeling perspective and explore the use\nof Mamba, a selective state space model that propagates global context through\ndirectional state transitions. These transitions yield an efficient global\nreceptive field while preserving positional continuity. Despite its potential,\ndirectly applying Mamba to image data is suboptimal, since it lacks awareness\nof shadow-non-shadow semantics and remains susceptible to color interference\nfrom nearby regions. To address these limitations, we propose CrossGate, a\ndirectional modulation mechanism that injects shadow-aware similarity into\nMamba's input gate, allowing selective integration of relevant context along\ntransition axes. To further ensure appearance fidelity, we introduce ColorShift\nregularization, a contrastive learning objective driven by global color\nstatistics. By synthesizing structured informative negatives, it guides the\nmodel to suppress color contamination and achieve robust color restoration.\nTogether, these components adapt sequence modeling to the structural integrity\nand chromatic consistency required for shadow removal. Extensive experiments on\npublic benchmarks demonstrate that DeshadowMamba achieves state-of-the-art\nvisual quality and strong quantitative performance.", "AI": {"tldr": "Mamba\u6a21\u578b\u5728\u56fe\u50cf\u53bb\u9634\u5f71\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u63d0\u51faCrossGate\u548cColorShift\u6765\u89e3\u51b3\u5176\u5c40\u9650\u6027\uff0c\u6700\u7ec8\u53d6\u5f97SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684\u56fe\u50cf\u53bb\u9634\u5f71\u6a21\u578b\u5bb9\u6613\u6df7\u6dc6\u4e0d\u76f8\u5173\u533a\u57df\u7684\u5149\u7167\u4fe1\u606f\uff0c\u5bfc\u81f4\u7ed3\u6784\u626d\u66f2\u548c\u989c\u8272\u4e0d\u4e00\u81f4\u3002Mamba\u6a21\u578b\u867d\u7136\u53ef\u4ee5\u6355\u6349\u5168\u5c40\u4e0a\u4e0b\u6587\uff0c\u4f46\u76f4\u63a5\u5e94\u7528\u4e8e\u56fe\u50cf\u6570\u636e\u65f6\u7f3a\u4e4f\u5bf9\u9634\u5f71\u548c\u975e\u9634\u5f71\u533a\u57df\u7684\u8bed\u4e49\u7406\u89e3\uff0c\u5e76\u4e14\u5bb9\u6613\u53d7\u5230\u90bb\u8fd1\u533a\u57df\u989c\u8272\u5e72\u6270\u3002", "method": "\u63d0\u51faCrossGate\uff0c\u4e00\u79cd\u5b9a\u5411\u8c03\u5236\u673a\u5236\uff0c\u5c06\u9634\u5f71\u611f\u77e5\u7684\u76f8\u4f3c\u6027\u6ce8\u5165Mamba\u7684\u8f93\u5165\u95e8\uff0c\u4ee5\u9009\u62e9\u6027\u5730\u6574\u5408\u6cbf\u8f6c\u6362\u8f74\u7684\u76f8\u5173\u4e0a\u4e0b\u6587\u3002\u63d0\u51faColorShift\u6b63\u5219\u5316\uff0c\u4e00\u79cd\u7531\u5168\u5c40\u989c\u8272\u7edf\u8ba1\u9a71\u52a8\u7684\u5bf9\u6bd4\u5b66\u4e60\u76ee\u6807\uff0c\u901a\u8fc7\u5408\u6210\u7ed3\u6784\u5316\u7684\u4fe1\u606f\u8d1f\u6837\u672c\uff0c\u5f15\u5bfc\u6a21\u578b\u6291\u5236\u989c\u8272\u6c61\u67d3\u5e76\u5b9e\u73b0\u9c81\u68d2\u7684\u989c\u8272\u6062\u590d\u3002", "result": "DeshadowMamba\u5728\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u89c6\u89c9\u8d28\u91cf\u548c\u5f3a\u5927\u7684\u91cf\u5316\u6027\u80fd\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u7ec4\u4ef6\u80fd\u591f\u5c06\u5e8f\u5217\u5efa\u6a21\u9002\u5e94\u4e8e\u56fe\u50cf\u53bb\u9634\u5f71\u6240\u9700\u7684\u7ed3\u6784\u5b8c\u6574\u6027\u548c\u8272\u5f69\u4e00\u81f4\u6027\u3002"}}
{"id": "2510.24713", "categories": ["quant-ph", "cond-mat.stat-mech", "cond-mat.str-el", "math-ph", "math.MP"], "pdf": "https://arxiv.org/pdf/2510.24713", "abs": "https://arxiv.org/abs/2510.24713", "authors": ["Lei Gioia", "Sanjay Moudgalya", "Olexei I. Motrunich"], "title": "Distinct Types of Parent Hamiltonians for Quantum States: Insights from the $W$ State as a Quantum Many-Body Scar", "comment": "24+22 pages, 6 figures", "summary": "The construction of parent Hamiltonians that possess a given state as their\nground state is a well-studied problem. In this work, we generalize this notion\nby considering simple quantum states and examining the local Hamiltonians that\nhave these states as exact eigenstates.These states often correspond to Quantum\nMany-Body Scars (QMBS) of their respective parent Hamiltonians.Motivated by\nearlier works on Hamiltonians with QMBS, in this work we formalize the\ndifferences between three distinct types of parent Hamiltonians, which differ\nin their decompositions into strictly local terms with the same eigenstates. We\nillustrate this classification using the $W$ state as the primary example, for\nwhich we rigorously derive the complete set of local parent Hamiltonians, which\nalso allows us to establish general results such as the existence of asymptotic\nQMBS, and distinct dynamical signatures associated with the different parent\nHamiltonian types. Finally, we derive more general results on the parent\nHamiltonian types that allow us to obtain some immediate results for simple\nquantum states such as product states, where only a single type exists, and for\nshort-range-entangled states, for which we identify constraints on the\nadmissible types. Altogether, our work opens the door to classifying the rich\nstructures and dynamical properties of parent Hamiltonians that arise from the\ninterplay between locality and QMBS.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5177\u6709\u7ed9\u5b9a\u91cf\u5b50\u6001\u4f5c\u4e3a\u672c\u5f81\u6001\u7684\u5c40\u90e8\u54c8\u5bc6\u987f\u91cf\uff0c\u7279\u522b\u662f\u4e0e\u91cf\u5b50\u591a\u4f53\u75a4\u75d5\uff08QMBS\uff09\u76f8\u5173\u7684\u54c8\u5bc6\u987f\u91cf\u3002\u6211\u4eec\u5bf9\u4e09\u79cd\u4e0d\u540c\u7c7b\u578b\u7684\u7236\u54c8\u5bc6\u987f\u91cf\u8fdb\u884c\u4e86\u5206\u7c7b\uff0c\u5e76\u4ee5W\u6001\u4e3a\u4f8b\u8fdb\u884c\u4e86\u8bf4\u660e\uff0c\u63a8\u5bfc\u4e86\u6240\u6709\u5c40\u57df\u7236\u54c8\u5bc6\u987f\u91cf\uff0c\u5e76\u5f97\u51fa\u4e86\u4e00\u4e9b\u4e00\u822c\u6027\u7ed3\u8bba\uff0c\u4f8b\u5982\u6e10\u8fd1QMBS\u7684\u5b58\u5728\u4ee5\u53ca\u4e0d\u540c\u7236\u54c8\u5bc6\u987f\u91cf\u7c7b\u578b\u7684\u52a8\u529b\u5b66\u7279\u5f81\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u5bf9\u4e58\u79ef\u6001\u548c\u77ed\u7a0b\u7ea0\u7f20\u6001\u7684\u7236\u54c8\u5bc6\u987f\u91cf\u7c7b\u578b\u8fdb\u884c\u4e86\u7814\u7a76\u3002", "motivation": "\u53d7\u5230\u5177\u6709QMBS\u7684\u54c8\u5bc6\u987f\u91cf\u76f8\u5173\u5de5\u4f5c\u7684\u542f\u53d1\uff0c\u672c\u6587\u65e8\u5728\u5f62\u5f0f\u5316\u533a\u5206\u4e09\u79cd\u4e0d\u540c\u7c7b\u578b\u7684\u7236\u54c8\u5bc6\u987f\u91cf\uff0c\u8fd9\u4e9b\u54c8\u5bc6\u987f\u91cf\u5728\u5206\u89e3\u4e3a\u5177\u6709\u76f8\u540c\u672c\u5f81\u6001\u7684\u4e25\u683c\u5c40\u57df\u9879\u65b9\u9762\u5b58\u5728\u5dee\u5f02\u3002", "method": "\u672c\u6587\u5bf9\u5177\u6709\u7ed9\u5b9a\u72b6\u6001\uff08\u7279\u522b\u662fQMBS\uff09\u4f5c\u4e3a\u672c\u5f81\u6001\u7684\u5c40\u57df\u7236\u54c8\u5bc6\u987f\u91cf\u8fdb\u884c\u4e86\u5206\u7c7b\u3002\u901a\u8fc7\u4ee5W\u6001\u4e3a\u4f8b\uff0c\u4e25\u683c\u63a8\u5bfc\u4e86\u5b8c\u6574\u7684\u5c40\u57df\u7236\u54c8\u5bc6\u987f\u91cf\u96c6\uff0c\u5e76\u5f97\u51fa\u4e86\u4e00\u822c\u6027\u7ed3\u679c\uff0c\u5982\u6e10\u8fd1QMBS\u7684\u5b58\u5728\u548c\u4e0d\u540c\u7236\u54c8\u5bc6\u987f\u91cf\u7c7b\u578b\u7684\u52a8\u529b\u5b66\u7279\u5f81\u3002\u6b64\u5916\uff0c\u8fd8\u5bf9\u4e58\u79ef\u6001\u548c\u77ed\u7a0b\u7ea0\u7f20\u6001\u7684\u7236\u54c8\u5bc6\u987f\u91cf\u7c7b\u578b\u8fdb\u884c\u4e86\u7814\u7a76\u3002", "result": "\u4ee5W\u6001\u4e3a\u4f8b\uff0c\u6211\u4eec\u4e25\u683c\u63a8\u5bfc\u4e86\u5b8c\u6574\u7684\u5c40\u57df\u7236\u54c8\u5bc6\u987f\u91cf\u96c6\u3002\u6211\u4eec\u5f97\u51fa\u4e86\u6e10\u8fd1QMBS\u7684\u5b58\u5728\u4ee5\u53ca\u4e0e\u4e0d\u540c\u7236\u54c8\u5bc6\u987f\u91cf\u7c7b\u578b\u76f8\u5173\u7684\u4e0d\u540c\u52a8\u529b\u5b66\u7279\u5f81\u3002\u5bf9\u4e8e\u4e58\u79ef\u6001\uff0c\u53ea\u5b58\u5728\u4e00\u79cd\u7c7b\u578b\u7684\u7236\u54c8\u5bc6\u987f\u91cf\uff1b\u5bf9\u4e8e\u77ed\u7a0b\u7ea0\u7f20\u6001\uff0c\u6211\u4eec\u786e\u5b9a\u4e86\u53ef\u63a5\u53d7\u7c7b\u578b\u7684\u7ea6\u675f\u3002", "conclusion": "\u672c\u6587\u5bf9\u5c40\u57df\u7236\u54c8\u5bc6\u987f\u91cf\u7684\u7ed3\u6784\u548c\u52a8\u529b\u5b66\u6027\u8d28\u8fdb\u884c\u4e86\u5206\u7c7b\uff0c\u63ed\u793a\u4e86\u5176\u4e0eQMBS\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u4e3a\u8fdb\u4e00\u6b65\u7814\u7a76\u6b64\u7c7b\u54c8\u5bc6\u987f\u91cf\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2510.23794", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23794", "abs": "https://arxiv.org/abs/2510.23794", "authors": ["Jun Liu", "Tao Zhou", "Jiarui Li", "Xiaohui Zhong", "Peng Zhang", "Jie Feng", "Lei Chen", "Hao Li"], "title": "Revealing the Potential of Learnable Perturbation Ensemble Forecast Model for Tropical Cyclone Prediction", "comment": "30 pages, 21 figures, 1 table", "summary": "Tropical cyclones (TCs) are highly destructive and inherently uncertain\nweather systems. Ensemble forecasting helps quantify these uncertainties, yet\ntraditional systems are constrained by high computational costs and limited\ncapability to fully represent atmospheric nonlinearity. FuXi-ENS introduces a\nlearnable perturbation scheme for ensemble generation, representing a novel\nAI-based forecasting paradigm. Here, we systematically compare FuXi-ENS with\nECMWF-ENS using all 90 global TCs in 2018, examining their performance in\nTC-related physical variables, track and intensity forecasts, and the\nassociated dynamical and thermodynamical fields. FuXi-ENS demonstrates clear\nadvantages in predicting TC-related physical variables, and achieves more\naccurate track forecasts with reduced ensemble spread, though it still\nunderestimates intensity relative to observations. Further dynamical and\nthermodynamical analyses reveal that FuXi-ENS better captures large-scale\ncirculation, with moisture turbulent energy more tightly concentrated around\nthe TC warm core, whereas ECMWF-ENS exhibits a more dispersed distribution.\nThese findings highlight the potential of learnable perturbations to improve TC\nforecasting skill and provide valuable insights for advancing AI-based ensemble\nprediction of extreme weather events that have significant societal impacts.", "AI": {"tldr": "FuXi-ENS\u662f\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8eAI\u7684\u96c6\u5408\u9884\u62a5\u65b9\u6848\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u6270\u52a8\u65b9\u6848\u6765\u751f\u6210\u96c6\u5408\u9884\u62a5\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u96c6\u5408\u9884\u62a5\u7684\u8ba1\u7b97\u6210\u672c\u9ad8\u548c\u975e\u7ebf\u6027\u8868\u793a\u80fd\u529b\u6709\u9650\u7684\u7f3a\u70b9\u3002\u901a\u8fc7\u5bf9\u6bd42018\u5e7490\u4e2a\u70ed\u5e26\u6c14\u65cb\u7684\u9884\u62a5\uff0cFuXi-ENS\u5728\u7269\u7406\u53d8\u91cf\u548c\u8def\u5f84\u9884\u62a5\u65b9\u9762\u4f18\u4e8eECMWF-ENS\uff0c\u4f46\u5f3a\u5ea6\u9884\u62a5\u4ecd\u6709\u4e0d\u8db3\u3002FuXi-ENS\u5728\u6355\u6349\u5927\u5c3a\u5ea6\u73af\u6d41\u548c\u6a21\u62dfTC\u6696\u5fc3\u5468\u56f4\u7684\u6c34\u6c7d\u6e4d\u6d41\u52a8\u80fd\u5206\u5e03\u65b9\u9762\u8868\u73b0\u66f4\u597d\u3002", "motivation": "\u70ed\u5e26\u6c14\u65cb\uff08TC\uff09\u7684\u7834\u574f\u6027\u5f3a\u4e14\u9884\u62a5\u4e0d\u786e\u5b9a\u6027\u9ad8\u3002\u4f20\u7edf\u7684\u96c6\u5408\u9884\u62a5\u7cfb\u7edf\u5728\u8ba1\u7b97\u6210\u672c\u548c\u975e\u7ebf\u6027\u8868\u793a\u80fd\u529b\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u6765\u91cf\u5316TC\u9884\u62a5\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFuXi-ENS\u7684\u53ef\u5b66\u4e60\u6270\u52a8\u65b9\u6848\uff0c\u7528\u4e8e\u751f\u6210\u96c6\u5408\u9884\u62a5\u3002\u7cfb\u7edf\u6027\u5730\u6bd4\u8f83\u4e86FuXi-ENS\u548cECMWF-ENS\u57282018\u5e74\u5168\u90e890\u4e2a\u70ed\u5e26\u6c14\u65cb\u4e0a\u7684\u8868\u73b0\uff0c\u5206\u6790\u4e86TC\u76f8\u5173\u7684\u7269\u7406\u53d8\u91cf\u3001\u8def\u5f84\u548c\u5f3a\u5ea6\u9884\u62a5\uff0c\u4ee5\u53ca\u76f8\u5173\u7684\u52a8\u529b\u5b66\u548c\u70ed\u529b\u5b66\u573a\u3002", "result": "FuXi-ENS\u5728TC\u76f8\u5173\u7269\u7406\u53d8\u91cf\u9884\u62a5\u65b9\u9762\u8868\u73b0\u51fa\u660e\u663e\u4f18\u52bf\uff0c\u8def\u5f84\u9884\u62a5\u66f4\u51c6\u786e\u4e14\u96c6\u5408\u79bb\u6563\u5ea6\u66f4\u5c0f\uff0c\u4f46\u5728\u5f3a\u5ea6\u9884\u62a5\u65b9\u9762\u4ecd\u4f4e\u4f30\u4e86\u89c2\u6d4b\u503c\u3002\u52a8\u529b\u5b66\u548c\u70ed\u529b\u5b66\u5206\u6790\u8868\u660e\uff0cFuXi-ENS\u80fd\u66f4\u597d\u5730\u6355\u6349\u5927\u5c3a\u5ea6\u73af\u6d41\uff0c\u4e14\u6c34\u6c7d\u6e4d\u6d41\u52a8\u80fd\u66f4\u96c6\u4e2d\u5728TC\u6696\u5fc3\u5468\u56f4\uff0c\u800cECMWF-ENS\u7684\u5206\u5e03\u5219\u66f4\u5206\u6563\u3002", "conclusion": "\u53ef\u5b66\u4e60\u6270\u52a8\u65b9\u6848\u5728\u63d0\u9ad8TC\u9884\u62a5\u80fd\u529b\u65b9\u9762\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u5e76\u4e3a\u6539\u8fdb\u5bf9\u793e\u4f1a\u4ea7\u751f\u91cd\u5927\u5f71\u54cd\u7684\u6781\u7aef\u5929\u6c14\u4e8b\u4ef6\u7684AI\u96c6\u5408\u9884\u62a5\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002"}}
{"id": "2510.24302", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24302", "abs": "https://arxiv.org/abs/2510.24302", "authors": ["Shangyu Xing", "Siyuan Wang", "Chenyuan Yang", "Xinyu Dai", "Xiang Ren"], "title": "Lookahead Tree-Based Rollouts for Enhanced Trajectory-Level Exploration in Reinforcement Learning with Verifiable Rewards", "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR), particularly with\nalgorithms like Group Relative Policy Optimization (GRPO), has proven highly\neffective in enhancing the reasoning capabilities of large language models.\nHowever, a critical bottleneck in current pipelines lies in the limited\ndiversity of sampled trajectories during group rollouts. Homogeneous\ntrajectories and their associated rewards would diminish the return signals for\npolicy updates, thereby hindering effective policy learning. This lack of\ndiversity stems primarily from token-level stochastic sampling, where local\nvariations are likely to collapse into near-identical reasoning paths. To\naddress this limitation, we propose Lookahead Tree-Based Rollouts (LATR), a\nnovel rollout strategy designed to explicitly promotes trajectory-level\ndiversity by enforcing branching into different candidate tokens likely to\nyield distinct continuations. Specifically, LATR iteratively operates in three\nstages: (1) branching at high-uncertainty generation steps, (2) performing\nlookahead simulation for each new branch, and (3) pruning branches that\nexhibits prolonged similarity during simulation. Compared with stochastic\nSampling, LATR accelerates policy learning by 131% on average and improves\nfinal pass@1 performance by 4.2% on both GRPO and Dynamic sAmpling Policy\nOptimization (DAPO) algorithms across different reasoning tasks. Our code and\ndata are publicly available at https://github.com/starreeze/latr.", "AI": {"tldr": "RLVR\u4e2d\u7684\u8f68\u8ff9\u91c7\u6837\u7f3a\u4e4f\u591a\u6837\u6027\uff0c\u5f71\u54cd\u7b56\u7565\u5b66\u4e60\u3002LATR\u901a\u8fc7\u5f15\u5165\u5206\u652f\u3001\u524d\u77bb\u6a21\u62df\u548c\u526a\u679d\u6765\u89e3\u51b3\u6b64\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u7b56\u7565\u5b66\u4e60\u901f\u5ea6\u548c\u6700\u7ec8\u6027\u80fd\u3002", "motivation": "\u5f53\u524dRLVR\u65b9\u6cd5\uff08\u5982GRPO\uff09\u5728\u63d0\u5347LLM\u63a8\u7406\u80fd\u529b\u65b9\u9762\u6548\u679c\u663e\u8457\uff0c\u4f46\u5176\u8f68\u8ff9\u91c7\u6837\u591a\u6837\u6027\u4e0d\u8db3\uff0c\u5bfc\u81f4\u5956\u52b1\u4fe1\u53f7\u51cf\u5f31\uff0c\u963b\u788d\u4e86\u6709\u6548\u7684\u7b56\u7565\u5b66\u4e60\u3002\u8fd9\u4e3b\u8981\u662f\u7531\u4ee3\u5e01\u7ea7\u522b\u7684\u968f\u673a\u91c7\u6837\u5f15\u8d77\u7684\uff0c\u5c40\u90e8\u53d8\u5316\u5bb9\u6613\u5bfc\u81f4\u63a8\u7406\u8def\u5f84\u8d8b\u540c\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLATR\uff08Lookahead Tree-Based Rollouts\uff09\u7684\u65b0\u578b\u6eda\u52a8\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u901a\u8fc7\u5f3a\u5236\u5206\u652f\u5230\u53ef\u80fd\u4ea7\u751f\u4e0d\u540c\u540e\u7eed\u7684\u5019\u9009\u4ee3\u5e01\u6765\u663e\u5f0f\u5730\u4fc3\u8fdb\u8f68\u8ff9\u7ea7\u522b\u7684\u591a\u6837\u6027\u3002LATR\u5206\u4e09\u4e2a\u9636\u6bb5\u8fdb\u884c\uff1a1. \u5728\u9ad8\u4e0d\u786e\u5b9a\u6027\u751f\u6210\u6b65\u9aa4\u4e2d\u8fdb\u884c\u5206\u652f\uff1b2. \u5bf9\u6bcf\u4e2a\u65b0\u5206\u652f\u8fdb\u884c\u524d\u77bb\u6a21\u62df\uff1b3. \u526a\u679d\u5728\u6a21\u62df\u8fc7\u7a0b\u4e2d\u8868\u73b0\u51fa\u6301\u7eed\u76f8\u4f3c\u6027\u7684\u5206\u652f\u3002", "result": "\u4e0e\u968f\u673a\u91c7\u6837\u76f8\u6bd4\uff0cLATR\u5728GRPO\u548cDAPO\u7b97\u6cd5\u4e0a\uff0c\u8de8\u4e0d\u540c\u63a8\u7406\u4efb\u52a1\u7684\u5e73\u5747\u7b56\u7565\u5b66\u4e60\u901f\u5ea6\u63d0\u9ad8\u4e86131%\uff0c\u6700\u7ec8pass@1\u6027\u80fd\u63d0\u9ad8\u4e864.2%\u3002", "conclusion": "LATR\u901a\u8fc7\u5f15\u5165\u8f68\u8ff9\u591a\u6837\u6027\uff0c\u6709\u6548\u89e3\u51b3\u4e86RLVR\u4e2d\u91c7\u6837\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7b56\u7565\u5b66\u4e60\u7684\u6548\u7387\u548cLLM\u7684\u63a8\u7406\u6027\u80fd\u3002"}}
{"id": "2510.24399", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24399", "abs": "https://arxiv.org/abs/2510.24399", "authors": ["Toan Van Nguyen", "Rasmus G. K. Christiansen", "Dirk Kraft", "Leon Bodenhagen"], "title": "GenTrack: A New Generation of Multi-Object Tracking", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "This paper introduces a novel multi-object tracking (MOT) method, dubbed\nGenTrack, whose main contributions include: a hybrid tracking approach\nemploying both stochastic and deterministic manners to robustly handle unknown\nand time-varying numbers of targets, particularly in maintaining target\nidentity (ID) consistency and managing nonlinear dynamics, leveraging particle\nswarm optimization (PSO) with some proposed fitness measures to guide\nstochastic particles toward their target distribution modes, enabling effective\ntracking even with weak and noisy object detectors, integration of social\ninteractions among targets to enhance PSO-guided particles as well as improve\ncontinuous updates of both strong (matched) and weak (unmatched) tracks,\nthereby reducing ID switches and track loss, especially during occlusions, a\nGenTrack-based redefined visual MOT baseline incorporating a comprehensive\nstate and observation model based on space consistency, appearance, detection\nconfidence, track penalties, and social scores for systematic and efficient\ntarget updates, and the first-ever publicly available source-code reference\nimplementation with minimal dependencies, featuring three variants, including\nGenTrack Basic, PSO, and PSO-Social, facilitating flexible reimplementation.\nExperimental results have shown that GenTrack provides superior performance on\nstandard benchmarks and real-world scenarios compared to state-of-the-art\ntrackers, with integrated implementations of baselines for fair comparison.\nPotential directions for future work are also discussed. The source-code\nreference implementations of both the proposed method and compared-trackers are\nprovided on GitHub: https://github.com/SDU-VelKoTek/GenTrack", "AI": {"tldr": "GenTrack\u662f\u4e00\u79cd\u6df7\u5408\u591a\u76ee\u6807\u8ddf\u8e2a\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u968f\u673a\u548c\u786e\u5b9a\u6027\u65b9\u6cd5\uff0c\u5e76\u4f7f\u7528\u7c92\u5b50\u7fa4\u4f18\u5316\u6765\u63d0\u9ad8\u76ee\u6807ID\u4e00\u81f4\u6027\u548c\u5904\u7406\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u3002\u5b83\u901a\u8fc7\u6574\u5408\u76ee\u6807\u95f4\u7684\u793e\u4ea4\u4e92\u52a8\u6765\u6539\u8fdb\u8ddf\u8e2a\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u7684\u89c6\u89c9MOT\u57fa\u7ebf\u3002\u8be5\u65b9\u6cd5\u5728\u6807\u51c6\u57fa\u51c6\u548c\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u8ddf\u8e2a\u5668\uff0c\u5e76\u63d0\u4f9b\u4e86\u516c\u5f00\u7684\u6e90\u4ee3\u7801\u5b9e\u73b0\u3002", "motivation": "\u5904\u7406\u672a\u77e5\u548c\u65f6\u53d8\u7684\u76ee\u6807\u6570\u91cf\uff0c\u7279\u522b\u662f\u5728\u4fdd\u6301\u76ee\u6807ID\u4e00\u81f4\u6027\u548c\u5904\u7406\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u65b9\u9762\uff0c\u5c24\u5176\u662f\u5728\u906e\u6321\u671f\u95f4\u3002", "method": "\u91c7\u7528\u6df7\u5408\u8ddf\u8e2a\u65b9\u6cd5\uff0c\u7ed3\u5408\u968f\u673a\uff08\u7c92\u5b50\u7fa4\u4f18\u5316\uff09\u548c\u786e\u5b9a\u6027\u65b9\u6cd5\u3002PSO\u901a\u8fc7\u63d0\u51fa\u7684\u9002\u5e94\u5ea6\u51fd\u6570\u5f15\u5bfc\u7c92\u5b50\uff0c\u5e76\u6574\u5408\u4e86\u76ee\u6807\u95f4\u7684\u793e\u4ea4\u4e92\u52a8\u6765\u66f4\u65b0\u5f3a\u5f31\u8f68\u9053\u3002GenTrack\u8fd8\u57fa\u4e8e\u7a7a\u95f4\u4e00\u81f4\u6027\u3001\u5916\u89c2\u3001\u68c0\u6d4b\u7f6e\u4fe1\u5ea6\u3001\u8f68\u9053\u60e9\u7f5a\u548c\u793e\u4f1a\u5206\u6570\u91cd\u65b0\u5b9a\u4e49\u4e86\u89c6\u89c9MOT\u57fa\u7ebf\u3002", "result": "GenTrack\u5728\u6807\u51c6\u57fa\u51c6\u548c\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u63d0\u4f9b\u4e86\u4f18\u4e8e\u6700\u5148\u8fdb\u8ddf\u8e2a\u5668\u7684\u6027\u80fd\uff0c\u5e76\u51cf\u5c11\u4e86ID\u5207\u6362\u548c\u8f68\u9053\u4e22\u5931\u3002", "conclusion": "GenTrack\u662f\u4e00\u79cd\u6709\u6548\u7684\u591a\u76ee\u6807\u8ddf\u8e2a\u65b9\u6cd5\uff0c\u7279\u522b\u64c5\u957f\u5904\u7406\u76ee\u6807\u6570\u91cf\u672a\u77e5\u548c\u65f6\u53d8\u7684\u60c5\u51b5\uff0c\u5e76\u80fd\u4fdd\u6301\u76ee\u6807ID\u7684\u4e00\u81f4\u6027\u3002"}}
{"id": "2510.24262", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24262", "abs": "https://arxiv.org/abs/2510.24262", "authors": ["Jiyu Guo", "Shuo Yang", "Yiming Huang", "Yancheng Long", "Xiaobo Xia", "Xiu Su", "Bo Zhao", "Zeke Xie", "Liqiang Nie"], "title": "UtilGen: Utility-Centric Generative Data Augmentation with Dual-Level Task Adaptation", "comment": "39th Conference on Neural Information Processing Systems (NeurIPS\n  2025)", "summary": "Data augmentation using generative models has emerged as a powerful paradigm\nfor enhancing performance in computer vision tasks. However, most existing\naugmentation approaches primarily focus on optimizing intrinsic data attributes\n-- such as fidelity and diversity -- to generate visually high-quality\nsynthetic data, while often neglecting task-specific requirements. Yet, it is\nessential for data generators to account for the needs of downstream tasks, as\ntraining data requirements can vary significantly across different tasks and\nnetwork architectures. To address these limitations, we propose UtilGen, a\nnovel utility-centric data augmentation framework that adaptively optimizes the\ndata generation process to produce task-specific, high-utility training data\nvia downstream task feedback. Specifically, we first introduce a weight\nallocation network to evaluate the task-specific utility of each synthetic\nsample. Guided by these evaluations, UtilGen iteratively refines the data\ngeneration process using a dual-level optimization strategy to maximize the\nsynthetic data utility: (1) model-level optimization tailors the generative\nmodel to the downstream task, and (2) instance-level optimization adjusts\ngeneration policies -- such as prompt embeddings and initial noise -- at each\ngeneration round. Extensive experiments on eight benchmark datasets of varying\ncomplexity and granularity demonstrate that UtilGen consistently achieves\nsuperior performance, with an average accuracy improvement of 3.87% over\nprevious SOTA. Further analysis of data influence and distribution reveals that\nUtilGen produces more impactful and task-relevant synthetic data, validating\nthe effectiveness of the paradigm shift from visual characteristics-centric to\ntask utility-centric data augmentation.", "AI": {"tldr": "UtilGen\u662f\u4e00\u4e2a\u4ee5\u6548\u7528\u4e3a\u4e2d\u5fc3\u7684\u6570\u636e\u589e\u5f3a\u6846\u67b6\uff0c\u901a\u8fc7\u4e0b\u6e38\u4efb\u52a1\u53cd\u9988\u81ea\u9002\u5e94\u5730\u4f18\u5316\u6570\u636e\u751f\u6210\u8fc7\u7a0b\uff0c\u4ee5\u4ea7\u751f\u7279\u5b9a\u4efb\u52a1\u7684\u9ad8\u6548\u7528\u8bad\u7ec3\u6570\u636e\uff0c\u5e73\u5747\u51c6\u786e\u7387\u63d0\u53473.87%\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u89c6\u89c9\u8d28\u91cf\uff0c\u5ffd\u7565\u4e86\u4e0b\u6e38\u4efb\u52a1\u7684\u5177\u4f53\u9700\u6c42\uff0c\u800c\u8fd9\u4e9b\u9700\u6c42\u5728\u4e0d\u540c\u4efb\u52a1\u548c\u7f51\u7edc\u67b6\u6784\u4e4b\u95f4\u5dee\u5f02\u5f88\u5927\u3002", "method": "UtilGen\u6846\u67b6\u5f15\u5165\u4e00\u4e2a\u6743\u91cd\u5206\u914d\u7f51\u7edc\u6765\u8bc4\u4f30\u6bcf\u4e2a\u5408\u6210\u6837\u672c\u7684\u4efb\u52a1\u7279\u5b9a\u6548\u7528\uff0c\u5e76\u91c7\u7528\u6a21\u578b\u7ea7\u548c\u5b9e\u4f8b\u7ea7\u7684\u53cc\u91cd\u4f18\u5316\u7b56\u7565\u6765\u6700\u5927\u5316\u5408\u6210\u6570\u636e\u6548\u7528\uff1a(1)\u6a21\u578b\u7ea7\u4f18\u5316\u8c03\u6574\u751f\u6210\u6a21\u578b\u4ee5\u9002\u5e94\u4e0b\u6e38\u4efb\u52a1\uff1b(2)\u5b9e\u4f8b\u7ea7\u4f18\u5316\u8c03\u6574\u6bcf\u6b21\u751f\u6210\u7684\u56de\u5408\u7684\u751f\u6210\u7b56\u7565\uff08\u5982\u63d0\u793a\u5d4c\u5165\u548c\u521d\u59cb\u566a\u58f0\uff09\u3002", "result": "\u5728\u516b\u4e2a\u4e0d\u540c\u590d\u6742\u5ea6\u548c\u7c92\u5ea6\u7684\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cUtilGen\u7684\u6027\u80fd\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e73\u5747\u51c6\u786e\u7387\u63d0\u9ad8\u4e863.87%\u3002\u6570\u636e\u5f71\u54cd\u529b\u548c\u5206\u5e03\u5206\u6790\u663e\u793a\uff0cUtilGen\u4ea7\u751f\u7684\u5408\u6210\u6570\u636e\u66f4\u5177\u5f71\u54cd\u529b\u548c\u4efb\u52a1\u76f8\u5173\u6027\u3002", "conclusion": "UtilGen\u901a\u8fc7\u5c06\u6570\u636e\u589e\u5f3a\u7684\u7126\u70b9\u4ece\u89c6\u89c9\u7279\u5f81\u8f6c\u5411\u4efb\u52a1\u6548\u7528\uff0c\u6709\u6548\u5730\u63d0\u9ad8\u4e86\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u8303\u5f0f\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.23802", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23802", "abs": "https://arxiv.org/abs/2510.23802", "authors": ["Nathan Paek", "Yongyi Zang", "Qihui Yang", "Randal Leistikow"], "title": "Learning Interpretable Features in Audio Latent Spaces via Sparse Autoencoders", "comment": "Accepted to NeurIPS 2025 Mechanistic Interpretability Workshop", "summary": "While sparse autoencoders (SAEs) successfully extract interpretable features\nfrom language models, applying them to audio generation faces unique\nchallenges: audio's dense nature requires compression that obscures semantic\nmeaning, and automatic feature characterization remains limited. We propose a\nframework for interpreting audio generative models by mapping their latent\nrepresentations to human-interpretable acoustic concepts. We train SAEs on\naudio autoencoder latents, then learn linear mappings from SAE features to\ndiscretized acoustic properties (pitch, amplitude, and timbre). This enables\nboth controllable manipulation and analysis of the AI music generation process,\nrevealing how acoustic properties emerge during synthesis. We validate our\napproach on continuous (DiffRhythm-VAE) and discrete (EnCodec, WavTokenizer)\naudio latent spaces, and analyze DiffRhythm, a state-of-the-art text-to-music\nmodel, to demonstrate how pitch, timbre, and loudness evolve throughout\ngeneration. While our work is only done on audio modality, our framework can be\nextended to interpretable analysis of visual latent space generation models.", "AI": {"tldr": "SAEs\u53ef\u7528\u4e8e\u89e3\u91ca\u97f3\u9891\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u5c06\u6f5c\u5728\u8868\u5f81\u6620\u5c04\u5230\u58f0\u5b66\u6982\u5ff5\uff08\u97f3\u9ad8\u3001\u5e45\u5ea6\u3001\u97f3\u8272\uff09\u6765\u5b9e\u73b0\u53ef\u63a7\u64cd\u4f5c\u548c\u5206\u6790\u3002", "motivation": "SAEs\u5728\u89e3\u91ca\u8bed\u8a00\u6a21\u578b\u65b9\u9762\u5f88\u6210\u529f\uff0c\u4f46\u5728\u97f3\u9891\u751f\u6210\u65b9\u9762\u9762\u4e34\u6311\u6218\uff0c\u56e0\u4e3a\u97f3\u9891\u7684\u7a20\u5bc6\u6027\u4f1a\u9690\u85cf\u8bed\u4e49\uff0c\u4e14\u81ea\u52a8\u7279\u5f81\u8868\u5f81\u80fd\u529b\u6709\u9650\u3002", "method": "\u8bad\u7ec3SAEs\u6765\u5904\u7406\u97f3\u9891\u81ea\u7f16\u7801\u5668\u7684\u6f5c\u5728\u8868\u5f81\uff0c\u7136\u540e\u5b66\u4e60\u4eceSAEs\u7279\u5f81\u5230\u79bb\u6563\u58f0\u5b66\u5c5e\u6027\uff08\u97f3\u9ad8\u3001\u5e45\u5ea6\u3001\u97f3\u8272\uff09\u7684\u7ebf\u6027\u6620\u5c04\u3002", "result": "\u8be5\u6846\u67b6\u80fd\u591f\u5bf9AI\u97f3\u4e50\u751f\u6210\u8fc7\u7a0b\u8fdb\u884c\u53ef\u63a7\u64cd\u4f5c\u548c\u5206\u6790\uff0c\u63ed\u793a\u58f0\u5b66\u5c5e\u6027\u5982\u4f55\u5728\u5408\u6210\u8fc7\u7a0b\u4e2d\u51fa\u73b0\u3002\u5728\u8fde\u7eed\uff08DiffRhythm-VAE\uff09\u548c\u79bb\u6563\uff08EnCodec, WavTokenizer\uff09\u97f3\u9891\u6f5c\u5728\u7a7a\u95f4\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u5e76\u5206\u6790\u4e86DiffRhythm\u6a21\u578b\u4e2d\u97f3\u9ad8\u3001\u97f3\u8272\u548c\u54cd\u5ea6\u7684\u6f14\u53d8\u8fc7\u7a0b\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u591f\u89e3\u91ca\u97f3\u9891\u751f\u6210\u6a21\u578b\uff0c\u5e76\u4e14\u53ef\u4ee5\u6269\u5c55\u5230\u89c6\u89c9\u751f\u6210\u6a21\u578b\u3002"}}
{"id": "2510.24320", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24320", "abs": "https://arxiv.org/abs/2510.24320", "authors": ["Zhiheng Xi", "Jixuan Huang", "Xin Guo", "Boyang Hong", "Dingwen Yang", "Xiaoran Fan", "Shuo Li", "Zehui Chen", "Junjie Ye", "Siyu Yuan", "Zhengyin Du", "Xuesong Yao", "Yufei Xu", "Jiecao Chen", "Rui Zheng", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "title": "Critique-RL: Training Language Models for Critiquing through Two-Stage Reinforcement Learning", "comment": "Preprint, 25 pages, 9 figures. Code:\n  https://github.com/WooooDyy/Critique-RL", "summary": "Training critiquing language models to assess and provide feedback on model\noutputs is a promising way to improve LLMs for complex reasoning tasks.\nHowever, existing approaches typically rely on stronger supervisors for\nannotating critique data. To address this, we propose Critique-RL, an online RL\napproach for developing critiquing language models without stronger\nsupervision. Our approach operates on a two-player paradigm: the actor\ngenerates a response, the critic provides feedback, and the actor refines the\nresponse accordingly. We first reveal that relying solely on indirect reward\nsignals from the actor's outputs for RL optimization often leads to\nunsatisfactory critics: while their helpfulness (i.e., providing constructive\nfeedback) improves, the discriminability (i.e., determining whether a response\nis high-quality or not) remains poor, resulting in marginal performance gains.\nTo overcome this, Critique-RL adopts a two-stage optimization strategy. In\nstage I, it reinforces the discriminability of the critic with direct\nrule-based reward signals; in stage II, it introduces indirect rewards based on\nactor refinement to improve the critic's helpfulness, while maintaining its\ndiscriminability via appropriate regularization. Extensive experiments across\nvarious tasks and models show that Critique-RL delivers substantial performance\nimprovements. For example, it achieves a 9.02% gain on in-domain tasks and a\n5.70% gain on out-of-domain tasks for Qwen2.5-7B, highlighting its potential.", "AI": {"tldr": "Critique-RL\u662f\u4e00\u79cd\u65e0\u9700\u66f4\u5f3a\u76d1\u7763\u5373\u53ef\u5f00\u53d1\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u7684\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u3002\u5b83\u901a\u8fc7\u4e00\u4e2a\u4e24\u9636\u6bb5\u7684\u4f18\u5316\u7b56\u7565\u6765\u63d0\u5347\u6279\u8bc4\u8005\u6a21\u578b\u7684\u533a\u5206\u5ea6\u548c\u6709\u7528\u6027\uff0c\u5e76\u5728\u5404\u79cd\u4efb\u52a1\u548c\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u7684\u6279\u8bc4\u6027\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u66f4\u5f3a\u7684\u76d1\u7763\u6765\u6807\u6ce8\u6279\u8bc4\u6570\u636e\uff0c\u800c\u672c\u6587\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u66f4\u5f3a\u76d1\u7763\u7684\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u6765\u5f00\u53d1\u6279\u8bc4\u6027\u8bed\u8a00\u6a21\u578b\u3002", "method": "Critique-RL\u91c7\u7528\u4e00\u79cd\u4e24\u9636\u6bb5\u4f18\u5316\u7b56\u7565\uff1a\u7b2c\u4e00\u9636\u6bb5\u5229\u7528\u57fa\u4e8e\u89c4\u5219\u7684\u76f4\u63a5\u5956\u52b1\u4fe1\u53f7\u6765\u589e\u5f3a\u6279\u8bc4\u8005\u7684\u533a\u5206\u80fd\u529b\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5f15\u5165\u57fa\u4e8e\u751f\u6210\u5668\uff08actor\uff09\u6539\u8fdb\u7684\u95f4\u63a5\u5956\u52b1\u6765\u63d0\u5347\u6279\u8bc4\u8005\u7684\u6709\u7528\u6027\uff0c\u540c\u65f6\u901a\u8fc7\u6b63\u5219\u5316\u4fdd\u6301\u5176\u533a\u5206\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCritique-RL\u5728\u5404\u79cd\u4efb\u52a1\u548c\u6a21\u578b\u4e0a\u5747\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u4f8b\u5982\u5728Qwen2.5-7B\u6a21\u578b\u4e0a\uff0c\u5728\u9886\u57df\u5185\u4efb\u52a1\u4e0a\u63d0\u9ad8\u4e869.02%\uff0c\u5728\u9886\u57df\u5916\u4efb\u52a1\u4e0a\u63d0\u9ad8\u4e865.70%\u3002", "conclusion": "Critique-RL\u901a\u8fc7\u5176\u4e24\u9636\u6bb5\u4f18\u5316\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u6279\u8bc4\u6027\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u65b9\u6cd5\u5bf9\u5f3a\u76d1\u7763\u7684\u4f9d\u8d56\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u5c55\u793a\u4e86\u5176\u63d0\u5347\u6a21\u578b\u6027\u80fd\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.24410", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24410", "abs": "https://arxiv.org/abs/2510.24410", "authors": ["Toan Van Nguyen", "Rasmus G. K. Christiansen", "Dirk Kraft", "Leon Bodenhagen"], "title": "A Hybrid Approach for Visual Multi-Object Tracking", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "This paper proposes a visual multi-object tracking method that jointly\nemploys stochastic and deterministic mechanisms to ensure identifier\nconsistency for unknown and time-varying target numbers under nonlinear\ndynamics. A stochastic particle filter addresses nonlinear dynamics and\nnon-Gaussian noise, with support from particle swarm optimization (PSO) to\nguide particles toward state distribution modes and mitigate divergence through\nproposed fitness measures incorporating motion consistency, appearance\nsimilarity, and social-interaction cues with neighboring targets. Deterministic\nassociation further enforces identifier consistency via a proposed cost matrix\nincorporating spatial consistency between particles and current detections,\ndetection confidences, and track penalties. Subsequently, a novel scheme is\nproposed for the smooth updating of target states while preserving their\nidentities, particularly for weak tracks during interactions with other targets\nand prolonged occlusions. Moreover, velocity regression over past states\nprovides trend-seed velocities, enhancing particle sampling and state updates.\nThe proposed tracker is designed to operate flexibly for both pre-recorded\nvideos and camera live streams, where future frames are unavailable.\nExperimental results confirm superior performance compared to state-of-the-art\ntrackers. The source-code reference implementations of both the proposed method\nand compared-trackers are provided on GitHub:\nhttps://github.com/SDU-VelKoTek/GenTrack2", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u968f\u673a\u548c\u786e\u5b9a\u6027\u673a\u5236\u7684\u89c6\u89c9\u591a\u76ee\u6807\u8ddf\u8e2a\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u76ee\u6807\u6570\u91cf\u672a\u77e5\u548c\u65f6\u53d8\u60c5\u51b5\u4e0b\u7684ID\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u5e76\u5904\u7406\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u3002", "motivation": "\u4e3a\u89e3\u51b3\u672a\u77e5\u548c\u65f6\u53d8\u76ee\u6807\u6570\u91cf\u4e0b\u7684ID\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u5e76\u5904\u7406\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u3002", "method": "\u91c7\u7528\u968f\u673a\u7c92\u5b50\u6ee4\u6ce2\u5668\u5904\u7406\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u548c\u975e\u9ad8\u65af\u566a\u58f0\uff0c\u5e76\u7ed3\u5408\u7c92\u5b50\u7fa4\u4f18\u5316\uff08PSO\uff09\u5f15\u5bfc\u7c92\u5b50\uff0c\u540c\u65f6\u4f7f\u7528\u786e\u5b9a\u6027\u5173\u8054\u548c\u65b0\u7684\u76ee\u6807\u72b6\u6001\u66f4\u65b0\u65b9\u6848\u6765\u7ef4\u62a4ID\u4e00\u81f4\u6027\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u5f31\u8f68\u8ff9\u3002\u6b64\u5916\uff0c\u5229\u7528\u8fc7\u53bb\u72b6\u6001\u7684\u901f\u5ea6\u56de\u5f52\u6765\u589e\u5f3a\u7c92\u5b50\u91c7\u6837\u548c\u72b6\u6001\u66f4\u65b0\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u8ddf\u8e2a\u5668\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u7684\u8ddf\u8e2a\u5668\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u968f\u673a\u548c\u786e\u5b9a\u6027\u673a\u5236\uff0c\u80fd\u591f\u6709\u6548\u5730\u5904\u7406\u76ee\u6807\u6570\u91cf\u672a\u77e5\u548c\u65f6\u53d8\u7684\u60c5\u51b5\uff0c\u5e76\u4fdd\u6301ID\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u5728\u5904\u7406\u5f31\u8f68\u8ff9\u548c\u957f\u65f6\u95f4\u906e\u6321\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2510.24278", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24278", "abs": "https://arxiv.org/abs/2510.24278", "authors": ["Pietro Bongini", "Valentina Molinari", "Andrea Costanzo", "Benedetta Tondi", "Mauro Barni"], "title": "Training-free Source Attribution of AI-generated Images via Resynthesis", "comment": "14 pages, 4 figures, 1 table, accepted at \"The 17th IEEE\n  INTERNATIONAL WORKSHOP ON INFORMATION FORENSICS AND SECURITY (WIFS2025)\",\n  Perth, Australia", "summary": "Synthetic image source attribution is a challenging task, especially in data\nscarcity conditions requiring few-shot or zero-shot classification\ncapabilities. We present a new training-free one-shot attribution method based\non image resynthesis. A prompt describing the image under analysis is\ngenerated, then it is used to resynthesize the image with all the candidate\nsources. The image is attributed to the model which produced the resynthesis\nclosest to the original image in a proper feature space. We also introduce a\nnew dataset for synthetic image attribution consisting of face images from\ncommercial and open-source text-to-image generators. The dataset provides a\nchallenging attribution framework, useful for developing new attribution models\nand testing their capabilities on different generative architectures. The\ndataset structure allows to test approaches based on resynthesis and to compare\nthem to few-shot methods. Results from state-of-the-art few-shot approaches and\nother baselines show that the proposed resynthesis method outperforms existing\ntechniques when only a few samples are available for training or fine-tuning.\nThe experiments also demonstrate that the new dataset is a challenging one and\nrepresents a valuable benchmark for developing and evaluating future few-shot\nand zero-shot methods.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u56fe\u50cf\u518d\u5408\u6210\u7684\u65e0\u8bad\u7ec3\u5355\u6b21\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u6570\u636e\u7a00\u758f\u6761\u4ef6\u4e0b\u8fdb\u884c\u5408\u6210\u56fe\u50cf\u6eaf\u6e90\uff0c\u5e76\u5f15\u5165\u65b0\u6570\u636e\u96c6\u4ee5\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u5408\u6210\u56fe\u50cf\u6eaf\u6e90\u5728\u6570\u636e\u7a00\u758f\u7684\u5c11\u6837\u672c\u6216\u96f6\u6837\u672c\u5206\u7c7b\u6761\u4ef6\u4e0b\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u901a\u8fc7\u751f\u6210\u56fe\u50cf\u63cf\u8ff0\u63d0\u793a\uff0c\u5e76\u5229\u7528\u8be5\u63d0\u793a\u5728\u6240\u6709\u5019\u9009\u6a21\u578b\u4e0b\u91cd\u65b0\u5408\u6210\u56fe\u50cf\uff0c\u7136\u540e\u5728\u7279\u5f81\u7a7a\u95f4\u4e2d\u6bd4\u8f83\u518d\u5408\u6210\u56fe\u50cf\u4e0e\u539f\u59cb\u56fe\u50cf\u7684\u76f8\u4f3c\u5ea6\u6765\u786e\u5b9a\u56fe\u50cf\u6765\u6e90\u3002", "result": "\u6240\u63d0\u51fa\u7684\u518d\u5408\u6210\u65b9\u6cd5\u5728\u53ea\u6709\u5c11\u91cf\u53ef\u7528\u8bad\u7ec3\u6837\u672c\u65f6\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5e76\u4e14\u65b0\u6570\u636e\u96c6\u662f\u8bc4\u4f30\u672a\u6765\u5c11\u6837\u672c\u548c\u96f6\u6837\u672c\u65b9\u6cd5\u7684\u6709\u6548\u57fa\u51c6\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8e\u56fe\u50cf\u518d\u5408\u6210\u7684\u65e0\u8bad\u7ec3\u5355\u6b21\u5b66\u4e60\u65b9\u6cd5\u5728\u6570\u636e\u7a00\u758f\u6761\u4ef6\u4e0b\u662f\u4e00\u79cd\u6709\u6548\u7684\u5408\u6210\u56fe\u50cf\u6eaf\u6e90\u65b9\u6cd5\uff0c\u65b0\u6570\u636e\u96c6\u4e3a\u8bc4\u4f30\u6b64\u7c7b\u65b9\u6cd5\u63d0\u4f9b\u4e86\u6311\u6218\u6027\u57fa\u51c6\u3002"}}
{"id": "2510.23804", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23804", "abs": "https://arxiv.org/abs/2510.23804", "authors": ["Adela DePavia", "Vasileios Charisopoulos", "Rebecca Willett"], "title": "How do simple rotations affect the implicit bias of Adam?", "comment": null, "summary": "Adaptive gradient methods such as Adam and Adagrad are widely used in machine\nlearning, yet their effect on the generalization of learned models -- relative\nto methods like gradient descent -- remains poorly understood. Prior work on\nbinary classification suggests that Adam exhibits a ``richness bias,'' which\ncan help it learn nonlinear decision boundaries closer to the Bayes-optimal\ndecision boundary relative to gradient descent. However, the coordinate-wise\npreconditioning scheme employed by Adam renders the overall method sensitive to\northogonal transformations of feature space. We show that this sensitivity can\nmanifest as a reversal of Adam's competitive advantage: even small rotations of\nthe underlying data distribution can make Adam forfeit its richness bias and\nconverge to a linear decision boundary that is farther from the Bayes-optimal\ndecision boundary than the one learned by gradient descent. To alleviate this\nissue, we show that a recently proposed reparameterization method -- which\napplies an orthogonal transformation to the optimization objective -- endows\nany first-order method with equivariance to data rotations, and we empirically\ndemonstrate its ability to restore Adam's bias towards rich decision\nboundaries.", "AI": {"tldr": "Adam \u4f18\u5316\u5668\u5bf9\u6b63\u4ea4\u53d8\u6362\u654f\u611f\uff0c\u53ef\u80fd\u5bfc\u81f4\u5176\u5b66\u4e60\u6027\u80fd\u52a3\u4e8e\u68af\u5ea6\u4e0b\u964d\uff0c\u4f46\u901a\u8fc7\u6b63\u4ea4\u53d8\u6362\u91cd\u53c2\u6570\u5316\u53ef\u6062\u590d\u5176\u4f18\u52bf\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5bf9 Adam \u7b49\u81ea\u9002\u5e94\u68af\u5ea6\u65b9\u6cd5\u5728\u6a21\u578b\u6cdb\u5316\u65b9\u9762\u7684\u4f5c\u7528\u4e0d\u5982\u68af\u5ea6\u4e0b\u964d\u7b49\u65b9\u6cd5\u7406\u89e3\u4e0d\u8db3\uff0c\u5c3d\u7ba1 Adam \u5728\u4e8c\u5143\u5206\u7c7b\u4e2d\u8868\u73b0\u51fa\u201c\u4e30\u5bcc\u6027\u504f\u5dee\u201d\u80fd\u5b66\u4e60\u5230\u66f4\u63a5\u8fd1 Bayes \u6700\u4f18\u51b3\u7b56\u8fb9\u754c\u7684\u975e\u7ebf\u6027\u8fb9\u754c\uff0c\u4f46\u5176\u5750\u6807\u8f74\u9884\u5904\u7406\u65b9\u5f0f\u4f7f\u5176\u5bf9\u7279\u5f81\u7a7a\u95f4\u7684\u6b63\u4ea4\u53d8\u6362\u654f\u611f\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u91cd\u53c2\u6570\u5316\uff08orthogonal transformation\uff09\u7684\u65b9\u6cd5\uff0c\u4f7f\u4f18\u5316\u5668\u5177\u6709\u6570\u636e\u65cb\u8f6c\u7684\u7b49\u53d8\u6027\uff0c\u5e76\u9a8c\u8bc1\u5176\u80fd\u6062\u590d Adam \u7684\u4e30\u5bcc\u6027\u504f\u5dee\u3002", "result": "\u6570\u636e\u5206\u5e03\u7684\u5c0f\u5e45\u65cb\u8f6c\u5373\u53ef\u80fd\u5bfc\u81f4 Adam \u4e22\u5931\u4e30\u5bcc\u6027\u504f\u5dee\uff0c\u6536\u655b\u5230\u8fdc\u79bb Bayes \u6700\u4f18\u8fb9\u754c\u7684\u7ebf\u6027\u8fb9\u754c\uff0c\u800c\u68af\u5ea6\u4e0b\u964d\u7684\u8868\u73b0\u5219\u66f4\u7a33\u5b9a\u3002\u6240\u63d0\u51fa\u7684\u91cd\u53c2\u6570\u5316\u65b9\u6cd5\u80fd\u591f\u6210\u529f\u6062\u590d Adam \u7684\u4e30\u5bcc\u6027\u504f\u5dee\u3002", "conclusion": "\u901a\u8fc7\u6b63\u4ea4\u53d8\u6362\u91cd\u53c2\u6570\u5316\u53ef\u4ee5\u4f7f Adam \u7b49\u4f18\u5316\u5668\u5728\u9762\u5bf9\u6570\u636e\u65cb\u8f6c\u65f6\u4fdd\u6301\u5176\u5b66\u4e60\u4f18\u52bf\uff0c\u4ece\u800c\u63d0\u9ad8\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2510.24461", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24461", "abs": "https://arxiv.org/abs/2510.24461", "authors": ["Korneel Van den Berghe", "Stein Stroobants", "Vijay Janapa Reddi", "G. C. H. E. de Croon"], "title": "Adaptive Surrogate Gradients for Sequential Reinforcement Learning in Spiking Neural Networks", "comment": null, "summary": "Neuromorphic computing systems are set to revolutionize energy-constrained\nrobotics by achieving orders-of-magnitude efficiency gains, while enabling\nnative temporal processing. Spiking Neural Networks (SNNs) represent a\npromising algorithmic approach for these systems, yet their application to\ncomplex control tasks faces two critical challenges: (1) the non-differentiable\nnature of spiking neurons necessitates surrogate gradients with unclear\noptimization properties, and (2) the stateful dynamics of SNNs require training\non sequences, which in reinforcement learning (RL) is hindered by limited\nsequence lengths during early training, preventing the network from bridging\nits warm-up period.\n  We address these challenges by systematically analyzing surrogate gradient\nslope settings, showing that shallower slopes increase gradient magnitude in\ndeeper layers but reduce alignment with true gradients. In supervised learning,\nwe find no clear preference for fixed or scheduled slopes. The effect is much\nmore pronounced in RL settings, where shallower slopes or scheduled slopes lead\nto a 2.1x improvement in both training and final deployed performance. Next, we\npropose a novel training approach that leverages a privileged guiding policy to\nbootstrap the learning process, while still exploiting online environment\ninteractions with the spiking policy. Combining our method with an adaptive\nslope schedule for a real-world drone position control task, we achieve an\naverage return of 400 points, substantially outperforming prior techniques,\nincluding Behavioral Cloning and TD3BC, which achieve at most --200 points\nunder the same conditions. This work advances both the theoretical\nunderstanding of surrogate gradient learning in SNNs and practical training\nmethodologies for neuromorphic controllers demonstrated in real-world robotic\nsystems.", "AI": {"tldr": "SNNs\u6709\u671b\u9769\u65b0\u80fd\u6e90\u7ea6\u675f\u7684\u673a\u5668\u4eba\u6280\u672f\uff0c\u4f46\u5b58\u5728\u795e\u7ecf\u5143\u6fc0\u6d3b\u548c\u8bad\u7ec3\u5e8f\u5217\u957f\u5ea6\u7684\u6311\u6218\u3002\u672c\u6587\u5206\u6790\u4e86\u66ff\u4ee3\u68af\u5ea6\u659c\u7387\u5bf9SNN\u4f18\u5316\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7279\u6743\u5f15\u5bfc\u7b56\u7565\u548c\u81ea\u9002\u5e94\u659c\u7387\u8c03\u5ea6\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5728\u65e0\u4eba\u673a\u5b9a\u4f4d\u63a7\u5236\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u89e3\u51b3\u5c16\u5cf0\u795e\u7ecf\u7f51\u7edc\uff08SNNs\uff09\u5728\u80fd\u91cf\u53d7\u9650\u7684\u673a\u5668\u4eba\u9886\u57df\u5e94\u7528\u4e2d\u9762\u4e34\u7684\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a\u975e\u53ef\u5fae\u5206\u795e\u7ecf\u5143\u6fc0\u6d3b\u7684\u4f18\u5316\u95ee\u9898\u548cSNNs\u72b6\u6001\u52a8\u529b\u5b66\u6240\u9700\u7684\u5e8f\u5217\u8bad\u7ec3\u53d7\u9650\u4e8e\u65e9\u671f\u8bad\u7ec3\u4e2d\u7684\u5e8f\u5217\u957f\u5ea6\u95ee\u9898\u3002", "method": "\uff081\uff09\u901a\u8fc7\u7cfb\u7edf\u5206\u6790\u66ff\u4ee3\u68af\u5ea6\u659c\u7387\u8bbe\u7f6e\uff0c\u7814\u7a76\u5176\u5bf9\u68af\u5ea6\u5e45\u5ea6\u548c\u771f\u5b9e\u68af\u5ea6\u7684\u5bf9\u9f50\u5ea6\u5f71\u54cd\uff1b\uff082\uff09\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5229\u7528\u7279\u6743\u5f15\u5bfc\u7b56\u7565\u5f15\u5bfc\u5b66\u4e60\u8fc7\u7a0b\uff0c\u5e76\u7ed3\u5408\u81ea\u9002\u5e94\u659c\u7387\u8c03\u5ea6\u3002", "result": "\uff081\uff09\u5728\u76d1\u7763\u5b66\u4e60\u4e2d\uff0c\u56fa\u5b9a\u548c\u8c03\u5ea6\u659c\u7387\u6ca1\u6709\u660e\u663e\u504f\u597d\uff1b\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u8f83\u6d45\u7684\u659c\u7387\u6216\u8c03\u5ea6\u7684\u659c\u7387\u53ef\u5c06\u8bad\u7ec3\u548c\u90e8\u7f72\u6027\u80fd\u63d0\u9ad82.1\u500d\u3002\uff082\uff09\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u65e0\u4eba\u673a\u5b9a\u4f4d\u63a7\u5236\u4efb\u52a1\u4e2d\uff0c\u5b9e\u73b0\u4e86\u5e73\u5747400\u70b9\u7684\u56de\u62a5\uff0c\u663e\u8457\u4f18\u4e8e\u884c\u4e3a\u514b\u9686\u548cTD3BC\u7b49\u5148\u524d\u6280\u672f\uff08-200\u70b9\uff09\u3002", "conclusion": "\u672c\u6587\u5728\u7406\u8bba\u4e0a\u52a0\u6df1\u4e86\u5bf9SNNs\u4e2d\u66ff\u4ee3\u68af\u5ea6\u5b66\u4e60\u7684\u7406\u89e3\uff0c\u5e76\u5728\u5b9e\u8df5\u4e2d\u901a\u8fc7\u771f\u5b9e\u673a\u5668\u4eba\u7cfb\u7edf\u9a8c\u8bc1\u4e86\u795e\u7ecf\u5f62\u6001\u63a7\u5236\u5668\u8bad\u7ec3\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.24328", "categories": ["cs.CL", "cs.AI", "68T50", "F.2.2; I.2.7"], "pdf": "https://arxiv.org/pdf/2510.24328", "abs": "https://arxiv.org/abs/2510.24328", "authors": ["Hunzalah Hassan Bhatti", "Firoj Alam"], "title": "Beyond MCQ: An Open-Ended Arabic Cultural QA Benchmark with Dialect Variants", "comment": "Cultural Knowledge, Everyday Knowledge, Open-Ended Question,\n  Chain-of-Thought, Large Language Models, Native, Multilingual, Language\n  Diversity", "summary": "Large Language Models (LLMs) are increasingly used to answer everyday\nquestions, yet their performance on culturally grounded and dialectal content\nremains uneven across languages. We propose a comprehensive method that (i)\ntranslates Modern Standard Arabic (MSA) multiple-choice questions (MCQs) into\nEnglish and several Arabic dialects, (ii) converts them into open-ended\nquestions (OEQs), (iii) benchmarks a range of zero-shot and fine-tuned LLMs\nunder both MCQ and OEQ settings, and (iv) generates chain-of-thought (CoT)\nrationales to fine-tune models for step-by-step reasoning. Using this method,\nwe extend an existing dataset in which QAs are parallelly aligned across\nmultiple language varieties, making it, to our knowledge, the first of its\nkind. We conduct extensive experiments with both open and closed models. Our\nfindings show that (i) models underperform on Arabic dialects, revealing\npersistent gaps in culturally grounded and dialect-specific knowledge; (ii)\nArabic-centric models perform well on MCQs but struggle with OEQs; and (iii)\nCoT improves judged correctness while yielding mixed n-gram-based metrics. The\ndeveloped dataset will be publicly released to support further research on\nculturally and linguistically inclusive evaluation.", "AI": {"tldr": "LLMs\u5728\u5904\u7406\u963f\u62c9\u4f2f\u8bed\u65b9\u8a00\u548c\u6587\u5316\u76f8\u5173\u5185\u5bb9\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b\u7ffb\u8bd1\u3001\u683c\u5f0f\u8f6c\u6362\u3001\u6a21\u578b\u8bc4\u6d4b\u548c\u601d\u7ef4\u94fe\u5fae\u8c03\u7684\u7efc\u5408\u65b9\u6cd5\uff0c\u5e76\u53d1\u5e03\u4e86\u9996\u4e2a\u8de8\u8bed\u8a00\u7684\u963f\u62c9\u4f2f\u8bed\u95ee\u7b54\u6570\u636e\u96c6\uff0c\u65e8\u5728\u63d0\u5347LLMs\u7684\u5305\u5bb9\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5904\u7406\u963f\u62c9\u4f2f\u8bed\u65b9\u8a00\u548c\u6587\u5316\u76f8\u5173\u5185\u5bb9\u65f6\u8868\u73b0\u4e0d\u5747\u8861\uff0c\u9700\u8981\u6539\u8fdb\u3002", "method": "1. \u5c06\u73b0\u4ee3\u6807\u51c6\u963f\u62c9\u4f2f\u8bed\uff08MSA\uff09\u7684\u591a\u9879\u9009\u62e9\u9898\uff08MCQs\uff09\u7ffb\u8bd1\u6210\u82f1\u8bed\u548c\u591a\u79cd\u963f\u62c9\u4f2f\u8bed\u65b9\u8a00\u30022. \u5c06MCQs\u8f6c\u6362\u4e3a\u5f00\u653e\u5f0f\u95ee\u9898\uff08OEQs\uff09\u30023. \u8bc4\u6d4b\u4e00\u7cfb\u5217\u96f6\u6837\u672c\u548c\u5fae\u8c03LLMs\u5728MCQ\u548cOEQ\u573a\u666f\u4e0b\u7684\u8868\u73b0\u30024. \u751f\u6210\u601d\u7ef4\u94fe\uff08CoT\uff09\u63a8\u7406\u8fc7\u7a0b\u4ee5\u8fdb\u884c\u9010\u6b65\u63a8\u7406\u5fae\u8c03\u30025. \u6784\u5efa\u4e86\u4e00\u4e2a\u8de8\u591a\u79cd\u8bed\u8a00\u7684\u963f\u62c9\u4f2f\u8bed\u95ee\u7b54\u6570\u636e\u96c6\u3002", "result": "1. LLMs\u5728\u963f\u62c9\u4f2f\u8bed\u65b9\u8a00\u4e0a\u7684\u8868\u73b0\u4e0d\u5982\u73b0\u4ee3\u6807\u51c6\u963f\u62c9\u4f2f\u8bed\uff0c\u8868\u660e\u5728\u6587\u5316\u548c\u65b9\u8a00\u7279\u5b9a\u77e5\u8bc6\u65b9\u9762\u5b58\u5728\u5dee\u8ddd\u30022. \u4ee5\u963f\u62c9\u4f2f\u8bed\u4e3a\u4e2d\u5fc3\u7684\u6a21\u578b\u5728MCQs\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728OEQs\u4e0a\u9047\u5230\u56f0\u96be\u30023. CoT\u65b9\u6cd5\u63d0\u9ad8\u4e86\u5224\u65ad\u7684\u6b63\u786e\u6027\uff0c\u4f46\u5bf9n-gram\u6307\u6807\u7684\u5f71\u54cd\u597d\u574f\u53c2\u534a\u3002", "conclusion": "LLMs\u5728\u5904\u7406\u963f\u62c9\u4f2f\u8bed\u65b9\u8a00\u548c\u6587\u5316\u76f8\u5173\u5185\u5bb9\u65b9\u9762\u4ecd\u6709\u63d0\u5347\u7a7a\u95f4\uff0c\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u548c\u6570\u636e\u96c6\u6709\u52a9\u4e8e\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\uff0c\u4fc3\u8fdb\u66f4\u5177\u5305\u5bb9\u6027\u7684\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u3002"}}
{"id": "2510.24285", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24285", "abs": "https://arxiv.org/abs/2510.24285", "authors": ["Juntian Zhang", "Song Jin", "Chuanqi Cheng", "Yuhan Liu", "Yankai Lin", "Xun Zhang", "Yufei Zhang", "Fei Jiang", "Guojun Yin", "Wei Lin", "Rui Yan"], "title": "ViPER: Empowering the Self-Evolution of Visual Perception Abilities in Vision-Language Model", "comment": null, "summary": "The limited capacity for fine-grained visual perception presents a critical\nbottleneck for Vision-Language Models (VLMs) in real-world applications.\nAddressing this is challenging due to the scarcity of high-quality data and the\nlimitations of existing methods: supervised fine-tuning (SFT) often compromises\ngeneral capabilities, while reinforcement fine-tuning (RFT) prioritizes textual\nreasoning over visual perception. To bridge this gap, we propose a novel\ntwo-stage task that structures visual perception learning as a coarse-to-fine\nprogressive process. Based on this task formulation, we develop ViPER, a\nself-bootstrapping framework specifically designed to enable iterative\nevolution through self-critiquing and self-prediction. By synergistically\nintegrating image-level and instance-level reconstruction with a two-stage\nreinforcement learning strategy, ViPER establishes a closed-loop training\nparadigm, where internally synthesized data directly fuel the enhancement of\nperceptual ability. Applied to the Qwen2.5-VL family, ViPER produces the\nQwen-Viper series. With an average gain of 1.7% on seven comprehensive\nbenchmarks spanning various tasks and up to 6.0% on fine-grained perception,\nQwen-Viper consistently demonstrates superior performance across different\nvision-language scenarios while maintaining generalizability. Beyond enabling\nself-improvement in perceptual capabilities, ViPER provides concrete evidence\nfor the reciprocal relationship between generation and understanding, a\nbreakthrough to developing more autonomous and capable VLMs.", "AI": {"tldr": "ViPER\u662f\u4e00\u4e2a\u521b\u65b0\u7684\u81ea\u4e3e\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u6211\u6279\u8bc4\u548c\u81ea\u6211\u9884\u6d4b\u6765\u589e\u5f3a\u89c6\u89c9\u611f\u77e5\u80fd\u529b\uff0c\u89e3\u51b3\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b(VLMs)\u5728\u7ec6\u7c92\u5ea6\u89c6\u89c9\u611f\u77e5\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u7ec6\u7c92\u5ea6\u89c6\u89c9\u611f\u77e5\u65f6\u5b58\u5728\u6570\u636e\u7a00\u7f3a\u3001\u76d1\u7763\u5fae\u8c03(SFT)\u635f\u5bb3\u901a\u7528\u80fd\u529b\u3001\u5f3a\u5316\u5fae\u8c03(RFT)\u504f\u91cd\u6587\u672c\u63a8\u7406\u7b49\u95ee\u9898\uff0c\u9650\u5236\u4e86VLMs\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4e24\u9636\u6bb5\u4efb\u52a1\uff0c\u5c06\u89c6\u89c9\u611f\u77e5\u5b66\u4e60\u6784\u5efa\u4e3a\u4ece\u7c97\u5230\u7cbe\u7684\u6e10\u8fdb\u8fc7\u7a0b\uff0c\u5e76\u5f00\u53d1\u4e86ViPER\u81ea\u4e3e\u6846\u67b6\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u6574\u5408\u56fe\u50cf\u7ea7\u548c\u5b9e\u4f8b\u7ea7\u91cd\u5efa\u4ee5\u53ca\u4e24\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u95ed\u73af\u8bad\u7ec3\uff0c\u5229\u7528\u5185\u90e8\u5408\u6210\u6570\u636e\u589e\u5f3a\u611f\u77e5\u80fd\u529b\u3002", "result": "\u5c06ViPER\u5e94\u7528\u4e8eQwen2.5-VL\u7cfb\u5217\uff0c\u4ea7\u751f\u4e86Qwen-Viper\u7cfb\u5217\u3002\u5728\u4e03\u4e2a\u7efc\u5408\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5e73\u5747\u63d0\u53471.7%\uff0c\u5728\u7ec6\u7c92\u5ea6\u611f\u77e5\u65b9\u9762\u6700\u9ad8\u63d0\u53476.0%\uff0c\u5728\u5404\u79cd\u89c6\u89c9-\u8bed\u8a00\u573a\u666f\u4e2d\u5747\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u901a\u7528\u6027\u3002", "conclusion": "ViPER\u6210\u529f\u5b9e\u73b0\u4e86\u5bf9\u611f\u77e5\u80fd\u529b\u7684\u81ea\u6211\u63d0\u5347\uff0c\u5e76\u4e3a\u751f\u6210\u548c\u7406\u89e3\u4e4b\u95f4\u7684\u76f8\u4e92\u5173\u7cfb\u63d0\u4f9b\u4e86\u8bc1\u636e\uff0c\u4e3a\u5f00\u53d1\u66f4\u81ea\u4e3b\u3001\u66f4\u5f3a\u5927\u7684VLMs\u5f00\u8f9f\u4e86\u9053\u8def\u3002"}}
{"id": "2510.23810", "categories": ["cs.LG", "math.AP", "physics.comp-ph", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.23810", "abs": "https://arxiv.org/abs/2510.23810", "authors": ["Sumanta Roy", "Bahador Bahmani", "Ioannis G. Kevrekidis", "Michael D. Shields"], "title": "A Physics-informed Multi-resolution Neural Operator", "comment": "26 pages, 14 figures, 4 tables", "summary": "The predictive accuracy of operator learning frameworks depends on the\nquality and quantity of available training data (input-output function pairs),\noften requiring substantial amounts of high-fidelity data, which can be\nchallenging to obtain in some real-world engineering applications. These\ndatasets may be unevenly discretized from one realization to another, with the\ngrid resolution varying across samples. In this study, we introduce a\nphysics-informed operator learning approach by extending the Resolution\nIndependent Neural Operator (RINO) framework to a fully data-free setup,\naddressing both challenges simultaneously. Here, the arbitrarily (but\nsufficiently finely) discretized input functions are projected onto a latent\nembedding space (i.e., a vector space of finite dimensions), using pre-trained\nbasis functions. The operator associated with the underlying partial\ndifferential equations (PDEs) is then approximated by a simple multi-layer\nperceptron (MLP), which takes as input a latent code along with spatiotemporal\ncoordinates to produce the solution in the physical space. The PDEs are\nenforced via a finite difference solver in the physical space. The validation\nand performance of the proposed method are benchmarked on several numerical\nexamples with multi-resolution data, where input functions are sampled at\nvarying resolutions, including both coarse and fine discretizations.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u6570\u636e\u5373\u53ef\u8bad\u7ec3\u7684\u7269\u7406\u4fe1\u606f\u7b97\u5b50\u5b66\u4e60\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5bf9\u6570\u636e\u8d28\u91cf\u548c\u6570\u91cf\u7684\u8981\u6c42\uff0c\u4ee5\u53ca\u6570\u636e\u79bb\u6563\u5316\u4e0d\u5747\u5300\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u7b97\u5b50\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\uff0c\u4e14\u6613\u53d7\u6570\u636e\u79bb\u6563\u5316\u4e0d\u5747\u5300\u7684\u9650\u5236\uff0c\u8fd9\u5728\u5b9e\u9645\u5de5\u7a0b\u5e94\u7528\u4e2d\u96be\u4ee5\u6ee1\u8db3\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u6570\u636e\u5373\u53ef\u8bad\u7ec3\u7684\u7269\u7406\u4fe1\u606f\u7b97\u5b50\u5b66\u4e60\u65b9\u6cd5\uff0c\u6269\u5c55\u4e86RINO\u6846\u67b6\u3002\u8be5\u65b9\u6cd5\u5c06\u4efb\u610f\u79bb\u6563\u5316\u7684\u8f93\u5165\u51fd\u6570\u6295\u5f71\u5230\u6f5c\u5728\u5d4c\u5165\u7a7a\u95f4\uff0c\u7136\u540e\u4f7f\u7528\u591a\u5c42\u611f\u77e5\u673a\uff08MLP\uff09\u903c\u8fd1\u7b97\u5b50\uff0c\u5e76\u901a\u8fc7\u6709\u9650\u5dee\u5206\u6c42\u89e3\u5668\u5728\u7269\u7406\u7a7a\u95f4\u4e2d\u5f3a\u5236\u6267\u884c\u504f\u5fae\u5206\u65b9\u7a0b\uff08PDE\uff09\u3002", "result": "\u5728\u5177\u6709\u591a\u5206\u8fa8\u7387\u6570\u636e\u7684\u6570\u503c\u793a\u4f8b\u4e0a\u5bf9\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u8fdb\u884c\u4e86\u9a8c\u8bc1\u548c\u6027\u80fd\u8bc4\u4f30\uff0c\u5176\u4e2d\u8f93\u5165\u51fd\u6570\u5728\u4e0d\u540c\u5206\u8fa8\u7387\u4e0b\u8fdb\u884c\u91c7\u6837\uff0c\u5305\u62ec\u7c97\u7565\u548c\u7cbe\u7ec6\u79bb\u6563\u5316\u3002", "conclusion": "\u8be5\u6570\u636e\u81ea\u7531\u7684\u7269\u7406\u4fe1\u606f\u7b97\u5b50\u5b66\u4e60\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u591a\u5206\u8fa8\u7387\u6570\u636e\uff0c\u5e76\u51c6\u786e\u5730\u903c\u8fd1\u504f\u5fae\u5206\u65b9\u7a0b\u7684\u89e3\u3002"}}
{"id": "2510.24528", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24528", "abs": "https://arxiv.org/abs/2510.24528", "authors": ["Zihan Chen", "Song Wang", "Xingbo Fu", "Chengshuai Shi", "Zhenyu Lei", "Cong Shen", "Jundong Li"], "title": "From Cross-Task Examples to In-Task Prompts: A Graph-Based Pseudo-Labeling Framework for In-context Learning", "comment": null, "summary": "The capability of in-context learning (ICL) enables large language models\n(LLMs) to perform novel tasks without parameter updates by conditioning on a\nfew input-output examples. However, collecting high-quality examples for new or\nchallenging tasks can be costly and labor-intensive. In this work, we propose a\ncost-efficient two-stage pipeline that reduces reliance on LLMs for data\nlabeling. Our approach first leverages readily available cross-task examples to\nprompt an LLM and pseudo-label a small set of target task instances. We then\nintroduce a graph-based label propagation method that spreads label information\nto the remaining target examples without additional LLM queries. The resulting\nfully pseudo-labeled dataset is used to construct in-task demonstrations for\nICL. This pipeline combines the flexibility of cross-task supervision with the\nscalability of LLM-free propagation. Experiments across five tasks demonstrate\nthat our method achieves strong performance while lowering labeling costs.", "AI": {"tldr": "\u901a\u8fc7\u5f15\u5165\u57fa\u4e8e\u56fe\u7684\u6807\u7b7e\u4f20\u64ad\u65b9\u6cd5\uff0c\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4f4e\u6210\u672c\u7684\u4e24\u9636\u6bb5\u6d41\u6c34\u7ebf\uff0c\u7528\u4e8e\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u4f2a\u6807\u7b7e\u6570\u636e\uff0c\u4ee5\u8fdb\u884c\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\uff0c\u4ece\u800c\u964d\u4f4e\u4e86\u5bf9LLM\u8fdb\u884c\u6570\u636e\u6807\u6ce8\u7684\u4f9d\u8d56\u3002", "motivation": "\u5728\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u4e2d\uff0c\u867d\u7136\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u53ef\u4ee5\u901a\u8fc7\u5c11\u91cf\u793a\u4f8b\u6765\u6267\u884c\u65b0\u4efb\u52a1\uff0c\u4f46\u6536\u96c6\u9ad8\u8d28\u91cf\u7684\u793a\u4f8b\u6210\u672c\u9ad8\u6602\u4e14\u8017\u65f6\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u5177\u6210\u672c\u6548\u76ca\u7684\u65b9\u6cd5\u6765\u751f\u6210\u7528\u4e8eICL\u7684\u6570\u636e\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u7684\u6d41\u6c34\u7ebf\u3002\u7b2c\u4e00\u9636\u6bb5\uff0c\u5229\u7528\u73b0\u6709\u7684\u8de8\u4efb\u52a1\u793a\u4f8b\u63d0\u793aLLM\uff0c\u4e3a\u76ee\u6807\u4efb\u52a1\u7684\u5c11\u91cf\u5b9e\u4f8b\u751f\u6210\u4f2a\u6807\u7b7e\u3002\u7b2c\u4e8c\u9636\u6bb5\uff0c\u5f15\u5165\u4e00\u79cd\u57fa\u4e8e\u56fe\u7684\u6807\u7b7e\u4f20\u64ad\u65b9\u6cd5\uff0c\u5728\u4e0d\u8fdb\u884c\u989d\u5916LLM\u67e5\u8be2\u7684\u60c5\u51b5\u4e0b\uff0c\u5c06\u6807\u7b7e\u4fe1\u606f\u6269\u6563\u5230\u5269\u4f59\u7684\u76ee\u6807\u5b9e\u4f8b\u4e2d\u3002\u6700\u7ec8\u751f\u6210\u7684\u4f2a\u6807\u7b7e\u6570\u636e\u96c6\u7528\u4e8e\u6784\u5efaICL\u7684\u6f14\u793a\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u4e94\u4e2a\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5b83\u80fd\u591f\u4ee5\u8f83\u4f4e\u7684\u6570\u636e\u6807\u6ce8\u6210\u672c\u5b9e\u73b0\u5f3a\u5927\u7684\u6027\u80fd\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6d41\u6c34\u7ebf\u7ed3\u5408\u4e86\u8de8\u4efb\u52a1\u76d1\u7763\u7684\u7075\u6d3b\u6027\u548c\u65e0LLM\u4f20\u64ad\u7684\u53ef\u6269\u5c55\u6027\uff0c\u6709\u6548\u5730\u964d\u4f4e\u4e86ICL\u7684\u6570\u636e\u6807\u6ce8\u6210\u672c\uff0c\u5e76\u80fd\u8fbe\u5230\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002"}}
{"id": "2510.24345", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24345", "abs": "https://arxiv.org/abs/2510.24345", "authors": ["Zikai Xiao", "Fei Huang", "Jianhong Tu", "Jianhui Wei", "Wen Ma", "Yuxuan Zhou", "Jian Wu", "Bowen Yu", "Zuozhu Liu", "Junyang Lin"], "title": "LongWeave: A Long-Form Generation Benchmark Bridging Real-World Relevance and Verifiability", "comment": "EMNLP Findings 2025", "summary": "Generating long, informative, and factual outputs remains a major challenge\nfor Large Language Models (LLMs). Existing benchmarks for long-form generation\ntypically assess real-world queries with hard-to-verify metrics or use\nsynthetic setups that ease evaluation but overlook real-world intricacies. In\nthis paper, we introduce \\textbf{LongWeave}, which balances real-world and\nverifiable assessment with Constraint-Verifier Evaluation (CoV-Eval). CoV-Eval\nconstructs tasks by first defining verifiable targets within real-world\nscenarios, then systematically generating corresponding queries, textual\nmaterials, and constraints based on these targets. This ensures that tasks are\nboth realistic and objectively assessable, enabling rigorous assessment of\nmodel capabilities in meeting complex real-world constraints. LongWeave\nsupports customizable input/output lengths (up to 64K/8K tokens) across seven\ndistinct tasks. Evaluation on 23 LLMs shows that even state-of-the-art models\nencounter significant challenges in long-form generation as real-world\ncomplexity and output length increase.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86LongWeave\u6846\u67b6\u548cCoV-Eval\u8bc4\u4f30\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u957f\u6587\u672c\u751f\u6210\u65b9\u9762\u7684\u6311\u6218\uff0c\u5e76\u901a\u8fc7\u53ef\u9a8c\u8bc1\u7684\u6307\u6807\u6765\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u957f\u6587\u672c\u751f\u6210\u8bc4\u4f30\u65b9\u6cd5\u8981\u4e48\u96be\u4ee5\u9a8c\u8bc1\uff0c\u8981\u4e48\u8fc7\u4e8e\u7b80\u5316\uff0c\u5ffd\u7565\u4e86\u73b0\u5b9e\u4e16\u754c\u7684\u590d\u6742\u6027\u3002", "method": "\u63d0\u51faCoV-Eval\u8bc4\u4f30\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bbe\u5b9a\u53ef\u9a8c\u8bc1\u7684\u76ee\u6807\uff0c\u5e76\u57fa\u4e8e\u8fd9\u4e9b\u76ee\u6807\u751f\u6210\u5bf9\u5e94\u7684\u67e5\u8be2\u3001\u6587\u672c\u6750\u6599\u548c\u7ea6\u675f\u6761\u4ef6\uff0c\u4ee5\u5b9e\u73b0\u5bf9\u6a21\u578b\u957f\u6587\u672c\u751f\u6210\u80fd\u529b\u7684\u73b0\u5b9e\u4e14\u5ba2\u89c2\u7684\u8bc4\u4f30\u3002LongWeave\u6846\u67b6\u652f\u6301\u81ea\u5b9a\u4e49\u8f93\u5165\u8f93\u51fa\u957f\u5ea6\uff0c\u5e76\u5305\u542b\u4e03\u79cd\u4e0d\u540c\u7684\u4efb\u52a1\u3002", "result": "\u572823\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0a\u7684\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0c\u968f\u7740\u73b0\u5b9e\u4e16\u754c\u590d\u6742\u6027\u548c\u8f93\u51fa\u957f\u5ea6\u7684\u589e\u52a0\uff0c\u5373\u4f7f\u662f\u5148\u8fdb\u7684\u6a21\u578b\u5728\u957f\u6587\u672c\u751f\u6210\u65b9\u9762\u4e5f\u9762\u4e34\u663e\u8457\u6311\u6218\u3002", "conclusion": "\u957f\u6587\u672c\u751f\u6210\u4ecd\u7136\u662f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9762\u4e34\u7684\u4e00\u5927\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u73b0\u5b9e\u4e16\u754c\u7684\u590d\u6742\u7ea6\u675f\u548c\u66f4\u957f\u7684\u8f93\u51fa\u65f6\u3002"}}
{"id": "2510.24482", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24482", "abs": "https://arxiv.org/abs/2510.24482", "authors": ["Klemens Iten", "Lenart Treven", "Bhavya Sukhija", "Florian D\u00f6rfler", "Andreas Krause"], "title": "Sample-efficient and Scalable Exploration in Continuous-Time RL", "comment": "26 pages, 6 figures, 6 tables", "summary": "Reinforcement learning algorithms are typically designed for discrete-time\ndynamics, even though the underlying real-world control systems are often\ncontinuous in time. In this paper, we study the problem of continuous-time\nreinforcement learning, where the unknown system dynamics are represented using\nnonlinear ordinary differential equations (ODEs). We leverage probabilistic\nmodels, such as Gaussian processes and Bayesian neural networks, to learn an\nuncertainty-aware model of the underlying ODE. Our algorithm, COMBRL, greedily\nmaximizes a weighted sum of the extrinsic reward and model epistemic\nuncertainty. This yields a scalable and sample-efficient approach to\ncontinuous-time model-based RL. We show that COMBRL achieves sublinear regret\nin the reward-driven setting, and in the unsupervised RL setting (i.e., without\nextrinsic rewards), we provide a sample complexity bound. In our experiments,\nwe evaluate COMBRL in both standard and unsupervised RL settings and\ndemonstrate that it scales better, is more sample-efficient than prior methods,\nand outperforms baselines across several deep RL tasks.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCOMBRL\u7684\u8fde\u7eed\u65f6\u95f4\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u5229\u7528\u6982\u7387\u6a21\u578b\u5b66\u4e60\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684ODE\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u6700\u5927\u5316\u5956\u52b1\u548c\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u7684\u52a0\u6743\u548c\u6765\u5b9e\u73b0\u9ad8\u6548\u5b66\u4e60\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u591a\u9879\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u63a7\u5236\u7cfb\u7edf\u901a\u5e38\u662f\u8fde\u7eed\u65f6\u95f4\u7cfb\u7edf\uff0c\u4f46\u73b0\u6709\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u591a\u662f\u4e3a\u79bb\u6563\u65f6\u95f4\u52a8\u6001\u8bbe\u8ba1\u7684\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76\u8fde\u7eed\u65f6\u95f4\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\u3002", "method": "\u5229\u7528\u9ad8\u65af\u8fc7\u7a0b\u548c\u8d1d\u53f6\u65af\u795e\u7ecf\u7f51\u7edc\u7b49\u6982\u7387\u6a21\u578b\u5b66\u4e60\u672a\u77e5\u7cfb\u7edf\u52a8\u529b\u5b66\uff08\u975e\u7ebf\u6027\u5e38\u5fae\u5206\u65b9\u7a0b\uff09\uff0c\u5e76\u63d0\u51faCOMBRL\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u901a\u8fc7\u8d2a\u5a6a\u5730\u6700\u5927\u5316\u5916\u5728\u5956\u52b1\u548c\u6a21\u578b\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u7684\u52a0\u6743\u548c\u6765\u8fdb\u884c\u5b66\u4e60\u3002", "result": "COMBRL\u7b97\u6cd5\u5728\u5956\u52b1\u9a71\u52a8\u8bbe\u7f6e\u4e0b\u5b9e\u73b0\u4e86\u6b21\u7ebf\u6027\u9057\u61be\uff0c\u5728\u65e0\u5916\u5728\u5956\u52b1\u7684\u65e0\u76d1\u7763\u5f3a\u5316\u5b66\u4e60\u8bbe\u7f6e\u4e0b\u63d0\u4f9b\u4e86\u6837\u672c\u590d\u6742\u5ea6\u754c\u9650\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cCOMBRL\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u5177\u53ef\u6269\u5c55\u6027\u548c\u6837\u672c\u6548\u7387\uff0c\u5e76\u5728\u591a\u4e2a\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "COMBRL\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u6837\u672c\u91cf\u9700\u6c42\u4f4e\u7684\u8fde\u7eed\u65f6\u95f4\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u80fd\u591f\u5904\u7406\u5177\u6709\u672a\u77e5\u975e\u7ebf\u6027\u5e38\u5fae\u5206\u65b9\u7a0b\u52a8\u529b\u5b66\u7684\u7cfb\u7edf\uff0c\u5e76\u5728\u591a\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002"}}
{"id": "2510.24321", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24321", "abs": "https://arxiv.org/abs/2510.24321", "authors": ["Ivica Dimitrovski", "Vlatko Spasev", "Ivan Kitanovski"], "title": "Few-Shot Remote Sensing Image Scene Classification with CLIP and Prompt Learning", "comment": null, "summary": "Remote sensing applications increasingly rely on deep learning for scene\nclassification. However, their performance is often constrained by the scarcity\nof labeled data and the high cost of annotation across diverse geographic and\nsensor domains. While recent vision-language models like CLIP have shown\npromise by learning transferable representations at scale by aligning visual\nand textual modalities, their direct application to remote sensing remains\nsuboptimal due to significant domain gaps and the need for task-specific\nsemantic adaptation. To address this critical challenge, we systematically\nexplore prompt learning as a lightweight and efficient adaptation strategy for\nfew-shot remote sensing image scene classification. We evaluate several\nrepresentative methods, including Context Optimization, Conditional Context\nOptimization, Multi-modal Prompt Learning, and Prompting with Self-Regulating\nConstraints. These approaches reflect complementary design philosophies: from\nstatic context optimization to conditional prompts for enhanced generalization,\nmulti-modal prompts for joint vision-language adaptation, and semantically\nregularized prompts for stable learning without forgetting. We benchmark these\nprompt-learning methods against two standard baselines: zero-shot CLIP with\nhand-crafted prompts and a linear probe trained on frozen CLIP features.\nThrough extensive experiments on multiple benchmark remote sensing datasets,\nincluding cross-dataset generalization tests, we demonstrate that prompt\nlearning consistently outperforms both baselines in few-shot scenarios.\nNotably, Prompting with Self-Regulating Constraints achieves the most robust\ncross-domain performance. Our findings underscore prompt learning as a scalable\nand efficient solution for bridging the domain gap in satellite and aerial\nimagery, providing a strong foundation for future research in this field.", "AI": {"tldr": "\u63d0\u793a\u5b66\u4e60\u662f\u89e3\u51b3\u9065\u611f\u9886\u57df\u6570\u636e\u7a00\u758f\u6027\u548c\u9ad8\u6602\u6807\u6ce8\u6210\u672c\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u80fd\u591f\u63d0\u5347\u6a21\u578b\u5728\u5c11\u6837\u672c\u573a\u666f\u4e0b\u7684\u9065\u611f\u56fe\u50cf\u573a\u666f\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u9065\u611f\u56fe\u50cf\u573a\u666f\u5206\u7c7b\u5728\u5c11\u6837\u672c\u548c\u8de8\u57df\u573a\u666f\u4e0b\u9762\u4e34\u6570\u636e\u7a00\u758f\u3001\u6807\u6ce8\u6210\u672c\u9ad8\u4ee5\u53ca\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u6311\u6218\u3002", "method": "\u672c\u6587\u7cfb\u7edf\u5730\u63a2\u7d22\u4e86\u63d0\u793a\u5b66\u4e60\uff08Prompt Learning\uff09\u4f5c\u4e3a\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u9ad8\u6548\u7684\u8fc1\u79fb\u5b66\u4e60\u7b56\u7565\uff0c\u7528\u4e8e\u5c11\u6837\u672c\u9065\u611f\u56fe\u50cf\u573a\u666f\u5206\u7c7b\u3002\u8bc4\u4f30\u4e86\u5305\u62ec\u4e0a\u4e0b\u6587\u4f18\u5316\uff08Context Optimization\uff09\u3001\u6761\u4ef6\u4e0a\u4e0b\u6587\u4f18\u5316\uff08Conditional Context Optimization\uff09\u3001\u591a\u6a21\u6001\u63d0\u793a\u5b66\u4e60\uff08Multi-modal Prompt Learning\uff09\u4ee5\u53ca\u5e26\u81ea\u8c03\u8282\u7ea6\u675f\u7684\u63d0\u793a\uff08Prompting with Self-Regulating Constraints\uff09\u7b49\u591a\u79cd\u4ee3\u8868\u6027\u65b9\u6cd5\uff0c\u5e76\u4e0e\u96f6\u6837\u672cCLIP\uff08Zero-shot CLIP\uff09\u548c\u7ebf\u6027\u63a2\u9488\uff08Linear Probe\uff09\u7b49\u57fa\u7ebf\u65b9\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "result": "\u901a\u8fc7\u5728\u591a\u4e2a\u9065\u611f\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u5305\u62ec\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u6d4b\u8bd5\uff0c\u8bc1\u660e\u4e86\u63d0\u793a\u5b66\u4e60\u5728\u5c11\u6837\u672c\u573a\u666f\u4e0b\u4f18\u4e8e\u4e24\u79cd\u57fa\u7ebf\u65b9\u6cd5\u3002\u5176\u4e2d\uff0c\u5e26\u81ea\u8c03\u8282\u7ea6\u675f\u7684\u63d0\u793a\u5728\u8de8\u57df\u6027\u80fd\u4e0a\u8868\u73b0\u6700\u4e3a\u7a33\u5065\u3002", "conclusion": "\u63d0\u793a\u5b66\u4e60\u662f\u89e3\u51b3\u536b\u661f\u548c\u822a\u7a7a\u5f71\u50cf\u9886\u57df\u57df\u8fc1\u79fb\u95ee\u9898\u7684\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u8be5\u9886\u57df\u7684\u672a\u6765\u7814\u7a76\u5960\u5b9a\u4e86\u575a\u5b9e\u7684\u57fa\u7840\u3002"}}
{"id": "2510.23817", "categories": ["cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.23817", "abs": "https://arxiv.org/abs/2510.23817", "authors": ["Pedro Cortes dos Santos", "Matheus Becali Rocha", "Renato A Krohling"], "title": "Combining SHAP and Causal Analysis for Interpretable Fault Detection in Industrial Processes", "comment": null, "summary": "Industrial processes generate complex data that challenge fault detection\nsystems, often yielding opaque or underwhelming results despite advanced\nmachine learning techniques. This study tackles such difficulties using the\nTennessee Eastman Process, a well-established benchmark known for its intricate\ndynamics, to develop an innovative fault detection framework. Initial attempts\nwith standard models revealed limitations in both performance and\ninterpretability, prompting a shift toward a more tractable approach. By\nemploying SHAP (SHapley Additive exPlanations), we transform the problem into a\nmore manageable and transparent form, pinpointing the most critical process\nfeatures driving fault predictions. This reduction in complexity unlocks the\nability to apply causal analysis through Directed Acyclic Graphs, generated by\nmultiple algorithms, to uncover the underlying mechanisms of fault propagation.\nThe resulting causal structures align strikingly with SHAP findings,\nconsistently highlighting key process elements-like cooling and separation\nsystems-as pivotal to fault development. Together, these methods not only\nenhance detection accuracy but also provide operators with clear, actionable\ninsights into fault origins, a synergy that, to our knowledge, has not been\npreviously explored in this context. This dual approach bridges predictive\npower with causal understanding, offering a robust tool for monitoring complex\nmanufacturing environments and paving the way for smarter, more interpretable\nfault detection in industrial systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408SHAP\u548c\u56e0\u679c\u5206\u6790\uff08\u6709\u5411\u65e0\u73af\u56fe\uff09\u7684\u5de5\u4e1a\u8fc7\u7a0b\u6545\u969c\u68c0\u6d4b\u65b0\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u590d\u6742\u5de5\u4e1a\u6570\u636e\u5e26\u6765\u7684\u6311\u6218\u3002", "motivation": "\u5de5\u4e1a\u8fc7\u7a0b\u6570\u636e\u590d\u6742\uff0c\u5bfc\u81f4\u6545\u969c\u68c0\u6d4b\u7cfb\u7edf\u7ed3\u679c\u4e0d\u900f\u660e\u6216\u6548\u679c\u4e0d\u4f73\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u521b\u65b0\u7684\u6545\u969c\u68c0\u6d4b\u6846\u67b6\u6765\u89e3\u51b3\u8fd9\u4e9b\u56f0\u96be\u3002", "method": "\u5229\u7528SHAP\uff08SHapley Additive exPlanations\uff09\u5c06\u95ee\u9898\u8f6c\u5316\u4e3a\u66f4\u6613\u4e8e\u7ba1\u7406\u548c\u900f\u660e\u7684\u5f62\u5f0f\uff0c\u5e76\u7ed3\u5408\u56e0\u679c\u5206\u6790\uff08\u6709\u5411\u65e0\u73af\u56fe\uff09\u6765\u63ed\u793a\u6545\u969c\u4f20\u64ad\u7684\u6f5c\u5728\u673a\u5236\u3002", "result": "SHAP\u548c\u56e0\u679c\u5206\u6790\u7684\u7ed3\u679c\u4e00\u81f4\uff0c\u5747\u5f3a\u8c03\u4e86\u51b7\u5374\u548c\u5206\u79bb\u7cfb\u7edf\u7b49\u5173\u952e\u8fc7\u7a0b\u8981\u7d20\u5728\u6545\u969c\u53d1\u5c55\u4e2d\u7684\u6838\u5fc3\u4f5c\u7528\u3002\u8be5\u6846\u67b6\u63d0\u9ad8\u4e86\u68c0\u6d4b\u51c6\u786e\u6027\uff0c\u5e76\u4e3a\u64cd\u4f5c\u5458\u63d0\u4f9b\u4e86\u5173\u4e8e\u6545\u969c\u6765\u6e90\u7684\u6e05\u6670\u3001\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u7ed3\u5408SHAP\u548c\u56e0\u679c\u5206\u6790\u7684\u65b9\u6cd5\uff0c\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u6545\u969c\u68c0\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u8fd8\u63d0\u4f9b\u4e86\u5bf9\u6545\u969c\u6839\u6e90\u7684\u56e0\u679c\u7406\u89e3\uff0c\u5f25\u5408\u4e86\u9884\u6d4b\u80fd\u529b\u4e0e\u56e0\u679c\u7406\u89e3\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u76d1\u63a7\u590d\u6742\u5236\u9020\u73af\u5883\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u5de5\u5177\uff0c\u5e76\u4e3a\u5de5\u4e1a\u7cfb\u7edf\u63d0\u4f9b\u66f4\u667a\u80fd\u3001\u66f4\u53ef\u89e3\u91ca\u7684\u6545\u969c\u68c0\u6d4b\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2510.24551", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24551", "abs": "https://arxiv.org/abs/2510.24551", "authors": ["Gang Chen", "Changshuo Liu", "Gene Anne Ooi", "Marcus Tan", "Zhongle Xie", "Jianwei Yin", "James Wei Luen Yip", "Wenqiao Zhang", "Jiaqi Zhu", "Beng Chin Ooi"], "title": "Generative AI for Healthcare: Fundamentals, Challenges, and Perspectives", "comment": null, "summary": "Generative Artificial Intelligence (GenAI) is taking the world by storm. It\npromises transformative opportunities for advancing and disrupting existing\npractices, including healthcare. From large language models (LLMs) for clinical\nnote synthesis and conversational assistance to multimodal systems that\nintegrate medical imaging, electronic health records, and genomic data for\ndecision support, GenAI is transforming the practice of medicine and the\ndelivery of healthcare, such as diagnosis and personalized treatments, with\ngreat potential in reducing the cognitive burden on clinicians, thereby\nimproving overall healthcare delivery. However, GenAI deployment in healthcare\nrequires an in-depth understanding of healthcare tasks and what can and cannot\nbe achieved. In this paper, we propose a data-centric paradigm in the design\nand deployment of GenAI systems for healthcare. Specifically, we reposition the\ndata life cycle by making the medical data ecosystem as the foundational\nsubstrate for generative healthcare systems. This ecosystem is designed to\nsustainably support the integration, representation, and retrieval of diverse\nmedical data and knowledge. With effective and efficient data processing\npipelines, such as semantic vector search and contextual querying, it enables\nGenAI-powered operations for upstream model components and downstream clinical\napplications. Ultimately, it not only supplies foundation models with\nhigh-quality, multimodal data for large-scale pretraining and domain-specific\nfine-tuning, but also serves as a knowledge retrieval backend to support\ntask-specific inference via the agentic layer. The ecosystem enables the\ndeployment of GenAI for high-quality and effective healthcare delivery.", "AI": {"tldr": "\u751f\u6210\u5f0fAI\u5728\u533b\u7597\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u90e8\u7f72\u9762\u4e34\u6311\u6218\u3002\u672c\u6587\u63d0\u51fa\u4ee5\u6570\u636e\u4e3a\u4e2d\u5fc3\u7684\u8303\u5f0f\uff0c\u5c06\u533b\u7597\u6570\u636e\u751f\u6001\u7cfb\u7edf\u4f5c\u4e3a\u57fa\u7840\uff0c\u901a\u8fc7\u9ad8\u6548\u7684\u6570\u636e\u5904\u7406\u7ba1\u9053\u652f\u6301\u751f\u6210\u5f0fAI\u7684\u8bad\u7ec3\u548c\u5e94\u7528\uff0c\u6700\u7ec8\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u533b\u7597\u670d\u52a1\u3002", "motivation": "\u751f\u6210\u5f0fAI\u5728\u533b\u7597\u9886\u57df\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u5176\u6210\u529f\u90e8\u7f72\u9700\u8981\u6df1\u5165\u7406\u89e3\u533b\u7597\u4efb\u52a1\u548c\u6280\u672f\u9650\u5236\u3002", "method": "\u63d0\u51fa\u4ee5\u6570\u636e\u4e3a\u4e2d\u5fc3\u7684\u8303\u5f0f\uff0c\u5c06\u533b\u7597\u6570\u636e\u751f\u6001\u7cfb\u7edf\u4f5c\u4e3a\u751f\u6210\u5f0fAI\u7cfb\u7edf\u7684\u57fa\u7840\uff0c\u901a\u8fc7\u6570\u636e\u751f\u547d\u5468\u671f\u7ba1\u7406\u3001\u8bed\u4e49\u5411\u91cf\u641c\u7d22\u548c\u4e0a\u4e0b\u6587\u67e5\u8be2\u7b49\u6280\u672f\uff0c\u652f\u6301\u6a21\u578b\u7684\u9884\u8bad\u7ec3\u3001\u5fae\u8c03\u548c\u63a8\u7406\u3002", "result": "\u6784\u5efa\u4e86\u4e00\u4e2a\u53ef\u6301\u7eed\u652f\u6301\u6574\u5408\u3001\u8868\u793a\u548c\u68c0\u7d22\u591a\u6837\u5316\u533b\u7597\u6570\u636e\u548c\u77e5\u8bc6\u7684\u751f\u6001\u7cfb\u7edf\uff0c\u4e3a\u751f\u6210\u5f0fAI\u63d0\u4f9b\u9ad8\u8d28\u91cf\u591a\u6a21\u6001\u6570\u636e\uff0c\u5e76\u4f5c\u4e3a\u77e5\u8bc6\u68c0\u7d22\u540e\u7aef\u652f\u6301\u7279\u5b9a\u4efb\u52a1\u7684\u63a8\u7406\u3002", "conclusion": "\u8be5\u751f\u6001\u7cfb\u7edf\u901a\u8fc7\u63d0\u4f9b\u9ad8\u8d28\u91cf\u3001\u591a\u6a21\u6001\u7684\u6570\u636e\u548c\u6709\u6548\u7684\u77e5\u8bc6\u68c0\u7d22\u80fd\u529b\uff0c\u80fd\u591f\u652f\u6301\u90e8\u7f72\u751f\u6210\u5f0fAI\u4ee5\u5b9e\u73b0\u9ad8\u8d28\u91cf\u548c\u6709\u6548\u7684\u533b\u7597\u670d\u52a1\u3002"}}
{"id": "2510.24365", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24365", "abs": "https://arxiv.org/abs/2510.24365", "authors": ["Matthew Shardlow"], "title": "Text Simplification with Sentence Embeddings", "comment": null, "summary": "Sentence embeddings can be decoded to give approximations of the original\ntexts used to create them. We explore this effect in the context of text\nsimplification, demonstrating that reconstructed text embeddings preserve\ncomplexity levels. We experiment with a small feed forward neural network to\neffectively learn a transformation between sentence embeddings representing\nhigh-complexity and low-complexity texts. We provide comparison to a Seq2Seq\nand LLM-based approach, showing encouraging results in our much smaller\nlearning setting. Finally, we demonstrate the applicability of our\ntransformation to an unseen simplification dataset (MedEASI), as well as\ndatasets from languages outside the training data (ES,DE). We conclude that\nlearning transformations in sentence embedding space is a promising direction\nfor future research and has potential to unlock the ability to develop small,\nbut powerful models for text simplification and other natural language\ngeneration tasks.", "AI": {"tldr": "\u7f16\u7801\u540e\u7684\u6587\u672c\u5d4c\u5165\u53ef\u4ee5\u7528\u6765\u91cd\u5efa\u539f\u6587\uff0c\u5e76\u4fdd\u6301\u6587\u672c\u7684\u590d\u6742\u5ea6\u6c34\u5e73\u3002\u6211\u4eec\u4f7f\u7528\u4e00\u4e2a\u5c0f\u7684\u524d\u9988\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u4e86\u9ad8\u4f4e\u590d\u6742\u5ea6\u6587\u672c\u5d4c\u5165\u4e4b\u95f4\u7684\u8f6c\u6362\uff0c\u5e76\u53d6\u5f97\u4e86\u6bd4Seq2Seq\u548cLLM\u6a21\u578b\u66f4\u597d\u7684\u7ed3\u679c\uff0c\u540c\u65f6\u8fd8\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u6570\u636e\u96c6\u548c\u8bed\u8a00\u4e0a\u7684\u6709\u6548\u6027\u3002", "motivation": "\u63a2\u8ba8\u6587\u672c\u5d4c\u5165\u5728\u6587\u672c\u7b80\u5316\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\uff0c\u4ee5\u53ca\u91cd\u5efa\u6587\u672c\u5d4c\u5165\u662f\u5426\u80fd\u4fdd\u6301\u539f\u6587\u7684\u590d\u6742\u5ea6\u6c34\u5e73\u3002", "method": "\u4f7f\u7528\u4e00\u4e2a\u5c0f\u7684\u524d\u9988\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u9ad8\u4f4e\u590d\u6742\u5ea6\u6587\u672c\u5d4c\u5165\u4e4b\u95f4\u7684\u8f6c\u6362\uff0c\u5e76\u4e0eSeq2Seq\u548cLLM\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u6587\u672c\u7b80\u5316\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u4ee4\u4eba\u9f13\u821e\u7684\u7ed3\u679c\uff0c\u5e76\u4e14\u5728\u672a\u89c1\u8fc7\u7684\u6570\u636e\u96c6\uff08MedEASI\uff09\u548c\u975e\u8bad\u7ec3\u8bed\u8a00\uff08ES, DE\uff09\u4e0a\u5747\u6709\u6548\u3002", "conclusion": "\u5728\u53e5\u5b50\u5d4c\u5165\u7a7a\u95f4\u4e2d\u5b66\u4e60\u8f6c\u6362\u662f\u4e00\u79cd\u6709\u524d\u9014\u7684\u7814\u7a76\u65b9\u5411\uff0c\u6709\u6f5c\u529b\u5f00\u53d1\u51fa\u5c0f\u800c\u5f3a\u5927\u7684\u6587\u672c\u7b80\u5316\u548c\u5176\u4ed6\u81ea\u7136\u8bed\u8a00\u751f\u6210\u6a21\u578b\u3002"}}
{"id": "2510.24366", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24366", "abs": "https://arxiv.org/abs/2510.24366", "authors": ["Thanh-Huy Nguyen", "Hoang-Thien Nguyen", "Ba-Thinh Lam", "Vi Vu", "Bach X. Nguyen", "Jianhua Xing", "Tianyang Wang", "Xingjian Li", "Min Xu"], "title": "Adaptive Knowledge Transferring with Switching Dual-Student Framework for Semi-Supervised Medical Image Segmentation", "comment": "The paper is under review at Pattern Recognition Journal", "summary": "Teacher-student frameworks have emerged as a leading approach in\nsemi-supervised medical image segmentation, demonstrating strong performance\nacross various tasks. However, the learning effects are still limited by the\nstrong correlation and unreliable knowledge transfer process between teacher\nand student networks. To overcome this limitation, we introduce a novel\nswitching Dual-Student architecture that strategically selects the most\nreliable student at each iteration to enhance dual-student collaboration and\nprevent error reinforcement. We also introduce a strategy of Loss-Aware\nExponential Moving Average to dynamically ensure that the teacher absorbs\nmeaningful information from students, improving the quality of pseudo-labels.\nOur plug-and-play framework is extensively evaluated on 3D medical image\nsegmentation datasets, where it outperforms state-of-the-art semi-supervised\nmethods, demonstrating its effectiveness in improving segmentation accuracy\nunder limited supervision.", "AI": {"tldr": "\u901a\u8fc7\u5f15\u5165\u5f00\u5173\u53cc\u5b66\u751f\u67b6\u6784\u548c\u635f\u5931\u611f\u77e5\u6307\u6570\u79fb\u52a8\u5e73\u5747\u7b56\u7565\uff0c\u6539\u8fdb\u4e86\u534a\u76d1\u7763\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u7684\u6559\u5e08-\u5b66\u751f\u6846\u67b6\uff0c\u63d0\u9ad8\u4e86\u5206\u5272\u7cbe\u5ea6\u3002", "motivation": "\u6559\u5e08-\u5b66\u751f\u6846\u67b6\u5728\u534a\u76d1\u7763\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u6559\u5e08\u548c\u5b66\u751f\u7f51\u7edc\u4e4b\u95f4\u5f3a\u76f8\u5173\u6027\u548c\u4e0d\u53ef\u9760\u7684\u77e5\u8bc6\u8f6c\u79fb\u9650\u5236\u4e86\u5b66\u4e60\u6548\u679c\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5f00\u5173\u53cc\u5b66\u751f\u67b6\u6784\uff0c\u8be5\u67b6\u6784\u5728\u6bcf\u6b21\u8fed\u4ee3\u65f6\u7b56\u7565\u6027\u5730\u9009\u62e9\u6700\u53ef\u9760\u7684\u5b66\u751f\u4ee5\u589e\u5f3a\u53cc\u5b66\u751f\u534f\u4f5c\u5e76\u9632\u6b62\u9519\u8bef\u7d2f\u79ef\u3002\u8fd8\u5f15\u5165\u4e86\u4e00\u79cd\u635f\u5931\u611f\u77e5\u6307\u6570\u79fb\u52a8\u5e73\u5747\u7b56\u7565\uff0c\u4ee5\u52a8\u6001\u5730\u786e\u4fdd\u6559\u5e08\u4ece\u5b66\u751f\u90a3\u91cc\u5438\u6536\u6709\u7528\u7684\u4fe1\u606f\uff0c\u4ece\u800c\u63d0\u9ad8\u4f2a\u6807\u7b7e\u7684\u8d28\u91cf\u3002", "result": "\u57283D\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u8bc4\u4f30\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u6846\u67b6\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u534a\u76d1\u7763\u65b9\u6cd5\uff0c\u5728\u6709\u9650\u76d1\u7763\u4e0b\u63d0\u9ad8\u4e86\u5206\u5272\u7cbe\u5ea6\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u5373\u63d2\u5373\u7528\u6846\u67b6\u901a\u8fc7\u6539\u8fdb\u7684\u5e08\u751f\u534f\u4f5c\u548c\u4f2a\u6807\u7b7e\u8d28\u91cf\uff0c\u5728\u534a\u76d1\u7763\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4efb\u52a1\u4e2d\u6709\u6548\u63d0\u9ad8\u4e86\u5206\u5272\u7cbe\u5ea6\u3002"}}
{"id": "2510.23818", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23818", "abs": "https://arxiv.org/abs/2510.23818", "authors": ["Yilang Zhang", "Xiaodong Yang", "Yiwei Cai", "Georgios B. Giannakis"], "title": "ScaLoRA: Optimally Scaled Low-Rank Adaptation for Efficient High-Rank Fine-Tuning", "comment": null, "summary": "As large language models (LLMs) continue to scale in size, the computational\noverhead has become a major bottleneck for task-specific fine-tuning. While\nlow-rank adaptation (LoRA) effectively curtails this cost by confining the\nweight updates to a low-dimensional subspace, such a restriction can hinder\neffectiveness and slow convergence. This contribution deals with these\nlimitations by accumulating progressively a high-rank weight update from\nconsecutive low-rank increments. Specifically, the per update optimal low-rank\nmatrix is identified to minimize the loss function and closely approximate full\nfine-tuning. To endow efficient and seamless optimization without restarting,\nthis optimal choice is formed by appropriately scaling the columns of the\noriginal low-rank matrix. Rigorous performance guarantees reveal that the\noptimal scaling can be found analytically. Extensive numerical tests with\npopular LLMs scaling up to 12 billion parameters demonstrate a consistent\nperformance gain and fast convergence relative to state-of-the-art LoRA\nvariants on diverse tasks including natural language understanding, commonsense\nreasoning, and mathematical problem solving.", "AI": {"tldr": "\u901a\u8fc7\u9010\u6b65\u7d2f\u52a0\u4f4e\u79e9\u589e\u91cf\u6765\u6784\u5efa\u9ad8\u79e9\u6743\u91cd\u66f4\u65b0\uff0c\u4ee5\u514b\u670dLoRA\u5728\u6a21\u578b\u5fae\u8c03\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u4ece\u800c\u63d0\u9ad8\u6027\u80fd\u548c\u6536\u655b\u901f\u5ea6\u3002", "motivation": "LoRA\u5728\u964d\u4f4eLLM\u5fae\u8c03\u6210\u672c\u7684\u540c\u65f6\uff0c\u4e5f\u9650\u5236\u4e86\u5176\u6709\u6548\u6027\u548c\u6536\u655b\u901f\u5ea6\u3002", "method": "\u901a\u8fc7\u5206\u6790\u786e\u5b9a\u6700\u4f18\u7684\u4f4e\u79e9\u77e9\u9635\u7f29\u653e\u65b9\u6cd5\uff0c\u5b9e\u73b0\u5bf9\u4f4e\u79e9\u589e\u91cf\u7684\u7d2f\u52a0\uff0c\u6784\u5efa\u9ad8\u79e9\u66f4\u65b0\uff0c\u800c\u65e0\u9700\u91cd\u65b0\u542f\u52a8\u3002", "result": "\u5728\u5305\u62ec\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u3001\u5e38\u8bc6\u63a8\u7406\u548c\u6570\u5b66\u95ee\u9898\u89e3\u51b3\u7b49\u591a\u79cd\u4efb\u52a1\u4e0a\uff0c\u4e0e\u73b0\u6709\u7684LoRA\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5728\u53c2\u6570\u91cf\u9ad8\u8fbe120\u4ebf\u7684LLM\u4e0a\u5c55\u73b0\u51fa\u6301\u7eed\u7684\u6027\u80fd\u63d0\u5347\u548c\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u901a\u8fc7\u7d2f\u52a0\u4f4e\u79e9\u66f4\u65b0\u6765\u6709\u6548\u89e3\u51b3LoRA\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u5404\u79cd\u4e0b\u6e38\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u6027\u80fd\u7684\u63d0\u5347\u548c\u6536\u655b\u7684\u52a0\u901f\u3002"}}
{"id": "2510.24645", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24645", "abs": "https://arxiv.org/abs/2510.24645", "authors": ["Zengzhuang Xu", "Bingguang Hao", "Zechuan Wang", "Yuntao Wen", "Maolin Wang", "Yang Liu", "Long Chen", "Dong Wang", "Yicheng Chen", "Cunyin Peng", "Chenyi Zhuang", "Jinjie Gu", "Leilei Gan", "Xiangyu Zhao", "Shi Gu"], "title": "FunReason-MT Technical Report: Overcoming the Complexity Barrier in Multi-Turn Function Calling", "comment": null, "summary": "Function calling (FC) empowers large language models (LLMs) and autonomous\nagents to interface with external tools, a critical capability for solving\ncomplex, real-world problems. As this ability becomes increasingly central to\nadvanced AI systems, the need for high-quality, multi-turn training data to\ndevelop and refine it cannot be overstated. Existing data synthesis methods,\nsuch as random environment sampling or multi-agent role-playing, are not\npowerful enough to generate high-quality data in real-world environments.\nPractical challenges come in three folds: targeted model training, isolation of\ntool architecture, and multi-turn logical dependency. To address these\nstructural deficiencies, we present FunReason-MT, a novel data synthesis\nframework for real-world multi-turn tool use. FunReason-MT resolves the\ncomplexity barrier in multi-turn FC data by employing 1) Environment-API Graph\nInteractions to gather varied high-quality trajectories, 2) Advanced Tool-Query\nSynthesis to simplify hard query construction, and 3) Guided Iterative Chain\nfor sophisticated CoT generation. Evaluations on Berkeley Function-Calling\nLeaderboard (BFCLv3) demonstrate the power of our framework: a 4B model built\nupon FunReason-MT generated data achieves state-of-the-art performance among\ncomparable-sized models, outperforming most close-source models. Further\nperformance improvements on BFCLv4 confirm that FunReason-MT provides a\nreliable and robust source for agentic learning.", "AI": {"tldr": "FunReason-MT\u662f\u4e00\u4e2a\u65b0\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u591a\u8f6e\u51fd\u6570\u8c03\u7528\uff08FC\uff09\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u4ee5\u63d0\u9ad8LLM\u548c\u81ea\u4e3b\u4ee3\u7406\u4e0e\u5916\u90e8\u5de5\u5177\u7684\u4ea4\u4e92\u80fd\u529b\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u73af\u5883-API\u56fe\u4ea4\u4e92\u3001\u9ad8\u7ea7\u5de5\u5177-\u67e5\u8be2\u5408\u6210\u548c\u5f15\u5bfc\u8fed\u4ee3\u94fe\u6765\u89e3\u51b3\u73b0\u6709\u6570\u636e\u5408\u6210\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728Berkeley\u51fd\u6570\u8c03\u7528\u6392\u884c\u699c\uff08BFCL\uff09\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u51fd\u6570\u8c03\u7528\uff08FC\uff09\u6570\u636e\u5408\u6210\u65b9\u6cd5\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u591a\u8f6e\u6b21\u7684\u771f\u5b9e\u4e16\u754c\u6570\u636e\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u96be\u4ee5\u6ee1\u8db3LLM\u548c\u81ea\u4e3b\u4ee3\u7406\u5728\u590d\u6742\u73b0\u5b9e\u4e16\u754c\u95ee\u9898\u4e2d\u4e0e\u5916\u90e8\u5de5\u5177\u4ea4\u4e92\u7684\u9700\u6c42\u3002", "method": "FunReason-MT\u6846\u67b6\u91c7\u7528\u4e09\u79cd\u6280\u672f\uff1a1\uff09\u73af\u5883-API\u56fe\u4ea4\u4e92\uff0c\u7528\u4e8e\u6536\u96c6\u591a\u6837\u5316\u7684\u9ad8\u8d28\u91cf\u8f68\u8ff9\uff1b2\uff09\u9ad8\u7ea7\u5de5\u5177-\u67e5\u8be2\u5408\u6210\uff0c\u7528\u4e8e\u7b80\u5316\u590d\u6742\u67e5\u8be2\u7684\u6784\u5efa\uff1b3\uff09\u5f15\u5bfc\u8fed\u4ee3\u94fe\uff0c\u7528\u4e8e\u751f\u6210\u590d\u6742\u7684\u601d\u7ef4\u94fe\uff08CoT\uff09\u3002", "result": "\u5728BFCLv3\u8bc4\u4f30\u4e2d\uff0c\u4f7f\u7528FunReason-MT\u751f\u6210\u7684\u6570\u636e\u8bad\u7ec3\u76844B\u6a21\u578b\uff0c\u5728\u540c\u7b49\u89c4\u6a21\u7684\u6a21\u578b\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u751a\u81f3\u4f18\u4e8e\u5927\u591a\u6570\u95ed\u6e90\u6a21\u578b\u3002\u5728BFCLv4\u4e0a\u7684\u8fdb\u4e00\u6b65\u6027\u80fd\u63d0\u5347\u4e5f\u8bc1\u5b9e\u4e86FunReason-MT\u5728\u667a\u80fd\u4f53\u5b66\u4e60\u4e2d\u7684\u53ef\u9760\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "FunReason-MT\u6846\u67b6\u80fd\u591f\u6709\u6548\u5730\u751f\u6210\u7528\u4e8e\u591a\u8f6e\u51fd\u6570\u8c03\u7528\u7684\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u5728\u4e0e\u5916\u90e8\u5de5\u5177\u4ea4\u4e92\u65b9\u9762\u7684\u80fd\u529b\uff0c\u5e76\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u9886\u5148\u6027\u80fd\u3002"}}
{"id": "2510.24425", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24425", "abs": "https://arxiv.org/abs/2510.24425", "authors": ["Guangyu Xie", "Yice Zhang", "Jianzhu Bao", "Qianlong Wang", "Yang Sun", "Bingbing Wang", "Ruifeng Xu"], "title": "Comprehensive and Efficient Distillation for Lightweight Sentiment Analysis Models", "comment": "Accepted by EMNLP 2025. 22 pages, 9 figures. The first two authors\n  contribute equally", "summary": "Recent efforts leverage knowledge distillation techniques to develop\nlightweight and practical sentiment analysis models. These methods are grounded\nin human-written instructions and large-scale user texts. Despite the promising\nresults, two key challenges remain: (1) manually written instructions are\nlimited in diversity and quantity, making them insufficient to ensure\ncomprehensive coverage of distilled knowledge; (2) large-scale user texts incur\nhigh computational cost, hindering the practicality of these methods. To this\nend, we introduce COMPEFFDIST, a comprehensive and efficient distillation\nframework for sentiment analysis. Our framework consists of two key modules:\nattribute-based automatic instruction construction and difficulty-based data\nfiltering, which correspondingly tackle the aforementioned challenges. Applying\nour method across multiple model series (Llama-3, Qwen-3, and Gemma-3), we\nenable 3B student models to match the performance of 20x larger teacher models\non most tasks. In addition, our approach greatly outperforms baseline methods\nin data efficiency, attaining the same performance level with only 10% of the\ndata.", "AI": {"tldr": "COMPEFFDIST\u662f\u4e00\u4e2a\u5168\u9762\u7684\u3001\u9ad8\u6548\u7684\u84b8\u998f\u6846\u67b6\uff0c\u7528\u4e8e\u60c5\u611f\u5206\u6790\uff0c\u901a\u8fc7\u81ea\u52a8\u6784\u5efa\u6307\u4ee4\u548c\u8fc7\u6ee4\u6570\u636e\uff0c\u4f7f\u5c0f\u578b\u6a21\u578b\u80fd\u591f\u5339\u914d\u5927\u578b\u6a21\u578b\uff0c\u5e76\u5927\u5927\u63d0\u9ad8\u6570\u636e\u6548\u7387\u3002", "motivation": "\u624b\u52a8\u7f16\u5199\u7684\u6307\u4ee4\u5728\u591a\u6837\u6027\u548c\u6570\u91cf\u4e0a\u5b58\u5728\u5c40\u9650\uff0c\u65e0\u6cd5\u5145\u5206\u8986\u76d6\u84b8\u998f\u77e5\u8bc6\uff1b\u5927\u89c4\u6a21\u7528\u6237\u6587\u672c\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u963b\u788d\u4e86\u65b9\u6cd5\u7684\u5b9e\u7528\u6027\u3002", "method": "COMPEFFDIST\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u5173\u952e\u6a21\u5757\uff1a\u57fa\u4e8e\u5c5e\u6027\u7684\u81ea\u52a8\u6307\u4ee4\u6784\u5efa\u548c\u57fa\u4e8e\u96be\u5ea6\u7684-\u6570\u636e\u8fc7\u6ee4\uff0c\u4ee5\u89e3\u51b3\u4e0a\u8ff0\u6311\u6218\u3002", "result": "\u5728Llama-3\u3001Qwen-3\u548cGemma-3\u7b49\u591a\u4e2a\u6a21\u578b\u7cfb\u5217\u4e0a\uff0c3B\u5b66\u751f\u6a21\u578b\u5728\u5927\u591a\u6570\u4efb\u52a1\u4e0a\u80fd\u591f\u5339\u914d20\u500d\u5927\u7684\u6559\u5e08\u6a21\u578b\u7684\u6027\u80fd\u3002\u4e0e\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5728\u6570\u636e\u6548\u7387\u65b9\u9762\u8868\u73b0\u66f4\u4f18\uff0c\u4ec5\u752810%\u7684\u6570\u636e\u5373\u53ef\u8fbe\u5230\u76f8\u540c\u7684\u6027\u80fd\u6c34\u5e73\u3002", "conclusion": "COMPEFFDIST\u6846\u67b6\u6210\u529f\u5730\u89e3\u51b3\u4e86\u73b0\u6709\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u5728\u60c5\u611f\u5206\u6790\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u548c\u9ad8\u6548\u7684\u6570\u636e\u5904\u7406\uff0c\u5b9e\u73b0\u4e86\u5c0f\u578b\u6a21\u578b\u4e0e\u5927\u578b\u6a21\u578b\u7684\u6027\u80fd\u76f8\u5f53\uff0c\u5e76\u663e\u8457\u63d0\u9ad8\u4e86\u6570\u636e\u6548\u7387\u3002"}}
{"id": "2510.24374", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24374", "abs": "https://arxiv.org/abs/2510.24374", "authors": ["Yuda Zou", "Zijian Zhang", "Yongchao Xu"], "title": "Decoupling What to Count and Where to See for Referring Expression Counting", "comment": null, "summary": "Referring Expression Counting (REC) extends class-level object counting to\nthe fine-grained subclass-level, aiming to enumerate objects matching a textual\nexpression that specifies both the class and distinguishing attribute. A\nfundamental challenge, however, has been overlooked: annotation points are\ntypically placed on class-representative locations (e.g., heads), forcing\nmodels to focus on class-level features while neglecting attribute information\nfrom other visual regions (e.g., legs for \"walking\"). To address this, we\npropose W2-Net, a novel framework that explicitly decouples the problem into\n\"what to count\" and \"where to see\" via a dual-query mechanism. Specifically,\nalongside the standard what-to-count (w2c) queries that localize the object, we\nintroduce dedicated where-to-see (w2s) queries. The w2s queries are guided to\nseek and extract features from attribute-specific visual regions, enabling\nprecise subclass discrimination. Furthermore, we introduce Subclass Separable\nMatching (SSM), a novel matching strategy that incorporates a repulsive force\nto enhance inter-subclass separability during label assignment. W2-Net\nsignificantly outperforms the state-of-the-art on the REC-8K dataset, reducing\ncounting error by 22.5% (validation) and 18.0% (test), and improving\nlocalization F1 by 7% and 8%, respectively. Code will be available.", "AI": {"tldr": "W2-Net\u901a\u8fc7\u5f15\u5165\u201c\u5728\u54ea\u91cc\u770b\u201d\u67e5\u8be2\u6765\u89e3\u51b3Referring Expression Counting\uff08REC\uff09\u4e2d\u7684\u5c5e\u6027\u8bc6\u522b\u95ee\u9898\uff0c\u5e76\u4f7f\u7528Subclass Separable Matching\uff08SSM\uff09\u6765\u63d0\u9ad8\u5b50\u7c7b\u53ef\u5206\u6027\uff0c\u5728REC-8K\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u73b0\u6709\u7684Referring Expression Counting\uff08REC\uff09\u65b9\u6cd5\u5728\u5904\u7406\u7ec6\u7c92\u5ea6\u7684\u5b50\u7c7b\u8ba1\u6570\u65f6\u5b58\u5728\u6311\u6218\uff0c\u56e0\u4e3a\u6ce8\u91ca\u70b9\u901a\u5e38\u4f4d\u4e8e\u7c7b\u4ee3\u8868\u6027\u4f4d\u7f6e\uff08\u4f8b\u5982\u5934\u90e8\uff09\uff0c\u5bfc\u81f4\u6a21\u578b\u4fa7\u91cd\u4e8e\u7c7b\u7ea7\u522b\u7279\u5f81\u800c\u5ffd\u7565\u4e86\u5176\u4ed6\u89c6\u89c9\u533a\u57df\uff08\u4f8b\u5982\u201c\u884c\u8d70\u201d\u7684\u817f\u90e8\uff09\u7684\u5c5e\u6027\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aW2-Net\u7684\u65b0\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u53cc\u67e5\u8be2\u673a\u5236\u5c06\u95ee\u9898\u660e\u786e\u5212\u5206\u4e3a\u201c\u8ba1\u6570\u4ec0\u4e48\u201d\u548c\u201c\u5728\u54ea\u91cc\u770b\u201d\u3002\u9664\u4e86\u6807\u51c6\u7684\u201c\u8ba1\u6570\u4ec0\u4e48\u201d\uff08w2c\uff09\u67e5\u8be2\u6765\u5b9a\u4f4d\u5bf9\u8c61\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u4e13\u95e8\u7684\u201c\u5728\u54ea\u91cc\u770b\u201d\uff08w2s\uff09\u67e5\u8be2\uff0c\u4ee5\u4ece\u7279\u5b9a\u5c5e\u6027\u7684\u89c6\u89c9\u533a\u57df\u63d0\u53d6\u7279\u5f81\uff0c\u4ece\u800c\u5b9e\u73b0\u7cbe\u786e\u7684\u5b50\u7c7b\u533a\u5206\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u5b50\u7c7b\u53ef\u5206\u79bb\u5339\u914d\uff08SSM\uff09\u7b56\u7565\uff0c\u901a\u8fc7\u589e\u52a0\u6392\u65a5\u529b\u6765\u589e\u5f3a\u6807\u7b7e\u5206\u914d\u8fc7\u7a0b\u4e2d\u7684\u5b50\u7c7b\u53ef\u5206\u6027\u3002", "result": "W2-Net\u5728REC-8K\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u8ba1\u6570\u9519\u8bef\u5728\u9a8c\u8bc1\u96c6\u4e0a\u964d\u4f4e\u4e8622.5%\uff0c\u5728\u6d4b\u8bd5\u96c6\u4e0a\u964d\u4f4e\u4e8618.0%\uff0c\u5b9a\u4f4dF1\u5206\u6570\u5206\u522b\u63d0\u9ad8\u4e867%\u548c8%\u3002", "conclusion": "W2-Net\u901a\u8fc7\u5176\u65b0\u9896\u7684\u53cc\u67e5\u8be2\u673a\u5236\u548cSSM\u7b56\u7565\uff0c\u6709\u6548\u5730\u89e3\u51b3\u4e86REC\u4e2d\u7684\u5c5e\u6027\u8bc6\u522b\u6311\u6218\uff0c\u5e76\u5728\u5173\u952e\u6307\u6807\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2510.23866", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23866", "abs": "https://arxiv.org/abs/2510.23866", "authors": ["Paul Rosu", "Muchang Bahng", "Erick Jiang", "Rico Zhu", "Vahid Tarokh"], "title": "A PDE-Informed Latent Diffusion Model for 2-m Temperature Downscaling", "comment": null, "summary": "This work presents a physics-conditioned latent diffusion model tailored for\ndynamical downscaling of atmospheric data, with a focus on reconstructing\nhigh-resolution 2-m temperature fields. Building upon a pre-existing diffusion\narchitecture and employing a residual formulation against a reference UNet, we\nintegrate a partial differential equation (PDE) loss term into the model's\ntraining objective. The PDE loss is computed in the full resolution (pixel)\nspace by decoding the latent representation and is designed to enforce physical\nconsistency through a finite-difference approximation of an effective\nadvection-diffusion balance. Empirical observations indicate that conventional\ndiffusion training already yields low PDE residuals, and we investigate how\nfine-tuning with this additional loss further regularizes the model and\nenhances the physical plausibility of the generated fields. The entirety of our\ncodebase is available on Github, for future reference and development.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7269\u7406\u6761\u4ef6\u5316\u7684\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u5927\u6c14\u6570\u636e\u7684\u52a8\u529b\u5b66\u964d\u5c3a\u5ea6\uff0c\u7279\u522b\u5173\u6ce8\u91cd\u5efa 2 \u7c73\u6e29\u5ea6\u573a\u3002\u901a\u8fc7\u5728\u73b0\u6709\u7684\u6269\u6563\u67b6\u6784\u57fa\u7840\u4e0a\uff0c\u5e76\u5229\u7528\u6b8b\u5dee UNet\uff0c\u5c06\u504f\u5fae\u5206\u65b9\u7a0b (PDE) \u635f\u5931\u9879\u6574\u5408\u5230\u6a21\u578b\u8bad\u7ec3\u4e2d\u3002PDE \u635f\u5931\u5728\u5b8c\u6574\u5206\u8fa8\u7387\uff08\u50cf\u7d20\uff09\u7a7a\u95f4\u4e2d\u8ba1\u7b97\uff0c\u901a\u8fc7\u89e3\u7801\u6f5c\u5728\u8868\u793a\u5e76\u4f7f\u7528\u6709\u9650\u5dee\u5206\u6cd5\u8fd1\u4f3c\u6709\u6548\u7684\u5e73\u6d41-\u6269\u6563\u5e73\u8861\u6765\u5f3a\u5236\u6267\u884c\u7269\u7406\u4e00\u81f4\u6027\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u4f20\u7edf\u7684\u6269\u6563\u8bad\u7ec3\u5df2\u7ecf\u4ea7\u751f\u4e86\u4f4e PDE \u6b8b\u5dee\uff0c\u5e76\u4e14\u8be5\u6a21\u578b\u901a\u8fc7\u989d\u5916\u7684 PDE \u635f\u5931\u8fdb\u884c\u5fae\u8c03\uff0c\u53ef\u4ee5\u8fdb\u4e00\u6b65\u6b63\u5219\u5316\u6a21\u578b\u5e76\u63d0\u9ad8\u751f\u6210\u573a\u7684\u53ef\u4fe1\u5ea6\u3002", "motivation": "\u5927\u6c14\u6570\u636e\u7684\u52a8\u529b\u5b66\u964d\u5c3a\u5ea6\uff0c\u7279\u522b\u662f\u91cd\u5efa\u9ad8\u5206\u8fa8\u7387 2 \u7c73\u6e29\u5ea6\u573a\u3002", "method": "\u5728\u73b0\u6709\u7684\u6269\u6563\u6a21\u578b\u57fa\u7840\u4e0a\uff0c\u7ed3\u5408\u7269\u7406\u6761\u4ef6\uff08PDE \u635f\u5931\u9879\uff09\u8fdb\u884c\u8bad\u7ec3\uff0c\u901a\u8fc7\u6709\u9650\u5dee\u5206\u6cd5\u8fd1\u4f3c\u5e73\u6d41-\u6269\u6563\u5e73\u8861\u6765\u5f3a\u5236\u6267\u884c\u7269\u7406\u4e00\u81f4\u6027\u3002", "result": "\u4f20\u7edf\u7684\u6269\u6563\u8bad\u7ec3\u5df2\u7ecf\u4ea7\u751f\u4e86\u4f4e PDE \u6b8b\u5dee\uff0c\u901a\u8fc7 PDE \u635f\u5931\u8fdb\u884c\u5fae\u8c03\u53ef\u4ee5\u8fdb\u4e00\u6b65\u6b63\u5219\u5316\u6a21\u578b\u5e76\u63d0\u9ad8\u751f\u6210\u573a\u7684\u53ef\u4fe1\u5ea6\u3002", "conclusion": "\u7269\u7406\u6761\u4ef6\u5316\u7684\u6f5c\u5728\u6269\u6563\u6a21\u578b\u80fd\u591f\u6709\u6548\u5730\u7528\u4e8e\u5927\u6c14\u6570\u636e\u7684\u52a8\u529b\u5b66\u964d\u5c3a\u5ea6\uff0c\u5e76\u63d0\u9ad8\u751f\u6210\u7ed3\u679c\u7684\u7269\u7406\u5408\u7406\u6027\u3002"}}
{"id": "2510.24650", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24650", "abs": "https://arxiv.org/abs/2510.24650", "authors": ["Nitin Rai", "Daeun", "Choi", "Nathan S. Boyd", "Arnold W. Schumann"], "title": "Advancing site-specific disease and pest management in precision agriculture: From reasoning-driven foundation models to adaptive, feedback-based learning", "comment": "26 pages, 8 figures, and 2 tables", "summary": "Site-specific disease management (SSDM) in crops has advanced rapidly through\nmachine and deep learning (ML and DL) for real-time computer vision. Research\nevolved from handcrafted feature extraction to large-scale automated feature\nlearning. With foundation models (FMs), crop disease datasets are now processed\nin fundamentally new ways. Unlike traditional neural networks, FMs integrate\nvisual and textual data, interpret symptoms in text, reason about\nsymptom-management relationships, and support interactive QA for growers and\neducators. Adaptive and imitation learning in robotics further enables\nfield-based disease management. This review screened approx. 40 articles on FM\napplications for SSDM, focusing on large-language models (LLMs) and\nvision-language models (VLMs), and discussing their role in adaptive learning\n(AL), reinforcement learning (RL), and digital twin frameworks for targeted\nspraying. Key findings: (a) FMs are gaining traction with surging literature in\n2023-24; (b) VLMs outpace LLMs, with a 5-10x increase in publications; (c) RL\nand AL are still nascent for smart spraying; (d) digital twins with RL can\nsimulate targeted spraying virtually; (e) addressing the sim-to-real gap is\ncritical for real-world deployment; (f) human-robot collaboration remains\nlimited, especially in human-in-the-loop approaches where robots detect early\nsymptoms and humans validate uncertain cases; (g) multi-modal FMs with\nreal-time feedback will drive next-gen SSDM. For updates, resources, and\ncontributions, visit, https://github.com/nitin-dominic/AgriPathogenDatabase, to\nsubmit papers, code, or datasets.", "AI": {"tldr": "\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\uff0c\u7279\u522b\u662f\u57fa\u7840\u6a21\u578b\uff08FMs\uff09\uff0c\u6b63\u5728\u6539\u53d8\u4f5c\u7269\u75c5\u5bb3\u7684\u73b0\u573a\u7ba1\u7406\uff08SSDM\uff09\u3002FMs\u80fd\u591f\u6574\u5408\u89c6\u89c9\u548c\u6587\u672c\u6570\u636e\uff0c\u7406\u89e3\u75c5\u75c7\uff0c\u5e76\u63a8\u7406\u75c5\u75c7\u4e0e\u7ba1\u7406\u65b9\u6cd5\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u540c\u65f6\u652f\u6301\u4ea4\u4e92\u5f0f\u95ee\u7b54\u3002\u7ed3\u5408\u673a\u5668\u4eba\u6280\u672f\u4e2d\u7684\u81ea\u9002\u5e94\u548c\u6a21\u4eff\u5b66\u4e60\uff0c\u53ef\u4ee5\u5b9e\u73b0\u7530\u95f4\u7684\u75c5\u5bb3\u7ba1\u7406\u3002", "motivation": "\u672c\u7efc\u8ff0\u65e8\u5728\u63a2\u8ba8\u57fa\u7840\u6a21\u578b\uff08FMs\uff09\uff0c\u7279\u522b\u662f\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\uff0c\u5728\u4f5c\u7269\u73b0\u573a\u7279\u5b9a\u75c5\u5bb3\u7ba1\u7406\uff08SSDM\uff09\u4e2d\u7684\u5e94\u7528\uff0c\u91cd\u70b9\u5173\u6ce8\u5b83\u4eec\u5728\u81ea\u9002\u5e94\u5b66\u4e60\uff08AL\uff09\u3001\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u548c\u6570\u5b57\u5b6a\u751f\u6846\u67b6\u4e2d\u5b9e\u73b0\u7cbe\u51c6\u55b7\u6d12\u7684\u4f5c\u7528\u3002", "method": "\u901a\u8fc7\u7b5b\u9009\u5927\u7ea640\u7bc7\u5173\u4e8eFMs\u5728SSDM\u4e2d\u5e94\u7528\u7684\u6587\u732e\uff0c\u91cd\u70b9\u5206\u6790\u4e86LLMs\u548cVLMs\u5728AL\u3001RL\u4ee5\u53ca\u6570\u5b57\u5b6a\u751f\u6846\u67b6\u4e2d\u7684\u4f5c\u7528\uff0c\u5e76\u8ba8\u8bba\u4e86\u5176\u5728\u7cbe\u51c6\u55b7\u6d12\u65b9\u9762\u7684\u6f5c\u529b\u3002", "result": "(a) FMs\u5728SSDM\u9886\u57df\u7684\u5e94\u7528\u6b63\u8fc5\u901f\u589e\u957f\uff0c\u5c24\u5176\u57282023-2024\u5e74\u6587\u732e\u6fc0\u589e\uff1b(b) VLMs\u7684\u53d1\u5c55\u901f\u5ea6\u8fdc\u8d85LLMs\uff0c\u51fa\u7248\u7269\u6570\u91cf\u589e\u52a0\u4e865-10\u500d\uff1b(c) RL\u548cAL\u5728\u667a\u80fd\u55b7\u6d12\u9886\u57df\u7684\u5e94\u7528\u4ecd\u5904\u4e8e\u521d\u7ea7\u9636\u6bb5\uff1b(d) \u57fa\u4e8eRL\u7684\u6570\u5b57\u5b6a\u751f\u6280\u672f\u80fd\u591f\u6a21\u62df\u7cbe\u51c6\u55b7\u6d12\uff1b(e) \u5f25\u5408\u4eff\u771f\u4e0e\u73b0\u5b9e\u4e4b\u95f4\u7684\u5dee\u8ddd\u662f\u5b9e\u9645\u90e8\u7f72\u7684\u5173\u952e\uff1b(f) \u4eba\u673a\u534f\u4f5c\uff0c\u7279\u522b\u662f\u5728\u673a\u5668\u4eba\u68c0\u6d4b\u65e9\u671f\u75c5\u75c7\u3001\u4eba\u7c7b\u9a8c\u8bc1\u4e0d\u786e\u5b9a\u75c5\u4f8b\u7684\u201c\u4eba\u673a\u5728\u73af\u201d\u65b9\u6cd5\u4e2d\uff0c\u4ecd\u6709\u5f85\u52a0\u5f3a\uff1b(g) \u5177\u5907\u5b9e\u65f6\u53cd\u9988\u7684\u591a\u6a21\u6001FMs\u5c06\u63a8\u52a8\u4e0b\u4e00\u4ee3SSDM\u7684\u53d1\u5c55\u3002", "conclusion": "\u57fa\u7840\u6a21\u578b\uff08FMs\uff09\u4e3a\u4f5c\u7269\u75c5\u5bb3\u7684\u73b0\u573a\u7ba1\u7406\uff08SSDM\uff09\u5e26\u6765\u4e86\u9769\u547d\u6027\u7684\u53d8\u5316\uff0c\u5c24\u5176\u662f\u5728\u6574\u5408\u591a\u6a21\u6001\u6570\u636e\u3001\u5b9e\u73b0\u667a\u80fd\u51b3\u7b56\u548c\u7cbe\u51c6\u5e72\u9884\u65b9\u9762\u3002\u5c3d\u7ba1\u4ecd\u9762\u4e34\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u5dee\u8ddd\u4ee5\u53ca\u4eba\u673a\u534f\u4f5c\u7b49\u6311\u6218\uff0c\u4f46FMs\uff0c\u7279\u522b\u662fVLMs\uff0c\u7ed3\u5408\u673a\u5668\u4eba\u548c\u6570\u5b57\u5b6a\u751f\u6280\u672f\uff0c\u9884\u793a\u7740SSDM\u9886\u57df\u66f4\u52a0\u667a\u80fd\u5316\u548c\u81ea\u52a8\u5316\u7684\u672a\u6765\u3002"}}
{"id": "2510.24427", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24427", "abs": "https://arxiv.org/abs/2510.24427", "authors": ["Ken Gu", "Advait Bhat", "Mike A Merrill", "Robert West", "Xin Liu", "Daniel McDuff", "Tim Althoff"], "title": "SynthWorlds: Controlled Parallel Worlds for Disentangling Reasoning and Knowledge in Language Models", "comment": null, "summary": "Evaluating the reasoning ability of language models (LMs) is complicated by\ntheir extensive parametric world knowledge, where benchmark performance often\nreflects factual recall rather than genuine reasoning. Existing datasets and\napproaches (e.g., temporal filtering, paraphrasing, adversarial substitution)\ncannot cleanly separate the two. We present SynthWorlds, a framework that\ndisentangles task reasoning complexity from factual knowledge. In SynthWorlds,\nwe construct parallel corpora representing two worlds with identical\ninterconnected structure: a real-mapped world, where models may exploit\nparametric knowledge, and a synthetic-mapped world, where such knowledge is\nmeaningless. On top of these corpora, we design two mirrored tasks as case\nstudies: multi-hop question answering and page navigation, which maintain equal\nreasoning difficulty across worlds. Experiments in parametric-only (e.g.,\nclosed-book QA) and knowledge-augmented (e.g., retrieval-augmented) LM settings\nreveal a persistent knowledge advantage gap, defined as the performance boost\nmodels gain from memorized parametric world knowledge. Knowledge acquisition\nand integration mechanisms reduce but do not eliminate this gap, highlighting\nopportunities for system improvements. Fully automatic and scalable,\nSynthWorlds provides a controlled environment for evaluating LMs in ways that\nwere previously challenging, enabling precise and testable comparisons of\nreasoning and memorization.", "AI": {"tldr": "SynthWorlds\u6846\u67b6\u901a\u8fc7\u521b\u5efa\u771f\u5b9e\u4e16\u754c\u548c\u5408\u6210\u4e16\u754c\u4e2d\u7684\u5e73\u884c\u8bed\u6599\u5e93\uff0c\u5206\u79bb\u4e86\u8bed\u8a00\u6a21\u578b\uff08LM\uff09\u7684\u63a8\u7406\u590d\u6742\u6027\u548c\u4e8b\u5b9e\u77e5\u8bc6\uff0c\u4ece\u800c\u89e3\u51b3\u4e86\u73b0\u6709\u6570\u636e\u96c6\u65e0\u6cd5\u533a\u5206\u4e24\u8005\u7684\u95ee\u9898\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5373\u4f7f\u5728\u77e5\u8bc6\u83b7\u53d6\u548c\u6574\u5408\u673a\u5236\u7684\u5e2e\u52a9\u4e0b\uff0cLM\u5728\u771f\u5b9e\u4e16\u754c\uff08\u53ef\u5229\u7528\u5176\u53c2\u6570\u77e5\u8bc6\uff09\u7684\u8868\u73b0\u4e5f\u6301\u7eed\u4f18\u4e8e\u5408\u6210\u4e16\u754c\uff0c\u7a81\u663e\u4e86\u5176\u77e5\u8bc6\u4f18\u52bf\uff0c\u5e76\u4e3a\u8bc4\u4f30LM\u7684\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u63a7\u4e14\u53ef\u6269\u5c55\u7684\u73af\u5883\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\uff08LM\uff09\u63a8\u7406\u80fd\u529b\u7684\u65b9\u6cd5\u53d7\u9650\u4e8e\u5176\u5e9e\u5927\u7684\u4e16\u754c\u77e5\u8bc6\uff0c\u5bfc\u81f4\u57fa\u51c6\u6d4b\u8bd5\u8868\u73b0\u66f4\u50cf\u662f\u4e8b\u5b9e\u56de\u5fc6\u800c\u975e\u771f\u5b9e\u63a8\u7406\u3002\u73b0\u6709\u7684\u6570\u636e\u96c6\u548c\u65b9\u6cd5\uff08\u4f8b\u5982\u65f6\u95f4\u8fc7\u6ee4\u3001\u91ca\u4e49\u3001\u5bf9\u6297\u6027\u66ff\u6362\uff09\u65e0\u6cd5\u6e05\u6670\u5730\u533a\u5206\u8fd9\u4e24\u8005\u3002", "method": "\u63d0\u51faSynthWorlds\u6846\u67b6\uff0c\u521b\u5efa\u5305\u542b\u771f\u5b9e\u6620\u5c04\u4e16\u754c\u548c\u5408\u6210\u6620\u5c04\u4e16\u754c\u7684\u5e73\u884c\u8bed\u6599\u5e93\uff0c\u8fd9\u4e24\u4e2a\u4e16\u754c\u5177\u6709\u76f8\u540c\u7684\u76f8\u4e92\u5173\u8054\u7684\u7ed3\u6784\u3002\u5728\u8fd9\u4e9b\u8bed\u6599\u5e93\u4e0a\u8bbe\u8ba1\u4e86\u591a\u8df3\u95ee\u7b54\u548c\u9875\u9762\u5bfc\u822a\u4e24\u4e2a\u955c\u50cf\u4efb\u52a1\uff0c\u4f7f\u4e24\u4e2a\u4e16\u754c\u4e2d\u7684\u63a8\u7406\u96be\u5ea6\u76f8\u540c\u3002\u5728\u4ec5\u53c2\u6570\uff08\u5982\u95ed\u5377\u95ee\u7b54\uff09\u548c\u77e5\u8bc6\u589e\u5f3a\uff08\u5982\u68c0\u7d22\u589e\u5f3a\uff09\u7684LM\u8bbe\u7f6e\u4e2d\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u63ed\u793a\u4e86\u6301\u7eed\u5b58\u5728\u7684\u77e5\u8bc6\u4f18\u52bf\u5dee\u8ddd\uff0c\u5373\u6a21\u578b\u4ece\u8bb0\u5fc6\u53c2\u6570\u4e16\u754c\u77e5\u8bc6\u4e2d\u83b7\u5f97\u7684\u6027\u80fd\u63d0\u5347\u3002\u77e5\u8bc6\u83b7\u53d6\u548c\u6574\u5408\u673a\u5236\u53ef\u4ee5\u51cf\u5c11\u4f46\u4e0d\u80fd\u5b8c\u5168\u6d88\u9664\u8fd9\u79cd\u5dee\u8ddd\uff0c\u8fd9\u8868\u660e\u7cfb\u7edf\u6709\u6539\u8fdb\u7684\u7a7a\u95f4\u3002", "conclusion": "SynthWorlds\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5168\u81ea\u52a8\u4e14\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30LM\u7684\u63a8\u7406\u80fd\u529b\uff0c\u80fd\u591f\u7cbe\u786e\u4e14\u53ef\u6d4b\u8bd5\u5730\u6bd4\u8f83\u63a8\u7406\u548c\u8bb0\u5fc6\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u4ee5\u5f80\u8bc4\u4f30LM\u63a8\u7406\u80fd\u529b\u7684\u6311\u6218\u3002"}}
{"id": "2510.24378", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24378", "abs": "https://arxiv.org/abs/2510.24378", "authors": ["Yann Kerverdo", "Florent Leray", "Youwan Mah\u00e9", "St\u00e9phanie Leplaideur", "Francesca Galassi"], "title": "Stroke Lesion Segmentation in Clinical Workflows: A Modular, Lightweight, and Deployment-Ready Tool", "comment": null, "summary": "Deep learning frameworks such as nnU-Net achieve state-of-the-art performance\nin brain lesion segmentation but remain difficult to deploy clinically due to\nheavy dependencies and monolithic design. We introduce \\textit{StrokeSeg}, a\nmodular and lightweight framework that translates research-grade stroke lesion\nsegmentation models into deployable applications. Preprocessing, inference, and\npostprocessing are decoupled: preprocessing relies on the Anima toolbox with\nBIDS-compliant outputs, and inference uses ONNX Runtime with \\texttt{Float16}\nquantisation, reducing model size by about 50\\%. \\textit{StrokeSeg} provides\nboth graphical and command-line interfaces and is distributed as Python scripts\nand as a standalone Windows executable. On a held-out set of 300 sub-acute and\nchronic stroke subjects, segmentation performance was equivalent to the\noriginal PyTorch pipeline (Dice difference $<10^{-3}$), demonstrating that\nhigh-performing research pipelines can be transformed into portable, clinically\nusable tools.", "AI": {"tldr": "StrokeSeg\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u5c06\u7814\u7a76\u7ea7\u8111\u5352\u4e2d\u5206\u5272\u6a21\u578b\u8f6c\u5316\u4e3a\u53ef\u90e8\u7f72\u7684\u4e34\u5e8a\u5e94\u7528\uff0c\u5176\u6027\u80fd\u4e0e\u539f\u59cb\u6a21\u578b\u76f8\u5f53\uff0c\u5e76\u63d0\u4f9b\u4e86\u6613\u4e8e\u4f7f\u7528\u7684\u63a5\u53e3\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff08\u5982nnU-Net\uff09\u5728\u4e34\u5e8a\u90e8\u7f72\u4e2d\u5b58\u5728\u7684\u4f9d\u8d56\u6027\u91cd\u3001\u8bbe\u8ba1\u5355\u4e00\u5316\u7684\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u4e00\u4e2a\u6a21\u5757\u5316\u3001\u8f7b\u91cf\u7ea7\u7684\u6846\u67b6\u3002", "method": "StrokeSeg\u6846\u67b6\u5c06\u9884\u5904\u7406\u3001\u63a8\u7406\u548c\u540e\u5904\u7406\u89e3\u8026\u3002\u9884\u5904\u7406\u4f7f\u7528Anima\u5de5\u5177\u7bb1\u5e76\u751f\u6210\u7b26\u5408BIDS\u6807\u51c6\u7684\u6570\u636e\u3002\u63a8\u7406\u4f7f\u7528ONNX Runtime\u548cFloat16\u91cf\u5316\u6280\u672f\uff0c\u5c06\u6a21\u578b\u5927\u5c0f\u51cf\u5c11\u7ea650%\u3002\u8be5\u6846\u67b6\u63d0\u4f9b\u56fe\u5f62\u548c\u547d\u4ee4\u884c\u754c\u9762\uff0c\u5e76\u53ef\u4f5c\u4e3aPython\u811a\u672c\u6216\u72ec\u7acb\u7684Windows\u53ef\u6267\u884c\u6587\u4ef6\u5206\u53d1\u3002", "result": "\u5728300\u540d\u53d7\u8bd5\u8005\u7684\u72ec\u7acb\u6d4b\u8bd5\u96c6\u4e2d\uff0cStrokeSeg\u7684\u5206\u5272\u6027\u80fd\u4e0e\u539f\u59cbPyTorch\u6d41\u7a0b\u76f8\u5f53\uff08Dice\u7cfb\u6570\u5dee\u5f02<10^{-3}\uff09\u3002", "conclusion": "\u9ad8\u6027\u80fd\u7684\u7814\u7a76\u7ea7\u8111\u5352\u4e2d\u5206\u5272\u6d41\u7a0b\u53ef\u4ee5\u88ab\u8f6c\u5316\u4e3a\u4fbf\u643a\u3001\u53ef\u4e34\u5e8a\u4f7f\u7528\u7684\u5de5\u5177\u3002"}}
{"id": "2510.23868", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23868", "abs": "https://arxiv.org/abs/2510.23868", "authors": ["Zhichao Wang"], "title": "GIFT: Group-relative Implicit Fine Tuning Integrates GRPO with DPO and UNA", "comment": null, "summary": "I propose \\textbf{G}roup-relative \\textbf{I}mplicit \\textbf{F}ine\n\\textbf{T}uning (GIFT), a novel reinforcement learning framework for aligning\nLLMs. Instead of directly maximizing cumulative rewards like PPO or GRPO, GIFT\nminimizes the discrepancy between implicit and explicit reward models. It\ncombines three key ideas: (1) the online multi-response generation and\nnormalization of GRPO, (2) the implicit reward formulation of DPO, and (3) the\nimplicit-explicit reward alignment principle of UNA. By jointly normalizing the\nimplicit and explicit rewards, GIFT eliminates an otherwise intractable term\nthat prevents effective use of implicit rewards. This normalization transforms\nthe complex reward maximization objective into a simple mean squared error\n(MSE) loss between the normalized reward functions, converting a non-convex\noptimization problem into a convex, stable, and analytically differentiable\nformulation. Unlike offline methods such as DPO and UNA, GIFT remains on-policy\nand thus retains exploration capability. Compared to GRPO, it requires fewer\nhyperparameters, converges faster, and generalizes better with significantly\nreduced training overfitting. Empirically, GIFT achieves superior reasoning and\nalignment performance on mathematical benchmarks while remaining\ncomputationally efficient.", "AI": {"tldr": "GIFT\u662f\u4e00\u79cd\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5bf9\u9f50\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u9690\u5f0f\u548c\u663e\u5f0f\u5956\u52b1\u6a21\u578b\u4e4b\u95f4\u7684\u5dee\u5f02\u6765\u5de5\u4f5c\uff0c\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5b83\u5177\u6709\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u3001\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u66f4\u9ad8\u7684\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u65b9\u6cd5\uff08\u5982PPO\u3001GRPO\u3001DPO\u3001UNA\uff09\u5b58\u5728\u4e00\u4e9b\u5c40\u9650\u6027\uff0c\u4f8b\u5982\u76f4\u63a5\u6700\u5927\u5316\u7d2f\u79ef\u5956\u52b1\u53ef\u80fd\u4e0d\u7a33\u5b9a\uff0c\u6216\u8005\u9700\u8981\u79bb\u7ebf\u8bad\u7ec3\uff0c\u9650\u5236\u4e86\u63a2\u7d22\u80fd\u529b\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6GIFT\uff0c\u65e8\u5728\u514b\u670d\u8fd9\u4e9b\u5c40\u9650\u6027\u3002", "method": "GIFT\u6846\u67b6\u7ed3\u5408\u4e86GRPO\u7684\u5728\u7ebf\u591a\u54cd\u5e94\u751f\u6210\u548c\u5f52\u4e00\u5316\u3001DPO\u7684\u9690\u5f0f\u5956\u52b1\u6a21\u578b\u4ee5\u53caUNA\u7684\u9690\u5f0f-\u663e\u5f0f\u5956\u52b1\u5bf9\u9f50\u539f\u5219\u3002\u901a\u8fc7\u8054\u5408\u5f52\u4e00\u5316\u9690\u5f0f\u548c\u663e\u5f0f\u5956\u52b1\uff0c\u6d88\u9664\u4e86\u4e00\u4e2a\u68d8\u624b\u7684\u9879\uff0c\u4ece\u800c\u53ef\u4ee5\u5c06\u590d\u6742\u7684\u5956\u52b1\u6700\u5927\u5316\u76ee\u6807\u8f6c\u5316\u4e3a\u4e00\u4e2a\u7b80\u5355\u7684\u5747\u65b9\u8bef\u5dee\uff08MSE\uff09\u635f\u5931\uff0c\u5c06\u975e\u51f8\u4f18\u5316\u95ee\u9898\u8f6c\u5316\u4e3a\u51f8\u7684\u3001\u7a33\u5b9a\u7684\u3001\u53ef\u89e3\u6790\u53ef\u5fae\u7684\u3002", "result": "GIFT\u5728\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u63a8\u7406\u548c\u5bf9\u9f50\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8ba1\u7b97\u6548\u7387\u3002\u4e0eDPO\u548cUNA\u7b49\u79bb\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0cGIFT\u4fdd\u6301\u5728\u7ebf\u7b56\u7565\uff0c\u4fdd\u7559\u4e86\u63a2\u7d22\u80fd\u529b\u3002\u4e0eGRPO\u76f8\u6bd4\uff0cGIFT\u9700\u8981\u66f4\u5c11\u7684\u8d85\u53c2\u6570\uff0c\u6536\u655b\u66f4\u5feb\uff0c\u6cdb\u5316\u80fd\u529b\u66f4\u597d\uff0c\u5e76\u4e14\u663e\u8457\u51cf\u5c11\u4e86\u8bad\u7ec3\u8fc7\u62df\u5408\u3002", "conclusion": "GIFT\u662f\u4e00\u79cd\u6709\u6548\u7684\u3001\u8ba1\u7b97\u9ad8\u6548\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5bf9\u9f50\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5b83\u901a\u8fc7\u6700\u5c0f\u5316\u9690\u5f0f\u548c\u663e\u5f0f\u5956\u52b1\u6a21\u578b\u4e4b\u95f4\u7684\u5dee\u5f02\uff0c\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\u548c\u7a33\u5b9a\u6027\uff0c\u5e76\u4e14\u6bd4\u73b0\u6709\u65b9\u6cd5\u5177\u6709\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u66f4\u5c11\u7684\u8fc7\u62df\u5408\u3002"}}
{"id": "2510.24663", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24663", "abs": "https://arxiv.org/abs/2510.24663", "authors": ["Yifu Lu", "Shengjie Liu", "Li Dong"], "title": "OrchDAG: Complex Tool Orchestration in Multi-Turn Interactions with Plan DAGs", "comment": "9 pages, 4 figures", "summary": "Agentic tool use has gained traction with the rise of agentic tool calling,\nyet most existing work overlooks the complexity of multi-turn tool\ninteractions. We introduce OrchDAG, a synthetic data generation pipeline that\nmodels tool execution as directed acyclic graphs (DAGs) with controllable\ncomplexity. Using this dataset, we benchmark model performance and propose a\ngraph-based reward to enhance RLVR training. Experiments show that the dataset\npresents a challenging but solvable benchmark, and the proposed reward is\neffective when combined with GRPO-style algorithms, highlighting the importance\nof leveraging topological structure and data complexity in multi-turn tool use.", "AI": {"tldr": "OrchDAG\u662f\u4e00\u4e2a\u7528\u4e8e\u751f\u6210\u591a\u8f6e\u5de5\u5177\u4ea4\u4e92\u6570\u636e\u7684\u5408\u6210\u6570\u636e\u751f\u6210\u7ba1\u9053\uff0c\u901a\u8fc7\u5c06\u5de5\u5177\u6267\u884c\u5efa\u6a21\u4e3a\u6709\u5411\u65e0\u73af\u56fe\uff08DAG\uff09\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u56fe\u7684\u5956\u52b1\u6765\u589e\u5f3aRLVR\u8bad\u7ec3\uff0c\u4ee5\u5e94\u5bf9\u73b0\u6709\u5de5\u4f5c\u4e2d\u5bf9\u591a\u8f6e\u5de5\u5177\u4ea4\u4e92\u590d\u6742\u6027\u5173\u6ce8\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684Agentic tool use\u5de5\u4f5c\u5ffd\u7565\u4e86\u591a\u8f6e\u5de5\u5177\u4ea4\u4e92\u7684\u590d\u6742\u6027\u3002", "method": "\u63d0\u51faOrchDAG\uff0c\u4e00\u4e2a\u5408\u6210\u6570\u636e\u751f\u6210\u7ba1\u9053\uff0c\u5c06\u5de5\u5177\u6267\u884c\u5efa\u6a21\u4e3a\u5177\u6709\u53ef\u63a7\u590d\u6742\u6027\u7684\u6709\u5411\u65e0\u73af\u56fe\uff08DAG\uff09\u3002\u4f7f\u7528\u8be5\u6570\u636e\u96c6\u5bf9\u6a21\u578b\u6027\u80fd\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u56fe\u7684\u5956\u52b1\u6765\u589e\u5f3aRLVR\u8bad\u7ec3\u3002", "result": "\u8be5\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u4f46\u53ef\u89e3\u51b3\u7684\u57fa\u51c6\uff0c\u5e76\u4e14\u6240\u63d0\u51fa\u7684\u5956\u52b1\u4e0eGRPO\u98ce\u683c\u7684\u7b97\u6cd5\u7ed3\u5408\u4f7f\u7528\u65f6\u662f\u6709\u6548\u7684\u3002", "conclusion": "\u5229\u7528\u62d3\u6251\u7ed3\u6784\u548c\u6570\u636e\u590d\u6742\u6027\u5bf9\u4e8e\u591a\u8f6e\u5de5\u5177\u4f7f\u7528\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2510.24434", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24434", "abs": "https://arxiv.org/abs/2510.24434", "authors": ["Julian Valline", "Cedric Lothritz", "Jordi Cabot"], "title": "LuxIT: A Luxembourgish Instruction Tuning Dataset from Monolingual Seed Data", "comment": null, "summary": "The effectiveness of instruction-tuned Large Language Models (LLMs) is often\nlimited in low-resource linguistic settings due to a lack of high-quality\ntraining data. We introduce LuxIT, a novel, monolingual instruction tuning\ndataset for Luxembourgish developed to mitigate this challenge. We synthesize\nthe dataset from a corpus of native Luxembourgish texts, utilizing\nDeepSeek-R1-0528, chosen for its shown proficiency in Luxembourgish. Following\ngeneration, we apply a quality assurance process, employing an LLM-as-a-judge\napproach. To investigate the practical utility of the dataset, we fine-tune\nseveral smaller-scale LLMs on LuxIT. Subsequent benchmarking against their base\nmodels on Luxembourgish language proficiency examinations, however, yields\nmixed results, with performance varying significantly across different models.\nLuxIT represents a critical contribution to Luxembourgish natural language\nprocessing and offers a replicable monolingual methodology, though our findings\nhighlight the need for further research to optimize its application.", "AI": {"tldr": "LuxIT\u662f\u4e00\u4e2a\u4e3a\u5362\u68ee\u5821\u8bed\u8bbe\u8ba1\u7684\u5355\u8bed\u6307\u4ee4\u8c03\u4f18\u6570\u636e\u96c6\uff0c\u65e8\u5728\u89e3\u51b3\u4f4e\u8d44\u6e90\u8bed\u8a00\u73af\u5883\u4e0bLLM\u7684\u8bad\u7ec3\u6570\u636e\u4e0d\u8db3\u95ee\u9898\u3002\u8be5\u6570\u636e\u96c6\u901a\u8fc7\u5408\u6210\u5362\u68ee\u5821\u8bed\u6587\u672c\u5e76\u4f7f\u7528LLM\u4f5c\u4e3a\u8bc4\u5224\u8005\u8fdb\u884c\u8d28\u91cf\u4fdd\u8bc1\u800c\u521b\u5efa\u3002\u7136\u800c\uff0c\u5728LuxIT\u4e0a\u5fae\u8c03\u7684LLM\u5728\u5362\u68ee\u5821\u8bed\u80fd\u529b\u6d4b\u8bd5\u4e2d\u7684\u8868\u73b0\u559c\u5fe7\u53c2\u534a\uff0c\u8868\u660e\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u6765\u4f18\u5316\u5176\u5e94\u7528\u3002", "motivation": "\u4f4e\u8d44\u6e90\u8bed\u8a00\u73af\u5883\u4e0b\uff0c\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u7684\u8bad\u7ec3\u6570\u636e\u9650\u5236\u4e86\u6307\u4ee4\u8c03\u4f18\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6709\u6548\u6027\u3002", "method": "\u4ece\u5362\u68ee\u5821\u8bed\u6587\u672c\u8bed\u6599\u5e93\u4e2d\u5408\u6210LuxIT\u6570\u636e\u96c6\uff0c\u5e76\u4f7f\u7528LLM\u4f5c\u4e3a\u8bc4\u5224\u8005\u8fdb\u884c\u8d28\u91cf\u4fdd\u8bc1\u3002\u7136\u540e\uff0c\u5728LuxIT\u4e0a\u5fae\u8c03\u51e0\u4e2a\u5c0f\u89c4\u6a21\u7684LLM\uff0c\u5e76\u5728\u5362\u68ee\u5821\u8bed\u8bed\u8a00\u80fd\u529b\u6d4b\u8bd5\u4e2d\u4e0e\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u5728LuxIT\u4e0a\u5fae\u8c03\u7684LLM\u5728\u5362\u68ee\u5821\u8bed\u80fd\u529b\u6d4b\u8bd5\u4e2d\u7684\u8868\u73b0\u559c\u5fe7\u53c2\u534a\uff0c\u6027\u80fd\u56e0\u6a21\u578b\u800c\u5f02\u3002", "conclusion": "LuxIT\u4e3a\u5362\u68ee\u5821\u8bed\u81ea\u7136\u8bed\u8a00\u5904\u7406\u505a\u51fa\u4e86\u91cd\u8981\u8d21\u732e\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u590d\u5236\u7684\u5355\u8bed\u65b9\u6cd5\u3002\u7136\u800c\uff0c\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u6765\u4f18\u5316\u5176\u5e94\u7528\u3002"}}
{"id": "2510.24379", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24379", "abs": "https://arxiv.org/abs/2510.24379", "authors": ["Zhuangfan Huang", "Xiaosong Li", "Gao Wang", "Tao Ye", "Haishu Tan", "Huafeng Li"], "title": "A Luminance-Aware Multi-Scale Network for Polarization Image Fusion with a Multi-Scene Dataset", "comment": null, "summary": "Polarization image fusion combines S0 and DOLP images to reveal surface\nroughness and material properties through complementary texture features, which\nhas important applications in camouflage recognition, tissue pathology\nanalysis, surface defect detection and other fields. To intergrate\ncoL-Splementary information from different polarized images in complex\nluminance environment, we propose a luminance-aware multi-scale network (MLSN).\nIn the encoder stage, we propose a multi-scale spatial weight matrix through a\nbrightness-branch , which dynamically weighted inject the luminance into the\nfeature maps, solving the problem of inherent contrast difference in polarized\nimages. The global-local feature fusion mechanism is designed at the bottleneck\nlayer to perform windowed self-attention computation, to balance the global\ncontext and local details through residual linking in the feature dimension\nrestructuring stage. In the decoder stage, to further improve the adaptability\nto complex lighting, we propose a Brightness-Enhancement module, establishing\nthe mapping relationship between luminance distribution and texture features,\nrealizing the nonlinear luminance correction of the fusion result. We also\npresent MSP, an 1000 pairs of polarized images that covers 17 types of indoor\nand outdoor complex lighting scenes. MSP provides four-direction polarization\nraw maps, solving the scarcity of high-quality datasets in polarization image\nfusion. Extensive experiment on MSP, PIF and GAND datasets verify that the\nproposed MLSN outperms the state-of-the-art methods in subjective and objective\nevaluations, and the MS-SSIM and SD metircs are higher than the average values\nof other methods by 8.57%, 60.64%, 10.26%, 63.53%, 22.21%, and 54.31%,\nrespectively. The source code and dataset is avalable at\nhttps://github.com/1hzf/MLS-UNet.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMLSN\uff08\u4eae\u5ea6\u611f\u77e5\u591a\u5c3a\u5ea6\u7f51\u7edc\uff09\u7684\u65b0\u578b\u504f\u632f\u56fe\u50cf\u878d\u5408\u65b9\u6cd5\uff0c\u5e76\u521b\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aMSP\u7684\u65b0\u6570\u636e\u96c6\uff0c\u4ee5\u63d0\u9ad8\u5728\u590d\u6742\u5149\u7167\u6761\u4ef6\u4e0b\u878d\u5408\u504f\u632f\u56fe\u50cf\u7684\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u5728\u590d\u6742\u7684\u4eae\u5ea6\u73af\u5883\u4e0b\u6574\u5408\u4e0d\u540c\u504f\u632f\u56fe\u50cf\u4e2d\u7684\u4e92\u8865\u4fe1\u606f\uff0c\u5e76\u89e3\u51b3\u504f\u632f\u56fe\u50cf\u672c\u8eab\u5b58\u5728\u7684\u5bf9\u6bd4\u5ea6\u5dee\u5f02\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4eae\u5ea6\u611f\u77e5\u591a\u5c3a\u5ea6\u7f51\u7edc\uff08MLSN\uff09\uff0c\u8be5\u7f51\u7edc\u5728\u7f16\u7801\u5668\u9636\u6bb5\u901a\u8fc7\u4eae\u5ea6\u5206\u652f\u5f15\u5165\u591a\u5c3a\u5ea6\u7a7a\u95f4\u6743\u91cd\u77e9\u9635\u6765\u52a8\u6001\u52a0\u6743\u4eae\u5ea6\u4fe1\u606f\uff0c\u5728\u74f6\u9888\u5c42\u8bbe\u8ba1\u4e86\u5168\u5c40-\u5c40\u90e8\u7279\u5f81\u878d\u5408\u673a\u5236\uff0c\u5728\u89e3\u7801\u5668\u9636\u6bb5\u5f15\u5165\u4e86\u4eae\u5ea6\u589e\u5f3a\u6a21\u5757\u4ee5\u8fdb\u884c\u975e\u7ebf\u6027\u4eae\u5ea6\u6821\u6b63\u3002", "result": "\u6240\u63d0\u51fa\u7684MLSN\u5728MSP\u3001PIF\u548cGAND\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u5176\u5728\u4e3b\u89c2\u548c\u5ba2\u89c2\u8bc4\u4f30\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u4e14\u5728MS-SSIM\u548cSD\u6307\u6807\u4e0a\u5206\u522b\u6bd4\u5176\u4ed6\u65b9\u6cd5\u7684\u5e73\u5747\u503c\u9ad8\u51fa8.57%\u300160.64%\u300110.26%\u300163.53%\u300122.21%\u548c54.31%\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684MLSN\u5728\u590d\u6742\u5149\u7167\u6761\u4ef6\u4e0b\u80fd\u591f\u6709\u6548\u5730\u878d\u5408\u504f\u632f\u56fe\u50cf\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2510.23879", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23879", "abs": "https://arxiv.org/abs/2510.23879", "authors": ["Ayse Irmak Ercevik", "Ahmet Murat Ozbayoglu"], "title": "Artificial Intelligence Based Predictive Maintenance for Electric Buses", "comment": null, "summary": "Predictive maintenance (PdM) is crucial for optimizing efficiency and\nminimizing downtime of electric buses. While these vehicles provide\nenvironmental benefits, they pose challenges for PdM due to complex electric\ntransmission and battery systems. Traditional maintenance, often based on\nscheduled inspections, struggles to capture anomalies in multi-dimensional\nreal-time CAN Bus data. This study employs a graph-based feature selection\nmethod to analyze relationships among CAN Bus parameters of electric buses and\ninvestigates the prediction performance of targeted alarms using artificial\nintelligence techniques. The raw data collected over two years underwent\nextensive preprocessing to ensure data quality and consistency. A hybrid\ngraph-based feature selection tool was developed by combining statistical\nfiltering (Pearson correlation, Cramer's V, ANOVA F-test) with\noptimization-based community detection algorithms (InfoMap, Leiden, Louvain,\nFast Greedy). Machine learning models, including SVM, Random Forest, and\nXGBoost, were optimized through grid and random search with data balancing via\nSMOTEEN and binary search-based down-sampling. Model interpretability was\nachieved using LIME to identify the features influencing predictions. The\nresults demonstrate that the developed system effectively predicts vehicle\nalarms, enhances feature interpretability, and supports proactive maintenance\nstrategies aligned with Industry 4.0 principles.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u56fe \uae30\ubc18\u7279\u5f81\u9009\u62e9\u65b9\u6cd5\u548c\u4eba\u5de5\u667a\u80fd\u6280\u672f\uff0c\u4e3a\u7535\u52a8\u516c\u4ea4\u8f66\u5f00\u53d1\u4e86\u4e00\u79cd\u9884\u6d4b\u6027\u7ef4\u62a4\u7cfb\u7edf\uff0c\u4ee5\u4f18\u5316\u6548\u7387\u5e76\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u505c\u673a\u65f6\u95f4\u3002", "motivation": "\u7535\u52a8\u516c\u4ea4\u8f66\u867d\u7136\u73af\u4fdd\uff0c\u4f46\u5176\u590d\u6742\u7684\u7535\u529b\u4f20\u8f93\u548c\u7535\u6c60\u7cfb\u7edf\u7ed9\u9884\u6d4b\u6027\u7ef4\u62a4\u5e26\u6765\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u591a\u7ef4\u5b9e\u65f6CAN\u603b\u7ebf\u6570\u636e\u4e2d\u7684\u5f02\u5e38\u3002", "method": "\u901a\u8fc7\u7ed3\u5408\u7edf\u8ba1\u8fc7\u6ee4\uff08\u76ae\u5c14\u900a\u76f8\u5173\u3001Cramer's V\u3001ANOVA F\u68c0\u9a8c\uff09\u548c\u57fa\u4e8e\u4f18\u5316\u7684\u793e\u533a\u68c0\u6d4b\u7b97\u6cd5\uff08InfoMap\u3001Leiden\u3001Louvain\u3001Fast Greedy\uff09\u5f00\u53d1\u4e86\u4e00\u4e2a\u6df7\u5408\u56fe \uae30\ubc18\u7279\u5f81\u9009\u62e9\u5de5\u5177\uff0c\u5e76\u4f7f\u7528SVM\u3001\u968f\u673a\u68ee\u6797\u548cXGBoost\u7b49\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u9884\u6d4b\uff0c\u5229\u7528LIME\u5b9e\u73b0\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u8be5\u7cfb\u7edf\u80fd\u6709\u6548\u9884\u6d4b\u8f66\u8f86\u8b66\u62a5\uff0c\u589e\u5f3a\u7279\u5f81\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u7cfb\u7edf\u652f\u6301\u4e0e\u5de5\u4e1a4.0\u539f\u5219\u76f8\u4e00\u81f4\u7684\u4e3b\u52a8\u7ef4\u62a4\u7b56\u7565\u3002"}}
{"id": "2510.24690", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24690", "abs": "https://arxiv.org/abs/2510.24690", "authors": ["Shengjie Liu", "Li Dong", "Zhenyu Zhang"], "title": "Bridging Tool Dependencies and Domain Knowledge: A Graph-Based Framework for In-Context Planning", "comment": "4 pages, 2 figures, short paper, NeurIPS 2025 workshop on Bridging\n  Language, Agent, and World Models for Reasoning and Planning", "summary": "We present a framework for uncovering and exploiting dependencies among tools\nand documents to enhance exemplar artifact generation. Our method begins by\nconstructing a tool knowledge graph from tool schemas,including descriptions,\narguments, and output payloads, using a DeepResearch-inspired analysis. In\nparallel, we derive a complementary knowledge graph from internal documents and\nSOPs, which is then fused with the tool graph. To generate exemplar plans, we\nadopt a deep-sparse integration strategy that aligns structural tool\ndependencies with procedural knowledge. Experiments demonstrate that this\nunified framework effectively models tool interactions and improves plan\ngeneration, underscoring the benefits of linking tool graphs with domain\nknowledge graphs for tool-augmented reasoning and planning.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u6846\u67b6\uff0c\u901a\u8fc7\u6316\u6398\u5de5\u5177\u548c\u6587\u6863\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\u6765\u6539\u8fdb\u793a\u4f8b\u6027\u4ea7\u51fa\u751f\u6210\u3002", "motivation": "\u4e3a\u4e86\u589e\u5f3a\u793a\u4f8b\u6027\u4ea7\u51fa\u751f\u6210\uff0c\u9700\u8981\u66f4\u597d\u5730\u7406\u89e3\u548c\u5229\u7528\u5de5\u5177\u53ca\u5176\u6587\u6863\u4e4b\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\u3002", "method": "1. \u4ece\u5de5\u5177\u6a21\u5f0f\uff08\u63cf\u8ff0\u3001\u53c2\u6570\u3001\u8f93\u51fa\uff09\u6784\u5efa\u5de5\u5177\u77e5\u8bc6\u56fe\u8c31\u30022. \u4ece\u5185\u90e8\u6587\u6863\u548cSOP\u4e2d\u63d0\u53d6\u77e5\u8bc6\uff0c\u6784\u5efa\u6587\u6863\u77e5\u8bc6\u56fe\u8c31\u30023. \u878d\u5408\u5de5\u5177\u77e5\u8bc6\u56fe\u8c31\u548c\u6587\u6863\u77e5\u8bc6\u56fe\u8c31\u30024. \u91c7\u7528\u6df1\u5ea6-\u7a00\u758f\u96c6\u6210\u7b56\u7565\u751f\u6210\u793a\u4f8b\u6027\u8ba1\u5212\uff0c\u8be5\u7b56\u7565\u6574\u5408\u4e86\u7ed3\u6784\u5316\u5de5\u5177\u4f9d\u8d56\u548c\u7a0b\u5e8f\u5316\u77e5\u8bc6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u80fd\u6709\u6548\u6a21\u62df\u5de5\u5177\u4ea4\u4e92\uff0c\u5e76\u63d0\u5347\u8ba1\u5212\u751f\u6210\u6548\u679c\u3002", "conclusion": "\u5c06\u5de5\u5177\u77e5\u8bc6\u56fe\u8c31\u4e0e\u9886\u57df\u77e5\u8bc6\u56fe\u8c31\u76f8\u7ed3\u5408\uff0c\u5bf9\u4e8e\u5de5\u5177\u589e\u5f3a\u7684\u63a8\u7406\u548c\u89c4\u5212\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2510.24385", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24385", "abs": "https://arxiv.org/abs/2510.24385", "authors": ["Herman Bergstr\u00f6m", "Zhongqi Yue", "Fredrik D. Johansson"], "title": "When are radiology reports useful for training medical image classifiers?", "comment": null, "summary": "Medical images used to train machine learning models are often accompanied by\nradiology reports containing rich expert annotations. However, relying on these\nreports as inputs for clinical prediction requires the timely manual work of a\ntrained radiologist. This raises a natural question: when can radiology reports\nbe leveraged during training to improve image-only classification? Prior works\nare limited to evaluating pre-trained image representations by fine-tuning them\nto predict diagnostic labels, often extracted from reports, ignoring tasks with\nlabels that are weakly associated with the text. To address this gap, we\nconduct a systematic study of how radiology reports can be used during both\npre-training and fine-tuning, across diagnostic and prognostic tasks (e.g.,\n12-month readmission), and under varying training set sizes. Our findings\nreveal that: (1) Leveraging reports during pre-training is beneficial for\ndownstream classification tasks where the label is well-represented in the\ntext; however, pre-training through explicit image-text alignment can be\ndetrimental in settings where it's not; (2) Fine-tuning with reports can lead\nto significant improvements and even have a larger impact than the pre-training\nmethod in certain settings. These results provide actionable insights into when\nand how to leverage privileged text data to train medical image classifiers\nwhile highlighting gaps in current research.", "AI": {"tldr": "\u5728\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u5229\u7528\u653e\u5c04\u5b66\u62a5\u544a\u8fdb\u884c\u9884\u8bad\u7ec3\u6216\u5fae\u8c03\u53ef\u4ee5\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\uff0c\u4f46\u9700\u6839\u636e\u6807\u7b7e\u4e0e\u6587\u672c\u7684\u5173\u8054\u7a0b\u5ea6\u548c\u4efb\u52a1\u7c7b\u578b\u8fdb\u884c\u6743\u8861\u3002", "motivation": "\u73b0\u6709\u7684\u533b\u5b66\u56fe\u50cf\u6a21\u578b\u8bad\u7ec3\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u56fe\u50cf\u6570\u636e\uff0c\u5ffd\u7565\u4e86\u653e\u5c04\u5b66\u62a5\u544a\u4e2d\u8574\u542b\u7684\u4e30\u5bcc\u4e13\u5bb6\u6ce8\u91ca\u4fe1\u606f\u3002\u672c\u7814\u7a76\u65e8\u5728\u7cfb\u7edf\u5730\u63a2\u8ba8\u5728\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u9636\u6bb5\uff0c\u5982\u4f55\u5229\u7528\u653e\u5c04\u5b66\u62a5\u544a\u6765\u63d0\u5347\u4ec5\u5305\u542b\u56fe\u50cf\u8f93\u5165\u7684\u5206\u7c7b\u6a21\u578b\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u6807\u7b7e\u4e0e\u6587\u672c\u5173\u8054\u5ea6\u4e0d\u9ad8\u7684\u60c5\u5883\u4e0b\u3002", "method": "\u672c\u7814\u7a76\u7cfb\u7edf\u5730\u7814\u7a76\u4e86\u5728\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u4e24\u4e2a\u9636\u6bb5\uff0c\u5229\u7528\u653e\u5c04\u5b66\u62a5\u544a\u6765\u8f85\u52a9\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u3002\u7814\u7a76\u6db5\u76d6\u4e86\u8bca\u65ad\u548c\u9884\u540e\u4efb\u52a1\uff08\u598212\u4e2a\u6708\u518d\u5165\u9662\u7387\u9884\u6d4b\uff09\uff0c\u5e76\u8003\u8651\u4e86\u4e0d\u540c\u8bad\u7ec3\u6570\u636e\u96c6\u5927\u5c0f\u7684\u5f71\u54cd\u3002\u5177\u4f53\u800c\u8a00\uff0c\u7814\u7a76\u8bc4\u4f30\u4e86\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\u8fdb\u884c\u663e\u5f0f\u56fe\u50cf-\u6587\u672c\u5bf9\u9f50\uff08image-text alignment\uff09\u7684\u6709\u6548\u6027\uff0c\u4ee5\u53ca\u5728\u5fae\u8c03\u9636\u6bb5\u5229\u7528\u62a5\u544a\u4fe1\u606f\u8fdb\u884c\u6a21\u578b\u8c03\u6574\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff1a\uff081\uff09\u5f53\u6807\u7b7e\u4fe1\u606f\u5728\u6587\u672c\u4e2d\u5145\u5206\u4f53\u73b0\u65f6\uff0c\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\u5229\u7528\u62a5\u544a\u80fd\u591f\u63d0\u5347\u4e0b\u6e38\u5206\u7c7b\u4efb\u52a1\u7684\u6027\u80fd\uff1b\u7136\u800c\uff0c\u5728\u6807\u7b7e\u4e0e\u6587\u672c\u5173\u8054\u5ea6\u4e0d\u9ad8\u7684\u60c5\u5883\u4e0b\uff0c\u663e\u5f0f\u7684\u56fe\u50cf-\u6587\u672c\u5bf9\u9f50\u9884\u8bad\u7ec3\u53cd\u800c\u53ef\u80fd\u5e26\u6765\u8d1f\u9762\u5f71\u54cd\u3002\uff082\uff09\u5728\u5fae\u8c03\u9636\u6bb5\u5229\u7528\u62a5\u544a\u4fe1\u606f\u80fd\u591f\u5e26\u6765\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5176\u6548\u679c\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u751a\u81f3\u4f18\u4e8e\u9884\u8bad\u7ec3\u65b9\u6cd5\u7684\u6539\u8fdb\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u5982\u4f55\u5728\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u6709\u6548\u5229\u7528\u653e\u5c04\u5b66\u62a5\u544a\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\uff0c\u660e\u786e\u4e86\u4f55\u65f6\u4ee5\u53ca\u5982\u4f55\u5229\u7528\u6587\u672c\u6570\u636e\u6765\u8bad\u7ec3\u6a21\u578b\uff0c\u540c\u65f6\u4e5f\u6307\u51fa\u4e86\u5f53\u524d\u7814\u7a76\u4e2d\u5b58\u5728\u7684\u4e0d\u8db3\u4e4b\u5904\u3002"}}
{"id": "2510.23901", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23901", "abs": "https://arxiv.org/abs/2510.23901", "authors": ["Cristobal Heredia", "Pedro Chumpitaz-Flores", "Kaixun Hua"], "title": "RS-ORT: A Reduced-Space Branch-and-Bound Algorithm for Optimal Regression Trees", "comment": "20 pages, 1 figure, uses ICLR 2026 LaTeX style. Submitted to arXiv as\n  a preprint version", "summary": "Mixed-integer programming (MIP) has emerged as a powerful framework for\nlearning optimal decision trees. Yet, existing MIP approaches for regression\ntasks are either limited to purely binary features or become computationally\nintractable when continuous, large-scale data are involved. Naively binarizing\ncontinuous features sacrifices global optimality and often yields needlessly\ndeep trees. We recast the optimal regression-tree training as a two-stage\noptimization problem and propose Reduced-Space Optimal Regression Trees\n(RS-ORT) - a specialized branch-and-bound (BB) algorithm that branches\nexclusively on tree-structural variables. This design guarantees the\nalgorithm's convergence and its independence from the number of training\nsamples. Leveraging the model's structure, we introduce several bound\ntightening techniques - closed-form leaf prediction, empirical threshold\ndiscretization, and exact depth-1 subtree parsing - that combine with\ndecomposable upper and lower bounding strategies to accelerate the training.\nThe BB node-wise decomposition enables trivial parallel execution, further\nalleviating the computational intractability even for million-size datasets.\nBased on the empirical studies on several regression benchmarks containing both\nbinary and continuous features, RS-ORT also delivers superior training and\ntesting performance than state-of-the-art methods. Notably, on datasets with up\nto 2,000,000 samples with continuous features, RS-ORT can obtain guaranteed\ntraining performance with a simpler tree structure and a better generalization\nability in four hours.", "AI": {"tldr": "\u4f7f\u7528\u5206\u652f\u5b9a\u754c\u7b97\u6cd5\u4f18\u5316\u56de\u5f52\u6811\u8bad\u7ec3\uff0c\u5904\u7406\u8fde\u7eed\u7279\u5f81\uff0c\u5e76\u63d0\u9ad8\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u73b0\u6709MIP\u65b9\u6cd5\u5728\u5904\u7406\u8fde\u7eed\u7279\u5f81\u7684\u56de\u5f52\u4efb\u52a1\u65f6\u8ba1\u7b97\u6210\u672c\u9ad8\u6216\u727a\u7272\u6700\u4f18\u6027\uff0c\u9700\u8981\u66f4\u4f18\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRS-ORT\u7684\u5206\u652f\u5b9a\u754c\uff08BB\uff09\u7b97\u6cd5\uff0c\u4ec5\u5728\u6811\u7ed3\u6784\u53d8\u91cf\u4e0a\u8fdb\u884c\u5206\u652f\uff0c\u5e76\u7ed3\u5408\u4e86\u591a\u79cd\u52a0\u901f\u6280\u672f\uff08\u5982\u7cbe\u786e\u53f6\u9884\u6d4b\u3001\u7ecf\u9a8c\u9608\u503c\u79bb\u6563\u5316\u3001\u6df1\u5ea61\u5b50\u6811\u89e3\u6790\u3001\u53ef\u5206\u89e3\u754c\u5b9a\u7b56\u7565\uff09\uff0c\u652f\u6301\u5e76\u884c\u8ba1\u7b97\u3002", "result": "RS-ORT\u5728\u5305\u542b\u4e8c\u5143\u548c\u8fde\u7eed\u7279\u5f81\u7684\u56de\u5f52\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5728\u8bad\u7ec3\u548c\u6d4b\u8bd5\u6027\u80fd\u4e0a\u5747\u6709\u63d0\u5347\uff0c\u4e14\u80fd\u5728\u5408\u7406\u65f6\u95f4\u5185\u83b7\u5f97\u6709\u4fdd\u8bc1\u7684\u8bad\u7ec3\u6027\u80fd\uff0c\u5e76\u4ea7\u751f\u66f4\u7b80\u5355\u7684\u6811\u7ed3\u6784\u548c\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5c24\u5176\u662f\u5728\u5927\u89c4\u6a21\uff08\u9ad8\u8fbe200\u4e07\u6837\u672c\uff09\u8fde\u7eed\u7279\u5f81\u6570\u636e\u96c6\u4e0a\u3002", "conclusion": "RS-ORT\u901a\u8fc7\u5176\u521b\u65b0\u7684\u5206\u652f\u5b9a\u754c\u65b9\u6cd5\u548c\u52a0\u901f\u6280\u672f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709MIP\u56de\u5f52\u6811\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5728\u5927\u89c4\u6a21\u8fde\u7eed\u7279\u5f81\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u8ba1\u7b97\u6548\u7387\u548c\u6a21\u578b\u6027\u80fd\u7684\u7a81\u7834\u3002"}}
{"id": "2411.09539", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2411.09539", "abs": "https://arxiv.org/abs/2411.09539", "authors": ["Marton Szep", "Daniel Rueckert", "R\u00fcdiger von Eisenhart-Rothe", "Florian Hinterwimmer"], "title": "Fine-tuning Large Language Models with Limited Data: A Survey and Practical Guide", "comment": "Accepted to TACL. Pre-MIT Press version. Major restructuring; added\n  preference alignment section and additional tables. 36 pages", "summary": "Fine-tuning large language models (LLMs) with limited data poses a practical\nchallenge in low-resource languages, specialized domains, and constrained\ndeployment settings. While pre-trained LLMs provide strong foundations,\neffective adaptation under data scarcity requires focused and efficient\nfine-tuning techniques. This paper presents a structured and practical survey\nof recent methods for fine-tuning LLMs in data-scarce scenarios. We\nsystematically review parameter-efficient fine-tuning techniques that lower\ntraining and deployment costs, domain and cross-lingual adaptation methods for\nboth encoder and decoder models, and model specialization strategies. We\nfurther examine preference alignment approaches that guide model behavior using\nlimited human or synthetic feedback, emphasizing sample and compute efficiency.\nThroughout, we highlight empirical trade-offs, selection criteria, and best\npractices for choosing suitable techniques based on task constraints, including\nmodel scaling, data scaling, and the mitigation of catastrophic forgetting. The\naim is to equip researchers and practitioners with actionable insights for\neffectively fine-tuning LLMs when data and resources are limited.", "AI": {"tldr": "\u672c\u8c03\u67e5\u5168\u9762\u6982\u8ff0\u4e86\u5728\u6570\u636e\u7a00\u7f3a\u60c5\u51b5\u4e0b\u5fae\u8c03\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5404\u79cd\u6280\u672f\uff0c\u91cd\u70b9\u662f\u53c2\u6570\u6548\u7387\u3001\u9886\u57df\u548c\u8de8\u8bed\u8a00\u9002\u5e94\u4ee5\u53ca\u504f\u597d\u5bf9\u9f50\uff0c\u4ee5\u6307\u5bfc\u7814\u7a76\u4eba\u5458\u548c\u4ece\u4e1a\u8005\u3002", "motivation": "\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u3001\u4e13\u4e1a\u9886\u57df\u548c\u90e8\u7f72\u53d7\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u7528\u6709\u9650\u7684\u6570\u636e\u5fae\u8c03\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u662f\u4e00\u4e2a\u5b9e\u9645\u6311\u6218\u3002\u672c\u8c03\u67e5\u65e8\u5728\u63d0\u4f9b\u9009\u62e9\u548c\u5b9e\u65bd\u6709\u6548\u5fae\u8c03\u6280\u672f\u7684\u5b9e\u7528\u89c1\u89e3\u3002", "method": "\u672c\u8c03\u67e5\u7cfb\u7edf\u5730\u56de\u987e\u4e86\u5404\u79cd\u5fae\u8c03\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u5305\u62ec\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08PEFT\uff09\u3001\u9886\u57df\u548c\u8de8\u8bed\u8a00\u9002\u5e94\u4ee5\u53ca\u504f\u597d\u5bf9\u9f50\u6280\u672f\u3002\u5b83\u8fd8\u5206\u6790\u4e86\u6a21\u578b\u6269\u5c55\u3001\u6570\u636e\u6269\u5c55\u548c\u707e\u96be\u6027\u9057\u5fd8\u7684\u7f13\u89e3\u7b49\u56e0\u7d20\u3002", "result": "\u672c\u8c03\u67e5\u603b\u7ed3\u4e86\u5728\u6570\u636e\u7a00\u7f3a\u7684\u60c5\u51b5\u4e0b\u5fae\u8c03\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6700\u65b0\u6280\u672f\uff0c\u5f3a\u8c03\u4e86\u5b9e\u9645\u7684\u6743\u8861\u3001\u9009\u62e9\u6807\u51c6\u548c\u6700\u4f73\u5b9e\u8df5\u3002", "conclusion": "\u672c\u8c03\u67e5\u4e3a\u7814\u7a76\u4eba\u5458\u548c\u4ece\u4e1a\u4eba\u5458\u63d0\u4f9b\u4e86\u6709\u5173\u5982\u4f55\u5728\u6570\u636e\u548c\u8d44\u6e90\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u6709\u6548\u5fae\u8c03\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5b9e\u7528\u89c1\u89e3\uff0c\u91cd\u70b9\u662f\u6837\u672c\u548c\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2510.24446", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24446", "abs": "https://arxiv.org/abs/2510.24446", "authors": ["Viktoriia Zinkovich", "Anton Antonov", "Andrei Spiridonov", "Denis Shepelev", "Andrey Moskalenko", "Daria Pugacheva", "Elena Tutubalina", "Andrey Kuznetsov", "Vlad Shakhuro"], "title": "SPARTA: Evaluating Reasoning Segmentation Robustness through Black-Box Adversarial Paraphrasing in Text Autoencoder Latent Space", "comment": null, "summary": "Multimodal large language models (MLLMs) have shown impressive capabilities\nin vision-language tasks such as reasoning segmentation, where models generate\nsegmentation masks based on textual queries. While prior work has primarily\nfocused on perturbing image inputs, semantically equivalent textual\nparaphrases-crucial in real-world applications where users express the same\nintent in varied ways-remain underexplored. To address this gap, we introduce a\nnovel adversarial paraphrasing task: generating grammatically correct\nparaphrases that preserve the original query meaning while degrading\nsegmentation performance. To evaluate the quality of adversarial paraphrases,\nwe develop a comprehensive automatic evaluation protocol validated with human\nstudies. Furthermore, we introduce SPARTA-a black-box, sentence-level\noptimization method that operates in the low-dimensional semantic latent space\nof a text autoencoder, guided by reinforcement learning. SPARTA achieves\nsignificantly higher success rates, outperforming prior methods by up to 2x on\nboth the ReasonSeg and LLMSeg-40k datasets. We use SPARTA and competitive\nbaselines to assess the robustness of advanced reasoning segmentation models.\nWe reveal that they remain vulnerable to adversarial paraphrasing-even under\nstrict semantic and grammatical constraints. All code and data will be released\npublicly upon acceptance.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5bf9\u6297\u6027\u91ca\u4e49\u4efb\u52a1\uff0c\u65e8\u5728\u751f\u6210\u4fdd\u6301\u539f\u67e5\u8be2\u542b\u4e49\u4f46\u4f1a\u964d\u4f4e\u5206\u5272\u6027\u80fd\u7684\u91ca\u4e49\uff0c\u5e76\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aSPARTA\u7684\u4f18\u5316\u65b9\u6cd5\u6765\u8bc4\u4f30\u548c\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5206\u5272\u4efb\u52a1\u4e2d\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u9488\u5bf9\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u4e8e\u6270\u52a8\u56fe\u50cf\u8f93\u5165\uff0c\u800c\u5ffd\u7565\u4e86\u5728\u771f\u5b9e\u5e94\u7528\u4e2d\u81f3\u5173\u91cd\u8981\u7684\u6587\u672c\u91ca\u4e49\uff08\u5373\u7528\u6237\u7528\u4e0d\u540c\u65b9\u5f0f\u8868\u8fbe\u76f8\u540c\u610f\u56fe\uff09\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5bf9\u6297\u6027\u91ca\u4e49\u4efb\u52a1\uff0c\u751f\u6210\u8bed\u6cd5\u6b63\u786e\u4e14\u4fdd\u6301\u539f\u610f\u56fe\u4f46\u4f1a\u964d\u4f4e\u5206\u5272\u6027\u80fd\u7684\u91ca\u4e49\u3002\u5f00\u53d1\u4e86\u4e00\u4e2a\u5305\u542b\u4eba\u7c7b\u7814\u7a76\u9a8c\u8bc1\u7684\u81ea\u52a8\u8bc4\u4f30\u534f\u8bae\u3002\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aSPARTA\u7684\u9ed1\u76d2\u3001\u53e5\u5b50\u7ea7\u4f18\u5316\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u6587\u672c\u81ea\u52a8\u7f16\u7801\u5668\u7684\u4f4e\u7ef4\u8bed\u4e49\u6f5c\u5728\u7a7a\u95f4\u4e2d\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u4f18\u5316\u3002", "result": "SPARTA\u5728ReasonSeg\u548cLLMSeg-40k\u6570\u636e\u96c6\u4e0a\uff0c\u76f8\u8f83\u4e8e\u5148\u524d\u65b9\u6cd5\uff0c\u6210\u529f\u7387\u663e\u8457\u63d0\u9ad8\uff0c\u6700\u9ad8\u53ef\u8fbe2\u500d\u3002\u7814\u7a76\u8868\u660e\uff0c\u73b0\u6709\u7684\u5148\u8fdb\u63a8\u7406\u5206\u5272\u6a21\u578b\u5728\u4e25\u683c\u7684\u8bed\u4e49\u548c\u8bed\u6cd5\u7ea6\u675f\u4e0b\uff0c\u4ecd\u7136\u5bb9\u6613\u53d7\u5230\u5bf9\u6297\u6027\u91ca\u4e49\u7684\u653b\u51fb\u3002", "conclusion": "\u5373\u4f7f\u5728\u4e25\u683c\u7684\u8bed\u4e49\u548c\u8bed\u6cd5\u7ea6\u675f\u4e0b\uff0c\u5148\u8fdb\u7684\u63a8\u7406\u5206\u5272\u6a21\u578b\u4ecd\u7136\u5bb9\u6613\u53d7\u5230\u5bf9\u6297\u6027\u91ca\u4e49\u7684\u653b\u51fb\uff0c\u8868\u660e\u5728\u771f\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u9700\u8981\u8fdb\u4e00\u6b65\u63d0\u9ad8\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.24398", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24398", "abs": "https://arxiv.org/abs/2510.24398", "authors": ["Youwan Mah\u00e9", "Elise Bannier", "St\u00e9phanie Leplaideur", "Elisa Fromont", "Francesca Galassi"], "title": "Unsupervised Detection of Post-Stroke Brain Abnormalities", "comment": null, "summary": "Post-stroke MRI not only delineates focal lesions but also reveals secondary\nstructural changes, such as atrophy and ventricular enlargement. These\nabnormalities, increasingly recognised as imaging biomarkers of recovery and\noutcome, remain poorly captured by supervised segmentation methods. We evaluate\nREFLECT, a flow-based generative model, for unsupervised detection of both\nfocal and non-lesional abnormalities in post-stroke patients. Using dual-expert\ncentral-slice annotations on ATLAS data, performance was assessed at the object\nlevel with Free-Response ROC analysis for anomaly maps. Two models were trained\non lesion-free slices from stroke patients (ATLAS) and on healthy controls\n(IXI) to test the effect of training data. On ATLAS test subjects, the\nIXI-trained model achieved higher lesion segmentation (Dice = 0.37 vs 0.27) and\nimproved sensitivity to non-lesional abnormalities (FROC = 0.62 vs 0.43).\nTraining on fully healthy anatomy improves the modelling of normal variability,\nenabling broader and more reliable detection of structural abnormalities.", "AI": {"tldr": "REFLECT, a flow-based generative model, outperforms traditional methods in detecting both focal and non-lesional abnormalities in post-stroke MRIs by training on healthy controls to better model normal anatomical variations.", "motivation": "Supervised segmentation methods struggle to capture secondary structural changes (atrophy, ventricular enlargement) visible in post-stroke MRIs, which are important biomarkers for recovery and outcome.", "method": "Evaluated REFLECT, a flow-based generative model, for unsupervised detection of abnormalities in post-stroke MRIs. Trained two models: one on lesion-free slices from stroke patients (ATLAS) and another on healthy controls (IXI). Assessed performance using dual-expert annotations and Free-Response ROC analysis on object-level anomaly maps.", "result": "The IXI-trained model (trained on healthy controls) achieved better results on ATLAS test subjects, with higher lesion segmentation (Dice = 0.37 vs 0.27) and improved sensitivity to non-lesional abnormalities (FROC = 0.62 vs 0.43) compared to the ATLAS-trained model.", "conclusion": "Training a generative model on healthy anatomy enhances its ability to model normal variability, leading to more effective and reliable detection of structural abnormalities in medical images like post-stroke MRIs."}}
{"id": "2510.23906", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23906", "abs": "https://arxiv.org/abs/2510.23906", "authors": ["Wasim Ahmad", "Maha Shadaydeh", "Joachim Denzler"], "title": "Group Interventions on Deep Networks for Causal Discovery in Subsystems", "comment": "Submitted to IEEE Access. We are working on the revised version", "summary": "Causal discovery uncovers complex relationships between variables, enhancing\npredictions, decision-making, and insights into real-world systems, especially\nin nonlinear multivariate time series. However, most existing methods primarily\nfocus on pairwise cause-effect relationships, overlooking interactions among\ngroups of variables, i.e., subsystems and their collective causal influence. In\nthis study, we introduce gCDMI, a novel multi-group causal discovery method\nthat leverages group-level interventions on trained deep neural networks and\nemploys model invariance testing to infer causal relationships. Our approach\ninvolves three key steps. First, we use deep learning to jointly model the\nstructural relationships among groups of all time series. Second, we apply\ngroup-wise interventions to the trained model. Finally, we conduct model\ninvariance testing to determine the presence of causal links among variable\ngroups. We evaluate our method on simulated datasets, demonstrating its\nsuperior performance in identifying group-level causal relationships compared\nto existing methods. Additionally, we validate our approach on real-world\ndatasets, including brain networks and climate ecosystems. Our results\nhighlight that applying group-level interventions to deep learning models,\ncombined with invariance testing, can effectively reveal complex causal\nstructures, offering valuable insights for domains such as neuroscience and\nclimate science.", "AI": {"tldr": "gCDMI\u662f\u4e00\u79cd\u65b0\u7684\u591a\u7ec4\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\uff0c\u5229\u7528\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u548c\u7ec4\u5e72\u9884\u6765\u63a8\u65ad\u53d8\u91cf\u7ec4\u4e4b\u95f4\u7684\u56e0\u679c\u5173\u7cfb\u3002", "motivation": "\u73b0\u6709\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u6210\u5bf9\u5173\u7cfb\uff0c\u5ffd\u7565\u4e86\u53d8\u91cf\u7ec4\uff08\u5b50\u7cfb\u7edf\uff09\u53ca\u5176\u96c6\u4f53\u56e0\u679c\u5f71\u54cd\u7684\u76f8\u4e92\u4f5c\u7528\u3002", "method": "1. \u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u5bf9\u6240\u6709\u65f6\u95f4\u5e8f\u5217\u7ec4\u4e4b\u95f4\u7684\u7ed3\u6784\u5173\u7cfb\u8fdb\u884c\u8054\u5408\u5efa\u6a21\u30022. \u5bf9\u8bad\u7ec3\u597d\u7684\u6a21\u578b\u5e94\u7528\u5206\u7ec4\u5e72\u9884\u30023. \u8fdb\u884c\u6a21\u578b\u4e0d\u53d8\u6027\u6d4b\u8bd5\uff0c\u4ee5\u786e\u5b9a\u53d8\u91cf\u7ec4\u4e4b\u95f4\u662f\u5426\u5b58\u5728\u56e0\u679c\u8054\u7cfb\u3002", "result": "\u5728\u6a21\u62df\u6570\u636e\u96c6\u4e0a\uff0cgCDMI\u5728\u8bc6\u522b\u7ec4\u7ea7\u56e0\u679c\u5173\u7cfb\u65b9\u9762\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u5728\u771f\u5b9e\u4e16\u754c\u7684\u5927\u8111\u7f51\u7edc\u548c\u6c14\u5019\u751f\u6001\u7cfb\u7edf\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "conclusion": "\u5c06\u5206\u7ec4\u5e72\u9884\u5e94\u7528\u4e8e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5e76\u7ed3\u5408\u4e0d\u53d8\u6027\u6d4b\u8bd5\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u63ed\u793a\u590d\u6742\u7684\u56e0\u679c\u7ed3\u6784\uff0c\u4e3a\u795e\u7ecf\u79d1\u5b66\u548c\u6c14\u5019\u79d1\u5b66\u7b49\u9886\u57df\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002"}}
{"id": "2505.22820", "categories": ["cs.LG", "cs.AI", "econ.TH", "stat.ML"], "pdf": "https://arxiv.org/pdf/2505.22820", "abs": "https://arxiv.org/abs/2505.22820", "authors": ["Ayush Sawarni", "Sahasrajit Sarmasarkar", "Vasilis Syrgkanis"], "title": "Preference Learning with Response Time: Robust Losses and Guarantees", "comment": "Accepted at NeurIPS 2025", "summary": "This paper investigates the integration of response time data into human\npreference learning frameworks for more effective reward model elicitation.\nWhile binary preference data has become fundamental in fine-tuning foundation\nmodels, generative AI systems, and other large-scale models, the valuable\ntemporal information inherent in user decision-making remains largely\nunexploited. We propose novel methodologies to incorporate response time\ninformation alongside binary choice data, leveraging the Evidence Accumulation\nDrift Diffusion (EZ) model, under which response time is informative of the\npreference strength. We develop Neyman-orthogonal loss functions that achieve\noracle convergence rates for reward model learning, matching the theoretical\noptimal rates that would be attained if the expected response times for each\nquery were known a priori. Our theoretical analysis demonstrates that for\nlinear reward functions, conventional preference learning suffers from error\nrates that scale exponentially with reward magnitude. In contrast, our response\ntime-augmented approach reduces this to polynomial scaling, representing a\nsignificant improvement in sample efficiency. We extend these guarantees to\nnon-parametric reward function spaces, establishing convergence properties for\nmore complex, realistic reward models. Our extensive experiments validate our\ntheoretical findings in the context of preference learning over images.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u54cd\u5e94\u65f6\u95f4\u4fe1\u606f\u548c\u4e8c\u5143\u9009\u62e9\u6570\u636e\u6765\u5b66\u4e60\u7528\u6237\u504f\u597d\u7684\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u5956\u52b1\u6a21\u578b\u7684\u51c6\u786e\u6027\uff0c\u5e76\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u7684\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u4e8c\u5143\u504f\u597d\u6570\u636e\u7684\u5956\u52b1\u6a21\u578b\u5b66\u4e60\u65b9\u6cd5\u5ffd\u7565\u4e86\u7528\u6237\u51b3\u7b56\u4e2d\u56fa\u6709\u7684\u65f6\u95f4\u4fe1\u606f\uff0c\u800c\u8fd9\u4e9b\u4fe1\u606f\u53ef\u4ee5\u53cd\u6620\u504f\u597d\u5f3a\u5ea6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u8bc1\u636e\u7d2f\u79ef\u6f02\u79fb\u6269\u6563\uff08EZ\uff09\u6a21\u578b\u6574\u5408\u54cd\u5e94\u65f6\u95f4\u4fe1\u606f\u548c\u4e8c\u5143\u9009\u62e9\u6570\u636e\u7684\u65b0\u65b9\u6cd5\uff0c\u5e76\u5f00\u53d1\u4e86 Neyman-\u6b63\u4ea4\u635f\u5931\u51fd\u6570\uff0c\u4ee5\u8fbe\u5230\u6700\u4f18\u7684\u5956\u52b1\u6a21\u578b\u5b66\u4e60\u6536\u655b\u901f\u7387\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\uff0c\u4e0e\u4ec5\u4f7f\u7528\u4e8c\u5143\u504f\u597d\u6570\u636e\u7684\u4f20\u7edf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u54cd\u5e94\u65f6\u95f4\u589e\u5f3a\u65b9\u6cd5\u5728\u6837\u672c\u6548\u7387\u65b9\u9762\u6709\u4e86\u663e\u8457\u63d0\u9ad8\uff0c\u5c24\u5176\u662f\u5728\u5956\u52b1\u5e45\u5ea6\u8f83\u5927\u65f6\uff0c\u5c06\u8bef\u5dee\u7387\u4ece\u6307\u6570\u7ea7\u7f29\u653e\u5230\u591a\u9879\u5f0f\u7ea7\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u5728\u975e\u53c2\u6570\u5956\u52b1\u51fd\u6570\u7a7a\u95f4\u4e2d\u4e5f\u5177\u6709\u6536\u655b\u6027\u4fdd\u8bc1\u3002", "conclusion": "\u672c\u7814\u7a76\u6210\u529f\u5730\u5c06\u54cd\u5e94\u65f6\u95f4\u4fe1\u606f\u878d\u5165\u4e86\u4eba\u7c7b\u504f\u597d\u5b66\u4e60\u6846\u67b6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5956\u52b1\u6a21\u578b\u7684\u5b66\u4e60\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u5e76\u5728\u56fe\u50cf\u504f\u597d\u5b66\u4e60\u7684\u5b9e\u9a8c\u4e2d\u5f97\u5230\u4e86\u9a8c\u8bc1\u3002"}}
{"id": "2510.24450", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24450", "abs": "https://arxiv.org/abs/2510.24450", "authors": ["\u0160pela Vintar", "Taja Kuzman Punger\u0161ek", "Mojca Brglez", "Nikola Ljube\u0161i\u0107"], "title": "Charting the European LLM Benchmarking Landscape: A New Taxonomy and a Set of Best Practices", "comment": "12 pages, 1 figure. Submitted to the LREC 2026 conference", "summary": "While new benchmarks for large language models (LLMs) are being developed\ncontinuously to catch up with the growing capabilities of new models and AI in\ngeneral, using and evaluating LLMs in non-English languages remains a\nlittle-charted landscape. We give a concise overview of recent developments in\nLLM benchmarking, and then propose a new taxonomy for the categorization of\nbenchmarks that is tailored to multilingual or non-English use scenarios. We\nfurther propose a set of best practices and quality standards that could lead\nto a more coordinated development of benchmarks for European languages. Among\nother recommendations, we advocate for a higher language and culture\nsensitivity of evaluation methods.", "AI": {"tldr": "LLM \u8bc4\u4f30\u5728\u975e\u82f1\u8bed\u8bed\u8a00\u4e2d\u662f\u4e00\u4e2a\u6311\u6218\uff0c\u9700\u8981\u65b0\u7684\u5206\u7c7b\u6cd5\u548c\u9488\u5bf9\u6b27\u6d32\u8bed\u8a00\u7684\u6700\u4f73\u5b9e\u8df5\u3002", "motivation": "LLM \u8bc4\u4f30\u5728\u975e\u82f1\u8bed\u8bed\u8a00\u4e2d\u662f\u4e00\u4e2a\u6311\u6218\uff0c\u9700\u8981\u65b0\u7684\u5206\u7c7b\u6cd5\u548c\u9488\u5bf9\u6b27\u6d32\u8bed\u8a00\u7684\u6700\u4f73\u5b9e\u8df5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5206\u7c7b\u6cd5\u6765\u5bf9 LLM \u8bc4\u4f30\u8fdb\u884c\u5206\u7c7b\uff0c\u5e76\u4e3a\u6b27\u6d32\u8bed\u8a00\u5236\u5b9a\u4e86\u6700\u4f73\u5b9e\u8df5\u3002", "result": "\u5bf9 LLM \u8bc4\u4f30\u8fdb\u884c\u4e86\u6982\u8ff0\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5206\u7c7b\u6cd5\u548c\u4e00\u5957\u6700\u4f73\u5b9e\u8df5\u3002", "conclusion": "\u4e3a\u4fc3\u8fdb\u591a\u8bed\u8a00 LLM \u8bc4\u4f30\u7684\u534f\u8c03\u53d1\u5c55\uff0c\u5c24\u5176\u662f\u5728\u6b27\u6d32\u8bed\u8a00\u65b9\u9762\uff0c\u9700\u8981\u63d0\u9ad8\u8bed\u8a00\u548c\u6587\u5316\u654f\u611f\u6027\u3002"}}
{"id": "2510.23912", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23912", "abs": "https://arxiv.org/abs/2510.23912", "authors": ["Marko Karbevski", "Antonij Mijoski"], "title": "Key and Value Weights Are Probably All You Need: On the Necessity of the Query, Key, Value weight Triplet in Decoder-Only Transformers", "comment": null, "summary": "The Query, Key, Value weight triplet is a building block of current attention\nmechanisms in state-of-the-art LLMs. We theoretically investigate whether this\ntriplet can be reduced, proving under simplifying assumptions that the Query\nweights are redundant, thereby reducing the number of non-embedding/lm-head\nparameters by over 8%. We validate the theory on full-complexity GPT-3 small\narchitectures (with layer normalization, skip connections, and weight decay)\ntrained from scratch, demonstrating that the reduced model achieves comparable\nvalidation loss to standard baselines. These findings motivate the\ninvestigation of the Query weight redundancy at scale.", "AI": {"tldr": "Query\u6743\u91cd\u5728\u6ce8\u610f\u529b\u673a\u5236\u4e2d\u662f\u5197\u4f59\u7684\uff0c\u53ef\u4ee5\u51cf\u5c118%\u7684\u53c2\u6570\uff0c\u4e14\u4e0d\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u63a2\u7a76\u5f53\u524d\u6700\u5148\u8fdb\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\u6ce8\u610f\u529b\u673a\u5236\u7684Query\u3001Key\u3001Value\u6743\u91cd\u4e09\u5143\u7ec4\u662f\u5426\u53ef\u4ee5\u88ab\u7b80\u5316\u3002", "method": "\u7406\u8bba\u4e0a\u8bc1\u660e\u5728\u7b80\u5316\u5047\u8bbe\u4e0bQuery\u6743\u91cd\u662f\u5197\u4f59\u7684\uff0c\u4ece\u800c\u51cf\u5c118%\u7684\u975e\u5d4c\u5165/lm-head\u53c2\u6570\u3002\u5e76\u5728\u5b8c\u6574\u590d\u6742\u5ea6\u7684GPT-3 small\u67b6\u6784\u4e0a\u8fdb\u884c\u9a8c\u8bc1\uff0c\u8bad\u7ec3\u6a21\u578b\u5e76\u6bd4\u8f83\u9a8c\u8bc1\u635f\u5931\u3002", "result": "\u7b80\u5316\u7684\u6a21\u578b\u5b9e\u73b0\u4e86\u4e0e\u6807\u51c6\u57fa\u7ebf\u76f8\u5f53\u7684\u9a8c\u8bc1\u635f\u5931\u3002", "conclusion": "Query\u6743\u91cd\u5197\u4f59\u7684\u53d1\u73b0\u542f\u53d1\u4e86\u5bf9\u5927\u89c4\u6a21\u6a21\u578b\u4e2dQuery\u6743\u91cd\u5197\u4f59\u6027\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2510.24469", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.24469", "abs": "https://arxiv.org/abs/2510.24469", "authors": ["Durga Prasad Maram", "Dhruvin Gandhi", "Zonghai Yao", "Gayathri Akkinapalli", "Franck Dernoncourt", "Yu Wang", "Ryan A. Rossi", "Nesreen K. Ahmed"], "title": "Iterative Critique-Refine Framework for Enhancing LLM Personalization", "comment": null, "summary": "Personalized text generation requires models not only to produce coherent\ntext but also to align with a target user's style, tone, and topical focus.\nExisting retrieval-augmented approaches such as LaMP and PGraphRAG enrich\nprofiles with user and neighbor histories, but they stop at generation and\noften yield outputs that drift in tone, topic, or style. We present PerFine, a\nunified, training-free critique-refine framework that enhances personalization\nthrough iterative, profile-grounded feedback. In each iteration, an LLM\ngenerator produces a draft conditioned on the retrieved profile, and a critic\nLLM - also conditioned on the same profile - provides structured feedback on\ntone, vocabulary, sentence structure, and topicality. The generator then\nrevises, while a novel knockout strategy retains the stronger draft across\niterations. We further study additional inference-time strategies such as\nBest-of-N and Topic Extraction to balance quality and efficiency. Across Yelp,\nGoodreads, and Amazon datasets, PerFine consistently improves personalization\nover PGraphRAG, with GEval gains of +7-13%, steady improvements over 3-5\nrefinement iterations, and scalability with increasing critic size. These\nresults highlight that post-hoc, profile-aware feedback offers a powerful\nparadigm for personalized LLM generation that is both training-free and\nmodel-agnostic.", "AI": {"tldr": "PerFine\u662f\u4e00\u4e2a\u521b\u65b0\u7684\u3001\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u8fed\u4ee3\u7684\u3001\u4ee5\u7528\u6237\u753b\u50cf\u4e3a\u57fa\u7840\u7684\u53cd\u9988\u6765\u63d0\u5347\u4e2a\u6027\u5316\u6587\u672c\u751f\u6210\u6548\u679c\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u8bed\u6c14\u3001\u4e3b\u9898\u6216\u98ce\u683c\u6f02\u79fb\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u68c0\u7d22\u589e\u5f3a\u65b9\u6cd5\uff08\u5982LaMP\u548cPGraphRAG\uff09\u5728\u751f\u6210\u4e2a\u6027\u5316\u6587\u672c\u65f6\uff0c\u867d\u7136\u80fd\u591f\u4e30\u5bcc\u7528\u6237\u753b\u50cf\uff0c\u4f46\u5f80\u5f80\u5728\u751f\u6210\u540e\u51fa\u73b0\u8bed\u6c14\u3001\u4e3b\u9898\u6216\u98ce\u683c\u6f02\u79fb\u7684\u95ee\u9898\uff0c\u65e0\u6cd5\u6ee1\u8db3\u7528\u6237\u4e2a\u6027\u5316\u9700\u6c42\u3002", "method": "PerFine\u6846\u67b6\u5305\u542b\u4e00\u4e2a\u751f\u6210\u5668\u548c\u4e00\u4e2a\u6279\u8bc4\u8005\u3002\u5728\u6bcf\u6b21\u8fed\u4ee3\u4e2d\uff0c\u751f\u6210\u5668\u6839\u636e\u68c0\u7d22\u5230\u7684\u7528\u6237\u753b\u50cf\u751f\u6210\u6587\u672c\u8349\u7a3f\uff0c\u540c\u65f6\u6279\u8bc4\u8005\u4e5f\u6839\u636e\u540c\u4e00\u7528\u6237\u753b\u50cf\uff0c\u4ece\u8bed\u6c14\u3001\u8bcd\u6c47\u3001\u53e5\u5b50\u7ed3\u6784\u548c\u4e3b\u9898\u7b49\u65b9\u9762\u63d0\u4f9b\u7ed3\u6784\u5316\u53cd\u9988\u3002\u751f\u6210\u5668\u968f\u540e\u6839\u636e\u53cd\u9988\u8fdb\u884c\u4fee\u6539\uff0c\u5e76\u91c7\u7528\u4e00\u79cd\u65b0\u9896\u7684\u201c\u6dd8\u6c70\u7b56\u7565\u201d\u6765\u4fdd\u7559\u8fc7\u7a0b\u4e2d\u66f4\u4f18\u7684\u8349\u7a3f\u3002\u6b64\u5916\uff0c\u8fd8\u7814\u7a76\u4e86\u201cN\u9009\u4e00\u201d\u548c\u201c\u4e3b\u9898\u63d0\u53d6\u201d\u7b49\u63a8\u7406\u65f6\u7b56\u7565\u4ee5\u5e73\u8861\u8d28\u91cf\u548c\u6548\u7387\u3002", "result": "\u5728Yelp\u3001Goodreads\u548cAmazon\u6570\u636e\u96c6\u4e0a\uff0cPerFine\u5728\u4e2a\u6027\u5316\u65b9\u9762\u6301\u7eed\u4f18\u4e8ePGraphRAG\uff0cGEval\u5f97\u5206\u63d0\u5347\u4e867-13%\uff0c\u5e76\u4e14\u57283-5\u6b21\u8fed\u4ee3\u540e\u6548\u679c\u7a33\u5b9a\u3002\u6b64\u5916\uff0c\u8be5\u6846\u67b6\u7684\u53ef\u6269\u5c55\u6027\u4e5f\u968f\u7740\u6279\u8bc4\u8005\u89c4\u6a21\u7684\u589e\u52a0\u800c\u63d0\u9ad8\u3002", "conclusion": "\u4e8b\u540e\u8fdb\u884c\u7684\u3001\u611f\u77e5\u7528\u6237\u753b\u50cf\u7684\u53cd\u9988\u662f\u4e00\u79cd\u5f3a\u5927\u7684\u8303\u5f0f\uff0c\u53ef\u4ee5\u5b9e\u73b0\u65e0\u9700\u8bad\u7ec3\u4e14\u6a21\u578b\u65e0\u5173\u7684\u4e2a\u6027\u5316\u8bed\u8a00\u6a21\u578b\u751f\u6210\u3002"}}
{"id": "2510.23914", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23914", "abs": "https://arxiv.org/abs/2510.23914", "authors": ["Arsenii Mustafin", "Xinyi Sheng", "Dominik Baumann"], "title": "Geometry-Inspired Unified Framework for Discounted and Average Reward MDPs", "comment": "12 pages, 1 figure", "summary": "The theoretical analysis of Markov Decision Processes (MDPs) is commonly\nsplit into two cases - the average-reward case and the discounted-reward case -\nwhich, while sharing similarities, are typically analyzed separately. In this\nwork, we extend a recently introduced geometric interpretation of MDPs for the\ndiscounted-reward case to the average-reward case, thereby unifying both. This\nallows us to extend a major result known for the discounted-reward case to the\naverage-reward case: under a unique and ergodic optimal policy, the Value\nIteration algorithm achieves a geometric convergence rate.", "AI": {"tldr": "\u672c\u7bc7\u8bba\u6587\u5c06\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08MDP\uff09\u7684\u7406\u8bba\u5206\u6790\u4ece\u5e73\u5747\u5956\u52b1\u548c\u6298\u6263\u5956\u52b1\u4e24\u4e2a\u72ec\u7acb\u7684\u6848\u4f8b\u7edf\u4e00\u8d77\u6765\uff0c\u5e76\u8bc1\u660e\u4e86\u5728\u552f\u4e00\u4e14\u904d\u5386\u7684\u6700\u4f18\u7b56\u7565\u4e0b\uff0c\u503c\u8fed\u4ee3\u7b97\u6cd5\u53ef\u4ee5\u5b9e\u73b0\u51e0\u4f55\u6536\u655b\u3002", "motivation": "\u5c06 MDP \u7684\u6298\u6263\u5956\u52b1\u6848\u4f8b\u7684\u51e0\u4f55\u89e3\u91ca\u6269\u5c55\u5230\u5e73\u5747\u5956\u52b1\u6848\u4f8b\uff0c\u4ece\u800c\u7edf\u4e00\u4e24\u79cd\u60c5\u51b5\u7684\u5206\u6790\u3002", "method": "\u6269\u5c55\u4e86 MDP \u7684\u51e0\u4f55\u89e3\u91ca\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u5e73\u5747\u5956\u52b1\u6848\u4f8b\u3002", "result": "\u5728\u552f\u4e00\u4e14\u904d\u5386\u7684\u6700\u4f18\u7b56\u7565\u4e0b\uff0c\u503c\u8fed\u4ee3\u7b97\u6cd5\u5b9e\u73b0\u4e86\u51e0\u4f55\u6536\u655b\u3002", "conclusion": "\u503c\u8fed\u4ee3\u7b97\u6cd5\u5728\u5e73\u5747\u5956\u52b1\u60c5\u51b5\u4e0b\uff0c\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u4e5f\u80fd\u8fbe\u5230\u51e0\u4f55\u6536\u655b\uff0c\u8fd9\u7edf\u4e00\u4e86\u4e4b\u524d\u5728\u6298\u6263\u5956\u52b1\u60c5\u51b5\u4e0b\u5df2\u77e5\u7684\u7ed3\u8bba\u3002"}}
{"id": "2510.24476", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24476", "abs": "https://arxiv.org/abs/2510.24476", "authors": ["Yihan Li", "Xiyuan Fu", "Ghanshyam Verma", "Paul Buitelaar", "Mingming Liu"], "title": "Mitigating Hallucination in Large Language Models (LLMs): An Application-Oriented Survey on RAG, Reasoning, and Agentic Systems", "comment": "25 pages, 7 figures, 3 tables", "summary": "Hallucination remains one of the key obstacles to the reliable deployment of\nlarge language models (LLMs), particularly in real-world applications. Among\nvarious mitigation strategies, Retrieval-Augmented Generation (RAG) and\nreasoning enhancement have emerged as two of the most effective and widely\nadopted approaches, marking a shift from merely suppressing hallucinations to\nbalancing creativity and reliability. However, their synergistic potential and\nunderlying mechanisms for hallucination mitigation have not yet been\nsystematically examined. This survey adopts an application-oriented perspective\nof capability enhancement to analyze how RAG, reasoning enhancement, and their\nintegration in Agentic Systems mitigate hallucinations. We propose a taxonomy\ndistinguishing knowledge-based and logic-based hallucinations, systematically\nexamine how RAG and reasoning address each, and present a unified framework\nsupported by real-world applications, evaluations, and benchmarks.", "AI": {"tldr": "RAG\u548c\u63a8\u7406\u589e\u5f3a\u662f\u89e3\u51b3LLM\u5e7b\u89c9\u7684\u4e24\u79cd\u6709\u6548\u65b9\u6cd5\uff0c\u672c\u6587\u4ece\u5e94\u7528\u89d2\u5ea6\u5206\u6790\u4e86\u5b83\u4eec\u5982\u4f55\u534f\u540c\u4f5c\u7528\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u533a\u5206\u77e5\u8bc6\u578b\u548c\u903b\u8f91\u578b\u5e7b\u89c9\u7684\u5206\u7c7b\u6cd5\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u672a\u7cfb\u7edf\u8003\u5bdfRAG\u548c\u63a8\u7406\u589e\u5f3a\u5728\u7f13\u89e3LLM\u5e7b\u89c9\u65b9\u9762\u7684\u534f\u540c\u6f5c\u529b\u53ca\u5176\u6f5c\u5728\u673a\u5236\u3002", "method": "\u4ece\u5e94\u7528\u5bfc\u5411\u7684\u80fd\u529b\u589e\u5f3a\u89c6\u89d2\u51fa\u53d1\uff0c\u5206\u6790RAG\u3001\u63a8\u7406\u589e\u5f3a\u53ca\u5176\u5728Agentic Systems\u4e2d\u7684\u96c6\u6210\u5982\u4f55\u7f13\u89e3\u5e7b\u89c9\u3002\u63d0\u51fa\u533a\u5206\u77e5\u8bc6\u578b\u548c\u903b\u8f91\u578b\u5e7b\u89c9\u7684\u5206\u7c7b\u6cd5\uff0c\u5e76\u7cfb\u7edf\u8003\u5bdfRAG\u548c\u63a8\u7406\u589e\u5f3a\u5982\u4f55\u89e3\u51b3\u8fd9\u4e24\u79cd\u5e7b\u89c9\u3002", "result": "RAG\u548c\u63a8\u7406\u589e\u5f3a\u53ca\u5176\u96c6\u6210\u5728Agentic Systems\u4e2d\u7684\u5e94\u7528\u80fd\u591f\u6709\u6548\u7f13\u89e3LLM\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u77e5\u8bc6\u578b\u548c\u903b\u8f91\u578b\u5e7b\u89c9\u65b9\u9762\u3002", "conclusion": "RAG\u548c\u63a8\u7406\u589e\u5f3a\u662f\u89e3\u51b3LLM\u5e7b\u89c9\u95ee\u9898\u7684\u4e24\u79cd\u4e3b\u8981\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u5b83\u4eec\u5e76\u5728Agentic Systems\u4e2d\u5e94\u7528\uff0c\u53ef\u4ee5\u66f4\u6709\u6548\u5730\u5e73\u8861\u6a21\u578b\u7684\u521b\u9020\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2510.24413", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24413", "abs": "https://arxiv.org/abs/2510.24413", "authors": ["Ali Ahmad Faour", "Nabil Amacha", "Ali J. Ghandour"], "title": "50 Years of Water Body Monitoring: The Case of Qaraaoun Reservoir, Lebanon", "comment": null, "summary": "The sustainable management of the Qaraaoun Reservoir, the largest surface\nwater body in Lebanon located in the Bekaa Plain, depends on reliable\nmonitoring of its storage volume despite frequent sensor malfunctions and\nlimited maintenance capacity. This study introduces a sensor-free approach that\nintegrates open-source satellite imagery, advanced water-extent segmentation,\nand machine learning to estimate the reservoir surface area and volume in near\nreal time. Sentinel-2 and Landsat images are processed, where surface water is\ndelineated using a newly proposed water segmentation index. A machine learning\nmodel based on Support Vector Regression (SVR) is trained on a curated dataset\nthat includes water surface area, water level, and water volume calculations\nusing a reservoir bathymetry survey. The model is then able to estimate\nreservoir volume relying solely on surface area extracted from satellite\nimagery, without the need for ground measurements. Water segmentation using the\nproposed index aligns with ground truth for more than 95 percent of the\nshoreline. Hyperparameter tuning with GridSearchCV yields an optimized SVR\nperformance with error under 1.5 percent of full reservoir capacity and\ncoefficients of determination exceeding 0.98. These results demonstrate the\nrobustness and cost-effectiveness of the method, offering a practical solution\nfor continuous, sensor-independent monitoring of reservoir storage. The\nproposed methodology can be replicated for other water bodies, and the\nresulting 50 years of time-series data is valuable for research on climate\nchange and environmental patterns.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2510.23926", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23926", "abs": "https://arxiv.org/abs/2510.23926", "authors": ["Ningfeng Yang", "Tor M. Aamodt"], "title": "Improving the Straight-Through Estimator with Zeroth-Order Information", "comment": "39th Conference on Neural Information Processing Systems (NeurIPS\n  2025)", "summary": "We study the problem of training neural networks with quantized parameters.\nLearning low-precision quantized parameters by enabling computation of\ngradients via the Straight-Through Estimator (STE) can be challenging. While\nthe STE enables back-propagation, which is a first-order method, recent works\nhave explored the use of zeroth-order (ZO) gradient descent for fine-tuning. We\nnote that the STE provides high-quality biased gradients, and ZO gradients are\nunbiased but can be expensive. We thus propose First-Order-Guided Zeroth-Order\nGradient Descent (FOGZO) that reduces STE bias while reducing computations\nrelative to ZO methods. Empirically, we show FOGZO improves the tradeoff\nbetween quality and training time in Quantization-Aware Pre-Training.\nSpecifically, versus STE at the same number of iterations, we show a 1-8\\%\naccuracy improvement for DeiT Tiny/Small, 1-2\\% accuracy improvement on ResNet\n18/50, and 1-22 perplexity point improvement for LLaMA models with up to 0.3\nbillion parameters. For the same loss, FOGZO yields a 796$\\times$ reduction in\ncomputation versus n-SPSA for a 2-layer MLP on MNIST. Code is available at\nhttps://github.com/1733116199/fogzo.", "AI": {"tldr": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFOGZO\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bad\u7ec3\u5177\u6709\u91cf\u5316\u53c2\u6570\u7684\u795e\u7ecf\u7f51\u7edc\uff0c\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u4e00\u9636\u68af\u5ea6\u4f30\u8ba1\uff08STE\uff09\u548c\u96f6\u9636\uff08ZO\uff09\u68af\u5ea6\u4e0b\u964d\u7684\u4f18\u70b9\uff0c\u4ee5\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\u548c\u6a21\u578b\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u7684\u901a\u8fc7\u68af\u5ea6\u8ba1\u7b97\u8fdb\u884c\u91cf\u5316\u53c2\u6570\u8bad\u7ec3\u7684\u65b9\u6cd5\u5b58\u5728\u6311\u6218\u3002STE\u867d\u7136\u80fd\u8fdb\u884c\u53cd\u5411\u4f20\u64ad\uff0c\u4f46\u53ef\u80fd\u5f15\u5165\u504f\u5dee\uff0c\u800cZO\u68af\u5ea6\u867d\u7136\u65e0\u504f\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u51cf\u5c11STE\u7684\u504f\u5dee\u5e76\u964d\u4f4eZO\u65b9\u6cd5\u7684\u8ba1\u7b97\u6210\u672c\u3002", "method": "FOGZO\uff08First-Order-Guided Zeroth-Order Gradient Descent\uff09\u7ed3\u5408\u4e86STE\u7684\u4e00\u9636\u68af\u5ea6\u4fe1\u606f\u548cZO\u68af\u5ea6\u4e0b\u964d\u7684\u7279\u70b9\uff0c\u65e8\u5728\u51cf\u5c11STE\u7684\u504f\u5dee\u5e76\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "result": "\u5728\u91cf\u5316\u611f\u77e5\u9884\u8bad\u7ec3\u4efb\u52a1\u4e2d\uff0cFOGZO\u5728\u6a21\u578b\u7cbe\u5ea6\u548c\u8bad\u7ec3\u65f6\u95f4\u4e4b\u95f4\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u6298\u8877\u3002\u5177\u4f53\u800c\u8a00\uff0c\u4e0eSTE\u76f8\u6bd4\uff0c\u5728\u76f8\u540c\u7684\u8fed\u4ee3\u6b21\u6570\u4e0b\uff0cDeiT Tiny/Small\u7684\u51c6\u786e\u7387\u63d0\u9ad8\u4e861-8%\uff0cResNet 18/50\u7684\u51c6\u786e\u7387\u63d0\u9ad8\u4e861-2%\uff0cLLaMA\u6a21\u578b\uff08\u591a\u8fbe0.3B\u53c2\u6570\uff09\u7684\u56f0\u60d1\u5ea6\u964d\u4f4e\u4e861-22\u4e2a\u70b9\u3002\u5728MNIST\u6570\u636e\u96c6\u4e0a\u76842\u5c42MLP\u5b9e\u9a8c\u4e2d\uff0cFOGZO\u5728\u8fbe\u5230\u76f8\u540c\u635f\u5931\u7684\u60c5\u51b5\u4e0b\uff0c\u8ba1\u7b97\u91cf\u6bd4n-SPSA\u51cf\u5c11\u4e86796\u500d\u3002", "conclusion": "FOGZO\u5728\u91cf\u5316\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u65b9\u9762\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5728\u4fdd\u8bc1\u6a21\u578b\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\u3002"}}
{"id": "2510.24478", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24478", "abs": "https://arxiv.org/abs/2510.24478", "authors": ["Frederik Broy", "Maike Z\u00fcfle", "Jan Niehues"], "title": "Talk2Ref: A Dataset for Reference Prediction from Scientific Talks", "comment": null, "summary": "Scientific talks are a growing medium for disseminating research, and\nautomatically identifying relevant literature that grounds or enriches a talk\nwould be highly valuable for researchers and students alike. We introduce\nReference Prediction from Talks (RPT), a new task that maps long, and\nunstructured scientific presentations to relevant papers. To support research\non RPT, we present Talk2Ref, the first large-scale dataset of its kind,\ncontaining 6,279 talks and 43,429 cited papers (26 per talk on average), where\nrelevance is approximated by the papers cited in the talk's corresponding\nsource publication. We establish strong baselines by evaluating\nstate-of-the-art text embedding models in zero-shot retrieval scenarios, and\npropose a dual-encoder architecture trained on Talk2Ref. We further explore\nstrategies for handling long transcripts, as well as training for domain\nadaptation. Our results show that fine-tuning on Talk2Ref significantly\nimproves citation prediction performance, demonstrating both the challenges of\nthe task and the effectiveness of our dataset for learning semantic\nrepresentations from spoken scientific content. The dataset and trained models\nare released under an open license to foster future research on integrating\nspoken scientific communication into citation recommendation systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4efb\u52a1\u201c\u4ece\u6f14\u8bb2\u4e2d\u9884\u6d4b\u53c2\u8003\u6587\u732e\u201d\uff08RPT\uff09\uff0c\u5e76\u53d1\u5e03\u4e86\u9996\u4e2a\u5927\u89c4\u6a21\u6570\u636e\u96c6Talk2Ref\uff0c\u65e8\u5728\u81ea\u52a8\u8bc6\u522b\u4e0e\u79d1\u5b66\u6f14\u8bb2\u76f8\u5173\u7684\u53c2\u8003\u6587\u732e\u3002", "motivation": "\u81ea\u52a8\u8bc6\u522b\u80fd\u652f\u6301\u6216\u4e30\u5bcc\u79d1\u5b66\u6f14\u8bb2\u7684\u6587\u732e\u5bf9\u7814\u7a76\u4eba\u5458\u548c\u5b66\u751f\u975e\u5e38\u6709\u4ef7\u503c\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u4ece\u6f14\u8bb2\u4e2d\u9884\u6d4b\u53c2\u8003\u6587\u732e\u201d\uff08RPT\uff09\u7684\u65b0\u4efb\u52a1\uff0c\u5e76\u6784\u5efa\u4e86Talk2Ref\u6570\u636e\u96c6\u3002\u4f7f\u7528\u6587\u672c\u5d4c\u5165\u6a21\u578b\u548c\u4e00\u79cd\u65b0\u7684\u53cc\u7f16\u7801\u5668\u67b6\u6784\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5e76\u63a2\u7d22\u4e86\u5904\u7406\u957f\u6587\u672c\u548c\u9886\u57df\u81ea\u9002\u5e94\u7684\u7b56\u7565\u3002", "result": "\u5728Talk2Ref\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5fae\u8c03\u663e\u8457\u63d0\u9ad8\u4e86\u5f15\u7528\u9884\u6d4b\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u8be5\u4efb\u52a1\u7684\u6311\u6218\u6027\u548c\u6570\u636e\u96c6\u5728\u5b66\u4e60\u53e3\u8bed\u79d1\u5b66\u5185\u5bb9\u8bed\u4e49\u8868\u793a\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "Talk2Ref\u6570\u636e\u96c6\u548c\u8bad\u7ec3\u597d\u7684\u6a21\u578b\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u8d44\u6e90\uff0c\u6709\u52a9\u4e8e\u5c06\u53e3\u8bed\u79d1\u5b66\u4ea4\u6d41\u6574\u5408\u5230\u5f15\u7528\u63a8\u8350\u7cfb\u7edf\u4e2d\u3002"}}
{"id": "2510.24414", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24414", "abs": "https://arxiv.org/abs/2510.24414", "authors": ["Reem Hammoud", "Abdul karim Gizzini", "Ali J. Ghandour"], "title": "XAI Evaluation Framework for Semantic Segmentation", "comment": null, "summary": "Ensuring transparency and trust in artificial intelligence (AI) models is\nessential, particularly as they are increasingly applied in safety-critical and\nhigh-stakes domains. Explainable AI (XAI) has emerged as a promising approach\nto address this challenge, yet the rigorous evaluation of XAI methods remains\ncrucial for optimizing the trade-offs between model complexity, predictive\nperformance, and interpretability. While extensive progress has been achieved\nin evaluating XAI techniques for classification tasks, evaluation strategies\ntailored to semantic segmentation remain relatively underexplored. This work\nintroduces a comprehensive and systematic evaluation framework specifically\ndesigned for assessing XAI in semantic segmentation, explicitly accounting for\nboth spatial and contextual task complexities. The framework employs\npixel-level evaluation strategies and carefully designed metrics to provide\nfine-grained interpretability insights. Simulation results using recently\nadapted class activation mapping (CAM)-based XAI schemes demonstrate the\nefficiency, robustness, and reliability of the proposed methodology. These\nfindings contribute to advancing transparent, trustworthy, and accountable\nsemantic segmentation models.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u8bed\u4e49\u5206\u5272\u7684XAI\u8bc4\u4f30\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5206\u7c7b\u4efb\u52a1\u4e0a\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u968f\u7740AI\u5728\u5173\u952e\u9886\u57df\u7684\u5e94\u7528\uff0c\u786e\u4fdd\u6a21\u578b\u7684\u900f\u660e\u5ea6\u548c\u53ef\u4fe1\u5ea6\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u7684XAI\u8bc4\u4f30\u65b9\u6cd5\u5728\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u4e0a\u4ecd\u9700\u6539\u8fdb\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b\u50cf\u7d20\u7ea7\u8bc4\u4f30\u7b56\u7565\u548c\u7cbe\u7ec6\u5316\u6307\u6807\u7684\u7efc\u5408\u6027\u8bc4\u4f30\u6846\u67b6\uff0c\u4ee5\u5e94\u5bf9\u8bed\u4e49\u5206\u5272\u7684\u7a7a\u95f4\u548c\u4e0a\u4e0b\u6587\u590d\u6742\u6027\u3002", "result": "\u4f7f\u7528\u57fa\u4e8eCAM\u7684XAI\u65b9\u6848\u8fdb\u884c\u7684\u6a21\u62df\u7ed3\u679c\u8bc1\u660e\u4e86\u8be5\u6846\u67b6\u5728\u6548\u7387\u3001\u9c81\u68d2\u6027\u548c\u53ef\u9760\u6027\u65b9\u9762\u7684\u4f18\u52bf\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u52a9\u4e8e\u63a8\u52a8\u66f4\u900f\u660e\u3001\u66f4\u53ef\u4fe1\u3001\u66f4\u8d1f\u8d23\u4efb\u7684\u8bed\u4e49\u5206\u5272\u6a21\u578b\u7684\u53d1\u5c55\u3002"}}
{"id": "2510.24488", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24488", "abs": "https://arxiv.org/abs/2510.24488", "authors": ["Katherine Abramski", "Giulio Rossetti", "Massimo Stella"], "title": "A word association network methodology for evaluating implicit biases in LLMs compared to humans", "comment": "24 pages, 13 figures, 3 tables", "summary": "As Large language models (LLMs) become increasingly integrated into our\nlives, their inherent social biases remain a pressing concern. Detecting and\nevaluating these biases can be challenging because they are often implicit\nrather than explicit in nature, so developing evaluation methods that assess\nthe implicit knowledge representations of LLMs is essential. We present a novel\nword association network methodology for evaluating implicit biases in LLMs\nbased on simulating semantic priming within LLM-generated word association\nnetworks. Our prompt-based approach taps into the implicit relational\nstructures encoded in LLMs, providing both quantitative and qualitative\nassessments of bias. Unlike most prompt-based evaluation methods, our method\nenables direct comparisons between various LLMs and humans, providing a\nvaluable point of reference and offering new insights into the alignment of\nLLMs with human cognition. To demonstrate the utility of our methodology, we\napply it to both humans and several widely used LLMs to investigate social\nbiases related to gender, religion, ethnicity, sexual orientation, and\npolitical party. Our results reveal both convergences and divergences between\nLLM and human biases, providing new perspectives on the potential risks of\nusing LLMs. Our methodology contributes to a systematic, scalable, and\ngeneralizable framework for evaluating and comparing biases across multiple\nLLMs and humans, advancing the goal of transparent and socially responsible\nlanguage technologies.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bcd\u8bed\u8054\u60f3\u7f51\u7edc\u65b9\u6cd5\u6765\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\u5b58\u5728\u7684\u9690\u6027\u793e\u4f1a\u504f\u89c1\uff0c\u901a\u8fc7\u6a21\u62df\u8bed\u4e49\u542f\u52a8\u6548\u5e94\u5e76\u5141\u8bb8\u4e0e\u4eba\u7c7b\u8fdb\u884c\u76f4\u63a5\u6bd4\u8f83\uff0c\u4e3aLLMs\u7684\u793e\u4f1a\u504f\u89c1\u8bc4\u4f30\u63d0\u4f9b\u4e86\u7cfb\u7edf\u3001\u53ef\u6269\u5c55\u7684\u6846\u67b6\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u793e\u4f1a\u504f\u89c1\u662f\u4e00\u4e2a\u65e5\u76ca\u4e25\u91cd\u7684\u95ee\u9898\uff0c\u4f46\u7531\u4e8e\u504f\u89c1\u5f80\u5f80\u662f\u9690\u6027\u7684\uff0c\u68c0\u6d4b\u548c\u8bc4\u4f30\u5177\u6709\u6311\u6218\u6027\u3002\u56e0\u6b64\uff0c\u6709\u5fc5\u8981\u5f00\u53d1\u8bc4\u4f30LLMs\u9690\u6027\u77e5\u8bc6\u8868\u5f81\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8bcd\u8bed\u8054\u60f3\u7f51\u7edc\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u62dfLLM\u751f\u6210\u8bcd\u8bed\u8054\u60f3\u7f51\u7edc\u4e2d\u7684\u8bed\u4e49\u542f\u52a8\u6548\u5e94\u6765\u8bc4\u4f30LLMs\u7684\u9690\u6027\u504f\u89c1\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u63d0\u793a\u6765\u6316\u6398LLMs\u4e2d\u7f16\u7801\u7684\u9690\u6027\u5173\u7cfb\u7ed3\u6784\uff0c\u5e76\u63d0\u4f9b\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u4f30\u3002\u8be5\u65b9\u6cd5\u8fd8\u80fd\u76f4\u63a5\u6bd4\u8f83\u4e0d\u540cLLMs\u4e0e\u4eba\u7c7b\u7684\u8868\u73b0\u3002", "result": "\u5c06\u8be5\u65b9\u6cd5\u5e94\u7528\u4e8e\u4eba\u7c7b\u548c\u591a\u4e2aLLMs\uff0c\u7814\u7a76\u4e86\u4e0e\u6027\u522b\u3001\u5b97\u6559\u3001\u79cd\u65cf\u3001\u6027\u53d6\u5411\u548c\u653f\u515a\u76f8\u5173\u7684\u793e\u4f1a\u504f\u89c1\u3002\u7ed3\u679c\u63ed\u793a\u4e86LLM\u4e0e\u4eba\u7c7b\u504f\u89c1\u4e4b\u95f4\u65e2\u6709\u5171\u6027\u4e5f\u6709\u5dee\u5f02\uff0c\u5e76\u63d0\u4f9b\u4e86\u5bf9\u4f7f\u7528LLMs\u6f5c\u5728\u98ce\u9669\u7684\u65b0\u89c1\u89e3\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u8bcd\u8bed\u8054\u60f3\u7f51\u7edc\u65b9\u6cd5\u4e3a\u8bc4\u4f30\u548c\u6bd4\u8f83\u591a\u4e2aLLMs\u548c\u4eba\u7c7b\u7684\u504f\u89c1\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7cfb\u7edf\u3001\u53ef\u6269\u5c55\u4e14\u53ef\u63a8\u5e7f\u7684\u6846\u67b6\uff0c\u6709\u52a9\u4e8e\u5b9e\u73b0\u900f\u660e\u4e14\u5bf9\u793e\u4f1a\u8d1f\u8d23\u4efb\u7684\u8bed\u8a00\u6280\u672f\u3002"}}
{"id": "2510.24437", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24437", "abs": "https://arxiv.org/abs/2510.24437", "authors": ["Zhineng Zhao", "Zhihai He", "Zikun Zhou", "Siwei Ma", "Yaowei Wang"], "title": "Deeply-Conditioned Image Compression via Self-Generated Priors", "comment": null, "summary": "Learned image compression (LIC) has shown great promise for achieving high\nrate-distortion performance. However, current LIC methods are often limited in\ntheir capability to model the complex correlation structures inherent in\nnatural images, particularly the entanglement of invariant global structures\nwith transient local textures within a single monolithic representation. This\nlimitation precipitates severe geometric deformation at low bitrates. To\naddress this, we introduce a framework predicated on functional decomposition,\nwhich we term Deeply-Conditioned Image Compression via self-generated priors\n(DCIC-sgp). Our central idea is to first encode a potent, self-generated prior\nto encapsulate the image's structural backbone. This prior is subsequently\nutilized not as mere side-information, but to holistically modulate the entire\ncompression pipeline. This deep conditioning, most critically of the analysis\ntransform, liberates it to dedicate its representational capacity to the\nresidual, high-entropy details. This hierarchical, dependency-driven approach\nachieves an effective disentanglement of information streams. Our extensive\nexperiments validate this assertion; visual analysis demonstrates that our\nmethod substantially mitigates the geometric deformation artifacts that plague\nconventional codecs at low bitrates. Quantitatively, our framework establishes\nhighly competitive performance, achieving significant BD-rate reductions of\n14.4%, 15.7%, and 15.1% against the VVC test model VTM-12.1 on the Kodak, CLIC,\nand Tecnick datasets.", "AI": {"tldr": "\u901a\u8fc7\u5f15\u5165\u57fa\u4e8e\u51fd\u6570\u5206\u89e3\u7684\u6df1\u5ea6\u6761\u4ef6\u56fe\u50cf\u538b\u7f29\u6846\u67b6\uff08DCIC-sgp\uff09\uff0c\u5229\u7528\u81ea\u751f\u6210\u5148\u9a8c\u6765\u5206\u79bb\u5168\u5c40\u7ed3\u6784\u548c\u5c40\u90e8\u7eb9\u7406\uff0c\u4ece\u800c\u5728\u4f4e\u6bd4\u7279\u7387\u4e0b\u663e\u8457\u51cf\u5c11\u51e0\u4f55\u53d8\u5f62\uff0c\u5e76\u5728\u4e0eVVC\u7684\u6bd4\u8f83\u4e2d\u53d6\u5f97\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u5b66\u4e60\u56fe\u50cf\u538b\u7f29\uff08LIC\uff09\u65b9\u6cd5\u5728\u6a21\u62df\u81ea\u7136\u56fe\u50cf\u7684\u590d\u6742\u76f8\u5173\u7ed3\u6784\uff08\u7279\u522b\u662f\u5168\u5c40\u7ed3\u6784\u4e0e\u5c40\u90e8\u7eb9\u7406\u7684\u7ea0\u7f20\uff09\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5bfc\u81f4\u4f4e\u6bd4\u7279\u7387\u4e0b\u51fa\u73b0\u4e25\u91cd\u7684\u51e0\u4f55\u53d8\u5f62\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u51fd\u6570\u5206\u89e3\u7684\u6846\u67b6\uff08DCIC-sgp\uff09\uff0c\u9996\u5148\u7f16\u7801\u4e00\u4e2a\u81ea\u751f\u6210\u7684\u5148\u9a8c\u6765\u6355\u6349\u56fe\u50cf\u7684\u7ed3\u6784\u9aa8\u5e72\uff0c\u7136\u540e\u5229\u7528\u8be5\u5148\u9a8c\u6574\u4f53\u8c03\u8282\u6574\u4e2a\u538b\u7f29\u6d41\u7a0b\uff0c\u7279\u522b\u662f\u5206\u6790\u53d8\u6362\uff0c\u4f7f\u5176\u80fd\u591f\u4e13\u6ce8\u4e8e\u8868\u793a\u6b8b\u5dee\u7ec6\u8282\uff0c\u4ece\u800c\u5b9e\u73b0\u4fe1\u606f\u6d41\u7684\u6709\u6548\u5206\u79bb\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4f4e\u6bd4\u7279\u7387\u4e0b\u80fd\u663e\u8457\u51cf\u5c11\u51e0\u4f55\u53d8\u5f62\u4f2a\u5f71\u3002\u5728Kodak\u3001CLIC\u548cTecnick\u6570\u636e\u96c6\u4e0a\uff0c\u4e0eVVC\u6d4b\u8bd5\u6a21\u578bVTM-12.1\u76f8\u6bd4\uff0cBD-rate\u5206\u522b\u964d\u4f4e\u4e8614.4%\u300115.7%\u548c15.1%\u3002", "conclusion": "DCIC-sgp\u6846\u67b6\u901a\u8fc7\u6df1\u5ea6\u6761\u4ef6\u5316\u548c\u4fe1\u606f\u6d41\u5206\u79bb\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edfLIC\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5728\u4f4e\u6bd4\u7279\u7387\u4e0b\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u5931\u771f\u6027\u80fd\uff0c\u5e76\u80fd\u663e\u8457\u51cf\u5c11\u51e0\u4f55\u53d8\u5f62\u3002"}}
{"id": "2510.23936", "categories": ["cs.LG", "physics.flu-dyn"], "pdf": "https://arxiv.org/pdf/2510.23936", "abs": "https://arxiv.org/abs/2510.23936", "authors": ["Junho Choi", "Teng-Yuan Chang", "Namjung Kim", "Youngjoon Hong"], "title": "A data free neural operator enabling fast inference of 2D and 3D Navier Stokes equations", "comment": null, "summary": "Ensemble simulations of high-dimensional flow models (e.g., Navier Stokes\ntype PDEs) are computationally prohibitive for real time applications. Neural\noperators enable fast inference but are limited by costly data requirements and\npoor generalization to 3D flows. We present a data-free operator network for\nthe Navier Stokes equations that eliminates the need for paired solution data\nand enables robust, real time inference for large ensemble forecasting. The\nphysics-grounded architecture takes initial and boundary conditions as well as\nforcing functions, yielding solutions robust to high variability and\nperturbations. Across 2D benchmarks and 3D test cases, the method surpasses\nprior neural operators in accuracy and, for ensembles, achieves greater\nefficiency than conventional numerical solvers. Notably, it delivers accurate\nsolutions of the three dimensional Navier Stokes equations, a regime not\npreviously demonstrated for data free neural operators. By uniting a\nnumerically grounded architecture with the scalability of machine learning,\nthis approach establishes a practical pathway toward data free, high fidelity\nPDE surrogates for end to end scientific simulation and prediction.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u914d\u5bf9\u6570\u636e\u5373\u53ef\u6c42\u89e3\u7eb3\u7ef4-\u65af\u6258\u514b\u65af\u65b9\u7a0b\u7684\u7b97\u5b50\u7f51\u7edc\uff0c\u5b9e\u73b0\u4e86\u5feb\u901f\u3001\u9c81\u68d2\u4e14\u5927\u89c4\u6a21\u7684\u9884\u6d4b\u3002", "motivation": "\u4f20\u7edf\u7684\u6d41\u52a8\u6a21\u578b\uff08\u5982\u7eb3\u7ef4-\u65af\u6258\u514b\u65af\u65b9\u7a0b\uff09\u7684\u96c6\u6210\u6a21\u62df\u5728\u8ba1\u7b97\u4e0a\u5bf9\u4e8e\u5b9e\u65f6\u5e94\u7528\u6765\u8bf4\u662f\u96be\u4ee5\u627f\u53d7\u7684\u3002\u867d\u7136\u795e\u7ecf\u7f51\u7edc\u7b97\u5b50\u53ef\u4ee5\u5feb\u901f\u63a8\u7406\uff0c\u4f46\u5b83\u4eec\u9700\u8981\u5927\u91cf\u6570\u636e\u5e76\u4e14\u5bf9\u4e09\u7ef4\u6d41\u7684\u6cdb\u5316\u80fd\u529b\u8f83\u5dee\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u914d\u5bf9\u6570\u636e\u5373\u53ef\u6c42\u89e3\u7eb3\u7ef4-\u65af\u6258\u514b\u65af\u65b9\u7a0b\u7684\u7b97\u5b50\u7f51\u7edc\u3002\u8be5\u7f51\u7edc\u7ed3\u5408\u4e86\u7269\u7406\u7ea6\u675f\u548c\u673a\u5668\u5b66\u4e60\u7684\u53ef\u6269\u5c55\u6027\uff0c\u53ef\u4ee5\u5904\u7406\u521d\u59cb\u6761\u4ef6\u3001\u8fb9\u754c\u6761\u4ef6\u548c\u5f3a\u5236\u51fd\u6570\uff0c\u4ece\u800c\u4ea7\u751f\u5bf9\u9ad8\u53d8\u7387\u548c\u6270\u52a8\u5177\u6709\u9c81\u68d2\u6027\u7684\u89e3\u3002", "result": "\u5728\u4e8c\u7ef4\u57fa\u51c6\u6d4b\u8bd5\u548c\u4e09\u7ef4\u6d4b\u8bd5\u7528\u4f8b\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u65b9\u9762\u4f18\u4e8e\u5148\u524d\u7684\u795e\u7ecf\u7f51\u7edc\u7b97\u5b50\uff0c\u5e76\u4e14\u5728\u96c6\u6210\u9884\u6d4b\u65b9\u9762\u6bd4\u4f20\u7edf\u7684\u6570\u503c\u6c42\u89e3\u5668\u5177\u6709\u66f4\u9ad8\u7684\u6548\u7387\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u5b83\u80fd\u591f\u51c6\u786e\u6c42\u89e3\u4e09\u7ef4\u7eb3\u7ef4-\u65af\u6258\u514b\u65af\u65b9\u7a0b\uff0c\u8fd9\u662f\u6b64\u524d\u6570\u636e\u81ea\u7531\u795e\u7ecf\u7f51\u7edc\u7b97\u5b50\u672a\u80fd\u5b9e\u73b0\u7684\u3002", "conclusion": "\u901a\u8fc7\u5c06\u57fa\u4e8e\u6570\u503c\u7684\u65b9\u6cd5\u4e0e\u673a\u5668\u5b66\u4e60\u7684\u53ef\u6269\u5c55\u6027\u76f8\u7ed3\u5408\uff0c\u8be5\u65b9\u6cd5\u4e3a\u65e0\u9700\u6570\u636e\u3001\u9ad8\u4fdd\u771f\u5ea6\u7684\u504f\u5fae\u5206\u65b9\u7a0b\u66ff\u4ee3\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u6761\u5b9e\u7528\u7684\u9014\u5f84\uff0c\u53ef\u7528\u4e8e\u7aef\u5230\u7aef\u7684\u79d1\u5b66\u6a21\u62df\u548c\u9884\u6d4b\u3002"}}
{"id": "2510.24505", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24505", "abs": "https://arxiv.org/abs/2510.24505", "authors": ["Qing Zong", "Jiayu Liu", "Tianshi Zheng", "Chunyang Li", "Baixuan Xu", "Haochen Shi", "Weiqi Wang", "Zhaowei Wang", "Chunkit Chan", "Yangqiu Song"], "title": "CritiCal: Can Critique Help LLM Uncertainty or Confidence Calibration?", "comment": null, "summary": "Accurate confidence calibration in Large Language Models (LLMs) is critical\nfor safe use in high-stakes domains, where clear verbalized confidence enhances\nuser trust. Traditional methods that mimic reference confidence expressions\noften fail to capture the reasoning needed for accurate confidence assessment.\nWe propose natural language critiques as a solution, ideally suited for\nconfidence calibration, as precise gold confidence labels are hard to obtain\nand often require multiple generations. This paper studies how natural language\ncritiques can enhance verbalized confidence, addressing: (1) What to critique:\nuncertainty (question-focused) or confidence (answer-specific)? Analysis shows\nconfidence suits multiple-choice tasks, while uncertainty excels in open-ended\nscenarios. (2) How to critique: self-critique or critique calibration training?\nWe propose Self-Critique, enabling LLMs to critique and optimize their\nconfidence beyond mere accuracy, and CritiCal, a novel Critique Calibration\ntraining method that leverages natural language critiques to improve confidence\ncalibration, moving beyond direct numerical optimization. Experiments show that\nCritiCal significantly outperforms Self-Critique and other competitive\nbaselines, even surpassing its teacher model, GPT-4o, in complex reasoning\ntasks. CritiCal also shows robust generalization in out-of-distribution\nsettings, advancing LLM's reliability.", "AI": {"tldr": "LLMs\u5728\u9700\u8981\u9ad8\u98ce\u9669\u9886\u57df\u5b89\u5168\u4f7f\u7528\u65f6\uff0c\u51c6\u786e\u7684\u7f6e\u4fe1\u5ea6\u6821\u51c6\u81f3\u5173\u91cd\u8981\u3002\u672c\u6587\u63d0\u51fa\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u6279\u8bc4\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86CritiCal\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u7684\u7f6e\u4fe1\u5ea6\u8868\u8fbe\u65b9\u6cd5\u5728\u6355\u6349\u7f6e\u4fe1\u5ea6\u8bc4\u4f30\u6240\u9700\u7684\u63a8\u7406\u65b9\u9762\u5e38\u5e38\u5931\u8d25\u3002\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u63d0\u9ad8LLM\u7684\u7f6e\u4fe1\u5ea6\u6821\u51c6\u3002", "method": "\u672c\u6587\u7814\u7a76\u4e86\u81ea\u7136\u8bed\u8a00\u6279\u8bc4\u5982\u4f55\u589e\u5f3aLLM\u7684\u7f6e\u4fe1\u5ea6\u8868\u8fbe\uff0c\u63a2\u8ba8\u4e86\u201c\u6279\u8bc4\u4ec0\u4e48\u201d\uff08\u4e0d\u786e\u5b9a\u6027\u6216\u7f6e\u4fe1\u5ea6\uff09\u548c\u201c\u5982\u4f55\u6279\u8bc4\u201d\uff08\u81ea\u6211\u6279\u8bc4\u6216\u6279\u8bc4\u6821\u51c6\u8bad\u7ec3\uff09\u3002\u63d0\u51fa\u4e86\u81ea\u6211\u6279\u8bc4\u548cCritiCal\uff08\u4e00\u79cd\u65b0\u9896\u7684\u6279\u8bc4\u6821\u51c6\u8bad\u7ec3\u65b9\u6cd5\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCritiCal\u5728\u7f6e\u4fe1\u5ea6\u6821\u51c6\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u81ea\u6211\u6279\u8bc4\u548c\u5176\u4ed6\u57fa\u7ebf\u65b9\u6cd5\uff0c\u751a\u81f3\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u8d85\u8d8a\u4e86\u5176\u6559\u5e08\u6a21\u578bGPT-4o\u3002CritiCal\u5728\u5206\u5e03\u5916\u8bbe\u7f6e\u4e2d\u4e5f\u8868\u73b0\u51fa\u9c81\u68d2\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u81ea\u7136\u8bed\u8a00\u6279\u8bc4\uff0c\u7279\u522b\u662fCritiCal\u65b9\u6cd5\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u9ad8LLM\u7684\u7f6e\u4fe1\u5ea6\u6821\u51c6\uff0c\u589e\u5f3a\u5176\u5728\u5173\u952e\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2510.24448", "categories": ["cs.CV", "cs.AI", "68T07, 68T45, 68T20", "I.2.10; I.4.8; I.5.1; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.24448", "abs": "https://arxiv.org/abs/2510.24448", "authors": ["Pablo Acuaviva", "Aram Davtyan", "Mariam Hassan", "Sebastian Stapf", "Ahmad Rahimi", "Alexandre Alahi", "Paolo Favaro"], "title": "Rethinking Visual Intelligence: Insights from Video Pretraining", "comment": "Updated version from preprint arXiv:2506.07280 (Gen2Gen) focused on\n  visual intelligence. This work can be considered as v2", "summary": "Large language models (LLMs) have demonstrated that large-scale pretraining\nenables systems to adapt rapidly to new problems with little supervision in the\nlanguage domain. This success, however, has not translated as effectively to\nthe visual domain, where models, including LLMs, continue to struggle with\ncompositional understanding, sample efficiency, and general-purpose\nproblem-solving. We investigate Video Diffusion Models (VDMs) as a promising\ndirection for bridging this gap. Pretraining on spatiotemporal data endows\nthese models with strong inductive biases for structure and dynamics, which we\nhypothesize can support broad task adaptability. To test this, we design a\ncontrolled evaluation in which both a pretrained LLM and a pretrained VDM are\nequipped with lightweight adapters and presented with tasks in their natural\nmodalities. Across benchmarks including ARC-AGI, ConceptARC, visual games,\nroute planning, and cellular automata, VDMs demonstrate higher data efficiency\nthan their language counterparts. Taken together, our results indicate that\nvideo pretraining offers inductive biases that support progress toward visual\nfoundation models.", "AI": {"tldr": "VDMs\u5728\u89c6\u89c9\u9886\u57df\u6bd4LLMs\u5177\u6709\u66f4\u597d\u7684\u6570\u636e\u6548\u7387\u548c\u4efb\u52a1\u9002\u5e94\u6027\u3002", "motivation": "LLMs\u5728\u89c6\u89c9\u9886\u57df\u5b58\u5728\u7ec4\u5408\u7406\u89e3\u3001\u6837\u672c\u6548\u7387\u548c\u901a\u7528\u95ee\u9898\u89e3\u51b3\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "method": "\u5728spatiotemporal\u6570\u636e\u4e0a\u9884\u8bad\u7ec3VDMs\uff0c\u5e76\u4e0e\u9884\u8bad\u7ec3\u7684LLM\u8fdb\u884c\u5bf9\u6bd4\u8bc4\u4f30\uff0c\u4e24\u8005\u90fd\u914d\u5907\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\uff0c\u5e76\u5728ARC-AGI\u3001ConceptARC\u3001\u89c6\u89c9\u6e38\u620f\u3001\u8def\u7ebf\u89c4\u5212\u548c\u7ec6\u80de\u81ea\u52a8\u673a\u7b49\u57fa\u51c6\u4e0a\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "VDMs\u5728\u6570\u636e\u6548\u7387\u65b9\u9762\u4f18\u4e8eLLMs\u3002", "conclusion": "\u89c6\u9891\u9884\u8bad\u7ec3\u4e3a\u5b9e\u73b0\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u5e0c\u671b\u7684\u5f52\u7eb3\u504f\u7f6e\u3002"}}
{"id": "2510.23940", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23940", "abs": "https://arxiv.org/abs/2510.23940", "authors": ["Anastasia-Maria Leventi-Peetz", "J\u00f6rg-Volker Peetz", "Kai Weber", "Nikolaos Zacharis"], "title": "Modeling Biological Multifunctionality with Echo State Networks", "comment": "26 pages, 17 figures, 6 tables, 23 references", "summary": "In this work, a three-dimensional multicomponent reaction-diffusion model has\nbeen developed, combining excitable-system dynamics with diffusion processes\nand sharing conceptual features with the FitzHugh-Nagumo model. Designed to\ncapture the spatiotemporal behavior of biological systems, particularly\nelectrophysiological processes, the model was solved numerically to generate\ntime-series data. These data were subsequently used to train and evaluate an\nEcho State Network (ESN), which successfully reproduced the system's dynamic\nbehavior. The results demonstrate that simulating biological dynamics using\ndata-driven, multifunctional ESN models is both feasible and effective.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u4e09\u7ef4\u591a\u7ec4\u5206\u53cd\u5e94-\u6269\u6563\u6a21\u578b\uff0c\u5e76\u4f7f\u7528\u56de\u58f0\u72b6\u6001\u7f51\u7edc\uff08ESN\uff09\u6210\u529f\u5730\u5bf9\u5176\u8fdb\u884c\u4e86\u6a21\u62df\uff0c\u8bc1\u660e\u4e86\u6570\u636e\u9a71\u52a8\u7684ESN\u5728\u6a21\u62df\u751f\u7269\u52a8\u529b\u5b66\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "motivation": "\u8be5\u6a21\u578b\u65e8\u5728\u6355\u6349\u751f\u7269\u7cfb\u7edf\uff08\u7279\u522b\u662f\u7535\u751f\u7406\u8fc7\u7a0b\uff09\u7684\u65f6\u7a7a\u884c\u4e3a\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u4e09\u7ef4\u591a\u7ec4\u5206\u53cd\u5e94-\u6269\u6563\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u7ed3\u5408\u4e86\u53ef\u5174\u594b\u7cfb\u7edf\u52a8\u529b\u5b66\u548c\u6269\u6563\u8fc7\u7a0b\uff0c\u5e76\u5177\u6709\u4e0e FitzHugh-Nagumo \u6a21\u578b\u76f8\u4f3c\u7684\u6982\u5ff5\u7279\u5f81\u3002\u7136\u540e\uff0c\u8be5\u6a21\u578b\u4f7f\u7528\u6570\u503c\u65b9\u6cd5\u6c42\u89e3\u4ee5\u751f\u6210\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0c\u5e76\u4f7f\u7528\u8fd9\u4e9b\u6570\u636e\u6765\u8bad\u7ec3\u548c\u8bc4\u4f30\u56de\u58f0\u72b6\u6001\u7f51\u7edc\uff08ESN\uff09\u3002", "result": "\u56de\u58f0\u72b6\u6001\u7f51\u7edc\uff08ESN\uff09\u6210\u529f\u5730\u518d\u73b0\u4e86\u7cfb\u7edf\u7684\u52a8\u6001\u884c\u4e3a\u3002", "conclusion": "\u7ed3\u679c\u8868\u660e\uff0c\u4f7f\u7528\u6570\u636e\u9a71\u52a8\u7684\u591a\u529f\u80fdESN\u6a21\u578b\u6765\u6a21\u62df\u751f\u7269\u52a8\u529b\u5b66\u662f\u53ef\u884c\u7684\u4e14\u6709\u6548\u7684\u3002"}}
{"id": "2510.24530", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24530", "abs": "https://arxiv.org/abs/2510.24530", "authors": ["Eric G. C. Laporte"], "title": "Lev\u00e9e d'ambigu\u00eft\u00e9s par grammaires locales", "comment": "in French language", "summary": "Many words are ambiguous in terms of their part of speech (POS). However,\nwhen a word appears in a text, this ambiguity is generally much reduced.\nDisambiguating POS involves using context to reduce the number of POS\nassociated with words, and is one of the main challenges of lexical tagging.\nThe problem of labeling words by POS frequently arises in natural language\nprocessing, for example for spelling correction, grammar or style checking,\nexpression recognition, text-to-speech conversion, text corpus analysis, etc.\nLexical tagging systems are thus useful as an initial component of many natural\nlanguage processing systems. A number of recent lexical tagging systems produce\nmultiple solutions when the text is lexically ambiguous or the uniquely correct\nsolution cannot be found. These contributions aim to guarantee a zero silence\nrate: the correct tag(s) for a word must never be discarded. This objective is\nunrealistic for systems that tag each word uniquely. This article concerns a\nlexical disambiguation method adapted to the objective of a zero silence rate\nand implemented in Silberztein's INTEX system (1993). We present here a formal\ndescription of this method. We show that to verify a local disambiguation\ngrammar in this framework, it is not sufficient to consider the transducer\npaths separately: one needs to verify their interactions. Similarly, if a\ncombination of multiple transducers is used, the result cannot be predicted by\nconsidering them in isolation. Furthermore, when examining the initial labeling\nof a text as produced by INTEX, ideas for disambiguation rules come\nspontaneously, but grammatical intuitions may turn out to be inaccurate, often\ndue to an unforeseen construction or ambiguity. If a zero silence rate is\ntargeted, local grammars must be carefully tested. This is where a detailed\nspecification of what a grammar will do once applied to texts would be\nnecessary.", "AI": {"tldr": "\u8bcd\u6027\u6807\u6ce8\u4e2d\u7684\u6b67\u4e49\u95ee\u9898\u53ef\u4ee5\u901a\u8fc7\u4e0a\u4e0b\u6587\u6d88\u6b67\uff0cINTEX\u7cfb\u7edf\u901a\u8fc7\u4e00\u79cd\u5c40\u90e8\u6d88\u6b67\u65b9\u6cd5\u6765\u5b9e\u73b0\u96f6\u9519\u8bef\u6f0f\u62a5\u7387\uff0c\u8be5\u65b9\u6cd5\u9700\u8981\u9a8c\u8bc1\u53d8\u6362\u8def\u5f84\u7684\u4ea4\u4e92\u4f5c\u7528\u3002", "motivation": "\u8bcd\u6027\u6807\u6ce8\u662f\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\u7684\u4e00\u4e2a\u6311\u6218\uff0c\u5e7f\u6cdb\u5e94\u7528\u4e8e\u62fc\u5199\u68c0\u67e5\u3001\u8bed\u6cd5\u68c0\u67e5\u7b49\u573a\u666f\u3002\u73b0\u6709\u7cfb\u7edf\u8ffd\u6c42\u96f6\u9519\u8bef\u6f0f\u62a5\u7387\uff0c\u5373\u4e0d\u4e22\u5f03\u4efb\u4f55\u6b63\u786e\u7684\u8bcd\u6027\u6807\u7b7e\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u9002\u7528\u4e8e\u96f6\u9519\u8bef\u6f0f\u62a5\u7387\u76ee\u6807\u7684\u8bcd\u6027\u6d88\u6b67\u65b9\u6cd5\uff0c\u5e76\u5728INTEX\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u3002\u8be5\u65b9\u6cd5\u9700\u8981\u9a8c\u8bc1\u5c40\u90e8\u6d88\u6b67\u6587\u6cd5\u7684\u53d8\u6362\u8def\u5f84\u7684\u4ea4\u4e92\u4f5c\u7528\uff0c\u800c\u975e\u5b64\u7acb\u5730\u8003\u8651\u3002", "result": "\u5728\u96f6\u9519\u8bef\u6f0f\u62a5\u7387\u7684\u76ee\u6807\u4e0b\uff0c\u9700\u8981\u4ed4\u7ec6\u6d4b\u8bd5\u5c40\u90e8\u6587\u6cd5\uff0c\u56e0\u4e3a\u5355\u72ec\u8003\u8651\u53d8\u6362\u8def\u5f84\u6216\u591a\u4e2a\u53d8\u6362\u5668\u7684\u7ec4\u5408\u65e0\u6cd5\u9884\u6d4b\u6574\u4f53\u7ed3\u679c\uff0c\u4e14\u6587\u6cd5\u89c4\u5219\u53ef\u80fd\u56e0\u672a\u9884\u89c1\u7684\u6784\u5efa\u6216\u6b67\u4e49\u800c\u53d8\u5f97\u4e0d\u51c6\u786e\u3002", "conclusion": "\u9a8c\u8bc1\u5c40\u90e8\u6d88\u6b67\u6587\u6cd5\u65f6\uff0c\u5fc5\u987b\u8003\u8651\u53d8\u6362\u8def\u5f84\u7684\u4ea4\u4e92\u4f5c\u7528\u3002\u82e5\u8981\u5b9e\u73b0\u96f6\u9519\u8bef\u6f0f\u62a5\u7387\uff0c\u5fc5\u987b\u4ed4\u7ec6\u6d4b\u8bd5\u5e76\u8be6\u7ec6\u8bf4\u660e\u6587\u6cd5\u5728\u5e94\u7528\u4e8e\u6587\u672c\u65f6\u7684\u884c\u4e3a\u3002"}}
{"id": "2510.24456", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24456", "abs": "https://arxiv.org/abs/2510.24456", "authors": ["Vivek Chetia", "Abdul Taher Khan", "Rahish Gogoi", "David Kapsian Khual", "Purnendu Bikash", "Sajal Saha"], "title": "A Critical Study towards the Detection of Parkinsons Disease using ML Technologies", "comment": null, "summary": "The proposed solution is Deep Learning Technique that will be able classify\nthree types of tea leaves diseases from which two diseases are caused by the\npests and one due to pathogens (infectious organisms) and environmental\nconditions and also show the area damaged by a disease in leaves. Namely Red\nRust, Helopeltis and Red spider mite respectively. In this paper we have\nevaluated two models namely SSD MobileNet V2 and Faster R-CNN ResNet50 V1 for\nthe object detection. The SSD MobileNet V2 gave precision of 0.209 for IOU\nrange of 0.50:0.95 with recall of 0.02 on IOU 0.50:0.95 and final mAP of 20.9%.\nWhile Faster R-CNN ResNet50 V1 has precision of 0.252 on IOU range of 0.50:0.95\nand recall of 0.044 on IOU of 0.50:0.95 with a mAP of 25%, which is better than\nSSD. Also used Mask R-CNN for Object Instance Segmentation where we have\nimplemented our custom method to calculate the damaged diseased portion of\nleaves. Keywords: Tea Leaf Disease, Deep Learning, Red Rust, Helopeltis and Red\nSpider Mite, SSD MobileNet V2, Faster R-CNN ResNet50 V1 and Mask RCNN.", "AI": {"tldr": "\u63d0\u51fa\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\uff0c\u7528\u4e8e\u8bc6\u522b\u8336\u6811\u53f6\u7247\u7684\u4e09\u79cd\u75be\u75c5\uff08\u4e24\u79cd\u7531\u5bb3\u866b\u5f15\u8d77\uff0c\u4e00\u79cd\u7531\u75c5\u539f\u4f53\u548c\u73af\u5883\u56e0\u7d20\u5f15\u8d77\uff09\uff0c\u5e76\u6807\u793a\u51fa\u53d7\u635f\u533a\u57df\u3002", "motivation": "\u4e3a\u8336\u6811\u53f6\u7247\u75be\u75c5\u7684\u81ea\u52a8\u5316\u8bc6\u522b\u548c\u53d7\u635f\u533a\u57df\u5206\u6790\u63d0\u4f9b\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u8bc4\u4f30\u4e86SSD MobileNet V2\u548cFaster R-CNN ResNet50 V1\u4e24\u79cd\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\uff0c\u5e76\u4f7f\u7528Mask R-CNN\u8fdb\u884c\u5b9e\u4f8b\u5206\u5272\u4ee5\u8ba1\u7b97\u53d7\u635f\u6bd4\u4f8b\u3002", "result": "Faster R-CNN ResNet50 V1\u6a21\u578b\u5728mAP\uff0825%\uff09\u4e0a\u4f18\u4e8eSSD MobileNet V2\uff0820.9%\uff09\u3002", "conclusion": "Faster R-CNN ResNet50 V1\u6a21\u578b\u5728\u8336\u6811\u53f6\u7247\u75be\u75c5\u68c0\u6d4b\u4efb\u52a1\u4e0a\u8868\u73b0\u66f4\u4f18\uff0c\u7ed3\u5408Mask R-CNN\u53ef\u5b9e\u73b0\u75c5\u53d8\u533a\u57df\u7684\u91cf\u5316\u5206\u6790\u3002"}}
{"id": "2510.23948", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23948", "abs": "https://arxiv.org/abs/2510.23948", "authors": ["Qianfeng Wen", "Zhenwei Tang", "Ashton Anderson"], "title": "ChessQA: Evaluating Large Language Models for Chess Understanding", "comment": "33 pages,8 figures", "summary": "Chess provides an ideal testbed for evaluating the reasoning, modeling, and\nabstraction capabilities of large language models (LLMs), as it has\nwell-defined structure and objective ground truth while admitting a wide\nspectrum of skill levels. However, existing evaluations of LLM ability in chess\nare ad hoc and narrow in scope, making it difficult to accurately measure LLM\nchess understanding and how it varies with scale, post-training methodologies,\nor architecture choices. We present ChessQA, a comprehensive benchmark that\nassesses LLM chess understanding across five task categories (Structural,\nMotifs, Short Tactics, Position Judgment, and Semantic), which approximately\ncorrespond to the ascending abstractions that players master as they accumulate\nchess knowledge, from understanding basic rules and learning tactical motifs to\ncorrectly calculating tactics, evaluating positions, and semantically\ndescribing high-level concepts. In this way, ChessQA captures a more\ncomprehensive picture of chess ability and understanding, going significantly\nbeyond the simple move quality evaluations done previously, and offers a\ncontrolled, consistent setting for diagnosis and comparison. Furthermore,\nChessQA is inherently dynamic, with prompts, answer keys, and construction\nscripts that can evolve as models improve. Evaluating a range of contemporary\nLLMs, we find persistent weaknesses across all five categories and provide\nresults and error analyses by category. We will release the code, periodically\nrefreshed datasets, and a public leaderboard to support further research.", "AI": {"tldr": "ChessQA\u662f\u4e00\u4e2a\u5168\u9762\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u56fd\u9645\u8c61\u68cb\u7406\u89e3\u80fd\u529b\uff0c\u6db5\u76d6\u4e94\u4e2a\u4efb\u52a1\u7c7b\u522b\uff0c\u5e76\u63d0\u4f9b\u9519\u8bef\u5206\u6790\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u5373\u4f7f\u662f\u5f53\u4ee3\u7684LLM\u5728\u6240\u6709\u7c7b\u522b\u4e2d\u4e5f\u5b58\u5728\u6301\u7eed\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u5bf9LLM\u5728\u56fd\u9645\u8c61\u68cb\u65b9\u9762\u7684\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u4e0d\u8db3\uff0c\u96be\u4ee5\u51c6\u786e\u8861\u91cf\u5176\u7406\u89e3\u80fd\u529b\u4ee5\u53ca\u80fd\u529b\u968f\u6a21\u578b\u89c4\u6a21\u3001\u8bad\u7ec3\u540e\u65b9\u6cd5\u6216\u67b6\u6784\u9009\u62e9\u7684\u53d8\u5316\u60c5\u51b5\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u4e2a\u66f4\u5168\u9762\u7684\u57fa\u51c6\u6765\u8bc4\u4f30LLM\u7684\u56fd\u9645\u8c61\u68cb\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86ChessQA\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8be5\u6d4b\u8bd5\u5305\u542b\u4e94\u4e2a\u4efb\u52a1\u7c7b\u522b\uff1a\u7ed3\u6784\u3001\u4e3b\u9898\u3001\u77ed\u6218\u672f\u3001\u5c40\u9762\u5224\u65ad\u548c\u8bed\u4e49\u3002\u8fd9\u4e9b\u7c7b\u522b\u5927\u81f4\u5bf9\u5e94\u4e8e\u68cb\u624b\u5728\u79ef\u7d2f\u56fd\u9645\u8c61\u68cb\u77e5\u8bc6\u65f6\u638c\u63e1\u7684\u9012\u589e\u62bd\u8c61\u6982\u5ff5\u3002\u8be5\u57fa\u51c6\u6d4b\u8bd5\u65e8\u5728\u63d0\u4f9b\u4e00\u4e2a\u53ef\u63a7\u3001\u4e00\u81f4\u7684\u8bc4\u4f30\u73af\u5883\uff0c\u5e76\u5305\u542b\u53ef\u52a8\u6001\u66f4\u65b0\u7684\u63d0\u793a\u3001\u7b54\u6848\u952e\u548c\u6784\u5efa\u811a\u672c\u3002", "result": "\u5728\u5bf9\u4e00\u7cfb\u5217\u5f53\u4ee3LLM\u8fdb\u884c\u8bc4\u4f30\u540e\uff0c\u53d1\u73b0\u5728\u6240\u6709\u4e94\u4e2a\u7c7b\u522b\u4e2d\u90fd\u5b58\u5728\u6301\u7eed\u7684\u4e0d\u8db3\u3002\u7814\u7a76\u63d0\u4f9b\u4e86\u6309\u7c7b\u522b\u5212\u5206\u7684\u7ed3\u679c\u548c\u9519\u8bef\u5206\u6790\u3002", "conclusion": "ChessQA\u4e3a\u8bc4\u4f30LLM\u7684\u56fd\u9645\u8c61\u68cb\u7406\u89e3\u80fd\u529b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u6846\u67b6\uff0c\u8d85\u8d8a\u4e86\u4ee5\u5f80\u7b80\u5355\u7684\u8d70\u5b50\u8d28\u91cf\u8bc4\u4f30\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5373\u4f7f\u662f\u5148\u8fdb\u7684LLM\u5728\u56fd\u9645\u8c61\u68cb\u7684\u5404\u4e2a\u65b9\u9762\u4e5f\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7684\u7814\u7a76\u6765\u6539\u8fdb\u5b83\u4eec\u7684\u8868\u73b0\u3002ChessQA\u7684\u52a8\u6001\u6027\u548c\u53ef\u6269\u5c55\u6027\u4f7f\u5176\u80fd\u591f\u9002\u5e94\u672a\u6765\u6a21\u578b\u7684\u53d1\u5c55\u3002"}}
{"id": "2510.24538", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24538", "abs": "https://arxiv.org/abs/2510.24538", "authors": ["Venkata S Govindarajan", "Laura Biester"], "title": "Dark & Stormy: Modeling Humor in the Worst Sentences Ever Written", "comment": null, "summary": "Textual humor is enormously diverse and computational studies need to account\nfor this range, including intentionally bad humor. In this paper, we curate and\nanalyze a novel corpus of sentences from the Bulwer-Lytton Fiction Contest to\nbetter understand \"bad\" humor in English. Standard humor detection models\nperform poorly on our corpus, and an analysis of literary devices finds that\nthese sentences combine features common in existing humor datasets (e.g., puns,\nirony) with metaphor, metafiction and simile. LLMs prompted to synthesize\ncontest-style sentences imitate the form but exaggerate the effect by\nover-using certain literary devices, and including far more novel\nadjective-noun bigrams than human writers. Data, code and analysis are\navailable at https://github.com/venkatasg/bulwer-lytton", "AI": {"tldr": "\u7814\u7a76\u4eba\u5458\u521b\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u201c\u7cdf\u7cd5\u201d\u5e7d\u9ed8\u4f8b\u5b50\u7684\u65b0\u8bed\u6599\u5e93\uff0c\u53d1\u73b0\u6807\u51c6\u5e7d\u9ed8\u6a21\u578b\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u7406\u89e3\u548c\u751f\u6210\u6b64\u7c7b\u5e7d\u9ed8\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "motivation": "\u4e3a\u4e86\u66f4\u597d\u5730\u7406\u89e3\u548c\u5904\u7406\u6587\u672c\u5e7d\u9ed8\u7684\u591a\u6837\u6027\uff0c\u7279\u522b\u662f\u201c\u7cdf\u7cd5\u201d\u7684\u5e7d\u9ed8\uff0c\u5e76\u4e3a\u8ba1\u7b97\u6a21\u578b\u63d0\u4f9b\u66f4\u5168\u9762\u7684\u6570\u636e\u96c6\u3002", "method": "\u6536\u96c6\u5e76\u5206\u6790\u4e86\u4fdd\u5c14-\u83b1\u987f\u5c0f\u8bf4\u7ade\u8d5b\uff08Bulwer-Lytton Fiction Contest\uff09\u7684\u53e5\u5b50\u8bed\u6599\u5e93\uff0c\u8bc4\u4f30\u4e86\u6807\u51c6\u5e7d\u9ed8\u68c0\u6d4b\u6a21\u578b\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u5904\u7406\u8be5\u8bed\u6599\u5e93\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u5206\u6790\u4e86\u5176\u4e2d\u4f7f\u7528\u7684\u6587\u5b66\u624b\u6cd5\u3002", "result": "\u6807\u51c6\u5e7d\u9ed8\u68c0\u6d4b\u6a21\u578b\u5728\u7cdf\u7cd5\u5e7d\u9ed8\u8bed\u6599\u5e93\u4e0a\u7684\u8868\u73b0\u4e0d\u4f73\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u8fd9\u4e9b\u53e5\u5b50\u7ed3\u5408\u4e86\u5e38\u89c1\u5e7d\u9ed8\u6570\u636e\u96c6\u4e2d\u7684\u7279\u5f81\uff08\u5982\u53cc\u5173\u3001\u8bbd\u523a\uff09\u4e0e\u9690\u55bb\u3001\u5143\u5c0f\u8bf4\u548c\u660e\u55bb\u7b49\u624b\u6cd5\u3002\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u867d\u7136\u80fd\u6a21\u4eff\u7ade\u8d5b\u98ce\u683c\u53e5\u5b50\u7684\u5f62\u5f0f\uff0c\u4f46\u4f1a\u8fc7\u5ea6\u4f7f\u7528\u67d0\u4e9b\u6587\u5b66\u624b\u6cd5\uff0c\u5e76\u751f\u6210\u6bd4\u4eba\u7c7b\u4f5c\u8005\u66f4\u591a\u7684\u3001\u65b0\u9896\u7684\u5f62\u5bb9\u8bcd-\u540d\u8bcd\u4e8c\u5143\u7ec4\u3002", "conclusion": "\u73b0\u6709\u7684\u5e7d\u9ed8\u68c0\u6d4b\u6a21\u578b\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7406\u89e3\u548c\u751f\u6210\u201c\u7cdf\u7cd5\u201d\u7684\u5e7d\u9ed8\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7684\u7814\u7a76\u6765\u6355\u6349\u5e7d\u9ed8\u7684\u590d\u6742\u6027\u548c\u591a\u6837\u6027\uff0c\u7279\u522b\u662f\u90a3\u4e9b\u6d89\u53ca\u591a\u79cd\u6587\u5b66\u624b\u6cd5\u7684\u5e7d\u9ed8\u3002"}}
{"id": "2510.24464", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24464", "abs": "https://arxiv.org/abs/2510.24464", "authors": ["Charles Javerliat", "Pierre Raimbaud", "Guillaume Lavou\u00e9"], "title": "Kineo: Calibration-Free Metric Motion Capture From Sparse RGB Cameras", "comment": null, "summary": "Markerless multiview motion capture is often constrained by the need for\nprecise camera calibration, limiting accessibility for non-experts and\nin-the-wild captures. Existing calibration-free approaches mitigate this\nrequirement but suffer from high computational cost and reduced reconstruction\naccuracy.\n  We present Kineo, a fully automatic, calibration-free pipeline for markerless\nmotion capture from videos captured by unsynchronized, uncalibrated,\nconsumer-grade RGB cameras. Kineo leverages 2D keypoints from off-the-shelf\ndetectors to simultaneously calibrate cameras, including Brown-Conrady\ndistortion coefficients, and reconstruct 3D keypoints and dense scene point\nmaps at metric scale. A confidence-driven spatio-temporal keypoint sampling\nstrategy, combined with graph-based global optimization, ensures robust\ncalibration at a fixed computational cost independent of sequence length. We\nfurther introduce a pairwise reprojection consensus score to quantify 3D\nreconstruction reliability for downstream tasks.\n  Evaluations on EgoHumans and Human3.6M demonstrate substantial improvements\nover prior calibration-free methods. Compared to previous state-of-the-art\napproaches, Kineo reduces camera translation error by approximately 83-85%,\ncamera angular error by 86-92%, and world mean-per-joint error (W-MPJPE) by\n83-91%.\n  Kineo is also efficient in real-world scenarios, processing multi-view\nsequences faster than their duration in specific configuration (e.g., 36min to\nprocess 1h20min of footage). The full pipeline and evaluation code are openly\nreleased to promote reproducibility and practical adoption at\nhttps://liris-xr.github.io/kineo/.", "AI": {"tldr": "Kineo\u662f\u4e00\u4e2a\u5168\u81ea\u52a8\u3001\u65e0\u9700\u6807\u5b9a\u7684\u591a\u89c6\u89d2\u52a8\u4f5c\u6355\u6349\u6d41\u7a0b\uff0c\u4f7f\u7528\u666e\u901aRGB\u76f8\u673a\u5373\u53ef\uff0c\u80fd\u540c\u65f6\u8fdb\u884c\u76f8\u673a\u6807\u5b9a\u548c3D\u91cd\u5efa\uff0c\u5e76\u63d0\u4f9b\u91cd\u5efa\u53ef\u9760\u6027\u8bc4\u4f30\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u5904\u7406\u901f\u5ea6\u5feb\u3002", "motivation": "\u73b0\u6709\u7684\u4e00\u7a3f\u591a\u5934\u52a8\u4f5c\u6355\u6349\u6280\u672f\u901a\u5e38\u9700\u8981\u7cbe\u786e\u7684\u76f8\u673a\u6807\u5b9a\uff0c\u8fd9\u9650\u5236\u4e86\u975e\u4e13\u4e1a\u7528\u6237\u548c\u91ce\u5916\u573a\u666f\u7684\u4f7f\u7528\u3002\u73b0\u6709\u7684\u65e0\u9700\u6807\u5b9a\u7684\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u4e14\u91cd\u5efa\u7cbe\u5ea6\u4e0d\u9ad8\u3002", "method": "Kineo\u5229\u7528\u73b0\u6210\u76842D\u5173\u952e\u70b9\u68c0\u6d4b\u5668\uff0c\u540c\u65f6\u8fdb\u884c\u76f8\u673a\u6807\u5b9a\uff08\u5305\u62ec\u76f8\u673a\u7578\u53d8\u7cfb\u6570\uff09\u548c3D\u5173\u952e\u70b9\u53ca\u7a20\u5bc6\u573a\u666f\u70b9\u56fe\u7684\u5ea6\u91cf\u5c3a\u5ea6\u91cd\u5efa\u3002\u901a\u8fc7\u7f6e\u4fe1\u5ea6\u9a71\u52a8\u7684\u65f6\u7a7a\u5173\u952e\u70b9\u91c7\u6837\u7b56\u7565\u548c\u57fa\u4e8e\u56fe\u7684\u5168\u5c40\u4f18\u5316\uff0c\u786e\u4fdd\u4e86\u9c81\u68d2\u7684\u6807\u5b9a\uff0c\u4e14\u8ba1\u7b97\u6210\u672c\u56fa\u5b9a\u3002\u6b64\u5916\uff0c\u5f15\u5165\u4e86\u6210\u5bf9\u91cd\u6295\u5f71\u4e00\u81f4\u6027\u5f97\u5206\u6765\u91cf\u53163D\u91cd\u5efa\u7684\u53ef\u9760\u6027\u3002", "result": "\u4e0e\u5148\u524d\u65e0\u9700\u6807\u5b9a\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0cKineo\u5728EgoHumans\u548cHuman3.6M\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u9ad8\u4e86\u76f8\u673a\u5e73\u79fb\u8bef\u5dee\uff08\u7ea683-85%\uff09\u3001\u76f8\u673a\u89d2\u5ea6\u8bef\u5dee\uff0886-92%\uff09\u548c\u4e16\u754c\u5e73\u5747\u5173\u8282\u8bef\u5dee\uff08W-MPJPE\uff09\uff0883-91%\uff09\u3002\u5728\u5b9e\u9645\u573a\u666f\u4e2d\uff0cKineo\u5904\u7406\u591a\u89c6\u89d2\u89c6\u9891\u7684\u901f\u5ea6\u6bd4\u89c6\u9891\u65f6\u957f\u8fd8\u5feb\u3002", "conclusion": "Kineo\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u51c6\u786e\u4e14\u6613\u4e8e\u4f7f\u7528\u7684\u65e0\u9700\u6807\u5b9a\u7684\u591a\u89c6\u89d2\u52a8\u4f5c\u6355\u6349\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u65b9\u9762\u5747\u8d85\u8d8a\u4e86\u73b0\u6709\u6280\u672f\uff0c\u5e76\u4e14\u901a\u8fc7\u5f00\u6e90\u4ee3\u7801\u4fc3\u8fdb\u4e86\u5176\u53ef\u590d\u73b0\u6027\u548c\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2510.23966", "categories": ["cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.23966", "abs": "https://arxiv.org/abs/2510.23966", "authors": ["Scott Emmons", "Roland S. Zimmermann", "David K. Elson", "Rohin Shah"], "title": "A Pragmatic Way to Measure Chain-of-Thought Monitorability", "comment": "The first two authors contributed equally", "summary": "While Chain-of-Thought (CoT) monitoring offers a unique opportunity for AI\nsafety, this opportunity could be lost through shifts in training practices or\nmodel architecture. To help preserve monitorability, we propose a pragmatic way\nto measure two components of it: legibility (whether the reasoning can be\nfollowed by a human) and coverage (whether the CoT contains all the reasoning\nneeded for a human to also produce the final output). We implement these\nmetrics with an autorater prompt that enables any capable LLM to compute the\nlegibility and coverage of existing CoTs. After sanity-checking our prompted\nautorater with synthetic CoT degradations, we apply it to several frontier\nmodels on challenging benchmarks, finding that they exhibit high\nmonitorability. We present these metrics, including our complete autorater\nprompt, as a tool for developers to track how design decisions impact\nmonitorability. While the exact prompt we share is still a preliminary version\nunder ongoing development, we are sharing it now in the hopes that others in\nthe community will find it useful. Our method helps measure the default\nmonitorability of CoT - it should be seen as a complement, not a replacement,\nfor the adversarial stress-testing needed to test robustness against\ndeliberately evasive models.", "AI": {"tldr": "CoT\u76d1\u63a7\u5bf9\u4e8eAI\u5b89\u5168\u5f88\u91cd\u8981\uff0c\u4f46\u53ef\u80fd\u56e0\u8bad\u7ec3\u6216\u6a21\u578b\u53d8\u5316\u800c\u4e22\u5931\u3002\u6211\u4eec\u63d0\u51fa\u4e86legibility\uff08\u53ef\u8bfb\u6027\uff09\u548ccoverage\uff08\u8986\u76d6\u5ea6\uff09\u4e24\u4e2a\u6307\u6807\u6765\u8861\u91cfCoT\u7684\u53ef\u76d1\u63a7\u6027\uff0c\u5e76\u7528\u4e00\u4e2a\u81ea\u52a8\u8bc4\u5206\u63d0\u793a\u6765\u8ba9LLM\u8ba1\u7b97\u8fd9\u4e9b\u6307\u6807\u3002\u8be5\u65b9\u6cd5\u5df2\u5728\u591a\u4e2a\u6a21\u578b\u4e0a\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c\u7ed3\u679c\u663e\u793a\u5b83\u4eec\u5177\u6709\u9ad8\u76d1\u63a7\u6027\u3002\u6211\u4eec\u5e0c\u671b\u8fd9\u4e9b\u6307\u6807\u548c\u5de5\u5177\u80fd\u5e2e\u52a9\u5f00\u53d1\u8005\u8ddf\u8e2a\u8bbe\u8ba1\u51b3\u7b56\u5bf9\u76d1\u63a7\u6027\u7684\u5f71\u54cd\uff0c\u5e76\u9f13\u52b1\u793e\u533a\u5171\u540c\u5f00\u53d1\u5b8c\u5584\u3002", "motivation": "CoT\u76d1\u63a7\u4e3aAI\u5b89\u5168\u63d0\u4f9b\u4e86\u72ec\u7279\u673a\u4f1a\uff0c\u4f46\u53ef\u80fd\u56e0\u8bad\u7ec3\u5b9e\u8df5\u6216\u6a21\u578b\u67b6\u6784\u7684\u53d8\u5316\u800c\u4e27\u5931\u3002\u4e3a\u4e86\u5e2e\u52a9\u4fdd\u6301\u53ef\u76d1\u63a7\u6027\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u8861\u91cf\u5176\u5173\u952e\u7ec4\u6210\u90e8\u5206\u3002", "method": "\u63d0\u51fa\u4e86legibility\uff08\u53ef\u8bfb\u6027\uff09\u548ccoverage\uff08\u8986\u76d6\u5ea6\uff09\u4e24\u4e2a\u6307\u6807\u6765\u91cf\u5316CoT\u7684\u53ef\u76d1\u63a7\u6027\u3002\u5f00\u53d1\u4e86\u4e00\u4e2a\u81ea\u52a8\u8bc4\u5206\u63d0\u793a\uff0c\u5229\u7528\u73b0\u6709\u7684LLM\u6765\u8ba1\u7b97\u8fd9\u4e9b\u6307\u6807\u3002\u901a\u8fc7\u5408\u6210CoT\u9000\u5316\u8fdb\u884c\u5065\u5168\u6027\u68c0\u67e5\uff0c\u5e76\u5728\u591a\u4e2a\u524d\u6cbf\u6a21\u578b\u548c\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u5e94\u7528\u8be5\u65b9\u6cd5\u3002", "result": "\u5728\u5bf9\u591a\u4e2a\u524d\u6cbf\u6a21\u578b\u8fdb\u884c\u8bc4\u4f30\u540e\uff0c\u53d1\u73b0\u5b83\u4eec\u8868\u73b0\u51fa\u9ad8\u6c34\u5e73\u7684\u76d1\u63a7\u6027\u3002\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u7684\u81ea\u52a8\u8bc4\u5206\u63d0\u793a\u5728\u8861\u91cfCoT\u53ef\u76d1\u63a7\u6027\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "legibility\u548ccoverage\u662f\u8861\u91cfCoT\u53ef\u76d1\u63a7\u6027\u7684\u91cd\u8981\u6307\u6807\u3002\u6240\u63d0\u51fa\u7684\u81ea\u52a8\u8bc4\u5206\u63d0\u793a\u662f\u4e00\u79cd\u5b9e\u7528\u7684\u5de5\u5177\uff0c\u53ef\u7528\u4e8e\u8bc4\u4f30\u548c\u8ddf\u8e2aLLM\u5728CoT\u65b9\u9762\u7684\u53ef\u76d1\u63a7\u6027\u3002\u8be5\u65b9\u6cd5\u662f\u5bf9\u6297\u6027\u538b\u529b\u6d4b\u8bd5\u7684\u8865\u5145\uff0c\u800c\u975e\u66ff\u4ee3\u54c1\u3002"}}
{"id": "2510.24541", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24541", "abs": "https://arxiv.org/abs/2510.24541", "authors": ["Seyoung Song", "Nawon Kim", "Songeun Chae", "Kiwoong Park", "Jiho Jin", "Haneul Yoo", "Kyunghyun Cho", "Alice Oh"], "title": "Open Korean Historical Corpus: A Millennia-Scale Diachronic Collection of Public Domain Texts", "comment": "Dataset and code available at https://github.com/seyoungsong/OKHC", "summary": "The history of the Korean language is characterized by a discrepancy between\nits spoken and written forms and a pivotal shift from Chinese characters to the\nHangul alphabet. However, this linguistic evolution has remained largely\nunexplored in NLP due to a lack of accessible historical corpora. To address\nthis gap, we introduce the Open Korean Historical Corpus, a large-scale, openly\nlicensed dataset spanning 1,300 years and 6 languages, as well as\nunder-represented writing systems like Korean-style Sinitic (Idu) and\nHanja-Hangul mixed script. This corpus contains 18 million documents and 5\nbillion tokens from 19 sources, ranging from the 7th century to 2025. We\nleverage this resource to quantitatively analyze major linguistic shifts: (1)\nIdu usage peaked in the 1860s before declining sharply; (2) the transition from\nHanja to Hangul was a rapid transformation starting around 1890; and (3) North\nKorea's lexical divergence causes modern tokenizers to produce up to 51 times\nhigher out-of-vocabulary rates. This work provides a foundational resource for\nquantitative diachronic analysis by capturing the history of the Korean\nlanguage. Moreover, it can serve as a pre-training corpus for large language\nmodels, potentially improving their understanding of Sino-Korean vocabulary in\nmodern Hangul as well as archaic writing systems.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u5f00\u653e\u97e9\u8bed\u5386\u53f2\u8bed\u6599\u5e93\uff0c\u8fd9\u662f\u4e00\u4e2a\u5305\u542b1800\u4e07\u4efd\u6587\u6863\u548c50\u4ebf\u8bcd\u8bed\u7684\u3001\u6db5\u76d61300\u5e74\u5386\u53f2\u548c6\u79cd\u8bed\u8a00\u7684\u5927\u578b\u6570\u636e\u96c6\uff0c\u65e8\u5728\u89e3\u51b3\u81ea\u7136\u8bed\u8a00\u5904\u7406\u9886\u57df\u4e2d\u5386\u53f2\u97e9\u8bed\u7814\u7a76\u7684\u8bed\u6599\u5e93\u7f3a\u4e4f\u95ee\u9898\u3002", "motivation": "\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u9886\u57df\u5bf9\u97e9\u8bed\u6f14\u53d8\uff0c\u7279\u522b\u662f\u5176\u53e3\u8bed\u548c\u4e66\u9762\u8bed\u4e4b\u95f4\u7684\u5dee\u5f02\u4ee5\u53ca\u4ece\u6c49\u5b57\u5230\u8c1a\u6587\uff08Hangul\uff09\u7684\u8f6c\u53d8\u7684\u7814\u7a76\u4e0d\u8db3\uff0c\u539f\u56e0\u662f\u7f3a\u4e4f\u53ef\u7528\u7684\u5386\u53f2\u8bed\u6599\u5e93\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b1800\u4e07\u4efd\u6587\u6863\u548c50\u4ebf\u8bcd\u8bed\u3001\u8de8\u8d8a1300\u5e74\uff087\u4e16\u7eaa\u81f32025\u5e74\uff09\u548c6\u79cd\u8bed\u8a00\u7684\u5927\u578b\u3001\u5f00\u653e\u8bb8\u53ef\u7684\u201c\u5f00\u653e\u97e9\u8bed\u5386\u53f2\u8bed\u6599\u5e93\u201d\uff0c\u5e76\u5229\u7528\u8be5\u8bed\u6599\u5e93\u5b9a\u91cf\u5206\u6790\u4e86\u97e9\u8bed\u7684\u4e3b\u8981\u8bed\u8a00\u6f14\u53d8\uff1a1. \u540f\u8bfb\uff08Idu\uff09\u4f7f\u7528\u91cf\u572819\u4e16\u7eaa60\u5e74\u4ee3\u8fbe\u5230\u9876\u5cf0\u540e\u6025\u5267\u4e0b\u964d\uff1b2. \u6c49\u5b57\uff08Hanja\uff09\u5230\u8c1a\u6587\uff08Hangul\uff09\u7684\u8f6c\u53d8\u59cb\u4e8e1890\u5e74\u5de6\u53f3\u5e76\u8fc5\u901f\u5b8c\u6210\uff1b3. \u671d\u9c9c\u5317\u65b9\u8bcd\u6c47\u5206\u6b67\u5bfc\u81f4\u73b0\u4ee3\u5206\u8bcd\u5668\u4ea7\u751f\u9ad8\u8fbe51\u500d\u7684\u8bcd\u5916\uff08OOV\uff09\u9519\u8bef\u7387\u3002", "result": "1. \u540f\u8bfb\uff08Idu\uff09\u4f7f\u7528\u91cf\u572819\u4e16\u7eaa60\u5e74\u4ee3\u8fbe\u5230\u9876\u5cf0\u540e\u6025\u5267\u4e0b\u964d\uff1b2. \u6c49\u5b57\uff08Hanja\uff09\u5230\u8c1a\u6587\uff08Hangul\uff09\u7684\u8f6c\u53d8\u59cb\u4e8e1890\u5e74\u5de6\u53f3\u5e76\u8fc5\u901f\u5b8c\u6210\uff1b3. \u671d\u9c9c\u5317\u65b9\u8bcd\u6c47\u5206\u6b67\u5bfc\u81f4\u73b0\u4ee3\u5206\u8bcd\u5668\u4ea7\u751f\u9ad8\u8fbe51\u500d\u7684\u8bcd\u5916\uff08OOV\uff09\u9519\u8bef\u7387\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u5f00\u653e\u97e9\u8bed\u5386\u53f2\u8bed\u6599\u5e93\u4e3a\u97e9\u8bed\u5386\u53f2\u5b9a\u91cf\u5206\u6790\u63d0\u4f9b\u4e86\u57fa\u7840\u8d44\u6e90\uff0c\u5e76\u53ef\u4f5c\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u9884\u8bad\u7ec3\u8bed\u6599\u5e93\uff0c\u6709\u671b\u63d0\u9ad8\u5176\u5bf9\u73b0\u4ee3\u8c1a\u6587\u4e2d\u6c49\u97e9\u8bcd\u6c47\u548c\u53e4\u4ee3\u4e66\u5199\u7cfb\u7edf\u7684\u7406\u89e3\u80fd\u529b\u3002"}}
{"id": "2510.24474", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24474", "abs": "https://arxiv.org/abs/2510.24474", "authors": ["Kyungmin Lee", "Sihyun Yu", "Jinwoo Shin"], "title": "Decoupled MeanFlow: Turning Flow Models into Flow Maps for Accelerated Sampling", "comment": null, "summary": "Denoising generative models, such as diffusion and flow-based models, produce\nhigh-quality samples but require many denoising steps due to discretization\nerror. Flow maps, which estimate the average velocity between timesteps,\nmitigate this error and enable faster sampling. However, their training\ntypically demands architectural changes that limit compatibility with\npretrained flow models. We introduce Decoupled MeanFlow, a simple decoding\nstrategy that converts flow models into flow map models without architectural\nmodifications. Our method conditions the final blocks of diffusion transformers\non the subsequent timestep, allowing pretrained flow models to be directly\nrepurposed as flow maps. Combined with enhanced training techniques, this\ndesign enables high-quality generation in as few as 1 to 4 steps. Notably, we\nfind that training flow models and subsequently converting them is more\nefficient and effective than training flow maps from scratch. On ImageNet\n256x256 and 512x512, our models attain 1-step FID of 2.16 and 2.12,\nrespectively, surpassing prior art by a large margin. Furthermore, we achieve\nFID of 1.51 and 1.68 when increasing the steps to 4, which nearly matches the\nperformance of flow models while delivering over 100x faster inference.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2510.23972", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23972", "abs": "https://arxiv.org/abs/2510.23972", "authors": ["Andra\u017e Jelin\u010di\u010d", "Owen Lockwood", "Akhil Garlapati", "Guillaume Verdon", "Trevor McCourt"], "title": "An efficient probabilistic hardware architecture for diffusion-like models", "comment": "9 pages, 6 figures", "summary": "The proliferation of probabilistic AI has promoted proposals for specialized\nstochastic computers. Despite promising efficiency gains, these proposals have\nfailed to gain traction because they rely on fundamentally limited modeling\ntechniques and exotic, unscalable hardware. In this work, we address these\nshortcomings by proposing an all-transistor probabilistic computer that\nimplements powerful denoising models at the hardware level. A system-level\nanalysis indicates that devices based on our architecture could achieve\nperformance parity with GPUs on a simple image benchmark using approximately\n10,000 times less energy.", "AI": {"tldr": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u5168\u6676\u4f53\u7ba1\u6982\u7387\u8ba1\u7b97\u673a\uff0c\u53ef\u4ee5\u5728\u786c\u4ef6\u5c42\u9762\u5b9e\u73b0\u5f3a\u5927\u7684\u53bb\u566a\u6a21\u578b\uff0c\u5e76\u53ef\u80fd\u4ee5\u66f4\u4f4e\u7684\u80fd\u8017\u8fbe\u5230\u4e0eGPU\u76f8\u5f53\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u4e13\u7528\u968f\u673a\u8ba1\u7b97\u7684\u63d0\u8bae\u56e0\u5176\u6709\u9650\u7684\u5efa\u6a21\u6280\u672f\u548c\u4e0d\u53ef\u6269\u5c55\u7684\u786c\u4ef6\u800c\u672a\u80fd\u666e\u53ca\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5168\u6676\u4f53\u7ba1\u6982\u7387\u8ba1\u7b97\u673a\uff0c\u8be5\u8ba1\u7b97\u673a\u5728\u786c\u4ef6\u5c42\u9762\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u53bb\u566a\u6a21\u578b\u3002", "result": "\u57fa\u4e8e\u8be5\u67b6\u6784\u7684\u8bbe\u5907\u5728\u7b80\u5355\u7684\u56fe\u50cf\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u80fd\u8017\u6709\u671b\u8fbe\u5230GPU\u76841/10000\uff0c\u540c\u65f6\u5b9e\u73b0\u6027\u80fd\u76f8\u5f53\u3002", "conclusion": "\u6211\u4eec\u63d0\u51fa\u7684\u5168\u6676\u4f53\u7ba1\u6982\u7387\u8ba1\u7b97\u673a\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u80fd\u6548\u65b9\u9762\u663e\u793a\u51fa\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2510.24570", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24570", "abs": "https://arxiv.org/abs/2510.24570", "authors": ["Rapha\u00ebl Bagat", "Irina Illina", "Emmanuel Vincent"], "title": "BEST-RQ-Based Self-Supervised Learning for Whisper Domain Adaptation", "comment": "Submitted to ICASSP 2026", "summary": "Automatic Speech Recognition (ASR) systems, despite large multilingual\ntraining, struggle in out-of-domain and low-resource scenarios where labeled\ndata is scarce. We propose BEARD (BEST-RQ Encoder Adaptation with Re-training\nand Distillation), a novel framework designed to adapt Whisper's encoder using\nunlabeled data. Unlike traditional self-supervised learning methods, BEARD\nuniquely combines a BEST-RQ objective with knowledge distillation from a frozen\nteacher encoder, ensuring the encoder's complementarity with the pre-trained\ndecoder. Our experiments focus on the ATCO2 corpus from the challenging Air\nTraffic Control (ATC) communications domain, characterized by non-native\nspeech, noise, and specialized phraseology. Using about 5,000 hours of\nuntranscribed speech for BEARD and 2 hours of transcribed speech for\nfine-tuning, the proposed approach significantly outperforms previous baseline\nand fine-tuned model, achieving a relative improvement of 12% compared to the\nfine-tuned model. To the best of our knowledge, this is the first work to use a\nself-supervised learning objective for domain adaptation of Whisper.", "AI": {"tldr": "BEARD\u6846\u67b6\u5229\u7528\u65e0\u6807\u7b7e\u6570\u636e\u81ea\u76d1\u7763\u5b66\u4e60\u9002\u5e94Whisper\u7684\u7f16\u7801\u5668\uff0c\u5728\u4f4e\u8d44\u6e90\u548c\u975e\u8bcd\u6c47\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "motivation": "\u73b0\u6709\u7684\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u7cfb\u7edf\u5728\u6570\u636e\u7a00\u758f\u7684\u9886\u57df\u5916\u548c\u4f4e\u8d44\u6e90\u573a\u666f\u4e0b\u8868\u73b0\u4e0d\u4f73\u3002", "method": "BEARD\u6846\u67b6\u7ed3\u5408\u4e86BEST-RQ\u76ee\u6807\u548c\u77e5\u8bc6\u84b8\u998f\uff0c\u5229\u7528\u65e0\u6807\u7b7e\u6570\u636e\u548c\u51bb\u7ed3\u7684\u6559\u5e08\u7f16\u7801\u5668\u6765\u9002\u5e94Whisper\u7684\u7f16\u7801\u5668\u3002", "result": "\u5728ATCO2\u8bed\u6599\u5e93\u4e0a\uff0cBEARD\u53d6\u5f97\u4e8612%\u7684\u76f8\u5bf9\u6027\u80fd\u63d0\u5347\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u548c\u5fae\u8c03\u6a21\u578b\u3002", "conclusion": "BEARD\u662f\u9996\u4e2a\u4f7f\u7528\u81ea\u76d1\u7763\u5b66\u4e60\u76ee\u6807\u8fdb\u884cWhisper\u9886\u57df\u81ea\u9002\u5e94\u7684\u7814\u7a76\uff0c\u80fd\u6709\u6548\u63d0\u5347ASR\u7cfb\u7edf\u5728\u6311\u6218\u6027\u573a\u666f\u4e0b\u7684\u6027\u80fd\u3002"}}
{"id": "2510.23974", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23974", "abs": "https://arxiv.org/abs/2510.23974", "authors": ["Byeonghu Na", "Minsang Park", "Gyuwon Sim", "Donghyeok Shin", "HeeSun Bae", "Mina Kang", "Se Jung Kwon", "Wanmo Kang", "Il-Chul Moon"], "title": "Diffusion Adaptive Text Embedding for Text-to-Image Diffusion Models", "comment": "Accepted at NeurIPS 2025", "summary": "Text-to-image diffusion models rely on text embeddings from a pre-trained\ntext encoder, but these embeddings remain fixed across all diffusion timesteps,\nlimiting their adaptability to the generative process. We propose Diffusion\nAdaptive Text Embedding (DATE), which dynamically updates text embeddings at\neach diffusion timestep based on intermediate perturbed data. We formulate an\noptimization problem and derive an update rule that refines the text embeddings\nat each sampling step to improve alignment and preference between the mean\npredicted image and the text. This allows DATE to dynamically adapts the text\nconditions to the reverse-diffused images throughout diffusion sampling without\nrequiring additional model training. Through theoretical analysis and empirical\nresults, we show that DATE maintains the generative capability of the model\nwhile providing superior text-image alignment over fixed text embeddings across\nvarious tasks, including multi-concept generation and text-guided image\nediting. Our code is available at https://github.com/aailab-kaist/DATE.", "AI": {"tldr": "DATE \u901a\u8fc7\u5728\u6bcf\u4e2a\u6269\u6563\u65f6\u95f4\u6b65\u957f\u52a8\u6001\u66f4\u65b0\u6587\u672c\u5d4c\u5165\u6765\u63d0\u9ad8\u6587\u672c-\u56fe\u50cf\u751f\u6210\u6a21\u578b\u7684\u5bf9\u9f50\u5ea6\u548c\u504f\u597d\u5ea6\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "motivation": "\u73b0\u6709\u7684\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u4f7f\u7528\u56fa\u5b9a\u7684\u6587\u672c\u5d4c\u5165\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u9002\u5e94\u6027\u3002 DATE \u65e8\u5728\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u901a\u8fc7\u52a8\u6001\u66f4\u65b0\u6587\u672c\u5d4c\u5165\u6765\u6539\u8fdb\u751f\u6210\u8d28\u91cf\u3002", "method": "DATE \u52a8\u6001\u66f4\u65b0\u6587\u672c\u5d4c\u5165\uff0c\u4ee5\u4f18\u5316\u4e2d\u95f4\u6270\u52a8\u6570\u636e\u4e0e\u6587\u672c\u7684\u5bf9\u9f50\u3002\u5b83\u901a\u8fc7\u6c42\u89e3\u4e00\u4e2a\u4f18\u5316\u95ee\u9898\u5e76\u63a8\u5bfc\u51fa\u4e00\u4e2a\u66f4\u65b0\u89c4\u5219\u6765\u5b9e\u73b0\u8fd9\u4e00\u70b9\uff0c\u8be5\u89c4\u5219\u5728\u6bcf\u6b21\u91c7\u6837\u65f6\u90fd\u4f1a\u6539\u8fdb\u6587\u672c\u5d4c\u5165\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDATE \u5728\u4fdd\u6301\u6a21\u578b\u751f\u6210\u80fd\u529b\u7684\u540c\u65f6\uff0c\u5728\u591a\u6982\u5ff5\u751f\u6210\u548c\u6587\u672c\u5f15\u5bfc\u7684\u56fe\u50cf\u7f16\u8f91\u7b49\u4efb\u52a1\u4e2d\u63d0\u4f9b\u4e86\u6bd4\u56fa\u5b9a\u6587\u672c\u5d4c\u5165\u66f4\u4f18\u8d8a\u7684\u6587\u672c-\u56fe\u50cf\u5bf9\u9f50\u6548\u679c\u3002", "conclusion": "DATE \u662f\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u901a\u8fc7\u52a8\u6001\u66f4\u65b0\u6587\u672c\u5d4c\u5165\u6765\u63d0\u9ad8\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7684\u6027\u80fd\uff0c\u800c\u65e0\u9700\u8fdb\u884c\u989d\u5916\u7684\u6a21\u578b\u8bad\u7ec3\u3002"}}
{"id": "2510.24591", "categories": ["cs.CL", "astro-ph.IM"], "pdf": "https://arxiv.org/pdf/2510.24591", "abs": "https://arxiv.org/abs/2510.24591", "authors": ["Christine Ye", "Sihan Yuan", "Suchetha Cooray", "Steven Dillmann", "Ian L. V. Roque", "Dalya Baron", "Philipp Frank", "Sergio Martin-Alvarez", "Nolan Koblischke", "Frank J Qu", "Diyi Yang", "Risa Wechsler", "Ioana Ciuca"], "title": "ReplicationBench: Can AI Agents Replicate Astrophysics Research Papers?", "comment": null, "summary": "Frontier AI agents show increasing promise as scientific research assistants,\nand may eventually be useful for extended, open-ended research workflows.\nHowever, in order to use agents for novel research, we must first assess the\nunderlying faithfulness and correctness of their work. To evaluate agents as\nresearch assistants, we introduce ReplicationBench, an evaluation framework\nthat tests whether agents can replicate entire research papers drawn from the\nastrophysics literature. Astrophysics, where research relies heavily on\narchival data and computational study while requiring little real-world\nexperimentation, is a particularly useful testbed for AI agents in scientific\nresearch. We split each paper into tasks which require agents to replicate the\npaper's core contributions, including the experimental setup, derivations, data\nanalysis, and codebase. Each task is co-developed with the original paper\nauthors and targets a key scientific result, enabling objective evaluation of\nboth faithfulness (adherence to original methods) and correctness (technical\naccuracy of results). ReplicationBench is extremely challenging for current\nfrontier language models: even the best-performing language models score under\n20%. We analyze ReplicationBench trajectories in collaboration with domain\nexperts and find a rich, diverse set of failure modes for agents in scientific\nresearch. ReplicationBench establishes the first benchmark of paper-scale,\nexpert-validated astrophysics research tasks, reveals insights about agent\nperformance generalizable to other domains of data-driven science, and provides\na scalable framework for measuring AI agents' reliability in scientific\nresearch.", "AI": {"tldr": "AI\u4ee3\u7406\u5728\u79d1\u5b66\u7814\u7a76\u4e2d\u7684\u5e94\u7528\u4ecd\u9762\u4e34\u4e25\u5cfb\u6311\u6218\uff0c\u5c24\u5176\u5728\u590d\u73b0\u5929\u4f53\u7269\u7406\u5b66\u7814\u7a76\u65b9\u9762\uff0c\u73b0\u6709\u6a21\u578b\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u8bc4\u4f30AI\u4ee3\u7406\u4f5c\u4e3a\u79d1\u5b66\u7814\u7a76\u52a9\u624b\u5728\u590d\u73b0\u5b8c\u6574\u7814\u7a76\u8bba\u6587\u65b9\u9762\u7684\u80fd\u529b\uff0c\u4ee5\u786e\u4fdd\u5176\u7814\u7a76\u5de5\u4f5c\u7684\u53ef\u9760\u6027\u548c\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51fa\u540d\u4e3aReplicationBench\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5305\u542b\u7531\u539f\u8bba\u6587\u4f5c\u8005\u5171\u540c\u5f00\u53d1\u7684\u3001\u9488\u5bf9\u8bba\u6587\u6838\u5fc3\u8d21\u732e\uff08\u5b9e\u9a8c\u8bbe\u7f6e\u3001\u63a8\u5bfc\u3001\u6570\u636e\u5206\u6790\u3001\u4ee3\u7801\u5e93\uff09\u7684\u4efb\u52a1\uff0c\u7528\u4e8e\u5ba2\u89c2\u8bc4\u4f30AI\u4ee3\u7406\u7684\u5fe0\u5b9e\u5ea6\u548c\u6b63\u786e\u6027\u3002", "result": "\u5728ReplicationBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5373\u4f7f\u662f\u8868\u73b0\u6700\u597d\u7684\u524d\u6cbf\u8bed\u8a00\u6a21\u578b\u5f97\u5206\u4e5f\u4f4e\u4e8e20%\uff0c\u8868\u660e\u5f53\u524dAI\u4ee3\u7406\u5728\u590d\u73b0\u5929\u4f53\u7269\u7406\u5b66\u7814\u7a76\u65b9\u9762\u5b58\u5728\u5de8\u5927\u6311\u6218\uff0c\u5e76\u53d1\u73b0\u4e86\u591a\u79cd\u5931\u8d25\u6a21\u5f0f\u3002", "conclusion": "ReplicationBench\u9996\u6b21\u5efa\u7acb\u4e86\u5927\u89c4\u6a21\u3001\u4e13\u5bb6\u9a8c\u8bc1\u7684\u5929\u4f53\u7269\u7406\u5b66\u7814\u7a76\u4efb\u52a1\u57fa\u51c6\uff0c\u63ed\u793a\u4e86AI\u4ee3\u7406\u5728\u6570\u636e\u9a71\u52a8\u79d1\u5b66\u9886\u57df\u7684\u4e00\u822c\u6027\u6027\u80fd\u6d1e\u5bdf\uff0c\u5e76\u4e3a\u8861\u91cfAI\u4ee3\u7406\u5728\u79d1\u5b66\u7814\u7a76\u4e2d\u7684\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u6846\u67b6\u3002"}}
{"id": "2510.24514", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24514", "abs": "https://arxiv.org/abs/2510.24514", "authors": ["Huanyu Zhang", "Wenshan Wu", "Chengzu Li", "Ning Shang", "Yan Xia", "Yangyu Huang", "Yifan Zhang", "Li Dong", "Zhang Zhang", "Liang Wang", "Tieniu Tan", "Furu Wei"], "title": "Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal Reasoning in MLLMs", "comment": null, "summary": "While Multimodal Large Language Models (MLLMs) excel at visual understanding,\nthey often struggle in complex scenarios that require visual planning and\nimagination. Inspired by how humans use sketching as a form of visual thinking\nto develop and communicate ideas, we introduce Latent Sketchpad, a framework\nthat equips MLLMs with an internal visual scratchpad. The internal visual\nrepresentations of MLLMs have traditionally been confined to perceptual\nunderstanding. We repurpose them to support generative visual thought without\ncompromising reasoning ability. Building on frontier MLLMs, our approach\nintegrates visual generation directly into their native autoregressive\nreasoning process. It allows the model to interleave textual reasoning with the\ngeneration of visual latents. These latents guide the internal thought process\nand can be translated into sketch images for interpretability. To realize this,\nwe introduce two components: a Context-Aware Vision Head autoregressively\nproduces visual representations, and a pretrained Sketch Decoder renders these\ninto human-interpretable images. We evaluate the framework on our new dataset\nMazePlanning. Experiments across various MLLMs show that Latent Sketchpad\ndelivers comparable or even superior reasoning performance to their backbone.\nIt further generalizes across distinct frontier MLLMs, including Gemma3 and\nQwen2.5-VL. By extending model's textual reasoning to visual thinking, our\nframework opens new opportunities for richer human-computer interaction and\nbroader applications. More details and resources are available on our project\npage: https://latent-sketchpad.github.io/.", "AI": {"tldr": "Latent Sketchpad\u662f\u4e00\u4e2a\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u5185\u90e8\u89c6\u89c9\u8349\u7a3f\u677f\u6765\u589e\u5f3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u89c6\u89c9\u89c4\u5212\u548c\u60f3\u8c61\u65b9\u9762\u7684\u80fd\u529b\uff0c\u4f7f\u5176\u80fd\u591f\u8fdb\u884c\u89c6\u89c9\u601d\u8003\u548c\u751f\u6210\uff0c\u5e76\u5728MazePlanning\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u663e\u793a\u51fa\u4e0e\u9aa8\u5e72\u6a21\u578b\u76f8\u5f53\u6216\u66f4\u4f18\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u9700\u8981\u89c6\u89c9\u89c4\u5212\u548c\u60f3\u8c61\u7684\u590d\u6742\u573a\u666f\u65f6\u5b58\u5728\u4e0d\u8db3\uff0c\u800c\u4eba\u7c7b\u901a\u5e38\u4f7f\u7528\u89c6\u89c9\u8349\u56fe\u4f5c\u4e3a\u4e00\u79cd\u89c6\u89c9\u601d\u8003\u65b9\u5f0f\u6765\u53d1\u5c55\u548c\u4ea4\u6d41\u60f3\u6cd5\u3002", "method": "\u63d0\u51faLatent Sketchpad\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u4e3aMLLMs\u914d\u5907\u5185\u90e8\u89c6\u89c9\u8349\u7a3f\u677f\uff0c\u5c06\u89c6\u89c9\u751f\u6210\u6574\u5408\u5230\u5176\u81ea\u56de\u5f52\u63a8\u7406\u8fc7\u7a0b\u4e2d\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u751f\u6210\u89c6\u89c9\u6f5c\u5728\u8868\u793a\u5e76\u5c06\u5176\u8f6c\u6362\u4e3a\u8349\u56fe\u56fe\u50cf\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u611f\u77e5\u89c6\u89c9\u5934\u548c\u9884\u8bad\u7ec3\u7684\u8349\u56fe\u89e3\u7801\u5668\u5b9e\u73b0\u3002", "result": "\u5728MazePlanning\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cLatent Sketchpad\u5728\u5404\u79cdMLLMs\u4e0a\u5b9e\u73b0\u4e86\u4e0e\u9aa8\u5e72\u6a21\u578b\u76f8\u5f53\u6216\u66f4\u4f18\u7684\u63a8\u7406\u6027\u80fd\uff0c\u5e76\u4e14\u80fd\u591f\u8de8\u4e0d\u540c\u7684\u524d\u6cbfMLLMs\uff08\u5982Gemma3\u548cQwen2.5-VL\uff09\u8fdb\u884c\u6cdb\u5316\u3002", "conclusion": "Latent Sketchpad\u901a\u8fc7\u5c06\u6a21\u578b\u7684\u6587\u672c\u63a8\u7406\u6269\u5c55\u5230\u89c6\u89c9\u601d\u8003\uff0c\u4e3a\u66f4\u4e30\u5bcc\u7684\u4eba\u673a\u4ea4\u4e92\u548c\u66f4\u5e7f\u6cdb\u7684\u5e94\u7528\u5f00\u8f9f\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2510.23977", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23977", "abs": "https://arxiv.org/abs/2510.23977", "authors": ["Yohan Abeysinghe", "Muhammad Akhtar Munir", "Sanoojan Baliah", "Ron Sarafian", "Fahad Shahbaz Khan", "Yinon Rudich", "Salman Khan"], "title": "Synergistic Neural Forecasting of Air Pollution with Stochastic Sampling", "comment": null, "summary": "Air pollution remains a leading global health and environmental risk,\nparticularly in regions vulnerable to episodic air pollution spikes due to\nwildfires, urban haze and dust storms. Accurate forecasting of particulate\nmatter (PM) concentrations is essential to enable timely public health warnings\nand interventions, yet existing models often underestimate rare but hazardous\npollution events. Here, we present SynCast, a high-resolution neural\nforecasting model that integrates meteorological and air composition data to\nimprove predictions of both average and extreme pollution levels. Built on a\nregionally adapted transformer backbone and enhanced with a diffusion-based\nstochastic refinement module, SynCast captures the nonlinear dynamics driving\nPM spikes more accurately than existing approaches. Leveraging on harmonized\nERA5 and CAMS datasets, our model shows substantial gains in forecasting\nfidelity across multiple PM variables (PM$_1$, PM$_{2.5}$, PM$_{10}$),\nespecially under extreme conditions. We demonstrate that conventional loss\nfunctions underrepresent distributional tails (rare pollution events) and show\nthat SynCast, guided by domain-aware objectives and extreme value theory,\nsignificantly enhances performance in highly impacted regions without\ncompromising global accuracy. This approach provides a scalable foundation for\nnext-generation air quality early warning systems and supports climate-health\nrisk mitigation in vulnerable regions.", "AI": {"tldr": "SynCast\u662f\u4e00\u4e2a\u7ed3\u5408\u6c14\u8c61\u548c\u7a7a\u6c14\u6210\u5206\u6570\u636e\u7684\u9ad8\u5206\u8fa8\u7387\u795e\u7ecf\u9884\u6d4b\u6a21\u578b\uff0c\u80fd\u66f4\u51c6\u786e\u5730\u9884\u6d4b\u5e73\u5747\u548c\u6781\u7aefPM\u6d53\u5ea6\uff0c\u5c24\u5176\u662f\u5728\u68ee\u6797\u706b\u707e\u3001\u57ce\u5e02\u96fe\u973e\u548c\u6c99\u5c18\u66b4\u7b49\u6c61\u67d3\u4e8b\u4ef6\u671f\u95f4\u3002\u8be5\u6a21\u578b\u91c7\u7528\u57fa\u4e8etransformer\u7684\u9aa8\u5e72\u7f51\u7edc\u548c\u57fa\u4e8e\u6269\u6563\u7684\u968f\u673a\u7cbe\u70bc\u6a21\u5757\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u80fd\u5728\u9ad8\u5f71\u54cd\u533a\u57df\u663e\u8457\u63d0\u9ad8\u9884\u6d4b\u7cbe\u5ea6\uff0c\u540c\u65f6\u4e0d\u5f71\u54cd\u5168\u5c40\u51c6\u786e\u6027\u3002", "motivation": "\u5f53\u524d\u7684\u7a7a\u6c14\u6c61\u67d3\u9884\u6d4b\u6a21\u578b\u4f4e\u4f30\u4e86\u7531\u68ee\u6797\u706b\u707e\u3001\u57ce\u5e02\u96fe\u973e\u548c\u6c99\u5c18\u66b4\u7b49\u56e0\u7d20\u5f15\u8d77\u7684\u7f55\u89c1\u4f46\u5371\u9669\u7684\u6c61\u67d3\u4e8b\u4ef6\u3002\u56e0\u6b64\uff0c\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u51c6\u786e\u9884\u6d4b\u5e73\u5747\u548c\u6781\u7aef\u7a7a\u6c14\u6c61\u67d3\u6c34\u5e73\u7684\u6a21\u578b\uff0c\u4ee5\u4fbf\u53ca\u65f6\u53d1\u5e03\u516c\u5171\u536b\u751f\u9884\u8b66\u548c\u5e72\u9884\u63aa\u65bd\u3002", "method": "SynCast\u6a21\u578b\u91c7\u7528\u57fa\u4e8etransformer\u7684\u9aa8\u5e72\u7f51\u7edc\uff0c\u5e76\u7ed3\u5408\u4e86\u57fa\u4e8e\u6269\u6563\u7684\u968f\u673a\u7cbe\u70bc\u6a21\u5757\uff0c\u80fd\u591f\u6355\u6349PM\u6d53\u5ea6\u5cf0\u503c\u80cc\u540e\u7684\u975e\u7ebf\u6027\u52a8\u6001\u3002\u8be5\u6a21\u578b\u8fd8\u5229\u7528\u4e86ERA5\u548cCAMS\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc7\u8003\u8651\u9886\u57df\u7279\u5b9a\u76ee\u6807\u548c\u6781\u503c\u7406\u8bba\u6765\u4f18\u5316\u6a21\u578b\uff0c\u4ee5\u89e3\u51b3\u4f20\u7edf\u635f\u5931\u51fd\u6570\u5728\u9884\u6d4b\u7f55\u89c1\u6c61\u67d3\u4e8b\u4ef6\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "result": "SynCast\u6a21\u578b\u5728PM1\u3001PM2.5\u548cPM10\u7684\u9884\u6d4b\u65b9\u9762\u8868\u73b0\u51fa\u663e\u8457\u7684\u6539\u8fdb\uff0c\u7279\u522b\u662f\u5728\u6781\u7aef\u60c5\u51b5\u4e0b\u3002\u8be5\u6a21\u578b\u5728\u9ad8\u5f71\u54cd\u533a\u57df\u7684\u9884\u6d4b\u6027\u80fd\u5f97\u5230\u4e86\u663e\u8457\u63d0\u5347\uff0c\u540c\u65f6\u5728\u5168\u7403\u51c6\u786e\u6027\u65b9\u9762\u6ca1\u6709\u53d7\u5230\u5f71\u54cd\u3002", "conclusion": "SynCast\u6a21\u578b\u901a\u8fc7\u6574\u5408\u6c14\u8c61\u548c\u7a7a\u6c14\u6210\u5206\u6570\u636e\uff0c\u5e76\u91c7\u7528\u5148\u8fdb\u7684\u795e\u7ecf\u9884\u6d4b\u6280\u672f\uff0c\u80fd\u591f\u66f4\u51c6\u786e\u5730\u9884\u6d4b\u7a7a\u6c14\u6c61\u67d3\uff0c\u5c24\u5176\u662f\u5728\u6781\u7aef\u6c61\u67d3\u4e8b\u4ef6\u7684\u60c5\u51b5\u4e0b\u3002\u8be5\u6a21\u578b\u4e3a\u4e0b\u4e00\u4ee3\u7a7a\u6c14\u8d28\u91cf\u9884\u8b66\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u57fa\u7840\uff0c\u5e76\u6709\u52a9\u4e8e\u51cf\u8f7b\u8106\u5f31\u5730\u533a\u7684\u6c14\u5019\u5065\u5eb7\u98ce\u9669\u3002"}}
{"id": "2510.24592", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24592", "abs": "https://arxiv.org/abs/2510.24592", "authors": ["Guoxin Chen", "Jing Wu", "Xinjie Chen", "Wayne Xin Zhao", "Ruihua Song", "Chengxi Li", "Kai Fan", "Dayiheng Liu", "Minpeng Liao"], "title": "ReForm: Reflective Autoformalization with Prospective Bounded Sequence Optimization", "comment": "Ongoing Work", "summary": "Autoformalization, which translates natural language mathematics into\nmachine-verifiable formal statements, is critical for using formal mathematical\nreasoning to solve math problems stated in natural language. While Large\nLanguage Models can generate syntactically correct formal statements, they\noften fail to preserve the original problem's semantic intent. This limitation\narises from the LLM approaches' treating autoformalization as a simplistic\ntranslation task which lacks mechanisms for self-reflection and iterative\nrefinement that human experts naturally employ. To address these issues, we\npropose ReForm, a Reflective Autoformalization method that tightly integrates\nsemantic consistency evaluation into the autoformalization process. This\nenables the model to iteratively generate formal statements, assess its\nsemantic fidelity, and self-correct identified errors through progressive\nrefinement. To effectively train this reflective model, we introduce\nProspective Bounded Sequence Optimization (PBSO), which employs different\nrewards at different sequence positions to ensure that the model develops both\naccurate autoformalization and correct semantic validations, preventing\nsuperficial critiques that would undermine the purpose of reflection. Extensive\nexperiments across four autoformalization benchmarks demonstrate that ReForm\nachieves an average improvement of 17.2 percentage points over the strongest\nbaselines. To further ensure evaluation reliability, we introduce\nConsistencyCheck, a benchmark of 859 expert-annotated items that not only\nvalidates LLMs as judges but also reveals that autoformalization is inherently\ndifficult: even human experts produce semantic errors in up to 38.5% of cases.", "AI": {"tldr": "ReForm\u662f\u4e00\u4e2a\u53cd\u601d\u6027\u81ea\u52a8\u5f62\u5f0f\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u96c6\u6210\u8bed\u4e49\u4e00\u81f4\u6027\u8bc4\u4f30\u6765\u6539\u8fdb\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u81ea\u7136\u8bed\u8a00\u6570\u5b66\u5f62\u5f0f\u5316\u65b9\u9762\u7684\u8868\u73b0\uff0c\u89e3\u51b3\u4e86LLM\u5728\u4fdd\u7559\u539f\u59cb\u95ee\u9898\u8bed\u4e49\u610f\u56fe\u65b9\u9762\u7684\u4e0d\u8db3\u3002\u5b83\u4f7f\u7528PBSO\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u5728ConsistencyCheck\u57fa\u51c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5e73\u5747\u6027\u80fd\u63d0\u5347\u4e8617.2%\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u5c06\u81ea\u7136\u8bed\u8a00\u6570\u5b66\u8f6c\u5316\u4e3a\u673a\u5668\u53ef\u9a8c\u8bc1\u5f62\u5f0f\u8bed\u53e5\u65f6\uff0c\u5e38\u56e0\u672a\u80fd\u4fdd\u7559\u539f\u59cb\u95ee\u9898\u7684\u8bed\u4e49\u610f\u56fe\u800c\u5931\u8d25\uff0c\u56e0\u4e3a\u5b83\u4eec\u5c06\u6b64\u89c6\u4e3a\u7b80\u5355\u7ffb\u8bd1\u4efb\u52a1\uff0c\u7f3a\u4e4f\u4eba\u7c7b\u4e13\u5bb6\u6240\u5177\u5907\u7684\u81ea\u6211\u53cd\u601d\u548c\u8fed\u4ee3\u4fee\u6b63\u673a\u5236\u3002", "method": "ReForm\u662f\u4e00\u79cd\u53cd\u601d\u6027\u81ea\u52a8\u5f62\u5f0f\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u8bed\u4e49\u4e00\u81f4\u6027\u8bc4\u4f30\u6574\u5408\u5230\u81ea\u52a8\u5f62\u5f0f\u5316\u8fc7\u7a0b\u4e2d\uff0c\u5b9e\u73b0\u8fed\u4ee3\u751f\u6210\u5f62\u5f0f\u8bed\u53e5\u3001\u8bc4\u4f30\u8bed\u4e49\u4fdd\u771f\u5ea6\u5e76\u901a\u8fc7\u6e10\u8fdb\u5f0f\u4fee\u6b63\u81ea\u6211\u7ea0\u9519\u3002\u4e3a\u4e86\u6709\u6548\u8bad\u7ec3\u6b64\u53cd\u601d\u6a21\u578b\uff0c\u5f15\u5165\u4e86\u524d\u77bb\u6027\u6709\u754c\u5e8f\u5217\u4f18\u5316\uff08PBSO\uff09\uff0c\u5728\u5e8f\u5217\u7684\u4e0d\u540c\u4f4d\u7f6e\u91c7\u7528\u4e0d\u540c\u7684\u5956\u52b1\uff0c\u4ee5\u786e\u4fdd\u6a21\u578b\u65e2\u80fd\u51c6\u786e\u5f62\u5f0f\u5316\u53c8\u80fd\u6b63\u786e\u8fdb\u884c\u8bed\u4e49\u9a8c\u8bc1\u3002", "result": "ReForm\u65b9\u6cd5\u5728\u56db\u4e2a\u81ea\u52a8\u5f62\u5f0f\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6210\u6548\uff0c\u5e73\u5747\u6bd4\u6700\u5f3a\u7684\u57fa\u7ebf\u65b9\u6cd5\u63d0\u9ad8\u4e8617.2\u4e2a\u767e\u5206\u70b9\u3002ConsistencyCheck\u57fa\u51c6\uff08\u5305\u542b859\u4e2a\u4e13\u5bb6\u6807\u6ce8\u9879\uff09\u7684\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u9a8c\u8bc1\u4e86LLM\u4f5c\u4e3a\u88c1\u5224\u7684\u53ef\u9760\u6027\uff0c\u8fd8\u63ed\u793a\u4e86\u81ea\u52a8\u5f62\u5f0f\u5316\u672c\u8eab\u7684\u96be\u5ea6\u2014\u2014\u5373\u4f7f\u662f\u4eba\u7c7b\u4e13\u5bb6\u4e5f\u4f1a\u5728\u9ad8\u8fbe38.5%\u7684\u60c5\u51b5\u4e0b\u4ea7\u751f\u8bed\u4e49\u9519\u8bef\u3002", "conclusion": "ReForm\u901a\u8fc7\u5f15\u5165\u53cd\u601d\u673a\u5236\u548cPBSO\u8bad\u7ec3\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u81ea\u52a8\u5f62\u5f0f\u5316\u7684\u51c6\u786e\u6027\u548c\u8bed\u4e49\u4fdd\u771f\u5ea6\u3002ConsistencyCheck\u57fa\u51c6\u7684\u5f15\u5165\uff0c\u4e0d\u4ec5\u4e3a\u8bc4\u4f30\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u4f9d\u636e\uff0c\u4e5f\u7a81\u663e\u4e86\u81ea\u52a8\u5f62\u5f0f\u5316\u4efb\u52a1\u7684\u56fa\u6709\u6311\u6218\uff0c\u5373\u4f7f\u5bf9\u4eba\u7c7b\u4e13\u5bb6\u800c\u8a00\u4ea6\u662f\u5982\u6b64\u3002"}}
{"id": "2510.24563", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24563", "abs": "https://arxiv.org/abs/2510.24563", "authors": ["Hongrui Jia", "Jitong Liao", "Xi Zhang", "Haiyang Xu", "Tianbao Xie", "Chaoya Jiang", "Ming Yan", "Si Liu", "Wei Ye", "Fei Huang"], "title": "OSWorld-MCP: Benchmarking MCP Tool Invocation In Computer-Use Agents", "comment": null, "summary": "With advances in decision-making and reasoning capabilities, multimodal\nagents show strong potential in computer application scenarios. Past\nevaluations have mainly assessed GUI interaction skills, while tool invocation\nabilities, such as those enabled by the Model Context Protocol (MCP), have been\nlargely overlooked. Comparing agents with integrated tool invocation to those\nevaluated only on GUI interaction is inherently unfair. We present OSWorld-MCP,\nthe first comprehensive and fair benchmark for assessing computer-use agents'\ntool invocation, GUI operation, and decision-making abilities in a real-world\nenvironment. We design a novel automated code-generation pipeline to create\ntools and combine them with a curated selection from existing tools. Rigorous\nmanual validation yields 158 high-quality tools (covering 7 common\napplications), each verified for correct functionality, practical\napplicability, and versatility. Extensive evaluations of state-of-the-art\nmultimodal agents on OSWorld-MCP show that MCP tools generally improve task\nsuccess rates (e.g., from 8.3% to 20.4% for OpenAI o3 at 15 steps, from 40.1%\nto 43.3% for Claude 4 Sonnet at 50 steps), underscoring the importance of\nassessing tool invocation capabilities. However, even the strongest models have\nrelatively low tool invocation rates, Only 36.3%, indicating room for\nimprovement and highlighting the benchmark's challenge. By explicitly measuring\nMCP tool usage skills, OSWorld-MCP deepens understanding of multimodal agents\nand sets a new standard for evaluating performance in complex, tool-assisted\nenvironments. Our code, environment, and data are publicly available at\nhttps://osworld-mcp.github.io.", "AI": {"tldr": "OSWorld-MCP \u662f\u4e00\u4e2a\u8bc4\u4f30\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u5de5\u5177\u8c03\u7528\u3001GUI \u64cd\u4f5c\u548c\u51b3\u7b56\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u5176\u4e2d MCP \u5de5\u5177\u7684\u5f15\u5165\u663e\u8457\u63d0\u9ad8\u4e86\u4efb\u52a1\u6210\u529f\u7387\uff0c\u4f46\u6a21\u578b\u5de5\u5177\u8c03\u7528\u7387\u4ecd\u6709\u63d0\u5347\u7a7a\u95f4\u3002", "motivation": "\u8bc4\u4f30\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u7684\u5de5\u5177\u8c03\u7528\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u4ee5\u5f80\u8bc4\u4f30\u4e3b\u8981\u96c6\u4e2d\u5728 GUI \u4ea4\u4e92\u800c\u5ffd\u7565\u5de5\u5177\u8c03\u7528\u7684\u4e0d\u516c\u5e73\u6027\u95ee\u9898\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b 158 \u4e2a\u9ad8\u8d28\u91cf\u5de5\u5177\u7684 OSWorld-MCP \u57fa\u51c6\uff0c\u5e76\u5bf9\u6700\u5148\u8fdb\u7684\u591a\u6a21\u6001\u4ee3\u7406\u8fdb\u884c\u4e86\u5e7f\u6cdb\u8bc4\u4f30\u3002", "result": "MCP \u5de5\u5177\u666e\u904d\u63d0\u9ad8\u4e86\u4efb\u52a1\u6210\u529f\u7387\uff08\u4f8b\u5982\uff0cOpenAI o3 \u7684\u6210\u529f\u7387\u4ece 8.3% \u63d0\u9ad8\u5230 20.4%\uff0cClaude 4 Sonnet \u7684\u6210\u529f\u7387\u4ece 40.1% \u63d0\u9ad8\u5230 43.3%\uff09\uff0c\u4f46\u6a21\u578b\u5de5\u5177\u8c03\u7528\u7387\u4ec5\u4e3a 36.3%\uff0c\u8868\u660e\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002", "conclusion": "OSWorld-MCP \u57fa\u51c6\u901a\u8fc7\u660e\u786e\u8861\u91cf MCP \u5de5\u5177\u7684\u4f7f\u7528\u6280\u80fd\uff0c\u52a0\u6df1\u4e86\u5bf9\u591a\u6a21\u6001\u4ee3\u7406\u7684\u7406\u89e3\uff0c\u5e76\u4e3a\u8bc4\u4f30\u590d\u6742\u3001\u5de5\u5177\u8f85\u52a9\u73af\u5883\u4e2d\u7684\u6027\u80fd\u8bbe\u5b9a\u4e86\u65b0\u6807\u51c6\u3002"}}
{"id": "2510.24605", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24605", "abs": "https://arxiv.org/abs/2510.24605", "authors": ["Yicun Yang", "Cong Wang", "Shaobo Wang", "Zichen Wen", "Biqing Qi", "Hanlin Xu", "Linfeng Zhang"], "title": "Diffusion LLM with Native Variable Generation Lengths: Let [EOS] Lead the Way", "comment": null, "summary": "Diffusion-based large language models (dLLMs) have exhibited substantial\npotential for parallel text generation, which may enable more efficient\ngeneration compared to autoregressive models. However, current dLLMs suffer\nfrom fixed generation lengths, which indicates the generation lengths of dLLMs\nhave to be determined before decoding as a hyper-parameter, leading to issues\nin efficiency and flexibility. To solve these problems, in this work, we\npropose to train a diffusion LLM with native variable generation lengths,\nabbreviated as dLLM-Var. Concretely, we aim to train a model to accurately\npredict the [EOS] token in the generated text, which makes a dLLM be able to\nnatively infer in a block diffusion manner, while still maintaining the ability\nof global bi-directional (full) attention and high parallelism. Experiments on\nstandard benchmarks demonstrate that our method achieves a 30.1x speedup over\ntraditional dLLM inference paradigms and a 2.4x speedup relative to\nautoregressive models such as Qwen and Llama. Our method achieves higher\naccuracy and faster inference, elevating dLLMs beyond mere academic novelty and\nsupporting their practical use in real-world applications. Codes and models\nhave been released.", "AI": {"tldr": "dLLM-Var \u662f\u4e00\u79cd\u80fd\u591f\u539f\u751f\u652f\u6301\u53ef\u53d8\u6587\u672c\u751f\u6210\u957f\u5ea6\u7684\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf dLLM \u56fa\u5b9a\u957f\u5ea6\u751f\u6210\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u63a8\u7406\u901f\u5ea6\u548c\u66f4\u9ad8\u7684\u51c6\u786e\u6027\uff0c\u5e76\u652f\u6301\u5b9e\u9645\u5e94\u7528\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u6269\u6563\uff08dLLM\uff09\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5e76\u884c\u6587\u672c\u751f\u6210\u65b9\u9762\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u5b58\u5728\u751f\u6210\u957f\u5ea6\u56fa\u5b9a\u7684\u95ee\u9898\uff0c\u9700\u8981\u5728\u89e3\u7801\u524d\u786e\u5b9a\u751f\u6210\u957f\u5ea6\uff0c\u5f71\u54cd\u4e86\u6548\u7387\u548c\u7075\u6d3b\u6027\u3002", "method": "\u63d0\u51fa\u8bad\u7ec3\u4e00\u4e2a\u80fd\u591f\u539f\u751f\u652f\u6301\u53ef\u53d8\u751f\u6210\u957f\u5ea6\u7684 dLLM\uff08dLLM-Var\uff09\uff0c\u5176\u76ee\u6807\u662f\u51c6\u786e\u9884\u6d4b\u751f\u6210\u6587\u672c\u4e2d\u7684 [EOS] \u6807\u8bb0\uff0c\u4ece\u800c\u5728\u4fdd\u6301\u5168\u5c40\u53cc\u5411\u6ce8\u610f\u529b\u548c\u9ad8\u5e76\u884c\u6027\u7684\u540c\u65f6\uff0c\u80fd\u591f\u4ee5\u5757\u6269\u6563\u7684\u65b9\u5f0f\u8fdb\u884c\u539f\u751f\u63a8\u7406\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cdLLM-Var \u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86 30.1 \u500d\u4e8e\u4f20\u7edf dLLM \u63a8\u7406\u8303\u5f0f\u7684\u52a0\u901f\uff0c\u4ee5\u53ca 2.4 \u500d\u4e8e Qwen \u548c Llama \u7b49\u81ea\u56de\u5f52\u6a21\u578b\u7684\u52a0\u901f\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u66f4\u5feb\u7684\u63a8\u7406\u901f\u5ea6\u3002", "conclusion": "dLLM-Var \u63d0\u5347\u4e86 dLLM \u7684\u5b9e\u7528\u6027\uff0c\u4f7f\u5176\u8d85\u8d8a\u4e86\u5b66\u672f\u65b0\u9896\u6027\u7684\u8303\u7574\uff0c\u80fd\u591f\u652f\u6301\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u4f7f\u7528\uff0c\u5e76\u5df2\u516c\u5f00\u4ee3\u7801\u548c\u6a21\u578b\u3002"}}
{"id": "2510.24579", "categories": ["cs.CV", "I.4.5; I.5"], "pdf": "https://arxiv.org/pdf/2510.24579", "abs": "https://arxiv.org/abs/2510.24579", "authors": ["Xu Jiang", "Huiying Pan", "Ligen Shi", "Jianing Sun", "Wenfeng Xu", "Xing Zhao"], "title": "Physics-Inspired Gaussian Kolmogorov-Arnold Networks for X-ray Scatter Correction in Cone-Beam CT", "comment": "8 pages, 6 figures", "summary": "Cone-beam CT (CBCT) employs a flat-panel detector to achieve\nthree-dimensional imaging with high spatial resolution. However, CBCT is\nsusceptible to scatter during data acquisition, which introduces CT value bias\nand reduced tissue contrast in the reconstructed images, ultimately degrading\ndiagnostic accuracy. To address this issue, we propose a deep learning-based\nscatter artifact correction method inspired by physical prior knowledge.\nLeveraging the fact that the observed point scatter probability density\ndistribution exhibits rotational symmetry in the projection domain. The method\nuses Gaussian Radial Basis Functions (RBF) to model the point scatter function\nand embeds it into the Kolmogorov-Arnold Networks (KAN) layer, which provides\nefficient nonlinear mapping capabilities for learning high-dimensional scatter\nfeatures. By incorporating the physical characteristics of the scattered photon\ndistribution together with the complex function mapping capacity of KAN, the\nmodel improves its ability to accurately represent scatter. The effectiveness\nof the method is validated through both synthetic and real-scan experiments.\nExperimental results show that the model can effectively correct the scatter\nartifacts in the reconstructed images and is superior to the current methods in\nterms of quantitative metrics.", "AI": {"tldr": "CBCT\u6210\u50cf\u6613\u53d7\u6563\u5c04\u5f71\u54cd\uff0c\u5bfc\u81f4\u56fe\u50cf\u4f2a\u5f71\u548c\u8bca\u65ad\u51c6\u786e\u6027\u4e0b\u964d\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u5e76\u7ed3\u5408\u7269\u7406\u5148\u9a8c\u77e5\u8bc6\u7684\u6563\u5c04\u4f2a\u5f71\u6821\u6b63\u65b9\u6cd5\uff0c\u5229\u7528KAN\u7f51\u7edc\u5efa\u6a21\u6563\u5c04\u5e76\u8fdb\u884c\u6821\u6b63\uff0c\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u5b9e\u9a8c\u4e2d\u5747\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "CBCT\u6210\u50cf\u6613\u53d7\u6563\u5c04\u5f71\u54cd\uff0c\u5bfc\u81f4\u56fe\u50cf\u4f2a\u5f71\u548c\u8bca\u65ad\u51c6\u786e\u6027\u4e0b\u964d\uff0c\u9700\u8981\u63d0\u51fa\u6709\u6548\u65b9\u6cd5\u8fdb\u884c\u6821\u6b63\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u5e76\u7ed3\u5408\u7269\u7406\u5148\u9a8c\u77e5\u8bc6\u7684\u6563\u5c04\u4f2a\u5f71\u6821\u6b63\u65b9\u6cd5\u3002\u5229\u7528\u9ad8\u65af\u5f84\u5411\u57fa\u51fd\u6570\uff08RBF\uff09\u5efa\u6a21\u70b9\u6563\u5c04\u51fd\u6570\uff0c\u5e76\u5c06\u5176\u5d4c\u5165Kolmogorov-Arnold Networks\uff08KAN\uff09\u5c42\uff0c\u4ee5\u5b66\u4e60\u9ad8\u7ef4\u6563\u5c04\u7279\u5f81\u3002", "result": "\u901a\u8fc7\u5408\u6210\u548c\u771f\u5b9e\u626b\u63cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u80fd\u6709\u6548\u6821\u6b63\u91cd\u5efa\u56fe\u50cf\u4e2d\u7684\u6563\u5c04\u4f2a\u5f71\uff0c\u5e76\u5728\u5b9a\u91cf\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8eKAN\u7684\u6563\u5c04\u6821\u6b63\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u63d0\u9ad8CBCT\u56fe\u50cf\u8d28\u91cf\uff0c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002"}}
{"id": "2510.24606", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24606", "abs": "https://arxiv.org/abs/2510.24606", "authors": ["Siheng Xiong", "Joe Zou", "Faramarz Fekri", "Yae Jee Cho"], "title": "Long-Context Modeling with Dynamic Hierarchical Sparse Attention for On-Device LLMs", "comment": "Accepted to NeurIPS 2025 Workshop on Efficient Reasoning", "summary": "The quadratic cost of attention hinders the scalability of long-context LLMs,\nespecially in resource-constrained settings. Existing static sparse methods\nsuch as sliding windows or global tokens utilizes the sparsity of attention to\nreduce the cost of attention, but poorly adapts to the content-dependent\nvariations in attention due to their staticity. While previous work has\nproposed several dynamic approaches to improve flexibility, they still depend\non predefined templates or heuristic mechanisms. Such strategies reduce\ngenerality and prune tokens that remain contextually important, limiting their\naccuracy across diverse tasks. To tackle these bottlenecks of existing methods\nfor long-context modeling, we introduce Dynamic Hierarchical Sparse Attention\n(DHSA), a data-driven framework that dynamically predicts attention sparsity\nonline without retraining. Our proposed DHSA adaptively segments sequences into\nvariable-length chunks, then computes chunk representations by aggregating the\ntoken embeddings within each chunk. To avoid the bias introduced by varying\nchunk lengths, we apply length-normalized aggregation that scales the averaged\nembeddings by the square root of the chunk size. Finally, DHSA upsamples the\nchunk-level similarity scores to token level similarities to calculate\nimportance scores that determine which token-level interactions should be\npreserved. Our experiments on Gemma2 with Needle-in-a-Haystack Test and\nLongBench show that DHSA matches dense attention in accuracy, while reducing\nprefill latency by 20-60% and peak memory usage by 35%. Compared to other\nrepresentative baselines such as block sparse attention, DHSA achieves\nconsistently higher accuracy (6-18% relative gains) with comparable or lower\ncost, offering an efficient and adaptable solution for long-context on-device\nLLMs.", "AI": {"tldr": "DHSA\u901a\u8fc7\u52a8\u6001\u9884\u6d4b\u6ce8\u610f\u529b\u7a00\u758f\u6027\u6765\u89e3\u51b3\u957f\u4e0a\u4e0b\u6587LLM\u7684\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u5ef6\u8fdf\u548c\u5185\u5b58\u4f7f\u7528\u3002", "motivation": "\u73b0\u6709\u7684\u9759\u6001\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6cd5\u548c\u4e00\u4e9b\u52a8\u6001\u65b9\u6cd5\u5728\u5904\u7406\u957f\u4e0a\u4e0b\u6587\u65f6\u5b58\u5728\u9002\u5e94\u6027\u5dee\u3001\u6cdb\u5316\u80fd\u529b\u6709\u9650\u548c\u53ef\u80fd\u4fee\u526a\u91cd\u8981\u4e0a\u4e0b\u6587\u6807\u8bb0\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "method": "DHSA\u662f\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7684\u6846\u67b6\uff0c\u80fd\u591f\u52a8\u6001\u9884\u6d4b\u6ce8\u610f\u529b\u7a00\u758f\u6027\u3002\u5b83\u5c06\u5e8f\u5217\u81ea\u9002\u5e94\u5730\u5206\u5272\u6210\u4e0d\u540c\u957f\u5ea6\u7684\u5757\uff0c\u901a\u8fc7\u5bf9\u5757\u5185\u6807\u8bb0\u5d4c\u5165\u8fdb\u884c\u957f\u5ea6\u5f52\u4e00\u5316\u805a\u5408\u6765\u8ba1\u7b97\u5757\u8868\u793a\uff0c\u5e76\u5c06\u5757\u7ea7\u76f8\u4f3c\u5ea6\u5206\u6570\u4e0a\u91c7\u6837\u5230\u6807\u8bb0\u7ea7\uff0c\u4ee5\u786e\u5b9a\u4fdd\u7559\u54ea\u4e9b\u6807\u8bb0\u7ea7\u4ea4\u4e92\u3002", "result": "DHSA\u5728Gemma2\u6a21\u578b\u4e0a\u4f7f\u7528Needle-in-a-Haystack Test\u548cLongBench\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDHSA\u5728\u51c6\u786e\u6027\u4e0a\u80fd\u4e0e\u5bc6\u96c6\u6ce8\u610f\u529b\u76f8\u5ab2\u7f8e\uff0c\u540c\u65f6\u5c06\u9884\u586b\u5145\u5ef6\u8fdf\u964d\u4f4e\u4e8620-60%\uff0c\u5e76\u5c06\u5cf0\u503c\u5185\u5b58\u4f7f\u7528\u91cf\u964d\u4f4e\u4e8635%\u3002\u4e0e\u5757\u7a00\u758f\u6ce8\u610f\u529b\u7b49\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0cDHSA\u5b9e\u73b0\u4e86\u6301\u7eed\u66f4\u9ad8\u7684\u51c6\u786e\u6027\uff08\u76f8\u5bf9\u63d0\u9ad8\u4e866-18%\uff09\uff0c\u540c\u65f6\u6210\u672c\u76f8\u5f53\u6216\u66f4\u4f4e\u3002", "conclusion": "DHSA\u4e3a\u8d44\u6e90\u53d7\u9650\u7684\u8bbe\u5907\u4e0a\u7684\u957f\u4e0a\u4e0b\u6587LLM\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u9002\u5e94\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5b83\u80fd\u591f\u52a8\u6001\u5730\u9002\u5e94\u5185\u5bb9\u53d8\u5316\uff0c\u5728\u4fdd\u6301\u9ad8\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u4e86\u6548\u7387\u3002"}}
{"id": "2510.24640", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24640", "abs": "https://arxiv.org/abs/2510.24640", "authors": ["Xin Zhang", "Yuqi Song", "Fei Zuo"], "title": "A Dual-Branch CNN for Robust Detection of AI-Generated Facial Forgeries", "comment": null, "summary": "The rapid advancement of generative AI has enabled the creation of highly\nrealistic forged facial images, posing significant threats to AI security,\ndigital media integrity, and public trust. Face forgery techniques, ranging\nfrom face swapping and attribute editing to powerful diffusion-based image\nsynthesis, are increasingly being used for malicious purposes such as\nmisinformation, identity fraud, and defamation. This growing challenge\nunderscores the urgent need for robust and generalizable face forgery detection\nmethods as a critical component of AI security infrastructure. In this work, we\npropose a novel dual-branch convolutional neural network for face forgery\ndetection that leverages complementary cues from both spatial and frequency\ndomains. The RGB branch captures semantic information, while the frequency\nbranch focuses on high-frequency artifacts that are difficult for generative\nmodels to suppress. A channel attention module is introduced to adaptively fuse\nthese heterogeneous features, highlighting the most informative channels for\nforgery discrimination. To guide the network's learning process, we design a\nunified loss function, FSC Loss, that combines focal loss, supervised\ncontrastive loss, and a frequency center margin loss to enhance class\nseparability and robustness. We evaluate our model on the DiFF benchmark, which\nincludes forged images generated from four representative methods:\ntext-to-image, image-to-image, face swap, and face edit. Our method achieves\nstrong performance across all categories and outperforms average human\naccuracy. These results demonstrate the model's effectiveness and its potential\ncontribution to safeguarding AI ecosystems against visual forgery attacks.", "AI": {"tldr": "\u751f\u6210\u5f0fAI\u751f\u6210\u7684\u903c\u771f\u4f2a\u9020\u4eba\u8138\u56fe\u50cf\u5bf9AI\u5b89\u5168\u3001\u6570\u5b57\u5a92\u4f53\u548c\u516c\u4f17\u4fe1\u4efb\u6784\u6210\u91cd\u5927\u5a01\u80c1\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u53cc\u5206\u652f\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff0c\u7ed3\u5408\u7a7a\u95f4\u57df\u548c\u9891\u7387\u57df\u7684\u4e92\u8865\u7ebf\u7d22\uff0c\u7528\u4e8e\u4eba\u8138\u4f2a\u9020\u68c0\u6d4b\u3002", "motivation": "\u751f\u6210\u5f0fAI\u7684\u5feb\u901f\u53d1\u5c55\u5bfc\u81f4\u4e86\u9ad8\u5ea6\u903c\u771f\u7684\u4f2a\u9020\u4eba\u8138\u56fe\u50cf\u7684\u51fa\u73b0\uff0c\u5bf9AI\u5b89\u5168\u3001\u6570\u5b57\u5a92\u4f53\u5b8c\u6574\u6027\u548c\u516c\u4f17\u4fe1\u4efb\u6784\u6210\u4e86\u91cd\u5927\u5a01\u80c1\u3002\u4ece\u4eba\u8138\u4ea4\u6362\u3001\u5c5e\u6027\u7f16\u8f91\u5230\u5f3a\u5927\u7684\u57fa\u4e8e\u6269\u6563\u7684\u56fe\u50cf\u5408\u6210\uff0c\u4eba\u8138\u4f2a\u9020\u6280\u672f\u8d8a\u6765\u8d8a\u591a\u5730\u88ab\u7528\u4e8e\u865a\u5047\u4fe1\u606f\u3001\u8eab\u4efd\u6b3a\u8bc8\u548c\u8bfd\u8c24\u7b49\u6076\u610f\u76ee\u7684\u3002\u8fd9\u4e00\u65e5\u76ca\u589e\u957f\u7684\u6311\u6218\u51f8\u663e\u4e86\u5bf9\u9c81\u68d2\u4e14\u53ef\u6cdb\u5316\u7684\u9762\u90e8\u4f2a\u9020\u68c0\u6d4b\u65b9\u6cd5\u4f5c\u4e3aAI\u5b89\u5168\u57fa\u7840\u8bbe\u65bd\u7684\u5173\u952e\u7ec4\u6210\u90e8\u5206\u7684\u8feb\u5207\u9700\u6c42\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u53cc\u5206\u652f\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff0c\u8be5\u7f51\u7edc\u5229\u7528\u6765\u81ea\u7a7a\u95f4\u57df\u548c\u9891\u7387\u57df\u7684\u4e92\u8865\u7ebf\u7d22\u3002RGB\u5206\u652f\u6355\u83b7\u8bed\u4e49\u4fe1\u606f\uff0c\u800c\u9891\u7387\u5206\u652f\u4e13\u6ce8\u4e8e\u751f\u6210\u6a21\u578b\u96be\u4ee5\u6291\u5236\u7684\u9ad8\u9891\u4f2a\u5f71\u3002\u5f15\u5165\u4e86\u901a\u9053\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u4ee5\u81ea\u9002\u5e94\u5730\u878d\u5408\u8fd9\u4e9b\u5f02\u6784\u7279\u5f81\uff0c\u7a81\u51fa\u4f2a\u9020\u5224\u522b\u4e2d\u6700\u5177\u4fe1\u606f\u91cf\u7684\u901a\u9053\u3002\u4e3a\u4e86\u6307\u5bfc\u7f51\u7edc\u7684\u5b66\u4e60\u8fc7\u7a0b\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u635f\u5931\u51fd\u6570\uff0c\u79f0\u4e3aFSC Loss\uff0c\u5b83\u7ed3\u5408\u4e86\u7126\u8ddd\u635f\u5931\u3001\u76d1\u7763\u5bf9\u6bd4\u635f\u5931\u548c\u9891\u7387\u4e2d\u5fc3\u88d5\u5ea6\u635f\u5931\uff0c\u4ee5\u589e\u5f3a\u7c7b\u522b\u53ef\u5206\u79bb\u6027\u548c\u9c81\u68d2\u6027\u3002", "result": "\u5728DiFF\u57fa\u51c6\u4e0a\u8bc4\u4f30\u6a21\u578b\uff0c\u8be5\u57fa\u51c6\u5305\u542b\u6765\u81ea\u56db\u79cd\u4ee3\u8868\u6027\u65b9\u6cd5\u7684\u4f2a\u9020\u56fe\u50cf\uff1a\u6587\u672c\u5230\u56fe\u50cf\u3001\u56fe\u50cf\u5230\u56fe\u50cf\u3001\u4eba\u8138\u4ea4\u6362\u548c\u4eba\u8138\u7f16\u8f91\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u6240\u6709\u7c7b\u522b\u4e2d\u5747\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u4f18\u4e8e\u4eba\u7c7b\u7684\u5e73\u5747\u51c6\u786e\u7387\u3002", "conclusion": "\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5728\u4eba\u8138\u4f2a\u9020\u68c0\u6d4b\u65b9\u9762\u662f\u6709\u6548\u7684\uff0c\u5e76\u6709\u53ef\u80fd\u4e3a\u4fdd\u62a4AI\u751f\u6001\u7cfb\u7edf\u514d\u53d7\u89c6\u89c9\u4f2a\u9020\u653b\u51fb\u505a\u51fa\u8d21\u732e\u3002"}}
{"id": "2510.23992", "categories": ["cs.LG", "cs.IT", "math.IT", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.23992", "abs": "https://arxiv.org/abs/2510.23992", "authors": ["Yuxiao Wen", "Yanjun Han", "Zhengyuan Zhou"], "title": "Optimal Arm Elimination Algorithms for Combinatorial Bandits", "comment": null, "summary": "Combinatorial bandits extend the classical bandit framework to settings where\nthe learner selects multiple arms in each round, motivated by applications such\nas online recommendation and assortment optimization. While extensions of upper\nconfidence bound (UCB) algorithms arise naturally in this context, adapting arm\nelimination methods has proved more challenging. We introduce a novel\nelimination scheme that partitions arms into three categories (confirmed,\nactive, and eliminated), and incorporates explicit exploration to update these\nsets. We demonstrate the efficacy of our algorithm in two settings: the\ncombinatorial multi-armed bandit with general graph feedback, and the\ncombinatorial linear contextual bandit. In both cases, our approach achieves\nnear-optimal regret, whereas UCB-based methods can provably fail due to\ninsufficient explicit exploration. Matching lower bounds are also provided.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7ec4\u5408\u571f\u532a\u7b97\u6cd5\uff0c\u901a\u8fc7\u5c06\u81c2\u5206\u4e3a\u5df2\u786e\u8ba4\u3001\u6d3b\u52a8\u548c\u5df2\u6dd8\u6c70\u4e09\u7c7b\uff0c\u5e76\u7ed3\u5408\u663e\u5f0f\u63a2\u7d22\u6765\u66f4\u65b0\u8fd9\u4e9b\u96c6\u5408\uff0c\u4ece\u800c\u89e3\u51b3\u4e86\u7ec4\u5408\u571f\u532a\u4e2d\u7684\u81c2\u6d88\u9664\u95ee\u9898\u3002", "motivation": "\u7ec4\u5408\u571f\u532a\u5728\u5728\u7ebf\u63a8\u8350\u548c\u6392\u5e8f\u4f18\u5316\u7b49\u5e94\u7528\u4e2d\u5f88\u5e38\u89c1\uff0c\u4f46\u5c06\u4f20\u7edf\u7684\u81c2\u6d88\u9664\u65b9\u6cd5\u5e94\u7528\u4e8e\u7ec4\u5408\u571f\u532a\u66f4\u5177\u6311\u6218\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u81c2\u6d88\u9664\u65b9\u6848\uff0c\u5c06\u81c2\u5206\u4e3a\u5df2\u786e\u8ba4\u3001\u6d3b\u52a8\u548c\u5df2\u6dd8\u6c70\u4e09\u7c7b\uff0c\u5e76\u7ed3\u5408\u663e\u5f0f\u63a2\u7d22\u6765\u66f4\u65b0\u8fd9\u4e9b\u96c6\u5408\u3002", "result": "\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u7ec4\u5408\u591a\u81c2\u8001\u864e\u673a\uff08\u5177\u6709\u4e00\u822c\u56fe\u53cd\u9988\uff09\u548c\u7ec4\u5408\u7ebf\u6027\u4e0a\u4e0b\u6587\u8001\u864e\u673a\u8fd9\u4e24\u79cd\u8bbe\u7f6e\u4e2d\u90fd\u53d6\u5f97\u4e86\u63a5\u8fd1\u6700\u4f18\u7684\u9057\u61be\u3002", "conclusion": "\u4e0e\u57fa\u4e8e UCB \u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u7b97\u6cd5\u901a\u8fc7\u663e\u5f0f\u63a2\u7d22\u63d0\u9ad8\u4e86\u6548\u7387\uff0c\u5e76\u8fbe\u5230\u4e86\u4e0e\u4e0b\u754c\u76f8\u5339\u914d\u7684\u6027\u80fd\u3002"}}
{"id": "2510.24619", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.24619", "abs": "https://arxiv.org/abs/2510.24619", "authors": ["Snegha A", "Sayambhu Sen", "Piyush Singh Pasi", "Abhishek Singhania", "Preethi Jyothi"], "title": "Zero-Shot Cross-Lingual Transfer using Prefix-Based Adaptation", "comment": "12 Pages", "summary": "With the release of new large language models (LLMs) like Llama and Mistral,\nzero-shot cross-lingual transfer has become increasingly feasible due to their\nmultilingual pretraining and strong generalization capabilities. However,\nadapting these decoder-only LLMs to new tasks across languages remains\nchallenging. While parameter-efficient fine-tuning (PeFT) techniques like\nLow-Rank Adaptation (LoRA) are widely used, prefix-based techniques such as\nsoft prompt tuning, prefix tuning, and Llama Adapter are less explored,\nespecially for zero-shot transfer in decoder-only models. We present a\ncomprehensive study of three prefix-based methods for zero-shot cross-lingual\ntransfer from English to 35+ high- and low-resource languages. Our analysis\nfurther explores transfer across linguistic families and scripts, as well as\nthe impact of scaling model sizes from 1B to 24B. With Llama 3.1 8B, prefix\nmethods outperform LoRA-baselines by up to 6% on the Belebele benchmark.\nSimilar improvements were observed with Mistral v0.3 7B as well. Despite using\nonly 1.23M learning parameters with prefix tuning, we achieve consistent\nimprovements across diverse benchmarks. These findings highlight the potential\nof prefix-based techniques as an effective and scalable alternative to LoRA,\nparticularly in low-resource multilingual settings.", "AI": {"tldr": "LLMs\u5982Llama\u548cMistral\u7684\u51fa\u73b0\u4f7f\u5f97\u96f6\u6837\u672c\u8de8\u8bed\u8a00\u8fc1\u79fb\u6210\u4e3a\u53ef\u80fd\uff0c\u4f46\u9488\u5bf9\u8fd9\u7c7b\u6a21\u578b\u8fdb\u884c\u8de8\u8bed\u8a00\u4efb\u52a1\u5fae\u8c03\u4ecd\u5177\u6311\u6218\u3002\u672c\u6587\u5168\u9762\u7814\u7a76\u4e86\u4e09\u79cd\u524d\u7f00\u7c7b\u65b9\u6cd5\uff08soft prompt tuning, prefix tuning, Llama Adapter\uff09\u572835+\u79cd\u8bed\u8a00\u4e0a\u7684\u96f6\u6837\u672c\u8de8\u8bed\u8a00\u8fc1\u79fb\u6548\u679c\uff0c\u5e76\u4e0eLoRA\u7b49\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u6280\u672f\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "motivation": "\u867d\u7136LLMs\u5728\u96f6\u6837\u672c\u8de8\u8bed\u8a00\u8fc1\u79fb\u65b9\u9762\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u9488\u5bf9decoder-only LLMs\u8fdb\u884c\u8de8\u8bed\u8a00\u4efb\u52a1\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08PeFT\uff09\u4ecd\u662f\u4e00\u4e2a\u6311\u6218\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u524d\u7f00\u7c7b\u6280\u672f\u7684\u7814\u7a76\u3002", "method": "\u7814\u7a76\u4e86\u4e09\u79cd\u524d\u7f00\u7c7b\u65b9\u6cd5\uff08soft prompt tuning, prefix tuning, Llama Adapter\uff09\u572835+\u79cd\u9ad8\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e0a\u7684\u96f6\u6837\u672c\u8de8\u8bed\u8a00\u8fc1\u79fb\u6548\u679c\uff0c\u5e76\u5206\u6790\u4e86\u8bed\u8a00\u5bb6\u65cf\u3001\u811a\u672c\u4ee5\u53ca\u6a21\u578b\u89c4\u6a21\uff081B\u523024B\uff09\u7684\u5f71\u54cd\u3002", "result": "\u5728Belebele\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4f7f\u7528Llama 3.1 8B\u6a21\u578b\u65f6\uff0c\u524d\u7f00\u7c7b\u65b9\u6cd5\u76f8\u6bd4LoRA\u57fa\u7ebf\u65b9\u6cd5\u6548\u679c\u63d0\u5347\u9ad8\u8fbe6%\u3002Mistral v0.3 7B\u6a21\u578b\u4e5f\u89c2\u5bdf\u5230\u7c7b\u4f3c\u63d0\u5347\u3002\u5373\u4f7f\u53ea\u4f7f\u75281.23M\u53ef\u5b66\u4e60\u53c2\u6570\u7684\u524d\u7f00\u8c03\u4f18\uff0c\u4e5f\u80fd\u5728\u591a\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u6301\u7eed\u6539\u8fdb\u3002", "conclusion": "\u524d\u7f00\u7c7b\u6280\u672f\u662fLoRA\u7684\u6709\u6548\u4e14\u53ef\u6269\u5c55\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5c24\u5176\u5728\u4f4e\u8d44\u6e90\u591a\u8bed\u8a00\u573a\u666f\u4e0b\uff0c\u5c55\u73b0\u51fa\u5de8\u5927\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.24653", "categories": ["cs.CV", "cs.HC", "J.3"], "pdf": "https://arxiv.org/pdf/2510.24653", "abs": "https://arxiv.org/abs/2510.24653", "authors": ["Veronica Thai", "Rui Li", "Meng Ling", "Shuning Jiang", "Jeremy Wolfe", "Raghu Machiraju", "Yan Hu", "Zaibo Li", "Anil Parwani", "Jian Chen"], "title": "Eye-Tracking, Mouse Tracking, Stimulus Tracking,and Decision-Making Datasets in Digital Pathology", "comment": "16 pages, 9 figures, submitted to Nature Scientific Data", "summary": "Interpretation of giga-pixel whole-slide images (WSIs) is an important but\ndifficult task for pathologists. Their diagnostic accuracy is estimated to\naverage around 70%. Adding a second pathologist does not substantially improve\ndecision consistency. The field lacks adequate behavioral data to explain\ndiagnostic errors and inconsistencies. To fill in this gap, we present\nPathoGaze1.0, a comprehensive behavioral dataset capturing the dynamic visual\nsearch and decision-making processes of the full diagnostic workflow during\ncancer diagnosis. The dataset comprises 18.69 hours of eye-tracking, mouse\ninteraction, stimulus tracking, viewport navigation, and diagnostic decision\ndata (EMSVD) collected from 19 pathologists interpreting 397 WSIs. The data\ncollection process emphasizes ecological validity through an\napplication-grounded testbed, called PTAH. In total, we recorded 171,909\nfixations, 263,320 saccades, and 1,867,362 mouse interaction events. In\naddition, such data could also be used to improve the training of both\npathologists and AI systems that might support human experts. All experiments\nwere preregistered at https://osf.io/hj9a7, and the complete dataset along with\nanalysis code is available at https://go.osu.edu/pathogaze.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86PathoGaze1.0\u6570\u636e\u96c6\uff0c\u4e00\u4e2a\u5305\u542b\u773c\u52a8\u8ffd\u8e2a\u3001\u9f20\u6807\u4ea4\u4e92\u548c\u8bca\u65ad\u51b3\u7b56\u6570\u636e\u7684\u7efc\u5408\u6570\u636e\u96c6\uff0c\u65e8\u5728\u89e3\u91ca\u75c5\u7406\u5b66\u8bca\u65ad\u4e2d\u7684\u9519\u8bef\u548c\u4e0d\u4e00\u81f4\u6027\u3002", "motivation": "\u75c5\u7406\u5b66\u5bb6\u5728\u89e3\u8bfb\u5de8\u50cf\u7ea7\u5168\u5207\u7247\u56fe\u50cf\uff08WSIs\uff09\u65f6\u9762\u4e34\u6311\u6218\uff0c\u8bca\u65ad\u51c6\u786e\u7387\u5e73\u5747\u7ea6\u4e3a70%\uff0c\u4e14\u589e\u52a0\u7b2c\u4e8c\u4f4d\u75c5\u7406\u5b66\u5bb6\u5e76\u4e0d\u80fd\u663e\u8457\u63d0\u9ad8\u51b3\u7b56\u4e00\u81f4\u6027\u3002\u76ee\u524d\u7f3a\u4e4f\u884c\u4e3a\u6570\u636e\u6765\u89e3\u91ca\u8bca\u65ad\u9519\u8bef\u548c\u4e0d\u4e00\u81f4\u6027\u3002", "method": "\u6536\u96c6\u4e8619\u4f4d\u75c5\u7406\u5b66\u5bb6\u5728\u4f7f\u7528PTAH\u6d4b\u8bd5\u5e73\u53f0\u89e3\u8bfb397\u5f20WSIs\u65f6\u4ea7\u751f\u768418.69\u5c0f\u65f6\u7684\u773c\u52a8\u8ffd\u8e2a\u3001\u9f20\u6807\u4ea4\u4e92\u3001\u523a\u6fc0\u8ffd\u8e2a\u3001\u89c6\u53e3\u5bfc\u822a\u548c\u8bca\u65ad\u51b3\u7b56\u6570\u636e\uff08EMSVD\uff09\u3002", "result": "\u8bb0\u5f55\u4e86\u603b\u8ba1171,909\u6b21\u6ce8\u89c6\u3001263,320\u6b21\u626b\u89c6\u548c1,867,362\u6b21\u9f20\u6807\u4ea4\u4e92\u4e8b\u4ef6\uff0c\u5f3a\u8c03\u4e86\u5b9e\u9a8c\u7684\u751f\u6001\u6548\u5ea6\u3002", "conclusion": "PathoGaze1.0\u6570\u636e\u96c6\u53ef\u4ee5\u7528\u6765\u6539\u8fdb\u75c5\u7406\u5b66\u5bb6\u548c\u8f85\u52a9\u4eba\u7c7b\u4e13\u5bb6\u7684AI\u7cfb\u7edf\u7684\u57f9\u8bad\u3002"}}
{"id": "2510.23994", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23994", "abs": "https://arxiv.org/abs/2510.23994", "authors": ["Geoffery Agorku", "Sarah Hernandez", "Hayley Hames", "Cade Wagner"], "title": "Predicting Barge Tow Size on Inland Waterways Using Vessel Trajectory Derived Features: Proof of Concept", "comment": null, "summary": "Accurate, real-time estimation of barge quantity on inland waterways remains\na critical challenge due to the non-self-propelled nature of barges and the\nlimitations of existing monitoring systems. This study introduces a novel\nmethod to use Automatic Identification System (AIS) vessel tracking data to\npredict the number of barges in tow using Machine Learning (ML). To train and\ntest the model, barge instances were manually annotated from satellite scenes\nacross the Lower Mississippi River. Labeled images were matched to AIS vessel\ntracks using a spatiotemporal matching procedure. A comprehensive set of 30\nAIS-derived features capturing vessel geometry, dynamic movement, and\ntrajectory patterns were created and evaluated using Recursive Feature\nElimination (RFE) to identify the most predictive variables. Six regression\nmodels, including ensemble, kernel-based, and generalized linear approaches,\nwere trained and evaluated. The Poisson Regressor model yielded the best\nperformance, achieving a Mean Absolute Error (MAE) of 1.92 barges using 12 of\nthe 30 features. The feature importance analysis revealed that metrics\ncapturing vessel maneuverability such as course entropy, speed variability and\ntrip length were most predictive of barge count. The proposed approach provides\na scalable, readily implementable method for enhancing Maritime Domain\nAwareness (MDA), with strong potential applications in lock scheduling, port\nmanagement, and freight planning. Future work will expand the proof of concept\npresented here to explore model transferability to other inland rivers with\ndiffering operational and environmental conditions.", "AI": {"tldr": "\u5229\u7528AIS\u6570\u636e\u548c\u673a\u5668\u5b66\u4e60\u51c6\u786e\u4f30\u7b97\u5185\u6cb3\u9a73\u8239\u6570\u91cf\uff0c\u63d0\u5347\u6d77\u4e8b\u6001\u52bf\u611f\u77e5\u80fd\u529b\u3002", "motivation": "\u51c6\u786e\u3001\u5b9e\u65f6\u5730\u4f30\u7b97\u5185\u6cb3\u9a73\u8239\u6570\u91cf\u5177\u6709\u6311\u6218\u6027\uff0c\u73b0\u6709\u76d1\u63a7\u7cfb\u7edf\u5b58\u5728\u5c40\u9650\u3002", "method": "\u4f7f\u7528\u81ea\u52a8\u8bc6\u522b\u7cfb\u7edf\uff08AIS\uff09\u8239\u8236\u8ffd\u8e2a\u6570\u636e\uff0c\u7ed3\u5408\u673a\u5668\u5b66\u4e60\uff0c\u901a\u8fc7\u5206\u6790\u8239\u8236\u51e0\u4f55\u5f62\u72b6\u3001\u52a8\u6001\u8fd0\u52a8\u548c\u8f68\u8ff9\u6a21\u5f0f\u7b4930\u4e2a\u7279\u5f81\uff0c\u8bad\u7ec3\u548c\u8bc4\u4f30\u4e86\u516d\u79cd\u56de\u5f52\u6a21\u578b\uff0c\u5e76\u4f7f\u7528\u9012\u5f52\u7279\u5f81\u6d88\u9664\uff08RFE\uff09\u8bc6\u522b\u4e86\u6700\u91cd\u8981\u7684\u9884\u6d4b\u53d8\u91cf\u3002", "result": "\u6cca\u677e\u56de\u5f52\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\uff08MAE\uff09\u4e3a1.92\u8258\u9a73\u8239\uff0c\u4f7f\u7528\u4e8612\u4e2a\u7279\u5f81\u3002\u7279\u5f81\u91cd\u8981\u6027\u5206\u6790\u663e\u793a\uff0c\u6355\u83b7\u8239\u8236\u673a\u52a8\u6027\u7684\u6307\u6807\uff08\u5982\u822a\u5411\u71b5\u3001\u901f\u5ea6\u53d8\u5316\u548c\u822a\u7a0b\u957f\u5ea6\uff09\u5bf9\u9a73\u8239\u6570\u91cf\u7684\u9884\u6d4b\u6027\u6700\u5f3a\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a\u589e\u5f3a\u6d77\u4e8b\u6001\u52bf\u611f\u77e5\u80fd\u529b\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u6613\u4e8e\u5b9e\u65bd\u7684\u65b9\u6cd5\uff0c\u5728\u8239\u95f8\u8c03\u5ea6\u3001\u6e2f\u53e3\u7ba1\u7406\u548c\u8d27\u8fd0\u89c4\u5212\u65b9\u9762\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002\u672a\u6765\u5de5\u4f5c\u5c06\u628a\u8be5\u6982\u5ff5\u9a8c\u8bc1\u6269\u5c55\u5230\u5176\u4ed6\u5185\u6cb3\uff0c\u4ee5\u63a2\u7d22\u6a21\u578b\u7684\u53ef\u8f6c\u79fb\u6027\u3002"}}
{"id": "2510.24626", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24626", "abs": "https://arxiv.org/abs/2510.24626", "authors": ["William Held", "David Hall", "Percy Liang", "Diyi Yang"], "title": "Relative Scaling Laws for LLMs", "comment": null, "summary": "Scaling laws describe how language models improve with additional data,\nparameters, and compute. While widely used, they are typically measured on\naggregate test sets. Aggregate evaluations yield clean trends but average over\nheterogeneous subpopulations, obscuring performance disparities. We introduce\nrelative scaling laws, which track how performance gaps between test\ndistributions evolve with scale rather than focusing solely on absolute error.\nUsing 255 decoder-only Transformers trained under matched-compute (IsoFLOP)\nbudgets from $10^{18}$--$10^{20}$ FLOPs on standard pretraining datasets, we\nfind diverse trajectories: academic domains on MMLU converge toward parity;\nregional English dialects shift depending on population size; and clusters of\nAI risk behaviours split, with capability- and influence-related risks\nincreasing during pretraining while adversarial risks do not. These results\nshow that although scaling improves overall performance, it is not a universal\nequalizer. To support further study, we release all model checkpoints from this\nwork to enable practitioners to measure relative alongside traditional scaling\nlaws, in order to better prioritize robustness challenges in light of the\nbitter lesson.", "AI": {"tldr": "\u867d\u7136\u6269\u5c55\u6027\u63d0\u9ad8\u4e86\u6574\u4f53\u6027\u80fd\uff0c\u4f46\u5e76\u975e\u4e07\u80fd\u7684\u3002", "motivation": "\u73b0\u6709\u7684\u6269\u5c55\u5b9a\u5f8b\u5728\u805a\u5408\u6d4b\u8bd5\u96c6\u4e0a\u8fdb\u884c\u6d4b\u91cf\uff0c\u8fd9\u4f1a\u63a9\u76d6\u4e0d\u540c\u5b50\u7fa4\u4f53\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u5f02\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5f15\u5165\u76f8\u5bf9\u6269\u5c55\u5b9a\u5f8b\u6765\u7814\u7a76\u6027\u80fd\u5dee\u8ddd\u5982\u4f55\u968f\u89c4\u6a21\u53d8\u5316\u3002", "method": "\u4f7f\u7528255\u4e2a\u89e3\u7801\u5668\u6a21\u578bTransformer\uff0c\u5728\u5339\u914d\u8ba1\u7b97\uff08IsoFLOP\uff09\u9884\u7b97\u4e0b\u4ece$10^{18}$--$10^{20}$ FLOPs\u7684\u6807\u51c6\u9884\u8bad\u7ec3\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u5206\u6790\u4e86\u4e0d\u540c\u6d4b\u8bd5\u5206\u5e03\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\u6f14\u53d8\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728MMLU\u4e0a\uff0c\u5b66\u672f\u9886\u57df\u8d8b\u4e8e\u8868\u73b0\u5747\u7b49\uff1b\u82f1\u8bed\u65b9\u8a00\u7684\u8868\u73b0\u4f1a\u6839\u636e\u4eba\u53e3\u89c4\u6a21\u800c\u53d8\u5316\uff1bAI\u98ce\u9669\u884c\u4e3a\u96c6\u7fa4\u51fa\u73b0\u5206\u5316\uff0c\u80fd\u529b\u548c\u5f71\u54cd\u76f8\u5173\u7684\u98ce\u9669\u5728\u9884\u8bad\u7ec3\u671f\u95f4\u589e\u52a0\uff0c\u800c\u5bf9\u6297\u6027\u98ce\u9669\u5219\u6ca1\u6709\u53d8\u5316\u3002", "conclusion": "\u6269\u5c55\u6027\u867d\u7136\u63d0\u9ad8\u4e86\u6574\u4f53\u6027\u80fd\uff0c\u4f46\u5e76\u975e\u5728\u6240\u6709\u65b9\u9762\u90fd\u8868\u73b0\u5747\u7b49\u3002\u672c\u6587\u63d0\u51fa\u7684\u76f8\u5bf9\u6269\u5c55\u5b9a\u5f8b\u4ee5\u53ca\u91ca\u653e\u7684\u6a21\u578b\u68c0\u67e5\u70b9\uff0c\u6709\u52a9\u4e8e\u7814\u7a76\u8005\u66f4\u597d\u5730\u5173\u6ce8\u6a21\u578b\u7684\u9c81\u68d2\u6027\u95ee\u9898\u3002"}}
{"id": "2510.24657", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24657", "abs": "https://arxiv.org/abs/2510.24657", "authors": ["Xuanpu Zhang", "Xuesong Niu", "Ruidong Chen", "Dan Song", "Jianhao Zeng", "Penghui Du", "Haoxiang Cao", "Kai Wu", "An-an Liu"], "title": "Group Relative Attention Guidance for Image Editing", "comment": null, "summary": "Recently, image editing based on Diffusion-in-Transformer models has\nundergone rapid development. However, existing editing methods often lack\neffective control over the degree of editing, limiting their ability to achieve\nmore customized results. To address this limitation, we investigate the\nMM-Attention mechanism within the DiT model and observe that the Query and Key\ntokens share a bias vector that is only layer-dependent. We interpret this bias\nas representing the model's inherent editing behavior, while the delta between\neach token and its corresponding bias encodes the content-specific editing\nsignals. Based on this insight, we propose Group Relative Attention Guidance, a\nsimple yet effective method that reweights the delta values of different tokens\nto modulate the focus of the model on the input image relative to the editing\ninstruction, enabling continuous and fine-grained control over editing\nintensity without any tuning. Extensive experiments conducted on existing image\nediting frameworks demonstrate that GRAG can be integrated with as few as four\nlines of code, consistently enhancing editing quality. Moreover, compared to\nthe commonly used Classifier-Free Guidance, GRAG achieves smoother and more\nprecise control over the degree of editing. Our code will be released at\nhttps://github.com/little-misfit/GRAG-Image-Editing.", "AI": {"tldr": "\u73b0\u6709\u57fa\u4e8eTransformer\u7684\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\u96be\u4ee5\u7cbe\u786e\u63a7\u5236\u7f16\u8f91\u7a0b\u5ea6\uff0c\u9650\u5236\u4e86\u5b9a\u5236\u5316\u7f16\u8f91\u7684\u6548\u679c\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGroup Relative Attention Guidance\uff08GRAG\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790DiT\u6a21\u578b\u4e2d\u7684MM-Attention\u673a\u5236\uff0c\u5229\u7528\u504f\u5dee\u5411\u91cf\u548c\u5185\u5bb9\u76f8\u5173\u7684\u7f16\u8f91\u4fe1\u53f7\u6765\u8c03\u8282\u6a21\u578b\u5bf9\u56fe\u50cf\u548c\u7f16\u8f91\u6307\u4ee4\u7684\u5173\u6ce8\u5ea6\uff0c\u4ece\u800c\u5b9e\u73b0\u5bf9\u7f16\u8f91\u5f3a\u5ea6\u7684\u8fde\u7eed\u3001\u7cbe\u7ec6\u63a7\u5236\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0cGRAG\u6613\u4e8e\u96c6\u6210\uff0c\u80fd\u63d0\u5347\u7f16\u8f91\u8d28\u91cf\uff0c\u5e76\u63d0\u4f9b\u6bd4Classifier-Free Guidance\u66f4\u5e73\u6ed1\u3001\u66f4\u7cbe\u786e\u7684\u7f16\u8f91\u5f3a\u5ea6\u63a7\u5236\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8eTransformer\u7684\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\u5728\u63a7\u5236\u7f16\u8f91\u7a0b\u5ea6\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u96be\u4ee5\u5b9e\u73b0\u7528\u6237\u6240\u9700\u7684\u5b9a\u5236\u5316\u7f16\u8f91\u6548\u679c\u3002", "method": "\u901a\u8fc7\u5206\u6790DiT\u6a21\u578b\u4e2d\u7684MM-Attention\u673a\u5236\uff0c\u63d0\u51faGroup Relative Attention Guidance\uff08GRAG\uff09\u65b9\u6cd5\u3002GRAG\u901a\u8fc7\u91cd\u65b0\u52a0\u6743\u4e0d\u540cToken\u7684delta\u503c\uff0c\u6765\u8c03\u8282\u6a21\u578b\u5bf9\u8f93\u5165\u56fe\u50cf\u76f8\u5bf9\u4e8e\u7f16\u8f91\u6307\u4ee4\u7684\u5173\u6ce8\u5ea6\uff0c\u4ece\u800c\u5b9e\u73b0\u5bf9\u7f16\u8f91\u5f3a\u5ea6\u7684\u8fde\u7eed\u3001\u7cbe\u7ec6\u63a7\u5236\u3002", "result": "GRAG\u80fd\u591f\u8f7b\u677e\u96c6\u6210\u5230\u73b0\u6709\u56fe\u50cf\u7f16\u8f91\u6846\u67b6\u4e2d\uff08\u4ec5\u9700\u56db\u884c\u4ee3\u7801\uff09\uff0c\u5e76\u6301\u7eed\u63d0\u5347\u7f16\u8f91\u8d28\u91cf\u3002\u4e0eClassifier-Free Guidance\u76f8\u6bd4\uff0cGRAG\u5728\u7f16\u8f91\u7a0b\u5ea6\u7684\u63a7\u5236\u4e0a\u66f4\u52a0\u5e73\u6ed1\u548c\u7cbe\u786e\u3002", "conclusion": "GRAG\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5b9e\u73b0\u5bf9\u57fa\u4e8eTransformer\u7684\u56fe\u50cf\u7f16\u8f91\u7684\u8fde\u7eed\u3001\u7cbe\u7ec6\u63a7\u5236\uff0c\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u80fd\u5728\u4e0d\u8fdb\u884c\u4efb\u4f55\u989d\u5916\u8c03\u6574\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u7f16\u8f91\u8d28\u91cf\u3002"}}
{"id": "2510.24012", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24012", "abs": "https://arxiv.org/abs/2510.24012", "authors": ["Byeonghu Na", "Mina Kang", "Jiseok Kwak", "Minsang Park", "Jiwoo Shin", "SeJoon Jun", "Gayoung Lee", "Jin-Hwa Kim", "Il-Chul Moon"], "title": "Training-Free Safe Text Embedding Guidance for Text-to-Image Diffusion Models", "comment": "Accepted at NeurIPS 2025", "summary": "Text-to-image models have recently made significant advances in generating\nrealistic and semantically coherent images, driven by advanced diffusion models\nand large-scale web-crawled datasets. However, these datasets often contain\ninappropriate or biased content, raising concerns about the generation of\nharmful outputs when provided with malicious text prompts. We propose Safe Text\nembedding Guidance (STG), a training-free approach to improve the safety of\ndiffusion models by guiding the text embeddings during sampling. STG adjusts\nthe text embeddings based on a safety function evaluated on the expected final\ndenoised image, allowing the model to generate safer outputs without additional\ntraining. Theoretically, we show that STG aligns the underlying model\ndistribution with safety constraints, thereby achieving safer outputs while\nminimally affecting generation quality. Experiments on various safety\nscenarios, including nudity, violence, and artist-style removal, show that STG\nconsistently outperforms both training-based and training-free baselines in\nremoving unsafe content while preserving the core semantic intent of input\nprompts. Our code is available at https://github.com/aailab-kaist/STG.", "AI": {"tldr": "STG\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u91c7\u6837\u8fc7\u7a0b\u4e2d\u5f15\u5bfc\u6587\u672c\u5d4c\u5165\u6765\u63d0\u9ad8\u6269\u6563\u6a21\u578b\u7684\u5b89\u5168\u6027\uff0c\u4ece\u800c\u5728\u751f\u6210\u56fe\u50cf\u65f6\u907f\u514d\u4e0d\u5f53\u5185\u5bb9\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5b58\u5728\u751f\u6210\u4e0d\u5f53\u6216\u5e26\u6709\u504f\u89c1\u5185\u5bb9\u7684\u98ce\u9669\uff0c\u5f53\u4f7f\u7528\u6076\u610f\u6587\u672c\u63d0\u793a\u65f6\uff0c\u53ef\u80fd\u4f1a\u4ea7\u751f\u6709\u5bb3\u8f93\u51fa\u3002", "method": "STG\u901a\u8fc7\u4e00\u4e2a\u5728\u9884\u671f\u6700\u7ec8\u53bb\u566a\u56fe\u50cf\u4e0a\u8bc4\u4f30\u7684\u5b89\u5168\u51fd\u6570\u6765\u8c03\u6574\u6587\u672c\u5d4c\u5165\uff0c\u4ece\u800c\u5728\u91c7\u6837\u8fc7\u7a0b\u4e2d\u6307\u5bfc\u6587\u672c\u5d4c\u5165\uff0c\u5b9e\u73b0\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u751f\u6210\u66f4\u5b89\u5168\u7684\u8f93\u51fa\u3002", "result": "STG\u5728\u53bb\u9664\u4e0d\u5b89\u5168\u5185\u5bb9\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u57fa\u4e8e\u8bad\u7ec3\u548c\u65e0\u9700\u8bad\u7ec3\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u540c\u65f6\u80fd\u6700\u5927\u9650\u5ea6\u5730\u4fdd\u7559\u8f93\u5165\u63d0\u793a\u7684\u6838\u5fc3\u8bed\u4e49\u610f\u56fe\u3002\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u5728\u53bb\u9664\u88f8\u9732\u3001\u66b4\u529b\u548c\u827a\u672f\u5bb6\u98ce\u683c\u7b49\u5404\u79cd\u5b89\u5168\u573a\u666f\u4e0b\u7684\u6709\u6548\u6027\u3002", "conclusion": "STG\u901a\u8fc7\u5c06\u57fa\u7840\u6a21\u578b\u5206\u5e03\u4e0e\u5b89\u5168\u7ea6\u675f\u5bf9\u9f50\uff0c\u5728\u4fdd\u8bc1\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u6709\u6548\u63d0\u9ad8\u4e86\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u7684\u5b89\u5168\u6027\u3002"}}
{"id": "2510.24628", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24628", "abs": "https://arxiv.org/abs/2510.24628", "authors": ["Anh Ngo", "Nicolas Rollet", "Catherine Pelachaud", "Chloe Clavel"], "title": "\"Mm, Wat?\" Detecting Other-initiated Repair Requests in Dialogue", "comment": "9 pages", "summary": "Maintaining mutual understanding is a key component in human-human\nconversation to avoid conversation breakdowns, in which repair, particularly\nOther-Initiated Repair (OIR, when one speaker signals trouble and prompts the\nother to resolve), plays a vital role. However, Conversational Agents (CAs)\nstill fail to recognize user repair initiation, leading to breakdowns or\ndisengagement. This work proposes a multimodal model to automatically detect\nrepair initiation in Dutch dialogues by integrating linguistic and prosodic\nfeatures grounded in Conversation Analysis. The results show that prosodic cues\ncomplement linguistic features and significantly improve the results of\npretrained text and audio embeddings, offering insights into how different\nfeatures interact. Future directions include incorporating visual cues,\nexploring multilingual and cross-context corpora to assess the robustness and\ngeneralizability.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u6a21\u578b\uff0c\u7528\u4e8e\u901a\u8fc7\u6574\u5408\u57fa\u4e8e\u5bf9\u8bdd\u5206\u6790\u7684\u8bed\u8a00\u548c\u97f5\u5f8b\u7279\u5f81\u6765\u81ea\u52a8\u68c0\u6d4b\u8377\u5170\u8bed\u5bf9\u8bdd\u4e2d\u7684\u4fee\u590d\u8bed. \u7ed3\u679c\u8868\u660e\uff0c\u97f5\u5f8b\u7ebf\u7d22\u53ef\u4ee5\u8865\u5145\u8bed\u8a00\u7279\u5f81\uff0c\u5e76\u663e\u8457\u63d0\u9ad8\u9884\u8bad\u7ec3\u6587\u672c\u548c\u97f3\u9891\u5d4c\u5165\u7684\u6548\u679c\u3002", "motivation": "\u5bf9\u8bdd\u4ee3\u7406\uff08CA\uff09\u672a\u80fd\u8bc6\u522b\u7528\u6237\u7684\u4fee\u590d\u8bed\uff0c\u5bfc\u81f4\u5bf9\u8bdd\u4e2d\u65ad\u6216\u7528\u6237\u5931\u53bb\u53c2\u4e0e\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5f00\u53d1\u4e00\u79cd\u591a\u6a21\u6001\u6a21\u578b\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u6574\u5408\u4e86\u57fa\u4e8e\u5bf9\u8bdd\u5206\u6790\u7684\u8bed\u8a00\u548c\u97f5\u5f8b\u7279\u5f81\uff0c\u4ee5\u81ea\u52a8\u68c0\u6d4b\u8377\u5170\u8bed\u5bf9\u8bdd\u4e2d\u7684\u4fee\u590d\u8bed\u3002", "result": "\u6240\u63d0\u51fa\u7684\u6a21\u578b\u901a\u8fc7\u6574\u5408\u8bed\u8a00\u548c\u97f5\u5f8b\u7279\u5f81\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4fee\u590d\u8bed\u68c0\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u5176\u4e2d\u97f5\u5f8b\u7ebf\u7d22\u5bf9\u9884\u8bad\u7ec3\u7684\u6587\u672c\u548c\u97f3\u9891\u5d4c\u5165\u8d77\u5230\u4e86\u8865\u5145\u4f5c\u7528\u3002", "conclusion": "\u591a\u6a21\u6001\u6a21\u578b\u80fd\u591f\u6709\u6548\u68c0\u6d4b\u4fee\u590d\u8bed\uff0c\u5e76\u4e14\u97f5\u5f8b\u7ebf\u7d22\u5728\u5176\u4e2d\u626e\u6f14\u7740\u91cd\u8981\u89d2\u8272\u3002\u672a\u6765\u7684\u5de5\u4f5c\u5c06\u63a2\u7d22\u52a0\u5165\u89c6\u89c9\u7ebf\u7d22\u4ee5\u53ca\u591a\u8bed\u8a00\u548c\u8de8\u8bed\u5883\u8bed\u6599\u5e93\u4ee5\u63d0\u9ad8\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2510.24667", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24667", "abs": "https://arxiv.org/abs/2510.24667", "authors": ["Mia Kan", "Yilin Liu", "Niloy Mitra"], "title": "SAGE: Structure-Aware Generative Video Transitions between Diverse Clips", "comment": "Website: https://kan32501.github.io/sage.github.io/", "summary": "Video transitions aim to synthesize intermediate frames between two clips,\nbut naive approaches such as linear blending introduce artifacts that limit\nprofessional use or break temporal coherence. Traditional techniques\n(cross-fades, morphing, frame interpolation) and recent generative inbetweening\nmethods can produce high-quality plausible intermediates, but they struggle\nwith bridging diverse clips involving large temporal gaps or significant\nsemantic differences, leaving a gap for content-aware and visually coherent\ntransitions. We address this challenge by drawing on artistic workflows,\ndistilling strategies such as aligning silhouettes and interpolating salient\nfeatures to preserve structure and perceptual continuity. Building on this, we\npropose SAGE (Structure-Aware Generative vidEo transitions) as a zeroshot\napproach that combines structural guidance, provided via line maps and motion\nflow, with generative synthesis, enabling smooth, semantically consistent\ntransitions without fine-tuning. Extensive experiments and comparison with\ncurrent alternatives, namely [FILM, TVG, DiffMorpher, VACE, GI], demonstrate\nthat SAGE outperforms both classical and generative baselines on quantitative\nmetrics and user studies for producing transitions between diverse clips. Code\nto be released on acceptance.", "AI": {"tldr": "SAGE\u662f\u4e00\u4e2a\u96f6\u6837\u672c\u89c6\u9891\u8f6c\u573a\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u7ed3\u6784\u5f15\u5bfc\u548c\u751f\u6210\u5408\u6210\uff0c\u5b9e\u73b0\u4e86\u4e0d\u540c\u89c6\u9891\u7247\u6bb5\u95f4\u7684\u5e73\u6ed1\u3001\u8bed\u4e49\u4e00\u81f4\u7684\u8f6c\u573a\uff0c\u5e76\u5728\u5b9a\u91cf\u6307\u6807\u548c\u7528\u6237\u7814\u7a76\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u8f6c\u573a\u65b9\u6cd5\u5728\u5904\u7406\u65f6\u95f4\u8de8\u5ea6\u5927\u6216\u8bed\u4e49\u5dee\u5f02\u5927\u7684\u89c6\u9891\u7247\u6bb5\u65f6\u5b58\u5728\u4e0d\u8db3\uff0c\u65e0\u6cd5\u5b9e\u73b0\u5185\u5bb9\u611f\u77e5\u548c\u89c6\u89c9\u8fde\u8d2f\u7684\u8f6c\u573a\u3002", "method": "SAGE\u878d\u5408\u4e86\u827a\u672f\u521b\u4f5c\u6d41\u7a0b\u4e2d\u7684\u7b56\u7565\uff0c\u5982\u5bf9\u9f50\u8f6e\u5ed3\u548c\u63d2\u503c\u663e\u8457\u7279\u5f81\uff0c\u5e76\u7ed3\u5408\u4e86\u7ebf\u56fe\u548c\u8fd0\u52a8\u6d41\u63d0\u4f9b\u7684\u7ed3\u6784\u5f15\u5bfc\uff0c\u4ee5\u53ca\u751f\u6210\u5408\u6210\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u96f6\u6837\u672c\u7684\u89c6\u9891\u8f6c\u573a\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cSAGE\u5728\u5904\u7406\u4e0d\u540c\u89c6\u9891\u7247\u6bb5\u7684\u8f6c\u573a\u65f6\uff0c\u5728\u5b9a\u91cf\u6307\u6807\u548c\u7528\u6237\u7814\u7a76\u65b9\u9762\u5747\u4f18\u4e8e\u4f20\u7edf\u548c\u751f\u6210\u5f0f\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "SAGE\u901a\u8fc7\u7ed3\u5408\u7ed3\u6784\u5f15\u5bfc\u548c\u751f\u6210\u5408\u6210\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u9891\u8f6c\u573a\u4e2d\u7684\u6311\u6218\uff0c\u80fd\u591f\u751f\u6210\u5e73\u6ed1\u4e14\u8bed\u4e49\u4e00\u81f4\u7684\u8f6c\u573a\uff0c\u5e76\u5728\u5404\u79cd\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2510.24025", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24025", "abs": "https://arxiv.org/abs/2510.24025", "authors": ["Guo Tianqi Guo", "Chen Liping", "Peng Ciyuan", "Guo Jingjing", "Ren Jing"], "title": "NeuroPathNet: Dynamic Path Trajectory Learning for Brain Functional Connectivity Analysis", "comment": null, "summary": "Understanding the evolution of brain functional networks over time is of\ngreat significance for the analysis of cognitive mechanisms and the diagnosis\nof neurological diseases. Existing methods often have difficulty in capturing\nthe temporal evolution characteristics of connections between specific\nfunctional communities. To this end, this paper proposes a new path-level\ntrajectory modeling framework (NeuroPathNet) to characterize the dynamic\nbehavior of connection pathways between brain functional partitions. Based on\nmedically supported static partitioning schemes (such as Yeo and Smith ICA), we\nextract the time series of connection strengths between each pair of functional\npartitions and model them using a temporal neural network. We validate the\nmodel performance on three public functional Magnetic Resonance Imaging (fMRI)\ndatasets, and the results show that it outperforms existing mainstream methods\nin multiple indicators. This study can promote the development of dynamic graph\nlearning methods for brain network analysis, and provide possible clinical\napplications for the diagnosis of neurological diseases.", "AI": {"tldr": "NeuroPathNet\u6846\u67b6\u901a\u8fc7\u5728\u9759\u6001\u5206\u533a\u65b9\u6848\u7684\u57fa\u7840\u4e0a\u63d0\u53d6\u65f6\u95f4\u5e8f\u5217\u8fde\u63a5\u5f3a\u5ea6\uff0c\u5e76\u4f7f\u7528\u65f6\u95f4\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u5efa\u6a21\uff0c\u6765\u8868\u5f81\u8111\u529f\u80fd\u5206\u533a\u4e4b\u95f4\u8fde\u63a5\u8def\u5f84\u7684\u52a8\u6001\u884c\u4e3a\uff0c\u5e76\u5728\u4e09\u4e2afMRI\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u4f18\u4e8e\u73b0\u6709\u4e3b\u6d41\u65b9\u6cd5\uff0c\u53ef\u7528\u4e8e\u52a8\u6001\u56fe\u5b66\u4e60\u548c\u795e\u7ecf\u7cfb\u7edf\u75be\u75c5\u7684\u4e34\u5e8a\u8bca\u65ad\u3002", "motivation": "\u7406\u89e3\u5927\u8111\u529f\u80fd\u7f51\u7edc\u968f\u65f6\u95f4\u6f14\u53d8\u5bf9\u4e8e\u8ba4\u77e5\u673a\u5236\u5206\u6790\u548c\u795e\u7ecf\u7cfb\u7edf\u75be\u75c5\u8bca\u65ad\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u529f\u80fd\u793e\u533a\u4e4b\u95f4\u8fde\u63a5\u7684\u65f6\u95f4\u6f14\u5316\u7279\u5f81\u3002", "method": "\u57fa\u4e8e\u9759\u6001\u5206\u533a\u65b9\u6848\uff08\u5982Yeo\u548cSmith ICA\uff09\uff0c\u63d0\u53d6\u529f\u80fd\u5206\u533a\u4e4b\u95f4\u8fde\u63a5\u5f3a\u5ea6\u7684\u65f6\u95f4\u5e8f\u5217\uff0c\u5e76\u4f7f\u7528\u65f6\u95f4\u795e\u7ecf\u7f51\u7edc\u5efa\u6a21\uff0c\u63d0\u51faNeuroPathNet\u6846\u67b6\u3002", "result": "\u5728\u4e09\u4e2afMRI\u6570\u636e\u96c6\u4e0a\u7684\u9a8c\u8bc1\u7ed3\u679c\u663e\u793a\uff0cNeuroPathNet\u7684\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u4e3b\u6d41\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u53ef\u4fc3\u8fdb\u52a8\u6001\u56fe\u5b66\u4e60\u65b9\u6cd5\u5728\u8111\u7f51\u7edc\u5206\u6790\u4e2d\u7684\u53d1\u5c55\uff0c\u5e76\u4e3a\u795e\u7ecf\u7cfb\u7edf\u75be\u75c5\u7684\u8bca\u65ad\u63d0\u4f9b\u53ef\u80fd\u7684\u4e34\u5e8a\u5e94\u7528\u3002"}}
{"id": "2510.24636", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24636", "abs": "https://arxiv.org/abs/2510.24636", "authors": ["Ziyou Hu", "Zhengliang Shi", "Minghang Zhu", "Haitao Li", "Teng Sun", "Pengjie Ren", "Suzan Verberne", "Zhaochun Ren"], "title": "OpenReward: Learning to Reward Long-form Agentic Tasks via Reinforcement Learning", "comment": null, "summary": "Reward models (RMs) have become essential for aligning large language models\n(LLMs), serving as scalable proxies for human evaluation in both training and\ninference. However, existing RMs struggle on knowledge-intensive and long-form\ntasks, where evaluating correctness requires grounding beyond the model's\ninternal knowledge. This limitation hinders them from reliably discriminating\nsubtle quality differences, especially when external evidence is necessary. To\naddress this, we introduce OpenRM, a tool-augmented long-form reward model that\nsystematically judges open-ended responses by invoking external tools to gather\nrelevant evidence. We train OpenRM with Group Relative Policy Optimization\n(GRPO) on over 27K synthesized pairwise examples generated through a\ncontrollable data synthesis framework. The training objective jointly\nsupervises intermediate tool usage and final outcome accuracy, incentivizing\nour reward model to learn effective evidence-based judgment strategies.\nExtensive experiments on three newly-collected datasets and two widely-used\nbenchmarks demonstrate that OpenRM substantially outperforms existing reward\nmodeling approaches. As a further step, we integrate OpenRM into both\ninference-time response selection and training-time data selection. This yields\nconsistent gains in downstream LLM alignment tasks, highlighting the potential\nof tool-augmented reward models for scaling reliable long-form evaluation.", "AI": {"tldr": "OpenRM\u662f\u4e00\u4e2a\u5de5\u5177\u589e\u5f3a\u7684\u957f\u7bc7\u5956\u52b1\u6a21\u578b\uff0c\u901a\u8fc7\u8c03\u7528\u5916\u90e8\u5de5\u5177\u6536\u96c6\u76f8\u5173\u8bc1\u636e\u6765\u7cfb\u7edf\u5730\u8bc4\u4f30\u5f00\u653e\u5f0f\u54cd\u5e94\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5956\u52b1\u6a21\u578b\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u548c\u957f\u7bc7\u4efb\u52a1\u4e0a\u7684\u4e0d\u8db3\uff0c\u5e76\u5728\u4e0b\u6e38LLM\u5bf9\u9f50\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6539\u8fdb\u3002", "motivation": "\u73b0\u6709\u7684\u5956\u52b1\u6a21\u578b\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u548c\u957f\u7bc7\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u4e3a\u8bc4\u4f30\u5176\u6b63\u786e\u6027\u9700\u8981\u6a21\u578b\u8d85\u51fa\u5176\u5185\u90e8\u77e5\u8bc6\u7684\u4f9d\u636e\uff0c\u8fd9\u963b\u788d\u4e86\u5b83\u4eec\u5728\u9700\u8981\u5916\u90e8\u8bc1\u636e\u65f6\u53ef\u9760\u5730\u533a\u5206\u7ec6\u5fae\u7684\u8d28\u91cf\u5dee\u5f02\u3002", "method": "OpenRM\u901a\u8fc7\u8c03\u7528\u5916\u90e8\u5de5\u5177\u6765\u6536\u96c6\u76f8\u5173\u8bc1\u636e\uff0c\u5e76\u4f7f\u7528\u7fa4\u4f53\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08GRPO\uff09\u5728\u8d85\u8fc727K\u7684\u5408\u6210\u914d\u5bf9\u6837\u672c\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u8bad\u7ec3\u76ee\u6807\u5171\u540c\u76d1\u7763\u4e2d\u95f4\u5de5\u5177\u7684\u4f7f\u7528\u548c\u6700\u7ec8\u7ed3\u679c\u7684\u51c6\u786e\u6027\u3002", "result": "\u5728\u4e09\u4e2a\u65b0\u6536\u96c6\u7684\u6570\u636e\u96c6\u548c\u4e24\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u57fa\u51c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cOpenRM\u7684\u6027\u80fd\u5927\u5927\u4f18\u4e8e\u73b0\u6709\u7684\u5956\u52b1\u5efa\u6a21\u65b9\u6cd5\u3002", "conclusion": "OpenRM\u5728\u4e0b\u6e38LLM\u5bf9\u9f50\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u6301\u7eed\u7684\u6536\u76ca\uff0c\u8bc1\u660e\u4e86\u5de5\u5177\u589e\u5f3a\u7684\u5956\u52b1\u6a21\u578b\u5728\u6269\u5c55\u53ef\u9760\u7684\u957f\u7bc7\u8bc4\u4f30\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.24688", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24688", "abs": "https://arxiv.org/abs/2510.24688", "authors": ["Yun Zhang", "Zhaoliang Zheng", "Johnson Liu", "Zhiyu Huang", "Zewei Zhou", "Zonglin Meng", "Tianhui Cai", "Jiaqi Ma"], "title": "MIC-BEV: Multi-Infrastructure Camera Bird's-Eye-View Transformer with Relation-Aware Fusion for 3D Object Detection", "comment": null, "summary": "Infrastructure-based perception plays a crucial role in intelligent\ntransportation systems, offering global situational awareness and enabling\ncooperative autonomy. However, existing camera-based detection models often\nunderperform in such scenarios due to challenges such as multi-view\ninfrastructure setup, diverse camera configurations, degraded visual inputs,\nand various road layouts. We introduce MIC-BEV, a Transformer-based\nbird's-eye-view (BEV) perception framework for infrastructure-based\nmulti-camera 3D object detection. MIC-BEV flexibly supports a variable number\nof cameras with heterogeneous intrinsic and extrinsic parameters and\ndemonstrates strong robustness under sensor degradation. The proposed\ngraph-enhanced fusion module in MIC-BEV integrates multi-view image features\ninto the BEV space by exploiting geometric relationships between cameras and\nBEV cells alongside latent visual cues. To support training and evaluation, we\nintroduce M2I, a synthetic dataset for infrastructure-based object detection,\nfeaturing diverse camera configurations, road layouts, and environmental\nconditions. Extensive experiments on both M2I and the real-world dataset\nRoScenes demonstrate that MIC-BEV achieves state-of-the-art performance in 3D\nobject detection. It also remains robust under challenging conditions,\nincluding extreme weather and sensor degradation. These results highlight the\npotential of MIC-BEV for real-world deployment. The dataset and source code are\navailable at: https://github.com/HandsomeYun/MIC-BEV.", "AI": {"tldr": "MIC-BEV\u662f\u4e00\u4e2aTransformer-based\u7684BEV\u611f\u77e5\u6846\u67b6\uff0c\u7528\u4e8e\u57fa\u7840\u8bbe\u65bd\u7684\u591a\u6444\u50cf\u59343D\u76ee\u6807\u68c0\u6d4b\uff0c\u5728M2I\u548cRoScenes\u6570\u636e\u96c6\u4e0a\u5747 \u0111\u1ea1t \u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5bf9\u5404\u79cd\u6311\u6218\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u76f8\u673a\u7684\u68c0\u6d4b\u6a21\u578b\u5728\u57fa\u7840\u8bbe\u65bd\u611f\u77e5\u573a\u666f\u4e0b\u9762\u4e34\u591a\u89c6\u56fe\u8bbe\u7f6e\u3001\u76f8\u673a\u914d\u7f6e\u591a\u6837\u3001\u89c6\u89c9\u8f93\u5165\u9000\u5316\u4ee5\u53ca\u9053\u8def\u5e03\u5c40\u590d\u6742\u7b49\u6311\u6218\uff0c\u6027\u80fd\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMIC-BEV\u7684Transformer-based BEV\u611f\u77e5\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u7075\u6d3b\u652f\u6301\u5177\u6709\u5f02\u6784\u5185\u5916\u53c2\u6570\u7684\u53ef\u53d8\u6570\u91cf\u7684\u6444\u50cf\u5934\uff0c\u5e76\u901a\u8fc7\u56fe\u589e\u5f3a\u878d\u5408\u6a21\u5757\u5229\u7528\u76f8\u673a\u4e0eBEV\u5355\u5143\u4e4b\u95f4\u7684\u51e0\u4f55\u5173\u7cfb\u4ee5\u53ca\u6f5c\u5728\u7684\u89c6\u89c9\u7ebf\u7d22\u6765\u878d\u5408\u591a\u89c6\u56fe\u56fe\u50cf\u7279\u5f81\u3002\u540c\u65f6\uff0c\u5f15\u5165\u4e86M2I\u5408\u6210\u6570\u636e\u96c6\u6765\u652f\u6301\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002", "result": "\u5728M2I\u548cRoScenes\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cMIC-BEV\u57283D\u76ee\u6807\u68c0\u6d4b\u65b9\u9762\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u5728\u6781\u7aef\u5929\u6c14\u548c\u4f20\u611f\u5668\u9000\u5316\u7b49\u5177\u6709\u6311\u6218\u6027\u7684\u6761\u4ef6\u4e0b\u4fdd\u6301\u4e86\u9c81\u68d2\u6027\u3002", "conclusion": "MIC-BEV\u57283D\u76ee\u6807\u68c0\u6d4b\u65b9\u9762\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5bf9\u5404\u79cd\u6311\u6218\u8868\u73b0\u51fa\u9c81\u68d2\u6027\uff0c\u663e\u793a\u4e86\u5176\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.24026", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24026", "abs": "https://arxiv.org/abs/2510.24026", "authors": ["Jiaqi Luo", "Shixin Xu", "Zhouwang Yang"], "title": "Efficient Global-Local Fusion Sampling for Physics-Informed Neural Networks", "comment": null, "summary": "The accuracy of Physics-Informed Neural Networks (PINNs) critically depends\non the placement of collocation points, as the PDE loss is approximated through\nsampling over the solution domain. Global sampling ensures stability by\ncovering the entire domain but requires many samples and is computationally\nexpensive, whereas local sampling improves efficiency by focusing on\nhigh-residual regions but may neglect well-learned areas, reducing robustness.\nWe propose a Global-Local Fusion (GLF) Sampling Strategy that combines the\nstrengths of both approaches. Specifically, new collocation points are\ngenerated by perturbing training points with Gaussian noise scaled inversely to\nthe residual, thereby concentrating samples in difficult regions while\npreserving exploration. To further reduce computational overhead, a lightweight\nlinear surrogate is introduced to approximate the global residual-based\ndistribution, achieving similar effectiveness at a fraction of the cost.\nTogether, these components, residual-adaptive sampling and residual-based\napproximation, preserve the stability of global methods while retaining the\nefficiency of local refinement. Extensive experiments on benchmark PDEs\ndemonstrate that GLF consistently improves both accuracy and efficiency\ncompared with global and local sampling strategies. This study provides a\npractical and scalable framework for enhancing the reliability and efficiency\nof PINNs in solving complex and high-dimensional PDEs.", "AI": {"tldr": "\u901a\u8fc7\u6b8b\u5dee\u81ea\u9002\u5e94\u91c7\u6837\u548c\u6b8b\u5dee\u8fd1\u4f3c\u6765\u63d0\u9ad8\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u7684\u7cbe\u5ea6\u548c\u6548\u7387\u3002", "motivation": "\u5168\u5c40\u91c7\u6837\u7a33\u5b9a\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u5c40\u90e8\u91c7\u6837\u9ad8\u6548\u4f46\u53ef\u80fd\u5ffd\u7565\u5df2\u5b66\u597d\u7684\u533a\u57df\uff0c\u9c81\u68d2\u6027\u5dee\u3002\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u6539\u8fdb\u3002", "method": "\u63d0\u51fa\u5168\u5c40-\u5c40\u90e8\u878d\u5408\uff08GLF\uff09\u91c7\u6837\u7b56\u7565\uff0c\u901a\u8fc7\u5bf9\u8bad\u7ec3\u70b9\u6dfb\u52a0\u9ad8\u65af\u566a\u58f0\u6765\u751f\u6210\u65b0\u7684\u914d\u7f6e\u70b9\uff0c\u566a\u58f0\u5e45\u5ea6\u4e0e\u6b8b\u5dee\u6210\u53cd\u6bd4\uff0c\u4ee5\u5173\u6ce8\u9ad8\u6b8b\u5dee\u533a\u57df\u5e76\u4fdd\u7559\u63a2\u7d22\u3002\u5f15\u5165\u8f7b\u91cf\u7ea7\u7ebf\u6027\u4ee3\u7406\u6a21\u578b\u6765\u8fd1\u4f3c\u5168\u5c40\u6b8b\u5dee\u5206\u5e03\uff0c\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "result": "GLF\u5728\u7cbe\u5ea6\u548c\u6548\u7387\u65b9\u9762\u4f18\u4e8e\u5168\u5c40\u548c\u5c40\u90e8\u91c7\u6837\u7b56\u7565\u3002", "conclusion": "GLF\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u53ef\u63d0\u9ad8PINNs\u5728\u6c42\u89e3\u590d\u6742\u548c\u9ad8\u7ef4\u504f\u5fae\u5206\u65b9\u7a0b\u65f6\u7684\u53ef\u9760\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2510.24647", "categories": ["cs.CL", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2510.24647", "abs": "https://arxiv.org/abs/2510.24647", "authors": ["Hugo Rydel-Johnston", "Alex Kafkas"], "title": "Quantifying the Effects of Word Length, Frequency, and Predictability on Dyslexia", "comment": null, "summary": "We ask where, and under what conditions, dyslexic reading costs arise in a\nlarge-scale naturalistic reading dataset. Using eye-tracking aligned to\nword-level features (word length, frequency, and predictability), we model how\neach feature influences dyslexic time costs. We find that all three features\nrobustly change reading times in both typical and dyslexic readers, and that\ndyslexic readers show stronger sensitivities to each, especially\npredictability. Counterfactual manipulations of these features substantially\nnarrow the dyslexic-control gap by about one third, with predictability showing\nthe strongest effect, followed by length and frequency. These patterns align\nwith dyslexia theories that posit heightened demands on linguistic working\nmemory and phonological encoding, and they motivate further work on lexical\ncomplexity and parafoveal preview benefits to explain the remaining gap. In\nshort, we quantify when extra dyslexic costs arise, how large they are, and\noffer actionable guidance for interventions and computational models for\ndyslexics.", "AI": {"tldr": "\u5728\u5927\u578b\u81ea\u7136\u9605\u8bfb\u6570\u636e\u96c6\u4e2d\uff0c\u7814\u7a76\u4e86\u9605\u8bfb\u969c\u788d\u7684\u9605\u8bfb\u65f6\u95f4\u6210\u672c\u7684\u4ea7\u751f\u6761\u4ef6\uff0c\u5e76\u91cf\u5316\u4e86\u5176\u5f71\u54cd\u7a0b\u5ea6\u3002", "motivation": "\u7814\u7a76\u5728\u4f55\u79cd\u6761\u4ef6\u4e0b\u4ee5\u53ca\u5728\u4f55\u79cd\u60c5\u51b5\u4e0b\u4f1a\u51fa\u73b0\u9605\u8bfb\u969c\u788d\u7684\u9605\u8bfb\u65f6\u95f4\u6210\u672c\uff0c\u5e76\u4e3a\u5e72\u9884\u63aa\u65bd\u548c\u8ba1\u7b97\u6a21\u578b\u63d0\u4f9b\u6307\u5bfc\u3002", "method": "\u5229\u7528\u773c\u52a8\u8ffd\u8e2a\u6280\u672f\uff0c\u7ed3\u5408\u8bcd\u957f\u3001\u8bcd\u9891\u548c\u53ef\u9884\u6d4b\u6027\u7b49\u8bcd\u8bed\u7ea7\u522b\u7279\u5f81\uff0c\u6784\u5efa\u6a21\u578b\u6765\u5206\u6790\u8fd9\u4e9b\u7279\u5f81\u5982\u4f55\u5f71\u54cd\u9605\u8bfb\u969c\u788d\u8005\u7684\u9605\u8bfb\u65f6\u95f4\u6210\u672c\u3002", "result": "\u6240\u6709\u4e09\u4e2a\u7279\u5f81\uff08\u8bcd\u957f\u3001\u8bcd\u9891\u3001\u53ef\u9884\u6d4b\u6027\uff09\u90fd\u4f1a\u663e\u8457\u5f71\u54cd\u5178\u578b\u9605\u8bfb\u8005\u548c\u9605\u8bfb\u969c\u788d\u8005\u7684\u9605\u8bfb\u65f6\u95f4\u3002\u9605\u8bfb\u969c\u788d\u8005\u5bf9\u8fd9\u4e9b\u7279\u5f81\u7684\u654f\u611f\u5ea6\u66f4\u9ad8\uff0c\u5c24\u5176\u662f\u53ef\u9884\u6d4b\u6027\u3002\u901a\u8fc7\u53cd\u4e8b\u5b9e\u64cd\u4f5c\uff08\u6539\u53d8\u8fd9\u4e9b\u7279\u5f81\uff09\u53ef\u4ee5\u7f29\u5c0f\u9605\u8bfb\u969c\u788d\u8005\u548c\u63a7\u5236\u7ec4\u4e4b\u95f4\u7684\u5dee\u8ddd\u7ea6\u4e09\u5206\u4e4b\u4e00\uff0c\u5176\u4e2d\u53ef\u9884\u6d4b\u6027\u7684\u5f71\u54cd\u6700\u5927\uff0c\u5176\u6b21\u662f\u8bcd\u957f\u548c\u8bcd\u9891\u3002", "conclusion": "\u9605\u8bfb\u969c\u788d\u8005\u7684\u9605\u8bfb\u65f6\u95f4\u6210\u672c\u7684\u4ea7\u751f\u53d7\u5230\u8bcd\u957f\u3001\u8bcd\u9891\u548c\u53ef\u9884\u6d4b\u6027\u7684\u5f71\u54cd\uff0c\u5e76\u4e14\u4ed6\u4eec\u5bf9\u8fd9\u4e9b\u56e0\u7d20\u66f4\u4e3a\u654f\u611f\u3002\u5e72\u9884\u63aa\u65bd\u53ef\u4ee5\u9488\u5bf9\u8fd9\u4e9b\u56e0\u7d20\u6765\u5e2e\u52a9\u9605\u8bfb\u969c\u788d\u8005\u3002\u672a\u6765\u7684\u7814\u7a76\u5e94\u5173\u6ce8\u8bcd\u6c47\u590d\u6742\u6027\u548c\u65c1\u4e2d\u5fc3\u9884\u89c8\u6548\u76ca\uff0c\u4ee5\u8fdb\u4e00\u6b65\u89e3\u91ca\u5269\u4f59\u7684\u5dee\u8ddd\u3002"}}
{"id": "2510.24709", "categories": ["cs.CV", "cs.AI", "cs.LG", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2510.24709", "abs": "https://arxiv.org/abs/2510.24709", "authors": ["Yihao Li", "Saeed Salehi", "Lyle Ungar", "Konrad P. Kording"], "title": "Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?", "comment": "Accepted as a Spotlight at NeurIPS 2025", "summary": "Object binding, the brain's ability to bind the many features that\ncollectively represent an object into a coherent whole, is central to human\ncognition. It groups low-level perceptual features into high-level object\nrepresentations, stores those objects efficiently and compositionally in\nmemory, and supports human reasoning about individual object instances. While\nprior work often imposes object-centric attention (e.g., Slot Attention)\nexplicitly to probe these benefits, it remains unclear whether this ability\nnaturally emerges in pre-trained Vision Transformers (ViTs). Intuitively, they\ncould: recognizing which patches belong to the same object should be useful for\ndownstream prediction and thus guide attention. Motivated by the quadratic\nnature of self-attention, we hypothesize that ViTs represent whether two\npatches belong to the same object, a property we term IsSameObject. We decode\nIsSameObject from patch embeddings across ViT layers using a similarity probe,\nwhich reaches over 90% accuracy. Crucially, this object-binding capability\nemerges reliably in self-supervised ViTs (DINO, MAE, CLIP), but markedly weaker\nin ImageNet-supervised models, suggesting that binding is not a trivial\narchitectural artifact, but an ability acquired through specific pretraining\nobjectives. We further discover that IsSameObject is encoded in a\nlow-dimensional subspace on top of object features, and that this signal\nactively guides attention. Ablating IsSameObject from model activations\ndegrades downstream performance and works against the learning objective,\nimplying that emergent object binding naturally serves the pretraining\nobjective. Our findings challenge the view that ViTs lack object binding and\nhighlight how symbolic knowledge of \"which parts belong together\" emerges\nnaturally in a connectionist system.", "AI": {"tldr": "ViT\u7684\u81ea\u6ce8\u610f\u529b\u673a\u5236\u53ef\u4ee5\u5b66\u4e60\u5230\u7269\u4f53\u7ed1\u5b9a\u80fd\u529b\uff0c\u5373\u5c06\u56fe\u50cf\u5757\u7ec4\u5408\u6210\u4e00\u81f4\u7684\u7269\u4f53\u8868\u793a\uff0c\u8fd9\u79cd\u80fd\u529b\u5728\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u7684\u6a21\u578b\u4e2d\u5c24\u4e3a\u663e\u8457\uff0c\u5e76\u4e14\u6709\u52a9\u4e8e\u4e0b\u6e38\u4efb\u52a1\u7684\u8868\u73b0\u3002", "motivation": "\u63a2\u7a76\u9884\u8bad\u7ec3\u7684Vision Transformers (ViTs) \u662f\u5426\u80fd\u591f\u81ea\u7136\u5730\u6d8c\u73b0\u51fa\u7269\u4f53\u7ed1\u5b9a\uff08\u5c06\u7269\u4f53\u7684\u4e0d\u540c\u7279\u5f81\u7ec4\u5408\u6210\u4e00\u4e2a\u6574\u4f53\uff09\u7684\u80fd\u529b\uff0c\u4ee5\u53ca\u8fd9\u79cd\u80fd\u529b\u662f\u5982\u4f55\u88ab\u8868\u5f81\u548c\u5229\u7528\u7684\u3002", "method": "\u4f7f\u7528\u76f8\u4f3c\u6027\u63a2\u9488\uff08similarity probe\uff09\u6765\u89e3\u7801ViT\u5c42\u7ea7\u4e2d\u7684patch\u5d4c\u5165\uff0c\u4ee5\u68c0\u6d4bIsSameObject\u5c5e\u6027\uff08\u5224\u65ad\u4e24\u4e2a\u56fe\u50cf\u5757\u662f\u5426\u5c5e\u4e8e\u540c\u4e00\u7269\u4f53\uff09\u3002\u901a\u8fc7\u6d88\u878dIsSameObject\u6765\u8bc4\u4f30\u5176\u5bf9\u4e0b\u6e38\u4efb\u52a1\u7684\u5f71\u54cd\u3002", "result": "ViTs\u7684IsSameObject\u89e3\u7801\u51c6\u786e\u7387\u8d85\u8fc790%\uff0c\u5e76\u4e14\u8fd9\u79cd\u80fd\u529b\u5728\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u6a21\u578b\uff08DINO, MAE, CLIP\uff09\u4e2d\u6bd4\u76d1\u7763\u5b66\u4e60\u6a21\u578b\u66f4\u5f3a\u3002IsSameObject\u4fe1\u53f7\u88ab\u7f16\u7801\u5728\u4f4e\u7ef4\u5b50\u7a7a\u95f4\u4e2d\u5e76\u80fd\u5f15\u5bfc\u81ea\u6ce8\u610f\u529b\u3002\u6d88\u878dIsSameObject\u4f1a\u964d\u4f4e\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "ViTs\u5177\u5907\u7269\u4f53\u7ed1\u5b9a\u80fd\u529b\uff0c\u8fd9\u79cd\u80fd\u529b\u5e76\u975e\u7b80\u5355\u7684\u67b6\u6784\u4ea7\u7269\uff0c\u800c\u662f\u901a\u8fc7\u7279\u5b9a\u7684\u9884\u8bad\u7ec3\u76ee\u6807\uff08\u5c24\u5176\u662f\u81ea\u76d1\u7763\u5b66\u4e60\uff09\u4e60\u5f97\u7684\u3002\u8fd9\u4e00\u53d1\u73b0\u6311\u6218\u4e86ViTs\u7f3a\u4e4f\u7269\u4f53\u7ed1\u5b9a\u80fd\u529b\u7684\u89c2\u70b9\uff0c\u5e76\u63ed\u793a\u4e86\u8fde\u63a5\u4e3b\u4e49\u7cfb\u7edf\u5982\u4f55\u81ea\u7136\u6d8c\u73b0\u51fa\u201c\u54ea\u4e9b\u90e8\u5206\u5c5e\u4e8e\u4e00\u8d77\u201d\u7684\u7b26\u53f7\u5316\u77e5\u8bc6\u3002"}}
{"id": "2510.24027", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24027", "abs": "https://arxiv.org/abs/2510.24027", "authors": ["Zibo Liu", "Zhe Jiang", "Zelin Xu", "Tingsong Xiao", "Yupu Zhang", "Zhengkun Xiao", "Haibo Wang", "Shigang Chen"], "title": "Spatio-temporal Multivariate Time Series Forecast with Chosen Variables", "comment": "In submission", "summary": "Spatio-Temporal Multivariate time series Forecast (STMF) uses the time series\nof $n$ spatially distributed variables in a period of recent past to forecast\ntheir values in a period of near future. It has important applications in\nspatio-temporal sensing forecast such as road traffic prediction and air\npollution prediction. Recent papers have addressed a practical problem of\nmissing variables in the model input, which arises in the sensing applications\nwhere the number $m$ of sensors is far less than the number $n$ of locations to\nbe monitored, due to budget constraints. We observe that the state of the art\nassumes that the $m$ variables (i.e., locations with sensors) in the model\ninput are pre-determined and the important problem of how to choose the $m$\nvariables in the input has never been studied. This paper fills the gap by\nstudying a new problem of STMF with chosen variables, which optimally selects\n$m$-out-of-$n$ variables for the model input in order to maximize the forecast\naccuracy. We propose a unified framework that jointly performs variable\nselection and model optimization for both forecast accuracy and model\nefficiency. It consists of three novel technical components: (1) masked\nvariable-parameter pruning, which progressively prunes less informative\nvariables and attention parameters through quantile-based masking; (2)\nprioritized variable-parameter replay, which replays low-loss past samples to\npreserve learned knowledge for model stability; (3) dynamic extrapolation\nmechanism, which propagates information from variables selected for the input\nto all other variables via learnable spatial embeddings and adjacency\ninformation. Experiments on five real-world datasets show that our work\nsignificantly outperforms the state-of-the-art baselines in both accuracy and\nefficiency, demonstrating the effectiveness of joint variable selection and\nmodel optimization.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728\u4f20\u611f\u5668\u6570\u91cf\uff08m\uff09\u8fdc\u5c11\u4e8e\u76d1\u6d4b\u5730\u70b9\u6570\u91cf\uff08n\uff09\u7684\u60c5\u51b5\u4e0b\uff0c\u5982\u4f55\u9009\u62e9m\u4e2a\u53d8\u91cf\u4ee5\u6700\u5927\u5316\u9884\u6d4b\u51c6\u786e\u6027\u7684\u65b0\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u5408\u53d8\u91cf\u9009\u62e9\u548c\u6a21\u578b\u4f18\u5316\u7684\u65b0\u6846\u67b6\u3002", "motivation": "\u5728\u5b9e\u9645\u7684\u65f6\u7a7a\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff08STMF\uff09\u5e94\u7528\u4e2d\uff0c\u7531\u4e8e\u9884\u7b97\u9650\u5236\uff0c\u4f20\u611f\u5668\u6570\u91cfm\u901a\u5e38\u8fdc\u5c0f\u4e8e\u9700\u8981\u76d1\u6d4b\u7684\u5730\u70b9\u6570\u91cfn\uff0c\u5bfc\u81f4\u6a21\u578b\u8f93\u5165\u4e2d\u5b58\u5728\u7f3a\u5931\u53d8\u91cf\u7684\u95ee\u9898\u3002\u73b0\u6709\u7814\u7a76\u5047\u8bbe\u8fd9m\u4e2a\u53d8\u91cf\u662f\u9884\u5148\u786e\u5b9a\u7684\uff0c\u4f46\u5982\u4f55\u9009\u62e9\u8fd9m\u4e2a\u53d8\u91cf\u4ee5\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u4e00\u76f4\u6ca1\u6709\u88ab\u7814\u7a76\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u5408\u53d8\u91cf\u9009\u62e9\u548c\u6a21\u578b\u4f18\u5316\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u65b0\u9896\u7684\u6280\u672f\u7ec4\u4ef6\uff1a1. \u63a9\u7801\u53d8\u91cf-\u53c2\u6570\u526a\u679d\uff08\u901a\u8fc7\u5206\u4f4d\u6570\u63a9\u7801\u9010\u6b65\u526a\u679d\u4fe1\u606f\u91cf\u5c11\u7684\u53d8\u91cf\u548c\u6ce8\u610f\u529b\u53c2\u6570\uff09\uff1b2. \u4f18\u5148\u53d8\u91cf-\u53c2\u6570\u56de\u653e\uff08\u56de\u653e\u4f4e\u635f\u5931\u7684\u8fc7\u53bb\u6837\u672c\u4ee5\u4fdd\u6301\u6a21\u578b\u7a33\u5b9a\u6027\uff09\uff1b3. \u52a8\u6001\u5916\u63a8\u673a\u5236\uff08\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u7a7a\u95f4\u5d4c\u5165\u548c\u90bb\u63a5\u4fe1\u606f\u5c06\u8f93\u5165\u53d8\u91cf\u7684\u4fe1\u606f\u4f20\u64ad\u5230\u6240\u6709\u5176\u4ed6\u53d8\u91cf\uff09\u3002", "result": "\u5728\u4e94\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u8054\u5408\u53d8\u91cf\u9009\u62e9\u548c\u6a21\u578b\u4f18\u5316\u6846\u67b6\u80fd\u6709\u6548\u63d0\u5347STMF\u7684\u9884\u6d4b\u51c6\u786e\u6027\u548c\u6a21\u578b\u6548\u7387\u3002"}}
{"id": "2510.24652", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.24652", "abs": "https://arxiv.org/abs/2510.24652", "authors": ["Jiawei Zhou", "Lei Chen"], "title": "Optimizing Retrieval for RAG via Reinforced Contrastive Learning", "comment": null, "summary": "As retrieval-augmented generation (RAG) becomes increasingly widespread, the\nrole of information retrieval (IR) is shifting from retrieving information for\nhuman users to retrieving contextual knowledge for artificial intelligence (AI)\nsystems, where relevance becomes difficult to define or annotate beforehand. To\naddress this challenge, we propose R3, a Retrieval framework optimized for RAG\nthrough trialand-feedback Reinforced contrastive learning. Unlike prior\napproaches that rely on annotated or synthetic data for supervised fine-tuning,\nR3 enables the retriever to dynamically explore and optimize relevance within\nthe RAG environment. During training, the retrieved results interact with the\nenvironment to produce contrastive signals that automatically guide the\nretriever's self-improvement. Extensive experiments across diverse tasks\ndemonstrate that R3 improves RAG performance by 5.2% over the original\nretriever and surpasses state-of-the-art retrievers by 4.9%, while achieving\ncomparable results to LLM-augmented retrieval and RAG systems built on\npost-trained or instruction-tuned LLMs. It is both efficient and practical,\nrequiring only 4 GPUs and completing training within a single day.", "AI": {"tldr": "R3\u662f\u4e00\u4e2a\u7528\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7684\u68c0\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5bf9\u6bd4\u5b66\u4e60\u8fdb\u884c\u8bad\u7ec3\uff0c\u65e0\u9700\u9884\u5148\u6807\u6ce8\u6570\u636e\uff0c\u5373\u53ef\u52a8\u6001\u4f18\u5316\u68c0\u7d22\u5668\u4ee5\u9002\u5e94RAG\u73af\u5883\uff0c\u5e76\u5728\u591a\u9879\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347RAG\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7684\u666e\u53ca\uff0c\u68c0\u7d22\u4fe1\u606f\u7684\u76ee\u6807\u4ece\u670d\u52a1\u4eba\u7c7b\u7528\u6237\u8f6c\u5411\u4e3a\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u7cfb\u7edf\u63d0\u4f9b\u4e0a\u4e0b\u6587\u77e5\u8bc6\uff0c\u8fd9\u4f7f\u5f97\u9884\u5148\u5b9a\u4e49\u6216\u6807\u6ce8\u76f8\u5173\u6027\u53d8\u5f97\u56f0\u96be\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aR3\u7684\u68c0\u7d22\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u201c\u8bd5\u9519\u548c\u53cd\u9988\u201d\u7684\u5f3a\u5316\u5bf9\u6bd4\u5b66\u4e60\u6765\u8fdb\u884c\u4f18\u5316\uff0c\u4f7f\u68c0\u7d22\u5668\u80fd\u591f\u5728RAG\u73af\u5883\u4e2d\u52a8\u6001\u63a2\u7d22\u548c\u4f18\u5316\u76f8\u5173\u6027\u3002\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u68c0\u7d22\u7ed3\u679c\u4e0e\u73af\u5883\u4ea4\u4e92\uff0c\u4ea7\u751f\u5bf9\u6bd4\u4fe1\u53f7\uff0c\u81ea\u52a8\u6307\u5bfc\u68c0\u7d22\u5668\u7684\u81ea\u6211\u6539\u8fdb\u3002", "result": "R3\u5728\u591a\u9879\u4efb\u52a1\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u4e2d\uff0c\u76f8\u6bd4\u539f\u59cb\u68c0\u7d22\u5668\u5c06RAG\u6027\u80fd\u63d0\u9ad8\u4e865.2%\uff0c\u8d85\u8fc7\u4e86\u6700\u5148\u8fdb\u7684\u68c0\u7d22\u56684.9%\uff0c\u5e76\u4e14\u53d6\u5f97\u4e86\u4e0e\u57fa\u4e8eLLM\u589e\u5f3a\u68c0\u7d22\u548c\u4f7f\u7528\u9884\u8bad\u7ec3\u6216\u6307\u4ee4\u8c03\u4f18LLM\u7684RAG\u7cfb\u7edf\u76f8\u5f53\u7684\u7ed3\u679c\u3002\u8be5\u65b9\u6cd5\u6548\u7387\u9ad8\u4e14\u5b9e\u7528\uff0c\u4ec5\u97004\u4e2aGPU\u5373\u53ef\u5728\u4e00\u5929\u5185\u5b8c\u6210\u8bad\u7ec3\u3002", "conclusion": "R3\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u63a2\u7d22\u548c\u5f3a\u5316\u5bf9\u6bd4\u5b66\u4e60\uff0c\u6210\u529f\u89e3\u51b3\u4e86RAG\u73af\u5883\u4e2d\u76f8\u5173\u6027\u5b9a\u4e49\u548c\u6807\u6ce8\u7684\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86RAG\u7684\u6027\u80fd\uff0c\u5e76\u8fbe\u5230\u4e86\u4e0e\u66f4\u590d\u6742\u7cfb\u7edf\u76f8\u5f53\u7684\u6548\u679c\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u6548\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2510.24711", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24711", "abs": "https://arxiv.org/abs/2510.24711", "authors": ["Yujie Wei", "Shiwei Zhang", "Hangjie Yuan", "Yujin Han", "Zhekai Chen", "Jiayu Wang", "Difan Zou", "Xihui Liu", "Yingya Zhang", "Yu Liu", "Hongming Shan"], "title": "Routing Matters in MoE: Scaling Diffusion Transformers with Explicit Routing Guidance", "comment": null, "summary": "Mixture-of-Experts (MoE) has emerged as a powerful paradigm for scaling model\ncapacity while preserving computational efficiency. Despite its notable success\nin large language models (LLMs), existing attempts to apply MoE to Diffusion\nTransformers (DiTs) have yielded limited gains. We attribute this gap to\nfundamental differences between language and visual tokens. Language tokens are\nsemantically dense with pronounced inter-token variation, while visual tokens\nexhibit spatial redundancy and functional heterogeneity, hindering expert\nspecialization in vision MoE. To this end, we present ProMoE, an MoE framework\nfeaturing a two-step router with explicit routing guidance that promotes expert\nspecialization. Specifically, this guidance encourages the router to partition\nimage tokens into conditional and unconditional sets via conditional routing\naccording to their functional roles, and refine the assignments of conditional\nimage tokens through prototypical routing with learnable prototypes based on\nsemantic content. Moreover, the similarity-based expert allocation in latent\nspace enabled by prototypical routing offers a natural mechanism for\nincorporating explicit semantic guidance, and we validate that such guidance is\ncrucial for vision MoE. Building on this, we propose a routing contrastive loss\nthat explicitly enhances the prototypical routing process, promoting\nintra-expert coherence and inter-expert diversity. Extensive experiments on\nImageNet benchmark demonstrate that ProMoE surpasses state-of-the-art methods\nunder both Rectified Flow and DDPM training objectives. Code and models will be\nmade publicly available.", "AI": {"tldr": "ProMoE\u662f\u4e00\u4e2a\u521b\u65b0\u7684MoE\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u6b65\u8def\u7531\u548c\u663e\u5f0f\u6307\u5bfc\u6765\u89e3\u51b3MoE\u5728\u89c6\u89c9Transformer\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u63d0\u5347\u4e86\u4e13\u5bb6\u4e13\u4e1a\u5316\uff0c\u5e76\u5728ImageNet\u4e0a\u53d6\u5f97\u4e86\u9886\u5148\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684MoE\u65b9\u6cd5\u5728\u8bed\u8a00\u6a21\u578b\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u89c6\u89c9Transformer\uff08DiTs\uff09\u4e0a\u7684\u6548\u679c\u6709\u9650\uff0c\u56e0\u4e3a\u89c6\u89c9\u548c\u8bed\u8a00\u6807\u8bb0\u5728\u8bed\u4e49\u5bc6\u5ea6\u3001\u5197\u4f59\u6027\u548c\u529f\u80fd\u5f02\u8d28\u6027\u65b9\u9762\u5b58\u5728\u5dee\u5f02\uff0c\u8fd9\u963b\u788d\u4e86\u89c6\u89c9MoE\u7684\u4e13\u5bb6\u4e13\u4e1a\u5316\u3002", "method": "\u63d0\u51faProMoE\u6846\u67b6\uff0c\u5305\u542b\u4e00\u4e2a\u4e24\u6b65\u8def\u7531\u5668\uff0c\u5229\u7528\u6761\u4ef6\u8def\u7531\u548c\u539f\u578b\u8def\u7531\u6765\u6307\u5bfc\u4e13\u5bb6\u5206\u914d\u3002\u6761\u4ef6\u8def\u7531\u6839\u636e\u529f\u80fd\u89d2\u8272\u5c06\u56fe\u50cf\u6807\u8bb0\u5212\u5206\u4e3a\u6761\u4ef6\u96c6\u548c\u975e\u6761\u4ef6\u96c6\uff1b\u539f\u578b\u8def\u7531\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u539f\u578b\u8fdb\u4e00\u6b65\u7ec6\u5316\u6761\u4ef6\u56fe\u50cf\u6807\u8bb0\u7684\u5206\u914d\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u8def\u7531\u5bf9\u6bd4\u635f\u5931\u6765\u589e\u5f3a\u539f\u578b\u8def\u7531\u8fc7\u7a0b\uff0c\u4fc3\u8fdb\u4e13\u5bb6\u5185\u90e8\u7684\u51dd\u805a\u529b\u548c\u4e13\u5bb6\u4e4b\u95f4\u7684\u591a\u6837\u6027\u3002", "result": "\u5728ImageNet\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cProMoE\u5728Rectified Flow\u548cDDPM\u8bad\u7ec3\u76ee\u6807\u4e0b\u5747\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "ProMoE\u901a\u8fc7\u663e\u5f0f\u7684\u8def\u7531\u6307\u5bfc\u548c\u57fa\u4e8e\u539f\u578b\u7684\u4e13\u5bb6\u5206\u914d\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u89c9MoE\u4e2d\u7684\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2510.24035", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24035", "abs": "https://arxiv.org/abs/2510.24035", "authors": ["Xinqi Li", "Yiqun Liu", "Shan Jiang", "Enrong Zheng", "Huaijin Zheng", "Wenhao Dai", "Haodong Deng", "Dianhai Yu", "Yanjun Ma"], "title": "GraphNet: A Large-Scale Computational Graph Dataset for Tensor Compiler Research", "comment": null, "summary": "We introduce GraphNet, a dataset of 2.7K real-world deep learning\ncomputational graphs with rich metadata, spanning six major task categories\nacross multiple deep learning frameworks. To evaluate tensor compiler\nperformance on these samples, we propose the benchmark metric Speedup Score\nS(t), which jointly considers runtime speedup and execution correctness under\ntunable tolerance levels, offering a reliable measure of general optimization\ncapability. Furthermore, we extend S(t) to the Error-aware Speedup Score ES(t),\nwhich incorporates error information and helps compiler developers identify key\nperformance bottlenecks. In this report, we benchmark the default tensor\ncompilers, CINN for PaddlePaddle and TorchInductor for PyTorch, on computer\nvision (CV) and natural language processing (NLP) samples to demonstrate the\npracticality of GraphNet. The full construction pipeline with graph extraction\nand compiler evaluation tools is available at\nhttps://github.com/PaddlePaddle/GraphNet .", "AI": {"tldr": "GraphNet\u662f\u4e00\u4e2a\u5305\u542b2.7K\u771f\u5b9e\u4e16\u754c\u6df1\u5ea6\u5b66\u4e60\u8ba1\u7b97\u56fe\u7684\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5f20\u91cf\u7f16\u8bd1\u5668\u6027\u80fd\u3002", "motivation": "\u8bc4\u4f30\u5f20\u91cf\u7f16\u8bd1\u5668\u5728\u771f\u5b9e\u4e16\u754c\u6df1\u5ea6\u5b66\u4e60\u8ba1\u7b97\u56fe\u4e0a\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51faSpeedup Score S(t)\u548cError-aware Speedup Score ES(t)\u4f5c\u4e3a\u57fa\u51c6\u6307\u6807\uff0c\u5e76\u4f7f\u7528GraphNet\u6570\u636e\u96c6\u5728CV\u548cNLP\u4efb\u52a1\u4e0a\u5bf9CINN\u548cTorchInductor\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u5728CV\u548cNLP\u4efb\u52a1\u4e0a\u5bf9CINN\u548cTorchInductor\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5c55\u793a\u4e86GraphNet\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "GraphNet\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u5de5\u5177\u4e3a\u5f20\u91cf\u7f16\u8bd1\u5668\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u7684\u5e73\u53f0\u3002"}}
{"id": "2510.24654", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24654", "abs": "https://arxiv.org/abs/2510.24654", "authors": ["Pengcheng Qiu", "Chaoyi Wu", "Junwei Liu", "Qiaoyu Zheng", "Yusheng Liao", "Haowen Wang", "Yun Yue", "Qianrui Fan", "Shuai Zhen", "Jian Wang", "Jinjie Gu", "Yanfeng Wang", "Ya Zhang", "Weidi Xie"], "title": "Evolving Diagnostic Agents in a Virtual Clinical Environment", "comment": null, "summary": "In this paper, we present a framework for training large language models\n(LLMs) as diagnostic agents with reinforcement learning, enabling them to\nmanage multi-turn diagnostic processes, adaptively select examinations, and\ncommit to final diagnoses. Unlike instruction-tuned models trained on static\ncase summaries, our method acquires diagnostic strategies through interactive\nexploration and outcome-based feedback. Our contributions are fourfold: (i) We\npresent DiagGym, a diagnostics world model trained with electronic health\nrecords that emits examination outcomes conditioned on patient history and\nrecommended examination, serving as a virtual clinical environment for\nrealistic diagnosis training and evaluation; (ii) We train DiagAgent via\nend-to-end, multi-turn reinforcement learning to learn diagnostic policies that\noptimize both information yield and diagnostic accuracy; (iii) We introduce\nDiagBench, a diagnostic benchmark comprising 750 cases with physician-validated\nexamination recommendations and 99 cases annotated with 973 physician-written\nrubrics on diagnosis process; (iv) we demonstrate superior performance across\ndiverse diagnostic settings. DiagAgent significantly outperforms 10\nstate-of-the-art LLMs, including DeepSeek-v3 and GPT-4o, as well as two\nprompt-engineered agents. In single-turn settings, DiagAgent achieves 9.34%\nhigher diagnostic accuracy and 44.03% improvement in examination recommendation\nhit ratio. In end-to-end settings, it delivers 15.12% increase in diagnostic\naccuracy and 23.09% boost in examination recommendation F1 score. In\nrubric-based evaluation, it surpasses the next-best model, Claude-sonnet-4, by\n7.1% in weighted rubric score. These findings indicate that learning policies\nin interactive clinical environments confers dynamic and clinically meaningful\ndiagnostic management abilities unattainable through passive training alone.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4f5c\u4e3a\u8bca\u65ad\u4ee3\u7406\u7684\u6846\u67b6\uff0c\u4f7f\u5176\u80fd\u591f\u8fdb\u884c\u591a\u8f6e\u8bca\u65ad\u3001\u81ea\u9002\u5e94\u9009\u62e9\u68c0\u67e5\u5e76\u6700\u7ec8\u786e\u8bca\u3002\u4e0e\u5728\u9759\u6001\u75c5\u4f8b\u6458\u8981\u4e0a\u8bad\u7ec3\u7684\u6307\u4ee4\u8c03\u4f18\u6a21\u578b\u4e0d\u540c\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u4ea4\u4e92\u5f0f\u63a2\u7d22\u548c\u57fa\u4e8e\u7ed3\u679c\u7684\u53cd\u9988\u6765\u5b66\u4e60\u8bca\u65ad\u7b56\u7565\u3002", "motivation": "\u4f20\u7edf\u7684\u6307\u4ee4\u8c03\u4f18\u6a21\u578b\u4f9d\u8d56\u9759\u6001\u75c5\u4f8b\u6458\u8981\uff0c\u65e0\u6cd5\u6709\u6548\u5b66\u4e60\u52a8\u6001\u8bca\u65ad\u7b56\u7565\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u4ea4\u4e92\u5f0f\u5b66\u4e60\uff0c\u4f7fLLM\u5177\u5907\u7ba1\u7406\u591a\u8f6e\u8bca\u65ad\u3001\u81ea\u9002\u5e94\u9009\u62e9\u68c0\u67e5\u548c\u6700\u7ec8\u786e\u8bca\u7684\u80fd\u529b\u3002", "method": "1. \u6784\u5efa\u4e86\u540d\u4e3aDiagGym\u7684\u8bca\u65ad\u4e16\u754c\u6a21\u578b\uff0c\u5229\u7528\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u8bad\u7ec3\uff0c\u80fd\u6839\u636e\u60a3\u8005\u75c5\u53f2\u548c\u63a8\u8350\u68c0\u67e5\u751f\u6210\u68c0\u67e5\u7ed3\u679c\uff0c\u4f5c\u4e3a\u865a\u62df\u4e34\u5e8a\u73af\u5883\u3002 2. \u91c7\u7528\u7aef\u5230\u7aef\u7684\u3001\u591a\u8f6e\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3DiagAgent\uff0c\u5b66\u4e60\u4f18\u5316\u4fe1\u606f\u83b7\u53d6\u548c\u8bca\u65ad\u51c6\u786e\u6027\u7684\u8bca\u65ad\u7b56\u7565\u3002 3. \u5f15\u5165\u4e86DiagBench\u8bca\u65ad\u57fa\u51c6\uff0c\u5305\u542b750\u4e2a\u75c5\u4f8b\uff08\u9644\u6709\u533b\u751f\u9a8c\u8bc1\u7684\u68c0\u67e5\u63a8\u8350\uff09\u548c99\u4e2a\u75c5\u4f8b\uff08\u9644\u6709973\u4e2a\u533b\u751f\u64b0\u5199\u7684\u8bca\u65ad\u8fc7\u7a0b\u8bc4\u5206\u6807\u51c6\uff09\u3002", "result": "DiagAgent\u5728\u5404\u79cd\u8bca\u65ad\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u4e8e10\u79cd\u6700\u5148\u8fdb\u7684LLM\uff08\u5305\u62ecDeepSeek-v3\u548cGPT-4o\uff09\u4ee5\u53ca\u4e24\u79cd\u63d0\u793a\u5de5\u7a0b\u4ee3\u7406\u3002\u5728\u5355\u8f6e\u8bca\u65ad\u4e2d\uff0c\u51c6\u786e\u7387\u63d0\u9ad89.34%\uff0c\u68c0\u67e5\u63a8\u8350\u547d\u4e2d\u7387\u63d0\u9ad844.03%\u3002\u5728\u7aef\u5230\u7aef\u8bca\u65ad\u4e2d\uff0c\u51c6\u786e\u7387\u63d0\u9ad815.12%\uff0c\u68c0\u67e5\u63a8\u8350F1\u5206\u6570\u63d0\u9ad823.09%\u3002\u5728\u57fa\u4e8e\u8bc4\u5206\u6807\u51c6\u7684\u8bc4\u4f30\u4e2d\uff0c\u5176\u52a0\u6743\u8bc4\u5206\u6bd4\u6b21\u4f18\u6a21\u578bClaude-sonnet-4\u9ad87.1%\u3002", "conclusion": "\u5728\u4ea4\u4e92\u5f0f\u4e34\u5e8a\u73af\u5883\u4e2d\u5b66\u4e60\u7b56\u7565\uff0c\u80fd\u591f\u8d4b\u4e88\u52a8\u6001\u4e14\u5177\u6709\u4e34\u5e8a\u610f\u4e49\u7684\u8bca\u65ad\u7ba1\u7406\u80fd\u529b\uff0c\u8fd9\u662f\u5355\u72ec\u901a\u8fc7\u88ab\u52a8\u8bad\u7ec3\u65e0\u6cd5\u5b9e\u73b0\u7684\u3002"}}
{"id": "2510.24717", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24717", "abs": "https://arxiv.org/abs/2510.24717", "authors": ["Haoge Deng", "Ting Pan", "Fan Zhang", "Yang Liu", "Zhuoyan Luo", "Yufeng Cui", "Wenxuan Wang", "Chunhua Shen", "Shiguang Shan", "Zhaoxiang Zhang", "Xinlong Wang"], "title": "Uniform Discrete Diffusion with Metric Path for Video Generation", "comment": "19 pages, 10 figures", "summary": "Continuous-space video generation has advanced rapidly, while discrete\napproaches lag behind due to error accumulation and long-context inconsistency.\nIn this work, we revisit discrete generative modeling and present Uniform\ndiscRete diffuSion with metric pAth (URSA), a simple yet powerful framework\nthat bridges the gap with continuous approaches for the scalable video\ngeneration. At its core, URSA formulates the video generation task as an\niterative global refinement of discrete spatiotemporal tokens. It integrates\ntwo key designs: a Linearized Metric Path and a Resolution-dependent Timestep\nShifting mechanism. These designs enable URSA to scale efficiently to\nhigh-resolution image synthesis and long-duration video generation, while\nrequiring significantly fewer inference steps. Additionally, we introduce an\nasynchronous temporal fine-tuning strategy that unifies versatile tasks within\na single model, including interpolation and image-to-video generation.\nExtensive experiments on challenging video and image generation benchmarks\ndemonstrate that URSA consistently outperforms existing discrete methods and\nachieves performance comparable to state-of-the-art continuous diffusion\nmethods. Code and models are available at https://github.com/baaivision/URSA", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2510.24039", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24039", "abs": "https://arxiv.org/abs/2510.24039", "authors": ["Nikolaos Karalias", "Akbar Rafiey", "Yifei Xu", "Zhishang Luo", "Behrooz Tahmasebi", "Connie Jiang", "Stefanie Jegelka"], "title": "Geometric Algorithms for Neural Combinatorial Optimization with Constraints", "comment": null, "summary": "Self-Supervised Learning (SSL) for Combinatorial Optimization (CO) is an\nemerging paradigm for solving combinatorial problems using neural networks. In\nthis paper, we address a central challenge of SSL for CO: solving problems with\ndiscrete constraints. We design an end-to-end differentiable framework that\nenables us to solve discrete constrained optimization problems with neural\nnetworks. Concretely, we leverage algorithmic techniques from the literature on\nconvex geometry and Carath\\'eodory's theorem to decompose neural network\noutputs into convex combinations of polytope corners that correspond to\nfeasible sets. This decomposition-based approach enables self-supervised\ntraining but also ensures efficient quality-preserving rounding of the neural\nnet output into feasible solutions. Extensive experiments in\ncardinality-constrained optimization show that our approach can consistently\noutperform neural baselines. We further provide worked-out examples of how our\nmethod can be applied beyond cardinality-constrained problems to a diverse set\nof combinatorial optimization tasks, including finding independent sets in\ngraphs, and solving matroid-constrained problems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7aef\u5230\u7aef\u53ef\u5fae\u6846\u67b6\uff0c\u7528\u4e8e\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u89e3\u51b3\u5177\u6709\u79bb\u6563\u7ea6\u675f\u7684\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u5728\u89e3\u51b3\u5177\u6709\u79bb\u6563\u7ea6\u675f\u7684\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u65f6\u9762\u4e34\u6311\u6218\u3002", "method": "\u5229\u7528\u51f8\u51e0\u4f55\u548cCarath\u00e9odory\u5b9a\u7406\uff0c\u5c06\u795e\u7ecf\u7f51\u7edc\u8f93\u51fa\u5206\u89e3\u4e3a\u591a\u9762\u4f53\u89d2\u70b9\u7684\u51f8\u7ec4\u5408\uff0c\u4ee5\u5904\u7406\u79bb\u6563\u7ea6\u675f\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u57fa\u6570\u7ea6\u675f\u4f18\u5316\u95ee\u9898\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u4e14\u53ef\u4ee5\u5e94\u7528\u4e8e\u56fe\u4e2d\u7684\u72ec\u7acb\u96c6\u67e5\u627e\u548c\u62df\u9635\u7ea6\u675f\u95ee\u9898\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u5730\u89e3\u51b3\u4e86\u5177\u6709\u79bb\u6563\u7ea6\u675f\u7684\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u6311\u6218\uff0c\u5e76\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2510.24664", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24664", "abs": "https://arxiv.org/abs/2510.24664", "authors": ["Parker Riley", "Daniel Deutsch", "Mara Finkelstein", "Colten DiIanni", "Juraj Juraska", "Markus Freitag"], "title": "MQM Re-Annotation: A Technique for Collaborative Evaluation of Machine Translation", "comment": null, "summary": "Human evaluation of machine translation is in an arms race with translation\nmodel quality: as our models get better, our evaluation methods need to be\nimproved to ensure that quality gains are not lost in evaluation noise. To this\nend, we experiment with a two-stage version of the current state-of-the-art\ntranslation evaluation paradigm (MQM), which we call MQM re-annotation. In this\nsetup, an MQM annotator reviews and edits a set of pre-existing MQM\nannotations, that may have come from themselves, another human annotator, or an\nautomatic MQM annotation system. We demonstrate that rater behavior in\nre-annotation aligns with our goals, and that re-annotation results in\nhigher-quality annotations, mostly due to finding errors that were missed\nduring the first pass.", "AI": {"tldr": "MQM \u518d\u6807\u6ce8\u63d0\u9ad8\u4e86\u673a\u5668\u7ffb\u8bd1\u8bc4\u4f30\u7684\u8d28\u91cf\uff0c\u4e3b\u8981\u901a\u8fc7\u53d1\u73b0\u7b2c\u4e00\u6b21\u6807\u6ce8\u4e2d\u9057\u6f0f\u7684\u9519\u8bef\u3002", "motivation": "\u673a\u5668\u7ffb\u8bd1\u6a21\u578b\u8d28\u91cf\u7684\u63d0\u5347\u9700\u8981\u66f4\u7cbe\u786e\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4ee5\u907f\u514d\u8bc4\u4f30\u566a\u58f0\u63a9\u76d6\u8d28\u91cf\u7684\u8fdb\u6b65\u3002", "method": "\u6211\u4eec\u6d4b\u8bd5\u4e86\u4e00\u79cd\u79f0\u4e3aMQM\u518d\u6807\u6ce8\u7684\u4e24\u9636\u6bb5MQM\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5176\u4e2d\u6807\u6ce8\u5458\u4f1a\u5ba1\u67e5\u548c\u7f16\u8f91\u5df2\u6709\u7684MQM\u6807\u6ce8\u3002", "result": "\u518d\u6807\u6ce8\u884c\u4e3a\u7b26\u5408\u9884\u671f\uff0c\u5e76\u4e14\u80fd\u4ea7\u751f\u66f4\u9ad8\u8d28\u91cf\u7684\u6807\u6ce8\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u53d1\u73b0\u4e86\u521d\u6b21\u6807\u6ce8\u4e2d\u9057\u6f0f\u7684\u9519\u8bef\u3002", "conclusion": "MQM\u518d\u6807\u6ce8\u662f\u4e00\u79cd\u6709\u6548\u7684\u63d0\u9ad8\u673a\u5668\u7ffb\u8bd1\u8bc4\u4f30\u8d28\u91cf\u7684\u65b9\u6cd5\u3002"}}
{"id": "2510.24718", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24718", "abs": "https://arxiv.org/abs/2510.24718", "authors": ["Chonghyuk Song", "Michal Stary", "Boyuan Chen", "George Kopanas", "Vincent Sitzmann"], "title": "Generative View Stitching", "comment": "Project website: https://andrewsonga.github.io/gvs", "summary": "Autoregressive video diffusion models are capable of long rollouts that are\nstable and consistent with history, but they are unable to guide the current\ngeneration with conditioning from the future. In camera-guided video generation\nwith a predefined camera trajectory, this limitation leads to collisions with\nthe generated scene, after which autoregression quickly collapses. To address\nthis, we propose Generative View Stitching (GVS), which samples the entire\nsequence in parallel such that the generated scene is faithful to every part of\nthe predefined camera trajectory. Our main contribution is a sampling algorithm\nthat extends prior work on diffusion stitching for robot planning to video\ngeneration. While such stitching methods usually require a specially trained\nmodel, GVS is compatible with any off-the-shelf video model trained with\nDiffusion Forcing, a prevalent sequence diffusion framework that we show\nalready provides the affordances necessary for stitching. We then introduce\nOmni Guidance, a technique that enhances the temporal consistency in stitching\nby conditioning on both the past and future, and that enables our proposed\nloop-closing mechanism for delivering long-range coherence. Overall, GVS\nachieves camera-guided video generation that is stable, collision-free,\nframe-to-frame consistent, and closes loops for a variety of predefined camera\npaths, including Oscar Reutersv\\\"ard's Impossible Staircase. Results are best\nviewed as videos at https://andrewsonga.github.io/gvs.", "AI": {"tldr": "\"\"\"GVS\u662f\u4e00\u79cd\u65b0\u7684\u89c6\u9891\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u5e76\u884c\u91c7\u6837\u89e3\u51b3\u4e86\u73b0\u6709\u81ea\u56de\u5f52\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u4e0e\u9884\u5b9a\u6444\u50cf\u673a\u8f68\u8ff9\u7684\u65e0\u78b0\u649e\u3001\u957f\u5e8f\u5217\u3001\u65f6\u95f4\u4e00\u81f4\u7684\u89c6\u9891\u751f\u6210\uff0c\u5e76\u4e14\u53ef\u4ee5\u4e0e\u73b0\u6709\u7684\u89c6\u9891\u6a21\u578b\u517c\u5bb9\u3002\"\"\"", "motivation": "\"\"\"\u73b0\u6709\u7684\u81ea\u56de\u5f52\u89c6\u9891\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u957f\u5e8f\u5217\u89c6\u9891\u65f6\uff0c\u65e0\u6cd5\u4ece\u672a\u6765\u5e27\u83b7\u53d6\u4fe1\u606f\uff0c\u5bfc\u81f4\u5728\u6444\u50cf\u673a\u5f15\u5bfc\u7684\u89c6\u9891\u751f\u6210\u4e2d\uff0c\u751f\u6210\u7684\u573a\u666f\u4e0e\u9884\u5b9a\u7684\u6444\u50cf\u673a\u8f68\u8ff9\u53d1\u751f\u78b0\u649e\uff0c\u6a21\u578b\u5bb9\u6613\u5d29\u6e83\u3002\"\"\"", "method": "\"\"\"\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u751f\u6210\u5f0f\u89c6\u56fe\u62fc\u63a5\uff08GVS\uff09\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5e76\u884c\u91c7\u6837\u6574\u4e2a\u89c6\u9891\u5e8f\u5217\u6765\u89e3\u51b3\u4e0a\u8ff0\u95ee\u9898\u3002GVS\u91c7\u7528\u4e86\u4e00\u79cd\u65b0\u7684\u91c7\u6837\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u6269\u5c55\u4e86\u5148\u524d\u5728\u673a\u5668\u4eba\u89c4\u5212\u9886\u57df\u4e2d\u7528\u4e8e\u6269\u6563\u6a21\u578b\u62fc\u63a5\u7684\u5de5\u4f5c\u3002GVS\u53ef\u4ee5\u517c\u5bb9\u4efb\u4f55\u4f7f\u7528\u6269\u6563\u5f3a\u5236\uff08Diffusion Forcing\uff09\u6846\u67b6\u8bad\u7ec3\u7684\u73b0\u6709\u89c6\u9891\u6a21\u578b\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u5168\u5411\u5f15\u5bfc\uff08Omni Guidance\uff09\u7684\u6280\u672f\uff0c\u901a\u8fc7\u540c\u65f6\u8003\u8651\u8fc7\u53bb\u548c\u672a\u6765\u7684\u4fe1\u606f\u6765\u589e\u5f3a\u62fc\u63a5\u4e2d\u7684\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u5e76\u5b9e\u73b0\u4e86\u4e00\u4e2a\u7528\u4e8e\u957f\u8ddd\u79bb\u8fde\u8d2f\u6027\u7684\u95ed\u73af\u673a\u5236\u3002\"\"\"", "result": "\"\"\"GVS\u5b9e\u73b0\u4e86\u7a33\u5b9a\u3001\u65e0\u78b0\u649e\u3001\u5e27\u5230\u5e27\u4e00\u81f4\u4e14\u80fd\u591f\u95ed\u5408\u5404\u79cd\u9884\u5b9a\u6444\u50cf\u673a\u8def\u5f84\uff08\u5305\u62ec\u4e0d\u53ef\u80fd\u7684\u697c\u68af\uff09\u7684\u6444\u50cf\u673a\u5f15\u5bfc\u89c6\u9891\u751f\u6210\u3002\"\"\"", "conclusion": "\"\"\"GVS\u901a\u8fc7\u5e76\u884c\u91c7\u6837\u548c\u5168\u5411\u5f15\u5bfc\u6280\u672f\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u81ea\u56de\u5f52\u89c6\u9891\u6269\u6563\u6a21\u578b\u5728\u6444\u50cf\u673a\u5f15\u5bfc\u89c6\u9891\u751f\u6210\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u3001\u957f\u5e8f\u5217\u3001\u65f6\u95f4\u4e00\u81f4\u4e14\u65e0\u78b0\u649e\u7684\u89c6\u9891\u751f\u6210\u3002\"\"\""}}
{"id": "2510.24043", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24043", "abs": "https://arxiv.org/abs/2510.24043", "authors": ["Akira Tamamori"], "title": "Localized Kernel Projection Outlyingness: A Two-Stage Approach for Multi-Modal Outlier Detection", "comment": "10 pages, 4 figures; submitted to The IEICE Transactions on\n  Information and Systems", "summary": "This paper presents Two-Stage LKPLO, a novel multi-stage outlier detection\nframework that overcomes the coexisting limitations of conventional\nprojection-based methods: their reliance on a fixed statistical metric and\ntheir assumption of a single data structure. Our framework uniquely synthesizes\nthree key concepts: (1) a generalized loss-based outlyingness measure (PLO)\nthat replaces the fixed metric with flexible, adaptive loss functions like our\nproposed SVM-like loss; (2) a global kernel PCA stage to linearize non-linear\ndata structures; and (3) a subsequent local clustering stage to handle\nmulti-modal distributions. Comprehensive 5-fold cross-validation experiments on\n10 benchmark datasets, with automated hyperparameter optimization, demonstrate\nthat Two-Stage LKPLO achieves state-of-the-art performance. It significantly\noutperforms strong baselines on datasets with challenging structures where\nexisting methods fail, most notably on multi-cluster data (Optdigits) and\ncomplex, high-dimensional data (Arrhythmia). Furthermore, an ablation study\nempirically confirms that the synergistic combination of both the kernelization\nand localization stages is indispensable for its superior performance. This\nwork contributes a powerful new tool for a significant class of outlier\ndetection problems and underscores the importance of hybrid, multi-stage\narchitectures.", "AI": {"tldr": "Two-Stage LKPLO\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u591a\u9636\u6bb5\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u5e7f\u4e49\u635f\u5931\u51fd\u6570\u3001\u6838PCA\u548c\u5c40\u90e8\u805a\u7c7b\u6765\u514b\u670d\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6295\u5f71\u7684\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u5728\u4f9d\u8d56\u56fa\u5b9a\u7edf\u8ba1\u6307\u6807\u548c\u5047\u8bbe\u5355\u4e00\u6570\u636e\u7ed3\u6784\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0cTwo-Stage LKPLO\u65e8\u5728\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u3002", "method": "Two-Stage LKPLO\u6846\u67b6\u7ed3\u5408\u4e86\u5e7f\u4e49\u635f\u5931\u51fd\u6570\uff08\u5982SVM-like\u635f\u5931\uff09\u4f5c\u4e3a\u5916\u79bb\u5ea6\u5ea6\u91cf\uff0c\u4f7f\u7528\u5168\u5c40\u6838PCA\u6765\u7ebf\u6027\u5316\u975e\u7ebf\u6027\u6570\u636e\u7ed3\u6784\uff0c\u5e76\u91c7\u7528\u5c40\u90e8\u805a\u7c7b\u9636\u6bb5\u6765\u5904\u7406\u591a\u6a21\u6001\u5206\u5e03\u3002", "result": "\u572810\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u76845\u6298\u4ea4\u53c9\u9a8c\u8bc1\u5b9e\u9a8c\u8868\u660e\uff0cTwo-Stage LKPLO\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u591a\u7c07\u6570\u636e\uff08Optdigits\uff09\u548c\u590d\u6742\u9ad8\u7ef4\u6570\u636e\uff08Arrhythmia\uff09\u4e0a\u3002\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u4e86\u6838\u5316\u548c\u5c40\u90e8\u5316\u9636\u6bb5\u7684\u534f\u540c\u4f5c\u7528\u5bf9\u4e8e\u5176\u5353\u8d8a\u6027\u80fd\u662f\u4e0d\u53ef\u6216\u7f3a\u7684\u3002", "conclusion": "Two-Stage LKPLO\u4e3a\u4e00\u7c7b\u91cd\u8981\u7684\u5f02\u5e38\u68c0\u6d4b\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u65b0\u5de5\u5177\uff0c\u5e76\u5f3a\u8c03\u4e86\u6df7\u5408\u3001\u591a\u9636\u6bb5\u67b6\u6784\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2510.24668", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24668", "abs": "https://arxiv.org/abs/2510.24668", "authors": ["Mingyi Deng", "Lijun Huang", "Yani Fan", "Jiayi Zhang", "Fashen Ren", "Jinyi Bai", "Fuzhen Yang", "Dayi Miao", "Zhaoyang Yu", "Yifan Wu", "Yanfei Zhang", "Fengwei Teng", "Yingjia Wan", "Song Hu", "Yude Li", "Xin Jin", "Conghao Hu", "Haoyu Li", "Qirui Fu", "Tai Zhong", "Xinyu Wang", "Xiangru Tang", "Nan Tang", "Chenglin Wu", "Yuyu Luo"], "title": "InteractComp: Evaluating Search Agents With Ambiguous Queries", "comment": null, "summary": "Language agents have demonstrated remarkable potential in web search and\ninformation retrieval. However, these search agents assume user queries are\ncomplete and unambiguous, an assumption that diverges from reality where users\nbegin with incomplete queries requiring clarification through interaction. Yet\nmost agents lack interactive mechanisms during the search process, and existing\nbenchmarks cannot assess this capability. To address this gap, we introduce\nInteractComp, a benchmark designed to evaluate whether search agents can\nrecognize query ambiguity and actively interact to resolve it during search.\nFollowing the principle of easy to verify, interact to disambiguate, we\nconstruct 210 expert-curated questions across 9 domains through a\ntarget-distractor methodology that creates genuine ambiguity resolvable only\nthrough interaction. Evaluation of 17 models reveals striking failure: the best\nmodel achieves only 13.73% accuracy despite 71.50% with complete context,\nexposing systematic overconfidence rather than reasoning deficits. Forced\ninteraction produces dramatic gains, demonstrating latent capability current\nstrategies fail to engage. Longitudinal analysis shows interaction capabilities\nstagnated over 15 months while search performance improved seven-fold,\nrevealing a critical blind spot. This stagnation, coupled with the immediate\nfeedback inherent to search tasks, makes InteractComp a valuable resource for\nboth evaluating and training interaction capabilities in search agents. The\ncode is available at https://github.com/FoundationAgents/InteractComp.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86InteractComp\u57fa\u51c6\u6765\u8bc4\u4f30\u8bed\u8a00\u641c\u7d22\u4ee3\u7406\u5728\u5904\u7406\u4e0d\u660e\u786e\u67e5\u8be2\u65f6\u7684\u4ea4\u4e92\u80fd\u529b\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u8fd9\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u4f46\u901a\u8fc7\u5f3a\u5236\u4ea4\u4e92\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u8bed\u8a00\u641c\u7d22\u4ee3\u7406\u5047\u8bbe\u7528\u6237\u67e5\u8be2\u662f\u5b8c\u6574\u4e14\u65e0\u6b67\u4e49\u7684\uff0c\u8fd9\u4e0e\u7528\u6237\u901a\u5e38\u4f7f\u7528\u4e0d\u5b8c\u6574\u67e5\u8be2\u5e76\u901a\u8fc7\u4ea4\u4e92\u6765\u6f84\u6e05\u7684\u5b9e\u9645\u60c5\u51b5\u4e0d\u7b26\u3002\u7136\u800c\uff0c\u5927\u591a\u6570\u4ee3\u7406\u7f3a\u4e4f\u4ea4\u4e92\u673a\u5236\uff0c\u5e76\u4e14\u73b0\u6709\u57fa\u51c6\u65e0\u6cd5\u8bc4\u4f30\u8fd9\u79cd\u80fd\u529b\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b210\u4e2a\u4e13\u5bb6\u7b56\u5212\u95ee\u9898\uff08\u6db5\u76d69\u4e2a\u9886\u57df\uff09\u7684InteractComp\u57fa\u51c6\uff0c\u8be5\u57fa\u51c6\u91c7\u7528\u76ee\u6807-\u5e72\u6270\u7269\u65b9\u6cd5\u5236\u9020\u4e86\u53ea\u6709\u901a\u8fc7\u4ea4\u4e92\u624d\u80fd\u89e3\u51b3\u7684\u771f\u5b9e\u6b67\u4e49\u3002", "result": "\u5728InteractComp\u57fa\u51c6\u4e0a\u5bf917\u4e2a\u6a21\u578b\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793a\uff0c\u5373\u4f7f\u5728\u67e5\u8be2\u8bed\u5883\u5b8c\u6574\u65f6\u51c6\u786e\u7387\u8fbe\u523071.50%\uff0c\u6700\u4f73\u6a21\u578b\u7684\u51c6\u786e\u7387\u4e5f\u4ec5\u4e3a13.73%\uff0c\u8fd9\u8868\u660e\u6a21\u578b\u5b58\u5728\u7cfb\u7edf\u6027\u8fc7\u5ea6\u81ea\u4fe1\u800c\u975e\u63a8\u7406\u7f3a\u9677\u3002\u5f3a\u5236\u4ea4\u4e92\u80fd\u5e26\u6765\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u7b56\u7565\u672a\u80fd\u53d1\u6398\u7684\u6f5c\u5728\u80fd\u529b\u3002\u4ea4\u4e92\u80fd\u529b\u7684\u505c\u6ede\u4e0e\u641c\u7d22\u6027\u80fd\u76847\u500d\u63d0\u5347\u540c\u65f6\u53d1\u751f\uff0c\u8868\u660e\u8fd9\u662f\u4e00\u4e2a\u5173\u952e\u7684\u76f2\u70b9\u3002", "conclusion": "InteractComp\u57fa\u51c6\u66b4\u9732\u4e86\u5f53\u524d\u8bed\u8a00\u641c\u7d22\u4ee3\u7406\u5728\u4ea4\u4e92\u5f0f\u67e5\u8be2\u6f84\u6e05\u65b9\u9762\u5b58\u5728\u7684\u4e25\u91cd\u4e0d\u8db3\u3002\u8be5\u57fa\u51c6\u5bf9\u4e8e\u8bc4\u4f30\u548c\u8bad\u7ec3\u4ee3\u7406\u7684\u4ea4\u4e92\u80fd\u529b\u975e\u5e38\u5b9d\u8d35\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2510.24044", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24044", "abs": "https://arxiv.org/abs/2510.24044", "authors": ["Hui Sun", "Zheng Xie", "Hao-Yuan He", "Ming Li"], "title": "Mitigating Negative Transfer via Reducing Environmental Disagreement", "comment": "13 pages, 5 figures", "summary": "Unsupervised Domain Adaptation~(UDA) focuses on transferring knowledge from a\nlabeled source domain to an unlabeled target domain, addressing the challenge\nof \\emph{domain shift}. Significant domain shifts hinder effective knowledge\ntransfer, leading to \\emph{negative transfer} and deteriorating model\nperformance. Therefore, mitigating negative transfer is essential. This study\nrevisits negative transfer through the lens of causally disentangled learning,\nemphasizing cross-domain discriminative disagreement on non-causal\nenvironmental features as a critical factor. Our theoretical analysis reveals\nthat overreliance on non-causal environmental features as the environment\nevolves can cause discriminative disagreements~(termed \\emph{environmental\ndisagreement}), thereby resulting in negative transfer. To address this, we\npropose Reducing Environmental Disagreement~(RED), which disentangles each\nsample into domain-invariant causal features and domain-specific non-causal\nenvironmental features via adversarially training domain-specific environmental\nfeature extractors in the opposite domains. Subsequently, RED estimates and\nreduces environmental disagreement based on domain-specific non-causal\nenvironmental features. Experimental results confirm that RED effectively\nmitigates negative transfer and achieves state-of-the-art performance.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRED\uff08Reducing Environmental Disagreement\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u56e0\u679c\u89e3\u8026\u5b66\u4e60\u6765\u89e3\u51b3\u65e0\u76d1\u7763\u57df\u81ea\u9002\u5e94\uff08UDA\uff09\u4e2d\u7684\u8d1f\u8fc1\u79fb\u95ee\u9898\uff0c\u5e76\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u65e0\u76d1\u7763\u57df\u81ea\u9002\u5e94\uff08UDA\uff09\u65e8\u5728\u5c06\u77e5\u8bc6\u4ece\u6807\u8bb0\u597d\u7684\u6e90\u57df\u8fc1\u79fb\u5230\u672a\u6807\u8bb0\u7684\u76ee\u6807\u57df\uff0c\u4f46\u201c\u57df\u8fc1\u79fb\u201d\u95ee\u9898\u4f1a\u963b\u788d\u6709\u6548\u7684\u77e5\u8bc6\u8fc1\u79fb\uff0c\u5bfc\u81f4\u201c\u8d1f\u8fc1\u79fb\u201d\u5e76\u964d\u4f4e\u6a21\u578b\u6027\u80fd\u3002\u56e0\u6b64\uff0c\u51cf\u8f7b\u8d1f\u8fc1\u79fb\u81f3\u5173\u91cd\u8981\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3UDA\u4e2d\u7684\u8d1f\u8fc1\u79fb\u95ee\u9898\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRED\uff08Reducing Environmental Disagreement\uff09\u7684\u65b0\u65b9\u6cd5\u3002RED\u901a\u8fc7\u5bf9\u6297\u6027\u5730\u8bad\u7ec3\u7279\u5b9a\u57df\u7684\u7279\u5b9a\u73af\u5883\u7279\u5f81\u63d0\u53d6\u5668\uff0c\u5c06\u6bcf\u4e2a\u6837\u672c\u89e3\u8026\u4e3a\u57df\u4e0d\u53d8\u7684\u56e0\u679c\u7279\u5f81\u548c\u7279\u5b9a\u57df\u7684\u975e\u56e0\u679c\u73af\u5883\u7279\u5f81\u3002\u968f\u540e\uff0cRED\u57fa\u4e8e\u7279\u5b9a\u57df\u7684\u975e\u56e0\u679c\u73af\u5883\u7279\u5f81\u6765\u4f30\u8ba1\u548c\u51cf\u5c11\u73af\u5883\u4e0d\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u5b9e\uff0cRED\u80fd\u591f\u6709\u6548\u5730\u51cf\u8f7b\u8d1f\u8fc1\u79fb\uff0c\u5e76\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u56e0\u679c\u89e3\u8026\u5b66\u4e60\u7684\u89c6\u89d2\uff0c\u5f3a\u8c03\u4e86\u5728\u975e\u56e0\u679c\u73af\u5883\u7279\u5f81\u4e0a\u7684\u8de8\u57df\u5224\u522b\u6027\u4e0d\u4e00\u81f4\u6027\u662f\u5bfc\u81f4\u8d1f\u8fc1\u79fb\u7684\u5173\u952e\u56e0\u7d20\u3002RED\u901a\u8fc7\u89e3\u8026\u56e0\u679c\u7279\u5f81\u548c\u975e\u56e0\u679c\u73af\u5883\u7279\u5f81\uff0c\u5e76\u51cf\u5c11\u5b83\u4eec\u4e4b\u95f4\u7684\u73af\u5883\u4e0d\u4e00\u81f4\u6027\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8d1f\u8fc1\u79fb\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u3002"}}
{"id": "2510.24677", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24677", "abs": "https://arxiv.org/abs/2510.24677", "authors": ["Xun Liang", "Huayi Lai", "Hanyu Wang", "Wentao Zhang", "Linfeng Zhang", "Yanfang Chen", "Feiyu Xiong", "Zhiyu Li"], "title": "Dissecting Role Cognition in Medical LLMs via Neuronal Ablation", "comment": "15 pages, 9 figures", "summary": "Large language models (LLMs) have gained significant traction in medical\ndecision support systems, particularly in the\n  context of medical question answering and role-playing simulations. A common\npractice, Prompt-Based Role Playing (PBRP),\n  instructs models to adopt different clinical roles (e.g., medical students,\nresidents, attending physicians) to simulate varied\n  professional behaviors. However, the impact of such role prompts on model\nreasoning capabilities remains unclear. This\n  study introduces the RP-Neuron-Activated Evaluation Framework(RPNA) to\nevaluate whether role prompts induce distinct,\n  role-specific cognitive processes in LLMs or merely modify linguistic style.\nWe test this framework on three medical QA\n  datasets, employing neuron ablation and representation analysis techniques to\nassess changes in reasoning pathways. Our\n  results demonstrate that role prompts do not significantly enhance the\nmedical reasoning abilities of LLMs. Instead, they\n  primarily affect surface-level linguistic features, with no evidence of\ndistinct reasoning pathways or cognitive differentiation\n  across clinical roles. Despite superficial stylistic changes, the core\ndecision-making mechanisms of LLMs remain uniform\n  across roles, indicating that current PBRP methods fail to replicate the\ncognitive complexity found in real-world medical\n  practice. This highlights the limitations of role-playing in medical AI and\nemphasizes the need for models that simulate genuine\n  cognitive processes rather than linguistic imitation.We have released the\nrelated code in the following repository:https:\n  //github.com/IAAR-Shanghai/RolePlay_LLMDoctor", "AI": {"tldr": "\u89d2\u8272\u63d0\u793a\uff08PBRP\uff09\u5e76\u672a\u663e\u8457\u63d0\u5347 LLM \u7684\u533b\u5b66\u63a8\u7406\u80fd\u529b\uff0c\u4e3b\u8981\u5f71\u54cd\u8bed\u8a00\u98ce\u683c\uff0c\u800c\u975e\u6df1\u5c42\u8ba4\u77e5\u8fc7\u7a0b\u3002", "motivation": "\u8bc4\u4f30\u89d2\u8272\u63d0\u793a\uff08PBRP\uff09\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u533b\u5b66\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u4e2d\u7684\u63a8\u7406\u80fd\u529b\u7684\u5f71\u54cd\uff0c\u4ee5\u53ca\u5176\u662f\u5426\u8bf1\u5bfc\u4e86\u4e0d\u540c\u4e8e\u8bed\u8a00\u98ce\u683c\u7684\u3001\u7279\u5b9a\u4e8e\u89d2\u8272\u7684\u8ba4\u77e5\u8fc7\u7a0b\u3002", "method": "\u63d0\u51fa RP-Neuron-Activated Evaluation Framework (RPNA) \u6846\u67b6\uff0c\u5e76\u7ed3\u5408\u795e\u7ecf\u5143\u70e7\u8680\u548c\u8868\u5f81\u5206\u6790\u6280\u672f\uff0c\u5728\u4e09\u4e2a\u533b\u5b66\u95ee\u7b54\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\u8be5\u6846\u67b6\u3002", "result": "\u89d2\u8272\u63d0\u793a\u4e3b\u8981\u5f71\u54cd LLM \u7684\u8868\u9762\u8bed\u8a00\u7279\u5f81\uff0c\u5e76\u672a\u663e\u8457\u589e\u5f3a\u5176\u533b\u5b66\u63a8\u7406\u80fd\u529b\uff0c\u4e5f\u672a\u663e\u793a\u51fa\u4e0d\u540c\u4e34\u5e8a\u89d2\u8272\u4e4b\u95f4\u5b58\u5728\u4e0d\u540c\u7684\u63a8\u7406\u901a\u8def\u6216\u8ba4\u77e5\u5206\u5316\u3002LLM \u7684\u6838\u5fc3\u51b3\u7b56\u673a\u5236\u5728\u4e0d\u540c\u89d2\u8272\u4e0b\u4fdd\u6301\u4e00\u81f4\u3002", "conclusion": "\u76ee\u524d\u7684 PBRP \u65b9\u6cd5\u672a\u80fd\u6709\u6548\u6a21\u62df\u771f\u5b9e\u4e16\u754c\u533b\u7597\u5b9e\u8df5\u4e2d\u7684\u8ba4\u77e5\u590d\u6742\u6027\uff0c\u5176\u4f5c\u7528\u4ec5\u9650\u4e8e\u8bed\u8a00\u98ce\u683c\u7684\u6539\u53d8\uff0c\u800c\u975e\u6df1\u5c42\u8ba4\u77e5\u8fc7\u7a0b\u7684\u6a21\u62df\u3002\u8fd9\u8868\u660e\u4ec5\u901a\u8fc7\u89d2\u8272\u626e\u6f14\u6765\u63d0\u5347\u533b\u7597 AI \u7684\u80fd\u529b\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u6a21\u62df\u771f\u5b9e\u8ba4\u77e5\u8fc7\u7a0b\u7684\u6a21\u578b\u3002"}}
{"id": "2510.24046", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24046", "abs": "https://arxiv.org/abs/2510.24046", "authors": ["Tu Anh Hoang Nguyen", "Dang Nguyen", "Tri-Nhan Vo", "Thuc Duy Le", "Sunil Gupta"], "title": "Causal-Aware Generative Adversarial Networks with Reinforcement Learning", "comment": null, "summary": "The utility of tabular data for tasks ranging from model training to\nlarge-scale data analysis is often constrained by privacy concerns or\nregulatory hurdles. While existing data generation methods, particularly those\nbased on Generative Adversarial Networks (GANs), have shown promise, they\nfrequently struggle with capturing complex causal relationship, maintaining\ndata utility, and providing provable privacy guarantees suitable for enterprise\ndeployment. We introduce CA-GAN, a novel generative framework specifically\nengineered to address these challenges for real-world tabular datasets. CA-GAN\nutilizes a two-step approach: causal graph extraction to learn a robust,\ncomprehensive causal relationship in the data's manifold, followed by a custom\nConditional WGAN-GP (Wasserstein GAN with Gradient Penalty) that operates\nexclusively as per the structure of nodes in the causal graph. More\nimportantly, the generator is trained with a new Reinforcement Learning-based\nobjective that aligns the causal graphs constructed from real and fake data,\nensuring the causal awareness in both training and sampling phases. We\ndemonstrate CA-GAN superiority over six SOTA methods across 14 tabular\ndatasets. Our evaluations, focused on core data engineering metrics: causal\npreservation, utility preservation, and privacy preservation. Our method offers\na practical, high-performance solution for data engineers seeking to create\nhigh-quality, privacy-compliant synthetic datasets to benchmark database\nsystems, accelerate software development, and facilitate secure data-driven\nresearch.", "AI": {"tldr": "CA-GAN\u662f\u4e00\u4e2a\u65b0\u7684\u751f\u6210\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u8868\u683c\u6570\u636e\u751f\u6210\u4e2d\u7684\u9690\u79c1\u548c\u6570\u636e\u6548\u7528\u95ee\u9898\uff0c\u5b83\u901a\u8fc7\u56e0\u679c\u56fe\u63d0\u53d6\u548c\u6761\u4ef6WGAN-GP\u6765\u786e\u4fdd\u56e0\u679c\u5173\u7cfb\u7684\u51c6\u786e\u6027\u548c\u9690\u79c1\u4fdd\u62a4\u3002", "motivation": "\u9690\u79c1\u95ee\u9898\u548c\u76d1\u7ba1\u9650\u5236\u4e86\u8868\u683c\u6570\u636e\u5728\u6a21\u578b\u8bad\u7ec3\u548c\u6570\u636e\u5206\u6790\u4e2d\u7684\u5e94\u7528\u3002\u73b0\u6709\u7684\u6570\u636e\u751f\u6210\u65b9\u6cd5\uff08\u7279\u522b\u662fGANs\uff09\u5728\u6355\u6349\u590d\u6742\u56e0\u679c\u5173\u7cfb\u3001\u4fdd\u6301\u6570\u636e\u6548\u7528\u548c\u63d0\u4f9b\u53ef\u9a8c\u8bc1\u7684\u9690\u79c1\u4fdd\u8bc1\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "CA-GAN\u91c7\u7528\u4e24\u6b65\u65b9\u6cd5\uff1a\u9996\u5148\u63d0\u53d6\u56e0\u679c\u56fe\u4ee5\u5b66\u4e60\u6570\u636e\u7684\u56e0\u679c\u5173\u7cfb\uff0c\u7136\u540e\u4f7f\u7528\u81ea\u5b9a\u4e49\u7684\u6761\u4ef6WGAN-GP\uff0c\u5e76\u6839\u636e\u56e0\u679c\u56fe\u7684\u7ed3\u6784\u8fdb\u884c\u64cd\u4f5c\u3002\u751f\u6210\u5668\u901a\u8fc7\u65b0\u7684\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u76ee\u6807\u8fdb\u884c\u8bad\u7ec3\uff0c\u4ee5\u5bf9\u9f50\u771f\u5b9e\u6570\u636e\u548c\u751f\u6210\u6570\u636e\u7684\u56e0\u679c\u56fe\uff0c\u786e\u4fdd\u56e0\u679c\u610f\u8bc6\u3002", "result": "CA-GAN\u572814\u4e2a\u8868\u683c\u6570\u636e\u96c6\u4e0a\uff0c\u5728\u56e0\u679c\u4fdd\u6301\u3001\u6548\u7528\u4fdd\u6301\u548c\u9690\u79c1\u4fdd\u6301\u4e09\u4e2a\u6838\u5fc3\u6570\u636e\u5de5\u7a0b\u6307\u6807\u4e0a\uff0c\u4f18\u4e8e\u516d\u79cd\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "CA-GAN\u4e3a\u5bfb\u6c42\u9ad8\u8d28\u91cf\u3001\u7b26\u5408\u9690\u79c1\u89c4\u5b9a\u7684\u5408\u6210\u6570\u636e\u96c6\u7684\u6570\u636e\u5de5\u7a0b\u5e08\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u3001\u9ad8\u6027\u80fd\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u7528\u4e8e\u6570\u636e\u5e93\u7cfb\u7edf\u57fa\u51c6\u6d4b\u8bd5\u3001\u52a0\u901f\u8f6f\u4ef6\u5f00\u53d1\u548c\u4fc3\u8fdb\u5b89\u5168\u7684\u6570\u636e\u9a71\u52a8\u7814\u7a76\u3002"}}
{"id": "2510.24684", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24684", "abs": "https://arxiv.org/abs/2510.24684", "authors": ["Bo Liu", "Chuanyang Jin", "Seungone Kim", "Weizhe Yuan", "Wenting Zhao", "Ilia Kulikov", "Xian Li", "Sainbayar Sukhbaatar", "Jack Lanchantin", "Jason Weston"], "title": "SPICE: Self-Play In Corpus Environments Improves Reasoning", "comment": null, "summary": "Self-improving systems require environmental interaction for continuous\nadaptation. We introduce SPICE (Self-Play In Corpus Environments), a\nreinforcement learning framework where a single model acts in two roles: a\nChallenger that mines documents from a large corpus to generate diverse\nreasoning tasks, and a Reasoner that solves them. Through adversarial dynamics,\nthe Challenger creates an automatic curriculum at the frontier of the\nReasoner's capability, while corpus grounding provides the rich,\nnear-inexhaustible external signal necessary for sustained improvement. Unlike\nexisting ungrounded self-play methods that offer more limited benefits, SPICE\nachieves consistent gains across mathematical (+8.9%) and general reasoning\n(+9.8%) benchmarks on multiple model families. Our analysis reveals how\ndocument grounding is a key ingredient in SPICE to continuously generate its\nown increasingly challenging goals and achieve them, enabling sustained\nself-improvement.", "AI": {"tldr": "SPICE\u662f\u4e00\u4e2a\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4e00\u4e2a\u6a21\u578b\u626e\u6f14\u6311\u6218\u8005\u548c\u63a8\u7406\u8005\u7684\u89d2\u8272\uff0c\u5229\u7528\u6587\u6863\u8bed\u6599\u5e93\u751f\u6210\u548c\u89e3\u51b3\u63a8\u7406\u4efb\u52a1\uff0c\u5b9e\u73b0\u4e86\u6301\u7eed\u7684\u81ea\u6211\u6539\u8fdb\uff0c\u5e76\u5728\u6570\u5b66\u548c\u901a\u7528\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u9700\u8981\u73af\u5883\u4ea4\u4e92\u6765\u5b9e\u73b0\u6301\u7eed\u9002\u5e94\u7684\u81ea\u6539\u8fdb\u7cfb\u7edf\uff0c\u800c\u73b0\u6709\u7684\u975e\u63a5\u5730\u81ea\u73a9\u65b9\u6cd5\u6548\u76ca\u6709\u9650\u3002", "method": "SPICE\u6846\u67b6\uff1a\u5355\u4e2a\u6a21\u578b\u626e\u6f14\u4e24\u4e2a\u89d2\u8272\u2014\u2014\u6311\u6218\u8005\uff08\u4ece\u5927\u578b\u8bed\u6599\u5e93\u4e2d\u6316\u6398\u6587\u6863\u4ee5\u751f\u6210\u591a\u6837\u5316\u7684\u63a8\u7406\u4efb\u52a1\uff09\u548c\u63a8\u7406\u8005\uff08\u89e3\u51b3\u8fd9\u4e9b\u4efb\u52a1\uff09\u3002\u901a\u8fc7\u5bf9\u6297\u6027\u52a8\u6001\uff0c\u6311\u6218\u8005\u5728\u63a8\u7406\u8005\u7684\u80fd\u529b\u524d\u6cbf\u521b\u5efa\u81ea\u52a8\u8bfe\u7a0b\uff0c\u800c\u8bed\u6599\u5e93\u63a5\u5730\u63d0\u4f9b\u4e86\u6301\u7eed\u6539\u8fdb\u6240\u9700\u7684\u4e30\u5bcc\u3001\u8fd1\u4e4e\u65e0\u9650\u7684\u5916\u90e8\u4fe1\u53f7\u3002", "result": "\u4e0e\u73b0\u6709\u7684\u975e\u63a5\u5730\u81ea\u73a9\u65b9\u6cd5\u4e0d\u540c\uff0cSPICE\u5728\u591a\u4e2a\u6a21\u578b\u7cfb\u5217\u7684\u6570\u5b66\uff08+8.9%\uff09\u548c\u901a\u7528\u63a8\u7406\uff08+9.8%\uff09\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6301\u7eed\u7684\u6536\u76ca\u3002", "conclusion": "\u6587\u6863\u63a5\u5730\u662fSPICE\u6301\u7eed\u751f\u6210\u81ea\u8eab\u65e5\u76ca\u590d\u6742\u7684\u76ee\u6807\u5e76\u5b9e\u73b0\u5b83\u4eec\uff0c\u4ece\u800c\u5b9e\u73b0\u6301\u7eed\u81ea\u6211\u6539\u8fdb\u7684\u5173\u952e\u56e0\u7d20\u3002"}}
{"id": "2510.24049", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24049", "abs": "https://arxiv.org/abs/2510.24049", "authors": ["Hao Jia", "Penghao Zhao", "Hao Wu", "Yuan Gao", "Yangyu Tao", "Bin Cui"], "title": "Learning from History: A Retrieval-Augmented Framework for Spatiotemporal Prediction", "comment": null, "summary": "Accurate and long-term spatiotemporal prediction for complex physical systems\nremains a fundamental challenge in scientific computing. While deep learning\nmodels, as powerful parametric approximators, have shown remarkable success,\nthey suffer from a critical limitation: the accumulation of errors during\nlong-term autoregressive rollouts often leads to physically implausible\nartifacts. This deficiency arises from their purely parametric nature, which\nstruggles to capture the full constraints of a system's intrinsic dynamics. To\naddress this, we introduce a novel \\textbf{Retrieval-Augmented Prediction\n(RAP)} framework, a hybrid paradigm that synergizes the predictive power of\ndeep networks with the grounded truth of historical data. The core philosophy\nof RAP is to leverage historical evolutionary exemplars as a non-parametric\nestimate of the system's local dynamics. For any given state, RAP efficiently\nretrieves the most similar historical analog from a large-scale database. The\ntrue future evolution of this analog then serves as a \\textbf{reference\ntarget}. Critically, this target is not a hard constraint in the loss function\nbut rather a powerful conditional input to a specialized dual-stream\narchitecture. It provides strong \\textbf{dynamic guidance}, steering the\nmodel's predictions towards physically viable trajectories. In extensive\nbenchmarks across meteorology, turbulence, and fire simulation, RAP not only\nsurpasses state-of-the-art methods but also significantly outperforms a strong\n\\textbf{analog-only forecasting baseline}. More importantly, RAP generates\npredictions that are more physically realistic by effectively suppressing error\ndivergence in long-term rollouts.", "AI": {"tldr": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u957f\u671f\u9884\u6d4b\u4e2d\u5b58\u5728\u8bef\u5dee\u7d2f\u79ef\u5bfc\u81f4\u7269\u7406\u4e0d\u5408\u7406\u7684\u95ee\u9898\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u548c\u5386\u53f2\u6570\u636e\u68c0\u7d22\u7684\u68c0\u7d22\u589e\u5f3a\u9884\u6d4b\uff08RAP\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u53c2\u8003\u5386\u53f2\u6570\u636e\u6f14\u5316\u6765\u6307\u5bfc\u6a21\u578b\u9884\u6d4b\uff0c\u4ece\u800c\u63d0\u9ad8\u7269\u7406\u771f\u5b9e\u6027\u5e76\u51cf\u5c11\u957f\u671f\u9884\u6d4b\u8bef\u5dee\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u7269\u7406\u7cfb\u7edf\u65f6\uff0c\u957f\u671f\u9884\u6d4b\u4f1a\u56e0\u8bef\u5dee\u7d2f\u79ef\u800c\u4ea7\u751f\u7269\u7406\u4e0a\u4e0d\u5408\u7406\u7684\u7ed3\u679c\uff0c\u8fd9\u662f\u56e0\u4e3a\u7eaf\u7cb9\u7684\u53c2\u6570\u5316\u6a21\u578b\u96be\u4ee5\u5b8c\u5168\u6355\u6349\u7cfb\u7edf\u7684\u5185\u5728\u52a8\u529b\u5b66\u7ea6\u675f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u68c0\u7d22\u589e\u5f3a\u9884\u6d4b\uff08RAP\uff09\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u6df1\u5ea6\u7f51\u7edc\u7684\u9884\u6d4b\u80fd\u529b\u548c\u5386\u53f2\u6570\u636e\u7684\u771f\u5b9e\u6027\u3002RAP\u7684\u6838\u5fc3\u601d\u60f3\u662f\u5229\u7528\u5386\u53f2\u6f14\u5316\u6837\u672c\u4f5c\u4e3a\u7cfb\u7edf\u5c40\u90e8\u52a8\u529b\u5b66\u7684\u975e\u53c2\u6570\u4f30\u8ba1\u3002\u5bf9\u4e8e\u4efb\u4f55\u7ed9\u5b9a\u72b6\u6001\uff0cRAP\u80fd\u4ece\u5927\u89c4\u6a21\u6570\u636e\u5e93\u4e2d\u9ad8\u6548\u68c0\u7d22\u51fa\u6700\u76f8\u4f3c\u7684\u5386\u53f2\u7c7b\u4f3c\u7269\uff0c\u5e76\u5c06\u5176\u771f\u5b9e\u672a\u6765\u6f14\u5316\u4f5c\u4e3a\u201c\u53c2\u8003\u76ee\u6807\u201d\u3002\u8fd9\u4e2a\u76ee\u6807\u5e76\u975e\u76f4\u63a5\u4f5c\u4e3a\u635f\u5931\u51fd\u6570\u7684\u786c\u7ea6\u675f\uff0c\u800c\u662f\u4f5c\u4e3a\u4e00\u79cd\u6761\u4ef6\u8f93\u5165\uff0c\u901a\u8fc7\u4e13\u95e8\u7684\u53cc\u6d41\u67b6\u6784\u4e3a\u6a21\u578b\u63d0\u4f9b\u5f3a\u5927\u7684\u52a8\u529b\u5b66\u6307\u5bfc\uff0c\u5f15\u5bfc\u6a21\u578b\u9884\u6d4b\u671d\u7740\u7269\u7406\u4e0a\u53ef\u884c\u7684\u8f68\u8ff9\u53d1\u5c55\u3002", "result": "\u5728\u6c14\u8c61\u5b66\u3001\u6e4d\u6d41\u548c\u706b\u707e\u6a21\u62df\u7b49\u9886\u57df\u7684\u5e7f\u6cdb\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRAP\u7684\u8868\u73b0\u4e0d\u4ec5\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u800c\u4e14\u663e\u8457\u4f18\u4e8e\u4ec5\u4f7f\u7528\u7c7b\u4f3c\u7269\u8fdb\u884c\u9884\u6d4b\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002\u66f4\u91cd\u8981\u7684\u662f\uff0cRAP\u901a\u8fc7\u6709\u6548\u6291\u5236\u957f\u671f\u9884\u6d4b\u4e2d\u7684\u8bef\u5dee\u53d1\u6563\uff0c\u751f\u6210\u4e86\u66f4\u7b26\u5408\u7269\u7406\u89c4\u5f8b\u7684\u9884\u6d4b\u3002", "conclusion": "RAP\u6846\u67b6\u6210\u529f\u5730\u5c06\u6df1\u5ea6\u5b66\u4e60\u7684\u9884\u6d4b\u80fd\u529b\u4e0e\u5386\u53f2\u6570\u636e\u68c0\u7d22\u76f8\u7ed3\u5408\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u957f\u671f\u9884\u6d4b\u4e2d\u51fa\u73b0\u7684\u8bef\u5dee\u7d2f\u79ef\u548c\u7269\u7406\u4e0d\u5408\u7406\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u590d\u6742\u7269\u7406\u7cfb\u7edf\u7684\u9884\u6d4b\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u9884\u6d4b\u7684\u7269\u7406\u771f\u5b9e\u6027\u3002"}}
{"id": "2510.24694", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24694", "abs": "https://arxiv.org/abs/2510.24694", "authors": ["Yida Zhao", "Kuan Li", "Xixi Wu", "Liwen Zhang", "Dingchu Zhang", "Baixuan Li", "Maojia Song", "Zhuo Chen", "Chenxi Wang", "Xinyu Wang", "Kewei Tu", "Pengjun Xie", "Jingren Zhou", "Yong Jiang"], "title": "Repurposing Synthetic Data for Fine-grained Search Agent Supervision", "comment": null, "summary": "LLM-based search agents are increasingly trained on entity-centric synthetic\ndata to solve complex, knowledge-intensive tasks. However, prevailing training\nmethods like Group Relative Policy Optimization (GRPO) discard this rich entity\ninformation, relying instead on sparse, outcome-based rewards. This critical\nlimitation renders them unable to distinguish informative \"near-miss\"\nsamples-those with substantially correct reasoning but a flawed final\nanswer-from complete failures, thus discarding valuable learning signals. We\naddress this by leveraging the very entities discarded during training. Our\nempirical analysis reveals a strong positive correlation between the number of\nground-truth entities identified during an agent's reasoning process and final\nanswer accuracy. Building on this insight, we introduce Entity-aware Group\nRelative Policy Optimization (E-GRPO), a novel framework that formulates a\ndense entity-aware reward function. E-GRPO assigns partial rewards to incorrect\nsamples proportional to their entity match rate, enabling the model to\neffectively learn from these \"near-misses\". Experiments on diverse\nquestion-answering (QA) and deep research benchmarks show that E-GRPO\nconsistently and significantly outperforms the GRPO baseline. Furthermore, our\nanalysis reveals that E-GRPO not only achieves superior accuracy but also\ninduces more efficient reasoning policies that require fewer tool calls,\ndemonstrating a more effective and sample-efficient approach to aligning search\nagents.", "AI": {"tldr": "LLM\u641c\u7d22\u4ee3\u7406\u4f7f\u7528\u57fa\u4e8e\u5b9e\u4f53\u7684\u5408\u6210\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u4e86\u5b9e\u4f53\u4fe1\u606f\u3002\u672c\u6587\u63d0\u51fa\u7684E-GRPO\u6846\u67b6\u901a\u8fc7\u5f15\u5165\u5b9e\u4f53\u611f\u77e5\u5956\u52b1\u51fd\u6570\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u4ece\u201c\u63a5\u8fd1\u4f46\u672a\u547d\u4e2d\u201d\u7684\u6837\u672c\u4e2d\u5b66\u4e60\uff0c\u4ece\u800c\u63d0\u9ad8\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709LLM\u641c\u7d22\u4ee3\u7406\u7684\u8bad\u7ec3\u65b9\u6cd5\uff08\u5982GRPO\uff09\u5ffd\u7565\u4e86\u5b9e\u4f53\u4fe1\u606f\uff0c\u4f9d\u8d56\u7a00\u758f\u7684\u57fa\u4e8e\u7ed3\u679c\u7684\u5956\u52b1\uff0c\u65e0\u6cd5\u533a\u5206\u6709\u4ef7\u503c\u7684\u201c\u63a5\u8fd1\u4f46\u672a\u547d\u4e2d\u201d\u6837\u672c\u548c\u5b8c\u5168\u5931\u8d25\u7684\u6837\u672c\uff0c\u5bfc\u81f4\u5b66\u4e60\u4fe1\u53f7\u4e22\u5931\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aE-GRPO\u7684\u65b0\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5bc6\u96c6\u7684\u3001\u5b9e\u4f53\u611f\u77e5\u7684\u5956\u52b1\u51fd\u6570\u3002E-GRPO\u6839\u636e\u4e0d\u6b63\u786e\u6837\u672c\u7684\u5b9e\u4f53\u5339\u914d\u7387\u5206\u914d\u90e8\u5206\u5956\u52b1\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u4ece\u201c\u63a5\u8fd1\u4f46\u672a\u547d\u4e2d\u201d\u7684\u6837\u672c\u4e2d\u6709\u6548\u5b66\u4e60\u3002", "result": "\u5728\u5404\u79cd\u95ee\u7b54\uff08QA\uff09\u548c\u6df1\u5ea6\u7814\u7a76\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cE-GRPO\u7684\u6027\u80fd\u6301\u7eed\u4e14\u663e\u8457\u4f18\u4e8eGRPO\u57fa\u7ebf\u3002\u6b64\u5916\uff0cE-GRPO\u4e0d\u4ec5\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u51c6\u786e\u6027\uff0c\u8fd8\u8bf1\u5bfc\u4e86\u66f4\u6709\u6548\u7684\u63a8\u7406\u7b56\u7565\uff0c\u51cf\u5c11\u4e86\u5de5\u5177\u8c03\u7528\u6b21\u6570\u3002", "conclusion": "E-GRPO\u901a\u8fc7\u5229\u7528\u88ab\u4e22\u5f03\u7684\u5b9e\u4f53\u4fe1\u606f\u5e76\u5f15\u5165\u5b9e\u4f53\u611f\u77e5\u5956\u52b1\uff0c\u80fd\u591f\u66f4\u6709\u6548\u5730\u8bad\u7ec3LLM\u641c\u7d22\u4ee3\u7406\uff0c\u63d0\u9ad8\u51c6\u786e\u6027\u5e76\u51cf\u5c11\u6837\u672c\u9700\u6c42\uff0c\u662f\u4e00\u79cd\u66f4\u6709\u6548\u4e14\u6837\u672c\u6548\u7387\u66f4\u9ad8\u7684\u65b9\u6cd5\u3002"}}
{"id": "2510.24053", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2510.24053", "abs": "https://arxiv.org/abs/2510.24053", "authors": ["Jacob B. Roberts", "Catherine R. Ji", "Isaac Donnell", "Thomas D. Young", "Allison N. Pearson", "Graham A. Hudson", "Leah S. Keiser", "Mia Wesselkamper", "Peter H. Winegar", "Janik Ludwig", "Sarah H. Klass", "Isha V. Sheth", "Ezechinyere C. Ukabiala", "Maria C. T. Astolfi", "Benjamin Eysenbach", "Jay D. Keasling"], "title": "Low-N Protein Activity Optimization with FolDE", "comment": "18 pages, 4 figures. Preprint. Open-source software available at\n  https://github.com/JBEI/foldy", "summary": "Proteins are traditionally optimized through the costly construction and\nmeasurement of many mutants. Active Learning-assisted Directed Evolution (ALDE)\nalleviates that cost by predicting the best improvements and iteratively\ntesting mutants to inform predictions. However, existing ALDE methods face a\ncritical limitation: selecting the highest-predicted mutants in each round\nyields homogeneous training data insufficient for accurate prediction models in\nsubsequent rounds. Here we present FolDE, an ALDE method designed to maximize\nend-of-campaign success. In simulations across 20 protein targets, FolDE\ndiscovers 23% more top 10% mutants than the best baseline ALDE method (p=0.005)\nand is 55% more likely to find top 1% mutants. FolDE achieves this primarily\nthrough naturalness-based warm-starting, which augments limited activity\nmeasurements with protein language model outputs to improve activity\nprediction. We also introduce a constant-liar batch selector, which improves\nbatch diversity; this is important in multi-mutation campaigns but had limited\neffect in our benchmarks. The complete workflow is freely available as\nopen-source software, making efficient protein optimization accessible to any\nlaboratory.", "AI": {"tldr": "FolDE\u901a\u8fc7\u81ea\u7136\u6027\u5f15\u5bfc\u7684\u9884\u70ed\u548c\u5e38\u6570\u8c0e\u8a00\u6279\u5904\u7406\u9009\u62e9\u5668\uff0c\u5728\u86cb\u767d\u8d28\u4f18\u5316\u4e2d\u63d0\u9ad8\u4e86\u6548\u7387\u548c\u6210\u529f\u7387\u3002", "motivation": "\u4f20\u7edf\u7684\u86cb\u767d\u8d28\u4f18\u5316\u65b9\u6cd5\u6210\u672c\u9ad8\u6602\uff0c\u800c\u73b0\u6709\u7684\u4e3b\u52a8\u5b66\u4e60\u8f85\u52a9\u5b9a\u5411\u6f14\u5316\uff08ALDE\uff09\u65b9\u6cd5\u5728\u751f\u6210\u8bad\u7ec3\u6570\u636e\u65f6\u5b58\u5728\u6570\u636e\u540c\u8d28\u5316\u7684\u95ee\u9898\uff0c\u5f71\u54cd\u4e86\u540e\u7eed\u9884\u6d4b\u6a21\u578b\u7684\u51c6\u786e\u6027\u3002", "method": "FolDE\u662f\u4e00\u79cd\u65b0\u7684ALDE\u65b9\u6cd5\uff0c\u5b83\u7ed3\u5408\u4e86\u81ea\u7136\u6027\u5f15\u5bfc\u7684\u9884\u70ed\uff08\u4f7f\u7528\u86cb\u767d\u8d28\u8bed\u8a00\u6a21\u578b\u8f93\u51fa\u589e\u5f3a\u6709\u9650\u7684\u6d3b\u6027\u6d4b\u91cf\uff09\u548c\u5e38\u6570\u8c0e\u8a00\u6279\u5904\u7406\u9009\u62e9\u5668\uff08\u63d0\u9ad8\u6279\u5904\u7406\u591a\u6837\u6027\uff09\uff0c\u4ee5\u6700\u5927\u5316\u4f18\u5316\u6d3b\u52a8\u7684\u6210\u529f\u7387\u3002", "result": "\u572820\u4e2a\u86cb\u767d\u8d28\u76ee\u6807\u7684\u6a21\u62df\u4e2d\uff0cFolDE\u53d1\u73b0\u7684\u6392\u540d\u524d10%\u7684\u7a81\u53d8\u4f53\u6bd4\u73b0\u6709\u7684\u6700\u4f73ALDE\u65b9\u6cd5\u591a23%\uff08p=0.005\uff09\uff0c\u5e76\u4e14\u627e\u5230\u6392\u540d\u524d1%\u7684\u7a81\u53d8\u4f53\u7684\u53ef\u80fd\u6027\u9ad855%\u3002", "conclusion": "FolDE\u901a\u8fc7\u5176\u72ec\u7279\u7684\u7b56\u7565\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u86cb\u767d\u8d28\u4f18\u5316\u8fc7\u7a0b\u7684\u6548\u7387\u548c\u53d1\u73b0\u9ad8\u8d28\u91cf\u7a81\u53d8\u4f53\u7684\u80fd\u529b\uff0c\u5e76\u4e14\u8be5\u5de5\u4f5c\u6d41\u7a0b\u5df2\u5f00\u6e90\uff0c\u65b9\u4fbf\u5e7f\u6cdb\u5e94\u7528\u3002"}}
{"id": "2510.24695", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24695", "abs": "https://arxiv.org/abs/2510.24695", "authors": ["Xuanzhong Chen", "Zile Qiao", "Guoxin Chen", "Liangcai Su", "Zhen Zhang", "Xinyu Wang", "Pengjun Xie", "Fei Huang", "Jingren Zhou", "Yong Jiang"], "title": "AgentFrontier: Expanding the Capability Frontier of LLM Agents with ZPD-Guided Data Synthesis", "comment": "https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/", "summary": "Training large language model agents on tasks at the frontier of their\ncapabilities is key to unlocking advanced reasoning. We introduce a data\nsynthesis approach inspired by the educational theory of the Zone of Proximal\nDevelopment (ZPD), which defines this frontier as tasks an LLM cannot solve\nalone but can master with guidance. To operationalize this, we present the\nAgentFrontier Engine, an automated pipeline that synthesizes high-quality,\nmultidisciplinary data situated precisely within the LLM's ZPD. This engine\nsupports both continued pre-training with knowledge-intensive data and targeted\npost-training on complex reasoning tasks. From the same framework, we derive\nthe ZPD Exam, a dynamic and automated benchmark designed to evaluate agent\ncapabilities on these frontier tasks. We train AgentFrontier-30B-A3B model on\nour synthesized data, which achieves state-of-the-art results on demanding\nbenchmarks like Humanity's Last Exam, even surpassing some leading proprietary\nagents. Our work demonstrates that a ZPD-guided approach to data synthesis\noffers a scalable and effective path toward building more capable LLM agents.", "AI": {"tldr": "LLM\u4ee3\u7406\u7684\u8bad\u7ec3\u53ef\u4ee5\u5229\u7528ZPD\u7406\u8bba\uff0c\u901a\u8fc7AgentFrontier\u5f15\u64ce\u751f\u6210\u6570\u636e\uff0c\u5e76\u7528ZPD Exam\u8fdb\u884c\u8bc4\u4f30\uff0c\u4ece\u800c\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\u4e0a\u53d6\u5f97\u6700\u5148\u8fdb\u7684\u6210\u679c\u3002", "motivation": "\u4e3a\u4e86\u63d0\u5347LLM\u4ee3\u7406\u89e3\u51b3\u590d\u6742\u63a8\u7406\u4efb\u52a1\u7684\u80fd\u529b\uff0c\u9700\u8981\u5bf9\u5176\u8fdb\u884c\u524d\u6cbf\u4efb\u52a1\u7684\u8bad\u7ec3\uff0c\u800c\u8fd9\u4e9b\u4efb\u52a1\u7684\u96be\u5ea6\u6070\u597d\u5728LLM\u7684\u201c\u6700\u8fd1\u53d1\u5c55\u533a\u201d\uff08ZPD\uff09\u5185\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAgentFrontier Engine\u7684\u6570\u636e\u5408\u6210\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u501f\u9274\u4e86ZPD\u7406\u8bba\uff0c\u81ea\u52a8\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u8de8\u5b66\u79d1\u7684\u6570\u636e\uff0c\u7528\u4e8eLLM\u7684\u6301\u7eed\u9884\u8bad\u7ec3\u548c\u6709\u9488\u5bf9\u6027\u7684\u8bad\u7ec3\u3002\u540c\u65f6\uff0c\u4e5f\u63d0\u51fa\u4e86ZPD Exam\u4f5c\u4e3a\u8bc4\u4f30\u57fa\u51c6\u3002", "result": "\u4f7f\u7528AgentFrontier Engine\u5408\u6210\u7684\u6570\u636e\u8bad\u7ec3\u7684AgentFrontier-30B-A3B\u6a21\u578b\uff0c\u5728Humanity's Last Exam\u7b49\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6210\u679c\uff0c\u751a\u81f3\u4f18\u4e8e\u4e00\u4e9b\u9886\u5148\u7684\u4e13\u6709\u4ee3\u7406\u3002", "conclusion": "\u57fa\u4e8eZPD\u7406\u8bba\u7684\u6570\u636e\u5408\u6210\u65b9\u6cd5\u4e3a\u6784\u5efa\u66f4\u5f3a\u5927\u7684LLM\u4ee3\u7406\u63d0\u4f9b\u4e86\u4e00\u6761\u53ef\u6269\u5c55\u4e14\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2510.24061", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24061", "abs": "https://arxiv.org/abs/2510.24061", "authors": ["Kanghyun Choi", "Hyeyoon Lee", "SunJong Park", "Dain Kwon", "Jinho Lee"], "title": "FALQON: Accelerating LoRA Fine-tuning with Low-Bit Floating-Point Arithmetic", "comment": "NeurIPS 2025", "summary": "Low-bit floating-point (FP) formats, such as FP8, provide significant\nacceleration and memory savings in model training thanks to native hardware\nsupport on modern GPUs and NPUs. However, we analyze that FP8 quantization\noffers speedup primarily for large-dimensional matrix multiplications, while\ninherent quantization overheads diminish speedup when applied to low-rank\nadaptation (LoRA), which uses small-dimensional matrices for efficient\nfine-tuning of large language models (LLMs). To address this limitation, we\npropose FALQON, a novel framework that eliminates the quantization overhead\nfrom separate LoRA computational paths by directly merging LoRA adapters into\nan FP8-quantized backbone during fine-tuning. Furthermore, we reformulate the\nforward and backward computations for merged adapters to significantly reduce\nquantization overhead, and introduce a row-wise proxy update mechanism that\nefficiently integrates substantial updates into the quantized backbone.\nExperimental evaluations demonstrate that FALQON achieves approximately a\n3$\\times$ training speedup over existing quantized LoRA methods with a similar\nlevel of accuracy, providing a practical solution for efficient large-scale\nmodel fine-tuning. Moreover, FALQON's end-to-end FP8 workflow removes the need\nfor post-training quantization, facilitating efficient deployment. Code is\navailable at https://github.com/iamkanghyunchoi/falqon.", "AI": {"tldr": "FALQON\u6846\u67b6\u901a\u8fc7\u5c06LoRA\u9002\u914d\u5668\u76f4\u63a5\u5408\u5e76\u5230FP8\u91cf\u5316\u4e3b\u5e72\u4e2d\uff0c\u5e76\u4f18\u5316\u8ba1\u7b97\u8fc7\u7a0b\uff0c\u89e3\u51b3\u4e86\u4f4e\u6bd4\u7279\u6d6e\u70b9\uff08FP8\uff09\u91cf\u5316\u5728\u4f4e\u79e9\u9002\u914d\uff08LoRA\uff09\u4e2d\u6548\u7387\u4e0d\u9ad8\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u8fd13\u500d\u7684\u8bad\u7ec3\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u76f8\u4f3c\u7684\u7cbe\u5ea6\u3002", "motivation": "FP8\u91cf\u5316\u5728\u5927\u578b\u6a21\u578b\u5fae\u8c03\u4e2d\u867d\u7136\u80fd\u63d0\u4f9b\u52a0\u901f\u548c\u8282\u7701\u5185\u5b58\uff0c\u4f46\u5728\u4f4e\u79e9\u9002\u914d\uff08LoRA\uff09\u4e2d\uff0c\u7531\u4e8e\u5176\u4f7f\u7528\u7684\u5c0f\u578b\u77e9\u9635\uff0c\u91cf\u5316\u5f00\u9500\u4f1a\u62b5\u6d88\u52a0\u901f\u6548\u679c\u3002\u73b0\u6709\u7684FP8\u91cf\u5316\u65b9\u6cd5\u4e3b\u8981\u52a0\u901f\u7ef4\u5ea6\u8f83\u9ad8\u7684\u77e9\u9635\u4e58\u6cd5\uff0c\u672a\u80fd\u6709\u6548\u89e3\u51b3LoRA\u7684\u6548\u7387\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFALQON\u7684\u65b0\u6846\u67b6\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u5728\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u5c06LoRA\u9002\u914d\u5668\u76f4\u63a5\u5408\u5e76\u5230FP8\u91cf\u5316\u4e3b\u5e72\u4e2d\uff0c\u6d88\u9664\u4e86LoRA\u8ba1\u7b97\u8def\u5f84\u7684\u72ec\u7acb\u91cf\u5316\u5f00\u9500\u3002\u6b64\u5916\uff0c\u5b83\u91cd\u65b0\u8bbe\u8ba1\u4e86\u5408\u5e76\u9002\u914d\u5668\u7684\u524d\u5411\u548c\u540e\u5411\u4f20\u64ad\u8ba1\u7b97\uff0c\u4ee5\u5927\u5e45\u964d\u4f4e\u91cf\u5316\u5f00\u9500\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u79cd\u9010\u884c\u4ee3\u7406\u66f4\u65b0\u673a\u5236\uff0c\u4ee5\u6709\u6548\u5730\u5c06\u5927\u91cf\u66f4\u65b0\u96c6\u6210\u5230\u91cf\u5316\u4e3b\u5e72\u4e2d\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0cFALQON\u4e0e\u73b0\u6709\u7684\u91cf\u5316LoRA\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5728\u76f8\u4f3c\u7684\u7cbe\u5ea6\u6c34\u5e73\u4e0b\uff0c\u8bad\u7ec3\u901f\u5ea6\u5927\u7ea6\u63d0\u9ad8\u4e863\u500d\u3002FALQON\u7684\u7aef\u5230\u7aefFP8\u5de5\u4f5c\u6d41\u7a0b\u65e0\u9700\u8bad\u7ec3\u540e\u91cf\u5316\uff0c\u4fbf\u4e8e\u9ad8\u6548\u90e8\u7f72\u3002", "conclusion": "FALQON\u4e3a\u9ad8\u6548\u7684\u5927\u89c4\u6a21\u6a21\u578b\u5fae\u8c03\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u9645\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5b83\u901a\u8fc7\u5c06FP8\u91cf\u5316\u4e0eLoRA\u5408\u5e76\u548c\u4f18\u5316\u8ba1\u7b97\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8bad\u7ec3\u901f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u7684\u7cbe\u5ea6\uff0c\u5e76\u7b80\u5316\u4e86\u90e8\u7f72\u6d41\u7a0b\u3002"}}
{"id": "2510.24697", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24697", "abs": "https://arxiv.org/abs/2510.24697", "authors": ["Zhengwei Tao", "Haiyang Shen", "Baixuan Li", "Wenbiao Yin", "Jialong Wu", "Kuan Li", "Zhongwang Zhang", "Huifeng Yin", "Rui Ye", "Liwen Zhang", "Xinyu Wang", "Pengjun Xie", "Jingren Zhou", "Yong Jiang"], "title": "WebLeaper: Empowering Efficiency and Efficacy in WebAgent via Enabling Info-Rich Seeking", "comment": null, "summary": "Large Language Model (LLM)-based agents have emerged as a transformative\napproach for open-ended problem solving, with information seeking (IS) being a\ncore capability that enables autonomous reasoning and decision-making. While\nprior research has largely focused on improving retrieval depth, we observe\nthat current IS agents often suffer from low search efficiency, which in turn\nconstrains overall performance. A key factor underlying this inefficiency is\nthe sparsity of target entities in training tasks, which limits opportunities\nfor agents to learn and generalize efficient search behaviors. To address these\nchallenges, we propose WebLeaper, a framework for constructing high-coverage IS\ntasks and generating efficient solution trajectories. We formulate IS as a\ntree-structured reasoning problem, enabling a substantially larger set of\ntarget entities to be embedded within a constrained context. Leveraging curated\nWikipedia tables, we propose three variants for synthesizing IS tasks, Basic,\nUnion, and Reverse-Union, to systematically increase both IS efficiency and\nefficacy. Finally, we curate training trajectories by retaining only those that\nare simultaneously accurate and efficient, ensuring that the model is optimized\nfor both correctness and search performance. Extensive experiments on both\nbasic and comprehensive settings, conducted on five IS benchmarks, BrowserComp,\nGAIA, xbench-DeepSearch, WideSearch, and Seal-0, demonstrate that our method\nconsistently achieves improvements in both effectiveness and efficiency over\nstrong baselines.", "AI": {"tldr": "LLM\u9a71\u52a8\u7684\u4fe1\u606f\u68c0\u7d22\u4ee3\u7406\u5b58\u5728\u641c\u7d22\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86WebLeaper\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u9ad8\u8986\u76d6\u7387\u7684\u4fe1\u606f\u68c0\u7d22\u4efb\u52a1\u548c\u751f\u6210\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u6765\u89e3\u51b3\u6b64\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684LLM\u4ee3\u7406\u5728\u5f00\u653e\u5f0f\u95ee\u9898\u89e3\u51b3\u65b9\u9762\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u4fe1\u606f\u68c0\u7d22\uff08IS\uff09\u4f5c\u4e3a\u5176\u6838\u5fc3\u80fd\u529b\uff0c\u5728\u6548\u7387\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u6574\u4f53\u6027\u80fd\u3002\u4e3b\u8981\u539f\u56e0\u662f\u8bad\u7ec3\u6570\u636e\u4e2d\u76ee\u6807\u5b9e\u4f53\u7a00\u758f\uff0c\u963b\u788d\u4e86\u4ee3\u7406\u5b66\u4e60\u548c\u6cdb\u5316\u9ad8\u6548\u641c\u7d22\u884c\u4e3a\u3002", "method": "\u63d0\u51faWebLeaper\u6846\u67b6\uff0c\u5c06IS\u89c6\u4e3a\u6811\u72b6\u7ed3\u6784\u63a8\u7406\u95ee\u9898\uff0c\u589e\u52a0\u4e86\u5728\u53d7\u9650\u4e0a\u4e0b\u6587\u4e2d\u5d4c\u5165\u76ee\u6807\u5b9e\u4f53\u7684\u6570\u91cf\u3002\u5229\u7528\u7ef4\u57fa\u767e\u79d1\u8868\u683c\uff0c\u63d0\u51fa\u4e86\u4e09\u79cdIS\u4efb\u52a1\u5408\u6210\u65b9\u6cd5\uff08Basic\u3001Union\u3001Reverse-Union\uff09\u6765\u63d0\u9ad8\u6548\u7387\u548c\u6548\u679c\u3002\u901a\u8fc7\u4fdd\u7559\u51c6\u786e\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u8f68\u8ff9\u6765\u4f18\u5316\u6a21\u578b\uff0c\u4ee5\u517c\u987e\u6b63\u786e\u6027\u548c\u641c\u7d22\u6027\u80fd\u3002", "result": "\u5728BrowserComp\u3001GAIA\u3001xbench-DeepSearch\u3001WideSearch\u548cSeal-0\u4e94\u4e2a\u4fe1\u606f\u68c0\u7d22\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cWebLeaper\u5728\u57fa\u672c\u548c\u7efc\u5408\u8bbe\u7f6e\u4e0b\u5747\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6548\u679c\u548c\u6548\u7387\u63d0\u5347\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "WebLeaper\u6846\u67b6\u901a\u8fc7\u6784\u5efa\u9ad8\u8986\u76d6\u7387\u7684IS\u4efb\u52a1\u548c\u751f\u6210\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u8f68\u8ff9\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLM\u4fe1\u606f\u68c0\u7d22\u4ee3\u7406\u7684\u6548\u7387\u548c\u6548\u679c\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002"}}
{"id": "2510.24088", "categories": ["cs.LG", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.24088", "abs": "https://arxiv.org/abs/2510.24088", "authors": ["Moongyu Jeon", "Sangwoo Shin", "Dongjae Jeon", "Albert No"], "title": "Information-Theoretic Discrete Diffusion", "comment": "Accepted at NeurIPS 2025", "summary": "We present an information-theoretic framework for discrete diffusion models\nthat yields principled estimators of log-likelihood using score-matching\nlosses. Inspired by the I-MMSE identity for the Gaussian setup, we derive\nanalogous results for the discrete setting. Specifically, we introduce the\nInformation-Minimum Denoising Score Entropy (I-MDSE) relation, which links\nmutual information between data and its diffused version to the minimum\ndenoising score entropy (DSE) loss. We extend this theory to masked diffusion\nand establish the Information-Minimum Denoising Cross-Entropy (I-MDCE)\nrelation, connecting cross-entropy losses to mutual information in discrete\nmasked processes. These results provide a time-integral decomposition of the\nlog-likelihood of the data in terms of optimal score-based losses, showing that\ncommonly used losses such as DSE and DCE are not merely variational bounds but\ntight and principled estimators of log-likelihood. The I-MDCE decomposition\nfurther enables practical extensions, including time-free formula, conditional\nlikelihood estimation in prompt-response tasks, and coupled Monte Carlo\nestimation of likelihood ratios. Experiments on synthetic and real-world data\nconfirm the accuracy, variance stability, and utility of our estimators. The\ncode is publicly available at https://github.com/Dongjae0324/infodis.", "AI": {"tldr": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u4fe1\u606f\u8bba\u6846\u67b6\uff0c\u7528\u4e8e\u79bb\u6563\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u5f97\u5206\u5339\u914d\u635f\u5931\u5f97\u5230\u4f3c\u7136\u7387\u7684\u4f30\u8ba1\u3002", "motivation": "\u53d7\u9ad8\u65af\u8bbe\u7f6e\u7684I-MMSE\u6052\u7b49\u5f0f\u7684\u542f\u53d1\uff0c\u6211\u4eec\u4e3a\u79bb\u6563\u8bbe\u7f6e\u63a8\u5bfc\u4e86\u7c7b\u4f3c\u7684\u7ed3\u679c\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86\u4fe1\u606f-\u6700\u5c0f\u53bb\u566a\u5f97\u5206\u71b5\uff08I-MDSE\uff09\u5173\u7cfb\uff0c\u5c06\u6570\u636e\u4e0e\u5176\u6269\u6563\u7248\u672c\u4e4b\u95f4\u7684\u4e92\u4fe1\u606f\u4e0e\u6700\u5c0f\u53bb\u566a\u5f97\u5206\u71b5\uff08DSE\uff09\u635f\u5931\u8054\u7cfb\u8d77\u6765\u3002\u6211\u4eec\u5c06\u6b64\u7406\u8bba\u6269\u5c55\u5230\u63a9\u7801\u6269\u6563\uff0c\u5e76\u5efa\u7acb\u4fe1\u606f-\u6700\u5c0f\u53bb\u566a\u4ea4\u53c9\u71b5\uff08I-MDCE\uff09\u5173\u7cfb\uff0c\u5c06\u4ea4\u53c9\u71b5\u635f\u5931\u4e0e\u79bb\u6563\u63a9\u7801\u8fc7\u7a0b\u4e2d\u7684\u4e92\u4fe1\u606f\u8054\u7cfb\u8d77\u6765\u3002", "result": "\u8fd9\u4e9b\u7ed3\u679c\u63d0\u4f9b\u4e86\u6570\u636e\u4f3c\u7136\u7387\u7684\u65f6\u95f4\u79ef\u5206\u5206\u89e3\uff0c\u4ee5\u6700\u4f18\u7684\u57fa\u4e8e\u5f97\u5206\u7684\u635f\u5931\u6765\u8868\u793a\uff0c\u8868\u660e\u5e38\u7528\u7684DSE\u548cDCE\u7b49\u635f\u5931\u4e0d\u4ec5\u4ec5\u662f\u53d8\u5206\u4e0b\u754c\uff0c\u800c\u662f\u4f3c\u7136\u7387\u7684\u7cbe\u786e\u4e14\u6709\u539f\u5219\u7684\u4f30\u8ba1\u91cf\u3002I-MDCE\u5206\u89e3\u8fdb\u4e00\u6b65\u5b9e\u73b0\u4e86\u5b9e\u9645\u6269\u5c55\uff0c\u5305\u62ec\u65e0\u65f6\u95f4\u516c\u5f0f\u3001\u63d0\u793a-\u54cd\u5e94\u4efb\u52a1\u4e2d\u7684\u6761\u4ef6\u4f3c\u7136\u4f30\u8ba1\u4ee5\u53ca\u4f3c\u7136\u6bd4\u7684\u8026\u5408\u8499\u7279\u5361\u6d1b\u4f30\u8ba1\u3002\u5b9e\u9a8c\u8bc1\u5b9e\u4e86\u6211\u4eec\u4f30\u8ba1\u91cf\u7684\u51c6\u786e\u6027\u3001\u65b9\u5dee\u7a33\u5b9a\u6027\u548c\u6548\u7528\u3002", "conclusion": "\u63d0\u51fa\u7684\u4fe1\u606f\u8bba\u6846\u67b6\u4e3a\u79bb\u6563\u6269\u6563\u6a21\u578b\u63d0\u4f9b\u4e86\u4f3c\u7136\u7387\u7684\u4f30\u8ba1\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7I-MDSE\u548cI-MDCE\u5173\u7cfb\u5c06\u4e92\u4fe1\u606f\u4e0e\u53bb\u566a\u5f97\u5206\u71b5\u635f\u5931\u8054\u7cfb\u8d77\u6765\u3002\u5b9e\u9a8c\u7ed3\u679c\u652f\u6301\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.24698", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24698", "abs": "https://arxiv.org/abs/2510.24698", "authors": ["Baixuan Li", "Dingchu Zhang", "Jialong Wu", "Wenbiao Yin", "Zhengwei Tao", "Yida Zhao", "Liwen Zhang", "Haiyang Shen", "Runnan Fang", "Pengjun Xie", "Jingren Zhou", "Yong Jiang"], "title": "ParallelMuse: Agentic Parallel Thinking for Deep Information Seeking", "comment": null, "summary": "Parallel thinking expands exploration breadth, complementing the deep\nexploration of information-seeking (IS) agents to further enhance\nproblem-solving capability. However, conventional parallel thinking faces two\nkey challenges in this setting: inefficiency from repeatedly rolling out from\nscratch, and difficulty in integrating long-horizon reasoning trajectories\nduring answer generation, as limited context capacity prevents full\nconsideration of the reasoning process. To address these issues, we propose\nParallelMuse, a two-stage paradigm designed for deep IS agents. The first\nstage, Functionality-Specified Partial Rollout, partitions generated sequences\ninto functional regions and performs uncertainty-guided path reuse and\nbranching to enhance exploration efficiency. The second stage, Compressed\nReasoning Aggregation, exploits reasoning redundancy to losslessly compress\ninformation relevant to answer derivation and synthesize a coherent final\nanswer. Experiments across multiple open-source agents and benchmarks\ndemonstrate up to 62% performance improvement with a 10--30% reduction in\nexploratory token consumption.", "AI": {"tldr": "ParallelMuse\u662f\u4e00\u79cd\u7528\u4e8e\u589e\u5f3a\u4fe1\u606f\u641c\u5bfb\uff08IS\uff09\u4ee3\u7406\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u7684\u53cc\u9636\u6bb5\u8303\u5f0f\uff0c\u901a\u8fc7\u51fd\u6570\u6307\u5b9a\u7684\u90e8\u5206\u56de\u6eda\u6765\u63d0\u9ad8\u63a2\u7d22\u6548\u7387\uff0c\u5e76\u901a\u8fc7\u538b\u7f29\u63a8\u7406\u805a\u5408\u6765\u6574\u5408\u957f\u89c6\u63a8\u7406\u8f68\u8ff9\u3002", "motivation": "\u4f20\u7edf\u7684\u5e76\u884c\u601d\u8003\u5728\u4fe1\u606f\u641c\u5bfb\u4ee3\u7406\u4e2d\u5b58\u5728\u6548\u7387\u4f4e\u4e0b\uff08\u91cd\u590d\u4ece\u5934\u5f00\u59cb\u56de\u6eda\uff09\u548c\u6574\u5408\u957f\u89c6\u63a8\u7406\u8f68\u8ff9\u56f0\u96be\uff08\u4e0a\u4e0b\u6587\u5bb9\u91cf\u9650\u5236\uff09\u7684\u95ee\u9898\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aParallelMuse\u7684\u53cc\u9636\u6bb5\u8303\u5f0f\u3002\u7b2c\u4e00\u9636\u6bb5\u201c\u51fd\u6570\u6307\u5b9a\u7684\u90e8\u5206\u56de\u6eda\u201d\u5c06\u751f\u6210\u5e8f\u5217\u5212\u5206\u4e3a\u529f\u80fd\u533a\u57df\uff0c\u5e76\u8fdb\u884c\u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\u7684\u8def\u5f84\u91cd\u7528\u548c\u5206\u652f\uff0c\u4ee5\u63d0\u9ad8\u63a2\u7d22\u6548\u7387\u3002\u7b2c\u4e8c\u9636\u6bb5\u201c\u538b\u7f29\u63a8\u7406\u805a\u5408\u201d\u5229\u7528\u63a8\u7406\u5197\u4f59\u6765\u65e0\u635f\u538b\u7f29\u4e0e\u7b54\u6848\u63a8\u5bfc\u76f8\u5173\u7684\u4fe1\u606f\uff0c\u5e76\u5408\u6210\u8fde\u8d2f\u7684\u6700\u7ec8\u7b54\u6848\u3002", "result": "\u5b9e\u9a8c\u5728\u591a\u4e2a\u5f00\u6e90\u4ee3\u7406\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u7ed3\u679c\u663e\u793a\u6027\u80fd\u63d0\u9ad8\u4e8662%\uff0c\u540c\u65f6\u63a2\u7d22\u6027\u4ee3\u5e01\u6d88\u8017\u51cf\u5c11\u4e8610%-30%\u3002", "conclusion": "ParallelMuse\u901a\u8fc7\u63d0\u9ad8\u63a2\u7d22\u6548\u7387\u548c\u6709\u6548\u6574\u5408\u957f\u89c6\u63a8\u7406\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4fe1\u606f\u641c\u5bfb\u4ee3\u7406\u7684\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u3002"}}
{"id": "2510.24699", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24699", "abs": "https://arxiv.org/abs/2510.24699", "authors": ["Rui Ye", "Zhongwang Zhang", "Kuan Li", "Huifeng Yin", "Zhengwei Tao", "Yida Zhao", "Liangcai Su", "Liwen Zhang", "Zile Qiao", "Xinyu Wang", "Pengjun Xie", "Fei Huang", "Siheng Chen", "Jingren Zhou", "Yong Jiang"], "title": "AgentFold: Long-Horizon Web Agents with Proactive Context Management", "comment": "26 pages, 9 figures", "summary": "LLM-based web agents show immense promise for information seeking, yet their\neffectiveness on long-horizon tasks is hindered by a fundamental trade-off in\ncontext management. Prevailing ReAct-based agents suffer from context\nsaturation as they accumulate noisy, raw histories, while methods that fixedly\nsummarize the full history at each step risk the irreversible loss of critical\ndetails. Addressing these, we introduce AgentFold, a novel agent paradigm\ncentered on proactive context management, inspired by the human cognitive\nprocess of retrospective consolidation. AgentFold treats its context as a\ndynamic cognitive workspace to be actively sculpted, rather than a passive log\nto be filled. At each step, it learns to execute a `folding' operation, which\nmanages its historical trajectory at multiple scales: it can perform granular\ncondensations to preserve vital, fine-grained details, or deep consolidations\nto abstract away entire multi-step sub-tasks. The results on prominent\nbenchmarks are striking: with simple supervised fine-tuning (without continual\npre-training or RL), our AgentFold-30B-A3B agent achieves 36.2% on BrowseComp\nand 47.3% on BrowseComp-ZH. Notably, this performance not only surpasses or\nmatches open-source models of a dramatically larger scale, such as the\nDeepSeek-V3.1-671B-A37B, but also surpasses leading proprietary agents like\nOpenAI's o4-mini.", "AI": {"tldr": "AgentFold\u901a\u8fc7\u591a\u5c3a\u5ea6\u5386\u53f2\u8f68\u8ff9\u7ba1\u7406\u6765\u89e3\u51b3LLM\u7f51\u7edc\u4ee3\u7406\u4e2d\u7684\u4e0a\u4e0b\u6587\u7ba1\u7406\u95ee\u9898\uff0c\u901a\u8fc7\u2018\u6298\u53e0\u2019\u64cd\u4f5c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u4f18\u4e8e\u66f4\u5927\u6a21\u578b\u548c\u9886\u5148\u7684\u4e13\u6709\u4ee3\u7406\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8eReAct\u7684LLM\u7f51\u7edc\u4ee3\u7406\u5728\u5904\u7406\u957f\u65f6\u5e8f\u4efb\u52a1\u65f6\uff0c\u7531\u4e8e\u4e0a\u4e0b\u6587\u7ba1\u7406\u4e2d\u7684\u6743\u8861\u95ee\u9898\uff08\u4e0a\u4e0b\u6587\u9971\u548c\u6216\u4e22\u5931\u5173\u952e\u7ec6\u8282\uff09\u800c\u6548\u679c\u53d7\u9650\u3002AgentFold\u65e8\u5728\u901a\u8fc7\u4e3b\u52a8\u5f0f\u4e0a\u4e0b\u6587\u7ba1\u7406\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "method": "AgentFold\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4ee3\u7406\u8303\u5f0f\uff0c\u5176\u6838\u5fc3\u662f\u4e3b\u52a8\u5f0f\u4e0a\u4e0b\u6587\u7ba1\u7406\uff0c\u5e76\u501f\u9274\u4e86\u4eba\u7c7b\u56de\u987e\u6027\u6574\u5408\u7684\u8ba4\u77e5\u8fc7\u7a0b\u3002\u5b83\u5c06\u4e0a\u4e0b\u6587\u89c6\u4e3a\u4e00\u4e2a\u52a8\u6001\u7684\u5de5\u4f5c\u7a7a\u95f4\uff0c\u901a\u8fc7\u6267\u884c\u2018\u6298\u53e0\u2019\u64cd\u4f5c\u6765\u7ba1\u7406\u5386\u53f2\u8f68\u8ff9\uff0c\u8be5\u64cd\u4f5c\u53ef\u4ee5\u5728\u591a\u4e2a\u5c3a\u5ea6\u4e0a\u8fdb\u884c\uff1a\u7ec6\u7c92\u5ea6\u538b\u7f29\u4ee5\u4fdd\u7559\u7ec6\u8282\uff0c\u6216\u6df1\u5ea6\u6574\u5408\u4ee5\u62bd\u8c61\u5316\u591a\u6b65\u5b50\u4efb\u52a1\u3002", "result": "AgentFold-30B-A3B\u4ee3\u7406\u5728BrowseComp\u4e0a\u8fbe\u523036.2%\uff0c\u5728BrowseComp-ZH\u4e0a\u8fbe\u523047.3%\uff0c\u5176\u6027\u80fd\u8d85\u8fc7\u6216\u5ab2\u7f8e\u4e86\u89c4\u6a21\u5927\u5f97\u591a\u7684\u5f00\u6e90\u6a21\u578b\uff08\u5982DeepSeek-V3.1-671B-A37B\uff09\uff0c\u5e76\u4f18\u4e8eOpenAI\u7684o4-mini\u7b49\u9886\u5148\u7684\u4e13\u6709\u4ee3\u7406\u3002", "conclusion": "AgentFold\u901a\u8fc7\u4e3b\u52a8\u5f0f\u4e0a\u4e0b\u6587\u7ba1\u7406\uff0c\u7279\u522b\u662f\u591a\u5c3a\u5ea6\u7684\u2018\u6298\u53e0\u2019\u64cd\u4f5c\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLM\u7f51\u7edc\u4ee3\u7406\u5728\u957f\u65f6\u5e8f\u4efb\u52a1\u4e2d\u7684\u4e0a\u4e0b\u6587\u7ba1\u7406\u6311\u6218\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2510.24120", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24120", "abs": "https://arxiv.org/abs/2510.24120", "authors": ["Ziyu Liu", "Yijing Liu", "Jianfei Yuan", "Minzhi Yan", "Le Yue", "Honghui Xiong", "Yi Yang"], "title": "Graph-Guided Concept Selection for Efficient Retrieval-Augmented Generation", "comment": null, "summary": "Graph-based RAG constructs a knowledge graph (KG) from text chunks to enhance\nretrieval in Large Language Model (LLM)-based question answering. It is\nespecially beneficial in domains such as biomedicine, law, and political\nscience, where effective retrieval often involves multi-hop reasoning over\nproprietary documents. However, these methods demand numerous LLM calls to\nextract entities and relations from text chunks, incurring prohibitive costs at\nscale. Through a carefully designed ablation study, we observe that certain\nwords (termed concepts) and their associated documents are more important.\nBased on this insight, we propose Graph-Guided Concept Selection (G2ConS). Its\ncore comprises a chunk selection method and an LLM-independent concept graph.\nThe former selects salient document chunks to reduce KG construction costs; the\nlatter closes knowledge gaps introduced by chunk selection at zero cost.\nEvaluations on multiple real-world datasets show that G2ConS outperforms all\nbaselines in construction cost, retrieval effectiveness, and answering quality.", "AI": {"tldr": "Graph-based RAG uses knowledge graphs for better retrieval in LLM-based QA, but is costly. G2ConS, a new method, uses concept importance and an LLM-independent concept graph to reduce costs and improve retrieval and answering quality.", "motivation": "Existing graph-based RAG methods are too expensive due to the need for numerous LLM calls to build knowledge graphs. However, graph-based RAG is beneficial for domains requiring multi-hop reasoning over proprietary documents, such as biomedicine, law, and political science.", "method": "G2ConS proposes a chunk selection method to reduce KG construction costs by identifying and selecting important document chunks (concepts). It also introduces an LLM-independent concept graph to fill knowledge gaps without incurring additional LLM costs.", "result": "G2ConS outperforms all baseline methods in terms of construction cost, retrieval effectiveness, and answering quality on multiple real-world datasets.", "conclusion": "G2ConS offers a more cost-effective and efficient solution for graph-based RAG by strategically selecting important concepts and leveraging an LLM-independent concept graph, leading to improved performance in LLM-based QA systems."}}
{"id": "2510.24125", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24125", "abs": "https://arxiv.org/abs/2510.24125", "authors": ["Kiran Bacsa", "Wei Liu", "Xudong Jian", "Huangbin Liang", "Eleni Chatzi"], "title": "Causal Convolutional Neural Networks as Finite Impulse Response Filters", "comment": "14 pages, 19 figures, Under review", "summary": "This study investigates the behavior of Causal Convolutional Neural Networks\n(CNNs) with quasi-linear activation functions when applied to time-series data\ncharacterized by multimodal frequency content. We demonstrate that, once\ntrained, such networks exhibit properties analogous to Finite Impulse Response\n(FIR) filters, particularly when the convolutional kernels are of extended\nlength exceeding those typically employed in standard CNN architectures. Causal\nCNNs are shown to capture spectral features both implicitly and explicitly,\noffering enhanced interpretability for tasks involving dynamic systems.\nLeveraging the associative property of convolution, we further show that the\nentire network can be reduced to an equivalent single-layer filter resembling\nan FIR filter optimized via least-squares criteria. This equivalence yields new\ninsights into the spectral learning behavior of CNNs trained on signals with\nsparse frequency content. The approach is validated on both simulated beam\ndynamics and real-world bridge vibration datasets, underlining its relevance\nfor modeling and identifying physical systems governed by dynamic responses.", "AI": {"tldr": "Causal CNNs with quasi-linear activations behave like FIR filters on time-series data, offering enhanced interpretability and spectral feature extraction, and can be reduced to a single equivalent filter.", "motivation": "The paper aims to investigate the behavior of Causal CNNs with quasi-linear activation functions on time-series data with multimodal frequency content, demonstrating their analogy to FIR filters and providing enhanced interpretability.", "method": "The study analyzes Causal CNNs, demonstrating their properties analogous to FIR filters, especially with extended kernel lengths. It leverages the associative property of convolution to reduce the network to an equivalent single-layer FIR filter optimized via least-squares criteria.", "result": "The research shows that trained Causal CNNs capture spectral features implicitly and explicitly, leading to enhanced interpretability. The equivalence to FIR filters provides insights into spectral learning for CNNs trained on signals with sparse frequency content.", "conclusion": "The approach is validated on simulated and real-world datasets, confirming its relevance for modeling and identifying physical systems governed by dynamic responses."}}
{"id": "2510.24702", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24702", "abs": "https://arxiv.org/abs/2510.24702", "authors": ["Yueqi Song", "Ketan Ramaneti", "Zaid Sheikh", "Ziru Chen", "Boyu Gou", "Tianbao Xie", "Yiheng Xu", "Danyang Zhang", "Apurva Gandhi", "Fan Yang", "Joseph Liu", "Tianyue Ou", "Zhihao Yuan", "Frank Xu", "Shuyan Zhou", "Xingyao Wang", "Xiang Yue", "Tao Yu", "Huan Sun", "Yu Su", "Graham Neubig"], "title": "Agent Data Protocol: Unifying Datasets for Diverse, Effective Fine-tuning of LLM Agents", "comment": null, "summary": "Public research results on large-scale supervised finetuning of AI agents\nremain relatively rare, since the collection of agent training data presents\nunique challenges. In this work, we argue that the bottleneck is not a lack of\nunderlying data sources, but that a large variety of data is fragmented across\nheterogeneous formats, tools, and interfaces. To this end, we introduce the\nagent data protocol (ADP), a light-weight representation language that serves\nas an \"interlingua\" between agent datasets in diverse formats and unified agent\ntraining pipelines downstream. The design of ADP is expressive enough to\ncapture a large variety of tasks, including API/tool use, browsing, coding,\nsoftware engineering, and general agentic workflows, while remaining simple to\nparse and train on without engineering at a per-dataset level. In experiments,\nwe unified a broad collection of 13 existing agent training datasets into ADP\nformat, and converted the standardized ADP data into training-ready formats for\nmultiple agent frameworks. We performed SFT on these data, and demonstrated an\naverage performance gain of ~20% over corresponding base models, and delivers\nstate-of-the-art or near-SOTA performance on standard coding, browsing, tool\nuse, and research benchmarks, without domain-specific tuning. All code and data\nare released publicly, in the hope that ADP could help lower the barrier to\nstandardized, scalable, and reproducible agent training.", "AI": {"tldr": "\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u4ee3\u7406\u7684\u5927\u89c4\u6a21\u76d1\u7763\u5fae\u8c03\u9762\u4e34\u6570\u636e\u6536\u96c6\u6311\u6218\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aADP\uff08Agent Data Protocol\uff09\u7684\u6570\u636e\u534f\u8bae\uff0c\u65e8\u5728\u89e3\u51b3\u6570\u636e\u788e\u7247\u5316\u95ee\u9898\uff0c\u5b9e\u73b0\u8de8\u591a\u79cd\u683c\u5f0f\u3001\u5de5\u5177\u548c\u63a5\u53e3\u7684\u6570\u636e\u7edf\u4e00\u3002ADP\u80fd\u591f\u8868\u793a\u591a\u79cd\u4efb\u52a1\uff0c\u5e76\u6613\u4e8e\u89e3\u6790\u548c\u8bad\u7ec3\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0c\u901a\u8fc7ADP\u7edf\u4e0013\u4e2a\u6570\u636e\u96c6\u5e76\u8fdb\u884c\u5fae\u8c03\uff0cAI\u4ee3\u7406\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u6027\u80fd\u5e73\u5747\u63d0\u5347\u7ea620%\uff0c\u8fbe\u5230\u6216\u63a5\u8fd1\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "motivation": "\u5927\u89c4\u6a21\u76d1\u7763\u5fae\u8c03AI\u4ee3\u7406\u7684\u7814\u7a76\u53d7\u9650\u4e8e\u6570\u636e\u6536\u96c6\u7684\u6311\u6218\uff0c\u4f5c\u8005\u8ba4\u4e3a\u74f6\u9888\u5728\u4e8e\u6570\u636e\u788e\u7247\u5316\u800c\u975e\u6570\u636e\u6e90\u7684\u7f3a\u4e4f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aADP\uff08Agent Data Protocol\uff09\u7684\u8f7b\u91cf\u7ea7\u8868\u793a\u8bed\u8a00\uff0c\u4f5c\u4e3a\u8fde\u63a5\u4e0d\u540c\u683c\u5f0f\u7684\u4ee3\u7406\u6570\u636e\u96c6\u548c\u7edf\u4e00\u7684\u4ee3\u7406\u8bad\u7ec3\u6d41\u7a0b\u7684\u201c\u901a\u7528\u8bed\u8a00\u201d\u3002ADP\u80fd\u591f\u8868\u793aAPI/\u5de5\u5177\u4f7f\u7528\u3001\u6d4f\u89c8\u3001\u7f16\u7801\u3001\u8f6f\u4ef6\u5de5\u7a0b\u548c\u901a\u7528\u4ee3\u7406\u5de5\u4f5c\u6d41\u7b49\u591a\u79cd\u4efb\u52a1\uff0c\u5e76\u4e14\u6613\u4e8e\u89e3\u6790\u548c\u8bad\u7ec3\u3002", "result": "\u5c0613\u4e2a\u73b0\u6709\u6570\u636e\u96c6\u7edf\u4e00\u4e3aADP\u683c\u5f0f\uff0c\u5e76\u8f6c\u6362\u4e3a\u591a\u79cd\u4ee3\u7406\u6846\u67b6\u7684\u8bad\u7ec3\u683c\u5f0f\u3002\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\uff0cAI\u4ee3\u7406\u7684\u5e73\u5747\u6027\u80fd\u63d0\u5347\u4e86\u7ea620%\uff0c\u5e76\u5728\u7f16\u7801\u3001\u6d4f\u89c8\u3001\u5de5\u5177\u4f7f\u7528\u548c\u7814\u7a76\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u6216\u63a5\u8fd1\u6700\u5148\u8fdb\u7684\u6c34\u5e73\uff0c\u4e14\u65e0\u9700\u8fdb\u884c\u9886\u57df\u7279\u5b9a\u7684\u8c03\u4f18\u3002", "conclusion": "ADP\u534f\u8bae\u80fd\u591f\u6709\u6548\u89e3\u51b3AI\u4ee3\u7406\u8bad\u7ec3\u6570\u636e\u7684\u788e\u7247\u5316\u95ee\u9898\uff0c\u964d\u4f4e\u4e86\u6807\u51c6\u5316\u3001\u53ef\u6269\u5c55\u548c\u53ef\u590d\u73b0\u7684\u4ee3\u7406\u8bad\u7ec3\u95e8\u69db\uff0c\u5e76\u663e\u8457\u63d0\u5347\u4e86AI\u4ee3\u7406\u7684\u6027\u80fd\u3002\u4f5c\u8005\u5df2\u516c\u5f00\u6240\u6709\u4ee3\u7801\u548c\u6570\u636e\u3002"}}
{"id": "2510.24331", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24331", "abs": "https://arxiv.org/abs/2510.24331", "authors": ["Gabriel O. dos Santos", "Esther Colombini", "Sandra Avila"], "title": "What do vision-language models see in the context? Investigating multimodal in-context learning", "comment": null, "summary": "In-context learning (ICL) enables Large Language Models (LLMs) to learn tasks\nfrom demonstration examples without parameter updates. Although it has been\nextensively studied in LLMs, its effectiveness in Vision-Language Models (VLMs)\nremains underexplored. In this work, we present a systematic study of ICL in\nVLMs, evaluating seven models spanning four architectures on three image\ncaptioning benchmarks. We analyze how prompt design, architectural choices, and\ntraining strategies influence multimodal ICL. To our knowledge, we are the\nfirst to analyze how attention patterns in VLMs vary with an increasing number\nof in-context demonstrations. Our results reveal that training on imag-text\ninterleaved data enhances ICL performance but does not imply effective\nintegration of visual and textual information from demonstration examples. In\ncontrast, instruction tuning improves instruction-following but can reduce\nreliance on in-context demonstrations, suggesting a trade-off between\ninstruction alignment and in-context adaptation. Attention analyses further\nshow that current VLMs primarily focus on textual cues and fail to leverage\nvisual information, suggesting a limited capacity for multimodal integration.\nThese findings highlight key limitations in the ICL abilities of current VLMs\nand provide insights for enhancing their ability to learn from multimodal\nin-context examples.", "AI": {"tldr": "ICL\u5728\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLMs)\u4e2d\u7684\u5e94\u7528\u6548\u679c\u6709\u5f85\u63a2\u7d22\u3002\u672c\u7814\u7a76\u7cfb\u7edf\u5730\u8bc4\u4f30\u4e86\u4e03\u79cd\u6a21\u578b\u5728\u4e09\u79cd\u56fe\u50cf\u63cf\u8ff0\u57fa\u51c6\u4e0a\u7684ICL\u8868\u73b0\uff0c\u5e76\u5206\u6790\u4e86\u63d0\u793a\u8bbe\u8ba1\u3001\u6a21\u578b\u67b6\u6784\u548c\u8bad\u7ec3\u7b56\u7565\u7684\u5f71\u54cd\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u56fe\u50cf-\u6587\u672c\u4ea4\u9519\u6570\u636e\u8bad\u7ec3\u80fd\u63d0\u5347ICL\u6027\u80fd\uff0c\u4f46\u5e76\u4e0d\u610f\u5473\u7740\u6709\u6548\u878d\u5408\u4e86\u6f14\u793a\u4e2d\u7684\u89c6\u89c9\u548c\u6587\u672c\u4fe1\u606f\u3002\u6307\u4ee4\u5fae\u8c03\u867d\u63d0\u9ad8\u4e86\u6307\u4ee4\u9075\u5faa\u80fd\u529b\uff0c\u4f46\u53ef\u80fd\u964d\u4f4e\u5bf9ICL\u7684\u4f9d\u8d56\u3002\u6ce8\u610f\u529b\u5206\u6790\u663e\u793a\uff0c\u5f53\u524dVLMs\u4e3b\u8981\u5173\u6ce8\u6587\u672c\u7ebf\u7d22\uff0c\u672a\u80fd\u6709\u6548\u5229\u7528\u89c6\u89c9\u4fe1\u606f\uff0c\u8868\u660e\u5176\u591a\u6a21\u6001\u878d\u5408\u80fd\u529b\u6709\u9650\u3002", "motivation": "ICL\u5728LLMs\u4e2d\u5f97\u5230\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46\u5728VLMs\u4e2d\u7684\u6709\u6548\u6027\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u672c\u7814\u7a76\u65e8\u5728\u7cfb\u7edf\u5730\u7814\u7a76ICL\u5728VLMs\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u5206\u6790\u5f71\u54cd\u5176\u6027\u80fd\u7684\u56e0\u7d20\u3002", "method": "\u5728\u4e09\u79cd\u56fe\u50cf\u63cf\u8ff0\u57fa\u51c6\u4e0a\u8bc4\u4f30\u4e03\u79cd\u6a21\u578b\uff08\u56db\u79cd\u67b6\u6784\uff09\u7684ICL\u8868\u73b0\u3002\u5206\u6790\u63d0\u793a\u8bbe\u8ba1\u3001\u6a21\u578b\u67b6\u6784\u548c\u8bad\u7ec3\u7b56\u7565\u5bf9\u591a\u6a21\u6001ICL\u7684\u5f71\u54cd\u3002\u5206\u6790\u6ce8\u610f\u529b\u6a21\u5f0f\u968fICL\u6f14\u793a\u6570\u91cf\u589e\u52a0\u7684\u53d8\u5316\u3002", "result": "\u8bad\u7ec3\u6570\u636e\u5305\u542b\u56fe\u50cf-\u6587\u672c\u4ea4\u9519\u6570\u636e\u80fd\u63d0\u5347ICL\u6027\u80fd\uff0c\u4f46\u5e76\u4e0d\u610f\u5473\u7740\u80fd\u6709\u6548\u878d\u5408\u6f14\u793a\u4e2d\u7684\u89c6\u89c9\u548c\u6587\u672c\u4fe1\u606f\u3002\u6307\u4ee4\u5fae\u8c03\u80fd\u63d0\u5347\u6307\u4ee4\u9075\u5faa\u80fd\u529b\uff0c\u4f46\u53ef\u80fd\u964d\u4f4e\u5bf9ICL\u6f14\u793a\u7684\u4f9d\u8d56\u3002\u6ce8\u610f\u529b\u5206\u6790\u663e\u793a\uff0c\u5f53\u524dVLMs\u4e3b\u8981\u5173\u6ce8\u6587\u672c\u7ebf\u7d22\uff0c\u672a\u80fd\u6709\u6548\u5229\u7528\u89c6\u89c9\u4fe1\u606f\u3002", "conclusion": "\u5f53\u524dVLMs\u5728\u591a\u6a21\u6001ICL\u80fd\u529b\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u4e3b\u8981\u4f53\u73b0\u5728\u672a\u80fd\u6709\u6548\u878d\u5408\u6f14\u793a\u4e2d\u7684\u89c6\u89c9\u4fe1\u606f\u3002\u7814\u7a76\u7ed3\u679c\u4e3a\u63d0\u5347VLMs\u4ece\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u793a\u4f8b\u4e2d\u5b66\u4e60\u7684\u80fd\u529b\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002"}}
{"id": "2510.24135", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24135", "abs": "https://arxiv.org/abs/2510.24135", "authors": ["Hojin Cheon", "Hyeongseok Seo", "Jihun Jeon", "Wooju Lee", "Dohyun Jeong", "Hongseok Kim"], "title": "Fixed Point Neural Acceleration and Inverse Surrogate Model for Battery Parameter Identification", "comment": "31 pages, 11 figures, submitted to Applied Energy", "summary": "The rapid expansion of electric vehicles has intensified the need for\naccurate and efficient diagnosis of lithium-ion batteries. Parameter\nidentification of electrochemical battery models is widely recognized as a\npowerful method for battery health assessment. However, conventional\nmetaheuristic approaches suffer from high computational cost and slow\nconvergence, and recent machine learning methods are limited by their reliance\non constant current data, which may not be available in practice. To overcome\nthese challenges, we propose deep learning-based framework for parameter\nidentification of electrochemical battery models. The proposed framework\ncombines a neural surrogate model of the single particle model with electrolyte\n(NeuralSPMe) and a deep learning-based fixed-point iteration method. NeuralSPMe\nis trained on realistic EV load profiles to accurately predict lithium\nconcentration dynamics under dynamic operating conditions while a parameter\nupdate network (PUNet) performs fixed-point iterative updates to significantly\nreduce both the evaluation time per sample and the overall number of iterations\nrequired for convergence. Experimental evaluations demonstrate that the\nproposed framework accelerates the parameter identification by more than 2000\ntimes, achieves superior sample efficiency and more than 10 times higher\naccuracy compared to conventional metaheuristic algorithms, particularly under\ndynamic load scenarios encountered in practical applications.", "AI": {"tldr": "\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u795e\u7ecf\u4ee3\u7406\u6a21\u578b\u548c\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u56fa\u5b9a\u70b9\u8fed\u4ee3\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u9502\u79bb\u5b50\u7535\u6c60\u6a21\u578b\u53c2\u6570\u8bc6\u522b\u7684\u901f\u5ea6\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u7535\u52a8\u6c7d\u8f66\u7684\u5feb\u901f\u53d1\u5c55\u9700\u8981\u7cbe\u786e\u9ad8\u6548\u7684\u9502\u79bb\u5b50\u7535\u6c60\u8bca\u65ad\u65b9\u6cd5\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u6536\u655b\u6162\uff0c\u800c\u73b0\u6709\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u53ef\u80fd\u4e0d\u53ef\u7528\u7684\u6052\u6d41\u6570\u636e\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u5305\u62ec\u7528\u4e8e\u9884\u6d4b\u9502\u79bb\u5b50\u6d53\u5ea6\u52a8\u529b\u5b66\u7684\u795e\u7ecf\u4ee3\u7406\u6a21\u578b\uff08NeuralSPMe\uff09\u548c\u7528\u4e8e\u56fa\u5b9a\u70b9\u8fed\u4ee3\u66f4\u65b0\u7684\u53c2\u6570\u66f4\u65b0\u7f51\u7edc\uff08PUNet\uff09\u3002", "result": "\u4e0e\u4f20\u7edf\u5143\u542f\u53d1\u5f0f\u7b97\u6cd5\u76f8\u6bd4\uff0c\u8be5\u6846\u67b6\u5c06\u53c2\u6570\u8bc6\u522b\u901f\u5ea6\u63d0\u9ad8\u4e862000\u500d\u4ee5\u4e0a\uff0c\u6837\u672c\u6548\u7387\u66f4\u9ad8\uff0c\u51c6\u786e\u6027\u63d0\u9ad8\u4e8610\u500d\u4ee5\u4e0a\uff0c\u5c24\u5176\u662f\u5728\u52a8\u6001\u8d1f\u8f7d\u6761\u4ef6\u4e0b\u3002", "conclusion": "\u8be5\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u80fd\u591f\u6709\u6548\u514b\u670d\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u7535\u52a8\u6c7d\u8f66\u7684\u9502\u79bb\u5b50\u7535\u6c60\u53c2\u6570\u8bc6\u522b\u63d0\u4f9b\u4e86\u4e00\u79cd\u5feb\u901f\u3001\u51c6\u786e\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.24706", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.24706", "abs": "https://arxiv.org/abs/2510.24706", "authors": ["Shuqing Li", "Jiayi Yan", "Chenyu Niu", "Jen-tse Huang", "Yun Peng", "Wenxuan Wang", "Yepang Liu", "Michael R. Lyu"], "title": "ComboBench: Can LLMs Manipulate Physical Devices to Play Virtual Reality Games?", "comment": null, "summary": "Virtual Reality (VR) games require players to translate high-level semantic\nactions into precise device manipulations using controllers and head-mounted\ndisplays (HMDs). While humans intuitively perform this translation based on\ncommon sense and embodied understanding, whether Large Language Models (LLMs)\ncan effectively replicate this ability remains underexplored. This paper\nintroduces a benchmark, ComboBench, evaluating LLMs' capability to translate\nsemantic actions into VR device manipulation sequences across 262 scenarios\nfrom four popular VR games: Half-Life: Alyx, Into the Radius, Moss: Book II,\nand Vivecraft. We evaluate seven LLMs, including GPT-3.5, GPT-4, GPT-4o,\nGemini-1.5-Pro, LLaMA-3-8B, Mixtral-8x7B, and GLM-4-Flash, compared against\nannotated ground truth and human performance. Our results reveal that while\ntop-performing models like Gemini-1.5-Pro demonstrate strong task decomposition\ncapabilities, they still struggle with procedural reasoning and spatial\nunderstanding compared to humans. Performance varies significantly across\ngames, suggesting sensitivity to interaction complexity. Few-shot examples\nsubstantially improve performance, indicating potential for targeted\nenhancement of LLMs' VR manipulation capabilities. We release all materials at\nhttps://sites.google.com/view/combobench.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aComboBench\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5c06\u8bed\u4e49\u52a8\u4f5c\u8f6c\u6362\u4e3a\u865a\u62df\u73b0\u5b9e\uff08VR\uff09\u8bbe\u5907\u64cd\u4f5c\u5e8f\u5217\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u5c3d\u7ba1\u9876\u5c16\u6a21\u578b\u8868\u73b0\u51fa\u4e00\u5b9a\u7684\u80fd\u529b\uff0c\u4f46\u4e0e\u4eba\u7c7b\u76f8\u6bd4\u4ecd\u5b58\u5728\u5dee\u8ddd\uff0c\u4e14\u6a21\u578b\u6027\u80fd\u53d7\u6e38\u620f\u4ea4\u4e92\u590d\u6742\u6027\u5f71\u54cd\uff0c\u4f46\u53ef\u901a\u8fc7\u5c11\u6837\u672c\u793a\u4f8b\u5f97\u5230\u63d0\u5347\u3002", "motivation": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5c06\u9ad8\u5c42\u8bed\u4e49\u52a8\u4f5c\u8f6c\u5316\u4e3a\u865a\u62df\u73b0\u5b9e\uff08VR\uff09\u8bbe\u5907\u7cbe\u786e\u64cd\u4f5c\u7684\u80fd\u529b\uff0c\u4ee5\u53ca\u4e0e\u4eba\u7c7b\u8868\u73b0\u7684\u5bf9\u6bd4\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b262\u4e2a\u573a\u666f\u7684\u57fa\u51c6\u6d4b\u8bd5ComboBench\uff0c\u6db5\u76d6\u4e86\u56db\u6b3e\u6d41\u884c\u7684VR\u6e38\u620f\uff0c\u5e76\u5bf9\u4e03\u79cd\u4e0d\u540c\u7684LLMs\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5c06\u5176\u8868\u73b0\u4e0e\u6807\u6ce8\u7684\u771f\u5b9e\u6570\u636e\u548c\u4eba\u7c7b\u7684\u5e73\u5747\u8868\u73b0\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "result": "\u9876\u5c16\u6a21\u578b\uff08\u5982Gemini-1.5-Pro\uff09\u5728\u4efb\u52a1\u5206\u89e3\u65b9\u9762\u8868\u73b0\u8f83\u597d\uff0c\u4f46\u5728\u7a0b\u5e8f\u63a8\u7406\u548c\u7a7a\u95f4\u7406\u89e3\u65b9\u9762\u4e0d\u5982\u4eba\u7c7b\u3002\u4e0d\u540c\u6e38\u620f\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u5f02\u663e\u8457\uff0c\u8868\u660e\u6a21\u578b\u5bf9\u4ea4\u4e92\u590d\u6742\u6027\u654f\u611f\u3002\u5c11\u6837\u672c\u793a\u4f8b\u7684\u5b66\u4e60\u65b9\u5f0f\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "LLMs\u5728\u7406\u89e3\u548c\u6267\u884cVR\u8bbe\u5907\u64cd\u4f5c\u65b9\u9762\u6709\u6f5c\u529b\uff0c\u4f46\u4ecd\u9700\u5728\u7a0b\u5e8f\u63a8\u7406\u548c\u7a7a\u95f4\u7406\u89e3\u65b9\u9762\u8fdb\u884c\u6539\u8fdb\uff0c\u4ee5\u8fbe\u5230\u6216\u8d85\u8d8a\u4eba\u7c7b\u6c34\u5e73\u3002\u7814\u7a76\u7ed3\u679c\u4e3a\u672a\u6765LLMs\u5728VR\u4ea4\u4e92\u9886\u57df\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2510.24160", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24160", "abs": "https://arxiv.org/abs/2510.24160", "authors": ["Aiqing Zhu", "Beatrice W. Soh", "Grigorios A. Pavliotis", "Qianxiao Li"], "title": "Identifiable learning of dissipative dynamics", "comment": null, "summary": "Complex dissipative systems appear across science and engineering, from\npolymers and active matter to learning algorithms. These systems operate far\nfrom equilibrium, where energy dissipation and time irreversibility are key to\ntheir behavior, but are difficult to quantify from data. Learning accurate and\ninterpretable models of such dynamics remains a major challenge: the models\nmust be expressive enough to describe diverse processes, yet constrained enough\nto remain physically meaningful and mathematically identifiable. Here, we\nintroduce I-OnsagerNet, a neural framework that learns dissipative stochastic\ndynamics directly from trajectories while ensuring both interpretability and\nuniqueness. I-OnsagerNet extends the Onsager principle to guarantee that the\nlearned potential is obtained from the stationary density and that the drift\ndecomposes cleanly into time-reversible and time-irreversible components, as\ndictated by the Helmholtz decomposition. Our approach enables us to calculate\nthe entropy production and to quantify irreversibility, offering a principled\nway to detect and quantify deviations from equilibrium. Applications to polymer\nstretching in elongational flow and to stochastic gradient Langevin dynamics\nreveal new insights, including super-linear scaling of barrier heights and\nsub-linear scaling of entropy production rates with the strain rate, and the\nsuppression of irreversibility with increasing batch size. I-OnsagerNet thus\nestablishes a general, data-driven framework for discovering and interpreting\nnon-equilibrium dynamics.", "AI": {"tldr": "I-OnsagerNet\u662f\u4e00\u4e2a\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u6846\u67b6\uff0c\u53ef\u4ee5\u76f4\u63a5\u4ece\u8f68\u8ff9\u6570\u636e\u4e2d\u5b66\u4e60\u8017\u6563\u968f\u673a\u52a8\u529b\u5b66\uff0c\u5e76\u4fdd\u8bc1\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u552f\u4e00\u6027\u3002\u5b83\u901a\u8fc7\u6269\u5c55Onsager\u539f\u7406\uff0c\u4ece\u7a33\u6001\u5bc6\u5ea6\u4e2d\u5b66\u4e60\u52bf\u80fd\uff0c\u5e76\u5c06\u6f02\u79fb\u5206\u89e3\u4e3a\u65f6\u95f4\u53ef\u9006\u548c\u65f6\u95f4\u4e0d\u53ef\u9006\u5206\u91cf\uff0c\u4ece\u800c\u5b9e\u73b0\u5bf9\u71b5\u4ea7\u751f\u548c\u4e0d\u53ef\u9006\u6027\u7684\u91cf\u5316\u3002", "motivation": "\u5b66\u4e60\u7cbe\u786e\u4e14\u53ef\u89e3\u91ca\u7684\u8017\u6563\u52a8\u529b\u5b66\u6a21\u578b\u662f\u4e00\u4e2a\u6311\u6218\uff0c\u73b0\u6709\u6a21\u578b\u9700\u8981\u8db3\u591f\u8868\u8fbe\u591a\u6837\u5316\u8fc7\u7a0b\uff0c\u53c8\u8981\u6ee1\u8db3\u7269\u7406\u610f\u4e49\u548c\u6570\u5b66\u53ef\u8bc6\u522b\u6027\u3002", "method": "I-OnsagerNet\u5c06Onsager\u539f\u7406\u6269\u5c55\u5230\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\uff0c\u5229\u7528Helmholtz\u5206\u89e3\u5c06\u6f02\u79fb\u6e05\u6670\u5730\u5206\u89e3\u4e3a\u65f6\u95f4\u53ef\u9006\u548c\u65f6\u95f4\u4e0d\u53ef\u9006\u5206\u91cf\uff0c\u4ece\u800c\u76f4\u63a5\u4ece\u8f68\u8ff9\u6570\u636e\u4e2d\u5b66\u4e60\u8017\u6563\u968f\u673a\u52a8\u529b\u5b66\u3002", "result": "\u5728\u805a\u5408\u7269\u62c9\u4f38\u548c\u968f\u673a\u68af\u5ea6Langevin\u52a8\u529b\u5b66\u7b49\u5e94\u7528\u4e2d\uff0cI-OnsagerNet\u63ed\u793a\u4e86\u65b0\u7684\u89c1\u89e3\uff0c\u5305\u62ec\u52bf\u5792\u9ad8\u5ea6\u4e0e\u5e94\u53d8\u7387\u7684\u8d85\u7ebf\u6027\u6807\u5ea6\u3001\u71b5\u4ea7\u751f\u7387\u4e0e\u5e94\u53d8\u7387\u7684\u6b21\u7ebf\u6027\u6807\u5ea6\uff0c\u4ee5\u53ca\u968f\u7740\u6279\u6b21\u5927\u5c0f\u589e\u52a0\u800c\u5f15\u8d77\u7684\u4e0d\u53ef\u9006\u6027\u6291\u5236\u3002", "conclusion": "I-OnsagerNet\u63d0\u4f9b\u4e86\u4e00\u4e2a\u901a\u7528\u3001\u6570\u636e\u9a71\u52a8\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u53d1\u73b0\u548c\u89e3\u91ca\u975e\u5e73\u8861\u52a8\u529b\u5b66\u3002"}}
{"id": "2510.24707", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24707", "abs": "https://arxiv.org/abs/2510.24707", "authors": ["Juraj Juraska", "Tobias Domhan", "Mara Finkelstein", "Tetsuji Nakagawa", "Geza Kovacs", "Daniel Deutsch", "Pidong Wang", "Markus Freitag"], "title": "MetricX-25 and GemSpanEval: Google Translate Submissions to the WMT25 Evaluation Shared Task", "comment": "Accepted to WMT25", "summary": "In this paper, we present our submissions to the unified WMT25 Translation\nEvaluation Shared Task. For the Quality Score Prediction subtask, we create a\nnew generation of MetricX with improvements in the input format and the\ntraining protocol, while for the Error Span Detection subtask we develop a new\nmodel, GemSpanEval, trained to predict error spans along with their severities\nand categories. Both systems are based on the state-of-the-art multilingual\nopen-weights model Gemma 3, fine-tuned on publicly available WMT data. We\ndemonstrate that MetricX-25, adapting Gemma 3 to an encoder-only architecture\nwith a regression head on top, can be trained to effectively predict both MQM\nand ESA quality scores, and significantly outperforms its predecessor. Our\ndecoder-only GemSpanEval model, on the other hand, we show to be competitive in\nerror span detection with xCOMET, a strong encoder-only sequence-tagging\nbaseline. With error span detection formulated as a generative task, we\ninstruct the model to also output the context for each predicted error span,\nthus ensuring that error spans are identified unambiguously.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u7684\u673a\u5668\u7ffb\u8bd1\u8d28\u91cf\u8bc4\u4f30\u6a21\u578b\uff1aMetricX-25\u7528\u4e8e\u8d28\u91cf\u5206\u6570\u9884\u6d4b\uff0cGemSpanEval\u7528\u4e8e\u9519\u8bef\u8de8\u5ea6\u68c0\u6d4b\u3002\u4e24\u79cd\u6a21\u578b\u5747\u57fa\u4e8eGemma 3\u8fdb\u884c\u5fae\u8c03\uff0c\u5e76\u5728WMT\u6570\u636e\u4e0a\u8fdb\u884c\u4e86\u8bad\u7ec3\u3002MetricX-25\u5728\u8d28\u91cf\u5206\u6570\u9884\u6d4b\u4e0a\u4f18\u4e8e\u524d\u4ee3\u6a21\u578b\uff0cGemSpanEval\u5728\u9519\u8bef\u8de8\u5ea6\u68c0\u6d4b\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u80fd\u751f\u6210\u9519\u8bef\u4e0a\u4e0b\u6587\u4ee5\u786e\u4fdd\u6b67\u4e49\u6027\u3002", "motivation": "\u5f00\u53d1\u7528\u4e8eWMT25\u7ffb\u8bd1\u8bc4\u4f30\u5171\u4eab\u4efb\u52a1\u7684\u65b0\u4e00\u4ee3\u673a\u5668\u7ffb\u8bd1\u8d28\u91cf\u8bc4\u4f30\u5de5\u5177\uff0c\u4ee5\u63d0\u9ad8\u7ffb\u8bd1\u8d28\u91cf\u8bc4\u4f30\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "method": "MetricX-25: \u57fa\u4e8eGemma 3\u7684\u591a\u8bed\u8a00\u6a21\u578b\uff0c\u91c7\u7528\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\uff0c\u5728\u56de\u5f52\u4efb\u52a1\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u7528\u4e8e\u9884\u6d4bMQM\u548cESA\u8d28\u91cf\u5206\u6570\u3002GemSpanEval: \u57fa\u4e8eGemma 3\u7684\u89e3\u7801\u5668-only\u6a21\u578b\uff0c\u5c06\u9519\u8bef\u8de8\u5ea6\u68c0\u6d4b\u89c6\u4e3a\u751f\u6210\u4efb\u52a1\uff0c\u540c\u65f6\u9884\u6d4b\u9519\u8bef\u8de8\u5ea6\u3001\u4e25\u91cd\u6027\u548c\u7c7b\u522b\uff0c\u5e76\u751f\u6210\u9519\u8bef\u4e0a\u4e0b\u6587\u3002", "result": "MetricX-25\u5728\u8d28\u91cf\u5206\u6570\u9884\u6d4b\u4e0a\u663e\u8457\u4f18\u4e8e\u5176\u524d\u4ee3\u6a21\u578b\u3002GemSpanEval\u5728\u9519\u8bef\u8de8\u5ea6\u68c0\u6d4b\u4efb\u52a1\u4e0a\u4e0e\u5f3a\u5927\u7684\u7f16\u7801\u5668-only\u5e8f\u5217\u6807\u6ce8\u57fa\u7ebf\u6a21\u578bxCOMET\u5177\u6709\u7ade\u4e89\u529b\u3002", "conclusion": "MetricX-25\u548cGemSpanEval\u5728WMT25\u7ffb\u8bd1\u8bc4\u4f30\u5171\u4eab\u4efb\u52a1\u4e2d\u5c55\u73b0\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u57fa\u4e8eGemma 3\u5fae\u8c03\u7684\u6a21\u578b\u5728\u673a\u5668\u7ffb\u8bd1\u8d28\u91cf\u8bc4\u4f30\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.24180", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24180", "abs": "https://arxiv.org/abs/2510.24180", "authors": ["Arpita Kundu", "Joyita Chakraborty", "Anindita Desarkar", "Aritra Sen", "Srushti Anil Patil", "Vishwanathan Raman"], "title": "V-SAT: Video Subtitle Annotation Tool", "comment": null, "summary": "The surge of audiovisual content on streaming platforms and social media has\nheightened the demand for accurate and accessible subtitles. However, existing\nsubtitle generation methods primarily speech-based transcription or OCR-based\nextraction suffer from several shortcomings, including poor synchronization,\nincorrect or harmful text, inconsistent formatting, inappropriate reading\nspeeds, and the inability to adapt to dynamic audio-visual contexts. Current\napproaches often address isolated issues, leaving post-editing as a\nlabor-intensive and time-consuming process. In this paper, we introduce V-SAT\n(Video Subtitle Annotation Tool), a unified framework that automatically\ndetects and corrects a wide range of subtitle quality issues. By combining\nLarge Language Models(LLMs), Vision-Language Models (VLMs), Image Processing,\nand Automatic Speech Recognition (ASR), V-SAT leverages contextual cues from\nboth audio and video. Subtitle quality improved, with the SUBER score reduced\nfrom 9.6 to 3.54 after resolving all language mode issues and F1-scores of\n~0.80 for image mode issues. Human-in-the-loop validation ensures high-quality\nresults, providing the first comprehensive solution for robust subtitle\nannotation.", "AI": {"tldr": "V-SAT\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408LLM\u3001VLM\u3001\u56fe\u50cf\u5904\u7406\u548cASR\u6765\u81ea\u52a8\u68c0\u6d4b\u548c\u7ea0\u6b63\u5b57\u5e55\u8d28\u91cf\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5b57\u5e55\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u5b57\u5e55\u751f\u6210\u65b9\u6cd5\u5728\u540c\u6b65\u3001\u6587\u672c\u51c6\u786e\u6027\u3001\u683c\u5f0f\u3001\u9605\u8bfb\u901f\u5ea6\u548c\u9002\u5e94\u52a8\u6001\u89c6\u542c\u4e0a\u4e0b\u6587\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5bfc\u81f4\u5b57\u5e55\u540e\u671f\u7f16\u8f91\u8017\u65f6\u8017\u529b\u3002", "method": "V-SAT\u7ed3\u5408\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u3001\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u3001\u56fe\u50cf\u5904\u7406\u548c\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\uff0c\u5229\u7528\u97f3\u9891\u548c\u89c6\u9891\u7684\u4e0a\u4e0b\u6587\u7ebf\u7d22\u6765\u68c0\u6d4b\u548c\u7ea0\u6b63\u5b57\u5e55\u8d28\u91cf\u95ee\u9898\u3002", "result": "V-SAT\u5c06SUBER\u5206\u6570\u4ece9.6\u964d\u4f4e\u52303.54\uff0c\u89e3\u51b3\u4e86\u6240\u6709\u8bed\u8a00\u6a21\u5f0f\u95ee\u9898\uff0c\u5e76\u5c06\u56fe\u50cf\u6a21\u5f0f\u95ee\u9898\u7684F1\u5206\u6570\u63d0\u9ad8\u5230\u7ea60.80\u3002", "conclusion": "V-SAT\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7528\u4e8e\u9c81\u68d2\u7684\u5b57\u5e55\u6807\u6ce8\uff0c\u901a\u8fc7\u7ed3\u5408\u591a\u79cd\u6280\u672f\u5e76\u8fdb\u884c\u4eba\u5de5\u9a8c\u8bc1\u6765\u786e\u4fdd\u9ad8\u8d28\u91cf\u7684\u5b57\u5e55\u3002"}}
{"id": "2510.24216", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24216", "abs": "https://arxiv.org/abs/2510.24216", "authors": ["Fan Xu", "Hao Wu", "Kun Wang", "Nan Wang", "Qingsong Wen", "Xian Wu", "Wei Gong", "Xibin Zhao"], "title": "Unlocking Out-of-Distribution Generalization in Dynamics through Physics-Guided Augmentation", "comment": null, "summary": "In dynamical system modeling, traditional numerical methods are limited by\nhigh computational costs, while modern data-driven approaches struggle with\ndata scarcity and distribution shifts. To address these fundamental\nlimitations, we first propose SPARK, a physics-guided quantitative augmentation\nplugin. Specifically, SPARK utilizes a reconstruction autoencoder to integrate\nphysical parameters into a physics-rich discrete state dictionary. This state\ndictionary then acts as a structured dictionary of physical states, enabling\nthe creation of new, physically-plausible training samples via principled\ninterpolation in the latent space. Further, for downstream prediction, these\naugmented representations are seamlessly integrated with a Fourier-enhanced\nGraph ODE, a combination designed to robustly model the enriched data\ndistribution while capturing long-term temporal dependencies. Extensive\nexperiments on diverse benchmarks demonstrate that SPARK significantly\noutperforms state-of-the-art baselines, particularly in challenging\nout-of-distribution scenarios and data-scarce regimes, proving the efficacy of\nour physics-guided augmentation paradigm.", "AI": {"tldr": "SPARK\u901a\u8fc7\u7269\u7406\u5f15\u5bfc\u7684\u6570\u636e\u589e\u5f3a\u548c\u5085\u91cc\u53f6\u589e\u5f3a\u56feODE\u6765\u89e3\u51b3\u52a8\u529b\u5b66\u7cfb\u7edf\u5efa\u6a21\u4e2d\u7684\u6570\u636e\u7a00\u758f\u6027\u548c\u8ba1\u7b97\u6210\u672c\u95ee\u9898\uff0c\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u5206\u5e03\u5916\u548c\u6570\u636e\u7a00\u758f\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u6570\u503c\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u5728\u6570\u636e\u7a00\u758f\u548c\u5206\u5e03\u504f\u79fb\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u672c\u7814\u7a76\u65e8\u5728\u514b\u670d\u8fd9\u4e9b\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51faSPARK\uff0c\u4e00\u79cd\u7269\u7406\u5f15\u5bfc\u7684\u5b9a\u91cf\u589e\u5f3a\u63d2\u4ef6\uff0c\u5229\u7528\u91cd\u5efa\u81ea\u7f16\u7801\u5668\u5c06\u7269\u7406\u53c2\u6570\u6574\u5408\u5230\u7269\u7406\u4fe1\u606f\u4e30\u5bcc\u7684\u79bb\u6563\u72b6\u6001\u5b57\u5178\u4e2d\uff0c\u5e76\u901a\u8fc7\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u63d2\u503c\u751f\u6210\u65b0\u7684\u3001\u7269\u7406\u4e0a\u5408\u7406\u7684\u8bad\u7ec3\u6837\u672c\u3002\u7136\u540e\u5c06\u589e\u5f3a\u540e\u7684\u8868\u793a\u4e0e\u5085\u91cc\u53f6\u589e\u5f3a\u56feODE\u76f8\u7ed3\u5408\uff0c\u4ee5\u5bf9\u4e30\u5bcc\u7684\u6570\u636e\u5206\u5e03\u8fdb\u884c\u5efa\u6a21\u5e76\u6355\u6349\u957f\u671f\u65f6\u95f4\u4f9d\u8d56\u6027\u3002", "result": "SPARK\u5728\u5404\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u5206\u5e03\u5916\u573a\u666f\u548c\u6570\u636e\u7a00\u758f\u7684\u60c5\u51b5\u4e0b\u3002", "conclusion": "SPARK\u7684\u7269\u7406\u5f15\u5bfc\u589e\u5f3a\u8303\u5f0f\u88ab\u8bc1\u660e\u662f\u6709\u6548\u7684\uff0c\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u52a8\u529b\u5b66\u7cfb\u7edf\u5efa\u6a21\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u6709\u9650\u548c\u5206\u5e03\u53d8\u5316\u7684\u60c5\u51b5\u4e0b\u3002"}}
{"id": "2510.24217", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24217", "abs": "https://arxiv.org/abs/2510.24217", "authors": ["Alisher Turubayev", "Anna Shopova", "Fabian Lange", "Mahmut Kamalak", "Paul Mattes", "Victoria Ayvasky", "Bert Arnrich", "Bjarne Pfitzner", "Robin P. van de Water"], "title": "Closing Gaps: An Imputation Analysis of ICU Vital Signs", "comment": "Preprint", "summary": "As more Intensive Care Unit (ICU) data becomes available, the interest in\ndeveloping clinical prediction models to improve healthcare protocols\nincreases. However, the lack of data quality still hinders clinical prediction\nusing Machine Learning (ML). Many vital sign measurements, such as heart rate,\ncontain sizeable missing segments, leaving gaps in the data that could\nnegatively impact prediction performance. Previous works have introduced\nnumerous time-series imputation techniques. Nevertheless, more comprehensive\nwork is needed to compare a representative set of methods for imputing ICU\nvital signs and determine the best practice. In reality, ad-hoc imputation\ntechniques that could decrease prediction accuracy, like zero imputation, are\nstill used. In this work, we compare established imputation techniques to guide\nresearchers in improving the performance of clinical prediction models by\nselecting the most accurate imputation technique. We introduce an extensible\nand reusable benchmark with currently 15 imputation and 4 amputation methods,\ncreated for benchmarking on major ICU datasets. We hope to provide a\ncomparative basis and facilitate further ML development to bring more models\ninto clinical practice.", "AI": {"tldr": "ICU\u6570\u636e\u8d28\u91cf\u5dee\uff0c\u7279\u522b\u662f\u751f\u547d\u4f53\u5f81\u6570\u636e\u7f3a\u5931\uff0c\u963b\u788d\u4e86\u673a\u5668\u5b66\u4e60\u5728\u4e34\u5e8a\u9884\u6d4b\u4e2d\u7684\u5e94\u7528\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b15\u79cd\u63d2\u8865\u65b9\u6cd5\u548c4\u79cd\u622a\u65ad\u65b9\u6cd5\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4ee5\u6bd4\u8f83\u4e0d\u540c\u63d2\u8865\u6280\u672f\u5728ICU\u751f\u547d\u4f53\u5f81\u6570\u636e\u4e0a\u7684\u8868\u73b0\uff0c\u65e8\u5728\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u9009\u62e9\u6700\u4f73\u63d2\u8865\u65b9\u6cd5\u7684\u6307\u5bfc\uff0c\u4ee5\u63d0\u9ad8\u4e34\u5e8a\u9884\u6d4b\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "ICU\u6570\u636e\u8d28\u91cf\u5dee\uff0c\u5c24\u5176\u662f\u751f\u547d\u4f53\u5f81\u6570\u636e\u7f3a\u5931\uff0c\u963b\u788d\u4e86\u673a\u5668\u5b66\u4e60\u5728\u4e34\u5e8a\u9884\u6d4b\u4e2d\u7684\u5e94\u7528\u3002\u73b0\u6709\u7814\u7a76\u9700\u8981\u66f4\u5168\u9762\u7684\u5de5\u4f5c\u6765\u6bd4\u8f83\u4e0d\u540c\u7684\u63d2\u8865\u65b9\u6cd5\uff0c\u5e76\u786e\u5b9a\u6700\u4f73\u5b9e\u8df5\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u3001\u53ef\u91cd\u7528\u7684\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u5305\u542b15\u79cd\u63d2\u8865\u65b9\u6cd5\u548c4\u79cd\u622a\u65ad\u65b9\u6cd5\uff0c\u5e76\u8ba1\u5212\u5728\u4e3b\u8981\u7684ICU\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u76ee\u524d\u5df2\u5305\u542b15\u79cd\u63d2\u8865\u65b9\u6cd5\u548c4\u79cd\u622a\u65ad\u65b9\u6cd5\uff0c\u65e8\u5728\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u6bd4\u8f83\u57fa\u7840\uff0c\u4ee5\u9009\u62e9\u6700\u51c6\u786e\u7684\u63d2\u8865\u6280\u672f\u3002", "conclusion": "\u901a\u8fc7\u6bd4\u8f83\u4e0d\u540c\u7684\u63d2\u8865\u6280\u672f\uff0c\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u9009\u62e9\u6700\u4f73\u63d2\u8865\u65b9\u6cd5\u7684\u6307\u5bfc\uff0c\u4ee5\u63d0\u9ad8\u4e34\u5e8a\u9884\u6d4b\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5e76\u4fc3\u8fdb\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u4e34\u5e8a\u5b9e\u8df5\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2510.24233", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24233", "abs": "https://arxiv.org/abs/2510.24233", "authors": ["Antoine Szatkownik", "Aur\u00e9lien Decelle", "Beatriz Seoane", "Nicolas Bereux", "L\u00e9o Planche", "Guillaume Charpiat", "Burak Yelmen", "Flora Jay", "Cyril Furtlehner"], "title": "PRIVET: Privacy Metric Based on Extreme Value Theory", "comment": null, "summary": "Deep generative models are often trained on sensitive data, such as genetic\nsequences, health data, or more broadly, any copyrighted, licensed or protected\ncontent. This raises critical concerns around privacy-preserving synthetic\ndata, and more specifically around privacy leakage, an issue closely tied to\noverfitting. Existing methods almost exclusively rely on global criteria to\nestimate the risk of privacy failure associated to a model, offering only\nquantitative non interpretable insights. The absence of rigorous evaluation\nmethods for data privacy at the sample-level may hinder the practical\ndeployment of synthetic data in real-world applications. Using extreme value\nstatistics on nearest-neighbor distances, we propose PRIVET, a generic\nsample-based, modality-agnostic algorithm that assigns an individual privacy\nleak score to each synthetic sample. We empirically demonstrate that PRIVET\nreliably detects instances of memorization and privacy leakage across diverse\ndata modalities, including settings with very high dimensionality, limited\nsample sizes such as genetic data and even under underfitting regimes. We\ncompare our method to existing approaches under controlled settings and show\nits advantage in providing both dataset level and sample level assessments\nthrough qualitative and quantitative outputs. Additionally, our analysis\nreveals limitations in existing computer vision embeddings to yield\nperceptually meaningful distances when identifying near-duplicate samples.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPRIVET\u7684\u7b97\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5408\u6210\u6570\u636e\u7684\u9690\u79c1\u6cc4\u9732\u98ce\u9669\u3002\u8be5\u7b97\u6cd5\u80fd\u591f\u4e3a\u6bcf\u4e2a\u5408\u6210\u6837\u672c\u5206\u914d\u4e00\u4e2a\u9690\u79c1\u6cc4\u9732\u5206\u6570\uff0c\u4ece\u800c\u5b9e\u73b0\u6837\u672c\u7ea7\u522b\u7684\u9690\u79c1\u8bc4\u4f30\u3002", "motivation": "\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u5e38\u4f7f\u7528\u654f\u611f\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u5f15\u53d1\u4e86\u5bf9\u9690\u79c1\u4fdd\u62a4\u5408\u6210\u6570\u636e\u7684\u62c5\u5fe7\uff0c\u7279\u522b\u662f\u4e0e\u6a21\u578b\u8fc7\u62df\u5408\u76f8\u5173\u7684\u9690\u79c1\u6cc4\u9732\u95ee\u9898\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u5168\u5c40\u6807\u51c6\u8bc4\u4f30\u9690\u79c1\u98ce\u9669\uff0c\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u4e14\u96be\u4ee5\u8fdb\u884c\u6837\u672c\u7ea7\u522b\u7684\u8bc4\u4f30\uff0c\u963b\u788d\u4e86\u5408\u6210\u6570\u636e\u5728\u73b0\u5b9e\u4e16\u754c\u7684\u5e94\u7528\u3002", "method": "\u5229\u7528\u6700\u8fd1\u90bb\u8ddd\u79bb\u7684\u6781\u503c\u7edf\u8ba1\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u3001\u4e0d\u4f9d\u8d56\u4e8e\u6570\u636e\u6a21\u6001\u7684\u7b97\u6cd5PRIVET\uff0c\u8be5\u7b97\u6cd5\u4e3a\u6bcf\u4e2a\u5408\u6210\u6837\u672c\u5206\u914d\u4e00\u4e2a\u5355\u72ec\u7684\u9690\u79c1\u6cc4\u9732\u5206\u6570\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cPRIVET\u80fd\u591f\u53ef\u9760\u5730\u68c0\u6d4b\u4e0d\u540c\u6570\u636e\u6a21\u6001\u4e2d\u7684\u6a21\u578b\u8bb0\u5fc6\u548c\u9690\u79c1\u6cc4\u9732\u73b0\u8c61\uff0c\u5373\u4f7f\u5728\u5904\u7406\u9ad8\u7ef4\u5ea6\u3001\u5c0f\u6837\u672c\uff08\u5982\u57fa\u56e0\u6570\u636e\uff09\u4ee5\u53ca\u6b20\u62df\u5408\u7684\u60c5\u51b5\u4e0b\u4e5f\u6709\u6548\u3002\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0cPRIVET\u5728\u6570\u636e\u96c6\u548c\u6837\u672c\u5c42\u9762\u90fd\u80fd\u63d0\u4f9b\u53ef\u5b9a\u6027\u548c\u53ef\u5b9a\u91cf\u7684\u8bc4\u4f30\u3002", "conclusion": "PRIVET\u662f\u4e00\u79cd\u6709\u6548\u7684\u6837\u672c\u7ea7\u522b\u9690\u79c1\u8bc4\u4f30\u7b97\u6cd5\uff0c\u80fd\u591f\u514b\u670d\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u5408\u6210\u6570\u636e\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u9690\u79c1\u4fdd\u969c\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u63ed\u793a\u4e86\u73b0\u6709\u8ba1\u7b97\u673a\u89c6\u89c9\u5d4c\u5165\u5728\u8bc6\u522b\u8fd1\u4f3c\u91cd\u590d\u6837\u672c\u65f6\u53ef\u80fd\u7f3a\u4e4f\u611f\u77e5\u610f\u4e49\u4e0a\u7684\u8ddd\u79bb\u5ea6\u91cf\u3002"}}
{"id": "2510.24234", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24234", "abs": "https://arxiv.org/abs/2510.24234", "authors": ["Ludovic Schwartz", "Hamish Flynn", "Gergely Neu"], "title": "Sparse Optimistic Information Directed Sampling", "comment": null, "summary": "Many high-dimensional online decision-making problems can be modeled as\nstochastic sparse linear bandits. Most existing algorithms are designed to\nachieve optimal worst-case regret in either the data-rich regime, where\npolynomial dependence on the ambient dimension is unavoidable, or the data-poor\nregime, where dimension-independence is possible at the cost of worse\ndependence on the number of rounds. In contrast, the sparse Information\nDirected Sampling (IDS) algorithm satisfies a Bayesian regret bound that has\nthe optimal rate in both regimes simultaneously. In this work, we explore the\nuse of Sparse Optimistic Information Directed Sampling (SOIDS) to achieve the\nsame adaptivity in the worst-case setting, without Bayesian assumptions.\nThrough a novel analysis that enables the use of a time-dependent learning\nrate, we show that SOIDS can optimally balance information and regret. Our\nresults extend the theoretical guarantees of IDS, providing the first algorithm\nthat simultaneously achieves optimal worst-case regret in both the data-rich\nand data-poor regimes. We empirically demonstrate the good performance of\nSOIDS.", "AI": {"tldr": "SOIDS\u7b97\u6cd5\u5728\u6700\u574f\u60c5\u51b5\u8bbe\u7f6e\u4e0b\u5b9e\u73b0\u4e86\u6570\u636e\u4e30\u5bcc\u548c\u6570\u636e\u8d2b\u4e4f\u4e24\u79cd\u60c5\u51b5\u4e0b\u7684\u6700\u4f18\u9057\u61be", "motivation": "\u73b0\u6709\u7b97\u6cd5\u5728\u6570\u636e\u4e30\u5bcc\u6216\u6570\u636e\u8d2b\u4e4f\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u9057\u61be\u754c\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u672c\u6587\u65e8\u5728\u8bbe\u8ba1\u4e00\u79cd\u80fd\u591f\u540c\u65f6\u5728\u8fd9\u4e24\u79cd\u60c5\u51b5\u4e0b\u90fd\u8fbe\u5230\u6700\u4f18\u9057\u61be\u7684\u7b97\u6cd5\u3002", "method": "\u901a\u8fc7\u65b0\u9896\u7684\u65f6\u53d8\u5b66\u4e60\u7387\u5206\u6790\uff0c\u5b9e\u73b0\u4e86\u7a00\u758f\u4e50\u89c2\u4fe1\u606f\u5bfc\u5411\u91c7\u6837\uff08SOIDS\uff09\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u80fd\u591f\u6700\u4f18\u5730\u5e73\u8861\u4fe1\u606f\u83b7\u53d6\u4e0e\u9057\u61be\u3002", "result": "SOIDS\u7b97\u6cd5\u5728\u6700\u574f\u60c5\u51b5\u4e0b\u7684\u9057\u61be\u754c\u65b9\u9762\uff0c\u80fd\u591f\u540c\u65f6\u8fbe\u5230\u6570\u636e\u4e30\u5bcc\u548c\u6570\u636e\u8d2b\u4e4f\u4e24\u79cd\u60c5\u51b5\u4e0b\u7684\u6700\u4f18\u9057\u61be\uff0c\u6269\u5c55\u4e86IDS\u7b97\u6cd5\u7684\u7406\u8bba\u4fdd\u8bc1\u3002", "conclusion": "SOIDS\u7b97\u6cd5\u662f\u7b2c\u4e00\u4e2a\u80fd\u591f\u540c\u65f6\u5728\u6570\u636e\u4e30\u5bcc\u548c\u6570\u636e\u8d2b\u4e4f\u4e24\u79cd\u60c5\u51b5\u4e0b\u5b9e\u73b0\u6700\u4f18\u6700\u574f\u60c5\u51b5\u9057\u61be\u7684\u7b97\u6cd5\uff0c\u5e76\u5728\u7ecf\u9a8c\u4e0a\u5f97\u5230\u4e86\u9a8c\u8bc1\u3002"}}
{"id": "2510.24235", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24235", "abs": "https://arxiv.org/abs/2510.24235", "authors": ["Ai Jian", "Jingqing Ruan", "Xing Ma", "Dailin Li", "QianLin Zhou", "Ke Zeng", "Xunliang Cai"], "title": "PaTaRM: Bridging Pairwise and Pointwise Signals via Preference-Aware Task-Adaptive Reward Modeling", "comment": null, "summary": "Reward models (RMs) are central to reinforcement learning from human feedback\n(RLHF), providing the critical supervision signals that align large language\nmodels (LLMs) with human preferences. While generative reward models (GRMs)\noffer greater interpretability than traditional scalar RMs, current training\nparadigms remain limited. Pair-wise methods rely on binary good-versus-bad\nlabels, which cause mismatches for point-wise inference and necessitate complex\npairing strategies for effective application in RLHF. On the other hand,\npoint-wise methods require more elaborate absolute labeling with rubric-driven\ncriteria, resulting in poor adaptability and high annotation costs. In this\nwork, we propose the Preference-Aware Task-Adaptive Reward Model (PaTaRM), a\nunified framework that integrates a preference-aware reward (PAR) mechanism\nwith dynamic rubric adaptation. PaTaRM leverages relative preference\ninformation from pairwise data to construct robust point-wise training signals,\neliminating the need for explicit point-wise labels. Simultaneously, it employs\na task-adaptive rubric system that flexibly generates evaluation criteria for\nboth global task consistency and instance-specific fine-grained reasoning. This\ndesign enables efficient, generalizable, and interpretable reward modeling for\nRLHF. Extensive experiments show that PaTaRM achieves an average relative\nimprovement of 4.7% on RewardBench and RMBench across Qwen3-8B and Qwen3-14B\nmodels. Furthermore, PaTaRM boosts downstream RLHF performance, with an average\nimprovement of 13.6% across IFEval and InFoBench benchmarks, confirming its\neffectiveness and robustness. Our code is available at\nhttps://github.com/JaneEyre0530/PaTaRM.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPaTaRM\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u4eba\u7c7b\u53cd\u9988\u4e2d\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\uff08RLHF\uff09\u7684\u5956\u52b1\u5efa\u6a21\u3002\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u504f\u597d\u611f\u77e5\u5956\u52b1\uff08PAR\uff09\u673a\u5236\u548c\u52a8\u6001\u8bc4\u5206\u5361\u81ea\u9002\u5e94\uff0c\u5229\u7528\u504f\u597d\u6570\u636e\u6784\u5efa\u70b9\u5f0f\u8bad\u7ec3\u4fe1\u53f7\uff0c\u65e0\u9700\u663e\u5f0f\u70b9\u5f0f\u6807\u7b7e\uff0c\u5e76\u901a\u8fc7\u81ea\u9002\u5e94\u8bc4\u5206\u5361\u751f\u6210\u8bc4\u4f30\u6807\u51c6\uff0c\u4ece\u800c\u5b9e\u73b0\u9ad8\u6548\u3001\u53ef\u6cdb\u5316\u4e14\u53ef\u89e3\u91ca\u7684\u5956\u52b1\u5efa\u6a21\u3002", "motivation": "\u73b0\u6709\u7684\u5956\u52b1\u6a21\u578b\uff08RM\uff09\u8bad\u7ec3\u8303\u5f0f\u5b58\u5728\u5c40\u9650\u6027\uff1a\u57fa\u4e8e\u6210\u5bf9\u7684\u65b9\u6cd5\u4f9d\u8d56\u4e8c\u5143\u6807\u7b7e\uff0c\u5bfc\u81f4\u70b9\u5f0f\u63a8\u7406\u4e0d\u5339\u914d\u4e14\u9700\u8981\u590d\u6742\u7684\u914d\u5bf9\u7b56\u7565\uff1b\u57fa\u4e8e\u70b9\u5f0f\u7684\u65b9\u6cd5\u9700\u8981\u8be6\u7ec6\u7684\u7edd\u5bf9\u6807\u7b7e\uff0c\u6210\u672c\u9ad8\u4e14\u9002\u5e94\u6027\u5dee\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u66f4\u4f18\u7684\u5956\u52b1\u5efa\u6a21\u65b9\u6cd5\u3002", "method": "\u63d0\u51faPaTaRM\u6846\u67b6\uff0c\u7ed3\u5408\u504f\u597d\u611f\u77e5\u5956\u52b1\uff08PAR\uff09\u673a\u5236\u548c\u52a8\u6001\u8bc4\u5206\u5361\u81ea\u9002\u5e94\u3002PAR\u5229\u7528\u6210\u5bf9\u6570\u636e\u4e2d\u7684\u76f8\u5bf9\u504f\u597d\u4fe1\u606f\u6784\u5efa\u70b9\u5f0f\u8bad\u7ec3\u4fe1\u53f7\uff0c\u65e0\u9700\u663e\u5f0f\u70b9\u5f0f\u6807\u7b7e\u3002\u52a8\u6001\u8bc4\u5206\u5361\u7cfb\u7edf\u7075\u6d3b\u751f\u6210\u8bc4\u4f30\u6807\u51c6\uff0c\u517c\u987e\u5168\u5c40\u4efb\u52a1\u4e00\u81f4\u6027\u548c\u5b9e\u4f8b\u7279\u5b9a\u7684\u7ec6\u7c92\u5ea6\u63a8\u7406\u3002", "result": "\u5728RewardBench\u548cRMBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPaTaRM\u5728Qwen3-8B\u548cQwen3-14B\u6a21\u578b\u4e0a\u53d6\u5f97\u4e86\u5e73\u57474.7%\u7684\u76f8\u5bf9\u63d0\u5347\u3002\u5728IFEval\u548cInFoBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4e0b\u6e38RLHF\u6027\u80fd\u5e73\u5747\u63d0\u5347\u4e8613.6%\u3002", "conclusion": "PaTaRM\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u80fd\u591f\u5b9e\u73b0\u9ad8\u6548\u3001\u53ef\u6cdb\u5316\u4e14\u53ef\u89e3\u91ca\u7684\u5956\u52b1\u5efa\u6a21\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u5728\u63d0\u5347\u5956\u52b1\u6a21\u578b\u6027\u80fd\u548c\u4e0b\u6e38RLHF\u4efb\u52a1\u65b9\u9762\u7684\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.24240", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24240", "abs": "https://arxiv.org/abs/2510.24240", "authors": ["Edward Markai", "Sina Molavipour"], "title": "Temporal Knowledge Graph Hyperedge Forecasting: Exploring Entity-to-Category Link Prediction", "comment": null, "summary": "Temporal Knowledge Graphs have emerged as a powerful way of not only modeling\nstatic relationships between entities but also the dynamics of how relations\nevolve over time. As these informational structures can be used to store\ninformation from a real-world setting, such as a news flow, predicting future\ngraph components to a certain extent equates predicting real-world events. Most\nof the research in this field focuses on embedding-based methods, often\nleveraging convolutional neural net architectures. These solutions act as black\nboxes, limiting insight. In this paper, we explore an extension to an\nestablished rule-based framework, TLogic, that yields a high accuracy in\ncombination with explainable predictions. This offers transparency and allows\nthe end-user to critically evaluate the rules applied at the end of the\nprediction stage. The new rule format incorporates entity category as a key\ncomponent with the purpose of limiting rule application only to relevant\nentities. When categories are unknown for building the graph, we propose a\ndata-driven method to generate them with an LLM-based approach. Additionally,\nwe investigate the choice of aggregation method for scores of retrieved\nentities when performing category prediction.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2510.24273", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24273", "abs": "https://arxiv.org/abs/2510.24273", "authors": ["Junlin Mu", "Hantao Huang", "Jihang Zhang", "Minghui Yu", "Tao Wang", "Yidong Li"], "title": "SALS: Sparse Attention in Latent Space for KV cache Compression", "comment": null, "summary": "Large Language Models capable of handling extended contexts are in high\ndemand, yet their inference remains challenging due to substantial Key-Value\ncache size and high memory bandwidth requirements. Previous research has\ndemonstrated that KV cache exhibits low-rank characteristics within the hidden\ndimension, suggesting the potential for effective compression. However, due to\nthe widely adopted Rotary Position Embedding mechanism in modern LLMs, naive\nlow-rank compression suffers severe accuracy degradation or creates a new speed\nbottleneck, as the low-rank cache must first be reconstructed in order to apply\nRoPE. In this paper, we introduce two key insights: first, the application of\nRoPE to the key vectors increases their variance, which in turn results in a\nhigher rank; second, after the key vectors are transformed into the latent\nspace, they largely maintain their representation across most layers. Based on\nthese insights, we propose the Sparse Attention in Latent Space framework. SALS\nprojects the KV cache into a compact latent space via low-rank projection, and\nperforms sparse token selection using RoPE-free query-key interactions in this\nspace. By reconstructing only a small subset of important tokens, it avoids the\noverhead of full KV cache reconstruction. We comprehensively evaluate SALS on\nvarious tasks using two large-scale models: LLaMA2-7b-chat and Mistral-7b, and\nadditionally verify its scalability on the RULER-128k benchmark with\nLLaMA3.1-8B-Instruct. Experimental results demonstrate that SALS achieves SOTA\nperformance by maintaining competitive accuracy. Under different settings, SALS\nachieves 6.4-fold KV cache compression and 5.7-fold speed-up in the attention\noperator compared to FlashAttention2 on the 4K sequence. For the end-to-end\nthroughput performance, we achieves 1.4-fold and 4.5-fold improvement compared\nto GPT-fast on 4k and 32K sequences, respectively.", "AI": {"tldr": "KV\u7f13\u5b58\u538b\u7f29\u548c\u52a0\u901f\u7684\u65b0\u65b9\u6cd5SALS\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\uff0c\u901a\u8fc7\u4f4e\u79e9\u6295\u5f71\u548c\u7a00\u758f\u6ce8\u610f\u529b\u9009\u62e9\u6765\u89e3\u51b3LLM\u63a8\u7406\u4e2d\u7684\u5185\u5b58\u548c\u901f\u5ea6\u74f6\u9888\u3002", "motivation": "\u73b0\u4ee3LLM\u9700\u8981\u5904\u7406\u957f\u4e0a\u4e0b\u6587\uff0c\u4f46KV\u7f13\u5b58\u7684\u5927\u5c0f\u548c\u5185\u5b58\u5e26\u5bbd\u9650\u5236\u4e86\u63a8\u7406\u6548\u7387\u3002\u73b0\u6709\u538b\u7f29\u65b9\u6cd5\u56e0RoPE\u673a\u5236\u800c\u53d7\u5230\u7cbe\u5ea6\u635f\u5931\u6216\u901f\u5ea6\u74f6\u9888\u7684\u9650\u5236\u3002", "method": "\u63d0\u51faSALS\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8fdb\u884cKV\u7f13\u5b58\u7684\u4f4e\u79e9\u6295\u5f71\uff0c\u5e76\u901a\u8fc7\u65e0RoPE\u7684\u67e5\u8be2-\u952e\u4ea4\u4e92\u8fdb\u884c\u7a00\u758f\u6807\u8bb0\u9009\u62e9\uff0c\u4ec5\u91cd\u5efa\u4e00\u5c0f\u90e8\u5206\u91cd\u8981\u6807\u8bb0\uff0c\u4ece\u800c\u907f\u514d\u4e86\u5b8c\u6574\u7684KV\u7f13\u5b58\u91cd\u5efa\u5f00\u9500\u3002", "result": "\u5728LLaMA2-7b-chat\u548cMistral-7b\u6a21\u578b\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u8bc4\u4f30\uff0c\u5e76\u5728RULER-128k\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u9a8c\u8bc1\u4e86\u5176\u53ef\u6269\u5c55\u6027\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSALS\u57284K\u5e8f\u5217\u4e0a\u5b9e\u73b0\u4e866.4\u500d\u7684KV\u7f13\u5b58\u538b\u7f29\u548c5.7\u500d\u7684\u6ce8\u610f\u529b\u7b97\u5b50\u52a0\u901f\uff08\u76f8\u6bd4FlashAttention2\uff09\uff0c\u5e76\u4e14\u57284K\u548c32K\u5e8f\u5217\u4e0a\u7aef\u5230\u7aef\u541e\u5410\u91cf\u5206\u522b\u6bd4GPT-fast\u63d0\u9ad8\u4e861.4\u500d\u548c4.5\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u51c6\u786e\u6027\u3002", "conclusion": "SALS\u901a\u8fc7\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8fdb\u884cKV\u7f13\u5b58\u538b\u7f29\u548c\u7a00\u758f\u6ce8\u610f\u529b\u9009\u62e9\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLM\u63a8\u7406\u4e2d\u7684\u5185\u5b58\u548c\u901f\u5ea6\u74f6\u9888\uff0c\u5e76\u5728\u5404\u9879\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2510.24310", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24310", "abs": "https://arxiv.org/abs/2510.24310", "authors": ["Guus Toussaint", "Arno Knobbe"], "title": "EDC: Equation Discovery for Classification", "comment": "This preprint has not undergone peer review or any post-submission\n  improvements or corrections. The Version of Record of this contribution is\n  published in Lecture Notes in Computer Science, and is available online at\n  https://doi.org/10.1007/978-3-032-05461-6_9", "summary": "Equation Discovery techniques have shown considerable success in regression\ntasks, where they are used to discover concise and interpretable models\n(\\textit{Symbolic Regression}). In this paper, we propose a new ED-based binary\nclassification framework. Our proposed method EDC finds analytical functions of\nmanageable size that specify the location and shape of the decision boundary.\nIn extensive experiments on artificial and real-life data, we demonstrate how\nEDC is able to discover both the structure of the target equation as well as\nthe value of its parameters, outperforming the current state-of-the-art\nED-based classification methods in binary classification and achieving\nperformance comparable to the state of the art in binary classification. We\nsuggest a grammar of modest complexity that appears to work well on the tested\ndatasets but argue that the exact grammar -- and thus the complexity of the\nmodels -- is configurable, and especially domain-specific expressions can be\nincluded in the pattern language, where that is required. The presented grammar\nconsists of a series of summands (additive terms) that include linear,\nquadratic and exponential terms, as well as products of two features (producing\nhyperbolic curves ideal for capturing XOR-like dependencies). The experiments\ndemonstrate that this grammar allows fairly flexible decision boundaries while\nnot so rich to cause overfitting.", "AI": {"tldr": "EDC\u662f\u4e00\u4e2a\u57fa\u4e8e\u65b9\u7a0b\u53d1\u73b0\u7684\u4e8c\u5143\u5206\u7c7b\u6846\u67b6\uff0c\u80fd\u591f\u53d1\u73b0\u89e3\u6790\u51fd\u6570\u6765\u5b9a\u4e49\u51b3\u7b56\u8fb9\u754c\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u5728\u56de\u5f52\u4efb\u52a1\u4e2d\uff0c\u65b9\u7a0b\u53d1\u73b0\u6280\u672f\uff08\u7b26\u53f7\u56de\u5f52\uff09\u5df2\u88ab\u8bc1\u660e\u53ef\u4ee5\u53d1\u73b0\u7b80\u6d01\u4e14\u53ef\u89e3\u91ca\u7684\u6a21\u578b\u3002\u672c\u6587\u65e8\u5728\u5c06\u8fd9\u79cd\u6280\u672f\u5e94\u7528\u4e8e\u4e8c\u5143\u5206\u7c7b\u4efb\u52a1\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEDC\u7684\u65b0\u578b\u57fa\u4e8e\u65b9\u7a0b\u53d1\u73b0\u7684\u4e8c\u5143\u5206\u7c7b\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u53d1\u73b0\u5b9a\u4e49\u51b3\u7b56\u8fb9\u754c\u7684\u4f4d\u7f6e\u548c\u5f62\u72b6\u7684\u89e3\u6790\u51fd\u6570\u3002EDC\u80fd\u591f\u540c\u65f6\u53d1\u73b0\u76ee\u6807\u65b9\u7a0b\u7684\u7ed3\u6784\u548c\u53c2\u6570\u503c\u3002\u6587\u7ae0\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u7531\u52a0\u6cd5\u9879\u7ec4\u6210\u7684\u8bed\u6cd5\uff0c\u5305\u62ec\u7ebf\u6027\u3001\u4e8c\u6b21\u548c\u6307\u6570\u9879\uff0c\u4ee5\u53ca\u4e24\u4e2a\u7279\u5f81\u7684\u4e58\u79ef\uff0c\u5e76\u8bc1\u660e\u4e86\u8be5\u8bed\u6cd5\u7684\u7075\u6d3b\u6027\u548c\u6297\u8fc7\u62df\u5408\u80fd\u529b\u3002", "result": "\u5728\u4eba\u5de5\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cEDC\u5728\u4e8c\u5143\u5206\u7c7b\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u4e8eED\u7684\u5206\u7c7b\u65b9\u6cd5\uff0c\u5e76\u4e14\u5728\u6027\u80fd\u4e0a\u53ef\u4e0e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u76f8\u5ab2\u7f8e\u3002", "conclusion": "EDC\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u6765\u53d1\u73b0\u7528\u4e8e\u4e8c\u5143\u5206\u7c7b\u7684\u89e3\u6790\u51fd\u6570\u3002\u6240\u63d0\u51fa\u7684\u8bed\u6cd5\u5728\u6240\u6d4b\u8bd5\u7684\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u5e76\u4e14\u53ef\u4ee5\u6839\u636e\u5177\u4f53\u5e94\u7528\u8fdb\u884c\u914d\u7f6e\uff0c\u4ee5\u5305\u542b\u7279\u5b9a\u4e8e\u57df\u7684\u8868\u8fbe\u5f0f\uff0c\u4ece\u800c\u5728\u6a21\u578b\u590d\u6742\u6027\u548c\u6027\u80fd\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002"}}
{"id": "2510.24318", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24318", "abs": "https://arxiv.org/abs/2510.24318", "authors": ["Prajit Bhaskaran", "Tom Viering"], "title": "Transformers can do Bayesian Clustering", "comment": null, "summary": "Bayesian clustering accounts for uncertainty but is computationally demanding\nat scale. Furthermore, real-world datasets often contain missing values, and\nsimple imputation ignores the associated uncertainty, resulting in suboptimal\nresults. We present Cluster-PFN, a Transformer-based model that extends\nPrior-Data Fitted Networks (PFNs) to unsupervised Bayesian clustering. Trained\nentirely on synthetic datasets generated from a finite Gaussian Mixture Model\n(GMM) prior, Cluster-PFN learns to estimate the posterior distribution over\nboth the number of clusters and the cluster assignments. Our method estimates\nthe number of clusters more accurately than handcrafted model selection\nprocedures such as AIC, BIC and Variational Inference (VI), and achieves\nclustering quality competitive with VI while being orders of magnitude faster.\nCluster-PFN can be trained on complex priors that include missing data,\noutperforming imputation-based baselines on real-world genomic datasets, at\nhigh missingness. These results show that the Cluster-PFN can provide scalable\nand flexible Bayesian clustering.", "AI": {"tldr": "Cluster-PFN\u662f\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u6a21\u578b\uff0c\u53ef\u4ee5\u8fdb\u884c\u65e0\u76d1\u7763\u8d1d\u53f6\u65af\u805a\u7c7b\uff0c\u5e76\u4e14\u5728\u5904\u7406\u7f3a\u5931\u6570\u636e\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u7684\u8d1d\u53f6\u65af\u805a\u7c7b\u65b9\u6cd5\u5728\u5904\u7406\u5927\u89c4\u6a21\u6570\u636e\u96c6\u548c\u7f3a\u5931\u503c\u65f6\u5b58\u5728\u8ba1\u7b97\u6210\u672c\u9ad8\u548c\u5ffd\u7565\u4e0d\u786e\u5b9a\u6027\u7b49\u95ee\u9898\u3002", "method": "Cluster-PFN\u6a21\u578b\u5b8c\u5168\u5728\u4ece\u6709\u9650\u9ad8\u65af\u6df7\u5408\u6a21\u578b\uff08GMM\uff09\u5148\u9a8c\u751f\u6210\u7684\u5408\u6210\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u53ef\u4ee5\u4f30\u8ba1\u7c07\u7684\u6570\u91cf\u548c\u7c07\u5206\u914d\u7684\u540e\u9a8c\u5206\u5e03\u3002\u6b64\u6a21\u578b\u53ef\u4ee5\u8bad\u7ec3\u590d\u6742\u6a21\u578b\uff0c\u5305\u62ec\u7f3a\u5931\u6570\u636e\u3002", "result": "Cluster-PFN\u5728\u4f30\u8ba1\u7c07\u6570\u91cf\u65b9\u9762\u6bd4AIC\u3001BIC\u548c\u53d8\u5206\u63a8\u65ad\uff08VI\uff09\u7b49\u65b9\u6cd5\u66f4\u51c6\u786e\uff0c\u5e76\u4e14\u5728\u805a\u7c7b\u8d28\u91cf\u65b9\u9762\u4e0eVI\u76f8\u5f53\uff0c\u4f46\u901f\u5ea6\u5374\u5feb\u51e0\u4e2a\u6570\u91cf\u7ea7\u3002\u5728\u7f3a\u5931\u503c\u6bd4\u4f8b\u5f88\u9ad8\u7684\u60c5\u51b5\u4e0b\uff0cCluster-PFN\u5728\u771f\u5b9e\u7684\u57fa\u56e0\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u4f18\u4e8e\u57fa\u4e8e\u63d2\u8865\u7684\u65b9\u6cd5\u3002", "conclusion": "Cluster-PFN\u80fd\u591f\u63d0\u4f9b\u53ef\u6269\u5c55\u4e14\u7075\u6d3b\u7684\u8d1d\u53f6\u65af\u805a\u7c7b\u3002"}}
{"id": "2510.24356", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.24356", "abs": "https://arxiv.org/abs/2510.24356", "authors": ["Suman Sanyal"], "title": "Perception Learning: A Formal Separation of Sensory Representation Learning from Decision Learning", "comment": null, "summary": "We introduce Perception Learning (PeL), a paradigm that optimizes an agent's\nsensory interface $f_\\phi:\\mathcal{X}\\to\\mathcal{Z}$ using task-agnostic\nsignals, decoupled from downstream decision learning\n$g_\\theta:\\mathcal{Z}\\to\\mathcal{Y}$. PeL directly targets label-free\nperceptual properties, such as stability to nuisances, informativeness without\ncollapse, and controlled geometry, assessed via objective\nrepresentation-invariant metrics. We formalize the separation of perception and\ndecision, define perceptual properties independent of objectives or\nreparameterizations, and prove that PeL updates preserving sufficient\ninvariants are orthogonal to Bayes task-risk gradients. Additionally, we\nprovide a suite of task-agnostic evaluation metrics to certify perceptual\nquality.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u611f\u77e5\u5b66\u4e60\uff08PeL\uff09\u7684\u65b0\u8303\u5f0f\uff0c\u8be5\u8303\u5f0f\u4f7f\u7528\u4e0e\u4efb\u52a1\u65e0\u5173\u7684\u4fe1\u53f7\u6765\u4f18\u5316\u4ee3\u7406\u7684\u611f\u5b98\u63a5\u53e3\uff0c\u5e76\u5c06\u5176\u4e0e\u4e0b\u6e38\u51b3\u7b56\u5b66\u4e60\u5206\u79bb\u3002", "motivation": "\u7531\u4e8e\u4e0b\u6e38\u4efb\u52a1\u7684\u5956\u52b1\u4fe1\u53f7\u65e0\u6cd5\u6709\u6548\u5730\u6307\u5bfc\u611f\u77e5\u63a5\u53e3\u7684\u4f18\u5316\uff0c\u800c\u611f\u77e5\u63a5\u53e3\u7684\u8d28\u91cf\u5bf9\u4e0b\u6e38\u4efb\u52a1\u7684\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u4f18\u5316\u611f\u77e5\u63a5\u53e3\u3002", "method": "PeL \u901a\u8fc7\u4f18\u5316\u4ee3\u7406\u7684\u611f\u5b98\u63a5\u53e3 $f_\bound:\tau \to \tau$ \u6765\u4f18\u5316\u611f\u77e5\u63a5\u53e3\uff0c\u5e76\u4f7f\u7528\u4e0e\u4efb\u52a1\u65e0\u5173\u7684\u4fe1\u53f7\u8fdb\u884c\u4f18\u5316\uff0c\u5c06\u5176\u4e0e\u4e0b\u6e38\u51b3\u7b56\u5b66\u4e60 $g_\theta:\tau \to \rho$ \u5206\u79bb\u3002PeL \u76f4\u63a5\u9488\u5bf9\u6807\u7b7e\u7684\u611f\u77e5\u7279\u6027\uff0c\u4f8b\u5982\u5bf9\u5e72\u6270\u7684\u7a33\u5b9a\u6027\u3001\u4fe1\u606f\u6027\u800c\u4e0d\u5d29\u6e83\uff0c\u4ee5\u53ca\u901a\u8fc7\u5ba2\u89c2\u7684\u8868\u793a\u4e0d\u53d8\u5ea6\u91cf\u8bc4\u4f30\u7684\u53ef\u63a7\u51e0\u4f55\u5f62\u72b6\u3002\u6211\u4eec\u5f62\u5f0f\u5316\u4e86\u611f\u77e5\u548c\u51b3\u7b56\u7684\u5206\u79bb\uff0c\u5b9a\u4e49\u4e86\u72ec\u7acb\u4e8e\u76ee\u6807\u6216\u91cd\u65b0\u53c2\u6570\u5316\u7684\u611f\u77e5\u7279\u6027\uff0c\u5e76\u8bc1\u660e\u4e86\u4fdd\u6301\u8db3\u591f\u4e0d\u53d8\u6027\u7684 PeL \u66f4\u65b0\u4e0e\u8d1d\u53f6\u65af\u4efb\u52a1\u98ce\u9669\u68af\u5ea6\u6b63\u4ea4\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u4f9b\u4e86\u4e00\u5957\u4e0e\u4efb\u52a1\u65e0\u5173\u7684\u8bc4\u4f30\u6307\u6807\u6765\u8ba4\u8bc1\u611f\u77e5\u8d28\u91cf\u3002", "result": "PeL \u80fd\u591f\u5b66\u4e60\u5230\u5177\u6709\u826f\u597d\u611f\u77e5\u7279\u6027\u7684\u63a5\u53e3\uff0c\u5373\u4f7f\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u6ca1\u6709\u660e\u786e\u7684\u5956\u52b1\u4fe1\u53f7\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u63d0\u9ad8\u6027\u80fd\u3002", "conclusion": "PeL \u662f\u4e00\u79cd\u6709\u524d\u666f\u7684\u8303\u5f0f\uff0c\u53ef\u4ee5\u6539\u5584\u4ee3\u7406\u7684\u5b66\u4e60\u80fd\u529b\uff0c\u4f7f\u5176\u5728\u5404\u79cd\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u597d\u3002"}}
{"id": "2510.24368", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24368", "abs": "https://arxiv.org/abs/2510.24368", "authors": ["Maria Gabriela Valeriano", "David Kohan Marzag\u00e3o", "Alfredo Montelongo", "Carlos Roberto Veiga Kiffer", "Natan Katz", "Ana Carolina Lorena"], "title": "Filtering instances and rejecting predictions to obtain reliable models in healthcare", "comment": "This paper is under review at Machine Learning (Springer)", "summary": "Machine Learning (ML) models are widely used in high-stakes domains such as\nhealthcare, where the reliability of predictions is critical. However, these\nmodels often fail to account for uncertainty, providing predictions even with\nlow confidence. This work proposes a novel two-step data-centric approach to\nenhance the performance of ML models by improving data quality and filtering\nlow-confidence predictions. The first step involves leveraging Instance\nHardness (IH) to filter problematic instances during training, thereby refining\nthe dataset. The second step introduces a confidence-based rejection mechanism\nduring inference, ensuring that only reliable predictions are retained. We\nevaluate our approach using three real-world healthcare datasets, demonstrating\nits effectiveness at improving model reliability while balancing predictive\nperformance and rejection rate. Additionally, we use alternative criteria -\ninfluence values for filtering and uncertainty for rejection - as baselines to\nevaluate the efficiency of the proposed method. The results demonstrate that\nintegrating IH filtering with confidence-based rejection effectively enhances\nmodel performance while preserving a large proportion of instances. This\napproach provides a practical method for deploying ML systems in\nsafety-critical applications.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4ee5\u6570\u636e\u4e3a\u4e2d\u5fc3\u7684\u53cc\u6b65\u9aa4\u65b9\u6cd5\uff0c\u5229\u7528\u5b9e\u4f8b\u96be\u5ea6\uff08IH\uff09\u8fc7\u6ee4\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u96be\u4f8b\uff0c\u5e76\u7ed3\u5408\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u62d2\u7edd\u673a\u5236\uff0c\u4ee5\u63d0\u9ad8\u533b\u7597\u7b49\u9ad8\u98ce\u9669\u9886\u57df\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u9ad8\u98ce\u9669\u9886\u57df\uff08\u5982\u533b\u7597\uff09\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u9884\u6d4b\u4e0d\u53ef\u9760\u6216\u7f6e\u4fe1\u5ea6\u4f4e\u65f6\u4ecd\u4f1a\u7ed9\u51fa\u9884\u6d4b\uff0c\u7f3a\u4e4f\u5bf9\u4e0d\u786e\u5b9a\u6027\u7684\u5904\u7406\uff0c\u800c\u6a21\u578b\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u53cc\u6b65\u9aa4\u3001\u4ee5\u6570\u636e\u4e3a\u4e2d\u5fc3\u7684\u65b9\u6cd5\uff1a1. \u8bad\u7ec3\u9636\u6bb5\uff1a\u5229\u7528\u5b9e\u4f8b\u96be\u5ea6\uff08IH\uff09\u8fc7\u6ee4\u6709\u95ee\u9898\u7684\u6837\u672c\uff0c\u4f18\u5316\u6570\u636e\u96c6\u30022. \u63a8\u7406\u9636\u6bb5\uff1a\u5f15\u5165\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u62d2\u7edd\u673a\u5236\uff0c\u4ec5\u4fdd\u7559\u53ef\u9760\u7684\u9884\u6d4b\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u7684\u533b\u7597\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u8be5\u65b9\u6cd5\u80fd\u63d0\u9ad8\u6a21\u578b\u53ef\u9760\u6027\uff0c\u540c\u65f6\u5e73\u8861\u9884\u6d4b\u6027\u80fd\u548c\u62d2\u7edd\u7387\u3002\u4e0e\u57fa\u4e8e\u5f71\u54cd\u503c\u8fc7\u6ee4\u548c\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u62d2\u7edd\u7684\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\u4fdd\u7559\u4e86\u5927\u90e8\u5206\u6837\u672c\u3002", "conclusion": "\u7ed3\u5408\u5b9e\u4f8b\u96be\u5ea6\u8fc7\u6ee4\u548c\u7f6e\u4fe1\u5ea6\u62d2\u7edd\u7684\u96c6\u6210\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u7684\u6027\u80fd\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2510.24375", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24375", "abs": "https://arxiv.org/abs/2510.24375", "authors": ["Yuanyuan Wu", "Zhenlin Qin", "Zhenliang Ma"], "title": "A Comprehensive Evaluation Framework for Synthetic Trip Data Generation in Public Transport", "comment": null, "summary": "Synthetic data offers a promising solution to the privacy and accessibility\nchallenges of using smart card data in public transport research. Despite rapid\nprogress in generative modeling, there is limited attention to comprehensive\nevaluation, leaving unclear how reliable, safe, and useful synthetic data truly\nare. Existing evaluations remain fragmented, typically limited to\npopulation-level representativeness or record-level privacy, without\nconsidering group-level variations or task-specific utility. To address this\ngap, we propose a Representativeness-Privacy-Utility (RPU) framework that\nsystematically evaluates synthetic trip data across three complementary\ndimensions and three hierarchical levels (record, group, population). The\nframework integrates a consistent set of metrics to quantify similarity,\ndisclosure risk, and practical usefulness, enabling transparent and balanced\nassessment of synthetic data quality. We apply the framework to benchmark\ntwelve representative generation methods, spanning conventional statistical\nmodels, deep generative networks, and privacy-enhanced variants. Results show\nthat synthetic data do not inherently guarantee privacy and there is no\n\"one-size-fits-all\" model, the trade-off between privacy and\nrepresentativeness/utility is obvious. Conditional Tabular generative\nadversarial network (CTGAN) provide the most balanced trade-off and is\nsuggested for practical applications. The RPU framework provides a systematic\nand reproducible basis for researchers and practitioners to compare synthetic\ndata generation techniques and select appropriate methods in public transport\napplications.", "AI": {"tldr": "\u73b0\u6709\u7684\u5408\u6210\u6570\u636e\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u4e0d\u8db3\uff0c\u65e0\u6cd5\u5168\u9762\u8861\u91cf\u5176\u5728\u516c\u5171\u4ea4\u901a\u7814\u7a76\u4e2d\u7684\u53ef\u9760\u6027\u3001\u5b89\u5168\u6027\u548c\u6709\u7528\u6027\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aRPU\uff08\u4ee3\u8868\u6027-\u9690\u79c1-\u6548\u7528\uff09\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u8bb0\u5f55\u3001\u5206\u7ec4\u548c\u603b\u4f53\u4e09\u4e2a\u5c42\u7ea7\u4e0a\u7cfb\u7edf\u5730\u8bc4\u4f30\u5408\u6210\u884c\u7a0b\u6570\u636e\uff0c\u5e76\u6574\u5408\u4e86\u4e00\u5957\u91cf\u5316\u76f8\u4f3c\u6027\u3001\u6cc4\u9732\u98ce\u9669\u548c\u5b9e\u9645\u6548\u7528\u7684\u5ea6\u91cf\u6807\u51c6\u3002\u8be5\u6846\u67b6\u5df2\u5e94\u7528\u4e8e\u5bf9\u5341\u4e8c\u79cd\u751f\u6210\u65b9\u6cd5\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7ed3\u679c\u8868\u660e\u5408\u6210\u6570\u636e\u5e76\u4e0d\u80fd\u81ea\u52a8\u4fdd\u8bc1\u9690\u79c1\uff0c\u4e14\u201c\u4e00\u5200\u5207\u201d\u7684\u6a21\u578b\u5e76\u4e0d\u5b58\u5728\uff0c\u9690\u79c1\u4e0e\u4ee3\u8868\u6027/\u6548\u7528\u4e4b\u95f4\u5b58\u5728\u660e\u663e\u7684\u6743\u8861\u3002\u6761\u4ef6\u5f0f\u8868\u683c\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08CTGAN\uff09\u5728\u9690\u79c1\u548c\u4ee3\u8868\u6027/\u6548\u7528\u4e4b\u95f4\u53d6\u5f97\u4e86\u6700\u4f73\u7684\u5e73\u8861\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u3002RPU\u6846\u67b6\u4e3a\u7814\u7a76\u4eba\u5458\u548c\u5b9e\u8df5\u8005\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7cfb\u7edf\u4e14\u53ef\u590d\u73b0\u7684\u57fa\u7840\uff0c\u7528\u4e8e\u6bd4\u8f83\u5408\u6210\u6570\u636e\u751f\u6210\u6280\u672f\u5e76\u9009\u62e9\u9002\u5408\u516c\u5171\u4ea4\u901a\u5e94\u7528\u7684\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u5408\u6210\u6570\u636e\u8bc4\u4f30\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u8003\u8651\u8bb0\u5f55\u3001\u5206\u7ec4\u548c\u603b\u4f53\u4e09\u4e2a\u5c42\u7ea7\u7684\u4ee3\u8868\u6027\u3001\u9690\u79c1\u6027\u53ca\u6548\u7528\uff0c\u5bfc\u81f4\u5bf9\u5408\u6210\u6570\u636e\u7684\u53ef\u9760\u6027\u3001\u5b89\u5168\u6027\u548c\u5b9e\u7528\u6027\u8bc4\u4f30\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u5e76\u5e94\u7528\u4e86\u4ee3\u8868\u6027-\u9690\u79c1-\u6548\u7528\uff08RPU\uff09\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5728\u8bb0\u5f55\u3001\u5206\u7ec4\u548c\u603b\u4f53\u4e09\u4e2a\u5c42\u7ea7\u4e0a\uff0c\u6574\u5408\u4e86\u4e00\u5957\u91cf\u5316\u76f8\u4f3c\u6027\u3001\u6cc4\u9732\u98ce\u9669\u548c\u5b9e\u9645\u6548\u7528\u7684\u5ea6\u91cf\u6807\u51c6\uff0c\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30\u5408\u6210\u884c\u7a0b\u6570\u636e\u3002\u5c06\u8be5\u6846\u67b6\u5e94\u7528\u4e8e\u57fa\u51c6\u6d4b\u8bd5\u4e86\u5341\u4e8c\u79cd\u751f\u6210\u65b9\u6cd5\u3002", "result": "\u5728\u5bf9\u5341\u4e8c\u79cd\u751f\u6210\u65b9\u6cd5\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u540e\u53d1\u73b0\uff0c\u5408\u6210\u6570\u636e\u5e76\u4e0d\u81ea\u52a8\u4fdd\u8bc1\u9690\u79c1\uff0c\u4e14\u4e0d\u5b58\u5728\u201c\u4e00\u5200\u5207\u201d\u7684\u6700\u4f73\u6a21\u578b\uff0c\u9690\u79c1\u4e0e\u4ee3\u8868\u6027/\u6548\u7528\u4e4b\u95f4\u5b58\u5728\u660e\u663e\u7684\u6743\u8861\u3002\u6761\u4ef6\u5f0f\u8868\u683c\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08CTGAN\uff09\u5728\u4e09\u8005\u4e4b\u95f4\u53d6\u5f97\u4e86\u6700\u4f73\u7684\u5e73\u8861\u3002", "conclusion": "RPU\u6846\u67b6\u4e3a\u7814\u7a76\u4eba\u5458\u548c\u5b9e\u8df5\u8005\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7cfb\u7edf\u4e14\u53ef\u590d\u73b0\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u6bd4\u8f83\u5408\u6210\u6570\u636e\u751f\u6210\u6280\u672f\uff0c\u5e76\u6839\u636e\u516c\u5171\u4ea4\u901a\u5e94\u7528\u7684\u5177\u4f53\u9700\u6c42\u9009\u62e9\u6700\u5408\u9002\u7684\u65b9\u6cd5\u3002CTGAN\u662f\u76ee\u524d\u5728\u9690\u79c1\u3001\u4ee3\u8868\u6027\u548c\u6548\u7528\u4e4b\u95f4\u53d6\u5f97\u6700\u4f73\u5e73\u8861\u7684\u5b9e\u7528\u6a21\u578b\u3002"}}
{"id": "2510.24380", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24380", "abs": "https://arxiv.org/abs/2510.24380", "authors": ["Aryan Pedawi", "Jordi Silvestre-Ryan", "Bradley Worley", "Darren J Hsu", "Kushal S Shah", "Elias Stehle", "Jingrong Zhang", "Izhar Wallach"], "title": "APEX: Approximate-but-exhaustive search for ultra-large combinatorial synthesis libraries", "comment": null, "summary": "Make-on-demand combinatorial synthesis libraries (CSLs) like Enamine REAL\nhave significantly enabled drug discovery efforts. However, their large size\npresents a challenge for virtual screening, where the goal is to identify the\ntop compounds in a library according to a computational objective (e.g.,\noptimizing docking score) subject to computational constraints under a limited\ncomputational budget. For current library sizes -- numbering in the tens of\nbillions of compounds -- and scoring functions of interest, a routine virtual\nscreening campaign may be limited to scoring fewer than 0.1% of the available\ncompounds, leaving potentially many high scoring compounds undiscovered.\nFurthermore, as constraints (and sometimes objectives) change during the course\nof a virtual screening campaign, existing virtual screening algorithms\ntypically offer little room for amortization. We propose the\napproximate-but-exhaustive search protocol for CSLs, or APEX. APEX utilizes a\nneural network surrogate that exploits the structure of CSLs in the prediction\nof objectives and constraints to make full enumeration on a consumer GPU\npossible in under a minute, allowing for exact retrieval of approximate top-$k$\nsets. To demonstrate APEX's capabilities, we develop a benchmark CSL comprised\nof more than 10 million compounds, all of which have been annotated with their\ndocking scores on five medically relevant targets along with physicohemical\nproperties measured with RDKit such that, for any objective and set of\nconstraints, the ground truth top-$k$ compounds can be identified and compared\nagainst the retrievals from any virtual screening algorithm. We show APEX's\nconsistently strong performance both in retrieval accuracy and runtime compared\nto alternative methods.", "AI": {"tldr": "Make-on-demand combinatorial synthesis libraries (CSLs) are powerful but challenging for virtual screening due to their massive size. This paper introduces APEX, a novel protocol using a neural network surrogate to enable full enumeration and exact retrieval of approximate top-k compounds on a consumer GPU in under a minute. APEX demonstrates strong performance in retrieval accuracy and runtime compared to existing methods, validated on a benchmark CSL of over 10 million compounds.", "motivation": "The immense size of combinatorial synthesis libraries (CSLs) poses a significant challenge for virtual screening, where only a small fraction of compounds can be computationally evaluated within a limited budget, potentially missing high-scoring compounds. Existing algorithms lack amortization for changing objectives or constraints.", "method": "APEX (approximate-but-exhaustive search protocol) utilizes a neural network surrogate that leverages the structure of CSLs to predict objectives and constraints. This allows for full enumeration of compounds on a consumer GPU in under a minute, enabling exact retrieval of approximate top-k sets.", "result": "APEX demonstrated consistently strong performance in both retrieval accuracy and runtime when compared to alternative virtual screening methods. A benchmark CSL with over 10 million compounds, annotated with docking scores and physicochemical properties, was developed to validate these findings.", "conclusion": "APEX offers an efficient and accurate solution for virtual screening of large CSLs, making full enumeration feasible on consumer hardware and enabling the discovery of potentially missed high-scoring compounds."}}
{"id": "2510.24432", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24432", "abs": "https://arxiv.org/abs/2510.24432", "authors": ["Seyed Mahdi Basiri Azad", "Joschka Boedecker"], "title": "Fill in the Blanks: Accelerating Q-Learning with a Handful of Demonstrations in Sparse Reward Settings", "comment": null, "summary": "Reinforcement learning (RL) in sparse-reward environments remains a\nsignificant challenge due to the lack of informative feedback. We propose a\nsimple yet effective method that uses a small number of successful\ndemonstrations to initialize the value function of an RL agent. By precomputing\nvalue estimates from offline demonstrations and using them as targets for early\nlearning, our approach provides the agent with a useful prior over promising\nactions. The agent then refines these estimates through standard online\ninteraction. This hybrid offline-to-online paradigm significantly reduces the\nexploration burden and improves sample efficiency in sparse-reward settings.\nExperiments on benchmark tasks demonstrate that our method accelerates\nconvergence and outperforms standard baselines, even with minimal or suboptimal\ndemonstration data.", "AI": {"tldr": "\u901a\u8fc7\u4f7f\u7528\u5c11\u91cf\u6210\u529f\u6f14\u793a\u6765\u521d\u59cb\u5316\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u7684\u503c\u51fd\u6570\uff0c\u4ee5\u89e3\u51b3\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e2d\u7684\u6311\u6218\u3002", "motivation": "\u5728\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e2d\uff0c\u7531\u4e8e\u7f3a\u4e4f\u4fe1\u606f\u53cd\u9988\uff0c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4ecd\u7136\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528\u5c11\u91cf\u6210\u529f\u7684\u6f14\u793a\u6765\u521d\u59cb\u5316RL\u4ee3\u7406\u7684\u503c\u51fd\u6570\u3002\u901a\u8fc7\u4ece\u79bb\u7ebf\u6f14\u793a\u4e2d\u9884\u8ba1\u7b97\u503c\u4f30\u8ba1\uff0c\u5e76\u5c06\u5176\u7528\u4f5c\u65e9\u671f\u5b66\u4e60\u7684\u76ee\u6807\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4e3a\u4ee3\u7406\u63d0\u4f9b\u4e86\u6709\u7528\u7684\u5148\u9a8c\u77e5\u8bc6\u3002\u7136\u540e\uff0c\u4ee3\u7406\u901a\u8fc7\u6807\u51c6\u7684\u5728\u7ebf\u4ea4\u4e92\u6765\u5b8c\u5584\u8fd9\u4e9b\u4f30\u8ba1\u3002\u8fd9\u79cd\u6df7\u5408\u79bb\u7ebf\u5230\u5728\u7ebf\u8303\u5f0f\u663e\u8457\u51cf\u5c11\u4e86\u63a2\u7d22\u8d1f\u62c5\uff0c\u5e76\u63d0\u9ad8\u4e86\u7a00\u758f\u5956\u52b1\u8bbe\u7f6e\u4e2d\u7684\u6837\u672c\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u53ef\u4ee5\u52a0\u901f\u6536\u655b\uff0c\u5e76\u4e14\u5373\u4f7f\u5728\u6f14\u793a\u6570\u636e\u5f88\u5c11\u6216\u4e0d\u7406\u60f3\u7684\u60c5\u51b5\u4e0b\uff0c\u4e5f\u4f18\u4e8e\u6807\u51c6\u57fa\u7ebf\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u901a\u8fc7\u4f7f\u7528\u6f14\u793a\u6570\u636e\u6765\u521d\u59cb\u5316\u503c\u51fd\u6570\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e2d\u5f3a\u5316\u5b66\u4e60\u7684\u6837\u672c\u6548\u7387\u548c\u6536\u655b\u901f\u5ea6\u3002"}}
{"id": "2510.24473", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24473", "abs": "https://arxiv.org/abs/2510.24473", "authors": ["Lucas Buk Cardoso", "Simone Aldrey Angelo", "Yasmin Pacheco Gil Bonilha", "Fernando Maia", "Adeylson Guimar\u00e3es Ribeiro", "Maria Paula Curado", "Gisele Aparecida Fernandes", "Vanderlei Cunha Parro", "Fl\u00e1vio Almeida de Magalh\u00e3es Cipparrone", "Alexandre Dias Porto Chiavegatto Filho", "Tatiana Natasha Toporcov"], "title": "Methodology for Comparing Machine Learning Algorithms for Survival Analysis", "comment": null, "summary": "This study presents a comparative methodological analysis of six machine\nlearning models for survival analysis (MLSA). Using data from nearly 45,000\ncolorectal cancer patients in the Hospital-Based Cancer Registries of S\\~ao\nPaulo, we evaluated Random Survival Forest (RSF), Gradient Boosting for\nSurvival Analysis (GBSA), Survival SVM (SSVM), XGBoost-Cox (XGB-Cox),\nXGBoost-AFT (XGB-AFT), and LightGBM (LGBM), capable of predicting survival\nconsidering censored data. Hyperparameter optimization was performed with\ndifferent samplers, and model performance was assessed using the Concordance\nIndex (C-Index), C-Index IPCW, time-dependent AUC, and Integrated Brier Score\n(IBS). Survival curves produced by the models were compared with predictions\nfrom classification algorithms, and predictor interpretation was conducted\nusing SHAP and permutation importance. XGB-AFT achieved the best performance\n(C-Index = 0.7618; IPCW = 0.7532), followed by GBSA and RSF. The results\nhighlight the potential and applicability of MLSA to improve survival\nprediction and support decision making.", "AI": {"tldr": "\u672c\u7814\u7a76\u6bd4\u8f83\u4e86\u516d\u79cd\u7528\u4e8e\u751f\u5b58\u5206\u6790\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff08MLSA\uff09\uff0c\u8bc4\u4f30\u4e86\u5b83\u4eec\u5728\u9884\u6d4b\u7ed3\u76f4\u80a0\u764c\u60a3\u8005\u751f\u5b58\u671f\u65b9\u9762\u7684\u80fd\u529b\uff0c\u5e76\u4f7f\u7528\u4e86\u5305\u62ecC-Index\u3001IPCW C-Index\u3001\u65f6\u95f4\u4f9d\u8d56\u6027AUC\u548cBrier\u5206\u6570\u5728\u5185\u7684\u591a\u79cd\u8bc4\u4f30\u6307\u6807\u3002", "motivation": "\u4e3a\u4e86\u6bd4\u8f83\u548c\u8bc4\u4f30\u516d\u79cd\u4e0d\u540c\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u5904\u7406\u7ed3\u76f4\u80a0\u764c\u60a3\u8005\u751f\u5b58\u6570\u636e\u65b9\u9762\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u8003\u8651\u5230\u5ba1\u67e5\u6570\u636e\uff0c\u5e76\u4e3a\u6539\u5584\u751f\u5b58\u9884\u6d4b\u548c\u652f\u6301\u51b3\u7b56\u63d0\u4f9b\u4f9d\u636e\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528\u968f\u673a\u751f\u5b58\u68ee\u6797\uff08RSF\uff09\u3001\u68af\u5ea6\u63d0\u5347\u751f\u5b58\u5206\u6790\uff08GBSA\uff09\u3001\u751f\u5b58SVM\uff08SSVM\uff09\u3001XGBoost-Cox\uff08XGB-Cox\uff09\u3001XGBoost-AFT\uff08XGB-AFT\uff09\u548cLightGBM\uff08LGBM\uff09\u8fd9\u516d\u79cdMLSA\u6a21\u578b\uff0c\u4f7f\u7528\u6765\u81ea\u5723\u4fdd\u7f57\u533b\u9662\u764c\u75c7\u767b\u8bb0\u5904\u7684\u8fd145,000\u540d\u7ed3\u76f4\u80a0\u764c\u60a3\u8005\u7684\u6570\u636e\u3002\u901a\u8fc7\u8d85\u53c2\u6570\u4f18\u5316\u548c\u4f7f\u7528C-Index\u3001IPCW C-Index\u3001\u65f6\u95f4\u4f9d\u8d56\u6027AUC\u548cBrier\u5206\u6570\u7b49\u6307\u6807\u8fdb\u884c\u6a21\u578b\u6027\u80fd\u8bc4\u4f30\uff0c\u5e76\u5229\u7528SHAP\u548c\u6392\u5217\u91cd\u8981\u6027\u8fdb\u884c\u9884\u6d4b\u56e0\u5b50\u89e3\u91ca\u3002", "result": "XGB-AFT\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u5176C-Index\u4e3a0.7618\uff0cIPCW C-Index\u4e3a0.7532\uff0c\u4f18\u4e8eGBSA\u548cRSF\u7b49\u5176\u4ed6\u6a21\u578b\u3002", "conclusion": "\u673a\u5668\u5b66\u4e60\u751f\u5b58\u5206\u6790\uff08MLSA\uff09\u5728\u63d0\u9ad8\u751f\u5b58\u9884\u6d4b\u80fd\u529b\u548c\u652f\u6301\u4e34\u5e8a\u51b3\u7b56\u65b9\u9762\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u5176\u4e2dXGB-AFT\u6a21\u578b\u5728\u8be5\u9879\u7814\u7a76\u4e2d\u8868\u73b0\u51fa\u6700\u4f73\u6027\u80fd\u3002"}}
{"id": "2510.24500", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24500", "abs": "https://arxiv.org/abs/2510.24500", "authors": ["Yong Huang", "Zhongqi Yang", "Amir Rahmani"], "title": "MIMIC-Sepsis: A Curated Benchmark for Modeling and Learning from Sepsis Trajectories in the ICU", "comment": null, "summary": "Sepsis is a leading cause of mortality in intensive care units (ICUs), yet\nexisting research often relies on outdated datasets, non-reproducible\npreprocessing pipelines, and limited coverage of clinical interventions. We\nintroduce MIMIC-Sepsis, a curated cohort and benchmark framework derived from\nthe MIMIC-IV database, designed to support reproducible modeling of sepsis\ntrajectories. Our cohort includes 35,239 ICU patients with time-aligned\nclinical variables and standardized treatment data, including vasopressors,\nfluids, mechanical ventilation and antibiotics. We describe a transparent\npreprocessing pipeline-based on Sepsis-3 criteria, structured imputation\nstrategies, and treatment inclusion-and release it alongside benchmark tasks\nfocused on early mortality prediction, length-of-stay estimation, and shock\nonset classification. Empirical results demonstrate that incorporating\ntreatment variables substantially improves model performance, particularly for\nTransformer-based architectures. MIMIC-Sepsis serves as a robust platform for\nevaluating predictive and sequential models in critical care research.", "AI": {"tldr": "MIMIC-Sepsis\u662f\u4e00\u4e2a\u65b0\u7684\u6570\u636e\u96c6\u548c\u57fa\u51c6\u6846\u67b6\uff0c\u7528\u4e8e\u53ef\u91cd\u590d\u7684\u8113\u6bd2\u75c7\u8f68\u8ff9\u5efa\u6a21\uff0c\u5305\u542b\u4e8635,239\u540dICU\u60a3\u8005\u7684\u4e34\u5e8a\u53d8\u91cf\u548c\u6cbb\u7597\u6570\u636e\uff0c\u5e76\u53d1\u5e03\u4e86\u76f8\u5173\u57fa\u51c6\u4efb\u52a1\u548c\u9884\u5904\u7406\u6d41\u7a0b\u3002", "motivation": "\u73b0\u6709\u8113\u6bd2\u75c7\u7814\u7a76\u4f9d\u8d56\u8fc7\u65f6\u7684\u6570\u636e\u96c6\u3001\u4e0d\u53ef\u590d\u73b0\u7684\u9884\u5904\u7406\u6d41\u7a0b\uff0c\u5e76\u4e14\u4e34\u5e8a\u5e72\u9884\u63aa\u65bd\u8986\u76d6\u6709\u9650\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u652f\u6301\u53ef\u590d\u73b0\u5efa\u6a21\u7684\u65b0\u6846\u67b6\u3002", "method": "\u4eceMIMIC-IV\u6570\u636e\u5e93\u4e2d\u63d0\u53d6\u4e8635,239\u540dICU\u60a3\u8005\u7684\u6570\u636e\uff0c\u5305\u62ec\u65f6\u95f4\u5bf9\u9f50\u7684\u4e34\u5e8a\u53d8\u91cf\u548c\u6807\u51c6\u5316\u6cbb\u7597\u6570\u636e\uff08\u5982\u8840\u7ba1\u5347\u538b\u836f\u3001\u6db2\u4f53\u3001\u673a\u68b0\u901a\u6c14\u548c\u6297\u751f\u7d20\uff09\u3002\u91c7\u7528\u57fa\u4e8eSepsis-3\u6807\u51c6\u3001\u7ed3\u6784\u5316\u63d2\u8865\u7b56\u7565\u548c\u6cbb\u7597\u7eb3\u5165\u7684\u900f\u660e\u9884\u5904\u7406\u6d41\u7a0b\u3002\u53d1\u5e03\u4e86\u65e9\u671f\u6b7b\u4ea1\u9884\u6d4b\u3001\u4f4f\u9662\u65f6\u95f4\u4f30\u8ba1\u548c\u4f11\u514b\u53d1\u4f5c\u5206\u7c7b\u7b49\u57fa\u51c6\u4efb\u52a1\u3002", "result": "\u5c06\u6cbb\u7597\u53d8\u91cf\u7eb3\u5165\u6a21\u578b\u80fd\u663e\u8457\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u57fa\u4e8eTransformer\u7684\u67b6\u6784\u3002", "conclusion": "MIMIC-Sepsis\u4e3a\u8bc4\u4f30\u9884\u6d4b\u6027\u548c\u5e8f\u5217\u6a21\u578b\u5728\u91cd\u75c7\u76d1\u62a4\u7814\u7a76\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u5e73\u53f0\u3002"}}
{"id": "2510.24561", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24561", "abs": "https://arxiv.org/abs/2510.24561", "authors": ["Qingyue Zhang", "Chang Chu", "Tianren Peng", "Qi Li", "Xiangyang Luo", "Zhihao Jiang", "Shao-Lun Huang"], "title": "LoRA-DA: Data-Aware Initialization for Low-Rank Adaptation via Asymptotic Analysis", "comment": null, "summary": "With the widespread adoption of LLMs, LoRA has become a dominant method for\nPEFT, and its initialization methods have attracted increasing attention.\nHowever, existing methods have notable limitations: many methods do not\nincorporate target-domain data, while gradient-based methods exploit data only\nat a shallow level by relying on one-step gradient decomposition, which remains\nunsatisfactory due to the weak empirical performance of the one-step\nfine-tuning model that serves as their basis, as well as the fact that these\nmethods either lack a rigorous theoretical foundation or depend heavily on\nrestrictive isotropic assumptions. In this paper, we establish a theoretical\nframework for data-aware LoRA initialization based on asymptotic analysis.\nStarting from a general optimization objective that minimizes the expectation\nof the parameter discrepancy between the fine-tuned and target models, we\nderive an optimization problem with two components: a bias term, which is\nrelated to the parameter distance between the fine-tuned and target models, and\nis approximated using a Fisher-gradient formulation to preserve anisotropy; and\na variance term, which accounts for the uncertainty introduced by sampling\nstochasticity through the Fisher information. By solving this problem, we\nobtain an optimal initialization strategy for LoRA. Building on this\ntheoretical framework, we develop an efficient algorithm, LoRA-DA, which\nestimates the terms in the optimization problem from a small set of target\ndomain samples and obtains the optimal LoRA initialization. Empirical results\nacross multiple benchmarks demonstrate that LoRA-DA consistently improves final\naccuracy over existing initialization methods. Additional studies show faster,\nmore stable convergence, robustness across ranks, and only a small\ninitialization overhead for LoRA-DA. The source code will be released upon\npublication.", "AI": {"tldr": "LoRA-DA\u662f\u4e00\u79cd\u57fa\u4e8e\u7406\u8bba\u6846\u67b6\u7684\u6570\u636e\u611f\u77e5LoRA\u521d\u59cb\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u53c2\u6570\u5dee\u5f02\uff0c\u5e76\u5728\u5c0f\u90e8\u5206\u76ee\u6807\u57df\u6837\u672c\u4e0a\u8fdb\u884c\u4f30\u8ba1\uff0c\u80fd\u63d0\u5347\u6a21\u578b\u7cbe\u5ea6\u3001\u52a0\u901f\u6536\u655b\u5e76\u4fdd\u6301\u7a33\u5b9a\u6027\u3002", "motivation": "\u73b0\u6709LoRA\u521d\u59cb\u5316\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5982\u672a\u8003\u8651\u76ee\u6807\u57df\u6570\u636e\u3001\u57fa\u4e8e\u6d45\u5c42\u68af\u5ea6\u5206\u89e3\u4e14\u7f3a\u4e4f\u7406\u8bba\u57fa\u7840\u6216\u4f9d\u8d56\u4e8e\u4e25\u683c\u7684\u5404\u5411\u5f02\u6027\u5047\u8bbe\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u6e10\u8fd1\u5206\u6790\u7684\u6570\u636e\u611f\u77e5LoRA\u521d\u59cb\u5316\u7684\u7406\u8bba\u6846\u67b6\uff0c\u63a8\u5bfc\u51fa\u4e00\u4e2a\u5305\u542b\u504f\u5dee\u9879\uff08\u4f7f\u7528Fisher-\u68af\u5ea6\u8fd1\u4f3c\u4ee5\u4fdd\u7559\u5404\u5411\u5f02\u6027\uff09\u548c\u65b9\u5dee\u9879\uff08\u901a\u8fc7Fisher\u4fe1\u606f\u5904\u7406\u91c7\u6837\u968f\u673a\u6027\u5f15\u5165\u7684\u4e0d\u786e\u5b9a\u6027\uff09\u7684\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u636e\u6b64\u63d0\u51faLoRA-DA\u7b97\u6cd5\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLoRA-DA\u76f8\u6bd4\u73b0\u6709\u521d\u59cb\u5316\u65b9\u6cd5\u80fd\u6301\u7eed\u63d0\u5347\u6700\u7ec8\u7cbe\u5ea6\uff0c\u5e76\u5c55\u793a\u4e86\u66f4\u5feb\u901f\u3001\u66f4\u7a33\u5b9a\u7684\u6536\u655b\u6027\u3001\u8de8\u79e9\u7684\u9c81\u68d2\u6027\u4ee5\u53ca\u8f83\u5c0f\u7684\u521d\u59cb\u5316\u5f00\u9500\u3002", "conclusion": "LoRA-DA\u662f\u4e00\u79cd\u6709\u6548\u4e14\u9ad8\u6548\u7684\u6570\u636e\u611f\u77e5LoRA\u521d\u59cb\u5316\u65b9\u6cd5\uff0c\u80fd\u5728\u4e0d\u663e\u8457\u589e\u52a0\u989d\u5916\u5f00\u9500\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2510.24574", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24574", "abs": "https://arxiv.org/abs/2510.24574", "authors": ["Hao Wang", "Licheng Pan", "Yuan Lu", "Zhixuan Chu", "Xiaoxi Li", "Shuting He", "Zhichao Chen", "Haoxuan Li", "Qingsong Wen", "Zhouchen Lin"], "title": "DistDF: Time-Series Forecasting Needs Joint-Distribution Wasserstein Alignment", "comment": null, "summary": "Training time-series forecast models requires aligning the conditional\ndistribution of model forecasts with that of the label sequence. The standard\ndirect forecast (DF) approach resorts to minimize the conditional negative\nlog-likelihood of the label sequence, typically estimated using the mean\nsquared error. However, this estimation proves to be biased in the presence of\nlabel autocorrelation. In this paper, we propose DistDF, which achieves\nalignment by alternatively minimizing a discrepancy between the conditional\nforecast and label distributions. Because conditional discrepancies are\ndifficult to estimate from finite time-series observations, we introduce a\nnewly proposed joint-distribution Wasserstein discrepancy for time-series\nforecasting, which provably upper bounds the conditional discrepancy of\ninterest. This discrepancy admits tractable, differentiable estimation from\nempirical samples and integrates seamlessly with gradient-based training.\nExtensive experiments show that DistDF improves the performance diverse\nforecast models and achieves the state-of-the-art forecasting performance. Code\nis available at https://anonymous.4open.science/r/DistDF-F66B.", "AI": {"tldr": "DistDF\u901a\u8fc7\u6700\u5c0f\u5316\u6761\u4ef6\u9884\u6d4b\u4e0e\u6807\u7b7e\u5206\u5e03\u4e4b\u95f4\u7684\u5dee\u5f02\u6765\u6539\u8fdb\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff0c\u4f18\u4e8e\u6807\u51c6\u7684\u76f4\u63a5\u9884\u6d4b\u65b9\u6cd5\u3002", "motivation": "\u6807\u51c6\u7684\u76f4\u63a5\u9884\u6d4b\uff08DF\uff09\u65b9\u6cd5\u5728\u4f30\u8ba1\u6807\u7b7e\u5e8f\u5217\u7684\u6761\u4ef6\u8d1f\u5bf9\u6570\u4f3c\u7136\u65f6\u5b58\u5728\u504f\u5dee\uff0c\u5c24\u5176\u662f\u5728\u5b58\u5728\u6807\u7b7e\u81ea\u76f8\u5173\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDistDF\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u6761\u4ef6\u9884\u6d4b\u548c\u6807\u7b7e\u5206\u5e03\u4e4b\u95f4\u7684\u5dee\u5f02\u6765\u5bf9\u9f50\u5206\u5e03\u3002\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u63d0\u51fa\u7684\u8054\u5408\u5206\u5e03 Wasserstein \u5dee\u5f02\uff0c\u7528\u4e8e\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff0c\u8be5\u5dee\u5f02\u53ef\u4e0a\u754c\u6709\u754c\u6761\u4ef6\u5dee\u5f02\uff0c\u5e76\u80fd\u4ece\u7ecf\u9a8c\u6837\u672c\u4e2d\u8fdb\u884c\u6613\u4e8e\u5904\u7406\u548c\u53ef\u5fae\u5206\u7684\u4f30\u8ba1\uff0c\u4ece\u800c\u4e0e\u57fa\u4e8e\u68af\u5ea6\u7684\u8bad\u7ec3\u65e0\u7f1d\u96c6\u6210\u3002", "result": "DistDF\u5728\u5404\u79cd\u9884\u6d4b\u6a21\u578b\u4e0a\u90fd\u8868\u73b0\u51fa\u4e86\u6027\u80fd\u7684\u63d0\u5347\uff0c\u5e76\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u9762\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6210\u679c\u3002", "conclusion": "DistDF\u901a\u8fc7\u5229\u7528Wasserstein\u5dee\u5f02\u514b\u670d\u4e86\u6807\u51c6DF\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4efb\u52a1\u4e2d\u63d0\u4f9b\u4e86\u663e\u8457\u7684\u6027\u80fd\u6539\u8fdb\u3002"}}
{"id": "2510.24577", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24577", "abs": "https://arxiv.org/abs/2510.24577", "authors": ["He Yang", "Fei Ren", "Hai-Sui Yu", "Xiaohui Chen", "Pei-Zhi Zhuang"], "title": "Physics-Informed Extreme Learning Machine (PIELM): Opportunities and Challenges", "comment": null, "summary": "We are very delighted to see the fast development of physics-informed extreme\nlearning machine (PIELM) in recent years for higher computation efficiency and\naccuracy in physics-informed machine learning. As a summary or review on PIELM\nis currently not available, we would like to take this opportunity to show our\nperspective and experience for this promising research direction. We can see\nmany efforts are made to solve PDEs with sharp gradients, nonlinearities,\nhigh-frequency behavior, hard constraints, uncertainty, multiphysics coupling.\nDespite the success, many urgent challenges remain to be tackled, which also\nprovides us opportunities to develop more robust, interpretable, and\ngeneralizable PIELM frameworks with applications in science and engineering.", "AI": {"tldr": "\u7269\u7406\u4fe1\u606f\u6781\u9650\u5b66\u4e60\u673a\uff08PIELM\uff09\u5728\u8fd1\u5e74\u6765\u5f97\u5230\u4e86\u5feb\u901f\u53d1\u5c55\uff0c\u5728\u7269\u7406\u4fe1\u606f\u673a\u5668\u5b66\u4e60\u65b9\u9762\u5177\u6709\u66f4\u9ad8\u7684\u8ba1\u7b97\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u76ee\u524d\u8fd8\u6ca1\u6709\u5173\u4e8ePIELM\u7684\u603b\u7ed3\u6216\u7efc\u8ff0\uff0c\u56e0\u6b64\u6211\u4eec\u5e0c\u671b\u501f\u6b64\u673a\u4f1a\u5206\u4eab\u6211\u4eec\u5bf9\u8fd9\u4e00\u6709\u524d\u666f\u7684\u7814\u7a76\u65b9\u5411\u7684\u770b\u6cd5\u548c\u7ecf\u9a8c\u3002", "method": "\u672c\u6587\u603b\u7ed3\u4e86PIELM\u5728\u89e3\u51b3\u5177\u6709\u9661\u5ced\u68af\u5ea6\u3001\u975e\u7ebf\u6027\u3001\u9ad8\u9891\u884c\u4e3a\u3001\u786c\u7ea6\u675f\u3001\u4e0d\u786e\u5b9a\u6027\u548c\u591a\u7269\u7406\u573a\u8026\u5408\u7684\u504f\u5fae\u5206\u65b9\u7a0b\u65b9\u9762\u7684\u8fdb\u5c55\u3002", "result": "\u5c3d\u7ba1\u53d6\u5f97\u4e86\u6210\u529f\uff0c\u4f46\u4ecd\u6709\u8bb8\u591a\u7d27\u8feb\u7684\u6311\u6218\u6709\u5f85\u89e3\u51b3\uff0c\u8fd9\u4e5f\u4e3a\u6211\u4eec\u5f00\u53d1\u66f4\u9c81\u68d2\u3001\u53ef\u89e3\u91ca\u548c\u53ef\u6cdb\u5316\u7684PIELM\u6846\u67b6\u63d0\u4f9b\u4e86\u673a\u4f1a\u3002", "conclusion": "PIELM\u5728\u79d1\u5b66\u548c\u5de5\u7a0b\u9886\u57df\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2510.24598", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.24598", "abs": "https://arxiv.org/abs/2510.24598", "authors": ["Sathwik Narkedimilli", "N V Saran Kumar", "Aswath Babu H", "Manjunath K Vanahalli", "Manish M", "Vinija Jain", "Aman Chadha"], "title": "A Novel XAI-Enhanced Quantum Adversarial Networks for Velocity Dispersion Modeling in MaNGA Galaxies", "comment": null, "summary": "Current quantum machine learning approaches often face challenges balancing\npredictive accuracy, robustness, and interpretability. To address this, we\npropose a novel quantum adversarial framework that integrates a hybrid quantum\nneural network (QNN) with classical deep learning layers, guided by an\nevaluator model with LIME-based interpretability, and extended through quantum\nGAN and self-supervised variants. In the proposed model, an adversarial\nevaluator concurrently guides the QNN by computing feedback loss, thereby\noptimizing both prediction accuracy and model explainability. Empirical\nevaluations show that the Vanilla model achieves RMSE = 0.27, MSE = 0.071, MAE\n= 0.21, and R^2 = 0.59, delivering the most consistent performance across\nregression metrics compared to adversarial counterparts. These results\ndemonstrate the potential of combining quantum-inspired methods with classical\narchitectures to develop lightweight, high-performance, and interpretable\npredictive models, advancing the applicability of QML beyond current\nlimitations.", "AI": {"tldr": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u5408\u4e86\u6df7\u5408\u91cf\u5b50\u795e\u7ecf\u7f51\u7edc\uff08QNN\uff09\u548c\u7ecf\u5178\u6df1\u5ea6\u5b66\u4e60\u5c42\u7684\u91cf\u5b50\u5bf9\u6297\u6846\u67b6\uff0c\u5e76\u901a\u8fc7 LIME \u53ef\u89e3\u91ca\u6027\u8bc4\u4f30\u6a21\u578b\u8fdb\u884c\u6307\u5bfc\uff0c\u4ee5\u53ca\u901a\u8fc7\u91cf\u5b50 GAN \u548c\u81ea\u76d1\u7763\u53d8\u4f53\u8fdb\u884c\u6269\u5c55\u3002\u8be5\u6846\u67b6\u65e8\u5728\u89e3\u51b3\u5f53\u524d\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5728\u9884\u6d4b\u51c6\u786e\u6027\u3001\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u7684\u6311\u6218\u3002", "motivation": "\u5f53\u524d\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5728\u9884\u6d4b\u51c6\u786e\u6027\u3001\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\u4e4b\u95f4\u96be\u4ee5\u5e73\u8861\uff0c\u9700\u8981\u65b0\u7684\u6846\u67b6\u6765\u89e3\u51b3\u6b64\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u91cf\u5b50\u5bf9\u6297\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u6574\u5408\u4e86\u6df7\u5408\u91cf\u5b50\u795e\u7ecf\u7f51\u7edc\uff08QNN\uff09\u548c\u7ecf\u5178\u6df1\u5ea6\u5b66\u4e60\u5c42\uff0c\u5e76\u5229\u7528\u57fa\u4e8e LIME \u7684\u53ef\u89e3\u91ca\u6027\u8bc4\u4f30\u6a21\u578b\u8fdb\u884c\u6307\u5bfc\uff0c\u540c\u65f6\u901a\u8fc7\u91cf\u5b50 GAN \u548c\u81ea\u76d1\u7763\u53d8\u4f53\u8fdb\u884c\u6269\u5c55\u3002\u8bc4\u4f30\u6a21\u578b\u4f1a\u901a\u8fc7\u8ba1\u7b97\u53cd\u9988\u635f\u5931\u6765\u6307\u5bfc QNN\uff0c\u4ece\u800c\u540c\u65f6\u4f18\u5316\u9884\u6d4b\u51c6\u786e\u6027\u548c\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u5728\u56de\u5f52\u4efb\u52a1\u4e2d\uff0cVanilla \u6a21\u578b\u53d6\u5f97\u4e86 RMSE = 0.27\u3001MSE = 0.071\u3001MAE = 0.21 \u548c R^2 = 0.59 \u7684\u7ed3\u679c\uff0c\u5728\u5404\u9879\u6307\u6807\u4e0a\u8868\u73b0\u6700\u4e3a\u7a33\u5b9a\u3002", "conclusion": "\u7ed3\u5408\u91cf\u5b50\u542f\u53d1\u65b9\u6cd5\u548c\u7ecf\u5178\u67b6\u6784\u7684\u6f5c\u529b\uff0c\u53ef\u4ee5\u5f00\u53d1\u51fa\u8f7b\u91cf\u7ea7\u3001\u9ad8\u6027\u80fd\u4e14\u53ef\u89e3\u91ca\u7684\u9884\u6d4b\u6a21\u578b\uff0c\u4ece\u800c\u63a8\u52a8\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u7684\u5e94\u7528\u8d85\u8d8a\u73b0\u6709\u5c40\u9650\u3002"}}
{"id": "2510.24639", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24639", "abs": "https://arxiv.org/abs/2510.24639", "authors": ["Pedro P. Sanchez", "Damian Machlanski", "Steven McDonagh", "Sotirios A. Tsaftaris"], "title": "Causal Ordering for Structure Learning From Time Series", "comment": "32 pages", "summary": "Predicting causal structure from time series data is crucial for\nunderstanding complex phenomena in physiology, brain connectivity, climate\ndynamics, and socio-economic behaviour. Causal discovery in time series is\nhindered by the combinatorial complexity of identifying true causal\nrelationships, especially as the number of variables and time points grow. A\ncommon approach to simplify the task is the so-called ordering-based methods.\nTraditional ordering methods inherently limit the representational capacity of\nthe resulting model. In this work, we fix this issue by leveraging multiple\nvalid causal orderings, instead of a single one as standard practice. We\npropose DOTS (Diffusion Ordered Temporal Structure), using diffusion-based\ncausal discovery for temporal data. By integrating multiple orderings, DOTS\neffectively recovers the transitive closure of the underlying directed acyclic\ngraph, mitigating spurious artifacts inherent in single-ordering approaches. We\nformalise the problem under standard assumptions such as stationarity and the\nadditive noise model, and leverage score matching with diffusion processes to\nenable efficient Hessian estimation. Extensive experiments validate the\napproach. Empirical evaluations on synthetic and real-world datasets\ndemonstrate that DOTS outperforms state-of-the-art baselines, offering a\nscalable and robust approach to temporal causal discovery. On synthetic\nbenchmarks ($d{=}\\!3-\\!6$ variables, $T{=}200\\!-\\!5{,}000$ samples), DOTS\nimproves mean window-graph $F1$ from $0.63$ (best baseline) to $0.81$. On the\nCausalTime real-world benchmark ($d{=}20\\!-\\!36$), while baselines remain the\nbest on individual datasets, DOTS attains the highest average summary-graph\n$F1$ while halving runtime relative to graph-optimisation methods. These\nresults establish DOTS as a scalable and accurate solution for temporal causal\ndiscovery.", "AI": {"tldr": "DOTS\u901a\u8fc7\u5229\u7528\u591a\u4e2a\u56e0\u679c\u6392\u5e8f\u800c\u975e\u5355\u4e00\u6392\u5e8f\u6765\u6539\u8fdb\u65f6\u95f4\u5e8f\u5217\u56e0\u679c\u53d1\u73b0\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u9884\u6d4b\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u7684\u56e0\u679c\u7ed3\u6784\u5bf9\u4e8e\u7406\u89e3\u751f\u7406\u5b66\u3001\u5927\u8111\u8fde\u63a5\u3001\u6c14\u5019\u52a8\u529b\u5b66\u548c\u793e\u4f1a\u7ecf\u6d4e\u884c\u4e3a\u7b49\u590d\u6742\u73b0\u8c61\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u65f6\u95f4\u5e8f\u5217\u56e0\u679c\u53d1\u73b0\u9762\u4e34\u7740\u8bc6\u522b\u771f\u5b9e\u56e0\u679c\u5173\u7cfb\u7684\u7ec4\u5408\u590d\u6742\u6027\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u53d8\u91cf\u6570\u91cf\u548c\u65f6\u95f4\u70b9\u589e\u52a0\u65f6\u3002\u4f20\u7edf\u65b9\u6cd5\uff08\u5982\u57fa\u4e8e\u6392\u5e8f\u7684\u65b9\u6cd5\uff09\u56e0\u5176\u56fa\u6709\u7684\u8868\u793a\u80fd\u529b\u9650\u5236\u800c\u6548\u679c\u4e0d\u4f73\u3002", "method": "DOTS\uff08Diffusion Ordered Temporal Structure\uff09\u5229\u7528\u57fa\u4e8e\u6269\u6563\u7684\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\u5904\u7406\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u6574\u5408\u591a\u4e2a\u6709\u6548\u7684\u56e0\u679c\u6392\u5e8f\uff0c\u800c\u4e0d\u662f\u50cf\u6807\u51c6\u505a\u6cd5\u90a3\u6837\u53ea\u4f7f\u7528\u4e00\u4e2a\u6392\u5e8f\uff0c\u6765\u514b\u670d\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002DOTS\u5229\u7528\u6269\u6563\u8fc7\u7a0b\u8fdb\u884c\u5f97\u5206\u5339\u914d\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u6548\u7684Hessian\u4f30\u8ba1\uff0c\u5e76\u5728\u5e73\u7a33\u6027\u548c\u52a0\u6027\u566a\u58f0\u6a21\u578b\u7b49\u6807\u51c6\u5047\u8bbe\u4e0b\u5bf9\u95ee\u9898\u8fdb\u884c\u4e86\u5f62\u5f0f\u5316\u3002", "result": "DOTS\u901a\u8fc7\u6574\u5408\u591a\u4e2a\u56e0\u679c\u6392\u5e8f\uff0c\u80fd\u591f\u6709\u6548\u5730\u6062\u590d\u5e95\u5c42\u6709\u5411\u65e0\u73af\u56fe\u7684\u4f20\u9012\u95ed\u5305\uff0c\u4ece\u800c\u51cf\u8f7b\u4e86\u5355\u4e00\u6392\u5e8f\u65b9\u6cd5\u56fa\u6709\u7684\u865a\u5047\u4f2a\u5f71\u3002\u5728\u5408\u6210\u57fa\u51c6\u6d4b\u8bd5\uff08\u53d8\u91cfd=3-6\uff0c\u6837\u672cT=200-5000\uff09\u4e2d\uff0cDOTS\u5c06\u5e73\u5747\u7a97\u53e3\u56feF1\u5206\u6570\u4ece\u6700\u4f73\u57fa\u7ebf0.63\u63d0\u9ad8\u52300.81\u3002\u5728CausalTime\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\uff08d=20-36\uff09\u4e2d\uff0c\u867d\u7136\u57fa\u7ebf\u5728\u5355\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u4f46DOTS\u5b9e\u73b0\u4e86\u6700\u9ad8\u7684\u5e73\u5747\u6458\u8981\u56feF1\uff0c\u540c\u65f6\u5c06\u8fd0\u884c\u65f6\u95f4\u7f29\u77ed\u5230\u56fe\u4f18\u5316\u65b9\u6cd5\u7684\u4e00\u534a\u3002", "conclusion": "DOTS\u662f\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u51c6\u786e\u7684\u65f6\u95f4\u5e8f\u5217\u56e0\u679c\u53d1\u73b0\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u5229\u7528\u591a\u4e2a\u56e0\u679c\u6392\u5e8f\u6765\u514b\u670d\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u5404\u79cd\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002"}}
{"id": "2510.24643", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24643", "abs": "https://arxiv.org/abs/2510.24643", "authors": ["Yujun Kim", "Chaewon Moon", "Chulhee Yun"], "title": "The Cost of Robustness: Tighter Bounds on Parameter Complexity for Robust Memorization in ReLU Nets", "comment": "Accepted to NeurIPS 2025, 72 pages, 8 figures", "summary": "We study the parameter complexity of robust memorization for $\\mathrm{ReLU}$\nnetworks: the number of parameters required to interpolate any given dataset\nwith $\\epsilon$-separation between differently labeled points, while ensuring\npredictions remain consistent within a $\\mu$-ball around each training sample.\nWe establish upper and lower bounds on the parameter count as a function of the\nrobustness ratio $\\rho = \\mu / \\epsilon$. Unlike prior work, we provide a\nfine-grained analysis across the entire range $\\rho \\in (0,1)$ and obtain\ntighter upper and lower bounds that improve upon existing results. Our findings\nreveal that the parameter complexity of robust memorization matches that of\nnon-robust memorization when $\\rho$ is small, but grows with increasing $\\rho$.", "AI": {"tldr": "\u7814\u7a76ReLU\u7f51\u7edc\u7684\u9c81\u68d2\u8bb0\u5fc6\u53c2\u6570\u590d\u6742\u5ea6\uff0c\u5e76\u63d0\u4f9b\u57280\u52301\u8303\u56f4\u5185\u03c1\u7684\u7cbe\u7ec6\u5206\u6790\uff0c\u5f97\u5230\u6bd4\u73b0\u6709\u7ed3\u679c\u66f4\u7d27\u5bc6\u7684\u4e0a\u4e0b\u754c\u3002", "motivation": "\u7814\u7a76ReLU\u7f51\u7edc\u7684\u9c81\u68d2\u8bb0\u5fc6\u53c2\u6570\u590d\u6742\u5ea6\uff0c\u4ee5\u786e\u5b9a\u5728\u4e0d\u540c\u9c81\u68d2\u6027\u6bd4\u7387\u4e0b\u6240\u9700\u7684\u53c2\u6570\u6570\u91cf\u3002", "method": "\u5bf9\u9c81\u68d2\u8bb0\u5fc6\u7684\u53c2\u6570\u590d\u6742\u5ea6\u8fdb\u884c\u7cbe\u7ec6\u5206\u6790\uff0c\u8986\u76d6\u4e86\u03c1\u5c5e\u4e8e(0,1)\u7684\u6574\u4e2a\u8303\u56f4\uff0c\u5e76\u63a8\u5bfc\u51fa\u4e0a\u754c\u548c\u4e0b\u754c\u3002", "result": "\u53d1\u73b0\u9c81\u68d2\u8bb0\u5fc6\u7684\u53c2\u6570\u590d\u6742\u5ea6\u5728\u03c1\u8f83\u5c0f\u65f6\u4e0e\u975e\u9c81\u68d2\u8bb0\u5fc6\u5339\u914d\uff0c\u4f46\u968f\u7740\u03c1\u7684\u589e\u52a0\u800c\u589e\u52a0\u3002", "conclusion": "ReLU\u7f51\u7edc\u7684\u9c81\u68d2\u8bb0\u5fc6\u53c2\u6570\u590d\u6742\u5ea6\u968f\u7740\u9c81\u68d2\u6027\u6bd4\u7387\u03c1\u7684\u589e\u52a0\u800c\u589e\u52a0\uff0c\u5728\u03c1\u8f83\u5c0f\u65f6\u4e0e\u975e\u9c81\u68d2\u8bb0\u5fc6\u590d\u6742\u5ea6\u76f8\u5f53\u3002"}}
{"id": "2510.24670", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2510.24670", "abs": "https://arxiv.org/abs/2510.24670", "authors": ["Genesis Research Team", "Alejandro Dobles", "Nina Jovic", "Kenneth Leidal", "Pranav Murugan", "David C. Williams", "Drausin Wulsin", "Nate Gruver", "Christina X. Ji", "Korrawat Pruegsanusak", "Gianluca Scarpellini", "Ansh Sharma", "Wojciech Swiderski", "Andrea Bootsma", "Richard Strong Bowen", "Charlotte Chen", "Jamin Chen", "Marc Andr\u00e9 D\u00e4mgen", "Roy Tal Dew", "Benjamin DiFrancesco", "J. D. Fishman", "Alla Ivanova", "Zach Kagin", "David Li-Bland", "Zuli Liu", "Igor Morozov", "Jeffrey Ouyang-Zhang", "Frank C. Pickard IV", "Kushal S. Shah", "Ben Shor", "Gabriel Monteiro da Silva", "Maxx Tessmer", "Carl Tilbury", "Cyr Vetcher", "Daniel Zeng", "Maruan Al-Shedivat", "Aleksandra Faust", "Evan N. Feinberg", "Michael V. LeVine", "Matteus Pan"], "title": "Pearl: A Foundation Model for Placing Every Atom in the Right Location", "comment": null, "summary": "Accurately predicting the three-dimensional structures of protein-ligand\ncomplexes remains a fundamental challenge in computational drug discovery that\nlimits the pace and success of therapeutic design. Deep learning methods have\nrecently shown strong potential as structural prediction tools, achieving\npromising accuracy across diverse biomolecular systems. However, their\nperformance and utility are constrained by scarce experimental data,\ninefficient architectures, physically invalid poses, and the limited ability to\nexploit auxiliary information available at inference. To address these issues,\nwe introduce Pearl (Placing Every Atom in the Right Location), a foundation\nmodel for protein-ligand cofolding at scale. Pearl addresses these challenges\nwith three key innovations: (1) training recipes that include large-scale\nsynthetic data to overcome data scarcity; (2) architectures that incorporate an\nSO(3)-equivariant diffusion module to inherently respect 3D rotational\nsymmetries, improving generalization and sample efficiency, and (3)\ncontrollable inference, including a generalized multi-chain templating system\nsupporting both protein and non-polymeric components as well as dual\nunconditional/conditional modes. Pearl establishes a new state-of-the-art\nperformance in protein-ligand cofolding. On the key metric of generating\naccurate (RMSD < 2 \\r{A}) and physically valid poses, Pearl surpasses AlphaFold\n3 and other open source baselines on the public Runs N' Poses and PoseBusters\nbenchmarks, delivering 14.5% and 14.2% improvements, respectively, over the\nnext best model. In the pocket-conditional cofolding regime, Pearl delivers\n$3.6\\times$ improvement on a proprietary set of challenging, real-world drug\ntargets at the more rigorous RMSD < 1 \\r{A} threshold. Finally, we demonstrate\nthat model performance correlates directly with synthetic dataset size used in\ntraining.", "AI": {"tldr": "Pearl\u662f\u4e00\u4e2a\u7528\u4e8e\u86cb\u767d\u8d28-\u914d\u4f53\u5171\u6298\u53e0\u7684\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u5408\u5927\u89c4\u6a21\u5408\u6210\u6570\u636e\u3001SO(3)-\u7b49\u53d8\u6269\u6563\u6a21\u5757\u548c\u53ef\u63a7\u63a8\u7406\uff0c\u5728\u51c6\u786e\u6027\u548c\u7269\u7406\u6709\u6548\u6027\u65b9\u9762\u53d6\u5f97\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u8d85\u8d8a\u4e86AlphaFold3\u548c\u5176\u4ed6\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u51c6\u786e\u9884\u6d4b\u86cb\u767d\u8d28-\u914d\u4f53\u590d\u5408\u7269\u7684\u4e09\u7ef4\u7ed3\u6784\u662f\u8ba1\u7b97\u836f\u7269\u53d1\u73b0\u4e2d\u7684\u4e00\u4e2a\u57fa\u672c\u6311\u6218\uff0c\u9650\u5236\u4e86\u6cbb\u7597\u8bbe\u8ba1\u7684\u8fdb\u5c55\u548c\u6210\u529f\u3002\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u867d\u7136\u6709\u6f5c\u529b\uff0c\u4f46\u53d7\u5230\u6570\u636e\u7a00\u758f\u3001\u67b6\u6784\u6548\u7387\u4f4e\u4e0b\u3001\u7269\u7406\u4e0a\u65e0\u6548\u7684\u59ff\u52bf\u4ee5\u53ca\u5229\u7528\u63a8\u7406\u65f6\u53ef\u7528\u8f85\u52a9\u4fe1\u606f\u7684\u6709\u9650\u80fd\u529b\u7684\u9650\u5236\u3002", "method": "Pearl\u6a21\u578b\u901a\u8fc7\u4ee5\u4e0b\u4e09\u4e2a\u5173\u952e\u521b\u65b0\u6765\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\uff1a(1) \u5305\u542b\u5927\u89c4\u6a21\u5408\u6210\u6570\u636e\u4ee5\u514b\u670d\u6570\u636e\u7a00\u758f\u6027\u7684\u8bad\u7ec3\u65b9\u6cd5\uff1b(2) \u7ed3\u5408SO(3)-\u7b49\u53d8\u6269\u6563\u6a21\u5757\u4ee5\u56fa\u6709\u5730\u5c0a\u91cd3D\u65cb\u8f6c\u5bf9\u79f0\u6027\u7684\u67b6\u6784\uff0c\u4ece\u800c\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\u548c\u6837\u672c\u6548\u7387\uff1b(3) \u53ef\u63a7\u63a8\u7406\uff0c\u5305\u62ec\u652f\u6301\u86cb\u767d\u8d28\u548c\u975e\u805a\u5408\u7ec4\u4ef6\u7684\u901a\u7528\u591a\u94fe\u6a21\u677f\u7cfb\u7edf\u4ee5\u53ca\u53cc\u65e0\u6761\u4ef6/\u6709\u6761\u4ef6\u6a21\u5f0f\u3002", "result": "Pearl\u5728\u86cb\u767d\u8d28-\u914d\u4f53\u5171\u6298\u53e0\u65b9\u9762\u5efa\u7acb\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002\u5728\u751f\u6210\u51c6\u786e\uff08RMSD < 2 \u00c5\uff09\u4e14\u7269\u7406\u4e0a\u6709\u6548\u7684\u59ff\u52bf\u7684\u5173\u952e\u6307\u6807\u4e0a\uff0cPearl\u5728\u516c\u5f00\u7684Runs N' Poses\u548cPoseBusters\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5206\u522b\u6bd4\u6b21\u4f18\u6a21\u578b\u63d0\u9ad8\u4e8614.5%\u548c14.2%\uff0c\u4f18\u4e8eAlphaFold3\u548c\u5176\u4ed6\u5f00\u6e90\u57fa\u7ebf\u3002\u5728\u53e3\u888b\u6761\u4ef6\u5171\u6298\u53e0\u65b9\u9762\uff0cPearl\u5728\u4e13\u6709\u7684\u5177\u6709\u6311\u6218\u6027\u7684\u771f\u5b9e\u4e16\u754c\u836f\u7269\u9776\u70b9\u6570\u636e\u96c6\u4e0a\uff0c\u5728\u66f4\u4e25\u683c\u7684RMSD < 1 \u00c5\u9608\u503c\u4e0b\uff0c\u5b9e\u73b0\u4e863.6\u500d\u7684\u6539\u8fdb\u3002\u6b64\u5916\uff0c\u6a21\u578b\u6027\u80fd\u4e0e\u8bad\u7ec3\u6240\u7528\u5408\u6210\u6570\u636e\u96c6\u7684\u5927\u5c0f\u76f4\u63a5\u76f8\u5173\u3002", "conclusion": "Pearl\u6a21\u578b\u901a\u8fc7\u5176\u521b\u65b0\u7684\u8bad\u7ec3\u65b9\u6cd5\u3001\u67b6\u6784\u8bbe\u8ba1\u548c\u53ef\u63a7\u63a8\u7406\u529f\u80fd\uff0c\u5728\u86cb\u767d\u8d28-\u914d\u4f53\u5171\u6298\u53e0\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u7684\u8fdb\u6b65\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8bbe\u5b9a\u4e86\u65b0\u7684\u6027\u80fd\u6807\u51c6\uff0c\u8bc1\u660e\u4e86\u5408\u6210\u6570\u636e\u89c4\u6a21\u5bf9\u5176\u6210\u529f\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2510.24672", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.24672", "abs": "https://arxiv.org/abs/2510.24672", "authors": ["Burak Var\u0131c\u0131", "Che-Ping Tsai", "Ritabrata Ray", "Nicholas M. Boffi", "Pradeep Ravikumar"], "title": "Eigenfunction Extraction for Ordered Representation Learning", "comment": null, "summary": "Recent advances in representation learning reveal that widely used\nobjectives, such as contrastive and non-contrastive, implicitly perform\nspectral decomposition of a contextual kernel, induced by the relationship\nbetween inputs and their contexts. Yet, these methods recover only the linear\nspan of top eigenfunctions of the kernel, whereas exact spectral decomposition\nis essential for understanding feature ordering and importance. In this work,\nwe propose a general framework to extract ordered and identifiable\neigenfunctions, based on modular building blocks designed to satisfy key\ndesiderata, including compatibility with the contextual kernel and scalability\nto modern settings. We then show how two main methodological paradigms,\nlow-rank approximation and Rayleigh quotient optimization, align with this\nframework for eigenfunction extraction. Finally, we validate our approach on\nsynthetic kernels and demonstrate on real-world image datasets that the\nrecovered eigenvalues act as effective importance scores for feature selection,\nenabling principled efficiency-accuracy tradeoffs via adaptive-dimensional\nrepresentations.", "AI": {"tldr": "\u73b0\u6709\u7684\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\uff08\u5982\u5bf9\u6bd4\u5b66\u4e60\u548c\u975e\u5bf9\u6bd4\u5b66\u4e60\uff09\u867d\u7136\u80fd\u8fdb\u884c\u8c31\u5206\u89e3\uff0c\u4f46\u53ea\u80fd\u6062\u590d\u6838\u51fd\u6570\u7684\u7ebf\u6027\u5c55\u5f00\uff0c\u800c\u65e0\u6cd5\u5f97\u5230\u7cbe\u786e\u7684\u7279\u5f81\u6392\u5e8f\u548c\u91cd\u8981\u6027\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u901a\u7528\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u7ec4\u4ef6\u5b9e\u73b0\u6309\u987a\u5e8f\u6392\u5217\u4e14\u53ef\u8bc6\u522b\u7684\u7279\u5f81\u63d0\u53d6\uff0c\u8be5\u6846\u67b6\u517c\u5bb9\u4e0a\u4e0b\u6587\u6838\u51fd\u6570\u4e14\u6613\u4e8e\u6269\u5c55\u3002\u6587\u7ae0\u8fd8\u5c55\u793a\u4e86\u4f4e\u79e9\u8fd1\u4f3c\u548c\u745e\u5229\u5546\u4f18\u5316\u8fd9\u4e24\u79cd\u65b9\u6cd5\u5982\u4f55\u5e94\u7528\u4e8e\u8be5\u6846\u67b6\u3002\u6700\u540e\uff0c\u901a\u8fc7\u5728\u5408\u6210\u6838\u51fd\u6570\u4e0a\u7684\u9a8c\u8bc1\u548c\u5728\u771f\u5b9e\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u63d0\u53d6\u51fa\u7684\u7279\u5f81\u503c\u53ef\u4f5c\u4e3a\u7279\u5f81\u9009\u62e9\u7684\u91cd\u8981\u4f9d\u636e\uff0c\u4ece\u800c\u5b9e\u73b0\u57fa\u4e8e\u81ea\u9002\u5e94\u7ef4\u5ea6\u8868\u793a\u7684\u6548\u7387-\u51c6\u786e\u6027\u6743\u8861\u3002", "motivation": "\u73b0\u6709\u7684\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\u5728\u8fdb\u884c\u8c31\u5206\u89e3\u65f6\uff0c\u53ea\u80fd\u6062\u590d\u6838\u51fd\u6570\u7684\u7ebf\u6027\u5c55\u5f00\uff0c\u800c\u65e0\u6cd5\u5f97\u5230\u7cbe\u786e\u7684\u7279\u5f81\u6392\u5e8f\u548c\u91cd\u8981\u6027\uff0c\u8fd9\u963b\u788d\u4e86\u5bf9\u7279\u5f81\u6392\u5e8f\u548c\u91cd\u8981\u6027\u7684\u6df1\u5165\u7406\u89e3\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u901a\u7528\u6846\u67b6\uff0c\u5229\u7528\u6a21\u5757\u5316\u7ec4\u4ef6\u6765\u63d0\u53d6\u6309\u987a\u5e8f\u6392\u5217\u4e14\u53ef\u8bc6\u522b\u7684\u7279\u5f81\uff0c\u8be5\u6846\u67b6\u517c\u5bb9\u4e0a\u4e0b\u6587\u6838\u51fd\u6570\u5e76\u6613\u4e8e\u6269\u5c55\u3002\u6587\u7ae0\u8fd8\u8bf4\u660e\u4e86\u4f4e\u79e9\u8fd1\u4f3c\u548c\u745e\u5229\u5546\u4f18\u5316\u5982\u4f55\u4e0e\u8be5\u6846\u67b6\u7ed3\u5408\u4ee5\u63d0\u53d6\u7279\u5f81\u3002", "result": "\u5728\u5408\u6210\u6838\u51fd\u6570\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u5e76\u5728\u771f\u5b9e\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u63d0\u53d6\u51fa\u7684\u7279\u5f81\u503c\u53ef\u4f5c\u4e3a\u7279\u5f81\u9009\u62e9\u7684\u91cd\u8981\u4f9d\u636e\uff0c\u5b9e\u73b0\u4e86\u57fa\u4e8e\u81ea\u9002\u5e94\u7ef4\u5ea6\u8868\u793a\u7684\u6548\u7387-\u51c6\u786e\u6027\u6743\u8861\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u901a\u7528\u6846\u67b6\u80fd\u591f\u7cbe\u786e\u63d0\u53d6\u7279\u5f81\uff0c\u5e76\u5c06\u7279\u5f81\u503c\u7528\u4f5c\u7279\u5f81\u9009\u62e9\u7684\u91cd\u8981\u4f9d\u636e\uff0c\u4ece\u800c\u5728\u6548\u7387\u548c\u51c6\u786e\u6027\u4e4b\u95f4\u53d6\u5f97\u826f\u597d\u7684\u5e73\u8861\u3002"}}
{"id": "2510.24700", "categories": ["cs.LG", "cs.AI", "cs.IT", "math.IT", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.24700", "abs": "https://arxiv.org/abs/2510.24700", "authors": ["Di Wu", "Chengshuai Shi", "Jing Yang", "Cong Shen"], "title": "Greedy Sampling Is Provably Efficient for RLHF", "comment": "NeurIPS 2025", "summary": "Reinforcement Learning from Human Feedback (RLHF) has emerged as a key\ntechnique for post-training large language models. Despite its empirical\nsuccess, the theoretical understanding of RLHF is still limited, as learning\nthe KL-regularized target with only preference feedback poses additional\nchallenges compared with canonical RL. Existing works mostly study the\nreward-based Bradley-Terry (BT) preference model, and extend classical designs\nutilizing optimism or pessimism. This work, instead, considers the general\npreference model (whose practical relevance has been observed recently) and\nobtains performance guarantees with major, order-wise improvements over\nexisting ones. Surprisingly, these results are derived from algorithms that\ndirectly use the empirical estimates (i.e., greedy sampling), as opposed to\nconstructing optimistic or pessimistic estimates in previous works. This\ninsight has a deep root in the unique structural property of the optimal policy\nclass under the KL-regularized target, and we further specialize it to the BT\nmodel, highlighting the surprising sufficiency of greedy sampling in RLHF.", "AI": {"tldr": "RLHF\u7406\u8bba\u7814\u7a76\u53d7\u9650\uff0c\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u901a\u7528\u504f\u597d\u6a21\u578b\u7684\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u5728KL\u6b63\u5219\u5316\u76ee\u6807\u4e0b\u5b9e\u73b0\u4e86\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\u5177\u6709\u4e3b\u8981\u3001\u9636\u6570\u7ea7\u6539\u8fdb\u7684\u6027\u80fd\u4fdd\u8bc1\uff0c\u5e76\u4e14\u4ec5\u4f7f\u7528\u7ecf\u9a8c\u4f30\u8ba1\uff08\u5373\u8d2a\u5a6a\u91c7\u6837\uff09\u3002", "motivation": "RLHF\u5728\u540e\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u65b9\u9762\u53d6\u5f97\u4e86\u7ecf\u9a8c\u4e0a\u7684\u6210\u529f\uff0c\u4f46\u5176\u7406\u8bba\u7406\u89e3\u4ecd\u7136\u6709\u9650\uff0c\u5c24\u5176\u662f\u5728\u4ec5\u6709\u504f\u597d\u53cd\u9988\u7684\u60c5\u51b5\u4e0b\u5b66\u4e60KL\u6b63\u5219\u5316\u76ee\u6807\u5e26\u6765\u4e86\u989d\u5916\u7684\u6311\u6218\u3002\u73b0\u6709\u5de5\u4f5c\u4e3b\u8981\u7814\u7a76\u57fa\u4e8e\u5956\u52b1\u7684Bradley-Terry\uff08BT\uff09\u504f\u597d\u6a21\u578b\uff0c\u5e76\u5229\u7528\u4e50\u89c2\u6216\u60b2\u89c2\u4e3b\u4e49\u8fdb\u884c\u6269\u5c55\u3002", "method": "\u8be5\u7814\u7a76\u8003\u8651\u4e86\u901a\u7528\u7684\u504f\u597d\u6a21\u578b\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u5f97\u5230\u4e86\u5177\u6709\u4e3b\u8981\u3001\u9636\u6570\u7ea7\u6539\u8fdb\u7684\u6027\u80fd\u4fdd\u8bc1\u3002\u8be5\u7b97\u6cd5\u76f4\u63a5\u4f7f\u7528\u7ecf\u9a8c\u4f30\u8ba1\uff08\u5373\u8d2a\u5a6a\u91c7\u6837\uff09\uff0c\u800c\u4e0d\u662f\u50cf\u4ee5\u5f80\u5de5\u4f5c\u90a3\u6837\u6784\u5efa\u4e50\u89c2\u6216\u60b2\u89c2\u4f30\u8ba1\u3002", "result": "\u7b97\u6cd5\u76f4\u63a5\u4f7f\u7528\u7ecf\u9a8c\u4f30\u8ba1\uff08\u5373\u8d2a\u5a6a\u91c7\u6837\uff09\u53d6\u5f97\u4e86\u6027\u80fd\u4fdd\u8bc1\uff0c\u5e76\u4e14\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\u5177\u6709\u4e3b\u8981\u3001\u9636\u6570\u7ea7\u6539\u8fdb\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86KL\u6b63\u5219\u5316\u76ee\u6807\u4e0b\u6700\u4f18\u7b56\u7565\u7c7b\u7684\u72ec\u7279\u7ed3\u6784\u5c5e\u6027\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8eBT\u6a21\u578b\uff0c\u8bc1\u660e\u4e86\u8d2a\u5a6a\u91c7\u6837\u5728RLHF\u4e2d\u7684\u5145\u5206\u6027\u3002"}}
{"id": "2509.00778", "categories": ["cs.AR", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.00778", "abs": "https://arxiv.org/abs/2509.00778", "authors": ["Pragun Jaswal", "L. Hemanth Krishna", "B. Srinivasu"], "title": "Energy Efficient Exact and Approximate Systolic Array Architecture for Matrix Multiplication", "comment": "Submitted to 39th International Conference on VLSI Design, 2026", "summary": "Deep Neural Networks (DNNs) require highly efficient matrix multiplication\nengines for complex computations. This paper presents a systolic array\narchitecture incorporating novel exact and approximate processing elements\n(PEs), designed using energy-efficient positive partial product and negative\npartial product cells, termed as PPC and NPPC, respectively. The proposed 8-bit\nexact and approximate PE designs are employed in a 8x8 systolic array, which\nachieves a energy savings of 22% and 32%, respectively, compared to the\nexisting design. To demonstrate their effectiveness, the proposed PEs are\nintegrated into a systolic array (SA) for Discrete Cosine Transform (DCT)\ncomputation, achieving high output quality with a PSNR of 38.21,dB.\nFurthermore, in an edge detection application using convolution, the\napproximate PE achieves a PSNR of 30.45,dB. These results highlight the\npotential of the proposed design to deliver significant energy efficiency while\nmaintaining competitive output quality, making it well-suited for\nerror-resilient image and vision processing applications.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5305\u542b\u65b0\u578b\u7cbe\u786e\u548c\u8fd1\u4f3c\u5904\u7406\u5355\u5143\uff08PE\uff09\u7684\u8109\u52a8\u9635\u5217\u67b6\u6784\uff0c\u4ee5\u63d0\u9ad8DNN\u8ba1\u7b97\u7684\u80fd\u6548\u3002", "motivation": "\u4e3a\u4e86\u6ee1\u8db3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNN\uff09\u5bf9\u9ad8\u6548\u77e9\u9635\u4e58\u6cd5\u5f15\u64ce\u7684\u9700\u6c42\uff0c\u672c\u6587\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u7684\u8109\u52a8\u9635\u5217\u67b6\u6784\u3002", "method": "\u8be5\u67b6\u6784\u91c7\u7528\u4e86\u80fd\u91cf\u6548\u7387\u9ad8\u7684\u6b63\u504f\u79ef\uff08PPC\uff09\u548c\u8d1f\u504f\u79ef\uff08NPPC\uff09\u5355\u5143\uff0c\u5e76\u8bbe\u8ba1\u4e868\u4f4d\u7cbe\u786e\u548c\u8fd1\u4f3cPE\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a8x8\u7684\u8109\u52a8\u9635\u5217\u3002", "result": "\u4e0e\u73b0\u6709\u8bbe\u8ba1\u76f8\u6bd4\uff0c\u8be5\u67b6\u6784\u5b9e\u73b0\u4e8622%\uff08\u7cbe\u786ePE\uff09\u548c32%\uff08\u8fd1\u4f3cPE\uff09\u7684\u80fd\u8017\u8282\u7701\u3002\u5728DCT\u8ba1\u7b97\u4e2d\uff0cPSNR\u8fbe\u523038.21dB\uff1b\u5728\u8fb9\u7f18\u68c0\u6d4b\u5e94\u7528\u4e2d\uff0c\u8fd1\u4f3cPE\u7684PSNR\u4e3a30.45dB\u3002", "conclusion": "\u63d0\u51fa\u7684PE\u8bbe\u8ba1\u5728\u4fdd\u6301\u53ef\u63a5\u53d7\u7684\u8f93\u51fa\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u80fd\u6548\uff0c\u9002\u7528\u4e8e\u5bf9\u9519\u8bef\u4e0d\u654f\u611f\u7684\u56fe\u50cf\u548c\u89c6\u89c9\u5904\u7406\u5e94\u7528\u3002"}}
