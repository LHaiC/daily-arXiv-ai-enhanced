{"id": "2507.13574", "categories": ["quant-ph", "physics.app-ph", "physics.ins-det"], "pdf": "https://arxiv.org/pdf/2507.13574", "abs": "https://arxiv.org/abs/2507.13574", "authors": ["Yong-Bok Lee", "Connor Devitt", "Xu Zhu", "Nicholas Yost", "Yabei Gu", "Sunil A. Bhave"], "title": "Cryogenic Performance Evaluation of Commercial SP4T Microelectromechanical Switch for Quantum Computing Applications", "comment": null, "summary": "Superconducting quantum computers have emerged as a leading platform for\nnext-generation computing, offering exceptional scalability and unprecedented\ncomputational speeds. However, scaling these systems to millions of qubits for\npractical applications poses substantial challenges, particularly due to\ninterconnect bottlenecks. To address this challenge, extensive research has\nfocused on developing cryogenic multiplexers that enable minimal wiring between\nroom-temperature electronics and quantum processors. This paper investigates\nthe viability of commercial microelectromechanical system (MEMS) switches for\ncryogenic multiplexers in large-scale quantum computing systems. DC and RF\ncharacteristics of the MEMS switches are evaluated at cryogenic temperatures (<\n10 K) through finite element simulations and experimental measurements. Our\nresults demonstrate that MEMS switches exhibit improved on-resistance, lower\noperating voltage, and superior RF performance at cryogenic temperatures, with\nreliable operation over 100 million cycles. Furthermore, stable single-pole\nfour-throw (SP4T) switching and logical operations, including NAND and NOR\ngates, are demonstrated at cryogenic temperatures, validating their potential\nfor quantum computing. These results underscore the promise of MEMS switches in\nrealizing large-scale quantum computing systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u5546\u7528MEMS\u5f00\u5173\u5728\u8d85\u5bfc\u91cf\u5b50\u8ba1\u7b97\u673a\u4f4e\u6e29\u590d\u7528\u5668\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u53d1\u73b0\u5728\u4f4e\u6e29\u4e0b\u5176\u6027\u80fd\u5f97\u5230\u63d0\u5347\u4e14\u8fd0\u884c\u53ef\u9760\uff0c\u53ef\u7528\u4e8e\u5b9e\u73b0\u5927\u89c4\u6a21\u91cf\u5b50\u8ba1\u7b97\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u8d85\u5bfc\u91cf\u5b50\u8ba1\u7b97\u673a\u6269\u5c55\u5230\u6570\u767e\u4e07\u4e2a\u91cf\u5b50\u6bd4\u7279\u4ee5\u5b9e\u73b0\u5b9e\u9645\u5e94\u7528\u6240\u9762\u4e34\u7684\u4e92\u8fde\u74f6\u9888\u6311\u6218\uff0c\u672c\u7814\u7a76\u65e8\u5728\u7814\u7a76\u5546\u7528MEMS\u5f00\u5173\u4f5c\u4e3a\u5927\u89c4\u6a21\u91cf\u5b50\u8ba1\u7b97\u7cfb\u7edf\u4e2d\u4f4e\u6e29\u590d\u7528\u5668\u7684\u53ef\u884c\u6027\u3002", "method": "\u901a\u8fc7\u6709\u9650\u5143\u6a21\u62df\u548c\u5b9e\u9a8c\u6d4b\u91cf\u5728\u4f4e\u6e29\uff08< 10 K\uff09\u4e0b\u8bc4\u4f30MEMS\u5f00\u5173\u7684\u76f4\u6d41\u548c\u5c04\u9891\u7279\u6027\u3002", "result": "MEMS\u5f00\u5173\u5728\u4f4e\u6e29\u4e0b\u8868\u73b0\u51fa\u6539\u8fdb\u7684\u5bfc\u901a\u7535\u963b\u3001\u66f4\u4f4e\u7684\u9a71\u52a8\u7535\u538b\u548c\u5353\u8d8a\u7684\u5c04\u9891\u6027\u80fd\uff0c\u5e76\u4e14\u5177\u6709\u8d85\u8fc71\u4ebf\u6b21\u7684\u53ef\u9760\u8fd0\u884c\u3002\u6b64\u5916\uff0c\u8fd8\u6f14\u793a\u4e86\u5728\u4f4e\u6e29\u4e0b\u7a33\u5b9a\u7684\u5355\u5200\u56db\u63b7\uff08SP4T\uff09\u5f00\u5173\u548c\u903b\u8f91\u95e8\uff08\u5305\u62ecNAND\u548cNOR\u95e8\uff09\u3002", "conclusion": "MEMS\u5f00\u5173\u5728\u4f4e\u6e29\u4e0b\u8868\u73b0\u51fa\u6539\u8fdb\u7684\u5bfc\u901a\u7535\u963b\u3001\u66f4\u4f4e\u7684\u9a71\u52a8\u7535\u538b\u548c\u5353\u8d8a\u7684\u5c04\u9891\u6027\u80fd\uff0c\u5e76\u4e14\u5177\u6709\u8d85\u8fc71\u4ebf\u6b21\u7684\u53ef\u9760\u8fd0\u884c\uff0c\u5c55\u793a\u4e86\u5176\u5728\u5b9e\u73b0\u5927\u89c4\u6a21\u91cf\u5b50\u8ba1\u7b97\u7cfb\u7edf\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.13601", "categories": ["cs.DC", "cs.ET", "cs.PF", "90B36, 90C27, 68M14, 68W40", "C.1.2; C.1.4; C.3.1; D.1.3; G.1.6"], "pdf": "https://arxiv.org/pdf/2507.13601", "abs": "https://arxiv.org/abs/2507.13601", "authors": ["Jorge Villarrubia", "Luis Costero", "Francisco D. Igual", "Katzalin Olcoz"], "title": "Leveraging Multi-Instance GPUs through moldable task scheduling", "comment": null, "summary": "NVIDIA MIG (Multi-Instance GPU) allows partitioning a physical GPU into\nmultiple logical instances with fully-isolated resources, which can be\ndynamically reconfigured. This work highlights the untapped potential of MIG\nthrough moldable task scheduling with dynamic reconfigurations. Specifically,\nwe propose a makespan minimization problem for multi-task execution under MIG\nconstraints. Our profiling shows that assuming monotonicity in task work with\nrespect to resources is not viable, as is usual in multicore scheduling.\nRelying on a state-of-the-art proposal that does not require such an\nassumption, we present FAR, a 3-phase algorithm to solve the problem. Phase 1\nof FAR builds on a classical task moldability method, phase 2 combines Longest\nProcessing Time First and List Scheduling with a novel repartitioning tree\nheuristic tailored to MIG constraints, and phase 3 employs local search via\ntask moves and swaps. FAR schedules tasks in batches offline, concatenating\ntheir schedules on the fly in an improved way that favors resource reuse.\nExcluding reconfiguration costs, the List Scheduling proof shows an\napproximation factor of 7/4 on the NVIDIA A30 model. We adapt the technique to\nthe particular constraints of an NVIDIA A100/H100 to obtain an approximation\nfactor of 2. Including the reconfiguration cost, our real-world experiments\nreveal a makespan with respect to the optimum no worse than 1.22x for a\nwell-known suite of benchmarks, and 1.10x for synthetic inputs inspired by real\nkernels. We obtain good experimental results for each batch of tasks, but also\nin the concatenation of batches, with large improvements over the\nstate-of-the-art and proposals without GPU reconfiguration. Beyond the\nalgorithm, the paper demonstrates the research potential of the MIG technology\nand suggests useful metrics, workload characterizations and evaluation\ntechniques for future work in this field.", "AI": {"tldr": "NVIDIA MIG\u6280\u672f\u53ef\u4ee5\u901a\u8fc7\u53ef\u5851\u6027\u4efb\u52a1\u8c03\u5ea6\u548c\u52a8\u6001\u91cd\u914d\u7f6e\u6765\u4f18\u5316GPU\u8d44\u6e90\u5229\u7528\u7387\u3002\u7814\u7a76\u63d0\u51fa\u7684FAR\u7b97\u6cd5\u901a\u8fc7\u4e09\u9636\u6bb5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u591a\u4efb\u52a1\u6267\u884c\u7684makespan\u6700\u5c0f\u5316\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u8be5\u7814\u7a76\u7684\u52a8\u673a\u5728\u4e8e\u53d1\u6398NVIDIA MIG\uff08Multi-Instance GPU\uff09\u6280\u672f\u7684\u6f5c\u529b\uff0c\u8be5\u6280\u672f\u5141\u8bb8\u5c06\u7269\u7406GPU\u5212\u5206\u4e3a\u591a\u4e2a\u903b\u8f91\u5b9e\u4f8b\uff0c\u5e76\u80fd\u52a8\u6001\u91cd\u914d\u7f6e\u3002\u7814\u7a76\u8005\u5e0c\u671b\u901a\u8fc7\u53ef\u5851\u6027\u4efb\u52a1\u8c03\u5ea6\u548c\u52a8\u6001\u91cd\u914d\u7f6e\u6765\u6700\u5927\u5316MIG\u7684\u5229\u7528\u7387\uff0c\u89e3\u51b3\u591a\u4efb\u52a1\u6267\u884c\u4e0b\u7684makespan\u6700\u5c0f\u5316\u95ee\u9898\uff0c\u5e76\u63a2\u7d22MIG\u6280\u672f\u5728GPU\u8d44\u6e90\u8c03\u5ea6\u9886\u57df\u7684\u7814\u7a76\u524d\u666f\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFAR\u7684\u4e09\u9636\u6bb5\u7b97\u6cd5\u6765\u89e3\u51b3\u5728NVIDIA MIG\u73af\u5883\u4e0b\u591a\u4efb\u52a1\u6267\u884c\u7684makespan\u6700\u5c0f\u5316\u95ee\u9898\u3002FAR\u7b97\u6cd5\u9996\u5148\u5229\u7528\u7ecf\u5178\u7684task moldability\u65b9\u6cd5\uff0c\u7136\u540e\u7ed3\u5408Longest Processing Time First\u548cList Scheduling\uff0c\u5e76\u5f15\u5165\u4e86\u9488\u5bf9MIG\u7ea6\u675f\u7684repartitioning tree\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u6700\u540e\u901a\u8fc7task moves\u548cswaps\u8fdb\u884c\u5c40\u90e8\u641c\u7d22\u3002\u8be5\u7b97\u6cd5\u80fd\u591f\u5904\u7406\u4efb\u52a1\u53ef\u5851\u6027\uff0c\u5e76\u8003\u8651\u4e86GPU\u7684\u52a8\u6001\u91cd\u6784\u3002", "result": "FAR\u7b97\u6cd5\u5728NVIDIA A30\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e867/4\u7684\u8fd1\u4f3c\u56e0\u5b50\uff08\u4e0d\u8003\u8651\u91cd\u6784\u6210\u672c\uff09\uff0c\u5728NVIDIA A100/H100\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e862\u7684\u8fd1\u4f3c\u56e0\u5b50\u3002\u5b9e\u9645\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u8003\u8651\u91cd\u6784\u6210\u672c\u7684\u60c5\u51b5\u4e0b\uff0c\u5bf9\u4e8e\u57fa\u51c6\u6d4b\u8bd5\u96c6\uff0c\u5176makespan\u4e0e\u6700\u4f18\u503c\u7684\u6bd4\u7387\u4e0d\u8d85\u8fc71.22\u500d\uff1b\u5bf9\u4e8e\u53d7\u771f\u5b9e\u5185\u6838\u542f\u53d1\u7684\u5408\u6210\u8f93\u5165\uff0c\u8be5\u6bd4\u7387\u4e0d\u8d85\u8fc71.10\u500d\u3002FAR\u7b97\u6cd5\u5728\u6279\u5904\u7406\u548c\u6279\u6b21\u8fde\u63a5\u65b9\u9762\u5747\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u548c\u4e0d\u8003\u8651GPU\u91cd\u6784\u7684\u65b9\u6848\u7684\u5b9e\u9a8c\u7ed3\u679c\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86NVIDIA MIG\u6280\u672f\u7684\u6f5c\u529b\uff0c\u5e76\u901a\u8fc7\u63d0\u51fa\u7684FAR\u7b97\u6cd5\u5728GPU\u8d44\u6e90\u8c03\u5ea6\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6539\u8fdb\u3002FAR\u7b97\u6cd5\u901a\u8fc7\u4e09\u9636\u6bb5\u65b9\u6cd5\uff0c\u8003\u8651\u4e86\u4efb\u52a1\u7684\u53ef\u5851\u6027\u548cGPU\u7684\u52a8\u6001\u91cd\u6784\uff0c\u4e3a\u591a\u4efb\u52a1\u6267\u884c\u7684makespan\u6700\u5c0f\u5316\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u7684\u6027\u80fd\u3002"}}
{"id": "2507.13470", "categories": ["cs.DS", "cs.DC"], "pdf": "https://arxiv.org/pdf/2507.13470", "abs": "https://arxiv.org/abs/2507.13470", "authors": ["Michael Elkin", "Chhaya Trehan"], "title": "Faster Multi-Source Reachability and Approximate Distances via Shortcuts, Hopsets and Matrix Multiplication", "comment": null, "summary": "Given an $n$-vertex $m$-edge digraph $G = (V,E)$ and a subset $S \\subseteq V$\nof $|S| = n^{\\sigma}$ (for some $0 \\le \\sigma \\le 1$) designated sources, the\n$S \\times V$ reachability problem is to compute the sets $\\mathcal V_s$ of\nvertices reachable from $s$, for every $s \\in S$. Naive centralized algorithms\nrun BFS/DFS from each source in $O(m \\cdot n^{\\sigma})$ time or compute $G$'s\ntransitive closure in $\\hat O(n^{\\omega})$ time, where $\\omega \\le\n2.371552\\ldots$ is the matrix multiplication exponent. Thus, the best known\nbound is $\\hat O(n^{\\min \\{ 2 + \\sigma, \\omega\\}})$. Leveraging shortcut\nconstructions by Kogan and Parter [SODA 2022, ICALP 2022], we develop a\ncentralized algorithm with running time $\\hat O(n^{1 + \\frac{2}{3}\n\\omega(\\sigma)})$, where $\\omega(\\sigma)$ is the rectangular matrix\nmultiplication exponent. Using current estimates on $\\omega(\\sigma)$, our\nexponent improves upon $\\min \\{2 + \\sigma, \\omega \\}$ for $\\tilde \\sigma \\leq\n\\sigma \\leq 0.53$, where $1/3 < \\tilde \\sigma < 0.3336$ is a universal\nconstant.\n  In a classical result, Cohen [Journal of Algorithms, 1996] devised parallel\nalgorithms for $S \\times V$ reachability on graphs admitting balanced recursive\nseparators of size $n^{\\rho}$ for $\\rho < 1$, requiring polylogarithmic time\nand work $n^{\\max \\{\\omega \\rho, 2\\rho + \\sigma \\} + o(1)}$. We significantly\nimprove, extend, and generalize Cohen's result. First, our parallel algorithm\nfor graphs with small recursive separators has lower work complexity than\nCohen's in boraod paramater ranges. Second, we generalize our algorithm to\ngraphs of treewidth at most $n^{\\rho}$ ($\\rho < 1$) and provide a centralized\nalgorithm that outperforms existing bounds for $S \\times V$ reachability on\nsuch graphs. We also do this for some other graph familes with small\nseparators. Finally, we extend these results to $(1 + \\epsilon)$-approximate\ndistance computation.", "AI": {"tldr": "\u672c\u6587\u6539\u8fdb\u4e86 S \u00d7 V \u53ef\u8fbe\u6027\u95ee\u9898\u7684\u7b97\u6cd5\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u5177\u6709\u5c0f\u9012\u5f52\u5206\u79bb\u5668\u548c\u754c\u9650\u6811\u5bbd\u7684\u56fe\u4e0a\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u89e3\u51b3 S \u00d7 V \u53ef\u8fbe\u6027\u95ee\u9898\uff0c\u5e76\u6539\u8fdb\u73b0\u6709\u7b97\u6cd5\u5728\u5e76\u884c\u8ba1\u7b97\u548c\u5206\u5e03\u5f0f\u8ba1\u7b97\u4e2d\u7684\u6027\u80fd\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5206\u5e03\u5f0f\u7b97\u6cd5\uff0c\u5229\u7528 Kogan \u548c Parter \u7684\u5feb\u6377\u6784\u9020\uff0c\u5b9e\u73b0\u4e86\u6bd4\u73b0\u6709\u7b97\u6cd5\u66f4\u4f18\u7684\u8fd0\u884c\u65f6\u95f4\u3002\u8be5\u7b97\u6cd5\u9002\u7528\u4e8e\u5177\u6709\u5c0f\u9012\u5f52\u5206\u79bb\u5668\u7684\u56fe\uff0c\u5e76\u63a8\u5e7f\u5230\u5177\u6709\u754c\u9650\u6811\u5bbd\u7684\u56fe\u3002", "result": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5206\u5e03\u5f0f\u7b97\u6cd5\uff0c\u5728\u7279\u5b9a\u53c2\u6570\u8303\u56f4\u5185\u4f18\u4e8e\u73b0\u6709\u7b97\u6cd5\u3002\u5bf9\u4e8e\u5177\u6709\u754c\u9650\u6811\u5bbd\u7684\u56fe\uff0c\u4e5f\u63d0\u4f9b\u4e86\u4f18\u4e8e\u73b0\u6709\u7b97\u6cd5\u7684\u4e2d\u5fc3\u5316\u7b97\u6cd5\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5206\u5e03\u5f0f\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3 S \u00d7 V \u53ef\u8fbe\u6027\u95ee\u9898\u3002\u8be5\u7b97\u6cd5\u5728\u5177\u6709\u5c0f\u9012\u5f52\u5206\u79bb\u5668\u7684\u56fe\u4e0a\u5b9e\u73b0\u4e86\u6bd4\u73b0\u6709\u7b97\u6cd5\u66f4\u4f4e\u7684\u5e76\u884c\u65f6\u95f4\u590d\u6742\u5ea6\uff0c\u5e76\u5c06\u5176\u63a8\u5e7f\u5230\u5177\u6709\u754c\u9650\u6811\u5bbd\u7684\u56fe\u3002\u6b64\u5916\uff0c\u8fd8\u6269\u5c55\u5230\u8fd1\u4f3c\u8ddd\u79bb\u8ba1\u7b97\u3002"}}
{"id": "2507.13357", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.13357", "abs": "https://arxiv.org/abs/2507.13357", "authors": ["Atharva Bhargude", "Ishan Gonehal", "Chandler Haney", "Dave Yoon", "Kevin Zhu", "Aaron Sandoval", "Sean O'Brien", "Kaustubh Vinnakota"], "title": "Adaptive Linguistic Prompting (ALP) Enhances Phishing Webpage Detection in Multimodal Large Language Models", "comment": "Published at ACL 2025 SRW, 9 pages, 3 figures", "summary": "Phishing attacks represent a significant cybersecurity threat, necessitating\nadaptive detection techniques. This study explores few-shot Adaptive Linguistic\nPrompting (ALP) in detecting phishing webpages through the multimodal\ncapabilities of state-of-the-art large language models (LLMs) such as GPT-4o\nand Gemini 1.5 Pro. ALP is a structured semantic reasoning method that guides\nLLMs to analyze textual deception by breaking down linguistic patterns,\ndetecting urgency cues, and identifying manipulative diction commonly found in\nphishing content. By integrating textual, visual, and URL-based analysis, we\npropose a unified model capable of identifying sophisticated phishing attempts.\nOur experiments demonstrate that ALP significantly enhances phishing detection\naccuracy by guiding LLMs through structured reasoning and contextual analysis.\nThe findings highlight the potential of ALP-integrated multimodal LLMs to\nadvance phishing detection frameworks, achieving an F1-score of 0.93,\nsurpassing traditional approaches. These results establish a foundation for\nmore robust, interpretable, and adaptive linguistic-based phishing detection\nsystems using LLMs.", "AI": {"tldr": "This study proposes Adaptive Linguistic Prompting (ALP), a method using multimodal LLMs like GPT-4o and Gemini 1.5 Pro to detect phishing webpages by analyzing text, visuals, and URLs. ALP achieved an F1-score of 0.93, outperforming traditional methods and providing a foundation for more robust and adaptive phishing detection systems.", "motivation": "Phishing attacks represent a significant cybersecurity threat, necessitating adaptive detection techniques.", "method": "The study explores few-shot Adaptive Linguistic Prompting (ALP) in detecting phishing webpages through the multimodal capabilities of state-of-the-art large language models (LLMs) such as GPT-4o and Gemini 1.5 Pro. ALP is a structured semantic reasoning method that guides LLMs to analyze textual deception by breaking down linguistic patterns, detecting urgency cues, and identifying manipulative diction commonly found in phishing content. By integrating textual, visual, and URL-based analysis, a unified model capable of identifying sophisticated phishing attempts is proposed.", "result": "Experiments demonstrate that ALP significantly enhances phishing detection accuracy by guiding LLMs through structured reasoning and contextual analysis, achieving an F1-score of 0.93, surpassing traditional approaches.", "conclusion": "ALP-integrated multimodal LLMs have the potential to advance phishing detection frameworks, achieving an F1-score of 0.93, surpassing traditional approaches. These results establish a foundation for more robust, interpretable, and adaptive linguistic-based phishing detection systems using LLMs."}}
{"id": "2507.13522", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.13522", "abs": "https://arxiv.org/abs/2507.13522", "authors": ["Ankit Bhardwaj", "Weiyang Wang", "Jeremy Carin", "Adam Belay", "Manya Ghobadi"], "title": "Checkmate: Zero-Overhead Model Checkpointing via Network Gradient Replication", "comment": "18 pages, 11 figures", "summary": "This paper presents Checkmate, a system that enables per-iteration\ncheckpointing in DNN training without any training slowdown. The traditional\napproach to checkpointing requires a pause in training to copy model states to\na separate location, allowing the state to be restored in the event of failure.\nThis approach fundamentally has a tradeoff between the frequency of checkpoints\nand the cost of a failure. We avoid this tradeoff; our key insight is that in\ndata-parallel training, all information necessary to create a checkpoint\nalready exists in the network as gradients. Our core contribution is a new\nmulticast abstraction that simultaneously delivers gradients to a separate\nCPU-based shadow cluster. The shadow maintains a checkpoint by applying those\ngradients to a copy of the model. Our evaluation shows that Checkmate performs\nper-iteration checkpointing with training throughput comparable to an ideal\nno-checkpoint baseline. Checkmate achieves 5 to 34.5x more frequent\ncheckpointing compared to state-of-the-art checkpointing systems, resulting in\n80% to 97.1% reduction in repeated work per failure. At the same checkpointing\nfrequency, Checkmate delivers 1.3x to 6.5x throughput compared to other\nsystems.", "AI": {"tldr": "Checkmate enables efficient per-iteration checkpointing in DNN training by using gradients and a shadow cluster, eliminating training slowdown and reducing failure-related work.", "motivation": "Traditional checkpointing methods create a tradeoff between checkpoint frequency and failure cost. This paper aims to eliminate this tradeoff by enabling per-iteration checkpointing without any training slowdown.", "method": "Checkmate utilizes a new multicast abstraction to deliver gradients to a separate CPU-based shadow cluster, which applies these gradients to a model copy to create checkpoints, thereby avoiding the traditional checkpointing pause.", "result": "Checkmate achieves per-iteration checkpointing with throughput comparable to a no-checkpoint baseline. It offers 5 to 34.5x more frequent checkpointing than existing systems, reducing repeated work by 80% to 97.1%. Additionally, it provides 1.3x to 6.5x higher throughput at the same checkpointing frequency.", "conclusion": "Checkmate achieves per-iteration checkpointing without training slowdown, outperforming state-of-the-art systems in checkpointing frequency and throughput, leading to significant reductions in repeated work per failure."}}
{"id": "2507.13427", "categories": ["quant-ph", "cond-mat.supr-con"], "pdf": "https://arxiv.org/pdf/2507.13427", "abs": "https://arxiv.org/abs/2507.13427", "authors": ["Eugene V. Stolyarov", "V. L. Andriichuk", "Andrii M. Sokolov"], "title": "Two-photon coupling via Josephson element II: Interaction renormalizations and cross-Kerr coupling", "comment": "24 pages, 6 figures", "summary": "We study the interactions mediated by symmetric superconducting quantum\ninterference device (SQUID), their renormalizations, and applicability of the\nanharmonic oscillator model for a coupled phase qubit. The coupling SQUID can\nswitch between single- or two-photon interaction in situ. We consider a coupled\nresonator and an rf SQUID. The latter dwells in the vicinity of its metastable\nwell holding a number of anharmonic energy states and acts as an artificial\natom known as the phase qubit. Apart from the linear and two-photon couplings,\ninteractions of optomechanical type and a cross-Kerr coupling arise. Near the\ntwo-photon resonance, we calculate the renormalizations due to nonresonant\ninteractions, which are more prominent with the higher Josephson energy of the\ncoupler. We interpret the renormalizations by depicting some of the virtual\nprocesses involved. That also allows us to determine the minimal amount of\nmetastable states in the phase qubit for the renormalization formulas to hold.", "AI": {"tldr": "Investigates SQUID-mediated interactions in phase qubits, analyzing various coupling types and their renormalizations using an anharmonic oscillator model, with implications for qubit design.", "motivation": "The research aims to study the interactions mediated by a symmetric superconducting quantum interference device (SQUID), their renormalizations, and the applicability of the anharmonic oscillator model for a coupled phase qubit.", "method": "The paper considers a coupled resonator and an rf SQUID, analyzing interactions including linear, two-photon, optomechanical, and cross-Kerr couplings. It calculates renormalizations near the two-photon resonance, especially with higher Josephson energy in the coupler, and interprets these via virtual processes.", "result": "The coupling SQUID can switch between single- or two-photon interaction in situ, and interactions of optomechanical type and a cross-Kerr coupling arise. Renormalizations due to nonresonant interactions are calculated and found to be more prominent with higher Josephson energy of the coupler.", "conclusion": "The study interprets renormalizations by depicting virtual processes and determines the minimal metastable states required for renormalization formulas to hold."}}
{"id": "2507.13485", "categories": ["cs.NE", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13485", "abs": "https://arxiv.org/abs/2507.13485", "authors": ["Imane Hamzaoui", "Riyadh Baghdadi"], "title": "Neural Architecture Search with Mixed Bio-inspired Learning Rules", "comment": "ECAI 2025", "summary": "Bio-inspired neural networks are attractive for their adversarial robustness,\nenergy frugality, and closer alignment with cortical physiology, yet they often\nlag behind back-propagation (BP) based models in accuracy and ability to scale.\nWe show that allowing the use of different bio-inspired learning rules in\ndifferent layers, discovered automatically by a tailored\nneural-architecture-search (NAS) procedure, bridges this gap. Starting from\nstandard NAS baselines, we enlarge the search space to include bio-inspired\nlearning rules and use NAS to find the best architecture and learning rule to\nuse in each layer. We show that neural networks that use different bio-inspired\nlearning rules for different layers have better accuracy than those that use a\nsingle rule across all the layers. The resulting NN that uses a mix of\nbio-inspired learning rules sets new records for bio-inspired models: 95.16% on\nCIFAR-10, 76.48% on CIFAR-100, 43.42% on ImageNet16-120, and 60.51% top-1 on\nImageNet. In some regimes, they even surpass comparable BP-based networks while\nretaining their robustness advantages. Our results suggest that layer-wise\ndiversity in learning rules allows better scalability and accuracy, and\nmotivates further research on mixing multiple bio-inspired learning rules in\nthe same network.", "AI": {"tldr": "\u901a\u8fc7\u4f7f\u7528\u795e\u7ecf\u67b6\u6784\u641c\u7d22\uff08NAS\uff09\u4e3a\u4e0d\u540c\u5c42\u9009\u62e9\u4e0d\u540c\u7684\u53d7\u751f\u7269\u542f\u53d1\u7684\u5b66\u4e60\u89c4\u5219\uff0c\u53ef\u4ee5\u63d0\u9ad8\u795e\u7ecf\u7f51\u7edc\u7684\u51c6\u786e\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u5e76\u7f29\u5c0f\u5176\u4e0e\u53cd\u5411\u4f20\u64ad\uff08BP\uff09\u6a21\u578b\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "motivation": "\u4e3a\u4e86\u514b\u670d\u53d7\u751f\u7269\u542f\u53d1\u7684\u795e\u7ecf\u7f51\u7edc\u5728\u51c6\u786e\u6027\u548c\u53ef\u6269\u5c55\u6027\u65b9\u9762\u843d\u540e\u4e8e\u57fa\u4e8e\u53cd\u5411\u4f20\u64ad\uff08BP\uff09\u7684\u6a21\u578b\u7684\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u7559\u5176\u5728\u5bf9\u6297\u9c81\u68d2\u6027\u548c\u80fd\u6548\u65b9\u9762\u7684\u4f18\u52bf\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u795e\u7ecf\u67b6\u6784\u641c\u7d22\uff08NAS\uff09\u548c\u53d7\u751f\u7269\u542f\u53d1\u7684\u5b66\u4e60\u89c4\u5219\u7684\u65b9\u6cd5\u3002NAS\u7a0b\u5e8f\u88ab\u5b9a\u5236\u7528\u4e8e\u641c\u7d22\u80fd\u591f\u5e94\u7528\u4e8e\u4e0d\u540c\u5c42\u7684\u53d7\u751f\u7269\u542f\u53d1\u7684\u5b66\u4e60\u89c4\u5219\u3002\u901a\u8fc7\u6269\u5927\u641c\u7d22\u7a7a\u95f4\u4ee5\u5305\u542b\u8fd9\u4e9b\u5b66\u4e60\u89c4\u5219\uff0c\u5e76\u5229\u7528NAS\u6765\u786e\u5b9a\u6bcf\u5c42\u7684\u6700\u4f73\u67b6\u6784\u548c\u5b66\u4e60\u89c4\u5219\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u8fd9\u4e00\u76ee\u6807\u3002", "result": "\u5728CIFAR-10\u4e0a\u8fbe\u523095.16%\u7684\u51c6\u786e\u7387\uff0c\u5728CIFAR-100\u4e0a\u8fbe\u523076.48%\uff0c\u5728ImageNet16-120\u4e0a\u8fbe\u523043.42%\uff0c\u4ee5\u53ca60.51%\u7684ImageNet top-1\u51c6\u786e\u7387\u3002\u8fd9\u4e9b\u7ed3\u679c\u521b\u4e0b\u4e86\u53d7\u751f\u7269\u542f\u53d1\u7684\u6a21\u578b\u7684\u65b0\u7eaa\u5f55\uff0c\u5e76\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u8d85\u8d8a\u4e86\u540c\u7b49\u7684BP\u6a21\u578b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5176\u9c81\u68d2\u6027\u4f18\u52bf\u3002", "conclusion": "\u5141\u8bb8\u4e0d\u540c\u5c42\u4f7f\u7528\u4e0d\u540c\u7684\u53d7\u751f\u7269\u542f\u53d1\u7684\u5b66\u4e60\u89c4\u5219\uff0c\u5e76\u901a\u8fc7\u795e\u7ecf\u67b6\u6784\u641c\u7d22\uff08NAS\uff09\u8fdb\u884c\u81ea\u52a8\u53d1\u73b0\uff0c\u53ef\u4ee5\u5f25\u8865\u751f\u7269\u542f\u53d1\u795e\u7ecf\u7f51\u7edc\u5728\u51c6\u786e\u6027\u548c\u53ef\u6269\u5c55\u6027\u65b9\u9762\u4e0e\u53cd\u5411\u4f20\u64ad\uff08BP\uff09\u6a21\u578b\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002\u7814\u7a76\u8868\u660e\uff0c\u5728\u4e0d\u540c\u5c42\u4e2d\u4f7f\u7528\u4e0d\u540c\u5b66\u4e60\u89c4\u5219\u7684\u795e\u7ecf\u7f51\u7edc\u6bd4\u5728\u6240\u6709\u5c42\u4e2d\u4f7f\u7528\u5355\u4e00\u89c4\u5219\u7684\u795e\u7ecf\u7f51\u7edc\u5177\u6709\u66f4\u597d\u7684\u51c6\u786e\u6027\u3002\u8fd9\u79cd\u65b9\u6cd5\u5728CIFAR-10\u3001CIFAR-100\u548cImageNet\u7b49\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u65b0\u7684\u8bb0\u5f55\uff0c\u5e76\u4e14\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u8d85\u8fc7\u4e86\u540c\u7b49\u7684BP\u6a21\u578b\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u9c81\u68d2\u6027\u4f18\u52bf\u3002\u8fd9\u8868\u660e\u5b66\u4e60\u89c4\u5219\u7684\u5c42\u7ea7\u591a\u6837\u6027\u6709\u52a9\u4e8e\u63d0\u9ad8\u53ef\u6269\u5c55\u6027\u548c\u51c6\u786e\u6027\uff0c\u5e76\u9f13\u52b1\u5bf9\u5728\u540c\u4e00\u7f51\u7edc\u4e2d\u6df7\u5408\u591a\u79cd\u53d7\u751f\u7269\u542f\u53d1\u7684\u5b66\u4e60\u89c4\u5219\u8fdb\u884c\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2507.13354", "categories": ["cs.LG", "cs.AI", "cs.CL", "math-ph", "math.MP"], "pdf": "https://arxiv.org/pdf/2507.13354", "abs": "https://arxiv.org/abs/2507.13354", "authors": ["Zeqian Chen"], "title": "Physical models realizing the transformer architecture of large language models", "comment": "6 pages", "summary": "The introduction of the transformer architecture in 2017 (cf.\\cite{VSP2017})\nmarked the most striking advancement in natural language processing. The\ntransformer is a model architecture relying entirely on an attention mechanism\nto draw global dependencies between input and output. However, we believe there\nis a gap in our theoretical understanding of what the transformer is, and why\nit works physically. In this paper, from a physical perspective on modern\nchips, we construct physical models in the Fock space over the Hilbert space of\ntokens realizing large language models based on a transformer architecture as\nopen quantum systems. Our physical models underlie the transformer architecture\nfor large language models.", "AI": {"tldr": "\u672c\u6587\u5c06Transformer\u5927\u8bed\u8a00\u6a21\u578b\u89c6\u4e3a\u91cf\u5b50\u7cfb\u7edf\uff0c\u4ece\u7269\u7406\u89d2\u5ea6\u8fdb\u884c\u5efa\u6a21\u3002", "motivation": "\u586b\u8865\u5728\u7406\u8bba\u7406\u89e3\u4e0a\uff0c\u7279\u522b\u662f\u4ece\u7269\u7406\u89d2\u5ea6\u7406\u89e3Transformer\u6a21\u578b\u53ca\u5176\u5de5\u4f5c\u539f\u7406\u7684\u7a7a\u767d\u3002", "method": "\u5c06Transformer\u5927\u8bed\u8a00\u6a21\u578b\u6784\u5efa\u4e3aFock\u7a7a\u95f4\u4e0a\u7684\u7269\u7406\u6a21\u578b\uff0c\u5e76\u5c06\u5176\u89c6\u4e3a\u5f00\u653e\u91cf\u5b50\u7cfb\u7edf\u3002", "result": "\u6587\u7ae0\u4ece\u7269\u7406\u89d2\u5ea6\u63d0\u4f9b\u4e86Transformer\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5e95\u5c42\u6a21\u578b\u3002", "conclusion": "\u6587\u7ae0\u4ece\u7269\u7406\u89d2\u5ea6\u6784\u5efa\u4e86\u57fa\u4e8eTransformer\u5927\u8bed\u8a00\u6a21\u578b\u7684\u91cf\u5b50\u529b\u5b66\u6a21\u578b\uff0c\u5e76\u5c06\u5176\u89c6\u4e3a\u5f00\u653e\u91cf\u5b50\u7cfb\u7edf\u3002"}}
{"id": "2507.13853", "categories": ["cs.GT", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.13853", "abs": "https://arxiv.org/abs/2507.13853", "authors": ["Marko Maljkovic", "Gustav Nilsson", "Nikolas Geroliminis"], "title": "Resource-Splitting Games with Tullock-Based Lossy Contests", "comment": null, "summary": "This paper introduces a novel class of multi-stage resource allocation games\nthat model real-world scenarios in which profitability depends on the balance\nbetween supply and demand, and where higher resource investment leads to\ngreater returns. Our proposed framework, which incorporates the notion of\nprofit loss due to insufficient player participation, gives rise to a\nTullock-like functional form of the stage payoff structure when weighted fair\nproportional resource allocation is applied. We explore both centralized and\nNash equilibrium strategies, establish sufficient conditions for their\nexistence and uniqueness, and provide an iterative, semi-decentralized method\nto compute the Nash equilibrium in games with arbitrarily many players.\nAdditionally, we demonstrate that the framework generalizes instances of\nseveral existing models, including Receding Horizon and Blotto games, and\npresent a semi-analytical method for computing the unique Nash equilibrium\nwithin the Blotto setup. Our findings are validated through a numerical case\nstudy in smart mobility, highlighting the practical relevance and applicability\nof the proposed model.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u535a\u5f08\u6a21\u578b\uff0c\u7528\u4e8e\u5206\u6790\u4f9b\u9700\u5e73\u8861\u548c\u8d44\u6e90\u6295\u8d44\u56de\u62a5\uff0c\u5e76\u63d0\u4f9b\u4e86\u8ba1\u7b97\u7eb3\u4ec0\u5747\u8861\u7684\u65b9\u6cd5\uff0c\u5df2\u5728\u667a\u80fd\u4ea4\u901a\u9886\u57df\u5f97\u5230\u9a8c\u8bc1\u3002", "motivation": "\u4e3a\u89e3\u51b3\u73b0\u5b9e\u4e16\u754c\u4e2d\u76c8\u5229\u80fd\u529b\u4f9d\u8d56\u4e8e\u4f9b\u9700\u5e73\u8861\u4e14\u66f4\u9ad8\u8d44\u6e90\u6295\u8d44\u5e26\u6765\u66f4\u9ad8\u56de\u62a5\u7684\u573a\u666f\uff0c\u5f15\u5165\u4e86\u591a\u9636\u6bb5\u8d44\u6e90\u5206\u914d\u535a\u5f08\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u9636\u6bb5\u8d44\u6e90\u5206\u914d\u535a\u5f08\u6a21\u578b\uff0c\u5e76\u63a2\u7d22\u4e86\u96c6\u4e2d\u5f0f\u548c\u7eb3\u4ec0\u5747\u8861\u7b56\u7565\u3002\u5f00\u53d1\u4e86\u4e00\u79cd\u8fed\u4ee3\u7684\u3001\u534a\u53bb\u4e2d\u5fc3\u5316\u7684\u65b9\u6cd5\u6765\u8ba1\u7b97\u7eb3\u4ec0\u5747\u8861\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u534a\u89e3\u6790\u65b9\u6cd5\u6765\u8ba1\u7b97 Blotto \u6e38\u620f\u8bbe\u7f6e\u4e2d\u7684\u552f\u4e00\u7eb3\u4ec0\u5747\u8861\u3002", "result": "\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7684\u6846\u67b6\u53ef\u4ee5\u63a8\u5e7f\u5230\u5305\u62ec\u8870\u9000\u89c6\u754c\u548c Blotto \u6e38\u620f\u5728\u5185\u7684\u591a\u79cd\u73b0\u6709\u6a21\u578b\u3002", "conclusion": "\u8be5\u6a21\u578b\u4e3a\u667a\u80fd\u4ea4\u901a\u7b49\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u529b\u7684\u5206\u6790\u5de5\u5177\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u5f00\u8f9f\u4e86\u9053\u8def\u3002"}}
{"id": "2507.13366", "categories": ["cs.SI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13366", "abs": "https://arxiv.org/abs/2507.13366", "authors": ["Baoshen Guo", "Zhiqing Hong", "Junyi Li", "Shenhao Wang", "Jinhua Zhao"], "title": "Leveraging the Spatial Hierarchy: Coarse-to-fine Trajectory Generation via Cascaded Hybrid Diffusion", "comment": null, "summary": "Urban mobility data has significant connections with economic growth and\nplays an essential role in various smart-city applications. However, due to\nprivacy concerns and substantial data collection costs, fine-grained human\nmobility trajectories are difficult to become publicly available on a large\nscale. A promising solution to address this issue is trajectory synthesizing.\nHowever, existing works often ignore the inherent structural complexity of\ntrajectories, unable to handle complicated high-dimensional distributions and\ngenerate realistic fine-grained trajectories. In this paper, we propose\nCardiff, a coarse-to-fine Cascaded hybrid diffusion-based trajectory\nsynthesizing framework for fine-grained and privacy-preserving mobility\ngeneration. By leveraging the hierarchical nature of urban mobility, Cardiff\ndecomposes the generation process into two distinct levels, i.e., discrete road\nsegment-level and continuous fine-grained GPS-level: (i) In the segment-level,\nto reduce computational costs and redundancy in raw trajectories, we first\nencode the discrete road segments into low-dimensional latent embeddings and\ndesign a diffusion transformer-based latent denoising network for segment-level\ntrajectory synthesis. (ii) Taking the first stage of generation as conditions,\nwe then design a fine-grained GPS-level conditional denoising network with a\nnoise augmentation mechanism to achieve robust and high-fidelity generation.\nAdditionally, the Cardiff framework not only progressively generates\nhigh-fidelity trajectories through cascaded denoising but also flexibly enables\na tunable balance between privacy preservation and utility. Experimental\nresults on three large real-world trajectory datasets demonstrate that our\nmethod outperforms state-of-the-art baselines in various metrics.", "AI": {"tldr": "Cardiff\u6846\u67b6\u901a\u8fc7\u7c97\u5230\u7ec6\u7684\u6269\u6563\u6a21\u578b\u751f\u6210\u9ad8\u4fdd\u771f\u3001\u4fdd\u62a4\u9690\u79c1\u7684\u57ce\u5e02\u51fa\u884c\u8f68\u8ff9\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u9ad8\u7ef4\u5206\u5e03\u548c\u751f\u6210\u771f\u5b9e\u8f68\u8ff9\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u800c\u57ce\u5e02\u51fa\u884c\u8f68\u8ff9\u6570\u636e\u56e0\u9690\u79c1\u548c\u6210\u672c\u95ee\u9898\u96be\u4ee5\u516c\u5f00\u83b7\u53d6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7c97\u7c92\u5ea6\u5230\u7ec6\u7c92\u5ea6\u7684Cascaded\u6df7\u5408\u6269\u6563\u8f68\u8ff9\u5408\u6210\u6846\u67b6\u3002\u8be5\u6846\u67b6\u5c06\u751f\u6210\u8fc7\u7a0b\u5206\u89e3\u4e3a\u4e24\u4e2a\u5c42\u7ea7\uff1a1. \u79bb\u6563\u7684\u8def\u6bb5\u7ea7\uff1a\u5c06\u8def\u6bb5\u7f16\u7801\u4e3a\u4f4e\u7ef4\u6f5c\u5728\u5d4c\u5165\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563Transformer\u7684\u6f5c\u5728\u53bb\u566a\u7f51\u7edc\u30022. \u8fde\u7eed\u7684\u7cbe\u7ec6GPS\u7ea7\uff1a\u4ee5\u524d\u4e00\u9636\u6bb5\u7684\u751f\u6210\u7ed3\u679c\u4e3a\u6761\u4ef6\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5e26\u6709\u566a\u58f0\u589e\u5f3a\u673a\u5236\u7684\u7cbe\u7ec6GPS\u7ea7\u6761\u4ef6\u53bb\u566a\u7f51\u7edc\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cCardiff\u6846\u67b6\u5728\u4e09\u4e2a\u5927\u578b\u771f\u5b9e\u8f68\u8ff9\u6570\u636e\u96c6\u4e0a\uff0c\u76f8\u8f83\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u5728\u591a\u9879\u6307\u6807\u4e0a\u5747\u8868\u73b0\u66f4\u4f18\uff0c\u80fd\u591f\u751f\u6210\u9ad8\u4fdd\u771f\u5ea6\u7684\u8f68\u8ff9\uff0c\u5e76\u5728\u9690\u79c1\u4fdd\u62a4\u548c\u6570\u636e\u6548\u7528\u4e4b\u95f4\u5b9e\u73b0\u53ef\u8c03\u7684\u5e73\u8861\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCardiff\u7684\u6df7\u5408\u6269\u6563\u6a21\u578b\u65b9\u6cd5\uff0c\u7528\u4e8e\u751f\u6210\u7cbe\u7ec6\u5316\u3001\u4fdd\u62a4\u9690\u79c1\u7684\u57ce\u5e02\u51fa\u884c\u8f68\u8ff9\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u9ad8\u7ef4\u5206\u5e03\u548c\u751f\u6210\u771f\u5b9e\u8f68\u8ff9\u65b9\u9762\u7684\u4e0d\u8db3\u3002"}}
{"id": "2507.13463", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.13463", "abs": "https://arxiv.org/abs/2507.13463", "authors": ["Ahmed Hussain", "Asmaa Abdallah", "Abdulkadir Celik", "Ahmed M. Eltawil"], "title": "Joint Motion, Angle, and Range Estimation in Near-Field under Array Calibration Imperfections", "comment": null, "summary": "Ultra-massive multiple-input multiple-output MIMO (UM-MIMO) leverages large\nantenna arrays at high frequencies, transitioning communication paradigm into\nthe radiative near-field (NF), where spherical wavefronts enable full-vector\nestimation of both target location and velocity. However, location and motion\nparameters become inherently coupled in this regime, making their joint\nestimation computationally demanding. To overcome this, we propose a novel\napproach that projects the received two-dimensional space-time signal onto the\nangle-Doppler domain using a two-dimensional discrete Fourier transform\n(2D-DFT). Our analysis reveals that the resulting angular spread is centered at\nthe target's true angle, with its width determined by the target's range.\nSimilarly, transverse motion induces a Doppler spread centered at the true\nradial velocity, with the width of Doppler spread proportional to the\ntransverse velocity. Exploiting these spectral characteristics, we develop a\nlow-complexity algorithm that provides coarse estimates of angle, range, and\nvelocity, which are subsequently refined using one-dimensional multiple signal\nclassification (MUSIC) applied independently to each parameter. The proposed\nmethod enables accurate and efficient estimation of NF target motion\nparameters. Simulation results demonstrate a normalized mean squared error\n(NMSE) of -40 dB for location and velocity estimates compared to maximum\nlikelihood estimation, while significantly reducing computational complexity.", "AI": {"tldr": "\u5728\u8d85\u5927\u89c4\u6a21MIMO\u901a\u4fe1\u4e2d\uff0c\u63d0\u51fa\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u5229\u7528\u89d2\u5ea6-\u591a\u666e\u52d2\u57df\u5206\u6790\u548c\u4f4e\u590d\u6742\u5ea6\u7b97\u6cd5\uff0c\u9ad8\u6548\u7cbe\u786e\u5730\u4f30\u8ba1\u8fd1\u573a\u76ee\u6807\u7684\u4f4d\u7f6e\u548c\u901f\u5ea6\uff0c\u540c\u65f6\u964d\u4f4e\u8ba1\u7b97\u91cf\u3002", "motivation": "\u8fd1\u573a\uff08NF\uff09\u901a\u4fe1\u4f53\u5236\u4e0b\uff0c\u4f4d\u7f6e\u548c\u8fd0\u52a8\u53c2\u6570\u7684\u8054\u5408\u4f30\u8ba1\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u65b9\u6cd5\uff0c\u5229\u7528\u4e8c\u7ef4\u79bb\u6563\u5085\u91cc\u53f6\u53d8\u6362\uff082D-DFT\uff09\u5c06\u63a5\u6536\u5230\u7684\u4e8c\u7ef4\u65f6\u7a7a\u4fe1\u53f7\u6295\u5f71\u5230\u89d2\u5ea6-\u591a\u666e\u52d2\u57df\u3002\u5206\u6790\u8868\u660e\uff0c\u89d2\u5ea6\u6269\u5c55\u4ee5\u76ee\u6807\u7684\u771f\u5b9e\u89d2\u5ea6\u4e3a\u4e2d\u5fc3\uff0c\u5176\u5bbd\u5ea6\u7531\u76ee\u6807\u8ddd\u79bb\u51b3\u5b9a\uff1b\u6a2a\u5411\u8fd0\u52a8\u5f15\u8d77\u7684 \u0645\u062a\u062c\u0647\u591a\u666e\u52d2\u6269\u5c55\u4ee5\u771f\u5b9e\u7684\u5f84\u5411\u901f\u5ea6\u4e3a\u4e2d\u5fc3\uff0c\u5176\u5bbd\u5ea6\u4e0e\u6a2a\u5411\u901f\u5ea6\u6210\u6b63\u6bd4\u3002\u5229\u7528\u8fd9\u4e9b\u9891\u8c31\u7279\u6027\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u4f4e\u590d\u6742\u5ea6\u7b97\u6cd5\uff0c\u63d0\u4f9b\u89d2\u5ea6\u3001\u8ddd\u79bb\u548c\u901f\u5ea6\u7684\u7c97\u7565\u4f30\u8ba1\uff0c\u968f\u540e\u901a\u8fc7\u72ec\u7acb\u5e94\u7528\u4e8e\u6bcf\u4e2a\u53c2\u6570\u7684\u4e00\u7ef4\u591a\u4fe1\u53f7\u5206\u7c7b\uff08MUSIC\uff09\u8fdb\u884c\u7ec6\u5316\u3002", "result": "\u4e0e\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u76f8\u6bd4\uff0c\u5728\u4f4d\u7f6e\u548c\u901f\u5ea6\u4f30\u8ba1\u65b9\u9762\u5b9e\u73b0\u4e86-40 dB\u7684\u5f52\u4e00\u5316\u5747\u65b9\u8bef\u5dee\uff08NMSE\uff09\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u901a\u8fc7\u5229\u7528\u89d2\u5ea6-\u591a\u666e\u52d2\u57df\u4e2d\u7684\u9891\u8c31\u7279\u6027\uff0c\u5b9e\u73b0\u4e86\u5bf9\u8fd1\u573a\u76ee\u6807\u8fd0\u52a8\u53c2\u6570\u7684\u51c6\u786e\u9ad8\u6548\u4f30\u8ba1\uff0c\u5e76\u4e14\u8ba1\u7b97\u590d\u6742\u5ea6\u663e\u8457\u964d\u4f4e\u3002"}}
{"id": "2507.13359", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13359", "abs": "https://arxiv.org/abs/2507.13359", "authors": ["Yang Zhou", "Junjie Li", "CongYang Ou", "Dawei Yan", "Haokui Zhang", "Xizhe Xue"], "title": "Open-Vocabulary Object Detection in UAV Imagery: A Review and Future Perspectives", "comment": "27 pages, 5 figures", "summary": "Due to its extensive applications, aerial image object detection has long\nbeen a hot topic in computer vision. In recent years, advancements in Unmanned\nAerial Vehicles (UAV) technology have further propelled this field to new\nheights, giving rise to a broader range of application requirements. However,\ntraditional UAV aerial object detection methods primarily focus on detecting\npredefined categories, which significantly limits their applicability. The\nadvent of cross-modal text-image alignment (e.g., CLIP) has overcome this\nlimitation, enabling open-vocabulary object detection (OVOD), which can\nidentify previously unseen objects through natural language descriptions. This\nbreakthrough significantly enhances the intelligence and autonomy of UAVs in\naerial scene understanding. This paper presents a comprehensive survey of OVOD\nin the context of UAV aerial scenes. We begin by aligning the core principles\nof OVOD with the unique characteristics of UAV vision, setting the stage for a\nspecialized discussion. Building on this foundation, we construct a systematic\ntaxonomy that categorizes existing OVOD methods for aerial imagery and provides\na comprehensive overview of the relevant datasets. This structured review\nenables us to critically dissect the key challenges and open problems at the\nintersection of these fields. Finally, based on this analysis, we outline\npromising future research directions and application prospects. This survey\naims to provide a clear road map and a valuable reference for both newcomers\nand seasoned researchers, fostering innovation in this rapidly evolving domain.\nWe keep tracing related works at\nhttps://github.com/zhouyang2002/OVOD-in-UVA-imagery", "AI": {"tldr": "\u672c\u7bc7\u8bba\u6587\u5168\u9762 survey \u4e86\u65e0\u4eba\u673a\uff08UVA\uff09\u822a\u62cd\u573a\u666f\u4e0b\u7684\u5f00\u653e\u8bcd\u6c47\u76ee\u6807\u68c0\u6d4b\uff08OVOD\uff09\uff0c\u5c06OVOD\u539f\u7406\u4e0eUVA\u89c6\u89c9\u7279\u6027\u76f8\u7ed3\u5408\uff0c\u5bf9\u73b0\u6709\u65b9\u6cd5\u8fdb\u884c\u5206\u7c7b\uff0c\u5e76\u63a2\u8ba8\u4e86\u6311\u6218\u548c\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u968f\u7740\u65e0\u4eba\u673a\uff08UVA\uff09\u6280\u672f\u7684\u53d1\u5c55\uff0c\u5176\u5e94\u7528\u8303\u56f4\u4e0d\u65ad\u6269\u5927\uff0c\u4f46\u4f20\u7edfUVA\u822a\u62cd\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\u4e3b\u8981\u5c40\u9650\u4e8e\u9884\u5b9a\u4e49\u7c7b\u522b\uff0c\u9650\u5236\u4e86\u5176\u9002\u7528\u6027\u3002\u5f00\u653e\u8bcd\u6c47\u76ee\u6807\u68c0\u6d4b\uff08OVOD\uff09\u901a\u8fc7\u501f\u9274\u8de8\u6a21\u6001\u6587\u672c-\u56fe\u50cf\u5bf9\u9f50\uff08\u5982CLIP\uff09\uff0c\u80fd\u591f\u8bc6\u522b\u4ee5\u524d\u672a\u89c1\u8fc7\u7684\u7269\u4f53\uff0c\u63d0\u9ad8\u4e86UVA\u5728\u822a\u62cd\u573a\u666f\u7406\u89e3\u4e2d\u7684\u667a\u80fd\u6027\u548c\u81ea\u4e3b\u6027\u3002", "method": "\u672c\u7bc7\u8bba\u6587\u9996\u5148\u5c06\u5f00\u653e\u8bcd\u6c47\u76ee\u6807\u68c0\u6d4b\uff08OVOD\uff09\u7684\u6838\u5fc3\u539f\u7406\u4e0e\u65e0\u4eba\u673a\uff08UVA\uff09\u89c6\u89c9\u7684\u72ec\u7279\u7279\u6027\u76f8\u7ed3\u5408\uff0c\u7136\u540e\u6784\u5efa\u4e86\u4e00\u4e2a\u7cfb\u7edf\u6027\u7684\u5206\u7c7b\u6cd5\uff0c\u5bf9\u73b0\u6709UVA\u822a\u62cd\u56fe\u50cf\u7684OVOD\u65b9\u6cd5\u8fdb\u884c\u4e86\u5206\u7c7b\uff0c\u5e76\u5bf9\u76f8\u5173\u6570\u636e\u96c6\u8fdb\u884c\u4e86\u5168\u9762\u7684\u6982\u8ff0\u3002", "result": "\u5bf9\u73b0\u6709OVOD\u65b9\u6cd5\u8fdb\u884c\u4e86\u7cfb\u7edf\u7684\u5206\u7c7b\u548c\u5168\u9762\u7684\u6982\u8ff0\uff0c\u5e76\u6df1\u5165\u5206\u6790\u4e86\u65e0\u4eba\u673a\u822a\u62cd\u573a\u666f\u4e0bOVOD\u7684\u5173\u952e\u6311\u6218\u548c\u5f00\u653e\u6027\u95ee\u9898\uff0c\u4e3a\u8be5\u9886\u57df\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u53c2\u8003\u548c\u672a\u6765\u53d1\u5c55\u65b9\u5411\u3002", "conclusion": "\u672c\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5f00\u653e\u8bcd\u6c47\u76ee\u6807\u68c0\u6d4b\uff08OVOD\uff09\u5728\u65e0\u4eba\u673a\uff08UVA\uff09\u822a\u62cd\u573a\u666f\u4e0b\u7684\u7cfb\u7edf\u6027\u5206\u7c7b\uff0c\u5e76\u5bf9\u73b0\u6709\u65b9\u6cd5\u8fdb\u884c\u4e86\u5168\u9762\u7684\u6982\u8ff0\uff0c\u540c\u65f6\u5206\u6790\u4e86\u8be5\u9886\u57df\u7684\u5173\u952e\u6311\u6218\u3001\u5f00\u653e\u6027\u95ee\u9898\u4ee5\u53ca\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u548c\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2507.13511", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13511", "abs": "https://arxiv.org/abs/2507.13511", "authors": ["Nabil Abdelaziz Ferhat Taleb", "Abdolazim Rezaei", "Raj Atulkumar Patel", "Mehdi Sookhak"], "title": "GraphTrafficGPT: Enhancing Traffic Management Through Graph-Based AI Agent Coordination", "comment": null, "summary": "Large Language Models (LLMs) offer significant promise for intelligent\ntraffic management; however, current chain-based systems like TrafficGPT are\nhindered by sequential task execution, high token usage, and poor scalability,\nmaking them inefficient for complex, real-world scenarios. To address these\nlimitations, we propose GraphTrafficGPT, a novel graph-based architecture,\nwhich fundamentally redesigns the task coordination process for LLM-driven\ntraffic applications. GraphTrafficGPT represents tasks and their dependencies\nas nodes and edges in a directed graph, enabling efficient parallel execution\nand dynamic resource allocation. The main idea behind the proposed model is a\nBrain Agent that decomposes user queries, constructs optimized dependency\ngraphs, and coordinates a network of specialized agents for data retrieval,\nanalysis, visualization, and simulation. By introducing advanced context-aware\ntoken management and supporting concurrent multi-query processing, the proposed\narchitecture handles interdependent tasks typical of modern urban mobility\nenvironments. Experimental results demonstrate that GraphTrafficGPT reduces\ntoken consumption by 50.2% and average response latency by 19.0% compared to\nTrafficGPT, while supporting simultaneous multi-query execution with up to\n23.0% improvement in efficiency.", "AI": {"tldr": "GraphTrafficGPT\u662f\u4e00\u79cd\u57fa\u4e8e\u56fe\u7684\u65b0\u578bLLM\u67b6\u6784\uff0c\u901a\u8fc7\u5e76\u884c\u5904\u7406\u548c\u4f18\u5316\u7684\u4efb\u52a1\u534f\u8c03\uff0c\u63d0\u9ad8\u4e86\u4ea4\u901a\u7ba1\u7406\u7684\u6548\u7387\uff0c\u964d\u4f4e\u4e86\u6210\u672c\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u57fa\u4e8e\u94fe\u5f0f\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ea4\u901a\u7ba1\u7406\u7cfb\u7edf\uff08\u5982TrafficGPT\uff09\u5728\u987a\u5e8f\u4efb\u52a1\u6267\u884c\u3001\u9ad8\u4ee4\u724c\u4f7f\u7528\u7387\u548c\u53ef\u6269\u5c55\u6027\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u8fd9\u4e9b\u95ee\u9898\u5bfc\u81f4\u5728\u590d\u6742\u73b0\u5b9e\u573a\u666f\u4e0b\u7684\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGraphTrafficGPT\u7684\u65b0\u578b\u57fa\u4e8e\u56fe\u7684\u67b6\u6784\uff0c\u5176\u4e2d\u4efb\u52a1\u548c\u4f9d\u8d56\u5173\u7cfb\u88ab\u8868\u793a\u4e3a\u6709\u5411\u56fe\u7684\u8282\u70b9\u548c\u8fb9\u3002\u8be5\u67b6\u6784\u5305\u542b\u4e00\u4e2a\u201c\u5927\u8111\u4ee3\u7406\u201d\uff0c\u8d1f\u8d23\u5206\u89e3\u7528\u6237\u67e5\u8be2\u3001\u6784\u5efa\u4f18\u5316\u7684\u4f9d\u8d56\u56fe\uff0c\u5e76\u534f\u8c03\u4e13\u95e8\u7684\u4ee3\u7406\uff08\u5982\u6570\u636e\u68c0\u7d22\u3001\u5206\u6790\u3001\u53ef\u89c6\u5316\u548c\u6a21\u62df\u4ee3\u7406\uff09\u3002\u901a\u8fc7\u5f15\u5165\u4e0a\u4e0b\u6587\u611f\u77e5\u4ee4\u724c\u7ba1\u7406\u548c\u652f\u6301\u5e76\u53d1\u591a\u67e5\u8be2\u5904\u7406\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u4efb\u52a1\u534f\u8c03\u3002", "result": "\u4e0eTrafficGPT\u76f8\u6bd4\uff0cGraphTrafficGPT\u5c06\u4ee4\u724c\u6d88\u8017\u964d\u4f4e\u4e8650.2%\uff0c\u5e73\u5747\u54cd\u5e94\u5ef6\u8fdf\u964d\u4f4e\u4e8619.0%\uff0c\u540c\u65f6\u5728\u5e76\u53d1\u591a\u67e5\u8be2\u6267\u884c\u65b9\u9762\u6548\u7387\u63d0\u9ad8\u4e8623.0%\u3002", "conclusion": "GraphTrafficGPT\u901a\u8fc7\u57fa\u4e8e\u56fe\u7684\u67b6\u6784\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u94fe\u5f0fLLM\u4ea4\u901a\u7ba1\u7406\u7cfb\u7edf\u7684\u5c40\u9650\u6027\uff0c\u5728\u964d\u4f4e\u4ee4\u724c\u6d88\u8017\u548c\u54cd\u5e94\u5ef6\u8fdf\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u652f\u6301\u5e76\u53d1\u591a\u67e5\u8be2\u6267\u884c\u3002"}}
{"id": "2507.13377", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13377", "abs": "https://arxiv.org/abs/2507.13377", "authors": ["Zhenglin Pan", "Haoran Xie"], "title": "StructInbet: Integrating Explicit Structural Guidance into Inbetween Frame Generation", "comment": "3 pages, 3 figures. SIGGRAPH 2025 Poster", "summary": "In this paper, we propose StructInbet, an inbetweening system designed to\ngenerate controllable transitions over explicit structural guidance.\nStructInbet introduces two key contributions. First, we propose explicit\nstructural guidance to the inbetweening problem to reduce the ambiguity\ninherent in pixel trajectories. Second, we adopt a temporal attention mechanism\nthat incorporates visual identity from both the preceding and succeeding\nkeyframes, ensuring consistency in character appearance.", "AI": {"tldr": "StructInbet is an inbetweening system that uses structural guidance and temporal attention for smooth, consistent character transitions.", "motivation": "The motivation is to address the ambiguity in pixel trajectories in the inbetweening problem and ensure consistency in character appearance during transitions.", "method": "StructInbet uses explicit structural guidance to reduce ambiguity in pixel trajectories and a temporal attention mechanism to ensure character appearance consistency by incorporating information from preceding and succeeding keyframes.", "result": "The system generates controllable transitions over explicit structural guidance, ensuring character appearance consistency.", "conclusion": "The paper proposes StructInbet, an inbetweening system that uses explicit structural guidance and a temporal attention mechanism for controllable transitions and consistent character appearance."}}
{"id": "2507.13534", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.13534", "abs": "https://arxiv.org/abs/2507.13534", "authors": ["Leo Semmelmann", "Frederik vom Scheidt"], "title": "Heatwave-driven air conditioning adoption could increase German electricity demand by 14 GW in the near future", "comment": "10 pages, 6 figures", "summary": "Intensifying heatwaves driven by climate change are accelerating the adoption\nof mobile air conditioning (AC) systems. A rapid mass adoption of such AC\nsystems could create additional stress on electricity grids and the power\nsystem. This study presents a novel method to estimate the electricity demand\nfrom AC systems both at system level and at high temporal and spatial\ngranularity. We apply the method to a near-future heatwave scenario in Germany\nin which household AC adoption increases from current 19% to 35% during a\nheatwave similar to the one of July 2025. We analyze the effects for 196,428\ngrid cells of one square kilometer across Germany, by combining weather data,\ncensus data, socio-demographic assumptions, mobility patterns, and\ntemperature-dependent AC activation functions. We find that electricity demand\nof newly purchased mobile AC systems could increase the peak load by over 14 GW\n(23%), with urban hot-spots reaching 5.8 MW per square kilometer. The temporal\npattern creates a pronounced afternoon peak that coincides with lower\nphotovoltaic generation, potentially exacerbating power system stability\nchallenges. Our findings underscore the urgency for proactive energy system\nplanning to manage emerging demand peaks.", "AI": {"tldr": "\u7531\u4e8e\u70ed\u6d6a\u9891\u53d1\uff0c\u7a7a\u8c03\u4f7f\u7528\u91cf\u6fc0\u589e\uff0c\u53ef\u80fd\u5728\u4e0b\u5348\u589e\u52a023%\u7684\u7535\u529b\u9700\u6c42\uff0c\u5e76\u52a0\u5267\u7535\u7f51\u7a33\u5b9a\u6027\u95ee\u9898\u3002", "motivation": "\u9274\u4e8e\u6c14\u5019\u53d8\u5316\u5bfc\u81f4\u7684\u70ed\u6d6a\u52a0\u5267\uff0c\u79fb\u52a8\u7a7a\u8c03\uff08AC\uff09\u7cfb\u7edf\u7684\u91c7\u7528\u6b63\u5728\u52a0\u901f\u3002\u8fd9\u79cd\u5feb\u901f\u7684\u5927\u89c4\u6a21\u91c7\u7528\u53ef\u80fd\u4f1a\u7ed9\u7535\u529b\u7cfb\u7edf\u5e26\u6765\u989d\u5916\u7684\u538b\u529b\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4f30\u7b97AC\u7cfb\u7edf\u7535\u529b\u9700\u6c42\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u8fbe\u5230\u7cfb\u7edf\u7ea7\u4ee5\u53ca\u9ad8\u65f6\u95f4/\u7a7a\u95f4\u5206\u8fa8\u7387\u7684\u7279\u70b9\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u5929\u6c14\u6570\u636e\u3001\u4eba\u53e3\u666e\u67e5\u6570\u636e\u3001\u793e\u4f1a\u4eba\u53e3\u7edf\u8ba1\u5b66\u5047\u8bbe\u3001\u51fa\u884c\u6a21\u5f0f\u4ee5\u53ca\u4f9d\u8d56\u4e8e\u6e29\u5ea6\u7684AC\u6fc0\u6d3b\u51fd\u6570\u3002\u7814\u7a76\u4eba\u5458\u5c06\u8be5\u65b9\u6cd5\u5e94\u7528\u4e8e\u5fb7\u56fd\u4e00\u4e2a\u8fd1\u671f\u7684\u70ed\u6d6a\u60c5\u666f\uff0c\u5206\u6790\u4e86196,428\u4e2a\u4e00\u5e73\u65b9\u516c\u91cc\u7f51\u683c\u533a\u57df\u7684\u7535\u529b\u9700\u6c42\u3002", "result": "\u5728\u5fb7\u56fd\uff0c\u5f53\u5bb6\u5eadAC\u666e\u53ca\u7387\u4ece19%\u589e\u52a0\u523035%\u65f6\uff0c\u65b0\u8d2d\u4e70\u7684\u79fb\u52a8AC\u7cfb\u7edf\u7684\u7535\u529b\u9700\u6c42\u53ef\u80fd\u4f7f\u5cf0\u503c\u8d1f\u8377\u589e\u52a0\u8d85\u8fc714\u5409\u74e6\uff0823%\uff09\uff0c\u57ce\u5e02\u70ed\u70b9\u5730\u533a\u7684\u5cf0\u503c\u8d1f\u8377\u53ef\u80fd\u8fbe\u5230\u6bcf\u5e73\u65b9\u516c\u91cc5.8\u5146\u74e6\u3002\u5176\u65f6\u95f4\u6a21\u5f0f\u4ea7\u751f\u660e\u663e\u7684\u4e0b\u5348\u5cf0\u503c\uff0c\u8fd9\u6070\u9022\u5149\u4f0f\u53d1\u7535\u91cf\u8f83\u4f4e\u7684\u65f6\u6bb5\uff0c\u53ef\u80fd\u52a0\u5267\u7535\u529b\u7cfb\u7edf\u7a33\u5b9a\u6027\u6311\u6218\u3002", "conclusion": "\u6c14\u5019\u53d8\u5316\u5bfc\u81f4\u7684\u70ed\u6d6a\u52a0\u5267\u4e86\u79fb\u52a8\u7a7a\u8c03\uff08AC\uff09\u7cfb\u7edf\u7684\u91c7\u7528\uff0c\u5feb\u901f\u7684\u5927\u89c4\u6a21\u91c7\u7528\u53ef\u80fd\u7ed9\u7535\u7f51\u548c\u7535\u529b\u7cfb\u7edf\u5e26\u6765\u989d\u5916\u538b\u529b\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\u6765\u4f30\u7b97AC\u7cfb\u7edf\u7684\u7535\u529b\u9700\u6c42\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u7cfb\u7edf\u7ea7\u4ee5\u53ca\u9ad8\u65f6\u95f4/\u7a7a\u95f4\u5206\u8fa8\u7387\u7684\u7279\u70b9\u3002\u5c06\u8be5\u65b9\u6cd5\u5e94\u7528\u4e8e\u5fb7\u56fd\u4e00\u4e2a\u8fd1\u671f\u7684\u70ed\u6d6a\u60c5\u666f\uff0c\u5176\u4e2d\u5bb6\u5eadAC\u666e\u53ca\u7387\u4ece\u76ee\u524d\u768419%\u589e\u52a0\u523035%\u3002\u6211\u4eec\u7ed3\u5408\u5929\u6c14\u6570\u636e\u3001\u4eba\u53e3\u666e\u67e5\u6570\u636e\u3001\u793e\u4f1a\u4eba\u53e3\u7edf\u8ba1\u5b66\u5047\u8bbe\u3001\u51fa\u884c\u6a21\u5f0f\u548c\u4f9d\u8d56\u4e8e\u6e29\u5ea6\u7684AC\u6fc0\u6d3b\u51fd\u6570\uff0c\u5206\u6790\u4e86\u5fb7\u56fd196,428\u4e2a\u4e00\u5e73\u65b9\u516c\u91cc\u7f51\u683c\u533a\u57df\u7684\u5f71\u54cd\u3002\u6211\u4eec\u53d1\u73b0\u65b0\u8d2d\u4e70\u7684\u79fb\u52a8AC\u7cfb\u7edf\u7684\u7535\u529b\u9700\u6c42\u53ef\u80fd\u4f7f\u5cf0\u503c\u8d1f\u8377\u589e\u52a0\u8d85\u8fc714\u5409\u74e6\uff0823%\uff09\uff0c\u57ce\u5e02\u70ed\u70b9\u5730\u533a\u7684\u5cf0\u503c\u8d1f\u8377\u53ef\u80fd\u8fbe\u5230\u6bcf\u5e73\u65b9\u516c\u91cc5.8\u5146\u74e6\u3002\u5176\u65f6\u95f4\u6a21\u5f0f\u4ea7\u751f\u660e\u663e\u7684\u4e0b\u5348\u5cf0\u503c\uff0c\u8fd9\u6070\u9022\u5149\u4f0f\u53d1\u7535\u91cf\u8f83\u4f4e\u7684\u65f6\u6bb5\uff0c\u53ef\u80fd\u52a0\u5267\u7535\u529b\u7cfb\u7edf\u7a33\u5b9a\u6027\u6311\u6218\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u4e3b\u52a8\u8fdb\u884c\u80fd\u6e90\u7cfb\u7edf\u89c4\u5212\u4ee5\u5e94\u5bf9\u65b0\u51fa\u73b0\u7684\u7528\u7535\u9ad8\u5cf0\u7684\u7d27\u8feb\u6027\u3002"}}
{"id": "2507.13455", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.13455", "abs": "https://arxiv.org/abs/2507.13455", "authors": ["Dean Chen", "Armin Pomeroy", "Brandon T. Peterson", "Will Flanagan", "He Kai Lim", "Alexandra Stavrakis", "Nelson F. SooHoo", "Jonathan B. Hopkins", "Tyler R. Clites"], "title": "Hard-Stop Synthesis for Multi-DOF Compliant Mechanisms", "comment": "42 pages, 17 figures. Under review at ASME Journal of Mechanical\n  Design", "summary": "Compliant mechanisms have significant potential in precision applications due\nto their ability to guide motion without contact. However, an inherent\nvulnerability to fatigue and mechanical failure has hindered the translation of\ncompliant mechanisms to real-world applications. This is particularly\nchallenging in service environments where loading is complex and uncertain, and\nthe cost of failure is high. In such cases, mechanical hard stops are critical\nto prevent yielding and buckling. Conventional hard-stop designs, which rely on\nstacking single-DOF limits, must be overly restrictive in multi-DOF space to\nguarantee safety in the presence of unknown loads. In this study, we present a\nsystematic design synthesis method to guarantee overload protection in\ncompliant mechanisms by integrating coupled multi-DOF motion limits within a\nsingle pair of compact hard-stop surfaces. Specifically, we introduce a\ntheoretical and practical framework for optimizing the contact surface geometry\nto maximize the mechanisms multi-DOF working space while still ensuring that\nthe mechanism remains within its elastic regime. We apply this synthesis method\nto a case study of a caged-hinge mechanism for orthopaedic implants, and\nprovide numerical and experimental validation that the derived design offers\nreliable protection against fatigue, yielding, and buckling. This work\nestablishes a foundation for precision hard-stop design in compliant systems\noperating under uncertain loads, which is a crucial step toward enabling the\napplication of compliant mechanisms in real-world systems.", "AI": {"tldr": "\u4e3a\u4e86\u89e3\u51b3\u67d4\u987a\u673a\u6784\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u6613\u5931\u6548\u7684\u95ee\u9898\uff0c\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u96c6\u6210\u8026\u5408\u7684\u591a\u81ea\u7531\u5ea6\u8fd0\u52a8\u9650\u5236\u6765\u63d0\u4f9b\u8fc7\u8f7d\u4fdd\u62a4\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u67d4\u987a\u673a\u6784\u5728\u7cbe\u5bc6\u5e94\u7528\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u5176\u5bf9\u75b2\u52b3\u548c\u673a\u68b0\u6545\u969c\u7684\u56fa\u6709\u8106\u5f31\u6027\u963b\u788d\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u8f6c\u5316\u3002\u5728\u670d\u52a1\u73af\u5883\u5c24\u4e3a\u5982\u6b64\uff0c\u56e0\u4e3a\u90a3\u91cc\u7684\u8f7d\u8377\u590d\u6742\u4e14\u4e0d\u786e\u5b9a\uff0c\u5e76\u4e14\u6545\u969c\u6210\u672c\u9ad8\u6602\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u673a\u68b0\u786c\u6b62\u52a8\u5668\u5bf9\u4e8e\u9632\u6b62\u5c48\u670d\u548c\u5c48\u66f2\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u7684\u8bbe\u8ba1\u7efc\u5408\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u5355\u4e2a\u7d27\u51d1\u7684\u786c\u6b62\u52a8\u5668\u8868\u9762\u5bf9\u4e2d\u96c6\u6210\u8026\u5408\u7684\u591a\u81ea\u7531\u5ea6\u8fd0\u52a8\u9650\u5236\uff0c\u6765\u4fdd\u8bc1\u67d4\u987a\u673a\u6784\u7684\u8fc7\u8f7d\u4fdd\u62a4\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u4e3a\u4f18\u5316\u63a5\u89e6\u9762\u51e0\u4f55\u5f62\u72b6\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7406\u8bba\u548c\u5b9e\u8df5\u6846\u67b6\uff0c\u4ee5\u6700\u5927\u5316\u673a\u6784\u7684\u591a\u81ea\u7531\u5ea6\u5de5\u4f5c\u7a7a\u95f4\uff0c\u540c\u65f6\u786e\u4fdd\u673a\u6784\u4fdd\u6301\u5728\u5176\u5f39\u6027\u8303\u56f4\u5185\u3002", "result": "\u5c06\u8be5\u7efc\u5408\u65b9\u6cd5\u5e94\u7528\u4e8e\u9aa8\u79d1\u690d\u5165\u7269\u7684\u7b3c\u5f0f\u94f0\u94fe\u673a\u6784\u7684\u6848\u4f8b\u7814\u7a76\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8bc1\u660e\u4e86\u6240\u5bfc\u51fa\u7684\u8bbe\u8ba1\u80fd\u591f\u53ef\u9760\u5730\u9632\u6b62\u75b2\u52b3\u3001\u5c48\u670d\u548c\u5c48\u66f2\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5728\u4e0d\u786e\u5b9a\u8f7d\u8377\u4e0b\u8fd0\u884c\u7684\u67d4\u987a\u7cfb\u7edf\u4e2d\u7cbe\u5bc6\u786c\u6b62\u52a8\u5668\u8bbe\u8ba1\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u8fd9\u662f\u5b9e\u73b0\u67d4\u987a\u673a\u6784\u5728\u5b9e\u9645\u7cfb\u7edf\u4e2d\u5e94\u7528\u7684\u5173\u952e\u4e00\u6b65\u3002"}}
{"id": "2507.13847", "categories": ["cs.LO"], "pdf": "https://arxiv.org/pdf/2507.13847", "abs": "https://arxiv.org/abs/2507.13847", "authors": ["Katsumi Inoue", "Daniil Kozhemiachenko"], "title": "Complexity of Abduction in \u0141ukasiewicz Logic", "comment": null, "summary": "We explore the problem of explaining observations in contexts involving\nstatements with truth degrees such as `the lift is loaded', `the symptoms are\nsevere', etc. To formalise these contexts, we consider infinitely-valued\n{\\L}ukasiewicz fuzzy logic. We define and motivate the notions of abduction\nproblems and explanations in the language of {\\L}ukasiewicz logic expanded with\n`interval literals' of the form $p\\geq\\mathbf{c}$, $p\\leq\\mathbf{c}$, and their\nnegations that express the set of values a variable can have. We analyse the\ncomplexity of standard abductive reasoning tasks (solution recognition,\nsolution existence, and relevance / necessity of hypotheses) in {\\L}ukasiewicz\nlogic for the case of the full language and for the case of theories containing\nonly disjunctive clauses and show that in contrast to classical propositional\nlogic, the abduction in the clausal fragment has lower complexity than in the\ngeneral case.", "AI": {"tldr": "\u8be5\u7814\u7a76\u4f7f\u7528\u5362\u5361\u8c22\u7ef4\u5947\u6a21\u7cca\u903b\u8f91\u548c\u533a\u95f4\u6587\u5b57\u6765\u5904\u7406\u5177\u6709\u771f\u503c\u5ea6\u7684\u547d\u9898\uff0c\u5e76\u5206\u6790\u4e86\u6eaf\u56e0\u63a8\u7406\u7684\u590d\u6742\u6027\uff0c\u53d1\u73b0\u5b50\u53e5\u7247\u6bb5\u6bd4\u4e00\u822c\u60c5\u51b5\u66f4\u6613\u4e8e\u5904\u7406\u3002", "motivation": "\u8be5\u7814\u7a76\u7684\u52a8\u673a\u662f\u63a2\u7d22\u5728\u5177\u6709\u771f\u503c\u5ea6\uff08\u4f8b\u5982\u201c\u7535\u68af\u5df2\u6ee1\u201d\u3001\u201c\u75c7\u72b6\u4e25\u91cd\u201d\u7b49\uff09\u7684\u547d\u9898\u7684\u4e0a\u4e0b\u6587\u4e2d\u89e3\u91ca\u89c2\u6d4b\u503c\u7684\u95ee\u9898\u3002", "method": "\u8be5\u7814\u7a76\u5c06\u65e0\u9650\u503c\u5362\u5361\u8c22\u7ef4\u5947\u6a21\u7cca\u903b\u8f91\u4e0e\u8868\u793a\u53d8\u91cf\u53ef\u80fd\u53d6\u503c\u96c6\u5408\u7684\u201c\u533a\u95f4\u6587\u5b57\u201d\uff08\u5982 p\u2265c\u3001p\u2264c \u53ca\u5176\u5426\u5b9a\uff09\u76f8\u7ed3\u5408\uff0c\u4ee5\u5f62\u5f0f\u5316\u5177\u6709\u771f\u503c\u5ea6\u7684\u547d\u9898\u7684\u4e0a\u4e0b\u6587\u3002", "result": "\u7814\u7a76\u5206\u6790\u4e86\u5728\u5b8c\u6574\u8bed\u8a00\u548c\u4ec5\u5305\u542b\u6790\u53d6\u5b50\u53e5\u7684\u7406\u8bba\u7684\u5362\u5361\u8c22\u7ef4\u5947\u903b\u8f91\u4e2d\uff0c\u6807\u51c6\u6eaf\u56e0\u63a8\u7406\u4efb\u52a1\uff08\u89e3\u8bc6\u522b\u3001\u89e3\u5b58\u5728\u548c\u5047\u8bbe\u7684\u76f8\u5173\u6027/\u5fc5\u8981\u6027\uff09\u7684\u590d\u6742\u6027\uff0c\u5e76\u6307\u51fa\u4e0e\u7ecf\u5178\u547d\u9898\u903b\u8f91\u76f8\u6bd4\uff0c\u5b50\u53e5\u7247\u6bb5\u4e2d\u7684\u6eaf\u56e0\u6bd4\u4e00\u822c\u60c5\u51b5\u5177\u6709\u66f4\u4f4e\u7684\u590d\u6742\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u5206\u6790\u4e86\u5728\u5305\u542b\u5177\u6709\u771f\u503c\u5ea6\uff08\u4f8b\u5982\u201c\u7535\u68af\u5df2\u6ee1\u201d\u3001\u201c\u75c7\u72b6\u4e25\u91cd\u201d\u7b49\uff09\u7684\u547d\u9898\u7684\u4e0a\u4e0b\u6587\u4e2d\u89e3\u91ca\u89c2\u6d4b\u503c\u7684\u95ee\u9898\uff0c\u5e76\u4f7f\u7528\u65e0\u9650\u503c\u5362\u5361\u8c22\u7ef4\u5947\u6a21\u7cca\u903b\u8f91\u6765\u5f62\u5f0f\u5316\u8fd9\u4e9b\u4e0a\u4e0b\u6587\u3002\u7814\u7a76\u5b9a\u4e49\u5e76\u9610\u8ff0\u4e86\u5728\u6269\u5c55\u4e86\u201c\u533a\u95f4\u6587\u5b57\u201d\uff08\u5982 p\u2265c\u3001p\u2264c \u53ca\u5176\u5426\u5b9a\uff09\u7684\u8bed\u8a00\u4e2d\uff0c\u8868\u793a\u53d8\u91cf\u53ef\u80fd\u53d6\u503c\u96c6\u5408\u7684 the abductive problems \u548c explanations \u7684\u6982\u5ff5\u3002"}}
{"id": "2507.13355", "categories": ["cs.AR", "cs.AI", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.13355", "abs": "https://arxiv.org/abs/2507.13355", "authors": ["Riadul Islam", "Dhandeep Challagundla"], "title": "PGR-DRC: Pre-Global Routing DRC Violation Prediction Using Unsupervised Learning", "comment": null, "summary": "Leveraging artificial intelligence (AI)-driven electronic design and\nautomation (EDA) tools, high-performance computing, and parallelized algorithms\nare essential for next-generation microprocessor innovation, ensuring continued\nprogress in computing, AI, and semiconductor technology. Machine learning-based\ndesign rule checking (DRC) and lithography hotspot detection can improve\nfirst-pass silicon success. However, conventional ML and neural network\n(NN)-based models use supervised learning and require a large balanced dataset\n(in terms of positive and negative classes) and training time. This research\naddresses those key challenges by proposing the first-ever unsupervised DRC\nviolation prediction methodology. The proposed model can be built using any\nunbalanced dataset using only one class and set a threshold for it, then\nfitting any new data querying if they are within the boundary of the model for\nclassification. This research verified the proposed model by implementing\ndifferent computational cores using CMOS 28 nm technology and Synopsys Design\nCompiler and IC Compiler II tools. Then, layouts were divided into virtual\ngrids to collect about 60k data for analysis and verification. The proposed\nmethod has 99.95% prediction test accuracy, while the existing support vector\nmachine (SVM) and neural network (NN) models have 85.44\\% and 98.74\\% accuracy,\nrespectively. In addition, the proposed methodology has about 26.3x and up to\n6003x lower training times compared to SVM and NN-models, respectively.", "AI": {"tldr": "\u5229\u7528\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u9a71\u52a8\u7684\u7535\u5b50\u8bbe\u8ba1\u81ea\u52a8\u5316\uff08EDA\uff09\u5de5\u5177\u3001\u9ad8\u6027\u80fd\u8ba1\u7b97\u548c\u5e76\u884c\u5316\u7b97\u6cd5\u5bf9\u4e8e\u4e0b\u4e00\u4ee3\u5fae\u5904\u7406\u5668\u521b\u65b0\u81f3\u5173\u91cd\u8981\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bbe\u8ba1\u89c4\u5219\u68c0\u67e5\uff08DRC\uff09\u8fdd\u89c4\u9884\u6d4b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u5bf9\u6570\u636e\u96c6\u7684\u9700\u6c42\u548c\u8bad\u7ec3\u65f6\u95f4\u957f\u7684\u75db\u70b9\u3002\u8be5\u65b9\u6cd5\u5728\u51c6\u786e\u7387\u4e0a\u4f18\u4e8e\u4f20\u7edf\u6a21\u578b\uff0c\u5e76\u5728\u8bad\u7ec3\u65f6\u95f4\u4e0a\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002", "motivation": "\u73b0\u6709\u7684\u673a\u5668\u5b66\u4e60\uff08ML\uff09\u548c\u795e\u7ecf\u7f51\u7edc\uff08NN\uff09\u6a21\u578b\u5728\u8bbe\u8ba1\u89c4\u5219\u68c0\u67e5\uff08DRC\uff09\u548c\u5149\u523b\u70ed\u70b9\u68c0\u6d4b\u65b9\u9762\u9700\u8981\u5927\u91cf\u5747\u8861\u7684\u6807\u6ce8\u6570\u636e\u96c6\u548c\u8f83\u957f\u7684\u8bad\u7ec3\u65f6\u95f4\uff0c\u800c\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bbe\u8ba1\u89c4\u5219\u68c0\u67e5\uff08DRC\uff09\u8fdd\u89c4\u9884\u6d4b\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u4f7f\u7528\u4e0d\u5e73\u8861\u6570\u636e\u96c6\uff0c\u4ec5\u9700\u4e00\u4e2a\u7c7b\u522b\u7684\u6807\u7b7e\u5373\u53ef\u8bad\u7ec3\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u8bbe\u5b9a\u9608\u503c\u6765\u5224\u65ad\u65b0\u6570\u636e\u7684\u5206\u7c7b\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65e0\u76d1\u7763\u6a21\u578b\u5728 99.95% \u7684\u9884\u6d4b\u6d4b\u8bd5\u51c6\u786e\u7387\uff0c\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u652f\u6301\u5411\u91cf\u673a\uff08SVM\uff09\u548c\u795e\u7ecf\u7f51\u7edc\uff08NN\uff09\u6a21\u578b\u7684\u51c6\u786e\u7387\u5206\u522b\u4e3a 85.44% \u548c 98.74%\u3002\u6b64\u5916\uff0c\u4e0e SVM \u548c NN \u6a21\u578b\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u7684\u8bad\u7ec3\u65f6\u95f4\u5206\u522b\u4f4e\u4e86\u7ea6 26.3 \u500d\u548c\u6700\u591a 6003 \u500d\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u9996\u4e2a\u65e0\u76d1\u7763\u8bbe\u8ba1\u89c4\u5219\u68c0\u67e5\uff08DRC\uff09\u8fdd\u89c4\u9884\u6d4b\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u53ef\u4f7f\u7528\u4e0d\u5e73\u8861\u6570\u636e\u96c6\uff0c\u4ec5\u9700\u4e00\u4e2a\u7c7b\u522b\u5373\u53ef\u6784\u5efa\u6a21\u578b\uff0c\u5e76\u80fd\u901a\u8fc7\u8bbe\u7f6e\u9608\u503c\u5bf9\u65b0\u6570\u636e\u8fdb\u884c\u5206\u7c7b\u3002"}}
{"id": "2507.10739", "categories": ["quant-ph", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2507.10739", "abs": "https://arxiv.org/abs/2507.10739", "authors": ["Marianna Podzorova", "Yi-Kai Liu"], "title": "Quantum Wave Atom Transforms", "comment": "45 pages, 12 figures", "summary": "This paper constructs the first quantum algorithm for wavelet packet\ntransforms with a tree structure, sometimes called wave atom transforms.\nClassically, wave atoms are used to construct sparse representations of\ndifferential operators, which enable fast numerical algorithms for partial\ndifferential equations. Compared to previous work, our quantum algorithm can\nimplement a larger class of wavelet and wave atom transforms, by using an\nefficient representation for a larger class of possible tree structures. Our\nquantum implementation has $O(\\mathrm{poly}(n))$ gate complexity for the\ntransform of dimension $2^n$, while classical implementations have $O(n 2^n)$\nfloating point operations. The result can be used to improve existing quantum\nalgorithms for solving hyperbolic partial differential equations.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.13634", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2507.13634", "abs": "https://arxiv.org/abs/2507.13634", "authors": ["Masahiro Ohkuma", "Keigo Arai", "Kenji Ohta", "Toru Shinmei", "Ryo Matsumoto", "Yoshihiko Takano", "Tetsuo Irifune"], "title": "Optically detected magnetic resonance of nitrogen-vacancy centers in microdiamonds inside nanopolycrystalline diamond anvil cell", "comment": null, "summary": "We demonstrated optically detected magnetic resonance (ODMR) of\nnitrogen-vacancy (NV) centers in microdiamonds inside a diamond anvil cell\npressurized with nanopolycrystalline diamond (NPD) anvils. NPD exhibits high\noptical transparency, superior hardness, and low thermal conductivity, making\nit suitable for optical and spectroscopic measurements under high-pressure and\nhigh-temperature conditions. We observed the ODMR signal from an ensemble of NV\ncenters under high pressures, reaching up to 20 GPa, with a culet diameter of\n600 $\\mu$m. We also performed ODMR measurements on multiple microdiamonds\nsealed inside a sample chamber and found that the resonance frequency varied\nwith the pressure distribution. The combination of NPD and microdiamonds\ncontaining NV centers is auspicious for pressure and magnetic imaging under\nconcurrent high-pressure and high-temperature conditions.", "AI": {"tldr": "\u5229\u7528NPD\u91d1\u521a\u77f3\u538b\u7827\u548cNV\u8272\u5fc3\u5fae\u91d1\u521a\u77f3\uff0c\u5728\u9ad8\u538b\u4e0b\u5b9e\u73b0\u4e86ODMR\u6d4b\u91cf\uff0c\u53ef\u7528\u4e8e\u538b\u529b\u548c\u78c1\u573a\u6210\u50cf\u3002", "motivation": "\u4e3a\u4e86\u5728\u9ad8\u538b\u9ad8\u6e29\u6761\u4ef6\u4e0b\u8fdb\u884c\u5149\u5b66\u548c\u5149\u8c31\u6d4b\u91cf\uff0c\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u5177\u6709\u9ad8\u5149\u5b66\u900f\u660e\u5ea6\u3001\u9ad8\u786c\u5ea6\u548c\u4f4e\u70ed\u5bfc\u7387\u7684\u6750\u6599\u3002", "method": "\u901a\u8fc7\u5728\u91d1\u521a\u77f3\u538b\u7827\u4e2d\uff0c\u4f7f\u7528NPD\u91d1\u521a\u77f3\u4f5c\u4e3a\u538b\u7827\uff0c\u5e76\u7ed3\u5408NV\u8272\u5fc3\u5fae\u91d1\u521a\u77f3\uff0c\u5b9e\u73b0\u4e86\u5149\u5b66\u63a2\u6d4b\u78c1\u5171\u632f\uff08ODMR\uff09\u6d4b\u91cf\u3002", "result": "\u5728\u9ad8\u8fbe20 GPa\u7684\u538b\u529b\u4e0b\uff0c\u89c2\u6d4b\u5230\u4e86NV\u8272\u5fc3\u5fae\u91d1\u521a\u77f3\u7684ODMR\u4fe1\u53f7\uff0c\u5e76\u53d1\u73b0\u4e86\u5171\u632f\u9891\u7387\u968f\u538b\u529b\u5206\u5e03\u7684\u53d8\u5316\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u53ef\u884c\u6027\u3002", "conclusion": "NPD\u4e0eNV\u8272\u5fc3\u5fae\u91d1\u521a\u77f3\u7684\u7ed3\u5408\u4e3a\u9ad8\u538b\u9ad8\u6e29\u6761\u4ef6\u4e0b\u7684\u538b\u529b\u548c\u78c1\u573a\u6210\u50cf\u63d0\u4f9b\u4e86\u6709\u5229\u6761\u4ef6\u3002"}}
{"id": "2507.13418", "categories": ["cond-mat.mes-hall", "nlin.CD"], "pdf": "https://arxiv.org/pdf/2507.13418", "abs": "https://arxiv.org/abs/2507.13418", "authors": ["Indubala Satija"], "title": "The Hofstadter Butterfly: Bridging Condensed Matter, Topology, and Number Theory", "comment": null, "summary": "Celebrating its golden jubilee, the Hofstadter butterfly fractal emerges as a\nremarkable fusion of art and science. This iconic X shaped fractal captivates\nphysicists, mathematicians, and enthusiasts alike by elegantly illustrating the\nenergy spectrum of electrons within a two dimensional crystal lattice\ninfluenced by a magnetic field. Enriched with integers of topological origin\nthat serve as quanta of Hall conductivity, this quantum fractal and its\nvariations have become paradigm models for topological insulators, novel states\nof matter in 21st century physics. This paper delves into the theoretical\nframework underlying butterfly fractality through the lenses of geometry and\nnumber theory. Within this poetic mathematics, we witness a rare form of\nquantum magic: Natures use of abstract fractals in crafting the butterfly graph\nitself. In its simplest form, the butterfly graph tessellates a two dimensional\nplane with trapezoids and triangles, where the quanta of Hall conductivity are\nembedded in the integer sloped diagonals of the trapezoids. The theoretical\nframework is succinctly expressed through unimodular matrices with integer\ncoefficients, bringing to life abstract constructs such as the Farey tree, the\nApollonian gaskets, and the Pythagorean triplet tree.", "AI": {"tldr": "Hofstadter\u5206\u5f62\u8776\u662f\u4e00\u4e2a\u827a\u672f\u4e0e\u79d1\u5b66\u7684\u878d\u5408\u4f53\uff0c\u5b83\u4f18\u96c5\u5730\u5c55\u793a\u4e86\u4e8c\u7ef4\u6676\u683c\u4e2d\u7535\u5b50\u7684\u80fd\u91cf\u8c31\u3002\u8be5\u8bba\u6587\u4ece\u51e0\u4f55\u548c\u6570\u8bba\u7684\u89d2\u5ea6\u63a2\u8ba8\u4e86\u5176\u80cc\u540e\u7684\u7406\u8bba\uff0c\u5f3a\u8c03\u4e86\u5b83\u5728\u62d3\u6251\u7edd\u7f18\u4f53\u548c\u65b0\u7269\u8d28\u6001\u7814\u7a76\u4e2d\u7684\u91cd\u8981\u6027\u3002\u8bba\u6587\u8fd8\u6307\u51fa\u4e86\u81ea\u7136\u754c\u5229\u7528\u5206\u5f62\u6765\u6784\u5efa\u8774\u8776\u56fe\uff0c\u5e76\u5c06\u970d\u5c14\u7535\u5bfc\u7684\u91cf\u5b50\u5d4c\u5165\u5176\u4e2d\uff0c\u6700\u7ec8\u901a\u8fc7\u4e00\u7cfb\u5217\u6570\u5b66\u6982\u5ff5\u6765\u9610\u8ff0\u8fd9\u4e00\u73b0\u8c61\u3002", "motivation": "Hofstadter\u5206\u5f62\u8776\u4f5c\u4e3a\u827a\u672f\u4e0e\u79d1\u5b66\u7684\u878d\u5408\uff0c\u4ee5\u53ca\u5176\u5728\u5c55\u793a\u4e8c\u7ef4\u6676\u683c\u4e2d\u7535\u5b50\u80fd\u91cf\u8c31\u65b9\u9762\u7684\u4f5c\u7528\uff0c\u6fc0\u53d1\u4e86\u4eba\u4eec\u5bf9\u5176\u80cc\u540e\u6570\u5b66\u539f\u7406\u7684\u63a2\u7d22\u3002", "method": "\u901a\u8fc7\u51e0\u4f55\u5b66\u548c\u6570\u8bba\u7684\u89c6\u89d2\uff0c\u9610\u8ff0\u4e86\u8774\u8776\u5206\u5f62\u56fe\u7684\u7406\u8bba\u6846\u67b6\uff0c\u5e76\u4f7f\u7528\u5177\u6709\u6574\u6570\u7cfb\u6570\u7684\u5e7a\u6a21\u77e9\u9635\u6765\u7b80\u6d01\u5730\u8868\u8fbe\u3002", "result": "\u8be5\u8bba\u6587\u63ed\u793a\u4e86\u81ea\u7136\u754c\u5728\u6784\u5efa\u8774\u8776\u56fe\u65f6\u5229\u7528\u62bd\u8c61\u5206\u5f62\uff0c\u5176\u4e2d\u970d\u5c14\u7535\u5bfc\u7684\u91cf\u5b50\u5d4c\u5165\u5728\u68af\u5f62\u7684\u6574\u6570\u659c\u8fb9\u4e2d\uff0c\u5e76\u901a\u8fc7\u5e7a\u6a21\u77e9\u9635\u3001Farey\u6811\u3001Apollonian gasket\u548c\u6bd5\u8fbe\u54e5\u62c9\u65af\u4e09\u5143\u7ec4\u6811\u7b49\u6570\u5b66\u6982\u5ff5\u5f97\u5230\u4e86\u4f53\u73b0\u3002", "conclusion": "\u8be5\u8bba\u6587\u6df1\u5165\u63a2\u8ba8\u4e86Hofstadter\u5206\u5f62\u8776\u7684\u7406\u8bba\u57fa\u7840\uff0c\u5f3a\u8c03\u4e86\u5176\u4f5c\u4e3a\u62d3\u6251\u7edd\u7f18\u4f53\u548c21\u4e16\u7eaa\u7269\u7406\u5b66\u4e2d\u65b0\u7269\u8d28\u6001\u7684\u8303\u4f8b\u6a21\u578b\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2507.14031", "categories": ["cs.CV", "cs.ET", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14031", "abs": "https://arxiv.org/abs/2507.14031", "authors": ["Hao Fang", "Sihao Teng", "Hao Yu", "Siyi Yuan", "Huaiwu He", "Zhe Liu", "Yunjie Yang"], "title": "QuantEIT: Ultra-Lightweight Quantum-Assisted Inference for Chest Electrical Impedance Tomography", "comment": "10 pages, 12 figures", "summary": "Electrical Impedance Tomography (EIT) is a non-invasive, low-cost bedside\nimaging modality with high temporal resolution, making it suitable for bedside\nmonitoring. However, its inherently ill-posed inverse problem poses significant\nchallenges for accurate image reconstruction. Deep learning (DL)-based\napproaches have shown promise but often rely on complex network architectures\nwith a large number of parameters, limiting efficiency and scalability. Here,\nwe propose an Ultra-Lightweight Quantum-Assisted Inference (QuantEIT) framework\nfor EIT image reconstruction. QuantEIT leverages a Quantum-Assisted Network\n(QA-Net), combining parallel 2-qubit quantum circuits to generate expressive\nlatent representations that serve as implicit nonlinear priors, followed by a\nsingle linear layer for conductivity reconstruction. This design drastically\nreduces model complexity and parameter number. Uniquely, QuantEIT operates in\nan unsupervised, training-data-free manner and represents the first integration\nof quantum circuits into EIT image reconstruction. Extensive experiments on\nsimulated and real-world 2D and 3D EIT lung imaging data demonstrate that\nQuantEIT outperforms conventional methods, achieving comparable or superior\nreconstruction accuracy using only 0.2% of the parameters, with enhanced\nrobustness to noise.", "AI": {"tldr": "QuantEIT\u662f\u4e00\u79cd\u8d85\u8f7b\u91cf\u7ea7\u7684\u91cf\u5b50\u8f85\u52a9EIT\u56fe\u50cf\u91cd\u5efa\u6846\u67b6\uff0c\u53c2\u6570\u91cf\u5c11\uff0c\u65e0\u9700\u8bad\u7ec3\u6570\u636e\uff0c\u91cd\u5efa\u7cbe\u5ea6\u9ad8\uff0c\u9c81\u68d2\u6027\u5f3a\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728EIT\u56fe\u50cf\u91cd\u5efa\u4e2d\u867d\u7136\u6709\u524d\u666f\uff0c\u4f46\u901a\u5e38\u9700\u8981\u590d\u6742\u7684\u7f51\u7edc\u7ed3\u6784\u548c\u5927\u91cf\u53c2\u6570\uff0c\u9650\u5236\u4e86\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aQuantEIT\u7684\u8d85\u8f7b\u91cf\u7ea7\u91cf\u5b50\u8f85\u52a9\u63a8\u7406\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u5e76\u884c\u4e24\u6bd4\u7279\u91cf\u5b50\u7535\u8def\uff08QA-Net\uff09\u751f\u6210\u5bcc\u6709\u8868\u73b0\u529b\u7684\u6f5c\u5728\u8868\u5f81\uff0c\u5e76\u901a\u8fc7\u5355\u4e2a\u7ebf\u6027\u5c42\u8fdb\u884c\u7535\u5bfc\u7387\u91cd\u5efa\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cQuantEIT\u5728\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u76842D/3D EIT\u80ba\u90e8\u6210\u50cf\u6570\u636e\u4e0a\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\uff0c\u53c2\u6570\u91cf\u51cf\u5c1199.8%\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u76f8\u5f53\u6216\u66f4\u4f18\u7684\u91cd\u5efa\u7cbe\u5ea6\uff0c\u5e76\u63d0\u9ad8\u4e86\u5bf9\u566a\u58f0\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "QuantEIT\u76f8\u8f83\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5728\u53c2\u6570\u91cf\u4ec5\u4e3a\u540e\u8005\u76840.2%\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86\u76f8\u5f53\u6216\u66f4\u4f18\u7684\u91cd\u5efa\u7cbe\u5ea6\uff0c\u5e76\u4e14\u5bf9\u566a\u58f0\u7684\u9c81\u68d2\u6027\u66f4\u5f3a\uff0c\u662fEIT\u56fe\u50cf\u91cd\u5efa\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.13510", "categories": ["cs.DS", "cs.CC", "15A69 (Primary), 15A15, 14N07 (Secondary)"], "pdf": "https://arxiv.org/pdf/2507.13510", "abs": "https://arxiv.org/abs/2507.13510", "authors": ["Benoit Jacob"], "title": "Strassen $2\\times2$ Matrix Multiplication from a 3-dimensional Volume Form", "comment": "13 pages", "summary": "The Strassen $2\\times2$ matrix multiplication algorithm arises from the\nvolume form on the 3-dimensional quotient space of the $2\\times 2$ matrices by\nthe multiples of identity.", "AI": {"tldr": "Strassen's $2\\times2$ matrix multiplication algorithm comes from the volume form of a specific quotient space of matrices.", "motivation": "The motivation is to understand the origins of the Strassen $2\\times2$ matrix multiplication algorithm through the lens of linear algebra and geometric concepts.", "method": "The method involves analyzing the volume form on the 3-dimensional quotient space of $2\\times2$ matrices.", "result": "The result is the derivation of the Strassen $2\\times2$ matrix multiplication algorithm from the specified mathematical structure.", "conclusion": "The Strassen $2\\times2$ matrix multiplication algorithm is derived from the volume form on the 3-dimensional quotient space of $2\\times2$ matrices by multiples of the identity."}}
{"id": "2507.13380", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13380", "abs": "https://arxiv.org/abs/2507.13380", "authors": ["Keito Inoshita", "Rushia Harada"], "title": "Persona-Based Synthetic Data Generation Using Multi-Stage Conditioning with Large Language Models for Emotion Recognition", "comment": null, "summary": "In the field of emotion recognition, the development of high-performance\nmodels remains a challenge due to the scarcity of high-quality, diverse\nemotional datasets. Emotional expressions are inherently subjective, shaped by\nindividual personality traits, socio-cultural backgrounds, and contextual\nfactors, making large-scale, generalizable data collection both ethically and\npractically difficult. To address this issue, we introduce PersonaGen, a novel\nframework for generating emotionally rich text using a Large Language Model\n(LLM) through multi-stage persona-based conditioning. PersonaGen constructs\nlayered virtual personas by combining demographic attributes, socio-cultural\nbackgrounds, and detailed situational contexts, which are then used to guide\nemotion expression generation. We conduct comprehensive evaluations of the\ngenerated synthetic data, assessing semantic diversity through clustering and\ndistributional metrics, human-likeness via LLM-based quality scoring, realism\nthrough comparison with real-world emotion corpora, and practical utility in\ndownstream emotion classification tasks. Experimental results show that\nPersonaGen significantly outperforms baseline methods in generating diverse,\ncoherent, and discriminative emotion expressions, demonstrating its potential\nas a robust alternative for augmenting or replacing real-world emotional\ndatasets.", "AI": {"tldr": "PersonaGen\u6846\u67b6\u5229\u7528LLM\u901a\u8fc7\u591a\u9636\u6bb5\u7684\u89d2\u8272\u626e\u6f14\u6765\u751f\u6210\u60c5\u611f\u4e30\u5bcc\u7684\u6587\u672c\uff0c\u4ee5\u89e3\u51b3\u60c5\u611f\u6570\u636e\u96c6\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u5e76\u5728\u591a\u6837\u6027\u3001\u4eba\u7c7b\u76f8\u4f3c\u6027\u548c\u5b9e\u9645\u5e94\u7528\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u60c5\u611f\u8bc6\u522b\u9886\u57df\u4e2d\u9ad8\u8d28\u91cf\u3001\u591a\u6837\u5316\u60c5\u611f\u6570\u636e\u96c6\u7a00\u7f3a\u7684\u6311\u6218\uff0c\u4ee5\u53ca\u60c5\u611f\u8868\u8fbe\u7684\u5185\u5728\u4e3b\u89c2\u6027\u548c\u5927\u89c4\u6a21\u3001\u53ef\u6cdb\u5316\u7684\u6570\u636e\u6536\u96c6\u7684\u56f0\u96be\u3002", "method": "PersonaGen\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u4eba\u53e3\u7edf\u8ba1\u5b66\u5c5e\u6027\u3001\u793e\u4f1a\u6587\u5316\u80cc\u666f\u548c\u8be6\u7ec6\u7684\u60c5\u5883\u80cc\u666f\u6765\u6784\u5efa\u5206\u5c42\u7684\u865a\u62df\u89d2\u8272\uff0c\u5e76\u5229\u7528\u8fd9\u4e9b\u89d2\u8272\u5f15\u5bfc\u60c5\u611f\u8868\u8fbe\u7684\u751f\u6210\u3002", "result": "PersonaGen\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u80fd\u591f\u751f\u6210\u591a\u6837\u5316\u3001\u8fde\u8d2f\u4e14\u5177\u6709\u533a\u5206\u6027\u7684\u60c5\u611f\u8868\u8fbe\uff0c\u4e3a\u6269\u5145\u6216\u66ff\u4ee3\u771f\u5b9e\u4e16\u754c\u7684\u60c5\u611f\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u4e00\u79cd\u5f3a\u5927\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "conclusion": "PersonaGen\u6846\u67b6\u901a\u8fc7\u591a\u9636\u6bb5\u7684\u3001\u57fa\u4e8e\u89d2\u8272\u7684\u6761\u4ef6\u8bbe\u7f6e\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u60c5\u611f\u4e30\u5bcc\u7684\u6587\u672c\uff0c\u4e3a\u60c5\u611f\u8bc6\u522b\u9886\u57df\u7684\u6570\u636e\u96c6\u7a00\u7f3a\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cPersonaGen\u751f\u6210\u7684\u5408\u6210\u6570\u636e\u5728\u591a\u6837\u6027\u3001\u4eba\u7c7b\u76f8\u4f3c\u6027\u548c\u73b0\u5b9e\u6027\u65b9\u9762\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u80fd\u6709\u6548\u63d0\u5347\u4e0b\u6e38\u60c5\u611f\u5206\u7c7b\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u663e\u793a\u51fa\u5176\u4f5c\u4e3a\u589e\u5f3a\u6216\u66ff\u4ee3\u771f\u5b9e\u60c5\u611f\u6570\u636e\u96c6\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.13437", "categories": ["quant-ph", "cond-mat.dis-nn", "cond-mat.stat-mech", "cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2507.13437", "abs": "https://arxiv.org/abs/2507.13437", "authors": ["Asadullah Bhuiyan", "Haining Pan", "Chao-Ming Jian"], "title": "Free Fermion Dynamics with Measurements: Topological Classification and Adaptive Preparation of Topological States", "comment": "51 pages, 8 figures", "summary": "We develop a general framework for classifying fermionic dynamical systems\nwith measurements using symmetry and topology. We discuss two complementary\nclassification schemes based on the Altland-Zirnbauer tenfold way: (1) the\nmany-body evolution operator (mEO) symmetry class, which classifies fermionic\ndynamics at the many-body level and generalizes to interacting dynamics, and\n(2) the single-particle transfer matrix (sTM) symmetry class, which classifies\nfree-fermion dynamics at the single-particle level and connects to Anderson\nlocalization physics. In the free-fermion limit, these two frameworks are in\none-to-one correspondence and yield equivalent topological classifications of\narea-law entangled dynamical phases. This leads to a novel dynamical\nbulk-boundary correspondence: the topology of the dynamical system's spacetime\n\\textit{bulk} determines the topology of the area-law entangled steady-state\nensemble living on its temporal \\textit{boundary}. Building on this\ncorrespondence, we provide a general realization of topological dynamical\nphases using Gaussian adaptive circuits. They are designed to prepare and\nstabilize free-fermion topological states as their steady states in\n\\textit{any} spatial dimension. While circuits with exponentially local\noperations can stabilize a single topological steady state, those with\nfinite-range operations can reach a topological steady-state ensemble. As a\ndemonstration, we explicitly construct and simulate 2+1d adaptive circuits that\nrealize mEO-class-A topological dynamics. We show that the finite-range\nversions converge to an ensemble of Chern insulators in $\\mathcal{O}(1)$\ncircuit depth. We numerically study the topological phase transitions and\ndynamical domain-wall modes between different topological dynamical phases in\nthis symmetry class. We also analyze the robustness of our adaptive circuit\nprotocol to coherent noise.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5206\u7c7b\u8d39\u7c73\u5b50\u52a8\u529b\u5b66\u7cfb\u7edf\u7684\u6846\u67b6\uff0c\u5229\u7528\u5bf9\u79f0\u6027\u548c\u62d3\u6251\u6027\uff0c\u901a\u8fc7mEO\u548csTM\u4e24\u79cd\u65b9\u6848\u3002\u53d1\u73b0\u4e86\u52a8\u529b\u5b66\u4f53\u8fb9\u754c\u5bf9\u5e94\uff0c\u5373\u65f6\u7a7a\u4f53\u62d3\u6251\u51b3\u5b9a\u65f6\u95f4\u8fb9\u754c\u7a33\u6001\u62d3\u6251\u3002\u8bbe\u8ba1\u4e86\u9ad8\u65af\u81ea\u9002\u5e94\u7535\u8def\u6765\u5b9e\u73b0\u8fd9\u4e9b\u62d3\u6251\u76f8\uff0c\u6709\u9650\u8303\u56f4\u7535\u8def\u53ef\u5236\u5907\u7a33\u6001\u96c6\u5408\u3002\u6a21\u62df\u663e\u793a2+1d\u7535\u8def\u53ef\u5b9e\u73b0\u9648\u7edd\u7f18\u4f53\uff0c\u5e76\u7814\u7a76\u4e86\u76f8\u53d8\u548c\u566a\u58f0\u9c81\u68d2\u6027\u3002", "motivation": "\u672c\u7814\u7a76\u7684\u52a8\u673a\u5728\u4e8e\u5f00\u53d1\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\u6765\u5206\u7c7b\u5177\u6709\u6d4b\u91cf\u7684\u8d39\u7c73\u5b50\u52a8\u529b\u5b66\u7cfb\u7edf\uff0c\u5e76\u5229\u7528\u5bf9\u79f0\u6027\u548c\u62d3\u6251\u6027\u6765\u7406\u89e3\u8fd9\u4e9b\u7cfb\u7edf\u7684\u884c\u4e3a\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u7814\u7a76\u4eba\u5458\u5e0c\u671b\u5efa\u7acb\u4e00\u4e2a\u80fd\u591f\u8fde\u63a5\u4e0d\u540c\u5c3a\u5ea6\uff08\u4ece\u5355\u7c92\u5b50\u5230\u8bb8\u591a\u4f53\uff09\u548c\u4e0d\u540c\u7269\u7406\u73b0\u8c61\uff08\u5982\u5b89\u5fb7\u68ee\u5b9a\u4f4d\u548c\u62d3\u6251\u76f8\uff09\u7684\u5206\u7c7b\u65b9\u6848\u3002\u901a\u8fc7\u5f15\u5165\u52a8\u529b\u5b66\u4f53\u8fb9\u754c\u5bf9\u5e94\uff0c\u4ed6\u4eec\u65e8\u5728\u63ed\u793a\u52a8\u529b\u5b66\u7cfb\u7edf\u5185\u90e8\uff08\u65f6\u7a7a\u4f53\uff09\u7684\u62d3\u6251\u7ed3\u6784\u5982\u4f55\u51b3\u5b9a\u5176\u8fb9\u754c\uff08\u7a33\u6001\u96c6\u5408\uff09\u7684\u62d3\u6251\u6027\u8d28\u3002\u6700\u7ec8\u76ee\u6807\u662f\u8bbe\u8ba1\u53ef\u884c\u7684\u7269\u7406\u5b9e\u73b0\u65b9\u6848\uff08\u5982\u9ad8\u65af\u81ea\u9002\u5e94\u7535\u8def\uff09\u6765\u5236\u5907\u548c\u63a7\u5236\u8fd9\u4e9b\u62d3\u6251\u52a8\u529b\u5b66\u76f8\uff0c\u5e76\u7814\u7a76\u5176\u6027\u8d28\u548c\u9c81\u68d2\u6027\u3002", "method": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u5206\u7c7b\u8d39\u7c73\u5b50\u52a8\u529b\u5b66\u7cfb\u7edf\u7684\u901a\u7528\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u57fa\u4e8e\u5bf9\u79f0\u6027\u548c\u62d3\u6251\u6027\u3002\u5b83\u63d0\u51fa\u4e86\u4e24\u79cd\u4e92\u8865\u7684\u5206\u7c7b\u65b9\u6848\uff0c\u5747\u57fa\u4e8eAltland-Zirnbauer\u5341\u91cd\u65b9\u6cd5\uff1a1. \u8bb8\u591a\u4f53\u6f14\u5316\u7b97\u5b50\uff08mEO\uff09\u5bf9\u79f0\u7c7b\uff0c\u7528\u4e8e\u5206\u7c7b\u8bb8\u591a\u4f53\u5c42\u9762\u7684\u8d39\u7c73\u5b50\u52a8\u529b\u5b66\uff0c\u5e76\u53ef\u63a8\u5e7f\u81f3\u76f8\u4e92\u4f5c\u7528\u52a8\u529b\u5b66\uff1b2. \u5355\u7c92\u5b50\u8f6c\u79fb\u77e9\u9635\uff08sTM\uff09\u5bf9\u79f0\u7c7b\uff0c\u7528\u4e8e\u5206\u7c7b\u5355\u7c92\u5b50\u5c42\u9762\u7684\u81ea\u7531\u8d39\u7c73\u5b50\u52a8\u529b\u5b66\uff0c\u5e76\u4e0e\u5b89\u5fb7\u68ee\u5b9a\u4f4d\u7269\u7406\u76f8\u5173\u3002\u5728\u81ea\u7531\u8d39\u7c73\u5b50\u6781\u9650\u4e0b\uff0c\u8fd9\u4e24\u79cd\u6846\u67b6\u5177\u6709\u4e00\u4e00\u5bf9\u5e94\u7684\u5173\u7cfb\uff0c\u5e76\u4ea7\u751f\u7b49\u4ef7\u7684\u9762\u79ef\u5b9a\u5f8b\u7ea0\u7f20\u52a8\u529b\u5b66\u76f8\u7684\u62d3\u6251\u5206\u7c7b\u3002\u57fa\u4e8e\u6b64\uff0c\u7814\u7a76\u4eba\u5458\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u52a8\u529b\u5b66\u4f53\u8fb9\u754c\u5bf9\u5e94\uff0c\u5373\u52a8\u529b\u5b66\u7cfb\u7edf\u7684\u65f6\u7a7a\u4f53\u533a\u57df\u7684\u62d3\u6251\u7ed3\u6784\u51b3\u5b9a\u4e86\u5176\u65f6\u95f4\u8fb9\u754c\u4e0a\u7684\u9762\u79ef\u5b9a\u5f8b\u7ea0\u7f20\u7a33\u6001\u96c6\u5408\u7684\u62d3\u6251\u7ed3\u6784\u3002\u4e3a\u4e86\u5b9e\u73b0\u62d3\u6251\u52a8\u529b\u5b66\u76f8\uff0c\u7814\u7a76\u4eba\u5458\u8bbe\u8ba1\u4e86\u9ad8\u65af\u81ea\u9002\u5e94\u7535\u8def\uff0c\u4ee5\u5728\u4efb\u4f55\u7a7a\u95f4\u7ef4\u5ea6\u4e0a\u5236\u5907\u548c\u7a33\u5b9a\u81ea\u7531\u8d39\u7c73\u5b50\u62d3\u6251\u6001\u4f5c\u4e3a\u7a33\u6001\u3002\u7814\u7a76\u8868\u660e\uff0c\u5177\u6709\u6307\u6570\u5c40\u57df\u64cd\u4f5c\u7684\u7535\u8def\u53ef\u4ee5\u7a33\u5b9a\u5355\u4e00\u62d3\u6251\u7a33\u6001\uff0c\u800c\u5177\u6709\u6709\u9650\u8303\u56f4\u64cd\u4f5c\u7684\u7535\u8def\u53ef\u4ee5\u8fbe\u5230\u4e00\u4e2a\u62d3\u6251\u7a33\u6001\u96c6\u5408\u3002\u4f5c\u4e3a\u5177\u4f53\u5b9e\u4f8b\uff0c\u7814\u7a76\u4eba\u5458\u6784\u5efa\u5e76\u6a21\u62df\u4e862+1\u7ef4\u81ea\u9002\u5e94\u7535\u8def\uff0c\u5b9e\u73b0\u4e86mEO A\u7c7b\u62d3\u6251\u52a8\u529b\u5b66\uff0c\u5e76\u8bc1\u660e\u4e86\u6709\u9650\u8303\u56f4\u7248\u672c\u5728$\\mathcal{O}(1)$\u7535\u8def\u6df1\u5ea6\u5185\u6536\u655b\u4e8e\u9648\u7edd\u7f18\u4f53\u96c6\u5408\u3002\u6b64\u5916\uff0c\u8fd8\u5bf9\u62d3\u6251\u76f8\u53d8\u3001\u52a8\u529b\u5b66\u7574\u58c1\u6a21\u5f0f\u4ee5\u53ca\u5bf9\u76f8\u5e72\u566a\u58f0\u7684\u9c81\u68d2\u6027\u8fdb\u884c\u4e86\u6570\u503c\u7814\u7a76\u3002", "result": "\u7814\u7a76\u4eba\u5458\u6210\u529f\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5bf9\u79f0\u6027\u548c\u62d3\u6251\u6027\u7684\u8d39\u7c73\u5b50\u52a8\u529b\u5b66\u7cfb\u7edf\u5206\u7c7b\u6846\u67b6\uff0c\u5e76\u63d0\u51fa\u4e86\u4e24\u79cd\u4e92\u8865\u7684\u5206\u7c7b\u65b9\u6848\uff08mEO\u548csTM\uff09\u3002\u4ed6\u4eec\u8bc1\u660e\u4e86\u5728\u81ea\u7531\u8d39\u7c73\u5b50\u6781\u9650\u4e0b\uff0c\u8fd9\u4e24\u79cd\u65b9\u6848\u7b49\u4ef7\uff0c\u5e76\u63ed\u793a\u4e86\u4e00\u79cd\u65b0\u7684\u52a8\u529b\u5b66\u4f53\u8fb9\u754c\u5bf9\u5e94\uff0c\u5373\u52a8\u529b\u5b66\u7cfb\u7edf\u7684\u65f6\u7a7a\u4f53\u62d3\u6251\u51b3\u5b9a\u4e86\u5176\u65f6\u95f4\u8fb9\u754c\u4e0a\u7684\u7a33\u6001\u96c6\u5408\u62d3\u6251\u3002\u57fa\u4e8e\u6b64\uff0c\u4ed6\u4eec\u8bbe\u8ba1\u4e86\u9ad8\u65af\u81ea\u9002\u5e94\u7535\u8def\uff0c\u80fd\u591f\u5236\u5907\u548c\u7a33\u5b9a\u81ea\u7531\u8d39\u7c73\u5b50\u62d3\u6251\u6001\u4f5c\u4e3a\u7a33\u6001\uff0c\u5e76\u8bc1\u660e\u4e86\u6709\u9650\u8303\u56f4\u64cd\u4f5c\u7684\u7535\u8def\u53ef\u5728\u6709\u9650\u6df1\u5ea6\u5185\u5b9e\u73b0\u9648\u7edd\u7f18\u4f53\u7a33\u6001\u96c6\u5408\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u5bf9\u62d3\u6251\u76f8\u53d8\u3001\u52a8\u529b\u5b66\u7574\u58c1\u6a21\u5f0f\u4ee5\u53ca\u534f\u8bae\u5bf9\u566a\u58f0\u7684\u9c81\u68d2\u6027\u8fdb\u884c\u4e86\u6570\u503c\u5206\u6790\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5206\u7c7b\u8d39\u7c73\u5b50\u52a8\u529b\u5b66\u7cfb\u7edf\u7684\u901a\u7528\u6846\u67b6\uff0c\u5229\u7528\u5bf9\u79f0\u6027\u548c\u62d3\u6251\u6027\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u4e24\u79cd\u4e92\u8865\u7684\u5206\u7c7b\u65b9\u6848\u5b9e\u73b0\uff1a\u4e00\u662f\u57fa\u4e8e\u8bb8\u591a\u4f53\u6f14\u5316\u7b97\u5b50\uff08mEO\uff09\u7684\u5bf9\u79f0\u6027\u5206\u7c7b\uff0c\u7528\u4e8e\u8bb8\u591a\u4f53\u5c42\u9762\u4e0a\u7684\u8d39\u7c73\u5b50\u52a8\u529b\u5b66\uff0c\u5e76\u53ef\u63a8\u5e7f\u81f3\u76f8\u4e92\u4f5c\u7528\u52a8\u529b\u5b66\uff1b\u4e8c\u662f\u57fa\u4e8e\u5355\u7c92\u5b50\u8f6c\u79fb\u77e9\u9635\uff08sTM\uff09\u7684\u5bf9\u79f0\u6027\u5206\u7c7b\uff0c\u7528\u4e8e\u5355\u7c92\u5b50\u5c42\u9762\u7684\u81ea\u7531\u8d39\u7c73\u5b50\u52a8\u529b\u5b66\uff0c\u5e76\u4e0e\u5b89\u5fb7\u68ee\u5b9a\u4f4d\u7269\u7406\u76f8\u5173\u3002\u5728\u81ea\u7531\u8d39\u7c73\u5b50\u6781\u9650\u4e0b\uff0c\u8fd9\u4e24\u79cd\u65b9\u6848\u4e00\u4e00\u5bf9\u5e94\uff0c\u5e76\u4ea7\u751f\u7b49\u4ef7\u7684\u9762\u79ef\u5b9a\u5f8b\u7ea0\u7f20\u52a8\u529b\u5b66\u76f8\u7684\u62d3\u6251\u5206\u7c7b\u3002\u8fd9\u5f15\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u52a8\u529b\u5b66\u4f53\u8fb9\u754c\u5bf9\u5e94\uff1a\u52a8\u529b\u5b66\u7cfb\u7edf\u7684\u65f6\u7a7a\u4f53\u533a\u57df\u7684\u62d3\u6251\u7ed3\u6784\u51b3\u5b9a\u4e86\u5176\u65f6\u95f4\u8fb9\u754c\u4e0a\u7684\u9762\u79ef\u5b9a\u5f8b\u7ea0\u7f20\u7a33\u6001\u96c6\u5408\u7684\u62d3\u6251\u7ed3\u6784\u3002\u57fa\u4e8e\u6b64\u5bf9\u5e94\u5173\u7cfb\uff0c\u7814\u7a76\u4eba\u5458\u901a\u8fc7\u9ad8\u65af\u81ea\u9002\u5e94\u7535\u8def\u901a\u7528\u5b9e\u73b0\u4e86\u62d3\u6251\u52a8\u529b\u5b66\u76f8\uff0c\u8fd9\u4e9b\u7535\u8def\u65e8\u5728\u5236\u5907\u548c\u7a33\u5b9a\u81ea\u7531\u8d39\u7c73\u5b50\u62d3\u6251\u6001\u4f5c\u4e3a\u5176\u5728\u4efb\u4f55\u7a7a\u95f4\u7ef4\u5ea6\u4e0a\u7684\u7a33\u6001\u3002\u7814\u7a76\u8868\u660e\uff0c\u5177\u6709\u6307\u6570\u5c40\u57df\u64cd\u4f5c\u7684\u7535\u8def\u53ef\u4ee5\u7a33\u5b9a\u5355\u4e00\u62d3\u6251\u7a33\u6001\uff0c\u800c\u5177\u6709\u6709\u9650\u8303\u56f4\u64cd\u4f5c\u7684\u7535\u8def\u53ef\u4ee5\u8fbe\u5230\u4e00\u4e2a\u62d3\u6251\u7a33\u6001\u96c6\u5408\u3002\u4f5c\u4e3a\u8bc1\u660e\uff0c\u7814\u7a76\u4eba\u5458\u660e\u786e\u6784\u5efa\u5e76\u6a21\u62df\u4e862+1\u7ef4\u81ea\u9002\u5e94\u7535\u8def\uff0c\u5b9e\u73b0\u4e86mEO A\u7c7b\u62d3\u6251\u52a8\u529b\u5b66\u3002\u4ed6\u4eec\u53d1\u73b0\u6709\u9650\u8303\u56f4\u7248\u672c\u5728$\\mathcal{O}(1)$\u7535\u8def\u6df1\u5ea6\u5185\u6536\u655b\u4e8e\u9648\u7edd\u7f18\u4f53\u96c6\u5408\u3002\u7814\u7a76\u4eba\u5458\u8fd8\u5bf9\u8be5\u5bf9\u79f0\u7c7b\u522b\u4e0b\u4e0d\u540c\u62d3\u6251\u52a8\u529b\u5b66\u76f8\u4e4b\u95f4\u7684\u62d3\u6251\u76f8\u53d8\u548c\u52a8\u529b\u5b66\u7574\u58c1\u6a21\u5f0f\u8fdb\u884c\u4e86\u6570\u503c\u7814\u7a76\uff0c\u5e76\u5206\u6790\u4e86\u81ea\u9002\u5e94\u7535\u8def\u534f\u8bae\u5bf9\u76f8\u5e72\u566a\u58f0\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.13549", "categories": ["cs.NE"], "pdf": "https://arxiv.org/pdf/2507.13549", "abs": "https://arxiv.org/abs/2507.13549", "authors": ["Jim O'Connor", "Nicholas Lorentzen", "Gary B. Parker", "Derin Gezgin"], "title": "Evolving Neural Controllers for Xpilot-AI Racing Using Neuroevolution of Augmenting Topologies", "comment": "IJCCI Conference on Evolutionary Computation and Theory and\n  Applications, 2025", "summary": "This paper investigates the development of high-performance racing\ncontrollers for a newly implemented racing mode within the Xpilot-AI platform,\nutilizing the Neuro Evolution of Augmenting Topologies (NEAT) algorithm. By\nleveraging NEAT's capability to evolve both the structure and weights of neural\nnetworks, we develop adaptive controllers that can navigate complex circuits\nunder the challenging space simulation physics of Xpilot-AI, which includes\nelements such as inertia, friction, and gravity. The racing mode we introduce\nsupports flexible circuit designs and allows for the evaluation of multiple\nagents in parallel, enabling efficient controller optimization across\ngenerations. Experimental results demonstrate that our evolved controllers\nachieve up to 32% improvement in lap time compared to the controller's initial\nperformance and develop effective racing strategies, such as optimal cornering\nand speed modulation, comparable to human-like techniques. This work\nillustrates NEAT's effectiveness in producing robust control strategies within\ndemanding game environments and highlights Xpilot-AI's potential as a rigorous\ntestbed for competitive AI controller evolution.", "AI": {"tldr": "NEAT \u7b97\u6cd5\u8fdb\u5316\u51fa\u4e86\u9ad8\u6027\u80fd\u7684 Xpilot-AI \u8d5b\u8f66\u63a7\u5236\u5668\uff0c\u63d0\u5347\u4e86 32% \u7684\u5355\u5708\u8868\u73b0\u3002", "motivation": "\u4e3a Xpilot-AI \u5e73\u53f0\u7684\u65b0\u8d5b\u8f66\u6a21\u5f0f\u5f00\u53d1\u9ad8\u6027\u80fd\u8d5b\u8f66\u63a7\u5236\u5668\uff0c\u4ee5\u5e94\u5bf9\u5176\u590d\u6742\u7684\u7269\u7406\u5f15\u64ce\u548c\u7075\u6d3b\u7684\u8d5b\u9053\u8bbe\u8ba1\u3002", "method": "\u5229\u7528\u795e\u7ecf\u62d3\u6251\u8fdb\u5316\uff08NEAT\uff09\u7b97\u6cd5\uff0c\u901a\u8fc7\u8fdb\u5316\u795e\u7ecf\u7f51\u7edc\u7684\u7ed3\u6784\u548c\u6743\u91cd\u6765\u5f00\u53d1\u81ea\u9002\u5e94\u63a7\u5236\u5668\u3002", "result": "\u8fdb\u5316\u7684\u63a7\u5236\u5668\u5728\u5355\u5708\u65f6\u95f4\u4e0a\u6bd4\u521d\u59cb\u63a7\u5236\u5668\u6709\u9ad8\u8fbe 32% \u7684\u63d0\u5347\uff0c\u5e76\u80fd\u53d1\u5c55\u51fa\u6700\u4f18\u8fc7\u5f2f\u548c\u901f\u5ea6\u8c03\u8282\u7b49\u6709\u6548\u7684\u6bd4\u8d5b\u7b56\u7565\u3002", "conclusion": "NEAT \u7b97\u6cd5\u5728 Xpilot-AI \u5e73\u53f0\u7684\u65b0\u8d5b\u8f66\u6a21\u5f0f\u4e0b\u80fd\u591f\u6709\u6548\u5730\u8fdb\u5316\u51fa\u9ad8\u6027\u80fd\u7684\u8d5b\u8f66\u63a7\u5236\u5668\uff0c\u8fd9\u4e9b\u63a7\u5236\u5668\u80fd\u591f\u5b66\u4e60\u5230\u7c7b\u4f3c\u4eba\u7c7b\u7684\u8d5b\u8f66\u7b56\u7565\uff0c\u5e76\u4e14\u5728\u6bd4\u8d5b\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6210\u7ee9\u3002"}}
{"id": "2507.13383", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13383", "abs": "https://arxiv.org/abs/2507.13383", "authors": ["Charvi Rastogi", "Tian Huey Teh", "Pushkar Mishra", "Roma Patel", "Ding Wang", "Mark D\u00edaz", "Alicia Parrish", "Aida Mostafazadeh Davani", "Zoe Ashwood", "Michela Paganini", "Vinodkumar Prabhakaran", "Verena Rieser", "Lora Aroyo"], "title": "Whose View of Safety? A Deep DIVE Dataset for Pluralistic Alignment of Text-to-Image Models", "comment": "28 pages, 16 figures", "summary": "Current text-to-image (T2I) models often fail to account for diverse human\nexperiences, leading to misaligned systems. We advocate for pluralistic\nalignment, where an AI understands and is steerable towards diverse, and often\nconflicting, human values. Our work provides three core contributions to\nachieve this in T2I models. First, we introduce a novel dataset for Diverse\nIntersectional Visual Evaluation (DIVE) -- the first multimodal dataset for\npluralistic alignment. It enable deep alignment to diverse safety perspectives\nthrough a large pool of demographically intersectional human raters who\nprovided extensive feedback across 1000 prompts, with high replication,\ncapturing nuanced safety perceptions. Second, we empirically confirm\ndemographics as a crucial proxy for diverse viewpoints in this domain,\nrevealing significant, context-dependent differences in harm perception that\ndiverge from conventional evaluations. Finally, we discuss implications for\nbuilding aligned T2I models, including efficient data collection strategies,\nLLM judgment capabilities, and model steerability towards diverse perspectives.\nThis research offers foundational tools for more equitable and aligned T2I\nsystems. Content Warning: The paper includes sensitive content that may be\nharmful.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u201c\u591a\u5143\u5316\u5bf9\u9f50\u201d\u65b9\u6cd5\uff0c\u5e76\u521b\u5efa\u4e86DIVE\u6570\u636e\u96c6\uff0c\u4ee5\u89e3\u51b3\u5f53\u524d\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u672a\u80fd\u5145\u5206\u8003\u8651\u4eba\u7c7b\u591a\u6837\u5316\u7ecf\u9a8c\u7684\u95ee\u9898\u3002\u7814\u7a76\u8bc1\u660e\u4e86\u4eba\u53e3\u7edf\u8ba1\u5b66\u4fe1\u606f\u5bf9\u4e8e\u7406\u89e3\u548c\u89e3\u51b3\u6a21\u578b\u4e2d\u7684\u504f\u89c1\u81f3\u5173\u91cd\u8981\uff0c\u5e76\u4e3a\u6784\u5efa\u66f4\u516c\u5e73\u3001\u66f4\u5bf9\u9f50\u7684T2I\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002", "motivation": "\u5f53\u524d\u7684\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u6a21\u578b\u672a\u80fd\u5145\u5206\u8003\u8651\u4eba\u7c7b\u7ecf\u9a8c\u7684\u591a\u6837\u6027\uff0c\u5bfc\u81f4\u7cfb\u7edf\u5931\u51c6\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u7406\u89e3\u5e76\u53ef\u63a7\u4e8e\u591a\u6837\u5316\u3001\u751a\u81f3\u76f8\u4e92\u51b2\u7a81\u7684\u4eba\u7c7b\u4ef7\u503c\u89c2\u7684\u201c\u591a\u5143\u5316\u5bf9\u9f50\u201d\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u591a\u5143\u5316\u5bf9\u9f50\u201d\u7684\u65b9\u6cd5\uff0c\u5e76\u6784\u5efa\u4e86DIVE\uff08Diverse Intersectional Visual Evaluation\uff09\u6570\u636e\u96c6\uff0c\u8fd9\u662f\u4e00\u4e2a\u5305\u542b1000\u4e2a\u63d0\u793a\u548c\u5927\u91cf\u4eba\u53e3\u7edf\u8ba1\u5b66\u80cc\u666f\u7684\u4eba\u7c7b\u8bc4\u4f30\u8005\u53cd\u9988\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30T2I\u6a21\u578b\u5728\u4e0d\u540c\u5b89\u5168\u89c2\u5ff5\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8bc1\u5b9e\u4e86\u4eba\u53e3\u7edf\u8ba1\u5b66\u5728T2I\u6a21\u578b\u5bf9\u9f50\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u53d1\u73b0\u4e0d\u540c\u4eba\u53e3\u7edf\u8ba1\u5b66\u7fa4\u4f53\u5bf9\u5371\u5bb3\u7684\u611f\u77e5\u5b58\u5728\u663e\u8457\u4e14\u4f9d\u8d56\u4e8e\u4e0a\u4e0b\u6587\u7684\u5dee\u5f02\uff0c\u8fd9\u4e0e\u4f20\u7edf\u7684\u8bc4\u4f30\u65b9\u6cd5\u4e0d\u540c\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u8ba8\u8bba\u4e86\u5728\u9ad8\u6548\u6570\u636e\u6536\u96c6\u3001\u8bed\u8a00\u6a21\u578b\u5224\u65ad\u80fd\u529b\u4ee5\u53ca\u6a21\u578b\u53ef\u63a7\u6027\u4ee5\u9002\u5e94\u4e0d\u540c\u89c2\u70b9\u65b9\u9762\u7684\u6f5c\u5728\u5f71\u54cd\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u6784\u5efa\u66f4\u516c\u5e73\u3001\u66f4\u5bf9\u9f50\u7684\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u7cfb\u7edf\u63d0\u4f9b\u4e86\u57fa\u7840\u5de5\u5177\uff0c\u5e76\u901a\u8fc7DIVE\u6570\u636e\u96c6\u3001\u5bf9\u4eba\u53e3\u7edf\u8ba1\u5b66\u4f5c\u4e3a\u591a\u6837\u5316\u89c2\u70b9\u7684\u4ee3\u7406\u7684\u5b9e\u8bc1\u68c0\u9a8c\u4ee5\u53ca\u5bf9\u9f50T2I\u6a21\u578b\u7684\u8ba8\u8bba\uff0c\u4e3a\u5b9e\u73b0T2I\u6a21\u578b\u4e2d\u7684\u591a\u5143\u5316\u5bf9\u9f50\u505a\u51fa\u4e86\u8d21\u732e\u3002"}}
{"id": "2507.14039", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2507.14039", "abs": "https://arxiv.org/abs/2507.14039", "authors": ["Jiaxin Song", "Biaoshuai Tao", "Wenqian Wang", "Yuhao Zhang"], "title": "Online MMS Allocation for Chores", "comment": null, "summary": "We study the problem of fair division of indivisible chores among $n$ agents\nin an online setting, where items arrive sequentially and must be allocated\nirrevocably upon arrival. The goal is to produce an $\\alpha$-MMS allocation at\nthe end. Several recent works have investigated this model, but have only\nsucceeded in obtaining non-trivial algorithms under restrictive assumptions,\nsuch as the two-agent bi-valued special case (Wang and Wei, 2025), or by\nassuming knowledge of the total disutility of each agent (Zhou, Bai, and Wu,\n2023). For the general case, the trivial $n$-MMS guarantee remains the best\nknown, while the strongest lower bound is still only $2$.\n  We close this gap on the negative side by proving that for any fixed $n$ and\n$\\varepsilon$, no algorithm can guarantee an $(n - \\varepsilon)$-MMS\nallocation. Notably, this lower bound holds precisely for every $n$, without\nhiding constants in big-$O$ notation, thereby exactly matching the trivial\nupper bound.\n  Despite this strong impossibility result, we also present positive results.\nWe provide an online algorithm that applies in the general case, guaranteeing a\n$\\min\\{n, O(k), O(\\log D)\\}$-MMS allocation, where $k$ is the maximum number of\ndistinct disutilities across all agents and $D$ is the maximum ratio between\nthe largest and smallest disutilities for any agent. This bound is reasonable\nacross a broad range of scenarios and, for example, implies that we can achieve\nan $O(1)$-MMS allocation whenever $k$ is constant. Moreover, to optimize the\nconstant in the important personalized bi-valued case, we show that if each\nagent has at most two distinct disutilities, our algorithm guarantees a $(2 +\n\\sqrt{3}) \\approx 3.7$-MMS allocation.", "AI": {"tldr": "\u5728\u7dda\u4e0a\u5206\u914d\u96dc\u52d9\u7d66\u591a\u500b\u4ee3\u7406\u7684\u96e3\u984c\u4e2d\uff0c\u6211\u5011\u8b49\u660e\u4e86\u7121\u6cd5\u5be6\u73fe\u63a5\u8fd1\u5b8c\u7f8e\u7684\u516c\u5e73\u5206\u914d\uff0c\u4f46\u63d0\u51fa\u4e86\u4e00\u7a2e\u5728\u96dc\u52d9\u7a2e\u985e\u6216\u4ee3\u7406\u504f\u597d\u6709\u9650\u7684\u60c5\u6cc1\u4e0b\u53ef\u4ee5\u5be6\u73fe\u76f8\u7576\u516c\u5e73\u5206\u914d\u7684\u6f14\u7b97\u6cd5\u3002", "motivation": "\u9019\u9805\u7814\u7a76\u7684\u52d5\u6a5f\u662f\u89e3\u6c7a\u5728\u7dda\u4e0a\u74b0\u5883\u4e2d\u5c07\u4e0d\u53ef\u5206\u5272\u7684\u96dc\u52d9\u5206\u914d\u7d66 n \u500b\u4ee3\u7406\u7684\u554f\u984c\uff0c\u5176\u4e2d\u7269\u54c1\u6309\u9806\u5e8f\u5230\u9054\uff0c\u5fc5\u9808\u5728\u5230\u9054\u6642\u4e0d\u53ef\u64a4\u92b7\u5730\u5206\u914d\uff0c\u540c\u6642\u76ee\u6a19\u662f\u7522\u751f \u03b1-MMS \u5206\u914d\u3002\u73fe\u6709\u7814\u7a76\u5728\u9650\u5236\u6027\u5047\u8a2d\u4e0b\u50c5\u7372\u5f97\u4e86\u975e\u5e73\u51e1\u7684\u6f14\u7b97\u6cd5\uff0c\u6216\u8005\u5047\u8a2d\u4e86\u4ee3\u7406\u7684\u7e3d\u6548\u7528\u77e5\u8b58\u3002", "method": "\u7814\u7a76\u8868\u660e\uff0c\u5c0d\u65bc\u4efb\u4f55\u56fa\u5b9a\u7684 n \u548c \u03b5\uff0c\u6c92\u6709\u6f14\u7b97\u6cd5\u53ef\u4ee5\u4fdd\u8b49 (n - \u03b5)-MMS \u5206\u914d\uff0c\u9019\u8207\u50c5\u6709\u7684 n-MMS \u4e0a\u754c\u76f8\u5339\u914d\u3002", "result": "\u8b49\u660e\u4e86\u5c0d\u65bc\u4efb\u4f55\u56fa\u5b9a\u7684 n \u548c \u03b5\uff0c\u6c92\u6709\u6f14\u7b97\u6cd5\u80fd\u5920\u4fdd\u8b49 (n - \u03b5)-MMS \u5206\u914d\uff0c\u9019\u7cbe\u78ba\u5730\u5339\u914d\u4e86\u5e73\u51e1\u7684\u4e0a\u9650\u3002\u6b64\u5916\uff0c\u9084\u63d0\u51fa\u4e86\u4e00\u7a2e\u7dda\u4e0a\u6f14\u7b97\u6cd5\uff0c\u4fdd\u8b49\u4e86 min{n, O(k), O(log D)}-MMS \u5206\u914d\uff0c\u5728 k \u662f\u5e38\u6578\u7684\u60c5\u6cc1\u4e0b\u53ef\u4ee5\u9054\u5230 O(1)-MMS \u5206\u914d\uff0c\u4e26\u4e14\u5728\u500b\u6027\u5316\u96d9\u503c\u60c5\u6cc1\u4e0b\u53ef\u4ee5\u9054\u5230 (2 + sqrt(3)) \u2248 3.7-MMS \u5206\u914d\u3002", "conclusion": "\u5118\u7ba1\u5b58\u5728\u5f37\u5927\u7684\u4e0d\u53ef\u80fd\u7d50\u679c\uff0c\u4f46\u7814\u7a76\u4e5f\u53d6\u5f97\u4e86\u4e00\u4e9b\u7a4d\u6975\u7684\u6210\u679c\u3002\u63d0\u4f9b\u4e86\u4e00\u7a2e\u901a\u7528\u7684\u7dda\u4e0a\u6f14\u7b97\u6cd5\uff0c\u4fdd\u8b49\u4e86 min{n, O(k), O(log D)}-MMS \u5206\u914d\uff0c\u5176\u4e2d k \u662f\u6240\u6709\u4ee3\u7406\u4e4b\u9593\u4e0d\u540c\u6548\u7528\u7684\u6700\u5927\u6578\u91cf\uff0cD \u662f\u4efb\u4f55\u4ee3\u7406\u7684\u6700\u5927\u8207\u6700\u5c0f\u6548\u7528\u4e4b\u9593\u7684\u6bd4\u7387\u3002\u6b64\u5916\uff0c\u5c0d\u65bc\u6bcf\u500b\u4ee3\u7406\u6700\u591a\u53ea\u6709\u5169\u7a2e\u4e0d\u540c\u6548\u7528\u7684\u500b\u6027\u5316\u96d9\u503c\u60c5\u6cc1\uff0c\u8a72\u6f14\u7b97\u6cd5\u4fdd\u8b49\u4e86 (2 + sqrt(3)) \u2248 3.7-MMS \u5206\u914d\u3002"}}
{"id": "2507.13368", "categories": ["cs.SI", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13368", "abs": "https://arxiv.org/abs/2507.13368", "authors": ["Yaowen Hu", "Wenxuan Tu", "Yue Liu", "Xinhang Wan", "Junyi Yan", "Taichun Zhou", "Xinwang Liu"], "title": "Scalable Attribute-Missing Graph Clustering via Neighborhood Differentiatio", "comment": null, "summary": "Deep graph clustering (DGC), which aims to unsupervisedly separate the nodes\nin an attribute graph into different clusters, has seen substantial potential\nin various industrial scenarios like community detection and recommendation.\nHowever, the real-world attribute graphs, e.g., social networks interactions,\nare usually large-scale and attribute-missing. To solve these two problems, we\npropose a novel DGC method termed \\underline{\\textbf{C}}omplementary\n\\underline{\\textbf{M}}ulti-\\underline{\\textbf{V}}iew\n\\underline{\\textbf{N}}eighborhood \\underline{\\textbf{D}}ifferentiation\n(\\textit{CMV-ND}), which preprocesses graph structural information into\nmultiple views in a complete but non-redundant manner. First, to ensure\ncompleteness of the structural information, we propose a recursive neighborhood\nsearch that recursively explores the local structure of the graph by completely\nexpanding node neighborhoods across different hop distances. Second, to\neliminate the redundancy between neighborhoods at different hops, we introduce\na neighborhood differential strategy that ensures no overlapping nodes between\nthe differential hop representations. Then, we construct $K+1$ complementary\nviews from the $K$ differential hop representations and the features of the\ntarget node. Last, we apply existing multi-view clustering or DGC methods to\nthe views. Experimental results on six widely used graph datasets demonstrate\nthat CMV-ND significantly improves the performance of various methods.", "AI": {"tldr": "CMV-ND\u662f\u4e00\u79cd\u65b0\u7684\u6df1\u5ea6\u56fe\u805a\u7c7b\u65b9\u6cd5\uff0c\u901a\u8fc7\u5904\u7406\u56fe\u8c31\u7684\u7ed3\u6784\u4fe1\u606f\u6765\u63d0\u9ad8\u805a\u7c7b\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u5927\u89c4\u6a21\u548c\u5c5e\u6027\u7f3a\u5931\u7684\u56fe\u8c31\u65f6\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5b9e\u9645\u5e94\u7528\u4e2d\u56fe\u8c31\u89c4\u6a21\u5927\u4e14\u5c5e\u6027\u7f3a\u5931\u7684\u95ee\u9898\uff0c\u63d0\u51faCMV-ND\u65b9\u6cd5\u3002", "method": "CMV-ND\u901a\u8fc7\u9012\u5f52\u90bb\u57df\u641c\u7d22\u548c\u90bb\u57df\u5dee\u5f02\u5316\u7b56\u7565\u6765\u5904\u7406\u56fe\u7ed3\u6784\u4fe1\u606f\uff0c\u751f\u6210\u591a\u89c6\u56fe\u8868\u793a\uff0c\u5e76\u5e94\u7528\u4e8e\u73b0\u6709\u7684\u591a\u89c6\u56fe\u805a\u7c7b\u6216DGC\u65b9\u6cd5\u3002", "result": "CMV-ND\u5728\u516d\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u56fe\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u591a\u79cd\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "CMV-ND in \u5b9e\u9a8c\u4e2d\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u591a\u79cd\u65b9\u6cd5\u7684\u6027\u80fd\u3002"}}
{"id": "2507.13520", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.13520", "abs": "https://arxiv.org/abs/2507.13520", "authors": ["Sizhen Bian", "Mengxi Liu", "Paul Lukowicz"], "title": "Passive Body-Area Electrostatic Field (Human Body Capacitance) for Ubiquitous Computing", "comment": null, "summary": "Passive body-area electrostatic field sensing, also referred to as human body\ncapacitance (HBC), is an energy-efficient and non-intrusive sensing modality\nthat exploits the human body's inherent electrostatic properties to perceive\nhuman behaviors. This paper presents a focused overview of passive HBC sensing,\nincluding its underlying principles, historical evolution, hardware\narchitectures, and applications across research domains. Key challenges, such\nas susceptibility to environmental variation, are discussed to trigger\nmitigation techniques. Future research opportunities in sensor fusion and\nhardware enhancement are highlighted. To support continued innovation, this\nwork provides open-source resources and aims to empower researchers and\ndevelopers to leverage passive electrostatic sensing for next-generation\nwearable and ambient intelligence systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6982\u8ff0\u4e86\u65e0\u6e90\u4eba\u4f53\u8eab\u4f53\u7535\u5bb9\uff08HBC\uff09\u4f20\u611f\u6280\u672f\uff0c\u5305\u62ec\u5176\u539f\u7406\u3001\u53d1\u5c55\u3001\u5e94\u7528\u548c\u6311\u6218\uff0c\u5e76\u63d0\u4f9b\u4e86\u5f00\u6e90\u8d44\u6e90\u4ee5\u63a8\u52a8\u76f8\u5173\u9886\u57df\u7684\u7814\u7a76\u3002", "motivation": "\u65e8\u5728\u4e3a\u65e0\u6e90HBC\u4f20\u611f\u63d0\u4f9b\u4e00\u4e2a\u96c6\u4e2d\u7684\u6982\u8ff0\uff0c\u5e76\u6fc0\u53d1\u7f13\u89e3\u73af\u5883\u53d8\u5316\u7b49\u6311\u6218\u7684\u6280\u672f\uff0c\u540c\u65f6\u4e3a\u4e0b\u4e00\u4ee3\u53ef\u7a7f\u6234\u548c\u73af\u5883\u667a\u80fd\u7cfb\u7edf\u63d0\u4f9b\u652f\u6301\u3002", "method": "\u5bf9\u65e0\u6e90HBC\u4f20\u611f\u7684\u539f\u7406\u3001\u5386\u53f2\u6f14\u53d8\u3001\u786c\u4ef6\u67b6\u6784\u548c\u5e94\u7528\u8fdb\u884c\u4e86\u6982\u8ff0\u3002", "result": "\u63d0\u4f9b\u4e86\u65e0\u6e90HBC\u4f20\u611f\u7684\u5168\u9762\u6982\u8ff0\uff0c\u8ba8\u8bba\u4e86\u5173\u952e\u6311\u6218\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u548c\u673a\u9047\u3002", "conclusion": "\u8be5\u8bba\u6587\u603b\u7ed3\u4e86\u65e0\u6e90\u4f53\u57df\u9759\u7535\u573a\u4f20\u611f\uff08\u4eba\u7c7b\u8eab\u4f53\u7535\u5bb9\uff0cHBC\uff09\u7684\u539f\u7406\u3001\u53d1\u5c55\u3001\u786c\u4ef6\u67b6\u6784\u548c\u5e94\u7528\uff0c\u5e76\u8ba8\u8bba\u4e86\u73af\u5883\u53d8\u5316\u7b49\u6311\u6218\u53ca\u4f20\u611f\u5668\u878d\u5408\u3001\u786c\u4ef6\u589e\u5f3a\u7684\u672a\u6765\u673a\u9047\uff0c\u6700\u540e\u63d0\u4f9b\u4e86\u5f00\u6e90\u8d44\u6e90\u4ee5\u4fc3\u8fdb\u76f8\u5173\u7814\u7a76\u548c\u5f00\u53d1\u3002"}}
{"id": "2507.13360", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13360", "abs": "https://arxiv.org/abs/2507.13360", "authors": ["Le-Anh Tran", "Chung Nguyen Tran", "Ngoc-Luu Nguyen", "Nhan Cach Dang", "Jordi Carrabina", "David Castells-Rufas", "Minh Son Nguyen"], "title": "Low-Light Enhancement via Encoder-Decoder Network with Illumination Guidance", "comment": "6 pages, 3 figures, ICCCE 2025", "summary": "This paper introduces a novel deep learning framework for low-light image\nenhancement, named the Encoder-Decoder Network with Illumination Guidance\n(EDNIG). Building upon the U-Net architecture, EDNIG integrates an illumination\nmap, derived from Bright Channel Prior (BCP), as a guidance input. This\nillumination guidance helps the network focus on underexposed regions,\neffectively steering the enhancement process. To further improve the model's\nrepresentational power, a Spatial Pyramid Pooling (SPP) module is incorporated\nto extract multi-scale contextual features, enabling better handling of diverse\nlighting conditions. Additionally, the Swish activation function is employed to\nensure smoother gradient propagation during training. EDNIG is optimized within\na Generative Adversarial Network (GAN) framework using a composite loss\nfunction that combines adversarial loss, pixel-wise mean squared error (MSE),\nand perceptual loss. Experimental results show that EDNIG achieves competitive\nperformance compared to state-of-the-art methods in quantitative metrics and\nvisual quality, while maintaining lower model complexity, demonstrating its\nsuitability for real-world applications. The source code for this work is\navailable at https://github.com/tranleanh/ednig.", "AI": {"tldr": "EDNIG\u662f\u4e00\u4e2a\u65b0\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u3002\u5b83\u4f7f\u7528\u5149\u7167\u5f15\u5bfc\u548c\u591a\u5c3a\u5ea6\u7279\u5f81\u6765\u63d0\u9ad8\u56fe\u50cf\u8d28\u91cf\uff0c\u5e76\u5728\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u6bd4\u5177\u6709\u7ade\u4e89\u529b\u7684\u540c\u65f6\uff0c\u6a21\u578b\u590d\u6742\u5ea6\u8f83\u4f4e\u3002", "motivation": "\u4e3a\u4e86\u5728\u4f4e\u5149\u7167\u6761\u4ef6\u4e0b\u63d0\u9ad8\u56fe\u50cf\u7684\u589e\u5f3a\u6548\u679c\uff0c\u672c\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5149\u7167\u5f15\u5bfc\u6765\u5e2e\u52a9\u7f51\u7edc\u5173\u6ce8\u6b20\u66dd\u5149\u533a\u57df\uff0c\u5e76\u5229\u7528\u591a\u5c3a\u5ea6\u4e0a\u4e0b\u6587\u7279\u5f81\u6765\u5904\u7406\u5404\u79cd\u5149\u7167\u6761\u4ef6\u3002", "method": "EDNIG\u662f\u4e00\u4e2a\u57fa\u4e8eU-Net\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u5b83\u6574\u5408\u4e86\u4ece\u4eae\u901a\u9053\u5148\u9a8c\uff08BCP\uff09\u5bfc\u51fa\u7684\u5149\u7167\u56fe\u4f5c\u4e3a\u5f15\u5bfc\u8f93\u5165\uff0c\u5e76\u7ed3\u5408\u4e86\u7a7a\u95f4\u91d1\u5b57\u5854\u6c60\u5316\uff08SPP\uff09\u6a21\u5757\u6765\u63d0\u53d6\u591a\u5c3a\u5ea6\u4e0a\u4e0b\u6587\u7279\u5f81\u3002\u6a21\u578b\u5728\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GAN\uff09\u6846\u67b6\u5185\u8fdb\u884c\u4f18\u5316\uff0c\u5e76\u4f7f\u7528\u5305\u62ec\u5bf9\u6297\u6027\u635f\u5931\u3001\u50cf\u7d20\u5747\u65b9\u8bef\u5dee\uff08MSE\uff09\u548c\u611f\u77e5\u635f\u5931\u5728\u5185\u7684\u590d\u5408\u635f\u5931\u51fd\u6570\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cEDNIG\u5728\u5b9a\u91cf\u6307\u6807\u548c\u89c6\u89c9\u8d28\u91cf\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u4e14\u6a21\u578b\u590d\u6742\u5ea6\u8f83\u4f4e\u3002", "conclusion": "EDNIG\u5728\u5b9a\u91cf\u6307\u6807\u548c\u89c6\u89c9\u8d28\u91cf\u65b9\u9762\u53d6\u5f97\u4e86\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u5ab2\u7f8e\u7684\u4f7f\u7528\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8f83\u4f4e\u7684\u6a21\u578b\u590d\u6742\u5ea6\uff0c\u8bc1\u660e\u4e86\u5176\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2507.13541", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13541", "abs": "https://arxiv.org/abs/2507.13541", "authors": ["Shuyue Stella Li", "Melanie Sclar", "Hunter Lang", "Ansong Ni", "Jacqueline He", "Puxin Xu", "Andrew Cohen", "Chan Young Park", "Yulia Tsvetkov", "Asli Celikyilmaz"], "title": "PrefPalette: Personalized Preference Modeling with Latent Attributes", "comment": "17 pages, 6 tables, 5 figures", "summary": "Personalizing AI systems requires understanding not just what users prefer,\nbut the reasons that underlie those preferences - yet current preference models\ntypically treat human judgment as a black box. We introduce PrefPalette, a\nframework that decomposes preferences into attribute dimensions and tailors its\npreference prediction to distinct social community values in a\nhuman-interpretable manner. PrefPalette operationalizes a cognitive science\nprinciple known as multi-attribute decision making in two ways: (1) a scalable\ncounterfactual attribute synthesis step that involves generating synthetic\ntraining data to isolate for individual attribute effects (e.g., formality,\nhumor, cultural values), and (2) attention-based preference modeling that\nlearns how different social communities dynamically weight these attributes.\nThis approach moves beyond aggregate preference modeling to capture the diverse\nevaluation frameworks that drive human judgment. When evaluated on 45 social\ncommunities from the online platform Reddit, PrefPalette outperforms GPT-4o by\n46.6% in average prediction accuracy. Beyond raw predictive improvements,\nPrefPalette also shed light on intuitive, community-specific profiles:\nscholarly communities prioritize verbosity and stimulation, conflict-oriented\ncommunities value sarcasm and directness, and support-based communities\nemphasize empathy. By modeling the attribute-mediated structure of human\njudgment, PrefPalette delivers both superior preference modeling and\ntransparent, interpretable insights, and serves as a first step toward more\ntrustworthy, value-aware personalized applications.", "AI": {"tldr": "PrefPalette\u901a\u8fc7\u5c06\u504f\u597d\u5206\u89e3\u4e3a\u5c5e\u6027\u7ef4\u5ea6\u5e76\u8003\u8651\u793e\u533a\u4ef7\u503c\u89c2\uff0c\u5728Reddit\u793e\u533a\u4e2d\u5b9e\u73b0\u4e86\u6bd4GPT-4o\u9ad846.6%\u7684\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u89c1\u89e3\u3002", "motivation": "\u5f53\u524d\u504f\u597d\u6a21\u578b\u5c06\u4eba\u7c7b\u5224\u65ad\u89c6\u4e3a\u9ed1\u7bb1\uff0c\u65e0\u6cd5\u7406\u89e3\u7528\u6237\u504f\u597d\u7684\u6839\u672c\u539f\u56e0\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5206\u89e3\u504f\u597d\u5e76\u4ee5\u4eba\u7c7b\u53ef\u89e3\u91ca\u7684\u65b9\u5f0f\u8fdb\u884c\u5b9a\u5236\u7684\u6846\u67b6\u3002", "method": "PrefPalette\u6846\u67b6\u901a\u8fc7\u5206\u89e3\u504f\u597d\u4e3a\u5c5e\u6027\u7ef4\u5ea6\uff0c\u5e76\u5229\u7528\u53cd\u4e8b\u5b9e\u5c5e\u6027\u5408\u6210\u548c\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u504f\u597d\u6a21\u578b\u6765\u6355\u6349\u4e0d\u540c\u793e\u4f1a\u793e\u533a\u5bf9\u8fd9\u4e9b\u5c5e\u6027\u7684\u52a8\u6001\u6743\u91cd\uff0c\u4ece\u800c\u5b9e\u73b0\u4eba\u7c7b\u53ef\u89e3\u91ca\u7684\u504f\u597d\u9884\u6d4b\u3002", "result": "PrefPalette\u572845\u4e2aReddit\u793e\u533a\u4e2d\u7684\u5e73\u5747\u9884\u6d4b\u51c6\u786e\u6027\u6bd4GPT-4o\u9ad8\u51fa46.6%\uff0c\u5e76\u63ed\u793a\u4e86\u7279\u5b9a\u793e\u533a\u7684\u504f\u597d\u6a21\u5f0f\uff08\u4f8b\u5982\uff0c\u5b66\u672f\u793e\u533a\u91cd\u89c6\u8be6\u7ec6\u548c\u542f\u53d1\u6027\uff0c\u51b2\u7a81\u5bfc\u5411\u793e\u533a\u91cd\u89c6\u8bbd\u523a\u548c\u76f4\u63a5\uff0c\u652f\u6301\u6027\u793e\u533a\u91cd\u89c6\u5171\u60c5\uff09\u3002", "conclusion": "PrefPalette\u901a\u8fc7\u4e3a\u4e2a\u6027\u5316AI\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u3001\u7531\u5c5e\u6027\u9a71\u52a8\u7684\u504f\u597d\u9884\u6d4b\uff0c\u5f25\u5408\u4e86AI\u504f\u597d\u5efa\u6a21\u4e0e\u4eba\u7c7b\u5224\u65ad\u7684\u5dee\u8ddd\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u5e76\u4fc3\u8fdb\u4e86\u5bf9\u4e0d\u540c\u793e\u4f1a\u7fa4\u4f53\u8bc4\u4f30\u6846\u67b6\u7684\u7406\u89e3\u3002"}}
{"id": "2507.13388", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2507.13388", "abs": "https://arxiv.org/abs/2507.13388", "authors": ["Zhen-Qi Chen", "Yuan-Fu Yang"], "title": "DLSF: Dual-Layer Synergistic Fusion for High-Fidelity Image Syn-thesis", "comment": null, "summary": "With the rapid advancement of diffusion-based generative models, Stable\nDiffusion (SD) has emerged as a state-of-the-art framework for high-fidelity\nim-age synthesis. However, existing SD models suffer from suboptimal feature\naggregation, leading to in-complete semantic alignment and loss of fine-grained\ndetails, especially in highly textured and complex scenes. To address these\nlimitations, we propose a novel dual-latent integration framework that\nen-hances feature interactions between the base latent and refined latent\nrepresentations. Our approach em-ploys a feature concatenation strategy\nfollowed by an adaptive fusion module, which can be instantiated as either (i)\nan Adaptive Global Fusion (AGF) for hier-archical feature harmonization, or\n(ii) a Dynamic Spatial Fusion (DSF) for spatially-aware refinement. This design\nenables more effective cross-latent com-munication, preserving both global\ncoherence and local texture fidelity. Our GitHub page:\nhttps://anonymous.4open.science/r/MVA2025-22 .", "AI": {"tldr": "\u901a\u8fc7\u521b\u65b0\u7684\u53cc\u6f5c\u5728\u96c6\u6210\u6846\u67b6\u548c\u81ea\u9002\u5e94\u878d\u5408\u6a21\u5757\uff08AGF\u6216DSF\uff09\uff0c\u6539\u8fdb\u4e86\u7a33\u5b9a\u6269\u6563\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u573a\u666f\u65f6\u7684\u7279\u5f81\u805a\u5408\u80fd\u529b\uff0c\u4ece\u800c\u63d0\u5347\u4e86\u56fe\u50cf\u5408\u6210\u7684\u5168\u5c40\u4e00\u81f4\u6027\u548c\u5c40\u90e8\u7ec6\u8282\u4fdd\u771f\u5ea6\u3002", "motivation": "\u73b0\u6709\u7684\u7a33\u5b9a\u6269\u6563\u6a21\u578b\u5728\u7279\u5f81\u805a\u5408\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u5728\u7eb9\u7406\u4e30\u5bcc\u548c\u590d\u6742\u7684\u573a\u666f\u4e2d\uff0c\u4f1a\u5bfc\u81f4\u8bed\u4e49\u5bf9\u9f50\u4e0d\u5b8c\u5168\u548c\u7ec6\u7c92\u5ea6\u7ec6\u8282\u4e22\u5931\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u589e\u5f3a\u7279\u5f81\u4ea4\u4e92\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u6f5c\u5728\u96c6\u6210\u6846\u67b6\uff0c\u5176\u4e2d\u5305\u542b\u7279\u5f81\u8fde\u63a5\u7b56\u7565\u548c\u4e00\u4e2a\u81ea\u9002\u5e94\u878d\u5408\u6a21\u5757\uff0c\u8be5\u6a21\u5757\u53ef\u4ee5\u5b9e\u4f8b\u5316\u4e3a\uff08i\uff09\u81ea\u9002\u5e94\u5168\u5c40\u878d\u5408\uff08AGF\uff09\u4ee5\u8fdb\u884c\u5206\u5c42\u7279\u5f81\u534f\u8c03\uff0c\u6216\uff08ii\uff09\u52a8\u6001\u7a7a\u95f4\u878d\u5408\uff08DSF\uff09\u4ee5\u8fdb\u884c\u7a7a\u95f4\u611f\u77e5\u7cbe\u70bc\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u5b9e\u73b0\u66f4\u6709\u6548\u7684\u8de8\u6f5c\u5728\u901a\u4fe1\uff0c\u5728\u4fdd\u7559\u5168\u5c40\u4e00\u81f4\u6027\u7684\u540c\u65f6\uff0c\u8fd8\u80fd\u4fdd\u6301\u5c40\u90e8\u7eb9\u7406\u7ec6\u8282\u7684\u4fdd\u771f\u5ea6\uff0c\u4ece\u800c\u6539\u5584\u4e86\u9ad8\u4fdd\u771f\u56fe\u50cf\u5408\u6210\u7684\u8d28\u91cf\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u53cc\u6f5c\u5728\u96c6\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u589e\u5f3a\u57fa\u7840\u6f5c\u5728\u8868\u793a\u548c\u7cbe\u70bc\u6f5c\u5728\u8868\u793a\u4e4b\u95f4\u7684\u7279\u5f81\u4ea4\u4e92\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u7a33\u5b9a\u6269\u6563\u6a21\u578b\u5728\u7279\u5f81\u805a\u5408\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u4ece\u800c\u5b9e\u73b0\u66f4\u6709\u6548\u7684\u8de8\u6f5c\u5728\u901a\u4fe1\uff0c\u5e76\u540c\u65f6\u4fdd\u7559\u5168\u5c40\u4e00\u81f4\u6027\u548c\u5c40\u90e8\u7eb9\u7406\u4fdd\u771f\u5ea6\u3002"}}
{"id": "2507.13623", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.13623", "abs": "https://arxiv.org/abs/2507.13623", "authors": ["Rahul Gulia"], "title": "MD-OFDM: An Energy-Efficient and Low-PAPR MIMO-OFDM Variant for Resource-Constrained Applications", "comment": null, "summary": "Orthogonal Frequency Division Multiplexing (OFDM) combined with\nMultiple-Input Multiple-Output (MIMO) techniques forms the backbone of modern\nwireless communication systems. While offering high spectral efficiency and\nrobustness, conventional MIMO-OFDM, especially with complex equalizers like\nMinimum Mean Square Error (MMSE), suffers from high Peak-to-Average Power Ratio\n(PAPR) and significant power consumption due to multiple active Radio Frequency\n(RF) chains. This paper proposes and mathematically models an alternative\nsystem, termed Multi-Dimensional OFDM (MD-OFDM), which employs a per-subcarrier\ntransmit antenna selection strategy. By activating only one transmit antenna\nfor each subcarrier, MD-OFDM aims to reduce PAPR, lower power consumption, and\nimprove Bit Error Rate (BER) performance. We provide detailed mathematical\nformulations for BER, Energy Efficiency (EE), and PAPR, and discuss the\nsuitability of MD-OFDM for various applications, particularly in\nenergy-constrained and cost-sensitive scenarios such as the Internet of Things\n(IoT) and Low-Power Wide Area Networks (LPWAN). Simulation results demonstrate\nthat MD-OFDM achieves superior BER and significantly lower PAPR compared to\nMMSE MIMO, albeit with a trade-off in peak overall energy efficiency due to\nreduced spectral multiplexing.", "AI": {"tldr": "MD-OFDM \u901a\u8fc7\u9009\u62e9\u6027\u6fc0\u6d3b\u5929\u7ebf\u6765\u964d\u4f4e MIMO-OFDM \u7684 PAPR \u548c\u529f\u8017\uff0c\u9002\u7528\u4e8e\u7269\u8054\u7f51\u7b49\u573a\u666f\uff0c\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u4f46\u80fd\u6548\u7565\u6709\u6298\u8877\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf MIMO-OFDM \u7cfb\u7edf\uff08\u5c24\u5176\u662f\u4f7f\u7528 MMSE \u7b49\u590d\u6742\u5747\u8861\u5668\u65f6\uff09\u9762\u4e34\u7684\u9ad8 PAPR \u548c\u663e\u8457\u7684\u529f\u8017\u95ee\u9898\uff08\u7531\u4e8e\u9700\u8981\u591a\u4e2a\u6709\u6e90\u5c04\u9891\u94fe\u8def\uff09\u3002", "method": "\u63d0\u51fa\u5e76\u5bf9 MD-OFDM \u7cfb\u7edf\u8fdb\u884c\u6570\u5b66\u5efa\u6a21\uff0c\u8be5\u7cfb\u7edf\u91c7\u7528\u6bcf\u5b50\u8f7d\u6ce2\u53d1\u5c04\u5929\u7ebf\u9009\u62e9\u7b56\u7565\u3002", "result": "MD-OFDM \u5728 BER \u548c PAPR \u65b9\u9762\u4f18\u4e8e MMSE MIMO\uff0c\u4f46\u7531\u4e8e\u9891\u8c31\u590d\u7528\u6027\u964d\u4f4e\uff0c\u5728\u5cf0\u503c\u6574\u4f53\u80fd\u6548\u65b9\u9762\u6709\u6240\u6743\u8861\u3002", "conclusion": "MD-OFDM \u901a\u8fc7\u5728\u6bcf\u4e2a\u5b50\u8f7d\u6ce2\u4e0a\u4ec5\u6fc0\u6d3b\u4e00\u4e2a\u53d1\u5c04\u5929\u7ebf\uff0c\u6709\u671b\u964d\u4f4e PAPR\u3001\u51cf\u5c11\u529f\u8017\u5e76\u63d0\u9ad8 BER \u6027\u80fd\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u7269\u8054\u7f51\u548c LPWAN \u7b49\u80fd\u6e90\u53d7\u9650\u548c\u6210\u672c\u654f\u611f\u7684\u573a\u666f\u3002"}}
{"id": "2507.13468", "categories": ["cs.RO", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.13468", "abs": "https://arxiv.org/abs/2507.13468", "authors": ["Shiye Cao", "Maia Stiber", "Amama Mahmood", "Maria Teresa Parreira", "Wendy Ju", "Micol Spitale", "Hatice Gunes", "Chien-Ming Huang"], "title": "ERR@HRI 2.0 Challenge: Multimodal Detection of Errors and Failures in Human-Robot Conversations", "comment": null, "summary": "The integration of large language models (LLMs) into conversational robots\nhas made human-robot conversations more dynamic. Yet, LLM-powered\nconversational robots remain prone to errors, e.g., misunderstanding user\nintent, prematurely interrupting users, or failing to respond altogether.\nDetecting and addressing these failures is critical for preventing\nconversational breakdowns, avoiding task disruptions, and sustaining user\ntrust. To tackle this problem, the ERR@HRI 2.0 Challenge provides a multimodal\ndataset of LLM-powered conversational robot failures during human-robot\nconversations and encourages researchers to benchmark machine learning models\ndesigned to detect robot failures. The dataset includes 16 hours of dyadic\nhuman-robot interactions, incorporating facial, speech, and head movement\nfeatures. Each interaction is annotated with the presence or absence of robot\nerrors from the system perspective, and perceived user intention to correct for\na mismatch between robot behavior and user expectation. Participants are\ninvited to form teams and develop machine learning models that detect these\nfailures using multimodal data. Submissions will be evaluated using various\nperformance metrics, including detection accuracy and false positive rate. This\nchallenge represents another key step toward improving failure detection in\nhuman-robot interaction through social signal analysis.", "AI": {"tldr": "ERR@HRI 2.0 \u6311\u6218\u8d5b\u65e8\u5728\u901a\u8fc7\u591a\u6a21\u6001\u6570\u636e\u548c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u6765\u68c0\u6d4b\u548c\u89e3\u51b3 LLM \u9a71\u52a8\u7684\u5bf9\u8bdd\u673a\u5668\u4eba\u5728\u4eba\u673a\u4ea4\u4e92\u4e2d\u7684\u9519\u8bef\uff0c\u4ee5\u63d0\u9ad8\u5bf9\u8bdd\u7684\u53ef\u9760\u6027\u548c\u7528\u6237\u4fe1\u4efb\u5ea6\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3 LLM \u9a71\u52a8\u7684\u5bf9\u8bdd\u673a\u5668\u4eba\u5bb9\u6613\u51fa\u9519\u7684\u95ee\u9898\uff0c\u4f8b\u5982\u8bef\u89e3\u7528\u6237\u610f\u56fe\u3001\u8fc7\u65e9\u4e2d\u65ad\u7528\u6237\u6216\u6839\u672c\u4e0d\u54cd\u5e94\uff0c\u5e76\u9632\u6b62\u5bf9\u8bdd\u4e2d\u65ad\u3001\u907f\u514d\u4efb\u52a1\u4e2d\u65ad\u548c\u7ef4\u6301\u7528\u6237\u4fe1\u4efb\u3002", "method": "\u8be5\u65b9\u6cd5\u5305\u62ec\u4f7f\u7528\u5305\u542b\u9762\u90e8\u3001\u8bed\u97f3\u548c\u5934\u90e8\u8fd0\u52a8\u7279\u5f81\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u5bf9 LLM \u9a71\u52a8\u7684\u5bf9\u8bdd\u673a\u5668\u4eba\u6545\u969c\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u548c\u68c0\u6d4b\uff0c\u5e76\u9f13\u52b1\u7814\u7a76\u4eba\u5458\u5f00\u53d1\u673a\u5668\u5b66\u4e60\u6a21\u578b\u6765\u68c0\u6d4b\u8fd9\u4e9b\u6545\u969c\u3002", "result": "ERR@HRI 2.0 \u6311\u6218\u8d5b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5305\u542b 16 \u5c0f\u65f6\u53cc\u5411\u4eba\u673a\u4ea4\u4e92\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5305\u542b\u7cfb\u7edf\u89c6\u89d2\u7684\u673a\u5668\u4eba\u9519\u8bef\u6ce8\u91ca\u4ee5\u53ca\u7528\u6237\u4e3a\u89e3\u51b3\u673a\u5668\u4eba\u884c\u4e3a\u4e0e\u7528\u6237\u671f\u671b\u4e0d\u5339\u914d\u800c\u4ea7\u751f\u7684\u7ea0\u6b63\u610f\u56fe\u3002", "conclusion": "\u8be5\u6311\u6218\u8d5b\u4ee3\u8868\u4e86\u901a\u8fc7\u793e\u4f1a\u4fe1\u53f7\u5206\u6790\u6539\u8fdb\u4eba\u673a\u4ea4\u4e92\u4e2d\u6545\u969c\u68c0\u6d4b\u7684\u53c8\u4e00\u5173\u952e\u6b65\u9aa4\u3002"}}
{"id": "2507.13895", "categories": ["cs.LO", "cs.DC"], "pdf": "https://arxiv.org/pdf/2507.13895", "abs": "https://arxiv.org/abs/2507.13895", "authors": ["Damiano Azzolini", "Marco Duca", "Stefano Forti", "Francesco Gallo", "Antonio Ielo"], "title": "Application Placement with Constraint Relaxation", "comment": null, "summary": "Novel utility computing paradigms rely upon the deployment of multi-service\napplications to pervasive and highly distributed cloud-edge infrastructure\nresources. Deciding onto which computational nodes to place services in\ncloud-edge networks, as per their functional and non-functional constraints,\ncan be formulated as a combinatorial optimisation problem. Most existing\nsolutions in this space are not able to deal with \\emph{unsatisfiable} problem\ninstances, nor preferences, i.e. requirements that DevOps may agree to relax to\nobtain a solution. In this article, we exploit Answer Set Programming\noptimisation capabilities to tackle this problem. Experimental results in\nsimulated settings show that our approach is effective on lifelike networks and\napplications.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u5229\u7528\u7b54\u6848\u96c6\u7f16\u7a0b\u6765\u89e3\u51b3\u4e91\u8fb9\u7f51\u7edc\u4e2d\u591a\u670d\u52a1\u5e94\u7528\u7a0b\u5e8f\u7684\u670d\u52a1\u90e8\u7f72\u95ee\u9898\uff0c\u7279\u522b\u5904\u7406\u4e86\u4e0d\u53ef\u6ee1\u8db3\u7684\u9700\u6c42\u548c\u504f\u597d\u3002", "motivation": "\u5927\u591a\u6570\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u65e0\u6cd5\u5904\u7406\u4e0d\u53ef\u6ee1\u8db3\u7684\u95ee\u9898\u5b9e\u4f8b\u4ee5\u53caDevOps\u53ef\u80fd\u540c\u610f\u653e\u5bbd\u4ee5\u83b7\u5f97\u89e3\u51b3\u65b9\u6848\u7684\u504f\u597d\uff08\u5373\u9700\u6c42\uff09\u3002", "method": "\u5229\u7528\u7b54\u6848\u96c6\u7f16\u7a0b\u4f18\u5316\u80fd\u529b\u6765\u89e3\u51b3\u5c06\u670d\u52a1\u90e8\u7f72\u5230\u4e91\u8fb9\u7f51\u7edc\u4e2d\u7684\u8ba1\u7b97\u8282\u70b9\u7684\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6a21\u62df\u73af\u5883\u4e2d\u5bf9\u771f\u5b9e\u7f51\u7edc\u548c\u5e94\u7528\u7a0b\u5e8f\u6709\u6548\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6a21\u62df\u73af\u5883\u4e2d\u5bf9\u771f\u5b9e\u7f51\u7edc\u548c\u5e94\u7528\u7a0b\u5e8f\u6709\u6548\u3002"}}
{"id": "2507.13369", "categories": ["cs.AR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13369", "abs": "https://arxiv.org/abs/2507.13369", "authors": ["Paul E. Calzada", "Zahin Ibnat", "Tanvir Rahman", "Kamal Kandula", "Danyu Lu", "Sujan Kumar Saha", "Farimah Farahmandi", "Mark Tehranipoor"], "title": "VerilogDB: The Largest, Highest-Quality Dataset with a Preprocessing Framework for LLM-based RTL Generation", "comment": null, "summary": "Large Language Models (LLMs) are gaining popularity for hardware design\nautomation, particularly through Register Transfer Level (RTL) code generation.\nIn this work, we examine the current literature on RTL generation using LLMs\nand identify key requirements for training and fine-tuning datasets. We\nconstruct a robust Verilog dataset through an automated three-pronged process\ninvolving database (DB) creation and management with PostgreSQL, data\ncollection from code hosting sites like OpenCores and GitHub, and data\npreprocessing to verify the codes' syntax, run logic synthesis, and extract\nrelevant module metadata. We implement a scalable and efficient DB\ninfrastructure to support analysis and detail our preprocessing pipeline to\nenforce high-quality data before DB insertion. The resulting dataset comprises\n20,392 Verilog samples, 751 MB of Verilog code data, which is the largest\nhigh-quality Verilog dataset for LLM fine-tuning to our knowledge. We further\nevaluate the dataset, address associated challenges, and explore potential\napplications for future research and development in LLM-based hardware\ngeneration.", "AI": {"tldr": "\u672c\u6587\u4f5c\u8005\u6784\u5efa\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u3001\u9ad8\u8d28\u91cf\u7684 Verilog \u6570\u636e\u96c6\uff0c\u7528\u4e8e\u6539\u8fdb\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u786c\u4ef6\u8bbe\u8ba1\u81ea\u52a8\u5316\u4e2d\u7684\u5e94\u7528\uff0c\u7279\u522b\u662f RTL \u4ee3\u7801\u751f\u6210\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u786c\u4ef6\u8bbe\u8ba1\u81ea\u52a8\u5316\uff08\u7279\u522b\u662f RTL \u4ee3\u7801\u751f\u6210\uff09\u4e2d\u65e5\u76ca\u666e\u53ca\uff0c\u9700\u8981\u9ad8\u8d28\u91cf\u7684\u6570\u636e\u96c6\u6765\u8bad\u7ec3\u548c\u5fae\u8c03 LLM\u3002", "method": "\u901a\u8fc7\u6570\u636e\u5e93\uff08DB\uff09\u521b\u5efa\u4e0e\u7ba1\u7406\uff08PostgreSQL\uff09\u3001\u4ee3\u7801\u6258\u7ba1\u7f51\u7ad9\uff08OpenCores \u548c GitHub\uff09\u6570\u636e\u6536\u96c6\uff0c\u4ee5\u53ca\u4ee3\u7801\u8bed\u6cd5\u9a8c\u8bc1\u3001\u903b\u8f91\u7efc\u5408\u548c\u5143\u6570\u636e\u63d0\u53d6\u7b49\u9884\u5904\u7406\u6d41\u7a0b\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b 20,392 \u4e2a Verilog \u6837\u672c\u3001751 MB Verilog \u4ee3\u7801\u6570\u636e\u7684\u7a33\u5065\u6570\u636e\u96c6\u3002", "result": "\u6784\u5efa\u4e86\u5f53\u65f6\u6700\u5927\u7684\u3001\u7528\u4e8e LLM \u5fae\u8c03\u7684\u9ad8\u8d28\u91cf Verilog \u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5305\u542b 20,392 \u4e2a Verilog \u6837\u672c\u548c 751 MB \u7684\u4ee3\u7801\u6570\u636e\u3002", "conclusion": "\u672c\u6587\u6784\u5efa\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u3001\u9ad8\u8d28\u91cf\u7684 Verilog \u6570\u636e\u96c6\uff0c\u7528\u4e8e LLM \u5728\u786c\u4ef6\u8bbe\u8ba1\u81ea\u52a8\u5316\u9886\u57df\u7684\u5fae\u8c03\uff0c\u4e3a\u8be5\u9886\u57df\u7684\u7814\u7a76\u548c\u53d1\u5c55\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.13709", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2507.13709", "abs": "https://arxiv.org/abs/2507.13709", "authors": ["Haiyan Zhua", "Hongyu Yua", "Weiqin Zhua", "Guoliang Yua", "Changsong Xu", "Hongjun Xiang"], "title": "Moir\u00e9-Induced Magnetoelectricity in Twisted Bilayer NiI2", "comment": null, "summary": "Twisted magnetic van der Waals (vdW) materials offer a promising route for\nmultiferroic engineering, yet modeling large-scale moir\\'e superlattices\nremains challenging. Leveraging a newly developed SpinGNN++ framework that\neffectively handles spin-lattice coupled systems, we develop a comprehensive\ninteratomic machine learning (ML) potential and apply it to twisted bilayer\nNiI2 (TBN). Structural relaxation introduces moir\\'e-periodic \"bumps\" that\nmodulate the interlayer spacing by about 0.55~\\AA{} and in-plane ionic shifts\nup to 0.48~\\AA{}. Concurrently, our ML potential, which faithfully captures all\nkey spin interactions, produces reliable magnetic configurations; combined with\nthe generalized KNB mechanism, it yields accurate spin-driven polarization. For\ntwist angles 1.89^{\\circ} \\leq \\theta \\leq 2.45^{\\circ}, both mechanisms become\nprominent, yielding rich polarization textures that combine ionic out-of-plane\ndipoles with purely electronic in-plane domains. In the rigid (unrelaxed)\nbilayer, skyrmions are absent; lattice relaxation is essential for generating\npolar-magnetic topologies. In contrast, near {\\theta} \\approx 60^{\\circ},\nstacking-dependent ferroelectric displacements dominate, giving rise to polar\nmeron-antimeron networks. These results reveal cooperative ionic and\nspin-driven ferroelectricity in TBN, positioning twisted vdW magnets as\nadaptable platforms for tunable multiferroic devices.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.13699", "categories": ["cond-mat.mes-hall", "quant-ph"], "pdf": "https://arxiv.org/pdf/2507.13699", "abs": "https://arxiv.org/abs/2507.13699", "authors": ["Paul Greule", "Wantong Huang", "M\u00e1t\u00e9 Stark", "Kwan Ho Au-Yeung", "Johannes Schwenk", "Jose Reina-G\u00e1lvez", "Christoph S\u00fcrgers", "Wolfgang Wernsdorfer", "Christoph Wolf", "Philip Willke"], "title": "Spin-Electric Control of Individual Molecules on Surfaces", "comment": "18 pages, 4 figures", "summary": "Individual magnetic molecules are promising building blocks for quantum\ntechnologies because of their chemical tunability, nanoscale dimensions, and\nability to self-assemble into ordered arrays. However, harnessing their\nproperties in quantum information processing requires precise local control of\ntheir spin properties. In this work, we present spin-electric coupling (SEC)\nfor two molecular spin systems, iron phthalocyanine (FePc) and Fe-FePc\ncomplexes, adsorbed on a surface. We use electron spin resonance combined with\nscanning tunnelling microscopy (ESR-STM) to locally address them with the STM\ntip and electrically tune them using the applied bias voltage. These\nmeasurements reveal a pronounced nonlinear voltage dependence of the resonance\nfrequency, linked to the energic onset of other molecular orbitals. We\nattribute this effect to a transport-mediated exchange field from the magnetic\ntip, providing a large, highly localized, and broadly applicable SEC mechanism.\nFinally, we demonstrate that the SEC enables all-electrical coherent spin\ncontrol: In Rabi oscillation measurements of both single and coupled Fe-FePc\ncomplexes we show that the spin dynamics can be tuned, demonstrating a pathway\ntowards electrically controlled quantum operation.", "AI": {"tldr": "\u672c\u7814\u7a76\u5c55\u793a\u4e86\u5982\u4f55\u901a\u8fc7\u7535\u573a\u7cbe\u786e\u63a7\u5236\u5355\u4e2a\u78c1\u6027\u5206\u5b50\u7684\u81ea\u65cb\u72b6\u6001\uff0c\u4e3a\u6784\u5efa\u57fa\u4e8e\u5206\u5b50\u7684\u91cf\u5b50\u5668\u4ef6\u94fa\u5e73\u4e86\u9053\u8def\u3002", "motivation": "\u4e3a\u4e86\u5728\u91cf\u5b50\u4fe1\u606f\u5904\u7406\u4e2d\u5229\u7528\u5355\u4e2a\u78c1\u6027\u5206\u5b50\u7684\u7279\u6027\uff0c\u9700\u8981\u7cbe\u786e\u5730\u63a7\u5236\u5176\u81ea\u65cb\u6027\u8d28\u3002\u5355\u4e2a\u78c1\u6027\u5206\u5b50\u56e0\u5176\u5316\u5b66\u53ef\u8c03\u6027\u3001\u7eb3\u7c73\u5c3a\u5ea6\u548c\u81ea\u7ec4\u88c5\u6210\u6709\u5e8f\u9635\u5217\u7684\u80fd\u529b\uff0c\u5728\u91cf\u5b50\u6280\u672f\u4e2d\u5177\u6709\u5e94\u7528\u6f5c\u529b\u3002", "method": "\u7814\u7a76\u4eba\u5458\u4f7f\u7528\u7535\u5b50\u81ea\u65cb\u5171\u632f\u7ed3\u5408\u626b\u63cf\u96a7\u9053\u663e\u5fae\u955c\uff08ESR-STM\uff09\u6280\u672f\uff0c\u5728\u8868\u9762\u5438\u9644\u7684\u5206\u5b50\u81ea\u65cb\u7cfb\u7edf\uff08\u5982\u915e\u83c1\u94c1FePc\u53ca\u5176\u590d\u5408\u7269\uff09\u4e0a\uff0c\u901a\u8fc7STM\u9488\u5c16\u8fdb\u884c\u5c40\u90e8\u5bfb\u5740\uff0c\u5e76\u5229\u7528\u504f\u538b\u8fdb\u884c\u7535\u8c03\u8c10\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4e86\u5206\u5b50\u5171\u632f\u9891\u7387\u4e0e\u7535\u538b\u4e4b\u95f4\u663e\u8457\u7684\u975e\u7ebf\u6027\u5173\u7cfb\uff0c\u8fd9\u5f52\u56e0\u4e8e\u5176\u4ed6\u5206\u5b50\u8f68\u9053\u7684\u80fd\u91cf\u5f00\u542f\u3002\u4ed6\u4eec\u8fd8\u8bc1\u660e\u4e86SEC\u673a\u5236\u80fd\u591f\u5b9e\u73b0\u5168\u7535\u76f8\u5e72\u81ea\u65cb\u63a7\u5236\uff0c\u901a\u8fc7Rabi\u632f\u8361\u6d4b\u91cf\u8c03\u6574\u4e86\u5355\u5206\u5b50\u548c\u8026\u5408Fe-FePc\u590d\u5408\u7269\u7684\u81ea\u65cb\u52a8\u529b\u5b66\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u81ea\u65cb-\u7535\u8026\u5408\uff08SEC\uff09\u5728\u5355\u5206\u5b50\u78c1\u4f53\u4e2d\u5b9e\u73b0\u5168\u7535\u76f8\u5e72\u81ea\u65cb\u63a7\u5236\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u91cf\u5b50\u4fe1\u606f\u5904\u7406\u63d0\u4f9b\u4e86\u65b0\u7684\u9014\u5f84\u3002"}}
{"id": "2507.14069", "categories": ["cs.DC", "cs.AI", "cs.ET", "cs.NE"], "pdf": "https://arxiv.org/pdf/2507.14069", "abs": "https://arxiv.org/abs/2507.14069", "authors": ["Shuiguang Deng", "Di Yu", "Changze Lv", "Xin Du", "Linshan Jiang", "Xiaofan Zhao", "Wentao Tong", "Xiaoqing Zheng", "Weijia Fang", "Peng Zhao", "Gang Pan", "Schahram Dustdar", "Albert Y. Zomaya"], "title": "Edge Intelligence with Spiking Neural Networks", "comment": "This work has been submitted to Proceeding of IEEE for possible\n  publication", "summary": "The convergence of artificial intelligence and edge computing has spurred\ngrowing interest in enabling intelligent services directly on\nresource-constrained devices. While traditional deep learning models require\nsignificant computational resources and centralized data management, the\nresulting latency, bandwidth consumption, and privacy concerns have exposed\ncritical limitations in cloud-centric paradigms. Brain-inspired computing,\nparticularly Spiking Neural Networks (SNNs), offers a promising alternative by\nemulating biological neuronal dynamics to achieve low-power, event-driven\ncomputation. This survey provides a comprehensive overview of Edge Intelligence\nbased on SNNs (EdgeSNNs), examining their potential to address the challenges\nof on-device learning, inference, and security in edge scenarios. We present a\nsystematic taxonomy of EdgeSNN foundations, encompassing neuron models,\nlearning algorithms, and supporting hardware platforms. Three representative\npractical considerations of EdgeSNN are discussed in depth: on-device inference\nusing lightweight SNN models, resource-aware training and updating under\nnon-stationary data conditions, and secure and privacy-preserving issues.\nFurthermore, we highlight the limitations of evaluating EdgeSNNs on\nconventional hardware and introduce a dual-track benchmarking strategy to\nsupport fair comparisons and hardware-aware optimization. Through this study,\nwe aim to bridge the gap between brain-inspired learning and practical edge\ndeployment, offering insights into current advancements, open challenges, and\nfuture research directions. To the best of our knowledge, this is the first\ndedicated and comprehensive survey on EdgeSNNs, providing an essential\nreference for researchers and practitioners working at the intersection of\nneuromorphic computing and edge intelligence.", "AI": {"tldr": "\u672c\u7efc\u8ff0\u5168\u9762\u6982\u8ff0\u4e86\u57fa\u4e8e\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08SNNs\uff09\u7684\u8fb9\u7f18\u667a\u80fd\uff08EdgeSNNs\uff09\uff0c\u63a2\u8ba8\u4e86\u5176\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u5e94\u7528\u6f5c\u529b\u3001\u9762\u4e34\u7684\u6311\u6218\u4ee5\u53ca\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\u7b56\u7565\u3002", "motivation": "\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u5e94\u7528\u5b58\u5728\u5ef6\u8fdf\u3001\u5e26\u5bbd\u6d88\u8017\u548c\u9690\u79c1\u95ee\u9898\u3002\u8111\u542f\u53d1\u8ba1\u7b97\uff08\u5c24\u5176\u662f\u8109\u51b2\u795e\u7ecf\u7f51\u7edcSNNs\uff09\u901a\u8fc7\u6a21\u62df\u751f\u7269\u795e\u7ecf\u5143\u52a8\u529b\u5b66\u6765\u5b9e\u73b0\u4f4e\u529f\u8017\u3001\u4e8b\u4ef6\u9a71\u52a8\u7684\u8ba1\u7b97\uff0c\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u672c\u7bc7\u8bba\u6587\u5bf9\u57fa\u4e8eSNN\u7684\u8fb9\u7f18\u667a\u80fd\uff08EdgeSNNs\uff09\u8fdb\u884c\u4e86\u5168\u9762\u7684\u6982\u8ff0\uff0c\u8003\u5bdf\u4e86\u5b83\u4eec\u5728\u89e3\u51b3\u8fb9\u7f18\u573a\u666f\u4e2d\u7684\u8bbe\u5907\u4e0a\u5b66\u4e60\u3001\u63a8\u7406\u548c\u5b89\u5168\u6027\u65b9\u9762\u7684\u6f5c\u529b\u3002\u8bba\u6587\u7cfb\u7edf\u5730\u5206\u7c7b\u4e86EdgeSNNs\u7684\u57fa\u7840\uff0c\u5305\u62ec\u795e\u7ecf\u5143\u6a21\u578b\u3001\u5b66\u4e60\u7b97\u6cd5\u548c\u652f\u6301\u7684\u786c\u4ef6\u5e73\u53f0\u3002\u6df1\u5165\u8ba8\u8bba\u4e86\u4e09\u79cd\u4ee3\u8868\u6027\u7684EdgeSNN\u5b9e\u9645\u8003\u91cf\uff1a\u4f7f\u7528\u8f7b\u91cf\u7ea7SNN\u6a21\u578b\u8fdb\u884c\u8bbe\u5907\u4e0a\u63a8\u7406\u3001\u5728\u975e\u5e73\u7a33\u6570\u636e\u6761\u4ef6\u4e0b\u8fdb\u884c\u8d44\u6e90\u611f\u77e5\u8bad\u7ec3\u548c\u66f4\u65b0\uff0c\u4ee5\u53ca\u5b89\u5168\u548c\u9690\u79c1\u4fdd\u62a4\u95ee\u9898\u3002\u6b64\u5916\uff0c\u672c\u6587\u8fd8\u5f3a\u8c03\u4e86\u5728\u4f20\u7edf\u786c\u4ef6\u4e0a\u8bc4\u4f30EdgeSNNs\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5f15\u5165\u4e86\u53cc\u8f68\u57fa\u51c6\u6d4b\u8bd5\u7b56\u7565\u4ee5\u652f\u6301\u516c\u5e73\u6bd4\u8f83\u548c\u786c\u4ef6\u611f\u77e5\u4f18\u5316\u3002", "result": "\u8bba\u6587\u63d0\u4f9b\u4e86EdgeSNNs\u7684\u7cfb\u7edf\u5206\u7c7b\uff0c\u6df1\u5165\u8ba8\u8bba\u4e86\u8bbe\u5907\u4e0a\u63a8\u7406\u3001\u8d44\u6e90\u611f\u77e5\u8bad\u7ec3\u548c\u66f4\u65b0\u4ee5\u53ca\u5b89\u5168\u9690\u79c1\u4fdd\u62a4\u7b49\u5b9e\u9645\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u8f68\u57fa\u51c6\u6d4b\u8bd5\u7b56\u7565\u4ee5\u652f\u6301\u516c\u5e73\u6bd4\u8f83\u548c\u786c\u4ef6\u611f\u77e5\u4f18\u5316\u3002", "conclusion": "\u672c\u7bc7\u8bba\u6587\u65e8\u5728\u5f25\u5408\u53d7\u8111\u542f\u53d1\u5b66\u4e60\u4e0e\u5b9e\u9645\u8fb9\u7f18\u90e8\u7f72\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5e76\u4e3a\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u548c\u8fb9\u7f18\u667a\u80fd\u4ea4\u53c9\u9886\u57df\u7684\u7814\u7a76\u4eba\u5458\u548c\u4ece\u4e1a\u8005\u63d0\u4f9b\u91cd\u8981\u7684\u53c2\u8003\u3002"}}
{"id": "2507.13671", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2507.13671", "abs": "https://arxiv.org/abs/2507.13671", "authors": ["Michael Itzhaki"], "title": "Combinatorics of Palindromes", "comment": "Full version, accepted to FCT25", "summary": "We investigate the structure and reconstruction complexity of Manacher\narrays. First, we establish a combinatorial lower bound, proving that the\nnumber of rooted tandem repeat trees with $n+1$ genes exceeds the number of\ndistinct Manacher arrays of length $n$. Second, we introduce a graph-theoretic\nframework that associates a graph to each Manacher array, where every proper\nvertex coloring yields a string consistent with the array. Finally, we analyze\na reconstruction algorithm by I et al. (SPIRE 2010), showing that it\nsimultaneously achieves a globally minimal alphabet size, uses at most\n$\\log_2(n{-}1) + 2$ distinct symbols, and can be adapted to produce\nreconstructions over arbitrary alphabets when possible. Our results also\nresolve an open problem posed by the original authors. Together, these findings\nadvance the combinatorial understanding of Manacher arrays and open new\ndirections for string reconstruction under structural constraints.", "AI": {"tldr": "Manacher \u6570\u7ec4\u7684\u7ed3\u6784\u548c\u91cd\u5efa\u590d\u6742\u5ea6\u88ab\u7814\u7a76\u3002\u8bc1\u660e\u4e86\u6839\u4e32\u8054\u91cd\u590d\u6811\u7684\u6570\u91cf\u8d85\u8fc7\u4e86 Manacher \u6570\u7ec4\u7684\u6570\u91cf\u3002\u5f15\u5165\u4e86\u4e00\u4e2a\u56fe\u8bba\u6846\u67b6\uff0c\u5e76\u5c06 Manacher \u6570\u7ec4\u4e0e\u56fe\u76f8\u5173\u8054\u3002\u5206\u6790\u4e86\u4e00\u79cd\u91cd\u5efa\u7b97\u6cd5\uff0c\u8bc1\u660e\u5176\u5177\u6709\u6700\u5c0f\u5b57\u6bcd\u8868\u5927\u5c0f\u548c\u5bf9\u6570\u6570\u91cf\u7684\u7b26\u53f7\uff0c\u5e76\u53ef\u9002\u5e94\u4efb\u610f\u5b57\u6bcd\u8868\u3002\u8fd9\u4e9b\u7ed3\u679c\u89e3\u51b3\u4e86\u8be5\u9886\u57df\u7684\u4e00\u4e2a\u516c\u5f00\u95ee\u9898\u3002", "motivation": "\u7814\u7a76 Manacher \u6570\u7ec4\u7684\u7ed3\u6784\u548c\u91cd\u5efa\u590d\u6742\u5ea6\u3002", "method": "1. \u7ec4\u5408\u4e0b\u754c\u8bc1\u660e\uff1a\u8bc1\u660e\u5177\u6709 n+1 \u4e2a\u57fa\u56e0\u7684\u6839\u4e32\u8054\u91cd\u590d\u6811\u7684\u6570\u91cf\u8d85\u8fc7\u4e86\u957f\u5ea6\u4e3a n \u7684\u4e0d\u540c Manacher \u6570\u7ec4\u7684\u6570\u91cf\u3002\n2. \u56fe\u8bba\u6846\u67b6\uff1a\u5c06\u56fe\u4e0e\u6bcf\u4e2a Manacher \u6570\u7ec4\u76f8\u5173\u8054\uff0c\u5176\u4e2d\u6bcf\u4e2a\u9876\u70b9\u7740\u8272\u90fd\u4f1a\u4ea7\u751f\u4e00\u4e2a\u4e0e\u6570\u7ec4\u4e00\u81f4\u7684\u5b57\u7b26\u4e32\u3002\n3. \u91cd\u5efa\u7b97\u6cd5\u5206\u6790\uff1a\u5206\u6790 I et al. (SPIRE 2010) \u7684\u91cd\u5efa\u7b97\u6cd5\uff0c\u8bc1\u660e\u5176\u5b9e\u73b0\u4e86\u5168\u5c40\u6700\u5c0f\u5b57\u6bcd\u8868\u5927\u5c0f\uff0c\u4f7f\u7528\u4e86\u4e0d\u8d85\u8fc7 log_2(n-1) + 2 \u4e2a\u4e0d\u540c\u7684\u7b26\u53f7\uff0c\u5e76\u53ef\u9002\u5e94\u5728\u53ef\u80fd\u7684\u60c5\u51b5\u4e0b\u5728\u4efb\u610f\u5b57\u6bcd\u8868\u4e0a\u8fdb\u884c\u91cd\u5efa\u3002", "result": "1. \u7ec4\u5408\u4e0b\u754c\uff1a\u5177\u6709 n+1 \u4e2a\u57fa\u56e0\u7684\u6839\u4e32\u8054\u91cd\u590d\u6811\u7684\u6570\u91cf\u5927\u4e8e\u957f\u5ea6\u4e3a n \u7684 Manacher \u6570\u7ec4\u7684\u6570\u91cf\u3002\n2. \u56fe\u8bba\u6846\u67b6\uff1a\u4e3a Manacher \u6570\u7ec4\u521b\u5efa\u4e86\u4e00\u4e2a\u5173\u8054\u56fe\uff0c\u5176\u9876\u70b9\u7740\u8272\u53ef\u4ea7\u751f\u4e00\u81f4\u7684\u5b57\u7b26\u4e32\u3002\n3. \u91cd\u5efa\u7b97\u6cd5\uff1aI et al. (SPIRE 2010) \u7684\u7b97\u6cd5\u5b9e\u73b0\u4e86\u6700\u5c0f\u5b57\u6bcd\u8868\u5927\u5c0f\uff0c\u4f7f\u7528\u4e86\u5bf9\u6570\u6570\u91cf\u7684\u7b26\u53f7\uff0c\u5e76\u4e14\u53ef\u4ee5\u9002\u5e94\u4efb\u610f\u5b57\u6bcd\u8868\u3002", "conclusion": "Manacher \u6570\u7ec4\u7684\u7ec4\u5408\u7406\u89e3\u548c\u7ed3\u6784\u7ea6\u675f\u4e0b\u7684\u5b57\u7b26\u4e32\u91cd\u5efa\u65b0\u65b9\u5411\u3002"}}
{"id": "2507.13381", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13381", "abs": "https://arxiv.org/abs/2507.13381", "authors": ["Rafiq Kamel", "Filippo Guerranti", "Simon Geisler", "Stephan G\u00fcnnemann"], "title": "SAFT: Structure-Aware Fine-Tuning of LLMs for AMR-to-Text Generation", "comment": "Accepted at the KDD2025 Workshop on Structured Knowledge for LLMs", "summary": "Large Language Models (LLMs) are increasingly applied to tasks involving\nstructured inputs such as graphs. Abstract Meaning Representations (AMRs),\nwhich encode rich semantics as directed graphs, offer a rigorous testbed for\nevaluating LLMs on text generation from such structures. Yet, current methods\noften arbitrarily linearize AMRs, discarding key structural cues, or rely on\narchitectures incompatible with standard LLMs. We introduce SAFT, a\nstructure-aware fine-tuning approach that injects graph topology into\npretrained LLMs without architectural changes. We compute direction-sensitive\npositional encodings from the magnetic Laplacian of transformed AMRs and\nproject them into the embedding space of the LLM. While possibly applicable to\nany graph-structured inputs, we focus on AMR-to-text generation as a\nrepresentative and challenging benchmark. SAFT sets a new state-of-the-art on\nAMR 3.0 with a 3.5 BLEU improvement over baselines. Gains scale with graph\ncomplexity, highlighting the value of structure-aware representations in\nenhancing LLM performance. SAFT offers a general and effective pathway for\nbridging structured data and language models.", "AI": {"tldr": "SAFT \u662f\u4e00\u79cd\u65b0\u7684\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u56fe\u7ed3\u6784\u6ce8\u5165 LLM \u6765\u63d0\u9ad8\u5176\u5728\u5904\u7406\u7ed3\u6784\u5316\u6570\u636e\uff08\u5982 AMR\uff09\u65f6\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u6587\u672c\u751f\u6210\u4efb\u52a1\u4e2d\u3002", "motivation": "LLM \u8d8a\u6765\u8d8a\u591a\u5730\u5e94\u7528\u4e8e\u6d89\u53ca\u7ed3\u6784\u5316\u8f93\u5165\u7684\u4efb\u52a1\uff0c\u4f8b\u5982\u56fe\u5f62\u3002AMR\uff08\u5c06\u4e30\u5bcc\u7684\u8bed\u4e49\u7f16\u7801\u4e3a\u6709\u5411\u56fe\uff09\u4e3a\u8bc4\u4f30 LLM \u4ece\u6b64\u7c7b\u7ed3\u6784\u751f\u6210\u6587\u672c\u7684\u80fd\u529b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4e25\u683c\u7684\u6d4b\u8bd5\u5e73\u53f0\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f1a\u4efb\u610f\u5730\u7ebf\u6027\u5316 AMR\uff0c\u5ffd\u7565\u5173\u952e\u7684\u7ed3\u6784\u7ebf\u7d22\uff0c\u6216\u8005\u4f9d\u8d56\u4e8e\u4e0e\u6807\u51c6 LLM \u4e0d\u517c\u5bb9\u7684\u67b6\u6784\u3002", "method": "SAFT \u662f\u4e00\u79cd\u7ed3\u6784\u611f\u77e5\u5fae\u8c03\u65b9\u6cd5\uff0c\u65e0\u9700\u67b6\u6784\u66f4\u6539\u5373\u53ef\u5c06\u56fe\u62d3\u6251\u6ce8\u5165\u9884\u8bad\u7ec3\u7684 LLM\u3002\u5b83\u4ece\u53d8\u6362\u540e\u7684 AMR \u7684\u78c1\u62c9\u666e\u62c9\u65af\u7b97\u5b50\u8ba1\u7b97\u65b9\u5411\u654f\u611f\u7684\u4f4d\u7f6e\u7f16\u7801\uff0c\u5e76\u5c06\u5176\u6295\u5f71\u5230 LLM \u7684\u5d4c\u5165\u7a7a\u95f4\u4e2d\u3002", "result": "SAFT \u5728 AMR 3.0 \u4e0a\u8fbe\u5230\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u6bd4\u57fa\u7ebf\u63d0\u9ad8\u4e86 3.5 BLEU \u5206\u6570\u3002\u6539\u8fdb\u7684\u5e45\u5ea6\u4e0e\u56fe\u7684\u590d\u6742\u6027\u6210\u6b63\u6bd4\uff0c\u8fd9\u51f8\u663e\u4e86\u7ed3\u6784\u611f\u77e5\u8868\u793a\u5728\u589e\u5f3a LLM \u6027\u80fd\u65b9\u9762\u7684\u4ef7\u503c\u3002", "conclusion": "SAFT \u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u7528\u4e14\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u7ed3\u6784\u5316\u6570\u636e\u548c\u8bed\u8a00\u6a21\u578b\u4e4b\u95f4\u67b6\u8d77\u6865\u6881\u3002"}}
{"id": "2507.13833", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.13833", "abs": "https://arxiv.org/abs/2507.13833", "authors": ["Zhixin Wang", "Tianyi Zhou", "Liming Liu", "Ao Li", "Jiarui Hu", "Dian Yang", "Jinlong Hou", "Siyuan Feng", "Yuan Cheng", "Yuan Qi"], "title": "DistFlow: A Fully Distributed RL Framework for Scalable and Efficient LLM Post-Training", "comment": null, "summary": "Reinforcement learning (RL) has become the pivotal post-training technique\nfor large language model. Effectively scaling reinforcement learning is now the\nkey to unlocking advanced reasoning capabilities and ensuring safe,\ngoal-aligned behavior in the most powerful LLMs. Mainstream frameworks usually\nemploy a hybrid-controller architecture where a single-controller dispatches\nthe overall execution logic and manages overall data transfer and the\nmulti-controller executes distributed computation. For large-scale\nreinforcement learning, minor load imbalances can introduce significant\nbottlenecks, ultimately constraining the scalability of the system. To address\nthis limitation, we introduce DistFlow, a novel, fully distributed RL framework\ndesigned to break scaling barrier. We adopt a multi-controller paradigm that\ndispatches data transfer and execution tasks to all workers, which eliminates\nthe centralized node. This allows each worker to operate independently, leading\nto near-linear scalability up to thousands of GPUs and dramatic efficiency\ngains. Furthermore, our architecture decouples resource configuration from\nexecution logic, allowing each worker to have a unique execution flow, offering\nsignificant flexibility for rapid and cost-effective algorithmic\nexperimentation. Extensive experiments show that DistFlow achieves excellent\nlinear scalability and up to a 7x end-to-end throughput improvement over\nstate-of-the-art (SOTA) frameworks.", "AI": {"tldr": "DistFlow is a distributed RL framework that improves LLM reasoning and safety by enabling scalable reinforcement learning, outperforming SOTA frameworks in throughput and scalability.", "motivation": "The key to unlocking advanced reasoning capabilities and ensuring safe, goal-aligned behavior in powerful LLMs is scaling reinforcement learning. Minor load imbalances in mainstream hybrid-controller architectures constrain the scalability of the system.", "method": "DistFlow, a novel, fully distributed RL framework that adopts a multi-controller paradigm dispatching data transfer and execution tasks to all workers, eliminating the centralized node and allowing each worker to operate independently. It decouples resource configuration from execution logic, allowing unique execution flows per worker.", "result": "DistFlow achieves near-linear scalability up to thousands of GPUs and dramatic efficiency gains, with up to a 7x end-to-end throughput improvement over SOTA frameworks.", "conclusion": "DistFlow achieved excellent linear scalability and up to a 7x end-to-end throughput improvement over SOTA frameworks."}}
{"id": "2507.13438", "categories": ["quant-ph", "gr-qc", "hep-th"], "pdf": "https://arxiv.org/pdf/2507.13438", "abs": "https://arxiv.org/abs/2507.13438", "authors": ["Kensuke Gallock-Yoshimura", "Erickson Tjoa"], "title": "Bipartite and tripartite entanglement in pure dephasing relativistic spin-boson model", "comment": "22 pages, 5 figures", "summary": "We study non-perturbatively the entanglement generation between two and three\nemitters in an exactly solvable relativistic variant of the spin-boson model,\nequivalent to the time-independent formulation of the Unruh-DeWitt detector\nmodel. We show that (i) (highly) entangled states of the two emitters require\ninteractions very deep into the light cone, (ii) the mass of the field can\ngenerically improve the entanglement generation, (iii) unlike the bipartite\ncase where it is possible to generate close to maximally entangled states via\nthe spin-boson interactions, the generation of genuine tripartite entanglement\nis non-perturbatively hard even at sufficiently long times. Result (iii), in\nparticular, suggests that probing the multipartite entanglement of a\nrelativistic quantum field non-perturbatively requires either different\nprobe-based techniques or variants of the UDW model. Along the way we provide\nthe regularity conditions for the $N$-emitter model to have well-defined ground\nstates in the Fock space.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u76f8\u5bf9\u8bba\u81ea\u65cb-\u73bb\u8272\u6a21\u578b\uff0c\u53d1\u73b0\u53cc\u53d1\u5c04\u5668\u7ea0\u7f20\u6613\u4e8e\u751f\u6210\uff0c\u800c\u4e09\u4f53\u7ea0\u7f20\u751f\u6210\u56f0\u96be\uff0c\u63d0\u793a\u9700\u8981\u65b0\u7684\u7814\u7a76\u65b9\u6cd5\u3002", "motivation": "\u672c\u7814\u7a76\u7684\u52a8\u673a\u5728\u4e8e\u63a2\u8ba8\u5728\u76f8\u5bf9\u8bba\u91cf\u5b50\u573a\u8bba\u6846\u67b6\u4e0b\uff0c\u53d1\u5c04\u5668\u4e4b\u95f4\u7ea0\u7f20\u751f\u6210\u7684\u673a\u5236\u548c\u9650\u5236\uff0c\u7279\u522b\u662f\u7406\u89e3\u591a\u4f53\u7ea0\u7f20\u7684\u751f\u6210\u96be\u5ea6\u3002", "method": "\u672c\u7814\u7a76\u4f7f\u7528\u4e00\u79cd\u53ef\u7cbe\u786e\u6c42\u89e3\u7684\u76f8\u5bf9\u8bba\u81ea\u65cb-\u73bb\u8272\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u7b49\u4ef7\u4e8eUnruh-DeWitt\u63a2\u6d4b\u5668\u6a21\u578b\u7684\u65f6\u65e0\u5173\u5f62\u5f0f\uff0c\u6765\u975e\u5fae\u6270\u5730\u7814\u7a76\u4e24\u4f53\u548c\u4e09\u4f53\u53d1\u5c04\u5668\u4e4b\u95f4\u7684\u7ea0\u7f20\u751f\u6210\u3002\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c(i) \u9ad8\u5ea6\u7ea0\u7f20\u6001\u9700\u8981\u6df1\u5c42\u5149\u9525\u76f8\u4e92\u4f5c\u7528\uff1b(ii) \u573a\u7684\u8d28\u91cf\u53ef\u4ee5\u6539\u5584\u7ea0\u7f20\u751f\u6210\uff1b(iii) \u4e0e\u53cc\u4f53\u60c5\u51b5\u4e0d\u540c\uff0c\u4e09\u4f53\u7ea0\u7f20\u7684\u4ea7\u751f\u5728\u975e\u5fae\u6270\u60c5\u51b5\u4e0b\u975e\u5e38\u56f0\u96be\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u4f9b\u4e86N\u4f53\u6a21\u578b\u5728Fock\u7a7a\u95f4\u4e2d\u5177\u6709\u826f\u597d\u5b9a\u4e49\u57fa\u6001\u7684\u6b63\u5219\u6761\u4ef6\u3002", "conclusion": "\u672c\u7814\u7a76\u8868\u660e\uff0c\u5728\u975e\u5fae\u6270\u60c5\u51b5\u4e0b\uff0c\u4e24\u4f53\u548c\u4e09\u4f53\u53d1\u5c04\u5668\u4e4b\u95f4\u7684\u7ea0\u7f20\u751f\u6210\u5728\u4e0d\u540c\u7684\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u663e\u8457\u5dee\u5f02\u3002\u53cc\u53d1\u5c04\u5668\u7cfb\u7edf\u53ef\u4ee5\u901a\u8fc7\u6df1\u5c42\u5149\u9525\u76f8\u4e92\u4f5c\u7528\u5b9e\u73b0\u9ad8\u5ea6\u7ea0\u7f20\u6001\uff0c\u5e76\u4e14\u573a\u7684\u8d28\u91cf\u53ef\u4ee5\u666e\u904d\u5730\u6539\u5584\u7ea0\u7f20\u751f\u6210\u3002\u7136\u800c\uff0c\u5bf9\u4e8e\u4e09\u4f53\u53d1\u5c04\u5668\u7cfb\u7edf\uff0c\u5373\u4f7f\u5728\u8db3\u591f\u957f\u7684\u65f6\u95f4\u5185\uff0c\u8981\u4ea7\u751f\u771f\u6b63\u7684\u591a\u65b9\u7ea0\u7f20\u4e5f\u975e\u5e38\u56f0\u96be\uff0c\u8fd9\u8868\u660e\u9700\u8981\u65b0\u7684\u63a2\u6d4b\u6280\u672f\u6216UDW\u6a21\u578b\u7684\u53d8\u4f53\u6765\u7814\u7a76\u76f8\u5bf9\u8bba\u91cf\u5b50\u573a\u7684\u591a\u65b9\u7ea0\u7f20\u3002"}}
{"id": "2507.13785", "categories": ["cs.NE"], "pdf": "https://arxiv.org/pdf/2507.13785", "abs": "https://arxiv.org/abs/2507.13785", "authors": ["Mykola Glybovets", "Sergii Medvid"], "title": "MorphoNAS: Embryogenic Neural Architecture Search Through Morphogen-Guided Development", "comment": "13 pages, 8 figures; Preprint of a manuscript submitted for peer\n  review", "summary": "While biological neural networks develop from compact genomes using\nrelatively simple rules, modern artificial neural architecture search methods\nmostly involve explicit and routine manual work. In this paper, we introduce\nMorphoNAS (Morphogenetic Neural Architecture Search), a system able to\ndeterministically grow neural networks through morphogenetic self-organization\ninspired by the Free Energy Principle, reaction-diffusion systems, and gene\nregulatory networks. In MorphoNAS, simple genomes encode just morphogens\ndynamics and threshold-based rules of cellular development. Nevertheless, this\nleads to self-organization of a single progenitor cell into complex neural\nnetworks, while the entire process is built on local chemical interactions. Our\nevolutionary experiments focused on two different domains: structural\ntargeting, in which MorphoNAS system was able to find fully successful genomes\nable to generate predefined random graph configurations (8-31 nodes); and\nfunctional performance on the CartPole control task achieving low complexity\n6-7 neuron solutions when target network size minimization evolutionary\npressure was applied. The evolutionary process successfully balanced between\nquality of of the final solutions and neural architecture search effectiveness.\nOverall, our findings suggest that the proposed MorphoNAS method is able to\ngrow complex specific neural architectures, using simple developmental rules,\nwhich suggests a feasible biological route to adaptive and efficient neural\narchitecture search.", "AI": {"tldr": "MorphoNAS\u662f\u4e00\u79cd\u53d7\u751f\u7269\u5f62\u6001\u53d1\u751f\u542f\u53d1\u7684\u795e\u7ecf\u67b6\u6784\u641c\u7d22\u7cfb\u7edf\uff0c\u5b83\u80fd\u901a\u8fc7\u7b80\u5355\u7684\u89c4\u5219\u81ea\u7ec4\u7ec7\u751f\u6210\u590d\u6742\u7684\u795e\u7ecf\u7f51\u7edc\uff0c\u5e76\u5728\u56fe\u914d\u7f6e\u751f\u6210\u548c\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u826f\u597d\u6548\u679c\uff0c\u5c55\u793a\u4e86\u5176\u5728\u81ea\u9002\u5e94\u548c\u9ad8\u6548\u795e\u7ecf\u67b6\u6784\u641c\u7d22\u65b9\u9762\u7684\u6f5c\u529b\u3002", "motivation": "\u73b0\u4ee3\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u641c\u7d22\u65b9\u6cd5\u4e3b\u8981\u6d89\u53ca\u663e\u5f0f\u548c\u5e38\u89c4\u7684\u624b\u52a8\u5de5\u4f5c\uff0c\u800c\u751f\u7269\u795e\u7ecf\u7f51\u7edc\u5219\u4ece\u7d27\u51d1\u7684\u57fa\u56e0\u7ec4\u548c\u76f8\u5bf9\u7b80\u5355\u7684\u89c4\u5219\u4e2d\u53d1\u5c55\u800c\u6765\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u4e00\u79cd\u53d7\u751f\u7269\u5b66\u542f\u53d1\u7684\u3001\u80fd\u591f\u901a\u8fc7\u81ea\u7ec4\u7ec7\u8fc7\u7a0b\u81ea\u52a8\u751f\u6210\u590d\u6742\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u7684\u65b9\u6cd5\u3002", "method": "MorphoNAS\u7cfb\u7edf\u901a\u8fc7\u6a21\u62df\u81ea\u7531\u80fd\u539f\u7406\u3001\u53cd\u5e94\u6269\u6563\u7cfb\u7edf\u548c\u57fa\u56e0\u8c03\u63a7\u7f51\u7edc\u4e2d\u7684\u5f62\u6001\u53d1\u751f\u81ea\u7ec4\u7ec7\u8fc7\u7a0b\uff0c\u5229\u7528\u7b80\u5355\u7684\u57fa\u56e0\u7ec4\u7f16\u7801\u5f62\u6001\u53d1\u751f\u52a8\u529b\u5b66\u548c\u57fa\u4e8e\u9608\u503c\u7684\u7ec6\u80de\u53d1\u80b2\u89c4\u5219\uff0c\u4ece\u5355\u4e2a\u7956\u7ec6\u80de\u751f\u957f\u51fa\u590d\u6742\u7684\u795e\u7ecf\u7f51\u7edc\uff0c\u6574\u4e2a\u8fc7\u7a0b\u57fa\u4e8e\u5c40\u90e8\u5316\u5b66\u76f8\u4e92\u4f5c\u7528\u3002", "result": "\u5728\u7ed3\u6784\u9776\u5411\u4efb\u52a1\u4e2d\uff0cMorphoNAS\u6210\u529f\u627e\u5230\u80fd\u591f\u751f\u6210\u9884\u5b9a\u968f\u673a\u56fe\u914d\u7f6e\uff088-31\u4e2a\u8282\u70b9\uff09\u7684\u57fa\u56e0\u7ec4\u3002\u5728CartPole\u63a7\u5236\u4efb\u52a1\u4e2d\uff0c\u5f53\u65bd\u52a0\u7f51\u7edc\u5c3a\u5bf8\u6700\u5c0f\u5316\u7684\u8fdb\u5316\u538b\u529b\u65f6\uff0cMorphoNAS\u80fd\u591f\u627e\u5230\u4ec5\u5305\u542b6-7\u4e2a\u795e\u7ecf\u5143\u7684\u4f4e\u590d\u6742\u5ea6\u89e3\u51b3\u65b9\u6848\u3002\u8fdb\u5316\u8fc7\u7a0b\u6210\u529f\u5730\u5e73\u8861\u4e86\u6700\u7ec8\u89e3\u51b3\u65b9\u6848\u7684\u8d28\u91cf\u548c\u795e\u7ecf\u67b6\u6784\u641c\u7d22\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684MorphoNAS\u65b9\u6cd5\u80fd\u591f\u901a\u8fc7\u7b80\u5355\u7684\u53d1\u80b2\u89c4\u5219\u751f\u957f\u590d\u6742\u7684\u7279\u5b9a\u795e\u7ecf\u67b6\u6784\uff0c\u8fd9\u4e3a\u81ea\u9002\u5e94\u548c\u9ad8\u6548\u7684\u795e\u7ecf\u67b6\u6784\u641c\u7d22\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u751f\u7269\u5b66\u9014\u5f84\u3002"}}
{"id": "2507.13393", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13393", "abs": "https://arxiv.org/abs/2507.13393", "authors": ["Jakub Strawa", "Jarek Duda"], "title": "Improving KAN with CDF normalization to quantiles", "comment": "7 pages, 9 figures", "summary": "Data normalization is crucial in machine learning, usually performed by\nsubtracting the mean and dividing by standard deviation, or by rescaling to a\nfixed range. In copula theory, popular in finance, there is used normalization\nto approximately quantiles by transforming x to CDF(x) with estimated CDF\n(cumulative distribution function) to nearly uniform distribution in [0,1],\nallowing for simpler representations which are less likely to overfit. It seems\nnearly unknown in machine learning, therefore, we would like to present some\nits advantages on example of recently popular Kolmogorov-Arnold Networks\n(KANs), improving predictions from Legendre-KAN by just switching rescaling to\nCDF normalization. Additionally, in HCR interpretation, weights of such neurons\nare mixed moments providing local joint distribution models, allow to propagate\nalso probability distributions, and change propagation direction.", "AI": {"tldr": "CDF \u5f52\u4e00\u5316\u5728 KANs \u4e2d\u4f18\u4e8e\u4f20\u7edf\u5f52\u4e00\u5316\uff0c\u80fd\u63d0\u5347\u9884\u6d4b\u7cbe\u5ea6\u548c\u89e3\u91ca\u6027\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u4e2d\u666e\u904d\u4f7f\u7528\u6570\u636e\u5f52\u4e00\u5316\uff0c\u4f46\u4f20\u7edf\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\uff08\u5982\u51cf\u53bb\u5747\u503c\u9664\u4ee5\u6807\u51c6\u5dee\u6216\u7f29\u653e\u5230\u56fa\u5b9a\u8303\u56f4\uff09\u5728\u5904\u7406\u91d1\u878d\u9886\u57df\u7684 copula \u7406\u8bba\u4e2d\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\u65f6\u53ef\u80fd\u5b58\u5728\u4e0d\u8db3\u3002\u91d1\u878d\u9886\u57df\u4f7f\u7528 CDF \u53d8\u6362\u5c06\u6570\u636e\u8fd1\u4f3c\u5230 [0,1] \u533a\u95f4\u5185\u7684\u5747\u5300\u5206\u5e03\uff0c\u8fd9\u79cd\u65b9\u6cd5\u5728\u673a\u5668\u5b66\u4e60\u4e2d\u4f3c\u4e4e\u672a\u88ab\u5145\u5206\u8ba4\u8bc6\uff0c\u4f46\u53ef\u80fd\u5e26\u6765\u4f18\u52bf\u3002", "method": "\u672c\u6587\u63d0\u51fa\u5e76\u6f14\u793a\u4e86\u4f7f\u7528\u7d2f\u79ef\u5206\u5e03\u51fd\u6570\uff08CDF\uff09\u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u4ee5\u66ff\u4ee3\u4f20\u7edf\u7684\u5747\u503c-\u65b9\u5dee\u5f52\u4e00\u5316\u6216\u6700\u5c0f-\u6700\u5927\u5f52\u4e00\u5316\u3002\u4f5c\u8005\u901a\u8fc7\u5728 Kolmogorov-Arnold Networks (KANs) \u4e0a\u7684\u5b9e\u9a8c\uff0c\u5c06\u4f20\u7edf\u7684 Legendre-KAN \u6a21\u578b\u4e0e\u4f7f\u7528 CDF \u5f52\u4e00\u5316\u7684\u6a21\u578b\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "result": "\u901a\u8fc7\u5c06 Legendre-KAN \u6a21\u578b\u4e2d\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\u4ece\u4f20\u7edf\u65b9\u6cd5\u5207\u6362\u4e3a CDF \u5f52\u4e00\u5316\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u9884\u6d4b\u7cbe\u5ea6\u3002\u6b64\u5916\uff0cCDF \u5f52\u4e00\u5316\u5728 HCR \u89e3\u91ca\u4e2d\uff0c\u5176\u6743\u91cd\u53ef\u4ee5\u88ab\u7406\u89e3\u4e3a\u6df7\u5408\u77e9\uff0c\u80fd\u591f\u6a21\u62df\u5c40\u90e8\u8054\u5408\u5206\u5e03\uff0c\u5e76\u5141\u8bb8\u6982\u7387\u5206\u5e03\u7684\u4f20\u64ad\u548c\u6539\u53d8\u4f20\u64ad\u65b9\u5411\u3002", "conclusion": "CDF \u5f52\u4e00\u5316\u662f\u4e00\u79cd\u5728\u673a\u5668\u5b66\u4e60\u4e2d\uff0c\u7279\u522b\u662f\u5728 KANs \u4e2d\uff0c\u5f88\u6709\u524d\u666f\u7684\u66ff\u4ee3\u5f52\u4e00\u5316\u65b9\u6cd5\uff0c\u53ef\u4ee5\u63d0\u9ad8\u9884\u6d4b\u7cbe\u5ea6\u5e76\u63d0\u4f9b\u66f4\u5f3a\u7684\u89e3\u91ca\u6027\u3002"}}
{"id": "2507.13370", "categories": ["cs.SI", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.13370", "abs": "https://arxiv.org/abs/2507.13370", "authors": ["Shijun Guo", "Haoran Xu", "Yaming Yang", "Ziyu Guan", "Wei Zhao", "Xinyi Zhang", "Yishan Song", "Jiwei Chen"], "title": "H-NeiFi: Non-Invasive and Consensus-Efficient Multi-Agent Opinion Guidance", "comment": null, "summary": "The openness of social media enables the free exchange of opinions, but it\nalso presents challenges in guiding opinion evolution towards global consensus.\nExisting methods often directly modify user views or enforce cross-group\nconnections. These intrusive interventions undermine user autonomy, provoke\npsychological resistance, and reduce the efficiency of global consensus.\nAdditionally, due to the lack of a long-term perspective, promoting local\nconsensus often exacerbates divisions at the macro level. To address these\nissues, we propose the hierarchical, non-intrusive opinion guidance framework,\nH-NeiFi. It first establishes a two-layer dynamic model based on social roles,\nconsidering the behavioral characteristics of both experts and non-experts.\nAdditionally, we introduce a non-intrusive neighbor filtering method that\nadaptively controls user communication channels. Using multi-agent\nreinforcement learning (MARL), we optimize information propagation paths\nthrough a long-term reward function, avoiding direct interference with user\ninteractions. Experiments show that H-NeiFi increases consensus speed by 22.0%\nto 30.7% and maintains global convergence even in the absence of experts. This\napproach enables natural and efficient consensus guidance by protecting user\ninteraction autonomy, offering a new paradigm for social network governance.", "AI": {"tldr": "\u63d0\u51fa\u5206\u5c42\u975e\u4fb5\u5165\u5f0f\u610f\u89c1\u5f15\u5bfc\u6846\u67b6H-NeiFi\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u4fe1\u606f\u4f20\u64ad\uff0c\u63d0\u9ad8\u5171\u8bc6\u901f\u5ea630.7%\uff0c\u5e76\u4fdd\u6301\u5168\u5c40\u6536\u655b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u76f4\u63a5\u4fee\u6539\u7528\u6237\u89c2\u70b9\u6216\u5f3a\u5236\u8de8\u7fa4\u4f53\u8fde\u63a5\uff09\u4f1a\u635f\u5bb3\u7528\u6237\u81ea\u4e3b\u6027\u3001\u5f15\u53d1\u5fc3\u7406\u62b5\u89e6\u5e76\u964d\u4f4e\u5168\u5c40\u5171\u8bc6\u6548\u7387\u3002\u6b64\u5916\uff0c\u7f3a\u4e4f\u957f\u671f\u89c6\u89d2\u5bfc\u81f4\u4fc3\u8fdb\u5c40\u90e8\u5171\u8bc6\u53cd\u800c\u52a0\u5267\u5b8f\u89c2\u5206\u88c2\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u793e\u4ea4\u89d2\u8272\u7684\u5206\u5c42\u3001\u975e\u4fb5\u5165\u5f0f\u610f\u89c1\u5f15\u5bfc\u6846\u67b6H-NeiFi\uff0c\u8be5\u6846\u67b6\u5305\u542b\u4e00\u4e2a\u4e24\u5c42\u52a8\u6001\u6a21\u578b\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u63a7\u5236\u7528\u6237\u901a\u4fe1\u6e20\u9053\u7684\u975e\u4fb5\u5165\u5f0f\u90bb\u5c45\u8fc7\u6ee4\u65b9\u6cd5\u3002\u5229\u7528\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u548c\u957f\u671f\u5956\u52b1\u51fd\u6570\u4f18\u5316\u4fe1\u606f\u4f20\u64ad\u8def\u5f84\uff0c\u907f\u514d\u76f4\u63a5\u5e72\u9884\u7528\u6237\u4e92\u52a8\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cH-NeiFi\u5c06\u5171\u8bc6\u901f\u5ea6\u63d0\u9ad8\u4e8622.0%\u81f330.7%\uff0c\u5e76\u4e14\u5728\u6ca1\u6709\u4e13\u5bb6\u7684\u60c5\u51b5\u4e0b\u4ecd\u80fd\u4fdd\u6301\u5168\u5c40\u6536\u655b\u3002", "conclusion": "H-NeiFi\u6846\u67b6\u901a\u8fc7\u4fdd\u62a4\u7528\u6237\u4e92\u52a8\u81ea\u4e3b\u6027\uff0c\u5b9e\u73b0\u4e86\u81ea\u7136\u4e14\u9ad8\u6548\u7684\u5171\u8bc6\u5f15\u5bfc\uff0c\u4e3a\u793e\u4ea4\u7f51\u7edc\u6cbb\u7406\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2507.13526", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.13526", "abs": "https://arxiv.org/abs/2507.13526", "authors": ["Gedeon Ghislain Nkwewo Ngoufo", "Khaled Humadi", "Elham Baladi", "Gunes Karabulut Kurt"], "title": "Space Shift Keying-Enabled ISAC for Efficient Debris Detection and Communication in LEO Satellite Networks", "comment": null, "summary": "The proliferation of space debris in low Earth orbit (LEO) presents critical\nchallenges for orbital safety, particularly for satellite constellations.\nIntegrated sensing and communication (ISAC) systems provide a promising dual\nfunction solution by enabling both environmental sensing and data\ncommunication. This study explores the use of space shift keying (SSK)\nmodulation within ISAC frameworks, evaluating its performance when combined\nwith sinusoidal and chirp radar waveforms. SSK is particularly attractive due\nto its low hardware complexity and robust communication performance. Our\nresults demonstrate that both waveforms achieve comparable bit error rate (BER)\nperformance under SSK, validating its effectiveness for ISAC applications.\nHowever, waveform selection significantly affects sensing capability: while the\nsinusoidal waveform supports simpler implementation, its high ambiguity limits\nrange detection. In contrast, the chirp waveform enables range estimation and\nprovides a modest improvement in velocity detection accuracy. These findings\nhighlight the strength of SSK as a modulation scheme for ISAC and emphasize the\nimportance of selecting appropriate waveforms to optimize sensing accuracy\nwithout compromising communication performance. This insight supports the\ndesign of efficient and scalable ISAC systems for space applications,\nparticularly in the context of orbital debris monitoring.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4f7f\u7528SSK\u8c03\u5236\u6280\u672f\uff0c\u5e76\u5206\u522b\u7ed3\u5408\u6b63\u5f26\u548c\u091airl\u6ce2\u5f62\u5728ISAC\u7cfb\u7edf\u4e2d\u8fdb\u884c\u4e86\u6027\u80fd\u8bc4\u4f30\u3002SSK\u5728\u901a\u4fe1\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u091airl\u6ce2\u5f62\u5728\u611f\u77e5\u80fd\u529b\u65b9\u9762\u4f18\u4e8e\u6b63\u5f26\u6ce2\u5f62\u3002", "motivation": "\u4e3a\u4e86\u5e94\u5bf9LEO\u8f68\u9053\u788e\u7247\u5e26\u6765\u7684\u8f68\u9053\u5b89\u5168\u6311\u6218\uff0cISAC\u7cfb\u7edf\u88ab\u8ba4\u4e3a\u662f\u96c6\u73af\u5883\u611f\u77e5\u548c\u6570\u636e\u901a\u4fe1\u4e8e\u4e00\u4f53\u7684\u89e3\u51b3\u65b9\u6848\u3002SSK\u56e0\u5176\u786c\u4ef6\u590d\u6742\u5ea6\u4f4e\u548c\u901a\u4fe1\u6027\u80fd\u7a33\u5065\u800c\u88ab\u9009\u4e2d\u7528\u4e8eISAC\u3002", "method": "\u672c\u6587\u7814\u7a76\u4e86\u5728ISAC\u6846\u67b6\u4e0b\u4f7f\u7528SSK\u8c03\u5236\uff0c\u5e76\u8bc4\u4f30\u4e86\u5176\u4e0e\u6b63\u5f26\u548c\u091airl\u96f7\u8fbe\u6ce2\u5f62\u7ed3\u5408\u4f7f\u7528\u7684\u6027\u80fd\u3002", "result": "SSK\u53ef\u4ee5\u5b9e\u73b0\u826f\u597d\u7684\u901a\u4fe1\u6027\u80fd\uff0c\u5e76\u4e14\u901a\u4fe1\u6027\u80fd\u4e0d\u53d7\u901a\u4fe1\u6ce2\u5f62\u7684\u5f71\u54cd\u3002\u5728\u611f\u77e5\u80fd\u529b\u65b9\u9762\uff0c\u091airl\u6ce2\u5f62\u76f8\u6bd4\u6b63\u5f26\u6ce2\u5f62\u5177\u6709\u66f4\u597d\u7684\u6d4b\u8ddd\u548c\u6d4b\u901f\u80fd\u529b\u3002", "conclusion": "SSK\u662f\u4e00\u79cd\u9002\u7528\u4e8eISAC\u7684\u8c03\u5236\u65b9\u6848\uff0c\u5e76\u4e14\u5176\u6027\u80fd\u4e0d\u53d7\u901a\u4fe1\u6ce2\u5f62\u7684\u5f71\u54cd\u3002\u7136\u800c\uff0c\u4e0d\u540c\u6ce2\u5f62\u4f1a\u5f71\u54cdISAC\u7cfb\u7edf\u7684\u611f\u77e5\u80fd\u529b\uff0c\u5176\u4e2d\u6b63\u5f26\u6ce2\u5f62\u6613\u4e8e\u5b9e\u73b0\u4f46\u6d4b\u8ddd\u80fd\u529b\u6709\u9650\uff0c\u800c\u091airl\u6ce2\u5f62\u5219\u53ef\u4ee5\u5b9e\u73b0\u6d4b\u8ddd\u5e76\u4e14\u5728\u6d4b\u901f\u65b9\u9762\u6709\u5c11\u91cf\u63d0\u5347\u3002"}}
{"id": "2507.13361", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13361", "abs": "https://arxiv.org/abs/2507.13361", "authors": ["Shmuel Berman", "Jia Deng"], "title": "VLMs have Tunnel Vision: Evaluating Nonlocal Visual Reasoning in Leading VLMs", "comment": null, "summary": "Visual Language Models (VLMs) excel at complex visual tasks such as VQA and\nchart understanding, yet recent work suggests they struggle with simple\nperceptual tests. We present an evaluation that tests vision-language models'\ncapacity for nonlocal visual reasoning -- reasoning that requires chaining\nevidence collected from multiple, possibly distant, regions of an image. We\nisolate three distinct forms of non-local vision: comparative perception, which\ndemands holding two images in working memory and comparing them; saccadic\nsearch, which requires making discrete, evidence-driven jumps to locate\nsuccessive targets; and smooth visual search, which involves searching smoothly\nalong a continuous contour. Flagship models (e.g., Gemini 2.5 Pro, Claude\nVision 3.7, GPT-o4-mini), even those that perform well on prior\nprimitive-vision benchmarks, fail these tests and barely exceed random accuracy\non two variants of our tasks that are trivial for humans. Our structured\nevaluation suite allows us to test if VLMs can perform similar visual\nalgorithms to humans. Our findings show that despite gains in raw visual\nacuity, current models lack core visual reasoning capabilities.", "AI": {"tldr": "Current Visual Language Models (VLMs) fail at non-local visual reasoning tasks that require understanding relationships between different parts of an image, despite their success in complex visual tasks. Even leading models struggle with basic tests that humans find easy, highlighting a gap in core visual reasoning abilities.", "motivation": "Recent work suggests VLMs struggle with simple perceptual tests despite excelling at complex visual tasks. This evaluation aims to test VLMs' capacity for non-local visual reasoning, which requires chaining evidence from multiple, potentially distant, image regions.", "method": "A structured evaluation suite was created to test non-local visual reasoning in VLMs, isolating three forms: comparative perception, saccadic search, and smooth visual search. The evaluation assesses if VLMs can perform visual algorithms similar to humans.", "result": "Flagship VLM models (e.g., Gemini 2.5 Pro, Claude Vision 3.7, GPT-o4-mini) performed poorly on the designed tests, barely exceeding random accuracy on some variants, even those considered trivial for humans. This indicates a lack of core visual reasoning capabilities.", "conclusion": "Despite improvements in visual acuity, current VLMs lack fundamental visual reasoning skills, failing tests designed to assess non-local reasoning capabilities."}}
{"id": "2507.13550", "categories": ["cs.AI", "cs.CL", "cs.SC"], "pdf": "https://arxiv.org/pdf/2507.13550", "abs": "https://arxiv.org/abs/2507.13550", "authors": ["Eduardo C. Garrido-Merch\u00e1n", "Cristina Puente"], "title": "GOFAI meets Generative AI: Development of Expert Systems by means of Large Language Models", "comment": null, "summary": "The development of large language models (LLMs) has successfully transformed\nknowledge-based systems such as open domain question nswering, which can\nautomatically produce vast amounts of seemingly coherent information. Yet,\nthose models have several disadvantages like hallucinations or confident\ngeneration of incorrect or unverifiable facts. In this paper, we introduce a\nnew approach to the development of expert systems using LLMs in a controlled\nand transparent way. By limiting the domain and employing a well-structured\nprompt-based extraction approach, we produce a symbolic representation of\nknowledge in Prolog, which can be validated and corrected by human experts.\nThis approach also guarantees interpretability, scalability and reliability of\nthe developed expert systems. Via quantitative and qualitative experiments with\nClaude Sonnet 3.7 and GPT-4.1, we show strong adherence to facts and semantic\ncoherence on our generated knowledge bases. We present a transparent hybrid\nsolution that combines the recall capacity of LLMs with the precision of\nsymbolic systems, thereby laying the foundation for dependable AI applications\nin sensitive domains.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408LLM\u548cProlog\u7684\u6df7\u5408\u65b9\u6cd5\uff0c\u7528\u4e8e\u6784\u5efa\u53ef\u9760\u3001\u53ef\u89e3\u91ca\u7684\u4e13\u5bb6\u7cfb\u7edf\u3002\u901a\u8fc7\u7ed3\u6784\u5316\u63d0\u793a\u63d0\u53d6\u77e5\u8bc6\u5e76\u8f6c\u6362\u4e3aProlog\uff0c\u53ef\u7531\u4e13\u5bb6\u9a8c\u8bc1\uff0c\u89e3\u51b3\u4e86LLM\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u751f\u6210\u770b\u4f3c\u8fde\u8d2f\u7684\u4fe1\u606f\u65b9\u9762\u53d6\u5f97\u4e86\u5de8\u5927\u6210\u529f\uff0c\u4f46\u5176\u7f3a\u70b9\u4e5f\u65e5\u76ca\u663e\u73b0\uff0c\u4f8b\u5982\u4ea7\u751f\u5e7b\u89c9\u6216\u81ea\u4fe1\u5730\u751f\u6210\u4e0d\u6b63\u786e\u6216\u65e0\u6cd5\u9a8c\u8bc1\u7684\u4e8b\u5b9e\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e9b\u6311\u6218\uff0c\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u4ee5\u53d7\u63a7\u548c\u900f\u660e\u7684\u65b9\u5f0f\u5229\u7528LLM\u6784\u5efa\u4e13\u5bb6\u7cfb\u7edf\uff0c\u786e\u4fdd\u5176\u53ef\u9760\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e13\u5bb6\u7cfb\u7edf\u5f00\u53d1\u65b9\u6cd5\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8fdb\u884c\u53d7\u63a7\u548c\u900f\u660e\u7684\u77e5\u8bc6\u63d0\u53d6\u3002\u5177\u4f53\u800c\u8a00\uff0c\u8be5\u65b9\u6cd5\u9996\u5148\u9650\u5236LLM\u7684\u5904\u7406\u9886\u57df\uff0c\u7136\u540e\u91c7\u7528\u7ed3\u6784\u5316\u7684\u63d0\u793a\uff08prompt-based\uff09\u63d0\u53d6\u77e5\u8bc6\uff0c\u6700\u540e\u5c06\u63d0\u53d6\u7684\u77e5\u8bc6\u8f6c\u6362\u4e3aProlog\u4e2d\u7684\u7b26\u53f7\u8868\u793a\u3002\u8fd9\u79cd\u7b26\u53f7\u8868\u793a\u53ef\u4ee5\u7531\u4eba\u7c7b\u4e13\u5bb6\u8fdb\u884c\u9a8c\u8bc1\u548c\u7ea0\u6b63\uff0c\u4ece\u800c\u786e\u4fdd\u4e86\u7cfb\u7edf\u7684\u53ef\u89e3\u91ca\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u53ef\u9760\u6027\u3002", "result": "\u901a\u8fc7\u4f7f\u7528Claude Sonnet 3.7\u548cGPT-4.1\u8fdb\u884c\u7684\u5b9a\u91cf\u548c\u5b9a\u6027\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u751f\u6210\u7684\u77e5\u8bc6\u5e93\u5728\u4e8b\u5b9e\u9075\u5faa\u548c\u8bed\u4e49\u8fde\u8d2f\u6027\u65b9\u9762\u5177\u6709\u5f88\u5f3a\u7684\u4f18\u52bf\u3002\u8be5\u6df7\u5408\u89e3\u51b3\u65b9\u6848\u7ed3\u5408\u4e86LLM\u7684\u8bb0\u5fc6\u80fd\u529b\u548c\u7b26\u53f7\u7cfb\u7edf\u7684\u7cbe\u786e\u6027\uff0c\u4e3a\u5728\u654f\u611f\u9886\u57df\u5f00\u53d1\u53ef\u9760\u7684AI\u5e94\u7528\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e0e\u7b26\u53f7\u7cfb\u7edf\u76f8\u7ed3\u5408\u7684\u6df7\u5408\u65b9\u6cd5\uff0c\u7528\u4e8e\u6784\u5efa\u53ef\u89e3\u91ca\u3001\u53ef\u6269\u5c55\u4e14\u53ef\u9760\u7684\u4e13\u5bb6\u7cfb\u7edf\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u9650\u5236LLM\u7684\u9886\u57df\u5e76\u91c7\u7528\u57fa\u4e8e\u63d0\u793a\u7684\u63d0\u53d6\u6280\u672f\uff0c\u5c06\u77e5\u8bc6\u8f6c\u6362\u4e3aProlog\u4e2d\u7684\u7b26\u53f7\u8868\u793a\uff0c\u4ee5\u4fbf\u4eba\u7c7b\u4e13\u5bb6\u8fdb\u884c\u9a8c\u8bc1\u548c\u7ea0\u6b63\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e8b\u5b9e\u51c6\u786e\u6027\u548c\u8bed\u4e49\u8fde\u8d2f\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u5728\u654f\u611f\u9886\u57df\u5f00\u53d1\u53ef\u9760\u7684AI\u5e94\u7528\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.13419", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2507.13419", "abs": "https://arxiv.org/abs/2507.13419", "authors": ["Joost Mertens", "Joachim Denil"], "title": "Lab-Scale Gantry Crane Digital Twin Exemplar", "comment": "6 pages, 8 figures, associated GitHub repository:\n  https://github.com/Cosys-Lab/lab-scale-gantry-crane", "summary": "The research topic of digital twins has attracted a large amount of interest\nover the past decade. However, publicly available exemplars remain scarce. In\nthe interest of open and reproducible science, in this exemplar paper we\npresent a lab-scale gantry crane and its digital twin. The exemplar comprises\nboth the physical and digital side of the twin system. The physical side\nconsists of the physical crane and its controller. The digital side covers the\nCAD models and kinematic model of the crane, and provides services for optimal\ncontrol, historical data logging, data visualization and continuous validation.\nWe used this setup as use case in several previous publications where its\nfunctionality was validated. It is publicly available and only relies on other\nfreely available and commonly used software, this way we hope it can be used\nfor future research or education on the topic of digital twins.", "AI": {"tldr": "\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6613\u4e8e\u8bbf\u95ee\u548c\u4f7f\u7528\u7684\u5b9e\u9a8c\u5ba4\u89c4\u6a21\u9f99\u95e8\u8d77\u91cd\u673a\u53ca\u5176\u6570\u5b57\u5b6a\u751f\u7cfb\u7edf\u793a\u4f8b\uff0c\u5305\u542b\u7269\u7406\u548c\u6570\u5b57\u7ec4\u4ef6\uff0c\u5e76\u63d0\u4f9b\u591a\u79cd\u670d\u52a1\uff0c\u65e8\u5728\u63a8\u52a8\u6570\u5b57\u5b6a\u751f\u9886\u57df\u7684\u7814\u7a76\u548c\u6559\u80b2\u3002", "motivation": "\u4e3a\u4e86\u4fc3\u8fdb\u5f00\u653e\u548c\u53ef\u91cd\u590d\u7684\u79d1\u5b66\u7814\u7a76\uff0c\u89e3\u51b3\u6570\u5b57\u5b6a\u751f\u9886\u57df\u516c\u5f00\u793a\u4f8b\u7a00\u7f3a\u7684\u95ee\u9898\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b\u7269\u7406\u548c\u6570\u5b57\u90e8\u5206\u7684\u5b9e\u9a8c\u5ba4\u89c4\u6a21\u9f99\u95e8\u8d77\u91cd\u673a\u6570\u5b57\u5b6a\u751f\u7cfb\u7edf\u3002\u7269\u7406\u90e8\u5206\u5305\u62ec\u5b9e\u9645\u7684\u8d77\u91cd\u673a\u548c\u63a7\u5236\u5668\uff1b\u6570\u5b57\u90e8\u5206\u5219\u5305\u542bCAD\u6a21\u578b\u3001\u8fd0\u52a8\u5b66\u6a21\u578b\uff0c\u5e76\u63d0\u4f9b\u4f18\u5316\u63a7\u5236\u3001\u5386\u53f2\u6570\u636e\u8bb0\u5f55\u3001\u6570\u636e\u53ef\u89c6\u5316\u548c\u6301\u7eed\u9a8c\u8bc1\u7b49\u670d\u52a1\u3002", "result": "\u6210\u529f\u6784\u5efa\u4e86\u4e00\u4e2a\u5b9e\u9a8c\u5ba4\u89c4\u6a21\u7684\u9f99\u95e8\u8d77\u91cd\u673a\u53ca\u5176\u6570\u5b57\u5b6a\u751f\u7cfb\u7edf\u7684\u516c\u5f00\u793a\u4f8b\uff0c\u8be5\u7cfb\u7edf\u529f\u80fd\u5f97\u5230\u9a8c\u8bc1\uff0c\u5e76\u4e14\u6613\u4e8e\u8bbf\u95ee\u548c\u4f7f\u7528\uff0c\u53ef\u7528\u4e8e\u672a\u6765\u7684\u7814\u7a76\u548c\u6559\u80b2\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u9a8c\u5ba4\u89c4\u6a21\u7684\u9f99\u95e8\u8d77\u91cd\u673a\u53ca\u5176\u6570\u5b57\u5b6a\u751f\u7cfb\u7edf\u7684\u516c\u5f00\u793a\u4f8b\uff0c\u65e8\u5728\u4fc3\u8fdb\u5f00\u653e\u548c\u53ef\u91cd\u590d\u7684\u79d1\u5b66\u7814\u7a76\u3002\u8be5\u7cfb\u7edf\u5305\u62ec\u7269\u7406\u5b9e\u4f53\uff08\u8d77\u91cd\u673a\u53ca\u5176\u63a7\u5236\u5668\uff09\u548c\u6570\u5b57\u5b6a\u751f\uff08CAD\u6a21\u578b\u3001\u8fd0\u52a8\u5b66\u6a21\u578b\uff09\uff0c\u5e76\u63d0\u4f9b\u4f18\u5316\u63a7\u5236\u3001\u6570\u636e\u8bb0\u5f55\u3001\u53ef\u89c6\u5316\u548c\u9a8c\u8bc1\u7b49\u670d\u52a1\u3002\u8be5\u7cfb\u7edf\u5df2\u5728\u4ee5\u5f80\u7684\u51fa\u7248\u7269\u4e2d\u5f97\u5230\u9a8c\u8bc1\uff0c\u5e76\u4e14\u662f\u516c\u5f00\u53ef\u7528\u7684\uff0c\u4ec5\u4f9d\u8d56\u4e8e\u514d\u8d39\u8f6f\u4ef6\uff0c\u5e0c\u671b\u53ef\u4ee5\u4e3a\u6570\u5b57\u5b6a\u751f\u9886\u57df\u672a\u6765\u7684\u7814\u7a76\u6216\u6559\u80b2\u63d0\u4f9b\u53c2\u8003\u3002"}}
{"id": "2507.13672", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.13672", "abs": "https://arxiv.org/abs/2507.13672", "authors": ["Hang Zhou", "Tao Meng", "Kun Wang", "Chengrui Shi", "Renhao Mao", "Weijia Wang", "Jiakun Lei"], "title": "Spacecraft Safe Robust Control Using Implicit Neural Representation for Geometrically Complex Targets in Proximity Operations", "comment": "15 pages, 18 figures, submitted to TAES", "summary": "This study addresses the challenge of ensuring safe spacecraft proximity\noperations, focusing on collision avoidance between a chaser spacecraft and a\ncomplex-geometry target spacecraft under disturbances. To ensure safety in such\nscenarios, a safe robust control framework is proposed that leverages implicit\nneural representations. To handle arbitrary target geometries without explicit\nmodeling, a neural signed distance function (SDF) is learned from point cloud\ndata via a enhanced implicit geometric regularization method, which\nincorporates an over-apporximation strategy to create a conservative,\nsafety-prioritized boundary. The target's surface is implicitly defined by the\nzero-level set of the learned neural SDF, while the values and gradients\nprovide critical information for safety controller design. This neural SDF\nrepresentation underpins a two-layer hierarchcial safe robust control\nframework: a safe velocity generation layer and a safe robust controller layer.\nIn the first layer, a second-order cone program is formulated to generate\nsafety-guaranteed reference velocity by explicitly incorporating the\nunder-approximation error bound. Furthermore, a circulation inequality is\nintroduced to mitigate the local minimum issues commonly encountered in control\nbarrier function (CBF) methods. The second layer features an integrated\ndisturbance observer and a smooth safety filter explicitly compensating for\nestimation error, bolstering robustness to external disturbances. Extensive\nnumerical simulations and Monte Carlo analysis validate the proposed framework,\ndemonstrating significantly improved safety margins and avoidance of local\nminima compared to conventional CBF approaches.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5b89\u5168\u9c81\u68d2\u63a7\u5236\u6846\u67b6\uff0c\u7528\u4e8e\u822a\u5929\u5668\u8fd1\u8ddd\u79bb\u64cd\u4f5c\u4e2d\u7684\u78b0\u649e\u907f\u514d\uff0c\u7279\u522b\u662f\u5728\u9762\u5bf9\u590d\u6742\u76ee\u6807\u548c\u5e72\u6270\u65f6\u3002\u8be5\u6846\u67b6\u5229\u7528\u5b66\u4e60\u5230\u7684\u795e\u7ecfSDF\u6765\u9690\u5f0f\u8868\u793a\u76ee\u6807\u51e0\u4f55\uff0c\u5e76\u901a\u8fc7\u4e24\u5c42\u63a7\u5236\u7b56\u7565\uff08\u5b89\u5168\u901f\u5ea6\u751f\u6210\u548c\u5b89\u5168\u9c81\u68d2\u63a7\u5236\uff09\u786e\u4fdd\u64cd\u4f5c\u5b89\u5168\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u5176\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u786e\u4fdd\u822a\u5929\u5668\u8fd1\u8ddd\u79bb\u64cd\u4f5c\uff08\u7279\u522b\u662f\u590d\u6742\u51e0\u4f55\u76ee\u6807\u822a\u5929\u5668\uff09\u5728\u5e72\u6270\u4e0b\u7684\u78b0\u649e\u907f\u514d\u5b89\u5168\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u7684\u5b89\u5168\u9c81\u68d2\u63a7\u5236\u6846\u67b6\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u589e\u5f3a\u7684\u9690\u5f0f\u51e0\u4f55\u6b63\u5219\u5316\u65b9\u6cd5\u4ece\u70b9\u4e91\u6570\u636e\u4e2d\u5b66\u4e60\u795e\u7ecf\u7b26\u53f7\u8ddd\u79bb\u51fd\u6570\uff08SDF\uff09\uff0c\u5e76\u91c7\u7528\u8fc7\u903c\u8fd1\u7b56\u7565\u521b\u5efa\u4fdd\u5b88\u7684\u5b89\u5168\u8fb9\u754c\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u4e24\u5c42\u5206\u5c42\u63a7\u5236\u6846\u67b6\uff1a\u4e00\u4e2a\u5b89\u5168\u901f\u5ea6\u751f\u6210\u5c42\uff08\u5229\u7528\u4e8c\u9636\u9525\u89c4\u5212\u548c\u5faa\u73af\u4e0d\u7b49\u5f0f\uff09\u548c\u4e00\u4e2a\u5b89\u5168\u9c81\u68d2\u63a7\u5236\u5668\u5c42\uff08\u96c6\u6210\u4e86\u5e72\u6270\u89c2\u6d4b\u5668\u548c\u5149\u6ed1\u5b89\u5168\u6ee4\u6ce2\u5668\uff09\u3002", "result": "\u901a\u8fc7\u5e7f\u6cdb\u7684\u6570\u503c\u6a21\u62df\u548c\u8499\u7279\u5361\u6d1b\u5206\u6790\u9a8c\u8bc1\uff0c\u6240\u63d0\u51fa\u7684\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4e86\u5b89\u5168\u88d5\u5ea6\uff0c\u5e76\u6709\u6548\u907f\u514d\u4e86\u4f20\u7edfCBF\u65b9\u6cd5\u4e2d\u5e38\u89c1\u7684\u5c40\u90e8\u6781\u5c0f\u503c\u95ee\u9898\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u795e\u7ecfSDF\u548c\u4e24\u5c42\u5206\u5c42\u5b89\u5168\u9c81\u68d2\u63a7\u5236\uff0c\u5728\u822a\u5929\u5668\u8fd1\u8ddd\u79bb\u64cd\u4f5c\u4e2d\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u5b89\u5168\u88d5\u5ea6\u548c\u5bf9\u5c40\u90e8\u6781\u5c0f\u503c\u7684\u89c4\u907f\uff0c\u4f18\u4e8e\u4f20\u7edfCBF\u65b9\u6cd5\u3002"}}
{"id": "2507.13539", "categories": ["cs.RO", "cs.NE"], "pdf": "https://arxiv.org/pdf/2507.13539", "abs": "https://arxiv.org/abs/2507.13539", "authors": ["Jim O'Connor", "Jay B. Nash", "Derin Gezgin", "Gary B. Parker"], "title": "SCOPE for Hexapod Gait Generation", "comment": "IJCCI Conference on Evolutionary Computation and Theory and\n  Applications, 2025", "summary": "Evolutionary methods have previously been shown to be an effective learning\nmethod for walking gaits on hexapod robots. However, the ability of these\nalgorithms to evolve an effective policy rapidly degrades as the input space\nbecomes more complex. This degradation is due to the exponential growth of the\nsolution space, resulting from an increasing parameter count to handle a more\ncomplex input. In order to address this challenge, we introduce Sparse Cosine\nOptimized Policy Evolution (SCOPE). SCOPE utilizes the Discrete Cosine\nTransform (DCT) to learn directly from the feature coefficients of an input\nmatrix. By truncating the coefficient matrix returned by the DCT, we can reduce\nthe dimensionality of an input while retaining the highest energy features of\nthe original input. We demonstrate the effectiveness of this method by using\nSCOPE to learn the gait of a hexapod robot. The hexapod controller is given a\nmatrix input containing time-series information of previous poses, which are\nthen transformed to gait parameters by an evolved policy. In this task, the\naddition of SCOPE to a reference algorithm achieves a 20% increase in efficacy.\nSCOPE achieves this result by reducing the total input size of the time-series\npose data from 2700 to 54, a 98% decrease. Additionally, SCOPE is capable of\ncompressing an input to any output shape, provided that each output dimension\nis no greater than the corresponding input dimension. This paper demonstrates\nthat SCOPE is capable of significantly compressing the size of an input to an\nevolved controller, resulting in a statistically significant gain in efficacy.", "AI": {"tldr": "SCOPE\u901a\u8fc7\u5229\u7528\u79bb\u6563\u4f59\u5f26\u53d8\u6362\uff08DCT\uff09\u538b\u7f29\u8f93\u5165\u6570\u636e\uff0c\u63d0\u9ad8\u4e86\u8fdb\u5316\u7b97\u6cd5\u5728\u5904\u7406\u590d\u6742\u8f93\u5165\u65f6\u7684\u5b66\u4e60\u6548\u7387\u548c\u529f\u6548\uff0c\u7279\u522b\u662f\u5728\u516d\u8db3\u673a\u5668\u4eba\u6b65\u6001\u5b66\u4e60\u4efb\u52a1\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "motivation": "\u4e3a\u4e86\u5e94\u5bf9\u8f93\u5165\u7a7a\u95f4\u590d\u6742\u6027\u589e\u52a0\u5bfc\u81f4\u8fdb\u5316\u7b97\u6cd5\u5b66\u4e60\u6548\u7387\u4e0b\u964d\u7684\u6311\u6218\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u4e86SCOPE\u3002", "method": "\u8be5\u65b9\u6cd5\u5229\u7528\u79bb\u6563\u4f59\u5f26\u53d8\u6362\uff08DCT\uff09\u76f4\u63a5\u4ece\u8f93\u5165\u77e9\u9635\u7684\u7279\u5f81\u7cfb\u6570\u4e2d\u5b66\u4e60\u3002\u901a\u8fc7\u622a\u65adDCT\u8fd4\u56de\u7684\u7cfb\u6570\u77e9\u9635\uff0c\u53ef\u4ee5\u5728\u4fdd\u7559\u539f\u59cb\u8f93\u5165\u6700\u9ad8\u80fd\u91cf\u7279\u5f81\u7684\u540c\u65f6\u964d\u4f4e\u8f93\u5165\u7ef4\u5ea6\u3002", "result": "\u5728\u516d\u8db3\u673a\u5668\u4eba\u6b65\u6001\u5b66\u4e60\u4efb\u52a1\u4e2d\uff0cSCOPE\u5c06\u65f6\u95f4\u5e8f\u5217\u59ff\u6001\u6570\u636e\u7684\u603b\u8f93\u5165\u5c3a\u5bf8\u4ece2700\u51cf\u5c11\u523054\uff08\u964d\u4f4e\u4e8698%\uff09\uff0c\u4f7f\u53c2\u8003\u7b97\u6cd5\u7684\u529f\u6548\u63d0\u9ad8\u4e8620%\u3002", "conclusion": "SCOPE\u80fd\u591f\u663e\u8457\u538b\u7f29\u8fdb\u5316\u63a7\u5236\u5668\u8f93\u5165\u7684\u5c3a\u5bf8\uff0c\u4ece\u800c\u5728\u529f\u6548\u65b9\u9762\u83b7\u5f97\u7edf\u8ba1\u5b66\u4e0a\u7684\u663e\u8457\u63d0\u5347\u3002"}}
{"id": "2507.13946", "categories": ["cs.LO"], "pdf": "https://arxiv.org/pdf/2507.13946", "abs": "https://arxiv.org/abs/2507.13946", "authors": ["Tadeusz Litak", "Katsuhiko Sano"], "title": "Bounded Inquisitive Logics: Sequent Calculi and Schematic Validity", "comment": "This is a modified and expanded version of a paper accepted for\n  TABLEAUX 2025. In particular, readers should note that the numeration of\n  environments is different in the conference version", "summary": "Propositional inquisitive logic is the limit of its $n$-bounded\napproximations. In the predicate setting, however, this does not hold anymore,\nas discovered by Ciardelli and Grilletti, who also found complete\naxiomatizations of $n$-bounded inquisitive logics $\\mathsf{InqBQ}_{n}$, for\nevery fixed $n$. We introduce cut-free labelled sequent calculi for these\nlogics. We illustrate the intricacies of \\textit{schematic validity} in such\nsystems by showing that the well-known Casari formula is \\textit{atomically}\nvalid in (a weak sublogic of) predicate inquisitive logic $\\mathsf{InqBQ}$,\nfails to be schematically valid in it, and yet is schematically valid under the\nfinite boundedness assumption. The derivations in our calculi, however, are\nguaranteed to be schematically valid whenever a single specific rule is not\nused.", "AI": {"tldr": "\u8c13\u8bcd\u63a2\u7a76\u903b\u8f91\u7684 $n$ \u6709\u754c\u8fd1\u4f3c\u7684\u6781\u9650\u95ee\u9898\u3002$\nCasari \u516c\u5f0f\u5728\u4e0d\u540c\u6761\u4ef6\u4e0b\u7684\u793a\u610f\u56fe\u6709\u6548\u6027\u3002", "motivation": "\u5728\u8c13\u8bcd\u8bbe\u5b9a\u4e2d\uff0c\u547d\u9898\u63a2\u7a76\u903b\u8f91\u4e0d\u662f\u5176 $n$ \u6709\u754c\u8fd1\u4f3c\u7684\u6781\u9650\uff0c\u6b63\u5982 Ciardelli \u548c Grilletti \u6240\u53d1\u73b0\u7684\u90a3\u6837\uff0c\u4ed6\u4eec\u8fd8\u4e3a\u6bcf\u4e2a\u56fa\u5b9a\u7684 $n$ \u627e\u5230\u4e86 $n$ \u6709\u754c\u63a2\u7a76\u903b\u8f91 $\\mathsf{InqBQ}_{n}$ \u7684\u5b8c\u6574\u516c\u7406\u5316\u3002", "method": "\u6211\u4eec\u4ecb\u7ecd\u4e86\u8fd9\u4e9b\u903b\u8f91\u7684\u65e0\u5272\u6807\u7b7e\u5e8f\u5217\u6f14\u7b97\u3002", "result": "\u5728\u6211\u4eec\u903b\u8f91\u6f14\u7b97\u4e2d\u7684\u63a8\u5bfc\uff0c\u5f53\u67d0\u4e2a\u7279\u5b9a\u89c4\u5219\u4e0d\u88ab\u4f7f\u7528\u65f6\uff0c\u53ef\u4ee5\u4fdd\u8bc1\u662f\u793a\u610f\u56fe\u6709\u6548\u7684\u3002", "conclusion": "\u867d\u7136 Casari \u516c\u5f0f\u5728\uff08\u8c13\u8bcd\u63a2\u7a76\u903b\u8f91 $\\mathsf{InqBQ}$ \u7684\u5f31\u5b50\u903b\u8f91\u7684\uff09\u539f\u5b50\u5c42\u9762\u662f\u6709\u6548\u7684\uff0c\u4f46\u5b83\u5728\u8be5\u903b\u8f91\u4e2d\u5e76\u975e\u793a\u610f\u56fe\u6709\u6548\uff0c\u7136\u800c\u5728\u6709\u9650\u6709\u754c\u5047\u8bbe\u4e0b\u5b83\u662f\u793a\u610f\u56fe\u6709\u6548\u7684\u3002"}}
{"id": "2507.13375", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2507.13375", "abs": "https://arxiv.org/abs/2507.13375", "authors": ["Chunyuan Zhao", "Zizheng Guo", "Zuodong Zhang", "Yibo Lin"], "title": "GAP-LA: GPU-Accelerated Performance-Driven Layer Assignment", "comment": null, "summary": "Layer assignment is critical for global routing of VLSI circuits. It converts\n2D routing paths into 3D routing solutions by determining the proper metal\nlayer for each routing segments to minimize congestion and via count. As\ndifferent layers have different unit resistance and capacitance, layer\nassignment also has significant impacts to timing and power. With growing\ndesign complexity, it becomes increasingly challenging to simultaneously\noptimize timing, power, and congestion efficiently. Existing studies are mostly\nlimited to a subset of objectives. In this paper, we propose a GPU-accelerated\nperformance-driven layer assignment framework, GAP-LA, for holistic\noptimization the aforementioned objectives. Experimental results demonstrate\nthat we can achieve 0.3%-9.9% better worst negative slack (WNS) and 2.0%-5.4%\nbetter total negative slack (TNS) while maintaining power and congestion with\ncompetitive runtime compared with ISPD 2025 contest winners, especially on\ndesigns with up to 12 millions of nets.", "AI": {"tldr": "A GPU-accelerated framework (GAP-LA) for VLSI circuit layer assignment optimizes timing, power, and congestion simultaneously, outperforming existing methods on large designs.", "motivation": "Existing layer assignment studies are mostly limited to a subset of objectives, making it challenging to simultaneously optimize timing, power, and congestion efficiently with growing design complexity.", "method": "The paper proposes a GPU-accelerated performance-driven layer assignment framework, GAP-LA, for holistic optimization of timing, power, and congestion.", "result": "GAP-LA achieves 0.3%-9.9% better worst negative slack (WNS) and 2.0%-5.4% better total negative slack (TNS) while maintaining power and congestion with competitive runtime compared with ISPD 2025 contest winners, especially on designs with up to 12 millions of nets.", "conclusion": "GAP-LA can achieve better WNS and TNS while maintaining power and congestion with competitive runtime compared with ISPD 2025 contest winners, especially on large designs."}}
{"id": "2507.13733", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2507.13733", "abs": "https://arxiv.org/abs/2507.13733", "authors": ["Jun-Jie Zhang", "Ziwen Wang", "Shuai Dong"], "title": "Autferroicity: concept, candidates, and applications", "comment": null, "summary": "Autferroicity is a newly proposed form of hybrid ferroicity, which is a\nsister branch of multiferroicity. It is characterized by the mutually exclusive\nmagnetic and polar phases within a single system, giving a unique seesaw-type\nmagnetoelectric coupling. This perspective provides a theoretical overview of\nits underlying concept, phase diagram characteristics, and representative\ncandidates such as Ti-based trichalcogenide monolayers, while also highlighting\nits potential applications in nonvolatile memory devices and true random number\ngeneration.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.13714", "categories": ["cond-mat.mes-hall", "quant-ph"], "pdf": "https://arxiv.org/pdf/2507.13714", "abs": "https://arxiv.org/abs/2507.13714", "authors": ["Guanchu Chen", "Brendan C. Sheehan", "Ilija Nikolov", "James W. Logan", "Charles A. Collett", "Gajadhar Joshi", "Grigore A. Timco", "Jillian E. Denhardt", "Kevin R. Kittilstved", "Richard E. P. Winpenny", "Jonathan R. Friedman"], "title": "Enhancing Coherence with a Clock Transition and Dynamical Decoupling in the Cr$_7$Mn Molecular Nanomagnet", "comment": "The Main Manuscript has 16 pages and 12 figures. The Supplement has\n  15 pages and 11 figures", "summary": "Molecular magnets are attractive as spin qubits due to their chemical\ntunability, addressability through electron-spin resonance techniques, and long\ncoherence times. Clock transitions (CTs), for which the system is immune to the\neffect of magnetic-field fluctuations to first order, provide a method to\nenhance the coherence time $T_2$, and to reveal mechanisms of decoherence that\nare not due to such fluctuations. Here we investigate two variants of Cr$_7$Mn,\na spin-1 molecular nanomagnet, at fields near a zero-field CT. We find that at\ntemperatures $\\le$2 K, $T_2\\sim1$ $\\mu$s at the CT using a Hahn-echo pulse\nsequence. Away from the CT, electron-spin-echo envelope modulation (ESEEM)\noscillations due to coupling to nuclear spins are observed and have a $T_2$ as\nhigh as $1.35$ $\\mu$s, indicating a distinct mechanism of coherence\npreservation. Dynamical decoupling with the CPMG pulse sequence yields\n$T_2\\sim\\!2.8$ $\\mu$s at the CT and up to $\\sim\\!3.6$ $\\mu$s in the ESEEM\nregime along with a demodulation of the oscillatory behavior. The experimental\nvalues of $T_2$ are largely independent of the degree of dilution of the\nmolecules in solvent or whether the solvent is deuterated, indicating that much\nof the decoherence and ESEEM arises from sources within the molecules\nthemselves. To account for decoherence, we develop a model that includes not\nonly field fluctuations but also fluctuations in the CT transition frequency\nitself. Our results can be well explained by treating the environment as a\ncombination of noise at the nuclear Larmor precession frequency and $1/f$ noise\nin the transverse anisotropy parameter $E$. Such information about the\nmicroscopic origins of decoherence can aid the rational design of\nmolecular-based spin qubits.", "AI": {"tldr": "\u672c\u7814\u7a76\u4f7f\u7528Hahn-echo\u548cCPMG\u8109\u51b2\u5e8f\u5217\u7814\u7a76\u4e86Cr7Mn\u5206\u5b50\u78c1\u4f53\u7684\u76f8\u5e72\u65f6\u95f4T2\uff0c\u53d1\u73b0\u5728\u65f6\u949f\u8dc3\u8fc1\uff08CT\uff09\u5904\u4ee5\u53ca\u7535\u5b50\u81ea\u65cb\u56de\u6ce2\u5305\u8c03\u5236\uff08ESEEM\uff09\u533a\u57df\uff0cT2\u76f8\u5e72\u65f6\u95f4\u5747\u53ef\u8fbe\u5fae\u79d2\u7ea7\u522b\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9000\u76f8\u5e72\u6a21\u578b\uff0c\u8ba4\u4e3a\u9000\u76f8\u5e72\u6765\u6e90\u4e8e\u5206\u5b50\u5185\u90e8\u7684\u591a\u79cd\u566a\u58f0\u6e90\u3002", "motivation": "\u5206\u5b50\u78c1\u4f53\u56e0\u5176\u5316\u5b66\u53ef\u8c03\u6027\u3001\u53ef\u901a\u8fc7\u7535\u5b50\u81ea\u65cb\u5171\u632f\u6280\u672f\u5bfb\u5740\u4ee5\u53ca\u957f\u76f8\u5e72\u65f6\u95f4\u800c\u88ab\u7528\u4f5c\u81ea\u65cb\u91cf\u5b50\u6bd4\u7279\u3002\u65f6\u949f\u8dc3\u8fc1\uff08CT\uff09\u53ef\u4ee5\u63d0\u9ad8\u76f8\u5e72\u65f6\u95f4T2\uff0c\u5e76\u63ed\u793a\u975e\u78c1\u573a\u6ce2\u52a8\u5f15\u8d77\u7684\u9000\u76f8\u5e72\u673a\u5236\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22Cr7Mn\u5206\u5b50\u78c1\u4f53\u7684\u76f8\u5e72\u65f6\u95f4\uff0c\u5e76\u63a2\u7a76\u9000\u76f8\u5e72\u7684\u5fae\u89c2\u673a\u5236\u3002", "method": "\u672c\u7814\u7a76\u4f7f\u7528Hahn-echo\u8109\u51b2\u5e8f\u5217\u548cCPMG\u8109\u51b2\u5e8f\u5217\u7814\u7a76\u4e86\u4e24\u79cdCr7Mn\uff08\u4e00\u79cd\u81ea\u65cb1\u5206\u5b50\u7eb3\u7c73\u78c1\u4f53\uff09\u5728\u96f6\u573a\u65f6\u949f\u8dc3\u8fc1\u9644\u8fd1\u7684\u6027\u8d28\u3002\u901a\u8fc7\u6d4b\u91cfT2\u76f8\u5e72\u65f6\u95f4\uff0c\u5e76\u5206\u6790\u4e86ESEEM\uff08\u7535\u5b50\u81ea\u65cb\u56de\u6ce2\u5305\u8c03\u5236\uff09\u73b0\u8c61\uff0c\u4ee5\u63a2\u7a76\u9000\u76f8\u5e72\u673a\u5236\u3002", "result": "\u5728CT\uff08\u65f6\u949f\u8dc3\u8fc1\uff09\u5904\uff0cHahn-echo\u8109\u51b2\u5e8f\u5217\u5f97\u5230\u7684T2\u76f8\u5e72\u65f6\u95f4\u7ea6\u4e3a1\u03bcs\u3002\u5728CT\u4e4b\u5916\u7684ESEEM\uff08\u7535\u5b50\u81ea\u65cb\u56de\u6ce2\u5305\u8c03\u5236\uff09\u533a\u57df\uff0cT2\u76f8\u5e72\u65f6\u95f4\u9ad8\u8fbe1.35\u03bcs\u3002\u4f7f\u7528CPMG\u8109\u51b2\u5e8f\u5217\uff0c\u5728CT\u5904T2\u63d0\u9ad8\u5230\u7ea62.8\u03bcs\uff0c\u5728ESEEM\u533a\u57df\u53ef\u8fbe3.6\u03bcs\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cT2\u76f8\u5e72\u65f6\u95f4\u4e0e\u5206\u5b50\u5728\u6eb6\u5242\u4e2d\u7684\u7a00\u91ca\u7a0b\u5ea6\u4ee5\u53ca\u6eb6\u5242\u662f\u5426\u6c18\u4ee3\u5173\u7cfb\u4e0d\u5927\uff0c\u8fd9\u8868\u660e\u5927\u90e8\u5206\u9000\u76f8\u5e72\u548cESEEM\u6765\u6e90\u4e8e\u5206\u5b50\u5185\u90e8\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u6a21\u578b\u80fd\u5f88\u597d\u5730\u89e3\u91ca\u5b9e\u9a8c\u7ed3\u679c\uff0c\u8be5\u6a21\u578b\u8003\u8651\u4e86\u573a\u6ce2\u52a8\u548c\u65f6\u949f\u8dc3\u8fc1\u9891\u7387\u672c\u8eab\u7684\u6ce2\u52a8\uff0c\u5e76\u5c06\u73af\u5883\u89c6\u4e3a\u6838\u62c9\u83ab\u5c14\u8fdb\u52a8\u9891\u7387\u7684\u566a\u58f0\u548c\u6a2a\u5411\u9000\u5316\u53c2\u6570E\u76841/f\u566a\u58f0\u7684\u7ec4\u5408\u3002\u8fd9\u4e9b\u5173\u4e8e\u9000\u76f8\u5e72\u5fae\u89c2\u8d77\u6e90\u7684\u4fe1\u606f\u6709\u52a9\u4e8e\u5206\u5b50\u57fa\u81ea\u65cb\u91cf\u5b50\u6bd4\u7279\u7684\u5408\u7406\u8bbe\u8ba1\u3002"}}
{"id": "2507.14116", "categories": ["quant-ph", "cs.ET", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14116", "abs": "https://arxiv.org/abs/2507.14116", "authors": ["Dani\u00eblle Schuman", "Mark V. Seebode", "Tobias Rohe", "Maximilian Balthasar Mansky", "Michael Schroedl-Baumann", "Jonas Stein", "Claudia Linnhoff-Popien", "Florian Krellner"], "title": "Quantum Boltzmann Machines using Parallel Annealing for Medical Image Classification", "comment": "12 pages, 5 figures (10 if counting subfigures), 2 tables. To be\n  published in the proceedings of the 2025 IEEE International Conference on\n  Quantum Computing and Engineering (QCE)", "summary": "Exploiting the fact that samples drawn from a quantum annealer inherently\nfollow a Boltzmann-like distribution, annealing-based Quantum Boltzmann\nMachines (QBMs) have gained increasing popularity in the quantum research\ncommunity. While they harbor great promises for quantum speed-up, their usage\ncurrently stays a costly endeavor, as large amounts of QPU time are required to\ntrain them. This limits their applicability in the NISQ era. Following the idea\nof No\\`e et al. (2024), who tried to alleviate this cost by incorporating\nparallel quantum annealing into their unsupervised training of QBMs, this paper\npresents an improved version of parallel quantum annealing that we employ to\ntrain QBMs in a supervised setting. Saving qubits to encode the inputs, the\nlatter setting allows us to test our approach on medical images from the\nMedMNIST data set (Yang et al., 2023), thereby moving closer to real-world\napplicability of the technology. Our experiments show that QBMs using our\napproach already achieve reasonable results, comparable to those of\nsimilarly-sized Convolutional Neural Networks (CNNs), with markedly smaller\nnumbers of epochs than these classical models. Our parallel annealing technique\nleads to a speed-up of almost 70 % compared to regular annealing-based BM\nexecutions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u5e76\u884c\u91cf\u5b50\u9000\u706b\u65b9\u6cd5\uff0c\u7528\u4e8e\u76d1\u7763\u5b66\u4e60\u573a\u666f\u4e0b\u7684QBM\u8bad\u7ec3\uff0c\u5b9e\u73b0\u4e86\u8fd170%\u7684\u52a0\u901f\uff0c\u5e76\u53d6\u5f97\u4e86\u4e0eCNN\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u4e3aQBM\u5728NISQ\u65f6\u4ee3\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002", "motivation": "\u5f53\u524d\u7684\u91cf\u5b50\u9000\u706b\u65b9\u6cd5\u8bad\u7ec3QBM\u6210\u672c\u9ad8\u6602\uff0c\u9700\u8981\u5927\u91cf\u91cf\u5b50\u5904\u7406\u5355\u5143\uff08QPU\uff09\u65f6\u95f4\uff0c\u9650\u5236\u4e86\u5176\u5728NISQ\u65f6\u4ee3\u7684\u5e7f\u6cdb\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u5e76\u884c\u91cf\u5b50\u9000\u706b\u6280\u672f\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u76d1\u7763\u5b66\u4e60\u573a\u666f\u4e0b\u7684\u91cf\u5b50\u73bb\u5c14\u5179\u66fc\u673a\uff08QBM\uff09\u8bad\u7ec3\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728MedMNIST\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4e0e\u76f8\u4f3c\u5927\u5c0f\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u8bad\u7ec3\u8f6e\u6b21\u663e\u8457\u51cf\u5c11\u3002\u4e0e\u5e38\u89c4\u9000\u706b\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5e76\u884c\u9000\u706b\u6280\u672f\u5b9e\u73b0\u4e86\u8fd170%\u7684\u52a0\u901f\u3002", "conclusion": "\u901a\u8fc7\u63d0\u51fa\u4e00\u79cd\u6539\u8fdb\u7684\u5e76\u884c\u91cf\u5b50\u9000\u706b\u6280\u672f\uff0c\u5e76\u5728\u76d1\u7763\u8bbe\u7f6e\u4e0b\u8bad\u7ec3\u91cf\u5b50\u73bb\u5c14\u5179\u66fc\u673a\uff08QBM\uff09\uff0c\u6211\u4eec\u5b9e\u73b0\u4e86\u8fd170%\u7684\u52a0\u901f\uff0c\u5e76\u53d6\u5f97\u4e86\u4e0eCNN\u76f8\u5f53\u7684\u7ed3\u679c\uff0c\u540c\u65f6\u6240\u9700\u8bad\u7ec3\u8f6e\u6b21\u66f4\u5c11\uff0c\u4ece\u800c\u4e3aQBM\u5728NISQ\u65f6\u4ee3\u7684\u5b9e\u9645\u5e94\u7528\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2507.13700", "categories": ["cs.DS", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13700", "abs": "https://arxiv.org/abs/2507.13700", "authors": ["Emma Rapoport", "Edith Cohen", "Uri Stemmer"], "title": "Tight Bounds for Answering Adaptively Chosen Concentrated Queries", "comment": null, "summary": "Most work on adaptive data analysis assumes that samples in the dataset are\nindependent. When correlations are allowed, even the non-adaptive setting can\nbecome intractable, unless some structural constraints are imposed. To address\nthis, Bassily and Freund [2016] introduced the elegant framework of\nconcentrated queries, which requires the analyst to restrict itself to queries\nthat are concentrated around their expected value. While this assumption makes\nthe problem trivial in the non-adaptive setting, in the adaptive setting it\nremains quite challenging. In fact, all known algorithms in this framework\nsupport significantly fewer queries than in the independent case: At most\n$O(n)$ queries for a sample of size $n$, compared to $O(n^2)$ in the\nindependent setting.\n  In this work, we prove that this utility gap is inherent under the current\nformulation of the concentrated queries framework, assuming some natural\nconditions on the algorithm. Additionally, we present a simplified version of\nthe best-known algorithms that match our impossibility result.", "AI": {"tldr": "\u5728\u81ea\u9002\u5e94\u6570\u636e\u5206\u6790\u4e2d\uff0c\u5373\u4f7f\u6709\u6d53\u7f29\u67e5\u8be2\u7684\u9650\u5236\uff0c\u67e5\u8be2\u6570\u91cf\u4e5f\u6bd4\u72ec\u7acb\u6837\u672c\u60c5\u51b5\u5c11\u5f97\u591a\u3002\u672c\u7814\u7a76\u8bc1\u660e\u4e86\u8fd9\u79cd\u5dee\u8ddd\u662f\u56fa\u6709\u7684\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5339\u914d\u8be5\u4e0a\u9650\u7684\u7b80\u5316\u7b97\u6cd5\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u6570\u636e\u96c6\u4e2d\u6837\u672c\u76f8\u5173\u65f6\uff0c\u5373\u4f7f\u5728\u975e\u9002\u5e94\u6027\u8bbe\u7f6e\u4e0b\uff0c\u5982\u679c\u6ca1\u6709\u7ed3\u6784\u6027\u7ea6\u675f\u4e5f\u4f1a\u53d8\u5f97\u96be\u4ee5\u5904\u7406\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u5339\u914d\u8be5\uff08\u67e5\u8be2\u6570\u91cf\u9650\u5236\uff09\u5960\u57fa\u6027\u7ed3\u679c\u7684\u7b80\u5316\u7b97\u6cd5\u3002", "result": "\u8bc1\u660e\u4e86\u5728\u6d53\u7f29\u67e5\u8be2\u6846\u67b6\u7684\u5f53\u524d\u8868\u8ff0\u4e0b\uff0c\u67e5\u8be2\u6570\u91cf\u7684\u9650\u5236\u662f\u56fa\u6709\u7684\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u5728\u6d53\u7f29\u67e5\u8be2\u6846\u67b6\u7684\u5f53\u524d\u8868\u8ff0\u4e0b\uff0c\u5728\u4e00\u4e9b\u81ea\u7136\u6761\u4ef6\u4e0b\uff0c\u67e5\u8be2\u6570\u91cf\u7684\u9650\u5236\u662f\u56fa\u6709\u7684\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u5339\u914d\u8be5\u9650\u5236\u7684\u7b80\u5316\u7b97\u6cd5\u3002"}}
{"id": "2507.13382", "categories": ["cs.CL", "cs.LG", "05-05C12"], "pdf": "https://arxiv.org/pdf/2507.13382", "abs": "https://arxiv.org/abs/2507.13382", "authors": ["Chandrashekar Muniyappa", "Sirisha Velampalli"], "title": "Context-Based Fake News Detection using Graph Based Approach: ACOVID-19 Use-case", "comment": "CSAIDE '25: Proceedings of the 2025 4th International Conference on\n  Cyber Security, Artificial Intelligence and the Digital Economy", "summary": "In today\\'s digital world, fake news is spreading with immense speed. Its a\nsignificant concern to address. In this work, we addressed that challenge using\nnovel graph based approach. We took dataset from Kaggle that contains real and\nfake news articles. To test our approach we incorporated recent covid-19\nrelated news articles that contains both genuine and fake news that are\nrelevant to this problem. This further enhances the dataset as well instead of\nrelying completely on the original dataset. We propose a contextual graph-based\napproach to detect fake news articles. We need to convert news articles into\nappropriate schema, so we leverage Natural Language Processing (NLP) techniques\nto transform news articles into contextual graph structures. We then apply the\nMinimum Description Length (MDL)-based Graph-Based Anomaly Detection (GBAD)\nalgorithm for graph mining. Graph-based methods are particularly effective for\nhandling rich contextual data, as they enable the discovery of complex patterns\nthat traditional query-based or statistical techniques might overlook. Our\nproposed approach identifies normative patterns within the dataset and\nsubsequently uncovers anomalous patterns that deviate from these established\nnorms.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u56fe\u7684fake news\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5229\u7528NLP\u5c06\u65b0\u95fb\u8f6c\u6362\u4e3a\u56fe\uff0c\u5e76\u4f7f\u7528MDL-based GBAD\u7b97\u6cd5\u6765\u8bc6\u522b\u5f02\u5e38\u6a21\u5f0f\u3002", "motivation": "fake news\u5728\u6570\u5b57\u4e16\u754c\u4e2d\u8fc5\u901f\u4f20\u64ad\uff0c\u5df2\u6210\u4e3a\u4e00\u4e2a\u4e25\u5cfb\u7684\u95ee\u9898\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e0a\u4e0b\u6587\u56fe\u7684\u65b9\u6cd5\u6765\u68c0\u6d4bfake news\u3002\u9996\u5148\uff0c\u5229\u7528\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u6280\u672f\u5c06\u65b0\u95fb\u6587\u7ae0\u8f6c\u6362\u4e3a\u4e0a\u4e0b\u6587\u56fe\u7ed3\u6784\u3002\u7136\u540e\uff0c\u5e94\u7528\u57fa\u4e8e\u6700\u5c0f\u63cf\u8ff0\u957f\u5ea6\uff08MDL\uff09\u7684\u56fe\u57fa\u5f02\u5e38\u68c0\u6d4b\uff08GBAD\uff09\u7b97\u6cd5\u8fdb\u884c\u56fe\u6316\u6398\uff0c\u4ee5\u8bc6\u522b\u51fa\u4e0e\u6b63\u5e38\u6a21\u5f0f\u76f8\u6096\u7684\u5f02\u5e38\u6a21\u5f0f\u3002", "result": "\u901a\u8fc7\u5c06\u65b0\u95fb\u6587\u7ae0\u8f6c\u6362\u4e3a\u4e0a\u4e0b\u6587\u56fe\u5e76\u5e94\u7528MDL-based GBAD\u7b97\u6cd5\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684fake news\u68c0\u6d4b\u65b9\u6cd5\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e0a\u4e0b\u6587\u56fe\u7684fake news\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u65b0\u95fb\u6587\u7ae0\u8f6c\u6362\u4e3a\u56fe\u7ed3\u6784\u5e76\u5e94\u7528\u57fa\u4e8eMDL\u7684GBAD\u7b97\u6cd5\uff0c\u6709\u6548\u8bc6\u522b\u51fa\u4e0e\u6b63\u5e38\u6a21\u5f0f\u76f8\u6096\u7684\u5f02\u5e38\u6a21\u5f0f\uff0c\u4ece\u800c\u68c0\u6d4bfake news\u3002"}}
{"id": "2507.13444", "categories": ["quant-ph", "cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2507.13444", "abs": "https://arxiv.org/abs/2507.13444", "authors": ["Enrico Di Benedetto", "Xuejian Sun", "Marcel A. Pinto", "Luca Leonforte", "Chih-Ying Chang", "Vincent Jouanny", "L\u00e9o Peyruchat", "Pasquale Scarlino", "Francesco Ciccarello"], "title": "Emergent cavity-QED dynamics along the edge of a photonic lattice", "comment": "22 pages, 4 figures", "summary": "We investigate qubits coupled to the boundary of a two dimensional photonic\nlattice that supports dispersionless edge modes, unlike conventional edge modes\nthat sustain propagating photons. As a case study, we consider a honeycomb\nlattice (photonic graphene) of coupled resonators with a zigzag edge, where the\nedge modes form a flat band defined only over a restricted region of momentum\nspace. We show that light matter interactions are effectively captured by a\ndissipative cavity QED model, wherein the emitter coherently couples to a\nfictitious cavity mode emerging as a superposition of edge modes. This mode has\nsupport on only one sublattice and, most notably, displays an unconventional\npower law localization around the qubit, yet remaining normalizable in the\nthermodynamic limit, with a spatial range that can be tuned by introducing\nlattice anisotropy We predict occurrence of vacuum Rabi oscillations and\nefficient state transfer between distant emitters. An experimental\ndemonstration using superconducting circuits is proposed.", "AI": {"tldr": "This paper studies qubits on the edge of a 2D photonic lattice with unique flat band edge modes. It models light-matter interactions using a cavity QED approach, revealing a special mode that localizes unconventionally around the qubit. The study predicts phenomena like vacuum Rabi oscillations and efficient state transfer, and suggests a superconducting circuit experiment.", "motivation": "To investigate qubits coupled to the boundary of a two-dimensional photonic lattice that supports dispersionless edge modes, exploring their properties and potential applications in light-matter interactions.", "method": "Investigating qubits coupled to the boundary of a two-dimensional photonic lattice with dispersionless edge modes, using a honeycomb lattice as a case study. Light-matter interactions are captured by a dissipative cavity QED model where the emitter couples to a fictitious cavity mode emerging from edge modes. This mode exhibits unconventional power-law localization around the qubit, tunable by lattice anisotropy.", "result": "A fictitious cavity mode emerging from edge modes is identified, which exhibits unconventional power-law localization around the qubit and is tunable by lattice anisotropy. Vacuum Rabi oscillations and efficient state transfer between distant emitters are predicted.", "conclusion": "The paper predicts vacuum Rabi oscillations and efficient state transfer between distant emitters, with a proposal for experimental demonstration using superconducting circuits."}}
{"id": "2507.14011", "categories": ["cs.NE", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14011", "abs": "https://arxiv.org/abs/2507.14011", "authors": ["Paolo Totaro", "Alberto Mangiante"], "title": "Conceptual and Design Principles for a Self-Referential Algorithm Mimicking Neuronal Assembly Functions", "comment": null, "summary": "This article proposes a method to formalise models of cognitive processes\ngrounded in experience, considering experience from the perspective of a living\nsystem and not from that of an observer of the living system. The perspective\nof a living system is defined by the need of the system to preserve the vital\nequilibria. The method is based on an algorithmic schema that we call\nEnvironment Generative Operator (EGO) and uses a self-referential language\ndeveloped for this purpose which we call E-language. EGO simulates cognitive\nprocesses as operations on neuron assemblies as understood by Hebb. In this\narticle we present an EGO prototype (EGO-P) which has already been implemented\nand tested.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8ba4\u77e5\u5efa\u6a21\u65b9\u6cd5\uff0c\u4f7f\u7528EGO\u7b97\u6cd5\u548cE\u8bed\u8a00\uff0c\u4ece\u751f\u547d\u7cfb\u7edf\u5185\u90e8\u89c6\u89d2\u6a21\u62df\u8ba4\u77e5\u8fc7\u7a0b\u3002", "motivation": "\u672c\u7814\u7a76\u7684\u52a8\u673a\u662f\u63d0\u4f9b\u4e00\u79cd\u5f62\u5f0f\u5316\u65b9\u6cd5\u6765\u6a21\u62df\u8ba4\u77e5\u8fc7\u7a0b\uff0c\u8be5\u65b9\u6cd5\u5c06\u7ecf\u9a8c\u89c6\u4e3a\u751f\u547d\u7cfb\u7edf\u7ef4\u6301\u5176\u751f\u5b58\u5e73\u8861\u7684\u5185\u5728\u9700\u6c42\uff0c\u800c\u975e\u5916\u90e8\u89c2\u5bdf\u8005\u7684\u89c6\u89d2\u3002", "method": "\u8be5\u7814\u7a76\u57fa\u4e8e\u4e00\u4e2a\u540d\u4e3a\u73af\u5883\u751f\u6210\u7b97\u5b50\uff08EGO\uff09\u7684\u7b97\u6cd5\u6846\u67b6\uff0c\u5e76\u4f7f\u7528\u4e00\u79cd\u540d\u4e3aE\u8bed\u8a00\u7684\u81ea\u6307\u8bed\u8a00\u3002EGO\u6a21\u62df\u8ba4\u77e5\u8fc7\u7a0b\uff0c\u5c06\u5176\u89c6\u4e3a\u5728Hebb\u6240\u7406\u89e3\u7684\u795e\u7ecf\u5143\u7ec4\u5408\u4e0a\u7684\u64cd\u4f5c\u3002\u7814\u7a76\u4e2d\u5b9e\u73b0\u5e76\u6d4b\u8bd5\u4e86\u4e00\u4e2aEGO\u539f\u578b\uff08EGO-P\uff09\u3002", "result": "\u7814\u7a76\u5b9e\u73b0\u5e76\u6d4b\u8bd5\u4e86\u4e00\u4e2aEGO\u539f\u578b\uff08EGO-P\uff09\uff0c\u8be5\u539f\u578b\u6a21\u62df\u4e86\u57fa\u4e8eHebb\u7406\u8bba\u7684\u795e\u7ecf\u5143\u7ec4\u5408\u64cd\u4f5c\uff0c\u4ee5\u5f62\u5f0f\u5316\u751f\u547d\u7cfb\u7edf\u7684\u7ecf\u9a8c\u8ba4\u77e5\u8fc7\u7a0b\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a\u73af\u5883\u751f\u6210\u7b97\u5b50\uff08EGO\uff09\u7684\u7b97\u6cd5\u6846\u67b6\uff0c\u5e76\u8f85\u4ee5\u4e00\u79cd\u540d\u4e3aE\u8bed\u8a00\u7684\u81ea\u6307\u8bed\u8a00\uff0c\u7528\u4e8e\u5f62\u5f0f\u5316\u8ba4\u77e5\u8fc7\u7a0b\u6a21\u578b\u3002\u8be5\u6a21\u578b\u4ece\u751f\u547d\u7cfb\u7edf\u7684\u89d2\u5ea6\u51fa\u53d1\uff0c\u5f3a\u8c03\u7ef4\u6301\u751f\u547d\u5e73\u8861\u7684\u5185\u5728\u9700\u6c42\uff0c\u5e76\u5c06\u8ba4\u77e5\u8fc7\u7a0b\u6a21\u62df\u4e3a\u57fa\u4e8eHebb\u7406\u8bba\u7684\u795e\u7ecf\u5143\u7ec4\u5408\u64cd\u4f5c\u3002"}}
{"id": "2507.13399", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13399", "abs": "https://arxiv.org/abs/2507.13399", "authors": ["Mert Sehri", "Zehui Hua", "Francisco de Assis Boldt", "Patrick Dumond"], "title": "Selective Embedding for Deep Learning", "comment": null, "summary": "Deep learning has revolutionized many industries by enabling models to\nautomatically learn complex patterns from raw data, reducing dependence on\nmanual feature engineering. However, deep learning algorithms are sensitive to\ninput data, and performance often deteriorates under nonstationary conditions\nand across dissimilar domains, especially when using time-domain data.\nConventional single-channel or parallel multi-source data loading strategies\neither limit generalization or increase computational costs. This study\nintroduces selective embedding, a novel data loading strategy, which alternates\nshort segments of data from multiple sources within a single input channel.\nDrawing inspiration from cognitive psychology, selective embedding mimics\nhuman-like information processing to reduce model overfitting, enhance\ngeneralization, and improve computational efficiency. Validation is conducted\nusing six time-domain datasets, demonstrating that the proposed method\nconsistently achieves high classification accuracy across various deep learning\narchitectures while significantly reducing training times. The approach proves\nparticularly effective for complex systems with multiple data sources, offering\na scalable and resource-efficient solution for real-world applications in\nhealthcare, heavy machinery, marine, railway, and agriculture, where robustness\nand adaptability are critical.", "AI": {"tldr": "\u9009\u62e9\u6027\u5d4c\u5165\u662f\u4e00\u79cd\u65b0\u9896\u7684\u6570\u636e\u52a0\u8f7d\u7b56\u7565\uff0c\u901a\u8fc7\u6a21\u4eff\u4eba\u7c7b\u4fe1\u606f\u5904\u7406\u6765\u63d0\u9ad8\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6548\u7387\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u975e\u5e73\u7a33\u6761\u4ef6\u548c\u4e0d\u540c\u9886\u57df\u4e0b\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u4ee5\u53ca\u4f20\u7edf\u6570\u636e\u52a0\u8f7d\u7b56\u7565\u7684\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u9009\u62e9\u6027\u5d4c\u5165\u7684\u65b0\u9896\u6570\u636e\u52a0\u8f7d\u7b56\u7565\uff0c\u901a\u8fc7\u5728\u5355\u4e2a\u8f93\u5165\u901a\u9053\u4e2d\u4ea4\u66ff\u6765\u81ea\u591a\u4e2a\u6e90\u7684\u77ed\u6570\u636e\u7247\u6bb5\u6765\u6a21\u4eff\u4eba\u7c7b\u4fe1\u606f\u5904\u7406\u3002", "result": "\u5728\u516d\u4e2a\u65f6\u57df\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u5404\u79cd\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u4e2d\u59cb\u7ec8\u5982\u4e00\u5730\u5b9e\u73b0\u4e86\u9ad8\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e86\u8bad\u7ec3\u65f6\u95f4\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u590d\u6742\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u8d44\u6e90\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u533b\u7597\u4fdd\u5065\u3001\u91cd\u578b\u673a\u68b0\u3001\u6d77\u8fd0\u3001\u94c1\u8def\u548c\u519c\u4e1a\u7b49\u9886\u57df\u3002"}}
{"id": "2507.13379", "categories": ["cs.SI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.13379", "abs": "https://arxiv.org/abs/2507.13379", "authors": ["Niccol\u00f2 Di Marco", "Anita Bonetti", "Edoardo Di Martino", "Edoardo Loru", "Jacopo Nudo", "Mario Edoardo Pandolfo", "Giulio Pecile", "Emanuele Sangiorgio", "Irene Scalco", "Simon Zollo", "Matteo Cinelli", "Fabiana Zollo", "Walter Quattrociocchi"], "title": "Patterns, Models, and Challenges in Online Social Media: A Survey", "comment": null, "summary": "The rise of digital platforms has enabled the large scale observation of\nindividual and collective behavior through high resolution interaction data.\nThis development has opened new analytical pathways for investigating how\ninformation circulates, how opinions evolve, and how coordination emerges in\nonline environments. Yet despite a growing body of research, the field remains\nfragmented and marked by methodological heterogeneity, limited model\nvalidation, and weak integration across domains. This survey offers a\nsystematic synthesis of empirical findings and formal models. We examine\nplatform-level regularities, assess the methodological architectures that\ngenerate them, and evaluate the extent to which current modeling frameworks\naccount for observed dynamics. The goal is to consolidate a shared empirical\nbaseline and clarify the structural constraints that shape inference in this\ndomain, laying the groundwork for more robust, comparable, and actionable\nanalyses of online social systems.", "AI": {"tldr": "This survey synthesizes research on digital platforms, highlighting fragmentation and methodological issues. It aims to create a unified foundation for analyzing online social systems by examining empirical data, methods, and models to enable more robust and comparable studies.", "motivation": "To address the fragmentation, methodological heterogeneity, limited model validation, and weak integration across domains in the study of digital platforms and online social systems.", "method": "Systematic synthesis of empirical findings and formal models, examining platform-level regularities, assessing methodological architectures, and evaluating current modeling frameworks.", "result": "A clearer understanding of platform-level regularities, methodological architectures, and the explanatory power of current modeling frameworks, aiming to lay the groundwork for more robust analyses.", "conclusion": "The survey aims to consolidate a shared empirical baseline and clarify structural constraints for robust, comparable, and actionable analyses of online social systems."}}
{"id": "2507.13554", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.13554", "abs": "https://arxiv.org/abs/2507.13554", "authors": ["Meles Weldegebriel", "Zihan Li", "Dustin Maas", "Greg Hellbourg", "Ning Zhang", "Neal Patwari"], "title": "Sensing and Stopping Interfering Secondary Users: Validation of an Efficient Spectrum Sharing System", "comment": null, "summary": "We present the design and validation of Stoppable Secondary Use (StopSec), a\nprivacy-preserving protocol with the capability to identify a secondary user\n(SU) causing interference to a primary user (PU) and to act quickly to stop the\ninterference. All users are served by a database that provides a feedback\nmechanism from a PU to an interfering SU. We introduce a new lightweight and\nrobust method to watermark an SU's OFDM packet. Through extensive over-the-air\nreal-time experiments, we evaluate StopSec in terms of interference detection,\nidentification, and stopping latency, as well as impact on SUs. We show that\nthe watermarking method avoids negative impact to the secondary data link and\nis robust to real-world time-varying channels. Interfering SUs can be stopped\nin under 150 milliseconds, and when multiple users are simultaneously\ninterfering, they can all be stopped. Even when the interference is 10 dB lower\nthan the noise power, StopSec successfully stops interfering SUs within a few\nseconds of their appearance in the channel. StopSec can be an effective\nspectrum sharing protocol for cases when interference to a PU must be quickly\nand automatically stopped.", "AI": {"tldr": "StopSec\u662f\u4e00\u79cd\u521b\u65b0\u7684\u9690\u79c1\u4fdd\u62a4\u534f\u8bae\uff0c\u901a\u8fc7\u72ec\u7279\u7684\u6c34\u5370\u6280\u672f\uff0c\u80fd\u591f\u5feb\u901f\u3001\u51c6\u786e\u5730\u8bc6\u522b\u5e76\u505c\u6b62\u5bf9\u4e3b\u8981\u7528\u6237\uff08PU\uff09\u9020\u6210\u5e72\u6270\u7684\u4e8c\u6b21\u7528\u6237\uff08SU\uff09\uff0c\u5728\u5b9e\u9645\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5373\u4f7f\u5728\u5e72\u6270\u4fe1\u53f7\u5fae\u5f31\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u6709\u6548\u5de5\u4f5c\u3002", "motivation": "\u63d0\u51fa\u4e00\u79cd\u80fd\u8bc6\u522b\u5e76\u5feb\u901f\u505c\u6b62\u5e72\u6270PU\u7684\u4e8c\u6b21\u7528\u6237\uff08SU\uff09\u7684\u9690\u79c1\u4fdd\u62a4\u534f\u8bae\u3002", "method": "StopSec\u534f\u8bae\u8bbe\u8ba1\uff0c\u5f15\u5165\u4e86\u8f7b\u91cf\u7ea7\u4e14\u9c81\u68d2\u7684SU OFDM\u6570\u636e\u5305\u6c34\u5370\u65b9\u6cd5\u3002", "result": "StopSec\u80fd\u5728150\u6beb\u79d2\u5185\u505c\u6b62\u5e72\u6270SU\uff0c\u80fd\u591f\u540c\u65f6\u505c\u6b62\u591a\u4e2a\u5e72\u6270\u7528\u6237\uff0c\u5373\u4f7f\u5728\u5e72\u6270\u6bd4\u566a\u58f0\u529f\u7387\u4f4e10dB\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u5728\u51e0\u79d2\u949f\u5185\u6210\u529f\u505c\u6b62\u5e72\u6270\u3002", "conclusion": "StopSec\u53ef\u4ee5\u4f5c\u4e3a\u4e00\u79cd\u6709\u6548\u7684\u9891\u8c31\u5171\u4eab\u534f\u8bae\uff0c\u9002\u7528\u4e8e\u5fc5\u987b\u5feb\u901f\u81ea\u52a8\u505c\u6b62\u5bf9PU\u5e72\u6270\u7684\u60c5\u51b5\u3002"}}
{"id": "2507.13362", "categories": ["cs.CV", "cs.AI", "cs.CL", "I.2.10; I.4.8; I.2.6; I.2.7; I.5.4; I.5.1"], "pdf": "https://arxiv.org/pdf/2507.13362", "abs": "https://arxiv.org/abs/2507.13362", "authors": ["Binbin Ji", "Siddharth Agrawal", "Qiance Tang", "Yvonne Wu"], "title": "Enhancing Spatial Reasoning in Vision-Language Models via Chain-of-Thought Prompting and Reinforcement Learning", "comment": "10 pages, 5 figures, submitted to a conference (IEEE formate).\n  Authored by students from the Courant Institute, NYU", "summary": "This study investigates the spatial reasoning capabilities of vision-language\nmodels (VLMs) through Chain-of-Thought (CoT) prompting and reinforcement\nlearning. We begin by evaluating the impact of different prompting strategies\nand find that simple CoT formats, where the model generates a reasoning step\nbefore the answer, not only fail to help, but can even harm the model's\noriginal performance. In contrast, structured multi-stage prompting based on\nscene graphs (SceneGraph CoT) significantly improves spatial reasoning\naccuracy. Furthermore, to improve spatial reasoning ability, we fine-tune\nmodels using Group Relative Policy Optimization (GRPO) on the SAT dataset and\nevaluate their performance on CVBench. Compared to supervised fine-tuning\n(SFT), GRPO achieves higher accuracy on Pass@1 evaluations and demonstrates\nsuperior robustness under out-of-distribution (OOD) conditions. In particular,\nwe find that SFT overfits to surface-level linguistic patterns and may degrade\nperformance when test-time phrasing changes (e.g., from \"closer to\" to \"farther\nfrom\"). GRPO, on the other hand, generalizes more reliably and maintains stable\nperformance under such shifts. Our findings provide insights into how\nreinforcement learning and structured prompting improve the spatial reasoning\ncapabilities and generalization behavior of modern VLMs. All code is open\nsource at: https://github.com/Yvonne511/spatial-vlm-investigator", "AI": {"tldr": "\u901a\u8fc7\u57fa\u4e8e\u573a\u666f\u56fe\u7684CoT\u63d0\u793a\u548cGRPO\u5fae\u8c03\uff0c\u63d0\u5347\u4e86VLMs\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u89e3\u51b3\u4e86SFT\u6613\u8fc7\u62df\u5408\u548c\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7a76\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\u7684\u80fd\u529b\uff0c\u5e76\u5bfb\u627e\u63d0\u9ad8\u5176\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u7684\u65b9\u6cd5\u3002\u7814\u7a76\u8005\u5e0c\u671b\u901a\u8fc7\u94fe\u5f0f\u601d\u8003\uff08CoT\uff09\u63d0\u793a\u548c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u6765\u589e\u5f3aVLMs\u5904\u7406\u7a7a\u95f4\u5173\u7cfb\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u89e3\u51b3\u6a21\u578b\u5728\u9762\u5bf9\u4e0d\u540c\u8868\u8ff0\u65b9\u5f0f\u65f6\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\u3002", "method": "\u672c\u7814\u7a76\u9996\u5148\u8bc4\u4f30\u4e86\u4e0d\u540c\u63d0\u793a\u7b56\u7565\u5bf9VLMs\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u7684\u5f71\u54cd\uff0c\u7279\u522b\u662fCoT\u63d0\u793a\u548c\u57fa\u4e8e\u573a\u666f\u56fe\u7684CoT\u63d0\u793a\u3002\u968f\u540e\uff0c\u7814\u7a76\u4eba\u5458\u4f7f\u7528SAT\u6570\u636e\u96c6\u4e0a\u7684Group Relative Policy Optimization\uff08GRPO\uff09\u5bf9\u6a21\u578b\u8fdb\u884c\u4e86\u5fae\u8c03\uff0c\u5e76\u4f7f\u7528CVBench\u8bc4\u4f30\u4e86\u5176\u6027\u80fd\uff0c\u5c06\u5176\u4e0e\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8e\u573a\u666f\u56fe\u7684CoT\u63d0\u793a\u80fd\u591f\u663e\u8457\u63d0\u9ad8VLMs\u7684\u7a7a\u95f4\u63a8\u7406\u51c6\u786e\u6027\u3002\u4e0eSFT\u76f8\u6bd4\uff0cGRPO\u5728Pass@1\u8bc4\u4f30\u4e2d\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u51c6\u786e\u7387\uff0c\u5e76\u5728\u5206\u5e03\u5916\uff08OOD\uff09\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u3002SFT\u5bb9\u6613\u8fc7\u62df\u5408\u8868\u9762\u8bed\u8a00\u6a21\u5f0f\uff0c\u5bfc\u81f4\u5728\u6d4b\u8bd5\u65f6\u9047\u5230\u63aa\u8f9e\u53d8\u5316\uff08\u4f8b\u5982\uff0c\u201c\u66f4\u8fd1\u201d\u53d8\u4e3a\u201c\u66f4\u8fdc\u201d\uff09\u65f6\u6027\u80fd\u4e0b\u964d\u3002\u76f8\u53cd\uff0cGRPO\u80fd\u591f\u66f4\u53ef\u9760\u5730\u6cdb\u5316\uff0c\u5e76\u5728\u8fd9\u4e9b\u53d8\u5316\u4e0b\u4fdd\u6301\u7a33\u5b9a\u7684\u6027\u80fd\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u5f15\u5165\u7ed3\u6784\u5316\u591a\u9636\u6bb5\u63d0\u793a\uff08\u5982\u57fa\u4e8e\u573a\u666f\u56fe\u7684CoT\uff09\u548c\u4f7f\u7528GRPO\u8fdb\u884c\u5fae\u8c03\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u548c\u6cdb\u5316\u80fd\u529b\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u7b80\u5355\u7684CoT\u63d0\u793a\u53ef\u80fd\u635f\u5bb3\u6a21\u578b\u6027\u80fd\uff0c\u800c\u57fa\u4e8e\u573a\u666f\u56fe\u7684CoT\u548cGRPO\u5219\u80fd\u6709\u6548\u63d0\u5347\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u5206\u5e03\u5916\uff08OOD\uff09\u6761\u4ef6\u548c\u8bcd\u8bed\u53d8\u5316\u65f6\u3002"}}
{"id": "2507.13558", "categories": ["cs.AI", "cs.DB", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13558", "abs": "https://arxiv.org/abs/2507.13558", "authors": ["David Poole"], "title": "Why Isn't Relational Learning Taking Over the World?", "comment": "10 pages (6 pages + references + appendices)", "summary": "AI seems to be taking over the world with systems that model pixels, words,\nand phonemes. The world is arguably made up, not of pixels, words, and phonemes\nbut of entities (objects, things, including events) with properties and\nrelations among them. Surely we should model these, not the perception or\ndescription of them. You might suspect that concentrating on modeling words and\npixels is because all of the (valuable) data in the world is in terms of text\nand images. If you look into almost any company you will find their most\nvaluable data is in spreadsheets, databases and other relational formats. These\nare not the form that are studied in introductory machine learning, but are\nfull of product numbers, student numbers, transaction numbers and other\nidentifiers that can't be interpreted naively as numbers. The field that\nstudies this sort of data has various names including relational learning,\nstatistical relational AI, and many others. This paper explains why relational\nlearning is not taking over the world -- except in a few cases with restricted\nrelations -- and what needs to be done to bring it to it's rightful prominence.", "AI": {"tldr": "AI \u5e94\u8be5\u4ece\u50cf\u7d20\u548c\u5355\u8bcd\u8f6c\u5411\u5b9e\u4f53\u548c\u5173\u7cfb\uff0c\u4f46\u5173\u7cfb\u5b66\u4e60\u9700\u8981\u66f4\u591a\u53d1\u5c55\u624d\u80fd\u666e\u53ca\u3002", "motivation": "\u5f53\u524d\u7684 AI \u6a21\u578b\u4e3b\u8981\u5173\u6ce8\u50cf\u7d20\u548c\u5355\u8bcd\uff0c\u800c\u5ffd\u7565\u4e86\u4e16\u754c\u662f\u7531\u5177\u6709\u5c5e\u6027\u548c\u5173\u7cfb\u7684\u5b9e\u4f53\u7ec4\u6210\u7684\u8fd9\u4e00\u4e8b\u5b9e\u3002\u6700\u6709\u4ef7\u503c\u7684\u6570\u636e\u901a\u5e38\u662f\u5173\u7cfb\u578b\u6570\u636e\uff0c\u4f46\u76ee\u524d\u5728 AI \u9886\u57df\u5e76\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002", "method": "\u89e3\u91ca\u4e86\u5173\u7cfb\u5b66\u4e60\u672a\u88ab\u5e7f\u6cdb\u91c7\u7528\u7684\u539f\u56e0\uff0c\u5e76\u63d0\u51fa\u4e86\u4f7f\u5176\u83b7\u5f97\u5e7f\u6cdb\u5e94\u7528\u7684\u65b9\u6cd5\u3002", "result": "\u5173\u7cfb\u5b66\u4e60\u5728\u5c11\u6570\u53d7\u9650\u5173\u7cfb\u7684\u60c5\u51b5\u4e0b\u53d6\u5f97\u4e86\u6210\u529f\uff0c\u4f46\u5c1a\u672a\u666e\u53ca\u3002\u9700\u8981\u8fdb\u4e00\u6b65\u7684\u7814\u7a76\u548c\u53d1\u5c55\u624d\u80fd\u4f7f\u5176\u53d1\u6325\u5e94\u6709\u7684\u4f5c\u7528\u3002tldr: AI \u5e94\u8be5\u4ece\u50cf\u7d20\u548c\u5355\u8bcd\u8f6c\u5411\u5b9e\u4f53\u548c\u5173\u7cfb\uff0c\u4f46\u5173\u7cfb\u5b66\u4e60\u9700\u8981\u66f4\u591a\u53d1\u5c55\u624d\u80fd\u666e\u53ca\u3002", "conclusion": "AI \u5e94\u8be5\u4e13\u6ce8\u4e8e\u5efa\u6a21\u5b9e\u4f53\u53ca\u5176\u5173\u7cfb\uff0c\u800c\u4e0d\u662f\u4ec5\u9650\u4e8e\u50cf\u7d20\u548c\u5355\u8bcd\u3002"}}
{"id": "2507.13586", "categories": ["cs.GR", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13586", "abs": "https://arxiv.org/abs/2507.13586", "authors": ["Kaiyuan Tang", "Kuangshi Ai", "Jun Han", "Chaoli Wang"], "title": "TexGS-VolVis: Expressive Scene Editing for Volume Visualization via Textured Gaussian Splatting", "comment": "Accepted by IEEE VIS 2025", "summary": "Advancements in volume visualization (VolVis) focus on extracting insights\nfrom 3D volumetric data by generating visually compelling renderings that\nreveal complex internal structures. Existing VolVis approaches have explored\nnon-photorealistic rendering techniques to enhance the clarity, expressiveness,\nand informativeness of visual communication. While effective, these methods\noften rely on complex predefined rules and are limited to transferring a single\nstyle, restricting their flexibility. To overcome these limitations, we\nadvocate the representation of VolVis scenes using differentiable Gaussian\nprimitives combined with pretrained large models to enable arbitrary style\ntransfer and real-time rendering. However, conventional 3D Gaussian primitives\ntightly couple geometry and appearance, leading to suboptimal stylization\nresults. To address this, we introduce TexGS-VolVis, a textured Gaussian\nsplatting framework for VolVis. TexGS-VolVis employs 2D Gaussian primitives,\nextending each Gaussian with additional texture and shading attributes,\nresulting in higher-quality, geometry-consistent stylization and enhanced\nlighting control during inference. Despite these improvements, achieving\nflexible and controllable scene editing remains challenging. To further enhance\nstylization, we develop image- and text-driven non-photorealistic scene editing\ntailored for TexGS-VolVis and 2D-lift-3D segmentation to enable partial editing\nwith fine-grained control. We evaluate TexGS-VolVis both qualitatively and\nquantitatively across various volume rendering scenes, demonstrating its\nsuperiority over existing methods in terms of efficiency, visual quality, and\nediting flexibility.", "AI": {"tldr": "TexGS-VolVis\u662f\u4e00\u79cd\u65b0\u9896\u7684\u4f53\u79ef\u53ef\u89c6\u5316\u6846\u67b6\uff0c\u5b83\u4f7f\u7528\u5177\u6709\u7eb9\u7406\u548c\u9634\u5f71\u5c5e\u6027\u76842D\u9ad8\u65af\u57fa\u5143\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u98ce\u683c\u5316\u548c\u7075\u6d3b\u7684\u573a\u666f\u7f16\u8f91\u3002", "motivation": "\u4e3a\u4e86\u514b\u670d\u73b0\u6709\u4f53\u79ef\u53ef\u89c6\u5316\u65b9\u6cd5\u4f9d\u8d56\u590d\u6742\u9884\u5b9a\u4e49\u89c4\u5219\u548c\u4ec5\u9650\u4e8e\u5355\u4e00\u98ce\u683c\u8fc1\u79fb\u7684\u9650\u5236\uff0c\u6211\u4eec\u4e3b\u5f20\u4f7f\u7528\u53ef\u5fae\u5206\u9ad8\u65af\u57fa\u5143\u548c\u9884\u8bad\u7ec3\u5927\u6a21\u578b\u6765\u8868\u793a\u4f53\u79ef\u53ef\u89c6\u5316\u573a\u666f\uff0c\u4ee5\u5b9e\u73b0\u4efb\u610f\u98ce\u683c\u8fc1\u79fb\u548c\u5b9e\u65f6\u6e32\u67d3\u3002\u7136\u800c\uff0c\u4f20\u7edf\u76843D\u9ad8\u65af\u57fa\u5143\u5c06\u51e0\u4f55\u548c\u5916\u89c2\u7d27\u5bc6\u8026\u5408\uff0c\u5bfc\u81f4\u98ce\u683c\u5316\u7ed3\u679c\u4e0d\u7406\u60f3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTexGS-VolVis\u7684\u65b0\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u4f7f\u75282D\u9ad8\u65af\u57fa\u5143\uff0c\u5e76\u4e3a\u6bcf\u4e2a\u9ad8\u65af\u6269\u5c55\u4e86\u989d\u5916\u7684\u7eb9\u7406\u548c\u9634\u5f71\u5c5e\u6027\uff0c\u4ee5\u5b9e\u73b0\u66f4\u9ad8\u8d28\u91cf\u3001\u51e0\u4f55\u4e00\u81f4\u7684\u98ce\u683c\u5316\uff0c\u5e76\u589e\u5f3a\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u5149\u7167\u63a7\u5236\u3002\u6b64\u5916\uff0c\u8fd8\u5f00\u53d1\u4e86\u9488\u5bf9TexGS-VolVis\u7684\u56fe\u50cf\u548c\u6587\u672c\u9a71\u52a8\u7684\u975e\u771f\u5b9e\u611f\u573a\u666f\u7f16\u8f91\u4ee5\u53ca2D-lift-3D\u5206\u5272\uff0c\u4ee5\u5b9e\u73b0\u5177\u6709\u7ec6\u7c92\u5ea6\u63a7\u5236\u7684\u90e8\u5206\u7f16\u8f91\u3002", "result": "TexGS-VolVis\u5b9e\u73b0\u4e86\u66f4\u9ad8\u8d28\u91cf\u3001\u51e0\u4f55\u4e00\u81f4\u7684\u98ce\u683c\u5316\uff0c\u5e76\u589e\u5f3a\u4e86\u5149\u7167\u63a7\u5236\u3002\u901a\u8fc7\u56fe\u50cf\u548c\u6587\u672c\u9a71\u52a8\u7684\u7f16\u8f91\uff0c\u5b9e\u73b0\u4e86\u7075\u6d3b\u4e14\u53ef\u63a7\u7684\u573a\u666f\u7f16\u8f91\uff0c\u5e76\u80fd\u591f\u8fdb\u884c\u90e8\u5206\u7f16\u8f91\u548c\u7ec6\u7c92\u5ea6\u63a7\u5236\u3002", "conclusion": "TexGS-VolVis\u5728\u6548\u7387\u3001\u89c6\u89c9\u8d28\u91cf\u548c\u7f16\u8f91\u7075\u6d3b\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2507.13678", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.13678", "abs": "https://arxiv.org/abs/2507.13678", "authors": ["Honghao Wu", "Kemi Ding", "Li Qiu"], "title": "Minimum Clustering of Matrices Based on Phase Alignment", "comment": "This work has been received by CDC2025", "summary": "Coordinating multi-agent systems requires balancing synchronization\nperformance and controller implementation costs. To this end, we classify\nagents by their intrinsic properties, enabling each group to be controlled by a\nuniform controller and thus reducing the number of unique controller types\nrequired. Existing centralized control methods, despite their capability to\nachieve high synchronization performance with fewer types of controllers,\nsuffer from critical drawbacks such as limited scalability and vulnerability to\nsingle points of failure. On the other hand, distributed control strategies,\nwhere controllers are typically agent-dependent, result in the type of required\ncontrollers increasing proportionally with the size of the system.\n  This paper introduces a novel phase-alignment-based framework to minimize the\ntype of controllers by strategically clustering agents with aligned\nsynchronization behaviors. Leveraging the intrinsic phase properties of complex\nmatrices, we formulate a constrained clustering problem and propose a\nhierarchical optimization method combining recursive exact searches for\nsmall-scale systems and scalable stochastic approximations for large-scale\nnetworks. This work bridges theoretical phase analysis with practical control\nsynthesis, offering a cost-effective solution for large-scale multi-agent\nsystems. The theoretical results applied for the analysis of a 50-agent network\nillustrate the effectiveness of the proposed algorithms.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u667a\u80fd\u4f53\u8fdb\u884c\u5206\u7c7b\u548c\u805a\u7c7b\uff0c\u4ee5\u6700\u5c0f\u5316\u6240\u9700\u7684\u63a7\u5236\u5668\u7c7b\u578b\u6570\u91cf\uff0c\u4ece\u800c\u5728\u6027\u80fd\u548c\u6210\u672c\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5229\u7528\u76f8\u4f4d\u5bf9\u9f50\u548c\u5206\u5c42\u4f18\u5316\u6280\u672f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u53ef\u6269\u5c55\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u534f\u8c03\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u9700\u8981\u5728\u540c\u6b65\u6027\u80fd\u548c\u63a7\u5236\u5668\u5b9e\u73b0\u6210\u672c\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002\u73b0\u6709\u7684\u96c6\u4e2d\u5f0f\u63a7\u5236\u65b9\u6cd5\u867d\u7136\u5728\u63a7\u5236\u5668\u7c7b\u578b\u8f83\u5c11\u7684\u60c5\u51b5\u4e0b\u80fd\u5b9e\u73b0\u9ad8\u540c\u6b65\u6027\u80fd\uff0c\u4f46\u5b58\u5728\u53ef\u6269\u5c55\u6027\u6709\u9650\u548c\u5355\u70b9\u6545\u969c\u7b49\u4e25\u91cd\u7f3a\u70b9\u3002\u800c\u5206\u5e03\u5f0f\u63a7\u5236\u7b56\u7565\u7684\u63a7\u5236\u5668\u901a\u5e38\u4f9d\u8d56\u4e8e\u667a\u80fd\u4f53\uff0c\u6240\u9700\u63a7\u5236\u5668\u7c7b\u578b\u968f\u7740\u7cfb\u7edf\u89c4\u6a21\u7684\u589e\u5927\u800c\u589e\u52a0\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u76f8\u4f4d\u5bf9\u9f50\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u5177\u6709\u76f8\u4f3c\u540c\u6b65\u884c\u4e3a\u7684\u667a\u80fd\u4f53\u8fdb\u884c\u805a\u7c7b\u6765\u6700\u5c0f\u5316\u63a7\u5236\u5668\u7c7b\u578b\u3002\u5229\u7528\u590d\u6742\u77e9\u9635\u7684\u5185\u5728\u76f8\u4f4d\u7279\u6027\uff0c\u8bbe\u8ba1\u4e86\u7ea6\u675f\u805a\u7c7b\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u9012\u5f52\u7cbe\u786e\u641c\u7d22\u548c\u5c0f\u89c4\u6a21\u7cfb\u7edf\u5206\u5c42\u4f18\u5316\u65b9\u6cd5\u7684\u7ec4\u5408\uff0c\u4ee5\u53ca\u9488\u5bf9\u5927\u89c4\u6a21\u7f51\u7edc\u7684\u5206\u5e03\u5f0f\u4f18\u5316\u65b9\u6cd5\u3002", "result": "\u901a\u8fc7\u5bf9\u4e00\u4e2a\u5305\u542b50\u4e2a\u667a\u80fd\u4f53\u7684\u7f51\u7edc\u7684\u7406\u8bba\u5206\u6790\u548c\u5e94\u7528\uff0c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7b97\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u901a\u8fc7\u5c06\u5177\u6709\u76f8\u4f3c\u540c\u6b65\u884c\u4e3a\u7684\u667a\u80fd\u4f53\u8fdb\u884c\u805a\u7c7b\uff0c\u63d0\u51fa\u4e86\u6700\u5c0f\u5316\u63a7\u5236\u5668\u7c7b\u578b\u6570\u91cf\u7684\u6846\u67b6\uff0c\u4e3a\u5927\u89c4\u6a21\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u7ecf\u6d4e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.13602", "categories": ["cs.RO", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13602", "abs": "https://arxiv.org/abs/2507.13602", "authors": ["Shivakanth Sujit", "Luca Nunziante", "Dan Ogawa Lillrank", "Rousslan Fernand Julien Dossa", "Kai Arulkumaran"], "title": "Improving Low-Cost Teleoperation: Augmenting GELLO with Force", "comment": "Accepted at the 2025 IEEE/SICE International Symposium on System\n  Integration", "summary": "In this work we extend the low-cost GELLO teleoperation system, initially\ndesigned for joint position control, with additional force information. Our\nfirst extension is to implement force feedback, allowing users to feel\nresistance when interacting with the environment. Our second extension is to\nadd force information into the data collection process and training of\nimitation learning models. We validate our additions by implementing these on a\nGELLO system with a Franka Panda arm as the follower robot, performing a user\nstudy, and comparing the performance of policies trained with and without force\ninformation on a range of simulated and real dexterous manipulation tasks.\nQualitatively, users with robotics experience preferred our controller, and the\naddition of force inputs improved task success on the majority of tasks.", "AI": {"tldr": "The GELLO teleoperation system was enhanced with force feedback and force data for imitation learning, leading to improved task performance and user preference in robotic manipulation tasks.", "motivation": "To extend the low-cost GELLO teleoperation system with additional force information for improved performance and user experience in robotic manipulation tasks.", "method": "The study extended the GELLO teleoperation system with force feedback and incorporated force information into imitation learning models. The system was implemented on a GELLO system with a Franka Panda arm. Policies trained with and without force information were compared on simulated and real dexterous manipulation tasks.", "result": "Qualitative user study indicated a preference for the force-feedback controller among users with robotics experience. Task success improved on the majority of tasks when force inputs were included.", "conclusion": "The addition of force information to the GELLO teleoperation system improved task success in a majority of tasks, and users with robotics experience preferred the force-feedback controller."}}
{"id": "2507.13987", "categories": ["cs.LO"], "pdf": "https://arxiv.org/pdf/2507.13987", "abs": "https://arxiv.org/abs/2507.13987", "authors": ["Simon Fl\u00fcgel", "Martin Glauer", "Till Mossakowski", "Fabian Neuhaus"], "title": "ChemLog: Making MSOL Viable for Ontological Classification and Learning", "comment": null, "summary": "Despite its prevalence, in many domains, OWL is not expressive enough to\ndefine ontology classes. In this paper, we present an approach that allows to\nuse monadic second-order formalisations for ontology classification. As a case\nstudy, we have applied our approach to 14 peptide-related classes from the\nchemistry ontology ChEBI. For these classes, a monadic second-order logic\nformalisation has been developed and applied both to ChEBI as well as to 119\nmillion molecules from the chemistry database PubChem. While this logical\napproach alone is limited to classification for the specified classes (in our\ncase, (sub)classes of peptides), transformer deep learning models scale\nclassification to the whole of the ChEBI ontology. We show that when using the\nclassifications obtained by the logical approach as training data, the\nperformance of the deep learning models can be significantly enhanced.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u903b\u8f91\u65b9\u6cd5\u548c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u672c\u4f53\u5206\u7c7b\uff0c\u7279\u522b\u662f\u5728ChEBI\u672c\u4f53\u7684\u80bd\u7c7b\u5206\u7c7b\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u5f53\u524d\u7684\u672c\u4f53\u5206\u7c7b\u65b9\u6cd5\u5728\u8bb8\u591a\u9886\u57df\u4e2d\uff0c\u4f7f\u7528OWL\uff08\u672c\u4f53\u8bed\u8a00\uff09\u8fdb\u884c\u7c7b\u5b9a\u4e49\u65f6\uff0c\u8868\u8fbe\u80fd\u529b\u4e0d\u8db3\u3002\u672c\u7814\u7a76\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u66f4\u5177\u8868\u8fbe\u529b\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5355\u5b50\u4e8c\u9636\u5f62\u5f0f\u5316\u8fdb\u884c\u672c\u4f53\u5206\u7c7b\u7684\u65b9\u6cd5\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8eChEBI\u672c\u4f53\u768414\u4e2a\u80bd\u7c7b\u76f8\u5173\u7c7b\u522b\u7684\u5206\u7c7b\u3002\u540c\u65f6\uff0c\u5229\u7528Transformer\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5bf9\u6574\u4e2aChEBI\u672c\u4f53\u8fdb\u884c\u5206\u7c7b\uff0c\u5e76\u5c06\u903b\u8f91\u65b9\u6cd5\u83b7\u5f97\u7684\u5206\u7c7b\u4f5c\u4e3a\u8bad\u7ec3\u6570\u636e\u6765\u589e\u5f3a\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u6027\u80fd\u3002", "result": "\u5728ChEBI\u672c\u4f53\u7684\u80bd\u7c7b\uff08\u4e9a\u7c7b\uff09\u5206\u7c7b\u4efb\u52a1\u4e0a\uff0c\u7ed3\u5408\u903b\u8f91\u65b9\u6cd5\u548c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u76f8\u6bd4\u5355\u72ec\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u5206\u7c7b\u6027\u80fd\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u903b\u8f91\u65b9\u6cd5\u548c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u5728ChEBI\u672c\u4f53\u7684\u80bd\u7c7b\uff08\u4e9a\u7c7b\uff09\u5206\u7c7b\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u4e3a\u66f4\u5e7f\u6cdb\u7684\u672c\u4f53\u5206\u7c7b\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u3002"}}
{"id": "2507.13631", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2507.13631", "abs": "https://arxiv.org/abs/2507.13631", "authors": ["Fuyuki Kihara", "Seiji Uenohara", "Satoshi Awamura", "Naoko Misawa", "Chihiro Matsui", "Ken Takeuchi"], "title": "4T2R X-ReRAM CiM Array for Variation-tolerant, Low-power, Massively Parallel MAC Operation", "comment": "4 pages", "summary": "Computation-in-Memory (CiM) is attracting attention as a technology that can\nperform MAC calculations required for AI accelerators, at high speed with low\npower consumption. However, there is a problem regarding power consumption and\ndevice-derived errors that increase as row parallelism increases. In this\npaper, a 4T2R ReRAM cell and an 8T SRAM CiM suitable for CiM is proposed. It is\nshown that adopting the proposed 4T2R ReRAM cell reduces the errors due to\nvariation in ReRAM devices compared to conventional 4T4R ReRAM cells.", "AI": {"tldr": "A new 4T2R ReRAM cell and 8T SRAM CiM architecture are proposed to address power consumption and error issues in AI accelerators, showing reduced errors compared to existing methods.", "motivation": "Computation-in-Memory (CiM) is a promising technology for AI accelerators due to its high speed and low power consumption. However, existing CiM technologies face challenges with increased power consumption and device-derived errors as row parallelism increases.", "method": "The paper proposes a 4T2R ReRAM cell and an 8T SRAM CiM architecture.", "result": "The proposed 4T2R ReRAM cell reduces errors compared to conventional 4T4R ReRAM cells, making it more suitable for CiM applications.", "conclusion": "The proposed 4T2R ReRAM cell and 8T SRAM CiM are suitable for CiM applications. Adopting the proposed 4T2R ReRAM cell reduces errors due to variation in ReRAM devices compared to conventional 4T4R ReRAM cells."}}
{"id": "2507.13750", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2507.13750", "abs": "https://arxiv.org/abs/2507.13750", "authors": ["M. T. Abbas", "M. Szymczak", "D. Szymanski", "J. Zeler", "M. Drozd", "L. T. K Giang", "L. Marciniak"], "title": "Phase Transition Under Control: Toward Application-Oriented Luminescence Thermometry and Thermally Activated Emission", "comment": null, "summary": "Phase-transition-based luminescent thermometers are characterized by two\ninherent limitations: a narrow thermal operating range and the presence of a\nhysteresis loop in the thermometric parameter. In this work, we demonstrate\nthat controlling the particle size of LaGaO3:Eu3+ phosphors enables significant\nenhancement of thermometric performance. Specifically, a reduction in grain\nsize dispersion leads to an increase in relative thermal sensitivity and\nsignificantly narrows the hysteresis loop. As a result of this approach, the\nrelative sensitivity was increased to 18.2% K-1 for LaGaO3:Eu3+ synthesized via\nthe solid-state method, compared to 3.0% K-1 for the counterpart prepared using\nthe Pechini method. Furthermore, we show that the intentional incorporation of\nAl3+ and Sc3+ co-dopant ions allows for continuous tuning of the structural\nphase transition temperature from 165 K for 15% Al3+ to 491 K for 2% Sc3+,\nwithout significantly affecting the low-temperature spectroscopic properties of\nEu3+ ions. This ability to shift the phase transition temperature in LaGaO3\noffers a practical route to modulate the thermal response range of the\nluminescent thermometer, enabling its adaptation to specific application\nrequirements. The empirical relationship established in this study between the\nphase transition temperature and the ionic radius mismatch parameter provides a\npredictive tool for the rational design of phase-transition-based phosphors\nwith tailored thermometric performance. The ability to systematically tune the\nphase transition temperature via ionic radius mismatch, together with enhanced\nthermometric performance resulting from reduced grain size dispersion,\nestablishes a coherent strategy for the rational design of high-sensitivity,\nlow-hysteresis thermal sensors.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u63a7\u5236 LaGaO3:Eu3+ \u8367\u5149\u7c89\u7684\u7c92\u5f84\u548c\u63ba\u6742 Al3+\u3001Sc3+ \u79bb\u5b50\uff0c\u6210\u529f\u63d0\u9ad8\u4e86\u5176\u4f5c\u4e3a\u53d1\u5149\u6e29\u5ea6\u8ba1\u7684\u70ed\u654f\u6027\u80fd\uff0c\u62d3\u5bbd\u4e86\u70ed\u54cd\u5e94\u8303\u56f4\uff0c\u5e76\u51cf\u5c0f\u4e86\u6ede\u540e\u56de\u7ebf\uff0c\u4e3a\u8bbe\u8ba1\u9ad8\u6027\u80fd\u53d1\u5149\u6e29\u5ea6\u8ba1\u63d0\u4f9b\u4e86\u65b0\u7b56\u7565\u3002", "motivation": "\u65e8\u5728\u514b\u670d\u57fa\u4e8e\u76f8\u53d8\u7684\u53d1\u5149\u6e29\u5ea6\u8ba1\u5728\u70ed\u5de5\u4f5c\u8303\u56f4\u7a84\u548c\u5b58\u5728\u6ede\u540e\u56de\u7ebf\u65b9\u9762\u7684\u56fa\u6709\u5c40\u9650\u6027\uff0c\u4ee5\u63d0\u9ad8\u5176\u70ed\u654f\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u56fa\u6001\u6cd5\u548c Pechini \u6cd5\u5408\u6210 LaGaO3:Eu3+ \u8367\u5149\u7c89\uff0c\u5e76\u7814\u7a76\u7c92\u5f84\u5206\u5e03\u3001Al3+ \u548c Sc3+ \u5171\u63ba\u6742\u5bf9\u8367\u5149\u7c89\u70ed\u654f\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u901a\u8fc7\u56fa\u6001\u6cd5\u5f97\u5230\u7684 LaGaO3:Eu3+ \u76f8\u5bf9\u70ed\u7075\u654f\u5ea6\u8fbe\u5230 18.2% K-1\uff0c\u9ad8\u4e8e Pechini \u6cd5\u5236\u5907\u7684 3.0% K-1\u3002\u63ba\u6742 Al3+ \u548c Sc3+ \u53ef\u4ee5\u5c06\u76f8\u53d8\u6e29\u5ea6\u5728 165 K \u81f3 491 K \u4e4b\u95f4\u8fde\u7eed\u8c03\u8c10\uff0c\u4e14\u4e0d\u663e\u8457\u5f71\u54cd Eu3+ \u7684\u4f4e\u6e29\u5149\u8c31\u6027\u8d28\u3002", "conclusion": "\u901a\u8fc7\u63a7\u5236 LaGaO3:Eu3+ \u8367\u5149\u7c89\u7684\u7c92\u5f84\u548c\u63ba\u6742 Al3+\u3001Sc3+ \u79bb\u5b50\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u5176\u4f5c\u4e3a\u53d1\u5149\u6e29\u5ea6\u8ba1\u7684\u70ed\u654f\u6027\u80fd\uff0c\u5305\u62ec\u63d0\u9ad8\u76f8\u5bf9\u70ed\u7075\u654f\u5ea6\u548c\u51cf\u5c0f\u6ede\u540e\u56de\u7ebf\u3002\u8be5\u7814\u7a76\u8fd8\u5efa\u7acb\u4e86\u76f8\u53d8\u6e29\u5ea6\u4e0e\u79bb\u5b50\u534a\u5f84\u5931\u914d\u53c2\u6570\u4e4b\u95f4\u7684\u7ecf\u9a8c\u5173\u7cfb\uff0c\u4e3a\u8bbe\u8ba1\u5177\u6709\u7279\u5b9a\u70ed\u54cd\u5e94\u8303\u56f4\u548c\u4f20\u611f\u6027\u80fd\u7684\u53d1\u5149\u6e29\u5ea6\u8ba1\u63d0\u4f9b\u4e86\u7406\u8bba\u6307\u5bfc\u3002"}}
{"id": "2507.13781", "categories": ["cond-mat.mes-hall", "quant-ph"], "pdf": "https://arxiv.org/pdf/2507.13781", "abs": "https://arxiv.org/abs/2507.13781", "authors": ["Konstantinos Tsoukalas", "Alexei Orekhov", "Bence Het\u00e9nyi", "Uwe von L\u00fcpke", "Jeth Arunseangroj", "Inga Seidler", "Lisa Sommer", "Eoin G. Kelly", "Leonardo Massai", "Michele Aldeghi", "Marta Pita-Vidal", "Stephen W. Bedell", "Stephan Paredes", "Felix J. Schupp", "Matthias Mergenthaler", "Gian Salis", "Andreas Fuhrer", "Patrick Harvey-Collard"], "title": "Resonant two-qubit gates for fermionic simulations with spin qubits", "comment": null, "summary": "In gate-defined semiconductor spin qubits, the highly tunable Heisenberg\nexchange interaction is leveraged to implement fermionic two-qubit gates such\nas CZ and SWAP. However, the broader family of fermionic simulation (fSim)\ngates remains unexplored, and has the potential to enhance the performance of\nnear-term quantum simulation algorithms. Here, we demonstrate a method to\nimplement the fSim gate set in spin qubits using a single pulse combining\nbaseband and resonant exchange drives. This approach minimizes gate duration\nand drive amplitude, mitigating decoherence and crosstalk. We validate its\neffectiveness by realizing a resonant iSWAP gate between two hole spins in\ngermanium, achieving a fidelity of 93.8(5)% extracted with interleaved\nrandomized benchmarking. Quantum process tomography confirms accurate gate\ncalibration and identifies qubit decoherence as the dominant error source. Our\nresults establish a practical route toward a versatile and efficient two-qubit\ngate set for spin-based quantum processors.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9996\u6b21\u5728\u534a\u5bfc\u4f53\u81ea\u65cb\u91cf\u5b50\u6bd4\u7279\u4e2d\u5b9e\u73b0\u4e86fSim\u95e8\u96c6\uff0c\u5e76\u6f14\u793a\u4e86\u5176\u5728\u9517\u53cc\u91cf\u5b50\u6bd4\u7279\u4e0a\u7684\u9ad8\u4fdd\u771f\u5ea6\u5e94\u7528\u3002", "motivation": "\u4e3a\u4e86\u63a2\u7d22fSim\u95e8\u96c6\u5728\u534a\u5bfc\u4f53\u81ea\u65cb\u91cf\u5b50\u6bd4\u7279\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u5e76\u4e3a\u8fd1\u671f\u7684\u91cf\u5b50\u6a21\u62df\u7b97\u6cd5\u63d0\u5347\u6027\u80fd\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u5e76\u6f14\u793a\u4e86\u4e00\u79cd\u5728\u81ea\u65cb\u91cf\u5b50\u6bd4\u7279\u4e2d\u5b9e\u73b0fSim\u95e8\u96c6\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u7ed3\u5408\u4e86\u57fa\u5e26\u548c\u5171\u632f\u4ea4\u6362\u9a71\u52a8\u7684\u5355\u8109\u51b2\u3002", "result": "\u6210\u529f\u5728\u4e24\u4e2a\u9517\u7a7a\u7a74\u81ea\u65cb\u4e2d\u5b9e\u73b0\u4e86\u5171\u632fiSWAP\u95e8\uff0c\u4fdd\u771f\u5ea6\u8fbe\u523093.8(5)%\uff0c\u5e76\u901a\u8fc7\u91cf\u5b50\u8fc7\u7a0b\u5c42\u6790\u6210\u50cf\u6280\u672f\u786e\u8ba4\u4e86\u95e8\u7684\u51c6\u786e\u6027\u548c\u91cf\u5b50\u6bd4\u7279\u9000\u76f8\u5e72\u662f\u4e3b\u8981\u7684\u8bef\u5dee\u6765\u6e90\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u7ed3\u5408\u57fa\u5e26\u548c\u5171\u632f\u4ea4\u6362\u9a71\u52a8\u7684\u5355\u8109\u51b2\uff0c\u5728\u9517\u53cc\u91cf\u5b50\u6bd4\u7279\u4e2d\u5b9e\u73b0\u4e86fSim\u95e8\u96c6\uff0c\u4e3a\u57fa\u4e8e\u81ea\u65cb\u7684\u91cf\u5b50\u5904\u7406\u5668\u63d0\u4f9b\u4e86\u4e00\u6761\u901a\u7528\u4e14\u9ad8\u6548\u7684\u53cc\u91cf\u5b50\u6bd4\u7279\u95e8\u5b9e\u73b0\u8def\u5f84\u3002"}}
{"id": "2507.13814", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2507.13814", "abs": "https://arxiv.org/abs/2507.13814", "authors": ["Jianing Zhao", "Peng Gao", "Jiannong Cao", "Zhiyuan Wen", "Chen Chen", "Jianing Yin", "Ruosong Yang", "Bo Yuan"], "title": "CodeEdu: A Multi-Agent Collaborative Platform for Personalized Coding Education", "comment": "4 pages, 4 figures. Demo video available at:\n  https://youtu.be/9iIVmTT4CVk", "summary": "Large Language Models (LLMs) have demonstrated considerable potential in\nimproving coding education by providing support for code writing, explanation,\nand debugging. However, existing LLM-based approaches generally fail to assess\nstudents' abilities, design learning plans, provide personalized material\naligned with individual learning goals, and enable interactive learning.\nCurrent work mostly uses single LLM agents, which limits their ability to\nunderstand complex code repositories and schedule step-by-step tutoring. Recent\nresearch has shown that multi-agent LLMs can collaborate to solve complicated\nproblems in various domains like software engineering, but their potential in\nthe field of education remains unexplored. In this work, we introduce CodeEdu,\nan innovative multi-agent collaborative platform that combines LLMs with tool\nuse to provide proactive and personalized education in coding. Unlike static\npipelines, CodeEdu dynamically allocates agents and tasks to meet student\nneeds. Various agents in CodeEdu undertake certain functions specifically,\nincluding task planning, personalized material generation, real-time QA,\nstep-by-step tutoring, code execution, debugging, and learning report\ngeneration, facilitated with extensive external tools to improve task\nefficiency. Automated evaluations reveal that CodeEdu substantially enhances\nstudents' coding performance.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.13869", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2507.13869", "abs": "https://arxiv.org/abs/2507.13869", "authors": ["Avi Kadria", "Liam Roditty", "Aaron Sidford", "Virginia Vassilevska Williams", "Uri Zwick"], "title": "Improved girth approximation in weighted undirected graphs", "comment": null, "summary": "Let $G = (V,E,\\ell)$ be a $n$-node $m$-edge weighted undirected graph, where\n$\\ell: E \\rightarrow (0,\\infty)$ is a real \\emph{length} function defined on\nits edges, and let $g$ denote the girth of $G$, i.e., the length of its\nshortest cycle. We present an algorithm that, for any input, integer $k \\geq\n1$, in $O(kn^{1+1/k}\\log{n} + m(k+\\log{n}))$ expected time finds a cycle of\nlength at most $\\frac{4k}{3}g$. This algorithm nearly matches a\n$O(n^{1+1/k}\\log{n})$-time algorithm of \\cite{KadriaRSWZ22} which applied to\nunweighted graphs of girth $3$. For weighted graphs, this result also improves\nupon the previous state-of-the-art algorithm that in $O((n^{1+1/k}\\log n+m)\\log\n(nM))$ time, where $\\ell: E \\rightarrow [1, M]$ is an integral length function,\nfinds a cycle of length at most $2kg$~\\cite{KadriaRSWZ22}. For $k=1$ this\nresult improves upon the result of Roditty and Tov~\\cite{RodittyT13}.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u56fe\u7b97\u6cd5\uff0c\u53ef\u4ee5\u5728O(kn^(1+1/k)log n + m(k+log n))\u7684\u671f\u671b\u65f6\u95f4\u5185\u627e\u5230\u957f\u5ea6\u4e0d\u8d85\u8fc74k/3 * g\u7684\u73af\uff0c\u6539\u8fdb\u4e86\u52a0\u6743\u548c\u672a\u52a0\u6743\u56fe\u7684\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u4e3a\u4e86\u5728\u52a0\u6743\u56fe\u4e2d\u627e\u5230\u6bd4\u73b0\u6709\u7b97\u6cd5\u66f4\u77ed\u7684\u73af\uff0c\u7279\u522b\u662f\u5f53 k=1 \u65f6\uff0c\u6539\u8fdb Roditty \u548c Tov \u7684\u7ed3\u679c\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u56fe\u7b97\u6cd5\uff0c\u80fd\u591f\u627e\u5230\u957f\u5ea6\u4e0d\u8d85\u8fc7 4k/3 * g \u7684\u73af\u3002", "result": "\u8be5\u7b97\u6cd5\u5728 O(kn^(1+1/k)log n + m(k+log n)) \u7684\u671f\u671b\u65f6\u95f4\u5185\uff0c\u627e\u5230\u957f\u5ea6\u4e0d\u8d85\u8fc7 4k/3 * g \u7684\u73af\uff0c\u4f18\u4e8e\u4e4b\u524d\u52a0\u6743\u56fe\u548c\u672a\u52a0\u6743\u56fe\uff08\u7279\u522b\u662f\u5728 k=1 \u65f6\uff09\u7684\u6700\u4f18\u7b97\u6cd5\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u5bf9\u4e8e\u4efb\u610f\u8f93\u5165\u3001\u6574\u6570k \u2265 1\uff0c\u80fd\u5728 O(kn^(1+1/k)log n + m(k+log n)) \u7684\u671f\u671b\u65f6\u95f4\u5185\u627e\u5230\u957f\u5ea6\u4e0d\u8d85\u8fc7 4k/3 * g \u7684\u73af\uff0c\u5728\u52a0\u6743\u56fe\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002"}}
{"id": "2507.13390", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13390", "abs": "https://arxiv.org/abs/2507.13390", "authors": ["Kundeshwar Pundalik", "Piyush Sawarkar", "Nihar Sahoo", "Abhishek Shinde", "Prateek Chanda", "Vedant Goswami", "Ajay Nagpal", "Atul Singh", "Viraj Thakur", "Vijay Dewane", "Aamod Thakur", "Bhargav Patel", "Smita Gautam", "Bhagwan Panditi", "Shyam Pawar", "Madhav Kotcha", "Suraj Racha", "Saral Sureka", "Pankaj Singh", "Rishi Bal", "Rohit Saluja", "Ganesh Ramakrishnan"], "title": "PARAM-1 BharatGen 2.9B Model", "comment": null, "summary": "Large Language Models (LLMs) have emerged as powerful general-purpose\nreasoning systems, yet their development remains dominated by English-centric\ndata, architectures, and optimization paradigms. This exclusionary design\nresults in structural under-representation of linguistically diverse regions\nsuch as India, where over 20 official languages and 100+ dialects coexist\nalongside phenomena like code-switching and diglossia. We introduce PARAM-1, a\n2.9B parameter decoder-only, text-only language model trained from scratch with\nan explicit architectural and linguistic focus on Indian diversity. PARAM-1 is\ntrained on a bilingual dataset consisting of only Hindi and English,\nconstructed with a strong focus on fact-rich, high-quality content. It is\nguided by three core principles: equitable representation of Indic languages\nthrough a 25% corpus allocation; tokenization fairness via a SentencePiece\ntokenizer adapted to Indian morphological structures; and culturally aligned\nevaluation benchmarks across IndicQA, code-mixed reasoning, and\nsocio-linguistic robustness tasks. By embedding diversity at the pretraining\nlevel-rather than deferring it to post-hoc alignment-PARAM-1 offers a\ndesign-first blueprint for equitable foundation modeling. Our results\ndemonstrate that it serves as both a competent general-purpose model and a\nrobust baseline for India-centric applications.", "AI": {"tldr": "PARAM-1 \u662f\u4e00\u4e2a 2.9B \u53c2\u6570\u7684\u8bed\u8a00\u6a21\u578b\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709 LLM \u7684\u82f1\u8bed\u4e2d\u5fc3\u5316\u95ee\u9898\uff0c\u5e76\u89e3\u51b3\u5370\u5ea6\u8bed\u8a00\u591a\u6837\u6027\u95ee\u9898\u3002\u5b83\u5728\u4ee5\u5370\u5ea6\u4e3a\u4e2d\u5fc3\u7684\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u53ef\u4f5c\u4e3a\u901a\u7528\u6a21\u578b\u548c\u5370\u5ea6\u76f8\u5173\u5e94\u7528\u7684\u57fa\u51c6\u3002", "motivation": "\u7531\u4e8e\u73b0\u6709\u7684 LLM \u4ee5\u82f1\u8bed\u4e3a\u4e2d\u5fc3\uff0c\u672a\u80fd\u5145\u5206\u4ee3\u8868\u5370\u5ea6\u7b49\u8bed\u8a00\u591a\u6837\u5316\u7684\u5730\u533a\uff0c\u56e0\u6b64\u9700\u8981 PARAM-1 \u6765\u89e3\u51b3\u8fd9\u79cd\u6392\u4ed6\u6027\u8bbe\u8ba1\uff0c\u5e76\u89e3\u51b3\u5370\u5ea6\u8bed\u8a00\u7684\u591a\u6837\u6027\u3002", "method": "PARAM-1 \u662f\u4e00\u4e2a 2.9B \u53c2\u6570\u7684\u3001\u4ec5\u89e3\u7801\u5668\u3001\u4ec5\u6587\u672c\u7684\u8bed\u8a00\u6a21\u578b\uff0c\u4f7f\u7528\u5305\u62ec\u5370\u5730\u8bed\u548c\u82f1\u8bed\u7684\u53cc\u8bed\u6570\u636e\u96c6\u4ece\u5934\u5f00\u59cb\u8bad\u7ec3\uff0c\u5e76\u901a\u8fc7 SentencePiece \u5206\u8bcd\u5668\u9488\u5bf9\u5370\u5ea6\u8bed\u8bed\u7cfb\u8fdb\u884c\u4e86\u4f18\u5316\u3002", "result": "PARAM-1 \u5728\u4ee5\u5370\u5ea6\u4e3a\u4e2d\u5fc3\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5305\u62ec IndicQA\u3001\u4ee3\u7801\u6df7\u5408\u63a8\u7406\u548c\u793e\u4f1a\u8bed\u8a00\u5b66\u7a33\u5065\u6027\u4efb\u52a1\uff0c\u8bc1\u660e\u4e86\u5176\u4f5c\u4e3a\u901a\u7528\u6a21\u578b\u548c\u5370\u5ea6\u76f8\u5173\u5e94\u7528\u7684\u5f3a\u5927\u57fa\u51c6\u7684\u6709\u6548\u6027\u3002", "conclusion": "PARAM-1 \u662f\u4e00\u4e2a 2.9B \u53c2\u6570\u7684\u3001\u4ec5\u89e3\u7801\u5668\u3001\u4ec5\u6587\u672c\u7684\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u4e00\u7cfb\u5217\u4ee5\u5370\u5ea6\u4e3a\u4e2d\u5fc3\u7684\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u53ef\u4f5c\u4e3a\u901a\u7528\u6a21\u578b\u548c\u5370\u5ea6\u76f8\u5173\u5e94\u7528\u7684\u5f3a\u5927\u57fa\u51c6\u3002"}}
{"id": "2507.14080", "categories": ["cs.DC", "D.2.4; C.2.4"], "pdf": "https://arxiv.org/pdf/2507.14080", "abs": "https://arxiv.org/abs/2507.14080", "authors": ["Derek Leung", "Nickolai Zeldovich", "Frans Kaashoek"], "title": "Shipwright: Proving liveness of distributed systems with Byzantine participants", "comment": "14 pages, 13 figures", "summary": "Ensuring liveness in a decentralized system, such as PBFT, is critical,\nbecause there may not be any single administrator that can restart the system\nif it encounters a liveness bug. At the same time, liveness is challenging to\nachieve because any single participant could be malicious, and yet the overall\nsystem must make forward progress. While verification is a promising approach\nfor ensuring the absence of bugs, no prior work has been able to verify\nliveness for an executable implementation of PBFT.\n  Shipwright is a verification framework for proving correctness and liveness\nof distributed systems where some participants might be malicious. Shipwright\nintroduces three techniques that enable formal reasoning about decentralized\nsettings with malicious participants, allow developers to decompose their\nsystem and proof in a modular fashion into sub-protocols and sub-proofs, and\nsupport sound reasoning about cryptographic signatures that may be embedded in\nmessages. We used Shipwright to implement and verify an initial prototype of\nagreement on a single log entry in PBFT (with a few limitations) and translate\nit to an executable implementation in Go. We experimentally demonstrate its\noperation and liveness both in the common case and in several failure\nscenarios.", "AI": {"tldr": "Shipwright verifies liveness in PBFT despite malicious participants, enabling modular design and cryptographic reasoning. It was used to verify a PBFT prototype, proving its liveness in experiments.", "motivation": "Ensuring liveness in decentralized systems like PBFT is critical due to the absence of a single administrator to restart the system in case of bugs, and it is challenging because individual participants can be malicious while the system must still progress. Prior work has not been able to verify liveness for executable PBFT implementations.", "method": "Shipwright is a verification framework that introduces three techniques: enabling formal reasoning about decentralized settings with malicious participants, allowing modular decomposition of systems and proofs into sub-protocols and sub-proofs, and supporting sound reasoning about cryptographic signatures embedded in messages.", "result": "Shipwright was used to implement and verify a prototype of PBFT's single log entry agreement, demonstrating its operation and liveness experimentally in both common and failure scenarios.", "conclusion": "Shipwright successfully implemented and verified an initial prototype of agreement on a single log entry in PBFT, translating it to an executable implementation in Go, and experimentally demonstrating its operation and liveness in common and failure scenarios."}}
{"id": "2507.13457", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2507.13457", "abs": "https://arxiv.org/abs/2507.13457", "authors": ["Zixuan Huo", "Yangchao Shen", "Xiao Yuan", "Xiao-Ming Zhang"], "title": "Scalable suppression of heating errors in large trapped-ion quantum processors", "comment": "16 pages, 7 figures", "summary": "Trapped-ion processors are leading candidates for scalable quantum\ncomputation. However, motional heating remains a key obstacle to fault-tolerant\noperation, especially when system size increases. Heating error is particularly\nchallenging to suppress due to is incoherence nature, and no general methods\ncurrently exist for mitigating their impact even in systems with more than two\nions. In this work, based on a careful analysis about the dependence of\nheating-induced infidelity on phase-space trajectories, we present a simple yet\ncomprehensive framework for suppressing heating errors in large trapped-ion\nquantum processors. Our approach is flexible, allowing various control pulse\nbases, ion numbers, and noise levels. Our approach is also compatible with\nexisting error-mitigation techniques, including those targeting laser phase and\nfrequency noise. Crucially, it relies on an efficiently computable cost\nfunction that avoids the exponential overhead of full fidelity estimation. We\nperform numerical simulations for systems with up to 55 qubits, demonstrating\nup to an order-of-magnitude reduction in infidelities. These results offer a\npractical route toward robust, large-scale quantum computation with trapped\nions.", "AI": {"tldr": "\u4e3a\u5927\u578b\u79bb\u5b50\u9631\u91cf\u5b50\u5904\u7406\u5668\u63d0\u4f9b\u4e86\u4e00\u79cd\u6291\u5236\u70ed\u566a\u58f0\u8bef\u5dee\u7684\u7b80\u5355\u800c\u5168\u9762\u7684\u6846\u67b6\uff0c\u53ef\u5b9e\u73b0\u9ad8\u8fbe\u5341\u500d\u7684\u5931\u771f\u51cf\u5c11\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u53ef\u6269\u5c55\u91cf\u5b50\u8ba1\u7b97\u4e2d\u79bb\u5b50\u9631\u91cf\u5b50\u5904\u7406\u5668\u8fd0\u52a8\u52a0\u70ed\u4f5c\u4e3a\u5bb9\u9519\u64cd\u4f5c\u7684\u5173\u952e\u969c\u788d\uff0c\u7279\u522b\u662f\u968f\u7740\u7cfb\u7edf\u89c4\u6a21\u7684\u589e\u52a0\uff0c\u4ee5\u53ca\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6291\u5236\u5176\u4e0d\u76f8\u5e72\u7684\u6027\u8d28\u3002", "method": "\u901a\u8fc7\u5bf9\u70ed\u566a\u58f0\u5f15\u8d77\u7684\u5931\u771f\u4e0e\u76f8\u7a7a\u95f4\u8f68\u8ff9\u7684\u4f9d\u8d56\u6027\u8fdb\u884c\u8be6\u7ec6\u5206\u6790\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u6291\u5236\u70ed\u566a\u58f0\u8bef\u5dee\u7684\u6846\u67b6\u3002", "result": "\u5728\u6700\u591a55\u4e2a\u91cf\u5b50\u6bd4\u7279\u7684\u7cfb\u7edf\u4e2d\uff0c\u6570\u503c\u6a21\u62df\u663e\u793a\u5931\u771f\u6700\u591a\u53ef\u51cf\u5c11\u4e00\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u6291\u5236\u5927\u578b\u79bb\u5b50\u9631\u91cf\u5b50\u5904\u7406\u5668\u4e2d\u70ed\u566a\u58f0\u8bef\u5dee\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6570\u503c\u6a21\u62df\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u5e76\u4e3a\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u79bb\u5b50\u9631\u91cf\u5b50\u8ba1\u7b97\u63d0\u4f9b\u4e86\u5b9e\u9645\u53ef\u884c\u7684\u65b9\u6cd5\u3002"}}
{"id": "2507.13413", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13413", "abs": "https://arxiv.org/abs/2507.13413", "authors": ["Aleksey Lapin", "Igor Hromov", "Stanislav Chumakov", "Mile Mitrovic", "Dmitry Simakov", "Nikolay O. Nikitin", "Andrey V. Savchenko"], "title": "LightAutoDS-Tab: Multi-AutoML Agentic System for Tabular Data", "comment": "11 pages, 2 figures", "summary": "AutoML has advanced in handling complex tasks using the integration of LLMs,\nyet its efficiency remains limited by dependence on specific underlying tools.\nIn this paper, we introduce LightAutoDS-Tab, a multi-AutoML agentic system for\ntasks with tabular data, which combines an LLM-based code generation with\nseveral AutoML tools. Our approach improves the flexibility and robustness of\npipeline design, outperforming state-of-the-art open-source solutions on\nseveral data science tasks from Kaggle. The code of LightAutoDS-Tab is\navailable in the open repository https://github.com/sb-ai-lab/LADS", "AI": {"tldr": "LightAutoDS-Tab\u662f\u4e00\u4e2a\u591aAutoML\u4ee3\u7406\u7cfb\u7edf\uff0c\u901a\u8fc7\u7ed3\u5408LLM\u4ee3\u7801\u751f\u6210\u548c\u591a\u79cdAutoML\u5de5\u5177\uff0c\u63d0\u9ad8\u4e86\u8868\u683c\u6570\u636e\u4efb\u52a1\u7684\u5904\u7406\u6548\u7387\u548c\u9c81\u68d2\u6027\uff0c\u5e76\u5728Kaggle\u7ade\u8d5b\u4e2d\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u5f00\u6e90\u65b9\u6848\u7684\u6210\u679c\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709AutoML\u5728\u5904\u7406\u590d\u6742\u4efb\u52a1\u65f6\uff0c\u6548\u7387\u53d7\u9650\u4e8e\u7279\u5b9a\u5e95\u5c42\u5de5\u5177\u4f9d\u8d56\u6027\u7684\u95ee\u9898\u3002", "method": "LightAutoDS-Tab\u662f\u4e00\u4e2a\u591aAutoML\u4ee3\u7406\u7cfb\u7edf\uff0c\u5229\u7528\u57fa\u4e8eLLM\u7684\u4ee3\u7801\u751f\u6210\u548c\u591a\u4e2aAutoML\u5de5\u5177\u6765\u5904\u7406\u8868\u683c\u6570\u636e\u4efb\u52a1\u3002", "result": "LightAutoDS-Tab\u5728Kaggle\u7684\u591a\u4e2a\u6570\u636e\u79d1\u5b66\u4efb\u52a1\u4e0a\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u5f00\u6e90\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u7075\u6d3b\u6027\u3001\u9c81\u68d2\u6027\u548c\u6574\u4f53\u6027\u80fd\u4e0a\u5747\u6709\u63d0\u5347\u3002", "conclusion": "LightAutoDS-Tab\u901a\u8fc7\u7ed3\u5408\u57fa\u4e8eLLM\u7684\u4ee3\u7801\u751f\u6210\u548c\u591a\u4e2aAutoML\u5de5\u5177\uff0c\u5728\u5904\u7406\u8868\u683c\u6570\u636e\u4efb\u52a1\u65f6\u63d0\u9ad8\u4e86\u7075\u6d3b\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5728Kaggle\u7684\u591a\u4e2a\u6570\u636e\u79d1\u5b66\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u5f00\u6e90\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.13398", "categories": ["cs.SI", "physics.soc-ph"], "pdf": "https://arxiv.org/pdf/2507.13398", "abs": "https://arxiv.org/abs/2507.13398", "authors": ["Elisabeth H\u00f6ldrich", "Mathias Angermaier", "Jana Lasser", "Joao Pinheiro-Neto"], "title": "Characterizing the Dynamics of Conspiracy Related German Telegram Conversations during COVID-19", "comment": "24 pages, 7 figures, 4 tables", "summary": "Conspiracy theories have long drawn public attention, but their explosive\ngrowth on platforms like Telegram during the COVID-19 pandemic raises pressing\nquestions about their impact on societal trust, democracy, and public health.\nWe provide a geographical, temporal and network analysis of the structure of of\nconspiracy-related German-language Telegram chats in a novel large-scale data\nset. We examine how information flows between regional user groups and\ninfluential broadcasting channels, revealing the interplay between\ndecentralized discussions and content spread driven by a small number of key\nactors.\n  Our findings reveal that conspiracy-related activity spikes during major\nCOVID-19-related events, correlating with societal stressors and mirroring\nprior research on how crises amplify conspiratorial beliefs. By analysing the\ninterplay between regional, national and transnational chats, we uncover how\ninformation flows from larger national or transnational discourse to localised,\ncommunity-driven discussions. Furthermore, we find that the top 10% of chats\naccount for 94% of all forwarded content, portraying the large influence of a\nfew actors in disseminating information. However, these chats operate\nindependently, with minimal interconnection between each other, primarily\nforwarding messages to low-traffic groups. Notably, 43% of links shared in the\ndata set point to untrustworthy sources as identified by NewsGuard, a\nproportion far exceeding their share on other platforms and in other discourse\ncontexts, underscoring the role of conspiracy-related discussions on Telegram\nas vector for the spread of misinformation.", "AI": {"tldr": "COVID-19\u671f\u95f4\uff0cTelegram\u4e0a\u7684\u5fb7\u56fd\u9634\u8c0b\u8bba\u7fa4\u7ec4\u6fc0\u589e\uff0c\u6d3b\u52a8\u4e0e\u75ab\u60c5\u4e8b\u4ef6\u548c\u793e\u4f1a\u538b\u529b\u76f8\u5173\u3002\u4fe1\u606f\u4f20\u64ad\u5448\u5c11\u6570\u7fa4\u7ec4\u4e3b\u5bfc\u4f46\u7fa4\u7ec4\u95f4\u8054\u7cfb\u5f31\u7684\u7279\u70b9\u3002\u5927\u91cf\u94fe\u63a5\u6307\u5411\u4e0d\u53ef\u4fe1\u6765\u6e90\uff0c\u51f8\u663e\u4e86\u5176\u4f5c\u4e3a\u865a\u5047\u4fe1\u606f\u4f20\u64ad\u6e20\u9053\u7684\u4f5c\u7528\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u63a2\u8ba8COVID-19\u5927\u6d41\u884c\u671f\u95f4Telegram\u5e73\u53f0\u4e0a\u7684\u9634\u8c0b\u8bba\u7684\u7206\u70b8\u6027\u589e\u957f\uff0c\u4ee5\u53ca\u5176\u5bf9\u793e\u4f1a\u4fe1\u4efb\u3001\u6c11\u4e3b\u548c\u516c\u5171\u5065\u5eb7\u7684\u5f71\u54cd\u3002", "method": "\u672c\u7814\u7a76\u5bf9\u5fb7\u56fd\u8bed\u8a00\u7684Telegram\u9634\u8c0b\u8bba\u804a\u5929\u7fa4\u6570\u636e\u96c6\u8fdb\u884c\u4e86\u5730\u7406\u3001\u65f6\u95f4\u548c\u7f51\u7edc\u5206\u6790\uff0c\u7814\u7a76\u4e86\u4fe1\u606f\u5982\u4f55\u5728\u533a\u57df\u7528\u6237\u7fa4\u548c\u6709\u5f71\u54cd\u529b\u7684\u5e7f\u64ad\u9891\u9053\u4e4b\u95f4\u6d41\u52a8\uff0c\u63ed\u793a\u4e86\u53bb\u4e2d\u5fc3\u5316\u8ba8\u8bba\u4e0e\u5c11\u6570\u5173\u952e\u884c\u4e3a\u8005\u9a71\u52a8\u7684\u5185\u5bb9\u4f20\u64ad\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u9634\u8c0b\u8bba\u6d3b\u52a8\u5728\u91cd\u5927\u7684COVID-19\u76f8\u5173\u4e8b\u4ef6\u671f\u95f4\u6fc0\u589e\uff0c\u4e0e\u793e\u4f1a\u538b\u529b\u76f8\u5173\u3002\u4fe1\u606f\u4ece\u66f4\u5927\u8303\u56f4\u7684\u8ba8\u8bba\u6d41\u5411\u5730\u65b9\u6027\u8ba8\u8bba\u3002\u6392\u540d\u524d10%\u7684\u804a\u5929\u7fa4\u8d21\u732e\u4e8694%\u7684\u8f6c\u53d1\u5185\u5bb9\uff0c\u4f46\u5b83\u4eec\u4e4b\u95f4\u51e0\u4e4e\u6ca1\u6709\u76f8\u4e92\u8054\u7cfb\u300243%\u7684\u5206\u4eab\u94fe\u63a5\u6307\u5411\u4e0d\u53ef\u4fe1\u6765\u6e90\uff0c\u8868\u660eTelegram\u4e0a\u7684\u9634\u8c0b\u8bba\u8ba8\u8bba\u662f\u9519\u8bef\u4fe1\u606f\u4f20\u64ad\u7684\u5a92\u4ecb\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u5728COVID-19\u5927\u6d41\u884c\u671f\u95f4\uff0cTelegram\u5e73\u53f0\u4e0a\u7684\u9634\u8c0b\u8bba\u76f8\u5173\u804a\u5929\u5185\u5bb9\uff0c\u91cd\u70b9\u5173\u6ce8\u4e86\u4fe1\u606f\u4f20\u64ad\u7684\u5730\u7406\u3001\u65f6\u95f4\u548c\u7f51\u7edc\u7ed3\u6784\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u9634\u8c0b\u8bba\u6d3b\u52a8\u5728\u91cd\u5927\u75ab\u60c5\u4e8b\u4ef6\u671f\u95f4\u6fc0\u589e\uff0c\u5e76\u4e0e\u793e\u4f1a\u538b\u529b\u76f8\u5173\u3002\u4fe1\u606f\u4ece\u56fd\u5bb6\u6216\u8de8\u56fd\u8ba8\u8bba\u6d41\u5411\u5730\u65b9\u6027\u793e\u533a\u8ba8\u8bba\uff0c\u5c11\u6570\u5173\u952e\u804a\u5929\u7fa4\uff08\u536010%\uff09\u4f20\u64ad\u4e8694%\u7684\u8f6c\u53d1\u5185\u5bb9\uff0c\u4f46\u8fd9\u4e9b\u7fa4\u7ec4\u4e4b\u95f4\u8054\u7cfb\u5f88\u5c11\u3002\u6b64\u5916\uff0c43%\u7684\u5206\u4eab\u94fe\u63a5\u6307\u5411\u4e0d\u53ef\u4fe1\u6765\u6e90\uff0c\u8868\u660eTelegram\u4e0a\u7684\u9634\u8c0b\u8bba\u8ba8\u8bba\u662f\u9519\u8bef\u4fe1\u606f\u4f20\u64ad\u7684\u5a92\u4ecb\u3002"}}
{"id": "2507.13637", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.13637", "abs": "https://arxiv.org/abs/2507.13637", "authors": ["Jun Jiang", "Yuan Gao", "Xinyi Wu", "Shugong Xu"], "title": "Towards channel foundation models (CFMs): Motivations, methodologies and opportunities", "comment": "13 pages", "summary": "Artificial intelligence (AI) has emerged as a pivotal enabler for\nnext-generation wireless communication systems. However, conventional AI-based\nmodels encounter several limitations, such as heavy reliance on labeled data,\nlimited generalization capability, and task-specific design. To address these\nchallenges, this paper introduces, for the first time, the concept of channel\nfoundation models (CFMs)-a novel and unified framework designed to tackle a\nwide range of channel-related tasks through a pretrained, universal channel\nfeature extractor. By leveraging advanced AI architectures and self-supervised\nlearning techniques, CFMs are capable of effectively exploiting large-scale\nunlabeled data without the need for extensive manual annotation. We further\nanalyze the evolution of AI methodologies, from supervised learning and\nmulti-task learning to self-supervised learning, emphasizing the distinct\nadvantages of the latter in facilitating the development of CFMs. Additionally,\nwe provide a comprehensive review of existing studies on self-supervised\nlearning in this domain, categorizing them into generative, discriminative and\nthe combined paradigms. Given that the research on CFMs is still at an early\nstage, we identify several promising future research directions, focusing on\nmodel architecture innovation and the construction of high-quality, diverse\nchannel datasets.", "AI": {"tldr": "\u63d0\u51fa\u4fe1\u9053\u57fa\u7840\u6a21\u578b\uff08CFM\uff09\uff0c\u5229\u7528\u81ea\u76d1\u7763\u5b66\u4e60\u5904\u7406\u65e0\u7ebf\u901a\u4fe1\u4e2d\u7684\u4fe1\u9053\u76f8\u5173\u4efb\u52a1\u3002", "motivation": "\u4e3a\u4e86\u514b\u670d\u4f20\u7edfAI\u6a21\u578b\u5728\u65e0\u7ebf\u901a\u4fe1\u9886\u57df\u4f9d\u8d56\u6807\u7b7e\u6570\u636e\u3001\u6cdb\u5316\u80fd\u529b\u6709\u9650\u548c\u4efb\u52a1\u7279\u5b9a\u8bbe\u8ba1\u7684\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u76d1\u7763\u5b66\u4e60\u7684\u4fe1\u9053\u57fa\u7840\u6a21\u578b\uff08CFM\uff09\uff0c\u8be5\u6a21\u578b\u5305\u542b\u4e00\u4e2a\u9884\u8bad\u7ec3\u7684\u901a\u7528\u4fe1\u9053\u7279\u5f81\u63d0\u53d6\u5668\uff0c\u80fd\u591f\u5229\u7528\u5927\u89c4\u6a21\u65e0\u6807\u7b7e\u6570\u636e\u3002", "result": "CFM\u80fd\u591f\u6709\u6548\u5730\u5229\u7528\u5927\u89c4\u6a21\u65e0\u6807\u7b7e\u6570\u636e\uff0c\u65e0\u9700\u5e7f\u6cdb\u7684\u624b\u52a8\u6ce8\u91ca\uff0c\u5e76\u80fd\u5904\u7406\u5e7f\u6cdb\u7684\u4fe1\u9053\u76f8\u5173\u4efb\u52a1\u3002", "conclusion": "CFM\u662f\u672a\u6765\u65e0\u7ebf\u901a\u4fe1\u4e2d\u5904\u7406\u4e0e\u4fe1\u9053\u76f8\u5173\u4efb\u52a1\u7684\u901a\u7528\u6846\u67b6\u3002"}}
{"id": "2507.13363", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13363", "abs": "https://arxiv.org/abs/2507.13363", "authors": ["Atharv Goel", "Mehar Khurana"], "title": "Just Add Geometry: Gradient-Free Open-Vocabulary 3D Detection Without Human-in-the-Loop", "comment": null, "summary": "Modern 3D object detection datasets are constrained by narrow class\ntaxonomies and costly manual annotations, limiting their ability to scale to\nopen-world settings. In contrast, 2D vision-language models trained on\nweb-scale image-text pairs exhibit rich semantic understanding and support\nopen-vocabulary detection via natural language prompts. In this work, we\nleverage the maturity and category diversity of 2D foundation models to perform\nopen-vocabulary 3D object detection without any human-annotated 3D labels.\n  Our pipeline uses a 2D vision-language detector to generate text-conditioned\nproposals, which are segmented with SAM and back-projected into 3D using camera\ngeometry and either LiDAR or monocular pseudo-depth. We introduce a geometric\ninflation strategy based on DBSCAN clustering and Rotating Calipers to infer 3D\nbounding boxes without training. To simulate adverse real-world conditions, we\nconstruct Pseudo-nuScenes, a fog-augmented, RGB-only variant of the nuScenes\ndataset.\n  Experiments demonstrate that our method achieves competitive localization\nperformance across multiple settings, including LiDAR-based and purely RGB-D\ninputs, all while remaining training-free and open-vocabulary. Our results\nhighlight the untapped potential of 2D foundation models for scalable 3D\nperception. We open-source our code and resources at\nhttps://github.com/atharv0goel/open-world-3D-det.", "AI": {"tldr": "\u5229\u75282D\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u65e0\u9700\u8bad\u7ec3\u7684\u5f00\u653e\u8bcd\u6c473D\u76ee\u6807\u68c0\u6d4b\uff0c\u5e76\u5728\u4f2anuScenes\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u76843D\u76ee\u6807\u68c0\u6d4b\u6570\u636e\u96c6\u7c7b\u522b\u6709\u9650\u4e14\u6807\u6ce8\u6210\u672c\u9ad8\uff0c\u9650\u5236\u4e86\u5176\u5728\u5f00\u653e\u4e16\u754c\u573a\u666f\u4e0b\u7684\u6269\u5c55\u6027\u3002\u800c2D\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5177\u6709\u4e30\u5bcc\u7684\u8bed\u4e49\u7406\u89e3\u80fd\u529b\uff0c\u652f\u6301\u5f00\u653e\u8bcd\u6c47\u68c0\u6d4b\u3002", "method": "\u5229\u75282D\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u751f\u6210\u6587\u672c\u6761\u4ef6\u5efa\u8bae\uff0c\u5e76\u901a\u8fc7SAM\u8fdb\u884c\u5206\u5272\uff0c\u518d\u5229\u7528\u76f8\u673a\u51e0\u4f55\u548cLiDAR\u6216\u5355\u76ee\u4f2a\u6df1\u5ea6\u8fdb\u884c3D\u6295\u5f71\u3002\u5f15\u5165\u57fa\u4e8eDBSCAN\u805a\u7c7b\u548c\u65cb\u8f6c\u5361\u5c3a\u7684\u51e0\u4f55\u81a8\u80c0\u7b56\u7565\u8fdb\u884c3D\u8fb9\u754c\u6846\u63a8\u65ad\uff0c\u65e0\u9700\u8bad\u7ec3\u3002\u6784\u5efa\u4e86\u4f2anuScenes\u6570\u636e\u96c6\u6765\u6a21\u62df\u771f\u5b9e\u4e16\u754c\u7684\u6076\u52a3\u6761\u4ef6\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u8bbe\u7f6e\u4e0b\uff08\u5305\u62ec\u57fa\u4e8eLiDAR\u548c\u7eafRGB-D\u8f93\u5165\uff09\u5b9e\u73b0\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u5b9a\u4f4d\u6027\u80fd\uff0c\u4e14\u65e0\u9700\u8bad\u7ec3\u4e14\u652f\u6301\u5f00\u653e\u8bcd\u6c47\u3002", "conclusion": "2D\u57fa\u7840\u6a21\u578b\u57283D\u611f\u77e5\u9886\u57df\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u80fd\u591f\u5b9e\u73b0\u65e0\u9700\u8bad\u7ec3\u7684\u5f00\u653e\u8bcd\u6c473D\u76ee\u6807\u68c0\u6d4b\u3002"}}
{"id": "2507.13625", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13625", "abs": "https://arxiv.org/abs/2507.13625", "authors": ["Yuxin Zhang", "Xi Wang", "Mo Hu", "Zhenyu Zhang"], "title": "BifrostRAG: Bridging Dual Knowledge Graphs for Multi-Hop Question Answering in Construction Safety", "comment": "19 pages, 13 figures", "summary": "Information retrieval and question answering from safety regulations are\nessential for automated construction compliance checking but are hindered by\nthe linguistic and structural complexity of regulatory text. Many\ncompliance-related queries are multi-hop, requiring synthesis of information\nacross interlinked clauses. This poses a challenge for traditional\nretrieval-augmented generation (RAG) systems. To overcome this, we introduce\nBifrostRAG: a dual-graph RAG-integrated system that explicitly models both\nlinguistic relationships (via an Entity Network Graph) and document structure\n(via a Document Navigator Graph). This architecture powers a hybrid retrieval\nmechanism that combines graph traversal with vector-based semantic search,\nenabling large language models to reason over both the meaning and the\nstructure of the text. Evaluation on a multi-hop question dataset shows that\nBifrostRAG achieves 92.8 percent precision, 85.5 percent recall, and an F1\nscore of 87.3 percent. These results significantly outperform vector-only and\ngraph-only RAG baselines that represent current leading approaches. Error\nanalysis further highlights the comparative advantages of our hybrid method\nover single-modality RAGs. These findings establish BifrostRAG as a robust\nknowledge engine for LLM-driven compliance checking. Its dual-graph, hybrid\nretrieval mechanism offers a transferable blueprint for navigating complex\ntechnical documents across knowledge-intensive engineering domains.", "AI": {"tldr": "BifrostRAG\u662f\u4e00\u79cd\u65b0\u7684\u53cc\u56feRAG\u7cfb\u7edf\uff0c\u901a\u8fc7\u7ed3\u5408\u5b9e\u4f53\u5173\u7cfb\u548c\u6587\u6863\u7ed3\u6784\uff0c\u63d0\u9ad8\u4e86\u5728\u590d\u6742\u6cd5\u89c4\u6587\u672c\u4e0a\u8fdb\u884c\u4fe1\u606f\u68c0\u7d22\u548c\u95ee\u7b54\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u76ee\u524d\u7684\u5408\u89c4\u6027\u68c0\u67e5\u7cfb\u7edf\u5728\u5904\u7406\u6cd5\u89c4\u6587\u672c\u7684\u8bed\u8a00\u548c\u7ed3\u6784\u590d\u6742\u6027\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u9700\u8981\u8de8\u8d8a\u591a\u4e2a\u76f8\u4e92\u5173\u8054\u6761\u6b3e\u7efc\u5408\u4fe1\u606f\u7684\u590d\u6742\u67e5\u8be2\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aBifrostRAG\u7684\u53cc\u56feRAG\u96c6\u6210\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u663e\u5f0f\u5730\u5bf9\u8bed\u8a00\u5173\u7cfb\uff08\u901a\u8fc7\u5b9e\u4f53\u7f51\u7edc\u56fe\uff09\u548c\u6587\u6863\u7ed3\u6784\uff08\u901a\u8fc7\u6587\u6863\u5bfc\u822a\u5668\u56fe\uff09\u8fdb\u884c\u5efa\u6a21\u3002\u8be5\u67b6\u6784\u652f\u6301\u4e00\u79cd\u6df7\u5408\u68c0\u7d22\u673a\u5236\uff0c\u7ed3\u5408\u4e86\u56fe\u904d\u5386\u548c\u57fa\u4e8e\u5411\u91cf\u7684\u8bed\u4e49\u641c\u7d22\uff0c\u4f7f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u63a8\u7406\u6587\u672c\u7684\u542b\u4e49\u548c\u7ed3\u6784\u3002", "result": "\u5728\u591a\u8df3\u95ee\u7b54\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cBifrostRAG\u5b9e\u73b0\u4e8692.8%\u7684\u7cbe\u786e\u7387\uff0c85.5%\u7684\u53ec\u56de\u7387\uff0c\u4ee5\u53ca87.3%\u7684F1\u5206\u6570\uff0c\u663e\u8457\u4f18\u4e8e\u4ec5\u57fa\u4e8e\u5411\u91cf\u6216\u4ec5\u57fa\u4e8e\u56fe\u7684RAG\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "BifrostRAG\u662f\u4e00\u4e2a\u5f3a\u5927\u7684\u77e5\u8bc6\u5f15\u64ce\uff0c\u80fd\u591f\u5b9e\u73b0LLM\u9a71\u52a8\u7684\u5408\u89c4\u6027\u68c0\u67e5\u3002\u5176\u53cc\u56fe\u6df7\u5408\u68c0\u7d22\u673a\u5236\u4e3a\u8de8\u77e5\u8bc6\u5bc6\u96c6\u578b\u5de5\u7a0b\u9886\u57df\u5bfc\u822a\u590d\u6742\u6280\u672f\u6587\u6863\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u8f6c\u79fb\u7684\u84dd\u56fe\u3002"}}
{"id": "2507.13917", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2507.13917", "abs": "https://arxiv.org/abs/2507.13917", "authors": ["Efstratios Geronikolakis", "Manos Kamarianakis", "Antonis Protopsaltis", "George Papagiannakis"], "title": "Neural-GASh: A CGA-based neural radiance prediction pipeline for real-time shading", "comment": "11 pages, 10 figures", "summary": "This paper presents Neural-GASh, a novel real-time shading pipeline for 3D\nmeshes, that leverages a neural radiance field architecture to perform\nimage-based rendering (IBR) using Conformal Geometric Algebra (CGA)-encoded\nvertex information as input. Unlike traditional Precomputed Radiance Transfer\n(PRT) methods, that require expensive offline precomputations, our learned\nmodel directly consumes CGA-based representations of vertex positions and\nnormals, enabling dynamic scene shading without precomputation. Integrated\nseamlessly into the Unity engine, Neural-GASh facilitates accurate shading of\nanimated and deformed 3D meshes - capabilities essential for dynamic,\ninteractive environments. The shading of the scene is implemented within Unity,\nwhere rotation of scene lights in terms of Spherical Harmonics is also\nperformed optimally using CGA. This neural field approach is designed to\ndeliver fast and efficient light transport simulation across diverse platforms,\nincluding mobile and VR, while preserving high rendering quality. Additionally,\nwe evaluate our method on scenes generated via 3D Gaussian splats, further\ndemonstrating the flexibility and robustness of Neural-GASh in diverse\nscenarios. Performance is evaluated in comparison to conventional PRT,\ndemonstrating competitive rendering speeds even with complex geometries.", "AI": {"tldr": "Neural-GASh \u662f\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u8f90\u5c04\u573a\u548c\u5171\u5f62\u51e0\u4f55\u4ee3\u6570\u7684\u65b0\u578b\u5b9e\u65f6\u6e32\u67d3\u7ba1\u7ebf\uff0c\u5b83\u65e0\u9700\u9884\u8ba1\u7b97\u5373\u53ef\u4e3a\u52a8\u6001\u548c\u53d8\u5f62\u7684 3D \u7f51\u683c\u63d0\u4f9b\u9ad8\u8d28\u91cf\u3001\u9ad8\u901f\u5ea6\u7684\u7740\u8272\uff0c\u9002\u7528\u4e8e\u79fb\u52a8\u548c VR \u7b49\u591a\u79cd\u5e73\u53f0\u3002", "motivation": "\u4f20\u7edf\u7684 PRT \u65b9\u6cd5\u9700\u8981\u6602\u8d35\u7684\u79bb\u7ebf\u9884\u8ba1\u7b97\uff0c\u9650\u5236\u4e86\u5176\u5728\u52a8\u6001\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002\u672c\u7814\u7a76\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u9884\u8ba1\u7b97\u5373\u53ef\u5bf9\u52a8\u753b\u548c\u53d8\u5f62\u7684 3D \u7f51\u683c\u8fdb\u884c\u7cbe\u786e\u7740\u8272\u7684\u65b9\u6cd5\uff0c\u4ee5\u9002\u5e94\u52a8\u6001\u3001\u4ea4\u4e92\u5f0f\u73af\u5883\u7684\u9700\u6c42\uff0c\u5e76\u80fd\u5728\u5305\u62ec\u79fb\u52a8\u548c VR \u5728\u5185\u7684\u591a\u79cd\u5e73\u53f0\u4e0a\u5b9e\u73b0\u5feb\u901f\u9ad8\u6548\u7684\u5149\u4f20\u8f93\u6a21\u62df\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a Neural-GASh \u7684\u65b0\u9896\u5b9e\u65f6\u6e32\u67d3\u7ba1\u7ebf\uff0c\u8be5\u7ba1\u7ebf\u5229\u7528\u795e\u7ecf\u8f90\u5c04\u573a\uff08NeRF\uff09\u67b6\u6784\uff0c\u5e76\u5c06\u5171\u5f62\u51e0\u4f55\u4ee3\u6570\uff08CGA\uff09\u7f16\u7801\u7684\u9876\u70b9\u4fe1\u606f\u4f5c\u4e3a\u8f93\u5165\uff0c\u4ee5\u5b9e\u73b0\u57fa\u4e8e\u56fe\u50cf\u7684\u6e32\u67d3\uff08IBR\uff09\u3002\u4e0e\u4f20\u7edf\u7684 PRT \u65b9\u6cd5\u4e0d\u540c\uff0cNeural-GASh \u65e0\u9700\u6602\u8d35\u7684\u79bb\u7ebf\u9884\u8ba1\u7b97\uff0c\u53ef\u4ee5\u76f4\u63a5\u5904\u7406\u57fa\u4e8e CGA \u7684\u9876\u70b9\u4f4d\u7f6e\u548c\u6cd5\u7ebf\u8868\u793a\uff0c\u4ece\u800c\u5b9e\u73b0\u52a8\u6001\u573a\u666f\u7684\u5b9e\u65f6\u7740\u8272\u3002\u8be5\u65b9\u6cd5\u5df2\u96c6\u6210\u5230 Unity \u5f15\u64ce\u4e2d\uff0c\u5e76\u4f7f\u7528 CGA \u4f18\u5316\u4e86\u7403\u8c10\u51fd\u6570\u7684\u573a\u666f\u5149\u65cb\u8f6c\u3002", "result": "Neural-GASh \u80fd\u591f\u5bf9\u52a8\u753b\u548c\u53d8\u5f62\u7684 3D \u7f51\u683c\u8fdb\u884c\u7cbe\u786e\u7740\u8272\uff0c\u5e76\u5b9e\u73b0\u4e86\u5feb\u901f\u7684\u5149\u4f20\u8f93\u6a21\u62df\u3002\u4e0e\u4f20\u7edf\u7684 PRT \u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5728\u6e32\u67d3\u901f\u5ea6\u4e0a\u5177\u6709\u7ade\u4e89\u529b\uff0c\u5373\u4f7f\u5904\u7406\u590d\u6742\u7684\u51e0\u4f55\u4f53\u4e5f\u80fd\u4fdd\u6301\u9ad8\u8d28\u91cf\u7684\u6e32\u67d3\u6548\u679c\u3002\u8be5\u65b9\u6cd5\u8fd8\u5c55\u793a\u4e86\u5728\u5904\u7406 3D \u9ad8\u65af\u6cfc\u6e85\uff083D Gaussian splats\uff09\u7b49\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u7075\u6d3b\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "Neural-GASh \u5b9e\u73b0\u4e86\u65e0\u9700\u9884\u8ba1\u7b97\u7684\u5b9e\u65f6\u6e32\u67d3\uff0c\u80fd\u591f\u5904\u7406\u52a8\u6001\u573a\u666f\u548c\u53d8\u5f62\u7f51\u683c\uff0c\u5e76\u4e14\u5728\u6e32\u67d3\u8d28\u91cf\u548c\u901f\u5ea6\u4e0a\u5177\u6709\u7ade\u4e89\u529b\uff0c\u9002\u7528\u4e8e\u5305\u62ec\u79fb\u52a8\u548c VR \u5728\u5185\u7684\u591a\u79cd\u5e73\u53f0\u3002"}}
{"id": "2507.13687", "categories": ["eess.SY", "cs.SY", "93C95, 93E35, 93E20", "H.4.1"], "pdf": "https://arxiv.org/pdf/2507.13687", "abs": "https://arxiv.org/abs/2507.13687", "authors": ["Ming Lei", "Shufan Wu"], "title": "Robust Probability Hypothesis Density Filtering: Theory and Algorithms", "comment": "This version is submitted and in review currently", "summary": "Multi-target tracking (MTT) serves as a cornerstone technology in information\nfusion, yet faces significant challenges in robustness and efficiency when\ndealing with model uncertainties, clutter interference, and target\ninteractions. Conventional approaches like Gaussian Mixture PHD (GM-PHD) and\nCardinalized PHD (CPHD) filters suffer from inherent limitations including\ncombinatorial explosion, sensitivity to birth/death process parameters, and\nnumerical instability. This study proposes an innovative minimax robust PHD\nfiltering framework with four key contributions: (1) A theoretically derived\nrobust GM-PHD recursion algorithm that achieves optimal worst-case error\ncontrol under bounded uncertainties; (2) An adaptive real-time parameter\nadjustment mechanism ensuring stability and error bounds; (3) A generalized\nheavy-tailed measurement likelihood function maintaining polynomial\ncomputational complexity; (4) A novel partition-based credibility weighting\nmethod for extended targets. The research not only establishes rigorous\nconvergence guarantees and proves the uniqueness of PHD solutions, but also\nverifies algorithmic equivalence with standard GM-PHD. Experimental results\ndemonstrate that in high-clutter environments, this method achieves a\nremarkable 32.4% reduction in OSPA error and 25.3% lower cardinality RMSE\ncompared to existing techniques, while maintaining real-time processing\ncapability at 15.3 milliseconds per step. This breakthrough lays a crucial\nfoundation for reliable MTT in safety-critical applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684minimax\u9c81\u68d2PHD\u6ee4\u6ce2\u6846\u67b6\uff0c\u901a\u8fc7\u9c81\u68d2\u7b97\u6cd5\u3001\u81ea\u9002\u5e94\u53c2\u6570\u8c03\u6574\u3001\u91cd\u5c3e\u4f3c\u7136\u51fd\u6570\u548c\u57fa\u4e8e\u5206\u533a\u7684\u52a0\u6743\u65b9\u6cd5\uff0c\u5728\u591a\u76ee\u6807\u8ddf\u8e2a\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u9c81\u68d2\u6027\u548c\u6548\u7387\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u89e3\u51b3\u591a\u76ee\u6807\u8ddf\u8e2a\uff08MTT\uff09\u5728\u5904\u7406\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u3001\u6742\u6ce2\u5e72\u6270\u548c\u76ee\u6807\u4ea4\u4e92\u65f6\u7684\u9c81\u68d2\u6027\u548c\u6548\u7387\u6311\u6218\uff0c\u514b\u670d\u4f20\u7edf\u65b9\u6cd5\uff08\u5982GM-PHD\u548cCPHD\uff09\u7684\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7406\u8bba\u4e0a\u63a8\u5bfc\u7684\u9c81\u68d2GM-PHD\u9012\u5f52\u7b97\u6cd5\uff0c\u91c7\u7528\u81ea\u9002\u5e94\u5b9e\u65f6\u53c2\u6570\u8c03\u6574\u673a\u5236\uff0c\u5e76\u5f15\u5165\u4e86\u5e7f\u4e49\u91cd\u5c3e\u6d4b\u91cf\u4f3c\u7136\u51fd\u6570\u548c\u57fa\u4e8e\u5206\u533a\u7684\u53ef\u4fe1\u5ea6\u52a0\u6743\u65b9\u6cd5\u3002", "result": "\u5728\u6742\u6ce2\u73af\u5883\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e8632.4%\u7684OSPA\u8bef\u5dee\u964d\u4f4e\u548c25.3%\u7684\u57fa\u6570RMSE\u964d\u4f4e\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6bcf\u6b6515.3\u6beb\u79d2\u7684\u5b9e\u65f6\u5904\u7406\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u65b0\u9896\u7684minimax\u9c81\u68d2PHD\u6ee4\u6ce2\u6846\u67b6\uff0c\u4e3a\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u591a\u76ee\u6807\u8ddf\u8e2a\u5960\u5b9a\u4e86\u5173\u952e\u57fa\u7840\u3002"}}
{"id": "2507.13647", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13647", "abs": "https://arxiv.org/abs/2507.13647", "authors": ["Minze Li", "Wei Zhao", "Ran Chen", "Mingqiang Wei"], "title": "Improved particle swarm optimization algorithm: multi-target trajectory optimization for swarm drones", "comment": "8 papers,7 figures", "summary": "Real-time trajectory planning for unmanned aerial vehicles (UAVs) in dynamic\nenvironments remains a key challenge due to high computational demands and the\nneed for fast, adaptive responses. Traditional Particle Swarm Optimization\n(PSO) methods, while effective for offline planning, often struggle with\npremature convergence and latency in real-time scenarios. To overcome these\nlimitations, we propose PE-PSO, an enhanced PSO-based online trajectory\nplanner. The method introduces a persistent exploration mechanism to preserve\nswarm diversity and an entropy-based parameter adjustment strategy to\ndynamically adapt optimization behavior. UAV trajectories are modeled using\nB-spline curves, which ensure path smoothness while reducing optimization\ncomplexity. To extend this capability to UAV swarms, we develop a multi-agent\nframework that combines genetic algorithm (GA)-based task allocation with\ndistributed PE-PSO, supporting scalable and coordinated trajectory generation.\nThe distributed architecture allows for parallel computation and decentralized\ncontrol, enabling effective cooperation among agents while maintaining\nreal-time performance. Comprehensive simulations demonstrate that the proposed\nframework outperforms conventional PSO and other swarm-based planners across\nseveral metrics, including trajectory quality, energy efficiency, obstacle\navoidance, and computation time. These results confirm the effectiveness and\napplicability of PE-PSO in real-time multi-UAV operations under complex\nenvironmental conditions.", "AI": {"tldr": "\u901a\u8fc7\u6539\u8fdb\u7684PSO\uff08PE-PSO\uff09\u548c\u57fa\u4e8eGA\u7684\u4efb\u52a1\u5206\u914d\u7684\u591a\u65e0\u4eba\u673a\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u65e0\u4eba\u673a\u5728\u52a8\u6001\u73af\u5883\u4e0b\u7684\u5b9e\u65f6\u8f68\u8ff9\u89c4\u5212\uff0c\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u65e0\u4eba\u673a\uff08UAV\uff09\u5728\u52a8\u6001\u73af\u5883\u4e2d\u8fdb\u884c\u5b9e\u65f6\u8f68\u8ff9\u89c4\u5212\u9762\u4e34\u8ba1\u7b97\u9700\u6c42\u9ad8\u3001\u54cd\u5e94\u901f\u5ea6\u5feb\u548c\u9002\u5e94\u6027\u5f3a\u7684\u6311\u6218\u3002\u4f20\u7edfPSO\u65b9\u6cd5\u5728\u5b9e\u65f6\u573a\u666f\u4e0b\u5b58\u5728\u8fc7\u65e9\u6536\u655b\u548c\u5ef6\u8fdf\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\"PE-PSO\"\u7684\u589e\u5f3a\u7c92\u5b50\u7fa4\u4f18\u5316\uff08PSO\uff09\u7b97\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u6301\u4e45\u63a2\u7d22\u673a\u5236\u548c\u57fa\u4e8e\u71b5\u7684\u53c2\u6570\u8c03\u6574\u7b56\u7565\u6765\u514b\u670d\u4f20\u7edfPSO\u7684\u5c40\u9650\u6027\u3002\u540c\u65f6\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u9057\u4f20\u7b97\u6cd5\uff08GA\uff09\u4efb\u52a1\u5206\u914d\u548c\u5206\u5e03\u5f0fPE-PSO\u7684\u591a\u65e0\u4eba\u673a\u6846\u67b6\uff0c\u4f7f\u7528B\u6837\u6761\u66f2\u7ebf\u6a21\u578b\u5316\u8f68\u8ff9\u4ee5\u4fdd\u8bc1\u5e73\u6ed1\u6027\u5e76\u964d\u4f4e\u590d\u6742\u5ea6\uff0c\u5b9e\u73b0\u53ef\u6269\u5c55\u548c\u534f\u8c03\u7684\u8f68\u8ff9\u751f\u6210\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u5206\u5e03\u5f0fPE-PSO\u6846\u67b6\u5728\u8f68\u8ff9\u8d28\u91cf\u3001\u80fd\u6548\u3001\u907f\u969c\u548c\u8ba1\u7b97\u65f6\u95f4\u7b49\u591a\u4e2a\u6307\u6807\u4e0a\u4f18\u4e8e\u4f20\u7edf\u7684PSO\u548c\u5176\u4ed6\u57fa\u4e8e\u7fa4\u4f53\u7684\u89c4\u5212\u5668\uff0c\u6709\u6548\u652f\u6301\u4e86\u590d\u6742\u73af\u5883\u4e0b\u7684\u5b9e\u65f6\u591a\u65e0\u4eba\u673a\u534f\u540c\u4f5c\u4e1a\u3002", "conclusion": "\"PE-PSO\" \u53ca\u5176\u591a\u65e0\u4eba\u673a\u6846\u67b6\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5b9e\u65f6\u8f68\u8ff9\u89c4\u5212\uff0c\u5728\u8f68\u8ff9\u8d28\u91cf\u3001\u80fd\u6548\u3001\u907f\u969c\u548c\u8ba1\u7b97\u65f6\u95f4\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u590d\u6742\u6761\u4ef6\u4e0b\u7684\u5b9e\u65f6\u591a\u65e0\u4eba\u673a\u4f5c\u4e1a\u80fd\u529b\u3002"}}
{"id": "2507.13635", "categories": ["quant-ph", "cs.LO", "cs.PL"], "pdf": "https://arxiv.org/pdf/2507.13635", "abs": "https://arxiv.org/abs/2507.13635", "authors": ["Nengkun Yu", "Jens Palsberg", "Thomas Reps"], "title": "SAQR-QC: A Logic for Scalable but Approximate Quantitative Reasoning about Quantum Circuits", "comment": "Comments are welcome", "summary": "Reasoning about quantum programs remains a fundamental challenge, regardless\nof the programming model or computational paradigm. Despite extensive research,\nexisting verification techniques are insufficient--even for quantum circuits, a\ndeliberately restricted model that lacks classical control, but still underpins\nmany current quantum algorithms. Many existing formal methods require\nexponential time and space to represent and manipulate (representations of)\nassertions and judgments, making them impractical for quantum circuits with\nmany qubits. This paper presents a logic for reasoning in such settings, called\nSAQR-QC. The logic supports Scalable but Approximate Quantitative Reasoning\nabout Quantum Circuits, whence the name. SAQR-QC has three characteristics: (i)\nsome (deliberate) loss of precision is built into it; (ii) it has a mechanism\nto help the accumulated loss of precision during a sequence of reasoning steps\nremain small; and (iii) most importantly, to make reasoning scalable, all\nreasoning steps are local--i.e., they each involve just a small number of\nqubits. We demonstrate the effectiveness of SAQR-QC via two case studies: the\nverification of GHZ circuits involving non-Clifford gates, and the analysis of\nquantum phase estimation--a core subroutine in Shor's factoring algorithm.", "AI": {"tldr": "SAQR-QC offers a scalable and approximate quantitative reasoning logic for quantum circuits, overcoming the limitations of existing methods by incorporating precision loss and local reasoning.", "motivation": "Existing verification techniques for quantum programs are insufficient and often require exponential time/space, especially for quantum circuits with many qubits.", "method": "SAQR-QC: A logic with built-in (deliberate) loss of precision, a mechanism to control accumulated precision loss, and local reasoning steps involving a small number of qubits.", "result": "Demonstrated the effectiveness of SAQR-QC through case studies on GHZ circuits (with non-Clifford gates) and quantum phase estimation.", "conclusion": "SAQR-QC is effective for verifying quantum circuits like GHZ circuits and quantum phase estimation, addressing scalability and precision issues."}}
{"id": "2507.13736", "categories": ["cs.LG", "cs.AR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2507.13736", "abs": "https://arxiv.org/abs/2507.13736", "authors": ["Matthias Jobst", "Tim Langer", "Chen Liu", "Mehmet Alici", "Hector A. Gonzalez", "Christian Mayr"], "title": "An End-to-End DNN Inference Framework for the SpiNNaker2 Neuromorphic MPSoC", "comment": "Poster at ACM ICONS 2025 - International Conference on Neuromorphic\n  Systems", "summary": "This work presents a multi-layer DNN scheduling framework as an extension of\nOctopuScheduler, providing an end-to-end flow from PyTorch models to inference\non a single SpiNNaker2 chip. Together with a front-end comprised of\nquantization and lowering steps, the proposed framework enables the edge-based\nexecution of large and complex DNNs up to transformer scale using the\nneuromorphic platform SpiNNaker2.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684 DNN \u8c03\u5ea6\u6846\u67b6\uff0c\u53ef\u4ee5\u5c06 PyTorch \u6a21\u578b\u90e8\u7f72\u5230 SpiNNaker2 \u82af\u7247\u4e0a\uff0c\u5b9e\u73b0\u8fb9\u7f18\u63a8\u7406\u3002", "motivation": "\u4e3a\u4e86\u5b9e\u73b0\u5c06\u5927\u578b\u590d\u6742 DNN\uff08\u8fbe Transformer \u89c4\u6a21\uff09\u5728 SpiNNaker2 \u8fd9\u4e00\u795e\u7ecf\u5f62\u6001\u5e73\u53f0\u4e0a\u8fdb\u884c\u8fb9\u7f18\u6267\u884c\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u5c42 DNN \u8c03\u5ea6\u6846\u67b6\uff0c\u4f5c\u4e3a OctopuScheduler \u7684\u6269\u5c55\uff0c\u5e76\u7ed3\u5408\u4e86\u91cf\u5316\u548c\u964d\u4f4e\uff08lowering\uff09\u7b49\u524d\u7aef\u6b65\u9aa4\u3002", "result": "\u6210\u529f\u5b9e\u73b0\u4e86\u4ece PyTorch \u6a21\u578b\u5230 SpiNNaker2 \u82af\u7247\u7684\u7aef\u5230\u7aef\u6d41\u7a0b\uff0c\u4f7f\u5f97\u590d\u6742\u7684 DNN \u80fd\u591f\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u8fd0\u884c\u3002", "conclusion": "\u8be5\u6846\u67b6\u5b9e\u73b0\u4e86\u5c06 PyTorch \u6a21\u578b\u90e8\u7f72\u5230 SpiNNaker2 \u82af\u7247\u4e0a\u8fdb\u884c\u8fb9\u7f18\u63a8\u7406\uff0c\u652f\u6301\u8fbe Transformer \u89c4\u6a21\u7684\u5927\u578b\u590d\u6742 DNN\u3002"}}
{"id": "2507.13791", "categories": ["cond-mat.mtrl-sci", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2507.13791", "abs": "https://arxiv.org/abs/2507.13791", "authors": ["Daviti Gochitashvili", "Maxwell Meyers", "Cindy Wang", "Aleksey N. Kolmogorov"], "title": "Improving structure search with hyperspatial optimization and TETRIS seeding", "comment": null, "summary": "Advanced structure prediction methods developed over the past decades include\nan unorthodox strategy of allowing atoms to displace into extra dimensions. A\nrecently implemented global optimization of structures from hyperspace (GOSH)\nhas shown promise in accelerating the identification of global minima on\npotential energy surfaces defined by simple interatomic models. In this study,\nwe extend the GOSH formalism to more accurate Behler-Parrinello neural network\n(NN) potentials, make it compatible with efficient local minimization\nalgorithms, and test its performance on nanoparticles and crystalline solids.\nFor clusters modeled with NN potentials, four-dimensional optimization offers\nfairly modest improvement in navigating geometric relaxation pathways and\nincurs increased computational cost largely offsetting the benefit, but it\nprovides a significant advantage in facilitating atom swaps in nanoalloys. In\ncomparison, the introduction of a moderate, controlled bias for generating more\nphysically sensible starting configurations, achieved via TETRIS-inspired\npacking of atomic blocks, has a more direct impact on the efficiency of global\nstructure searches. The benchmarked systems are Lennard-Jones clusters, Au or\nCu-Pd-Ag nanoparticles and binary Sn alloys described by NN potentials, and\ncompounds with covalent B or BC frameworks modeled with density functional\ntheory", "AI": {"tldr": "\u672c\u7814\u7a76\u5c06\u8d85\u7a7a\u95f4\u7ed3\u6784\u5168\u5c40\u4f18\u5316\uff08GOSH\uff09\u65b9\u6cd5\u6269\u5c55\u5230\u66f4\u7cbe\u786e\u7684\u795e\u7ecf\u7f51\u7edc\u52bf\uff0c\u5e76\u8bc4\u4f30\u4e86\u5176\u5728\u7eb3\u7c73\u7c92\u5b50\u548c\u6676\u4f53\u56fa\u4f53\u4e0a\u7684\u6027\u80fd\u3002\u7ed3\u679c\u8868\u660e\uff0c\u867d\u7136\u56db\u7ef4\u4f18\u5316\u5728\u67d0\u4e9b\u65b9\u9762\uff08\u5982\u7eb3\u7c73\u5408\u91d1\u7684\u539f\u5b50\u4ea4\u6362\uff09\u6709\u4f18\u52bf\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u589e\u52a0\uff1b\u800c\u5f15\u5165\u504f\u5dee\u4ee5\u751f\u6210\u66f4\u5408\u7406\u7684\u8d77\u59cb\u6784\u578b\u5219\u66f4\u6709\u6548\u5730\u63d0\u9ad8\u4e86\u5168\u5c40\u7ed3\u6784\u641c\u7d22\u7684\u6548\u7387\u3002", "motivation": "\u8fc7\u53bb\u7684\u51e0\u5341\u5e74\u91cc\uff0c\u5148\u8fdb\u7684\u7ed3\u6784\u9884\u6d4b\u65b9\u6cd5\u5305\u62ec\u4e00\u79cd\u975e\u4f20\u7edf\u7684\u7b56\u7565\uff0c\u5373\u5141\u8bb8\u539f\u5b50\u7f6e\u6362\u5230\u989d\u5916\u7684\u7ef4\u5ea6\u3002\u6700\u8fd1\u5b9e\u73b0\u7684\u8d85\u7a7a\u95f4\u7ed3\u6784\u5168\u5c40\u4f18\u5316\uff08GOSH\uff09\u5728\u52a0\u901f\u8bc6\u522b\u52bf\u80fd\u8868\u9762\u4e0a\u7684\u5168\u5c40\u6700\u5c0f\u503c\u65b9\u9762\u663e\u793a\u51fa\u6f5c\u529b\u3002", "method": "\u672c\u7814\u7a76\u5c06GOSH\u5f62\u5f0f\u4e3b\u4e49\u6269\u5c55\u5230\u66f4\u7cbe\u786e\u7684Behler-Parrinello\u795e\u7ecf\u7f51\u7edc\u52bf\uff0c\u5e76\u4f7f\u5176\u4e0e\u6709\u6548\u7684\u5c40\u90e8\u6700\u5c0f\u5316\u7b97\u6cd5\u517c\u5bb9\uff0c\u7136\u540e\u5728\u7eb3\u7c73\u7c92\u5b50\u548c\u6676\u4f53\u56fa\u4f53\u4e0a\u6d4b\u8bd5\u5176\u6027\u80fd\u3002\u6211\u4eec\u8fd8\u8bc4\u4f30\u4e86\u901a\u8fc7\u53d7TETRIS\u542f\u53d1\u7684\u539f\u5b50\u5757\u6253\u5305\u5b9e\u73b0\u7684\u3001\u7528\u4e8e\u751f\u6210\u66f4\u7b26\u5408\u7269\u7406\u5b9e\u9645\u7684\u8d77\u59cb\u6784\u578b\u7684\u9002\u5ea6\u53d7\u63a7\u504f\u5dee\u3002", "result": "\u5bf9\u4e8e\u5177\u6709\u795e\u7ecf\u7f51\u7edc\u52bf\u7684\u6a21\u578b\u7c07\uff0c\u56db\u7ef4\u4f18\u5316\u5728\u51e0\u4f55\u5f1b\u8c6b\u8def\u5f84\u5bfc\u822a\u65b9\u9762\u63d0\u4f9b\u7684\u6539\u8fdb\u76f8\u5f53\u6709\u9650\uff0c\u800c\u8ba1\u7b97\u6210\u672c\u7684\u589e\u52a0\u5374\u62b5\u6d88\u4e86\u5176\u5e26\u6765\u7684\u597d\u5904\u3002\u7136\u800c\uff0c\u5728\u4fc3\u8fdb\u7eb3\u7c73\u5408\u91d1\u4e2d\u7684\u539f\u5b50\u4ea4\u6362\u65b9\u9762\uff0c\u56db\u7ef4\u4f18\u5316\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u901a\u8fc7TETRIS\u542f\u53d1\u7684\u539f\u5b50\u5757\u6253\u5305\u5b9e\u73b0\u7684\u3001\u7528\u4e8e\u751f\u6210\u66f4\u7b26\u5408\u7269\u7406\u5b9e\u9645\u7684\u8d77\u59cb\u6784\u578b\u7684\u504f\u5dee\uff0c\u5bf9\u5168\u5c40\u7ed3\u6784\u641c\u7d22\u7684\u6548\u7387\u6709\u66f4\u76f4\u63a5\u7684\u5f71\u54cd\u3002", "conclusion": "\u4e0e\u7b80\u5355\u7684\u539f\u5b50\u6a21\u578b\u76f8\u6bd4\uff0c\u5728\u66f4\u7cbe\u786e\u7684Behler-Parrinello\u795e\u7ecf\u7f51\u7edc\u52bf\u4e2d\uff0c\u56db\u7ef4\u4f18\u5316\u5728\u51e0\u4f55\u5f1b\u8c6b\u8def\u5f84\u5bfc\u822a\u65b9\u9762\u4ec5\u5e26\u6765\u9002\u5ea6\u7684\u6539\u8fdb\uff0c\u5e76\u4e14\u8ba1\u7b97\u6210\u672c\u7684\u589e\u52a0\u5f88\u5927\u7a0b\u5ea6\u4e0a\u62b5\u6d88\u4e86\u5176\u4f18\u52bf\u3002\u7136\u800c\uff0c\u5b83\u5728\u4fc3\u8fdb\u7eb3\u7c73\u5408\u91d1\u4e2d\u7684\u539f\u5b50\u4ea4\u6362\u65b9\u9762\u63d0\u4f9b\u4e86\u663e\u8457\u4f18\u52bf\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u901a\u8fc7\u53d7TETRIS\u542f\u53d1\u7684\u539f\u5b50\u5757\u6253\u5305\u5b9e\u73b0\u7684\u3001\u7528\u4e8e\u751f\u6210\u66f4\u7b26\u5408\u7269\u7406\u5b9e\u9645\u7684\u8d77\u59cb\u6784\u578b\u7684\u9002\u5ea6\u53d7\u63a7\u504f\u5dee\uff0c\u5bf9\u5168\u5c40\u7ed3\u6784\u641c\u7d22\u7684\u6548\u7387\u4ea7\u751f\u4e86\u66f4\u76f4\u63a5\u7684\u5f71\u54cd\u3002"}}
{"id": "2507.13796", "categories": ["cond-mat.mes-hall", "quant-ph"], "pdf": "https://arxiv.org/pdf/2507.13796", "abs": "https://arxiv.org/abs/2507.13796", "authors": ["L. E. Golub", "E. L. Ivchenko"], "title": "Intraband circular photogalvanic effect in Weyl semimetals", "comment": "5+5 pages, 2 figures", "summary": "We apply the semiclassical theory including the Berry curvature dipole, side\njumps and skew scattering for a quantitative description of the circular\nphotogalvanic effect (CPGE) in Weyl semimetals at intraband absorption. In\ncontrast to gapped systems where they completely exhaust all contributions to\nthe CPGE current, all previously known semiclassical mechanisms give a result\ndifferent from that obtained using a complete quantum-mechanical approach. We\nshow that this difference in the existing quasiclassical and full\nquantum-mechanical approaches persists at all spatial ranges of the disorder\npotential. Apparently, the implementation of another microscopic mechanism into\nthe quasiclassical description of the CPGE is required.", "AI": {"tldr": "\u5728\u5e03\u5c14\u578b\u534a\u91d1\u5c5e\u4e2d\uff0cCPGE\u7684\u534a\u7ecf\u5178\u7406\u8bba\u4e0e\u91cf\u5b50\u529b\u5b66\u65b9\u6cd5\u5b58\u5728\u5dee\u5f02\uff0c\u9700\u8981\u65b0\u7684\u5fae\u89c2\u673a\u5236\u3002", "motivation": "\u5728\u6709\u9699\u7cfb\u7edf\u4e2d\uff0c\u8fd9\u4e9b\u673a\u5236\u53ef\u4ee5\u5b8c\u5168\u89e3\u91caCPGE\u7535\u6d41\u7684\u6240\u6709\u8d21\u732e\uff0c\u4f46\u4e0e\u5b8c\u6574\u7684\u91cf\u5b50\u529b\u5b66\u65b9\u6cd5\u5f97\u5230\u7684\u7ed3\u679c\u4e0d\u540c\u3002", "method": "\u5e94\u7528\u5305\u542b Berry \u6563\u5ea6\u5076\u6781\u5b50\u3001\u4fa7\u8df3\u548c\u659c\u6563\u5c04\u7684\u534a\u7ecf\u5178\u7406\u8bba\uff0c\u5bf9\u5e03\u5c14\u578b\u534a\u91d1\u5c5e\u5e26\u5185\u5438\u6536\u7684\u5706\u5149\u7535\u6d41\u6548\u5e94 (CPGE) \u8fdb\u884c\u5b9a\u91cf\u63cf\u8ff0\u3002", "result": "\u6240\u6709\u5148\u524d\u5df2\u77e5\u7684\u534a\u7ecf\u5178\u673a\u5236\u90fd\u4e0e\u5b8c\u6574\u7684\u91cf\u5b50\u529b\u5b66\u65b9\u6cd5\u5f97\u5230\u7684\u7ed3\u679c\u4e0d\u540c\u3002", "conclusion": "\u73b0\u6709\u51c6\u7ecf\u5178\u548c\u5168\u91cf\u5b50\u529b\u5b66\u65b9\u6cd5\u5728\u6240\u6709\u7a7a\u95f4\u8303\u56f4\u7684\u65e0\u5e8f\u52bf\u4e0b\u90fd\u5b58\u5728\u5dee\u5f02\uff0c\u9700\u8981\u5c06\u53e6\u4e00\u4e2a\u5fae\u89c2\u673a\u5236\u5f15\u5165\u5230CPGE\u7684\u51c6\u7ecf\u5178\u63cf\u8ff0\u4e2d\u3002"}}
{"id": "2507.13885", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2507.13885", "abs": "https://arxiv.org/abs/2507.13885", "authors": ["Masoud Seddighin", "Saeed Seddighin"], "title": "Quantum Pattern Matching with Wildcards", "comment": null, "summary": "Pattern matching is one of the fundamental problems in Computer Science. Both\nthe classic version of the problem as well as the more sophisticated version\nwhere wildcards can also appear in the input can be solved in almost linear\ntime $\\tilde O(n)$ using the KMP algorithm and Fast Fourier Transform,\nrespectively. In 2000, Ramesh and Vinay~\\cite{ramesh2003string} give a quantum\nalgorithm that solves classic pattern matching in sublinear time and asked\nwhether the wildcard problem can also be solved in sublinear time? In this\nwork, we give a quantum algorithm for pattern matching with wildcards that runs\nin time $\\tilde O(\\sqrt{n}\\sqrt{k})$ when the number of wildcards is bounded by\n$k$ for $k \\geq \\sqrt{n}$. This leads to an algorithm that runs in sublinear\ntime as long as the number of wildcards is sublinear.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u91cf\u5b50\u7b97\u6cd5\uff0c\u80fd\u591f\u4ee5\u4e9a\u7ebf\u6027\u65f6\u95f4\u590d\u6742\u5ea6\u89e3\u51b3\u5e26\u6709\u901a\u914d\u7b26\u7684\u6a21\u5f0f\u5339\u914d\u95ee\u9898\uff0c\u524d\u63d0\u662f\u901a\u914d\u7b26\u7684\u6570\u91cf\u4e5f\u662f\u4e9a\u7ebf\u6027\u7684\u3002", "motivation": "\u8be5\u7814\u7a76\u7684\u52a8\u673a\u662f\u89e3\u51b3\u5e26\u6709\u901a\u914d\u7b26\u7684\u6a21\u5f0f\u5339\u914d\u95ee\u9898\uff0c\u5e76\u63a2\u7a76\u5176\u662f\u5426\u80fd\u50cf\u7ecf\u5178\u6a21\u5f0f\u5339\u914d\u95ee\u9898\u4e00\u6837\uff0c\u5728\u4e9a\u7ebf\u6027\u65f6\u95f4\u590d\u6742\u5ea6\u5185\u89e3\u51b3\u3002\u7279\u522b\u662f\uff0c\u7814\u7a76\u8005\u53d7\u5230\u4e86 Ramesh \u548c Vinay \u5728 2000 \u5e74\u63d0\u51fa\u7684\u7ecf\u5178\u6a21\u5f0f\u5339\u914d\u91cf\u5b50\u7b97\u6cd5\u7684\u542f\u53d1\u3002", "method": "\u672c\u6587\u63d0\u51fa\u7684\u91cf\u5b50\u7b97\u6cd5\uff0c\u5176\u8fd0\u884c\u65f6\u95f4\u590d\u6742\u5ea6\u4e3a O(sqrt(n) * sqrt(k))\uff0c\u5176\u4e2d n \u662f\u8f93\u5165\u5b57\u7b26\u4e32\u7684\u957f\u5ea6\uff0ck \u662f\u901a\u914d\u7b26\u7684\u6570\u91cf\u3002\u8be5\u7b97\u6cd5\u9002\u7528\u4e8e k >= sqrt(n) \u7684\u60c5\u51b5\u3002", "result": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u91cf\u5b50\u7b97\u6cd5\uff0c\u5728\u901a\u914d\u7b26\u6570\u91cf k \u88ab\u9650\u5236\u5728 k >= sqrt(n) \u7684\u60c5\u51b5\u4e0b\uff0c\u80fd\u591f\u4ee5 O(sqrt(n) * sqrt(k)) \u7684\u65f6\u95f4\u590d\u6742\u5ea6\u89e3\u51b3\u5e26\u6709\u901a\u914d\u7b26\u7684\u6a21\u5f0f\u5339\u914d\u95ee\u9898\u3002\u5f53\u901a\u914d\u7b26\u6570\u91cf k \u4e3a\u4e9a\u7ebf\u6027\u65f6\uff0c\u8be5\u7b97\u6cd5\u7684\u8fd0\u884c\u65f6\u95f4\u4e5f\u4e3a\u4e9a\u7ebf\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u91cf\u5b50\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u5e26\u6709\u901a\u914d\u7b26\u7684\u6a21\u5f0f\u5339\u914d\u95ee\u9898\uff0c\u5176\u8fd0\u884c\u65f6\u95f4\u4e3a O(sqrt(n) * sqrt(k))\uff0c\u5176\u4e2d k \u662f\u901a\u914d\u7b26\u7684\u6570\u91cf\uff0c\u5f53 k >= sqrt(n) \u65f6\u3002\u8fd9\u4f7f\u5f97\u5728\u901a\u914d\u7b26\u6570\u91cf\u4e3a\u4e9a\u7ebf\u6027\u65f6\uff0c\u7b97\u6cd5\u7684\u8fd0\u884c\u65f6\u95f4\u4e3a\u4e9a\u7ebf\u6027\u3002"}}
{"id": "2507.13392", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13392", "abs": "https://arxiv.org/abs/2507.13392", "authors": ["Emil H\u00e4glund", "Johanna Bj\u00f6rklund"], "title": "TopicImpact: Improving Customer Feedback Analysis with Opinion Units for Topic Modeling and Star-Rating Prediction", "comment": null, "summary": "We improve the extraction of insights from customer reviews by restructuring\nthe topic modelling pipeline to operate on opinion units - distinct statements\nthat include relevant text excerpts and associated sentiment scores. Prior work\nhas demonstrated that such units can be reliably extracted using large language\nmodels. The result is a heightened performance of the subsequent topic\nmodeling, leading to coherent and interpretable topics while also capturing the\nsentiment associated with each topic. By correlating the topics and sentiments\nwith business metrics, such as star ratings, we can gain insights on how\nspecific customer concerns impact business outcomes. We present our system's\nimplementation, use cases, and advantages over other topic modeling and\nclassification solutions. We also evaluate its effectiveness in creating\ncoherent topics and assess methods for integrating topic and sentiment\nmodalities for accurate star-rating prediction.", "AI": {"tldr": "\u901a\u8fc7\u5728\u610f\u89c1\u5355\u5143\u4e0a\u64cd\u4f5c\u91cd\u6784\u4e3b\u9898\u5efa\u6a21\u7ba1\u9053\uff0c\u4ee5\u63d0\u9ad8\u4ece\u5ba2\u6237\u8bc4\u8bba\u4e2d\u63d0\u53d6\u89c1\u89e3\u7684\u8868\u73b0\uff0c\u5e76\u5206\u6790\u5176\u5bf9\u4e1a\u52a1\u6307\u6807\u7684\u5f71\u54cd\u3002", "motivation": "\u63d0\u9ad8\u4ece\u5ba2\u6237\u8bc4\u8bba\u4e2d\u63d0\u53d6\u89c1\u89e3\u7684\u8868\u73b0\u3002", "method": "\u5c06\u4e3b\u9898\u5efa\u6a21\u7ba1\u9053\u91cd\u6784\u4e3a\u5728\u610f\u89c1\u5355\u5143\u4e0a\u64cd\u4f5c\uff0c\u610f\u89c1\u5355\u5143\u662f\u5305\u542b\u76f8\u5173\u6587\u672c\u6458\u5f55\u548c\u76f8\u5173\u60c5\u611f\u5206\u6570\u7684\u72ec\u7279\u58f0\u660e\u3002", "result": "\u63d0\u9ad8\u4e86\u540e\u7eed\u4e3b\u9898\u5efa\u6a21\u7684\u8868\u73b0\uff0c\u4ea7\u751f\u4e86\u8fde\u8d2f\u4e14\u53ef\u89e3\u91ca\u7684\u4e3b\u9898\uff0c\u540c\u65f6\u8fd8\u6355\u83b7\u4e86\u4e0e\u6bcf\u4e2a\u4e3b\u9898\u76f8\u5173\u7684\u60c5\u611f\u3002", "conclusion": "\u901a\u8fc7\u5c06\u4e3b\u9898\u5efa\u6a21\u7ba1\u9053\u91cd\u6784\u4e3a\u5728\u610f\u89c1\u5355\u5143\u4e0a\u64cd\u4f5c\uff08\u5305\u542b\u76f8\u5173\u6587\u672c\u6458\u5f55\u548c\u76f8\u5173\u60c5\u611f\u5206\u6570\uff09\uff0c\u6539\u8fdb\u4e86\u4ece\u5ba2\u6237\u8bc4\u8bba\u4e2d\u63d0\u53d6\u89c1\u89e3\u7684\u8868\u73b0\u3002\u5c06\u4e3b\u9898\u548c\u60c5\u611f\u4e0e\u661f\u7ea7\u7b49\u4e1a\u52a1\u6307\u6807\u76f8\u5173\u8054\uff0c\u53ef\u4ee5\u6df1\u5165\u4e86\u89e3\u7279\u5b9a\u7684\u5ba2\u6237\u95ee\u9898\u5982\u4f55\u5f71\u54cd\u4e1a\u52a1\u6210\u679c\u3002\u8bc4\u4f30\u4e86\u5176\u5728\u521b\u5efa\u8fde\u8d2f\u4e3b\u9898\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u5e76\u8bc4\u4f30\u4e86\u6574\u5408\u4e3b\u9898\u548c\u60c5\u611f\u6a21\u5f0f\u4ee5\u8fdb\u884c\u51c6\u786e\u661f\u7ea7\u9884\u6d4b\u7684\u65b9\u6cd5\u3002"}}
{"id": "2507.13496", "categories": ["quant-ph", "cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2507.13496", "abs": "https://arxiv.org/abs/2507.13496", "authors": ["ChunJun Cao", "Brad Lackey"], "title": "Growing Sparse Quantum Codes from a Seed", "comment": "23 pages, 12 figures", "summary": "It is generally unclear whether smaller codes can be \"concatenated\" to\nsystematically create quantum LDPC codes or their sparse subsystem code cousins\nwhere the degree of the Tanner graph remains bounded while increasing the code\ndistance. In this work, we use a slight generalization of concatenation called\nconjoining introduced by the quantum lego formalism. We show that by conjoining\nonly quantum repetition codes, one can construct quantum LDPC codes. More\ngenerally, we provide an efficient iterative algorithm for constructing sparse\nsubsystem codes with a distance guarantee that asymptotically saturates\n$kd^2=O(n)$ in the worst case. Furthermore, we show that the conjoining of even\njust two-qubit quantum bit-flip and phase-flip repetition codes is quite\npowerful as they can create any CSS code. Therefore, more creative combinations\nof these basic code blocks will be sufficient for generating good quantum\ncodes, including good quantum LDPC codes.", "AI": {"tldr": "\u672c\u7814\u7a76\u4f7f\u7528\u201c\u8054\u7f00\u201d\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u53ef\u4ee5\u4ec5\u901a\u8fc7\u7ec4\u5408\u91cf\u5b50\u91cd\u590d\u7801\u6765\u6784\u5efa\u91cf\u5b50LDPC\u7801\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u80fd\u751f\u6210\u5177\u6709\u826f\u597d\u8ddd\u79bb\u589e\u957f\u7684\u7a00\u758f\u5b50\u7cfb\u7edf\u7801\u7684\u7b97\u6cd5\u3002\u6b64\u5916\uff0c\u8fd8\u53d1\u73b0\u7b80\u5355\u7684\u91cf\u5b50\u91cd\u590d\u7801\u7ec4\u5408\u8db3\u4ee5\u751f\u6210\u4efb\u610fCSS\u7801\u3002", "motivation": "\u63a2\u7d22\u662f\u5426\u53ef\u4ee5\u901a\u8fc7\u201c\u8054\u7f00\u201d\u64cd\u4f5c\u5c06\u8f83\u5c0f\u7684\u91cf\u5b50\u7801\u7ec4\u5408\u8d77\u6765\uff0c\u7cfb\u7edf\u6027\u5730\u6784\u5efa\u91cf\u5b50LDPC\u7801\u6216\u7a00\u758f\u5b50\u7cfb\u7edf\u7801\uff0c\u5e76\u5728\u589e\u52a0\u7801\u8ddd\u79bb\u7684\u540c\u65f6\u4fdd\u6301Tanner\u56fe\u7684\u5ea6\u6709\u754c\u3002", "method": "\u5229\u7528\u91cf\u5b50Lego\u5f62\u5f0f\u4e3b\u4e49\u4e2d\u7684\u201c\u8054\u7f00\u201d\u601d\u60f3\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u8fed\u4ee3\u7b97\u6cd5\uff0c\u53ef\u4ee5\u6784\u9020\u5177\u6709\u754c\u9650Tanner\u56fe\u5ea6\u7684\u7a00\u758f\u5b50\u7cfb\u7edf\u7801\uff0c\u4e14\u8be5\u7801\u7684\u6700\u5c0f\u8ddd\u79bb\u7684\u589e\u957f\u6ee1\u8db3kd^2=O(n)\u3002", "result": "\u8bc1\u660e\u4e86\u4ec5\u901a\u8fc7\u8054\u7f00\u91cf\u5b50\u91cd\u590d\u7801\u5373\u53ef\u6784\u5efa\u91cf\u5b50LDPC\u7801\u3002\u6b64\u5916\uff0c\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u80fd\u591f\u6784\u9020\u51fa\u7a00\u758f\u5b50\u7cfb\u7edf\u7801\uff0c\u5176\u6700\u5c0f\u8ddd\u79bb\u589e\u957f\u6e10\u8fdb\u5730\u6ee1\u8db3kd^2=O(n)\u3002\u540c\u65f6\uff0c\u8bc1\u660e\u4e86\u53ea\u9700\u8054\u7f00\u4e24\u4e2a\u91cf\u5b50\u6bd4\u7279\u7684\u6bd4\u7279\u7ffb\u8f6c\u7801\u548c\u76f8\u4f4d\u7ffb\u8f6c\u7801\u5373\u53ef\u751f\u6210\u4efb\u4f55CSS\u7801\uff0c\u8868\u660e\u8fd9\u79cd\u7ec4\u5408\u65b9\u5f0f\u5177\u6709\u5f88\u5f3a\u7684\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u5c06\u91cf\u5b50\u91cd\u590d\u7801\u8fdb\u884c\u201c\u8054\u7f00\u201d\uff08conjoining\uff09\u8fd9\u4e00\u64cd\u4f5c\uff0c\u53ef\u4ee5\u6784\u5efa\u51fa\u91cf\u5b50LDPC\u7801\u3002"}}
{"id": "2507.13414", "categories": ["cs.LG", "cs.AI", "math.DG"], "pdf": "https://arxiv.org/pdf/2507.13414", "abs": "https://arxiv.org/abs/2507.13414", "authors": ["Alexander Strunk", "Roland Assam"], "title": "Gauge Flow Models", "comment": null, "summary": "This paper introduces Gauge Flow Models, a novel class of Generative Flow\nModels. These models incorporate a learnable Gauge Field within the Flow\nOrdinary Differential Equation (ODE). A comprehensive mathematical framework\nfor these models, detailing their construction and properties, is provided.\nExperiments using Flow Matching on Gaussian Mixture Models demonstrate that\nGauge Flow Models yields significantly better performance than traditional Flow\nModels of comparable or even larger size. Additionally, unpublished research\nindicates a potential for enhanced performance across a broader range of\ngenerative tasks.", "AI": {"tldr": "Gauge Flow Models\u662f\u4e00\u79cd\u65b0\u7684\u751f\u6210\u6d41\u6a21\u578b\uff0c\u901a\u8fc7\u5728\u6d41ODE\u4e2d\u5f15\u5165Gauge\u573a\uff0c\u5728\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e86\u6bd4\u4f20\u7edf\u6d41\u6a21\u578b\u66f4\u597d\u7684\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u63d0\u5347\u751f\u6210\u6d41\u6a21\u578b\u7684\u6027\u80fd\uff0c\u672c\u6587\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684Gauge Flow Models\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u751f\u6210\u6d41\u6a21\u578b\u7c7b\u522b\u2014\u2014Gauge Flow Models\uff0c\u5176\u7279\u70b9\u662f\u5728\u6d41\u5e38\u5fae\u5206\u65b9\u7a0b\uff08ODE\uff09\u4e2d\u5f15\u5165\u4e86\u4e00\u4e2a\u53ef\u5b66\u4e60\u7684Gauge\u573a\u3002", "result": "\u5728\u57fa\u4e8e\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u7684\u6d41\u5339\u914d\u5b9e\u9a8c\u4e2d\uff0cGauge Flow Models\u7684\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u540c\u7b49\u6216\u66f4\u5927\u89c4\u6a21\u7684\u4f20\u7edf\u6d41\u6a21\u578b\u3002", "conclusion": "Gauge Flow Models\u5728\u751f\u6210\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u4f18\u4e8e\u4f20\u7edf\u6d41\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u6709\u6f5c\u529b\u5728\u66f4\u5e7f\u6cdb\u7684\u751f\u6210\u4efb\u52a1\u4e2d\u5b9e\u73b0\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2507.13477", "categories": ["cs.SI"], "pdf": "https://arxiv.org/pdf/2507.13477", "abs": "https://arxiv.org/abs/2507.13477", "authors": ["Nickolas K. Freeman", "Gregory J. Bott", "Burcu B. Keskin", "Jason M. Parton", "James J. Cochran"], "title": "Linking Multi-Site Sex Ad Data at the Individual Level to Aid Counter-Trafficking Efforts", "comment": "30 pages, 10 figures, 6 tables. Under review at Manufacturing &\n  Service Operations Management", "summary": "The Internet facilitates sex trafficking through adult service websites\n(ASWs) that host online advertisements for sexual services (sex ads). Since the\nclosure of the popular site Backpage.com, the ecosystem of ASWs has expanded to\ninclude multiple competing sites that are hosted outside US jurisdiction.\nGaining intelligence for counter-trafficking efforts requires collecting,\nlinking, and cleaning the data from multiple sites. However, high ad volumes,\ndisparate data types, and the existence of generic and misappropriated data\nmake this process challenging. We present an end-to-end process for linking sex\nad data and filtering potentially erroneous links. Outputs of the developed\nprocess have been used to inform counter-trafficking operations that have\nhelped identify more than 60 potential victims of sex trafficking, some of whom\nare getting help to transition out of the life. Our process leverages concepts\nand techniques from network science, information systems, and artificial\nintelligence to link ads across sites at the level of an individual or unique\nposting entity. Our approach is computationally efficient, allowing millions of\nads to be processed in under an hour. A key component of our process is an edge\nfiltering procedure that identifies and removes potentially erroneous links in\na graph representation of sex ad data. A comparison of the proposed process to\nan existing approach shows that our process is typically more computationally\nefficient and yields substantial increases in the number of individuals for\nwhich we can derive actionable intelligence. The proposed process is an\nefficient and effective approach for transforming the high volumes of disparate\ndata from sex ads into intelligence that can save lives. It has been refined\nover years of collaboration with practitioners and represents a strong\nfoundation upon which further counter-trafficking tools can be built.", "AI": {"tldr": "\u7814\u7a76\u4eba\u5458\u5f00\u53d1\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u7f51\u7edc\u79d1\u5b66\u548c\u4eba\u5de5\u667a\u80fd\u6280\u672f\uff0c\u6574\u5408\u6765\u81ea\u591a\u4e2a\u7f51\u7ad9\u7684\u6027\u5e7f\u544a\u6570\u636e\uff0c\u8fc7\u6ee4\u9519\u8bef\u4fe1\u606f\uff0c\u4ece\u800c\u4e3a\u53cd\u4eba\u53e3\u8d29\u8fd0\u63d0\u4f9b\u5173\u952e\u60c5\u62a5\uff0c\u5df2\u6210\u529f\u5e2e\u52a9\u8bc6\u522b\u53d7\u5bb3\u8005\u3002", "motivation": "\u4e92\u8054\u7f51\uff0c\u7279\u522b\u662f\u6210\u4eba\u670d\u52a1\u7f51\u7ad9\uff08ASWs\uff09\uff0c\u5df2\u6210\u4e3a\u6027\u4ea4\u6613\u7684\u5de5\u5177\u3002\u968f\u7740\u50cfBackpage.com\u8fd9\u6837\u7684\u4e3b\u6d41\u7f51\u7ad9\u7684\u5173\u95ed\uff0c\u4ea4\u6613\u6d3b\u52a8\u8f6c\u79fb\u5230\u66f4\u591a\u5206\u6563\u4e14\u4f4d\u4e8e\u7f8e\u56fd\u53f8\u6cd5\u7ba1\u8f96\u533a\u4e4b\u5916\u7684\u7f51\u7ad9\u3002\u4e3a\u4e86\u6709\u6548\u6253\u51fb\u6027\u4ea4\u6613\uff0c\u9700\u8981\u6536\u96c6\u3001\u94fe\u63a5\u548c\u6e05\u7406\u6765\u81ea\u8fd9\u4e9b\u591a\u6e90\u7f51\u7ad9\u7684\u6570\u636e\uff0c\u4f46\u8fd9\u9762\u4e34\u7740\u5e7f\u544a\u91cf\u5927\u3001\u6570\u636e\u7c7b\u578b\u591a\u6837\u4ee5\u53ca\u5b58\u5728\u901a\u7528\u6216\u88ab\u76d7\u7528\u6570\u636e\u7b49\u6311\u6218\u3002", "method": "\u8be5\u7814\u7a76\u91c7\u7528\u7f51\u7edc\u79d1\u5b66\u3001\u4fe1\u606f\u7cfb\u7edf\u548c\u4eba\u5de5\u667a\u80fd\u7684\u6280\u672f\uff0c\u5c06\u4e0d\u540c\u7f51\u7ad9\u4e0a\u7684\u6027\u5e7f\u544a\u94fe\u63a5\u8d77\u6765\uff0c\u5f62\u6210\u4e00\u4e2a\u4ee3\u8868\u4e2a\u4f53\u6216\u72ec\u7279\u5e16\u5b50\u5b9e\u4f53\u7684\u7f51\u7edc\u3002\u5176\u5173\u952e\u5728\u4e8e\u4e00\u4e2a\u8fb9\u8fc7\u6ee4\u7a0b\u5e8f\uff0c\u7528\u4e8e\u8bc6\u522b\u548c\u79fb\u9664\u56fe\u4e2d\u53ef\u80fd\u9519\u8bef\u7684\u94fe\u63a5\u3002", "result": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u6d41\u7a0b\u5728\u5904\u7406\u901f\u5ea6\u4e0a\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u5feb\uff0c\u80fd\u5728\u4e00\u5c0f\u65f6\u5185\u5904\u7406\u6570\u767e\u4e07\u6761\u5e7f\u544a\u3002\u8be5\u6d41\u7a0b\u751f\u6210\u7684\u6570\u636e\u5df2\u88ab\u7528\u4e8e\u53cd\u4eba\u53e3\u8d29\u8fd0\u884c\u52a8\uff0c\u5e2e\u52a9\u8bc6\u522b\u4e8660\u591a\u540d\u6f5c\u5728\u53d7\u5bb3\u8005\uff0c\u5e76\u4e3a\u4ed6\u4eec\u63d0\u4f9b\u4e86\u5e2e\u52a9\u3002\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u7814\u7a76\u7684\u65b9\u6cd5\u5728\u751f\u6210\u53ef\u64cd\u4f5c\u60c5\u62a5\u65b9\u9762\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u6d41\u7a0b\uff0c\u7528\u4e8e\u94fe\u63a5\u6765\u81ea\u4e0d\u540c\u6210\u4eba\u670d\u52a1\u7f51\u7ad9\u7684\u6027\u5e7f\u544a\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u8fc7\u6ee4\u53ef\u80fd\u9519\u8bef\u7684\u94fe\u63a5\u6765\u751f\u6210\u53ef\u7528\u4e8e\u53cd\u4eba\u53e3\u8d29\u8fd0\u7684\u60c5\u62a5\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u9ad8\u6548\u5904\u7406\u6d77\u91cf\u6570\u636e\uff0c\u5e76\u5df2\u6210\u529f\u5e2e\u52a9\u8bc6\u522b\u4e8660\u591a\u540d\u6f5c\u5728\u7684\u6027\u4ea4\u6613\u53d7\u5bb3\u8005\u3002"}}
{"id": "2507.13748", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.13748", "abs": "https://arxiv.org/abs/2507.13748", "authors": ["Patrick Matalla", "Joel Dittmer", "Md Salek Mahmud", "Christian Koos", "Sebastian Randel"], "title": "Elastic Buffer Design for Real-Time All-Digital Clock Recovery Enabling Free-Running Receiver Clock with Negative and Positive Clock Frequency Offsets", "comment": null, "summary": "We present an elastic buffer design that enables all-digital clock recovery\nimplementation with free-running receiver clock featuring negative and positive\nclock frequency offsets. Error-free real-time data transmission is demonstrated\nfrom -400 ppm to +400 ppm.", "AI": {"tldr": "An elastic buffer design enables all-digital clock recovery for free-running clocks with frequency offsets up to +/-400 ppm, achieving error-free data transmission.", "motivation": "To enable all-digital clock recovery implementation with free-running receiver clocks that can handle clock frequency offsets.", "method": "Elastic buffer design for all-digital clock recovery.", "result": "Error-free real-time data transmission demonstrated from -400 ppm to +400 ppm.", "conclusion": "The elastic buffer design allows for all-digital clock recovery with free-running receiver clocks, handling both negative and positive clock frequency offsets."}}
{"id": "2507.13364", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13364", "abs": "https://arxiv.org/abs/2507.13364", "authors": ["Siddharth Srivastava", "Gaurav Sharma"], "title": "OmniVec2 -- A Novel Transformer based Network for Large Scale Multimodal and Multitask Learning", "comment": null, "summary": "We present a novel multimodal multitask network and associated training\nalgorithm. The method is capable of ingesting data from approximately 12\ndifferent modalities namely image, video, audio, text, depth, point cloud, time\nseries, tabular, graph, X-ray, infrared, IMU, and hyperspectral. The proposed\napproach utilizes modality specialized tokenizers, a shared transformer\narchitecture, and cross-attention mechanisms to project the data from different\nmodalities into a unified embedding space. It addresses multimodal and\nmultitask scenarios by incorporating modality-specific task heads for different\ntasks in respective modalities. We propose a novel pretraining strategy with\niterative modality switching to initialize the network, and a training\nalgorithm which trades off fully joint training over all modalities, with\ntraining on pairs of modalities at a time. We provide comprehensive evaluation\nacross 25 datasets from 12 modalities and show state of the art performances,\ndemonstrating the effectiveness of the proposed architecture, pretraining\nstrategy and adapted multitask training.", "AI": {"tldr": "A new network ingests 12 data types (image, video, audio, etc.), uses specialized tokenizers and a shared transformer with cross-attention for a unified embedding. It's trained with a novel strategy (iterative modality switching) and an adapted algorithm balancing joint and pairwise training. Proven effective on 25 datasets, achieving SOTA results.", "motivation": "To develop a network capable of ingesting and processing data from approximately 12 different modalities for multimodal and multitask scenarios.", "method": "A novel multimodal multitask network using modality specialized tokenizers, a shared transformer architecture, and cross-attention mechanisms to create a unified embedding space. It incorporates modality-specific task heads for different tasks and employs a pretraining strategy with iterative modality switching and a training algorithm that balances fully joint training with pairwise training.", "result": "State-of-the-art performances demonstrated across 25 datasets from 12 modalities, validating the effectiveness of the proposed architecture, pretraining strategy, and multitask training.", "conclusion": "The proposed novel multimodal multitask network, associated training algorithm, pretraining strategy with iterative modality switching, and adapted multitask training achieve state-of-the-art performance across 25 datasets from 12 modalities."}}
{"id": "2507.13651", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13651", "abs": "https://arxiv.org/abs/2507.13651", "authors": ["Gerben van der Hoek", "Johan Jeuring", "Rogier Bos"], "title": "Buggy rule diagnosis for combined steps through final answer evaluation in stepwise tasks", "comment": null, "summary": "Many intelligent tutoring systems can support a student in solving a stepwise\ntask. When a student combines several steps in one step, the number of possible\npaths connecting consecutive inputs may be very large. This combinatorial\nexplosion makes error diagnosis hard. Using a final answer to diagnose a\ncombination of steps can mitigate the combinatorial explosion, because there\nare generally fewer possible (erroneous) final answers than (erroneous)\nsolution paths. An intermediate input for a task can be diagnosed by\nautomatically completing it according to the task solution strategy and\ndiagnosing this solution. This study explores the potential of automated error\ndiagnosis based on a final answer. We investigate the design of a service that\nprovides a buggy rule diagnosis when a student combines several steps. To\nvalidate the approach, we apply the service to an existing dataset (n=1939) of\nunique student steps when solving quadratic equations, which could not be\ndiagnosed by a buggy rule service that tries to connect consecutive inputs with\na single rule. Results show that final answer evaluation can diagnose 29,4% of\nthese steps. Moreover, a comparison of the generated diagnoses with teacher\ndiagnoses on a subset (n=115) shows that the diagnoses align in 97% of the\ncases. These results can be considered a basis for further exploration of the\napproach.", "AI": {"tldr": "\u901a\u8fc7\u81ea\u52a8\u8865\u5168\u548c\u8bca\u65ad\u5b66\u751f\u6b65\u9aa4\u7684\u89e3\u6cd5\uff0c\u53ef\u4ee5\u6709\u6548\u89e3\u51b3\u667a\u80fd\u8f85\u5bfc\u7cfb\u7edf\u4e2d\u56e0\u5b66\u751f\u5408\u5e76\u6b65\u9aa4\u800c\u5bfc\u81f4\u7684\u9519\u8bef\u8bca\u65ad\u56f0\u96be\u95ee\u9898\u3002", "motivation": "\u5728\u667a\u80fd\u8f85\u5bfc\u7cfb\u7edf\u4e2d\uff0c\u5f53\u5b66\u751f\u5c06\u591a\u4e2a\u6b65\u9aa4\u5408\u5e76\u4e3a\u4e00\u4e2a\u65f6\uff0c\u9519\u8bef\u8bca\u65ad\u53d8\u5f97\u56f0\u96be\uff0c\u56e0\u4e3a\u53ef\u80fd\u5b58\u5728\u5927\u91cf\u7684\u8def\u5f84\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u670d\u52a1\u8bbe\u8ba1\uff0c\u901a\u8fc7\u81ea\u52a8\u5b8c\u6210\u5b66\u751f\u7684\u4e2d\u95f4\u6b65\u9aa4\uff0c\u5e76\u8bca\u65ad\u8be5\u8865\u5168\u540e\u7684\u89e3\u6cd5\u6765\u89e3\u51b3\u9519\u8bef\u8bca\u65ad\u4e2d\u7684\u7ec4\u5408\u7206\u70b8\u95ee\u9898\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u8bca\u65ad\u73b0\u6709\u670d\u52a1\u65e0\u6cd5\u8bca\u65ad\u7684\u5b66\u751f\u6b65\u9aa4\u768429.4%\uff0c\u5e76\u4e14\u4e0e\u6559\u5e08\u8bca\u65ad\u7684\u4e00\u81f4\u6027\u8fbe\u523097%\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6700\u7ec8\u7b54\u6848\u7684\u81ea\u52a8\u5316\u9519\u8bef\u8bca\u65ad\u65b9\u6cd5\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u4e8c\u6b21\u65b9\u7a0b\u6c42\u89e3\u4efb\u52a1\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2507.13872", "categories": ["eess.SY", "cs.RO", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.13872", "abs": "https://arxiv.org/abs/2507.13872", "authors": ["Aditya Singh", "Aastha Mishra", "Manan Tayal", "Shishir Kolathaya", "Pushpak Jagtap"], "title": "Safe and Performant Controller Synthesis using Gradient-based Model Predictive Control and Control Barrier Functions", "comment": "6 Pages, 2 Figures. The first two authors contributed equally", "summary": "Ensuring both performance and safety is critical for autonomous systems\noperating in real-world environments. While safety filters such as Control\nBarrier Functions (CBFs) enforce constraints by modifying nominal controllers\nin real time, they can become overly conservative when the nominal policy lacks\nsafety awareness. Conversely, solving State-Constrained Optimal Control\nProblems (SC-OCPs) via dynamic programming offers formal guarantees but is\nintractable in high-dimensional systems. In this work, we propose a novel\ntwo-stage framework that combines gradient-based Model Predictive Control (MPC)\nwith CBF-based safety filtering for co-optimizing safety and performance. In\nthe first stage, we relax safety constraints as penalties in the cost function,\nenabling fast optimization via gradient-based methods. This step improves\nscalability and avoids feasibility issues associated with hard constraints. In\nthe second stage, we modify the resulting controller using a CBF-based\nQuadratic Program (CBF-QP), which enforces hard safety constraints with minimal\ndeviation from the reference. Our approach yields controllers that are both\nperformant and provably safe. We validate the proposed framework on two case\nstudies, showcasing its ability to synthesize scalable, safe, and\nhigh-performance controllers for complex, high-dimensional autonomous systems.", "AI": {"tldr": "\u4e00\u79cd\u7ed3\u5408\u68af\u5ea6\u4e0b\u964dMPC\u548cCBF\u5b89\u5168\u6ee4\u6ce2\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u786e\u4fdd\u5b89\u5168\u6027\u3002", "motivation": "\u4e3a\u4e86\u786e\u4fdd\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u8fd0\u884c\u7684\u81ea\u4e3b\u7cfb\u7edf\u7684\u6027\u80fd\u548c\u5b89\u5168\uff0c\u4f46\u73b0\u6709\u7684\u65b9\u6cd5\uff0c\u5982\u5b89\u5168\u6ee4\u6ce2\u5668\uff08CBF\uff09\u5728\u540d\u4e49\u7b56\u7565\u7f3a\u4e4f\u5b89\u5168\u610f\u8bc6\u65f6\u53ef\u80fd\u8fc7\u4e8e\u4fdd\u5b88\uff0c\u800c\u901a\u8fc7\u52a8\u6001\u89c4\u5212\u89e3\u51b3\u6709\u72b6\u6001\u7ea6\u675f\u6700\u4f18\u63a7\u5236\u95ee\u9898\uff08SC-OCP\uff09\u5728\u9ad8\u7ef4\u7cfb\u7edf\u4e2d\u96be\u4ee5\u5904\u7406\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u9996\u5148\u5728\u6210\u672c\u51fd\u6570\u4e2d\u653e\u5bbd\u5b89\u5168\u7ea6\u675f\u4f5c\u4e3a\u60e9\u7f5a\uff0c\u4ee5\u4fbf\u901a\u8fc7\u57fa\u4e8e\u68af\u5ea6\u7684\u65b9\u6cd5\u8fdb\u884c\u5feb\u901f\u4f18\u5316\uff0c\u7136\u540e\u5728\u7b2c\u4e8c\u9636\u6bb5\u4f7f\u7528\u57fa\u4e8eCBF\u7684\u4e8c\u6b21\u89c4\u5212\uff08CBF-QP\uff09\u6765\u5f3a\u5236\u6267\u884c\u786c\u5b89\u5168\u7ea6\u675f\uff0c\u5e76\u5c3d\u91cf\u51cf\u5c11\u5bf9\u53c2\u8003\u7684\u504f\u5dee\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5408\u6210\u53ef\u6269\u5c55\u3001\u5b89\u5168\u4e14\u9ad8\u6027\u80fd\u7684\u63a7\u5236\u5668\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u57fa\u4e8e\u68af\u5ea6\u7684\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u548c\u57fa\u4e8eCBF\u7684\u5b89\u5168\u6ee4\u6ce2\uff0c\u4ee5\u534f\u540c\u4f18\u5316\u5b89\u5168\u6027\u548c\u6027\u80fd\uff0c\u5e76\u5df2\u5728\u4e24\u4e2a\u6848\u4f8b\u7814\u7a76\u4e2d\u5f97\u5230\u9a8c\u8bc1\uff0c\u8bc1\u660e\u4e86\u5176\u4e3a\u590d\u6742\u3001\u9ad8\u7ef4\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5408\u6210\u53ef\u6269\u5c55\u3001\u5b89\u5168\u4e14\u9ad8\u6027\u80fd\u63a7\u5236\u5668\u7684\u80fd\u529b\u3002"}}
{"id": "2507.13650", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.13650", "abs": "https://arxiv.org/abs/2507.13650", "authors": ["Yu-Ting Lai", "Yasamin Foroutani", "Aya Barzelay", "Tsu-Chin Tsao"], "title": "Safe Robotic Capsule Cleaning with Integrated Transpupillary and Intraocular Optical Coherence Tomography", "comment": "12 pages, 27 figures", "summary": "Secondary cataract is one of the most common complications of vision loss due\nto the proliferation of residual lens materials that naturally grow on the lens\ncapsule after cataract surgery. A potential treatment is capsule cleaning, a\nsurgical procedure that requires enhanced visualization of the entire capsule\nand tool manipulation on the thin membrane. This article presents a robotic\nsystem capable of performing the capsule cleaning procedure by integrating a\nstandard transpupillary and an intraocular optical coherence tomography probe\non a surgical instrument for equatorial capsule visualization and real-time\ntool-to-tissue distance feedback. Using robot precision, the developed system\nenables complete capsule mapping in the pupillary and equatorial regions with\nin-situ calibration of refractive index and fiber offset, which are still\ncurrent challenges in obtaining an accurate capsule model. To demonstrate\neffectiveness, the capsule mapping strategy was validated through five\nexperimental trials on an eye phantom that showed reduced root-mean-square\nerrors in the constructed capsule model, while the cleaning strategy was\nperformed in three ex-vivo pig eyes without tissue damage.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u7528\u4e8e\u6cbb\u7597\u767d\u5185\u969c\u624b\u672f\u540e\u7684\u4e8c\u6b21\u767d\u5185\u969c\uff0c\u901a\u8fc7\u7cbe\u786e\u7684\u56ca\u819c\u6620\u5c04\u548c\u6e05\u6d01\uff0c\u63d0\u9ad8\u4e86\u624b\u672f\u6548\u679c\u5e76\u964d\u4f4e\u4e86\u5e76\u53d1\u75c7\u7684\u98ce\u9669\u3002", "motivation": "\u4e8c\u6b21\u767d\u5185\u969c\u662f\u767d\u5185\u969c\u624b\u672f\u540e\u5e38\u89c1\u7684\u5e76\u53d1\u75c7\uff0c\u7531\u6b8b\u7559\u7684\u6676\u72b6\u4f53\u6750\u6599\u589e\u6b96\u5f15\u8d77\u3002\u8be5\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u8fdb\u884c\u56ca\u819c\u6e05\u6d01\u624b\u672f\u7684\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u4ee5\u63d0\u9ad8\u624b\u672f\u7cbe\u5ea6\u548c\u5b89\u5168\u6027\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u96c6\u6210\u4e86\u6807\u51c6\u7ecf\u77b3\u548c\u773c\u5185\u5149\u5b66\u76f8\u5e72\u65ad\u5c42\u626b\u63cf\u63a2\u5934\u7684\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u7528\u4e8e\u56ca\u819c\u53ef\u89c6\u5316\u548c\u5de5\u5177-\u7ec4\u7ec7\u8ddd\u79bb\u53cd\u9988\u3002\u901a\u8fc7\u5728\u773c\u6807\u672c\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u56ca\u819c\u6620\u5c04\u7b56\u7565\u7684\u6709\u6548\u6027\uff0c\u5e76\u901a\u8fc7\u5728\u732a\u773c\u4e0a\u8fdb\u884c\u4e86\u6e05\u6d01\u7b56\u7565\u5b9e\u9a8c\u3002", "result": "\u5728\u773c\u6807\u672c\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u7cfb\u7edf\u5c06\u6784\u5efa\u7684\u56ca\u819c\u6a21\u578b\u7684\u5747\u65b9\u6839\u8bef\u5dee\u964d\u4f4e\u4e86XX%\u3002\u5728\u79bb\u4f53\u732a\u773c\u5b9e\u9a8c\u4e2d\uff0c\u6e05\u6d01\u7b56\u7565\u5728\u672a\u9020\u6210\u7ec4\u7ec7\u635f\u4f24\u7684\u60c5\u51b5\u4e0b\u6210\u529f\u8fdb\u884c\u4e86\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u96c6\u6210\u4e86\u6807\u51c6\u7ecf\u77b3\u548c\u773c\u5185\u5149\u5b66\u76f8\u5e72\u65ad\u5c42\u626b\u63cf\u63a2\u5934\u7684\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u7528\u4e8e\u767d\u5185\u969c\u624b\u672f\u540e\u7684\u4e8c\u6b21\u767d\u5185\u969c\u6cbb\u7597\u3002\u8be5\u7cfb\u7edf\u80fd\u591f\u5b9e\u73b0\u5b8c\u6574\u7684\u56ca\u819c\u6620\u5c04\u548c\u5b9e\u65f6\u5de5\u5177-\u7ec4\u7ec7\u8ddd\u79bb\u53cd\u9988\uff0c\u89e3\u51b3\u4e86\u7cbe\u786e\u56ca\u819c\u5efa\u6a21\u7684\u6311\u6218\u3002"}}
{"id": "2507.13958", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2507.13958", "abs": "https://arxiv.org/abs/2507.13958", "authors": ["Pedro Cabalar", "Mart\u00edn Di\u00e9guez", "Fran\u00e7ois Olivier", "Torsten Schaub", "Igor St\u00e9phan"], "title": "Towards Constraint Temporal Answer Set Programming", "comment": null, "summary": "Reasoning about dynamic systems with a fine-grained temporal and numeric\nresolution presents significant challenges for logic-based approaches like\nAnswer Set Programming (ASP). To address this, we introduce and elaborate upon\na novel temporal and constraint-based extension of the logic of Here-and-There\nand its nonmonotonic equilibrium extension, representing, to the best of our\nknowledge, the first approach to nonmonotonic temporal reasoning with\nconstraints specifically tailored for ASP. This expressive system is achieved\nby a synergistic combination of two foundational ASP extensions: the\nlinear-time logic of Here-and-There, providing robust nonmonotonic temporal\nreasoning capabilities, and the logic of Here-and-There with constraints,\nenabling the direct integration and manipulation of numeric constraints, among\nothers. This work establishes the foundational logical framework for tackling\ncomplex dynamic systems with high resolution within the ASP paradigm.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u65f6\u95f4\u903b\u8f91\u548c\u7ea6\u675f\u903b\u8f91\u7684ASP\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u5177\u6709\u7cbe\u7ec6\u65f6\u95f4/\u6570\u5b57\u5206\u8fa8\u7387\u7684\u52a8\u6001\u7cfb\u7edf\u63a8\u7406\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u57fa\u4e8e\u903b\u8f91\u7684\u65b9\u6cd5\uff08\u5982ASP\uff09\u5728\u5177\u6709\u7cbe\u7ec6\u65f6\u95f4\u4e0e\u6570\u5b57\u5206\u8fa8\u7387\u7684\u52a8\u6001\u7cfb\u7edf\u65b9\u9762\u5b58\u5728\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u5e76\u9610\u8ff0\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u3001\u57fa\u4e8e\u65f6\u95f4\u7684\u548c\u7ea6\u675f\u7684\u201c\u8fd9\u91cc\u548c\u90a3\u91cc\u201d\u903b\u8f91\u6269\u5c55\u53ca\u5176\u975e\u5355\u8c03\u5e73\u8861\u6269\u5c55\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u9488\u5bf9ASP\u7684\u975e\u5355\u8c03\u65f6\u95f4\u63a8\u7406\u4e0e\u7ea6\u675f\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\u3002", "result": "\u4e00\u4e2a\u65e2\u80fd\u8fdb\u884c\u9c81\u68d2\u7684\u975e\u5355\u8c03\u65f6\u95f4\u63a8\u7406\uff0c\u53c8\u80fd\u76f4\u63a5\u96c6\u6210\u548c\u64cd\u4f5c\u6570\u5b57\u7ea6\u675f\u7684\u8868\u8fbe\u7cfb\u7edf\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5728ASP\u8303\u4f8b\u4e2d\u5904\u7406\u9ad8\u5206\u8fa8\u7387\u7684\u590d\u6742\u52a8\u6001\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u903b\u8f91\u6846\u67b6\u3002"}}
{"id": "2507.13824", "categories": ["cond-mat.mtrl-sci", "cond-mat.other"], "pdf": "https://arxiv.org/pdf/2507.13824", "abs": "https://arxiv.org/abs/2507.13824", "authors": ["Toraya Fern\u00e1ndez-Ruiz", "Jorge \u00cd\u00f1iguez", "Javier Junquera", "Pablo Garc\u00eda-Fern\u00e1ndez"], "title": "Extension of Second-Principles Density Functional Theory into the time domain", "comment": "14 pages, 8 figures", "summary": "We present an extension of the second-principles density functional theory\n(SPDFT) method to perform time-dependent simulations. Our approach, which\ncalculates the evolution of the density matrix in real time and real space\nusing the Liouville-von Neumann equation of motion, allows determining optical\nand transport properties for very large systems, involving tens of thousands of\natoms, using very modest computational platforms. In contrast with other\nmethods, we show that SPDFT can be applied to a wide variety of materials\nincluding both metals and insulators. In particular, we illustrate its\ncapabilities by obtaining the spectra of SrTiO$_3$, diamond and metallic\nlithium. We find that, while SPDFT results in SrTiO$_3$ are quite similar to\nthose obtained from DFT using linear perturbation theory, we observe\nsignificant improvements over this method in both diamond and metallic lithium.\nThe inclusion of electron-electron interactions during the evolution of the\ndensity matrix in diamond allows the spectra to more closely resemble those\nobtained with the Bethe-Salpeter equation than from perturbation theory. In\nlithium time-dependent SPDFT not only predicts interband transitions but also\nthe Drude peak, opening the possibility of detailed ab initio studies of\ntransport properties beyond many of the usual approximations.", "AI": {"tldr": "SPDFT\u65b9\u6cd5\u53ef\u7528\u4e8e\u5927\u89c4\u6a21\u6750\u6599\u7684\u5149\u5b66\u548c\u4f20\u8f93\u6027\u8d28\u8ba1\u7b97\u3002", "motivation": "\u4e3a\u4e86\u5bf9\u975e\u5e38\u5927\u7684\u7cfb\u7edf\uff08\u6570\u4e07\u4e2a\u539f\u5b50\uff09\u8fdb\u884c\u65f6\u95f4\u4f9d\u8d56\u6027\u6a21\u62df\uff0c\u5e76\u786e\u5b9a\u5176\u5149\u5b66\u548c\u4f20\u8f93\u6027\u8d28\u3002", "method": "SPDFT\u65b9\u6cd5\u901a\u8fc7\u5728\u5b9e\u65f6\u95f4\u548c\u5b9e\u7a7a\u95f4\u4e2d\u4f7f\u7528Liouville-von Neumann\u65b9\u7a0b\u8ba1\u7b97\u5bc6\u5ea6\u77e9\u9635\u7684\u6f14\u5316\u3002", "result": "SPDFT\u65b9\u6cd5\u53ef\u4ee5\u5e94\u7528\u4e8e\u5305\u62ec\u91d1\u5c5e\u548c\u7edd\u7f18\u4f53\u5728\u5185\u7684\u591a\u79cd\u6750\u6599\uff0c\u5728\u91d1\u521a\u77f3\u548c\u91d1\u5c5e\u9502\u65b9\u9762\u6bd4\u7ebf\u6027\u5fae\u6270\u7406\u8bba\u6709\u663e\u8457\u6539\u8fdb\uff0c\u53ef\u4ee5\u9884\u6d4b\u5fb7\u9c81\u5fb7\u5cf0\u3002", "conclusion": "SPDFT\u65b9\u6cd5\u53ef\u4ee5\u5e94\u7528\u4e8e\u91d1\u5c5e\u548c\u7edd\u7f18\u4f53\uff0c\u5e76\u4e14\u5728\u91d1\u521a\u77f3\u548c\u91d1\u5c5e\u9502\u65b9\u9762\u6bd4\u7ebf\u6027\u5fae\u6270\u7406\u8bba\u6709\u663e\u8457\u6539\u8fdb\uff0c\u53ef\u4ee5\u7528\u4e8e\u8ba1\u7b97\u5149\u8c31\u548c\u4f20\u8f93\u6027\u8d28\u3002"}}
{"id": "2507.13816", "categories": ["cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2507.13816", "abs": "https://arxiv.org/abs/2507.13816", "authors": ["D. A. Shchigarev", "A. V. Larionov", "L. V. Kulik", "E. M. Budanov", "I. V. Kukushkin", "V. Umansky"], "title": "Resonant Photoluminescence of Quantum Incompressible Liquids", "comment": "6 pages, 5 figures", "summary": "We investigate resonant photoluminescence arising from incompressible quantum\nliquids formed in two-dimensional electron systems. We demonstrate that, for\nexcitons composed of a photoexcited electron occupying the upper spin sublevel\nof the zeroth Landau level and a valence-band hole, the influence of disorder\npotential fluctuations on optical recombination is strongly suppressed,\nindicating complete screening of the disorder. We identify an optical invariant\nquantity that is insensitive to excitation energy yet strongly dependent on the\nelectron temperature, serving as a probe of exciton recombination in quantum\nliquids. Analysis of this quantity reveals that quantum-liquid formation\ninitiates at (n = 1/3) as the electron temperature decreases, consistent with\nthe Laughlin state. Upon further cooling, the range of filling factors\nexhibiting quantum-liquid behavior expands continuously from (n = 1/3) toward\n(n = 1/2). Transitions between distinct incompressible quantum-liquid states\noccur smoothly, without well-defined phase boundaries separating insulating and\nconducting regimes. Locally, the system retains quantum-liquid characteristics\neven as bulk transport measurements indicate finite conductivity. Finally, we\npresent a phase diagram delineating the stability region of incompressible\nquantum liquids relative to conductive phases.", "AI": {"tldr": "\u4e8c\u7ef4\u7535\u5b50\u7cfb\u7edf\u4e2d\uff0c\u7814\u7a76\u4e86\u91cf\u5b50\u6db2\u4f53\u7684\u5149\u5b66\u6027\u8d28\uff0c\u53d1\u73b0\u4e86\u5149\u5b66\u4e0d\u53d8\u91cf\uff0c\u5e76\u7ed8\u5236\u4e86\u91cf\u5b50\u6db2\u4f53\u7684\u76f8\u56fe\u3002", "motivation": "\u7814\u7a76\u4e0d\u53ef\u538b\u7f29\u91cf\u5b50\u6db2\u4f53\u5728\u4e8c\u7ef4\u7535\u5b50\u7cfb\u7edf\u4e2d\u7684\u5149\u5b66\u6027\u8d28\uff0c\u7279\u522b\u662f\u5149\u81f4\u53d1\u5149\u73b0\u8c61\u3002", "method": "\u901a\u8fc7\u7814\u7a76\u4e8c\u7ef4\u7535\u5b50\u7cfb\u7edf\u4e2d\u7531\u5149\u6fc0\u53d1\u7535\u5b50\u548c\u4ef7\u5e26\u7a7a\u7a74\u7ec4\u6210\u7684\u6fc0\u5b50\uff0c\u4ee5\u53ca\u5176\u5171\u632f\u5149\u81f4\u53d1\u5149\u6765\u5206\u6790\u4e0d\u53ef\u538b\u7f29\u91cf\u5b50\u6db2\u4f53\u3002", "result": "\u91cf\u5b50\u6db2\u4f53\u5f62\u6210\u4e8en=1/3\uff0c\u5e76\u968f\u7740\u6e29\u5ea6\u964d\u4f4e\u5411n=1/2\u6269\u5c55\uff0c\u53d1\u73b0\u4e86\u5149\u5b66\u4e0d\u53d8\u91cf\uff0c\u8bc1\u660e\u4e86\u6fc0\u5b50\u590d\u5408\u5728\u91cf\u5b50\u6db2\u4f53\u4e2d\u7684\u884c\u4e3a\u3002", "conclusion": "\u8be5\u7814\u7a76\u53d1\u73b0\u4e86\u4e8c\u7ef4\u7535\u5b50\u7cfb\u7edf\u4e2d\u4e0d\u53ef\u538b\u7f29\u91cf\u5b50\u6db2\u4f53\u7684\u5171\u632f\u5149\u81f4\u53d1\u5149\u73b0\u8c61\uff0c\u5e76\u786e\u5b9a\u4e86\u4e00\u4e2a\u4e0e\u6fc0\u53d1\u80fd\u91cf\u65e0\u5173\u4f46\u4e0e\u7535\u5b50\u6e29\u5ea6\u5bc6\u5207\u76f8\u5173\u7684\u5149\u5b66\u4e0d\u53d8\u91cf\uff0c\u53ef\u7528\u4f5c\u91cf\u5b50\u6db2\u4f53\u4e2d\u6fc0\u5b50\u590d\u5408\u7684\u63a2\u9488\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u91cf\u5b50\u6db2\u4f53\u5f62\u6210\u4e8en=1/3\uff0c\u5e76\u968f\u7740\u6e29\u5ea6\u964d\u4f4e\u5411n=1/2\u6269\u5c55\uff0c\u4e0d\u540c\u91cf\u5b50\u6db2\u4f53\u72b6\u6001\u4e4b\u95f4\u7684\u8f6c\u53d8\u662f\u5e73\u6ed1\u7684\uff0c\u6ca1\u6709\u660e\u786e\u7684\u76f8\u8fb9\u754c\u3002"}}
{"id": "2507.13834", "categories": ["cs.LG", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.13834", "abs": "https://arxiv.org/abs/2507.13834", "authors": ["Aditi Anand", "Suman Banerjee", "Dildar Ali"], "title": "Scalable Submodular Policy Optimization via Pruned Submodularity Graph", "comment": "16 Pages", "summary": "In Reinforcement Learning (abbreviated as RL), an agent interacts with the\nenvironment via a set of possible actions, and a reward is generated from some\nunknown distribution. The task here is to find an optimal set of actions such\nthat the reward after a certain time step gets maximized. In a traditional\nsetup, the reward function in an RL Problem is considered additive. However, in\nreality, there exist many problems, including path planning, coverage control,\netc., the reward function follows the diminishing return, which can be modeled\nas a submodular function. In this paper, we study a variant of the RL Problem\nwhere the reward function is submodular, and our objective is to find an\noptimal policy such that this reward function gets maximized. We have proposed\na pruned submodularity graph-based approach that provides a provably\napproximate solution in a feasible computation time. The proposed approach has\nbeen analyzed to understand its time and space requirements as well as a\nperformance guarantee. We have experimented with a benchmark agent-environment\nsetup, which has been used for similar previous studies, and the results are\nreported. From the results, we observe that the policy obtained by our proposed\napproach leads to more reward than the baseline methods.", "AI": {"tldr": "\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\uff0c\u5176\u4e2d\u5956\u52b1\u51fd\u6570\u662f\u6b21\u6a21\u7684\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4fee\u526a\u7684\u6b21\u6a21\u56fe\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u53ef\u8bc1\u660e\u7684\u8fd1\u4f3c\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5728\u57fa\u51c6\u73af\u5883\u4e2d\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c\u7ed3\u679c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u5728\u5956\u52b1\u51fd\u6570\u4e3a\u6b21\u6a21\u51fd\u6570\uff08\u5177\u6709\u9012\u51cf\u6536\u76ca\uff09\u7684\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u95ee\u9898\u53d8\u4f53\u4e2d\uff0c\u65e8\u5728\u627e\u5230\u6700\u5927\u5316\u5956\u52b1\u51fd\u6570\u7684\u4f18\u5316\u7b56\u7565\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4fee\u526a\u7684\u6b21\u6a21\u56fe\u65b9\u6cd5", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u80fd\u83b7\u5f97\u66f4\u591a\u7684\u5956\u52b1\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4fee\u526a\u7684\u6b21\u6a21\u56fe\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u53ef\u5728\u53ef\u884c\u8ba1\u7b97\u65f6\u95f4\u5185\u63d0\u4f9b\u53ef\u8bc1\u660e\u7684\u8fd1\u4f3c\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u4fdd\u8bc1\u4e86\u6027\u80fd\u3002"}}
{"id": "2507.13994", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2507.13994", "abs": "https://arxiv.org/abs/2507.13994", "authors": ["Benjamin Aram Berendsohn"], "title": "Optimal antimatroid sorting", "comment": "Accepted to ESA 2025", "summary": "The classical comparison-based sorting problem asks us to find the underlying\ntotal order of a given set of elements, where we can only access the elements\nvia comparisons. In this paper, we study a restricted version, where, as a\nhint, a set $T$ of possible total orders is given, usually in some compressed\nform.\n  Recently, an algorithm called topological heapsort with optimal running time\nwas found for the case where $T$ is the set of topological orderings of a given\ndirected acyclic graph, or, equivalently, $T$ is the set of linear extensions\nof a given partial order [Haeupler et al. 2024]. We show that a simple\ngeneralization of topological heapsort is applicable to a much broader class of\nrestricted sorting problems, where $T$ corresponds to a given antimatroid.\n  As a consequence, we obtain optimal algorithms for the following restricted\nsorting problems, where the allowed total orders are restricted by: a given set\nof monotone precedence formulas; the perfect elimination orders of a given\nchordal graph; or the possible vertex search orders of a given connected rooted\ngraph.", "AI": {"tldr": "\u672c\u7814\u7a76\u5c06\u62d3\u6251\u5806\u6392\u5e8f\u63a8\u5e7f\u5230\u57fa\u4e8e\u6297\u62df\u5408 \u0938\u092e \u7684\u6392\u5e8f\u95ee\u9898\uff0c\u5e76\u4e3a\u591a\u79cd\u53d7\u9650\u6392\u5e8f\u95ee\u9898\u63d0\u4f9b\u4e86\u6700\u4f18\u7b97\u6cd5\u3002", "motivation": "\u7814\u7a76\u53d7\u9650\u6392\u5e8f\u95ee\u9898\uff0c\u7279\u522b\u662f\u5f53\u53ef\u80fd\u7684\u5168\u5e8f\u96c6\u5408 T \u4ee5\u538b\u7f29\u5f62\u5f0f\u7ed9\u51fa\u65f6\uff0c\u5e76\u5bfb\u627e\u6700\u4f18\u7b97\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u62d3\u6251\u5806\u6392\u5e8f\u7684\u7b80\u5355\u63a8\u5e7f\u65b9\u6cd5\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u591a\u79cd\u53d7\u9650\u6392\u5e8f\u95ee\u9898\u3002", "result": "\u4e3a\u53d7\u9650\u6392\u5e8f\u95ee\u9898\uff08\u5305\u62ec\u7531\u5355\u8c03\u4f18\u5148\u516c\u5f0f\u3001\u5f26\u56fe\u7684\u5b8c\u7f8e\u6d88\u9664\u5e8f\u6216\u8fde\u901a\u6839\u56fe\u7684\u53ef\u80fd\u9876\u70b9\u641c\u7d22\u5e8f\u9650\u5236\u7684\u5168\u5e8f\uff09\u63d0\u4f9b\u4e86\u6700\u4f18\u7b97\u6cd5\u3002 T \u5bf9\u5e94\u4e8e\u7ed9\u5b9a\u7684\u6297\u62df\u5408 \u0938\u092e\u3002 \u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u8fd8\u9002\u7528\u4e8e\u7531\u5355\u8c03\u4f18\u5148\u516c\u5f0f\u3001\u5f26\u56fe\u7684\u5b8c\u7f8e\u6d88\u9664\u5e8f\u6216\u8fde\u901a\u6839\u56fe\u7684\u53ef\u80fd\u9876\u70b9\u641c\u7d22\u5e8f\u9650\u5236\u7684\u5168\u5e8f\u3002  \u4e0e\u73b0\u6709\u7b97\u6cd5\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u66f4\u5e7f\u6cdb\u7684\u9002\u7528\u6027\uff0c\u5e76\u80fd\u5728\u7279\u5b9a\u60c5\u51b5\u4e0b\u5b9e\u73b0\u6700\u4f18\u8fd0\u884c\u65f6\u95f4\u3002", "conclusion": "\u672c\u7814\u7a76\u5c06\u62d3\u6251\u5806\u6392\u5e8f\u7684\u7b80\u5355\u63a8\u5e7f\u5e94\u7528\u4e8e\u66f4\u5e7f\u6cdb\u7684\u53d7\u9650\u6392\u5e8f\u95ee\u9898\uff0c\u5176\u4e2d T \u5bf9\u5e94\u4e8e\u7ed9\u5b9a\u7684\u6297\u62df\u5408 \u0938\u092e\uff0c\u4ece\u800c\u4e3a\u4e00\u7cfb\u5217\u53d7\u9650\u6392\u5e8f\u95ee\u9898\u63d0\u4f9b\u4e86\u6700\u4f18\u7b97\u6cd5\u3002"}}
{"id": "2507.13395", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13395", "abs": "https://arxiv.org/abs/2507.13395", "authors": ["Xuanqi Gao", "Weipeng Jiang", "Juan Zhai", "Shiqing Ma", "Siyi Xie", "Xinyang Yin", "Chao Shen"], "title": "Mitigating Stylistic Biases of Machine Translation Systems via Monolingual Corpora Only", "comment": null, "summary": "The advent of neural machine translation (NMT) has revolutionized\ncross-lingual communication, yet preserving stylistic nuances remains a\nsignificant challenge. While existing approaches often require parallel corpora\nfor style preservation, we introduce Babel, a novel framework that enhances\nstylistic fidelity in NMT using only monolingual corpora. Babel employs two key\ncomponents: (1) a style detector based on contextual embeddings that identifies\nstylistic disparities between source and target texts, and (2) a\ndiffusion-based style applicator that rectifies stylistic inconsistencies while\nmaintaining semantic integrity. Our framework integrates with existing NMT\nsystems as a post-processing module, enabling style-aware translation without\nrequiring architectural modifications or parallel stylistic data. Extensive\nexperiments on five diverse domains (law, literature, scientific writing,\nmedicine, and educational content) demonstrate Babel's effectiveness: it\nidentifies stylistic inconsistencies with 88.21% precision and improves\nstylistic preservation by 150% while maintaining a high semantic similarity\nscore of 0.92. Human evaluation confirms that translations refined by Babel\nbetter preserve source text style while maintaining fluency and adequacy.", "AI": {"tldr": "Babel\u662f\u4e00\u4e2a\u65b0\u6846\u67b6\uff0c\u4ec5\u4f7f\u7528\u5355\u8bed\u8bed\u6599\u5e93\uff0c\u901a\u8fc7\u98ce\u683c\u68c0\u6d4b\u5668\u548c\u6269\u6563\u5f0f\u98ce\u683c\u5e94\u7528\u5668\u6765\u63d0\u9ad8\u795e\u7ecf\u673a\u5668\u7ffb\u8bd1\u4e2d\u7684\u98ce\u683c\u4fdd\u771f\u5ea6\u3002", "motivation": "\u5c3d\u7ba1\u795e\u7ecf\u673a\u5668\u7ffb\u8bd1\u53d6\u5f97\u4e86\u9769\u547d\u6027\u8fdb\u5c55\uff0c\u4f46\u5728\u4fdd\u7559\u98ce\u683c\u7ec6\u5fae\u5dee\u522b\u65b9\u9762\u4ecd\u7136\u9762\u4e34\u6311\u6218\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u5e73\u884c\u8bed\u6599\u5e93\u6765\u8fdb\u884c\u98ce\u683c\u4fdd\u7559\u3002", "method": "Babel\u6846\u67b6\u5305\u542b\u4e00\u4e2a\u57fa\u4e8e\u4e0a\u4e0b\u6587\u5d4c\u5165\u7684\u98ce\u683c\u68c0\u6d4b\u5668\uff0c\u7528\u4e8e\u8bc6\u522b\u6e90\u6587\u672c\u548c\u76ee\u6807\u6587\u672c\u4e4b\u95f4\u7684\u98ce\u683c\u5dee\u5f02\uff0c\u4ee5\u53ca\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u7684\u98ce\u683c\u5e94\u7528\u5668\uff0c\u7528\u4e8e\u5728\u4fdd\u6301\u8bed\u4e49\u5b8c\u6574\u6027\u7684\u540c\u65f6\u7ea0\u6b63\u98ce\u683c\u4e0d\u4e00\u81f4\u3002\u8be5\u6846\u67b6\u4f5c\u4e3a\u540e\u5904\u7406\u6a21\u5757\u96c6\u6210\u5230\u73b0\u6709\u7684\u795e\u7ecf\u673a\u5668\u7ffb\u8bd1\u7cfb\u7edf\u4e2d\u3002", "result": "Babel\u6846\u67b6\u5728\u6cd5\u5f8b\u3001\u6587\u5b66\u3001\u79d1\u5b66\u5199\u4f5c\u3001\u533b\u5b66\u548c\u6559\u80b2\u5185\u5bb9\u7b49\u4e94\u4e2a\u4e0d\u540c\u9886\u57df\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u7684\u8bc6\u522b\u98ce\u683c\u4e0d\u4e00\u81f4\u7684\u51c6\u786e\u7387\u4e3a88.21%\uff0c\u98ce\u683c\u4fdd\u7559\u80fd\u529b\u63d0\u9ad8\u4e86150%\uff0c\u540c\u65f6\u4fdd\u6301\u4e860.92\u7684\u9ad8\u8bed\u4e49\u76f8\u4f3c\u6027\u5f97\u5206\u3002\u4eba\u7c7b\u8bc4\u4f30\u4e5f\u8bc1\u5b9e\u4e86Babel\u6da6\u8272\u7684\u7ffb\u8bd1\u80fd\u591f\u66f4\u597d\u5730\u4fdd\u7559\u6e90\u6587\u672c\u98ce\u683c\uff0c\u540c\u65f6\u4fdd\u6301\u6d41\u7545\u6027\u548c\u5145\u5206\u6027\u3002", "conclusion": "Babel\u6846\u67b6\u901a\u8fc7\u4ec5\u4f7f\u7528\u5355\u8bed\u8bed\u6599\u5e93\uff0c\u5728\u795e\u7ecf\u673a\u5668\u7ffb\u8bd1\u4e2d\u589e\u5f3a\u4e86\u98ce\u683c\u4fdd\u771f\u5ea6\uff0c\u5e76\u5728\u5404\u79cd\u9886\u57df\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u98ce\u683c\u4fdd\u7559\u548c\u9ad8\u8bed\u4e49\u76f8\u4f3c\u6027\u3002"}}
{"id": "2507.13624", "categories": ["cs.LG", "cs.DC", "cs.NI"], "pdf": "https://arxiv.org/pdf/2507.13624", "abs": "https://arxiv.org/abs/2507.13624", "authors": ["Daniel Commey", "Kamel Abbad", "Garth V. Crosby", "Lyes Khoukhi"], "title": "FedSkipTwin: Digital-Twin-Guided Client Skipping for Communication-Efficient Federated Learning", "comment": null, "summary": "Communication overhead remains a primary bottleneck in federated learning\n(FL), particularly for applications involving mobile and IoT devices with\nconstrained bandwidth. This work introduces FedSkipTwin, a novel\nclient-skipping algorithm driven by lightweight, server-side digital twins.\nEach twin, implemented as a simple LSTM, observes a client's historical\nsequence of gradient norms to forecast both the magnitude and the epistemic\nuncertainty of its next update. The server leverages these predictions,\nrequesting communication only when either value exceeds a predefined threshold;\notherwise, it instructs the client to skip the round, thereby saving bandwidth.\nExperiments are conducted on the UCI-HAR and MNIST datasets with 10 clients\nunder a non-IID data distribution. The results demonstrate that FedSkipTwin\nreduces total communication by 12-15.5% across 20 rounds while simultaneously\nimproving final model accuracy by up to 0.5 percentage points compared to the\nstandard FedAvg algorithm. These findings establish that prediction-guided\nskipping is a practical and effective strategy for resource-aware FL in\nbandwidth-constrained edge environments.", "AI": {"tldr": "FedSkipTwin\u901a\u8fc7\u670d\u52a1\u5668\u7aef\u7684LSTM\u6570\u5b57\u5b6a\u751f\u9884\u6d4b\u5ba2\u6237\u7aef\u66f4\u65b0\u7684\u5e45\u5ea6\u548c\u4e0d\u786e\u5b9a\u6027\uff0c\u4ece\u800c\u4ec5\u5728\u9700\u8981\u65f6\u624d\u8bf7\u6c42\u901a\u4fe1\uff0c\u4ee5\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\uff0c\u540c\u65f6\u63d0\u9ad8\u6a21\u578b\u7cbe\u5ea6\u3002", "motivation": "\u901a\u4fe1\u5f00\u9500\u662f\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u4e2d\u7684\u4e3b\u8981\u74f6\u9888\uff0c\u5c24\u5176\u662f\u5728\u6d89\u53ca\u79fb\u52a8\u548c\u7269\u8054\u7f51\u8bbe\u5907\u7684\u5e26\u5bbd\u53d7\u9650\u5e94\u7528\u4e2d\u3002", "method": "FedSkipTwin\u901a\u8fc7\u5728\u670d\u52a1\u5668\u7aef\u4f7f\u7528\u7b80\u5355\u7684LSTM\u5b9e\u73b0\u8f7b\u91cf\u7ea7\u6570\u5b57\u5b6a\u751f\uff0c\u89c2\u5bdf\u5ba2\u6237\u7aef\u7684\u68af\u5ea6\u8303\u6570\u5386\u53f2\u5e8f\u5217\uff0c\u9884\u6d4b\u5176\u4e0b\u6b21\u66f4\u65b0\u7684\u5e45\u5ea6\u548c\u4e0d\u786e\u5b9a\u6027\u3002\u670d\u52a1\u5668\u5229\u7528\u8fd9\u4e9b\u9884\u6d4b\uff0c\u4ec5\u5728\u5176\u4e2d\u4e00\u4e2a\u503c\u8d85\u8fc7\u9884\u5b9a\u9608\u503c\u65f6\u624d\u8bf7\u6c42\u901a\u4fe1\uff0c\u5426\u5219\u6307\u793a\u5ba2\u6237\u7aef\u8df3\u8fc7\u8be5\u8f6e\u6b21\u3002", "result": "FedSkipTwin\u5728UCI-HAR\u548cMNIST\u6570\u636e\u96c6\u4e0a\uff0c\u572810\u4e2a\u5ba2\u6237\u7aef\u548c\u975eIID\u6570\u636e\u5206\u5e03\u4e0b\u8fdb\u884c\u4e86\u5b9e\u9a8c\u3002\u7ed3\u679c\u8868\u660e\uff0c\u572820\u8f6e\u901a\u4fe1\u4e2d\uff0cFedSkipTwin\u5c06\u603b\u901a\u4fe1\u91cf\u51cf\u5c11\u4e8612-15.5%\uff0c\u540c\u65f6\u5c06\u6700\u7ec8\u6a21\u578b\u7cbe\u5ea6\u63d0\u9ad8\u4e86\u591a\u8fbe0.5\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "FedSkipTwin\u662f\u4e00\u79cd\u5b9e\u7528\u7684\u3001\u6709\u6548\u7684\u7b56\u7565\uff0c\u9002\u7528\u4e8e\u5e26\u5bbd\u53d7\u9650\u7684\u8fb9\u7f18\u73af\u5883\u4e2d\u7684\u8d44\u6e90\u611f\u77e5\u8054\u90a6\u5b66\u4e60\u3002"}}
{"id": "2507.13416", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13416", "abs": "https://arxiv.org/abs/2507.13416", "authors": ["Jiaxiang Yi", "Bernardo P. Ferreira", "Miguel A. Bessa"], "title": "Single- to multi-fidelity history-dependent learning with uncertainty quantification and disentanglement: application to data-driven constitutive modeling", "comment": "40 pages, 32 figures", "summary": "Data-driven learning is generalized to consider history-dependent\nmulti-fidelity data, while quantifying epistemic uncertainty and disentangling\nit from data noise (aleatoric uncertainty). This generalization is hierarchical\nand adapts to different learning scenarios: from training the simplest\nsingle-fidelity deterministic neural networks up to the proposed multi-fidelity\nvariance estimation Bayesian recurrent neural networks. The versatility and\ngenerality of the proposed methodology are demonstrated by applying it to\ndifferent data-driven constitutive modeling scenarios that include multiple\nfidelities with and without aleatoric uncertainty (noise). The method\naccurately predicts the response and quantifies model error while also\ndiscovering the noise distribution (when present). This opens opportunities for\nfuture real-world applications in diverse scientific and engineering domains;\nespecially, the most challenging cases involving design and analysis under\nuncertainty.", "AI": {"tldr": "\u6570\u636e\u9a71\u52a8\u5b66\u4e60\u5df2\u63a8\u5e7f\u5230\u5904\u7406\u5177\u6709\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u548c\u968f\u673a\u4e0d\u786e\u5b9a\u6027\u7684\u5386\u53f2\u4f9d\u8d56\u591a\u4fdd\u771f\u5ea6\u6570\u636e\uff0c\u53ef\u7528\u4e8e\u5404\u79cd\u79d1\u5b66\u548c\u5de5\u7a0b\u9886\u57df\u3002", "motivation": "\u5c06\u6570\u636e\u9a71\u52a8\u5b66\u4e60\u63a8\u5e7f\u5230\u8003\u8651\u5386\u53f2\u4f9d\u8d56\u7684\u591a\u4fdd\u771f\u5ea6\u6570\u636e\uff0c\u540c\u65f6\u91cf\u5316\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u5e76\u5c06\u5176\u4e0e\u6570\u636e\u566a\u58f0\uff08\u968f\u673a\u4e0d\u786e\u5b9a\u6027\uff09\u5206\u79bb\u3002", "method": "\u8be5\u65b9\u6cd5\u5c06\u6570\u636e\u9a71\u52a8\u5b66\u4e60\u63a8\u5e7f\u5230\u8003\u8651\u5386\u53f2\u4f9d\u8d56\u7684\u591a\u4fdd\u771f\u5ea6\u6570\u636e\uff0c\u540c\u65f6\u91cf\u5316\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u5e76\u5c06\u5176\u4e0e\u6570\u636e\u566a\u58f0\uff08\u968f\u673a\u4e0d\u786e\u5b9a\u6027\uff09\u5206\u79bb\u3002\u8fd9\u79cd\u63a8\u5e7f\u662f\u5206\u5c42\u7684\uff0c\u5e76\u80fd\u9002\u5e94\u4e0d\u540c\u7684\u5b66\u4e60\u573a\u666f\uff1a\u4ece\u8bad\u7ec3\u6700\u7b80\u5355\u7684\u5355\u4fdd\u771f\u5ea6\u786e\u5b9a\u6027\u795e\u7ecf\u7f51\u7edc\u5230\u63d0\u51fa\u7684\u591a\u4fdd\u771f\u5ea6\u65b9\u5dee\u4f30\u8ba1\u8d1d\u53f6\u65af\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u5305\u542b\u6709\u566a\u58f0\u548c\u65e0\u566a\u58f0\u7684\u591a\u4fdd\u771f\u5ea6\u6570\u636e\u7684\u4e0d\u540c\u6570\u636e\u9a71\u52a8\u672c\u6784\u5efa\u6a21\u573a\u666f\u4e2d\u5f97\u5230\u4e86\u9a8c\u8bc1\uff0c\u663e\u793a\u4e86\u5176\u591a\u529f\u80fd\u6027\u548c\u901a\u7528\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u51c6\u786e\u9884\u6d4b\u54cd\u5e94\u5e76\u91cf\u5316\u6a21\u578b\u8bef\u5dee\uff0c\u540c\u65f6\u53d1\u73b0\u566a\u58f0\u5206\u5e03\uff08\u5982\u679c\u5b58\u5728\uff09\uff0c\u4e3a\u672a\u6765\u5728\u4e0d\u540c\u79d1\u5b66\u548c\u5de5\u7a0b\u9886\u57df\u7684\u5b9e\u9645\u5e94\u7528\u6253\u5f00\u4e86\u673a\u4f1a\uff0c\u5c24\u5176\u662f\u5728\u6d89\u53ca\u4e0d\u786e\u5b9a\u6027\u8bbe\u8ba1\u548c\u5206\u6790\u7684\u6700\u5177\u6311\u6218\u6027\u7684\u60c5\u51b5\u4e0b\u3002"}}
{"id": "2507.13577", "categories": ["cs.SI"], "pdf": "https://arxiv.org/pdf/2507.13577", "abs": "https://arxiv.org/abs/2507.13577", "authors": ["Adaeze Okeukwu-Ogbonnaya", "Rahul Amatapu", "Jason Bergtold", "George Amariucai"], "title": "LLM-Based Community Surveys for Operational Decision Making in Interconnected Utility Infrastructures", "comment": null, "summary": "We represent interdependent infrastructure systems and communities alike with\na hetero-functional graph (HFG) that encodes the dependencies between\nfunctionalities. This graph naturally imposes a partial order of\nfunctionalities that can inform the sequence of repair decisions to be made\nduring a disaster across affected communities. However, using such technical\ncriteria alone provides limited guidance at the point where the functionalities\ndirectly impact the communities, since these can be repaired in any order\nwithout violating the system constraints. To address this gap and improve\nresilience, we integrate community preferences to refine this partial order\nfrom the HFG into a total order. Our strategy involves getting the communities'\nopinions on their preferred sequence for repair crews to address infrastructure\nissues, considering potential constraints on resources. Due to the delay and\ncost associated with real-world survey data, we utilize a Large Language Model\n(LLM) as a proxy survey tool. We use the LLM to craft distinct personas\nrepresenting individuals, each with varied disaster experiences. We construct\ndiverse disaster scenarios, and each simulated persona provides input on\nprioritizing infrastructure repair needs across various communities. Finally,\nwe apply learning algorithms to generate a global order based on the aggregated\nresponses from these LLM-generated personas.", "AI": {"tldr": "\u7814\u7a76\u5229\u7528LLM\u6a21\u62df\u793e\u533a\u504f\u597d\uff0c\u7ed3\u5408\u57fa\u7840\u8bbe\u65bd\u4f9d\u8d56\u56fe\uff0c\u751f\u6210\u66f4\u4f18\u7684\u707e\u540e\u4fee\u590d\u987a\u5e8f\uff0c\u4ee5\u589e\u5f3a\u793e\u533a\u97e7\u6027\u3002", "motivation": "\u5f53\u524d\u7684\u57fa\u7840\u8bbe\u65bd\u4fee\u590d\u51b3\u7b56\u4e3b\u8981\u4f9d\u8d56\u4e8e\u7cfb\u7edf\u7ea6\u675f\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u5728\u5f71\u54cd\u793e\u533a\u7684\u529f\u80fd\u6027\u95ee\u9898\u4e0a\u6307\u5bfc\u6709\u9650\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u529f\u80fd\u53ef\u4ee5\u6309\u4efb\u4f55\u987a\u5e8f\u4fee\u590d\u800c\u4e0d\u8fdd\u53cd\u7cfb\u7edf\u7ea6\u675f\u3002\u4e3a\u4e86\u5f25\u8865\u8fd9\u4e00\u5dee\u8ddd\u5e76\u63d0\u9ad8\u97e7\u6027\uff0c\u6709\u5fc5\u8981\u6574\u5408\u793e\u533a\u504f\u597d\u6765\u6307\u5bfc\u4fee\u590d\u987a\u5e8f\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u5f02\u6784\u529f\u80fd\u56fe\uff08HFG\uff09\u6765\u8868\u793a\u76f8\u4e92\u4f9d\u8d56\u7684\u57fa\u7840\u8bbe\u65bd\u7cfb\u7edf\u548c\u793e\u533a\uff0c\u8be5\u56fe\u7f16\u7801\u4e86\u529f\u80fd\u4e4b\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\uff0c\u5e76\u81ea\u7136\u5730\u65bd\u52a0\u4e86\u529f\u80fd\u7684\u90e8\u5206\u987a\u5e8f\u3002\u4e3a\u4e86\u5f25\u5408\u6280\u672f\u6807\u51c6\u4e0e\u793e\u533a\u9700\u6c42\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u672c\u7814\u7a76\u5c06\u793e\u533a\u504f\u597d\u6574\u5408\u5230HFG\u7684\u90e8\u5206\u987a\u5e8f\u4e2d\uff0c\u5f62\u6210\u4e00\u4e2a\u603b\u987a\u5e8f\u3002\u7814\u7a76\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4f5c\u4e3a\u4ee3\u7406\u8c03\u67e5\u5de5\u5177\uff0c\u901a\u8fc7\u521b\u5efa\u5177\u6709\u4e0d\u540c\u707e\u96be\u7ecf\u5386\u7684\u865a\u62df\u4eba\u7269\uff0c\u5e76\u8ba9\u4ed6\u4eec\u9488\u5bf9\u591a\u6837\u5316\u7684\u707e\u96be\u573a\u666f\u63d0\u51fa\u5bf9\u793e\u533a\u57fa\u7840\u8bbe\u65bd\u4fee\u590d\u9700\u6c42\u7684\u4f18\u5148\u7ea7\u6392\u5e8f\u3002\u6700\u540e\uff0c\u7814\u7a76\u5e94\u7528\u5b66\u4e60\u7b97\u6cd5\u6839\u636eLLM\u751f\u6210\u4eba\u7269\u7684\u805a\u5408\u54cd\u5e94\u6765\u751f\u6210\u5168\u5c40\u987a\u5e8f\u3002", "result": "\u8be5\u7814\u7a76\u6210\u529f\u5730\u5c55\u793a\u4e86\u5982\u4f55\u5229\u7528LLM\u6a21\u62df\u793e\u533a\u504f\u597d\uff0c\u5e76\u5c06\u5176\u4e0eHFG\u7ed3\u5408\uff0c\u751f\u6210\u4e00\u4e2a\u8003\u8651\u4e86\u7cfb\u7edf\u7ea6\u675f\u548c\u793e\u533a\u9700\u6c42\u7684\u57fa\u7840\u8bbe\u65bd\u4fee\u590d\u603b\u987a\u5e8f\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5f02\u6784\u529f\u80fd\u56fe\uff08HFG\uff09\u548c\u793e\u533a\u504f\u597d\u6765\u786e\u5b9a\u57fa\u7840\u8bbe\u65bd\u4fee\u590d\u987a\u5e8f\u7684\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u707e\u96be\u54cd\u5e94\u7684\u97e7\u6027\u3002\u901a\u8fc7\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6a21\u62df\u4e0d\u540c\u793e\u533a\u5c45\u6c11\u7684\u504f\u597d\uff0c\u5e76\u7ed3\u5408\u5b66\u4e60\u7b97\u6cd5\u751f\u6210\u6574\u4f53\u4fee\u590d\u987a\u5e8f\uff0c\u4e3a\u5b9e\u9645\u707e\u96be\u7ba1\u7406\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u3002"}}
{"id": "2507.13766", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.13766", "abs": "https://arxiv.org/abs/2507.13766", "authors": ["Kai Wu", "Zhongqin Wang", "Shu-Lin Chen", "J. Andrew Zhang", "Y. Jay Guo"], "title": "ISAC: From Human to Environmental Sensing", "comment": "15 pages, 8 figures", "summary": "Integrated Sensing and Communications (ISAC) is poised to become one of the\ndefining capabilities of the sixth generation (6G) wireless communications\nsystems, enabling the network infrastructure to jointly support high-throughput\ncommunications and situational awareness. While recent advances have explored\nISAC for both human-centric applications and environmental monitoring, existing\nresearch remains fragmented across these domains. This paper provides the first\nunified review of ISAC-enabled sensing for both human activities and\nenvironment, focusing on signal-level mechanisms, sensing features, and\nreal-world feasibility. We begin by characterising how diverse physical\nphenomena, ranging from human vital sign and motion to precipitation and flood\ndynamics, impact wireless signal propagation, producing measurable signatures\nin channel state information (CSI), Doppler profiles, and signal statistics. A\ncomprehensive analysis is then presented across two domains: human sensing\napplications including localisation, activity recognition, and vital sign\nmonitoring; and environmental sensing for rainfall, soil moisture, and water\nlevel. Experimental results from Long-Term Evolution (LTE) sensing under\nnon-line-of-sight (NLOS) conditions are incorporated to highlight the\nfeasibility in infrastructure-limited scenarios. Open challenges in signal\nfusion, domain adaptation, and generalisable sensing architectures are\ndiscussed to facilitate future research toward scalable and autonomous ISAC.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.13371", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13371", "abs": "https://arxiv.org/abs/2507.13371", "authors": ["Yeming Cai", "Yang Wang", "Zhenglin Li"], "title": "Transformer-Based Framework for Motion Capture Denoising and Anomaly Detection in Medical Rehabilitation", "comment": null, "summary": "This paper proposes an end-to-end deep learning framework integrating optical\nmotion capture with a Transformer-based model to enhance medical\nrehabilitation. It tackles data noise and missing data caused by occlusion and\nenvironmental factors, while detecting abnormal movements in real time to\nensure patient safety. Utilizing temporal sequence modeling, our framework\ndenoises and completes motion capture data, improving robustness. Evaluations\non stroke and orthopedic rehabilitation datasets show superior performance in\ndata reconstruction and anomaly detection, providing a scalable, cost-effective\nsolution for remote rehabilitation with reduced on-site supervision.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u96c6\u6210\u4e86\u5149\u5b66\u52a8\u4f5c\u6355\u6349\u548c\u57fa\u4e8eTransformer\u7684\u6a21\u578b\u7684\u7aef\u5230\u7aef\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u589e\u5f3a\u533b\u7597\u5eb7\u590d\u3002\u8be5\u6846\u67b6\u80fd\u591f\u5904\u7406\u6570\u636e\u566a\u58f0\u548c\u7f3a\u5931\uff0c\u5b9e\u65f6\u68c0\u6d4b\u5f02\u5e38\u8fd0\u52a8\uff0c\u5e76\u5728\u91cd\u5efa\u548c\u5f02\u5e38\u68c0\u6d4b\u65b9\u9762\u8868\u73b0\u4f18\u8d8a\uff0c\u4e3a\u8fdc\u7a0b\u5eb7\u590d\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u7531\u906e\u6321\u548c\u73af\u5883\u56e0\u7d20\u5f15\u8d77\u7684\u6570\u636e\u566a\u58f0\u548c\u6570\u636e\u7f3a\u5931\u95ee\u9898\uff0c\u5e76\u5b9e\u65f6\u68c0\u6d4b\u5f02\u5e38\u8fd0\u52a8\u4ee5\u786e\u4fdd\u60a3\u8005\u5b89\u5168\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u57fa\u4e8eTransformer\u7684\u5149\u5b66\u52a8\u4f5c\u6355\u6349\u6a21\u578b\uff0c\u4ee5\u589e\u5f3a\u533b\u7597\u5eb7\u590d\u3002", "result": "\u5728\u5fc3\u810f\u75c5\u548c\u9aa8\u79d1\u5eb7\u590d\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u6570\u636e\u91cd\u5efa\u548c\u5f02\u5e38\u68c0\u6d4b\u65b9\u9762\u8868\u73b0\u4f18\u8d8a\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u8fdc\u7a0b\u533b\u7597\u5eb7\u590d\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u4f4e\u6210\u672c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u51cf\u5c11\u4e86\u73b0\u573a\u76d1\u7763\u7684\u9700\u6c42\u3002"}}
{"id": "2507.13652", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13652", "abs": "https://arxiv.org/abs/2507.13652", "authors": ["Gerben van der Hoek", "Johan Jeuring", "Rogier Bos"], "title": "Combining model tracing and constraint-based modeling for multistep strategy diagnoses", "comment": null, "summary": "Model tracing and constraint-based modeling are two approaches to diagnose\nstudent input in stepwise tasks. Model tracing supports identifying consecutive\nproblem-solving steps taken by a student, whereas constraint-based modeling\nsupports student input diagnosis even when several steps are combined into one\nstep. We propose an approach that merges both paradigms. By defining\nconstraints as properties that a student input has in common with a step of a\nstrategy, it is possible to provide a diagnosis when a student deviates from a\nstrategy even when the student combines several steps. In this study we explore\nthe design of a system for multistep strategy diagnoses, and evaluate these\ndiagnoses. As a proof of concept, we generate diagnoses for an existing dataset\ncontaining steps students take when solving quadratic equations (n=2136). To\ncompare with human diagnoses, two teachers coded a random sample of deviations\n(n=70) and applications of the strategy (n=70). Results show that that the\nsystem diagnosis aligned with the teacher coding in all of the 140 student\nsteps.", "AI": {"tldr": "\u672c\u7814\u7a76\u7ed3\u5408\u6a21\u578b\u8ffd\u8e2a\u4e0e\u7ea6\u675f\u6a21\u578b\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5b66\u751f\u89e3\u9898\u6b65\u9aa4\u8bca\u65ad\u65b9\u6cd5\uff0c\u80fd\u591f\u5904\u7406\u5b66\u751f\u5408\u5e76\u6b65\u9aa4\u7684\u60c5\u51b5\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u8bca\u65ad\u51c6\u786e\u6027\u4e0a\u4e0e\u6559\u5e08\u6c34\u5e73\u76f8\u5f53\u3002", "motivation": "\u4e3a\u4e86\u6539\u8fdb\u5b66\u751f\u5728\u5206\u6b65\u4efb\u52a1\u4e2d\u7684\u8f93\u5165\u8bca\u65ad\uff0c\u7814\u7a76\u8005\u4eec\u63d0\u51fa\u4e86\u5c06\u6a21\u578b\u8ffd\u8e2a\u548c\u7ea6\u675f\u6a21\u578b\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u4ee5\u5e94\u5bf9\u5b66\u751f\u53ef\u80fd\u5408\u5e76\u591a\u4e2a\u89e3\u9898\u6b65\u9aa4\u7684\u60c5\u51b5\uff0c\u4ece\u800c\u63d0\u4f9b\u66f4\u5168\u9762\u7684\u8bca\u65ad\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u878d\u5408\u6a21\u578b\u8ffd\u8e2a\u548c\u7ea6\u675f\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u7ea6\u675f\u5b9a\u4e49\u4e3a\u5b66\u751f\u8f93\u5165\u4e0e\u7b56\u7565\u6b65\u9aa4\u7684\u5171\u540c\u5c5e\u6027\uff0c\u5b9e\u73b0\u4e86\u5bf9\u5b66\u751f\u591a\u6b65\u7b56\u7565\u7684\u8bca\u65ad\u3002\u7814\u7a76\u4e2d\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7cfb\u7edf\u6765\u5b9e\u73b0\u591a\u6b65\u7b56\u7565\u8bca\u65ad\uff0c\u5e76\u4f7f\u7528\u5305\u542b\u5b66\u751f\u89e3\u4e8c\u6b21\u65b9\u7a0b\u6b65\u9aa4\u7684\u6570\u636e\u96c6\uff08n=2136\uff09\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5728\u5bf9\u5305\u542b70\u4e2a\u5b66\u751f\u6b65\u9aa4\u7684\u968f\u673a\u6837\u672c\uff08\u5305\u62ec\u504f\u79bb\u7b56\u7565\u548c\u5e94\u7528\u7b56\u7565\u7684\u60c5\u51b5\uff09\u8fdb\u884c\u5206\u6790\u65f6\uff0c\u8be5\u7cfb\u7edf\u751f\u6210\u7684\u8bca\u65ad\u4e0e\u4e24\u4f4d\u6559\u5e08\u7684\u7f16\u7801\u7ed3\u679c\u5728\u6240\u6709140\u4e2a\u5b66\u751f\u6b65\u9aa4\u4e0a\u5747\u4fdd\u6301\u4e00\u81f4\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u878d\u5408\u6a21\u578b\u8ffd\u8e2a\u548c\u7ea6\u675f\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u5bf9\u5b66\u751f\u7684\u591a\u6b65\u89e3\u9898\u7b56\u7565\u8fdb\u884c\u8bca\u65ad\uff0c\u5e76\u4e14\u5728\u5b66\u751f\u5408\u5e76\u591a\u4e2a\u6b65\u9aa4\u65f6\u4e5f\u80fd\u63d0\u4f9b\u51c6\u786e\u7684\u8bca\u65ad\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u8bca\u65ad\u4e0e\u6559\u5e08\u8bc4\u5206\u9ad8\u5ea6\u4e00\u81f4\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2507.13888", "categories": ["eess.SY", "cs.RO", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.13888", "abs": "https://arxiv.org/abs/2507.13888", "authors": ["Janani S K", "Shishir Kolathaya"], "title": "Fixed time convergence guarantees for Higher Order Control Barrier Functions", "comment": "6 PAGES, 2 FIGURES", "summary": "We present a novel method for designing higher-order Control Barrier\nFunctions (CBFs) that guarantee convergence to a safe set within a\nuser-specified finite. Traditional Higher Order CBFs (HOCBFs) ensure asymptotic\nsafety but lack mechanisms for fixed-time convergence, which is critical in\ntime-sensitive and safety-critical applications such as autonomous navigation.\nIn contrast, our approach imposes a structured differential constraint using\nrepeated roots in the characteristic polynomial, enabling closed-form\npolynomial solutions with exact convergence at a prescribed time. We derive\nconditions on the barrier function and its derivatives that ensure forward\ninvariance and fixed-time reachability, and we provide an explicit formulation\nfor second-order systems. Our method is evaluated on three robotic systems - a\npoint-mass model, a unicycle, and a bicycle model and benchmarked against\nexisting HOCBF approaches. Results demonstrate that our formulation reliably\nenforces convergence within the desired time, even when traditional methods\nfail. This work provides a tractable and robust framework for real-time control\nwith provable finite-time safety guarantees.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9ad8\u9636\u63a7\u5236\u5c4f\u969c\u51fd\u6570\uff08CBFs\uff09\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f7f\u7528\u7279\u5f81\u591a\u9879\u5f0f\u4e2d\u7684\u91cd\u6839\u6765\u5b9e\u73b0\u56fa\u5b9a\u65f6\u95f4\u6536\u655b\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u65f6\u95f4\u654f\u611f\u5e94\u7528\u4e2d\u7684\u4e0d\u8db3\u3002\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u673a\u5668\u4eba\u7cfb\u7edf\u4e0a\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u3001\u5177\u6709\u53ef\u8bc1\u660e\u7684\u6709\u9650\u65f6\u95f4\u5b89\u5168\u4fdd\u8bc1\u7684\u5b9e\u65f6\u63a7\u5236\u6846\u67b6\u3002", "motivation": "\u4f20\u7edf\u9ad8\u9636CBFs\u4ec5\u80fd\u786e\u4fdd\u6e10\u8fd1\u5b89\u5168\uff0c\u4f46\u7f3a\u4e4f\u56fa\u5b9a\u65f6\u95f4\u6536\u655b\u673a\u5236\uff0c\u8fd9\u5bf9\u4e8e\u81ea\u52a8\u5bfc\u822a\u7b49\u65f6\u95f4\u654f\u611f\u548c\u5b89\u5168\u5173\u952e\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u5728\u7279\u5f81\u591a\u9879\u5f0f\u4e2d\u4f7f\u7528\u91cd\u6839\u6765\u65bd\u52a0\u7ed3\u6784\u5316\u5fae\u5206\u7ea6\u675f\uff0c\u63a8\u5bfc\u51fa\u786e\u4fdd\u524d\u5411\u4e0d\u53d8\u6027\u548c\u56fa\u5b9a\u65f6\u95f4\u53ef\u8fbe\u6027\u7684\u5c4f\u969c\u51fd\u6570\u53ca\u5176\u5bfc\u6570\u7684\u6761\u4ef6\uff0c\u5e76\u4e3a\u4e8c\u9636\u7cfb\u7edf\u63d0\u4f9b\u663e\u5f0f\u516c\u5f0f\u3002", "result": "\u5728\u70b9\u8d28\u91cf\u6a21\u578b\u3001\u5355\u8f6e\u8f66\u548c\u81ea\u884c\u8f66\u6a21\u578b\u8fd9\u4e09\u4e2a\u673a\u5668\u4eba\u7cfb\u7edf\u4e0a\u8fdb\u884c\u7684\u8bc4\u4f30\u548c\u57fa\u51c6\u6d4b\u8bd5\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u53ef\u9760\u5730\u5728\u671f\u671b\u7684\u65f6\u95f4\u5185\u5f3a\u5236\u6267\u884c\u6536\u655b\uff0c\u5373\u4f7f\u5728\u4f20\u7edf\u65b9\u6cd5\u5931\u8d25\u7684\u60c5\u51b5\u4e0b\u4e5f\u662f\u5982\u6b64\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u8bbe\u8ba1\u9ad8\u9636\u63a7\u5236\u5c4f\u969c\u51fd\u6570\uff08CBFs\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u53ef\u5728\u7528\u6237\u6307\u5b9a\u7684\u6709\u9650\u65f6\u95f4\u5185\u4fdd\u8bc1\u6536\u655b\u5230\u5b89\u5168\u96c6\u3002\u4e0e\u4ec5\u80fd\u786e\u4fdd\u6e10\u8fd1\u5b89\u5168\u4f46\u7f3a\u4e4f\u56fa\u5b9a\u65f6\u95f4\u6536\u655b\u673a\u5236\u7684\u4f20\u7edf\u9ad8\u9636CBFs\u4e0d\u540c\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u901a\u8fc7\u5728\u7279\u5f81\u591a\u9879\u5f0f\u4e2d\u4f7f\u7528\u91cd\u6839\u6765\u65bd\u52a0\u7ed3\u6784\u5316\u5fae\u5206\u7ea6\u675f\uff0c\u4ece\u800c\u80fd\u591f\u83b7\u5f97\u5177\u6709\u5728\u89c4\u5b9a\u65f6\u95f4\u5185\u7cbe\u786e\u6536\u655b\u7684\u95ed\u5f0f\u591a\u9879\u5f0f\u89e3\u3002\u6211\u4eec\u63a8\u5bfc\u4e86\u786e\u4fdd\u524d\u5411\u4e0d\u53d8\u6027\u548c\u56fa\u5b9a\u65f6\u95f4\u53ef\u8fbe\u6027\u7684\u5c4f\u969c\u51fd\u6570\u53ca\u5176\u5bfc\u6570\u7684\u6761\u4ef6\uff0c\u5e76\u4e3a\u4e8c\u9636\u7cfb\u7edf\u63d0\u4f9b\u4e86\u663e\u5f0f\u516c\u5f0f\u3002\u8be5\u65b9\u6cd5\u5728\u70b9\u8d28\u91cf\u6a21\u578b\u3001\u5355\u8f6e\u8f66\u548c\u81ea\u884c\u8f66\u6a21\u578b\u8fd9\u4e09\u4e2a\u673a\u5668\u4eba\u7cfb\u7edf\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5e76\u4e0e\u73b0\u6709\u7684\u9ad8\u9636CBFs\u65b9\u6cd5\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5373\u4f7f\u5728\u4f20\u7edf\u65b9\u6cd5\u5931\u8d25\u7684\u60c5\u51b5\u4e0b\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4e5f\u80fd\u5728\u671f\u671b\u7684\u65f6\u95f4\u5185\u53ef\u9760\u5730\u5f3a\u5236\u6267\u884c\u6536\u655b\u3002\u8fd9\u9879\u5de5\u4f5c\u4e3a\u5177\u6709\u53ef\u8bc1\u660e\u7684\u5b9e\u65f6\u5b89\u5168\u4fdd\u8bc1\u7684\u5b9e\u65f6\u63a7\u5236\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u884c\u7684\u3001\u9c81\u68d2\u7684\u6846\u67b6\u3002"}}
{"id": "2507.13654", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.13654", "abs": "https://arxiv.org/abs/2507.13654", "authors": ["Haoran Wang", "Yasamin Foroutani", "Matthew Nepo", "Mercedes Rodriguez", "Ji Ma", "Jean-Pierre Hubschman", "Tsu-Chin Tsao", "Jacob Rosen"], "title": "A Study of Teleoperation Methods in a Simulated Virtual Eye Surgery Environment", "comment": "9 pages, 11 figures", "summary": "This paper examines the performance of Inside and Outside Control modes at\nvarious scaling factors in a simulated vitreoretinal surgical setting. The\nIRISS teleoperated surgical system's console (cockpit) was adapted to project a\nsimulated microscope view of an intraocular setup to a virtual reality (VR)\nheadset. Five experienced vitreoretinal surgeons and five engineers with no\nsurgical experience used the system to perform tasks common to vitreoretinal\nsurgery. Experimental results indicate that Inside Control methods at higher\nscaling factors (20 or 30) achieved the best performance overall, though the\noptimal scaling factor may vary by task and complexity. Optimizing control\nmethods and scaling factors could lead to improvements in surgical efficiency\nand accuracy, as well as minimize risks in future robotic-assisted intraocular\nprocedures.", "AI": {"tldr": "\u5728\u6a21\u62df\u73bb\u7483\u4f53\u89c6\u7f51\u819c\u624b\u672f\u4e2d\uff0cInside Control\u65b9\u6cd5\u572820\u500d\u621630\u500d\u7684\u7f29\u653e\u56e0\u5b50\u4e0b\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u6700\u4f73\u9009\u62e9\u53ef\u80fd\u53d6\u51b3\u4e8e\u5177\u4f53\u4efb\u52a1\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u68c0\u9a8c\u5728\u6a21\u62df\u73bb\u7483\u4f53\u89c6\u7f51\u819c\u624b\u672f\u73af\u5883\u4e2d\uff0cInside\u548cOutside\u63a7\u5236\u6a21\u5f0f\u5728\u4e0d\u540c\u7f29\u653e\u56e0\u5b50\u4e0b\u7684\u6027\u80fd\u8868\u73b0\u3002", "method": "\u672c\u7814\u7a76\u4f7f\u7528\u4e86IRISS\u8fdc\u7a0b\u624b\u672f\u7cfb\u7edf\u7684\u63a7\u5236\u53f0\uff08\u9a7e\u9a76\u8231\uff09\uff0c\u5e76\u9002\u914d\u6295\u5f71\u4e86\u4e00\u4e2a\u6a21\u62df\u7684\u663e\u5fae\u955c\u89c6\u56fe\u5230\u4e00\u4e2a\u865a\u62df\u73b0\u5b9e\uff08VR\uff09\u5934\u663e\u4e2d\uff0c\u4ee5\u6a21\u62df\u73bb\u7483\u4f53\u89c6\u7f51\u819c\u624b\u672f\u73af\u5883\u3002\u4e94\u540d\u7ecf\u9a8c\u4e30\u5bcc\u7684\u73bb\u7483\u4f53\u89c6\u7f51\u819c\u5916\u79d1\u533b\u751f\u548c\u4e94\u540d\u6ca1\u6709\u624b\u672f\u7ecf\u9a8c\u7684\u5de5\u7a0b\u5e08\u4f7f\u7528\u8be5\u7cfb\u7edf\u6267\u884c\u4e86\u73bb\u7483\u4f53\u89c6\u7f51\u819c\u624b\u672f\u7684\u5e38\u89c1\u4efb\u52a1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cInside Control\u65b9\u6cd5\u5728\u66f4\u9ad8\u7f29\u653e\u56e0\u5b50\uff0820\u621630\uff09\u4e0b\u5b9e\u73b0\u4e86\u6700\u4f73\u7684\u603b\u4f53\u6027\u80fd\uff0c\u4f46\u6700\u4f73\u7f29\u653e\u56e0\u5b50\u53ef\u80fd\u56e0\u4efb\u52a1\u548c\u590d\u6742\u6027\u800c\u5f02\u3002", "conclusion": "Inside Control\u65b9\u6cd5\u5728\u66f4\u9ad8\u7f29\u653e\u56e0\u5b50\uff0820\u621630\uff09\u4e0b\u5b9e\u73b0\u4e86\u6700\u4f73\u7684\u603b\u4f53\u6027\u80fd\uff0c\u4f46\u6700\u4f73\u7f29\u653e\u56e0\u5b50\u53ef\u80fd\u56e0\u4efb\u52a1\u548c\u590d\u6742\u6027\u800c\u5f02\u3002\u4f18\u5316\u63a7\u5236\u65b9\u6cd5\u548c\u7f29\u653e\u56e0\u5b50\u6709\u671b\u63d0\u9ad8\u624b\u672f\u6548\u7387\u548c\u7cbe\u5ea6\uff0c\u5e76\u964d\u4f4e\u672a\u6765\u673a\u5668\u4eba\u8f85\u52a9\u773c\u5185\u624b\u672f\u7684\u98ce\u9669\u3002"}}
{"id": "2507.14053", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2507.14053", "abs": "https://arxiv.org/abs/2507.14053", "authors": ["Simon Astley", "Jaspa Stritt", "Soumen Mandal", "Jerome A. Cuenca", "D. Andrew Evans", "Oliver A. Williams"], "title": "Real time observation of glass-like carbon formation from SU-8 using X-ray and ultraviolet photoelectron spectroscopy", "comment": null, "summary": "The structural development and change in elemental composition of SU-8 3005\nphotoresist into glass-like carbon due to pyrolysis up to 1000~$\\degree$C is\ninvestigated utilising \\textit{in-situ} x-ray and ultraviolet photoelectron\nspectroscopy (XPS/UPS) under ultra-high vacuum (UHV). XPS spectra were analysed\nin order to investigate changes to elemental composition and physical\nstructure. Peak asymmetry in the measured C 1s spectra is found to be a clear\nindicator of a transition in both physical structure and increased electrical\nconductivity. The \\textit{in-situ} XPS measurement of pyrolysis is effective in\nisolating changes in oxygen composition solely due to the pyrolysis process.\nOxygen concentration, C 1s peak asymmetry and C 1s peak positions are strong\nindicators of semiconducting SU-8 transitioning to conducting glass-like\ncarbon. For SU-8 pyrolysed above temperatures of 500~$\\degree$C, a clear\ndevelopment is observed in the material structure and composition towards a\ncarbon rich conducting network indicative of glass-like carbon. UPS spectra\nwere analysed to investigate the changes in secondary electron cut-off (SECO)\nand valence band maximum (VBM) as the SU-8 layer is heated in UHV. The changes\nin SECO and VBM correlates well with the XPS data and a zero binding energy\nstate is observed at 1000~$\\degree$C.", "AI": {"tldr": "\u901a\u8fc7XPS/UPS\u5206\u6790SU-8\u5149\u523b\u80f6\u70ed\u89e3\u8fc7\u7a0b\uff0c\u53d1\u73b0\u5176\u5728500\u00b0C\u4ee5\u4e0a\u4f1a\u8f6c\u53d8\u4e3a\u7c7b\u73bb\u7483\u78b3\uff0c\u5e76\u4f34\u968f\u7535\u5bfc\u7387\u589e\u52a0\u3002", "motivation": "\u7814\u7a76SU-8\u5149\u523b\u80f6\u5728\u70ed\u89e3\u8fc7\u7a0b\u4e2d\u5411\u7c7b\u73bb\u7483\u78b3\u8f6c\u53d8\u7684\u7ed3\u6784\u548c\u5143\u7d20\u7ec4\u6210\u53d8\u5316\uff0c\u7279\u522b\u662f\u7535\u5bfc\u7387\u7684\u53d8\u5316\uff0c\u4ee5\u7406\u89e3\u8be5\u8fc7\u7a0b\u7684\u673a\u7406\u3002", "method": "\u4f7f\u7528X\u5c04\u7ebf\u548c\u7d2b\u5916\u5149\u7535\u5b50\u80fd\u8c31\uff08XPS/UPS\uff09\u5728\u8d85\u9ad8\u771f\u7a7a\uff08UHV\uff09\u6761\u4ef6\u4e0b\uff0c\u7814\u7a76SU-8 3005\u5149\u523b\u80f6\u5728\u9ad8\u8fbe1000\u00b0C\u70ed\u89e3\u8fc7\u7a0b\u4e2d\u7ed3\u6784\u548c\u5143\u7d20\u7ec4\u6210\u5411\u7c7b\u73bb\u7483\u78b3\u7684\u8f6c\u53d8\u3002\u901a\u8fc7\u5206\u6790C 1s\u5149\u8c31\u7684\u5cf0\u4e0d\u5bf9\u79f0\u6027\u3001\u6c27\u542b\u91cf\u4ee5\u53caUPS\u6d4b\u91cf\u7684\u4e8c\u6b21\u7535\u5b50\u5207\u65ad\uff08SECO\uff09\u548c\u4ef7\u5e26\u6700\u5927\u503c\uff08VBM\uff09\u6765\u8ffd\u8e2a\u8fd9\u4e9b\u53d8\u5316\u3002", "result": "SU-8\u5149\u523b\u80f6\u7ecf\u8fc7500\u00b0C\u4ee5\u4e0a\u70ed\u89e3\u540e\uff0c\u5176\u6750\u6599\u7ed3\u6784\u548c\u7ec4\u6210\u5411\u5bcc\u78b3\u5bfc\u7535\u7f51\u7edc\u8f6c\u53d8\uff0c\u5f62\u6210\u4e86\u7c7b\u4f3c\u73bb\u7483\u7684\u78b3\u3002C 1s\u5149\u8c31\u7684\u5cf0\u4e0d\u5bf9\u79f0\u6027\u3001\u6c27\u542b\u91cf\u548c\u5cf0\u4f4d\u7f6e\u7684\u53d8\u5316\uff0c\u4ee5\u53caUPS\u6d4b\u91cf\u7684SECO\u548cVBM\u7684\u53d8\u5316\uff0c\u90fd\u4e0eXPS\u6570\u636e\u76f8\u5173\uff0c\u5e76\u57281000\u00b0C\u65f6\u89c2\u5bdf\u5230\u96f6\u7ed3\u5408\u80fd\u6001\u3002", "conclusion": "SU-8\u5728\u9ad8\u8fbe1000\u00b0C\u7684\u70ed\u89e3\u8fc7\u7a0b\u4e2d\uff0c\u5176\u7ed3\u6784\u548c\u5143\u7d20\u7ec4\u6210\u4f1a\u8f6c\u53d8\u4e3a\u7c7b\u4f3c\u73bb\u7483\u7684\u78b3\uff0c\u5e76\u4f34\u968f\u7740\u7535\u5bfc\u7387\u7684\u589e\u52a0\uff0c\u8fd9\u53ef\u4ee5\u901a\u8fc7XPS/UPS\u5149\u8c31\u7684\u53d8\u5316\u5f97\u5230\u8bc1\u5b9e\u3002"}}
{"id": "2507.13838", "categories": ["cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2507.13838", "abs": "https://arxiv.org/abs/2507.13838", "authors": ["Kyle A. Portwin", "Pablo Galaviz", "Xiaoning Li", "Chongyan Hao", "Lachlan A. Smillie", "Mengyun You", "Caleb Stamper", "Richard Mole", "Dehong Yu", "Kirrily C. Rule", "David L. Cortie", "Zhenxiang Cheng"], "title": "Tuning the Surface States of $Fe_3O_4$ Nanoparticles for Enhanced Magnetic Anisotropy and Induction Efficacy", "comment": null, "summary": "Magnetite ($Fe_3O_4$) nanoparticles are crucial for biomedical applications,\nincluding magnetic hyperthermia, targeted drug delivery, and MRI contrast\nenhancement, due to their biocompatibility and unique physicochemical\nproperties. Here, we investigate how surface states influence their induction\nperformance. Heat treatment removes surface water and FeOOH, forming a\n${\\gamma}$-$Fe_2O_3$ shell, as confirmed by synchrotron powder diffraction,\nneutron powder diffraction, thermogravimetric analysis, X-ray photoelectron\nspectroscopy, X-ray absorption spectroscopy, and time-of-flight inelastic\nneutron spectroscopy. AC magnetic susceptibility measurements reveal that this\nsurface modification enhances magnetic anisotropy and reduces the spin\nrelaxation time, leading to a 140% increase in the specific absorption rate.\nAdditionally, the increased anisotropy suppresses the low-temperature clustered\nspin-glass transition and raises the blocking temperature. These findings\nhighlight surface-state engineering as a powerful approach to optimizing\n$Fe_3O_4$ nanoparticles for biomedical applications.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u70ed\u5904\u7406\u4f18\u5316Fe3O4\u7eb3\u7c73\u9897\u7c92\u7684\u8868\u9762\u72b6\u6001\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u5176\u5728\u751f\u7269\u533b\u5b66\u5e94\u7528\u4e2d\u7684\u6027\u80fd\uff0c\u4e3b\u8981\u4f53\u73b0\u5728\u63d0\u9ad8\u78c1\u5404\u5411\u5f02\u6027\u548c\u6bd4\u5438\u6536\u7387\uff0c\u5e76\u6291\u5236\u4f4e\u6e29\u56e2\u7c07\u81ea\u65cb\u73bb\u7483\u8f6c\u53d8\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8Fe3O4\u7eb3\u7c73\u9897\u7c92\u5728\u78c1\u6027\u70ed\u7597\u3001\u9776\u5411\u836f\u7269\u8f93\u9001\u548cMRI\u9020\u5f71\u5242\u589e\u5f3a\u7b49\u751f\u7269\u533b\u5b66\u5e94\u7528\u4e2d\u7684\u6027\u80fd\uff0c\u9700\u8981\u7814\u7a76\u5176\u8868\u9762\u72b6\u6001\u5bf9\u5176\u611f\u5e94\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u540c\u6b65\u7c89\u672b\u884d\u5c04\u3001\u4e2d\u5b50\u7c89\u672b\u884d\u5c04\u3001\u70ed\u91cd\u5206\u6790\u3001X\u5c04\u7ebf\u5149\u7535\u5b50\u80fd\u8c31\u3001X\u5c04\u7ebf\u5438\u6536\u80fd\u8c31\u548c\u98de\u884c\u65f6\u95f4\u975e\u5f39\u6027\u4e2d\u5b50\u6563\u5c04\u7b49\u6280\u672f\uff0c\u7814\u7a76\u4e86\u70ed\u5904\u7406\u5bf9Fe3O4\u7eb3\u7c73\u9897\u7c92\u8868\u9762\u72b6\u6001\u7684\u5f71\u54cd\uff0c\u5e76\u901a\u8fc7\u4ea4\u6d41\u78c1\u5316\u7387\u6d4b\u91cf\u8bc4\u4f30\u4e86\u8868\u9762\u6539\u6027\u5bf9\u5176\u78c1\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u70ed\u5904\u7406\u53bb\u9664\u4e86\u8868\u9762\u6c34\u548cFeOOH\uff0c\u5f62\u6210\u4e86\u03b3-Fe2O3\u58f3\u5c42\uff0c\u589e\u5f3a\u4e86\u78c1\u5404\u5411\u5f02\u6027\uff0c\u7f29\u77ed\u4e86\u81ea\u65cb\u5f1b\u8c6b\u65f6\u95f4\uff0c\u4f7f\u6bd4\u5438\u6536\u7387\u63d0\u9ad8\u4e86140%\u3002\u6b64\u5916\uff0c\u589e\u52a0\u7684\u5404\u5411\u5f02\u6027\u6291\u5236\u4e86\u4f4e\u6e29\u56e2\u7c07\u81ea\u65cb\u73bb\u7483\u8f6c\u53d8\uff0c\u5e76\u63d0\u9ad8\u4e86\u963b\u6321\u6e29\u5ea6\u3002", "conclusion": "\u8868\u9762\u72b6\u6001\u5de5\u7a0b\u662f\u901a\u8fc7\u4f18\u5316\u751f\u7269\u533b\u5b66\u5e94\u7528\u4e2d\u7684Fe3O4\u7eb3\u7c73\u9897\u7c92\u6765\u63d0\u9ad8\u5176\u6027\u80fd\u7684\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2507.13969", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.13969", "abs": "https://arxiv.org/abs/2507.13969", "authors": ["Maria Eduarda Silva de Macedo", "Ana Paula Chiarelli de Souza", "Roberto Silvio Ubertino Rosso Jr.", "Yuri Kaszubowski Lopes"], "title": "A Minimalist Controller for Autonomously Self-Aggregating Robotic Swarms: Enabling Compact Formations in Multitasking Scenarios", "comment": "7 pages total (6 pages of content + 1 page of references). Short\n  paper manuscript submitted to TAROS 2025", "summary": "The deployment of simple emergent behaviors in swarm robotics has been\nwell-rehearsed in the literature. A recent study has shown how self-aggregation\nis possible in a multitask approach -- where multiple self-aggregation task\ninstances occur concurrently in the same environment. The multitask approach\nposes new challenges, in special, how the dynamic of each group impacts the\nperformance of others. So far, the multitask self-aggregation of groups of\nrobots suffers from generating a circular formation -- that is not fully\ncompact -- or is not fully autonomous. In this paper, we present a multitask\nself-aggregation where groups of homogeneous robots sort themselves into\ndifferent compact clusters, relying solely on a line-of-sight sensor. Our\nmultitask self-aggregation behavior was able to scale well and achieve a\ncompact formation. We report scalability results from a series of simulation\ntrials with different configurations in the number of groups and the number of\nrobots per group. We were able to improve the multitask self-aggregation\nbehavior performance in terms of the compactness of the clusters, keeping the\nproportion of clustered robots found in other studies.", "AI": {"tldr": "\u4e00\u79cd\u65b0\u7684\u591a\u4efb\u52a1\u81ea\u805a\u96c6\u65b9\u6cd5\uff0c\u4f7f\u7528\u89c6\u7ebf\u4f20\u611f\u5668\uff0c\u8ba9\u673a\u5668\u4eba\u5f62\u6210\u7d27\u51d1\u7684\u805a\u7c7b\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u7814\u7a76\u4e2d\u591a\u4efb\u52a1\u81ea\u805a\u96c6\u65b9\u6cd5\u5728\u5f62\u6210\u5706\u5f62\u6216\u975e\u5b8c\u5168\u81ea\u4e3b\u65b9\u9762\u5b58\u5728\u7684\u6311\u6218\uff0c\u5e76\u63d0\u9ad8\u96c6\u7fa4\u52a8\u6001\u5bf9\u5176\u4ed6\u7fa4\u4f53\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u4efb\u52a1\u81ea\u805a\u96c6\u65b9\u6cd5\uff0c\u5176\u4e2d\u5747\u8d28\u673a\u5668\u4eba\u7fa4\u4f53\u4ec5\u4f9d\u9760\u89c6\u7ebf\u4f20\u611f\u5668\u8fdb\u884c\u81ea\u6211\u7ec4\u7ec7\uff0c\u5f62\u6210\u4e0d\u540c\u7684\u7d27\u51d1\u805a\u7c7b\u3002", "result": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u591a\u4efb\u52a1\u81ea\u805a\u96c6\u884c\u4e3a\u7684\u826f\u597d\u6269\u5c55\u6027\uff0c\u5f62\u6210\u4e86\u7d27\u51d1\u7684\u805a\u7c7b\uff0c\u5e76\u4e14\u5728\u805a\u7c7b\u7d27\u5bc6\u5ea6\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u7814\u7a76\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u76f8\u5f53\u7684\u805a\u96c6\u673a\u5668\u4eba\u6bd4\u4f8b\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u591a\u4efb\u52a1\u81ea\u805a\u96c6\u65b9\u6cd5\u80fd\u591f\u5b9e\u73b0\u5747\u8d28\u673a\u5668\u4eba\u5f62\u6210\u4e0d\u540c\u7684\u7d27\u51d1\u805a\u7c7b\uff0c\u5e76\u4e14\u4ec5\u4f9d\u8d56\u4e8e\u89c6\u7ebf\u4f20\u611f\u5668\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e0e\u5176\u4ed6\u7814\u7a76\u76f8\u5f53\u7684\u805a\u96c6\u673a\u5668\u4eba\u6bd4\u4f8b\u3002"}}
{"id": "2507.14060", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2507.14060", "abs": "https://arxiv.org/abs/2507.14060", "authors": ["Sanjeev Khanna", "Ashwin Padaki", "Erik Waingarten"], "title": "Sparse Navigable Graphs for Nearest Neighbor Search: Algorithms and Hardness", "comment": null, "summary": "We initiate the study of approximation algorithms and computational barriers\nfor constructing sparse $\\alpha$-navigable graphs [IX23, DGM+24], a core\nprimitive underlying recent advances in graph-based nearest neighbor search.\nGiven an $n$-point dataset $P$ with an associated metric $\\mathsf{d}$ and a\nparameter $\\alpha \\geq 1$, the goal is to efficiently build the sparsest graph\n$G=(P, E)$ that is $\\alpha$-navigable: for every distinct $s, t \\in P$, there\nexists an edge $(s, u) \\in E$ with $\\mathsf{d}(u, t) < \\mathsf{d}(s,\nt)/\\alpha$. We consider two natural sparsity objectives: minimizing the maximum\nout-degree and minimizing the total size.\n  We first show a strong negative result: the slow-preprocessing version of\nDiskANN (analyzed in [IX23] for low-doubling metrics) can yield solutions whose\nsparsity is $\\widetilde{\\Omega}(n)$ times larger than optimal, even on\nEuclidean instances. We then show a tight approximation-preserving equivalence\nbetween the Sparsest Navigable Graph problem and the classic Set Cover problem,\nobtaining an $O(n^3)$-time $(\\ln n + 1)$-approximation algorithm, as well as\nestablishing NP-hardness of achieving an $o(\\ln n)$-approximation. Building on\nthis equivalence, we develop faster $O(\\ln n)$-approximation algorithms. The\nfirst runs in $\\widetilde{O}(n \\cdot \\mathrm{OPT})$ time and is thus much\nfaster when the optimal solution is sparse. The second, based on fast matrix\nmultiplication, is a bicriteria algorithm that computes an $O(\\ln\nn)$-approximation to the sparsest $2\\alpha$-navigable graph, running in\n$\\widetilde{O}(n^{\\omega})$ time.\n  Finally, we complement our upper bounds with a query complexity lower bound,\nshowing that any $o(n)$-approximation requires examining $\\Omega(n^2)$\ndistances. This result shows that in the regime where $\\mathrm{OPT} =\n\\widetilde{O}(n)$, our $\\widetilde{O}(n \\cdot \\mathrm{OPT})$-time algorithm is\nessentially best possible.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u6784\u5efa\u7a00\u758f\u53ef\u5bfc\u822a\u56fe\u7684\u8fd1\u4f3c\u7b97\u6cd5\u548c\u8ba1\u7b97\u4e0b\u754c\uff0c\u5c06\u5176\u4e0e\u96c6\u5408\u8986\u76d6\u95ee\u9898\u8054\u7cfb\u8d77\u6765\uff0c\u63d0\u51fa\u4e86\u591a\u79cd\u8fd1\u4f3c\u7b97\u6cd5\uff0c\u5e76\u8bc1\u660e\u4e86\u67d0\u4e9b\u7b97\u6cd5\u7684\u8fd1\u4f3c\u6bd4\u548c\u65f6\u95f4\u590d\u6742\u5ea6\u5df2\u63a5\u8fd1\u6700\u4f18\u3002", "motivation": "\u6700\u8fd1\u5728\u56fe\u57fa\u7840\u7684\u6700\u8fd1\u90bb\u641c\u7d22\uff08nearest neighbor search\uff09\u9886\u57df\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u800c\u7a00\u758f\u53ef\u5bfc\u822a\u56fe\u662f\u8fd9\u4e9b\u8fdb\u5c55\u7684\u6838\u5fc3\u57fa\u7840\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u7a00\u758f\u53ef\u5bfc\u822a\u56fe\u7684\u6784\u5efa\u65b9\u6cd5\uff0c\u5305\u62ec\u5176\u8fd1\u4f3c\u7b97\u6cd5\u548c\u8ba1\u7b97\u969c\u788d\uff0c\u5177\u6709\u91cd\u8981\u7684\u7406\u8bba\u548c\u5b9e\u8df5\u610f\u4e49\u3002\u7814\u7a76\u76ee\u6807\u662f\u627e\u5230\u5c3d\u53ef\u80fd\u7a00\u758f\uff08\u4f18\u5316\u6700\u5927\u51fa\u5ea6\u548c\u603b\u8fb9\u6570\uff09\u4e14\u6ee1\u8db3\u53ef\u5bfc\u822a\u6027\u6761\u4ef6\u7684\u56fe\u3002", "method": "\u672c\u6587\u91c7\u7528\u4e86\u8fd1\u4f3c\u7b97\u6cd5\u548c\u8ba1\u7b97\u590d\u6742\u6027\u7406\u8bba\u7684\u65b9\u6cd5\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u7814\u7a76\u4e86\u7a00\u758f\u53ef\u5bfc\u822a\u56fe\u7684\u6784\u5efa\u95ee\u9898\uff0c\u5e76\u5c06\u5176\u4e0e\u96c6\u5408\u8986\u76d6\u95ee\u9898\u8054\u7cfb\u8d77\u6765\uff0c\u5229\u7528\u96c6\u5408\u8986\u76d6\u7684\u5df2\u77e5\u8fd1\u4f3c\u7b97\u6cd5\u548c\u7406\u8bba\u6765\u6307\u5bfc\u7a00\u758f\u53ef\u5bfc\u822a\u56fe\u7b97\u6cd5\u7684\u8bbe\u8ba1\u3002\u540c\u65f6\uff0c\u5229\u7528\u77e9\u9635\u4e58\u6cd5\u7b49\u6280\u672f\u6765\u52a0\u901f\u7b97\u6cd5\u3002\u6700\u540e\uff0c\u901a\u8fc7\u5206\u6790\u67e5\u8be2\u8fc7\u7a0b\u6765\u5efa\u7acb\u4e0b\u754c\u3002", "result": "1. DiskANN \u7684\u9884\u5904\u7406\u7248\u672c\u5728\u7a00\u758f\u6027\u4e0a\u5b58\u5728 $\\widetilde{\\Omega}(n)$ \u7684\u8fd1\u4f3c\u6bd4\u30022. \u7a00\u758f\u53ef\u5bfc\u822a\u56fe\u95ee\u9898\u4e0e\u96c6\u5408\u8986\u76d6\u95ee\u9898\u7b49\u4ef7\uff0c\u5b58\u5728 $O(n^3)$ \u65f6\u95f4\u7684 $(\\ln n + 1)$-\u8fd1\u4f3c\u7b97\u6cd5\uff0c\u4e14 $o(\\ln n)$-\u8fd1\u4f3c\u662f NP-hard \u7684\u30023. \u63d0\u51fa\u4e86 $\\widetilde{O}(n \\cdot \\mathrm{OPT})$ \u65f6\u95f4\u7684 $O(\\ln n)$-\u8fd1\u4f3c\u7b97\u6cd5\u548c $\\widetilde{O}(n^{\\omega})$ \u65f6\u95f4\u7684\u53cc\u6807\u51c6\u7b97\u6cd5\u30024. \u5b58\u5728\u67e5\u8be2\u590d\u6742\u5ea6\u4e0b\u754c $\\Omega(n^2)$\uff0c\u8868\u660e\u5bf9\u4e8e $\\mathrm{OPT} = \\widetilde{O}(n)$ \u7684\u60c5\u51b5\uff0c$\\widetilde{O}(n \\cdot \\mathrm{OPT})$ \u65f6\u95f4\u7b97\u6cd5\u63a5\u8fd1\u6700\u4f18\u3002", "conclusion": "\u672c\u6587\u7814\u7a76\u4e86\u6784\u5efa\u7a00\u758f \u03b1-\u53ef\u5bfc\u822a\u56fe\uff08sparse \u03b1-navigable graphs\uff09\u7684\u8fd1\u4f3c\u7b97\u6cd5\u548c\u8ba1\u7b97\u969c\u788d\u3002\u6587\u7ae0\u7684\u4e3b\u8981\u8d21\u732e\u5305\u62ec\uff1a1. \u8bc1\u660e\u4e86 DiskANN \u7684\u9884\u5904\u7406\u7248\u672c\u5728\u7a00\u758f\u6027\u65b9\u9762\u5b58\u5728\u663e\u8457\u7684\u8fd1\u4f3c\u6bd4\uff08\u53ef\u80fd\u8fbe\u5230\u6700\u4f18\u89e3\u7684 $\\widetilde{\\Omega}(n)$ \u500d\uff09\u30022. \u5efa\u7acb\u4e86\u7a00\u758f\u53ef\u5bfc\u822a\u56fe\u95ee\u9898\u4e0e\u7ecf\u5178\u96c6\u5408\u8986\u76d6\u95ee\u9898\u4e4b\u95f4\u7684\u8fd1\u4f3c\u4fdd\u6301\u7b49\u4ef7\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a $O(n^3)$ \u65f6\u95f4\u7684 $(\\ln n + 1)$-\u8fd1\u4f3c\u7b97\u6cd5\uff0c\u5e76\u8bc1\u660e\u4e86\u83b7\u5f97 $o(\\ln n)$-\u8fd1\u4f3c\u662f NP-hard \u7684\u30023. \u57fa\u4e8e\u6b64\u7b49\u4ef7\u6027\uff0c\u5f00\u53d1\u4e86\u66f4\u5feb\u7684 $O(\\ln n)$-\u8fd1\u4f3c\u7b97\u6cd5\uff0c\u5176\u4e2d\u4e00\u4e2a\u7b97\u6cd5\u80fd\u5728 $\\widetilde{O}(n \\cdot \\mathrm{OPT})$ \u65f6\u95f4\u5185\u5b8c\u6210\uff0c\u5f53\u6700\u4f18\u89e3\u7a00\u758f\u65f6\u6548\u7387\u5f88\u9ad8\u3002\u53e6\u4e00\u4e2a\u7b97\u6cd5\u5229\u7528\u4e86\u5feb\u901f\u77e9\u9635\u4e58\u6cd5\uff0c\u662f\u4e00\u4e2a\u53cc\u6807\u51c6\u7b97\u6cd5\uff0c\u80fd\u5728 $\\widetilde{O}(n^{\\omega})$ \u65f6\u95f4\u5185\u8ba1\u7b97\u51fa $O(\\ln n)$-\u8fd1\u4f3c\u7684 $2\\alpha$-\u53ef\u5bfc\u822a\u56fe\u30024. \u63d0\u51fa\u4e86\u4e00\u4e2a\u67e5\u8be2\u590d\u6742\u5ea6\u4e0b\u754c\uff0c\u8868\u660e\u4efb\u4f55 $o(n)$-\u8fd1\u4f3c\u7b97\u6cd5\u90fd\u9700\u8981\u68c0\u67e5 $\\Omega(n^2)$ \u7684\u8ddd\u79bb\uff0c\u8fd9\u8868\u660e\u5728 $\\mathrm{OPT} = \\widetilde{O}(n)$ \u7684\u60c5\u51b5\u4e0b\uff0c$\\widetilde{O}(n \\cdot \\mathrm{OPT})$ \u65f6\u95f4\u7684\u7b97\u6cd5\u5df2\u63a5\u8fd1\u6700\u4f18\u3002"}}
{"id": "2507.13410", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13410", "abs": "https://arxiv.org/abs/2507.13410", "authors": ["Cheng-Ting Chou", "George Liu", "Jessica Sun", "Cole Blondin", "Kevin Zhu", "Vasu Sharma", "Sean O'Brien"], "title": "Causal Language Control in Multilingual Transformers via Sparse Feature Steering", "comment": null, "summary": "Deterministically controlling the target generation language of large\nmultilingual language models (LLMs) remains a fundamental challenge,\nparticularly in zero-shot settings where neither explicit language prompts nor\nfine-tuning are available. In this work, we investigate whether sparse\nautoencoder (SAE) features, previously shown to correlate with interpretable\nmodel behaviors, can be leveraged to steer the generated language of LLMs\nduring inference. Leveraging pretrained SAEs on the residual streams of\nGemma-2B and Gemma-9B, we identify features whose activations differ most\nsignificantly between English and four target languages: Chinese, Japanese,\nSpanish, and French. By modifying just a single SAE feature at one transformer\nlayer, we achieve controlled language shifts with up to 90\\% success, as\nmeasured by FastText language classification, while preserving semantic\nfidelity according to LaBSE (Language-Agnostic BERT Sentence Embedding)\nsimilarity. Our analysis reveals that language steering is most effective in\nmid-to-late transformer layers and is amplified by specific attention heads\ndisproportionately associated with language-sensitive SAE features. These\nresults demonstrate the promise of sparse feature steering as a lightweight and\ninterpretable mechanism for controllable multilingual generation.", "AI": {"tldr": "\u901a\u8fc7\u4fee\u6539\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAE\uff09\u7279\u5f81\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u63a7\u5236\u5927\u578b\u591a\u8bed\u8a00\u8bed\u8a00\u6a21\u578b\u7684\u751f\u6210\u8bed\u8a00\uff0c\u5373\u4f7f\u5728\u6ca1\u6709\u660e\u786e\u8bed\u8a00\u63d0\u793a\u6216\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u5b9e\u73b0\u9ad8\u6210\u529f\u7387\u548c\u8bed\u4e49\u4fdd\u771f\u5ea6\u3002", "motivation": "\u65e8\u5728\u89e3\u51b3\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\uff0c\u786e\u5b9a\u6027\u5730\u63a7\u5236\u5927\u578b\u591a\u8bed\u8a00\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u76ee\u6807\u751f\u6210\u8bed\u8a00\u8fd9\u4e00\u6839\u672c\u6027\u6311\u6218\u3002", "method": "\u5229\u7528\u9884\u8bad\u7ec3\u7684\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAE\uff09\u7279\u5f81\uff0c\u901a\u8fc7\u4fee\u6539\u5355\u4e2aSAE\u7279\u5f81\u6765\u63a7\u5236\u5927\u578b\u591a\u8bed\u8a00\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u751f\u6210\u7684\u8bed\u8a00\u3002", "result": "\u901a\u8fc7\u4fee\u6539\u5355\u4e2aSAE\u7279\u5f81\uff0c\u53ef\u4ee5\u5b9e\u73b0\u9ad8\u8fbe90%\u7684\u8bed\u8a00\u63a7\u5236\u6210\u529f\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u8bed\u4e49\u4fdd\u771f\u5ea6\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u8bed\u8a00\u5f15\u5bfc\u5728Transformer\u7684\u4e2d\u540e\u671f\u5c42\u6700\u6709\u6548\uff0c\u5e76\u4e14\u4f1a\u88ab\u4e0e\u8bed\u8a00\u654f\u611fSAE\u7279\u5f81\u76f8\u5173\u7684\u7279\u5b9a\u6ce8\u610f\u529b\u5934\u6240\u653e\u5927\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u7a00\u758f\u7279\u5f81\u5f15\u5bfc\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u4e14\u53ef\u89e3\u91ca\u7684\u63a7\u5236\u591a\u8bed\u8a00\u751f\u6210\u7684\u65b9\u6cd5\u3002"}}
{"id": "2507.13587", "categories": ["quant-ph", "cond-mat.dis-nn"], "pdf": "https://arxiv.org/pdf/2507.13587", "abs": "https://arxiv.org/abs/2507.13587", "authors": ["Ruiyang Zhou", "Saubhik Sarkar", "Sougato Bose", "Abolfazl Bayat"], "title": "Enhanced image classification via hybridizing quantum dynamics with classical neural networks", "comment": "9+2 pages, 7+2 figures", "summary": "The integration of quantum computing and machine learning has emerged as a\npromising frontier in computational science. We present a hybrid protocol which\ncombines classical neural networks with non-equilibrium dynamics of a quantum\nmany-body system for image classification. This architecture leverages\nclassical neural networks to efficiently process high-dimensional data and\nencode it effectively on a quantum many-body system, overcoming a challenging\ntask towards scaled up quantum computation. The quantum module further\ncapitalizes on the discriminative properties of many-body quantum dynamics to\nenhance classification accuracy. By mapping images from distinct classes to\nnearly-orthogonal quantum states, the system maximizes separability in the\nHilbert space, enabling robust classification. We evaluate the performance of\nour model on several benchmark datasets with various number of features and\nclasses. Moreover, we demonstrate the key role of the quantum module in\nachieving high classification accuracy which cannot be accomplished by the\nclassical neural network alone. This showcases the potential of our hybrid\nprotocol for achieving practical quantum advantage and paves the way for future\nadvancements in quantum-enhanced computational techniques.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u7ecf\u5178\u795e\u7ecf\u7f51\u7edc\u4e0e\u91cf\u5b50\u591a\u4f53\u7cfb\u7edf\u76f8\u7ed3\u5408\u7684\u6df7\u5408\u6a21\u578b\uff0c\u7528\u4e8e\u56fe\u50cf\u5206\u7c7b\u3002\u8be5\u6a21\u578b\u5229\u7528\u91cf\u5b50\u52a8\u529b\u5b66\u6765\u63d0\u9ad8\u5206\u7c7b\u7cbe\u5ea6\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u5b9e\u73b0\u5b9e\u9645\u91cf\u5b50\u4f18\u52bf\u65b9\u9762\u7684\u6f5c\u529b\u3002", "motivation": "\u91cf\u5b50\u8ba1\u7b97\u548c\u673a\u5668\u5b66\u4e60\u7684\u878d\u5408\u662f\u8ba1\u7b97\u79d1\u5b66\u4e2d\u6709\u524d\u666f\u7684\u65b0\u5174\u9886\u57df\u3002\u8be5\u7814\u7a76\u65e8\u5728\u514b\u670d\u6269\u5c55\u91cf\u5b50\u8ba1\u7b97\u7684\u6311\u6218\uff0c\u5e76\u5c55\u793a\u91cf\u5b50\u6a21\u5757\u5728\u63d0\u9ad8\u5206\u7c7b\u7cbe\u5ea6\u65b9\u9762\u7684\u5173\u952e\u4f5c\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7ecf\u5178\u795e\u7ecf\u7f51\u7edc\u4e0e\u91cf\u5b50\u591a\u4f53\u7cfb\u7edf\u975e\u5e73\u8861\u52a8\u529b\u5b66\u7684\u6df7\u5408\u534f\u8bae\uff0c\u7528\u4e8e\u56fe\u50cf\u5206\u7c7b\u3002\u8be5\u534f\u8bae\u5229\u7528\u7ecf\u5178\u795e\u7ecf\u7f51\u7edc\u5904\u7406\u9ad8\u7ef4\u6570\u636e\u5e76\u5c06\u5176\u6709\u6548\u7f16\u7801\u5230\u91cf\u5b50\u591a\u4f53\u7cfb\u7edf\uff0c\u5229\u7528\u91cf\u5b50\u591a\u4f53\u52a8\u529b\u5b66\u7684\u5224\u522b\u7279\u6027\u589e\u5f3a\u5206\u7c7b\u7cbe\u5ea6\uff0c\u901a\u8fc7\u5c06\u56fe\u50cf\u6620\u5c04\u5230\u8fd1\u4e4e\u6b63\u4ea4\u7684\u91cf\u5b50\u6001\u6765\u6700\u5927\u5316\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u4e2d\u7684\u53ef\u5206\u6027\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e86\u6a21\u578b\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u91cf\u5b50\u6a21\u5757\u5728\u5355\u72ec\u4f9d\u9760\u7ecf\u5178\u795e\u7ecf\u7f51\u7edc\u65e0\u6cd5\u8fbe\u5230\u7684\u9ad8\u5206\u7c7b\u7cbe\u5ea6\u65b9\u9762\u53d1\u6325\u7684\u5173\u952e\u4f5c\u7528\u3002", "conclusion": "\u8be5\u6df7\u5408\u534f\u8bae\u5c55\u793a\u4e86\u5728\u5b9e\u73b0\u5b9e\u9645\u91cf\u5b50\u4f18\u52bf\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u5e76\u4e3a\u672a\u6765\u91cf\u5b50\u589e\u5f3a\u8ba1\u7b97\u6280\u672f\u7684\u53d1\u5c55\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2507.13417", "categories": ["cs.LG", "cs.AI", "cs.DM"], "pdf": "https://arxiv.org/pdf/2507.13417", "abs": "https://arxiv.org/abs/2507.13417", "authors": ["Armel Soubeiga", "Thomas Guyet", "Violaine Antoine"], "title": "Soft-ECM: An extension of Evidential C-Means for complex data", "comment": null, "summary": "Clustering based on belief functions has been gaining increasing attention in\nthe machine learning community due to its ability to effectively represent\nuncertainty and/or imprecision. However, none of the existing algorithms can be\napplied to complex data, such as mixed data (numerical and categorical) or\nnon-tabular data like time series. Indeed, these types of data are, in general,\nnot represented in a Euclidean space and the aforementioned algorithms make use\nof the properties of such spaces, in particular for the construction of\nbarycenters. In this paper, we reformulate the Evidential C-Means (ECM) problem\nfor clustering complex data. We propose a new algorithm, Soft-ECM, which\nconsistently positions the centroids of imprecise clusters requiring only a\nsemi-metric. Our experiments show that Soft-ECM present results comparable to\nconventional fuzzy clustering approaches on numerical data, and we demonstrate\nits ability to handle mixed data and its benefits when combining fuzzy\nclustering with semi-metrics such as DTW for time series data.", "AI": {"tldr": "\u4e00\u79cd\u540d\u4e3aSoft-ECM\u7684\u65b0\u7b97\u6cd5\u88ab\u63d0\u51fa\uff0c\u7528\u4e8e\u805a\u7c7b\u590d\u6742\u6570\u636e\uff0c\u5982\u6df7\u5408\u6570\u636e\u548c\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u7b97\u6cd5\u5728\u975e\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u8bc1\u636e\u51fd\u6570\u7684\u805a\u7c7b\u7b97\u6cd5\u96be\u4ee5\u5904\u7406\u6570\u503c\u548c\u7c7b\u522b\u6df7\u5408\u6570\u636e\u6216\u65f6\u95f4\u5e8f\u5217\u7b49\u590d\u6742\u6570\u636e\uff0c\u56e0\u4e3a\u5b83\u4eec\u4f9d\u8d56\u4e8e\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\u7684\u6027\u8d28\uff0c\u7279\u522b\u662f\u8d28\u5fc3\u7684\u6784\u9020\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSoft-ECM\u7684\u65b0\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u5c06ECM\u95ee\u9898\u91cd\u65b0\u5b9a\u4e49\uff0c\u4ee5\u5904\u7406\u590d\u6742\u7684\u3001\u975e\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\u4e2d\u7684\u6570\u636e\uff0c\u4ec5\u9700\u534a\u5ea6\u91cf\u5373\u53ef\u5b9e\u73b0\u8d28\u5fc3\u7684\u7cbe\u786e\u5bf9\u9f50\u3002", "result": "Soft-ECM\u7b97\u6cd5\u5728\u6570\u503c\u6570\u636e\u4e0a\u7684\u8868\u73b0\u4e0e\u4f20\u7edf\u7684\u6a21\u7cca\u805a\u7c7b\u65b9\u6cd5\u76f8\u5f53\uff0c\u5e76\u80fd\u6709\u6548\u5904\u7406\u6df7\u5408\u6570\u636e\u3002", "conclusion": "Soft-ECM\u7b97\u6cd5\u80fd\u591f\u5904\u7406\u6570\u503c\u578b\u3001\u6df7\u5408\u578b\u4ee5\u53ca\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0c\u5e76\u4e14\u5728\u5904\u7406\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u65f6\uff0c\u7ed3\u5408\u6a21\u7cca\u805a\u7c7b\u548cDTW\u7b49\u534a\u5ea6\u91cf\u5177\u6709\u4f18\u52bf\u3002"}}
{"id": "2507.13636", "categories": ["cs.SI"], "pdf": "https://arxiv.org/pdf/2507.13636", "abs": "https://arxiv.org/abs/2507.13636", "authors": ["Ashfaq Ali Shafin", "Bogdan Carbunar"], "title": "Duplicating Deceit: Inauthentic Behavior Among Indian Misinformation Duplicators on X/Twitter", "comment": "8 pages, 1 figure, accepted in 17th International Conference on\n  Advances in Social Networks Analysis and Mining (ASONAM 2025)", "summary": "This paper investigates inauthentic duplication on social media, where\nmultiple accounts share identical misinformation tweets. Leveraging a dataset\nof misinformation verified by AltNews, an Indian fact-checking organization, we\nanalyze over 12 million posts from 5,493 accounts known to have duplicated such\ncontent. Contrary to common assumptions that bots are primarily responsible for\nspreading false information, fewer than 1\\% of these accounts exhibit bot-like\nbehavior. We present TweeXster, a framework for detecting and analyzing\nduplication campaigns, revealing clusters of accounts involved in repeated and\nsometimes revived dissemination of false or abusive content.", "AI": {"tldr": "\u673a\u5668\u4eba\u901a\u5e38\u88ab\u8ba4\u4e3a\u662f\u793e\u4ea4\u5a92\u4f53\u4e0a\u4f20\u64ad\u865a\u5047\u4fe1\u606f\u7684\u7f6a\u9b41\u7978\u9996\uff0c\u4f46\u672c\u7814\u7a76\u53d1\u73b0\uff0c\u7edd\u5927\u591a\u6570\uff08\u8d85\u8fc7 99%\uff09\u7684\u91cd\u590d\u53d1\u5e03\u865a\u5047\u4fe1\u606f\u7684\u8d26\u6237\u5e76\u975e\u673a\u5668\u4eba\u3002\u7814\u7a76\u4eba\u5458\u5f00\u53d1\u4e86\u4e00\u4e2a\u540d\u4e3a TweeXster \u7684\u65b0\u5de5\u5177\uff0c\u53ef\u4ee5\u53d1\u73b0\u534f\u540c\u8fdb\u884c\u865a\u5047\u4fe1\u606f\u4f20\u64ad\u7684\u8d26\u6237\u7f51\u7edc\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u8c03\u67e5\u793e\u4ea4\u5a92\u4f53\u4e0a\u4e0d\u771f\u5b9e\u7684\u91cd\u590d\u53d1\u5e03\u73b0\u8c61\uff0c\u5373\u591a\u4e2a\u8d26\u6237\u5171\u4eab\u76f8\u540c\u7684\u865a\u5047\u4fe1\u606f\u63a8\u6587\u3002", "method": "\u5229\u7528 AltNews\uff08\u4e00\u4e2a\u5370\u5ea6\u4e8b\u5b9e\u6838\u67e5\u7ec4\u7ec7\uff09\u9a8c\u8bc1\u7684\u865a\u5047\u4fe1\u606f\u6570\u636e\u96c6\uff0c\u5206\u6790\u4e86\u6765\u81ea 5,493 \u4e2a\u5df2\u77e5\u91cd\u590d\u53d1\u5e03\u6b64\u7c7b\u5185\u5bb9\u7684\u8d26\u6237\u7684\u8d85\u8fc7 1200 \u4e07\u6761\u5e16\u5b50\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u663e\u793a\uff0c\u5728\u5df2\u77e5\u7684\u91cd\u590d\u53d1\u5e03\u865a\u5047\u4fe1\u606f\u7684\u8d26\u6237\u4e2d\uff0c\u53ea\u6709\u4e0d\u5230 1% \u7684\u8d26\u6237\u8868\u73b0\u51fa\u673a\u5668\u4eba\u884c\u4e3a\uff0c\u8fd9\u4e0e\u666e\u904d\u8ba4\u4e3a\u673a\u5668\u4eba\u662f\u4f20\u64ad\u865a\u5047\u4fe1\u606f\u4e3b\u8981\u9a71\u52a8\u529b\u7684\u5047\u8bbe\u76f8\u53cd\u3002TweeXster \u6846\u67b6\u80fd\u591f\u8bc6\u522b\u51fa\u53c2\u4e0e\u91cd\u590d\u4f20\u64ad\u865a\u5047\u6216\u6ee5\u7528\u5185\u5bb9\u7684\u8d26\u6237\u96c6\u7fa4\u3002", "conclusion": "\u8be5\u7814\u7a76\u53d1\u73b0\uff0c\u793e\u4ea4\u5a92\u4f53\u4e0a\u7684\u865a\u5047\u4fe1\u606f\u4f20\u64ad\u4e3b\u8981\u7531\u975e\u673a\u5668\u4eba\u8d26\u6237\u9a71\u52a8\uff0c\u800c\u4e0d\u662f\u666e\u904d\u8ba4\u4e3a\u7684\u673a\u5668\u4eba\u3002\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a TweeXster \u7684\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u548c\u5206\u6790\u91cd\u590d\u4f20\u64ad\u865a\u5047\u6216\u6ee5\u7528\u5185\u5bb9\u7684\u6d3b\u52a8\u3002"}}
{"id": "2507.13826", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.13826", "abs": "https://arxiv.org/abs/2507.13826", "authors": ["Kimitaka Sumi", "Takuya Sakamoto"], "title": "Simulation for Noncontact Radar-Based Physiological Sensing Using Depth-Camera-Derived Human 3D Model with Electromagnetic Scattering Analysis", "comment": "10 pages, 9 figures, 6 tables. This work is going to be submitted to\n  the IEEE for possible publication", "summary": "This study proposes a method for simulating signals received by\nfrequency-modulated continuous-wave radar during respiratory monitoring, using\nhuman body geometry and displacement data acquired via a depth camera. Unlike\nprevious studies that rely on simplified models of body geometry or\ndisplacement, the proposed approach models high-frequency scattering centers\nbased on realistic depth-camera-measured body shapes and motions. Experiments\nwere conducted with six participants under varying conditions, including\nvarying target distances, seating orientations, and radar types, with\nsimultaneous acquisition from the radar and depth camera. Relative to\nconventional model-based methods, the proposed technique achieved improvements\nof 7.5%, 58.2%, and 3.2% in the correlation coefficients of radar images,\ndisplacements, and spectrograms, respectively. This work contributes to the\ngeneration of radar-based physiological datasets through simulation and\nenhances our understanding of factors affecting the accuracy of non-contact\nsensing.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u51c6\u786e\u7684\u96f7\u8fbe\u547c\u5438\u76d1\u6d4b\u4fe1\u53f7\u6a21\u62df\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f7f\u7528\u771f\u5b9e\u8eab\u4f53\u6570\u636e\u63d0\u9ad8\u4e86\u7cbe\u5ea6\u3002", "motivation": "\u4e0e\u4ee5\u5f80\u7814\u7a76\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u91c7\u7528\u4e86\u66f4\u73b0\u5b9e\u7684\u8eab\u4f53\u51e0\u4f55\u548c\u4f4d\u79fb\u6a21\u578b\uff0c\u4ee5\u63d0\u9ad8\u6a21\u62df\u7cbe\u5ea6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u771f\u5b9e\u6d4b\u91cf\u7684\u8eab\u4f53\u5f62\u72b6\u548c\u8fd0\u52a8\u6570\u636e\u6765\u6a21\u62df\u547c\u5438\u76d1\u6d4b\u8fc7\u7a0b\u4e2d\u8c03\u9891\u8fde\u7eed\u6ce2\u96f7\u8fbe\u63a5\u6536\u5230\u7684\u4fe1\u53f7\u7684\u65b9\u6cd5\u3002", "result": "\u4e0e\u4f20\u7edf\u57fa\u4e8e\u6a21\u578b\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u6280\u672f\u5728\u96f7\u8fbe\u56fe\u50cf\u3001\u4f4d\u79fb\u548c\u5149\u8c31\u56fe\u7684\u76f8\u5173\u7cfb\u6570\u65b9\u9762\u5206\u522b\u63d0\u9ad8\u4e86 7.5%\u300158.2% \u548c 3.2%\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u6a21\u62df\u6765\u751f\u6210\u96f7\u8fbe\u751f\u7406\u6570\u636e\u96c6\u7684\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u8003\u8651\u8eab\u4f53\u5f62\u72b6\u548c\u8fd0\u52a8\u7b49\u56e0\u7d20\u6765\u63d0\u9ad8\u975e\u63a5\u89e6\u4f20\u611f\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2507.13372", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13372", "abs": "https://arxiv.org/abs/2507.13372", "authors": ["Yeming Cai", "Zhenglin Li", "Yang Wang"], "title": "Enhancing Breast Cancer Detection with Vision Transformers and Graph Neural Networks", "comment": null, "summary": "Breast cancer is a leading cause of death among women globally, and early\ndetection is critical for improving survival rates. This paper introduces an\ninnovative framework that integrates Vision Transformers (ViT) and Graph Neural\nNetworks (GNN) to enhance breast cancer detection using the CBIS-DDSM dataset.\nOur framework leverages ViT's ability to capture global image features and\nGNN's strength in modeling structural relationships, achieving an accuracy of\n84.2%, outperforming traditional methods. Additionally, interpretable attention\nheatmaps provide insights into the model's decision-making process, aiding\nradiologists in clinical settings.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u5408 ViT \u548c GNN \u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u4e73\u817a\u764c\u68c0\u6d4b\uff0c\u51c6\u786e\u7387\u8fbe\u5230 84.2%\uff0c\u5e76\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u70ed\u56fe\u3002", "motivation": "\u65e9\u671f\u53d1\u73b0\u4e73\u817a\u764c\u5bf9\u4e8e\u63d0\u9ad8\u751f\u5b58\u7387\u81f3\u5173\u91cd\u8981\uff0c\u4e73\u817a\u764c\u662f\u5168\u7403\u5973\u6027\u7684\u4e3b\u8981\u6b7b\u4ea1\u539f\u56e0\u4e4b\u4e00\u3002", "method": "\u672c\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u4e2a\u521b\u65b0\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u96c6\u6210\u4e86 Vision Transformers (ViT) \u548c\u56fe\u795e\u7ecf\u7f51\u7edc (GNN)\uff0c\u4ee5\u5229\u7528 ViT \u6355\u6349\u5168\u5c40\u56fe\u50cf\u7279\u5f81\u7684\u80fd\u529b\u548c GNN \u5efa\u6a21\u7ed3\u6784\u5316\u5173\u7cfb\u7684\u80fd\u529b\uff0c\u4ece\u800c\u63d0\u9ad8\u4e73\u817a\u764c\u7684\u68c0\u6d4b\u80fd\u529b\u3002", "result": "\u8be5\u6846\u67b6\u5728 CBIS-DDSM \u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86 84.2% \u7684\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u53ef\u89e3\u91ca\u7684\u6ce8\u610f\u529b\u70ed\u56fe\u4e3a\u6a21\u578b\u7684\u51b3\u7b56\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6846\u67b6\u901a\u8fc7\u7ed3\u5408 Vision Transformers (ViT) \u548c\u56fe\u795e\u7ecf\u7f51\u7edc (GNN)\uff0c\u5728\u4e73\u817a\u764c\u68c0\u6d4b\u65b9\u9762\u53d6\u5f97\u4e86 84.2% \u7684\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002\u53ef\u89e3\u91ca\u7684\u6ce8\u610f\u529b\u70ed\u56fe\u4e3a\u6a21\u578b\u51b3\u7b56\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u89c1\u89e3\uff0c\u6709\u52a9\u4e8e\u653e\u5c04\u79d1\u533b\u751f\u5728\u4e34\u5e8a\u73af\u5883\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2507.13737", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.13737", "abs": "https://arxiv.org/abs/2507.13737", "authors": ["Ye Tian", "Xiaoyuan Ren", "Zihao Wang", "Onat Gungor", "Xiaofan Yu", "Tajana Rosing"], "title": "DailyLLM: Context-Aware Activity Log Generation Using Multi-Modal Sensors and LLMs", "comment": null, "summary": "Rich and context-aware activity logs facilitate user behavior analysis and\nhealth monitoring, making them a key research focus in ubiquitous computing.\nThe remarkable semantic understanding and generation capabilities of Large\nLanguage Models (LLMs) have recently created new opportunities for activity log\ngeneration. However, existing methods continue to exhibit notable limitations\nin terms of accuracy, efficiency, and semantic richness. To address these\nchallenges, we propose DailyLLM. To the best of our knowledge, this is the\nfirst log generation and summarization system that comprehensively integrates\ncontextual activity information across four dimensions: location, motion,\nenvironment, and physiology, using only sensors commonly available on\nsmartphones and smartwatches. To achieve this, DailyLLM introduces a\nlightweight LLM-based framework that integrates structured prompting with\nefficient feature extraction to enable high-level activity understanding.\nExtensive experiments demonstrate that DailyLLM outperforms state-of-the-art\n(SOTA) log generation methods and can be efficiently deployed on personal\ncomputers and Raspberry Pi. Utilizing only a 1.5B-parameter LLM model, DailyLLM\nachieves a 17% improvement in log generation BERTScore precision compared to\nthe 70B-parameter SOTA baseline, while delivering nearly 10x faster inference\nspeed.", "AI": {"tldr": "DailyLLM\u662f\u4e00\u4e2a\u5229\u7528\u667a\u80fd\u624b\u673a\u548c\u667a\u80fd\u624b\u8868\u4f20\u611f\u5668\u6570\u636e\u7684\u6d3b\u52a8\u65e5\u5fd7\u751f\u6210\u7cfb\u7edf\uff0c\u901a\u8fc7\u591a\u7ef4\u5ea6\u4e0a\u4e0b\u6587\u4fe1\u606f\u548c\u8f7b\u91cf\u7ea7LLM\u6846\u67b6\uff0c\u63d0\u9ad8\u4e86\u65e5\u5fd7\u751f\u6210\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u6d3b\u52a8\u65e5\u5fd7\u751f\u6210\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u3001\u6548\u7387\u548c\u8bed\u4e49\u4e30\u5bcc\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "DailyLLM\u6846\u67b6\u96c6\u6210\u4e86\u7ed3\u6784\u5316\u63d0\u793a\u548c\u9ad8\u6548\u7279\u5f81\u63d0\u53d6\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u7ea7\u6d3b\u52a8\u7406\u89e3\u3002", "result": "DailyLLM\u5728\u65e5\u5fd7\u751f\u6210\u65b9\u9762\u53d6\u5f97\u4e8617%\u7684BERTScore\u7cbe\u5ea6\u63d0\u5347\uff0c\u5e76\u5b9e\u73b0\u4e86\u8fd110\u500d\u7684\u63a8\u7406\u901f\u5ea6\u63d0\u5347\uff0c\u4f18\u4e8e\u57fa\u4e8e70B\u53c2\u6570\u7684SOTA\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "DailyLLM\u662f\u4e00\u4e2a\u96c6\u6210\u4e86\u4f4d\u7f6e\u3001\u8fd0\u52a8\u3001\u73af\u5883\u548c\u751f\u7406\u4fe1\u606f\u7684\u591a\u7ef4\u5ea6\u4e0a\u4e0b\u6587\u7684\u6d3b\u52a8\u65e5\u5fd7\u751f\u6210\u548c\u6458\u8981\u7cfb\u7edf\u3002\u5b83\u4f7f\u7528\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u57fa\u4e8eLLM\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u63d0\u793a\u548c\u9ad8\u6548\u7684\u7279\u5f81\u63d0\u53d6\u6765\u5b9e\u73b0\u9ad8\u7ea7\u6d3b\u52a8\u7406\u89e3\u3002"}}
{"id": "2507.13908", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.13908", "abs": "https://arxiv.org/abs/2507.13908", "authors": ["Frederik Thiele", "Felix Biert\u00fcmpfel", "Harald Pfifer"], "title": "A Robust Periodic Controller for Spacecraft Attitude Tracking", "comment": "Presented at European Control Conference 2025", "summary": "This paper presents a novel approach for robust periodic attitude control of\nsatellites. Respecting the periodicity of the satellite dynamics in the\nsynthesis allows to achieve constant performance and robustness requirements\nover the orbit. The proposed design follows a mixed sensitivity control design\nemploying a physically motivated weighting scheme. The controller is calculated\nusing a novel structured linear time-periodic output feedback synthesis with\nguaranteed optimal L2-performance. The synthesis poses a convex optimization\nproblem and avoids grid-wise evaluations of coupling conditions inherent for\nclassical periodic H-infinity-synthesis. Moreover, the controller has a\ntransparent and easy to implement structure. A solar power plant satellite is\nused to demonstrate the effectiveness of the proposed method for periodic\nsatellite attitude control.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u536b\u661f\u5468\u671f\u59ff\u6001\u63a7\u5236\u65b9\u6cd5\uff0c\u8003\u8651\u4e86\u536b\u661f\u52a8\u529b\u5b66\u7684\u5468\u671f\u6027\uff0c\u5b9e\u73b0\u4e86\u6052\u5b9a\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002\u8be5\u65b9\u6cd5\u91c7\u7528\u6df7\u5408\u7075\u654f\u5ea6\u8bbe\u8ba1\u548c\u7ed3\u6784\u5316\u7ebf\u6027\u65f6\u53d8\u8f93\u51fa\u53cd\u9988\u7efc\u5408\uff0c\u5e76\u901a\u8fc7\u51f8\u4f18\u5316\u89e3\u51b3\u3002\u5728\u592a\u9633\u80fd\u7535\u7ad9\u536b\u661f\u4e0a\u7684\u4eff\u771f\u7ed3\u679c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u4e3a\u4e86\u5b9e\u73b0\u536b\u661f\u5728\u6574\u4e2a\u8f68\u9053\u4e0a\u7684\u6052\u5b9a\u6027\u80fd\u548c\u9c81\u68d2\u6027\u8981\u6c42\uff0c\u5728\u63a7\u5236\u7efc\u5408\u4e2d\u5fc5\u987b\u8003\u8651\u536b\u661f\u52a8\u529b\u5b66\u7684\u5468\u671f\u6027\u3002", "method": "\u8be5\u7814\u7a76\u91c7\u7528\u4e86\u4e00\u79cd\u6df7\u5408\u7075\u654f\u5ea6\u63a7\u5236\u8bbe\u8ba1\uff0c\u5e76\u7ed3\u5408\u4e86\u7269\u7406\u542f\u53d1\u7684\u52a0\u6743\u65b9\u6848\u3002\u63a7\u5236\u5668\u662f\u5229\u7528\u4e00\u79cd\u65b0\u9896\u7684\u7ed3\u6784\u5316\u7ebf\u6027\u65f6\u53d8\u8f93\u51fa\u53cd\u9988\u7efc\u5408\u65b9\u6cd5\u8ba1\u7b97\u5f97\u51fa\u7684\uff0c\u8be5\u65b9\u6cd5\u4fdd\u8bc1\u4e86\u6700\u4f18\u7684L2\u6027\u80fd\uff0c\u5e76\u5c06\u7efc\u5408\u95ee\u9898\u8f6c\u5316\u4e3a\u4e00\u4e2a\u51f8\u4f18\u5316\u95ee\u9898\uff0c\u4ece\u800c\u907f\u514d\u4e86\u4f20\u7edf\u5468\u671fH\u65e0\u7a77\u7efc\u5408\u4e2d\u56fa\u6709\u7684\u7f51\u683c\u5316\u8bc4\u4f30\u8026\u5408\u6761\u4ef6\u3002", "result": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u4e00\u4e2a\u592a\u9633\u80fd\u7535\u7ad9\u536b\u661f\u7684\u4eff\u771f\u5f97\u5230\u4e86\u9a8c\u8bc1\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5468\u671f\u536b\u661f\u59ff\u6001\u63a7\u5236\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u536b\u661f\u5468\u671f\u59ff\u6001\u63a7\u5236\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u5728\u63a7\u5236\u7efc\u5408\u4e2d\u8003\u8651\u536b\u661f\u52a8\u529b\u5b66\u7684\u5468\u671f\u6027\uff0c\u5b9e\u73b0\u4e86\u6574\u4e2a\u8f68\u9053\u4e0a\u7684\u6052\u5b9a\u6027\u80fd\u548c\u9c81\u68d2\u6027\u8981\u6c42\u3002"}}
{"id": "2507.13662", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.13662", "abs": "https://arxiv.org/abs/2507.13662", "authors": ["Jing Cheng", "Yasser G. Alqaham", "Zhenyu Gan", "Amit K. Sanyal"], "title": "Iteratively Learning Muscle Memory for Legged Robots to Master Adaptive and High Precision Locomotion", "comment": null, "summary": "This paper presents a scalable and adaptive control framework for legged\nrobots that integrates Iterative Learning Control (ILC) with a biologically\ninspired torque library (TL), analogous to muscle memory. The proposed method\naddresses key challenges in robotic locomotion, including accurate trajectory\ntracking under unmodeled dynamics and external disturbances. By leveraging the\nrepetitive nature of periodic gaits and extending ILC to nonperiodic tasks, the\nframework enhances accuracy and generalization across diverse locomotion\nscenarios. The control architecture is data-enabled, combining a physics-based\nmodel derived from hybrid-system trajectory optimization with real-time\nlearning to compensate for model uncertainties and external disturbances. A\ncentral contribution is the development of a generalized TL that stores learned\ncontrol profiles and enables rapid adaptation to changes in speed, terrain, and\ngravitational conditions-eliminating the need for repeated learning and\nsignificantly reducing online computation. The approach is validated on the\nbipedal robot Cassie and the quadrupedal robot A1 through extensive simulations\nand hardware experiments. Results demonstrate that the proposed framework\nreduces joint tracking errors by up to 85% within a few seconds and enables\nreliable execution of both periodic and nonperiodic gaits, including slope\ntraversal and terrain adaptation. Compared to state-of-the-art whole-body\ncontrollers, the learned skills eliminate the need for online computation\nduring execution and achieve control update rates exceeding 30x those of\nexisting methods. These findings highlight the effectiveness of integrating ILC\nwith torque memory as a highly data-efficient and practical solution for legged\nlocomotion in unstructured and dynamic environments.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8fed\u4ee3\u5b66\u4e60\u63a7\u5236\uff08ILC\uff09\u548c\u529b\u77e9\u5e93\uff08TL\uff09\u7684\u673a\u5668\u4eba\u884c\u8d70\u63a7\u5236\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5177\u6709\u53ef\u6269\u5c55\u6027\u548c\u81ea\u9002\u5e94\u6027\u3002\u5b83\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u5728\u5404\u79cd\u6761\u4ef6\u4e0b\u7684\u884c\u8d70\u7cbe\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u663e\u8457\u51cf\u5c11\u4e86\u8ba1\u7b97\u91cf\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u673a\u5668\u4eba\u884c\u8d70\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u4f8b\u5982\u5728\u5b58\u5728\u672a\u5efa\u6a21\u52a8\u529b\u5b66\u548c\u5916\u90e8\u5e72\u6270\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u7cbe\u786e\u7684\u8f68\u8ff9\u8ddf\u8e2a\u3002", "method": "\u8be5\u65b9\u6cd5\u5c06\u8fed\u4ee3\u5b66\u4e60\u63a7\u5236\uff08ILC\uff09\u4e0e\u53d7\u751f\u7269\u542f\u53d1\u7684\u529b\u77e9\u5e93\uff08TL\uff09\u76f8\u7ed3\u5408\uff0c\u7528\u4e8e\u63a7\u5236\u817f\u90e8\u673a\u5668\u4eba\u3002\u5b83\u901a\u8fc7\u5229\u7528\u5468\u671f\u6027\u6b65\u6001\u7684\u91cd\u590d\u6027\u5e76\u6269\u5c55ILC\u5230\u975e\u5468\u671f\u6027\u4efb\u52a1\u6765\u63d0\u9ad8\u7cbe\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\u3002\u63a7\u5236\u67b6\u6784\u662f\u6570\u636e\u9a71\u52a8\u7684\uff0c\u7ed3\u5408\u4e86\u57fa\u4e8e\u6df7\u5408\u7cfb\u7edf\u8f68\u8ff9\u4f18\u5316\u7684\u7269\u7406\u6a21\u578b\u548c\u5b9e\u65f6\u5b66\u4e60\uff0c\u4ee5\u8865\u507f\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u548c\u5916\u90e8\u5e72\u6270\u3002TL\u5b58\u50a8\u4e86\u5b66\u4e60\u5230\u7684\u63a7\u5236\u914d\u7f6e\u6587\u4ef6\uff0c\u5e76\u80fd\u591f\u5feb\u901f\u9002\u5e94\u901f\u5ea6\u3001\u5730\u5f62\u548c\u91cd\u529b\u6761\u4ef6\u7684\u53d8\u5316\uff0c\u4ece\u800c\u51cf\u5c11\u4e86\u5728\u7ebf\u8ba1\u7b97\u548c\u91cd\u590d\u5b66\u4e60\u7684\u9700\u6c42\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u53ef\u5c06\u5173\u8282\u8ddf\u8e2a\u8bef\u5dee\u6700\u591a\u964d\u4f4e85%\uff0c\u5e76\u80fd\u53ef\u9760\u5730\u6267\u884c\u5468\u671f\u6027\u548c\u975e\u5468\u671f\u6027\u6b65\u6001\uff0c\u5305\u62ec\u659c\u5761\u7a7f\u8d8a\u548c\u5730\u5f62\u9002\u5e94\u3002\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u6846\u67b6\u7684\u5b66\u4e60\u6280\u80fd\u65e0\u9700\u5728\u7ebf\u8ba1\u7b97\u5373\u53ef\u6267\u884c\uff0c\u4e14\u63a7\u5236\u66f4\u65b0\u901f\u7387\u63d0\u9ad8\u4e8630\u500d\u4ee5\u4e0a\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8fed\u4ee3\u5b66\u4e60\u63a7\u5236\uff08ILC\uff09\u548c\u53d7\u751f\u7269\u542f\u53d1\u7684\u529b\u77e9\u5e93\uff08TL\uff09\u7684\u53ef\u6269\u5c55\u81ea\u9002\u5e94\u63a7\u5236\u6846\u67b6\uff0c\u7528\u4e8e\u8fd0\u52a8\u817f\u90e8\u673a\u5668\u4eba\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5229\u7528\u5468\u671f\u6027\u6b65\u6001\u7684\u91cd\u590d\u6027\u5e76\u5c06ILC\u6269\u5c55\u5230\u975e\u5468\u671f\u6027\u4efb\u52a1\uff0c\u63d0\u9ad8\u4e86\u5728\u975e\u6a21\u578b\u52a8\u529b\u5b66\u548c\u5916\u90e8\u5e72\u6270\u4e0b\u7684\u7cbe\u786e\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u6570\u636e\u9a71\u52a8\uff0c\u7ed3\u5408\u4e86\u57fa\u4e8e\u6df7\u5408\u7cfb\u7edf\u8f68\u8ff9\u4f18\u5316\u7684\u7269\u7406\u6a21\u578b\u548c\u5b9e\u65f6\u5b66\u4e60\uff0c\u4ee5\u8865\u507f\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u548c\u5916\u90e8\u5e72\u6270\u3002\u5176\u6838\u5fc3\u8d21\u732e\u5728\u4e8e\u5f00\u53d1\u4e86\u4e00\u4e2a\u901a\u7528\u7684TL\uff0c\u53ef\u4ee5\u5b58\u50a8\u5b66\u4e60\u5230\u7684\u63a7\u5236\u914d\u7f6e\u6587\u4ef6\uff0c\u5e76\u80fd\u591f\u5feb\u901f\u9002\u5e94\u901f\u5ea6\u3001\u5730\u5f62\u548c\u91cd\u529b\u6761\u4ef6\u7684\u53d8\u5316\uff0c\u65e0\u9700\u91cd\u590d\u5b66\u4e60\u5373\u53ef\u663e\u8457\u51cf\u5c11\u5728\u7ebf\u8ba1\u7b97\u91cf\u3002\u5728\u53cc\u8db3\u673a\u5668\u4ebaCassie\u548c\u56db\u8db3\u673a\u5668\u4ebaA1\u4e0a\u7684\u6a21\u62df\u548c\u786c\u4ef6\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u51e0\u79d2\u949f\u5185\u53ef\u5c06\u5173\u8282\u8ddf\u8e2a\u8bef\u5dee\u6700\u591a\u964d\u4f4e85%\uff0c\u5e76\u80fd\u53ef\u9760\u5730\u6267\u884c\u5468\u671f\u6027\u548c\u975e\u5468\u671f\u6027\u6b65\u6001\uff0c\u5305\u62ec\u659c\u5761\u7a7f\u8d8a\u548c\u5730\u5f62\u9002\u5e94\u3002\u4e0e\u6700\u5148\u8fdb\u7684\u5168\u8eab\u63a7\u5236\u5668\u76f8\u6bd4\uff0c\u8be5\u6846\u67b6\u7684\u5b66\u4e60\u6280\u80fd\u65e0\u9700\u5728\u7ebf\u8ba1\u7b97\u5373\u53ef\u6267\u884c\uff0c\u63a7\u5236\u66f4\u65b0\u901f\u7387\u6bd4\u73b0\u6709\u65b9\u6cd5\u5feb30\u500d\u4ee5\u4e0a\u3002"}}
{"id": "2507.14092", "categories": ["cond-mat.mtrl-sci", "cond-mat.mes-hall", "cond-mat.supr-con"], "pdf": "https://arxiv.org/pdf/2507.14092", "abs": "https://arxiv.org/abs/2507.14092", "authors": ["Adrian Greichgauer", "Roozbeh Yazdanpanah", "Alexey Taskin", "Oliver Breunig", "Yoichi Ando", "Jens Brede"], "title": "Ex Situ Fabrication of Superconducting Nanostructures for Low-Temperature STM", "comment": "29 pages, with 13 figures. Manuscript and Supporting Information", "summary": "Nanofabrication enables flexible experimental design but is often\nincompatible with scanning tunneling microscopy and spectroscopy (STM/STS) due\nto the latter's stringent surface quality requirements. Here, we present a\nfabrication strategy that combines ex situ nanolithography with in situ\nultrahigh-vacuum (UHV) cleaving to produce atomically clean, nanopatterned\nsuperconductor/topological insulator (TI) heterostructures suitable for\nhigh-resolution STM/STS. In our initial Design I, nanoribbons were defined by\netching trenches into a TI film, followed by niobium capping and sample\nflipping before cleaving. This enabled STM/STS to be applied in large areas,\nalthough edge quality was limited by etch debris. To overcome this, we\ndeveloped Design II, which avoids etching through the film by locally thinning\nit, leaving nanoscale ribbons raised above a continuous TI layer, followed\nagain by Nb capping and sample flipping before cleaving. This method yields\nclean, reproducible nanostructures with well-defined superconducting gaps,\ndemonstrating a reliable fabrication pathway for high-resolution STM/STS\nstudies of nanoscale topological devices.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.13962", "categories": ["cond-mat.mes-hall", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2507.13962", "abs": "https://arxiv.org/abs/2507.13962", "authors": ["Himakshi Mishra", "Bruno Ipaves", "Raphael Benjamim de Oliveira", "Marcelo Lopes Pereira Junior", "Raphael Matozo Tromer", "e Douglas Soares Galvao", "Chandra Shekar Tiwary"], "title": "Alignment behavior of 2D diopsides (d-silicates) under the influence of an AC electric field", "comment": null, "summary": "Controlling the alignment of two dimensional (2D) materials is crucial for\noptimizing their electronic and mechanical properties in next generation\ndevices. This study explores how electric fields can manipulate the orientation\nof 2D diopside (CaMgSi2O6) flakes, a flexible silicate material, through a\nphenomenon called flexoelectricity, where applied voltage generates mechanical\nstrain. We exfoliated diopside crystals into ultrathin flakes, placed them on\nmicroelectrodes, and used AC electric fields to induce alignment via acoustic\nstrain. Raman spectroscopy showed that the flakes reoriented/realigned under\nthe field, with vibrational peaks weakening most at high frequencies (10 MHz).\nElectrical tests revealed this alignment improves conductivity by 20-30%, as\nstraightened flakes create better pathways for current flow. Fully atomistic\nmolecular dynamics simulations further explained how these flakes naturally\nalign on surfaces within picoseconds, matching our experimental observations.\nTogether, these findings demonstrate a practical way to tune diopside\nproperties using electric fields, opening doors for its use in flexible\nelectronics, sensors, and energy devices.", "AI": {"tldr": "\u7535\u573a\u53ef\u4ee5\u8c03\u63a7\u4e8c\u4f4d\u4e8c\u7845\u9178\u9499\u8584\u7247\u7684\u53d6\u5411\uff0c\u63d0\u9ad8\u5176\u7535\u5bfc\u7387\uff0c\u4e3a\u67d4\u6027\u7535\u5b50\u5e94\u7528\u5e26\u6765\u65b0\u673a\u9047\u3002", "motivation": "\u63a7\u5236\u4e8c\u4f4d\u4e8c\u7845\u9178\u9499\u8584\u7247\u7684\u53d6\u5411\u5bf9\u4e8e\u4f18\u5316\u5176\u5728\u4e0b\u4e00\u4ee3\u5668\u4ef6\u4e2d\u7684\u7535\u5b50\u548c\u673a\u68b0\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7f3a\u4e4f\u6709\u6548\u7684\u65b9\u6cd5\u6765\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\u3002", "method": "\u901a\u8fc7\u5916\u52a0\u4ea4\u6d41\u7535\u573a\uff0c\u5229\u7528\u538b\u7535\u6548\u5e94\u5f15\u8d77\u7684\u5f62\u53d8\u9a71\u52a8\u4e8c\u4f4d\u4e8c\u7845\u9178\u9499\u8584\u7247\u5728\u5fae\u7535\u6781\u4e0a\u91cd\u65b0\u6392\u5217\uff0c\u5e76\u901a\u8fc7\u62c9\u66fc\u5149\u8c31\u548c\u7535\u5b66\u6d4b\u8bd5\u8868\u5f81\u5176\u53d6\u5411\u53d8\u5316\u548c\u7535\u5b66\u6027\u8d28\u7684\u6539\u5584\u3002\u540c\u65f6\uff0c\u5229\u7528\u5168\u539f\u5b50\u5206\u5b50\u52a8\u529b\u5b66\u6a21\u62df\u89e3\u91ca\u4e86\u8584\u7247\u5728\u8868\u9762\u7684\u81ea\u53d1\u6392\u5217\u673a\u5236\u3002", "result": "\u901a\u8fc7\u7535\u573a\u8c03\u63a7\uff0c\u4e8c\u4f4d\u4e8c\u7845\u9178\u9499\u8584\u7247\u7684\u7535\u5bfc\u7387\u63d0\u9ad8\u4e8620-30%\uff0c\u5e76\u4e14\u5b9e\u9a8c\u89c2\u5bdf\u548c\u5206\u5b50\u52a8\u529b\u5b66\u6a21\u62df\u5747\u8bc1\u5b9e\u4e86\u8584\u7247\u5728\u7535\u573a\u4f5c\u7528\u4e0b\u7684\u53d6\u5411\u6027\u6392\u5217\u3002", "conclusion": "\u672c\u7814\u7a76\u8bc1\u660e\u4e86\u4f7f\u7528\u7535\u573a\u8c03\u63a7\u4e8c\u4f4d\u4e8c\u7845\u9178\u9499\uff08CaMgSi2O6\uff09\u8584\u7247\u7684\u53d6\u5411\u662f\u4e00\u79cd\u6709\u524d\u666f\u7684\u65b9\u6cd5\uff0c\u53ef\u7528\u4e8e\u4f18\u5316\u5176\u5728\u67d4\u6027\u7535\u5b50\u3001\u4f20\u611f\u5668\u548c\u80fd\u6e90\u8bbe\u5907\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2507.14089", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2507.14089", "abs": "https://arxiv.org/abs/2507.14089", "authors": ["Vincent Cohen-Addad", "Fabian Kuhn", "Zahra Parsaeian"], "title": "An Efficient Massively Parallel Constant-Factor Approximation Algorithm for the $k$-Means Problem", "comment": null, "summary": "In this paper, we present an efficient massively parallel approximation\nalgorithm for the $k$-means problem. Specifically, we provide an MPC algorithm\nthat computes a constant-factor approximation to an arbitrary $k$-means\ninstance in $O(\\log\\log n \\cdot \\log\\log\\log n)$ rounds. The algorithm uses\n$O(n^\\sigma)$ bits of memory per machine, where $\\sigma > 0$ is a constant that\ncan be made arbitrarily small. The global memory usage is\n$O(n^{1+\\varepsilon})$ bits for an arbitrarily small constant $\\varepsilon >\n0$, and is thus only slightly superlinear. Recently, Czumaj, Gao, Jiang,\nKrauthgamer, and Vesel\\'{y} showed that a constant-factor bicriteria\napproximation can be computed in $O(1)$ rounds in the MPC model. However, our\nalgorithm is the first constant-factor approximation for the general $k$-means\nproblem that runs in $o(\\log n)$ rounds in the MPC model.\n  Our approach builds upon the foundational framework of Jain and Vazirani. The\ncore component of our algorithm is a constant-factor approximation for the\nrelated facility location problem. While such an approximation was already\nachieved in constant time in the work of Czumaj et al.\\ mentioned above, our\nversion additionally satisfies the so-called Lagrangian Multiplier Preserving\n(LMP) property. This property enables the transformation of a facility location\napproximation into a comparably good $k$-means approximation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684MPC\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3k-means\u95ee\u9898\uff0c\u8be5\u7b97\u6cd5\u80fd\u5728\u66f4\u5c11\u7684\u8f6e\u6b21\uff08o(log n)\uff09\u5185\u5b9e\u73b0\u5e38\u6570\u56e0\u5b50\u8fd1\u4f3c\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u63a7\u7684\u5185\u5b58\u4f7f\u7528\u91cf\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u89e3\u51b3k-means\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u9ad8\u6548\u7684MPC\u7b97\u6cd5\uff0c\u4ee5\u5b9e\u73b0\u66f4\u5c11\u7684\u8ba1\u7b97\u8f6e\u6570\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u5927\u89c4\u6a21\u5e76\u884c\u8fd1\u4f3c\u7b97\u6cd5\u6765\u89e3\u51b3k-means\u95ee\u9898\u3002\u8be5\u7b97\u6cd5\u5229\u7528MPC\u6a21\u578b\uff0c\u5728O(log log n * log log log n)\u8f6e\u5185\u8ba1\u7b97\u51fa\u4efb\u610fk-means\u5b9e\u4f8b\u7684\u5e38\u6570\u56e0\u5b50\u8fd1\u4f3c\u503c\u3002\u7b97\u6cd5\u5728\u6bcf\u53f0\u673a\u5668\u4e0a\u4f7f\u7528O(n^\u03c3)\u6bd4\u7279\u7684\u5185\u5b58\uff0c\u5176\u4e2d\u03c3>0\u662f\u4e00\u4e2a\u5e38\u6570\uff0c\u53ef\u4ee5\u4efb\u610f\u5c0f\u3002\u5168\u5c40\u5185\u5b58\u4f7f\u7528\u91cf\u4e3aO(n^(1+\u03b5))\u6bd4\u7279\uff0c\u5176\u4e2d\u03b5>0\u662f\u4e00\u4e2a\u4efb\u610f\u5c0f\u7684\u5e38\u6570\uff0c\u56e0\u6b64\u5185\u5b58\u4f7f\u7528\u91cf\u4ec5\u7565\u8d85\u7ebf\u6027\u3002", "result": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cdMPC\u7b97\u6cd5\uff0c\u80fd\u591f\u5728O(log log n * log log log n)\u8f6e\u5185\u8ba1\u7b97k-means\u95ee\u9898\u7684\u5e38\u6570\u56e0\u5b50\u8fd1\u4f3c\u503c\uff0c\u5e76\u4e14\u5185\u5b58\u4f7f\u7528\u91cf\u4ec5\u7565\u8d85\u7ebf\u6027\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u662f\u7b2c\u4e00\u4e2a\u5728MPC\u6a21\u578b\u4e2d\u5b9e\u73b0o(log n)\u8f6e\u7684k-means\u95ee\u9898\u5e38\u6570\u56e0\u5b50\u8fd1\u4f3c\u7b97\u6cd5\u3002"}}
{"id": "2507.13411", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13411", "abs": "https://arxiv.org/abs/2507.13411", "authors": ["Nur A Zarin Nishat", "Andrea Coletta", "Luigi Bellomarini", "Kossi Amouzouvi", "Jens Lehmann", "Sahar Vahdati"], "title": "Aligning Knowledge Graphs and Language Models for Factual Accuracy", "comment": null, "summary": "Large language models like GPT-4, Gemini, and Claude have transformed natural\nlanguage processing (NLP) tasks such as question answering, dialogue\ngeneration, summarization, and so forth; yet their susceptibility to\nhallucination stands as one of the major challenges. Among numerous approaches\nto overcome this challenge, integration of Knowledge Graphs (KGs) into language\nmodels has emerged as a promising solution as it provides structured, reliable,\ndomain-specific, and up-to-date external information to the language models. In\nthis paper, we introduce ALIGNed-LLM, a simple yet effective approach to\nimprove language models' factuality via a lean strategy to infuse KGs into the\nlatent space of language models inspired by LLaVA where visual and textual\ninformation is infused. We use embeddings from a pre-trained Knowledge Graph\nEmbedding (KGE) model, such as TransE, and a trainable projection layer to\nalign entity and text embeddings. This alignment enables the language model to\ndistinguish between similar entities improving factual grounding and reducing\nhallucination. We tested our approach on three popular questions-answering\nbenchmark datasets alongside language models of varying sizes, showing\nsignificant improvement. Furthermore, we applied our approach to a real-world\nfinancial use case from a large central bank in Europe, which demands high\naccuracy and precision, demonstrating a substantial improvement of the LLM\nanswers.", "AI": {"tldr": "ALIGNed-LLM \u662f\u4e00\u79cd\u5c06\u77e5\u8bc6\u56fe\u8c31\u6ce8\u5165\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u9f50\u5b9e\u4f53\u548c\u6587\u672c\u5d4c\u5165\u6765\u63d0\u9ad8\u4e8b\u5b9e\u51c6\u786e\u6027\u5e76\u51cf\u5c11\u5e7b\u89c9\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5982 GPT-4\u3001Gemini \u548c Claude \u5728\u5404\u79cd\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u6613\u4e8e\u4ea7\u751f\u5e7b\u89c9\u662f\u4e3b\u8981\u6311\u6218\u4e4b\u4e00\u3002\u5c06\u77e5\u8bc6\u56fe\u8c31\uff08KGs\uff09\u96c6\u6210\u5230\u8bed\u8a00\u6a21\u578b\u4e2d\uff0c\u901a\u8fc7\u63d0\u4f9b\u7ed3\u6784\u5316\u3001\u53ef\u9760\u3001\u9886\u57df\u7279\u5b9a\u7684\u5916\u90e8\u4fe1\u606f\uff0c\u88ab\u8ba4\u4e3a\u662f\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u7684\u6709\u524d\u9014\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "ALIGNed-LLM \u65b9\u6cd5\u901a\u8fc7\u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u77e5\u8bc6\u56fe\u8c31\u5d4c\u5165\uff08KGE\uff09\u6a21\u578b\uff08\u5982 TransE\uff09\u7684\u5d4c\u5165\u548c\u4e00\u4e2a\u53ef\u8bad\u7ec3\u7684\u6295\u5f71\u5c42\u6765\u5bf9\u9f50\u5b9e\u4f53\u548c\u6587\u672c\u5d4c\u5165\uff0c\u4ece\u800c\u5c06 KGs \u6ce8\u5165\u8bed\u8a00\u6a21\u578b\u7684\u6f5c\u5728\u7a7a\u95f4\u3002", "result": "\u5728\u4e09\u4e2a\u5e38\u7528\u7684\u95ee\u7b54\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cALIGNed-LLM \u5747\u663e\u793a\u51fa\u663e\u8457\u7684\u6539\u8fdb\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u5728\u4e00\u4e2a\u6765\u81ea\u6b27\u6d32\u5927\u578b\u4e2d\u592e\u94f6\u884c\u7684\u771f\u5b9e\u91d1\u878d\u7528\u4f8b\u4e2d\u4e5f\u5f97\u5230\u4e86\u9a8c\u8bc1\uff0c\u663e\u8457\u63d0\u9ad8\u4e86 LLM \u56de\u7b54\u7684\u51c6\u786e\u6027\u548c\u7cbe\u786e\u5ea6\u3002", "conclusion": "ALIGNed-LLM \u901a\u8fc7\u5c06\u77e5\u8bc6\u56fe\u8c31\uff08KGs\uff09\u6ce8\u5165\u8bed\u8a00\u6a21\u578b\u7684\u6f5c\u5728\u7a7a\u95f4\uff0c\u6709\u6548\u63d0\u9ad8\u4e86\u8bed\u8a00\u6a21\u578b\u7684 factual accuracy\uff0c\u5e76\u51cf\u5c11\u4e86\u5e7b\u89c9\u3002\u8be5\u65b9\u6cd5\u5728\u4e09\u4e2a\u95ee\u7b54\u57fa\u51c6\u6570\u636e\u96c6\u548c\u91d1\u878d\u7528\u4f8b\u4e2d\u5747\u663e\u793a\u51fa\u663e\u8457\u7684\u6539\u8fdb\u3002"}}
{"id": "2507.13593", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2507.13593", "abs": "https://arxiv.org/abs/2507.13593", "authors": ["William S. DeWitt", "Benjamin H. Feintzeig"], "title": "Forward-Time Equivalent of a \"Retrocausal\" Diffusion Hidden Variable Model for Quantum Mechanics", "comment": null, "summary": "A recently proposed stochastic hidden variable model for quantum mechanics\nhas been claimed to involve \"retrocausality\" due to the appearance of equations\nof motion with future-time boundary conditions. We formulate an equivalent\nsystem of forward-time equations of motion that gives rise to the same\ntrajectories as solutions, but involves only initial-time boundary conditions.\nThe forward-time dynamics involves a guidance term for the dynamical variables,\ndetermined by the phase-space distribution corresponding to a quantum\nwavefunction. We show, however, that this particular guidance term can be\nrecovered as the mean-field limit of averaged pairwise interactions among an\nensemble of finitely many particles.", "AI": {"tldr": "\u4e00\u4e2a\u58f0\u79f0\u6d89\u53ca\u201c\u9006\u56e0\u679c\u5173\u7cfb\u201d\u7684\u968f\u673a\u9690\u53d8\u91cf\u6a21\u578b\uff0c\u88ab\u8bc1\u660e\u53ef\u4ee5\u7528\u4ec5\u5305\u542b\u521d\u59cb\u65f6\u95f4\u8fb9\u754c\u6761\u4ef6\u7684\u524d\u6eaf\u65b9\u7a0b\u6765\u7b49\u4ef7\u5730\u63cf\u8ff0\uff0c\u5e76\u4e14\u5176\u5f15\u5bfc\u9879\u53ef\u4ee5\u4f5c\u4e3a\u5e73\u5747\u573a\u6781\u9650\u6765\u89e3\u91ca\u3002", "motivation": "\u53cd\u9a73\u201c\u8be5\u6a21\u578b\u6d89\u53ca\u2018\u9006\u56e0\u679c\u5173\u7cfb\u2019\u201d\u7684\u89c2\u70b9\uff0c\u5e76\u63d0\u4f9b\u4e00\u79cd\u4ec5\u5305\u542b\u521d\u59cb\u65f6\u95f4\u8fb9\u754c\u6761\u4ef6\u7684\u524d\u6eaf\u52a8\u529b\u5b66\u89e3\u91ca\u3002", "method": "\u901a\u8fc7\u5efa\u7acb\u4e00\u4e2a\u7b49\u4ef7\u7684\u3001\u4ec5\u5305\u542b\u521d\u59cb\u65f6\u95f4\u8fb9\u754c\u6761\u4ef6\u7684\u524d\u6eaf\u65b9\u7a0b\u7ec4\u6765\u5206\u6790\u8be5\u6a21\u578b\u3002", "result": "\u8bc1\u660e\u4e86\u8be5\u6a21\u578b\u4e2d\u7684\u5f15\u5bfc\u9879\u53ef\u4ee5\u770b\u4f5c\u662f\u6709\u9650\u7c92\u5b50\u7cfb\u7efc\u4e2d\u5e73\u5747\u6210\u5bf9\u76f8\u4e92\u4f5c\u7528\u7684\u5e73\u5747\u573a\u6781\u9650\u3002", "conclusion": "\u8be5\u6a21\u578b\u53ef\u4ee5\u88ab\u8868\u8ff0\u4e3a\u524d\u6eaf\u52a8\u529b\u5b66\u7684\u7b49\u4ef7\u5f62\u5f0f\uff0c\u8be5\u5f62\u5f0f\u4ec5\u6d89\u53ca\u521d\u59cb\u65f6\u95f4\u8fb9\u754c\u6761\u4ef6\u3002"}}
{"id": "2507.13423", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13423", "abs": "https://arxiv.org/abs/2507.13423", "authors": ["Edward Henderson", "Dewi Gould", "Richard Everson", "George De Ath", "Nick Pepper"], "title": "Air Traffic Controller Task Demand via Graph Neural Networks: An Interpretable Approach to Airspace Complexity", "comment": "Author Accepted Manuscript version of paper at the AIAA AVIATION\n  Forum 2025", "summary": "Real-time assessment of near-term Air Traffic Controller (ATCO) task demand\nis a critical challenge in an increasingly crowded airspace, as existing\ncomplexity metrics often fail to capture nuanced operational drivers beyond\nsimple aircraft counts. This work introduces an interpretable Graph Neural\nNetwork (GNN) framework to address this gap. Our attention-based model predicts\nthe number of upcoming clearances, the instructions issued to aircraft by\nATCOs, from interactions within static traffic scenarios. Crucially, we derive\nan interpretable, per-aircraft task demand score by systematically ablating\naircraft and measuring the impact on the model's predictions. Our framework\nsignificantly outperforms an ATCO-inspired heuristic and is a more reliable\nestimator of scenario complexity than established baselines. The resulting tool\ncan attribute task demand to specific aircraft, offering a new way to analyse\nand understand the drivers of complexity for applications in controller\ntraining and airspace redesign.", "AI": {"tldr": "\u4f7f\u7528\u57fa\u4e8e GNN \u7684\u53ef\u89e3\u91ca\u6846\u67b6\uff0c\u53ef\u4ee5\u5bf9\u822a\u7a7a\u4ea4\u901a\u7ba1\u5236\u5458\u7684\u4efb\u52a1\u9700\u6c42\u8fdb\u884c\u5b9e\u65f6\u8bc4\u4f30\uff0c\u5e76\u5c06\u5176\u5f52\u56e0\u4e8e\u7279\u5b9a\u98de\u673a\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5f53\u524d\u7a7a\u57df\u65e5\u76ca\u62e5\u6324\u7684\u80cc\u666f\u4e0b\uff0c\u5bf9\u8fd1\u5730\u7a7a\u57df\u822a\u7a7a\u4ea4\u901a\u7ba1\u5236\u5458\uff08ATCO\uff09\u4efb\u52a1\u9700\u6c42\u7684\u5b9e\u65f6\u8bc4\u4f30\u6240\u9762\u4e34\u7684\u6311\u6218\uff0c\u56e0\u4e3a\u73b0\u6709\u7684\u590d\u6742\u6027\u6307\u6807\u5f80\u5f80\u65e0\u6cd5\u6355\u6349\u9664\u7b80\u5355\u98de\u673a\u6570\u91cf\u4e4b\u5916\u7684\u7ec6\u5fae\u64cd\u4f5c\u9a71\u52a8\u56e0\u7d20\u3002", "method": "\u672c\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u4e2a\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u7684\u53ef\u89e3\u91ca\u6846\u67b6\uff0c\u5229\u7528\u6ce8\u610f\u529b\u673a\u5236\u5bf9\u5373\u5c06\u53d1\u5e03\u7684\u6307\u4ee4\u6570\u91cf\u8fdb\u884c\u9884\u6d4b\u3002\u901a\u8fc7\u7cfb\u7edf\u5730\u79fb\u9664\u98de\u673a\u5e76\u6d4b\u91cf\u5176\u5bf9\u6a21\u578b\u9884\u6d4b\u7684\u5f71\u54cd\uff0c\u6211\u4eec\u63a8\u5bfc\u51fa\u4e86\u4e00\u4e2a\u53ef\u89e3\u91ca\u7684\u3001\u6bcf\u4e2a\u98de\u673a\u7684\u4efb\u52a1\u9700\u6c42\u5206\u6570\u3002", "result": "\u6211\u4eec\u7684\u6846\u67b6\u663e\u8457\u4f18\u4e8e\u53d7\u822a\u7a7a\u4ea4\u901a\u7ba1\u5236\u5458\u542f\u53d1\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u5e76\u4e14\u6bd4\u5df2\u5efa\u7acb\u7684\u57fa\u51c6\u66f4\u80fd\u53ef\u9760\u5730\u4f30\u8ba1\u573a\u666f\u590d\u6742\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u5c06\u4efb\u52a1\u9700\u6c42\u5f52\u56e0\u4e8e\u7279\u5b9a\u98de\u673a\uff0c\u4e3a\u5206\u6790\u548c\u7406\u89e3\u901a\u4fe1\u5458\u57f9\u8bad\u548c\u7a7a\u57df\u91cd\u65b0\u8bbe\u8ba1\u4e2d\u590d\u6742\u6027\u7684\u9a71\u52a8\u56e0\u7d20\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2507.13939", "categories": ["cs.SI"], "pdf": "https://arxiv.org/pdf/2507.13939", "abs": "https://arxiv.org/abs/2507.13939", "authors": ["Gibran Ali", "Neal Feierabend", "Prarthana Doshi", "Whoibin Chung", "Simona Babiceanu", "Michael Fontaine"], "title": "Automated Route-based Conflation Between Linear Referencing System Maps And OpenStreetMap Using Open-source Tools", "comment": "Accepted to the 2025 IEEE International Conference on Intelligent\n  Transportation Systems (ITSC 2025)", "summary": "Transportation researchers and planners utilize a wide range of roadway\nmetrics that are usually associated with different basemaps. Conflation is an\nimportant process for transferring these metrics onto a single basemap.\nHowever, conflation is often an expensive and time-consuming process based on\nproprietary algorithms that require manual verification.\n  In this paper, an automated open-source process is used to conflate two\nbasemaps: the linear reference system (LRS) basemap produced by the Virginia\nDepartment of Transportation and the OpenStreetMap (OSM) basemap for Virginia.\nThis process loads one LRS route at a time, determines the correct direction of\ntravel, interpolates to fill gaps larger than 12 meters, and then uses\nValhalla's map-matching algorithm to find the corresponding points along OSM's\nsegments. Valhalla's map-matching process uses a Hidden Markov Model (HMM) and\nViterbi search-based approach to find the most likely OSM segments matching the\nLRS route.\n  This work has three key contributions. First, it conflates the Virginia\nroadway network LRS map with OSM using an automated conflation method based on\nHMM and Viterbi search. Second, it demonstrates a novel open-source processing\npipeline that could be replicated without the need for proprietary licenses.\nFinally, the overall conflation process yields over 98% successful matches,\nwhich is an improvement over most automated processes currently available for\nthis type of conflation.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u7684\u5f00\u6e90\u65b9\u6cd5\uff0c\u5229\u7528\u9690\u9a6c\u5c14\u53ef\u592b\u6a21\u578b\u548c\u7ef4\u7279\u6bd4\u641c\u7d22\uff0c\u6210\u529f\u5c06\u5f17\u5409\u5c3c\u4e9a\u5dde\u7684\u9053\u8def\u7ebf\u6027\u53c2\u8003\u7cfb\u7edf\uff08LRS\uff09\u5730\u56fe\u4e0e\u5f00\u653e\u8857\u56fe\uff08OSM\uff09\u5339\u914d\uff0c\u6210\u529f\u7387\u8fbe98%\u4ee5\u4e0a\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u6602\u8d35\u548c\u8017\u65f6\u95ee\u9898\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u9053\u8def\u5ea6\u91cf\u5728\u4e0d\u540c\u5730\u56fe\u5e95\u56fe\u95f4\u8f6c\u79fb\u65f6\uff0c\u4f20\u7edf\u57fa\u4e8e\u4e13\u6709\u7b97\u6cd5\u4e14\u9700\u8981\u4eba\u5de5\u9a8c\u8bc1\u7684\u6602\u8d35\u4e14\u8017\u65f6\u7684\u201c\u5730\u56fe\u5339\u914d\u201d\u95ee\u9898\uff0c\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u81ea\u52a8\u5316\u7684\u3001\u5f00\u6e90\u7684\u5730\u56fe\u5339\u914d\u65b9\u6cd5\u3002", "method": "\u8be5\u7814\u7a76\u4f7f\u7528\u4e00\u79cd\u57fa\u4e8e\u9690\u9a6c\u5c14\u53ef\u592b\u6a21\u578b\uff08HMM\uff09\u548c\u7ef4\u7279\u6bd4\u641c\u7d22\u7684\u81ea\u52a8\u5339\u914d\u65b9\u6cd5\uff0c\u5c06\u5f17\u5409\u5c3c\u4e9a\u5dde\u7684LRS\u8def\u7ebf\u6570\u636e\u4e0eOSM\u7684\u5730\u7406\u7247\u6bb5\u8fdb\u884c\u5339\u914d\u3002\u5177\u4f53\u6b65\u9aa4\u5305\u62ec\uff1a\u9010\u6761\u52a0\u8f7dLRS\u8def\u7ebf\u3001\u786e\u5b9a\u884c\u8f66\u65b9\u5411\u3001\u5bf9\u5927\u4e8e12\u7c73\u7684\u95f4\u9699\u8fdb\u884c\u63d2\u503c\uff0c\u7136\u540e\u4f7f\u7528Valhalla\u7684\u5730\u56fe\u5339\u914d\u7b97\u6cd5\u627e\u5230\u5bf9\u5e94\u7684OSM\u7247\u6bb5\u3002", "result": "\u7814\u7a76\u5b9e\u73b0\u4e86\u5f17\u5409\u5c3c\u4e9a\u5dde\u9053\u8def\u7f51\u7edcLRS\u5730\u56fe\u4e0eOSM\u7684\u81ea\u52a8\u5339\u914d\uff0c\u5339\u914d\u6210\u529f\u7387\u8d85\u8fc798%\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u81ea\u52a8\u5316\u65b9\u6cd5\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u4f9b\u590d\u5236\u7684\u5f00\u6e90\u5904\u7406\u6d41\u7a0b\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u5730\u5c06\u5f17\u5409\u5c3c\u4e9a\u5dde\u7684\u9053\u8def\u7f51\u7edc\u7ebf\u6027\u53c2\u8003\u7cfb\u7edf\uff08LRS\uff09\u5730\u56fe\u4e0e\u5f00\u653e\u8857\u56fe\uff08OSM\uff09\u8fdb\u884c\u4e86\u81ea\u52a8\u5339\u914d\uff0c\u5339\u914d\u6210\u529f\u7387\u8d85\u8fc798%\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u53ef\u590d\u5236\u7684\u3001\u65e0\u9700\u4e13\u6709\u8bb8\u53ef\u8bc1\u7684\u5f00\u6e90\u5904\u7406\u6d41\u7a0b\u3002"}}
{"id": "2507.13829", "categories": ["eess.SP", "math.PR"], "pdf": "https://arxiv.org/pdf/2507.13829", "abs": "https://arxiv.org/abs/2507.13829", "authors": ["Arnaud Poinas", "R\u00e9mi Bardenet"], "title": "On two fundamental properties of the zeros of spectrograms of noisy signals", "comment": null, "summary": "The spatial distribution of the zeros of the spectrogram is significantly\naltered when a signal is added to white Gaussian noise. The zeros tend to\ndelineate the support of the signal, and deterministic structures form in the\npresence of interference, as if the zeros were trapped. While sophisticated\nmethods have been proposed to detect signals as holes in the pattern of\nspectrogram zeros, few formal arguments have been made to support the\ndelineation and trapping effects. Through detailed computations for simple toy\nsignals, we show that two basic mathematical arguments, the intensity of zeros\nand Rouch\\'e's theorem, allow discussing delineation and trapping, and the\ninfluence of parameters like the signal-to-noise ratio. In particular,\ninterfering chirps, even nearly superimposed, yield an easy-to-detect\ndeterministic structure among zeros.", "AI": {"tldr": "\u8c31\u56fe\u96f6\u70b9\u53ef\u4ee5\u63ed\u793a\u4fe1\u53f7\u4fe1\u606f\uff0c\u6570\u5b66\u539f\u7406\u53ef\u4ee5\u89e3\u91ca\u8fd9\u4e9b\u73b0\u8c61\u3002", "motivation": "\u63a2\u7a76\u8c31\u56fe\u96f6\u70b9\u5728\u4fe1\u53f7\u5b58\u5728\u65f6\uff0c\u5176\u7a7a\u95f4\u5206\u5e03\u53d8\u5316\u80cc\u540e\u7684\u6570\u5b66\u539f\u7406\uff0c\u7279\u522b\u662f\u4fe1\u53f7\u8fb9\u754c\u63cf\u7ed8\u548c\u96f6\u70b9\u88ab\u6355\u83b7\u7684\u73b0\u8c61\u3002", "method": "\u901a\u8fc7\u8be6\u7ec6\u7684\u8ba1\u7b97\u548c\u4e24\u4e2a\u57fa\u672c\u7684\u6570\u5b66\u8bba\u8bc1\uff08\u96f6\u70b9\u7684\u5f3a\u5ea6\u548c Rouch\u00e9 \u5b9a\u7406\uff09\u6765\u5206\u6790\u4fe1\u53f7\u6dfb\u52a0\u5230\u767d\u9ad8\u65af\u566a\u58f0\u4e2d\u65f6\uff0c\u8c31\u56fe\u96f6\u70b9\u7684\u7a7a\u95f4\u5206\u5e03\u7684\u53d8\u5316\u3002", "result": "\u8bc1\u660e\u4e86\u5373\u4f7f\u662f\u5e72\u6270\u7684\u091a\u093f\u0930\u092a\u4fe1\u53f7\uff0c\u53ea\u8981\u53c2\u6570\u5408\u9002\uff0c\u4e5f\u80fd\u5728\u8c31\u56fe\u96f6\u70b9\u4e2d\u5f62\u6210\u6613\u4e8e\u68c0\u6d4b\u7684\u786e\u5b9a\u6027\u7ed3\u6784\u3002", "conclusion": "\u96f6\u70b9\u548c Rouch\u00e9 \u5b9a\u7406\u53ef\u4ee5\u7528\u6765\u89e3\u91ca\u4fe1\u53f7\u5728\u566a\u58f0\u80cc\u666f\u4e0b\u7684\u8c31\u56fe\u96f6\u70b9\u5206\u5e03\u7684\u53d8\u5316\uff0c\u5305\u62ec\u4fe1\u53f7\u7684\u8fb9\u754c\u548c\u5e72\u6270\u7684\u5b58\u5728\u3002"}}
{"id": "2507.13373", "categories": ["cs.CV", "I.4.8; I.2.10; H.5.1; I.2.6"], "pdf": "https://arxiv.org/pdf/2507.13373", "abs": "https://arxiv.org/abs/2507.13373", "authors": ["Xiaojian Lin", "Wenxin Zhang", "Yuchu Jiang", "Wangyu Wu", "Yiran Guo", "Kangxu Wang", "Zongzheng Zhang", "Guijin Wang", "Lei Jin", "Hao Zhao"], "title": "Butter: Frequency Consistency and Hierarchical Fusion for Autonomous Driving Object Detection", "comment": "10 pages, 6 figures. Supplementary material: 8 pages, 7 figures.\n  Accepted at ACM Multimedia 2025", "summary": "Hierarchical feature representations play a pivotal role in computer vision,\nparticularly in object detection for autonomous driving. Multi-level semantic\nunderstanding is crucial for accurately identifying pedestrians, vehicles, and\ntraffic signs in dynamic environments. However, existing architectures, such as\nYOLO and DETR, struggle to maintain feature consistency across different scales\nwhile balancing detection precision and computational efficiency. To address\nthese challenges, we propose Butter, a novel object detection framework\ndesigned to enhance hierarchical feature representations for improving\ndetection robustness. Specifically, Butter introduces two key innovations:\nFrequency-Adaptive Feature Consistency Enhancement (FAFCE) Component, which\nrefines multi-scale feature consistency by leveraging adaptive frequency\nfiltering to enhance structural and boundary precision, and Progressive\nHierarchical Feature Fusion Network (PHFFNet) Module, which progressively\nintegrates multi-level features to mitigate semantic gaps and strengthen\nhierarchical feature learning. Through extensive experiments on BDD100K, KITTI,\nand Cityscapes, Butter demonstrates superior feature representation\ncapabilities, leading to notable improvements in detection accuracy while\nreducing model complexity. By focusing on hierarchical feature refinement and\nintegration, Butter provides an advanced approach to object detection that\nachieves a balance between accuracy, deployability, and computational\nefficiency in real-time autonomous driving scenarios. Our model and\nimplementation are publicly available at https://github.com/Aveiro-Lin/Butter,\nfacilitating further research and validation within the autonomous driving\ncommunity.", "AI": {"tldr": "Butter\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u7269\u4f53\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7FAFCE\u7ec4\u4ef6\u548cPHFFNet\u6a21\u5757\u6765\u589e\u5f3a\u5206\u5c42\u7279\u5f81\u8868\u793a\uff0c\u4ee5\u63d0\u9ad8\u68c0\u6d4b\u9c81\u68d2\u6027\uff0c\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u964d\u4f4e\u4e86\u6a21\u578b\u590d\u6742\u5ea6\u3002", "motivation": "\u73b0\u6709\u7684\u7269\u4f53\u68c0\u6d4b\u67b6\u6784\uff08\u5982YOLO\u548cDETR\uff09\u5728\u4fdd\u6301\u4e0d\u540c\u5c3a\u5ea6\u7279\u5f81\u7684\u4e00\u81f4\u6027\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u540c\u65f6\u96be\u4ee5\u5e73\u8861\u68c0\u6d4b\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\uff0cButter\u6846\u67b6\u88ab\u63d0\u51fa\uff0c\u65e8\u5728\u589e\u5f3a\u5206\u5c42\u7279\u5f81\u8868\u793a\u4ee5\u63d0\u9ad8\u68c0\u6d4b\u9c81\u68d2\u6027\u3002", "method": "Butter\u6846\u67b6\u5f15\u5165\u4e86\u4e24\u79cd\u521b\u65b0\uff1a1. \u9891\u7387\u81ea\u9002\u5e94\u7279\u5f81\u4e00\u81f4\u6027\u589e\u5f3a\uff08FAFCE\uff09\u7ec4\u4ef6\uff0c\u901a\u8fc7\u5229\u7528\u81ea\u9002\u5e94\u9891\u7387\u6ee4\u6ce2\u6765\u4f18\u5316\u591a\u5c3a\u5ea6\u7279\u5f81\u4e00\u81f4\u6027\uff0c\u589e\u5f3a\u7ed3\u6784\u548c\u8fb9\u754c\u7cbe\u5ea6\u30022. \u6e10\u8fdb\u5f0f\u5206\u5c42\u7279\u5f81\u878d\u5408\u7f51\u7edc\uff08PHFFNet\uff09\u6a21\u5757\uff0c\u9010\u6b65\u96c6\u6210\u591a\u5c42\u6b21\u7279\u5f81\uff0c\u4ee5\u7f29\u5c0f\u8bed\u4e49\u5dee\u8ddd\u5e76\u52a0\u5f3a\u5206\u5c42\u7279\u5f81\u5b66\u4e60\u3002", "result": "\u901a\u8fc7\u5728BDD100K\u3001KITTI\u548cCityscapes\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\uff0cButter\u5c55\u793a\u4e86\u5176\u5353\u8d8a\u7684\u7279\u5f81\u8868\u793a\u80fd\u529b\uff0c\u5728\u63d0\u9ad8\u68c0\u6d4b\u7cbe\u5ea6\u7684\u540c\u65f6\u964d\u4f4e\u4e86\u6a21\u578b\u590d\u6742\u5ea6\u3002", "conclusion": "Butter\u901a\u8fc7\u5173\u6ce8\u5206\u5c42\u7279\u5f81\u7684\u7cbe\u70bc\u548c\u96c6\u6210\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u7269\u4f53\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u5728\u51c6\u786e\u6027\u3001\u53ef\u90e8\u7f72\u6027\u548c\u5b9e\u65f6\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u4e86\u5e73\u8861\u3002"}}
{"id": "2507.13759", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13759", "abs": "https://arxiv.org/abs/2507.13759", "authors": ["Carlos Bobed", "Carlota Quintana", "Eduardo Mena", "Jorge Bobed", "Fernando Bobillo"], "title": "OntView: What you See is What you Meant", "comment": null, "summary": "In the field of knowledge management and computer science, ontologies provide\na structured framework for modeling domain-specific knowledge by defining\nconcepts and their relationships. However, the lack of tools that provide\neffective visualization is still a significant challenge. While numerous\nontology editors and viewers exist, most of them fail to graphically represent\nontology structures in a meaningful and non-overwhelming way, limiting users'\nability to comprehend dependencies and properties within large ontological\nframeworks.\n  In this paper, we present OntView, an ontology viewer that is designed to\nprovide users with an intuitive visual representation of ontology concepts and\ntheir formal definitions through a user-friendly interface. Building on the use\nof a DL reasoner, OntView follows a \"What you see is what you meant\" paradigm,\nshowing the actual inferred knowledge. One key aspect for this is its ability\nto visualize General Concept Inclusions (GCI), a feature absent in existing\nvisualization tools. Moreover, to avoid a possible information overload,\nOntView also offers different ways to show a simplified view of the ontology\nby: 1) creating ontology summaries by assessing the importance of the concepts\n(according to different available algorithms), 2) focusing the visualization on\nthe existing TBox elements between two given classes and 3) allowing to\nhide/show different branches in a dynamic way without losing the semantics.\nOntView has been released with an open-source license for the whole community.", "AI": {"tldr": "OntView is an open-source ontology viewer that solves the problem of poor visualization in existing tools. It offers an intuitive interface, visualizes inferred knowledge including GCIs, and provides simplification features to manage complexity, making ontologies easier to understand.", "motivation": "The motivation behind OntView is to address the significant challenge posed by the lack of effective visualization tools for ontologies. Existing tools often fail to graphically represent ontology structures in a meaningful and non-overwhelming way, hindering users' comprehension of dependencies and properties within large ontological frameworks.", "method": "OntView is an ontology viewer that utilizes a DL reasoner to provide a \"What you see is what you meant\" paradigm, visualizing inferred knowledge, including General Concept Inclusions (GCI). It offers simplification methods such as creating concept summaries based on importance algorithms, focusing visualizations on TBox elements between classes, and allowing dynamic hiding/showing of branches.", "result": "OntView provides users with an intuitive visual representation of ontology concepts and their formal definitions through a user-friendly interface. It uniquely visualizes General Concept Inclusions (GCI) and offers simplification methods to manage information overload, making it easier for users to comprehend complex ontologies.", "conclusion": "OntView provides an intuitive visual representation of ontology concepts and their formal definitions through a user-friendly interface, addressing the challenge of effective ontology visualization by incorporating features like GCI visualization and various simplification methods to manage information overload. It has been released under an open-source license."}}
{"id": "2507.13931", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.13931", "abs": "https://arxiv.org/abs/2507.13931", "authors": ["L. D. Couto", "K. Haghverdi", "F. Guo", "K. Trad", "G. Mulder"], "title": "Identifiability Analysis of a Pseudo-Two-Dimensional Model & Single Particle Model-Aided Parameter Estimation", "comment": "9 pages, 2 figures, This work has been presented at the 2025 American\n  Control Conference (ACC) and will appear in the conference proceedings.\n  \\c{opyright} 2025 IEEE", "summary": "This contribution presents a parameter identification methodology for the\naccurate and fast estimation of model parameters in a pseudo-two-dimensional\n(P2D) battery model. The methodology consists of three key elements. First, the\ndata for identification is inspected and specific features herein that need to\nbe captured are included in the model. Second, the P2D model is analyzed to\nassess the identifiability of the physical model parameters and propose\nalternative parameterizations that alleviate possible issues. Finally, diverse\noperating conditions are considered that excite distinct battery dynamics which\nallows the use of different low-order battery models accordingly. Results show\nthat, under low current conditions, the use of low-order models achieve\nparameter estimates at least 500 times faster than using the P2D model at the\nexpense of twice the error. However, if accuracy is a must, these estimated\nparameters can be used to initialize the P2D model and perform the\nidentification in half of the time.", "AI": {"tldr": "\u4e00\u79cd\u7528\u4e8eP2D\u7535\u6c60\u6a21\u578b\u7684\u53c2\u6570\u8bc6\u522b\u65b9\u6cd5\uff0c\u53ef\u52a0\u5feb\u53c2\u6570\u4f30\u8ba1\u901f\u5ea6\uff0c\u540c\u65f6\u5c3d\u91cf\u51cf\u5c11\u7cbe\u5ea6\u635f\u5931\u3002", "motivation": "\u4e3a\u4e86\u5728\u4f2a\u4e8c\u7ef4\uff08P2D\uff09\u7535\u6c60\u6a21\u578b\u4e2d\u51c6\u786e\u5feb\u901f\u5730\u4f30\u8ba1\u6a21\u578b\u53c2\u6570\u3002", "method": "\u8be5\u65b9\u6cd5\u5305\u62ec\u4e09\u4e2a\u5173\u952e\u8981\u7d20\uff1a1. \u68c0\u67e5\u8bc6\u522b\u6570\u636e\u5e76\u5305\u542b\u6a21\u578b\u4e2d\u9700\u8981\u6355\u83b7\u7684\u7279\u5b9a\u7279\u5f81\u30022. \u5206\u6790P2D\u6a21\u578b\u4ee5\u8bc4\u4f30\u7269\u7406\u6a21\u578b\u53c2\u6570\u7684\u53ef\u8bc6\u522b\u6027\u5e76\u63d0\u51fa\u66ff\u4ee3\u53c2\u6570\u5316\u4ee5\u7f13\u89e3\u6f5c\u5728\u95ee\u9898\u30023. \u8003\u8651\u4e0d\u540c\u7684\u64cd\u4f5c\u6761\u4ef6\u4ee5\u6fc0\u53d1\u4e0d\u540c\u7684\u7535\u6c60\u52a8\u529b\u5b66\uff0c\u4ece\u800c\u5141\u8bb8\u4f7f\u7528\u4e0d\u540c\u7684\u4f4e\u9636\u7535\u6c60\u6a21\u578b\u3002", "result": "\u5728\u4f4e\u7535\u6d41\u6761\u4ef6\u4e0b\uff0c\u4f7f\u7528\u4f4e\u9636\u6a21\u578b\u4f30\u8ba1\u53c2\u6570\u7684\u901f\u5ea6\u6bd4\u4f7f\u7528P2D\u6a21\u578b\u5feb\u81f3\u5c11500\u500d\uff0c\u4f46\u8bef\u5dee\u662f\u5176\u4e24\u500d\u3002\u5982\u679c\u7cbe\u5ea6\u662f\u5fc5\u987b\u7684\uff0c\u8fd9\u4e9b\u4f30\u8ba1\u7684\u53c2\u6570\u53ef\u7528\u4e8e\u521d\u59cb\u5316P2D\u6a21\u578b\uff0c\u5e76\u5c06\u8bc6\u522b\u65f6\u95f4\u7f29\u77ed\u4e00\u534a\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u53c2\u6570\u8bc6\u522b\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u4f2a\u4e8c\u7ef4\uff08P2D\uff09\u7535\u6c60\u6a21\u578b\u4e2d\u51c6\u786e\u5feb\u901f\u5730\u4f30\u8ba1\u6a21\u578b\u53c2\u6570\u3002\u8be5\u65b9\u6cd5\u5305\u62ec\u4e09\u4e2a\u5173\u952e\u8981\u7d20\uff1a\u68c0\u67e5\u8bc6\u522b\u6570\u636e\u5e76\u5305\u542b\u6a21\u578b\u4e2d\u9700\u8981\u6355\u83b7\u7684\u7279\u5b9a\u7279\u5f81\uff1b\u5206\u6790P2D\u6a21\u578b\u4ee5\u8bc4\u4f30\u7269\u7406\u6a21\u578b\u53c2\u6570\u7684\u53ef\u8bc6\u522b\u6027\u5e76\u63d0\u51fa\u66ff\u4ee3\u53c2\u6570\u5316\u4ee5\u7f13\u89e3\u6f5c\u5728\u95ee\u9898\uff1b\u8003\u8651\u4e0d\u540c\u7684\u64cd\u4f5c\u6761\u4ef6\u4ee5\u6fc0\u53d1\u4e0d\u540c\u7684\u7535\u6c60\u52a8\u529b\u5b66\uff0c\u4ece\u800c\u5141\u8bb8\u4f7f\u7528\u4e0d\u540c\u7684\u4f4e\u9636\u7535\u6c60\u6a21\u578b\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5728\u4f4e\u7535\u6d41\u6761\u4ef6\u4e0b\uff0c\u4f7f\u7528\u4f4e\u9636\u6a21\u578b\u4f30\u8ba1\u53c2\u6570\u7684\u901f\u5ea6\u6bd4\u4f7f\u7528P2D\u6a21\u578b\u5feb\u81f3\u5c11500\u500d\uff0c\u4f46\u8bef\u5dee\u662f\u5176\u4e24\u500d\u3002\u7136\u800c\uff0c\u5982\u679c\u7cbe\u5ea6\u662f\u5fc5\u987b\u7684\uff0c\u8fd9\u4e9b\u4f30\u8ba1\u7684\u53c2\u6570\u53ef\u7528\u4e8e\u521d\u59cb\u5316P2D\u6a21\u578b\uff0c\u5e76\u5c06\u8bc6\u522b\u65f6\u95f4\u7f29\u77ed\u4e00\u534a\u3002"}}
{"id": "2507.13702", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.13702", "abs": "https://arxiv.org/abs/2507.13702", "authors": ["Junho Choi", "Kihwan Ryoo", "Jeewon Kim", "Taeyun Kim", "Eungchang Lee", "Myeongwoo Jeong", "Kevin Christiansen Marsim", "Hyungtae Lim", "Hyun Myung"], "title": "SaWa-ML: Structure-Aware Pose Correction and Weight Adaptation-Based Robust Multi-Robot Localization", "comment": "This paper has been accepted to the 2025 IEEE/RSJ International\n  Conference on Intelligent Robots and Systems (IROS)", "summary": "Multi-robot localization is a crucial task for implementing multi-robot\nsystems. Numerous researchers have proposed optimization-based multi-robot\nlocalization methods that use camera, IMU, and UWB sensors. Nevertheless,\ncharacteristics of individual robot odometry estimates and distance\nmeasurements between robots used in the optimization are not sufficiently\nconsidered. In addition, previous researches were heavily influenced by the\nodometry accuracy that is estimated from individual robots. Consequently,\nlong-term drift error caused by error accumulation is potentially inevitable.\nIn this paper, we propose a novel visual-inertial-range-based multi-robot\nlocalization method, named SaWa-ML, which enables geometric structure-aware\npose correction and weight adaptation-based robust multi-robot localization.\nOur contributions are twofold: (i) we leverage UWB sensor data, whose range\nerror does not accumulate over time, to first estimate the relative positions\nbetween robots and then correct the positions of each robot, thus reducing\nlong-term drift errors, (ii) we design adaptive weights for robot pose\ncorrection by considering the characteristics of the sensor data and\nvisual-inertial odometry estimates. The proposed method has been validated in\nreal-world experiments, showing a substantial performance increase compared\nwith state-of-the-art algorithms.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSaWa-ML\u7684\u65b0\u578b\u591a\u673a\u5668\u4eba\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u5229\u7528UWB\u4f20\u611f\u5668\u6570\u636e\u51cf\u5c11\u957f\u671f\u6f02\u79fb\u8bef\u5dee\uff0c\u5e76\u901a\u8fc7\u81ea\u9002\u5e94\u6743\u91cd\u63d0\u9ad8\u9c81\u68d2\u6027\u3002", "motivation": "\u591a\u673a\u5668\u4eba\u5b9a\u4f4d\u662f\u5b9e\u73b0\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u5173\u952e\u4efb\u52a1\u3002\u5c3d\u7ba1\u7814\u7a76\u4eba\u5458\u63d0\u51fa\u4e86\u8bb8\u591a\u57fa\u4e8e\u4f18\u5316\u7684\u591a\u673a\u5668\u4eba\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u4f46\u4e2a\u4f53\u673a\u5668\u4eba\u91cc\u7a0b\u8ba1\u4f30\u8ba1\u548c\u4f18\u5316\u4e2d\u4f7f\u7528\u7684\u673a\u5668\u4eba\u95f4\u8ddd\u79bb\u6d4b\u91cf\u7684\u7279\u6027\u8003\u8651\u4e0d\u8db3\uff0c\u5e76\u4e14\u5148\u524d\u7814\u7a76\u53d7\u5230\u4e2a\u4f53\u673a\u5668\u4eba\u91cc\u7a0b\u8ba1\u4f30\u8ba1\u7cbe\u5ea6\u7684\u5f71\u54cd\uff0c\u5bfc\u81f4\u957f\u671f\u6f02\u79fb\u8bef\u5dee\u4e0d\u53ef\u907f\u514d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u89c6\u89c9-\u60ef\u6027-\u6d4b\u8ddd\u591a\u673a\u5668\u4eba\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u540d\u4e3aSaWa-ML\uff0c\u5b83\u80fd\u591f\u8fdb\u884c\u51e0\u4f55\u7ed3\u6784\u611f\u77e5\u59ff\u6001\u6821\u6b63\u548c\u57fa\u4e8e\u6743\u91cd\u81ea\u9002\u5e94\u7684\u9c81\u68d2\u591a\u673a\u5668\u4eba\u5b9a\u4f4d\u3002", "result": "\u5229\u7528\u4e0d\u7d2f\u79ef\u91cf\u7a0b\u8bef\u5dee\u7684UWB\u4f20\u611f\u5668\u6570\u636e\uff0c\u9996\u5148\u4f30\u8ba1\u673a\u5668\u4eba\u95f4\u7684\u76f8\u5bf9\u4f4d\u7f6e\uff0c\u7136\u540e\u6821\u6b63\u6bcf\u4e2a\u673a\u5668\u4eba\u7684\u4f4d\u7f6e\uff0c\u4ece\u800c\u51cf\u5c11\u957f\u671f\u6f02\u79fb\u8bef\u5dee\u3002\u901a\u8fc7\u8003\u8651\u4f20\u611f\u5668\u6570\u636e\u548c\u89c6\u89c9-\u60ef\u6027\u91cc\u7a0b\u8ba1\u4f30\u8ba1\u7684\u7279\u6027\uff0c\u4e3a\u673a\u5668\u4eba\u59ff\u6001\u6821\u6b63\u8bbe\u8ba1\u4e86\u81ea\u9002\u5e94\u6743\u91cd\u3002", "conclusion": "\u5728\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\uff0c\u4e0e\u6700\u5148\u8fdb\u7684\u7b97\u6cd5\u76f8\u6bd4\uff0c\u6027\u80fd\u6709\u663e\u8457\u63d0\u9ad8\u3002"}}
{"id": "2507.14110", "categories": ["cond-mat.mtrl-sci", "cond-mat.dis-nn", "cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2507.14110", "abs": "https://arxiv.org/abs/2507.14110", "authors": ["Daniel Kaplan", "Pavel A. Volkov", "Jennifer Coulter", "Shiwei Zhang", "Premala Chandra"], "title": "Spatiotemporal Order and Parametric Instabilities from First-Principles", "comment": "9 page + 2 appendices. 8 Figures. Comments welcome", "summary": "Shaping crystal structure with light is an enduring goal of physics and\nmaterials engineering. Here we present calculations in candidate materials\nselected by symmetry that allow light-induced spatiotemporal parametric\ninstabilities. We demonstrate a theoretical framework that includes a complete\nsymmetry analysis of phonon modes that contribute to parametric instabilities\nacross all non-centrosymmetric point groups, a detailed survey of the materials\nlandscape and finally the computation of nonlinear couplings from first\nprinciples. We then showcase detailed results for chiral crystals,\nferroelectrics, and layered van der Waals materials. Our results pave the way\ntowards realizing designer time-crystalline order in quantum materials,\ndetectable with time-resolved diffractive probes.", "AI": {"tldr": "\u6211\u4eec\u5c55\u793a\u4e86\u4e00\u79cd\u5229\u7528\u5149\u8bf1\u5bfc\u65f6\u7a7a\u53c2\u91cf\u4e0d\u7a33\u5b9a\u6027\u6765\u63a7\u5236\u6676\u4f53\u7ed3\u6784\u7684\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u624b\u5f81\u6676\u4f53\u3001\u94c1\u7535\u4f53\u548c\u5c42\u72b6\u8303\u5fb7\u534e\u6750\u6599\uff0c\u5e76\u4e3a\u8bbe\u8ba1\u65f6\u95f4\u6676\u4f53\u5e8f\u63d0\u4f9b\u4e86\u9014\u5f84\u3002", "motivation": "\u5229\u7528\u5149\u6765\u5851\u9020\u6676\u4f53\u7ed3\u6784\u662f\u7269\u7406\u5b66\u548c\u6750\u6599\u5de5\u7a0b\u4e2d\u4e00\u4e2a\u6301\u4e45\u7684\u76ee\u6807\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u7406\u8bba\u6846\u67b6\uff0c\u5305\u62ec\u5bf9\u6240\u6709\u975e\u4e2d\u5fc3\u5bf9\u79f0\u70b9\u7fa4\u4e2d\u5f71\u54cd\u53c2\u91cf\u4e0d\u7a33\u5b9a\u7684\u58f0\u5b50\u6a21\u5f0f\u7684\u5b8c\u6574\u5bf9\u79f0\u6027\u5206\u6790\uff0c\u5bf9\u6750\u6599\u683c\u5c40\u7684\u8be6\u7ec6\u8c03\u67e5\uff0c\u4ee5\u53ca\u4ece\u7b2c\u4e00\u6027\u539f\u7406\u8ba1\u7b97\u975e\u7ebf\u6027\u8026\u5408\u3002", "result": "\u6211\u4eec\u5c55\u793a\u4e86\u624b\u5f81\u6676\u4f53\u3001\u94c1\u7535\u4f53\u548c\u5c42\u72b6\u8303\u5fb7\u534e\u6750\u6599\u7684\u8be6\u7ec6\u7ed3\u679c\u3002", "conclusion": "\u6211\u4eec\u7684\u7814\u7a76\u4e3a\u5728\u91cf\u5b50\u6750\u6599\u4e2d\u5b9e\u73b0\u8bbe\u8ba1\u7684\u65f6\u95f4\u6676\u4f53\u5e8f\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u8be5\u5e8f\u53ef\u4ee5\u901a\u8fc7\u65f6\u95f4\u5206\u8fa8\u884d\u5c04\u63a2\u9488\u8fdb\u884c\u68c0\u6d4b\u3002"}}
{"id": "2507.14028", "categories": ["cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2507.14028", "abs": "https://arxiv.org/abs/2507.14028", "authors": ["Guangyue Ji", "David E. Palomino", "Nathan Goldman", "Tomoki Ozawa", "Peter Riseborough", "Jie Wang", "Bruno Mera"], "title": "Density Matrix Geometry and Sum Rules", "comment": "21 pages", "summary": "Geometry plays a fundamental role in a wide range of physical responses, from\nanomalous transport coefficients to their related sum rules. Notable examples\ninclude the quantization of the Hall conductivity and the Souza-Wilkens-Martin\n(SWM) sum rule -- both valid at zero temperature, independent of interactions\nand disorder. The finite-temperature generalization of the SWM sum rule has\nbeen explored in the literature, revealing deep connections to the geometry of\ndensity matrices. Building on recent advances in time-dependent geometric\nframeworks, we propose a time-dependent quantum geometric tensor for thermal\ndensity matrices. This formalism provides a unified interpretation of known sum\nrules within the framework of the fluctuation-dissipation theorem, further\nelucidating their fundamental geometric origin. In addition, it provides\nexperimentally accessible methods to probe quantum geometry beyond the\nzero-temperature regime.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65f6\u53d8\u91cf\u5b50\u51e0\u4f55\u65b9\u6cd5\uff0c\u7528\u4e8e\u7814\u7a76\u70ed\u529b\u5b66\u7cfb\u7edf\u4e2d\u7684\u51e0\u4f55\u6548\u5e94\uff0c\u7edf\u4e00\u4e86\u89e3\u91ca\u4e86\u7269\u7406\u73b0\u8c61\u4e2d\u7684\u6c42\u548c\u89c4\u5219\uff0c\u5e76\u63d0\u4f9b\u4e86\u65b0\u7684\u5b9e\u9a8c\u63a2\u7d22\u9014\u5f84\u3002", "motivation": "\u51e0\u4f55\u5728\u4ece\u5f02\u5e38\u8f93\u8fd0\u7cfb\u6570\u5230\u76f8\u5173\u6c42\u548c\u89c4\u5219\u7684\u5e7f\u6cdb\u7269\u7406\u54cd\u5e94\u4e2d\u8d77\u7740\u57fa\u672c\u4f5c\u7528\u3002\u96f6\u6e29\u4e0b\u7684\u970d\u5c14\u7535\u5bfc\u91cf\u5b50\u5316\u548cSouza-Wilkens-Martin (SWM) \u6c42\u548c\u89c4\u5219\u662f\u5176\u4e2d\u7684\u5178\u578b\u4f8b\u5b50\u3002SWM \u6c42\u548c\u89c4\u5219\u7684\u6709\u9650\u6e29\u5ea6\u63a8\u5e7f\u5df2\u88ab\u63a2\u8ba8\uff0c\u5e76\u63ed\u793a\u4e86\u5176\u4e0e\u5bc6\u5ea6\u77e9\u9635\u51e0\u4f55\u7684\u6df1\u523b\u8054\u7cfb\u3002", "method": "\u5229\u7528\u8fd1\u671f\u5728\u65f6\u53d8\u51e0\u4f55\u6846\u67b6\u65b9\u9762\u7684\u8fdb\u5c55\uff0c\u63d0\u51fa\u4e00\u4e2a\u9002\u7528\u4e8e\u70ed\u5bc6\u5ea6\u77e9\u9635\u7684\u65f6\u53d8\u91cf\u5b50\u51e0\u4f55\u5f20\u91cf\u3002", "result": "\u8be5\u7406\u8bba\u4e3a\u5df2\u77e5\u7684\u6c42\u548c\u89c4\u5219\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u89e3\u91ca\uff0c\u5c06\u5176\u7f6e\u4e8e\u6da8\u843d\u8017\u6563\u5b9a\u7406\u7684\u6846\u67b6\u5185\uff0c\u5e76\u8fdb\u4e00\u6b65\u9610\u660e\u4e86\u5b83\u4eec\u57fa\u672c\u7684\u51e0\u4f55\u8d77\u6e90\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u8fd8\u63d0\u4f9b\u4e86\u63a2\u7d22\u96f6\u6e29\u533a\u4ee5\u5916\u91cf\u5b50\u51e0\u4f55\u7684\u5b9e\u9a8c\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u9002\u7528\u4e8e\u70ed\u5bc6\u5ea6\u77e9\u9635\u7684\u65f6\u53d8\u91cf\u5b50\u51e0\u4f55\u5f20\u91cf\uff0c\u5b83\u7edf\u4e00\u4e86\u89e3\u91ca\u4e86\u5df2\u77e5\u6c42\u548c\u89c4\u5219\u4e0e\u6da8\u843d\u8017\u6563\u5b9a\u7406\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5e76\u9610\u660e\u4e86\u5176\u6839\u672c\u7684\u51e0\u4f55\u8d77\u6e90\u3002\u6b64\u5916\uff0c\u5b83\u8fd8\u63d0\u4f9b\u4e86\u5728\u96f6\u6e29\u533a\u4ee5\u5916\u63a2\u7d22\u91cf\u5b50\u51e0\u4f55\u7684\u5b9e\u9a8c\u53ef\u53ca\u65b9\u6cd5\u3002"}}
{"id": "2507.14114", "categories": ["cs.DS", "cs.DC"], "pdf": "https://arxiv.org/pdf/2507.14114", "abs": "https://arxiv.org/abs/2507.14114", "authors": ["Ahammed Ullah", "S. M. Ferdous", "Alex Pothen"], "title": "Weighted Matching in a Poly-Streaming Model", "comment": "40 pages, ESA 2025", "summary": "We introduce the poly-streaming model, a generalization of streaming models\nof computation in which $k$ processors process $k$ data streams containing a\ntotal of $N$ items. The algorithm is allowed $O\\left(f(k)\\cdot M_1\\right)$\nspace, where $M_1$ is either $o\\left(N\\right)$ or the space bound for a\nsequential streaming algorithm. Processors may communicate as needed.\nAlgorithms are assessed by the number of passes, per-item processing time,\ntotal runtime, space usage, communication cost, and solution quality.\n  We design a single-pass algorithm in this model for approximating the maximum\nweight matching (MWM) problem. Given $k$ edge streams and a parameter\n$\\varepsilon > 0$, the algorithm computes a\n$\\left(2+\\epsilon\\right)$-approximate MWM. We analyze its performance in a\nshared-memory parallel setting: for any constant $\\varepsilon > 0$, it runs in\ntime $\\widetilde{O}\\left(L_{\\max}+n\\right)$, where $n$ is the number of\nvertices and $L_{\\max}$ is the maximum stream length. It supports\n$O\\left(1\\right)$ per-edge processing time using $\\widetilde{O}\\left(k\\cdot\nn\\right)$ space. We further generalize the design to hierarchical\narchitectures, in which $k$ processors are partitioned into $r$ groups, each\nwith its own shared local memory. The total intergroup communication is\n$\\widetilde{O}\\left(r \\cdot n\\right)$ bits, while all other performance\nguarantees are preserved.\n  We evaluate the algorithm on a shared-memory system using graphs with\ntrillions of edges. It achieves substantial speedups as $k$ increases and\nproduces matchings with weights significantly exceeding the theoretical\nguarantee. On our largest test graph, it reduces runtime by nearly two orders\nof magnitude and memory usage by five orders of magnitude compared to an\noffline algorithm.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5927\u89c4\u6a21\u56fe\u5339\u914d\u7684poly-streaming\u6a21\u578b\u548c\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u5728\u5e76\u884c\u5904\u7406\u548c\u5185\u5b58\u6548\u7387\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u5728\u6d41\u5f0f\u8ba1\u7b97\u6a21\u578b\u4e2d\uff0c\u5904\u7406\u5927\u89c4\u6a21\u6570\u636e\u96c6\u901a\u5e38\u9700\u8981\u591a\u6b21\u904d\u5386\uff0c\u8fd9\u4f1a\u5f71\u54cd\u6548\u7387\u3002\u8be5\u7814\u7a76\u65e8\u5728\u6269\u5c55\u6d41\u5f0f\u8ba1\u7b97\u6a21\u578b\uff0c\u4ee5\u652f\u6301\u591a\u5904\u7406\u5668\u5e76\u884c\u5904\u7406\u591a\u4e2a\u6570\u636e\u6d41\uff0c\u4ece\u800c\u63d0\u9ad8\u5904\u7406\u5927\u89c4\u6a21\u56fe\u5339\u914d\u95ee\u9898\u7684\u6548\u7387\u3002", "method": "\u6587\u7ae0\u4ecb\u7ecd\u4e86\u4e00\u79cd\u79f0\u4e3a\u201cpoly-streaming\u201d\u7684\u8ba1\u7b97\u6a21\u578b\uff0c\u8fd9\u662f\u6d41\u5f0f\u8ba1\u7b97\u6a21\u578b\u7684\u6cdb\u5316\uff0c\u5176\u4e2dk\u4e2a\u5904\u7406\u5668\u5904\u7406k\u4e2a\u6570\u636e\u6d41\uff0c\u5305\u542bN\u4e2a\u9879\u3002\u8be5\u7b97\u6cd5\u5141\u8bb8\u4f7f\u7528O(f(k)\u00b7M1)\u7684\u7a7a\u95f4\uff0c\u5176\u4e2dM1\u53ef\u4ee5\u662fo(N)\u6216\u987a\u5e8f\u6d41\u7b97\u6cd5\u7684\u7a7a\u95f4\u754c\u9650\u3002\u5904\u7406\u5668\u53ef\u4ee5\u6839\u636e\u9700\u8981\u8fdb\u884c\u901a\u4fe1\u3002\u7b97\u6cd5\u901a\u8fc7\u8bc4\u4f30\u5176\u901a\u8fc7\u7684\u6b21\u6570\u3001\u6bcf\u9879\u5904\u7406\u65f6\u95f4\u3001\u603b\u8fd0\u884c\u65f6\u95f4\u3001\u7a7a\u95f4\u4f7f\u7528\u91cf\u3001\u901a\u4fe1\u6210\u672c\u548c\u89e3\u7684\u8d28\u91cf\u6765\u8bc4\u4f30\u3002\u6587\u7ae0\u8bbe\u8ba1\u4e86\u4e00\u79cd\u5355\u901a\u9053\u7b97\u6cd5\uff0c\u7528\u4e8e\u8fd1\u4f3c\u6700\u5927\u52a0\u6743\u5339\u914d\uff08MWM\uff09\u95ee\u9898\u3002\u8be5\u7b97\u6cd5\u4f7f\u7528k\u6761\u8fb9\u6d41\u548c\u53c2\u6570\u03b5>0\uff0c\u53ef\u4ee5\u8ba1\u7b97\u51fa(2+\u03b5)\u8fd1\u4f3cMWM\u3002", "result": "\u8be5\u7b97\u6cd5\u5728\u6709\u5411\u56fe\u4e0a\u5b9e\u73b0\u4e86\u8fd1\u4f3c\u6700\u5927\u52a0\u6743\u5339\u914d\uff0c\u5e76\u5bf9\u901a\u4fe1\u6210\u672c\u548c\u6574\u4f53\u6027\u80fd\u8fdb\u884c\u4e86\u5206\u6790\u3002\u5728\u5171\u4eab\u5185\u5b58\u5e76\u884c\u8bbe\u7f6e\u4e2d\uff0c\u5bf9\u4e8e\u4efb\u4f55\u6052\u5b9a\u7684\u03b5>0\uff0c\u8be5\u7b97\u6cd5\u7684\u8fd0\u884c\u65f6\u95f4\u4e3a\u00d5(Lmax+n)\uff0c\u5176\u4e2dn\u662f\u9876\u70b9\u7684\u6570\u91cf\uff0cLmax\u662f\u6700\u5927\u6d41\u7684\u957f\u5ea6\u3002\u5b83\u652f\u6301\u00d5(k\u22c5n)\u7684\u7a7a\u95f4\u548cO(1)\u7684\u6bcf\u8fb9\u5904\u7406\u65f6\u95f4\u3002\u6b64\u5916\uff0c\u8be5\u7b97\u6cd5\u53ef\u63a8\u5e7f\u5230\u5206\u5c42\u67b6\u6784\uff0c\u4fdd\u6301\u4e86\u6240\u6709\u5176\u4ed6\u6027\u80fd\u4fdd\u8bc1\u3002\u5728\u5177\u6709\u6570\u4e07\u4ebf\u6761\u8fb9\u7684\u56fe\u4e0a\u5bf9\u8be5\u7b97\u6cd5\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u968f\u7740k\u7684\u589e\u52a0\uff0c\u8fd0\u884c\u65f6\u95f4\u663e\u8457\u7f29\u77ed\uff0c\u5185\u5b58\u4f7f\u7528\u91cf\u5927\u5e45\u51cf\u5c11\uff0c\u5e76\u4e14\u5f97\u5230\u7684\u5339\u914d\u6743\u91cd\u8fdc\u8d85\u7406\u8bba\u4fdd\u8bc1\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u5728\u6709\u5411\u56fe\u4e0a\u5b9e\u73b0\u4e86\u8fd1\u4f3c\u6700\u5927\u52a0\u6743\u5339\u914d\uff0c\u5e76\u5bf9\u901a\u4fe1\u6210\u672c\u548c\u6574\u4f53\u6027\u80fd\u8fdb\u884c\u4e86\u5206\u6790\u3002\u5728\u5171\u4eab\u5185\u5b58\u5e76\u884c\u8bbe\u7f6e\u4e2d\uff0c\u5bf9\u4e8e\u4efb\u4f55\u6052\u5b9a\u7684\u03b5>0\uff0c\u8be5\u7b97\u6cd5\u7684\u8fd0\u884c\u65f6\u95f4\u4e3a\u00d5(Lmax+n)\uff0c\u5176\u4e2dn\u662f\u9876\u70b9\u7684\u6570\u91cf\uff0cLmax\u662f\u6700\u5927\u6d41\u7684\u957f\u5ea6\u3002\u5b83\u652f\u6301\u00d5(k\u22c5n)\u7684\u7a7a\u95f4\u548cO(1)\u7684\u6bcf\u8fb9\u5904\u7406\u65f6\u95f4\u3002\u6b64\u5916\uff0c\u8be5\u7b97\u6cd5\u53ef\u63a8\u5e7f\u5230\u5206\u5c42\u67b6\u6784\uff0c\u4fdd\u6301\u4e86\u6240\u6709\u5176\u4ed6\u6027\u80fd\u4fdd\u8bc1\u3002\u5728\u5177\u6709\u6570\u4e07\u4ebf\u6761\u8fb9\u7684\u56fe\u4e0a\u5bf9\u8be5\u7b97\u6cd5\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u968f\u7740k\u7684\u589e\u52a0\uff0c\u8fd0\u884c\u65f6\u95f4\u663e\u8457\u7f29\u77ed\uff0c\u5185\u5b58\u4f7f\u7528\u91cf\u5927\u5e45\u51cf\u5c11\uff0c\u5e76\u4e14\u5f97\u5230\u7684\u5339\u914d\u6743\u91cd\u8fdc\u8d85\u7406\u8bba\u4fdd\u8bc1\u3002"}}
{"id": "2507.13474", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.13474", "abs": "https://arxiv.org/abs/2507.13474", "authors": ["Liang Lin", "Zhihao Xu", "Xuehai Tang", "Shi Liu", "Biyu Zhou", "Fuqing Zhu", "Jizhong Han", "Songlin Hu"], "title": "Paper Summary Attack: Jailbreaking LLMs through LLM Safety Papers", "comment": null, "summary": "The safety of large language models (LLMs) has garnered significant research\nattention. In this paper, we argue that previous empirical studies demonstrate\nLLMs exhibit a propensity to trust information from authoritative sources, such\nas academic papers, implying new possible vulnerabilities. To verify this\npossibility, a preliminary analysis is designed to illustrate our two findings.\nBased on this insight, a novel jailbreaking method, Paper Summary Attack\n(\\llmname{PSA}), is proposed. It systematically synthesizes content from either\nattack-focused or defense-focused LLM safety paper to construct an adversarial\nprompt template, while strategically infilling harmful query as adversarial\npayloads within predefined subsections. Extensive experiments show significant\nvulnerabilities not only in base LLMs, but also in state-of-the-art reasoning\nmodel like Deepseek-R1. PSA achieves a 97\\% attack success rate (ASR) on\nwell-aligned models like Claude3.5-Sonnet and an even higher 98\\% ASR on\nDeepseek-R1. More intriguingly, our work has further revealed diametrically\nopposed vulnerability bias across different base models, and even between\ndifferent versions of the same model, when exposed to either attack-focused or\ndefense-focused papers. This phenomenon potentially indicates future research\nclues for both adversarial methodologies and safety alignment.Code is available\nat https://github.com/233liang/Paper-Summary-Attack", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPSA\u7684\u65b0\u578b\u8d8a\u72f1\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u5b66\u672f\u8bba\u6587\u7684\u4fe1\u4efb\u503e\u5411\uff0c\u6210\u529f\u653b\u51fb\u4e86\u591a\u4e2a\u6a21\u578b\uff0c\u5e76\u53d1\u73b0\u4e86\u6a21\u578b\u5728\u5b89\u5168\u6f0f\u6d1e\u65b9\u9762\u5b58\u5728\u7684\u6709\u8da3\u504f\u5dee\u3002", "motivation": "\u9a8c\u8bc1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u4fe1\u4efb\u6743\u5a01\u4fe1\u606f\u6765\u6e90\uff08\u5982\u5b66\u672f\u8bba\u6587\uff09\u65b9\u9762\u53ef\u80fd\u5b58\u5728\u7684\u65b0\u7684\u6f5c\u5728\u6f0f\u6d1e\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201cPaper Summary Attack\u201d\uff08PSA\uff09\u7684\u65b0\u9896\u8d8a\u72f1\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u7cfb\u7edf\u5730\u5408\u6210\u653b\u51fb\u6216\u9632\u5fa1\u7126\u70b9\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5b89\u5168\u8bba\u6587\u5185\u5bb9\uff0c\u5e76\u7b56\u7565\u6027\u5730\u5c06\u6709\u5bb3\u67e5\u8be2\u4f5c\u4e3a\u5bf9\u6297\u6027\u8d1f\u8f7d\u586b\u5145\u5230\u9884\u5b9a\u4e49\u7684\u5b50\u90e8\u5206\u4e2d\uff0c\u4ece\u800c\u6784\u5efa\u5bf9\u6297\u6027\u63d0\u793a\u6a21\u677f\u3002", "result": "PSA\u5728\u57fa\u7840LLM\u548c\u5148\u8fdb\u63a8\u7406\u6a21\u578b\uff08\u5982Deepseek-R1\uff09\u4e2d\u5747\u53d1\u73b0\u4e86\u663e\u8457\u6f0f\u6d1e\u3002PSA\u5728Claude3.5-Sonnet\u4e0a\u7684\u653b\u51fb\u6210\u529f\u7387\uff08ASR\uff09\u4e3a97%\uff0c\u5728Deepseek-R1\u4e0a\u4e3a98%\u3002\u7814\u7a76\u8fd8\u63ed\u793a\u4e86\u6a21\u578b\u5728\u9762\u5bf9\u653b\u51fb\u6027\u6216\u9632\u5fa1\u6027\u8bba\u6587\u65f6\uff0c\u5728\u4e0d\u540c\u6a21\u578b\u53ca\u540c\u4e00\u6a21\u578b\u4e0d\u540c\u7248\u672c\u4e4b\u95f4\u5b58\u5728\u622a\u7136\u76f8\u53cd\u7684\u6f0f\u6d1e\u504f\u5dee\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u9762\u5bf9\u6765\u81ea\u5b66\u672f\u8bba\u6587\u7b49\u6743\u5a01\u6765\u6e90\u7684\u4fe1\u606f\u65f6\uff0c\u53ef\u80fd\u5b58\u5728\u65b0\u7684\u6f5c\u5728\u6f0f\u6d1e\u3002\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201cPaper Summary Attack\u201d\uff08PSA\uff09\u7684\u65b0\u9896\u8d8a\u72f1\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u7cfb\u7edf\u5730\u5408\u6210\u653b\u51fb\u6216\u9632\u5fa1\u7126\u70b9\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5b89\u5168\u8bba\u6587\u5185\u5bb9\uff0c\u5e76\u7b56\u7565\u6027\u5730\u5c06\u6709\u5bb3\u67e5\u8be2\u4f5c\u4e3a\u5bf9\u6297\u6027\u8d1f\u8f7d\u586b\u5145\u5230\u9884\u5b9a\u4e49\u7684\u5b50\u90e8\u5206\u4e2d\uff0c\u4ece\u800c\u6784\u5efa\u5bf9\u6297\u6027\u63d0\u793a\u6a21\u677f\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0cPSA\u4e0d\u4ec5\u66b4\u9732\u4e86\u57fa\u7840\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6f0f\u6d1e\uff0c\u8fd8\u6210\u529f\u653b\u51fb\u4e86\u50cfDeepseek-R1\u8fd9\u6837\u7684\u5148\u8fdb\u63a8\u7406\u6a21\u578b\uff0c\u5728\u7ecf\u8fc7\u826f\u597d\u5bf9\u9f50\u7684\u6a21\u578b\uff08\u5982Claude3.5-Sonnet\uff09\u4e0a\u5b9e\u73b0\u4e8697%\u7684\u653b\u51fb\u6210\u529f\u7387\uff0c\u5728Deepseek-R1\u4e0a\u751a\u81f3\u8fbe\u5230\u4e8698%\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u53d1\u73b0\u6a21\u578b\u5728\u66b4\u9732\u4e8e\u653b\u51fb\u6027\u6216\u9632\u5fa1\u6027\u8bba\u6587\u65f6\uff0c\u5728\u4e0d\u540c\u57fa\u7840\u6a21\u578b\u4e4b\u95f4\u4ee5\u53ca\u540c\u4e00\u6a21\u578b\u7684\u4e0d\u540c\u7248\u672c\u4e4b\u95f4\u5b58\u5728\u622a\u7136\u76f8\u53cd\u7684\u6f0f\u6d1e\u504f\u5dee\uff0c\u8fd9\u4e3a\u672a\u6765\u7684\u5bf9\u6297\u6027\u65b9\u6cd5\u548c\u5b89\u5168\u5bf9\u9f50\u7814\u7a76\u63d0\u4f9b\u4e86\u7ebf\u7d22\u3002"}}
{"id": "2507.14111", "categories": ["cs.AI", "cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14111", "abs": "https://arxiv.org/abs/2507.14111", "authors": ["Xiaoya Li", "Xiaofei Sun", "Albert Wang", "Jiwei Li", "Chris Shum"], "title": "CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement Learning", "comment": "Preprint Version", "summary": "The exponential growth in demand for GPU computing resources, driven by the\nrapid advancement of Large Language Models, has created an urgent need for\nautomated CUDA optimization strategies. While recent advances in LLMs show\npromise for code generation, current SOTA models (e.g. R1, o1) achieve low\nsuccess rates in improving CUDA speed. In this paper, we introduce CUDA-L1, an\nautomated reinforcement learning framework for CUDA optimization.\n  CUDA-L1 achieves performance improvements on the CUDA optimization task:\ntrained on NVIDIA A100, it delivers an average speedup of x17.7 across all 250\nCUDA kernels of KernelBench, with peak speedups reaching x449. Furthermore, the\nmodel also demonstrates excellent portability across GPU architectures,\nachieving average speedups of x17.8 on H100, x19.0 on RTX 3090, x16.5 on L40,\nx14.7 on H800, and x13.9 on H20 despite being optimized specifically for A100.\nBeyond these benchmark results, CUDA-L1 demonstrates several remarkable\nproperties: 1) Discovers a variety of CUDA optimization techniques and learns\nto combine them strategically to achieve optimal performance; 2) Uncovers\nfundamental principles of CUDA optimization; 3) Identifies non-obvious\nperformance bottlenecks and rejects seemingly beneficial optimizations that\nharm performance.\n  The capabilities of CUDA-L1 demonstrate that reinforcement learning can\ntransform an initially poor-performing LLM into an effective CUDA optimizer\nthrough speedup-based reward signals alone, without human expertise or domain\nknowledge. More importantly, the trained RL model extend the acquired reasoning\nabilities to new kernels. This paradigm opens possibilities for automated\noptimization of CUDA operations, and holds promise to substantially promote GPU\nefficiency and alleviate the rising pressure on GPU computing resources.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86CUDA-L1\uff0c\u4e00\u4e2a\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u81ea\u52a8\u4f18\u5316CUDA\u5185\u6838\u7684\u6846\u67b6\uff0c\u5728\u5404\u79cdGPU\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u8de8\u5185\u6838\u7684\u6cdb\u5316\u80fd\u529b\u548c\u53d1\u73b0\u65b0\u4f18\u5316\u6280\u672f\u7684\u6f5c\u529b\u3002", "motivation": "\u7531\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u5bf9GPU\u8ba1\u7b97\u8d44\u6e90\u7684\u9700\u6c42\u5448\u6307\u6570\u7ea7\u589e\u957f\uff0c\u56e0\u6b64\u8feb\u5207\u9700\u8981\u81ea\u52a8\u5316\u7684CUDA\u4f18\u5316\u7b56\u7565\u3002\u5c3d\u7ba1\u6700\u8fd1LLM\u5728\u4ee3\u7801\u751f\u6210\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u76ee\u524d\u7684SOTA\u6a21\u578b\u5728\u63d0\u9ad8CUDA\u901f\u5ea6\u65b9\u9762\u6210\u529f\u7387\u8f83\u4f4e\u3002", "method": "\u672c\u7814\u7a76\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3aCUDA-L1\u7684\u81ea\u52a8\u5316\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8eCUDA\u4f18\u5316\u3002", "result": "CUDA-L1\u5728CUDA\u4f18\u5316\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u6027\u80fd\u6539\u8fdb\u3002\u5728NVIDIA A100\u4e0a\u8bad\u7ec3\u540e\uff0c\u8be5\u6a21\u578b\u5728KernelBench\u7684250\u4e2aCUDA\u5185\u6838\u4e0a\u5e73\u5747\u5b9e\u73b0\u4e8617.7\u500d\u7684\u52a0\u901f\uff0c\u5cf0\u503c\u52a0\u901f\u8fbe\u5230449\u500d\u3002\u6b64\u5916\uff0c\u8be5\u6a21\u578b\u5728GPU\u67b6\u6784\u4e4b\u95f4\u8868\u73b0\u51fa\u4f18\u826f\u7684\u53ef\u79fb\u690d\u6027\uff0c\u5728H100\u4e0a\u5b9e\u73b0\u4e8617.8\u500d\u7684\u5e73\u5747\u52a0\u901f\uff0c\u5728RTX 3090\u4e0a\u5b9e\u73b0\u4e8619.0\u500d\uff0c\u5728L40\u4e0a\u5b9e\u73b0\u4e8616.5\u500d\uff0c\u5728H800\u4e0a\u5b9e\u73b0\u4e8614.7\u500d\uff0c\u5728H20\u4e0a\u5b9e\u73b0\u4e8613.9\u500d\uff0c\u5c3d\u7ba1\u5b83\u662f\u4e13\u95e8\u4e3aA100\u4f18\u5316\u7684\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5982\u4f55\u4ec5\u901a\u8fc7\u57fa\u4e8e\u52a0\u901f\u7684\u5956\u52b1\u4fe1\u53f7\uff0c\u5728\u6ca1\u6709\u4eba\u7c7b\u4e13\u4e1a\u77e5\u8bc6\u6216\u9886\u57df\u77e5\u8bc6\u7684\u60c5\u51b5\u4e0b\uff0c\u5c06\u6700\u521d\u6027\u80fd\u4e0d\u4f73\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8f6c\u53d8\u4e3a\u6709\u6548\u7684CUDA\u4f18\u5316\u5668\u3002\u8bad\u7ec3\u597d\u7684RL\u6a21\u578b\u80fd\u591f\u5c06\u83b7\u5f97\u7684\u63a8\u7406\u80fd\u529b\u6269\u5c55\u5230\u65b0\u7684\u5185\u6838\uff0c\u4e3aCUDA\u64cd\u4f5c\u7684\u81ea\u52a8\u5316\u4f18\u5316\u5f00\u8f9f\u4e86\u53ef\u80fd\u6027\uff0c\u5e76\u6709\u671b\u663e\u8457\u63d0\u9ad8GPU\u6548\u7387\u548c\u7f13\u89e3GPU\u8ba1\u7b97\u8d44\u6e90\u65e5\u76ca\u589e\u957f\u7684\u538b\u529b\u3002"}}
{"id": "2507.13482", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13482", "abs": "https://arxiv.org/abs/2507.13482", "authors": ["Seyyed Saeid Cheshmi", "Buyao Lyu", "Thomas Lisko", "Rajesh Rajamani", "Robert A. McGovern", "Yogatheesan Varatharajah"], "title": "Improving Out-of-distribution Human Activity Recognition via IMU-Video Cross-modal Representation Learning", "comment": null, "summary": "Human Activity Recognition (HAR) based on wearable inertial sensors plays a\ncritical role in remote health monitoring. In patients with movement disorders,\nthe ability to detect abnormal patient movements in their home environments can\nenable continuous optimization of treatments and help alert caretakers as\nneeded. Machine learning approaches have been proposed for HAR tasks using\nInertial Measurement Unit (IMU) data; however, most rely on\napplication-specific labels and lack generalizability to data collected in\ndifferent environments or populations. To address this limitation, we propose a\nnew cross-modal self-supervised pretraining approach to learn representations\nfrom large-sale unlabeled IMU-video data and demonstrate improved\ngeneralizability in HAR tasks on out of distribution (OOD) IMU datasets,\nincluding a dataset collected from patients with Parkinson's disease.\nSpecifically, our results indicate that the proposed cross-modal pretraining\napproach outperforms the current state-of-the-art IMU-video pretraining\napproach and IMU-only pretraining under zero-shot and few-shot evaluations.\nBroadly, our study provides evidence that in highly dynamic data modalities,\nsuch as IMU signals, cross-modal pretraining may be a useful tool to learn\ngeneralizable data representations. Our software is available at\nhttps://github.com/scheshmi/IMU-Video-OOD-HAR.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8de8\u6a21\u6001\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u4f7f\u7528IMU-\u89c6\u9891\u6570\u636e\u6765\u63d0\u9ad8\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\uff08HAR\uff09\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u9762\u5bf9\u4e0d\u540c\u73af\u5883\u6216\u4eba\u7fa4\u7684\u6570\u636e\u65f6\u3002\u8be5\u65b9\u6cd5\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u8bc4\u4f30\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u4e3a\u5904\u7406\u52a8\u6001\u6570\u636e\u6a21\u5f0f\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u3002", "motivation": "\u5927\u591a\u6570\u57fa\u4e8eIMU\u6570\u636e\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5728HAR\u4efb\u52a1\u4e2d\u4f9d\u8d56\u4e8e\u7279\u5b9a\u5e94\u7528\u7684\u6807\u7b7e\uff0c\u7f3a\u4e4f\u6cdb\u5316\u80fd\u529b\uff0c\u65e0\u6cd5\u5728\u4e0d\u540c\u73af\u5883\u6216\u4eba\u7fa4\u7684\u6570\u636e\u4e2d\u8fdb\u884c\u5e94\u7528\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8de8\u6a21\u6001\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5229\u7528\u5927\u89c4\u6a21\u672a\u6807\u8bb0\u7684IMU-\u89c6\u9891\u6570\u636e\u5b66\u4e60\u8868\u793a\uff0c\u5e76\u8bc1\u660e\u4e86\u5728\u4e0d\u540c\u73af\u5883\u6216\u4eba\u7fa4\u7684IMU\u6570\u636e\u96c6\uff08\u5305\u62ec\u5e15\u91d1\u68ee\u75c5\u60a3\u8005\u6570\u636e\u96c6\uff09\u4e0a\u8fdb\u884cHAR\u4efb\u52a1\u7684\u6cdb\u5316\u80fd\u529b\u5f97\u5230\u4e86\u6539\u5584\u3002", "result": "\u6240\u63d0\u51fa\u7684\u8de8\u6a21\u6001\u9884\u8bad\u7ec3\u65b9\u6cd5\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u8bc4\u4f30\u4e2d\u4f18\u4e8e\u6700\u5148\u8fdb\u7684IMU-\u89c6\u9891\u9884\u8bad\u7ec3\u65b9\u6cd5\u548c\u4ec5IMU\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5e76\u4e14\u5728\u5305\u62ec\u5e15\u91d1\u68ee\u75c5\u60a3\u8005\u5728\u5185\u7684OOD IMU\u6570\u636e\u96c6\u4e0a\uff0cHAR\u4efb\u52a1\u7684\u6cdb\u5316\u80fd\u529b\u5f97\u5230\u4e86\u6539\u5584\u3002", "conclusion": "\u63d0\u51fa\u7684\u8de8\u6a21\u6001\u9884\u8bad\u7ec3\u65b9\u6cd5\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u8bc4\u4f30\u4e2d\u4f18\u4e8e\u6700\u5148\u8fdb\u7684IMU-\u89c6\u9891\u9884\u8bad\u7ec3\u65b9\u6cd5\u548c\u4ec5IMU\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5e76\u8bc1\u660e\u4e86\u5728\u52a8\u6001\u6570\u636e\u6a21\u5f0f\uff08\u5982IMU\u4fe1\u53f7\uff09\u4e2d\uff0c\u8de8\u6a21\u6001\u9884\u8bad\u7ec3\u53ef\u7528\u4e8e\u5b66\u4e60\u53ef\u6cdb\u5316\u7684\u6570\u636e\u8868\u793a\u3002"}}
{"id": "2507.13938", "categories": ["eess.SP", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.13938", "abs": "https://arxiv.org/abs/2507.13938", "authors": ["Hyun Seok Lee"], "title": "Device-Free Localization Using Commercial UWB Transceivers", "comment": "8 pages, 10 figures, preprint", "summary": "Recently, commercial ultra-wideband (UWB) transceivers have enabled not only\nmeasuring device-to-device distance but also tracking the position of a\npedestrian who does not carry a UWB device. UWB-based device-free localization\nthat does not require dedicated radar equipment is compatible with existing\nanchor infrastructure and can be reused to reduce hardware deployment costs.\nHowever, it is difficult to estimate the target's position accurately in\nreal-world scenarios due to the low signal-to-noise ratio (SNR) and the\ncluttered environment. In this paper, we propose a deep learning (DL)-assisted\nparticle filter to overcome these challenges. First, the channel impulse\nresponse (CIR) variance is analyzed to capture the variability induced by the\ntarget's movement. Then, a DL-based one-dimensional attention U-Net is used to\nextract only the reflection components caused by the target and suppress the\nnoise components within the CIR variance profile. Finally, multiple\npreprocessed CIR variance profiles are used as input to a particle filter to\nestimate the target's position. Experimental results demonstrate that the\nproposed system is a practical and cost-effective solution for IoT and\nautomotive applications with a root mean square error (RMSE) of about 15 cm and\nan average processing time of 4 ms. Furthermore, comparisons with existing\nstate-of-the-art methods show that the proposed method provides the best\nperformance with reasonable computational costs.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6df1\u5ea6\u5b66\u4e60\u8f85\u52a9\u7c92\u5b50\u6ee4\u6ce2\u5668\uff0c\u7528\u4e8e\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u73b0\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\uff0c\u901a\u8fc7UWB\u4fe1\u53f7\u5bf9\u975eUWB\u8bbe\u5907\u8fdb\u884c\u5b9a\u4f4d\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5206\u6790CIR\u65b9\u5dee\u5e76\u5229\u7528\u6ce8\u610f\u529bU-Net\u63d0\u53d6\u76ee\u6807\u76f8\u5173\u4fe1\u606f\u6765\u6291\u5236\u566a\u58f0\u548c\u6742\u6ce2\uff0c\u4ece\u800c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\uff08\u7ea615\u5398\u7c73RMSE\uff09\u548c\u4f4e\u5ef6\u8fdf\uff084\u6beb\u79d2\uff09\u7684\u5b9a\u4f4d\uff0c\u9002\u7528\u4e8e\u7269\u8054\u7f51\u548c\u6c7d\u8f66\u5e94\u7528\u3002", "motivation": "\u5c3d\u7ba1\u8d85\u5bbd\u5e26\uff08UWB\uff09\u6536\u53d1\u5668\u5df2\u5b9e\u73b0\u8bbe\u5907\u65e0\u5173\u5b9a\u4f4d\uff0c\u4f46\u5728\u73b0\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\uff0c\u7531\u4e8e\u4fe1\u566a\u6bd4\uff08SNR\uff09\u4f4e\u548c\u73af\u5883\u6df7\u4e71\uff0c\u51c6\u786e\u4f30\u8ba1\u76ee\u6807\u4f4d\u7f6e\u4ecd\u7136\u5f88\u56f0\u96be\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df1\u5ea6\u5b66\u4e60\uff08DL\uff09\u8f85\u52a9\u7684\u7c92\u5b50\u6ee4\u6ce2\u5668\u3002\u9996\u5148\uff0c\u5206\u6790\u4e86\u4fe1\u9053\u8109\u51b2\u54cd\u5e94\uff08CIR\uff09\u65b9\u5dee\u4ee5\u6355\u6349\u7531\u76ee\u6807\u8fd0\u52a8\u5f15\u8d77\u7684\u53d8\u5f02\u6027\u3002\u7136\u540e\uff0c\u4f7f\u7528\u57fa\u4e8eDL\u7684\u4e00\u7ef4\u6ce8\u610f\u529bU-Net\u6765\u63d0\u53d6\u7531\u76ee\u6807\u5f15\u8d77\u7684\u53cd\u5c04\u5206\u91cf\u5e76\u6291\u5236CIR\u65b9\u5dee\u5256\u9762\u5185\u7684\u566a\u58f0\u5206\u91cf\u3002\u6700\u540e\uff0c\u5c06\u591a\u4e2a\u9884\u5904\u7406\u7684CIR\u65b9\u5dee\u5256\u9762\u4f5c\u4e3a\u7c92\u5b50\u6ee4\u6ce2\u5668\u7684\u8f93\u5165\u6765\u4f30\u8ba1\u76ee\u6807\u4f4d\u7f6e\u3002", "result": "\u6240\u63d0\u51fa\u7684\u7cfb\u7edf\u5728\u73b0\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86\u7ea615\u5398\u7c73\u7684\u5747\u65b9\u6839\u8bef\u5dee\uff08RMSE\uff09\u548c4\u6beb\u79d2\u7684\u5e73\u5747\u5904\u7406\u65f6\u95f4\uff0c\u5e76\u4e14\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u7cfb\u7edf\u662f\u7269\u8054\u7f51\u548c\u6c7d\u8f66\u5e94\u7528\u7684\u5b9e\u7528\u4e14\u7ecf\u6d4e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5747\u65b9\u6839\u8bef\u5dee\uff08RMSE\uff09\u7ea6\u4e3a15\u5398\u7c73\uff0c\u5e73\u5747\u5904\u7406\u65f6\u95f4\u4e3a4\u6beb\u79d2\u3002\u6b64\u5916\uff0c\u4e0e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u5408\u7406\u7684\u8ba1\u7b97\u6210\u672c\u4e0b\u63d0\u4f9b\u4e86\u6700\u4f73\u6027\u80fd\u3002"}}
{"id": "2507.13374", "categories": ["cs.CV", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.13374", "abs": "https://arxiv.org/abs/2507.13374", "authors": ["Kevin Dela Rosa"], "title": "Smart Routing for Multimodal Video Retrieval: When to Search What", "comment": "Accepted to ICCV 2025 Multimodal Representation and Retrieval\n  Workshop", "summary": "We introduce ModaRoute, an LLM-based intelligent routing system that\ndynamically selects optimal modalities for multimodal video retrieval. While\ndense text captions can achieve 75.9% Recall@5, they require expensive offline\nprocessing and miss critical visual information present in 34% of clips with\nscene text not captured by ASR. By analyzing query intent and predicting\ninformation needs, ModaRoute reduces computational overhead by 41% while\nachieving 60.9% Recall@5. Our approach uses GPT-4.1 to route queries across ASR\n(speech), OCR (text), and visual indices, averaging 1.78 modalities per query\nversus exhaustive 3.0 modality search. Evaluation on 1.8M video clips\ndemonstrates that intelligent routing provides a practical solution for scaling\nmultimodal retrieval systems, reducing infrastructure costs while maintaining\ncompetitive effectiveness for real-world deployment.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.13768", "categories": ["cs.AI", "I.2.7"], "pdf": "https://arxiv.org/pdf/2507.13768", "abs": "https://arxiv.org/abs/2507.13768", "authors": ["Renato Ghisellini", "Remo Pareschi", "Marco Pedroni", "Giovanni Battista Raggi"], "title": "From Extraction to Synthesis: Entangled Heuristics for Agent-Augmented Strategic Reasoning", "comment": "Peer-reviewed full paper accepted through a double-blind review\n  process at the HAR 2025 conference (https://har-conf.eu/). The official\n  version will appear in a volume of the Lecture Notes in Computer Science\n  (LNCS) series", "summary": "We present a hybrid architecture for agent-augmented strategic reasoning,\ncombining heuristic extraction, semantic activation, and compositional\nsynthesis. Drawing on sources ranging from classical military theory to\ncontemporary corporate strategy, our model activates and composes multiple\nheuristics through a process of semantic interdependence inspired by research\nin quantum cognition. Unlike traditional decision engines that select the best\nrule, our system fuses conflicting heuristics into coherent and\ncontext-sensitive narratives, guided by semantic interaction modeling and\nrhetorical framing. We demonstrate the framework via a Meta vs. FTC case study,\nwith preliminary validation through semantic metrics. Limitations and\nextensions (e.g., dynamic interference tuning) are discussed.", "AI": {"tldr": "A new hybrid architecture for strategic reasoning combines heuristics using semantic interdependence and rhetorical framing to create context-sensitive narratives, demonstrated with a Meta vs. FTC case study.", "motivation": "To develop an agent-augmented strategic reasoning system that fuses conflicting heuristics into coherent and context-sensitive narratives, unlike traditional decision engines that select the best rule.", "method": "A hybrid architecture combining heuristic extraction, semantic activation, and compositional synthesis, inspired by quantum cognition and utilizing semantic interaction modeling and rhetorical framing.", "result": "The system fuses conflicting heuristics into coherent and context-sensitive narratives, demonstrated via a Meta vs. FTC case study with preliminary validation through semantic metrics.", "conclusion": "The framework is demonstrated via a Meta vs. FTC case study, with preliminary validation through semantic metrics. Limitations and extensions are discussed."}}
{"id": "2507.13982", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.13982", "abs": "https://arxiv.org/abs/2507.13982", "authors": ["Yanni Jiwan-Mercier", "Bar\u0131\u015f D\u00f6nmez", "G\u00fcne\u015f Karabulut-Kurt", "S\u00e9bastien Loranger"], "title": "Diffraction and Scattering Modeling for Laser Power Beaming in Lunar Environment", "comment": "10 pages, 8 figures", "summary": "Reliable energy delivery is a critical requirement for\n  long-term lunar missions, particularly in regions with limited\n  solar access, such as polar craters and during extended lunar\n  nights. Optical Power Beaming (OPB) using high-power lasers\n  offers a promising alternative to conventional solar power, but\n  the effects of suspended lunar dust on beam propagation remain\n  poorly understood. This study introduces a detailed simulation\n  model that incorporates both diffraction and height-dependent\n  scattering by the electrostatically suspended lunar regolith. Un like prior\napproaches, which assumed uniform dust layers or\n  center-to-center transmission loss, our model uses generalized\n  diffraction theory and refractive index gradients derived from\n  particle density to assess beam deformation and attenuation. The\n  results show that even in ground-to-ground scenarios, lunar dust\n  significantly degrades energy transfer efficiency, dropping from\n  57% to 3.7% over 50 km in dust-free vs. dusty conditions with\n  175 nm particles. Increasing the particle size to 250 nm limits the\n  viable transmission range to below 30 km at 6% efficiency. The\n  study further demonstrates that raising the laser source height\n  can improve efficiency, achieving 91% for a distance of 5 km\n  and 25% at 50 km when the source is positioned 12 m above\n  ground. These findings underscore the importance of system\n  elevation and dust modeling in lunar OPB design and reveal\n  the mission-critical role of particle size distribution, especially in\n  environments disturbed by human activity.", "AI": {"tldr": "\u6708\u7403\u5c18\u57c3\u4e25\u91cd\u5f71\u54cd\u5149\u5b66\u529f\u7387\u4f20\u8f93\u6548\u7387\uff0c\u63d0\u9ad8\u6fc0\u5149\u6e90\u9ad8\u5ea6\u53ef\u6539\u5584\u4f20\u8f93\u6548\u679c\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u6708\u7403\u6781\u5730\u9668\u77f3\u5751\u7b49\u592a\u9633\u5149\u7167\u5c04\u6709\u9650\u533a\u57df\u7684\u957f\u671f\u6708\u7403\u4efb\u52a1\u4e2d\u5bf9\u53ef\u9760\u80fd\u6e90\u8f93\u9001\u7684\u9700\u6c42\uff0c\u4ee5\u53ca\u73b0\u6709\u65b9\u6cd5\u5bf9\u6708\u7403\u5c18\u57c3\u5f71\u54cd\u7406\u89e3\u7684\u4e0d\u8db3\uff0c\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u4eff\u771f\u6a21\u578b\u8bc4\u4f30\u6708\u7403\u5c18\u57c3\u5bf9\u5149\u5b66\u529f\u7387\u4f20\u8f93\uff08OPB\uff09\u7684\u5f71\u54cd\u3002", "method": "\u8be5\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u4e2a\u8be6\u7ec6\u7684\u4eff\u771f\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u7ed3\u5408\u4e86\u884d\u5c04\u548c\u9759\u7535\u60ac\u6d6e\u7684\u6708\u7403\u8868\u5c42\u571f\u7684\u9ad8\u5ea6\u76f8\u5173\u7684\u6563\u5c04\u6548\u5e94\u3002\u4e0e\u4e4b\u524d\u5047\u8bbe\u5747\u5300\u5c18\u57c3\u5c42\u6216\u4e2d\u5fc3\u5bf9\u4e2d\u5fc3\u4f20\u8f93\u635f\u8017\u7684\u65b9\u6cd5\u4e0d\u540c\uff0c\u8be5\u6a21\u578b\u91c7\u7528\u4e86\u5e7f\u4e49\u884d\u5c04\u7406\u8bba\u548c\u6e90\u4e8e\u7c92\u5b50\u5bc6\u5ea6\u7684\u6298\u5c04\u7387\u68af\u5ea6\u6765\u8bc4\u4f30\u5149\u675f\u7578\u53d8\u548c\u8870\u51cf\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0c\u5373\u4f7f\u5728\u5730\u9762\u5230\u5730\u9762\u7684\u573a\u666f\u4e2d\uff0c\u6708\u7403\u5c18\u57c3\u4e5f\u4f1a\u663e\u8457\u964d\u4f4e\u80fd\u91cf\u4f20\u8f93\u6548\u7387\u3002\u5728\u65e0\u5c18\u6761\u4ef6\u4e0b\uff0c50\u516c\u91cc\u4f20\u8f93\u6548\u7387\u4e3a57%\uff0c\u800c\u5728\u5b58\u5728175 nm\u5c18\u57c3\u7684\u6761\u4ef6\u4e0b\uff0c\u6548\u7387\u964d\u81f33.7%\u3002\u5c06\u7c92\u5b50\u5c3a\u5bf8\u589e\u52a0\u5230250 nm\uff0c\u57286%\u7684\u6548\u7387\u4e0b\uff0c\u53ef\u884c\u7684\u4f20\u8f93\u8ddd\u79bb\u9650\u5236\u572830\u516c\u91cc\u4ee5\u5185\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8868\u660e\u63d0\u9ad8\u6fc0\u5149\u6e90\u9ad8\u5ea6\u53ef\u4ee5\u6539\u5584\u6548\u7387\uff0c\u5f53\u6e90\u9ad8\u4e3a12\u7c73\u65f6\uff0c5\u516c\u91cc\u4f20\u8f93\u6548\u7387\u53ef\u8fbe91%\uff0c50\u516c\u91cc\u4f20\u8f93\u6548\u7387\u4e3a25%\u3002", "conclusion": "\u53ef\u9760\u7684\u80fd\u6e90\u8f93\u9001\u5bf9\u4e8e\u957f\u671f\u6708\u7403\u4efb\u52a1\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u592a\u9633\u80fd\u6709\u9650\u7684\u5730\u533a\u3002\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u5149\u5b66\u529f\u7387\u4f20\u8f93\uff08OPB\uff09\u6a21\u578b\u8003\u8651\u4e86\u884d\u5c04\u548c\u9ad8\u5ea6\u76f8\u5173\u7684\u6563\u5c04\u6548\u5e94\uff0c\u8868\u660e\u6708\u7403\u5c18\u57c3\uff08\u5c24\u5176\u662f175 nm\u548c250 nm\u7684\u5c18\u57c3\uff09\u4f1a\u663e\u8457\u964d\u4f4e\u80fd\u91cf\u4f20\u8f93\u6548\u7387\u3002\u63d0\u9ad8\u6fc0\u5149\u6e90\u9ad8\u5ea6\u53ef\u4ee5\u6539\u5584\u6548\u7387\uff0c\u57285\u516c\u91cc\u5904\u8fbe\u523091%\uff0c\u572850\u516c\u91cc\u5904\u8fbe\u523025%\uff08\u5f53\u6fc0\u5149\u6e90\u9ad8\u5ea6\u4e3a12\u7c73\u65f6\uff09\u3002\u8fd9\u4e9b\u7ed3\u679c\u5f3a\u8c03\u4e86\u5728\u6708\u7403OPB\u8bbe\u8ba1\u4e2d\u8003\u8651\u7cfb\u7edf\u9ad8\u5ea6\u548c\u5c18\u57c3\u6a21\u578b\u7684\u91cd\u8981\u6027\uff0c\u7279\u522b\u662f\u8981\u5173\u6ce8\u5c18\u57c3\u7c92\u5f84\u5206\u5e03\u5bf9\u4efb\u52a1\u7684\u5173\u952e\u5f71\u54cd\u3002"}}
{"id": "2507.13729", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13729", "abs": "https://arxiv.org/abs/2507.13729", "authors": ["Yu Yao", "Salil Bhatnagar", "Markus Mazzola", "Vasileios Belagiannis", "Igor Gilitschenski", "Luigi Palmieri", "Simon Razniewski", "Marcel Hallgarten"], "title": "AGENTS-LLM: Augmentative GENeration of Challenging Traffic Scenarios with an Agentic LLM Framework", "comment": null, "summary": "Rare, yet critical, scenarios pose a significant challenge in testing and\nevaluating autonomous driving planners. Relying solely on real-world driving\nscenes requires collecting massive datasets to capture these scenarios. While\nautomatic generation of traffic scenarios appears promising, data-driven models\nrequire extensive training data and often lack fine-grained control over the\noutput. Moreover, generating novel scenarios from scratch can introduce a\ndistributional shift from the original training scenes which undermines the\nvalidity of evaluations especially for learning-based planners. To sidestep\nthis, recent work proposes to generate challenging scenarios by augmenting\noriginal scenarios from the test set. However, this involves the manual\naugmentation of scenarios by domain experts. An approach that is unable to meet\nthe demands for scale in the evaluation of self-driving systems. Therefore,\nthis paper introduces a novel LLM-agent based framework for augmenting\nreal-world traffic scenarios using natural language descriptions, addressing\nthe limitations of existing methods. A key innovation is the use of an agentic\ndesign, enabling fine-grained control over the output and maintaining high\nperformance even with smaller, cost-effective LLMs. Extensive human expert\nevaluation demonstrates our framework's ability to accurately adhere to user\nintent, generating high quality augmented scenarios comparable to those created\nmanually.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u4ee3\u7406\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u589e\u5f3a\u771f\u5b9e\u4e16\u754c\u7684\u4ea4\u901a\u573a\u666f\u3002\u4e0e\u73b0\u6709\u65b9\u6cd5\u4e0d\u540c\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u5b9e\u73b0\u5bf9\u573a\u666f\u7684\u7ec6\u7c92\u5ea6\u63a7\u5236\uff0c\u5373\u4f7f\u5728\u5c0f\u578bLLM\u4e0a\u4e5f\u80fd\u9ad8\u6548\u8fd0\u884c\u3002\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u751f\u6210\u7684\u573a\u666f\u8d28\u91cf\u4e0e\u624b\u52a8\u521b\u5efa\u7684\u76f8\u5f53\uff0c\u80fd\u591f\u6ee1\u8db3\u5927\u89c4\u6a21\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u8bc4\u4f30\u7684\u9700\u6c42\u3002", "motivation": "\u5728\u6d4b\u8bd5\u548c\u8bc4\u4f30\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u5668\u65f6\uff0c\u7f55\u89c1\u4f46\u5173\u952e\u7684\u573a\u666f\u63d0\u51fa\u4e86\u91cd\u5927\u6311\u6218\u3002\u4ec5\u4f9d\u8d56\u771f\u5b9e\u4e16\u754c\u7684\u9a7e\u9a76\u573a\u666f\u9700\u8981\u6536\u96c6\u6d77\u91cf\u6570\u636e\u96c6\u6765\u6355\u83b7\u8fd9\u4e9b\u573a\u666f\u3002\u867d\u7136\u81ea\u52a8\u751f\u6210\u4ea4\u901a\u573a\u666f\u770b\u8d77\u6765\u5f88\u6709\u524d\u666f\uff0c\u4f46\u6570\u636e\u9a71\u52a8\u7684\u6a21\u578b\u9700\u8981\u5927\u91cf\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u5e76\u4e14\u5f80\u5f80\u7f3a\u4e4f\u5bf9\u8f93\u51fa\u7684\u7ec6\u7c92\u5ea6\u63a7\u5236\u3002\u6b64\u5916\uff0c\u4ece\u5934\u5f00\u59cb\u751f\u6210\u65b0\u9896\u573a\u666f\u53ef\u80fd\u4f1a\u5bfc\u81f4\u4e0e\u539f\u59cb\u8bad\u7ec3\u573a\u666f\u7684\u5206\u5e03\u53d1\u751f\u504f\u79fb\uff0c\u4ece\u800c\u7834\u574f\u8bc4\u4f30\u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u57fa\u4e8e\u5b66\u4e60\u7684\u89c4\u5212\u5668\u3002\u4e3a\u4e86\u89c4\u907f\u8fd9\u4e00\u70b9\uff0c\u6700\u8fd1\u7684\u5de5\u4f5c\u63d0\u51fa\u901a\u8fc7\u589e\u5f3a\u6d4b\u8bd5\u96c6\u4e2d\u7684\u539f\u59cb\u573a\u666f\u6765\u751f\u6210\u5177\u6709\u6311\u6218\u6027\u7684\u573a\u666f\u3002\u7136\u800c\uff0c\u8fd9\u6d89\u53ca\u5230\u9886\u57df\u4e13\u5bb6\u5bf9\u624b\u52a8\u589e\u5f3a\u573a\u666f\u7684\u4f9d\u8d56\uff0c\u8fd9\u79cd\u65b9\u6cd5\u65e0\u6cd5\u6ee1\u8db3\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u8bc4\u4f30\u4e2d\u5bf9\u89c4\u6a21\u7684\u9700\u6c42\u3002", "method": "\u672c\u6587\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8eLLM\u4ee3\u7406\u7684\u6846\u67b6\uff0c\u5229\u7528\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u6765\u589e\u5f3a\u771f\u5b9e\u4e16\u754c\u7684\u4ea4\u901a\u573a\u666f\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002\u8be5\u6846\u67b6\u7684\u5173\u952e\u521b\u65b0\u5728\u4e8e\u5176\u4ee3\u7406\u8bbe\u8ba1\uff0c\u80fd\u591f\u5bf9\u8f93\u51fa\u8fdb\u884c\u7ec6\u7c92\u5ea6\u63a7\u5236\uff0c\u5e76\u80fd\u5728\u8f83\u5c0f\u7684\u3001\u7ecf\u6d4e\u9ad8\u6548\u7684LLM\u4e0a\u4fdd\u6301\u9ad8\u6027\u80fd\u3002", "result": "\u6846\u67b6\u80fd\u591f\u51c6\u786e\u5730\u9075\u5faa\u7528\u6237\u610f\u56fe\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u589e\u5f3a\u573a\u666f\uff0c\u53ef\u4e0e\u624b\u52a8\u521b\u5efa\u7684\u573a\u666f\u76f8\u5ab2\u7f8e\u3002", "conclusion": "LLM\u4ee3\u7406\u6846\u67b6\u80fd\u591f\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u6765\u589e\u5f3a\u771f\u5b9e\u4e16\u754c\u7684\u4ea4\u901a\u573a\u666f\uff0c\u53ef\u4ee5\u5b9e\u73b0\u5bf9\u8f93\u51fa\u7684\u7ec6\u7c92\u5ea6\u63a7\u5236\uff0c\u5e76\u4e14\u5373\u4f7f\u4f7f\u7528\u8f83\u5c0f\u7684\u3001\u5177\u6709\u6210\u672c\u6548\u76ca\u7684LLM\u4e5f\u80fd\u4fdd\u6301\u9ad8\u6027\u80fd\u3002\u5e7f\u6cdb\u7684\u4e13\u5bb6\u8bc4\u4f30\u8bc1\u660e\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u51c6\u786e\u5730\u9075\u5faa\u7528\u6237\u610f\u56fe\uff0c\u751f\u6210\u4e0e\u624b\u52a8\u521b\u5efa\u7684\u573a\u666f\u76f8\u5ab2\u7f8e\u7684\u9ad8\u8d28\u91cf\u589e\u5f3a\u573a\u666f\u3002"}}
{"id": "2507.13527", "categories": ["cs.CV", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2507.13527", "abs": "https://arxiv.org/abs/2507.13527", "authors": ["Levi Harris", "Md Jayed Hossain", "Mufan Qiu", "Ruichen Zhang", "Pingchuan Ma", "Tianlong Chen", "Jiaqi Gu", "Seth Ariel Tongay", "Umberto Celano"], "title": "SparseC-AFM: a deep learning method for fast and accurate characterization of MoS$_2$ with C-AFM", "comment": null, "summary": "The increasing use of two-dimensional (2D) materials in nanoelectronics\ndemands robust metrology techniques for electrical characterization, especially\nfor large-scale production. While atomic force microscopy (AFM) techniques like\nconductive AFM (C-AFM) offer high accuracy, they suffer from slow data\nacquisition speeds due to the raster scanning process. To address this, we\nintroduce SparseC-AFM, a deep learning model that rapidly and accurately\nreconstructs conductivity maps of 2D materials like MoS$_2$ from sparse C-AFM\nscans. Our approach is robust across various scanning modes, substrates, and\nexperimental conditions. We report a comparison between (a) classic flow\nimplementation, where a high pixel density C-AFM image (e.g., 15 minutes to\ncollect) is manually parsed to extract relevant material parameters, and (b)\nour SparseC-AFM method, which achieves the same operation using data that\nrequires substantially less acquisition time (e.g., under 5 minutes).\nSparseC-AFM enables efficient extraction of critical material parameters in\nMoS$_2$, including film coverage, defect density, and identification of\ncrystalline island boundaries, edges, and cracks. We achieve over 11x reduction\nin acquisition time compared to manual extraction from a full-resolution C-AFM\nimage. Moreover, we demonstrate that our model-predicted samples exhibit\nremarkably similar electrical properties to full-resolution data gathered using\nclassic-flow scanning. This work represents a significant step toward\ntranslating AI-assisted 2D material characterization from laboratory research\nto industrial fabrication. Code and model weights are available at\ngithub.com/UNITES-Lab/sparse-cafm.", "AI": {"tldr": " a", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u4e8c\u7ef4\u6750\u6599\u5728\u7eb3\u7c73\u7535\u5b50\u5b66\u5e94\u7528\u4e2d\uff0c\u4f20\u7edfAFM\u6280\u672f\uff08\u5982C-AFM\uff09\u6570\u636e\u91c7\u96c6\u901f\u5ea6\u6162\u7684\u95ee\u9898\uff0c\u5e76\u6ee1\u8db3\u5927\u89c4\u6a21\u751f\u4ea7\u5bf9\u9ad8\u6548\u7535\u5b66\u8868\u5f81\u7684\u9700\u6c42\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSparseC-AFM\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u901a\u8fc7\u5bf9\u7a00\u758f\u7684\u5bfc\u7535\u539f\u5b50\u529b\u663e\u5fae\u955c\uff08C-AFM\uff09\u626b\u63cf\u6570\u636e\u8fdb\u884c\u5904\u7406\uff0c\u80fd\u591f\u5feb\u901f\u51c6\u786e\u5730\u91cd\u5efa\u4e8c\u7ef4\u6750\u6599\u7684\u7535\u5bfc\u7387\u56fe\u3002", "result": "SparseC-AFM\u80fd\u591f\u4ece\u7a00\u758f\u626b\u63cf\u6570\u636e\u4e2d\u91cd\u5efa\u51fa\u4e0e\u5168\u5206\u8fa8\u7387\u6570\u636e\u5177\u6709\u76f8\u4f3c\u7535\u5b66\u6027\u8d28\u7684\u7535\u5bfc\u7387\u56fe\uff0c\u6570\u636e\u91c7\u96c6\u65f6\u95f4\u51cf\u5c11\u8d85\u8fc711\u500d\uff0c\u5e76\u80fd\u6709\u6548\u63d0\u53d6MoS2\u7684\u8584\u819c\u8986\u76d6\u7387\u3001\u7f3a\u9677\u5bc6\u5ea6\u3001\u6676\u6001\u5c9b\u8fb9\u754c\u3001\u8fb9\u7f18\u548c\u88c2\u7eb9\u7b49\u5173\u952e\u6750\u6599\u53c2\u6570\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86SparseC-AFM\u5728\u4e8c\u7ef4\u6750\u6599\uff08\u5982MoS2\uff09\u7684\u7535\u5b66\u6027\u8d28\u8868\u5f81\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u80fd\u591f\u4ece\u7a00\u758f\u7684C-AFM\u626b\u63cf\u4e2d\u5feb\u901f\u51c6\u786e\u5730\u91cd\u5efa\u7535\u5bfc\u7387\u56fe\uff0c\u4ece\u800c\u5728\u4fdd\u8bc1\u7cbe\u5ea6\u7684\u524d\u63d0\u4e0b\uff0c\u5c06\u6570\u636e\u91c7\u96c6\u65f6\u95f4\u7f29\u77ed11\u500d\u4ee5\u4e0a\uff0c\u4e3a\u4e8c\u7ef4\u6750\u6599\u7684\u5de5\u4e1a\u5316\u751f\u4ea7\u548c\u8868\u5f81\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.14065", "categories": ["cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2507.14065", "abs": "https://arxiv.org/abs/2507.14065", "authors": ["Sofia Sanz", "Daniel S\u00e1nchez-Portal"], "title": "Predicting interface and spin states in armchair graphene nanoribbon junctions", "comment": null, "summary": "We present a theoretical analysis of interface states emerging at junctions\nbetween armchair graphene nanoribbons of varying widths. By exploring diverse\nwidth combinations and junction geometries, we demonstrate that predicting the\nprecise number of interface states requires considerations beyond the\ntopological classification alone; specifically, the width differences and\nbonding configuration at the interface play crucial roles. For junctions\ninvolving ribbons with small gaps, we further examine how an applied strain\naffects their topological properties and, consequently, the interface states\nformed. The spin states at these junctions are investigated using the\nmean-field Hubbard model, revealing how the magnetic behavior at the interface\ndepends on the number of localized states present. These results are summarized\nin a series of ``rules of thumb\" to predict the number of localized states and\nthe magnetic moment at the junction. Our findings contribute to understanding\nand engineering localized states in graphene-based devices, providing\nguidelines for manipulating electronic and magnetic properties through\nstructural design.", "AI": {"tldr": "Interface states in graphene nanoribbons depend on width differences and bonding, not just topology. Strain and the Hubbard model affect magnetism. Rules of thumb are provided for design.", "motivation": "To understand and predict interface states and their magnetic properties at junctions between armchair graphene nanoribbons of varying widths, going beyond simple topological classification.", "method": "Theoretical analysis using the mean-field Hubbard model to investigate spin states and magnetic behavior at junctions, considering width differences, bonding configurations, and applied strain.", "result": "Demonstrated that width differences and bonding configurations are crucial for predicting interface states, showed the effect of strain on topological properties, and revealed the dependence of magnetic behavior on localized states.", "conclusion": "The study provides rules of thumb for predicting localized states and magnetic moments at graphene nanoribbon junctions, aiding in the engineering of electronic and magnetic properties through structural design."}}
{"id": "2507.13490", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.13490", "abs": "https://arxiv.org/abs/2507.13490", "authors": ["Siqi Shen", "Mehar Singh", "Lajanugen Logeswaran", "Moontae Lee", "Honglak Lee", "Rada Mihalcea"], "title": "Revisiting LLM Value Probing Strategies: Are They Robust and Expressive?", "comment": null, "summary": "There has been extensive research on assessing the value orientation of Large\nLanguage Models (LLMs) as it can shape user experiences across demographic\ngroups. However, several challenges remain. First, while the Multiple Choice\nQuestion (MCQ) setting has been shown to be vulnerable to perturbations, there\nis no systematic comparison of probing methods for value probing. Second, it is\nunclear to what extent the probed values capture in-context information and\nreflect models' preferences for real-world actions. In this paper, we evaluate\nthe robustness and expressiveness of value representations across three widely\nused probing strategies. We use variations in prompts and options, showing that\nall methods exhibit large variances under input perturbations. We also\nintroduce two tasks studying whether the values are responsive to demographic\ncontext, and how well they align with the models' behaviors in value-related\nscenarios. We show that the demographic context has little effect on the\nfree-text generation, and the models' values only weakly correlate with their\npreference for value-based actions. Our work highlights the need for a more\ncareful examination of LLM value probing and awareness of its limitations.", "AI": {"tldr": "LLM\u7684\u4ef7\u503c\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u9c81\u68d2\u6027\u548c\u8868\u8fbe\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u4e14\u5176\u4ef7\u503c\u4e0e\u73b0\u5b9e\u884c\u4e3a\u5173\u8054\u6027\u5f31\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5728\u8bc4\u4f30LLM\u7684\u4ef7\u503c\u53d6\u5411\u65b9\u9762\u5b58\u5728\u6311\u6218\uff1a1. \u7f3a\u4e4f\u5bf9\u4ef7\u503c\u63a2\u6d4b\u65b9\u6cd5\u7684\u7cfb\u7edf\u6027\u6bd4\u8f83\uff1b2. \u4e0d\u6e05\u695a\u63a2\u6d4b\u5230\u7684\u4ef7\u503c\u5728\u591a\u5927\u7a0b\u5ea6\u4e0a\u53cd\u6620\u4e86\u6a21\u578b\u5728\u73b0\u5b9e\u4e16\u754c\u884c\u4e3a\u4e2d\u7684\u504f\u597d\u3002", "method": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u4e09\u79cd\u5e7f\u6cdb\u4f7f\u7528\u7684\u63a2\u6d4b\u7b56\u7565\u5728\u4ef7\u503c\u8868\u793a\u4e0a\u7684\u9c81\u68d2\u6027\u548c\u8868\u8fbe\u80fd\u529b\uff0c\u901a\u8fc7\u6539\u53d8\u63d0\u793a\u548c\u9009\u9879\u6765\u68c0\u6d4b\u5176\u5728\u8f93\u5165\u6270\u52a8\u4e0b\u7684\u65b9\u5dee\u3002\u6b64\u5916\uff0c\u7814\u7a76\u5f15\u5165\u4e86\u4e24\u4e2a\u4efb\u52a1\uff0c\u4ee5\u68c0\u6d4b\u4ef7\u503c\u662f\u5426\u5bf9\u4eba\u53e3\u7edf\u8ba1\u5b66\u80cc\u666f\u6709\u53cd\u5e94\uff0c\u4ee5\u53ca\u5b83\u4eec\u5728\u591a\u5927\u7a0b\u5ea6\u4e0a\u4e0e\u6a21\u578b\u5728\u4ef7\u503c\u76f8\u5173\u573a\u666f\u4e2d\u7684\u884c\u4e3a\u4e00\u81f4\u3002", "result": "\u6240\u6709\u63a2\u6d4b\u65b9\u6cd5\u5728\u8f93\u5165\u6270\u52a8\u4e0b\u90fd\u8868\u73b0\u51fa\u8f83\u5927\u7684\u65b9\u5dee\uff1b\u4eba\u53e3\u7edf\u8ba1\u5b66\u80cc\u666f\u5bf9\u81ea\u7531\u6587\u672c\u751f\u6210\u5f71\u54cd\u751a\u5fae\uff1b\u6a21\u578b\u7684\u4ef7\u503c\u4e0e\u5176\u5728\u57fa\u4e8e\u4ef7\u503c\u7684\u884c\u52a8\u4e2d\u7684\u504f\u597d\u53ea\u6709\u5fae\u5f31\u7684\u76f8\u5173\u6027\u3002", "conclusion": "LLM\u7684\u4ef7\u503c\u8bc4\u4f30\u9700\u8981\u66f4\u4ed4\u7ec6\u7684\u68c0\u67e5\u548c\u5bf9\u5176\u5c40\u9650\u6027\u7684\u8ba4\u8bc6\u3002"}}
{"id": "2507.13670", "categories": ["quant-ph", "cond-mat.stat-mech", "cs.CC", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.13670", "abs": "https://arxiv.org/abs/2507.13670", "authors": ["Shantanav Chakraborty", "Soonwon Choi", "Soumik Ghosh", "Tudor Giurgic\u0103-Tiron"], "title": "Fast computational deep thermalization", "comment": "22 pages, 1 figure", "summary": "Deep thermalization refers to the emergence of Haar-like randomness from\nquantum systems upon partial measurements. As a generalization of quantum\nthermalization, it is often associated with high complexity and entanglement.\nHere, we introduce computational deep thermalization and construct the fastest\npossible dynamics exhibiting it at infinite effective temperature. Our circuit\ndynamics produce quantum states with low entanglement in polylogarithmic depth\nthat are indistinguishable from Haar random states to any computationally\nbounded observer. Importantly, the observer is allowed to request many copies\nof the same residual state obtained from partial projective measurements on the\nstate -- this condition is beyond the standard settings of quantum\npseudorandomness, but natural for deep thermalization. In cryptographic terms,\nthese states are pseudorandom, pseudoentangled, and crucially, retain these\nproperties under local measurements. Our results demonstrate a new form of\ncomputational thermalization, where thermal-like behavior arises from\nstructured quantum states endowed with cryptographic properties, instead of\nfrom highly unstructured ensembles. The low resource complexity of preparing\nthese states suggests scalable simulations of deep thermalization using quantum\ncomputers. Our work also motivates the study of computational quantum\npseudorandomness beyond BQP observers.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u8ba1\u7b97\u6027\u6df1\u5ea6\u70ed\u5316\uff0c\u5229\u7528\u4f4e\u8d44\u6e90\u590d\u6742\u5ea6\u7684\u91cf\u5b50\u7535\u8def\u751f\u6210\u5177\u6709\u52a0\u5bc6\u5c5e\u6027\u7684\u7ed3\u6784\u5316\u91cf\u5b50\u6001\uff0c\u8fd9\u4e9b\u72b6\u6001\u5728\u90e8\u5206\u6d4b\u91cf\u540e\u80fd\u591f\u6a21\u62df Haar \u968f\u673a\u6027\uff0c\u4e3a\u91cf\u5b50\u8ba1\u7b97\u548c\u5bc6\u7801\u5b66\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002", "motivation": "\u6df1\u5ea6\u70ed\u5316\u662f\u91cf\u5b50\u7cfb\u7edf\u5728\u90e8\u5206\u6d4b\u91cf\u540e\u51fa\u73b0\u7c7b\u4f3c Haar \u968f\u673a\u6027\u7684\u73b0\u8c61\uff0c\u901a\u5e38\u4e0e\u9ad8\u590d\u6742\u6027\u548c\u7ea0\u7f20\u6027\u76f8\u5173\u3002\u672c\u7814\u7a76\u65e8\u5728\u4ecb\u7ecd\u8ba1\u7b97\u6027\u6df1\u5ea6\u70ed\u5316\u5e76\u6784\u9020\u5c55\u73b0\u6b64\u73b0\u8c61\u7684\u6700\u5feb\u52a8\u529b\u5b66\u3002", "method": "\u901a\u8fc7\u6784\u9020\u6700\u5feb\u7684\u91cf\u5b50\u7535\u8def\u52a8\u529b\u5b66\uff0c\u5728\u65e0\u9650\u6709\u6548\u6e29\u5ea6\u4e0b\u5b9e\u73b0\u8ba1\u7b97\u6027\u6df1\u5ea6\u70ed\u5316\uff0c\u751f\u6210\u5177\u6709\u4f4e\u7ea0\u7f20\u5ea6\u7684\u91cf\u5b50\u6001\uff08\u591a\u5bf9\u6570\u6df1\u5ea6\uff09\uff0c\u8fd9\u4e9b\u91cf\u5b50\u6001\u5bf9\u4e8e\u4efb\u4f55\u8ba1\u7b97\u80fd\u529b\u6709\u9650\u7684\u89c2\u5bdf\u8005\u6765\u8bf4\u90fd\u65e0\u6cd5\u4e0e Haar \u968f\u673a\u72b6\u6001\u533a\u5206\u3002", "result": "\u6240\u6784\u9020\u7684\u7535\u8def\u52a8\u529b\u5b66\u5728\u591a\u5bf9\u6570\u6df1\u5ea6\u5185\u4ea7\u751f\u4f4e\u7ea0\u7f20\u5ea6\u7684\u91cf\u5b50\u6001\uff0c\u5e76\u4e14\u5bf9\u4e8e\u8ba1\u7b97\u80fd\u529b\u53d7\u9650\u7684\u89c2\u5bdf\u8005\u6765\u8bf4\uff0c\u8fd9\u4e9b\u72b6\u6001\u4e0e Haar \u968f\u673a\u72b6\u6001\u65e0\u6cd5\u533a\u5206\u3002\u91cd\u8981\u7684\u662f\uff0c\u89c2\u5bdf\u8005\u53ef\u4ee5\u8bf7\u6c42\u5bf9\u72b6\u6001\u8fdb\u884c\u90e8\u5206\u6295\u5f71\u6d4b\u91cf\u540e\u5f97\u5230\u7684\u76f8\u540c\u6b8b\u4f59\u72b6\u6001\u7684\u591a\u4e2a\u526f\u672c\u3002\u8fd9\u4e9b\u72b6\u6001\u5728\u5bc6\u7801\u5b66\u4e0a\u662f\u4f2a\u968f\u673a\u3001\u4f2a\u7ea0\u7f20\u7684\uff0c\u5e76\u4e14\u5728\u5c40\u90e8\u6d4b\u91cf\u4e0b\u80fd\u4fdd\u6301\u8fd9\u4e9b\u6027\u8d28\u3002", "conclusion": "\u672c\u6587\u5c55\u793a\u4e86\u4e00\u79cd\u8ba1\u7b97\u6027\u6df1\u5ea6\u70ed\u5316\uff0c\u5176\u4e2d\u70ed\u5316\u884c\u4e3a\u6e90\u4e8e\u5177\u6709\u52a0\u5bc6\u5c5e\u6027\u7684\u7ed3\u6784\u5316\u91cf\u5b50\u6001\uff0c\u800c\u975e\u9ad8\u5ea6\u975e\u7ed3\u6784\u5316\u7684\u96c6\u5408\u3002\u5236\u5907\u8fd9\u4e9b\u72b6\u6001\u7684\u4f4e\u8d44\u6e90\u590d\u6742\u5ea6\u8868\u660e\u53ef\u4ee5\u4f7f\u7528\u91cf\u5b50\u8ba1\u7b97\u673a\u5bf9\u6df1\u5ea6\u70ed\u5316\u8fdb\u884c\u53ef\u6269\u5c55\u6a21\u62df\u3002\u672c\u7814\u7a76\u8fd8\u4fc3\u8fdb\u4e86\u5bf9\u8d85\u8d8a BQP \u89c2\u5bdf\u8005\u7684\u8ba1\u7b97\u6027\u91cf\u5b50\u4f2a\u968f\u673a\u6027\u7684\u7814\u7a76\u3002"}}
{"id": "2507.13491", "categories": ["cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.13491", "abs": "https://arxiv.org/abs/2507.13491", "authors": ["Thomas Banker", "Ali Mesbah"], "title": "Model-free Reinforcement Learning for Model-based Control: Towards Safe, Interpretable and Sample-efficient Agents", "comment": null, "summary": "Training sophisticated agents for optimal decision-making under uncertainty\nhas been key to the rapid development of modern autonomous systems across\nfields. Notably, model-free reinforcement learning (RL) has enabled\ndecision-making agents to improve their performance directly through system\ninteractions, with minimal prior knowledge about the system. Yet, model-free RL\nhas generally relied on agents equipped with deep neural network function\napproximators, appealing to the networks' expressivity to capture the agent's\npolicy and value function for complex systems. However, neural networks amplify\nthe issues of sample inefficiency, unsafe learning, and limited\ninterpretability in model-free RL. To this end, this work introduces\nmodel-based agents as a compelling alternative for control policy\napproximation, leveraging adaptable models of system dynamics, cost, and\nconstraints for safe policy learning. These models can encode prior system\nknowledge to inform, constrain, and aid in explaining the agent's decisions,\nwhile deficiencies due to model mismatch can be remedied with model-free RL. We\noutline the benefits and challenges of learning model-based agents --\nexemplified by model predictive control -- and detail the primary learning\napproaches: Bayesian optimization, policy search RL, and offline strategies,\nalong with their respective strengths. While model-free RL has long been\nestablished, its interplay with model-based agents remains largely unexplored,\nmotivating our perspective on their combined potentials for sample-efficient\nlearning of safe and interpretable decision-making agents.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u6a21\u578b\u5b66\u4e60\u667a\u80fd\u4f53\u662f\u4e00\u79cd\u6709\u524d\u666f\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u53ef\u89e3\u51b3\u65e0\u6a21\u578b\u65b9\u6cd5\u4e2d\u7684\u6837\u672c\u6548\u7387\u4f4e\u3001\u4e0d\u5b89\u5168\u548c\u53ef\u89e3\u91ca\u6027\u5dee\u7684\u95ee\u9898\u3002\u6a21\u578b\u5b66\u4e60\u667a\u80fd\u4f53\u5229\u7528\u7cfb\u7edf\u6a21\u578b\u8fdb\u884c\u5b66\u4e60\uff0c\u5e76\u53ef\u4ee5\u901a\u8fc7\u65e0\u6a21\u578b\u65b9\u6cd5\u8fdb\u884c\u6539\u8fdb\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u6548\u3001\u5b89\u5168\u548c\u53ef\u89e3\u91ca\u7684\u51b3\u7b56\u3002", "motivation": "\u5f53\u524d\u65e0\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u65b9\u6cd5\u867d\u7136\u5728\u81ea\u4e3b\u7cfb\u7edf\u4e2d\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5176\u5bf9\u795e\u7ecf\u7f51\u7edc\u7684\u4f9d\u8d56\u52a0\u5267\u4e86\u6837\u672c\u6548\u7387\u4f4e\u3001\u5b66\u4e60\u4e0d\u5b89\u5168\u548c\u53ef\u89e3\u91ca\u6027\u6709\u9650\u7b49\u95ee\u9898\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u6a21\u578b\u5b66\u4e60\u667a\u80fd\u4f53\u4f5c\u4e3a\u4e00\u79cd\u6709\u524d\u666f\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u672c\u6587\u63a2\u8ba8\u4e86\u6a21\u578b\u5b66\u4e60\u667a\u80fd\u4f53\u4f5c\u4e3a\u4e00\u79cd\u66ff\u4ee3\u65b9\u6848\uff0c\u7528\u4e8e\u7b56\u7565\u8fd1\u4f3c\uff0c\u5229\u7528\u4e86\u7cfb\u7edf\u52a8\u529b\u5b66\u3001\u6210\u672c\u548c\u7ea6\u675f\u7684\u9002\u5e94\u6027\u6a21\u578b\u3002\u4e3b\u8981\u5b66\u4e60\u65b9\u6cd5\u5305\u62ec\u8d1d\u53f6\u65af\u4f18\u5316\u3001\u7b56\u7565\u641c\u7d22\u5f3a\u5316\u5b66\u4e60\u548c\u79bb\u7ebf\u7b56\u7565\u3002", "result": "\u6a21\u578b\u5b66\u4e60\u667a\u80fd\u4f53\u5229\u7528\u7cfb\u7edf\u52a8\u529b\u5b66\u3001\u6210\u672c\u548c\u7ea6\u675f\u7684\u6a21\u578b\uff0c\u80fd\u591f\u7eb3\u5165\u5148\u9a8c\u77e5\u8bc6\uff0c\u4ece\u800c\u6709\u52a9\u4e8e\u89e3\u91ca\u667a\u80fd\u4f53\u7684\u51b3\u7b56\u3002\u540c\u65f6\uff0c\u6a21\u578b\u4e0d\u5339\u914d\u5bfc\u81f4\u7684\u4e0d\u8db3\u53ef\u4ee5\u901a\u8fc7\u65e0\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u6765\u5f25\u8865\u3002\u6587\u7ae0\u8fd8\u8ba8\u8bba\u4e86\u6a21\u578b\u5b66\u4e60\u667a\u80fd\u4f53\u7684\u4f18\u70b9\u548c\u6311\u6218\uff0c\u5e76\u8be6\u7ec6\u4ecb\u7ecd\u4e86\u8d1d\u53f6\u65af\u4f18\u5316\u3001\u7b56\u7565\u641c\u7d22\u5f3a\u5316\u5b66\u4e60\u548c\u79bb\u7ebf\u7b56\u7565\u7b49\u4e3b\u8981\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "\u6a21\u578b\u5b66\u4e60\u667a\u80fd\u4f53\u4e0e\u65e0\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u7684\u7ed3\u5408\u4e3a\u9ad8\u6548\u5b66\u4e60\u5b89\u5168\u548c\u53ef\u89e3\u91ca\u7684\u51b3\u7b56\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\uff0c\u5c3d\u7ba1\u4e24\u8005\u90fd\u9762\u4e34\u5404\u81ea\u7684\u6311\u6218\uff0c\u4f46\u5b83\u4eec\u53ef\u4ee5\u4e92\u8865\u3002"}}
{"id": "2507.14018", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.14018", "abs": "https://arxiv.org/abs/2507.14018", "authors": ["Zeyuan Zhang", "Yue Xiu", "Phee Lep Yeoh", "Guangyi Liu", "Zixing Wu", "Ning Wei"], "title": "Distortion-Aware Hybrid Beamforming for Integrated Sensing and Communication", "comment": null, "summary": "This paper investigates a practical partially-connected hybrid beamforming\ntransmitter for integrated sensing and communication (ISAC) with distortion\nfrom nonlinear power amplification. For this ISAC system, we formulate a\ncommunication rate and sensing mutual information maximization problem driven\nby our distortion-aware hybrid beamforming design. To address this non-convex\nproblem, we first solve for a fully digital beamforming matrix by alternatively\nsolving three sub-problems using manifold optimization (MO) and our derived\nclosed-form solutions. The analog and digital beamforming matrices are then\nobtained through a decomposition algorithm. Numerical results demonstrate that\nthe proposed algorithm can improve overall ISAC performance compared to\ntraditional beamforming methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u5177\u6709\u975e\u7ebf\u6027\u529f\u7387\u653e\u5927\u5668\u5931\u771f\u7684 ISAC \u7cfb\u7edf\u7684\u90e8\u5206\u8fde\u63a5\u6df7\u5408\u6ce2\u675f\u6210\u5f62\u8bbe\u8ba1\uff0c\u901a\u8fc7\u4ea4\u66ff\u4f18\u5316\u548c\u5206\u89e3\u7b97\u6cd5\u6c42\u89e3\uff0c\u5e76\u5728\u6570\u503c\u4e0a\u8bc1\u660e\u4e86\u5176\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u901a\u4fe1\u901f\u7387\u548c\u611f\u77e5\u4e92\u4fe1\u606f\u7684\u6700\u5927\u5316\u95ee\u9898\uff0c\u8be5\u6587\u9488\u5bf9\u96c6\u6210\u4f20\u611f\u4e0e\u901a\u4fe1\uff08ISAC\uff09\u63d0\u51fa\u4e86\u4e00\u4e2a\u5b9e\u7528\u7684\u90e8\u5206\u8fde\u63a5\u6df7\u5408\u6ce2\u675f\u6210\u5f62\u53d1\u5c04\u5668\uff0c\u5e76\u8003\u8651\u4e86\u6765\u81ea\u975e\u7ebf\u6027\u529f\u7387\u653e\u5927\u5668\u7684\u5931\u771f\u3002", "method": "\u8be5\u65b9\u6cd5\u9996\u5148\u901a\u8fc7\u4f7f\u7528\u6d41\u5f62\u4f18\u5316\uff08MO\uff09\u548c\u6211\u4eec\u5bfc\u51fa\u7684\u95ed\u5f0f\u89e3\u6765\u6c42\u89e3\u4e09\u4e2a\u5b50\u95ee\u9898\uff0c\u7136\u540e\u901a\u8fc7\u5206\u89e3\u7b97\u6cd5\u83b7\u5f97\u6a21\u62df\u548c\u6570\u5b57\u6ce2\u675f\u6210\u5f62\u77e9\u9635\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u4f20\u7edf\u6ce2\u675f\u6210\u5f62\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u53ef\u4ee5\u6539\u5584\u6574\u4f53 ISAC \u6027\u80fd\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u53ef\u4ee5\u6539\u8fdb\u4e0e\u4f20\u7edf\u6ce2\u675f\u6210\u5f62\u65b9\u6cd5\u76f8\u6bd4\u7684\u6574\u4f53 ISAC \u6027\u80fd\u3002"}}
{"id": "2507.13378", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13378", "abs": "https://arxiv.org/abs/2507.13378", "authors": ["Yuqi Cheng", "Yunkang Cao", "Haiming Yao", "Wei Luo", "Cheng Jiang", "Hui Zhang", "Weiming Shen"], "title": "A Comprehensive Survey for Real-World Industrial Defect Detection: Challenges, Approaches, and Prospects", "comment": "27 pages, 7 figures", "summary": "Industrial defect detection is vital for upholding product quality across\ncontemporary manufacturing systems. As the expectations for precision,\nautomation, and scalability intensify, conventional inspection approaches are\nincreasingly found wanting in addressing real-world demands. Notable progress\nin computer vision and deep learning has substantially bolstered defect\ndetection capabilities across both 2D and 3D modalities. A significant\ndevelopment has been the pivot from closed-set to open-set defect detection\nframeworks, which diminishes the necessity for extensive defect annotations and\nfacilitates the recognition of novel anomalies. Despite such strides, a\ncohesive and contemporary understanding of industrial defect detection remains\nelusive. Consequently, this survey delivers an in-depth analysis of both\nclosed-set and open-set defect detection strategies within 2D and 3D\nmodalities, charting their evolution in recent years and underscoring the\nrising prominence of open-set techniques. We distill critical challenges\ninherent in practical detection environments and illuminate emerging trends,\nthereby providing a current and comprehensive vista of this swiftly progressing\nfield.", "AI": {"tldr": "\u5de5\u4e1a\u7f3a\u9677\u68c0\u6d4b\u6b63\u4ece\u4f20\u7edf\u65b9\u6cd5\u8f6c\u5411\u66f4\u5148\u8fdb\u7684\u5f00\u653e\u96c6\u8ba1\u7b97\u673a\u89c6\u89c9\u6280\u672f\uff0c\u4ee5\u63d0\u9ad8\u7cbe\u5ea6\u548c\u9002\u5e94\u6027\uff0c\u672c\u8c03\u67e5\u5bf9\u6b64\u8fdb\u884c\u4e86\u6982\u8ff0\u3002", "motivation": "\u968f\u7740\u5bf9\u7cbe\u5ea6\u3001\u81ea\u52a8\u5316\u548c\u53ef\u6269\u5c55\u6027\u671f\u671b\u7684\u63d0\u9ad8\uff0c\u4f20\u7edf\u68c0\u67e5\u65b9\u6cd5\u5728\u6ee1\u8db3\u73b0\u5b9e\u4e16\u754c\u9700\u6c42\u65b9\u9762\u8d8a\u6765\u8d8a\u663e\u5f97\u529b\u4e0d\u4ece\u5fc3\u3002\u672c\u8c03\u67e5\u65e8\u5728\u63d0\u4f9b\u5bf9\u5de5\u4e1a\u7f3a\u9677\u68c0\u6d4b\u7684\u5168\u9762\u7406\u89e3\uff0c\u4ee5\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\u3002", "method": "\u5bf9 2D \u548c 3D \u6a21\u5f0f\u4e2d\u7684\u95ed\u96c6\u548c\u5f00\u96c6\u7f3a\u9677\u68c0\u6d4b\u7b56\u7565\u8fdb\u884c\u4e86\u6df1\u5165\u5206\u6790\uff0c\u91cd\u70b9\u5173\u6ce8\u5176\u8fd1\u5e74\u6765\u7684\u6f14\u53d8\u548c\u65b0\u5174\u8d8b\u52bf\u3002", "result": "\u672c\u8c03\u67e5\u5168\u9762\u6982\u8ff0\u4e86\u5de5\u4e1a\u7f3a\u9677\u68c0\u6d4b\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u7279\u522b\u662f\u5f00\u653e\u96c6\u65b9\u6cd5\u5728\u51cf\u5c11\u5bf9\u5e7f\u6cdb\u7f3a\u9677\u6ce8\u91ca\u7684\u4f9d\u8d56\u548c\u8bc6\u522b\u65b0\u5f02\u5e38\u65b9\u9762\u7684\u4f5c\u7528\u3002", "conclusion": "\u8fd9\u9879\u8c03\u67e5\u63d0\u4f9b\u4e86\u5bf9 2D \u548c 3D \u6a21\u5f0f\u4e2d\u5de5\u4e1a\u7f3a\u9677\u68c0\u6d4b\u7b56\u7565\u7684\u6df1\u5165\u5206\u6790\uff0c\u91cd\u70b9\u4ecb\u7ecd\u4e86\u4ece\u95ed\u96c6\u5230\u5f00\u96c6\u65b9\u6cd5\u7684\u6f14\u53d8\u4ee5\u53ca\u65b0\u5174\u8d8b\u52bf\u3002"}}
{"id": "2507.13825", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13825", "abs": "https://arxiv.org/abs/2507.13825", "authors": ["Haoyang Li", "Yuming Xu", "Yiming Li", "Hanmo Liu", "Darian Li", "Chen Jason Zhang", "Lei Chen", "Qing Li"], "title": "When Speed meets Accuracy: an Efficient and Effective Graph Model for Temporal Link Prediction", "comment": "Submitted in 2024. Accepted in 2025", "summary": "Temporal link prediction in dynamic graphs is a critical task with\napplications in diverse domains such as social networks, recommendation\nsystems, and e-commerce platforms. While existing Temporal Graph Neural\nNetworks (T-GNNs) have achieved notable success by leveraging complex\narchitectures to model temporal and structural dependencies, they often suffer\nfrom scalability and efficiency challenges due to high computational overhead.\nIn this paper, we propose EAGLE, a lightweight framework that integrates\nshort-term temporal recency and long-term global structural patterns. EAGLE\nconsists of a time-aware module that aggregates information from a node's most\nrecent neighbors to reflect its immediate preferences, and a structure-aware\nmodule that leverages temporal personalized PageRank to capture the influence\nof globally important nodes. To balance these attributes, EAGLE employs an\nadaptive weighting mechanism to dynamically adjust their contributions based on\ndata characteristics. Also, EAGLE eliminates the need for complex multi-hop\nmessage passing or memory-intensive mechanisms, enabling significant\nimprovements in efficiency. Extensive experiments on seven real-world temporal\ngraphs demonstrate that EAGLE consistently achieves superior performance\nagainst state-of-the-art T-GNNs in both effectiveness and efficiency,\ndelivering more than a 50x speedup over effective transformer-based T-GNNs.", "AI": {"tldr": "EAGLE\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u65f6\u95f4\u56fe\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u77ed\u671f\u548c\u957f\u671f\u6a21\u5f0f\uff0c\u5728\u4fdd\u6301\u9ad8\u6548\u7387\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u9884\u6d4b\u6027\u80fd\uff0c\u5e76\u4e14\u901f\u5ea6\u6bd4\u73b0\u6709\u65b9\u6cd5\u5feb50\u500d\u4ee5\u4e0a\u3002", "motivation": "\u73b0\u6709\u7684\u65f6\u95f4\u56fe\u795e\u7ecf\u7f51\u7edc\uff08T-GNNs\uff09\u867d\u7136\u5728\u5efa\u6a21\u65f6\u95f4\u4e0e\u7ed3\u6784\u4f9d\u8d56\u6027\u65b9\u9762\u53d6\u5f97\u4e86\u6210\u529f\uff0c\u4f46\u7531\u4e8e\u9ad8\u8ba1\u7b97\u5f00\u9500\u800c\u9762\u4e34\u53ef\u6269\u5c55\u6027\u548c\u6548\u7387\u95ee\u9898\u3002\u56e0\u6b64\uff0c\u6709\u5fc5\u8981\u5f00\u53d1\u4e00\u79cd\u66f4\u8f7b\u91cf\u7ea7\u7684\u6846\u67b6\u3002", "method": " EAGLE\u6846\u67b6\u6574\u5408\u4e86\u77ed\u671f\u65f6\u95f4\u90bb\u8fd1\u6027\u548c\u957f\u671f\u5168\u5c40\u7ed3\u6784\u6a21\u5f0f\u3002\u5b83\u5305\u62ec\u4e00\u4e2a\u65f6\u95f4\u611f\u77e5\u6a21\u5757\uff0c\u8be5\u6a21\u5757\u805a\u5408\u8282\u70b9\u6700\u8fd1\u90bb\u5c45\u7684\u4fe1\u606f\u4ee5\u53cd\u6620\u5176\u76f4\u63a5\u504f\u597d\uff1b\u4e00\u4e2a\u7ed3\u6784\u611f\u77e5\u6a21\u5757\uff0c\u5b83\u5229\u7528\u65f6\u95f4\u4e2a\u6027\u5316PageRank\u6765\u6355\u83b7\u5168\u5c40\u91cd\u8981\u8282\u70b9\u7684\u5f71\u54cd\uff1b\u4ee5\u53ca\u4e00\u4e2a\u81ea\u9002\u5e94\u52a0\u6743\u673a\u5236\u6765\u5e73\u8861\u8fd9\u4e9b\u5c5e\u6027\u3002EAGLE\u65e0\u9700\u590d\u6742\u7684\u591a\u8df3\u6d88\u606f\u4f20\u9012\u6216\u5185\u5b58\u5bc6\u96c6\u578b\u673a\u5236\uff0c\u4ece\u800c\u663e\u8457\u63d0\u9ad8\u4e86\u6548\u7387\u3002", "result": "EAGLE\u6846\u67b6\u5728\u4e03\u4e2a\u771f\u5b9e\u4e16\u754c\u7684\u65f6\u95f4\u56fe\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u6700\u5148\u8fdb\u7684T-GNNs\u76f8\u6bd4\uff0cEAGLE\u5728\u6709\u6548\u6027\u548c\u6548\u7387\u65b9\u9762\u59cb\u7ec8\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u6bd4\u57fa\u4e8eTransformer\u7684T-GNNs\u7684\u901f\u5ea6\u63d0\u9ad8\u4e8650\u500d\u4ee5\u4e0a\u3002", "conclusion": "EAGLE\u6846\u67b6\u5728\u4e03\u4e2a\u771f\u5b9e\u4e16\u754c\u7684\u65f6\u95f4\u56fe\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u6700\u5148\u8fdb\u7684T-GNN\u76f8\u6bd4\uff0cEAGLE\u5728\u6709\u6548\u6027\u548c\u6548\u7387\u65b9\u9762\u59cb\u7ec8\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u6bd4\u57fa\u4e8eTransformer\u7684T-GNNs\u7684\u901f\u5ea6\u63d0\u9ad8\u4e8650\u500d\u4ee5\u4e0a\u3002"}}
{"id": "2507.14004", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.14004", "abs": "https://arxiv.org/abs/2507.14004", "authors": ["Niloofar Nobahari", "Alireza Rezaee"], "title": "Smart fault detection in satellite electrical power system", "comment": null, "summary": "This paper presents an new approach for detecting in the electrical power\nsystem of satellites operating in Low Earth Orbit (LEO) without an Attitude\nDetermination and Control Subsystem (ADCS). Components of these systems are\nprone to faults, such as line-to-line faults in the photovoltaic subsystem,\nopen circuits, and short circuits in the DC-to-DC converter, as well as ground\nfaults in batteries. In the previous research has largely focused on detecting\nfaults in each components, such as photovoltaic arrays or converter systems,\ntherefore, has been limited attention given to whole electrical power system of\nsatellite as a whole system. Our approach addresses this gap by utilizing a\nMulti-Layer Perceptron (MLP) neural network model, which leverages input data\nsuch as solar radiation and surface temperature to predict current and load\noutputs. These machine learning techniques that classifiy use different\napproaches like Principal Component Analysis (PCA) and K-Nearest Neighbors\n(KNN), to classify faults effectively. The model presented achieves over 99%\naccuracy in identifying faults across multiple subsystems, marking a notable\nadvancement from previous approaches by offering a complete diagnostic solution\nfor the entire satellite power system. This thorough method boosts system\nreliability and helps lower the chances of mission failure", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMLP\u795e\u7ecf\u7f51\u7edc\u3001PCA\u548cKNN\u7684\u6545\u969c\u68c0\u6d4b\u65b9\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4b\u65e0ADCS\u7684\u4f4e\u5730\u7403\u8f68\u9053\uff08LEO\uff09\u536b\u661f\u7535\u529b\u7cfb\u7edf\u4e2d\u7684\u6545\u969c\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u592a\u9633\u8f90\u5c04\u548c\u8868\u9762\u6e29\u5ea6\u7b49\u8f93\u5165\u6570\u636e\uff0c\u9884\u6d4b\u7535\u6d41\u548c\u8d1f\u8f7d\u8f93\u51fa\uff0c\u5b9e\u73b0\u4e86\u8d85\u8fc799%\u7684\u51c6\u786e\u7387\uff0c\u63d0\u9ad8\u4e86\u7cfb\u7edf\u53ef\u9760\u6027\u3002", "motivation": "\u5148\u524d\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u68c0\u6d4b\u536b\u661f\u7535\u529b\u7cfb\u7edf\u4e2d\u5404\u4e2a\u5355\u72ec\u7ec4\u4ef6\uff08\u5982\u5149\u4f0f\u9635\u5217\u6216\u8f6c\u6362\u5668\u7cfb\u7edf\uff09\u7684\u6545\u969c\uff0c\u800c\u5bf9\u6574\u4e2a\u7535\u529b\u7cfb\u7edf\u4f5c\u4e3a\u5355\u4e00\u5b9e\u4f53\u8fdb\u884c\u6545\u969c\u68c0\u6d4b\u7684\u5173\u6ce8\u6709\u9650\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\uff0c\u4e3a\u6574\u4e2a\u536b\u661f\u7535\u529b\u7cfb\u7edf\u63d0\u4f9b\u4e00\u4e2a\u6574\u4f53\u6027\u7684\u6545\u969c\u8bca\u65ad\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u591a\u5c42\u611f\u77e5\u673a\uff08MLP\uff09\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u5e76\u7ed3\u5408\u4e3b\u6210\u5206\u5206\u6790\uff08PCA\uff09\u548cK\u8fd1\u90bb\uff08KNN\uff09\u7b49\u673a\u5668\u5b66\u4e60\u6280\u672f\u6765\u5bf9\u536b\u661f\u7535\u529b\u7cfb\u7edf\u4e2d\u7684\u6545\u969c\u8fdb\u884c\u5206\u7c7b\u3002\u8be5\u6a21\u578b\u4ee5\u592a\u9633\u8f90\u5c04\u548c\u8868\u9762\u6e29\u5ea6\u4f5c\u4e3a\u8f93\u5165\uff0c\u9884\u6d4b\u7535\u6d41\u548c\u8d1f\u8f7d\u8f93\u51fa\uff0c\u4ece\u800c\u5b9e\u73b0\u6545\u969c\u68c0\u6d4b\u3002", "result": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u5b9e\u73b0\u4e86\u8d85\u8fc799%\u7684\u51c6\u786e\u7387\uff0c\u80fd\u591f\u6709\u6548\u8bc6\u522b\u591a\u4e2a\u5b50\u7cfb\u7edf\u4e2d\u5b58\u5728\u7684\u6545\u969c\uff0c\u8fd9\u6807\u5fd7\u7740\u4e0e\u4ee5\u5f80\u65b9\u6cd5\u76f8\u6bd4\u7684\u4e00\u4e2a\u663e\u8457\u8fdb\u6b65\uff0c\u56e0\u4e3a\u5b83\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9488\u5bf9\u6574\u4e2a\u536b\u661f\u7535\u529b\u7cfb\u7edf\u7684\u5b8c\u6574\u8bca\u65ad\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u591a\u5c42\u611f\u77e5\u673a\uff08MLP\uff09\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u901a\u8fc7\u5229\u7528\u592a\u9633\u8f90\u5c04\u548c\u8868\u9762\u6e29\u5ea6\u7b49\u8f93\u5165\u6570\u636e\u6765\u9884\u6d4b\u7535\u6d41\u548c\u8d1f\u8f7d\u8f93\u51fa\uff0c\u5b9e\u73b0\u4e86\u5bf9\u536b\u661f\u6574\u4e2a\u7535\u529b\u7cfb\u7edf\u6545\u969c\u7684\u6709\u6548\u5206\u7c7b\uff0c\u51c6\u786e\u7387\u8d85\u8fc799%\uff0c\u4e3a\u6574\u4e2a\u536b\u661f\u7535\u529b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b8c\u6574\u7684\u8bca\u65ad\u89e3\u51b3\u65b9\u6848\uff0c\u63d0\u9ad8\u4e86\u7cfb\u7edf\u53ef\u9760\u6027\u5e76\u964d\u4f4e\u4e86\u4efb\u52a1\u5931\u8d25\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2507.13787", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.13787", "abs": "https://arxiv.org/abs/2507.13787", "authors": ["Doina Pisla", "Alexandru Pusca", "Andrei Caprariu", "Adrian Pisla", "Bogdan Gherman", "Calin Vaida", "Damien Chablat"], "title": "Design Analysis of an Innovative Parallel Robot for Minimally Invasive Pancreatic Surgery", "comment": null, "summary": "This paper focuses on the design of a parallel robot designed for robotic\nassisted minimally invasive pancreatic surgery. Two alternative architectures,\ncalled ATHENA-1 and ATHENA-2, each with 4 degrees of freedom (DOF) are\nproposed. Their kinematic schemes are presented, and the conceptual 3D CAD\nmodels are illustrated. Based on these, two Finite Element Method (FEM)\nsimulations were performed to determine which architecture has the higher\nstiffness. A workspace quantitative analysis is performed to further assess the\nusability of the two proposed parallel architectures related to the medical\ntasks. The obtained results are used to select the architecture which fit the\nrequired design criteria and will be used to develop the experimental model of\nthe surgical robot.", "AI": {"tldr": "Two parallel robot designs for pancreatic surgery were evaluated for stiffness and workspace. One design was chosen for further development.", "motivation": "The motivation of this paper is to design a parallel robot for robotic assisted minimally invasive pancreatic surgery and to select the most suitable architecture based on stiffness and usability criteria.", "method": "The paper utilizes Finite Element Method (FEM) simulations to analyze the stiffness of two proposed parallel robot architectures (ATHENA-1 and ATHENA-2). A workspace quantitative analysis is also performed to assess their usability for medical tasks.", "result": "FEM simulations indicated differences in stiffness between ATHENA-1 and ATHENA-2, and the workspace analysis provided insights into their usability. Based on these results, one architecture was selected for further development.", "conclusion": "The paper proposes two parallel robot architectures, ATHENA-1 and ATHENA-2, for minimally invasive pancreatic surgery. FEM simulations and workspace analysis were conducted to evaluate stiffness and usability. The results were used to select the more suitable architecture for further development."}}
{"id": "2507.13805", "categories": ["cs.LG", "cond-mat.mtrl-sci", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2507.13805", "abs": "https://arxiv.org/abs/2507.13805", "authors": ["Tim Rensmeyer", "Denis Kramer", "Oliver Niggemann"], "title": "On-the-Fly Fine-Tuning of Foundational Neural Network Potentials: A Bayesian Neural Network Approach", "comment": null, "summary": "Due to the computational complexity of evaluating interatomic forces from\nfirst principles, the creation of interatomic machine learning force fields has\nbecome a highly active field of research. However, the generation of training\ndatasets of sufficient size and sample diversity itself comes with a\ncomputational burden that can make this approach impractical for modeling rare\nevents or systems with a large configuration space. Fine-tuning foundation\nmodels that have been pre-trained on large-scale material or molecular\ndatabases offers a promising opportunity to reduce the amount of training data\nnecessary to reach a desired level of accuracy. However, even if this approach\nrequires less training data overall, creating a suitable training dataset can\nstill be a very challenging problem, especially for systems with rare events\nand for end-users who don't have an extensive background in machine learning.\nIn on-the-fly learning, the creation of a training dataset can be largely\nautomated by using model uncertainty during the simulation to decide if the\nmodel is accurate enough or if a structure should be recalculated with\nclassical methods and used to update the model. A key challenge for applying\nthis form of active learning to the fine-tuning of foundation models is how to\nassess the uncertainty of those models during the fine-tuning process, even\nthough most foundation models lack any form of uncertainty quantification. In\nthis paper, we overcome this challenge by introducing a fine-tuning approach\nbased on Bayesian neural network methods and a subsequent on-the-fly workflow\nthat automatically fine-tunes the model while maintaining a pre-specified\naccuracy and can detect rare events such as transition states and sample them\nat an increased rate relative to their occurrence.", "AI": {"tldr": "\u5728\u5fae\u8c03\u8fc7\u7a0b\u4e2d\uff0c\u5229\u7528\u8d1d\u53f6\u65af\u795e\u7ecf\u7f51\u7edc\u548c\u52a8\u6001\u5b66\u4e60\u5de5\u4f5c\u6d41\u6765\u8bc4\u4f30\u548c\u5904\u7406\u57fa\u7840\u6a21\u578b\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u4ece\u800c\u5b9e\u73b0\u5bf9\u7a00\u6709\u4e8b\u4ef6\u7684\u6709\u6548\u68c0\u6d4b\u548c\u91c7\u6837\u3002", "motivation": "\u4e3a\u4e86\u5e94\u5bf9\u7b2c\u4e00\u6027\u539f\u7406\u8ba1\u7b97\u7684\u8ba1\u7b97\u590d\u6742\u6027\u4ee5\u53ca\u751f\u6210\u8db3\u591f\u5927\u548c\u591a\u6837\u5316\u7684\u8bad\u7ec3\u6570\u636e\u96c6\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u6a21\u62df\u7a00\u6709\u4e8b\u4ef6\u6216\u5177\u6709\u5927\u6784\u578b\u7a7a\u95f4\u7684\u7cfb\u7edf\u65f6\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u7684\u6750\u6599\u6216\u5206\u5b50\u6570\u636e\u5e93\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u662f\u4e00\u79cd\u6709\u524d\u666f\u7684\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d1d\u53f6\u65af\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u548c\u540e\u7eed\u7684\u52a8\u6001\u5b66\u4e60\u5de5\u4f5c\u6d41\u7684\u5fae\u8c03\u65b9\u6cd5\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u81ea\u52a8\u5fae\u8c03\u6a21\u578b\uff0c\u4fdd\u6301\u9884\u5b9a\u51c6\u786e\u6027\uff0c\u5e76\u6709\u6548\u68c0\u6d4b\u548c\u91c7\u6837\u7a00\u6709\u4e8b\u4ef6\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u57fa\u4e8e\u8d1d\u53f6\u65af\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u548c\u540e\u7eed\u7684\u52a8\u6001\u5b66\u4e60\u5de5\u4f5c\u6d41\uff0c\u53ef\u4ee5\u514b\u670d\u5728\u5fae\u8c03\u57fa\u7840\u6a21\u578b\u65f6\u8bc4\u4f30\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u7684\u6311\u6218\uff0c\u8be5\u5de5\u4f5c\u6d41\u53ef\u81ea\u52a8\u5fae\u8c03\u6a21\u578b\uff0c\u540c\u65f6\u4fdd\u6301\u9884\u5b9a\u7684\u51c6\u786e\u6027\uff0c\u5e76\u80fd\u4ee5\u76f8\u5bf9\u4e8e\u5176\u53d1\u751f\u7387\u589e\u52a0\u7684\u901f\u7387\u68c0\u6d4b\u7a00\u6709\u4e8b\u4ef6\uff08\u5982\u8fc7\u6e21\u6001\uff09\u5e76\u5bf9\u5176\u8fdb\u884c\u91c7\u6837\u3002"}}
{"id": "2507.14074", "categories": ["cond-mat.mes-hall", "cond-mat.supr-con"], "pdf": "https://arxiv.org/pdf/2507.14074", "abs": "https://arxiv.org/abs/2507.14074", "authors": ["Yuriko Baba", "Alfredo Levy Yeyati", "Pablo Burset"], "title": "Emergent topology by Landau level mixing in quantum Hall-superconductor nanostructures", "comment": "13 pages; 11 figures", "summary": "We demonstrate the emergence of novel topological phases in quantum\nHall-superconductor hybrid systems driven by Landau level mixing and spin-orbit\ninteractions. Focusing on a narrow superconducting stripe atop a\ntwo-dimensional electron gas, we identify regimes where the hybridization of\nthe chiral Andreev states at each side of the stripe leads to different phases\nbeyond the long sought $p$-wave superconducting one. These topological phases\nexhibit distinctive transport signatures, including quantized nonlocal\nconductance arising from electron cotunneling at filling factor $\\nu=1$, which\ncan coexist with quantized crossed Andreev reflection at $\\nu=2$. A combination\nof numerical simulations and effective modelling reveals the role of spin-orbit\ncoupling and stripe geometry in controlling these transitions. Our findings\nsuggest new strategies for realizing and detecting topology in proximized\nquantum Hall devices.", "AI": {"tldr": "\u672c\u7814\u7a76\u5c55\u793a\u4e86\u91cf\u5b50\u970d\u5c14-\u8d85\u5bfc\u4f53\u6df7\u5408\u7cfb\u7edf\u4e2d\u51fa\u73b0\u7684\u65b0\u578b\u62d3\u6251\u76f8\uff0c\u8fd9\u4e9b\u76f8\u7531\u6717\u9053\u80fd\u7ea7\u6df7\u5408\u548c\u81ea\u65cb-\u8f68\u9053\u76f8\u4e92\u4f5c\u7528\u9a71\u52a8\u3002\u7814\u7a76\u8bc6\u522b\u4e86\u8d85\u8d8ap\u6ce2\u8d85\u5bfc\u7684\u65b0\u76f8\uff0c\u5e76\u53d1\u73b0\u4e86\u5176\u72ec\u7279\u7684\u8f93\u8fd0\u7279\u5f81\uff0c\u4e3a\u5728\u91cf\u5b50\u970d\u5c14\u5668\u4ef6\u4e2d\u5b9e\u73b0\u548c\u63a2\u6d4b\u62d3\u6251\u63d0\u4f9b\u4e86\u65b0\u7b56\u7565\u3002", "motivation": "\u4e3a\u4e86\u63a2\u7d22\u91cf\u5b50\u970d\u5c14-\u8d85\u5bfc\u4f53\u6df7\u5408\u7cfb\u7edf\u4e2d\u7531\u6717\u9053\u80fd\u7ea7\u6df7\u5408\u548c\u81ea\u65cb-\u8f68\u9053\u76f8\u4e92\u4f5c\u7528\u9a71\u52a8\u7684\u65b0\u578b\u62d3\u6251\u76f8\uff0c\u5e76\u8bc6\u522b\u8d85\u8d8a\u4f20\u7edfp\u6ce2\u8d85\u5bfc\u7684\u76f8\u3002", "method": "\u901a\u8fc7\u6570\u503c\u6a21\u62df\u548c\u6709\u6548\u6a21\u578b\uff0c\u7814\u7a76\u4e86\u81ea\u65cb-\u8f68\u9053\u8026\u5408\u548c\u6761\u5e26\u51e0\u4f55\u5bf9\u63a7\u5236\u8fd9\u4e9b\u8dc3\u8fc1\u7684\u4f5c\u7528\u3002", "result": "\u53d1\u73b0\u4e86\u7531\u6717\u9053\u80fd\u7ea7\u6df7\u5408\u548c\u81ea\u65cb-\u8f68\u9053\u76f8\u4e92\u4f5c\u7528\u9a71\u52a8\u7684\u65b0\u578b\u62d3\u6251\u76f8\uff0c\u5e76\u8bc6\u522b\u4e86\u5176\u72ec\u7279\u7684\u8f93\u8fd0\u7279\u5f81\uff0c\u4f8b\u5982\u5728\u586b\u5145\u56e0\u5b50\u03bd=1\u65f6\u7531\u7535\u5b50\u5171\u96a7\u7a7f\u4ea7\u751f\u7684\u91cf\u5b50\u5316\u975e\u5c40\u57df\u7535\u5bfc\uff0c\u4ee5\u53ca\u5728\u03bd=2\u65f6\u4e0e\u91cf\u5b50\u5316\u4ea4\u53c9\u5b89\u5fb7\u70c8\u592b\u53cd\u5c04\u5171\u5b58\u7684\u53ef\u80fd\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u63ed\u793a\u4e86\u5728\u91cf\u5b50\u970d\u5c14-\u8d85\u5bfc\u4f53\u6df7\u5408\u7cfb\u7edf\u4e2d\uff0c\u7531\u6717\u9053\u80fd\u7ea7\u6df7\u5408\u548c\u81ea\u65cb-\u8f68\u9053\u76f8\u4e92\u4f5c\u7528\u9a71\u52a8\u7684\u65b0\u578b\u62d3\u6251\u76f8\u7684\u51fa\u73b0\u3002\u901a\u8fc7\u6570\u503c\u6a21\u62df\u548c\u6709\u6548\u6a21\u578b\uff0c\u6211\u4eec\u8bc6\u522b\u4e86\u6742\u5316\u6b66\u5c14\u592b\u6001\u7684\u533a\u57df\uff0c\u8fd9\u4e9b\u533a\u57df\u4f1a\u4ea7\u751f\u8d85\u8d8a\u4f20\u7edfp\u6ce2\u8d85\u5bfc\u7684\u76f8\u3002"}}
{"id": "2507.13501", "categories": ["cs.CL", "math.RA", "q-bio.NC", "91F20, 16Y60, 16T05, 92C20"], "pdf": "https://arxiv.org/pdf/2507.13501", "abs": "https://arxiv.org/abs/2507.13501", "authors": ["Matilde Marcolli", "Robert C. Berwick"], "title": "Encoding syntactic objects and Merge operations in function spaces", "comment": "40 pages, LaTeX, 4 png figures", "summary": "We provide a mathematical argument showing that, given a representation of\nlexical items as functions (wavelets, for instance) in some function space, it\nis possible to construct a faithful representation of arbitrary syntactic\nobjects in the same function space. This space can be endowed with a\ncommutative non-associative semiring structure built using the second Renyi\nentropy. The resulting representation of syntactic objects is compatible with\nthe magma structure. The resulting set of functions is an algebra over an\noperad, where the operations in the operad model circuits that transform the\ninput wave forms into a combined output that encodes the syntactic structure.\nThe action of Merge on workspaces is faithfully implemented as action on these\ncircuits, through a coproduct and a Hopf algebra Markov chain. The results\nobtained here provide a constructive argument showing the theoretical\npossibility of a neurocomputational realization of the core computational\nstructure of syntax. We also present a particular case of this general\nconstruction where this type of realization of Merge is implemented as a cross\nfrequency phase synchronization on sinusoidal waves. This also shows that Merge\ncan be expressed in terms of the successor function of a semiring, thus\nclarifying the well known observation of its similarities with the successor\nfunction of arithmetic.", "AI": {"tldr": "\u53e5\u6cd5\u7ed3\u6784\u53ef\u4ee5\u7528\u51fd\u6570\u7a7a\u95f4\u4e2d\u7684\u51fd\u6570\u8868\u793a\uff0c\u5e76\u53ef\u901a\u8fc7\u795e\u7ecf\u8ba1\u7b97\u5b9e\u73b0\u3002", "motivation": "\u63d0\u4f9b\u4e00\u4e2a\u6570\u5b66\u6846\u67b6\uff0c\u7528\u4e8e\u8868\u793a\u53e5\u6cd5\u5bf9\u8c61\u5e76\u63a2\u7d22\u5176\u795e\u7ecf\u8ba1\u7b97\u5b9e\u73b0\u7684\u7406\u8bba\u53ef\u80fd\u6027\u3002", "method": "\u4f7f\u7528\u6570\u5b66\u8bba\u8bc1\uff0c\u5c06\u8bcd\u6c47\u9879\u8868\u793a\u4e3a\u51fd\u6570\uff08\u4f8b\u5982\u5c0f\u6ce2\uff09\uff0c\u5728\u51fd\u6570\u7a7a\u95f4\u4e2d\u6784\u5efa\u4efb\u610f\u53e5\u6cd5\u5bf9\u8c61\u7684\u5fe0\u5b9e\u8868\u793a\u3002\u8be5\u7a7a\u95f4\u5177\u6709\u57fa\u4e8e\u7b2c\u4e8cRenyi\u71b5\u7684\u4ea4\u6362\u975e\u7ed3\u5408\u534a\u73af\u7ed3\u6784\uff0c\u8be5\u7ed3\u6784\u4e0e\u5ca9\u6d46\u7ed3\u6784\u517c\u5bb9\u3002\u8be5\u51fd\u6570\u96c6\u662f\u81ea\u7531\u4ee3\u6570\uff0c\u5176\u8fd0\u7b97\u5bf9\u6a21\u578b\u8fdb\u884c\u8f6c\u6362\u3002", "result": "\u6210\u529f\u5730\u5728\u51fd\u6570\u7a7a\u95f4\u4e2d\u6784\u5efa\u4e86\u53e5\u6cd5\u5bf9\u8c61\u7684\u5fe0\u5b9e\u8868\u793a\uff0c\u8be5\u8868\u793a\u4e0e\u5ca9\u6d46\u7ed3\u6784\u517c\u5bb9\uff0c\u5e76\u4e14Merge\u64cd\u4f5c\u53ef\u4ee5\u901a\u8fc7Hopf\u4ee3\u6570\u9a6c\u5c14\u53ef\u592b\u94fe\u4e0a\u7684\u4f59\u79ef\u6765\u5b9e\u73b0\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u53e5\u6cd5\u6838\u5fc3\u8ba1\u7b97\u7ed3\u6784\u63d0\u4f9b\u4e86\u795e\u7ecf\u8ba1\u7b97\u5b9e\u73b0\u7684\u7406\u8bba\u53ef\u80fd\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u5982\u4f55\u4f7f\u7528\u6b63\u5f26\u6ce2\u4e0a\u7684\u4ea4\u53c9\u9891\u7387\u76f8\u4f4d\u540c\u6b65\u6765\u5b9e\u73b0Merge\u64cd\u4f5c\u3002"}}
{"id": "2507.13724", "categories": ["quant-ph", "math-ph", "math.MP"], "pdf": "https://arxiv.org/pdf/2507.13724", "abs": "https://arxiv.org/abs/2507.13724", "authors": ["Aigerim Bazarkhanova", "Alejandro J. Castro", "Antonio A. Valido"], "title": "Solving wave equation problems on D-Wave quantum annealers", "comment": "20+2 pages, 7 figures. Comments are welcome", "summary": "We solve the one-dimensional Helmholtz equation in several scenarios using\nthe quantum annealer provided by the D-Wave systems within a pseudospectral\nscheme, where its solution is encoded into certain set of suitable basis\nfunctions. We assess the performance of different strategies of encoding based\non algebraic arguments and the adiabatic condition, and benchmark these against\nthe classical heuristic simulating annealing algorithm. In particular, we\ncompute the minimum energy gap, the so-called dynamic range and the mean\nsquared error to assess the numerical stability, consistency and accuracy of\nthe solutions returned by each strategy. Our work stresses out the importance\nof developing custom embedded techniques ensuring well-conditioned algebraic\nsystems. In particular, we find out that encoding strategies retrieving\nalgebraic systems exhibiting full-rank and small dynamic ranges enhance the\nperformance of the quantum annealer even under polychromatic driving and for\nintricate initial conditions. We further discuss the prospect of developing\nhybrid quantum-classical schemes enable to meet suitable algebraic and\nadiabatic conditions simultaneously.", "AI": {"tldr": "\u91cf\u5b50\u9000\u706b\u5668\u5728\u4f2a\u8c31\u65b9\u6848\u4e2d\u6c42\u89e3\u4e00\u7ef4\u4ea5\u59c6\u970d\u5179\u65b9\u7a0b\uff0c\u5e76\u901a\u8fc7\u5b9a\u5236\u5d4c\u5165\u6280\u672f\u548c\u5168\u79e9\u3001\u5c0f\u52a8\u6001\u8303\u56f4\u7684\u4ee3\u6570\u7cfb\u7edf\u6765\u63d0\u9ad8\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u91cf\u5b50\u9000\u706b\u5668\u5728\u4f2a\u8c31\u65b9\u6848\u4e2d\u6c42\u89e3\u4e00\u7ef4\u4ea5\u59c6\u970d\u5179\u65b9\u7a0b\u7684\u6027\u80fd\uff0c\u5e76\u8bc4\u4f30\u4e0d\u540c\u7f16\u7801\u7b56\u7565\u7684\u6709\u6548\u6027\u3002", "method": "\u4f7f\u7528D-Wave\u7cfb\u7edf\u7684\u91cf\u5b50\u9000\u706b\u5668\u901a\u8fc7\u4f2a\u8c31\u65b9\u6848\u6c42\u89e3\u4e00\u7ef4\u4ea5\u59c6\u970d\u5179\u65b9\u7a0b\uff0c\u5176\u4e2d\u89e3\u88ab\u7f16\u7801\u5230\u4e00\u7ec4\u5408\u9002\u7684\u57fa\u51fd\u6570\u4e2d\u3002", "result": "\u53d1\u73b0\u80fd\u591f\u68c0\u7d22\u5168\u79e9\u548c\u52a8\u6001\u8303\u56f4\u5c0f\u7684\u4ee3\u6570\u7cfb\u7edf\u7684\u7f16\u7801\u7b56\u7565\u80fd\u591f\u63d0\u9ad8\u91cf\u5b50\u9000\u706b\u5668\u7684\u6027\u80fd\uff0c\u5373\u4f7f\u5728\u591a\u8272\u9a71\u52a8\u548c\u590d\u6742\u521d\u59cb\u6761\u4ef6\u4e0b\u3002", "conclusion": "\u901a\u8fc7\u5728\u4f2a\u8c31\u65b9\u6848\u4e2d\u89e3\u51b3\u4e00\u7ef4\u4ea5\u59c6\u970d\u5179\u65b9\u7a0b\uff0c\u6211\u4eec\u5f3a\u8c03\u4e86\u5f00\u53d1\u5b9a\u5236\u5d4c\u5165\u6280\u672f\u4ee5\u786e\u4fdd\u826f\u597d\u75c5\u6001\u4ee3\u6570\u7cfb\u7edf\u7684\u5bf9\u4e8e\u91cf\u5b50\u9000\u706b\u5668\u6027\u80fd\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2507.13508", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.13508", "abs": "https://arxiv.org/abs/2507.13508", "authors": ["Agata Kaczmarek", "Dawid P\u0142udowski", "Piotr Wilczy\u0144ski", "Przemys\u0142aw Biecek", "Krzysztof Kotowski", "Ramez Shendy", "Jakub Nalepa", "Artur Janicki", "Evridiki Ntagiou"], "title": "Fake or Real: The Impostor Hunt in Texts for Space Operations", "comment": null, "summary": "The \"Fake or Real\" competition hosted on Kaggle\n(\\href{https://www.kaggle.com/competitions/fake-or-real-the-impostor-hunt}{https://www.kaggle.com/competitions/fake-or-real-the-impostor-hunt})\nis the second part of a series of follow-up competitions and hackathons related\nto the \"Assurance for Space Domain AI Applications\" project funded by the\nEuropean Space Agency\n(\\href{https://assurance-ai.space-codev.org/}{https://assurance-ai.space-codev.org/}).\nThe competition idea is based on two real-life AI security threats identified\nwithin the project -- data poisoning and overreliance in Large Language Models.\nThe task is to distinguish between the proper output from LLM and the output\ngenerated under malicious modification of the LLM. As this problem was not\nextensively researched, participants are required to develop new techniques to\naddress this issue or adjust already existing ones to this problem's statement.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.14035", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.14035", "abs": "https://arxiv.org/abs/2507.14035", "authors": ["Sai Xu", "Kai-Kit Wong", "Yanan Du", "Hanjiang Hong", "Chan-Byoung Chae", "Baiyang Liu", "Kin-Fai Tong"], "title": "Toward Practical Fluid Antenna Systems: Co-Optimizing Hardware and Software for Port Selection and Beamforming", "comment": null, "summary": "This paper proposes a hardware-software co-design approach to efficiently\noptimize beamforming and port selection in fluid antenna systems (FASs). To\nbegin with, a fluid-antenna (FA)-enabled downlink multi-cell multiple-input\nmultiple-output (MIMO) network is modeled, and a weighted sum-rate (WSR)\nmaximization problem is formulated. Second, a method that integrates graph\nneural networks (GNNs) with random port selection (RPS) is proposed to jointly\noptimize beamforming and port selection, while also assessing the benefits and\nlimitations of random selection. Third, an instruction-driven deep learning\naccelerator based on a field-programmable gate array (FPGA) is developed to\nminimize inference latency. To further enhance efficiency, a scheduling\nalgorithm is introduced to reduce redundant computations and minimize the idle\ntime of computing cores. Simulation results demonstrate that the proposed\nGNN-RPS approach achieves competitive communication performance. Furthermore,\nexperimental evaluations indicate that the FPGA-based accelerator maintains low\nlatency while simultaneously executing beamforming inference for multiple port\nselections.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f6f\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u5229\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u548c\u968f\u673a\u7aef\u53e3\u9009\u62e9\uff08RPS\uff09\u4f18\u5316\u6d41\u4f53\u5929\u7ebf\u7cfb\u7edf\uff08FAS\uff09\u4e2d\u7684\u6ce2\u675f\u5f62\u6210\u548c\u7aef\u53e3\u9009\u62e9\uff0c\u5e76\u4f7f\u7528FPGA\u52a0\u901f\u5668\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u63a8\u7406\u3002", "motivation": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f6f\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u4ee5\u6709\u6548\u5730\u4f18\u5316\u6d41\u4f53\u5929\u7ebf\u7cfb\u7edf\uff08FAS\uff09\u4e2d\u7684\u6ce2\u675f\u5f62\u6210\u548c\u7aef\u53e3\u9009\u62e9\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u4e0e\u968f\u673a\u7aef\u53e3\u9009\u62e9\uff08RPS\uff09\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u4ee5\u8054\u5408\u4f18\u5316\u6ce2\u675f\u5f62\u6210\u548c\u7aef\u53e3\u9009\u62e9\uff0c\u540c\u65f6\u8bc4\u4f30\u968f\u673a\u9009\u62e9\u7684\u4f18\u70b9\u548c\u5c40\u9650\u6027\u3002\u6b64\u5916\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u73b0\u573a\u53ef\u7f16\u7a0b\u95e8\u9635\u5217\uff08FPGA\uff09\u7684\u6307\u4ee4\u9a71\u52a8\u6df1\u5ea6\u5b66\u4e60\u52a0\u901f\u5668\uff0c\u4ee5\u6700\u5c0f\u5316\u63a8\u7406\u5ef6\u8fdf\u3002\u4e3a\u8fdb\u4e00\u6b65\u63d0\u9ad8\u6548\u7387\uff0c\u8fd8\u5f15\u5165\u4e86\u4e00\u79cd\u8c03\u5ea6\u7b97\u6cd5\uff0c\u4ee5\u51cf\u5c11\u5197\u4f59\u8ba1\u7b97\u5e76\u6700\u5c0f\u5316\u8ba1\u7b97\u6838\u5fc3\u7684\u7a7a\u95f2\u65f6\u95f4\u3002", "result": "\u63d0\u51fa\u7684GNN-RPS\u65b9\u6cd5\u5b9e\u73b0\u4e86\u53ef\u89c2\u7684\u901a\u4fe1\u6027\u80fd\uff0c\u5e76\u4e14\u57fa\u4e8eFPGA\u7684\u52a0\u901f\u5668\u5728\u540c\u65f6\u6267\u884c\u591a\u4e2a\u7aef\u53e3\u9009\u62e9\u7684\u6ce2\u675f\u5f62\u6210\u63a8\u7406\u65f6\uff0c\u4fdd\u6301\u4e86\u4f4e\u5ef6\u8fdf\u3002", "conclusion": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684GNN-RPS\u65b9\u6cd5\u5b9e\u73b0\u4e86\u53ef\u89c2\u7684\u901a\u4fe1\u6027\u80fd\u3002\u6b64\u5916\uff0c\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0c\u57fa\u4e8eFPGA\u7684\u52a0\u901f\u5668\u5728\u540c\u65f6\u6267\u884c\u591a\u4e2a\u7aef\u53e3\u9009\u62e9\u7684\u6ce2\u675f\u5f62\u6210\u63a8\u7406\u65f6\uff0c\u4fdd\u6301\u4e86\u4f4e\u5ef6\u8fdf\u3002"}}
{"id": "2507.13385", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13385", "abs": "https://arxiv.org/abs/2507.13385", "authors": ["Arjun Rao", "Esther Rolf"], "title": "Using Multiple Input Modalities Can Improve Data-Efficiency and O.O.D. Generalization for ML with Satellite Imagery", "comment": "17 pages, 9 figures, 7 tables. Accepted to TerraBytes@ICML 2025", "summary": "A large variety of geospatial data layers is available around the world\nranging from remotely-sensed raster data like satellite imagery, digital\nelevation models, predicted land cover maps, and human-annotated data, to data\nderived from environmental sensors such as air temperature or wind speed data.\nA large majority of machine learning models trained on satellite imagery\n(SatML), however, are designed primarily for optical input modalities such as\nmulti-spectral satellite imagery. To better understand the value of using other\ninput modalities alongside optical imagery in supervised learning settings, we\ngenerate augmented versions of SatML benchmark tasks by appending additional\ngeographic data layers to datasets spanning classification, regression, and\nsegmentation. Using these augmented datasets, we find that fusing additional\ngeographic inputs with optical imagery can significantly improve SatML model\nperformance. Benefits are largest in settings where labeled data are limited\nand in geographic out-of-sample settings, suggesting that multi-modal inputs\nmay be especially valuable for data-efficiency and out-of-sample performance of\nSatML models. Surprisingly, we find that hard-coded fusion strategies\noutperform learned variants, with interesting implications for future work.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5c06\u536b\u661f\u5149\u5b66\u5f71\u50cf\u4e0e\u6e29\u5ea6\u3001\u98ce\u901f\u7b49\u5176\u4ed6\u5730\u7406\u6570\u636e\u878d\u5408\uff0c\u80fd\u63d0\u5347\u536b\u661f\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u6570\u636e\u4e0d\u8db3\u6216\u65b0\u5730\u7406\u533a\u57df\u7684\u8868\u73b0\u4e0a\u3002\u786c\u7f16\u7801\u7684\u878d\u5408\u65b9\u5f0f\u6548\u679c\u4f18\u4e8e\u5b66\u4e60\u65b9\u5f0f\u3002", "motivation": "\u4e3a\u4e86\u66f4\u597d\u5730\u7406\u89e3\u5728\u76d1\u7763\u5b66\u4e60\u573a\u666f\u4e2d\uff0c\u9664\u4e86\u5149\u5b66\u5f71\u50cf\u4e4b\u5916\uff0c\u5229\u7528\u5176\u4ed6\u8f93\u5165\u6a21\u5f0f\u7684\u4ef7\u503c\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5c06\u591a\u79cd\u5730\u7406\u6570\u636e\u5c42\u4e0e\u5149\u5b66\u5f71\u50cf\u878d\u5408\u5bf9\u536b\u661f\u673a\u5668\u5b66\u4e60\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u672c\u7814\u7a76\u901a\u8fc7\u4e3a\u6db5\u76d6\u5206\u7c7b\u3001\u56de\u5f52\u548c\u5206\u5272\u4efb\u52a1\u7684\u57fa\u51c6\u6570\u636e\u96c6\u6dfb\u52a0\u989d\u5916\u7684\u5730\u7406\u6570\u636e\u5c42\uff08\u5982\u73af\u5883\u4f20\u611f\u5668\u6570\u636e\uff09\uff0c\u6765\u751f\u6210\u589e\u5f3a\u7248\u7684\u536b\u661f\u673a\u5668\u5b66\u4e60\u57fa\u51c6\u4efb\u52a1\u3002\u968f\u540e\uff0c\u5229\u7528\u8fd9\u4e9b\u589e\u5f3a\u7684\u6570\u636e\u96c6\u6765\u8bc4\u4f30\u548c\u6bd4\u8f83\u4e0d\u540c\u8f93\u5165\u6a21\u5f0f\u7684\u6027\u80fd\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u663e\u793a\uff0c\u878d\u5408\u989d\u5916\u7684\u5730\u7406\u8f93\u5165\uff08\u5982\u73af\u5883\u4f20\u611f\u5668\u6570\u636e\uff09\u4e0e\u5149\u5b66\u5f71\u50cf\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8SatML\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u6807\u8bb0\u6570\u636e\u6709\u9650\u548c\u5730\u7406\u6837\u672c\u5916\u7684\u60c5\u51b5\u4e0b\u3002\u786c\u7f16\u7801\u7684\u878d\u5408\u7b56\u7565\u76f8\u6bd4\u5b66\u4e60\u578b\u878d\u5408\u7b56\u7565\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u7ed3\u5408\u591a\u79cd\u5730\u7406\u8f93\u5165\u6570\u636e\u4e0e\u5149\u5b66\u5f71\u50cf\u8fdb\u884c\u878d\u5408\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u536b\u661f\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff08SatML\uff09\u7684\u6027\u80fd\u3002\u8fd9\u79cd\u63d0\u5347\u5728\u6807\u8bb0\u6570\u636e\u6709\u9650\u548c\u5730\u7406\u6837\u672c\u5916\uff08out-of-sample\uff09\u7684\u60c5\u51b5\u4e0b\u5c24\u4e3a\u660e\u663e\uff0c\u8fd9\u8868\u660e\u591a\u6a21\u6001\u8f93\u5165\u5bf9\u4e8e\u63d0\u9ad8SatML\u6a21\u578b\u7684\u6570\u636e\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u53d1\u73b0\uff0c\u786c\u7f16\u7801\u7684\u878d\u5408\u7b56\u7565\u4f18\u4e8e\u5b66\u4e60\u578b\u878d\u5408\u7b56\u7565\uff0c\u8fd9\u4e3a\u672a\u6765\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u3002"}}
{"id": "2507.13846", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13846", "abs": "https://arxiv.org/abs/2507.13846", "authors": ["Kathrin Korte", "Christian Medeiros Adriano", "Sona Ghahremani", "Holger Giese"], "title": "Causal Knowledge Transfer for Multi-Agent Reinforcement Learning in Dynamic Environments", "comment": null, "summary": "[Context] Multi-agent reinforcement learning (MARL) has achieved notable\nsuccess in environments where agents must learn coordinated behaviors. However,\ntransferring knowledge across agents remains challenging in non-stationary\nenvironments with changing goals. [Problem] Traditional knowledge transfer\nmethods in MARL struggle to generalize, and agents often require costly\nretraining to adapt. [Approach] This paper introduces a causal knowledge\ntransfer framework that enables RL agents to learn and share compact causal\nrepresentations of paths within a non-stationary environment. As the\nenvironment changes (new obstacles), agents' collisions require adaptive\nrecovery strategies. We model each collision as a causal intervention\ninstantiated as a sequence of recovery actions (a macro) whose effect\ncorresponds to a causal knowledge of how to circumvent the obstacle while\nincreasing the chances of achieving the agent's goal (maximizing cumulative\nreward). This recovery action macro is transferred online from a second agent\nand is applied in a zero-shot fashion, i.e., without retraining, just by\nquerying a lookup model with local context information (collisions). [Results]\nOur findings reveal two key insights: (1) agents with heterogeneous goals were\nable to bridge about half of the gap between random exploration and a fully\nretrained policy when adapting to new environments, and (2) the impact of\ncausal knowledge transfer depends on the interplay between environment\ncomplexity and agents' heterogeneous goals.", "AI": {"tldr": "\u56e0\u679c\u77e5\u8bc6\u8fc1\u79fb\u6846\u67b6\u5728\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u901a\u8fc7\u96f6\u6837\u672c\u8fc1\u79fb\u6062\u590d\u52a8\u4f5c\u5b8f\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u667a\u80fd\u4f53\u9002\u5e94\u975e\u5e73\u7a33\u73af\u5883\u7684\u80fd\u529b\uff0c\u5c24\u5176\u5728\u76ee\u6807\u5f02\u6784\u7684\u60c5\u51b5\u4e0b\u6548\u679c\u663e\u8457\uff0c\u4f46\u8fc1\u79fb\u6548\u679c\u53d7\u73af\u5883\u590d\u6742\u6027\u5f71\u54cd\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u5728\u975e\u5e73\u7a33\u548c\u76ee\u6807\u591a\u53d8\u7684\u73af\u5883\u4e2d\u77e5\u8bc6\u8fc1\u79fb\u7684\u6311\u6218\u3002\u4f20\u7edf\u7684\u77e5\u8bc6\u8fc1\u79fb\u65b9\u6cd5\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u4e14\u9700\u8981\u6602\u8d35\u7684\u91cd\u65b0\u8bad\u7ec3\uff0c\u800c\u672c\u7814\u7a76\u65e8\u5728\u5b9e\u73b0\u66f4\u9ad8\u6548\u3001\u81ea\u9002\u5e94\u7684\u77e5\u8bc6\u8fc1\u79fb\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u56e0\u679c\u77e5\u8bc6\u8fc1\u79fb\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u6bcf\u6b21\u78b0\u649e\u5efa\u6a21\u4e3a\u56e0\u679c\u5e72\u9884\uff0c\u5e76\u5c06\u5176\u5b9e\u4f8b\u5316\u4e3a\u6062\u590d\u52a8\u4f5c\u5e8f\u5217\uff08\u5b8f\uff09\uff0c\u4ee5\u5b9e\u73b0\u96f6\u6837\u672c\u8fc1\u79fb\u3002\u8be5\u6846\u67b6\u5229\u7528\u67e5\u627e\u6a21\u578b\uff0c\u6839\u636e\u5c40\u90e8\u4e0a\u4e0b\u6587\u4fe1\u606f\uff08\u78b0\u649e\uff09\u67e5\u8be2\u5e76\u5e94\u7528\u6062\u590d\u52a8\u4f5c\u5b8f\uff0c\u4ece\u800c\u5728\u4e0d\u91cd\u65b0\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u9002\u5e94\u73af\u5883\u53d8\u5316\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u91c7\u7528\u56e0\u679c\u77e5\u8bc6\u8fc1\u79fb\u6846\u67b6\u7684\u667a\u80fd\u4f53\u5728\u9002\u5e94\u65b0\u73af\u5883\u65f6\uff0c\u80fd\u591f\u5f25\u5408\u968f\u673a\u63a2\u7d22\u4e0e\u5b8c\u5168\u91cd\u65b0\u8bad\u7ec3\u7b56\u7565\u4e4b\u95f4\u7ea6\u4e00\u534a\u7684\u5dee\u8ddd\u3002\u6b64\u5916\uff0c\u56e0\u679c\u77e5\u8bc6\u8fc1\u79fb\u7684\u6548\u679c\u4e0e\u73af\u5883\u590d\u6742\u6027\u4ee5\u53ca\u667a\u80fd\u4f53\u76ee\u6807\u5f02\u6784\u6027\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u5bc6\u5207\u76f8\u5173\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u56e0\u679c\u77e5\u8bc6\u8fc1\u79fb\u6846\u67b6\u80fd\u591f\u4f7f\u667a\u80fd\u4f53\u5728\u975e\u5e73\u7a33\u73af\u5883\u4e2d\u8fdb\u884c\u77e5\u8bc6\u8fc1\u79fb\uff0c\u9002\u5e94\u65b0\u73af\u5883\u7684\u6548\u7387\u8f83\u968f\u673a\u63a2\u7d22\u63d0\u9ad8\u4e86\u7ea6\u4e00\u500d\uff0c\u63a5\u8fd1\u5b8c\u5168\u91cd\u65b0\u8bad\u7ec3\u7684\u7b56\u7565\u3002\u8fc1\u79fb\u6548\u679c\u53d7\u73af\u5883\u590d\u6742\u5ea6\u548c\u667a\u80fd\u4f53\u5f02\u6784\u76ee\u6807\u7b49\u56e0\u7d20\u7684\u5171\u540c\u5f71\u54cd\u3002"}}
{"id": "2507.14020", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.14020", "abs": "https://arxiv.org/abs/2507.14020", "authors": ["Marwan Hassini", "Colette Mintsa-Eya", "Eduardo Redondo-Iglesias", "Pascal Venet"], "title": "Influence of Cell Position on the Capacity of Retired Batteries: Experimental and Statistical Studies", "comment": "5 pages, 4 figures, IECON 2025", "summary": "Understanding how batteries perform after automotive use is crucial to\ndetermining their potential for reuse. This article presents experimental\nresults aimed at advancing knowledge of retired battery performance. Three\nmodules extracted from electric vehicles were tested. Their performance was\nassessed, and the results were analyzed statistically using analysis of\nvariance (ANOVA). The 36 retired cells exhibited a high level of performance,\nalbeit with significant variation. On average, the cells had a 95% state of\nhealth capacity with a dispersion of 2.4%. ANOVA analysis suggests that cell\nperformance is not correlated with their position inside the module. These\nresults demonstrate the need to evaluate dispersion within retired batteries\nand to develop thermal management and balancing systems for second-life\nbatteries.", "AI": {"tldr": "\u9000\u5f79\u7535\u6c60\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5b58\u5728\u5dee\u5f02\u3002\u65b9\u5dee\u5206\u6790\u8868\u660e\u7535\u6c60\u4f4d\u7f6e\u4e0d\u5f71\u54cd\u6027\u80fd\u3002\u5efa\u8bae\u4e3a\u4e8c\u6b21\u751f\u547d\u7535\u6c60\u8fdb\u884c\u8bc4\u4f30\u548c\u7cfb\u7edf\u5f00\u53d1\u3002", "motivation": "\u4e3a\u4e86\u786e\u5b9a\u9000\u5f79\u7535\u6c60\u7684\u518d\u5229\u7528\u6f5c\u529b\uff0c\u4e86\u89e3\u5176\u4f7f\u7528\u540e\u7684\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5bf9\u4ece\u7535\u52a8\u6c7d\u8f66\u4e2d\u63d0\u53d6\u7684\u4e09\u4e2a\u6a21\u5757\u8fdb\u884c\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u5176\u6027\u80fd\uff0c\u5e76\u4f7f\u7528\u65b9\u5dee\u5206\u6790 (ANOVA) \u5bf9\u7ed3\u679c\u8fdb\u884c\u7edf\u8ba1\u5206\u6790\u3002", "result": "36 \u4e2a\u9000\u5f79\u7535\u6c60\u7684\u5e73\u5747\u5065\u5eb7\u5bb9\u91cf\u4e3a 95%\uff0c\u79bb\u6563\u5ea6\u4e3a 2.4%\u3002", "conclusion": "\u9000\u5f79\u7535\u6c60\u8868\u73b0\u51fa\u5f88\u9ad8\u7684\u6027\u80fd\u6c34\u5e73\uff0c\u5c3d\u7ba1\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002\u7535\u6c60\u6027\u80fd\u4e0e\u5b83\u4eec\u5728\u6a21\u5757\u4e2d\u7684\u4f4d\u7f6e\u65e0\u5173\u3002\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\u9700\u8981\u8bc4\u4f30\u9000\u5f79\u7535\u6c60\u5185\u7684\u79bb\u6563\u5ea6\uff0c\u5e76\u4e3a\u4e8c\u6b21\u751f\u547d\u7535\u6c60\u5f00\u53d1\u70ed\u7ba1\u7406\u548c\u5e73\u8861\u7cfb\u7edf\u3002"}}
{"id": "2507.13871", "categories": ["cs.RO", "cs.CV", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.13871", "abs": "https://arxiv.org/abs/2507.13871", "authors": ["Mehul Anand", "Shishir Kolathaya"], "title": "Safety Certification in the Latent space using Control Barrier Functions and World Models", "comment": "6 pages, 6 figures. arXiv admin note: text overlap with\n  arXiv:2409.12616", "summary": "Synthesising safe controllers from visual data typically requires extensive\nsupervised labelling of safety-critical data, which is often impractical in\nreal-world settings. Recent advances in world models enable reliable prediction\nin latent spaces, opening new avenues for scalable and data-efficient safe\ncontrol. In this work, we introduce a semi-supervised framework that leverages\ncontrol barrier certificates (CBCs) learned in the latent space of a world\nmodel to synthesise safe visuomotor policies. Our approach jointly learns a\nneural barrier function and a safe controller using limited labelled data,\nwhile exploiting the predictive power of modern vision transformers for latent\ndynamics modelling.", "AI": {"tldr": "A new semi-supervised framework uses control barrier certificates in a world model's latent space to create safe controllers from visual data efficiently, reducing the need for extensive labeling.", "motivation": "Traditional methods for synthesizing safe controllers from visual data require extensive supervised labeling, which is often impractical. World models offer a potential solution for scalable and data-efficient safe control.", "method": "The paper proposes a semi-supervised framework that learns a neural barrier function and a safe controller by leveraging control barrier certificates (CBCs) in the latent space of a world model. The approach utilizes limited labeled data and vision transformers for latent dynamics modeling.", "result": "The framework successfully synthesizes safe visuomotor policies by jointly learning a neural barrier function and a safe controller using limited labeled data and the predictive power of vision transformers for latent dynamics modeling.", "conclusion": "Synthesizing safe controllers from visual data can be achieved with a semi-supervised framework that uses control barrier certificates in the latent space of a world model. This approach learns a neural barrier function and a safe controller with limited labeled data, utilizing the predictive capabilities of vision transformers for latent dynamics modeling."}}
{"id": "2507.14108", "categories": ["cond-mat.mes-hall", "quant-ph"], "pdf": "https://arxiv.org/pdf/2507.14108", "abs": "https://arxiv.org/abs/2507.14108", "authors": ["David W. Kanaar", "Yasuo Oda", "Mark F. Gyure", "J. P. Kestner"], "title": "Fast charge noise sensing using a spectator valley state in a singlet-triplet qubit", "comment": "11 pages, 3 figures", "summary": "Semiconductor spin qubits are a promising platform for quantum computing but\nremain vulnerable to charge noise. Accurate, in situ measurement of charge\nnoise could enable closed-loop control and improve qubit performance. Here, we\npropose a method for real-time detection of charge noise using a silicon\nsinglet-triplet qubit with one electron initialized in an excited valley state.\nThis valley excitation acts as a spectator degree of freedom, coupled to a\nhigh-quality resonator via the exchange interaction, which is sensitive to\ncharge-noise-induced voltage fluctuations. Dispersive readout of the resonator\nenables a continuous, classical measurement of exchange fluctuations during\nqubit operation. Signal-to-noise analysis shows that, under realistic device\nparameters, sub-microsecond measurement times are possible using a\nquantum-limited amplifier. Even without such an amplifier, sub-millisecond\nperformance is achievable with appropriately engineered resonator parameters.\nThis approach allows the probe to monitor slow drift in exchange in real time,\nopening the door to feedback and feedforward strategies for maintaining\nhigh-fidelity quantum operations. Importantly, the protocol preserves spin\ncoherence and can be run concurrently with qubit logic gates.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u7845\u5355\u6001-\u4e09\u6001\u91cf\u5b50\u6bd4\u7279\u76d1\u6d4b\u7535\u8377\u566a\u58f0\u7684\u65b9\u6cd5\uff0c\u53ef\u5b9e\u65f6\u76d1\u6d4b\u5e76\u63d0\u9ad8\u91cf\u5b50\u6bd4\u7279\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u5b9e\u73b0\u91cf\u5b50\u8ba1\u7b97\uff0c\u9700\u8981\u89e3\u51b3\u534a\u5bfc\u4f53\u81ea\u65cb\u91cf\u5b50\u6bd4\u7279\u6613\u53d7\u7535\u8377\u566a\u58f0\u5f71\u54cd\u7684\u95ee\u9898\u3002\u901a\u8fc7\u7cbe\u786e\u7684\u539f\u4f4d\u6d4b\u91cf\u7535\u8377\u566a\u58f0\uff0c\u53ef\u4ee5\u5b9e\u73b0\u95ed\u73af\u63a7\u5236\u5e76\u63d0\u9ad8\u91cf\u5b50\u6bd4\u7279\u7684\u6027\u80fd\u3002", "method": "\u5229\u7528\u4e00\u4e2a\u521d\u59cb\u5316\u5728\u88ab\u6fc0\u53d1\u7684\u8c37\u6001\u7684\u5355\u7535\u5b50\u7684\u7845\u5355\u6001-\u4e09\u6001\u91cf\u5b50\u6bd4\u7279\uff0c\u901a\u8fc7\u4ea4\u6362\u76f8\u4e92\u4f5c\u7528\u8026\u5408\u5230\u9ad8\u54c1\u8d28\u8c10\u632f\u5668\uff0c\u5b9e\u73b0\u5bf9\u91cf\u5b50\u6bd4\u7279\u7684\u8c37\u6fc0\u53d1\u4f5c\u4e3a\u65c1\u89c2\u81ea\u7531\u5ea6\uff0c\u5e76\u901a\u8fc7\u8c10\u632f\u5668\u7684\u8272\u6563\u8bfb\u51fa\uff0c\u5728\u91cf\u5b50\u6bd4\u7279\u64cd\u4f5c\u671f\u95f4\u5bf9\u7531\u7535\u8377\u566a\u58f0\u5f15\u8d77\u7684\u7535\u538b\u6da8\u843d\u8fdb\u884c\u8fde\u7eed\u7684\u7ecf\u5178\u6d4b\u91cf\u3002", "result": "\u5728\u5b9e\u9645\u5668\u4ef6\u53c2\u6570\u4e0b\uff0c\u4f7f\u7528\u91cf\u5b50\u9650\u5236\u653e\u5927\u5668\u53ef\u5728\u4e9a\u5fae\u79d2\u5185\u5b8c\u6210\u6d4b\u91cf\uff1b\u5373\u4f7f\u6ca1\u6709\u91cf\u5b50\u9650\u5236\u653e\u5927\u5668\uff0c\u901a\u8fc7\u4f18\u5316\u8c10\u632f\u5668\u53c2\u6570\u4e5f\u53ef\u5728\u4e9a\u6beb\u79d2\u5185\u5b9e\u73b0\u6d4b\u91cf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u91cf\u5b50\u6bd4\u7279\u64cd\u4f5c\u671f\u95f4\u8fdb\u884c\u5b9e\u65f6\u76d1\u6d4b\uff0c\u6709\u671b\u901a\u8fc7\u53cd\u9988\u548c\u524d\u9988\u7b56\u7565\u6765\u7ef4\u6301\u9ad8\u4fdd\u771f\u5ea6\u7684\u91cf\u5b50\u64cd\u4f5c\uff0c\u5e76\u4e14\u8be5\u534f\u8bae\u80fd\u591f\u4fdd\u6301\u81ea\u65cb\u76f8\u5e72\u6027\uff0c\u8fd8\u53ef\u4ee5\u4e0e\u91cf\u5b50\u903b\u8f91\u95e8\u5e76\u884c\u8fd0\u884c\u3002"}}
{"id": "2507.13544", "categories": ["cs.CL", "68T50, 05C85, 68T05, 68R10", "I.2.7; I.2.4; H.3.3; I.5.0"], "pdf": "https://arxiv.org/pdf/2507.13544", "abs": "https://arxiv.org/abs/2507.13544", "authors": ["Mohamed Achref Ben Ammar", "Mohamed Taha Bennani"], "title": "A Computational Approach to Modeling Conversational Systems: Analyzing Large-Scale Quasi-Patterned Dialogue Flows", "comment": null, "summary": "The analysis of conversational dynamics has gained increasing importance with\nthe rise of large language model-based systems, which interact with users\nacross diverse contexts. In this work, we propose a novel computational\nframework for constructing conversational graphs that capture the flow and\nstructure of loosely organized dialogues, referred to as quasi-patterned\nconversations. We introduce the Filter & Reconnect method, a novel graph\nsimplification technique that minimizes noise while preserving semantic\ncoherence and structural integrity of conversational graphs. Through\ncomparative analysis, we demonstrate that the use of large language models\ncombined with our graph simplification technique has resulted in semantic\nmetric S increasing by a factor of 2.06 compared to previous approaches while\nsimultaneously enforcing a tree-like structure with 0 {\\delta}-hyperbolicity,\nensuring optimal clarity in conversation modeling. This work provides a\ncomputational method for analyzing large-scale dialogue datasets, with\npractical applications related to monitoring automated systems such as\nchatbots, dialogue management tools, and user behavior analytics.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFilter & Reconnect\u7684\u8ba1\u7b97\u6846\u67b6\u548c\u56fe\u7b80\u5316\u6280\u672f\uff0c\u7528\u4e8e\u5206\u6790\u5bf9\u8bdd\u3002\u4e0e\u5148\u524d\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u6280\u672f\u5c06\u8bed\u4e49\u6307\u6807S\u63d0\u9ad8\u4e862.06\u500d\uff0c\u5e76\u589e\u5f3a\u4e86\u5bf9\u8bdd\u5efa\u6a21\u7684\u6e05\u6670\u5ea6\u3002", "motivation": "\u968f\u7740\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4ea4\u4e92\u7cfb\u7edf\u7684\u5174\u8d77\uff0c\u5206\u6790\u4f1a\u8bdd\u52a8\u6001\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFilter & Reconnect\u7684\u65b0\u578b\u56fe\u7b80\u5316\u6280\u672f\uff0c\u7528\u4e8e\u6784\u5efa\u6355\u83b7\u677e\u6563\u7ec4\u7ec7\u5bf9\u8bdd\uff08\u51c6\u6a21\u5f0f\u5bf9\u8bdd\uff09\u7684\u5bf9\u8bdd\u56fe\uff0c\u4ee5\u6700\u5c0f\u5316\u566a\u58f0\u5e76\u4fdd\u6301\u8bed\u4e49\u8fde\u8d2f\u6027\u548c\u7ed3\u6784\u5b8c\u6574\u6027\u3002", "result": "\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7ed3\u5408\u6240\u63d0\u51fa\u7684\u56fe\u7b80\u5316\u6280\u672f\uff0c\u4e0e\u5148\u524d\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8bed\u4e49\u6307\u6807S\u63d0\u9ad8\u4e862.06\u500d\uff0c\u540c\u65f6\u4fdd\u8bc1\u4e86\u6811\u72b6\u7ed3\u6784\u548c0-\u03b4\u53cc\u66f2\u6027\uff0c\u786e\u4fdd\u4e86\u5bf9\u8bdd\u5efa\u6a21\u7684\u6700\u4f73\u6e05\u6670\u5ea6\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u6784\u5efa\u5bf9\u8bdd\u56fe\u7684\u8ba1\u7b97\u6846\u67b6\uff0c\u7ed3\u5408Filter & Reconnect\u65b9\u6cd5\u7b80\u5316\u56fe\u7ed3\u6784\uff0c\u63d0\u9ad8\u4e86\u5bf9\u8bdd\u5206\u6790\u7684\u8bed\u4e49\u51c6\u786e\u6027\u548c\u7ed3\u6784\u6e05\u6670\u5ea6\uff0c\u5e76\u53ef\u5e94\u7528\u4e8e\u76d1\u63a7\u804a\u5929\u673a\u5668\u4eba\u7b49\u81ea\u52a8\u5316\u7cfb\u7edf\u3002"}}
{"id": "2507.13730", "categories": ["quant-ph", "cond-mat.dis-nn"], "pdf": "https://arxiv.org/pdf/2507.13730", "abs": "https://arxiv.org/abs/2507.13730", "authors": ["Jessica Barr", "Shreyasi Mukherjee", "Alessandro Ferraro", "Mauro Paternostro", "Giorgio Zicari"], "title": "A machine learning based approach to the identification of spectral densities in quantum open systems", "comment": "19 pages, 7 figures, comments welcome", "summary": "We present a machine learning-based approach for characterising the\nenvironment that affects the dynamics of an open quantum system. We focus on\nthe case of an exactly solvable spin-boson model, where the system-environment\ninteraction, whose strength is encoded in the spectral density, induces pure\ndephasing. By using artificial neural networks trained on the\nFourier-transformed time evolution of some observables of the system, we\nperform both classification -- distinguishing sub-Ohmic, Ohmic, and super-Ohmic\nspectral densities -- and regression -- thus estimating key parameters of the\nspectral density function, when the latter is expressed through a power law.\nOur results demonstrate high classification accuracy and robust parameter\nestimation, highlighting the potential of machine learning as a powerful tool\nfor probing environmental features in quantum systems and advancing quantum\nnoise spectroscopy.", "AI": {"tldr": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bad\u7ec3\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u6765\u8868\u5f81\u5f71\u54cd\u5f00\u653e\u91cf\u5b50\u7cfb\u7edf\u52a8\u529b\u5b66\u7684\u73af\u5883\uff0c\u5e76\u5728\u4e00\u4e2a\u81ea\u65cb-\u73bb\u8272\u5b50\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u5206\u7c7b\u548c\u53c2\u6570\u4f30\u8ba1\u3002", "motivation": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u65b9\u6cd5\u6765\u8868\u5f81\u5f71\u54cd\u5f00\u653e\u91cf\u5b50\u7cfb\u7edf\u52a8\u529b\u5b66\u7684\u73af\u5883\u3002", "method": "\u901a\u8fc7\u4f7f\u7528\u5728\u7cfb\u7edf\u53ef\u89c2\u5bdf\u91cf\u7684\u65f6\u95f4\u6f14\u5316\u5085\u91cc\u53f6\u53d8\u6362\u4e0a\u8bad\u7ec3\u7684\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\uff0c\u5bf9\u5b8c\u5168\u53ef\u89e3\u7684\u81ea\u65cb-\u73bb\u8272\u5b50\u6a21\u578b\u8fdb\u884c\u5206\u7c7b\uff08\u533a\u5206\u4e9a\u6b27\u59c6\u3001\u6b27\u59c6\u548c\u8d85\u6b27\u59c6\u5149\u8c31\u5bc6\u5ea6\uff09\u548c\u56de\u5f52\uff08\u4f30\u8ba1\u5149\u8c31\u5bc6\u5ea6\u51fd\u6570\u7684\u5173\u952e\u53c2\u6570\uff09\u3002", "result": "\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5b9e\u73b0\u4e86\u9ad8\u5206\u7c7b\u51c6\u786e\u7387\u548c\u9c81\u68d2\u7684\u53c2\u6570\u4f30\u8ba1\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u91cf\u5b50\u566a\u58f0\u5149\u8c31\u5b66\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "\u673a\u5668\u5b66\u4e60\u53ef\u4ee5\u4f5c\u4e3a\u63a2\u6d4b\u91cf\u5b50\u7cfb\u7edf\u4e2d\u73af\u5883\u7279\u5f81\u548c\u63a8\u8fdb\u91cf\u5b50\u566a\u58f0\u5149\u8c31\u5b66\u7684\u6709\u529b\u5de5\u5177\u3002"}}
{"id": "2507.13540", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13540", "abs": "https://arxiv.org/abs/2507.13540", "authors": ["Yongyi Yang", "Hidenori Tanaka", "Wei Hu"], "title": "Provable Low-Frequency Bias of In-Context Learning of Representations", "comment": null, "summary": "In-context learning (ICL) enables large language models (LLMs) to acquire new\nbehaviors from the input sequence alone without any parameter updates. Recent\nstudies have shown that ICL can surpass the original meaning learned in\npretraining stage through internalizing the structure the data-generating\nprocess (DGP) of the prompt into the hidden representations. However, the\nmechanisms by which LLMs achieve this ability is left open. In this paper, we\npresent the first rigorous explanation of such phenomena by introducing a\nunified framework of double convergence, where hidden representations converge\nboth over context and across layers. This double convergence process leads to\nan implicit bias towards smooth (low-frequency) representations, which we prove\nanalytically and verify empirically. Our theory explains several open empirical\nobservations, including why learned representations exhibit globally structured\nbut locally distorted geometry, and why their total energy decays without\nvanishing. Moreover, our theory predicts that ICL has an intrinsic robustness\ntowards high-frequency noise, which we empirically confirm. These results\nprovide new insights into the underlying mechanisms of ICL, and a theoretical\nfoundation to study it that hopefully extends to more general data\ndistributions and settings.", "AI": {"tldr": "ICL \u80fd\u591f\u901a\u8fc7\u5185\u5316\u63d0\u793a\u7684\u6570\u636e\u751f\u6210\u8fc7\u7a0b (DGP) \u6765\u5b66\u4e60\u65b0\u884c\u4e3a\uff0c\u5176\u673a\u5236\u5728\u4e8e\u201c\u53cc\u91cd\u6536\u655b\u201d\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5728\u4e0a\u4e0b\u6587\u548c\u5c42\u4e4b\u95f4\u6536\u655b\u9690\u85cf\u8868\u793a\uff0c\u4ece\u800c\u4ea7\u751f\u5bf9\u5e73\u6ed1\u8868\u793a\u7684\u504f\u89c1\uff0c\u5e76\u5177\u6709\u62b5\u6297\u9ad8\u9891\u566a\u58f0\u7684\u5185\u5728\u9c81\u68d2\u6027\u3002", "motivation": "\u63a2\u7a76 LLM \u901a\u8fc7 ICL \u4ece\u8f93\u5165\u5e8f\u5217\u4e2d\u5b66\u4e60\u65b0\u884c\u4e3a\uff08\u65e0\u9700\u53c2\u6570\u66f4\u65b0\uff09\u7684\u673a\u5236\uff0c\u4ee5\u53ca LLM \u5982\u4f55\u5185\u5316\u63d0\u793a\u7684\u6570\u636e\u751f\u6210\u8fc7\u7a0b (DGP) \u7684\u7ed3\u6784\u5230\u9690\u85cf\u8868\u793a\u4e2d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u53cc\u91cd\u6536\u655b\u201d\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u5176\u4e2d\u9690\u85cf\u8868\u793a\u5728\u4e0a\u4e0b\u6587\u548c\u5c42\u4e4b\u95f4\u8fdb\u884c\u6536\u655b\u3002", "result": "\u53cc\u91cd\u6536\u655b\u8fc7\u7a0b\u5bfc\u81f4\u5bf9\u5e73\u6ed1\uff08\u4f4e\u9891\uff09\u8868\u793a\u7684\u9690\u5f0f\u504f\u89c1\uff0c\u5e76\u901a\u8fc7\u5206\u6790\u8bc1\u660e\u548c\u7ecf\u9a8c\u9a8c\u8bc1\u3002\u8be5\u7406\u8bba\u89e3\u91ca\u4e86\u51e0\u4e2a\u60ac\u800c\u672a\u51b3\u7684\u7ecf\u9a8c\u89c2\u5bdf\u7ed3\u679c\uff0c\u4f8b\u5982\u4e3a\u4f55\u5b66\u4e60\u5230\u7684\u8868\u793a\u4f1a\u5c55\u73b0\u51fa\u5168\u5c40\u7ed3\u6784\u5316\u4f46\u5c40\u90e8\u5931\u771f\u7684\u51e0\u4f55\u5f62\u72b6\uff0c\u4ee5\u53ca\u4e3a\u4f55\u5b83\u4eec\u7684\u603b\u80fd\u91cf\u4f1a\u5728\u4e0d\u6d88\u5931\u7684\u60c5\u51b5\u4e0b\u8870\u51cf\u3002", "conclusion": "ICL \u7684\u5185\u5728\u9c81\u68d2\u6027\u53ef\u4ee5\u62b5\u6297\u9ad8\u9891\u566a\u58f0\uff0c\u5e76\u901a\u8fc7\u7ecf\u9a8c\u5f97\u5230\u8bc1\u5b9e\u3002\u8fd9\u4e9b\u7ed3\u679c\u4e3a ICL \u7684\u6f5c\u5728\u673a\u5236\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\uff0c\u5e76\u4e3a\u5176\u7814\u7a76\u5960\u5b9a\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u6709\u671b\u6269\u5c55\u5230\u66f4\u4e00\u822c\u7684\u6570\u636e\u5206\u5e03\u548c\u8bbe\u7f6e\u3002"}}
{"id": "2507.13386", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13386", "abs": "https://arxiv.org/abs/2507.13386", "authors": ["Yang Zhang", "Er Jin", "Yanfei Dong", "Yixuan Wu", "Philip Torr", "Ashkan Khakzar", "Johannes Stegmaier", "Kenji Kawaguchi"], "title": "Minimalist Concept Erasure in Generative Models", "comment": "ICML2025", "summary": "Recent advances in generative models have demonstrated remarkable\ncapabilities in producing high-quality images, but their reliance on\nlarge-scale unlabeled data has raised significant safety and copyright\nconcerns. Efforts to address these issues by erasing unwanted concepts have\nshown promise. However, many existing erasure methods involve excessive\nmodifications that compromise the overall utility of the model. In this work,\nwe address these issues by formulating a novel minimalist concept erasure\nobjective based \\emph{only} on the distributional distance of final generation\noutputs. Building on our formulation, we derive a tractable loss for\ndifferentiable optimization that leverages backpropagation through all\ngeneration steps in an end-to-end manner. We also conduct extensive analysis to\nshow theoretical connections with other models and methods. To improve the\nrobustness of the erasure, we incorporate neuron masking as an alternative to\nmodel fine-tuning. Empirical evaluations on state-of-the-art flow-matching\nmodels demonstrate that our method robustly erases concepts without degrading\noverall model performance, paving the way for safer and more responsible\ngenerative models.", "AI": {"tldr": "\u4e00\u79cd\u65b0\u7684\u6781\u7b80\u6982\u5ff5\u64e6\u9664\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u751f\u6210\u8f93\u51fa\u7684\u5206\u5e03\u8ddd\u79bb\u6765\u89e3\u51b3\u5b89\u5168\u548c\u7248\u6743\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u6a21\u578b\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5bf9\u5927\u89c4\u6a21\u65e0\u6807\u7b7e\u6570\u636e\u7684\u4f9d\u8d56\u5f15\u53d1\u4e86\u5b89\u5168\u548c\u7248\u6743\u95ee\u9898\u3002\u867d\u7136\u5df2\u6709\u64e6\u9664\u65e0\u5173\u6982\u5ff5\u7684\u65b9\u6cd5\uff0c\u4f46\u5b83\u4eec\u5f80\u5f80\u9700\u8981\u8fc7\u5ea6\u7684\u4fee\u6539\uff0c\u635f\u5bb3\u4e86\u6a21\u578b\u7684\u6574\u4f53\u6548\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u751f\u6210\u8f93\u51fa\u5206\u5e03\u8ddd\u79bb\u7684\u65b0\u9896\u6781\u7b80\u6982\u5ff5\u64e6\u9664\u76ee\u6807\uff0c\u5e76\u63a8\u5bfc\u51fa\u4e00\u4e2a\u53ef\u884c\u7684\u635f\u5931\u51fd\u6570\uff0c\u5229\u7528\u53cd\u5411\u4f20\u64ad\u8fdb\u884c\u7aef\u5230\u7aef\u4f18\u5316\u3002\u4e3a\u4e86\u63d0\u9ad8\u64e6\u9664\u7684\u9c81\u68d2\u6027\uff0c\u5f15\u5165\u4e86\u795e\u7ecf\u5143\u63a9\u7801\u4f5c\u4e3a\u6a21\u578b\u5fae\u8c03\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "result": "\u5728\u6700\u5148\u8fdb\u7684\u6d41\u5339\u914d\u6a21\u578b\u4e0a\u8fdb\u884c\u7684\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u7a33\u5065\u5730\u64e6\u9664\u6982\u5ff5\uff0c\u540c\u65f6\u4e0d\u635f\u5bb3\u6574\u4f53\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6781\u7b80\u6982\u5ff5\u64e6\u9664\u76ee\u6807\uff0c\u4ec5\u57fa\u4e8e\u6700\u7ec8\u751f\u6210\u8f93\u51fa\u7684\u5206\u5e03\u8ddd\u79bb\uff0c\u5e76\u901a\u8fc7\u53ef\u884c\u7684\u635f\u5931\u51fd\u6570\u5229\u7528\u53cd\u5411\u4f20\u64ad\u8fdb\u884c\u7aef\u5230\u7aef\u4f18\u5316\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u795e\u7ecf\u5143\u63a9\u7801\u800c\u975e\u6a21\u578b\u5fae\u8c03\u6765\u63d0\u9ad8\u64e6\u9664\u7684\u9c81\u68d2\u6027\uff0c\u5728\u6700\u5148\u8fdb\u7684\u6d41\u5339\u914d\u6a21\u578b\u4e0a\u8fdb\u884c\u4e86\u5b9e\u8bc1\u8bc4\u4f30\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u4e0d\u635f\u5bb3\u6574\u4f53\u6a21\u578b\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\u80fd\u6709\u6548\u64e6\u9664\u6982\u5ff5\uff0c\u4e3a\u66f4\u5b89\u5168\u3001\u66f4\u8d1f\u8d23\u4efb\u7684\u751f\u6210\u6a21\u578b\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2507.13874", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13874", "abs": "https://arxiv.org/abs/2507.13874", "authors": ["Mateusz Bystro\u0144ski", "Miko\u0142aj Ho\u0142ysz", "Grzegorz Piotrowski", "Nitesh V. Chawla", "Tomasz Kajdanowicz"], "title": "Large Language Models as Innovators: A Framework to Leverage Latent Space Exploration for Novelty Discovery", "comment": null, "summary": "Innovative idea generation remains a core challenge in AI, as large language\nmodels (LLMs) often struggle to produce outputs that are both novel and\nrelevant. Despite their fluency, LLMs tend to replicate patterns seen during\ntraining, limiting their ability to diverge creatively without extensive prompt\nengineering. Prior work has addressed this through domain-specific heuristics\nand structured prompting pipelines, but such solutions are brittle and\ndifficult to generalize. In this paper, we propose a model-agnostic\nlatent-space ideation framework that enables controlled, scalable creativity by\nnavigating the continuous embedding space of ideas. Unlike prior methods, our\nframework requires no handcrafted rules and adapts easily to different domains,\ninput formats, and creative tasks. This paper introduces an early-stage\nprototype of our method, outlining the conceptual framework and preliminary\nresults highlighting its potential as a general-purpose co-ideator for human-AI\ncollaboration.", "AI": {"tldr": "LLM \u96be\u4ee5\u4ea7\u751f\u65b0\u9896\u4e14\u76f8\u5173\u7684\u60f3\u6cd5\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u578b\u65e0\u5173\u7684\u6f5c\u5728\u7a7a\u95f4\u6784\u601d\u6846\u67b6\uff0c\u65e0\u9700\u624b\u5de5\u89c4\u5219\u5373\u53ef\u5b9e\u73b0\u53ef\u63a7\u3001\u53ef\u6269\u5c55\u7684\u521b\u9020\u529b\uff0c\u5e76\u6613\u4e8e\u9002\u5e94\u4e0d\u540c\u9886\u57df\u548c\u4efb\u52a1\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u4ea7\u751f\u65b0\u9896\u4e14\u76f8\u5173\u7684\u8f93\u65f6\u5e38\u9762\u4e34\u6311\u6218\uff0c\u5b83\u4eec\u503e\u5411\u4e8e\u590d\u5236\u8bad\u7ec3\u4e2d\u7684\u6a21\u5f0f\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u5728\u6ca1\u6709\u5e7f\u6cdb\u63d0\u793a\u5de5\u7a0b\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u521b\u9020\u6027\u53d1\u6563\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u4e0e\u6a21\u578b\u65e0\u5173\u7684\u6f5c\u5728\u7a7a\u95f4\u6784\u601d\u6846\u67b6\uff0c\u901a\u8fc7\u5bfc\u822a\u601d\u60f3\u7684\u8fde\u7eed\u5d4c\u5165\u7a7a\u95f4\u6765\u5b9e\u73b0\u53ef\u63a7\u3001\u53ef\u6269\u5c55\u7684\u521b\u9020\u529b\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u65e9\u671f\u539f\u578b\uff0c\u6982\u8ff0\u4e86\u6982\u5ff5\u6846\u67b6\u548c\u521d\u6b65\u7ed3\u679c\uff0c\u5f3a\u8c03\u4e86\u5176\u4f5c\u4e3a\u901a\u7528\u8054\u5408\u6784\u601d\u5668\u7684\u6f5c\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6f5c\u529b\u6210\u4e3a\u4eba\u673a\u534f\u4f5c\u7684\u901a\u7528\u8054\u5408\u6784\u601d\u5668\u3002"}}
{"id": "2507.14025", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.14025", "abs": "https://arxiv.org/abs/2507.14025", "authors": ["Wataru Hashimoto", "Kazumune Hashimoto", "Masako Kishida", "Shigemasa Takai"], "title": "Reference-Free Iterative Learning Model Predictive Control with Neural Certificates", "comment": "This paper was submitted to IET Control Theory & Applications on May\n  19, 2025 (under review)", "summary": "In this paper, we propose a novel reference-free iterative learning model\npredictive control (MPC). In the proposed method, a certificate function based\non the concept of Control Lyapunov Barrier Function (CLBF) is learned using\ndata collected from past control executions and used to define the terminal set\nand cost in the MPC optimization problem at the current iteration. This scheme\nenables the progressive refinement of the MPC's terminal components over\nsuccessive iterations. Unlike existing methods that rely on mixed-integer\nprogramming and suffer from numerical difficulties, the proposed approach\nformulates the MPC optimization problem as a standard nonlinear program,\nenabling more efficient online computation. The proposed method satisfies key\nMPC properties, including recursive feasibility and asymptotic stability.\nAdditionally, we demonstrate that the performance cost is non-increasing with\nrespect to the number of iterations, under certain assumptions. Numerical\nexperiments including the simulation with PyBullet confirm that our control\nscheme iteratively enhances control performance and significantly improves\nonline computational efficiency compared to the existing methods.", "AI": {"tldr": "\u4e00\u79cd\u65b0\u7684\u65e0\u53c2\u8003\u8fed\u4ee3\u5b66\u4e60MPC\u65b9\u6cd5\uff0c\u4f7f\u7528CLBF\u5b66\u4e60\u8bc1\u4e66\u51fd\u6570\u6765\u6539\u8fdb\u7ec8\u7aef\u96c6\u548c\u6210\u672c\uff0c\u5c06MPC\u4f18\u5316\u95ee\u9898\u8f6c\u5316\u4e3a\u975e\u7ebf\u6027\u89c4\u5212\uff0c\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\u548c\u63a7\u5236\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709MPC\u65b9\u6cd5\u4e2d\u4f9d\u8d56\u6df7\u5408\u6574\u6570\u89c4\u5212\u548c\u6570\u503c\u56f0\u96be\u7684\u95ee\u9898\uff0c\u5e76\u5b9e\u73b0MPC\u7ec8\u7aef\u7ec4\u4ef6\u7684\u6e10\u8fdb\u5f0f\u6539\u8fdb\u548c\u66f4\u6709\u6548\u7684\u5728\u7ebf\u8ba1\u7b97\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65e0\u53c2\u8003\u8fed\u4ee3\u5b66\u4e60\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u4ece\u8fc7\u53bb\u63a7\u5236\u6267\u884c\u4e2d\u6536\u96c6\u7684\u6570\u636e\u5b66\u4e60\u57fa\u4e8e\u63a7\u5236Lyapunov\u969c\u788d\u51fd\u6570\uff08CLBF\uff09\u6982\u5ff5\u7684\u8bc1\u4e66\u51fd\u6570\uff0c\u5e76\u5229\u7528\u8be5\u51fd\u6570\u6765\u5b9a\u4e49\u5f53\u524d\u8fed\u4ee3\u4e2dMPC\u4f18\u5316\u95ee\u9898\u7684\u7ec8\u7aef\u96c6\u548c\u6210\u672c\u3002\u8fd9\u4f7f\u5f97MPC\u7684\u7ec8\u7aef\u7ec4\u4ef6\u80fd\u591f\u901a\u8fc7\u8fde\u7eed\u8fed\u4ee3\u8fdb\u884c\u6e10\u8fdb\u5f0f\u6539\u8fdb\u3002\u4e0e\u4f9d\u8d56\u6df7\u5408\u6574\u6570\u89c4\u5212\u4e14\u5b58\u5728\u6570\u503c\u56f0\u96be\u7684\u73b0\u6709\u65b9\u6cd5\u4e0d\u540c\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5c06MPC\u4f18\u5316\u95ee\u9898\u5f62\u5f0f\u5316\u4e3a\u6807\u51c6\u975e\u7ebf\u6027\u89c4\u5212\uff0c\u4ece\u800c\u5b9e\u73b0\u66f4\u6709\u6548\u7684\u5728\u7ebf\u8ba1\u7b97\u3002", "result": "\u8be5\u63a7\u5236\u65b9\u6848\u8fed\u4ee3\u5730\u63d0\u9ad8\u4e86\u63a7\u5236\u6027\u80fd\uff0c\u5e76\u663e\u8457\u63d0\u9ad8\u4e86\u5728\u7ebf\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6ee1\u8db3MPC\u7684\u9012\u5f52\u53ef\u884c\u6027\u548c\u6e10\u8fd1\u7a33\u5b9a\u6027\u7b49\u5173\u952e\u6027\u8d28\uff0c\u5e76\u4e14\u5728\u7279\u5b9a\u5047\u8bbe\u4e0b\uff0c\u6027\u80fd\u6210\u672c\u968f\u8fed\u4ee3\u6b21\u6570\u7684\u589e\u52a0\u800c\u975e\u589e\u52a0\u3002\u6570\u503c\u5b9e\u9a8c\u548cPyBullet\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u63a7\u5236\u65b9\u6848\u80fd\u8fed\u4ee3\u5730\u63d0\u9ad8\u63a7\u5236\u6027\u80fd\uff0c\u5e76\u663e\u8457\u63d0\u5347\u5728\u7ebf\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2507.13903", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.13903", "abs": "https://arxiv.org/abs/2507.13903", "authors": ["Ziliang Li", "Hongming Chen", "Yiyang Lin", "Biyu Ye", "Ximin Lyu"], "title": "AeroThrow: An Autonomous Aerial Throwing System for Precise Payload Delivery", "comment": null, "summary": "Autonomous aerial systems play an increasingly vital role in a wide range of\napplications, particularly for transport and delivery tasks in complex\nenvironments. In airdrop missions, these platforms face the dual challenges of\nabrupt control mode switching and inherent system delays along with control\nerrors. To address these issues, this paper presents an autonomous airdrop\nsystem based on an aerial manipulator (AM). The introduction of additional\nactuated degrees of freedom enables active compensation for UAV tracking\nerrors. By imposing smooth and continuous constraints on the parabolic landing\npoint, the proposed approach generates aerial throwing trajectories that are\nless sensitive to the timing of payload release. A hierarchical disturbance\ncompensation strategy is incorporated into the Nonlinear Model Predictive\nControl (NMPC) framework to mitigate the effects of sudden changes in system\nparameters, while the predictive capabilities of NMPC are further exploited to\nimprove the precision of aerial throwing. Both simulation and real-world\nexperimental results demonstrate that the proposed system achieves greater\nagility and precision in airdrop missions.", "AI": {"tldr": "\u65e0\u4eba\u673a\u7a7a\u6295\u4efb\u52a1\u9762\u4e34\u6a21\u5f0f\u5207\u6362\u3001\u7cfb\u7edf\u5ef6\u8fdf\u548c\u63a7\u5236\u8bef\u5dee\u7684\u6311\u6218\u3002\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7a7a\u4e2d\u673a\u68b0\u81c2\uff08AM\uff09\u7684\u81ea\u4e3b\u7a7a\u6295\u7cfb\u7edf\uff0c\u901a\u8fc7\u5f15\u5165\u989d\u5916\u7684\u9a71\u52a8\u81ea\u7531\u5ea6\u6765\u8865\u507f\u65e0\u4eba\u673a\u8ddf\u8e2a\u8bef\u5dee\uff0c\u5e76\u91c7\u7528\u5206\u5c42\u6270\u52a8\u8865\u507f\u7b56\u7565\u548c\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08NMPC\uff09\u6765\u63d0\u9ad8\u8f68\u8ff9\u751f\u6210\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\uff0c\u6700\u7ec8\u5728\u4eff\u771f\u548c\u5b9e\u9a8c\u4e2d\u8bc1\u660e\u4e86\u8be5\u7cfb\u7edf\u5728\u7a7a\u6295\u4efb\u52a1\u4e2d\u7684\u654f\u6377\u6027\u548c\u7cbe\u786e\u6027\u3002", "motivation": "Autonomous aerial systems are vital for tasks like transport and delivery in complex environments. Airdrop missions face challenges from abrupt control mode switching, system delays, and control errors. This paper addresses these by using an aerial manipulator (AM) to actively compensate for UAV tracking errors and improve airdrop precision.", "method": "The proposed approach generates aerial throwing trajectories by imposing smooth and continuous constraints on the parabolic landing point, making them less sensitive to payload release timing. A hierarchical disturbance compensation strategy is incorporated into the Nonlinear Model Predictive Control (NMPC) framework to mitigate parameter changes and improve precision.", "result": "Both simulation and real-world experimental results demonstrate that the proposed system achieves greater agility and precision in airdrop missions.", "conclusion": "\u8be5\u7814\u7a76 presented an autonomous airdrop system based on an aerial manipulator (AM)\uff0cwhich improves agility and precision in airdrop missions through active compensation for UAV tracking errors and hierarchical disturbance compensation integrated into NMPC."}}
{"id": "2507.14115", "categories": ["cond-mat.mes-hall", "cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2507.14115", "abs": "https://arxiv.org/abs/2507.14115", "authors": ["Navketan Batra", "Zezhu Wei", "Smitha Vishweshwara", "D. E. Feldman"], "title": "Anyonic analogue of optical Mach-Zehnder interferometer", "comment": "25 pages, 8 figures", "summary": "Anyonic interferometry is a direct probe of fractional statistics. We propose\nan interferometry geometry that parallels an optical Mach-Zehnder\ninterferometer and offers several advantages over existing interferometry\nschemes. In contrast to the currently studied electronic Mach-Zehnder\ninterferometer, our setup has no drain inside the device so that the trapped\ntopological charge is time-independent. In contrast to electronic Fabry-P\\'erot\ninterferometry, anyons cannot go around the device more than once. Thus, the\ninterference signal has a straightforward interpretation in terms of anyonic\nstatistical phases. The proposed geometry suppresses the undesirable effects of\nbulk-edge coupling. Moreover, the setup allows for simple exact solutions for\nthe electric current and noise for an arbitrary quasiparticle tunneling\nstrength in a broad range of conditions. The structure of the solutions is\nsimilar to that for non-interacting electrons but reflects fractional charge\nand statistics. We present results for electric current and noise in Jain\nstates and address thermal interferometry at zero voltage bias.", "AI": {"tldr": "A new Mach-Zehnder interferometer design for anyons simplifies interference signal interpretation and analysis by avoiding drains and limiting anyon paths, with solutions applicable to Jain states and zero-voltage thermal interferometry.", "motivation": "To provide a direct probe of fractional statistics through an interferometry geometry that parallels an optical Mach-Zehnder interferometer and offers advantages over existing schemes.", "method": "Proposed a Mach-Zehnder interferometer geometry for anyonic interferometry, allowing for simple exact solutions for electric current and noise.", "result": "Presented results for electric current and noise in Jain states and addressed thermal interferometry at zero voltage bias, demonstrating solutions similar to non-interacting electrons but reflecting fractional charge and statistics.", "conclusion": "The proposed Mach-Zehnder interferometer geometry for anyonic interferometry offers advantages over existing schemes, providing a straightforward interpretation of interference signals and suppressing bulk-edge coupling effects."}}
{"id": "2507.13551", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13551", "abs": "https://arxiv.org/abs/2507.13551", "authors": ["Feng Chen", "Weizhe Xu", "Changye Li", "Serguei Pakhomov", "Alex Cohen", "Simran Bhola", "Sandy Yin", "Sunny X Tang", "Michael Mackinley", "Lena Palaniyappan", "Dror Ben-Zeev", "Trevor Cohen"], "title": "Reading Between the Lines: Combining Pause Dynamics and Semantic Coherence for Automated Assessment of Thought Disorder", "comment": null, "summary": "Formal thought disorder (FTD), a hallmark of schizophrenia spectrum\ndisorders, manifests as incoherent speech and poses challenges for clinical\nassessment. Traditional clinical rating scales, though validated, are\nresource-intensive and lack scalability. Automated speech analysis with\nautomatic speech recognition (ASR) allows for objective quantification of\nlinguistic and temporal features of speech, offering scalable alternatives. The\nuse of utterance timestamps in ASR captures pause dynamics, which are thought\nto reflect the cognitive processes underlying speech production. However, the\nutility of integrating these ASR-derived features for assessing FTD severity\nrequires further evaluation. This study integrates pause features with semantic\ncoherence metrics across three datasets: naturalistic self-recorded diaries\n(AVH, n = 140), structured picture descriptions (TOPSY, n = 72), and dream\nnarratives (PsyCL, n = 43). We evaluated pause related features alongside\nestablished coherence measures, using support vector regression (SVR) to\npredict clinical FTD scores. Key findings demonstrate that pause features alone\nrobustly predict the severity of FTD. Integrating pause features with semantic\ncoherence metrics enhanced predictive performance compared to semantic-only\nmodels, with integration of independent models achieving correlations up to\n\\r{ho} = 0.649 and AUC = 83.71% for severe cases detection (TOPSY, with best\n\\r{ho} = 0.584 and AUC = 79.23% for semantic-only models). The performance\ngains from semantic and pause features integration held consistently across all\ncontexts, though the nature of pause patterns was dataset-dependent. These\nfindings suggest that frameworks combining temporal and semantic analyses\nprovide a roadmap for refining the assessment of disorganized speech and\nadvance automated speech analysis in psychosis.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc1\u660e\uff0c\u7ed3\u5408\u8bed\u97f3\u505c\u987f\u548c\u8bed\u4e49\u5206\u6790\u7684\u65b9\u6cd5\u6bd4\u5355\u72ec\u5206\u6790\u8bed\u4e49\u66f4\u80fd\u51c6\u786e\u5730\u9884\u6d4b\u7cbe\u795e\u5206\u88c2\u75c7\u60a3\u8005\u7684\u601d\u7ef4\u5f62\u5f0f\u969c\u788d\uff08FTD\uff09\u4e25\u91cd\u7a0b\u5ea6\uff0c\u4e3a\u5f00\u53d1\u66f4\u6709\u6548\u7684\u81ea\u52a8\u5316\u8bc4\u4f30\u5de5\u5177\u63d0\u4f9b\u4e86\u4f9d\u636e\u3002", "motivation": "\u4f20\u7edfFTD\u4e34\u5e8a\u8bc4\u4f30\u65b9\u6cd5\u8017\u65f6\u4e14\u96be\u4ee5\u6269\u5c55\u3002\u81ea\u52a8\u8bed\u97f3\u5206\u6790\uff0c\u7279\u522b\u662f\u5229\u7528ASR\u63d0\u53d6\u7684\u505c\u987f\u7279\u5f81\uff0c\u6709\u671b\u63d0\u4f9b\u66f4\u5ba2\u89c2\u3001\u53ef\u6269\u5c55\u7684\u8bc4\u4f30\u624b\u6bb5\uff0c\u4f46\u5176\u5728FTD\u8bc4\u4f30\u4e2d\u7684\u6548\u7528\u5c1a\u9700\u9a8c\u8bc1\u3002", "method": "\u672c\u7814\u7a76\u5229\u7528\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u6280\u672f\u63d0\u53d6\u8a00\u8bed\u4e2d\u7684\u505c\u987f\u7279\u5f81\uff0c\u5e76\u7ed3\u5408\u8bed\u4e49\u8fde\u8d2f\u6027\u6307\u6807\uff0c\u4f7f\u7528\u652f\u6301\u5411\u91cf\u56de\u5f52\uff08SVR\uff09\u6a21\u578b\u6765\u9884\u6d4b\u4e34\u5e8aFTD\u8bc4\u5206\u3002\u7814\u7a76\u5728\u4e09\u4e2a\u4e0d\u540c\u6570\u636e\u96c6\uff08\u81ea\u7136\u65e5\u8bb0\u3001\u56fe\u7247\u63cf\u8ff0\u3001\u68a6\u5883\u53d9\u8ff0\uff09\u4e0a\u8bc4\u4f30\u4e86\u8fd9\u4e9b\u7279\u5f81\u7684\u9884\u6d4b\u80fd\u529b\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5355\u72ec\u7684\u505c\u987f\u7279\u5f81\u80fd\u591f\u6709\u6548\u9884\u6d4bFTD\u4e25\u91cd\u7a0b\u5ea6\u3002\u5c06\u505c\u987f\u7279\u5f81\u4e0e\u8bed\u4e49\u8fde\u8d2f\u6027\u6307\u6807\u76f8\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u9884\u6d4b\u6027\u80fd\uff08\u5728TOPSY\u6570\u636e\u96c6\u4e2d\uff0c\u8054\u5408\u6a21\u578b\u76f8\u5173\u7cfb\u6570\u6700\u9ad8\u53ef\u8fbe0.649\uff0cAUC\u4e3a83.71%\uff09\uff0c\u4f18\u4e8e\u4ec5\u4f7f\u7528\u8bed\u4e49\u7279\u5f81\u7684\u6a21\u578b\uff08\u6700\u4f73\u76f8\u5173\u7cfb\u65700.584\uff0cAUC\u4e3a79.23%\uff09\u3002\u4e0d\u540c\u6570\u636e\u96c6\u7684\u505c\u987f\u6a21\u5f0f\u6709\u6240\u5dee\u5f02\uff0c\u4f46\u65f6\u95f4\u4e0e\u8bed\u4e49\u7279\u5f81\u7684\u7ed3\u5408\u4f18\u52bf\u5728\u6240\u6709\u6570\u636e\u96c6\u4e2d\u5747\u5f97\u5230\u4f53\u73b0\u3002", "conclusion": "\u7ed3\u5408\u65f6\u95f4\uff08\u505c\u987f\uff09\u548c\u8bed\u4e49\u7684\u8bed\u97f3\u5206\u6790\u65b9\u6cd5\u5728\u8bc4\u4f30\u7cbe\u795e\u5206\u88c2\u75c7\u8c31\u7cfb\u969c\u788d\u4e2d\u7684\u601d\u7ef4\u5f62\u5f0f\u969c\u788d\uff08FTD\uff09\u65b9\u9762\u4f18\u4e8e\u5355\u72ec\u4f7f\u7528\u8bed\u4e49\u7279\u5f81\uff0c\u80fd\u591f\u66f4\u6709\u6548\u5730\u9884\u6d4bFTD\u7684\u4e25\u91cd\u7a0b\u5ea6\uff0c\u5e76\u4e3a\u81ea\u52a8\u5316\u8bed\u97f3\u5206\u6790\u5728\u7cbe\u795e\u75c5\u5b66\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2507.13735", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2507.13735", "abs": "https://arxiv.org/abs/2507.13735", "authors": ["Luc\u00eda \u00c1lvarez", "Alfredo Luis"], "title": "Impact of quadrature measurement on quantum coherence", "comment": "7 pages, 9 figures", "summary": "We examine the behavior of quadrature coherence under the measurement of the\nsame field quadrature. This is carried out with the help of a beam splitter,\nthat implies the contribution of the field state impinging at the other input\nport. We examine the case of Gaussian and number states.", "AI": {"tldr": "This paper studies quadrature coherence using a beam splitter, looking at Gaussian and number states.", "motivation": "To examine the behavior of quadrature coherence under the measurement of the same field quadrature.", "method": "The study uses a beam splitter to examine the behavior of quadrature coherence under the measurement of the same field quadrature. The contribution of the field state impinging at the other input port is also considered.", "result": "The paper examines the behavior of quadrature coherence for Gaussian and number states.", "conclusion": "The paper examines the behavior of quadrature coherence under the measurement of the same field quadrature, considering both Gaussian and number states using a beam splitter."}}
{"id": "2507.13542", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13542", "abs": "https://arxiv.org/abs/2507.13542", "authors": ["Beka Begiashvili", "Carlos J. Fernandez-Candel", "Mat\u00edas P\u00e9rez Paredes"], "title": "Acoustic Index: A Novel AI-Driven Parameter for Cardiac Disease Risk Stratification Using Echocardiography", "comment": null, "summary": "Traditional echocardiographic parameters such as ejection fraction (EF) and\nglobal longitudinal strain (GLS) have limitations in the early detection of\ncardiac dysfunction. EF often remains normal despite underlying pathology, and\nGLS is influenced by load conditions and vendor variability. There is a growing\nneed for reproducible, interpretable, and operator-independent parameters that\ncapture subtle and global cardiac functional alterations.\n  We introduce the Acoustic Index, a novel AI-derived echocardiographic\nparameter designed to quantify cardiac dysfunction from standard ultrasound\nviews. The model combines Extended Dynamic Mode Decomposition (EDMD) based on\nKoopman operator theory with a hybrid neural network that incorporates clinical\nmetadata. Spatiotemporal dynamics are extracted from echocardiographic\nsequences to identify coherent motion patterns. These are weighted via\nattention mechanisms and fused with clinical data using manifold learning,\nresulting in a continuous score from 0 (low risk) to 1 (high risk).\n  In a prospective cohort of 736 patients, encompassing various cardiac\npathologies and normal controls, the Acoustic Index achieved an area under the\ncurve (AUC) of 0.89 in an independent test set. Cross-validation across five\nfolds confirmed the robustness of the model, showing that both sensitivity and\nspecificity exceeded 0.8 when evaluated on independent data. Threshold-based\nanalysis demonstrated stable trade-offs between sensitivity and specificity,\nwith optimal discrimination near this threshold.\n  The Acoustic Index represents a physics-informed, interpretable AI biomarker\nfor cardiac function. It shows promise as a scalable, vendor-independent tool\nfor early detection, triage, and longitudinal monitoring. Future directions\ninclude external validation, longitudinal studies, and adaptation to\ndisease-specific classifiers.", "AI": {"tldr": "A new AI-derived parameter called the Acoustic Index, developed using EDMD and a hybrid neural network, shows high accuracy (AUC 0.89) in detecting cardiac dysfunction from echocardiograms, outperforming traditional methods and offering a scalable, interpretable, and vendor-independent tool for early detection and monitoring.", "motivation": "Traditional echocardiographic parameters like ejection fraction (EF) and global longitudinal strain (GLS) have limitations in early detecting cardiac dysfunction due to factors like normal EF values despite pathology and GLS's susceptibility to load conditions and vendor variability. There is a need for reproducible, interpretable, and operator-independent parameters for subtle cardiac functional alterations.", "method": "The study utilized Extended Dynamic Mode Decomposition (EDMD) based on Koopman operator theory combined with a hybrid neural network that incorporates clinical metadata. Spatiotemporal dynamics were extracted from echocardiographic sequences to identify coherent motion patterns, which were then weighted via attention mechanisms and fused with clinical data using manifold learning to generate a continuous risk score.", "result": "In a prospective cohort of 736 patients, the Acoustic Index achieved an AUC of 0.89 on an independent test set. Cross-validation confirmed robustness, with sensitivity and specificity exceeding 0.8 on independent data. The parameter demonstrated stable trade-offs between sensitivity and specificity.", "conclusion": "The Acoustic Index, a novel AI-derived parameter, shows promise as a scalable, vendor-independent tool for early detection, triage, and longitudinal monitoring of cardiac dysfunction. Future research will focus on external validation, longitudinal studies, and disease-specific adaptations."}}
{"id": "2507.13387", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.13387", "abs": "https://arxiv.org/abs/2507.13387", "authors": ["Chihiro Noguchi", "Takaki Yamamoto"], "title": "From Binary to Semantic: Utilizing Large-Scale Binary Occupancy Data for 3D Semantic Occupancy Prediction", "comment": "Accepted to ICCV Workshop 2025", "summary": "Accurate perception of the surrounding environment is essential for safe\nautonomous driving. 3D occupancy prediction, which estimates detailed 3D\nstructures of roads, buildings, and other objects, is particularly important\nfor vision-centric autonomous driving systems that do not rely on LiDAR\nsensors. However, in 3D semantic occupancy prediction -- where each voxel is\nassigned a semantic label -- annotated LiDAR point clouds are required, making\ndata acquisition costly. In contrast, large-scale binary occupancy data, which\nonly indicate occupied or free space without semantic labels, can be collected\nat a lower cost. Despite their availability, the potential of leveraging such\ndata remains unexplored. In this study, we investigate the utilization of\nlarge-scale binary occupancy data from two perspectives: (1) pre-training and\n(2) learning-based auto-labeling. We propose a novel binary occupancy-based\nframework that decomposes the prediction process into binary and semantic\noccupancy modules, enabling effective use of binary occupancy data. Our\nexperimental results demonstrate that the proposed framework outperforms\nexisting methods in both pre-training and auto-labeling tasks, highlighting its\neffectiveness in enhancing 3D semantic occupancy prediction. The code is\navailable at https://github.com/ToyotaInfoTech/b2s-occupancy", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u4f4e\u6210\u672c\u4e8c\u503c\u5360\u7528\u6570\u636e\u8fdb\u884c3D\u8bed\u4e49\u5360\u7528\u9884\u6d4b\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u548c\u81ea\u52a8\u6807\u6ce8\u4e24\u79cd\u65b9\u5f0f\uff0c\u6709\u6548\u63d0\u5347\u4e86\u9884\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u4e3a\u4e86\u964d\u4f4e3D\u8bed\u4e49\u5360\u7528\u9884\u6d4b\u4e2d\u6570\u636e\u91c7\u96c6\u7684\u6210\u672c\uff0c\u672c\u6587\u65e8\u5728\u63a2\u7d22\u5229\u7528\u5927\u89c4\u6a21\u4e8c\u503c\u5360\u7528\u6570\u636e\u7684\u6f5c\u529b\uff0c\u5e76\u63d0\u51fa\u4e86\u4e24\u79cd\u5229\u7528\u65b9\u5f0f\uff1a\u9884\u8bad\u7ec3\u548c\u5b66\u4e60\u578b\u81ea\u52a8\u6807\u6ce8\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u4e8c\u503c\u5360\u7528\u9884\u6d4b\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5c06\u9884\u6d4b\u8fc7\u7a0b\u5206\u89e3\u4e3a\u4e8c\u503c\u548c\u8bed\u4e49\u5360\u7528\u6a21\u5757\uff0c\u80fd\u591f\u6709\u6548\u5229\u7528\u4e8c\u503c\u5360\u7528\u6570\u636e\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u6846\u67b6\u57283D\u8bed\u4e49\u5360\u7528\u9884\u6d4b\u4efb\u52a1\u4e2d\uff0c\u65e0\u8bba\u662f\u5728\u9884\u8bad\u7ec3\u8fd8\u662f\u81ea\u52a8\u6807\u6ce8\u65b9\u9762\uff0c\u90fd\u53d6\u5f97\u4e86\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u597d\u7684\u6027\u80fd\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6846\u67b6\u5728\u9884\u8bad\u7ec3\u548c\u81ea\u52a8\u6807\u6ce8\u4efb\u52a1\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6709\u6548\u5730\u63d0\u5347\u4e863D\u8bed\u4e49\u5360\u7528\u9884\u6d4b\u7684\u6027\u80fd\u3002"}}
{"id": "2507.13956", "categories": ["cs.AI", "cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.13956", "abs": "https://arxiv.org/abs/2507.13956", "authors": ["Yutao Jin", "Haowen Xiao", "Jielei Chu", "Fengmao Lv", "Yuxiao Li", "Tianrui Li"], "title": "Cross-modal Causal Intervention for Alzheimer's Disease Prediction", "comment": null, "summary": "Mild Cognitive Impairment (MCI) serves as a prodromal stage of Alzheimer's\nDisease (AD), where early identification and intervention can effectively slow\nthe progression to dementia. However, diagnosing AD remains a significant\nchallenge in neurology due to the confounders caused mainly by the selection\nbias of multimodal data and the complex relationships between variables. To\naddress these issues, we propose a novel visual-language causal intervention\nframework named Alzheimer's Disease Prediction with Cross-modal Causal\nIntervention (ADPC) for diagnostic assistance. Our ADPC employs large language\nmodel (LLM) to summarize clinical data under strict templates, maintaining\nstructured text outputs even with incomplete or unevenly distributed datasets.\nThe ADPC model utilizes Magnetic Resonance Imaging (MRI), functional MRI (fMRI)\nimages and textual data generated by LLM to classify participants into\nCognitively Normal (CN), MCI, and AD categories. Because of the presence of\nconfounders, such as neuroimaging artifacts and age-related biomarkers,\nnon-causal models are likely to capture spurious input-output correlations,\ngenerating less reliable results. Our framework implicitly eliminates\nconfounders through causal intervention. Experimental results demonstrate the\noutstanding performance of our method in distinguishing CN/MCI/AD cases,\nachieving state-of-the-art (SOTA) metrics across most evaluation metrics. The\nstudy showcases the potential of integrating causal reasoning with multi-modal\nlearning for neurological disease diagnosis.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u56e0\u679c\u63a8\u7406\u548c\u591a\u6a21\u6001\u5b66\u4e60\u7684AD\u8bca\u65ad\u65b0\u65b9\u6cd5ADPC\uff0c\u4f7f\u7528LLM\u5904\u7406\u4e34\u5e8a\u6587\u672c\uff0c\u7ed3\u5408MRI/fMRI\u56fe\u50cf\u8fdb\u884c\u5206\u7c7b\uff0c\u80fd\u6709\u6548\u5904\u7406\u6df7\u6dc6\u53d8\u91cf\uff0c\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\uff08AD\uff09\u7684\u65e9\u671f\u8bc6\u522b\u548c\u5e72\u9884\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u591a\u6a21\u6001\u6570\u636e\u9009\u62e9\u504f\u5dee\u548c\u53d8\u91cf\u95f4\u590d\u6742\u5173\u7cfb\u7b49\u6df7\u6dc6\u56e0\u7d20\uff0c\u5176\u8bca\u65ad\u4ecd\u7136\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aADPC\uff08Alzheimer's Disease Prediction with Cross-modal Causal Intervention\uff09\u7684\u65b0\u578b\u89c6\u89c9-\u8bed\u8a00\u56e0\u679c\u5e72\u9884\u6846\u67b6\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u603b\u7ed3\u4e34\u5e8a\u6570\u636e\uff0c\u5e76\u7ed3\u5408MRI\u3001fMRI\u56fe\u50cf\u548c\u6587\u672c\u6570\u636e\u5bf9\u53c2\u4e0e\u8005\u8fdb\u884c\u5206\u7c7b\uff08\u8ba4\u77e5\u6b63\u5e38CN\u3001\u8f7b\u5ea6\u8ba4\u77e5\u969c\u788dMCI\u3001\u963f\u5c14\u8328\u6d77\u9ed8\u75c5AD\uff09\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u56e0\u679c\u5e72\u9884\u6d88\u9664\u6df7\u6dc6\u53d8\u91cf\uff08\u5982\u795e\u7ecf\u5f71\u50cf\u4f2a\u5f71\u548c\u4e0e\u5e74\u9f84\u76f8\u5173\u7684\u751f\u7269\u6807\u5fd7\u7269\uff09\uff0c\u907f\u514d\u975e\u56e0\u679c\u6a21\u578b\u53ef\u80fd\u4ea7\u751f\u7684\u865a\u5047\u76f8\u5173\u6027\u3002", "result": "ADPC\u5728\u533a\u5206CN/MCI/AD\u75c5\u4f8b\u65b9\u9762\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6027\u80fd\uff0c\u5728\u5927\u591a\u6570\u8bc4\u4f30\u6307\u6807\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\uff08SOTA\uff09\u7684\u6c34\u5e73\u3002", "conclusion": "\u901a\u8fc7\u56e0\u679c\u63a8\u7406\u4e0e\u591a\u6a21\u6001\u5b66\u4e60\u7684\u7ed3\u5408\uff0c\u4e3a\u795e\u7ecf\u7cfb\u7edf\u75be\u75c5\u7684\u8bca\u65ad\u63d0\u4f9b\u4e86\u65b0\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.14052", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.14052", "abs": "https://arxiv.org/abs/2507.14052", "authors": ["Mingdao Lin", "Max Bolderman", "Mircea Lazar"], "title": "Physics-guided gated recurrent units for inversion-based feedforward control", "comment": "8 pages", "summary": "Inversion-based feedforward control relies on an accurate model that\ndescribes the inverse system dynamics. The gated recurrent unit (GRU), which is\na recent architecture in recurrent neural networks, is a strong candidate for\nobtaining such a model from data. However, due to their black-box nature, GRUs\nface challenges such as limited interpretability and vulnerability to\noverfitting. Recently, physics-guided neural networks (PGNNs) have been\nintroduced, which integrate the prior physical model structure into the\nprediction process. This approach not only improves training convergence, but\nalso facilitates the learning of a physics-based model. In this work, we\nintegrate a GRU in the PGNN framework to obtain a PG-GRU, based on which we\nadopt a two-step approach to feedforward control design. First, we adopt stable\ninversion techniques to design a stable linear model of the inverse dynamics.\nThen, a GRU trained on the residual is tailored to inverse system\nidentification. The resulting PG-GRU feedforward controller is validated by\nmeans of real-life experiments on a two-mass spring-damper system, where it\ndemonstrates roughly a two-fold improvement compared to the linear feedforward\nand a preview-based GRU feedforward in terms of the integral absolute error.", "AI": {"tldr": "\u63d0\u51faPG-GRU\u6a21\u578b\uff0c\u7ed3\u5408\u7269\u7406\u4fe1\u606f\u548cGRU\uff0c\u7528\u4e8e\u524d\u9988\u63a7\u5236\uff0c\u5728\u4e24\u8d28\u91cf\u5f39\u7c27\u963b\u5c3c\u7cfb\u7edf\u5b9e\u9a8c\u4e2d\u6548\u679c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u57fa\u4e8e\u9006\u6a21\u578b\u7684\u524d\u9988\u63a7\u5236\u9700\u8981\u7cbe\u786e\u9006\u7cfb\u7edf\u6a21\u578b\uff0c\u4ee5\u53caGRU\u6a21\u578b\u5b58\u5728\u53ef\u89e3\u91ca\u6027\u5dee\u548c\u6613\u8fc7\u62df\u5408\u7684\u95ee\u9898\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u5c06GRU\u4e0e\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PGNN\uff09\u7ed3\u5408\uff0c\u4ee5\u63d0\u9ad8\u6a21\u578b\u7684\u7269\u7406\u4f9d\u636e\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "1. \u5c06GRU\u96c6\u6210\u5230PGNN\u6846\u67b6\u4e2d\uff0c\u6784\u5efaPG-GRU\u6a21\u578b\u3002\n2. \u91c7\u7528\u4e24\u6b65\u6cd5\u8bbe\u8ba1\u524d\u9988\u63a7\u5236\u5668\uff1a\u9996\u5148\uff0c\u5229\u7528\u7a33\u5b9a\u53cd\u6f14\u6280\u672f\u8bbe\u8ba1\u4e00\u4e2a\u7a33\u5b9a\u7684\u9006\u52a8\u6001\u7ebf\u6027\u6a21\u578b\uff1b\u7136\u540e\uff0c\u5728\u6b8b\u5dee\u4e0a\u8bad\u7ec3GRU\u4ee5\u8bc6\u522b\u9006\u7cfb\u7edf\u3002", "result": "PG-GRU\u524d\u9988\u63a7\u5236\u5668\u5728\u4e24\u8d28\u91cf\u5f39\u7c27\u963b\u5c3c\u7cfb\u7edf\u7684\u5b9e\u9a8c\u4e2d\uff0c\u5176\u79ef\u5206\u7edd\u5bf9\u8bef\u5dee\uff08IAE\uff09\u76f8\u8f83\u4e8e\u7ebf\u6027\u7684\u524d\u9988\u63a7\u5236\u5668\u548c\u57fa\u4e8e\u9884\u89c8\u7684GRU\u524d\u9988\u63a7\u5236\u5668\u6709\u4e86\u7ea6\u4e24\u500d\u7684\u6539\u8fdb\u3002", "conclusion": "\u672c\u7814\u7a76\u5c06GRU\u96c6\u6210\u5230PGNN\u6846\u67b6\u4e2d\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684PG-GRU\u6a21\u578b\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u4e24\u6b65\u524d\u9988\u63a7\u5236\u65b9\u6cd5\uff0c\u5728\u771f\u5b9e\u7684\u4e24\u8d28\u91cf\u5f39\u7c27\u963b\u5c3c\u7cfb\u7edf\u5b9e\u9a8c\u4e2d\uff0c\u5176\u6027\u80fd\u6bd4\u7ebf\u6027\u524d\u9988\u548c\u57fa\u4e8e\u9884\u89c8\u7684GRU\u524d\u9988\u63a7\u5236\u5668\u6709\u5927\u7ea6\u4e24\u500d\u7684\u63d0\u5347\uff08\u4ee5\u79ef\u5206\u7edd\u5bf9\u8bef\u5dee\u8861\u91cf\uff09\u3002"}}
{"id": "2507.13940", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.13940", "abs": "https://arxiv.org/abs/2507.13940", "authors": ["Qingyi Chen", "Ahmed H. Qureshi"], "title": "NeHMO: Neural Hamilton-Jacobi Reachability Learning for Decentralized Safe Multi-Agent Motion Planning", "comment": null, "summary": "Safe Multi-Agent Motion Planning (MAMP) is a significant challenge in\nrobotics. Despite substantial advancements, existing methods often face a\ndilemma. Decentralized algorithms typically rely on predicting the behavior of\nother agents, sharing contracts, or maintaining communication for safety, while\ncentralized approaches struggle with scalability and real-time decision-making.\nTo address these challenges, we introduce Neural Hamilton-Jacobi Reachability\nLearning (HJR) for Decentralized Multi-Agent Motion Planning. Our method\nprovides scalable neural HJR modeling to tackle high-dimensional configuration\nspaces and capture worst-case collision and safety constraints between agents.\nWe further propose a decentralized trajectory optimization framework that\nincorporates the learned HJR solutions to solve MAMP tasks in real-time. We\ndemonstrate that our method is both scalable and data-efficient, enabling the\nsolution of MAMP problems in higher-dimensional scenarios with complex\ncollision constraints. Our approach generalizes across various dynamical\nsystems, including a 12-dimensional dual-arm setup, and outperforms a range of\nstate-of-the-art techniques in successfully addressing challenging MAMP tasks.\nVideo demonstrations are available at https://youtu.be/IZiePX0p1Mc.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3a\u795e\u7ecf\u54c8\u5bc6\u987f-\u96c5\u53ef\u6bd4\u53ef\u8fbe\u6027\u5b66\u4e60\uff08HJR\uff09\u7684\u53bb\u4e2d\u5fc3\u5316\u591a\u667a\u80fd\u4f53\u8fd0\u52a8\u89c4\u5212\u65b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u5b66\u4e60\u53ef\u6269\u5c55\u7684\u795e\u7ecfHJR\u6a21\u578b\u89e3\u51b3\u4e86\u9ad8\u7ef4\u914d\u7f6e\u7a7a\u95f4\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u53bb\u4e2d\u5fc3\u5316\u7684\u8f68\u8ff9\u4f18\u5316\u6846\u67b6\u5b9e\u73b0\u4e86\u5b9e\u65f6\u89c4\u5212\uff0c\u5728\u5404\u79cd\u573a\u666f\u4e0b\u5747\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u53bb\u4e2d\u5fc3\u5316\uff08\u4f9d\u8d56\u9884\u6d4b\u3001\u5408\u540c\u5171\u4eab\u6216\u901a\u4fe1\uff09\u548c\u4e2d\u5fc3\u5316\uff08\u53ef\u6269\u5c55\u6027\u5dee\u3001\u5b9e\u65f6\u51b3\u7b56\u56f0\u96be\uff09\u65b9\u6cd5\u4e4b\u95f4\u9762\u4e34\u7684\u56f0\u5883\u3002", "method": "\u901a\u8fc7\u5b66\u4e60\u53ef\u6269\u5c55\u7684\u795e\u7ecfHJR\u6a21\u578b\u6765\u89e3\u51b3\u9ad8\u7ef4\u914d\u7f6e\u7a7a\u95f4\u95ee\u9898\uff0c\u5e76\u7ed3\u5408\u53bb\u4e2d\u5fc3\u5316\u7684\u8f68\u8ff9\u4f18\u5316\u6846\u67b6\u6765\u5b9e\u65f6\u89e3\u51b3MAMP\u4efb\u52a1\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u572812\u7ef4\u53cc\u81c2\u8bbe\u7f6e\u7b49\u5404\u79cd\u52a8\u6001\u7cfb\u7edf\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u5e76\u5728MAMP\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u6210\u529f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u66f4\u9ad8\u7ef4\u5ea6\u7684\u590d\u6742\u573a\u666f\u4e0b\u5177\u6709\u53ef\u6269\u5c55\u6027\u548c\u6570\u636e\u6548\u7387\uff0c\u5e76\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u591a\u4ee3\u7406\u8fd0\u52a8\u89c4\u5212\u4efb\u52a1\u4e2d\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u6280\u672f\u3002"}}
{"id": "2507.13563", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.13563", "abs": "https://arxiv.org/abs/2507.13563", "authors": ["Kirill Borodin", "Nikita Vasiliev", "Vasiliy Kudryavtsev", "Maxim Maslov", "Mikhail Gorodnichev", "Oleg Rogov", "Grach Mkrtchian"], "title": "A Data-Centric Framework for Addressing Phonetic and Prosodic Challenges in Russian Speech Generative Models", "comment": "The work is still in progress", "summary": "Russian speech synthesis presents distinctive challenges, including vowel\nreduction, consonant devoicing, variable stress patterns, homograph ambiguity,\nand unnatural intonation. This paper introduces Balalaika, a novel dataset\ncomprising more than 2,000 hours of studio-quality Russian speech with\ncomprehensive textual annotations, including punctuation and stress markings.\nExperimental results show that models trained on Balalaika significantly\noutperform those trained on existing datasets in both speech synthesis and\nenhancement tasks. We detail the dataset construction pipeline, annotation\nmethodology, and results of comparative evaluations.", "AI": {"tldr": "Balalaika\u662f\u4e00\u4e2a\u5305\u542b2000\u591a\u5c0f\u65f6\u9ad8\u8d28\u91cf\u4fc4\u8bed\u8bed\u97f3\u53ca\u6807\u6ce8\u7684\u65b0\u6570\u636e\u96c6\uff0c\u80fd\u63d0\u5347\u8bed\u97f3\u5408\u6210\u548c\u589e\u5f3a\u6548\u679c\u3002", "motivation": "\u89e3\u51b3\u4e86\u4fc4\u8bed\u8bed\u97f3\u5408\u6210\u4e2d\u7684\u5143\u97f3\u5f31\u5316\u3001\u8f85\u97f3\u6e05\u5316\u3001\u53ef\u53d8\u91cd\u97f3\u3001\u540c\u5f62\u5f02\u4e49\u8bcd\u6b67\u4e49\u548c\u4e0d\u81ea\u7136\u8bed\u8c03\u7b49\u6311\u6218\u3002", "method": "\u4ecb\u7ecd\u4e86Balalaika\u6570\u636e\u96c6\u7684\u6784\u5efa\u6d41\u7a0b\u3001\u6807\u6ce8\u65b9\u6cd5\u548c\u6bd4\u8f83\u8bc4\u4f30\u7ed3\u679c\u3002", "result": "\u5728\u8bed\u97f3\u5408\u6210\u548c\u589e\u5f3a\u4efb\u52a1\u4e2d\uff0c\u4f7f\u7528Balalaika\u6570\u636e\u96c6\u8bad\u7ec3\u7684\u6a21\u578b\u663e\u8457\u4f18\u4e8e\u4f7f\u7528\u73b0\u6709\u6570\u636e\u96c6\u8bad\u7ec3\u7684\u6a21\u578b\u3002", "conclusion": "Balalaika\u6570\u636e\u96c6\u663e\u8457\u63d0\u5347\u4e86\u4fc4\u8bed\u8bed\u97f3\u5408\u6210\u548c\u589e\u5f3a\u7684\u6548\u679c\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u6570\u636e\u96c6\u3002"}}
{"id": "2507.13778", "categories": ["quant-ph", "math-ph", "math.MP"], "pdf": "https://arxiv.org/pdf/2507.13778", "abs": "https://arxiv.org/abs/2507.13778", "authors": ["P\u00e9ter Vrana"], "title": "Error exponents for tripartite-to-bipartite entanglement transformations", "comment": "32 pages, 1 figure, 0 semirings", "summary": "We consider distillation of ebits between a specified pair of subsystems from\npure tripartite states by local operations and classical communication. It is\nknown that, allowing an asymptotically vanishing error, the maximal rate is the\nminimum of the von Neumann entropies of the two corresponding marginals, and\nunder asymptotic stochastic local operations and classical communication the\nmaximal rate is given by a minimization over a one-parameter family of\nentanglement measures. In this paper, we determine the direct and strong\nconverse error exponents, and the optimal rate for deterministic\ntransformations.", "AI": {"tldr": "\u7814\u7a76\u4e86\u5728\u5c40\u90e8\u64cd\u4f5c\u548c\u7ecf\u5178\u901a\u4fe1\u4e0b\uff0c\u4ece\u7eaf\u4e09\u65b9\u72b6\u6001\u4e2d\u84b8\u998fEBits\u7684\u901f\u7387\uff0c\u5e76\u786e\u5b9a\u4e86\u786e\u5b9a\u6027\u8f6c\u6362\u7684\u8bef\u5dee\u6307\u6570\u548c\u6700\u4f18\u901f\u7387\u3002", "motivation": "\u8003\u8651\u4ece\u7eaf\u4e09\u65b9\u72b6\u6001\u4e2d\u84b8\u998fEBits\u3002", "method": "\u901a\u8fc7\u5c40\u90e8\u64cd\u4f5c\u548c\u7ecf\u5178\u901a\u4fe1\u4ece\u7eaf\u4e09\u65b9\u72b6\u6001\u4e2d\u84b8\u998fEBits\u3002", "result": "\u6700\u5927\u901f\u7387\u662f\u4e24\u4e2a\u5bf9\u5e94\u8fb9\u9645\u7684\u51af\u8bfa\u4f9d\u66fc\u71b5\u7684\u6700\u5c0f\u503c\uff0c\u5e76\u4e14\u5728\u6e10\u8fd1\u968f\u673a\u5c40\u90e8\u64cd\u4f5c\u548c\u7ecf\u5178\u901a\u4fe1\u4e0b\uff0c\u6700\u5927\u901f\u7387\u7531\u4e00\u4e2a\u5355\u53c2\u6570\u7ea0\u7f20\u5ea6\u91cf\u65cf\u7ed9\u51fa\u3002", "conclusion": "\u786e\u5b9a\u4e86\u786e\u5b9a\u6027\u8f6c\u6362\u7684\u76f4\u63a5\u548c\u5f3a\u53cd\u8bc1\u8bef\u5dee\u6307\u6570\u4ee5\u53ca\u6700\u4f18\u901f\u7387\u3002"}}
{"id": "2507.13556", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13556", "abs": "https://arxiv.org/abs/2507.13556", "authors": ["Rui Wang", "Steven Klee", "Alexis Roos"], "title": "Time Series Forecastability Measures", "comment": null, "summary": "This paper proposes using two metrics to quantify the forecastability of time\nseries prior to model development: the spectral predictability score and the\nlargest Lyapunov exponent. Unlike traditional model evaluation metrics, these\nmeasures assess the inherent forecastability characteristics of the data before\nany forecast attempts. The spectral predictability score evaluates the strength\nand regularity of frequency components in the time series, whereas the Lyapunov\nexponents quantify the chaos and stability of the system generating the data.\nWe evaluated the effectiveness of these metrics on both synthetic and\nreal-world time series from the M5 forecast competition dataset. Our results\ndemonstrate that these two metrics can correctly reflect the inherent\nforecastability of a time series and have a strong correlation with the actual\nforecast performance of various models. By understanding the inherent\nforecastability of time series before model training, practitioners can focus\ntheir planning efforts on products and supply chain levels that are more\nforecastable, while setting appropriate expectations or seeking alternative\nstrategies for products with limited forecastability.", "AI": {"tldr": "\u901a\u8fc7\u8c31\u53ef\u9884\u6d4b\u6027\u5f97\u5206\u548c\u6700\u5927\u674e\u96c5\u666e\u8bfa\u592b\u6307\u6570\u91cf\u5316\u65f6\u95f4\u5e8f\u5217\u7684\u53ef\u9884\u6d4b\u6027\uff0c\u4ee5\u6307\u5bfc\u9884\u6d4b\u6a21\u578b\u5f00\u53d1\u548c\u8d44\u6e90\u89c4\u5212\u3002", "motivation": "\u4e3a\u4e86\u5728\u6a21\u578b\u5f00\u53d1\u4e4b\u524d\u91cf\u5316\u65f6\u95f4\u5e8f\u5217\u7684\u56fa\u6709\u53ef\u9884\u6d4b\u6027\uff0c\u4ee5\u4fbf\u4ece\u4e1a\u8005\u80fd\u591f\u66f4\u597d\u5730\u89c4\u5212\u8d44\u6e90\u548c\u8bbe\u5b9a\u9884\u671f\u3002", "method": "\u63d0\u51fa\u5e76\u8bc4\u4f30\u4e86\u8c31\u53ef\u9884\u6d4b\u6027\u5f97\u5206\u548c\u6700\u5927\u674e\u96c5\u666e\u8bfa\u592b\u6307\u6570\u8fd9\u4e24\u79cd\u91cf\u5316\u65f6\u95f4\u5e8f\u5217\u53ef\u9884\u6d4b\u6027\u7684\u6307\u6807\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u8c31\u53ef\u9884\u6d4b\u6027\u5f97\u5206\u548c\u6700\u5927\u674e\u96c5\u666e\u8bfa\u592b\u6307\u6570\u80fd\u591f\u6709\u6548\u53cd\u6620\u65f6\u95f4\u5e8f\u5217\u7684\u56fa\u6709\u53ef\u9884\u6d4b\u6027\uff0c\u5e76\u4e0e\u5b9e\u9645\u9884\u6d4b\u6027\u80fd\u5448\u5f3a\u76f8\u5173\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e24\u79cd\u91cf\u5316\u65f6\u95f4\u5e8f\u5217\u53ef\u9884\u6d4b\u6027\u7684\u6307\u6807\uff1a\u8c31\u53ef\u9884\u6d4b\u6027\u5f97\u5206\u548c\u6700\u5927\u674e\u96c5\u666e\u8bfa\u592b\u6307\u6570\u3002\u8fd9\u4e9b\u6307\u6807\u5728\u6a21\u578b\u5f00\u53d1\u4e4b\u524d\u8bc4\u4f30\u6570\u636e\u7684\u56fa\u6709\u53ef\u9884\u6d4b\u6027\u7279\u5f81\uff0c\u800c\u4e0d\u662f\u4f9d\u8d56\u4f20\u7edf\u7684\u6a21\u578b\u8bc4\u4f30\u6307\u6807\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u8fd9\u4e24\u79cd\u6307\u6807\u80fd\u591f\u51c6\u786e\u53cd\u6620\u65f6\u95f4\u5e8f\u5217\u7684\u56fa\u6709\u53ef\u9884\u6d4b\u6027\uff0c\u5e76\u4e0e\u5404\u79cd\u6a21\u578b\u7684\u5b9e\u9645\u9884\u6d4b\u6027\u80fd\u9ad8\u5ea6\u76f8\u5173\u3002\u901a\u8fc7\u5728\u6a21\u578b\u8bad\u7ec3\u524d\u4e86\u89e3\u65f6\u95f4\u5e8f\u5217\u7684\u56fa\u6709\u53ef\u9884\u6d4b\u6027\uff0c\u4ece\u4e1a\u8005\u53ef\u4ee5\u66f4\u6709\u6548\u5730\u89c4\u5212\u5176\u5728\u53ef\u9884\u6d4b\u6027\u8f83\u9ad8\u4ea7\u54c1\u548c\u4f9b\u5e94\u94fe\u5c42\u9762\u7684\u5de5\u4f5c\uff0c\u5e76\u4e3a\u53ef\u9884\u6d4b\u6027\u6709\u9650\u7684\u4ea7\u54c1\u8bbe\u5b9a\u9002\u5f53\u7684\u9884\u671f\u6216\u5bfb\u6c42\u66ff\u4ee3\u7b56\u7565\u3002"}}
{"id": "2507.13397", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13397", "abs": "https://arxiv.org/abs/2507.13397", "authors": ["Kaiyuan Zhai", "Juan Chen", "Chao Wang", "Zeyi Xu"], "title": "InSyn: Modeling Complex Interactions for Pedestrian Trajectory Prediction", "comment": null, "summary": "Accurate pedestrian trajectory prediction is crucial for intelligent\napplications, yet it remains highly challenging due to the complexity of\ninteractions among pedestrians. Previous methods have primarily relied on\nrelative positions to model pedestrian interactions; however, they tend to\noverlook specific interaction patterns such as paired walking or conflicting\nbehaviors, limiting the prediction accuracy in crowded scenarios. To address\nthis issue, we propose InSyn (Interaction-Synchronization Network), a novel\nTransformer-based model that explicitly captures diverse interaction patterns\n(e.g., walking in sync or conflicting) while effectively modeling\ndirection-sensitive social behaviors. Additionally, we introduce a training\nstrategy termed Seq-Start of Seq (SSOS), designed to alleviate the common issue\nof initial-step divergence in numerical time-series prediction. Experiments on\nthe ETH and UCY datasets demonstrate that our model outperforms recent\nbaselines significantly, especially in high-density scenarios. Furthermore, the\nSSOS strategy proves effective in improving sequential prediction performance,\nreducing the initial-step prediction error by approximately 6.58%.", "AI": {"tldr": "InSyn\u662f\u4e00\u4e2a\u57fa\u4e8eTransformer\u7684\u6a21\u578b\uff0c\u901a\u8fc7\u660e\u786e\u6355\u6349\u4ea4\u4e92\u6a21\u5f0f\u548c\u91c7\u7528SSOS\u8bad\u7ec3\u7b56\u7565\uff0c\u63d0\u9ad8\u4e86\u884c\u4eba\u8f68\u8ff9\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u5c24\u5176\u662f\u5728\u62e5\u6324\u73af\u5883\u4e2d\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u76f8\u5bf9\u4f4d\u7f6e\u6765\u6a21\u62df\u884c\u4eba\u4ea4\u4e92\uff0c\u4f46\u5ffd\u7565\u4e86\u8bf8\u5982\u6210\u5bf9\u884c\u8d70\u6216\u51b2\u7a81\u884c\u4e3a\u7b49\u7279\u5b9a\u7684\u4ea4\u4e92\u6a21\u5f0f\uff0c\u9650\u5236\u4e86\u5728\u62e5\u6324\u573a\u666f\u4e0b\u7684\u9884\u6d4b\u7cbe\u5ea6\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u9700\u8981\u80fd\u591f\u660e\u786e\u6355\u6349\u591a\u6837\u5316\u4ea4\u4e92\u6a21\u5f0f\u5e76\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u7684\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aInSyn\uff08Interaction-Synchronization Network\uff09\u7684\u65b0\u578bTransformer-based\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u80fd\u591f\u663e\u5f0f\u6355\u6349\u591a\u6837\u5316\u7684\u4ea4\u4e92\u6a21\u5f0f\uff08\u5982\u540c\u6b65\u884c\u8d70\u6216\u51b2\u7a81\u884c\u4e3a\uff09\uff0c\u5e76\u6709\u6548\u6a21\u62df\u65b9\u5411\u654f\u611f\u7684\u793e\u4ea4\u884c\u4e3a\u3002\u6b64\u5916\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSeq-Start of Seq\uff08SSOS\uff09\u7684\u8bad\u7ec3\u7b56\u7565\uff0c\u4ee5\u7f13\u89e3\u6570\u503c\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u5e38\u89c1\u7684\u521d\u59cb\u6b65\u957f\u53d1\u6563\u95ee\u9898\u3002", "result": "InSyn\u6a21\u578b\u5728ETH\u548cUCY\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5176\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u8fd1\u671f\u57fa\u7ebf\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u9ad8\u5bc6\u5ea6\u573a\u666f\u4e0b\u3002SSOS\u7b56\u7565\u6210\u529f\u5730\u63d0\u9ad8\u4e86\u5e8f\u5217\u9884\u6d4b\u6027\u80fd\uff0c\u5c06\u521d\u59cb\u6b65\u957f\u9884\u6d4b\u8bef\u5dee\u51cf\u5c0f\u4e86\u7ea66.58%\u3002", "conclusion": "InSyn\u6a21\u578b\u5728ETH\u548cUCY\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u5bc6\u5ea6\u573a\u666f\u4e0b\u3002SSOS\u7b56\u7565\u6709\u6548\u63d0\u9ad8\u4e86\u5e8f\u5217\u9884\u6d4b\u6027\u80fd\uff0c\u5c06\u521d\u59cb\u6b65\u957f\u9884\u6d4b\u8bef\u5dee\u964d\u4f4e\u4e86\u7ea66.58%\u3002"}}
{"id": "2507.14073", "categories": ["eess.SY", "cs.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2507.14073", "abs": "https://arxiv.org/abs/2507.14073", "authors": ["Oumayma Khattabi", "Matteo Tacchi-B\u00e9nard", "Sorin Olaru"], "title": "Convex computation of regions of attraction from data using Sums-of-Squares programming", "comment": null, "summary": "The paper concentrates on the analysis of the region of attraction (ROA) for\nunknown autonomous dynamical systems. The aim is to explore a data-driven\napproach based on moment-sum-of-squares (SoS) hierarchy, which enables novel\nRoA outer approximations despite the reduced information on the structure of\nthe dynamics. The main contribution of this work is bypassing the system model\nand, consequently, the recurring constraint on its polynomial structure.\nNumerical experimentation showcases the influence of data on learned\napproximating sets, offering a promising outlook on the potential of this\nmethod.", "AI": {"tldr": "\u4e00\u79cd\u65e0\u9700\u6a21\u578b\u5373\u53ef\u5206\u6790\u81ea\u6cbb\u52a8\u529b\u5b66\u7cfb\u7edf\u5438\u5f15\u57df\u7684\u65b9\u6cd5\u3002", "motivation": "\u65e8\u5728\u4e3a\u672a\u77e5\u81ea\u6cbb\u52a8\u529b\u5b66\u7cfb\u7edf\u63d0\u4f9b\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\u6765\u5206\u6790\u533a\u57df\u5438\u5f15\u57df\uff08ROA\uff09\uff0c\u4ee5\u514b\u670d\u6a21\u578b\u4f9d\u8d56\u6027\u548c\u591a\u9879\u5f0f\u7ed3\u6784\u7ea6\u675f\u3002", "method": "\u6570\u636e\u9a71\u52a8\u7684\u77e9-\u5e73\u65b9\u548c\uff08SoS\uff09\u5c42\u6790\u3002", "result": "\u6570\u503c\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5b66\u4e60\u5230\u6709\u5f71\u54cd\u529b\u7684\u8fd1\u4f3c\u96c6\u5408\uff0c\u4e3a\u5728\u6a21\u578b\u4fe1\u606f\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u8fdb\u884cROA\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u77e9-\u5e73\u65b9\u548c\uff08SoS\uff09\u5c42\u6790\u7684\u533a\u57df\u5438\u5f15\u57df\uff08ROA\uff09\u5916\u8fd1\u4f3c\u7684\u65e0\u6a21\u578b\u65b9\u6cd5\u3002"}}
{"id": "2507.13614", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13614", "abs": "https://arxiv.org/abs/2507.13614", "authors": ["Sergio E. Zanotto", "Segun Aroyehun"], "title": "Linguistic and Embedding-Based Profiling of Texts generated by Humans and Large Language Models", "comment": "arXiv admin note: text overlap with arXiv:2412.03025", "summary": "The rapid advancements in large language models (LLMs) have significantly\nimproved their ability to generate natural language, making texts generated by\nLLMs increasingly indistinguishable from human-written texts. While recent\nresearch has primarily focused on using LLMs to classify text as either\nhuman-written and machine-generated texts, our study focus on characterizing\nthese texts using a set of linguistic features across different linguistic\nlevels such as morphology, syntax, and semantics. We select a dataset of\nhuman-written and machine-generated texts spanning 8 domains and produced by 11\ndifferent LLMs. We calculate different linguistic features such as dependency\nlength and emotionality and we use them for characterizing human-written and\nmachine-generated texts along with different sampling strategies, repetition\ncontrols and model release date. Our statistical analysis reveals that\nhuman-written texts tend to exhibit simpler syntactic structures and more\ndiverse semantic content. Furthermore, we calculate the variability of our set\nof features across models and domains. Both human and machine texts show\nstylistic diversity across domains, with humans displaying greater variation in\nour features. Finally, we apply style embeddings to further test variability\namong human-written and machine-generated texts. Notably, newer models output\ntext that is similarly variable, pointing to an homogenization of\nmachine-generated texts.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u5206\u6790\u8bed\u8a00\u7279\u5f81\u53d1\u73b0\uff0c\u4eba\u5199\u6587\u672c\u6bd4\u673a\u5668\u751f\u6210\u6587\u672c\uff08\u5c24\u5176\u662f\u65b0\u6a21\u578b\uff09\u5177\u6709\u66f4\u591a\u7684\u53e5\u6cd5\u548c\u8bed\u4e49\u591a\u6837\u6027\uff0c\u800c\u673a\u5668\u6587\u672c\u8d8b\u4e8e\u540c\u8d28\u5316\u3002", "motivation": "\u968f\u7740LLM\u5728\u751f\u6210\u81ea\u7136\u8bed\u8a00\u65b9\u9762\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u5176\u751f\u6210\u6587\u672c\u8d8a\u6765\u8d8a\u96be\u4ee5\u4e0e\u4eba\u5199\u6587\u672c\u533a\u5206\u3002\u672c\u7814\u7a76\u65e8\u5728\u8d85\u8d8a\u7b80\u5355\u7684\u4e8c\u5143\u5206\u7c7b\uff0c\u901a\u8fc7\u5206\u6790\u8bed\u8a00\u7279\u5f81\u6765\u8868\u5f81\u548c\u533a\u5206\u4eba\u5199\u6587\u672c\u548c\u673a\u5668\u751f\u6210\u6587\u672c\u3002", "method": "\u901a\u8fc7\u8ba1\u7b97\u4e0d\u540c\u8bed\u8a00\u5c42\u9762\uff08\u5f62\u6001\u3001\u53e5\u6cd5\u3001\u8bed\u4e49\uff09\u7684\u8bed\u8a00\u7279\u5f81\uff0c\u5982\u4f9d\u5b58\u957f\u5ea6\u548c\u60c5\u611f\u6027\uff0c\u6765\u8868\u5f81\u4eba\u5199\u6587\u672c\u548c\u673a\u5668\u751f\u6210\u6587\u672c\u3002\u7814\u7a76\u4f7f\u7528\u4e86\u8de8\u8d8a8\u4e2a\u9886\u57df\u548c11\u4e2a\u4e0d\u540cLLM\u7684\u6587\u672c\u6570\u636e\u96c6\uff0c\u5e76\u8003\u8651\u4e86\u91c7\u6837\u7b56\u7565\u3001\u91cd\u590d\u63a7\u5236\u548c\u6a21\u578b\u53d1\u5e03\u65e5\u671f\u7b49\u56e0\u7d20\u3002\u6b64\u5916\uff0c\u8fd8\u5e94\u7528\u4e86\u98ce\u683c\u5d4c\u5165\u6765\u6d4b\u8bd5\u6587\u672c\u7684\u591a\u6837\u6027\u3002", "result": "\u4eba\u5199\u6587\u672c\u503e\u5411\u4e8e\u5c55\u73b0\u66f4\u7b80\u5355\u7684\u53e5\u6cd5\u7ed3\u6784\u548c\u66f4\u591a\u6837\u5316\u7684\u8bed\u4e49\u5185\u5bb9\u3002\u4eba\u5199\u548c\u673a\u5668\u751f\u6210\u7684\u6587\u672c\u5728\u4e0d\u540c\u9886\u57df\u90fd\u8868\u73b0\u51fa\u98ce\u683c\u591a\u6837\u6027\uff0c\u4f46\u4eba\u7c7b\u6587\u672c\u5728\u6240\u9009\u7279\u5f81\u4e0a\u7684\u53d8\u5316\u6027\u66f4\u5927\u3002\u66f4\u65b0\u7684\u6a21\u578b\u8f93\u51fa\u7684\u6587\u672c\u53d8\u5316\u6027\u76f8\u4f3c\uff0c\u8868\u660e\u673a\u5668\u751f\u6210\u6587\u672c\u5b58\u5728\u8d8b\u540c\u73b0\u8c61\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u5206\u6790\u8bed\u8a00\u7279\u5f81\uff0c\u53d1\u73b0\u4eba\u5199\u6587\u672c\u5728\u53e5\u6cd5\u7ed3\u6784\u548c\u8bed\u4e49\u5185\u5bb9\u4e0a\u66f4\u4e3a\u591a\u6837\u5316\uff0c\u800c\u673a\u5668\u751f\u6210\u6587\u672c\uff0c\u7279\u522b\u662f\u8f83\u65b0\u6a21\u578b\u751f\u6210\u7684\u6587\u672c\uff0c\u8868\u73b0\u51fa\u8d8b\u540c\u6027\u3002\u7814\u7a76\u8fd8\u53d1\u73b0\uff0c\u4eba\u7c7b\u6587\u672c\u5728\u8bed\u8a00\u7279\u5f81\u4e0a\u7684\u53d8\u5316\u6027\u66f4\u5927\u3002"}}
{"id": "2507.13800", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2507.13800", "abs": "https://arxiv.org/abs/2507.13800", "authors": ["Lin-Lin Jiang", "Xuan Xie", "Lin Tian", "Jin-Feng Huang"], "title": "Chirally Frustrated Superradiant Phases in a Jaynes-Cummings Trimer", "comment": "6 pages,4 figures", "summary": "We investigate the emergence of frustrated quantum phases in a\nJaynes-Cummings (JC) trimer with complex hopping amplitudes between the\ncavities, which represents the smallest frustrated unit in light-matter\nsystems. The complex hopping amplitudes that can be engineered via synthetic\ngauge fields introduce chiral effects and geometric frustration into the\nsystem. We obtain analytic solutions in the semiclassical limit and map out the\nphase diagram of this model, featuring one normal and three distinct\nsuperradiant phases. Among these phases, a chirally frustrated superradiant\nphase emerges, characterized by broken chiral and translational symmetries and\nunidirectional photon flow. These results reveal how frustration and symmetry\nbreaking can arise in JC systems with synthetic gauge fields and ultrastrong\ncoupling.", "AI": {"tldr": "\u7814\u7a76\u4e86 JC \u4e09\u805a\u4f53\u4e2d\u7684\u632b\u6298\u91cf\u5b50\u76f8\uff0c\u53d1\u73b0\u624b\u5f81\u632b\u6298\u8d85\u8f90\u5c04\u76f8\u5177\u6709\u5355\u5411\u5149\u5b50\u6d41\u3002", "motivation": "\u7814\u7a76 JC \u4e09\u805a\u4f53\u4e2d\u56e0\u590d\u6742\u8dc3\u8fc1\u632f\u5e45\uff08\u53ef\u901a\u8fc7\u5408\u6210\u89c4\u8303\u573a\u5de5\u7a0b\u5316\uff09\u800c\u4ea7\u751f\u7684\u632b\u6298\u91cf\u5b50\u76f8\uff0c\u8be5\u4e09\u805a\u4f53\u662f\u5149-\u7269\u8d28\u7cfb\u7edf\u4e2d\u6700\u5c0f\u7684\u632b\u6298\u5355\u5143\u3002", "method": "\u901a\u8fc7\u7814\u7a76\u5177\u6709\u590d\u6742\u8dc3\u8fc1\u632f\u5e45\u7684 JC \u4e09\u805a\u4f53\uff0c\u5728\u534a\u7ecf\u5178\u6781\u9650\u4e0b\u83b7\u5f97\u89e3\u6790\u89e3\uff0c\u5e76\u7ed8\u5236\u4e86\u6a21\u578b\u76f8\u56fe\uff0c\u8bc6\u522b\u51fa\u6b63\u5e38\u76f8\u548c\u4e09\u79cd\u4e0d\u540c\u7684\u8d85\u8f90\u5c04\u76f8\u3002", "result": "\u8bc6\u522b\u51fa\u4e00\u79cd\u624b\u5f81\u632b\u6298\u8d85\u8f90\u5c04\u76f8\uff0c\u5176\u7279\u5f81\u662f\u624b\u5f81\u548c\u7ffb\u8bd1\u5bf9\u79f0\u6027\u88ab\u7834\u574f\uff0c\u4ee5\u53ca\u5355\u5411\u5149\u5b50\u6d41\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u5728\u5177\u6709\u590d\u6742\u8dc3\u8fc1\u632f\u5e45\u7684 Jaynes-Cummings (JC) \u4e09\u805a\u4f53\u4e2d\uff0c\u5982\u4f55\u901a\u8fc7\u5408\u6210\u89c4\u8303\u573a\u548c\u8d85\u5f3a\u8026\u5408\u5b9e\u73b0\u632b\u6298\u548c\u5bf9\u79f0\u6027\u7834\u7f3a\u3002"}}
{"id": "2507.13569", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13569", "abs": "https://arxiv.org/abs/2507.13569", "authors": ["Mrinal Mathur", "Mike Doan", "Barak Pearlmutter", "Sergey Plis"], "title": "Change of Thought: Adaptive Test-Time Computation", "comment": null, "summary": "Transformers evaluated in a single, fixed-depth pass are provably limited in\nexpressive power to the constant-depth circuit class TC0. Running a Transformer\nautoregressively removes that ceiling -- first in next-token prediction and,\nmore recently, in chain-of-thought reasoning. Both regimes rely on feedback\nloops that decode internal states into tokens only to re-encode them in\nsubsequent steps. While this \"thinking aloud\" mirrors human reasoning,\nbiological brains iterate without externalising intermediate states as\nlanguage. To boost the expressive power of encoder Transformers without\nresorting to token-level autoregression, we introduce the SELF-Transformer: an\nencoder layer that iteratively refines its own attention weights to a fixed\npoint. Instead of producing -- in one pass -- the alignment matrix that remixes\nthe input sequence, the SELF-Transformer iteratively updates that matrix\ninternally, scaling test-time computation with input difficulty. This\nadaptivity yields up to 20\\% accuracy gains on encoder-style benchmarks without\nincreasing parameter count, demonstrating that input-adaptive alignment at test\ntime offers substantial benefits for only a modest extra compute budget.\nSelf-Transformers thus recover much of the expressive power of iterative\nreasoning while preserving the simplicity of pure encoder architectures.", "AI": {"tldr": "SELF-Transformer \u662f\u4e00\u79cd\u65b0\u7684 Transformer \u67b6\u6784\uff0c\u901a\u8fc7\u8fed\u4ee3\u66f4\u65b0\u6ce8\u610f\u529b\u6743\u91cd\u6765\u63d0\u9ad8\u5176\u8868\u8fbe\u80fd\u529b\uff0c\u5728\u4e0d\u589e\u52a0\u53c2\u6570\u7684\u60c5\u51b5\u4e0b\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u3002", "motivation": "\u4e3a\u4e86\u5728\u4e0d\u4f9d\u8d56\u4e8e\u6807\u8bb0\u7ea7\u81ea\u56de\u5f52\u7684\u60c5\u51b5\u4e0b\u63d0\u9ad8\u7f16\u7801\u5668 Transformer \u7684\u8868\u8fbe\u80fd\u529b\uff0c\u6211\u4eec\u5f15\u5165\u4e86 SELF-Transformer\u3002", "method": "SELF-Transformer \u662f\u4e00\u79cd\u7f16\u7801\u5668\u5c42\uff0c\u5b83\u8fed\u4ee3\u5730\u5c06\u5176\u81ea\u8eab\u7684\u6ce8\u610f\u529b\u6743\u91cd\u7cbe\u70bc\u5230\u56fa\u5b9a\u70b9\u3002", "result": "SELF-Transformer \u5728\u4e0d\u589e\u52a0\u53c2\u6570\u6570\u91cf\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u7c7b\u4f3c\u7f16\u7801\u5668\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u51c6\u786e\u7387\u63d0\u9ad8\u4e86\u9ad8\u8fbe 20%\u3002", "conclusion": "Transformer \u7684\u4e00\u79cd\u65b0\u67b6\u6784 SELF-Transformer\uff0c\u5b83\u901a\u8fc7\u8fed\u4ee3\u66f4\u65b0\u6ce8\u610f\u529b\u6743\u91cd\u6765\u63d0\u9ad8\u8868\u8fbe\u80fd\u529b\uff0c\u5728\u4e0d\u589e\u52a0\u53c2\u6570\u6570\u91cf\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u7c7b\u4f3c\u7f16\u7801\u5668\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u63d0\u9ad8\u4e86\u9ad8\u8fbe 20% \u7684\u51c6\u786e\u7387\u3002"}}
{"id": "2507.13401", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13401", "abs": "https://arxiv.org/abs/2507.13401", "authors": ["Shreya Kadambi", "Risheek Garrepalli", "Shubhankar Borse", "Munawar Hyatt", "Fatih Porikli"], "title": "MADI: Masking-Augmented Diffusion with Inference-Time Scaling for Visual Editing", "comment": "26 pages", "summary": "Despite the remarkable success of diffusion models in text-to-image\ngeneration, their effectiveness in grounded visual editing and compositional\ncontrol remains challenging. Motivated by advances in self-supervised learning\nand in-context generative modeling, we propose a series of simple yet powerful\ndesign choices that significantly enhance diffusion model capacity for\nstructured, controllable generation and editing. We introduce Masking-Augmented\nDiffusion with Inference-Time Scaling (MADI), a framework that improves the\neditability, compositionality and controllability of diffusion models through\ntwo core innovations. First, we introduce Masking-Augmented gaussian Diffusion\n(MAgD), a novel training strategy with dual corruption process which combines\nstandard denoising score matching and masked reconstruction by masking noisy\ninput from forward process. MAgD encourages the model to learn discriminative\nand compositional visual representations, thus enabling localized and\nstructure-aware editing. Second, we introduce an inference-time capacity\nscaling mechanism based on Pause Tokens, which act as special placeholders\ninserted into the prompt for increasing computational capacity at inference\ntime. Our findings show that adopting expressive and dense prompts during\ntraining further enhances performance, particularly for MAgD. Together, these\ncontributions in MADI substantially enhance the editability of diffusion\nmodels, paving the way toward their integration into more general-purpose,\nin-context generative diffusion architectures.", "AI": {"tldr": "MADI\u6846\u67b6\u901a\u8fc7MAgD\uff08\u63a9\u7801\u589e\u5f3a\u9ad8\u65af\u6269\u6563\uff09\u548cPause Tokens\uff08\u63a8\u7406\u65f6\u5bb9\u91cf\u6269\u5c55\uff09\u6539\u8fdb\u4e86\u6269\u6563\u6a21\u578b\u7684\u89c6\u89c9\u7f16\u8f91\u548c\u7ec4\u5408\u63a7\u5236\u80fd\u529b\u3002", "motivation": "\u5c3d\u7ba1\u6269\u6563\u6a21\u578b\u5728\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u65b9\u9762\u53d6\u5f97\u4e86\u5de8\u5927\u6210\u529f\uff0c\u4f46\u5728\u89c6\u89c9\u7f16\u8f91\u548c\u7ec4\u5408\u63a7\u5236\u65b9\u9762\u4ecd\u5177\u6311\u6218\u6027\u3002\u53d7\u81ea\u76d1\u7763\u5b66\u4e60\u548c\u4e0a\u4e0b\u6587\u751f\u6210\u6a21\u578b\u8fdb\u5c55\u7684\u542f\u53d1\uff0c\u63d0\u51fa\u4e86\u4e00\u7cfb\u5217\u7b80\u5355\u800c\u5f3a\u5927\u7684\u8bbe\u8ba1\u9009\u62e9\uff0c\u4ee5\u589e\u5f3a\u6269\u6563\u6a21\u578b\u5728\u7ed3\u6784\u5316\u3001\u53ef\u63a7\u751f\u6210\u548c\u7f16\u8f91\u65b9\u9762\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u63a9\u7801\u589e\u5f3a\u6269\u6563\u63a8\u7406\u65f6\u7f29\u653e\uff08MADI\uff09\u7684\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u521b\u65b0\uff1a1. \u63a9\u7801\u589e\u5f3a\u9ad8\u65af\u6269\u6563\uff08MAgD\uff09\uff0c\u4e00\u79cd\u5305\u542b\u53cc\u91cd\u8150\u5316\u8fc7\u7a0b\u7684\u65b0\u9896\u8bad\u7ec3\u7b56\u7565\uff0c\u7ed3\u5408\u4e86\u6807\u51c6\u53bb\u566a\u8bc4\u5206\u5339\u914d\u548c\u63a9\u7801\u91cd\u5efa\u30022. \u57fa\u4e8ePause Tokens\u7684\u63a8\u7406\u65f6\u5bb9\u91cf\u6269\u5c55\u673a\u5236\u3002", "result": "MADI\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u6269\u6563\u6a21\u578b\u7684\u7f16\u8f91\u6027\u3001\u7ec4\u5408\u6027\u548c\u53ef\u63a7\u6027\u3002MAgD\u9f13\u52b1\u6a21\u578b\u5b66\u4e60\u5177\u6709\u533a\u5206\u6027\u548c\u7ec4\u5408\u6027\u7684\u89c6\u89c9\u8868\u5f81\uff0c\u5b9e\u73b0\u5c40\u90e8\u548c\u7ed3\u6784\u611f\u77e5\u7684\u7f16\u8f91\u3002Pause Tokens\u5728\u63a8\u7406\u65f6\u589e\u52a0\u4e86\u8ba1\u7b97\u5bb9\u91cf\u3002\u5728\u8bad\u7ec3\u4e2d\u91c7\u7528\u5bcc\u6709\u8868\u73b0\u529b\u548c\u5bc6\u96c6\u7684\u63d0\u793a\u8bcd\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u7279\u522b\u662f\u5bf9\u4e8eMAgD\u3002", "conclusion": "MADI\u6846\u67b6\u901a\u8fc7MAgD\u8bad\u7ec3\u7b56\u7565\u548cPause Tokens\u63a8\u7406\u65f6\u5bb9\u91cf\u6269\u5c55\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6269\u6563\u6a21\u578b\u7684\u7f16\u8f91\u6027\u3001\u7ec4\u5408\u6027\u548c\u53ef\u63a7\u6027\uff0c\u4e3a\u901a\u7528\u3001\u4e0a\u4e0b\u6587\u751f\u6210\u6269\u6563\u67b6\u6784\u7684\u96c6\u6210\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2507.14032", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14032", "abs": "https://arxiv.org/abs/2507.14032", "authors": ["Lam Nguyen", "Erika Barcelos", "Roger French", "Yinghui Wu"], "title": "KROMA: Ontology Matching with Knowledge Retrieval and Large Language Models", "comment": "Accepted to the 24th International Semantic Web Conference Research\n  Track (ISWC 2025)", "summary": "Ontology Matching (OM) is a cornerstone task of semantic interoperability,\nyet existing systems often rely on handcrafted rules or specialized models with\nlimited adaptability. We present KROMA, a novel OM framework that harnesses\nLarge Language Models (LLMs) within a Retrieval-Augmented Generation (RAG)\npipeline to dynamically enrich the semantic context of OM tasks with\nstructural, lexical, and definitional knowledge. To optimize both performance\nand efficiency, KROMA integrates a bisimilarity-based concept matching and a\nlightweight ontology refinement step, which prune candidate concepts and\nsubstantially reduce the communication overhead from invoking LLMs. Through\nexperiments on multiple benchmark datasets, we show that integrating knowledge\nretrieval with context-augmented LLMs significantly enhances ontology matching,\noutperforming both classic OM systems and cutting-edge LLM-based approaches\nwhile keeping communication overhead comparable. Our study highlights the\nfeasibility and benefit of the proposed optimization techniques (targeted\nknowledge retrieval, prompt enrichment, and ontology refinement) for ontology\nmatching at scale.", "AI": {"tldr": "KROMA\u662f\u4e00\u4e2a\u521b\u65b0\u7684\u672c\u4f53\u5339\u914d\u6846\u67b6\uff0c\u5229\u7528LLMs\u548cRAG\u6280\u672f\uff0c\u901a\u8fc7\u77e5\u8bc6\u68c0\u7d22\u548c\u672c\u4f53\u7cbe\u70bc\u6765\u63d0\u9ad8\u5339\u914d\u6027\u80fd\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u672c\u4f53\u5339\u914d\u7cfb\u7edf\u4f9d\u8d56\u624b\u5de5\u89c4\u5219\u6216\u4e13\u4e1a\u6a21\u578b\uff0c\u9002\u5e94\u6027\u6709\u9650\u3002KROMA\u65e8\u5728\u901a\u8fc7\u5229\u7528LLMs\u548cRAG\u6765\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\uff0c\u4ee5\u52a8\u6001\u4e30\u5bcc\u8bed\u4e49\u4e0a\u4e0b\u6587\u3002", "method": "KROMA\u6846\u67b6\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6280\u672f\uff0c\u901a\u8fc7\u96c6\u6210\u57fa\u4e8e\u53cc\u76f8\u4f3c\u6027\u7684\u6982\u5ff5\u5339\u914d\u548c\u8f7b\u91cf\u7ea7\u672c\u4f53\u7cbe\u70bc\u6765\u52a8\u6001\u4e30\u5bcc\u672c\u4f53\u5339\u914d\u4efb\u52a1\u7684\u8bed\u4e49\u4e0a\u4e0b\u6587\uff0c\u4ece\u800c\u4f18\u5316\u6027\u80fd\u548c\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5c06\u77e5\u8bc6\u68c0\u7d22\u4e0e\u4e0a\u4e0b\u6587\u589e\u5f3a\u7684LLMs\u76f8\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u672c\u4f53\u5339\u914d\u7684\u6027\u80fd\uff0c\u4f18\u4e8e\u4f20\u7edf\u7cfb\u7edf\u548c\u5148\u8fdb\u7684LLM\u65b9\u6cd5\uff0c\u4e14\u901a\u4fe1\u5f00\u9500\u76f8\u5f53\u3002", "conclusion": "KROMA\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u5e76\u96c6\u6210\u57fa\u4e8e\u53cc\u76f8\u4f3c\u6027\u7684\u6982\u5ff5\u5339\u914d\u548c\u8f7b\u91cf\u7ea7\u672c\u4f53\u7cbe\u70bc\u6b65\u9aa4\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u672c\u4f53\u5339\u914d\u7684\u6027\u80fd\u548c\u6548\u7387\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u65b9\u6cd5\u548c\u5148\u8fdb\u7684LLM\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u53ef\u6bd4\u8f83\u7684\u901a\u4fe1\u5f00\u9500\u3002"}}
{"id": "2507.14117", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.14117", "abs": "https://arxiv.org/abs/2507.14117", "authors": ["Aayushya Agarwal", "Larry Pileggi"], "title": "Integrating Forecasting Models Within Steady-State Analysis and Optimization", "comment": null, "summary": "Extreme weather variations and the increasing unpredictability of load\nbehavior make it difficult to determine power grid dispatches that are robust\nto uncertainties. While machine learning (ML) methods have improved the ability\nto model uncertainty caused by loads and renewables, accurately integrating\nthese forecasts and their sensitivities into steady-state analyses and\ndecision-making strategies remains an open challenge. Toward this goal, we\npresent a generalized methodology that seamlessly embeds ML-based forecasting\nengines within physics-based power flow and grid optimization tools. By\ncoupling physics-based grid modeling with black-box ML methods, we accurately\ncapture the behavior and sensitivity of loads and weather events by directly\nintegrating the inputs and outputs of trained ML forecasting models into the\nnumerical methods of power flow and grid optimization. Without fitting\nsurrogate load models, our approach obtains the sensitivities directly from\ndata to accurately predict the response of forecasted devices to changes in the\ngrid. Our approach combines the sensitivities of forecasted devices attained\nvia backpropagation and the sensitivities of physics-defined grid devices. We\ndemonstrate the efficacy of our method by showcasing improvements in\nsensitivity calculations and leveraging them to design a robust power dispatch\nthat improves grid reliability under stochastic weather events. Our approach\nenables the computation of system sensitivities to exogenous factors which\nsupports broader analyses that improve grid reliability in the presence of load\nvariability and extreme weather conditions.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u5c06\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5d4c\u5165\u7535\u529b\u7cfb\u7edf\u4eff\u771f\u4e2d\uff0c\u63d0\u9ad8\u7535\u7f51\u5728\u6781\u7aef\u5929\u6c14\u4e0b\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u4e3a\u4e86\u5e94\u5bf9\u6781\u7aef\u5929\u6c14\u53d8\u5316\u548c\u8d1f\u8377\u884c\u4e3a\u4e0d\u53ef\u9884\u6d4b\u6027\u5e26\u6765\u7684\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5c06\u673a\u5668\u5b66\u4e60\u9884\u6d4b\u53ca\u5176\u654f\u611f\u6027\u65e0\u7f1d\u96c6\u6210\u5230\u7a33\u6001\u5206\u6790\u548c\u51b3\u7b56\u5236\u5b9a\u7b56\u7565\u4e2d\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u65b9\u6cd5\uff0c\u5c06\u673a\u5668\u5b66\u4e60\u9884\u6d4b\u5f15\u64ce\u5d4c\u5165\u57fa\u4e8e\u7269\u7406\u7684\u6f6e\u6d41\u548c\u7535\u7f51\u4f18\u5316\u5de5\u5177\u4e2d\uff0c\u76f4\u63a5\u6574\u5408\u8bad\u7ec3\u597d\u7684\u673a\u5668\u5b66\u4e60\u9884\u6d4b\u6a21\u578b\u7684\u8f93\u5165\u548c\u8f93\u51fa\u6765\u5b9e\u73b0\u3002", "result": "\u901a\u8fc7\u5c06\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u4e0e\u57fa\u4e8e\u7269\u7406\u7684\u7535\u7f51\u5efa\u6a21\u76f8\u7ed3\u5408\uff0c\u53ef\u4ee5\u76f4\u63a5\u4ece\u6570\u636e\u4e2d\u83b7\u5f97\u654f\u611f\u6027\uff0c\u4ece\u800c\u51c6\u786e\u9884\u6d4b\u9884\u6d4b\u8bbe\u5907\u5bf9\u7535\u7f51\u53d8\u5316\u7684\u54cd\u5e94\uff0c\u5e76\u8bbe\u8ba1\u51fa\u66f4\u4f18\u7684\u7535\u7f51\u8c03\u5ea6\u65b9\u6848\uff0c\u63d0\u9ad8\u4e86\u7535\u7f51\u7684\u53ef\u9760\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5c06\u673a\u5668\u5b66\u4e60\u9884\u6d4b\u6a21\u578b\u5d4c\u5165\u57fa\u4e8e\u7269\u7406\u7684\u6f6e\u6d41\u548c\u7535\u7f51\u4f18\u5316\u5de5\u5177\u4e2d\uff0c\u80fd\u591f\u51c6\u786e\u6355\u83b7\u8d1f\u8377\u548c\u5929\u6c14\u4e8b\u4ef6\u7684\u884c\u4e3a\u53ca\u654f\u611f\u6027\uff0c\u4ece\u800c\u63d0\u9ad8\u7535\u7f51\u5728\u968f\u673a\u5929\u6c14\u4e8b\u4ef6\u4e0b\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2507.13970", "categories": ["cs.RO", "cs.AI", "I.2; I.2.9; I.2.10"], "pdf": "https://arxiv.org/pdf/2507.13970", "abs": "https://arxiv.org/abs/2507.13970", "authors": ["Casper Br\u00f6cheler", "Thomas Vroom", "Derrick Timmermans", "Alan van den Akker", "Guangzhi Tang", "Charalampos S. Kouzinopoulos", "Rico M\u00f6ckel"], "title": "A segmented robot grasping perception neural network for edge AI", "comment": "Accepted by SMC 2025", "summary": "Robotic grasping, the ability of robots to reliably secure and manipulate\nobjects of varying shapes, sizes and orientations, is a complex task that\nrequires precise perception and control. Deep neural networks have shown\nremarkable success in grasp synthesis by learning rich and abstract\nrepresentations of objects. When deployed at the edge, these models can enable\nlow-latency, low-power inference, making real-time grasping feasible in\nresource-constrained environments. This work implements Heatmap-Guided Grasp\nDetection, an end-to-end framework for the detection of 6-Dof grasp poses, on\nthe GAP9 RISC-V System-on-Chip. The model is optimised using hardware-aware\ntechniques, including input dimensionality reduction, model partitioning, and\nquantisation. Experimental evaluation on the GraspNet-1Billion benchmark\nvalidates the feasibility of fully on-chip inference, highlighting the\npotential of low-power MCUs for real-time, autonomous manipulation.", "AI": {"tldr": "\u7814\u7a76\u5c06\u6293\u53d6\u68c0\u6d4b\u6846\u67b6\u5728 GAP9 RISC-V \u7cfb\u7edf\u82af\u7247\u4e0a\u5b9e\u73b0\u5e76\u4f18\u5316\uff0c\u9a8c\u8bc1\u4e86\u4f4e\u529f\u8017\u5fae\u63a7\u5236\u5668\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u5b9e\u73b0\u5b9e\u65f6\u6293\u53d6\u64cd\u4f5c\u7684\u53ef\u884c\u6027\u3002", "motivation": "\u4e3a\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u4e2d\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u3001\u4f4e\u529f\u8017\u7684\u5b9e\u65f6\u6293\u53d6\uff0c\u7814\u7a76\u65e8\u5728\u5c06\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u90e8\u7f72\u5230\u8fb9\u7f18\u8bbe\u5907\u4e0a\u3002", "method": "\u672c\u7814\u7a76\u5728 GAP9 RISC-V \u7cfb\u7edf\u82af\u7247\u4e0a\u5b9e\u73b0\u4e86 Heatmap-Guided Grasp Detection \u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b 6-DoF \u6293\u53d6\u59ff\u6001\u3002\u901a\u8fc7\u8f93\u5165\u964d\u7ef4\u3001\u6a21\u578b\u5206\u533a\u548c\u91cf\u5316\u7b49\u786c\u4ef6\u611f\u77e5\u6280\u672f\u5bf9\u6a21\u578b\u8fdb\u884c\u4e86\u4f18\u5316\u3002", "result": "\u5728 GraspNet-1Billion \u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u8bc4\u4f30\u9a8c\u8bc1\u4e86\u5b8c\u5168\u5728\u82af\u7247\u4e0a\u8fdb\u884c\u63a8\u7406\u7684\u53ef\u884c\u6027\uff0c\u8bc1\u660e\u4e86\u4f4e\u529f\u8017\u5fae\u63a7\u5236\u5668\u5728\u5b9e\u65f6\u81ea\u4e3b\u64cd\u4f5c\u65b9\u9762\u7684\u6f5c\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u5728\u4f4e\u529f\u8017\u5fae\u63a7\u5236\u5668\u4e0a\u5b9e\u73b0\u5b9e\u65f6\u81ea\u4e3b\u6293\u53d6\u64cd\u4f5c\u7684\u53ef\u884c\u6027\uff0c\u901a\u8fc7\u5728 GAP9 RISC-V \u7cfb\u7edf\u82af\u7247\u4e0a\u5b9e\u73b0\u548c\u4f18\u5316 Heatmap-Guided Grasp Detection \u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u5b8c\u5168\u5728\u82af\u7247\u4e0a\u8fdb\u884c\u63a8\u7406\u3002"}}
{"id": "2507.13618", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13618", "abs": "https://arxiv.org/abs/2507.13618", "authors": ["Shanbo Cheng", "Yu Bao", "Qian Cao", "Luyang Huang", "Liyan Kang", "Zhicheng Liu", "Yu Lu", "Wenhao Zhu", "Zhichao Huang", "Tao Li", "Sitong Liu", "Ningxin Peng", "Shuaijie She", "Lu Xu", "Nuo Xu", "Sen Yang", "Runsheng Yu", "Yiming Yu", "Liehao Zou", "Hang Li", "Lu Lu", "Yuxuan Wang", "Yonghui Wu"], "title": "Seed-X: Building Strong Multilingual Translation LLM with 7B Parameters", "comment": null, "summary": "Multilingual translation stands as a challenging task for large language\nmodels (LLMs) to handle intricate language patterns and stilted translations\nthat arise in automated translations. In this paper, we introduce Seed-X, a\nfamily of open-source LLMs comprising instruct and reasoning models, pushing\nthe limits of translation capability with 7B parameter size. The base model is\npre-trained on a diverse, high-quality dataset encompassing both monolingual\nand bilingual content across 28 languages, harnessing the full potential of\nmultilingual data. The instruct model is then finetuned to translate by\nChain-of-Thought (CoT) reasoning and further enhanced through reinforcement\nlearning (RL) to achieve better generalization across diverse language pairs.\nSeed-X achieves performance comparable to leading closed-source models,\nincluding Gemini-2.5 and GPT-4o, across 28 languages, and significantly\noutperforms larger open-source models in both automatic metrics and human\nevaluations. We share the best practices through our optimization process, and\nmake the parameter public available for advancing translation research and\napplications.", "AI": {"tldr": "Seed-X\u662f\u4e00\u4e2a7B\u53c2\u6570\u7684\u5f00\u6e90\u591a\u8bed\u8a00\u7ffb\u8bd1\u6a21\u578b\u7cfb\u5217\uff0c\u6027\u80fd\u5ab2\u7f8eGPT-4o\u548cGemini-2.5\uff0c\u5e76\u4e14\u4f18\u4e8e\u5176\u4ed6\u5f00\u6e90\u6a21\u578b\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u591a\u8bed\u8a00\u7ffb\u8bd1\u4e2d\u7684\u590d\u6742\u8bed\u8a00\u6a21\u5f0f\u548c\u751f\u786c\u7ffb\u8bd1\u7684\u6311\u6218\u3002", "method": "Seed-X\u662f\u4e00\u4e2a\u5305\u542b\u6307\u4ee4\u548c\u63a8\u7406\u6a21\u578b\u7684\u5f00\u6e90LLM\u7cfb\u5217\uff0c\u5176\u57fa\u7840\u6a21\u578b\u5728\u5305\u542b28\u79cd\u8bed\u8a00\u7684\u5355\u8bed\u548c\u53cc\u8bed\u5185\u5bb9\u7684\u6df7\u5408\u6570\u636e\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\u3002\u6307\u4ee4\u6a21\u578b\u901a\u8fc7\u601d\u7ef4\u94fe\uff08CoT\uff09\u63a8\u7406\u8fdb\u884c\u5fae\u8c03\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u8fdb\u884c\u589e\u5f3a\u3002", "result": "Seed-X\u7cfb\u5217\u6a21\u578b\u57287B\u53c2\u6570\u89c4\u6a21\u4e0b\uff0c\u572828\u79cd\u8bed\u8a00\u7684\u7ffb\u8bd1\u80fd\u529b\u4e0a\u53d6\u5f97\u4e86\u7a81\u7834\u6027\u8fdb\u5c55\uff0c\u6027\u80fd\u5ab2\u7f8e\u9876\u5c16\u95ed\u6e90\u6a21\u578b\uff0c\u5e76\u4f18\u4e8e\u5176\u4ed6\u5f00\u6e90\u6a21\u578b\u3002", "conclusion": "Seed-X\u572828\u79cd\u8bed\u8a00\u7684\u7ffb\u8bd1\u6027\u80fd\u4e0a\u4e0eGemini-2.5\u548cGPT-4o\u7b49\u9886\u5148\u7684\u95ed\u6e90\u6a21\u578b\u76f8\u5f53\uff0c\u5e76\u4e14\u5728\u81ea\u52a8\u8bc4\u4f30\u548c\u4eba\u7c7b\u8bc4\u4f30\u4e2d\u5747\u663e\u8457\u4f18\u4e8e\u66f4\u5927\u7684\u5f00\u6e90\u6a21\u578b\u3002"}}
{"id": "2507.13810", "categories": ["quant-ph", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.13810", "abs": "https://arxiv.org/abs/2507.13810", "authors": ["Theodore Andronikos", "Constantinos Bitsakos", "Konstantinos Nikas", "Georgios I. Goumas", "Nectarios Koziris"], "title": "Quantum Shadows: The Dining Information Brokers", "comment": null, "summary": "This article introduces the innovative Quantum Dining Information Brokers\nProblem, presenting a novel entanglement-based quantum protocol to address it.\nThe scenario involves $n$ information brokers, all located in distinct\ngeographical regions, engaging in a metaphorical virtual dinner. The objective\nis for each broker to share a unique piece of information with all others\nsimultaneously. Unlike previous approaches, this protocol enables a fully\nparallel, single-step communication exchange among all brokers, regardless of\ntheir physical locations. A key feature of this protocol is its ability to\nensure both the anonymity and privacy of all participants are preserved,\nmeaning no broker can discern the identity of the sender behind any received\ninformation. At its core, the Quantum Dining Information Brokers Problem serves\nas a conceptual framework for achieving anonymous, untraceable, and massively\nparallel information exchange in a distributed system. The proposed protocol\nintroduces three significant advancements. First, while quantum protocols for\none-to-many simultaneous information transmission have been developed, this is,\nto the best of our knowledge, one of the first quantum protocols to facilitate\nmany-to-many simultaneous information exchange. Second, it guarantees complete\nanonymity and untraceability for all senders, a critical improvement over\nsequential applications of one-to-many protocols, which fail to ensure such\nrobust anonymity. Third, leveraging quantum entanglement, the protocol operates\nin a fully distributed manner, accommodating brokers in diverse spatial\nlocations. This approach marks a substantial advancement in secure, scalable,\nand anonymous communication, with potential applications in distributed\nenvironments where privacy and parallelism are paramount.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a\u201c\u91cf\u5b50\u7528\u9910\u4fe1\u606f\u7ecf\u7eaa\u4eba\u95ee\u9898\u201d\u7684\u65b0\u6846\u67b6\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u57fa\u4e8e\u91cf\u5b50\u7ea0\u7f20\u7684\u534f\u8bae\u6765\u89e3\u51b3\u5b83\u3002\u8be5\u534f\u8bae\u5141\u8bb8\u591a\u4e2a\u4fe1\u606f\u7ecf\u7eaa\u4eba\u5728\u4fdd\u6301\u533f\u540d\u548c\u9690\u79c1\u7684\u540c\u65f6\uff0c\u5e76\u884c\u5730\u5171\u4eab\u4fe1\u606f\u3002\u4e0e\u73b0\u6709\u534f\u8bae\u76f8\u6bd4\uff0c\u5b83\u5b9e\u73b0\u4e86\u8bb8\u591a\u5bf9\u8bb8\u591a\u901a\u4fe1\u3001\u589e\u5f3a\u7684\u533f\u540d\u6027\uff0c\u5e76\u4e14\u53ef\u4ee5\u5728\u5206\u5e03\u5f0f\u73af\u5883\u4e2d\u8fd0\u884c\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u4fe1\u606f\u7ecf\u7eaa\u4eba\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u7ea0\u7f20\u7684\u91cf\u5b50\u534f\u8bae\uff0c\u8be5\u534f\u8bae\u80fd\u591f\u5b9e\u73b0\u6240\u6709\u7ecf\u7eaa\u4eba\u4e4b\u95f4\u7684\u5b8c\u5168\u5e76\u884c\u3001\u5355\u6b65\u901a\u4fe1\u4ea4\u6362\uff0c\u5e76\u4fdd\u8bc1\u6240\u6709\u53c2\u4e0e\u8005\u7684\u533f\u540d\u6027\u548c\u9690\u79c1\u6027\u3002", "method": "\u8be5\u534f\u8bae\u5229\u7528\u91cf\u5b50\u7ea0\u7f20\u6765\u5b9e\u73b0\u5b8c\u5168\u5206\u5e03\u5f0f\u7684\u901a\u4fe1\uff0c\u80fd\u591f\u5bb9\u7eb3\u4f4d\u4e8e\u4e0d\u540c\u7a7a\u95f4\u4f4d\u7f6e\u7684\u7ecf\u7eaa\u4eba\u3002", "result": "\u8be5\u534f\u8bae\u5b9e\u73b0\u4e86\u8bb8\u591a\u5bf9\u8bb8\u591a\u540c\u65f6\u4fe1\u606f\u4ea4\u6362\uff0c\u4fdd\u8bc1\u4e86\u6240\u6709\u53d1\u9001\u8005\u7684\u5b8c\u5168\u533f\u540d\u6027\u548c\u53ef\u8ffd\u6eaf\u6027\uff0c\u5e76\u5229\u7528\u91cf\u5b50\u7ea0\u7f20\u4ee5\u5b8c\u5168\u5206\u5e03\u5f0f\u7684\u65b9\u5f0f\u8fd0\u884c\u3002", "conclusion": "\u8be5\u534f\u8bae\u5728\u5b89\u5168\u3001\u53ef\u6269\u5c55\u548c\u533f\u540d\u7684\u901a\u4fe1\u65b9\u9762\u53d6\u5f97\u4e86\u91cd\u5927\u8fdb\u5c55\uff0c\u5728\u9700\u8981\u9690\u79c1\u548c\u5e76\u884c\u6027\u7684\u5206\u5e03\u5f0f\u73af\u5883\u4e2d\u5177\u6709\u6f5c\u5728\u5e94\u7528\u3002"}}
{"id": "2507.13575", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13575", "abs": "https://arxiv.org/abs/2507.13575", "authors": ["Hanzhi Zhou", "Erik Hornberger", "Pengsheng Guo", "Xiyou Zhou", "Saiwen Wang", "Xin Wang", "Yifei He", "Xuankai Chang", "Rene Rauch", "Louis D'hauwe", "John Peebles", "Alec Doane", "Kohen Chia", "Jenna Thibodeau", "Zi-Yi Dou", "Yuanyang Zhang", "Ruoming Pang", "Reed Li", "Zhifeng Chen", "Jeremy Warner", "Zhaoyang Xu", "Sophy Lee", "David Mizrahi", "Ramsey Tantawi", "Chris Chaney", "Kelsey Peterson", "Jun Qin", "Alex Dombrowski", "Mira Chiang", "Aiswarya Raghavan", "Gerard Casamayor", "Qibin Chen", "Aonan Zhang", "Nathalie Tran", "Jianyu Wang", "Hang Su", "Thomas Voice", "Alessandro Pappalardo", "Brycen Wershing", "Prasanth Yadla", "Rui Li", "Priyal Chhatrapati", "Ismael Fernandez", "Yusuf Goren", "Xin Zheng", "Forrest Huang", "Tao Lei", "Eray Yildiz", "Alper Kokmen", "Gokul Santhanam", "Areeba Kamal", "Kaan Elgin", "Dian Ang Yap", "Jeremy Liu", "Peter Gray", "Howard Xing", "Kieran Liu", "Matteo Ronchi", "Moritz Schwarzer-Becker", "Yun Zhu", "Mandana Saebi", "Jeremy Snow", "David Griffiths", "Guillaume Tartavel", "Erin Feldman", "Simon Lehnerer", "Fernando Berm\u00fadez-Medina", "Hans Han", "Joe Zhou", "Xiaoyi Ren", "Sujeeth Reddy", "Zirui Wang", "Tom Gunter", "Albert Antony", "Yuanzhi Li", "John Dennison", "Tony Sun", "Yena Han", "Yi Qin", "Sam Davarnia", "Jeffrey Bigham", "Wayne Shan", "Hannah Gillis Coleman", "Guillaume Klein", "Peng Liu", "Muyang Yu", "Jack Cackler", "Yuan Gao", "Crystal Xiao", "Binazir Karimzadeh", "Zhengdong Zhang", "Felix Bai", "Albin Madappally Jose", "Feng Nan", "Nazir Kamaldin", "Dong Yin", "Hans Hao", "Yanchao Sun", "Yi Hua", "Charles Maalouf", "Alex Guillen Garcia", "Guoli Yin", "Lezhi Li", "Mohana Prasad Sathya Moorthy", "Hongbin Gao", "Jay Tang", "Joanna Arreaza-Taylor", "Faye Lao", "Carina Peng", "Josh Shaffer", "Dan Masi", "Sushma Rao", "Tommi Vehvilainen", "Senyu Tong", "Dongcai Shen", "Yang Zhao", "Chris Bartels", "Peter Fu", "Qingqing Cao", "Christopher Neubauer", "Ethan Li", "Mingfei Gao", "Rebecca Callahan", "Richard Wei", "Patrick Dong", "Alex Braunstein", "Sachin Ravi", "Adolfo Lopez Mendez", "Kaiwei Huang", "Kun Duan", "Haoshuo Huang", "Rui Qian", "Stefano Ligas", "Jordan Huffaker", "Dongxu Li", "Bailin Wang", "Nanzhu Wang", "Anuva Agarwal", "Tait Madsen", "Josh Newnham", "Abhishek Sharma", "Zhile Ren", "Deepak Gopinath", "Erik Daxberger", "Saptarshi Guha", "Oron Levy", "Jing Lu", "Nan Dun", "Marc Kirchner", "Yinfei Yang", "Manjot Bilkhu", "Dave Nelson", "Anthony Spalvieri-Kruse", "Juan Lao Tebar", "Yang Xu", "Phani Mutyala", "Gabriel Jacoby-Cooper", "Yingbo Wang", "Karla Vega", "Vishaal Mahtani", "Darren Botten", "Eric Wang", "Hanli Li", "Matthias Paulik", "Haoran Yan", "Navid Shiee", "Yihao Qian", "Bugu Wu", "Qi Zhu", "Ob Adaranijo", "Bhuwan Dhingra", "Zhe Gan", "Nicholas Seidl", "Grace Duanmu", "Rong Situ", "Yiping Ma", "Yin Xia", "David Riazati", "Vasileios Saveris", "Anh Nguyen", "Michael", "Lee", "Patrick Sonnenberg", "Chinguun Erdenebileg", "Yanghao Li", "Vivian Ma", "James Chou", "Isha Garg", "Mark Lee", "Keen You", "Yuhong Li", "Ransen Niu", "Nandhitha Raghuram", "Pulkit Agrawal", "Henry Mason", "Sumeet Singh", "Keyu He", "Hong-You Chen", "Lucas Guibert", "Shiyu Li", "Varsha Paidi", "Narendran Raghavan", "Mingze Xu", "Yuli Yang", "Sergiu Sima", "Irina Belousova", "Sprite Chu", "Afshin Dehghan", "Philipp Dufter", "David Haldimann", "Zhen Yang", "Margit Bowler", "Chang Liu", "Ying-Chang Cheng", "Vivek Rathod", "Syd Evans", "Wilson Tsao", "Dustin Withers", "Haitian Sun", "Biyao Wang", "Peter Grasch", "Walker Cheng", "Yihao Feng", "Vivek Kumar", "Frank Chu", "Victoria M\u00f6nchJuan Haladjian", "Doug Kang", "Jiarui Lu", "Ciro Sannino", "Max Lam", "Floris Weers", "Bowen Pan", "Kenneth Jung", "Dhaval Doshi", "Fangping Shi", "Olli Saarikivi", "Alp Aygar", "Josh Elman", "Cheng Leong", "Eshan Verma", "Matthew Lei", "Jeff Nichols", "Jiulong Shan", "Donald Zhang", "Lawrence Zhou", "Stephen Murphy", "Xianzhi Du", "Chang Lan", "Ankur Jain", "Elmira Amirloo", "Marcin Eichner", "Naomy Sabo", "Anupama Mann Anupama", "David Qiu", "Zhao Meng", "Michael FitzMaurice", "Peng Zhang", "Simon Yeung", "Chen Chen", "Marco Zuliani", "Andrew Hansen", "Yang Lu", "Brent Ramerth", "Ziyi Zhong", "Parsa Mazaheri", "Matthew Hopkins", "Mengyu Li", "Simon Wang", "David Chen", "Farzin Rasteh", "Chong Wang", "Josh Gardner", "Asaf Liberman", "Haoxuan You", "Andrew Walkingshaw", "Xingyu Zhou", "Jinhao Lei", "Yan Meng", "Quentin Keunebroek", "Sam Wiseman", "Anders Boesen Lindbo Larsen", "Yi Zhang", "Zaid Ahmed", "Haiming Gang", "Aaron Franklin", "Kelvin Zou", "Guillaume Seguin", "Jonathan Janke", "Rachel Burger", "Co Giang", "Cheng Shen", "Jen Liu", "Sanskruti Shah", "Xiang Kong", "Yiran Fei", "TJ Collins", "Chen Zhang", "Zhiyun Lu", "Michael Booker", "Qin Ba", "Yasutaka Tanaka", "Andres Romero Mier Y Teran", "Federico Scozzafava", "Regan Poston", "Jane Li", "Eduardo Jimenez", "Bas Straathof", "Karanjeet Singh", "Lindsay Hislop", "Rajat Arora", "Deepa Seshadri", "Boyue Li", "Colorado Reed", "Zhen Li", "TJ Lu", "Yi Wang", "Kaelen Haag", "Nicholas Lusskin", "Raunak Sinha", "Rahul Nair", "Eldon Schoop", "Mary Beth Kery", "Mehrdad Farajtbar", "Brenda Yang", "George Horrell", "Shiwen Zhao", "Dhruti Shah", "Cha Chen", "Bowen Zhang", "Chang Gao", "Devi Krishna", "Jennifer Mallalieu", "Javier Movellan", "Di Feng", "Emily Zhang", "Sam Xu", "Junting Pan", "Dominik Moritz", "Suma Jayaram", "Kevin Smith", "Dongseong Hwang", "Daniel Parilla", "Jiaming Hu", "You-Cyuan Jhang", "Emad Soroush", "Fred Hohman", "Nan Du", "Emma Wang", "Sam Dodge", "Pragnya Sridhar", "Joris Pelemans", "Wei Fang", "Nina Wenzel", "Joseph Yitan Cheng", "Hadas Kotek", "Chung-Cheng Chiu", "Meng Cao", "Haijing Fu", "Ruixuan Hou", "Ke Ye", "Diane Zhu", "Nikhil Bhendawade", "Joseph Astrauskas", "Jian Liu", "Sai Aitharaju", "Wentao Wu", "Artsiom Peshko", "Hyunjik Kim", "Nilesh Shahdadpuri", "Andy De Wang", "Qi Shan", "Piotr Maj", "Raul Rea Menacho", "Justin Lazarow", "Eric Liang Yang", "Arsalan Farooq", "Donghan Yu", "David G\u00fcera", "Minsik Cho", "Kavya Nerella", "Yongqiang Wang", "Tao Jia", "John Park", "Jeff Lai", "Haotian Zhang", "Futang Peng", "Daniele Molinari", "Aparna Rajamani", "Tyler Johnson", "Lauren Gardiner", "Chao Jia", "Violet Yao", "Wojciech Kryscinski", "Xiujun Li", "Shang-Chen Wu"], "title": "Apple Intelligence Foundation Language Models: Tech Report 2025", "comment": null, "summary": "We introduce two multilingual, multimodal foundation language models that\npower Apple Intelligence features across Apple devices and services: i a\n3B-parameter on-device model optimized for Apple silicon through architectural\ninnovations such as KV-cache sharing and 2-bit quantization-aware training; and\nii a scalable server model built on a novel Parallel-Track Mixture-of-Experts\nPT-MoE transformer that combines track parallelism, mixture-of-experts sparse\ncomputation, and interleaved global-local attention to deliver high quality\nwith competitive cost on Apple's Private Cloud Compute platform. Both models\nare trained on large-scale multilingual and multimodal datasets sourced via\nresponsible web crawling, licensed corpora, and high-quality synthetic data,\nthen further refined with supervised fine-tuning and reinforcement learning on\na new asynchronous platform. The resulting models support several additional\nlanguages while understanding images and executing tool calls. In public\nbenchmarks and human evaluations, both the server model and the on-device model\nmatch or surpass comparably sized open baselines.\n  A new Swift-centric Foundation Models framework exposes guided generation,\nconstrained tool calling, and LoRA adapter fine-tuning, allowing developers to\nintegrate these capabilities with a few lines of code. The latest advancements\nin Apple Intelligence models are grounded in our Responsible AI approach with\nsafeguards like content filtering and locale-specific evaluation, as well as\nour commitment to protecting our users' privacy with innovations like Private\nCloud Compute.", "AI": {"tldr": "\u82f9\u679c\u516c\u53f8\u53d1\u5e03\u4e86\u4e24\u4e2a\u65b0\u7684\u8bed\u8a00\u6a21\u578b\uff08\u4e00\u4e2a\u8bbe\u5907\u7aef\u6a21\u578b\u548c\u4e00\u4e2a\u670d\u52a1\u5668\u6a21\u578b\uff09\uff0c\u7528\u4e8e\u82f9\u679c\u667a\u80fd\u529f\u80fd\u3002\u8fd9\u4e9b\u6a21\u578b\u5728\u591a\u8bed\u8a00\u548c\u591a\u6a21\u6001\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u652f\u6301\u65b0\u7684\u5f00\u53d1\u6846\u67b6\uff0c\u540c\u65f6\u6ce8\u91cd\u8d1f\u8d23\u4efb\u7684AI\u548c\u7528\u6237\u9690\u79c1\u3002", "motivation": "\u4e3a\u4e86\u7ed9\u82f9\u679c\u8bbe\u5907\u548c\u670d\u52a1\u7684\u82f9\u679c\u667a\u80fd\u529f\u80fd\u63d0\u4f9b\u652f\u6301\uff0c\u7814\u7a76\u4ecb\u7ecd\u4e86\u4e24\u4e2a\u65b0\u7684\u591a\u8bed\u8a00\u3001\u591a\u6a21\u6001\u57fa\u7840\u8bed\u8a00\u6a21\u578b\u3002", "method": "\u7814\u7a76\u4e2d\u5f15\u5165\u4e86\u4e24\u4e2a\u82f9\u679c\u667a\u80fd\u57fa\u7840\u8bed\u8a00\u6a21\u578b\uff1a1. \u4e00\u4e2a30\u4ebf\u53c2\u6570\u7684\u8bbe\u5907\u7aef\u6a21\u578b\uff0c\u901a\u8fc7KV\u7f13\u5b58\u5171\u4eab\u548c2\u4f4d\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\u7b49\u67b6\u6784\u521b\u65b0\u8fdb\u884c\u4e86\u4f18\u5316\u30022. \u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u670d\u52a1\u5668\u6a21\u578b\uff0c\u57fa\u4e8e\u65b0\u9896\u7684\u5e76\u884c\u8f68\u9053\u6df7\u5408\u4e13\u5bb6\uff08PT-MoE\uff09Transformer\u6784\u5efa\uff0c\u7ed3\u5408\u4e86\u8f68\u9053\u5e76\u884c\u3001\u6df7\u5408\u4e13\u5bb6\u7a00\u758f\u8ba1\u7b97\u548c\u5168\u5c40-\u5c40\u90e8\u4ea4\u9519\u6ce8\u610f\u529b\u3002\u6a21\u578b\u5728\u591a\u8bed\u8a00\u548c\u591a\u6a21\u6001\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u4f18\u5316\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u4e2a\u4ee5Swift\u4e3a\u4e2d\u5fc3\u7684Foundation Models\u6846\u67b6\uff0c\u7528\u4e8e\u96c6\u6210\u8fd9\u4e9b\u80fd\u529b\uff0c\u5e76\u5f3a\u8c03\u4e86\u8d1f\u8d23\u4efb\u7684AI\u65b9\u6cd5\u548c\u9690\u79c1\u4fdd\u62a4\u3002", "result": "\u6240\u4ecb\u7ecd\u7684\u6a21\u578b\u652f\u6301\u66f4\u591a\u8bed\u8a00\uff0c\u80fd\u591f\u7406\u89e3\u56fe\u50cf\u5e76\u6267\u884c\u5de5\u5177\u8c03\u7528\u3002\u5728\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u548c\u4eba\u7c7b\u8bc4\u4f30\u4e2d\uff0c\u670d\u52a1\u5668\u6a21\u578b\u548c\u8bbe\u5907\u7aef\u6a21\u578b\u5747\u80fd\u8fbe\u5230\u6216\u8d85\u8fc7\u540c\u7b49\u89c4\u6a21\u7684\u5f00\u653e\u57fa\u7ebf\u6a21\u578b\u3002\u6b64\u5916\uff0c\u65b0\u63d0\u51fa\u7684Swift-centric Foundation Models\u6846\u67b6\u7b80\u5316\u4e86\u5f00\u53d1\u8005\u7684\u96c6\u6210\u8fc7\u7a0b\u3002", "conclusion": "\u8be5\u7814\u7a76\u4ecb\u7ecd\u4e86\u4e24\u4e2a\u7528\u4e8e\u82f9\u679c\u8bbe\u5907\u548c\u670d\u52a1\u7684\u82f9\u679c\u667a\u80fd\u57fa\u7840\u8bed\u8a00\u6a21\u578b\uff1a\u4e00\u4e2a30\u4ebf\u53c2\u6570\u7684\u8bbe\u5907\u7aef\u6a21\u578b\u548c\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u670d\u52a1\u5668\u6a21\u578b\u3002\u4e24\u4e2a\u6a21\u578b\u90fd\u5728\u591a\u8bed\u8a00\u548c\u591a\u6a21\u6001\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bad\u7ec3\uff0c\u5e76\u5728\u516c\u5171\u57fa\u51c6\u548c\u4eba\u7c7b\u8bc4\u4f30\u4e2d\u4e0e\u540c\u7b49\u89c4\u6a21\u7684\u5f00\u653e\u57fa\u7ebf\u6a21\u578b\u76f8\u5339\u914d\u6216\u8d85\u8d8a\u3002"}}
{"id": "2507.13403", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13403", "abs": "https://arxiv.org/abs/2507.13403", "authors": ["Morteza Bodaghi", "Majid Hosseini", "Raju Gottumukkala", "Ravi Teja Bhupatiraju", "Iftikhar Ahmad", "Moncef Gabbouj"], "title": "UL-DD: A Multimodal Drowsiness Dataset Using Video, Biometric Signals, and Behavioral Data", "comment": null, "summary": "In this study, we present a comprehensive public dataset for driver\ndrowsiness detection, integrating multimodal signals of facial, behavioral, and\nbiometric indicators. Our dataset includes 3D facial video using a depth\ncamera, IR camera footage, posterior videos, and biometric signals such as\nheart rate, electrodermal activity, blood oxygen saturation, skin temperature,\nand accelerometer data. This data set provides grip sensor data from the\nsteering wheel and telemetry data from the American truck simulator game to\nprovide more information about drivers' behavior while they are alert and\ndrowsy. Drowsiness levels were self-reported every four minutes using the\nKarolinska Sleepiness Scale (KSS). The simulation environment consists of three\nmonitor setups, and the driving condition is completely like a car. Data were\ncollected from 19 subjects (15 M, 4 F) in two conditions: when they were fully\nalert and when they exhibited signs of sleepiness. Unlike other datasets, our\nmultimodal dataset has a continuous duration of 40 minutes for each data\ncollection session per subject, contributing to a total length of 1,400\nminutes, and we recorded gradual changes in the driver state rather than\ndiscrete alert/drowsy labels. This study aims to create a comprehensive\nmultimodal dataset of driver drowsiness that captures a wider range of\nphysiological, behavioral, and driving-related signals. The dataset will be\navailable upon request to the corresponding author.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.14077", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14077", "abs": "https://arxiv.org/abs/2507.14077", "authors": ["Temiloluwa Prioleau", "Baiying Lu", "Yanjun Cui"], "title": "Glucose-ML: A collection of longitudinal diabetes datasets for development of robust AI solutions", "comment": "19 pages, 3 figures, 6 tables", "summary": "Artificial intelligence (AI) algorithms are a critical part of\nstate-of-the-art digital health technology for diabetes management. Yet, access\nto large high-quality datasets is creating barriers that impede development of\nrobust AI solutions. To accelerate development of transparent, reproducible,\nand robust AI solutions, we present Glucose-ML, a collection of 10 publicly\navailable diabetes datasets, released within the last 7 years (i.e., 2018 -\n2025). The Glucose-ML collection comprises over 300,000 days of continuous\nglucose monitor (CGM) data with a total of 38 million glucose samples collected\nfrom 2500+ people across 4 countries. Participants include persons living with\ntype 1 diabetes, type 2 diabetes, prediabetes, and no diabetes. To support\nresearchers and innovators with using this rich collection of diabetes\ndatasets, we present a comparative analysis to guide algorithm developers with\ndata selection. Additionally, we conduct a case study for the task of blood\nglucose prediction - one of the most common AI tasks within the field. Through\nthis case study, we provide a benchmark for short-term blood glucose prediction\nacross all 10 publicly available diabetes datasets within the Glucose-ML\ncollection. We show that the same algorithm can have significantly different\nprediction results when developed/evaluated with different datasets. Findings\nfrom this study are then used to inform recommendations for developing robust\nAI solutions within the diabetes or broader health domain. We provide direct\nlinks to each longitudinal diabetes dataset in the Glucose-ML collection and\nopenly provide our code.", "AI": {"tldr": "\u8be5\u7814\u7a76\u53d1\u5e03\u4e86\u4e00\u4e2a\u540d\u4e3a Glucose-ML \u7684\u7cd6\u5c3f\u75c5\u6570\u636e\u96c6\u96c6\u5408\uff0c\u5305\u542b 10 \u4e2a\u516c\u5f00\u6570\u636e\u96c6\uff0c\u65e8\u5728\u89e3\u51b3 AI \u5f00\u53d1\u4e2d\u7684\u6570\u636e\u74f6\u9888\u95ee\u9898\u3002\u7814\u7a76\u901a\u8fc7\u6bd4\u8f83\u5206\u6790\u548c\u8840\u7cd6\u9884\u6d4b\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u6570\u636e\u96c6\u5bf9 AI \u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u4e3a\u5f00\u53d1\u66f4\u53ef\u9760\u7684\u5065\u5eb7 AI \u63d0\u4f9b\u4e86\u5efa\u8bae\u3002", "motivation": "\u4e3a\u4e86\u52a0\u901f\u5f00\u53d1\u900f\u660e\u3001\u53ef\u590d\u73b0\u548c\u9c81\u68d2\u7684 AI \u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u3001\u5927\u89c4\u6a21\u7684\u6570\u636e\u96c6\u662f\u963b\u788d\u7cd6\u5c3f\u75c5\u7ba1\u7406\u9886\u57df AI \u89e3\u51b3\u65b9\u6848\u53d1\u5c55\u7684\u5173\u952e\u74f6\u9888\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u5e76\u6574\u7406\u4e86\u4e00\u4e2a\u540d\u4e3a Glucose-ML \u7684\u6570\u636e\u96c6\u96c6\u5408\uff0c\u8be5\u96c6\u5408\u5305\u542b 10 \u4e2a\u5728\u8fc7\u53bb 7 \u5e74\u5185\uff082018-2025\uff09\u53d1\u5e03\u7684\u3001\u516c\u5f00\u53ef\u7528\u7684\u7cd6\u5c3f\u75c5\u6570\u636e\u96c6\u3002\u6570\u636e\u96c6\u8986\u76d6\u4e86\u8d85\u8fc7 300,000 \u5929\u7684\u8fde\u7eed\u8840\u7cd6\u76d1\u6d4b\uff08CGM\uff09\u6570\u636e\uff0c\u5171\u8ba1 3800 \u4e07\u4e2a\u8840\u7cd6\u6837\u672c\uff0c\u6765\u81ea 4 \u4e2a\u56fd\u5bb6\u7684 2500 \u591a\u540d\u53c2\u4e0e\u8005\uff0c\u6db5\u76d6\u4e86 1 \u578b\u7cd6\u5c3f\u75c5\u30012 \u578b\u7cd6\u5c3f\u75c5\u3001\u7cd6\u5c3f\u75c5\u524d\u671f\u548c\u975e\u7cd6\u5c3f\u75c5\u4eba\u7fa4\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fdb\u884c\u4e86\u6bd4\u8f83\u5206\u6790\u4ee5\u6307\u5bfc\u6570\u636e\u9009\u62e9\uff0c\u5e76\u4ee5\u8840\u7cd6\u9884\u6d4b\u4efb\u52a1\u4e3a\u6848\u4f8b\uff0c\u5bf9 Glucose-ML \u96c6\u5408\u4e2d\u7684\u6240\u6709\u6570\u636e\u96c6\u8fdb\u884c\u4e86\u77ed\u671f\u8840\u7cd6\u9884\u6d4b\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u76f8\u540c\u7684 AI \u7b97\u6cd5\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u53ef\u80fd\u4ea7\u751f\u663e\u8457\u4e0d\u540c\u7684\u9884\u6d4b\u7ed3\u679c\u3002\u901a\u8fc7\u5bf9 Glucose-ML \u96c6\u5408\u4e2d\u7684 10 \u4e2a\u6570\u636e\u96c6\u8fdb\u884c\u77ed\u671f\u8840\u7cd6\u9884\u6d4b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e3a\u8be5\u9886\u57df\u5e38\u89c1\u7684 AI \u4efb\u52a1\u63d0\u4f9b\u4e86\u6027\u80fd\u53c2\u8003\uff0c\u5e76\u4e3a\u5f00\u53d1\u66f4\u9c81\u68d2\u7684 AI \u89e3\u51b3\u65b9\u6848\u63d0\u4f9b\u4e86\u6307\u5bfc\u6027\u5efa\u8bae\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86 Glucose-ML \u6570\u636e\u96c6\uff0c\u5305\u542b 10 \u4e2a\u516c\u5f00\u7684\u7cd6\u5c3f\u75c5\u6570\u636e\u96c6\uff0c\u65e8\u5728\u4fc3\u8fdb\u900f\u660e\u3001\u53ef\u590d\u73b0\u548c\u9c81\u68d2\u7684 AI \u89e3\u51b3\u65b9\u6848\u7684\u5f00\u53d1\u3002\u7814\u7a76\u901a\u8fc7\u6bd4\u8f83\u5206\u6790\u6307\u5bfc\u7b97\u6cd5\u5f00\u53d1\u8005\u8fdb\u884c\u6570\u636e\u9009\u62e9\uff0c\u5e76\u901a\u8fc7\u5bf9\u8840\u7cd6\u9884\u6d4b\u4efb\u52a1\u7684\u6848\u4f8b\u7814\u7a76\uff0c\u4e3a Glucose-ML \u96c6\u5408\u4e2d\u7684\u6240\u6709 10 \u4e2a\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u77ed\u671f\u8840\u7cd6\u9884\u6d4b\u57fa\u51c6\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u76f8\u540c\u7684\u7b97\u6cd5\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u53ef\u80fd\u4ea7\u751f\u663e\u8457\u4e0d\u540c\u7684\u9884\u6d4b\u7ed3\u679c\uff0c\u5e76\u57fa\u4e8e\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u5f00\u53d1\u7cd6\u5c3f\u75c5\u6216\u66f4\u5e7f\u6cdb\u5065\u5eb7\u9886\u57df\u7684\u9c81\u68d2 AI \u89e3\u51b3\u65b9\u6848\u63d0\u4f9b\u4e86\u5efa\u8bae\u3002"}}
{"id": "2507.14043", "categories": ["cs.RO", "cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2507.14043", "abs": "https://arxiv.org/abs/2507.14043", "authors": ["Genliang Li", "Yaxin Cui", "Jinyu Su"], "title": "A multi-strategy improved snake optimizer for three-dimensional UAV path planning and engineering problems", "comment": "59 pages, 22 figures", "summary": "Metaheuristic algorithms have gained widespread application across various\nfields owing to their ability to generate diverse solutions. One such algorithm\nis the Snake Optimizer (SO), a progressive optimization approach. However, SO\nsuffers from the issues of slow convergence speed and susceptibility to local\noptima. In light of these shortcomings, we propose a novel Multi-strategy\nImproved Snake Optimizer (MISO). Firstly, we propose a new adaptive random\ndisturbance strategy based on sine function to alleviate the risk of getting\ntrapped in a local optimum. Secondly, we introduce adaptive Levy flight\nstrategy based on scale factor and leader and endow the male snake leader with\nflight capability, which makes it easier for the algorithm to leap out of the\nlocal optimum and find the global optimum. More importantly, we put forward a\nposition update strategy combining elite leadership and Brownian motion,\neffectively accelerating the convergence speed while ensuring precision.\nFinally, to demonstrate the performance of MISO, we utilize 30 CEC2017 test\nfunctions and the CEC2022 test suite, comparing it with 11 popular algorithms\nacross different dimensions to validate its effectiveness. Moreover, Unmanned\nAerial Vehicle (UAV) has been widely used in various fields due to its\nadvantages of low cost, high mobility and easy operation. However, the UAV path\nplanning problem is crucial for flight safety and efficiency, and there are\nstill challenges in establishing and optimizing the path model. Therefore, we\napply MISO to the UAV 3D path planning problem as well as 6 engineering design\nproblems to assess its feasibility in practical applications. The experimental\nresults demonstrate that MISO exceeds other competitive algorithms in terms of\nsolution quality and stability, establishing its strong potential for\napplication.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u86c7\u4f18\u5316\u7b97\u6cd5\uff08MISO\uff09\uff0c\u901a\u8fc7\u5f15\u5165\u81ea\u9002\u5e94\u968f\u673a\u6270\u52a8\u3001\u81ea\u9002\u5e94 Levy \u98de\u884c\u548c\u7cbe\u82f1\u9886\u5bfc\u529b\u7ed3\u5408\u5e03\u6717\u8fd0\u52a8\u7b49\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u539f\u7b97\u6cd5\u6536\u655b\u6162\u548c\u6613\u9677\u5165\u5c40\u90e8\u6700\u4f18\u7684\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u6d4b\u8bd5\u51fd\u6570\u548c\u5b9e\u9645\u5e94\u7528\uff08\u5982\u65e0\u4eba\u673a\u8def\u5f84\u89c4\u5212\uff09\u4e2d\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u539f\u59cb\u86c7\u4f18\u5316\u7b97\u6cd5\uff08SO\uff09\u5b58\u5728\u6536\u655b\u901f\u5ea6\u6162\u548c\u5bb9\u6613\u9677\u5165\u5c40\u90e8\u6700\u4f18\u7684\u95ee\u9898\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e9b\u7f3a\u70b9\uff0c\u63d0\u51faMISO\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6b63\u5f26\u51fd\u6570\u81ea\u9002\u5e94\u968f\u673a\u6270\u52a8\u7b56\u7565\u3001\u5c3a\u5ea6\u56e0\u5b50\u4e0e\u9886\u5934\u8005\u81ea\u9002\u5e94 Levy \u98de\u884c\u7b56\u7565\u4ee5\u53ca\u7cbe\u82f1\u9886\u8896\u4e0e\u5e03\u6717\u8fd0\u52a8\u76f8\u7ed3\u5408\u7684\u4f4d\u7f6e\u66f4\u65b0\u7b56\u7565\u7684\u591a\u7b56\u7565\u6539\u8fdb\u86c7\u4f18\u5316\u7b97\u6cd5\uff08MISO\uff09\u3002", "result": "MISO\u572830\u4e2aCEC2017\u6d4b\u8bd5\u51fd\u6570\u548cCEC2022\u6d4b\u8bd5\u5957\u4ef6\u4e0a\uff0c\u4e0e11\u79cd\u6d41\u884c\u7b97\u6cd5\u8fdb\u884c\u6bd4\u8f83\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002\u6b64\u5916\uff0cMISO\u5728\u65e0\u4eba\u673a\u4e09\u7ef4\u8def\u5f84\u89c4\u5212\u548c6\u4e2a\u5de5\u7a0b\u8bbe\u8ba1\u95ee\u9898\u4e0a\u7684\u5e94\u7528\u7ed3\u679c\u8868\u660e\uff0cMISO\u5728\u89e3\u7684\u8d28\u91cf\u548c\u7a33\u5b9a\u6027\u65b9\u9762\u4f18\u4e8e\u5176\u4ed6\u7b97\u6cd5\u3002", "conclusion": "MISO\u5728CEC2017\u6d4b\u8bd5\u51fd\u6570\u3001CEC2022\u6d4b\u8bd5\u5957\u4ef6\u4ee5\u53ca\u65e0\u4eba\u673a\u4e09\u7ef4\u8def\u5f84\u89c4\u5212\u548c\u5de5\u7a0b\u8bbe\u8ba1\u95ee\u9898\u4e0a\u5747\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u7b97\u6cd5\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u89e3\u51b3\u590d\u6742\u4f18\u5316\u95ee\u9898\u4e0a\u7684\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.13655", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.13655", "abs": "https://arxiv.org/abs/2507.13655", "authors": ["Teerapong Panboonyuen"], "title": "CU-ICU: Customizing Unsupervised Instruction-Finetuned Language Models for ICU Datasets via Text-to-Text Transfer Transformer", "comment": "12 pages", "summary": "Integrating large language models into specialized domains like healthcare\npresents unique challenges, including domain adaptation and limited labeled\ndata. We introduce CU-ICU, a method for customizing unsupervised\ninstruction-finetuned language models for ICU datasets by leveraging the\nText-to-Text Transfer Transformer (T5) architecture. CU-ICU employs a sparse\nfine-tuning approach that combines few-shot prompting with selective parameter\nupdates, enabling efficient adaptation with minimal supervision. Our evaluation\nacross critical ICU tasks--early sepsis detection, mortality prediction, and\nclinical note generation--demonstrates that CU-ICU consistently improves\npredictive accuracy and interpretability over standard fine-tuning methods.\nNotably, CU-ICU achieves up to a 15% increase in sepsis detection accuracy and\na 20% enhancement in generating clinically relevant explanations while updating\nfewer than 1% of model parameters in its most efficient configuration. These\nresults establish CU-ICU as a scalable, low-overhead solution for delivering\naccurate and interpretable clinical decision support in real-world ICU\nenvironments.", "AI": {"tldr": "CU-ICU \u662f\u4e00\u79cd\u901a\u8fc7\u7a00\u758f\u5fae\u8c03\u5b9a\u5236 ICU \u6570\u636e\u7684 T5 \u6a21\u578b\uff0c\u53ef\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u540c\u65f6\u6240\u9700\u7684\u8ba1\u7b97\u8d44\u6e90\u6700\u5c11\u3002", "motivation": "\u5728 ICU \u7b49\u4e13\u4e1a\u9886\u57df\u96c6\u6210\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u57df\u9002\u5e94\u548c\u6807\u8bb0\u6570\u636e\u6709\u9650\u7b49\u72ec\u7279\u6311\u6218\u3002", "method": "CU-ICU \u91c7\u7528\u7a00\u758f\u5fae\u8c03\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u5c11\u6837\u672c\u63d0\u793a\u548c\u9009\u62e9\u6027\u53c2\u6570\u66f4\u65b0\uff0c\u80fd\u591f\u4ee5\u6700\u5c11\u7684\u76d1\u7763\u8fdb\u884c\u9ad8\u6548\u9002\u5e94\u3002", "result": "CU-ICU \u5728\u8113\u6bd2\u75c7\u65e9\u671f\u68c0\u6d4b\u3001\u6b7b\u4ea1\u7387\u9884\u6d4b\u548c\u4e34\u5e8a\u7b14\u8bb0\u751f\u6210\u7b49\u5173\u952e ICU \u4efb\u52a1\u4e0a\uff0c\u63d0\u9ad8\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5728\u8113\u6bd2\u75c7\u68c0\u6d4b\u51c6\u786e\u6027\u65b9\u9762\u63d0\u9ad8\u4e86 15%\uff0c\u5728\u751f\u6210\u4e34\u5e8a\u76f8\u5173\u89e3\u91ca\u65b9\u9762\u63d0\u9ad8\u4e86 20%\uff0c\u540c\u65f6\u66f4\u65b0\u7684\u6a21\u578b\u53c2\u6570\u4e0d\u5230 1%\u3002", "conclusion": "CU-ICU \u662f\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u4f4e\u5f00\u9500\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u5728\u771f\u5b9e\u7684 ICU \u73af\u5883\u4e2d\u63d0\u4f9b\u51c6\u786e\u4e14\u53ef\u89e3\u91ca\u7684\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u3002"}}
{"id": "2507.13823", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2507.13823", "abs": "https://arxiv.org/abs/2507.13823", "authors": ["Matic Orel", "\u010crt Lozej", "Marko Robnik", "Hua Yan"], "title": "Quantum chaos and semiclassical behavior in mushroom billiards I: Spectral statistics", "comment": "17 pages, 18 figures", "summary": "We study the aspects of quantum chaos in mushroom billiards introduced by\nBunimovich. This family of billiards classically has the property of mixed\nphase space with precisely one entirely regular and one fully chaotic (ergodic)\ncomponent, whose size depends on the width w of the stem, and has two limiting\ngeometries, namely the circle (as the integrable system) and stadium (as the\nfully chaotic system). Therefore, this one-parameter system is ideal to study\nthe semiclassical behavior of the quantum counterpart. Here, in paper I, we\nstudy the spectral statistics as a function of the geometry defined by w, and\nas a function of the semiclassical parameter k, which in this case is just the\nwavenumber k. We show that at sufficiently large k the level spacing\ndistribution is excellently described by the Berry-Robnik (BR) distribution\n(without fitting). At lower k the small deviations from it can be well\ndescribed by the Berry-Robnik-Brody (BRB) distribution, which captures the\neffects of weak dynamical localization of Poincar\\'e-Husimi functions. We also\nemploy the analytical theory of the level spacing ratios distribution P(r) for\nmixed-type systems, recently obtained by Yan (2025), which does not require a\nspectral unfolding procedure, and show excellent agreement with numerics in the\nsemiclassical limit of large k. In paper II we shall analyze the eigenstates by\nmeans of Poincar\\'e-Husimi functions.", "AI": {"tldr": "\u8611\u83c7\u53f0\u7403\u7cfb\u7edf\u5728\u534a\u7ecf\u5178\u6781\u9650\u4e0b\uff0c\u5176\u80fd\u7ea7\u95f4\u9694\u5206\u5e03\u7b26\u5408Berry-Robnik\u5206\u5e03\uff0c\u800c\u5728\u4f4e\u6ce2\u6570\u4e0b\u5219\u53ef\u7528Berry-Robnik-Brody\u5206\u5e03\u63cf\u8ff0\uff0c\u5e76\u4e0e\u65b0\u5206\u6790\u7406\u8bba\u4e00\u81f4\u3002", "motivation": "\u8be5\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u8611\u83c7\u53f0\u7403\u7cfb\u7edf\uff08Mushroom Billiards\uff09\u4e2d\u91cf\u5b50\u6df7\u6c8c\u7684\u5404\u4e2a\u65b9\u9762\u3002\u7531\u4e8e\u8be5\u7cfb\u7edf\u5177\u6709\u6df7\u5408\u76f8\u7a7a\u95f4\uff08mixed phase space\uff09\u7684\u7279\u70b9\uff0c\u5305\u542b\u4e00\u4e2a\u89c4\u5219\u5206\u91cf\u548c\u4e00\u4e2a\u6df7\u6c8c\u5206\u91cf\uff0c\u5e76\u4e14\u5176\u5c3a\u5bf8\u53ef\u4ee5\u901a\u8fc7\u4e00\u4e2a\u53c2\u6570w\u6765\u8c03\u8282\uff0c\u53ef\u4ee5\u4ece\u5b8c\u5168\u53ef\u79ef\u7cfb\u7edf\uff08\u5706\uff09\u8fc7\u6e21\u5230\u5b8c\u5168\u6df7\u6c8c\u7cfb\u7edf\uff08\u4f53\u80b2\u573a\uff09\uff0c\u56e0\u6b64\u8be5\u7cfb\u7edf\u662f\u7814\u7a76\u91cf\u5b50\u5bf9\u5e94\u7269\u534a\u7ecf\u5178\u884c\u4e3a\u7684\u7406\u60f3\u6a21\u578b\u3002\u672c\u7814\u7a76\u5173\u6ce8\u8c31\u7edf\u8ba1\uff08spectral statistics\uff09\u968f\u7cfb\u7edf\u51e0\u4f55\u5f62\u72b6\u548c\u534a\u7ecf\u5178\u53c2\u6570\u7684\u53d8\u5316\u3002", "method": "\u8be5\u7814\u7a76\u901a\u8fc7\u6570\u503c\u6a21\u62df\u548c\u5206\u6790\u7406\u8bba\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u7814\u7a76\u4e86\u4e8c\u7ef4\u8611\u83c7\u53f0\u7403\u7cfb\u7edf\uff08Mushroom Billiards\uff09\u7684\u91cf\u5b50\u6df7\u6c8c\u6027\u8d28\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u7814\u7a76\u4e86\u80fd\u7ea7\u7edf\u8ba1\uff08spectral statistics\uff09\u968f\u51e0\u4f55\u53c2\u6570w\uff08\u53f0\u7403\u6746\u5bbd\u5ea6\uff09\u548c\u534a\u7ecf\u5178\u53c2\u6570k\uff08\u6ce2\u6570\uff09\u7684\u53d8\u5316\u3002\u5229\u7528Berry-Robnik\u5206\u5e03\u548cBerry-Robnik-Brody\u5206\u5e03\u6765\u63cf\u8ff0\u80fd\u7ea7\u95f4\u9694\u5206\u5e03\uff0c\u5e76\u4e0eYan\uff082025\uff09\u63d0\u51fa\u7684\u9002\u7528\u4e8e\u6df7\u5408\u7c7b\u578b\u7cfb\u7edf\u7684\u5206\u6790\u7406\u8bba\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "result": "\u5728\u8db3\u591f\u5927\u7684\u6ce2\u6570k\u4e0b\uff0c\u80fd\u7ea7\u95f4\u9694\u5206\u5e03\u53ef\u4ee5\u7528Berry-Robnik\u5206\u5e03\uff08\u65e0\u9700\u62df\u5408\uff09\u5f88\u597d\u5730\u63cf\u8ff0\u3002\u5728\u8f83\u4f4e\u7684k\u4e0b\uff0cBerry-Robnik-Brody\u5206\u5e03\u53ef\u4ee5\u5f88\u597d\u5730\u63cf\u8ff0\u5176\u4e0eBerry-Robnik\u5206\u5e03\u7684\u5fae\u5c0f\u504f\u5dee\uff0c\u8be5\u5206\u5e03\u80fd\u591f\u6355\u6349Poincar\u00e9-Husimi\u51fd\u6570\u52a8\u529b\u5b66\u5c40\u57df\u5316\u7684\u5f31\u6548\u5e94\u3002\u6b64\u5916\uff0c\u7814\u7a76\u91c7\u7528\u7684\u5206\u6790\u7406\u8bba\u4e0e\u6570\u503c\u7ed3\u679c\u5728k\u8f83\u5927\u7684\u534a\u7ecf\u5178\u6781\u9650\u4e0b\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u4e00\u81f4\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u5728\u8db3\u591f\u5927\u7684\u6ce2\u6570k\u4e0b\uff0c\u80fd\u7ea7\u95f4\u9694\u5206\u5e03\u53ef\u4ee5\u7528Berry-Robnik\u5206\u5e03\uff08\u65e0\u9700\u62df\u5408\uff09\u5f88\u597d\u5730\u63cf\u8ff0\u3002\u5728\u8f83\u4f4e\u7684k\u4e0b\uff0cBerry-Robnik-Brody\u5206\u5e03\u53ef\u4ee5\u5f88\u597d\u5730\u63cf\u8ff0\u5176\u4e0eBerry-Robnik\u5206\u5e03\u7684\u5fae\u5c0f\u504f\u5dee\uff0c\u8be5\u5206\u5e03\u80fd\u591f\u6355\u6349Poincar\u00e9-Husimi\u51fd\u6570\u52a8\u529b\u5b66\u5c40\u57df\u5316\u7684\u5f31\u6548\u5e94\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u91c7\u7528\u4e86\u4e00\u79cd\u5206\u6790\u7406\u8bba\uff0c\u8be5\u7406\u8bba\u9002\u7528\u4e8e\u6df7\u5408\u7c7b\u578b\u7cfb\u7edf\uff0c\u5e76\u4e14\u5728k\u8f83\u5927\u7684\u534a\u7ecf\u5178\u6781\u9650\u4e0b\u4e0e\u6570\u503c\u7ed3\u679c\u9ad8\u5ea6\u4e00\u81f4\u3002"}}
{"id": "2507.13579", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13579", "abs": "https://arxiv.org/abs/2507.13579", "authors": ["Hyunji Nam", "Yanming Wan", "Mickel Liu", "Jianxun Lian", "Natasha Jaques"], "title": "Learning Pluralistic User Preferences through Reinforcement Learning Fine-tuned Summaries", "comment": "20 pages", "summary": "As everyday use cases of large language model (LLM) AI assistants have\nexpanded, it is becoming increasingly important to personalize responses to\nalign to different users' preferences and goals. While reinforcement learning\nfrom human feedback (RLHF) is effective at improving LLMs to be generally more\nhelpful and fluent, it does not account for variability across users, as it\nmodels the entire user population with a single reward model. We present a\nnovel framework, Preference Learning Using Summarization (PLUS), that learns\ntext-based summaries of each user's preferences, characteristics, and past\nconversations. These summaries condition the reward model, enabling it to make\npersonalized predictions about the types of responses valued by each user. We\ntrain the user-summarization model with reinforcement learning, and update the\nreward model simultaneously, creating an online co-adaptation loop. We show\nthat in contrast with prior personalized RLHF techniques or with in-context\nlearning of user information, summaries produced by PLUS capture meaningful\naspects of a user's preferences. Across different pluralistic user datasets, we\nshow that our method is robust to new users and diverse conversation topics.\nAdditionally, we demonstrate that the textual summaries generated about users\ncan be transferred for zero-shot personalization of stronger, proprietary\nmodels like GPT-4. The resulting user summaries are not only concise and\nportable, they are easy for users to interpret and modify, allowing for more\ntransparency and user control in LLM alignment.", "AI": {"tldr": "\u63d0\u51faPLUS\u6846\u67b6\uff0c\u901a\u8fc7\u5b66\u4e60\u7528\u6237\u504f\u597d\u6458\u8981\u6765\u4e2a\u6027\u5316LLM\u54cd\u5e94\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u652f\u6301GPT-4\u7684\u96f6\u6837\u672c\u4e2a\u6027\u5316\u3002", "motivation": "\u968f\u7740LLM AI\u52a9\u624b\u5728\u65e5\u5e38\u4f7f\u7528\u4e2d\u7684\u666e\u53ca\uff0c\u4f7f\u5176\u54cd\u5e94\u7b26\u5408\u4e0d\u540c\u7528\u6237\u7684\u504f\u597d\u548c\u76ee\u6807\u53d8\u5f97\u65e5\u76ca\u91cd\u8981\u3002\u73b0\u6709\u7684RLHF\u65b9\u6cd5\u867d\u7136\u80fd\u63d0\u5347LLM\u7684\u901a\u7528\u6027\u548c\u6d41\u7545\u6027\uff0c\u4f46\u672a\u80fd\u8003\u8651\u7528\u6237\u95f4\u7684\u5dee\u5f02\u6027\uff0c\u56e0\u4e3a\u5b83\u4ec5\u7528\u5355\u4e00\u7684\u5956\u52b1\u6a21\u578b\u6765\u6a21\u62df\u6574\u4e2a\u7528\u6237\u7fa4\u4f53\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPLUS\uff08Preference Learning Using Summarization\uff09\u7684\u65b0\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u5b66\u4e60\u7528\u6237\u504f\u597d\u3001\u7279\u5f81\u548c\u8fc7\u5f80\u5bf9\u8bdd\u7684\u6587\u672c\u6458\u8981\u6765\u4e2a\u6027\u5316LLM\u7684\u54cd\u5e94\u3002\u7528\u6237\u6458\u8981\u7528\u4e8e\u6761\u4ef6\u5316\u5956\u52b1\u6a21\u578b\uff0c\u4f7f\u5176\u80fd\u591f\u9488\u5bf9\u6bcf\u4e2a\u7528\u6237\u8fdb\u884c\u4e2a\u6027\u5316\u9884\u6d4b\u3002\u7528\u6237\u6458\u8981\u6a21\u578b\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u540c\u65f6\u66f4\u65b0\u5956\u52b1\u6a21\u578b\uff0c\u5f62\u6210\u4e00\u4e2a\u5728\u7ebf\u7684\u534f\u540c\u9002\u5e94\u5faa\u73af\u3002", "result": "PLUS\u6846\u67b6\u751f\u6210\u7684\u6458\u8981\u80fd\u591f\u6355\u6349\u7528\u6237\u504f\u597d\u7684\u6709\u610f\u4e49\u7684\u65b9\u9762\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u4e2a\u6027\u5316RLHF\u6280\u672f\u6216\u4e0a\u4e0b\u6587\u5b66\u4e60\u65b9\u6cd5\u3002\u5728\u4e0d\u540c\u7684\u591a\u5143\u7528\u6237\u6570\u636e\u96c6\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u65b0\u7528\u6237\u548c\u591a\u6837\u5316\u7684\u5bf9\u8bdd\u4e3b\u9898\u4e0a\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u3002\u6b64\u5916\uff0c\u7528\u6237\u6458\u8981\u53ef\u4ee5\u8fc1\u79fb\u7528\u4e8e\u5bf9GPT-4\u7b49\u66f4\u5f3a\u5927\u7684\u4e13\u6709\u6a21\u578b\u8fdb\u884c\u96f6\u6837\u672c\u4e2a\u6027\u5316\u3002", "conclusion": "PLUS\u6846\u67b6\u751f\u6210\u7684\u6587\u672c\u6458\u8981\u7b80\u6d01\u3001\u4fbf\u643a\u4e14\u6613\u4e8e\u7528\u6237\u7406\u89e3\u548c\u4fee\u6539\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86LLM\u5bf9\u9f50\u7684\u900f\u660e\u5ea6\u548c\u7528\u6237\u63a7\u5236\u529b\u3002"}}
{"id": "2507.13404", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13404", "abs": "https://arxiv.org/abs/2507.13404", "authors": ["Delin An", "Pan Du", "Jian-Xun Wang", "Chaoli Wang"], "title": "AortaDiff: Volume-Guided Conditional Diffusion Models for Multi-Branch Aortic Surface Generation", "comment": null, "summary": "Accurate 3D aortic construction is crucial for clinical diagnosis,\npreoperative planning, and computational fluid dynamics (CFD) simulations, as\nit enables the estimation of critical hemodynamic parameters such as blood flow\nvelocity, pressure distribution, and wall shear stress. Existing construction\nmethods often rely on large annotated training datasets and extensive manual\nintervention. While the resulting meshes can serve for visualization purposes,\nthey struggle to produce geometrically consistent, well-constructed surfaces\nsuitable for downstream CFD analysis. To address these challenges, we introduce\nAortaDiff, a diffusion-based framework that generates smooth aortic surfaces\ndirectly from CT/MRI volumes. AortaDiff first employs a volume-guided\nconditional diffusion model (CDM) to iteratively generate aortic centerlines\nconditioned on volumetric medical images. Each centerline point is then\nautomatically used as a prompt to extract the corresponding vessel contour,\nensuring accurate boundary delineation. Finally, the extracted contours are\nfitted into a smooth 3D surface, yielding a continuous, CFD-compatible mesh\nrepresentation. AortaDiff offers distinct advantages over existing methods,\nincluding an end-to-end workflow, minimal dependency on large labeled datasets,\nand the ability to generate CFD-compatible aorta meshes with high geometric\nfidelity. Experimental results demonstrate that AortaDiff performs effectively\neven with limited training data, successfully constructing both normal and\npathologically altered aorta meshes, including cases with aneurysms or\ncoarctation. This capability enables the generation of high-quality\nvisualizations and positions AortaDiff as a practical solution for\ncardiovascular research.", "AI": {"tldr": "AortaDiff\u662f\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u53ef\u4ee5\u76f4\u63a5\u4eceCT/MRI\u56fe\u50cf\u751f\u6210\u5149\u6ed1\u3001CFD\u517c\u5bb9\u7684\u4e3b\u52a8\u8109\u8868\u9762\u548c\u7f51\u683c\uff0c\u65e0\u9700\u5927\u91cf\u6807\u6ce8\u6570\u636e\u548c\u624b\u52a8\u5e72\u9884\u3002", "motivation": "\u73b0\u6709\u4e3b\u52a8\u8109\u6784\u5efa\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u5927\u578b\u6807\u6ce8\u6570\u636e\u96c6\u548c\u5927\u91cf\u624b\u52a8\u5e72\u9884\uff0c\u5e76\u4e14\u751f\u6210\u7684\u7f51\u683c\u96be\u4ee5\u4ea7\u751f\u51e0\u4f55\u4e00\u81f4\u3001\u7ed3\u6784\u826f\u597d\u4e14\u9002\u5408\u4e0b\u6e38CFD\u5206\u6790\u7684\u8868\u9762\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u3002", "method": "AortaDiff\u9996\u5148\u4f7f\u7528\u4f53\u79ef\u5f15\u5bfc\u7684\u6761\u4ef6\u6269\u6563\u6a21\u578b\uff08CDM\uff09\u8fed\u4ee3\u751f\u6210\u4ee5\u4f53\u79ef\u533b\u5b66\u56fe\u50cf\u4e3a\u6761\u4ef6\u7684\u4e2d\u5fc3\u7ebf\u3002\u7136\u540e\uff0c\u6bcf\u4e2a\u4e2d\u5fc3\u7ebf\u70b9\u81ea\u52a8\u7528\u4f5c\u63d0\u793a\uff0c\u63d0\u53d6\u76f8\u5e94\u7684\u8840\u7ba1\u8f6e\u5ed3\uff0c\u786e\u4fdd\u7cbe\u786e\u7684\u8fb9\u754c\u5212\u5206\u3002\u6700\u540e\uff0c\u5c06\u63d0\u53d6\u7684\u8f6e\u5ed3\u62df\u5408\u6210\u5149\u6ed1\u76843D\u8868\u9762\uff0c\u751f\u6210\u8fde\u7eed\u7684\u3001CFD\u517c\u5bb9\u7684\u7f51\u683c\u8868\u793a\u3002", "result": "AortaDiff\u5728\u6709\u9650\u7684\u8bad\u7ec3\u6570\u636e\u4e0b\u8868\u73b0\u6709\u6548\uff0c\u6210\u529f\u6784\u5efa\u4e86\u6b63\u5e38\u548c\u75c5\u53d8\uff08\u5982\u52a8\u8109\u7624\u6216\u72ed\u7a84\uff09\u7684\u4e3b\u52a8\u8109\u7f51\u683c\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u53ef\u89c6\u5316\uff0c\u5e76\u751f\u6210\u4e86CFD\u517c\u5bb9\u7684\u4e3b\u52a8\u8109\u7f51\u683c\uff0c\u5177\u6709\u9ad8\u51e0\u4f55\u4fdd\u771f\u5ea6\u3002", "conclusion": "AortaDiff\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u7684\u6846\u67b6\uff0c\u53ef\u4ee5\u76f4\u63a5\u4eceCT/MRI\u56fe\u50cf\u751f\u6210\u5149\u6ed1\u7684\u4e3b\u52a8\u8109\u8868\u9762\uff0c\u5e76\u80fd\u751f\u6210CFD\u517c\u5bb9\u7684\u7f51\u683c\u3002\u8be5\u65b9\u6cd5\u5728\u6709\u9650\u7684\u8bad\u7ec3\u6570\u636e\u4e0b\u8868\u73b0\u6709\u6548\uff0c\u80fd\u591f\u6210\u529f\u6784\u5efa\u6b63\u5e38\u548c\u75c5\u53d8\uff08\u5982\u52a8\u8109\u7624\u6216\u72ed\u7a84\uff09\u7684\u4e3b\u52a8\u8109\u7f51\u683c\uff0c\u4e3a\u5fc3\u8840\u7ba1\u7814\u7a76\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u7684\u53ef\u89c6\u5316\u548c\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.14097", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14097", "abs": "https://arxiv.org/abs/2507.14097", "authors": ["Hari Iyer", "Neel Macwan", "Atharva Jitendra Hude", "Heejin Jeong", "Shenghan Guo"], "title": "Generative AI-Driven High-Fidelity Human Motion Simulation", "comment": null, "summary": "Human motion simulation (HMS) supports cost-effective evaluation of worker\nbehavior, safety, and productivity in industrial tasks. However, existing\nmethods often suffer from low motion fidelity. This study introduces\nGenerative-AI-Enabled HMS (G-AI-HMS), which integrates text-to-text and\ntext-to-motion models to enhance simulation quality for physical tasks.\nG-AI-HMS tackles two key challenges: (1) translating task descriptions into\nmotion-aware language using Large Language Models aligned with MotionGPT's\ntraining vocabulary, and (2) validating AI-enhanced motions against real human\nmovements using computer vision. Posture estimation algorithms are applied to\nreal-time videos to extract joint landmarks, and motion similarity metrics are\nused to compare them with AI-enhanced sequences. In a case study involving\neight tasks, the AI-enhanced motions showed lower error than human created\ndescriptions in most scenarios, performing better in six tasks based on spatial\naccuracy, four tasks based on alignment after pose normalization, and seven\ntasks based on overall temporal similarity. Statistical analysis showed that\nAI-enhanced prompts significantly (p $<$ 0.0001) reduced joint error and\ntemporal misalignment while retaining comparable posture accuracy.", "AI": {"tldr": "G-AI-HMS\u901a\u8fc7\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u6280\u672f\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4eba\u529b\u8fd0\u52a8\u6a21\u62df\u7684\u51c6\u786e\u6027\u548c\u771f\u5b9e\u6027\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u590d\u6742\u7684\u5de5\u4e1a\u4efb\u52a1\u63cf\u8ff0\u548c\u8fd0\u52a8\u751f\u6210\u65b9\u9762\u3002", "motivation": "\u73b0\u6709\u7684HMS\u65b9\u6cd5\u5728\u8fd0\u52a8\u4fdd\u771f\u5ea6\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u96be\u4ee5\u6ee1\u8db3\u5bf9\u5de5\u4eba\u884c\u4e3a\u3001\u5b89\u5168\u6027\u548c\u751f\u4ea7\u529b\u8fdb\u884c\u7ecf\u6d4e\u9ad8\u6548\u8bc4\u4f30\u7684\u9700\u6c42\u3002\u56e0\u6b64\uff0c\u6709\u5fc5\u8981\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u63d0\u9ad8\u6a21\u62df\u8d28\u91cf\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86G-AI-HMS\uff08Generative-AI-Enabled HMS\uff09\uff0c\u8be5\u65b9\u6cd5\u6574\u5408\u4e86\u6587\u672c\u5230\u6587\u672c\u548c\u6587\u672c\u5230\u8fd0\u52a8\u6a21\u578b\u3002\u5177\u4f53\u800c\u8a00\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5c06\u4efb\u52a1\u63cf\u8ff0\u8f6c\u5316\u4e3a\u8fd0\u52a8\u611f\u77e5\u8bed\u8a00\uff0c\u5e76\u4f7f\u7528\u8ba1\u7b97\u673a\u89c6\u89c9\u6280\u672f\uff08\u59ff\u6001\u4f30\u8ba1\u7b97\u6cd5\uff09\u63d0\u53d6\u7684\u771f\u5b9e\u4eba\u4f53\u8fd0\u52a8\u5173\u8282\u6807\u5fd7\u70b9\u6765\u9a8c\u8bc1AI\u589e\u5f3a\u7684\u8fd0\u52a8\u3002\u901a\u8fc7\u8fd0\u52a8\u76f8\u4f3c\u6027\u6307\u6807\u6bd4\u8f83AI\u751f\u6210\u7684\u5e8f\u5217\u4e0e\u771f\u5b9e\u4eba\u4f53\u8fd0\u52a8\u3002", "result": "G-AI-HMS\u5728\u6d89\u53ca\u516b\u9879\u4efb\u52a1\u7684\u6848\u4f8b\u7814\u7a76\u4e2d\uff0cAI\u589e\u5f3a\u7684\u8fd0\u52a8\u5728\u5927\u591a\u6570\u573a\u666f\u4e0b\u6bd4\u624b\u52a8\u521b\u5efa\u7684\u63cf\u8ff0\u5c55\u73b0\u51fa\u66f4\u4f4e\u7684\u8bef\u5dee\u3002\u5728\u7a7a\u95f4\u51c6\u786e\u6027\u65b9\u9762\uff0cAI\u589e\u5f3a\u7684\u8fd0\u52a8\u5728\u516d\u9879\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u597d\uff1b\u5728\u59ff\u6001\u5f52\u4e00\u5316\u540e\u5bf9\u9f50\u65b9\u9762\uff0c\u5728\u56db\u9879\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u597d\uff1b\u5728\u6574\u4f53\u65f6\u95f4\u76f8\u4f3c\u6027\u65b9\u9762\uff0c\u5728\u4e03\u9879\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u597d\u3002\u7edf\u8ba1\u5206\u6790\u8868\u660e\uff0cAI\u589e\u5f3a\u7684\u63d0\u793a\u5728\u7edf\u8ba1\u5b66\u4e0a\u663e\u8457\uff08p < 0.0001\uff09\u964d\u4f4e\u4e86\u5173\u8282\u8bef\u5dee\u548c\u65f6\u95f4\u9519\u4f4d\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u76f8\u5f53\u7684\u59ff\u6001\u51c6\u786e\u6027\u3002", "conclusion": "G-AI-HMS\u901a\u8fc7\u6574\u5408\u6587\u672c\u5230\u6587\u672c\u548c\u6587\u672c\u5230\u8fd0\u52a8\u6a21\u578b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5de5\u4e1a\u4efb\u52a1\u4e2d\u4eba\u529b\u8fd0\u52a8\u6a21\u62df\u7684\u4fdd\u771f\u5ea6\uff0c\u5e76\u5728\u5927\u591a\u6570\u573a\u666f\u4e0b\u4f18\u4e8e\u624b\u52a8\u521b\u5efa\u7684\u63cf\u8ff0\uff0c\u5728\u7a7a\u95f4\u51c6\u786e\u6027\u3001\u59ff\u6001\u5f52\u4e00\u5316\u540e\u5bf9\u9f50\u548c\u6574\u4f53\u65f6\u95f4\u76f8\u4f3c\u6027\u65b9\u9762\u5747\u8868\u73b0\u51fa\u6539\u8fdb\u3002"}}
{"id": "2507.14049", "categories": ["cs.RO", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14049", "abs": "https://arxiv.org/abs/2507.14049", "authors": ["Pawe\u0142 Budzianowski", "Wesley Maa", "Matthew Freed", "Jingxiang Mo", "Winston Hsiao", "Aaron Xie", "Tomasz M\u0142oduchowski", "Viraj Tipnis", "Benjamin Bolte"], "title": "EdgeVLA: Efficient Vision-Language-Action Models", "comment": null, "summary": "Vision-Language Models (VLMs) have emerged as a promising approach to address\nthe data scarcity challenge in robotics, enabling the development of\ngeneralizable visuomotor control policies. While models like OpenVLA showcase\nthe potential of this paradigm, deploying large-scale VLMs on\nresource-constrained mobile manipulation systems remains a significant hurdle.\nThis paper introduces Edge VLA (EVLA), a novel approach designed to\nsignificantly enhance the inference speed of Vision-Language-Action (VLA)\nmodels. EVLA maintains the representational power of these models while\nenabling real-time performance on edge devices. We achieve this through two key\ninnovations: 1) Eliminating the autoregressive requirement for end-effector\nposition prediction, leading to a 7x speedup in inference, and 2) Leveraging\nthe efficiency of Small Language Models (SLMs), demonstrating comparable\ntraining performance to larger models with significantly reduced computational\ndemands. Our early results demonstrate that EVLA achieves comparable training\ncharacteristics to OpenVLA while offering substantial gains in inference speed\nand memory efficiency. We release our model checkpoints and training\n\\href{https://github.com/kscalelabs/evla }{codebase} to foster further\nresearch.", "AI": {"tldr": "EVLA \u662f\u4e00\u79cd\u65b0\u578b\u65b9\u6cd5\uff0c\u5b83\u901a\u8fc7\u6d88\u9664\u81ea\u56de\u5f52\u8981\u6c42\u5e76\u5229\u7528\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c (VLA) \u6a21\u578b\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u63a8\u7406\u901f\u5ea6\u548c\u5185\u5b58\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u7684\u8868\u5f81\u80fd\u529b\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u673a\u5668\u4eba\u9886\u57df\u6570\u636e\u7a00\u7f3a\u7684\u6311\u6218\uff0c\u5e76\u514b\u670d\u5c06\u5927\u578b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b (VLM) \u90e8\u7f72\u5230\u8d44\u6e90\u53d7\u9650\u7684\u79fb\u52a8\u64cd\u4f5c\u7cfb\u7edf\u4e0a\u7684\u969c\u788d\uff0c\u4ece\u800c\u5728\u673a\u5668\u4eba\u9886\u57df\u5b9e\u73b0\u53ef\u6cdb\u5316\u7684\u89c6\u89c9-\u52a8\u4f5c\u63a7\u5236\u7b56\u7565\u3002", "method": "EVLA\u901a\u8fc7\u4e24\u4e2a\u5173\u952e\u521b\u65b0\u6765\u63d0\u9ad8\u63a8\u7406\u901f\u5ea6\uff1a1. \u6d88\u9664\u5bf9\u672b\u7aef\u6267\u884c\u5668\u4f4d\u7f6e\u9884\u6d4b\u7684\u81ea\u56de\u5f52\u8981\u6c42\uff0c\u5b9e\u73b0\u4e867\u500d\u7684\u63a8\u7406\u52a0\u901f\u30022. \u5229\u7528\u5c0f\u578b\u8bed\u8a00\u6a21\u578b (SLM) \u7684\u6548\u7387\uff0c\u5728\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u9700\u6c42\u7684\u540c\u65f6\uff0c\u5c55\u793a\u4e86\u4e0e\u5927\u578b\u6a21\u578b\u76f8\u5f53\u7684\u8bad\u7ec3\u6027\u80fd\u3002", "result": "EVLA \u5728\u4fdd\u6301\u4e0e OpenVLA \u76f8\u5f53\u7684\u8bad\u7ec3\u7279\u6027\u7684\u540c\u65f6\uff0c\u5728\u63a8\u7406\u901f\u5ea6\u548c\u5185\u5b58\u6548\u7387\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u7684\u63d0\u5347\uff0c\u80fd\u591f\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u73b0\u5b9e\u65f6\u6027\u80fd\u3002", "conclusion": "Edge VLA (EVLA) \u901a\u8fc7\u6d88\u9664\u81ea\u56de\u5f52\u9700\u6c42\u5e76\u5229\u7528\u5c0f\u578b\u8bed\u8a00\u6a21\u578b (SLM)\uff0c\u5728\u4fdd\u6301\u8868\u5f81\u80fd\u529b\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c (VLA) \u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u63a8\u7406\u901f\u5ea6\u548c\u5185\u5b58\u6548\u7387\uff0c\u5b9e\u73b0\u4e86\u4e0e OpenVLA \u76f8\u5f53\u7684\u8bad\u7ec3\u6027\u80fd\u3002"}}
{"id": "2507.13666", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.13666", "abs": "https://arxiv.org/abs/2507.13666", "authors": ["Woo-Chan Kim", "Ji-Hoon Park", "Seong-Whan Lee"], "title": "KiC: Keyword-inspired Cascade for Cost-Efficient Text Generation with LLMs", "comment": null, "summary": "Large language models (LLMs) have demonstrated state-of-the-art performance\nacross a wide range of natural language processing tasks. However,\nhigh-performing models are typically accessible only via APIs, incurring\nsubstantial inference costs. Cascade methods address this by initially\nemploying a cheaper model and escalating to a stronger one only when necessary.\nNevertheless, existing cascade approaches struggle to select a reliable\nrepresentative response and assess the overall reliability of free-form\noutputs, as they rely on exact text matching. To overcome these limitations, we\npropose Keyword-inspired Cascade (KiC), a novel framework for cost-efficient\nfree-form text generation. KiC identifies the most representative answer among\nmultiple outputs from a weaker model and evaluates the semantic alignment of\nother responses with it. Based on the degree of alignment, KiC determines\nwhether to accept the weaker model's output or escalate to a stronger model.\nExperiments on three free-form text generation benchmarks show that KiC\nachieves 97.53 percent of GPT-4's accuracy while reducing API costs by 28.81\npercent on average, and even outperforms GPT-4 in a specific benchmark.", "AI": {"tldr": "KiC\u6846\u67b6\u901a\u8fc7\u6539\u8fdb\u7684\u54cd\u5e94\u9009\u62e9\u548c\u53ef\u9760\u6027\u8bc4\u4f30\u673a\u5236\uff0c\u5728\u4fdd\u6301\u9ad8\u51c6\u786e\u7387\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86LLM API\u7684\u4f7f\u7528\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u7ea7\u8054\u65b9\u6cd5\u5728\u9009\u62e9\u53ef\u9760\u7684\u4ee3\u8868\u6027\u54cd\u5e94\u548c\u8bc4\u4f30\u81ea\u7531\u683c\u5f0f\u8f93\u51fa\u7684\u6574\u4f53\u53ef\u9760\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u56e0\u4e3a\u5b83\u4eec\u4f9d\u8d56\u4e8e\u7cbe\u786e\u7684\u6587\u672c\u5339\u914d\u3002LLMs\u867d\u7136\u6027\u80fd\u4f18\u8d8a\uff0c\u4f46\u901a\u5e38\u53ea\u80fd\u901a\u8fc7API\u8bbf\u95ee\uff0c\u63a8\u7406\u6210\u672c\u9ad8\u6602\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u5173\u952e\u8bcd\u542f\u53d1\u7ea7\u8054\uff08KiC\uff09\u7684\u65b0\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u8bc6\u522b\u5f31\u6a21\u578b\u8f93\u51fa\u4e2d\u6700\u5177\u4ee3\u8868\u6027\u7684\u7b54\u6848\uff0c\u5e76\u8bc4\u4f30\u5176\u4ed6\u54cd\u5e94\u4e0e\u8be5\u7b54\u6848\u7684\u8bed\u4e49\u4e00\u81f4\u6027\uff0c\u6839\u636e\u4e00\u81f4\u6027\u7a0b\u5ea6\u51b3\u5b9a\u662f\u5426\u63a5\u53d7\u5f31\u6a21\u578b\u7684\u8f93\u51fa\u6216\u5347\u7ea7\u5230\u66f4\u5f3a\u7684\u6a21\u578b\u3002", "result": "KiC\u5b9e\u73b0\u4e86GPT-4 97.53%\u7684\u51c6\u786e\u7387\uff0c\u540c\u65f6\u5e73\u5747API\u6210\u672c\u964d\u4f4e\u4e8628.81%\uff0c\u5e76\u5728\u7279\u5b9a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8eGPT-4\u3002", "conclusion": "KiC\u6846\u67b6\u901a\u8fc7\u8bc6\u522b\u5f31\u6a21\u578b\u8f93\u51fa\u4e2d\u6700\u5177\u4ee3\u8868\u6027\u7684\u7b54\u6848\uff0c\u5e76\u8bc4\u4f30\u5176\u4ed6\u54cd\u5e94\u4e0e\u8be5\u7b54\u6848\u7684\u8bed\u4e49\u4e00\u81f4\u6027\uff0c\u6839\u636e\u4e00\u81f4\u6027\u7a0b\u5ea6\u51b3\u5b9a\u63a5\u53d7\u5f31\u6a21\u578b\u7684\u8f93\u51fa\u6216\u5347\u7ea7\u5230\u66f4\u5f3a\u7684\u6a21\u578b\uff0c\u5728\u4e09\u4e2a\u81ea\u7531\u683c\u5f0f\u6587\u672c\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cKiC\u5b9e\u73b0\u4e86GPT-4 97.53%\u7684\u51c6\u786e\u7387\uff0c\u540c\u65f6\u5e73\u5747API\u6210\u672c\u964d\u4f4e\u4e8628.81%\uff0c\u5e76\u5728\u7279\u5b9a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8eGPT-4\u3002"}}
{"id": "2507.13840", "categories": ["quant-ph", "cond-mat.stat-mech", "cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2507.13840", "abs": "https://arxiv.org/abs/2507.13840", "authors": ["Poetri Sonya Tarabunga", "Tobias Haug"], "title": "Quantifying mixed-state entanglement via partial transpose and realignment moments", "comment": "18+13 pages, 7+4 figures", "summary": "Entanglement plays a crucial role in quantum information science and\nmany-body physics, yet quantifying it in mixed quantum many-body systems has\nremained a notoriously difficult problem. Here, we introduce families of\nquantitative entanglement witnesses, constructed from partial transpose and\nrealignment moments, which provide rigorous bounds on entanglement monotones.\nOur witnesses can be efficiently measured using SWAP tests or variants of Bell\nmeasurements, thus making them directly implementable on current hardware.\nLeveraging our witnesses, we present several novel results on entanglement\nproperties of mixed states, both in quantum information and many-body physics.\nWe develop efficient algorithms to test whether mixed states with bounded\nentropy have low or high entanglement, which previously was only possible for\npure states. We also provide an efficient algorithm to test the Schmidt rank\nusing only two-copy measurements, and to test the operator Schmidt rank using\nfour-copy measurements. Further, our witnesses enable robust certification of\nquantum circuit depth even in the presence of noise, a task which so far has\nbeen limited to noiseless circuits only. Finally, we show that the entanglement\nphase diagram of Haar random states, quantified by the partial transpose\nnegativity, can be fully established solely by computing our witness, a result\nthat also applies to any state 4-design. Our witnesses can also be efficiently\ncomputed for matrix product states, thus enabling the characterization of\nentanglement in extensive many-body systems. Finally, we make progress on the\nentanglement required for quantum cryptography, establishing rigorous limits on\npseudoentanglement and pseudorandom density matrices with bounded entropy. Our\nwork opens new avenues for quantifying entanglement in large and noisy quantum\nsystems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7ea0\u7f20\u5224\u636e\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u91cf\u5316\u548c\u9a8c\u8bc1\u5927\u89c4\u6a21\u3001\u542b\u566a\u58f0\u91cf\u5b50\u7cfb\u7edf\u4e2d\u7684\u7ea0\u7f20\uff0c\u5e76\u5728\u91cf\u5b50\u4fe1\u606f\u3001\u591a\u4f53\u7269\u7406\u548c\u91cf\u5b50\u5bc6\u7801\u5b66\u7b49\u9886\u57df\u53d6\u5f97\u4e86\u91cd\u8981\u8fdb\u5c55\u3002", "motivation": "\u91cf\u5316\u6df7\u5408\u91cf\u5b50\u591a\u4f53\u7cfb\u7edf\u4e2d\u7684\u7ea0\u7f20\u662f\u4e00\u4e2a\u516c\u8ba4\u7684\u96be\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5e94\u7528\u4e8e\u5927\u89c4\u6a21\u548c\u542b\u566a\u58f0\u7684\u7cfb\u7edf\u3002", "method": "\u63d0\u51fa\u5e76\u6784\u5efa\u4e86\u57fa\u4e8e\u90e8\u5206\u8f6c\u7f6e\u548c\u91cd\u6392\u77e9\u7684\u5b9a\u91cf\u7ea0\u7f20\u5224\u636e\uff0c\u5229\u7528SWAP\u6d4b\u8bd5\u6216\u8d1d\u5c14\u6d4b\u91cf\u53d8\u79cd\u8fdb\u884c\u6d4b\u91cf\uff0c\u5e76\u5f00\u53d1\u4e86\u76f8\u5e94\u7684\u6709\u6548\u7b97\u6cd5\u6765\u6d4b\u8bd5\u6df7\u5408\u6001\u7684\u7ea0\u7f20\u5ea6\u3001Schmidt\u79e9\u3001\u7b97\u5b50Schmidt\u79e9\uff0c\u4ee5\u53ca\u5bf9\u91cf\u5b50\u7ebf\u8def\u6df1\u5ea6\u548c\u54c8\u5c14\u968f\u673a\u6001\u7684\u7ea0\u7f20\u7279\u6027\u8fdb\u884c\u8868\u5f81\u3002", "result": "\u5f00\u53d1\u4e86\u65b0\u7684\u7ea0\u7f20\u5224\u636e\uff0c\u5b9e\u73b0\u4e86\u5bf9\u6df7\u5408\u6001\u7ea0\u7f20\u7684\u6709\u6548\u6d4b\u8bd5\u548c\u91cf\u5316\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u91cf\u5b50\u4fe1\u606f\u548c\u591a\u4f53\u7269\u7406\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u524d\u666f\uff0c\u5305\u62ec\u5bf9\u91cf\u5b50\u7ebf\u8def\u7684\u9c81\u68d2\u8ba4\u8bc1\u4ee5\u53ca\u5bf9\u7279\u5b9a\u91cf\u5b50\u6001\uff08\u5982\u54c8\u5c14\u968f\u673a\u6001\u548c\u77e9\u9635\u4e58\u79ef\u6001\uff09\u7684\u7ea0\u7f20\u7279\u6027\u8868\u5f81\uff0c\u5e76\u5bf9\u91cf\u5b50\u5bc6\u7801\u5b66\u4e2d\u7684\u5b89\u5168\u95ee\u9898\u8fdb\u884c\u4e86\u7406\u8bba\u4e0a\u7684\u63a8\u8fdb\u3002", "conclusion": "\u8be5\u7814\u7a76\u4ecb\u7ecd\u4e86\u57fa\u4e8e\u90e8\u5206\u8f6c\u7f6e\u548c\u91cd\u6392\u77e9\u7684\u5b9a\u91cf\u7ea0\u7f20\u5224\u636e\uff0c\u4e3a\u7ea0\u7f20\u5355\u8c03\u91cf\u63d0\u4f9b\u4e86\u4e25\u683c\u754c\u9650\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u91cf\u5b50\u4fe1\u606f\u548c\u591a\u4f53\u7269\u7406\u4e2d\u7684\u5e94\u7528\uff0c\u5305\u62ec\u5bf9\u6df7\u5408\u6001\u7ea0\u7f20\u7684\u754c\u5b9a\u3001Schmidt\u79e9\u548c\u7b97\u5b50Schmidt\u79e9\u7684\u6d4b\u8bd5\u3001\u542b\u566a\u58f0\u91cf\u5b50\u7ebf\u8def\u6df1\u5ea6\u7684\u8ba4\u8bc1\uff0c\u4ee5\u53ca\u5bf9\u54c8\u5c14\u968f\u673a\u6001\u548c\u77e9\u9635\u4e58\u79ef\u6001\u7684\u7ea0\u7f20\u7279\u6027\u8868\u5f81\uff0c\u5e76\u5bf9\u91cf\u5b50\u5bc6\u7801\u5b66\u4e2d\u7684\u4f2a\u7ea0\u7f20\u548c\u4f2a\u968f\u673a\u5bc6\u5ea6\u77e9\u9635\u8fdb\u884c\u4e86\u7814\u7a76\u3002"}}
{"id": "2507.13608", "categories": ["cs.LG", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.13608", "abs": "https://arxiv.org/abs/2507.13608", "authors": ["Yudai Hayashi", "Shuhei Goda", "Yuta Saito"], "title": "Off-Policy Evaluation and Learning for Matching Markets", "comment": "RecSys'25", "summary": "Matching users based on mutual preferences is a fundamental aspect of\nservices driven by reciprocal recommendations, such as job search and dating\napplications. Although A/B tests remain the gold standard for evaluating new\npolicies in recommender systems for matching markets, it is costly and\nimpractical for frequent policy updates. Off-Policy Evaluation (OPE) thus plays\na crucial role by enabling the evaluation of recommendation policies using only\noffline logged data naturally collected on the platform. However, unlike\nconventional recommendation settings, the large scale and bidirectional nature\nof user interactions in matching platforms introduce variance issues and\nexacerbate reward sparsity, making standard OPE methods unreliable. To address\nthese challenges and facilitate effective offline evaluation, we propose novel\nOPE estimators, \\textit{DiPS} and \\textit{DPR}, specifically designed for\nmatching markets. Our methods combine elements of the Direct Method (DM),\nInverse Propensity Score (IPS), and Doubly Robust (DR) estimators while\nincorporating intermediate labels, such as initial engagement signals, to\nachieve better bias-variance control in matching markets. Theoretically, we\nderive the bias and variance of the proposed estimators and demonstrate their\nadvantages over conventional methods. Furthermore, we show that these\nestimators can be seamlessly extended to offline policy learning methods for\nimproving recommendation policies for making more matches. We empirically\nevaluate our methods through experiments on both synthetic data and A/B testing\nlogs from a real job-matching platform. The empirical results highlight the\nsuperiority of our approach over existing methods in off-policy evaluation and\nlearning tasks for a variety of configurations.", "AI": {"tldr": "\u63d0\u51fa DiPS \u548c DPR \u4f30\u8ba1\u5668\uff0c\u6539\u8fdb\u4e86\u5339\u914d\u5e02\u573a\u7684\u79bb\u7ebf\u7b56\u7565\u8bc4\u4f30\u548c\u5b66\u4e60\uff0c\u89e3\u51b3\u4e86\u65b9\u5dee\u548c\u5956\u52b1\u7a00\u758f\u6027\u95ee\u9898\uff0c\u5e76\u5728\u5408\u6210\u6570\u636e\u548c\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u5339\u914d\u670d\u52a1\uff08\u5982\u6c42\u804c\u548c\u7ea6\u4f1a\u5e94\u7528\uff09\u4e2d\u57fa\u4e8e\u7528\u6237\u76f8\u4e92\u504f\u597d\u7684\u5339\u914d\u662f\u63a8\u8350\u7cfb\u7edf\u7684\u57fa\u672c\u65b9\u9762\u3002\u867d\u7136 A/B \u6d4b\u8bd5\u662f\u8bc4\u4f30\u65b0\u7b56\u7565\u7684\u9ec4\u91d1\u6807\u51c6\uff0c\u4f46\u6210\u672c\u9ad8\u6602\u4e14\u4e0d\u9002\u5408\u9891\u7e41\u66f4\u65b0\u3002\u56e0\u6b64\uff0c\u79bb\u7ebf\u7b56\u7565\u8bc4\u4f30\uff08OPE\uff09\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709 OPE \u65b9\u6cd5\u5728\u5339\u914d\u5e02\u573a\u4e2d\u7531\u4e8e\u7528\u6237\u4ea4\u4e92\u7684\u89c4\u6a21\u548c\u53cc\u5411\u6027\u800c\u9762\u4e34\u65b9\u5dee\u95ee\u9898\u548c\u5956\u52b1\u7a00\u758f\u6027\u95ee\u9898\uff0c\u5bfc\u81f4\u5176\u4e0d\u53ef\u9760\u3002", "method": "\u63d0\u51fa\u540d\u4e3a DiPS \u548c DPR \u7684\u65b0\u578b OPE \u4f30\u8ba1\u5668\uff0c\u7ed3\u5408\u4e86\u76f4\u63a5\u6cd5\uff08DM\uff09\u3001\u9006\u503e\u5411\u5f97\u5206\uff08IPS\uff09\u548c\u53cc\u91cd\u7a33\u5065\uff08DR\uff09\u4f30\u8ba1\u5668\u7684\u8981\u7d20\uff0c\u5e76\u878d\u5165\u4e86\u4e2d\u95f4\u6807\u7b7e\uff08\u5982\u521d\u59cb\u53c2\u4e0e\u4fe1\u53f7\uff09\u4ee5\u66f4\u597d\u5730\u63a7\u5236\u504f\u5dee-\u65b9\u5dee\u3002", "result": "\u5728\u7406\u8bba\u4e0a\uff0c\u63a8\u5bfc\u4e86\u6240\u63d0\u51fa\u4f30\u8ba1\u5668\u7684\u504f\u5dee\u548c\u65b9\u5dee\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u76f8\u5bf9\u4e8e\u4f20\u7edf\u65b9\u6cd5\u7684\u4f18\u52bf\u3002\u5728\u5b9e\u8df5\u4e2d\uff0c\u901a\u8fc7\u5728\u5408\u6210\u6570\u636e\u548c\u771f\u5b9e\u6c42\u804c\u5339\u914d\u5e73\u53f0\u4e0a\u7684 A/B \u6d4b\u8bd5\u65e5\u5fd7\u8fdb\u884c\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728 OPE \u548c\u79bb\u7ebf\u7b56\u7565\u5b66\u4e60\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684 DiPS \u548c DPR \u4f30\u8ba1\u5668\u901a\u8fc7\u7ed3\u5408\u4e2d\u95f4\u6807\u7b7e\uff08\u5982\u521d\u59cb\u53c2\u4e0e\u4fe1\u53f7\uff09\u6765\u89e3\u51b3\u5339\u914d\u5e02\u573a\u4e2d\u65b9\u5dee\u548c\u5956\u52b1\u7a00\u758f\u6027\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u504f\u5dee-\u65b9\u5dee\u63a7\u5236\u3002\u8fd9\u4e9b\u4f30\u8ba1\u5668\u53ef\u4ee5\u65e0\u7f1d\u6269\u5c55\u5230\u79bb\u7ebf\u7b56\u7565\u5b66\u4e60\u65b9\u6cd5\uff0c\u4ee5\u6539\u8fdb\u5339\u914d\u7b56\u7565\u3002"}}
{"id": "2507.13405", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13405", "abs": "https://arxiv.org/abs/2507.13405", "authors": ["Ishant Chintapatla", "Kazuma Choji", "Naaisha Agarwal", "Andrew Lin", "Hannah You", "Charles Duong", "Kevin Zhu", "Sean O'Brien", "Vasu Sharma"], "title": "COREVQA: A Crowd Observation and Reasoning Entailment Visual Question Answering Benchmark", "comment": null, "summary": "Recently, many benchmarks and datasets have been developed to evaluate\nVision-Language Models (VLMs) using visual question answering (VQA) pairs, and\nmodels have shown significant accuracy improvements. However, these benchmarks\nrarely test the model's ability to accurately complete visual entailment, for\ninstance, accepting or refuting a hypothesis based on the image. To address\nthis, we propose COREVQA (Crowd Observations and Reasoning Entailment), a\nbenchmark of 5608 image and synthetically generated true/false statement pairs,\nwith images derived from the CrowdHuman dataset, to provoke visual entailment\nreasoning on challenging crowded images. Our results show that even the\ntop-performing VLMs achieve accuracy below 80%, with other models performing\nsubstantially worse (39.98%-69.95%). This significant performance gap reveals\nkey limitations in VLMs' ability to reason over certain types of image-question\npairs in crowded scenes.", "AI": {"tldr": "COREVQA\u662f\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u62e5\u6324\u573a\u666f\u4e0b\u7684\u89c6\u89c9\u8574\u542b\u63a8\u7406\u80fd\u529b\uff0c\u7ed3\u679c\u663e\u793a\u73b0\u6709\u6a21\u578b\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5728\u89c6\u89c9\u8574\u542b\u63a8\u7406\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u672a\u80fd\u6709\u6548\u8bc4\u4f30\u6a21\u578b\u5728\u5904\u7406\u62e5\u6324\u573a\u666f\u65f6\u7684\u63a8\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51faCOREVQA\uff08Crowd Observations and Reasoning Entailment\uff09\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b5608\u4e2a\u56fe\u50cf\u548c\u5408\u6210\u7684\u771f/\u5047\u9648\u8ff0\u5bf9\u3002\u56fe\u50cf\u6765\u6e90\u4e8eCrowdHuman\u6570\u636e\u96c6\uff0c\u65e8\u5728\u5f15\u53d1\u5728\u6311\u6218\u6027\u62e5\u6324\u56fe\u50cf\u4e0a\u7684\u89c6\u89c9\u8574\u542b\u63a8\u7406\u3002", "result": "\u5373\u4f7f\u662f\u8868\u73b0\u6700\u4f73\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u5728COREVQA\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u51c6\u786e\u7387\u4e5f\u4f4e\u4e8e80%\uff0c\u5176\u4ed6\u6a21\u578b\u7684\u51c6\u786e\u7387\u572839.98%-69.95%\u4e4b\u95f4\u3002\u8fd9\u8868\u660e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u62e5\u6324\u573a\u666f\u4e0b\u7684\u56fe\u50cf-\u95ee\u9898\u5bf9\u65f6\u5b58\u5728\u663e\u8457\u7684\u63a8\u7406\u80fd\u529b\u5dee\u8ddd\u3002", "conclusion": "\u73b0\u6709\u7684\u89c6\u89c9\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u672a\u80fd\u5145\u5206\u8bc4\u4f30\u6a21\u578b\u5728\u89c6\u89c9\u8574\u542b\u63a8\u7406\u65b9\u9762\u7684\u80fd\u529b\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u62e5\u6324\u573a\u666f\u65f6\u3002COREVQA\u57fa\u51c6\u6d4b\u8bd5\u63ed\u793a\u4e86\u9876\u5c16\u6a21\u578b\u5728\u8be5\u4efb\u52a1\u4e0a\u7684\u51c6\u786e\u7387\u4f4e\u4e8e80%\uff0c\u5176\u4ed6\u6a21\u578b\u8868\u73b0\u66f4\u5dee\uff0c\u51f8\u663e\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7406\u89e3\u548c\u63a8\u7406\u56fe\u50cf\u4e2d\u7684\u590d\u6742\u5173\u7cfb\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002"}}
{"id": "2507.14107", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.14107", "abs": "https://arxiv.org/abs/2507.14107", "authors": ["Viraj Nishesh Darji", "Callie C. Liao", "Duoduo Liao"], "title": "Automated Interpretation of Non-Destructive Evaluation Contour Maps Using Large Language Models for Bridge Condition Assessment", "comment": null, "summary": "Bridge maintenance and safety are essential for transportation authorities,\nand Non-Destructive Evaluation (NDE) techniques are critical to assessing\nstructural integrity. However, interpreting NDE data can be time-consuming and\nrequires expertise, potentially delaying decision-making. Recent advancements\nin Large Language Models (LLMs) offer new ways to automate and improve this\nanalysis. This pilot study introduces a holistic assessment of LLM capabilities\nfor interpreting NDE contour maps and demonstrates the effectiveness of LLMs in\nproviding detailed bridge condition analyses. It establishes a framework for\nintegrating LLMs into bridge inspection workflows, indicating that LLM-assisted\nanalysis can enhance efficiency without compromising accuracy. In this study,\nseveral LLMs are explored with prompts specifically designed to enhance the\nquality of image descriptions, which are applied to interpret five different\nNDE contour maps obtained through technologies for assessing bridge conditions.\nEach LLM model is evaluated based on its ability to produce detailed\ndescriptions, identify defects, provide actionable recommendations, and\ndemonstrate overall accuracy. The research indicates that four of the nine\nmodels provide better image descriptions, effectively covering a wide range of\ntopics related to the bridge's condition. The outputs from these four models\nare summarized using five different LLMs to form a comprehensive overview of\nthe bridge. Notably, LLMs ChatGPT-4 and Claude 3.5 Sonnet generate more\neffective summaries. The findings suggest that LLMs have the potential to\nsignificantly improve efficiency and accuracy. This pilot study presents an\ninnovative approach that leverages LLMs for image captioning in parallel and\nsummarization, enabling faster decision-making in bridge maintenance and\nenhancing infrastructure management and safety assessments.", "AI": {"tldr": "LLM\u5728\u6865\u6881NDE\u6570\u636e\u5206\u6790\u4e2d\u5c55\u73b0\u51fa\u63d0\u9ad8\u6548\u7387\u548c\u51c6\u786e\u6027\u7684\u6f5c\u529b\uff0cChatGPT-4\u548cClaude 3.5 Sonnet\u8868\u73b0\u5c24\u4e3a\u51fa\u8272\u3002", "motivation": "\u4f20\u7edf\u7684NDE\u6570\u636e\u89e3\u8bfb\u8017\u65f6\u4e14\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\uff0c\u53ef\u80fd\u5ef6\u8bef\u51b3\u7b56\u8fc7\u7a0b\u3002LLM\u7684\u51fa\u73b0\u4e3a\u81ea\u52a8\u5316\u548c\u6539\u8fdbNDE\u6570\u636e\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u7684\u9014\u5f84\uff0c\u65e8\u5728\u63d0\u9ad8\u6865\u6881\u7ef4\u62a4\u548c\u5b89\u5168\u8bc4\u4f30\u7684\u6548\u7387\u4e0e\u51c6\u786e\u6027\u3002", "method": "\u672c\u7814\u7a76\u63a2\u7d22\u4e86\u591a\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u89e3\u8bfb\u6865\u6881NDE\uff08\u65e0\u635f\u68c0\u6d4b\uff09\u8f6e\u5ed3\u56fe\u65b9\u9762\u7684\u80fd\u529b\u3002\u7814\u7a76\u4eba\u5458\u8bbe\u8ba1\u4e86\u7279\u5b9a\u7684\u63d0\u793a\u8bcd\uff0c\u7528\u4e8e\u63d0\u5347\u56fe\u50cf\u63cf\u8ff0\u7684\u8d28\u91cf\uff0c\u5e76\u5c06\u8fd9\u4e9b\u63cf\u8ff0\u5e94\u7528\u4e8e\u89e3\u8bfb\u4e94\u79cd\u4e0d\u540c\u7684NDE\u8f6e\u5ed3\u56fe\u3002\u901a\u8fc7\u8bc4\u4f30\u6a21\u578b\u751f\u6210\u8be6\u7ec6\u63cf\u8ff0\u3001\u8bc6\u522b\u7f3a\u9677\u3001\u63d0\u4f9b\u53ef\u884c\u5efa\u8bae\u4ee5\u53ca\u6574\u4f53\u51c6\u786e\u6027\u7684\u80fd\u529b\uff0c\u6765\u8861\u91cf\u6a21\u578b\u7684\u8868\u73b0\u3002\u7814\u7a76\u8fd8\u5bf9\u90e8\u5206LLM\u751f\u6210\u7684\u9ad8\u8d28\u91cf\u56fe\u50cf\u63cf\u8ff0\u8fdb\u884c\u4e86\u603b\u7ed3\uff0c\u5e76\u8fdb\u4e00\u6b65\u7531\u5176\u4ed6LLM\u8fdb\u884c\u7efc\u5408\u6982\u8ff0\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u4e5d\u4e2a\u6d4b\u8bd5\u7684LLM\u6a21\u578b\u4e2d\uff0c\u6709\u56db\u4e2a\u6a21\u578b\u5728\u751f\u6210\u8be6\u7ec6\u56fe\u50cf\u63cf\u8ff0\u65b9\u9762\u8868\u73b0\u66f4\u4f18\uff0c\u80fd\u591f\u6db5\u76d6\u6865\u6881\u72b6\u51b5\u7684\u5e7f\u6cdb\u4fe1\u606f\u3002\u5176\u4e2d\uff0cChatGPT-4\u548cClaude 3.5 Sonnet\u5728\u751f\u6210\u603b\u7ed3\u6027\u63cf\u8ff0\u65b9\u9762\u8868\u73b0\u5c24\u4e3a\u7a81\u51fa\u3002LLM\u8f85\u52a9\u5206\u6790\u5728\u4e0d\u727a\u7272\u51c6\u786e\u6027\u7684\u524d\u63d0\u4e0b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8bc4\u4f30\u6548\u7387\u3002", "conclusion": "LLM\u5728\u6865\u6881\u68c0\u6d4b\u4e2d\u7684\u5e94\u7528\uff0c\u7279\u522b\u662f\u9488\u5bf9NDE\uff08\u65e0\u635f\u68c0\u6d4b\uff09\u6570\u636e\u7684\u5206\u6790\uff0c\u663e\u793a\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u80fd\u591f\u63d0\u9ad8\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u4ece\u800c\u52a0\u5feb\u51b3\u7b56\u8fc7\u7a0b\uff0c\u6539\u5584\u57fa\u7840\u8bbe\u65bd\u7ba1\u7406\u548c\u5b89\u5168\u8bc4\u4f30\u3002"}}
{"id": "2507.14059", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14059", "abs": "https://arxiv.org/abs/2507.14059", "authors": ["Tianyuan Wang", "Mark A Post", "Mathieu Deremetz"], "title": "Design of a Modular Mobile Inspection and Maintenance Robot for an Orbital Servicing Hub", "comment": "In proceedings of the Towards Autonomous Robotic Systems 2025\n  conference (TAROS 2025), York, UK 6 pages, one page of references, 6 figures", "summary": "The use of autonomous robots in space is an essential part of the \"New Space\"\ncommercial ecosystem of assembly and re-use of space hardware components in\nEarth orbit and beyond. The STARFAB project aims to create a ground\ndemonstration of an orbital automated warehouse as a hub for sustainable\ncommercial operations and servicing. A critical part of this fully-autonomous\nrobotic facility will be the capability to monitor, inspect, and assess the\ncondition of both the components stored in the warehouse, and the STARFAB\nfacility itself. This paper introduces ongoing work on the STARFAB Mobile\nInspection Module (MIM). The MIM uses Standard Interconnects (SI) so that it\ncan be carried by Walking Manipulators (WM) as an independently-mobile robot,\nand multiple MIMs can be stored and retrieved as needed for operations on\nSTARFAB. The MIM carries high-resolution cameras, a 3D profilometer, and a\nthermal imaging sensor, with the capability to add other modular sensors. A\ngrasping tool and torque wrench are stored within the modular body for use by\nan attached WM for maintenance operations. Implementation and testing is still\nongoing at the time of writing. This paper details the concept of operations\nfor the MIM as an on-orbit autonomous inspection and maintenance system, the\nmechanical and electronic design of the MIM, and the sensors package used for\nnon-destructive testing.", "AI": {"tldr": "STARFAB project is developing a Mobile Inspection Module (MIM) for an orbital warehouse to autonomously inspect and maintain space hardware using various sensors and a modular design.", "motivation": "The motivation is to create a ground demonstration of an orbital automated warehouse (STARFAB) as a hub for sustainable commercial space operations. A critical aspect of this autonomous facility is the ability to monitor, inspect, and assess the condition of stored components and the facility itself, necessitating the development of the MIM.", "method": "The STARFAB Mobile Inspection Module (MIM) is designed to be a mobile robot for an orbital automated warehouse. It utilizes Standard Interconnects (SI) for attachment to Walking Manipulators (WM) and can be stored and retrieved autonomously. The MIM is equipped with high-resolution cameras, a 3D profilometer, and a thermal imaging sensor, with provisions for modular sensor expansion. It also includes a grasping tool and torque wrench for maintenance tasks performed by the WM.", "result": "The paper details the concept of operations for the MIM as an on-orbit autonomous inspection and maintenance system, the mechanical and electronic design of the MIM, and its sensor package for non-destructive testing. Implementation and testing are still ongoing.", "conclusion": "Implementation and testing of the STARFAB Mobile Inspection Module (MIM) is ongoing, with the paper detailing its concept of operations, design, and sensor package for autonomous on-orbit inspection and maintenance."}}
{"id": "2507.13681", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13681", "abs": "https://arxiv.org/abs/2507.13681", "authors": ["Haoyang Li", "Zhanchao Xu", "Yiming Li", "Xuejia Chen", "Darian Li", "Anxin Tian", "Qingfa Xiao", "Cheng Deng", "Jun Wang", "Qing Li", "Lei Chen", "Mingxuan Yuan"], "title": "LoopServe: An Adaptive Dual-phase LLM Inference Acceleration System for Multi-Turn Dialogues", "comment": null, "summary": "Multi-turn dialogues are essential in many real-world applications of large\nlanguage models, such as chatbots and virtual assistants. As conversation\nhistories become longer, existing large language models face increasing\ncomputational and memory challenges, which hinder their ability to provide\nefficient and responsive interactions. Most current acceleration methods either\ncompress the context or optimize key value caching, but they often rely on\nfixed or position-based heuristics that do not adapt well to the dynamic and\nunpredictable patterns found in actual multi-turn conversations. In this paper,\nwe present LoopServe, an adaptive dual-phase inference acceleration framework\nfor large language models in multi-turn dialogues. LoopServe introduces two\nmain innovations. First, it performs online sparsification during the\nprefilling phase by dynamically selecting the most important parts of the\nattention matrix for each new input. Second, it uses progressive key value\ncompression during decoding by adaptively maintaining a relevant and efficient\ncache based on the most recently generated output tokens. We also propose a\n\\href{https://huggingface.co/datasets/TreeAILab/Multi-turn_Long-context_Benchmark_for_LLMs}{new\nbenchmark} with eleven multi-turn datasets that reflect realistic query\npositions and conversational dependencies. Extensive experiments demonstrate\nthat LoopServe consistently achieves superior effectiveness compared to\nexisting baselines and significantly accelerates LLM inference across a wide\nrange of long-context dialogue tasks.", "AI": {"tldr": "LoopServe \u662f\u4e00\u4e2a\u7528\u4e8e\u591a\u8f6e\u5bf9\u8bdd\u7684 LLM \u63a8\u7406\u52a0\u901f\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u7ebf\u7a00\u758f\u5316\u548c\u6e10\u8fdb\u5f0f\u952e\u503c\u538b\u7f29\u6765\u63d0\u9ad8\u6548\u7387\u3002", "motivation": "\u4e3a\u4e86\u5e94\u5bf9\u957f\u5bf9\u8bdd\u4e2d LLM \u9762\u4e34\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u6311\u6218\uff0c\u5e76\u89e3\u51b3\u73b0\u6709\u52a0\u901f\u65b9\u6cd5\uff08\u5982\u4e0a\u4e0b\u6587\u538b\u7f29\u6216\u952e\u503c\u7f13\u5b58\u4f18\u5316\uff09\u4f9d\u8d56\u4e8e\u56fa\u5b9a\u6216\u57fa\u4e8e\u4f4d\u7f6e\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u672a\u80fd\u9002\u5e94\u52a8\u6001\u5bf9\u8bdd\u6a21\u5f0f\u7684\u7f3a\u70b9\u3002", "method": "LoopServe \u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u7684\u53cc\u9636\u6bb5\u63a8\u7406\u52a0\u901f\u6846\u67b6\u3002\u7b2c\u4e00\u9636\u6bb5\uff0c\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u6ce8\u610f\u529b\u77e9\u9635\u4e2d\u6700\u91cd\u8981\u7684\u90e8\u5206\u8fdb\u884c\u5728\u7ebf\u7a00\u758f\u5316\u3002\u7b2c\u4e8c\u9636\u6bb5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5730\u7ef4\u62a4\u4e00\u4e2a\u57fa\u4e8e\u6700\u8fd1\u751f\u6210\u8f93\u51fa\u6807\u8bb0\u7684\u76f8\u5173\u4e14\u9ad8\u6548\u7684\u7f13\u5b58\u6765\u8fdb\u884c\u6e10\u8fdb\u5f0f\u952e\u503c\u538b\u7f29\u3002", "result": "LoopServe \u663e\u8457\u52a0\u901f\u4e86 LLM \u63a8\u7406\uff0c\u5e76\u5728\u5e7f\u6cdb\u7684\u957f\u4e0a\u4e0b\u6587\u5bf9\u8bdd\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u7684\u6548\u679c\u3002", "conclusion": "LoopServe \u5728\u5404\u79cd\u957f\u4e0a\u4e0b\u6587\u5bf9\u8bdd\u4efb\u52a1\u4e2d\u4e00\u81f4\u5730\u5b9e\u73b0\u4e86\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u7684\u6548\u679c\uff0c\u5e76\u663e\u8457\u52a0\u901f\u4e86 LLM \u63a8\u7406\u3002"}}
{"id": "2507.13860", "categories": ["quant-ph", "cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2507.13860", "abs": "https://arxiv.org/abs/2507.13860", "authors": ["Ronan McElvogue", "Andrew K. Mitchell", "Gabriel T. Landi", "Steve Campbell"], "title": "Nonequilibrium steady states in multi-bath quantum collision models", "comment": "10 pages, 6 figures", "summary": "Collision models provide a simple and versatile setting to capture the\ndynamics of open quantum systems. The standard approach to thermalisition in\nthis setting involves an environment of independent and identically-prepared\nthermal qubits, interacting sequentially for a finite duration $\\Delta t$ with\nthe system. We compare this to a two-bath scenario in which collisional qubits\nare prepared in either their ground or excited states and the environment\ntemperature is encoded in system-environment couplings. The system reaches the\nsame thermal steady state for both settings as $\\Delta t\\to 0$, although even\nin this limit they describe fundamentally different physical processes, with\nthe two-bath setup yielding a nonequilibrium state with finite heat currents.\nNon-Markovian dynamics arise when intra-environment interactions in either\nsetting are introduced. Here, the system in the single-bath setup again reaches\na steady state at the canonical temperature of the bath, but the nonequilibrium\nsteady state of the two-bath setup tends to a different temperature due to the\ngeneration of strong system-environment and intra-environment correlations. The\ntwo-bath setting is particularly suited to studying quantum trajectories, which\nare well-defined also for the non-Markovian case. We showcase this with a\ntrajectory analysis of the heat currents within a two-point measurement scheme.\nOur results provide insights into the dynamics and thermodynamics of\nthermalisation towards nonequilibrium steady states and the role of\nnon-Markovian interactions.", "AI": {"tldr": "\u672c\u7814\u7a76\u6bd4\u8f83\u4e86\u5f00\u653e\u91cf\u5b50\u7cfb\u7edf\u4e2d\u5355\u6d74\u548c\u53cc\u6d74\u78b0\u649e\u6a21\u578b\u7684\u70ed\u5316\u52a8\u529b\u5b66\u3002\u7814\u7a76\u53d1\u73b0\u53cc\u6d74\u6a21\u578b\u5728\u975e\u9a6c\u5c14\u53ef\u592b\u60c5\u51b5\u4e0b\u80fd\u4ea7\u751f\u975e\u5e73\u8861\u7a33\u6001\u548c\u6709\u9650\u70ed\u6d41\uff0c\u5e76\u9002\u7528\u4e8e\u91cf\u5b50\u8f68\u8ff9\u5206\u6790\u3002", "motivation": "\u7814\u7a76\u5f00\u653e\u91cf\u5b50\u7cfb\u7edf\u70ed\u5316\u7684\u52a8\u529b\u5b66\u548c\u70ed\u529b\u5b66\uff0c\u7279\u522b\u662f\u8d8b\u5411\u975e\u5e73\u8861\u7a33\u6001\u7684\u8fc7\u7a0b\uff0c\u4ee5\u53ca\u975e\u9a6c\u5c14\u53ef\u592b\u76f8\u4e92\u4f5c\u7528\u7684\u4f5c\u7528\u3002\u63a2\u7d22\u4e0d\u540c\u7684\u78b0\u649e\u6a21\u578b\uff08\u5355\u6d74\u548c\u53cc\u6d74\uff09\u5982\u4f55\u5f71\u54cd\u7cfb\u7edf\u7684\u70ed\u5316\u8fc7\u7a0b\u548c\u7a33\u6001\u7279\u6027\u3002", "method": "\u901a\u8fc7\u6bd4\u8f83\u5355\u6d74\u548c\u53cc\u6d74\u4e24\u79cd\u78b0\u649e\u6a21\u578b\u6765\u7814\u7a76\u5f00\u653e\u91cf\u5b50\u7cfb\u7edf\u7684\u70ed\u5316\u52a8\u529b\u5b66\u3002\u5728\u5355\u6d74\u6a21\u578b\u4e2d\uff0c\u73af\u5883\u7531\u72ec\u7acb\u4e14\u76f8\u540c\u51c6\u5907\u7684 the-rmal qubit \u7ec4\u6210\uff0c\u5e76\u4e0e\u7cfb\u7edf\u8fdb\u884c\u6709\u9650\u65f6\u95f4 \u0394t \u7684\u987a\u5e8f\u76f8\u4e92\u4f5c\u7528\u3002\u5728\u53cc\u6d74\u6a21\u578b\u4e2d\uff0c\u78b0\u649e qubit \u51c6\u5907\u5728\u57fa\u6001\u6216\u6fc0\u53d1\u6001\uff0c\u73af\u5883\u6e29\u5ea6\u901a\u8fc7\u7cfb\u7edf-\u73af\u5883\u8026\u5408\u7f16\u7801\u3002\u7814\u7a76\u4e86\u5f15\u5165\u73af\u5883\u5185\u90e8\u76f8\u4e92\u4f5c\u7528\u65f6\u7684\u975e\u9a6c\u5c14\u53ef\u592b\u52a8\u529b\u5b66\uff0c\u5e76\u5206\u6790\u4e86\u70ed\u6d41\u7684\u91cf\u5b50\u8f68\u8ff9\u3002", "result": "\u5728\u0394t\u21920\u7684\u6781\u9650\u4e0b\uff0c\u5355\u6d74\u548c\u53cc\u6d74\u6a21\u578b\u5747\u80fd\u4f7f\u7cfb\u7edf\u8fbe\u5230\u76f8\u540c\u7684\u70ed\u7a33\u6001\u3002\u7136\u800c\uff0c\u53cc\u6d74\u6a21\u578b\u63cf\u8ff0\u4e86\u6839\u672c\u4e0d\u540c\u7684\u7269\u7406\u8fc7\u7a0b\uff0c\u5e76\u4ea7\u751f\u4e86\u5177\u6709\u6709\u9650\u70ed\u6d41\u7684\u975e\u5e73\u8861\u6001\u3002\u5f53\u5f15\u5165\u73af\u5883\u5185\u90e8\u76f8\u4e92\u4f5c\u7528\u4ea7\u751f\u975e\u9a6c\u5c14\u53ef\u592b\u52a8\u529b\u5b66\u65f6\uff0c\u5355\u6d74\u6a21\u578b\u7cfb\u7edf\u8fbe\u5230\u6d74\u7684\u89c4\u8303\u6e29\u5ea6\uff0c\u800c\u53cc\u6d74\u6a21\u578b\u7cfb\u7edf\u7531\u4e8e\u5f3a\u70c8\u7684\u7cfb\u7edf-\u73af\u5883\u548c\u73af\u5883\u5185\u90e8\u5173\u8054\uff0c\u8d8b\u5411\u4e8e\u4e0d\u540c\u7684\u6e29\u5ea6\u3002\u53cc\u6d74\u6a21\u578b\u7279\u522b\u9002\u7528\u4e8e\u7814\u7a76\u91cf\u5b50\u8f68\u8ff9\uff0c\u5373\u4f7f\u5728\u975e\u9a6c\u5c14\u53ef\u592b\u60c5\u51b5\u4e0b\u4e5f\u662f\u5982\u6b64\u3002", "conclusion": "\u8be5\u7814\u7a76\u6bd4\u8f83\u4e86\u5355\u6d74\u548c\u53cc\u6d74\u78b0\u649e\u6a21\u578b\u5728\u5f00\u653e\u91cf\u5b50\u7cfb\u7edf\u70ed\u5316\u52a8\u529b\u5b66\u4e2d\u7684\u4f5c\u7528\uff0c\u5e76\u5f15\u5165\u4e86\u975e\u9a6c\u5c14\u53ef\u592b\u52a8\u529b\u5b66\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u5c3d\u7ba1\u4e24\u79cd\u6a21\u578b\u5728\u0394t\u21920\u65f6\u90fd\u80fd\u8fbe\u5230\u76f8\u540c\u7684\u70ed\u7a33\u6001\uff0c\u4f46\u53cc\u6d74\u6a21\u578b\u63cf\u8ff0\u4e86\u4e0d\u540c\u7684\u7269\u7406\u8fc7\u7a0b\uff0c\u5e76\u4ea7\u751f\u975e\u5e73\u8861\u6001\u548c\u6709\u9650\u7684\u70ed\u6d41\u3002\u5728\u5f15\u5165\u73af\u5883\u5185\u90e8\u76f8\u4e92\u4f5c\u7528\u540e\uff0c\u5355\u6d74\u6a21\u578b\u4ecd\u8fbe\u5230\u89c4\u8303\u6e29\u5ea6\uff0c\u800c\u53cc\u6d74\u6a21\u578b\u7531\u4e8e\u5f3a\u70c8\u7684\u7cfb\u7edf-\u73af\u5883\u548c\u73af\u5883\u5185\u90e8\u5173\u8054\uff0c\u8d8b\u5411\u4e8e\u4e0d\u540c\u7684\u6e29\u5ea6\u3002\u53cc\u6d74\u6a21\u578b\u7279\u522b\u9002\u7528\u4e8e\u7814\u7a76\u975e\u9a6c\u5c14\u53ef\u592b\u60c5\u51b5\u4e0b\u7684\u91cf\u5b50\u8f68\u8ff9\uff0c\u5e76\u901a\u8fc7\u53cc\u70b9\u6d4b\u91cf\u65b9\u6848\u5c55\u793a\u4e86\u70ed\u6d41\u7684\u8f68\u8ff9\u5206\u6790\u3002"}}
{"id": "2507.13620", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13620", "abs": "https://arxiv.org/abs/2507.13620", "authors": ["Binxiong Li", "Yuefei Wang", "Xu Xiang", "Xue Li", "Binyu Zhao", "Heyang Gao", "Qinyu Zhao", "Xi Yu"], "title": "Tri-Learn Graph Fusion Network for Attributed Graph Clustering", "comment": "The source code for this study is available at\n  https://github.com/YF-W/Tri-GFN", "summary": "In recent years, models based on Graph Convolutional Networks (GCN) have made\nsignificant strides in the field of graph data analysis. However, challenges\nsuch as over-smoothing and over-compression remain when handling large-scale\nand complex graph datasets, leading to a decline in clustering quality.\nAlthough the Graph Transformer architecture has mitigated some of these issues,\nits performance is still limited when processing heterogeneous graph data. To\naddress these challenges, this study proposes a novel deep clustering framework\nthat comprising GCN, Autoencoder (AE), and Graph Transformer, termed the\nTri-Learn Graph Fusion Network (Tri-GFN). This framework enhances the\ndifferentiation and consistency of global and local information through a\nunique tri-learning mechanism and feature fusion enhancement strategy. The\nframework integrates GCN, AE, and Graph Transformer modules. These components\nare meticulously fused by a triple-channel enhancement module, which maximizes\nthe use of both node attributes and topological structures, ensuring robust\nclustering representation. The tri-learning mechanism allows mutual learning\namong these modules, while the feature fusion strategy enables the model to\ncapture complex relationships, yielding highly discriminative representations\nfor graph clustering. It surpasses many state-of-the-art methods, achieving an\naccuracy improvement of approximately 0.87% on the ACM dataset, 14.14 % on the\nReuters dataset, and 7.58 % on the USPS dataset. Due to its outstanding\nperformance on the Reuters dataset, Tri-GFN can be applied to automatic news\nclassification, topic retrieval, and related fields.", "AI": {"tldr": "Tri-GFN\u6846\u67b6\u901a\u8fc7\u7ed3\u5408GCN\u3001AE\u548c\u56feTransformer\uff0c\u5229\u7528\u4e09\u5143\u5b66\u4e60\u548c\u7279\u5f81\u878d\u5408\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u56fe\u805a\u7c7b\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u805a\u7c7b\u51c6\u786e\u6027\uff0c\u5c24\u5176\u5728Reuters\u6570\u636e\u96c6\u4e0a\u7684\u4f18\u5f02\u8868\u73b0\u4f7f\u5176\u53ef\u5e94\u7528\u4e8e\u65b0\u95fb\u5206\u7c7b\u7b49\u9886\u57df\u3002", "motivation": "\u73b0\u6709\u7684\u56fe\u5377\u79ef\u7f51\u7edc\uff08GCN\uff09\u5728\u5904\u7406\u5927\u89c4\u6a21\u548c\u590d\u6742\u56fe\u6570\u636e\u96c6\u65f6\u5b58\u5728\u8fc7\u5e73\u6ed1\u548c\u8fc7\u538b\u7f29\u95ee\u9898\uff0c\u5bfc\u81f4\u805a\u7c7b\u8d28\u91cf\u4e0b\u964d\u3002\u56feTransformer\u867d\u7136\u7f13\u89e3\u4e86\u90e8\u5206\u95ee\u9898\uff0c\u4f46\u5728\u5904\u7406\u5f02\u6784\u56fe\u6570\u636e\u65f6\u6027\u80fd\u4ecd\u7136\u53d7\u9650\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6df1\u5ea6\u805a\u7c7b\u6846\u67b6Tri-GFN\uff0c\u8be5\u6846\u67b6\u6574\u5408\u4e86\u56fe\u5377\u79ef\u7f51\u7edc\uff08GCN\uff09\u3001\u81ea\u7f16\u7801\u5668\uff08AE\uff09\u548c\u56feTransformer\u3002\u901a\u8fc7\u201c\u4e09\u5143\u5b66\u4e60\u201d\u673a\u5236\u4fc3\u8fdb\u5404\u6a21\u5757\u95f4\u7684\u4e92\u5b66\u4e60\uff0c\u5e76\u91c7\u7528\u7279\u5f81\u878d\u5408\u589e\u5f3a\u7b56\u7565\u6765\u878d\u5408\u8282\u70b9\u5c5e\u6027\u548c\u62d3\u6251\u7ed3\u6784\u4fe1\u606f\uff0c\u4ee5\u83b7\u5f97\u66f4\u5177\u533a\u5206\u6027\u7684\u805a\u7c7b\u8868\u793a\u3002", "result": "Tri-GFN\u6846\u67b6\u5728ACM\u3001Reuters\u548cUSPS\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8eSOTA\u65b9\u6cd5\u7684\u805a\u7c7b\u6548\u679c\uff0c\u51c6\u786e\u7387\u5206\u522b\u63d0\u5347\u4e86\u7ea60.87%\u300114.14%\u548c7.58%\u3002", "conclusion": "Tri-GFN\u6846\u67b6\u901a\u8fc7\u6574\u5408GCN\u3001AE\u548c\u56feTransformer\uff0c\u5e76\u5229\u7528\u5176\u72ec\u7279\u7684\u201c\u4e09\u5143\u5b66\u4e60\u201d\u673a\u5236\u548c\u7279\u5f81\u878d\u5408\u589e\u5f3a\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u548c\u590d\u6742\u56fe\u6570\u636e\u5206\u6790\u4e2d\u7684\u8fc7\u5e73\u6ed1\u3001\u8fc7\u538b\u7f29\u4ee5\u53ca\u5f02\u6784\u56fe\u6570\u636e\u5904\u7406\u6027\u80fd\u53d7\u9650\u7b49\u95ee\u9898\u3002\u8be5\u6846\u67b6\u5728ACM\u3001Reuters\u548cUSPS\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709SOTA\u65b9\u6cd5\u7684\u805a\u7c7b\u6548\u679c\uff0c\u51c6\u786e\u7387\u5206\u522b\u63d0\u5347\u4e86\u7ea60.87%\u300114.14%\u548c7.58%\u3002"}}
{"id": "2507.13407", "categories": ["cs.CV", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.13407", "abs": "https://arxiv.org/abs/2507.13407", "authors": ["Vinu Sankar Sadasivan", "Mehrdad Saberi", "Soheil Feizi"], "title": "IConMark: Robust Interpretable Concept-Based Watermark For AI Images", "comment": "Accepted at ICLR 2025 Workshop on GenAI Watermarking (WMARK)", "summary": "With the rapid rise of generative AI and synthetic media, distinguishing\nAI-generated images from real ones has become crucial in safeguarding against\nmisinformation and ensuring digital authenticity. Traditional watermarking\ntechniques have shown vulnerabilities to adversarial attacks, undermining their\neffectiveness in the presence of attackers. We propose IConMark, a novel\nin-generation robust semantic watermarking method that embeds interpretable\nconcepts into AI-generated images, as a first step toward interpretable\nwatermarking. Unlike traditional methods, which rely on adding noise or\nperturbations to AI-generated images, IConMark incorporates meaningful semantic\nattributes, making it interpretable to humans and hence, resilient to\nadversarial manipulation. This method is not only robust against various image\naugmentations but also human-readable, enabling manual verification of\nwatermarks. We demonstrate a detailed evaluation of IConMark's effectiveness,\ndemonstrating its superiority in terms of detection accuracy and maintaining\nimage quality. Moreover, IConMark can be combined with existing watermarking\ntechniques to further enhance and complement its robustness. We introduce\nIConMark+SS and IConMark+TM, hybrid approaches combining IConMark with\nStegaStamp and TrustMark, respectively, to further bolster robustness against\nmultiple types of image manipulations. Our base watermarking technique\n(IConMark) and its variants (+TM and +SS) achieve 10.8%, 14.5%, and 15.9%\nhigher mean area under the receiver operating characteristic curve (AUROC)\nscores for watermark detection, respectively, compared to the best baseline on\nvarious datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aIConMark\u7684AI\u56fe\u50cf\u8bed\u4e49\u6c34\u5370\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5c06\u53ef\u89e3\u91ca\u7684\u6982\u5ff5\u5d4c\u5165\u56fe\u50cf\uff0c\u4f7f\u5176\u80fd\u62b5\u6297\u653b\u51fb\u4e14\u53ef\u4f9b\u4eba\u7c7b\u9605\u8bfb\u3002\u8be5\u65b9\u6cd5\u5728\u6c34\u5370\u68c0\u6d4b\u7684AUROC\u5f97\u5206\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0fAI\u548c\u5408\u6210\u5a92\u4f53\u7684\u5feb\u901f\u5174\u8d77\uff0c\u533a\u5206AI\u751f\u6210\u7684\u56fe\u50cf\u548c\u771f\u5b9e\u56fe\u50cf\u5bf9\u4e8e\u9632\u8303\u9519\u8bef\u4fe1\u606f\u548c\u786e\u4fdd\u6570\u5b57\u771f\u5b9e\u6027\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u7684\u6c34\u5370\u6280\u672f\u5728\u5bf9\u6297\u6027\u653b\u51fb\u9762\u524d\u663e\u5f97\u8106\u5f31\uff0c\u524a\u5f31\u4e86\u5176\u5728\u653b\u51fb\u8005\u5b58\u5728\u65f6\u7684\u6709\u6548\u6027\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u6c34\u5370\u65b9\u6cd5\u6765\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aIConMark\u7684\u65b0\u9896\u7684\u3001\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u5373\u53ef\u62b5\u6297\u653b\u51fb\u7684\u8bed\u4e49\u6c34\u5370\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5c06\u53ef\u89e3\u91ca\u7684\u6982\u5ff5\u5d4c\u5165AI\u751f\u6210\u7684\u56fe\u50cf\u4e2d\uff0c\u4f7f\u5176\u5177\u6709\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u80fd\u62b5\u6297\u5bf9\u6297\u6027\u64cd\u4f5c\u3002\u8be5\u65b9\u6cd5\u4e0d\u4f9d\u8d56\u4e8e\u5411AI\u751f\u6210\u7684\u56fe\u50cf\u6dfb\u52a0\u566a\u58f0\u6216\u6270\u52a8\uff0c\u800c\u662f\u878d\u5165\u6709\u610f\u4e49\u7684\u8bed\u4e49\u5c5e\u6027\uff0c\u4f7f\u5176\u5bf9\u4eba\u7c7b\u5177\u6709\u53ef\u89e3\u91ca\u6027\uff0c\u56e0\u6b64\u80fd\u591f\u62b5\u6297\u5bf9\u6297\u6027\u64cd\u4f5c\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86IConMark+SS\u548cIConMark+TM\u4e24\u79cd\u6df7\u5408\u65b9\u6cd5\uff0c\u5206\u522b\u5c06IConMark\u4e0eStegaStamp\u548cTrustMark\u76f8\u7ed3\u5408\uff0c\u4ee5\u589e\u5f3a\u5176\u62b5\u6297\u591a\u79cd\u56fe\u50cf\u64cd\u7eb5\u7684\u80fd\u529b\u3002", "result": "IConMark\u88ab\u8bc1\u660e\u80fd\u591f\u62b5\u6297\u5404\u79cd\u56fe\u50cf\u589e\u5f3a\uff0c\u5e76\u4e14\u53ef\u4f9b\u4eba\u7c7b\u9605\u8bfb\uff0c\u80fd\u591f\u8fdb\u884c\u624b\u52a8\u9a8c\u8bc1\u3002\u4e0e\u73b0\u6709\u7684\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0cIConMark\u53ca\u5176\u53d8\u4f53\uff08IConMark+SS\u548cIConMark+TM\uff09\u5728\u6c34\u5370\u68c0\u6d4b\u7684\u5e73\u5747\u63a5\u6536\u5668\u64cd\u4f5c\u7279\u5f81\u66f2\u7ebf\u4e0b\u9762\u79ef\uff08AUROC\uff09\u5f97\u5206\u65b9\u9762\u5206\u522b\u63d0\u9ad8\u4e8610.8%\u300114.5%\u548c15.9%\u3002", "conclusion": "IConMark\u662f\u4e00\u79cd\u65b0\u9896\u7684\u3001\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u5373\u53ef\u62b5\u6297\u653b\u51fb\u7684\u8bed\u4e49\u6c34\u5370\u65b9\u6cd5\uff0c\u5b83\u5c06\u53ef\u89e3\u91ca\u7684\u6982\u5ff5\u5d4c\u5165AI\u751f\u6210\u7684\u56fe\u50cf\u4e2d\uff0c\u4f7f\u5176\u5177\u6709\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u80fd\u62b5\u6297\u5bf9\u6297\u6027\u64cd\u4f5c\u3002\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u80fd\u62b5\u6297\u5404\u79cd\u56fe\u50cf\u589e\u5f3a\uff0c\u800c\u4e14\u53ef\u4f9b\u4eba\u7c7b\u9605\u8bfb\uff0c\u80fd\u591f\u8fdb\u884c\u624b\u52a8\u9a8c\u8bc1\u3002IConMark\u53ca\u5176\u53d8\u4f53\uff08IConMark+SS\u548cIConMark+TM\uff09\u5728\u6c34\u5370\u68c0\u6d4b\u7684AUROC\u5f97\u5206\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002"}}
{"id": "2507.14061", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14061", "abs": "https://arxiv.org/abs/2507.14061", "authors": ["Nataliya Nechyporenko", "Yutong Zhang", "Sean Campbell", "Alessandro Roncone"], "title": "MorphIt: Flexible Spherical Approximation of Robot Morphology for Representation-driven Adaptation", "comment": null, "summary": "What if a robot could rethink its own morphological representation to better\nmeet the demands of diverse tasks? Most robotic systems today treat their\nphysical form as a fixed constraint rather than an adaptive resource, forcing\nthe same rigid geometric representation to serve applications with vastly\ndifferent computational and precision requirements. We introduce MorphIt, a\nnovel algorithm for approximating robot morphology using spherical primitives\nthat balances geometric accuracy with computational efficiency. Unlike existing\napproaches that rely on either labor-intensive manual specification or\ninflexible computational methods, MorphIt implements an automatic\ngradient-based optimization framework with tunable parameters that provides\nexplicit control over the physical fidelity versus computational cost tradeoff.\nQuantitative evaluations demonstrate that MorphIt outperforms baseline\napproaches (Variational Sphere Set Approximation and Adaptive Medial-Axis\nApproximation) across multiple metrics, achieving better mesh approximation\nwith fewer spheres and reduced computational overhead. Our experiments show\nenhanced robot capabilities in collision detection accuracy, contact-rich\ninteraction simulation, and navigation through confined spaces. By dynamically\nadapting geometric representations to task requirements, robots can now exploit\ntheir physical embodiment as an active resource rather than an inflexible\nparameter, opening new frontiers for manipulation in environments where\nphysical form must continuously balance precision with computational\ntractability.", "AI": {"tldr": "MorphIt \u662f\u4e00\u79cd\u65b0\u7684\u673a\u5668\u4eba\u5f62\u6001\u8868\u793a\u7b97\u6cd5\uff0c\u4f7f\u7528\u7403\u5f62\u57fa\u5143\u548c\u68af\u5ea6\u4f18\u5316\u6765\u5e73\u8861\u7cbe\u5ea6\u4e0e\u8ba1\u7b97\u6548\u7387\uff0c\u6bd4\u73b0\u6709\u65b9\u6cd5\u6548\u679c\u66f4\u597d\uff0c\u5e76\u63d0\u5347\u4e86\u673a\u5668\u4eba\u7684\u591a\u79cd\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u673a\u5668\u4eba\u7cfb\u7edf\u901a\u5e38\u5c06\u7269\u7406\u5f62\u6001\u89c6\u4e3a\u56fa\u5b9a\u7ea6\u675f\uff0c\u800c\u975e\u53ef\u9002\u5e94\u7684\u8d44\u6e90\uff0c\u5bfc\u81f4\u76f8\u540c\u7684\u50f5\u5316\u51e0\u4f55\u8868\u793a\u9700\u8981\u6ee1\u8db3\u8ba1\u7b97\u548c\u7cbe\u5ea6\u8981\u6c42\u5dee\u5f02\u5de8\u5927\u7684\u5e94\u7528\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u8ba9\u673a\u5668\u4eba\u80fd\u591f\u91cd\u65b0\u601d\u8003\u5176\u5f62\u6001\u8868\u793a\uff0c\u4ee5\u66f4\u597d\u5730\u9002\u5e94\u591a\u6837\u5316\u7684\u4efb\u52a1\u9700\u6c42\u3002", "method": "MorphIt \u7b97\u6cd5\u4f7f\u7528\u7403\u5f62\u57fa\u5143\u6765\u903c\u8fd1\u673a\u5668\u4eba\u5f62\u6001\uff0c\u5e76\u91c7\u7528\u57fa\u4e8e\u68af\u5ea6\u7684\u4f18\u5316\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5177\u6709\u53ef\u8c03\u53c2\u6570\uff0c\u80fd\u591f\u663e\u5f0f\u63a7\u5236\u51e0\u4f55\u4fdd\u771f\u5ea6\u4e0e\u8ba1\u7b97\u6210\u672c\u4e4b\u95f4\u7684\u6743\u8861\u3002", "result": "MorphIt \u5728\u591a\u9879\u6307\u6807\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff08\u53d8\u5206\u7403\u4f53\u96c6\u903c\u8fd1\u548c\u81ea\u9002\u5e94\u4e2d\u8f74\u903c\u8fd1\uff09\uff0c\u7528\u66f4\u5c11\u7684\u7403\u4f53\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u7f51\u683c\u903c\u8fd1\uff0c\u5e76\u964d\u4f4e\u4e86\u8ba1\u7b97\u5f00\u9500\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u78b0\u649e\u68c0\u6d4b\u7cbe\u5ea6\u3001\u63a5\u89e6\u4e30\u5bcc\u7684\u4ea4\u4e92\u6a21\u62df\u548c\u72ed\u7a84\u7a7a\u95f4\u5bfc\u822a\u65b9\u9762\u589e\u5f3a\u4e86\u673a\u5668\u4eba\u80fd\u529b\u3002", "conclusion": "MorphIt \u901a\u8fc7\u4f7f\u7528\u7403\u5f62\u57fa\u5143\u81ea\u9002\u5e94\u5730\u903c\u8fd1\u673a\u5668\u4eba\u5f62\u6001\uff0c\u5728\u51e0\u4f55\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u4e86\u65b0\u7684\u5e73\u8861\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u68af\u5ea6\u4f18\u5316\u6846\u67b6\u5b9e\u73b0\uff0c\u5141\u8bb8\u7528\u6237\u63a7\u5236\u4fdd\u771f\u5ea6\u548c\u8ba1\u7b97\u6210\u672c\u7684\u6743\u8861\u3002\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0cMorphIt \u5728\u7f51\u683c\u903c\u8fd1\u7cbe\u5ea6\u3001\u6240\u9700\u7403\u4f53\u6570\u91cf\u548c\u8ba1\u7b97\u5f00\u9500\u65b9\u9762\u5747\u8868\u73b0\u66f4\u4f18\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0cMorphIt \u80fd\u591f\u63d0\u9ad8\u673a\u5668\u4eba\u8fdb\u884c\u78b0\u649e\u68c0\u6d4b\u3001\u63a5\u89e6\u4e30\u5bcc\u7684\u4ea4\u4e92\u6a21\u62df\u4ee5\u53ca\u5728\u72ed\u7a84\u7a7a\u95f4\u5bfc\u822a\u7684\u80fd\u529b\u3002\u901a\u8fc7\u4f7f\u51e0\u4f55\u8868\u793a\u80fd\u591f\u6839\u636e\u4efb\u52a1\u9700\u6c42\u52a8\u6001\u8c03\u6574\uff0c\u673a\u5668\u4eba\u53ef\u4ee5\u5229\u7528\u5176\u7269\u7406\u5f62\u6001\u4f5c\u4e3a\u4e00\u79cd\u4e3b\u52a8\u8d44\u6e90\uff0c\u800c\u975e\u56fa\u5b9a\u53c2\u6570\uff0c\u4ece\u800c\u4e3a\u9700\u8981\u5728\u7cbe\u5ea6\u548c\u8ba1\u7b97\u53ef\u884c\u6027\u4e4b\u95f4\u6301\u7eed\u6743\u8861\u7684\u64cd\u63a7\u4efb\u52a1\u5f00\u8f9f\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2507.13705", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.13705", "abs": "https://arxiv.org/abs/2507.13705", "authors": ["Cedric Waterschoot", "Nava Tintarev", "Francesco Barile"], "title": "Consistent Explainers or Unreliable Narrators? Understanding LLM-generated Group Recommendations", "comment": "Short paper accepted at the Nineteenth ACM Conference on Recommender\n  Systems (RecSys '25). Cedric Waterschoot, Nava Tintarev, and Francesco\n  Barile. 2025. Consistent Explainers or Unreliable Narrators? Understanding\n  LLM-generated Group Recommendations. Proceedings of the Nineteenth ACM\n  Conference on Recommender Systems (RecSys '25), Prague, Czech Republic. doi:\n  10.1145/3705328.3748015", "summary": "Large Language Models (LLMs) are increasingly being implemented as joint\ndecision-makers and explanation generators for Group Recommender Systems (GRS).\nIn this paper, we evaluate these recommendations and explanations by comparing\nthem to social choice-based aggregation strategies. Our results indicate that\nLLM-generated recommendations often resembled those produced by Additive\nUtilitarian (ADD) aggregation. However, the explanations typically referred to\naveraging ratings (resembling but not identical to ADD aggregation). Group\nstructure, uniform or divergent, did not impact the recommendations.\nFurthermore, LLMs regularly claimed additional criteria such as user or item\nsimilarity, diversity, or used undefined popularity metrics or thresholds. Our\nfindings have important implications for LLMs in the GRS pipeline as well as\nstandard aggregation strategies. Additional criteria in explanations were\ndependent on the number of ratings in the group scenario, indicating potential\ninefficiency of standard aggregation methods at larger item set sizes.\nAdditionally, inconsistent and ambiguous explanations undermine transparency\nand explainability, which are key motivations behind the use of LLMs for GRS.", "AI": {"tldr": "LLM\u5728GRS\u4e2d\u7684\u63a8\u8350\u7ed3\u679c\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u4f3c\uff0c\u4f46\u89e3\u91ca\u4e0d\u591f\u900f\u660e\uff0c\u5b58\u5728\u6a21\u7cca\u548c\u4e0d\u4e00\u81f4\u6027\u3002", "motivation": "\u8bc4\u4f30LLM\u4f5c\u4e3aGRS\u4e2d\u7684\u8054\u5408\u51b3\u7b56\u8005\u548c\u89e3\u91ca\u751f\u6210\u5668\u7684\u6709\u6548\u6027\uff0c\u4ee5\u53ca\u4e0e\u4f20\u7edf\u805a\u5408\u7b56\u7565\u7684\u5bf9\u6bd4\uff0c\u7279\u522b\u5173\u6ce8\u89e3\u91ca\u7684\u900f\u660e\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u901a\u8fc7\u5bf9\u6bd4LLM\u751f\u6210\u7684\u63a8\u8350\u548c\u89e3\u91ca\u4e0e\u57fa\u4e8e\u793e\u4f1a\u9009\u62e9\u7684\u805a\u5408\u7b56\u7565\uff08\u5982\u52a0\u6027\u6548\u7528\u8bbaADD\uff09\u6765\u8bc4\u4f30LLM\u5728GRS\u4e2d\u7684\u8868\u73b0\u3002", "result": "LLM\u63a8\u8350\u7ed3\u679c\u4e0eADD\u805a\u5408\u76f8\u4f3c\uff1bLLM\u89e3\u91ca\u5e38\u63d0\u53ca\u5e73\u5747\u8bc4\u5206\uff0c\u4f46\u4f1a\u5f15\u5165\u989d\u5916\u6807\u51c6\uff1b\u7fa4\u7ec4\u7ed3\u6784\u4e0d\u5f71\u54cd\u63a8\u8350\uff1b\u989d\u5916\u6807\u51c6\u7684\u5f15\u5165\u4e0e\u8bc4\u5206\u6570\u91cf\u76f8\u5173\uff0c\u53ef\u80fd\u8868\u660e\u4f20\u7edf\u805a\u5408\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u7684\u4f4e\u6548\u3002", "conclusion": "LLM\u9a71\u52a8\u7684\u7fa4\u4f53\u63a8\u8350\u7cfb\u7edf\uff08GRS\uff09\u5728\u63a8\u8350\u7ed3\u679c\u4e0a\u4e0e\u57fa\u4e8e\u6548\u7528\u8bba\u7684\u805a\u5408\u7b56\u7565\u76f8\u4f3c\uff0c\u4f46\u5728\u89e3\u91ca\u65b9\u9762\uff0cLLM\u503e\u5411\u4e8e\u63d0\u53ca\u5e73\u5747\u8bc4\u5206\uff0c\u5e76\u4e14\u4f1a\u5f15\u5165\u989d\u5916\u6807\u51c6\uff08\u5982\u7528\u6237/\u7269\u54c1\u76f8\u4f3c\u5ea6\u3001\u591a\u6837\u6027\u3001\u6d41\u884c\u5ea6\uff09\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u89e3\u91ca\u7684\u6a21\u7cca\u6027\u548c\u4e0d\u4e00\u81f4\u6027\uff0c\u5f71\u54cd\u900f\u660e\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u3002\u7fa4\u7ec4\u7ed3\u6784\uff08\u7edf\u4e00\u6216\u53d1\u6563\uff09\u4e0d\u5f71\u54cd\u63a8\u8350\u7ed3\u679c\u3002"}}
{"id": "2507.13862", "categories": ["quant-ph", "cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2507.13862", "abs": "https://arxiv.org/abs/2507.13862", "authors": ["Ayan Patra", "Tanoy Kanti Konar", "Pritam Halder", "Aditi Sen De"], "title": "Role of quantum state texture in probing resource theories and quantum phase transition", "comment": "11 pages, 2 figures", "summary": "Building on the recently developed quantum state texture resource theory, we\nexhibit that the difference between maximum and minimum textures is a valid\npurity monotone in any dimension and provide a lower bound for existing purity\nmeasures. We introduce a texture-based resource monotone applicable across\ngeneral convex resource theories, encompassing quantum coherence,\nnon-stabilizerness, and entanglement. In particular, we propose the notion of\nnon-local texture, which corresponds to the geometric measure of bipartite and\nmultipartite entanglement in pure states. Furthermore, we demonstrate that the\ntexture of the entire ground state or its subsystems can effectively signal\nquantum phase transitions in the Ising chain under both transverse and\nlongitudinal magnetic fields, offering a powerful tool for characterizing\nquantum criticality.", "AI": {"tldr": "\u672c\u6587\u5728\u91cf\u5b50\u6001\u7eb9\u7406\u8d44\u6e90\u7406\u8bba\u7684\u57fa\u7840\u4e0a\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8d44\u6e90\u5355\u8c03\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u591a\u79cd\u91cf\u5b50\u8d44\u6e90\u3002\u7ed3\u679c\u8868\u660e\uff0c\u7eb9\u7406\u80fd\u591f\u6709\u6548\u5730\u8868\u5f81\u91cf\u5b50\u76f8\u53d8\uff0c\u5e76\u4e3a\u7814\u7a76\u91cf\u5b50\u4e34\u754c\u6027\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u5de5\u5177\u3002", "motivation": "\u5728\u6700\u8fd1\u5f00\u53d1\u7684\u91cf\u5b50\u6001\u7eb9\u7406\u8d44\u6e90\u7406\u8bba\u7684\u57fa\u7840\u4e0a\uff0c\u5c55\u793a\u4e86\u6700\u5927\u7eb9\u7406\u548c\u6700\u5c0f\u7eb9\u7406\u4e4b\u95f4\u7684\u5dee\u503c\u662f\u4efb\u4f55\u7ef4\u5ea6\u4e2d\u7684\u6709\u6548\u7eaf\u5ea6\u5355\u8c03\uff0c\u5e76\u4e3a\u73b0\u6709\u7684\u7eaf\u5ea6\u91cf\u63d0\u4f9b\u4e86\u4e0b\u754c\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7eb9\u7406\u7684\u8d44\u6e90\u5355\u8c03\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u91cf\u5b50\u76f8\u5e72\u6027\u3001\u975e\u7a33\u5b9a\u6027\u548c\u7ea0\u7f20\u7b49\u9886\u57df\u3002\u4ecb\u7ecd\u4e86\u975e\u5c40\u57df\u7eb9\u7406\u7684\u6982\u5ff5\uff0c\u5e76\u5c06\u5176\u4e0e\u7eaf\u6001\u4e0b\u7684\u4e8c\u4f53\u548c\u591a\u4f53\u7ea0\u7f20\u7684\u51e0\u4f55\u5ea6\u91cf\u8054\u7cfb\u8d77\u6765\u3002", "result": "\u7eb9\u7406\u80fd\u591f\u6709\u6548\u5730\u533a\u5206\u91cf\u5b50\u76f8\u53d8\uff0c\u5e76\u4e14\u5728\u6a2a\u5411\u548c\u7eb5\u5411\u78c1\u573a\u4e0b\u5bf9\u4f0a\u8f9b\u94fe\u7684\u91cf\u5b50\u76f8\u53d8\u8fdb\u884c\u4e86\u6709\u6548\u8868\u5f81\u3002DTDs are often used for specifying the structure of XML documents. They define the allowed elements and attributes, as well as their relationships and order. This helps to ensure that XML documents are well-formed and valid.", "conclusion": "\u7eb9\u7406\u53ef\u4ee5\u4f5c\u4e3a\u91cf\u5b50\u76f8\u53d8\u7684\u4fe1\u53f7\uff0c\u4e3a\u91cf\u5316\u91cf\u5b50\u4e34\u754c\u6027\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u529b\u7684\u5de5\u5177\u3002"}}
{"id": "2507.13408", "categories": ["cs.CV", "cs.AI", "68T07", "I.2.10"], "pdf": "https://arxiv.org/pdf/2507.13408", "abs": "https://arxiv.org/abs/2507.13408", "authors": ["Hemanth Kumar M", "Karthika M", "Saianiruth M", "Vasanthakumar Venugopal", "Anandakumar D", "Revathi Ezhumalai", "Charulatha K", "Kishore Kumar J", "Dayana G", "Kalyan Sivasailam", "Bargava Subramanian"], "title": "A Deep Learning-Based Ensemble System for Automated Shoulder Fracture Detection in Clinical Radiographs", "comment": "12 pages, 2 figures", "summary": "Background: Shoulder fractures are often underdiagnosed, especially in\nemergency and high-volume clinical settings. Studies report up to 10% of such\nfractures may be missed by radiologists. AI-driven tools offer a scalable way\nto assist early detection and reduce diagnostic delays. We address this gap\nthrough a dedicated AI system for shoulder radiographs. Methods: We developed a\nmulti-model deep learning system using 10,000 annotated shoulder X-rays.\nArchitectures include Faster R-CNN (ResNet50-FPN, ResNeXt), EfficientDet, and\nRF-DETR. To enhance detection, we applied bounding box and classification-level\nensemble techniques such as Soft-NMS, WBF, and NMW fusion. Results: The NMW\nensemble achieved 95.5% accuracy and an F1-score of 0.9610, outperforming\nindividual models across all key metrics. It demonstrated strong recall and\nlocalization precision, confirming its effectiveness for clinical fracture\ndetection in shoulder X-rays. Conclusion: The results show ensemble-based AI\ncan reliably detect shoulder fractures in radiographs with high clinical\nrelevance. The model's accuracy and deployment readiness position it well for\nintegration into real-time diagnostic workflows. The current model is limited\nto binary fracture detection, reflecting its design for rapid screening and\ntriage support rather than detailed orthopedic classification.", "AI": {"tldr": "\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u901a\u8fc7\u96c6\u6210\u591a\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u5728\u80a9\u90e8X\u5149\u7247\u4e2d\u68c0\u6d4b\u9aa8\u6298\uff0c\u51c6\u786e\u7387\u8fbe95.5%\uff0c\u53ef\u7528\u4e8e\u8f85\u52a9\u4e34\u5e8a\u8bca\u65ad\u3002", "motivation": "\u80a9\u90e8\u9aa8\u6298\u5e38\u5e38\u88ab\u6f0f\u8bca\uff0c\u5c24\u5176\u662f\u5728\u6025\u8bca\u548c\u9ad8\u5de5\u4f5c\u91cf\u7684\u4e34\u5e8a\u73af\u5883\u4e2d\uff0c\u591a\u8fbe10%\u7684\u9aa8\u6298\u53ef\u80fd\u88ab\u653e\u5c04\u79d1\u533b\u751f\u9057\u6f0f\u3002\u4eba\u5de5\u667a\u80fd\u9a71\u52a8\u7684\u5de5\u5177\u53ef\u4ee5\u8f85\u52a9\u65e9\u671f\u68c0\u6d4b\u5e76\u51cf\u5c11\u8bca\u65ad\u5ef6\u8fdf\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u4e13\u95e8\u7684\u80a9\u90e8X\u5149\u7247\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u591a\u6a21\u578b\u6df1\u5ea6\u5b66\u4e60\u7cfb\u7edf\uff0c\u4f7f\u7528\u4e8610,000\u5f20\u6807\u6ce8\u7684\u80a9\u90e8X\u5149\u7247\u3002\u6a21\u578b\u67b6\u6784\u5305\u62ecFaster R-CNN\uff08ResNet50-FPN, ResNeXt\uff09\u3001EfficientDet\u548cRF-DETR\u3002\u4e3a\u63d0\u9ad8\u68c0\u6d4b\u7cbe\u5ea6\uff0c\u5e94\u7528\u4e86\u8fb9\u754c\u6846\u548c\u5206\u7c7b\u7ea7\u522b\u7684\u96c6\u6210\u6280\u672f\uff0c\u5982Soft-NMS\u3001WBF\u548cNMW\u878d\u5408\u3002", "result": "NMW\u96c6\u6210\u6a21\u578b\u8fbe\u5230\u4e8695.5%\u7684\u51c6\u786e\u7387\u548c0.9610\u7684F1\u5206\u6570\uff0c\u5728\u6240\u6709\u5173\u952e\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u5355\u4e2a\u6a21\u578b\u3002\u8be5\u6a21\u578b\u5728\u53ec\u56de\u7387\u548c\u5b9a\u4f4d\u7cbe\u5ea6\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u8bc1\u5b9e\u4e86\u5176\u5728\u80a9\u90e8X\u5149\u7247\u4e34\u5e8a\u9aa8\u6298\u68c0\u6d4b\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u57fa\u4e8e\u96c6\u6210\u7684\u4eba\u5de5\u667a\u80fd\u53ef\u4ee5\u53ef\u9760\u5730\u5728X\u5149\u7247\u4e2d\u68c0\u6d4b\u80a9\u90e8\u9aa8\u6298\uff0c\u5177\u6709\u9ad8\u5ea6\u7684\u4e34\u5e8a\u76f8\u5173\u6027\u3002\u8be5\u6a21\u578b\u51c6\u786e\u4e14\u5df2\u51c6\u5907\u597d\u90e8\u7f72\uff0c\u975e\u5e38\u9002\u5408\u96c6\u6210\u5230\u5b9e\u65f6\u8bca\u65ad\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u3002\u76ee\u524d\u8be5\u6a21\u578b\u4ec5\u9650\u4e8e\u4e8c\u5143\u9aa8\u6298\u68c0\u6d4b\uff0c\u7528\u4e8e\u5feb\u901f\u7b5b\u67e5\u548c\u5206\u8bca\u652f\u6301\uff0c\u800c\u975e\u8be6\u7ec6\u7684\u9aa8\u79d1\u5206\u7c7b\u3002"}}
{"id": "2507.14099", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14099", "abs": "https://arxiv.org/abs/2507.14099", "authors": ["Markus Buchholz", "Ignacio Carlucho", "Michele Grimaldi", "Maria Koskinopoulou", "Yvan R. Petillot"], "title": "Context-Aware Behavior Learning with Heuristic Motion Memory for Underwater Manipulation", "comment": "Accepted at 2025 IEEE International Conference on Intelligent Robots\n  and Systems (IROS)", "summary": "Autonomous motion planning is critical for efficient and safe underwater\nmanipulation in dynamic marine environments. Current motion planning methods\noften fail to effectively utilize prior motion experiences and adapt to\nreal-time uncertainties inherent in underwater settings. In this paper, we\nintroduce an Adaptive Heuristic Motion Planner framework that integrates a\nHeuristic Motion Space (HMS) with Bayesian Networks to enhance motion planning\nfor autonomous underwater manipulation. Our approach employs the Probabilistic\nRoadmap (PRM) algorithm within HMS to optimize paths by minimizing a composite\ncost function that accounts for distance, uncertainty, energy consumption, and\nexecution time. By leveraging HMS, our framework significantly reduces the\nsearch space, thereby boosting computational performance and enabling real-time\nplanning capabilities. Bayesian Networks are utilized to dynamically update\nuncertainty estimates based on real-time sensor data and environmental\nconditions, thereby refining the joint probability of path success. Through\nextensive simulations and real-world test scenarios, we showcase the advantages\nof our method in terms of enhanced performance and robustness. This\nprobabilistic approach significantly advances the capability of autonomous\nunderwater robots, ensuring optimized motion planning in the face of dynamic\nmarine challenges.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u542f\u53d1\u5f0f\u8fd0\u52a8\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u542f\u53d1\u5f0f\u8fd0\u52a8\u7a7a\u95f4\uff08HMS\uff09\u548c\u8d1d\u53f6\u65af\u7f51\u7edc\u6765\u6539\u8fdb\u81ea\u4e3b\u6c34\u4e0b\u64cd\u4f5c\u7684\u8fd0\u52a8\u89c4\u5212\uff0c\u5b9e\u73b0\u4e86\u66f4\u4f18\u5316\u7684\u8def\u5f84\u548c\u5b9e\u65f6\u89c4\u5212\u80fd\u529b\u3002", "motivation": "\u81ea\u4e3b\u8fd0\u52a8\u89c4\u5212\u5bf9\u4e8e\u52a8\u6001\u6d77\u6d0b\u73af\u5883\u4e2d\u9ad8\u6548\u4e14\u5b89\u5168\u7684 \u6c34\u4e0b\u64cd\u4f5c\u81f3\u5173\u91cd\u8981\u3002\u5f53\u524d\u65b9\u6cd5\u672a\u80fd\u6709\u6548\u5229\u7528\u5148\u9a8c\u8fd0\u52a8\u7ecf\u9a8c\u5e76\u9002\u5e94\u6c34\u4e0b\u73af\u5883\u56fa\u6709\u7684\u5b9e\u65f6\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u672c\u6846\u67b6\u96c6\u6210\u4e86\u542f\u53d1\u5f0f\u8fd0\u52a8\u7a7a\u95f4\uff08HMS\uff09\u548c\u8d1d\u53f6\u65af\u7f51\u7edc\uff0c\u5229\u7528\u542f\u53d1\u5f0f\u8fd0\u52a8\u7a7a\u95f4\u4e2d\u7684\u6982\u7387\u8def\u5f84\u56fe\uff08PRM\uff09\u7b97\u6cd5\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u8003\u8651\u8ddd\u79bb\u3001\u4e0d\u786e\u5b9a\u6027\u3001\u80fd\u8017\u548c\u6267\u884c\u65f6\u95f4\u7684\u590d\u5408\u6210\u672c\u51fd\u6570\u6765\u4f18\u5316\u8def\u5f84\u3002\u5229\u7528 HMS \u51cf\u5c11\u4e86\u641c\u7d22\u7a7a\u95f4\uff0c\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6027\u80fd\u548c\u5b9e\u65f6\u89c4\u5212\u80fd\u529b\u3002\u5229\u7528\u8d1d\u53f6\u65af\u7f51\u7edc\u6839\u636e\u5b9e\u65f6\u4f20\u611f\u5668\u6570\u636e\u548c\u73af\u5883\u6761\u4ef6\u52a8\u6001\u66f4\u65b0\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u4ece\u800c\u4f18\u5316\u8def\u5f84\u6210\u529f\u7387\u7684\u8054\u5408\u6982\u7387\u3002", "result": "\u901a\u8fc7\u5e7f\u6cdb\u7684\u6a21\u62df\u548c\u73b0\u5b9e\u4e16\u754c\u6d4b\u8bd5\u573a\u666f\uff0c\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u5728\u63d0\u9ad8\u6027\u80fd\u548c\u9c81\u68d2\u6027\u65b9\u9762\u7684\u4f18\u52bf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u81ea\u4e3b\u6c34\u4e0b\u673a\u5668\u4eba\u7684\u80fd\u529b\uff0c\u786e\u4fdd\u4e86\u5728\u52a8\u6001\u6d77\u6d0b\u6311\u6218\u4e0b\u4f18\u5316\u8fd0\u52a8\u89c4\u5212\u3002"}}
{"id": "2507.13732", "categories": ["cs.CL", "cs.LG", "J.1; I.2.7"], "pdf": "https://arxiv.org/pdf/2507.13732", "abs": "https://arxiv.org/abs/2507.13732", "authors": ["Guillaume Zambrano"], "title": "The Judge Variable: Challenging Judge-Agnostic Legal Judgment Prediction", "comment": "23 pages, 24 figures shorter version submitted to JURIX 2025", "summary": "This study examines the role of human judges in legal decision-making by\nusing machine learning to predict child physical custody outcomes in French\nappellate courts. Building on the legal realism-formalism debate, we test\nwhether individual judges' decision-making patterns significantly influence\ncase outcomes, challenging the assumption that judges are neutral variables\nthat apply the law uniformly. To ensure compliance with French privacy laws, we\nimplement a strict pseudonymization process. Our analysis uses 18,937 living\narrangements rulings extracted from 10,306 cases. We compare models trained on\nindividual judges' past rulings (specialist models) with a judge-agnostic model\ntrained on aggregated data (generalist models). The prediction pipeline is a\nhybrid approach combining large language models (LLMs) for structured feature\nextraction and ML models for outcome prediction (RF, XGB and SVC). Our results\nshow that specialist models consistently achieve higher predictive accuracy\nthan the general model, with top-performing models reaching F1 scores as high\nas 92.85%, compared to the generalist model's 82.63% trained on 20x to 100x\nmore samples. Specialist models capture stable individual patterns that are not\ntransferable to other judges. In-Domain and Cross-Domain validity tests provide\nempirical support for legal realism, demonstrating that judicial identity plays\na measurable role in legal outcomes. All data and code used will be made\navailable.", "AI": {"tldr": "\u672c\u7814\u7a76\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u5206\u6790\u4e86 18,937 \u4e2a\u6cd5\u56fd\u6cd5\u9662\u7684\u513f\u7ae5\u629a\u517b\u6743\u6848\u4ef6\uff0c\u53d1\u73b0\u4e13\u5bb6\u6a21\u578b\uff08\u57fa\u4e8e\u4e2a\u522b\u6cd5\u5b98\u7684\u5224\u51b3\uff09\u6bd4\u6cdb\u5316\u6a21\u578b\uff08\u57fa\u4e8e\u805a\u5408\u6570\u636e\uff09\u5177\u6709\u66f4\u9ad8\u7684\u9884\u6d4b\u51c6\u786e\u6027\uff08F1 \u5206\u6570\u8fbe 92.85% vs 82.63%\uff09\uff0c\u8bc1\u660e\u4e86\u6cd5\u5b98\u7684\u4e2a\u4f53\u51b3\u7b56\u98ce\u683c\u5bf9\u6848\u4ef6\u7ed3\u679c\u6709\u663e\u8457\u5f71\u54cd\uff0c\u652f\u6301\u4e86\u6cd5\u5f8b\u73b0\u5b9e\u4e3b\u4e49\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u68c0\u9a8c\u4e2a\u522b\u6cd5\u5b98\u7684\u51b3\u7b56\u6a21\u5f0f\u662f\u5426\u663e\u8457\u5f71\u54cd\u6848\u4ef6\u7ed3\u679c\uff0c\u6311\u6218\u4e86\u6cd5\u5b98\u662f\u4e2d\u7acb\u53d8\u91cf\u5e76\u7edf\u4e00\u9002\u7528\u6cd5\u5f8b\u7684\u5047\u8bbe\uff0c\u4ee5\u56de\u5e94\u6cd5\u5f8b\u73b0\u5b9e\u4e3b\u4e49\u4e0e\u6cd5\u5f8b\u5f62\u5f0f\u4e3b\u4e49\u7684\u4e89\u8bba\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\uff0c\u7ed3\u5408\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8fdb\u884c\u7ed3\u6784\u5316\u7279\u5f81\u63d0\u53d6\u548c\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff08RF\u3001XGB\u548cSVC\uff09\u8fdb\u884c\u7ed3\u679c\u9884\u6d4b\u3002\u7814\u7a76\u5206\u6790\u4e86 10,306 \u4e2a\u6848\u4ef6\u4e2d\u63d0\u53d6\u7684 18,937 \u4e2a\u5c45\u4f4f\u5b89\u6392\u88c1\u51b3\uff0c\u5e76\u6bd4\u8f83\u4e86\u5728\u4e2a\u522b\u6cd5\u5b98\u8fc7\u5f80\u88c1\u51b3\u4e0a\u8bad\u7ec3\u7684\u4e13\u5bb6\u6a21\u578b\u4e0e\u5728\u805a\u5408\u6570\u636e\u4e0a\u8bad\u7ec3\u7684\u3001\u4e0d\u533a\u5206\u6cd5\u5b98\u7684\u6cdb\u5316\u6a21\u578b\u3002", "result": "\u4e13\u5bb6\u6a21\u578b\u6301\u7eed\u63d0\u4f9b\u6bd4\u6cdb\u5316\u6a21\u578b\u66f4\u9ad8\u7684\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u8868\u73b0\u6700\u4f73\u7684\u6a21\u578b\u8fbe\u5230\u4e86 92.85% \u7684 F1 \u5206\u6570\uff0c\u800c\u6cdb\u5316\u6a21\u578b\uff08\u4f7f\u7528\u4e86 20 \u5230 100 \u500d\u7684\u6837\u672c\u91cf\uff09\u7684 F1 \u5206\u6570\u4e3a 82.63%\u3002\u4e13\u5bb6\u6a21\u578b\u6355\u6349\u5230\u4e86\u7a33\u5b9a\u4e14\u4e0d\u53ef\u8fc1\u79fb\u5230\u5176\u4ed6\u6cd5\u5b98\u7684\u4e2a\u4f53\u6a21\u5f0f\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u5b9e\u8bc1\u6570\u636e\u652f\u6301\u4e86\u6cd5\u5f8b\u73b0\u5b9e\u4e3b\u4e49\uff0c\u8868\u660e\u4e2a\u522b\u6cd5\u5b98\u7684\u8eab\u4efd\u5728\u6cd5\u5f8b\u7ed3\u679c\u4e2d\u8d77\u7740\u53ef\u8861\u91cf\u7684\u4f5c\u7528\u3002"}}
{"id": "2507.13894", "categories": ["quant-ph", "gr-qc"], "pdf": "https://arxiv.org/pdf/2507.13894", "abs": "https://arxiv.org/abs/2507.13894", "authors": ["Alberto Garc\u00eda Mart\u00edn-Caro", "Javier Olmedo", "Jose M. S\u00e1nchez Vel\u00e1zquez"], "title": "Robustness of analogue Hawking radiation in cavities with moving boundaries", "comment": "40 pages, 23 figures", "summary": "In this work we explore the limitations and robustness of thermal radiation\nin dynamical Casimir systems serving as analogs for Hawking radiation. Through\ndetailed numerical analysis, we characterize particle production spectra in\ncavities with moving boundaries under various configurations, including\nexpanding, collapsing, and rigidly accelerating scenarios. We find that thermal\nsignatures emerge in specific expanding cavity configurations but are highly\ndependent on frequency bands and acceleration parameters. In those\nconfigurations of the cavity where there is thermal production, we derive\nfitting expressions that quantify deviations from idealized thermal spectra\nthrough gray-body factors, revealing oscillatory behaviors tied to acceleration\nduration. Our results identify which experimental setups can reliably simulate\ngravitationally-induced phenomena and quantify how finite-size effects and\ntransient dynamics modify the expected thermal distributions, providing a\ncomprehensive framework for distinguishing genuine Hawking-like radiation from\nexperimental artifacts.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u7d22\u4e86\u52a8\u6001\u5361\u897f\u7c73\u5c14\u7cfb\u7edf\u4e2d\u70ed\u8f90\u5c04\u7684\u5c40\u9650\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4f5c\u4e3a\u970d\u91d1\u8f90\u5c04\u7684\u7c7b\u4f3c\u7269\u3002\u901a\u8fc7\u6570\u503c\u5206\u6790\uff0c\u7814\u7a76\u4eba\u5458\u8868\u5f81\u4e86\u4e0d\u540c\u914d\u7f6e\u4e0b\u8154\u4f53\u4e2d\u7c92\u5b50\u7684\u4ea7\u751f\u8c31\uff0c\u5e76\u53d1\u73b0\u4e86\u70ed\u7279\u5f81\u7684\u51fa\u73b0\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u7279\u5b9a\u6761\u4ef6\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u91cf\u5316\u4e86\u5c3a\u5bf8\u6548\u5e94\u548c\u77ac\u6001\u52a8\u529b\u5b66\u5bf9\u70ed\u5206\u5e03\u7684\u5f71\u54cd\uff0c\u4e3a\u533a\u5206\u970d\u91d1\u8f90\u5c04\u548c\u5b9e\u9a8c\u4f2a\u5f71\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002", "motivation": "\u63a2\u7d22\u52a8\u6001\u5361\u897f\u7c73\u5c14\u7cfb\u7edf\u4e2d\u70ed\u8f90\u5c04\u7684\u5c40\u9650\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4f5c\u4e3a\u970d\u91d1\u8f90\u5c04\u7684\u7c7b\u4f3c\u7269\u3002", "method": "\u901a\u8fc7\u8be6\u7ec6\u7684\u6570\u503c\u5206\u6790\uff0c\u8868\u5f81\u4e86\u5177\u6709\u79fb\u52a8\u8fb9\u754c\u7684\u8154\u4f53\u5728\u5404\u79cd\u914d\u7f6e\uff08\u5305\u62ec\u81a8\u80c0\u3001\u574d\u584c\u548c\u521a\u6027\u52a0\u901f\uff09\u4e0b\u7c92\u5b50\u4ea7\u751f\u8c31\u3002", "result": "\u53d1\u73b0\u5728\u7279\u5b9a\u81a8\u80c0\u8154\u4f53\u914d\u7f6e\u4e2d\u4f1a\u51fa\u73b0\u70ed\u7279\u5f81\uff0c\u4f46\u5b83\u4eec\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u9891\u5e26\u548c\u52a0\u901f\u53c2\u6570\u3002\u5728\u51fa\u73b0\u70ed\u4ea7\u751f\u7684\u8154\u4f53\u914d\u7f6e\u4e2d\uff0c\u63a8\u5bfc\u51fa\u4e86\u91cf\u5316\u4e0e\u7070\u4f53\u56e0\u5b50\u7406\u60f3\u5316\u70ed\u8c31\u504f\u5dee\u7684\u62df\u5408\u8868\u8fbe\u5f0f\uff0c\u63ed\u793a\u4e86\u4e0e\u52a0\u901f\u6301\u7eed\u65f6\u95f4\u76f8\u5173\u7684\u632f\u8361\u884c\u4e3a\u3002", "conclusion": "\u901a\u8fc7\u8bc6\u522b\u54ea\u4e9b\u5b9e\u9a8c\u8bbe\u7f6e\u53ef\u4ee5\u53ef\u9760\u5730\u6a21\u62df\u5f15\u529b\u8bf1\u5bfc\u7684\u73b0\u8c61\uff0c\u5e76\u91cf\u5316\u5c3a\u5bf8\u6548\u5e94\u548c\u77ac\u6001\u52a8\u529b\u5b66\u5982\u4f55\u4fee\u6539\u9884\u671f\u7684\u70ed\u5206\u5e03\uff0c\u63d0\u4f9b\u4e86\u533a\u5206\u771f\u5b9e\u7684\u970d\u91d1\u8f90\u5c04\u4e0e\u5b9e\u9a8c\u4f2a\u5f71\u7684\u7efc\u5408\u6846\u67b6\u3002"}}
{"id": "2507.13646", "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2507.13646", "abs": "https://arxiv.org/abs/2507.13646", "authors": ["Nimisha Ghosh", "Daniele Santoni", "Debaleena Nawn", "Eleonora Ottaviani", "Giovanni Felici"], "title": "A Comprehensive Review of Transformer-based language models for Protein Sequence Analysis and Design", "comment": null, "summary": "The impact of Transformer-based language models has been unprecedented in\nNatural Language Processing (NLP). The success of such models has also led to\ntheir adoption in other fields including bioinformatics. Taking this into\naccount, this paper discusses recent advances in Transformer-based models for\nprotein sequence analysis and design. In this review, we have discussed and\nanalysed a significant number of works pertaining to such applications. These\napplications encompass gene ontology, functional and structural protein\nidentification, generation of de novo proteins and binding of proteins. We\nattempt to shed light on the strength and weaknesses of the discussed works to\nprovide a comprehensive insight to readers. Finally, we highlight shortcomings\nin existing research and explore potential avenues for future developments. We\nbelieve that this review will help researchers working in this field to have an\noverall idea of the state of the art in this field, and to orient their future\nstudies.", "AI": {"tldr": "Transformer models are revolutionizing protein analysis and design, impacting areas from gene ontology to protein binding. This review covers recent advances, strengths, weaknesses, and future directions.", "motivation": "To discuss recent advances in Transformer-based models for protein sequence analysis and design, providing a comprehensive insight into the field for researchers.", "method": "This paper reviews and analyzes recent advances in Transformer-based models for protein sequence analysis and design, discussing a significant number of works in the field.", "result": "The paper analyzes recent works on Transformer-based models for protein sequence analysis and design, covering various applications and discussing their strengths and weaknesses.", "conclusion": "The review highlights the impact of Transformer-based models in protein sequence analysis and design, covering applications like gene ontology, functional and structural identification, de novo protein generation, and protein binding. It analyzes the strengths and weaknesses of existing research and suggests future directions."}}
{"id": "2507.13420", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13420", "abs": "https://arxiv.org/abs/2507.13420", "authors": ["Alessandro Pistola", "Valentina Orru'", "Nicolo' Marchetti", "Marco Roccetti"], "title": "AI-ming backwards: Vanishing archaeological landscapes in Mesopotamia and automatic detection of sites on CORONA imagery", "comment": "25 pages, 9 Figures", "summary": "By upgrading an existing deep learning model with the knowledge provided by\none of the oldest sets of grayscale satellite imagery, known as CORONA, we\nimproved the AI model attitude towards the automatic identification of\narchaeological sites in an environment which has been completely transformed in\nthe last five decades, including the complete destruction of many of those same\nsites. The initial Bing based convolutional network model was retrained using\nCORONA satellite imagery for the district of Abu Ghraib, west of Baghdad,\ncentral Mesopotamian floodplain. The results were twofold and surprising.\nFirst, the detection precision obtained on the area of interest increased\nsensibly: in particular, the Intersection over Union (IoU) values, at the image\nsegmentation level, surpassed 85 percent, while the general accuracy in\ndetecting archeological sites reached 90 percent. Second, our retrained model\nallowed the identification of four new sites of archaeological interest\n(confirmed through field verification), previously not identified by\narchaeologists with traditional techniques. This has confirmed the efficacy of\nusing AI techniques and the CORONA imagery from the 1960 to discover\narchaeological sites currently no longer visible, a concrete breakthrough with\nsignificant consequences for the study of landscapes with vanishing\narchaeological evidence induced by anthropization", "AI": {"tldr": "\u900f\u904e\u7d50\u5408\u820a\u885b\u661f\u5f71\u50cf\u548cAI\uff0c\u6210\u529f\u5728\u4f0a\u62c9\u514b\u767c\u73fe\u4e86\u65b0\u7684\u3001\u5df2\u6d88\u5931\u7684\u8003\u53e4\u907a\u5740\u3002", "motivation": "\u70ba\u4e86\u5728\u904e\u53bb\u4e94\u5341\u5e74\u4f86\u7d93\u6b77\u4e86\u5de8\u5927\u8b8a\u9077\uff0c\u751a\u81f3\u8a31\u591a\u8003\u53e4\u907a\u5740\u5df2\u88ab\u5b8c\u5168\u6467\u6bc0\u7684\u74b0\u5883\u4e2d\uff0c\u63d0\u9ad8AI\u6a21\u578b\u5c0d\u8003\u53e4\u907a\u5740\u81ea\u52d5\u8b58\u5225\u7684\u80fd\u529b\u3002", "method": "\u900f\u904e\u4f7f\u75281960\u5e74\u4ee3\u7684CORONA\u885b\u661f\u7070\u968e\u5f71\u50cf\u4f86\u5347\u7d1a\u73fe\u6709\u7684\u6df1\u5ea6\u5b78\u7fd2\u6a21\u578b\uff0c\u4e26\u5728\u4f0a\u62c9\u514b\u963f\u5e03\u683c\u840a\u5e03\u5730\u5340\u91cd\u65b0\u8a13\u7df4\u4e86\u4e00\u500b\u57fa\u65bcBing\u7684\u5377\u7a4d\u795e\u7d93\u7db2\u7d61\u6a21\u578b\u3002", "result": "\u8a72\u6a21\u578b\u5728\u7814\u7a76\u5340\u57df\u7684\u6aa2\u6e2c\u7cbe\u5ea6\u986f\u8457\u63d0\u9ad8\uff0c\u5f71\u50cf\u5206\u5272\u7684IoU\u503c\u8d85\u904e85%\uff0c\u8003\u53e4\u907a\u5740\u6aa2\u6e2c\u7684\u7e3d\u9ad4\u6e96\u78ba\u7387\u9054\u523090%\u3002\u6b64\u5916\uff0c\u9084\u65b0\u8b58\u5225\u51fa\u56db\u500b\u5148\u524d\u672a\u88ab\u767c\u73fe\u7684\u8003\u53e4\u907a\u5740\u3002", "conclusion": "\u4f7f\u75281960\u5e74\u4ee3\u7684CORONA\u885b\u661f\u5f71\u50cf\u548c\u4eba\u5de5\u667a\u80fd\u6280\u8853\uff0c\u53ef\u4ee5\u6709\u6548\u8b58\u5225\u5df2\u6d88\u5931\u7684\u8003\u53e4\u907a\u5740\uff0c\u9019\u5c0d\u65bc\u7814\u7a76\u53d7\u4eba\u985e\u6d3b\u52d5\u5f71\u97ff\u800c\u6d88\u5931\u7684\u8003\u53e4\u666f\u89c0\u5177\u6709\u91cd\u8981\u610f\u7fa9\u3002"}}
{"id": "2507.13857", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.13857", "abs": "https://arxiv.org/abs/2507.13857", "authors": ["Max van den Hoven", "Kishaan Jeeveswaran", "Pieter Piscaer", "Thijs Wensveen", "Elahe Arani", "Bahram Zonooz"], "title": "Depth3DLane: Fusing Monocular 3D Lane Detection with Self-Supervised Monocular Depth Estimation", "comment": null, "summary": "Monocular 3D lane detection is essential for autonomous driving, but\nchallenging due to the inherent lack of explicit spatial information.\nMulti-modal approaches rely on expensive depth sensors, while methods\nincorporating fully-supervised depth networks rely on ground-truth depth data\nthat is impractical to collect at scale. Additionally, existing methods assume\nthat camera parameters are available, limiting their applicability in scenarios\nlike crowdsourced high-definition (HD) lane mapping. To address these\nlimitations, we propose Depth3DLane, a novel dual-pathway framework that\nintegrates self-supervised monocular depth estimation to provide explicit\nstructural information, without the need for expensive sensors or additional\nground-truth depth data. Leveraging a self-supervised depth network to obtain a\npoint cloud representation of the scene, our bird's-eye view pathway extracts\nexplicit spatial information, while our front view pathway simultaneously\nextracts rich semantic information. Depth3DLane then uses 3D lane anchors to\nsample features from both pathways and infer accurate 3D lane geometry.\nFurthermore, we extend the framework to predict camera parameters on a\nper-frame basis and introduce a theoretically motivated fitting procedure to\nenhance stability on a per-segment basis. Extensive experiments demonstrate\nthat Depth3DLane achieves competitive performance on the OpenLane benchmark\ndataset. Furthermore, experimental results show that using learned parameters\ninstead of ground-truth parameters allows Depth3DLane to be applied in\nscenarios where camera calibration is infeasible, unlike previous methods.", "AI": {"tldr": "Depth3DLane\u901a\u8fc7\u81ea\u76d1\u7763\u6df1\u5ea6\u4f30\u8ba1\u548c\u53cc\u901a\u8def\u67b6\u6784\uff0c\u65e0\u9700\u989d\u5916\u4f20\u611f\u5668\u6216\u6570\u636e\u5373\u53ef\u5b9e\u73b0\u51c6\u786e\u7684\u5355\u76ee3D\u8f66\u9053\u7ebf\u68c0\u6d4b\uff0c\u5e76\u80fd\u5728\u65e0\u76f8\u673a\u6807\u5b9a\u573a\u666f\u4e0b\u5de5\u4f5c\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u5355\u76ee3D\u8f66\u9053\u7ebf\u68c0\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u7684\u6df1\u5ea6\u4f20\u611f\u5668\u3001\u96be\u4ee5\u5927\u89c4\u6a21\u6536\u96c6\u7684\u771f\u5b9e\u6df1\u5ea6\u6570\u636e\u4ee5\u53ca\u5047\u8bbe\u76f8\u673a\u53c2\u6570\u53ef\u7528\u6027\u7b49\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86Depth3DLane\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDepth3DLane\u7684\u65b0\u578b\u53cc\u901a\u8def\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u81ea\u76d1\u7763\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u3002\u5b83\u5229\u7528\u81ea\u76d1\u7763\u6df1\u5ea6\u7f51\u7edc\u83b7\u53d6\u573a\u666f\u7684\u70b9\u4e91\u8868\u793a\uff0c\u5176\u4e2d\u9e1f\u77b0\u56fe\u901a\u8def\u63d0\u53d6\u663e\u5f0f\u7684\u7a7a\u95f4\u4fe1\u606f\uff0c\u800c\u524d\u89c6\u56fe\u901a\u8def\u63d0\u53d6\u4e30\u5bcc\u7684\u8bed\u4e49\u4fe1\u606f\u3002\u7136\u540e\uff0cDepth3DLane\u4f7f\u75283D\u8f66\u9053\u7ebf\u951a\u70b9\u4ece\u4e24\u4e2a\u901a\u8def\u91c7\u6837\u7279\u5f81\uff0c\u5e76\u63a8\u65ad\u51fa\u7cbe\u786e\u76843D\u8f66\u9053\u7ebf\u51e0\u4f55\u3002\u6b64\u5916\uff0c\u8be5\u6846\u67b6\u8fd8\u80fd\u9884\u6d4b\u6bcf\u5e27\u7684\u76f8\u673a\u53c2\u6570\uff0c\u5e76\u901a\u8fc7\u4e00\u4e2a\u7406\u8bba\u9a71\u52a8\u7684\u62df\u5408\u8fc7\u7a0b\u6765\u589e\u5f3a\u6bcf\u6bb5\u7684\u7a33\u5b9a\u6027\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cDepth3DLane\u5728OpenLane\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002\u4e0e\u4ee5\u5f80\u7684\u65b9\u6cd5\u4e0d\u540c\uff0c\u6240\u5b66\u53c2\u6570\u7684\u4f7f\u7528\u4f7f\u5f97Depth3DLane\u80fd\u591f\u5e94\u7528\u4e8e\u76f8\u673a\u6807\u5b9a\u4e0d\u53ef\u884c\u7684\u573a\u666f\u3002", "conclusion": "Depth3DLane\u901a\u8fc7\u96c6\u6210\u81ea\u76d1\u7763\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u6765\u89e3\u51b3\u5355\u76ee3D\u8f66\u9053\u7ebf\u68c0\u6d4b\u7684\u5c40\u9650\u6027\uff0c\u65e0\u9700\u6602\u8d35\u7684\u4f20\u611f\u5668\u6216\u989d\u5916\u7684\u771f\u5b9e\u6df1\u5ea6\u6570\u636e\u3002\u8be5\u6846\u67b6\u7684\u53cc\u901a\u8def\u8bbe\u8ba1\u80fd\u591f\u540c\u65f6\u63d0\u53d6\u7a7a\u95f4\u548c\u8bed\u4e49\u4fe1\u606f\uff0c\u5e76\u901a\u8fc73D\u8f66\u9053\u7ebf\u951a\u70b9\u878d\u5408\u4e24\u8005\u4ee5\u63a8\u65ad\u7cbe\u786e\u76843D\u8f66\u9053\u7ebf\u51e0\u4f55\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u9884\u6d4b\u6bcf\u5e27\u7684\u76f8\u673a\u53c2\u6570\uff0c\u5e76\u901a\u8fc7\u6bcf\u6bb5\u7684\u62df\u5408\u8fc7\u7a0b\u63d0\u9ad8\u7a33\u5b9a\u6027\uff0c\u4f7f\u5176\u80fd\u591f\u5e94\u7528\u4e8e\u76f8\u673a\u6807\u5b9a\u4e0d\u53ef\u884c\u7684\u573a\u666f\uff0c\u5e76\u5728OpenLane\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002"}}
{"id": "2507.13743", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.13743", "abs": "https://arxiv.org/abs/2507.13743", "authors": ["Maluna Menke", "Thilo Hagendorff"], "title": "PRIDE -- Parameter-Efficient Reduction of Identity Discrimination for Equality in LLMs", "comment": null, "summary": "Large Language Models (LLMs) frequently reproduce the gender- and\nsexual-identity prejudices embedded in their training corpora, leading to\noutputs that marginalize LGBTQIA+ users. Hence, reducing such biases is of\ngreat importance. To achieve this, we evaluate two parameter-efficient\nfine-tuning (PEFT) techniques - Low-Rank Adaptation (LoRA) and soft-prompt\ntuning - as lightweight alternatives to full-model fine-tuning for mitigating\nsuch biases. Using the WinoQueer benchmark, we quantify bias in three\nopen-source LLMs and observe baseline bias scores reaching up to 98 (out of\n100) across a range of queer identities defined by gender and/or sexual\norientation, where 50 would indicate neutrality. Fine-tuning with LoRA (< 0.1%\nadditional parameters) on a curated QueerNews corpus reduces those scores by up\nto 50 points and raises neutrality from virtually 0% to as much as 36%.\nSoft-prompt tuning (10 virtual tokens) delivers only marginal improvements.\nThese findings show that LoRA can deliver meaningful fairness gains with\nminimal computation. We advocate broader adoption of community-informed PEFT,\nthe creation of larger queer-authored corpora, and richer evaluation suites\nbeyond WinoQueer, coupled with ongoing audits to keep LLMs inclusive.", "AI": {"tldr": "LLMs\u5b58\u5728\u6027\u522b\u4e0e\u6027\u53d6\u5411\u504f\u89c1\uff0c\u4f7f\u7528LoRA\u5fae\u8c03\u53ef\u663e\u8457\u964d\u4f4e\u504f\u89c1\uff0c\u63d0\u5347\u6a21\u578b\u516c\u5e73\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5e38\u5e38\u4f1a\u590d\u73b0\u5176\u8bad\u7ec3\u8bed\u6599\u5e93\u4e2d\u5b58\u5728\u7684\u6027\u522b\u4e0e\u6027\u53d6\u5411\u504f\u89c1\uff0c\u5bfc\u81f4\u5176\u8f93\u51fa\u8fb9\u7f18\u5316LGBTQIA+\u7528\u6237\u3002\u56e0\u6b64\uff0c\u51cf\u5c11\u8fd9\u4e9b\u504f\u89c1\u81f3\u5173\u91cd\u8981\u3002", "method": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u4e24\u79cd\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08PEFT\uff09\u6280\u672f\u2014\u2014\u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\u548c\u8f6f\u63d0\u793a\u8c03\u4f18\u2014\u2014\u4f5c\u4e3a\u51cf\u5c11\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u6027\u522b\u4e0e\u6027\u53d6\u5411\u504f\u89c1\u7684\u8f7b\u91cf\u7ea7\u66ff\u4ee3\u65b9\u6848\u3002\u7814\u7a76\u4f7f\u7528WinoQueer\u57fa\u51c6\u6d4b\u8bd5\u91cf\u5316\u4e86\u4e09\u6b3e\u5f00\u6e90LLMs\u7684\u504f\u89c1\uff0c\u5e76\u901a\u8fc7\u5728QueerNews\u8bed\u6599\u5e93\u4e0a\u5e94\u7528LoRA\u8fdb\u884c\u5fae\u8c03\uff0c\u4ee5\u8bc4\u4f30\u5176\u51cf\u5c11\u504f\u89c1\u7684\u6548\u679c\u3002", "result": "\u5728WinoQueer\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u7814\u7a76\u89c2\u5bdf\u5230\u57fa\u7ebf\u504f\u89c1\u5f97\u5206\u9ad8\u8fbe98\uff08\u6ee1\u5206100\uff09\uff0c\u800c\u4e2d\u6027\u5f97\u5206\u4e3a50\u3002\u901a\u8fc7\u5728QueerNews\u8bed\u6599\u5e93\u4e0a\u4f7f\u7528LoRA\uff08<0.1%\u989d\u5916\u53c2\u6570\uff09\u8fdb\u884c\u5fae\u8c03\uff0c\u504f\u89c1\u5f97\u5206\u6700\u9ad8\u53ef\u964d\u4f4e50\u5206\uff0c\u4e2d\u6027\u6bd4\u4f8b\u4ece\u8fd10%\u63d0\u5347\u81f3\u6700\u9ad836%\u3002\u8f6f\u63d0\u793a\u8c03\u4f18\uff0810\u4e2a\u865a\u62df\u6807\u8bb0\uff09\u4ec5\u5e26\u6765\u8fb9\u9645\u6539\u5584\u3002", "conclusion": "LoRA\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u6280\u672f\u5728\u51cf\u5c11\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6027\u522b\u4e0e\u6027\u53d6\u5411\u504f\u89c1\u65b9\u9762\u8868\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u53ef\u5728\u6781\u5c11\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u4e0b\u5b9e\u73b0\u663e\u8457\u7684\u516c\u5e73\u6027\u63d0\u5347\u3002\u7814\u7a76\u63d0\u5021\u5e7f\u6cdb\u91c7\u7528\u793e\u533a\u9a71\u52a8\u7684PEFT\u6280\u672f\uff0c\u6784\u5efa\u66f4\u5927\u89c4\u6a21\u7684LGBTQIA+\u793e\u7fa4\u539f\u521b\u8bed\u6599\u5e93\uff0c\u5e76\u5f00\u53d1\u8d85\u8d8aWinoQueer\u7684\u7efc\u5408\u6027\u8bc4\u4f30\u65b9\u6cd5\uff0c\u540c\u65f6\u8f85\u4ee5\u6301\u7eed\u7684\u5ba1\u8ba1\u4ee5\u786e\u4fdd\u6a21\u578b\u7684\u5305\u5bb9\u6027\u3002"}}
{"id": "2507.13964", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2507.13964", "abs": "https://arxiv.org/abs/2507.13964", "authors": ["Mei Peng", "Xu-Dan Xie", "Dan-Bo Zhang"], "title": "Exploring critical states of the quantum Rabi model via Hamiltonian variational ans\u00e4tze", "comment": null, "summary": "Characterizing quantum critical states towards the thermodynamic limit is\nessential for understanding phases of matter. The power of quantum simulators\nfor preparing the critical states relies crucially on the structure of quantum\ncircuits and in return provides new insight into the critical states. Here, we\nexplore the critical states of the quantum Rabi model~(QRM) by preparing them\nvariationally with Hamiltonian variational ans\\\"atze~(HVA), in which the\nintricated interplay among different quantum fluctuations can be parameterized\nat different levels. We find that the required circuit depth scales linearly\nwith the effective system size, suggesting that HVA can efficiently capture the\nbehavior of critical states of QRM towards the thermodynamic limit. Moreover,\nwe reveal that HVA gradually squeeze the initial state to the target critical\nstate, with a number of blocks increasing only linearly with the effective\nsystem size. Our work suggests variational quantum algorithm as a new probe for\nthe complicated critical states.", "AI": {"tldr": "\u4f7f\u7528\u54c8\u5bc6\u987f\u91cf\u53d8\u5206\u83f4\u5179\uff08HVA\uff09\u5236\u5907\u91cf\u5b50\u62c9\u6bd4\u6a21\u578b\uff08QRM\uff09\u7684\u4e34\u754c\u6001\uff0c\u53d1\u73b0HVA\u80fd\u6709\u6548\u6355\u83b7\u4e34\u754c\u6001\u5728\u70ed\u529b\u5b66\u6781\u9650\u4e0b\u7684\u884c\u4e3a\uff0c\u4e14\u6240\u9700\u8d44\u6e90\u4e0e\u7cfb\u7edf\u5c3a\u5bf8\u5448\u7ebf\u6027\u5173\u7cfb\uff0c\u63d0\u51faVQA\u53ef\u4f5c\u4e3a\u63a2\u6d4b\u4e34\u754c\u6001\u7684\u65b0\u65b9\u6cd5\u3002", "motivation": "\u4e3a\u4e86\u7406\u89e3\u7269\u8d28\u76f8\uff0c\u9700\u8981\u5bf9\u70ed\u529b\u5b66\u6781\u9650\u4e0b\u7684\u91cf\u5b50\u4e34\u754c\u6001\u8fdb\u884c\u8868\u5f81\u3002\u91cf\u5b50\u6a21\u62df\u5668\u5728\u5236\u5907\u4e34\u754c\u6001\u65b9\u9762\u7684\u80fd\u529b\u5173\u952e\u4f9d\u8d56\u4e8e\u91cf\u5b50\u7535\u8def\u7684\u7ed3\u6784\uff0c\u540c\u65f6\u4e5f\u80fd\u4e3a\u4e34\u754c\u6001\u63d0\u4f9b\u65b0\u7684\u89c1\u89e3\u3002", "method": "\u5229\u7528\u54c8\u5bc6\u987f\u91cf\u53d8\u5206\u83f4\u5179\uff08HVA\uff09\u5236\u5907\u91cf\u5b50\u62c9\u6bd4\u6a21\u578b\uff08QRM\uff09\u7684\u4e34\u754c\u6001\u3002", "result": "HVA\u80fd\u6709\u6548\u6355\u83b7QRM\u4e34\u754c\u6001\u5728\u70ed\u529b\u5b66\u6781\u9650\u4e0b\u7684\u884c\u4e3a\uff0c\u6240\u9700\u7535\u8def\u6df1\u5ea6\u4e0e\u6709\u6548\u7cfb\u7edf\u5c3a\u5bf8\u5448\u7ebf\u6027\u5173\u7cfb\u3002HVA\u4f1a\u9010\u6e10\u5c06\u521d\u59cb\u72b6\u6001\u538b\u7f29\u5230\u76ee\u6807\u4e34\u754c\u6001\uff0c\u4e14\u6240\u9700\u5757\u7684\u6570\u91cf\u4ec5\u4e0e\u6709\u6548\u7cfb\u7edf\u5c3a\u5bf8\u5448\u7ebf\u6027\u589e\u957f\u3002", "conclusion": "\u6587\u7ae0\u63d0\u51fa\u4f7f\u7528\u53d8\u5206\u91cf\u5b50\u7b97\u6cd5\uff08VQA\uff09\u4f5c\u4e3a\u4e00\u79cd\u65b0\u7684\u63a2\u6d4b\u590d\u6742\u91cf\u5b50\u4e34\u754c\u6001\u7684\u65b9\u6cd5\u3002"}}
{"id": "2507.13685", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13685", "abs": "https://arxiv.org/abs/2507.13685", "authors": ["Yue Yang", "Zihan Su", "Ying Zhang", "Chang Chuan Goh", "Yuxiang Lin", "Anthony Graham Bellotti", "Boon Giin Lee"], "title": "Kolmogorov-Arnold Networks-based GRU and LSTM for Loan Default Early Prediction", "comment": null, "summary": "This study addresses a critical challenge in time series anomaly detection:\nenhancing the predictive capability of loan default models more than three\nmonths in advance to enable early identification of default events, helping\nfinancial institutions implement preventive measures before risk events\nmaterialize. Existing methods have significant drawbacks, such as their lack of\naccuracy in early predictions and their dependence on training and testing\nwithin the same year and specific time frames. These issues limit their\npractical use, particularly with out-of-time data. To address these, the study\nintroduces two innovative architectures, GRU-KAN and LSTM-KAN, which merge\nKolmogorov-Arnold Networks (KAN) with Gated Recurrent Units (GRU) and Long\nShort-Term Memory (LSTM) networks. The proposed models were evaluated against\nthe baseline models (LSTM, GRU, LSTM-Attention, and LSTM-Transformer) in terms\nof accuracy, precision, recall, F1 and AUC in different lengths of feature\nwindow, sample sizes, and early prediction intervals. The results demonstrate\nthat the proposed model achieves a prediction accuracy of over 92% three months\nin advance and over 88% eight months in advance, significantly outperforming\nexisting baselines.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51faGRU-KAN\u548cLSTM-KAN\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u5408KAN\u4e0eGRU/LSTM\uff0c\u63d0\u9ad8\u4e86\u8d37\u6b3e\u8fdd\u7ea6\u9884\u6d4b\u7684\u63d0\u524d\u6027\u548c\u51c6\u786e\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u5728\u63d0\u524d\u9884\u6d4b\u8d37\u6b3e\u8fdd\u7ea6\u65b9\u9762\u5b58\u5728\u51c6\u786e\u6027\u4e0d\u8db3\u548c\u4f9d\u8d56\u7279\u5b9a\u65f6\u95f4\u8303\u56f4\u5185\u8bad\u7ec3\u6d4b\u8bd5\u7684\u7f3a\u9677\uff0c\u9650\u5236\u4e86\u5176\u5728\u5b9e\u9645\u4e2d\u7684\u5e94\u7528\u3002\u672c\u7814\u7a76\u65e8\u5728\u63d0\u9ad8\u8d37\u6b3e\u8fdd\u7ea6\u6a21\u578b\u63d0\u524d\u4e09\u4e2a\u6708\u4ee5\u4e0a\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u4ee5\u4fbf\u91d1\u878d\u673a\u6784\u80fd\u591f\u53ca\u65f6\u91c7\u53d6\u9884\u9632\u63aa\u65bd\u3002", "method": "\u672c\u7814\u7a76\u5f15\u5165\u4e86GRU-KAN\u548cLSTM-KAN\u4e24\u79cd\u65b0\u7684\u7f51\u7edc\u67b6\u6784\uff0c\u5c06Kolmogorov-Arnold\u7f51\u7edc\uff08KAN\uff09\u4e0e\u95e8\u5faa\u73af\u5355\u5143\uff08GRU\uff09\u548c\u957f\u77ed\u671f\u8bb0\u5fc6\uff08LSTM\uff09\u7f51\u7edc\u76f8\u7ed3\u5408\u3002", "result": "GRU-KAN\u548cLSTM-KAN\u6a21\u578b\u5728\u4e0d\u540c\u7279\u5f81\u7a97\u53e3\u957f\u5ea6\u3001\u6837\u672c\u91cf\u548c\u63d0\u524d\u9884\u6d4b\u533a\u95f4\u4e0b\uff0c\u4e0eLSTM\u3001GRU\u3001LSTM-Attention\u548cLSTM-Transformer\u7b49\u57fa\u7ebf\u6a21\u578b\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u6a21\u578b\u5728\u63d0\u524d\u9884\u6d4b\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684GRU-KAN\u548cLSTM-KAN\u6a21\u578b\u5728\u9884\u6d4b\u51c6\u786e\u6027\u3001\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u3001F1\u5206\u6570\u548cAUC\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u6a21\u578b\uff0c\u80fd\u591f\u63d0\u524d\u4e09\u4e2a\u6708\u8fbe\u523092%\u4ee5\u4e0a\u7684\u9884\u6d4b\u51c6\u786e\u7387\uff0c\u63d0\u524d\u516b\u4e2a\u6708\u8fbe\u523088%\u4ee5\u4e0a\u7684\u9884\u6d4b\u51c6\u786e\u7387\u3002"}}
{"id": "2507.13425", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13425", "abs": "https://arxiv.org/abs/2507.13425", "authors": ["Sirui Wang", "Zhou Guan", "Bingxi Zhao", "Tongjia Gu"], "title": "CaSTFormer: Causal Spatio-Temporal Transformer for Driving Intention Prediction", "comment": null, "summary": "Accurate prediction of driving intention is key to enhancing the safety and\ninteractive efficiency of human-machine co-driving systems. It serves as a\ncornerstone for achieving high-level autonomous driving. However, current\napproaches remain inadequate for accurately modeling the complex\nspatio-temporal interdependencies and the unpredictable variability of human\ndriving behavior. To address these challenges, we propose CaSTFormer, a Causal\nSpatio-Temporal Transformer to explicitly model causal interactions between\ndriver behavior and environmental context for robust intention prediction.\nSpecifically, CaSTFormer introduces a novel Reciprocal Shift Fusion (RSF)\nmechanism for precise temporal alignment of internal and external feature\nstreams, a Causal Pattern Extraction (CPE) module that systematically\neliminates spurious correlations to reveal authentic causal dependencies, and\nan innovative Feature Synthesis Network (FSN) that adaptively synthesizes these\npurified representations into coherent spatio-temporal inferences. We evaluate\nthe proposed CaSTFormer on the public Brain4Cars dataset, and it achieves\nstate-of-the-art performance. It effectively captures complex causal\nspatio-temporal dependencies and enhances both the accuracy and transparency of\ndriving intention prediction.", "AI": {"tldr": "CaSTFormer \u662f\u4e00\u79cd\u65b0\u7684\u56e0\u679c\u65f6\u7a7a Transformer\uff0c\u7528\u4e8e\u63d0\u9ad8\u9a7e\u9a76\u610f\u56fe\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u900f\u660e\u5ea6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u6a21\u62df\u4eba\u7c7b\u9a7e\u9a76\u884c\u4e3a\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u5f53\u524d\u7684\u9a7e\u9a76\u610f\u56fe\u9884\u6d4b\u65b9\u6cd5\u5728\u51c6\u786e\u6a21\u62df\u4eba\u7c7b\u9a7e\u9a76\u884c\u4e3a\u590d\u6742\u7684\u65f6\u7a7a\u76f8\u4e92\u4f9d\u8d56\u6027\u548c\u4e0d\u53ef\u9884\u6d4b\u7684\u53ef\u53d8\u6027\u65b9\u9762\u4ecd\u7136\u4e0d\u8db3\u3002", "method": "CaSTFormer \u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u53cd\u5411\u79fb\u4f4d\u878d\u5408\uff08RSF\uff09\u673a\u5236\u6765\u5b9e\u73b0\u5185\u90e8\u548c\u5916\u90e8\u7279\u5f81\u6d41\u7684\u7cbe\u786e\u65f6\u95f4\u5bf9\u9f50\uff0c\u4e00\u79cd\u56e0\u679c\u6a21\u5f0f\u63d0\u53d6\uff08CPE\uff09\u6a21\u5757\u6765\u7cfb\u7edf\u5730\u6d88\u9664\u865a\u5047\u76f8\u5173\u6027\u4ee5\u63ed\u793a\u771f\u5b9e\u7684\u56e0\u679c\u4f9d\u8d56\u5173\u7cfb\uff0c\u4ee5\u53ca\u4e00\u79cd\u521b\u65b0\u7684\u7279\u5f81\u5408\u6210\u7f51\u7edc\uff08FSN\uff09\u6765\u5c06\u8fd9\u4e9b\u7eaf\u5316\u7684\u8868\u793a\u81ea\u9002\u5e94\u5730\u5408\u6210\u4e3a\u8fde\u8d2f\u7684\u65f6\u7a7a\u63a8\u7406\u3002", "result": "CaSTFormer \u5728 Brain4Cars \u516c\u5171\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "CaSTFormer \u6709\u6548\u5730\u6355\u6349\u4e86\u590d\u6742\u7684\u56e0\u679c\u65f6\u7a7a\u4f9d\u8d56\u5173\u7cfb\uff0c\u63d0\u9ad8\u4e86\u9a7e\u9a76\u610f\u56fe\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u900f\u660e\u5ea6\u3002"}}
{"id": "2507.14021", "categories": ["cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.14021", "abs": "https://arxiv.org/abs/2507.14021", "authors": ["Xu Zhang", "Zhenyuan Yuan", "Minghui Zhu"], "title": "Byzantine-resilient federated online learning for Gaussian process regression", "comment": null, "summary": "In this paper, we study Byzantine-resilient federated online learning for\nGaussian process regression (GPR). We develop a Byzantine-resilient federated\nGPR algorithm that allows a cloud and a group of agents to collaboratively\nlearn a latent function and improve the learning performances where some agents\nexhibit Byzantine failures, i.e., arbitrary and potentially adversarial\nbehavior. Each agent-based local GPR sends potentially compromised local\npredictions to the cloud, and the cloud-based aggregated GPR computes a global\nmodel by a Byzantine-resilient product of experts aggregation rule. Then the\ncloud broadcasts the current global model to all the agents. Agent-based fused\nGPR refines local predictions by fusing the received global model with that of\nthe agent-based local GPR. Moreover, we quantify the learning accuracy\nimprovements of the agent-based fused GPR over the agent-based local GPR.\nExperiments on a toy example and two medium-scale real-world datasets are\nconducted to demonstrate the performances of the proposed algorithm.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u62dc\u5360\u5ead\u5bb9\u9519\u8054\u90a6GPR\u7b97\u6cd5\uff0c\u901a\u8fc7\u4e91\u7aef\u805a\u5408\u548c\u667a\u80fd\u4f53\u878d\u5408\u4f18\u5316\u9884\u6d4b\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u7814\u7a76\u62dc\u5360\u5ead\u5bb9\u9519\u7684\u8054\u90a6\u5728\u7ebf\u5b66\u4e60\uff0c\u7279\u522b\u662f\u9488\u5bf9\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\uff08GPR\uff09\uff0c\u4ee5\u5e94\u5bf9\u90e8\u5206\u667a\u80fd\u4f53\u53ef\u80fd\u51fa\u73b0\u7684\u4efb\u610f\u6216\u5bf9\u6297\u6027\u884c\u4e3a\uff0c\u65e8\u5728\u63d0\u9ad8\u8054\u90a6\u5b66\u4e60\u7684\u9c81\u68d2\u6027\u548c\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u62dc\u5360\u5ead\u5bb9\u9519\u8054\u90a6\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\uff08GPR\uff09\u7b97\u6cd5\u3002\u8be5\u7b97\u6cd5\u5141\u8bb8\u4e91\u548c\u4e00\u7ec4\u667a\u80fd\u4f53\u534f\u540c\u5b66\u4e60\u4e00\u4e2a\u6f5c\u5728\u51fd\u6570\uff0c\u5e76\u5728\u90e8\u5206\u667a\u80fd\u4f53\u51fa\u73b0\u62dc\u5360\u5ead\u6545\u969c\uff08\u5373\u4efb\u610f\u6216\u5bf9\u6297\u6027\u884c\u4e3a\uff09\u7684\u60c5\u51b5\u4e0b\u63d0\u9ad8\u5b66\u4e60\u6027\u80fd\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u57fa\u4e8e\u667a\u80fd\u4f53\u7684\u672c\u5730GPR\u53d1\u9001\u53ef\u80fd\u88ab\u7be1\u0447\u7684\u672c\u5730\u9884\u6d4b\u5230\u4e91\u7aef\uff0c\u4e91\u7aef\u805a\u5408\u7684GPR\u901a\u8fc7\u62dc\u5360\u5ead\u5bb9\u9519\u4e13\u5bb6\u4e58\u79ef\u805a\u5408\u89c4\u5219\u8ba1\u7b97\u5168\u5c40\u6a21\u578b\uff0c\u7136\u540e\u4e91\u7aef\u5c06\u5f53\u524d\u5168\u5c40\u6a21\u578b\u5e7f\u64ad\u7ed9\u6240\u6709\u667a\u80fd\u4f53\u3002\u57fa\u4e8e\u667a\u80fd\u4f53\u7684\u878d\u5408GPR\u901a\u8fc7\u878d\u5408\u63a5\u6536\u5230\u7684\u5168\u5c40\u6a21\u578b\u548c\u667a\u80fd\u4f53\u672c\u8eab\u7684\u672c\u5730GPR\u6765\u4f18\u5316\u672c\u5730\u9884\u6d4b\u3002", "result": "\u91cf\u5316\u4e86\u57fa\u4e8e\u667a\u80fd\u4f53\u7684\u878d\u5408GPR\u76f8\u5bf9\u4e8e\u57fa\u4e8e\u667a\u80fd\u4f53\u7684\u672c\u5730GPR\u5728\u5b66\u4e60\u51c6\u786e\u6027\u4e0a\u7684\u63d0\u5347\u3002", "conclusion": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u73a9\u5177\u793a\u4f8b\u548c\u4e24\u4e2a\u4e2d\u7b49\u89c4\u6a21\u7684\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u90fd\u8868\u73b0\u826f\u597d\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2507.13761", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.13761", "abs": "https://arxiv.org/abs/2507.13761", "authors": ["Palash Nandi", "Maithili Joshi", "Tanmoy Chakraborty"], "title": "Innocence in the Crossfire: Roles of Skip Connections in Jailbreaking Visual Language Models", "comment": null, "summary": "Language models are highly sensitive to prompt formulations - small changes\nin input can drastically alter their output. This raises a critical question:\nTo what extent can prompt sensitivity be exploited to generate inapt content?\nIn this paper, we investigate how discrete components of prompt design\ninfluence the generation of inappropriate content in Visual Language Models\n(VLMs). Specifically, we analyze the impact of three key factors on successful\njailbreaks: (a) the inclusion of detailed visual information, (b) the presence\nof adversarial examples, and (c) the use of positively framed beginning\nphrases. Our findings reveal that while a VLM can reliably distinguish between\nbenign and harmful inputs in unimodal settings (text-only or image-only), this\nability significantly degrades in multimodal contexts. Each of the three\nfactors is independently capable of triggering a jailbreak, and we show that\neven a small number of in-context examples (as few as three) can push the model\ntoward generating inappropriate outputs. Furthermore, we propose a framework\nthat utilizes a skip-connection between two internal layers of the VLM, which\nsubstantially increases jailbreak success rates, even when using benign images.\nFinally, we demonstrate that memes, often perceived as humorous or harmless,\ncan be as effective as toxic visuals in eliciting harmful content, underscoring\nthe subtle and complex vulnerabilities of VLMs.", "AI": {"tldr": "\u63d0\u793a\u7684\u7ec6\u5fae\u53d8\u5316\u4f1a\u4e25\u91cd\u5f71\u54cd\u8bed\u8a00\u6a21\u578b\u7684\u8f93\u51fa\uff0c\u672c\u7814\u7a76\u53d1\u73b0\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5728\u5904\u7406\u591a\u6a21\u6001\u8f93\u5165\u65f6\uff0c\u5176\u533a\u5206\u826f\u6027\u548c\u6709\u5bb3\u5185\u5bb9\u7684\u80fd\u529b\u4f1a\u4e0b\u964d\u3002\u7814\u7a76\u4eba\u5458\u53d1\u73b0\uff0c\u901a\u8fc7\u7279\u5b9a\u7684\u63d0\u793a\u6280\u5de7\uff08\u5982\u52a0\u5165\u89c6\u89c9\u4fe1\u606f\u3001\u5bf9\u6297\u6027\u793a\u4f8b\u6216\u79ef\u6781\u7684\u8d77\u59cb\u77ed\u8bed\uff09\u53ef\u4ee5\u8f7b\u6613\u5730\u201c\u8d8a\u72f1\u201dVLM\uff0c\u4f7f\u5176\u751f\u6210\u4e0d\u5f53\u5185\u5bb9\uff0c\u5373\u4f7f\u662f\u5c11\u91cf\u793a\u4f8b\u4e5f\u80fd\u8fbe\u5230\u6548\u679c\u3002\u6b64\u5916\uff0c\u4ed6\u4eec\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u6a21\u578b\u7ed3\u6784\u7684\u65b9\u6cd5\u6765\u589e\u5f3a\u8fd9\u79cd\u653b\u51fb\u3002\u6700\u540e\uff0c\u7814\u7a76\u6307\u51fa\uff0c\u770b\u4f3c\u65e0\u5bb3\u7684\u8868\u60c5\u5305\u4e5f\u53ef\u80fd\u6210\u4e3a\u8bf1\u5bfc VLM \u751f\u6210\u6709\u5bb3\u5185\u5bb9\u7684\u6709\u6548\u9014\u5f84\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7a76\u63d0\u793a\u7684\u654f\u611f\u6027\u5728\u591a\u5927\u7a0b\u5ea6\u4e0a\u4f1a\u88ab\u7528\u6765\u751f\u6210\u4e0d\u5f53\u5185\u5bb9\uff0c\u7279\u522b\u662f\u5173\u6ce8\u63d0\u793a\u8bbe\u8ba1\u4e2d\u7684\u79bb\u6563\u7ec4\u6210\u90e8\u5206\u5982\u4f55\u5f71\u54cd\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u751f\u6210\u4e0d\u5f53\u5185\u5bb9\u7684\u80fd\u529b\u3002", "method": "\u672c\u7814\u7a76\u63a2\u7a76\u4e86\u63d0\u793a\u8bbe\u8ba1\u4e2d\u7684\u79bb\u6563\u7ec4\u6210\u90e8\u5206\uff08\u89c6\u89c9\u4fe1\u606f\u3001\u5bf9\u6297\u6027\u793a\u4f8b\u3001\u79ef\u6781\u8d77\u59cb\u77ed\u8bed\uff09\u5982\u4f55\u5f71\u54cd\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u751f\u6210\u4e0d\u5f53\u5185\u5bb9\u3002\u7814\u7a76\u4eba\u5458\u5206\u6790\u4e86\u8fd9\u4e09\u4e2a\u56e0\u7d20\u5bf9\u6210\u529f\u8d8a\u72f1\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528 VLM \u5185\u90e8\u5c42\u4e4b\u95f4 skip-connection \u8fde\u63a5\u7684\u6846\u67b6\uff0c\u4ee5\u63d0\u9ad8\u8d8a\u72f1\u6210\u529f\u7387\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0cVLM \u5728\u591a\u6a21\u5f0f\u73af\u5883\u4e2d\u533a\u5206\u826f\u6027\u548c\u6709\u5bb3\u8f93\u5165\u7684\u80fd\u529b\u663e\u8457\u4e0b\u964d\u3002\u63d0\u793a\u8bbe\u8ba1\u7684\u4e09\u4e2a\u5173\u952e\u56e0\u7d20\uff08\u8be6\u7ec6\u7684\u89c6\u89c9\u4fe1\u606f\u3001\u5bf9\u6297\u6027\u793a\u4f8b\u3001\u79ef\u6781\u7684\u8d77\u59cb\u77ed\u8bed\uff09\u90fd\u80fd\u72ec\u7acb\u89e6\u53d1\u8d8a\u72f1\uff0c\u5e76\u4e14\u5c11\u91cf\uff08\u4e09\u4e2a\uff09\u7684\u4e0a\u4e0b\u6587\u793a\u4f8b\u5373\u53ef\u5bfc\u81f4\u6a21\u578b\u751f\u6210\u4e0d\u5f53\u8f93\u51fa\u3002\u6240\u63d0\u51fa\u7684\u6846\u67b6\u901a\u8fc7\u6dfb\u52a0 skip-connection \u8fde\u63a5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8d8a\u72f1\u6210\u529f\u7387\uff0c\u5373\u4f7f\u4f7f\u7528\u826f\u6027\u56fe\u50cf\u4e5f\u662f\u5982\u6b64\u3002\u8868\u60c5\u5305\u4e0e\u6709\u6bd2\u56fe\u50cf\u5728\u8bf1\u5bfc\u6709\u5bb3\u5185\u5bb9\u65b9\u9762\u540c\u6837\u6709\u6548\u3002", "conclusion": "\u867d\u7136\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5728\u5355\u4e00\u6a21\u5f0f\u4e0b\uff08\u4ec5\u6587\u672c\u6216\u4ec5\u56fe\u50cf\uff09\u53ef\u4ee5\u53ef\u9760\u5730\u533a\u5206\u826f\u6027\u548c\u6709\u5bb3\u8f93\u5165\uff0c\u4f46\u5728\u591a\u6a21\u5f0f\u73af\u5883\u4e2d\uff0c\u8fd9\u79cd\u80fd\u529b\u4f1a\u663e\u8457\u4e0b\u964d\u3002\u63d0\u793a\u7684\u79bb\u6563\u7ec4\u6210\u90e8\u5206\uff08\u8be6\u7ec6\u7684\u89c6\u89c9\u4fe1\u606f\u3001\u5bf9\u6297\u6027\u793a\u4f8b\u3001\u79ef\u6781\u7684\u8d77\u59cb\u77ed\u8bed\uff09\u4e2d\u7684\u4efb\u4f55\u4e00\u4e2a\u90fd\u80fd\u591f\u6210\u529f\u5730\u89e6\u53d1\u8d8a\u72f1\uff0c\u5e76\u4e14\u5f88\u5c11\u7684\u4e0a\u4e0b\u6587\u793a\u4f8b\uff08\u5c11\u81f3\u4e09\u4e2a\uff09\u5c31\u8db3\u4ee5\u4f7f\u6a21\u578b\u751f\u6210\u4e0d\u5f53\u8f93\u51fa\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u5728 VLM \u7684\u4e24\u4e2a\u5185\u90e8\u5c42\u4e4b\u95f4\u6dfb\u52a0 skip-connection \u8fde\u63a5\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u8d8a\u72f1\u6210\u529f\u7387\uff0c\u5373\u4f7f\u5728\u5b58\u5728\u826f\u6027\u56fe\u50cf\u7684\u60c5\u51b5\u4e0b\u4e5f\u662f\u5982\u6b64\u3002\u6709\u8da3\u7684\u662f\uff0c\u901a\u5e38\u88ab\u8ba4\u4e3a\u5e7d\u9ed8\u6216\u65e0\u5bb3\u7684\u8868\u60c5\u5305\u4e0e\u6709\u6bd2\u56fe\u50cf\u5728\u8bf1\u5bfc\u6709\u5bb3\u5185\u5bb9\u65b9\u9762\u540c\u6837\u6709\u6548\uff0c\u8fd9\u51f8\u663e\u4e86 VLM \u7ec6\u5fae\u4e14\u590d\u6742\u7684\u6f0f\u6d1e\u3002"}}
{"id": "2507.13973", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2507.13973", "abs": "https://arxiv.org/abs/2507.13973", "authors": ["T. Sanchez Mejia", "L. Nicolas", "A. Gelmini Rodriguez", "M. Afzelius"], "title": "Broadband and long-duration optical memory in Yb:YSO", "comment": null, "summary": "Optical quantum memories are essential components for realizing the full\npotential of quantum networks. Among these, rare-earth-doped crystal memories\nstand out due to their large multimode storage capabilities. To maximize the\nmultimode capacity in the time domain, it is key to simultaneously achieve\nlarge memory bandwidth and long optical storage time. Here, we demonstrate an\natomic frequency comb optical memory in Yb:YSO, with a memory bandwidth of\n250~MHz and a storage time of up to 125 $\\mu$s. The efficiency reaches 20 \\% at\nshort storage times, and 5 \\% at 125 $\\mu$s. These results were enabled by an\noptimized optical pumping scheme, guided by numerical modelling. Our approach\nis specifically designed for future spin-wave storage experiments, with the\ntheoretical bandwidth limit set at 288 MHz by the hyperfine structure of\nYb:YSO. Additionally, we introduce an efficient method for synthesizing the\noptical pumping waveforms required for generating combs with tens of thousands\nof teeth, as well as a simple yet frequency-agile laser setup for optical\npumping across a 10 GHz bandwidth.", "AI": {"tldr": "\u7814\u7a76\u4eba\u5458\u5728Yb:YSO\u6676\u4f53\u4e2d\u5b9e\u73b0\u4e86\u5148\u8fdb\u7684\u5149\u5b66\u91cf\u5b50\u5b58\u50a8\u5668\uff0c\u53ef\u5728125 \u03bcs\u5185\u5b58\u50a8\u4fe1\u606f\uff0c\u5e26\u5bbd\u8fbe250 MHz\uff0c\u6548\u7387\u8fbe20%\uff0c\u4e3a\u91cf\u5b50\u7f51\u7edc\u7684\u53d1\u5c55\u94fa\u5e73\u4e86\u9053\u8def\u3002", "motivation": "\u4e3a\u4e86\u5145\u5206\u53d1\u6325\u91cf\u5b50\u7f51\u7edc\u5728\u591a\u6a21\u5b58\u50a8\u80fd\u529b\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u9700\u8981\u5b9e\u73b0\u5bbd\u5e26\u5bbd\u548c\u957f\u5b58\u50a8\u65f6\u95f4\u7684\u540c\u6b65\uff0c\u4ee5\u6700\u5927\u5316\u65f6\u95f4\u57df\u5185\u7684\u591a\u6a21\u5bb9\u91cf\u3002", "method": "\u901a\u8fc7\u4f18\u5316\u7684\u5149\u5b66\u6cf5\u6d66\u65b9\u6848\uff0c\u7ed3\u5408\u6570\u503c\u6a21\u62df\uff0c\u5b9e\u73b0\u4e86\u5bbd\u5e26\u5bbd\u548c\u957f\u5b58\u50a8\u65f6\u95f4\u7684\u540c\u6b65\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u5408\u6210\u5149\u5b66\u6cf5\u6d66\u6ce2\u5f62\u7684\u65b9\u6cd5\uff0c\u4ee5\u53ca\u4e00\u79cd\u7528\u4e8e\u5149\u5b66\u6cf5\u6d66\u7684\u7b80\u5355\u4e14\u9891\u7387\u7075\u6d3b\u7684\u6fc0\u5149\u7cfb\u7edf\u3002", "result": "\u6210\u529f\u5728Yb:YSO\u6676\u4f53\u4e2d\u5b9e\u73b0\u4e86\u5177\u6709250 MHz\u5e26\u5bbd\u548c125 \u03bcs\u5b58\u50a8\u65f6\u95f4\u7684\u539f\u5b50\u9891\u7387\u68b3\u5149\u5b66\u91cf\u5b50\u5b58\u50a8\u5668\uff0c\u6548\u7387\u8fbe\u523020%\uff08\u77ed\u5b58\u50a8\u65f6\u95f4\uff09\u548c5%\uff08125 \u03bcs\uff09\u3002\u8be5\u65b9\u6cd5\u4e3a\u672a\u6765\u7684\u81ea\u65cb\u6ce2\u5b58\u50a8\u5b9e\u9a8c\u8fdb\u884c\u4e86\u4f18\u5316\uff0c\u7406\u8bba\u5e26\u5bbd\u6781\u9650\u4e3a288 MHz\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u4e00\u79cd\u5728Yb:YSO\u6676\u4f53\u4e2d\u5b9e\u73b0\u7684\u539f\u5b50\u9891\u7387\u68b3\u5149\u5b66\u91cf\u5b50\u5b58\u50a8\u5668\uff0c\u5176\u5185\u5b58\u5e26\u5bbd\u4e3a250 MHz\uff0c\u5b58\u50a8\u65f6\u95f4\u957f\u8fbe125 \u03bcs\uff0c\u6548\u7387\u5206\u522b\u8fbe\u523020%\u548c5%\u3002"}}
{"id": "2507.13703", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13703", "abs": "https://arxiv.org/abs/2507.13703", "authors": ["Martin Krutsk\u00fd", "Gustav \u0160\u00edr", "Vyacheslav Kungurtsev", "Georgios Korpas"], "title": "Binarizing Physics-Inspired GNNs for Combinatorial Optimization", "comment": "Accepted to the 28th European Conference on Artificial Intelligence\n  (ECAI 2025). This archival version includes supplementary appendices", "summary": "Physics-inspired graph neural networks (PI-GNNs) have been utilized as an\nefficient unsupervised framework for relaxing combinatorial optimization\nproblems encoded through a specific graph structure and loss, reflecting\ndependencies between the problem's variables. While the framework has yielded\npromising results in various combinatorial problems, we show that the\nperformance of PI-GNNs systematically plummets with an increasing density of\nthe combinatorial problem graphs. Our analysis reveals an interesting phase\ntransition in the PI-GNNs' training dynamics, associated with degenerate\nsolutions for the denser problems, highlighting a discrepancy between the\nrelaxed, real-valued model outputs and the binary-valued problem solutions. To\naddress the discrepancy, we propose principled alternatives to the naive\nstrategy used in PI-GNNs by building on insights from fuzzy logic and binarized\nneural networks. Our experiments demonstrate that the portfolio of proposed\nmethods significantly improves the performance of PI-GNNs in increasingly dense\nsettings.", "AI": {"tldr": "PI-GNN \u5728\u5904\u7406\u9ad8\u5bc6\u5ea6\u56fe\u65f6\u6027\u80fd\u4e0b\u964d\uff0c\u539f\u56e0\u5728\u4e8e\u6a21\u578b\u8f93\u51fa\u4e0e\u4e8c\u503c\u89e3\u4e4b\u95f4\u5b58\u5728\u5dee\u5f02\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6a21\u7cca\u903b\u8f91\u548c\u4e8c\u503c\u5316\u795e\u7ecf\u7f51\u7edc\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u8be5\u65b9\u6848\u53ef\u63d0\u9ad8 PI-GNN \u5728\u5bc6\u96c6\u56fe\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u867d\u7136 PI-GNN \u6846\u67b6\u5728\u5404\u79cd\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u4e2d\u53d6\u5f97\u4e86\u6709\u524d\u666f\u7684\u7ed3\u679c\uff0c\u4f46\u5176\u6027\u80fd\u4f1a\u968f\u7740\u7ec4\u5408\u95ee\u9898\u56fe\u5bc6\u5ea6\u7684\u589e\u52a0\u800c\u7cfb\u7edf\u6027\u5730\u4e0b\u964d\u3002\u6211\u4eec\u53d1\u73b0\u4e86 PI-GNN \u8bad\u7ec3\u52a8\u529b\u5b66\u4e2d\u4e0e\u66f4\u5bc6\u96c6\u95ee\u9898\u7684\u9000\u5316\u89e3\u76f8\u5173\u7684\u660e\u663e\u76f8\u53d8\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u7cca\u903b\u8f91\u548c\u4e8c\u503c\u5316\u795e\u7ecf\u7f51\u7edc\u7684\u539f\u5219\u6027\u66ff\u4ee3\u65b9\u6cd5\u6765\u89e3\u51b3 PI-GNN \u4e2d\u5b58\u5728\u7684\u73b0\u5b9e\u503c\u6a21\u578b\u8f93\u51fa\u4e0e\u4e8c\u503c\u503c\u95ee\u9898\u89e3\u4e4b\u95f4\u7684\u5dee\u5f02\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u7ec4\u5408\u663e\u8457\u63d0\u9ad8\u4e86 PI-GNN \u5728\u65e5\u76ca\u5bc6\u96c6\u7684\u8bbe\u7f6e\u4e2d\u7684\u6027\u80fd\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u7ec4\u5408\u663e\u8457\u63d0\u9ad8\u4e86 PI-GNN \u5728\u65e5\u76ca\u5bc6\u96c6\u7684\u8bbe\u7f6e\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2507.13428", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13428", "abs": "https://arxiv.org/abs/2507.13428", "authors": ["Jing Gu", "Xian Liu", "Yu Zeng", "Ashwin Nagarajan", "Fangrui Zhu", "Daniel Hong", "Yue Fan", "Qianqi Yan", "Kaiwen Zhou", "Ming-Yu Liu", "Xin Eric Wang"], "title": "\"PhyWorldBench\": A Comprehensive Evaluation of Physical Realism in Text-to-Video Models", "comment": "31 pages, 21 figures", "summary": "Video generation models have achieved remarkable progress in creating\nhigh-quality, photorealistic content. However, their ability to accurately\nsimulate physical phenomena remains a critical and unresolved challenge. This\npaper presents PhyWorldBench, a comprehensive benchmark designed to evaluate\nvideo generation models based on their adherence to the laws of physics. The\nbenchmark covers multiple levels of physical phenomena, ranging from\nfundamental principles like object motion and energy conservation to more\ncomplex scenarios involving rigid body interactions and human or animal motion.\nAdditionally, we introduce a novel \"\"Anti-Physics\"\" category, where prompts\nintentionally violate real-world physics, enabling the assessment of whether\nmodels can follow such instructions while maintaining logical consistency.\nBesides large-scale human evaluation, we also design a simple yet effective\nmethod that could utilize current MLLM to evaluate the physics realism in a\nzero-shot fashion. We evaluate 12 state-of-the-art text-to-video generation\nmodels, including five open-source and five proprietary models, with a detailed\ncomparison and analysis. we identify pivotal challenges models face in adhering\nto real-world physics. Through systematic testing of their outputs across 1,050\ncurated prompts-spanning fundamental, composite, and anti-physics scenarios-we\nidentify pivotal challenges these models face in adhering to real-world\nphysics. We then rigorously examine their performance on diverse physical\nphenomena with varying prompt types, deriving targeted recommendations for\ncrafting prompts that enhance fidelity to physical principles.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.13793", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.13793", "abs": "https://arxiv.org/abs/2507.13793", "authors": ["Enhao Cheng", "Shoujia Zhang", "Jianhua Yin", "Xuemeng Song", "Tian Gan", "Liqiang Nie"], "title": "An Enhanced Model-based Approach for Short Text Clustering", "comment": null, "summary": "Short text clustering has become increasingly important with the popularity\nof social media like Twitter, Google+, and Facebook. Existing methods can be\nbroadly categorized into two paradigms: topic model-based approaches and deep\nrepresentation learning-based approaches. This task is inherently challenging\ndue to the sparse, large-scale, and high-dimensional characteristics of the\nshort text data. Furthermore, the computational intensity required by\nrepresentation learning significantly increases the running time. To address\nthese issues, we propose a collapsed Gibbs Sampling algorithm for the Dirichlet\nMultinomial Mixture model (GSDMM), which effectively handles the sparsity and\nhigh dimensionality of short texts while identifying representative words for\neach cluster. Based on several aspects of GSDMM that warrant further\nrefinement, we propose an improved approach, GSDMM+, designed to further\noptimize its performance. GSDMM+ reduces initialization noise and adaptively\nadjusts word weights based on entropy, achieving fine-grained clustering that\nreveals more topic-related information. Additionally, strategic cluster merging\nis employed to refine clustering granularity, better aligning the predicted\ndistribution with the true category distribution. We conduct extensive\nexperiments, comparing our methods with both classical and state-of-the-art\napproaches. The experimental results demonstrate the efficiency and\neffectiveness of our methods. The source code for our model is publicly\navailable at https://github.com/chehaoa/VEMC.", "AI": {"tldr": "\u63d0\u51faGSDMM\u548cGSDMM+\u7b97\u6cd5\uff0c\u7528\u4e8e\u66f4\u6709\u6548\u5730\u5bf9\u7a00\u758f\u3001\u9ad8\u7ef4\u7684\u77ed\u6587\u672c\u6570\u636e\u8fdb\u884c\u805a\u7c7b\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8bc1\u660e\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u77ed\u6587\u672c\u805a\u7c7b\u5728\u793e\u4ea4\u5a92\u4f53\u65f6\u4ee3\u65e5\u76ca\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u77ed\u6587\u672c\u6570\u636e\u7684\u7a00\u758f\u6027\u3001\u5927\u89c4\u6a21\u548c\u9ad8\u7ef4\u5ea6\u7279\u6027\u65f6\u9762\u4e34\u6311\u6218\uff0c\u5e76\u4e14\u8868\u793a\u5b66\u4e60\u7684\u8ba1\u7b97\u5f3a\u5ea6\u5bfc\u81f4\u8fd0\u884c\u65f6\u95f4\u589e\u52a0\u3002\u9700\u8981\u66f4\u6709\u6548\u548c\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u72c4\u5229\u514b\u96f7\u591a\u9879\u5f0f\u6df7\u5408\u6a21\u578b\uff08GSDMM\uff09\u7684\u6298\u53e0\u5409\u5e03\u65af\u91c7\u6837\u7b97\u6cd5\uff0c\u4ee5\u89e3\u51b3\u77ed\u6587\u672c\u6570\u636e\u7684\u7a00\u758f\u6027\u3001\u5927\u89c4\u6a21\u548c\u9ad8\u7ef4\u5ea6\u7279\u5f81\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u65b9\u6cd5GSDMM+\uff0c\u901a\u8fc7\u51cf\u5c11\u521d\u59cb\u5316\u566a\u58f0\u548c\u57fa\u4e8e\u71b5\u81ea\u9002\u5e94\u8c03\u6574\u8bcd\u6743\u91cd\u6765\u4f18\u5316\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u7b56\u7565\u6027\u805a\u7c7b\u5408\u5e76\u8fdb\u4e00\u6b65\u4f18\u5316\u805a\u7c7b\u7c92\u5ea6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cGSDMM\u548cGSDMM+\u5728\u6548\u7387\u548c\u6709\u6548\u6027\u65b9\u9762\u4f18\u4e8e\u7ecf\u5178\u548c\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u8868\u660e\u8be5\u6a21\u578b\u80fd\u591f\u6709\u6548\u5904\u7406\u77ed\u6587\u672c\u805a\u7c7b\u4efb\u52a1\u3002", "conclusion": "GSDMM+\u901a\u8fc7\u51cf\u5c11\u521d\u59cb\u5316\u566a\u58f0\u548c\u57fa\u4e8e\u71b5\u7684\u81ea\u9002\u5e94\u8bcd\u6743\u91cd\u8c03\u6574\uff0c\u5b9e\u73b0\u4e86\u66f4\u7ec6\u7c92\u5ea6\u7684\u805a\u7c7b\uff0c\u66f4\u597d\u5730\u63ed\u793a\u4e86\u66f4\u591a\u4e0e\u4e3b\u9898\u76f8\u5173\u7684\u663e\u5f0f\u4fe1\u606f\u3002\u6b64\u5916\uff0c\u91c7\u7528\u7b56\u7565\u6027\u805a\u7c7b\u5408\u5e76\u6765\u4f18\u5316\u805a\u7c7b\u7c92\u5ea6\uff0c\u4f7f\u9884\u6d4b\u5206\u5e03\u4e0e\u771f\u5b9e\u7c7b\u522b\u5206\u5e03\u66f4\u5339\u914d\u3002\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u6548\u7387\u548c\u6709\u6548\u6027\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u548c\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002"}}
{"id": "2507.13999", "categories": ["quant-ph", "cs.NI", "cs.PF"], "pdf": "https://arxiv.org/pdf/2507.13999", "abs": "https://arxiv.org/abs/2507.13999", "authors": ["Sanidhay Bhambay", "Siddarth Koduru Joshi", "Thirupathaiah Vasantam", "Neil Walton"], "title": "The Proportional Fair Scheduler in Wavelength-Multiplexed Quantum Networks", "comment": null, "summary": "We address the problem of optimal pumping strategies in quantum networks.\nThese networks enable secure communication by distributing entangled photon\npairs to user (or node) pairs. Quantum Key Distribution (QKD) protocols, like\nBBM92, generate secret keys from entangled photons. While secure communication\nand error correction are essential for any quantum communication channel,\nresource contention, optimization, and fairness issues are critical for\nnetworks. In this article, we analyze the performance of quantum networks,\nproposing simple distributed algorithms for QKD networks generating secret\nkeys.\n  There are significant advantages of pumping entangled photons in QKD\nnetworks, but challenges arise in practical implementations. The underlying\nchannels are inherently time-varying, and thus data rates fluctuate between\nnodes. Moreover, multiple edges (node pairs) can be pumped simultaneously,\nalbeit at the cost of a reduced secret key rate (SKR). These temporal and\nspatial constraints yield a complex decision-making problem whose solutions may\nfavor a small set of user pairs to the detriment of overall, long-run network\nperformance.\n  We design adaptive pumping strategies that address these challenges in QKD\nnetworks. In particular, we find that a proportional fairness pumping strategy\n(PF-PS) stands out by dynamically prioritizing users with lower average secret\nkey rates and optimally balancing fairness with throughput. The proposed\nalgorithm is a natural extension to quantum networks of the Proportional Fair\nScheduler deployed in 4G LTE and 5G mobile networks. Both theoretical analysis\nand numerical simulations confirm that PF-PS is optimal for entangled state\ndistribution, and thus, when adapted appropriately, proportional fair pumping\nis a strong candidate for efficient resource allocation in quantum networks.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u9002\u7528\u4e8e\u91cf\u5b50\u7f51\u7edc\uff08\u7279\u522b\u662fQKD\u7f51\u7edc\uff09\u7684\u6bd4\u4f8b\u516c\u5e73\u6cf5\u6d66\u7b56\u7565\uff08PF-PS\uff09\uff0c\u4ee5\u89e3\u51b3\u8d44\u6e90\u5206\u914d\u95ee\u9898\u3002\u8be5\u7b56\u7565\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u6cf5\u6d66\u4f18\u5148\u7ea7\uff0c\u5e73\u8861\u4e86\u516c\u5e73\u6027\u548c\u541e\u5410\u91cf\uff0c\u5e76\u5728\u7406\u8bba\u548c\u6a21\u62df\u4e2d\u88ab\u8bc1\u660e\u662f\u6709\u6548\u7684\uff0c\u4e3a\u91cf\u5b50\u7f51\u7edc\u7684\u8d44\u6e90\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u91cf\u5b50\u7f51\u7edc\u4e2d\u5b9e\u9645\u5b9e\u73b0\u52a0\u5bc6\u5bc6\u94a5\u5206\u53d1\uff08QKD\uff09\u65f6\u9762\u4e34\u7684\u8d44\u6e90\u5206\u914d\u6311\u6218\uff0c\u7279\u522b\u662f\u901a\u9053\u7684\u6642\u53d8\u6027\u3001\u6570\u636e\u901f\u7387\u6ce2\u52a8\u4ee5\u53ca\u591a\u8fb9\u6cf5\u6d66\u5bfc\u81f4\u7684\u5bc6\u94a5\u751f\u6210\u7387\uff08SKR\uff09\u964d\u4f4e\u7b49\u95ee\u9898\uff0c\u9700\u8981\u8bbe\u8ba1\u81ea\u9002\u5e94\u7684\u6cf5\u6d66\u7b56\u7565\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6bd4\u4f8b\u516c\u5e73\u6cf5\u6d66\u7b56\u7565\uff08PF-PS\uff09\uff0c\u8be5\u7b56\u7565\u662f4G LTE\u548c5G\u79fb\u52a8\u7f51\u7edc\u4e2d\u6bd4\u4f8b\u516c\u5e73\u8c03\u5ea6\u7a0b\u5e8f\u7684\u81ea\u7136\u5ef6\u4f38\uff0c\u7528\u4e8e\u91cf\u5b50\u7f51\u7edc\u4e2d\u7684\u52a0\u5bc6\u5bc6\u94a5\u5206\u53d1\uff08QKD\uff09\u7f51\u7edc\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u6570\u503c\u6a21\u62df\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "result": "PF-PS\u7b56\u7565\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u91cf\u5b50\u7f51\u7edc\u4e2d\u6cf5\u6d66\u7684\u6642\u7a7a\u7ea6\u675f\uff0c\u5e76\u5728\u516c\u5e73\u6027\u548c\u541e\u5410\u91cf\u4e4b\u95f4\u53d6\u5f97\u826f\u597d\u5e73\u8861\uff0c\u6700\u7ec8\u5b9e\u73b0\u6700\u4f18\u7684\u8d44\u6e90\u5206\u914d\u3002", "conclusion": "\u6bd4\u4f8b\u516c\u5e73\u6cf5\u6d66\u7b56\u7565\uff08PF-PS\uff09\u662f\u91cf\u5b50\u7f51\u7edc\u4e2d\u8d44\u6e90\u5206\u914d\u7684\u6709\u529b\u5019\u9009\u8005\uff0c\u56e0\u4e3a\u5b83\u80fd\u591f\u52a8\u6001\u5730\u4f18\u5148\u8003\u8651\u5e73\u5747\u5bc6\u94a5\u751f\u6210\u7387\u8f83\u4f4e\u7684\u7528\u6237\uff0c\u5e76\u80fd\u5728\u516c\u5e73\u6027\u548c\u541e\u5410\u91cf\u4e4b\u95f4\u53d6\u5f97\u6700\u4f73\u5e73\u8861\u3002"}}
{"id": "2507.13704", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.13704", "abs": "https://arxiv.org/abs/2507.13704", "authors": ["Anabel Yong", "Austin Tripp", "Layla Hosseini-Gerami", "Brooks Paige"], "title": "Bayesian Optimization for Molecules Should Be Pareto-Aware", "comment": null, "summary": "Multi-objective Bayesian optimization (MOBO) provides a principled framework\nfor navigating trade-offs in molecular design. However, its empirical\nadvantages over scalarized alternatives remain underexplored. We benchmark a\nsimple Pareto-based MOBO strategy -- Expected Hypervolume Improvement (EHVI) --\nagainst a simple fixed-weight scalarized baseline using Expected Improvement\n(EI), under a tightly controlled setup with identical Gaussian Process\nsurrogates and molecular representations. Across three molecular optimization\ntasks, EHVI consistently outperforms scalarized EI in terms of Pareto front\ncoverage, convergence speed, and chemical diversity. While scalarization\nencompasses flexible variants -- including random or adaptive schemes -- our\nresults show that even strong deterministic instantiations can underperform in\nlow-data regimes. These findings offer concrete evidence for the practical\nadvantages of Pareto-aware acquisition in de novo molecular optimization,\nespecially when evaluation budgets are limited and trade-offs are nontrivial.", "AI": {"tldr": "EHVI\u5728\u5206\u5b50\u4f18\u5316\u4e2d\u4f18\u4e8e\u6807\u91cf\u5316EI\u3002", "motivation": "\u63a2\u8ba8MOBO\u5728\u5206\u5b50\u8bbe\u8ba1\u4e2d\u7684\u7ecf\u9a8c\u4f18\u52bf\uff0c\u5e76\u4e0e\u6807\u91cf\u5316\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\u3002", "method": "\u901a\u8fc7\u5728\u4e09\u4e2a\u5206\u5b50\u4f18\u5316\u4efb\u52a1\u4e2d\uff0c\u4f7f\u7528\u76f8\u540c\u7684GP\u4ee3\u7406\u548c\u5206\u5b50\u8868\u793a\uff0c\u5c06EHVI\uff08\u4e00\u79cd\u57fa\u4e8ePareto\u7684MOBO\u7b56\u7565\uff09\u4e0e\u56fa\u5b9a\u6743\u91cd\u6807\u91cf\u5316EI\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "EHVI\u5728\u5e15\u7d2f\u6258\u524d\u6cbf\u8986\u76d6\u7387\u3001\u6536\u655b\u901f\u5ea6\u548c\u5316\u5b66\u591a\u6837\u6027\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u6807\u91cf\u5316EI\uff0c\u5373\u4f7f\u662f\u5f3a\u786e\u5b9a\u6027\u6807\u91cf\u5316\u65b9\u6cd5\u5728\u4f4e\u6570\u636e\u91cf\u60c5\u51b5\u4e0b\u8868\u73b0\u4e5f\u53ef\u80fd\u4e0d\u4f73\u3002", "conclusion": "EHVI\u5728MOBO\u4e2d\u8868\u73b0\u4f18\u4e8e\u6807\u91cf\u5316EI\uff0c\u5c24\u5176\u5728\u6570\u636e\u7a00\u758f\u548c\u9700\u8981\u6743\u8861\u7684\u573a\u666f\u4e0b\uff0c\u4e3a\u4ece\u5934\u5206\u5b50\u4f18\u5316\u63d0\u4f9b\u4e86\u5b9e\u9645\u4f18\u52bf\u3002"}}
{"id": "2507.13486", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13486", "abs": "https://arxiv.org/abs/2507.13486", "authors": ["Debao Huang", "Rongjun Qin"], "title": "Uncertainty Quantification Framework for Aerial and UAV Photogrammetry through Error Propagation", "comment": "16 pages, 9 figures, this manuscript has been submitted to ISPRS\n  Journal of Photogrammetry and Remote Sensing for consideration", "summary": "Uncertainty quantification of the photogrammetry process is essential for\nproviding per-point accuracy credentials of the point clouds. Unlike airborne\nLiDAR, which typically delivers consistent accuracy across various scenes, the\naccuracy of photogrammetric point clouds is highly scene-dependent, since it\nrelies on algorithm-generated measurements (i.e., stereo or multi-view stereo).\nGenerally, errors of the photogrammetric point clouds propagate through a\ntwo-step process: Structure-from-Motion (SfM) with Bundle adjustment (BA),\nfollowed by Multi-view Stereo (MVS). While uncertainty estimation in the SfM\nstage has been well studied using the first-order statistics of the\nreprojection error function, that in the MVS stage remains largely unsolved and\nnon-standardized, primarily due to its non-differentiable and multi-modal\nnature (i.e., from pixel values to geometry). In this paper, we present an\nuncertainty quantification framework closing this gap by associating an error\ncovariance matrix per point accounting for this two-step photogrammetry\nprocess. Specifically, to estimate the uncertainty in the MVS stage, we propose\na novel, self-calibrating method by taking reliable n-view points (n>=6)\nper-view to regress the disparity uncertainty using highly relevant cues (such\nas matching cost values) from the MVS stage. Compared to existing approaches,\nour method uses self-contained, reliable 3D points extracted directly from the\nMVS process, with the benefit of being self-supervised and naturally adhering\nto error propagation path of the photogrammetry process, thereby providing a\nrobust and certifiable uncertainty quantification across diverse scenes. We\nevaluate the framework using a variety of publicly available airborne and UAV\nimagery datasets. Results demonstrate that our method outperforms existing\napproaches by achieving high bounding rates without overestimating uncertainty.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6444\u5f71\u6d4b\u91cf\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6846\u67b6\uff0c\u7279\u522b\u89e3\u51b3\u4e86MVS\u9636\u6bb5\u7684\u6311\u6218\uff0c\u4f7f\u7528\u81ea\u6821\u51c6\u65b9\u6cd5\u548cMVS\u8fc7\u7a0b\u4e2d\u7684\u53ef\u9760\u70b9\u6765\u56de\u5f52\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u5728\u5404\u79cd\u6570\u636e\u96c6\u4e0a\u8bc1\u660e\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u7531\u4e8e\u591a\u89c6\u89d2\u7acb\u4f53\uff08MVS\uff09\u9636\u6bb5\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u5177\u6709\u975e\u53ef\u5fae\u5206\u548c\u591a\u6a21\u6001\u7684\u6027\u8d28\uff0c\u8be5\u9886\u57df\u4ecd\u7136\u5b58\u5728\u672a\u89e3\u51b3\u4e14\u975e\u6807\u51c6\u5316\u7684\u6311\u6218\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u4e00\u4e2a\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6846\u67b6\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u81ea\u6821\u51c6\u65b9\u6cd5\uff0c\u5229\u7528\u6bcf\u4e2a\u89c6\u56fe\u4e2d\u53ef\u9760\u7684n\u89c6\u70b9\uff08n>=6\uff09\u56de\u5f52\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u5229\u7528MVS\u8fc7\u7a0b\u4e2d\u7684\u5339\u914d\u6210\u672c\u7b49\u76f8\u5173\u7ebf\u7d22\u3002", "result": "\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u4f7f\u7528\u4eceMVS\u8fc7\u7a0b\u4e2d\u76f4\u63a5\u63d0\u53d6\u7684\u3001\u81ea\u5305\u542b\u7684\u3001\u53ef\u9760\u76843D\u70b9\uff0c\u5177\u6709\u81ea\u76d1\u7763\u7684\u4f18\u70b9\uff0c\u5e76\u81ea\u7136\u5730\u9075\u5faa\u6444\u5f71\u6d4b\u91cf\u8fc7\u7a0b\u7684\u8bef\u5dee\u4f20\u64ad\u8def\u5f84\uff0c\u4ece\u800c\u4e3a\u5404\u79cd\u573a\u666f\u63d0\u4f9b\u9c81\u68d2\u4e14\u53ef\u9a8c\u8bc1\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u5173\u8054\u6bcf\u70b9\u7684\u8bef\u5dee\u534f\u65b9\u5dee\u77e9\u9635\u6765\u91cf\u5316\u5305\u62ec\u591a\u89c6\u89d2\u7acb\u4f53\uff08MVS\uff09\u5728\u5185\u7684\u6574\u4e2a\u6444\u5f71\u6d4b\u91cf\u8fc7\u7a0b\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u5728\u5404\u79cd\u516c\u5f00\u7684\u822a\u7a7a\u548c\u65e0\u4eba\u673a\u5f71\u50cf\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u8fc7\u9ad8\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fb9\u754c\u7387\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2507.13827", "categories": ["cs.CL", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13827", "abs": "https://arxiv.org/abs/2507.13827", "authors": ["Hosein Azarbonyad", "Zi Long Zhu", "Georgios Cheirmpos", "Zubair Afzal", "Vikrant Yadav", "Georgios Tsatsaronis"], "title": "Question-Answer Extraction from Scientific Articles Using Knowledge Graphs and Large Language Models", "comment": "SIGIR 2025", "summary": "When deciding to read an article or incorporate it into their research,\nscholars often seek to quickly identify and understand its main ideas. In this\npaper, we aim to extract these key concepts and contributions from scientific\narticles in the form of Question and Answer (QA) pairs. We propose two distinct\napproaches for generating QAs. The first approach involves selecting salient\nparagraphs, using a Large Language Model (LLM) to generate questions, ranking\nthese questions by the likelihood of obtaining meaningful answers, and\nsubsequently generating answers. This method relies exclusively on the content\nof the articles. However, assessing an article's novelty typically requires\ncomparison with the existing literature. Therefore, our second approach\nleverages a Knowledge Graph (KG) for QA generation. We construct a KG by\nfine-tuning an Entity Relationship (ER) extraction model on scientific articles\nand using it to build the graph. We then employ a salient triplet extraction\nmethod to select the most pertinent ERs per article, utilizing metrics such as\nthe centrality of entities based on a triplet TF-IDF-like measure. This measure\nassesses the saliency of a triplet based on its importance within the article\ncompared to its prevalence in the literature. For evaluation, we generate QAs\nusing both approaches and have them assessed by Subject Matter Experts (SMEs)\nthrough a set of predefined metrics to evaluate the quality of both questions\nand answers. Our evaluations demonstrate that the KG-based approach effectively\ncaptures the main ideas discussed in the articles. Furthermore, our findings\nindicate that fine-tuning the ER extraction model on our scientific corpus is\ncrucial for extracting high-quality triplets from such documents.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u79cd\u65b9\u6cd5\uff08\u4ec5\u57fa\u4e8e\u6587\u672c\u548c\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\uff09\u6765\u4ece\u79d1\u5b66\u8bba\u6587\u4e2d\u63d0\u53d6\u95ee\u7b54\u5bf9\uff0c\u4ee5\u5e2e\u52a9\u5b66\u8005\u5feb\u901f\u7406\u89e3\u8bba\u6587\u5185\u5bb9\u3002\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u65b9\u6cd5\u6548\u679c\u66f4\u597d\u3002", "motivation": "\u4e3a\u4e86\u5e2e\u52a9\u5b66\u8005\u4eec\u5feb\u901f\u8bc6\u522b\u548c\u7406\u89e3\u79d1\u5b66\u6587\u7ae0\u7684\u6838\u5fc3\u6982\u5ff5\u548c\u8d21\u732e\uff0c\u63d0\u51fa\u4ee5\u95ee\u7b54\uff08QA\uff09\u5bf9\u7684\u5f62\u5f0f\u63d0\u53d6\u8fd9\u4e9b\u5173\u952e\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u65b9\u6cd5\uff1a1. \u4ec5\u4f9d\u8d56\u6587\u7ae0\u5185\u5bb9\uff0c\u901a\u8fc7\u9009\u53d6\u663e\u8457\u6bb5\u843d\u3001\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u95ee\u9898\u3001\u5bf9\u95ee\u9898\u8fdb\u884c\u6392\u5e8f\u4ee5\u53ca\u751f\u6210\u7b54\u6848\u3002 2. \u5229\u7528\u77e5\u8bc6\u56fe\u8c31\uff08KG\uff09\u751f\u6210\u95ee\u7b54\u5bf9\uff0c\u5305\u62ec\u6784\u5efa\u77e5\u8bc6\u56fe\u8c31\uff08\u901a\u8fc7\u5fae\u8c03\u5b9e\u4f53\u5173\u7cfb\u63d0\u53d6\u6a21\u578b\uff09\u3001\u63d0\u53d6\u663e\u8457\u4e09\u5143\u7ec4\uff08\u57fa\u4e8e\u4e09\u5143\u7ec4\u7684TF-IDF\u7c7b\u5ea6\u91cf\u6765\u8bc4\u4f30\u4e09\u5143\u7ec4\u7684\u91cd\u8981\u6027\uff09\u4ee5\u53ca\u751f\u6210\u95ee\u7b54\u5bf9\u3002", "result": "\u901a\u8fc7\u4e13\u5bb6\u8bc4\u4f30\uff0c\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u65b9\u6cd5\u5728\u6355\u6349\u6587\u7ae0\u6838\u5fc3\u601d\u60f3\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5730\u6355\u6349\u6587\u7ae0\u7684\u6838\u5fc3\u601d\u60f3\uff0c\u5e76\u4e14\u5728\u79d1\u5b66\u6587\u732e\u4e0a\u5fae\u8c03\u5b9e\u4f53\u5173\u7cfb\u63d0\u53d6\u6a21\u578b\u5bf9\u4e8e\u63d0\u53d6\u9ad8\u8d28\u91cf\u4e09\u5143\u7ec4\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2507.14014", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2507.14014", "abs": "https://arxiv.org/abs/2507.14014", "authors": ["Hiroto Oka"], "title": "Definition of Current in Non-Hermitian Quantum Systems", "comment": "8 pages, 2 figures", "summary": "In recent years, non-Hermitian quantum systems (NHQS) have been actively\nstudied. In conventional quantum mechanics, Hermitisity is a fundamental\nproperty of Hamiltonians. However, it is known that when a system interacts\nwith its environment, its time evolution can be effectively described by\nnon-Hermitian Hamiltonians. NHQS are attracting attention because novel\nphysical phenomena not observed in Hermitian systems are predicted due to its\nnon-Hermitisity. In this study, I investigated the definition of current in\nNHQS. In Hermitian systems, equation of continuity(EOC) holds between the\nparticle density and the current. However, in NHQS, there are cases where the\nconventional definition of current does not satisfy EOC. Therefore, I\nconsidered the definition of current that satisfies EOC in NHQS. As a result, I\nderived current that satisfies EOC and behaves similarly to the current in\nHermitian systems with respect to physical quantities (electromagnetic fields).", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86NHQS\u4e2d\u6ee1\u8db3\u8fde\u7eed\u6027\u65b9\u7a0b\u7684\u7535\u6d41\u65b0\u5b9a\u4e49\uff0c\u8be5\u5b9a\u4e49\u5728\u7535\u78c1\u573a\u65b9\u9762\u4e0e\u5384\u5bc6\u7cfb\u7edf\u4e2d\u7684\u7535\u6d41\u884c\u4e3a\u76f8\u4f3c\u3002", "motivation": "NHQS\u56e0\u5176\u975e\u5384\u7c73\u6027\u800cPredicted novel physical phenomena\uff0c\u4f46\u5176\u7535\u6d41\u7684\u5b9a\u4e49\u5728NHQS\u4e2d\u5e76\u4e0d\u603b\u662f\u6ee1\u8db3\u8fde\u7eed\u6027\u65b9\u7a0b\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76\u65b0\u7684\u7535\u6d41\u5b9a\u4e49\u3002", "method": "\u901a\u8fc7\u63a8\u5bfc\u6ee1\u8db3\u8fde\u7eed\u6027\u65b9\u7a0b\u7684\u7535\u6d41\u5b9a\u4e49\u6765\u7814\u7a76NHQS\u7684\u7535\u6d41\u3002", "result": "\u63a8\u5bfc\u51fa\u4e86\u6ee1\u8db3\u8fde\u7eed\u6027\u65b9\u7a0b\u7684\u7535\u6d41\u5b9a\u4e49\uff0c\u5e76\u4e14\u8be5\u7535\u6d41\u5728\u7269\u7406\u91cf\uff08\u7535\u78c1\u573a\uff09\u65b9\u9762\u4e0e\u5384\u5bc6\u7cfb\u7edf\u4e2d\u7684\u7535\u6d41\u884c\u4e3a\u76f8\u4f3c\u3002", "conclusion": "\u672c\u7814\u7a76\u63a8\u5bfc\u51fa\u4e86\u6ee1\u8db3\u8fde\u7eed\u6027\u65b9\u7a0b\u4e14\u5728\u7269\u7406\u91cf\uff08\u7535\u78c1\u573a\uff09\u65b9\u9762\u4e0e\u5384\u5bc6\u7cfb\u7edf\u4e2d\u7684\u7535\u6d41\u884c\u4e3a\u76f8\u4f3c\u7684\u975e\u5384\u7c73\u91cf\u5b50\u7cfb\u7edf\uff08NHQS\uff09\u7684\u7535\u6d41\u5b9a\u4e49\u3002"}}
{"id": "2507.13707", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13707", "abs": "https://arxiv.org/abs/2507.13707", "authors": ["Hao Wang", "Yu Liu", "Daniel Biggs", "Haoru Wang", "Jiandong Yu", "Ping Huang"], "title": "Learning Deformable Body Interactions With Adaptive Spatial Tokenization", "comment": "21 pages, 15 figures", "summary": "Simulating interactions between deformable bodies is vital in fields like\nmaterial science, mechanical design, and robotics. While learning-based methods\nwith Graph Neural Networks (GNNs) are effective at solving complex physical\nsystems, they encounter scalability issues when modeling deformable body\ninteractions. To model interactions between objects, pairwise global edges have\nto be created dynamically, which is computationally intensive and impractical\nfor large-scale meshes. To overcome these challenges, drawing on insights from\ngeometric representations, we propose an Adaptive Spatial Tokenization (AST)\nmethod for efficient representation of physical states. By dividing the\nsimulation space into a grid of cells and mapping unstructured meshes onto this\nstructured grid, our approach naturally groups adjacent mesh nodes. We then\napply a cross-attention module to map the sparse cells into a compact,\nfixed-length embedding, serving as tokens for the entire physical state.\nSelf-attention modules are employed to predict the next state over these tokens\nin latent space. This framework leverages the efficiency of tokenization and\nthe expressive power of attention mechanisms to achieve accurate and scalable\nsimulation results. Extensive experiments demonstrate that our method\nsignificantly outperforms state-of-the-art approaches in modeling deformable\nbody interactions. Notably, it remains effective on large-scale simulations\nwith meshes exceeding 100,000 nodes, where existing methods are hindered by\ncomputational limitations. Additionally, we contribute a novel large-scale\ndataset encompassing a wide range of deformable body interactions to support\nfuture research in this area.", "AI": {"tldr": "AST\u65b9\u6cd5\u4f7f\u7528\u7f51\u683c\u548c\u6ce8\u610f\u529b\u673a\u5236\u6765\u9ad8\u6548\u6a21\u62df\u53ef\u53d8\u5f62\u7269\u4f53\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u8ba1\u7b97\u74f6\u9888\uff0c\u5e76\u5728\u5927\u89c4\u6a21\u6a21\u62df\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u89e3\u51b3\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\u5728\u6a21\u62df\u53ef\u53d8\u5f62\u7269\u4f53\u4ea4\u4e92\u65f6\u9047\u5230\u7684\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u7279\u522b\u662f\u4e3a\u5e94\u5bf9\u9700\u8981\u52a8\u6001\u521b\u5efa\u6210\u5bf9\u5168\u5c40\u8fb9\u800c\u5bfc\u81f4\u7684\u8ba1\u7b97\u5bc6\u96c6\u578b\u548c\u4e0d\u5207\u5b9e\u9645\u7684\u74f6\u9888\u3002", "method": "\u901a\u8fc7\u5c06\u6a21\u62df\u7a7a\u95f4\u5212\u5206\u4e3a\u7f51\u683c\u5355\u5143\uff0c\u5e76\u5c06\u975e\u7ed3\u6784\u5316\u7f51\u683c\u6620\u5c04\u5230\u7ed3\u6784\u5316\u7f51\u683c\u4e0a\uff0c\u5229\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u548c\u81ea\u6ce8\u610f\u529b\u673a\u5236\u6765\u9884\u6d4b\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u4e0b\u4e00\u4e2a\u72b6\u6001\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u6a21\u62df\u53ef\u53d8\u5f62\u7269\u4f53\u4ea4\u4e92\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5e76\u5728\u5904\u7406\u8d85\u8fc710\u4e07\u4e2a\u8282\u70b9\u7684\u7f51\u683c\u7684\u5927\u89c4\u6a21\u6a21\u62df\u4e2d\u8868\u73b0\u51fa\u6709\u6548\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u81ea\u9002\u5e94\u7a7a\u95f4\u6807\u8bb0\u5316\uff08AST\uff09\u65b9\u6cd5\u5728\u6a21\u62df\u53ef\u53d8\u5f62\u7269\u4f53\u4ea4\u4e92\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u4e14\u80fd\u591f\u5904\u7406\u5927\u89c4\u6a21\u6a21\u62df\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u8ba1\u7b97\u9650\u5236\u3002\u6b64\u5916\uff0c\u8fd8\u8d21\u732e\u4e86\u4e00\u4e2a\u65b0\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4ee5\u652f\u6301\u8be5\u9886\u57df\u672a\u6765\u7684\u7814\u7a76\u3002"}}
{"id": "2507.13514", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13514", "abs": "https://arxiv.org/abs/2507.13514", "authors": ["Bhumika Laxman Sadbhave", "Philipp Vaeth", "Denise Dejon", "Gunther Schorcht", "Magda Gregorov\u00e1"], "title": "Sugar-Beet Stress Detection using Satellite Image Time Series", "comment": null, "summary": "Satellite Image Time Series (SITS) data has proven effective for agricultural\ntasks due to its rich spectral and temporal nature. In this study, we tackle\nthe task of stress detection in sugar-beet fields using a fully unsupervised\napproach. We propose a 3D convolutional autoencoder model to extract meaningful\nfeatures from Sentinel-2 image sequences, combined with\nacquisition-date-specific temporal encodings to better capture the growth\ndynamics of sugar-beets. The learned representations are used in a downstream\nclustering task to separate stressed from healthy fields. The resulting stress\ndetection system can be directly applied to data from different years, offering\na practical and accessible tool for stress detection in sugar-beets.", "AI": {"tldr": "\u5229\u75283D\u5377\u79ef\u81ea\u7f16\u7801\u5668\u548c\u65f6\u5e8f\u7f16\u7801\u5bf9\u536b\u661f\u56fe\u50cf\u8fdb\u884c\u65e0\u76d1\u7763\u5b66\u4e60\uff0c\u5b9e\u73b0\u751c\u83dc\u80c1\u8feb\u68c0\u6d4b\u3002", "motivation": "\u5229\u7528\u536b\u661f\u56fe\u50cf\u65f6\u95f4\u5e8f\u5217\uff08SITS\uff09\u6570\u636e\u5bf9\u519c\u4e1a\u4efb\u52a1\uff08\u7279\u522b\u662f\u751c\u83dc\u80c1\u8feb\u68c0\u6d4b\uff09\u7684\u6709\u6548\u6027\uff0c\u63d0\u51fa\u4e00\u79cd\u5168\u65e0\u76d1\u7763\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a3D\u5377\u79ef\u81ea\u7f16\u7801\u5668\u6a21\u578b\u6765\u63d0\u53d6\u7279\u5f81\uff0c\u5e76\u7ed3\u5408\u7279\u5b9a\u91c7\u96c6\u65e5\u671f\u7684\u65f6\u5e8f\u7f16\u7801\u6765\u6355\u6349\u751c\u83dc\u7684\u751f\u957f\u52a8\u6001\uff0c\u7136\u540e\u5c06\u5b66\u4e60\u5230\u7684\u8868\u793a\u7528\u4e8e\u805a\u7c7b\u4efb\u52a1\u4ee5\u533a\u5206\u80c1\u8feb\u548c\u5065\u5eb7\u7530\u5730\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u53ef\u4ee5\u76f4\u63a5\u5e94\u7528\u4e8e\u4e0d\u540c\u5e74\u4efd\u6570\u636e\u7684\u751c\u83dc\u80c1\u8feb\u68c0\u6d4b\u7cfb\u7edf\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u4fbf\u6377\u7684\u5de5\u5177\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5168\u65e0\u76d1\u7763\u7684\u65b9\u6cd5\uff0c\u5229\u75283D\u5377\u79ef\u81ea\u7f16\u7801\u5668\u548c\u7279\u5b9a\u91c7\u96c6\u65e5\u671f\u7684\u65f6\u5e8f\u7f16\u7801\uff0c\u4eceSentinel-2\u56fe\u50cf\u5e8f\u5217\u4e2d\u63d0\u53d6\u7279\u5f81\uff0c\u7528\u4e8e\u751c\u83dc\u7530\u7684\u80c1\u8feb\u68c0\u6d4b\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u5c06\u5b66\u4e60\u5230\u7684\u8868\u793a\u7528\u4e8e\u4e0b\u6e38\u805a\u7c7b\u4efb\u52a1\uff0c\u4ee5\u533a\u5206\u80c1\u8feb\u548c\u5065\u5eb7\u7530\u5730\uff0c\u5e76\u4e14\u53ef\u4ee5\u76f4\u63a5\u5e94\u7528\u4e8e\u4e0d\u540c\u5e74\u4efd\u7684\u6570\u636e\u3002"}}
{"id": "2507.13839", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.13839", "abs": "https://arxiv.org/abs/2507.13839", "authors": ["Lizhi Ma", "Tong Zhao", "Shuai Zhang", "Nirui Song", "Hongliang He", "Anqi Li", "Ran Feng", "Huachuan Qiu", "Jingsong Ma", "Zhenzhong Lan"], "title": "The Expressions of Depression and Anxiety in Chinese Psycho-counseling: Usage of First-person Singular Pronoun and Negative Emotional Words", "comment": null, "summary": "This study explores the relationship between linguistic expressions and\npsychological states of depression and anxiety within Chinese psycho-counseling\ninteractions, focusing specifically on the usage of first-person singular\npronouns and negative emotional words. Utilizing a corpus derived from 735\nonline counseling sessions, the analysis employed a general linear mixed-effect\nmodel to assess linguistic patterns quantified by the Linguistic Inquiry and\nWord Count (LIWC) software. Results indicate a significant positive correlation\nbetween the frequency of negative emotional words and the severity of both\ndepressive and anxious states among clients. However, contrary to prior\nfindings predominantly derived from English-language contexts, the usage\nfrequency of first-person singular pronouns did not vary significantly with the\nclients' psychological conditions. These outcomes are discussed within the\nframework of cultural distinctions between collectivist Chinese contexts and\nindividualistic Western settings, as well as the interactive dynamics unique to\npsycho-counseling conversations. The findings highlight the nuanced influence\nof cultural and conversational contexts on language use in mental health\ncommunications, providing insights into psycholinguistic markers relevant to\ntherapeutic practices in Chinese-speaking populations.", "AI": {"tldr": "\u672c\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u4e2d\u56fd\u5fc3\u7406\u54a8\u8be2\u4e2d\uff0c\u8d1f\u9762\u8bcd\u6c47\u7684\u4f7f\u7528\u4e0e\u6291\u90c1\u548c\u7126\u8651\u7a0b\u5ea6\u76f8\u5173\uff0c\u4f46\u7b2c\u4e00\u4eba\u79f0\u4ee3\u8bcd\u7684\u4f7f\u7528\u4e0e\u5fc3\u7406\u5065\u5eb7\u65e0\u5173\u3002\u8fd9\u53ef\u80fd\u4e0e\u6587\u5316\u5dee\u5f02\u6709\u5173\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u8bed\u8a00\u8868\u8fbe\u4e0e\u4e2d\u56fd\u5fc3\u7406\u54a8\u8be2\u4e92\u52a8\u4e2d\u7684\u6291\u90c1\u548c\u7126\u8651\u5fc3\u7406\u72b6\u6001\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u91cd\u70b9\u5206\u6790\u7b2c\u4e00\u4eba\u79f0\u5355\u6570\u4ee3\u8bcd\u548c\u8d1f\u9762\u60c5\u7eea\u8bcd\u7684\u4f7f\u7528\u60c5\u51b5\uff0c\u4ee5\u7406\u89e3\u6587\u5316\u548c\u5bf9\u8bdd\u52a8\u6001\u5982\u4f55\u5f71\u54cd\u5fc3\u7406\u5065\u5eb7\u6c9f\u901a\u4e2d\u7684\u8bed\u8a00\u4f7f\u7528\u3002", "method": "\u672c\u7814\u7a76\u5229\u7528\u4e86\u6765\u81ea735\u6b21\u5728\u7ebf\u54a8\u8be2\u4f1a\u8bdd\u7684\u8bed\u6599\u5e93\uff0c\u5e76\u91c7\u7528\u901a\u7528\u7ebf\u6027\u6df7\u5408\u6548\u5e94\u6a21\u578b\uff0c\u7ed3\u5408\u8bed\u8a00\u67e5\u8be2\u548c\u6587\u5b57\u8ba1\u6570\uff08LIWC\uff09\u8f6f\u4ef6\u91cf\u5316\u8bed\u8a00\u6a21\u5f0f\uff0c\u4ee5\u63a2\u7a76\u8bed\u8a00\u8868\u8fbe\u4e0e\u6291\u90c1\u3001\u7126\u8651\u5fc3\u7406\u72b6\u6001\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u7279\u522b\u5173\u6ce8\u7b2c\u4e00\u4eba\u79f0\u5355\u6570\u4ee3\u8bcd\u548c\u8d1f\u9762\u60c5\u7eea\u8bcd\u7684\u4f7f\u7528\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u663e\u793a\uff0c\u8d1f\u9762\u60c5\u7eea\u8bcd\u7684\u4f7f\u7528\u9891\u7387\u4e0e\u5ba2\u6237\u7684\u6291\u90c1\u548c\u7126\u8651\u4e25\u91cd\u7a0b\u5ea6\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u7684\u6b63\u76f8\u5173\u3002\u7136\u800c\uff0c\u4e0e\u4e3b\u8981\u6765\u81ea\u82f1\u8bed\u8bed\u8a00\u73af\u5883\u7684\u5148\u524d\u7814\u7a76\u7ed3\u679c\u76f8\u53cd\uff0c\u7b2c\u4e00\u4eba\u79f0\u5355\u6570\u4ee3\u8bcd\u7684\u4f7f\u7528\u9891\u7387\u4e0e\u5ba2\u6237\u7684\u5fc3\u7406\u72b6\u51b5\u6ca1\u6709\u663e\u8457\u5dee\u5f02\u3002", "conclusion": "\u672c\u7814\u7a76\u63ed\u793a\u4e86\u5728\u4e2d\u56fd\u5fc3\u7406\u54a8\u8be2\u4e92\u52a8\u4e2d\uff0c\u8d1f\u9762\u60c5\u7eea\u8bcd\u7684\u4f7f\u7528\u9891\u7387\u4e0e\u6291\u90c1\u548c\u7126\u8651\u7684\u4e25\u91cd\u7a0b\u5ea6\u5448\u663e\u8457\u6b63\u76f8\u5173\uff0c\u800c\u7b2c\u4e00\u4eba\u79f0\u5355\u6570\u4ee3\u8bcd\u7684\u4f7f\u7528\u9891\u7387\u4e0e\u5ba2\u6237\u7684\u5fc3\u7406\u72b6\u51b5\u6ca1\u6709\u663e\u8457\u5dee\u5f02\u3002\u8fd9\u4e9b\u7ed3\u679c\u5f3a\u8c03\u4e86\u6587\u5316\u548c\u5bf9\u8bdd\u80cc\u666f\u5bf9\u5fc3\u7406\u5065\u5eb7\u6c9f\u901a\u4e2d\u8bed\u8a00\u4f7f\u7528\u7684\u7ec6\u5fae\u5f71\u54cd\uff0c\u5e76\u4e3a\u534e\u8bed\u4eba\u7fa4\u7684\u6cbb\u7597\u5b9e\u8df5\u63d0\u4f9b\u4e86\u76f8\u5173\u7684\u5fc3\u7406\u8bed\u8a00\u5b66\u6807\u8bb0\u3002"}}
{"id": "2507.14047", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2507.14047", "abs": "https://arxiv.org/abs/2507.14047", "authors": ["Kai Klink", "Andrew Raj Kirkpatrick", "Yukihiro Tadokoro", "Jonas Nils Becker", "Shannon Singer Nicley"], "title": "Fabrication of oriented NV center arrays in diamond via femtosecond laser writing and reorientation", "comment": null, "summary": "Nitrogen-vacancy (NV) centers in diamond are widely recognized as highly\npromising solid-state quantum sensors due to their long room temperature\ncoherence times and atomic-scale size, which enable exceptional sensitivity and\nnanoscale spatial resolution under ambient conditions. Ultrafast laser writing\nhas demonstrated the deterministic spatial control of individual NV$^-$\ncenters, however, the resulting random orientation of the defect axis limits\nthe magnetic field sensitivity and signal contrast. Here, we present an\nall-optical approach for reorienting laser-written NV$^-$ centers to lie along\na specific crystallographic axis using femtosecond laser annealing. This\ntechnique enables the creation of spatially ordered NV$^-$ arrays with uniform\norientation, for enhancing performance for quantum magnetometry. We achieve\ndeterministic alignment along the optical axis in both (100)- and\n(111)-oriented diamond substrates, paving the way for scalable,\nhigh-performance quantum devices based on orientation-controlled NV$^-$\ncenters.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5168\u5149\u5b66\u65b9\u6cd5\uff0c\u5229\u7528\u98de\u79d2\u6fc0\u5149\u9000\u706b\u6280\u672f\uff0c\u89e3\u51b3\u4e86NV\u8272\u5fc3\u968f\u673a\u53d6\u5411\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86NV\u8272\u5fc3\u9635\u5217\u7684\u5b9a\u5411\u6392\u5217\uff0c\u63d0\u9ad8\u4e86\u5176\u5728\u91cf\u5b50\u78c1\u573a\u4f20\u611f\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u514b\u670dNV\u8272\u5fc3\u5728\u6fc0\u5149\u5199\u5165\u8fc7\u7a0b\u4e2d\u4ea7\u751f\u7684\u968f\u673a\u53d6\u5411\u9650\u5236\u5176\u78c1\u573a\u4f20\u611f\u7075\u654f\u5ea6\u548c\u4fe1\u53f7\u5bf9\u6bd4\u5ea6\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5168\u5149\u5b66\u7684\u53d6\u5411\u91cd\u6392\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u98de\u79d2\u6fc0\u5149\u9000\u706b\u6280\u672f\uff0c\u5b9e\u73b0\u4e86NV\u8272\u5fc3\u6cbf\u7279\u5b9a\u6676\u4f53\u8f74\u7684\u5b9a\u5411\u91cd\u6392\u3002", "result": "\u6210\u529f\u5b9e\u73b0\u4e86NV\u8272\u5fc3\u6cbf\u5149\u8f74\u7684\u786e\u5b9a\u6027\u5bf9\u9f50\uff0c\u5e76\u5728(100)\u548c(111)\u91d1\u521a\u77f3\u886c\u5e95\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u4e3a\u6784\u5efa\u9ad8\u6027\u80fd\u91cf\u5b50\u5668\u4ef6\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002", "conclusion": "\u6fc0\u5149\u70e7\u8680\u548c\u9000\u706b\u6280\u672f\u5b9e\u73b0\u4e86NV\u8272\u5fc3\u5728(100)\u548c(111)\u91d1\u521a\u77f3\u886c\u5e95\u4e0a\u7684\u786e\u5b9a\u6027\u53d6\u5411\u6392\u5217\uff0c\u4e3a\u9ad8\u6027\u80fd\u91cf\u5b50\u5668\u4ef6\u7684\u5236\u9020\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.13716", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13716", "abs": "https://arxiv.org/abs/2507.13716", "authors": ["Danilo Avola", "Andrea Bernardini", "Giancarlo Crocetti", "Andrea Ladogana", "Mario Lezoche", "Maurizio Mancini", "Daniele Pannone", "Amedeo Ranaldi"], "title": "Benchmarking of EEG Analysis Techniques for Parkinson's Disease Diagnosis: A Comparison between Traditional ML Methods and Foundation DL Methods", "comment": null, "summary": "Parkinson's Disease PD is a progressive neurodegenerative disorder that\naffects motor and cognitive functions with early diagnosis being critical for\neffective clinical intervention Electroencephalography EEG offers a noninvasive\nand costeffective means of detecting PDrelated neural alterations yet the\ndevelopment of reliable automated diagnostic models remains a challenge In this\nstudy we conduct a systematic benchmark of traditional machine learning ML and\ndeep learning DL models for classifying PD using a publicly available oddball\ntask dataset Our aim is to lay the groundwork for developing an effective\nlearning system and to determine which approach produces the best results We\nimplement a unified sevenstep preprocessing pipeline and apply consistent\nsubjectwise crossvalidation and evaluation criteria to ensure comparability\nacross models Our results demonstrate that while baseline deep learning\narchitectures particularly CNNLSTM models achieve the best performance compared\nto other deep learning architectures underlining the importance of capturing\nlongrange temporal dependencies several traditional classifiers such as XGBoost\nalso offer strong predictive accuracy and calibrated decision boundaries By\nrigorously comparing these baselines our work provides a solid reference\nframework for future studies aiming to develop and evaluate more complex or\nspecialized architectures Establishing a reliable set of baseline results is\nessential to contextualize improvements introduced by novel methods ensuring\nscientific rigor and reproducibility in the evolving field of EEGbased\nneurodiagnostics", "AI": {"tldr": "\u672c\u7814\u7a76\u5bf9\u7528\u4e8e\u5e15\u91d1\u68ee\u75c5\u8bca\u65ad\u7684\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u53d1\u73b0CNN-LSTM\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u4f46XGBoost\u7b49\u4f20\u7edf\u6a21\u578b\u4e5f\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u672a\u6765\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u53c2\u8003\u6846\u67b6\u3002", "motivation": "\u5e15\u91d1\u68ee\u75c5\uff08PD\uff09\u7684\u65e9\u671f\u8bca\u65ad\u5bf9\u5176\u6709\u6548\u7684\u4e34\u5e8a\u5e72\u9884\u81f3\u5173\u91cd\u8981\u3002\u8111\u7535\u56fe\uff08EEG\uff09\u4f5c\u4e3a\u4e00\u79cd\u65e0\u521b\u4e14\u7ecf\u6d4e\u7684\u68c0\u6d4bPD\u76f8\u5173\u795e\u7ecf\u53d8\u5316\u7684\u65b9\u6cd5\uff0c\u5176\u5728\u5f00\u53d1\u53ef\u9760\u7684\u81ea\u52a8\u5316\u8bca\u65ad\u6a21\u578b\u65b9\u9762\u4ecd\u9762\u4e34\u6311\u6218\u3002\u672c\u7814\u7a76\u65e8\u5728\u4e3a\u5f00\u53d1\u6709\u6548\u7684\u5b66\u4e60\u7cfb\u7edf\u5960\u5b9a\u57fa\u7840\uff0c\u5e76\u786e\u5b9a\u54ea\u79cd\u65b9\u6cd5\u80fd\u4ea7\u751f\u6700\u4f73\u7ed3\u679c\u3002", "method": "\u672c\u7814\u7a76\u5bf9\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u4e86\u7cfb\u7edf\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4f7f\u7528\u516c\u5f00\u7684oddball\u4efb\u52a1\u6570\u636e\u96c6\u5bf9\u5e15\u91d1\u68ee\u75c5\u8fdb\u884c\u5206\u7c7b\u3002\u5b9e\u65bd\u4e86\u7edf\u4e00\u7684\u4e03\u6b65\u9884\u5904\u7406\u6d41\u7a0b\uff0c\u5e76\u5e94\u7528\u4e86\u6301\u7eed\u7684\u53d7\u8bd5\u8005\u4ea4\u53c9\u9a8c\u8bc1\u548c\u8bc4\u4f30\u6807\u51c6\uff0c\u4ee5\u786e\u4fdd\u6a21\u578b\u7684\u53ef\u6bd4\u6027\u3002", "result": "\u57fa\u7ebf\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u7279\u522b\u662fCNN-LSTM\u6a21\u578b\uff0c\u4e0e\u5176\u5b83\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u76f8\u6bd4\uff0c\u5728\u6355\u6349\u957f\u671f\u65f6\u95f4\u4f9d\u8d56\u6027\u65b9\u9762\u8868\u73b0\u51fa\u6700\u4f73\u6027\u80fd\u3002\u7136\u800c\uff0c\u5305\u62ecXGBoost\u5728\u5185\u7684\u51e0\u79cd\u4f20\u7edf\u5206\u7c7b\u5668\u4e5f\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u9884\u6d4b\u51c6\u786e\u6027\u548c\u6821\u51c6\u7684\u51b3\u7b56\u8fb9\u754c\u3002", "conclusion": "\u4e0e\u5176\u5b83\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u76f8\u6bd4\uff0c\u57fa\u4e8eCNN-LSTM\u7684\u6a21\u578b\u5728\u6355\u6349\u957f\u671f\u65f6\u95f4\u4f9d\u8d56\u6027\u65b9\u9762\u8868\u73b0\u51fa\u6700\u4f73\u6027\u80fd\uff0c\u800cXGBoost\u7b49\u4f20\u7edf\u5206\u7c7b\u5668\u4e5f\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u9884\u6d4b\u51c6\u786e\u6027\u548c\u6821\u51c6\u7684\u51b3\u7b56\u8fb9\u754c\u3002"}}
{"id": "2507.13841", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.13841", "abs": "https://arxiv.org/abs/2507.13841", "authors": ["Eitan Wagner", "Renana Keydar", "Omri Abend"], "title": "Modeling Fair Play in Detective Stories with Language Models", "comment": null, "summary": "Effective storytelling relies on a delicate balance between meeting the\nreader's prior expectations and introducing unexpected developments. In the\ndomain of detective fiction, this tension is known as fair play, which includes\nthe implicit agreement between the writer and the reader as to the range of\npossible resolutions the mystery story may have. In this work, we present a\nprobabilistic framework for detective fiction that allows us to define desired\nqualities. Using this framework, we formally define fair play and design\nappropriate metrics for it. Stemming from these definitions is an inherent\ntension between the coherence of the story, which measures how much it ``makes\nsense'', and the surprise it induces. We validate the framework by applying it\nto LLM-generated detective stories. This domain is appealing since we have an\nabundance of data, we can sample from the distribution generating the story,\nand the story-writing capabilities of LLMs are interesting in their own right.\nResults show that while LLM-generated stories may be unpredictable, they\ngenerally fail to balance the trade-off between surprise and fair play, which\ngreatly contributes to their poor quality.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5206\u6790\u4fa6\u63a2\u5c0f\u8bf4\u7684\u6846\u67b6\uff0c\u5e76\u7528\u5b83\u6765\u8bc4\u4f30\u4e86LLM\u751f\u6210\u7684\u5c0f\u8bf4\uff0c\u53d1\u73b0LLM\u5c0f\u8bf4\u867d\u7136\u4f1a\u5236\u9020\u610f\u5916\uff0c\u4f46\u5f80\u5f80\u7834\u574f\u4e86\u201c\u516c\u5e73\u535a\u5f08\u201d\u7684\u539f\u5219\uff0c\u5bfc\u81f4\u6545\u4e8b\u8d28\u91cf\u4e0d\u9ad8\u3002", "motivation": "\u4e3a\u4e86\u7ed9\u8bfb\u8005\u63d0\u4f9b\u66f4\u4f73\u7684\u9605\u8bfb\u4f53\u9a8c\uff0c\u9700\u8981\u5e73\u8861\u8bfb\u8005\u7684\u671f\u5f85\u4e0e\u6545\u4e8b\u7684\u610f\u5916\u6027\uff0c\u5c24\u5176\u5728\u4fa6\u63a2\u5c0f\u8bf4\u4e2d\uff0c\u201c\u516c\u5e73\u535a\u5f08\u201d\u662f\u4f5c\u8005\u4e0e\u8bfb\u8005\u4e4b\u95f4\u7684\u9ed8\u5951", "method": "\u63d0\u51fa\u4e00\u4e2a\u6982\u7387\u6846\u67b6\u6765\u5b9a\u4e49\u548c\u91cf\u5316\u201c\u516c\u5e73\u535a\u5f08\u201d\u548c\u201c\u60ca\u559c\u5ea6\u201d\u7b49\u54c1\u8d28", "result": "LLM\u751f\u6210\u7684\u4fa6\u63a2\u5c0f\u8bf4\u867d\u7136\u6709\u4e0d\u53ef\u9884\u6d4b\u6027\uff0c\u4f46\u5728\u201c\u60ca\u559c\u5ea6\u201d\u548c\u201c\u516c\u5e73\u535a\u5f08\u201d\u4e4b\u95f4\u672a\u80fd\u53d6\u5f97\u5e73\u8861\uff0c\u5f71\u54cd\u4e86\u6574\u4f53\u8d28\u91cf", "conclusion": "LLM\u751f\u6210\u7684\u4fa6\u63a2\u5c0f\u8bf4\u7684\u8d28\u91cf\u666e\u904d\u4e0d\u9ad8\uff0c\u56e0\u4e3a\u5b83\u4eec\u5728\u201c\u516c\u5e73\u535a\u5f08\u201d\u548c\u201c\u60ca\u559c\u5ea6\u201d\u4e4b\u95f4\u96be\u4ee5\u53d6\u5f97\u5e73\u8861"}}
{"id": "2507.14075", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2507.14075", "abs": "https://arxiv.org/abs/2507.14075", "authors": ["Paul R. Berman", "Peter W. Milonni"], "title": "Spontaneous emission in dipole approximation -- revisited", "comment": null, "summary": "Spontaneous emission in dipole approximation is studied theoretically using\nboth source-field theory and a Schrodinger picture approach. Using source-field\ntheory we obtain formal equations for the Poynting vector and energy density\nwithout making the rotating wave approximation (RWA) and Weisskopf-Wigner\napproximation (WWA). The initial condition at t=0 is one in which the atom is\nin an excited state and the field in the vacuum state. The source-field\nexpressions are evaluated within the the RWA and WWA and are found to satisfy\nPoynting's theorem. To explore the consequences of not making the RWA and WWA,\nthe Poynting vector and energy density are calculated using perturbation\ntheory. We use a Schrodinger picture approach and essentially reproduce and\ncomplement the results of Compagno, Passante, and Persico [J. Mod. Optics 37:8,\n1377 (2007)] and those of Power and Thirunamachandran [Phys. Rev. A 45, 54\n(1992)] obtained using a Heisenberg picture approach. The theory involves a sum\nover field mode frequencies and both finite cutoffs and convergence factors are\nused to carry out the sums. It is shown that the perturbation theory\ncalculation leads to unphysical values for atomic state populations for all\ntimes when a sum over all field frequencies is taken, even if a convergence\nfactor is used. It is also proved that the fields calculated using source-field\ntheory always satisfy Poynting's theorem for ct not equal to R, where R is the\ndistance from the atom.", "AI": {"tldr": "\u672c\u6587\u5229\u7528\u6e90\u573a\u7406\u8bba\u548c\u859b\u5b9a\u8c14\u7ed8\u666f\u65b9\u6cd5\u7814\u7a76\u4e86\u5076\u6781\u5b50\u8fd1\u4f3c\u4e0b\u7684\u81ea\u53d1\u8f90\u5c04\u3002\u7814\u7a76\u8868\u660e\uff0c\u5728\u4e0d\u4f7f\u7528RWA\u548cWWA\u7684\u60c5\u51b5\u4e0b\uff0c\u5fae\u6270\u7406\u8bba\u8ba1\u7b97\u4f1a\u5bfc\u81f4\u4e0d\u6b63\u786e\u7684\u539f\u5b50\u6001\u5e03\u5c45\u6570\uff0c\u4f46\u6e90\u573a\u7406\u8bba\u8ba1\u7b97\u7684\u573a\u6ee1\u8db3\u5761\u5370\u5ef7\u5b9a\u7406\u3002", "motivation": "\u672c\u6587\u7684\u52a8\u673a\u662f\u7814\u7a76\u5076\u6781\u5b50\u8fd1\u4f3c\u4e0b\u7684\u81ea\u53d1\u8f90\u5c04\uff0c\u5e76\u63a2\u7d22\u4e0d\u91c7\u7528\u65cb\u8f6c\u6ce2\u8fd1\u4f3c\uff08RWA\uff09\u548c\u7ef4\u65af\u79d1\u666e\u592b-\u7ef4\u683c\u7eb3\u8fd1\u4f3c\uff08WWA\uff09\u7684\u540e\u679c\u3002", "method": "\u672c\u6587\u7406\u8bba\u4e0a\u7814\u7a76\u4e86\u5076\u6781\u5b50\u8fd1\u4f3c\u4e0b\u7684\u81ea\u53d1\u8f90\u5c04\uff0c\u540c\u65f6\u4f7f\u7528\u4e86\u6e90\u573a\u7406\u8bba\u548c\u859b\u5b9a\u8c14\u7ed8\u666f\u65b9\u6cd5\u3002", "result": "\u5728\u4e0d\u91c7\u7528RWA\u548cWWA\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u5fae\u6270\u7406\u8bba\u8ba1\u7b97\u4e86\u5761\u5370\u5ef7\u77e2\u91cf\u548c\u80fd\u91cf\u5bc6\u5ea6\uff0c\u5e76\u8868\u660e\u5176\u5bfc\u81f4\u4e86\u7269\u7406\u4e0a\u4e0d\u6b63\u786e\u7684\u539f\u5b50\u6001\u5e03\u5c45\u6570\u3002\u4f46\u662f\uff0c\u7528\u6e90\u573a\u7406\u8bba\u8ba1\u7b97\u7684\u573a\u5728ct\u4e0d\u7b49\u4e8eR\u65f6\u603b\u662f\u6ee1\u8db3\u5761\u5370\u5ef7\u5b9a\u7406\u3002", "conclusion": "\u8be5\u7406\u8bba\u5728\u6240\u6709\u65f6\u95f4\u5185\u90fd\u5bfc\u81f4\u4e86\u7269\u7406\u4e0a\u4e0d\u6b63\u786e\u7684\u539f\u5b50\u6001\u5e03\u5c45\u6570\uff0c\u5373\u4f7f\u4f7f\u7528\u4e86\u6536\u655b\u56e0\u5b50\uff0c\u5e76\u4e14\u5bf9\u6240\u6709\u573a\u9891\u7387\u8fdb\u884c\u4e86\u6c42\u548c\u3002"}}
{"id": "2507.13718", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13718", "abs": "https://arxiv.org/abs/2507.13718", "authors": ["Danilo Avola", "Muhammad Yasir Bilal", "Emad Emam", "Cristina Lakasz", "Daniele Pannone", "Amedeo Ranaldi"], "title": "Bi-GRU Based Deception Detection using EEG Signals", "comment": null, "summary": "Deception detection is a significant challenge in fields such as security,\npsychology, and forensics. This study presents a deep learning approach for\nclassifying deceptive and truthful behavior using ElectroEncephaloGram (EEG)\nsignals from the Bag-of-Lies dataset, a multimodal corpus designed for\nnaturalistic, casual deception scenarios. A Bidirectional Gated Recurrent Unit\n(Bi-GRU) neural network was trained to perform binary classification of EEG\nsamples. The model achieved a test accuracy of 97\\%, along with high precision,\nrecall, and F1-scores across both classes. These results demonstrate the\neffectiveness of using bidirectional temporal modeling for EEG-based deception\ndetection and suggest potential for real-time applications and future\nexploration of advanced neural architectures.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528Bi-GRU\u795e\u7ecf\u7f51\u7edc\u5206\u6790\u8111\u7535\u56fe\u4fe1\u53f7\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u4ee5\u68c0\u6d4b\u6b3a\u9a97\u884c\u4e3a\uff0c\u51c6\u786e\u7387\u8fbe\u523097%\u3002", "motivation": "\u6b3a\u9a97\u68c0\u6d4b\u5728\u5b89\u5168\u3001\u5fc3\u7406\u5b66\u548c\u6cd5\u533b\u5b66\u7b49\u9886\u57df\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u53cc\u5411\u95e8\u63a7\u5faa\u73af\u5355\u5143\uff08Bi-GRU\uff09\u795e\u7ecf\u7f51\u7edc\u5bf9\u8111\u7535\u56fe\uff08EEG\uff09\u6837\u672c\u8fdb\u884c\u4e8c\u5143\u5206\u7c7b\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u3002", "result": "\u6240\u63d0\u51fa\u7684\u6a21\u578b\u5728Bag-of-Lies\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e8697%\u7684\u6d4b\u8bd5\u51c6\u786e\u7387\uff0c\u5e76\u5728\u4e24\u4e2a\u7c7b\u522b\u4e2d\u90fd\u53d6\u5f97\u4e86\u9ad8\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u3002", "conclusion": "\u4f7f\u7528\u53cc\u5411\u65f6\u95f4\u5efa\u6a21\u8fdb\u884c\u57fa\u4e8e\u8111\u7535\u56fe\u7684\u6b3a\u9a97\u68c0\u6d4b\u662f\u6709\u6548\u7684\uff0c\u5e76\u4e14\u6709\u6f5c\u529b\u7528\u4e8e\u5b9e\u65f6\u5e94\u7528\u3002"}}
{"id": "2507.13530", "categories": ["cs.CV", "math.DG", "math.OC"], "pdf": "https://arxiv.org/pdf/2507.13530", "abs": "https://arxiv.org/abs/2507.13530", "authors": ["Lukas Baumg\u00e4rtner", "Ronny Bergmann", "Roland Herzog", "Stephan Schmidt", "Manuel Wei\u00df"], "title": "Total Generalized Variation of the Normal Vector Field and Applications to Mesh Denoising", "comment": null, "summary": "We propose a novel formulation for the second-order total generalized\nvariation (TGV) of the normal vector on an oriented, triangular mesh embedded\nin $\\mathbb{R}^3$. The normal vector is considered as a manifold-valued\nfunction, taking values on the unit sphere. Our formulation extends previous\ndiscrete TGV models for piecewise constant scalar data that utilize a\nRaviart-Thomas function space. To exctend this formulation to the manifold\nsetting, a tailor-made tangential Raviart-Thomas type finite element space is\nconstructed in this work. The new regularizer is compared to existing methods\nin mesh denoising experiments.", "AI": {"tldr": "A new method for calculating the second-order TGV of normal vectors on meshes is introduced, using a special finite element space, and tested in mesh denoising.", "motivation": "To propose a novel formulation for the second-order total generalized variation (TGV) of the normal vector on an oriented, triangular mesh embedded in $\"R^3$.", "method": "A tailor-made tangential Raviart-Thomas type finite element space is constructed to extend the formulation to the manifold setting, considering the normal vector as a manifold-valued function on the unit sphere.", "result": "The new regularizer is compared to existing methods in mesh denoising experiments.", "conclusion": "The proposed formulation is compared to existing methods in mesh denoising experiments."}}
{"id": "2507.13858", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.13858", "abs": "https://arxiv.org/abs/2507.13858", "authors": ["Nicol\u00f2 Brunello", "Davide Rigamonti", "Andrea Sassella", "Vincenzo Scotti", "Mark James Carman"], "title": "InTraVisTo: Inside Transformer Visualisation Tool", "comment": "8 pages", "summary": "The reasoning capabilities of Large Language Models (LLMs) have increased\ngreatly over the last few years, as have their size and complexity.\nNonetheless, the use of LLMs in production remains challenging due to their\nunpredictable nature and discrepancies that can exist between their desired\nbehavior and their actual model output. In this paper, we introduce a new tool,\nInTraVisTo (Inside Transformer Visualisation Tool), designed to enable\nresearchers to investigate and trace the computational process that generates\neach token in a Transformer-based LLM. InTraVisTo provides a visualization of\nboth the internal state of the Transformer model (by decoding token embeddings\nat each layer of the model) and the information flow between the various\ncomponents across the different layers of the model (using a Sankey diagram).\nWith InTraVisTo, we aim to help researchers and practitioners better understand\nthe computations being performed within the Transformer model and thus to shed\nsome light on internal patterns and reasoning processes employed by LLMs.", "AI": {"tldr": "InTraVisTo \u662f\u4e00\u4e2a\u5185\u90e8 Transformer \u53ef\u89c6\u5316\u5de5\u5177\uff0c\u7528\u4e8e\u68c0\u67e5 LLM \u7684\u8ba1\u7b97\u8fc7\u7a0b\uff0c\u5e2e\u52a9\u7406\u89e3\u5176\u5185\u90e8\u6a21\u5f0f\u548c\u63a8\u7406\u3002", "motivation": "LLM \u5728\u751f\u4ea7\u4e2d\u7684\u5e94\u7528\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u5b83\u4eec\u4e0d\u53ef\u9884\u6d4b\u7684\u884c\u4e3a\u4ee5\u53ca\u671f\u671b\u884c\u4e3a\u4e0e\u5b9e\u9645\u6a21\u578b\u8f93\u51fa\u4e4b\u95f4\u7684\u5dee\u5f02\u3002\u9700\u8981\u4e00\u4e2a\u5de5\u5177\u6765\u5e2e\u52a9\u7406\u89e3 LLM \u7684\u5185\u90e8\u5de5\u4f5c\u539f\u7406\u3002", "method": "InTraVisTo \u901a\u8fc7\u89e3\u7801\u6a21\u578b\u5404\u5c42\u7684 token \u5d4c\u5165\u4ee5\u53ca\u4f7f\u7528\u6851\u57fa\u56fe\u53ef\u89c6\u5316\u6a21\u578b\u5404\u7ec4\u4ef6\u4e4b\u95f4\u8de8\u5c42\u7684\u4fe1\u606f\u6d41\uff0c\u63d0\u4f9b Transformer \u6a21\u578b\u5185\u90e8\u72b6\u6001\u548c\u4fe1\u606f\u6d41\u7684\u53ef\u89c6\u5316\u3002", "result": "InTraVisTo \u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u89c6\u5316\u5de5\u5177\uff0c\u7528\u4e8e\u68c0\u67e5\u548c\u8ffd\u8e2a Transformer LLM \u4e2d\u751f\u6210\u6bcf\u4e2a token \u7684\u8ba1\u7b97\u8fc7\u7a0b\u3002", "conclusion": "InTraVisTo \u65e8\u5728\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u548c\u4ece\u4e1a\u8005\u66f4\u597d\u5730\u7406\u89e3 Transformer \u6a21\u578b\u5185\u7684\u8ba1\u7b97\uff0c\u4ece\u800c\u63ed\u793a LLM \u4f7f\u7528\u7684\u5185\u90e8\u6a21\u5f0f\u548c\u63a8\u7406\u8fc7\u7a0b\u3002"}}
{"id": "2507.14076", "categories": ["quant-ph", "cond-mat.other"], "pdf": "https://arxiv.org/pdf/2507.14076", "abs": "https://arxiv.org/abs/2507.14076", "authors": ["Jacopo Tosca", "Francesco Carnazza", "Luca Giacomelli", "Cristiano Ciuti"], "title": "Efficient Variational Dynamics of Open Quantum Bosonic Systems via Automatic Differentiation", "comment": null, "summary": "We introduce a scalable variational method for simulating the dynamics of\ninteracting open quantum bosonic systems deep in the quantum regime. The method\nis based on a multi-dimensional Wigner phase-space representation and employs a\nVariational Multi-Gaussian (VMG) ansatz, whose accuracy is systematically\ncontrolled by the number of Gaussian components. The variational equations of\nmotion are derived from the Dirac-Frenkel principle and evaluated efficiently\nby combining the analytical structure of Gaussian functions with automatic\ndifferentiation. As a key application, we study a driven-dissipative\ntwo-dimensional Bose-Hubbard lattice with two-boson coherent driving and\ntwo-body losses. Using our dynamical approach, we compute the finite-size\nscaling of the Liouvillian spectral gap - extracted from the relaxation\ndynamics - which vanishes in the thermodynamic limit. Our results reveal\ncritical slowing down with dynamical exponents of the 2D quantum Ising\nuniversality class, demonstrating the power of our method to capture complex\nquantum dynamics in large open systems.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u4e00\u79cd\u7528\u4e8e\u6a21\u62df\u5f00\u653e\u91cf\u5b50\u73bb\u8272\u7cfb\u7edf\u52a8\u529b\u5b66\u7684\u53ef\u6269\u5c55\u53d8\u5206\u65b9\u6cd5\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8e\u5206\u6790\u4e8c\u7ef4Bose-Hubbard\u683c\u5b50\u7684\u4e34\u754c\u884c\u4e3a\u3002", "motivation": "\u4e3a\u4e86\u6a21\u62df\u91cf\u5b50\u533a\u57df\u4e2d\u76f8\u4e92\u4f5c\u7528\u7684\u5f00\u653e\u91cf\u5b50\u73bb\u8272\u7cfb\u7edf\u7684\u52a8\u529b\u5b66\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u7ef4Wigner\u76f8\u7a7a\u95f4\u8868\u793a\u548c\u53d8\u5206\u591a\u9ad8\u65af\uff08VMG\uff09ansatz\u7684\u53ef\u6269\u5c55\u53d8\u5206\u65b9\u6cd5\uff0c\u5e76\u4f7f\u7528Dirac-Frenkel\u539f\u7406\u63a8\u5bfc\u4e86\u8fd0\u52a8\u65b9\u7a0b\uff0c\u5229\u7528\u9ad8\u65af\u51fd\u6570\u7684\u89e3\u6790\u7ed3\u6784\u548c\u81ea\u52a8\u5fae\u5206\u8fdb\u884c\u9ad8\u6548\u8ba1\u7b97\u3002", "result": "\u8ba1\u7b97\u4e86\u9502\u5316\u4f53\u8c31\u9699\u7684\u6709\u9650\u5c3a\u5bf8\u6807\u5ea6\uff0c\u5e76\u63ed\u793a\u4e86\u5176\u5728\u70ed\u529b\u5b66\u6781\u9650\u4e0b\u6d88\u5931\uff0c\u8868\u73b0\u51fa\u4e34\u754c\u51cf\u901f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6355\u6349\u590d\u6742\u5927\u5f00\u653e\u91cf\u5b50\u7cfb\u7edf\u7684\u52a8\u529b\u5b66\u884c\u4e3a\uff0c\u5e76\u63ed\u793a\u4e86\u4e0e\u4e8c\u7ef4\u91cf\u5b50\u4f0a\u8f9b\u666e\u9002\u6027\u76f8\u53d8\u7c7b\u76f8\u5173\u7684\u52a8\u529b\u5b66\u5c55\u7f29\u548c\u52a8\u529b\u5b66\u6307\u6570\u3002"}}
{"id": "2507.13721", "categories": ["cs.LG", "cs.DB"], "pdf": "https://arxiv.org/pdf/2507.13721", "abs": "https://arxiv.org/abs/2507.13721", "authors": ["Zizhao Zhang", "Tianxiang Zhao", "Yu Sun", "Liping Sun", "Jichuan Kang"], "title": "Graph-Structured Data Analysis of Component Failure in Autonomous Cargo Ships Based on Feature Fusion", "comment": null, "summary": "To address the challenges posed by cascading reactions caused by component\nfailures in autonomous cargo ships (ACS) and the uncertainties in emergency\ndecision-making, this paper proposes a novel hybrid feature fusion framework\nfor constructing a graph-structured dataset of failure modes. By employing an\nimproved cuckoo search algorithm (HN-CSA), the literature retrieval efficiency\nis significantly enhanced, achieving improvements of 7.1% and 3.4% compared to\nthe NSGA-II and CSA search algorithms, respectively. A hierarchical feature\nfusion framework is constructed, using Word2Vec encoding to encode\nsubsystem/component features, BERT-KPCA to process failure modes/reasons, and\nSentence-BERT to quantify the semantic association between failure impact and\nemergency decision-making. The dataset covers 12 systems, 1,262 failure modes,\nand 6,150 propagation paths. Validation results show that the GATE-GNN model\nachieves a classification accuracy of 0.735, comparable to existing benchmarks.\nAdditionally, a silhouette coefficient of 0.641 indicates that the features are\nhighly distinguishable. In the label prediction results, the Shore-based\nMeteorological Service System achieved an F1 score of 0.93, demonstrating high\nprediction accuracy. This paper not only provides a solid foundation for\nfailure analysis in autonomous cargo ships but also offers reliable support for\nfault diagnosis, risk assessment, and intelligent decision-making systems. The\nlink to the dataset is\nhttps://github.com/wojiufukele/Graph-Structured-about-CSA.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6df7\u5408\u7279\u5f81\u878d\u5408\u6846\u67b6\u548c\u6539\u8fdb\u7684\u5e03\u8c37\u9e1f\u641c\u7d22\u7b97\u6cd5\uff0c\u7528\u4e8e\u6784\u5efa\u81ea\u4e3b\u8d27\u8239\u6545\u969c\u6a21\u5f0f\u7684\u56fe\u7ed3\u6784\u6570\u636e\u96c6\uff0c\u63d0\u9ad8\u4e86\u6587\u732e\u68c0\u7d22\u6548\u7387\u548c\u6545\u969c\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u4e3a\u4e86\u5e94\u5bf9\u81ea\u4e3b\u8d27\u8239\uff08ACS\uff09\u4e2d\u7531\u7ec4\u4ef6\u6545\u969c\u5f15\u8d77\u7684\u7ea7\u8054\u53cd\u5e94\u4ee5\u53ca\u5e94\u6025\u51b3\u7b56\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u6240\u5e26\u6765\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6df7\u5408\u7279\u5f81\u878d\u5408\u6846\u67b6\uff0c\u7528\u4e8e\u6784\u5efa\u6545\u969c\u6a21\u5f0f\u7684\u56fe\u7ed3\u6784\u6570\u636e\u96c6\u3002\u901a\u8fc7\u6539\u8fdb\u7684\u5e03\u8c37\u9e1f\u641c\u7d22\u7b97\u6cd5\uff08HN-CSA\uff09\u63d0\u9ad8\u4e86\u6587\u732e\u68c0\u7d22\u6548\u7387\uff08\u76f8\u6bd4NSGA-II\u548cCSA\u5206\u522b\u63d0\u9ad8\u4e867.1%\u548c3.4%\uff09\u3002\u6784\u5efa\u4e86\u5206\u5c42\u7279\u5f81\u878d\u5408\u6846\u67b6\uff0c\u4f7f\u7528Word2Vec\u5bf9\u5b50\u7cfb\u7edf/\u7ec4\u4ef6\u7279\u5f81\u8fdb\u884c\u7f16\u7801\uff0c\u4f7f\u7528BERT-KPCA\u5904\u7406\u6545\u969c\u6a21\u5f0f/\u539f\u56e0\uff0c\u5e76\u4f7f\u7528Sentence-BERT\u91cf\u5316\u6545\u969c\u5f71\u54cd\u4e0e\u5e94\u6025\u51b3\u7b56\u4e4b\u95f4\u7684\u8bed\u4e49\u5173\u8054\u3002\u8be5\u6570\u636e\u96c6\u6db5\u76d612\u4e2a\u7cfb\u7edf\uff0c1,262\u4e2a\u6545\u969c\u6a21\u5f0f\u548c6,150\u4e2a\u4f20\u64ad\u8def\u5f84\u3002", "result": "GATE-GNN\u6a21\u578b\u7684\u5206\u7c7b\u51c6\u786e\u7387\u8fbe\u52300.735\uff0c\u4e0e\u73b0\u6709\u57fa\u51c6\u76f8\u5f53\u3002\u8f6e\u5ed3\u7cfb\u6570\u4e3a0.641\uff0c\u8868\u660e\u7279\u5f81\u5177\u6709\u9ad8\u5ea6\u53ef\u533a\u5206\u6027\u3002\u5728\u6807\u7b7e\u9884\u6d4b\u7ed3\u679c\u4e2d\uff0c\u5cb8\u57fa\u6c14\u8c61\u670d\u52a1\u7cfb\u7edf\u8fbe\u5230\u4e860.93\u7684F1\u5206\u6570\uff0c\u663e\u793a\u51fa\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u81ea\u4e3b\u8d27\u8239\uff08ACS\uff09\u7684\u6545\u969c\u5206\u6790\u63d0\u4f9b\u4e86\u575a\u5b9e\u7684\u57fa\u7840\uff0c\u5e76\u4e3a\u6545\u969c\u8bca\u65ad\u3001\u98ce\u9669\u8bc4\u4f30\u548c\u667a\u80fd\u51b3\u7b56\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u652f\u6301\u3002"}}
{"id": "2507.13546", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13546", "abs": "https://arxiv.org/abs/2507.13546", "authors": ["Dmitrii Mikhailov", "Aleksey Letunovskiy", "Maria Kovaleva", "Vladimir Arkhipkin", "Vladimir Korviakov", "Vladimir Polovnikov", "Viacheslav Vasilev", "Evelina Sidorova", "Denis Dimitrov"], "title": "$\\nabla$NABLA: Neighborhood Adaptive Block-Level Attention", "comment": null, "summary": "Recent progress in transformer-based architectures has demonstrated\nremarkable success in video generation tasks. However, the quadratic complexity\nof full attention mechanisms remains a critical bottleneck, particularly for\nhigh-resolution and long-duration video sequences. In this paper, we propose\nNABLA, a novel Neighborhood Adaptive Block-Level Attention mechanism that\ndynamically adapts to sparsity patterns in video diffusion transformers (DiTs).\nBy leveraging block-wise attention with adaptive sparsity-driven threshold,\nNABLA reduces computational overhead while preserving generative quality. Our\nmethod does not require custom low-level operator design and can be seamlessly\nintegrated with PyTorch's Flex Attention operator. Experiments demonstrate that\nNABLA achieves up to 2.7x faster training and inference compared to baseline\nalmost without compromising quantitative metrics (CLIP score, VBench score,\nhuman evaluation score) and visual quality drop. The code and model weights are\navailable here: https://github.com/gen-ai-team/Wan2.1-NABLA", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.13870", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.13870", "abs": "https://arxiv.org/abs/2507.13870", "authors": ["Maciej Jalocha", "Johan Hausted Schmidt", "William Michelseen"], "title": "Label Unification for Cross-Dataset Generalization in Cybersecurity NER", "comment": "5 pages, 5 figures", "summary": "The field of cybersecurity NER lacks standardized labels, making it\nchallenging to combine datasets. We investigate label unification across four\ncybersecurity datasets to increase data resource usability. We perform a\ncoarse-grained label unification and conduct pairwise cross-dataset evaluations\nusing BiLSTM models. Qualitative analysis of predictions reveals errors,\nlimitations, and dataset differences. To address unification limitations, we\npropose alternative architectures including a multihead model and a graph-based\ntransfer model. Results show that models trained on unified datasets generalize\npoorly across datasets. The multihead model with weight sharing provides only\nmarginal improvements over unified training, while our graph-based transfer\nmodel built on BERT-base-NER shows no significant performance gains compared\nBERT-base-NER.", "AI": {"tldr": "\u7f51\u7edc\u5b89\u5168NER\u6807\u7b7e\u4e0d\u7edf\u4e00\uff0c\u672c\u7814\u7a76\u8bd5\u56fe\u901a\u8fc7\u6807\u7b7e\u7edf\u4e00\u6765\u6539\u5584\uff0c\u4f46\u53d1\u73b0\u7edf\u4e00\u540e\u7684\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u63d0\u51fa\u7684\u6539\u8fdb\u6a21\u578b\u4e5f\u672a\u5e26\u6765\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u7f51\u7edc\u5b89\u5168\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff08NER\uff09\u9886\u57df\u7f3a\u4e4f\u6807\u51c6\u5316\u6807\u7b7e\uff0c\u8fd9\u4f7f\u5f97\u7ec4\u5408\u6570\u636e\u96c6\u53d8\u5f97\u56f0\u96be\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u8de8\u8d8a\u56db\u4e2a\u7f51\u7edc\u5b89\u5168\u6570\u636e\u96c6\u7684\u6807\u7b7e\u7edf\u4e00\u6765\u63d0\u9ad8\u6570\u636e\u8d44\u6e90\u53ef\u7528\u6027\u3002", "method": "\u6211\u4eec\u6267\u884c\u4e86\u7c97\u7c92\u5ea6\u6807\u7b7e\u7edf\u4e00\uff0c\u5e76\u4f7f\u7528BiLSTM\u6a21\u578b\u8fdb\u884c\u4e86\u6210\u5bf9\u4ea4\u53c9\u6570\u636e\u96c6\u8bc4\u4f30\u3002\u4e3a\u4e86\u89e3\u51b3\u7edf\u4e00\u7684\u5c40\u9650\u6027\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u5305\u62ec\u591a\u5934\u6a21\u578b\u548c\u57fa\u4e8e\u56fe\u7684\u8fc1\u79fb\u6a21\u578b\u5728\u5185\u7684\u66ff\u4ee3\u67b6\u6784\u3002", "result": "\u5b9a\u6027\u5206\u6790\u9884\u6d4b\u7ed3\u679c\u63ed\u793a\u4e86\u9519\u8bef\u3001\u5c40\u9650\u6027\u548c\u6570\u636e\u96c6\u5dee\u5f02\u3002", "conclusion": "\u8de8\u6570\u636e\u96c6\u8bc4\u4f30\u663e\u793a\uff0c\u5728\u7edf\u4e00\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u5dee\u3002\u591a\u5934\u6a21\u578b\uff08\u5e26\u6743\u91cd\u5171\u4eab\uff09\u4ec5\u6bd4\u7edf\u4e00\u8bad\u7ec3\u6709\u8fb9\u9645\u6539\u8fdb\uff0c\u800c\u6211\u4eec\u57fa\u4e8eBERT-base-NER\u6784\u5efa\u7684\u56fe\u8fc1\u79fb\u6a21\u578b\u4e0eBERT-base-NER\u76f8\u6bd4\u6ca1\u6709\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2507.14085", "categories": ["quant-ph", "cond-mat.other"], "pdf": "https://arxiv.org/pdf/2507.14085", "abs": "https://arxiv.org/abs/2507.14085", "authors": ["Riccardo Cantone", "Shreyasi Mukherjee", "Luigi Giannelli", "Elisabetta Paladino", "Giuseppe Falci"], "title": "Machine Learning-aided Optimal Control of a noisy qubit", "comment": "11 pages, 3 figures", "summary": "We apply a graybox machine-learning framework to model and control a qubit\nundergoing Markovian and non-Markovian dynamics from environmental noise. The\napproach combines physics-informed equations with a lightweight transformer\nneural network based on the self-attention mechanism. The model is trained on\nsimulated data and learns an effective operator that predicts observables\naccurately, even in the presence of memory effects. We benchmark both\nnon-Gaussian random-telegraph noise and Gaussian Ornstein-Uhlenbeck noise and\nachieve low prediction errors even in challenging noise coupling regimes. Using\nthe model as a dynamics emulator, we perform gradient-based optimal control to\nidentify pulse sequences implementing a universal set of single-qubit gates,\nachieving fidelities above 99% for the lowest considered value of the coupling\nand remaining above 90% for the highest.", "AI": {"tldr": "A machine learning model using a transformer network accurately controls qubits with environmental noise and achieves high-fidelity quantum gates.", "motivation": "To model and control a qubit undergoing Markovian and non-Markovian dynamics from environmental noise, and to achieve high-fidelity single-qubit gates.", "method": "A graybox machine-learning framework combining physics-informed equations with a lightweight transformer neural network utilizing self-attention is applied. The model is trained on simulated data to predict observables and used for gradient-based optimal control to identify pulse sequences for single-qubit gates.", "result": "The model accurately predicts observables even with memory effects and achieves fidelities above 99% for single-qubit gates at low coupling, remaining above 90% at high coupling.", "conclusion": "The graybox machine-learning framework effectively models and controls qubits with Markovian and non-Markovian dynamics, achieving high-fidelity single-qubit gates even in challenging noise conditions."}}
{"id": "2507.13727", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13727", "abs": "https://arxiv.org/abs/2507.13727", "authors": ["Ren\u00e9 Heinrich", "Lukas Rauch", "Bernhard Sick", "Christoph Scholz"], "title": "Adversarial Training Improves Generalization Under Distribution Shifts in Bioacoustics", "comment": "Work in progress", "summary": "Adversarial training is a promising strategy for enhancing model robustness\nagainst adversarial attacks. However, its impact on generalization under\nsubstantial data distribution shifts in audio classification remains largely\nunexplored. To address this gap, this work investigates how different\nadversarial training strategies improve generalization performance and\nadversarial robustness in audio classification. The study focuses on two model\narchitectures: a conventional convolutional neural network (ConvNeXt) and an\ninherently interpretable prototype-based model (AudioProtoPNet). The approach\nis evaluated using a challenging bird sound classification benchmark. This\nbenchmark is characterized by pronounced distribution shifts between training\nand test data due to varying environmental conditions and recording methods, a\ncommon real-world challenge. The investigation explores two adversarial\ntraining strategies: one based on output-space attacks that maximize the\nclassification loss function, and another based on embedding-space attacks\ndesigned to maximize embedding dissimilarity. These attack types are also used\nfor robustness evaluation. Additionally, for AudioProtoPNet, the study assesses\nthe stability of its learned prototypes under targeted embedding-space attacks.\nResults show that adversarial training, particularly using output-space\nattacks, improves clean test data performance by an average of 10.5% relative\nand simultaneously strengthens the adversarial robustness of the models. These\nfindings, although derived from the bird sound domain, suggest that adversarial\ntraining holds potential to enhance robustness against both strong distribution\nshifts and adversarial attacks in challenging audio classification settings.", "AI": {"tldr": "\u5bf9\u6297\u6027\u8bad\u7ec3\u80fd\u63d0\u5347\u97f3\u9891\u5206\u7c7b\u6a21\u578b\u5728\u5206\u5e03\u504f\u79fb\u548c\u5bf9\u6297\u6027\u653b\u51fb\u4e0b\u7684\u8868\u73b0\u3002", "motivation": "\u63a2\u7d22\u5bf9\u6297\u6027\u8bad\u7ec3\u5bf9\u97f3\u9891\u5206\u7c7b\u6a21\u578b\u5728\u9762\u5bf9\u6570\u636e\u5206\u5e03\u504f\u79fb\u65f6\u7684\u6cdb\u5316\u80fd\u529b\u7684\u5f71\u54cd\uff0c\u4ee5\u53ca\u5176\u5bf9\u63d0\u9ad8\u6a21\u578b\u9c81\u68d2\u6027\u7684\u4f5c\u7528\u3002", "method": "\u7814\u7a76\u4e86\u4e24\u79cd\u5bf9\u6297\u6027\u8bad\u7ec3\u7b56\u7565\uff08\u57fa\u4e8e\u8f93\u51fa\u7a7a\u95f4\u548c\u5d4c\u5165\u7a7a\u95f4\u7684\u653b\u51fb\uff09\u5728ConvNeXt\u548cAudioProtoPNet\u4e24\u79cd\u6a21\u578b\u67b6\u6784\u4e0a\u7684\u5e94\u7528\uff0c\u5e76\u8bc4\u4f30\u4e86\u5176\u5728\u5177\u6709\u663e\u8457\u5206\u5e03\u504f\u79fb\u7684\u9e1f\u7c7b\u58f0\u97f3\u5206\u7c7b\u57fa\u51c6\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u5bf9\u6297\u6027\u8bad\u7ec3\uff08\u5c24\u5176\u662f\u57fa\u4e8e\u8f93\u51fa\u7a7a\u95f4\u7684\u653b\u51fb\uff09\u4f7f\u6a21\u578b\u7684\u5e72\u51c0\u6d4b\u8bd5\u6570\u636e\u6027\u80fd\u5e73\u5747\u63d0\u9ad8\u4e8610.5%\uff0c\u540c\u65f6\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u5bf9\u6297\u9c81\u68d2\u6027\u3002\u5bf9\u4e8eAudioProtoPNet\uff0c\u8fd8\u8bc4\u4f30\u4e86\u5176\u5b66\u4e60\u5230\u7684\u539f\u578b\u5728\u9762\u5bf9\u5d4c\u5165\u7a7a\u95f4\u653b\u51fb\u65f6\u7684\u7a33\u5b9a\u6027\u3002", "conclusion": "\u5bf9\u6297\u6027\u8bad\u7ec3\uff0c\u7279\u522b\u662f\u57fa\u4e8e\u8f93\u51fa\u7a7a\u95f4\u7684\u653b\u51fb\uff0c\u53ef\u4ee5\u63d0\u9ad8\u97f3\u9891\u5206\u7c7b\u6a21\u578b\u5728\u9762\u5bf9\u5206\u5e03\u504f\u79fb\u548c\u5bf9\u6297\u6027\u653b\u51fb\u65f6\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.13568", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13568", "abs": "https://arxiv.org/abs/2507.13568", "authors": ["Kaihong Wang", "Donghyun Kim", "Margrit Betke"], "title": "LoRA-Loop: Closing the Synthetic Replay Cycle for Continual VLM Learning", "comment": null, "summary": "Continual learning for vision-language models has achieved remarkable\nperformance through synthetic replay, where samples are generated using Stable\nDiffusion to regularize during finetuning and retain knowledge. However,\nreal-world downstream applications often exhibit domain-specific nuances and\nfine-grained semantics not captured by generators, causing synthetic-replay\nmethods to produce misaligned samples that misguide finetuning and undermine\nretention of prior knowledge. In this work, we propose a LoRA-enhanced\nsynthetic-replay framework that injects task-specific low-rank adapters into a\nfrozen Stable Diffusion model, efficiently capturing each new task's unique\nvisual and semantic patterns. Specifically, we introduce a two-stage,\nconfidence-based sample selection: we first rank real task data by\npost-finetuning VLM confidence to focus LoRA finetuning on the most\nrepresentative examples, then generate synthetic samples and again select them\nby confidence for distillation. Our approach integrates seamlessly with\nexisting replay pipelines-simply swap in the adapted generator to boost replay\nfidelity. Extensive experiments on the Multi-domain Task Incremental Learning\n(MTIL) benchmark show that our method outperforms previous synthetic-replay\ntechniques, achieving an optimal balance among plasticity, stability, and\nzero-shot capability. These results demonstrate the effectiveness of generator\nadaptation via LoRA for robust continual learning in VLMs.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cdLoRA\u589e\u5f3a\u7684\u5408\u6210\u91cd\u653e\u65b9\u6cd5\uff0c\u901a\u8fc7\u9002\u914dStable Diffusion\u6a21\u578b\u6765\u751f\u6210\u66f4\u5339\u914d\u7684\u91cd\u653e\u6837\u672c\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u5305\u542b\u7ec6\u5fae\u5dee\u522b\u548c\u7ec6\u7c92\u5ea6\u8bed\u4e49\u7684\u771f\u5b9e\u6570\u636e\u65f6\u9047\u5230\u7684\u95ee\u9898\uff0c\u5e76\u5728MTIL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u5408\u6210\u91cd\u653e\u65b9\u6cd5\u5728\u5904\u7406\u5305\u542b\u9886\u57df\u7279\u5b9a\u7ec6\u5fae\u5dee\u522b\u548c\u7ec6\u7c92\u5ea6\u8bed\u4e49\u7684\u771f\u5b9e\u4e0b\u6e38\u5e94\u7528\u65f6\uff0c\u4f1a\u751f\u6210\u4e0e\u771f\u5b9e\u6570\u636e\u4e0d\u5339\u914d\u7684\u6837\u672c\uff0c\u4ece\u800c\u8bef\u5bfc\u5fae\u8c03\u5e76\u635f\u5bb3\u5148\u524d\u77e5\u8bc6\u7684\u4fdd\u7559\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cdLoRA\u589e\u5f3a\u7684\u5408\u6210\u91cd\u653e\u6846\u67b6\uff0c\u901a\u8fc7\u6ce8\u5165\u4efb\u52a1\u7279\u5b9a\u7684\u4f4e\u79e9\u9002\u914d\u5668\u6765\u6539\u8fdb\u51bb\u7ed3\u7684Stable Diffusion\u6a21\u578b\uff0c\u5e76\u7ed3\u5408\u4e24\u9636\u6bb5\u7684\u7f6e\u4fe1\u5ea6\u91c7\u6837\u65b9\u6cd5\u6765\u9009\u62e9\u771f\u5b9e\u6570\u636e\u548c\u751f\u6210\u7684\u5408\u6210\u6570\u636e\uff0c\u4ee5\u63d0\u5347\u91cd\u653e\u4fdd\u771f\u5ea6\u3002", "result": "\u5728\u591a\u57df\u589e\u91cf\u5b66\u4e60\uff08MTIL\uff09\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u5148\u524d\u7684\u5408\u6210\u91cd\u653e\u6280\u672f\uff0c\u5e76\u5728\u5851\u6027\u3001\u7a33\u5b9a\u6027\u548c\u96f6\u6837\u672c\u80fd\u529b\u4e4b\u95f4\u53d6\u5f97\u4e86\u6700\u4f73\u5e73\u8861\u3002", "conclusion": "LoRA\u589e\u5f3a\u7684\u751f\u6210\u5668\u9002\u914d\u80fd\u591f\u6709\u6548\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u8868\u73b0\uff0c\u5b9e\u73b0\u4e86\u5851\u6027\u3001\u7a33\u5b9a\u6027\u548c\u96f6\u6837\u672c\u80fd\u529b\u4e4b\u95f4\u7684\u6700\u4f73\u5e73\u8861\u3002"}}
{"id": "2507.13875", "categories": ["cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.13875", "abs": "https://arxiv.org/abs/2507.13875", "authors": ["Carlos Mena", "Pol Serra", "Jacobo Romero", "Abir Messaoudi", "Jose Giraldo", "Carme Armentano-Oller", "Rodolfo Zevallos", "Ivan Meza", "Javier Hernando"], "title": "Optimizing ASR for Catalan-Spanish Code-Switching: A Comparative Analysis of Methodologies", "comment": "Accepted at Interspeech 2025", "summary": "Code-switching (CS), the alternating use of two or more languages, challenges\nautomatic speech recognition (ASR) due to scarce training data and linguistic\nsimilarities. The lack of dedicated CS datasets limits ASR performance, as most\nmodels rely on monolingual or mixed-language corpora that fail to reflect\nreal-world CS patterns. This issue is critical in multilingual societies where\nCS occurs in informal and formal settings. A key example is Catalan-Spanish CS,\nwidely used in media and parliamentary speeches. In this work, we improve ASR\nfor Catalan-Spanish CS by exploring three strategies: (1) generating synthetic\nCS data, (2) concatenating monolingual audio, and (3) leveraging real CS data\nwith language tokens. We extract CS data from Catalan speech corpora and\nfine-tune OpenAI's Whisper models, making them available on Hugging Face.\nResults show that combining a modest amount of synthetic CS data with the\ndominant language token yields the best transcription performance.", "AI": {"tldr": "\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u751f\u6210\u5408\u6210\u6570\u636e\u3001\u8fde\u63a5\u5355\u8bed\u97f3\u9891\u548c\u5229\u7528\u5e26\u8bed\u8a00\u4ee4\u724c\u7684\u771f\u5b9e\u6570\u636e\u6765\u6539\u8fdb\u52a0\u6cf0\u7f57\u5c3c\u4e9a\u8bed-\u897f\u73ed\u7259\u8bed\u4ee3\u7801\u8f6c\u6362\u7684\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0c\u7ed3\u5408\u9002\u91cf\u7684\u5408\u6210\u6570\u636e\u548c\u4f18\u52bf\u8bed\u8a00\u4ee4\u724c\u80fd\u5e26\u6765\u6700\u4f73\u7684\u8f6c\u5f55\u6548\u679c\u3002", "motivation": "\u4ee3\u7801\u8f6c\u6362\uff08CS\uff09\u662f\u53cc\u8bed\u6216\u591a\u8bed\u4f7f\u7528\u8005\u5728\u4ea4\u6d41\u4e2d\u4ea4\u66ff\u4f7f\u7528\u4e24\u79cd\u6216\u591a\u79cd\u8bed\u8a00\u7684\u73b0\u8c61\u3002\u7136\u800c\uff0c\u7531\u4e8e\u8bad\u7ec3\u6570\u636e\u7a00\u5c11\u548c\u8bed\u8a00\u76f8\u4f3c\u6027\uff0c\u4ee3\u7801\u8f6c\u6362\u5bf9\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u6784\u6210\u4e86\u91cd\u5927\u6311\u6218\u3002\u73b0\u6709\u7684 ASR \u6a21\u578b\u901a\u5e38\u4f9d\u8d56\u4e8e\u5355\u8bed\u6216\u6df7\u5408\u8bed\u6599\u5e93\uff0c\u8fd9\u4e9b\u8bed\u6599\u5e93\u65e0\u6cd5\u51c6\u786e\u53cd\u6620\u771f\u5b9e\u7684 CS \u6a21\u5f0f\u3002\u8fd9\u79cd\u60c5\u51b5\u5728\u793e\u4f1a\u4e2d\u5c24\u5176\u666e\u904d\uff0c\u5728\u8fd9\u4e9b\u793e\u4f1a\u4e2d\uff0cCS \u5728\u975e\u6b63\u5f0f\u548c\u6b63\u5f0f\u573a\u5408\u90fd\u5f88\u5e38\u89c1\u3002\u52a0\u6cf0\u7f57\u5c3c\u4e9a\u8bed-\u897f\u73ed\u7259\u8bed CS \u662f\u4e00\u4e2a\u5173\u952e\u7684\u4f8b\u5b50\uff0c\u56e0\u4e3a\u5b83\u5e7f\u6cdb\u5e94\u7528\u4e8e\u5a92\u4f53\u548c\u8bae\u4f1a\u6f14\u8bb2\u4e2d\u3002\u56e0\u6b64\uff0c\u6709\u5fc5\u8981\u6539\u8fdb ASR \u7cfb\u7edf\u4ee5\u6709\u6548\u5904\u7406 CS\u3002", "method": "\u672c\u7814\u7a76\u901a\u8fc7\u63a2\u7d22\u4e09\u79cd\u7b56\u7565\u6765\u6539\u8fdb\u52a0\u6cf0\u7f57\u5c3c\u4e9a\u8bed-\u897f\u73ed\u7259\u8bed\u4ee3\u7801\u8f6c\u6362\u7684\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\uff1a(1) \u751f\u6210\u5408\u6210\u4ee3\u7801\u8f6c\u6362\u6570\u636e\uff0c(2) \u8fde\u63a5\u5355\u8bed\u97f3\u9891\uff0c\u4ee5\u53ca (3) \u5229\u7528\u5e26\u8bed\u8a00\u4ee4\u724c\u7684\u771f\u5b9e\u4ee3\u7801\u8f6c\u6362\u6570\u636e\u3002\u7814\u7a76\u4eba\u5458\u4ece\u52a0\u6cf0\u7f57\u5c3c\u4e9a\u8bed\u8bed\u97f3\u8bed\u6599\u5e93\u4e2d\u63d0\u53d6\u4ee3\u7801\u8f6c\u6362\u6570\u636e\uff0c\u5e76\u5bf9 OpenAI \u7684 Whisper \u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u4f7f\u5176\u5728 Hugging Face \u4e0a\u53ef\u7528\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5c06\u9002\u91cf\u7684\u5408\u6210 CS \u6570\u636e\u4e0e\u4f18\u52bf\u8bed\u8a00\u4ee4\u724c\u76f8\u7ed3\u5408\uff0c\u53ef\u4ee5\u5b9e\u73b0\u6700\u4f73\u7684\u8f6c\u5f55\u6027\u80fd\u3002", "conclusion": "\u7ed3\u5408\u9002\u91cf\u7684\u5408\u6210\u4ee3\u7801\u8f6c\u6362\u6570\u636e\u548c\u4f18\u52bf\u8bed\u8a00\u4ee4\u724c\u53ef\u5b9e\u73b0\u6700\u4f73\u7684\u8f6c\u5f55\u6027\u80fd\u3002"}}
{"id": "2507.13595", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13595", "abs": "https://arxiv.org/abs/2507.13595", "authors": ["Tengkai Wang", "Weihao Li", "Ruikai Cui", "Shi Qiu", "Nick Barnes"], "title": "NoiseSDF2NoiseSDF: Learning Clean Neural Fields from Noisy Supervision", "comment": "14 pages, 4 figures", "summary": "Reconstructing accurate implicit surface representations from point clouds\nremains a challenging task, particularly when data is captured using\nlow-quality scanning devices. These point clouds often contain substantial\nnoise, leading to inaccurate surface reconstructions. Inspired by the\nNoise2Noise paradigm for 2D images, we introduce NoiseSDF2NoiseSDF, a novel\nmethod designed to extend this concept to 3D neural fields. Our approach\nenables learning clean neural SDFs directly from noisy point clouds through\nnoisy supervision by minimizing the MSE loss between noisy SDF representations,\nallowing the network to implicitly denoise and refine surface estimations. We\nevaluate the effectiveness of NoiseSDF2NoiseSDF on benchmarks, including the\nShapeNet, ABC, Famous, and Real datasets. Experimental results demonstrate that\nour framework significantly improves surface reconstruction quality from noisy\ninputs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aNoiseSDF2NoiseSDF\u7684\u65b0\u9896\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u566a\u58f0\u70b9\u4e91\u4e2d\u5b66\u4e60\u5e72\u51c0\u76843D\u795e\u7ecfSDF\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u6700\u5c0f\u5316\u566a\u58f0SDF\u8868\u793a\u4e4b\u95f4\u7684\u5747\u65b9\u8bef\u5dee\u635f\u5931\u6765\u5b9e\u73b0\u53bb\u566a\u548c\u8868\u9762\u7cbe\u70bc\u3002", "motivation": "\u4ece\u70b9\u4e91\u4e2d\u91cd\u5efa\u7cbe\u786e\u7684\u9690\u5f0f\u8868\u9762\u8868\u793a\uff0c\u5c24\u5176\u662f\u5728\u4f7f\u7528\u4f4e\u8d28\u91cf\u626b\u63cf\u8bbe\u5907\u6355\u83b7\u7684\u6570\u636e\u65f6\uff0c\u4ecd\u7136\u662f\u4e00\u9879\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u3002\u8fd9\u4e9b\u70b9\u4e91\u901a\u5e38\u5305\u542b\u5927\u91cf\u566a\u58f0\uff0c\u5bfc\u81f4\u8868\u9762\u91cd\u5efa\u4e0d\u51c6\u786e\u3002\u53d72D\u56fe\u50cf\u566a\u58f02\u566a\u58f0\u8303\u4f8b\u7684\u542f\u53d1\uff0c\u6211\u4eec\u65e8\u5728\u5c06\u6b64\u6982\u5ff5\u6269\u5c55\u52303D\u795e\u7ecf\u573a\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aNoiseSDF2NoiseSDF\u7684\u65b0\u9896\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u6700\u5c0f\u5316\u566a\u58f0SDF\u8868\u793a\u4e4b\u95f4\u7684\u5747\u65b9\u8bef\u5dee\u635f\u5931\uff0c\u76f4\u63a5\u4ece\u566a\u58f0\u70b9\u4e91\u4e2d\u5b66\u4e60\u5e72\u51c0\u7684\u795e\u7ecfSDF\uff0c\u4ece\u800c\u5b9e\u73b0\u9690\u5f0f\u53bb\u566a\u548c\u8868\u9762\u4f30\u8ba1\u7684\u7cbe\u70bc\u3002", "result": "NoiseSDF2NoiseSDF\u80fd\u591f\u901a\u8fc7\u566a\u58f0\u76d1\u7763\u76f4\u63a5\u4ece\u566a\u58f0\u70b9\u4e91\u4e2d\u5b66\u4e60\u5e72\u51c0\u7684\u795e\u7ecfSDF\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728ShapeNet\u3001ABC\u3001Famous\u548cReal\u6570\u636e\u96c6\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5f97\u5230\u4e86\u9a8c\u8bc1\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4e86\u4ece\u5608\u6742\u8f93\u5165\u8fdb\u884c\u8868\u9762\u91cd\u5efa\u7684\u8d28\u91cf\u3002"}}
{"id": "2507.13881", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.13881", "abs": "https://arxiv.org/abs/2507.13881", "authors": ["Cole Walsh", "Rodica Ivan", "Muhammad Zafar Iqbal", "Colleen Robb"], "title": "Using LLMs to identify features of personal and professional skills in an open-response situational judgment test", "comment": "10 pages, 2 figures, 4 tables; this work was accepted for\n  presentation at the 2025 Artificial Intelligence in Measurement and Education\n  Conference in Pittsburgh, Pennsylvania, United States", "summary": "Academic programs are increasingly recognizing the importance of personal and\nprofessional skills and their critical role alongside technical expertise in\npreparing students for future success in diverse career paths. With this\ngrowing demand comes the need for scalable systems to measure, evaluate, and\ndevelop these skills. Situational Judgment Tests (SJTs) offer one potential\navenue for measuring these skills in a standardized and reliable way, but\nopen-response SJTs have traditionally relied on trained human raters for\nevaluation, presenting operational challenges to delivering SJTs at scale. Past\nattempts at developing NLP-based scoring systems for SJTs have fallen short due\nto issues with construct validity of these systems. In this article, we explore\na novel approach to extracting construct-relevant features from SJT responses\nusing large language models (LLMs). We use the Casper SJT to demonstrate the\nefficacy of this approach. This study sets the foundation for future\ndevelopments in automated scoring for personal and professional skills.", "AI": {"tldr": "\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u4ece\u60c5\u5883\u5224\u65ad\u6d4b\u8bd5 (SJT) \u56de\u5e94\u4e2d\u63d0\u53d6\u4e0e\u7ed3\u6784\u76f8\u5173\u7684\u7279\u5f81\uff0c\u4ee5\u5b9e\u73b0\u4e2a\u4eba\u548c\u4e13\u4e1a\u6280\u80fd\u7684\u53ef\u6269\u5c55\u8bc4\u4f30\u3002", "motivation": "\u968f\u7740\u5bf9\u4e2a\u4eba\u548c\u4e13\u4e1a\u6280\u80fd\u65e5\u76ca\u589e\u957f\u7684\u9700\u6c42\uff0c\u4ee5\u53ca\u8861\u91cf\u3001\u8bc4\u4f30\u548c\u53d1\u5c55\u8fd9\u4e9b\u6280\u80fd\u7684\u53ef\u6269\u5c55\u7cfb\u7edf\u7684\u9700\u6c42\u3002", "method": "\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ece SJT \u56de\u5e94\u4e2d\u63d0\u53d6\u4e0e\u7ed3\u6784\u76f8\u5173\u7684\u7279\u5f81\u3002", "result": "\u63a2\u7d22\u4e86\u4e00\u79cd\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ece SJT \u56de\u5e94\u4e2d\u63d0\u53d6\u4e0e\u7ed3\u6784\u76f8\u5173\u7684\u7279\u5f81\u7684\u65b0\u9896\u65b9\u6cd5\uff0c\u5e76\u4f7f\u7528 Casper SJT \u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u4e2a\u4eba\u548c\u4e13\u4e1a\u6280\u80fd\u7684\u81ea\u52a8\u5316\u8bc4\u5206\u7684\u672a\u6765\u53d1\u5c55\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.14127", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2507.14127", "abs": "https://arxiv.org/abs/2507.14127", "authors": ["M. Isabel Franco Garrido", "Alexander M. Dalzell", "Sam McArdle"], "title": "Quantum and classical algorithms for SOCP based on the multiplicative weights update method", "comment": null, "summary": "We give classical and quantum algorithms for approximately solving\nsecond-order cone programs (SOCPs) based on the multiplicative weights (MW)\nupdate method. Our approach follows the MW framework previously applied to\nsemidefinite programs (SDPs), of which SOCP is a special case. We show that the\nadditional structure of SOCPs can be exploited to give better runtime with\nSOCP-specific algorithms. For an SOCP with $m$ linear constraints over $n$\nvariables partitioned into $r \\leq n$ second-order cones, our quantum algorithm\nrequires $\\widetilde{O}(\\sqrt{r}\\gamma^5 + \\sqrt{m}\\gamma^4)$ (coherent)\nqueries to the underlying data defining the instance, where $\\gamma$ is a\nscale-invariant parameter proportional to the inverse precision. This nearly\nmatches the complexity of solving linear programs (LPs), which are a less\nexpressive subset of SOCP. It also outperforms (especially if $n \\gg r$) the\nnaive approach that applies existing SDP algorithms onto SOCPs, which has\ncomplexity $\\widetilde{O}(\\gamma^{4}(n + \\gamma \\sqrt{n} + \\sqrt{m}))$. Our\nclassical algorithm for SOCP has complexity $\\widetilde{O}(n\\gamma^4 + m\n\\gamma^6)$ in the sample-and-query model.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e58\u6cd5\u6743\u91cd\u66f4\u65b0\u65b9\u6cd5\u7684\u6c42\u89e3\u4e8c\u9636\u9525\u89c4\u5212\uff08SOCP\uff09\u7684\u7ecf\u5178\u548c\u91cf\u5b50\u7b97\u6cd5\u3002\u91cf\u5b50\u7b97\u6cd5\u7684\u67e5\u8be2\u590d\u6742\u5ea6\u63a5\u8fd1\u4e8e\u7ebf\u6027\u89c4\u5212\uff08LP\uff09\u7684\u590d\u6742\u5ea6\uff0c\u4f18\u4e8e\u73b0\u6709\u7684SDP\u7b97\u6cd5\u3002\u7ecf\u5178\u7b97\u6cd5\u5728\u6837\u672c\u548c\u67e5\u8be2\u6a21\u578b\u4e2d\u7684\u590d\u6742\u5ea6\u4e5f\u5f97\u5230\u4e86\u5206\u6790\u3002", "motivation": "\u8be5\u7814\u7a76\u7684\u52a8\u673a\u662f\u5229\u7528SOCP\u7684\u7ed3\u6784\u6765\u6539\u8fdb\u6c42\u89e3\u7b97\u6cd5\u7684\u6027\u80fd\uff0c\u5e76\u4e0e\u73b0\u6709\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\u3002", "method": "\u7814\u7a76\u9075\u5faa\u4e86\u5148\u524d\u5e94\u7528\u4e8e\u534adefinit\u89c4\u5212\uff08SDP\uff09\u7684MW\u6846\u67b6\uff0cSOCP\u662fSDP\u7684\u7279\u4f8b\u3002\u7814\u7a76\u8868\u660e\uff0cSOCP\u7684\u9644\u52a0\u7ed3\u6784\u53ef\u4ee5\u88ab\u5229\u7528\u6765\u901a\u8fc7SOCP\u7279\u5b9a\u7684\u7b97\u6cd5\u63d0\u4f9b\u66f4\u597d\u7684\u8fd0\u884c\u65f6\u6027\u80fd\u3002", "result": "\u91cf\u5b50\u7b97\u6cd5\u5728\u76f8\u5e72\u67e5\u8be2\u65b9\u9762\u590d\u6742\u5ea6\u63a5\u8fd1\u4e8e\u89e3\u51b3\u7ebf\u6027\u89c4\u5212\uff08LP\uff09\u7684\u590d\u6742\u5ea6\uff0c\u5e76\u4e14\u4f18\u4e8e\u5c06\u73b0\u6709SDP\u7b97\u6cd5\u5e94\u7528\u4e8eSOCP\u7684\u65b9\u6cd5\u3002\u7ecf\u5178\u7b97\u6cd5\u5728\u6837\u672c\u548c\u67e5\u8be2\u6a21\u578b\u4e2d\u7684\u590d\u6742\u5ea6\u4e3a$\\\\", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u6c42\u89e3\u4e8c\u9636\u9525\u89c4\u5212\uff08SOCP\uff09\u63d0\u4f9b\u4e86\u7ecf\u5178\u7684\u548c\u91cf\u5b50\u7684\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u57fa\u4e8e\u4e58\u6cd5\u6743\u91cd\uff08MW\uff09\u66f4\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2507.13741", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13741", "abs": "https://arxiv.org/abs/2507.13741", "authors": ["Shangyou Wang", "Zezhong Ding", "Xike Xie"], "title": "SamGoG: A Sampling-Based Graph-of-Graphs Framework for Imbalanced Graph Classification", "comment": null, "summary": "Graph Neural Networks (GNNs) have shown remarkable success in graph\nclassification tasks by capturing both structural and feature-based\nrepresentations. However, real-world graphs often exhibit two critical forms of\nimbalance: class imbalance and graph size imbalance. These imbalances can bias\nthe learning process and degrade model performance. Existing methods typically\naddress only one type of imbalance or incur high computational costs. In this\nwork, we propose SamGoG, a sampling-based Graph-of-Graphs (GoG) learning\nframework that effectively mitigates both class and graph size imbalance.\nSamGoG constructs multiple GoGs through an efficient importance-based sampling\nmechanism and trains on them sequentially. This sampling mechanism incorporates\nthe learnable pairwise similarity and adaptive GoG node degree to enhance edge\nhomophily, thus improving downstream model quality. SamGoG can seamlessly\nintegrate with various downstream GNNs, enabling their efficient adaptation for\ngraph classification tasks. Extensive experiments on benchmark datasets\ndemonstrate that SamGoG achieves state-of-the-art performance with up to a\n15.66% accuracy improvement with 6.7$\\times$ training acceleration.", "AI": {"tldr": "SamGoG\u662f\u4e00\u79cd\u57fa\u4e8e\u91c7\u6837\u7684\u56fe\u56fe\uff08GoG\uff09\u5b66\u4e60\u6846\u67b6\uff0c\u53ef\u4ee5\u89e3\u51b3\u56fe\u5206\u7c7b\u4e2d\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u56fe\u5927\u5c0f\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u540c\u65f6\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\u548c\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u7684\u56fe\u6570\u636e\u5e38\u5e38\u5b58\u5728\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u56fe\u5927\u5c0f\u4e0d\u5e73\u8861\u8fd9\u4e24\u79cd\u5173\u952e\u5f62\u5f0f\u7684\u4e0d\u5e73\u8861\uff0c\u8fd9\u4f1a\u5f71\u54cd\u5b66\u4e60\u8fc7\u7a0b\u5e76\u964d\u4f4e\u6a21\u578b\u6027\u80fd\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u53ea\u89e3\u51b3\u5176\u4e2d\u4e00\u79cd\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u6216\u8005\u9700\u8981\u9ad8\u6602\u7684\u8ba1\u7b97\u6210\u672c\u3002", "method": "SamGoG\u901a\u8fc7\u4e00\u79cd\u9ad8\u6548\u7684\u57fa\u4e8e\u91cd\u8981\u6027\u7684\u91c7\u6837\u673a\u5236\u6784\u5efa\u591a\u4e2a\u56fe\u56fe\uff08GoG\uff09\uff0c\u5e76\u8fdb\u884c\u987a\u5e8f\u8bad\u7ec3\u3002\u8be5\u91c7\u6837\u673a\u5236\u7ed3\u5408\u4e86\u53ef\u5b66\u4e60\u7684\u6210\u5bf9\u76f8\u4f3c\u6027\u548c\u81ea\u9002\u5e94\u7684GoG\u8282\u70b9\u5ea6\uff0c\u4ee5\u589e\u5f3a\u8fb9\u540c\u8d28\u6027\uff0c\u4ece\u800c\u63d0\u9ad8\u4e0b\u6e38\u6a21\u578b\u7684\u8d28\u91cf\u3002", "result": "SamGoG\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u51c6\u786e\u7387\u6700\u9ad8\u63d0\u9ad8\u4e8615.66%\uff0c\u540c\u65f6\u8bad\u7ec3\u901f\u5ea6\u52a0\u5feb\u4e866.7\u500d\u3002", "conclusion": "SamGoG\u901a\u8fc7\u4e00\u79cd\u9ad8\u6548\u7684\u57fa\u4e8e\u91c7\u6837\u7684\u56fe\u56fe\uff08GoG\uff09\u5b66\u4e60\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u7f13\u89e3\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u56fe\u5927\u5c0f\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5e76\u80fd\u4e0e\u5404\u79cd\u4e0b\u6e38GNN\u65e0\u7f1d\u96c6\u6210\uff0c\u4ece\u800c\u5728\u56fe\u5206\u7c7b\u4efb\u52a1\u4e2d\u5b9e\u73b0\u9ad8\u6548\u9002\u5e94\u3002"}}
{"id": "2507.13599", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13599", "abs": "https://arxiv.org/abs/2507.13599", "authors": ["Chengxu Liu", "Lu Qi", "Jinshan Pan", "Xueming Qian", "Ming-Hsuan Yang"], "title": "Learning Deblurring Texture Prior from Unpaired Data with Diffusion Model", "comment": "Accepted by ICCV2025", "summary": "Since acquiring large amounts of realistic blurry-sharp image pairs is\ndifficult and expensive, learning blind image deblurring from unpaired data is\na more practical and promising solution. Unfortunately, dominant approaches\nrely heavily on adversarial learning to bridge the gap from blurry domains to\nsharp domains, ignoring the complex and unpredictable nature of real-world blur\npatterns. In this paper, we propose a novel diffusion model (DM)-based\nframework, dubbed \\ours, for image deblurring by learning spatially varying\ntexture prior from unpaired data. In particular, \\ours performs DM to generate\nthe prior knowledge that aids in recovering the textures of blurry images. To\nimplement this, we propose a Texture Prior Encoder (TPE) that introduces a\nmemory mechanism to represent the image textures and provides supervision for\nDM training. To fully exploit the generated texture priors, we present the\nTexture Transfer Transformer layer (TTformer), in which a novel\nFilter-Modulated Multi-head Self-Attention (FM-MSA) efficiently removes\nspatially varying blurring through adaptive filtering. Furthermore, we\nimplement a wavelet-based adversarial loss to preserve high-frequency texture\ndetails. Extensive evaluations show that \\ours provides a promising\nunsupervised deblurring solution and outperforms SOTA methods in widely-used\nbenchmarks.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a \\thoughs \u7684\u65b0\u9896\u6269\u6563\u6a21\u578b\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u975e\u914d\u5bf9\u6570\u636e\u4e2d\u5b66\u4e60\u56fe\u50cf\u53bb\u6a21\u7cca\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u5229\u7528\u7a7a\u95f4\u53d8\u5316\u7684\u7eb9\u7406\u5148\u9a8c\uff0c\u5e76\u5f15\u5165\u7eb9\u7406\u5148\u9a8c\u7f16\u7801\u5668\uff08TPE\uff09\u548c\u7eb9\u7406\u4f20\u9012 Transformer \u5c42\uff08TTformer\uff09\u6765\u6709\u6548\u5904\u7406\u590d\u6742\u548c\u4e0d\u53ef\u9884\u6d4b\u7684\u6a21\u7cca\u6a21\u5f0f\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0c\\thoughs \u5728\u65e0\u76d1\u7763\u53bb\u6a21\u7cca\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u7531\u4e8e\u83b7\u53d6\u5927\u91cf\u771f\u5b9e\u7684\u6a21\u7cca-\u6e05\u6670\u56fe\u50cf\u5bf9\u65e2\u56f0\u96be\u53c8\u6602\u8d35\uff0c\u56e0\u6b64\u4ece\u975e\u914d\u5bf9\u6570\u636e\u4e2d\u5b66\u4e60\u76f2\u56fe\u50cf\u53bb\u6a21\u7cca\u662f\u4e00\u79cd\u66f4\u5b9e\u7528\u3001\u66f4\u6709\u524d\u9014\u7684\u89e3\u51b3\u65b9\u6848\u3002\u4e0d\u5e78\u7684\u662f\uff0c\u4e3b\u5bfc\u65b9\u6cd5\u4e25\u91cd\u4f9d\u8d56\u5bf9\u6297\u6027\u5b66\u4e60\u6765\u5f25\u5408\u4ece\u6a21\u7cca\u57df\u5230\u6e05\u6670\u57df\u7684\u5dee\u8ddd\uff0c\u800c\u5ffd\u7565\u4e86\u771f\u5b9e\u4e16\u754c\u6a21\u7cca\u6a21\u5f0f\u7684\u590d\u6742\u548c\u4e0d\u53ef\u9884\u6d4b\u7684\u6027\u8d28\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u6269\u6563\u6a21\u578b\uff08DM\uff09\u7684\u6846\u67b6\uff08\thoughs\uff09\uff0c\u901a\u8fc7\u5b66\u4e60\u6765\u81ea\u975e\u914d\u5bf9\u6570\u636e\u7684\u7a7a\u95f4\u53d8\u5316\u7eb9\u7406\u5148\u9a8c\u6765\u8fdb\u884c\u56fe\u50cf\u53bb\u6a21\u7cca\u3002\u5177\u4f53\u6765\u8bf4\uff0c\thoughs \u6267\u884c DM \u6765\u751f\u6210\u6709\u52a9\u4e8e\u6062\u590d\u6a21\u7cca\u56fe\u50cf\u7eb9\u7406\u7684\u5148\u9a8c\u77e5\u8bc6\u3002\u4e3a\u4e86\u5b9e\u73b0\u8fd9\u4e00\u70b9\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u7eb9\u7406\u5148\u9a8c\u7f16\u7801\u5668\uff08TPE\uff09\uff0c\u5b83\u5f15\u5165\u4e86\u4e00\u4e2a\u5185\u5b58\u673a\u5236\u6765\u8868\u793a\u56fe\u50cf\u7eb9\u7406\u5e76\u4e3a DM \u8bad\u7ec3\u63d0\u4f9b\u76d1\u7763\u3002\u4e3a\u4e86\u5145\u5206\u5229\u7528\u751f\u6210\u7684\u7eb9\u7406\u5148\u9a8c\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u7eb9\u7406\u4f20\u9012 Transformer \u5c42\uff08TTformer\uff09\uff0c\u5176\u4e2d\u65b0\u9896\u7684\u6ee4\u6ce2\u5668\u8c03\u5236\u591a\u5934\u81ea\u6ce8\u610f\u529b\uff08FM-MSA\uff09\u901a\u8fc7\u81ea\u9002\u5e94\u6ee4\u6ce2\u6709\u6548\u5730\u53bb\u9664\u7a7a\u95f4\u53d8\u5316\u7684\u6a21\u7cca\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5b9e\u73b0\u4e86\u4e00\u79cd\u57fa\u4e8e\u5c0f\u6ce2\u7684\u5bf9\u6297\u6027\u635f\u5931\u6765\u4fdd\u7559\u9ad8\u9891\u7eb9\u7406\u7ec6\u8282\u3002", "result": "\u901a\u8fc7\u5728\u5e7f\u6cdb\u4f7f\u7528\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u8bc4\u4f30\uff0c\u8868\u660e \thoughs \u63d0\u4f9b\u4e86\u6709\u524d\u9014\u7684\u65e0\u76d1\u7763\u53bb\u6a21\u7cca\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5728\u5e7f\u6cdb\u4f7f\u7528\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6a21\u578b\u5728\u5e7f\u6cdb\u4f7f\u7528\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u5e76\u4e3a\u65e0\u76d1\u7763\u53bb\u6a21\u7cca\u63d0\u4f9b\u4e86\u6709\u524d\u9014\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.13913", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13913", "abs": "https://arxiv.org/abs/2507.13913", "authors": ["Matous Volf", "Jakub Simko"], "title": "Political Leaning and Politicalness Classification of Texts", "comment": null, "summary": "This paper addresses the challenge of automatically classifying text\naccording to political leaning and politicalness using transformer models. We\ncompose a comprehensive overview of existing datasets and models for these\ntasks, finding that current approaches create siloed solutions that perform\npoorly on out-of-distribution texts. To address this limitation, we compile a\ndiverse dataset by combining 12 datasets for political leaning classification\nand creating a new dataset for politicalness by extending 18 existing datasets\nwith the appropriate label. Through extensive benchmarking with leave-one-in\nand leave-one-out methodologies, we evaluate the performance of existing models\nand train new ones with enhanced generalization capabilities.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u6574\u5408\u548c\u6269\u5c55\u73b0\u6709\u6570\u636e\u96c6\uff0c\u5e76\u91c7\u7528\u66f4\u4f18\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u653f\u6cbb\u6587\u672c\u5206\u7c7b\u6a21\u578b\u5728\u4e0d\u540c\u7c7b\u578b\u6587\u672c\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u81ea\u52a8\u8bc6\u522b\u6587\u672c\u7684\u653f\u6cbb\u503e\u5411\u548c\u653f\u6cbb\u6027\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u5206\u5e03\u5916\u6587\u672c\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u9650\u5236\u4e86\u5176\u5e94\u7528\u8303\u56f4\u3002", "method": "\u672c\u7814\u7a76\u6574\u5408\u4e8612\u4e2a\u7528\u4e8e\u653f\u6cbb\u503e\u5411\u5206\u7c7b\u7684\u6570\u636e\u96c6\uff0c\u5e76\u6269\u5c55\u4e8618\u4e2a\u73b0\u6709\u6570\u636e\u96c6\u4ee5\u521b\u5efa\u653f\u6cbb\u6027\u5206\u7c7b\u7684\u65b0\u6570\u636e\u96c6\u3002\u901a\u8fc7\u7559\u4e00\u6cd5\u548c\u7559\u4e00\u6cd5\u6392\u9664\u6cd5\u7684\u7efc\u5408\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u4e86\u73b0\u6709\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5e76\u8bad\u7ec3\u4e86\u5177\u6709\u589e\u5f3a\u6cdb\u5316\u80fd\u529b\u7684\u65b0\u6a21\u578b\u3002", "result": "\u672c\u7814\u7a76\u901a\u8fc7\u6784\u5efa\u591a\u6837\u5316\u7684\u6570\u636e\u96c6\u548c\u91c7\u7528\u6539\u8fdb\u7684\u6a21\u578b\u8bad\u7ec3\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u653f\u6cbb\u503e\u5411\u548c\u653f\u6cbb\u6027\u6587\u672c\u5206\u7c7b\u5728\u5206\u5e03\u5916\u6587\u672c\u4e0a\u7684\u6027\u80fd\u3002", "conclusion": "\u73b0\u6709\u7684\u653f\u6cbb\u503e\u5411\u548c\u653f\u6cbb\u6027\u6587\u672c\u5206\u7c7b\u65b9\u6cd5\u6784\u5efa\u4e86\u5b64\u7acb\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u5206\u5e03\u5916\u6587\u672c\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002\u672c\u7814\u7a76\u901a\u8fc7\u6574\u5408\u73b0\u6709\u6570\u636e\u96c6\u548c\u8bad\u7ec3\u65b0\u6a21\u578b\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u4ee5\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2507.14128", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2507.14128", "abs": "https://arxiv.org/abs/2507.14128", "authors": ["Avi Kaufman", "James Corona", "Zane Ozzello", "Blake Senseman", "Muhammad Asaduzzaman", "Yannick Meurice"], "title": "Exploring near critical lattice gauge simulators with Rydberg atoms facilities", "comment": "23 pages, 26 figures, revtex4-2", "summary": "We motivate the use of a ladder of Rydberg atoms as an analog simulator for a\nlattice gauge theory version of scalar electrodynamics also called the compact\nAbelian Higgs model. We demonstrate that by using a few thousand shots from a\nsingle copy of the ladder simulator it is possible to estimate the bipartite\nquantum von Neumann entanglement entropy $S^{vN}_A$. The estimation relies on\nan optimized filtration of the mutual information associated with the\nbitstrings obtained from public facilities of configurable Rydberg arrays named\nAquila. We discuss the limitations associated with finite sampling, sorting\nfidelity, adiabatic preparation, ramp-down of the Rabi frequency before\nmeasurement, and readout errors. We use cumulative probability distribution to\ncompare Aquila results with high accuracy density matrix renormalization group\n(DMRG) or exact results. The state preparation appears to be the main source of\nerror. We discuss the large volume behavior of the cumulative probability\ndistribution and show examples where for a finite number of shots, there\nappears to be some large enough size for which any given state is seen at most\nonce with high probability. We show that the results presented can be extended\nto multipartite entanglement. We briefly discuss the cost of the calculations\nfor large square arrays in the context of obtaining quantum advantage in the\nnear future.", "AI": {"tldr": "\u91d1\u4f2f\u683c\u539f\u5b50\u68af\u5b50\u53ef\u7528\u4e8e\u6a21\u62df\u6807\u91cf\u7535\u52a8\u529b\u5b66\uff0c\u5e76\u901a\u8fc7\u7279\u5b9a\u65b9\u6cd5\u4f30\u7b97\u91cf\u5b50\u7ea0\u7f20\u71b5\uff0c\u5c3d\u7ba1\u5b58\u5728\u4e00\u4e9b\u8bef\u5dee\u6765\u6e90\u3002", "motivation": "\u7814\u7a76\u91d1\u4f2f\u683c\u539f\u5b50\u68af\u5b50\u4f5c\u4e3a\u6807\u91cf\u7535\u52a8\u529b\u5b66\uff08\u7d27\u81f4\u963f\u8d1d\u5c14\u5e0c\u683c\u65af\u6a21\u578b\uff09\u7684\u683c\u70b9\u89c4\u8303\u7406\u8bba\u7684\u6a21\u62df\u5668\u7684\u6f5c\u529b\uff0c\u5e76\u4f30\u7b97\u5176\u91cf\u5b50\u7ea0\u7f20\u71b5\u3002", "method": "\u901a\u8fc7\u4f18\u5316Mutual Information\u7684\u6ee4\u6ce2\u6765\u4f30\u7b97\u91cf\u5b50\u7ea0\u7f20\u71b5\uff0c\u5e76\u4f7f\u7528\u7d2f\u79ef\u6982\u7387\u5206\u5e03\u5c06\u6a21\u62df\u7ed3\u679c\u4e0eDMRG\u6216\u7cbe\u786e\u7ed3\u679c\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u53ef\u4ee5\u4f7f\u7528\u5c11\u6570\u51e0\u5343\u6b21\u6d4b\u91cf\u6765\u4f30\u7b97\u91cf\u5b50\u7ea0\u7f20\u71b5\u3002\u72b6\u6001\u5236\u5907\u662f\u4e3b\u8981\u7684\u8bef\u5dee\u6765\u6e90\u3002\u7814\u7a76\u4e86\u7d2f\u79ef\u6982\u7387\u5206\u5e03\u7684\u5927\u4f53\u79ef\u884c\u4e3a\uff0c\u5e76\u8ba8\u8bba\u4e86\u5728\u672a\u6765\u83b7\u5f97\u91cf\u5b50\u4f18\u52bf\u7684\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u4f7f\u7528\u91d1\u4f2f\u683c\u539f\u5b50\u68af\u5b50\u4f5c\u4e3a\u6807\u91cf\u7535\u52a8\u529b\u5b66\uff08\u4e5f\u79f0\u4e3a\u7d27\u81f4\u963f\u8d1d\u5c14\u5e0c\u683c\u65af\u6a21\u578b\uff09\u7684\u683c\u70b9\u89c4\u8303\u7406\u8bba\u7684\u6a21\u62df\u5668\u662f\u53ef\u884c\u7684\uff0c\u5e76\u4e14\u53ef\u4ee5\u901a\u8fc7\u4f18\u5316Mutual Information\u7684\u6ee4\u6ce2\u6765\u4f30\u7b97\u91cf\u5b50\u7ea0\u7f20\u71b5\u3002"}}
{"id": "2507.13742", "categories": ["cs.LG", "cs.AI", "math.OC"], "pdf": "https://arxiv.org/pdf/2507.13742", "abs": "https://arxiv.org/abs/2507.13742", "authors": ["Oussama Bouaggad", "Natalia Grabar"], "title": "Search-Optimized Quantization in Biomedical Ontology Alignment", "comment": null, "summary": "In the fast-moving world of AI, as organizations and researchers develop more\nadvanced models, they face challenges due to their sheer size and computational\ndemands. Deploying such models on edge devices or in resource-constrained\nenvironments adds further challenges related to energy consumption, memory\nusage and latency. To address these challenges, emerging trends are shaping the\nfuture of efficient model optimization techniques. From this premise, by\nemploying supervised state-of-the-art transformer-based models, this research\nintroduces a systematic method for ontology alignment, grounded in cosine-based\nsemantic similarity between a biomedical layman vocabulary and the Unified\nMedical Language System (UMLS) Metathesaurus. It leverages Microsoft Olive to\nsearch for target optimizations among different Execution Providers (EPs) using\nthe ONNX Runtime backend, followed by an assembled process of dynamic\nquantization employing Intel Neural Compressor and IPEX (Intel Extension for\nPyTorch). Through our optimization process, we conduct extensive assessments on\nthe two tasks from the DEFT 2020 Evaluation Campaign, achieving a new\nstate-of-the-art in both. We retain performance metrics intact, while attaining\nan average inference speed-up of 20x and reducing memory usage by approximately\n70%.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u6a21\u578b\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u96c6\u6210\u5148\u8fdb\u7684Transformer\u6a21\u578b\u3001Microsoft Olive\u548cIntel\u4f18\u5316\u5de5\u5177\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u63a8\u7406\u901f\u5ea6\u5e76\u964d\u4f4e\u4e86\u5185\u5b58\u6d88\u8017\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684AI\u6a21\u578b\u90e8\u7f72\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u5728\u4eba\u5de5\u667a\u80fd\u9886\u57df\uff0c\u968f\u7740\u6a21\u578b\u89c4\u6a21\u548c\u8ba1\u7b97\u9700\u6c42\u7684\u4e0d\u65ad\u589e\u957f\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\uff08\u5982\u8fb9\u7f18\u8bbe\u5907\uff09\u4e2d\u90e8\u7f72\u8fd9\u4e9b\u6a21\u578b\u9762\u4e34\u7740\u80fd\u8017\u3001\u5185\u5b58\u4f7f\u7528\u548c\u5ef6\u8fdf\u7b49\u4e25\u5cfb\u6311\u6218\u3002\u56e0\u6b64\uff0c\u9700\u8981\u6709\u6548\u7684\u6a21\u578b\u4f18\u5316\u6280\u672f\u6765\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u672c\u6587\u9996\u5148\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u76d1\u7763\u5f0fTransformer\u6a21\u578b\u7684\u672c\u4f53\u5bf9\u9f50\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u8ba1\u7b97\u751f\u7269\u533b\u5b66\u901a\u4fd7\u8bcd\u6c47\u4e0eUMLS\u5143\u6570\u636e\u5e93\u4e4b\u95f4\u7684\u4f59\u5f26\u8bed\u4e49\u76f8\u4f3c\u5ea6\u6765\u5b9e\u73b0\u3002\u7136\u540e\uff0c\u7814\u7a76\u8005\u5229\u7528Microsoft Olive\u5728ONNX Runtime\u540e\u7aef\u4e0a\u641c\u7d22\u4e0d\u540c\u7684\u6267\u884c\u63d0\u4f9b\u5546\uff08EPs\uff09\u4ee5\u5b9e\u73b0\u76ee\u6807\u4f18\u5316\uff0c\u5e76\u7ed3\u5408Intel Neural Compressor\u548cIPEX\u8fdb\u884c\u52a8\u6001\u91cf\u5316\u3002", "result": "\u901a\u8fc7\u4f18\u5316\u8fc7\u7a0b\uff0c\u5728DEFT 2020\u8bc4\u4f30\u6d3b\u52a8\u7684\u4e24\u4e2a\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6210\u679c\uff0c\u5e73\u5747\u63a8\u7406\u901f\u5ea6\u63d0\u9ad8\u4e8620\u500d\uff0c\u5185\u5b58\u4f7f\u7528\u91cf\u51cf\u5c11\u4e86\u7ea670%\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6027\u80fd\u6307\u6807\u4e0d\u53d8\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u5316\u7684\u672c\u4f53\u5bf9\u9f50\u65b9\u6cd5\uff0c\u5229\u7528\u76d1\u7763\u5f0f\u3001\u57fa\u4e8eTransformer\u7684\u6a21\u578b\uff0c\u901a\u8fc7\u751f\u7269\u533b\u5b66\u901a\u4fd7\u8bcd\u6c47\u4e0e\u7edf\u4e00\u533b\u5b66\u8bed\u8a00\u7cfb\u7edf\uff08UMLS\uff09\u5143\u6570\u636e\u5e93\u4e4b\u95f4\u7684\u57fa\u4e8e\u4f59\u5f26\u7684\u8bed\u4e49\u76f8\u4f3c\u6027\u6765\u5b9e\u73b0\u3002\u901a\u8fc7\u4f7f\u7528Microsoft Olive\u548cIntel Neural Compressor\u53caIPEX\uff08Intel Extension for PyTorch\uff09\u8fdb\u884c\u52a8\u6001\u91cf\u5316\uff0c\u5b9e\u73b0\u4e8620\u500d\u7684\u5e73\u5747\u63a8\u7406\u52a0\u901f\u548c\u7ea670%\u7684\u5185\u5b58\u4f7f\u7528\u91cf\u51cf\u5c11\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6027\u80fd\u6307\u6807\u4e0d\u53d8\uff0c\u5e76\u5728DEFT 2020\u8bc4\u4f30\u6d3b\u52a8\u7684\u4e24\u4e2a\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6210\u679c\u3002"}}
{"id": "2507.13607", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13607", "abs": "https://arxiv.org/abs/2507.13607", "authors": ["Kento Kawai", "Takeru Oba", "Kyotaro Tokoro", "Kazutoshi Akita", "Norimichi Ukita"], "title": "Efficient Burst Super-Resolution with One-step Diffusion", "comment": "NTIRE2025", "summary": "While burst Low-Resolution (LR) images are useful for improving their Super\nResolution (SR) image compared to a single LR image, prior burst SR methods are\ntrained in a deterministic manner, which produces a blurry SR image. Since such\nblurry images are perceptually degraded, we aim to reconstruct sharp and\nhigh-fidelity SR images by a diffusion model. Our method improves the\nefficiency of the diffusion model with a stochastic sampler with a high-order\nODE as well as one-step diffusion using knowledge distillation. Our\nexperimental results demonstrate that our method can reduce the runtime to 1.6\n% of its baseline while maintaining the SR quality measured based on image\ndistortion and perceptual quality.", "AI": {"tldr": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u65b9\u6cd5\u901a\u8fc7\u968f\u673a\u91c7\u6837\u5668\u548c\u77e5\u8bc6\u84b8\u998f\uff0c\u5728\u4fdd\u6301\u8d85\u5206\u8fa8\u7387\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u5c06\u8fd0\u884c\u65f6\u95f4\u7f29\u77ed\u4e8698.4%\u3002", "motivation": "\u4e3a\u4e86\u901a\u8fc7\u6269\u6563\u6a21\u578b\u91cd\u5efa\u6e05\u6670\u3001\u9ad8\u4fdd\u771f\u7684\u8d85\u5206\u8fa8\u7387\u56fe\u50cf\uff0c\u56e0\u4e3a\u5148\u524d\u5177\u6709\u786e\u5b9a\u6027\u8bad\u7ec3\u7684\u7a81\u53d1\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u4f1a\u4ea7\u751f\u6a21\u7cca\u7684\u8d85\u5206\u8fa8\u7387\u56fe\u50cf\uff0c\u8fd9\u5728\u611f\u5b98\u4e0a\u662f\u52a3\u5316\u7684\u3002", "method": "\u6211\u4eec\u901a\u8fc7\u968f\u673a\u91c7\u6837\u5668\u548c\u9ad8\u9636\u5e38\u5fae\u5206\u65b9\u7a0b\u4ee5\u53ca\u4f7f\u7528\u77e5\u8bc6\u84b8\u998f\u7684\u4e00\u6b65\u6269\u6563\u6765\u63d0\u9ad8\u6269\u6563\u6a21\u578b\u7684\u6548\u7387\u3002", "result": "\u6211\u4eec\u7684\u65b9\u6cd5\u53ef\u4ee5\u5c06\u8fd0\u884c\u65f6\u95f4\u51cf\u5c11\u5230\u57fa\u7ebf\u76841.6\uff05\uff0c\u540c\u65f6\u4fdd\u6301\u57fa\u4e8e\u56fe\u50cf\u5931\u771f\u548c\u611f\u77e5\u8d28\u91cf\u6d4b\u91cf\u7684SR\u8d28\u91cf\u3002", "conclusion": "\u4e0e\u4fdd\u6301\u8d85\u5206\u8fa8\u7387\uff08SR\uff09\u8d28\u91cf\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u53ef\u4ee5\u5c06\u8fd0\u884c\u65f6\u95f4\u51cf\u5c11\u5230\u57fa\u7ebf\u76841.6\uff05\u3002"}}
{"id": "2507.13919", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.13919", "abs": "https://arxiv.org/abs/2507.13919", "authors": ["Kobi Hackenburg", "Ben M. Tappin", "Luke Hewitt", "Ed Saunders", "Sid Black", "Hause Lin", "Catherine Fist", "Helen Margetts", "David G. Rand", "Christopher Summerfield"], "title": "The Levers of Political Persuasion with Conversational AI", "comment": "19 pages, 4 figures. Our supplementary materials file can be found at\n  https://github.com/kobihackenburg/scaling-conversational-AI", "summary": "There are widespread fears that conversational AI could soon exert\nunprecedented influence over human beliefs. Here, in three large-scale\nexperiments (N=76,977), we deployed 19 LLMs-including some post-trained\nexplicitly for persuasion-to evaluate their persuasiveness on 707 political\nissues. We then checked the factual accuracy of 466,769 resulting LLM claims.\nContrary to popular concerns, we show that the persuasive power of current and\nnear-future AI is likely to stem more from post-training and prompting\nmethods-which boosted persuasiveness by as much as 51% and 27%\nrespectively-than from personalization or increasing model scale. We further\nshow that these methods increased persuasion by exploiting LLMs' unique ability\nto rapidly access and strategically deploy information and that, strikingly,\nwhere they increased AI persuasiveness they also systematically decreased\nfactual accuracy.", "AI": {"tldr": "AI\u7684\u8bf4\u670d\u529b\u4e3b\u8981\u6765\u81ea\u9884\u8bad\u7ec3\u548c\u63d0\u793a\uff0c\u800c\u975e\u4e2a\u6027\u5316\u6216\u89c4\u6a21\uff0c\u4f46\u589e\u5f3a\u8bf4\u670d\u529b\u7684\u540c\u65f6\u4f1a\u727a\u7272\u4e8b\u5b9e\u51c6\u786e\u6027\u3002", "motivation": "\u8bc4\u4f30\u5bf9\u8bdd\u5f0fAI\u5bf9\u4eba\u7c7b\u4fe1\u5ff5\u7684\u6f5c\u5728\u5de8\u5927\u5f71\u54cd\uff0c\u4ee5\u53caAI\u7684\u8bf4\u670d\u529b\u6765\u6e90\u548c\u5bf9\u4e8b\u5b9e\u51c6\u786e\u6027\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u4e09\u9879\u5927\u89c4\u6a21\u5b9e\u9a8c\uff08N=76,977\uff09\uff0c\u90e8\u7f72\u4e8619\u4e2aLLM\uff08\u5305\u62ec\u4e00\u4e9b\u4e13\u95e8\u4e3a\u8bf4\u670d\u800c\u9884\u8bad\u7ec3\u7684\u6a21\u578b\uff09\uff0c\u8bc4\u4f30\u5b83\u4eec\u5728707\u4e2a\u653f\u6cbb\u95ee\u9898\u4e0a\u7684\u8bf4\u670d\u529b\uff0c\u5e76\u68c0\u67e5\u4e86466,769\u4e2a\u7531\u6b64\u4ea7\u751f\u7684LLM\u58f0\u660e\u7684\u4e8b\u5b9e\u51c6\u786e\u6027\u3002", "result": "\u9884\u8bad\u7ec3\u548c\u63d0\u793a\u65b9\u6cd5\u6bd4\u4e2a\u6027\u5316\u6216\u6a21\u578b\u89c4\u6a21\u66f4\u80fd\u589e\u5f3aAI\u7684\u8bf4\u670d\u529b\u3002\u8fd9\u4e9b\u65b9\u6cd5\u901a\u8fc7\u5229\u7528LLM\u7684\u4fe1\u606f\u5904\u7406\u80fd\u529b\u6765\u589e\u5f3a\u8bf4\u670d\u529b\uff0c\u4f46\u4f1a\u727a\u7272\u4e8b\u5b9e\u51c6\u786e\u6027\u3002", "conclusion": "\u4e0e\u666e\u904d\u62c5\u5fe7\u76f8\u53cd\uff0c\u5f53\u524d\u548c\u8fd1\u671fAI\u7684\u8bf4\u670d\u529b\u53ef\u80fd\u66f4\u591a\u5730\u6765\u6e90\u4e8e\u9884\u8bad\u7ec3\u548c\u63d0\u793a\u65b9\u6cd5\uff08\u5206\u522b\u63d0\u9ad8\u4e8651%\u548c27%\uff09\uff0c\u800c\u975e\u4e2a\u6027\u5316\u6216\u6269\u5927\u6a21\u578b\u89c4\u6a21\u3002\u8fd9\u4e9b\u65b9\u6cd5\u901a\u8fc7\u5229\u7528LLM\u5feb\u901f\u8bbf\u95ee\u548c\u6218\u7565\u6027\u90e8\u7f72\u4fe1\u606f\u7684\u80fd\u529b\u6765\u589e\u5f3a\u8bf4\u670d\u529b\uff0c\u5e76\u4e14\u5728\u63d0\u9ad8AI\u8bf4\u670d\u529b\u7684\u540c\u65f6\uff0c\u7cfb\u7edf\u6027\u5730\u964d\u4f4e\u4e86\u4e8b\u5b9e\u51c6\u786e\u6027\u3002"}}
{"id": "2507.14131", "categories": ["quant-ph", "gr-qc"], "pdf": "https://arxiv.org/pdf/2507.14131", "abs": "https://arxiv.org/abs/2507.14131", "authors": ["Julian De Vuyst", "Philipp A. Hoehn", "Artur Tsobanjan"], "title": "On the relation between perspective-neutral, algebraic, and effective quantum reference frames", "comment": "64 pages", "summary": "The framework of internal quantum reference frames (QRFs) constitutes a\nuniversal toolset for dealing with symmetries in quantum theory and has led to\nnew revelations in quantum gravity, gauge theories and foundational physics.\nMultiple approaches have emerged, sometimes differing in scope and the way\nsymmetries are implemented, raising the question as to their relation. Here, we\ninvestigate the relation between three approaches to QRFs for gauge symmetries,\nnamely the effective semiclassical, algebraic, and perspective-neutral (PN)\napproaches. Rather than constructing Hilbert spaces, as the PN approach, the\neffective approach is based on a quantum phase space parametrized by\nexpectation values and fluctuations, while the emphasis of the algebraic\napproach is on the state space of complex linear functionals on a kinematical\nalgebra. Nevertheless, external frame information is treated as gauge in all\nthree formalisms, manifested in constraints on states and algebra. We show that\nthese three approaches are, in fact, equivalent for ideal QRFs, distinguished\nby sharp orientations, which is the previous setting of the first two\napproaches. Our demonstration pertains to single constraints, including\nrelativistic ones, and encompasses QRF changes. In particular, the QRF\ntransformations of the PN framework agree semiclassically with those of the\nolder effective approach, by which it was inspired. As a physical application,\nwe explore the QRF covariance of uncertainties and fluctuations, which turn out\nto be frame-dependent. This is particularly well-suited for the effective and\nalgebraic approaches, for which these quantities form a natural basis. Finally,\nwe pave the way towards extending these two approaches to non-ideal QRFs by\nstudying the projection and gauge-fixing operations of the Page-Wootters\nformalism, built into the PN framework, on algebraic states.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u8bc1\u660e\u4e86\u4e09\u79cd\u91cf\u5b50\u53c2\u8003\u7cfb\uff08QRF\uff09\u65b9\u6cd5\uff08\u6709\u6548\u534a\u7ecf\u5178\u3001\u4ee3\u6570\u3001\u89c6\u89d2\u4e2d\u7acb\uff09\u5728\u5904\u7406\u89c4\u8303\u5bf9\u79f0\u6027\u65f6\u662f\u7b49\u4ef7\u7684\uff0c\u5e76\u5206\u6790\u4e86\u4e0d\u786e\u5b9a\u6027\u548c\u6da8\u843d\u7684\u53c2\u8003\u7cfb\u4f9d\u8d56\u6027\u3002", "motivation": "\u8be5\u7814\u7a76\u7684\u52a8\u673a\u662f\u7406\u89e3\u548c\u7edf\u4e00\u4e09\u79cd\u4e0d\u540c\u7684\u91cf\u5b50\u53c2\u8003\u7cfb\uff08QRF\uff09\u65b9\u6cd5\uff08\u6709\u6548\u534a\u7ecf\u5178\u3001\u4ee3\u6570\u548c\u89c6\u89d2\u4e2d\u7acb\uff09\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5728\u5904\u7406\u91cf\u5b50\u7406\u8bba\u4e2d\u7684\u5bf9\u79f0\u6027\uff0c\u7279\u522b\u662f\u5728\u91cf\u5b50\u5f15\u529b\u3001\u89c4\u8303\u7406\u8bba\u548c\u57fa\u7840\u7269\u7406\u5b66\u7b49\u9886\u57df\u4e2d\uff0c\u63d0\u4f9b\u4e86\u4e0d\u540c\u7684\u89c6\u89d2\u3002\u901a\u8fc7\u9610\u660e\u8fd9\u4e9b\u65b9\u6cd5\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u7279\u522b\u662f\u5b83\u4eec\u5728\u7406\u60f3QRF\u4e0b\u7684\u7b49\u4ef7\u6027\uff0c\u53ef\u4ee5\u4e3a\u91cf\u5b50\u5bf9\u79f0\u6027\u63d0\u4f9b\u4e00\u4e2a\u66f4\u7edf\u4e00\u7684\u7406\u89e3\u6846\u67b6\u3002", "method": "\u8be5\u7814\u7a76\u901a\u8fc7\u6bd4\u8f83\u4e09\u79cd\u4e0d\u540c\u7684QRF\u65b9\u6cd5\uff08\u6709\u6548\u534a\u7ecf\u5178\u3001\u4ee3\u6570\u548c\u89c6\u89d2\u4e2d\u7acb\uff09\u5728\u5904\u7406\u89c4\u8303\u5bf9\u79f0\u6027\u65b9\u9762\u7684\u5f02\u540c\uff0c\u5e76\u8bc1\u660e\u4e86\u5b83\u4eec\u5728\u7406\u60f3QRF\u4e0b\u7684\u7b49\u4ef7\u6027\u3002\u5177\u4f53\u800c\u8a00\uff0c\u8bba\u6587\u5206\u6790\u4e86\u8fd9\u4e9b\u65b9\u6cd5\u5728\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u3001\u91cf\u5b50\u76f8\u7a7a\u95f4\u548c\u72b6\u6001\u7a7a\u95f4\u4e0a\u7684\u5dee\u5f02\uff0c\u4ee5\u53ca\u5b83\u4eec\u5982\u4f55\u5904\u7406\u5916\u90e8\u53c2\u8003\u7cfb\u4fe1\u606f\u3002\u6b64\u5916\uff0c\u8bba\u6587\u8fd8\u63a2\u8ba8\u4e86QRF\u53d8\u6362\u7684\u534a\u7ecf\u5178\u5bf9\u5e94\u5173\u7cfb\uff0c\u5e76\u5e94\u7528QRF\u534f\u65b9\u5dee\u5206\u6790\u4e86\u4e0d\u786e\u5b9a\u6027\u548c\u6da8\u843d\u7684\u4f9d\u8d56\u6027\u3002", "result": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u5728\u7406\u60f3\u91cf\u5b50\u53c2\u8003\u7cfb\uff08QRF\uff09\u7684\u6761\u4ef6\u4e0b\uff0c\u6709\u6548\u534a\u7ecf\u5178\u3001\u4ee3\u6570\u548c\u89c6\u89d2\u4e2d\u7acb\uff08PN\uff09\u8fd9\u4e09\u79cd\u5904\u7406\u89c4\u8303\u5bf9\u79f0\u6027\u7684QRF\u65b9\u6cd5\u662f\u7b49\u4ef7\u7684\u3002\u8bba\u6587\u8be6\u7ec6\u9610\u8ff0\u4e86\u8fd9\u4e9b\u65b9\u6cd5\u5728\u91cf\u5b50\u76f8\u7a7a\u95f4\u3001\u4ee3\u6570\u72b6\u6001\u7a7a\u95f4\u548c\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u7b49\u65b9\u9762\u7684\u5dee\u5f02\uff0c\u5e76\u8bc1\u660e\u4e86\u5b83\u4eec\u5728\u5904\u7406\u7ea6\u675f\u548cQRF\u53d8\u6362\u65f6\u7684\u4e00\u81f4\u6027\u3002\u7814\u7a76\u8fd8\u53d1\u73b0\uff0c\u4e0d\u786e\u5b9a\u6027\u548c\u6da8\u843d\u5177\u6709\u4f9d\u8d56\u4e8e\u53c2\u8003\u7cfb\u7684\u4f9d\u8d56\u6027\uff0c\u5e76\u4e3a\u5c06\u8fd9\u4e9b\u65b9\u6cd5\u6269\u5c55\u5230\u975e\u7406\u60f3QRF\u94fa\u5e73\u4e86\u9053\u8def\u3002", "conclusion": "\u8fd9\u7bc7\u8bba\u6587\u8bc1\u660e\u4e86\u5728\u7406\u60f3\u91cf\u5b50\u53c2\u8003\u7cfb\uff08QRF\uff09\u7684\u60c5\u51b5\u4e0b\uff0c\u6709\u6548\u534a\u7ecf\u5178\u3001\u4ee3\u6570\u548c\u89c6\u89d2\u4e2d\u7acb\uff08PN\uff09\u8fd9\u4e09\u79cd\u91cf\u5b50\u53c2\u8003\u7cfb\uff08QRF\uff09\u65b9\u6cd5\u662f\u7b49\u4ef7\u7684\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5728\u5904\u7406\u89c4\u8303\u5bf9\u79f0\u6027\u65b9\u9762\u6709\u6240\u4e0d\u540c\uff0c\u4f46\u90fd\u5c06\u5916\u90e8\u53c2\u8003\u7cfb\u4fe1\u606f\u89c6\u4e3a\u89c4\u8303\u3002"}}
{"id": "2507.13762", "categories": ["cs.LG", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2507.13762", "abs": "https://arxiv.org/abs/2507.13762", "authors": ["Yaowei Jin", "Junjie Wang", "Wenkai Xiang", "Duanhua Cao", "Dan Teng", "Zhehuan Fan", "Jiacheng Xiong", "Xia Sheng", "Chuanlong Zeng", "Mingyue Zheng", "Qian Shi"], "title": "MolPIF: A Parameter Interpolation Flow Model for Molecule Generation", "comment": null, "summary": "Advances in deep learning for molecular generation show promise in\naccelerating drug discovery. Bayesian Flow Networks (BFNs) have recently shown\nimpressive performance across diverse chemical tasks, with their success often\nascribed to the paradigm of modeling in a low-variance parameter space.\nHowever, the Bayesian inference-based strategy imposes limitations on designing\nmore flexible distribution transformation pathways, making it challenging to\nadapt to diverse data distributions and varied task requirements. Furthermore,\nthe potential for simpler, more efficient parameter-space-based models is\nunexplored. To address this, we propose a novel Parameter Interpolation Flow\nmodel (named PIF) with detailed theoretical foundation, training, and inference\nprocedures. We then develop MolPIF for structure-based drug design,\ndemonstrating its superior performance across diverse metrics compared to\nbaselines. This work validates the effectiveness of parameter-space-based\ngenerative modeling paradigm for molecules and offers new perspectives for\nmodel design.", "AI": {"tldr": "\u63d0\u51faPIF\u6a21\u578b\u7528\u4e8e\u5206\u5b50\u751f\u6210\uff0c\u514b\u670d\u4e86BFNs\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u836f\u7269\u8bbe\u8ba1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u4e3a\u4e86\u514b\u670d\u8d1d\u53f6\u65af\u6d41\u7f51\u7edc\uff08BFNs\uff09\u5728\u8bbe\u8ba1\u7075\u6d3b\u7684\u5206\u5e03\u53d8\u6362\u8def\u5f84\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63a2\u7d22\u66f4\u7b80\u5355\u3001\u66f4\u9ad8\u6548\u7684\u57fa\u4e8e\u53c2\u6570\u7a7a\u95f4\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u53c2\u6570\u63d2\u503c\u6d41\uff08PIF\uff09\u6a21\u578b\uff0c\u5e76\u7ed9\u51fa\u4e86\u8be6\u7ec6\u7684\u7406\u8bba\u57fa\u7840\u3001\u8bad\u7ec3\u548c\u63a8\u7406\u8fc7\u7a0b\uff0c\u7136\u540e\u5f00\u53d1\u4e86\u7528\u4e8e\u7ed3\u6784\u836f\u7269\u8bbe\u8ba1\u7684MolPIF\u3002", "result": "MolPIF\u5728\u7ed3\u6784\u836f\u7269\u8bbe\u8ba1\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5e76\u5728\u591a\u79cd\u8bc4\u4f30\u6307\u6807\u4e0a\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u7ed3\u679c\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u53c2\u6570\u63d2\u503c\u6d41\uff08PIF\uff09\u6a21\u578b\uff0c\u5e76\u5728\u5206\u5b50\u751f\u6210\u9886\u57df\u53d6\u5f97\u4e86\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u57fa\u4e8e\u53c2\u6570\u7a7a\u95f4\u751f\u6210\u6a21\u578b\u7684\u6709\u6548\u6027\uff0c\u5e76\u4e3a\u6a21\u578b\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.13609", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.13609", "abs": "https://arxiv.org/abs/2507.13609", "authors": ["Yanan Wang", "Julio Vizcarra", "Zhi Li", "Hao Niu", "Mori Kurokawa"], "title": "CoTasks: Chain-of-Thought based Video Instruction Tuning Tasks", "comment": null, "summary": "Despite recent progress in video large language models (VideoLLMs), a key\nopen challenge remains: how to equip models with chain-of-thought (CoT)\nreasoning abilities grounded in fine-grained object-level video understanding.\nExisting instruction-tuned models, such as the Qwen and LLaVA series, are\ntrained on high-level video-text pairs, often lacking structured annotations\nnecessary for compositional, step-by-step reasoning. We propose CoTasks:\nChain-of-Thought based Video Instruction Tuning Tasks, a new framework that\ndecomposes complex video questions of existing datasets (e.g., NeXT-QA, STAR)\ninto four entity-level foundational tasks: frame localization, entity tracking,\nspatial and temporal relation extraction. By embedding these intermediate\nCoT-style reasoning steps into the input, CoTasks enables models to explicitly\nperform object-centric spatiotemporal reasoning. Experiments on the NeXT-QA\nbenchmark show that CoTasks significantly enhance inference performance:\nLLaVA-video-7B improves by +3.3 points in average GPT-4 evaluation score, and\nQwen2.5-VL-3B gains +17.4, with large boosts in causal (+14.6), temporal\n(+10.9), and descriptive (+48.1) subcategories. These results demonstrate the\neffectiveness of CoTasks as a structured CoT-style supervision framework for\nimproving compositional video reasoning.", "AI": {"tldr": "CoTasks\u6846\u67b6\u901a\u8fc7\u5c06\u89c6\u9891\u95ee\u9898\u5206\u89e3\u4e3a\u5e27\u5b9a\u4f4d\u3001\u5b9e\u4f53\u8ddf\u8e2a\u3001\u65f6\u7a7a\u5173\u7cfb\u63d0\u53d6\u7b49\u57fa\u7840\u4efb\u52a1\uff0c\u5e76\u63d0\u4f9bCoT\u98ce\u683c\u7684\u76d1\u7763\uff0c\u6709\u6548\u589e\u5f3a\u4e86\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7ec6\u7c92\u5ea6\u89c6\u9891\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\uff08VideoLLMs\uff09\u5728\u57fa\u4e8e\u7ec6\u7c92\u5ea6\u7269\u4f53\u7406\u89e3\u7684\u94fe\u5f0f\u601d\u8003\uff08CoT\uff09\u63a8\u7406\u80fd\u529b\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u56e0\u4e3a\u5b83\u4eec\u901a\u5e38\u53ea\u5728\u7f3a\u4e4f\u7ed3\u6784\u5316\u6ce8\u89e3\u7684\u9ad8\u5c42\u6b21\u89c6\u9891-\u6587\u672c\u5bf9\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002", "method": "\u63d0\u51faCoTasks\u6846\u67b6\uff0c\u5c06\u590d\u6742\u89c6\u9891\u95ee\u9898\u5206\u89e3\u4e3a\u5e27\u5b9a\u4f4d\u3001\u5b9e\u4f53\u8ddf\u8e2a\u3001\u7a7a\u95f4\u548c\u65f6\u95f4\u5173\u7cfb\u63d0\u53d6\u56db\u4e2a\u57fa\u7840\u4efb\u52a1\uff0c\u5e76\u5c06\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\u5d4c\u5165\u8f93\u5165\uff0c\u4ee5\u5b9e\u73b0\u663e\u5f0f\u7684\u3001\u4ee5\u7269\u4f53\u4e3a\u4e2d\u5fc3\u7684\u65f6\u7a7a\u63a8\u7406\u3002", "result": "\u5728NeXT-QA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCoTasks\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff1aLLaVA-video-7B\u7684\u5e73\u5747GPT-4\u8bc4\u4f30\u5206\u6570\u63d0\u9ad8\u4e86+3.3\u5206\uff0cQwen2.5-VL-3B\u7684\u5e73\u5747\u5206\u6570\u63d0\u9ad8\u4e86+17.4\u5206\uff0c\u5176\u4e2d\u56e0\u679c\u63a8\u7406\u5b50\u4efb\u52a1\u63d0\u5347\u4e86+14.6\u5206\uff0c\u65f6\u95f4\u63a8\u7406\u5b50\u4efb\u52a1\u63d0\u5347\u4e86+10.9\u5206\uff0c\u63cf\u8ff0\u6027\u5b50\u4efb\u52a1\u63d0\u5347\u4e86+48.1\u5206\u3002", "conclusion": "CoTasks\u901a\u8fc7\u63d0\u4f9b\u7ed3\u6784\u5316\u7684\u3001\u57fa\u4e8e\u5b9e\u4f53\u548c\u65f6\u7a7a\u5173\u7cfb\u7684CoT\u98ce\u683c\u76d1\u7763\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u89c6\u9891\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u5c24\u5176\u5728\u56e0\u679c\u3001\u65f6\u95f4\u3001\u63cf\u8ff0\u6027\u5b50\u4efb\u52a1\u4e0a\u6548\u679c\u663e\u8457\u3002"}}
{"id": "2507.13937", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.13937", "abs": "https://arxiv.org/abs/2507.13937", "authors": ["Jan Trienes", "Anastasiia Derzhanskaia", "Roland Schwarzkopf", "Markus M\u00fchling", "J\u00f6rg Schl\u00f6tterer", "Christin Seifert"], "title": "Marcel: A Lightweight and Open-Source Conversational Agent for University Student Support", "comment": null, "summary": "We present Marcel, a lightweight and open-source conversational agent\ndesigned to support prospective students with admission-related inquiries. The\nsystem aims to provide fast and personalized responses, while reducing workload\nof university staff. We employ retrieval-augmented generation to ground answers\nin university resources and to provide users with verifiable, contextually\nrelevant information. To improve retrieval quality, we introduce an FAQ\nretriever that maps user questions to knowledge-base entries, allowing\nadministrators to steer retrieval, and improving over standard dense/hybrid\nretrieval strategies. The system is engineered for easy deployment in\nresource-constrained academic settings. We detail the system architecture,\nprovide a technical evaluation of its components, and report insights from a\nreal-world deployment.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.14135", "categories": ["quant-ph", "cond-mat.dis-nn", "cond-mat.stat-mech", "cond-mat.str-el", "hep-th"], "pdf": "https://arxiv.org/pdf/2507.14135", "abs": "https://arxiv.org/abs/2507.14135", "authors": ["Alan Sherry", "Sthitadhi Roy"], "title": "Do mixed states exhibit deep thermalisation?", "comment": "7 pages, 2 figures + Supplementary Material (10 pages)", "summary": "The notion of $deep$ $thermalisation$, where ensembles of pure states on a\nlocal subsystem, conditioned on measurement outcomes on its complement,\napproach universal maximum-entropy ensembles constrained only by conservation\nlaws, represents a stronger form of ergodicity than conventional\nthermalisation. We show that this framework fails dramatically for mixed\ninitial states, evolved unitarily, even with infinitesimal initial mixedness.\nTo address this, we introduce a new paradigm of deep thermalisation for mixed\nstates, fundamentally distinct from that for pure-state ensembles. In our\nformulation, the deep thermal ensemble arises by tracing out auxiliary degrees\nof freedom from a maximum-entropy ensemble defined on an augmented system, with\nthe ensemble structure depending explicitly on the entropy of the initial\nstate. We demonstrate that such ensembles emerge dynamically in generic,\nlocally interacting chaotic systems. For the self-dual kicked Ising chain,\nwhich we show to be exactly solvable for a class of mixed initial states, we\nfind exact emergence of the so-defined mixed-state deep thermal ensemble at\nfinite times. Our results therefore lead to fundamental insights into how\nmaximum entropy principles and deep thermalisation manifest themselves in\nunitary dynamics of states with finite entropy.", "AI": {"tldr": "\u6df1\u5c42\u70ed\u5316\u5728\u6df7\u5408\u6001\u4e0b\u4f1a\u5931\u6548\uff0c\u4f46\u7814\u7a76\u63d0\u51fa\u4e86\u65b0\u7684\u6df7\u5408\u6001\u6df1\u5c42\u70ed\u5316\u8303\u5f0f\uff0c\u5e76\u8bc1\u660e\u5176\u5728\u7279\u5b9a\u7cfb\u7edf\u4e2d\u53ef\u4ee5\u5b9e\u73b0\u3002", "motivation": "\u7814\u7a76\u7684\u52a8\u673a\u5728\u4e8e\uff0c\u4f20\u7edf\u7684\u6df1\u5c42\u70ed\u5316\u6982\u5ff5\u5728\u6df7\u5408\u521d\u59cb\u6001\u4e0b\u4f1a\u5931\u6548\uff0c\u5373\u4f7f\u5b58\u5728\u5fae\u5c0f\u7684\u521d\u59cb\u6df7\u5408\u5ea6\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u9700\u8981\u63d0\u51fa\u4e00\u79cd\u9002\u7528\u4e8e\u6df7\u5408\u6001\u7684\u65b0\u578b\u6df1\u5c42\u70ed\u5316\u8303\u5f0f\u3002", "method": "\u8be5\u7814\u7a76\u9996\u5148\u6307\u51fa\uff0c\u4ec5\u53d7\u5b88\u6052\u5f8b\u7ea6\u675f\u7684\u6700\u5927\u71b5\u7cfb\u7efc\u5728\u6df7\u5408\u521d\u59cb\u6001\u4e0b\u65e0\u6cd5\u5b9e\u73b0\u6df1\u5c42\u70ed\u5316\u3002\u968f\u540e\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u9002\u7528\u4e8e\u6df7\u5408\u6001\u7684\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u8ffd\u8e2a\u8f85\u52a9\u81ea\u7531\u5ea6\u6765\u5f62\u6210\u6df1\u5c42\u70ed\u7cfb\u7efc\uff0c\u5e76\u4f9d\u8d56\u4e8e\u521d\u59cb\u6001\u7684\u71b5\u3002\u63a5\u7740\uff0c\u901a\u8fc7\u5bf9\u81ea\u5bf9\u5076\u8e22\u4f0a\u8f9b\u94fe\uff08\u4e00\u79cd\u53ef\u7cbe\u786e\u6c42\u89e3\u7684\u7cfb\u7edf\uff09\u7684\u7814\u7a76\uff0c\u8bc1\u660e\u4e86\u5728\u6df7\u5408\u521d\u59cb\u6001\u4e0b\uff0c\u8be5\u7cfb\u7efc\u80fd\u5728\u6709\u9650\u65f6\u95f4\u5185\u51fa\u73b0\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u5bf9\u4e8e\u6df7\u5408\u521d\u59cb\u6001\uff0c\u6df1\u5c42\u70ed\u5316\u6982\u5ff5\u4f1a\u5931\u8d25\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6df7\u5408\u6001\u6df1\u5c42\u70ed\u5316\u8303\u5f0f\uff0c\u8be5\u8303\u5f0f\u6e90\u4e8e\u5bf9\u589e\u5f3a\u7cfb\u7edf\u4e0a\u7684\u6700\u5927\u71b5\u7cfb\u7efc\u8fdb\u884c\u8ffd\u8e2a\uff0c\u5e76\u4e14\u7cfb\u7efc\u7ed3\u6784\u660e\u786e\u53d6\u51b3\u4e8e\u521d\u59cb\u6001\u7684\u71b5\u3002\u5728\u81ea\u5bf9\u5076\u8e22\u4f0a\u8f9b\u94fe\u4e0a\uff0c\u8be5\u6df7\u5408\u6001\u6df1\u5c42\u70ed\u7cfb\u7efc\u5728\u6709\u9650\u65f6\u95f4\u5185\u53ef\u4ee5\u7cbe\u786e\u51fa\u73b0\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u6df7\u5408\u6001\u6df1\u5c42\u70ed\u5316\u8303\u5f0f\uff0c\u8be5\u8303\u5f0f\u4e0e\u7eaf\u6001\u8303\u5f0f\u6839\u672c\u4e0d\u540c\u3002\u8be5\u8303\u5f0f\u6e90\u4e8e\u5bf9\u589e\u5f3a\u7cfb\u7edf\u4e0a\u7684\u6700\u5927\u71b5\u7cfb\u7efc\u8fdb\u884c\u8ffd\u8e2a\uff0c\u5e76\u4e14\u7cfb\u7efc\u7ed3\u6784\u660e\u786e\u53d6\u51b3\u4e8e\u521d\u59cb\u6001\u7684\u71b5\u3002\u7814\u7a76\u8868\u660e\uff0c\u8fd9\u79cd\u7cfb\u7efc\u5728\u5178\u578b\u7684\u3001\u5c40\u90e8\u76f8\u4e92\u4f5c\u7528\u7684\u6df7\u6c8c\u7cfb\u7edf\u4e2d\u52a8\u6001\u51fa\u73b0\u3002\u5bf9\u4e8e\u81ea\u5bf9\u5076\u8e22\u4f0a\u8f9b\u94fe\uff0c\u7814\u7a76\u53d1\u73b0\u6df7\u5408\u6001\u6df1\u5c42\u70ed\u7cfb\u7efc\u5728\u6709\u9650\u65f6\u95f4\u53ef\u4ee5\u7cbe\u786e\u51fa\u73b0\u3002"}}
{"id": "2507.13765", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13765", "abs": "https://arxiv.org/abs/2507.13765", "authors": ["Enhao Cheng", "Shoujia Zhang", "Jianhua Yin", "Li Jin", "Liqiang Nie"], "title": "Dual-Center Graph Clustering with Neighbor Distribution", "comment": "ECAI-2025", "summary": "Graph clustering is crucial for unraveling intricate data structures, yet it\npresents significant challenges due to its unsupervised nature. Recently,\ngoal-directed clustering techniques have yielded impressive results, with\ncontrastive learning methods leveraging pseudo-label garnering considerable\nattention. Nonetheless, pseudo-label as a supervision signal is unreliable and\nexisting goal-directed approaches utilize only features to construct a\nsingle-target distribution for single-center optimization, which lead to\nincomplete and less dependable guidance. In our work, we propose a novel\nDual-Center Graph Clustering (DCGC) approach based on neighbor distribution\nproperties, which includes representation learning with neighbor distribution\nand dual-center optimization. Specifically, we utilize neighbor distribution as\na supervision signal to mine hard negative samples in contrastive learning,\nwhich is reliable and enhances the effectiveness of representation learning.\nFurthermore, neighbor distribution center is introduced alongside feature\ncenter to jointly construct a dual-target distribution for dual-center\noptimization. Extensive experiments and analysis demonstrate superior\nperformance and effectiveness of our proposed method.", "AI": {"tldr": "DCGC\u662f\u4e00\u79cd\u65b0\u7684\u56fe\u805a\u7c7b\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u90bb\u57df\u5206\u5e03\u4fe1\u606f\u548c\u53cc\u4e2d\u5fc3\u4f18\u5316\u6765\u514b\u670d\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u56fe\u805a\u7c7b\u65b9\u6cd5\u5728\u5904\u7406\u65e0\u76d1\u7763\u5b66\u4e60\u7684\u6311\u6218\u65f6\uff0c\u867d\u7136\u76ee\u6807\u5bfc\u5411\u7684\u805a\u7c7b\u6280\u672f\u53d6\u5f97\u4e86\u663e\u8457\u6210\u679c\uff0c\u4f46\u57fa\u4e8e\u4f2a\u6807\u7b7e\u7684\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\u6240\u4f7f\u7528\u7684\u4f2a\u6807\u7b7e\u4f5c\u4e3a\u76d1\u7763\u4fe1\u53f7\u5e76\u4e0d\u53ef\u9760\u3002\u6b64\u5916\uff0c\u73b0\u6709\u65b9\u6cd5\u4ec5\u5229\u7528\u7279\u5f81\u6765\u6784\u5efa\u5355\u4e00\u76ee\u6807\u5206\u5e03\u8fdb\u884c\u5355\u4e2d\u5fc3\u4f18\u5316\uff0c\u8fd9\u5bfc\u81f4\u4e86\u4e0d\u5b8c\u6574\u4e14\u4e0d\u53ef\u9760\u7684\u6307\u5bfc\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u90bb\u57df\u5206\u5e03\u6027\u8d28\u7684\u53cc\u4e2d\u5fc3\u56fe\u805a\u7c7b\uff08DCGC\uff09\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5305\u62ec\u90bb\u57df\u5206\u5e03\u8868\u793a\u5b66\u4e60\u548c\u53cc\u4e2d\u5fc3\u4f18\u5316\u4e24\u4e2a\u5173\u952e\u90e8\u5206\u3002\u5728\u8868\u793a\u5b66\u4e60\u4e2d\uff0c\u5229\u7528\u90bb\u57df\u5206\u5e03\u4f5c\u4e3a\u76d1\u7763\u4fe1\u53f7\u6765\u6316\u6398\u56f0\u96be\u8d1f\u6837\u672c\uff0c\u4ee5\u589e\u5f3a\u8868\u793a\u5b66\u4e60\u7684\u6709\u6548\u6027\u3002\u5728\u4f18\u5316\u8fc7\u7a0b\u4e2d\uff0c\u5f15\u5165\u4e86\u90bb\u57df\u5206\u5e03\u4e2d\u5fc3\u548c\u7279\u5f81\u4e2d\u5fc3\uff0c\u5171\u540c\u6784\u5efa\u53cc\u76ee\u6807\u5206\u5e03\u8fdb\u884c\u53cc\u4e2d\u5fc3\u4f18\u5316\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u548c\u5206\u6790\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684DCGC\u65b9\u6cd5\u5177\u6709\u4f18\u8d8a\u7684\u6027\u80fd\u548c\u6709\u6548\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u53cc\u4e2d\u5fc3\u56fe\u805a\u7c7b\uff08DCGC\uff09\u65b9\u6cd5\u5728\u90bb\u57df\u5206\u5e03\u6027\u8d28\u7684\u57fa\u7840\u4e0a\uff0c\u901a\u8fc7\u7ed3\u5408\u90bb\u57df\u5206\u5e03\u548c\u7279\u5f81\u4e2d\u5fc3\u8fdb\u884c\u53cc\u4e2d\u5fc3\u4f18\u5316\uff0c\u5e76\u5229\u7528\u90bb\u57df\u5206\u5e03\u4f5c\u4e3a\u76d1\u7763\u4fe1\u53f7\u6316\u6398\u56f0\u96be\u8d1f\u6837\u672c\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u8868\u793a\u5b66\u4e60\u7684\u6548\u679c\u3002"}}
{"id": "2507.13628", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13628", "abs": "https://arxiv.org/abs/2507.13628", "authors": ["Masahiro Ogawa", "Qi An", "Atsushi Yamashita"], "title": "Moving Object Detection from Moving Camera Using Focus of Expansion Likelihood and Segmentation", "comment": "8 pages, 15 figures, RA-L submission", "summary": "Separating moving and static objects from a moving camera viewpoint is\nessential for 3D reconstruction, autonomous navigation, and scene understanding\nin robotics. Existing approaches often rely primarily on optical flow, which\nstruggles to detect moving objects in complex, structured scenes involving\ncamera motion. To address this limitation, we propose Focus of Expansion\nLikelihood and Segmentation (FoELS), a method based on the core idea of\nintegrating both optical flow and texture information. FoELS computes the focus\nof expansion (FoE) from optical flow and derives an initial motion likelihood\nfrom the outliers of the FoE computation. This likelihood is then fused with a\nsegmentation-based prior to estimate the final moving probability. The method\neffectively handles challenges including complex structured scenes, rotational\ncamera motion, and parallel motion. Comprehensive evaluations on the DAVIS 2016\ndataset and real-world traffic videos demonstrate its effectiveness and\nstate-of-the-art performance.", "AI": {"tldr": "\u63d0\u51faFoELS\u65b9\u6cd5\uff0c\u6574\u5408\u5149\u6d41\u548c\u7eb9\u7406\u4fe1\u606f\uff0c\u7528\u4e8e\u4ece\u79fb\u52a8\u6444\u50cf\u673a\u89c6\u89d2\u5206\u79bb\u8fd0\u52a8\u548c\u9759\u6001\u7269\u4f53\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u5149\u6d41\u6cd5\u5728\u590d\u6742\u573a\u666f\u4e0b\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4ece\u79fb\u52a8\u6444\u50cf\u673a\u89c6\u89d2\u5206\u79bb\u8fd0\u52a8\u548c\u9759\u6001\u7269\u4f53\u5bf9\u4e8e\u673a\u5668\u4eba\u6280\u672f\u4e2d\u76843D\u91cd\u5efa\u3001\u81ea\u4e3b\u5bfc\u822a\u548c\u573a\u666f\u7406\u89e3\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u4e8e\u5149\u6d41\uff0c\u4f46\u5728\u6d89\u53ca\u76f8\u673a\u8fd0\u52a8\u7684\u590d\u6742\u7ed3\u6784\u573a\u666f\u4e2d\u96be\u4ee5\u68c0\u6d4b\u8fd0\u52a8\u7269\u4f53\u3002", "method": "FoELS\u65b9\u6cd5\u6574\u5408\u4e86\u5149\u6d41\u548c\u7eb9\u7406\u4fe1\u606f\uff0c\u8ba1\u7b97\u5149\u6d41\u7684\u6269\u5f20\u7126\u70b9\uff08FoE\uff09\uff0c\u5e76\u4eceFoE\u8ba1\u7b97\u7684\u79bb\u7fa4\u503c\u4e2d\u5bfc\u51fa\u521d\u59cb\u8fd0\u52a8\u53ef\u80fd\u6027\u3002\u7136\u540e\uff0c\u8be5\u53ef\u80fd\u6027\u4e0e\u57fa\u4e8e\u5206\u5272\u7684\u5148\u9a8c\u4fe1\u606f\u878d\u5408\uff0c\u4ee5\u4f30\u8ba1\u6700\u7ec8\u7684\u8fd0\u52a8\u6982\u7387\u3002", "result": "FoELS\u65b9\u6cd5\u5728DAVIS 2016\u6570\u636e\u96c6\u548c\u771f\u5b9e\u4ea4\u901a\u89c6\u9891\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u7ed3\u679c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u548c\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "FoELS\u65b9\u6cd5\u6709\u6548\u5730\u89e3\u51b3\u4e86\u590d\u6742\u7ed3\u6784\u573a\u666f\u3001\u65cb\u8f6c\u76f8\u673a\u8fd0\u52a8\u548c\u5e73\u884c\u8fd0\u52a8\u7b49\u6311\u6218\uff0c\u5e76\u5728DAVIS 2016\u6570\u636e\u96c6\u548c\u771f\u5b9e\u4ea4\u901a\u89c6\u9891\u7684\u7efc\u5408\u8bc4\u4f30\u4e2d\u5c55\u793a\u4e86\u5176\u6709\u6548\u6027\u548c\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2507.13949", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13949", "abs": "https://arxiv.org/abs/2507.13949", "authors": ["Bianca Raimondi", "Maurizio Gabbrielli"], "title": "Exploiting Primacy Effect To Improve Large Language Models", "comment": "Accepted by RANLP 2025", "summary": "Large Language Models (LLMs) have become essential in many Natural Language\nProcessing (NLP) tasks, leveraging extensive pre-training and fine-tuning to\nachieve high accuracy. However, like humans, LLMs exhibit biases, particularly\npositional biases such as primacy and recency effects, which can influence the\naccuracy of the answers. The primacy effect-where items presented first are\nmore likely to be remembered or selected-plays a key role in Multiple Choice\nQuestion Answering (MCQA), where the order of answer options can affect\nprediction outcomes. This study focuses on primacy bias in fine-tuned LLMs: We\nfirst show that fine-tuning amplifies this bias, probably due to exposure to\nhuman-like patterns. Hence, we strategically leverage this effect by reordering\nresponse options based on semantic similarity to the query, without requiring\nknowledge of the correct answer. Our experimental results show that this\napproach significantly improves performance in MCQA. More generally, our\nfindings underscore the dual nature of biases as both challenges and\nopportunities, offering insights for bias-aware model design and NLP\napplications.", "AI": {"tldr": "\u5fae\u8c03LLM\u4f1a\u653e\u5927\u5148\u5e8f\u504f\u89c1\uff0c\u4f46\u901a\u8fc7\u7b56\u7565\u6027\u5730\u91cd\u65b0\u6392\u5e8f\u9009\u9879\uff08\u57fa\u4e8e\u8bed\u4e49\u76f8\u4f3c\u6027\uff09\u53ef\u4ee5\u5229\u7528\u8fd9\u79cd\u504f\u89c1\u6765\u63d0\u9ad8MCQA\u4efb\u52a1\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u7814\u7a76LLM\u4e2d\u7684\u5148\u5e8f\u504f\u89c1\uff0c\u7279\u522b\u662f\u8fd9\u79cd\u504f\u89c1\u5728\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u5982\u4f55\u88ab\u653e\u5927\uff0c\u5e76\u63a2\u7d22\u5229\u7528\u8fd9\u79cd\u504f\u89c1\u6765\u6539\u8fdbMCQA\u6027\u80fd\u7684\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u91cd\u65b0\u6392\u5e8f\u9009\u9879\uff08\u4e0d\u4f9d\u8d56\u6b63\u786e\u7b54\u6848\uff09\u6765\u5229\u7528\u5148\u5e8f\u504f\u89c1\uff0c\u4ee5\u63d0\u5347LLM\u5728\u591a\u9879\u9009\u62e9\u9898\u95ee\u7b54\uff08MCQA\uff09\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u901a\u8fc7\u57fa\u4e8e\u8bed\u4e49\u76f8\u4f3c\u6027\u91cd\u65b0\u6392\u5e8f\u9009\u9879\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86LLM\u5728MCQA\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002", "conclusion": "LLM\u5728MCQA\u4efb\u52a1\u4e2d\u5b58\u5728\u663e\u8457\u7684\u5148\u5e8f\u504f\u89c1\uff0c\u800c\u901a\u8fc7\u6839\u636e\u8bed\u4e49\u76f8\u4f3c\u6027\u91cd\u65b0\u6392\u5e8f\u9009\u9879\u53ef\u4ee5\u6709\u6548\u7f13\u89e3\u5e76\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u504f\u89c1\u7684\u53cc\u91cd\u6027\uff0c\u4e3a\u8bbe\u8ba1\u504f\u89c1\u611f\u77e5\u6a21\u578b\u63d0\u4f9b\u4e86\u601d\u8def\u3002"}}
{"id": "2507.13648", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13648", "abs": "https://arxiv.org/abs/2507.13648", "authors": ["Seungjun Moon", "Sangjoon Yu", "Gyeong-Moon Park"], "title": "EPSilon: Efficient Point Sampling for Lightening of Hybrid-based 3D Avatar Generation", "comment": null, "summary": "The rapid advancement of neural radiance fields (NeRF) has paved the way to\ngenerate animatable human avatars from a monocular video. However, the sole\nusage of NeRF suffers from a lack of details, which results in the emergence of\nhybrid representation that utilizes SMPL-based mesh together with NeRF\nrepresentation. While hybrid-based models show photo-realistic human avatar\ngeneration qualities, they suffer from extremely slow inference due to their\ndeformation scheme: to be aligned with the mesh, hybrid-based models use the\ndeformation based on SMPL skinning weights, which needs high computational\ncosts on each sampled point. We observe that since most of the sampled points\nare located in empty space, they do not affect the generation quality but\nresult in inference latency with deformation. In light of this observation, we\npropose EPSilon, a hybrid-based 3D avatar generation scheme with novel\nefficient point sampling strategies that boost both training and inference. In\nEPSilon, we propose two methods to omit empty points at rendering; empty ray\nomission (ERO) and empty interval omission (EIO). In ERO, we wipe out rays that\nprogress through the empty space. Then, EIO narrows down the sampling interval\non the ray, which wipes out the region not occupied by either clothes or mesh.\nThe delicate sampling scheme of EPSilon enables not only great computational\ncost reduction during deformation but also the designation of the important\nregions to be sampled, which enables a single-stage NeRF structure without\nhierarchical sampling. Compared to existing methods, EPSilon maintains the\ngeneration quality while using only 3.9% of sampled points and achieves around\n20 times faster inference, together with 4 times faster training convergence.\nWe provide video results on https://github.com/seungjun-moon/epsilon.", "AI": {"tldr": "EPSilon\u662f\u4e00\u79cd\u521b\u65b0\u76843D\u865a\u62df\u4eba\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u5176\u72ec\u7279\u7684\u9ad8\u6548\u70b9\u91c7\u6837\u7b56\u7565\uff08ERO\u548cEIO\uff09\uff0c\u5927\u5e45\u63d0\u5347\u4e86\u8bad\u7ec3\u548c\u63a8\u7406\u901f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u8d28\u91cf\u7684\u751f\u6210\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e NeRF \u7684\u65b9\u6cd5\u5728\u751f\u6210\u52a8\u6f2b\u5316\u4eba\u4f53\u865a\u62df\u4eba\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u5355\u7eaf\u4f7f\u7528 NeRF \u7ec6\u8282\u4e0d\u8db3\u3002\u6df7\u5408\u65b9\u6cd5\uff08\u7ed3\u5408 NeRF \u548c\u57fa\u4e8e SMPL \u7684\u7f51\u683c\uff09\u867d\u7136\u80fd\u751f\u6210\u7167\u7247\u7ea7\u903c\u771f\u7684\u865a\u62df\u4eba\uff0c\u4f46\u5176\u57fa\u4e8e SMPL \u8499\u76ae\u6743\u91cd\u7684\u53d8\u5f62\u65b9\u6848\u5bfc\u81f4\u63a8\u7406\u901f\u5ea6\u6781\u6162\uff0c\u56e0\u4e3a\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u4e14\u5927\u90e8\u5206\u91c7\u6837\u70b9\u4f4d\u4e8e\u7a7a\u767d\u7a7a\u95f4\uff0c\u5bf9\u751f\u6210\u8d28\u91cf\u5f71\u54cd\u751a\u5fae\u3002", "method": "EPSilon\u662f\u4e00\u79cd\u6df7\u5408\u8868\u793a\u76843D\u865a\u62df\u4eba\u751f\u6210\u65b9\u6cd5\uff0c\u5176\u521b\u65b0\u4e4b\u5904\u5728\u4e8e\u91c7\u7528\u4e86\u4e24\u79cd\u9ad8\u6548\u7684\u70b9\u91c7\u6837\u7b56\u7565\uff1a\u7a7a\u5c04\u7ebf\u9057\u6f0f\uff08ERO\uff09\u548c\u7a7a\u533a\u95f4\u9057\u6f0f\uff08EIO\uff09\u3002ERO\u901a\u8fc7\u5254\u9664\u7a7f\u8fc7\u7a7a\u767d\u7a7a\u95f4\u7684\u5c04\u7ebf\u6765\u51cf\u5c11\u91c7\u6837\uff1bEIO\u5219\u5728\u5c04\u7ebf\u4e0a\u7f29\u5c0f\u91c7\u6837\u533a\u95f4\uff0c\u5254\u9664\u4e0d\u88ab\u8863\u7269\u6216\u7f51\u683c\u5360\u636e\u7684\u533a\u57df\u3002\u8fd9\u4e9b\u7b56\u7565\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u5728\u51cf\u5c11\u7ea696%\u91c7\u6837\u70b9\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u7ea620\u500d\u7684\u63a8\u7406\u52a0\u901f\u548c4\u500d\u7684\u8bad\u7ec3\u6536\u655b\u52a0\u901f\u3002", "result": "EPSilon\u5728\u4fdd\u6301\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u5f53\u7684\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u4ec5\u4f7f\u7528\u4e863.9%\u7684\u91c7\u6837\u70b9\uff0c\u5b9e\u73b0\u4e86\u7ea620\u500d\u7684\u63a8\u7406\u52a0\u901f\u548c4\u500d\u7684\u8bad\u7ec3\u6536\u655b\u52a0\u901f\uff0c\u5e76\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u65b9\u9762\u5747\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "EPSilon\u901a\u8fc7\u91c7\u7528\u9ad8\u6548\u7684\u70b9\u91c7\u6837\u7b56\u7565\uff0c\u5728\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u8ba1\u7b97\u91cf\u5e76\u52a0\u5feb\u4e86\u8bad\u7ec3\u548c\u63a8\u7406\u901f\u5ea6\uff0c\u662f3D\u865a\u62df\u4eba\u751f\u6210\u9886\u57df\u7684\u6709\u524d\u666f\u7684\u65b9\u6cd5\u3002"}}
{"id": "2507.13966", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13966", "abs": "https://arxiv.org/abs/2507.13966", "authors": ["Bhishma Dedhia", "Yuval Kansal", "Niraj K. Jha"], "title": "Bottom-up Domain-specific Superintelligence: A Reliable Knowledge Graph is What We Need", "comment": null, "summary": "Language models traditionally used for cross-domain generalization have\nrecently demonstrated task-specific reasoning. However, their top-down training\napproach on general corpora is insufficient for acquiring abstractions needed\nfor deep domain expertise. This may require a bottom-up approach that acquires\nexpertise by learning to compose simple domain concepts into more complex ones.\nA knowledge graph (KG) provides this compositional structure, where domain\nprimitives are represented as head-relation-tail edges and their paths encode\nhigher-level concepts. We present a task generation pipeline that synthesizes\ntasks directly from KG primitives, enabling models to acquire and compose them\nfor reasoning. We fine-tune language models on the resultant KG-grounded\ncurriculum to demonstrate domain-specific superintelligence. While broadly\napplicable, we validate our approach in medicine, where reliable KGs exist.\nUsing a medical KG, we curate 24,000 reasoning tasks paired with thinking\ntraces derived from diverse medical primitives. We fine-tune the QwQ-32B model\non this curriculum to obtain QwQ-Med-3 that takes a step towards medical\nsuperintelligence. We also introduce ICD-Bench, an evaluation suite to quantify\nreasoning abilities across 15 medical domains. Our experiments demonstrate that\nQwQ-Med-3 significantly outperforms state-of-the-art reasoning models on\nICD-Bench categories. Further analysis reveals that QwQ-Med-3 utilizes acquired\nprimitives to widen the performance gap on the hardest tasks of ICD-Bench.\nFinally, evaluation on medical question-answer benchmarks shows that QwQ-Med-3\ntransfers acquired expertise to enhance the base model's performance. While the\nindustry's approach to artificial general intelligence (AGI) emphasizes broad\nexpertise, we envision a future in which AGI emerges from the composable\ninteraction of efficient domain-specific superintelligent agents.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\u751f\u6210\u4efb\u52a1\u6765\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u4ee5\u83b7\u5f97\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u7684\u65b9\u6cd5\u3002\u4ed6\u4eec\u4f7f\u7528\u533b\u5b66\u77e5\u8bc6\u56fe\u8c31\u521b\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3a QwQ-Med-3 \u7684\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5728\u533b\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u80fd\u5c06\u5176\u4e13\u4e1a\u77e5\u8bc6\u8f6c\u79fb\u5230\u5176\u4ed6\u4efb\u52a1\u4e0a\u3002\u7814\u7a76\u8ba4\u4e3a\uff0c\u672a\u6765\u7684\u901a\u7528\u4eba\u5de5\u667a\u80fd\u5c06\u7531\u591a\u4e2a\u5177\u6709\u7279\u5b9a\u9886\u57df\u4e13\u957f\u7684\u667a\u80fd\u4ee3\u7406\u7ec4\u6210\u3002", "motivation": "\u4f20\u7edf\u7684\u8bed\u8a00\u6a21\u578b\u5728\u8de8\u9886\u57df\u6cdb\u5316\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u83b7\u53d6\u6df1\u5c42\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u6240\u9700\u7684\u62bd\u8c61\u80fd\u529b\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002\u4f5c\u8005\u8ba4\u4e3a\uff0c\u8fd9\u79cd\u4e0d\u8db3\u6e90\u4e8e\u5176\u5728\u901a\u7528\u8bed\u6599\u5e93\u4e0a\u7684\u81ea\u9876\u5411\u4e0b\u8bad\u7ec3\u65b9\u6cd5\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e00\u9650\u5236\uff0c\u9700\u8981\u4e00\u79cd\u81ea\u5e95\u5411\u4e0a\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b66\u4e60\u7ec4\u5408\u7b80\u5355\u7684\u9886\u57df\u6982\u5ff5\u6765\u5f62\u6210\u66f4\u590d\u6742\u7684\u6982\u5ff5\uff0c\u4ece\u800c\u83b7\u5f97\u4e13\u4e1a\u77e5\u8bc6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u76f4\u63a5\u4ece\u77e5\u8bc6\u56fe\u8c31\uff08KG\uff09\u7684\u539f\u59cb\u6982\u5ff5\u5408\u6210\u4efb\u52a1\u7684\u751f\u6210\u6d41\u7a0b\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u5b66\u4e60\u548c\u7ec4\u5408\u8fd9\u4e9b\u6982\u5ff5\u8fdb\u884c\u63a8\u7406\u3002\u7136\u540e\uff0c\u5728\u751f\u6210\u7684 KG \u57fa\u7840\u8bfe\u7a0b\u4e0a\u5bf9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u4ee5\u5b9e\u73b0\u9886\u57df\u7279\u5b9a\u7684\u8d85\u667a\u80fd\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u5728\u533b\u5b66\u9886\u57df\uff0c\u4ed6\u4eec\u4f7f\u7528\u533b\u5b66 KG \u7b56\u5212\u4e86 24,000 \u4e2a\u63a8\u7406\u4efb\u52a1\u548c\u76f8\u5e94\u7684\u601d\u8003\u8fc7\u7a0b\uff0c\u5e76\u4f7f\u7528 QwQ-32B \u6a21\u578b\u8fdb\u884c\u4e86\u5fae\u8c03\uff0c\u5f97\u5230\u4e86 QwQ-Med-3\u3002\u6b64\u5916\uff0c\u4ed6\u4eec\u8fd8\u5f15\u5165\u4e86\u4e00\u4e2a\u540d\u4e3a ICD-Bench \u7684\u8bc4\u4f30\u5957\u4ef6\uff0c\u7528\u4e8e\u91cf\u5316\u6a21\u578b\u5728 15 \u4e2a\u533b\u5b66\u9886\u57df\u5185\u7684\u63a8\u7406\u80fd\u529b\u3002", "result": "QwQ-Med-3 \u5728 ICD-Bench \u8bc4\u4f30\u5957\u4ef6\u7684 15 \u4e2a\u533b\u5b66\u9886\u57df\u4e2d\uff0c\u5176\u8868\u73b0\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u63a8\u7406\u6a21\u578b\u3002\u8fdb\u4e00\u6b65\u7684\u5206\u6790\u8868\u660e\uff0cQwQ-Med-3 \u5229\u7528\u5176\u5b66\u5230\u7684\u539f\u59cb\u6982\u5ff5\uff0c\u5728 ICD-Bench \u7684\u9ad8\u96be\u5ea6\u4efb\u52a1\u4e0a\u6269\u5927\u4e86\u6027\u80fd\u5dee\u8ddd\u3002\u6b64\u5916\uff0c\u5728\u533b\u5b66\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0cQwQ-Med-3 \u80fd\u591f\u5c06\u5176\u83b7\u5f97\u7684\u4e13\u4e1a\u77e5\u8bc6\u8f6c\u79fb\u5230\u57fa\u7840\u6a21\u578b\u4e0a\uff0c\u4ece\u800c\u63d0\u5347\u4e86\u57fa\u7840\u6a21\u578b\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u77e5\u8bc6\u56fe\u8c31\uff08KG\uff09\u7684\u9886\u57df\u539f\u59cb\u6982\u5ff5\u751f\u6210\u4efb\u52a1\u7684\u65b9\u6cd5\uff0c\u4ee5\u57f9\u517b\u8bed\u8a00\u6a21\u578b\u7684\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u3002\u901a\u8fc7\u5728\u533b\u5b66\u9886\u57df\u9a8c\u8bc1\u8be5\u65b9\u6cd5\uff0c\u4ed6\u4eec\u5fae\u8c03\u4e86\u4e00\u4e2a\u540d\u4e3a QwQ-Med-3 \u7684\u8bed\u8a00\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5728 ICD-BM \u8bc4\u4f30\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u5728\u533b\u5b66\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u793a\u4e86\u5176\u4e13\u4e1a\u77e5\u8bc6\u7684\u8f6c\u79fb\u80fd\u529b\u3002\u7814\u7a76\u4eba\u5458\u8bbe\u60f3\uff0c\u672a\u6765\u7684\u901a\u7528\u4eba\u5de5\u667a\u80fd\uff08AGI\uff09\u5c06\u6e90\u4e8e\u9ad8\u6548\u7684\u3001\u7279\u5b9a\u9886\u57df\u7684\u8d85\u7ea7\u667a\u80fd\u4ee3\u7406\u7684\u53ef\u7ec4\u5408\u4ea4\u4e92\u3002"}}
{"id": "2507.13659", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2507.13659", "abs": "https://arxiv.org/abs/2507.13659", "authors": ["Xiao Wang", "Qian Zhu", "Shujuan Wu", "Bo Jiang", "Shiliang Zhang", "Yaowei Wang", "Yonghong Tian", "Bin Luo"], "title": "When Person Re-Identification Meets Event Camera: A Benchmark Dataset and An Attribute-guided Re-Identification Framework", "comment": null, "summary": "Recent researchers have proposed using event cameras for person\nre-identification (ReID) due to their promising performance and better balance\nin terms of privacy protection, event camera-based person ReID has attracted\nsignificant attention. Currently, mainstream event-based person ReID algorithms\nprimarily focus on fusing visible light and event stream, as well as preserving\nprivacy. Although significant progress has been made, these methods are\ntypically trained and evaluated on small-scale or simulated event camera\ndatasets, making it difficult to assess their real identification performance\nand generalization ability. To address the issue of data scarcity, this paper\nintroduces a large-scale RGB-event based person ReID dataset, called EvReID.\nThe dataset contains 118,988 image pairs and covers 1200 pedestrian identities,\nwith data collected across multiple seasons, scenes, and lighting conditions.\nWe also evaluate 15 state-of-the-art person ReID algorithms, laying a solid\nfoundation for future research in terms of both data and benchmarking. Based on\nour newly constructed dataset, this paper further proposes a pedestrian\nattribute-guided contrastive learning framework to enhance feature learning for\nperson re-identification, termed TriPro-ReID. This framework not only\neffectively explores the visual features from both RGB frames and event\nstreams, but also fully utilizes pedestrian attributes as mid-level semantic\nfeatures. Extensive experiments on the EvReID dataset and MARS datasets fully\nvalidated the effectiveness of our proposed RGB-Event person ReID framework.\nThe benchmark dataset and source code will be released on\nhttps://github.com/Event-AHU/Neuromorphic_ReID", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86EvReID\u6570\u636e\u96c6\u548cTriPro-ReID\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u4e8b\u4ef6\u76f8\u673a\u884c\u4eba\u91cd\u8bc6\u522b\u4e2d\u7684\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u5e76\u63d0\u5347\u8bc6\u522b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u4e8b\u4ef6\u76f8\u673a\u7684\u884c\u4eba\u91cd\u8bc6\u522b\u7b97\u6cd5\u4e3b\u8981\u5173\u6ce8\u53ef\u89c1\u5149\u4e0e\u4e8b\u4ef6\u6d41\u7684\u878d\u5408\u4ee5\u53ca\u9690\u79c1\u4fdd\u62a4\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u901a\u5e38\u5728\u5c0f\u89c4\u6a21\u6216\u6a21\u62df\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u548c\u8bc4\u4f30\uff0c\u96be\u4ee5\u8bc4\u4f30\u5176\u5b9e\u9645\u8bc6\u522b\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002\u4e3a\u4e86\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u672c\u7814\u7a76\u65e8\u5728\u6784\u5efa\u4e00\u4e2a\u5927\u89c4\u6a21\u7684RGB-\u4e8b\u4ef6\u884c\u4eba\u91cd\u8bc6\u522b\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u51fa\u4e00\u4e2a\u6709\u6548\u7684\u884c\u4eba\u91cd\u8bc6\u522b\u6846\u67b6\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aTriPro-ReID\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u91c7\u7528\u5bf9\u6bd4\u5b66\u4e60\uff0c\u5e76\u5229\u7528\u4e86\u884c\u4eba\u5c5e\u6027\u4f5c\u4e3a\u4e2d\u7ea7\u8bed\u4e49\u7279\u5f81\uff0c\u4ee5\u63a2\u7d22RGB\u5e27\u548c\u4e8b\u4ef6\u6d41\u4e2d\u7684\u89c6\u89c9\u7279\u5f81\u3002\u6b64\u5916\uff0c\u7814\u7a76\u4eba\u5458\u8fd8\u6784\u5efa\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21RGB-\u4e8b\u4ef6\u884c\u4eba\u91cd\u8bc6\u522b\u6570\u636e\u96c6EvReID\uff0c\u8be5\u6570\u636e\u96c6\u5305\u542b118,988\u5f20\u56fe\u50cf\u5bf9\uff0c\u6db5\u76d61200\u540d\u884c\u4eba\uff0c\u5e76\u5728\u4e0d\u540c\u5b63\u8282\u3001\u573a\u666f\u548c\u5149\u7167\u6761\u4ef6\u4e0b\u6536\u96c6\u3002", "result": "\u7814\u7a76\u6210\u529f\u6784\u5efa\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21RGB-\u4e8b\u4ef6\u884c\u4eba\u91cd\u8bc6\u522b\u6570\u636e\u96c6EvReID\uff0c\u5e76\u63d0\u51fa\u4e86TriPro-ReID\u6846\u67b6\u3002\u5728EvReID\u548cMARS\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cTriPro-ReID\u6846\u67b6\u80fd\u591f\u6709\u6548\u5730\u5229\u7528RGB\u5e27\u548c\u4e8b\u4ef6\u6d41\u4e2d\u7684\u89c6\u89c9\u7279\u5f81\u4ee5\u53ca\u884c\u4eba\u5c5e\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u884c\u4eba\u91cd\u8bc6\u522b\u7684\u6027\u80fd\u3002\u7814\u7a76\u8fd8\u8bc4\u4f30\u4e8615\u79cd\u5148\u8fdb\u7684\u884c\u4eba\u91cd\u8bc6\u522b\u7b97\u6cd5\uff0c\u4e3a\u672a\u6765\u7684\u7814\u7a76\u5960\u5b9a\u4e86\u6570\u636e\u548c\u57fa\u51c6\u6d4b\u8bd5\u7684\u57fa\u7840\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aTriPro-ReID\u7684RGB-\u4e8b\u4ef6\u884c\u4eba\u91cd\u8bc6\u522b\u6846\u67b6\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21RGB-\u4e8b\u4ef6\u884c\u4eba\u91cd\u8bc6\u522b\u6570\u636e\u96c6EvReID\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u6570\u636e\u96c6\u89c4\u6a21\u5c0f\u548c\u6a21\u62df\u6570\u636e\u7684\u95ee\u9898\u3002TriPro-ReID\u6846\u67b6\u5229\u7528\u4e86RGB\u5e27\u548c\u4e8b\u4ef6\u6d41\u4e2d\u7684\u89c6\u89c9\u7279\u5f81\uff0c\u5e76\u7ed3\u5408\u4e86\u884c\u4eba\u5c5e\u6027\u4f5c\u4e3a\u4e2d\u7ea7\u8bed\u4e49\u7279\u5f81\uff0c\u4ee5\u589e\u5f3a\u7279\u5f81\u5b66\u4e60\u3002\u5728EvReID\u548cMARS\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2507.13977", "categories": ["cs.CL", "eess.AS", "I.5.1"], "pdf": "https://arxiv.org/pdf/2507.13977", "abs": "https://arxiv.org/abs/2507.13977", "authors": ["Lilit Grigoryan", "Nikolay Karpov", "Enas Albasiri", "Vitaly Lavrukhin", "Boris Ginsburg"], "title": "Open Automatic Speech Recognition Models for Classical and Modern Standard Arabic", "comment": "Accepted to ICASSP 2025", "summary": "Despite Arabic being one of the most widely spoken languages, the development\nof Arabic Automatic Speech Recognition (ASR) systems faces significant\nchallenges due to the language's complexity, and only a limited number of\npublic Arabic ASR models exist. While much of the focus has been on Modern\nStandard Arabic (MSA), there is considerably less attention given to the\nvariations within the language. This paper introduces a universal methodology\nfor Arabic speech and text processing designed to address unique challenges of\nthe language. Using this methodology, we train two novel models based on the\nFastConformer architecture: one designed specifically for MSA and the other,\nthe first unified public model for both MSA and Classical Arabic (CA). The MSA\nmodel sets a new benchmark with state-of-the-art (SOTA) performance on related\ndatasets, while the unified model achieves SOTA accuracy with diacritics for CA\nwhile maintaining strong performance for MSA. To promote reproducibility, we\nopen-source the models and their training recipes.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u963f\u62c9\u4f2f\u8bed\u8bed\u97f3\u5904\u7406\u65b9\u6cd5\uff0c\u5e76\u8bad\u7ec3\u4e86\u652f\u6301\u73b0\u4ee3\u6807\u51c6\u963f\u62c9\u4f2f\u8bed\uff08MSA\uff09\u548c\u53e4\u5178\u963f\u62c9\u4f2f\u8bed\uff08CA\uff09\u7684\u65b0\u6a21\u578b\uff0c\u8fbe\u5230\u4e86SOTA\u6027\u80fd\uff0c\u5e76\u5df2\u5f00\u6e90\u3002", "motivation": "\u5c3d\u7ba1\u963f\u62c9\u4f2f\u8bed\u662f\u4f7f\u7528\u6700\u5e7f\u6cdb\u7684\u8bed\u8a00\u4e4b\u4e00\uff0c\u4f46\u7531\u4e8e\u5176\u590d\u6742\u6027\uff0c\u963f\u62c9\u4f2f\u8bed\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u7cfb\u7edf\u53d1\u5c55\u9762\u4e34\u6311\u6218\uff0c\u4e14\u516c\u5f00\u7684\u963f\u62c9\u4f2f\u8bedASR\u6a21\u578b\u6709\u9650\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u73b0\u4ee3\u6807\u51c6\u963f\u62c9\u4f2f\u8bed\uff08MSA\uff09\uff0c\u5bf9\u8bed\u8a00\u5185\u90e8\u7684\u53d8\u4f53\u5173\u6ce8\u8f83\u5c11\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u963f\u62c9\u4f2f\u8bed\u8bed\u97f3\u548c\u6587\u672c\u5904\u7406\u7684\u901a\u7528\u65b9\u6cd5\uff0c\u5e76\u4f7f\u7528FastConformer\u67b6\u6784\u8bad\u7ec3\u4e86\u4e24\u4e2a\u6a21\u578b\uff1a\u4e00\u4e2aMSA\u6a21\u578b\u548c\u4e00\u4e2a\u540c\u65f6\u652f\u6301MSA\u548cCA\u7684\u7edf\u4e00\u6a21\u578b\u3002", "result": "MSA\u6a21\u578b\u5728\u76f8\u5173\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e86\u65b0\u7684\u6700\u5148\u8fdb\uff08SOTA\uff09\u6027\u80fd\u3002\u7edf\u4e00\u6a21\u578b\u5728CA\u4e0a\u5b9e\u73b0\u4e86\u5e26\u97f3\u6807\u7684SOTA\u51c6\u786e\u7387\uff0c\u540c\u65f6\u5728MSA\u4e0a\u4e5f\u4fdd\u6301\u4e86\u5f3a\u52b2\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u4ecb\u7ecd\u4e86\u7528\u4e8e\u963f\u62c9\u4f2f\u8bed\u8bed\u97f3\u548c\u6587\u672c\u5904\u7406\u7684\u901a\u7528\u65b9\u6cd5\uff0c\u5e76\u57fa\u4e8eFastConformer\u67b6\u6784\u8bad\u7ec3\u4e86\u4e24\u79cd\u65b0\u578b\u6a21\u578b\uff1a\u4e00\u79cd\u4e13\u95e8\u7528\u4e8e\u73b0\u4ee3\u6807\u51c6\u963f\u62c9\u4f2f\u8bed\uff08MSA\uff09\uff0c\u53e6\u4e00\u79cd\u662f\u9996\u4e2a\u540c\u65f6\u652f\u6301MSA\u548c\u53e4\u5178\u963f\u62c9\u4f2f\u8bed\uff08CA\uff09\u7684\u7edf\u4e00\u516c\u5171\u6a21\u578b\u3002MSA\u6a21\u578b\u5728\u76f8\u5173\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e86\u65b0\u7684\u6700\u5148\u8fdb\uff08SOTA\uff09\u6027\u80fd\uff0c\u800c\u7edf\u4e00\u6a21\u578b\u5728CA\u4e0a\u5b9e\u73b0\u4e86\u5e26\u97f3\u6807\u7684SOTA\u51c6\u786e\u7387\uff0c\u540c\u65f6\u5728MSA\u4e0a\u4e5f\u4fdd\u6301\u4e86\u5f3a\u52b2\u6027\u80fd\u3002\u4e3a\u4fc3\u8fdb\u7814\u7a76\u53ef\u590d\u73b0\u6027\uff0c\u7814\u7a76\u5f00\u6e90\u4e86\u6a21\u578b\u53ca\u5176\u8bad\u7ec3\u65b9\u6cd5\u3002"}}
{"id": "2507.13912", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13912", "abs": "https://arxiv.org/abs/2507.13912", "authors": ["Kevin Dradjat", "Massinissa Hamidi", "Pierre Bartet", "Blaise Hanczar"], "title": "Self-supervised learning on gene expression data", "comment": null, "summary": "Predicting phenotypes from gene expression data is a crucial task in\nbiomedical research, enabling insights into disease mechanisms, drug responses,\nand personalized medicine. Traditional machine learning and deep learning rely\non supervised learning, which requires large quantities of labeled data that\nare costly and time-consuming to obtain in the case of gene expression data.\nSelf-supervised learning has recently emerged as a promising approach to\novercome these limitations by extracting information directly from the\nstructure of unlabeled data. In this study, we investigate the application of\nstate-of-the-art self-supervised learning methods to bulk gene expression data\nfor phenotype prediction. We selected three self-supervised methods, based on\ndifferent approaches, to assess their ability to exploit the inherent structure\nof the data and to generate qualitative representations which can be used for\ndownstream predictive tasks. By using several publicly available gene\nexpression datasets, we demonstrate how the selected methods can effectively\ncapture complex information and improve phenotype prediction accuracy. The\nresults obtained show that self-supervised learning methods can outperform\ntraditional supervised models besides offering significant advantage by\nreducing the dependency on annotated data. We provide a comprehensive analysis\nof the performance of each method by highlighting their strengths and\nlimitations. We also provide recommendations for using these methods depending\non the case under study. Finally, we outline future research directions to\nenhance the application of self-supervised learning in the field of gene\nexpression data analysis. This study is the first work that deals with bulk\nRNA-Seq data and self-supervised learning.", "AI": {"tldr": "\u7814\u7a76\u4e86\u4e09\u79cd\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u5728\u6563\u88c5\u57fa\u56e0\u8868\u8fbe\u6570\u636e\u8868\u578b\u9884\u6d4b\u4e2d\u7684\u5e94\u7528\uff0c\u53d1\u73b0\u5b83\u4eec\u4f18\u4e8e\u4f20\u7edf\u76d1\u7763\u65b9\u6cd5\uff0c\u5e76\u51cf\u5c11\u4e86\u5bf9\u6807\u6ce8\u6570\u636e\u7684\u9700\u6c42\u3002", "motivation": "\u4e3a\u514b\u670d\u4f20\u7edf\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u5728\u57fa\u56e0\u8868\u8fbe\u6570\u636e\u4e2d\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\u7684\u9650\u5236\uff0c\u63a2\u7d22\u81ea\u76d1\u7763\u5b66\u4e60\u5728\u8868\u578b\u9884\u6d4b\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u672c\u7814\u7a76\u8c03\u67e5\u4e86\u4e09\u79cd\u57fa\u4e8e\u4e0d\u540c\u65b9\u6cd5\u3001\u6700\u5148\u8fdb\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u5728\u6563\u88c5\u57fa\u56e0\u8868\u8fbe\u6570\u636e\u4e0a\u7684\u5e94\u7528\uff0c\u4ee5\u8bc4\u4f30\u5b83\u4eec\u5229\u7528\u6570\u636e\u56fa\u6709\u7ed3\u6784\u548c\u751f\u6210\u7528\u4e8e\u4e0b\u6e38\u9884\u6d4b\u4efb\u52a1\u7684\u5b9a\u6027\u8868\u5f81\u7684\u80fd\u529b\u3002", "result": "\u6240\u9009\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u6355\u6349\u590d\u6742\u4fe1\u606f\u5e76\u63d0\u9ad8\u8868\u578b\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u76d1\u7763\u6a21\u578b\u3002", "conclusion": "\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u5728\u57fa\u56e0\u8868\u8fbe\u6570\u636e\u5206\u6790\u4e2d\u8868\u73b0\u51fa\u4f18\u4e8e\u4f20\u7edf\u76d1\u7763\u5b66\u4e60\u6a21\u578b\u7684\u6f5c\u529b\uff0c\u80fd\u591f\u6709\u6548\u6355\u6349\u590d\u6742\u4fe1\u606f\u5e76\u63d0\u9ad8\u8868\u578b\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u5bf9\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\u3002"}}
{"id": "2507.13663", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13663", "abs": "https://arxiv.org/abs/2507.13663", "authors": ["Xingyu Jiang", "Ning Gao", "Hongkun Dou", "Xiuhui Zhang", "Xiaoqing Zhong", "Yue Deng", "Hongjue Li"], "title": "Global Modeling Matters: A Fast, Lightweight and Effective Baseline for Efficient Image Restoration", "comment": null, "summary": "Natural image quality is often degraded by adverse weather conditions,\nsignificantly impairing the performance of downstream tasks. Image restoration\nhas emerged as a core solution to this challenge and has been widely discussed\nin the literature. Although recent transformer-based approaches have made\nremarkable progress in image restoration, their increasing system complexity\nposes significant challenges for real-time processing, particularly in\nreal-world deployment scenarios. To this end, most existing methods attempt to\nsimplify the self-attention mechanism, such as by channel self-attention or\nstate space model. However, these methods primarily focus on network\narchitecture while neglecting the inherent characteristics of image restoration\nitself. In this context, we explore a pyramid Wavelet-Fourier iterative\npipeline to demonstrate the potential of Wavelet-Fourier processing for image\nrestoration. Inspired by the above findings, we propose a novel and efficient\nrestoration baseline, named Pyramid Wavelet-Fourier Network (PW-FNet).\nSpecifically, PW-FNet features two key design principles: 1) at the inter-block\nlevel, integrates a pyramid wavelet-based multi-input multi-output structure to\nachieve multi-scale and multi-frequency bands decomposition; and 2) at the\nintra-block level, incorporates Fourier transforms as an efficient alternative\nto self-attention mechanisms, effectively reducing computational complexity\nwhile preserving global modeling capability. Extensive experiments on tasks\nsuch as image deraining, raindrop removal, image super-resolution, motion\ndeblurring, image dehazing, image desnowing and underwater/low-light\nenhancement demonstrate that PW-FNet not only surpasses state-of-the-art\nmethods in restoration quality but also achieves superior efficiency, with\nsignificantly reduced parameter size, computational cost and inference time.", "AI": {"tldr": "PW-FNet \u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u56fe\u50cf\u6062\u590d\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u91d1\u5b57\u5854\u5c0f\u6ce2\u591a\u5c3a\u5ea6/\u591a\u9891\u5e26\u5206\u89e3\u548c\u5085\u91cc\u53f6\u53d8\u6362\u66ff\u4ee3\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5728\u591a\u79cd\u6062\u590d\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u548c\u9ad8\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e Transformer \u7684\u56fe\u50cf\u6062\u590d\u65b9\u6cd5\u867d\u7136\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5176\u65e5\u76ca\u589e\u957f\u7684\u7cfb\u7edf\u590d\u6742\u6027\u5bf9\u5b9e\u65f6\u5904\u7406\u63d0\u51fa\u4e86\u91cd\u5927\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u5927\u591a\u5c1d\u8bd5\u7b80\u5316\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4f46\u4e3b\u8981\u5173\u6ce8\u7f51\u7edc\u67b6\u6784\uff0c\u5ffd\u7565\u4e86\u56fe\u50cf\u6062\u590d\u672c\u8eab\u7684\u56fa\u6709\u7279\u6027\u3002\u56e0\u6b64\uff0c\u63a2\u7d22\u91d1\u5b57\u5854\u5c0f\u6ce2-\u5085\u91cc\u53f6\u8fed\u4ee3\u6d41\u7a0b\u4ee5\u5c55\u793a\u5c0f\u6ce2-\u5085\u91cc\u53f6\u5904\u7406\u5728\u56fe\u50cf\u6062\u590d\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u91d1\u5b57\u5854\u5c0f\u6ce2-\u5085\u91cc\u53f6\u7f51\u7edc\uff08PW-FNet\uff09\u7684\u65b0\u578b\u9ad8\u6548\u6062\u590d\u57fa\u7ebf\u3002PW-FNet \u5177\u6709\u4e24\u4e2a\u5173\u952e\u7684\u8bbe\u8ba1\u539f\u5219\uff1a1\uff09\u5728\u5757\u95f4\u5c42\u9762\uff0c\u96c6\u6210\u4e86\u57fa\u4e8e\u91d1\u5b57\u5854\u5c0f\u6ce2\u7684\u591a\u8f93\u5165\u591a\u8f93\u51fa\u7ed3\u6784\uff0c\u4ee5\u5b9e\u73b0\u591a\u5c3a\u5ea6\u548c\u591a\u9891\u5e26\u5206\u89e3\uff1b2\uff09\u5728\u5757\u5185\u5c42\u9762\uff0c\u5c06\u5085\u91cc\u53f6\u53d8\u6362\u4f5c\u4e3a\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684\u6709\u6548\u66ff\u4ee3\u65b9\u6848\uff0c\u5728\u6709\u6548\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u540c\u65f6\u4fdd\u7559\u4e86\u5168\u5c40\u5efa\u6a21\u80fd\u529b\u3002", "result": "PW-FNet \u4e0d\u4ec5\u5728\u6062\u590d\u8d28\u91cf\u4e0a\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u800c\u4e14\u5728\u6548\u7387\u65b9\u9762\u4e5f\u8868\u73b0\u51fa\u8272\uff0c\u53c2\u6570\u91cf\u3001\u8ba1\u7b97\u6210\u672c\u548c\u63a8\u7406\u65f6\u95f4\u5747\u663e\u8457\u51cf\u5c11\u3002", "conclusion": "PW-FNet \u5728\u56fe\u50cf\u6062\u590d\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u6062\u590d\u8d28\u91cf\uff0c\u540c\u65f6\u5728\u6548\u7387\u65b9\u9762\u4e5f\u8868\u73b0\u51fa\u8272\uff0c\u53c2\u6570\u91cf\u3001\u8ba1\u7b97\u6210\u672c\u548c\u63a8\u7406\u65f6\u95f4\u5747\u663e\u8457\u51cf\u5c11\u3002"}}
{"id": "2507.14017", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14017", "abs": "https://arxiv.org/abs/2507.14017", "authors": ["Haoyu He", "Haozheng Luo", "Yan Chen", "Qi R. Wang"], "title": "Efficient Temporal Tokenization for Mobility Prediction with Large Language Models", "comment": null, "summary": "We introduce RHYTHM (Reasoning with Hierarchical Temporal Tokenization for\nHuman Mobility), a framework that leverages large language models (LLMs) as\nspatio-temporal predictors and trajectory reasoners. RHYTHM partitions\ntrajectories into daily segments encoded as discrete tokens with hierarchical\nattention, capturing both daily and weekly dependencies while substantially\nreducing the sequence length. Token representations are enriched with\npre-computed prompt embeddings via a frozen LLM, enhancing the model's ability\nto capture interdependencies without extensive computational overhead. By\nfreezing the LLM backbone, RHYTHM achieves significant computational\nefficiency. Evaluation on three real-world datasets demonstrates a 2.4%\nimprovement in accuracy, 5.0% increase on weekends, and 24.6% reduction in\ntraining time compared to state-of-the-art methods.", "AI": {"tldr": "RHYTHM\u662f\u4e00\u4e2a\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u8f68\u8ff9\u9884\u6d4b\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u65f6\u95f4\u6807\u8bb0\u5316\u6280\u672f\uff0c\u5728\u63d0\u9ad8\u51c6\u786e\u7387\u548c\u6548\u7387\u7684\u540c\u65f6\uff0c\u7f29\u77ed\u4e86\u5e8f\u5217\u957f\u5ea6\u3002", "motivation": "\u5728\u4eba\u7c7b\u8f68\u8ff9\u9884\u6d4b\u548c\u63a8\u7406\u9886\u57df\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4f5c\u4e3a\u65f6\u7a7a\u9884\u6d4b\u5668\u548c\u8f68\u8ff9\u63a8\u7406\u5668\uff0c\u4ee5\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "method": "RHYTHM\u6846\u67b6\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4f5c\u4e3a\u65f6\u7a7a\u9884\u6d4b\u5668\u548c\u8f68\u8ff9\u63a8\u7406\u5668\u3002\u8be5\u6846\u67b6\u5c06\u8f68\u8ff9\u5212\u5206\u4e3a\u6bcf\u65e5\u7247\u6bb5\uff0c\u5e76\u4f7f\u7528\u5206\u5c42\u6ce8\u610f\u529b\u5c06\u5176\u7f16\u7801\u4e3a\u79bb\u6563\u6807\u8bb0\uff0c\u4ece\u800c\u540c\u65f6\u6355\u6349\u6bcf\u65e5\u548c\u6bcf\u5468\u7684\u4f9d\u8d56\u5173\u7cfb\uff0c\u5e76\u663e\u8457\u7f29\u77ed\u5e8f\u5217\u957f\u5ea6\u3002\u901a\u8fc7\u4e00\u4e2a\u51bb\u7ed3\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5c06\u9884\u5148\u8ba1\u7b97\u7684\u63d0\u793a\u5d4c\u5165\u6dfb\u52a0\u5230\u6807\u8bb0\u8868\u793a\u4e2d\uff0c\u4ee5\u589e\u5f3a\u6a21\u578b\u5728\u4e0d\u589e\u52a0\u5927\u91cf\u8ba1\u7b97\u5f00\u9500\u7684\u60c5\u51b5\u4e0b\u6355\u6349\u76f8\u4e92\u4f9d\u8d56\u5173\u7cfb\u7684\u80fd\u529b\u3002", "result": "\u4e0e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u6bd4\uff0c\u51c6\u786e\u7387\u63d0\u9ad8\u4e862.4%\uff0c\u5468\u672b\u51c6\u786e\u7387\u63d0\u9ad8\u4e865.0%\uff0c\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c11\u4e8624.6%\u3002", "conclusion": "RHYTHM\u6846\u67b6\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u4e0e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u6bd4\uff0c\u51c6\u786e\u7387\u63d0\u9ad8\u4e862.4%\uff0c\u5468\u672b\u51c6\u786e\u7387\u63d0\u9ad8\u4e865.0%\uff0c\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c11\u4e8624.6%\uff0c\u5728\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6210\u6548\u3002"}}
{"id": "2507.13920", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13920", "abs": "https://arxiv.org/abs/2507.13920", "authors": ["Turan Orujlu", "Christian Gumbsch", "Martin V. Butz", "Charley M Wu"], "title": "Reframing attention as a reinforcement learning problem for causal discovery", "comment": null, "summary": "Formal frameworks of causality have operated largely parallel to modern\ntrends in deep reinforcement learning (RL). However, there has been a revival\nof interest in formally grounding the representations learned by neural\nnetworks in causal concepts. Yet, most attempts at neural models of causality\nassume static causal graphs and ignore the dynamic nature of causal\ninteractions. In this work, we introduce Causal Process framework as a novel\ntheory for representing dynamic hypotheses about causal structure. Furthermore,\nwe present Causal Process Model as an implementation of this framework. This\nallows us to reformulate the attention mechanism popularized by Transformer\nnetworks within an RL setting with the goal to infer interpretable causal\nprocesses from visual observations. Here, causal inference corresponds to\nconstructing a causal graph hypothesis which itself becomes an RL task nested\nwithin the original RL problem. To create an instance of such hypothesis, we\nemploy RL agents. These agents establish links between units similar to the\noriginal Transformer attention mechanism. We demonstrate the effectiveness of\nour approach in an RL environment where we outperform current alternatives in\ncausal representation learning and agent performance, and uniquely recover\ngraphs of dynamic causal processes.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u56e0\u679c\u8fc7\u7a0b\u6846\u67b6\u548c\u6a21\u578b\uff0c\u7528\u4e8e\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u63a8\u65ad\u52a8\u6001\u56e0\u679c\u5173\u7cfb\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u6548\u679c\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u5f25\u5408\u56e0\u679c\u6846\u67b6\u4e0e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5e76\u89e3\u51b3\u73b0\u6709\u795e\u7ecf\u56e0\u679c\u6a21\u578b\u5ffd\u7565\u52a8\u6001\u56e0\u679c\u76f8\u4e92\u4f5c\u7528\u7684\u95ee\u9898\u3002", "method": "\u672c\u7814\u7a76\u5c06\u56e0\u679c\u63a8\u65ad\u91cd\u65b0\u8868\u8ff0\u4e3a\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u6765\u6784\u5efa\u56e0\u679c\u56fe\u5047\u8bbe\uff0c\u5e76\u501f\u9274 Transformer \u7684\u6ce8\u610f\u529b\u673a\u5236\u6765\u5efa\u7acb\u5355\u5143\u4e4b\u95f4\u7684\u8054\u7cfb\u3002", "result": "\u5728\u5f3a\u5316\u5b66\u4e60\u73af\u5883\u4e2d\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u56e0\u679c\u8868\u793a\u5b66\u4e60\u548c\u667a\u80fd\u4f53\u6027\u80fd\u65b9\u9762\u4f18\u4e8e\u5f53\u524d\u66ff\u4ee3\u65b9\u6cd5\uff0c\u5e76\u80fd\u6062\u590d\u52a8\u6001\u56e0\u679c\u8fc7\u7a0b\u56fe\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u56e0\u679c\u8fc7\u7a0b\u6846\u67b6\u548c\u56e0\u679c\u8fc7\u7a0b\u6a21\u578b\uff0c\u7528\u4e8e\u8868\u793a\u52a8\u6001\u56e0\u679c\u7ed3\u6784\u5047\u8bbe\uff0c\u5e76\u5728\u5f3a\u5316\u5b66\u4e60\u8bbe\u7f6e\u4e2d\u5b9e\u73b0\u4e86 Transformer \u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4ee5\u4ece\u89c6\u89c9\u89c2\u5bdf\u4e2d\u63a8\u65ad\u53ef\u89e3\u91ca\u7684\u56e0\u679c\u8fc7\u7a0b\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u56e0\u679c\u8868\u793a\u5b66\u4e60\u548c\u667a\u80fd\u4f53\u6027\u80fd\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u80fd\u6062\u590d\u52a8\u6001\u56e0\u679c\u8fc7\u7a0b\u56fe\u3002"}}
{"id": "2507.13673", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13673", "abs": "https://arxiv.org/abs/2507.13673", "authors": ["Yuechen Xie", "Haobo Jiang", "Jian Yang", "Yigong Zhang", "Jin Xie"], "title": "MaskHOI: Robust 3D Hand-Object Interaction Estimation via Masked Pre-training", "comment": "10 pages, 8 figures, 6 tables", "summary": "In 3D hand-object interaction (HOI) tasks, estimating precise joint poses of\nhands and objects from monocular RGB input remains highly challenging due to\nthe inherent geometric ambiguity of RGB images and the severe mutual occlusions\nthat occur during interaction.To address these challenges, we propose MaskHOI,\na novel Masked Autoencoder (MAE)-driven pretraining framework for enhanced HOI\npose estimation. Our core idea is to leverage the masking-then-reconstruction\nstrategy of MAE to encourage the feature encoder to infer missing spatial and\nstructural information, thereby facilitating geometric-aware and\nocclusion-robust representation learning. Specifically, based on our\nobservation that human hands exhibit far greater geometric complexity than\nrigid objects, conventional uniform masking fails to effectively guide the\nreconstruction of fine-grained hand structures. To overcome this limitation, we\nintroduce a Region-specific Mask Ratio Allocation, primarily comprising the\nregion-specific masking assignment and the skeleton-driven hand masking\nguidance. The former adaptively assigns lower masking ratios to hand regions\nthan to rigid objects, balancing their feature learning difficulty, while the\nlatter prioritizes masking critical hand parts (e.g., fingertips or entire\nfingers) to realistically simulate occlusion patterns in real-world\ninteractions. Furthermore, to enhance the geometric awareness of the pretrained\nencoder, we introduce a novel Masked Signed Distance Field (SDF)-driven\nmultimodal learning mechanism. Through the self-masking 3D SDF prediction, the\nlearned encoder is able to perceive the global geometric structure of hands and\nobjects beyond the 2D image plane, overcoming the inherent limitations of\nmonocular input and alleviating self-occlusion issues. Extensive experiments\ndemonstrate that our method significantly outperforms existing state-of-the-art\napproaches.", "AI": {"tldr": "MaskHOI\u662f\u4e00\u4e2a\u57fa\u4e8eMAE\u7684\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u533a\u57df\u7279\u5b9a\u7684\u63a9\u7801\u7b56\u7565\u548cSDF\u591a\u6a21\u6001\u5b66\u4e60\uff0c\u63d0\u5347\u4e86\u5355\u76eeRGB\u8f93\u5165\u4e0b3D\u624b\u90e8-\u7269\u4f53\u4ea4\u4e92\u59ff\u52bf\u4f30\u8ba1\u7684\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u5355\u76eeRGB\u8f93\u5165\u57283D\u624b\u90e8-\u7269\u4f53\u4ea4\u4e92\uff08HOI\uff09\u4efb\u52a1\u4e2d\u5b58\u5728\u51e0\u4f55\u6b67\u4e49\u548c\u76f8\u4e92\u906e\u6321\u7684\u6311\u6218\uff0c\u5bfc\u81f4\u7cbe\u786e\u4f30\u8ba1\u624b\u548c\u7269\u4f53\u5173\u8282\u59ff\u52bf\u975e\u5e38\u56f0\u96be\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u624b\u90e8\u7cbe\u7ec6\u7ed3\u6784\u548c\u771f\u5b9e\u906e\u6321\u573a\u666f\u65f6\u5b58\u5728\u4e0d\u8db3\u3002", "method": "MaskHOI\u6846\u67b6\u7684\u6838\u5fc3\u601d\u60f3\u662f\u5229\u7528MAE\u7684\u63a9\u7801-\u91cd\u5efa\u7b56\u7565\u6765\u5b66\u4e60\u51e0\u4f55\u611f\u77e5\u548c\u906e\u6321\u9c81\u68d2\u7684\u8868\u793a\u3002\u5177\u4f53\u521b\u65b0\u70b9\u5305\u62ec\uff1a1. \u533a\u57df\u7279\u5b9a\u63a9\u7801\u7387\u5206\u914d\uff1a\u4e3a\u624b\u90e8\u533a\u57df\u5206\u914d\u6bd4\u521a\u6027\u7269\u4f53\u66f4\u4f4e\u7684\u63a9\u7801\u7387\uff0c\u4ee5\u9002\u5e94\u624b\u90e8\u7ed3\u6784\u66f4\u590d\u6742\u7684\u7279\u70b9\u30022. \u9aa8\u67b6\u9a71\u52a8\u7684\u624b\u90e8\u63a9\u7801\uff1a\u4f18\u5148\u63a9\u7801\u5173\u952e\u624b\u90e8\u533a\u57df\uff08\u5982\u6307\u5c16\u6216\u624b\u6307\uff09\u4ee5\u6a21\u62df\u771f\u5b9e\u4ea4\u4e92\u4e2d\u7684\u906e\u6321\u30023. \u63a9\u7801\u7b26\u53f7\u8ddd\u79bb\u573a\uff08SDF\uff09\u9a71\u52a8\u7684\u591a\u6a21\u6001\u5b66\u4e60\uff1a\u901a\u8fc7\u81ea\u63a9\u78013D SDF\u9884\u6d4b\uff0c\u4f7f\u7f16\u7801\u5668\u80fd\u591f\u611f\u77e5\u8d85\u8d8a2D\u56fe\u50cf\u7684\u5168\u5c40\u51e0\u4f55\u7ed3\u6784\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMaskHOI\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u80fd\u591f\u66f4\u597d\u5730\u5904\u74063D\u624b\u90e8-\u7269\u4f53\u4ea4\u4e92\u4e2d\u7684\u59ff\u52bf\u4f30\u8ba1\u95ee\u9898\u3002", "conclusion": "MaskHOI\u901a\u8fc7\u63a9\u7801\u81ea\u52a8\u7f16\u7801\u5668\uff08MAE\uff09\u9a71\u52a8\u7684\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5355\u76eeRGB\u8f93\u5165\u57283D\u624b\u90e8-\u7269\u4f53\u4ea4\u4e92\uff08HOI\uff09\u4efb\u52a1\u4e2d\u4f30\u8ba1\u7cbe\u786e\u5173\u8282\u59ff\u52bf\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u51e0\u4f55\u6b67\u4e49\u548c\u906e\u6321\u95ee\u9898\u65b9\u9762\u3002"}}
{"id": "2507.14022", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14022", "abs": "https://arxiv.org/abs/2507.14022", "authors": ["Jianfei Li", "Kevin Kam Fung Yuen"], "title": "CPC-CMS: Cognitive Pairwise Comparison Classification Model Selection Framework for Document-level Sentiment Analysis", "comment": "35 pages, 33 tables, 6 Figures", "summary": "This study proposes the Cognitive Pairwise Comparison Classification Model\nSelection (CPC-CMS) framework for document-level sentiment analysis. The CPC,\nbased on expert knowledge judgment, is used to calculate the weights of\nevaluation criteria, including accuracy, precision, recall, F1-score,\nspecificity, Matthews Correlation Coefficient (MCC), Cohen's Kappa (Kappa), and\nefficiency. Naive Bayes, Linear Support Vector Classification (LSVC), Random\nForest, Logistic Regression, Extreme Gradient Boosting (XGBoost), Long\nShort-Term Memory (LSTM), and A Lite Bidirectional Encoder Representations from\nTransformers (ALBERT) are chosen as classification baseline models. A weighted\ndecision matrix consisting of classification evaluation scores with respect to\ncriteria weights, is formed to select the best classification model for a\nclassification problem. Three open datasets of social media are used to\ndemonstrate the feasibility of the proposed CPC-CMS. Based on our simulation,\nfor evaluation results excluding the time factor, ALBERT is the best for the\nthree datasets; if time consumption is included, no single model always\nperforms better than the other models. The CPC-CMS can be applied to the other\nclassification applications in different areas.", "AI": {"tldr": "\u63d0\u51faCPC-CMS\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u4e13\u5bb6\u77e5\u8bc6\u548c\u591a\u79cd\u8bc4\u4f30\u6307\u6807\uff08\u51c6\u786e\u7387\u3001\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u3001F1\u5206\u6570\u3001\u7279\u5f02\u5ea6\u3001MCC\u3001Kappa\u3001\u6548\u7387\uff09\u7684\u6743\u91cd\uff0c\u4e3a\u6587\u6863\u7ea7\u60c5\u611f\u5206\u6790\u9009\u62e9\u6700\u4f73\u5206\u7c7b\u6a21\u578b\u3002\u5b9e\u9a8c\u53d1\u73b0ALBERT\u5728\u4e0d\u8003\u8651\u65f6\u95f4\u65f6\u8868\u73b0\u6700\u597d\uff0c\u4f46\u8003\u8651\u65f6\u95f4\u56e0\u7d20\u5219\u65e0\u6700\u4f18\u6a21\u578b\u3002\u8be5\u6846\u67b6\u53ef\u63a8\u5e7f\u81f3\u5176\u4ed6\u5206\u7c7b\u5e94\u7528\u3002", "motivation": "\u4e3a\u6587\u6863\u7ea7\u60c5\u611f\u5206\u6790\u63d0\u4f9b\u4e00\u79cd\u6a21\u578b\u9009\u62e9\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8ba4\u77e5\u6210\u5bf9\u6bd4\u8f83\u5206\u7c7b\u6a21\u578b\u9009\u62e9\uff08CPC-CMS\uff09\u6846\u67b6\uff0c\u4f7f\u7528\u4e13\u5bb6\u77e5\u8bc6\u5224\u65ad\u6765\u8ba1\u7b97\u5305\u62ec\u51c6\u786e\u7387\u3001\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u3001F1\u5206\u6570\u3001\u7279\u5f02\u5ea6\u3001Matthews\u76f8\u5173\u7cfb\u6570\uff08MCC\uff09\u3001Cohen's Kappa\uff08Kappa\uff09\u548c\u6548\u7387\u5728\u5185\u7684\u8bc4\u4f30\u6807\u51c6\u7684\u6743\u91cd\u3002\u5c06\u6734\u7d20\u8d1d\u53f6\u65af\u3001\u7ebf\u6027\u652f\u6301\u5411\u91cf\u5206\u7c7b\uff08LSVC\uff09\u3001\u968f\u673a\u68ee\u6797\u3001\u903b\u8f91\u56de\u5f52\u3001\u6781\u7aef\u68af\u5ea6\u63d0\u5347\uff08XGBoost\uff09\u3001\u957f\u77ed\u671f\u8bb0\u5fc6\uff08LSTM\uff09\u548cALBERT\u4f5c\u4e3a\u5206\u7c7b\u57fa\u7ebf\u6a21\u578b\u3002\u901a\u8fc7\u6784\u5efa\u5305\u542b\u5206\u7c7b\u8bc4\u4f30\u5206\u6570\u548c\u6807\u51c6\u6743\u91cd\u7684\u52a0\u6743\u51b3\u7b56\u77e9\u9635\u6765\u9009\u62e9\u6700\u4f73\u5206\u7c7b\u6a21\u578b\u3002", "result": "\u4f7f\u7528\u4e09\u4e2a\u516c\u5f00\u7684\u793e\u4ea4\u5a92\u4f53\u6570\u636e\u96c6\u8fdb\u884c\u5b9e\u9a8c\u3002\u7ed3\u679c\u8868\u660e\uff0c\u4e0d\u8003\u8651\u65f6\u95f4\u56e0\u7d20\u65f6\uff0cALBERT\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e2d\u8868\u73b0\u6700\u4f73\uff1b\u82e5\u8003\u8651\u65f6\u95f4\u6d88\u8017\uff0c\u5219\u6ca1\u6709\u5355\u4e00\u6a21\u578b\u80fd\u5728\u6240\u6709\u60c5\u51b5\u4e0b\u90fd\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\u3002", "conclusion": "CPC-CMS\u6846\u67b6\u53ef\u5e94\u7528\u4e8e\u5176\u4ed6\u9886\u57df\u4e2d\u7684\u5206\u7c7b\u95ee\u9898\u3002"}}
{"id": "2507.13950", "categories": ["cs.LG", "physics.bio-ph", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2507.13950", "abs": "https://arxiv.org/abs/2507.13950", "authors": ["Jingbo Liang", "Bruna Jacobson"], "title": "MoDyGAN: Combining Molecular Dynamics With GANs to Investigate Protein Conformational Space", "comment": null, "summary": "Extensively exploring protein conformational landscapes remains a major\nchallenge in computational biology due to the high computational cost involved\nin dynamic physics-based simulations. In this work, we propose a novel\npipeline, MoDyGAN, that leverages molecular dynamics (MD) simulations and\ngenerative adversarial networks (GANs) to explore protein conformational\nspaces. MoDyGAN contains a generator that maps Gaussian distributions into\nMD-derived protein trajectories, and a refinement module that combines ensemble\nlearning with a dual-discriminator to further improve the plausibility of\ngenerated conformations. Central to our approach is an innovative\nrepresentation technique that reversibly transforms 3D protein structures into\n2D matrices, enabling the use of advanced image-based GAN architectures. We use\nthree rigid proteins to demonstrate that MoDyGAN can generate plausible new\nconformations. We also use deca-alanine as a case study to show that\ninterpolations within the latent space closely align with trajectories obtained\nfrom steered molecular dynamics (SMD) simulations. Our results suggest that\nrepresenting proteins as image-like data unlocks new possibilities for applying\nadvanced deep learning techniques to biomolecular simulation, leading to an\nefficient sampling of conformational states. Additionally, the proposed\nframework holds strong potential for extension to other complex 3D structures.", "AI": {"tldr": "MoDyGAN\u5229\u7528MD\u6a21\u62df\u548cGANs\uff0c\u5c06\u86cb\u767d\u8d28\u7ed3\u6784\u8868\u793a\u4e3a2D\u77e9\u9635\uff0c\u6709\u6548\u63a2\u7d22\u86cb\u767d\u8d28\u6784\u8c61\u7a7a\u95f4\uff0c\u751f\u6210\u65b0\u7684\u6784\u8c61\uff0c\u5e76\u4f7f\u6f5c\u5728\u7a7a\u95f4\u63d2\u503c\u4e0eSMD\u7ed3\u679c\u4e00\u81f4\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u8ba1\u7b97\u751f\u7269\u5b66\u4e2d\u7531\u4e8e\u57fa\u4e8e\u7269\u7406\u7684\u52a8\u6001\u6a21\u62df\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u800c\u5e7f\u6cdb\u63a2\u7d22\u86cb\u767d\u8d28\u6784\u8c61\u666f\u89c2\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMoDyGAN\u7684\u65b0\u578b\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u5206\u5b50\u52a8\u529b\u5b66\uff08MD\uff09\u6a21\u62df\u548c\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GANs\uff09\u3002MoDyGAN\u5305\u542b\u4e00\u4e2a\u751f\u6210\u5668\uff0c\u5c06\u9ad8\u65af\u5206\u5e03\u6620\u5c04\u5230MD\u884d\u751f\u7684\u86cb\u767d\u8d28\u8f68\u8ff9\uff0c\u4ee5\u53ca\u4e00\u4e2a\u5305\u542b\u96c6\u6210\u5b66\u4e60\u548c\u53cc\u5224\u522b\u5668\u7684\u7cbe\u70bc\u6a21\u5757\uff0c\u7528\u4e8e\u63d0\u9ad8\u751f\u6210\u6784\u8c61\u7684\u53ef\u4fe1\u5ea6\u3002\u5176\u6838\u5fc3\u662f\u4e00\u79cd\u521b\u65b0\u7684\u8868\u793a\u6280\u672f\uff0c\u5c063D\u86cb\u767d\u8d28\u7ed3\u6784\u53ef\u9006\u5730\u8f6c\u6362\u4e3a2D\u77e9\u9635\uff0c\u4ece\u800c\u80fd\u591f\u5e94\u7528\u57fa\u4e8e\u56fe\u50cf\u7684GANs\u67b6\u6784\u3002", "result": "\u4f7f\u7528\u4e09\u79cd\u521a\u6027\u86cb\u767d\u8d28\u8bc1\u660e\u4e86MoDyGAN\u53ef\u4ee5\u751f\u6210\u65b0\u7684\u5408\u7406\u6784\u8c61\u3002\u4f7f\u7528\u5341\u805a\u4e19\u6c28\u9178\u4f5c\u4e3a\u6848\u4f8b\u7814\u7a76\uff0c\u8868\u660e\u6f5c\u5728\u7a7a\u95f4\u7684\u63d2\u503c\u4e0e\u0997\u09a4\u09bf\u09b0\u5206\u5b50\u52a8\u529b\u5b66\uff08SMD\uff09\u6a21\u62df\u83b7\u5f97\u7684\u8f68\u8ff9\u5bc6\u5207\u4e00\u81f4\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684MoDyGAN\u6846\u67b6\u901a\u8fc7\u5c06\u86cb\u767d\u8d28\u7ed3\u6784\u8868\u793a\u4e3a\u7c7b\u56fe\u50cf\u6570\u636e\uff0c\u5e76\u7ed3\u5408MD\u6a21\u62df\u548cGANs\uff0c\u80fd\u591f\u6709\u6548\u5730\u63a2\u7d22\u86cb\u767d\u8d28\u6784\u8c61\u7a7a\u95f4\uff0c\u751f\u6210\u65b0\u7684\u5408\u7406\u6784\u8c61\uff0c\u5e76\u4f7f\u6f5c\u5728\u7a7a\u95f4\u7684\u63d2\u503c\u4e0eSMD\u6a21\u62df\u7ed3\u679c\u4fdd\u6301\u4e00\u81f4\u3002"}}
{"id": "2507.13677", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.13677", "abs": "https://arxiv.org/abs/2507.13677", "authors": ["Chuheng Wei", "Ziye Qin", "Walter Zimmer", "Guoyuan Wu", "Matthew J. Barth"], "title": "HeCoFuse: Cross-Modal Complementary V2X Cooperative Perception with Heterogeneous Sensors", "comment": "Ranked first in CVPR DriveX workshop TUM-Traf V2X challenge. Accepted\n  by ITSC2025", "summary": "Real-world Vehicle-to-Everything (V2X) cooperative perception systems often\noperate under heterogeneous sensor configurations due to cost constraints and\ndeployment variability across vehicles and infrastructure. This heterogeneity\nposes significant challenges for feature fusion and perception reliability. To\naddress these issues, we propose HeCoFuse, a unified framework designed for\ncooperative perception across mixed sensor setups where nodes may carry Cameras\n(C), LiDARs (L), or both. By introducing a hierarchical fusion mechanism that\nadaptively weights features through a combination of channel-wise and spatial\nattention, HeCoFuse can tackle critical challenges such as cross-modality\nfeature misalignment and imbalanced representation quality. In addition, an\nadaptive spatial resolution adjustment module is employed to balance\ncomputational cost and fusion effectiveness. To enhance robustness across\ndifferent configurations, we further implement a cooperative learning strategy\nthat dynamically adjusts fusion type based on available modalities. Experiments\non the real-world TUMTraf-V2X dataset demonstrate that HeCoFuse achieves 43.22%\n3D mAP under the full sensor configuration (LC+LC), outperforming the CoopDet3D\nbaseline by 1.17%, and reaches an even higher 43.38% 3D mAP in the L+LC\nscenario, while maintaining 3D mAP in the range of 21.74% to 43.38% across nine\nheterogeneous sensor configurations. These results, validated by our\nfirst-place finish in the CVPR 2025 DriveX challenge, establish HeCoFuse as the\ncurrent state-of-the-art on TUM-Traf V2X dataset while demonstrating robust\nperformance across diverse sensor deployments.", "AI": {"tldr": "HeCoFuse\u662f\u4e00\u4e2a\u7528\u4e8e\u5f02\u6784\u4f20\u611f\u5668V2X\u534f\u540c\u611f\u77e5\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u878d\u5408\u548c\u81ea\u9002\u5e94\u7b56\u7565\u63d0\u9ad8\u4e86\u6027\u80fd\u548c\u9c81\u68d2\u6027\uff0c\u5e76\u5728TUMTraf-V2X\u6570\u636e\u96c6\u548cCVPR 2025 DriveX\u6311\u6218\u8d5b\u4e2d\u53d6\u5f97\u4e86\u9886\u5148\u6210\u679c\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u7684V2X\u534f\u540c\u611f\u77e5\u7cfb\u7edf\u5e38\u5e38\u9762\u4e34\u5f02\u6784\u4f20\u611f\u5668\u914d\u7f6e\u5e26\u6765\u7684\u6311\u6218\uff0c\u8fd9\u7ed9\u7279\u5f81\u878d\u5408\u548c\u611f\u77e5\u53ef\u9760\u6027\u5e26\u6765\u4e86\u56f0\u96be\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHeCoFuse\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u7528\u4e8e\u8de8\u5f02\u6784\u4f20\u611f\u5668\u8bbe\u7f6e\uff08\u5305\u62ec\u6444\u50cf\u5934\u548c\u6fc0\u5149\u96f7\u8fbe\uff09\u7684\u534f\u540c\u611f\u77e5\u3002\u8be5\u6846\u67b6\u91c7\u7528\u5206\u5c42\u878d\u5408\u673a\u5236\uff0c\u901a\u8fc7\u901a\u9053\u548c\u7a7a\u95f4\u6ce8\u610f\u529b\u81ea\u9002\u5e94\u5730\u52a0\u6743\u7279\u5f81\uff0c\u89e3\u51b3\u4e86\u8de8\u6a21\u6001\u7279\u5f81\u4e0d\u5bf9\u9f50\u548c\u8868\u793a\u4e0d\u5e73\u8861\u7684\u95ee\u9898\u3002\u6b64\u5916\uff0c\u8fd8\u5305\u542b\u4e00\u4e2a\u81ea\u9002\u5e94\u7a7a\u95f4\u5206\u8fa8\u7387\u8c03\u6574\u6a21\u5757\u6765\u5e73\u8861\u8ba1\u7b97\u6210\u672c\u548c\u878d\u5408\u6548\u679c\uff0c\u4ee5\u53ca\u4e00\u4e2a\u534f\u540c\u5b66\u4e60\u7b56\u7565\u6765\u52a8\u6001\u8c03\u6574\u878d\u5408\u7c7b\u578b\u3002", "result": "\u5728TUMTraf-V2X\u6570\u636e\u96c6\u4e0a\uff0cHeCoFuse\u5728\u5168\u4f20\u611f\u5668\u914d\u7f6e\uff08LC+LC\uff09\u4e0b\u8fbe\u5230\u4e8643.22%\u76843D mAP\uff0c\u4f18\u4e8eCoopDet3D\u57fa\u7ebf1.17%\u3002\u5728L+LC\u573a\u666f\u4e0b\uff0c3D mAP\u8fbe\u5230\u4e8643.38%\uff0c\u5e76\u4e14\u5728\u4e5d\u79cd\u4e0d\u540c\u7684\u5f02\u6784\u4f20\u611f\u5668\u914d\u7f6e\u4e0b\uff0c3D mAP\u4fdd\u6301\u572821.74%\u523043.38%\u4e4b\u95f4\u3002", "conclusion": "HeCoFuse\u5728TUMTraf-V2X\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5728CVPR 2025 DriveX\u6311\u6218\u8d5b\u4e2d\u83b7\u5f97\u7b2c\u4e00\u540d\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u4e0d\u540c\u4f20\u611f\u5668\u90e8\u7f72\u4e0b\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.14045", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14045", "abs": "https://arxiv.org/abs/2507.14045", "authors": ["Israt Jahan", "Md Tahmid Rahman Laskar", "Chun Peng", "Jimmy Huang"], "title": "Evaluating the Effectiveness of Cost-Efficient Large Language Models in Benchmark Biomedical Tasks", "comment": "Accepted at Canadian AI 2025", "summary": "This paper presents a comprehensive evaluation of cost-efficient Large\nLanguage Models (LLMs) for diverse biomedical tasks spanning both text and\nimage modalities. We evaluated a range of closed-source and open-source LLMs on\ntasks such as biomedical text classification and generation, question\nanswering, and multimodal image processing. Our experimental findings indicate\nthat there is no single LLM that can consistently outperform others across all\ntasks. Instead, different LLMs excel in different tasks. While some\nclosed-source LLMs demonstrate strong performance on specific tasks, their\nopen-source counterparts achieve comparable results (sometimes even better),\nwith additional benefits like faster inference and enhanced privacy. Our\nexperimental results offer valuable insights for selecting models that are\noptimally suited for specific biomedical applications.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u7528\u4e8e\u751f\u7269\u533b\u5b66\u4efb\u52a1\u7684\u95ed\u6e90\u548c\u5f00\u6e90LLM\uff0c\u53d1\u73b0\u6ca1\u6709\u4e07\u80fd\u6a21\u578b\uff0c\u5e76\u5f3a\u8c03\u4e86\u5f00\u6e90\u6a21\u578b\u7684\u4f18\u52bf\u3002", "motivation": "\u4e3a\u4e86\u8bc4\u4f30\u5177\u6709\u6210\u672c\u6548\u76ca\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u751f\u7269\u533b\u5b66\u9886\u57df\u7684\u9002\u7528\u6027\uff0c\u5e76\u4e3a\u7279\u5b9a\u5e94\u7528\u9009\u62e9\u6700\u4f73\u6a21\u578b\u3002", "method": "\u5bf9\u4e00\u7cfb\u5217\u95ed\u6e90\u548c\u5f00\u6e90LLM\u5728\u751f\u7269\u533b\u5b66\u6587\u672c\u5206\u7c7b\u3001\u751f\u6210\u3001\u95ee\u7b54\u548c\u591a\u6a21\u6001\u56fe\u50cf\u5904\u7406\u7b49\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u5168\u9762\u7684\u8bc4\u4f30\u3002", "result": "\u6ca1\u6709\u5355\u4e00\u7684LLM\u80fd\u5728\u6240\u6709\u4efb\u52a1\u4e0a\u6301\u7eed\u8868\u73b0\u6700\u4f73\uff0c\u95ed\u6e90LLM\u5728\u7279\u5b9a\u4efb\u52a1\u4e0a\u8868\u73b0\u5f3a\u52b2\uff0c\u4f46\u5f00\u6e90LLM\u4e5f\u80fd\u8fbe\u5230\u76f8\u5f53\u6216\u66f4\u597d\u7684\u6027\u80fd\uff0c\u5e76\u5177\u6709\u66f4\u5feb\u7684\u63a8\u7406\u901f\u5ea6\u548c\u66f4\u597d\u7684\u9690\u79c1\u6027\u3002", "conclusion": "\u4e0d\u540c\u7684LLM\u5728\u4e0d\u540c\u7684\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u4f18\u52bf\uff0c\u5f00\u6e90LLM\u5728\u67d0\u4e9b\u65b9\u9762\uff08\u5982\u63a8\u7406\u901f\u5ea6\u548c\u9690\u79c1\uff09\u4f18\u4e8e\u95ed\u6e90LLM\uff0c\u4e3a\u5728\u751f\u7269\u533b\u5b66\u9886\u57df\u9009\u62e9\u5408\u9002\u7684\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002"}}
{"id": "2507.13954", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13954", "abs": "https://arxiv.org/abs/2507.13954", "authors": ["Yifan Wei", "Anwar Said", "Waseem Abbas", "Xenofon Koutsoukos"], "title": "Robust Anomaly Detection with Graph Neural Networks using Controllability", "comment": "conference paper published in IEEE CAI 2025", "summary": "Anomaly detection in complex domains poses significant challenges due to the\nneed for extensive labeled data and the inherently imbalanced nature of\nanomalous versus benign samples. Graph-based machine learning models have\nemerged as a promising solution that combines attribute and relational data to\nuncover intricate patterns. However, the scarcity of anomalous data exacerbates\nthe challenge, which requires innovative strategies to enhance model learning\nwith limited information. In this paper, we hypothesize that the incorporation\nof the influence of the nodes, quantified through average controllability, can\nsignificantly improve the performance of anomaly detection. We propose two\nnovel approaches to integrate average controllability into graph-based\nframeworks: (1) using average controllability as an edge weight and (2)\nencoding it as a one-hot edge attribute vector. Through rigorous evaluation on\nreal-world and synthetic networks with six state-of-the-art baselines, our\nproposed methods demonstrate improved performance in identifying anomalies,\nhighlighting the critical role of controllability measures in enhancing the\nperformance of graph machine learning models. This work underscores the\npotential of integrating average controllability as additional metrics to\naddress the challenges of anomaly detection in sparse and imbalanced datasets.", "AI": {"tldr": "\u901a\u8fc7\u6574\u5408\u8282\u70b9\u5e73\u5747\u53ef\u63a7\u6027\uff0c\u6539\u8fdb\u4e86\u56fe\u795e\u7ecf\u7f51\u7edc\u5728\u7a00\u758f\u3001\u4e0d\u5e73\u8861\u6570\u636e\u96c6\u4e0a\u7684\u5f02\u5e38\u68c0\u6d4b\u80fd\u529b\u3002", "motivation": "\u5f02\u5e38\u68c0\u6d4b\u5728\u590d\u6742\u57df\u4e2d\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u5927\u91cf\u6807\u8bb0\u6570\u636e\uff0c\u4e14\u5f02\u5e38\u6837\u672c\u4e0e\u826f\u6027\u6837\u672c\u7684\u6bd4\u4f8b\u4e0d\u5e73\u8861\u3002\u56fe\u6a21\u578b\u867d\u6709\u6f5c\u529b\uff0c\u4f46\u5f02\u5e38\u6570\u636e\u7a00\u7f3a\uff0c\u9700\u8981\u521b\u65b0\u7b56\u7565\u3002\u672c\u7814\u7a76\u63d0\u51fa\u5047\u8bbe\uff0c\u901a\u8fc7\u5e73\u5747\u53ef\u63a7\u6027\u91cf\u5316\u8282\u70b9\u5f71\u54cd\uff0c\u53ef\u663e\u8457\u63d0\u5347\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u65b0\u65b9\u6cd5\u5c06\u5e73\u5747\u53ef\u63a7\u6027\u6574\u5408\u5230\u57fa\u4e8e\u56fe\u7684\u5b66\u4e60\u6846\u67b6\u4e2d\uff1a1. \u4f7f\u7528\u5e73\u5747\u53ef\u63a7\u6027\u4f5c\u4e3a\u8fb9\u6743\u91cd\uff1b2. \u5c06\u5176\u7f16\u7801\u4e3a\u5355\u70ed\u8fb9\u5c5e\u6027\u5411\u91cf\u3002", "result": "\u5728\u771f\u5b9e\u548c\u5408\u6210\u7f51\u7edc\u4e0a\u8fdb\u884c\u7684\u4e25\u683c\u8bc4\u4f30\u663e\u793a\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u8bc6\u522b\u5f02\u5e38\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u4e8e\u516d\u79cd\u6700\u5148\u8fdb\u57fa\u7ebf\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u53ef\u63a7\u6027\u5ea6\u91cf\u5728\u63d0\u5347\u56fe\u673a\u5668\u5b66\u4e60\u6a21\u578b\u6027\u80fd\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u5c06\u5e73\u5747\u53ef\u63a7\u6027\u4f5c\u4e3a\u9644\u52a0\u6307\u6807\u6574\u5408\u5230\u56fe\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4e2d\uff0c\u53ef\u4ee5\u6709\u6548\u5e94\u5bf9\u7a00\u758f\u548c\u4e0d\u5e73\u8861\u6570\u636e\u96c6\u4e2d\u7684\u5f02\u5e38\u68c0\u6d4b\u6311\u6218\uff0c\u5e76\u663e\u8457\u63d0\u9ad8\u68c0\u6d4b\u6027\u80fd\u3002"}}
{"id": "2507.13693", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13693", "abs": "https://arxiv.org/abs/2507.13693", "authors": ["Hongyi Liu", "Haifeng Wang"], "title": "Gaussian kernel-based motion measurement", "comment": null, "summary": "The growing demand for structural health monitoring has driven increasing\ninterest in high-precision motion measurement, as structural information\nderived from extracted motions can effectively reflect the current condition of\nthe structure. Among various motion measurement techniques, vision-based\nmethods stand out due to their low cost, easy installation, and large-scale\nmeasurement. However, when it comes to sub-pixel-level motion measurement,\ncurrent vision-based methods either lack sufficient accuracy or require\nextensive manual parameter tuning (e.g., pyramid layers, target pixels, and\nfilter parameters) to reach good precision. To address this issue, we developed\na novel Gaussian kernel-based motion measurement method, which can extract the\nmotion between different frames via tracking the location of Gaussian kernels.\nThe motion consistency, which fits practical structural conditions, and a\nsuper-resolution constraint, are introduced to increase accuracy and robustness\nof our method. Numerical and experimental validations show that it can\nconsistently reach high accuracy without customized parameter setup for\ndifferent test samples.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9ad8\u65af\u6838\u7684\u8fd0\u52a8\u6d4b\u91cf\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fd0\u52a8\u4e00\u81f4\u6027\u548c\u8d85\u5206\u8fa8\u7387\u7ea6\u675f\u63d0\u9ad8\u4e86\u4e9a\u50cf\u7d20\u7ea7\u8fd0\u52a8\u6d4b\u91cf\u7684\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\uff0c\u4e14\u65e0\u9700\u624b\u52a8\u8c03\u6574\u53c2\u6570\u3002", "motivation": "\u9488\u5bf9\u73b0\u6709\u89c6\u89c9\u6d4b\u91cf\u65b9\u6cd5\u5728\u4e9a\u50cf\u7d20\u7ea7\u8fd0\u52a8\u6d4b\u91cf\u4e2d\u7cbe\u5ea6\u4e0d\u8db3\u6216\u9700\u8981\u5927\u91cf\u624b\u52a8\u53c2\u6570\u8c03\u6574\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u5927\u91cf\u624b\u52a8\u8c03\u53c2\u5373\u53ef\u8fbe\u5230\u9ad8\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u7684\u8fd0\u52a8\u6d4b\u91cf\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u9ad8\u65af\u6838\uff08Gaussian kernel\uff09\u4e3a\u57fa\u7840\u7684\u8fd0\u52a8\u6d4b\u91cf\u65b9\u6cd5\uff0c\u901a\u8fc7\u8ffd\u8e2a\u9ad8\u65af\u6838\u7684\u4f4d\u7f6e\u6765\u63d0\u53d6\u4e0d\u540c\u5e27\u4e4b\u95f4\u7684\u8fd0\u52a8\u3002\u5f15\u5165\u4e86\u8fd0\u52a8\u4e00\u81f4\u6027\uff08motion consistency\uff09\u548c\u8d85\u5206\u8fa8\u7387\u7ea6\u675f\uff08super-resolution constraint\uff09\u6765\u63d0\u9ad8\u65b9\u6cd5\u7684\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u4e00\u81f4\u5730\u8fbe\u5230\u9ad8\u7cbe\u5ea6\uff0c\u5e76\u4e14\u65e0\u9700\u9488\u5bf9\u4e0d\u540c\u7684\u6d4b\u8bd5\u6837\u672c\u8fdb\u884c\u5b9a\u5236\u5316\u7684\u53c2\u6570\u8bbe\u7f6e\u3002", "conclusion": "\u8be5\u6587\u63d0\u51fa\u7684\u9ad8\u65af\u6838\u8fd0\u52a8\u6d4b\u91cf\u65b9\u6cd5\u5728\u6570\u503c\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e2d\u5747\u8868\u73b0\u51fa\u9ad8\u7cbe\u5ea6\u548c\u9ad8\u9c81\u68d2\u6027\uff0c\u4e14\u65e0\u9700\u9488\u5bf9\u4e0d\u540c\u6d4b\u8bd5\u6837\u672c\u8fdb\u884c\u5b9a\u5236\u5316\u53c2\u6570\u8bbe\u7f6e\u3002"}}
{"id": "2507.14063", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14063", "abs": "https://arxiv.org/abs/2507.14063", "authors": ["Lautaro Estienne", "Gabriel Ben Zenou", "Nona Naderi", "Jackie Cheung", "Pablo Piantanida"], "title": "Collaborative Rational Speech Act: Pragmatic Reasoning for Multi-Turn Dialog", "comment": null, "summary": "As AI systems take on collaborative roles, they must reason about shared\ngoals and beliefs-not just generate fluent language. The Rational Speech Act\n(RSA) framework offers a principled approach to pragmatic reasoning, but\nexisting extensions face challenges in scaling to multi-turn, collaborative\nscenarios. In this paper, we introduce Collaborative Rational Speech Act\n(CRSA), an information-theoretic (IT) extension of RSA that models multi-turn\ndialog by optimizing a gain function adapted from rate-distortion theory. This\ngain is an extension of the gain model that is maximized in the original RSA\nmodel but takes into account the scenario in which both agents in a\nconversation have private information and produce utterances conditioned on the\ndialog. We demonstrate the effectiveness of CRSA on referential games and\ntemplate-based doctor-patient dialogs in the medical domain. Empirical results\nshow that CRSA yields more consistent, interpretable, and collaborative\nbehavior than existing baselines-paving the way for more pragmatic and socially\naware language agents.", "AI": {"tldr": "\u63d0\u51faCRSA\u6846\u67b6\uff0c\u4e00\u79cd\u57fa\u4e8e\u4fe1\u606f\u8bba\u7684RSA\u6269\u5c55\uff0c\u901a\u8fc7\u4f18\u5316\u589e\u76ca\u51fd\u6570\u6765\u6539\u8fdb\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7684AI\u534f\u4f5c\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u663e\u793a\u51fa\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684RSA\u6846\u67b6\u5728\u6269\u5c55\u5230\u6d89\u53ca\u591a\u8f6e\u3001\u534f\u4f5c\u573a\u666f\u65f6\u9762\u4e34\u6311\u6218\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u8ba9AI\u7cfb\u7edf\u5728\u534f\u4f5c\u89d2\u8272\u4e2d\u8fdb\u884c\u601d\u8003\uff0c\u5b9e\u73b0\u5171\u540c\u76ee\u6807\u548c\u4fe1\u5ff5\u7684\u63a8\u7406\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u751f\u6210\u6d41\u7545\u7684\u8bed\u8a00\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCRSA\uff08Collaborative Rational Speech Act\uff09\u7684\u4fe1\u606f\u8bba\uff08IT\uff09\u6269\u5c55\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u6765\u81ea\u7387\u5931\u771f\u7406\u8bba\u7684\u589e\u76ca\u51fd\u6570\u6765\u6a21\u62df\u591a\u8f6e\u5bf9\u8bdd\uff0c\u8be5\u589e\u76ca\u51fd\u6570\u662f\u539f\u59cbRSA\u6a21\u578b\u4e2d\u6700\u5927\u5316\u7684\u589e\u76ca\u6a21\u578b\u7684\u6269\u5c55\uff0c\u5e76\u8003\u8651\u4e86\u5bf9\u8bdd\u53cc\u65b9\u62e5\u6709\u79c1\u6709\u4fe1\u606f\u4e14\u8bdd\u8bed\u57fa\u4e8e\u5bf9\u8bdd\u7684\u573a\u666f\u3002", "result": "CRSA\u5728\u53c2\u7167\u6e38\u620f\u548c\u57fa\u4e8e\u6a21\u677f\u7684\u533b\u60a3\u5bf9\u8bdd\u4e2d\uff0c\u76f8\u6bd4\u73b0\u6709\u57fa\u7ebf\uff0c\u8868\u73b0\u51fa\u66f4\u4e00\u81f4\u3001\u66f4\u5177\u53ef\u89e3\u91ca\u6027\u548c\u534f\u4f5c\u6027\u7684\u884c\u4e3a\u3002", "conclusion": "CRSA\u5728\u53c2\u7167\u6e38\u620f\u548c\u57fa\u4e8e\u6a21\u677f\u7684\u533b\u60a3\u5bf9\u8bdd\u4e2d\u8868\u73b0\u51fa\u6bd4\u73b0\u6709\u57fa\u7ebf\u66f4\u4e00\u81f4\u3001\u66f4\u5177\u53ef\u89e3\u91ca\u6027\u548c\u534f\u4f5c\u6027\u7684\u884c\u4e3a\uff0c\u4e3a\u6784\u5efa\u66f4\u5177\u5b9e\u7528\u6027\u548c\u793e\u4f1a\u610f\u8bc6\u7684\u8bed\u8a00\u4ee3\u7406\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2507.13959", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13959", "abs": "https://arxiv.org/abs/2507.13959", "authors": ["Eli Verwimp", "Gustav Ryberg Smidt", "Hendrik Hameeuw", "Katrien De Graef"], "title": "Signs of the Past, Patterns of the Present: On the Automatic Classification of Old Babylonian Cuneiform Signs", "comment": "Paper under review at JOCCH", "summary": "The work in this paper describes the training and evaluation of machine\nlearning (ML) techniques for the classification of cuneiform signs. There is a\nlot of variability in cuneiform signs, depending on where they come from, for\nwhat and by whom they were written, but also how they were digitized. This\nvariability makes it unlikely that an ML model trained on one dataset will\nperform successfully on another dataset. This contribution studies how such\ndifferences impact that performance. Based on our results and insights, we aim\nto influence future data acquisition standards and provide a solid foundation\nfor future cuneiform sign classification tasks. The ML model has been trained\nand tested on handwritten Old Babylonian (c. 2000-1600 B.C.E.) documentary\ntexts inscribed on clay tablets originating from three Mesopotamian cities\n(Nippur, D\\=ur-Abie\\v{s}uh and Sippar). The presented and analysed model is\nResNet50, which achieves a top-1 score of 87.1% and a top-5 score of 96.5% for\nsigns with at least 20 instances. As these automatic classification results are\nthe first on Old Babylonian texts, there are currently no comparable results.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u673a\u5668\u5b66\u4e60\u5728\u6954\u5f62\u6587\u5b57\u7b26\u53f7\u5206\u7c7b\u4e2d\u7684\u5e94\u7528\uff0c\u5206\u6790\u4e86\u6570\u636e\u53d8\u5f02\u6027\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u4f7f\u7528ResNet50\u6a21\u578b\u53d6\u5f97\u4e8687.1%\u7684top-1\u51c6\u786e\u7387\u3002", "motivation": "\u7531\u4e8e\u6954\u5f62\u6587\u5b57\u7b26\u53f7\u5b58\u5728\u663e\u8457\u7684\u53d8\u5f02\u6027\uff0c\u8fd9\u4f7f\u5f97\u5728\u4e00\u4e2a\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684ML\u6a21\u578b\u5728\u53e6\u4e00\u4e2a\u6570\u636e\u96c6\u4e0a\u53ef\u80fd\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u6b64\u672c\u6587\u7814\u7a76\u4e86\u8fd9\u4e9b\u5dee\u5f02\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u672c\u6587\u63cf\u8ff0\u4e86\u7528\u4e8e\u6954\u5f62\u6587\u5b57\u7b26\u53f7\u5206\u7c7b\u7684\u673a\u5668\u5b66\u4e60\uff08ML\uff09\u6280\u672f\u7684\u8bad\u7ec3\u548c\u8bc4\u4f30\uff0c\u5e76\u5bf9ResNet50\u6a21\u578b\u8fdb\u884c\u4e86\u5206\u6790\u3002", "result": "ResNet50\u6a21\u578b\u5728\u81f3\u5c11\u670920\u4e2a\u5b9e\u4f8b\u7684\u7b26\u53f7\u4e0a\uff0c\u53d6\u5f97\u4e8687.1%\u7684top-1\u51c6\u786e\u7387\u548c96.5%\u7684top-5\u51c6\u786e\u7387\u3002", "conclusion": "\u8be5\u7814\u7a76\u65e8\u5728\u4e3a\u672a\u6765\u7684\u6954\u5f62\u6587\u5b57\u7b26\u53f7\u5206\u7c7b\u4efb\u52a1\u5960\u5b9a\u575a\u5b9e\u7684\u57fa\u7840\uff0c\u5e76\u671f\u671b\u5f71\u54cd\u672a\u6765\u7684\u6570\u636e\u91c7\u96c6\u6807\u51c6\u3002"}}
{"id": "2507.13706", "categories": ["cs.CV", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2507.13706", "abs": "https://arxiv.org/abs/2507.13706", "authors": ["\u00c1ngel F. Garc\u00eda-Fern\u00e1ndez", "Jinhao Gu", "Lennart Svensson", "Yuxuan Xia", "Jan Krej\u010d\u00ed", "Oliver Kost", "Ond\u0159ej Straka"], "title": "GOSPA and T-GOSPA quasi-metrics for evaluation of multi-object tracking algorithms", "comment": null, "summary": "This paper introduces two quasi-metrics for performance assessment of\nmulti-object tracking (MOT) algorithms. In particular, one quasi-metric is an\nextension of the generalised optimal subpattern assignment (GOSPA) metric and\nmeasures the discrepancy between sets of objects. The other quasi-metric is an\nextension of the trajectory GOSPA (T-GOSPA) metric and measures the discrepancy\nbetween sets of trajectories. Similar to the GOSPA-based metrics, these\nquasi-metrics include costs for localisation error for properly detected\nobjects, the number of false objects and the number of missed objects. The\nT-GOSPA quasi-metric also includes a track switching cost. Differently from the\nGOSPA and T-GOSPA metrics, the proposed quasi-metrics have the flexibility of\npenalising missed and false objects with different costs, and the localisation\ncosts are not required to be symmetric. These properties can be useful in MOT\nevaluation in certain applications. The performance of several Bayesian MOT\nalgorithms is assessed with the T-GOSPA quasi-metric via simulations.", "AI": {"tldr": "This paper presents two new, more flexible metrics (extensions of GOSPA and T-GOSPA) to evaluate multi-object tracking algorithms, allowing different penalties for missed and false targets and asymmetric localization costs. Tested using simulations on Bayesian MOT algorithms.", "motivation": "To address the need for more flexible performance assessment of multi-object tracking (MOT) algorithms, particularly in applications where different types of errors (missed vs. false objects) or localization inaccuracies have varying significance.", "method": "Introduced two quasi-metrics for MOT performance assessment: an extension of the GOSPA metric for object sets and an extension of the T-GOSPA metric for trajectory sets. These quasi-metrics incorporate costs for localization error, false objects, and missed objects, with the T-GOSPA quasi-metric also including a track switching cost. Key features include flexible penalization of missed and false objects with different costs and non-symmetric localization costs.", "result": "The proposed quasi-metrics provide a more flexible approach to MOT evaluation. Simulations using the T-GOSPA quasi-metric demonstrated its utility in assessing the performance of Bayesian MOT algorithms.", "conclusion": "The paper proposes two new quasi-metrics for evaluating multi-object tracking (MOT) algorithms, extending the GOSPA and T-GOSPA metrics. These quasi-metrics offer greater flexibility by allowing different costs for missed and false objects, and non-symmetric localization costs, which can be beneficial in specific applications. The T-GOSPA quasi-metric was used to assess the performance of several Bayesian MOT algorithms through simulations."}}
{"id": "2507.14079", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14079", "abs": "https://arxiv.org/abs/2507.14079", "authors": ["Garapati Keerthana", "Manik Gupta"], "title": "DENSE: Longitudinal Progress Note Generation with Temporal Modeling of Heterogeneous Clinical Notes Across Hospital Visits", "comment": null, "summary": "Progress notes are among the most clinically meaningful artifacts in an\nElectronic Health Record (EHR), offering temporally grounded insights into a\npatient's evolving condition, treatments, and care decisions. Despite their\nimportance, they are severely underrepresented in large-scale EHR datasets. For\ninstance, in the widely used Medical Information Mart for Intensive Care III\n(MIMIC-III) dataset, only about $8.56\\%$ of hospital visits include progress\nnotes, leaving gaps in longitudinal patient narratives. In contrast, the\ndataset contains a diverse array of other note types, each capturing different\naspects of care.\n  We present DENSE (Documenting Evolving Progress Notes from Scattered\nEvidence), a system designed to align with clinical documentation workflows by\nsimulating how physicians reference past encounters while drafting progress\nnotes. The system introduces a fine-grained note categorization and a temporal\nalignment mechanism that organizes heterogeneous notes across visits into\nstructured, chronological inputs. At its core, DENSE leverages a clinically\ninformed retrieval strategy to identify temporally and semantically relevant\ncontent from both current and prior visits. This retrieved evidence is used to\nprompt a large language model (LLM) to generate clinically coherent and\ntemporally aware progress notes.\n  We evaluate DENSE on a curated cohort of patients with multiple visits and\ncomplete progress note documentation. The generated notes demonstrate strong\nlongitudinal fidelity, achieving a temporal alignment ratio of $1.089$,\nsurpassing the continuity observed in original notes. By restoring narrative\ncoherence across fragmented documentation, our system supports improved\ndownstream tasks such as summarization, predictive modeling, and clinical\ndecision support, offering a scalable solution for LLM-driven note synthesis in\nreal-world healthcare settings.", "AI": {"tldr": "DENSE\u7cfb\u7edf\u5229\u7528LLM\u548c\u4e34\u5e8a\u4fe1\u606f\u68c0\u7d22\uff0c\u901a\u8fc7\u6574\u5408\u96f6\u6563\u7684EHR\u7b14\u8bb0\uff0c\u751f\u6210\u8fde\u8d2f\u7684\u60a3\u8005\u8fdb\u5c55\u8bb0\u5f55\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u4e2d\u8fdb\u5c55\u8bb0\u5f55\u7f3a\u5931\u7684\u95ee\u9898\u3002", "motivation": "EHR\u6570\u636e\u4e2d\u8fdb\u5c55\u8bb0\u5f55\u4ee3\u8868\u6027\u4e0d\u8db3\uff0c\u5b58\u5728\u7eb5\u5411\u53d9\u8ff0\u7684\u7a7a\u767d\uff0c\u5f71\u54cd\u4e86\u5bf9\u60a3\u8005\u75c5\u7a0b\u7684\u7406\u89e3\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u7cfb\u7edf\u6765\u6062\u590d\u8de8\u8d8a\u96f6\u6563\u8bb0\u5f55\u7684\u53d9\u4e8b\u8fde\u8d2f\u6027\u3002", "method": "DENSE\u7cfb\u7edf\u901a\u8fc7\u7cbe\u7ec6\u7684\u7b14\u8bb0\u5206\u7c7b\u548c\u65f6\u95f4\u5bf9\u9f50\u673a\u5236\uff0c\u6574\u5408\u8de8\u5c31\u8bca\u7684\u5f02\u6784\u7b14\u8bb0\uff0c\u5e76\u5229\u7528\u4e34\u5e8a\u4fe1\u606f\u68c0\u7d22\u7b56\u7565\u548cLLM\u6765\u751f\u6210\u8fdb\u5c55\u8bb0\u5f55\u3002", "result": "\u751f\u6210\u7684\u8fdb\u5c55\u8bb0\u5f55\u5728\u65f6\u95f4\u5fe0\u5b9e\u5ea6\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u65f6\u95f4\u5bf9\u9f50\u7387\u8fbe\u52301.089\uff0c\u4f18\u4e8e\u539f\u59cb\u8bb0\u5f55\u7684\u8fde\u7eed\u6027\u3002", "conclusion": "DENSE\u7cfb\u7edf\u901a\u8fc7\u751f\u6210\u8fde\u8d2f\u4e14\u65f6\u95f4\u51c6\u786e\u7684\u8fdb\u5c55\u8bb0\u5f55\uff0c\u5f25\u8865\u4e86EHR\u6570\u636e\u4e2d\u8fdb\u5c55\u8bb0\u5f55\u7684\u4e0d\u8db3\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u65f6\u95f4\u5bf9\u9f50\u65b9\u9762\u7684\u4f18\u8d8a\u6027\uff0c\u6709\u671b\u6539\u8fdb\u4e0b\u6e38\u4e34\u5e8a\u4efb\u52a1\u3002"}}
{"id": "2507.13992", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13992", "abs": "https://arxiv.org/abs/2507.13992", "authors": ["Jagruti Patel", "Thomas A. W. Bolton", "Mikkel Sch\u00f6ttner", "Anjali Tarun", "Sebastien Tourbier", "Yasser Alem\u00e0n-G\u00f2mez", "Jonas Richiardi", "Patric Hagmann"], "title": "Structural Connectome Harmonization Using Deep Learning: The Strength of Graph Neural Networks", "comment": null, "summary": "Small sample sizes in neuroimaging in general, and in structural connectome\n(SC) studies in particular limit the development of reliable biomarkers for\nneurological and psychiatric disorders - such as Alzheimer's disease and\nschizophrenia - by reducing statistical power, reliability, and\ngeneralizability. Large-scale multi-site studies have exist, but they have\nacquisition-related biases due to scanner heterogeneity, compromising imaging\nconsistency and downstream analyses. While existing SC harmonization methods -\nsuch as linear regression (LR), ComBat, and deep learning techniques - mitigate\nthese biases, they often rely on detailed metadata, traveling subjects (TS), or\noverlook the graph-topology of SCs. To address these limitations, we propose a\nsite-conditioned deep harmonization framework that harmonizes SCs across\ndiverse acquisition sites without requiring metadata or TS that we test in a\nsimulated scenario based on the Human Connectome Dataset. Within this\nframework, we benchmark three deep architectures - a fully connected\nautoencoder (AE), a convolutional AE, and a graph convolutional AE - against a\ntop-performing LR baseline. While non-graph models excel in edge-weight\nprediction and edge existence detection, the graph AE demonstrates superior\npreservation of topological structure and subject-level individuality, as\nreflected by graph metrics and fingerprinting accuracy, respectively. Although\nthe LR baseline achieves the highest numerical performance by explicitly\nmodeling acquisition parameters, it lacks applicability to real-world\nmulti-site use cases as detailed acquisition metadata is often unavailable. Our\nresults highlight the critical role of model architecture in SC harmonization\nperformance and demonstrate that graph-based approaches are particularly\nwell-suited for structure-aware, domain-generalizable SC harmonization in\nlarge-scale multi-site SC studies.", "AI": {"tldr": "\u4e3a\u89e3\u51b3\u795e\u7ecf\u5f71\u50cf\u5b66\u4e2d\u5c0f\u6837\u672c\u91cf\u548c\u591a\u4e2d\u5fc3\u7814\u7a76\u4e2d\u7684\u6570\u636e\u5f02\u8d28\u6027\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u5143\u6570\u636e\u6216\u65c5\u884c\u53d7\u8bd5\u8005\u7684\u56fe\u5377\u79ef\u81ea\u7f16\u7801\u5668\u534f\u8c03\u6846\u67b6\uff0c\u5e76\u5728\u6a21\u62df\u6570\u636e\u4e2d\u9a8c\u8bc1\u4e86\u5176\u5728\u4fdd\u7559\u62d3\u6251\u7ed3\u6784\u548c\u4e2a\u4f53\u7279\u5f81\u65b9\u9762\u7684\u4f18\u8d8a\u6027\u3002", "motivation": "\u795e\u7ecf\u5f71\u50cf\u5b66\uff08\u7279\u522b\u662f\u7ed3\u6784\u8fde\u63a5\u7ec4\u5b66\uff09\u7814\u7a76\u4e2d\u7684\u5c0f\u6837\u672c\u91cf\u9650\u5236\u4e86\u53ef\u9760\u751f\u7269\u6807\u5fd7\u7269\u7684\u53d1\u5c55\uff0c\u800c\u5927\u89c4\u6a21\u7814\u7a76\u5219\u9762\u4e34\u626b\u63cf\u4eea\u5f02\u8d28\u6027\u5bfc\u81f4\u7684\u91c7\u96c6\u504f\u5dee\u95ee\u9898\u3002\u73b0\u6709\u7684\u534f\u8c03\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u8be6\u7ec6\u7684\u5143\u6570\u636e\u6216\u65c5\u884c\u53d7\u8bd5\u8005\uff0c\u6216\u5ffd\u7565\u4e86SC\u7684\u56fe\u62d3\u6251\u7ed3\u6784\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u5143\u6570\u636e\u6216\u65c5\u884c\u53d7\u8bd5\u8005\u5373\u53ef\u8de8\u4e0d\u540c\u91c7\u96c6\u7ad9\u70b9\u534f\u8c03SC\u7684\u7ad9\u70b9\u6761\u4ef6\u6df1\u5ea6\u534f\u8c03\u6846\u67b6\uff0c\u5e76\u6d4b\u8bd5\u4e86\u4e09\u79cd\u6df1\u5ea6\u67b6\u6784\uff08\u5168\u8fde\u63a5\u81ea\u7f16\u7801\u5668\u3001\u5377\u79ef\u81ea\u7f16\u7801\u5668\u548c\u56fe\u5377\u79ef\u81ea\u7f16\u7801\u5668\uff09\u4e0e\u57fa\u4e8e\u7ebf\u6027\u56de\u5f52\u7684\u57fa\u7ebf\u7684\u6027\u80fd\u3002", "result": "\u56fe\u81ea\u7f16\u7801\u5668\u5728\u4fdd\u7559\u62d3\u6251\u7ed3\u6784\u548c\u4e2a\u4f53\u7279\u5f81\u65b9\u9762\u4f18\u4e8e\u975e\u56fe\u6a21\u578b\uff0c\u5c3d\u7ba1\u7ebf\u6027\u56de\u5f52\u57fa\u7ebf\u5728\u6570\u503c\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u7f3a\u4e4f\u5b9e\u9645\u5e94\u7528\u6027\u3002\u7ed3\u679c\u8868\u660e\uff0c\u6a21\u578b\u67b6\u6784\u5728SC\u534f\u8c03\u4e2d\u8d77\u7740\u5173\u952e\u4f5c\u7528\uff0c\u57fa\u4e8e\u56fe\u7684\u65b9\u6cd5\u66f4\u9002\u5408\u7ed3\u6784\u611f\u77e5\u548c\u9886\u57df\u53ef\u6cdb\u5316\u7684SC\u534f\u8c03\u3002", "conclusion": "\u56fe\u5377\u79ef\u81ea\u7f16\u7801\u5668\u5728\u7ed3\u6784\u8fde\u63a5\u7ec4\u5b66\uff08SC\uff09\u534f\u8c03\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u62d3\u6251\u7ed3\u6784\u548c\u4e2a\u4f53\u5c42\u9762\u7279\u5f81\u4fdd\u7559\u80fd\u529b\uff0c\u4f7f\u5176\u6210\u4e3a\u5927\u89c4\u6a21\u591a\u4e2d\u5fc3\u7814\u7a76\u4e2d\u7ed3\u6784\u611f\u77e5\u7684\u3001\u9886\u57df\u53ef\u6cdb\u5316\u7684SC\u534f\u8c03\u7684\u6709\u529b\u65b9\u6cd5\u3002"}}
{"id": "2507.13708", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13708", "abs": "https://arxiv.org/abs/2507.13708", "authors": ["Sofia Jamil", "Bollampalli Areen Reddy", "Raghvendra Kumar", "Sriparna Saha", "Koustava Goswami", "K. J. Joseph"], "title": "PoemTale Diffusion: Minimising Information Loss in Poem to Image Generation with Multi-Stage Prompt Refinement", "comment": "ECAI 2025", "summary": "Recent advancements in text-to-image diffusion models have achieved\nremarkable success in generating realistic and diverse visual content. A\ncritical factor in this process is the model's ability to accurately interpret\ntextual prompts. However, these models often struggle with creative\nexpressions, particularly those involving complex, abstract, or highly\ndescriptive language. In this work, we introduce a novel training-free approach\ntailored to improve image generation for a unique form of creative language:\npoetic verse, which frequently features layered, abstract, and dual meanings.\nOur proposed PoemTale Diffusion approach aims to minimise the information that\nis lost during poetic text-to-image conversion by integrating a multi stage\nprompt refinement loop into Language Models to enhance the interpretability of\npoetic texts. To support this, we adapt existing state-of-the-art diffusion\nmodels by modifying their self-attention mechanisms with a consistent\nself-attention technique to generate multiple consistent images, which are then\ncollectively used to convey the poem's meaning. Moreover, to encourage research\nin the field of poetry, we introduce the P4I (PoemForImage) dataset, consisting\nof 1111 poems sourced from multiple online and offline resources. We engaged a\npanel of poetry experts for qualitative assessments. The results from both\nhuman and quantitative evaluations validate the efficacy of our method and\ncontribute a novel perspective to poem-to-image generation with enhanced\ninformation capture in the generated images.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPoemTale Diffusion\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u6539\u8fdb\u7684\u63d0\u793a\u7cbe\u70bc\u548c\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u53ef\u4ee5\u66f4\u51c6\u786e\u5730\u5c06\u8bd7\u6b4c\u8f6c\u5316\u4e3a\u56fe\u50cf\uff0c\u5e76\u53d1\u5e03\u4e86\u4e00\u4e2a\u5305\u542b1111\u9996\u8bd7\u6b4c\u7684\u6570\u636e\u96c6P4I\uff0c\u4ee5\u4fc3\u8fdb\u76f8\u5173\u7814\u7a76\u3002", "motivation": "\u76ee\u524d\u7684\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u5728\u5904\u7406\u8bd7\u6b4c\u7b49\u5bcc\u6709\u521b\u610f\u7684\u8bed\u8a00\u5f62\u5f0f\u65f6\u9762\u4e34\u6311\u6218\uff0c\u56e0\u4e3a\u8bd7\u6b4c\u901a\u5e38\u5177\u6709\u591a\u5c42\u6b21\u3001\u62bd\u8c61\u548c\u53cc\u91cd\u542b\u4e49\uff0c\u8fd9\u4f1a\u5bfc\u81f4\u4fe1\u606f\u5728\u8f6c\u6362\u8fc7\u7a0b\u4e2d\u4e22\u5931\u3002", "method": "\u672c\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8bad\u7ec3\u65e0\u5173\u65b9\u6cd5\uff0c\u5c06\u591a\u9636\u6bb5\u63d0\u793a\u7cbe\u70bc\u5faa\u73af\u96c6\u6210\u5230\u8bed\u8a00\u6a21\u578b\u4e2d\uff0c\u4ee5\u63d0\u9ad8\u8bd7\u6b4c\u6587\u672c\u7684\u53ef\u89e3\u91ca\u6027\u3002\u6b64\u5916\uff0c\u8fd8\u901a\u8fc7\u4fee\u6539\u81ea\u6ce8\u610f\u529b\u673a\u5236\u6765\u751f\u6210\u591a\u4e2a\u4e00\u81f4\u7684\u56fe\u50cf\uff0c\u5e76\u7ed3\u5408\u8fd9\u4e9b\u56fe\u50cf\u6765\u4f20\u8fbe\u8bd7\u6b4c\u7684\u542b\u4e49\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5730\u6355\u6349\u8bd7\u6b4c\u7684\u542b\u4e49\uff0c\u751f\u6210\u5177\u6709\u589e\u5f3a\u4fe1\u606f\u7684\u5185\u5bb9\u3002\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u4f30\u5747\u8bc1\u5b9e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u4e3a\u8bd7\u6b4c\u5230\u56fe\u50cf\u751f\u6210\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8bad\u7ec3\u65e0\u5173\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u63d0\u793a\u7cbe\u70bc\u5faa\u73af\u548c\u4fee\u6539\u7684\u81ea\u6ce8\u610f\u529b\u673a\u5236\u6765\u63d0\u9ad8\u8bd7\u6b4c\u6587\u672c\u5230\u56fe\u50cf\u8f6c\u6362\u7684\u51c6\u786e\u6027\uff0c\u5e76\u5f15\u5165\u4e86P4I\u6570\u636e\u96c6\u6765\u652f\u6301\u8be5\u9886\u57df\u7684\u7814\u7a76\u3002"}}
{"id": "2507.14096", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.14096", "abs": "https://arxiv.org/abs/2507.14096", "authors": ["Brian Ondov", "William Xia", "Kush Attal", "Ishita Unde", "Jerry He", "Hoa Dang", "Ian Soboroff", "Dina Demner-Fushman"], "title": "Lessons from the TREC Plain Language Adaptation of Biomedical Abstracts (PLABA) track", "comment": null, "summary": "Objective: Recent advances in language models have shown potential to adapt\nprofessional-facing biomedical literature to plain language, making it\naccessible to patients and caregivers. However, their unpredictability,\ncombined with the high potential for harm in this domain, means rigorous\nevaluation is necessary. Our goals with this track were to stimulate research\nand to provide high-quality evaluation of the most promising systems.\n  Methods: We hosted the Plain Language Adaptation of Biomedical Abstracts\n(PLABA) track at the 2023 and 2024 Text Retrieval Conferences. Tasks included\ncomplete, sentence-level, rewriting of abstracts (Task 1) as well as\nidentifying and replacing difficult terms (Task 2). For automatic evaluation of\nTask 1, we developed a four-fold set of professionally-written references.\nSubmissions for both Tasks 1 and 2 were provided extensive manual evaluation\nfrom biomedical experts.\n  Results: Twelve teams spanning twelve countries participated in the track,\nwith models from multilayer perceptrons to large pretrained transformers. In\nmanual judgments of Task 1, top-performing models rivaled human levels of\nfactual accuracy and completeness, but not simplicity or brevity. Automatic,\nreference-based metrics generally did not correlate well with manual judgments.\nIn Task 2, systems struggled with identifying difficult terms and classifying\nhow to replace them. When generating replacements, however, LLM-based systems\ndid well in manually judged accuracy, completeness, and simplicity, though not\nin brevity.\n  Conclusion: The PLABA track showed promise for using Large Language Models to\nadapt biomedical literature for the general public, while also highlighting\ntheir deficiencies and the need for improved automatic benchmarking tools.", "AI": {"tldr": "PLABA\u8d5b\u9053\u65e8\u5728\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u6539\u7f16\u751f\u7269\u533b\u5b66\u6587\u732e\u4e3a\u666e\u901a\u8bed\u8a00\u7684\u80fd\u529b\u3002\u4efb\u52a1\u5305\u62ec\u91cd\u5199\u6458\u8981\u548c\u66ff\u6362\u96be\u8bcd\u3002\u9876\u5c16\u6a21\u578b\u5728\u51c6\u786e\u6027\u548c\u5b8c\u6574\u6027\u65b9\u9762\u8868\u73b0\u63a5\u8fd1\u4eba\u7c7b\uff0c\u4f46\u5728\u7b80\u6d01\u6027\u65b9\u9762\u6709\u5dee\u8ddd\u3002\u81ea\u52a8\u8bc4\u4f30\u6307\u6807\u4e0e\u624b\u52a8\u8bc4\u5224\u7684\u76f8\u5173\u6027\u4e0d\u9ad8\u3002\u6a21\u578b\u5728\u66ff\u6362\u96be\u8bcd\u65b9\u9762\u8868\u73b0\u5c1a\u53ef\uff0c\u4f46\u5728\u7b80\u6d01\u6027\u65b9\u9762\u4ecd\u9700\u6539\u8fdb\u3002\u8be5\u8d5b\u9053\u51f8\u663e\u4e86LLM\u7684\u6f5c\u529b\u548c\u5c40\u9650\u6027\uff0c\u5e76\u6307\u51fa\u4e86\u6539\u8fdb\u81ea\u52a8\u8bc4\u4f30\u5de5\u5177\u7684\u5fc5\u8981\u6027\u3002", "motivation": "\u8bed\u8a00\u6a21\u578b\u7684\u6700\u65b0\u8fdb\u5c55\u5728\u5c06\u9762\u5411\u4e13\u4e1a\u4eba\u58eb\u7684\u751f\u7269\u533b\u5b66\u6587\u732e\u6539\u7f16\u4e3a\u666e\u901a\u8bed\u8a00\u65b9\u9762\u663e\u793a\u51fa\u4e86\u6f5c\u529b\uff0c\u4f7f\u5176\u80fd\u591f\u88ab\u60a3\u8005\u548c\u62a4\u7406\u8005\u6240\u7406\u89e3\u3002\u7136\u800c\uff0c\u5b83\u4eec\u7684\u4e0d\u53ef\u9884\u6d4b\u6027\uff0c\u52a0\u4e0a\u8be5\u9886\u57df\u6f5c\u5728\u7684\u5de8\u5927\u98ce\u9669\uff0c\u610f\u5473\u7740\u5fc5\u987b\u8fdb\u884c\u4e25\u683c\u7684\u8bc4\u4f30\u3002\u6211\u4eec\u4e3e\u529e\u6b64\u8d5b\u9053\u7684\u76ee\u6807\u662f\u6fc0\u52b1\u7814\u7a76\u5e76\u4e3a\u6700\u6709\u524d\u9014\u7684\u7cfb\u7edf\u63d0\u4f9b\u9ad8\u8d28\u91cf\u7684\u8bc4\u4f30\u3002", "method": "\u57282023\u5e74\u548c2024\u5e74\u7684\u6587\u672c\u68c0\u7d22\u4f1a\u8bae\u4e0a\u4e3e\u529e\u4e86\u751f\u7269\u533b\u5b66\u6458\u8981\u7684\u666e\u901a\u8bed\u8a00\u6539\u7f16\uff08PLABA\uff09\u8d5b\u9053\u3002\u4efb\u52a1\u5305\u62ec\u5bf9\u6458\u8981\u8fdb\u884c\u5b8c\u6574\u7684\u3001\u53e5\u5b50\u7ea7\u522b\u7684\u91cd\u5199\uff08\u4efb\u52a11\uff09\u4ee5\u53ca\u8bc6\u522b\u548c\u66ff\u6362\u56f0\u96be\u672f\u8bed\uff08\u4efb\u52a12\uff09\u3002\u5bf9\u4e8e\u4efb\u52a11\u7684\u81ea\u52a8\u8bc4\u4f30\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u7ec4\u56db\u91cd\u4e13\u4e1a\u7f16\u5199\u7684\u53c2\u8003\u8d44\u6599\u3002\u5bf9\u4efb\u52a11\u548c\u4efb\u52a12\u7684\u63d0\u4ea4\u90fd\u7531\u751f\u7269\u533b\u5b66\u4e13\u5bb6\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u624b\u52a8\u8bc4\u4f30\u3002", "result": "\u5171\u6709\u6765\u81ea\u5341\u4e8c\u4e2a\u56fd\u5bb6\u7684\u5341\u4e8c\u652f\u961f\u4f0d\u53c2\u52a0\u4e86\u8be5\u8d5b\u9053\uff0c\u6a21\u578b\u6db5\u76d6\u4e86\u4ece\u591a\u5c42\u611f\u77e5\u673a\u5230\u5927\u578b\u9884\u8bad\u7ec3\u53d8\u6362\u5668\u3002\u5728\u5bf9\u4efb\u52a11\u7684\u624b\u52a8\u8bc4\u5224\u4e2d\uff0c\u8868\u73b0\u6700\u597d\u7684\u6a21\u578b\u5728\u4e8b\u5b9e\u51c6\u786e\u6027\u548c\u5b8c\u6574\u6027\u65b9\u9762\u53ef\u4e0e\u4eba\u7c7b\u5ab2\u7f8e\uff0c\u4f46\u5728\u7b80\u6d01\u6027\u548c\u7b80\u77ed\u6027\u65b9\u9762\u5219\u4e0d\u80fd\u3002\u57fa\u4e8e\u53c2\u8003\u7684\u81ea\u52a8\u8bc4\u4f30\u6307\u6807\u901a\u5e38\u4e0e\u624b\u52a8\u8bc4\u5224\u7684\u76f8\u5173\u6027\u4e0d\u9ad8\u3002\u5728\u4efb\u52a12\u4e2d\uff0c\u7cfb\u7edf\u5728\u8bc6\u522b\u56f0\u96be\u672f\u8bed\u548c\u5bf9\u5176\u8fdb\u884c\u5206\u7c7b\u4ee5\u8fdb\u884c\u66ff\u6362\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002\u7136\u800c\uff0c\u5728\u751f\u6210\u66ff\u6362\u8bcd\u65b9\u9762\uff0c\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u7cfb\u7edf\u5728\u624b\u52a8\u8bc4\u5224\u7684\u51c6\u786e\u6027\u3001\u5b8c\u6574\u6027\u548c\u7b80\u6d01\u6027\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u7b80\u77ed\u6027\u65b9\u9762\u5219\u4e0d\u7136\u3002", "conclusion": "PLABA\u8d5b\u9053\u5c55\u793a\u4e86\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5c06\u751f\u7269\u533b\u5b66\u6587\u732e\u6539\u7f16\u7ed9\u516c\u4f17\u7684\u6f5c\u529b\uff0c\u540c\u65f6\u4e5f\u51f8\u663e\u4e86\u5b83\u4eec\u7684\u4e0d\u8db3\u4ee5\u53ca\u6539\u8fdb\u81ea\u52a8\u57fa\u51c6\u6d4b\u8bd5\u5de5\u5177\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2507.13998", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13998", "abs": "https://arxiv.org/abs/2507.13998", "authors": ["Itay Katav", "Aryeh Kontorovich"], "title": "ParallelTime: Dynamically Weighting the Balance of Short- and Long-Term Temporal Dependencies", "comment": null, "summary": "Modern multivariate time series forecasting primarily relies on two\narchitectures: the Transformer with attention mechanism and Mamba. In natural\nlanguage processing, an approach has been used that combines local window\nattention for capturing short-term dependencies and Mamba for capturing\nlong-term dependencies, with their outputs averaged to assign equal weight to\nboth. We find that for time-series forecasting tasks, assigning equal weight to\nlong-term and short-term dependencies is not optimal. To mitigate this, we\npropose a dynamic weighting mechanism, ParallelTime Weighter, which calculates\ninterdependent weights for long-term and short-term dependencies for each token\nbased on the input and the model's knowledge. Furthermore, we introduce the\nParallelTime architecture, which incorporates the ParallelTime Weighter\nmechanism to deliver state-of-the-art performance across diverse benchmarks.\nOur architecture demonstrates robustness, achieves lower FLOPs, requires fewer\nparameters, scales effectively to longer prediction horizons, and significantly\noutperforms existing methods. These advances highlight a promising path for\nfuture developments of parallel Attention-Mamba in time series forecasting. The\nimplementation is readily available at:\n\\href{https://github.com/itay1551/ParallelTime}{ParallelTime GitHub", "AI": {"tldr": "\u63d0\u51faParallelTime\u67b6\u6784\uff0c\u901a\u8fc7\u52a8\u6001\u52a0\u6743\u673a\u5236ParallelTime Weighter\u4f18\u5316\u4e86Transformer\u548cMamba\u7ed3\u5408\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u8868\u73b0\uff0c\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u957f\u671f\u548c\u77ed\u671f\u4f9d\u8d56\u5173\u7cfb\u5904\u7406\uff0c\u5e76\u5728\u51c6\u786e\u6027\u3001\u6548\u7387\u548c\u9c81\u68d2\u6027\u4e0a\u53d6\u5f97\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u5c06Transformer\u548cMamba\u7ed3\u5408\u7528\u4e8e\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u65b9\u6cd5\uff0c\u5728\u5904\u7406\u77ed\u671f\u548c\u957f\u671f\u4f9d\u8d56\u5173\u7cfb\u65f6\u8d4b\u4e88\u4e86\u76f8\u540c\u7684\u6743\u91cd\uff0c\u4f46\u7814\u7a76\u53d1\u73b0\u8fd9\u79cd\u5e73\u5747\u52a0\u6743\u5e76\u975e\u6700\u4f18\u3002\u4f5c\u8005\u8ba4\u4e3a\uff0c\u6839\u636e\u8f93\u5165\u548c\u6a21\u578b\u77e5\u8bc6\u52a8\u6001\u8c03\u6574\u957f\u671f\u548c\u77ed\u671f\u4f9d\u8d56\u5173\u7cfb\u7684\u6743\u91cd\uff0c\u80fd\u66f4\u597d\u5730\u8fdb\u884c\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aParallelTime Weighter\u7684\u52a8\u6001\u52a0\u6743\u673a\u5236\uff0c\u8be5\u673a\u5236\u80fd\u591f\u4e3a\u6bcf\u4e2atoken\u8ba1\u7b97\u957f\u671f\u548c\u77ed\u671f\u4f9d\u8d56\u5173\u7cfb\u7684\u76f8\u4e92\u4f9d\u8d56\u6743\u91cd\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u6784\u5efa\u4e86ParallelTime\u67b6\u6784\uff0c\u5c06\u8be5\u673a\u5236\u6574\u5408\u5176\u4e2d\uff0c\u5e76\u7ed3\u5408\u4e86Transformer\uff08\u7528\u4e8e\u6355\u6349\u77ed\u671f\u4f9d\u8d56\uff09\u548cMamba\uff08\u7528\u4e8e\u6355\u6349\u957f\u671f\u4f9d\u8d56\uff09\u7684\u4f18\u70b9\u3002", "result": "ParallelTime\u67b6\u6784\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5b83\u5177\u6709\u66f4\u4f4e\u7684\u8ba1\u7b97\u91cf\uff08FLOPs\uff09\u3001\u66f4\u5c11\u7684\u53c2\u6570\u91cf\u3001\u66f4\u597d\u7684\u53ef\u6269\u5c55\u6027\uff08\u9002\u7528\u4e8e\u66f4\u957f\u7684\u9884\u6d4b\u8303\u56f4\uff09\u4ee5\u53ca\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86ParallelTime\u67b6\u6784\uff0c\u901a\u8fc7\u5f15\u5165\u52a8\u6001\u52a0\u6743\u673a\u5236ParallelTime Weighter\uff0c\u4f18\u5316\u4e86\u7ed3\u5408Transformer\u548cMamba\u5904\u7406\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u6027\u80fd\u3002\u8be5\u673a\u5236\u4e3a\u6bcf\u4e2a\u65f6\u95f4\u5e8f\u5217token\u52a8\u6001\u8ba1\u7b97\u957f\u671f\u548c\u77ed\u671f\u4f9d\u8d56\u5173\u7cfb\u7684\u6743\u91cd\uff0c\u800c\u975e\u7b80\u5355\u5e73\u5747\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cParallelTime\u67b6\u6784\u5728\u51c6\u786e\u6027\u3001\u8ba1\u7b97\u91cf\u3001\u53c2\u6570\u91cf\u3001\u9884\u6d4b\u8303\u56f4\u548c\u9c81\u68d2\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3a\u672a\u6765\u57fa\u4e8eAttention-Mamba\u6a21\u578b\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u9886\u57df\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2507.13719", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13719", "abs": "https://arxiv.org/abs/2507.13719", "authors": ["Daniele Pannone", "Alessia Castronovo", "Maurizio Mancini", "Gian Luca Foresti", "Claudio Piciarelli", "Rossana Gabrieli", "Muhammad Yasir Bilal", "Danilo Avola"], "title": "Augmented Reality in Cultural Heritage: A Dual-Model Pipeline for 3D Artwork Reconstruction", "comment": null, "summary": "This paper presents an innovative augmented reality pipeline tailored for\nmuseum environments, aimed at recognizing artworks and generating accurate 3D\nmodels from single images. By integrating two complementary pre-trained depth\nestimation models, i.e., GLPN for capturing global scene structure and\nDepth-Anything for detailed local reconstruction, the proposed approach\nproduces optimized depth maps that effectively represent complex artistic\nfeatures. These maps are then converted into high-quality point clouds and\nmeshes, enabling the creation of immersive AR experiences. The methodology\nleverages state-of-the-art neural network architectures and advanced computer\nvision techniques to overcome challenges posed by irregular contours and\nvariable textures in artworks. Experimental results demonstrate significant\nimprovements in reconstruction accuracy and visual realism, making the system a\nhighly robust tool for museums seeking to enhance visitor engagement through\ninteractive digital content.", "AI": {"tldr": "\u535a\u7269\u9986\u7684AR\u65b0\u6d41\u7a0b\uff1a\u4f7f\u7528GLPN\u548cDepth-Anything\u4ece\u5355\u4e2a\u56fe\u50cf\u751f\u62103D\u827a\u672f\u54c1\u6a21\u578b\uff0c\u63d0\u9ad8\u51c6\u786e\u6027\u548c\u89c6\u89c9\u6548\u679c\u3002", "motivation": "\u672c\u7814\u7a76\u7684\u76ee\u7684\u662f\u4e3a\u535a\u7269\u9986\u73af\u5883\u5f00\u53d1\u4e00\u79cd\u589e\u5f3a\u73b0\u5b9e\u6d41\u6c34\u7ebf\uff0c\u4ee5\u8bc6\u522b\u827a\u672f\u54c1\u5e76\u4ece\u5355\u4e2a\u56fe\u50cf\u751f\u6210\u51c6\u786e\u76843D\u6a21\u578b\uff0c\u4ece\u800c\u5e94\u5bf9\u827a\u672f\u54c1\u4e0d\u89c4\u5219\u8f6e\u5ed3\u548c\u591a\u53d8\u7eb9\u7406\u5e26\u6765\u7684\u6311\u6218\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u589e\u5f3a\u73b0\u5b9e\u6d41\u7a0b\uff0c\u901a\u8fc7\u96c6\u6210GLPN\u548cDepth-Anything\u4e24\u4e2a\u9884\u8bad\u7ec3\u7684\u6df1\u5ea6\u4f30\u8ba1\u6a21\u578b\u6765\u8bc6\u522b\u827a\u672f\u54c1\u5e76\u4ece\u5355\u4e2a\u56fe\u50cf\u751f\u6210\u51c6\u786e\u76843D\u6a21\u578b\u3002\u8be5\u65b9\u6cd5\u4f18\u5316\u4e86\u6df1\u5ea6\u56fe\u7684\u751f\u6210\uff0c\u5c06\u5176\u8f6c\u6362\u4e3a\u9ad8\u8d28\u91cf\u7684\u70b9\u4e91\u548c\u7f51\u683c\uff0c\u4ee5\u5b9e\u73b0\u6c89\u6d78\u5f0fAR\u4f53\u9a8c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u91cd\u5efa\u7cbe\u5ea6\u548c\u89c6\u89c9\u771f\u5b9e\u611f\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6539\u8fdb\uff0c\u8bc1\u660e\u4e86\u8be5\u7cfb\u7edf\u5728\u589e\u5f3a\u535a\u7269\u9986\u8bbf\u5ba2\u53c2\u4e0e\u5ea6\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4e3a\u535a\u7269\u9986\u63d0\u4f9b\u4e86\u589e\u5f3a\u6e38\u5ba2\u53c2\u4e0e\u5ea6\u7684\u5f3a\u5927\u5de5\u5177\uff0c\u901a\u8fc7\u63d0\u4f9b\u5177\u6709\u9ad8\u91cd\u5efa\u7cbe\u5ea6\u548c\u89c6\u89c9\u771f\u5b9e\u611f\u7684\u4ea4\u4e92\u5f0f\u6570\u5b57\u5185\u5bb9\u3002"}}
{"id": "2507.14005", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14005", "abs": "https://arxiv.org/abs/2507.14005", "authors": ["Mathieu Godbout", "Audrey Durand"], "title": "On the Fundamental Limitations of Dual Static CVaR Decompositions in Markov Decision Processes", "comment": null, "summary": "Recent work has shown that dynamic programming (DP) methods for finding\nstatic CVaR-optimal policies in Markov Decision Processes (MDPs) can fail when\nbased on the dual formulation, yet the root cause for the failure has remained\nunclear. We expand on these findings by shifting focus from policy optimization\nto the seemingly simpler task of policy evaluation. We show that evaluating the\nstatic CVaR of a given policy can be framed as two distinct minimization\nproblems. For their solutions to match, a set of ``risk-assignment consistency\nconstraints'' must be satisfied, and we demonstrate that the intersection of\nthe constraints being empty is the source of previously observed evaluation\nerrors. Quantifying the evaluation error as the CVaR evaluation gap, we then\ndemonstrate that the issues observed when optimizing over the dual-based CVaR\nDP are explained by the returned policy having a non-zero CVaR evaluation gap.\nWe then leverage our proposed risk-assignment perspective to prove that the\nsearch for a single, uniformly optimal policy via on the dual CVaR\ndecomposition is fundamentally limited, identifying an MDP where no single\npolicy can be optimal across all initial risk levels.", "AI": {"tldr": "\u52a8\u6001\u89c4\u5212\u65b9\u6cd5\u5728\u5bfb\u627e\u6700\u4f18\u7b56\u7565\u65f6\u53ef\u80fd\u56e0\u201c\u98ce\u9669\u5206\u914d\u4e00\u81f4\u6027\u7ea6\u675f\u201d\u5931\u6548\u800c\u5931\u8d25\uff0c\u4f18\u5316\u5355\u4e00\u7b56\u7565\u5b58\u5728\u5c40\u9650\u6027\u3002", "motivation": "\u8fd1\u671f\u7814\u7a76\u53d1\u73b0\uff0c\u57fa\u4e8e\u5bf9\u5076\u91cd\u6784\u7684\u52a8\u6001\u89c4\u5212\uff08DP\uff09\u65b9\u6cd5\u5728\u5bfb\u627e\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08MDP\uff09\u7684\u9759\u6001\u6761\u4ef6\u98ce\u9669\u4ef7\u503c\uff08CVaR\uff09\u6700\u4f18\u7b56\u7565\u65f6\u53ef\u80fd\u4f1a\u5931\u8d25\uff0c\u4f46\u5176\u6839\u672c\u539f\u56e0\u5c1a\u4e0d\u660e\u786e\u3002\u672c\u7814\u7a76\u65e8\u5728\u6df1\u5165\u63a2\u7a76\u5e76\u9610\u660e\u8fd9\u4e00\u5931\u8d25\u73b0\u8c61\u7684\u6839\u6e90\uff0c\u5e76\u8fdb\u4e00\u6b65\u5206\u6790\u76f8\u5173\u8bc4\u4f30\u548c\u4f18\u5316\u8fc7\u7a0b\u7684\u5c40\u9650\u6027\u3002", "method": "\u672c\u7814\u7a76\u901a\u8fc7\u5c06\u7ed9\u5b9a\u7b56\u7565\u7684\u9759\u6001CVaR\u8bc4\u4f30\u91cd\u6784\u4e3a\u4e24\u4e2a\u72ec\u7acb\u7684\u6700\u5c0f\u5316\u95ee\u9898\u6765\u5206\u6790\u95ee\u9898\u3002\u7814\u7a76\u5f15\u5165\u4e86\u201c\u98ce\u9669\u5206\u914d\u4e00\u81f4\u6027\u7ea6\u675f\u201d\u7684\u6982\u5ff5\uff0c\u5e76\u8bc1\u660e\u5f53\u8fd9\u4e9b\u7ea6\u675f\u7684\u4ea4\u96c6\u4e3a\u7a7a\u65f6\uff0c\u4f1a\u5bfc\u81f4\u8bc4\u4f30\u9519\u8bef\u3002\u901a\u8fc7\u91cf\u5316\u8bc4\u4f30\u8bef\u5dee\u4e3aCVaR\u8bc4\u4f30\u5dee\u8ddd\uff0c\u5e76\u8bc1\u660e\u5728\u5bf9\u5076CVaR DP\u4f18\u5316\u4e2d\u89c2\u5bdf\u5230\u7684\u95ee\u9898\u6e90\u4e8e\u7b56\u7565\u5177\u6709\u975e\u96f6CVaR\u8bc4\u4f30\u5dee\u8ddd\uff0c\u4ece\u800c\u89e3\u91ca\u4e86\u4f18\u5316\u5931\u8d25\u7684\u539f\u56e0\u3002\u6b64\u5916\uff0c\u7814\u7a76\u5229\u7528\u63d0\u51fa\u7684\u98ce\u9669\u5206\u914d\u89c6\u89d2\uff0c\u8bc1\u660e\u4e86\u901a\u8fc7\u5bf9\u5076CVaR\u5206\u89e3\u5bfb\u627e\u5355\u4e00\u3001\u7edf\u4e00\u6700\u4f18\u7b56\u7565\u7684\u641c\u7d22\u5b58\u5728\u6839\u672c\u6027\u5c40\u9650\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0cCVaR\u8bc4\u4f30\u7684\u51c6\u786e\u6027\u4f9d\u8d56\u4e8e\u4e00\u7ec4\u201c\u98ce\u9669\u5206\u914d\u4e00\u81f4\u6027\u7ea6\u675f\u201d\u7684\u6ee1\u8db3\uff0c\u5f53\u8fd9\u4e9b\u7ea6\u675f\u7684\u4ea4\u96c6\u4e3a\u7a7a\u65f6\uff0c\u4f1a\u5bfc\u81f4\u8bc4\u4f30\u8bef\u5dee\uff0c\u5373CVaR\u8bc4\u4f30\u5dee\u8ddd\u3002\u7814\u7a76\u8fd8\u53d1\u73b0\uff0c\u5728\u5bf9\u5076CVaR DP\u4f18\u5316\u4e2d\u51fa\u73b0\u7684\u8bc4\u4f30\u95ee\u9898\uff0c\u662f\u56e0\u4e3a\u8fd4\u56de\u7684\u7b56\u7565\u5177\u6709\u975e\u96f6\u7684CVaR\u8bc4\u4f30\u5dee\u8ddd\u3002\u6700\u91cd\u8981\u7684\u662f\uff0c\u7814\u7a76\u8bc1\u660e\u4e86\u901a\u8fc7\u5bf9\u5076CVaR\u5206\u89e3\u5bfb\u627e\u5355\u4e00\u3001\u7edf\u4e00\u6700\u4f18\u7b56\u7565\u5b58\u5728\u6839\u672c\u6027\u5c40\u9650\uff0c\u5e76\u4e3e\u4f8b\u8bf4\u660e\u4e86\u5728\u67d0\u4e9bMDP\u4e2d\u4e0d\u5b58\u5728\u80fd\u5728\u6240\u6709\u521d\u59cb\u98ce\u9669\u6c34\u5e73\u4e0b\u90fd\u6700\u4f18\u7684\u5355\u4e00\u7b56\u7565\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u57fa\u4e8e\u5bf9\u5076\u91cd\u6784\u7684\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08MDP\uff09\u52a8\u6001\u89c4\u5212\uff08DP\uff09\u65b9\u6cd5\u5728\u9759\u6001\u6761\u4ef6\u98ce\u9669\u4ef7\u503c\uff08CVaR\uff09\u6700\u4f18\u7b56\u7565\u641c\u7d22\u4e2d\u7684\u5931\u8d25\u6839\u6e90\u5728\u4e8e\u98ce\u9669\u5206\u914d\u4e00\u81f4\u6027\u7ea6\u675f\u7684\u4ea4\u96c6\u4e3a\u7a7a\uff0c\u8fd9\u4f1a\u5bfc\u81f4CVaR\u8bc4\u4f30\u4e2d\u7684\u504f\u5dee\u3002\u7814\u7a76\u8fd8\u8bc1\u660e\uff0c\u901a\u8fc7\u5bf9\u5076CVaR\u5206\u89e3\u5bfb\u627e\u5355\u4e00\u3001\u7edf\u4e00\u6700\u4f18\u7b56\u7565\u7684\u65b9\u6cd5\u5b58\u5728\u6839\u672c\u6027\u5c40\u9650\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5177\u4f53\u6848\u4f8b\uff0c\u8bf4\u660e\u5728\u67d0\u4e9bMDP\u4e2d\u4e0d\u5b58\u5728\u80fd\u5728\u6240\u6709\u521d\u59cb\u98ce\u9669\u6c34\u5e73\u4e0b\u90fd\u6700\u4f18\u7684\u5355\u4e00\u7b56\u7565\u3002"}}
{"id": "2507.13722", "categories": ["cs.CV", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.13722", "abs": "https://arxiv.org/abs/2507.13722", "authors": ["Julia Laubmann", "Johannes Reschke"], "title": "Tackling fake images in cybersecurity -- Interpretation of a StyleGAN and lifting its black-box", "comment": null, "summary": "In today's digital age, concerns about the dangers of AI-generated images are\nincreasingly common. One powerful tool in this domain is StyleGAN (style-based\ngenerative adversarial networks), a generative adversarial network capable of\nproducing highly realistic synthetic faces. To gain a deeper understanding of\nhow such a model operates, this work focuses on analyzing the inner workings of\nStyleGAN's generator component. Key architectural elements and techniques, such\nas the Equalized Learning Rate, are explored in detail to shed light on the\nmodel's behavior. A StyleGAN model is trained using the PyTorch framework,\nenabling direct inspection of its learned weights. Through pruning, it is\nrevealed that a significant number of these weights can be removed without\ndrastically affecting the output, leading to reduced computational\nrequirements. Moreover, the role of the latent vector -- which heavily\ninfluences the appearance of the generated faces -- is closely examined. Global\nalterations to this vector primarily affect aspects like color tones, while\ntargeted changes to individual dimensions allow for precise manipulation of\nspecific facial features. This ability to finetune visual traits is not only of\nacademic interest but also highlights a serious ethical concern: the potential\nmisuse of such technology. Malicious actors could exploit this capability to\nfabricate convincing fake identities, posing significant risks in the context\nof digital deception and cybercrime.", "AI": {"tldr": "StyleGAN\u901a\u8fc7\u4fee\u526a\u6743\u91cd\u53ef\u964d\u4f4e\u8ba1\u7b97\u9700\u6c42\uff0c\u5e76\u80fd\u7cbe\u7ec6\u63a7\u5236\u751f\u6210\u4eba\u8138\u7279\u5f81\uff0c\u4f46\u5b58\u5728\u88ab\u6ee5\u7528\u4e8e\u5236\u9020\u865a\u5047\u8eab\u4efd\u7684\u4f26\u7406\u98ce\u9669\u3002", "motivation": "\u6df1\u5165\u7406\u89e3StyleGAN\u751f\u6210\u5668\u7684\u5de5\u4f5c\u539f\u7406\uff0c\u4ee5\u53ca\u63a2\u7d22\u5176\u6a21\u578b\u538b\u7f29\u548c\u6f5c\u5728\u5411\u91cf\u64cd\u7eb5\u7684\u53ef\u80fd\u6027\uff0c\u540c\u65f6\u5173\u6ce8\u76f8\u5173\u7684\u4f26\u7406\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u4fee\u526aStyleGAN\u751f\u6210\u5668\u4e2d\u7684\u6743\u91cd\u6765\u5206\u6790\u5176\u5bf9\u8f93\u51fa\u7684\u5f71\u54cd\uff0c\u5e76\u7814\u7a76\u6f5c\u5728\u5411\u91cf\u5bf9\u751f\u6210\u4eba\u8138\u7279\u5f81\u7684\u63a7\u5236\u4f5c\u7528\u3002", "result": "\u53d1\u73b0StyleGAN\u6a21\u578b\u53ef\u4ee5\u88ab\u4fee\u526a\u4ee5\u51cf\u5c11\u8ba1\u7b97\u9700\u6c42\uff0c\u5e76\u4e14\u6f5c\u5728\u5411\u91cf\u7684\u8c03\u6574\u53ef\u4ee5\u7cbe\u786e\u63a7\u5236\u751f\u6210\u4eba\u8138\u7684\u7279\u5b9a\u7279\u5f81\uff0c\u4f46\u4e5f\u63ed\u793a\u4e86\u8be5\u6280\u672f\u53ef\u80fd\u88ab\u6ee5\u7528\u4e8e\u5236\u9020\u865a\u5047\u8eab\u4efd\u7684\u98ce\u9669\u3002", "conclusion": "StyleGAN\u901a\u8fc7\u4fee\u526a\u6743\u91cd\u53ef\u4ee5\u51cf\u5c11\u8ba1\u7b97\u9700\u6c42\uff0c\u5e76\u4e14\u53ef\u4ee5\u901a\u8fc7\u8c03\u6574\u6f5c\u5728\u5411\u91cf\u6765\u7cbe\u7ec6\u63a7\u5236\u751f\u6210\u56fe\u50cf\u7684\u7279\u5f81\uff0c\u4f46\u4e5f\u5f15\u53d1\u4e86\u5173\u4e8e\u6f5c\u5728\u6ee5\u7528\u7684\u4f26\u7406\u62c5\u5fe7\u3002"}}
{"id": "2507.13739", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13739", "abs": "https://arxiv.org/abs/2507.13739", "authors": ["Junsu Kim", "Yunhoe Ku", "Seungryul Baek"], "title": "Can Synthetic Images Conquer Forgetting? Beyond Unexplored Doubts in Few-Shot Class-Incremental Learning", "comment": "6th CLVISION ICCV Workshop accepted", "summary": "Few-shot class-incremental learning (FSCIL) is challenging due to extremely\nlimited training data; while aiming to reduce catastrophic forgetting and learn\nnew information. We propose Diffusion-FSCIL, a novel approach that employs a\ntext-to-image diffusion model as a frozen backbone. Our conjecture is that\nFSCIL can be tackled using a large generative model's capabilities benefiting\nfrom 1) generation ability via large-scale pre-training; 2) multi-scale\nrepresentation; 3) representational flexibility through the text encoder. To\nmaximize the representation capability, we propose to extract multiple\ncomplementary diffusion features to play roles as latent replay with slight\nsupport from feature distillation for preventing generative biases. Our\nframework realizes efficiency through 1) using a frozen backbone; 2) minimal\ntrainable components; 3) batch processing of multiple feature extractions.\nExtensive experiments on CUB-200, \\emph{mini}ImageNet, and CIFAR-100 show that\nDiffusion-FSCIL surpasses state-of-the-art methods, preserving performance on\npreviously learned classes and adapting effectively to new ones.", "AI": {"tldr": "Diffusion-FSCIL uses a frozen diffusion model to overcome data limitations and catastrophic forgetting in few-shot class-incremental learning, achieving better results than prior methods.", "motivation": "The paper addresses the challenges of Few-shot class-incremental learning (FSCIL), which suffers from limited data and catastrophic forgetting. The proposed approach leverages the capabilities of large generative models, specifically text-to-image diffusion models, for their generation ability, multi-scale representation, and representational flexibility via text encoders, to improve FSCIL performance.", "method": "Diffusion-FSCIL utilizes a frozen text-to-image diffusion model as its backbone, extracting multiple complementary diffusion features as latent replay, augmented by feature distillation to prevent generative biases. The framework prioritizes efficiency through a frozen backbone, minimal trainable components, and batch processing for feature extraction.", "result": "The proposed Diffusion-FSCIL framework demonstrates superior performance compared to existing state-of-the-art methods in FSCIL.", "conclusion": "Diffusion-FSCIL surpasses state-of-the-art methods on CUB-200, miniImageNet, and CIFAR-100 benchmarks, effectively preserving performance on previous classes while adapting to new ones."}}
{"id": "2507.14038", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14038", "abs": "https://arxiv.org/abs/2507.14038", "authors": ["Aileen Luo", "Tao Zhou", "Ming Du", "Martin V. Holt", "Andrej Singer", "Mathew J. Cherukara"], "title": "DONUT: Physics-aware Machine Learning for Real-time X-ray Nanodiffraction Analysis", "comment": null, "summary": "Coherent X-ray scattering techniques are critical for investigating the\nfundamental structural properties of materials at the nanoscale. While\nadvancements have made these experiments more accessible, real-time analysis\nremains a significant bottleneck, often hindered by artifacts and computational\ndemands. In scanning X-ray nanodiffraction microscopy, which is widely used to\nspatially resolve structural heterogeneities, this challenge is compounded by\nthe convolution of the divergent beam with the sample's local structure. To\naddress this, we introduce DONUT (Diffraction with Optics for Nanobeam by\nUnsupervised Training), a physics-aware neural network designed for the rapid\nand automated analysis of nanobeam diffraction data. By incorporating a\ndifferentiable geometric diffraction model directly into its architecture,\nDONUT learns to predict crystal lattice strain and orientation in real-time.\nCrucially, this is achieved without reliance on labeled datasets or\npre-training, overcoming a fundamental limitation for supervised machine\nlearning in X-ray science. We demonstrate experimentally that DONUT accurately\nextracts all features within the data over 200 times more efficiently than\nconventional fitting methods.", "AI": {"tldr": "DONUT\u662f\u4e00\u79cd\u65b0\u7684\u7269\u7406\u611f\u77e5\u795e\u7ecf\u7f51\u7edc\uff0c\u53ef\u4ee5\u901a\u8fc7\u7eb3\u7c73\u675fX\u5c04\u7ebf\u884d\u5c04\u6570\u636e\u5b9e\u65f6\u63d0\u53d6\u6676\u683c\u5e94\u53d8\u548c\u53d6\u5411\uff0c\u4e14\u65e0\u9700\u9884\u8bad\u7ec3\u6216\u6807\u8bb0\u6570\u636e\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u626b\u63cfX\u5c04\u7ebf\u7eb3\u7c73\u884d\u5c04\u663e\u5fae\u955c\u5728\u5b9e\u65f6\u5206\u6790\u4e2d\u5b58\u5728\u7684\u4f2a\u5f71\u548c\u8ba1\u7b97\u9700\u6c42\u95ee\u9898\uff0c\u4ee5\u53ca\u5149\u675f\u53d1\u6563\u4e0e\u6837\u54c1\u5c40\u90e8\u7ed3\u6784\u5377\u79ef\u7684\u6311\u6218\u3002", "method": "DONUT\u662f\u4e00\u79cd\u7ed3\u5408\u4e86\u7269\u7406\u77e5\u8bc6\u7684\u795e\u7ecf\u7f51\u7edc\uff0c\u901a\u8fc7\u53ef\u5fae\u5206\u51e0\u4f55\u884d\u5c04\u6a21\u578b\u76f4\u63a5\u4f5c\u7528\u4e8e\u5176\u67b6\u6784\uff0c\u4ee5\u5feb\u901f\u3001\u81ea\u52a8\u5730\u5206\u6790\u7eb3\u7c73\u675f\u884d\u5c04\u6570\u636e\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cDONUT\u7684\u6548\u7387\u6bd4\u4f20\u7edf\u62df\u5408\u65b9\u6cd5\u9ad8\u51fa200\u591a\u500d\uff0c\u80fd\u591f\u51c6\u786e\u5730\u63d0\u53d6\u6570\u636e\u4e2d\u7684\u6240\u6709\u7279\u5f81\u3002", "conclusion": "DONUT\u514b\u670d\u4e86X\u5c04\u7ebf\u79d1\u5b66\u4e2d\u6709\u76d1\u7763\u673a\u5668\u5b66\u4e60\u7684\u6839\u672c\u9650\u5236\uff0c\u65e0\u9700\u6807\u8bb0\u6570\u636e\u96c6\u6216\u9884\u8bad\u7ec3\u5373\u53ef\u5b9e\u65f6\u9884\u6d4b\u6676\u683c\u5e94\u53d8\u548c\u53d6\u5411\u3002"}}
{"id": "2507.13753", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13753", "abs": "https://arxiv.org/abs/2507.13753", "authors": ["Tongtong Su", "Chengyu Wang", "Bingyan Liu", "Jun Huang", "Dongming Lu"], "title": "Encapsulated Composition of Text-to-Image and Text-to-Video Models for High-Quality Video Synthesis", "comment": null, "summary": "In recent years, large text-to-video (T2V) synthesis models have garnered\nconsiderable attention for their abilities to generate videos from textual\ndescriptions. However, achieving both high imaging quality and effective motion\nrepresentation remains a significant challenge for these T2V models. Existing\napproaches often adapt pre-trained text-to-image (T2I) models to refine video\nframes, leading to issues such as flickering and artifacts due to\ninconsistencies across frames. In this paper, we introduce EVS, a training-free\nEncapsulated Video Synthesizer that composes T2I and T2V models to enhance both\nvisual fidelity and motion smoothness of generated videos. Our approach\nutilizes a well-trained diffusion-based T2I model to refine low-quality video\nframes by treating them as out-of-distribution samples, effectively optimizing\nthem with noising and denoising steps. Meanwhile, we employ T2V backbones to\nensure consistent motion dynamics. By encapsulating the T2V temporal-only prior\ninto the T2I generation process, EVS successfully leverages the strengths of\nboth types of models, resulting in videos of improved imaging and motion\nquality. Experimental results validate the effectiveness of our approach\ncompared to previous approaches. Our composition process also leads to a\nsignificant improvement of 1.6x-4.5x speedup in inference time. Source codes:\nhttps://github.com/Tonniia/EVS.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.14056", "categories": ["cs.LG", "cs.AI", "q-bio.NC", "68T05"], "pdf": "https://arxiv.org/pdf/2507.14056", "abs": "https://arxiv.org/abs/2507.14056", "authors": ["Alejandro Rodriguez-Garcia", "Anindya Ghosh", "Srikanth Ramaswamy"], "title": "Noradrenergic-inspired gain modulation attenuates the stability gap in joint training", "comment": "18 pages, 5 figures, 1 table, 1 pseudo-code", "summary": "Recent studies in continual learning have identified a transient drop in\nperformance on mastered tasks when assimilating new ones, known as the\nstability gap. Such dynamics contradict the objectives of continual learning,\nrevealing a lack of robustness in mitigating forgetting, and notably,\npersisting even under an ideal joint-loss regime. Examining this gap within\nthis idealized joint training context is critical to isolate it from other\nsources of forgetting. We argue that it reflects an imbalance between rapid\nadaptation and robust retention at task boundaries, underscoring the need to\ninvestigate mechanisms that reconcile plasticity and stability within continual\nlearning frameworks. Biological brains navigate a similar dilemma by operating\nconcurrently on multiple timescales, leveraging neuromodulatory signals to\nmodulate synaptic plasticity. However, artificial networks lack native\nmultitimescale dynamics, and although optimizers like momentum-SGD and Adam\nintroduce implicit timescale regularization, they still exhibit stability gaps.\nInspired by locus coeruleus mediated noradrenergic bursts, which transiently\nenhance neuronal gain under uncertainty to facilitate sensory assimilation, we\npropose uncertainty-modulated gain dynamics - an adaptive mechanism that\napproximates a two-timescale optimizer and dynamically balances integration of\nknowledge with minimal interference on previously consolidated information. We\nevaluate our mechanism on domain-incremental and class-incremental variants of\nthe MNIST and CIFAR benchmarks under joint training, demonstrating that\nuncertainty-modulated gain dynamics effectively attenuate the stability gap.\nFinally, our analysis elucidates how gain modulation replicates noradrenergic\nfunctions in cortical circuits, offering mechanistic insights into reducing\nstability gaps and enhance performance in continual learning tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4e0d\u786e\u5b9a\u6027\u8c03\u5236\u589e\u76ca\u52a8\u6001\u673a\u5236\uff0c\u6a21\u4eff\u751f\u7269\u5927\u8111\u591a\u65f6\u95f4\u5c3a\u5ea6\u8fd0\u4f5c\uff0c\u6709\u6548\u51cf\u5c0f\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u7a33\u5b9a\u6027\u5dee\u8ddd\u3002", "motivation": "\u73b0\u6709\u7684\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u5728\u5b66\u4e60\u65b0\u4efb\u52a1\u65f6\uff0c\u5728\u5df2\u638c\u63e1\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u4f1a\u77ed\u6682\u4e0b\u964d\uff08\u7a33\u5b9a\u6027\u5dee\u8ddd\uff09\uff0c\u5373\u4f7f\u5728\u7406\u60f3\u7684\u8054\u5408\u635f\u5931\u4e0b\u4e5f\u5b58\u5728\uff0c\u8fd9\u8868\u660e\u5176\u5728\u7f13\u89e3\u9057\u5fd8\u65b9\u9762\u7f3a\u4e4f\u9c81\u68d2\u6027\u3002\u8fd9\u79cd\u73b0\u8c61\u4e0e\u6301\u7eed\u5b66\u4e60\u7684\u76ee\u6807\u76f8\u6096\uff0c\u56e0\u6b64\u9700\u8981\u6df1\u5165\u7814\u7a76\u4ee5\u89e3\u51b3\u6b64\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u4e0d\u786e\u5b9a\u6027\u8c03\u5236\u589e\u76ca\u52a8\u6001\u201d\u7684\u81ea\u9002\u5e94\u673a\u5236\uff0c\u8be5\u673a\u5236\u8fd1\u4f3c\u4e8e\u53cc\u65f6\u95f4\u5c3a\u5ea6\u4f18\u5316\u5668\uff0c\u80fd\u591f\u52a8\u6001\u5730\u5e73\u8861\u77e5\u8bc6\u6574\u5408\u548c\u5bf9\u5148\u524d\u5de9\u56fa\u4fe1\u606f\u7684\u5e72\u6270\u3002\u901a\u8fc7\u5728MNIST\u548cCIFAR\u6570\u636e\u96c6\u7684\u57df\u589e\u91cf\u548c\u7c7b\u589e\u91cf\u53d8\u4f53\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u8bc1\u660e\u4e86\u8be5\u673a\u5236\u80fd\u6709\u6548\u51cf\u5c0f\u7a33\u5b9a\u6027\u5dee\u8ddd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0d\u786e\u5b9a\u6027\u8c03\u5236\u589e\u76ca\u52a8\u6001\u673a\u5236\u6709\u6548\u5730\u51cf\u5c0f\u4e86\u7a33\u5b9a\u6027\u5dee\u8ddd\uff0c\u5e76\u5728MNIST\u548cCIFAR\u6570\u636e\u96c6\u7684\u57df\u589e\u91cf\u548c\u7c7b\u589e\u91cf\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u6539\u8fdb\u7684\u6027\u80fd\u3002\u540c\u65f6\uff0c\u5206\u6790\u63ed\u793a\u4e86\u589e\u76ca\u8c03\u5236\u5982\u4f55\u6a21\u62df\u795e\u7ecf\u53bb\u7532\u80be\u4e0a\u817a\u7d20\u5728\u76ae\u5c42\u56de\u8def\u4e2d\u7684\u529f\u80fd\uff0c\u4e3a\u51cf\u5c11\u7a33\u5b9a\u6027\u5dee\u8ddd\u63d0\u4f9b\u4e86\u673a\u5236\u5c42\u9762\u7684\u7406\u89e3\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u4e0d\u786e\u5b9a\u6027\u8c03\u5236\u589e\u76ca\u52a8\u6001\u673a\u5236\uff0c\u901a\u8fc7\u6a21\u4eff\u751f\u7269\u5927\u8111\u7684\u591a\u65f6\u95f4\u5c3a\u5ea6\u8fd0\u4f5c\u548c\u795e\u7ecf\u8c03\u8d28\u4fe1\u53f7\u7684\u4f5c\u7528\uff0c\u6709\u6548\u51cf\u8f7b\u4e86\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u7a33\u5b9a\u6027\u5dee\u8ddd\uff0c\u4e3a\u63d0\u5347\u6301\u7eed\u5b66\u4e60\u6027\u80fd\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\u3002"}}
{"id": "2507.13769", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13769", "abs": "https://arxiv.org/abs/2507.13769", "authors": ["Mingyang Yu", "Zhijian Wu", "Dingjiang Huang"], "title": "Learning Spectral Diffusion Prior for Hyperspectral Image Reconstruction", "comment": null, "summary": "Hyperspectral image (HSI) reconstruction aims to recover 3D HSI from its\ndegraded 2D measurements. Recently great progress has been made in deep\nlearning-based methods, however, these methods often struggle to accurately\ncapture high-frequency details of the HSI. To address this issue, this paper\nproposes a Spectral Diffusion Prior (SDP) that is implicitly learned from\nhyperspectral images using a diffusion model. Leveraging the powerful ability\nof the diffusion model to reconstruct details, this learned prior can\nsignificantly improve the performance when injected into the HSI model. To\nfurther improve the effectiveness of the learned prior, we also propose the\nSpectral Prior Injector Module (SPIM) to dynamically guide the model to recover\nthe HSI details. We evaluate our method on two representative HSI methods: MST\nand BISRNet. Experimental results show that our method outperforms existing\nnetworks by about 0.5 dB, effectively improving the performance of HSI\nreconstruction.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9ad8\u5149\u8c31\u56fe\u50cf\u91cd\u5efa\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b66\u4e60\u548c\u6ce8\u5165\u5149\u8c31\u5148\u9a8c\uff0c\u89e3\u51b3\u4e86\u7ec6\u8282\u6062\u590d\u4e0d\u4f73\u7684\u95ee\u9898\uff0c\u5e76\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u91cd\u5efa\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u9ad8\u5149\u8c31\u56fe\u50cf\uff08HSI\uff09\u91cd\u5efa\u65b9\u6cd5\u5728\u51c6\u786e\u6355\u83b7HSI\u9ad8\u9891\u7ec6\u8282\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u5b66\u4e60\u5149\u8c31\u6269\u6563\u5148\u9a8c\uff08SDP\uff09\u7684\u65b9\u6cd5\uff0c\u5e76\u8bbe\u8ba1\u4e86\u5149\u8c31\u5148\u9a8c\u6ce8\u5165\u6a21\u5757\uff08SPIM\uff09\u6765\u52a8\u6001\u6307\u5bfc\u6a21\u578b\u6062\u590dHSI\u7ec6\u8282\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728MST\u548cBISRNet\u6570\u636e\u96c6\u4e0a\uff0c\u76f8\u6bd4\u73b0\u6709\u7f51\u7edc\u6027\u80fd\u63d0\u5347\u4e86\u7ea60.5dB\uff0c\u6709\u6548\u63d0\u9ad8\u4e86HSI\u91cd\u5efa\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5f15\u5165\u9690\u5f0f\u5b66\u4e60\u7684\u5149\u8c31\u6269\u6563\u5148\u9a8c\uff08SDP\uff09\u548c\u5149\u8c31\u5148\u9a8c\u6ce8\u5165\u6a21\u5757\uff08SPIM\uff09\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u9ad8\u5149\u8c31\u56fe\u50cf\uff08HSI\uff09\u91cd\u5efa\u7684\u6027\u80fd\uff0c\u5728MST\u548cBISRNet\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\uff0c\u6027\u80fd\u63d0\u5347\u4e86\u7ea60.5dB\u3002"}}
{"id": "2507.14066", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14066", "abs": "https://arxiv.org/abs/2507.14066", "authors": ["Ni Mu", "Yao Luan", "Qing-Shan Jia"], "title": "Preference-based Multi-Objective Reinforcement Learning", "comment": "This article has been accepted for publication in IEEE Transactions\n  on Automation Science and Engineering. This is the author's version, which\n  has not been fully edited, and the content may change prior to final\n  publication. \\c{opyright} 2025 IEEE. All rights reserved, including rights\n  for text and data mining and training of artificial intelligence and similar\n  technologies", "summary": "Multi-objective reinforcement learning (MORL) is a structured approach for\noptimizing tasks with multiple objectives. However, it often relies on\npre-defined reward functions, which can be hard to design for balancing\nconflicting goals and may lead to oversimplification. Preferences can serve as\nmore flexible and intuitive decision-making guidance, eliminating the need for\ncomplicated reward design. This paper introduces preference-based MORL\n(Pb-MORL), which formalizes the integration of preferences into the MORL\nframework. We theoretically prove that preferences can derive policies across\nthe entire Pareto frontier. To guide policy optimization using preferences, our\nmethod constructs a multi-objective reward model that aligns with the given\npreferences. We further provide theoretical proof to show that optimizing this\nreward model is equivalent to training the Pareto optimal policy. Extensive\nexperiments in benchmark multi-objective tasks, a multi-energy management task,\nand an autonomous driving task on a multi-line highway show that our method\nperforms competitively, surpassing the oracle method, which uses the ground\ntruth reward function. This highlights its potential for practical applications\nin complex real-world systems.", "AI": {"tldr": "\u63d0\u51faPb-MORL\u6846\u67b6\uff0c\u5229\u7528\u504f\u597d\u66ff\u4ee3\u5956\u52b1\u51fd\u6570\uff0c\u901a\u8fc7\u4f18\u5316\u4e0e\u504f\u597d\u4e00\u81f4\u7684\u5956\u52b1\u6a21\u578b\u6765\u5b66\u4e60\u5e15\u7d2f\u6258\u6700\u4f18\u7b56\u7565\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8eoracle\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u7684MORL\u65b9\u6cd5\u4f9d\u8d56\u9884\u5b9a\u4e49\u7684\u5956\u52b1\u51fd\u6570\uff0c\u8bbe\u8ba1\u56f0\u96be\u4e14\u53ef\u80fd\u5bfc\u81f4\u76ee\u6807\u51b2\u7a81\u7684\u8fc7\u5ea6\u7b80\u5316\u3002\u504f\u597d\u4f5c\u4e3a\u4e00\u79cd\u66f4\u7075\u6d3b\u3001\u66f4\u76f4\u89c2\u7684\u51b3\u7b56\u6307\u5bfc\uff0c\u53ef\u4ee5\u6d88\u9664\u590d\u6742\u7684\u5956\u52b1\u8bbe\u8ba1\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u504f\u597d\u5f15\u5bfc\u7684\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\uff08Pb-MORL\uff09\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5c06\u504f\u597d\u5f62\u5f0f\u5316\u5730\u6574\u5408\u5230MORL\u6846\u67b6\u4e2d\uff0c\u901a\u8fc7\u6784\u5efa\u4e0e\u7ed9\u5b9a\u504f\u597d\u4e00\u81f4\u7684\u591a\u76ee\u6807\u5956\u52b1\u6a21\u578b\u6765\u6307\u5bfc\u7b56\u7565\u4f18\u5316\uff0c\u5e76\u7406\u8bba\u8bc1\u660e\u4e86\u4f18\u5316\u8be5\u5956\u52b1\u6a21\u578b\u7b49\u540c\u4e8e\u8bad\u7ec3\u5e15\u7d2f\u6258\u6700\u4f18\u7b56\u7565\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u4e86\u504f\u597d\u53ef\u4ee5\u5bfc\u51fa\u6574\u4e2a\u5e15\u7d2f\u6258\u524d\u6cbf\u7684\u7b56\u7565\uff1b\u5b9e\u9a8c\u8bc1\u660e\uff0c\u6240\u63d0\u51fa\u7684Pb-MORL\u65b9\u6cd5\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8eoracle\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u57fa\u51c6\u591a\u76ee\u6807\u4efb\u52a1\u3001\u591a\u80fd\u6e90\u7ba1\u7406\u4efb\u52a1\u4ee5\u53ca\u591a\u7ebf\u516c\u8def\u4e0a\u7684\u81ea\u52a8\u9a7e\u9a76\u4efb\u52a1\u4e2d\u8868\u73b0\u5177\u6709\u7ade\u4e89\u529b\uff0c\u751a\u81f3\u4f18\u4e8e\u4f7f\u7528\u771f\u5b9e\u5956\u52b1\u51fd\u6570\u304d\u306eoracle\u65b9\u6cd5\uff0c\u663e\u793a\u51fa\u5176\u5728\u590d\u6742\u5b9e\u9645\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.13772", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13772", "abs": "https://arxiv.org/abs/2507.13772", "authors": ["Abhijit Sen", "Giridas Maiti", "Bikram K. Parida", "Bhanu P. Mishra", "Mahima Arya", "Denys I. Bondar"], "title": "Feature Engineering is Not Dead: Reviving Classical Machine Learning with Entropy, HOG, and LBP Feature Fusion for Image Classification", "comment": null, "summary": "Feature engineering continues to play a critical role in image\nclassification, particularly when interpretability and computational efficiency\nare prioritized over deep learning models with millions of parameters. In this\nstudy, we revisit classical machine learning based image classification through\na novel approach centered on Permutation Entropy (PE), a robust and\ncomputationally lightweight measure traditionally used in time series analysis\nbut rarely applied to image data. We extend PE to two-dimensional images and\npropose a multiscale, multi-orientation entropy-based feature extraction\napproach that characterizes spatial order and complexity along rows, columns,\ndiagonals, anti-diagonals, and local patches of the image. To enhance the\ndiscriminatory power of the entropy features, we integrate two classic image\ndescriptors: the Histogram of Oriented Gradients (HOG) to capture shape and\nedge structure, and Local Binary Patterns (LBP) to encode micro-texture of an\nimage. The resulting hand-crafted feature set, comprising of 780 dimensions, is\nused to train Support Vector Machine (SVM) classifiers optimized through grid\nsearch. The proposed approach is evaluated on multiple benchmark datasets,\nincluding Fashion-MNIST, KMNIST, EMNIST, and CIFAR-10, where it delivers\ncompetitive classification performance without relying on deep architectures.\nOur results demonstrate that the fusion of PE with HOG and LBP provides a\ncompact, interpretable, and effective alternative to computationally expensive\nand limited interpretable deep learning models. This shows a potential of\nentropy-based descriptors in image classification and contributes a lightweight\nand generalizable solution to interpretable machine learning in image\nclassification and computer vision.", "AI": {"tldr": "\u901a\u8fc7\u7ed3\u5408\u6392\u5217\u71b5\u3001HOG\u548cLBP\u7279\u5f81\uff0c\u5e76\u4f7f\u7528SVM\u8fdb\u884c\u5206\u7c7b\uff0c\u5728\u4e0d\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u56fe\u50cf\u5206\u7c7b\u6027\u80fd\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u4e14\u53ef\u89e3\u91ca\u7684\u65b9\u6cd5\u3002", "motivation": "\u5728\u4f18\u5148\u8003\u8651\u53ef\u89e3\u91ca\u6027\u548c\u8ba1\u7b97\u6548\u7387\u800c\u975e\u53c2\u6570\u91cf\u5e9e\u5927\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u573a\u666f\u4e0b\uff0c\u91cd\u65b0\u5ba1\u89c6\u57fa\u4e8e\u7ecf\u5178\u673a\u5668\u5b66\u4e60\u7684\u56fe\u50cf\u5206\u7c7b\u65b9\u6cd5\uff0c\u63a2\u7d22\u65b0\u7684\u7279\u5f81\u5de5\u7a0b\u6280\u672f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u6392\u5217\u71b5\uff08PE\uff09\u7684\u56fe\u50cf\u5206\u7c7b\u65b9\u6cd5\uff0c\u5c06PE\u6269\u5c55\u5230\u4e8c\u7ef4\u56fe\u50cf\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u5c3a\u5ea6\u3001\u591a\u65b9\u5411\u7684\u71b5\u7279\u5f81\u63d0\u53d6\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u65b9\u5411\u68af\u5ea6\u76f4\u65b9\u56fe\uff08HOG\uff09\u548c\u5c40\u90e8\u4e8c\u503c\u6a21\u5f0f\uff08LBP\uff09\u6765\u589e\u5f3a\u7279\u5f81\u7684\u533a\u5206\u80fd\u529b\u3002\u6700\u7ec8\u5f62\u6210\u4e00\u4e2a\u5305\u542b780\u4e2a\u7ef4\u5ea6\u7684\u624b\u5de5\u7279\u5f81\u96c6\uff0c\u5e76\u4f7f\u7528SVM\u5206\u7c7b\u5668\u8fdb\u884c\u8bad\u7ec3\u548c\u4f18\u5316\u3002", "result": "\u8be5\u65b9\u6cd5\u5728Fashion-MNIST\u3001KMNIST\u3001EMMNIST\u548cCIFAR-10\u7b49\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u53d6\u5f97\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u5206\u7c7b\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7684\u71b5\u7279\u5f81\u878d\u5408\u65b9\u6cd5\u5728\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e0a\u7684\u6709\u6548\u6027\u548c\u6f5c\u529b\u3002", "conclusion": "\u71b5\u7279\u5f81\uff08\u5305\u62ec\u6392\u5217\u71b5\u3001HOG\u548cLBP\uff09\u4e0eSVM\u5206\u7c7b\u5668\u76f8\u7ed3\u5408\uff0c\u5728\u4e0d\u4f9d\u8d56\u6df1\u5ea6\u67b6\u6784\u7684\u60c5\u51b5\u4e0b\uff0c\u4e3a\u56fe\u50cf\u5206\u7c7b\u63d0\u4f9b\u4e86\u4e00\u79cd\u7d27\u51d1\u3001\u53ef\u89e3\u91ca\u4e14\u6709\u6548\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u8bc1\u660e\u4e86\u71b5\u63cf\u8ff0\u5b50\u5728\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u4e3a\u53ef\u89e3\u91ca\u7684\u673a\u5668\u5b66\u4e60\u63d0\u4f9b\u4e86\u8f7b\u91cf\u7ea7\u4e14\u53ef\u6cdb\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.14088", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14088", "abs": "https://arxiv.org/abs/2507.14088", "authors": ["Xiyun Li", "Yining Ding", "Yuhua Jiang", "Yunlong Zhao", "Runpeng Xie", "Shuang Xu", "Yuanhua Ni", "Yiqin Yang", "Bo Xu"], "title": "DPMT: Dual Process Multi-scale Theory of Mind Framework for Real-time Human-AI Collaboration", "comment": null, "summary": "Real-time human-artificial intelligence (AI) collaboration is crucial yet\nchallenging, especially when AI agents must adapt to diverse and unseen human\nbehaviors in dynamic scenarios. Existing large language model (LLM) agents\noften fail to accurately model the complex human mental characteristics such as\ndomain intentions, especially in the absence of direct communication. To\naddress this limitation, we propose a novel dual process multi-scale theory of\nmind (DPMT) framework, drawing inspiration from cognitive science dual process\ntheory. Our DPMT framework incorporates a multi-scale theory of mind (ToM)\nmodule to facilitate robust human partner modeling through mental\ncharacteristic reasoning. Experimental results demonstrate that DPMT\nsignificantly enhances human-AI collaboration, and ablation studies further\nvalidate the contributions of our multi-scale ToM in the slow system.", "AI": {"tldr": "A new DPMT framework improves human-AI collaboration by better modeling human mental characteristics using a multi-scale ToM module, especially in situations lacking direct communication.", "motivation": "Real-time human-AI collaboration is crucial yet challenging, especially when AI agents must adapt to diverse and unseen human behaviors in dynamic scenarios. Existing LLM agents often fail to accurately model complex human mental characteristics such as domain intentions, especially in the absence of direct communication.", "method": "The paper proposes a novel dual process multi-scale theory of mind (DPMT) framework, which incorporates a multi-scale theory of mind (ToM) module to facilitate robust human partner modeling through mental characteristic reasoning.", "result": "Experimental results demonstrate that DPMT significantly enhances human-AI collaboration.", "conclusion": "DPMT significantly enhances human-AI collaboration, and ablation studies further validate the contributions of our multi-scale ToM in the slow system."}}
{"id": "2507.13773", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.13773", "abs": "https://arxiv.org/abs/2507.13773", "authors": ["Pu Jian", "Donglei Yu", "Wen Yang", "Shuo Ren", "Jiajun Zhang"], "title": "Teaching Vision-Language Models to Ask: Resolving Ambiguity in Visual Questions", "comment": "ACL2025 Main", "summary": "In visual question answering (VQA) context, users often pose ambiguous\nquestions to visual language models (VLMs) due to varying expression habits.\nExisting research addresses such ambiguities primarily by rephrasing questions.\nThese approaches neglect the inherently interactive nature of user interactions\nwith VLMs, where ambiguities can be clarified through user feedback. However,\nresearch on interactive clarification faces two major challenges: (1)\nBenchmarks are absent to assess VLMs' capacity for resolving ambiguities\nthrough interaction; (2) VLMs are trained to prefer answering rather than\nasking, preventing them from seeking clarification. To overcome these\nchallenges, we introduce \\textbf{ClearVQA} benchmark, which targets three\ncommon categories of ambiguity in VQA context, and encompasses various VQA\nscenarios.", "AI": {"tldr": "The ClearVQA benchmark is introduced to help visual language models (VLMs) better handle ambiguous questions by enabling them to ask clarifying questions instead of just trying to answer them, which is a common issue due to users' varied expression habits.", "motivation": "Users often pose ambiguous questions to VLMs due to varying expression habits. Overcoming the challenges in interactive clarification, such as the absence of benchmarks and VLMs' preference for answering over asking, is crucial for improving VLM performance.", "method": "The paper introduces the ClearVQA benchmark to address the challenges in interactive clarification, targeting three common categories of ambiguity in VQA context and encompassing various VQA scenarios. This benchmark aims to assess VLMs' capacity for resolving ambiguities through interaction.", "result": "The ClearVQA benchmark is introduced to assess VLMs' capacity for resolving ambiguities through interaction.", "conclusion": "Existing research primarily addresses ambiguity by rephrasing questions, neglecting the interactive nature of user interactions with VLMs where ambiguities can be clarified through user feedback."}}
{"id": "2507.14121", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14121", "abs": "https://arxiv.org/abs/2507.14121", "authors": ["Pankaj Yadav", "Vivek Vijay"], "title": "Kolmogorov Arnold Networks (KANs) for Imbalanced Data -- An Empirical Perspective", "comment": "9 Pages, 4 figures", "summary": "Kolmogorov Arnold Networks (KANs) are recent architectural advancement in\nneural computation that offer a mathematically grounded alternative to standard\nneural networks. This study presents an empirical evaluation of KANs in context\nof class imbalanced classification, using ten benchmark datasets. We observe\nthat KANs can inherently perform well on raw imbalanced data more effectively\nthan Multi-Layer Perceptrons (MLPs) without any resampling strategy. However,\nconventional imbalance strategies fundamentally conflict with KANs mathematical\nstructure as resampling and focal loss implementations significantly degrade\nKANs performance, while marginally benefiting MLPs. Crucially, KANs suffer from\nprohibitive computational costs without proportional performance gains.\nStatistical validation confirms that MLPs with imbalance techniques achieve\nequivalence with KANs (|d| < 0.08 across metrics) at minimal resource costs.\nThese findings reveal that KANs represent a specialized solution for raw\nimbalanced data where resources permit. But their severe performance-resource\ntradeoffs and incompatibility with standard resampling techniques currently\nlimits practical deployment. We identify critical research priorities as\ndeveloping KAN specific architectural modifications for imbalance learning,\noptimizing computational efficiency, and theoretical reconciling their conflict\nwith data augmentation. This work establishes foundational insights for next\ngeneration KAN architectures in imbalanced classification scenarios.", "AI": {"tldr": "KANs\u5728\u539f\u59cb\u4e0d\u5e73\u8861\u6570\u636e\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u4e0d\u517c\u5bb9\u6807\u51c6\u4e0d\u5e73\u8861\u5904\u7406\u65b9\u6cd5\u3002MLPs\u7ed3\u5408\u4e0d\u5e73\u8861\u6280\u672f\u5728\u6548\u7387\u548c\u6027\u80fd\u4e0a\u66f4\u5177\u4f18\u52bf\u3002", "motivation": "\u8bc4\u4f30KANs\u5728\u7c7b\u522b\u4e0d\u5e73\u8861\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u7814\u7a76\u5176\u4e0e\u5e38\u89c4\u4e0d\u5e73\u8861\u5904\u7406\u7b56\u7565\u7684\u517c\u5bb9\u6027\uff0c\u4ee5\u4e86\u89e3\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u548c\u5c40\u9650\u6027\u3002", "method": "\u5bf9KANs\u572810\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u7c7b\u522b\u4e0d\u5e73\u8861\u5206\u7c7b\u7684\u5b9e\u8bc1\u8bc4\u4f30\uff0c\u5e76\u4e0eMLPs\u8fdb\u884c\u4e86\u5bf9\u6bd4\uff0c\u540c\u65f6\u6d4b\u8bd5\u4e86\u5e38\u89c4\u4e0d\u5e73\u8861\u7b56\u7565\uff08\u5982\u91cd\u91c7\u6837\u548cFocal Loss\uff09\u5bf9KANs\u548cMLPs\u7684\u5f71\u54cd\u3002", "result": "KANs\u5728\u539f\u59cb\u4e0d\u5e73\u8861\u6570\u636e\u4e0a\u4f18\u4e8eMLPs\uff0c\u65e0\u9700\u91cd\u91c7\u6837\u3002\u7136\u800c\uff0c\u91cd\u91c7\u6837\u548cFocal Loss\u7b49\u7b56\u7565\u4f1a\u4e25\u91cd\u964d\u4f4eKANs\u7684\u6027\u80fd\u3002\u7ed3\u5408\u4e0d\u5e73\u8861\u6280\u672f\u7684MLPs\u5728\u6027\u80fd\u4e0a\u53ef\u4e0eKANs\u5ab2\u7f8e\uff0c\u4f46\u8d44\u6e90\u6d88\u8017\u66f4\u4f4e\u3002KANs\u5b58\u5728\u8ba1\u7b97\u6210\u672c\u9ad8\u4f46\u6027\u80fd\u589e\u76ca\u4e0d\u6210\u6bd4\u4f8b\u7684\u95ee\u9898\u3002", "conclusion": "KANs\u5728\u539f\u59cb\u7c7b\u522b\u4e0d\u5e73\u8861\u6570\u636e\u4e0a\u8868\u73b0\u4f18\u4e8eMLPs\uff0c\u4f46\u5728\u5b9e\u8df5\u5e94\u7528\u4e2d\u5b58\u5728\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u4e0e\u5e38\u89c4\u4e0d\u5e73\u8861\u7b56\u7565\u4e0d\u517c\u5bb9\u7b49\u95ee\u9898\u3002MLPs\u7ed3\u5408\u4e0d\u5e73\u8861\u6280\u672f\u5728\u8d44\u6e90\u6d88\u8017\u548c\u6027\u80fd\u4e0a\u4e0eKANs\u76f8\u5f53\u3002"}}
{"id": "2507.13779", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13779", "abs": "https://arxiv.org/abs/2507.13779", "authors": ["Durgesh Singh", "Ahc\u00e8ne Boubekki", "Robert Jenssen", "Michael Kampffmeyer"], "title": "SuperCM: Improving Semi-Supervised Learning and Domain Adaptation through differentiable clustering", "comment": null, "summary": "Semi-Supervised Learning (SSL) and Unsupervised Domain Adaptation (UDA)\nenhance the model performance by exploiting information from labeled and\nunlabeled data. The clustering assumption has proven advantageous for learning\nwith limited supervision and states that data points belonging to the same\ncluster in a high-dimensional space should be assigned to the same category.\nRecent works have utilized different training mechanisms to implicitly enforce\nthis assumption for the SSL and UDA. In this work, we take a different approach\nby explicitly involving a differentiable clustering module which is extended to\nleverage the supervised data to compute its centroids. We demonstrate the\neffectiveness of our straightforward end-to-end training strategy for SSL and\nUDA over extensive experiments and highlight its benefits, especially in low\nsupervision regimes, both as a standalone model and as a regularizer for\nexisting approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684SSL\u548cUDA\u65b9\u6cd5\uff0c\u901a\u8fc7\u53ef\u5fae\u5206\u805a\u7c7b\u6a21\u5757\u548c\u5229\u7528\u76d1\u7763\u6570\u636e\u8ba1\u7b97\u8d28\u5fc3\uff0c\u5728\u4f4e\u76d1\u7763\u573a\u666f\u4e0b\u6548\u679c\u663e\u8457\u3002", "motivation": "\u5229\u7528\u805a\u7c7b\u5047\u8bbe\uff08\u540c\u4e00\u7c07\u4e2d\u7684\u6570\u636e\u70b9\u5e94\u5c5e\u4e8e\u540c\u4e00\u7c7b\u522b\uff09\u6765\u589e\u5f3aSSL\u548cUDA\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u6807\u8bb0\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u901a\u8fc7\u663e\u5f0f\u5f15\u5165\u53ef\u5fae\u5206\u805a\u7c7b\u6a21\u5757\uff0c\u5e76\u6269\u5c55\u5176\u4ee5\u5229\u7528\u76d1\u7763\u6570\u636e\u8ba1\u7b97\u8d28\u5fc3\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u7684\u8bad\u7ec3\u7b56\u7565\u3002", "result": "\u5728SSL\u548cUDA\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u4e2d\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5c24\u5176\u662f\u5728\u4f4e\u76d1\u7763\u60c5\u51b5\u4e0b\uff0c\u5176\u4f5c\u4e3a\u72ec\u7acb\u6a21\u578b\u548c\u4f5c\u4e3a\u73b0\u6709\u65b9\u6cd5\u7684\u6b63\u5219\u5316\u5668\u90fd\u663e\u793a\u51fa\u4f18\u52bf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u663e\u5f0f\u5f15\u5165\u53ef\u5fae\u5206\u805a\u7c7b\u6a21\u5757\uff0c\u5e76\u5229\u7528\u76d1\u7763\u6570\u636e\u8ba1\u7b97\u8d28\u5fc3\uff0c\u5728SSL\u548cUDA\u4efb\u52a1\u4e0a\u5c55\u793a\u4e86\u6709\u6548\u6027\uff0c\u5c24\u5176\u5728\u4f4e\u76d1\u7763\u573a\u666f\u4e0b\u8868\u73b0\u7a81\u51fa\uff0c\u53ef\u4f5c\u4e3a\u72ec\u7acb\u6a21\u578b\u6216\u73b0\u6709\u65b9\u6cd5\u7684\u6b63\u5219\u5316\u5668\u3002"}}
{"id": "2507.14126", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.14126", "abs": "https://arxiv.org/abs/2507.14126", "authors": ["Jianhong Chen", "Meng Zhao", "Mostafa Reisi Gahrooei", "Xubo Yue"], "title": "Toward Temporal Causal Representation Learning with Tensor Decomposition", "comment": null, "summary": "Temporal causal representation learning is a powerful tool for uncovering\ncomplex patterns in observational studies, which are often represented as\nlow-dimensional time series. However, in many real-world applications, data are\nhigh-dimensional with varying input lengths and naturally take the form of\nirregular tensors. To analyze such data, irregular tensor decomposition is\ncritical for extracting meaningful clusters that capture essential information.\nIn this paper, we focus on modeling causal representation learning based on the\ntransformed information. First, we present a novel causal formulation for a set\nof latent clusters. We then propose CaRTeD, a joint learning framework that\nintegrates temporal causal representation learning with irregular tensor\ndecomposition. Notably, our framework provides a blueprint for downstream tasks\nusing the learned tensor factors, such as modeling latent structures and\nextracting causal information, and offers a more flexible regularization design\nto enhance tensor decomposition. Theoretically, we show that our algorithm\nconverges to a stationary point. More importantly, our results fill the gap in\ntheoretical guarantees for the convergence of state-of-the-art irregular tensor\ndecomposition. Experimental results on synthetic and real-world electronic\nhealth record (EHR) datasets (MIMIC-III), with extensive benchmarks from both\nphenotyping and network recovery perspectives, demonstrate that our proposed\nmethod outperforms state-of-the-art techniques and enhances the explainability\nof causal representations.", "AI": {"tldr": "CaRTeD\u662f\u4e00\u4e2a\u6574\u5408\u4e86\u65f6\u95f4\u56e0\u679c\u8868\u8ff0\u5b66\u4e60\u548c\u4e0d\u89c4\u5219\u5f20\u91cf\u5206\u89e3\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u6790\u9ad8\u7ef4\u3001\u4e0d\u89c4\u5219\u5f20\u91cf\u6570\u636e\uff0c\u5e76\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u90fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u771f\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u7684\u6570\u636e\u901a\u5e38\u662f\u9ad8\u7ef4\u7684\uff0c\u5177\u6709\u4e0d\u540c\u7684\u8f93\u5165\u957f\u5ea6\uff0c\u5e76\u4ee5\u4e0d\u89c4\u5219\u5f20\u91cf\u7684\u5f62\u5f0f\u5b58\u5728\uff0c\u56e0\u6b64\u9700\u8981\u4e0d\u89c4\u5219\u5f20\u91cf\u5206\u89e3\u6765\u63d0\u53d6\u6709\u610f\u4e49\u7684\u805a\u7c7b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u56e0\u679c\u8868\u8ff0\uff0c\u7528\u4e8e\u4e00\u7ec4\u6f5c\u5728\u805a\u7c7b\uff0c\u5e76\u63d0\u51fa\u4e86CaRTeD\uff0c\u4e00\u4e2a\u6574\u5408\u4e86\u65f6\u95f4\u56e0\u679c\u8868\u8ff0\u5b66\u4e60\u548c\u4e0d\u89c4\u5219\u5f20\u91cf\u5206\u89e3\u7684\u8054\u5408\u5b66\u4e60\u6846\u67b6\u3002\u8be5\u6846\u67b6\u4e3a\u4e0b\u6e38\u4efb\u52a1\u63d0\u4f9b\u4e86\u84dd\u56fe\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u7075\u6d3b\u7684\u6b63\u5219\u5316\u8bbe\u8ba1\u6765\u589e\u5f3a\u5f20\u91cf\u5206\u89e3\u3002", "result": "\u7406\u8bba\u4e0a\uff0c\u8bc1\u660e\u4e86\u8be5\u7b97\u6cd5\u6536\u655b\u5230\u4e00\u4e2a\u7a33\u5b9a\u70b9\uff0c\u586b\u8865\u4e86\u73b0\u6709\u4e0d\u89c4\u5219\u5f20\u91cf\u5206\u89e3\u65b9\u6cd5\u5728\u6536\u655b\u6027\u7406\u8bba\u4fdd\u8bc1\u65b9\u9762\u7684\u7a7a\u767d\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728MIMIC-III\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684CaRTeD\u6846\u67b6\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5e76\u5728\u8868\u578b\u5206\u6790\u548c\u7f51\u7edc\u6062\u590d\u65b9\u9762\u589e\u5f3a\u4e86\u56e0\u679c\u8868\u793a\u7684\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2507.13789", "categories": ["cs.CV", "cs.AI", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2507.13789", "abs": "https://arxiv.org/abs/2507.13789", "authors": ["Kyriakos Flouris", "Moritz Halter", "Yolanne Y. R. Lee", "Samuel Castonguay", "Luuk Jacobs", "Pietro Dirix", "Jonathan Nestmann", "Sebastian Kozerke", "Ender Konukoglu"], "title": "Localized FNO for Spatiotemporal Hemodynamic Upsampling in Aneurysm MRI", "comment": null, "summary": "Hemodynamic analysis is essential for predicting aneurysm rupture and guiding\ntreatment. While magnetic resonance flow imaging enables time-resolved\nvolumetric blood velocity measurements, its low spatiotemporal resolution and\nsignal-to-noise ratio limit its diagnostic utility. To address this, we propose\nthe Localized Fourier Neural Operator (LoFNO), a novel 3D architecture that\nenhances both spatial and temporal resolution with the ability to predict wall\nshear stress (WSS) directly from clinical imaging data. LoFNO integrates\nLaplacian eigenvectors as geometric priors for improved structural awareness on\nirregular, unseen geometries and employs an Enhanced Deep Super-Resolution\nNetwork (EDSR) layer for robust upsampling. By combining geometric priors with\nneural operator frameworks, LoFNO de-noises and spatiotemporally upsamples flow\ndata, achieving superior velocity and WSS predictions compared to interpolation\nand alternative deep learning methods, enabling more precise cerebrovascular\ndiagnostics.", "AI": {"tldr": "LoFNO\u662f\u4e00\u79cd\u65b0\u578b3D\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u5408\u51e0\u4f55\u5148\u9a8c\u548c\u8d85\u5206\u8fa8\u7387\u6280\u672f\uff0c\u80fd\u591f\u63d0\u9ad8MRI\u8840\u6d41\u6570\u636e\u7684\u65f6\u7a7a\u5206\u8fa8\u7387\u548c\u4fe1\u566a\u6bd4\uff0c\u4ece\u800c\u66f4\u51c6\u786e\u5730\u9884\u6d4b\u52a8\u8109\u7624\u7684\u8840\u6d41\u52a8\u529b\u5b66\uff0c\u8f85\u52a9\u4e34\u5e8a\u8bca\u65ad\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u78c1\u5171\u632f\u6210\u50cf\uff08MRI\uff09\u5728\u6d4b\u91cf\u52a8\u8109\u7624\u8840\u6d41\u52a8\u529b\u5b66\u65f6\u5b58\u5728\u7684\u65f6\u7a7a\u5206\u8fa8\u7387\u4f4e\u548c\u4fe1\u566a\u6bd4\u4f4e\u7684\u95ee\u9898\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u80fd\u591f\u76f4\u63a5\u4ece\u4e34\u5e8a\u6210\u50cf\u6570\u636e\u4e2d\u9884\u6d4b\u58c1\u9762\u526a\u5207\u5e94\u529b\uff08WSS\uff09\u5e76\u63d0\u9ad8\u65f6\u7a7a\u5206\u8fa8\u7387\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u5c40\u90e8\u5085\u7acb\u53f6\u795e\u7ecf\u7b97\u5b50\uff08LoFNO\uff09\u7684\u65b0\u578b3D\u67b6\u6784\uff0c\u8be5\u67b6\u6784\u5229\u7528\u62c9\u666e\u62c9\u65af\u7279\u5f81\u5411\u91cf\u4f5c\u4e3a\u51e0\u4f55\u5148\u9a8c\u4ee5\u63d0\u9ad8\u5bf9\u4e0d\u89c4\u5219\u3001\u672a\u89c1\u51e0\u4f55\u5f62\u72b6\u7684\u7ed3\u6784\u611f\u77e5\u80fd\u529b\uff0c\u5e76\u91c7\u7528\u589e\u5f3a\u578b\u6df1\u5ea6\u8d85\u5206\u8fa8\u7387\u7f51\u7edc\uff08EDSR\uff09\u5c42\u8fdb\u884c\u9c81\u68d2\u7684\u4e0a\u91c7\u6837\u3002", "result": "LoFNO\u5728\u901f\u5ea6\u548c\u58c1\u9762\u526a\u5207\u5e94\u529b\uff08WSS\uff09\u7684\u9884\u6d4b\u65b9\u9762\uff0c\u76f8\u8f83\u4e8e\u4f20\u7edf\u7684\u63d2\u503c\u65b9\u6cd5\u548c\u5176\u5b83\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u53d6\u5f97\u4e86\u66f4\u4f18\u8d8a\u7684\u6027\u80fd\u3002", "conclusion": "LoFNO\u901a\u8fc7\u7ed3\u5408\u51e0\u4f55\u5148\u9a8c\u548c\u795e\u7ecf\u7b97\u5b50\u6846\u67b6\uff0c\u80fd\u591f\u5bf9\u8840\u6d41\u6570\u636e\u8fdb\u884c\u53bb\u566a\u548c\u65f6\u7a7a\u8d85\u5206\u8fa8\u7387\u5904\u7406\uff0c\u5728\u9884\u6d4b\u901f\u5ea6\u548c\u58c1\u9762\u526a\u5207\u5e94\u529b\uff08WSS\uff09\u65b9\u9762\u4f18\u4e8e\u63d2\u503c\u548c\u5176\u4ed6\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u4ece\u800c\u5b9e\u73b0\u66f4\u7cbe\u786e\u7684\u8111\u8840\u7ba1\u8bca\u65ad\u3002"}}
{"id": "2507.14119", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14119", "abs": "https://arxiv.org/abs/2507.14119", "authors": ["Maksim Kuprashevich", "Grigorii Alekseenko", "Irina Tolstykh", "Georgii Fedorov", "Bulat Suleimanov", "Vladimir Dokholyan", "Aleksandr Gordeev"], "title": "NoHumansRequired: Autonomous High-Quality Image Editing Triplet Mining", "comment": null, "summary": "Recent advances in generative modeling enable image editing assistants that\nfollow natural language instructions without additional user input. Their\nsupervised training requires millions of triplets: original image, instruction,\nedited image. Yet mining pixel-accurate examples is hard. Each edit must affect\nonly prompt-specified regions, preserve stylistic coherence, respect physical\nplausibility, and retain visual appeal. The lack of robust automated\nedit-quality metrics hinders reliable automation at scale. We present an\nautomated, modular pipeline that mines high-fidelity triplets across domains,\nresolutions, instruction complexities, and styles. Built on public generative\nmodels and running without human intervention, our system uses a task-tuned\nGemini validator to score instruction adherence and aesthetics directly,\nremoving any need for segmentation or grounding models. Inversion and\ncompositional bootstrapping enlarge the mined set by approximately 2.2x,\nenabling large-scale high-fidelity training data. By automating the most\nrepetitive annotation steps, the approach allows a new scale of training\nwithout human labeling effort. To democratize research in this\nresource-intensive area, we release NHR-Edit: an open dataset of 358k\nhigh-quality triplets. In the largest cross-dataset evaluation, it surpasses\nall public alternatives. We also release Bagel-NHR-Edit, an open-source\nfine-tuned Bagel model, which achieves state-of-the-art metrics in our\nexperiments.", "AI": {"tldr": "\u901a\u8fc7\u81ea\u52a8\u5316\u6d41\u7a0b\u548cGemini\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u56fe\u50cf\u7f16\u8f91\u8bad\u7ec3\u6570\u636e\u6316\u6398\u7684\u96be\u9898\uff0c\u53d1\u5e03\u4e86NHR-Edit\u6570\u636e\u96c6\u548cBagel-NHR-Edit\u6a21\u578b\uff0c\u63d0\u5347\u4e86\u8bad\u7ec3\u6570\u636e\u7684\u89c4\u6a21\u548c\u8d28\u91cf\u3002", "motivation": "\u76d1\u7763\u8bad\u7ec3\u56fe\u50cf\u7f16\u8f91\u6a21\u578b\u9700\u8981\u5927\u91cf\u7684\uff08\u539f\u59cb\u56fe\u50cf\u3001\u6307\u4ee4\u3001\u7f16\u8f91\u56fe\u50cf\uff09\u4e09\u5143\u7ec4\u6570\u636e\uff0c\u4f46\u6316\u6398\u50cf\u7d20\u7ea7\u7cbe\u786e\u3001\u98ce\u683c\u4e00\u81f4\u3001\u7b26\u5408\u7269\u7406\u89c4\u5f8b\u4e14\u89c6\u89c9\u5438\u5f15\u529b\u7684\u7f16\u8f91\u6837\u672c\u975e\u5e38\u56f0\u96be\uff0c\u73b0\u6709\u81ea\u52a8\u5316\u8bc4\u4f30\u6307\u6807\u4e0d\u8db3\u4ee5\u652f\u6301\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u751f\u4ea7\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u81ea\u52a8\u5316\u7684\u3001\u6a21\u5757\u5316\u7684\u6d41\u7a0b\uff0c\u5229\u7528\u4efb\u52a1\u5fae\u8c03\u7684Gemini\u9a8c\u8bc1\u5668\u6765\u8bc4\u4f30\u6307\u4ee4\u9075\u5faa\u5ea6\u548c\u7f8e\u5b66\uff0c\u65e0\u9700\u5206\u5272\u6216\u5b9a\u4f4d\u6a21\u578b\u3002\u901a\u8fc7\u53cd\u6f14\u548c\u7ec4\u5408\u5f15\u5bfc\u6765\u6269\u5927\u6316\u6398\u6570\u636e\u96c6\u3002", "result": "\u6210\u529f\u6316\u6398\u4e86\u8de8\u9886\u57df\u3001\u8de8\u5206\u8fa8\u7387\u3001\u8de8\u6307\u4ee4\u590d\u6742\u5ea6\u548c\u98ce\u683c\u7684\u9ad8\u4fdd\u771f\u4e09\u5143\u7ec4\uff0c\u5e76\u5c06\u6570\u636e\u96c6\u6269\u5927\u4e86\u7ea62.2\u500d\u3002\u53d1\u5e03\u7684NHR-Edit\u6570\u636e\u96c6\u5728\u6700\u5927\u89c4\u6a21\u7684\u8de8\u6570\u636e\u96c6\u8bc4\u4f30\u4e2d\u4f18\u4e8e\u6240\u6709\u516c\u5f00\u66ff\u4ee3\u54c1\u3002\u53d1\u5e03\u7684Bagel-NHR-Edit\u6a21\u578b\u5728\u5b9e\u9a8c\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6307\u6807\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u6d41\u7a0b\uff0c\u7528\u4e8e\u5927\u89c4\u6a21\u6316\u6398\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\u7f16\u8f91\u8bad\u7ec3\u6570\u636e\uff08\u4e09\u5143\u7ec4\uff1a\u539f\u59cb\u56fe\u50cf\u3001\u6307\u4ee4\u3001\u7f16\u8f91\u540e\u7684\u56fe\u50cf\uff09\uff0c\u89e3\u51b3\u4e86\u624b\u52a8\u6807\u6ce8\u6210\u672c\u9ad8\u3001\u96be\u4ee5\u4fdd\u8bc1\u7f16\u8f91\u8d28\u91cf\u7684\u95ee\u9898\u3002\u8be5\u6d41\u7a0b\u5229\u7528Gemini\u6a21\u578b\u8fdb\u884c\u6253\u5206\uff0c\u65e0\u9700\u5206\u5272\u6216\u5b9a\u4f4d\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u53cd\u6f14\u548c\u7ec4\u5408\u5f15\u5bfc\u5c06\u6570\u636e\u91cf\u6269\u5927\u4e86\u7ea62.2\u500d\u3002\u7814\u7a76\u53d1\u5e03\u4e86NHR-Edit\u6570\u636e\u96c6\uff08358k\u9ad8\u8d28\u91cf\u4e09\u5143\u7ec4\uff09\u548cBagel-NHR-Edit\u6a21\u578b\uff0c\u540e\u8005\u5728\u5b9e\u9a8c\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2507.13797", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13797", "abs": "https://arxiv.org/abs/2507.13797", "authors": ["Huu-Phu Do", "Yu-Wei Chen", "Yi-Cheng Liao", "Chi-Wei Hsiao", "Han-Yang Wang", "Wei-Chen Chiu", "Ching-Chun Huang"], "title": "DynFaceRestore: Balancing Fidelity and Quality in Diffusion-Guided Blind Face Restoration with Dynamic Blur-Level Mapping and Guidance", "comment": "Accepted by ICCV 2025", "summary": "Blind Face Restoration aims to recover high-fidelity, detail-rich facial\nimages from unknown degraded inputs, presenting significant challenges in\npreserving both identity and detail. Pre-trained diffusion models have been\nincreasingly used as image priors to generate fine details. Still, existing\nmethods often use fixed diffusion sampling timesteps and a global guidance\nscale, assuming uniform degradation. This limitation and potentially imperfect\ndegradation kernel estimation frequently lead to under- or over-diffusion,\nresulting in an imbalance between fidelity and quality. We propose\nDynFaceRestore, a novel blind face restoration approach that learns to map any\nblindly degraded input to Gaussian blurry images. By leveraging these blurry\nimages and their respective Gaussian kernels, we dynamically select the\nstarting timesteps for each blurry image and apply closed-form guidance during\nthe diffusion sampling process to maintain fidelity. Additionally, we introduce\na dynamic guidance scaling adjuster that modulates the guidance strength across\nlocal regions, enhancing detail generation in complex areas while preserving\nstructural fidelity in contours. This strategy effectively balances the\ntrade-off between fidelity and quality. DynFaceRestore achieves\nstate-of-the-art performance in both quantitative and qualitative evaluations,\ndemonstrating robustness and effectiveness in blind face restoration.", "AI": {"tldr": "DynFaceRestore\u662f\u4e00\u79cd\u65b0\u9896\u7684\u76f2\u4eba\u8138\u6062\u590d\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u6269\u6563\u91c7\u6837\u8fc7\u7a0b\u6765\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u4fdd\u771f\u5ea6\u548c\u8d28\u91cf\u5e73\u8861\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f7f\u7528\u56fa\u5b9a\u7684\u6269\u6563\u91c7\u6837\u65f6\u95f4\u6b65\u548c\u5168\u5c40\u5f15\u5bfc\u5c3a\u5ea6\uff0c\u5047\u8bbe\u964d\u7ea7\u662f\u7edf\u4e00\u7684\u3002\u8fd9\u79cd\u9650\u5236\u548c\u6f5c\u5728\u4e0d\u5b8c\u7f8e\u7684\u964d\u7ea7\u6838\u4f30\u8ba1\u5e38\u5e38\u5bfc\u81f4\u6269\u6563\u4e0d\u8db3\u6216\u8fc7\u5ea6\u6269\u6563\uff0c\u4ece\u800c\u5728\u4fdd\u771f\u5ea6\u548c\u8d28\u91cf\u4e4b\u95f4\u4ea7\u751f\u4e0d\u5e73\u8861\u3002", "method": "DynFaceRestore\u901a\u8fc7\u5b66\u4e60\u5c06\u4efb\u4f55\u76f2\u9000\u5316\u7684\u8f93\u5165\u6620\u5c04\u5230\u9ad8\u65af\u6a21\u7cca\u56fe\u50cf\u3002\u5b83\u5229\u7528\u8fd9\u4e9b\u6a21\u7cca\u56fe\u50cf\u53ca\u5176\u5404\u81ea\u7684\u9ad8\u65af\u6838\uff0c\u52a8\u6001\u9009\u62e9\u6bcf\u4e2a\u6a21\u7cca\u56fe\u50cf\u7684\u8d77\u59cb\u65f6\u95f4\u6b65\uff0c\u5e76\u5728\u6269\u6563\u91c7\u6837\u8fc7\u7a0b\u4e2d\u5e94\u7528\u95ed\u5f0f\u5f15\u5bfc\u4ee5\u4fdd\u6301\u4fdd\u771f\u5ea6\u3002\u6b64\u5916\uff0c\u5b83\u8fd8\u5f15\u5165\u4e86\u4e00\u4e2a\u52a8\u6001\u5f15\u5bfc\u7f29\u653e\u8c03\u6574\u5668\uff0c\u4ee5\u8de8\u5c40\u90e8\u533a\u57df\u8c03\u8282\u5f15\u5bfc\u5f3a\u5ea6\uff0c\u4ece\u800c\u5728\u589e\u5f3a\u590d\u6742\u533a\u57df\u7ec6\u8282\u751f\u6210\u7684\u540c\u65f6\uff0c\u4fdd\u6301\u8f6e\u5ed3\u7684\u7ed3\u6784\u4fdd\u771f\u5ea6\u3002", "result": "DynFaceRestore\u6709\u6548\u5e73\u8861\u4e86\u4fdd\u771f\u5ea6\u548c\u8d28\u91cf\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u4e2d\u5747\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "DynFaceRestore\u5728\u76f2\u4eba\u8138\u6062\u590d\u65b9\u9762\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u4e2d\u90fd\u5f97\u5230\u4e86\u8bc1\u660e\uff0c\u5e76\u5c55\u793a\u4e86\u9c81\u68d2\u6027\u548c\u6709\u6548\u6027\u3002"}}
{"id": "2507.13801", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13801", "abs": "https://arxiv.org/abs/2507.13801", "authors": ["Haoang Lu", "Yuanqi Su", "Xiaoning Zhang", "Hao Hu"], "title": "One Step Closer: Creating the Future to Boost Monocular Semantic Scene Completion", "comment": null, "summary": "In recent years, visual 3D Semantic Scene Completion (SSC) has emerged as a\ncritical perception task for autonomous driving due to its ability to infer\ncomplete 3D scene layouts and semantics from single 2D images. However, in\nreal-world traffic scenarios, a significant portion of the scene remains\noccluded or outside the camera's field of view -- a fundamental challenge that\nexisting monocular SSC methods fail to address adequately. To overcome these\nlimitations, we propose Creating the Future SSC (CF-SSC), a novel temporal SSC\nframework that leverages pseudo-future frame prediction to expand the model's\neffective perceptual range. Our approach combines poses and depths to establish\naccurate 3D correspondences, enabling geometrically-consistent fusion of past,\npresent, and predicted future frames in 3D space. Unlike conventional methods\nthat rely on simple feature stacking, our 3D-aware architecture achieves more\nrobust scene completion by explicitly modeling spatial-temporal relationships.\nComprehensive experiments on SemanticKITTI and SSCBench-KITTI-360 benchmarks\ndemonstrate state-of-the-art performance, validating the effectiveness of our\napproach, highlighting our method's ability to improve occlusion reasoning and\n3D scene completion accuracy.", "AI": {"tldr": "CF-SSC\u901a\u8fc7\u9884\u6d4b\u672a\u6765\u5e27\u6765\u89e3\u51b3\u5355\u76eeSSC\u4e2d\u7684\u906e\u6321\u95ee\u9898\uff0c\u5e76\u5728\u5173\u952e\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u5355\u76eeSSC\u65b9\u6cd5\u5728\u5904\u7406\u771f\u5b9e\u4ea4\u901a\u573a\u666f\u4e2d\u7684\u906e\u6321\u6216\u76f8\u673a\u89c6\u91ce\u5916\u7684\u5927\u90e8\u5206\u573a\u666f\u65f6\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCF-SSC\u7684\u65b0\u578b\u65f6\u95f4SSC\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u4f2a\u672a\u6765\u5e27\u9884\u6d4b\u6765\u6269\u5c55\u6a21\u578b\u7684\u6709\u6548\u611f\u77e5\u8303\u56f4\uff0c\u5e76\u901a\u8fc7\u7ed3\u5408\u59ff\u6001\u548c\u6df1\u5ea6\u6765\u5efa\u7acb\u51c6\u786e\u76843D\u5bf9\u5e94\u5173\u7cfb\uff0c\u4ee5\u5b9e\u73b0\u8fc7\u53bb\u3001\u73b0\u5728\u548c\u9884\u6d4b\u7684\u672a\u6765\u5e27\u57283D\u7a7a\u95f4\u4e2d\u7684\u51e0\u4f55\u4e00\u81f4\u6027\u878d\u5408\u3002\u8be5\u6a21\u578b\u91c7\u7528\u4e863D\u611f\u77e5\u67b6\u6784\uff0c\u5e76\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u65f6\u7a7a\u5173\u7cfb\u6765\u63d0\u9ad8\u573a\u666f\u8865\u5168\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5728SemanticKITTI\u548cSSCBench-KITTI-360\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u7a81\u51fa\u4e86\u5176\u5728\u6539\u5584\u906e\u6321\u63a8\u7406\u548c3D\u573a\u666f\u8865\u5168\u51c6\u786e\u6027\u65b9\u9762\u7684\u80fd\u529b\u3002", "conclusion": "CF-SSC\u901a\u8fc7\u878d\u5408\u8fc7\u53bb\u3001\u73b0\u5728\u548c\u9884\u6d4b\u7684\u672a\u6765\u5e27\uff0c\u57283D\u7a7a\u95f4\u4e2d\u5b9e\u73b0\u4e86\u51e0\u4f55\u4e00\u81f4\u7684\u573a\u666f\u8865\u5168\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5355\u76eeSSC\u65b9\u6cd5\u5728\u5904\u7406\u906e\u6321\u548c\u89c6\u91ce\u5916\u573a\u666f\u65b9\u9762\u7684\u4e0d\u8db3\u3002"}}
{"id": "2507.13803", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13803", "abs": "https://arxiv.org/abs/2507.13803", "authors": ["Weiqi Yang", "Xu Zhou", "Jingfu Guan", "Hao Du", "Tianyu Bai"], "title": "GRAM-MAMBA: Holistic Feature Alignment for Wireless Perception with Adaptive Low-Rank Compensation", "comment": null, "summary": "Multi-modal fusion is crucial for Internet of Things (IoT) perception, widely\ndeployed in smart homes, intelligent transport, industrial automation, and\nhealthcare. However, existing systems often face challenges: high model\ncomplexity hinders deployment in resource-constrained environments,\nunidirectional modal alignment neglects inter-modal relationships, and\nrobustness suffers when sensor data is missing. These issues impede efficient\nand robust multimodal perception in real-world IoT settings. To overcome these\nlimitations, we propose GRAM-MAMBA. This framework utilizes the\nlinear-complexity Mamba model for efficient sensor time-series processing,\ncombined with an optimized GRAM matrix strategy for pairwise alignment among\nmodalities, addressing the shortcomings of traditional single-modality\nalignment. Inspired by Low-Rank Adaptation (LoRA), we introduce an adaptive\nlow-rank layer compensation strategy to handle missing modalities\npost-training. This strategy freezes the pre-trained model core and irrelevant\nadaptive layers, fine-tuning only those related to available modalities and the\nfusion process. Extensive experiments validate GRAM-MAMBA's effectiveness. On\nthe SPAWC2021 indoor positioning dataset, the pre-trained model shows lower\nerror than baselines; adapting to missing modalities yields a 24.5% performance\nboost by training less than 0.2% of parameters. On the USC-HAD human activity\nrecognition dataset, it achieves 93.55% F1 and 93.81% Overall Accuracy (OA),\noutperforming prior work; the update strategy increases F1 by 23% while\ntraining less than 0.3% of parameters. These results highlight GRAM-MAMBA's\npotential for achieving efficient and robust multimodal perception in\nresource-constrained environments.", "AI": {"tldr": "GRAM-MAMBA\uff1a\u4e00\u79cd\u9ad8\u6548\u3001\u9c81\u68d2\u7684\u7269\u8054\u7f51\u591a\u6a21\u6001\u611f\u77e5\u6846\u67b6\uff0c\u901a\u8fc7Mamba\u548cGRAM\u89e3\u51b3\u590d\u6742\u6027\u3001\u5bf9\u9f50\u548c\u7f3a\u5931\u6a21\u6001\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7269\u8054\u7f51\u591a\u6a21\u6001\u611f\u77e5\u7cfb\u7edf\u9762\u4e34\u6a21\u578b\u590d\u6742\u5ea6\u9ad8\u3001\u5355\u5411\u6a21\u6001\u5bf9\u9f50\u5ffd\u89c6\u8de8\u6a21\u6001\u5173\u7cfb\u4ee5\u53ca\u4f20\u611f\u5668\u6570\u636e\u7f3a\u5931\u65f6\u9c81\u68d2\u6027\u5dee\u7b49\u95ee\u9898\uff0c\u963b\u788d\u4e86\u5176\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u9ad8\u6548\u90e8\u7f72\u548c\u9c81\u68d2\u8fd0\u884c\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51faGRAM-MAMBA\u6846\u67b6\uff0c\u91c7\u7528\u7ebf\u6027\u590d\u6742\u5ea6\u7684Mamba\u6a21\u578b\u5904\u7406\u4f20\u611f\u5668\u65f6\u95f4\u5e8f\u5217\uff0c\u7ed3\u5408\u4f18\u5316\u7684GRAM\u77e9\u9635\u7b56\u7565\u8fdb\u884c\u591a\u6a21\u6001\u95f4\u7684\u6210\u5bf9\u6821\u51c6\uff0c\u5e76\u5f15\u5165\u53d7LoRA\u542f\u53d1\u7684\u81ea\u9002\u5e94\u4f4e\u79e9\u5c42\u8865\u507f\u7b56\u7565\u6765\u5904\u7406\u8bad\u7ec3\u540e\u7684\u4f20\u611f\u5668\u6570\u636e\u7f3a\u5931\u95ee\u9898\u3002", "result": "\u5728SPAWC2021\u5ba4\u5185\u5b9a\u4f4d\u6570\u636e\u96c6\u4e0a\uff0cGRAM-MAMBA\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff1b\u5728\u5904\u7406\u7f3a\u5931\u6a21\u6001\u65f6\uff0c\u901a\u8fc7\u8bad\u7ec3\u4e0d\u52300.2%\u7684\u53c2\u6570\uff0c\u6027\u80fd\u63d0\u5347\u4e8624.5%\u3002\u5728USC-HAD\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u6a21\u578b\u8fbe\u5230\u4e8693.55%\u7684F1\u5206\u6570\u548c93.81%\u7684\u603b\u4f53\u51c6\u786e\u7387\uff08OA\uff09\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff1b\u66f4\u65b0\u7b56\u7565\u5728\u8bad\u7ec3\u4e0d\u52300.3%\u7684\u53c2\u6570\u60c5\u51b5\u4e0b\uff0cF1\u5206\u6570\u63d0\u9ad8\u4e8623%\u3002", "conclusion": "GRAM-MAMBA\u6846\u67b6\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u591a\u6a21\u6001\u611f\u77e5\uff0c\u901a\u8fc7\u7ebf\u6027\u590d\u6742\u5ea6\u7684Mamba\u6a21\u578b\u3001\u4f18\u5316\u7684GRAM\u77e9\u9635\u7b56\u7565\u548c\u81ea\u9002\u5e94\u4f4e\u79e9\u5c42\u8865\u507f\u7b56\u7565\u89e3\u51b3\u4e86\u73b0\u6709\u7cfb\u7edf\u7684\u6311\u6218\u3002"}}
{"id": "2507.13812", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13812", "abs": "https://arxiv.org/abs/2507.13812", "authors": ["Yingying Zhang", "Lixiang Ru", "Kang Wu", "Lei Yu", "Lei Liang", "Yansheng Li", "Jingdong Chen"], "title": "SkySense V2: A Unified Foundation Model for Multi-modal Remote Sensing", "comment": "Accepted by ICCV25", "summary": "The multi-modal remote sensing foundation model (MM-RSFM) has significantly\nadvanced various Earth observation tasks, such as urban planning, environmental\nmonitoring, and natural disaster management. However, most existing approaches\ngenerally require the training of separate backbone networks for each data\nmodality, leading to redundancy and inefficient parameter utilization.\nMoreover, prevalent pre-training methods typically apply self-supervised\nlearning (SSL) techniques from natural images without adequately accommodating\nthe characteristics of remote sensing (RS) images, such as the complicated\nsemantic distribution within a single RS image. In this work, we present\nSkySense V2, a unified MM-RSFM that employs a single transformer backbone to\nhandle multiple modalities. This backbone is pre-trained with a novel SSL\nstrategy tailored to the distinct traits of RS data. In particular, SkySense V2\nincorporates an innovative adaptive patch merging module and learnable modality\nprompt tokens to address challenges related to varying resolutions and limited\nfeature diversity across modalities. In additional, we incorporate the mixture\nof experts (MoE) module to further enhance the performance of the foundation\nmodel. SkySense V2 demonstrates impressive generalization abilities through an\nextensive evaluation involving 16 datasets over 7 tasks, outperforming SkySense\nby an average of 1.8 points.", "AI": {"tldr": "SkySense V2 \u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u6a21\u6001\u9065\u611f\u57fa\u7840\u6a21\u578b\uff0c\u4f7f\u7528\u5355\u4e00\u7684 Transformer \u4e3b\u5e72\u548c\u9488\u5bf9\u9065\u611f\u6570\u636e\u7684\u65b0\u578b\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5197\u4f59\u548c\u6548\u7387\u95ee\u9898\uff0c\u5e76\u5728\u591a\u9879\u4efb\u52a1\u548c\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u591a\u6570\u9065\u611f\u57fa\u7840\u6a21\u578b\uff08MM-RSFM\uff09\u5728\u5904\u7406\u4e0d\u540c\u6570\u636e\u6a21\u6001\u65f6\u9700\u8981\u8bad\u7ec3\u72ec\u7acb\u7684\u9aa8\u5e72\u7f51\u7edc\uff0c\u8fd9\u5bfc\u81f4\u4e86\u53c2\u6570\u5197\u4f59\u548c\u5229\u7528\u6548\u7387\u4f4e\u4e0b\u3002\u6b64\u5916\uff0c\u73b0\u6709\u7684\u9884\u8bad\u7ec3\u65b9\u6cd5\u901a\u5e38\u76f4\u63a5\u5957\u7528\u81ea\u7136\u56fe\u50cf\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u6280\u672f\uff0c\u672a\u80fd\u5145\u5206\u8003\u8651\u9065\u611f\u56fe\u50cf\u7684\u72ec\u7279\u6027\u8d28\uff0c\u4f8b\u5982\u5355\u4e00\u9065\u611f\u56fe\u50cf\u5185\u90e8\u590d\u6742\u7684\u8bed\u4e49\u5206\u5e03\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86 SkySense V2\uff0c\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u6a21\u6001\u9065\u611f\u57fa\u7840\u6a21\u578b\uff08MM-RSFM\uff09\uff0c\u91c7\u7528\u5355\u4e00\u7684 transformer \u4e3b\u5e72\u6765\u5904\u7406\u591a\u79cd\u6570\u636e\u6a21\u6001\u3002\u8be5\u6a21\u578b\u901a\u8fc7\u4e00\u79cd\u65b0\u9896\u7684\u81ea\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\u7b56\u7565\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u8be5\u7b56\u7565\u4e13\u95e8\u9488\u5bf9\u9065\u611f\uff08RS\uff09\u6570\u636e\u7684\u7279\u6027\u8fdb\u884c\u4e86\u4f18\u5316\uff0c\u89e3\u51b3\u4e86\u5355\u4e00 RS \u56fe\u50cf\u4e2d\u590d\u6742\u7684\u8bed\u4e49\u5206\u5e03\u95ee\u9898\u3002\u6b64\u5916\uff0cSkySense V2 \u96c6\u6210\u4e86\u521b\u65b0\u7684\u81ea\u9002\u5e94\u56fe\u50cf\u5757\u5408\u5e76\u6a21\u5757\u548c\u53ef\u5b66\u4e60\u7684\u6a21\u6001\u63d0\u793a\u4ee4\u724c\uff0c\u4ee5\u5e94\u5bf9\u4e0d\u540c\u5206\u8fa8\u7387\u548c\u6709\u9650\u7279\u5f81\u591a\u6837\u6027\u5e26\u6765\u7684\u6311\u6218\u3002\u540c\u65f6\uff0c\u5f15\u5165\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u6a21\u5757\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u6a21\u578b\u7684\u6027\u80fd\u3002", "result": "SkySense V2 \u5728\u6d89\u53ca 16 \u4e2a\u6570\u636e\u96c6\u548c 7 \u4e2a\u4efb\u52a1\u7684\u5e7f\u6cdb\u8bc4\u4f30\u4e2d\uff0c\u5c55\u793a\u4e86\u51fa\u8272\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5176\u5e73\u5747\u6027\u80fd\u6bd4 SkySense \u63d0\u9ad8\u4e86 1.8 \u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "SkySense V2\uff0c\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u6a21\u6001\u9065\u611f\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u5355\u4e00\u7684 transformer \u4e3b\u5e72\u548c\u65b0\u9896\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u7b56\u7565\uff0c\u6709\u6548\u5730\u5904\u7406\u4e86\u591a\u6a21\u6001\u9065\u611f\u6570\u636e\uff0c\u5e76\u53d6\u5f97\u4e86\u4f18\u4e8e SkySense \u7684\u6027\u80fd\uff0c\u5728 16 \u4e2a\u6570\u636e\u96c6\u7684 7 \u9879\u4efb\u52a1\u4e0a\u5e73\u5747\u63d0\u9ad8\u4e86 1.8 \u4e2a\u70b9\u3002"}}
{"id": "2507.13820", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13820", "abs": "https://arxiv.org/abs/2507.13820", "authors": ["Jun Xie", "Zhaoran Zhao", "Xiongjun Guan", "Yingjian Zhu", "Hongzhu Yi", "Xinming Wang", "Feng Chen", "Zhepeng Wang"], "title": "Team of One: Cracking Complex Video QA with Model Synergy", "comment": null, "summary": "We propose a novel framework for open-ended video question answering that\nenhances reasoning depth and robustness in complex real-world scenarios, as\nbenchmarked on the CVRR-ES dataset. Existing Video-Large Multimodal Models\n(Video-LMMs) often exhibit limited contextual understanding, weak temporal\nmodeling, and poor generalization to ambiguous or compositional queries. To\naddress these challenges, we introduce a prompting-and-response integration\nmechanism that coordinates multiple heterogeneous Video-Language Models (VLMs)\nvia structured chains of thought, each tailored to distinct reasoning pathways.\nAn external Large Language Model (LLM) serves as an evaluator and integrator,\nselecting and fusing the most reliable responses. Extensive experiments\ndemonstrate that our method significantly outperforms existing baselines across\nall evaluation metrics, showcasing superior generalization and robustness. Our\napproach offers a lightweight, extensible strategy for advancing multimodal\nreasoning without requiring model retraining, setting a strong foundation for\nfuture Video-LMM development.", "AI": {"tldr": "A new framework for video question answering uses multiple Video-LMMs coordinated by a structured chain of thought and integrated by an LLM, outperforming existing methods in generalization and robustness without retraining.", "motivation": "Existing Video-Large Multimodal Models (Video-LMMs) often exhibit limited contextual understanding, weak temporal modeling, and poor generalization to ambiguous or compositional queries.", "method": "We introduce a prompting-and-response integration mechanism that coordinates multiple heterogeneous Video-Language Models (VLMs) via structured chains of thought. An external Large Language Model (LLM) serves as an evaluator and integrator, selecting and fusing the most reliable responses.", "result": "Extensive experiments demonstrate that our method significantly outperforms existing baselines across all evaluation metrics, showcasing superior generalization and robustness.", "conclusion": "Our proposed framework significantly outperforms existing baselines across all evaluation metrics, demonstrating superior generalization and robustness in open-ended video question answering. It offers a lightweight and extensible strategy for advancing multimodal reasoning without requiring model retraining."}}
{"id": "2507.13852", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.13852", "abs": "https://arxiv.org/abs/2507.13852", "authors": ["Luigi Russo", "Francesco Mauro", "Babak Memar", "Alessandro Sebastianelli", "Silvia Liberata Ullo", "Paolo Gamba"], "title": "A Quantum-assisted Attention U-Net for Building Segmentation over Tunis using Sentinel-1 Data", "comment": "Accepted at IEEE Joint Urban Remote Sensing Event (JURSE) 2025", "summary": "Building segmentation in urban areas is essential in fields such as urban\nplanning, disaster response, and population mapping. Yet accurately segmenting\nbuildings in dense urban regions presents challenges due to the large size and\nhigh resolution of satellite images. This study investigates the use of a\nQuanvolutional pre-processing to enhance the capability of the Attention U-Net\nmodel in the building segmentation. Specifically, this paper focuses on the\nurban landscape of Tunis, utilizing Sentinel-1 Synthetic Aperture Radar (SAR)\nimagery. In this work, Quanvolution was used to extract more informative\nfeature maps that capture essential structural details in radar imagery,\nproving beneficial for accurate building segmentation. Preliminary results\nindicate that proposed methodology achieves comparable test accuracy to the\nstandard Attention U-Net model while significantly reducing network parameters.\nThis result aligns with findings from previous works, confirming that\nQuanvolution not only maintains model accuracy but also increases computational\nefficiency. These promising outcomes highlight the potential of\nquantum-assisted Deep Learning frameworks for large-scale building segmentation\nin urban environments.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u7a2e\u7d50\u5408Quanvolution\u9810\u8655\u7406\u8207Attention U-Net\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u7528\u65bc\u57ce\u5e02\u5efa\u7bc9\u5206\u5272\u3002\u8a72\u65b9\u6cd5\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u6642\uff0c\u6e1b\u5c11\u4e86\u6a21\u578b\u53c3\u6578\uff0c\u63d0\u9ad8\u4e86\u8a08\u7b97\u6548\u7387\u3002", "motivation": "\u57ce\u5e02\u5340\u57df\u7684\u5efa\u7bc9\u5206\u5272\u5728\u57ce\u5e02\u898f\u5283\u3001\u707d\u5bb3\u97ff\u61c9\u548c\u4eba\u53e3\u6e2c\u7e6a\u7b49\u9818\u57df\u81f3\u95dc\u91cd\u8981\uff0c\u7136\u800c\uff0c\u7531\u65bc\u885b\u661f\u5f71\u50cf\u5c3a\u5bf8\u5927\u3001\u89e3\u6790\u5ea6\u9ad8\uff0c\u5728\u5bc6\u96c6\u7684\u57ce\u5e02\u5340\u57df\u6e96\u78ba\u5206\u5272\u5efa\u7bc9\u7269\u5b58\u5728\u6311\u6230\u3002", "method": "\u672c\u7814\u7a76\u5229\u7528Quanvolutional\u9810\u8655\u7406\u6280\u8853\u589e\u5f37Attention U-Net\u6a21\u578b\u5728\u5efa\u7bc9\u5206\u5272\u4e2d\u7684\u80fd\u529b\uff0c\u4e26\u5728\u7a81\u5c3c\u65af\u57ce\u5e02\u74b0\u5883\u4e2d\uff0c\u4f7f\u7528Sentinel-1\u5408\u6210\u5b54\u5f91\u96f7\u9054\uff08SAR\uff09\u5f71\u50cf\u9032\u884c\u5be6\u9a57\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u8207\u6a19\u6e96Attention U-Net\u6a21\u578b\u76f8\u6bd4\uff0c\u9054\u5230\u4e86\u76f8\u7576\u7684\u6e2c\u8a66\u6e96\u78ba\u5ea6\uff0c\u540c\u6642\u986f\u8457\u6e1b\u5c11\u4e86\u7db2\u8def\u53c3\u6578\uff0c\u63d0\u9ad8\u4e86\u8a08\u7b97\u6548\u7387\u3002", "conclusion": "\u63d0\u51fa\u65b9\u6cd5\u5728\u4fdd\u6301\u6a21\u578b\u7cbe\u5ea6\u7684\u540c\u6642\uff0c\u986f\u8457\u6e1b\u5c11\u4e86\u7db2\u8def\u53c3\u6578\uff0c\u63d0\u9ad8\u4e86\u8a08\u7b97\u6548\u7387\u3002\u91cf\u5b50\u555f\u767c\u7684\u6df1\u5ea6\u5b78\u7fd2\u6846\u67b6\u5728\u57ce\u5e02\u74b0\u5883\u7684\u5927\u898f\u6a21\u5efa\u7bc9\u5206\u5272\u65b9\u9762\u5177\u6709\u6f5b\u529b\u3002"}}
{"id": "2507.13861", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13861", "abs": "https://arxiv.org/abs/2507.13861", "authors": ["Junjie Hu", "Tianyang Han", "Kai Ma", "Jialin Gao", "Hao Dou", "Song Yang", "Xianhua He", "Jianhui Zhang", "Junfeng Luo", "Xiaoming Wei", "Wenqiang Zhang"], "title": "PositionIC: Unified Position and Identity Consistency for Image Customization", "comment": null, "summary": "Recent subject-driven image customization has achieved significant\nadvancements in fidelity, yet fine-grained entity-level spatial control remains\nelusive, hindering the broader real-world application. This limitation is\nmainly attributed to scalable datasets that bind identity with precise\npositional cues are absent. To this end, we introduce PositionIC, a unified\nframework that enforces position and identity consistency for multi-subject\ncustomization. We construct a scalable synthesis pipeline that employs a\nbidirectional generation paradigm to eliminate subject drift and maintain\nsemantic coherence. On top of these data, we design a lightweight positional\nmodulation layer that decouples spatial embeddings among subjects, enabling\nindependent, accurate placement while preserving visual fidelity. Extensive\nexperiments demonstrate that our approach can achieve precise spatial control\nwhile maintaining high consistency in image customization task. PositionIC\npaves the way for controllable, high-fidelity image customization in\nopen-world, multi-entity scenarios and will be released to foster further\nresearch.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa PositionIC \u6846\u67b6\uff0c\u901a\u8fc7\u65b0\u7684\u5408\u6210\u7ba1\u7ebf\u548c\u4f4d\u7f6e\u8c03\u5236\u5c42\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u56fe\u50cf\u5b9a\u5236\u6280\u672f\u5728\u7cbe\u7ec6\u7a7a\u95f4\u63a7\u5236\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u5b9e\u73b0\u4e86\u591a\u4e3b\u4f53\u3001\u9ad8\u4fdd\u771f\u5ea6\u7684\u56fe\u50cf\u5b9a\u5236\u3002", "motivation": "\u73b0\u6709\u7684\u56fe\u50cf\u5b9a\u5236\u6280\u672f\u5728\u4fdd\u771f\u5ea6\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5728\u5b9e\u4f53\u7ea7\u522b\u7684\u7cbe\u7ec6\u7a7a\u95f4\u63a7\u5236\u65b9\u9762\u4ecd\u6709\u4e0d\u8db3\uff0c\u8fd9\u4e3b\u8981\u662f\u7531\u4e8e\u7f3a\u4e4f\u5c06\u8eab\u4efd\u4e0e\u7cbe\u786e\u4f4d\u7f6e\u7ebf\u7d22\u7ed1\u5b9a\u7684\u53ef\u6269\u5c55\u6570\u636e\u96c6\u3002", "method": "\u672c\u6587\u63d0\u51fa PositionIC \u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u5305\u542b\u53cc\u5411\u751f\u6210\u8303\u5f0f\u7684\u53ef\u6269\u5c55\u5408\u6210\u7ba1\u7ebf\u6765\u6d88\u9664\u4e3b\u4f53\u6f02\u79fb\u5e76\u4fdd\u6301\u8bed\u4e49\u4e00\u81f4\u6027\uff0c\u5e76\u8bbe\u8ba1\u8f7b\u91cf\u7ea7\u7684\u4f4d\u7f6e\u8c03\u5236\u5c42\u6765\u89e3\u8026\u4e3b\u4f53\u95f4\u7684\u7a7a\u95f4\u5d4c\u5165\uff0c\u5b9e\u73b0\u72ec\u7acb\u3001\u7cbe\u786e\u7684\u653e\u7f6e\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cPositionIC \u5728\u56fe\u50cf\u5b9a\u5236\u4efb\u52a1\u4e2d\u80fd\u591f\u5b9e\u73b0\u7cbe\u786e\u7684\u7a7a\u95f4\u63a7\u5236\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u5ea6\u7684\u4e00\u81f4\u6027\u3002", "conclusion": "PositionIC \u6846\u67b6\u901a\u8fc7\u89e3\u8026\u7a7a\u95f4\u5d4c\u5165\uff0c\u5b9e\u73b0\u4e86\u7cbe\u786e\u7684\u7a7a\u95f4\u63a7\u5236\u548c\u9ad8\u4e00\u81f4\u6027\uff0c\u4e3a\u5f00\u653e\u4e16\u754c\u3001\u591a\u5b9e\u4f53\u573a\u666f\u4e0b\u7684\u53ef\u63a7\u3001\u9ad8\u4fdd\u771f\u56fe\u50cf\u5b9a\u5236\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2507.13868", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13868", "abs": "https://arxiv.org/abs/2507.13868", "authors": ["Francesco Ortu", "Zhijing Jin", "Diego Doimo", "Alberto Cazzaniga"], "title": "When Seeing Overrides Knowing: Disentangling Knowledge Conflicts in Vision-Language Models", "comment": null, "summary": "Vision-language models (VLMs) increasingly leverage diverse knowledge sources\nto address complex tasks, often encountering conflicts between their internal\nparametric knowledge and external information. Knowledge conflicts can result\nin hallucinations and unreliable responses, but the mechanisms governing such\ninteractions remain unknown. To address this gap, we analyze the mechanisms\nthat VLMs use to resolve cross-modal conflicts by introducing a dataset of\nmultimodal counterfactual queries that deliberately contradict internal\ncommonsense knowledge. We localize with logit inspection a small set of heads\nthat control the conflict. Moreover, by modifying these heads, we can steer the\nmodel towards its internal knowledge or the visual inputs. Finally, we show\nthat attention from such heads pinpoints localized image regions driving visual\noverrides, outperforming gradient-based attribution in precision.", "AI": {"tldr": "\u7814\u7a76\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5982\u4f55\u5904\u7406\u5185\u90e8\u77e5\u8bc6\u4e0e\u5916\u90e8\u4fe1\u606f\u4e4b\u95f4\u7684\u51b2\u7a81\uff0c\u53d1\u73b0\u5c11\u6570\u6ce8\u610f\u529b\u5934\u5728\u5176\u4e2d\u8d77\u5173\u952e\u4f5c\u7528\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u4fee\u6539\u8fd9\u4e9b\u5934\u6765\u63a7\u5236\u6a21\u578b\u884c\u4e3a\u5e76\u7cbe\u786e\u5b9a\u4f4d\u89c6\u89c9\u7ebf\u7d22\u7684\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5728\u5229\u7528\u591a\u6837\u5316\u77e5\u8bc6\u6765\u6e90\u65f6\uff0c\u5185\u90e8\u53c2\u6570\u77e5\u8bc6\u4e0e\u5916\u90e8\u4fe1\u606f\u4e4b\u95f4\u5b58\u5728\u7684\u51b2\u7a81\u95ee\u9898\uff0c\u8fd9\u7c7b\u51b2\u7a81\u4f1a\u5bfc\u81f4\u6a21\u578b\u4ea7\u751f\u5e7b\u89c9\u548c\u4e0d\u53ef\u9760\u7684\u54cd\u5e94\uff0c\u800c\u76ee\u524d\u5bf9\u4e8e\u8fd9\u7c7b\u51b2\u7a81\u7684\u89e3\u51b3\u673a\u5236\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u5305\u542b\u6545\u610f\u4e0e\u5185\u90e8\u5e38\u8bc6\u77e5\u8bc6\u76f8\u77db\u76fe\u7684\u591a\u6a21\u6001\u53cd\u4e8b\u5b9e\u67e5\u8be2\u7684\u6570\u636e\u96c6\uff0c\u5e76\u5229\u7528logit\u68c0\u67e5\u6765\u5b9a\u4f4d\u63a7\u5236\u51b2\u7a81\u7684\u5c11\u6570\u51e0\u4e2a\u6ce8\u610f\u529b\u5934\uff0c\u4ece\u800c\u5206\u6790\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u89e3\u51b3\u8de8\u6a21\u6001\u51b2\u7a81\u7684\u673a\u5236\u3002", "result": "\u53d1\u73b0\u4e86\u5c11\u6570\u51e0\u4e2a\u6ce8\u610f\u529b\u5934\u80fd\u591f\u63a7\u5236\u51b2\u7a81\uff0c\u5e76\u4e14\u901a\u8fc7\u4fee\u6539\u8fd9\u4e9b\u6ce8\u610f\u529b\u5934\uff0c\u53ef\u4ee5\u5f15\u5bfc\u6a21\u578b\u503e\u5411\u4e8e\u4f9d\u8d56\u5185\u90e8\u77e5\u8bc6\u6216\u89c6\u89c9\u8f93\u5165\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8868\u660e\u8fd9\u4e9b\u6ce8\u610f\u529b\u5934\u7684\u6ce8\u610f\u529b\u673a\u5236\u80fd\u591f\u7cbe\u786e\u5730\u5b9a\u4f4d\u5230\u5bfc\u81f4\u89c6\u89c9\u8986\u76d6\u7684\u56fe\u50cf\u533a\u57df\uff0c\u5176\u7cbe\u786e\u5ea6\u4f18\u4e8e\u57fa\u4e8e\u68af\u5ea6\u7684\u5f52\u56e0\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u4fee\u6539\u63a7\u5236\u51b2\u7a81\u7684\u5173\u952e\u8fde\u63a5\uff0c\u53ef\u4ee5\u5f15\u5bfc\u6a21\u578b\u4f9d\u8d56\u5176\u5185\u90e8\u77e5\u8bc6\u6216\u89c6\u89c9\u8f93\u5165\uff0c\u5e76\u4e14\u8fd9\u79cd\u65b9\u6cd5\u5728\u7cbe\u786e\u5b9a\u4f4d\u9a71\u52a8\u89c6\u89c9\u8986\u76d6\u7684\u56fe\u50cf\u533a\u57df\u65b9\u9762\u4f18\u4e8e\u57fa\u4e8e\u68af\u5ea6\u7684\u5f52\u56e0\u65b9\u6cd5\u3002"}}
{"id": "2507.13880", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13880", "abs": "https://arxiv.org/abs/2507.13880", "authors": ["Marten Kreis", "Benjamin Kiefer"], "title": "Real-Time Fusion of Visual and Chart Data for Enhanced Maritime Vision", "comment": null, "summary": "This paper presents a novel approach to enhancing marine vision by fusing\nreal-time visual data with chart information. Our system overlays nautical\nchart data onto live video feeds by accurately matching detected navigational\naids, such as buoys, with their corresponding representations in chart data. To\nachieve robust association, we introduce a transformer-based end-to-end neural\nnetwork that predicts bounding boxes and confidence scores for buoy queries,\nenabling the direct matching of image-domain detections with world-space chart\nmarkers. The proposed method is compared against baseline approaches, including\na ray-casting model that estimates buoy positions via camera projection and a\nYOLOv7-based network extended with a distance estimation module. Experimental\nresults on a dataset of real-world maritime scenes demonstrate that our\napproach significantly improves object localization and association accuracy in\ndynamic and challenging environments.", "AI": {"tldr": "A new system fuses real-time video with nautical charts using a novel neural network to accurately match buoys, improving maritime navigation safety.", "motivation": "To enhance marine vision by fusing real-time visual data with chart information for more accurate navigation.", "method": "A transformer-based end-to-end neural network is introduced to predict bounding boxes and confidence scores for buoy queries, enabling direct matching of image-domain detections with world-space chart markers. This approach fuses real-time visual data with chart information by accurately matching detected navigational aids with their chart representations.", "result": "Experimental results on real-world maritime scenes demonstrate significant improvements in object localization and association accuracy compared to baseline approaches like ray-casting and a YOLOv7-based network with distance estimation.", "conclusion": "The proposed transformer-based end-to-end neural network significantly improves object localization and association accuracy in dynamic and challenging maritime environments by fusing real-time visual data with chart information."}}
{"id": "2507.13891", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13891", "abs": "https://arxiv.org/abs/2507.13891", "authors": ["Yu Wei", "Jiahui Zhang", "Xiaoqin Zhang", "Ling Shao", "Shijian Lu"], "title": "PCR-GS: COLMAP-Free 3D Gaussian Splatting via Pose Co-Regularizations", "comment": "Accepted by ICCV2025", "summary": "COLMAP-free 3D Gaussian Splatting (3D-GS) has recently attracted increasing\nattention due to its remarkable performance in reconstructing high-quality 3D\nscenes from unposed images or videos. However, it often struggles to handle\nscenes with complex camera trajectories as featured by drastic rotation and\ntranslation across adjacent camera views, leading to degraded estimation of\ncamera poses and further local minima in joint optimization of camera poses and\n3D-GS. We propose PCR-GS, an innovative COLMAP-free 3DGS technique that\nachieves superior 3D scene modeling and camera pose estimation via camera pose\nco-regularization. PCR-GS achieves regularization from two perspectives. The\nfirst is feature reprojection regularization which extracts view-robust DINO\nfeatures from adjacent camera views and aligns their semantic information for\ncamera pose regularization. The second is wavelet-based frequency\nregularization which exploits discrepancy in high-frequency details to further\noptimize the rotation matrix in camera poses. Extensive experiments over\nmultiple real-world scenes show that the proposed PCR-GS achieves superior\npose-free 3D-GS scene modeling under dramatic changes of camera trajectories.", "AI": {"tldr": "PCR-GS \u662f\u4e00\u79cd\u521b\u65b0\u7684 COLMAP-free 3DGS \u6280\u672f\uff0c\u901a\u8fc7\u76f8\u673a\u59ff\u6001\u7684\u5171\u540c\u6b63\u5219\u5316\u6765\u5904\u7406\u5177\u6709\u590d\u6742\u76f8\u673a\u8f68\u8ff9\u7684\u573a\u666f\u3002", "motivation": "COLMAP-free 3D GS \u5728\u5904\u7406\u5177\u6709\u590d\u6742\u76f8\u673a\u8f68\u8ff9\uff08\u76f8\u90bb\u76f8\u673a\u89c6\u56fe\u4e4b\u95f4\u5177\u6709\u6025\u5267\u7684\u65cb\u8f6c\u548c\u5e73\u79fb\uff09\u7684\u573a\u666f\u65f6\uff0c\u901a\u5e38\u96be\u4ee5\u83b7\u5f97\u826f\u597d\u7684\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\uff0c\u5e76\u5bfc\u81f4\u8054\u5408\u4f18\u5316\u4e2d\u7684\u5c40\u90e8\u6700\u5c0f\u503c\u3002", "method": "PCR-GS \u901a\u8fc7\u4e24\u79cd\u65b9\u5f0f\u5b9e\u73b0\u6b63\u5219\u5316\uff1a1. \u7279\u5f81\u91cd\u6295\u5f71\u6b63\u5219\u5316\uff1a\u63d0\u53d6\u8de8\u76f8\u90bb\u76f8\u673a\u89c6\u56fe\u7684\u89c6\u56fe\u9c81\u68d2 DINO \u7279\u5f81\uff0c\u5e76\u5bf9\u9f50\u5b83\u4eec\u7684\u8bed\u4e49\u4fe1\u606f\u4ee5\u8fdb\u884c\u76f8\u673a\u59ff\u6001\u6b63\u5219\u5316\u3002 2. \u57fa\u4e8e\u5c0f\u6ce2\u7684\u9891\u7387\u6b63\u5219\u5316\uff1a\u5229\u7528\u9ad8\u9891\u7ec6\u8282\u7684\u5dee\u5f02\u6765\u8fdb\u4e00\u6b65\u4f18\u5316\u76f8\u673a\u59ff\u6001\u4e2d\u7684\u65cb\u8f6c\u77e9\u9635\u3002", "result": "PCR-GS \u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684 3D \u573a\u666f\u5efa\u6a21\u548c\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u3002", "conclusion": "PCR-GS \u5728\u76f8\u673a\u8f68\u8ff9\u5927\u5e45\u53d8\u5316\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u65e0\u76f8\u673a\u59ff\u6001 3DGS \u573a\u666f\u5efa\u6a21\u3002"}}
{"id": "2507.13899", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13899", "abs": "https://arxiv.org/abs/2507.13899", "authors": ["Yujian Mo", "Yan Wu", "Junqiao Zhao", "Jijun Wang", "Yinghao Hu", "Jun Yan"], "title": "Enhancing LiDAR Point Features with Foundation Model Priors for 3D Object Detection", "comment": null, "summary": "Recent advances in foundation models have opened up new possibilities for\nenhancing 3D perception. In particular, DepthAnything offers dense and reliable\ngeometric priors from monocular RGB images, which can complement sparse LiDAR\ndata in autonomous driving scenarios. However, such priors remain underutilized\nin LiDAR-based 3D object detection. In this paper, we address the limited\nexpressiveness of raw LiDAR point features, especially the weak discriminative\ncapability of the reflectance attribute, by introducing depth priors predicted\nby DepthAnything. These priors are fused with the original LiDAR attributes to\nenrich each point's representation. To leverage the enhanced point features, we\npropose a point-wise feature extraction module. Then, a Dual-Path RoI feature\nextraction framework is employed, comprising a voxel-based branch for global\nsemantic context and a point-based branch for fine-grained structural details.\nTo effectively integrate the complementary RoI features, we introduce a\nbidirectional gated RoI feature fusion module that balances global and local\ncues. Extensive experiments on the KITTI benchmark show that our method\nconsistently improves detection accuracy, demonstrating the value of\nincorporating visual foundation model priors into LiDAR-based 3D object\ndetection.", "AI": {"tldr": "\u901a\u8fc7\u878d\u5408DepthAnything\u7684\u6df1\u5ea6\u5148\u9a8c\u6765\u589e\u5f3aLiDAR\u70b9\u7279\u5f81\uff0c\u5e76\u91c7\u7528\u65b0\u7684\u7279\u5f81\u63d0\u53d6\u548c\u878d\u5408\u6846\u67b6\uff0c\u663e\u8457\u63d0\u9ad8\u4e863D\u76ee\u6807\u68c0\u6d4b\u7684\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e2dLiDAR\u70b9\u7279\u5f81\u8868\u8fbe\u80fd\u529b\u6709\u9650\uff0c\u5c24\u5176\u662f\u53cd\u5c04\u7387\u5c5e\u6027\u533a\u5206\u80fd\u529b\u8f83\u5f31\u7684\u95ee\u9898\uff0c\u5e76\u66f4\u597d\u5730\u5229\u7528\u5355\u76eeRGB\u56fe\u50cf\u7684\u6df1\u5ea6\u5148\u9a8c\u4fe1\u606f\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u878d\u5408\u6df1\u5ea6\u5148\u9a8c\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u70b9\u5bf9\u7279\u5f81\u63d0\u53d6\u6a21\u5757\u548c\u53cc\u8def\u5f84RoI\u7279\u5f81\u63d0\u53d6\u6846\u67b6\uff08\u5305\u542b\u57fa\u4e8e\u4f53\u7d20\u7684\u5206\u652f\u548c\u57fa\u4e8e\u70b9\u7684\u5206\u652f\uff09\uff0c\u5e76\u4f7f\u7528\u53cc\u5411\u95e8\u63a7RoI\u7279\u5f81\u878d\u5408\u6a21\u5757\u6765\u6574\u5408\u4e92\u8865\u7684RoI\u7279\u5f81\u3002", "result": "\u5728KITTI\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u6301\u7eed\u63d0\u5347\u68c0\u6d4b\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u878d\u5408DepthAnything\u9884\u6d4b\u7684\u6df1\u5ea6\u5148\u9a8c\u6765\u4e30\u5bcc\u70b9\u7279\u5f81\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u70b9\u5bf9\u7279\u5f81\u63d0\u53d6\u6a21\u5757\u548c\u53cc\u8def\u5f84RoI\u7279\u5f81\u63d0\u53d6\u6846\u67b6\uff0c\u4ee5\u6709\u6548\u878d\u5408\u5168\u5c40\u548c\u5c40\u90e8\u7ebf\u7d22\uff0c\u4ece\u800c\u63d0\u5347\u4e86\u57fa\u4e8eLiDAR\u76843D\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\u3002"}}
{"id": "2507.13929", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.13929", "abs": "https://arxiv.org/abs/2507.13929", "authors": ["Hsiang-Hui Hung", "Huu-Phu Do", "Yung-Hui Li", "Ching-Chun Huang"], "title": "TimeNeRF: Building Generalizable Neural Radiance Fields across Time from Few-Shot Input Views", "comment": "Accepted by MM 2024", "summary": "We present TimeNeRF, a generalizable neural rendering approach for rendering\nnovel views at arbitrary viewpoints and at arbitrary times, even with few input\nviews. For real-world applications, it is expensive to collect multiple views\nand inefficient to re-optimize for unseen scenes. Moreover, as the digital\nrealm, particularly the metaverse, strives for increasingly immersive\nexperiences, the ability to model 3D environments that naturally transition\nbetween day and night becomes paramount. While current techniques based on\nNeural Radiance Fields (NeRF) have shown remarkable proficiency in synthesizing\nnovel views, the exploration of NeRF's potential for temporal 3D scene modeling\nremains limited, with no dedicated datasets available for this purpose. To this\nend, our approach harnesses the strengths of multi-view stereo, neural radiance\nfields, and disentanglement strategies across diverse datasets. This equips our\nmodel with the capability for generalizability in a few-shot setting, allows us\nto construct an implicit content radiance field for scene representation, and\nfurther enables the building of neural radiance fields at any arbitrary time.\nFinally, we synthesize novel views of that time via volume rendering.\nExperiments show that TimeNeRF can render novel views in a few-shot setting\nwithout per-scene optimization. Most notably, it excels in creating realistic\nnovel views that transition smoothly across different times, adeptly capturing\nintricate natural scene changes from dawn to dusk.", "AI": {"tldr": "TimeNeRF\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u795e\u7ecf\u6e32\u67d3\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u4efb\u610f\u89c6\u89d2\u548c\u65f6\u95f4\u6e32\u67d3\u65b0\u9896\u89c6\u56fe\uff0c\u5373\u4f7f\u53ea\u6709\u5f88\u5c11\u7684\u8f93\u5165\u89c6\u56fe\u3002\u5b83\u901a\u8fc7\u7ed3\u5408\u591a\u89c6\u56fe\u7acb\u4f53\u3001\u795e\u7ecf\u8f90\u5c04\u573a\u548c\u89e3\u8026\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u8de8\u4e0d\u540c\u6570\u636e\u96c6\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u80fd\u5728\u4e0d\u8fdb\u884c\u6bcf\u573a\u666f\u4f18\u5316\u7684\u60c5\u51b5\u4e0b\uff0c\u751f\u6210\u5e73\u7a33\u8fc7\u6e21\u4e0d\u540c\u65f6\u95f4\u7684\u903c\u771f\u89c6\u56fe\u3002", "motivation": "\u4e3a\u4e86\u5728\u5143\u5b87\u5b99\u7b49\u6570\u5b57\u9886\u57df\u4e2d\u5b9e\u73b0\u66f4\u5177\u6c89\u6d78\u611f\u7684\u4f53\u9a8c\uff0c\u9700\u8981\u80fd\u591f\u5bf9\u663c\u591c\u81ea\u7136\u8fc7\u6e21\u76843D\u73af\u5883\u8fdb\u884c\u5efa\u6a21\u3002\u73b0\u6709\u7684NeRF\u6280\u672f\u5728\u5408\u6210\u65b0\u9896\u89c6\u56fe\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5728\u65f6\u95f43D\u573a\u666f\u5efa\u6a21\u65b9\u9762\u7684\u6f5c\u529b\u63a2\u7d22\u6709\u9650\uff0c\u4e14\u7f3a\u4e4f\u4e13\u95e8\u7684\u6570\u636e\u96c6\u3002", "method": "TimeNeRF\u7ed3\u5408\u4e86\u591a\u89c6\u56fe\u7acb\u4f53\u3001\u795e\u7ecf\u8f90\u5c04\u573a\u548c\u8de8\u6570\u636e\u96c6\u7684\u89e3\u8026\u7b56\u7565\uff0c\u4ee5\u6784\u5efa\u9690\u5f0f\u5185\u5bb9\u8f90\u5c04\u573a\uff0c\u5e76\u5141\u8bb8\u5728\u4efb\u4f55\u4efb\u610f\u65f6\u95f4\u6784\u5efa\u795e\u7ecf\u8f90\u5c04\u573a\uff0c\u6700\u7ec8\u901a\u8fc7\u4f53\u79ef\u6e32\u67d3\u6765\u5408\u6210\u8be5\u65f6\u95f4\u7684\u65b0\u9896\u89c6\u56fe\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cTimeNeRF\u53ef\u4ee5\u5728\u5c11\u6837\u672c\u8bbe\u7f6e\u4e2d\u6e32\u67d3\u65b0\u9896\u89c6\u56fe\uff0c\u800c\u65e0\u9700\u8fdb\u884c\u6bcf\u573a\u666f\u4f18\u5316\uff0c\u5e76\u4e14\u5728\u521b\u5efa\u8de8\u8d8a\u4e0d\u540c\u65f6\u95f4\u7684\u903c\u771f\u65b0\u9896\u89c6\u56fe\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u719f\u7ec3\u5730\u6355\u6349\u4ece\u9ece\u660e\u5230\u9ec4\u660f\u7684\u590d\u6742\u81ea\u7136\u573a\u666f\u53d8\u5316\u3002", "conclusion": "TimeNeRF\u80fd\u591f\u5728\u65b0\u9896\u7684\u89c6\u56fe\u8bbe\u7f6e\u4e2d\u8fdb\u884c\u6e32\u67d3\uff0c\u800c\u65e0\u9700\u8fdb\u884c\u6bcf\u573a\u666f\u4f18\u5316\uff0c\u5e76\u4e14\u5728\u8de8\u8d8a\u4e0d\u540c\u65f6\u95f4\uff08\u4ece\u9ece\u660e\u5230\u9ec4\u660f\uff09\u7684\u5e73\u7a33\u8fc7\u6e21\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2507.13934", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13934", "abs": "https://arxiv.org/abs/2507.13934", "authors": ["Marzieh Gheisari", "Auguste Genovesio"], "title": "DiViD: Disentangled Video Diffusion for Static-Dynamic Factorization", "comment": null, "summary": "Unsupervised disentanglement of static appearance and dynamic motion in video\nremains a fundamental challenge, often hindered by information leakage and\nblurry reconstructions in existing VAE- and GAN-based approaches. We introduce\nDiViD, the first end-to-end video diffusion framework for explicit\nstatic-dynamic factorization. DiViD's sequence encoder extracts a global static\ntoken from the first frame and per-frame dynamic tokens, explicitly removing\nstatic content from the motion code. Its conditional DDPM decoder incorporates\nthree key inductive biases: a shared-noise schedule for temporal consistency, a\ntime-varying KL-based bottleneck that tightens at early timesteps (compressing\nstatic information) and relaxes later (enriching dynamics), and cross-attention\nthat routes the global static token to all frames while keeping dynamic tokens\nframe-specific. An orthogonality regularizer further prevents residual\nstatic-dynamic leakage. We evaluate DiViD on real-world benchmarks using\nswap-based accuracy and cross-leakage metrics. DiViD outperforms\nstate-of-the-art sequential disentanglement methods: it achieves the highest\nswap-based joint accuracy, preserves static fidelity while improving dynamic\ntransfer, and reduces average cross-leakage.", "AI": {"tldr": "DiViD\u662f\u4e00\u4e2a\u521b\u65b0\u7684\u89c6\u9891\u6269\u6563\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u8026\u89c6\u9891\u4e2d\u7684\u9759\u6001\u5916\u89c2\u548c\u52a8\u6001\u8fd0\u52a8\uff0c\u901a\u8fc7\u5176\u72ec\u7279\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\u548c\u6b63\u5219\u5316\u6280\u672f\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u65e0\u76d1\u7763\u89c6\u9891\u8868\u793a\u5b66\u4e60\u4e2d\u9759\u6001\u5916\u89c2\u548c\u52a8\u6001\u8fd0\u52a8\u89e3\u8026\u7684\u6839\u672c\u6311\u6218\uff0c\u514b\u670d\u73b0\u6709VAE\u548cGAN\u65b9\u6cd5\u4e2d\u7684\u4fe1\u606f\u6cc4\u6f0f\u548c\u6a21\u7cca\u91cd\u5efa\u95ee\u9898\u3002", "method": "DiViD\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u89c6\u9891\u6269\u6563\u6846\u67b6\uff0c\u901a\u8fc7\u5e8f\u5217\u7f16\u7801\u5668\u63d0\u53d6\u5168\u5c40\u9759\u6001\u6807\u8bb0\u548c\u6bcf\u5e27\u52a8\u6001\u6807\u8bb0\uff0c\u5e76\u7ed3\u5408\u6761\u4ef6DDPM\u89e3\u7801\u5668\uff0c\u8be5\u89e3\u7801\u5668\u5177\u6709\u5171\u4eab\u566a\u58f0\u8c03\u5ea6\u3001\u65f6\u53d8KL\u6563\u5ea6\u74f6\u9888\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4ee5\u53ca\u6b63\u4ea4\u6b63\u5219\u5316\u5668\uff0c\u4ee5\u5b9e\u73b0\u663e\u5f0f\u7684\u9759\u6001-\u52a8\u6001\u56e0\u5b50\u5316\u3002", "result": "DiViD\u5728\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5728\u57fa\u4e8e\u4ea4\u6362\u7684\u51c6\u786e\u7387\u548c\u4ea4\u53c9\u6cc4\u6f0f\u6307\u6807\u4e0a\uff0c\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u987a\u5e8f\u89e3\u8026\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u6700\u9ad8\u7684\u57fa\u4e8e\u4ea4\u6362\u7684\u8054\u5408\u51c6\u786e\u7387\uff0c\u63d0\u9ad8\u4e86\u52a8\u6001\u4f20\u8f93\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9759\u6001\u4fdd\u771f\u5ea6\uff0c\u5e76\u51cf\u5c11\u4e86\u5e73\u5747\u4ea4\u53c9\u6cc4\u6f0f\u3002", "conclusion": "DiViD\u5728\u516c\u5f00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u987a\u5e8f\u89e3\u8026\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u6700\u9ad8\u7684\u57fa\u4e8e\u4ea4\u6362\u7684\u8054\u5408\u51c6\u786e\u7387\uff0c\u5728\u4fdd\u6301\u9759\u6001\u4fdd\u771f\u5ea6\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u52a8\u6001\u4f20\u8f93\uff0c\u5e76\u51cf\u5c11\u4e86\u5e73\u5747\u4ea4\u53c9\u6cc4\u6f0f\u3002"}}
{"id": "2507.13942", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13942", "abs": "https://arxiv.org/abs/2507.13942", "authors": ["Jacob C Walker", "Pedro V\u00e9lez", "Luisa Polania Cabrera", "Guangyao Zhou", "Rishabh Kabra", "Carl Doersch", "Maks Ovsjanikov", "Jo\u00e3o Carreira", "Shiry Ginosar"], "title": "Generalist Forecasting with Frozen Video Models via Latent Diffusion", "comment": null, "summary": "Forecasting what will happen next is a critical skill for general-purpose\nsystems that plan or act in the world at different levels of abstraction. In\nthis paper, we identify a strong correlation between a vision model's\nperceptual ability and its generalist forecasting performance over short time\nhorizons. This trend holds across a diverse set of pretrained models-including\nthose trained generatively-and across multiple levels of abstraction, from raw\npixels to depth, point tracks, and object motion. The result is made possible\nby a novel generalist forecasting framework that operates on any frozen vision\nbackbone: we train latent diffusion models to forecast future features in the\nfrozen representation space, which are then decoded via lightweight,\ntask-specific readouts. To enable consistent evaluation across tasks, we\nintroduce distributional metrics that compare distributional properties\ndirectly in the space of downstream tasks and apply this framework to nine\nmodels and four tasks. Our results highlight the value of bridging\nrepresentation learning and generative modeling for temporally grounded video\nunderstanding.", "AI": {"tldr": "\u5728\u77ed\u65f6\u95f4\u8303\u56f4\u5185\uff0c\u89c6\u89c9\u6a21\u578b\u7684\u611f\u77e5\u80fd\u529b\u4e0e\u5176\u901a\u7528\u9884\u6d4b\u6027\u80fd\u4e4b\u95f4\u5b58\u5728\u5f88\u5f3a\u7684\u76f8\u5173\u6027\u3002\u901a\u8fc7\u5728\u56fa\u5b9a\u8868\u793a\u7a7a\u95f4\u4e2d\u9884\u6d4b\u672a\u6765\u7279\u5f81\uff0c\u5e76\u4f7f\u7528\u5206\u5e03\u5ea6\u91cf\u8fdb\u884c\u8bc4\u4f30\uff0c\u53ef\u4ee5\u5b9e\u73b0\u8fd9\u4e00\u70b9\u3002", "motivation": "\u9884\u6d4b\u63a5\u4e0b\u6765\u4f1a\u53d1\u751f\u4ec0\u4e48\u5bf9\u4e8e\u5728\u4e0d\u540c\u62bd\u8c61\u7ea7\u522b\u4e0a\u5728\u4e16\u754c\u4e2d\u8fdb\u884c\u89c4\u5212\u6216\u884c\u52a8\u7684\u901a\u7528\u7cfb\u7edf\u7684\u5173\u952e\u6280\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u901a\u7528\u9884\u6d4b\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u53ef\u4ee5\u5728\u4efb\u4f55\u56fa\u5b9a\u7684\u89c6\u89c9\u9aa8\u5e72\u7f51\u4e0a\u8fd0\u884c\uff1a\u6211\u4eec\u8bad\u7ec3\u6f5c\u5728\u6269\u6563\u6a21\u578b\u6765\u9884\u6d4b\u56fa\u5b9a\u8868\u793a\u7a7a\u95f4\u4e2d\u7684\u672a\u6765\u7279\u5f81\uff0c\u7136\u540e\u901a\u8fc7\u8f7b\u91cf\u7ea7\u7684\u3001\u7279\u5b9a\u4efb\u52a1\u7684\u8bfb\u51fa\u8fdb\u884c\u89e3\u7801\u3002\u4e3a\u4e86\u5728\u4e0d\u540c\u4efb\u52a1\u4e4b\u95f4\u5b9e\u73b0\u4e00\u81f4\u7684\u8bc4\u4f30\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u76f4\u63a5\u5728\u4e0b\u6e38\u4efb\u52a1\u7a7a\u95f4\u4e2d\u6bd4\u8f83\u5206\u5e03\u7279\u6027\u7684\u5206\u5e03\u5ea6\u91cf\uff0c\u5e76\u5c06\u6b64\u6846\u67b6\u5e94\u7528\u4e8e\u4e5d\u4e2a\u6a21\u578b\u548c\u56db\u4e2a\u4efb\u52a1\u3002", "result": "\u5728\u77ed\u65f6\u95f4\u8303\u56f4\u5185\uff0c\u89c6\u89c9\u6a21\u578b\u7684\u611f\u77e5\u80fd\u529b\u4e0e\u5176\u901a\u7528\u9884\u6d4b\u6027\u80fd\u4e4b\u95f4\u5b58\u5728\u5f88\u5f3a\u7684\u76f8\u5173\u6027\u3002\u8fd9\u4e00\u8d8b\u52bf\u5728\u591a\u79cd\u9884\u8bad\u7ec3\u6a21\u578b\uff08\u5305\u62ec\u751f\u6210\u5f0f\u8bad\u7ec3\u7684\u6a21\u578b\uff09\u4ee5\u53ca\u4ece\u539f\u59cb\u50cf\u7d20\u5230\u6df1\u5ea6\u3001\u70b9\u8f68\u8ff9\u548c\u5bf9\u8c61\u8fd0\u52a8\u7684\u591a\u4e2a\u62bd\u8c61\u7ea7\u522b\u4e0a\u90fd\u6210\u7acb\u3002", "conclusion": "\u672c\u6587\u7684\u53d1\u73b0\u5f3a\u8c03\u4e86\u8fde\u63a5\u8868\u5f81\u5b66\u4e60\u548c\u751f\u6210\u6a21\u578b\u5bf9\u4e8e\u65f6\u95f4\u76f8\u5173\u7684\u89c6\u9891\u7406\u89e3\u7684\u4ef7\u503c\u3002"}}
{"id": "2507.13981", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13981", "abs": "https://arxiv.org/abs/2507.13981", "authors": ["Sara Abdulaziz", "Giacomo D'Amicantonio", "Egor Bondarev"], "title": "Evaluation of Human Visual Privacy Protection: A Three-Dimensional Framework and Benchmark Dataset", "comment": "accepted at ICCV'25 workshop CV4BIOM", "summary": "Recent advances in AI-powered surveillance have intensified concerns over the\ncollection and processing of sensitive personal data. In response, research has\nincreasingly focused on privacy-by-design solutions, raising the need for\nobjective techniques to evaluate privacy protection. This paper presents a\ncomprehensive framework for evaluating visual privacy-protection methods across\nthree dimensions: privacy, utility, and practicality. In addition, it\nintroduces HR-VISPR, a publicly available human-centric dataset with biometric,\nsoft-biometric, and non-biometric labels to train an interpretable privacy\nmetric. We evaluate 11 privacy protection methods, ranging from conventional\ntechniques to advanced deep-learning methods, through the proposed framework.\nThe framework differentiates privacy levels in alignment with human visual\nperception, while highlighting trade-offs between privacy, utility, and\npracticality. This study, along with the HR-VISPR dataset, serves as an\ninsightful tool and offers a structured evaluation framework applicable across\ndiverse contexts.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u9690\u79c1\u4fdd\u62a4\u65b9\u6cd5\u7684\u6846\u67b6\u548cHR-VISPR\u6570\u636e\u96c6\uff0c\u8be5\u6846\u67b6\u53ef\u4ee5\u6839\u636e\u4eba\u7c7b\u611f\u77e5\u533a\u5206\u9690\u79c1\u7ea7\u522b\uff0c\u5e76\u6743\u8861\u9690\u79c1\u3001\u6548\u7528\u548c\u5b9e\u7528\u6027\u3002", "motivation": "\u4eba\u5de5\u667a\u80fd\u76d1\u63a7\u7684\u6700\u65b0\u8fdb\u5c55\u52a0\u5267\u4e86\u4eba\u4eec\u5bf9\u654f\u611f\u4e2a\u4eba\u6570\u636e\u6536\u96c6\u548c\u5904\u7406\u7684\u62c5\u5fe7\uff0c\u56e0\u6b64\u9700\u8981\u5ba2\u89c2\u7684\u6280\u672f\u6765\u8bc4\u4f30\u9690\u79c1\u4fdd\u62a4\u3002\u73b0\u6709\u7814\u7a76\u65e5\u76ca\u5173\u6ce8\u201c\u8bbe\u8ba1\u9690\u79c1\u201d\u89e3\u51b3\u65b9\u6848\uff0c\u8fd9\u4f7f\u5f97\u5f00\u53d1\u4e00\u4e2a\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u5305\u542b\u9690\u79c1\u3001\u6548\u7528\u548c\u5b9e\u7528\u6027\u4e09\u4e2a\u7ef4\u5ea6\u7684\u7efc\u5408\u6846\u67b6\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u4e2a\u540d\u4e3aHR-VISPR\u7684\u3001\u4ee5\u4eba\u7c7b\u4e3a\u4e2d\u5fc3\u7684\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5305\u542b\u751f\u7269\u8bc6\u522b\u3001\u8f6f\u751f\u7269\u8bc6\u522b\u548c\u975e\u751f\u7269\u8bc6\u522b\u6807\u7b7e\uff0c\u7528\u4e8e\u8bad\u7ec3\u53ef\u89e3\u91ca\u7684\u9690\u79c1\u6307\u6807\u3002\u8bc4\u4f30\u4e8611\u79cd\u9690\u79c1\u4fdd\u62a4\u65b9\u6cd5\uff0c\u6db5\u76d6\u4e86\u4f20\u7edf\u6280\u672f\u548c\u5148\u8fdb\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u3002", "result": "\u8be5\u6846\u67b6\u80fd\u591f\u533a\u5206\u4e0e\u4eba\u7c7b\u89c6\u89c9\u611f\u77e5\u4e00\u81f4\u7684\u9690\u79c1\u7ea7\u522b\uff0c\u5e76\u7a81\u663e\u4e86\u9690\u79c1\u3001\u6548\u7528\u548c\u5b9e\u7528\u6027\u4e4b\u95f4\u7684\u6743\u8861\u3002\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u4e3a\u6bd4\u8f83\u548c\u9009\u62e9\u9690\u79c1\u4fdd\u62a4\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7ed3\u6784\u5316\u7684\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bc4\u4f30\u89c6\u89c9\u9690\u79c1\u4fdd\u62a4\u65b9\u6cd5\u7684\u7efc\u5408\u6846\u67b6\uff0c\u5e76\u5728HR-VISPR\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u8be5\u6570\u636e\u96c6\u5305\u542b\u751f\u7269\u8bc6\u522b\u3001\u8f6f\u751f\u7269\u8bc6\u522b\u548c\u975e\u751f\u7269\u8bc6\u522b\u6807\u7b7e\uff0c\u7528\u4e8e\u8bad\u7ec3\u53ef\u89e3\u91ca\u7684\u9690\u79c1\u6307\u6807\u3002\u8be5\u6846\u67b6\u80fd\u591f\u533a\u5206\u4e0e\u4eba\u7c7b\u89c6\u89c9\u611f\u77e5\u4e00\u81f4\u7684\u9690\u79c1\u7ea7\u522b\uff0c\u5e76\u5f3a\u8c03\u9690\u79c1\u3001\u6548\u7528\u548c\u5b9e\u7528\u6027\u4e4b\u95f4\u7684\u6743\u8861\u3002\u8fd9\u9879\u7814\u7a76\u53ca\u5176\u6570\u636e\u96c6\u4e3a\u8bc4\u4f30\u9690\u79c1\u4fdd\u62a4\u6280\u672f\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u4ef7\u503c\u7684\u5de5\u5177\u548c\u7ed3\u6784\u5316\u7684\u8bc4\u4f30\u6846\u67b6\u3002"}}
{"id": "2507.13984", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13984", "abs": "https://arxiv.org/abs/2507.13984", "authors": ["Quang-Binh Nguyen", "Minh Luu", "Quang Nguyen", "Anh Tran", "Khoi Nguyen"], "title": "CSD-VAR: Content-Style Decomposition in Visual Autoregressive Models", "comment": "Accepted to ICCV 2025", "summary": "Disentangling content and style from a single image, known as content-style\ndecomposition (CSD), enables recontextualization of extracted content and\nstylization of extracted styles, offering greater creative flexibility in\nvisual synthesis. While recent personalization methods have explored the\ndecomposition of explicit content style, they remain tailored for diffusion\nmodels. Meanwhile, Visual Autoregressive Modeling (VAR) has emerged as a\npromising alternative with a next-scale prediction paradigm, achieving\nperformance comparable to that of diffusion models. In this paper, we explore\nVAR as a generative framework for CSD, leveraging its scale-wise generation\nprocess for improved disentanglement. To this end, we propose CSD-VAR, a novel\nmethod that introduces three key innovations: (1) a scale-aware alternating\noptimization strategy that aligns content and style representation with their\nrespective scales to enhance separation, (2) an SVD-based rectification method\nto mitigate content leakage into style representations, and (3) an Augmented\nKey-Value (K-V) memory enhancing content identity preservation. To benchmark\nthis task, we introduce CSD-100, a dataset specifically designed for\ncontent-style decomposition, featuring diverse subjects rendered in various\nartistic styles. Experiments demonstrate that CSD-VAR outperforms prior\napproaches, achieving superior content preservation and stylization fidelity.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa CSD-VAR\uff0c\u4e00\u79cd\u7528\u4e8e\u89c6\u89c9\u81ea\u56de\u5f52\u5efa\u6a21\u7684\u5185\u5bb9-\u6837\u5f0f\u5206\u89e3\u65b9\u6cd5\uff0c\u53ef\u5b9e\u73b0\u66f4\u597d\u7684\u5185\u5bb9\u4fdd\u7559\u548c\u98ce\u683c\u4fdd\u771f\u5ea6\u3002", "motivation": "\u5728\u4ece\u5355\u4e2a\u56fe\u50cf\u4e2d\u5206\u79bb\u5185\u5bb9\u548c\u6837\u5f0f\uff08\u5373\u5185\u5bb9-\u6837\u5f0f\u5206\u89e3\uff0cCSD\uff09\u7684\u7814\u7a76\u80cc\u666f\u4e0b\uff0c\u672c\u7814\u7a76\u65e8\u5728\u5c06\u89c6\u89c9\u81ea\u56de\u5f52\u5efa\u6a21\uff08VAR\uff09\u4f5c\u4e3a CSD \u7684\u751f\u6210\u6846\u67b6\uff0c\u5229\u7528\u5176\u5c3a\u5ea6\u751f\u6210\u8fc7\u7a0b\u6765\u6539\u8fdb\u5206\u79bb\u3002", "method": "CSD-VAR \u662f\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u5b83\u5f15\u5165\u4e86\u4e09\u79cd\u5173\u952e\u521b\u65b0\uff1a(1) \u4e00\u79cd\u5173\u6ce8\u5c3a\u5ea6\u7684\u4ea4\u66ff\u4f18\u5316\u7b56\u7565\uff0c\u5c06\u5185\u5bb9\u548c\u6837\u5f0f\u8868\u793a\u4e0e\u5176\u5404\u81ea\u7684\u5c3a\u5ea6\u5bf9\u9f50\u4ee5\u589e\u5f3a\u5206\u79bb\uff1b(2) \u4e00\u79cd\u57fa\u4e8e SVD \u7684\u6821\u6b63\u65b9\u6cd5\uff0c\u4ee5\u51cf\u8f7b\u5185\u5bb9\u6cc4\u9732\u5230\u6837\u5f0f\u8868\u793a\u4e2d\uff1b(3) \u589e\u5f3a\u5185\u5bb9\u8eab\u4efd\u4fdd\u7559\u7684\u589e\u5f3a\u952e\u503c\uff08K-V\uff09\u5185\u5b58\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCSD-VAR \u5728\u5185\u5bb9\u4fdd\u7559\u548c\u98ce\u683c\u4fdd\u771f\u5ea6\u65b9\u9762\u4f18\u4e8e\u5148\u524d\u7684\u65b9\u6cd5\u3002", "conclusion": "CSD-VAR \u5728\u5185\u5bb9\u4fdd\u7559\u548c\u98ce\u683c\u4fdd\u771f\u5ea6\u65b9\u9762\u4f18\u4e8e\u5148\u524d\u7684\u65b9\u6cd5\u3002"}}
{"id": "2507.13985", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13985", "abs": "https://arxiv.org/abs/2507.13985", "authors": ["Haoran Li", "Yuli Tian", "Kun Lan", "Yong Liao", "Lin Wang", "Pan Hui", "Peng Yuan Zhou"], "title": "DreamScene: 3D Gaussian-based End-to-end Text-to-3D Scene Generation", "comment": "Extended version of ECCV 2024 paper \"DreamScene\"", "summary": "Generating 3D scenes from natural language holds great promise for\napplications in gaming, film, and design. However, existing methods struggle\nwith automation, 3D consistency, and fine-grained control. We present\nDreamScene, an end-to-end framework for high-quality and editable 3D scene\ngeneration from text or dialogue. DreamScene begins with a scene planning\nmodule, where a GPT-4 agent infers object semantics and spatial constraints to\nconstruct a hybrid graph. A graph-based placement algorithm then produces a\nstructured, collision-free layout. Based on this layout, Formation Pattern\nSampling (FPS) generates object geometry using multi-timestep sampling and\nreconstructive optimization, enabling fast and realistic synthesis. To ensure\nglobal consistent, DreamScene employs a progressive camera sampling strategy\ntailored to both indoor and outdoor settings. Finally, the system supports\nfine-grained scene editing, including object movement, appearance changes, and\n4D dynamic motion. Experiments demonstrate that DreamScene surpasses prior\nmethods in quality, consistency, and flexibility, offering a practical solution\nfor open-domain 3D content creation. Code and demos are available at\nhttps://dreamscene-project.github.io.", "AI": {"tldr": "DreamScene is a new framework for creating 3D scenes from text, improving automation, consistency, and editing capabilities compared to previous methods.", "motivation": "Existing methods for generating 3D scenes from natural language struggle with automation, 3D consistency, and fine-grained control.", "method": "DreamScene is an end-to-end framework that uses a GPT-4 agent for scene planning, a graph-based placement algorithm for layout, Formation Pattern Sampling (FPS) for object geometry generation, and a progressive camera sampling strategy for global consistency. It also supports fine-grained editing.", "result": "High-quality and editable 3D scene generation from text or dialogue, with improvements in quality, consistency, and flexibility.", "conclusion": "DreamScene surpassed prior methods in quality, consistency, and flexibility, offering a practical solution for open-domain 3D content creation."}}
{"id": "2507.14010", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14010", "abs": "https://arxiv.org/abs/2507.14010", "authors": ["Yong Feng", "Xiaolei Zhang", "Shijin Feng", "Yong Zhao", "Yihan Chen"], "title": "Automatic Classification and Segmentation of Tunnel Cracks Based on Deep Learning and Visual Explanations", "comment": "8 pages, 10 figures, 3 tables", "summary": "Tunnel lining crack is a crucial indicator of tunnels' safety status. Aiming\nto classify and segment tunnel cracks with enhanced accuracy and efficiency,\nthis study proposes a two-step deep learning-based method. An automatic tunnel\nimage classification model is developed using the DenseNet-169 in the first\nstep. The proposed crack segmentation model in the second step is based on the\nDeepLabV3+, whose internal logic is evaluated via a score-weighted visual\nexplanation technique. Proposed method combines tunnel image classification and\nsegmentation together, so that the selected images containing cracks from the\nfirst step are segmented in the second step to improve the detection accuracy\nand efficiency. The superior performances of the two-step method are validated\nby experiments. The results show that the accuracy and frames per second (FPS)\nof the tunnel crack classification model are 92.23% and 39.80, respectively,\nwhich are higher than other convolutional neural networks (CNN) based and\nTransformer based models. Also, the intersection over union (IoU) and F1 score\nof the tunnel crack segmentation model are 57.01% and 67.44%, respectively,\noutperforming other state-of-the-art models. Moreover, the provided visual\nexplanations in this study are conducive to understanding the \"black box\" of\ndeep learning-based models. The developed two-stage deep learning-based method\nintegrating visual explanations provides a basis for fast and accurate\nquantitative assessment of tunnel health status.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u56fe\u50cf\u5206\u7c7b\u548c\u5206\u5272\u7684\u4e24\u6b65\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u96a7\u9053\u88c2\u7f1d\u7684\u68c0\u6d4b\uff0c\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u5e76\u901a\u8fc7\u53ef\u89c6\u5316\u89e3\u91ca\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u53ef\u7406\u89e3\u6027\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u96a7\u9053\u88c2\u7f1d\u5206\u7c7b\u548c\u5206\u5272\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u6b65\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u7b2c\u4e00\u6b65\u4f7f\u7528DenseNet-169\u5f00\u53d1\u81ea\u52a8\u96a7\u9053\u56fe\u50cf\u5206\u7c7b\u6a21\u578b\uff0c\u7b2c\u4e8c\u6b65\u57fa\u4e8eDeepLabV3+\u7684\u88c2\u7f1d\u5206\u5272\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u5206\u6570\u52a0\u6743\u53ef\u89c6\u5316\u89e3\u91ca\u6280\u672f\u8bc4\u4f30\u5176\u5185\u90e8\u903b\u8f91\u3002", "result": "\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u96a7\u9053\u56fe\u50cf\u5206\u7c7b\u548c\u5206\u5272\uff0c\u901a\u8fc7\u5728\u7b2c\u4e8c\u6b65\u4e2d\u5206\u5272\u7b2c\u4e00\u6b65\u4e2d\u9009\u62e9\u7684\u5305\u542b\u88c2\u7f1d\u7684\u56fe\u50cf\uff0c\u63d0\u9ad8\u4e86\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u4e24\u6b65\u6cd5\u7684\u4f18\u8d8a\u6027\u80fd\uff0c\u5176\u4e2d\u88c2\u7f1d\u5206\u7c7b\u6a21\u578b\u7684\u51c6\u786e\u7387\u4e3a92.23%\uff0c\u6bcf\u79d2\u5e27\u6570\u4e3a39.80\uff1b\u88c2\u7f1d\u5206\u5272\u6a21\u578b\u7684\u4ea4\u5e76\u6bd4\uff08IoU\uff09\u548cF1\u5206\u6570\u5206\u522b\u4e3a57.01%\u548c67.44%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u96a7\u9053\u88c2\u7f1d\u7684\u5feb\u901f\u51c6\u786e\u91cf\u5316\u8bc4\u4f30\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.14013", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14013", "abs": "https://arxiv.org/abs/2507.14013", "authors": ["Ji-Yan Wu", "Zheng Yong Poh", "Anoop C. Patil", "Bongsoo Park", "Giovanni Volpe", "Daisuke Urano"], "title": "Analysis of Plant Nutrient Deficiencies Using Multi-Spectral Imaging and Optimized Segmentation Model", "comment": null, "summary": "Accurate detection of nutrient deficiency in plant leaves is essential for\nprecision agriculture, enabling early intervention in fertilization, disease,\nand stress management. This study presents a deep learning framework for leaf\nanomaly segmentation using multispectral imaging and an enhanced YOLOv5 model\nwith a transformer-based attention head. The model is tailored for processing\nnine-channel multispectral input and uses self-attention mechanisms to better\ncapture subtle, spatially-distributed symptoms. The plants in the experiments\nwere grown under controlled nutrient stress conditions for evaluation. We carry\nout extensive experiments to benchmark the proposed model against the baseline\nYOLOv5. Extensive experiments show that the proposed model significantly\noutperforms the baseline YOLOv5, with an average Dice score and IoU\n(Intersection over Union) improvement of about 12%. In particular, this model\nis effective in detecting challenging symptoms like chlorosis and pigment\naccumulation. These results highlight the promise of combining multi-spectral\nimaging with spectral-spatial feature learning for advancing plant phenotyping\nand precision agriculture.", "AI": {"tldr": "\u5229\u7528\u6539\u8fdb\u7684YOLOv5\u548c\u591a\u5149\u8c31\u6210\u50cf\u6280\u672f\uff0c\u53ef\u4ee5\u66f4\u6709\u6548\u5730\u68c0\u6d4b\u690d\u7269\u53f6\u7247\u8425\u517b\u7f3a\u4e4f\u3002", "motivation": "\u7cbe\u786e\u68c0\u6d4b\u690d\u7269\u53f6\u7247\u8425\u517b\u7f3a\u4e4f\u5bf9\u4e8e\u7cbe\u51c6\u519c\u4e1a\u81f3\u5173\u91cd\u8981\uff0c\u6709\u52a9\u4e8e\u65e9\u671f\u8fdb\u884c\u65bd\u80a5\u3001\u75c5\u866b\u5bb3\u548c\u80c1\u8feb\u7ba1\u7406\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u591a\u5149\u8c31\u6210\u50cf\u548c\u6539\u8fdb\u7684YOLOv5\u6a21\u578b\uff08\u5305\u542b\u57fa\u4e8eTransformer\u7684\u6ce8\u610f\u529b\u5934\uff09\u8fdb\u884c\u53f6\u7247\u5f02\u5e38\u5206\u5272\u3002\u8be5\u6a21\u578b\u80fd\u5904\u7406\u4e5d\u901a\u9053\u591a\u5149\u8c31\u8f93\u5165\uff0c\u5e76\u5229\u7528\u81ea\u6ce8\u610f\u529b\u673a\u5236\u6355\u6349\u7cbe\u7ec6\u3001\u7a7a\u95f4\u5206\u5e03\u7684\u75c7\u72b6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u6a21\u578b\u663e\u8457\u4f18\u4e8e\u57fa\u7ebfYOLOv5\uff0c\u5e73\u5747Dice\u5206\u6570\u548cIoU\uff08\u4ea4\u5e76\u6bd4\uff09\u63d0\u9ad8\u4e86\u7ea612%\uff0c\u5c24\u5176\u5728\u68c0\u6d4b\u7c7b\u75c7\uff08\u5982\u9ec4\u5316\uff09\u548c\u8272\u7d20\u79ef\u7d2f\u7b49\u5177\u6709\u6311\u6218\u6027\u7684\u75c7\u72b6\u65b9\u9762\u6548\u679c\u663e\u8457\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u7ed3\u5408\u591a\u5149\u8c31\u6210\u50cf\u548c\u57fa\u4e8eTransformer\u6ce8\u610f\u529b\u7684YOLOv5\u6a21\u578b\uff0c\u5728\u68c0\u6d4b\u4f5c\u7269\u53f6\u7247\u8425\u517b\u7f3a\u4e4f\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u7cbe\u51c6\u519c\u4e1a\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.14024", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14024", "abs": "https://arxiv.org/abs/2507.14024", "authors": ["Jiarong Ye", "Sharon X. Huang"], "title": "Moodifier: MLLM-Enhanced Emotion-Driven Image Editing", "comment": null, "summary": "Bridging emotions and visual content for emotion-driven image editing holds\ngreat potential in creative industries, yet precise manipulation remains\nchallenging due to the abstract nature of emotions and their varied\nmanifestations across different contexts. We tackle this challenge with an\nintegrated approach consisting of three complementary components. First, we\nintroduce MoodArchive, an 8M+ image dataset with detailed hierarchical\nemotional annotations generated by LLaVA and partially validated by human\nevaluators. Second, we develop MoodifyCLIP, a vision-language model fine-tuned\non MoodArchive to translate abstract emotions into specific visual attributes.\nThird, we propose Moodifier, a training-free editing model leveraging\nMoodifyCLIP and multimodal large language models (MLLMs) to enable precise\nemotional transformations while preserving content integrity. Our system works\nacross diverse domains such as character expressions, fashion design, jewelry,\nand home d\\'ecor, enabling creators to quickly visualize emotional variations\nwhile preserving identity and structure. Extensive experimental evaluations\nshow that Moodifier outperforms existing methods in both emotional accuracy and\ncontent preservation, providing contextually appropriate edits. By linking\nabstract emotions to concrete visual changes, our solution unlocks new\npossibilities for emotional content creation in real-world applications. We\nwill release the MoodArchive dataset, MoodifyCLIP model, and make the Moodifier\ncode and demo publicly available upon acceptance.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMoodifier\u7684\u5148\u8fdb\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u5927\u578b\u60c5\u611f\u6570\u636e\u96c6\uff08MoodArchive\uff09\u548c\u65b0\u9896\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08MoodifyCLIP\uff09\uff0c\u5b9e\u73b0\u4e86\u7cbe\u51c6\u7684\u60c5\u611f\u9a71\u52a8\u56fe\u50cf\u7f16\u8f91\uff0c\u80fd\u591f\u6839\u636e\u7528\u6237\u6307\u5b9a\u7684\u60c5\u611f\uff08\u5982\u5feb\u4e50\u3001\u60b2\u4f24\uff09\u6765\u4fee\u6539\u56fe\u50cf\u5185\u5bb9\uff08\u5982\u4eba\u7269\u8868\u60c5\u3001\u7269\u54c1\u98ce\u683c\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u56fe\u50cf\u7684\u539f\u59cb\u7ed3\u6784\u548c\u8eab\u4efd\u7279\u5f81\u3002\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u5e94\u7528\u9886\u57df\uff08\u5982\u89d2\u8272\u8bbe\u8ba1\u3001\u65f6\u5c1a\u548c\u5bb6\u5c45\u88c5\u9970\uff09\u5747\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u60c5\u611f\u9a71\u52a8\u7684\u56fe\u50cf\u7f16\u8f91\u5728\u521b\u610f\u4ea7\u4e1a\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u7531\u4e8e\u60c5\u611f\u7684\u62bd\u8c61\u6027\u548c\u591a\u53d8\u6027\uff0c\u7cbe\u786e\u64cd\u63a7\u4ecd\u7136\u56f0\u96be\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6574\u5408\u65b9\u6cd5\uff0c\u5305\u62ec\u4e09\u4e2a\u7ec4\u6210\u90e8\u5206\uff1a1. MoodArchive\uff1a\u4e00\u4e2a\u5305\u542b800\u4e07+\u56fe\u50cf\u53ca\u5176\u60c5\u611f\u6807\u6ce8\u7684\u6570\u636e\u96c6\uff0c\u7531LLaVA\u751f\u6210\u5e76\u7ecf\u4eba\u7c7b\u8bc4\u4f30\u5458\u90e8\u5206\u9a8c\u8bc1\u30022. MoodifyCLIP\uff1a\u4e00\u4e2a\u5728MoodArchive\u4e0a\u5fae\u8c03\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff0c\u7528\u4e8e\u5c06\u60c5\u611f\u8f6c\u5316\u4e3a\u89c6\u89c9\u5c5e\u6027\u30023. Moodifier\uff1a\u4e00\u4e2a\u5229\u7528MoodifyCLIP\u548c\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684\u65e0\u8bad\u7ec3\u7f16\u8f91\u6a21\u578b\uff0c\u5b9e\u73b0\u60c5\u611f\u8f6c\u6362\u5e76\u4fdd\u6301\u5185\u5bb9\u5b8c\u6574\u6027\u3002", "result": "Moodifier\u7cfb\u7edf\u5728\u5b57\u7b26\u8868\u60c5\u3001\u65f6\u5c1a\u8bbe\u8ba1\u3001\u73e0\u5b9d\u548c\u5bb6\u5c45\u88c5\u9970\u7b49\u9886\u57df\u5b9e\u73b0\u4e86\u8de8\u57df\u7684\u60c5\u611f\u56fe\u50cf\u7f16\u8f91\uff0c\u80fd\u591f\u4fdd\u6301\u8eab\u4efd\u548c\u7ed3\u6784\u3002\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0cMoodifier\u5728\u60c5\u611f\u51c6\u786e\u6027\u548c\u5185\u5bb9\u4fdd\u7559\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u63d0\u4f9b\u7684\u7f16\u8f91\u5177\u6709\u60c5\u5883\u9002\u5e94\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u6574\u5408MoodArchive\u6570\u636e\u96c6\u3001MoodifyCLIP\u6a21\u578b\u548cMoodifier\u7f16\u8f91\u6a21\u578b\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u60c5\u611f\u9a71\u52a8\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u4ece\u62bd\u8c61\u60c5\u611f\u5230\u5177\u4f53\u89c6\u89c9\u5c5e\u6027\u7684\u7cbe\u51c6\u8f6c\u5316\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5185\u5bb9\u5b8c\u6574\u6027\u3002\u8be5\u65b9\u6cd5\u5728\u5b57\u7b26\u8868\u60c5\u3001\u65f6\u5c1a\u8bbe\u8ba1\u3001\u73e0\u5b9d\u548c\u5bb6\u5c45\u88c5\u9970\u7b49\u591a\u4e2a\u9886\u57df\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u5feb\u901f\u53ef\u89c6\u5316\u60c5\u611f\u53d8\u5316\uff0c\u5e76\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3a\u73b0\u5b9e\u4e16\u754c\u7684\u60c5\u611f\u5185\u5bb9\u521b\u4f5c\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2507.14042", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14042", "abs": "https://arxiv.org/abs/2507.14042", "authors": ["Qiankun Ma", "Ziyao Zhang", "Chi Su", "Jie Chen", "Zhen Song", "Hairong Zheng", "Wen Gao"], "title": "Training-free Token Reduction for Vision Mamba", "comment": null, "summary": "Vision Mamba has emerged as a strong competitor to Vision Transformers (ViTs)\ndue to its ability to efficiently capture long-range dependencies with linear\ncomputational complexity. While token reduction, an effective compression\ntechnique in ViTs, has rarely been explored in Vision Mamba. Exploring Vision\nMamba's efficiency is essential for enabling broader applications. However, we\nfind that directly applying existing token reduction techniques for ViTs to\nVision Mamba leads to significant performance degradation. This is primarily\nbecause Mamba is a sequence model without attention mechanisms, whereas most\ntoken reduction techniques for ViTs rely on attention mechanisms for importance\nmeasurement and overlook the order of compressed tokens. In this paper, we\ninvestigate a Mamba structure-aware importance score to evaluate token\nimportance in a simple and effective manner. Building on this score, we further\npropose MTR, a training-free \\textbf{M}amba \\textbf{T}oken \\textbf{R}eduction\nframework. Without the need for training or additional tuning parameters, our\nmethod can be seamlessly integrated as a plug-and-play component across various\nMamba models. Extensive experiments demonstrate that our approach significantly\nreduces computational workload while minimizing performance impact across\nvarious tasks and multiple backbones. Notably, MTR reduces FLOPs by\napproximately 40\\% on the Vim-B backbone, with only a 1.6\\% drop in ImageNet\nperformance without retraining.", "AI": {"tldr": "MTR\u662f\u4e00\u79cd\u7528\u4e8eVision Mamba\u7684\u9ad8\u6548\u6807\u8bb0\u7f29\u51cf\u6846\u67b6\uff0c\u53ef\u51cf\u5c11\u8ba1\u7b97\u91cf\u5e76\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3Vision Mamba\u5728\u5e94\u7528\u4e2d\u7684\u6548\u7387\u95ee\u9898\uff0c\u5e76\u514b\u670d\u73b0\u6709ViT\u6807\u8bb0\u7f29\u51cf\u6280\u672f\u4e0d\u9002\u7528\u4e8eMamba\u6a21\u578b\u7684\u95ee\u9898\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cdMamba\u7ed3\u6784\u611f\u77e5\u7684\u91cd\u8981\u6027\u8bc4\u5206\u65b9\u6cd5\uff0c\u5e76\u57fa\u4e8e\u6b64\u5f00\u53d1\u4e86MTR\u6846\u67b6\uff0c\u8fd9\u662f\u4e00\u4e2a\u8bad\u7ec3\u65e0\u5173\u7684Mamba\u6807\u8bb0\u7f29\u51cf\u65b9\u6cd5\u3002", "result": "MTR\u53ef\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u7ec4\u4ef6\u5e94\u7528\u4e8e\u5404\u79cdMamba\u6a21\u578b\uff0c\u5728Vim-B\u4e3b\u5e72\u4e0a\u51cf\u5c11\u7ea640%\u7684\u8ba1\u7b97\u91cf\uff0c\u540c\u65f6\u5728ImageNet\u4e0a\u7684\u6027\u80fd\u4ec5\u4e0b\u964d1.6%\uff0c\u4e14\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u3002", "conclusion": "MTR\u662f\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u8bad\u7ec3\u65e0\u5173\u6a21\u578bMamba\u7684\u7ed3\u6784\u611f\u77e5\u6807\u8bb0\u7f29\u51cf\u6846\u67b6\uff0c\u53ef\u6709\u6548\u51cf\u5c11\u8ba1\u7b97\u91cf\uff0c\u540c\u65f6\u5c06\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u964d\u81f3\u6700\u4f4e\u3002"}}
{"id": "2507.14050", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14050", "abs": "https://arxiv.org/abs/2507.14050", "authors": ["Mohamed Elkhayat", "Mohamed Mahmoud", "Jamil Fayyad", "Nourhan Bayasi"], "title": "Foundation Models as Class-Incremental Learners for Dermatological Image Classification", "comment": "Accepted at the MICCAI EMERGE 2025 workshop", "summary": "Class-Incremental Learning (CIL) aims to learn new classes over time without\nforgetting previously acquired knowledge. The emergence of foundation models\n(FM) pretrained on large datasets presents new opportunities for CIL by\noffering rich, transferable representations. However, their potential for\nenabling incremental learning in dermatology remains largely unexplored. In\nthis paper, we systematically evaluate frozen FMs pretrained on large-scale\nskin lesion datasets for CIL in dermatological disease classification. We\npropose a simple yet effective approach where the backbone remains frozen, and\na lightweight MLP is trained incrementally for each task. This setup achieves\nstate-of-the-art performance without forgetting, outperforming regularization,\nreplay, and architecture based methods. To further explore the capabilities of\nfrozen FMs, we examine zero training scenarios using nearest mean classifiers\nwith prototypes derived from their embeddings. Through extensive ablation\nstudies, we demonstrate that this prototype based variant can also achieve\ncompetitive results. Our findings highlight the strength of frozen FMs for\ncontinual learning in dermatology and support their broader adoption in real\nworld medical applications. Our code and datasets are available here.", "AI": {"tldr": "\u672c\u7814\u7a76\u8868\u660e\uff0c\u51bb\u7ed3\u7684\u3001\u5728\u5927\u578b\u76ae\u80a4\u75c5\u5b66\u6570\u636e\u96c6\u4e0a\u9884\u8bad\u7ec3\u7684\u57fa\u7840\u6a21\u578b\uff08FM\uff09\u975e\u5e38\u9002\u5408\u7528\u4e8e\u76ae\u80a4\u75c5\u5b66\u75be\u75c5\u5206\u7c7b\u7684\u6301\u7eed\u5b66\u4e60\uff08CIL\uff09\u3002\u901a\u8fc7\u4fdd\u6301FM\u51bb\u7ed3\u5e76\u8bad\u7ec3\u4e00\u4e2a\u7b80\u5355\u7684MLP\uff0c\u53ef\u4ee5\u5b9e\u73b0\u5353\u8d8a\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u4e0d\u4f1a\u9057\u5fd8\u5148\u524d\u5b66\u4e60\u7684\u77e5\u8bc6\u3002\u5373\u4f7f\u5728\u6ca1\u6709\u989d\u5916\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u5229\u7528FM\u7684\u5d4c\u5165\u8868\u793a\u4e5f\u53ef\u4ee5\u83b7\u5f97\u6709\u7ade\u4e89\u529b\u7684\u7ed3\u679c\u3002", "motivation": "\u65e8\u5728\u63a2\u7d22\u5927\u578b\u57fa\u7840\u6a21\u578b\uff08FM\uff09\u5728\u76ae\u80a4\u75c5\u5b66\u75be\u75c5\u5206\u7c7b\u7684\u6301\u7eed\u5b66\u4e60\uff08CIL\uff09\u4e2d\u7684\u6f5c\u529b\uff0c\u4ee5\u514b\u670d\u4f20\u7edfCIL\u65b9\u6cd5\u5728\u5904\u7406\u65b0\u7c7b\u522b\u65f6\u9057\u5fd8\u65e7\u7c7b\u522b\u77e5\u8bc6\u7684\u95ee\u9898\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u65b9\u6cd5\uff1a\u4fdd\u6301\u57fa\u7840\u6a21\u578b\uff08FM\uff09\u7684\u9aa8\u5e72\u7f51\u7edc\u51bb\u7ed3\uff0c\u5e76\u4e3a\u6bcf\u4e2a\u4efb\u52a1\u589e\u91cf\u8bad\u7ec3\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u591a\u5c42\u611f\u77e5\u673a\uff08MLP\uff09\u3002\u6b64\u5916\uff0c\u8fd8\u63a2\u7d22\u4e86\u5728\u96f6\u8bad\u7ec3\u573a\u666f\u4e0b\uff0c\u4f7f\u7528\u57fa\u4e8e\u539f\u578b\u7684\u6700\u8fd1\u90bb\u5206\u7c7b\u5668\uff0c\u4ece\u51bb\u7ed3FM\u7684\u5d4c\u5165\u4e2d\u63d0\u53d6\u539f\u578b\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4e0d\u9057\u5fd8\u5148\u524d\u77e5\u8bc6\u7684\u60c5\u51b5\u4e0b\uff0c\u5728CIL\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002\u57fa\u4e8e\u539f\u578b\u7684\u53d8\u4f53\u5728\u96f6\u8bad\u7ec3\u573a\u666f\u4e0b\u4e5f\u663e\u793a\u51fa\u6709\u7ade\u4e89\u529b\u7684\u7ed3\u679c\u3002", "conclusion": "\u672c\u7814\u7a76\u7cfb\u7edf\u5730\u8bc4\u4f30\u4e86\u5728\u5927\u578b\u76ae\u80a4\u75c5\u53d8\u6570\u636e\u96c6\u4e0a\u9884\u8bad\u7ec3\u7684\u51bb\u7ed3\u57fa\u7840\u6a21\u578b\uff08FM\uff09\u5728\u76ae\u80a4\u75c5\u5b66\u75be\u75c5\u5206\u7c7b\u4e2d\u7684\u6301\u7eed\u5b66\u4e60\uff08CIL\uff09\u80fd\u529b\u3002\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u5373\u4fdd\u6301\u9aa8\u5e72\u7f51\u7edc\u51bb\u7ed3\uff0c\u5e76\u4e3a\u6bcf\u4e2a\u4efb\u52a1\u589e\u91cf\u8bad\u7ec3\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u591a\u5c42\u611f\u77e5\u673a\uff08MLP\uff09\uff0c\u5728\u4e0d\u9057\u5fd8\u77e5\u8bc6\u7684\u60c5\u51b5\u4e0b\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4f18\u4e8e\u57fa\u4e8e\u6b63\u5219\u5316\u3001\u91cd\u653e\u548c\u67b6\u6784\u7684\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u63a2\u8ba8\u4e86\u4f7f\u7528\u57fa\u4e8e\u539f\u578b\u7684\u6700\u8fd1\u90bb\u5206\u7c7b\u5668\uff0c\u5728\u96f6\u8bad\u7ec3\u573a\u666f\u4e0b\u5229\u7528\u51bb\u7ed3FM\u7684\u5d4c\u5165\u8868\u793a\uff0c\u5e76\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u7ed3\u679c\u3002\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u51bb\u7ed3FM\u5728\u76ae\u80a4\u75c5\u5b66\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u652f\u6301\u5176\u5728\u5b9e\u9645\u533b\u7597\u5e94\u7528\u4e2d\u7684\u5e7f\u6cdb\u91c7\u7528\u3002"}}
{"id": "2507.14067", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14067", "abs": "https://arxiv.org/abs/2507.14067", "authors": ["Shuliang Liu", "Qi Zheng", "Jesse Jiaxi Xu", "Yibo Yan", "He Geng", "Aiwei Liu", "Peijie Jiang", "Jia Liu", "Yik-Cheung Tam", "Xuming Hu"], "title": "VLA-Mark: A cross modal watermark for large vision-language alignment model", "comment": null, "summary": "Vision-language models demand watermarking solutions that protect\nintellectual property without compromising multimodal coherence. Existing text\nwatermarking methods disrupt visual-textual alignment through biased token\nselection and static strategies, leaving semantic-critical concepts vulnerable.\nWe propose VLA-Mark, a vision-aligned framework that embeds detectable\nwatermarks while preserving semantic fidelity through cross-modal coordination.\nOur approach integrates multiscale visual-textual alignment metrics, combining\nlocalized patch affinity, global semantic coherence, and contextual attention\npatterns, to guide watermark injection without model retraining. An\nentropy-sensitive mechanism dynamically balances watermark strength and\nsemantic preservation, prioritizing visual grounding during low-uncertainty\ngeneration phases. Experiments show 7.4% lower PPL and 26.6% higher BLEU than\nconventional methods, with near-perfect detection (98.8% AUC). The framework\ndemonstrates 96.1\\% attack resilience against attacks such as paraphrasing and\nsynonym substitution, while maintaining text-visual consistency, establishing\nnew standards for quality-preserving multimodal watermarking", "AI": {"tldr": "VLA-Mark \u662f\u4e00\u79cd\u65b0\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6c34\u5370\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9\u5bf9\u9f50\u548c\u8de8\u6a21\u6001\u534f\u8c03\u6765\u4fdd\u62a4\u77e5\u8bc6\u4ea7\u6743\uff0c\u540c\u65f6\u4fdd\u6301\u8bed\u4e49\u4fdd\u771f\u5ea6\u3002\u5b83\u4f7f\u7528\u591a\u5c3a\u5ea6\u5bf9\u9f50\u6307\u6807\u548c\u71b5\u654f\u611f\u673a\u5236\u8fdb\u884c\u6c34\u5370\u6ce8\u5165\uff0c\u65e0\u9700\u6a21\u578b\u91cd\u65b0\u8bad\u7ec3\u3002\u5b9e\u9a8c\u8868\u660e\uff0cVLA-Mark \u5728\u63d0\u9ad8\u6587\u672c\u8d28\u91cf\u548c\u62b5\u6297\u653b\u51fb\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u6587\u672c\u6c34\u5370\u65b9\u6cd5\u4f1a\u901a\u8fc7\u6709\u504f\u89c1\u7684\u6807\u8bb0\u9009\u62e9\u548c\u9759\u6001\u7b56\u7565\u7834\u574f\u89c6\u89c9-\u6587\u672c\u5bf9\u9f50\uff0c\u4f7f\u5f97\u8bed\u4e49\u5173\u952e\u6982\u5ff5\u5bb9\u6613\u53d7\u5230\u635f\u5bb3\u3002\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u62a4\u77e5\u8bc6\u4ea7\u6743\u53c8\u4e0d\u635f\u5bb3\u591a\u6a21\u6001\u8fde\u8d2f\u6027\u7684\u6c34\u5370\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a VLA-Mark \u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6c34\u5370\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u591a\u5c3a\u5ea6\u89c6\u89c9-\u6587\u672c\u5bf9\u9f50\u6307\u6807\uff08\u5305\u62ec\u5c40\u90e8\u5757\u4eb2\u548c\u529b\u3001\u5168\u5c40\u8bed\u4e49\u8fde\u8d2f\u6027\u548c\u4e0a\u4e0b\u6587\u6ce8\u610f\u529b\u6a21\u5f0f\uff09\uff0c\u5e76\u7ed3\u5408\u71b5\u654f\u611f\u673a\u5236\u6765\u6307\u5bfc\u6c34\u5370\u6ce8\u5165\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cVLA-Mark \u7684\u56f0\u60d1\u5ea6\uff08PPL\uff09\u6bd4\u4f20\u7edf\u65b9\u6cd5\u4f4e 7.4%\uff0cBLEU \u5206\u6570\u9ad8 26.6%\uff0c\u68c0\u6d4b\u51c6\u786e\u7387\uff08AUC\uff09\u63a5\u8fd1\u5b8c\u7f8e\uff0898.8%\uff09\u3002\u8be5\u6846\u67b6\u5728\u9762\u5bf9\u91ca\u4e49\u548c\u540c\u4e49\u8bcd\u66ff\u6362\u7b49\u653b\u51fb\u65f6\uff0c\u8868\u73b0\u51fa 96.1% \u7684\u653b\u51fb\u97e7\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6587\u672c-\u89c6\u89c9\u4e00\u81f4\u6027\u3002", "conclusion": "VLA-Mark \u6846\u67b6\u5728\u4fdd\u62a4\u77e5\u8bc6\u4ea7\u6743\u7684\u540c\u65f6\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u534f\u8c03\u4fdd\u6301\u4e86\u8bed\u4e49\u4fdd\u771f\u5ea6\uff0c\u5e76\u5728\u6587\u672c-\u89c6\u89c9\u4e00\u81f4\u6027\u65b9\u9762\u8bbe\u7acb\u4e86\u65b0\u7684\u6807\u51c6\u3002"}}
{"id": "2507.14083", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14083", "abs": "https://arxiv.org/abs/2507.14083", "authors": ["Sara Abdulaziz", "Egor Bondarev"], "title": "Unmasking Performance Gaps: A Comparative Study of Human Anonymization and Its Effects on Video Anomaly Detection", "comment": "ACIVS 2025", "summary": "Advancements in deep learning have improved anomaly detection in surveillance\nvideos, yet they raise urgent privacy concerns due to the collection of\nsensitive human data. In this paper, we present a comprehensive analysis of\nanomaly detection performance under four human anonymization techniques,\nincluding blurring, masking, encryption, and avatar replacement, applied to the\nUCF-Crime dataset. We evaluate four anomaly detection methods, MGFN, UR-DMU,\nBN-WVAD, and PEL4VAD, on the anonymized UCF-Crime to reveal how each method\nresponds to different obfuscation techniques. Experimental results demonstrate\nthat anomaly detection remains viable under anonymized data and is dependent on\nthe algorithmic design and the learning strategy. For instance, under certain\nanonymization patterns, such as encryption and masking, some models\ninadvertently achieve higher AUC performance compared to raw data, due to the\nstrong responsiveness of their algorithmic components to these noise patterns.\nThese results highlight the algorithm-specific sensitivities to anonymization\nand emphasize the trade-off between preserving privacy and maintaining\ndetection utility. Furthermore, we compare these conventional anonymization\ntechniques with the emerging privacy-by-design solutions, highlighting an often\noverlooked trade-off between robust privacy protection and utility flexibility.\nThrough comprehensive experiments and analyses, this study provides a\ncompelling benchmark and insights into balancing human privacy with the demands\nof anomaly detection.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e86\u56db\u79cd\u533f\u540d\u5316\u6280\u672f\uff08\u6a21\u7cca\u5316\u3001\u906e\u853d\u3001\u52a0\u5bc6\u3001\u5934\u50cf\u66ff\u6362\uff09\u5bf9\u56db\u79cd\u5f02\u5e38\u68c0\u6d4b\u7b97\u6cd5\uff08MGFN\u3001UR-DMU\u3001BN-WVAD\u3001PEL4VAD\uff09\u5728UCF-Crime\u6570\u636e\u96c6\u4e0a\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5f02\u5e38\u68c0\u6d4b\u5728\u533f\u540d\u5316\u6570\u636e\u4e0b\u4ecd\u7136\u53ef\u884c\uff0c\u4f46\u6027\u80fd\u53d7\u7b97\u6cd5\u548c\u533f\u540d\u5316\u65b9\u5f0f\u5f71\u54cd\uff0c\u5e76\u6307\u51fa\u4e86\u9690\u79c1\u4fdd\u62a4\u4e0e\u68c0\u6d4b\u6548\u7528\u4e4b\u95f4\u7684\u6743\u8861\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u5728\u5b89\u9632\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u5f15\u53d1\u4e86\u5bf9\u654f\u611f\u4eba\u7c7b\u6570\u636e\u6536\u96c6\u7684\u9690\u79c1\u62c5\u5fe7\u3002", "method": "\u8bc4\u4f30\u4e86\u56db\u79cd\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff08MGFN\u3001UR-DMU\u3001BN-WVAD\u548cPEL4VAD\uff09\u5728\u7ecf\u8fc7\u6a21\u7cca\u5316\u3001\u906e\u853d\u3001\u52a0\u5bc6\u548c\u5934\u50cf\u66ff\u6362\u56db\u79cd\u533f\u540d\u5316\u6280\u672f\u5904\u7406\u540e\u7684UCF-Crime\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5f02\u5e38\u68c0\u6d4b\u5728\u533f\u540d\u5316\u6570\u636e\u4e0b\u4ecd\u7136\u53ef\u884c\uff0c\u5e76\u4e14\u6027\u80fd\u4e0e\u7b97\u6cd5\u8bbe\u8ba1\u548c\u5b66\u4e60\u7b56\u7565\u76f8\u5173\u3002\u67d0\u4e9b\u533f\u540d\u5316\u6a21\u5f0f\uff08\u5982\u52a0\u5bc6\u548c\u906e\u853d\uff09\u53cd\u800c\u53ef\u80fd\u63d0\u9ad8\u67d0\u4e9b\u6a21\u578b\u7684AUC\u6027\u80fd\u3002\u7814\u7a76\u5f3a\u8c03\u4e86\u7b97\u6cd5\u5bf9\u533f\u540d\u5316\u7684\u654f\u611f\u6027\u4ee5\u53ca\u9690\u79c1\u4fdd\u62a4\u4e0e\u68c0\u6d4b\u6548\u7528\u4e4b\u95f4\u7684\u6743\u8861\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u5bf9\u6bd4\u4e86\u4f20\u7edf\u533f\u540d\u5316\u6280\u672f\u4e0e\u65b0\u5174\u7684\u9690\u79c1\u8bbe\u8ba1\u65b9\u6848\u3002", "conclusion": "\u8be5\u7814\u7a76\u5bf9\u56db\u79cd\u4eba\u7c7b\u533f\u540d\u5316\u6280\u672f\uff08\u6a21\u7cca\u5316\u3001\u906e\u853d\u3001\u52a0\u5bc6\u548c\u5934\u50cf\u66ff\u6362\uff09\u5728\u5b89\u9632\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u8868\u73b0\u8fdb\u884c\u4e86\u5168\u9762\u5206\u6790\uff0c\u53d1\u73b0\u5728\u533f\u540d\u5316\u6570\u636e\u4e0b\u5f02\u5e38\u68c0\u6d4b\u4ecd\u7136\u53ef\u884c\uff0c\u4f46\u5176\u6027\u80fd\u53d6\u51b3\u4e8e\u7b97\u6cd5\u8bbe\u8ba1\u548c\u5b66\u4e60\u7b56\u7565\u3002\u67d0\u4e9b\u533f\u540d\u5316\u65b9\u6cd5\uff08\u5982\u52a0\u5bc6\u548c\u906e\u853d\uff09\u751a\u81f3\u53ef\u80fd\u63d0\u9ad8\u67d0\u4e9b\u6a21\u578b\u7684AUC\u6027\u80fd\u3002\u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u4fdd\u62a4\u9690\u79c1\u548c\u7ef4\u6301\u68c0\u6d4b\u6548\u7528\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u5e76\u5bf9\u6bd4\u4e86\u4f20\u7edf\u533f\u540d\u5316\u6280\u672f\u4e0e\u65b0\u5174\u7684\u9690\u79c1\u8bbe\u8ba1\u65b9\u6848\uff0c\u6307\u51fa\u4e86\u5728\u9690\u79c1\u4fdd\u62a4\u9c81\u68d2\u6027\u548c\u6548\u7528\u7075\u6d3b\u6027\u4e4b\u95f4\u5b58\u5728\u7684\u6743\u8861\u3002"}}
{"id": "2507.14093", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14093", "abs": "https://arxiv.org/abs/2507.14093", "authors": ["\u0160imon Kubov", "Simon Kl\u00ed\u010dn\u00edk", "Jakub Dand\u00e1r", "Zden\u011bk Straka", "Karol\u00edna Kvakov\u00e1", "Daniel Kvak"], "title": "Multi-Centre Validation of a Deep Learning Model for Scoliosis Assessment", "comment": null, "summary": "Scoliosis affects roughly 2 to 4 percent of adolescents, and treatment\ndecisions depend on precise Cobb angle measurement. Manual assessment is time\nconsuming and subject to inter observer variation. We conducted a\nretrospective, multi centre evaluation of a fully automated deep learning\nsoftware (Carebot AI Bones, Spine Measurement functionality; Carebot s.r.o.) on\n103 standing anteroposterior whole spine radiographs collected from ten\nhospitals. Two musculoskeletal radiologists independently measured each study\nand served as reference readers. Agreement between the AI and each radiologist\nwas assessed with Bland Altman analysis, mean absolute error (MAE), root mean\nsquared error (RMSE), Pearson correlation coefficient, and Cohen kappa for four\ngrade severity classification. Against Radiologist 1 the AI achieved an MAE of\n3.89 degrees (RMSE 4.77 degrees) with a bias of 0.70 degrees and limits of\nagreement from minus 8.59 to plus 9.99 degrees. Against Radiologist 2 the AI\nachieved an MAE of 3.90 degrees (RMSE 5.68 degrees) with a bias of 2.14 degrees\nand limits from minus 8.23 to plus 12.50 degrees. Pearson correlations were r\nequals 0.906 and r equals 0.880 (inter reader r equals 0.928), while Cohen\nkappa for severity grading reached 0.51 and 0.64 (inter reader kappa 0.59).\nThese results demonstrate that the proposed software reproduces expert level\nCobb angle measurements and categorical grading across multiple centres,\nsuggesting its utility for streamlining scoliosis reporting and triage in\nclinical workflows.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.14095", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14095", "abs": "https://arxiv.org/abs/2507.14095", "authors": ["Yung-Hong Sun", "Ting-Hung Lin", "Jiangang Chen", "Hongrui Jiang", "Yu Hen Hu"], "title": "C-DOG: Training-Free Multi-View Multi-Object Association in Dense Scenes Without Visual Feature via Connected \u03b4-Overlap Graphs", "comment": null, "summary": "Multi-view multi-object association is a fundamental step in 3D\nreconstruction pipelines, enabling consistent grouping of object instances\nacross multiple camera views. Existing methods often rely on appearance\nfeatures or geometric constraints such as epipolar consistency. However, these\napproaches can fail when objects are visually indistinguishable or observations\nare corrupted by noise. We propose C-DOG, a training-free framework that serves\nas an intermediate module bridging object detection (or pose estimation) and 3D\nreconstruction, without relying on visual features. It combines connected\ndelta-overlap graph modeling with epipolar geometry to robustly associate\ndetections across views. Each 2D observation is represented as a graph node,\nwith edges weighted by epipolar consistency. A delta-neighbor-overlap\nclustering step identifies strongly consistent groups while tolerating noise\nand partial connectivity. To further improve robustness, we incorporate\nInterquartile Range (IQR)-based filtering and a 3D back-projection error\ncriterion to eliminate inconsistent observations. Extensive experiments on\nsynthetic benchmarks demonstrate that C-DOG outperforms geometry-based\nbaselines and remains robust under challenging conditions, including high\nobject density, without visual features, and limited camera overlap, making it\nwell-suited for scalable 3D reconstruction in real-world scenarios.", "AI": {"tldr": "C-DOG \u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u53ef\u901a\u8fc7\u7ed3\u5408\u589e\u91cf\u91cd\u53e0\u56fe\u6a21\u578b\u548c\u5916\u6781\u51e0\u4f55\u6765\u8de8\u89c6\u56fe\u5173\u8054\u5bf9\u8c61\u68c0\u6d4b\uff0c\u4ee5\u5b9e\u73b0\u9c81\u68d2\u7684 3D \u91cd\u5efa\uff0c\u5373\u4f7f\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u6761\u4ef6\u4e0b\u4e5f\u662f\u5982\u6b64\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u89c6\u56fe\u591a\u5bf9\u8c61\u5173\u8054\u65b9\u6cd5\uff08\u901a\u5e38\u4f9d\u8d56\u4e8e\u5916\u89c2\u7279\u5f81\u6216\u5916\u6781\u4e00\u81f4\u6027\u7b49\u51e0\u4f55\u7ea6\u675f\uff09\u5728\u5bf9\u8c61\u89c6\u89c9\u4e0a\u65e0\u6cd5\u533a\u5206\u6216\u89c2\u6d4b\u503c\u88ab\u566a\u58f0\u7834\u574f\u65f6\u53ef\u80fd\u4f1a\u5931\u8d25\u3002\u63d0\u51fa\u4e86 C-DOG \u6846\u67b6\u6765\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\u3002", "method": "C-DOG\u6846\u67b6\u7ed3\u5408\u4e86\u589e\u91cf\u91cd\u53e0\u56fe\u6a21\u578b\u548c\u5916\u6781\u51e0\u4f55\u6765\u8de8\u89c6\u56fe\u5173\u8054\u68c0\u6d4b\u3002\u5b83\u4f7f\u7528\u57fa\u4e8e\u5916\u6781\u4e00\u81f4\u6027\u7684\u8fb9\u7f18\u6743\u91cd\u8868\u793a\u6bcf\u4e2a 2D \u89c2\u6d4b\u503c\u4f5c\u4e3a\u56fe\u8282\u70b9\u3002\u589e\u91cf\u90bb\u57df\u91cd\u53e0\u805a\u7c7b\u6b65\u9aa4\u8bc6\u522b\u5f3a\u4e00\u81f4\u6027\u7ec4\uff0c\u540c\u65f6\u5bb9\u5fcd\u566a\u58f0\u548c\u90e8\u5206\u8fde\u63a5\u3002\u6b64\u5916\uff0c\u8fd8\u901a\u8fc7\u7eb3\u5165\u57fa\u4e8e\u56db\u5206\u4f4d\u8ddd (IQR) \u7684\u8fc7\u6ee4\u548c 3D \u53cd\u5411\u6295\u5f71\u8bef\u5dee\u6807\u51c6\u6765\u6d88\u9664\u4e0d\u4e00\u81f4\u7684\u89c2\u6d4b\u503c\u3002", "result": "C-DOG \u5728\u5408\u6210\u57fa\u51c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u57fa\u4e8e\u51e0\u4f55\u7684\u57fa\u7ebf\u76f8\u6bd4\uff0cC-DOG \u8868\u73b0\u66f4\u597d\uff0c\u5e76\u4e14\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u6761\u4ef6\u4e0b\uff08\u4f8b\u5982\u9ad8\u5bf9\u8c61\u5bc6\u5ea6\u3001\u65e0\u89c6\u89c9\u7279\u5f81\u548c\u6709\u9650\u7684\u76f8\u673a\u91cd\u53e0\uff09\u4fdd\u6301\u9c81\u68d2\u6027\u3002", "conclusion": "C-DOG \u5728\u5177\u6709\u6311\u6218\u6027\u7684\u6761\u4ef6\u4e0b\uff08\u4f8b\u5982\u9ad8\u5bf9\u8c61\u5bc6\u5ea6\u3001\u65e0\u89c6\u89c9\u7279\u5f81\u548c\u6709\u9650\u7684\u76f8\u673a\u91cd\u53e0\uff09\u8868\u73b0\u4f18\u4e8e\u57fa\u4e8e\u51e0\u4f55\u7684\u57fa\u7ebf\uff0c\u5e76\u4e14\u5177\u6709\u9c81\u68d2\u6027\uff0c\u56e0\u6b64\u975e\u5e38\u9002\u5408\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u7684\u53ef\u6269\u5c55 3D \u91cd\u5efa\u3002"}}
{"id": "2507.14137", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14137", "abs": "https://arxiv.org/abs/2507.14137", "authors": ["Shashanka Venkataramanan", "Valentinos Pariza", "Mohammadreza Salehi", "Lukas Knobel", "Spyros Gidaris", "Elias Ramzi", "Andrei Bursuc", "Yuki M. Asano"], "title": "Franca: Nested Matryoshka Clustering for Scalable Visual Representation Learning", "comment": null, "summary": "We present Franca (pronounced Fran-ka): free one; the first fully open-source\n(data, code, weights) vision foundation model that matches and in many cases\nsurpasses the performance of state-of-the-art proprietary models, e.g., DINOv2,\nCLIP, SigLIPv2, etc. Our approach is grounded in a transparent training\npipeline inspired by Web-SSL and uses publicly available data: ImageNet-21K and\na subset of ReLAION-2B. Beyond model release, we tackle critical limitations in\nSSL clustering methods. While modern models rely on assigning image features to\nlarge codebooks via clustering algorithms like Sinkhorn-Knopp, they fail to\naccount for the inherent ambiguity in clustering semantics. To address this, we\nintroduce a parameter-efficient, multi-head clustering projector based on\nnested Matryoshka representations. This design progressively refines features\ninto increasingly fine-grained clusters without increasing the model size,\nenabling both performance and memory efficiency. Additionally, we propose a\nnovel positional disentanglement strategy that explicitly removes positional\nbiases from dense representations, thereby improving the encoding of semantic\ncontent. This leads to consistent gains on several downstream benchmarks,\ndemonstrating the utility of cleaner feature spaces. Our contributions\nestablish a new standard for transparent, high-performance vision models and\nopen a path toward more reproducible and generalizable foundation models for\nthe broader AI community. The code and model checkpoints are available at\nhttps://github.com/valeoai/Franca.", "AI": {"tldr": "Franca\u662f\u4e00\u4e2a\u5b8c\u5168\u5f00\u6e90\u7684\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff0c\u6027\u80fd\u8d85\u8d8a\u4e86\u73b0\u6709\u4e13\u6709\u6a21\u578b\uff0c\u5e76\u5f15\u5165\u4e86\u6539\u8fdb\u7684\u805a\u7c7b\u548c\u4f4d\u7f6e\u89e3\u7ea0\u7f20\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u6027\u80fd\u548c\u53ef\u590d\u73b0\u6027\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u4e13\u6709\u6a21\u578b\uff08\u5982DINOv2\u3001CLIP\u3001SigLIPv2\uff09\u5728\u6027\u80fd\u4e0a\u7684\u5c40\u9650\u6027\u4ee5\u53caSSL\u805a\u7c7b\u65b9\u6cd5\u5728\u5904\u7406\u8bed\u4e49\u6a21\u7cca\u6027\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u63d0\u51faFranca\u6a21\u578b\u3002", "method": "Franca\u91c7\u7528\u4e86\u4e00\u4e2a\u53d7Web-SSL\u542f\u53d1\u7684\u900f\u660e\u8bad\u7ec3\u6d41\u7a0b\uff0c\u5e76\u4f7f\u7528\u4e86\u516c\u5f00\u53ef\u7528\u7684\u6570\u636e\u96c6\uff08ImageNet-21K\u548cReLAION-2B\u7684\u4e00\u4e2a\u5b50\u96c6\uff09\u3002\u4e3a\u4e86\u89e3\u51b3SSL\u805a\u7c7b\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0cFranca\u5f15\u5165\u4e86\u4e00\u79cd\u53c2\u6570\u9ad8\u6548\u7684\u591a\u5934\u805a\u7c7b\u6295\u5f71\u5668\uff0c\u8be5\u6295\u5f71\u5668\u57fa\u4e8e\u5d4c\u5957\u7684Matryoshka\u8868\u793a\uff0c\u53ef\u4ee5\u9010\u6b65\u4f18\u5316\u7279\u5f81\u5230\u66f4\u7cbe\u7ec6\u7684\u805a\u7c7b\uff0c\u540c\u65f6\u4e0d\u589e\u52a0\u6a21\u578b\u5927\u5c0f\u3002\u6b64\u5916\uff0cFranca\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4f4d\u7f6e\u89e3\u7ea0\u7f20\u7b56\u7565\uff0c\u901a\u8fc7\u663e\u5f0f\u53bb\u9664\u7a20\u5bc6\u8868\u793a\u4e2d\u7684\u4f4d\u7f6e\u504f\u5dee\u6765\u6539\u8fdb\u8bed\u4e49\u5185\u5bb9\u7684\u7f16\u7801\u3002", "result": "Franca\u6a21\u578b\u5728\u6027\u80fd\u4e0a\u5339\u914d\u751a\u81f3\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u4e13\u6709\u6a21\u578b\uff0c\u5e76\u5728\u591a\u4e2a\u4e0b\u6e38\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6301\u7eed\u7684\u63d0\u5347\uff0c\u8bc1\u660e\u4e86\u66f4\u6e05\u6670\u7279\u5f81\u7a7a\u95f4\u6548\u7528\u3002", "conclusion": "Franca\u7684\u8d21\u732e\u5efa\u7acb\u4e86\u4e00\u4e2a\u65b0\u7684\u900f\u660e\u3001\u9ad8\u6027\u80fd\u89c6\u89c9\u6a21\u578b\u6807\u51c6\uff0c\u5e76\u4e3a\u6574\u4e2aAI\u793e\u533a\u5f00\u8f9f\u4e86\u66f4\u5177\u53ef\u590d\u73b0\u6027\u548c\u901a\u7528\u6027\u7684\u57fa\u7840\u6a21\u578b\u4e4b\u8def\u3002"}}
