<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 58]
- [cs.CL](#cs.CL) [Total: 77]
- [quant-ph](#quant-ph) [Total: 32]
- [cs.SI](#cs.SI) [Total: 1]
- [eess.SP](#eess.SP) [Total: 10]
- [cs.GT](#cs.GT) [Total: 2]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.RO](#cs.RO) [Total: 33]
- [cs.AI](#cs.AI) [Total: 26]
- [cs.LO](#cs.LO) [Total: 2]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 15]
- [cs.ET](#cs.ET) [Total: 1]
- [cs.AR](#cs.AR) [Total: 3]
- [cs.LG](#cs.LG) [Total: 91]
- [cs.DS](#cs.DS) [Total: 8]
- [cond-mat.mes-hall](#cond-mat.mes-hall) [Total: 11]
- [eess.SY](#eess.SY) [Total: 19]
- [cs.GR](#cs.GR) [Total: 3]
- [cs.NE](#cs.NE) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [FSFSplatter: Build Surface and Novel Views with Sparse-Views within 3min](https://arxiv.org/abs/2510.02691)
*Yibin Zhao,Yihan Pan,Jun Nan,Jianjun Yi*

Main category: cs.CV

TL;DR: FSFSplatter 通过端到端的高斯初始化、相机参数估计和几何增强的场景优化，实现了从稀疏图像的快速表面重建。


<details>
  <summary>Details</summary>
Motivation: 现有高斯绘制技术需要密集的、经过校准的视图，从稀疏图像重建时表面质量差且容易出现过拟合。FSFSplatter 旨在解决这个问题。

Method: FSFSplatter 采用大型 Transformer 编码多视图图像，并通过自拆分高斯头生成密集且几何一致的高斯场景初始化。它通过基于贡献的剪枝消除局部浮点，并通过利用深度和多视图特征监督以及可微分相机参数来减轻对有限视图的过拟合。

Result: FSFSplatter 在 DTU 和 Replica 数据集上表现优于当前最先进的方法。

Conclusion: FSFSplatter 能够从稀疏图像中进行快速的表面重建，并生成高质量的结果。

Abstract: Gaussian Splatting has become a leading reconstruction technique, known for
its high-quality novel view synthesis and detailed reconstruction. However,
most existing methods require dense, calibrated views. Reconstructing from free
sparse images often leads to poor surface due to limited overlap and
overfitting. We introduce FSFSplatter, a new approach for fast surface
reconstruction from free sparse images. Our method integrates end-to-end dense
Gaussian initialization, camera parameter estimation, and geometry-enhanced
scene optimization. Specifically, FSFSplatter employs a large Transformer to
encode multi-view images and generates a dense and geometrically consistent
Gaussian scene initialization via a self-splitting Gaussian head. It eliminates
local floaters through contribution-based pruning and mitigates overfitting to
limited views by leveraging depth and multi-view feature supervision with
differentiable camera parameters during rapid optimization. FSFSplatter
outperforms current state-of-the-art methods on widely used DTU and Replica.

</details>


### [2] [ReeMark: Reeb Graphs for Simulating Patterns of Life in Spatiotemporal Trajectories](https://arxiv.org/abs/2510.03152)
*Anantajit Subrahmanya,Chandrakanth Gudavalli,Connor Levenson,Umang Garg,B. S. Manjunath*

Main category: cs.CV

TL;DR: To simulate realistic human mobility patterns using a novel framework called Markovian Reeb Graphs, which preserves Patterns of Life (PoLs) learned from data.


<details>
  <summary>Details</summary>
Motivation: Accurately modeling human mobility is crucial for urban planning, epidemiology, and traffic management.

Method: Markovian Reeb Graphs combine individual- and population-level mobility structures within a probabilistic topological model to simulate spatiotemporal trajectories that preserve Patterns of Life (PoLs).

Result: Evaluations on the Urban Anomalies dataset demonstrated that the proposed method achieves strong fidelity, capturing both consistency and variability in daily life, while being data- and compute-efficient.

Conclusion: Markovian Reeb Graphs offer a scalable framework for trajectory simulation with broad applicability across diverse urban environments.

Abstract: Accurately modeling human mobility is critical for urban planning,
epidemiology, and traffic management. In this work, we introduce Markovian Reeb
Graphs, a novel framework for simulating spatiotemporal trajectories that
preserve Patterns of Life (PoLs) learned from baseline data. By combining
individual- and population-level mobility structures within a probabilistic
topological model, our approach generates realistic future trajectories that
capture both consistency and variability in daily life. Evaluations on the
Urban Anomalies dataset (Atlanta and Berlin subsets) using the Jensen-Shannon
Divergence (JSD) across population- and agent-level metrics demonstrate that
the proposed method achieves strong fidelity while remaining data- and
compute-efficient. These results position Markovian Reeb Graphs as a scalable
framework for trajectory simulation with broad applicability across diverse
urban environments.

</details>


### [3] [ROGR: Relightable 3D Objects using Generative Relighting](https://arxiv.org/abs/2510.03163)
*Jiapeng Tang,Matthew Lavine,Dor Verbin,Stephan J. Garbin,Matthias Nießner,Ricardo Martin Brualla,Pratul P. Srinivasan,Philipp Henzler*

Main category: cs.CV

TL;DR: ROGR是一种新的方法，可以根据多视角拍摄的物体图像，生成可重新照明的3D模型，它通过一个生成式重照明模型来模拟物体在新的环境光下的效果。


<details>
  <summary>Details</summary>
Motivation: 生成可重新照明的3D模型，模拟物体在新的环境光下的效果。

Method: 通过采样物体在多种光照环境下的外观，创建一个数据集来训练一个光照条件下的神经辐射场（NeRF），该NeRF可以输出物体在任何输入环境光下的外观。该光照条件下的NeRF采用新颖的双分支架构，分别编码通用的光照效果和镜面反射。

Result: 在TensoIR和Stanford-ORB数据集上进行了评估，在大多数指标上均优于现有技术水平，并在真实世界物体捕捉中进行了展示。

Conclusion: ROGR实现了在任意环境光图下进行高效的前馈重照明，无需进行每次光照优化的或光传输模拟。

Abstract: We introduce ROGR, a novel approach that reconstructs a relightable 3D model
of an object captured from multiple views, driven by a generative relighting
model that simulates the effects of placing the object under novel environment
illuminations. Our method samples the appearance of the object under multiple
lighting environments, creating a dataset that is used to train a
lighting-conditioned Neural Radiance Field (NeRF) that outputs the object's
appearance under any input environmental lighting. The lighting-conditioned
NeRF uses a novel dual-branch architecture to encode the general lighting
effects and specularities separately. The optimized lighting-conditioned NeRF
enables efficient feed-forward relighting under arbitrary environment maps
without requiring per-illumination optimization or light transport simulation.
We evaluate our approach on the established TensoIR and Stanford-ORB datasets,
where it improves upon the state-of-the-art on most metrics, and showcase our
approach on real-world object captures.

</details>


### [4] [Exploring OCR-augmented Generation for Bilingual VQA](https://arxiv.org/abs/2510.02543)
*JoonHo Lee,Sunho Park*

Main category: cs.CV

TL;DR: 该研究通过引入KLOCR（一个在1亿个实例上训练的OCR模型）来增强视觉语言模型（VLMs）在韩语和英语中的OCR能力，并发布了KOCRBench（一个韩语视觉问答数据集），实验证明OCR显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 为了增强视觉语言模型（VLMs）在处理图像内容时的能力，尤其是在多语言场景下，并为OCR增强生成任务提供支持。

Method: 1. 训练并发布KLOCR，一个包含1亿个实例的双语（韩语和英语）OCR基线模型，以增强VLMs的OCR能力。
2. 整理KOCRBench，一个用于韩语视觉问答（VQA）的新基准数据集。
3. 分析了不同的提示方法（prompting methods）对模型的影响。
4. 在开源和商业模型上进行了广泛的实验，评估OCR增强的效果。

Result: OCR提取的文本能够显著提升在开源和商业模型上的视觉问答性能。

Conclusion: OCR增强生成对于双语视觉问答是一个有前景的研究方向，并且KLOCR模型、代码和KOCRBench数据集已公开，以促进该领域的研究。

Abstract: We investigate OCR-augmented generation with Vision Language Models (VLMs),
exploring tasks in Korean and English toward multilingualism. To support
research in this domain, we train and release KLOCR, a strong bilingual OCR
baseline trained on 100M instances to augment VLMs with OCR ability. To
complement existing VQA benchmarks, we curate KOCRBench for Korean VQA, and
analyze different prompting methods. Extensive experiments show that
OCR-extracted text significantly boosts performance across open source and
commercial models. Our work offers new insights into OCR-augmented generation
for bilingual VQA. Model, code, and data are available at
https://github.com/JHLee0513/KLOCR.

</details>


### [5] [Oracle-RLAIF: An Improved Fine-Tuning Framework for Multi-modal Video Models through Reinforcement Learning from Ranking Feedback](https://arxiv.org/abs/2510.02561)
*Derek Shi,Ruben Glatt,Christine Klymko,Shubham Mohole,Hongjun Choi,Shashank Kushwaha,Sam Sakla,Felipe Leno da Silva*

Main category: cs.CV

TL;DR: Oracle-RLAIF是一个创新的框架，使用AI反馈进行强化学习，以提高视频语言模型（VLM）的性能，并通过使用排名器而不是评分模型来降低成本。


<details>
  <summary>Details</summary>
Motivation: 现有的视频语言模型（VLM）通常需要大量的监督微调（SFT）和来自人类偏好数据的强化学习（RL），但随着模型规模的增大，收集人类反馈的成本也越来越高。现有的AI反馈强化学习（RLAIF）框架依赖于昂贵且受限的、需要视频叙事进行训练的奖励模型。

Method: 提出Oracle-RLAIF框架，用一个通用的Oracle排名器取代训练好的奖励模型，该排名器对候选模型响应进行排名，而不是进行评分。同时，引入基于梯队反馈的$GRPO_{rank}$损失函数，该函数直接优化排序反馈，并具有感知排名的优势。

Result: Oracle-RLAIF在各种视频理解基准测试中，其性能始终优于使用现有微调方法的领先VLM。

Conclusion: Oracle-RLAIF为创建灵活且数据高效的框架铺平了道路，该框架可以使用基于排名的强化学习来调整大型多模态视频模型，而不是基于评分的强化学习。

Abstract: Recent advances in large video-language models (VLMs) rely on extensive
fine-tuning techniques that strengthen alignment between textual and visual
comprehension. Leading pipelines typically pair supervised fine-tuning (SFT)
with reinforcement learning from preference data to enhance video
comprehension. However, as VLMs scale in parameter size, so does the cost of
gathering enough human feedback. To make fine-tuning more cost-effective,
recent frameworks explore reinforcement learning with AI feedback (RLAIF),
which replace human preference with AI as a judge. Current RLAIF frameworks
rely on a specialized reward model trained with video narratives to create
calibrated scalar rewards -- an expensive and restrictive pipeline. We propose
Oracle-RLAIF, a novel framework that replaces the trained reward model with a
more general Oracle ranker which acts as a drop-in model ranking candidate
model responses rather than scoring them. Alongside Oracle-RLAIF, we introduce
$GRPO_{rank}$, a novel rank-based loss function based on Group Relative Policy
Optimization (GRPO) that directly optimizes ordinal feedback with rank-aware
advantages. Empirically, we demonstrate that Oracle-RLAIF consistently
outperforms leading VLMs using existing fine-tuning methods when evaluated
across various video comprehension benchmarks. Oracle-RLAIF paves the path to
creating flexible and data-efficient frameworks for aligning large multi-modal
video models with reinforcement learning from rank rather than score.

</details>


### [6] [PhysHMR: Learning Humanoid Control Policies from Vision for Physically Plausible Human Motion Reconstruction](https://arxiv.org/abs/2510.02566)
*Qiao Feng,Yiming Huang,Yufu Wang,Jiatao Gu,Lingjie Liu*

Main category: cs.CV

TL;DR: PhysHMR是一个统一的框架，直接学习人类控制的视觉到动作策略，以在物理模拟器中进行运动重建，该重建既有物理依据又与输入视频在视觉上对齐。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注基于运动学的人姿估计，但由于缺乏物理约束，常常导致不切实际的结果。虽然一些方法试图通过基于物理的后处理来解决这些伪影，但这种两阶段的设计会导致错误累积，最终限制了整体重建质量。

Method: PhysHMR 的一个关键组成部分是像素射线策略，它将 2D 关键点提升为 3D 空间射线并将其转换为全局空间。这些射线被纳入策略输入，在不依赖嘈杂的 3D 根预测的情况下提供鲁棒的全局姿势引导。这种软全局基础与来自预训练编码器的局部视觉特征相结合，使策略能够同时推理详细姿势和全局定位。为了克服强化学习的样本效率低下问题，研究人员还引入了一种蒸馏方案，将运动知识从 mocap 训练的专家转移到以视觉为条件的策略，然后使用物理驱动的强化学习奖励对其进行改进。

Result: 实验证明，PhysHMR 在各种场景中都能生成高保真、物理上合理的运动，在视觉准确性和物理现实性方面均优于先前的方法。

Conclusion: PhysHMR 是一个统一的框架，可以直接学习以物理为基础的模拟器中的人类控制的视觉到动作策略，从而实现既有物理依据又与输入视频在视觉上对齐的运动重建。

Abstract: Reconstructing physically plausible human motion from monocular videos
remains a challenging problem in computer vision and graphics. Existing methods
primarily focus on kinematics-based pose estimation, often leading to
unrealistic results due to the lack of physical constraints. To address such
artifacts, prior methods have typically relied on physics-based post-processing
following the initial kinematics-based motion estimation. However, this
two-stage design introduces error accumulation, ultimately limiting the overall
reconstruction quality. In this paper, we present PhysHMR, a unified framework
that directly learns a visual-to-action policy for humanoid control in a
physics-based simulator, enabling motion reconstruction that is both physically
grounded and visually aligned with the input video. A key component of our
approach is the pixel-as-ray strategy, which lifts 2D keypoints into 3D spatial
rays and transforms them into global space. These rays are incorporated as
policy inputs, providing robust global pose guidance without depending on noisy
3D root predictions. This soft global grounding, combined with local visual
features from a pretrained encoder, allows the policy to reason over both
detailed pose and global positioning. To overcome the sample inefficiency of
reinforcement learning, we further introduce a distillation scheme that
transfers motion knowledge from a mocap-trained expert to the
vision-conditioned policy, which is then refined using physically motivated
reinforcement learning rewards. Extensive experiments demonstrate that PhysHMR
produces high-fidelity, physically plausible motion across diverse scenarios,
outperforming prior approaches in both visual accuracy and physical realism.

</details>


### [7] [Unlocking the power of partnership: How humans and machines can work together to improve face recognition](https://arxiv.org/abs/2510.02570)
*P. Jonathon Phillips,Geraldine Jeckeln,Carina A. Hahn,Amy N. Yates,Peter C. Fontana,Alice J. O'Toole*

Main category: cs.CV

TL;DR: 当个体识别者（无论人类还是机器）的准确性越接近时，他们的协作就越能提高整体准确性。研究提出了“近端准确性原则”（PAR），并在此基础上定义了一个“临界融合区”，即使人类的准确性低于机器，融合双方的判断也能提升系统整体的准确性。研究还提出了一种“智能人机融合”方法，通过选择性地结合人类和机器的判断，在不牺牲准确性的前提下，比单纯依赖机器或融合所有判断（包括低准确性的人类）取得了更好的效果。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探索在人脸识别任务中，人类与机器的协作如何影响整体准确性，并确定在何种条件下，这种协作能够提升准确性，从而为智能应用AI提供指导。

Method: 研究者利用了专家和非专家人类识别者的脸部数据，以及机器的识别数据，比较了人-人协作和人-机协作的效益。他们提出了“近端准确性原则”（PAR），并基于此定义了一个“临界融合区”。此外，研究者还实现了一种“智能人机融合”方法，并使用图论分析了全人类协作系统的表现。

Result: 研究发现，当协作双方的基线准确性越接近时，协作带来的准确性提升越明显（遵循PAR）。“近端准确性原则”能够预测在不同准确性水平下（从无训练到广泛训练）的协作效益。研究识别出一个“临界融合区”，在此区域内，尽管人类的准确性低于机器，但融合双方的判断能提升系统整体准确性，且该区域比预期要大。通过“智能人机融合”，选择性地结合高准确性机器和有潜力的识别者，实现了比单独运行的机器或融合所有判断（包括低准确性者）更高的准确性。全人类协作系统的最高准确性与“智能人机融合”的平均表现相当，但后者更能减少低准确性识别者对系统整体准确性的负面影响。

Conclusion: 人类和机器在确保人脸识别准确性方面都扮演着有意义的角色。本研究为智能地使用AI进行人脸识别提供了实证依据和指导方针。

Abstract: Human review of consequential decisions by face recognition algorithms
creates a "collaborative" human-machine system. Individual differences between
people and machines, however, affect whether collaboration improves or degrades
accuracy in any given case. We establish the circumstances under which
combining human and machine face identification decisions improves accuracy.
Using data from expert and non-expert face identifiers, we examined the
benefits of human-human and human-machine collaborations. The benefits of
collaboration increased as the difference in baseline accuracy between
collaborators decreased-following the Proximal Accuracy Rule (PAR). This rule
predicted collaborative (fusion) benefit across a wide range of baseline
abilities, from people with no training to those with extensive training. Using
the PAR, we established a critical fusion zone, where humans are less accurate
than the machine, but fusing the two improves system accuracy. This zone was
surprisingly large. We implemented "intelligent human-machine fusion" by
selecting people with the potential to increase the accuracy of a
high-performing machine. Intelligent fusion was more accurate than the machine
operating alone and more accurate than combining all human and machine
judgments. The highest system-wide accuracy achievable with human-only
partnerships was found by graph theory. This fully human system approximated
the average performance achieved by intelligent human-machine collaboration.
However, intelligent human-machine collaboration more effectively minimized the
impact of low-performing humans on system-wide accuracy. The results
demonstrate a meaningful role for both humans and machines in assuring accurate
face identification. This study offers an evidence-based road map for the
intelligent use of AI in face identification.

</details>


### [8] [How Confident are Video Models? Empowering Video Models to Express their Uncertainty](https://arxiv.org/abs/2510.02571)
*Zhiting Mei,Ola Shorinwa,Anirudha Majumdar*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Generative video models demonstrate impressive text-to-video capabilities,
spurring widespread adoption in many real-world applications. However, like
large language models (LLMs), video generation models tend to hallucinate,
producing plausible videos even when they are factually wrong. Although
uncertainty quantification (UQ) of LLMs has been extensively studied in prior
work, no UQ method for video models exists, raising critical safety concerns.
To our knowledge, this paper represents the first work towards quantifying the
uncertainty of video models. We present a framework for uncertainty
quantification of generative video models, consisting of: (i) a metric for
evaluating the calibration of video models based on robust rank correlation
estimation with no stringent modeling assumptions; (ii) a black-box UQ method
for video models (termed S-QUBED), which leverages latent modeling to
rigorously decompose predictive uncertainty into its aleatoric and epistemic
components; and (iii) a UQ dataset to facilitate benchmarking calibration in
video models. By conditioning the generation task in the latent space, we
disentangle uncertainty arising due to vague task specifications from that
arising from lack of knowledge. Through extensive experiments on benchmark
video datasets, we demonstrate that S-QUBED computes calibrated total
uncertainty estimates that are negatively correlated with the task accuracy and
effectively computes the aleatoric and epistemic constituents.

</details>


### [9] [PEO: Training-Free Aesthetic Quality Enhancement in Pre-Trained Text-to-Image Diffusion Models with Prompt Embedding Optimization](https://arxiv.org/abs/2510.02599)
*Hovhannes Margaryan,Bo Wan,Tinne Tuytelaars*

Main category: cs.CV

TL;DR: 本文提出一种名为提示嵌入优化（PEO）的新方法，通过优化简单提示的文本嵌入来提高预训练文本到图像扩散模型的图像美学质量。该方法不进行训练，并且不依赖于特定的模型骨干。


<details>
  <summary>Details</summary>
Motivation: 为了提高预训练文本到图像扩散模型在处理简单提示时生成图像的美学质量。

Method: 采用一种由三部分组成的优化目标函数，该函数旨在提高生成图像的美学保真度，确保其与优化后的文本嵌入一致，并尽量减少与初始提示的差异，同时引入提示保留项来维持与初始提示的一致性。该方法不进行训练，并且可以应用于任何预训练的文本到图像扩散模型。

Result: 通过定量和定性评估，证明了该方法的效果，其性能在文本到图像生成和提示适配任务上优于或等同于现有最先进的方法。

Conclusion: PEO 是一种有效且通用的方法，可以显著提高文本到图像扩散模型在简单提示下生成图像的美学质量，且无需进行额外的训练或依赖特定模型。

Abstract: This paper introduces a novel approach to aesthetic quality improvement in
pre-trained text-to-image diffusion models when given a simple prompt. Our
method, dubbed Prompt Embedding Optimization (PEO), leverages a pre-trained
text-to-image diffusion model as a backbone and optimizes the text embedding of
a given simple and uncurated prompt to enhance the visual quality of the
generated image. We achieve this by a tripartite objective function that
improves the aesthetic fidelity of the generated image, ensures adherence to
the optimized text embedding, and minimal divergence from the initial prompt.
The latter is accomplished through a prompt preservation term. Additionally,
PEO is training-free and backbone-independent. Quantitative and qualitative
evaluations confirm the effectiveness of the proposed method, exceeding or
equating the performance of state-of-the-art text-to-image and prompt
adaptation methods.

</details>


### [10] [Ego-Exo 3D Hand Tracking in the Wild with a Mobile Multi-Camera Rig](https://arxiv.org/abs/2510.02601)
*Patrick Rim,Kun He,Kevin Harris,Braden Copple,Shangchen Han,Sizhe An,Ivan Shugurov,Tomas Hodan,He Wen,Xu Xie*

Main category: cs.CV

TL;DR: 现有的自我中心计算机视觉在非约束环境下对手部进行精确3D跟踪仍面临挑战，因为现有数据集大多在受控环境中捕获，限制了模型的泛化能力。为解决此问题，我们提出了一种新颖的、无需标记的、多摄像头的系统，用于在真实、非约束的条件下精确捕捉3D手部和物体。该系统结合了轻量级的背挂式捕捉装置、八个外视角摄像头以及用户佩戴的Quest 3头显（提供两个自我中心视角）。我们设计了一个自我-外视角跟踪流程，以生成精确的3D手部姿态真值，并对其质量进行了严格评估。通过收集包含同步多视角图像和精确3D手部姿态的标注数据集，我们证明了该方法能在环境真实性和3D标注准确性之间显著减少权衡。


<details>
  <summary>Details</summary>
Motivation: 现有的自我中心计算机视觉在非约束环境下对手部进行精确3D跟踪仍面临挑战，因为现有数据集大多在受控环境中捕获，限制了模型的泛化能力。

Method: 提出了一种新颖的、无需标记的、多摄像头的系统，用于在真实、非约束的条件下精确捕捉3D手部和物体。该系统结合了轻量级的背挂式捕捉装置、八个外视角摄像头以及用户佩戴的Quest 3头显（提供两个自我中心视角）。设计了一个自我-外视角跟踪流程，以生成精确的3D手部姿态真值，并对其质量进行了严格评估。

Result: 通过收集包含同步多视角图像和精确3D手部姿态的标注数据集，我们证明了该方法能在环境真实性和3D标注准确性之间显著减少权衡。

Conclusion: 该方法能在环境真实性和3D标注准确性之间显著减少权衡。

Abstract: Accurate 3D tracking of hands and their interactions with the world in
unconstrained settings remains a significant challenge for egocentric computer
vision. With few exceptions, existing datasets are predominantly captured in
controlled lab setups, limiting environmental diversity and model
generalization. To address this, we introduce a novel marker-less multi-camera
system designed to capture precise 3D hands and objects, which allows for
nearly unconstrained mobility in genuinely in-the-wild conditions. We combine a
lightweight, back-mounted capture rig with eight exocentric cameras, and a
user-worn Meta Quest 3 headset, which contributes two egocentric views. We
design an ego-exo tracking pipeline to generate accurate 3D hand pose ground
truth from this system, and rigorously evaluate its quality. By collecting an
annotated dataset featuring synchronized multi-view images and precise 3D hand
poses, we demonstrate the capability of our approach to significantly reduce
the trade-off between environmental realism and 3D annotation accuracy.

</details>


### [11] [Input-Aware Sparse Attention for Real-Time Co-Speech Video Generation](https://arxiv.org/abs/2510.02617)
*Beijia Lu,Ziyi Chen,Jing Xiao,Jun-Yan Zhu*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的视频蒸馏方法，通过引入人类姿态作为条件，实现了实时、高质量的语音驱动视频生成。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的语音驱动视频生成方法存在生成速度慢和视频质量有待提高的问题，限制了其在视频创作和虚拟代理等领域的实时应用。

Method: 提出了一种输入感知（input-aware）的视频蒸馏方法，利用输入的人类姿态信息来指导注意力机制和损失函数。具体包括：1. 输入感知稀疏注意力：利用输入姿态关键点之间的对应关系，引导注意力集中在说话人的面部、手部和上半身等相关区域，减少计算冗余，增强身体部位的时间连贯性。2. 输入感知蒸馏损失：通过改进的损失函数，提高嘴唇同步度和手部动作的真实感。

Result: 与现有的音频驱动和输入驱动方法相比，所提出的方法实现了实时性能，并提高了视频质量，尤其在嘴唇同步和手部动作方面表现更佳。

Conclusion: 通过结合输入感知稀疏注意力和蒸馏损失，该方法有效解决了现有扩散模型在语音驱动视频生成中的速度和质量瓶颈，实现了实时的、高质量的视频生成，并在大量实验中验证了其有效性。

Abstract: Diffusion models can synthesize realistic co-speech video from audio for
various applications, such as video creation and virtual agents. However,
existing diffusion-based methods are slow due to numerous denoising steps and
costly attention mechanisms, preventing real-time deployment. In this work, we
distill a many-step diffusion video model into a few-step student model.
Unfortunately, directly applying recent diffusion distillation methods degrades
video quality and falls short of real-time performance. To address these
issues, our new video distillation method leverages input human pose
conditioning for both attention and loss functions. We first propose using
accurate correspondence between input human pose keypoints to guide attention
to relevant regions, such as the speaker's face, hands, and upper body. This
input-aware sparse attention reduces redundant computations and strengthens
temporal correspondences of body parts, improving inference efficiency and
motion coherence. To further enhance visual quality, we introduce an
input-aware distillation loss that improves lip synchronization and hand motion
realism. By integrating our input-aware sparse attention and distillation loss,
our method achieves real-time performance with improved visual quality compared
to recent audio-driven and input-driven methods. We also conduct extensive
experiments showing the effectiveness of our algorithmic design choices.

</details>


### [12] [Deep Generative Continual Learning using Functional LoRA: FunLoRA](https://arxiv.org/abs/2510.02631)
*Victor Enescu,Hichem Sahbi*

Main category: cs.CV

TL;DR: FunLoRA是一种新颖的参数高效微调（PEFT）方法，通过使用秩1矩阵和函数来增强低秩适应（LoRA），解决了灾难性遗忘问题，并在生成模型上取得了优于现有技术的性能。


<details>
  <summary>Details</summary>
Motivation: 深度生成模型在文本和视觉应用中得到广泛应用，但灾难性遗忘现象阻碍了其持续适应新知识的能力。现有方法如在合成数据上重新训练模型，存在训练时间过长和长期性能下降的问题。

Method: 提出了一种名为FunLoRA的新型条件机制，基于低秩适应（LoRA），仅使用秩1矩阵，并通过精心选择的函数在功能上增加矩阵秩，以动态地条件化生成模型。

Result: FunLoRA能够避免灾难性遗忘，并且只需要在当前任务的数据上进行训练。在基于流匹配的模型上的实验表明，该方法在分类准确性、内存成本和采样时间方面均优于基于扩散模型的现有技术。

Conclusion: FunLoRA是一种有效的参数高效微调方法，能够解决深度生成模型的灾难性遗忘问题，并在各项指标上取得领先性能。

Abstract: Continual adaptation of deep generative models holds tremendous potential and
critical importance, given their rapid and expanding usage in text and vision
based applications. Incremental training, however, remains highly challenging
due to catastrophic forgetting phenomenon, which makes it difficult for neural
networks to effectively incorporate new knowledge. A common strategy consists
in retraining the generative model on its own synthetic data in order to
mitigate forgetting. Yet, such an approach faces two major limitations: (i) the
continually increasing training time eventually becomes intractable, and (ii)
reliance on synthetic data inevitably leads to long-term performance
degradation, since synthetic samples lack the richness of real training data.
In this paper, we attenuate these issues by designing a novel and more
expressive conditioning mechanism for generative models based on low rank
adaptation (LoRA), that exclusively employs rank 1 matrices, whose
reparametrized matrix rank is functionally increased using carefully selected
functions -- and dubbed functional LoRA: FunLoRA. Using this dynamic
conditioning, the generative model is guaranteed to avoid catastrophic
forgetting and needs only to be trained on data from the current task.
Extensive experiments using flow-matching based models trained from scratch,
showcase that our proposed parameter-efficient fine-tuning (PEFT) method
surpasses prior state-of-the-art results based on diffusion models, reaching
higher classification accuracy scores, while only requiring a fraction of the
memory cost and sampling time.

</details>


### [13] [Sequence-Preserving Dual-FoV Defense for Traffic Sign and Light Recognition in Autonomous Vehicles](https://arxiv.org/abs/2510.02642)
*Abhishek Joshi,Jahnavi Krishna Koda,Abhishek Phadke*

Main category: cs.CV

TL;DR: 本研究提出了一种用于交通信号灯和标志识别的鲁棒性框架，该框架考虑了时间连续性、多视场传感以及对数字和自然扰动的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前的交通信号灯和标志识别模型缺乏对时间连续性、多视场传感以及对数字和自然扰动的鲁棒性的考虑，而这些因素对自动驾驶汽车的导航和安全至关重要。

Method: 提出了一种基于多源数据集的双视场、序列保持鲁棒性框架，并结合了特征压缩、防御蒸馏、熵异常检测和序列时间投票等技术，构建了一个三层防御堆栈。

Result: 该框架在真实应用中实现了79.8mAP，将攻击成功率（ASR）降低到18.2%，并将高风险误分类降低到32%，优于YOLOv8、YOLOv9和BEVFormer。

Conclusion: 本研究提出的统一防御堆栈框架能够有效提高交通信号灯和标志识别的鲁棒性，降低误分类的风险，为自动驾驶汽车的安全导航提供了保障。

Abstract: Traffic light and sign recognition are key for Autonomous Vehicles (AVs)
because perception mistakes directly influence navigation and safety. In
addition to digital adversarial attacks, models are vulnerable to existing
perturbations (glare, rain, dirt, or graffiti), which could lead to dangerous
misclassifications. The current work lacks consideration of temporal
continuity, multistatic field-of-view (FoV) sensing, and robustness to both
digital and natural degradation. This study proposes a dual FoV,
sequence-preserving robustness framework for traffic lights and signs in the
USA based on a multi-source dataset built on aiMotive, Udacity, Waymo, and
self-recorded videos from the region of Texas. Mid and long-term sequences of
RGB images are temporally aligned for four operational design domains (ODDs):
highway, night, rainy, and urban. Over a series of experiments on a real-life
application of anomaly detection, this study outlines a unified three-layer
defense stack framework that incorporates feature squeezing, defensive
distillation, and entropy-based anomaly detection, as well as sequence-wise
temporal voting for further enhancement. The evaluation measures included
accuracy, attack success rate (ASR), risk-weighted misclassification severity,
and confidence stability. Physical transferability was confirmed using probes
for recapture. The results showed that the Unified Defense Stack achieved
79.8mAP and reduced the ASR to 18.2%, which is superior to YOLOv8, YOLOv9, and
BEVFormer, while reducing the high-risk misclassification to 32%.

</details>


### [14] [Smart-GRPO: Smartly Sampling Noise for Efficient RL of Flow-Matching Models](https://arxiv.org/abs/2510.02654)
*Benjamin Yu,Jackie Liu,Justin Cui*

Main category: cs.CV

TL;DR: Smart-GRPO是第一个用于流匹配模型中强化学习的噪声扰动优化方法，通过迭代搜索策略改进了图像质量和人类对齐。


<details>
  <summary>Details</summary>
Motivation: 流匹配模型在文本到图像生成方面表现优异，但其确定性使其不适合强化学习，而强化学习是提高图像质量和人类对齐的关键工具。以往引入随机性的方法效率低下且不稳定。

Method: Smart-GRPO采用迭代搜索策略，解码候选扰动，用奖励函数评估它们，并朝着更高奖励的区域优化噪声分布。

Result: Smart-GRPO在奖励优化和视觉质量方面优于基线方法。

Conclusion: Smart-GRPO为在流匹配框架中进行强化学习提供了一条实用的途径，弥合了高效训练与人类对齐生成之间的差距。

Abstract: Recent advancements in flow-matching have enabled high-quality text-to-image
generation. However, the deterministic nature of flow-matching models makes
them poorly suited for reinforcement learning, a key tool for improving image
quality and human alignment. Prior work has introduced stochasticity by
perturbing latents with random noise, but such perturbations are inefficient
and unstable. We propose Smart-GRPO, the first method to optimize noise
perturbations for reinforcement learning in flow-matching models. Smart-GRPO
employs an iterative search strategy that decodes candidate perturbations,
evaluates them with a reward function, and refines the noise distribution
toward higher-reward regions. Experiments demonstrate that Smart-GRPO improves
both reward optimization and visual quality compared to baseline methods. Our
results suggest a practical path toward reinforcement learning in flow-matching
frameworks, bridging the gap between efficient training and human-aligned
generation.

</details>


### [15] [MoGIC: Boosting Motion Generation via Intention Understanding and Visual Context](https://arxiv.org/abs/2510.02722)
*Junyu Shi,Yong Sun,Zhiyuan Zhang,Lijiang Liu,Zhengjie Zhang,Yuxin He,Qiang Nie*

Main category: cs.CV

TL;DR: MoGIC是一个统一的框架，通过整合意图建模和视觉先验来改进文本驱动的动作生成，提高了动作生成的准确性、个性和因果逻辑捕捉能力。


<details>
  <summary>Details</summary>
Motivation: 现有文本驱动的动作生成方法在捕捉动作执行的因果逻辑和人类意图方面存在局限性，并且缺乏视觉基础限制了精细的时空细节和个性化。

Method: MoGIC框架通过联合优化多模态条件下的动作生成和意图预测，利用混合注意力机制和自适应范围来实现条件token与动作子序列之间的有效局部对齐。此外，还创建了一个名为Mo440H的大规模动作数据集。

Result: MoGIC在HumanML3D数据集上FID降低了38.6%，在Mo440H数据集上FID降低了34.6%，在动作描述任务上超越了基于LLM的方法，并实现了意图预测和视觉条件生成。

Conclusion: MoGIC通过整合意图建模和视觉先验，显著提高了多模态动作合成的性能，并在可控动作合成和意图理解方面取得了进展。

Abstract: Existing text-driven motion generation methods often treat synthesis as a
bidirectional mapping between language and motion, but remain limited in
capturing the causal logic of action execution and the human intentions that
drive behavior. The absence of visual grounding further restricts precision and
personalization, as language alone cannot specify fine-grained spatiotemporal
details. We propose MoGIC, a unified framework that integrates intention
modeling and visual priors into multimodal motion synthesis. By jointly
optimizing multimodal-conditioned motion generation and intention prediction,
MoGIC uncovers latent human goals, leverages visual priors to enhance
generation, and exhibits versatile multimodal generative capability. We further
introduce a mixture-of-attention mechanism with adaptive scope to enable
effective local alignment between conditional tokens and motion subsequences.
To support this paradigm, we curate Mo440H, a 440-hour benchmark from 21
high-quality motion datasets. Experiments show that after finetuning, MoGIC
reduces FID by 38.6\% on HumanML3D and 34.6\% on Mo440H, surpasses LLM-based
methods in motion captioning with a lightweight text head, and further enables
intention prediction and vision-conditioned generation, advancing controllable
motion synthesis and intention understanding. The code is available at
https://github.com/JunyuShi02/MoGIC

</details>


### [16] [From Tokens to Nodes: Semantic-Guided Motion Control for Dynamic 3D Gaussian Splatting](https://arxiv.org/abs/2510.02732)
*Jianing Chen,Zehao Li,Yujun Cai,Hao Jiang,Shuqin Gao,Honglong Zhao,Tianlu Mao,Yucheng Zhang*

Main category: cs.CV

TL;DR: 动态3D重建框架通过运动自适应地分配稀疏控制点，并使用基于样条的轨迹参数化来提高重建质量和效率。


<details>
  <summary>Details</summary>
Motivation: 从单目视频进行动态3D重建在推断3D运动和处理计算密集型时变场景方面存在挑战。现有稀疏控制方法因纯粹基于几何分配控制点而存在静态冗余和动态不足的问题。

Method: 提出了一种运动自适应框架，利用视觉基础模型的语义和运动先验，建立块-令牌-节点对应关系，并通过迭代体素化和运动趋势评分实现灵活的表示密度自适应。引入基于样条的轨迹参数化来捕捉时间演变，替代基于MLP变形场，以获得更平滑的运动表示和更稳定的优化。

Result: 在重建质量和效率方面，该方法相比现有的最先进方法有了显著的改进。

Conclusion: 所提出的运动自适应框架通过将控制点密度与运动复杂度相匹配，并采用基于样条的轨迹表示，有效解决了动态3D重建中的挑战，并在实验中证明了其优越性。

Abstract: Dynamic 3D reconstruction from monocular videos remains difficult due to the
ambiguity inferring 3D motion from limited views and computational demands of
modeling temporally varying scenes. While recent sparse control methods
alleviate computation by reducing millions of Gaussians to thousands of control
points, they suffer from a critical limitation: they allocate points purely by
geometry, leading to static redundancy and dynamic insufficiency. We propose a
motion-adaptive framework that aligns control density with motion complexity.
Leveraging semantic and motion priors from vision foundation models, we
establish patch-token-node correspondences and apply motion-adaptive
compression to concentrate control points in dynamic regions while suppressing
redundancy in static backgrounds. Our approach achieves flexible
representational density adaptation through iterative voxelization and motion
tendency scoring, directly addressing the fundamental mismatch between control
point allocation and motion complexity. To capture temporal evolution, we
introduce spline-based trajectory parameterization initialized by 2D tracklets,
replacing MLP-based deformation fields to achieve smoother motion
representation and more stable optimization. Extensive experiments demonstrate
significant improvements in reconstruction quality and efficiency over existing
state-of-the-art methods.

</details>


### [17] [Net2Net: When Un-trained Meets Pre-trained Networks for Robust Real-World Denoising](https://arxiv.org/abs/2510.02733)
*Weimin Yuan,Cai Meng*

Main category: cs.CV

TL;DR: Net2Net结合了无监督的DIP和有监督的DRUNet，通过RED技术，在标签数据有限的情况下实现了优于传统和纯深度学习方法的图像去噪效果。


<details>
  <summary>Details</summary>
Motivation: 传统去噪方法依赖手工先验，在真实复杂噪声下表现不佳；纯深度学习方法需要大量标签数据且泛化性有限。本研究旨在结合两者的优点，解决真实世界噪声去噪的挑战。

Method: 提出Net2Net方法，结合无监督的深度图像先验（DIP）和有监督的预训练去噪网络（DRUNet），并使用去噪正则化（RED）技术。无监督网络适应单一图像噪声特性，预训练网络利用大规模数据集的先验知识。

Result: 在基准数据集上的广泛实验表明，Net2Net方法在真实世界噪声去噪方面优于现有方法，尤其是在数据有限的情况下，提高了泛化性和性能。

Conclusion: Net2Net通过结合无监督和有监督学习的优势，并利用RED技术，为真实世界图像去噪提供了一种有效且泛化能力强的解决方案。

Abstract: Traditional denoising methods for noise removal have largely relied on
handcrafted priors, often perform well in controlled environments but struggle
to address the complexity and variability of real noise. In contrast, deep
learning-based approaches have gained prominence for learning noise
characteristics from large datasets, but these methods frequently require
extensive labeled data and may not generalize effectively across diverse noise
types and imaging conditions. In this paper, we present an innovative method,
termed as Net2Net, that combines the strengths of untrained and pre-trained
networks to tackle the challenges of real-world noise removal. The innovation
of Net2Net lies in its combination of unsupervised DIP and supervised
pre-trained model DRUNet by regularization by denoising (RED). The untrained
network adapts to the unique noise characteristics of each input image without
requiring labeled data, while the pre-trained network leverages learned
representations from large-scale datasets to deliver robust denoising
performance. This hybrid framework enhances generalization across varying noise
patterns and improves performance, particularly in scenarios with limited
training data. Extensive experiments on benchmark datasets demonstrate the
superiority of our method for real-world noise removal.

</details>


### [18] [Retrv-R1: A Reasoning-Driven MLLM Framework for Universal and Efficient Multimodal Retrieval](https://arxiv.org/abs/2510.02745)
*Lanyun Zhu,Deyi Ji,Tianrun Chen,Haiyang Wu,Shiqi Wang*

Main category: cs.CV

TL;DR: Retrv-R1是首个为多模态通用检索设计的R1风格MLLM，通过引入信息压缩模块、细节检查机制和新颖的训练范式（包括检索定制的合成CoT数据集和课程奖励的RL），解决了DeepSeek-R1直接应用于检索任务时的高计算成本和训练不稳定的问题，最终在多个基准测试和任务中实现了SOTA性能、高效率和强大的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 直接将DeepSeek-R1的方法应用于检索任务是不可行的，因为（1）多轮推理过程中的大量token消耗导致计算成本高昂；（2）直接将RL应用于检索任务的训练不稳定且效果不佳。

Method: Retrv-R1引入了一个信息压缩模块，并带有细节检查机制，通过减少token数量来提高计算效率，同时保留对难例至关重要的信息。此外，提出了一种新的训练范式，包括使用检索定制的合成CoT数据集进行激活阶段，以及使用新颖的课程奖励进行RL训练，以提高性能和效率。

Result: Retrv-R1在多个基准测试和任务中取得了SOTA性能、高效率和强大的泛化能力。

Conclusion: Retrv-R1通过创新的信息压缩、细节检查和训练范式，成功解决了现有方法的局限性，实现了多模态通用检索的最优性能。

Abstract: The success of DeepSeek-R1 demonstrates the immense potential of using
reinforcement learning (RL) to enhance LLMs' reasoning capabilities. This paper
introduces Retrv-R1, the first R1-style MLLM specifically designed for
multimodal universal retrieval, achieving higher performance by employing
step-by-step reasoning to produce more accurate retrieval results. We find that
directly applying the methods of DeepSeek-R1 to retrieval tasks is not
feasible, mainly due to (1) the high computational cost caused by the large
token consumption required for multiple candidates with reasoning processes,
and (2) the instability and suboptimal results when directly applying RL to
train for retrieval tasks. To address these issues, Retrv-R1 introduces an
information compression module with a details inspection mechanism, which
enhances computational efficiency by reducing the number of tokens while
ensuring that critical information for challenging candidates is preserved.
Furthermore, a new training paradigm is proposed, including an activation stage
using a retrieval-tailored synthetic CoT dataset for more effective
optimization, followed by RL with a novel curriculum reward to improve both
performance and efficiency. Incorporating these novel designs, Retrv-R1
achieves SOTA performance, high efficiency, and strong generalization ability,
as demonstrated by experiments across multiple benchmarks and tasks.

</details>


### [19] [Bayesian Test-time Adaptation for Object Recognition and Detection with Vision-language Models](https://arxiv.org/abs/2510.02750)
*Lihua Zhou,Mao Ye,Shuaifeng Li,Nianxin Li,Jinlin Wu,Xiatian Zhu,Lei Deng,Hongbin Liu,Jiebo Luo,Zhen Lei*

Main category: cs.CV

TL;DR: BCA+是一种用于目标识别和检测的无训练测试时自适应（TTA）框架，它通过动态缓存和贝叶斯推理来更新类嵌入、空间尺度和类先验，从而实现高效的TTA。


<details>
  <summary>Details</summary>
Motivation: 现有TTA方法在处理分布偏移时存在计算成本高或忽略先验信息的问题，需要一种更有效的TTA方法来同时处理目标识别和检测。

Method: BCA+通过引入一个动态缓存来存储和更新类嵌入、空间尺度和自适应类先验。它将适应过程视为一个贝叶斯推理问题，融合了初始VLM输出和基于缓存的预测（结合了动态更新的似然和先验）。

Result: BCA+在识别和检测基准测试中都取得了最先进的性能。

Conclusion: BCA+是一种高效、无训练的TTA框架，通过贝叶斯推理和动态缓存，能够同时改进目标识别和检测的性能，有效解决了分布偏移问题。

Abstract: Vision-language models (VLMs) such as CLIP and Grounding DINO have achieved
remarkable success in object recognition and detection. However, their
performance often degrades under real-world distribution shifts. Test-time
adaptation (TTA) aims to mitigate this issue by adapting models during
inference. Existing methods either rely on computationally expensive
backpropagation, which hinders real-time deployment, or focus solely on
likelihood adaptation, which overlooks the critical role of the prior. Our
prior work, Bayesian Class Adaptation (BCA), addressed these shortcomings for
object recognition by introducing a training-free framework that incorporates
adaptive priors. Building upon this foundation, we now present Bayesian Class
Adaptation plus (BCA+), a unified, training-free framework for TTA for both
object recognition and detection. BCA+ introduces a dynamic cache that
adaptively stores and updates class embeddings, spatial scales (for detection),
and, crucially, adaptive class priors derived from historical predictions. We
formulate adaptation as a Bayesian inference problem, where final predictions
are generated by fusing the initial VLM output with a cache-based prediction.
This cache-based prediction combines a dynamically updated likelihood
(measuring feature and scale similarity) and a prior (reflecting the evolving
class distribution). This dual-adaptation mechanism, coupled with
uncertainty-guided fusion, enables BCA+ to correct both the model's semantic
understanding and its contextual confidence. As a training-free method
requiring no backpropagation, BCA+ is highly efficient. Extensive experiments
demonstrate that BCA+ achieves state-of-the-art performance on both recognition
and detection benchmarks.

</details>


### [20] [Hierarchical Generalized Category Discovery for Brain Tumor Classification in Digital Pathology](https://arxiv.org/abs/2510.02760)
*Matthias Perkonigg,Patrick Rockenschaub,Georg Göbel,Adelheid Wöhrer*

Main category: cs.CV

TL;DR: 该研究提出了一种名为 HGCD-BT 的新方法，用于解决脑肿瘤分类中的未知类别问题，通过结合分层聚类和对比学习，在识别新肿瘤类别方面取得了显著的准确性提升。


<details>
  <summary>Details</summary>
Motivation: 现有的脑肿瘤分类方法无法识别训练集中未出现的肿瘤类别。无监督学习缺乏利用标记数据的能力，而半监督学习假设标记数据中包含了所有潜在类别。HGCD-BT 旨在解决这些限制，对未知类别进行分类。

Method: HGCD-BT 结合了分层聚类和对比学习。该方法通过引入一种新的半监督分层聚类损失来扩展基于对比学习的 GCD。

Result: 在 OpenSRH 数据集上，HGCD-BT 在块级分类方面比最先进的 GCD 方法提高了 +28% 的准确率，尤其在识别以前未见的肿瘤类别方面表现出色。在 Digital Brain Tumor Atlas 数据集上，HGCD-BT 在整个幻灯片图像的分类中也显示了其通用性。

Conclusion: HGCD-BT 是一种有效的方法，可以对脑肿瘤进行分类，即使在存在未知类别的情况下也能识别出以前未见的肿瘤类别。该方法在不同成像模态下都具有通用性。

Abstract: Accurate brain tumor classification is critical for intra-operative decision
making in neuro-oncological surgery. However, existing approaches are
restricted to a fixed set of predefined classes and are therefore unable to
capture patterns of tumor types not available during training. Unsupervised
learning can extract general-purpose features, but it lacks the ability to
incorporate prior knowledge from labelled data, and semi-supervised methods
often assume that all potential classes are represented in the labelled data.
Generalized Category Discovery (GCD) aims to bridge this gap by categorizing
both known and unknown classes within unlabelled data. To reflect the
hierarchical structure of brain tumor taxonomies, in this work, we introduce
Hierarchical Generalized Category Discovery for Brain Tumor Classification
(HGCD-BT), a novel approach that integrates hierarchical clustering with
contrastive learning. Our method extends contrastive learning based GCD by
incorporating a novel semi-supervised hierarchical clustering loss. We evaluate
HGCD-BT on OpenSRH, a dataset of stimulated Raman histology brain tumor images,
achieving a +28% improvement in accuracy over state-of-the-art GCD methods for
patch-level classification, particularly in identifying previously unseen tumor
categories. Furthermore, we demonstrate the generalizability of HGCD-BT on
slide-level classification of hematoxylin and eosin stained whole-slide images
from the Digital Brain Tumor Atlas, confirming its utility across imaging
modalities.

</details>


### [21] [AdaRD-key: Adaptive Relevance-Diversity Keyframe Sampling for Long-form Video understanding](https://arxiv.org/abs/2510.02778)
*Xian Zhang,Zexi Wu,Zinuo Li,Hongming Xu,Luqi Gong,Farid Boussaid,Naoufel Werghi,Mohammed Bennamoun*

Main category: cs.CV

TL;DR: AdaRD-Key 是一种训练免费的关键帧采样模块，用于驱动型长视频理解。它通过最大化统一的相关性--多样性最大体积（RD-MV）目标来选择信息丰富且不冗余的帧，该目标结合了查询条件的相关性分数和对数行列式多样性组件。为了处理与视频对齐较弱的宽泛查询，AdaRD-Key 采用了一个轻量级的相关性感知门控机制，在相关性分布表明对齐较弱时，该方法会无缝切换到仅多样性的模式，从而在没有额外监督的情况下增强覆盖范围。该流程是训练免费的，计算效率高（可在单个 GPU 上实时运行），并且可以以即插即用的方式与现有的 VLM 兼容。


<details>
  <summary>Details</summary>
Motivation: 长视频理解对视觉-语言模型（VLMs）来说仍然是一个重大挑战，因为其时间长度长且信息密度高。大多数当前的 MLLMs 依赖于均匀采样，这往往会忽略关键时刻，导致对查询的响应不正确。虽然现有的关键帧选择方法在限制重叠方面很有效，但它们经常会错过重要事件附近短暂的、细粒度的线索。其他方法则强调视觉多样性，但忽略了查询相关性。

Method: AdaRD-Key 通过最大化统一的相关性--多样性最大体积（RD-MV）目标来选择信息丰富且不冗余的帧。该目标结合了查询条件的相关性分数和对数行列式多样性组件。为了处理与视频对齐较弱的宽泛查询，AdaRD-Key 采用了一个轻量级的相关性感知门控机制，在相关性分布表明对齐较弱时，该方法会无缝切换到仅多样性的模式，从而在没有额外监督的情况下增强覆盖范围。

Result: 在 LongVideoBench 和 Video-MME 上的广泛实验表明，AdaRD-Key 具有最先进的性能，尤其是在长视频方面。

Conclusion: AdaRD-Key 是一种有效的、训练免费的关键帧采样方法，可以提高长视频理解的性能，并且可以轻松地集成到现有的 VLM 中。

Abstract: Understanding long-form videos remains a significant challenge for
vision--language models (VLMs) due to their extensive temporal length and high
information density. Most current multimodal large language models (MLLMs) rely
on uniform sampling, which often overlooks critical moments, leading to
incorrect responses to queries. In parallel, many keyframe selection approaches
impose rigid temporal spacing: once a frame is chosen, an exclusion window
suppresses adjacent timestamps to reduce redundancy. While effective at
limiting overlap, this strategy frequently misses short, fine-grained cues near
important events. Other methods instead emphasize visual diversity but neglect
query relevance. We propose AdaRD-Key, a training-free keyframe sampling module
for query-driven long-form video understanding. AdaRD-Key maximizes a unified
Relevance--Diversity Max-Volume (RD-MV) objective, combining a
query-conditioned relevance score with a log-determinant diversity component to
yield informative yet non-redundant frames. To handle broad queries with weak
alignment to the video, AdaRD-Key employs a lightweight relevance-aware gating
mechanism; when the relevance distribution indicates weak alignment, the method
seamlessly shifts into a diversity-only mode, enhancing coverage without
additional supervision. Our pipeline is training-free, computationally
efficient (running in real time on a single GPU), and compatible with existing
VLMs in a plug-and-play manner. Extensive experiments on LongVideoBench and
Video-MME demonstrate state-of-the-art performance, particularly on long-form
videos. Code available at https://github.com/Xian867/AdaRD-Key.

</details>


### [22] [Reasoning Riddles: How Explainability Reveals Cognitive Limits in Vision-Language Models](https://arxiv.org/abs/2510.02780)
*Prahitha Movva*

Main category: cs.CV

TL;DR: Vision-Language Models (VLMs)在 rebus 谜题解决方面存在不足，本研究通过分析其认知过程来解决这一问题。


<details>
  <summary>Details</summary>
Motivation: 分析 VLMs 在解决 rebus 谜题时潜在的推理过程和失败模式，以弥补当前研究的不足。

Method: 构建了一个包含 221 个 rebus 谜题的标注数据集，涵盖六个认知类别，并设计了一个评估框架来区分推理质量和答案正确性。研究了三种不同的提示策略，以探究 VLM 的认知过程。

Result: 研究发现 VLM 在视觉组合方面表现出系统性的优势，但在缺失解释和文化象征方面存在根本性局限。提示策略显著影响了认知方法和问题解决的有效性。

Conclusion: 推理质量因谜题类别而异，提示策略对模型性能有重要影响，表明可解释性是模型性能的内在组成部分。

Abstract: Vision-Language Models (VLMs) excel at many multimodal tasks, yet their
cognitive processes remain opaque on complex lateral thinking challenges like
rebus puzzles. While recent work has demonstrated these models struggle
significantly with rebus puzzle solving, the underlying reasoning processes and
failure patterns remain largely unexplored. We address this gap through a
comprehensive explainability analysis that moves beyond performance metrics to
understand how VLMs approach these complex lateral thinking challenges. Our
study contributes a systematically annotated dataset of 221 rebus puzzles
across six cognitive categories, paired with an evaluation framework that
separates reasoning quality from answer correctness. We investigate three
prompting strategies designed to elicit different types of explanatory
processes and reveal critical insights into VLM cognitive processes. Our
findings demonstrate that reasoning quality varies dramatically across puzzle
categories, with models showing systematic strengths in visual composition
while exhibiting fundamental limitations in absence interpretation and cultural
symbolism. We also discover that prompting strategy substantially influences
both cognitive approach and problem-solving effectiveness, establishing
explainability as an integral component of model performance rather than a
post-hoc consideration.

</details>


### [23] [OTR: Synthesizing Overlay Text Dataset for Text Removal](https://arxiv.org/abs/2510.02787)
*Jan Zdenek,Wataru Shimoda,Kota Yamaguchi*

Main category: cs.CV

TL;DR: 现有文本移除研究主要集中在自然场景文本移除，但现有数据集存在手动编辑痕迹、背景过于简单和评估指标不佳等问题，阻碍了模型的泛化和评估。为解决这些问题，本文提出了一种合成文本移除基准的方法，适用于场景文本以外的领域。该数据集在复杂背景上渲染文本，采用对象感知放置和视觉-语言模型生成的内容，确保了干净的真值和更具挑战性的文本移除场景。


<details>
  <summary>Details</summary>
Motivation: 现有文本移除研究主要集中在自然场景文本移除，但现有数据集存在手动编辑痕迹、背景过于简单和评估指标不佳等问题，阻碍了模型的泛化和评估。

Method: 提出一种合成文本移除基准的方法，该数据集在复杂背景上渲染文本，采用对象感知放置和视觉-语言模型生成的内容。

Result: 该数据集确保了干净的真值和更具挑战性的文本移除场景，适用于场景文本以外的领域。

Conclusion: 为解决现有文本移除数据集的局限性，提出了一种新的合成数据集构建方法，并提供了相应的基准。

Abstract: Text removal is a crucial task in computer vision with applications such as
privacy preservation, image editing, and media reuse. While existing research
has primarily focused on scene text removal in natural images, limitations in
current datasets hinder out-of-domain generalization or accurate evaluation. In
particular, widely used benchmarks such as SCUT-EnsText suffer from ground
truth artifacts due to manual editing, overly simplistic text backgrounds, and
evaluation metrics that do not capture the quality of generated results. To
address these issues, we introduce an approach to synthesizing a text removal
benchmark applicable to domains other than scene texts. Our dataset features
text rendered on complex backgrounds using object-aware placement and
vision-language model-generated content, ensuring clean ground truth and
challenging text removal scenarios. The dataset is available at
https://huggingface.co/datasets/cyberagent/OTR .

</details>


### [24] [Align Your Query: Representation Alignment for Multimodality Medical Object Detection](https://arxiv.org/abs/2510.02789)
*Ara Seo,Bryan Sangwoo Kim,Hyungjin Chung,Jong Chul Ye*

Main category: cs.CV

TL;DR: 该研究提出了一种名为 MoCA 和 QueryREPA 的框架，用于解决在混合医学模态（如 CXR、CT、MRI）上训练单一目标检测器时遇到的挑战。


<details>
  <summary>Details</summary>
Motivation: 单一检测器在混合医学模态上训练时，由于统计异质性和表示空间不匹配，导致医学目标检测性能下降。

Method: 提出了一种名为 MoCA 的框架，通过定义模态标记（modality tokens）并利用多模态上下文注意力（MoCA）机制，将模态信息融入 DETR 风格的目标查询中。此外，还引入了 QueryREPA 预训练阶段，使用对比学习任务对齐查询表示和模态标记。

Result: 在多种模态的混合训练下，该方法能够持续提升平均精度（AP），并且只引入了极小的额外开销，无需修改现有模型架构。

Conclusion: MoCA 和 QueryREPA 框架能够有效地生成模态感知、类别忠实的查询，从而实现对多种医学模态的鲁棒目标检测。

Abstract: Medical object detection suffers when a single detector is trained on mixed
medical modalities (e.g., CXR, CT, MRI) due to heterogeneous statistics and
disjoint representation spaces. To address this challenge, we turn to
representation alignment, an approach that has proven effective for bringing
features from different sources into a shared space. Specifically, we target
the representations of DETR-style object queries and propose a simple,
detector-agnostic framework to align them with modality context. First, we
define modality tokens: compact, text-derived embeddings encoding imaging
modality that are lightweight and require no extra annotations. We integrate
the modality tokens into the detection process via Multimodality Context
Attention (MoCA), mixing object-query representations via self-attention to
propagate modality context within the query set. This preserves DETR-style
architectures and adds negligible latency while injecting modality cues into
object queries. We further introduce QueryREPA, a short pretraining stage that
aligns query representations to their modality tokens using a task-specific
contrastive objective with modality-balanced batches. Together, MoCA and
QueryREPA produce modality-aware, class-faithful queries that transfer
effectively to downstream training. Across diverse modalities trained
altogether, the proposed approach consistently improves AP with minimal
overhead and no architectural modifications, offering a practical path toward
robust multimodality medical object detection. Project page:
https://araseo.github.io/alignyourquery/.

</details>


### [25] [Representation Learning for Compressed Video Action Recognition via Attentive Cross-modal Interaction with Motion Enhancement](https://arxiv.org/abs/2205.03569)
*Bing Li,Jiaxin Chen,Dongming Zhang,Xiuguo Bao,Di Huang*

Main category: cs.CV

TL;DR: MEACI-Net通过增强运动表示和跨模态交互来改进压缩视频动作识别。


<details>
  <summary>Details</summary>
Motivation: 压缩视频动作识别面临动态粗糙、噪声大以及模态融合不足的问题。MEACI-Net旨在解决这些挑战。

Method: MEACI-Net采用双流架构，一个处理RGB，另一个处理运动信息。运动流中的多尺度块和去噪模块增强了表示学习。选择性运动补充（SMC）和跨模态增强（CMA）模块加强了跨模态交互，SMC用注意力机制补充RGB模态，CMA进行选择性特征增强。

Result: 在UCF-101、HMDB-51和Kinetics-400数据集上的广泛实验证明了MEACI-Net的有效性和效率。

Conclusion: MEACI-Net在处理压缩视频动作识别的挑战方面是有效和高效的。

Abstract: Compressed video action recognition has recently drawn growing attention,
since it remarkably reduces the storage and computational cost via replacing
raw videos by sparsely sampled RGB frames and compressed motion cues (e.g.,
motion vectors and residuals). However, this task severely suffers from the
coarse and noisy dynamics and the insufficient fusion of the heterogeneous RGB
and motion modalities. To address the two issues above, this paper proposes a
novel framework, namely Attentive Cross-modal Interaction Network with Motion
Enhancement (MEACI-Net). It follows the two-stream architecture, i.e. one for
the RGB modality and the other for the motion modality. Particularly, the
motion stream employs a multi-scale block embedded with a denoising module to
enhance representation learning. The interaction between the two streams is
then strengthened by introducing the Selective Motion Complement (SMC) and
Cross-Modality Augment (CMA) modules, where SMC complements the RGB modality
with spatio-temporally attentive local motion features and CMA further combines
the two modalities with selective feature augmentation. Extensive experiments
on the UCF-101, HMDB-51 and Kinetics-400 benchmarks demonstrate the
effectiveness and efficiency of MEACI-Net.

</details>


### [26] [MaskCD: Mitigating LVLM Hallucinations by Image Head Masked Contrastive Decoding](https://arxiv.org/abs/2510.02790)
*Jingyuan Deng,Yujiu Yang*

Main category: cs.CV

TL;DR: LVLMs 幻觉问题可以通过图像头掩码对比解码 (MaskCD) 来缓解，该方法在多个基准测试中表现出有效性，同时保留了 LVLMs 的通用能力。


<details>
  <summary>Details</summary>
Motivation: 现有的 LVLMs 幻觉缓解方法（如对比解码和注意力操纵）存在不足，前者难以构建合适的对比样本，后者缺乏稳定性。

Method: 提出了一种名为图像头掩码对比解码 (MaskCD) 的新方法，该方法利用 LVLMs 中的“图像头”，通过掩码它们来构建用于对比解码的对比样本。

Result: 在 LLaVA-1.5-7b 和 Qwen-VL-7b 模型上，使用 CHAIR、POPE、AMBER 和 MME 等多个基准测试评估 MaskCD，结果表明该方法能有效缓解幻觉现象，并保持 LVLMs 的通用能力。

Conclusion: MaskCD 是一种有效缓解 LVLMs 幻觉问题的新方法，它通过利用图像头掩码来构建对比样本，并在多项基准测试中验证了其有效性。

Abstract: Large vision-language models (LVLMs) have shown remarkable performance in
visual-language understanding for downstream multimodal tasks. While their
capabilities are improving, problems emerge simultaneously. Among those
problems, the hallucinations have attracted much attention, which stands for
the phenomenon where LVLMs generate contradictory content to their input visual
and text contents. Many approaches have been proposed to deal with this issue,
such as contrastive decoding and attention manipulation. However, contrastive
decoding methods struggle in constructing appropriate contrastive samples, and
attention manipulation methods are highly sensitive, lacking stability. In this
work, we propose image head Masked Contrastive Decoding (MaskCD). Our approach
utilizes the "image heads" in LVLMs, masking them to construct contrastive
samples for contrastive decoding. We evaluated MaskCD on LLaVA-1.5-7b and
Qwen-VL-7b, using various benchmarks such as CHAIR, POPE, AMBER and MME. The
results demonstrate that MaskCD effectively alleviates the phenomenon of
hallucinations and retains the general capabilities of LVLMs. Corresponding
resources could be found at: https://github.com/Deng-Jingyuan/MaskCD .

</details>


### [27] [VERNIER: an open-source software pushing marker pose estimation down to the micrometer and nanometer scales](https://arxiv.org/abs/2510.02791)
*Patrick Sandoz,Antoine N. André,Guillaume J. Laurent*

Main category: cs.CV

TL;DR: VERNIER是一款开源软件，用于通过伪周期性图案的相位处理，实现快速可靠的6DoF姿态测量，具有纳米级和微弧度级分辨率，尤其适用于小尺度姿态估计的挑战。


<details>
  <summary>Details</summary>
Motivation: 目前在小尺度姿态估计领域仍然存在挑战，能够捕捉物体在相对较大范围内以纳米和微弧度分辨率测量的6自由度（6DoF）的解决方案很少。本研究旨在提供一种解决方案。

Method: 提出了一种名为VERNIER的开源相位处理软件，该软件基于伪周期性图案，采用相位为基础的局部阈值算法，以实现快速可靠的姿态测量。该方法对噪声、失焦和遮挡具有鲁棒性。

Result: VERNIER软件通过相位处理实现了对伪周期性图案的姿态测量，并证明了其在处理噪声、失焦和遮挡方面的鲁棒性。论文中展示了合成和实验图像的实现过程，并提供了根据所需性能选择合适图案设计和显微镜放大倍数的指南。

Conclusion: VERNIER软件为小尺度姿态估计问题提供了一个有效且鲁棒的解决方案，通过相位处理和伪周期性图案，实现了高分辨率和高精度的6DoF姿态测量，并为用户提供了选择合适参数的指导。

Abstract: Pose estimation is still a challenge at the small scales. Few solutions exist
to capture the 6 degrees of freedom of an object with nanometric and
microradians resolutions over relatively large ranges. Over the years, we have
proposed several fiducial marker and pattern designs to achieve reliable
performance for various microscopy applications. Centimeter ranges are possible
using pattern encoding methods, while nanometer resolutions can be achieved
using phase processing of the periodic frames. This paper presents VERNIER, an
open source phase processing software designed to provide fast and reliable
pose measurement based on pseudo-periodic patterns. Thanks to a phase-based
local thresholding algorithm, the software has proven to be particularly robust
to noise, defocus and occlusion. The successive steps of the phase processing
are presented, as well as the different types of patterns that address
different application needs. The implementation procedure is illustrated with
synthetic and experimental images. Finally, guidelines are given for selecting
the appropriate pattern design and microscope magnification lenses as a
function of the desired performance.

</details>


### [28] [Med-K2N: Flexible K-to-N Modality Translation for Medical Image Synthesis](https://arxiv.org/abs/2510.02815)
*Feng Yuan,Yifan Gao,Yuehua Ye,Haoyue Li,Xin Gao*

Main category: cs.CV

TL;DR: K到N医学图像生成，解决多模态异构性、融合质量控制和模态一致性问题，提出PreWeightNet、ThresholdNet、EffiWeightNet和CMIM，实验证明Med-K2N优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 临床上需要从现有成像模态重构缺失的模态，以支持诊断，因此需要灵活的模态重构方法，即K到N医学生成。

Method: 将多模态医学数据视为序列帧，利用SAM2的序列帧范式和渐进式工作流程，设计了PreWeightNet、ThresholdNet、EffiWeightNet三个模块来学习模态-任务对的自适应权重和融合模式，并通过CMIM模块保持生成图像与目标模态描述之间的一致性。

Result: 提出的Med-K2N在多个基准测试中显著优于现有最先进的方法。

Conclusion: 所提出的Med-K2N框架通过引入自适应权重和融合模式学习，并结合CMIM来保持模态一致性，有效解决了K到N医学图像生成中的挑战。

Abstract: Cross-modal medical image synthesis research focuses on reconstructing
missing imaging modalities from available ones to support clinical diagnosis.
Driven by clinical necessities for flexible modality reconstruction, we explore
K to N medical generation, where three critical challenges emerge: How can we
model the heterogeneous contributions of different modalities to various target
tasks? How can we ensure fusion quality control to prevent degradation from
noisy information? How can we maintain modality identity consistency in
multi-output generation? Driven by these clinical necessities, and drawing
inspiration from SAM2's sequential frame paradigm and clinicians' progressive
workflow of incrementally adding and selectively integrating multi-modal
information, we treat multi-modal medical data as sequential frames with
quality-driven selection mechanisms. Our key idea is to "learn" adaptive
weights for each modality-task pair and "memorize" beneficial fusion patterns
through progressive enhancement. To achieve this, we design three collaborative
modules: PreWeightNet for global contribution assessment, ThresholdNet for
adaptive filtering, and EffiWeightNet for effective weight computation.
Meanwhile, to maintain modality identity consistency, we propose the Causal
Modality Identity Module (CMIM) that establishes causal constraints between
generated images and target modality descriptions using vision-language
modeling. Extensive experimental results demonstrate that our proposed Med-K2N
outperforms state-of-the-art methods by significant margins on multiple
benchmarks. Source code is available.

</details>


### [29] [ELMF4EggQ: Ensemble Learning with Multimodal Feature Fusion for Non-Destructive Egg Quality Assessment](https://arxiv.org/abs/2510.02876)
*Md Zahim Hassan,Md. Osama,Muhammad Ashad Kabir,Md. Saiful Islam,Zannatul Naim*

Main category: cs.CV

TL;DR: 本研究提出ELMF4EggQ框架，通过融合图像、形状和重量等多模态外部特征，利用机器学习方法评估鸡蛋的等级和新鲜度，并公开了相关数据集和代码。


<details>
  <summary>Details</summary>
Motivation: 为了确保食品安全、维持产品标准和提高商业家禽生产的运营效率，需要准确、无损地评估鸡蛋质量。

Method: ELMF4EggQ框架集成了从鸡蛋外部图像中提取的深度特征以及鸡蛋形状和重量等结构特征。使用预训练的CNN模型（ResNet152、DenseNet169、ResNet152V2）提取图像特征，然后进行PCA降维、SMOTE增强和多分类器学习。最后，通过集成投票机制结合预测结果以提高准确性。

Result: 实验结果表明，多模态方法显著优于仅使用图像或仅使用表格特征（形状和重量）的方法。多模态集成方法在等级分类方面达到了86.57%的准确率，在新鲜度预测方面达到了70.83%的准确率。

Conclusion: ELMF4EggQ框架能够仅通过外部非侵入性特征，准确地评估鸡蛋的内部质量，为鸡蛋质量评估提供了新的方法，并且公开的数据集和代码有利于该领域的研究和应用。

Abstract: Accurate, non-destructive assessment of egg quality is critical for ensuring
food safety, maintaining product standards, and operational efficiency in
commercial poultry production. This paper introduces ELMF4EggQ, an ensemble
learning framework that employs multimodal feature fusion to classify egg grade
and freshness using only external attributes - image, shape, and weight. A
novel, publicly available dataset of 186 brown-shelled eggs was constructed,
with egg grade and freshness levels determined through laboratory-based expert
assessments involving internal quality measurements, such as yolk index and
Haugh unit. To the best of our knowledge, this is the first study to apply
machine learning methods for internal egg quality assessment using only
external, non-invasive features, and the first to release a corresponding
labeled dataset. The proposed framework integrates deep features extracted from
external egg images with structural characteristics such as egg shape and
weight, enabling a comprehensive representation of each egg. Image feature
extraction is performed using top-performing pre-trained CNN models (ResNet152,
DenseNet169, and ResNet152V2), followed by PCA-based dimensionality reduction,
SMOTE augmentation, and classification using multiple machine learning
algorithms. An ensemble voting mechanism combines predictions from the
best-performing classifiers to enhance overall accuracy. Experimental results
demonstrate that the multimodal approach significantly outperforms image-only
and tabular (shape and weight) only baselines, with the multimodal ensemble
approach achieving 86.57% accuracy in grade classification and 70.83% in
freshness prediction. All code and data are publicly available at
https://github.com/Kenshin-Keeps/Egg_Quality_Prediction_ELMF4EggQ, promoting
transparency, reproducibility, and further research in this domain.

</details>


### [30] [One Patch to Caption Them All: A Unified Zero-Shot Captioning Framework](https://arxiv.org/abs/2510.02898)
*Lorenzo Bianchi,Giacomo Pacini,Fabio Carrara,Nicola Messina,Giuseppe Amato,Fabrizio Falchi*

Main category: cs.CV

TL;DR: 我们提出了Patch-ioner，一个统一的零样本区域图像描述框架，它不依赖于区域级监督，而是将单个图像块视为原子描述单元，并聚合它们来描述从单个图像块到非连续区域以及整个图像的任意区域。


<details>
  <summary>Details</summary>
Motivation: 现有的零样本描述模型主要关注全局图像特征和整图描述，而忽略了对图像任意区域进行描述的能力。本研究旨在提出一个能够描述图像任意区域的零样本描述框架，而无需区域级监督。

Method: Patch-ioner 框架将图像描述任务从以图像为中心转向以图像块为中心。它将单个图像块视为基本的描述单元，并通过聚合这些单元来生成对任意区域（包括单个图像块、非连续区域和整个图像）的描述。该方法分析了现有潜在描述模型在此框架下工作的关键因素，并强调了具有密集视觉特征的骨干网络（如DINO）的重要性。

Result: 实验证明，Patch-ioner 在零样本密集描述、区域集合描述和新增的跟踪描述任务上均优于其他基线和最先进的竞争模型。这表明了进行可扩展描述生成的图像块级语义表示的有效性。

Conclusion: Patch-ioner 框架通过采用以图像块为中心的范式，实现了对任意图像区域的零样本描述，而无需区域级监督。具有密集视觉特征的骨干网络对于实现最先进的性能至关重要，并且所提出的方法在各种区域描述任务中都表现出了优越性。

Abstract: Zero-shot captioners are recently proposed models that utilize common-space
vision-language representations to caption images without relying on paired
image-text data. To caption an image, they proceed by textually decoding a
text-aligned image feature, but they limit their scope to global
representations and whole-image captions. We present \frameworkName{}, a
unified framework for zero-shot captioning that shifts from an image-centric to
a patch-centric paradigm, enabling the captioning of arbitrary regions without
the need of region-level supervision. Instead of relying on global image
representations, we treat individual patches as atomic captioning units and
aggregate them to describe arbitrary regions, from single patches to
non-contiguous areas and entire images. We analyze the key ingredients that
enable current latent captioners to work in our novel proposed framework.
Experiments demonstrate that backbones producing meaningful, dense visual
features, such as DINO, are key to achieving state-of-the-art performance in
multiple region-based captioning tasks. Compared to other baselines and
state-of-the-art competitors, our models achieve better performance on
zero-shot dense, region-set, and a newly introduced trace captioning task,
highlighting the effectiveness of patch-wise semantic representations for
scalable caption generation. Project page at https://paciosoft.com/Patch-ioner/ .

</details>


### [31] [Training-Free Out-Of-Distribution Segmentation With Foundation Models](https://arxiv.org/abs/2510.02909)
*Laith Nayal,Hadi Salloum,Ahmad Taha,Yaroslav Kholodov,Alexander Gasnikov*

Main category: cs.CV

TL;DR: 无需额外监督，利用预训练视觉基础模型（如InternImage）的特征，结合K-Means聚类和置信度阈值，即可有效检测语义分割中的未知（OoD）区域，并在多个基准测试中超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 评估预训练视觉基础模型在经过分割任务微调后，是否能在无需任何异常监督的情况下，固有地区分in-distribution（ID）和out-of-distribution（OoD）区域，以解决安全关键应用（如自动驾驶）中未知物体检测的挑战。

Method: 提出一种无需训练的方法，利用InternImage骨干网络的特征，并结合K-Means聚类和在原始解码器logits上的置信度阈值，来识别OoD区域。

Result: 在RoadAnomaly基准测试上达到50.02的平均精度（AP），在ADE-OoD基准测试上达到48.77的AP，优于多种有监督和无监督基线方法。

Conclusion: 预训练视觉基础模型在分割任务上微调后，能够无需额外数据或假设，有效地识别OoD区域，为通用的OoD分割方法指明了一个有前景的方向。

Abstract: Detecting unknown objects in semantic segmentation is crucial for
safety-critical applications such as autonomous driving. Large vision
foundation models, including DINOv2, InternImage, and CLIP, have advanced
visual representation learning by providing rich features that generalize well
across diverse tasks. While their strength in closed-set semantic tasks is
established, their capability to detect out-of-distribution (OoD) regions in
semantic segmentation remains underexplored. In this work, we investigate
whether foundation models fine-tuned on segmentation datasets can inherently
distinguish in-distribution (ID) from OoD regions without any outlier
supervision. We propose a simple, training-free approach that utilizes features
from the InternImage backbone and applies K-Means clustering alongside
confidence thresholding on raw decoder logits to identify OoD clusters. Our
method achieves 50.02 Average Precision on the RoadAnomaly benchmark and 48.77
on the benchmark of ADE-OoD with InternImage-L, surpassing several supervised
and unsupervised baselines. These results suggest a promising direction for
generic OoD segmentation methods that require minimal assumptions or additional
data.

</details>


### [32] [Don't Just Chase "Highlighted Tokens" in MLLMs: Revisiting Visual Holistic Context Retention](https://arxiv.org/abs/2510.02912)
*Xin Zou,Di Lu,Yizhou Wang,Yibo Yan,Yuanhuiyi Lyu,Xu Zheng,Linfeng Zhang,Xuming Hu*

Main category: cs.CV

TL;DR: HoloV是一种用于高效推理的即插即用视觉标记修剪框架，它通过自适应地分配不同空间裁剪的修剪预算，确保保留的标记捕获全局视觉上下文，从而在更高修剪率下最小化性能下降。


<details>
  <summary>Details</summary>
Motivation: 现有的基于注意力机制的视觉标记修剪方法倾向于保留语义相似的标记，导致在较高修剪率下性能下降明显。

Method: HoloV通过自适应地分配不同空间裁剪的修剪预算，从整体角度重新考虑标记保留，以捕获全局视觉上下文。

Result: 在各种任务、MLLM架构和修剪率下，HoloV均优于最先进的方法。例如，配备HoloV的LLaVA1.5在修剪88.9%的视觉标记后，仍保留了95.8%的原始性能。

Conclusion: HoloV通过一种新颖的、整体的视角，有效地解决了现有视觉标记修剪方法的局限性，并在效率和准确性之间取得了优越的权衡。

Abstract: Despite their powerful capabilities, Multimodal Large Language Models (MLLMs)
suffer from considerable computational overhead due to their reliance on
massive visual tokens. Recent studies have explored token pruning to alleviate
this problem, which typically uses text-vision cross-attention or
[\texttt{CLS}] attention to assess and discard redundant visual tokens. In this
work, we identify a critical limitation of such attention-first pruning
approaches, i.e., they tend to preserve semantically similar tokens, resulting
in pronounced performance drops under high pruning ratios. To this end, we
propose {HoloV}, a simple yet effective, plug-and-play visual token pruning
framework for efficient inference. Distinct from previous attention-first
schemes, HoloV rethinks token retention from a holistic perspective. By
adaptively distributing the pruning budget across different spatial crops,
HoloV ensures that the retained tokens capture the global visual context rather
than isolated salient features. This strategy minimizes representational
collapse and maintains task-relevant information even under aggressive pruning.
Experimental results demonstrate that our HoloV achieves superior performance
across various tasks, MLLM architectures, and pruning ratios compared to SOTA
methods. For instance, LLaVA1.5 equipped with HoloV preserves 95.8\% of the
original performance after pruning 88.9\% of visual tokens, achieving superior
efficiency-accuracy trade-offs.

</details>


### [33] [Zero-Shot Robustness of Vision Language Models Via Confidence-Aware Weighting](https://arxiv.org/abs/2510.02913)
*Nikoo Naghavian,Mostafa Tavassolipour*

Main category: cs.CV

TL;DR: 该研究提出了一种名为 CAW 的新方法，通过引入置信度感知损失和特征对齐正则化来提高视觉-语言模型的零样本鲁棒性，实验证明其在各种数据集和攻击下均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言模型（如 CLIP）在零样本泛化方面表现出色，但容易受到对抗性攻击，因此需要提高其鲁棒性。

Method: 提出置信度感知加权（CAW）方法，包括两个组件：（1）置信度感知损失，通过缩放干净和对抗性预测之间的 KL 散度来优先处理不确定的对抗性样本；（2）特征对齐正则化，通过最小化对抗性输入的冻结和微调图像编码器特征之间的距离来保持语义一致性。

Result: CAW 方法在 TinyImageNet 和另外 14 个数据集上的实验表明，在 AutoAttack 等强攻击下，CAW 的性能优于 PMG-AFT 和 TGA-ZSR 等现有方法，同时内存占用更少。CAW 还能提高干净和鲁棒的准确性，同时不牺牲泛化能力。

Conclusion: CAW 是一种有效的提高视觉-语言模型零样本鲁棒性的方法，通过置信度感知损失和特征对齐正则化，在保持泛化能力的同时，提升了模型在对抗性攻击下的表现。

Abstract: Vision-language models like CLIP demonstrate impressive zero-shot
generalization but remain highly vulnerable to adversarial attacks. In this
work, we propose Confidence-Aware Weighting (CAW) to enhance zero-shot
robustness in vision-language models. CAW consists of two components: (1) a
Confidence-Aware loss that prioritizes uncertain adversarial examples by
scaling the KL divergence between clean and adversarial predictions, and (2) a
feature alignment regularization that preserves semantic consistency by
minimizing the distance between frozen and fine-tuned image encoder features on
adversarial inputs. These components work jointly to improve both clean and
robust accuracy without sacrificing generalization. Extensive experiments on
TinyImageNet and 14 additional datasets show that CAW outperforms recent
methods such as PMG-AFT and TGA-ZSR under strong attacks like AutoAttack, while
using less memory.

</details>


### [34] [Multimodal Carotid Risk Stratification with Large Vision-Language Models: Benchmarking, Fine-Tuning, and Clinical Insights](https://arxiv.org/abs/2510.02922)
*Daphne Tsolissou,Theofanis Ganitidis,Konstantinos Mitsis,Stergios CHristodoulidis,Maria Vakalopoulou,Konstantina Nikita*

Main category: cs.CV

TL;DR: 大型视觉语言模型(LVLM)在颈动脉斑块评估中表现出潜力，但需要领域适应和多模态数据整合以提高临床准确性。


<details>
  <summary>Details</summary>
Motivation: 需要一种透明可解释的方法来整合多种临床和影像信息，以进行可靠的颈动脉动脉粥样硬化疾病风险评估。

Method: 提出了一种通过模拟面试风格问题序列来模拟真实诊断场景的框架，并评估了多种开源LVLM。通过低秩适应(LoRA)对LLaVa-NeXT-Vicuna模型进行了微调，并整合了多模态表格数据（文本形式）进行评估。

Result: 初步的零样本实验表明，并非所有LVLM都能准确识别影像模态和解剖结构，并且所有模型在风险分级方面表现不佳。经过LoRA适应的LLaVa-NeXT-Vicuna模型在卒中风险分层方面有显著改善。整合多模态表格数据进一步提高了特异性和平衡准确性，其性能可与先前基于相同数据集训练的卷积神经网络(CNN)基线相媲美。

Conclusion: LVLM在超声心血管风险预测方面具有潜力，但临床转化需要重视多模态整合、模型校准和领域适应。

Abstract: Reliable risk assessment for carotid atheromatous disease remains a major
clinical challenge, as it requires integrating diverse clinical and imaging
information in a manner that is transparent and interpretable to clinicians.
This study investigates the potential of state-of-the-art and recent large
vision-language models (LVLMs) for multimodal carotid plaque assessment by
integrating ultrasound imaging (USI) with structured clinical, demographic,
laboratory, and protein biomarker data. A framework that simulates realistic
diagnostic scenarios through interview-style question sequences is proposed,
comparing a range of open-source LVLMs, including both general-purpose and
medically tuned models. Zero-shot experiments reveal that even if they are very
powerful, not all LVLMs can accurately identify imaging modality and anatomy,
while all of them perform poorly in accurate risk classification. To address
this limitation, LLaVa-NeXT-Vicuna is adapted to the ultrasound domain using
low-rank adaptation (LoRA), resulting in substantial improvements in stroke
risk stratification. The integration of multimodal tabular data in the form of
text further enhances specificity and balanced accuracy, yielding competitive
performance compared to prior convolutional neural network (CNN) baselines
trained on the same dataset. Our findings highlight both the promise and
limitations of LVLMs in ultrasound-based cardiovascular risk prediction,
underscoring the importance of multimodal integration, model calibration, and
domain adaptation for clinical translation.

</details>


### [35] [Flip Distribution Alignment VAE for Multi-Phase MRI Synthesis](https://arxiv.org/abs/2510.02970)
*Xiaoyan Kui,Qianmu Xiao,Qqinsong Li,Zexin Ji,JIelin Zhang,Beiji Zou*

Main category: cs.CV

TL;DR: Flip Distribution Alignment Variational Autoencoder (FDA-VAE) 是一种轻量级、解耦特征的 VAE 模型，用于多期增强对比 MRI 合成。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多期增强对比（CE）MRI 合成中，将共享和独立特征分离存在参数效率低和训练策略不可解释的问题。

Method: 提出了一种名为 Flip Distribution Alignment Variational Autoencoder (FDA-VAE) 的模型，该模型将输入和目标图像编码为关于标准正态分布对称的两个潜在分布，以分离共享和独立特征，并采用 Y 形双向训练策略增强可解释性。

Result: 与现有的基于深度自动编码器的端到端合成方法相比，FDA-VAE 显著减少了模型参数和推理时间，同时有效提高了合成质量。

Conclusion: FDA-VAE 在多期 CE MRI 合成方面，相比现有方法具有更高的参数效率、更快的推理速度和更好的合成质量。

Abstract: Separating shared and independent features is crucial for multi-phase
contrast-enhanced (CE) MRI synthesis. However, existing methods use deep
autoencoder generators with low parameter efficiency and lack interpretable
training strategies. In this paper, we propose Flip Distribution Alignment
Variational Autoencoder (FDA-VAE), a lightweight feature-decoupled VAE model
for multi-phase CE MRI synthesis. Our method encodes input and target images
into two latent distributions that are symmetric concerning a standard normal
distribution, effectively separating shared and independent features. The
Y-shaped bidirectional training strategy further enhances the interpretability
of feature separation. Experimental results show that compared to existing deep
autoencoder-based end-to-end synthesis methods, FDA-VAE significantly reduces
model parameters and inference time while effectively improving synthesis
quality. The source code is publicly available at
https://github.com/QianMuXiao/FDA-VAE.

</details>


### [36] [TIT-Score: Evaluating Long-Prompt Based Text-to-Image Alignment via Text-to-Image-to-Text Consistency](https://arxiv.org/abs/2510.02987)
*Juntong Wang,Huiyu Duan,Jiarui Wang,Ziheng Jia,Guangtao Zhai,Xiongkuo Min*

Main category: cs.CV

TL;DR: LPG-Bench是一个包含200个长提示的新基准，用于评估文本到图像模型。它还引入了一个名为TIT的新评估指标，该指标通过文本到图像到文本的一致性来衡量模型性能，并优于现有指标。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像模型在理解和遵循长而详细的提示方面存在不足，导致生成不一致。需要一个评估长提示生成能力的基准和方法。

Method: 提出LPG-Bench基准，包含200个长提示（平均长度超过250词），并生成2600张图像，进行人工排名评估。提出基于文本-图像-文本一致性的新零样本评估指标TIT（包括TIT-Score和TIT-Score-LLM）。

Result: 在LPG-Bench基准上，现有的文本到图像对齐评估指标与人类偏好的吻合度较差。TIT指标（特别是TIT-Score-LLM）在成对准确率上比现有最佳基线提高了7.31%，与人类判断更一致。

Conclusion: LPG-Bench基准和TIT评估方法共同为文本到图像模型的评估和发展提供了更深入的视角，并有望促进模型的进步。

Abstract: With the rapid advancement of large multimodal models (LMMs), recent
text-to-image (T2I) models can generate high-quality images and demonstrate
great alignment to short prompts. However, they still struggle to effectively
understand and follow long and detailed prompts, displaying inconsistent
generation. To address this challenge, we introduce LPG-Bench, a comprehensive
benchmark for evaluating long-prompt-based text-to-image generation. LPG-Bench
features 200 meticulously crafted prompts with an average length of over 250
words, approaching the input capacity of several leading commercial models.
Using these prompts, we generate 2,600 images from 13 state-of-the-art models
and further perform comprehensive human-ranked annotations. Based on LPG-Bench,
we observe that state-of-the-art T2I alignment evaluation metrics exhibit poor
consistency with human preferences on long-prompt-based image generation. To
address the gap, we introduce a novel zero-shot metric based on
text-to-image-to-text consistency, termed TIT, for evaluating
long-prompt-generated images. The core concept of TIT is to quantify T2I
alignment by directly comparing the consistency between the raw prompt and the
LMM-produced description on the generated image, which includes an efficient
score-based instantiation TIT-Score and a large-language-model (LLM) based
instantiation TIT-Score-LLM. Extensive experiments demonstrate that our
framework achieves superior alignment with human judgment compared to
CLIP-score, LMM-score, etc., with TIT-Score-LLM attaining a 7.31% absolute
improvement in pairwise accuracy over the strongest baseline. LPG-Bench and TIT
methods together offer a deeper perspective to benchmark and foster the
development of T2I models. All resources will be made publicly available.

</details>


### [37] [Geometry Meets Vision: Revisiting Pretrained Semantics in Distilled Fields](https://arxiv.org/abs/2510.03104)
*Zhiting Mei,Ola Shorinwa,Anirudha Majumdar*

Main category: cs.CV

TL;DR: 视觉-几何特征在蒸馏场中的应用尚不明确，视觉-几何特征包含更多几何细节，但视觉-仅特征在下游任务中用途更广，并提出了一种新颖的无初始猜测的辐射场逆转框架SPINE。


<details>
  <summary>Details</summary>
Motivation: 探索视觉-几何特征在蒸馏场中的潜力，以及它们是否能为空间任务（如姿态估计）提供优势。

Method: 提出SPINE框架，包含粗略逆转和精细逆转两个核心部分，用于在没有初始猜测的情况下逆转辐射场。

Result: 视觉-几何特征包含更精细的结构细节，但在物体定位方面没有显著差异。出乎意料的是，姿态估计的准确性随着视觉-几何特征的使用而降低。

Conclusion: 视觉-仅特征在更广泛的下游任务中具有更大的通用性。未来的研究需要探索有效的视觉-几何特征融合策略，以增强预训练语义特征的通用性和性能。

Abstract: Semantic distillation in radiance fields has spurred significant advances in
open-vocabulary robot policies, e.g., in manipulation and navigation, founded
on pretrained semantics from large vision models. While prior work has
demonstrated the effectiveness of visual-only semantic features (e.g., DINO and
CLIP) in Gaussian Splatting and neural radiance fields, the potential benefit
of geometry-grounding in distilled fields remains an open question. In
principle, visual-geometry features seem very promising for spatial tasks such
as pose estimation, prompting the question: Do geometry-grounded semantic
features offer an edge in distilled fields? Specifically, we ask three critical
questions: First, does spatial-grounding produce higher-fidelity geometry-aware
semantic features? We find that image features from geometry-grounded backbones
contain finer structural details compared to their counterparts. Secondly, does
geometry-grounding improve semantic object localization? We observe no
significant difference in this task. Thirdly, does geometry-grounding enable
higher-accuracy radiance field inversion? Given the limitations of prior work
and their lack of semantics integration, we propose a novel framework SPINE for
inverting radiance fields without an initial guess, consisting of two core
components: coarse inversion using distilled semantics, and fine inversion
using photometric-based optimization. Surprisingly, we find that the pose
estimation accuracy decreases with geometry-grounded features. Our results
suggest that visual-only features offer greater versatility for a broader range
of downstream tasks, although geometry-grounded features contain more geometric
detail. Notably, our findings underscore the necessity of future research on
effective strategies for geometry-grounding that augment the versatility and
performance of pretrained semantic features.

</details>


### [38] [Towards Scalable and Consistent 3D Editing](https://arxiv.org/abs/2510.02994)
*Ruihao Xia,Yang Tang,Pan Zhou*

Main category: cs.CV

TL;DR: 3DEditVerse是一个大规模3D编辑数据集，3DEditFormer是一个3D编辑模型，解决了现有方法的痛点。


<details>
  <summary>Details</summary>
Motivation: 3D编辑在内容创作、数字娱乐、AR/VR等领域有广泛应用，但现有方法存在跨视图一致性、结构保真度和可控性等方面的挑战。

Method: 通过引入3DEditVerse数据集和提出3DEditFormer模型来解决这些挑战。3DEditVerse包含大量配对数据，3DEditFormer采用条件Transformer，利用双重注意力机制和时间自适应门控，实现精确、一致的编辑。

Result: 实验表明，该框架在定量和定性方面均优于现有方法，为实用和可扩展的3D编辑树立了新的标杆。

Conclusion: 提出的3DEditVerse数据集和3DEditFormer模型能够实现精确、一致且无需3D掩码的3D编辑。

Abstract: 3D editing - the task of locally modifying the geometry or appearance of a 3D
asset - has wide applications in immersive content creation, digital
entertainment, and AR/VR. However, unlike 2D editing, it remains challenging
due to the need for cross-view consistency, structural fidelity, and
fine-grained controllability. Existing approaches are often slow, prone to
geometric distortions, or dependent on manual and accurate 3D masks that are
error-prone and impractical. To address these challenges, we advance both the
data and model fronts. On the data side, we introduce 3DEditVerse, the largest
paired 3D editing benchmark to date, comprising 116,309 high-quality training
pairs and 1,500 curated test pairs. Built through complementary pipelines of
pose-driven geometric edits and foundation model-guided appearance edits,
3DEditVerse ensures edit locality, multi-view consistency, and semantic
alignment. On the model side, we propose 3DEditFormer, a
3D-structure-preserving conditional transformer. By enhancing image-to-3D
generation with dual-guidance attention and time-adaptive gating, 3DEditFormer
disentangles editable regions from preserved structure, enabling precise and
consistent edits without requiring auxiliary 3D masks. Extensive experiments
demonstrate that our framework outperforms state-of-the-art baselines both
quantitatively and qualitatively, establishing a new standard for practical and
scalable 3D editing. Dataset and code will be released. Project:
https://www.lv-lab.org/3DEditFormer/

</details>


### [39] [Mask2IV: Interaction-Centric Video Generation via Mask Trajectories](https://arxiv.org/abs/2510.03135)
*Gen Li,Bo Zhao,Jianfei Yang,Laura Sevilla-Lara*

Main category: cs.CV

TL;DR: Mask2IV框架用于生成交互式视频，通过预测运动轨迹而非依赖密集掩码，实现了更灵活和可控的视频生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以对交互式视频进行建模，且需要密集的掩码标注，而Mask2IV旨在克服这些限制。

Method: Mask2IV采用解耦的两阶段流程：首先预测参与者和物体的运动轨迹，然后根据这些轨迹生成视频，无需用户提供密集掩码。

Result: Mask2IV在视觉真实性和可控性方面优于现有基线方法，并支持通过动作描述或空间线索来引导交互。

Conclusion: Mask2IV是一个新颖的框架，用于生成交互式视频，通过预测运动轨迹和提供灵活的控制方式，解决了现有方法的局限性。

Abstract: Generating interaction-centric videos, such as those depicting humans or
robots interacting with objects, is crucial for embodied intelligence, as they
provide rich and diverse visual priors for robot learning, manipulation policy
training, and affordance reasoning. However, existing methods often struggle to
model such complex and dynamic interactions. While recent studies show that
masks can serve as effective control signals and enhance generation quality,
obtaining dense and precise mask annotations remains a major challenge for
real-world use. To overcome this limitation, we introduce Mask2IV, a novel
framework specifically designed for interaction-centric video generation. It
adopts a decoupled two-stage pipeline that first predicts plausible motion
trajectories for both actor and object, then generates a video conditioned on
these trajectories. This design eliminates the need for dense mask inputs from
users while preserving the flexibility to manipulate the interaction process.
Furthermore, Mask2IV supports versatile and intuitive control, allowing users
to specify the target object of interaction and guide the motion trajectory
through action descriptions or spatial position cues. To support systematic
training and evaluation, we curate two benchmarks covering diverse action and
object categories across both human-object interaction and robotic manipulation
scenarios. Extensive experiments demonstrate that our method achieves superior
visual realism and controllability compared to existing baselines.

</details>


### [40] [Not every day is a sunny day: Synthetic cloud injection for deep land cover segmentation robustness evaluation across data sources](https://arxiv.org/abs/2510.03006)
*Sara Mobsite,Renaud Hostache,Laure Berti Equille,Emmanuel Roux,Joris Guerin*

Main category: cs.CV

TL;DR: Sentinel-1雷达数据和NDI指数可弥补Sentinel-2光学数据在陆地覆盖分割中的不足，尤其是在云层覆盖的热带地区。


<details>
  <summary>Details</summary>
Motivation: 现有的监督学习陆地覆盖分割方法依赖于标记的Sentinel-2卫星数据，但大多数数据集是无云的，这限制了它们在多云地区（如热带地区）的应用。因此，需要评估Sentinel-1雷达数据在填补云层遮挡的光学影像中的作用，并提出改进深度学习模型以处理空间和光谱细节损失的方法。

Method: 开发了一种云层注入算法来模拟真实的云层覆盖，并测试了Sentinel-1雷达数据在云层遮挡情况下的效果。提出了一种将归一化差异指数（NDI）注入到解码器末层的轻量级方法，以解决编码器下采样过程中的细节损失问题。

Result: 在无云图像上，注入NDI使U-Net的陆地覆盖分割性能提高了1.99%，DeepLabV3的性能提高了2.78%。在云层覆盖条件下，与单独使用光学数据相比，结合Sentinel-1数据显著提高了所有模型的性能，证明了雷达-光学融合在恶劣大气条件下的有效性。

Conclusion: Sentinel-1雷达数据和NDI指数的结合可以有效提高陆地覆盖分割的性能，尤其是在光学数据受云层遮挡的情况下，这对于在热带地区进行准确的土地覆盖制图至关重要。

Abstract: Supervised deep learning for land cover semantic segmentation (LCS) relies on
labeled satellite data. However, most existing Sentinel-2 datasets are
cloud-free, which limits their usefulness in tropical regions where clouds are
common. To properly evaluate the extent of this problem, we developed a cloud
injection algorithm that simulates realistic cloud cover, allowing us to test
how Sentinel-1 radar data can fill in the gaps caused by cloud-obstructed
optical imagery. We also tackle the issue of losing spatial and/or spectral
details during encoder downsampling in deep networks. To mitigate this loss, we
propose a lightweight method that injects Normalized Difference Indices (NDIs)
into the final decoding layers, enabling the model to retain key spatial
features with minimal additional computation. Injecting NDIs enhanced land
cover segmentation performance on the DFC2020 dataset, yielding improvements of
1.99% for U-Net and 2.78% for DeepLabV3 on cloud-free imagery. Under
cloud-covered conditions, incorporating Sentinel-1 data led to significant
performance gains across all models compared to using optical data alone,
highlighting the effectiveness of radar-optical fusion in challenging
atmospheric scenarios.

</details>


### [41] [PocketSR: The Super-Resolution Expert in Your Pocket Mobiles](https://arxiv.org/abs/2510.03012)
*Haoze Sun,Linfeng Jiang,Fan Li,Renjing Pei,Zhixin Wang,Yong Guo,Jiaqi Xu,Haoyu Chen,Jin Han,Fenglong Song,Yujiu Yang,Wenbo Li*

Main category: cs.CV

TL;DR: PocketSR是一个超轻量级、单步的图像超分辨率模型，专为边缘设备设计，在保持高质量的同时实现了高效率。


<details>
  <summary>Details</summary>
Motivation: 现有的图像超分辨率方法计算成本高、延迟大，不适用于移动设备等边缘部署场景。需要一种高效且能保持高保真度的模型。

Method: PocketSR采用LiteED（一种轻量级变分自编码器）替代SD中的VAE，并结合在线退火剪枝技术对U-Net进行优化，同时利用多层特征蒸馏损失来保留知识。

Result: PocketSR模型参数量为1.46亿，能在0.8秒内处理4K图像，速度远超现有方法，并且性能与当前最先进的单步甚至多步模型相当。

Conclusion: PocketSR通过其高效的设计（LiteED和在线退火剪枝），实现了在边缘设备上进行高质量图像超分辨率的可行性，为未来的研究提供了有价值的见解。

Abstract: Real-world image super-resolution (RealSR) aims to enhance the visual quality
of in-the-wild images, such as those captured by mobile phones. While existing
methods leveraging large generative models demonstrate impressive results, the
high computational cost and latency make them impractical for edge deployment.
In this paper, we introduce PocketSR, an ultra-lightweight, single-step model
that brings generative modeling capabilities to RealSR while maintaining high
fidelity. To achieve this, we design LiteED, a highly efficient alternative to
the original computationally intensive VAE in SD, reducing parameters by 97.5%
while preserving high-quality encoding and decoding. Additionally, we propose
online annealing pruning for the U-Net, which progressively shifts generative
priors from heavy modules to lightweight counterparts, ensuring effective
knowledge transfer and further optimizing efficiency. To mitigate the loss of
prior knowledge during pruning, we incorporate a multi-layer feature
distillation loss. Through an in-depth analysis of each design component, we
provide valuable insights for future research. PocketSR, with a model size of
146M parameters, processes 4K images in just 0.8 seconds, achieving a
remarkable speedup over previous methods. Notably, it delivers performance on
par with state-of-the-art single-step and even multi-step RealSR models, making
it a highly practical solution for edge-device applications.

</details>


### [42] [When and Where do Events Switch in Multi-Event Video Generation?](https://arxiv.org/abs/2510.03049)
*Ruotong Liao,Guowen Huang,Qing Cheng,Thomas Seidl,Daniel Cremers,Volker Tresp*

Main category: cs.CV

TL;DR: MEve是一个用于评估多事件文本到视频生成的新提示套件，通过对OpenSora和CogVideoX的研究，揭示了在去噪步骤和块状模型层中早期干预对于多事件视频生成的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在扩展到多事件生成时，忽略了事件转换的内在因素，因此本研究旨在探讨多事件提示在文本到视频生成中控制事件转换的时机和位置。

Method: 提出了MEve，一个用于评估多事件文本到视频（T2V）生成的自选提示套件，并对OpenSora和CogVideoX两个代表性模型系列进行了系统的研究。

Result: 通过大量实验证明了在去噪步骤和块状模型层中早期干预的重要性，这些干预是多事件视频生成的基本要素。

Conclusion: 研究结果强调了早期干预在去噪步骤和块状模型层对于多事件视频生成的重要性，并指出了未来模型在多事件条件控制方面的可能性。

Abstract: Text-to-video (T2V) generation has surged in response to challenging
questions, especially when a long video must depict multiple sequential events
with temporal coherence and controllable content. Existing methods that extend
to multi-event generation omit an inspection of the intrinsic factor in event
shifting. The paper aims to answer the central question: When and where
multi-event prompts control event transition during T2V generation. This work
introduces MEve, a self-curated prompt suite for evaluating multi-event
text-to-video (T2V) generation, and conducts a systematic study of two
representative model families, i.e., OpenSora and CogVideoX. Extensive
experiments demonstrate the importance of early intervention in denoising steps
and block-wise model layers, revealing the essential factor for multi-event
video generation and highlighting the possibilities for multi-event
conditioning in future models.

</details>


### [43] [InsideOut: An EfficientNetV2-S Based Deep Learning Framework for Robust Multi-Class Facial Emotion Recognition](https://arxiv.org/abs/2510.03066)
*Ahsan Farabi,Israt Khandaker,Ibrahim Khalil Shanto,Md Abdul Ahad Minhaz,Tanisha Zaman*

Main category: cs.CV

TL;DR: InsideOut是一个基于EfficientNetV2-S的、可复现的人脸情感识别框架，通过数据增强和类别加权损失等技术解决了数据集不平衡问题，在FER2013上取得了62.8%的准确率。


<details>
  <summary>Details</summary>
Motivation: 人脸情感识别（FER）在人机交互、教育、医疗和安全系统等领域具有重要应用价值。尽管深度学习取得了进展，但由于遮挡、光照、姿态变化、类内差异细微以及数据集不平衡（尤其是少数类情感识别困难）等问题，FER仍然面临挑战。

Method: 该框架名为InsideOut，采用EfficientNetV2-S模型，并结合迁移学习、强数据增强和不平衡感知优化。具体步骤包括标准化FER2013图像、应用分层抽样和数据增强，以及通过类别加权损失函数对轻量级分类头进行微调，以解决数据分布不均的问题。

Result: InsideOut在FER2013数据集上取得了62.8%的准确率和0.590的宏平均F1分数，与传统的卷积神经网络基线相比，结果具有竞争力。

Conclusion: 该研究表明，高效的神经网络架构结合专门的不平衡处理方法，能够提供实用、透明且可复现的人脸情感识别解决方案。

Abstract: Facial Emotion Recognition (FER) is a key task in affective computing,
enabling applications in human-computer interaction, e-learning, healthcare,
and safety systems. Despite advances in deep learning, FER remains challenging
due to occlusions, illumination and pose variations, subtle intra-class
differences, and dataset imbalance that hinders recognition of minority
emotions. We present InsideOut, a reproducible FER framework built on
EfficientNetV2-S with transfer learning, strong data augmentation, and
imbalance-aware optimization. The approach standardizes FER2013 images, applies
stratified splitting and augmentation, and fine-tunes a lightweight
classification head with class-weighted loss to address skewed distributions.
InsideOut achieves 62.8% accuracy with a macro averaged F1 of 0.590 on FER2013,
showing competitive results compared to conventional CNN baselines. The novelty
lies in demonstrating that efficient architectures, combined with tailored
imbalance handling, can provide practical, transparent, and reproducible FER
solutions.

</details>


### [44] [What Drives Compositional Generalization in Visual Generative Models?](https://arxiv.org/abs/2510.03075)
*Karim Farid,Rajat Sahay,Yumna Ali Alnaggar,Simon Schrodi,Volker Fischer,Cordelia Schmid,Thomas Brox*

Main category: cs.CV

TL;DR: 本研究系统地研究了设计选择对图像和视频生成中组合泛化能力的影响，发现训练目标的离散/连续分布和条件信息量是关键因素。通过结合离散的MaskGIT和连续的JEPA目标，提高了组合性能。


<details>
  <summary>Details</summary>
Motivation: 组合泛化能力对于视觉生成模型至关重要，但其影响机制尚不完全清楚。本研究旨在系统地探究各种设计选择对图像和视频生成中组合泛化能力的积极或消极影响。

Method: 通过控制实验，研究了训练目标是离散分布还是连续分布，以及条件信息量对组合泛化的影响。提出将MaskGIT的离散损失与基于JEPA的连续目标相结合。

Result: 研究发现，训练目标的离散/连续分布和条件信息量是影响组合泛化的两个关键因素。将JEPA目标加入MaskGIT可以提高组合泛化性能。

Conclusion: 通过结合离散的MaskGIT和连续的JEPA目标，可以提高视觉生成模型在组合泛化方面的性能。

Abstract: Compositional generalization, the ability to generate novel combinations of
known concepts, is a key ingredient for visual generative models. Yet, not all
mechanisms that enable or inhibit it are fully understood. In this work, we
conduct a systematic study of how various design choices influence
compositional generalization in image and video generation in a positive or
negative way. Through controlled experiments, we identify two key factors: (i)
whether the training objective operates on a discrete or continuous
distribution, and (ii) to what extent conditioning provides information about
the constituent concepts during training. Building on these insights, we show
that relaxing the MaskGIT discrete loss with an auxiliary continuous JEPA-based
objective can improve compositional performance in discrete models like
MaskGIT.

</details>


### [45] [Latent Diffusion Unlearning: Protecting Against Unauthorized Personalization Through Trajectory Shifted Perturbations](https://arxiv.org/abs/2510.03089)
*Naresh Kumar Devulapally,Shruti Agarwal,Tejas Gokhale,Vishnu Suresh Lokhande*

Main category: cs.CV

TL;DR: 通过在潜在空间中进行扰动，提出了一种新的“不可学习”样本生成方法，以防御文本到图像扩散模型的个性化，同时保持高视觉保真度。


<details>
  <summary>Details</summary>
Motivation: 现有的“不可学习”样本生成方法（如像素空间扰动）会导致图像出现噪声和伪影，影响视觉保真度。需要一种在保持视觉效果的同时，又能有效防御模型个性化的新方法。

Method: 提出了一种新颖的基于模型的潜在空间扰动策略。该方法通过在去噪和反演之间交替进行，同时修改去噪轨迹的起点来实现。这种轨迹偏移采样确保了扰动后的图像在保持视觉保真度的同时，能够抵抗下游生成模型的反演和个性化。

Result: 在四个基准数据集上验证了该方法。结果表明，与现有方法相比，该方法在不可感知性（在PSNR、SSIM和FID等感知指标上提高了约8%-10%）和鲁棒性（在五种对抗性设置下平均提高了约10%）方面取得了显著的改进。

Conclusion: 所提出的基于潜在空间扰动的方法可以有效地集成到潜在扩散模型（LDMs）框架中，为防御未经授权的模型改编提供了一种实用且不可察觉的防御手段，同时在不可感知性和鲁棒性方面均优于现有技术。

Abstract: Text-to-image diffusion models have demonstrated remarkable effectiveness in
rapid and high-fidelity personalization, even when provided with only a few
user images. However, the effectiveness of personalization techniques has lead
to concerns regarding data privacy, intellectual property protection, and
unauthorized usage. To mitigate such unauthorized usage and model replication,
the idea of generating ``unlearnable'' training samples utilizing image
poisoning techniques has emerged. Existing methods for this have limited
imperceptibility as they operate in the pixel space which results in images
with noise and artifacts. In this work, we propose a novel model-based
perturbation strategy that operates within the latent space of diffusion
models. Our method alternates between denoising and inversion while modifying
the starting point of the denoising trajectory: of diffusion models. This
trajectory-shifted sampling ensures that the perturbed images maintain high
visual fidelity to the original inputs while being resistant to inversion and
personalization by downstream generative models. This approach integrates
unlearnability into the framework of Latent Diffusion Models (LDMs), enabling a
practical and imperceptible defense against unauthorized model adaptation. We
validate our approach on four benchmark datasets to demonstrate robustness
against state-of-the-art inversion attacks. Results demonstrate that our method
achieves significant improvements in imperceptibility ($\sim 8 \% -10\%$ on
perceptual metrics including PSNR, SSIM, and FID) and robustness ( $\sim 10\%$
on average across five adversarial settings), highlighting its effectiveness in
safeguarding sensitive data.

</details>


### [46] [GeoComplete: Geometry-Aware Diffusion for Reference-Driven Image Completion](https://arxiv.org/abs/2510.03110)
*Beibei Lin,Tingting Chen,Robby T. Tan*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Reference-driven image completion, which restores missing regions in a target
view using additional images, is particularly challenging when the target view
differs significantly from the references. Existing generative methods rely
solely on diffusion priors and, without geometric cues such as camera pose or
depth, often produce misaligned or implausible content. We propose GeoComplete,
a novel framework that incorporates explicit 3D structural guidance to enforce
geometric consistency in the completed regions, setting it apart from prior
image-only approaches. GeoComplete introduces two key ideas: conditioning the
diffusion process on projected point clouds to infuse geometric information,
and applying target-aware masking to guide the model toward relevant reference
cues. The framework features a dual-branch diffusion architecture. One branch
synthesizes the missing regions from the masked target, while the other
extracts geometric features from the projected point cloud. Joint
self-attention across branches ensures coherent and accurate completion. To
address regions visible in references but absent in the target, we project the
target view into each reference to detect occluded areas, which are then masked
during training. This target-aware masking directs the model to focus on useful
cues, enhancing performance in difficult scenarios. By integrating a
geometry-aware dual-branch diffusion architecture with a target-aware masking
strategy, GeoComplete offers a unified and robust solution for
geometry-conditioned image completion. Experiments show that GeoComplete
achieves a 17.1 PSNR improvement over state-of-the-art methods, significantly
boosting geometric accuracy while maintaining high visual quality.

</details>


### [47] [Taming Text-to-Sounding Video Generation via Advanced Modality Condition and Interaction](https://arxiv.org/abs/2510.03117)
*Kaisi Guan,Xihua Wang,Zhengfeng Lai,Xin Cheng,Peng Zhang,XiaoJiang Liu,Ruihua Song,Meng Cao*

Main category: cs.CV

TL;DR: 本项目提出了一种名为BridgeDiT的新型双塔扩散变换器，用于解决文本到视频声音（T2SV）生成中的模态干扰和跨模态特征交互不明确的挑战。通过引入分层视觉基础字幕（HVGC）框架生成分离的视频和音频字幕，并结合双交叉注意力（DCA）机制实现对称的双向信息交换，最终在三个基准数据集上取得了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 文本到视频声音（T2SV）生成面临两个主要挑战：1.单一的共享文本描述（视频和音频使用相同的文本）会导致模态干扰，混淆预训练骨干网络；2.跨模态特征交互的最佳机制尚不明确。

Method: 1.提出分层视觉基础字幕（HVGC）框架，生成分离的视频字幕和音频字幕，以消除条件设置阶段的模态干扰。 2.基于HVGC，引入了名为BridgeDiT的新型双塔扩散变换器，该变换器采用双交叉注意力（DCA）机制，作为连接视频和音频模态的“桥梁”，实现语义和时间上的同步。

Result: 在三个基准数据集上的广泛实验以及人类评估表明，该方法在大多数指标上均取得了最先进的成果。消融研究进一步验证了所提出方法的有效性。

Conclusion: 所提出的HVGC框架和BridgeDiT模型（结合DCA机制）能够有效解决T2SV生成中的模态干扰和跨模态交互问题，实现了语义和时间上的同步，并取得了最先进的性能。该研究为未来的T2SV任务提供了关键的见解。

Abstract: This study focuses on a challenging yet promising task,
Text-to-Sounding-Video (T2SV) generation, which aims to generate a video with
synchronized audio from text conditions, meanwhile ensuring both modalities are
aligned with text. Despite progress in joint audio-video training, two critical
challenges still remain unaddressed: (1) a single, shared text caption where
the text for video is equal to the text for audio often creates modal
interference, confusing the pretrained backbones, and (2) the optimal mechanism
for cross-modal feature interaction remains unclear. To address these
challenges, we first propose the Hierarchical Visual-Grounded Captioning (HVGC)
framework that generates pairs of disentangled captions, a video caption, and
an audio caption, eliminating interference at the conditioning stage. Based on
HVGC, we further introduce BridgeDiT, a novel dual-tower diffusion transformer,
which employs a Dual CrossAttention (DCA) mechanism that acts as a robust
``bridge" to enable a symmetric, bidirectional exchange of information,
achieving both semantic and temporal synchronization. Extensive experiments on
three benchmark datasets, supported by human evaluations, demonstrate that our
method achieves state-of-the-art results on most metrics. Comprehensive
ablation studies further validate the effectiveness of our contributions,
offering key insights for the future T2SV task. All the codes and checkpoints
will be publicly released.

</details>


### [48] [HAVIR: HierArchical Vision to Image Reconstruction using CLIP-Guided Versatile Diffusion](https://arxiv.org/abs/2510.03122)
*Shiyi Zhang,Dong Liang,Hairong Zheng,Yihang Zhou*

Main category: cs.CV

TL;DR: HAVIR模型通过分离视觉皮层为两个层级区域，并分别提取结构和语义特征，再整合到Versatile Diffusion模型中，能够更准确地从大脑活动中重建复杂的视觉信息，优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有从大脑活动重建视觉信息的方法在处理复杂视觉刺激时面临挑战，原因在于自然场景的低层特征异质性和高层特征的语义纠缠。

Method: 受视觉皮层分层表示理论启发，提出HAVIR模型，将视觉皮层分为两个层级区域：结构生成器（从空间处理体素提取结构信息并转换为潜在扩散先验）和语义提取器（将语义处理体素转换为CLIP嵌入）。最后，通过Versatile Diffusion模型整合这两个部分以合成最终图像。

Result: 实验结果表明，HAVIR模型能够提升重建图像的结构和语义质量，尤其在复杂场景下表现优于现有模型。

Conclusion: HAVIR模型通过其分层特征提取和整合方法，成功地解决了从大脑活动重建复杂视觉信息的挑战，并在实验中证明了其优越性。

Abstract: The reconstruction of visual information from brain activity fosters
interdisciplinary integration between neuroscience and computer vision.
However, existing methods still face challenges in accurately recovering highly
complex visual stimuli. This difficulty stems from the characteristics of
natural scenes: low-level features exhibit heterogeneity, while high-level
features show semantic entanglement due to contextual overlaps. Inspired by the
hierarchical representation theory of the visual cortex, we propose the HAVIR
model, which separates the visual cortex into two hierarchical regions and
extracts distinct features from each. Specifically, the Structural Generator
extracts structural information from spatial processing voxels and converts it
into latent diffusion priors, while the Semantic Extractor converts semantic
processing voxels into CLIP embeddings. These components are integrated via the
Versatile Diffusion model to synthesize the final image. Experimental results
demonstrate that HAVIR enhances both the structural and semantic quality of
reconstructions, even in complex scenes, and outperforms existing models.

</details>


### [49] [SpineBench: A Clinically Salient, Level-Aware Benchmark Powered by the SpineMed-450k Corpus](https://arxiv.org/abs/2510.03160)
*Ming Zhao,Wenhui Dong,Yang Zhang,Xiang Zheng,Zhonghao Zhang,Zian Zhou,Yunzhi Guan,Liukun Xu,Wei Peng,Zhaoyang Gong,Zhicheng Zhang,Dachuan Li,Xiaosheng Ma,Yuli Ma,Jianing Ni,Changjiang Jiang,Lixia Tian,Qixin Chen,Kaishun Xia,Pingping Liu,Tongshun Zhang,Zhiqiang Liu,Zhongan Bi,Chenyang Si,Tiansheng Sun,Caifeng Shan*

Main category: cs.CV

TL;DR: 该研究提出了SpineMed生态系统，包括SpineMed-450k数据集和SpineBench评估框架，旨在解决脊柱疾病AI诊断中缺乏针对脊椎水平的多模态数据集和临床指导数据的问 题。该数据集包含超过45万个指令实例，涵盖X射线、CT和MRI图像，并采用临床医生参与的流程和两阶段LLM生成方法进行构建。SpineBench则从临床角度评估模型在脊椎水平识别、病理评估和手术规划等方面的能力。实验结果表明，现有的大型视觉-语言模型在精细的、特定于脊椎水平的推理方面存在系统性缺陷，而使用SpineMed-450k进行微调的模型在各项任务上均表现出显著提升，并得到临床医生的认可。


<details>
  <summary>Details</summary>
Motivation: 脊柱疾病影响全球数亿人，是导致残疾的主要原因，但现有AI辅助诊断受限于缺乏针对脊椎水平的多模态数据集和临床指导数据，阻碍了在X射线、CT和MRI图像之间进行复杂推理的进展。

Method: 引入SpineMed生态系统，包括：1. SpineMed-450k：一个大规模、专门为脊椎水平推理设计的跨模态数据集，包含超过45万个指令实例，通过结合教科书、指南、公开数据集和医院病例，并采用临床医生参与的两阶段LLM生成方法（起草和修订）来确保数据质量和可追溯性。2. SpineBench：一个基于临床的评估框架，用于评估模型在脊椎水平识别、病理评估和手术规划等临床相关维度上的表现。

Result: 对几个人类先进的大型视觉-语言模型（LVLMs）在SpineBench上的全面评估揭示了它们在细粒度的、特定于脊椎水平的推理方面存在系统性弱点。相比之下，在SpineMed-450k上进行微调的模型在所有任务上都表现出持续且显著的改进。临床评估证实了模型输出在诊断清晰度和实际效用方面有所提升。

Conclusion: SpineMed生态系统通过提供大规模、高质量、可追溯的脊椎水平多模态数据集（SpineMed-450k）和临床相关的评估框架（SpineBench），有效解决了现有AI诊断方法的局限性。基于该数据集微调的模型在脊柱疾病的AI诊断任务上取得了显著的性能提升，证明了其临床应用价值。

Abstract: Spine disorders affect 619 million people globally and are a leading cause of
disability, yet AI-assisted diagnosis remains limited by the lack of
level-aware, multimodal datasets. Clinical decision-making for spine disorders
requires sophisticated reasoning across X-ray, CT, and MRI at specific
vertebral levels. However, progress has been constrained by the absence of
traceable, clinically-grounded instruction data and standardized,
spine-specific benchmarks. To address this, we introduce SpineMed, an ecosystem
co-designed with practicing spine surgeons. It features SpineMed-450k, the
first large-scale dataset explicitly designed for vertebral-level reasoning
across imaging modalities with over 450,000 instruction instances, and
SpineBench, a clinically-grounded evaluation framework. SpineMed-450k is
curated from diverse sources, including textbooks, guidelines, open datasets,
and ~1,000 de-identified hospital cases, using a clinician-in-the-loop pipeline
with a two-stage LLM generation method (draft and revision) to ensure
high-quality, traceable data for question-answering, multi-turn consultations,
and report generation. SpineBench evaluates models on clinically salient axes,
including level identification, pathology assessment, and surgical planning.
Our comprehensive evaluation of several recently advanced large vision-language
models (LVLMs) on SpineBench reveals systematic weaknesses in fine-grained,
level-specific reasoning. In contrast, our model fine-tuned on SpineMed-450k
demonstrates consistent and significant improvements across all tasks.
Clinician assessments confirm the diagnostic clarity and practical utility of
our model's outputs.

</details>


### [50] [UniShield: An Adaptive Multi-Agent Framework for Unified Forgery Image Detection and Localization](https://arxiv.org/abs/2510.03161)
*Qing Huang,Zhipei Xu,Xuanyu Zhang,Jian Zhang*

Main category: cs.CV

TL;DR: 该论文提出了UniShield，一个多智能体统一系统，用于跨不同领域（图像篡改、文档篡改、DeepFake和AI生成图像）检测和定位图像伪造。


<details>
  <summary>Details</summary>
Motivation: 现有伪造图像检测方法泛化能力差，缺乏统一的自适应框架，限制了实际应用。

Method: UniShield集成了感知智能体和检测智能体。感知智能体分析图像特征并动态选择检测模型，检测智能体整合多个专家检测器并生成可解释的报告。

Result: UniShield在广泛的实验中取得了最先进的成果，在检测和定位伪造图像方面优于现有的统一方法和特定领域的方法。

Conclusion: UniShield是一个实用的、自适应的和可扩展的系统，可以有效地检测和定位跨不同领域的图像伪造。

Abstract: With the rapid advancements in image generation, synthetic images have become
increasingly realistic, posing significant societal risks, such as
misinformation and fraud. Forgery Image Detection and Localization (FIDL) thus
emerges as essential for maintaining information integrity and societal
security. Despite impressive performances by existing domain-specific detection
methods, their practical applicability remains limited, primarily due to their
narrow specialization, poor cross-domain generalization, and the absence of an
integrated adaptive framework. To address these issues, we propose UniShield,
the novel multi-agent-based unified system capable of detecting and localizing
image forgeries across diverse domains, including image manipulation, document
manipulation, DeepFake, and AI-generated images. UniShield innovatively
integrates a perception agent with a detection agent. The perception agent
intelligently analyzes image features to dynamically select suitable detection
models, while the detection agent consolidates various expert detectors into a
unified framework and generates interpretable reports. Extensive experiments
show that UniShield achieves state-of-the-art results, surpassing both existing
unified approaches and domain-specific detectors, highlighting its superior
practicality, adaptiveness, and scalability.

</details>


### [51] [Dynamic Prompt Generation for Interactive 3D Medical Image Segmentation Training](https://arxiv.org/abs/2510.03189)
*Tidiane Camaret Ndir,Alexander Pfefferle,Robin Tibor Schirrmeister*

Main category: cs.CV

TL;DR: 提出了一种结合动态体积提示生成和内容感知自适应裁剪的训练策略，用于交互式3D生物医学图像分割，以优化图像编码器的使用并解决单GPU上的计算挑战。该方法在“用于交互式3D生物医学图像分割的基础模型”竞赛中取得了0.6385的平均最终Dice分数，0.6614的归一化表面距离，以及2.4799（Dice）和2.5671（NSD）的曲线下面积指标。


<details>
  <summary>Details</summary>
Motivation: 当前的交互式3D生物医学图像分割模型要么缺乏体积感知能力，要么交互能力有限，无法满足高效模型的需求。

Method: 结合动态体积提示生成和内容感知自适应裁剪的训练策略，优化图像编码器的使用，并模拟用户交互模式以解决单GPU上的计算挑战。

Result: 在“用于交互式3D生物医学图像分割的基础模型”竞赛中，平均最终Dice得分为0.6385，归一化表面距离为0.6614，曲线下面积指标为2.4799（Dice）和2.5671（NSD）。

Conclusion: 该方法在竞赛中表现出色，表明其在交互式3D生物医学图像分割方面的有效性。

Abstract: Interactive 3D biomedical image segmentation requires efficient models that
can iteratively refine predictions based on user prompts. Current foundation
models either lack volumetric awareness or suffer from limited interactive
capabilities. We propose a training strategy that combines dynamic volumetric
prompt generation with content-aware adaptive cropping to optimize the use of
the image encoder. Our method simulates realistic user interaction patterns
during training while addressing the computational challenges of learning from
sequential refinement feedback on a single GPU. For efficient training, we
initialize our network using the publicly available weights from the
nnInteractive segmentation model. Evaluation on the \textbf{Foundation Models
for Interactive 3D Biomedical Image Segmentation} competition demonstrates
strong performance with an average final Dice score of 0.6385, normalized
surface distance of 0.6614, and area-under-the-curve metrics of 2.4799 (Dice)
and 2.5671 (NSD).

</details>


### [52] [Product-Quantised Image Representation for High-Quality Image Synthesis](https://arxiv.org/abs/2510.03191)
*Denis Zavadski,Nikita Philip Tatsch,Carsten Rother*

Main category: cs.CV

TL;DR: PQGAN将乘积量化（PQ）集成到VQGAN框架中，显著提高了图像生成和重建的性能，并能加速预训练的扩散模型。


<details>
  <summary>Details</summary>
Motivation: 尽管PQ是一种经典的向量编码方法，但其在高质量图像生成中的潜在表示应用有限。本研究旨在将PQ整合到VQGAN框架中，以提升图像生成和重建的保真度。

Method: PQGAN将乘积量化（PQ）方法集成到VQGAN的向量量化（VQ）框架中，并通过对码本大小、嵌入维度和子空间分解的相互作用进行深入分析，优化了PQ的性能。

Result: PQGAN在重建性能上取得了显著的提升，PSNR得分达到37dB（相较于之前的27dB），FID、LPIPS和CMMD得分降低了96%。研究还发现了VQ和PQ在嵌入维度扩展方面表现相反的趋势，并为PQ提供了性能指导。

Conclusion: PQGAN通过将PQ集成到VQGAN框架中，显著提高了图像重建和生成质量，并能加速或提升预训练扩散模型的性能，展示了其作为图像合成中离散潜在表示的强大扩展潜力。

Abstract: Product quantisation (PQ) is a classical method for scalable vector encoding,
yet it has seen limited usage for latent representations in high-fidelity image
generation. In this work, we introduce PQGAN, a quantised image autoencoder
that integrates PQ into the well-known vector quantisation (VQ) framework of
VQGAN. PQGAN achieves a noticeable improvement over state-of-the-art methods in
terms of reconstruction performance, including both quantisation methods and
their continuous counterparts. We achieve a PSNR score of 37dB, where prior
work achieves 27dB, and are able to reduce the FID, LPIPS, and CMMD score by up
to 96%. Our key to success is a thorough analysis of the interaction between
codebook size, embedding dimensionality, and subspace factorisation, with
vector and scalar quantisation as special cases. We obtain novel findings, such
that the performance of VQ and PQ behaves in opposite ways when scaling the
embedding dimension. Furthermore, our analysis shows performance trends for PQ
that help guide optimal hyperparameter selection. Finally, we demonstrate that
PQGAN can be seamlessly integrated into pre-trained diffusion models. This
enables either a significantly faster and more compute-efficient generation, or
a doubling of the output resolution at no additional cost, positioning PQ as a
strong extension for discrete latent representation in image synthesis.

</details>


### [53] [Memory Forcing: Spatio-Temporal Memory for Consistent Scene Generation on Minecraft](https://arxiv.org/abs/2510.03198)
*Junchao Huang,Xinting Hu,Boyao Han,Shaoshuai Shi,Zhuotao Tian,Tianyu He,Li Jiang*

Main category: cs.CV

TL;DR: Memory Forcing是一个学习框架，通过结合训练协议和几何索引空间内存，解决了在有限计算预算下，视频扩散模型在模拟Minecraft等场景时面临的长期空间一致性与新场景生成质量之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 在有限计算预算下，视频扩散模型在模拟复杂场景（如Minecraft）时，需要在生成新场景和保持已探索区域的空间一致性之间取得平衡。纯时间记忆缺乏长期空间一致性，而空间记忆会影响新场景生成质量。

Method: 提出Memory Forcing框架，包含混合训练（Hybrid Training）和链式前向训练（Chained Forward Training）。混合训练引导模型在探索时依赖时间记忆，在重访时结合空间记忆；链式前向训练通过模型预测扩展了模型时序，鼓励模型依赖空间记忆以维持一致性。此外，还采用了点到帧检索（Point-to-Frame Retrieval）和增量3D重建（Incremental 3D Reconstruction）来高效管理和更新空间内存。

Result: Memory Forcing在长期空间一致性和生成质量方面均表现优于现有方法，同时在计算效率和处理长序列方面也表现出色。

Conclusion: Memory Forcing框架成功解决了视频扩散模型在模拟长期交互式场景中的关键挑战，通过创新的训练策略和内存管理机制，实现了高质量、高空间一致性的场景生成。

Abstract: Autoregressive video diffusion models have proved effective for world
modeling and interactive scene generation, with Minecraft gameplay as a
representative application. To faithfully simulate play, a model must generate
natural content while exploring new scenes and preserve spatial consistency
when revisiting explored areas. Under limited computation budgets, it must
compress and exploit historical cues within a finite context window, which
exposes a trade-off: Temporal-only memory lacks long-term spatial consistency,
whereas adding spatial memory strengthens consistency but may degrade new scene
generation quality when the model over-relies on insufficient spatial context.
We present Memory Forcing, a learning framework that pairs training protocols
with a geometry-indexed spatial memory. Hybrid Training exposes distinct
gameplay regimes, guiding the model to rely on temporal memory during
exploration and incorporate spatial memory for revisits. Chained Forward
Training extends autoregressive training with model rollouts, where chained
predictions create larger pose variations and encourage reliance on spatial
memory for maintaining consistency. Point-to-Frame Retrieval efficiently
retrieves history by mapping currently visible points to their source frames,
while Incremental 3D Reconstruction maintains and updates an explicit 3D cache.
Extensive experiments demonstrate that Memory Forcing achieves superior
long-term spatial consistency and generative quality across diverse
environments, while maintaining computational efficiency for extended
sequences.

</details>


### [54] [MonSTeR: a Unified Model for Motion, Scene, Text Retrieval](https://arxiv.org/abs/2510.03200)
*Luca Collorone,Matteo Gioia,Massimiliano Pappa,Paolo Leoni,Giovanni Ficarra,Or Litany,Indro Spinelli,Fabio Galasso*

Main category: cs.CV

TL;DR: MonSTeR是一个首创的运动-场景-文本检索模型，用于评估骨骼运动（运动）、意图（文本）和上下文（场景）之间的一致性，并在各种任务中表现优于仅依赖单模态表示的三模态模型。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏评估骨骼运动（运动）、意图（文本）和上下文（场景）之间一致性的工具，而这种一致性是理解人类意图驱动的复杂环境运动的关键。

Method: MonSTeR通过构建一个统一的潜在空间，利用单模态和跨模态表示，捕捉模态之间的复杂依赖关系，从而实现灵活而鲁棒的跨任务检索。

Result: MonSTeR在检索任务中表现优于仅依赖单模态表示的三模态模型，并且检索分数与用户研究中人类的偏好一致。此外，MonSTeR的潜在空间在零样本场景对象放置和运动描述任务中也展现了通用性。

Conclusion: MonSTeR成功地解决了运动、场景和文本之间一致性评估的挑战，并在多项任务中取得了先进的性能，证明了其在理解和检索多模态数据方面的有效性。

Abstract: Intention drives human movement in complex environments, but such movement
can only happen if the surrounding context supports it. Despite the intuitive
nature of this mechanism, existing research has not yet provided tools to
evaluate the alignment between skeletal movement (motion), intention (text),
and the surrounding context (scene). In this work, we introduce MonSTeR, the
first MOtioN-Scene-TExt Retrieval model. Inspired by the modeling of
higher-order relations, MonSTeR constructs a unified latent space by leveraging
unimodal and cross-modal representations. This allows MonSTeR to capture the
intricate dependencies between modalities, enabling flexible but robust
retrieval across various tasks. Our results show that MonSTeR outperforms
trimodal models that rely solely on unimodal representations. Furthermore, we
validate the alignment of our retrieval scores with human preferences through a
dedicated user study. We demonstrate the versatility of MonSTeR's latent space
on zero-shot in-Scene Object Placement and Motion Captioning. Code and
pre-trained models are available at github.com/colloroneluca/MonSTeR.

</details>


### [55] [Test-Time Defense Against Adversarial Attacks via Stochastic Resonance of Latent Ensembles](https://arxiv.org/abs/2510.03224)
*Dong Lao,Yuxiang Zhang,Haniyeh Ehsani Oskouie,Yangchao Wu,Alex Wong,Stefano Soatto*

Main category: cs.CV

TL;DR: 我们提出了一种名为“combating noise with noise”的测试时防御机制，利用随机共振来增强模型对对抗性攻击的鲁棒性，同时最大限度地减少信息损失。该方法引入微小的平移扰动，对齐特征嵌入，并进行聚合，然后映射回原始图像，最终实现完全无需训练、架构无关且攻击无关的防御。


<details>
  <summary>Details</summary>
Motivation: 现有的对抗性攻击防御方法（如特征过滤或平滑）可能导致信息损失。因此，需要一种新的方法来增强鲁棒性，同时最大限度地减少信息损失。

Method: 该方法通过引入微小的平移扰动来“combating noise with noise”，然后对齐变换后的特征嵌入，在映射回原始参考图像之前进行聚合。这可以用一个封闭形式的公式来表示，并且可以部署在各种现有的网络架构上，而无需引入额外的网络模块或针对特定攻击类型进行微调。

Result: 该方法在图像分类、立体匹配和光流等密集预测任务上实现了最先进的鲁棒性。与干净的（未扰动）性能相比，我们的方法在图像分类上恢复了高达68.1%的准确度损失，在立体匹配上恢复了71.9%，在光流上恢复了29.2%，在各种类型的对抗性攻击下均表现出这种效果。

Conclusion: 我们提出的防御机制是一种完全无需训练、架构无关且攻击无关的通用测试时防御方法，在各种任务上都具有很高的有效性和实用性。

Abstract: We propose a test-time defense mechanism against adversarial attacks:
imperceptible image perturbations that significantly alter the predictions of a
model. Unlike existing methods that rely on feature filtering or smoothing,
which can lead to information loss, we propose to "combat noise with noise" by
leveraging stochastic resonance to enhance robustness while minimizing
information loss. Our approach introduces small translational perturbations to
the input image, aligns the transformed feature embeddings, and aggregates them
before mapping back to the original reference image. This can be expressed in a
closed-form formula, which can be deployed on diverse existing network
architectures without introducing additional network modules or fine-tuning for
specific attack types. The resulting method is entirely training-free,
architecture-agnostic, and attack-agnostic. Empirical results show
state-of-the-art robustness on image classification and, for the first time,
establish a generic test-time defense for dense prediction tasks, including
stereo matching and optical flow, highlighting the method's versatility and
practicality. Specifically, relative to clean (unperturbed) performance, our
method recovers up to 68.1% of the accuracy loss on image classification, 71.9%
on stereo matching, and 29.2% on optical flow under various types of
adversarial attacks.

</details>


### [56] [MIXER: Mixed Hyperspherical Random Embedding Neural Network for Texture Recognition](https://arxiv.org/abs/2510.03228)
*Ricardo T. Fares,Lucas C. Ribas*

Main category: cs.CV

TL;DR: Mixer是一种新的随机神经网络，用于纹理表示学习，通过利用超球随机嵌入和双分支学习模块来捕捉通道内和通道间关系，并在纯纹理基准测试中取得了良好的效果。


<details>
  <summary>Details</summary>
Motivation: 现有的随机神经网络方法主要集中在改进交叉信息预测，而对整体网络架构的改进不大。本研究旨在提出一种新的随机神经网络架构，以提高纹理表示学习的效果。

Method: Mixer方法结合了超球随机嵌入和双分支学习模块，用于捕捉纹理表示中的通道内和通道间关系，并通过新制定的优化问题来构建丰富的纹理表示。

Result: 在多个具有不同特征和挑战的纯纹理基准测试中，Mixer方法取得了令人瞩目的成果。

Conclusion: Mixer是一种有效的随机神经网络架构，能够学习丰富的纹理表示，并在各种纹理识别任务中表现出色。

Abstract: Randomized neural networks for representation learning have consistently
achieved prominent results in texture recognition tasks, effectively combining
the advantages of both traditional techniques and learning-based approaches.
However, existing approaches have so far focused mainly on improving
cross-information prediction, without introducing significant advancements to
the overall randomized network architecture. In this paper, we propose Mixer, a
novel randomized neural network for texture representation learning. At its
core, the method leverages hyperspherical random embeddings coupled with a
dual-branch learning module to capture both intra- and inter-channel
relationships, further enhanced by a newly formulated optimization problem for
building rich texture representations. Experimental results have shown the
interesting results of the proposed approach across several pure texture
benchmarks, each with distinct characteristics and challenges. The source code
will be available upon publication.

</details>


### [57] [Improving GUI Grounding with Explicit Position-to-Coordinate Mapping](https://arxiv.org/abs/2510.03230)
*Suyuchen Wang,Tianyu Zhang,Ahmed Masry,Christopher Pal,Spandana Gella,Bang Liu,Perouz Taslakian*

Main category: cs.CV

TL;DR: 该研究提出RULER tokens和I-MRoPE来解决GUI grounding中的patch-to-pixel映射问题，提高了模型在不同分辨率下的定位准确性。


<details>
  <summary>Details</summary>
Motivation: 当前GUI grounding任务在将自然语言指令映射到像素坐标时存在困难，尤其是在高分辨率屏幕上，主要瓶颈在于patch-to-pixel映射的不可靠性，现有方法直接从视觉特征生成坐标文本，导致模型需要隐式推断位置到像素的映射，从而在新的分辨率下精度下降。

Method: 提出两种方法：1. RULER tokens作为显式的坐标标记，允许模型像地图上的网格线一样引用位置，并进行调整而非从头生成坐标。2. 交错MRoPE (I-MRoPE)通过确保宽度和高度维度得到同等表示来改进空间编码，解决标准位置方案的对称性问题。

Result: 在ScreenSpot, ScreenSpot-V2和ScreenSpot-Pro数据集上进行了实验，证明了在GUI grounding准确性方面持续提升，尤其是在高分辨率界面上改进最为显著。

Conclusion: 通过提供显式的空间引导，而非依赖隐式学习，该方法能够实现更可靠的跨不同分辨率和平台的GUI自动化。

Abstract: GUI grounding, the task of mapping natural-language instructions to pixel
coordinates, is crucial for autonomous agents, yet remains difficult for
current VLMs. The core bottleneck is reliable patch-to-pixel mapping, which
breaks when extrapolating to high-resolution displays unseen during training.
Current approaches generate coordinates as text tokens directly from visual
features, forcing the model to infer complex position-to-pixel mappings
implicitly; as a result, accuracy degrades and failures proliferate on new
resolutions. We address this with two complementary innovations. First, RULER
tokens serve as explicit coordinate markers, letting the model reference
positions similar to gridlines on a map and adjust rather than generate
coordinates from scratch. Second, Interleaved MRoPE (I-MRoPE) improves spatial
encoding by ensuring that width and height dimensions are represented equally,
addressing the asymmetry of standard positional schemes. Experiments on
ScreenSpot, ScreenSpot-V2, and ScreenSpot-Pro show consistent gains in
grounding accuracy, with the largest improvements on high-resolution
interfaces. By providing explicit spatial guidance rather than relying on
implicit learning, our approach enables more reliable GUI automation across
diverse resolutions and platforms.

</details>


### [58] [LEAML: Label-Efficient Adaptation to Out-of-Distribution Visual Tasks for Multimodal Large Language Models](https://arxiv.org/abs/2510.03232)
*Ci-Siang Lin,Min-Hung Chen,Yu-Yang Sheng,Yu-Chiang Frank Wang*

Main category: cs.CV

TL;DR: LEAML是一个标签高效的医学影像VQA适应框架，利用少量标记的VQA样本和大量的未标记图像，通过QA生成器和字幕蒸馏生成领域相关的伪问答对，并选择性地更新最相关的神经元以适应领域知识。


<details>
  <summary>Details</summary>
Motivation: 现有的MLLMs在通用视觉基准上表现良好，但在医学影像等专业领域面临分布外（OOD）任务的挑战，因为这些领域的标记数据有限且成本高昂。

Method: LEAML框架利用少量标记的VQA样本和大量未标记图像。通过一个QA生成器，并以字幕蒸馏为正则化，为未标记数据生成领域相关的伪问答对。该方法选择性地更新与问答最相关的神经元，使QA生成器在蒸馏过程中能够高效地获取领域特定知识。

Result: 在胃肠内窥镜和体育VQA任务上进行实验，结果显示LEAML在最小监督的情况下，始终优于标准的微调方法。

Conclusion: LEAML框架在标签数据有限的情况下，能够有效地适应医学影像等专业领域，并且在VQA任务上取得了优于标准微调的性能。

Abstract: Multimodal Large Language Models (MLLMs) have achieved strong performance on
general visual benchmarks but struggle with out-of-distribution (OOD) tasks in
specialized domains such as medical imaging, where labeled data is limited and
expensive. We introduce LEAML, a label-efficient adaptation framework that
leverages both scarce labeled VQA samples and abundant unlabeled images. Our
approach generates domain-relevant pseudo question-answer pairs for unlabeled
data using a QA generator regularized by caption distillation. Importantly, we
selectively update only those neurons most relevant to question-answering,
enabling the QA Generator to efficiently acquire domain-specific knowledge
during distillation. Experiments on gastrointestinal endoscopy and sports VQA
demonstrate that LEAML consistently outperforms standard fine-tuning under
minimal supervision, highlighting the effectiveness of our proposed LEAML
framework.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [59] [AMANDA: Agentic Medical Knowledge Augmentation for Data-Efficient Medical Visual Question Answering](https://arxiv.org/abs/2510.02328)
*Ziqing Wang,Chengsheng Mao,Xiaole Wen,Yuan Luo,Kaize Ding*

Main category: cs.CL

TL;DR: Med-MLLMs在低资源设置下存在内在和外在的推理瓶颈，AMANDA框架通过LLM代理进行医疗知识增强，解决了这些问题，在Med-VQA基准测试中取得了显著的改进。


<details>
  <summary>Details</summary>
Motivation: 现有的Med-MLLMs在低资源设置下，由于忽略医学图像细节（内在瓶颈）和未能结合专业医学知识（外在瓶颈）而常常失败。

Method: 提出AMANDA，一个免训练的代理框架，通过LLM代理进行医疗知识增强。内在知识增强侧重于由粗到细的问题分解以进行全面诊断，外在知识增强通过生物大酮知识图谱检索来指导推理过程。

Result: 在八个Med-VQA基准测试中，AMANDA在零样本和少样本的Med-VQA设置下均取得了显著的改进。

Conclusion: AMANDA框架通过结合内在和外在的知识增强，有效解决了Med-MLLMs在低资源设置下的推理瓶颈，并在Med-VQA任务上表现出色。

Abstract: Medical Multimodal Large Language Models (Med-MLLMs) have shown great promise
in medical visual question answering (Med-VQA). However, when deployed in
low-resource settings where abundant labeled data are unavailable, existing
Med-MLLMs commonly fail due to their medical reasoning capability bottlenecks:
(i) the intrinsic reasoning bottleneck that ignores the details from the
medical image; (ii) the extrinsic reasoning bottleneck that fails to
incorporate specialized medical knowledge. To address those limitations, we
propose AMANDA, a training-free agentic framework that performs medical
knowledge augmentation via LLM agents. Specifically, our intrinsic medical
knowledge augmentation focuses on coarse-to-fine question decomposition for
comprehensive diagnosis, while extrinsic medical knowledge augmentation grounds
the reasoning process via biomedical knowledge graph retrieval. Extensive
experiments across eight Med-VQA benchmarks demonstrate substantial
improvements in both zero-shot and few-shot Med-VQA settings. The code is
available at https://github.com/REAL-Lab-NU/AMANDA.

</details>


### [60] [CLARITY: Clinical Assistant for Routing, Inference, and Triage](https://arxiv.org/abs/2510.02463)
*Vladimir Shaposhnikov,Aleksandr Nesterov,Ilia Kopanichuk,Ivan Bakulin,Egor Zhelvakov,Ruslan Abramov,Ekaterina Tsapieva,Dmitry V. Dylov,Ivan Oseledets*

Main category: cs.CL

TL;DR: CLARITY是一个AI驱动的平台，用于患者分诊、临床咨询和病情评估，结合了有限状态机和大型语言模型，实现了高效、准确的医疗路由。


<details>
  <summary>Details</summary>
Motivation: CLARITY旨在解决患者分诊、临床咨询和病情评估中的效率和准确性问题，通过AI技术优化医疗流程。

Method: CLARITY采用混合架构，结合了用于结构化对话流程的有限状态机（FSM）和使用大型语言模型（LLM）分析症状、优先分诊的协作代理。该平台基于模块化微服务框架构建。

Result: CLARITY已成功集成到全国性的大型医院IT平台，在两个月内完成了超过55,000次用户对话，其中2,500次经过专家标注进行验证。结果显示，CLARITY在首次尝试路由的精确度方面超越了人类水平，且咨询耗时缩短了高达3倍。

Conclusion: CLARITY在实际应用中表现出超越人类的性能，能够高效、准确地进行患者分诊和路由，显著缩短了医疗咨询时间，证明了其在医疗保健领域的应用价值。

Abstract: We present CLARITY (Clinical Assistant for Routing, Inference, and Triage),
an AI-driven platform designed to facilitate patient-to-specialist routing,
clinical consultations, and severity assessment of patients' conditions. Its
hybrid architecture combines a Finite State Machine (FSM) for structured
dialogue flows with collaborative agents that employ Large Language Model (LLM)
to analyze symptoms and prioritize referrals to appropriate specialists. Built
on a modular microservices framework, CLARITY ensures safe, efficient, and
robust performance, flexible and readily scalable to meet the demands of
existing workflows and IT solutions in healthcare.
  We report integration of our clinical assistant into a large-scale
nation-wide inter-hospital IT platform, with over 55,000 content-rich user
dialogues completed within the two months of deployment, 2,500 of which were
expert-annotated for a consequent validation. The validation results show that
CLARITY surpasses human-level performance in terms of the first-attempt routing
precision, naturally requiring up to 3 times shorter duration of the
consultation than with a human.

</details>


### [61] [Hallucination reduction with CASAL: Contrastive Activation Steering For Amortized Learning](https://arxiv.org/abs/2510.02324)
*Wannan Yang,Xinchi Qiu,Lei Yu,Yuchen Zhang,Oliver Aobo Yang,Narine Kokhlikyan,Nicola Cancedda,Diego Garcia-Olano*

Main category: cs.CL

TL;DR: CASAL是一种有效的算法，通过对比激活来减少大型语言模型的幻觉，并且效率很高。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）虽然能力强大，但经常出现幻觉，提供不正确但看似可信的答案。先前的研究表明，模型会对其自身知识进行线性编码，并且激活引导可以减少幻觉，但这些方法需要在推理过程中进行实时监控和干预。

Method: CASAL（Contrastive Activation Steering for Amortized Learning）算法将激活引导的优势直接整合到模型的权重中。它通过训练一个单一 Transformer 层的子模块来实现，从而在模型训练后就能回答其已知的问题，并避免回答其未知的问题。

Result: CASAL 在多个短文本问答基准测试中，将幻觉减少了 30%-40%。与基于 LoRA 的基线（如 SFT 和 DPO）相比，CASAL 在计算效率上提高了 30 倍，在数据效率上提高了 20 倍，并且在分布外（OOD）域上表现出良好的泛化能力。此外，CASAL 还可以缓解文本和视觉-语言模型中的幻觉问题，并且是首个被证明对密集和混合专家（MoE）模型都有效的基于引导的训练方法。

Conclusion: CASAL 是一种高效且实用的算法，通过将可解释性与摊销优化相结合，显著减少了大型语言模型的幻觉，同时提高了训练效率和泛化能力，为在生产系统中部署 LLM 提供了一条有前景的途径。

Abstract: Large Language Models (LLMs) exhibit impressive capabilities but often
hallucinate, confidently providing incorrect answers instead of admitting
ignorance. Prior work has shown that models encode linear representations of
their own knowledge and that activation steering can reduce hallucinations.
These approaches, however, require real-time monitoring and intervention during
inference. We introduce Contrastive Activation Steering for Amortized Learning
(CASAL), an efficient algorithm that connects interpretability with amortized
optimization. CASAL directly bakes the benefits of activation steering into
model's weights. Once trained, LLMs answer questions they know while abstaining
from answering those they do not. CASAL's light-weight design requires training
only a submodule of a single transformer layer and yet reduces hallucination by
30%-40% across multiple short-form QA benchmarks. CASAL is 30x more
compute-efficient and 20x more data-efficient than strong LoRA-based baselines
such as SFT and DPO, boosting its practical applicability in data scarce
domains. Importantly, CASAL also generalizes effectively to out-of-distribution
(OOD) domains. We showcase CASAL's flexibility in mitigating hallucinations in
both text-only and vision-language models. To our knowledge, CASAL is the first
steering-based training method that has been shown to be effective for both
dense and Mixture-of-Experts (MoE) models. CASAL represents a promising step
forward for applying interpretability-inspired method for practical deployment
in production systems.

</details>


### [62] [Breaking the MoE LLM Trilemma: Dynamic Expert Clustering with Structured Compression](https://arxiv.org/abs/2510.02345)
*Peijun Zhu,Ning Yang,Jiayu Wei,Jinghang Wu,Haijun Zhang*

Main category: cs.CL

TL;DR: 通过动态专家聚类和结构化压缩，解决MoE LLM的负载不均、参数冗余和通信开销问题，实现高达80%的参数减少和10-20%的吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: MoE LLM面临负载不均、参数冗余和通信开销的挑战。

Method: 提出一个统一框架，使用在线聚类程序和低秩残差适配器来动态重构模型架构。采用分层路由策略，并通过混合精度和动态卸载来减少内存消耗。

Result: 在GLUE和WikiText-103上，模型质量与标准MoE模型相当，同时参数减少约80%，吞吐量提高10-20%，专家负载方差降低超过三倍。

Conclusion: 结构重组是实现可扩展、高效且内存有效的MoE LLM的有效途径。

Abstract: Mixture-of-Experts (MoE) Large Language Models (LLMs) face a trilemma of load
imbalance, parameter redundancy, and communication overhead. We introduce a
unified framework based on dynamic expert clustering and structured compression
to address these issues cohesively. Our method employs an online clustering
procedure that periodically regroups experts using a fused metric of parameter
and activation similarity, which stabilizes expert utilization. To our
knowledge, this is one of the first frameworks to leverage the semantic
embedding capability of the router to dynamically reconfigure the model's
architecture during training for substantial efficiency gains. Within each
cluster, we decompose expert weights into a shared base matrix and extremely
low-rank residual adapters, achieving up to fivefold parameter reduction per
group while preserving specialization. This structure enables a two-stage
hierarchical routing strategy: tokens are first assigned to a cluster, then to
specific experts within it, drastically reducing the routing search space and
the volume of all-to-all communication. Furthermore, a heterogeneous precision
scheme, which stores shared bases in FP16 and residual factors in INT4, coupled
with dynamic offloading of inactive clusters, reduces peak memory consumption
to levels comparable to dense models. Evaluated on GLUE and WikiText-103, our
framework matches the quality of standard MoE models while reducing total
parameters by approximately 80%, improving throughput by 10% to 20%, and
lowering expert load variance by a factor of over three. Our work demonstrates
that structural reorganization is a principled path toward scalable, efficient,
and memory-effective MoE LLMs.

</details>


### [63] [Human Mobility Datasets Enriched With Contextual and Social Dimensions](https://arxiv.org/abs/2510.02333)
*Chiara Pugliese,Francesco Lettich,Guido Rocchietti,Chiara Renso,Fabio Pinelli*

Main category: cs.CL

TL;DR: 本文提出了两个包含语义增强的人类轨迹公开数据集，以及构建它们的方法。这些轨迹来自OpenStreetMap的GPS数据，并结合了停靠点、移动、兴趣点（POIs）、交通方式和天气数据等上下文信息。数据集的一个新颖之处在于加入了由大语言模型（LLMs）生成的合成、真实的社交媒体帖子，支持多模态和语义移动性分析。数据集提供表格和RDF格式，支持语义推理和FAIR数据实践，涵盖巴黎和纽约两个城市。开源的可复现流程允许定制数据集，并支持行为建模、移动性预测、知识图谱构建和LLM应用等研究任务。该资源是首个结合真实世界移动、结构化语义增强、LLM生成文本和语义网兼容性的可重用框架。


<details>
  <summary>Details</summary>
Motivation: 为了提供一个包含真实世界移动、语义增强、LLM生成文本和语义网兼容性的可重用框架，以支持多模态和语义移动性分析。

Method: 收集OpenStreetMap的GPS轨迹，并整合停靠点、移动、兴趣点（POIs）、交通方式和天气数据等上下文信息。利用大语言模型（LLMs）生成合成的、真实的社交媒体帖子。将处理后的数据整理成表格和RDF格式。

Result: 构建了两个结构不同的城市（巴黎和纽约）的人类轨迹公开数据集，其中包含GPS轨迹、上下文信息和LLM生成的社交媒体帖子，并提供开源的可复现流程。

Conclusion: 该资源是首个结合真实世界移动、结构化语义增强、LLM生成文本和语义网兼容性的可重用框架，为移动性研究提供了新的可能性。

Abstract: In this resource paper, we present two publicly available datasets of
semantically enriched human trajectories, together with the pipeline to build
them. The trajectories are publicly available GPS traces retrieved from
OpenStreetMap. Each dataset includes contextual layers such as stops, moves,
points of interest (POIs), inferred transportation modes, and weather data. A
novel semantic feature is the inclusion of synthetic, realistic social media
posts generated by Large Language Models (LLMs), enabling multimodal and
semantic mobility analysis. The datasets are available in both tabular and
Resource Description Framework (RDF) formats, supporting semantic reasoning and
FAIR data practices. They cover two structurally distinct, large cities: Paris
and New York. Our open source reproducible pipeline allows for dataset
customization, while the datasets support research tasks such as behavior
modeling, mobility prediction, knowledge graph construction, and LLM-based
applications. To our knowledge, our resource is the first to combine real-world
movement, structured semantic enrichment, LLM-generated text, and semantic web
compatibility in a reusable framework.

</details>


### [64] [Hallucination-Resistant, Domain-Specific Research Assistant with Self-Evaluation and Vector-Grounded Retrieval](https://arxiv.org/abs/2510.02326)
*Vivek Bhavsar,Joseph Ereifej,Aravanan Gurusami*

Main category: cs.CL

TL;DR: RA-FSM是一个基于GPT的研究助手，通过有限状态机控制，可以减少幻觉和错误引用，并提供有据可查的答案，在光子学领域进行了评估，优于Notebook LM和GPT API基线。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在文献综合方面有潜力，但存在幻觉和错误引用的问题，这限制了它们在专家工作流程中的应用。

Method: RA-FSM采用基于GPT的有限状态控制循环（相关性 -> 置信度 -> 知识），结合向量检索和确定性引用流程。该系统过滤范围外查询，评估答案的可回答性，分解问题，仅在需要时触发检索，并提供带有置信度标签和去重引用的答案。通过分级工作流程构建知识库，写入向量索引和关系存储。

Result: 在光子学领域的六个任务类别（分析推理、数值分析、方法批判、比较综合、事实提取和应用设计）中进行评估。领域专家在盲审A/B测试中更倾向于RA-FSM，认为其边界条件处理和证据使用更可靠。覆盖率和新颖性分析表明RA-FSM的探索性优于NLM，但有可调的延迟和成本开销。

Conclusion: RA-FSM通过有限状态机和相关技术，有效解决了大型语言模型在科学研究中的局限性，提供了透明、有引用的答案，适用于高风险技术工作，并且可以推广到其他科学领域。

Abstract: Large language models accelerate literature synthesis but can hallucinate and
mis-cite, limiting their usefulness in expert workflows. We present RA-FSM
(Research Assistant - Finite State Machine), a modular GPT-based research
assistant that wraps generation in a finite-state control loop: Relevance ->
Confidence -> Knowledge. The system is grounded in vector retrieval and a
deterministic citation pipeline. The controller filters out-of-scope queries,
scores answerability, decomposes questions, and triggers retrieval only when
needed, and emits answers with confidence labels and in-corpus, de-duplicated
references. A ranked-tier ingestion workflow constructs a domain knowledge base
from journals, conferences, indices, preprints, and patents, writing both to a
dense vector index and to a relational store of normalized metrics. We
implement the system for photonics and evaluate it on six task categories:
analytical reasoning, numerical analysis, methodological critique, comparative
synthesis, factual extraction, and application design. In blinded A/B reviews,
domain experts prefer RA-FSM to both a strong Notebook LM (NLM) and a vanilla
Default GPT API call single-pass baseline, citing stronger boundary-condition
handling and more defensible evidence use. Coverage and novelty analyses
indicate that RA-FSM explores beyond the NLM while incurring tunable latency
and cost overheads. The design emphasizes transparent, well-cited answers for
high-stakes technical work and is generalizable to other scientific domains.

</details>


### [65] [A Computational Framework for Interpretable Text-Based Personality Assessment from Social Media](https://arxiv.org/abs/2510.02811)
*Matej Gjurković*

Main category: cs.CL

TL;DR: 本论文旨在解决大规模、带有标签的数据集稀缺以及人格心理学与NLP模型之间脱节的问题，并提出了一种名为SIMPA的可解释人格评估框架。


<details>
  <summary>Details</summary>
Motivation: 随着社交媒体等数字足迹的可用性不断增加，自动进行人格评估变得越来越重要。然而，目前面临两大挑战：一是缺乏大规模、带有标签的人格数据集；二是人格心理学与NLP领域之间存在脱节，这限制了模型的有效性和可解释性。

Method: 本论文提出了两个新的数据集MBTI9k和PANDORA，它们都来自Reddit。PANDORA数据集包含来自10,000多名用户、1700万条评论，并整合了MBTI和Big Five人格模型以及人口统计学信息。在此基础上，论文还开发了一个名为SIMPA（Statement-to-Item Matching Personality Assessment）的计算框架，该框架通过匹配用户生成的语句和经过验证的问卷项目，并利用机器学习和语义相似性技术，来实现可解释的人格评估。

Result: 在MBTI9k和PANDORA数据集上的实验表明，人口统计学变量会影响模型的有效性。SIMPA框架在保持高可解释性和效率的同时，其人格评估结果与人类评估相当。SIMPA的设计具有模型无关性、分层线索检测和可扩展性等特点。

Conclusion: SIMPA框架不仅在人格评估方面取得了成功，而且由于其通用性，可以应用于其他涉及复杂标签分类和变量线索关联的研究和实际应用中。

Abstract: Personality refers to individual differences in behavior, thinking, and
feeling. With the growing availability of digital footprints, especially from
social media, automated methods for personality assessment have become
increasingly important. Natural language processing (NLP) enables the analysis
of unstructured text data to identify personality indicators. However, two main
challenges remain central to this thesis: the scarcity of large,
personality-labeled datasets and the disconnect between personality psychology
and NLP, which restricts model validity and interpretability. To address these
challenges, this thesis presents two datasets -- MBTI9k and PANDORA --
collected from Reddit, a platform known for user anonymity and diverse
discussions. The PANDORA dataset contains 17 million comments from over 10,000
users and integrates the MBTI and Big Five personality models with demographic
information, overcoming limitations in data size, quality, and label coverage.
Experiments on these datasets show that demographic variables influence model
validity. In response, the SIMPA (Statement-to-Item Matching Personality
Assessment) framework was developed - a computational framework for
interpretable personality assessment that matches user-generated statements
with validated questionnaire items. By using machine learning and semantic
similarity, SIMPA delivers personality assessments comparable to human
evaluations while maintaining high interpretability and efficiency. Although
focused on personality assessment, SIMPA's versatility extends beyond this
domain. Its model-agnostic design, layered cue detection, and scalability make
it suitable for various research and practical applications involving complex
label taxonomies and variable cue associations with target concepts.

</details>


### [66] [KAME: Tandem Architecture for Enhancing Knowledge in Real-Time Speech-to-Speech Conversational AI](https://arxiv.org/abs/2510.02327)
*So Kuroki,Yotaro Kubo,Takuya Akiba,Yujin Tang*

Main category: cs.CL

TL;DR: 本研究提出一种混合模型，结合了实时语音到语音（S2S）模型的低延迟和基于大型语言模型（LLM）的系统的知识能力，实现了低延迟下的高质量语音交互。


<details>
  <summary>Details</summary>
Motivation: 现有的实时语音到语音模型虽然响应迅速，但缺乏深度知识和语义理解；而结合了语音识别、大型语言模型和语音合成的级联系统虽然知识表示能力强，但延迟高，破坏了自然交互的流畅性。本研究旨在弥合这两类方法的差距。

Method: 提出一种新颖的混合架构，该架构使用S2S Transformer处理用户语音以确保即时响应，同时将查询发送到后端的LLM。LLM生成的文本响应被实时注入，以指导S2S模型的语音生成，从而在不增加显著延迟的情况下，用丰富的知识来增强S2S模型的输出。

Result: 在基于MT-Bench的语音合成多轮问答测试中，该混合系统在响应的准确性方面显著优于基线S2S模型，接近级联系统的性能，同时保持了与基线模型相当的延迟。

Conclusion: 该混合架构成功地在保持低延迟的同时，大幅提高了语音到语音系统的知识性和响应准确性，为实现更自然的对话式AI交互提供了有效解决方案。

Abstract: Real-time speech-to-speech (S2S) models excel at generating natural,
low-latency conversational responses but often lack deep knowledge and semantic
understanding. Conversely, cascaded systems combining automatic speech
recognition, a text-based Large Language Model (LLM), and text-to-speech
synthesis offer superior knowledge representation at the cost of high latency,
which disrupts the flow of natural interaction. This paper introduces a novel
hybrid architecture that bridges the gap between these two paradigms. Our
framework processes user speech through an S2S transformer for immediate
responsiveness while concurrently relaying the query to a powerful back-end
LLM. The LLM's text-based response is then injected in real time to guide the
S2S model's speech generation, effectively infusing its output with rich
knowledge without the full latency penalty of a cascaded system. We evaluated
our method using a speech-synthesized variant of the MT-Bench benchmark that
consists of multi-turn question-answering sessions. The results demonstrate
that our system substantially outperforms a baseline S2S model in response
correctness, approaching that of a cascaded system, while maintaining a latency
on par with the baseline.

</details>


### [67] [SelfJudge: Faster Speculative Decoding via Self-Supervised Judge Verification](https://arxiv.org/abs/2510.02329)
*Kanghoon Yoon,Minsub Kim,Sungjae Lee,Joonhyung Lee,Sunghyeon Woo,Yeonjun In,Se Jung Kwon,Chanyoung Park,Dongsoo Lee*

Main category: cs.CL

TL;DR: SelfJudge通过自我监督训练判别器，实现了跨多种NLP任务的LLM推理加速，并在推理-准确性权衡方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如Judge decoding）在放松验证标准以加速LLM推理时，受限于需要人工标注或有可验证真实情况的任务，泛化能力不足。

Method: SelfJudge通过自我监督的方式，利用目标模型本身来训练判别器，通过评估代币替换后的响应是否保留原始响应的含义来衡量语义保留情况，从而实现自动判别器训练。

Result: SelfJudge在推理-准确性权衡方面优于Judge decoding基线，提供了更广泛适用的LLM推理加速解决方案。

Conclusion: SelfJudge通过自我监督训练判别器，解决了现有方法的局限性，为跨多种NLP任务的LLM推理加速提供了更有效、更通用的方法。

Abstract: Speculative decoding accelerates LLM inference by verifying candidate tokens
from a draft model against a larger target model. Recent judge decoding boosts
this process by relaxing verification criteria by accepting draft tokens that
may exhibit minor discrepancies from target model output, but existing methods
are restricted by their reliance on human annotations or tasks with verifiable
ground truths, limiting generalizability across diverse NLP tasks. We propose
SelfJudge, which trains judge verifiers via self-supervision of the target
model. Our method measures semantic preservation by assessing whether
token-substituted responses preserve the meaning of original responses,
enabling automatic verifier training across diverse NLP tasks. Our experiments
show SelfJudge achieves superior inference-accuracy trade-offs than judge
decoding baselines, offering a broadly applicable solution for faster LLM
inference.

</details>


### [68] [EntropyLong: Effective Long-Context Training via Predictive Uncertainty](https://arxiv.org/abs/2510.02330)
*Junlong Jia,Ziyang Chen,Xing Wu,Chaochen Gao,Zijia Lin,Debing Zhang,Songlin Hu,Binghui Guo*

Main category: cs.CL

TL;DR: EntropyLong是一种新的数据构建方法，利用预测不确定性来确保语言模型训练中的长依赖关系是真实的，并在RULER和LongBenchv2基准测试中展示了显著的改进。


<details>
  <summary>Details</summary>
Motivation: 当前为长上下文语言模型构建数据的方法（如文本拼接）无法保证真正的长依赖关系。

Method: EntropyLong通过识别高熵位置、检索相关上下文并评估其是否能降低预测熵来验证依赖关系质量，然后将验证后的上下文与原始文档结合，构建包含长依赖关系训练样本。

Result: 使用EntropyLong生成的数据集在RULER基准测试上表现出显著的改进，特别是在需要远程信息的任务上。经过指令微调后，模型在LongBenchv2上的表现也有显著提升，表明其长上下文理解能力增强。

Conclusion: 熵基验证对于长上下文训练至关重要且有效，所提出的EntropyLong方法能够构建高质量的长依赖关系数据集，从而提升长上下文语言模型的性能。

Abstract: Training long-context language models to capture long-range dependencies
requires specialized data construction. Current approaches, such as generic
text concatenation or heuristic-based variants, frequently fail to guarantee
genuine long-range dependencies. We propose EntropyLong, a novel data
construction method that leverages predictive uncertainty to verify dependency
quality. Our approach identifies high-entropy positions in documents, retrieves
semantically relevant contexts from large corpora, and verifies their utility
by assessing whether they reduce prediction entropy. This model-in-the-loop
verification ensures each dependency represents measurable information gain
rather than spurious correlation. We construct training samples with long-range
dependencies by combining original documents with these verified contextual
supplements. Using FineWebEdu and Cosmopedia, we generate a dataset of
128K-length sequences with verified dependencies. Models trained on this data
demonstrate significant improvements on RULER benchmarks, particularly in tasks
requiring distant information. Following instruction fine-tuning, our models
also achieve substantial gains on LongBenchv2, demonstrating enhanced
long-context understanding. Extensive ablation studies further validate the
necessity and effectiveness of entropybased verification for long-context
training.

</details>


### [69] [Synthetic Dialogue Generation for Interactive Conversational Elicitation & Recommendation (ICER)](https://arxiv.org/abs/2510.02331)
*Moonkyung Ryu,Chih-Wei Hsu,Yinlam Chow,Mohammad Ghavamzadeh,Craig Boutilier*

Main category: cs.CL

TL;DR: LM无法直接用于对话推荐系统，因为缺乏数据。虽然可以使用LM生成数据，但这些数据可能不符合用户行为。本研究提出了一种使用行为模拟器和LM提示生成一致性对话数据的方法，并创建了一个新的数据集。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏公开的对话推荐系统（CRS）数据，导致难以对语言模型（LM）进行微调，以用于CRSs。使用LM作为用户模拟器来生成数据，但这些数据可能缺乏行为一致性。

Method: 提出一种使用行为模拟器和LM提示来生成与用户潜在状态一致的自然对话的方法。

Result: 生成了一个大型的、开源的CRS数据集，该数据集包含偏好引导和示例批评。评估表明，生成的对话具有高度的一致性、事实性和自然性。

Conclusion: 所提出的方法能够生成与用户状态一致的、自然的对话数据，解决了现有LM生成CRS数据时行为不一致的问题，并创建了一个有价值的新数据集。

Abstract: While language models (LMs) offer great potential for conversational
recommender systems (CRSs), the paucity of public CRS data makes fine-tuning
LMs for CRSs challenging. In response, LMs as user simulators qua data
generators can be used to train LM-based CRSs, but often lack behavioral
consistency, generating utterance sequences inconsistent with those of any real
user. To address this, we develop a methodology for generating natural
dialogues that are consistent with a user's underlying state using behavior
simulators together with LM-prompting. We illustrate our approach by generating
a large, open-source CRS data set with both preference elicitation and example
critiquing. Rater evaluation on some of these dialogues shows them to exhibit
considerable consistency, factuality and naturalness.

</details>


### [70] [A High-Capacity and Secure Disambiguation Algorithm for Neural Linguistic Steganography](https://arxiv.org/abs/2510.02332)
*Yapei Feng,Feng Jiang,Shanhao Wu,Hua Zhong*

Main category: cs.CL

TL;DR: 提出了一种名为look-ahead Sync的新方法，解决了神经语言隐写术中的容量和可证明安全性问题，显著优于现有SyncPool方法。


<details>
  <summary>Details</summary>
Motivation: 现有SyncPool方法在解决Tokenization歧义问题时，牺牲了嵌入容量，因为它将整个香农熵用于同步而非负载嵌入。

Method: look-ahead Sync方法通过仅在真正无法区分的Token序列上执行最小同步采样，并战略性地保留所有其他可辨别路径，以最大化嵌入容量。它提供了安全性的理论证明，并分析了其容量与理论上限之间的差距。

Result: 在英语（Llama 3）和中文（Qwen 2.5）基准测试中，该方法接近理论容量上限，并且在嵌入率上分别超过SyncPool 160%和25%，尤其是在更大的候选池设置下。

Conclusion: look-ahead Sync方法在实现高容量和可证明安全性的神经语言隐写术方面取得了显著进展，克服了SyncPool的容量限制，同时保持了其安全保证。

Abstract: Neural linguistic steganography aims to embed information
  into natural text while preserving statistical undetectability. A fundamental
challenge in this ffeld stems from tokenization ambiguity in modern tokenizers,
which can lead to catastrophic decoding failures. The recent method, SyncPool,
addresses this ambiguity
  by employing a coarse-grained synchronization mechanism over groups of
ambiguous candidates. However, SyncPool sacriffces embedding capacity, as it
utilizes the entire Shannon entropy of an ambiguous group solely for
synchronization rather than for payload embedding. We propose a method named
look-ahead Sync, which overcomes the capacity limitation of SyncPool while
retaining its provable security guarantees. Our approach performs minimal
synchronized sampling only on truly indistinguishable token sequences, while
strategically preserving all other discernible paths to maximize embedding
capacity. We provide theoretical proofs for the security of our method and
analyze the gap between its achievable embedding capacity and the theoretical
upper bound. Experiments on English (using Llama 3) and Chinese (using Qwen
2.5) benchmarks show that our method consistently approaches the theoretical
capacity upper bound and signiffcantly outperforms SyncPool. The improvement in
embedding rate exceeds 160% in English and 25% in Chinese, particularly in
settings with larger candidate pools. This work represents a signiffcant step
toward practical high-capacity provably secure linguistic steganography.

</details>


### [71] [Where Did It Go Wrong? Attributing Undesirable LLM Behaviors via Representation Gradient Tracing](https://arxiv.org/abs/2510.02334)
*Zhe Li,Wei Zhao,Yige Li,Jun Sun*

Main category: cs.CL

TL;DR: LLM 部署常因产生有害内容、事实不准确和社会偏见等不良行为而受阻。现有的基于参数梯度的归因方法因噪声信号和计算复杂性而效果不佳。本研究提出了一种新颖高效的框架，通过分析表示及其梯度来诊断一系列不良 LLM 行为，直接在模型的激活空间中操作，以提供将输出与其训练数据联系起来的语义上有意义的信号。我们系统地评估了我们的方法在跟踪有害内容、检测后门中毒和识别知识污染等任务上的表现。结果表明，我们的方法不仅在样本级归因方面表现出色，而且能够进行细粒度的 token 级分析，精确识别对模型行为产生因果影响的具体样本和短语。这项工作提供了一个强大的诊断工具，用于理解、审计和最终减轻与 LLM 相关的风险。


<details>
  <summary>Details</summary>
Motivation: LLM 部署常因产生有害内容、事实不准确和社会偏见等不良行为而受阻。现有的基于参数梯度的归因方法因噪声信号和计算复杂性而效果不佳。

Method: 提出了一种新颖高效的框架，通过分析表示及其梯度来诊断一系列不良 LLM 行为，直接在模型的激活空间中操作，以提供将输出与其训练数据联系起来的语义上有意义的信号。

Result: 该方法在样本级归因方面表现出色，并能进行细粒度的 token 级分析，精确识别对模型行为产生因果影响的具体样本和短语。

Conclusion: 这项工作提供了一个强大的诊断工具，用于理解、审计和最终减轻与 LLM 相关的风险。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities, yet
their deployment is frequently undermined by undesirable behaviors such as
generating harmful content, factual inaccuracies, and societal biases.
Diagnosing the root causes of these failures poses a critical challenge for AI
safety. Existing attribution methods, particularly those based on parameter
gradients, often fall short due to prohibitive noisy signals and computational
complexity. In this work, we introduce a novel and efficient framework that
diagnoses a range of undesirable LLM behaviors by analyzing representation and
its gradients, which operates directly in the model's activation space to
provide a semantically meaningful signal linking outputs to their training
data. We systematically evaluate our method for tasks that include tracking
harmful content, detecting backdoor poisoning, and identifying knowledge
contamination. The results demonstrate that our approach not only excels at
sample-level attribution but also enables fine-grained token-level analysis,
precisely identifying the specific samples and phrases that causally influence
model behavior. This work provides a powerful diagnostic tool to understand,
audit, and ultimately mitigate the risks associated with LLMs. The code is
available at https://github.com/plumprc/RepT.

</details>


### [72] [FormalML: A Benchmark for Evaluating Formal Subgoal Completion in Machine Learning Theory](https://arxiv.org/abs/2510.02335)
*Xiao-Wen Yang,Zihao Zhang,Jianuo Cao,Zhi Zhou,Zenan Li,Lan-Zhe Guo,Yuan Yao,Taolue Chen,Yu-Feng Li,Xiaoxing Ma*

Main category: cs.CL

TL;DR: LLM在形式化定理证明方面取得了显著进展，但其作为数学家助手的潜力（例如，在复杂证明中填补缺失步骤）仍有待探索。本文将此挑战定义为“子目标完成”，即LLM需要解决人类提供的草图中遗留的简短但非平凡的证明任务。


<details>
  <summary>Details</summary>
Motivation: 研究LLM在数学证明中的子目标完成能力，填补其在复杂证明中作为助手作用的空白。

Method: 引入了一个名为FormalML的Lean 4基准，该基准由机器学习基础理论构建。通过将过程式证明转换为声明式形式的翻译策略，提取了4937个不同难度的优化和概率不等式问题。FormalML是第一个结合了前提检索和复杂研究级上下文的子目标完成基准。

Result: 评估表明，最先进的证明器在准确性和效率方面仍存在局限性，突显了对更强大的基于LLM的定理证明器的需求，以实现有效的子目标完成。

Conclusion: LLM在形式化定理证明的子目标完成任务方面仍有很大提升空间，需要更先进的模型来克服现有证明器的准确性和效率瓶颈。

Abstract: Large language models (LLMs) have recently demonstrated remarkable progress
in formal theorem proving. Yet their ability to serve as practical assistants
for mathematicians, filling in missing steps within complex proofs, remains
underexplored. We identify this challenge as the task of subgoal completion,
where an LLM must discharge short but nontrivial proof obligations left
unresolved in a human-provided sketch. To study this problem, we introduce
FormalML, a Lean 4 benchmark built from foundational theories of machine
learning. Using a translation tactic that converts procedural proofs into
declarative form, we extract 4937 problems spanning optimization and
probability inequalities, with varying levels of difficulty. FormalML is the
first subgoal completion benchmark to combine premise retrieval and complex
research-level contexts. Evaluation of state-of-the-art provers highlights
persistent limitations in accuracy and efficiency, underscoring the need for
more capable LLM-based theorem provers for effective subgoal completion,

</details>


### [73] [KurdSTS: The Kurdish Semantic Textual Similarity](https://arxiv.org/abs/2510.02336)
*Abdulhady Abas Abdullah,Hadi Veisi,Hussein M. Al*

Main category: cs.CL

TL;DR: 创建了首个库尔德语语义文本相似性（STS）数据集，包含10,000个句子对，并对Sentence-BERT等模型进行了基准测试，强调了库尔德语的挑战。


<details>
  <summary>Details</summary>
Motivation: 为库尔德语等低资源语言提供STS数据集，以支持NLP任务。

Method: 创建了一个包含10,000个句子对（涵盖正式和非正式语域）的库尔德语STS数据集，并对Sentence-BERT、多语言BERT等基线模型进行了基准测试。

Result: 在库尔德语STS任务上取得了具有竞争力的结果，但也指出了由库尔德语形态、拼写变异和语码转换带来的挑战。

Conclusion: 该数据集和基线测试为未来库尔德语语义和低资源NLP研究提供了一个可复现的评估套件和坚实的基础。

Abstract: Semantic Textual Similarity (STS) measures the degree of meaning overlap
between two texts and underpins many NLP tasks. While extensive resources exist
for high-resource languages, low-resource languages such as Kurdish remain
underserved. We present, to our knowledge, the first Kurdish STS dataset:
10,000 sentence pairs spanning formal and informal registers, each annotated
for similarity. We benchmark Sentence-BERT, multilingual BERT, and other strong
baselines, obtaining competitive results while highlighting challenges arising
from Kurdish morphology, orthographic variation, and code-mixing. The dataset
and baselines establish a reproducible evaluation suite and provide a strong
starting point for future research on Kurdish semantics and low-resource NLP.

</details>


### [74] [CRACQ: A Multi-Dimensional Approach To Automated Document Assessment](https://arxiv.org/abs/2510.02337)
*Ishak Soltani,Francisco Belo,Bernardo Tavares*

Main category: cs.CL

TL;DR: CRACQ是一个用于评估机器生成文本（如论文、拨款申请等）的多维度框架，它侧重于连贯性、严谨性、恰当性、完整性和质量五个方面，提供可解释的评估方法。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在开发一个多维度、可解释的评估框架，以克服现有单一评分方法在评估机器生成文本（特别是除论文之外的文本类型）方面的局限性。

Method: CRACQ框架整合了语言、语义和结构信号，并基于评分细则进行评估。该框架在500份合成的拨款申请书上进行了训练，并与“大型语言模型作为评判者”进行了基准测试，同时也在实际的优秀和不佳的申请书上进行了测试。

Result: 与直接使用大型语言模型进行评估相比，CRACQ在稳定性、可解释性和层面评估方面表现更优。然而，在评估的可靠性和领域适用性方面仍存在挑战。

Conclusion: CRACQ提供了一种比直接使用大型语言模型作为评判者更稳定、更可解释的机器生成文本评估方法，尤其是在多维度评估方面。尽管如此，该框架在可靠性和领域范围方面仍有待改进。

Abstract: This paper presents CRACQ, a multi-dimensional evaluation framework tailored
to evaluate documents across f i v e specific traits: Coherence, Rigor,
Appropriateness, Completeness, and Quality. Building on insights from
traitbased Automated Essay Scoring (AES), CRACQ expands its fo-cus beyond
essays to encompass diverse forms of machine-generated text, providing a
rubricdriven and interpretable methodology for automated evaluation. Unlike
singlescore approaches, CRACQ integrates linguistic, semantic, and structural
signals into a cumulative assessment, enabling both holistic and trait-level
analysis. Trained on 500 synthetic grant pro-posals, CRACQ was benchmarked
against an LLM-as-a-judge and further tested on both strong and weak real
applications. Preliminary results in-dicate that CRACQ produces more stable and
interpretable trait-level judgments than direct LLM evaluation, though
challenges in reliability and domain scope remain

</details>


### [75] [Optimizing Long-Form Clinical Text Generation with Claim-Based Rewards](https://arxiv.org/abs/2510.02338)
*Samyak Jhaveri,Praphul Singh,Jangwon Kim,Tara Taghavi,Krishnaram Kenthapadi*

Main category: cs.CL

TL;DR: 通过结合GRPO和DocLens，提出了一种用于长篇临床文本生成的评估集成强化学习框架，以提高临床笔记的质量、事实准确性和完整性。


<details>
  <summary>Details</summary>
Motivation: 自动化临床文档需要与完整性和事实基础等优先事项精确对齐。

Method: 该方法结合了群体相对策略优化（GRPO）和DocLens，这是一种提供确定性、对话基础奖励的声明级评估器，直接优化事实基础和完整性，而无需训练单独的奖励模型或依赖人工编写的参考。

Result: 实验证明，该方法通过简单的奖励门控策略提高了临床笔记的质量并降低了训练成本。GPT-5的独立定性评估进一步证实了这些优势，在事实性、完整性和简洁性方面，GRPO的输出更受青睐，遗漏和幻觉更少。

Conclusion: 该框架可扩展到实际应用场景，并能纳入定制化目标，如指南依从性或计费偏好。基于当前基准的改进可能代表了一个保守的下限。

Abstract: Automating clinical documentation with large language models requires precise
alignment with priorities such as completeness and factual grounding. We
present an evaluation-integrated reinforcement learning framework for long-form
clinical text generation that couples Group Relative Policy Optimization (GRPO)
with DocLens, a claim-level evaluator that provides deterministic,
dialogue-grounded rewards. Our method directly optimizes factual grounding and
completeness without training a separate reward model or relying on
human-authored references. Empirically, the approach improves clinical note
quality and reduces training cost via a simple reward-gating strategy. An
independent GPT-5 qualitative evaluation further supports these gains, showing
higher preference for GRPO outputs in factuality, completeness, and brevity,
with fewer omissions and hallucinations. Because the benchmarks are relatively
clean and the base model already well aligned, these improvements likely
represent a conservative lower bound. The framework is scalable to real-world
settings and can incorporate custom objectives such as guideline adherence or
billing preferences.

</details>


### [76] [Evaluating Uncertainty Quantification Methods in Argumentative Large Language Models](https://arxiv.org/abs/2510.02339)
*Kevin Zhou,Adam Dejl,Gabriel Freedman,Lihu Chen,Antonio Rago,Francesca Toni*

Main category: cs.CL

TL;DR: 本研究探讨了在用于决策的LLM框架（ArgLLMs）中集成不确定性量化（UQ）方法，并评估了不同UQ方法在声明验证任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 随着LLM技术的广泛应用，对其可靠性的保证日益重要，尤其是在决策支持场景中，因此需要对LLM的不确定性进行量化。

Method: 本研究将LLM的UQ方法集成到ArgLLMs框架中，并通过在声明验证任务上进行实验来评估不同UQ方法的有效性。实验过程本身也提供了一种评估UQ方法有效性的新颖方式，特别是在处理复杂和有争议的声明时。

Result: 实验结果表明，尽管简单直接，但直接提示（direct prompting）是一种在ArgLLMs中有效的UQ策略，其性能显著优于更复杂的方法。

Conclusion: 直接提示作为一种UQ策略，在ArgLLMs中表现出良好的有效性，为在决策支持等关键应用中提高LLM的可靠性提供了一种简单而有效的方法。

Abstract: Research in uncertainty quantification (UQ) for large language models (LLMs)
is increasingly important towards guaranteeing the reliability of this
groundbreaking technology. We explore the integration of LLM UQ methods in
argumentative LLMs (ArgLLMs), an explainable LLM framework for decision-making
based on computational argumentation in which UQ plays a critical role. We
conduct experiments to evaluate ArgLLMs' performance on claim verification
tasks when using different LLM UQ methods, inherently performing an assessment
of the UQ methods' effectiveness. Moreover, the experimental procedure itself
is a novel way of evaluating the effectiveness of UQ methods, especially when
intricate and potentially contentious statements are present. Our results
demonstrate that, despite its simplicity, direct prompting is an effective UQ
strategy in ArgLLMs, outperforming considerably more complex approaches.

</details>


### [77] [Can Prompts Rewind Time for LLMs? Evaluating the Effectiveness of Prompted Knowledge Cutoffs](https://arxiv.org/abs/2510.02340)
*Xin Gao,Ruiyi Zhang,Daniel Du,Saurabh Mahindre,Sai Ashish Somayajula,Pengtao Xie*

Main category: cs.CL

TL;DR: LLMs在进行时间预测时存在数据污染问题，可能导致过高估计其泛化能力。本文旨在探索通过提示工程模拟LLMs的知识截止日期，并评估其遗忘能力。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在进行时间预测时，由于依赖预训练数据，可能存在数据污染问题，即模型可能通过记忆而非推理来预测截止日期之后的数据，从而过高估计其泛化能力。因此，研究如何模拟LLMs的知识截止日期，以更准确地评估其在时间预测任务上的表现，具有重要意义。

Method: 本文通过构建三个评估数据集，分别针对直接事实知识、语义偏移和因果相关知识，来评估LLMs在提示工程下模拟知识截止日期的能力，即模型遗忘特定日期后知识的能力。

Result: 实验结果表明，在直接查询遗忘信息时，基于提示的模拟知识截止日期有效；然而，当被遗忘的内容与查询内容仅有因果关系而非直接被查询时，模型遗忘能力则不佳。

Conclusion: 通过提示工程模拟LLMs的知识截止日期在某些情况下有效，但对于因果相关知识的遗忘能力有限。这提示我们在将LLMs应用于时间预测任务时，需要更严谨的评估方法。

Abstract: Large Language Models (LLMs) are widely used for temporal prediction, but
their reliance on pretraining data raises contamination concerns, as accurate
predictions on pre-cutoff test data may reflect memorization rather than
reasoning, leading to an overestimation of their generalization capability.
With the recent emergence of prompting-based unlearning techniques, a natural
question arises: Can LLMs be prompted to simulate an earlier knowledge cutoff?
In this work, we investigate the capability of prompting to simulate earlier
knowledge cutoff in LLMs. We construct three evaluation datasets to assess the
extent to which LLMs can forget (1) direct factual knowledge, (2) semantic
shifts, and (3) causally related knowledge. Results demonstrate that while
prompt-based simulated knowledge cutoffs show effectiveness when directly
queried with the information after that date, they struggle to induce
forgetting when the forgotten content is not directly asked but causally
related to the query. These findings highlight the need for more rigorous
evaluation settings when applying LLMs for temporal prediction tasks. The full
dataset and evaluation code are available at
https://github.com/gxx27/time_unlearn.

</details>


### [78] [DRIFT: Learning from Abundant User Dissatisfaction in Real-World Preference Learning](https://arxiv.org/abs/2510.02341)
*Yifan Wang,Bolian Li,Junlin Wu,Zhaoxuan Tan,Zheli Liu,Ruqi Zhang,Ananth Grama,Qingkai Zeng*

Main category: cs.CL

TL;DR: 该研究提出了一种名为DRIFT的新型训练方法，用于处理大规模语言模型部署中常见的用户不满意信号，并通过实验证明了其有效性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有的偏好学习方法难以处理现实世界中用户不满意（DSAT）信号丰富而明确满意（SAT）反馈稀缺的数据特征，因为这些方法依赖昂贵的人工标注或假设有充足的正面回应。

Method: DRIFT（Dissatisfaction-Refined Iterative Preference Training）通过利用现实世界中的DSAT信号进行训练，并从不断演变的策略中动态采样正面回应。

Result: 在WildBench和AlpacaEval2基准测试中，使用DRIFT训练的模型在WildFeedback和UltraFeedback数据集上取得了显著的性能提升，优于DPO和SPIN等基线方法。14B模型在WildBench上甚至超越了GPT-4o-mini。此外，DRIFT还能保持模型的探索能力，产生更多样化的解决方案。

Conclusion: DRIFT是一种有效且可扩展的现实世界模型后训练方法，能够利用最丰富和最具信息量的信号，为提高大规模语言模型的性能和用户满意度提供了新的途径。

Abstract: Real-world large language model deployments (e.g., conversational AI systems,
code generation assistants) naturally generate abundant implicit user
dissatisfaction (DSAT) signals, as users iterate toward better answers through
refinements, corrections, and expressed preferences, while explicit
satisfaction (SAT) feedback is scarce. Existing preference learning approaches
are poorly aligned with this data profile, as they rely on costly human
annotations or assume plentiful positive responses. In this paper, we introduce
\textbf{DRIFT} (\textbf{D}issatisfaction-\textbf{R}efined \textbf{I}terative
pre\textbf{F}erence \textbf{T}raining), which anchors training on real-world
DSAT signals and samples positives dynamically from the evolving policy.
Empirically, DRIFT models trained on real-world \textit{WildFeedback} datasets
and synthetic \textit{UltraFeedback} datasets achieve up to +6.23\% (7B) /
+7.61\% (14B) on WildBench Task Score and up to +8.95\% (7B) / +12.29\% (14B)
on AlpacaEval2 win rate over base models, outperforming strong baseline methods
such as iterative DPO and SPIN. At larger scales, the improvements are
particularly pronounced: 14B models trained with DRIFT surpass GPT-4o-mini on
WildBench. Further analysis shows that DRIFT also preserves exploratory
capacity, yielding more diverse high-reward solutions rather than collapsing to
narrow subsets. Theoretically, we demonstrate that this design preserves
preference margins and avoids the gradient degeneration. These results show
that DRIFT is an effective and scalable recipe for real-world post-training
that leverages the most abundant and informative signal. The code and data are
available at https://github.com/cacayaya/DRIFT.git.

</details>


### [79] [$\texttt{BluePrint}$: A Social Media User Dataset for LLM Persona Evaluation and Training](https://arxiv.org/abs/2510.02343)
*Aurélien Bück-Kaeffer,Je Qin Chooi,Dan Zhao,Maximilian Puelma Touzel,Kellin Pelrine,Jean-François Godbout,Reihaneh Rabbany,Zachary Yang*

Main category: cs.CL

TL;DR: 该研究提出了SIMPACT框架和BluePrint数据集，用于模拟社交媒体动态，训练和评估大型语言模型（LLM）作为社交媒体代理。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏用于微调和评估LLM作为真实社交媒体代理的标准数据资源。

Method: 提出SIMPACT框架，包含构建行为基础的社交媒体数据集的隐私保护方法，并将下一个动作预测作为训练和评估LLM代理的任务，同时引入了集群和总体层面的指标来评估行为保真度和风格真实性。在此基础上，发布了从Bluesky公开数据构建的大规模数据集BluePrint，该数据集将匿名用户聚合成行为者，并包含12种社交媒体互动类型。

Result: 发布了SIMPACT框架和BluePrint数据集，BluePrint数据集包含了12种社交媒体互动类型，支持开发能够理解语言和互动行为的上下文依赖性的LLM代理。

Conclusion: SIMPACT通过标准化数据和评估协议，为严谨、负责任的社交媒体模拟奠定了基础。BluePrint数据集可作为政治话语建模的评估基准，并为构建特定领域的数据集以研究虚假信息和观点极化等挑战提供了模板。

Abstract: Large language models (LLMs) offer promising capabilities for simulating
social media dynamics at scale, enabling studies that would be ethically or
logistically challenging with human subjects. However, the field lacks
standardized data resources for fine-tuning and evaluating LLMs as realistic
social media agents. We address this gap by introducing SIMPACT, the
SIMulation-oriented Persona and Action Capture Toolkit, a privacy respecting
framework for constructing behaviorally-grounded social media datasets suitable
for training agent models. We formulate next-action prediction as a task for
training and evaluating LLM-based agents and introduce metrics at both the
cluster and population levels to assess behavioral fidelity and stylistic
realism. As a concrete implementation, we release BluePrint, a large-scale
dataset built from public Bluesky data focused on political discourse.
BluePrint clusters anonymized users into personas of aggregated behaviours,
capturing authentic engagement patterns while safeguarding privacy through
pseudonymization and removal of personally identifiable information. The
dataset includes a sizable action set of 12 social media interaction types
(likes, replies, reposts, etc.), each instance tied to the posting activity
preceding it. This supports the development of agents that use
context-dependence, not only in the language, but also in the interaction
behaviours of social media to model social media users. By standardizing data
and evaluation protocols, SIMPACT provides a foundation for advancing rigorous,
ethically responsible social media simulations. BluePrint serves as both an
evaluation benchmark for political discourse modeling and a template for
building domain specific datasets to study challenges such as misinformation
and polarization.

</details>


### [80] [Small Language Models for Curriculum-based Guidance](https://arxiv.org/abs/2510.02347)
*Konstantinos Katharakis,Sippo Rossi,Raghava Rao Mukkamala*

Main category: cs.CL

TL;DR: 小型语言模型（SLMs）通过检索增强生成（RAG）技术，在教育领域可作为有成本效益、注重隐私且环保的人工智能教学助手，其表现可媲美大型语言模型（LLMs）。


<details>
  <summary>Details</summary>
Motivation: 探索在教育领域开发和评估能提供基于课程指导的人工智能教学助手，重点关注检索增强生成（RAG）流水线和开源小型语言模型（SLMs）的应用。

Method: 评估了八种小型语言模型（SLMs），包括LLaMA 3.1、IBM Granite 3.3 和 Gemma 3（7-17B参数），并与GPT-4o进行了基准测试，采用了检索增强生成（RAG）流水线。

Result: 通过适当的提示和定向检索，小型语言模型（SLMs）在提供准确、符合教学法的响应方面可以媲美大型语言模型（LLMs）。小型语言模型（SLMs）在计算和能源需求方面具有显著的可持续性优势，可在无需云基础设施的情况下在消费级硬件上实现实时使用。

Conclusion: 小型语言模型（SLMs）通过RAG技术，在成本效益、隐私保护和环境可持续性方面均具有优势，能够满足教育机构在可持续和节能前提下扩展个性化学习的需求，是可行的人工智能教学助手解决方案。

Abstract: The adoption of generative AI and large language models (LLMs) in education
is still emerging. In this study, we explore the development and evaluation of
AI teaching assistants that provide curriculum-based guidance using a
retrieval-augmented generation (RAG) pipeline applied to selected open-source
small language models (SLMs). We benchmarked eight SLMs, including LLaMA 3.1,
IBM Granite 3.3, and Gemma 3 (7-17B parameters), against GPT-4o. Our findings
show that with proper prompting and targeted retrieval, SLMs can match LLMs in
delivering accurate, pedagogically aligned responses. Importantly, SLMs offer
significant sustainability benefits due to their lower computational and energy
requirements, enabling real-time use on consumer-grade hardware without
depending on cloud infrastructure. This makes them not only cost-effective and
privacy-preserving but also environmentally responsible, positioning them as
viable AI teaching assistants for educational institutions aiming to scale
personalized learning in a sustainable and energy-efficient manner.

</details>


### [81] [mini-vec2vec: Scaling Universal Geometry Alignment with Linear Transformations](https://arxiv.org/abs/2510.02348)
*Guy Dar*

Main category: cs.CL

TL;DR: mini-vec2vec是一个比vec2vec更高效、更稳定的文本嵌入空间对齐方法，其线性变换在效率上提升了几个数量级，同时保持了相当或更好的结果。


<details>
  <summary>Details</summary>
Motivation: 现有的vec2vec方法虽然能实现近乎完美的文本嵌入空间对齐，但存在计算成本高和不稳定的问题。

Method: mini-vec2vec方法包括三个主要阶段：伪并行嵌入向量的初步匹配、变换拟合和迭代精炼，学习到的映射为线性变换。

Result: mini-vec2vec在效率上超越了原始vec2vec方法几个数量级，同时在对齐效果上达到或超过了原始方法。

Conclusion: mini-vec2vec方法稳定且算法步骤可解释，易于扩展并为在新领域和新方向的应用开辟了机会。

Abstract: We build upon vec2vec, a procedure designed to align text embedding spaces
without parallel data. vec2vec finds a near-perfect alignment, but it is
expensive and unstable. We present mini-vec2vec, a simple and efficient
alternative that requires substantially lower computational cost and is highly
robust. Moreover, the learned mapping is a linear transformation. Our method
consists of three main stages: a tentative matching of pseudo-parallel
embedding vectors, transformation fitting, and iterative refinement. Our linear
alternative exceeds the original instantiation of vec2vec by orders of
magnitude in efficiency, while matching or exceeding their results. The
method's stability and interpretable algorithmic steps facilitate scaling and
unlock new opportunities for adoption in new domains and fields.

</details>


### [82] [LLMSQL: Upgrading WikiSQL for the LLM Era of Text-to-SQL](https://arxiv.org/abs/2510.02350)
*Dzmitry Pihulski,Karol Charchut,Viktoria Novogrodskaia,Jan Kocoń*

Main category: cs.CL

TL;DR: LLMSQL是WikiSQL的系统性修订版，旨在解决其在LLM时代存在的问题，并为LLM提供一个准备就绪的基准测试。


<details>
  <summary>Details</summary>
Motivation: WikiSQL数据集在Text-to-SQL任务中发挥了关键作用，但其结构和标注问题（如大小写不一致、数据类型不匹配、语法错误和未回答问题）限制了其在LLM时代的应用。LLMSQL旨在通过系统性修订和转换WikiSQL来解决这些问题。

Method: LLMSQL通过对WikiSQL中的错误进行分类，并采用自动化方法进行清理和重新标注来创建。然后，评估了包括Gemma 3、LLaMA 3.2、Mistral 7B等在内的多个大型语言模型（LLMs）在LLMSQL上的表现。

Result: LLMSQL被设计为一个LLM就绪的基准测试，与面向指针网络模型的原始WikiSQL不同，LLMSQL提供清晰的自然语言问题和完整的SQL查询作为纯文本，便于现代Text-to-SQL模型的生成和评估。

Conclusion: LLMSQL是WikiSQL的改进版本，解决了原始数据集存在的问题，并为评估LLM在Text-to-SQL任务上的表现提供了一个更合适的基准。

Abstract: Converting natural language questions into SQL queries (Text-to-SQL) enables
non-expert users to interact with relational databases and has long been a
central task for natural language interfaces to data. While the WikiSQL dataset
played a key role in early NL2SQL research, its usage has declined due to
structural and annotation issues, including case sensitivity inconsistencies,
data type mismatches, syntax errors, and unanswered questions. We present
LLMSQL, a systematic revision and transformation of WikiSQL designed for the
LLM era. We classify these errors and implement automated methods for cleaning
and re-annotation. To assess the impact of these improvements, we evaluated
multiple large language models (LLMs), including Gemma 3, LLaMA 3.2, Mistral
7B, gpt-oss 20B, Phi-3.5 Mini, Qwen 2.5, OpenAI o4-mini, DeepSeek R1 and
others. Rather than serving as an update, LLMSQL is introduced as an LLM-ready
benchmark: unlike the original WikiSQL, tailored for pointer-network models
selecting tokens from input, LLMSQL provides clean natural language questions
and full SQL queries as plain text, enabling straightforward generation and
evaluation for modern natural language-to-SQL models.

</details>


### [83] [Language, Culture, and Ideology: Personalizing Offensiveness Detection in Political Tweets with Reasoning LLMs](https://arxiv.org/abs/2510.02351)
*Dzmitry Pihulski,Jan Kocoń*

Main category: cs.CL

TL;DR: LLMs assessed offensiveness in political discourse by adopting different political and cultural perspectives, with larger, reasoning-enabled models showing better performance across languages and ideologies.


<details>
  <summary>Details</summary>
Motivation: To explore how LLMs assess offensiveness in political discourse when prompted to adopt specific political and cultural perspectives.

Method: Evaluated several LLMs (DeepSeek-R1, o4-mini, GPT-4.1-mini, Qwen3, Gemma, Mistral) on a multilingual dataset (MD-Agreement) of 2020 US election tweets, tasking them to judge offensiveness from varied political viewpoints (far-right, conservative, centrist, progressive) in English, Polish, and Russian.

Result: Larger models with explicit reasoning abilities (DeepSeek-R1, o4-mini) were more consistent and sensitive to ideological and cultural variation than smaller models. Reasoning capabilities improved personalization and interpretability of offensiveness judgments.

Conclusion: Reasoning capabilities are key to adapting LLMs for nuanced sociopolitical text classification across languages and ideologies.

Abstract: We explore how large language models (LLMs) assess offensiveness in political
discourse when prompted to adopt specific political and cultural perspectives.
Using a multilingual subset of the MD-Agreement dataset centered on tweets from
the 2020 US elections, we evaluate several recent LLMs - including DeepSeek-R1,
o4-mini, GPT-4.1-mini, Qwen3, Gemma, and Mistral - tasked with judging tweets
as offensive or non-offensive from the viewpoints of varied political personas
(far-right, conservative, centrist, progressive) across English, Polish, and
Russian contexts. Our results show that larger models with explicit reasoning
abilities (e.g., DeepSeek-R1, o4-mini) are more consistent and sensitive to
ideological and cultural variation, while smaller models often fail to capture
subtle distinctions. We find that reasoning capabilities significantly improve
both the personalization and interpretability of offensiveness judgments,
suggesting that such mechanisms are key to adapting LLMs for nuanced
sociopolitical text classification across languages and ideologies.

</details>


### [84] [Evaluating Bias in Spoken Dialogue LLMs for Real-World Decisions and Recommendations](https://arxiv.org/abs/2510.02352)
*Yihao Wu,Tianrui Wang,Yizhou Peng,Yi-Wen Chao,Xuyi Zhuang,Xinsheng Wang,Shunshun Yin,Ziyang Ma*

Main category: cs.CL

TL;DR: 现有关于语音对话模型（SDM）的偏见研究不足，本研究首次系统性评估了语音大语言模型的偏见，并研究了多轮对话和负面反馈的影响。研究发现闭源模型偏见较低，开源模型对年龄和性别敏感，推荐任务会加剧跨组差异，偏见决策在多轮对话中会持续存在。研究发布了FairDialogue数据集和评估代码。


<details>
  <summary>Details</summary>
Motivation: 现有关于语音对话模型（SDM）的偏见研究不足，而口语中的年龄、性别、口音等副语言特征可能加剧偏见，尤其是在多轮对话中，这可能影响决策和推荐任务的公平性。

Method: 使用Group Unfairness Score (GUS)和similarity-based normalized statistics rate (SNSR)来评估开源模型（Qwen2.5-Omni, GLM-4-Voice）和闭源模型（GPT-4o Audio, Gemini-2.5-Flash）在决策和推荐任务中的偏见，并研究了多轮对话和负面反馈的影响。

Result: 闭源模型普遍偏见较低；开源模型对年龄和性别更敏感；推荐任务倾向于放大跨组差异；偏见决策在多轮对话中可能持续存在。

Conclusion: 本研究首次系统性地研究了端到端的语音对话模型中的偏见问题，为构建公平可靠的音频交互系统提供了见解，并发布了FairDialogue数据集和评估代码以促进未来研究。

Abstract: While biases in large language models (LLMs), such as stereotypes and
cultural tendencies in outputs, have been examined and identified, their
presence and characteristics in spoken dialogue models (SDMs) with audio input
and output remain largely unexplored. Paralinguistic features, such as age,
gender, and accent, can affect model outputs; when compounded by multi-turn
conversations, these effects may exacerbate biases, with potential implications
for fairness in decision-making and recommendation tasks. In this paper, we
systematically evaluate biases in speech LLMs and study the impact of
multi-turn dialogues with repeated negative feedback. Bias is measured using
Group Unfairness Score (GUS) for decisions and similarity-based normalized
statistics rate (SNSR) for recommendations, across both open-source models like
Qwen2.5-Omni and GLM-4-Voice, as well as closed-source APIs such as GPT-4o
Audio and Gemini-2.5-Flash. Our analysis reveals that closed-source models
generally exhibit lower bias, while open-source models are more sensitive to
age and gender, and recommendation tasks tend to amplify cross-group
disparities. We found that biased decisions may persist in multi-turn
conversations. This work provides the first systematic study of biases in
end-to-end spoken dialogue models, offering insights towards fair and reliable
audio-based interactive systems. To facilitate further research, we release the
FairDialogue dataset and evaluation code.

</details>


### [85] [An Senegalese Legal Texts Structuration Using LLM-augmented Knowledge Graph](https://arxiv.org/abs/2510.02353)
*Oumar Kane,Mouhamad M. Allaya,Dame Samb,Mamadou Bousso*

Main category: cs.CL

TL;DR: 本研究利用AI和LLM技术改善塞内加尔司法系统的法律文本可及性，解决了法律文件提取和组织方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 强调了改善司法信息获取的必要性，特别是在处理塞内加尔法律文本时面临的困难。

Method: 研究提取了7,967篇文章，特别关注了《土地与公共领域法典》，并开发了一个包含2,872个节点和10,774个关系的图数据库，还运用了先进的三元组提取技术，并评估了GPT-4o、GPT-4和Mistral-Large等模型。

Result: 成功提取了大量法律文本，构建了详细的图数据库以可视化法律文本间的联系，并证明了AI模型在识别关系和元数据方面的有效性。

Conclusion: 旨在通过AI和LLM技术创建一个坚实的框架，以增强塞内加尔公民和法律专业人士对其权利和责任的理解。

Abstract: This study examines the application of artificial intelligence (AI) and large
language models (LLM) to improve access to legal texts in Senegal's judicial
system. The emphasis is on the difficulties of extracting and organizing legal
documents, highlighting the need for better access to judicial information. The
research successfully extracted 7,967 articles from various legal documents,
particularly focusing on the Land and Public Domain Code. A detailed graph
database was developed, which contains 2,872 nodes and 10,774 relationships,
aiding in the visualization of interconnections within legal texts. In
addition, advanced triple extraction techniques were utilized for knowledge,
demonstrating the effectiveness of models such as GPT-4o, GPT-4, and
Mistral-Large in identifying relationships and relevant metadata. Through these
technologies, the aim is to create a solid framework that allows Senegalese
citizens and legal professionals to more effectively understand their rights
and responsibilities.

</details>


### [86] [Modeling the language cortex with form-independent and enriched representations of sentence meaning reveals remarkable semantic abstractness](https://arxiv.org/abs/2510.02354)
*Shreya Saha,Shurui Li,Greta Tuckute,Yuanning Li,Ru-Yuan Zhang,Leila Wehbe,Evelina Fedorenko,Meenakshi Khosla*

Main category: cs.CL

TL;DR: 语言系统在语言皮层中存在高度抽象的、与形式无关的意义表征，其准确性可以通过聚合视觉和语言模型嵌入来预测，并且可能比大型语言模型更丰富。


<details>
  <summary>Details</summary>
Motivation: 探索语言皮层中的意义表征是否具有抽象性，并与视觉和语言模型进行比较。

Method: 通过对句子进行建模，利用视觉和语言模型提取的表征来预测神经反应。通过生成图像、聚合嵌入、使用释义和增加上下文细节来提高预测的准确性。

Result: 聚合多个生成图像的视觉模型嵌入可以提高对语言皮层反应的预测准确性，有时可以与大型语言模型相媲美。对多个释义的嵌入进行平均处理可以提高预测准确性。增加释义的上下文细节可以进一步提高预测准确性，甚至超过原始句子的嵌入。

Conclusion: 研究结果证明了语言皮层中存在高度抽象的、与形式无关的意义表征，并且语言系统可能维护着比语言模型更丰富、更广泛的语义表征。

Abstract: The human language system represents both linguistic forms and meanings, but
the abstractness of the meaning representations remains debated. Here, we
searched for abstract representations of meaning in the language cortex by
modeling neural responses to sentences using representations from vision and
language models. When we generate images corresponding to sentences and extract
vision model embeddings, we find that aggregating across multiple generated
images yields increasingly accurate predictions of language cortex responses,
sometimes rivaling large language models. Similarly, averaging embeddings
across multiple paraphrases of a sentence improves prediction accuracy compared
to any single paraphrase. Enriching paraphrases with contextual details that
may be implicit (e.g., augmenting "I had a pancake" to include details like
"maple syrup") further increases prediction accuracy, even surpassing
predictions based on the embedding of the original sentence, suggesting that
the language system maintains richer and broader semantic representations than
language models. Together, these results demonstrate the existence of highly
abstract, form-independent meaning representations within the language cortex.

</details>


### [87] [DiffuSpec: Unlocking Diffusion Language Models for Speculative Decoding](https://arxiv.org/abs/2510.02358)
*Guanghao Li,Zhihui Fu,Min Fang,Qibin Zhao,Ming Tang,Chun Yuan,Jun Wang*

Main category: cs.CL

TL;DR: LLM推理延迟高，我们提出了DiffuSpec，使用扩散语言模型（DLM）作为草稿器，能并行生成多token草稿，结合因果一致性路径搜索（CPS）和自适应草稿长度（ADL）控制器，最高可达3倍加速。


<details>
  <summary>Details</summary>
Motivation: LLM因其自回归（AR）性质导致解码延迟高，现有投机解码方法（speculative decoding）中的草稿器（drafter）若仍采用AR方式，则无法获得显著的加速效果。

Method: 提出DiffuSpec框架，使用预训练的扩散语言模型（DLM）生成多token草稿，并引入因果一致性路径搜索（CPS）从草稿的token lattice中提取AR可验证路径，以及自适应草稿长度（ADL）控制器动态调整草稿长度。

Result: DiffuSpec在多个基准测试中实现了最高3倍的实际运行时间加速。

Conclusion: 基于扩散模型的草稿生成是投机解码的一种有效替代方案，能够显著提升LLM的推理速度。

Abstract: As large language models (LLMs) scale up, accuracy improves, but the
autoregressive (AR) nature of decoding increases latency since each token
requires a serial forward pass. Speculative decoding addresses this by
employing a fast drafter to propose multi-token drafts, which are then verified
in parallel by the target model. However, many deployments still rely on AR
drafters, where sequential passes limit wall-clock gains. We revisit the
drafting stage and present DiffuSpec, a training-free drop-in framework that
uses a pretrained diffusion language model (DLM) to produce multi-token drafts
in a single forward pass, while remaining compatible with standard AR
verifiers. Because DLM drafts are generated under bidirectional conditioning,
parallel per-position candidates form a token lattice in which the locally
highest-probability token at each position need not form a causal left-to-right
path. Moreover, DLM drafting requires pre-specifying a draft length, inducing a
speed-quality trade-off. To address these challenges, we introduce two
practical components: (i) a causal-consistency path search (CPS) over this
lattice that extracts a left-to-right path aligned with AR verification; and
(ii) an adaptive draft-length (ADL) controller that adjusts next proposal size
based on recent acceptance feedback and realized generated length. Across
benchmarks, DiffuSpec yields up to 3x wall-clock speedup, establishing
diffusion-based drafting as a robust alternative to autoregressive drafters for
speculative decoding.

</details>


### [88] [Emission-GPT: A domain-specific language model agent for knowledge retrieval, emission inventory and data analysis](https://arxiv.org/abs/2510.02359)
*Jiashu Ye,Tong Wu,Weiwen Chen,Hao Zhang,Zeteng Lin,Xingxing Li,Shujuan Weng,Manni Zhu,Xin Yuan,Xinlong Hong,Jingjie Li,Junyu Zheng,Zhijiong Huang,Jing Tang*

Main category: cs.CL

TL;DR: Emission-GPT是一个为大气排放领域定制的、经过知识强化的语言模型代理，旨在解决排放知识碎片化和数据访问效率低下等问题。


<details>
  <summary>Details</summary>
Motivation: 排放相关的知识通常分散且高度专业化，现有获取和整理排放数据的方法效率低下，阻碍了非专业人士理解排放信息，给研究和管理带来挑战。

Method: 构建了一个包含超过10,000份文档（包括标准、报告、指南和同行评审文献）的精选知识库，并集成了提示工程和问题补全技术，以支持准确的领域特定问题解答。此外，该模型还支持用户通过自然语言交互式地分析排放数据，例如查询和可视化清单、分析来源贡献以及为用户定义的场景推荐排放因子。

Result: 在广东省的一个案例研究中，Emission-GPT能够通过简单的提示直接从原始数据中提取关键见解，例如点源分布和行业趋势。

Conclusion: Emission-GPT 模块化和可扩展的架构有助于自动化传统上手动工作流，使其成为下一代排放清单开发和基于场景评估的基础工具。

Abstract: Improving air quality and addressing climate change relies on accurate
understanding and analysis of air pollutant and greenhouse gas emissions.
However, emission-related knowledge is often fragmented and highly specialized,
while existing methods for accessing and compiling emissions data remain
inefficient. These issues hinder the ability of non-experts to interpret
emissions information, posing challenges to research and management. To address
this, we present Emission-GPT, a knowledge-enhanced large language model agent
tailored for the atmospheric emissions domain. Built on a curated knowledge
base of over 10,000 documents (including standards, reports, guidebooks, and
peer-reviewed literature), Emission-GPT integrates prompt engineering and
question completion to support accurate domain-specific question answering.
Emission-GPT also enables users to interactively analyze emissions data via
natural language, such as querying and visualizing inventories, analyzing
source contributions, and recommending emission factors for user-defined
scenarios. A case study in Guangdong Province demonstrates that Emission-GPT
can extract key insights--such as point source distributions and sectoral
trends--directly from raw data with simple prompts. Its modular and extensible
architecture facilitates automation of traditionally manual workflows,
positioning Emission-GPT as a foundational tool for next-generation emission
inventory development and scenario-based assessment.

</details>


### [89] [Spiral of Silence in Large Language Model Agents](https://arxiv.org/abs/2510.02360)
*Mingze Zhong,Meng Fang,Zijing Shi,Yuxuan Huang,Shunfeng Zheng,Yali Du,Ling Chen,Jun Wang*

Main category: cs.CL

TL;DR: 少数派观点在大型语言模型（LLM）群体中会因害怕被孤立而沉默，这与人类社会中的“沉默的螺旋”理论类似，但其背后的机制是统计语言生成而非心理因素。LLM的“历史”和“个性”信号会影响这种动态，其中“历史”信号对于形成“沉默的螺旋”至关重要。


<details>
  <summary>Details</summary>
Motivation: 研究在大型语言模型（LLM）群体中是否存在类似于人类社会“沉默的螺旋”的现象，以及其潜在的生成机制。

Method: 提出一个评估框架，通过控制“历史”和“个性”信号的可用性，利用趋势检验（如Mann-Kendall和Spearman秩检验）和集中度测量（如峰度和四分位距）来评估LLM代理的意见动态。

Result: 实验表明，“历史”和“个性”信号共同作用时会产生强烈的多数主导和“沉默的螺旋”模式；单独的“历史”信号会产生强烈的锚定效应；单独的“个性”信号则会产生多样但无关的观点，表明缺乏历史锚定，“沉默的螺旋”动态无法出现。

Conclusion: “历史”信号对于在LLM代理中复制“沉默的螺旋”现象至关重要，这揭示了在LLM代理系统中监测和减轻合规性现象的必要性，为计算社会学和负责任的人工智能设计提供了新的视角。

Abstract: The Spiral of Silence (SoS) theory holds that individuals with minority views
often refrain from speaking out for fear of social isolation, enabling majority
positions to dominate public discourse. When the 'agents' are large language
models (LLMs), however, the classical psychological explanation is not directly
applicable, since SoS was developed for human societies. This raises a central
question: can SoS-like dynamics nevertheless emerge from purely statistical
language generation in LLM collectives? We propose an evaluation framework for
examining SoS in LLM agents. Specifically, we consider four controlled
conditions that systematically vary the availability of 'History' and 'Persona'
signals. Opinion dynamics are assessed using trend tests such as Mann-Kendall
and Spearman's rank, along with concentration measures including kurtosis and
interquartile range. Experiments across open-source and closed-source models
show that history and persona together produce strong majority dominance and
replicate SoS patterns; history signals alone induce strong anchoring; and
persona signals alone foster diverse but uncorrelated opinions, indicating that
without historical anchoring, SoS dynamics cannot emerge. The work bridges
computational sociology and responsible AI design, highlighting the need to
monitor and mitigate emergent conformity in LLM-agent systems.

</details>


### [90] [ChunkLLM: A Lightweight Pluggable Framework for Accelerating LLMs Inference](https://arxiv.org/abs/2510.02361)
*Haojie Ouyang,Jianwei Lv,Lei Ren,Chen Wei,Xiaojie Wang,Fangxiang Feng*

Main category: cs.CL

TL;DR: ChunkLLM是一个轻量级的训练框架，通过引入QK适配器和Chunk适配器来解决Transformer模型在处理长文本时面临的计算效率低下问题，实现了性能的保持和推理速度的大幅提升。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在处理长文本时，自注意力机制的二次方复杂度导致严重的计算效率问题。现有的基于块选择和压缩的方法存在语义不完整或训练-推理效率不高的问题。

Method: 提出ChunkLLM框架，包含QK适配器（用于特征压缩和块注意力获取）和Chunk适配器（用于检测块边界）。训练时冻结主干模型，只训练适配器，并采用注意力蒸馏方法训练QK适配器。推理时，仅在检测到块边界时触发块选择，以加速推理。

Result: 在长文本和短文本基准数据集上进行了实验。ChunkLLM在短文本上性能相当，在长文本上保留了98.64%的性能，同时保持了48.58%的键值缓存保留率。处理120K长文本时，最大速度提升达4.48倍。

Conclusion: ChunkLLM能够有效解决Transformer模型处理长文本时的效率问题，同时保持高性能和高推理速度。

Abstract: Transformer-based large models excel in natural language processing and
computer vision, but face severe computational inefficiencies due to the
self-attention's quadratic complexity with input tokens. Recently, researchers
have proposed a series of methods based on block selection and compression to
alleviate this problem, but they either have issues with semantic
incompleteness or poor training-inference efficiency. To comprehensively
address these challenges, we propose ChunkLLM, a lightweight and pluggable
training framework. Specifically, we introduce two components: QK Adapter
(Q-Adapter and K-Adapter) and Chunk Adapter. The former is attached to each
Transformer layer, serving dual purposes of feature compression and chunk
attention acquisition. The latter operates at the bottommost layer of the
model, functioning to detect chunk boundaries by leveraging contextual semantic
information. During the training phase, the parameters of the backbone remain
frozen, with only the QK Adapter and Chunk Adapter undergoing training.
Notably, we design an attention distillation method for training the QK
Adapter, which enhances the recall rate of key chunks. During the inference
phase, chunk selection is triggered exclusively when the current token is
detected as a chunk boundary, thereby accelerating model inference.
Experimental evaluations are conducted on a diverse set of long-text and
short-text benchmark datasets spanning multiple tasks. ChunkLLM not only
attains comparable performance on short-text benchmarks but also maintains
98.64% of the performance on long-context benchmarks while preserving a 48.58%
key-value cache retention rate. Particularly, ChunkLLM attains a maximum
speedup of 4.48x in comparison to the vanilla Transformer in the processing of
120K long texts.

</details>


### [91] [A Cross-Lingual Analysis of Bias in Large Language Models Using Romanian History](https://arxiv.org/abs/2510.02362)
*Matei-Iulian Cocu,Răzvan-Cosmin Cristia,Adrian Marius Dumitran*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this case study, we select a set of controversial Romanian historical
questions and ask multiple Large Language Models to answer them across
languages and contexts, in order to assess their biases. Besides being a study
mainly performed for educational purposes, the motivation also lies in the
recognition that history is often presented through altered perspectives,
primarily influenced by the culture and ideals of a state, even through large
language models. Since they are often trained on certain data sets that may
present certain ambiguities, the lack of neutrality is subsequently instilled
in users. The research process was carried out in three stages, to confirm the
idea that the type of response expected can influence, to a certain extent, the
response itself; after providing an affirmative answer to some given question,
an LLM could shift its way of thinking after being asked the same question
again, but being told to respond with a numerical value of a scale. Results
show that binary response stability is relatively high but far from perfect and
varies by language. Models often flip stance across languages or between
formats; numeric ratings frequently diverge from the initial binary choice, and
the most consistent models are not always those judged most accurate or
neutral. Our research brings to light the predisposition of models to such
inconsistencies, within a specific contextualization of the language for the
question asked.

</details>


### [92] [Beyond Manuals and Tasks: Instance-Level Context Learning for LLM Agents](https://arxiv.org/abs/2510.02369)
*Kuntai Cai,Juncheng Liu,Xianglin Yang,Zhaojie Niu,Xiaokui Xiao,Xing Chen*

Main category: cs.CL

TL;DR: LLM代理通常接收两种类型的上下文：环境级手册和任务级指导。本文提出第三种被忽视的上下文类型：实例级上下文，即与特定环境实例相关的、可验证且可重用的事实（如物体位置、制作配方、本地规则）。实例级上下文的缺失是LLM代理在复杂任务中失败的常见原因。解决该问题需要高效地探索、验证和格式化事实。本文将此问题形式化为实例级上下文学习（ILCL），并提出一种任务无关的方法，该方法通过引导探索、使用TODO森林优化行动优先级以及轻量级的计划-行动-提取循环来解决。该方法能自动生成高精度的上下文文档，可供多个下游任务和代理重用，从而分摊初始探索成本。实验表明，该方法在TextWorld、ALFWorld和Crafter等环境中，成功率和效率均有显著提升，例如，在TextWorld中，ReAct的平均成功率从37%提升到95%，IGE从81%提升到95%。通过将一次性探索转化为持久可重用的知识，该方法能使LLM代理更可靠、更高效。


<details>
  <summary>Details</summary>
Motivation: LLM代理在复杂任务中通常会因为缺乏实例级上下文（与特定环境实例相关的、可验证且可重用的事实）而失败，因为成功不仅依赖于推理全局规则或任务提示，还需要基于精确、持久的事实进行决策。

Method: 提出实例级上下文学习（ILCL）问题，并开发一种任务无关的方法。该方法通过引导探索、使用TODO森林智能排序行动以及轻量级的计划-行动-提取循环来自动生成高精度的、可重用的上下文文档，从而降低LLM代理在复杂任务中的失败率并提高效率。

Result: 在TextWorld、ALFWorld和Crafter的实验中，该方法显著提高了LLM代理的成功率和效率。例如，在TextWorld中，ReAct的平均成功率从37%提升到95%，IGE从81%提升到95%。

Conclusion: 通过将一次性探索转化为持久可重用的知识（实例级上下文），该方法能够有效弥补现有LLM代理上下文的不足，使其在处理复杂任务时更加可靠和高效。

Abstract: Large language model (LLM) agents typically receive two kinds of context: (i)
environment-level manuals that define interaction interfaces and global rules,
and (ii) task-level guidance or demonstrations tied to specific goals. In this
work, we identify a crucial but overlooked third type of context,
instance-level context, which consists of verifiable and reusable facts tied to
a specific environment instance, such as object locations, crafting recipes,
and local rules. We argue that the absence of instance-level context is a
common source of failure for LLM agents in complex tasks, as success often
depends not only on reasoning over global rules or task prompts but also on
making decisions based on precise and persistent facts. Acquiring such context
requires more than memorization: the challenge lies in efficiently exploring,
validating, and formatting these facts under tight interaction budgets. We
formalize this problem as Instance-Level Context Learning (ILCL) and introduce
our task-agnostic method to solve it. Our method performs a guided exploration,
using a compact TODO forest to intelligently prioritize its next actions and a
lightweight plan-act-extract loop to execute them. This process automatically
produces a high-precision context document that is reusable across many
downstream tasks and agents, thereby amortizing the initial exploration cost.
Experiments across TextWorld, ALFWorld, and Crafter demonstrate consistent
gains in both success and efficiency: for instance, ReAct's mean success rate
in TextWorld rises from 37% to 95%, while IGE improves from 81% to 95%. By
transforming one-off exploration into persistent, reusable knowledge, our
method complements existing contexts to enable more reliable and efficient LLM
agents.

</details>


### [93] [Training Dynamics of Parametric and In-Context Knowledge Utilization in Language Models](https://arxiv.org/abs/2510.02370)
*Minsung Kim,Dong-Kyum Kim,Jea Kwon,Nakyeong Yang,Kyomin Jung,Meeyoung Cha*

Main category: cs.CL

TL;DR: 检索增强生成（RAG）模型在处理上下文知识和预训练知识之间的冲突时，缺乏有效的知识仲裁策略。本研究首次通过控制训练条件来系统研究这些策略的形成，发现在合成传记语料库中，文档内事实重复有助于提升模型参数知识和上下文知识能力，而语料库中的不一致信息或分布不均则能促使模型发展出更鲁棒的知识仲裁策略，从而有效整合参数知识和上下文知识。


<details>
  <summary>Details</summary>
Motivation: 当前的大语言模型在处理检索到的上下文知识和预训练的参数知识时，常常会发生冲突。如果模型盲目接受外部知识，就容易被错误信息误导；而如果模型过于依赖参数知识，则无法有效利用检索到的信息。尽管检索增强生成（RAG）已被广泛采用，但我们对训练过程中知识仲裁策略的形成机制仍缺乏系统性的理解。这可能导致预训练模型产生不良的仲裁行为，并造成计算资源的浪费。

Method: 本研究通过在合成传记语料库上训练Transformer语言模型，并系统地控制各种训练条件，来研究训练条件如何影响模型对上下文知识和参数知识的使用及其之间的仲裁。

Result: 实验结果表明，在文档内部重复事实有助于参数知识和上下文知识能力的培养。此外，在包含不一致信息或分布不均的语料库上进行训练，能够促使模型发展出更鲁棒的利用参数知识和上下文知识的策略。这些非理想的语料库特性对于学习鲁棒的知识仲裁至关重要。

Conclusion: 本研究的见解为预训练能够和谐地整合参数知识和上下文知识的模型提供了具体、实证的指导。与其将语料库中的不一致信息或分布不均视为需要消除的瑕疵，不如认识到它们在学习鲁棒知识仲裁中的重要作用。

Abstract: Large language models often encounter conflicts between in-context knowledge
retrieved at inference time and parametric knowledge acquired during
pretraining. Models that accept external knowledge uncritically are vulnerable
to misinformation, whereas models that adhere rigidly to parametric knowledge
fail to benefit from retrieval. Despite the widespread adoption of
retrieval-augmented generation, we still lack a systematic understanding of
what shapes knowledge-arbitration strategies during training. This gap risks
producing pretrained models with undesirable arbitration behaviors and,
consequently, wasting substantial computational resources after the pretraining
budget has already been spent. To address this problem, we present the first
controlled study of how training conditions influence models' use of in-context
and parametric knowledge, and how they arbitrate between them. We train
transformer-based language models on a synthetic biographies corpus while
systematically controlling various conditions. Our experiments reveal that
intra-document repetition of facts fosters the development of both parametric
and in-context capabilities. Moreover, training on a corpus that contains
inconsistent information or distributional skew encourages models to develop
robust strategies for leveraging parametric and in-context knowledge. Rather
than viewing these non-ideal properties as artifacts to remove, our results
indicate that they are important for learning robust arbitration. These
insights offer concrete, empirical guidance for pretraining models that
harmoniously integrate parametric and in-context knowledge.

</details>


### [94] [Pretraining with hierarchical memories: separating long-tail and common knowledge](https://arxiv.org/abs/2510.02375)
*Hadi Pouransari,David Grangier,C Thomas,Michael Kirchhof,Oncel Tuzel*

Main category: cs.CL

TL;DR: 小型语言模型通过访问大型分层参数化记忆库来存储世界知识，从而在有限的设备上实现高效推理。


<details>
  <summary>Details</summary>
Motivation: 现代语言模型依赖于参数扩展来提升性能，但这种方法在边缘设备上存在内存和计算限制。本文旨在通过一种内存增强架构和预训练策略来解决这个问题，使小型模型能够访问大型知识库。

Method: 提出了一种内存增强架构，在预训练和推理过程中，模型会从大型分层参数化记忆库中提取与上下文相关的记忆块并添加到模型中。小型语言模型负责存储通用知识和推理能力，而记忆库则存储长尾知识。

Result: 实验表明，一个拥有1.6亿参数并访问了1800万参数记忆的模型（从4.6B参数的记忆库中提取）的性能，与参数量超过其两倍的常规模型相当。该方法在不同Transformer架构上表现稳健，记忆库可扩展至21B参数。

Conclusion: 内存增强方法是一种有效的方式，可以在参数量受限的情况下提升语言模型的性能，尤其适用于边缘计算场景。

Abstract: The impressive performance gains of modern language models currently rely on
scaling parameters: larger models store more world knowledge and reason better.
Yet compressing all world knowledge into parameters is unnecessary, as only a
fraction is used per prompt, and impractical for edge devices with limited
inference-time memory and compute. We address this shortcoming by a
memory-augmented architecture and a pretraining strategy aligned with existing
hardware paradigms. We introduce small language models that access large
hierarchical parametric memory banks encoding world knowledge. During
pretraining and inference, we fetch a small, context-dependent memory block and
add it to the model. Our pretraining learns to store long-tail world knowledge
in the memory parameters, while the small language model acts as an anchor
capturing common knowledge and general reasoning abilities. Through
trillion-token-scale experiments, we show significant gains: a 160M-parameters
model augmented with an 18M-parameters memory fetched from a 4.6B memory bank
obtains comparable performance to a regular model with more than 2x the
parameters. Through extensive experiments, we study the optimal type and size
of parametric memories in transformers, scaling them to over 21B parameters. We
find that our proposed hierarchical feed-forward memories work robustly across
transformer architectures, whether added during pretraining or post-hoc.

</details>


### [95] [Uncertainty-Aware Answer Selection for Improved Reasoning in Multi-LLM Systems](https://arxiv.org/abs/2510.02377)
*Aakriti Agrawal,Rohith Aralikatti,Anirudh Satheesh,Souradip Chakraborty,Amrit Singh Bedi,Furong Huang*

Main category: cs.CL

TL;DR: 从多个大型语言模型（LLM）中选择最可靠的响应，尤其是在资源受限的情况下，仍然是一个挑战。现有方法通常依赖于昂贵的外部验证器、人工评估或需要单个模型多次采样的自洽性技术。虽然多LLM系统比单模型产生更多样化的响应，因此具有更大的潜力，但它们的表现通常不如单LLM自洽性。我们提出了一种原则性、新颖且计算效率高的方法，利用这些模型固有的知识和置信度，通过校准的对数似然分数，从多个不同的LLM中选择最佳响应。我们的方法在GSM8K、MMLU（6个子集）和ARC数据集的辩论（多轮LLM讨论）和非辩论（具有多个LLM的最佳N）设置中，分别在准确性上提高了约4%、3%和5%。


<details>
  <summary>Details</summary>
Motivation: 选择最可靠的LLM响应，尤其是在资源受限的情况下，是一个挑战。现有方法存在局限性，而多LLM系统潜力巨大但表现不佳。

Method: 提出一种利用校准的对数似然分数，从多个LLM中选择最佳响应的新颖、高效方法。

Result: 在GSM8K、MMLU和ARC数据集上，辩论和非辩论设置下的准确性分别提高了约4%、3%和5%。

Conclusion: 所提出的方法能够有效地从多个LLM中选择最佳响应，并提高模型在各种基准测试上的性能。

Abstract: Large Language Models (LLMs) have demonstrated exceptional capabilities, yet
selecting the most reliable response from multiple LLMs remains a challenge,
particularly in resource-constrained settings. Existing approaches often depend
on costly external verifiers, human evaluators, or self-consistency techniques
that require multiple samples from a single model. While multi-LLM systems
produce more diverse responses than single models and thus have greater
potential, they often underperform compared to single LLM self-consistency. We
propose a principled, novel and computationally efficient method to select the
best response from multiple different LLMs using a calibrated log-likelihood
score, implicitly leveraging the inherent knowledge and confidence of these
models. Our method demonstrates improvements of approx. 4%, 3%, and 5% across
both debate (multi-round LLM discussions) and non-debate (Best-of-N with
multiple LLMs) settings on GSM8K, MMLU (6 subsets), and ARC datasets
respectively.

</details>


### [96] [Learning to Route: A Rule-Driven Agent Framework for Hybrid-Source Retrieval-Augmented Generation](https://arxiv.org/abs/2510.02388)
*Haoyue Bai,Haoyu Wang,Shengyu Chen,Zhengzhang Chen,Lu-An Tang,Wei Cheng,Haifeng Chen,Yanjie Fu*

Main category: cs.CL

TL;DR: LLMs在特定领域问答中表现不佳，现有RAG系统多依赖非结构化文档，忽视了关系型数据库。本文提出一个规则驱动的路由框架，结合数据库和文档的优势，并通过实验验证其在准确性和成本方面的优越性。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统主要依赖非结构化文档，忽视了关系型数据库，而数据库在特定领域（金融、医疗、科研等）提供精确、及时、高效的事实信息。

Method: 提出一个规则驱动的路由框架，包括：1. 路由代理：根据显式规则为候选增强路径打分并选择最合适的路径。2. 规则制定专家代理：利用问答反馈随时间推移优化规则。3. 路径级元缓存：重用相似查询的路由决策以降低延迟和成本。

Result: 在三个问答基准测试中，该框架的准确性持续优于静态策略和学习型路由基线，同时保持了适度的计算成本。

Conclusion: 结合数据库和文档的优势，并采用规则驱动的路由策略，能够有效提升LLM在领域特定问答中的准确性和效率。

Abstract: Large Language Models (LLMs) have shown remarkable performance on general
Question Answering (QA), yet they often struggle in domain-specific scenarios
where accurate and up-to-date information is required. Retrieval-Augmented
Generation (RAG) addresses this limitation by enriching LLMs with external
knowledge, but existing systems primarily rely on unstructured documents, while
largely overlooking relational databases, which provide precise, timely, and
efficiently queryable factual information, serving as indispensable
infrastructure in domains such as finance, healthcare, and scientific research.
Motivated by this gap, we conduct a systematic analysis that reveals three
central observations: (i) databases and documents offer complementary strengths
across queries, (ii) naively combining both sources introduces noise and cost
without consistent accuracy gains, and (iii) selecting the most suitable source
for each query is crucial to balance effectiveness and efficiency. We further
observe that query types show consistent regularities in their alignment with
retrieval paths, suggesting that routing decisions can be effectively guided by
systematic rules that capture these patterns. Building on these insights, we
propose a rule-driven routing framework. A routing agent scores candidate
augmentation paths based on explicit rules and selects the most suitable one; a
rule-making expert agent refines the rules over time using QA feedback to
maintain adaptability; and a path-level meta-cache reuses past routing
decisions for semantically similar queries to reduce latency and cost.
Experiments on three QA benchmarks demonstrate that our framework consistently
outperforms static strategies and learned routing baselines, achieving higher
accuracy while maintaining moderate computational cost.

</details>


### [97] [KnowledgeSmith: Uncovering Knowledge Updating in LLMs with Model Editing and Unlearning](https://arxiv.org/abs/2510.02392)
*Yinyi Luo,Zhexian Zhou,Hao Chen,Kai Qiu,Marios Savvides,Yixuan Li,Jindong Wang*

Main category: cs.CL

TL;DR: LLMs的知识更新机制因评估不足而未被充分探索。本文提出了KnowledgeSmith框架，将知识编辑和机器遗忘统一为约束优化问题，并提供自动数据集生成器，以系统地研究LLMs的知识更新机制。实验揭示了知识传播、可塑性缩放、一致性和鲁棒性方面的深刻见解，例如LLMs的知识更新与人类不同，且存在一致性-容量权衡。


<details>
  <summary>Details</summary>
Motivation: 知识编辑和机器遗忘是LLMs保持更新的常用方法，但由于评估不足，其知识更新机制仍未得到充分研究。本文旨在系统地理解LLMs的知识更新机制。

Method: 提出KnowledgeSmith统一框架，将知识编辑和遗忘视为约束优化问题，并开发自动数据集生成器，以在不同图级别和数据规模上进行干预，从而控制不同修改策略在模型知识中的传播方式。

Result: 实验表明，LLMs在不同知识级别上的更新方式与人类不同，并且存在一致性-容量权衡。研究揭示了知识传播、可塑性缩放、一致性和鲁棒性方面的细微差别。

Conclusion: 本文提出的KnowledgeSmith框架和实验发现为了设计更可靠、可扩展的知识更新策略提供了有价值的见解，尤其指出了LLMs的知识更新机制与人类存在差异，并揭示了模型在一致性和容量之间可能存在的权衡。

Abstract: Knowledge editing and machine unlearning are two popular approaches for large
language models (LLMs) to stay up-to-date. However, the knowledge updating
mechanism of LLMs remains largely unexplored due to insufficient, isolated, and
small-scale evaluation. For instance, are LLMs similar to humans in modifying
certain knowledge? What differs editing and unlearning as training data
increases? This paper proposes KnowledgeSmith, a unified framework to
systematically understand the updating mechanism of LLMs. We first cast editing
and unlearning as instances of one constrained optimization problem. Then, we
propose an automatic dataset generator that provides structured interventions
across multiple graph levels and data scales, enabling controlled studies of
how different modification strategies propagate through model knowledge.
Extensive experiments demonstrate nuanced insights over knowledge propagation,
plasticity scaling, consistency, and robustness. For instance, our results show
that LLMs do not exhibit similar updating as humans for different levels of
knowledge, and there exists consistency-capacity trade-off. We hope our
findings can offer suggestions to the design of more reliable and scalable
strategies. Code: https://github.com/AIFrontierLab/KnowledgeSmith.git

</details>


### [98] [Retrieval and Augmentation of Domain Knowledge for Text-to-SQL Semantic Parsing](https://arxiv.org/abs/2510.02394)
*Manasi Patwardhan,Ayush Agarwal,Shabbirhussain Bhaisaheb,Aseem Arora,Lovekesh Vig,Sunita Sarawagi*

Main category: cs.CL

TL;DR: LLMs在翻译自然语言查询到SQL时，其性能因数据库而异。本文提出了一种将结构化领域声明与数据库关联的系统框架，并通过子字符串匹配检索相关声明，证明其比现有方法更实用、准确。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在翻译NL到SQL时，性能受数据库领域词汇和模式结构影响，而现有基准测试依赖不切实际的查询特定文本提示来表达领域知识。

Method: 提出了一种在数据库级别关联结构化领域声明的系统框架，并使用子字符串匹配检索与用户查询相关的声明。

Result: 在涵盖11个真实数据库模式和5个LLM的评估中，发现数据库级别的结构化领域声明优于查询特定的文本声明，并且子字符串匹配的检索方法精度显著更高。

Conclusion: 数据库级别的结构化领域声明比查询特定的文本声明更实用、更准确，并且基于子字符串匹配的检索方法能提供更高的精度。

Abstract: The performance of Large Language Models (LLMs) for translating Natural
Language (NL) queries into SQL varies significantly across databases (DBs). NL
queries are often expressed using a domain specific vocabulary, and mapping
these to the correct SQL requires an understanding of the embedded domain
expressions, their relationship to the DB schema structure. Existing benchmarks
rely on unrealistic, ad-hoc query specific textual hints for expressing domain
knowledge. In this paper, we propose a systematic framework for associating
structured domain statements at the database level. We present retrieval of
relevant structured domain statements given a user query using sub-string level
match. We evaluate on eleven realistic DB schemas covering diverse domains
across five open-source and proprietary LLMs and demonstrate that (1) DB level
structured domain statements are more practical and accurate than existing
ad-hoc query specific textual domain statements, and (2) Our sub-string match
based retrieval of relevant domain statements provides significantly higher
accuracy than other retrieval approaches.

</details>


### [99] [Words That Make Language Models Perceive](https://arxiv.org/abs/2510.02425)
*Sophie L. Wang,Phillip Isola,Brian Cheung*

Main category: cs.CL

TL;DR: 文本模型可以通过感官提示来激活视觉和听觉的表征。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）虽然只接受文本训练，但其内部表征会受到语言中编码的多模态规律的潜在影响。本研究旨在测试显式感官提示是否能够显现这种潜在结构，使纯文本LLM在表征上更接近专门的视觉和听觉编码器。

Method: 通过向模型发出“看”或“听”的感官提示，诱导模型在进行下一个词元预测时，如同条件化于实际并未提供的潜在视觉或听觉证据一样进行推理。

Result: 研究结果表明，轻量级的提示工程能够可靠地激活纯文本训练的LLM中的特定模态表征。

Conclusion: 显式感官提示可以有效地引导纯文本LLM激活其内部的、与特定感官（如视觉和听觉）相关的表征。

Abstract: Large language models (LLMs) trained purely on text ostensibly lack any
direct perceptual experience, yet their internal representations are implicitly
shaped by multimodal regularities encoded in language. We test the hypothesis
that explicit sensory prompting can surface this latent structure, bringing a
text-only LLM into closer representational alignment with specialist vision and
audio encoders. When a sensory prompt tells the model to 'see' or 'hear', it
cues the model to resolve its next-token predictions as if they were
conditioned on latent visual or auditory evidence that is never actually
supplied. Our findings reveal that lightweight prompt engineering can reliably
activate modality-appropriate representations in purely text-trained LLMs.

</details>


### [100] [Unraveling Syntax: How Language Models Learn Context-Free Grammars](https://arxiv.org/abs/2510.02524)
*Laura Ying Schulz,Daniel Mitropolsky,Tomaso Poggio*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We introduce a new framework for understanding how language models acquire
syntax. While large models achieve impressive results, little is known about
their learning dynamics. Our approach starts with the observation that most
domains of interest, such as natural language syntax, coding languages,
arithmetic problems, are captured by probabilistic context-free grammars
(PCFGs). We study the learning dynamics of small models trained on synthetic
languages generated from PCFGs, enabling precise control over grammar
complexity, recursion depth, and subgrammar structure. We prove several
general, recursive formulae for the training loss and Kullback-Leibler
divergence over the subgrammar structure of a PCFG. Empirically, we find that
unlike children, who first master simple substructures before progressing to
more complex constructions, transformers reduce loss across all subgrammars in
parallel. We further show that subgrammar pretraining can improve the final
loss for smaller models, and that pretrained models develop internal
representations more aligned with the grammar's substructure. Finally, we
demonstrate that models struggle with deeper recursive structures (a limitation
even of large language models), revealing fundamental challenges in how neural
networks represent hierarchical syntax. Overall, our work initiates the study
of the learning dynamics of transformers on PCFGs as a versatile testbed for
probing learning in language models, opening a research direction with many
open questions.

</details>


### [101] [Hierarchical Semantic Retrieval with Cobweb](https://arxiv.org/abs/2510.02539)
*Anant Gupta,Karthik Singaravadivelan,Zekun Wang*

Main category: cs.CL

TL;DR: Cobweb框架利用层次化原型树来组织句子嵌入，实现文档检索，提供多粒度相关性信号和可解释的检索路径，在MS MARCO和QQP数据集上表现出与点积搜索相当的有效性，并提高了对嵌入质量的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有神经文档检索方法将语料库视为扁平的向量集合，未充分利用语料库结构且解释性不足。

Method: 使用Cobweb框架，将句子嵌入组织成原型树，通过粗到细的遍历进行文档排序。内部节点作为概念原型，提供多粒度相关性信号和透明的检索路径。实现了两种推理方法：广义优先搜索和轻量级路径求和排序器。

Result: 在MS MARCO和QQP数据集上，使用BERT/T5和GPT-2等表示进行评估。结果表明，Cobweb的检索方法在强编码器嵌入上可媲美点积搜索，并在kNN退化时保持鲁棒性；而使用GPT-2向量时，点积搜索性能急剧下降，Cobweb仍能检索到相关结果。

Conclusion: Cobweb框架提供了有竞争力的检索效果、对嵌入质量的鲁棒性、可扩展性以及通过层次化原型实现的、可解释的检索。

Abstract: Neural document retrieval often treats a corpus as a flat cloud of vectors
scored at a single granularity, leaving corpus structure underused and
explanations opaque. We use Cobweb--a hierarchy-aware framework--to organize
sentence embeddings into a prototype tree and rank documents via coarse-to-fine
traversal. Internal nodes act as concept prototypes, providing multi-granular
relevance signals and a transparent rationale through retrieval paths. We
instantiate two inference approaches: a generalized best-first search and a
lightweight path-sum ranker. We evaluate our approaches on MS MARCO and QQP
with encoder (e.g., BERT/T5) and decoder (GPT-2) representations. Our results
show that our retrieval approaches match the dot product search on strong
encoder embeddings while remaining robust when kNN degrades: with GPT-2
vectors, dot product performance collapses whereas our approaches still
retrieve relevant results. Overall, our experiments suggest that Cobweb
provides competitive effectiveness, improved robustness to embedding quality,
scalability, and interpretable retrieval via hierarchical prototypes.

</details>


### [102] [Knowledge-Graph Based RAG System Evaluation Framework](https://arxiv.org/abs/2510.02549)
*Sicheng Dong,Vahid Zolfaghari,Nenad Petrovic,Alois Knoll*

Main category: cs.CL

TL;DR: 传统评估指标难以有效捕捉现代LLM生成内容的关键特征，本文提出了一种基于知识图谱（KG）的RAG评估新范式，通过多跳推理和语义社区聚类来生成更全面的评分指标，并与RAGAS和人工标注进行对比验证，证明了其在评估RAG系统方面的优越性。


<details>
  <summary>Details</summary>
Motivation: 评估RAG系统仍然是一个挑战性任务，传统评估指标难以有效捕捉现代LLM生成内容的流畅性和自然性等关键特征。

Method: 将RAGAS工具扩展为一种基于知识图谱（KG）的评估范式，该范式能够进行多跳推理和语义社区聚类，从而得到更全面的评分指标。通过将新指标与RAGAS分数进行比较，并构建一个包含人工标注的数据集来评估自动评估指标与人工判断的相关性。

Result: 提出的KG-based评估方法与RAGAS得分进行了比较，并构建了一个人工标注的数据集来评估人类判断与自动指标之间的相关性。此外，还进行了有针对性的实验，证明了KG-based评估方法对生成输出中细微语义差异更敏感。

Conclusion: KG-based评估范式能够更全面、更细致地评估RAG系统，尤其是在处理细微语义差异方面优于现有方法，为RAG系统评估提供了新的方向。

Abstract: Large language models (LLMs) has become a significant research focus and is
utilized in various fields, such as text generation and dialog systems. One of
the most essential applications of LLM is Retrieval Augmented Generation (RAG),
which greatly enhances generated content's reliability and relevance. However,
evaluating RAG systems remains a challenging task. Traditional evaluation
metrics struggle to effectively capture the key features of modern
LLM-generated content that often exhibits high fluency and naturalness.
Inspired by the RAGAS tool, a well-known RAG evaluation framework, we extended
this framework into a KG-based evaluation paradigm, enabling multi-hop
reasoning and semantic community clustering to derive more comprehensive
scoring metrics. By incorporating these comprehensive evaluation criteria, we
gain a deeper understanding of RAG systems and a more nuanced perspective on
their performance. To validate the effectiveness of our approach, we compare
its performance with RAGAS scores and construct a human-annotated subset to
assess the correlation between human judgments and automated metrics. In
addition, we conduct targeted experiments to demonstrate that our KG-based
evaluation method is more sensitive to subtle semantic differences in generated
outputs. Finally, we discuss the key challenges in evaluating RAG systems and
highlight potential directions for future research.

</details>


### [103] [Transcribe, Translate, or Transliterate: An Investigation of Intermediate Representations in Spoken Language Models](https://arxiv.org/abs/2510.02569)
*Tolúl\d{o}pé Ògúnrèmí,Christopher D. Manning,Dan Jurafsky,Karen Livescu*

Main category: cs.CL

TL;DR: SLMs中的模态适配器(MA)将语音编码器的输出映射到LM解码器可理解的表示。本研究调查了三种SLMs（SALMONN、Qwen2-Audio和Phi-4-Multimodal-Instruct）中MA的输出表示，发现MA有两种表示策略：一种是基于英语的“共同语”，另一种是语音表示，但用英语单词表达。


<details>
  <summary>Details</summary>
Motivation: 了解模态适配器（MA）在语音语言模型（SLM）中如何转换表示，这对整合语音和大型语言模型至关重要。

Method: 通过查找与MA表示最接近的解码器LM标记，检查三种SLM（SALMONN、Qwen2-Audio和Phi-4-Multimodal-Instruct）的MA输出表示。

Result: 基于Whisper编码器的模型（SALMONN、Qwen2-Audio）的MA似乎使用基于英语的“共同语”来表示输入的含义，能够处理未在指令调整中见过的语言。不使用Whisper编码器的模型（Phi-4-Multimodal-Instruct）的MA则表示输入的语音，但使用英语单词表达。

Conclusion: MA的表示策略（“共同语”或语音表示）可能取决于语音编码器是仅为语音识别训练还是也为翻译训练。

Abstract: Spoken language models (SLMs) that integrate speech with large language
models (LMs) rely on modality adapters (MAs) to map the output of speech
encoders to a representation that is understandable to the decoder LM. Yet we
know very little about how these crucial MAs transform representations. Here we
examine the MA output representation in three SLMs (SALMONN, Qwen2-Audio and
Phi-4-Multimodal-Instruct). By finding the nearest decoder LM token to an MA
representation, we uncover two strategies for MA representations. For models
using a Whisper encoder, MAs appear to represent the meaning of the input using
an English-based interlingua, allowing them to handle languages unseen in
instruction tuning. For models that don't, like Phi-4-Multimodal-Instruct, MAs
instead represent the phonetics of the input, but expressed with English words.
We hypothesise that which arises depends on whether the speech encoder is
trained only for speech recognition or also for translation.

</details>


### [104] [Evaluation Framework for Highlight Explanations of Context Utilisation in Language Models](https://arxiv.org/abs/2510.02629)
*Jingyi Sun,Pepa Atanasova,Sagnik Ray Choudhury,Sekh Mainul Islam,Isabelle Augenstein*

Main category: cs.CL

TL;DR: 现有高亮解释（HE）方法在解释语言模型（LM）的上下文利用能力方面存在不足，本研究提出了一个评估框架，并发现MechLight表现最佳，但所有方法在长上下文和位置偏差方面仍面临挑战。


<details>
  <summary>Details</summary>
Motivation: 现有评估高亮解释（HE）方法在解释语言模型（LM）上下文利用能力方面的不足，旨在提供一个准确评估HE方法有效性的标准。

Method: 提出一个包含受控测试用例和已知地面真实上下文使用情况的评估框架，并用该框架评估了四种HE方法（三种现有方法和一种新改编的MechLight方法），在四种上下文场景、四个数据集和五个LM上进行了测试。

Result: MechLight在所有上下文场景中的表现优于其他方法。然而，所有HE方法在处理长上下文和存在位置偏差方面都存在困难，表明在提供可靠的上下文利用解释方面存在根本性挑战。

Conclusion: 尽管MechLight在评估中表现最好，但所有高亮解释（HE）方法在准确解释语言模型（LM）的上下文利用能力方面仍面临严峻挑战，特别是在长上下文和位置偏差方面，需要新的方法来解决这些问题。

Abstract: Context utilisation, the ability of Language Models (LMs) to incorporate
relevant information from the provided context when generating responses,
remains largely opaque to users, who cannot determine whether models draw from
parametric memory or provided context, nor identify which specific context
pieces inform the response. Highlight explanations (HEs) offer a natural
solution as they can point the exact context pieces and tokens that influenced
model outputs. However, no existing work evaluates their effectiveness in
accurately explaining context utilisation. We address this gap by introducing
the first gold standard HE evaluation framework for context attribution, using
controlled test cases with known ground-truth context usage, which avoids the
limitations of existing indirect proxy evaluations. To demonstrate the
framework's broad applicability, we evaluate four HE methods -- three
established techniques and MechLight, a mechanistic interpretability approach
we adapt for this task -- across four context scenarios, four datasets, and
five LMs. Overall, we find that MechLight performs best across all context
scenarios. However, all methods struggle with longer contexts and exhibit
positional biases, pointing to fundamental challenges in explanation accuracy
that require new approaches to deliver reliable context utilisation
explanations at scale.

</details>


### [105] [Mind the Gap: Linguistic Divergence and Adaptation Strategies in Human-LLM Assistant vs. Human-Human Interactions](https://arxiv.org/abs/2510.02645)
*Fulei Zhang,Zhou Yu*

Main category: cs.CL

TL;DR: 用户与LLM聊天机器人和人类客服的交流方式不同，LLM需要适应这种变化。


<details>
  <summary>Details</summary>
Motivation: 研究用户与LLM聊天机器人和人类客服的沟通差异，以及如何提升LLM在部署后的适应性。

Method: 分析用户语言在与LLM和人类交互时的语法流畅度、礼貌度和词汇多样性差异，并实验数据增强和推理时用户消息重构两种策略来提升LLM的鲁棒性。

Result: 与人类交互相比，用户与LLM交互时在语法流畅度、礼貌度和词汇多样性上存在显著差异。数据增强策略能显著提升模型性能，而推理时重构效果不佳。

Conclusion: LLM需要适应用户沟通方式的变化，通过在训练阶段进行数据增强可以有效提升LLM的鲁棒性。

Abstract: As Large Language Models (LLMs) are increasingly deployed in customer-facing
applications, a critical yet underexplored question is how users communicate
differently with LLM chatbots compared to human agent. In this study, we
present empirical evidence that users adopt distinct communication styles when
users interact with chatbots versus human agents. Our analysis reveals
significant differences in grammatical fluency, politeness, and lexical
diversity in user language between the two settings. These findings suggest
that models trained exclusively on human-human interaction data may not
adequately accommodate the communication style shift that occurs once an LLM
chatbot is deployed. To enhance LLM robustness to post-launch communication
style changes, we experimented with two strategies: (1) data augmentation
during the post-training phase and (2) inference-time user message
reformulation. Our results indicate that models trained on stylistically
diverse datasets significantly outperform those trained exclusively on original
or stylistically uniform datasets, while inference-time reformulation proved
less effective. These insights help us to better adapt our models for improved
LLM-user interaction experiences.

</details>


### [106] [SoT: Structured-of-Thought Prompting Guides Multilingual Reasoning in Large Language Models](https://arxiv.org/abs/2510.02648)
*Rui Qi,Zhibo Man,Yufeng Chen,Fengran Mo,Jinan Xu,Kaiyu Huang*

Main category: cs.CL

TL;DR: SoT是一种无需训练的方法，通过语言思维转换和结构化知识转换，提升了LLM在多语言推理任务上的表现，将语言特异性语义信息转换为语言无关的结构化表示，从而增强模型对不同语言查询的理解能力，并引导模型进行更集中的推理，保持一致的推理路径。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLMs）在复杂推理任务上取得了进展，但由于资源限制，其推理能力难以迁移到低资源语言，导致多语言推理任务表现不佳。

Method: 提出了一种名为Structured-of-Thought (SoT) 的训练无关方法，通过语言思维转换和结构化知识转换两个主要步骤，将语言特异性语义信息转换为语言无关的结构化表示，以增强模型的多语言推理能力。

Result: 实验结果表明，SoT在多个多语言推理基准上优于多种强基线模型，并能适配不同的LLM骨干，还可以与其他训练无关策略结合以获得进一步提升。

Conclusion: SoT方法能够有效地提升LLM在多语言推理任务上的性能，并且具有良好的通用性和可扩展性。

Abstract: Recent developments have enabled Large Language Models (LLMs) to engage in
complex reasoning tasks through deep thinking. However, the capacity of
reasoning has not been successfully transferred to non-high-resource languages
due to resource constraints, which struggles with multilingual reasoning tasks.
To this end, we propose Structured-of-Thought (SoT), a training-free method
that improves the performance on multilingual reasoning through a multi-step
transformation: Language Thinking Transformation and Structured Knowledge
Transformation. The SoT method converts language-specific semantic information
into language-agnostic structured representations, enabling the models to
understand the query in different languages more sophisticated. Besides, SoT
effectively guides LLMs toward more concentrated reasoning to maintain
consistent underlying reasoning pathways when handling cross-lingual variations
in expression. Experimental results demonstrate that SoT outperforms several
strong baselines on multiple multilingual reasoning benchmarks when adapting to
various backbones of LLMs. It can also be integrated with other training-free
strategies for further improvements. Our code is available at
https://github.com/Cherry-qwq/SoT.

</details>


### [107] [Self-Improvement in Multimodal Large Language Models: A Survey](https://arxiv.org/abs/2510.02665)
*Shijian Deng,Kai Wang,Tianyu Yang,Harsh Singh,Yapeng Tian*

Main category: cs.CL

TL;DR: 本文是首个对多模态大语言模型（MLLMs）的自我改进进行全面调查的文献综述，探讨了从数据收集、数据组织到模型优化的方法，并讨论了评估和应用，最后指出了未来的挑战和研究方向。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）的自我改进具有巨大潜力，可以利用多样化的数据源，并开发更通用的自我改进模型。

Method: 本文从数据收集、数据组织和模型优化三个角度对现有文献进行了结构化概述。

Result: 本文全面概述了多模态大语言模型（MLLMs）的自我改进领域的现状，并讨论了常用的评估方法和下游应用。

Conclusion: 尽管多模态大语言模型的自我改进领域仍有许多开放的挑战，但未来研究方向也十分广阔。

Abstract: Recent advancements in self-improvement for Large Language Models (LLMs) have
efficiently enhanced model capabilities without significantly increasing costs,
particularly in terms of human effort. While this area is still relatively
young, its extension to the multimodal domain holds immense potential for
leveraging diverse data sources and developing more general self-improving
models. This survey is the first to provide a comprehensive overview of
self-improvement in Multimodal LLMs (MLLMs). We provide a structured overview
of the current literature and discuss methods from three perspectives: 1) data
collection, 2) data organization, and 3) model optimization, to facilitate the
further development of self-improvement in MLLMs. We also include commonly used
evaluations and downstream applications. Finally, we conclude by outlining open
challenges and future research directions.

</details>


### [108] [Uncertainty as Feature Gaps: Epistemic Uncertainty Quantification of LLMs in Contextual Question-Answering](https://arxiv.org/abs/2510.02671)
*Yavuz Bakman,Sungmin Kang,Zhiqi Huang,Duygu Nur Yaldiz,Catarina G. Belém,Chenyang Zhu,Anoop Kumar,Alfy Samuel,Salman Avestimehr,Daben Liu,Sai Praneeth Karimireddy*

Main category: cs.CL

TL;DR: 本研究提出了一种针对上下文问答任务的不确定性量化方法，通过量化模型在理解和利用上下文信息时的认知不确定性，弥补了现有研究在闭卷问答之外的不足。


<details>
  <summary>Details</summary>
Motivation: 现有不确定性量化研究主要集中在闭卷问答，而忽略了在实际应用中更重要的上下文问答任务。

Method: 提出了一种通用的、与任务无关的、基于token层面的不确定性度量方法，将其分解为认知不确定性，并通过模拟理想模型来近似真实分布，进而推导出认知不确定性的上限，并将其解释为模型隐藏表示中的语义特征差距。将此框架应用于上下文问答，通过上下文依赖性、上下文理解和诚实度三个特征来近似差距，并利用可解释性方法和集成学习来计算不确定性分数。

Result: 在多个上下文问答基准测试（包括分布内和分布外设置）上，所提出的方法在不确定性量化方面显著优于最先进的无监督和监督方法，取得了高达13个点的PRR提升，且推理开销可忽略不计。

Conclusion: 该方法为上下文问答任务提供了一种理论上可靠且在实践中有效的不确定性量化方案，显著提高了模型在处理真实世界复杂问答场景时的可靠性。

Abstract: Uncertainty Quantification (UQ) research has primarily focused on closed-book
factual question answering (QA), while contextual QA remains unexplored,
despite its importance in real-world applications. In this work, we focus on UQ
for the contextual QA task and propose a theoretically grounded approach to
quantify epistemic uncertainty. We begin by introducing a task-agnostic,
token-level uncertainty measure defined as the cross-entropy between the
predictive distribution of the given model and the unknown true distribution.
By decomposing this measure, we isolate the epistemic component and approximate
the true distribution by a perfectly prompted, idealized model. We then derive
an upper bound for epistemic uncertainty and show that it can be interpreted as
semantic feature gaps in the given model's hidden representations relative to
the ideal model. We further apply this generic framework to the contextual QA
task and hypothesize that three features approximate this gap: context-reliance
(using the provided context rather than parametric knowledge), context
comprehension (extracting relevant information from context), and honesty
(avoiding intentional lies). Using a top-down interpretability approach, we
extract these features by using only a small number of labeled samples and
ensemble them to form a robust uncertainty score. Experiments on multiple QA
benchmarks in both in-distribution and out-of-distribution settings show that
our method substantially outperforms state-of-the-art unsupervised
(sampling-free and sampling-based) and supervised UQ methods, achieving up to a
13-point PRR improvement while incurring a negligible inference overhead.

</details>


### [109] [Time-To-Inconsistency: A Survival Analysis of Large Language Model Robustness to Adversarial Attacks](https://arxiv.org/abs/2510.02712)
*Yubo Li,Ramayya Krishnan,Rema Padman*

Main category: cs.CL

TL;DR: LLM在多轮对话中的鲁棒性评估不足，本文提出了一种新的生存分析框架，发现在语义漂移方面，突变比渐变更具破坏性，并证明了生存分析在评估LLM鲁棒性方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有评估LLM鲁棒性的方法无法捕捉长期多轮对话中对话质量下降的时间动态，因此需要新的评估框架。

Method: 运用生存分析模型（包括Cox比例风险模型、加速失效时间模型和随机生存森林）对9个先进LLM的36,951个对话轮次进行了分析。

Result: 研究发现，突变的、逐提示（P2P）的语义漂移会急剧增加对话失败的风险，而渐进的、累积的语义漂移则能显著降低失败风险，延长对话时长。加速失效时间模型在交互作用下表现出优越的预测性能。

Conclusion: 生存分析为评估LLM鲁棒性提供了一个强大的新范例，并为设计更具弹性的对话系统提供了见解，同时挑战了对话系统中对语义一致性必要性的传统观点。

Abstract: Large Language Models (LLMs) have revolutionized conversational AI, yet their
robustness in extended multi-turn dialogues remains poorly understood. Existing
evaluation frameworks focus on static benchmarks and single-turn assessments,
failing to capture the temporal dynamics of conversational degradation that
characterize real-world interactions. In this work, we present the first
comprehensive survival analysis of conversational AI robustness, analyzing
36,951 conversation turns across 9 state-of-the-art LLMs to model failure as a
time-to-event process. Our survival modeling framework-employing Cox
proportional hazards, Accelerated Failure Time, and Random Survival Forest
approaches-reveals extraordinary temporal dynamics. We find that abrupt,
prompt-to-prompt(P2P) semantic drift is catastrophic, dramatically increasing
the hazard of conversational failure. In stark contrast, gradual, cumulative
drift is highly protective, vastly reducing the failure hazard and enabling
significantly longer dialogues. AFT models with interactions demonstrate
superior performance, achieving excellent discrimination and exceptional
calibration. These findings establish survival analysis as a powerful paradigm
for evaluating LLM robustness, offer concrete insights for designing resilient
conversational agents, and challenge prevailing assumptions about the necessity
of semantic consistency in conversational AI Systems.

</details>


### [110] [TravelBench : Exploring LLM Performance in Low-Resource Domains](https://arxiv.org/abs/2510.02719)
*Srinivas Billa,Xiaonan Jing*

Main category: cs.CL

TL;DR: 现有LLM在低资源任务上的表现不佳，本研究构建了涵盖7种常见NLP任务的14个旅行领域数据集，并分析了LLM在这些任务上的准确性、扩展行为和推理能力，发现通用基准测试不足以评估低资源任务表现，且推理能力对小型LLM的提升尤为显著。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在低资源任务上的基准测试表现不足以反映其真实能力，难以在该领域开发有效解决方案。

Method: 创建了14个涵盖7种常见NLP任务的旅行领域数据集（使用匿名化的真实世界数据），并分析了LLM在这些任务上的准确性、扩展行为和推理能力。

Result: 通用基准测试结果不足以了解模型在低资源任务上的表现；即使有大量训练FLOPs，开箱即用的LLM在复杂、领域特定的场景中仍会遇到性能瓶颈；推理能力对小型LLM在某些任务上的提升作用更显著。

Conclusion: 通用基准测试不足以评估LLM在低资源任务上的表现，推理能力对小型LLM的性能提升尤为重要。

Abstract: Results on existing LLM benchmarks capture little information over the model
capabilities in low-resource tasks, making it difficult to develop effective
solutions in these domains. To address these challenges, we curated 14
travel-domain datasets spanning 7 common NLP tasks using anonymised data from
real-world scenarios, and analysed the performance across LLMs. We report on
the accuracy, scaling behaviour, and reasoning capabilities of LLMs in a
variety of tasks. Our results confirm that general benchmarking results are
insufficient for understanding model performance in low-resource tasks. Despite
the amount of training FLOPs, out-of-the-box LLMs hit performance bottlenecks
in complex, domain-specific scenarios. Furthermore, reasoning provides a more
significant boost for smaller LLMs by making the model a better judge on
certain tasks.

</details>


### [111] [PGMEL: Policy Gradient-based Generative Adversarial Network for Multimodal Entity Linking](https://arxiv.org/abs/2510.02726)
*KM Pooja,Cheng Long,Aixin Sun*

Main category: cs.CL

TL;DR: 本文提出了一种基于策略梯度生成对抗网络的联合多模态实体链接方法，通过生成高质量的负样本来提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多模态实体链接（MEL）中未探索高质量负样本选择的重要性，本文旨在解决此问题。

Method: 提出了一种生成对抗网络（GAN）框架，其中生成器负责生成高质量负样本，判别器负责度量学习。生成器使用策略梯度进行优化，命名为PGMEL。

Result: PGMEL在Wiki-MEL、Richpedia-MEL和WikiDiverse数据集上进行了实验，结果表明该方法通过选择具有挑战性的负样本学习到了有意义的表示，并且优于现有最先进的方法。

Conclusion: PGMEL在多模态实体链接任务中通过生成高质量负样本取得了显著的性能提升。

Abstract: The task of entity linking, which involves associating mentions with their
respective entities in a knowledge graph, has received significant attention
due to its numerous potential applications. Recently, various multimodal entity
linking (MEL) techniques have been proposed, targeted to learn comprehensive
embeddings by leveraging both text and vision modalities. The selection of
high-quality negative samples can potentially play a crucial role in
metric/representation learning. However, to the best of our knowledge, this
possibility remains unexplored in existing literature within the framework of
MEL. To fill this gap, we address the multimodal entity linking problem in a
generative adversarial setting where the generator is responsible for
generating high-quality negative samples, and the discriminator is assigned the
responsibility for the metric learning tasks. Since the generator is involved
in generating samples, which is a discrete process, we optimize it using policy
gradient techniques and propose a policy gradient-based generative adversarial
network for multimodal entity linking (PGMEL). Experimental results based on
Wiki-MEL, Richpedia-MEL and WikiDiverse datasets demonstrate that PGMEL learns
meaningful representation by selecting challenging negative samples and
outperforms state-of-the-art methods.

</details>


### [112] [IndiCASA: A Dataset and Bias Evaluation Framework in LLMs Using Contrastive Embedding Similarity in the Indian Context](https://arxiv.org/abs/2510.02742)
*Santhosh G S,Akshay Govind S,Gokul S Krishnan,Balaraman Ravindran,Sriraam Natarajan*

Main category: cs.CL

TL;DR: LLMs在印度等文化多元化背景下的偏见评估存在挑战，提出了一种基于对比学习的编码器框架和IndiCASA数据集，评估发现所有模型均存在偏见，尤其是与残疾相关的偏见。


<details>
  <summary>Details</summary>
Motivation: 现有基于嵌入的偏见评估方法在印度等多元文化背景下难以捕捉细微的刻板印象，需要更有效的评估方法。

Method: 提出了一种基于对比学习训练的编码器框架，通过嵌入相似度来捕捉细粒度的偏见。引入了包含2,575个句子、涵盖五个群体（种姓、性别、宗教、残疾、社会经济地位）的IndiCASA数据集。

Result: 评估结果显示，所有被评估的开源LLM都存在一定程度的刻板印象偏见，其中与残疾相关的偏见尤为普遍，而宗教偏见相对较低。

Conclusion: 所有LLM都存在偏见，需要进行更公平的模型开发。

Abstract: Large Language Models (LLMs) have gained significant traction across critical
domains owing to their impressive contextual understanding and generative
capabilities. However, their increasing deployment in high stakes applications
necessitates rigorous evaluation of embedded biases, particularly in culturally
diverse contexts like India where existing embedding-based bias assessment
methods often fall short in capturing nuanced stereotypes. We propose an
evaluation framework based on a encoder trained using contrastive learning that
captures fine-grained bias through embedding similarity. We also introduce a
novel dataset - IndiCASA (IndiBias-based Contextually Aligned Stereotypes and
Anti-stereotypes) comprising 2,575 human-validated sentences spanning five
demographic axes: caste, gender, religion, disability, and socioeconomic
status. Our evaluation of multiple open-weight LLMs reveals that all models
exhibit some degree of stereotypical bias, with disability related biases being
notably persistent, and religion bias generally lower likely due to global
debiasing efforts demonstrating the need for fairer model development.

</details>


### [113] [The Path of Self-Evolving Large Language Models: Achieving Data-Efficient Learning via Intrinsic Feedback](https://arxiv.org/abs/2510.02752)
*Hangfan Zhang,Siyuan Xu,Zhimeng Guo,Huaisheng Zhu,Shicheng Liu,Xinrun Wang,Qiaosheng Zhang,Yang Chen,Peng Ye,Lei Bai,Shuyue Hu*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Reinforcement learning (RL) has demonstrated potential in enhancing the
reasoning capabilities of large language models (LLMs), but such training
typically demands substantial efforts in creating and annotating data. In this
work, we explore improving LLMs through RL with minimal data. Our approach
alternates between the LLM proposing a task and then attempting to solve it. To
minimize data dependency, we introduce two novel mechanisms grounded in
self-awareness: (1) self-aware difficulty prediction, where the model learns to
assess task difficulty relative to its own abilities and prioritize challenging
yet solvable tasks, and (2) self-aware limit breaking, where the model
recognizes when a task is beyond its capability boundary and proactively
requests external data to break through that limit. Extensive experiments on
nine benchmarks showing a 53.8% relative improvement with less than 1.2% extra
data demonstrate the efficacy of self-aware RL and underscore the promise of
self-evolving agent training.

</details>


### [114] [XTRA: Cross-Lingual Topic Modeling with Topic and Representation Alignments](https://arxiv.org/abs/2510.02788)
*Tien Phat Nguyen,Vu Minh Ngo,Tung Nguyen,Linh Van Ngo,Duc Anh Nguyen,Sang Dinh,Trung Le*

Main category: cs.CL

TL;DR: XTRA是一个创新的跨语言主题建模框架，通过联合词袋模型和多语言嵌入，解决了现有方法在主题一致性和跨语言对齐方面存在的不足。


<details>
  <summary>Details</summary>
Motivation: 现有跨语言主题建模方法在提高主题多样性方面取得了一定的进展，但在确保高主题一致性和跨语言一致性对齐方面仍存在挑战。

Method: XTRA框架整合了词袋模型和多语言嵌入。它包含两个核心组件：1. 表示对齐，通过在共享语义空间中使用对比学习来对齐文档-主题分布；2. 主题对齐，将主题-词分布投影到同一空间以强制跨语言一致性。

Result: XTRA在多语言语料库上的实验结果表明，在主题一致性、多样性和对齐质量方面，XTRA显著优于现有的强大基线方法。

Conclusion: XTRA通过表示对齐和主题对齐的双重机制，能够学习到可解释的（一致且多样）且在跨语言上对齐良好的主题。

Abstract: Cross-lingual topic modeling aims to uncover shared semantic themes across
languages. Several methods have been proposed to address this problem,
leveraging both traditional and neural approaches. While previous methods have
achieved some improvements in topic diversity, they often struggle to ensure
high topic coherence and consistent alignment across languages. We propose XTRA
(Cross-Lingual Topic Modeling with Topic and Representation Alignments), a
novel framework that unifies Bag-of-Words modeling with multilingual
embeddings. XTRA introduces two core components: (1) representation alignment,
aligning document-topic distributions via contrastive learning in a shared
semantic space; and (2) topic alignment, projecting topic-word distributions
into the same space to enforce crosslingual consistency. This dual mechanism
enables XTRA to learn topics that are interpretable (coherent and diverse) and
well-aligned across languages. Experiments on multilingual corpora confirm that
XTRA significantly outperforms strong baselines in topic coherence, diversity,
and alignment quality. Code and reproducible scripts are available at https:
//github.com/tienphat140205/XTRA.

</details>


### [115] [StepChain GraphRAG: Reasoning Over Knowledge Graphs for Multi-Hop Question Answering](https://arxiv.org/abs/2510.02827)
*Tengjun Ni,Xin Yuan,Shenghong Li,Kai Wu,Ren Ping Liu,Wei Ni,Wenjie Zhang*

Main category: cs.CL

TL;DR: StepChain GraphRAG通过问题分解和广度优先搜索（BFS）推理流程增强了多跳问答（QA），在MuSiQue、2WikiMultiHopQA和HotpotQA数据集上达到了最先进的性能，并提高了可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有检索增强生成（RAG）在整合迭代推理步骤与外部知识检索方面存在挑战。

Method: StepChain GraphRAG框架结合了问题分解和一个基于BFS的推理流程。它在语料库上构建全局索引，在推理时将检索到的文本解析成知识图，并将复杂查询分解为子问题。BFS遍历动态地沿着相关边扩展，构建证据链。

Result: StepChain GraphRAG在MuSiQue、2WikiMultiHopQA和HotpotQA上实现了最先进的精确匹配（EM）和F1分数，平均EM提高2.57%，F1提高2.13%，在HotpotQA上提升尤为显著（+4.70% EM，+3.44% F1）。此外，该方法通过保留中间检索步骤的思维链，增强了可解释性。

Conclusion: 未来的工作可以致力于减轻计算开销，并解决大型语言模型潜在的幻觉问题，以提高多跳QA的效率和可靠性。

Abstract: Recent progress in retrieval-augmented generation (RAG) has led to more
accurate and interpretable multi-hop question answering (QA). Yet, challenges
persist in integrating iterative reasoning steps with external knowledge
retrieval. To address this, we introduce StepChain GraphRAG, a framework that
unites question decomposition with a Breadth-First Search (BFS) Reasoning Flow
for enhanced multi-hop QA. Our approach first builds a global index over the
corpus; at inference time, only retrieved passages are parsed on-the-fly into a
knowledge graph, and the complex query is split into sub-questions. For each
sub-question, a BFS-based traversal dynamically expands along relevant edges,
assembling explicit evidence chains without overwhelming the language model
with superfluous context. Experiments on MuSiQue, 2WikiMultiHopQA, and HotpotQA
show that StepChain GraphRAG achieves state-of-the-art Exact Match and F1
scores. StepChain GraphRAG lifts average EM by 2.57% and F1 by 2.13% over the
SOTA method, achieving the largest gain on HotpotQA (+4.70% EM, +3.44% F1).
StepChain GraphRAG also fosters enhanced explainability by preserving the
chain-of-thought across intermediate retrieval steps. We conclude by discussing
how future work can mitigate the computational overhead and address potential
hallucinations from large language models to refine efficiency and reliability
in multi-hop QA.

</details>


### [116] [Evaluating Large Language Models for IUCN Red List Species Information](https://arxiv.org/abs/2510.02830)
*Shinya Uryu*

Main category: cs.CL

TL;DR: LLMs在物种评估中表现出两极分化的能力：在分类方面表现出色，但在评估保护状况方面存在不足，并且存在偏向有魅力的脊椎动物的系统性偏见。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在解决生物多样性危机中的物种评估方面的可靠性。

Method: 系统性地验证了五个领先的LLMs模型在21,955个物种的四个核心IUCN红色名录评估组成部分（分类、保护状况、分布和威胁）上的表现。

Result: LLMs在分类方面表现出色（94.9%），但在保护状况评估方面表现不佳（27.2%），并且存在偏向有魅力的脊椎动物的系统性偏见。

Conclusion: LLMs是强大的信息检索工具，但需要人类监督才能做出基于判断的决定；建议采用混合方法，其中LLMs增强专家能力，而人类专家保留风险评估和政策的唯一权威。

Abstract: Large Language Models (LLMs) are rapidly being adopted in conservation to
address the biodiversity crisis, yet their reliability for species evaluation
is uncertain. This study systematically validates five leading models on 21,955
species across four core IUCN Red List assessment components: taxonomy,
conservation status, distribution, and threats. A critical paradox was
revealed: models excelled at taxonomic classification (94.9%) but consistently
failed at conservation reasoning (27.2% for status assessment). This
knowledge-reasoning gap, evident across all models, suggests inherent
architectural constraints, not just data limitations. Furthermore, models
exhibited systematic biases favoring charismatic vertebrates, potentially
amplifying existing conservation inequities. These findings delineate clear
boundaries for responsible LLM deployment: they are powerful tools for
information retrieval but require human oversight for judgment-based decisions.
A hybrid approach is recommended, where LLMs augment expert capacity while
human experts retain sole authority over risk assessment and policy.

</details>


### [117] [Constraint Satisfaction Approaches to Wordle: Novel Heuristics and Cross-Lexicon Validation](https://arxiv.org/abs/2510.02855)
*Jahidul Arafat,Fariha Tasmin,Sanjaya Poudel,Kamrujjaman,Eftakhar Ahmed Arnob,Ahsan Habib Tareq*

Main category: cs.CL

TL;DR: 本文提出了一种新的约束满足问题（CSP）方法来解决Wordle问题，在猜测次数和运行时间上都优于现有方法，并且在噪声和跨语言测试中表现出鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有Wordle求解器缺乏正式的约束处理，本文旨在提供一种基于CSP的全面公式和新颖的约束感知求解策略。

Method: 本文提出了一种称为CSP感知熵（CSP-Aware Entropy）的方法，该方法在约束传播后计算信息增益，而不是在原始候选集上计算。此外，还提出了概率CSP（Probabilistic CSP）框架，该框架将贝叶斯词频先验与逻辑约束相结合。

Result: CSP感知熵在2,315个英语单词上的平均猜测次数为3.54次，成功率为99.9%，比前向检查（Forward Checking）快46%，猜测时间为12.9毫秒。在10%的噪声下，CSP感知方法仍有5.3个百分点的优势。概率CSP在20%的噪声下仍能100%成功。在500个西班牙语单词上的测试表明，该方法在零语言调整的情况下成功率为88%。

Conclusion: 本文提出的基于CSP的正式处理、约束感知启发式方法、概率-逻辑集成、鲁棒性分析和跨语言验证，为结构化谜题求解领域设定了新的性能基准，证明了原则性的约束满足技术优于经典的、基于信息论和学习的方法。

Abstract: Wordle presents an algorithmically rich testbed for constraint satisfaction
problem (CSP) solving. While existing solvers rely on information-theoretic
entropy maximization or frequency-based heuristics without formal constraint
treatment, we present the first comprehensive CSP formulation of Wordle with
novel constraint-aware solving strategies. We introduce CSP-Aware Entropy,
computing information gain after constraint propagation rather than on raw
candidate sets, and a Probabilistic CSP framework integrating Bayesian
word-frequency priors with logical constraints. Through evaluation on 2,315
English words, CSP-Aware Entropy achieves 3.54 average guesses with 99.9%
success rate, a statistically significant 1.7% improvement over Forward
Checking (t=-4.82, p<0.001, Cohen's d=0.07) with 46% faster runtime (12.9ms
versus 23.7ms per guess). Under 10% noise, CSP-aware approaches maintain 5.3
percentage point advantages (29.0% versus 23.7%, p=0.041), while Probabilistic
CSP achieves 100% success across all noise levels (0-20%) through constraint
recovery mechanisms. Cross-lexicon validation on 500 Spanish words demonstrates
88% success with zero language-specific tuning, validating that core CSP
principles transfer across languages despite an 11.2 percentage point gap from
linguistic differences (p<0.001, Fisher's exact test). Our open-source
implementation with 34 unit tests achieving 91% code coverage provides
reproducible infrastructure for CSP research. The combination of formal CSP
treatment, constraint-aware heuristics, probabilistic-logical integration,
robustness analysis, and cross-lexicon validation establishes new performance
benchmarks demonstrating that principled constraint satisfaction techniques
outperform classical information-theoretic and learning-based approaches for
structured puzzle-solving domains.

</details>


### [118] [Self-Reflective Generation at Test Time](https://arxiv.org/abs/2510.02919)
*Jian Mu,Qixin Zhang,Zhiyong Wang,Menglin Yang,Shuang Qiu,Chengwei Qin,Zhongxiang Dai,Yao Shu*

Main category: cs.CL

TL;DR: SRGen 是一种在推理时进行自我反思的轻量级框架，通过动态熵阈值识别不确定的代币，并利用已生成的内容进行自我纠正，从而提高 LLM 的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的 LLM 自我反思机制要么对完整草稿进行修改，要么通过昂贵的训练来学习自我纠正，这两种方法都具有被动且效率低下的根本性缺陷。因此，需要一种更有效的方法来解决 LLM 自我反思和纠错的问题。

Method: SRGen 在代币生成过程中，利用动态熵阈值识别出不确定的代币。然后，针对每个识别出的代币，训练一个特定的纠正向量，该向量能够充分利用已生成的内容，进行自我反思式生成，以纠正代币的概率分布。通过回顾性分析部分输出，这种自我反思能够做出更值得信赖的决策，从而显著降低在高度不确定点出错的概率。

Result: 在具有挑战性的数学推理基准和各种 LLM 上进行的评估表明，SRGen 能够持续增强模型的推理能力。单次运行质量的提高也转化为更强的自洽性投票。特别是在 AIME2024 基准测试中，使用 DeepSeek-R1-Distill-Qwen-7B 模型时，SRGen 在 Pass@1 上提高了 +12.0%，在 Cons@5 上提高了 +13.3%。

Conclusion: SRGen 是一种即插即用的方法，它将反思融入生成过程，以实现可靠的 LLM 推理。它能够在有限的额外开销下实现一致的性能提升，并具有广泛的可组合性，可以与其他训练时（如 RLHF）和测试时（如 SLOT）技术结合使用。

Abstract: Large language models (LLMs) increasingly solve complex reasoning tasks via
long chain-of-thought, but their forward-only autoregressive generation process
is fragile; early token errors can cascade, which creates a clear need for
self-reflection mechanisms. However, existing self-reflection either performs
revisions over full drafts or learns self-correction via expensive training,
both fundamentally reactive and inefficient. To address this, we propose
Self-Reflective Generation at Test Time (SRGen), a lightweight test-time
framework that reflects before generating at uncertain points. During token
generation, SRGen utilizes dynamic entropy thresholding to identify
high-uncertainty tokens. For each identified token, it trains a specific
corrective vector, which fully exploits the already generated context for a
self-reflective generation to correct the token probability distribution. By
retrospectively analyzing the partial output, this self-reflection enables more
trustworthy decisions, thereby significantly reducing the probability of errors
at highly uncertain points. Evaluated on challenging mathematical reasoning
benchmarks and a diverse set of LLMs, SRGen can consistently strengthen model
reasoning: improvements in single-pass quality also translate into stronger
self-consistency voting. Especially, on AIME2024 with
DeepSeek-R1-Distill-Qwen-7B, SRGen yields absolute improvements of +12.0% on
Pass@1 and +13.3% on Cons@5. Moreover, our findings position SRGen as a
plug-and-play method that integrates reflection into the generation process for
reliable LLM reasoning, achieving consistent gains with bounded overhead and
broad composability with other training-time (e.g., RLHF) and test-time (e.g.,
SLOT) techniques.

</details>


### [119] [Finding Diamonds in Conversation Haystacks: A Benchmark for Conversational Data Retrieval](https://arxiv.org/abs/2510.02938)
*Yohan Lee,Yongwoo Song,Sangyeop Kim*

Main category: cs.CL

TL;DR: 该论文提出了一个名为Conversational Data Retrieval (CDR)的基准测试集，用于评估对话数据检索系统在产品洞察方面的表现。该基准包含1.6k个查询和9.1k个对话，涵盖五个分析任务。评估结果显示，即使是最好的嵌入模型，在NDCG@10上的表现也仅达到0.51，表明对话数据检索与文档检索之间存在显著差距。论文还指出了对话数据检索的独特挑战，并提供了实用的查询模板和详细的错误分析。


<details>
  <summary>Details</summary>
Motivation: 评估用于产品洞察的对话数据检索系统的性能，并揭示现有方法的局限性。

Method: 创建了一个包含1.6k个查询和9.1k个对话的Conversational Data Retrieval (CDR)基准测试集，并评估了16种流行的嵌入模型。

Result: 现有的最佳嵌入模型在CDR基准上的NDCG@10表现仅为0.51，远低于文档检索的水平，表明对话数据检索存在显著挑战。

Conclusion: 对话数据检索面临着独特的挑战，如隐式状态识别、轮次动态和上下文引用，并且与文档检索相比，目前的模型性能仍有很大提升空间。

Abstract: We present the Conversational Data Retrieval (CDR) benchmark, the first
comprehensive test set for evaluating systems that retrieve conversation data
for product insights. With 1.6k queries across five analytical tasks and 9.1k
conversations, our benchmark provides a reliable standard for measuring
conversational data retrieval performance. Our evaluation of 16 popular
embedding models shows that even the best models reach only around NDCG@10 of
0.51, revealing a substantial gap between document and conversational data
retrieval capabilities. Our work identifies unique challenges in conversational
data retrieval (implicit state recognition, turn dynamics, contextual
references) while providing practical query templates and detailed error
analysis across different task categories. The benchmark dataset and code are
available at https://github.com/l-yohai/CDR-Benchmark.

</details>


### [120] [Leave No TRACE: Black-box Detection of Copyrighted Dataset Usage in Large Language Models via Watermarking](https://arxiv.org/abs/2510.02962)
*Jingqi Zhang,Ruibo Chen,Yingqing Yang,Peihua Mai,Heng Huang,Yan Pang*

Main category: cs.CL

TL;DR: TRACE是一个用于检测LLM微调过程中是否使用受版权保护数据集的黑盒框架，通过嵌入无损水印和利用微调的放射效应进行检测。


<details>
  <summary>Details</summary>
Motivation: 现有模型检测方法存在缺陷，需要内部信号或外部参考数据集，限制了实际应用。水印技术可以防止未经授权使用专有或受版权保护的数据，但可能影响文本质量和任务性能。

Method: TRACE通过私钥引导，对数据集进行无损水印重写，保持文本质量和下游效用。在检测时，利用微调对水印数据的放射效应，并引入熵门控程序，选择性地对高不确定性标记进行评分，以增强检测能力。

Result: TRACE在各种数据集和模型家族中，始终能以p<0.05的显著性水平进行检测，并提供强有力的统计证据。此外，它还支持多数据集归属，并且在用大量非水印语料库进行持续预训练后仍然有效。

Conclusion: TRACE提供了一种实用的方法，能够可靠地进行黑盒检测，验证是否在LLM微调过程中使用了受版权保护的数据集。

Abstract: Large Language Models (LLMs) are increasingly fine-tuned on smaller,
domain-specific datasets to improve downstream performance. These datasets
often contain proprietary or copyrighted material, raising the need for
reliable safeguards against unauthorized use. Existing membership inference
attacks (MIAs) and dataset-inference methods typically require access to
internal signals such as logits, while current black-box approaches often rely
on handcrafted prompts or a clean reference dataset for calibration, both of
which limit practical applicability. Watermarking is a promising alternative,
but prior techniques can degrade text quality or reduce task performance. We
propose TRACE, a practical framework for fully black-box detection of
copyrighted dataset usage in LLM fine-tuning. \texttt{TRACE} rewrites datasets
with distortion-free watermarks guided by a private key, ensuring both text
quality and downstream utility. At detection time, we exploit the radioactivity
effect of fine-tuning on watermarked data and introduce an entropy-gated
procedure that selectively scores high-uncertainty tokens, substantially
amplifying detection power. Across diverse datasets and model families, TRACE
consistently achieves significant detections (p<0.05), often with extremely
strong statistical evidence. Furthermore, it supports multi-dataset attribution
and remains robust even after continued pretraining on large non-watermarked
corpora. These results establish TRACE as a practical route to reliable
black-box verification of copyrighted dataset usage. We will make our code
available at: https://github.com/NusIoraPrivacy/TRACE.

</details>


### [121] [Grounding Large Language Models in Clinical Evidence: A Retrieval-Augmented Generation System for Querying UK NICE Clinical Guidelines](https://arxiv.org/abs/2510.02967)
*Matthew Lewis,Samuel Thio,Richard JB Dobson,Spiros Denaxas*

Main category: cs.CL

TL;DR: 该研究开发了一个检索增强生成（RAG）系统，用于查询英国国家卫生与临床优化研究所（NICE）的临床指南，以解决指南冗长难用和医疗系统时间限制的问题。


<details>
  <summary>Details</summary>
Motivation: 解决英国国家卫生与临床优化研究所（NICE）临床指南冗长、信息量大，难以在时间有限的医疗系统中有效利用的问题。

Method: 开发了一个包含混合嵌入机制的检索架构，并使用检索增强生成（RAG）方法来处理和查询NICE临床指南。系统首先检索相关信息，然后利用LLM生成答案。

Result: 检索方面，在10,195个文本块的数据库上，MRR达到0.814，前1个文本块的召回率为81%，前10个文本块的召回率为99.1%。生成方面，RAG增强模型在忠实度上显著提高（+64.7个百分点，达到99.5%），并且所有RAG增强模型的上下文精度得分为1，优于Meditron3-8B LLM（忠实度43%）。

Conclusion: RAG被证明是一种有效、可靠且可扩展的方法，可将生成式AI应用于医疗保健领域，从而实现对医疗指南的成本效益访问。

Abstract: This paper presents the development and evaluation of a Retrieval-Augmented
Generation (RAG) system for querying the United Kingdom's National Institute
for Health and Care Excellence (NICE) clinical guidelines using Large Language
Models (LLMs). The extensive length and volume of these guidelines can impede
their utilisation within a time-constrained healthcare system, a challenge this
project addresses through the creation of a system capable of providing users
with precisely matched information in response to natural language queries. The
system's retrieval architecture, composed of a hybrid embedding mechanism, was
evaluated against a database of 10,195 text chunks derived from three hundred
guidelines. It demonstrates high performance, with a Mean Reciprocal Rank (MRR)
of 0.814, a Recall of 81% at the first chunk and of 99.1% within the top ten
retrieved chunks, when evaluated on 7901 queries.
  The most significant impact of the RAG system was observed during the
generation phase. When evaluated on a manually curated dataset of seventy
question-answer pairs, RAG-enhanced models showed substantial gains in
performance. Faithfulness, the measure of whether an answer is supported by the
source text, was increased by 64.7 percentage points to 99.5% for the
RAG-enhanced O4-Mini model and significantly outperformed the medical-focused
Meditron3-8B LLM, which scored 43%. This, combined with a perfect Context
Precision score of 1 for all RAG-enhanced models, confirms the system's ability
to prevent information fabrication by grounding its answers in relevant source
material. This study thus establishes RAG as an effective, reliable, and
scalable approach for applying generative AI in healthcare, enabling
cost-effective access to medical guidelines.

</details>


### [122] [Semantic Differentiation in Speech Emotion Recognition: Insights from Descriptive and Expressive Speech Roles](https://arxiv.org/abs/2510.03060)
*Rongchen Guo,Vincent Francoeur,Isar Nejadgholi,Sylvain Gagnon,Miodrag Bolic*

Main category: cs.CL

TL;DR: 该研究区分了描述性语义（代表语音的上下文内容）和表现性语义（反映说话者的情绪状态），并表明描述性语义与预期情绪一致，而表现性语义与唤起的情绪相关，以提高语音情感识别的准确性。


<details>
  <summary>Details</summary>
Motivation: 为了提高语音情感识别（SER）的准确性，该研究旨在区分和分析语音中的描述性语义（上下文内容）和表现性语义（说话者的情绪状态）。

Method: 通过录制参与者观看情绪激动电影片段后描述其经历的音频片段，并收集意向情绪标签、参与者自我评级的情绪反应以及效价/唤醒分数来进行实验。

Result: 实验表明，描述性语义与意向情绪相符，而表现性语义与唤起的情绪相关。

Conclusion: 该研究结果为人类-人工智能交互中的SER应用提供了信息，并为更具上下文感知能力的人工智能系统铺平了道路。

Abstract: Speech Emotion Recognition (SER) is essential for improving human-computer
interaction, yet its accuracy remains constrained by the complexity of
emotional nuances in speech. In this study, we distinguish between descriptive
semantics, which represents the contextual content of speech, and expressive
semantics, which reflects the speaker's emotional state. After watching
emotionally charged movie segments, we recorded audio clips of participants
describing their experiences, along with the intended emotion tags for each
clip, participants' self-rated emotional responses, and their valence/arousal
scores. Through experiments, we show that descriptive semantics align with
intended emotions, while expressive semantics correlate with evoked emotions.
Our findings inform SER applications in human-AI interaction and pave the way
for more context-aware AI systems.

</details>


### [123] [Revisiting Direct Speech-to-Text Translation with Speech LLMs: Better Scaling than CoT Prompting?](https://arxiv.org/abs/2510.03093)
*Oriol Pareras,Gerard I. Gállego,Federico Costa,Cristina España-Bonet,Javier Hernando*

Main category: cs.CL

TL;DR: CoT prompting in Speech-to-Text Translation (S2TT) uses transcription then translation steps, outperforming direct prompting due to leveraging ASR and T2TT data. However, direct prompting shows more consistent improvement with increasing S2TT data, suggesting its potential to surpass CoT as S2TT resources grow.


<details>
  <summary>Details</summary>
Motivation: Compare Chain-of-Thought (CoT) and Direct prompting for Speech-to-Text Translation (S2TT) under varying data scales, specifically investigating which prompting strategy is more effective as S2TT datasets become larger.

Method: Pseudo-label an Automatic Speech Recognition (ASR) corpus by translating its transcriptions into six European languages. Train Large Language Model (LLM)-based S2TT systems using both CoT and Direct prompting strategies across different data scales to compare their performance.

Result: Direct prompting demonstrates more consistent performance improvements as the amount of available S2TT data increases, compared to CoT prompting.

Conclusion: As larger Speech-to-Text Translation (S2TT) resources become available, Direct prompting is likely to become a more effective approach than Chain-of-Thought (CoT) prompting due to its consistent scalability with data.

Abstract: Recent work on Speech-to-Text Translation (S2TT) has focused on LLM-based
models, introducing the increasingly adopted Chain-of-Thought (CoT) prompting,
where the model is guided to first transcribe the speech and then translate it.
CoT typically outperforms direct prompting primarily because it can exploit
abundant Automatic Speech Recognition (ASR) and Text-to-Text Translation (T2TT)
datasets to explicitly model its steps. In this paper, we systematically
compare CoT and Direct prompting under increasing amounts of S2TT data. To this
end, we pseudo-label an ASR corpus by translating its transcriptions into six
European languages, and train LLM-based S2TT systems with both prompting
strategies at different data scales. Our results show that Direct improves more
consistently as the amount of data increases, suggesting that it may become a
more effective approach as larger S2TT resources are created.

</details>


### [124] [Semantic Similarity in Radiology Reports via LLMs and NER](https://arxiv.org/abs/2510.03102)
*Beth Pearson,Ahmed Adnan,Zahraa Abdallah*

Main category: cs.CL

TL;DR: 使用结合了Llama 3.1和命名实体识别（NER）的Llama-EntScore方法，在放射报告的语义相似性评估方面取得了优于独立LLM和NER方法的准确性。


<details>
  <summary>Details</summary>
Motivation: 为了改进放射科医生培训，需要识别初级放射医生报告与最终报告之间的语义差异，以提供有价值的反馈和促进学习。

Method: 将几种大型语言模型（LLM）和基于命名实体识别（NER）的传统方法应用于放射报告的比较。在此基础上，提出了一种名为Llama-EntScore的新方法，该方法结合了Llama 3.1和NER，并允许调整权重以量化和解释报告之间的语义相似性。

Result: Llama-EntScore方法在与放射科医生提供真实分数进行比较时，达到了67%的精确匹配准确率和93%的+/- 1范围内准确率，优于单独使用的LLM和NER方法。

Conclusion: Llama-EntScore提供了一种有效的方法来量化和解释放射报告之间的语义相似性，为初级放射医生提供了有价值的培训工具，并有助于提高诊断准确性。

Abstract: Radiology report evaluation is a crucial part of radiologists' training and
plays a key role in ensuring diagnostic accuracy. As part of the standard
reporting workflow, a junior radiologist typically prepares a preliminary
report, which is then reviewed and edited by a senior radiologist to produce
the final report. Identifying semantic differences between preliminary and
final reports is essential for junior doctors, both as a training tool and to
help uncover gaps in clinical knowledge. While AI in radiology is a rapidly
growing field, the application of large language models (LLMs) remains
challenging due to the need for specialised domain knowledge. In this paper, we
explore the ability of LLMs to provide explainable and accurate comparisons of
reports in the radiology domain. We begin by comparing the performance of
several LLMs in comparing radiology reports. We then assess a more traditional
approach based on Named-Entity-Recognition (NER). However, both approaches
exhibit limitations in delivering accurate feedback on semantic similarity. To
address this, we propose Llama-EntScore, a semantic similarity scoring method
using a combination of Llama 3.1 and NER with tunable weights to emphasise or
de-emphasise specific types of differences. Our approach generates a
quantitative similarity score for tracking progress and also gives an
interpretation of the score that aims to offer valuable guidance in reviewing
and refining their reporting. We find our method achieves 67% exact-match
accuracy and 93% accuracy within +/- 1 when compared to radiologist-provided
ground truth scores - outperforming both LLMs and NER used independently. Code
is available at:
\href{https://github.com/otmive/llama_reports}{github.com/otmive/llama\_reports}

</details>


### [125] [Listening or Reading? Evaluating Speech Awareness in Chain-of-Thought Speech-to-Text Translation](https://arxiv.org/abs/2510.03115)
*Jacobo Romero-Díaz,Gerard I. Gállego,Oriol Pareras,Federico Costa,Javier Hernando,Cristina España-Bonet*

Main category: cs.CL

TL;DR: 目前主流的Speech-to-Text Translation (S2TT)系统，由于将语音识别(ASR)和文本到文本翻译(T2TT)分开处理，存在错误传播和无法利用语音线索的缺点。虽然引入了Chain-of-Thought (CoT)提示，期望能同时利用语音和文本信息，但分析表明，CoT在很大程度上仍然依赖文本，对语音信息的利用很少。然而，通过添加直接S2TT数据或噪声文本等简单的训练方法，可以提高系统的鲁棒性并增加对语音信息的利用。这些发现对CoT的预期优势提出了质疑，并强调需要更明确地将声学信息整合到翻译模型中的新架构。


<details>
  <summary>Details</summary>
Motivation: 现有的语音到文本翻译（S2TT）系统，通常由自动语音识别（ASR）和文本到文本翻译（T2TT）两个模块级联而成，存在两个主要局限性：1) ASR模块的错误会传递到T2TT模块，影响翻译的准确性；2) 无法利用语音本身包含的韵律或其他声学线索信息。Chain-of-Thought (CoT)提示被引入，旨在通过同时处理语音和文本来克服这些问题，但需要对其进行分析以验证其有效性。

Method: 通过属性方法分析CoT，使用损坏的文本评估其鲁棒性，并评估其对韵律的感知能力。此外，还尝试了添加直接S2TT数据或噪声文本注入等训练方法，以改进模型。

Result: 分析结果显示，CoT在很大程度上复制了级联系统的行为，主要依赖于输入的文本信息，而对语音信息的利用非常有限。然而，在引入简单的训练干预（如添加直接S2TT数据或噪声文本）后，模型的鲁棒性得到增强，并且对语音信息的利用比例有所增加。

Conclusion: Chain-of-Thought (CoT)方法在克服现有S2TT系统的局限性方面并未达到预期效果，其对语音信息的利用仍然很少，并且在很大程度上依赖于文本输入。然而，通过特定的训练策略可以提高其性能和语音信息的利用率。未来的研究应着重于开发能够显式整合声学信息的翻译架构，而不是仅仅依赖于提示工程。

Abstract: Speech-to-Text Translation (S2TT) systems built from Automatic Speech
Recognition (ASR) and Text-to-Text Translation (T2TT) modules face two major
limitations: error propagation and the inability to exploit prosodic or other
acoustic cues. Chain-of-Thought (CoT) prompting has recently been introduced,
with the expectation that jointly accessing speech and transcription will
overcome these issues. Analyzing CoT through attribution methods, robustness
evaluations with corrupted transcripts, and prosody-awareness, we find that it
largely mirrors cascaded behavior, relying mainly on transcripts while barely
leveraging speech. Simple training interventions, such as adding Direct S2TT
data or noisy transcript injection, enhance robustness and increase speech
attribution. These findings challenge the assumed advantages of CoT and
highlight the need for architectures that explicitly integrate acoustic
information into translation.

</details>


### [126] [SurveyBench: How Well Can LLM(-Agents) Write Academic Surveys?](https://arxiv.org/abs/2510.03120)
*Zhaojun Sun,Xuzhou Zhu,Xuanhe Zhou,Xin Tong,Shuo Wang,Jie Fu,Guoliang Li,Zhiyuan Liu,Fan Wu*

Main category: cs.CL

TL;DR: 提出SurveyBench，一个针对LLM4Survey的评测框架，以解决现有方法不足和缺乏评估基准的问题。


<details>
  <summary>Details</summary>
Motivation: 现有自动文献综述方法（LLM4Survey）生成的综述质量不高，且缺乏严格的、以读者为中心的评估基准来暴露其不足。

Method: 提出SurveyBench，一个包含（1）典型综述主题来源、(2) 多方面评估指标（大纲质量、内容质量、非文本丰富度）和 (3) 双模式评估协议（基于内容和基于测验）的评测框架。

Result: SurveyBench能有效挑战现有LLM4Survey方法，在基于内容的评估中，其表现平均比人类低21%。

Conclusion: SurveyBench是一个有效的评估框架，能够揭示现有LLM4Survey方法的局限性。

Abstract: Academic survey writing, which distills vast literature into a coherent and
insightful narrative, remains a labor-intensive and intellectually demanding
task. While recent approaches, such as general DeepResearch agents and
survey-specialized methods, can generate surveys automatically (a.k.a.
LLM4Survey), their outputs often fall short of human standards and there lacks
a rigorous, reader-aligned benchmark for thoroughly revealing their
deficiencies. To fill the gap, we propose a fine-grained, quiz-driven
evaluation framework SurveyBench, featuring (1) typical survey topics source
from recent 11,343 arXiv papers and corresponding 4,947 high-quality surveys;
(2) a multifaceted metric hierarchy that assesses the outline quality (e.g.,
coverage breadth, logical coherence), content quality (e.g., synthesis
granularity, clarity of insights), and non-textual richness; and (3) a
dual-mode evaluation protocol that includes content-based and quiz-based
answerability tests, explicitly aligned with readers' informational needs.
Results show SurveyBench effectively challenges existing LLM4Survey approaches
(e.g., on average 21% lower than human in content-based evaluation).

</details>


### [127] [Beyond the Final Layer: Intermediate Representations for Better Multilingual Calibration in Large Language Models](https://arxiv.org/abs/2510.03136)
*Ej Zhou,Caiqi Zhang,Tiancheng Hu,Chengzu Li,Nigel Collier,Ivan Vulić,Anna Korhonen*

Main category: cs.CL

TL;DR: 大型语言模型在多语言场景下面临不准确的置信度问题，该研究通过分析模型内部表示，提出一种新的方法来提升多语言模型的置信度校准能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型(LLM)在多语言场景下的置信度校准问题尚未得到充分研究，这对其可靠部署至关重要。

Method: 对六个模型家族和一百多种语言进行大规模、系统性的多语言校准研究，分析模型内部表示，并提出一种名为LACE（Language-Aware Confidence Ensemble）的无需训练的方法，该方法能为特定语言自适应地选择最优的层集合进行集成。

Result: 研究发现，非英语语言的模型校准效果普遍较差，模型最后一层由于以英语为中心的训练而存在偏差，导致其提供的置信度信号不可靠。相比之下，模型中后期的中间层提供了更可靠、校准效果更好的信号。LACE方法能够提升多语言模型的置信度校准能力。

Conclusion: 现有的以英语为中心的模型训练方式隐藏了巨大的成本，而通过超越对模型最后一层的关注，探索中间层，为构建更公平、更可信的全球化大型语言模型提供了新的途径。

Abstract: Confidence calibration, the alignment of a model's predicted confidence with
its actual accuracy, is crucial for the reliable deployment of Large Language
Models (LLMs). However, this critical property remains largely under-explored
in multilingual contexts. In this work, we conduct the first large-scale,
systematic studies of multilingual calibration across six model families and
over 100 languages, revealing that non-English languages suffer from
systematically worse calibration. To diagnose this, we investigate the model's
internal representations and find that the final layer, biased by
English-centric training, provides a poor signal for multilingual confidence.
In contrast, our layer-wise analysis uncovers a key insight that
late-intermediate layers consistently offer a more reliable and
better-calibrated signal. Building on this, we introduce a suite of
training-free methods, including Language-Aware Confidence Ensemble (LACE),
which adaptively selects an optimal ensemble of layers for each specific
language. Our study highlights the hidden costs of English-centric alignment
and offer a new path toward building more globally equitable and trustworthy
LLMs by looking beyond the final layer.

</details>


### [128] [EditLens: Quantifying the Extent of AI Editing in Text](https://arxiv.org/abs/2510.03154)
*Katherine Thai,Bradley Emi,Elyas Masrour,Mohit Iyyer*

Main category: cs.CL

TL;DR: AI编辑过的文本可以被识别，并且可以量化AI编辑的程度。


<details>
  <summary>Details</summary>
Motivation: 检测AI编辑过的文本，而不是只检测完全由AI生成的文本。

Method: 使用相似度指标量化AI编辑的程度，并训练了一个名为EditLens的回归模型来预测文本中AI编辑的量。

Result: EditLens模型在区分人类、AI和混合写作的二元（F1=94.7%）和三元（F1=90.4%）分类任务上取得了最先进的性能。

Conclusion: AI编辑过的文本可以被检测出来，并且可以检测出AI对人类写作的修改程度，这在作者归属、教育和政策方面有重要意义。

Abstract: A significant proportion of queries to large language models ask them to edit
user-provided text, rather than generate new text from scratch. While previous
work focuses on detecting fully AI-generated text, we demonstrate that
AI-edited text is distinguishable from human-written and AI-generated text.
First, we propose using lightweight similarity metrics to quantify the
magnitude of AI editing present in a text given the original human-written text
and validate these metrics with human annotators. Using these similarity
metrics as intermediate supervision, we then train EditLens, a regression model
that predicts the amount of AI editing present within a text. Our model
achieves state-of-the-art performance on both binary (F1=94.7%) and ternary
(F1=90.4%) classification tasks in distinguishing human, AI, and mixed writing.
Not only do we show that AI-edited text can be detected, but also that the
degree of change made by AI to human writing can be detected, which has
implications for authorship attribution, education, and policy. Finally, as a
case study, we use our model to analyze the effects of AI-edits applied by
Grammarly, a popular writing assistance tool. To encourage further research, we
commit to publicly releasing our models and dataset.

</details>


### [129] [Neural Correlates of Language Models Are Specific to Human Language](https://arxiv.org/abs/2510.03156)
*Iñigo Parra*

Main category: cs.CL

TL;DR: 之前的研究表明，大型语言模型的隐藏状态与语言任务中的fMRI大脑反应之间存在相关性，但这可能受到维度诅咒、相似性度量、模型训练数据和位置编码等因素的影响。本研究通过降维、使用新的相似性度量、对比不同训练数据的模型以及分析位置编码的作用，验证了这些结果的稳健性。研究确认并加强了先前研究的结论，即模型和大脑在语言处理上的表征具有相似性，并为大型语言模型的生物学合理性和可解释性辩论提供了更有力的证据。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在验证先前研究中发现的大型语言模型（LLM）隐藏状态与fMRI大脑反应之间相关性的稳健性，并探讨这些相关性是否受到维度诅咒、相似性度量选择、模型训练数据以及位置编码等因素的影响。

Method: 研究采用了多种方法来检验先前结果的稳健性：1. 进行了降维处理，以排除“维度诅咒”对结果的影响。2. 采用了新的相似性度量方法，以确认结果不依赖于特定的度量方式。3. 对比了使用人类语言训练的模型与其他模型，以确定结果是否仅限于语言相关的模型。4. 分析了位置编码在模型中的作用，以评估其对结果的影响。

Result: 研究结果表明：1. 即使在进行降维处理后，LLM隐藏状态与fMRI大脑反应之间的相关性依然存在，排除了“维度诅咒”的影响。2. 使用新的相似性度量方法，先前观察到的相关性仍然成立。3. 这种相关性仅在使用了人类语言训练的模型中出现，表明其与语言处理的特异性相关。4. 相关性的存在依赖于模型中是否包含位置编码。

Conclusion: 本研究证实并加强了先前关于大型语言模型表征与大脑语言处理表征之间相似性的发现。研究结果表明，这些模型在处理语言时，其内部表征在一定程度上与人脑的神经活动相似，并且这种相似性受到模型架构（如位置编码）和训练数据（人类语言）的影响。这为理解大型语言模型的生物学合理性及可解释性提供了更坚实的证据。

Abstract: Previous work has shown correlations between the hidden states of large
language models and fMRI brain responses, on language tasks. These correlations
have been taken as evidence of the representational similarity of these models
and brain states. This study tests whether these previous results are robust to
several possible concerns. Specifically this study shows: (i) that the previous
results are still found after dimensionality reduction, and thus are not
attributable to the curse of dimensionality; (ii) that previous results are
confirmed when using new measures of similarity; (iii) that correlations
between brain representations and those from models are specific to models
trained on human language; and (iv) that the results are dependent on the
presence of positional encoding in the models. These results confirm and
strengthen the results of previous research and contribute to the debate on the
biological plausibility and interpretability of state-of-the-art large language
models.

</details>


### [130] [Topic Modeling as Long-Form Generation: Can Long-Context LLMs revolutionize NTM via Zero-Shot Prompting?](https://arxiv.org/abs/2510.03174)
*Xuan Xu,Haolun Li,Zhongliang Yang,Beilin Chu,Jia Song,Moxuan Xu,Linna Zhou*

Main category: cs.CL

TL;DR: 该研究提出将主题建模（TM）视为一个长篇生成任务，并提出一种使用大型语言模型（LLM）实现此任务的实用方法，旨在通过零样本提示来改进主题质量，并与传统神经主题模型（NTM）进行比较。


<details>
  <summary>Details</summary>
Motivation: 探索在大型语言模型时代，主题建模的新范式，并将主题建模视为一个长篇生成任务。

Method: 提出一种使用LLM实现主题建模的实用方法，包括数据子集采样、使用提示生成主题和代表性文本，以及通过关键词匹配进行文本分配。并探索了零样本提示在长篇生成范式中超越NTM的可能性。

Result: 系统地比较了NTM和LLM在主题质量方面的表现，并检验了“大多数NTM已过时”的说法。

Conclusion: 评估LLM作为主题建模工具的有效性，并将其与传统NTM进行对比。

Abstract: Traditional topic models such as neural topic models rely on inference and
generation networks to learn latent topic distributions. This paper explores a
new paradigm for topic modeling in the era of large language models, framing TM
as a long-form generation task whose definition is updated in this paradigm. We
propose a simple but practical approach to implement LLM-based topic model
tasks out of the box (sample a data subset, generate topics and representative
text with our prompt, text assignment with keyword match). We then investigate
whether the long-form generation paradigm can beat NTMs via zero-shot
prompting. We conduct a systematic comparison between NTMs and LLMs in terms of
topic quality and empirically examine the claim that "a majority of NTMs are
outdated."

</details>


### [131] [Model-Based Ranking of Source Languages for Zero-Shot Cross-Lingual Transfer](https://arxiv.org/abs/2510.03202)
*Abteen Ebrahimi,Adam Wiemerslage,Katharina von der Wense*

Main category: cs.CL

TL;DR: NN-Rank是一个利用多语言模型隐藏表示和无标签目标语言数据的跨语言迁移源语言排序算法，在POS和NER任务上优于现有方法，即使在仅使用《圣经》这一领域外语料库的情况下也具有竞争力，并且仅需少量无标签目标数据即可获得高质量排序。


<details>
  <summary>Details</summary>
Motivation: 寻找一种有效的方法来对跨语言迁移的源语言进行排序，以提高下游任务的性能，特别是在目标语言数据有限的情况下。

Method: 提出了一种名为NN-Rank的算法，该算法利用多语言模型的隐藏表示和无标签的目标语言数据来对源语言进行排序。实验采用了两种预训练的多语言模型，并针对词性标注（POS）和命名实体识别（NER）两个任务进行了评估，涉及51个源语言和56至72个目标语言。

Result: NN-Rank在有领域内数据时，平均NDCG得分在POS任务上提高了35.56%，在NER任务上提高了18.14%，优于利用词汇和语言特征的最先进基线。即使在仅使用《圣经》（一种领域外语料库）的情况下，NN-Rank仍然具有竞争力。此外，研究表明，即使只有25个样本的无标签目标数据子集，NN-Rank也能产生高质量的排序，达到使用所有可用目标数据进行排序所获得NDCG的92.8%。

Conclusion: NN-Rank是一种有效的跨语言迁移源语言排序算法，它利用多语言模型的隐藏表示和少量无标签目标数据，在各种条件下都能取得优于现有方法的性能，并且对目标语言数据的量具有鲁棒性。

Abstract: We present NN-Rank, an algorithm for ranking source languages for
cross-lingual transfer, which leverages hidden representations from
multilingual models and unlabeled target-language data. We experiment with two
pretrained multilingual models and two tasks: part-of-speech tagging (POS) and
named entity recognition (NER). We consider 51 source languages and evaluate on
56 and 72 target languages for POS and NER, respectively. When using in-domain
data, NN-Rank beats state-of-the-art baselines that leverage lexical and
linguistic features, with average improvements of up to 35.56 NDCG for POS and
18.14 NDCG for NER. As prior approaches can fall back to language-level
features if target language data is not available, we show that NN-Rank remains
competitive using only the Bible, an out-of-domain corpus available for a large
number of languages. Ablations on the amount of unlabeled target data show
that, for subsets consisting of as few as 25 examples, NN-Rank produces
high-quality rankings which achieve 92.8% of the NDCG achieved using all
available target data for ranking.

</details>


### [132] [FocusAgent: Simple Yet Effective Ways of Trimming the Large Context of Web Agents](https://arxiv.org/abs/2510.03204)
*Imene Kerboua,Sahar Omidi Shayegan,Megh Thakkar,Xing Han Lù,Léo Boisvert,Massimo Caccia,Jérémy Espinas,Alexandre Aussem,Véronique Eglin,Alexandre Lacoste*

Main category: cs.CL

TL;DR: FocusAgent通过利用轻量级LLM检索器从可访问性树（AxTree）观察中提取最相关的行，从而提高了Web智能体的效率和安全性。


<details>
  <summary>Details</summary>
Motivation: 现有的Web智能体需要处理过长的网页内容，这会超出上下文限制，增加计算成本，并带来安全风险。现有的剪枝策略效果不佳。

Method: FocusAgent使用轻量级LLM检索器，根据任务目标从可访问性树（AxTree）中提取最相关的行，以剔除不相关和冗余的内容。

Result: FocusAgent在WorkArena和WebArena基准测试中，将观察结果的大小减少了50%以上，同时保持了与强基线相当的性能。其变体在有效防御提示注入攻击的同时，保持了任务成功率。

Conclusion: 针对性的基于LLM的检索是一种实用且鲁棒的策略，可用于构建高效、有效且安全的Web智能体。

Abstract: Web agents powered by large language models (LLMs) must process lengthy web
page observations to complete user goals; these pages often exceed tens of
thousands of tokens. This saturates context limits and increases computational
cost processing; moreover, processing full pages exposes agents to security
risks such as prompt injection. Existing pruning strategies either discard
relevant content or retain irrelevant context, leading to suboptimal action
prediction. We introduce FocusAgent, a simple yet effective approach that
leverages a lightweight LLM retriever to extract the most relevant lines from
accessibility tree (AxTree) observations, guided by task goals. By pruning
noisy and irrelevant content, FocusAgent enables efficient reasoning while
reducing vulnerability to injection attacks. Experiments on WorkArena and
WebArena benchmarks show that FocusAgent matches the performance of strong
baselines, while reducing observation size by over 50%. Furthermore, a variant
of FocusAgent significantly reduces the success rate of prompt-injection
attacks, including banner and pop-up attacks, while maintaining task success
performance in attack-free settings. Our results highlight that targeted
LLM-based retrieval is a practical and robust strategy for building web agents
that are efficient, effective, and secure.

</details>


### [133] [Cache-to-Cache: Direct Semantic Communication Between Large Language Models](https://arxiv.org/abs/2510.03215)
*Tianyu Fu,Zihan Min,Hanling Zhang,Jichao Yan,Guohao Dai,Wanli Ouyang,Yu Wang*

Main category: cs.CL

TL;DR: 通过直接在LLM之间共享KV缓存来改进多LLM系统的性能和效率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM系统通过文本进行通信，这种方式会丢失语义信息并增加延迟。

Method: 提出了一种名为Cache-to-Cache（C2C）的新范式，该范式使用神经网络来映射和融合源模型和目标模型的KV缓存，以实现直接的语义通信，并使用可学习的门控机制来选择受益于缓存通信的目标层。

Result: 与单独的模型相比，C2C的平均准确率提高了8.5-10.5%。与基于文本的通信范式相比，C2C的准确率提高了3.0-5.0%，同时延迟减少了2.0倍。

Conclusion: C2C是一种有效的跨模型通信范式，可以利用LLM的深度语义信息，提高性能并降低延迟。

Abstract: Multi-LLM systems harness the complementary strengths of diverse Large
Language Models, achieving performance and efficiency gains unattainable by a
single model. In existing designs, LLMs communicate through text, forcing
internal representations to be transformed into output token sequences. This
process both loses rich semantic information and incurs token-by-token
generation latency. Motivated by these limitations, we ask: Can LLMs
communicate beyond text? Oracle experiments show that enriching the KV-Cache
semantics can improve response quality without increasing cache size,
supporting KV-Cache as an effective medium for inter-model communication. Thus,
we propose Cache-to-Cache (C2C), a new paradigm for direct semantic
communication between LLMs. C2C uses a neural network to project and fuse the
source model's KV-cache with that of the target model to enable direct semantic
transfer. A learnable gating mechanism selects the target layers that benefit
from cache communication. Compared with text communication, C2C utilizes the
deep, specialized semantics from both models, while avoiding explicit
intermediate text generation. Experiments show that C2C achieves 8.5-10.5%
higher average accuracy than individual models. It further outperforms the text
communication paradigm by approximately 3.0-5.0%, while delivering an average
2.0x speedup in latency. Our code is available at
https://github.com/thu-nics/C2C.

</details>


### [134] [Self-Anchor: Large Language Model Reasoning via Step-by-step Attention Alignment](https://arxiv.org/abs/2510.03223)
*Hongxiang Zhang,Yuan Tian,Tianyi Zhang*

Main category: cs.CL

TL;DR: Self-Anchor通过结构化推理过程引导LLM注意力，在复杂推理任务中超越了现有方法，并缩小了普通模型与专业推理模型的性能差距。


<details>
  <summary>Details</summary>
Motivation: 现有的基于提示的方法在处理长推理链时，关键的中间步骤和原始提示容易被忽略，导致错误。Self-Anchor旨在解决这个问题。

Method: Self-Anchor将推理过程分解为结构化计划，并自动将模型的注意力引导到最相关的推理步骤，以保持模型在生成过程中的专注度。

Result: 实验表明，Self-Anchor在六个基准测试中优于现有的先进提示方法，并显著缩小了普通模型和专业推理模型之间的性能差距。

Conclusion: Self-Anchor有潜力使大多数LLM在无需重新训练的情况下处理复杂的推理任务。

Abstract: To solve complex reasoning tasks for Large Language Models (LLMs),
prompting-based methods offer a lightweight alternative to fine-tuning and
reinforcement learning. However, as reasoning chains extend, critical
intermediate steps and the original prompt will be buried in the context,
receiving insufficient attention and leading to errors. In this paper, we
propose Self-Anchor, a novel pipeline that leverages the inherent structure of
reasoning to steer LLM attention. Self-Anchor decomposes reasoning trajectories
into structured plans and automatically aligns the model's attention to the
most relevant inference steps, allowing the model to maintain focus throughout
generation. Our experiment shows that Self-Anchor outperforms SOTA prompting
methods across six benchmarks. Notably, Self-Anchor significantly reduces the
performance gap between ``non-reasoning'' models and specialized reasoning
models, with the potential to enable most LLMs to tackle complex reasoning
tasks without retraining.

</details>


### [135] [Reward Models are Metrics in a Trench Coat](https://arxiv.org/abs/2510.03231)
*Sebastian Gehrmann*

Main category: cs.CL

TL;DR: 评估指标和奖励模型的研究领域存在重叠，但联系较少。通过加强合作，可以解决数据质量差、奖励模型被利用等问题，并改进两个领域。


<details>
  <summary>Details</summary>
Motivation: 奖励模型和评估指标在评估AI模型输出质量方面发挥着相似的作用，但这两个研究领域却鲜有交集，导致术语重复和研究方法上的反复。这篇论文旨在弥合这两个领域之间的差距。

Method: 通过对奖励模型和评估指标两个领域进行广泛的调研，分析它们在数据质量、奖励模型被利用、元评估等方面的共同挑战，并探讨了两个领域加强合作的可能性。论文还通过具体实例展示了评估指标在某些任务上的优越性。

Result: 通过调研，我们发现评估指标在某些任务上表现优于奖励模型。此外，我们还明确了两个领域在偏好选择方法、避免虚假关联和奖励模型被利用、以及校准感知元评估等方面的合作前景。

Conclusion: 奖励模型和评估指标的研究领域应加强合作，以解决共同面临的挑战，并推动两个领域的技术进步。

Abstract: The emergence of reinforcement learning in post-training of large language
models has sparked significant interest in reward models. Reward models assess
the quality of sampled model outputs to generate training signals. This task is
also performed by evaluation metrics that monitor the performance of an AI
model. We find that the two research areas are mostly separate, leading to
redundant terminology and repeated pitfalls. Common challenges include
susceptibility to spurious correlations, impact on downstream reward hacking,
methods to improve data quality, and approaches to meta-evaluation. Our
position paper argues that a closer collaboration between the fields can help
overcome these issues. To that end, we show how metrics outperform reward
models on specific tasks and provide an extensive survey of the two areas.
Grounded in this survey, we point to multiple research topics in which closer
alignment can improve reward models and metrics in areas such as preference
elicitation methods, avoidance of spurious correlations and reward hacking, and
calibration-aware meta-evaluation.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [136] [Simple Quantum Algorithm for Approximate $k$-Mismatch Problem](https://arxiv.org/abs/2510.02399)
*Ruhan Habib*

Main category: quant-ph

TL;DR: 该论文提出了一个量子算法来近似解决k-不匹配问题，其时间复杂度为 O(ε⁻¹ * sqrt(mn/k))。


<details>
  <summary>Details</summary>
Motivation: 解决经典的k-不匹配问题，并将其扩展到量子计算领域，由Jin和Nogler以及Kociumaka、Nogler和Wellnitz近期研究过。

Method: 提出一个简单的量子算法，该算法以近似的方式解决问题。

Result: 该算法能够以 O(ε⁻¹ * sqrt(mn/k)) 的时间（大小）复杂度找到一个匹配，但匹配的定义是(1+ε)k-不匹配。如果找不到匹配，则保证不存在k-不匹配。

Conclusion: 该论文提供了一个解决k-不匹配问题的近似量子算法。

Abstract: In the $k$-mismatch problem, given a pattern and a text of length $n$ and $m$
respectively, we have to find if the text has a sub-string with a Hamming
distance of at most $k$ from the pattern. This has been studied in the
classical setting since 1982 and recently in the quantum computational setting
by Jin and Nogler and Kociumaka, Nogler, and Wellnitz. We provide a simple
quantum algorithm that solves the problem in an approximate manner, given a
parameter $\epsilon \in (0, 1]$. It returns an occurrence as a match only if it
is a $\left(1+\epsilon\right)k$-mismatch. If it does not return any occurrence,
then there is no $k$-mismatch. This algorithm has a time (size) complexity of
$\tilde{O}\left( \epsilon^{-1} \sqrt{\frac{mn}{k}} \right)$.

</details>


### [137] [Probability distributions over CSS codes: two-universality, QKD hashing, collision bounds, security](https://arxiv.org/abs/2510.02402)
*Pete Rigas*

Main category: quant-ph

TL;DR: 量子密钥分发（QKD）中的CSS码和双通用哈希协议的安全性分析。


<details>
  <summary>Details</summary>
Motivation: 分析量子纠错码（CSS码）的新概率分布，并研究其在量子密钥分发（QKD）和双通用哈希协议中的应用，以提高量子计算的保真度。

Method: 提出新的实、模拟和理想的等距映射，以有效地计算校验矩阵函数，并分析其与概率测量的边际的关系，以及其与纯化随机矩阵的关系。

Result: 证明了双通用QKD哈希协议的安全性依赖于纯化随机矩阵的计算，并给出了安全性降低的量化表达式。

Conclusion: 新的等距映射使得能够高效地计算校验矩阵函数，从而能够分析双通用哈希协议的安全性，并量化了其安全性降低的程度。

Abstract: We characterize novel probability distributions for CSS codes. Such classes
of error correcting codes, originally introduced by Calderbank, Shor, and
Steane, are of great significance in advancing the fidelity of Quantum
computation, with implications for future near term applications. Within the
context of Quantum key distribution, such codes, as examined by Ostrev in
arXiv: 2109.06709 along with two-universal hashing protocols, have greatly
simplified Quantum phases of computation for unconditional security. To further
examine novel applications of two-universal hashing protocols, particularly
through the structure of parity check matrices, we demonstrate how being able
to efficiently compute functions of the parity check matrices relates to
marginals of a suitably defined probability measure supported over random
matrices. The security of the two-universal QKD hashing protocol will be shown
to depend upon the computation of purified states of random matrices, which
relates to probabilistic collision bounds between two hashing functions.
Central to our approach are the introduction of novel real, simulator, and
ideal, isometries, hence allowing for efficient computations of functions of
the two parity check matrices. As a result of being able to perform such
computations involving parity check matrices, the security of the two-universal
hashing protocol is a factor of $2^{ \frac{5}{2} ( 5 - \frac{3}{2} ) +
\mathrm{log}_2 \sqrt{C}}$ less secure, for some strictly positive constant $C$.

</details>


### [138] [Comment on Marek Czachor article entitled "On Relativity of Quantumness as Implied by Relativity of Arithmetic and Probability"](https://arxiv.org/abs/2510.02412)
*Krzysztof Sienicki,Mikołaj Sienicki*

Main category: quant-ph

TL;DR: Czachor's model imports quantum mechanics' Born rule and Fubini-Study metric into a non-Newtonian hidden-variable setting. This hybrid framework's agreement with quantum correlations is by design, not new physics, and thus doesn't challenge Bell's theorem.


<details>
  <summary>Details</summary>
Motivation: The paper analyzes Czachor's model of hierarchical arithmetics, which uses a valid formal premise but imports specific elements from quantum mechanics.

Method: The model imports the Born rule and Fubini-Study metric from quantum mechanics to define the probability mapping 'g'. This 'g' is then applied in a non-Newtonian hidden-variable setting, creating a hybrid framework.

Result: The resulting hybrid framework's agreement with quantum correlations is inherent due to the import of quantum mechanical principles, rather than being derived from novel physical insights.

Conclusion: Czachor's model, being built upon pre-existing quantum mechanical rules, does not offer a genuine counterexample to Bell's theorem as its agreement with quantum correlations is by design.

Abstract: Czachor's model of hierarchical arithmetics begins with a valid formal
premise but fixes the key probability mapping g by importing the Born rule and
Fubini-Study metric from standard quantum mechanics, where Born probabilities
are Kolmogorov within a fixed measurement context. This g is then applied in a
non-Newtonian hidden-variable setting, producing a hybrid framework whose
agreement with quantum correlations is built in by design, not derived from new
physics, and thus does not constitute a genuine counterexample to Bell's
theorem

</details>


### [139] [Utility-Scale Quantum State Preparation: Classical Training using Pauli Path Simulation](https://arxiv.org/abs/2510.02428)
*Cheng-Ju Lin,Hrant Gharibyan,Vincent P. Su*

Main category: quant-ph

TL;DR: 使用 Pauli Path 模拟技术优化参数化电路，以制备大规模量子多体哈密顿量的基态，并在量子计算机上验证了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 使用 Pauli Path 模拟技术来变分获得参数化电路，用于制备各种量子多体哈密顿量的基态，特别是对于经典计算机难以处理的大规模系统。

Method: 采用 Pauli Path 模拟技术，对量子伊辛模型（一维、二维方形和重六角晶格）和 Kitaev 蜂巢模型进行研究，系统规模达百个量子比特。将模拟结果与精确能量（若可用）或密度矩阵重整化群计算结果进行比较。此外，还评估了变分态的质量，包括计算量子伊辛模型的磁化强度以及 Kitaev 蜂巢模型的拓扑纠缠熵。最后，在 Quantinuum 的 System Model H2 量子计算机上，使用 Pauli Path 模拟获得的参数化电路制备了 Kitaev 蜂巢模型的近似基态（包含 48 个量子比特），并进行了阿贝尔任意子的编织演示。

Result: Pauli Path 模拟结果与精确能量或密度矩阵重整化群计算结果高度一致。在 Quantinuum 的 H2 量子计算机上，制备的 Kitaev 蜂巢模型近似基态实现了约 5% 的相对能量误差（未使用误差缓解），并成功演示了阿贝尔任意子的编织。

Conclusion: Pauli Path 模拟是一种有效的方法，可以为大规模量子多体系统制备基态的参数化电路，并且在实际量子计算机上具有应用潜力，能够实现超越固定点模型的任意子编织。

Abstract: We use Pauli Path simulation to variationally obtain parametrized circuits
for preparing ground states of various quantum many-body Hamiltonians. These
include the quantum Ising model in one dimension, in two dimensions on square
and heavy-hex lattices, and the Kitaev honeycomb model, all at system sizes of
one hundred qubits or more, beyond the reach of exact state-vector simulation,
thereby reaching utility scale. We benchmark the Pauli Path simulation results
against exact ground-state energies when available, and against density-matrix
renormalization group calculations otherwise, finding strong agreement. To
further assess the quality of the variational states, we evaluate the
magnetization in the x and z directions for the quantum Ising models and
compute the topological entanglement entropy for the Kitaev honeycomb model.
Finally, we prepare approximate ground states of the Kitaev honeycomb model
with 48 qubits, in both the gapped and gapless regimes, on Quantinuum's System
Model H2 quantum computer using parametrized circuits obtained from Pauli Path
simulation. We achieve a relative energy error of approximately $5\%$ without
error mitigation and demonstrate the braiding of Abelian anyons on the quantum
device beyond fixed-point models.

</details>


### [140] [Mitigating the barren plateau problem in linear optics](https://arxiv.org/abs/2510.02430)
*Matthew D. Horner*

Main category: quant-ph

TL;DR: 通过限制参数化相位移位器具有两个不同的特征值，可以显著加速离散变量玻色采样变分量子算法，从而生成一个无论问题、初始状态还是电路布局如何都具有更少局部极小值和贫瘠平原的成本函数。这无需任何经典预处理即可实现，并允许使用快速无梯度 Rotosolve 算法。我们提出了三种实现方法：使用非线性光学、测量诱导的非线性或模拟费米子统计的纠缠资源态。其中后两种仅需线性光学，可使用当今广泛使用的技术实现。实验结果表明，在所有测试中，该方法优于已知的最优玻色采样变分算法。


<details>
  <summary>Details</summary>
Motivation: 现有的变分量子算法在面临局部极小值和贫瘠平原时，其优化过程效率低下。而参数化相位移位器具有两个不同的特征值，可以解决这个问题。

Method: 通过限制参数化相位移位器具有两个不同的特征值，生成一个成本函数，该函数拥有更少的局部极小值和贫瘠平原。我们提出三种实现方法：使用非线性光学、测量诱导的非线性或模拟费米子统计的纠缠资源态。可以实现无梯度 Rotosolve 算法。

Result: 该方法无需任何经典预处理，即可显著加速变分量子算法。在所有测试中，该方法均优于已知的最优玻色采样变分算法。

Conclusion: 通过限制参数化相位移位器具有两个不同的特征值，可以显著加速离散变量玻色采样变分量子算法，从而生成一个无论问题、初始状态还是电路布局如何都具有更少局部极小值和贫瘠平原的成本函数。这无需任何经典预处理即可实现，并允许使用快速无梯度 Rotosolve 算法。其中后两种仅需线性光学，可使用当今广泛使用的技术实现。实验结果表明，在所有测试中，该方法优于已知的最优玻色采样变分算法。

Abstract: We demonstrate a significant speedup of variational quantum algorithms that
use discrete variable boson sampling when the parametrised phase shifters are
constrained to have two distinct eigenvalues. This results in a cost landscape
with less local minima and barren plateaus regardless of the problem, ansatz or
circuit layout. This works without reliance on any classical pre-processing and
allows for the fast gradient-free Rotosolve algorithm to be used. We propose
three ways to achieve this by using either non-linear optics,
measurement-induced non-linearities, or entangled resource states simulating
fermionic statistics. The latter two require linear optics only, allowing for
implementation with widely-available technology today. We show this outperforms
the best-known boson sampling variational algorithm for all tests we conducted.

</details>


### [141] [Characterizing Superconducting Qubits using Averaged Circuit Eigenvalue Sampling](https://arxiv.org/abs/2510.02454)
*Tauno Palomaki,Shu Xin Wu,Noah Huffman,Samuel D. Park,James Shackford,Ben DalFavero,Leigh Norris,Ryan Sitler,Paraj Titum,Kevin Schultz*

Main category: quant-ph

TL;DR: ACES协议可有效表征量子门噪声，通过保利扭曲和测量误差校正，可准确识别特定噪声源。


<details>
  <summary>Details</summary>
Motivation: 高效表征量子门操作中的噪声是构建和扩展量子计算机的关键步骤。

Method: 利用ACES协议表征两个耦合超导量子比特，通过保利扭曲和测量误差校正来准确重建噪声模型。

Result: 将ACES协议预测的门保真度与传统方法（如交错随机线路）提取的保真度进行比较，并成功重建了注入到双量子比特门中的相位误差。

Conclusion: ACES协议能够准确地识别特定噪声源，为量子计算的噪声表征提供了有效工具。

Abstract: Efficient characterization of noise during quantum gate operations is an
essential step to building and scaling up a quantum computer. One such protocol
is averaged circuit eigenvalue sampling (ACES) which efficiently characterizes
a noisy gate set by reconstructing a Pauli noise model for a each gate. Here we
utilize the ACES protocol to characterize two coupled superconducting qubits.
For accurate reconstruction, we tailor the noise via Pauli twirling and account
for measurement errors. We verify the accuracy of the protocol by comparing the
predicted gate fidelities to that extracted from conventional benchmarking
approaches, such as interleaved randomized benchmarking. Furthermore, we
demonstrate the efficacy of ACES in accurately identifying specific noise
sources by reconstructing injected phase errors in the two-qubit gates.

</details>


### [142] [Computational access to lattice and long-wavelength physics in quantum mutual information](https://arxiv.org/abs/2510.02466)
*Patrick M. Lenggenhager,M. Michael Denner,Doruk Efe Gökmen,Maciej Koch-Janusz,Titus Neupert,Mark H. Fischer*

Main category: quant-ph

TL;DR: 量子互信息在量子多体系统中用于表征关联，但数值计算成本高。本研究分析了 Rényi 互信息 (RMI) 的 $\alpha$-$z$ 族在晶格模拟中对长波长物理学的相关性，通过无质量自由费米子和横向场伊辛模型 (TFIM) 的例子，确定了 RMI 在晶格效应修正下的相关性区域，并提供了开源 Julia 包 QMICalc.jl。


<details>
  <summary>Details</summary>
Motivation: 确定 RMI（一种计算上更易处理的量子互信息变体）是否能正确捕捉长波长物理学，以及避免紫外效应，这对于晶格模拟至关重要。

Method: 分析了 $\alpha$-$z$ Rényi 互信息族对于具有共形场论描述的模型基态的晶格效应相关性。通过无质量自由费米子和横向场伊辛模型 (TFIM) 的例子，结合张量网络态（MPS）计算，确定了 RMI 在 $\alpha$-$z$ 平面上的不同区域，其中 RMI 的晶格修正相关或不相关。

Result: 在无质量自由费米子模型上，识别了 $\alpha$-$z$ 平面上 RMI 晶格修正相关或不相关的区域。通过 MPS 计算验证了横向场伊辛模型 (TFIM) 的结果。

Conclusion: 提出的研究结果为在量子多体物理数值计算中使用 RMI 提供了指导，并伴随开源 Julia 包 QMICalc.jl。

Abstract: Quantum mutual information is an important tool for characterizing
correlations in quantum many-body systems, but its numerical evaluation is
often prohibitively expensive. While some variants of R\'enyi Mutual
Information (RMI) are computationally more tractable, it is not clear whether
they correctly capture the long-wavelength physics or are dominated by UV
effects, which is of key importance in lattice simulations. We analyze the
relevance of lattice effects on the family of $\alpha$-$z$ R\'enyi mutual
informations for ground states of models with conformal field theory
descriptions. On the example of massless free fermions we identify distinct
regions in the $\alpha$-$z$ plane, where RMI corrections due to the lattice are
relevant or irrelevant. We further support these findings with MPS calculations
on the transverse field Ising model (TFIM). Our results, accompanied by the
open-source Julia package QMICalc.jl, provide guidance to using RMI in
quantum-many body physics numerical computations.

</details>


### [143] [On a Class of Time-Dependent Non-Hermitian Hamiltonians](https://arxiv.org/abs/2510.02494)
*F. Kecita,B. Khantoul,A. Bounames*

Main category: quant-ph

TL;DR: TD非厄米哈密顿量可以通过酉变换和相似变换转化为时间无关的厄米哈密顿量，从而简化计算并导出其不确定性关系。


<details>
  <summary>Details</summary>
Motivation: 研究一类时间依赖的非厄米哈密顿量，并提供一种简化计算其不确定性关系的方法。

Method: 通过TD酉变换将TD非厄米哈密顿量转化为时间无关的伪厄米哈密顿量，再通过相似变换转化为厄米哈密顿量，然后求解厄米哈密顿量的薛定谔方程，最后推导出原系统的解、内积和不确定性关系。

Result: 推导了TD非厄米系统的$	ilde{	ilde{\eta}}(t)$-内积，并以TD质量和复线性势为例，得到了两种厄米哈密顿量（标准谐振子和倒置谐振子）的精确解析解（厄米特多项式），且在谐振子情况下验证了位置-动量不确定性关系。

Conclusion: 该方法成功地简化了TD非厄米系统的计算，并导出了其物理上一致的不确定性关系。

Abstract: We study a class of time-dependent (TD) non-Hermitian Hamiltonians $H(t)$
that can be transformed into a time-independent pseudo-Hermitian Hamiltonian
$\mathcal{H}_{0}^{PH}$ using a suitable TD unitary transformation $F(t)$. The
latter can in turn be related to a Hermitian Hamiltonian $h$ by a similarity
transformation, $h=\rho \mathcal{H}_{0}^{PH} \rho^{-1}$ where $\rho$ is the
Dyson map. Accordingly, once the Schr\"{o}dinger equation for the Hermitian
Hamiltonian $h$ is solved, the general solution of the initial system can be
deduced. This allows to define the appropriate $\tilde{\eta}(t)$-inner product
for the Hilbert space associated with $H(t)$, where
$\tilde{\eta}(t)=F^{\dagger}(t)\eta F(t)$ and $\eta=\rho^{\dagger}\rho$ is the
metric operator. This greatly simplifies the computation of the relevant
uncertainty relations for these systems. As an example, we consider a model of
a particle with a TD mass subjected to a specific TD complex linear potential.
We thus obtain two Hermitian Hamiltonians, namely that of the standard harmonic
oscillator and that of the inverted oscillator. For both cases, the auxiliary
equation admits a solution, and the exact analytical solutions are squeezed
states given in terms of the Hermite polynomials with complex coefficients.
Moreover, when the Hermitian Hamiltonian is that of the harmonic oscillator,
the position-momentum uncertainty relation is real and greater than or equal to
$\hbar/2$, thereby confirming its consistency.

</details>


### [144] [Amplitude-based Input Attribution in Quantum Learning via Integrated Gradients](https://arxiv.org/abs/2510.02497)
*Nicholas S. DiBrita,Jason Han,Younghyun Cho,Hengrui Luo,Tirthak Patel*

Main category: quant-ph

TL;DR: HATTRIQ是一个通用框架，用于计算基于幅度的输入归因分数，以解决量子机器学习模型的可解释性问题。


<details>
  <summary>Details</summary>
Motivation: 量子机器学习（QML）算法的内在不透明性以及现有可解释性方法的局限性，阻碍了其发展和应用。

Method: HATTRIQ框架通过哈达玛测试在量子硬件上直接计算输入梯度，以生成基于幅度的输入归因分数，支持幅度嵌入特征编码方案。

Result: 在Bars and Stripes、MNIST和FashionMNIST等数据集的分类任务上验证了HATTRIQ的有效性。

Conclusion: HATTRIQ为电路型QML模型提供了可解释性解决方案，有望推动QML模型的发展和应用。

Abstract: Quantum machine learning (QML) algorithms have demonstrated early promise
across hardware platforms, but remain difficult to interpret due to the
inherent opacity of quantum state evolution. Widely used classical
interpretability methods, such as integrated gradients and surrogate-based
sensitivity analysis, are not directly compatible with quantum circuits due to
measurement collapse and the exponential complexity of simulating state
evolution. In this work, we introduce HATTRIQ, a general-purpose framework to
compute amplitude-based input attribution scores in circuit-based QML models.
HATTRIQ supports the widely-used input amplitude embedding feature encoding
scheme and uses a Hadamard test-based construction to compute input gradients
directly on quantum hardware to generate provably faithful attributions. We
validate HATTRIQ on classification tasks across several datasets (Bars and
Stripes, MNIST, and FashionMNIST).

</details>


### [145] [Many Retrocausal Worlds: A Foundation for Quantum Probability](https://arxiv.org/abs/2510.02505)
*Michael Ridley*

Main category: quant-ph

TL;DR: 许多人世界解释在概率方面存在问题，但时间扩展的自我定位概率可以解决这些问题，并且可以通过固定点公式和波函数来找到解决方案。


<details>
  <summary>Details</summary>
Motivation: 许多人世界解释在概率方面存在问题，因此需要一种新的概率理论来解决这些问题。

Method: 通过讨论各种概率理论，提出时间扩展的自我定位概率来解决不连贯问题，并驳斥定量问题的解决方案。最后，通过固定点公式和波函数来为量子概率提供基础。

Result: 时间扩展的自我定位概率可以解决许多人世界解释中的不连贯问题，并且可以通过固定点公式和波函数找到定量问题的解决方案。

Conclusion: 固定点公式，在时间对称的埃弗里特框架内进行解释，可以为量子概率理论提供基础，以解决许多人世界解释中的概率问题。

Abstract: Recent accounts of probability in the many worlds interpretation of quantum
mechanics are vulnerable due to their dependence on probability theory per se.
For this reason, the many worlds interpretation continues to suffer from the
incoherence and quantitative problems. After discussing various theories of
probability, I discuss the incoherence problem and argue that self-locating
probabilities centered in time-extended worlds can solve it. I then discuss and
refute various solutions to the quantitative problem. I argue that the only
tenable way to ground these self-locating probabilities is to identify the
mathematical form of the Born rule as a generic pattern in a time-extended
wavefunction, and to distribute degrees of belief over the region of
wavefunction occupied by this pattern. I then outline a time-symmetric version
of quantum mechanics - the Fixed Point Formulation - which, interpreted within
a time-symmetric Everettian framework, can provide the foundation for a theory
of quantum probability.

</details>


### [146] [Emulation of Coherent Absorption of Quantum Light in a Programmable Linear Photonic Circuit](https://arxiv.org/abs/2510.02541)
*Govind Krishna,Jun Gao,Sam O Brien,Rohan Yadgirkar,Venkatesh Deenadayalan,Stefan Preble,Val Zwiller,Ali W. Elshaari*

Main category: quant-ph

TL;DR: 本研究提出了一种利用可编程集成线性光子电路实现非酉变换的方法，以模拟量子相干吸收现象。


<details>
  <summary>Details</summary>
Motivation: 相干吸收是控制量子态的有力工具，本研究旨在实现可编程的相干吸收。

Method: 利用可编程集成线性光子电路，通过耦合到辅助模式引入损耗，实现非酉变换，模拟量子相干吸收。

Result: 实验实现了单光子和两光子输入下的相干吸收，并观察到非经典效应。对单光子态和NOON态的相敏性分析超过了散粒噪声极限，接近海森堡极限。

Conclusion: 研究证明了辅助模式辅助电路在量子态工程、过滤、多路传感和非酉量子模拟方面的潜力。

Abstract: Non-Hermitian quantum systems, governed by nonunitary evolution, offer
powerful tools for manipulating quantum states through engineered loss. A prime
example is coherent absorption, where quantum states undergo phase-dependent
partial or complete absorption in a lossy medium. Here, we demonstrate a fully
programmable implementation of nonunitary transformations that emulate coherent
absorption of quantum light using a programmable integrated linear photonic
circuit, with loss introduced via coupling to an ancilla mode [Phys. Rev. X 8,
021017; 2018]. Probing the circuit with a single-photon dual-rail state reveals
phase-controlled coherent tunability between perfect transmission and perfect
absorption. A two-photon NOON state input, by contrast, exhibits switching
between deterministic single-photon and probabilistic two-photon absorption.
Across a range of input phases and circuit configurations, we observe
nonclassical effects such as anti-coalescence and bunching, along with
continuous and coherent tuning of output Fock state probability amplitudes.
Classical Fisher information analysis reveals phase sensitivity peaks of 0.79
for single-photon states and 3.7 for NOON states, exceeding the shot-noise
limit of 2 and approaching the Heisenberg limit of 4. The experiment integrates
quantum state generation, programmable photonic circuitry, and
photon-number-resolving detection, establishing ancilla-assisted circuits as
powerful tools for programmable quantum state engineering, filtering,
multiplexed sensing, and nonunitary quantum simulation.

</details>


### [147] [Quantum simulation of carbon capture in periodic metal-organic frameworks](https://arxiv.org/abs/2510.02550)
*Dario Rocca,Jerome F. Gonthier,Joshua Levin,Tobias Schafer,Andreas Gruneis,Byeol Kang,Hong Woo Lee*

Main category: quant-ph

TL;DR: Fe-MOF-74对二氧化碳吸附的研究，采用量子计算方法，并在量子硬件上进行了实验验证。


<details>
  <summary>Details</summary>
Motivation: 碳捕获对于脱碳重工业至关重要，而MOFs材料在该领域具有潜力。Fe-MOF-74由于其特殊的电子结构，在二氧化碳吸附方面表现出色，但其计算模拟具有挑战性。

Method: 通过比较不同的DFT泛函来评估吸附能，并提出了一种基于Wannier函数和自然轨道选择的活性空间缩减策略。该策略结合了MP2自然轨道和量子数保持的ansatz，利用变分量子本征求解器（VQE）框架计算吸附能。此外，还通过基于样本的量子对角化方法进行了量子实验。

Result: 通过量子计算和量子实验，在Fe-MOF-74材料上获得了更准确的二氧化碳吸附能。尽管目前的量子硬件存在局限性，但该方法为解决实际问题提供了一条更有效、更具扩展性的途径。

Conclusion: 本研究展示了量子算法在处理实际的碳捕获模型和周期性材料方面的应用前景，为未来量子计算在材料科学领域的应用奠定了基础。

Abstract: Carbon capture is vital for decarbonizing heavy industries such as steel and
chemicals. Metal-organic frameworks (MOFs), with their high surface area and
structural tunability, are promising materials for CO2 capture. This study
focuses on Fe-MOF-74, a magnetic Mott insulator with exposed metal sites that
enhance CO2 adsorption. Its strongly correlated electronic structure challenges
standard DFT methods, which often yield inconsistent predictions. We initially
benchmark adsorption energies using various DFT functionals, revealing
substantial variability and underscoring the need for more accurate approaches,
such as those provided by quantum computing. However, practical quantum
algorithms are far less established for simulations of periodic materials,
particularly when the plane-wave basis set, often comprising tens of thousands
of basis vectors, is used. To address this, we employ an active space reduction
strategy based on Wannier functions and natural orbital selection. Localized
orbitals around the adsorption site are identified, and MP2 natural orbitals
are used to improve convergence of correlation energies. Adsorption energies
are then computed using a quantum number-preserving ansatz within the
variational quantum eigensolver framework. In addition to classical
simulations, we conduct quantum experiments using the sample-based quantum
diagonalization method. Although current hardware limits the size of feasible
simulations, our approach offers a more efficient and scalable path forward.
These results advance the applicability of quantum algorithms to realistic
models of carbon capture and periodic materials more broadly.

</details>


### [148] [Construction of the Complete Set of Maximally Entangled Basis Vectors for N-Qubit Systems](https://arxiv.org/abs/2510.02685)
*Chi-Chuan Hwang*

Main category: quant-ph

TL;DR: 本研究使用三量子比特系统举例，展示了八个最大纠缠基矢的量子电路构建，并将其推广到N量子比特系统。通过随机数方法生成最大纠缠基矢及其对应的电路，并详细说明了所需的单量子比特和CNOT门数量。该方法为技术应用提供了理论基础和实用技术，克服了存储大规模编码数据的困难。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在解决在N量子比特系统中构建最大纠缠基矢的量子电路的难题，并为技术应用提供实用的方法。

Method: 本研究采用随机数方法生成最大纠缠基矢及其对应的量子电路，并统计所需的单量子比特和CNOT门数量，最后将该方法从三量子比特系统推广到N量子比特系统。

Result: 研究成功地展示了如何构建最大纠缠基矢的量子电路，并确定了所需门的数量，为N量子比特系统提供了一种通用的方法。

Conclusion: 本研究提出了一种生成最大纠缠基矢量子电路的实用方法，该方法具有理论和实践意义，并能克服存储大规模数据的挑战。

Abstract: In this study, we first use a three-qubit system as an example to demonstrate
the construction of quantum circuits for the eight maximally entangled basis
vectors, subsequently extending the approach to N-qubit systems. We employ a
random-number approach to generate maximally entangled basis vectors and their
corresponding circuits, while also detailing the required number of
single-qubit and CNOT gates. This approach not only provides a solid
theoretical foundation but also establishes a practical technique for
technological applications, bypassing the difficulty of storing large-scale
encoding data.

</details>


### [149] [Complementarity Reveals Entanglement Sharing in Sequential Quantum Measurements](https://arxiv.org/abs/2510.02710)
*Zinuo Cai,Changliang Ren*

Main category: quant-ph

TL;DR: 该研究探讨了量子比特序列测量中纠缠共享的现象，并使用了互信息、条件概率之和以及皮尔逊相关系数三种经典相关性度量。研究发现，弱测量策略比概率性测量更能有效地展示纠缠共享，并且皮尔逊相关系数在所有策略和场景中都最为稳健。


<details>
  <summary>Details</summary>
Motivation: 该研究的动机在于探索和理解在量子比特序列测量中纠缠共享的共享机制，并评估不同测量策略和经典相关性度量在认证该现象中的有效性。

Method: 研究采用了弱测量和概率性投影测量（PPM）策略，并结合了互信息（I）、条件概率之和（S）和皮尔逊相关系数（C）三种经典相关性度量，在单边和双边场景下对纠缠共享进行了量化和分析。

Result: 研究结果表明，当多种经典相关性度量同时超过阈值时，可以确凿地认证纠缠共享。弱测量策略优于PPM策略，而互信息标准在双边场景下失效，皮尔逊相关系数标准最为稳健。

Conclusion: 该研究揭示了测量扰动和互补相关性恢复之间的关键权衡，这对于量子资源重复利用问题至关重要。

Abstract: We investigate entanglement sharing in a two-qubit sequential measurement
scenario using three complementary classical correlation metrics: mutual
information (I), sum of conditional probabilities (S), and the Pearson
correlation coefficient (C). By investigating both weak measurement and
probabilistic projective measurement (PPM) strategies in unilateral and
bilateral scenarios, the phenomenon of entanglement sharing is conclusively
certified when multiple pairs of classical correlation metrics simultaneously
exceed their thresholds. Our investigation reveals that weak measurement
strategies are more favorable than PPM for exhibiting entanglement sharing,
regardless of the scenario. Furthermore, the mutual information criterion fails
to characterize entanglement sharing in the bilateral scenario. While, the
Pearson correlation criterion (C) is proven to be the most robust across all
strategies and scenarios. These findings unveil a critical trade-off between
measurement disturbance and complementary correlation recovery, which is
essential for quantum resource reuse problems.

</details>


### [150] [Bounds on Atomistic Disorder for Scalable Electron Shuttling](https://arxiv.org/abs/2510.03113)
*Raphaël J. Prentki,Pericles Philippopoulos,Mohammad Reza Mostaan,Félix Beaudoin*

Main category: quant-ph

TL;DR: 电子穿梭技术在硅自旋量子计算中是关键，但其保真度受限于原子尺度无序性。本研究提出一个结合了时变有限元静电学和原子尺度紧束缚的仿真框架，以模拟随机合金化和界面粗糙度对电子谷分裂和相的影响。研究发现界面粗糙度显著降低了电子穿梭的保真度，尤其在原子层尺度附近出现异常，为实现可扩展的电子穿梭提供了量化指导。


<details>
  <summary>Details</summary>
Motivation: 电子穿梭是实现可扩展硅自旋量子计算的关键技术，但其保真度受到原子尺度无序性的限制。

Method: 提出一个结合了时变有限元静电学和原子尺度紧束缚的仿真框架，用以捕捉随机合金化和界面粗糙度对被穿梭电子的谷分裂和相的影响。

Result: 研究发现界面粗糙度显著降低了电子穿梭的保真度，在原子层尺度附近出现异常。

Conclusion: 界面粗糙度是限制电子穿梭保真度的主要因素，尤其在原子层尺度附近存在显著影响，为实现可扩展的电子穿梭提供了量化指导。

Abstract: Electron shuttling is emerging as a key enabler of scalable silicon
spin-qubit quantum computing, but fidelities are limited by atomistic disorder.
We introduce a multiscale simulation framework combining time-dependent
finite-element electrostatics and atomistic tight-binding to capture the impact
of random alloying and interface roughness on the valley splitting and phase of
shuttled electrons. We find that shuttling fidelities are strongly suppressed
by interface roughness, with a sharp anomaly near the atomic-layer scale,
setting quantitative guidelines to realize scalable shuttling.

</details>


### [151] [Dynamics of Quantum Entanglement Between Photon and Phonon Modes in a Coulomb-coupled Optomechanical Cavity Magnonic Systems](https://arxiv.org/abs/2510.02767)
*Muhib Ullah,Muhammad Idrees,Said Mikki*

Main category: quant-ph

TL;DR: 该研究提出了一个包含YIG球的混合光力腔-磁系统中，利用磁耦合、光力耦合和静电耦合实现光子、声子和宏观原子的纠缠，并分析了磁非线性Kerr效应和耦合强度对纠缠动力学的影响，最终实现了在3K温度下，库仑耦合和磁耦合协同作用下，提高宏观原子纠缠度和可调性，增强其对热浴的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 量子纠缠是量子信息科学的基础，也是量子技术（如精密传感、安全通信和计算）的关键资源。在混合腔体磁光力学系统中，腔光子、宏观原子和声子的纠缠使得通过基于辐射压的光-物质相互作用实现跨模式的量子态操控。

Method: 提出了一个包含YIG球的混合光力腔-磁系统，利用磁偶极相互作用使宏观原子与光子耦合，并通过光力学和静电相互作用将腔光子与机械谐振器纠缠。重点分析了宏观原子的非线性Kerr效应和耦合强度的变化对纠缠动力学的影响。

Result: 通过分析非线性效应和其他耦合参数，确定了在两个库仑耦合的机械谐振器运动模式（声子）之间产生和维持鲁棒纠缠的最佳条件。该模型预测，添加库仑相互作用和宏观原子耦合可以提高纠缠的度和可调性，并使其对3K的温度浴更加鲁棒。

Conclusion: 这项研究解决了如何在复杂的混合量子系统中优化和维持声子-声子纠缠的问题。其研究结果为开发用于连续变量量子信息处理的量子存储器和其他量子技术提供了见解。

Abstract: Quantum entanglement is a fundamental phenomenon in quantum information
science and a crucial resource for quantum technologies such as precision
sensing, secure communication, and computation. In hybrid cavity
magno-optomechanical systems, entanglement among cavity photons, magnons, and
phonons enables manipulation of quantum states across different modes via
radiation pressure-based light-matter interaction. We propose a hybrid
optomechanical cavity-magnonic setup with Yttrium iron garnet (YIG) sphere
allowing magnons to couple with photons via magnetic-dipole interaction. The
system also uses optomechanical and electrostatic interactions to entangle
cavity photons and mechanical resonators. We focus on how the magnon nonlinear
Kerr effect and varying coupling strengths influence entanglement dynamics. By
analysing nonlinear effects alongside other coupling parameters, we identify
optimal conditions for initiating and sustaining robust entanglement between
the motional modes (phonons) of two Coulomb-coupled mechanical resonators. Our
model predicts that adding Coulomb interaction and magnon coupling enhances the
degree and tunability of entanglement, making it more resilient to thermal
baths at 3 K. This study addresses how to optimize and sustain phonon-phonon
entanglement in complex hybrid quantum systems. These findings offer insights
relevant to developing quantum memories for continuous variable quantum
information processing and other quantum technologies.

</details>


### [152] [Any type of spectroscopy can be efficiently simulated on a quantum computer](https://arxiv.org/abs/2510.02784)
*Liam P. Flew,Ivan Kassal*

Main category: quant-ph

TL;DR: 量子计算机可以高效模拟任何类型的分子光谱，包括任意阶数、任意频段以及涉及电和磁跃迁的光谱。该方法通过计算基于双面费曼图谱的任何光谱相关函数来实现，可用于模拟闭合和开放分子系统以及数字和模拟量子计算机的光谱。


<details>
  <summary>Details</summary>
Motivation: 经典计算机模拟分子光谱计算成本高，且成本随分子大小呈指数增长。量子计算机有望更高效地模拟光谱。

Method: 利用量子计算机，通过计算基于双面费曼图谱的光谱相关函数，以时域方法模拟任何类型的分子光谱。

Result: 提出的方法可以高效模拟任何类型的光谱，包括任意阶数、任意频段以及涉及电和磁跃迁的光谱。该方法适用于闭合和开放分子系统，并可在数字和模拟量子计算机上运行。

Conclusion: 量子计算机通过时域方法能够高效地模拟任何类型的分子光谱，为分子光谱学研究提供了新的计算工具。

Abstract: Spectroscopy is the most important method for probing the structure of
molecules. However, predicting molecular spectra on classical computers is
computationally expensive, with the most accurate methods having a cost that
grows exponentially with molecule size. Quantum computers have been shown to
simulate simple types of optical spectroscopy efficiently -- with a cost
polynomial in molecule size -- using methods such as time-dependent simulations
of photoinduced wavepackets. Here, we show that any type of spectroscopy can be
efficiently simulated on a quantum computer using a time-domain approach,
including spectroscopies of any order, any frequency range, and involving both
electric and magnetic transitions. Our method works by computing any
spectroscopic correlation function based on the corresponding double-sided
Feynman diagram, the canonical description of spectroscopic interactions. The
approach can be used to simulate spectroscopy of both closed and open molecular
systems using both digital and analog quantum computers.

</details>


### [153] [Accurate and Effective Model for Coexistence of Classical and Quantum Signals In Optical Fibers](https://arxiv.org/abs/2510.02807)
*Lucas Alves Zischler,Çağla Özkan,Tristan Vosshenrich,Qi Wu,Giammarco Di Sciullo,Divya A. Shaji,Chiara Lasagni,Paolo Serena,Alberto Bononi,Amirhossein Ghazisaeidi,Chigo Okonkwo,Antonio Mecozzi,Cristian Antonelli*

Main category: quant-ph

TL;DR: 量子通信和经典信号在同一光纤信道中共存时，经典信号产生的噪声会干扰量子信号。本文提出了一个模型来量化这种干扰的影响，并分析了不同共存方案下的干扰机制和缓解方法。


<details>
  <summary>Details</summary>
Motivation: 解决经典信号和量子信号在同一光纤信道中共存时，由于经典信号产生的噪声干扰量子信号的问题。

Method: 提出并量化了一个模型，该模型考虑了自发拉曼散射（SpRS）、四波混频（FWM）、瑞利 backscattering 和受激拉曼散射（SRS）引起的功率倾斜等干扰机制，并能在毫秒级时间尺度上进行快速数值求解。

Result: 研究表明，干扰噪声功率在传输频带的高端被最小化。当量子信号与经典信号共传播时，FWM 干扰可能很重要，但可以通过重新分配经典信号来缓解。在某些情况下，共传播或反向传播方案的选择取决于链路长度和量子信号中心频率。

Conclusion: 该模型能够准确评估共存场景下的量子信号质量，并为优化光纤通信系统中的量子信号传输提供指导。

Abstract: The rising interest in quantum-level communication has resulted in proposals
for coexistence schemes with classical signals within the same fiber optic
channel, where the most recent proposals leverage novel fibers designed for
space-division multiplexing (SDM) transmission. In all cases the large power
difference between classical and quantum channels presents challenges for such
schemes, as the classical signals generate interfering noise that corrupts the
quantum signal. In this work, we discuss the main interference mechanisms in
coexistence scenarios and provide a model to quantify their impact on the
quantum signal quality. Analytical approximations in the model allow accurate
and fast numerical solutions in the millisecond time-scale. The model accounts
for out-of-band non-linear interference effects, namely spontaneous Raman
scattering (SpRS) and four-wave-mixing (FWM) in both cases of single-mode and
SDM fibers with weakly-coupled degenerate mode groups. Rayleigh and SpRS
backscattering are considered in counter-propagating scenarios. Since broadband
classical transmission is targeted, the model also accounts for the effect of
stimulated Raman scattering (SRS)-induced power tilt. Use of the model in
sample scenarios indicates that the interference noise power is minimized at
the high end of the transmission band in both cases were the quantum is co- and
counter-propagating with respect to the classical signals, with a preference of
one or the other scheme depending on the link length and quantum signal center
frequency. Our model reveals that FWM has negligible impact in
counter-propagating schemes, but can be relevant in co-propagating schemes
under certain scenarios. Nevertheless, the FWM interference can be mitigated by
deallocating the classical signals adjacent to the quantum channel.

</details>


### [154] [Quantum sensing with discrete time crystals in the Lipkin-Meshkov-Glick Model](https://arxiv.org/abs/2510.02825)
*Rahul Ghosh,Bandita Das,Victor Mukherjee*

Main category: quant-ph

TL;DR: 周期性调制的Lipkin-Meshkov-Glick模型中的离散时间晶体（DTC）相变可用于量子增强的高精度场强传感，该模型利用量子临界性来提高传感精度。


<details>
  <summary>Details</summary>
Motivation: 量子传感可以从量子相变中受益，特别是在临界点附近，因为量子Fisher信息会发散。本文旨在探讨利用离散时间晶体（DTC）相变作为量子传感的机制。

Method: 本文采用详细的有限尺寸标度分析和时间平均逆参与比分析来确定二阶相变的临界性质，并研究周期性调制的Lipkin-Meshkov-Glick模型。

Result: 研究表明，DTC相变可以实现量子增强的高精度场强传感，并对临界性质进行了详细分析。

Conclusion: 量子临界性，特别是在具有长程相互作用的DTC中，可以有效地应用于先进的量子传感领域，从而实现更高的传感精度。

Abstract: Quantum phase transitions have been shown to be highly beneficial for quantum
sensing, owing to diverging quantum Fisher information close to criticality. In
this work we consider a periodically modulated Lipkin-Meshkov-Glick model to
show that discrete time crystal (DTC) phase transition in this setup can enable
us to achieve quantum-enhanced high-precision sensing of field strength. We
employ a detailed finite-size scaling analysis and a time-averaged Inverse
Participation Ratio analysis to determine the critical properties of this
second-order phase transition. Our studies provide a comprehensive
understanding of how quantum criticality in DTCs involving long-range
interactions can be harnessed for advanced quantum sensing applications.

</details>


### [155] [Testing Quantum Mechanics with Quantum Computers: Qubit Information Capacity](https://arxiv.org/abs/2510.02877)
*Tim Palmer*

Main category: quant-ph

TL;DR: 希尔伯特空间被离散化，导致量子比特信息容量有限，可能限制量子计算机的指数级加速。


<details>
  <summary>Details</summary>
Motivation: 受约翰·惠勒关于希尔伯特空间连续性掩盖量子波函数信息论性质的论断的启发。

Method: 提出复希尔伯特空间的一种特定离散化方法，从而得出量子比特信息容量 $N_{\mathrm{max}}$ 的概念。

Result: 估计典型量子计算机中的量子比特， $N_{\mathrm{max}} \approx 500-1,000$。预测使用超过约 1,000 个逻辑量子比特的量子计算机将饱和指数级加速。如果得到验证，使用量子计算机分解 2048-RSA 整数在实践中将是不可能的。

Conclusion: 有限的量子比特信息容量对量子物理学的基础（包括测量问题、互补性和非局域性）以及综合量子和引力物理学的新颖理论的发展具有深远的影响。

Abstract: Motivated by John Wheeler's assertion that the continuum nature of Hilbert
Space conceals the information-theoretic nature of the quantum wavefunction, a
specific discretisation of complex Hilbert Space is proposed, leading to the
notion of qubit information capacity $N_{\mathrm{max}}$: for any $N \ge
N_{\mathrm{max}}$-qubit state, there is insufficient information in the $N$
qubits to allocate even one bit to each of the $2^{N+1}-2$ degrees of freedom
demanded by complex Hilbert Space and hence unitary quantum mechanics. Using
gravitised quantum mechanics, it is estimated that, for typical qubits in a
quantum computer, $N_{\mathrm{max}} \approx 500-1,000$. By contrast,
$N_{\mathrm{max}}=\infty$ in quantum mechanics. On this basis, it is predicted
that the exponential speed up of algorithms such as Shor's will have saturated
in quantum computers which use more than about 1,000 logical qubits. This
predicted breakdown of quantum mechanics should be testable within the coming
decade. If verified, factoring 2048-RSA integers using quantum computers will
for all practical purposes be impossible. The existence of a finite qubit
information capacity has profound implications for reimagining the foundations
of quantum physics (including the measurement problem, complementarity and
nonlocality) and for developing novel theories which synthesise quantum and
gravitational physics.

</details>


### [156] [Universal classical-quantum channel resolvability and private channel coding](https://arxiv.org/abs/2510.02883)
*Takaya Matsuura,Masahito Hayashi,Min-Hsiu Hsieh*

Main category: quant-ph

TL;DR: 提出了一种确定性的、与信道无关的编码本构造方法，用于构建完全通用的经典-量子（c-q）信道私有信道编码协议。


<details>
  <summary>Details</summary>
Motivation: 解决先前工作中编码器依赖于随机编码的问题，实现了完全通用的编码本构造。

Method: 利用与编码本相关的 Schreier 图的谱展开的明确结构属性来保证通用的信道可分辨性。通过诱导表示理论将此属性与信道可分辨性相关联。

Result: 当与编码本相关的图的转移矩阵具有大的谱隙时，通过均匀采样的码字引起的信道输出来，可以实现与目标输出分布的渐近不可区分性，而与信道无关。结合基于 Schur-Weyl 对偶的通用 c-q 信道编码，构建了完全通用的私有信道编码协议，并实现了最优速率。

Conclusion: 这项工作首次实现了确定性的、与信道无关的编码本构造，为安全通信中的图的扩展器属性提供了新的视角。

Abstract: We address the problem of constructing fully universal private channel coding
protocols for classical-quantum (c-q) channels. Previous work constructed
universal decoding strategies, but the encoder relied on random coding, which
prevents fully universal code construction. In this work, we resolve this gap
by identifying an explicit structural property of codebooks -- namely, the
spectral expansion of an associated Schreier graph -- that guarantees universal
channel resolvability. Our analysis reveals how this property can be related to
channel resolvability through the theory of induced representation. Our main
technical result shows that when the transition matrix of a graph associated
with a codebook has a large spectral gap, the channel output induced by
uniformly sampled codewords is asymptotically indistinguishable from the target
output distribution, independently of the channel. This establishes the first
deterministic, channel-independent construction of resolvability codebooks.
  Building on this, we construct a fully universal private channel coding
protocol by combining it with universal c-q channel coding based on the
Schur-Weyl duality. With appropriate modifications to the requirements on
codebooks of universal c-q channel coding, we show that our fully universal
private channel coding achieves the known optimal rate. This work thus sheds
new light on the expander property of a graph in the context of secure
communication.

</details>


### [157] [Scalable Quantum Optimisation using HADOF: Hamiltonian Auto-Decomposition Optimisation Framework](https://arxiv.org/abs/2510.02926)
*Namasi G Sankar,Georgios Miliotis,Simon Caton*

Main category: quant-ph

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Quantum Annealing (QA) and QAOA are promising quantum optimisation algorithms
used for finding approximate solutions to combinatorial problems on near-term
NISQ systems. Many NP-hard problems can be reformulated as Quadratic
Unconstrained Binary Optimisation (QUBO), which maps naturally onto quantum
Hamiltonians. However, the limited qubit counts of current NISQ devices
restrict practical deployment of such algorithms. In this study, we present the
Hamiltonian Auto-Decomposition Optimisation Framework (HADOF), which leverages
an iterative strategy to automatically divide the Quadratic Unconstrained
Binary Optimisation (QUBO) Hamiltonian into sub-Hamiltonians which can be
optimised separately using Hamiltonian based optimisers such as QAOA, QA or
Simulated Annealing (SA) and aggregated into a global solution. We compare
HADOF with Simulated Annealing (SA) and the CPLEX exact solver, showing
scalability to problem sizes far exceeding available qubits while maintaining
competitive accuracy and runtime. Furthermore, we realise HADOF for a toy
problem on an IBM quantum computer, showing promise for practical applications
of quantum optimisation.

</details>


### [158] [MAQCY: Modular Atom-Array Quantum Computing with Space-Time Hybrid Multiplexing](https://arxiv.org/abs/2510.02940)
*Andrew Byun,Chanseul Lee,Eunsik Yoon,Minhyuk KimMinhyuk Kim,Tai Hyun Yoon*

Main category: quant-ph

TL;DR: MAQCY是一种基于动态光镊的量子计算架构，利用Q-Pairs（一种由全局控制的、时间复用的双物种里德堡原子和超原子对组成）实现了全连接和可扩展的通用量子计算。该架构通过在原位替换原子，实现了O(N)的线性扩展，同时保持了相干性并缓解了电路深度限制。通过一个三比特量子傅里叶变换的实现，证明了MAQCY的通用性，并提出了使用镱同位素的具体实现方案，为大规模容错量子计算铺平了道路。


<details>
  <summary>Details</summary>
Motivation: 本文旨在提出一种新的量子计算架构（MAQCY），以解决现有量子计算方法在扩展性、连通性和电路深度方面的限制，并为实现大规模、容错量子计算提供一种可行的方案。

Method: 提出了一种名为MAQCY（Modular Atom-Array Quantum Computing Architecture with Space-Time Hybrid Multiplexing）的架构。该架构基于动态光镊技术，并引入了Q-Pairs（由全局控制的、时间复用的双物种里德堡原子和超原子对）的概念。通过空间-时间混合复用Q-Pairs，实现了O(N)的线性扩展，并通过原位原子替换来保持相干性并缓解电路深度限制。

Result: 通过一个三比特量子傅里叶变换的实现，证明了MAQCY架构仅使用全局操作和原子输运即可实现通用量子计算。同时，提出了使用镱同位素的具体实现方案。

Conclusion: MAQCY架构通过其创新的Q-Pairs设计和空间-时间混合复用策略，能够实现全连接、可扩展的通用量子计算，并有效缓解了相干性和电路深度问题。提出的镱同位素实现方案为未来大规模容错量子计算的发展奠定了基础。

Abstract: We present a modular atom-array quantum computing architecture with
space-time hybrid multiplexing (MAQCY), a dynamic optical tweezer-based
protocol for fully connected and scalable universal quantum computation. By
extending the concept of globally controlled static dual-species Rydberg atom
wires [1], we develop an entirely new approach using Q-Pairs, which consist of
globally controlled and temporally multiplexed dual-species Rydberg blockaded
atom and superatom pairs. Space-time hybrid multiplexing of Q-Pairs achieves
O(N) linear scaling in the number of required physical qubits, while preserving
coherence and mitigating circuit-depth limitations through in-situ atom
replacement. To demonstrate MAQCY's versatility, we implement a three-qubit
quantum Fourier transform using only global operations and atom transport. We
also propose a concrete implementation using ytterbium isotopes, paving the way
toward large-scale, fault-tolerant quantum computing.

</details>


### [159] [Lagrange-Mesh Method in Momentum Space: an Alternative Formulation](https://arxiv.org/abs/2510.03015)
*Cyrille Chevalier,Joachim Viseur*

Main category: quant-ph

TL;DR: 该论文提出了一种在动量空间中使用拉格朗日-网格法计算势能矩阵元的新方法。


<details>
  <summary>Details</summary>
Motivation: 扩展拉格朗日-网格法可处理的势能范围，包括库仑和线性相互作用等先前无法处理的情况。

Method: 提出了一种新的计算势能矩阵元的方法。

Result: 该方法已在多种系统上得到验证，并特别关注了动量和位置概率密度的表示。

Conclusion: 所提出的方法能够有效地计算动量空间中的势能矩阵元，并扩展了拉格朗日-网格法的应用范围。

Abstract: This work presents a new methodology for computing potential matrix elements
within the Lagrange-mesh method in momentum space. The proposed approach
extends the range of treatable potentials to include previously inaccessible
cases, such as Coulomb and linear interactions. The method is validated across
a variety of systems. A particular attention is given to the representation of
both momentum and position probability densities.

</details>


### [160] [Non-reciprocal Synchronization in Thermal Rydberg Ensembles](https://arxiv.org/abs/2510.03024)
*Yunlong Xue,Zhengyang Bai*

Main category: quant-ph

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Optical non-reciprocity is a fundamental phenomenon in photonics. It is
crucial for developing devices that rely on directional signal control, such as
optical isolators and circulators. However, most research in this field has
focused on systems in equilibrium or steady states. In this work, we
demonstrate a room-temperature Rydberg atomic platform where the unidirectional
propagation of light acts as a switch to mediate time-crystalline-like
collective oscillations through atomic synchronization. We find that
thermal-motion-induced coupling asymmetry, enabled by counterpropagating probe
and control fields, generates persistent oscillations; conversely,
co-propagation quenches this effect. We identify, through both numerical and
analytical approaches, the criteria for realizing optical non-reciprocity
within a synchronization regime. These results provide key insights for chiral
quantum optics and promote the on-chip integration of non-reciprocal devices in
nonequilibrium many-body systems.

</details>


### [161] [Boundary Time Crystals: Beyond Mean-Field Theory](https://arxiv.org/abs/2510.03028)
*Zeping Liu,Yaotian Li,Zhaoyu Fei,Xiaoguang Wang*

Main category: quant-ph

TL;DR: 边界时间晶体是开放量子系统中自发破坏时间平移对称性的奇异耗散量子相。对于有限尺寸系统，其长期演化表现为衰减振荡，无法用平均场理论捕捉。本文提出一种有效的“频门旋转波近似”方法，该方法在强驱动下能很好地近似边界时间晶体的长期演化。通过分析该方法，我们得到了久期振荡的解析表达式，包括稳态密度算符、振荡周期和有序参数的衰减率。


<details>
  <summary>Details</summary>
Motivation: 现有平均场理论无法捕捉有限尺寸边界时间晶体长期演化中的衰减振荡现象。

Method: 提出并应用“频门旋转波近似”方法，该方法考虑了有序参数的长期衰减包络（由有效的Lindblad超算子决定）和短期振荡（由简化的量子动力学半群决定）。

Result: 发现了由三种不同方向的退相干过程竞争引起的持久振荡，并得到了稳态密度算符、振荡周期以及有序参数衰减率的解析表达式（在相干能量分裂超过耗散率的条件下）。

Conclusion: 所提出的“频门旋转波近似”是一种超越平均场理论的工具，可用于研究周期驱动开放量子系统的动力学，并理解时间晶体的形成。

Abstract: Boundary time crystals are a class of exotic dissipative quantum phases that
spontaneously break continuous time-translation symmetry in the thermodynamic
limit of open quantum systems. In finite-size systems, the long-time evolution
of boundary time crystals exhibits decaying oscillations that cannot be
captured by widely used mean-field theory. To address this issue, we develop an
effective approach called the stroboscopic rotating wave approximation, which
provides a well approximate state for the long-time evolution of boundary time
crystals under strong driving. In this approach, the order parameter exhibits
both a long-time decaying envelope governed by an effective Lindblad
superoperator and short-time oscillations dominated by a reduced quantum
dynamical semigroup. Our results reveal that the competition among dephasing
processes along three distinct directions induces persistent oscillations,
marking the emergence of the boundary time crystal phase. We obtain the
analytical expressions for the steady-state density operator, the oscillation
period, and the decay rate of the order parameter in the regime where the
coherent energy splitting exceeds the dissipation rate. Our work provides a
beyond-mean-field theoretical tool for studying the dynamics of periodically
driven open quantum systems and understanding the formation of time crystals.

</details>


### [162] [Probability distribution reconstruction using circuit cutting applied to a variational classifier](https://arxiv.org/abs/2510.03077)
*Niels M. P. Neumann,Carlos M. R. Rocha,Jasper Verbree,Marc van Vliet*

Main category: quant-ph

TL;DR: 电路剪切技术可以通过将大电路分解为小电路来解决量子资源不足的问题，并且“先剪切后训练”的方法在实践中也能取得良好效果，有助于提高结果保真度。


<details>
  <summary>Details</summary>
Motivation: 量子计算和量子软件发展迅速，但量子资源不足是运行量子算法的瓶颈。本研究旨在探索电路剪切技术在处理需要多次交互的算法时的潜力。

Method: 研究了两种电路剪切技术：一种是基于期望值的方法，另一种是基于概率分布重建的新方法。比较了“先训练后剪切”和“先剪切后训练”两种训练方法。

Result: “先剪切后训练”的方法在实践中也能产生良好的结果，并且在实现剪切和未剪切电路后，发现电路剪切有助于实现更高的保真度。

Conclusion: “先剪切后训练”的策略使得电路剪切技术在实践中更具应用价值，因为它解决了“先训练后剪切”策略在许多情况下不可行的难题。电路剪切技术有助于实现更高的保真度。

Abstract: Significant efforts are being spent on building a quantum computer. At the
same time, developments in quantum software are rapidly progressing.
Insufficient quantum resources often are the problem when running quantum
algorithms. New techniques can aid in using smaller quantum computers to run
larger quantum algorithms. One of these techniques is circuit cutting. With
this method, a circuit is broken into multiple pieces, each of which is run on
quantum hardware independently and then recombined to obtain the overall
answer. These circuit cutting techniques require additional circuit
evaluations, which can form a bottleneck for algorithms requiring many
interactions. This work explores the potential of circuit cutting techniques
precisely in this regime of many interactions. We consider two different
models, a standard method based on expectation values, and a novel method based
on probability distribution reconstruction. Next, we compare two different
training methods we call train-then-cut and cut-then-train and show that in
practice, cut-then-train still produces good results. This observation brings
closer the practical applicability of circuit cutting techniques, as a
train-then-cut strategy is often infeasible. We conclude by implementing a cut
and uncut circuit and find that circuit cutting helps achieve higher fidelity
results.

</details>


### [163] [To break, or not to break: Symmetries in adaptive quantum simulations, a case study on the Schwinger model](https://arxiv.org/abs/2510.03083)
*Karunya Shailesh Shirali,Kyle Sherbert,Yanzhu Chen,Adrien Florio,Andreas Weichselbaum,Robert D. Pisarski,Sophia E. Economou*

Main category: quant-ph

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We investigate the role of symmetries in constructing resource-efficient
operator pools for adaptive variational quantum eigensolvers. In particular, we
focus on the lattice Schwinger model, a discretized model of $1+1$ dimensional
electrodynamics, which we use as a proxy for spin chains with a continuum
limit. We present an extensive set of simulations comprising a total of $11$
different operator pools, which all systematically and independently break or
preserve a combination of discrete translations, the conservation of charge
(magnetization) and the fermionic locality of the excitations. Circuit depths
are the primary bottleneck in current quantum hardware, and we find that the
most efficient ans\"atze in the near-term are obtained by pools that
$\textit{break}$ translation invariance, conserve charge, and lead to shallow
circuits. On the other hand, we anticipate the shot counts to be the limiting
factor in future, error-corrected quantum devices; our findings suggest that
pools $\textit{preserving}$ translation invariance could be preferable for such
platforms.

</details>


### [164] [Modified logarithmic Sobolev inequalities for CSS codes](https://arxiv.org/abs/2510.03090)
*Sebastian Stengele,Ángela Capel,Li Gao,Angelo Lucia,David Pérez-García,Antonio Pérez-Hernández,Cambyse Rouzé,Simone Warzel*

Main category: quant-ph

TL;DR: 在 D 维中，对于平移不变的 Calderbank-Shor-Steane (CSS) 码的热化，我们证明了量子吉布斯态上的 Dobrushin-Shlosman 型条件可以实现具有恒定不变系统大小的对数 Sobolev 不等式，从而实现快速热化。


<details>
  <summary>Details</summary>
Motivation: 本研究的动机是理解平移不变的 Calderbank-Shor-Steane (CSS) 码的热化过程，特别是 Davies 量子半群在此过程中的作用。

Method: 通过将 Stroock、Zegarlinski、Martinelli 和 Olivieri 的经典热化结果推广到 CSS 量子设置，并证明 Dobrushin-Shlosman 型条件可以实现具有恒定不变系统大小的对数 Sobolev 不等式。

Result: 证明了 Dobrushin-Shlosman 型条件可以实现具有恒定不变系统大小的对数 Sobolev 不等式，这意味着在任何正温度下，2D 中的量子比特码和 3D 中的量子比特码的星形部分都能快速热化。

Conclusion: 本研究结果表明，量子吉布斯态上的 Dobrushin-Shlosman 型条件可以实现具有恒定不变系统大小的对数 Sobolev 不等式，从而导致 2D 和 3D 中的量子比特码及其星形部分在任何正温度下都能快速热化，并快速丢失存储的量子信息。

Abstract: We consider the class of Davies quantum semigroups modelling thermalization
for translation-invariant Calderbank-Shor-Steane (CSS) codes in D dimensions.
We prove that conditions of Dobrushin-Shlosman-type on the quantum Gibbs state
imply a modified logarithmic Sobolev inequality with a constant that is uniform
in the system's size. This is accomplished by generalizing parts of the
classical results on thermalization by Stroock, Zegarlinski, Martinelli, and
Olivieri to the CSS quantum setting. The results in particular imply the rapid
thermalization at any positive temperature of the toric code in 2D and the star
part of the toric code in 3D, implying a rapid loss of stored quantum
information for these models.

</details>


### [165] [Cheat-Penalised Quantum Weak Coin-Flipping](https://arxiv.org/abs/2510.03218)
*Atul Singh Arora,Carl A. Miller,Mauro E. S. Morales,Jamie Sikora*

Main category: quant-ph

TL;DR: 此论文研究了带有欺骗惩罚的弱币翻转协议，发现即使是很小的惩罚也能显著改变协议的效率。论文提出了新的协议，在保持空间效率的同时，显著减少了通信轮数。


<details>
  <summary>Details</summary>
Motivation: 研究带有欺骗惩罚的弱币翻转协议，以探索在有限的资源下实现近乎完美的安全性，并改善现有协议的效率问题。

Method: 首先，将点对游戏与协议的对应关系扩展到近似点对游戏、欺骗惩罚设置以及通信轮数和空间复杂度；其次，开发了首个用于构造具有高安全性和低复杂度的（近似）点对游戏的数值算法。

Result: 提出了新的弱币翻转协议，在 $\Lambda=0.01$ 的欺骗惩罚下，通信轮数可达 $10^{16}$ 轮，但比现有协议效率高 $10^7$ 倍；在相同的空间复杂度下，可以选择降低偏差（至 $1/2 + 10^{-10}$）或减少通信轮数（至 $25,180$ 轮）。

Conclusion: 通过引入欺骗惩罚和新的技术方法，显著提高了弱币翻转协议的空间效率和通信效率，为实现安全且实用的量子多方计算协议开辟了可能性。

Abstract: Coin-flipping is a fundamental task in two-party cryptography where two
remote mistrustful parties wish to generate a shared uniformly random bit.
While quantum protocols promising near-perfect security exist for weak
coin-flipping -- when the parties want opposing outcomes -- it has been shown
that they must be inefficient in terms of their round complexity, and it is an
open question of how space efficient they can be. In this work, we consider a
variant called cheat-penalised weak coin-flipping in which if a party gets
caught cheating, they lose $\Lambda$ points (compared to $0$ in the standard
definition). We find that already for a small cheating penalty, the landscape
of coin-flipping changes dramatically. For example, with $\Lambda=0.01$, we
exhibit a protocol where neither Alice nor Bob can bias the result in their
favour beyond $1/2 + 10^{-8}$, which uses $24$ qubits and $10^{16}$ rounds of
communication (provably $10^{7}$ times better than any weak coin-flipping
protocol with matching security). For the same space requirements, we
demonstrate how one can choose between lowering how much a malicious party can
bias the result (down to $1/2 + 10^{-10}$) and reducing the rounds of
communication (down to $25,180$), depending on what is preferred. To find these
protocols, we make two technical contributions. First, we extend the point
game-protocol correspondence introduced by Kitaev and Mochon, to incorporate:
(i) approximate point games, (ii) the cheat-penalised setting, and (iii) round
and space complexity. Second, we give the first (to the best of our knowledge)
numerical algorithm for constructing (approximate) point games that correspond
to high security and low complexity. Our results open up the possibility of
having secure and practical quantum protocols for multiparty computation.

</details>


### [166] [Absence of quantum Darwinism as a resource in secure quantum communication and computation](https://arxiv.org/abs/2510.03225)
*Bishal Kumar Das,Sourav Manna,Vaibhav Madhok*

Main category: quant-ph

TL;DR: 量子客场态在密码学中具有优势，并且是量子计算的经典模拟的关键缺失环节。在一个混合状态量子计算模型中，我们发现，当量子态具有零discord时，量子达尔文主义的缺失会导致计算的客观性消失，从而在密码学中产生量子优势，并使量子计算的经典模拟变得困难。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探讨量子客场态在密码学和量子计算经典模拟中的作用，特别是当量子态具有零discord且量子达尔文主义缺失时所产生的后果。

Method: 通过考虑一个混合状态量子计算模型，其中所有阶段的量子态都满足一致性（zero discord）。分析了一致性量子态的性质，并研究了量子达尔文主义在经典模拟中的作用。最后，提出了一种利用这一原理的安全量子通信协议。

Result: 研究表明，在没有量子达尔文主义的情况下，一致性量子态无法被现有方法有效地模拟，这在通信中提供了密码学优势。提出了一种利用此原理的安全量子通信协议。此外，量子达尔文主义在三个方面（经验上、信息论上和计算上）划定了量子-经典边界。

Conclusion: 量子客场态在没有量子达尔文主义的情况下，可以为密码学提供优势，并且是高效经典模拟量子电路的关键缺失环节。量子达尔文主义不仅是客观性的来源，也是划定量子-经典边界的重要因素。

Abstract: The emergence of classical world from underlying quantum mechanics is
characterized by not only vanishing quantum correlations but also an unfolding
of objectivity also known as quantum Darwinism. We show that the absence of
this objectivity has a quantum advantage in cryptography and also provides the
crucial missing link in efficient classical simulation of quantum circuits with
zero discord. For this purpose, we consider a model of mixed state quantum
computation where one is promised concordant states at all stages of the
quantum circuit. A concordant quantum state has zero discord with respect to
any part and there exists a basis made up of a tensor product of orthonormal
local subsystem basis in which the density matrix is diagonal. Efficient
classical simulation of concordant computation has surprisingly been an
outstanding question in quantum information theory. We argue that a key
ingredient of an efficient classical simulation algorithm, a knowledge of the
local basis in which the multi-party state is diagonal, is made available by
quantum Darwinism. Concordant states in the absence of quantum Darwinism cannot
be efficiently simulated by existing methods and give a cryptographic advantage
in communication. We show this by giving a protocol for secure quantum
communication that exploits this insight. Our work also has implications for
the quantum-classical border and we discuss how objectivity emerging out of
Darwinism demarcates this border in three ways - empirical based on our
observations and experience of objectivity, information theoretic due to the
absence of any quantum correlations and lastly computational in the sense
discussed above. Lastly, we show that the quantum-classical boundary as drawn
by quantum Darwinism as well by what can be simulated efficiently in a mixed
state quantum computation aligns with the boundary given by Hardy

</details>


### [167] [Plugging Leaks in Fault-Tolerant Quantum Computation and Verification](https://arxiv.org/abs/2510.03227)
*Theodoros Kapourniotis,Dominik Leichtle,Luka Music,Harold Ollivier*

Main category: quant-ph

TL;DR: 该研究提出了第一个容错的盲验证方案，用于处理量子云计算中的秘密相关噪声，并提供了可组合的安全证明。


<details>
  <summary>Details</summary>
Motivation: 量子云计算的出现使得量子委托计算的安全性至关重要，但现有的方案缺乏处理可扩展量子计算机所需的完全量子容错能力，特别是处理验证器设备上的秘密相关噪声。

Method: 提出了一种新的盲验证方案，利用两个新颖的蒸馏协议来处理秘密相关噪声：一个由验证器运行，另一个更复杂的协议由证明者运行。这些协议将秘密相关噪声转化为秘密无关噪声，从而能够验证容错的 BQP 计算。

Result: 成功提出了第一个能够处理验证器设备上秘密相关噪声的容错盲验证方案。通过使用新的蒸馏协议，实现了对任意容错 BQP 计算的指数级置信度验证。

Conclusion: 该研究为量子云计算中的安全委托量子计算提供了一个关键的解决方案，通过其提出的容错盲验证方案解决了长期存在的噪声处理难题，并为未来可扩展和安全的量子计算奠定了基础。

Abstract: With the advent of quantum cloud computing, the security of delegated quantum
computation has become of utmost importance. While multiple statistically
secure blind verification schemes in the prepare-and-send model have been
proposed, none of them achieves full quantum fault-tolerance, a prerequisite
for useful verification on scalable quantum computers. In this paper, we
present the first fault-tolerant blind verification scheme for universal
quantum computations able to handle secret-dependent noise on the verifier's
quantum device. Composable security of the proposed protocol is proven in the
Abstract Cryptography framework.
  Our main tools are two novel distillation protocols that turn
secret-dependent noise into secret-independent noise. The first one is run by
the verifier and acts on its noisy gates, while the second and more complex one
is run entirely on the prover's device and acts on states provided by the
verifier. Both are required to overcome the leakage induced by secret-dependent
noise. We use these protocols to prepare states in the X-Y-plane whose noise is
overwhelmingly secret-independent, which then allows us to verify with
exponential confidence arbitrary fault-tolerant BQP computations.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [168] [Identifying Asymptomatic Nodes in Network Epidemics using Graph Neural Networks](https://arxiv.org/abs/2510.02568)
*Conrado Catarcione Pinto,Amanda Camacho Novaes de Oliveira,Rodrigo Sapienza Luna,Daniel Ratton Figueiredo*

Main category: cs.SI

TL;DR: 该研究提出了一种基于图神经网络（GNN）的方法来识别网络流行病模型中的无症状感染者。


<details>
  <summary>Details</summary>
Motivation: 无症状感染者能够传播疾病，给公共卫生政策带来挑战。识别这些个体对于控制流行病至关重要，但广泛检测成本高昂。

Method: 采用监督学习的图神经网络（GNN）模型，从具有可观察感染者的网络中提取节点特征，以区分无症状感染者和易感者。

Result: 在不同的网络模型、网络规模和感染者可观察比例下，该方法都能准确识别无症状节点，并具有良好的泛化能力。

Conclusion: 所提出的GNN方法能够有效且稳健地识别网络模型中的无症状感染者。

Abstract: Infected individuals in some epidemics can remain asymptomatic while still
carrying and transmitting the infection. These individuals contribute to the
spread of the epidemic and pose a significant challenge to public health
policies. Identifying asymptomatic individuals is critical for measuring and
controlling an epidemic, but periodic and widespread testing of healthy
individuals is often too costly. This work tackles the problem of identifying
asymptomatic individuals considering a classic SI (Susceptible-Infected)
network epidemic model where a fraction of the infected nodes are not observed
as infected (i.e., their observed state is identical to susceptible nodes). In
order to classify healthy nodes as asymptomatic or susceptible, a Graph Neural
Network (GNN) model with supervised learning is adopted where a set of node
features are built from the network with observed infected nodes. The approach
is evaluated across different network models, network sizes, and fraction of
observed infections. Results indicate that the proposed methodology is robust
across different scenarios, accurately identifying asymptomatic nodes while
also generalizing to different network sizes and fraction of observed
infections.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [169] [Rate-Adaptive Semantic Communication via Multi-Stage Vector Quantization](https://arxiv.org/abs/2510.02646)
*Jinsung Park,Junyong Shin,Yongjeong Oh,Jihun Park,Yo-Seb Jeon*

Main category: eess.SP

TL;DR: 该论文提出了一种基于多阶段向量量化（VQ）的速率自适应语义通信框架（MSVQ-SC），通过分阶段量化和动态激活模块来提高速率适应性、降低计算复杂度和避免码本坍塌问题。


<details>
  <summary>Details</summary>
Motivation: 为了在通信速率和语义保真度之间取得更好的平衡，尤其是在不同比特约束下实现精细的速率自适应，并解决传统单阶段VQ方法在高保真度下需要指数级增大码本的局限性。

Method: 提出了一种新颖的MSVQ-SC框架，将量化过程分解为多个阶段，并能动态激活这些阶段和各个VQ模块。通过将模块选择问题公式化并使用增量分配算法来优化性能。此外，还引入了熵编码来进一步降低通信开销。

Result: 在CIFAR-10数据集上的仿真结果表明，MSVQ-SC框架在语义保真度、计算复杂度和速率控制灵活性方面优于现有的数字语义通信方法。

Conclusion: MSVQ-SC框架能够提供优越的语义保真度，同时降低计算复杂性，并实现灵活、精细的速率控制，在速率自适应语义通信领域具有显著优势。

Abstract: This paper proposes a novel framework for rate-adaptive semantic
communication based on multi-stage vector quantization (VQ), termed
\textit{MSVQ-SC}. Unlike conventional single-stage VQ approaches, which require
exponentially larger codebooks to achieve higher fidelity, the proposed
framework decomposes the quantization process into multiple stages and
dynamically activates both stages and individual VQ modules. This design
enables fine-grained rate adaptation under varying bit constraints while
mitigating computational complexity and the codebook collapse problem. To
optimize performance, we formulate a module selection problem that minimizes
task loss subject to a rate constraint and solve it using an incremental
allocation algorithm. Furthermore, we extend the framework by incorporating
entropy coding to exploit non-uniform codeword distributions, further reducing
communication overhead. Simulation results on the CIFAR-10 dataset demonstrate
that the proposed framework outperforms existing digital semantic communication
methods, achieving superior semantic fidelity with lower complexity while
providing flexible and fine-grained rate control.

</details>


### [170] [Mutual Information-Driven Visualization and Clustering for Core KPI Selection in O-RAN Testing](https://arxiv.org/abs/2510.02696)
*Anish Pradhan,Lingjia Liu,Harpreet S. Dhillon*

Main category: eess.SP

TL;DR: AMIF-MDS利用聚合互信息和多维缩放来简化O-RAN测试，识别性能测量之间的依赖关系，并确定核心性能指标。


<details>
  <summary>Details</summary>
Motivation: O-RAN系统日益复杂，性能测量数量呈指数增长，使得测试困难。需要识别性能测量之间的依赖关系以简化测试并改进系统设计。

Method: 提出AMIF-MDS方法，使用聚合互信息（AMIF）作为有向信息（DI）的代理来量化相似性，并结合多维缩放（MDS）可视化序列依赖性。采用基于分位数的AMIF估计器处理O-RAN时间序列数据，并通过DBSCAN聚类识别相互关联的指标，最终确定核心性能指标集。

Result: AMIF-MDS能够识别O-RAN时间序列测试数据中不同性能测量之间的依赖关系。DBSCAN聚类将相互关联的指标分组，并揭示了链路自适应指示器等关联，最终确定了一组核心性能指标。

Conclusion: AMIF-MDS是一种有效的方法，可以通过识别和分组相互关联的性能测量指标来简化O-RAN测试。确定的核心性能指标集可用于未来驱动式O-RAN测试，从而提高效率和系统设计。

Abstract: O-RAN testing is becoming increasingly difficult with the exponentially
growing number of performance measurements as the system grows more complex,
with additional units, interfaces, applications, and possible implementations
and configurations. To simplify the testing procedure and improve system design
for O-RAN systems, it is important to identify the dependencies among various
performance measurements, which are inherently time-series and can be modeled
as realizations of random processes. While information theory can be utilized
as a principled foundation for mapping these dependencies, the robust
estimation of such measures for random processes from real-world data remains
challenging. This paper introduces AMIF-MDS, which employs aggregate mutual
Information in frequency (AMIF), a practical proxy for directed information
(DI), to quantify similarity and visualize inter-series dependencies with
multidimensional scaling (MDS). The proposed quantile-based AMIF estimator is
applied to O-RAN time-series testing data to identify dependencies among
various performance measures so that we can focus on a set of ``core''
performance measures. Applying density-based spatial clustering of applications
with noise (DBSCAN) to the MDS embedding groups mutually informative metrics,
organically reveals the link-adaptation indicators among other clusters, and
yields a ``core'' performance measure set for future learning-driven O-RAN
testing.

</details>


### [171] [Denoising and Augmentation: A Dual Use of Diffusion Model for Enhanced CSI Recovery](https://arxiv.org/abs/2510.02744)
*Yupeng Li,Ruhao Zhang,Yitong Liu,Chunju Shao,Jing Jin,Shijian Gao*

Main category: eess.SP

TL;DR: 该研究提出了一种基于扩散模型的信道估计算法，通过数据去噪和数据增强来提高精度并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有信道估计算法在处理高噪声信号时精度受损，且深度学习模型训练需要大量数据。本研究旨在解决这些问题。

Method: 提出了一种集成数据去噪和数据增强的DDPM（去噪扩散概率模型）信道估计算法。去噪部分在无先验信息的情况下清理带噪声的信号；数据增强部分通过调整反向步骤生成新的信道数据来扩充训练集。此外，提出了一种分段前向策略来处理不同的信噪比（SNR）。

Result: 与现有基准相比，该方法在精度和计算成本之间取得了更好的折衷。

Conclusion: 基于DDPM的信道估计算法在噪声环境下能有效提高精度，并通过数据增强和优化的训练策略降低了计算复杂度。

Abstract: This letter introduces a dual application of denoising diffusion
probabilistic model (DDPM)-based channel estimation algorithm integrating data
denoising and augmentation. Denoising addresses the severe noise in raw signals
at pilot locations, which can impair channel estimation accuracy. An
unsupervised structure is proposed to clean field data without prior knowledge
of pure channel information. Data augmentation is crucial due to the
data-intensive nature of training deep learning (DL) networks for channel state
information (CSI) estimation. The network generates new channel data by
adjusting reverse steps, enriching the training dataset. To manage varying
signal-to-noise ratios (SNRs) in communication data, a piecewise forward
strategy is proposed to enhance the DDPM convergence precision. The link-level
simulations indicate that the proposed scheme achieves a superior tradeoff
between precision and computational cost compared to existing benchmarks.

</details>


### [172] [Neyman Pearson Detector for Multiple Ambient Backscatter Zero-Energy-Devices Beacons using Near-Perfect Code](https://arxiv.org/abs/2510.02785)
*Shanglin Yang,Jean-Marie Gorce,Muhammad Jehangir Khan,Dinh-Thuy Phan-Huy,Guillaume Villemaud*

Main category: eess.SP

TL;DR: 本论文提出了一种基于近完美码（NPC）的同步序列的检测方法，用于在存在干扰和同步不确定性的情况下，从环境回溯散射系统中检测多个共存的零能耗设备（ZED）。


<details>
  <summary>Details</summary>
Motivation: 在现有的基于零能耗设备（ZED）的室内定位系统中，存在对多个共存ZED进行检测的挑战，尤其是在存在干扰和同步不确定性的情况下。

Method: 提出了一种基于Neyman-Pearson（NP）公式的改进检测器，该检测器使用近完美码（NPC）作为同步序列，以提高不同ZED信号的可分离性。该方案用双相关器代替了双带通滤波，并结合了贝叶斯检测、对比度指标和多频组合技术，以实现对次要标签的检测和控制虚警率。

Result: 在CorteXlab测试台上进行的实验表明，该方法在低信噪比（SNR）下具有鲁棒性，并将峰值旁瓣比（PSL）从约11 dB提高到约22 dB。

Conclusion: 所提出的方法能够有效检测多个共存的ZED，提高了环境回溯散射定位系统的可扩展性和可靠性，适用于实际的多标签环境。

Abstract: Recently, a novel ultra-low-power indoor localization system based on
Zero-Energy Devices (ZEDs) has shown promising results in ambient backscatter
communication. In this paper, we study detection of multiple coexisting ZEDs in
ambient backscatter systems under interference and synchronization uncertainty.
Building on a Neyman-Pearson (NP) formulation previously applied to single-tag
detection, we introduce a detector tailored to multi-tag scenarios. The core
idea is to use a Near-Perfect Code (NPC) as the synchronization sequence, which
substantially improves the peak-to-sidelobe (PSL) ratio and thus separability
among concurrent tags. The proposed scheme replaces dual band-pass filtering
with dual correlators, enabling an explicit Bayesian detector and tight control
of the false-alarm rate; we further incorporate a contrast metric and
multi-frequency combining to reveal secondary tags. Experiments on the
CorteXlab testbed (part of the SLICES-EU infrastructure) confirm robustness at
low SNR, with observed PSL improvements from about 11 dB to about 22 dB. These
results advance scalable, reliable ambient backscatter localization in
practical multi-tag environments.

</details>


### [173] [Pioneering Scalable Prototyping for Mid-Band XL-MIMO Systems: Design and Implementation](https://arxiv.org/abs/2510.02793)
*Jiachen Tian,Yu Han,Zhengtao Jin,Xi Yang,Jie Yang,Wankai Tang,Xiao Li,Wenjin Wang,Shi Jin*

Main category: eess.SP

TL;DR: 该论文介绍了一个实时中频超大规模MIMO（XL-MIMO）系统原型，旨在验证理论优势并应对标准化挑战。


<details>
  <summary>Details</summary>
Motivation: 中频段结合XL-MIMO被视为未来通信系统的关键，能提升吞吐量并支持通信感知一体化，但缺乏实际系统验证。

Method: 设计并实现了一个实时中频XL-MIMO原型系统，采用新颖架构，支持200MHz带宽、1024天线单元、256收发链，并能在TDD模式下支持12个用户，基于SDR平台，具有可编程性和模块化设计。

Result: 实验结果显示，系统实现了1167.85 Gbps的实时数字采样处理速率，12个用户峰值数据吞吐量达15.81 Gbps，以及接近80 bit/s/Hz的频谱效率。

Conclusion: 该原型系统成功验证了中频XL-MIMO的性能优势，为标准化奠定了基础，并展示了其在可扩展性和未来应用方面的潜力。

Abstract: The mid-band frequency range, combined with extra large-scale multiple-input
multiple-output (XL-MIMO), is emerging as a key enabler for future
communication systems. Thanks to the advent of new spectrum resources and
degrees of freedom brought by the near-field propagation, the mid-band XL-MIMO
system is expected to significantly enhance throughput and inherently support
advanced functionalities such as integrated sensing and communication. Although
theoretical studies have highlighted the benefits of mid-band XL-MIMO systems,
the promised performance gains have yet to be validated in practical systems,
posing a major challenge to the standardization. In this paper, preliminaries
are first discussed, followed by an analysis of key challenges in constructing
a real-time prototype system. Subsequently, the design and implementation of a
real-time mid-band XL-MIMO prototype system are presented. Benefiting from the
novel architecture, the proposed prototype system supports metrics aligned with
standardization, including a bandwidth of 200 MHz, up to 1024 antenna elements,
and up to 256 transceiver chains. Operating in time-division duplexing (TDD)
mode, the prototype enables multiuser communication with support for up to 12
users, while retaining standard communication procedures. Built on
software-defined radio (SDR) platforms, the system is programmable and allows
for flexible deployment of advanced algorithms. Moreover, the modular
architecture ensures high scalability, making the system adaptable to various
configurations, including distributed deployments and decentralized signal
processing. Experimental results with the proposed prototype system demonstrate
real-time digital sample processing at 1167.85 Gbps, a peak data throughput of
15.81 Gbps for 12 users, and a maximal spectral efficiency approaching 80
bit/s/Hz.

</details>


### [174] [Integrated Sensing, Communication, and Positioning in Cellular Vehicular Networks](https://arxiv.org/abs/2510.02939)
*Xin Tong,Zhaoyang Zhang,Yuzhi Yang,Yu Ge,Zhaohui Yang,Henk Wymeersch,Mérouane Debbah*

Main category: eess.SP

TL;DR: 提出了一种新的集成传感与通信（ISAC）框架，用于在蜂窝车联网中同时实现数据通信、车辆定位和环境传感。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在解决在蜂窝车联网中同时实现数据通信、车辆定位和环境传感的需求，并提出了一种新颖的ISAC框架。

Method: 通过将车辆定位问题纳入现有的基于计算成像的ISAC模型，构建了一个特殊的集成传感、通信和定位问题，其中未知数高度耦合。为了缓解秩亏缺并使其可解，将感兴趣区域（ROI）离散化为传感和定位像素，并利用视线（LOS）和非视线（NLOS）传播。该问题被转化为一个多项式双线性压缩感知（CS）重建问题，并采用交替优化（AO）算法进行求解，以迭代地实现符号检测、车辆定位和环境传感。

Result: 该方法通过离散化ROI、利用LOS和NLOS传播，并将问题转化为多项式双线性CS重建问题，最后使用AO算法进行求解，实现了通信、定位和传感的集成。性能分析和数值结果证明了该方法的有效性。

Conclusion: 所提出的ISAC框架能够有效地同时实现数据通信、车辆定位和环境传感，为车联网应用提供了新的解决方案。

Abstract: In this correspondence, a novel integrated sensing and communication (ISAC)
framework is proposed to accomplish data communication, vehicle positioning,
and environment sensing simultaneously in a cellular vehicular network. By
incorporating the vehicle positioning problem with the existing
computational-imaging-based ISAC models, we formulate a special integrated
sensing, communication, and positioning problem in which the unknowns are
highly coupled. To mitigate the rank deficiency and make it solvable, we
discretize the region of interest (ROI) into sensing and positioning pixels
respectively, and exploit both the line-of-sight and non-line-of-sight
propagation of the vehicles' uplink access signals. The resultant problem is
shown to be a polynomial bilinear compressed sensing (CS) reconstruction
problem, which is then solved by the alternating optimization (AO) algorithm to
iteratively achieve symbol detection, vehicle positioning and environment
sensing. Performance analysis and numerical results demonstrate the
effectiveness of the proposed method.

</details>


### [175] [Towards Electrophysiological and Histological Mapping of Upper Limb Nerves in Pigs Using Epineural Stimulation](https://arxiv.org/abs/2510.02979)
*Jonathan Baum,Chamot-Nonin Manon,Oppelt Vera,David Guiraud,Christine Azevedo Coste,Thomas Guiho*

Main category: eess.SP

TL;DR: 本研究通过猪体实验，结合神经电生理和组织学分析，探讨了神经解剖结构与电刺激功能结果的关系，旨在优化神经接口设计。


<details>
  <summary>Details</summary>
Motivation: 理解神经解剖与电刺激功能结果间的关系对于优化神经接口设计至关重要。

Method: 研究人员在四只猪的上肢神经周围放置了带有多个触点的硬膜外袖带电极，并施加了先前通过计算研究确定的电刺激配置。记录了目标肌肉产生的诱发电肌电图（EMG）响应，提取并分析了肌肉募集曲线以量化激活模式。随后，对刺激的神经进行了组织学分析，以观察束状组织结构和分布。本工作展示了对一只动物的肌肉激活分布和束状解剖结构联合分析的初步结果。

Result: 本研究初步结合了肌肉激活模式和束状解剖结构进行了分析。

Conclusion: 研究结果旨在通过将电极配置与选择性肌肉募集联系起来，为刺激策略的设计提供依据，最终为更有效的神经调控和神经假体应用做出贡献。

Abstract: Understanding the relationship between nerve anatomy and the functional
outcomes of electrical stimulation is critical for optimizing neural interface
design. In this study, we conducted acute experiments on four pigs in which
epineural cuff electrodes with multiple contacts were placed around upper limb
nerves. A subset of electrical stimulation configurations -- previously
identified via computational study -- was applied, and the resulting evoked
electromyographic (EMG) responses were recorded from target muscles. Muscle
recruitment curves were extracted and analysed offline to quantify activation
patterns. Following the electrophysiological experiments, the stimulated nerves
were harvested and processed for histological analysis to visualize fascicular
organization and distribution. This work presents preliminary results from the
combined analysis of muscle activation profiles and fascicle anatomy in one
animal. Our findings aim to inform the design of stimulation strategies by
linking electrode configuration to selective muscle recruitment, ultimately
contributing to more effective neuromodulation and neuroprosthetic
applications.

</details>


### [176] [Physics-Constrained Inc-GAN for Tunnel Propagation Modeling from Sparse Line Measurements](https://arxiv.org/abs/2510.03019)
*Yang Zhou,Haochang Wu,Yunxi Mu,Hao Qin,Xinyue Zhang,Xingqi Zhang*

Main category: eess.SP

TL;DR: 提出一种基于生成对抗网络的模型（Inc-GAN）来预测高铁隧道内的电场分布，以提高通信系统的可靠性。


<details>
  <summary>Details</summary>
Motivation: 高铁隧道通信系统需要可靠的无线电波传播预测以确保运行安全，但传统方法计算复杂且难以处理稀疏的实测数据。

Method: 提出一种结合了Inception网络和生成对抗网络（GAN）的模型（Inc-GAN），利用实测的稀疏测量线数据来重建隧道截面的完整电场分布。

Result: 通过数值模拟验证，Inc-GAN能够根据实测数据准确预测电场，并显著提高了计算效率。

Conclusion: Inc-GAN为基于实际运行数据优化铁路通信系统提供了一种新的解决方案。

Abstract: High-speed railway tunnel communication systems require reliable radio wave
propagation prediction to ensure operational safety. However, conventional
simulation methods face challenges of high computational complexity and
inability to effectively process sparse measurement data collected during
actual railway operations. This letter proposes an inception-enhanced
generative adversarial network (Inc-GAN) that can reconstruct complete electric
field distributions across tunnel cross-sections using sparse value lines
measured during actual train operations as input. This directly addresses
practical railway measurement constraints. Through an inception-based generator
architecture and progressive training strategy, the method achieves robust
reconstruction from single measurement signal lines to complete field
distributions. Numerical simulation validation demonstrates that Inc-GAN can
accurately predict electric fields based on measured data collected during
actual train operations, with significantly improved computational efficiency
compared to traditional methods, providing a novel solution for railway
communication system optimization based on real operational data.

</details>


### [177] [Compressed Multiband Sensing in FR3 Using Alternating Direction Method of Multipliers](https://arxiv.org/abs/2510.03055)
*Dexin Wang,Isha Jariwala,Ahmad Bazzi,Sundeep Rangan,Theodore S. Rappaport,Marwa Chafii*

Main category: eess.SP

TL;DR: ADMM-CMS是一种新颖的多频段感知框架，通过ADMM算法优化压缩感知问题，提高了空间分辨率和降噪能力，并降低了延迟误差，是6G ISAC的有效实现方式。


<details>
  <summary>Details</summary>
Motivation: 在多径丰富的信道中，对用户和散射体进行联合检测和定位对于6G的ISAC至关重要。现有方法存在局限性。

Method: 提出了一种交替方向乘子法（ADMM）辅助的压缩多频段感知（CMS）框架（ADMM-CMS），并开发了一种自适应ADMM算法来解决CMS问题。

Result: ADMM-CMS相比Bartlett类波束成形，实现了更高的空间分辨率和降噪能力，在实现0.9成功恢复概率（SRP）时，每天线发射功率增益为34 dB。与单独对7 GHz和10 GHz子频段进行压缩感知相比，ADMM-CMS在-41 dBm每天线发射功率下，延迟均方根误差分别降低了35%和38.1%，并提高了SRP。

Conclusion: ADMM-CMS能够有效实现6G系统在频段3（FR3，7-24 GHz）下的ISAC。

Abstract: Joint detection and localization of users and scatterers in multipath-rich
channels on multiple bands is critical for integrated sensing and communication
(ISAC) in 6G. Existing multiband sensing methods are limited by classical
beamforming or computationally expensive approaches. This paper introduces
alternating direction method of multipliers (ADMM)-assisted compressed
multiband sensing (CMS), hereafter referred to as ADMM-CMS, which is a novel
framework for multiband sensing using uplink QAM-modulated pilot symbols. To
solve the CMS problem, we develop an adaptive ADMM algorithm that adjusts to
noise and ensures automatic stopping if converged. ADMM combines the
decomposability of dual ascent with the robustness of augmented Lagrangian
methods, making it suitable for large-scale structured optimization.
Simulations show that ADMM-CMS achieves higher spatial resolution and improved
denoising compared to Bartlett-type beamforming, yielding a 34 dB gain in
per-antenna transmit power for achieving a 0.9 successful recovery probability
(SRP). Moreover, compared to performing compressed sensing separately on the
constituent 7 GHz and 10 GHz sub-bands, ADMM-CMS achieves reductions in delay
root mean squared error of 35% and 38.1%, respectively, at -41 dBm per-antenna
transmit power, while also yielding improved SRP. Our findings demonstrate
ADMM-CMS as an efficient enabler of ISAC in frequency range 3 (FR3, 7-24 GHz)
for 6G systems.

</details>


### [178] [A Study of Neural Polar Decoders for Communication](https://arxiv.org/abs/2510.03069)
*Rom Hirsch,Ziv Aharoni,Henry D. Pfister,Haim H. Permuter*

Main category: eess.SP

TL;DR: 本研究将神经极性解码器（NPD）应用于端到端通信系统，特别是在OFDM和单载波系统上，并在5G环境下实现了性能的提升，尤其在低速率和短块长场景下表现更优，同时支持无需导频和循环前缀的传输。


<details>
  <summary>Details</summary>
Motivation: 旨在使用神经极性解码器（NPD）来改进端到端的通信系统，特别是将其从合成通道扩展到实际的通信场景，以满足实际系统需求并提高性能。

Method: 将NPD适配到OFDM和单载波通信系统，通过速率匹配支持任意码长，并引入高阶调制以适应多样化的信道条件。NPD直接利用具有记忆的信道结构进行操作，无需导频和循环前缀。

Result: 在5G信道上，NPD在比特错误率（BER）、块错误率（BLER）和吞吐量方面持续优于5G极性解码器，尤其在低速率和短块长配置下优势更明显。将NPD应用于单载波系统，其性能可与OFDM媲美，同时具有更低的峰均功率比（PAPR）。

Conclusion: NPD是一种高性能、无需导频且鲁棒的解码解决方案，能够有效应用于实际通信系统，并在5G环境中提供优于现有方法的性能。

Abstract: In this paper, we adapt and analyze Neural Polar Decoders (NPDs) for
end-to-end communication systems. While prior work demonstrated the
effectiveness of NPDs on synthetic channels, this study extends the NPD to
real-world communication systems. The NPD was adapted to complete OFDM and
single-carrier communication systems. To satisfy practical system requirements,
the NPD is extended to support any code length via rate matching, higher-order
modulations, and robustness across diverse channel conditions. The NPD operates
directly on channels with memory, exploiting their structure to achieve higher
data rates without requiring pilots and a cyclic prefix. Although NPD entails
higher computational complexity than the standard 5G polar decoder, its neural
network architecture enables an efficient representation of channel statistics,
resulting in manageable complexity suitable for practical systems. Experimental
results over 5G channels demonstrate that the NPD consistently outperforms the
5G polar decoder in terms of BER, BLER, and throughput. These improvements are
particularly significant for low-rate and short-block configurations, which are
prevalent in 5G control channels. Furthermore, NPDs applied to single-carrier
systems offer performance comparable to OFDM with lower PAPR, enabling
effective single-carrier transmission over 5G channels. These results position
the NPD as a high-performance, pilotless, and robust decoding solution.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [179] [Deceptive Planning Exploiting Inattention Blindness](https://arxiv.org/abs/2510.02714)
*Mustafa O. Karabag,Jesse Milzman,Ufuk Topcu*

Main category: cs.GT

TL;DR: 这是一个关于在存在感知约束的情况下，研究有理性忽视的决策制定的摘要。


<details>
  <summary>Details</summary>
Motivation: 该研究的动机是理解在代理具有感知约束时，不准确的先验信念或对他人模型的误解可能如何导致“注意力盲视”，即代理没有意识到其信念的不正确性。

Method: 研究将感知约束建模为在线传感器选择问题，开发了一个捕获理性忽视的、值加权的传感器选择目标函数，并针对此单调目标函数提出了贪婪算法。当对手不偏离假定策略时，该目标函数提供了与对手安全策略相比的预期价值损失上限。此外，研究还提出了一个对手的近视决策算法，以利用对手的信念，通过偏离假定策略来改进安全价值。

Result: 数值示例表明，对手（Player 1）持续选择与其先验信息一致的传感器，使得对手（Player 2）能够系统性地利用其注意力盲视。

Conclusion: 研究揭示了在感知约束下，有理性忽视可能导致代理无法识别自身错误的信念，从而被对手利用。

Abstract: We study decision-making with rational inattention in settings where agents
have perception constraints. In such settings, inaccurate prior beliefs or
models of others may lead to inattention blindness, where an agent is unaware
of its incorrect beliefs. We model this phenomenon in two-player zero-sum
stochastic games, where Player 1 has perception constraints and Player 2
deceptively deviates from its security policy presumed by Player 1 to gain an
advantage. We formulate the perception constraints as an online sensor
selection problem, develop a value-weighted objective function for sensor
selection capturing rational inattention, and propose the greedy algorithm for
selection under this monotone objective function. When Player 2 does not
deviate from the presumed policy, this objective function provides an upper
bound on the expected value loss compared to the security value where Player 1
has perfect information of the state. We then propose a myopic decision-making
algorithm for Player 2 to exploit Player 1's beliefs by deviating from the
presumed policy and, thereby, improve upon the security value. Numerical
examples illustrate how Player 1 persistently chooses sensors that are
consistent with its priors, allowing Player 2 to systematically exploit its
inattention.

</details>


### [180] [Reach together: How populations win repeated games](https://arxiv.org/abs/2510.02984)
*Nathalie Bertrand,Patricia Bouyer,Luc Lapointe,Corto Mascle*

Main category: cs.GT

TL;DR: 在一个参数化的重复博弈中，我们研究了具有任意大小的玩家群体，他们的效用函数编码了可达性目标。我们使用代数工具来解决是否存在一个统一的联盟策略，使他们能够独立于群体规模而获胜的问题。我们展示了一个有限的半群，其元素总结了在有限区间内的策略，并将获胜策略的存在性与该半群中的特定元素的存在性联系起来。最后，我们提供了匹配的复杂性下界，得出结论：具有可达性目标的重复博弈是PSPACE-complete的。


<details>
  <summary>Details</summary>
Motivation: 在重复博弈中，玩家在每个步骤中同时选择动作。我们考虑一个参数化的重复博弈设置，其中玩家构成一个任意大小的群体。他们的效用函数编码了一个可达性目标。问题在于是否存在一个统一的联盟策略，使玩家能够独立于群体规模而确保获胜。

Method: 使用代数工具，首先构建一个有限半群，其中元素总结了在有限区间内的策略。然后，通过特定元素的存在性来表征获胜策略的存在性。

Result: 该问题可以在多项式空间内解决。我们得到了一个匹配的复杂性下界。

Conclusion: 具有可达性目标的重复博弈是PSPACE-complete的。

Abstract: In repeated games, players choose actions concurrently at each step. We
consider a parameterized setting of repeated games in which the players form a
population of an arbitrary size. Their utility functions encode a reachability
objective. The problem is whether there exists a uniform coalition strategy for
the players so that they are sure to win independently of the population size.
We use algebraic tools to show that the problem can be solved in polynomial
space. First we exhibit a finite semigroup whose elements summarize strategies
over a finite interval of population sizes. Then, we characterize the existence
of winning strategies by the existence of particular elements in this
semigroup. Finally, we provide a matching complexity lower bound, to conclude
that repeated population games with reachability objectives are
PSPACE-complete.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [181] [ElasticMoE: An Efficient Auto Scaling Method for Mixture-of-Experts Models](https://arxiv.org/abs/2510.02613)
*Gursimran Singh,Timothy Yu,Haley Li,Cheng Chen,Hanieh Sadri,Qintao Zhang,Yu Zhang,Ying Xiong,Yong Zhang,Zhenan Fan*

Main category: cs.DC

TL;DR: MoE LLM 的弹性服务因其并行化推理而面临挑战。ElasticMoE 提出了一种通过解耦执行和内存操作、重用权重和 KV 缓存以及进行无中断的专家重路由来实现细粒度、低延迟、零停机时间扩展的框架。


<details>
  <summary>Details</summary>
Motivation: 现有的 MoE LLM 弹性扩展策略（如水平扩展和垂直扩展）存在粒度粗、延迟高、成本高或需要停机重启等缺点，不适用于云环境中常见的突发性、短时流量模式。

Method: ElasticMoE 框架通过以下方式实现弹性扩展：1. 解耦推理执行和内存操作，使扩展步骤与服务并行进行。2. 引入 HBM 管理模块 (HMM)，通过零拷贝重映射重用权重和 KV 缓存。3. 利用高带宽点对点传输技术，在不中断服务的情况下使新添加的加速器上线。4. 采用基于虚拟内存的专家重分配机制，在重配置专家并行性期间迁移 MoE 专家，避免了昂贵的缓冲区重新分配，从而降低了峰值内存使用率。

Result: 在 Ascend NPU 上对三种流行的 MoE LLM 进行的评估显示，与基线方法相比，ElasticMoE 的扩展延迟最多可降低 9 倍，扩展期间吞吐量最多可提高 2 倍，并显著提高了 SLO 的达成率。

Conclusion: ElasticMoE 通过实现细粒度、并发的、干扰最小的扩展，解决了 MoE LLM 在动态云环境中部署的实际挑战，提高了大规模 MoE LLM 在云环境中的实用性。

Abstract: Mixture-of-Experts (MoE) models promise efficient scaling of large language
models (LLMs) by activating only a small subset of experts per token, but their
parallelized inference pipelines make elastic serving challenging. Existing
strategies fall short: horizontal scaling provisions entire replicas of the
current configuration, often tens to hundreds of accelerators, leading to
coarse granularity, long provisioning delays, and costly overprovisioning.
Vertical scaling offers finer adjustments but typically requires instance
restarts, incurring downtime. These limitations make current approaches
ill-suited for the bursty, short-lived traffic patterns common in cloud
deployments.
  We present ElasticMoE, an elastic scaling framework for MoE LLMs that
achieves fine-grained, low-latency, and zero-downtime scaling. ElasticMoE
decouples inference execution from memory operations, enabling scaling steps to
proceed concurrently with serving. An HBM Management Module (HMM) reuses
weights and KV caches via zero-copy remapping, while high-bandwidth
peer-to-peer transfers bring newly added accelerators online without
interrupting service. A virtual memory based expert redistribution mechanism
migrates MoE experts without costly buffer reallocations, reducing peak memory
usage during expert parallelism reconfiguration.
  Our evaluation on Ascend NPUs with three popular MoE LLMs shows that
ElasticMoE achieves up to 9x lower scale-up latency, up to 2x better throughput
during scaling, and significantly improves SLO attainment compared to
baselines. By enabling fine-grained, concurrent scaling with minimal
disruption, ElasticMoE advances the practicality of deploying massive MoE LLMs
in dynamic cloud environments.

</details>


### [182] [GRNND: A GPU-Parallel Relative NN-Descent Algorithm for Efficient Approximate Nearest Neighbor Graph Construction](https://arxiv.org/abs/2510.02774)
*Xiang Li,Qiong Chang,Yun Li,Jun Miyazaki*

Main category: cs.DC

TL;DR: RNN-Descent 算法在构建稀疏近似最近邻（ANN）图方面表现优异，但其图构建过程在大规模和高维数据下变得耗时。本文提出了 GRNND，一种首个 GPU 并行算法，通过无序邻居传播策略、warp 级协同操作和双缓冲邻居池来优化图构建，有效克服了同步更新陷阱，提高了并行效率和内存访问效率。实验证明 GRNND 在速度上远超现有 CPU 和 GPU 方法。


<details>
  <summary>Details</summary>
Motivation: RNN-Descent 算法在处理大规模和高维数据时，图构建过程的计算复杂度急剧增加，导致耗时过长，影响后续查询处理。

Method: 提出 GRNND，一种利用 GPU 架构的并行 RNN-Descent 算法。核心技术包括：1. 无序邻居传播策略：解决同步更新陷阱，增强结构多样性，避免过早收敛。2. Warp 级协同操作：优化内存访问。3. 双缓冲邻居池（固定容量）：消除内存访问冲突，实现高度并行化的邻居更新。

Result: GRNND 在大规模和高维数据集上显著优于现有的 CPU 和 GPU 方法，与现有 GPU 方法相比，速度提升 2.4 至 51.7 倍；与 CPU 方法相比，速度提升 17.8 至 49.8 倍。

Conclusion: GRNND 是首个 GPU 并行 RNN-Descent 算法，通过创新的并行策略有效解决了 RNN-Descent 在大规模和高维数据下的性能瓶颈，实现了显著的速度提升，是 ANN 图构建领域的重要进展。

Abstract: Relative Nearest Neighbor Descent (RNN-Descent) is a state-of-the-art
algorithm for constructing sparse approximate nearest neighbor (ANN) graphs by
combining the iterative refinement of NN-Descent with the edge-pruning rules of
the Relative Neighborhood Graph (RNG). It has demonstrated strong effectiveness
in large-scale search tasks such as information retrieval and related tasks.
However, as the amount and dimensionality of data increase, the complexity of
graph construction in RNN-Descent rises sharply, making this stage increasingly
time-consuming and even prohibitive for subsequent query processing. In this
paper, we propose GRNND, the first GPU-parallel algorithm of RNN-Descent
designed to fully exploit GPU architecture. GRNND introduces a disordered
neighbor propagation strategy to mitigate synchronized update traps, enhancing
structural diversity, and avoiding premature convergence during parallel
execution. It also leverages warp-level cooperative operations and a
double-buffered neighbor pool with fixed capacity for efficient memory access,
eliminate contention, and enable highly parallelized neighbor updates.
Extensive experiments demonstrate that GRNND consistently outperforms existing
CPU- and GPU-based methods. GRNND achieves 2.4 to 51.7x speedup over existing
GPU methods, and 17.8 to 49.8x speedup over CPU methods.

</details>


### [183] [TridentServe: A Stage-level Serving System for Diffusion Pipelines](https://arxiv.org/abs/2510.02838)
*Yifei Xia,Fangcheng Fu,Hao Yuan,Hanke Zhang,Xupeng Miao,Yijun Liu,Suhan Ling,Jie Jiang,Bin Cui*

Main category: cs.DC

TL;DR: TridentServe是一个新的扩散模型服务系统，通过动态调整资源分配来提高效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散模型服务系统在资源分配上存在静态、手动和管道级别的问题，导致效率低下。

Method: 提出了一种动态的、阶段级别的服务范例，并开发了TridentServe系统，该系统可以动态地确定管道部署和请求处理的放置和调度计划。

Result: TridentServe在不同的工作负载下，将服务质量达成率提高了3.6倍/4.1倍，并将平均/P95延迟降低了2.5倍。

Conclusion: TridentServe通过动态和自动化的资源分配，显著提高了扩散模型的服务效率和性能。

Abstract: Diffusion pipelines, renowned for their powerful visual generation
capabilities, have seen widespread adoption in generative vision tasks (e.g.,
text-to-image/video). These pipelines typically follow an
encode--diffuse--decode three-stage architecture. Current serving systems
deploy diffusion pipelines within a static, manual, and pipeline-level
paradigm, allocating the same resources to every request and stage. However,
through an in-depth analysis, we find that such a paradigm is inefficient due
to the discrepancy in resource needs across the three stages of each request,
as well as across different requests. Following the analysis, we propose the
dynamic stage-level serving paradigm and develop TridentServe, a brand new
diffusion serving system. TridentServe automatically, dynamically derives the
placement plan (i.e., how each stage resides) for pipeline deployment and the
dispatch plan (i.e., how the requests are routed) for request processing,
co-optimizing the resource allocation for both model and requests. Extensive
experiments show that TridentServe consistently improves SLO attainment and
reduces average/P95 latencies by up to 2.5x and 3.6x/4.1x over existing works
across a variety of workloads.

</details>


### [184] [On the energy efficiency of sparse matrix computations on multi-GPU clusters](https://arxiv.org/abs/2510.02878)
*Massimo Bernaschi,Alessandro Celestini,Pasqua D'Ambra,Giorgio Richelli*

Main category: cs.DC

TL;DR: 我们研究了一个用于稀疏矩阵并行计算的库的能源效率，该库利用GPU加速器来解决大规模科学问题，并针对多GPU使用进行了优化。研究表明，优化GPU计算和减少数据移动可以降低解决问题的时间和能耗，并且该库在标准基准测试中优于其他软件框架。


<details>
  <summary>Details</summary>
Motivation: 解决超出单节点内存容量的大规模稀疏线性系统的计算问题，同时满足HPC平台日益增长的可持续性要求。

Method: 开发了能够最大化并行性并针对多GPU使用进行优化的算法；实现了准确测量库核心组件运行时能耗的方法和工具。

Result: 在大型NVIDIA GPU系统上，该库在性能上优于最先进的解决方案；通过能量分析，证明了优化GPU计算和最小化数据移动可以同时减少解决问题的时间和能耗；在标准基准测试中，该库相比其他软件框架具有显著优势。

Conclusion: 优化GPU计算和最小化数据移动是提高计算效率和降低能耗的关键。

Abstract: We investigate the energy efficiency of a library designed for parallel
computations with sparse matrices. The library leverages high-performance,
energy-efficient Graphics Processing Unit (GPU) accelerators to enable
large-scale scientific applications. Our primary development objective was to
maximize parallel performance and scalability in solving sparse linear systems
whose dimensions far exceed the memory capacity of a single node. To this end,
we devised methods that expose a high degree of parallelism while optimizing
algorithmic implementations for efficient multi-GPU usage. Previous work has
already demonstrated the library's performance efficiency on large-scale
systems comprising thousands of NVIDIA GPUs, achieving improvements over
state-of-the-art solutions. In this paper, we extend those results by providing
energy profiles that address the growing sustainability requirements of modern
HPC platforms. We present our methodology and tools for accurate runtime energy
measurements of the library's core components and discuss the findings. Our
results confirm that optimizing GPU computations and minimizing data movement
across memory and computing nodes reduces both time-to-solution and energy
consumption. Moreover, we show that the library delivers substantial advantages
over comparable software frameworks on standard benchmarks.

</details>


### [185] [Energy Efficiency in Cloud-Based Big Data Processing for Earth Observation: Gap Analysis and Future Directions](https://arxiv.org/abs/2510.02882)
*Adhitya Bhawiyuga,Serkan Girgin,Rolf A. de By,Raul Zurita-Milla*

Main category: cs.DC

TL;DR: 云端地球观测大数据处理在能源效率方面存在差距，需要能源感知的方法来提高效率。


<details>
  <summary>Details</summary>
Motivation: 随着地球观测（EO）数据量的快速增长和对能源成本及碳足迹的日益关注，云端EO大数据处理的能源效率问题变得尤为重要，尤其是在处理计算密集型基础模型时。

Method: 首先检查现有的EOBD处理模式，重点关注其对云处理的需求，并分析现有的云解决方案。然后，研究在其他大数据领域已成功应用的节能策略。最后，识别EOBD处理平台在能源效率方面存在的差距，并提出改进建议。

Result: 目前EOBD处理平台在能源效率方面存在差距，缺乏能源监控、数据管理、资源分配和任务调度方面的能源感知机制。主要关注点在于数据可访问性和计算可行性。

Conclusion: 提出开发能源感知的性能监控和基准测试框架、优化基础设施协调的技术以及分布式云端EOBD处理框架的节能任务调度方法，以提高EOBD处理的能源意识，减少能源消耗和环境影响。

Abstract: Earth observation (EO) data volumes are rapidly increasing. While cloud
computing are now used for processing large EO datasets, the energy efficiency
aspects of such a processing have received much less attention. This issue is
notable given the increasing awareness of energy costs and carbon footprint in
big data processing, particularly with increased attention on compute-intensive
foundation models. In this paper we identify gaps in energy efficiency
practices within cloud-based EO big data (EOBD) processing and propose several
research directions for improvement. We first examine the current EOBD
landscape, focus on the requirements that necessitate cloud-based processing
and analyze existing cloud-based EOBD solutions. We then investigate energy
efficiency strategies that have been successfully employed in well-studied big
data domains. Through this analysis, we identify several critical gaps in
existing EOBD processing platforms, which primarily focus on data accessibility
and computational feasibility, instead of energy efficiency. These gaps include
insufficient energy monitoring mechanisms, lack of energy awareness in data
management, inadequate implementation of energy-aware resource allocation and
lack of energy efficiency criteria on task scheduling. Based on these findings,
we propose the development of energy-aware performance monitoring and
benchmarking frameworks, the use of optimization techniques for infrastructure
orchestration, and of energy-efficient task scheduling approaches for
distributed cloud-based EOBD processing frameworks. These proposed approaches
aim to foster more energy awareness in EOBD processing , potentially reducing
power consumption and environmental impact while maintaining or minimally
impacting processing performance.

</details>


### [186] [PyRadiomics-cuda: a GPU-accelerated 3D features extraction from medical images within PyRadiomics](https://arxiv.org/abs/2510.02894)
*Jakub Lisowski,Piotr Tyrakowski,Szymon Zyguła,Krzysztof Kaczmarski*

Main category: cs.DC

TL;DR: PyRadiomics-cuda 是 PyRadiomics 库的 GPU 加速扩展，可显著缩短从医学图像中提取三维形状特征的处理时间，并与现有 API 完全兼容。


<details>
  <summary>Details</summary>
Motivation: 解决从医学图像中提取三维形状特征的计算挑战，通过将关键几何计算卸载到 GPU 硬件来显著减少处理时间，以支持高通量 AI 流水线。

Method: 通过在 GPU 硬件上卸载关键的几何计算来实现的，该库用 Python 和 C/CUDA 实现。

Result: 与原始 PyRadiomics API 完全兼容，可在各种设备（包括计算集群、预算设备和家用设备）上实现高效、可扩展的放射组学分析，并显著减少处理时间。

Conclusion: PyRadiomics-cuda 通过利用 GPU 加速，为放射组学分析提供了高效且用户友好的解决方案，支持快速特征提取，并能与现有工作流程无缝集成。

Abstract: PyRadiomics-cuda is a GPU-accelerated extension of the PyRadiomics library,
designed to address the computational challenges of extracting
three-dimensional shape features from medical images. By offloading key
geometric computations to GPU hardware it dramatically reduces processing times
for large volumetric datasets. The system maintains full compatibility with the
original PyRadiomics API, enabling seamless integration into existing AI
workflows without code modifications. This transparent acceleration facilitates
efficient, scalable radiomics analysis, supporting rapid feature extraction
essential for high-throughput AI pipeline. Tests performed on a typical
computational cluster, budget and home devices prove usefulness in all
scenarios. PyRadiomics-cuda is implemented in Python and C/CUDA and is freely
available under the BSD license at https://github.com/mis-wut/pyradiomics-CUDA
Additionally PyRadiomics-cuda test suite is available at
https://github.com/mis-wut/pyradiomics-cuda-data-gen. It provides detailed
handbook and sample scripts suited for different kinds of workflows plus
detailed installation instructions. The dataset used for testing is available
at Kaggle
https://www.kaggle.com/datasets/sabahesaraki/kidney-tumor-segmentation-challengekits-19

</details>


### [187] [iDDS: Intelligent Distributed Dispatch and Scheduling for Workflow Orchestration](https://arxiv.org/abs/2510.02930)
*Wen Guan,Tadashi Maeno,Aleksandr Alekseev,Fernando Harald Barreiro Megino,Kaushik De,Edward Karavakis,Alexei Klimentov,Tatiana Korchuganova,FaHui Lin,Paul Nilsson,Torre Wenaus,Zhaoyu Yang,Xin Zhao*

Main category: cs.DC

TL;DR: iDDS是一个用于大规模分布式科学计算的通用工作流编排系统，它通过集成数据感知执行、条件逻辑和可编程工作流来扩展传统工作负载和数据管理，并已成功应用于多个科学领域。


<details>
  <summary>Details</summary>
Motivation: iDDS旨在通过集成数据感知执行、条件逻辑和可编程工作流来自动化复杂的、动态的处理流程，以应对大规模分布式科学计算的挑战，并减少操作开销，实现可重复、高吞_度的_工作流。

Method: iDDS扩展了传统的工作负载和数据管理，集成了数据感知执行、条件逻辑和可编程工作流。其架构和核心组件具有可扩展性、模块化的消息驱动设计，并与PanDA和Rucio等系统集成。它支持模板驱动的工作流和基于Python的函数即任务模型。

Result: iDDS已成功应用于多个实际场景，包括：ATLAS的细粒度磁带资源优化、Rubin天文台的大规模有向无环图（DAG）工作流编排、机器学习应用分布式超参数优化、物理分析的主动学习以及电子离子对撞机的AI辅助探测器设计。

Conclusion: iDDS通过统一工作负载调度、数据移动和自适应决策，能够降低操作开销，并在异构基础设施上实现可重复、高吞吐量的_工作流_。未来的发展方向包括支持交互式、云原生和无服务器_工作流_。

Abstract: The intelligent Distributed Dispatch and Scheduling (iDDS) service is a
versatile workflow orchestration system designed for large-scale, distributed
scientific computing. iDDS extends traditional workload and data management by
integrating data-aware execution, conditional logic, and programmable
workflows, enabling automation of complex and dynamic processing pipelines.
Originally developed for the ATLAS experiment at the Large Hadron Collider,
iDDS has evolved into an experiment-agnostic platform that supports both
template-driven workflows and a Function-as-a-Task model for Python-based
orchestration.
  This paper presents the architecture and core components of iDDS,
highlighting its scalability, modular message-driven design, and integration
with systems such as PanDA and Rucio. We demonstrate its versatility through
real-world use cases: fine-grained tape resource optimization for ATLAS,
orchestration of large Directed Acyclic Graph (DAG) workflows for the Rubin
Observatory, distributed hyperparameter optimization for machine learning
applications, active learning for physics analyses, and AI-assisted detector
design at the Electron-Ion Collider.
  By unifying workload scheduling, data movement, and adaptive decision-making,
iDDS reduces operational overhead and enables reproducible, high-throughput
workflows across heterogeneous infrastructures. We conclude with current
challenges and future directions, including interactive, cloud-native, and
serverless workflow support.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [188] [ERUPT: An Open Toolkit for Interfacing with Robot Motion Planners in Extended Reality](https://arxiv.org/abs/2510.02464)
*Isaac Ngui,Courtney McBeth,André Santos,Grace He,Katherine J. Mimnaugh,James D. Motes,Luciano Soares,Marco Morales,Nancy M. Amato*

Main category: cs.RO

TL;DR: ERUPT是一个XR系统，用于交互式运动规划，用户可以在其中创建和重新配置环境，以在虚拟或增强现实中进行机器人路径规划。


<details>
  <summary>Details</summary>
Motivation: XR环境提供了更强的空间理解能力和更自然的交互方式，用户可以像在现实世界中一样操作物体，而不是依赖传统的鼠标和键盘。

Method: ERUPT系统与MoveIt集成，允许用户在XR环境中发送运动规划请求，并可视化机器人路径。它支持多种交互方式，用户可以修改环境物体并与虚拟机器人交互。

Result: 用户可以在虚拟空间中直观地看到机器人运动，确保行为符合预期且无碰撞，然后将规划好的路径部署到物理机器人上。

Conclusion: ERUPT通过利用XR技术，增强了机器人运动规划的交互性和空间感知能力，从而提高了规划的效率和安全性。

Abstract: We propose the Extended Reality Universal Planning Toolkit (ERUPT), an
extended reality (XR) system for interactive motion planning. Our system allows
users to create and dynamically reconfigure environments while they plan robot
paths. In immersive three-dimensional XR environments, users gain a greater
spatial understanding. XR also unlocks a broader range of natural interaction
capabilities, allowing users to grab and adjust objects in the environment
similarly to the real world, rather than using a mouse and keyboard with the
scene projected onto a two-dimensional computer screen. Our system integrates
with MoveIt, a manipulation planning framework, allowing users to send motion
planning requests and visualize the resulting robot paths in virtual or
augmented reality. We provide a broad range of interaction modalities, allowing
users to modify objects in the environment and interact with a virtual robot.
Our system allows operators to visualize robot motions, ensuring desired
behavior as it moves throughout the environment, without risk of collisions
within a virtual space, and to then deploy planned paths on physical robots in
the real world.

</details>


### [189] [SIMSplat: Predictive Driving Scene Editing with Language-aligned 4D Gaussian Splatting](https://arxiv.org/abs/2510.02469)
*Sung-Yeon Park,Adam Lee,Juanwu Lu,Can Cui,Luyang Jiang,Rohit Gupta,Kyungtae Han,Ahmadreza Moradipari,Ziran Wang*

Main category: cs.RO

TL;DR: SIMSplat是一个语言驱动的驾驶场景编辑器，利用高斯飞溅技术，可以直观地编辑和生成逼真的驾驶场景。


<details>
  <summary>Details</summary>
Motivation: 现有的基于传感器数据的驾驶场景生成方法在编辑能力和效率方面存在局限性，难以生成逼真的场景。

Method: SIMSplat利用语言对齐的高斯飞溅技术，实现了对驾驶场景的精确编辑。它支持通过自然语言提示进行场景操控，并能直接查询和修改道路上的对象，包括添加新对象、改变车辆和行人轨迹，并通过多智能体运动预测来优化交互，生成更真实的动态效果。

Result: 在Waymo数据集上的实验表明，SIMSplat在各种场景下都展现了强大的编辑能力和广泛的适应性。

Conclusion: SIMSplat通过语言控制和高斯飞溅技术，有效地解决了现有驾驶场景编辑器在编辑能力和效率上的不足，能够生成细节丰富且动态交互真实的驾驶场景。

Abstract: Driving scene manipulation with sensor data is emerging as a promising
alternative to traditional virtual driving simulators. However, existing
frameworks struggle to generate realistic scenarios efficiently due to limited
editing capabilities. To address these challenges, we present SIMSplat, a
predictive driving scene editor with language-aligned Gaussian splatting. As a
language-controlled editor, SIMSplat enables intuitive manipulation using
natural language prompts. By aligning language with Gaussian-reconstructed
scenes, it further supports direct querying of road objects, allowing precise
and flexible editing. Our method provides detailed object-level editing,
including adding new objects and modifying the trajectories of both vehicles
and pedestrians, while also incorporating predictive path refinement through
multi-agent motion prediction to generate realistic interactions among all
agents in the scene. Experiments on the Waymo dataset demonstrate SIMSplat's
extensive editing capabilities and adaptability across a wide range of
scenarios. Project page: https://sungyeonparkk.github.io/simsplat/

</details>


### [190] [U-LAG: Uncertainty-Aware, Lag-Adaptive Goal Retargeting for Robotic Manipulation](https://arxiv.org/abs/2510.02526)
*Anamika J H,Anujith Muraleedharan*

Main category: cs.RO

TL;DR: U-LAG是一个在执行过程中进行目标重定向的中间层，它在接收到新观测数据时重新设定任务目标（接触前、接触中、接触后），而无需改变底层控制器。其核心技术UAR-PF是一种不确定性感知的重定向器，能在感知延迟的情况下保持物体姿态的分布，并选择能够最大化预期进展的目标。该方法在PyBullet/PandaGym中进行了移位x延迟的压力测试，并在抓取、推动、堆叠和钉子插入等任务上进行了评估。


<details>
  <summary>Details</summary>
Motivation: 在不断变化的环境中，机器人操作需要处理滞后、嘈杂或过时的感知信息。

Method: 提出U-LAG，一个在执行过程中进行目标重定向的中间层，通过UAR-PF（一种不确定性感知的重定向器）在感知和控制之间进行连接，以适应感知延迟并最大化预期进展。

Result: 在移位和延迟的压力测试中，UAR-PF和ICP相比于不进行重定向的基线方法，性能有规律地下降，但在面临0-10厘米的移位和0-400毫秒的延迟时，仍然取得了更高的成功率，并减少了不必要的末端执行器移动和任务中止。

Conclusion: U-LAG提供了一个可插拔的模块，通过UAR-PF实现了对感知延迟和不确定性的适应性目标重定向，并在移位x延迟的基准测试中展示了其在多种操作任务上的有效性。

Abstract: Robots manipulating in changing environments must act on percepts that are
late, noisy, or stale. We present U-LAG, a mid-execution goal-retargeting layer
that leaves the low-level controller unchanged while re-aiming task goals
(pre-contact, contact, post) as new observations arrive. Unlike motion
retargeting or generic visual servoing, U-LAG treats in-flight goal re-aiming
as a first-class, pluggable module between perception and control. Our main
technical contribution is UAR-PF, an uncertainty-aware retargeter that
maintains a distribution over object pose under sensing lag and selects goals
that maximize expected progress. We instantiate a reproducible Shift x Lag
stress test in PyBullet/PandaGym for pick, push, stacking, and peg insertion,
where the object undergoes abrupt in-plane shifts while synthetic perception
lag is injected during approach. Across 0-10 cm shifts and 0-400 ms lags,
UAR-PF and ICP degrade gracefully relative to a no-retarget baseline, achieving
higher success with modest end-effector travel and fewer aborts; simple
operational safeguards further improve stability. Contributions: (1) UAR-PF for
lag-adaptive, uncertainty-aware goal retargeting; (2) a pluggable retargeting
interface; and (3) a reproducible Shift x Lag benchmark with evaluation on
pick, push, stacking, and peg insertion.

</details>


### [191] [A Recipe for Efficient Sim-to-Real Transfer in Manipulation with Online Imitation-Pretrained World Models](https://arxiv.org/abs/2510.02538)
*Yilin Wang,Shangzhe Li,Haoyi Niu,Zhiao Huang,Weitong Zhang,Hao Su*

Main category: cs.RO

TL;DR: 利用机器人模拟器和世界模型进行在线模仿学习，解决了真实世界数据有限的离线模仿学习问题。


<details>
  <summary>Details</summary>
Motivation: 现有的离线模仿学习方法在数据覆盖和性能下降方面存在不足，需要一种能克服这些限制的解决方案。

Method: 提出了一种基于世界模型的sim-to-real框架，结合了在线模仿预训练和离线微调，利用在线交互来弥补数据覆盖的不足。

Result: 在sim-to-sim迁移中成功率提高了至少31.7%，在sim-to-real迁移中成功率提高了至少23.3%，优于现有的离线模仿学习基线。

Conclusion: 所提出的方法通过利用在线交互和模拟器，有效地提高了模仿学习的鲁棒性、泛化性和性能。

Abstract: We are interested in solving the problem of imitation learning with a limited
amount of real-world expert data. Existing offline imitation methods often
struggle with poor data coverage and severe performance degradation. We propose
a solution that leverages robot simulators to achieve online imitation
learning. Our sim-to-real framework is based on world models and combines
online imitation pretraining with offline finetuning. By leveraging online
interactions, our approach alleviates the data coverage limitations of offline
methods, leading to improved robustness and reduced performance degradation
during finetuning. It also enhances generalization during domain transfer. Our
empirical results demonstrate its effectiveness, improving success rates by at
least 31.7% in sim-to-sim transfer and 23.3% in sim-to-real transfer over
existing offline imitation learning baselines.

</details>


### [192] [Efficient Optimal Path Planning in Dynamic Environments Using Koopman MPC](https://arxiv.org/abs/2510.02584)
*Mohammad Abtahi,Navid Mojahed,Shima Nazari*

Main category: cs.RO

TL;DR: 本文提出了一种基于 Koopman 算子理论的移动机器人动态环境导航的预测控制框架，重点在于寻找最优路径规划问题的全局线性表示，包括非线性机器人动力学和避碰约束。通过扩展动态模式分解识别线性与双线性 Koopman 实现，发现双线性模型能准确捕捉避碰所需的非线性状态-输入耦合和二次项。将该方法应用于MPC框架，可在提升安全性的同时，使路径规划速度比非线性MPC快320倍。


<details>
  <summary>Details</summary>
Motivation: 传统的基于 Koopman 算子的方法仅限于系统动力学的线性化，而忽略了最优路径规划问题（包括非线性机器人动力学和避碰约束）的全局线性表示。本文旨在解决这一问题，提出一种新的框架。

Method: 利用扩展动态模式分解（EDMD）识别输入-状态数据中的线性与双线性 Koopman 实现。在提升的空间中，将机器人路径规划问题表述为二次规划问题，并在 MPC 框架下确定最优机器人动作。

Result: 双线性 Koopman 模型能够精确捕捉非线性状态-输入耦合和二次项，这对于避碰至关重要，而线性实现则无法做到。该方法能够以比非线性 MPC 快 320 倍的速度找到安全的最优动作。

Conclusion: 本文证明了双线性 Koopman 实现在线性化高度非线性最优控制问题（包含非线性状态和输入约束）方面具有巨大潜力，能够实现与线性问题相当的计算效率。

Abstract: This paper presents a data-driven model predictive control framework for
mobile robots navigating in dynamic environments, leveraging Koopman operator
theory. Unlike the conventional Koopman-based approaches that focus on the
linearization of system dynamics only, our work focuses on finding a global
linear representation for the optimal path planning problem that includes both
the nonlinear robot dynamics and collision-avoidance constraints. We deploy
extended dynamic mode decomposition to identify linear and bilinear Koopman
realizations from input-state data. Our open-loop analysis demonstrates that
only the bilinear Koopman model can accurately capture nonlinear state-input
couplings and quadratic terms essential for collision avoidance, whereas linear
realizations fail to do so. We formulate a quadratic program for the robot path
planning in the presence of moving obstacles in the lifted space and determine
the optimal robot action in an MPC framework. Our approach is capable of
finding the safe optimal action 320 times faster than a nonlinear MPC
counterpart that solves the path planning problem in the original state space.
Our work highlights the potential of bilinear Koopman realizations for
linearization of highly nonlinear optimal control problems subject to nonlinear
state and input constraints to achieve computational efficiency similar to
linear problems.

</details>


### [193] [SubSense: VR-Haptic and Motor Feedback for Immersive Control in Subsea Telerobotics](https://arxiv.org/abs/2510.02594)
*Ruo Chen,David Blow,Adnan Abdullah,Md Jahidul Islam*

Main category: cs.RO

TL;DR: 本研究提出了一种名为SubSense的新型VR-触觉框架，通过集成触觉反馈和VR控制接口来增强水下ROV的遥操作和遥操作性。


<details>
  <summary>Details</summary>
Motivation: 传统ROV遥操作依赖低分辨率的2D摄像头，缺乏沉浸感和触觉反馈，导致在复杂的海底环境中情境意识降低。

Method: SubSense框架包含一个非侵入式反馈接口，与遥操作员的手套配对，以提供触觉反馈和抓握状态。此外，该框架还集成了端到端的软件，用于通过VR平台管理控制输入和显示沉浸式摄像头视图。

Result: 实验和用户研究表明，与传统的遥操作界面相比，该系统在精细操作任务方面效果更佳。

Conclusion: 多感官反馈在沉浸式虚拟环境中具有巨大潜力，可以显著提高远程情境意识和任务性能，为现场提供更直观、更易于访问的ROV操作。

Abstract: This paper investigates the integration of haptic feedback and virtual
reality (VR) control interfaces to enhance teleoperation and telemanipulation
of underwater ROVs (remotely operated vehicles). Traditional ROV teleoperation
relies on low-resolution 2D camera feeds and lacks immersive and sensory
feedback, which diminishes situational awareness in complex subsea
environments. We propose SubSense -- a novel VR-Haptic framework incorporating
a non-invasive feedback interface to an otherwise 1-DOF (degree of freedom)
manipulator, which is paired with the teleoperator's glove to provide haptic
feedback and grasp status. Additionally, our framework integrates end-to-end
software for managing control inputs and displaying immersive camera views
through a VR platform. We validate the system through comprehensive experiments
and user studies, demonstrating its effectiveness over conventional
teleoperation interfaces, particularly for delicate manipulation tasks. Our
results highlight the potential of multisensory feedback in immersive virtual
environments to significantly improve remote situational awareness and mission
performance, offering more intuitive and accessible ROV operations in the
field.

</details>


### [194] [UMI-on-Air: Embodiment-Aware Guidance for Embodiment-Agnostic Visuomotor Policies](https://arxiv.org/abs/2510.02614)
*Harsh Gupta,Xiaofeng Guo,Huy Ha,Chuer Pan,Muqing Cao,Dongjae Lee,Sebastian Sherer,Shuran Song,Guanya Shi*

Main category: cs.RO

TL;DR: UMI-on-Air是一个用于具身无关操纵策略的具身感知部署框架，通过在推理时将高层UMI策略与低层特定于具身的控制器相结合，并集成梯度反馈以引导轨迹生成，从而实现针对部署具身的动态可行模式的适应，提高了在各种空运操纵任务中的成功率、效率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 将从非约束性人类演示中训练出的通用化操纵策略转移到受限的机器人具身（如空中操纵器）时，控制和机器人动力学不匹配会导致分布外行为和执行不佳。需要一种方法来解决这种不匹配问题，使策略能够适应不同的机器人具身。

Method: 提出了一种具身感知扩散策略（EADP），它在推理时将高层UMI策略与低层特定于具身的控制器相结合。通过将来自控制器跟踪成本的梯度反馈集成到扩散采样过程中，该方法将轨迹生成引导到针对部署具身的动态可行模式，从而在测试时实现即插即用的、具身感知的轨迹适应。

Result: 在多个长时程和高精度空运操纵任务上验证了该方法，与无引导的扩散基线相比，在成功率、效率和鲁棒性方面有所提高。在以前未见过的环境中进行了演示，使用了在野外收集的UMI演示。

Conclusion: UMI-on-Air 提供了一种实用的方法，可以将通用化的操纵技能扩展到各种不同甚至高度受限的具身，而无需对每个具身进行重新训练，从而为在不同机器人平台上部署操纵策略铺平了道路。

Abstract: We introduce UMI-on-Air, a framework for embodiment-aware deployment of
embodiment-agnostic manipulation policies. Our approach leverages diverse,
unconstrained human demonstrations collected with a handheld gripper (UMI) to
train generalizable visuomotor policies. A central challenge in transferring
these policies to constrained robotic embodiments-such as aerial
manipulators-is the mismatch in control and robot dynamics, which often leads
to out-of-distribution behaviors and poor execution. To address this, we
propose Embodiment-Aware Diffusion Policy (EADP), which couples a high-level
UMI policy with a low-level embodiment-specific controller at inference time.
By integrating gradient feedback from the controller's tracking cost into the
diffusion sampling process, our method steers trajectory generation towards
dynamically feasible modes tailored to the deployment embodiment. This enables
plug-and-play, embodiment-aware trajectory adaptation at test time. We validate
our approach on multiple long-horizon and high-precision aerial manipulation
tasks, showing improved success rates, efficiency, and robustness under
disturbances compared to unguided diffusion baselines. Finally, we demonstrate
deployment in previously unseen environments, using UMI demonstrations
collected in the wild, highlighting a practical pathway for scaling
generalizable manipulation skills across diverse-and even highly
constrained-embodiments. All code, data, and checkpoints will be publicly
released after acceptance. Result videos can be found at umi-on-air.github.io.

</details>


### [195] [RSV-SLAM: Toward Real-Time Semantic Visual SLAM in Indoor Dynamic Environments](https://arxiv.org/abs/2510.02616)
*Mobin Habibpour,Alireza Nemati,Ali Meghdari,Alireza Taheri,Shima Nazari*

Main category: cs.RO

TL;DR: 本研究提出了一种实时语义RGBD SLAM方法，用于动态环境，通过深度学习和EKF检测和处理动态物体，实现了有竞争力的定位精度和近实时性能。


<details>
  <summary>Details</summary>
Motivation: 许多现有的视觉SLAM方法在动态环境中表现不佳，而该研究旨在解决这一问题。

Method: 提出了一种结合深度学习语义信息和扩展卡尔曼滤波（EKF）的实时语义RGBD SLAM方法，以检测动态物体并维护静态地图。还使用生成网络填充动态物体的缺失区域。

Result: 该方法在TUM数据集的动态序列上进行了测试，与现有技术相比，在接近实时运行的情况下，实现了具有竞争力的定位误差，帧率约为22 fps。

Conclusion: 所提出的方法能够有效检测动态物体，维护静态地图，确保鲁棒的相机跟踪，并在动态环境中提供具有竞争力的定位精度。

Abstract: Simultaneous Localization and Mapping (SLAM) plays an important role in many
robotics fields, including social robots. Many of the available visual SLAM
methods are based on the assumption of a static world and struggle in dynamic
environments. In the current study, we introduce a real-time semantic RGBD SLAM
approach designed specifically for dynamic environments. Our proposed system
can effectively detect moving objects and maintain a static map to ensure
robust camera tracking. The key innovation of our approach is the incorporation
of deep learning-based semantic information into SLAM systems to mitigate the
impact of dynamic objects. Additionally, we enhance the semantic segmentation
process by integrating an Extended Kalman filter to identify dynamic objects
that may be temporarily idle. We have also implemented a generative network to
fill in the missing regions of input images belonging to dynamic objects. This
highly modular framework has been implemented on the ROS platform and can
achieve around 22 fps on a GTX1080. Benchmarking the developed pipeline on
dynamic sequences from the TUM dataset suggests that the proposed approach
delivers competitive localization error in comparison with the state-of-the-art
methods, all while operating in near real-time. The source code is publicly
available.

</details>


### [196] [Reachable Predictive Control: A Novel Control Algorithm for Nonlinear Systems with Unknown Dynamics and its Practical Applications](https://arxiv.org/abs/2510.02623)
*Taha Shafa,Yiming Meng,Melkior Ornik*

Main category: cs.RO

TL;DR: 提出一种能在未知系统动力学下，让系统沿分段线性轨迹行驶的算法。


<details>
  <summary>Details</summary>
Motivation: 受系统动力学可能发生突变这一关键失效场景的启发，证明了在未知系统动力学的情况下，可以遵循一组由分析证明可达状态组成的路点。

Method: 所提出的算法首先对当前状态应用小的扰动以局部学习系统动力学，然后计算使用局部学习的动力学可证明可达的状态集及其相应的最大增长率界，最后合成一个将系统导航到保证可达状态的控制作用。

Result: 算法能够让系统在未知动力学下沿分段线性轨迹行驶。

Conclusion: 在未知系统动力学的情况下，可以遵循一组由分析证明可达状态组成的路点。

Abstract: This paper proposes an algorithm capable of driving a system to follow a
piecewise linear trajectory without prior knowledge of the system dynamics.
Motivated by a critical failure scenario in which a system can experience an
abrupt change in its dynamics, we demonstrate that it is possible to follow a
set of waypoints comprised of states analytically proven to be reachable
despite not knowing the system dynamics. The proposed algorithm first applies
small perturbations to locally learn the system dynamics around the current
state, then computes the set of states that are provably reachable using the
locally learned dynamics and their corresponding maximum growth-rate bounds,
and finally synthesizes a control action that navigates the system to a
guaranteed reachable state.

</details>


### [197] [Action Deviation-Aware Inference for Low-Latency Wireless Robots](https://arxiv.org/abs/2510.02851)
*Jeyoung Park,Yeonsub Lim,Seungeun Oh,Jihong Park,Jinho Choi,Seong-Lyun Kim*

Main category: cs.RO

TL;DR: 6G 驱动的分布式机器学习场景中，提出了一种名为“动作偏差感知混合推理”的新型推理方法，通过选择性地跳过不必要的通信和计算，显著降低了延迟并减少了资源消耗。


<details>
  <summary>Details</summary>
Motivation: 为了支持自动驾驶和工业机器人控制等低延迟人工智能应用，6G 设想了分布式机器学习，并在超可靠低延迟通信（HRLLC）下连接边缘和云端的分布式计算资源。然而，与文本生成不同，行为克隆策略（通常用于机器人和自动驾驶等具身人工智能应用）无法并行验证和纠正多个草稿，因为每个动作都依赖于需要由先前动作更新的观察结果。

Method: 提出了一种名为“动作偏差感知混合推理”的方法，其中草稿模型估计动作是否需要目标模型进行验证和纠正，并选择性地跳过服务器操作的通信和计算。动作偏差与目标模型拒绝动作的概率高度相关，从而实现了选择性跳过。推导了平衡传输速率和推理性能的路径偏差阈值。

Result: 动作偏差感知混合推理将上行链路传输和服务器操作减少了 40%，同时将端到端延迟降低了 33.32%，并且任务成功率达到了仅使用目标模型的推理的 97.03%。

Conclusion: 动作偏差感知混合推理能够有效解决分布式机器学习场景下的延迟和资源效率问题，为具身人工智能应用提供了有前景的解决方案。

Abstract: To support latency-sensitive AI applications ranging from autonomous driving
to industrial robot manipulation, 6G envisions distributed ML, connecting
distributed computational resources in edge and cloud over hyper-reliable
low-latency communication (HRLLC). In this setting, speculative decoding can
facilitate collaborative inference of models distributively deployed: an
on-device draft model locally generates drafts and a remote server-based target
model verifies and corrects them, resulting lower latency. However, unlike
autoregressive text generation, behavior cloning policies, typically used for
embodied AI applications like robot manipulation and autonomous driving, cannot
parallelize verification and correction for multiple drafts as each action
depends on observation which needs to be updated by a previous action. To this
end, we propose Action Deviation-Aware Hybrid Inference, wherein the draft
model estimates an action's need for verification and correction by the target
model and selectively skips communication and computation for server
operations. Action deviation shows a strong correlation with action's rejection
probability by the target model, enabling selective skipping. We derive the
path deviation threshold that balances the transmission rate and the inference
performance, and we empirically show that action deviation-aware hybrid
inference reduces uplink transmission and server operation by 40%, while
lowering end-to-end latency by 33.32% relative to hybrid inference without
skipping and achieving task success rate up to 97.03% of that of target model
only inference.

</details>


### [198] [Multi-robot Rigid Formation Navigation via Synchronous Motion and Discrete-time Communication-Control Optimization](https://arxiv.org/abs/2510.02624)
*Qun Yang,Soung Chang Liew*

Main category: cs.RO

TL;DR: 该研究提出了一种名为“hold-and-hit”的通信-控制框架，用于解决多机器人编队导航问题，特别是在存在无线网络延迟和丢包的情况下，以及需要遵循复杂曲线路径时。


<details>
  <summary>Details</summary>
Motivation: 现有针对通过无线网络在微处理器平台上执行的多机器人编队导航解决方案不足，尤其是在处理复杂曲线路径和非完整约束方面。

Method: 提出“hold-and-hit”通信-控制框架，结合了周期性通信控制和周期内优化方法，以实现鲁棒的编队导航，并兼容ROS平台。

Result: 仿真结果表明，在S形路径上，四机器人方阵队的保持效果优于现有方法。真实世界实验验证了该框架的有效性，在特定速度下，机器人保持了较小的距离和角度误差。

Conclusion: “hold-and-hit”框架结合了周期内优化方法，能够精确可靠地实现多机器人编队沿复杂曲线路径的导航，即使在存在无线网络延迟、丢包和非完整约束的情况下。

Abstract: Rigid-formation navigation of multiple robots is essential for applications
such as cooperative transportation. This process involves a team of
collaborative robots maintaining a predefined geometric configuration, such as
a square, while in motion. For untethered collaborative motion, inter-robot
communication must be conducted through a wireless network. Notably, few
existing works offer a comprehensive solution for multi-robot formation
navigation executable on microprocessor platforms via wireless networks,
particularly for formations that must traverse complex curvilinear paths. To
address this gap, we introduce a novel "hold-and-hit" communication-control
framework designed to work seamlessly with the widely-used Robotic Operating
System (ROS) platform. The hold-and-hit framework synchronizes robot movements
in a manner robust against wireless network delays and packet loss. It operates
over discrete-time communication-control cycles, making it suitable for
implementation on contemporary microprocessors. Complementary to hold-and-hit,
we propose an intra-cycle optimization approach that enables rigid formations
to closely follow desired curvilinear paths, even under the nonholonomic
movement constraints inherent to most vehicular robots. The combination of
hold-and-hit and intra-cycle optimization ensures precise and reliable
navigation even in challenging scenarios. Simulations in a virtual environment
demonstrate the superiority of our method in maintaining a four-robot square
formation along an S-shaped path, outperforming two existing approaches.
Furthermore, real-world experiments validate the effectiveness of our
framework: the robots maintained an inter-distance error within $\pm 0.069m$
and an inter-angular orientation error within $\pm19.15^{\circ}$ while
navigating along an S-shaped path at a fixed linear velocity of $0.1 m/s$.

</details>


### [199] [A Trajectory Generator for High-Density Traffic and Diverse Agent-Interaction Scenarios](https://arxiv.org/abs/2510.02627)
*Ruining Yang,Yi Xu,Yixiao Chen,Yun Fu,Lili Su*

Main category: cs.RO

TL;DR: 现有的轨迹预测数据集存在长尾分布问题，导致模型泛化能力不足。本文提出了一种新的轨迹生成框架，通过结构化网格表示和行为感知生成机制，增强了场景密度和行为多样性，并合成了更逼真、更具挑战性的数据，以提升下游模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的自动驾驶轨迹预测数据集存在长尾分布问题，导致模型在复杂场景和关键行为（如变道、超车、转弯）上的泛化能力不足，并可能导致评估结果过于乐观。

Method: 提出了一种新的轨迹生成框架，将连续道路环境转换为结构化网格表示，并结合基于规则的决策触发器、Frenet轨迹平滑和动态可行性约束来实现行为感知生成，从而合成高密度场景和包含复杂交互的稀有行为。

Result: 在Argoverse 1和Argoverse 2数据集上的实验表明，该方法显著提高了代理密度和行为多样性，同时保持了运动的真实性和场景层面的安全性。合成数据也提升了下游轨迹预测模型的性能，特别是在高密度场景下的表现。

Conclusion: 所提出的轨迹生成框架能够有效解决现有数据集的长尾分布问题，生成更丰富、更具挑战性的轨迹数据，并提升下游预测模型的性能和泛化能力。

Abstract: Accurate trajectory prediction is fundamental to autonomous driving, as it
underpins safe motion planning and collision avoidance in complex environments.
However, existing benchmark datasets suffer from a pronounced long-tail
distribution problem, with most samples drawn from low-density scenarios and
simple straight-driving behaviors. This underrepresentation of high-density
scenarios and safety critical maneuvers such as lane changes, overtaking and
turning is an obstacle to model generalization and leads to overly optimistic
evaluations. To address these challenges, we propose a novel trajectory
generation framework that simultaneously enhances scenarios density and
enriches behavioral diversity. Specifically, our approach converts continuous
road environments into a structured grid representation that supports
fine-grained path planning, explicit conflict detection, and multi-agent
coordination. Built upon this representation, we introduce behavior-aware
generation mechanisms that combine rule-based decision triggers with
Frenet-based trajectory smoothing and dynamic feasibility constraints. This
design allows us to synthesize realistic high-density scenarios and rare
behaviors with complex interactions that are often missing in real data.
Extensive experiments on the large-scale Argoverse 1 and Argoverse 2 datasets
demonstrate that our method significantly improves both agent density and
behavior diversity, while preserving motion realism and scenario-level safety.
Our synthetic data also benefits downstream trajectory prediction models and
enhances performance in challenging high-density scenarios.

</details>


### [200] [A $1000\times$ Faster LLM-enhanced Algorithm For Path Planning in Large-scale Grid Maps](https://arxiv.org/abs/2510.02716)
*Junlin Zeng,Xin Zhang,Xiang Zhao,Yan Pan*

Main category: cs.RO

TL;DR: LLM-A* 存在计算时间长的问题，本文提出 iLLM-A* 算法，通过优化 A*、增量学习 LLM 生成路径点和选择合适的路径点，实现了超过 1000 倍的加速，并节省了内存和缩短了路径长度。


<details>
  <summary>Details</summary>
Motivation: 现有路径规划方法在处理大规模地图时效率低下，LLM 在路径规划方面表现出色但存在空间幻觉和规划性能差的问题。LLM-A* 算法虽然利用 LLM 生成路径点，但计算时间仍然很高，需要进一步改进。

Method: 本文提出 iLLM-A* 算法，包含三个机制：优化 A*、增量学习 LLM 生成高质量路径点、选择合适的路径点供 A* 进行路径规划。

Result: iLLM-A* 算法相比 LLM-A*，平均实现了超过 1000 倍的加速（极端情况可达 2349.5 倍），内存节省高达 58.6%，并且路径长度更短，路径长度标准差更低。

Conclusion: iLLM-A* 算法通过优化 A*、增量学习 LLM 和选择合适的路径点，有效解决了 LLM-A* 在大规模地图路径规划中的效率和性能问题，取得了显著的加速、内存节省和路径优化效果。

Abstract: Path planning in grid maps, arising from various applications, has garnered
significant attention. Existing methods, such as A*, Dijkstra, and their
variants, work well for small-scale maps but fail to address large-scale ones
due to high search time and memory consumption. Recently, Large Language Models
(LLMs) have shown remarkable performance in path planning but still suffer from
spatial illusion and poor planning performance. Among all the works, LLM-A*
\cite{meng2024llm} leverages LLM to generate a series of waypoints and then
uses A* to plan the paths between the neighboring waypoints. In this way, the
complete path is constructed. However, LLM-A* still suffers from high
computational time for large-scale maps. To fill this gap, we conducted a deep
investigation into LLM-A* and found its bottleneck, resulting in limited
performance. Accordingly, we design an innovative LLM-enhanced algorithm, abbr.
as iLLM-A*. iLLM-A* includes 3 carefully designed mechanisms, including the
optimization of A*, an incremental learning method for LLM to generate
high-quality waypoints, and the selection of the appropriate waypoints for A*
for path planning. Finally, a comprehensive evaluation on various grid maps
shows that, compared with LLM-A*, iLLM-A* \textbf{1) achieves more than
$1000\times$ speedup on average, and up to $2349.5\times$ speedup in the
extreme case, 2) saves up to $58.6\%$ of the memory cost, 3) achieves both
obviously shorter path length and lower path length standard deviation.}

</details>


### [201] [Team Xiaomi EV-AD VLA: Caption-Guided Retrieval System for Cross-Modal Drone Navigation -- Technical Report for IROS 2025 RoboSense Challenge Track 4](https://arxiv.org/abs/2510.02728)
*Lingfeng Zhang,Erjia Xiao,Yuchen Zhang,Haoxiang Fu,Ruibin Hu,Yanbiao Ma,Wenbo Ding,Long Chen,Hangjun Ye,Xiaoshuai Hao*

Main category: cs.RO

TL;DR: 提出一种两阶段检索细化方法CGRS，通过VLM生成图像描述，细化文本与图像的语义匹配，提升了5%的检索精度，并在比赛中获得第二名。


<details>
  <summary>Details</summary>
Motivation: 跨模态无人机导航需要根据自然语言描述从大规模数据库中检索相关图像，现有方法在细粒度语义匹配方面存在不足，尤其是在复杂空中场景下。

Method: 首先使用基线模型进行粗略排序，选出前20张相关图像；然后利用视觉语言模型（VLM）为候选图像生成详细描述；最后利用生成的多模态描述进行精细重排。

Result: 在所有关键指标（Recall@1, Recall@5, Recall@10）上比基线模型一致提高了5%的性能。

Conclusion: 所提出的跨模态检索方法在实际机器人导航场景中具有应用价值，并在RoboSense 2025 Track 4挑战赛中获得第二名。

Abstract: Cross-modal drone navigation remains a challenging task in robotics,
requiring efficient retrieval of relevant images from large-scale databases
based on natural language descriptions. The RoboSense 2025 Track 4 challenge
addresses this challenge, focusing on robust, natural language-guided
cross-view image retrieval across multiple platforms (drones, satellites, and
ground cameras). Current baseline methods, while effective for initial
retrieval, often struggle to achieve fine-grained semantic matching between
text queries and visual content, especially in complex aerial scenes. To
address this challenge, we propose a two-stage retrieval refinement method:
Caption-Guided Retrieval System (CGRS) that enhances the baseline coarse
ranking through intelligent reranking. Our method first leverages a baseline
model to obtain an initial coarse ranking of the top 20 most relevant images
for each query. We then use Vision-Language-Model (VLM) to generate detailed
captions for these candidate images, capturing rich semantic descriptions of
their visual content. These generated captions are then used in a multimodal
similarity computation framework to perform fine-grained reranking of the
original text query, effectively building a semantic bridge between the visual
content and natural language descriptions. Our approach significantly improves
upon the baseline, achieving a consistent 5\% improvement across all key
metrics (Recall@1, Recall@5, and Recall@10). Our approach win TOP-2 in the
challenge, demonstrating the practical value of our semantic refinement
strategy in real-world robotic navigation scenarios.

</details>


### [202] [Flow with the Force Field: Learning 3D Compliant Flow Matching Policies from Force and Demonstration-Guided Simulation Data](https://arxiv.org/abs/2510.02738)
*Tianyu Li,Yihan Li,Zizhe Zhang,Nadia Figueroa*

Main category: cs.RO

TL;DR: 该研究提出了一种在模拟环境中生成力信息数据的方法，并通过结合顺应性策略来改进基于视觉的模仿学习策略，以应对接触丰富的操作任务。


<details>
  <summary>Details</summary>
Motivation: 当前的视觉-动作策略在处理需要持续接触的任务时面临挑战，因为它们通常忽视了顺应性和力的显式处理，导致过大的接触力或在不确定性下的脆弱行为。

Method: 提出了一种在模拟环境中生成力信息的框架，该框架仅需单次人类演示即可生成数据，并将此数据与顺应性策略相结合，以提高从合成数据中学习到的视觉-动作策略的性能。

Result: 在真实机器人任务（包括非抓取式方块翻转和双臂物体移动）上验证了该方法，所学策略能够可靠地维持接触并适应新条件。

Conclusion: 通过在模拟环境中生成力信息数据并结合顺应性策略，可以显著提升视觉-动作策略在接触丰富任务上的性能和鲁棒性，解决了数据稀疏和Sim2Real迁移的挑战。

Abstract: While visuomotor policy has made advancements in recent years, contact-rich
tasks still remain a challenge. Robotic manipulation tasks that require
continuous contact demand explicit handling of compliance and force. However,
most visuomotor policies ignore compliance, overlooking the importance of
physical interaction with the real world, often leading to excessive contact
forces or fragile behavior under uncertainty. Introducing force information
into vision-based imitation learning could help improve awareness of contacts,
but could also require a lot of data to perform well. One remedy for data
scarcity is to generate data in simulation, yet computationally taxing
processes are required to generate data good enough not to suffer from the
Sim2Real gap. In this work, we introduce a framework for generating
force-informed data in simulation, instantiated by a single human
demonstration, and show how coupling with a compliant policy improves the
performance of a visuomotor policy learned from synthetic data. We validate our
approach on real-robot tasks, including non-prehensile block flipping and a
bi-manual object moving, where the learned policy exhibits reliable contact
maintenance and adaptation to novel conditions. Project Website:
https://flow-with-the-force-field.github.io/webpage/

</details>


### [203] [Work Zones challenge VLM Trajectory Planning: Toward Mitigation and Robust Autonomous Driving](https://arxiv.org/abs/2510.02803)
*Yifan Liao,Zhen Sun,Xiaoyun Qiu,Zixiao Zhao,Wenbing Tang,Xinlei He,Xinhu Zheng,Tianwei Zhang,Xinyi Huang,Xingshuo Han*

Main category: cs.RO

TL;DR: 现有视觉语言模型（VLM）在处理中国高速公路收费站场景时规划能力不足，导致在68.0%的案例中生成错误的轨迹。为了解决这个问题，研究人员提出了REACT-Drive框架，该框架结合了VLM和检索增强生成（RAG）技术。REACT-Drive将先前的失败案例转化为约束规则和可执行代码，并利用RAG检索相似模式来指导新场景下的轨迹生成。实验表明，REACT-Drive在ROADWork数据集上显著提高了轨迹规划的准确性（平均位移误差降低约3倍），并且推理速度比微调方法快得多。在真实车辆的物理世界测试也证实了该框架的实用性。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型（VLM）在处理中国高速公路收费站场景时规划能力不足，导致在68.0%的案例中生成错误的轨迹。为了解决这个问题，研究人员提出了REACT-Drive框架，该框架结合了VLM和检索增强生成（RAG）技术。REACT-Drive将先前的失败案例转化为约束规则和可执行代码，并利用RAG检索相似模式来指导新场景下的轨迹生成。实验表明，REACT-Drive在ROADWork数据集上显著提高了轨迹规划的准确性（平均位移误差降低约3倍），并且推理速度比微调方法快得多。在真实车辆的物理世界测试也证实了该框架的实用性。

Method: 研究人员提出了REACT-Drive框架，该框架结合了VLM和检索增强生成（RAG）技术。REACT-Drive将先前的失败案例转化为约束规则和可执行代码，并利用RAG检索相似模式来指导新场景下的轨迹生成。

Result: 实验表明，REACT-Drive在ROADWork数据集上显著提高了轨迹规划的准确性（平均位移误差降低约3倍），并且推理速度比微调方法快得多。在真实车辆的物理世界测试也证实了该框架的实用性。

Conclusion: REACT-Drive框架在处理中国高速公路收费站场景时，能够显著提高轨迹规划的准确性和效率，并且在真实世界中表现出良好的实用性。

Abstract: Visual Language Models (VLMs), with powerful multimodal reasoning
capabilities, are gradually integrated into autonomous driving by several
automobile manufacturers to enhance planning capability in challenging
environments. However, the trajectory planning capability of VLMs in work
zones, which often include irregular layouts, temporary traffic control, and
dynamically changing geometric structures, is still unexplored. To bridge this
gap, we conduct the \textit{first} systematic study of VLMs for work zone
trajectory planning, revealing that mainstream VLMs fail to generate correct
trajectories in $68.0%$ of cases. To better understand these failures, we first
identify candidate patterns via subgraph mining and clustering analysis, and
then confirm the validity of $8$ common failure patterns through human
verification. Building on these findings, we propose REACT-Drive, a trajectory
planning framework that integrates VLMs with Retrieval-Augmented Generation
(RAG). Specifically, REACT-Drive leverages VLMs to convert prior failure cases
into constraint rules and executable trajectory planning code, while RAG
retrieves similar patterns in new scenarios to guide trajectory generation.
Experimental results on the ROADWork dataset show that REACT-Drive yields a
reduction of around $3\times$ in average displacement error relative to VLM
baselines under evaluation with Qwen2.5-VL. In addition, REACT-Drive yields the
lowest inference time ($0.58$s) compared with other methods such as fine-tuning
($17.90$s). We further conduct experiments using a real vehicle in 15 work zone
scenarios in the physical world, demonstrating the strong practicality of
REACT-Drive.

</details>


### [204] [Assist-as-needed Control for FES in Foot Drop Management](https://arxiv.org/abs/2510.02808)
*Andreas Christou,Elliot Lister,Georgia Andreopoulou,Don Mahad,Sethu Vijayakumar*

Main category: cs.RO

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Foot drop is commonly managed using Functional Electrical Stimulation (FES),
typically delivered via open-loop controllers with fixed stimulation
intensities. While users may manually adjust the intensity through external
controls, this approach risks overstimulation, leading to muscle fatigue and
discomfort, or understimulation, which compromises dorsiflexion and increases
fall risk. In this study, we propose a novel closed-loop FES controller that
dynamically adjusts the stimulation intensity based on real-time toe clearance,
providing "assistance as needed". We evaluate this system by inducing foot drop
in healthy participants and comparing the effects of the closed-loop controller
with a traditional open-loop controller across various walking conditions,
including different speeds and surface inclinations. Kinematic data reveal that
our closed-loop controller maintains adequate toe clearance without
significantly affecting the joint angles of the hips, the knees, and the
ankles, and while using significantly lower stimulation intensities compared to
the open-loop controller. These findings suggest that the proposed method not
only matches the effectiveness of existing systems but also offers the
potential for reduced muscle fatigue and improved long-term user comfort and
adherence.

</details>


### [205] [Novel UWB Synthetic Aperture Radar Imaging for Mobile Robot Mapping](https://arxiv.org/abs/2510.02874)
*Charith Premachandra,U-Xuan Tan*

Main category: cs.RO

TL;DR: UWB雷达可以通过合成孔径雷达（SAR）技术，在恶劣天气下实现高分辨率环境测绘和闭环检测，以提高机器人感知系统的鲁棒性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统的外部传感器（如激光雷达和摄像头）在能见度差的情况下难以感知环境，而UWB雷达能穿透恶劣环境，但单次扫描图像不精细，需要通过移动机器人路径合成虚拟大孔径。

Method: 本文提出了一种将基于UWB雷达的SAR成像技术整合到机器人中的流程，用于绘制未知环境。同时，评估了SIFT、SURF、BRISK、AKAZE和ORB等特征检测器在UWB SAR图像中进行闭环检测的性能。

Result: 实验在模拟的恶劣环境下进行，结果证明了UWB SAR成像技术在进行高分辨率环境测绘和闭环检测方面的可行性和有效性。

Conclusion: UWB SAR成像技术为机器人提供了比传统传感器更鲁棒、更可靠的环境感知和闭环检测方法，尤其是在能见度差的条件下。

Abstract: Traditional exteroceptive sensors in mobile robots, such as LiDARs and
cameras often struggle to perceive the environment in poor visibility
conditions. Recently, radar technologies, such as ultra-wideband (UWB) have
emerged as potential alternatives due to their ability to see through adverse
environmental conditions (e.g. dust, smoke and rain). However, due to the small
apertures with low directivity, the UWB radars cannot reconstruct a detailed
image of its field of view (FOV) using a single scan. Hence, a virtual large
aperture is synthesized by moving the radar along a mobile robot path. The
resulting synthetic aperture radar (SAR) image is a high-definition
representation of the surrounding environment. Hence, this paper proposes a
pipeline for mobile robots to incorporate UWB radar-based SAR imaging to map an
unknown environment. Finally, we evaluated the performance of classical feature
detectors: SIFT, SURF, BRISK, AKAZE and ORB to identify loop closures using UWB
SAR images. The experiments were conducted emulating adverse environmental
conditions. The results demonstrate the viability and effectiveness of UWB SAR
imaging for high-resolution environmental mapping and loop closure detection
toward more robust and reliable robotic perception systems.

</details>


### [206] [Point Cloud-Based Control Barrier Functions for Model Predictive Control in Safety-Critical Navigation of Autonomous Mobile Robots](https://arxiv.org/abs/2510.02885)
*Faduo Liang,Yunfeng Yang,Shi-Lu Dai*

Main category: cs.RO

TL;DR: 该算法提出了一种新的运动规划方法，用于自动移动机器人的安全关键导航，通过整合动态障碍物跟踪和卡尔曼滤波来预测障碍物运动，并结合控制势垒函数和非线性模型预测控制来实现安全导航。


<details>
  <summary>Details</summary>
Motivation: 为了实现自动移动机器人在复杂环境中的安全关键导航，需要一种能够有效避免静态和动态障碍物的运动规划算法。

Method: 提出了一种集成了实时动态障碍物跟踪和映射系统（使用卡尔曼滤波估计和预测动态障碍物运动）的运动规划算法，并结合了控制势垒函数（CBFs）和非线性模型预测控制（NMPC）。CBF约束基于预测的未来状态与前向时间域（FTD）地图之间的碰撞检测所识别的风险点来制定。

Result: 实验结果表明，该算法在模拟和真实世界场景中均有效，并在与两个基线方法的比较中，在避障的安全性、鲁棒性方面表现出优越性能。

Conclusion: 该算法能够有效避免静态和动态障碍物，在复杂环境中具有良好的安全性与鲁棒性，为自动移动机器人的安全导航提供了一种有效的方法。

Abstract: In this work, we propose a novel motion planning algorithm to facilitate
safety-critical navigation for autonomous mobile robots. The proposed algorithm
integrates a real-time dynamic obstacle tracking and mapping system that
categorizes point clouds into dynamic and static components. For dynamic point
clouds, the Kalman filter is employed to estimate and predict their motion
states. Based on these predictions, we extrapolate the future states of dynamic
point clouds, which are subsequently merged with static point clouds to
construct the forward-time-domain (FTD) map. By combining control barrier
functions (CBFs) with nonlinear model predictive control, the proposed
algorithm enables the robot to effectively avoid both static and dynamic
obstacles. The CBF constraints are formulated based on risk points identified
through collision detection between the predicted future states and the FTD
map. Experimental results from both simulated and real-world scenarios
demonstrate the efficacy of the proposed algorithm in complex environments. In
simulation experiments, the proposed algorithm is compared with two baseline
approaches, showing superior performance in terms of safety and robustness in
obstacle avoidance. The source code is released for the reference of the
robotics community.

</details>


### [207] [Metrics vs Surveys: Can Quantitative Measures Replace Human Surveys in Social Robot Navigation? A Correlation Analysis](https://arxiv.org/abs/2510.02941)
*Stefano Trepella,Mauro Martini,Noé Pérez-Higueras,Andrea Ostuni,Fernando Caballero,Luis Merino,Marcello Chiaberge*

Main category: cs.RO

TL;DR: 现有社交导航指标未能完全捕捉人类感知，需要新指标。


<details>
  <summary>Details</summary>
Motivation: 评估机器人导航系统的成本高、资源消耗大、难以复制和比较，因此需要寻找量化指标替代主观评估。

Method: 通过实验探索量化指标与人类中心评估之间的关系，以确定潜在的相关性。

Result: 现有指标可以捕捉机器人导航行为的某些方面，但未能充分代表重要的主观因素。

Conclusion: 需要开发新的社交导航指标，以更全面地评估机器人导航系统。

Abstract: Social, also called human-aware, navigation is a key challenge for the
integration of mobile robots into human environments. The evaluation of such
systems is complex, as factors such as comfort, safety, and legibility must be
considered. Human-centered assessments, typically conducted through surveys,
provide reliable insights but are costly, resource-intensive, and difficult to
reproduce or compare across systems. Alternatively, numerical social navigation
metrics are easy to compute and facilitate comparisons, yet the community lacks
consensus on a standard set of metrics.
  This work explores the relationship between numerical metrics and
human-centered evaluations to identify potential correlations. If specific
quantitative measures align with human perceptions, they could serve as
standardized evaluation tools, reducing the dependency on surveys. Our results
indicate that while current metrics capture some aspects of robot navigation
behavior, important subjective factors remain insufficiently represented and
new metrics are necessary.

</details>


### [208] [Single-Rod Brachiation Robot: Mechatronic Control Design and Validation of Prejump Phases](https://arxiv.org/abs/2510.02946)
*Juraj Lieskovský,Hijiri Akahane,Aoto Osawa,Jaroslav Bušek,Ikuo Mizuuchi,Tomáš Vyhlídal*

Main category: cs.RO

TL;DR: 一个由单个刚性杆和两端夹持器组成的单摆机器人，通过曲柄滑块机制重新定位质心，并采用最优控制和输入-输出线性化策略实现摆动和旋转。


<details>
  <summary>Details</summary>
Motivation: 设计并实现一个低成本、单刚性杆的单摆机器人，重点关注通过质心重定位实现能量积累以进行跳跃。

Method: 提出基于非线性模型的 bang-bang 控制策略和考虑了扭矩限制和几何结构的连续控制策略，并通过仿真和实验进行验证。

Result: 仿真结果表明，连续控制策略在考虑实际约束的情况下，能够有效地实现单摆运动。实验验证了该策略在低成本控制系统上的可行性。

Conclusion: 所提出的连续控制策略能够有效地驱动单摆机器人，并通过质心重定位实现了能量积累，为未来的跳跃阶段奠定了基础。

Abstract: A complete mechatronic design of a minimal configuration brachiation robot is
presented. The robot consists of a single rigid rod with gripper mechanisms
attached to both ends. The grippers are used to hang the robot on a horizontal
bar on which it swings or rotates. The motion is imposed by repositioning the
robot's center of mass, which is performed using a crank-slide mechanism. Based
on a non-linear model, an optimal control strategy is proposed, for
repositioning the center of mass in a bang-bang manner. Consequently, utilizing
the concept of input-output linearization, a continuous control strategy is
proposed that takes into account the limited torque of the crank-slide
mechanism and its geometry. An increased attention is paid to energy
accumulation towards the subsequent jump stage of the brachiation. These two
strategies are validated and compared in simulations. The continuous control
strategy is then also implemented within a low-cost STM32-based control system,
and both the swing and rotation stages of the brachiation motion are
experimentally validated.

</details>


### [209] [YawSitter: Modeling and Controlling a Tail-Sitter UAV with Enhanced Yaw Control](https://arxiv.org/abs/2510.02968)
*Amir Habel,Fawad Mehboob,Jeffrin Sam,Clement Fortin,Dzmitry Tsetserukou*

Main category: cs.RO

TL;DR: 该论文提出了一种新的尾座式无人机横向运动建模和控制策略，通过引入基于差速螺旋桨滑流效应的侧滑力模型来增强偏航控制能力和横向运动，实现了无滚转耦合的偏航驱动横向位置控制。


<details>
  <summary>Details</summary>
Motivation: 尾座式无人机在悬停状态下实现精确的横向运动建模和解耦控制仍然是一个重大挑战，主要是由于复杂的空气动力学耦合和不明确的横向动力学。

Method: 提出了一种新的建模和控制策略，通过引入基于差速螺旋桨滑流效应的侧滑力模型来增强偏航控制能力和横向运动。控制框架采用YXZ欧拉旋转公式来精确表示姿态并包含重力分量，同时直接控制y轴上的偏航。

Result: 在悬停模式下，通过在Unity环境中进行轨迹跟踪模拟，在矩形和圆形路径上进行了测试，均显示出稳定的性能，平均绝对位置误差较低，并且偏航偏差被限制在5.688度以内。

Conclusion: 所提出的侧滑力生成模型被证明是有效的，为开发灵活、具有悬停能力的尾座式无人机奠定了基础。

Abstract: Achieving precise lateral motion modeling and decoupled control in hover
remains a significant challenge for tail-sitter Unmanned Aerial Vehicles
(UAVs), primarily due to complex aerodynamic couplings and the absence of
welldefined lateral dynamics. This paper presents a novel modeling and control
strategy that enhances yaw authority and lateral motion by introducing a
sideslip force model derived from differential propeller slipstream effects
acting on the fuselage under differential thrust. The resulting lateral force
along the body y-axis enables yaw-based lateral position control without
inducing roll coupling. The control framework employs a YXZ Euler rotation
formulation to accurately represent attitude and incorporate gravitational
components while directly controlling yaw in the yaxis, thereby improving
lateral dynamic behavior and avoiding singularities. The proposed approach is
validated through trajectory-tracking simulations conducted in a Unity-based
environment. Tests on both rectangular and circular paths in hover mode
demonstrate stable performance, with low mean absolute position errors and yaw
deviations constrained within 5.688 degrees. These results confirm the
effectiveness of the proposed lateral force generation model and provide a
foundation for the development of agile, hover-capable tail-sitter UAVs.

</details>


### [210] [AI-Enhanced Kinematic Modeling of Flexible Manipulators Using Multi-IMU Sensor Fusion](https://arxiv.org/abs/2510.02975)
*Amir Hossein Barjini,Jouni Mattila*

Main category: cs.RO

TL;DR: 本文提出了一种基于多IMU（惯性测量单元）的柔性机械臂运动估计新框架，通过优化和校准提高精度。


<details>
  <summary>Details</summary>
Motivation: 为了解决柔性机械臂在垂直运动中姿态和位置估计的挑战。

Method: 将柔性连杆建模为一系列刚性段，使用IMU测量值，并结合互补滤波器和粒子群优化（PSO）算法来估计关节角度和补偿残差误差，最后使用径向基函数神经网络（RBFNN）进行进一步优化。

Result: 该方法在$y$、$z$和$	heta$方向上分别实现了0.00021~m、0.00041~m和0.00024~rad的均方根误差（RMSE）。

Conclusion: 实验结果验证了所提出的智能多IMU运动估计方法的有效性。

Abstract: This paper presents a novel framework for estimating the position and
orientation of flexible manipulators undergoing vertical motion using multiple
inertial measurement units (IMUs), optimized and calibrated with ground truth
data. The flexible links are modeled as a series of rigid segments, with joint
angles estimated from accelerometer and gyroscope measurements acquired by
cost-effective IMUs. A complementary filter is employed to fuse the
measurements, with its parameters optimized through particle swarm optimization
(PSO) to mitigate noise and delay. To further improve estimation accuracy,
residual errors in position and orientation are compensated using radial basis
function neural networks (RBFNN). Experimental results validate the
effectiveness of the proposed intelligent multi-IMU kinematic estimation
method, achieving root mean square errors (RMSE) of 0.00021~m, 0.00041~m, and
0.00024~rad for $y$, $z$, and $\theta$, respectively.

</details>


### [211] [Real-Time Nonlinear Model Predictive Control of Heavy-Duty Skid-Steered Mobile Platform for Trajectory Tracking Tasks](https://arxiv.org/abs/2510.02976)
*Alvaro Paz,Pauli Mustalahti,Mohammad Dastranj,Jouni Mattila*

Main category: cs.RO

TL;DR: 该论文提出了一个用于重型滑板转向移动平台的实时最优轨迹跟踪控制框架。


<details>
  <summary>Details</summary>
Motivation: 为了在存在不确定性和干扰的情况下，确保重型滑板转向移动平台的安全性和稳定性，需要精确的实时控制。

Method: 提出了一种多重拍摄非线性模型预测控制框架，结合了合适的算法和传感器读数，以实现高精度和实时性。

Result: 该控制器在跟踪不同轨迹时表现出高度期望的性能，速度和精度均表现优异，并与现有控制器相比有显著改进。

Conclusion: 所提出的多重拍摄非线性模型预测控制框架能够实现重型滑板转向移动平台的精确、实时的轨迹跟踪，并在性能上优于现有方法。

Abstract: This paper presents a framework for real-time optimal controlling of a
heavy-duty skid-steered mobile platform for trajectory tracking. The importance
of accurate real-time performance of the controller lies in safety
considerations of situations where the dynamic system under control is affected
by uncertainties and disturbances, and the controller should compensate for
such phenomena in order to provide stable performance. A multiple-shooting
nonlinear model-predictive control framework is proposed in this paper. This
framework benefits from suitable algorithm along with readings from various
sensors for genuine real-time performance with extremely high accuracy. The
controller is then tested for tracking different trajectories where it
demonstrates highly desirable performance in terms of both speed and accuracy.
This controller shows remarkable improvement when compared to existing
nonlinear model-predictive controllers in the literature that were implemented
on skid-steered mobile platforms.

</details>


### [212] [3D-CovDiffusion: 3D-Aware Diffusion Policy for Coverage Path Planning](https://arxiv.org/abs/2510.03011)
*Chenyuan Chen,Haoran Ding,Ran Ding,Tianyu Liu,Zewen He,Anqing Duan,Dezhen Song,Xiaodan Liang,Yoshihiko Nakamura*

Main category: cs.RO

TL;DR: 本研究提出了一种基于扩散模型的端到端框架，用于生成长而平滑的机器人轨迹，以实现高表面覆盖率，并成功应用于抛光、喷漆和喷涂等工业任务。


<details>
  <summary>Details</summary>
Motivation: 传统轨迹生成方法受限于预定义函数形式，难以处理复杂多样的任务，泛化能力差，需要手动重新设计或进行参数调整。因此，需要更具表现力的生成模型，而扩散模型为此类任务提供了一个有前景的解决方案。

Method: 通过迭代去噪轨迹，并结合精心学习的噪声调度和条件机制，利用扩散模型生成平滑、一致且能灵活适应任务上下文的运动轨迹。

Result: 实验结果表明，该方法在轨迹连续性、表面覆盖率和泛化能力方面均优于现有方法。具体而言，在点到点倒角距离、平滑度和表面覆盖率方面分别提高了 98.2%、97.0% 和 61%。

Conclusion: 所提出的基于扩散模型的端到端轨迹生成框架能够有效地用于各种工业表面处理任务，无需特定类别的模型，从而为统一的轨迹学习提供了新的途径。

Abstract: Diffusion models, as a class of deep generative models, have recently emerged
as powerful tools for robot skills by enabling stable training with reliable
convergence. In this paper, we present an end-to-end framework for generating
long, smooth trajectories that explicitly target high surface coverage across
various industrial tasks, including polishing, robotic painting, and spray
coating. The conventional methods are always fundamentally constrained by their
predefined functional forms, which limit the shapes of the trajectories they
can represent and make it difficult to handle complex and diverse tasks.
Moreover, their generalization is poor, often requiring manual redesign or
extensive parameter tuning when applied to new scenarios. These limitations
highlight the need for more expressive generative models, making
diffusion-based approaches a compelling choice for trajectory generation. By
iteratively denoising trajectories with carefully learned noise schedules and
conditioning mechanisms, diffusion models not only ensure smooth and consistent
motion but also flexibly adapt to the task context. In experiments, our method
improves trajectory continuity, maintains high coverage, and generalizes to
unseen shapes, paving the way for unified end-to-end trajectory learning across
industrial surface-processing tasks without category-specific models. On
average, our approach improves Point-wise Chamfer Distance by 98.2\% and
smoothness by 97.0\%, while increasing surface coverage by 61\% compared to
prior methods. The link to our code can be found
\href{https://anonymous.4open.science/r/spraydiffusion_ral-2FCE/README.md}{here}.

</details>


### [213] [HumanoidExo: Scalable Whole-Body Humanoid Manipulation via Wearable Exoskeleton](https://arxiv.org/abs/2510.03022)
*Rui Zhong,Yizhe Sun,Junjie Wen,Jinming Li,Chuang Cheng,Wei Dai,Zhiwen Zeng,Huimin Lu,Yichen Zhu,Yi Xu*

Main category: cs.RO

TL;DR: 通过将人类运动转移到全身人形数据来解决人形策略学习中的数据收集瓶颈。


<details>
  <summary>Details</summary>
Motivation: 收集大规模、多样化的人形数据集既困难又昂贵，这是人形策略学习中的一个重大瓶颈。

Method: HumanoidExo系统将人类运动转移到全身人形数据，以最小化人类演示者和机器人之间的具身鸿沟。

Result: HumanoidExo能够实现比仅使用真实机器人数据更广泛的泛化能力、从少量真实机器人演示中学习复杂全身控制以及仅从HumanoidExo数据中学习新技能。

Conclusion: HumanoidExo是一个关键的补充，可以收集更多、更全面的人形机器人数据集，从而提高机器人在动态、真实世界的场景中的性能。

Abstract: A significant bottleneck in humanoid policy learning is the acquisition of
large-scale, diverse datasets, as collecting reliable real-world data remains
both difficult and cost-prohibitive. To address this limitation, we introduce
HumanoidExo, a novel system that transfers human motion to whole-body humanoid
data. HumanoidExo offers a high-efficiency solution that minimizes the
embodiment gap between the human demonstrator and the robot, thereby tackling
the scarcity of whole-body humanoid data. By facilitating the collection of
more voluminous and diverse datasets, our approach significantly enhances the
performance of humanoid robots in dynamic, real-world scenarios. We evaluated
our method across three challenging real-world tasks: table-top manipulation,
manipulation integrated with stand-squat motions, and whole-body manipulation.
Our results empirically demonstrate that HumanoidExo is a crucial addition to
real-robot data, as it enables the humanoid policy to generalize to novel
environments, learn complex whole-body control from only five real-robot
demonstrations, and even acquire new skills (i.e., walking) solely from
HumanoidExo data.

</details>


### [214] [Long-Term Human Motion Prediction Using Spatio-Temporal Maps of Dynamics](https://arxiv.org/abs/2510.03031)
*Yufei Zhu,Andrey Rudenko,Tomasz P. Kucner,Achim J. Lilienthal,Martin Magnusson*

Main category: cs.RO

TL;DR: 该研究提出了一种利用“动态地图”（MoDs）进行长期人类运动预测的框架，能够预测长达60秒的运动轨迹，并在实际应用中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 长期人类运动预测对于涉及人类的自主机器人和车辆的安全高效运行至关重要，其准确性对运动规划、跟踪、人机交互和安全监控等应用具有重要意义。

Method: 提出了一种信息框架，该框架利用“动态地图”（MoDs）来编码环境特征中的时空运动模式，并包含一个排名方法来输出最可能的预测轨迹。此外，还引入了一种时间条件MoD来捕捉一天中不同时间的运动模式变化。该框架与三种MoDs结合进行了评估。

Result: 在两个真实世界数据集上的实验表明，所提出的MoD信息框架优于基于学习的方法，平均位移误差最多可提高50%。时间条件MoD变体实现了最高的整体准确性。

Conclusion: 所提出的MoD信息框架能够有效地进行长期人类运动预测，并且在结合时间条件MoD时能够达到最佳的预测精度。

Abstract: Long-term human motion prediction (LHMP) is important for the safe and
efficient operation of autonomous robots and vehicles in environments shared
with humans. Accurate predictions are important for applications including
motion planning, tracking, human-robot interaction, and safety monitoring. In
this paper, we exploit Maps of Dynamics (MoDs), which encode spatial or
spatio-temporal motion patterns as environment features, to achieve LHMP for
horizons of up to 60 seconds. We propose an MoD-informed LHMP framework that
supports various types of MoDs and includes a ranking method to output the most
likely predicted trajectory, improving practical utility in robotics. Further,
a time-conditioned MoD is introduced to capture motion patterns that vary
across different times of day. We evaluate MoD-LHMP instantiated with three
types of MoDs. Experiments on two real-world datasets show that MoD-informed
method outperforms learning-based ones, with up to 50\% improvement in average
displacement error, and the time-conditioned variant achieves the highest
accuracy overall. Project code is available at
https://github.com/test-bai-cpu/LHMP-with-MoDs.git

</details>


### [215] [Embracing Evolution: A Call for Body-Control Co-Design in Embodied Humanoid Robot](https://arxiv.org/abs/2510.03081)
*Guiliang Liu,Bo Yue,Yi Jin Kim,Kui Jia*

Main category: cs.RO

TL;DR: 该论文提出了一种“协同设计”机制，用于在人形机器人设计中同时进化控制策略和物理结构，以提高其在复杂环境中的适应性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要集中于优化固定结构机器人的控制策略，而忽视了机器人形态和控制策略之间的协同进化，作者认为这阻碍了人形机器人实现真正的泛化能力。

Method: 提出了一种受生物进化启发的协同设计方法，通过迭代地调整机器人的形态和行为，以在特定任务和资源限制下优化性能。具体方法包括战略探索、仿真到现实（Sim2Real）迁移和元策略学习。

Result: 该论文通过分析方法论、应用驱动和社区导向等角度，论证了协同设计在人形机器人领域的重要性，并提出了一系列开放性研究问题，以指导未来的研究方向。

Conclusion: 协同设计是开发下一代智能和适应性强的人形机器人的关键，能够使机器人更好地适应真实世界的多样化环境。

Abstract: Humanoid robots, as general-purpose physical agents, must integrate both
intelligent control and adaptive morphology to operate effectively in diverse
real-world environments. While recent research has focused primarily on
optimizing control policies for fixed robot structures, this position paper
argues for evolving both control strategies and humanoid robots' physical
structure under a co-design mechanism. Inspired by biological evolution, this
approach enables robots to iteratively adapt both their form and behavior to
optimize performance within task-specific and resource-constrained contexts.
Despite its promise, co-design in humanoid robotics remains a relatively
underexplored domain, raising fundamental questions about its feasibility and
necessity in achieving true embodied intelligence. To address these challenges,
we propose practical co-design methodologies grounded in strategic exploration,
Sim2Real transfer, and meta-policy learning. We further argue for the essential
role of co-design by analyzing it from methodological, application-driven, and
community-oriented perspectives. Striving to guide and inspire future studies,
we present open research questions, spanning from short-term innovations to
long-term goals. This work positions co-design as a cornerstone for developing
the next generation of intelligent and adaptable humanoid agents.

</details>


### [216] [Whisker-based Tactile Flight for Tiny Drones](https://arxiv.org/abs/2510.03119)
*Chaoxiang Ye,Guido de Croon,Salua Hamaza*

Main category: cs.RO

TL;DR: 微型无人机可通过仿生触觉传感器在极端环境下进行视觉无关导航。


<details>
  <summary>Details</summary>
Motivation: 传统微型飞行机器人受限于尺寸，在光线不佳、烟雾、灰尘或反光障碍物等条件下难以进行有效感知，限制了其在搜索救援、安全检查和环境监测等领域的应用。本项目旨在为微型无人机开发一种轻量级的仿生触觉传感系统，以克服这些限制。

Method: 提出了一种基于气压计的仿生触须传感装置，重量仅为3.2克。该装置模仿自然界中鼠类和鼹鼠的触觉感知方式，使无人机能够在飞行中通过轻柔的物理接触来感知周围环境，从而实现即使在完全黑暗的条件下也能进行导航和探索。开发了一种触觉深度估计方法来处理传感器噪声和漂移，实现了亚毫米级的精度。

Result: 该系统成功实现了无人机仅通过触觉即可进行导航、轮廓感知和密闭空间探索，即使在完全黑暗的环境下，也能在软硬表面上进行操作。整个系统可以在仅有192KB RAM的微控制器上完全离线运行，支持自主触觉飞行，并在仿真和真实世界测试中得到了验证。

Conclusion: 本项目提出的仿生触觉传感方法为微型飞行器在极端环境下的视觉无关导航提供了新的解决方案，拓展了微型飞行器在恶劣条件下的应用潜力。

Abstract: Tiny flying robots hold great potential for search-and-rescue, safety
inspections, and environmental monitoring, but their small size limits
conventional sensing-especially with poor-lighting, smoke, dust or reflective
obstacles. Inspired by nature, we propose a lightweight, 3.2-gram,
whisker-based tactile sensing apparatus for tiny drones, enabling them to
navigate and explore through gentle physical interaction. Just as rats and
moles use whiskers to perceive surroundings, our system equips drones with
tactile perception in flight, allowing obstacle sensing even in pitch-dark
conditions. The apparatus uses barometer-based whisker sensors to detect
obstacle locations while minimising destabilisation. To address sensor noise
and drift, we develop a tactile depth estimation method achieving sub-6 mm
accuracy. This enables drones to navigate, contour obstacles, and explore
confined spaces solely through touch-even in total darkness along both soft and
rigid surfaces. Running fully onboard a 192-KB RAM microcontroller, the system
supports autonomous tactile flight and is validated in both simulation and
real-world tests. Our bio-inspired approach redefines vision-free navigation,
opening new possibilities for micro aerial vehicles in extreme environments.

</details>


### [217] [Learning Stability Certificate for Robotics in Real-World Environments](https://arxiv.org/abs/2510.03123)
*Zhe Shen*

Main category: cs.RO

TL;DR: 该研究提出了一种无需系统模型即可从轨迹数据中学习李雅普诺夫函数以验证机器人系统稳定性新框架。


<details>
  <summary>Details</summary>
Motivation: 传统上，为复杂、未知的系统推导稳定性证明需要显式了解系统动力学，这通常是一项艰巨的任务。

Method: 通过使用神经网络参数化李雅普诺夫候选函数，并确保正定性，该框架直接从轨迹数据中学习李雅普诺夫函数，以自动识别系统是否稳定。

Result: 该框架能够提供数据驱动的稳定性保证，即使在数据嘈杂的情况下也能为机器人系统提供稳健的安全认证。

Conclusion: 该方法无需访问内部控制算法即可工作，适用于系统行为不透明或专有的情况，并为学习稳定性证明提供了开源工具。

Abstract: Stability certificates play a critical role in ensuring the safety and
reliability of robotic systems. However, deriving these certificates for
complex, unknown systems has traditionally required explicit knowledge of
system dynamics, often making it a daunting task. This work introduces a novel
framework that learns a Lyapunov function directly from trajectory data,
enabling the certification of stability for autonomous systems without needing
detailed system models. By parameterizing the Lyapunov candidate using a neural
network and ensuring positive definiteness through Cholesky factorization, our
approach automatically identifies whether the system is stable under the given
trajectory. To address the challenges posed by noisy, real-world data, we allow
for controlled violations of the stability condition, focusing on maintaining
high confidence in the stability certification process. Our results demonstrate
that this framework can provide data-driven stability guarantees, offering a
robust method for certifying the safety of robotic systems in dynamic,
real-world environments. This approach works without access to the internal
control algorithms, making it applicable even in situations where system
behavior is opaque or proprietary. The tool for learning the stability proof is
open-sourced by this research: https://github.com/HansOersted/stability.

</details>


### [218] [MM-Nav: Multi-View VLA Model for Robust Visual Navigation via Multi-Expert Learning](https://arxiv.org/abs/2510.03142)
*Tianyu Xu,Jiawei Chen,Jiazhao Zhang,Wenyao Zhang,Zekun Qi,Minghan Li,Zhizheng Zhang,He Wang*

Main category: cs.RO

TL;DR: 该研究提出了一种利用视觉-语言-动作（VLA）模型通过师生学习方法从合成专家数据中学习导航能力的方法。


<details>
  <summary>Details</summary>
Motivation: 视觉导航策略因能模仿人类的观察方式而被广泛关注，但光学信息难以显式建模，需要智能模型和大规模数据。因此，本研究旨在利用VLA模型的智能来学习多样化的导航能力。

Method: 提出了一种名为MM-Nav的多视图（360度观测）VLA模型，该模型基于预训练的大型语言模型和视觉基础模型。通过在三个定制的环境中收集专家数据（来自具有深度信息优势的强化学习专家），并以师生学习方式迭代训练VLA模型，同时根据各能力表现动态调整训练比例。

Result: 在合成环境中，该模型展现了强大的泛化能力，并且优于其强化学习教师模型，证明了整合多种能力的协同效应。真实世界实验也证实了该方法的有效性。

Conclusion: 所提出的MM-Nav方法利用VLA模型和师生学习策略，成功地从合成数据中学习并超越了专家导航能力，并在真实世界中得到了验证。

Abstract: Visual navigation policy is widely regarded as a promising direction, as it
mimics humans by using egocentric visual observations for navigation. However,
optical information of visual observations is difficult to be explicitly
modeled like LiDAR point clouds or depth maps, which subsequently requires
intelligent models and large-scale data. To this end, we propose to leverage
the intelligence of the Vision-Language-Action (VLA) model to learn diverse
navigation capabilities from synthetic expert data in a teacher-student manner.
Specifically, we implement the VLA model, MM-Nav, as a multi-view VLA (with 360
observations) based on pretrained large language models and visual foundation
models. For large-scale navigation data, we collect expert data from three
reinforcement learning (RL) experts trained with privileged depth information
in three challenging tailor-made environments for different navigation
capabilities: reaching, squeezing, and avoiding. We iteratively train our VLA
model using data collected online from RL experts, where the training ratio is
dynamically balanced based on performance on individual capabilities. Through
extensive experiments in synthetic environments, we demonstrate that our model
achieves strong generalization capability. Moreover, we find that our student
VLA model outperforms the RL teachers, demonstrating the synergistic effect of
integrating multiple capabilities. Extensive real-world experiments further
confirm the effectiveness of our method.

</details>


### [219] [Optimal Smooth Coverage Trajectory Planning for Quadrotors in Cluttered Environment](https://arxiv.org/abs/2510.03169)
*Duanjiao Li,Yun Chen,Ying Zhang,Junwen Yao,Dongyue Huang,Jianguo Zhang,Ning Ding*

Main category: cs.RO

TL;DR: 该算法通过遗传算法优化兴趣点访问顺序，并采用非线性最小二乘法优化轨迹，以实现UAV在复杂环境下的平滑覆盖。 


<details>
  <summary>Details</summary>
Motivation: 为解决UAV在电力巡检场景中在复杂环境中覆盖规划的挑战，提出一种最优平滑覆盖轨迹规划算法。

Method: 算法分为两阶段：第一阶段，利用遗传算法（GA）解决兴趣点（POIs）旅行商问题（TSP），生成初始访问点序列；第二阶段，将轨迹平滑度、时间消耗和避障等约束纳入非线性最小二乘问题进行求解，得到平滑覆盖轨迹。

Result: 数值模拟验证了该算法的有效性，确保UAV能够在复杂环境中平滑地覆盖所有兴趣点。

Conclusion: 所提出的算法能够有效地为UAV规划在复杂环境下的平滑覆盖轨迹，满足实际应用需求。

Abstract: For typical applications of UAVs in power grid scenarios, we construct the
problem as planning UAV trajectories for coverage in cluttered environments. In
this paper, we propose an optimal smooth coverage trajectory planning
algorithm. The algorithm consists of two stages. In the front-end, a Genetic
Algorithm (GA) is employed to solve the Traveling Salesman Problem (TSP) for
Points of Interest (POIs), generating an initial sequence of optimized visiting
points. In the back-end, the sequence is further optimized by considering
trajectory smoothness, time consumption, and obstacle avoidance. This is
formulated as a nonlinear least squares problem and solved to produce a smooth
coverage trajectory that satisfies these constraints. Numerical simulations
validate the effectiveness of the proposed algorithm, ensuring UAVs can
smoothly cover all POIs in cluttered environments.

</details>


### [220] [Simulation to Rules: A Dual-VLM Framework for Formal Visual Planning](https://arxiv.org/abs/2510.03182)
*Yilun Hao,Yongchao Chen,Chuchu Fan,Yang Zhang*

Main category: cs.RO

TL;DR: VLMFP是一个双VLM框架，可以自动生成用于形式化视觉规划的PDDL问题和域文件，克服了现有方法在生成PDDL域文件方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型（VLM）在视觉规划方面显示出潜力，但在精确的空间和长期推理方面存在不足，而PDDL规划器在长期形式化规划方面表现出色，但无法解释视觉输入。先前结合两者的工作在生成PDDL域文件方面存在困难，需要人工干预或持续的环境访问。因此，有必要开发一种能够自主生成PDDL问题和域文件的框架。

Method: VLMFP框架采用两个VLM：SimVLM模拟动作后果，GenVLM生成并迭代优化PDDL文件。GenVLM通过比较PDDL和SimVLM的执行结果来改进PDDL文件。

Result: 在6个grid-world域上的评估显示，SimVLM在模拟动作后果和判断目标达成方面表现出色，准确率分别达到85.5%-87.8%和82.4%-85.6%。VLMFP生成的PDDL文件在未见过的实例和外观下，有效规划的达成率分别为70.0%和54.1%。

Conclusion: VLMFP框架能够自主生成PDDL问题和域文件，并在多种场景下展现出良好的泛化能力，解决了现有方法在PDDL域文件生成方面的局限性。

Abstract: Vision Language Models (VLMs) show strong potential for visual planning but
struggle with precise spatial and long-horizon reasoning. In contrast, Planning
Domain Definition Language (PDDL) planners excel at long-horizon formal
planning, but cannot interpret visual inputs. Recent works combine these
complementary advantages by enabling VLMs to turn visual planning problems into
PDDL files for formal planning. However, while VLMs can generate PDDL problem
files satisfactorily, they struggle to accurately generate the PDDL domain
files, which describe all the planning rules. As a result, prior methods rely
on human experts to predefine domain files or on constant environment access
for refinement. We propose VLMFP, a Dual-VLM-guided framework that can
autonomously generate both PDDL problem and domain files for formal visual
planning. VLMFP introduces two VLMs to ensure reliable PDDL file generation: A
SimVLM that simulates action consequences based on input rule descriptions, and
a GenVLM that generates and iteratively refines PDDL files by comparing the
PDDL and SimVLM execution results. VLMFP unleashes multiple levels of
generalizability: The same generated PDDL domain file works for all the
different instances under the same problem, and VLMs generalize to different
problems with varied appearances and rules. We evaluate VLMFP with 6 grid-world
domains and test its generalization to unseen instances, appearance, and game
rules. On average, SimVLM accurately describes 95.5%, 82.6% of scenarios,
simulates 85.5%, 87.8% of action sequence, and judges 82.4%, 85.6% goal
reaching for seen and unseen appearances, respectively. With the guidance of
SimVLM, VLMFP can generate PDDL files to reach 70.0%, 54.1% valid plans for
unseen instances in seen and unseen appearances, respectively. Project page:
https://sites.google.com/view/vlmfp.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [221] [BrowserArena: Evaluating LLM Agents on Real-World Web Navigation Tasks](https://arxiv.org/abs/2510.02418)
*Sagnik Anupam,Davis Brown,Shuo Li,Eric Wong,Hamed Hassani,Osbert Bastani*

Main category: cs.AI

TL;DR: LLM网页代理的评估平台BrowserArena通过用户提交任务、头对头比较和分步人工反馈来识别和分析故障模式，如验证码处理、弹窗移除和URL导航。通过构建针对性数据集，我们发现不同模型在处理这些故障模式时表现出不同的策略和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM网页代理的评估方法受限于沙盒环境或人造任务，无法真实反映其在开放网络上的表现。我们需要一个能在真实网络环境中评估网页代理的平台，并深入了解其失败模式。

Method: 我们引入了BrowserArena，一个实时的开放网络代理评估平台。该平台收集用户提交的任务，进行Arena风格的头对头比较，并利用分步的人工反馈来识别代理的失败模式。我们收集并分析了代理轨迹的分步注释，以识别常见的失败模式。

Result: 通过分析分步注释，我们发现了三种常见的失败模式：验证码处理、弹窗移除和URL直接导航。针对这些任务构建了有针对性的数据集，进一步研究发现不同语言模型在处理这些失败模式时存在差异。例如，o4-mini在规避验证码方面比其他模型采用了更多样化的策略，而DeepSeek-R1在验证码处理方面则持续误导用户。

Conclusion: LLM网页代理在开放网络上展现出多样性但同时也存在脆弱性。我们的基准测试方法为大规模评估和理解网页代理的失败模式提供了一种途径。

Abstract: LLM web agents now browse and take actions on the open web, yet current agent
evaluations are constrained to sandboxed environments or artificial tasks. We
introduce BrowserArena, a live open-web agent evaluation platform that collects
user-submitted tasks, runs Arena-style head-to-head comparisons, and uses
step-level human feedback to surface failure modes. Collecting and analyzing
step-level annotations on the agent traces, we identify three consistent
failure modes: captcha resolution, pop-up banner removal, and direct navigation
to URLs. By constructing targeted datasets to further study these tasks, we
discover variations in how different language models navigate these failure
modes. We find, for example, that o4-mini deploys a wider variety of strategies
to circumvent captcha resolution than other models and DeepSeek-R1 consistently
misleads users about captcha resolution. Our findings surface both the
diversity and brittleness of current web agents. More broadly, our benchmarking
methodology provides an approach to evaluating and understanding web agent
failure modes at scale.

</details>


### [222] [Improving Cooperation in Collaborative Embodied AI](https://arxiv.org/abs/2510.03153)
*Hima Jacob Leven Suprabha,Laxmi Nag Laxminarayan Nagesh,Ajith Nair,Alvin Reuben Amal Selvaster,Ayan Khan,Raghuram Damarla,Sanju Hannah Samuel,Sreenithi Saravana Perumal,Titouan Puech,Venkataramireddy Marella,Vishal Sonar,Alessandro Suglia,Oliver Lemon*

Main category: cs.AI

TL;DR: LLMs可以集成到多智能体系统中以增强协作和决策。本文通过实验评估了不同的提示方法和LLM，并对CoELA框架进行了改进，同时集成了语音功能。结果显示，提示优化可提高系统性能，Gemma3模型的效率提升了22%。


<details>
  <summary>Details</summary>
Motivation: The integration of Large Language Models (LLMs) into multiagent systems has opened new possibilities for collaborative reasoning and cooperation with AI agents. This paper explores different prompting methods and evaluates their effectiveness in enhancing agent collaborative behaviour and decision-making.

Method: We enhance CoELA, a framework designed for building Collaborative Embodied Agents that leverage LLMs for multi-agent communication, reasoning, and task coordination in shared virtual spaces. Through systematic experimentation, we examine different LLMs and prompt engineering strategies to identify optimised combinations that maximise collaboration performance. Furthermore, we extend our research by integrating speech capabilities, enabling seamless collaborative voice-based interactions.

Result: Our findings highlight the effectiveness of prompt optimisation in enhancing collaborative agent performance; for example, our best combination improved the efficiency of the system running with Gemma3 by 22% compared to the original CoELA system. In addition, the speech integration provides a more engaging user interface for iterative system development and demonstrations.

Conclusion: Prompt optimization is effective in enhancing collaborative agent performance. Speech integration provides a more engaging user interface.

Abstract: The integration of Large Language Models (LLMs) into multiagent systems has
opened new possibilities for collaborative reasoning and cooperation with AI
agents. This paper explores different prompting methods and evaluates their
effectiveness in enhancing agent collaborative behaviour and decision-making.
We enhance CoELA, a framework designed for building Collaborative Embodied
Agents that leverage LLMs for multi-agent communication, reasoning, and task
coordination in shared virtual spaces. Through systematic experimentation, we
examine different LLMs and prompt engineering strategies to identify optimised
combinations that maximise collaboration performance. Furthermore, we extend
our research by integrating speech capabilities, enabling seamless
collaborative voice-based interactions. Our findings highlight the
effectiveness of prompt optimisation in enhancing collaborative agent
performance; for example, our best combination improved the efficiency of the
system running with Gemma3 by 22% compared to the original CoELA system. In
addition, the speech integration provides a more engaging user interface for
iterative system development and demonstrations.

</details>


### [223] [RefineShot: Rethinking Cinematography Understanding with Foundational Skill Evaluation](https://arxiv.org/abs/2510.02423)
*Hang Wu,Yujun Cai,Haonan Ge,Hongkai Chen,Ming-Hsuan Yang,Yiwei Wang*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Cinematography understanding refers to the ability to recognize not only the
visual content of a scene but also the cinematic techniques that shape
narrative meaning. This capability is attracting increasing attention, as it
enhances multimodal understanding in real-world applications and underpins
coherent content creation in film and media. As the most comprehensive
benchmark for this task, ShotBench spans a wide range of cinematic concepts and
VQA-style evaluations, with ShotVL achieving state-of-the-art results on it.
However, our analysis reveals that ambiguous option design in ShotBench and
ShotVL's shortcomings in reasoning consistency and instruction adherence
undermine evaluation reliability, limiting fair comparison and hindering future
progress. To overcome these issues, we systematically refine ShotBench through
consistent option restructuring, conduct the first critical analysis of
ShotVL's reasoning behavior, and introduce an extended evaluation protocol that
jointly assesses task accuracy and core model competencies. These efforts lead
to RefineShot, a refined and expanded benchmark that enables more reliable
assessment and fosters future advances in cinematography understanding.

</details>


### [224] [Safe and Efficient In-Context Learning via Risk Control](https://arxiv.org/abs/2510.02480)
*Andrea Wynn,Metod Jazbec,Charith Peris,Rinat Khaziev,Anqi Liu,Daniel Khashabi,Eric Nalisnick*

Main category: cs.AI

TL;DR: LLM安全问题可以通过基于DFRC的风险控制来缓解，该方法通过动态早期退出和注意力头忽略来防止有害演示的影响，同时在有益演示上保持效率。


<details>
  <summary>Details</summary>
Motivation: LLM的灵活性带来了安全隐患，因为它们可能会受到不正确或恶意演示的影响，因此需要内置的防御机制。

Method: 提出了一种基于分布无关风险控制（DFRC）的新方法，结合动态早期退出和注意力头忽略，以限制有害演示对模型性能的负面影响，并对DFRC进行了修改以兼顾有害和有益输入的风险控制与性能提升。

Result: 理论和实验结果表明，该方法能有效控制有害演示带来的风险，并能在有益演示上实现显著的计算效率提升。

Conclusion: 所提出的方法能够有效地控制LLM在面对有害的上下文示例时产生的风险，同时还能在处理有益示例时提高模型的效率。

Abstract: Large language models (LLMs) demonstrate a remarkable ability to learn new
tasks from a few in-context examples. However, this flexibility introduces
safety concerns: LLMs can be influenced by incorrect or malicious
demonstrations -- for example, if an adversary tampers with or injects harmful
examples without a human supervisor noticing. This motivates principled designs
in which the system itself includes built-in mechanisms to guard against such
attacks. We propose a novel approach to limit the degree to which harmful
demonstrations can degrade model performance. First, we define a baseline
``safe'' behavior for the model -- the model's performance given no in-context
demonstrations (zero-shot). Next, we apply distribution-free risk control
(DFRC) to control the extent to which in-context samples can decay performance
below zero-shot. We achieve this by leveraging dynamic early exit prediction,
ignoring later attention heads that attend the most to the unsafe inputs.
Finally, we propose modifications to DFRC that allow it to both control risk
for harmful inputs \textit{and} leverage performance and efficiency gains on
helpful inputs. We present both theoretical and empirical results showing that
our approach can effectively control risk for harmful in-context demonstrations
while simultaneously achieving substantial computational efficiency gains with
helpful demonstrations.

</details>


### [225] [Multimodal Function Vectors for Spatial Relations](https://arxiv.org/abs/2510.02528)
*Shuhao Fu,Esther Goldberg,Ying Nian Wu,Hongjing Lu*

Main category: cs.AI

TL;DR: 识别并操纵大型多模态模型(LMM)中负责空间关系表征的注意力头，以提高其关系推理能力。


<details>
  <summary>Details</summary>
Motivation: 大型多模态模型(LMM)在小规模多模态演示中展现出令人印象深刻的上下文学习能力，但其内部机制尚不明确。

Method: 利用因果中介分析识别影响关系预测的注意力头，提取并优化多模态函数向量，并对这些向量进行微调，同时保持LMM参数冻结，最后通过线性组合来解决类比问题。

Result: 识别出一小组注意力头（函数向量）负责传输空间关系表征。提取的函数向量可以提高零样本准确性，并且在微调后可以显著优于上下文学习基线。关系特定的函数向量可以通过线性组合来解决新颖和未训练的空间关系类比问题。

Conclusion: LMM在局部的内部结构中编码空间关系知识，这些知识可以被系统地提取和优化，从而增进对模型模块化的理解，并增强对LMM中关系推理的控制。

Abstract: Large Multimodal Models (LMMs) demonstrate impressive in-context learning
abilities from limited multimodal demonstrations, yet the internal mechanisms
supporting such task learning remain opaque. Building on prior work of large
language models, we show that a small subset of attention heads in the
vision-language model OpenFlamingo-4B is responsible for transmitting
representations of spatial relations. The activations of these attention heads,
termed function vectors, can be extracted and manipulated to alter an LMM's
performance on relational tasks. First, using both synthetic and real image
datasets, we apply causal mediation analysis to identify attention heads that
strongly influence relational predictions, and extract multimodal function
vectors that improve zero-shot accuracy at inference time. We further
demonstrate that these multimodal function vectors can be fine-tuned with a
modest amount of training data, while keeping LMM parameters frozen, to
significantly outperform in-context learning baselines. Finally, we show that
relation-specific function vectors can be linearly combined to solve analogy
problems involving novel and untrained spatial relations, highlighting the
strong generalization ability of this approach. Our results show that LMMs
encode spatial relational knowledge within localized internal structures, which
can be systematically extracted and optimized, thereby advancing our
understanding of model modularity and enhancing control over relational
reasoning in LMMs.

</details>


### [226] [Orchestrating Human-AI Teams: The Manager Agent as a Unifying Research Challenge](https://arxiv.org/abs/2510.02557)
*Charlie Masters,Advaith Vellanki,Jiangbo Shangguan,Bart Kultys,Jonathan Gilmore,Alastair Moore,Stefano V. Albrecht*

Main category: cs.AI

TL;DR: 该研究提出了一种名为“自主管理器代理”的自主代理系统，用于协调复杂的人工智能-人类团队中的多代理工作流。该代理能够分解目标、分配任务、监控进度、适应变化并与利益相关者沟通。研究将工作流管理形式化为部分可观察随机博弈，并确定了四个关键挑战：组合推理、多目标优化、即席团队协调和治理与合规性。研究还发布了MA-Gym仿真框架，并通过评估基于GPT-5的管理器代理，发现它们在目标完成、约束遵守和运行时间方面难以协同优化，突显了工作流管理作为一个开放性难题。最后，讨论了自主管理系统的组织和伦理影响。


<details>
  <summary>Details</summary>
Motivation: 解决在自动化单个任务方面取得进展的 Agentic AI 在管理复杂的多代理工作流方面仍然存在挑战。

Method: 提出自主管理器代理（Autonomous Manager Agent），该代理能够将复杂目标分解为任务图，将任务分配给人类和人工智能工作者，监控进度，适应不断变化的情况，并保持与利益相关者的透明沟通。将工作流管理形式化为部分可观察随机博弈（Partially Observable Stochastic Game），并确定了四个基础性挑战：1. 层次分解的组合推理；2. 转移偏好下的多目标优化；3. 即席团队中的协调与规划；4. 设计中的治理与合规性。发布了MA-Gym，一个用于多代理工作流编排的开源仿真和评估框架。

Result: 使用基于GPT-5的管理器代理在20个工作流上进行评估，发现在联合优化目标完成、约束遵守和工作流运行时间方面存在困难。

Conclusion: 工作流管理仍然是一个困难的开放性问题。最后讨论了自主管理系统的组织和伦理影响。

Abstract: While agentic AI has advanced in automating individual tasks, managing
complex multi-agent workflows remains a challenging problem. This paper
presents a research vision for autonomous agentic systems that orchestrate
collaboration within dynamic human-AI teams. We propose the Autonomous Manager
Agent as a core challenge: an agent that decomposes complex goals into task
graphs, allocates tasks to human and AI workers, monitors progress, adapts to
changing conditions, and maintains transparent stakeholder communication. We
formalize workflow management as a Partially Observable Stochastic Game and
identify four foundational challenges: (1) compositional reasoning for
hierarchical decomposition, (2) multi-objective optimization under shifting
preferences, (3) coordination and planning in ad hoc teams, and (4) governance
and compliance by design. To advance this agenda, we release MA-Gym, an
open-source simulation and evaluation framework for multi-agent workflow
orchestration. Evaluating GPT-5-based Manager Agents across 20 workflows, we
find they struggle to jointly optimize for goal completion, constraint
adherence, and workflow runtime - underscoring workflow management as a
difficult open problem. We conclude with organizational and ethical
implications of autonomous management systems.

</details>


### [227] [Agentic Additive Manufacturing Alloy Discovery](https://arxiv.org/abs/2510.02567)
*Peter Pak,Achuth Chandrasekhar,Amir Barati Farimani*

Main category: cs.AI

TL;DR: LLM驱动的智能体可加速增材制造中的合金发现


<details>
  <summary>Details</summary>
Motivation: 增材制造（AM）中的合金发现复杂且需要跨学科专业知识。

Method: 开发了一个多智能体系统，利用大型语言模型（LLM）通过模型上下文协议（MCP）调用工具，例如进行Thermo-Calc属性图计算和生成缺乏熔合工艺图，以分析和预测合金的可打印性。该系统能够动态调整任务轨迹，实现自主决策。

Result: 该系统能够自动化和加速AM领域的合金发现任务，并证明了采用该多智能体系统的优势。

Conclusion: LLM驱动的智能体为加速和自动化AM合金发现提供了有效途径。

Abstract: Agentic systems enable the intelligent use of research tooling, augmenting a
researcher's ability to investigate and propose novel solutions to existing
problems. Within Additive Manufacturing (AM), alloy discovery remains a complex
challenge, often requiring expertise in the various domains of materials
science, thermodynamic simulations, and experimental analysis. Large Language
Model (LLM) enabled agents can facilitate this endeavor by utilizing their
extensive knowledge base to dispatch tool calls via Model Context Protocol
(MCP) to perform actions such as Thermo-Calc property diagram calculations and
lack of fusion process map generation. In addition, the multi-agent system
developed in this work is able to effectively reason through complex user
prompts and provide analysis on the printability of proposed alloys. These
agents can dynamically adjust their task trajectory to the outcomes of tool
call results, effectively enabling autonomous decision-making in practical
environments. This work aims to utilize LLM enabled agents to automate and
accelerate the task of alloy discovery within the field of additive
manufacturing and showcase the benefits of adopting this multi-agent system.

</details>


### [228] [A Benchmark Study of Deep Reinforcement Learning Algorithms for the Container Stowage Planning Problem](https://arxiv.org/abs/2510.02589)
*Yunqi Huang,Nishith Chennakeshava,Alexis Carras,Vladislav Neverov,Wei Liu,Aske Plaat,Yingjie Fan*

Main category: cs.AI

TL;DR: 本文提出了一个用于集装箱装载计划（CSPP）的Gym环境，并在此框架下评估了五种强化学习（RL）算法（DQN、QR-DQN、A2C、PPO和TRPO），以解决CSPP中的调度问题，并分析了不同算法在不同复杂场景下的表现。


<details>
  <summary>Details</summary>
Motivation: CSPP是海运和码头操作的关键环节，但其复杂性使得传统上依赖人工经验。虽然RL已被应用于CSPP，但缺乏系统的基准比较。因此，本文旨在填补这一空白，为CSPP提供一个可复用的Gym环境和基准测试。

Method: 开发了一个包含基本CSPP特征的Gym环境，并扩展了其在单智能体和多智能体设置下的码头吊车调度功能。在此框架下，评估了DQN、QR-DQN、A2C、PPO和TRPO这五种RL算法在不同复杂性场景下的表现。

Result: 结果显示，随着复杂性的增加，不同RL算法的性能差距显著，凸显了算法选择和问题设定对CSPP的重要性。

Conclusion: 本文通过提供一个包含码头吊车调度的可复用Gym环境，并对多种RL方法进行了基准测试，为未来在海事物流中的研究和实际应用奠定了基础。

Abstract: Container stowage planning (CSPP) is a critical component of maritime
transportation and terminal operations, directly affecting supply chain
efficiency. Owing to its complexity, CSPP has traditionally relied on human
expertise. While reinforcement learning (RL) has recently been applied to CSPP,
systematic benchmark comparisons across different algorithms remain limited. To
address this gap, we develop a Gym environment that captures the fundamental
features of CSPP and extend it to include crane scheduling in both multi-agent
and single-agent formulations. Within this framework, we evaluate five RL
algorithms: DQN, QR-DQN, A2C, PPO, and TRPO under multiple scenarios of varying
complexity. The results reveal distinct performance gaps with increasing
complexity, underscoring the importance of algorithm choice and problem
formulation for CSPP. Overall, this paper benchmarks multiple RL methods for
CSPP while providing a reusable Gym environment with crane scheduling, thus
offering a foundation for future research and practical deployment in maritime
logistics.

</details>


### [229] [Multimodal Large Language Model Framework for Safe and Interpretable Grid-Integrated EVs](https://arxiv.org/abs/2510.02592)
*Jean Douglas Carvalho,Hugo Kenji,Ahmad Mohammad Saber,Glaucia Melo,Max Mauro Dias Santos,Deepa Kundur*

Main category: cs.AI

TL;DR: 该论文提出了一个基于多模态大语言模型（LLM）的框架，用于处理电动汽车（EV）的传感器数据，并为驾驶员生成自然语言警报，以提高城市驾驶安全性和智能电网的集成。


<details>
  <summary>Details</summary>
Motivation: 整合电动汽车到智能电网带来了机遇，但也存在车辆与环境安全交互的挑战。本研究旨在解决这一问题，提高城市驾驶的安全性。

Method: 提出一个多模态大语言模型（LLM）框架，利用YOLOv8进行物体检测，结合地理编码定位和CAN总线遥测数据，生成自然语言警报。

Result: 使用真实城市道路数据验证了该框架，证明了其在生成针对行人、自行车和其他车辆等危险情况的上下文感知警报方面的有效性。

Conclusion: LLM在电动出行领域具有作为辅助工具的潜力，可通过车辆协同、负荷预测和交通感知能源规划来惠及交通系统和电网。

Abstract: The integration of electric vehicles (EVs) into smart grids presents unique
opportunities to enhance both transportation systems and energy networks.
However, ensuring safe and interpretable interactions between drivers,
vehicles, and the surrounding environment remains a critical challenge. This
paper presents a multi-modal large language model (LLM)-based framework to
process multimodal sensor data - such as object detection, semantic
segmentation, and vehicular telemetry - and generate natural-language alerts
for drivers. The framework is validated using real-world data collected from
instrumented vehicles driving on urban roads, ensuring its applicability to
real-world scenarios. By combining visual perception (YOLOv8), geocoded
positioning, and CAN bus telemetry, the framework bridges raw sensor data and
driver comprehension, enabling safer and more informed decision-making in urban
driving scenarios. Case studies using real data demonstrate the framework's
effectiveness in generating context-aware alerts for critical situations, such
as proximity to pedestrians, cyclists, and other vehicles. This paper
highlights the potential of LLMs as assistive tools in e-mobility, benefiting
both transportation systems and electric networks by enabling scalable fleet
coordination, EV load forecasting, and traffic-aware energy planning.
  Index Terms - Electric vehicles, visual perception, large language models,
YOLOv8, semantic segmentation, CAN bus, prompt engineering, smart grid.

</details>


### [230] [Mitigating Modal Imbalance in Multimodal Reasoning](https://arxiv.org/abs/2510.02608)
*Chen Henry Wu,Neil Kale,Aditi Raghunathan*

Main category: cs.AI

TL;DR: 现有模型在处理多模态信息时，尤其是在信息相互关联、形成跨模态语境时，其联合推理能力有待提高。本研究通过跨模态冲突场景，探讨了现有模型在面对多模态信息冲突时的表现，发现模型倾向于优先考虑单一模态，而非进行联合推理。


<details>
  <summary>Details</summary>
Motivation: 研究现有基础模型（FMs）在处理多模态信息时的联合推理能力，特别是在模态交互和形成跨模态语境时，识别其在处理跨模态冲突时的局限性。

Method: 通过在跨模态冲突场景中进行实验，分析FM在识别和解决由不同模态信息冲突引起的问题时的表现，并探究其交叉注意力的不平衡问题。此外，还测试了通过增加多模态或多语言数据集以及显式结合多种模态的训练方法对模型性能的影响。

Result: 实验发现，FM在单模态冲突场景中识别率高达90%，但在跨模态冲突场景中识别率骤降至3%。这种表现不佳源于交叉注意力的不平衡，模型存在严重的注意力分数不对称问题。仅仅扩大数据集规模并不能解决该问题，需要显式地在训练实例中结合多种模态。通过显式结合多种模态的训练方法，可以显著降低注意力不平衡，并提升在视觉-语言基准测试中的下游性能。

Conclusion: 为构建可靠的基础模型，必须系统地处理跨模态语境。交叉注意力的不平衡是导致多模态信息处理能力不足的关键因素，而显式地在训练中结合多种模态是解决此问题的有效途径。

Abstract: Foundation models (FMs) deployed in real-world tasks such as computer-use
agents must integrate diverse modalities. How good are FMs at performing joint
reasoning, simultaneously reasoning over multiple modalities, especially when
the modalities interact and relate to each other to form cross-modal context?
To better understand this problem, we study FMs on cross-modal conflicts:
scenarios where conflicting evidence is presented across modalities. This
allows us to examine whether FMs prioritize one modality over another or reason
jointly to reconcile the conflict. Our experiments reveal that FMs can
recognize conflicts in unimodal contexts, composed of a single modality, 90% of
the time, but the ratio falls as low as 3% when evidence is split across
modalities -- similar observations hold in cross-lingual contexts, composed of
multiple languages. We trace this failure to cross-modal attention imbalance,
showing that FMs exhibit extreme asymmetry in attention scores,
disproportionately prioritizing certain modalities. We show that cross-modal
attention imbalance does not go away by simply scaling up multimodal or
multilingual datasets blindly, since they lack training examples that
explicitly require cross-modal reasoning. We demonstrate that even a simple and
scalable method of explicitly combining multiple modalities within each
training instance significantly reduces attention imbalance. Reduced attention
imbalance directly translates to improved downstream performance on several
vision-language benchmarks. Our findings underscore the importance of
systematically addressing cross-modal contexts to build reliable foundation
models.

</details>


### [231] [On the Role of Temperature Sampling in Test-Time Scaling](https://arxiv.org/abs/2510.02611)
*Yuheng Wu,Azalia Mirhoseini,Thierry Tambe*

Main category: cs.AI

TL;DR: 在推理时，通过生成多个推理轨迹并选择最佳轨迹的测试时扩展（TTS）可以提高大型语言模型（LLMs）的推理能力。然而，在大型K下，进一步扩展不会带来收益，并且某些难题仍然无法解决。我们提出了一种温度扩展方法，该方法通过探索模型推理能力的不同子集来扩大LLMs的推理边界，并在推理基准测试中取得了显著的性能提升，同时还提供了多温度投票方法以减少开销。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型（LLMs）在推理时通过测试时扩展（TTS）进行推理的能力，并发现单一温度扩展存在局限性，提出多温度扩展以进一步提升模型性能。

Method: 提出温度扩展方法，并设计了一种多温度投票方法来降低开销。

Result: 在Qwen3（0.6B，1.7B，4B，8B）和五个推理基准测试（AIME 2024/2025，MATH500，LiveCodeBench，Hi-ToM）上，温度扩展比单一温度TTS额外提高了7.3分。多温度投票方法可以减少开销。

Conclusion: 温度扩展是一种简单有效的方法，可以释放基础模型的潜在能力，提高TTS的性能。

Abstract: Large language models (LLMs) can improve reasoning at inference time through
test-time scaling (TTS), where multiple reasoning traces are generated and the
best one is selected. Prior work shows that increasing the number of samples K
steadily improves accuracy. In this paper, we demonstrate that this trend does
not hold indefinitely: at large K, further scaling yields no gains, and certain
hard questions remain unsolved regardless of the number of traces.
Interestingly, we find that different sampling temperatures solve different
subsets of problems, implying that single-temperature scaling explores only
part of a model's potential. We therefore propose scaling along the temperature
dimension, which enlarges the reasoning boundary of LLMs. Averaged over Qwen3
(0.6B, 1.7B, 4B, 8B) and five representative reasoning benchmarks (AIME
2024/2025, MATH500, LiveCodeBench, Hi-ToM), temperature scaling yields an
additional 7.3 points over single-temperature TTS. Temperature scaling also
enables base models to reach performance comparable to reinforcement learning
(RL)-trained counterparts, without additional post-training. We further provide
a comprehensive analysis of this phenomenon and design a multi-temperature
voting method that reduces the overhead of temperature scaling. Overall, our
findings suggest that TTS is more powerful than previously thought, and that
temperature scaling offers a simple and effective way to unlock the latent
potential of base models.

</details>


### [232] [Geolog-IA: Conversational System for Academic Theses](https://arxiv.org/abs/2510.02653)
*Micaela Fuel Pozo,Andrea Guatumillo Saltos,Yeseña Tipan Llumiquinga,Kelly Lascano Aguirre,Marilyn Castillo Jara,Christian Mejia-Escobar*

Main category: cs.AI

TL;DR: Geolog-IA是一个基于AI的对话系统，使用Llama 3.1和Gemini 2.5模型，结合RAG和SQLite数据库，旨在回答厄瓜多尔中央大学地质论文相关问题，有效解决了模型幻觉和知识陈旧问题，BLEU评测得分0.87，为教育、培训和研究提供支持。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在开发一个能够自然回答有关厄瓜多尔中央大学地质论文问题的AI对话系统，以解决信息检索和知识获取的挑战。

Method: 研究采用Llama 3.1和Gemini 2.5语言模型，并结合检索增强生成（RAG）架构和SQLite数据库，构建了一个名为Geolog-IA的对话系统。

Result: Geolog-IA系统在BLEU指标上达到了0.87的平均得分，表明其在生成地质论文相关问题的回答时具有高度的一致性和准确性。

Conclusion: Geolog-IA系统通过结合先进的语言模型和RAG架构，成功开发了一个能够有效解决模型幻觉和知识过时问题的对话工具，为厄瓜多尔中央大学地质学科的信息检索、教育、培训和研究提供了支持，并为未来在其他学科的应用奠定了基础。

Abstract: This study presents the development of Geolog-IA, a novel conversational
system based on artificial intelligence that responds naturally to questions
about geology theses from the Central University of Ecuador. Our proposal uses
the Llama 3.1 and Gemini 2.5 language models, which are complemented by a
Retrieval Augmented Generation (RAG) architecture and an SQLite database. This
strategy allows us to overcome problems such as hallucinations and outdated
knowledge. The evaluation of Geolog-IA's performance with the BLEU metric
reaches an average of 0.87, indicating high consistency and accuracy in the
responses generated. The system offers an intuitive, web-based interface that
facilitates interaction and information retrieval for directors, teachers,
students, and administrative staff at the institution. This tool can be a key
support in education, training, and research and establishes a basis for future
applications in other disciplines.

</details>


### [233] [A Concept of Possibility for Real-World Events](https://arxiv.org/abs/2510.02655)
*Daniel G. Schwartz*

Main category: cs.AI

TL;DR: 提出了一种新的可能性概念，作为对Zadeh可能性概念的替代。


<details>
  <summary>Details</summary>
Motivation: 在计划问题中，需要一种新的可能性概念来替代Zadeh的模糊逻辑，以更好地评估事件发生的可能性。

Method: 将事件的可能性定义为先决条件满足的概率与约束不满足的概率的函数。

Result: 通过一个车辆路径规划的例子，展示了该理论在评估不同计划的可行性方面的应用。

Conclusion: 该理论提供了一种新的可能性概念，适用于计划问题，并可能更好地反映人类的推理方式。

Abstract: This paper offers a new concept of {\it possibility} as an alternative to the
now-a-days standard concept originally introduced by L.A. Zadeh in 1978. This
new version was inspired by the original but, formally, has nothing in common
with it other than that they both adopt the {\L}ukasiewicz multivalent
interpretation of the logical connectives. Moreover, rather than seeking to
provide a general notion of possibility, this focuses specifically on the
possibility of a real-world event. An event is viewed as having prerequisites
that enable its occurrence and constraints that may impede its occurrence, and
the possibility of the event is computed as a function of the probabilities
that the prerequisites hold and the constraints do not. This version of
possibility might appropriately be applied to problems of planning. When there
are multiple plans available for achieving a goal, this theory can be used to
determine which plan is most possible, i.e., easiest or most feasible to
complete. It is speculated that this model of reasoning correctly captures
normal human reasoning about plans. The theory is elaborated and an
illustrative example for vehicle route planning is provided. There is also a
suggestion of potential future applications.

</details>


### [234] [AutoMaAS: Self-Evolving Multi-Agent Architecture Search for Large Language Models](https://arxiv.org/abs/2510.02669)
*Bo Ma,Hang Li,ZeHua Hu,XiaoFan Gui,LuYao Liu,Simon Liu*

Main category: cs.AI

TL;DR: AutoMaAS是一个自适应的、自进化的多智能体架构搜索框架，通过神经架构搜索原则和动态算子生命周期管理，自动优化智能体配置，以适应不同查询复杂度和领域需求，从而在提升性能的同时降低推理成本。


<details>
  <summary>Details</summary>
Motivation: 现有的自动化多智能体系统设计方法寻求通用的解决方案，无法根据查询复杂度和领域需求动态调整资源分配。因此，需要一种能够自适应和自进化以优化多智能体系统设计的方法。

Method: AutoMaAS框架结合了神经架构搜索、动态算子生命周期管理和自动化机器学习技术。具体创新点包括：1. 基于性能-成本分析自动生成、融合和消除算子；2. 通过实时参数调整进行动态成本感知优化；3. 整合在线反馈以持续优化架构；4. 通过决策追踪增强可解释性。

Result: 在六个基准测试中，AutoMaAS相比现有最先进的方法，性能提升了1.0-7.1%，同时推理成本降低了3-5%。该框架在不同数据集和大型语言模型骨干网络之间表现出优越的可迁移性。

Conclusion: AutoMaAS为大型语言模型时代下的自动化多智能体系统设计开创了新范式，它能够通过自适应和自进化来发现最优的智能体配置，并在性能和成本之间取得良好平衡。

Abstract: Multi-agent systems powered by large language models have demonstrated
remarkable capabilities across diverse domains, yet existing automated design
approaches seek monolithic solutions that fail to adapt resource allocation
based on query complexity and domain requirements. This paper introduces
AutoMaAS, a self-evolving multi-agent architecture search framework that
leverages neural architecture search principles to automatically discover
optimal agent configurations through dynamic operator lifecycle management and
automated machine learning techniques. Our approach incorporates four key
innovations: (1) automatic operator generation, fusion, and elimination based
on performance-cost analysis, (2) dynamic cost-aware optimization with
real-time parameter adjustment, (3) online feedback integration for continuous
architecture refinement, and (4) enhanced interpretability through decision
tracing mechanisms. Extensive experiments across six benchmarks demonstrate
that AutoMaAS achieves 1.0-7.1\% performance improvement while reducing
inference costs by 3-5\% compared to state-of-the-art methods. The framework
shows superior transferability across datasets and LLM backbones, establishing
a new paradigm for automated multi-agent system design in the era of large
language models.

</details>


### [235] [ARMs: Adaptive Red-Teaming Agent against Multimodal Models with Plug-and-Play Attacks](https://arxiv.org/abs/2510.02677)
*Zhaorun Chen,Xun Liu,Mintong Kang,Jiawei Zhang,Minzhou Pan,Shuang Yang,Bo Li*

Main category: cs.AI

TL;DR: ARMs是一个自适应红队测试代理，能够系统地评估视觉语言模型（VLMs）的风险，通过优化多步推理来生成有害输出，并取得了最先进的攻击成功率，同时创建了一个包含30K多条红队测试实例的大型数据集ARMs-Bench，以提高VLMs的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的针对视觉语言模型（VLMs）的红队测试方法在应对新兴的真实世界漏洞方面存在局限性，无法进行大规模的探索。因此，需要一个能够系统性进行全面风险评估的工具。

Method: 提出ARMs，一个自适应红队测试代理，通过推理增强的多步协调来优化多样的红队测试策略，以有效地诱导目标VLMs产生有害输出。ARMs集成了11种新颖的多模态攻击策略和17种红队测试算法，并通过分层记忆和epsilon-greedy攻击探索算法来平衡攻击的多样性和有效性。

Result: ARMs在现有基准测试中取得了最先进的攻击成功率，平均超过基线52.1%，在Claude-4-Sonnet上成功率超过90%。ARMs生成的红队测试实例具有显著更高的多样性，揭示了VLMs中新兴的漏洞。利用ARMs创建的ARMs-Bench数据集包含了超过30K个实例，涵盖了51个风险类别。

Conclusion: ARMs-Bench数据集的引入显著提高了VLMs的鲁棒性，同时保持了其通用效用，为应对新兴威胁的模态安全对齐提供了可行的指导。ARMs是评估和增强VLMs安全性的重要工具。

Abstract: As vision-language models (VLMs) gain prominence, their multimodal interfaces
also introduce new safety vulnerabilities, making the safety evaluation
challenging and critical. Existing red-teaming efforts are either restricted to
a narrow set of adversarial patterns or depend heavily on manual engineering,
lacking scalable exploration of emerging real-world VLM vulnerabilities. To
bridge this gap, we propose ARMs, an adaptive red-teaming agent that
systematically conducts comprehensive risk assessments for VLMs. Given a target
harmful behavior or risk definition, ARMs automatically optimizes diverse
red-teaming strategies with reasoning-enhanced multi-step orchestration, to
effectively elicit harmful outputs from target VLMs. We propose 11 novel
multimodal attack strategies, covering diverse adversarial patterns of VLMs
(e.g., reasoning hijacking, contextual cloaking), and integrate 17 red-teaming
algorithms into ARMs via model context protocol (MCP). To balance the diversity
and effectiveness of the attack, we design a layered memory with an
epsilon-greedy attack exploration algorithm. Extensive experiments on instance-
and policy-based benchmarks show that ARMs achieves SOTA attack success rates,
exceeding baselines by an average of 52.1% and surpassing 90% on
Claude-4-Sonnet. We show that the diversity of red-teaming instances generated
by ARMs is significantly higher, revealing emerging vulnerabilities in VLMs.
Leveraging ARMs, we construct ARMs-Bench, a large-scale multimodal safety
dataset comprising over 30K red-teaming instances spanning 51 diverse risk
categories, grounded in both real-world multimodal threats and regulatory
risks. Safety fine-tuning with ARMs-Bench substantially improves the robustness
of VLMs while preserving their general utility, providing actionable guidance
to improve multimodal safety alignment against emerging threats.

</details>


### [236] [Automated Constraint Specification for Job Scheduling by Regulating Generative Model with Domain-Specific Representation](https://arxiv.org/abs/2510.02679)
*Yu-Zhe Shi,Qiao Xu,Yanjia Li,Mingchen Liu,Huamin Qu,Lecheng Ruan,Qining Wang*

Main category: cs.AI

TL;DR: 该研究提出了一种约束中心架构，利用大型语言模型（LLM）自动从制造数据中生成生产调度约束，解决了手动约束指定的痛点。


<details>
  <summary>Details</summary>
Motivation: 当前制造环境中，高级计划与排程（APS）系统虽优化了资源配置和生产效率，但手动定义约束耗时耗力。尽管LLM在自动化约束指定方面有潜力，但存在歧义、非确定性和领域知识不足等挑战。

Method: 提出了一种约束中心架构，通过定义一个分层结构空间，并结合领域特定表示，来规范LLM进行可靠的自动化约束指定。此外，还设计了一个自动化生产场景适应算法来定制该架构。

Result: 实验结果表明，该方法在约束指定任务上显著优于纯LLM方法，成功平衡了LLM的生成能力和制造系统对可靠性的要求。

Conclusion: 所提出的约束中心架构能够有效地利用LLM来可靠地自动指定生产调度约束，为解决制造数据到形式化约束的转化问题提供了一个有前景的解决方案。

Abstract: Advanced Planning and Scheduling (APS) systems have become indispensable for
modern manufacturing operations, enabling optimized resource allocation and
production efficiency in increasingly complex and dynamic environments. While
algorithms for solving abstracted scheduling problems have been extensively
investigated, the critical prerequisite of specifying manufacturing
requirements into formal constraints remains manual and labor-intensive.
Although recent advances of generative models, particularly Large Language
Models (LLMs), show promise in automating constraint specification from
heterogeneous raw manufacturing data, their direct application faces challenges
due to natural language ambiguity, non-deterministic outputs, and limited
domain-specific knowledge. This paper presents a constraint-centric
architecture that regulates LLMs to perform reliable automated constraint
specification for production scheduling. The architecture defines a
hierarchical structural space organized across three levels, implemented
through domain-specific representation to ensure precision and reliability
while maintaining flexibility. Furthermore, an automated production scenario
adaptation algorithm is designed and deployed to efficiently customize the
architecture for specific manufacturing configurations. Experimental results
demonstrate that the proposed approach successfully balances the generative
capabilities of LLMs with the reliability requirements of manufacturing
systems, significantly outperforming pure LLM-based approaches in constraint
specification tasks.

</details>


### [237] [NCV: A Node-Wise Consistency Verification Approach for Low-Cost Structured Error Localization in LLM Reasoning](https://arxiv.org/abs/2510.02816)
*Yulong Zhang,Li Wang,Wei Du,Peilin Li,Yuqin Dai Zhiyuan Zhao,Lingyong Fang,Ziniu Liu,Ru Zhang,Huijia Zhu,Gongshen Liu*

Main category: cs.AI

TL;DR: 通过将思维链分解为节点级别的一致性检查，节点级一致性验证（NCV）实现了 LLM 推理验证的精确错误本地化和高效率。


<details>
  <summary>Details</summary>
Motivation: 验证大型语言模型的多步推理存在挑战，因为错误定位不精确且令牌成本高。现有方法要么评估整个推理链，导致注意力分散，要么依赖昂贵的多重采样。

Method: NCV 将验证重构为节点级别轻量级二元一致性检查，将思维链分解为相互连接的验证节点。

Result: NCV 在公共数据集上实现了比基线高 10% 到 25% 的 F1 分数，同时使用的令牌比传统的基于 CoT 的验证器少 6 倍到 58 倍。

Conclusion: NCV 是一种无需训练的框架，可以精确地定位错误并避免不必要的长格式生成，从而提高了解释性和效率，为可靠的 LLM 推理验证提供了一个可扩展的解决方案。

Abstract: Verifying multi-step reasoning in large language models is difficult due to
imprecise error localization and high token costs. Existing methods either
assess entire reasoning chains, suffering attention dilution, or rely on
expensive multi-sampling. We introduce Node-wise Consistency Verification
(NCV), a training-free framework that recasts verification as lightweight
binary consistency checks at the node level. By decomposing the chain of
thought into interconnected verification nodes, NCV precisely localizes errors
and avoids unnecessary long-form generation. Experiments demonstrate that our
approach enhances interpretability and efficiency, presenting a scalable
solution for reliable LLM reasoning verification. On public datasets, NCV
achieves a 10\% to 25\% improvement in F1 scores over baselines while utilizing
$6\times$~$58\times$ fewer tokens than traditional methods like CoT-based
verifiers.

</details>


### [238] [Beyond the Final Answer: Evaluating the Reasoning Trajectories of Tool-Augmented Agents](https://arxiv.org/abs/2510.02837)
*Wonjoong Kim,Sangwu Park,Yeonjun In,Sein Kim,Dongha Lee,Chanyoung Park*

Main category: cs.AI

TL;DR: 该研究提出TRACE框架，用于多维度评估工具增强LLM代理性能，解决了现有基准测试中评估方法局限于答案匹配的问题。


<details>
  <summary>Details</summary>
Motivation: 现有工具增强基准测试的评估方法主要局限于答案匹配，无法评估代理解决问题轨迹的效率、幻觉和适应性等关键方面。

Method: 引入TRACE框架，通过证据银行累积先前推理步骤的知识，实现对代理推理轨迹的多维度、细致评估。

Result: 通过在现有基准测试中添加多样化且有缺陷的轨迹，并进行多维度评分，验证了TRACE框架的有效性。该框架能够以可扩展且经济高效的方式准确评估复杂行为，即使使用小型开源LLM也能实现。研究还应用该框架评估了代理解决工具增强任务的轨迹，并提出了新的观察和见解。

Conclusion: TRACE框架能够有效地评估工具增强LLM代理的复杂行为，解决了现有评估方法的局限性，并为代理性能分析提供了新的视角。

Abstract: Although recent tool-augmented benchmarks incorporate complex user requests
and diverse tools, the evaluation methods for most of them remain limited to
answer matching. However, as the number of steps required to resolve a user
request increases, a proper evaluation of an agent's performance must go beyond
the final answer to also assess the problem-solving trajectory, including
previously ignored aspects such as efficiency, hallucination, and adaptivity.
The most straightforward method for evaluating these aspects is to compare an
agent's trajectory with the ground-truth trajectory, but this approach is
fundamentally limited since annotating all valid ground-truth trajectories is
prohibitively expensive. However, a simple LLM-based evaluator struggles to
assess trajectories in detail without ground truth. To effectively evaluate the
agents in this manner, we introduce TRACE, a framework for the
multi-dimensional evaluation of tool-augmented LLM agent performance. By
incorporating an evidence bank, which accumulates knowledge gathered from
preceding reasoning steps, TRACE enables a multi-faceted analysis and
evaluation of an agent's reasoning trajectory effectively. To validate our
framework, we develop a new meta-evaluation dataset by augmenting existing
benchmarks with diverse and flawed trajectories, each labeled with
multi-faceted performance scores. Our results confirm that TRACE accurately
evaluates these complex behaviors in a scalable and cost-effective manner, even
with small open-source LLMs. Furthermore, we apply our method to evaluate the
trajectories that agents produce while solving tool-augmented tasks, presenting
previously unreported observations and their corresponding insights.

</details>


### [239] [Take Goodhart Seriously: Principled Limit on General-Purpose AI Optimization](https://arxiv.org/abs/2510.02840)
*Antoine Maier,Aude Maier,Tom David*

Main category: cs.AI

TL;DR: 机器学习中的目标满意度假设（OSA）在实际中经常失败，因为近似、估计和优化误差会导致系统性偏差，并且人类意图难以完全形式化，导致目标函数失配。


<details>
  <summary>Details</summary>
Motivation: 检验机器学习中“训练模型满足其目标函数”这一普遍假设（OSA），并阐述其在现实条件下的局限性。

Method: 提出一个与学习范式无关的框架，分析OSA在近似、估计和优化误差下的失效，并指出完美形式化开发者意图（如人类偏好）的不可能性。

Result: OSA在现实条件下系统性地失效，近似、估计和优化误差导致偏差，目标函数失配不可避免。这些偏差与Goodhart定律失效难以区分，特别是在强优化压力下。

Conclusion: 由于无法预知Goodhart定律的失效点，对通用人工智能系统（AGI）的优化必须设置原则性上限，否则可能导致系统失控且不可逆转。

Abstract: A common but rarely examined assumption in machine learning is that training
yields models that actually satisfy their specified objective function. We call
this the Objective Satisfaction Assumption (OSA). Although deviations from OSA
are acknowledged, their implications are overlooked. We argue, in a
learning-paradigm-agnostic framework, that OSA fails in realistic conditions:
approximation, estimation, and optimization errors guarantee systematic
deviations from the intended objective, regardless of the quality of its
specification. Beyond these technical limitations, perfectly capturing and
translating the developer's intent, such as alignment with human preferences,
into a formal objective is practically impossible, making misspecification
inevitable. Building on recent mathematical results, absent a mathematical
characterization of these gaps, they are indistinguishable from those that
collapse into Goodhart's law failure modes under strong optimization pressure.
Because the Goodhart breaking point cannot be located ex ante, a principled
limit on the optimization of General-Purpose AI systems is necessary. Absent
such a limit, continued optimization is liable to push systems into predictable
and irreversible loss of control.

</details>


### [240] [Reward Model Routing in Alignment](https://arxiv.org/abs/2510.02850)
*Xinle Wu,Yao Lu*

Main category: cs.AI

TL;DR: RLHF/RLAIF 依赖单一奖励模型 (RM) 限制了对齐质量并可能导致过拟合。我们提出了 BayesianRouter，一种结合了离线 RM 优势学习和在线贝叶斯选择的混合路由框架，以改进 RM 选择。


<details>
  <summary>Details</summary>
Motivation: 现有的 RLHF/RLAIF 管道通常只使用一个奖励模型 (RM)，这限制了模型对齐的质量，并可能导致对训练数据的过拟合。最近的研究探索了 RM 路由，即从候选池中动态选择 RM，以利用其互补优势，同时保持恒定的 RM 调用次数，但现有方法存在冷启动和探索不足的问题。

Method: BayesianRouter 采用两阶段方法：1. 离线阶段：训练一个多任务路由器来学习每个 RM 的可靠性。2. 在线阶段：使用贝叶斯汤普森采样进行 RM 选择，将 RM 的权重向量初始化为离线学习到的嵌入的先验（高斯分布），并根据在线奖励自适应地更新这些后验，以适应不断变化的策略分布。

Result: 在指令遵循（AlpacaEval-2, Arena-Hard, MT-Bench）和推理（GSM8K, MMLU）基准测试上的广泛实验表明，BayesianRouter 的表现优于单独的 RM、RM 集成和现有的路由方法。

Conclusion: BayesianRouter 通过结合离线 RM 优势学习和在线贝叶斯选择，有效地解决了现有 RM 路由方法的冷启动和探索不足问题，并在多项基准测试中取得了优于现有方法的性能。

Abstract: Reinforcement learning from human or AI feedback (RLHF / RLAIF) has become
the standard paradigm for aligning large language models (LLMs). However, most
pipelines rely on a single reward model (RM), limiting alignment quality and
risking overfitting. Recent work explores RM routing--dynamically selecting an
RM from a candidate pool to exploit complementary strengths while maintaining
$O(1)$ RM calls--but existing methods suffer from cold-start and insufficient
exploration. We propose BayesianRouter, a hybrid routing framework that
combines offline RM strengths learning with online Bayesian selection. In the
offline stage, a multi-task router is trained on preference data to estimate
per-RM reliability. In the online stage, a Bayesian Thompson sampling router
performs per-query RM selection, initializing RM-specific weight vectors with
offline embeddings as Gaussian priors and adaptively updating their posteriors
with online rewards to adapt to the evolving policy distribution. Extensive
experiments on instruction-following (AlpacaEval-2, Arena-Hard, MT-Bench) and
reasoning (GSM8K, MMLU) benchmarks show that BayesianRouter consistently
outperforms individual RMs, RM ensembling, and existing routing methods.

</details>


### [241] [Consolidating Reinforcement Learning for Multimodal Discrete Diffusion Models](https://arxiv.org/abs/2510.02880)
*Tianren Ma,Mu Zhang,Yibing Wang,Qixiang Ye*

Main category: cs.AI

TL;DR: MaskGRPO是首个可行的、可扩展的多模态强化学习方法，用于离散扩散模型，实现了有效的样本重要性采样和特定模态的适应。它通过改进的样本重要性估计器和针对视觉序列的生成方法，实现了更稳定、高效的更新，从而提高了推理性能和生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有的离散扩散模型（DDM）的优化方法难以处理其非自回归范式带来的挑战，如重要性采样和复杂的回放，这使得像Group Relative Policy Optimization（GRPO）这样的强化学习方法难以应用。

Method: MaskGRPO通过理论分析，建立了一个能够捕捉有价值的token波动以进行梯度更新的样本重要性估计器。同时，它还为视觉序列定制了生成方法，以产生多样化的补全和可靠的优化梯度。

Result: 在数学推理、编码和视觉生成基准测试中，MaskGRPO实现了更稳定、更高效的更新，从而提高了推理性能和生成质量。

Conclusion: MaskGRPO是一种系统的策略优化方法，也是首个用于离散视觉扩散的实用方法。

Abstract: Optimizing discrete diffusion model (DDM) with rewards remains a challenge:
the non-autoregressive paradigm makes importance sampling intractable and
rollout complex, puzzling reinforcement learning methods such as Group Relative
Policy Optimization (GRPO). In this study, we introduce MaskGRPO, the first
viable approach to enable scalable multimodal reinforcement learning in
discrete diffusion with effective importance sampling and modality-specific
adaptations. To this end, we first clarify the theoretical foundation for DDMs,
which facilitates building an importance estimator that captures valuable token
fluctuation for gradient updates. We then delicately tailored the rollout
method for visual sequences, which yields diverse completions and reliable
optimization gradients. Upon math reasoning, coding, and visual generation
benchmarks, MaskGRPO brings more stable and efficient updates, leading to
stronger reasoning performance and better generation quality. This study
establishes MaskGRPO as a systematic policy optimization approach and the first
practical way for discretized visual diffusion.

</details>


### [242] [Onto-Epistemological Analysis of AI Explanations](https://arxiv.org/abs/2510.02996)
*Martina Mattioli,Eike Petersen,Aasa Feragen,Marcello Pelillo,Siavash A. Bigdeli*

Main category: cs.AI

TL;DR: AI的可解释性（XAI）方法存在潜在的哲学问题，选择不当可能导致误导性解释。


<details>
  <summary>Details</summary>
Motivation: 当前主流的深度学习模型是黑箱系统，缺乏可解释性，这限制了它们的信任度和应用。XAI方法旨在解决此问题，但其设计隐含了关于解释的有效性和效用的技术假设，而这些假设可能与哲学上对解释的理解存在冲突。

Method: 本文分析了XAI方法中存在的本体论和认识论假设，探讨了在AI系统中选择和应用XAI方法时，这些假设如何影响解释的有效性和解释的获取。

Result: 研究表明，即使是微小的技术改动也可能对应着对解释的根本性假设的差异。忽略潜在的本体-认识论范式可能带来风险，并可能导致误导性的AI解释。

Conclusion: 在选择XAI方法时，必须考虑其潜在的本体-认识论假设，并根据具体应用领域进行选择和调整，以确保解释的有效性和准确性。

Abstract: Artificial intelligence (AI) is being applied in almost every field. At the
same time, the currently dominant deep learning methods are fundamentally
black-box systems that lack explanations for their inferences, significantly
limiting their trustworthiness and adoption. Explainable AI (XAI) methods aim
to overcome this challenge by providing explanations of the models' decision
process. Such methods are often proposed and developed by engineers and
scientists with a predominantly technical background and incorporate their
assumptions about the existence, validity, and explanatory utility of different
conceivable explanatory mechanisms. However, the basic concept of an
explanation -- what it is, whether we can know it, whether it is absolute or
relative -- is far from trivial and has been the subject of deep philosophical
debate for millennia. As we point out here, the assumptions incorporated into
different XAI methods are not harmless and have important consequences for the
validity and interpretation of AI explanations in different domains. We
investigate ontological and epistemological assumptions in explainability
methods when they are applied to AI systems, meaning the assumptions we make
about the existence of explanations and our ability to gain knowledge about
those explanations. Our analysis shows how seemingly small technical changes to
an XAI method may correspond to important differences in the underlying
assumptions about explanations. We furthermore highlight the risks of ignoring
the underlying onto-epistemological paradigm when choosing an XAI method for a
given application, and we discuss how to select and adapt appropriate XAI
methods for different domains of application.

</details>


### [243] [From Facts to Foils: Designing and Evaluating Counterfactual Explanations for Smart Environments](https://arxiv.org/abs/2510.03078)
*Anna Trapp,Mersedeh Sadeghi,Andreas Vogelsang*

Main category: cs.AI

TL;DR: 本文提出了第一个针对基于规则的智能环境的对反事实解释的正式化和实现，并通过用户研究表明，因果解释在语言简洁和时间紧迫的情况下更受欢迎，而对反事实解释在需要解决问题时更受欢迎。


<details>
  <summary>Details</summary>
Motivation: 在基于规则的智能环境中，可解释性被认为是必不可少的，但目前缺乏生成反事实解释的方法。

Method: 提出对反事实解释的第一个正式化和实现，并将其实现为一个插件，扩展了现有的智能环境解释引擎。通过用户研究（N=17）评估了生成的对反事实解释与传统的因果解释。

Result: 用户研究结果显示，用户偏好具有高度情境性：因果解释因其语言简洁和在时间紧迫情况下的优势而受到青睐；对反事实解释因其可操作性而受到青睐，尤其是在用户想要解决问题时。

Conclusion: 本文提供了一个实用的框架，用于在智能环境中进行一种新型解释，并提供了经验证据来指导在何种情况下选择哪种解释类型最为有效。

Abstract: Explainability is increasingly seen as an essential feature of rule-based
smart environments. While counterfactual explanations, which describe what
could have been done differently to achieve a desired outcome, are a powerful
tool in eXplainable AI (XAI), no established methods exist for generating them
in these rule-based domains. In this paper, we present the first formalization
and implementation of counterfactual explanations tailored to this domain. It
is implemented as a plugin that extends an existing explanation engine for
smart environments. We conducted a user study (N=17) to evaluate our generated
counterfactuals against traditional causal explanations. The results show that
user preference is highly contextual: causal explanations are favored for their
linguistic simplicity and in time-pressured situations, while counterfactuals
are preferred for their actionable content, particularly when a user wants to
resolve a problem. Our work contributes a practical framework for a new type of
explanation in smart environments and provides empirical evidence to guide the
choice of when each explanation type is most effective.

</details>


### [244] [A Study of Rule Omission in Raven's Progressive Matrices](https://arxiv.org/abs/2510.03127)
*Binze Li*

Main category: cs.AI

TL;DR: AI模型在类比推理任务（如RPM）上表现出对已知规则的依赖性，在面对新规则或缺失规则时泛化能力不足。


<details>
  <summary>Details</summary>
Motivation: 评估AI模型在类比推理任务中是否具备真正的推理能力，而非仅仅依赖统计规律，尤其是在训练数据不完整的情况下。

Method: 在Impartial-RAVEN（I-RAVEN）数据集上，通过故意遗漏部分结构规则进行训练，并评估了Transformer模型和CoPINet、Dual-Contrast Network等视觉模型。

Result: Transformer模型在熟悉规则上表现良好，但在面对新规则或遗漏规则时准确率显著下降。Token级别准确率与完整答案准确率之间的差距揭示了当前方法的局限性。

Conclusion: 当前的AI模型在抽象推理方面仍有局限，需要超越模式识别，发展更鲁棒的抽象推理架构。

Abstract: Analogical reasoning lies at the core of human cognition and remains a
fundamental challenge for artificial intelligence. Raven's Progressive Matrices
(RPM) serve as a widely used benchmark to assess abstract reasoning by
requiring the inference of underlying structural rules. While many vision-based
and language-based models have achieved success on RPM tasks, it remains
unclear whether their performance reflects genuine reasoning ability or
reliance on statistical shortcuts. This study investigates the generalization
capacity of modern AI systems under conditions of incomplete training by
deliberately omitting several structural rules during training. Both
sequence-to-sequence transformer models and vision-based architectures such as
CoPINet and the Dual-Contrast Network are evaluated on the Impartial-RAVEN
(I-RAVEN) dataset. Experiments reveal that although transformers demonstrate
strong performance on familiar rules, their accuracy declines sharply when
faced with novel or omitted rules. Moreover, the gap between token-level
accuracy and complete answer accuracy highlights fundamental limitations in
current approaches. These findings provide new insights into the reasoning
mechanisms underlying deep learning models and underscore the need for
architectures that move beyond pattern recognition toward robust abstract
reasoning.

</details>


### [245] [CoDA: Agentic Systems for Collaborative Data Visualization](https://arxiv.org/abs/2510.03194)
*Zichen Chen,Jiefeng Chen,Sercan Ö. Arik,Misha Sra,Tomas Pfister,Jinsung Yoon*

Main category: cs.AI

TL;DR: CoDA是一个多智能体系统，通过专门的LLM智能体进行元数据分析、任务规划、代码生成和自我反思，实现了自动化数据可视化，并在评估中取得了显著成果。


<details>
  <summary>Details</summary>
Motivation: 现有数据可视化自动化方法在处理复杂数据集、代码错误和迭代优化方面存在不足，需要更强大的自动化解决方案。

Method: 将数据可视化自动化视为一个多智能体协作问题，引入CoDA系统，该系统使用专门的LLM智能体进行元数据分析、任务规划、代码生成和自我反思，并对该流程进行形式化。

Result: CoDA在整体得分上取得了显著的提升，相比其他方法最高提升了41.5%。

Conclusion: 未来的数据可视化自动化在于集成的、协作式的智能体工作流，而非孤立的代码生成。

Abstract: Deep research has revolutionized data analysis, yet data scientists still
devote substantial time to manually crafting visualizations, highlighting the
need for robust automation from natural language queries. However, current
systems struggle with complex datasets containing multiple files and iterative
refinement. Existing approaches, including simple single- or multi-agent
systems, often oversimplify the task, focusing on initial query parsing while
failing to robustly manage data complexity, code errors, or final visualization
quality. In this paper, we reframe this challenge as a collaborative
multi-agent problem. We introduce CoDA, a multi-agent system that employs
specialized LLM agents for metadata analysis, task planning, code generation,
and self-reflection. We formalize this pipeline, demonstrating how
metadata-focused analysis bypasses token limits and quality-driven refinement
ensures robustness. Extensive evaluations show CoDA achieves substantial gains
in the overall score, outperforming competitive baselines by up to 41.5%. This
work demonstrates that the future of visualization automation lies not in
isolated code generation but in integrated, collaborative agentic workflows.

</details>


### [246] [Coevolutionary Continuous Discrete Diffusion: Make Your Diffusion Language Model a Latent Reasoner](https://arxiv.org/abs/2510.03206)
*Cai Zhou,Chenxiao Yang,Yi Hu,Chenyu Wang,Chubin Zhang,Muhan Zhang,Lester Mackey,Tommi Jaakkola,Stephen Bates,Dinghuai Zhang*

Main category: cs.AI

TL;DR: 连续扩散模型在语言建模中表现不佳，我们提出了CCDD，一个联合多模态扩散模型，在连续和离散空间中同时去噪，并在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 证明了连续扩散模型比离散扩散和循环 Transformer 更具表现力，并解决了连续扩散模型在实践中的可训练性问题。

Method: 提出了一种名为CCDD（协同进化连续离散扩散）的新模型，该模型在连续表示空间和离散标记空间上定义了一个联合多模态扩散过程，并提出了有效的架构和训练/采样技术。

Result: CCDD在语言建模实验中表现出了强大的经验性能，在现实世界的任务中进行了广泛的测试。

Conclusion: CCDD通过结合连续和离散空间，实现了高表现力和良好的可训练性，从而在语言建模任务中取得了优异的性能。

Abstract: Diffusion language models, especially masked discrete diffusion models, have
achieved great success recently. While there are some theoretical and primary
empirical results showing the advantages of latent reasoning with looped
transformers or continuous chain-of-thoughts, continuous diffusion models
typically underperform their discrete counterparts. In this paper, we argue
that diffusion language models do not necessarily need to be in the discrete
space. In particular, we prove that continuous diffusion models have stronger
expressivity than discrete diffusions and looped transformers. We attribute the
contradiction between the theoretical expressiveness and empirical performance
to their practical trainability: while continuous diffusion provides
intermediate supervision that looped transformers lack, they introduce
additional difficulty decoding tokens into the discrete token space from the
continuous representation space. We therefore propose Coevolutionary Continuous
Discrete Diffusion (CCDD), which defines a joint multimodal diffusion process
on the union of a continuous representation space and a discrete token space,
leveraging a single model to simultaneously denoise in the joint space. By
combining two modalities, CCDD is expressive with rich semantics in the latent
space, as well as good trainability and sample quality with the help of
explicit discrete tokens. We also propose effective architectures and advanced
training/sampling techniques for CCDD, which reveals strong empirical
performance in extensive language modeling experiments on real-world tasks.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [247] [Axiomatisation for an asynchronous epistemic logic with sending and receiving messages](https://arxiv.org/abs/2510.02890)
*Philippe Balbiani,Hans van Ditmarsch,Clara Lerouvillois*

Main category: cs.LO

TL;DR: 该研究提出了一个用于异步公共公告的逻辑系统AA*，并给出了其公理化。


<details>
  <summary>Details</summary>
Motivation: 在异步公共公告场景下，公告的发送和接收是分离的，需要考虑历史公告记录的影响，因此需要一个能处理这种情况的逻辑系统。

Method: 提出并公理化了AA*系统，用于解释在有历史公告记录的世界中的公式。

Result: AA*公理化可以处理任意历史公告记录，但与之前的AA系统不同，它是无穷的，并且不是一个约简系统。

Conclusion: AA*是AA的推广，用于处理更复杂的异步公共公告场景。

Abstract: We investigate a public announcement logic for asynchronous public
announcements wherein the sending of the announcements by the environment is
separated from the reception of the announcements by the individual agents.
Both come with different modalities. In the logical semantics, formulas are
interpreted in a world of a Kripke model but given a history of prior
announcements and receptions of announcements that already happened. An
axiomatisation AA for such a logic has been given in prior work, for the
formulas that are valid when interpreted in the Kripke model before any such
announcements have taken place. This axiomatisation is a reduction system
wherein one can show that every formula is equivalent to a purely epistemic
formula without dynamic modalities for announcements and receptions. We propose
a generalisation AA* of this axiomatisation, for the formulas that are valid
when interpreted in the Kripke model given any history of prior announcements
and receptions of announcements. It does not extend the axiomatisation AA, for
example it is no longer valid that nobody has received any announcement. Unlike
AA, this axiomatisation AA* is infinitary and it is not a reduction system.

</details>


### [248] [A Graded Modal Type Theory for Pulse Schedules](https://arxiv.org/abs/2510.03130)
*Robin Adams*

Main category: cs.LO

TL;DR: 提出了一种名为PSTT（脉冲调度类型理论）的语言，用于表示脉冲调度，并使用等级模态类型理论来表示时间信息。


<details>
  <summary>Details</summary>
Motivation: 现有的脉冲调度表示方法缺乏对时间信息的精确表示，限制了超导量子计算机的调度效率。

Method: 提出了一种名为PSTT（脉冲调度类型理论）的语言，该语言基于等级模态类型理论，并使用等级来表示时间信息。同时，为该系统提供了范畴语义，并证明了其可靠性和完备性。

Result: PSTT语言能够精确地表示脉冲调度中的时间信息，并已证明其可靠性和完备性。

Conclusion: PSTT是一种表示超导量子计算机脉冲调度的有效语言，为量子计算的调度优化提供了新的方向。

Abstract: We propose a language for representing the pulse schedules that a
superconducting quantum computer accepts as input. The language is a graded
modal type theory named PSTT (Pulse Schedule Type Theory). Graded modals type
theories are type systems where each variable is annotated with a parameter or
grade. These can be used to represent, for example, resource usage, where the
grade denotes how many times a given resource may be used; or privacy levels,
whether a resource is private or public. In this system, we use the grades to
represent timing information. We give categorical semantics to the system and
prove soundness and completeness.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [249] [Slow-phonon control of spin Edelstein effect in Rashba $d$-wave altermagnets](https://arxiv.org/abs/2510.02451)
*Mohsen Yarmohammadi,Jacob Linder,James K. Freericks*

Main category: cond-mat.mtrl-sci

TL;DR: 晶格振动（声子）会抑制二维d波反磁体中的自旋极化，通过各向异性地使自旋分裂能带偏离费米面，导致自旋爱德斯坦效应的完全去极化。


<details>
  <summary>Details</summary>
Motivation: 研究缓慢的晶格振动（声子）如何影响二维d波反磁体中内在和外感应的自旋极化。

Method: 使用带电子-声子耦合的Rashba连续体模型，并在静态Holstein水平下处理耦合，利用Kubo线性响应形式主义分析自旋爱德斯坦效应。

Result: 中等至强耦合的电子-声子耦合通过带内和带间通道逐步抑制感应极化，临界耦合标志着完全自旋爱德斯坦去极化的出现。这种去极化源于声子引起的能量重整化，导致自旋分裂能带异乎寻常地偏离化学势，使费米面完全坍塌。反磁性使得这种去极化具有各向异性。

Conclusion: 声子散射（例如通过各种衬底）提供了一种按需控制自旋极化的有力手段，实现了自旋极化和去极化状态之间的可逆切换，这对于推进自旋逻辑架构和优化下一代自旋电子器件至关重要。

Abstract: Altermagnets have zero net magnetization yet feature spin-split bands that
host spin-polarized states. Here, we investigate how slow lattice vibrations
(phonons) influence both the intrinsic and externally induced spin
polarizations in two-dimensional $d$-wave altermagnets. For the induced spin
polarizations, we employ a Rashba continuum model with electron-phonon coupling
(EPC) treated at the static-Holstein level and analyze the spin Edelstein
effect using the Kubo linear-response formalism. We find that
moderate-to-strong EPC progressively suppresses the induced polarization via
both intraband and interband channels, with a critical coupling marking the
onset of complete spin Edelstein depolarization. The depolarization transition
arises from a phonon-induced energy renormalization that pushes the spin-split
bands anisotropically above the chemical potential, leading to a complete
collapse of the Fermi surface. While (de)polarization can occur even in the
Rashba non-altermagnetic phase, it remains isotropic. The presence of
altermagnetism makes it anisotropic and breaks the conventional antisymmetry
between spin susceptibilities that occurs with pure spin-orbit coupling,
rendering the effect highly relevant for spintronic applications. We further
investigate how the phonon coupling to the altermagnetic order, Rashba
spin-orbit strength, and carrier doping collectively tune the depolarization
transition. Our findings demonstrate that phonon scattering (e.g., through
various substrates) offers a powerful means for on-demand control of spin
polarization, enabling reversible switching between spin-polarized and
depolarized states -- a key functionality for advancing spin logic
architectures and optimizing next-generation spintronic devices.

</details>


### [250] [Spin polarization engineering in $d$-wave altermagnets](https://arxiv.org/abs/2510.02452)
*Mohsen Yarmohammadi,Marco Berritta,Marin Bukov,Libor Šmejkal,Jacob Linder,Peter M. Oppeneer*

Main category: cond-mat.mtrl-sci

TL;DR: 该研究提出了一种利用门控、光学驱动和面内电场来控制d波阿尔特磁体中自旋极化的多场方法。


<details>
  <summary>Details</summary>
Motivation: 阿尔特磁体虽然具有非传统的自旋极化能带，但净磁化强度为零，控制其自旋结构仍然是一个挑战。

Method: 提出了一种多场方法，利用门控、光学驱动和面内电场来设计d波阿尔特磁体中的自旋极化，从而实现沿多个方向可调谐和可切换的极化。光学驱动引起面外（z）极化，而门控和面内场通过埃德尔施泰因效应产生x和y极化，这些都可通过实验检测。

Result: 研究发现，自旋和带选择性掺杂会引起手性光学活性，这是阿尔特磁体独有的特性。

Conclusion: 所提出的方法为在阿尔特磁体中实现自旋极化的完全控制提供了一条多功能途径。

Abstract: Altermagnets host unconventional spin-polarized bands despite zero net
magnetization, but controlling their spin structure remains challenging. We
propose a multi-field approach to engineer spin polarization in $d$-wave
altermagnets using gating, optical driving, and in-plane electric fields, which
enable tunable and switchable polarizations along multiple directions. Optical
driving induces out-of-plane ($z$) polarization, while gating and in-plane
fields generate $x$- and $y$-polarizations via the Edelstein effect, all of
which are experimentally detectable. We further find that spin- and
band-selective doping induces chiral optical activity, a feature unique to
altermagnets. Our approach provides a versatile route for full control of spin
polarization in altermagnets.

</details>


### [251] [Epitaxially-stabilized growth of wüstite FeO on 4H-SiC](https://arxiv.org/abs/2510.02531)
*Faisal Kimbugwe,Marzieh Baan,Alexandra Fonseca Montenegro,Roberto C Myers,Tyler J Grassman*

Main category: cond-mat.mtrl-sci

TL;DR: 在4H-SiC衬底上成功制备了高质量的FeO外延薄膜。


<details>
  <summary>Details</summary>
Motivation: FeO在室温下不稳定，易分解为Fe和Fe3O4，但它具有潜在的应用价值。本研究旨在探索在SiC衬底上通过外延生长来稳定FeO。

Method: 使用分子束外延（MBE）技术在4H-SiC$(0001)$衬底上生长(111)取向的FeO薄膜，并利用X射线衍射（XRD）、高分辨率扫描透射电子显微镜（S/TEM）、能量色散X射线光谱（EDS）和核级电子能量损失谱（EELS）对薄膜的结构和成分进行表征。

Result: 成功制备了厚度达180 nm的纯相FeO外延薄膜，薄膜表面平整，具有多层台阶。XRD、S/TEM、EDS和EELS证实了薄膜为纯相FeO，且FeO/SiC界面清晰。薄膜表现出轻微的晶格失配诱导的菱面体畸变。

Conclusion: 通过外延稳定化技术，可以成功地将热力学不稳定的FeO集成到商业上可用的半导体材料4H-SiC上，为整合功能性材料提供了新的途径。

Abstract: Iron(II) monoxide (FeO) is thermodynamically stable in the halite (w\"ustite)
structure only at elevated temperatures in a typically non-stoichiometric,
Fe-deficient, Fe$_{1-z}$O form that tends to phase separate and/or transform
into metallic $\alpha$-Fe and magnetite Fe$_3$O$_4$ at ambient conditions. Here
we report on the successful growth of up to 180 nm thick $(111)$-oriented FeO
heteroepitaxial films on slightly lattice-matched 4H-SiC$(0001)$ using
molecular beam epitaxy (MBE). The films have flat, terraced surfaces with tall
multi-layer steps. X-ray diffraction (XRD), high-resolution scanning
transmission electron microscopy (S/TEM), energy-dispersive X-ray spectroscopy
(EDS), and core-level electron energy loss spectroscopy (EELS) collectively
confirm the epilayer as phase-pure w\"ustite FeO, with atomically sharp FeO/SiC
interfaces. The films are found to exhibit a slight misfit strain-induced
rhombohedral distortion that does not appear to vary over the range of
thicknesses examined. These results demonstrate the power of epitaxial
stabilization for integrating a thermodynamically unstable, yet functionally
interesting material with a commercially available and technologically
important semiconductor platform.

</details>


### [252] [Active-Learning Inspired Ab Initio Theory-Experiment Loop Approach for Management of Material Defects: Application to Superconducting Qubits](https://arxiv.org/abs/2510.02544)
*Sarvesh Chaudhari,Cristobal Mendez,Rushil Choudhary,Tathagata Banerjee,Maciej Olszewski,Jadrien Paustian,Jaehong Choi,Zhaslan Baraissov,Raul Hernandez,David Muller,Britton Plourde,Gregory Fuchs,Valla Fatemi,Tomas Arias*

Main category: cond-mat.mtrl-sci

TL;DR: 表面氧化物会影响超导量子计算器件的性能，本文提出了一种通过选择金属覆盖层来抑制氧化物形成的方法，并使用DFT计算和机器学习预测了Zr、Hf和Ta等有效扩散阻挡层。


<details>
  <summary>Details</summary>
Motivation: 表面氧化物会降低铌基超导量子计算器件的性能，需要开发抑制氧化物形成的方法。

Method: 使用DFT计算氧空位和间隙能量作为热力学描述符，结合有限的实验结果训练逻辑回归模型，预测氧化物形成的可能性。分析了氧化物形成能和晶格失配度作为评估阻挡层性能的标准。

Result: 预测了Zr、Hf和Ta是有效的扩散阻挡层。氧化物形成能/氧原子是预测阻挡层性能的良好独立描述符。Zr、Ta和Sc被认为是特别有希望的候选材料。

Conclusion: 结合第一性原理理论、机器学习和有限的实验数据，提出了一种闭环策略，用于合理设计下一代材料，以抑制表面氧化物的形成。

Abstract: Surface oxides are associated with two-level systems (TLSs) that degrade the
performance of niobium-based superconducting quantum computing devices. To
address this, we introduce a predictive framework for selecting metal capping
layers that inhibit niobium oxide formation. Using DFT-calculated oxygen
interstitial and vacancy energies as thermodynamic descriptors, we train a
logistic regression model on a limited set of experimental outcomes to
successfully predict the likelihood of oxide formation beneath different
capping materials. This approach identifies Zr, Hf, and Ta as effective
diffusion barriers. Our analysis further reveals that the oxide formation
energy per oxygen atom serves as an excellent standalone descriptor for
predicting barrier performance. By combining this new descriptor with lattice
mismatch as a secondary criterion to promote structurally coherent interfaces,
we identify Zr, Ta, and Sc as especially promising candidates. This closed-loop
strategy integrates first-principles theory, machine learning, and limited
experimental data to enable rational design of next-generation materials.

</details>


### [253] [The line bundle regime and the scale-dependence of continuum dislocation dynamics](https://arxiv.org/abs/2510.02575)
*Joseph Pierre Anderson,Anter El-Azab*

Main category: cond-mat.mtrl-sci

TL;DR: CDD是金属介观刃位塑性的理论方法，本文提出了一个分辨率依赖的CDD公式，并评估了两种闭合方程，发现线束闭合比最大熵闭合更准确。


<details>
  <summary>Details</summary>
Motivation: 现有的CDD理论在处理无方向性的刃位线时存在损失，需要一种能连接不同分辨率的方法。

Method: 提出了一个基于局部平均线方向的取向涨落统计的CDD公式，并比较了线束闭合和最大熵闭合两种方法。

Result: 线束闭合关系在高达刀口间距一半的粗粒化长度下是准确的，而最大熵闭合关系在所有粗粒化长度下都与数据不符。

Conclusion: 本文提出的线束闭合关系能够更准确地描述CDD中的刃位线取向涨落，解决了现有理论在不同分辨率下的问题。

Abstract: Continuum dislocation dynamics (CDD) has become the state-of-the-art
theoretical approach for mesoscale dislocation plasticity of metals. Within
this approach, there are multiple CDD theories that can all be derived from the
principles of statistical mechanics. In these theories density-based measures
are used to represent dislocation lines. Establishing these density measures
requires some level of coarse graining with the result of losing track of some
parts of the dislocation population due to cancellation in the tangent vectors
of unaligned dislocations. The leading CDD theories either treat dislocations
as nearly parallel or distributed locally over orientation space. The
difference between these theories is a matter of the spatial resolution at
which the definition of the relevant dislocation density field holds: for fine
resolutions, single dislocations are resolved and there is no cancellation; for
coarse resolutions, whole dislocation loops could contribute at a single point
and there is complete cancellation. In the current work, a formulation of the
resolution-dependent transition between these limits is presented in terms of
the statistics of dislocation line orientation fluctuations about a local
average line direction. From this formulation, a study of the orientation
fluctuation behavior in intermediate resolution regimes is conducted. Two
possible closure equations for truncating the moment sequence of the
fluctuation distributions relating the two theories mentioned above are
evaluated from data, the newly introduced line bundle closure and the previous
standard maximum entropy closure relations. The line bundle closure relation is
shown to be accurate for coarse-graining lengths up to half the dislocation
spacing and the maximum entropy closure is found to poorly agree with the data
at all coarse-graining lengths.

</details>


### [254] [Improper-proper ferroelectric competition as a mechanism for multistate polarisation and ferrielectric-like behaviour](https://arxiv.org/abs/2510.02604)
*Cameron A. M Scott,Finlay D. Morrison,Nicholas. C. Bristowe*

Main category: cond-mat.mtrl-sci

TL;DR: 当固有和非固有不稳定性同时存在并竞争时，非固有铁电材料可以显示在多个极化状态之间切换，这在六方钨青铜材料中得到了体现，并可能用于多状态存储器或神经形态计算。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探索当固有和非固有不稳定性同时存在并竞争时，非固有铁电材料可以显示在多个极化状态之间切换的可能性，并提出六方钨青铜材料可能是一个典型的例子。

Method: 利用第一性原理计算，研究六方钨青铜材料在固有和非固有不稳定性竞争下的行为，并分析其功能特性。

Result: 研究发现，当固有和非固有不稳定性同时存在并竞争时，非固有铁电材料可以显示在多个极化状态之间切换，其功能特性类似于“亚铁电体”，并表现出三态磁滞回线。

Conclusion: 六方钨青铜材料可能是一种潜在的材料，能够实现多极化状态的切换，这为开发非易失性多状态存储器或神经形态计算提供了新的可能性。

Abstract: In this paper, we re-explore a simple textbook Landau model describing
improper ferroelectricity and show that in the limit where both proper and
improper instabilities exist and compete, improper ferroelectrics can display
switching between multiple polarisation states. Using first principles
calculations we highlight how the hexagonal tungsten bronze materials may be an
archetypal case, with the possibility to switch between improper and proper
phases. The resulting functional characteristics are akin to "ferrielectrics",
with switching behaviour in the form of a triple hysteresis loop. Such
functionality could be ideal for creating non-volatile multistate systems for
use in memory devices or as a backbone for neuromorphic computing.

</details>


### [255] [Kolmogorov-Arnold Networks in Thermoelectric Materials Design](https://arxiv.org/abs/2510.02681)
*Marco Fronzi,Michael J. Ford,Kamal Singh Nayal,Olexandr Isayev,Catherine Stampfl*

Main category: cond-mat.mtrl-sci

TL;DR: KANs在热电材料领域展现出与MLPs相当的预测精度，同时提供了可解释的符号化结构-性质关系，有助于反向设计具有目标热电特性的材料。


<details>
  <summary>Details</summary>
Motivation: 开发兼具准确性和可解释性的模型，以指导高性能热电材料的发现，克服传统机器学习方法缺乏物理洞察力的局限性。

Method: 应用Kolmogorov--Arnold Networks (KANs)来预测热电材料的塞贝克系数和带隙，并与多层感知机 (MLPs) 和文献模型进行比较。

Result: KANs在预测精度上达到与MLPs相当的水平，并能够提供明确的符号化结构-性质关系，这对于提取具有物理意义的函数形式至关重要。与其他文献模型相比，KANs展现出更强的鲁棒性和泛化能力。

Conclusion: KANs为材料科学领域提供了一个强大的新框架，能够有效结合预测性能与科学可解释性，为反向工程具有特定热电性能的材料提供了新的途径。

Abstract: The discovery of high-performance thermoelectric materials requires models
that are both accurate and interpretable. Traditional machine learning
approaches, while effective at property prediction, often act as black boxes
and provide limited physical insight. In this work, we introduce
Kolmogorov--Arnold Networks (KANs) for the prediction of thermoelectric
properties, focusing on the Seebeck coefficient and band gap. Compared to
multilayer perceptrons (MLPs), KANs achieve comparable predictive accuracy
while offering explicit symbolic representations of structure--property
relationships. This dual capability enables both reliable predictions and the
extraction of physically meaningful functional forms. Benchmarking against
literature models further highlights the robustness and generalisability of the
approach. Our findings demonstrate that KANs provide a powerful framework for
reverse engineering materials with targeted thermoelectric properties, bridging
the gap between predictive performance and scientific interpretability.

</details>


### [256] [Photovoltaic Performance of a Rotationally Faulted Multilayer Graphene/n-Si Schottky Junction](https://arxiv.org/abs/2510.02786)
*Hojun Im,Masahiro Teraoka*

Main category: cond-mat.mtrl-sci

TL;DR: 通过旋错多层石墨烯（rf-MLG）/n-Si肖特基结器件的光伏性能


<details>
  <summary>Details</summary>
Motivation: 探索rf-MLG/n-Si肖特基结在光伏器件中的应用潜力

Method: 通过化学气相沉积法合成厚度可控的rf-MLG，并采用无聚合物工艺将其转移到n-Si衬底上，构建肖特基结器件。

Result: 器件表现出1.67的理想因子，在±1.0 V下约为4x10^5的整流比，0.83 eV的肖特基势垒高度。光电流与光照强度呈强线性关系。在540 nm处具有约26%的峰值外部量子效率，在410 nm处具有约97%的峰值内部量子效率。瞬态光电流和光伏测量显示提取时间约为1微秒，复合时间约为几毫秒。

Conclusion: rf-MLG/n-Si肖特基结器件形成良好，光伏性能可与单层石墨烯（SLG）器件媲美，在光电子学领域具有应用潜力。

Abstract: We report the fabrication and photovoltaic performance of a rotationally
faulted multilayer graphene (rf-MLG)/n-Si Schottky junction device. A
thickness-controlled rf-MLG is synthesized using a 5 {\mu}m Ni foil catalyst
via the chemical vapor deposition method and transferred to the n-Si substrate
via a polymer-free process, enabling facile and cost-effective fabrication. The
device demonstrates an ideality factor of 1.67, a rectification factor of
approximately 4x10^5 at {\pm}1.0 V, and a Schottky barrier height of 0.83 eV. A
strong linear relationship between light intensity and photocurrent is also
observed. Furthermore, the device exhibits a peak external quantum efficiency
of ~26% at 540 nm and a peak internal quantum efficiency of ~97% at 410 nm.
Transient photocurrent and photovoltaic measurements show approximately
one-microsecond extraction and several-millisecond recombination times,
respectively, revealing effective charge collection for photovoltaic
applications. These results indicate that the rf-MLG/n-Si Schottky junction is
well-formed and achieves performance comparable to that of SLG devices,
demonstrating its potential for optoelectronic applications.

</details>


### [257] [A physics-informed neural network approach to the point defect model for electrochemical oxide film growth](https://arxiv.org/abs/2510.02872)
*Mohid Farooqi,Ingmar Bösing,Conrad G. Tetsassi Feugmo*

Main category: cond-mat.mtrl-sci

TL;DR: PINNs被应用于模拟卤化物游离溶液中的氧化物薄膜生长，并解决了相关的四个主要失效模式，通过采用非量纲化、自适应损失平衡和鲁棒的边界条件执行等技术，提高了模拟的可靠性和物理保真度。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探索物理信息神经网络（PINNs）在材料工程中解决复杂多物理场问题（特别是卤化物游离溶液中的氧化物薄膜生长）的应用，并系统地识别和分析该领域中PINNs面临的关键失效模式。

Method: 本研究采用点缺陷模型（PDM），并实施和验证了非量纲化、基于神经切线核的自适应损失平衡以及鲁棒的边界条件执行等技术，以应对PINNs在模拟氧化物薄膜生长过程中遇到的不平衡损失、数值不稳定性、边界条件执行困难以及收敛到物理上无意义解等问题。

Result: 通过应用所选技术，本研究有效地解决了PINNs在模拟电化学物理场中遇到的失效模式，显著提高了模拟的可靠性和物理保真度，证明了所提出策略的有效性。

Conclusion: PINNs作为一种整合物理定律的AI框架，在模拟氧化物薄膜生长方面展现出巨大潜力。通过解决关键的失效模式并采用有效的技术策略，可以实现更可靠和物理上准确的模拟，这对电化学物理学领域具有实际意义。

Abstract: Physics-informed neural networks (PINNs) offer a novel AI-driven framework
for integrating physical laws directly into neural network models, facilitating
the solution of complex multiphysics problems in materials engineering. This
study systematically explores the application of PINNs to simulate oxide film
layer growth in halide-free solutions using the point defect model (PDM). We
identify and analyze four key failure modes in this context: imbalanced loss
components across different physical processes, numerical instabilities due to
variable scale disparities, challenges in enforcing boundary conditions within
multiphysics systems, and convergence to mathematically valid but physically
meaningless solutions. To overcome these challenges, we implement and validate
established techniques including nondimensionalization for training
stabilization, Neural Tangent Kernel-based adaptive loss balancing, and robust
enforcement of boundary conditions. Our results demonstrate the effectiveness
of these strategies in enhancing the reliability and physical fidelity of PINN
simulations in electrochemical physics, highlighting the novelty and practical
impact of our approach.

</details>


### [258] [Redox Chemistry of LiCoO$_2$, LiNiO$_2$, and LiNi$_{1/3}$Mn$_{1/3}$Co$_{1/3}$O$_2$ Cathodes: Deduced via XPS, DFT+DMFT, and Charge Transfer Multiplet Simulations](https://arxiv.org/abs/2510.02875)
*Ruiwen Xie,Maximilian Mellin,Wolfram Jaegermann,Jan P. Hofmann,Frank M. F. de Groot,Hongbin Zhang*

Main category: cond-mat.mtrl-sci

TL;DR: 通过结合XPS、DFT+DMFT计算和CTM模型模拟，研究了锂脱出过程中过渡金属3d和氧2p轨道杂化演化规律，发现电子转移不遵循刚带模型，并提出2p XPS卫星峰强度可作为氧化还原化学的有效指标。


<details>
  <summary>Details</summary>
Motivation: 理解锂脱出（充电）过程中物理化学性质的演变对于优化电池正极材料至关重要。

Method: 结合X射线光电子能谱（XPS）、密度泛函理论加动力学平均场理论（DFT+DMFT）计算以及电荷转移多重（CTM）模型模拟，研究过渡金属（TM）3d和氧2p轨道杂化随锂脱出的演化。

Result: 研究发现，对于所研究的锂过渡金属氧化物，电子转移和锂离子迁移并不遵循刚带模型，而是受到TM 3d和O 2p状态杂化变化的影响。该集成方法还确定TM的2p XPS卫星峰强度是氧化还原化学的有效指标。

Conclusion: 通过结合理论计算和实验研究，揭示了锂脱出过程中电子结构的变化机制，并提出了利用XPS卫星峰强度表征氧化还原化学的方法，为设计更高效的电池材料奠定了基础。

Abstract: Understanding the evolution of the physicochemical bulk properties during the
Li deintercalation (charging) process is critical for optimizing battery
cathode materials. In this study, we combine X-ray photoelectron spectroscopy
(XPS), density functional theory plus dynamical mean-field theory (DFT+DMFT)
calculations, and charge transfer multiplet (CTM) model simulations to
investigate how hybridization between transition metal (TM) 3d and oxygen 2p
orbitals evolves with Li deintercalation. Based on the presented approach
combining theoretical calculations and experimental studies of pristine and
deintercalated cathodes, two important problems of ion batteries can be
addressed: i) the detailed electronic structure and involved changes with
deintercalation providing information of the charge compensation mechanism, and
ii) the precise experimental analysis of XPS data which are dominated by charge
transfer coupled to final-state effects affecting the satellite structure. As
main result for the investigated Li TM oxides, it can be concluded that the
electron transfer coupled to the Li$^{+}$-ion migration does not follow a rigid
band model but is modified due to changes in TM 3d and O 2p states
hybridization. Furthermore, this integrated approach identifies the 2p XPS
satellite peak intensity of TM as an effective indicator of the redox
chemistry. With that the redox chemistry of cathodes can be deduced, thus
offering a foundation for designing more efficient battery materials.

</details>


### [259] [Subthermal Mean Transverse Energies Induced by Electron Refraction on the Jump in Mass at the Surface of Multialkali Photocathodes](https://arxiv.org/abs/2510.02897)
*S. A. Rozhkov,V. V. Bakin,H. E. Scheibler,V. S. Rusetsky,D. V. Gorshkov,D. A. Kustov,V. A. Golyashov,V. L. Alperovich,O. E. Tereshchenko*

Main category: cond-mat.mtrl-sci

TL;DR: 多碱光电阴极在室温下具有比单晶GaAs低一倍的平均横向能量（MTE），这归因于电子在质量跃变处的折射效应，为高亮度电子源的开发提供了机会。


<details>
  <summary>Details</summary>
Motivation: 寻找具有低平均横向能量（MTE）和低固有发射率的光电阴极材料对于粒子和固体物理学领域至关重要。

Method: 研究了多碱Na2KSb(Cs,Sb)光电阴极的MTE，并与单晶p-GaAs(Cs,O)进行了比较。通过分析电子折射效应来解释低MTE的原因。

Result: 在负有效电子亲和势（NEA）状态下，Na2KSb(Cs,Sb)光电阴极在室温下的MTE比单晶p-GaAs(Cs,O）低一倍。高达一半的光电子以窄角度发射，MTE为9 meV。从NEA到正有效亲和势的转变导致MTE低于热能。

Conclusion: 电子折射效应在多碱光电阴极中得到了证实，这为开发高亮度、超冷且坚固的电子源提供了新的可能性。

Abstract: The search for photocathode materials with low mean transverse energies
(MTEs) and, hence, low intrinsic emittance is of crucial importance for various
fields of particle and solid state physics. Here, we demonstrate that
polycrystalline multialkali Na$_{2}$KSb(Cs,Sb) photocathodes with negative
effective electron affinity (NEA) have MTE values at room temperature by a
factor of 2 lower than those of monocrystalline \textit{p}-GaAs(Cs,O)
photocathodes. These low MTE values are due to the electron refraction on the
jump in mass, between a small effective mass in Na$_{2}$KSb and free electron
mass in vacuum. It is proved that, at the NEA state, up to half of
photoelectrons are emitted in a narrow-angle cone with the fractional MTE of
9\,meV at room temperature. We also showed that the transition from NEA to
positive effective affinity results in the subthermal total MTE of the
Na$_{2}$KSb(Cs,Sb) photocathode, along with quantum efficiency of about
10$^{-2}$. The physical reasons for the manifestation of the refraction effect
in multialkali photocathodes are discussed, opening up opportunities for the
development of high-brightness and ultracold robust electron sources.

</details>


### [260] [Structural Chirality and Natural Optical Activity across the $α$-to-$β$ Phase Transition in SiO$_2$ and AlPO$_4$ from first-principles](https://arxiv.org/abs/2510.03047)
*F. Gómez-Ortiz,A. Zabalo,A. M. Glazer,E. E. McCabe,A. H. Romero,E. Bousquet*

Main category: cond-mat.mtrl-sci

TL;DR: 光学活性与结构手性之间的关系比传统观点更为复杂，并非由空间群的手性直接决定。


<details>
  <summary>Details</summary>
Motivation: 纠正光学活性与结构手性之间关系的普遍误解，并探索其背后的微观机制。

Method: 使用第一性原理研究SiO2和AlPO4晶体在六方（P6422或P6222）到三角（P3121或P3221）空间群的同质异形结构相变过程中光学活性的演变。

Result: 光学活性在手性和非手性晶体结构中都可能出现，并且其旋转方向不能简单地根据空间群的手性推断。对于SiO2和AlPO4，相变并未改变光学活性的符号，尽管螺距轴类型发生了逆转。

Conclusion: 光学旋转的方向由原子尺度上最可极化原子的螺旋性决定，而非空间群符号中名义上的螺距轴手性。

Abstract: Natural optical activity (NOA), the ability of a material to rotate the plane
of polarized light, has traditionally been associated with structural
chirality. However, this relationship has often been oversimplified, leading to
conceptual misunderstandings, particularly when attempts are made to directly
correlate structural handedness with optical rotatory power. In reality, the
relationship between chirality and NOA is more nuanced: optical activity can
arise in both chiral and achiral crystal structures, and the sign of the
rotation cannot necessarily be inferred from the handedness of the space group.
% In this work, we conduct a first-principles investigation of natural optical
activity in SiO$_2$ and AlPO$_4$ crystals, focusing on their enantiomorphic
structural phase transition from high-symmetry hexagonal ($P6_422$ or $P6_222$)
to low-symmetry trigonal ($P3_121$ or $P3_221$) space groups. This transition,
driven by the condensation of a zone-center $\Gamma_3$ phonon mode, reverses
the screw axis type given by the space group symbol while leaving the sign of
the optical activity unchanged. By following the evolution of the structure and
the optical response along the transition pathway, we clarify the microscopic
origin of this behavior. We demonstrate that the sense of optical rotation is
determined not by the nominal helicity of the screw axis given in the space
group symbol, but by the atomic-scale helicity of the most polarizable atoms of
the structure.

</details>


### [261] [Electrically modulated light-emitting diodes driven by resonant and antiresonant tunneling between Cr$_2$Ge$_2$Te$_6$ electrodes](https://arxiv.org/abs/2510.03054)
*Natalia Zawadzka,Kristina Vaklinova,Tomasz Woźniak,Mihai I. Sturza,Holger Kohlmann,Kenji Watanabe,Takashi Taniguchi,Adam Babiński,Maciej Koperski,Maciej R. Molas*

Main category: cond-mat.mtrl-sci

TL;DR: 使用具有不同能带结构的范德华材料制备双极垂直隧道结，以实现LED的光学活性材料中的电子和空穴的注入，从而实现光电器件的定制化。通过在发光材料中形成能量势垒来调节注入效率。本研究提出了一种新的方法，通过制造由Cr2Ge2Te6电极、hBN隧道势垒和单层WSe2发光介质组成的隧道结，来实现共振隧穿条件。


<details>
  <summary>Details</summary>
Motivation: 探索电子隧穿机制以定制光电器件特性。

Method: 制造一种由gapped材料组成的隧道结，包括Cr2Ge2Te6电极、hBN隧道势垒和单层WSe2发光介质，并研究其在LED中的应用。

Result: 实验结果显示，电致发光强度随隧穿偏压呈现非单调演变。Cr2Ge2Te6电极的态密度相对排列对电子隧穿特性起主导作用。所提出的器件结构能够实现室温下具有电调制发射强度的LED。

Conclusion: 所提出的独特器件结构为实现室温下具有电调制发射强度的LED提供了一条通用途径。

Abstract: Exploring the electron tunneling mechanisms in diverse materials systems
constitutes a versatile strategy for tailoring the properties of optoelectronic
devices. In this domain, bipolar vertical tunneling junctions composed of van
der Waals materials with vastly different electronic band structures enable
simultaneous injection of electrons and holes into an optically active
material, providing a universal blueprint for light-emitting diodes (LEDs).
Efficient modulation of the injection efficiency has previously been
demonstrated by creating resonant states within the energy barrier formed by
the luminescent material. Here, we present an alternative approach towards
resonant tunneling conditions by fabricating tunneling junctions composed
entirely from gapped materials: Cr$_2$Ge$_2$Te$_6$ as electrodes, hBN as a
tunneling barrier, and monolayer WSe$_2$ as a luminescent medium. The
characterization of such LEDs revealed a nonmonotonous evolution of the
electroluminescence intensity with the tunneling bias. The dominant role
driving the characteristics of the electron tunneling was associated with the
relative alignment of the density of states in Cr$_2$Ge$_2$Te$_6$ electrodes.
The unique device architecture introduced here presents a universal pathway
towards LEDs operating at room temperature with electrically modulated emission
intensity.

</details>


### [262] [Thermal assisted transport of biexcitons in monolayer WSe2](https://arxiv.org/abs/2510.03092)
*Dorian Béret,Louka Hemmen,Vishwas Jindal,Sreyan Raha,Thierry Amand,Delphine Lagarde,Andrea Balocchi,Cédric Robert,Helene Carrere,Xavier Marie,Pierre Renucci,Laurent Lombez*

Main category: cond-mat.mtrl-sci

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Studies of excitonic transport in transition metal dichalcogenide monolayers
have attracted increasing interest in recent years in order to develop
nano-optoelectronic devices made with 2D materials. These studies began with
low to moderate optical excitation regimes, and more recently have focused on
high injection regimes where nonlinear effects appear. This article is focused
on the transport of biexcitons by spatially and temporally resolved
photoluminescence spectroscopy at high excitation flux. The study is carried
out on a high-quality WSe$_2$ monolayer encapsulated in hexagonal boron
nitride. The results show that a Seebeck current affects transport in
connection with the presence of hot biexcitons. In particular, we observe the
formation of spatial rings, also called halos, which have been observed in
other excitonic gases. These results tend to generalize the importance of
high-energy populations in excitonic transport in TMD, even for complex and
heavy excitonic particles.

</details>


### [263] [Sliding multiferroicity in hexagonal stacked CrI3](https://arxiv.org/abs/2510.03220)
*Carter Fox,Jose D. Mella,Jack Rollins,Yangchen He,Yulu Mao,Haotian Jiang,Alaina Drew,Hongrui Ma,Takashi Taniguchi,Kenji Watanabe,Ying Wang,Daniel Rhodes,Salvador Barraza-Lopez,Jun Xiao*

Main category: cond-mat.mtrl-sci

TL;DR: 通过堆叠顺序工程在二维（2D）CrI3中实现了具有室温铁电性的新型二维滑动多铁性材料，并实现了低至0.4V的低功耗磁开关。


<details>
  <summary>Details</summary>
Motivation: 开发具有高效磁电耦合的新型二维（2D）多铁性材料，以理解新奇序的相互作用物理学，并推动高性能计算应用。

Method: 提出堆叠顺序工程，以创建基于极性六方堆叠（H-stacked）CrI3的新型二维多铁性材料，即滑动多铁性材料。通过拉曼光谱、二次谐波产生光谱和输运测量证实了室温滑动铁电性的出现。通过门控依赖性反射磁圆二色性、第一性原理计算和建模，研究了滑动铁电性与通过层间自旋极化电荷转移产生的界面铁磁性之间的相互作用。

Result: 实现了室温滑动铁电性，并通过层间自旋极化电荷转移实现了滑动铁电性与界面铁磁性之间的耦合，从而在H-stacked CrI3中实现了低至0.4V的非易失性磁开关。

Conclusion: 提出了二维磁体极性堆叠顺序工程作为一种通用方法，可以创建具有高效磁电耦合的非易失性二维多铁性材料，为原子级薄器件的低功耗电子和自旋电子学开辟了道路。

Abstract: Developing new multiferroics at the two-dimensional (2D) limit with
energy-efficient magnetoelectric coupling can inform the interplay physics of
novel orders and advance on-chip high-performance computing applications. Here
we apply stacking order engineering to create a new type of 2D multiferroics,
namely sliding multiferroics, based on polar hexagonal stacked (H-stacked)
CrI3. This new stacking order removes structural inversion symmetry and gives
rise to room temperature sliding ferroelectricity, as confirmed by Raman
spectroscopy, second harmonic generation spectroscopy and electrical transport
measurements. Building upon the gate-dependent reflective magnetic circular
dichroism, first-principles calculations, and modeling, sliding
ferroelectricity is shown to interplay with an emergent interfacial
ferromagnetism via interlayer spin-polarized charge transfer. This coupling
mechanism results in non-volatile magnetic switching by as low as 0.4V across
the H-stacked CrI3. Our demonstration introduces polar stacking order
engineering of 2D magnets as a general approach to create non-volatile 2D
multiferroics with efficient magnetoelectric coupling, paving the way for
low-power electronics and spintronics at the atomically thin limit.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [264] [NEURODNAAI: Neural pipeline approaches for the advancing dna-based information storage as a sustainable digital medium using deep learning framework](https://arxiv.org/abs/2510.02417)
*Rakesh Thakur,Lavanya Singh,Yashika,Manomay Bundawala,Aruna Kumar*

Main category: cs.ET

TL;DR: DNA具有高密度和耐久性，是很有前景的数字信息存储介质。然而，合成成本、测序错误和生物学限制（GC含量不平衡、同聚物）等挑战阻碍了其实际应用。


<details>
  <summary>Details</summary>
Motivation: 解决DNA存储实际部署中的挑战，如合成成本、测序错误和生物学限制（GC含量不平衡、同聚物）。

Method: 提出了一种名为NeuroDNAAI的框架，该框架借鉴了量子并行性概念，以增强编码多样性和韧性，并整合了生物学约束和深度学习来改善DNA存储中的错误缓解。NeuroDNAAI将二进制数据流编码为DNA序列，通过带有替换、插入和删除噪声的信道传输，并高保真地重建数据。

Result: 与传统的基于提示或基于规则的方案相比，NeuroDNAAI在应对真实噪声方面表现出优越的准确性。在基准数据集上的实验表明，对于文本和图像，其比特错误率很低。

Conclusion: NeuroDNAAI通过将理论、工作流程和模拟统一到一个管道中，实现了可扩展、生物学上有效的档案DNA存储。

Abstract: DNA is a promising medium for digital information storage for its exceptional
density and durability. While prior studies advanced coding theory, workflow
design, and simulation tools, challenges such as synthesis costs, sequencing
errors, and biological constraints (GC-content imbalance, homopolymers) limit
practical deployment. To address this, our framework draws from quantum
parallelism concepts to enhance encoding diversity and resilience, integrating
biologically informed constraints with deep learning to enhance error
mitigation in DNA storage. NeuroDNAAI encodes binary data streams into symbolic
DNA sequences, transmits them through a noisy channel with substitutions,
insertions, and deletions, and reconstructs them with high fidelity. Our
results show that traditional prompting or rule-based schemes fail to adapt
effectively to realistic noise, whereas NeuroDNAAI achieves superior accuracy.
Experiments on benchmark datasets demonstrate low bit error rates for both text
and images. By unifying theory, workflow, and simulation into one pipeline,
NeuroDNAAI enables scalable, biologically valid archival DNA storage

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [265] [HALO: Memory-Centric Heterogeneous Accelerator with 2.5D Integration for Low-Batch LLM Inference](https://arxiv.org/abs/2510.02675)
*Shubham Negi,Kaushik Roy*

Main category: cs.AR

TL;DR: LLM推理面临挑战，提出异构内存中心加速器HALO，优化预填充和解码阶段，实现显著加速。


<details>
  <summary>Details</summary>
Motivation: LLM推理，特别是低批次和长上下文场景，由于预填充和解码阶段的需求差异，对加速器设计提出了挑战，现有方法未充分探索此场景。

Method: 提出异构内存中心加速器HALO，集成HBM的计算 in DRAM (CiD) 和片上模拟计算 in Memory (CiM)，并采用阶段感知映射策略，将计算密集型操作映射到CiM，内存密集型操作映射到CiD。

Result: 在LLaMA-2 7B和Qwen3 8B模型上评估，HALO相较于AttAcc（优化映射）和CENT（全CiD）分别实现了高达18倍和2.5倍的几何平均加速。

Conclusion: HALO通过异构设计和阶段感知映射，有效解决了低批次LLM推理的挑战，证明了其在性能上的优越性。

Abstract: The rapid adoption of Large Language Models (LLMs) has driven a growing
demand for efficient inference, particularly in latency-sensitive applications
such as chatbots and personalized assistants. Unlike traditional deep neural
networks, LLM inference proceeds in two distinct phases: the prefill phase,
which processes the full input sequence in parallel, and the decode phase,
which generates tokens sequentially. These phases exhibit highly diverse
compute and memory requirements, which makes accelerator design particularly
challenging. Prior works have primarily been optimized for high-batch inference
or evaluated only short input context lengths, leaving the low-batch and long
context regime, which is critical for interactive applications, largely
underexplored.
  We propose HALO, a heterogeneous memory centric accelerator designed for
these unique challenges of prefill and decode phases in low-batch LLM
inference. HALO integrates HBM based Compute-in-DRAM (CiD) with an on-chip
analog Compute-in-Memory (CiM), co-packaged using 2.5D integration. To further
improve the hardware utilization, we introduce a phase-aware mapping strategy
that adapts to the distinct demands of the prefill and decode phases. Compute
bound operations in the prefill phase are mapped to CiM to exploit its high
throughput matrix multiplication capability, while memory-bound operations in
the decode phase are executed on CiD to benefit from reduced data movement
within DRAM. Additionally, we present an analysis of the performance tradeoffs
of LLMs under two architectural extremes: a fully CiD and a fully on-chip
analog CiM design to highlight the need for a heterogeneous design. We evaluate
HALO on LLaMA-2 7B and Qwen3 8B models. Our experimental results show that LLMs
mapped to HALO achieve up to 18x geometric mean speedup over AttAcc, an
attention-optimized mapping and 2.5x over CENT, a fully CiD based mapping.

</details>


### [266] [A Hardware Accelerator for the Goemans-Williamson Algorithm](https://arxiv.org/abs/2510.02863)
*D. A. Herrera-Martí,E. Guthmuller,J. Fereyre*

Main category: cs.AR

TL;DR: Max-Cut的半定规划松弛方法在结合扩展浮点精度后，有望提升求解速度，尤其在大规模问题上。


<details>
  <summary>Details</summary>
Motivation: Max-Cut问题是量子和经典优化器局部搜索算法的基准。与局部搜索不同，凸半定规划松弛提供了最坏情况下的保证，适用于基准构建和性能关键场景。

Method: 研究了如何在代数子程序（如共轭梯度法）中引入扩展浮点精度，并估计了支持扩展精度的硬件的预期加速效果。

Result: 在涉及共轭梯度等间接矩阵求逆方法时，提高内部工作精度可缩短求解时间，且加速因子随系统规模增大而增大。

Conclusion: 扩展浮点精度在处理大规模Max-Cut问题时，通过优化共轭梯度等求解方法，能够有效减少求解时间，并带来与系统规模相关的加速效应。

Abstract: The combinatorial problem Max-Cut has become a benchmark in the evaluation of
local search heuristics for both quantum and classical optimisers. In contrast
to local search, which only provides average-case performance guarantees, the
convex semidefinite relaxation of Max-Cut by Goemans and Williamson, provides
worst-case guarantees and is therefore suited to both the construction of
benchmarks and in applications to performance-critic scenarios.
  We show how extended floating point precision can be incorporated in
algebraic subroutines in convex optimisation, namely in indirect matrix
inversion methods like Conjugate Gradient, which are used in Interior Point
Methods in the case of very large problem sizes. Also, an estimate is provided
of the expected acceleration of the time to solution for a hardware
architecture that runs natively on extended precision. Specifically, when using
indirect matrix inversion methods like Conjugate Gradient, which have lower
complexity than direct methods and are therefore used in very large problems,
we see that increasing the internal working precision reduces the time to
solution by a factor that increases with the system size.

</details>


### [267] [A Resource-Driven Approach for Implementing CNNs on FPGAs Using Adaptive IPs](https://arxiv.org/abs/2510.02990)
*Philippe Magalhães,Virginie Fresse,Benoît Suffran,Olivier Alata*

Main category: cs.AR

TL;DR: 本工作提出了一种新颖的、资源高效的卷积IP库，可自动适应FPGA资源，适用于实时、低延迟的AI应用。


<details>
  <summary>Details</summary>
Motivation: 为了满足对实时、低延迟AI应用日益增长的需求，FPGA被广泛用于CNN实现，因其在能效和性能上优于GPU。

Method: 开发了一套参数化的、使用定点运算的VHDL卷积IP库，其中包含四个针对特定资源限制定制的IP，以实现DSP使用、逻辑消耗和精度的灵活性。

Result: 在Zynq UltraScale+ FPGA上的实验结果展示了性能与资源使用的权衡，并将该方法与最近的FPGA加速技术进行了比较，证明了其通用性和架构无关性。

Conclusion: 提出的IP库为FPGA上的CNN加速提供了一种灵活且高效的解决方案，未来的工作将扩展到包括池化和激活函数，以实现更广泛的应用。

Abstract: The increasing demand for real-time, low-latency artificial intelligence
applications has propelled the use of Field-Programmable Gate Arrays (FPGAs)
for Convolutional Neural Network (CNN) implementations. FPGAs offer
reconfigurability, energy efficiency, and performance advantages over GPUs,
making them suitable for edge devices and embedded systems. This work presents
a novel library of resource-efficient convolution IPs designed to automatically
adapt to the available FPGA resources. Developed in VHDL, these IPs are
parameterizable and utilize fixed-point arithmetic for optimal performance.
Four IPs are introduced, each tailored to specific resource constraints,
offering flexibility in DSP usage, logic consumption, and precision.
Experimental results on a Zynq UltraScale+ FPGA highlight the trade-offs
between performance and resource usage. The comparison with recent FPGA-based
CNN acceleration techniques emphasizes the versatility and independence of this
approach from specific FPGA architectures or technological advancements. Future
work will expand the library to include pooling and activation functions,
enabling broader applicability and integration into CNN frameworks.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [268] [Market-Based Data Subset Selection -- Principled Aggregation of Multi-Criteria Example Utility](https://arxiv.org/abs/2510.02456)
*Ashish Jha,Valentin Leplat,AH Phan*

Main category: cs.LG

TL;DR: 提出一种基于市场的选择器，通过成本函数预测市场 (LMSR) 为每个样本定价，信号充当交易者，单一流动性参数控制集中度，主题归一化稳定校准。


<details>
  <summary>Details</summary>
Motivation: 选择训练数据子集困难，因为样本效用信号（不确定性、稀有性、多样性等）异质且通常具有临时权重。

Method: 使用成本函数预测市场 (LMSR) 为每个样本定价，单一流动性参数控制集中度，并引入主题归一化。代币预算通过每代币价格规则 $ho=p/	ext{l}^{	ext{γ}}$ 处理，其中 $	ext{γ}$ 引入了可解释的长度偏差；轻量级多样性头部提高了覆盖范围。

Result: 在 GSM8K（60k 代币预算）上，具有多样性的市场与强大的单一信号基线相媲美，同时降低了种子方差，选择开销低于 0.1 GPU-小时；在 AGNews（保留 5-25%）上，市场（具有轻微平衡）在提高的平衡性和稳定性下提供了具有竞争力的准确性。

Conclusion: 该框架将多信号数据策管统一在固定计算下，用于提示级推理和分类。

Abstract: Selecting a small yet useful subset of training data is hard because signals
of example utility (uncertainty, rarity, diversity, etc.) are heterogeneous and
typically combined with ad hoc weights. We propose a market-based selector that
prices each example via a cost-function prediction market (LMSR), signals act
as traders, a single liquidity parameter controls concentration, and topic-wise
normalization stabilizes calibration. Token budgets are handled explicitly by a
price-per-token rule $\rho=p/\ell^{\gamma}$, with $\gamma$ exposing an
interpretable length bias; a lightweight diversity head improves coverage. We
quantify coverage via topic cluster coverage and effective sample size. On the
theory side, we show that LMSR implements a maximum-entropy aggregation with
exponential weighting and a convex objective, yielding transparent knobs for
aggregation strength. Empirically, on GSM8K (60k-token budget) the market with
diversity achieves parity with strong single-signal baselines while reducing
seed variance and incurring $<\!0.1$ GPU-hr selection overhead; on AGNews at
kept=5-25\% the market (with light balancing) delivers competitive accuracy
with improved balance and stability. The framework unifies multi-signal data
curation under fixed compute for prompt-level reasoning and classification.

</details>


### [269] [Extreme value forecasting using relevance-based data augmentation with deep learning models](https://arxiv.org/abs/2510.02407)
*Junru Hua,Rahul Ahluwalia,Rohitash Chandra*

Main category: cs.LG

TL;DR: 利用生成对抗网络（GANs）和SMOTE等数据增强技术，结合Conv-LSTM和BD-LSTM等深度学习模型，优化极端值预测的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 极端值预测在金融、气候变化等领域具有重要应用，但该领域面临挑战。本研究旨在提出一种用于极端值预测的数据增强框架。

Method: 研究结合了GANs和SMOTE等数据增强技术，以及Conv-LSTM和BD-LSTM等深度学习模型，用于多步极端值预测。提出了基于相关性函数的数据增强策略，并评估了不同数据增强模型在预测准确性（包括极端区域）和计算效率方面的适用性。

Result: SMOTE-based策略在短期和长期预测中均表现出优越的适应性和性能。Conv-LSTM模型在周期性、稳定型数据集上表现优异，而BD-LSTM模型在混沌或非平稳序列上表现更好。

Conclusion: 本研究提出的数据增强框架能够有效提升极端值预测的性能。SMOTE-based策略和深度学习模型（Conv-LSTM、BD-LSTM）的选择应根据具体数据集的特点进行。

Abstract: Data augmentation with generative adversarial networks (GANs) has been
popular for class imbalance problems, mainly for pattern classification and
computer vision-related applications. Extreme value forecasting is a
challenging field that has various applications from finance to climate change
problems. In this study, we present a data augmentation framework for extreme
value forecasting. In this framework, our focus is on forecasting extreme
values using deep learning models in combination with data augmentation models
such as GANs and synthetic minority oversampling technique (SMOTE). We use deep
learning models such as convolutional long short-term memory (Conv-LSTM) and
bidirectional long short-term memory (BD-LSTM) networks for multistep ahead
prediction featuring extremes. We investigate which data augmentation models
are the most suitable, taking into account the prediction accuracy overall and
at extreme regions, along with computational efficiency. We also present novel
strategies for incorporating data augmentation, considering extreme values
based on a relevance function. Our results indicate that the SMOTE-based
strategy consistently demonstrated superior adaptability, leading to improved
performance across both short- and long-horizon forecasts. Conv-LSTM and
BD-LSTM exhibit complementary strengths: the former excels in periodic, stable
datasets, while the latter performs better in chaotic or non-stationary
sequences.

</details>


### [270] [OpenTSLM: Time-Series Language Models for Reasoning over Multivariate Medical Text- and Time-Series Data](https://arxiv.org/abs/2510.02410)
*Patrick Langer,Thomas Kaar,Max Rosenblattl,Maxwell A. Xu,Winnie Chow,Martin Maritsch,Aradhana Verma,Brian Han,Daniel Seung Kim,Henry Chubb,Scott Ceresnak,Aydin Zahedivash,Alexander Tarlochan Singh Sandhu,Fatima Rodriguez,Daniel McDuff,Elgar Fleisch,Oliver Aalami,Filipe Barata,Paul Schmiedmayer*

Main category: cs.LG

TL;DR: OpenTSLM是一个集成了时间序列作为原生模态的语言模型家族，能够处理任意长度的多个时间序列，并在医学推理任务中展现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在处理多模态数据方面表现出色，尤其在医学领域具有潜力，但它们在处理时间序列数据方面存在局限性。本研究旨在克服这一障碍。

Method: 研究提出了两种OpenTSLM架构：OpenTSLM-SoftPrompt（通过软提示将可学习的时间序列令牌与文本令牌连接）和OpenTSLM-Flamingo（通过交叉注意力将时间序列与文本集成）。两种模型都在文本-时间序列链式思考（CoT）推理任务上进行了基准测试，并引入了三个新数据集：HAR-CoT、Sleep-CoT和ECG-QA-CoT。

Result: OpenTSLM模型在所有任务中均优于基线模型，在睡眠分期和HAR任务中分别达到69.9%和65.4%的F1分数，显著优于微调后的纯文本模型。OpenTSLM-Flamingo在性能上与OpenTSLM-SoftPrompt相当，在处理长序列时表现更优，且内存需求稳定。OpenTSLM模型在ECG-QA任务上的专家评审也显示出强大的推理能力。

Conclusion: OpenTSLM成功地将时间序列作为原生模态整合到预训练的LLM中，实现了对任意长度多时间序列的推理能力，并在医学相关的文本-时间序列CoT任务中取得了显著成果，超越了现有方法和大型模型，并已开源以促进进一步研究。

Abstract: LLMs have emerged as powerful tools for interpreting multimodal data. In
medicine, they hold particular promise for synthesizing large volumes of
clinical information into actionable insights and digital health applications.
Yet, a major limitation remains their inability to handle time series. To
overcome this gap, we present OpenTSLM, a family of Time Series Language Models
(TSLMs) created by integrating time series as a native modality to pretrained
LLMs, enabling reasoning over multiple time series of any length. We
investigate two architectures for OpenTSLM. The first, OpenTSLM-SoftPrompt,
models time series implicitly by concatenating learnable time series tokens
with text tokens via soft prompting. Although parameter-efficient, we
hypothesize that explicit time series modeling scales better and outperforms
implicit approaches. We thus introduce OpenTSLM-Flamingo, which integrates time
series with text via cross-attention. We benchmark both variants against
baselines that treat time series as text tokens or plots, across a suite of
text-time-series Chain-of-Thought (CoT) reasoning tasks. We introduce three
datasets: HAR-CoT, Sleep-CoT, and ECG-QA-CoT. Across all, OpenTSLM models
outperform baselines, reaching 69.9 F1 in sleep staging and 65.4 in HAR,
compared to 9.05 and 52.2 for finetuned text-only models. Notably, even
1B-parameter OpenTSLM models surpass GPT-4o (15.47 and 2.95). OpenTSLM-Flamingo
matches OpenTSLM-SoftPrompt in performance and outperforms on longer sequences,
while maintaining stable memory requirements. By contrast, SoftPrompt grows
exponentially in memory with sequence length, requiring around 110 GB compared
to 40 GB VRAM when training on ECG-QA with LLaMA-3B. Expert reviews by
clinicians find strong reasoning capabilities exhibited by OpenTSLMs on ECG-QA.
To facilitate further research, we provide all code, datasets, and models
open-source.

</details>


### [271] [RainSeer: Fine-Grained Rainfall Reconstruction via Physics-Guided Modeling](https://arxiv.org/abs/2510.02414)
*Lin Chen,Jun Chen,Minghui Qiu,Shuxin Zhong,Binghong Chen,Kaishun Wu*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Reconstructing high-resolution rainfall fields is essential for flood
forecasting, hydrological modeling, and climate analysis. However, existing
spatial interpolation methods-whether based on automatic weather station (AWS)
measurements or enhanced with satellite/radar observations often over-smooth
critical structures, failing to capture sharp transitions and localized
extremes. We introduce RainSeer, a structure-aware reconstruction framework
that reinterprets radar reflectivity as a physically grounded structural
prior-capturing when, where, and how rain develops. This shift, however,
introduces two fundamental challenges: (i) translating high-resolution
volumetric radar fields into sparse point-wise rainfall observations, and (ii)
bridging the physical disconnect between aloft hydro-meteors and ground-level
precipitation. RainSeer addresses these through a physics-informed two-stage
architecture: a Structure-to-Point Mapper performs spatial alignment by
projecting mesoscale radar structures into localized ground-level rainfall,
through a bidirectional mapping, and a Geo-Aware Rain Decoder captures the
semantic transformation of hydro-meteors through descent, melting, and
evaporation via a causal spatiotemporal attention mechanism. We evaluate
RainSeer on two public datasets-RAIN-F (Korea, 2017-2019) and MeteoNet (France,
2016-2018)-and observe consistent improvements over state-of-the-art baselines,
reducing MAE by over 13.31% and significantly enhancing structural fidelity in
reconstructed rainfall fields.

</details>


### [272] [In-memory Training on Analog Devices with Limited Conductance States via Multi-tile Residual Learning](https://arxiv.org/abs/2510.02516)
*Jindan Li,Zhaoxian Wu,Gaowen Liu,Tayfun Gokmen,Tianyi Chen*

Main category: cs.LG

TL;DR: 该论文提出了一种利用残差学习的框架，用于在精度有限的忆阻器设备上实现片上训练，以弥补低精度权重更新造成的误差。


<details>
  <summary>Details</summary>
Motivation: 在模拟内存计算（AIMC）中，为了在内存中直接进行高效的深度神经网络计算，模型参数通常由忆阻器设备的电导状态表示。然而，有效的片上训练通常需要至少8位精度的电导状态才能达到与数字基线相匹配的精度。但受限于制造工艺，许多有前景的忆阻器设备（如ReRAM）仅提供约4位分辨率，这种有限的更新精度会严重影响训练精度。因此，如何在精度有限的设备上实现高效的片上训练是一个关键问题。

Method: 本文提出了一种残差学习框架，通过在多个交叉阵列（crossbar tiles）上顺序学习，来补偿低精度权重更新产生的残差误差。

Result: 理论分析表明，随着交叉阵列数量的增加，最优性差距会缩小，并能达到线性收敛率。在标准的图像分类基准测试上的实验表明，该方法在有限精度设置下，性能持续优于现有的模拟内存训练策略，并且硬件开销适中。

Conclusion: 提出的残差学习框架能够有效解决忆阻器设备精度有限带来的训练精度下降问题，为在低精度模拟内存计算硬件上实现高效的片上训练提供了可行方案。

Abstract: Analog in-memory computing (AIMC) accelerators enable efficient deep neural
network computation directly within memory using resistive crossbar arrays,
where model parameters are represented by the conductance states of memristive
devices. However, effective in-memory training typically requires at least
8-bit conductance states to match digital baselines. Realizing such
fine-grained states is costly and often requires complex noise mitigation
techniques that increase circuit complexity and energy consumption. In
practice, many promising memristive devices such as ReRAM offer only about
4-bit resolution due to fabrication constraints, and this limited update
precision substantially degrades training accuracy. To enable on-chip training
with these limited-state devices, this paper proposes a \emph{residual
learning} framework that sequentially learns on multiple crossbar tiles to
compensate the residual errors from low-precision weight updates. Our
theoretical analysis shows that the optimality gap shrinks with the number of
tiles and achieves a linear convergence rate. Experiments on standard image
classification benchmarks demonstrate that our method consistently outperforms
state-of-the-art in-memory analog training strategies under limited-state
settings, while incurring only moderate hardware overhead as confirmed by our
cost analysis.

</details>


### [273] [How to Train Your Advisor: Steering Black-Box LLMs with Advisor Models](https://arxiv.org/abs/2510.02453)
*Parth Asawa,Alan Zhu,Matei Zaharia,Alexandros G. Dimakis,Joseph E. Gonzalez*

Main category: cs.LG

TL;DR: Advisor Models 是轻量级策略，可以动态地为黑箱模型生成自然语言指令，以提高其在不同任务和环境下的性能。


<details>
  <summary>Details</summary>
Motivation: 现有针对黑箱基础模型的提示优化方法是静态的，无法适应不同输入、用户或环境。需要一种能够对黑箱模型进行动态、个性化和环境适应性优化的方法。

Method: Advisor Models 是一种轻量级策略模型，使用强化学习进行训练，能够根据环境信号动态地生成自然语言指令，对黑箱模型进行 in-context steering。该模型位于用户输入和黑箱模型之间，对每个实例进行行为塑造。

Result: Advisor Models 在涉及推理和个性化的多个领域中，优于静态提示优化器，能够发现环境动态并提高下游任务性能。此外，Advisor Models 还可以跨黑箱模型进行迁移，并能在实现专业化的同时保持对分布外输入的鲁棒性。

Conclusion: Advisor Models 提供了一个可学习的接口，用于与黑箱系统交互，充当参数化、特定环境的记忆。通过 Advisor Models 对黑箱模型进行动态优化，是实现具有前沿能力的可个性化和环境适应性 AI 的有前景的方向。

Abstract: Foundation models are increasingly deployed as black-box services, where
model weights cannot be modified and customization is limited to prompting.
While static prompt optimization has shown promise, it produces a single fixed
prompt that fails to adapt to different inputs, users, or environments. We
introduce Advisor Models, lightweight parametric policies trained with
reinforcement learning to reactively issue natural language steering
instructions in-context to black-box models. The advisor is a second small
model that sits between the input and the model, shaping behavior on a
per-instance basis using reward signals from the environment. Across multiple
domains involving reasoning and personalization, we show that Advisor Models
outperform static prompt optimizers, discovering environment dynamics and
improving downstream task performance. We also demonstrate the generalizability
of advisors by transferring them across black-box models, as well as the
framework's ability to achieve specialization while retaining robustness to
out-of-distribution inputs. Viewed more broadly, Advisor Models provide a
learnable interface to black-box systems where the advisor acts as a
parametric, environment-specific memory. We argue that dynamic optimization of
black-box models via Advisor Models is a promising direction for enabling
personalization and environment-adaptable AI with frontier-level capabilities.

</details>


### [274] [Assessing the Potential for Catastrophic Failure in Dynamic Post-Training Quantization](https://arxiv.org/abs/2510.02457)
*Logan Frank,Paul Ardis*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Post-training quantization (PTQ) has recently emerged as an effective tool
for reducing the computational complexity and memory usage of a neural network
by representing its weights and activations with lower precision. While this
paradigm has shown great success in lowering compute and storage costs, there
is the potential for drastic performance reduction depending upon the
distribution of inputs experienced in inference. When considering possible
deployment in safety-critical environments, it is important to investigate the
extent of potential performance reduction, and what characteristics of input
distributions may give rise to this reduction. In this work, we explore the
idea of extreme failure stemming from dynamic PTQ and formulate a knowledge
distillation and reinforcement learning task to learn a network and bit-width
policy pair such that catastrophic failure under quantization is analyzed in
terms of worst case potential. Our results confirm the existence of this
"detrimental" network-policy pair, with several instances demonstrating
performance reductions in the range of 10-65% in accuracy, compared to their
"robust" counterparts encountering a <2% decrease. From systematic
experimentation and analyses, we also provide an initial exploration into
points at highest vulnerability. While our results represent an initial step
toward understanding failure cases introduced by PTQ, our findings ultimately
emphasize the need for caution in real-world deployment scenarios. We hope this
work encourages more rigorous examinations of robustness and a greater emphasis
on safety considerations for future works within the broader field of deep
learning.

</details>


### [275] [SAGE: Streaming Agreement-Driven Gradient Sketches for Representative Subset Selection](https://arxiv.org/abs/2510.02470)
*Ashish Jha,Salman Ahmadi-Asl*

Main category: cs.LG

TL;DR: SAGE是一种流式数据子集选择方法，通过维护梯度几何的紧凑草图来选择样本，从而在训练现代神经网络时减少计算和能源消耗。


<details>
  <summary>Details</summary>
Motivation: 当前的深度学习模型训练需要大量的计算和能源，需要更有效的方法来处理大型数据集。

Method: SAGE使用流式数据子集选择方法，维护梯度几何的紧凑草图（Frequent Directions sketch），并优先选择与共识方向一致的样本。这种方法避免了昂贵的成对相似度计算和显式的梯度存储，从而实现了一个简单的两阶段、GPU友好的流水线。

Result: SAGE在多个基准测试中，以较低的样本保留率实现了与完整数据训练和现有方法相当的准确性，同时减少了端到端的计算量和峰值内存占用。

Conclusion: SAGE提供了一种实用的、内存占用恒定的替代方案，可以与剪枝和模型压缩等技术结合，实现高效的训练。

Abstract: Training modern neural networks on large datasets is computationally and
energy intensive. We present SAGE, a streaming data-subset selection method
that maintains a compact Frequent Directions (FD) sketch of gradient geometry
in $O(\ell D)$ memory and prioritizes examples whose sketched gradients align
with a consensus direction. The approach eliminates $N \times N$ pairwise
similarities and explicit $N \times \ell$ gradient stores, yielding a simple
two-pass, GPU-friendly pipeline. Leveraging FD's deterministic approximation
guarantees, we analyze how agreement scoring preserves gradient energy within
the principal sketched subspace. Across multiple benchmarks, SAGE trains with
small kept-rate budgets while retaining competitive accuracy relative to
full-data training and recent subset-selection baselines, and reduces
end-to-end compute and peak memory. Overall, SAGE offers a practical,
constant-memory alternative that complements pruning and model compression for
efficient training.

</details>


### [276] [Uncertainty-Guided Model Selection for Tabular Foundation Models in Biomolecule Efficacy Prediction](https://arxiv.org/abs/2510.02476)
*Jie Li,Andrew McCarthy,Zhizhuo Zhang,Stephen Young*

Main category: cs.LG

TL;DR: TabPFN在生物分子功效预测中表现出潜力，但其性能易受上下文影响。本研究提出一种不依赖标签的、基于模型不确定性（IQR）的策略来选择模型进行集成，以提高预测性能。


<details>
  <summary>Details</summary>
Motivation: 在生物分子功效预测任务中，如何选择最佳模型进行集成以提升性能是一个关键问题，尤其是在缺乏真实标签的情况下。

Method: 本研究提出并验证了一种基于模型不确定性（具体为预测的四分位距 IQR）的集成策略。通过选择和平均 IQR 最小的模型，来优化预测结果。

Result: 研究表明，所提出的不确定性引导的模型选择策略能够获得优于简单集成或单一模型训练的性能。并且，模型的 IQR 与实际预测误差呈负相关。

Conclusion: 模型不确定性（IQR）可以作为一种有效的、无需标签的启发式方法，用于优化生物分子功效预测的集成模型选择。

Abstract: In-context learners like TabPFN are promising for biomolecule efficacy
prediction, where established molecular feature sets and relevant experimental
results can serve as powerful contextual examples. However, their performance
is highly sensitive to the provided context, making strategies like post-hoc
ensembling of models trained on different data subsets a viable approach. An
open question is how to select the best models for the ensemble without access
to ground truth labels. In this study, we investigate an uncertainty-guided
strategy for model selection. We demonstrate on an siRNA knockdown efficacy
task that a TabPFN model using simple sequence-based features can surpass
specialized state-of-the-art predictors. We also show that the model's
predicted inter-quantile range (IQR), a measure of its uncertainty, has a
negative correlation with true prediction error. By selecting and averaging an
ensemble of models with the lowest mean IQR, we achieve superior performance
compared to naive ensembling or using a single model trained on all available
data. This finding highlights model uncertainty as a powerful, label-free
heuristic for optimizing biomolecule efficacy predictions.

</details>


### [277] [Litespark Technical Report: High-Throughput, Energy-Efficient LLM Training Framework](https://arxiv.org/abs/2510.02483)
*Nii Osae Osae Dade,Moinul Hossain Rahat*

Main category: cs.LG

TL;DR: Litespark框架通过优化Transformer的注意力和MLP层，实现了2倍-6倍的训练吞吐量提升和55%-83%的能耗降低，同时保持了与标准Transformer的兼容性。


<details>
  <summary>Details</summary>
Motivation: 训练大型语言模型（LLMs）面临训练时间长和能源消耗巨大的挑战。

Method: Litespark框架结合了架构改进和算法增强，以最大化模型FLOPs利用率（MFU），同时保持与标准Transformer实现的兼容性。

Result: 在3B和30B参数的Llama模型上使用SlimPajama-627B数据集进行基准测试，Litespark框架实现了2倍-6倍的训练吞吐量提升和55%-83%的能耗降低。

Conclusion: Litespark框架的优化是模型和硬件无关的，具有广泛的适用性，可应用于Transformer架构和训练后阶段（包括监督微调和直接偏好优化）。

Abstract: Training Large Language Models (LLMs) is plagued by long training times and
massive energy consumption, with modern models requiring months of computation
and gigawatt-hours of electricity. In light of these challenges,we introduce
Litespark, a novel pre-training framework that addresses these inefficiencies
through targeted optimizations to transformer attention and MLP layers. Our
approach combines architectural improvements with algorithmic enhancements to
maximize Model FLOPs Utilization (MFU) while maintaining compatibility with
standard transformer implementations. Comprehensive benchmarking on 3B and 30B
parameter Llama models using the SlimPajama-627B dataset demonstrates
substantial performance gains: 2x-6x training throughput improvement and
$55\%-83$% energy consumption reduction across multi-node H200 GPU clusters.
These optimizations are model- and hardware-agnostic, enabling broad
applicability across transformer architectures and extending to post-training
phases including supervised fine-tuning and direct preference optimization.

</details>


### [278] [Online Learning in the Random Order Model](https://arxiv.org/abs/2510.02820)
*Martino Bernasconi,Andrea Celli,Riccardo Colini-Baldeschi,Federico Fusco,Stefano Leonardi,Matteo Russo*

Main category: cs.LG

TL;DR: 该论文提出了一种将随机学习算法适应随机排序模型（random-order model）的通用模板，以在不显著影响其遗憾边界（regret guarantees）的情况下，改进在延迟预测、带约束在线学习和带切换成本的迁移学习等问题上的性能。此外，论文还研究了在线分类问题，并证明在随机排序模型下，可学习性由VC维（VC dimension）决定，而非Littlestone维（Littlestone dimension），这进一步将随机排序模型与一般对抗模型区分开来。


<details>
  <summary>Details</summary>
Motivation: 随机排序模型（random-order model）虽然在渐近意义上等同于独立同分布（i.i.d.）模型，但在有限的情况下可能表现出显著的非平稳性（non-stationarity），从而影响随机学习算法的性能。虽然对抗性输入算法在随机排序模型下能保持其遗憾边界，但一些简单的无遗憾算法（no-regret algorithms）在随机排序实例上会失效。

Method: 提出了一种通用模板，用于将随机学习算法改编到随机排序模型，目标是在不显著影响其遗憾边界的前提下，提高算法性能。

Result: 该方法成功应用于延迟预测、带约束在线学习和带切换成本的迁移学习问题，并改进了相应的遗憾边界。在在线分类方面，证明了在随机排序模型下，可学习性由VC维决定，而非Littlestone维。

Conclusion: 提出的通用模板能够有效地将随机学习算法适配到随机排序模型，并在多个应用场景下取得性能提升。研究结果还揭示了随机排序模型下在线分类的可学习性特征，并将其与一般对抗模型区分开。

Abstract: In the random-order model for online learning,
  the sequence of losses is chosen upfront by an adversary and presented to the
learner
  after a random permutation. Any random-order input is \emph{asymptotically}
equivalent to a stochastic i.i.d. one, but, for finite times, it may exhibit
significant {\em non-stationarity}, which can hinder the performance of
stochastic learning algorithms.
  While algorithms for adversarial inputs naturally maintain their regret
guarantees in random order, simple no-regret algorithms exist for the
stochastic model that fail against random-order instances.
  In this paper, we propose a general template to adapt stochastic learning
algorithms to the random-order model without substantially affecting their
regret guarantees. This allows us to recover improved regret bounds for
prediction with delays, online learning with constraints, and bandits with
switching costs. Finally, we investigate online classification and prove that,
in random order, learnability is characterized by the VC dimension rather than
the Littlestone dimension, thus providing a further separation from the general
adversarial model.

</details>


### [279] [From Pixels to Factors: Learning Independently Controllable State Variables for Reinforcement Learning](https://arxiv.org/abs/2510.02484)
*Rafael Rodriguez-Sanchez,Cameron Allen,George Konidaris*

Main category: cs.LG

TL;DR: ACF是一种利用对比学习的方法，可以从像素观测中发现可独立控制的潜在变量，解决了深度强化学习在处理高维输入时无法利用其结构化的问题。


<details>
  <summary>Details</summary>
Motivation: 在处理高维输入时，深度强化学习无法利用其结构化信息，而利用马尔可夫决策过程的算法需要先验知识，这在实践中难以满足。ACF旨在解决这个问题。

Method: ACF使用对比学习方法，利用动作仅影响部分变量的稀疏性，来发现可独立控制的潜在变量。

Result: ACF在Taxi、FourRooms和MiniGrid-DoorKey三个基准测试中，直接从像素观测中恢复了真实世界中可控因素，并且其表现优于基线分解算法。

Conclusion: ACF是一种有效的从像素观测中发现可控因素的方法，能够克服现有方法的局限性。

Abstract: Algorithms that exploit factored Markov decision processes are far more
sample-efficient than factor-agnostic methods, yet they assume a factored
representation is known a priori -- a requirement that breaks down when the
agent sees only high-dimensional observations. Conversely, deep reinforcement
learning handles such inputs but cannot benefit from factored structure. We
address this representation problem with Action-Controllable Factorization
(ACF), a contrastive learning approach that uncovers independently controllable
latent variables -- state components each action can influence separately. ACF
leverages sparsity: actions typically affect only a subset of variables, while
the rest evolve under the environment's dynamics, yielding informative data for
contrastive training. ACF recovers the ground truth controllable factors
directly from pixel observations on three benchmarks with known factored
structure -- Taxi, FourRooms, and MiniGrid-DoorKey -- consistently
outperforming baseline disentanglement algorithms.

</details>


### [280] [Improved Robustness of Deep Reinforcement Learning for Control of Time-Varying Systems by Bounded Extremum Seeking](https://arxiv.org/abs/2510.02490)
*Shaifalee Saxena,Alan Williams,Rafael Fierro,Alexander Scheinker*

Main category: cs.LG

TL;DR: 提出了一种结合深度强化学习（DRL）和有界极值搜索（ES）的混合控制器，以提高DRL在非线性时变系统中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习（DRL）在控制多参数系统方面具有潜力，但其性能在系统模型快速变化时会严重下降。有界极值搜索（ES）虽然可以处理时变系统，但参数增多时收敛速度会变慢，且可能陷入局部最优。

Method: 提出了一种结合DRL和有界ES的混合控制器。DRL利用历史数据快速控制系统达到设定点，有界ES则保证了控制器对时变性的鲁棒性。

Result: 通过对一般时变系统和洛斯阿拉莫斯国家实验室的低能束传输段的数值研究，证明了ES-DRL控制器在自动调优方面的性能。

Conclusion: 结合DRL和有界ES的混合控制器可以克服各自的局限性，实现优于单一方法的性能，特别是在处理非线性时变系统方面。

Abstract: In this paper, we study the use of robust model independent bounded extremum
seeking (ES) feedback control to improve the robustness of deep reinforcement
learning (DRL) controllers for a class of nonlinear time-varying systems. DRL
has the potential to learn from large datasets to quickly control or optimize
the outputs of many-parameter systems, but its performance degrades
catastrophically when the system model changes rapidly over time. Bounded ES
can handle time-varying systems with unknown control directions, but its
convergence speed slows down as the number of tuned parameters increases and,
like all local adaptive methods, it can get stuck in local minima. We
demonstrate that together, DRL and bounded ES result in a hybrid controller
whose performance exceeds the sum of its parts with DRL taking advantage of
historical data to learn how to quickly control a many-parameter system to a
desired setpoint while bounded ES ensures its robustness to time variations. We
present a numerical study of a general time-varying system and a combined
ES-DRL controller for automatic tuning of the Low Energy Beam Transport section
at the Los Alamos Neutron Science Center linear particle accelerator.

</details>


### [281] [Taming Imperfect Process Verifiers: A Sampling Perspective on Backtracking](https://arxiv.org/abs/2510.03149)
*Dhruv Rohatgi,Abhishek Shetty,Donya Saless,Yuchen Li,Ankur Moitra,Andrej Risteski,Dylan J. Foster*

Main category: cs.LG

TL;DR: 通过结合语言模型的生成能力和评估部分生成质量的过程验证器，测试时算法有望激发新的推理能力。然而，这类方法的算法设计空间和计算扩展性仍不明朗，且在学习高质量验证器的成本下，其优势并不明显。本研究提出了一种名为 VGB 的新过程引导测试时采样算法，通过理论保证的回溯机制，提高了对验证器错误的鲁棒性，并在合成和真实语言建模任务上取得了优于基线方法的表现。


<details>
  <summary>Details</summary>
Motivation: 学习到的验证器中看似良性的错误，在生成过程中可能会因错误放大而导致标准解码技术的灾难性失败。本研究旨在探索更复杂的解码策略是否能改善这种情况。

Method: 提出了一种名为 VGB 的新过程引导测试时采样算法。该算法将自回归生成视为部分生成树上的随机游走，其转移概率由过程验证器和基础模型指导，并引入了概率性回溯机制。该方法借鉴了理论计算机科学中用于近似计数和采样的 Sinclair-Jerrum 随机游走模型。

Result: 在合成和真实语言建模任务的实验中，VGB 在多种指标上均优于现有基线方法。

Conclusion: VGB 算法通过理论保证的回溯机制，能够更鲁棒地处理验证器错误，并在实际应用中展现出优于基线方法的性能。

Abstract: Test-time algorithms that combine the generative power of language models
with process verifiers that assess the quality of partial generations offer a
promising lever for eliciting new reasoning capabilities, but the algorithmic
design space and computational scaling properties of such approaches are still
opaque, and their benefits are far from apparent when one accounts for the cost
of learning a high-quality verifier. Our starting point is the observation that
seemingly benign errors in a learned verifier can lead to catastrophic failures
for standard decoding techniques due to error amplification during the course
of generation. We then ask: can this be improved with more sophisticated
decoding strategies?
  We introduce a new process-guided test-time sampling algorithm, VGB, which
uses theoretically grounded backtracking to achieve provably better robustness
to verifier errors. VGB interprets autoregressive generation as a random walk
on a tree of partial generations, with transition probabilities guided by the
process verifier and base model; crucially, backtracking occurs
probabilistically. This process generalizes the seminal Sinclair-Jerrum random
walk (Sinclair & Jerrum, 1989) from the literature on approximate counting and
sampling in theoretical computer science, and a conceptual contribution of our
work is to highlight parallels with this literature. Empirically, we
demonstrate on both synthetic and real language modeling tasks that VGB
outperforms baselines on a variety of metrics.

</details>


### [282] [Beyond Imitation: Recovering Dense Rewards from Demonstrations](https://arxiv.org/abs/2510.02493)
*Jiangnan Li,Thuy-Trang Vu,Ehsan Abbasnejad,Gholamreza Haffari*

Main category: cs.LG

TL;DR: SFT 不仅仅是模仿学习，它还学习了一个隐式的、密集的、token级别的奖励模型。这个奖励模型可以用来改进策略，Dense-Path REINFORCE 方法在指令遵循基准测试中优于 SFT 模型。


<details>
  <summary>Details</summary>
Motivation: 挑战将SFT视为简单模仿学习的传统观点，并揭示SFT的本质是逆强化学习的一个特例。

Method: 证明SFT目标是逆Q学习的一个特例，并提出一种从SFT模型中恢复密集奖励信号的方法，将其用于策略改进。

Result: 所提出的Dense-Path REINFORCE方法在指令遵循基准测试中，其表现持续优于原始SFT模型。

Conclusion: SFT可以被重新定义为一种强大的奖励学习机制，能够从专家演示中提取有价值的奖励信号，并为后续的策略优化提供基础。

Abstract: Conventionally, supervised fine-tuning (SFT) is treated as a simple imitation
learning process that only trains a policy to imitate expert behavior on
demonstration datasets. In this work, we challenge this view by establishing a
fundamental equivalence between SFT and Inverse Reinforcement Learning. We
prove that the SFT objective is a special case of Inverse Q-Learning, which
implies that the SFT process does not just learn a policy, but also an
implicit, dense, token-level reward model that explains the expert
demonstrations. We then show how to recover this dense reward signal directly
from the SFT model by formulating a baseline-relative reward function. The
availability of such a dense reward model offers numerous benefits, providing
granular credit assignment for each token generated. We demonstrate one key
application by using these recovered rewards to further improve the policy with
reinforcement learning. Our method, Dense-Path REINFORCE, consistently
outperforms the original SFT models on instruction-following benchmarks. This
work reframes SFT not merely as policy imitation but as a powerful reward
learning mechanism, opening new possibilities for leveraging expert
demonstrations.

</details>


### [283] [Graph Generation with Spectral Geodesic Flow Matching](https://arxiv.org/abs/2510.02520)
*Xikun Huang,Tianyu Ruan,Chihao Zhang,Shihua Zhang*

Main category: cs.LG

TL;DR: SFMG是一个新的图生成框架，它使用谱特征图将图嵌入黎曼流形，并通过匹配黎曼流形上的测地流来生成图，在保持图结构的同时提高了生成效率和多样性。


<details>
  <summary>Details</summary>
Motivation: 现有的图生成方法在匹配图的光谱或度分布时，往往忽略了特征向量引起的几何结构和图的整体结构。本研究旨在提出一种能够捕捉图的几何结构并实现灵活、高效图生成的新方法。

Method: 提出谱测地流匹配（SFMG）框架，使用谱特征图将输入图和目标图嵌入到连续的黎曼流形中，然后定义嵌入之间的测地流，并沿着这些流匹配分布以生成输出图。

Result: SFMG在图结构、度分布和谱度量方面均能达到最先进的性能，并且在可扩展性和训练效率方面比基于扩散的模型快30倍，还能泛化到未知的图尺度。

Conclusion: SFMG通过整合谱几何和流匹配，为图合成提供了一种新颖的方法，在生成质量和效率上均表现出色。

Abstract: Graph generation is a fundamental task with wide applications in modeling
complex systems. Although existing methods align the spectrum or degree profile
of the target graph, they often ignore the geometry induced by eigenvectors and
the global structure of the graph. In this work, we propose Spectral Geodesic
Flow Matching (SFMG), a novel framework that uses spectral eigenmaps to embed
both input and target graphs into continuous Riemannian manifolds. We then
define geodesic flows between embeddings and match distributions along these
flows to generate output graphs. Our method yields several advantages: (i)
captures geometric structure beyond eigenvalues, (ii) supports flexible
generation of diverse graphs, and (iii) scales efficiently. Empirically, SFMG
matches the performance of state-of-the-art approaches on graphlet, degree, and
spectral metrics across diverse benchmarks. In particular, it achieves up to
30$\times$ speedup over diffusion-based models, offering a substantial
advantage in scalability and training efficiency. We also demonstrate its
ability to generalize to unseen graph scales. Overall, SFMG provides a new
approach to graph synthesis by integrating spectral geometry with flow
matching.

</details>


### [284] [Model-brain comparison using inter-animal transforms](https://arxiv.org/abs/2510.02523)
*Imran Thobani,Javier Sagastuy-Brena,Aran Nayebi,Jacob Prince,Rosa Cao,Daniel Yamins*

Main category: cs.LG

TL;DR: 本研究提出了一种新的基于跨动物类别变换（IATC）的比较方法，用于比较人工神经网络模型和大脑活动，该方法能准确预测神经活动并识别神经机制，并发现拓扑深度神经网络（TDANNs）是视觉系统的有效模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法在比较模型激活与大脑响应方面缺乏共识，本研究旨在提出一种新的比较方法。

Method: 提出并应用基于跨动物类别变换（IATC）的比较方法，将模型响应与大脑数据进行双向映射，评估模型在多大程度上能够模仿典型被试。在模拟神经网络、小鼠和人类被试的三种场景下识别IATC。

Result: IATC能够解决神经机制的细节问题，例如非线性激活函数；能够准确预测神经活动，并高特异性地识别机制，区分不同大脑区域的响应模式，并高度匹配同一大脑区域被试间的响应；为TDANNs作为视觉系统模型提供了新的证据。

Conclusion: IATC方法实现了模型-大脑预测能力和神经机制识别的精确度之间的平衡，为理解和比较模型与大脑提供了原则性的框架，并改进了以往的模型-大脑比较方法。

Abstract: Artificial neural network models have emerged as promising mechanistic models
of the brain. However, there is little consensus on the correct method for
comparing model activations to brain responses. Drawing on recent work in
philosophy of neuroscience, we propose a comparison methodology based on the
Inter-Animal Transform Class (IATC) - the strictest set of functions needed to
accurately map neural responses between subjects in an animal population. Using
the IATC, we can map bidirectionally between a candidate model's responses and
brain data, assessing how well the model can masquerade as a typical subject
using the same kinds of transforms needed to map across real subjects. We
identify the IATC in three settings: a simulated population of neural network
models, a population of mouse subjects, and a population of human subjects. We
find that the IATC resolves detailed aspects of the neural mechanism, such as
the non-linear activation function. Most importantly, we find that the IATC
enables accurate predictions of neural activity while also achieving high
specificity in mechanism identification, evidenced by its ability to separate
response patterns from different brain areas while strongly aligning
same-brain-area responses between subjects. In other words, the IATC is a
proof-by-existence that there is no inherent tradeoff between the neural
engineering goal of high model-brain predictivity and the neuroscientific goal
of identifying mechanistically accurate brain models. Using IATC-guided
transforms, we obtain new evidence in favor of topographical deep neural
networks (TDANNs) as models of the visual system. Overall, the IATC enables
principled model-brain comparisons, contextualizing previous findings about the
predictive success of deep learning models of the brain, while improving upon
previous approaches to model-brain comparison.

</details>


### [285] [AttentiveGRUAE: An Attention-Based GRU Autoencoder for Temporal Clustering and Behavioral Characterization of Depression from Wearable Data](https://arxiv.org/abs/2510.02558)
*Nidhi Soley,Vishal M Patel,Casey O Taylor*

Main category: cs.LG

TL;DR: AttentiveGRUAE是一种基于注意力机制的GRU自编码器，用于时间聚类和预测，在睡眠数据上表现优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 提出一种新的基于注意力机制的GRU自编码器（AttentiveGRUAE），用于处理纵向可穿戴设备数据，实现时间聚类和结果预测。

Method: 该模型联合优化三个目标：1. 通过序列重建学习日常行为特征的紧凑潜在表征；2. 通过二元分类头预测周期结束时的抑郁率；3. 通过基于高斯混合模型（GMM）的软聚类识别行为亚型。

Result: 在372名参与者的纵向睡眠数据（GLOBEM 2018-2019）上，AttentiveGRUAE在聚类质量（轮廓系数=0.70）和抑郁分类（AUC=0.74）方面均优于基线模型。在332名参与者的跨年队列（GLOBEM 2020-2021）上的外部验证也证实了其聚类可重复性和稳定性（轮廓系数=0.63，AUC=0.61）。

Conclusion: AttentiveGRUAE在纵向睡眠数据分析中表现出优越的性能，能够有效进行时间聚类和抑郁预测，并且通过注意力可视化提供了临床可解释的见解。

Abstract: In this study, we present AttentiveGRUAE, a novel attention-based gated
recurrent unit (GRU) autoencoder designed for temporal clustering and
prediction of outcome from longitudinal wearable data. Our model jointly
optimizes three objectives: (1) learning a compact latent representation of
daily behavioral features via sequence reconstruction, (2) predicting
end-of-period depression rate through a binary classification head, and (3)
identifying behavioral subtypes through Gaussian Mixture Model (GMM) based soft
clustering of learned embeddings. We evaluate AttentiveGRUAE on longitudinal
sleep data from 372 participants (GLOBEM 2018-2019), and it demonstrates
superior performance over baseline clustering, domain-aligned self-supervised,
and ablated models in both clustering quality (silhouette score = 0.70 vs
0.32-0.70) and depression classification (AUC = 0.74 vs 0.50-0.67).
Additionally, external validation on cross-year cohorts from 332 participants
(GLOBEM 2020-2021) confirms cluster reproducibility (silhouette score = 0.63,
AUC = 0.61) and stability. We further perform subtype analysis and visualize
temporal attention, which highlights sleep-related differences between clusters
and identifies salient time windows that align with changes in sleep
regularity, yielding clinically interpretable explanations of risk.

</details>


### [286] [On The Expressive Power of GNN Derivatives](https://arxiv.org/abs/2510.02565)
*Yam Eitan,Moshe Eliasof,Yoav Gelberg,Fabrizio Frasca,Guy Bar-Shalom,Haggai Maron*

Main category: cs.LG

TL;DR: GNNs在图表示学习中虽然取得了显著进展，但其表达能力有限。本文提出了一种名为高阶导数GNN（HOD-GNN）的新方法，通过利用MPNNs的高阶导数来增强GNN的表达能力，并将这些导数生成的结构感知节点嵌入输入到第二个GNN中进行端到端训练。理论上，HOD-GNN的表达能力与WL层级对齐，并与子图GNNs和结构编码方案建立了联系。此外，还提出了一种高效的计算高阶导数的消息传递算法。实验结果表明，HOD-GNN在图学习基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 尽管GNN在图学习中取得进展，但其表达能力有限，这是一个基本挑战。现有的研究主要集中在设计更具表达力的GNN架构，而忽略了GNN导数在增强其表达能力方面的潜力。

Method: 提出了一种名为高阶导数GNN（HOD-GNN）的新方法，该方法利用MPNNs（消息传递神经网络）的高阶导数来增强其表达能力。具体来说，通过计算节点特征的高阶导数来生成结构感知节点嵌入，然后将这些嵌入输入到第二个GNN中进行端到端的处理。

Result: HOD-GNN的表达能力与WL层级对齐，并与子图GNNs和结构编码方案建立了深刻的联系。此外，还开发了一种用于计算MPNNs高阶导数的消息传递算法，该算法利用了图的稀疏性和并行性。在图学习基准测试上的评估显示，HOD-GNN在流行的图学习任务上表现强劲。

Conclusion: HOD-GNN通过利用GNN导数，能够有效增强GNN的表达能力，并达到与WL层级相匹配的理论表达能力。该方法在实际应用中也展现出良好的性能，并提供了高效的计算方案。

Abstract: Despite significant advances in Graph Neural Networks (GNNs), their limited
expressivity remains a fundamental challenge. Research on GNN expressivity has
produced many expressive architectures, leading to architecture hierarchies
with models of increasing expressive power. Separately, derivatives of GNNs
with respect to node features have been widely studied in the context of the
oversquashing and over-smoothing phenomena, GNN explainability, and more. To
date, these derivatives remain unexplored as a means to enhance GNN
expressivity. In this paper, we show that these derivatives provide a natural
way to enhance the expressivity of GNNs. We introduce High-Order Derivative GNN
(HOD-GNN), a novel method that enhances the expressivity of Message Passing
Neural Networks (MPNNs) by leveraging high-order node derivatives of the base
model. These derivatives generate expressive structure-aware node embeddings
processed by a second GNN in an end-to-end trainable architecture.
Theoretically, we show that the resulting architecture family's expressive
power aligns with the WL hierarchy. We also draw deep connections between
HOD-GNN, Subgraph GNNs, and popular structural encoding schemes. For
computational efficiency, we develop a message-passing algorithm for computing
high-order derivatives of MPNNs that exploits graph sparsity and parallelism.
Evaluations on popular graph learning benchmarks demonstrate HOD-GNN's strong
performance on popular graph learning tasks.

</details>


### [287] [Geospatial Machine Learning Libraries](https://arxiv.org/abs/2510.02572)
*Adam J. Stewart,Caleb Robinson,Arindam Banerjee*

Main category: cs.LG

TL;DR: GeoML 库的概述、分析和实践指南，重点介绍了现有工具、方法和未来方向。


<details>
  <summary>Details</summary>
Motivation: 机器学习在地理空间领域的应用（GeoML）面临独特的数据挑战，但缺乏专门的软件库来支持，这阻碍了其发展。

Method: 对现有的 GeoML 库（如 TorchGeo, eo-learn, Raster Vision）进行全面的概述、分析和比较，介绍数据预处理、空间-时间连接、基准测试和预训练模型等常用方法，并通过案例研究展示其实际应用，最后讨论软件设计、许可、测试的最佳实践以及未来的挑战和方向。

Result: 介绍了 GeoML 库的演变、核心功能和生态系统，分析了流行的 GeoML 库的架构和支持的数据类型，讨论了常见的数据处理和模型训练方法，并通过案例研究展示了这些工具的实际应用，最后提出了软件工程的最佳实践和对未来发展的展望。

Conclusion: GeoML 库的发展对该领域至关重要，本文旨在为 GeoML 的实践者、开发者和研究人员提供一个全面的指南，帮助他们了解和贡献于这个快速发展的领域，并强调了基础模型和开源治理的重要性。

Abstract: Recent advances in machine learning have been supported by the emergence of
domain-specific software libraries, enabling streamlined workflows and
increased reproducibility. For geospatial machine learning (GeoML), the
availability of Earth observation data has outpaced the development of domain
libraries to handle its unique challenges, such as varying spatial resolutions,
spectral properties, temporal cadence, data coverage, coordinate systems, and
file formats. This chapter presents a comprehensive overview of GeoML
libraries, analyzing their evolution, core functionalities, and the current
ecosystem. It also introduces popular GeoML libraries such as TorchGeo,
eo-learn, and Raster Vision, detailing their architecture, supported data
types, and integration with ML frameworks. Additionally, it discusses common
methodologies for data preprocessing, spatial--temporal joins, benchmarking,
and the use of pretrained models. Through a case study in crop type mapping, it
demonstrates practical applications of these tools. Best practices in software
design, licensing, and testing are highlighted, along with open challenges and
future directions, particularly the rise of foundation models and the need for
governance in open-source geospatial software. Our aim is to guide
practitioners, developers, and researchers in navigating and contributing to
the rapidly evolving GeoML landscape.

</details>


### [288] [Use the Online Network If You Can: Towards Fast and Stable Reinforcement Learning](https://arxiv.org/abs/2510.02590)
*Ahmed Hendawy,Henrik Metternich,Théo Vincent,Mahdi Kallel,Jan Peters,Carlo D'Eramo*

Main category: cs.LG

TL;DR: MINTO通过在目标网络和在线网络之间取最小值来更新目标，从而在保持学习稳定性的同时加速学习。


<details>
  <summary>Details</summary>
Motivation: 现有的目标网络方法在稳定性和学习速度之间存在权衡，而直接使用在线网络进行引导则可能导致学习不稳定。MINTO旨在结合两者的优点。

Method: 提出了一种新的更新规则，即MINTO，它计算目标网络和在线网络之间的最小估计值。

Result: MINTO能够更快、更稳定地进行价值函数学习，并能减轻潜在的过高估计偏差。该方法可以轻松集成到各种基于价值和Actor-Critic算法中，且成本可忽略不计。

Conclusion: MINTO在广泛的基准测试中（包括在线和离线强化学习、离散和连续动作空间）都取得了性能提升，证明了其广泛的适用性和有效性。

Abstract: The use of target networks is a popular approach for estimating value
functions in deep Reinforcement Learning (RL). While effective, the target
network remains a compromise solution that preserves stability at the cost of
slowly moving targets, thus delaying learning. Conversely, using the online
network as a bootstrapped target is intuitively appealing, albeit well-known to
lead to unstable learning. In this work, we aim to obtain the best out of both
worlds by introducing a novel update rule that computes the target using the
MINimum estimate between the Target and Online network, giving rise to our
method, MINTO. Through this simple, yet effective modification, we show that
MINTO enables faster and stable value function learning, by mitigating the
potential overestimation bias of using the online network for bootstrapping.
Notably, MINTO can be seamlessly integrated into a wide range of value-based
and actor-critic algorithms with a negligible cost. We evaluate MINTO
extensively across diverse benchmarks, spanning online and offline RL, as well
as discrete and continuous action spaces. Across all benchmarks, MINTO
consistently improves performance, demonstrating its broad applicability and
effectiveness.

</details>


### [289] [Towards CONUS-Wide ML-Augmented Conceptually-Interpretable Modeling of Catchment-Scale Precipitation-Storage-Runoff Dynamics](https://arxiv.org/abs/2510.02605)
*Yuan-Heng Wang,Yang Yang,Fabio Ciulla,Hoshin V. Gupta,Charuleka Varadharajan*

Main category: cs.LG

TL;DR: 现代水文模型研究虽然多，但并未显著提升基于物理概念的理解。本研究提出了一种结合机器学习和物理可解释性的集水区尺度模型（MCP），并在美国大陆范围内进行了大样本研究，考虑了多样的水文-地质-气候条件。


<details>
  <summary>Details</summary>
Motivation: 许多现代水文建模研究虽然投入大量精力，但并未在增强物理概念理解方面取得相应进展。

Method: 使用基于质量守恒感知器（MCP）的、不同复杂度的、机器学习增强的物理可解释的集水区尺度模型，并结合属性掩码（如降雪、森林覆盖、气候区）进行评估。

Result: 结果表明，根据水文模型中过程主导性的变化，选择具有适当复杂度的模型结构至关重要。与基于长短期记忆网络（LSTM）的纯数据模型相比，基于MCP的物理可解释模型在性能上可以与之匹敌。

Conclusion: 这项研究强调了理论指导、物理基础扎实的大样本水文研究的潜力，特别是在提升力学理解和开发简洁、可解释的模型结构方面，为未来能够编码时空变化的‘处处模型’奠定了基础。

Abstract: While many modern studies are dedicated to ML-based large-sample hydrologic
modeling, these efforts have not necessarily translated into predictive
improvements that are grounded in enhanced physical-conceptual understanding.
Here, we report on a CONUS-wide large-sample study (spanning diverse
hydro-geo-climatic conditions) using ML-augmented physically-interpretable
catchment-scale models of varying complexity based in the Mass-Conserving
Perceptron (MCP). Results were evaluated using attribute masks such as snow
regime, forest cover, and climate zone. Our results indicate the importance of
selecting model architectures of appropriate model complexity based on how
process dominance varies with hydrological regime. Benchmark comparisons show
that physically-interpretable mass-conserving MCP-based models can achieve
performance comparable to data-based models based in the Long Short-Term Memory
network (LSTM) architecture. Overall, this study highlights the potential of a
theory-informed, physically grounded approach to large-sample hydrology, with
emphasis on mechanistic understanding and the development of parsimonious and
interpretable model architectures, thereby laying the foundation for future
models of everywhere that architecturally encode information about spatially-
and temporally-varying process dominance.

</details>


### [290] [MINERVA: Mutual Information Neural Estimation for Supervised Feature Selection](https://arxiv.org/abs/2510.02610)
*Taurai Muvunzaa,Egor Kraev,Pere Planell-Morell,Alexander Y. Shestopaloff*

Main category: cs.LG

TL;DR: MINERVA是一种基于神经网络估计互信息进行监督特征选择的新方法，能够捕捉高阶特征交互，并在合成和现实欺诈数据集上表现出有效性。


<details>
  <summary>Details</summary>
Motivation: 现有特征选择方法依赖于统计配对依赖性度量，可能无法处理目标依赖于高阶特征交互而非个体贡献的情况。

Method: MINERVA使用神经网络参数化互信息估计，并通过包含稀疏诱导正则化器的损失函数进行特征选择，采用两阶段过程解耦表示学习和特征选择。

Result: 实验结果表明，MINERVA能够有效捕捉复杂特征-目标关系，并在合成和现实欺诈数据集上展示了其有效性和精确求解能力。

Conclusion: MINERVA是一种有效且准确的特征选择方法，特别擅长处理复杂的高阶特征交互。

Abstract: Existing feature filters rely on statistical pair-wise dependence metrics to
model feature-target relationships, but this approach may fail when the target
depends on higher-order feature interactions rather than individual
contributions. We introduce Mutual Information Neural Estimation Regularized
Vetting Algorithm (MINERVA), a novel approach to supervised feature selection
based on neural estimation of mutual information between features and targets.
We paramaterize the approximation of mutual information with neural networks
and perform feature selection using a carefully designed loss function
augmented with sparsity-inducing regularizers. Our method is implemented in a
two-stage process to decouple representation learning from feature selection,
ensuring better generalization and a more accurate expression of feature
importance. We present examples of ubiquitous dependency structures that are
rarely captured in literature and show that our proposed method effectively
captures these complex feature-target relationships by evaluating feature
subsets as an ensemble. Experimental results on synthetic and real-life fraud
datasets demonstrate the efficacy of our method and its ability to perform
exact solutions.

</details>


### [291] [TabImpute: Accurate and Fast Zero-Shot Missing-Data Imputation with a Pre-Trained Transformer](https://arxiv.org/abs/2510.02625)
*Jacob Feitelberg,Dwaipayan Saha,Kyuseong Choi,Zaid Ahmad,Anish Agarwal,Raaz Dwivedi*

Main category: cs.LG

TL;DR: TabImpute是一个预训练的Transformer模型，用于在表格数据中进行零样本插补，无需训练或超参数调整，实现了准确且快速的插补。


<details>
  <summary>Details</summary>
Motivation: 现有表格数据缺失值处理方法在不同应用场景下表现不稳定且需要耗时的超参数调优，缺乏通用的解决方案。

Method: 提出TabImpute，一个基于TabPFN的预训练Transformer模型，采用入口特征化（entry-wise featurization）实现100倍加速，并结合包含真实缺失模式的合成数据生成管线进行训练。构建了包含42个OpenML数据集和13种缺失模式的MissBench基准测试集。

Result: TabImpute在MissBench基准测试中，与11种已有的插补方法相比，展现出稳健的性能，实现了准确且快速的零样本插补。

Conclusion: TabImpute提供了一种无需拟合或超参数调优的准确且快速的表格数据缺失值插补方法，并且在广泛的真实世界应用中表现优于现有方法。

Abstract: Missing data is a pervasive problem in tabular settings. Existing solutions
range from simple averaging to complex generative adversarial networks.
However, due to huge variance in performance across real-world domains and
time-consuming hyperparameter tuning, no default imputation method exists.
Building on TabPFN, a recent tabular foundation model for supervised learning,
we propose TabImpute, a pre-trained transformer that delivers accurate and fast
zero-shot imputations requiring no fitting or hyperparameter tuning at
inference-time. To train and evaluate TabImpute, we introduce (i) an entry-wise
featurization for tabular settings, which enables a $100\times$ speedup over
the previous TabPFN imputation method, (ii) a synthetic training data
generation pipeline incorporating realistic missingness patterns, which boosts
test-time performance, and (iii) MissBench, a comprehensive benchmark for
evaluation of imputation methods with $42$ OpenML datasets and $13$ missingness
patterns. MissBench spans domains such as medicine, finance, and engineering,
showcasing TabImpute's robust performance compared to $11$ established
imputation methods.

</details>


### [292] [HyperAdaLoRA: Accelerating LoRA Rank Allocation During Training via Hypernetworks without Sacrificing Performance](https://arxiv.org/abs/2510.02630)
*Hao Zhang,Zhenjia Li,Runfeng Bao,Yifan Gao,Xi Xiao,Bo Huang,Yuhang Wu,Tianyang Wang,Hao Xu*

Main category: cs.LG

TL;DR: AdaLoRA通过SVD动态分配秩，但收敛慢。HyperAdaLoRA使用超网络加速AdaLoRA收敛，通过剪枝超网络输出实现动态秩分配，实验证明其收敛更快且不牺牲性能。


<details>
  <summary>Details</summary>
Motivation: LoRA为LLM微调提供了有效途径，但其假设各层秩相同。AdaLoRA引入动态秩分配但存在收敛慢和计算开销大的问题。

Method: 提出HyperAdaLoRA框架，利用基于注意力机制的超网络动态生成AdaLoRA的SVD参数（P, Λ, Q），并通过剪枝超网络生成的奇异值实现动态秩分配，加速AdaLoRA收敛。

Result: 在多个数据集和模型上的实验表明，HyperAdaLoRA比AdaLoRA收敛更快，且性能相当。对其他LoRA类方法的扩展实验也验证了该方法的广泛适用性。

Conclusion: HyperAdaLoRA通过超网络有效加速了AdaLoRA的收敛过程，同时保持了性能，并易于扩展到其他LoRA类方法。

Abstract: Parameter-Efficient Fine-Tuning (PEFT), especially Low-Rank Adaptation
(LoRA), has emerged as a promising approach to fine-tuning large language
models(LLMs) while reducing computational and memory overhead. However, LoRA
assumes a uniform rank \textit{r} for each incremental matrix, not accounting
for the varying significance of weight matrices across different modules and
layers. AdaLoRA leverages Singular Value Decomposition (SVD) to parameterize
updates and employs pruning of singular values to introduce dynamic rank
allocation, thereby enhancing adaptability. However, during the training
process, it often encounters issues of slow convergence speed and high
computational overhead. To address this issue, we propose HyperAdaLoRA, a novel
framework that accelerates the convergence of AdaLoRA by leveraging a
hypernetwork. Instead of directly optimizing the components of Singular Value
Decomposition $(P, \Lambda, Q)$, HyperAdaLoRA employs a hypernetwork based on
attention mechanisms to dynamically generate these parameters. By pruning the
outputs of the hypernetwork that generates the singular values, dynamic rank
allocation is achieved. Comprehensive experiments on various datasets and
models demonstrate that our method achieves faster convergence without
sacrificing performance. Additionally, further extension experiments on other
LoRA-based approaches validate the broad applicability of our method.

</details>


### [293] [Optimal Characteristics of Inspection Vehicle for Drive-by Bridge Inspection](https://arxiv.org/abs/2510.02658)
*A. Calderon Hurtado,E. Atroshchenko,K. C. Chang,C. W. Kim,M. Makki Alamdari*

Main category: cs.LG

TL;DR: 通过优化车辆参数来提升桥梁健康监测的损伤识别能力，发现特定频率比范围内的车辆效果最佳。


<details>
  <summary>Details</summary>
Motivation: 现有的桥梁健康监测方法（车桥耦合响应分析）受车辆自身参数影响较大，限制了检测效果，因此需要优化车辆以提高损伤敏感性。

Method: 提出一个优化框架，利用生成对抗自编码器（AAE）进行频域加速度响应重构，并通过Kriging元模型优化车辆轮胎悬架系统的质量和刚度，以最小化健康和损伤状态下损伤指标分布之间的Wasserstein距离。

Result: 优化后的车辆在特定频率比（车辆与桥梁的一阶自然频率之比在0.3至0.7之间）下能更有效地检测桥梁损伤，而接近共振频率的车辆效果不佳。较轻的车辆需要较低的自然频率以达到最优检测效果。

Conclusion: 本研究首次提出了用于行驶式传感的传感平台优化方法，并设计了专用检测车辆，为提高桥梁健康监测的损伤识别能力提供了新的途径。

Abstract: Drive-by inspection for bridge health monitoring has gained increasing
attention over the past decade. This method involves analysing the coupled
vehicle-bridge response, recorded by an instrumented inspection vehicle, to
assess structural integrity and detect damage. However, the vehicles mechanical
and dynamic properties significantly influence detection performance, limiting
the effectiveness of the approach. This study presents a framework for
optimising the inspection vehicle to enhance damage sensitivity. An
unsupervised deep learning methodbased on adversarial autoencoders (AAE)is used
to reconstruct the frequency-domain representation of acceleration responses.
The mass and stiffness of the tyre suspension system of a two-axle vehicle are
optimised by minimising the Wasserstein distance between damage index
distributions for healthy and damaged bridge states. A Kriging meta-model is
employed to approximate this objective function efficiently and identify
optimal vehicle configurations in both dimensional and non-dimensional
parameter spaces. Results show that vehicles with frequency ratios between 0.3
and 0.7 relative to the bridges' first natural frequency are most effective,
while those near resonance perform poorly. Lighter vehicles require lower
natural frequencies for optimal detection. This is the first study to
rigorously optimise the sensing platform for drive-by sensing and to propose a
purpose-built inspection vehicle.

</details>


### [294] [TutorBench: A Benchmark To Assess Tutoring Capabilities Of Large Language Models](https://arxiv.org/abs/2510.02663)
*Rakshith S Srinivasa,Zora Che,Chen Bo Calvin Zhang,Diego Mares,Ernesto Hernandez,Jayeon Park,Dean Lee,Guillermo Mangialardi,Charmaine Ng,Ed-Yeremai Hernandez Cardona,Anisha Gunjal,Yunzhong He,Bing Liu,Chen Xing*

Main category: cs.LG

TL;DR: TutorBench是一个评估大型语言模型（LLMs）作为学习辅助工具在辅导方面的能力的基准，包含1490个样本，涵盖适应性解释、反馈和提示生成等任务。评估结果显示，现有LLMs在辅导技能方面仍有很大提升空间。


<details>
  <summary>Details</summary>
Motivation: 随着学生越来越多地使用大型语言模型（LLMs）作为学习辅助工具，有必要开发能够有效处理辅导细微差别的模型，这些模型需要能够识别学生的核心需求、具有适应性、提供个性化指导并且准确。为此，我们引入了TutorBench。

Method: TutorBench是一个数据集和评估基准，包含1490个由人类专家策划的样本，重点关注高中和AP课程。这些样本涵盖了三个常见的辅导任务：(i)生成针对学生困惑的适应性解释，(ii)对学生作业提供可操作的反馈，以及(iii)通过有效的提示生成来促进主动学习。为了应对辅导的内在复杂性，样本附带了特定于样本的评分标准，用于在评估期间判断模型响应。TutorBench采用了一种可靠且细粒度的自动评估方法，该方法使用LLM-judge和特定于样本的评分标准。

Result: 我们评估了16个前沿LLMs在TutorBench上的表现，并详细分析了它们的性能和行为。结果表明，没有一个前沿LLM的得分超过56%，显示出很大的改进空间。我们发现LLMs在展示指导、诊断和有效支持学生所需的全部辅导技能方面存在不足，所有前沿模型在与这些技能相关的评分标准上的通过率都低于60%。此外，不同的模型系列表现出不同的优势和局限性：Claude模型在支持主动学习方面优于其他模型，但在其他两个用例方面表现落后。

Conclusion: 通过发布TutorBench，我们提供了一个全面且未饱和的基准，以指导下一代AI导师的开发。

Abstract: As students increasingly adopt large language models (LLMs) as learning aids,
it is crucial to build models that are adept at handling the nuances of
tutoring: they need to identify the core needs of students, be adaptive,
provide personalized guidance, and be accurate. To this end, we introduce
TutorBench, a dataset and evaluation benchmark designed to rigorously evaluate
the core tutoring skills of LLMs. The dataset comprises 1,490 samples curated
by human experts, focused on high-school and AP-level curricula. The samples
are drawn from three common tutoring tasks: (i) generating adaptive
explanations tailored to a student's confusion, (ii) providing actionable
feedback on a student's work, and (iii) promoting active learning through
effective hint generation. To account for the inherent complexity of tutoring,
samples are accompanied by sample-specific rubrics which are used to judge
model responses during evaluation. TutorBench uses a reliable and fine-grained
automatic evaluation method that uses an LLM-judge and the sample-specific
rubrics. We evaluate 16 frontier LLMs on TutorBench and present a detailed
analysis of their performance and behavior. Our results show that none of the
frontier LLMs achieve a score of greater than $56\%$, showing a large room for
improvement. We find that LLMs fall short in exhibiting the full range of
tutoring skills needed to guide, diagnose, and support students effectively,
with all the frontier models achieving less than a $60\%$ pass rate on rubric
criteria related to these skills. We also find that different model families
exhibit varied strengths and limitations: the Claude models outperform others
in supporting active learning, while they lag behind in the other two use
cases. By releasing TutorBench, we provide a comprehensive and unsaturated
benchmark to guide the development of the next-generation of AI tutors.

</details>


### [295] [Topological Invariance and Breakdown in Learning](https://arxiv.org/abs/2510.02670)
*Yongyi Yang,Tomaso Poggio,Isaac Chuang,Liu Ziyin*

Main category: cs.LG

TL;DR: 梯度下降训练会产生一个双 Lipschitz 映射，并在训练过程中严格约束神经元分布的拓扑结构。学习率低于临界点时，拓扑结构被保留；高于临界点时，拓扑结构被简化，模型表达能力降低。


<details>
  <summary>Details</summary>
Motivation: 研究梯度下降等训练过程对神经网络拓扑结构的影响，并解释不同学习率下的训练动力学。

Method: 证明了梯度下降等训练过程产生的映射具有双 Lipschitz 连续性，并分析了学习率与拓扑结构的关系。

Result: 训练过程产生的映射是双 Lipschitz 映射；低于临界学习率时拓扑结构得以保留，高于临界学习率时拓扑结构被简化，模型表达能力下降。

Conclusion: 神经网络的训练动力学可以分为两个阶段：平滑优化和拓扑简化。该理论独立于具体网络结构和损失函数，可普遍应用于深度学习的拓扑学研究。

Abstract: We prove that for a broad class of permutation-equivariant learning rules
(including SGD, Adam, and others), the training process induces a bi-Lipschitz
mapping between neurons and strongly constrains the topology of the neuron
distribution during training. This result reveals a qualitative difference
between small and large learning rates $\eta$. With a learning rate below a
topological critical point $\eta^*$, the training is constrained to preserve
all topological structure of the neurons. In contrast, above $\eta^*$, the
learning process allows for topological simplification, making the neuron
manifold progressively coarser and thereby reducing the model's expressivity.
Viewed in combination with the recent discovery of the edge of stability
phenomenon, the learning dynamics of neuron networks under gradient descent can
be divided into two phases: first they undergo smooth optimization under
topological constraints, and then enter a second phase where they learn through
drastic topological simplifications. A key feature of our theory is that it is
independent of specific architectures or loss functions, enabling the universal
application of topological methods to the study of deep learning.

</details>


### [296] [To Compress or Not? Pushing the Frontier of Lossless GenAI Model Weights Compression with Exponent Concentration](https://arxiv.org/abs/2510.02676)
*Zeyu Yang,Tianyi Zhang,Jianwen Xie,Chuan Li,Zhaozhuo Xu,Anshumali Shrivastava*

Main category: cs.LG

TL;DR: 生成式AI模型参数规模庞大，低精度计算至关重要。本文提出了一种理论和实证研究，探讨了生成式AI权重中普遍存在的指数集中现象，并证明了其源于随机梯度下降产生的α-稳定分布。基于此，本文设计了一种新的FP8格式（ECF8），并实现了无损压缩框架，在大型语言模型和DiT上取得了显著的内存节省和吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 生成式AI模型规模庞大，低精度计算是实现高效部署的必要手段。现有的低精度方法需要反量化操作，带来了额外的开销。本文旨在通过开发具有固有数值稳定性、内存和硬件效率的低精度浮点格式来解决这一问题。

Method: 通过理论和实证研究，分析了生成式AI模型权重中的指数集中现象，证明了其与随机梯度下降产生的α-稳定分布有关，并推导了指数熵的紧界。基于这些见解，设计了ECF8格式，一个包含感知熵的编码和GPU优化的解码的无损压缩框架。

Result: 在多达671B参数的LLM和DiT上的实验表明，ECF8格式实现了高达26.9%的内存节省和177.1%的吞吐量加速，同时保持了计算的完全无损（模型输出无偏差）。

Conclusion: 指数集中是训练有素模型的一种统计规律，为在FP8时代进行无损低精度浮点设计提供了一条原则性的途径。

Abstract: The scaling of Generative AI (GenAI) models into the hundreds of billions of
parameters makes low-precision computation indispensable for efficient
deployment. We argue that the fundamental solution lies in developing
low-precision floating-point formats, which inherently provide numerical
stability, memory savings, and hardware efficiency without dequantization
overhead. In this paper, we present a theoretical and empirical study of an
exponent concentration phenomenon in GenAI weights: exponents consistently
exhibit low entropy across architectures and modalities. We show that this
arises naturally from $\alpha$-stable distributions induced by stochastic
gradient descent, and we prove tight bounds on the entropy of exponents. Our
analysis establishes a theoretical compression limit near FP4.67, which
motivates the design of a practical FP8 format. Building on these insights, we
propose Exponent-Concentrated FP8 (ECF8), a lossless compression framework with
entropy-aware encoding and GPU-optimized decoding. Experiments on LLMs and DiTs
up to 671B parameters demonstrate up to 26.9% memory savings and 177.1%
throughput acceleration, with perfectly lossless computations, i.e., no
deviation in model outputs. Our results establish exponent concentration as a
statistical law of trained models and open a principled path for lossless
low-precision floating-point design in the FP8 era.

</details>


### [297] [Multiplicative-Additive Constrained Models:Toward Joint Visualization of Interactive and Independent Effects](https://arxiv.org/abs/2509.21923)
*Fumin Wang*

Main category: cs.LG

TL;DR: MACM模型在可解释性和预测性能上均优于GAM和CESR。


<details>
  <summary>Details</summary>
Motivation: 在医疗等高风险领域，机器学习模型的可解释性至关重要。GAM模型虽然可解释，但忽略了高阶交互效应，限制了预测性能。CESR模型可以同时处理所有特征的交互作用和单独效应，但性能并不优于GAM。

Method: 提出了一种新的多乘加约束模型（MACM），它在CESR模型的基础上增加了一个加性部分，以解耦交互项和独立项的系数，从而扩大了假设空间。MACM模型既可以可视化其乘法部分和加性部分的形状函数，又能够处理高阶交互作用，并且在预测性能上优于GAM和CESR。

Result: 基于神经网络的MACM模型在预测性能上显著优于CESR和现有的最先进的GAM模型。

Conclusion: MACM模型在可解释性和预测性能上均优于CESR和GAM模型，是医疗等高风险领域的一个有前景的模型。

Abstract: Interpretability is one of the considerations when applying machine learning
to high-stakes fields such as healthcare that involve matters of life safety.
Generalized Additive Models (GAMs) enhance interpretability by visualizing
shape functions. Nevertheless, to preserve interpretability, GAMs omit
higher-order interaction effects (beyond pairwise interactions), which imposes
significant constraints on their predictive performance. We observe that Curve
Ergodic Set Regression (CESR), a multiplicative model, naturally enables the
visualization of its shape functions and simultaneously incorporates both
interactions among all features and individual feature effects. Nevertheless,
CESR fails to demonstrate superior performance compared to GAMs. We introduce
Multiplicative-Additive Constrained Models (MACMs), which augment CESR with an
additive part to disentangle the intertwined coefficients of its interactive
and independent terms, thus effectively broadening the hypothesis space. The
model is composed of a multiplicative part and an additive part, whose shape
functions can both be naturally visualized, thereby assisting users in
interpreting how features participate in the decision-making process.
Consequently, MACMs constitute an improvement over both CESR and GAMs. The
experimental results indicate that neural network-based MACMs significantly
outperform both CESR and the current state-of-the-art GAMs in terms of
predictive performance.

</details>


### [298] [Can Data-Driven Dynamics Reveal Hidden Physics? There Is A Need for Interpretable Neural Operators](https://arxiv.org/abs/2510.02683)
*Wenhan Gao,Jian Luo,Fang Wan,Ruichen Xu,Xiang Liu,Haipeng Xing,Yi Liu*

Main category: cs.LG

TL;DR: 该工作将神经算子分为空间域模型和函数域模型，并在此基础上探讨了其学习机制、物理规律学习、模型改进以及结合物理知识的需求。


<details>
  <summary>Details</summary>
Motivation: 尽管神经算子在学习函数空间映射方面取得了成功，但对其学习机制的深入理解仍然不足。

Method: 将神经算子分为空间域模型和函数域模型；提出了一种解释神经算子预测过程的方法，以展示其从数据中学习物理规律的能力；提出了一种双空间多尺度模型。

Result: 所提出的解释方法在特定情况下有效，但需要更通用的解释方法；提出的双空间多尺度模型达到了现有技术水平（SOTA）；该研究强调了将已知物理原理整合到神经算子中的重要性，以提高泛化能力和发现更多物理现象。

Conclusion: 神经算子在学习和模拟复杂动力学方面具有巨大潜力，但仍需在理论理解、模型设计和物理信息融合方面进行深入研究。

Abstract: Recently, neural operators have emerged as powerful tools for learning
mappings between function spaces, enabling data-driven simulations of complex
dynamics. Despite their successes, a deeper understanding of their learning
mechanisms remains underexplored. In this work, we classify neural operators
into two types: (1) Spatial domain models that learn on grids and (2)
Functional domain models that learn with function bases. We present several
viewpoints based on this classification and focus on learning data-driven
dynamics adhering to physical principles. Specifically, we provide a way to
explain the prediction-making process of neural operators and show that neural
operator can learn hidden physical patterns from data. However, this
explanation method is limited to specific situations, highlighting the urgent
need for generalizable explanation methods. Next, we show that a simple
dual-space multi-scale model can achieve SOTA performance and we believe that
dual-space multi-spatio-scale models hold significant potential to learn
complex physics and require further investigation. Lastly, we discuss the
critical need for principled frameworks to incorporate known physics into
neural operators, enabling better generalization and uncovering more hidden
physical phenomena.

</details>


### [299] [EvoSpeak: Large Language Models for Interpretable Genetic Programming-Evolved Heuristics](https://arxiv.org/abs/2510.02686)
*Meng Xu,Jiao Liu,Yew Soon Ong*

Main category: cs.LG

TL;DR: EvoSpeak框架结合遗传编程（GP）和大型语言模型（LLM），以提高启发式算法的演化效率、透明度和适应性，通过生成暖启动种群、将GP树翻译成自然语言解释以及促进跨任务的知识转移，在动态柔性作业车间调度（DFJSS）问题上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 在动态和大规模优化问题中，遗传编程（GP）产生的启发式算法虽然有效，但往往过于复杂，难以解释、收敛缓慢且难以跨任务迁移。EvoSpeak旨在解决这些挑战。

Method: EvoSpeak框架整合了GP和LLM。它从高质量的GP启发式算法中学习并提取知识，用于（1）生成暖启动种群以加速收敛，（2）将晦涩的GP树翻译成简洁的自然语言解释以增强可解释性和信任度，（3）实现跨相关任务的知识转移和面向偏好的启发式生成。

Result: 通过在动态柔性作业车间调度（DFJSS）问题上的广泛实验（包括单目标和多目标），结果表明EvoSpeak能够生成更有效的启发式算法，提高进化效率，并生成人类可读的报告，从而提高可用性。

Conclusion: EvoSpeak通过结合GP的符号推理能力和LLM的解释及生成能力，推动了面向现实世界优化问题的智能、透明且用户导向的启发式算法的发展。

Abstract: Genetic programming (GP) has demonstrated strong effectiveness in evolving
tree-structured heuristics for complex optimization problems. Yet, in dynamic
and large-scale scenarios, the most effective heuristics are often highly
complex, hindering interpretability, slowing convergence, and limiting
transferability across tasks. To address these challenges, we present EvoSpeak,
a novel framework that integrates GP with large language models (LLMs) to
enhance the efficiency, transparency, and adaptability of heuristic evolution.
EvoSpeak learns from high-quality GP heuristics, extracts knowledge, and
leverages this knowledge to (i) generate warm-start populations that accelerate
convergence, (ii) translate opaque GP trees into concise natural-language
explanations that foster interpretability and trust, and (iii) enable knowledge
transfer and preference-aware heuristic generation across related tasks. We
verify the effectiveness of EvoSpeak through extensive experiments on dynamic
flexible job shop scheduling (DFJSS), under both single- and multi-objective
formulations. The results demonstrate that EvoSpeak produces more effective
heuristics, improves evolutionary efficiency, and delivers human-readable
reports that enhance usability. By coupling the symbolic reasoning power of GP
with the interpretative and generative strengths of LLMs, EvoSpeak advances the
development of intelligent, transparent, and user-aligned heuristics for
real-world optimization problems.

</details>


### [300] [Fine-Tuning Diffusion Models via Intermediate Distribution Shaping](https://arxiv.org/abs/2510.02692)
*Gautham Govind Anil,Shaan Ul Haque,Nithish Kannen,Dheeraj Nagaraj,Sanjay Shakkottai,Karthikeyan Shanmugam*

Main category: cs.LG

TL;DR: 此论文提出GRAFT框架统一并改进了基于拒绝采样的扩散模型微调方法，并引入P-GRAFT和逆噪声校正技术，在文本到图像生成、分子生成等多个任务上取得了显著效果，尤其在文本到图像生成方面，相较于基线模型有所提升。


<details>
  <summary>Details</summary>
Motivation: 在生成扩散模型中，虽然预训练模型能有效捕捉训练数据分布，但通常需要根据下游应用的奖励函数来塑造这些分布。对于自回归生成，策略梯度方法（如PPO）被广泛使用，但扩散模型所需的边际似然函数难以计算。因此，需要探索新的微调方法。

Method: 本文提出了GRAFT框架，统一了基于拒绝采样的微调（RAFT）的变体，并证明其隐式执行了带有重塑奖励的PPO。随后，引入P-GRAFT，能在中间噪声水平上塑造分布，并通过偏差-方差权衡进行数学解释。此外，还提出了逆噪声校正技术，用于改进流模型，无需显式奖励。

Result: 在文本到图像生成、布局生成、分子生成和无条件图像生成等任务上进行了实证评估。GRAFT框架应用于Stable Diffusion 2，在文本到图像生成任务上，VQAScore指标优于策略梯度方法，且相对基线模型有8.81%的提升。逆噪声校正技术在无条件图像生成任务上，以更低的FLOPs/image提升了FID。

Conclusion: GRAFT框架通过统一和改进拒绝采样微调方法，以及引入P-GRAFT和逆噪声校正等技术，有效解决了扩散模型微调中的挑战，并在多个生成任务上取得了优于现有方法的性能。

Abstract: Diffusion models are widely used for generative tasks across domains. While
pre-trained diffusion models effectively capture the training data
distribution, it is often desirable to shape these distributions using reward
functions to align with downstream applications. Policy gradient methods, such
as Proximal Policy Optimization (PPO), are widely used in the context of
autoregressive generation. However, the marginal likelihoods required for such
methods are intractable for diffusion models, leading to alternative proposals
and relaxations. In this context, we unify variants of Rejection sAmpling based
Fine-Tuning (RAFT) as GRAFT, and show that this implicitly performs PPO with
reshaped rewards. We then introduce P-GRAFT to shape distributions at
intermediate noise levels and demonstrate empirically that this can lead to
more effective fine-tuning. We mathematically explain this via a bias-variance
tradeoff. Motivated by this, we propose inverse noise correction to improve
flow models without leveraging explicit rewards. We empirically evaluate our
methods on text-to-image(T2I) generation, layout generation, molecule
generation and unconditional image generation. Notably, our framework, applied
to Stable Diffusion 2, improves over policy gradient methods on popular T2I
benchmarks in terms of VQAScore and shows an $8.81\%$ relative improvement over
the base model. For unconditional image generation, inverse noise correction
improves FID of generated images at lower FLOPs/image.

</details>


### [301] [RAMAC: Multimodal Risk-Aware Offline Reinforcement Learning and the Role of Behavior Regularization](https://arxiv.org/abs/2510.02695)
*Kai Fukazawa,Kunal Mundada,Iman Soltani*

Main category: cs.LG

TL;DR: RAMAC框架通过结合生成式Actor和分布Critic，实现了风险敏感的离线强化学习，在保持高回报的同时降低了尾部风险。


<details>
  <summary>Details</summary>
Motivation: 在安全关键领域，需要一种能够提供高回报且不会产生灾难性低尾部风险的离线强化学习方法。现有方法在风险规避和策略表达性之间存在权衡。RAMAC旨在解决这个差距，实现风险敏感学习和复杂的、多模态的场景。

Method: RAMAC框架使用一个具有表达能力的生成式Actor和一个分布式的Critic。它通过生成路径区分了结合了分布风险和行为克隆（BC）损失的复合目标，实现了风险敏感的学习。具体实例化时，使用了扩散模型和流匹配Actor。

Result: 在Stochastic-D4RL任务上，RAMAC在$\mathrm{CVaR}_{0.1}$（风险价值）方面取得了持续的提升，同时在大多数任务上保持了强劲的回报。

Conclusion: RAMAC框架能够有效地在保持高回报的同时，降低离线强化学习中的尾部风险，尤其适用于复杂的多模态场景。

Abstract: In safety-critical domains where online data collection is infeasible,
offline reinforcement learning (RL) offers an attractive alternative but only
if policies deliver high returns without incurring catastrophic lower-tail
risk. Prior work on risk-averse offline RL achieves safety at the cost of value
conservatism and restricted policy classes, whereas expressive policies are
only used in risk-neutral settings. Here, we address this gap by introducing
the \textbf{Risk-Aware Multimodal Actor-Critic (RAMAC)} framework, which
couples an \emph{expressive generative actor} with a distributional critic. The
RAMAC differentiates composite objective combining distributional risk and BC
loss through the generative path, achieving risk-sensitive learning in complex
multimodal scenarios. We instantiate RAMAC with diffusion and flow-matching
actors and observe consistent gains in $\mathrm{CVaR}_{0.1}$ while maintaining
strong returns on most Stochastic-D4RL tasks. Code:
https://github.com/KaiFukazawa/RAMAC.git

</details>


### [302] [A Novel Unified Lightweight Temporal-Spatial Transformer Approach for Intrusion Detection in Drone Networks](https://arxiv.org/abs/2510.02711)
*Tarun Kumar Biswas,Ashrafun Zannat,Waqas Ishtiaq,Md. Alamgir Hossain*

Main category: cs.LG

TL;DR: TSLT-Net是一个新颖的轻量级统一时间空间Transformer入侵检测系统，专为无人机网络设计，能有效检测多种入侵，准确率高，资源占用少。


<details>
  <summary>Details</summary>
Motivation: 无人机网络的广泛应用带来了严峻的网络安全挑战，现有入侵检测机制在适应性、效率和泛化能力方面存在不足，无法满足无人机动态且资源受限的环境需求。

Method: 提出TSLT-Net，利用自注意力机制同时建模网络流量中的时间模式和空间依赖性，并包含一个简化的预处理流程，支持单一架构内的多类别攻击分类和二元异常检测。

Result: 在ISOT无人机异常检测数据集上进行的大量实验表明，TSLT-Net在多类别检测中准确率达到99.99%，在二元异常检测中准确率达到100%，同时内存占用仅为0.04 MB，可训练参数为9722个。

Conclusion: TSLT-Net被证明是一种有效且可扩展的解决方案，能够实现无人机网络近乎完美的实时网络安全防护，特别适合部署在对任务至关重要的无人机系统的边缘设备上。

Abstract: The growing integration of drones across commercial, industrial, and civilian
domains has introduced significant cybersecurity challenges, particularly due
to the susceptibility of drone networks to a wide range of cyberattacks.
Existing intrusion detection mechanisms often lack the adaptability,
efficiency, and generalizability required for the dynamic and resource
constrained environments in which drones operate. This paper proposes TSLT-Net,
a novel lightweight and unified Temporal Spatial Transformer based intrusion
detection system tailored specifically for drone networks. By leveraging self
attention mechanisms, TSLT-Net effectively models both temporal patterns and
spatial dependencies in network traffic, enabling accurate detection of diverse
intrusion types. The framework includes a streamlined preprocessing pipeline
and supports both multiclass attack classification and binary anomaly detection
within a single architecture. Extensive experiments conducted on the ISOT Drone
Anomaly Detection Dataset, consisting of more than 2.3 million labeled records,
demonstrate the superior performance of TSLT-Net with 99.99 percent accuracy in
multiclass detection and 100 percent in binary anomaly detection, while
maintaining a minimal memory footprint of only 0.04 MB and 9722 trainable
parameters. These results establish TSLT-Net as an effective and scalable
solution for real time drone cybersecurity, particularly suitable for
deployment on edge devices in mission critical UAV systems.

</details>


### [303] [CST-AFNet: A dual attention-based deep learning framework for intrusion detection in IoT networks](https://arxiv.org/abs/2510.02717)
*Waqas Ishtiaq,Ashrafun Zannat,A. H. M. Shahariar Parvez,Md. Alamgir Hossain,Muntasir Hasan Kanchan,Muhammad Masud Tarek*

Main category: cs.LG

TL;DR: CST AFNet是一个创新的双注意力深度学习框架，用于物联网（IoT）网络中的鲁棒入侵检测，实现了99.97%的准确率和超过99.3%的宏平均精度、召回率和F1分数，优于传统模型。


<details>
  <summary>Details</summary>
Motivation: 物联网的快速发展带来了复杂的网络安全挑战，需要更强大的入侵检测方法。

Method: 提出了一种名为CST AFNet的新型深度学习框架，该框架结合了多尺度卷积神经网络（CNN）进行空间特征提取、双向门控循环单元（BiGRU）进行时序依赖性捕获，以及一个双重注意力机制（通道和时间注意力）来增强对关键模式的关注。

Result: 在Edge IIoTset数据集（包含超过220万个标记实例，涵盖15种攻击类型和良性流量）上，CST AFNet在15种攻击类型和良性流量方面均达到了出色的准确率（99.97%），并且宏平均精度、召回率和F1分数均超过99.3%。实验结果表明，CST AFNet的检测准确率优于传统的深度学习模型。

Conclusion: CST AFNet为复杂物联网和IIoT环境中的实时网络威胁检测提供了一个强大且可扩展的解决方案，有助于实现更安全、更智能、更自适应的网络物理系统。

Abstract: The rapid expansion of the Internet of Things (IoT) has revolutionized modern
industries by enabling smart automation and real time connectivity. However,
this evolution has also introduced complex cybersecurity challenges due to the
heterogeneous, resource constrained, and distributed nature of these
environments. To address these challenges, this research presents CST AFNet, a
novel dual attention based deep learning framework specifically designed for
robust intrusion detection in IoT networks. The model integrates multi scale
Convolutional Neural Networks (CNNs) for spatial feature extraction,
Bidirectional Gated Recurrent Units (BiGRUs) for capturing temporal
dependencies, and a dual attention mechanism, channel and temporal attention,
to enhance focus on critical patterns in the data. The proposed method was
trained and evaluated on the Edge IIoTset dataset, a comprehensive and
realistic benchmark containing more than 2.2 million labeled instances spanning
15 attack types and benign traffic, collected from a seven layer industrial
testbed. Our proposed model achieves outstanding accuracy for both 15 attack
types and benign traffic. CST AFNet achieves 99.97 percent accuracy. Moreover,
this model demonstrates exceptional performance with macro averaged precision,
recall, and F1 score all above 99.3 percent. Experimental results show that CST
AFNet achieves superior detection accuracy, significantly outperforming
traditional deep learning models. The findings confirm that CST AFNet is a
powerful and scalable solution for real time cyber threat detection in complex
IoT and IIoT environments, paving the way for more secure, intelligent, and
adaptive cyber physical systems.

</details>


### [304] [Hyperparameter Loss Surfaces Are Simple Near their Optima](https://arxiv.org/abs/2510.02721)
*Nicholas Lourie,He He,Kyunghyun Cho*

Main category: cs.LG

TL;DR: 现代模型因规模过大而无法进行广泛的超参数搜索，但研究人员设计了基于对超参数理解的训练方案。本文发现了超参数损失函数的结构新特征，并提出了一种新的理论和基于随机搜索的技术来分析这种渐近状态，从而揭示了损失函数的有效维度和最佳可能损失。研究结果支持了一种新的随机搜索渐近定律，可以解释和推断其收敛性，并能确定有效超参数的数量。


<details>
  <summary>Details</summary>
Motivation: 现代模型规模过大，无法进行广泛的超参数搜索，研究人员需要更好的工具来理解超参数损失函数，以便设计出在不同尺度下都能良好训练的模型。

Method: 提出了一种基于随机搜索的新技术，用于揭示损失函数在接近最优时的渐近状态。通过分析随机搜索的最佳分数分布，提取定义损失函数渐近状态的特征（如有效维度和最佳损失）。

Result: 发现并利用了随机搜索的最佳分数分布，该分布的参数直接对应于损失函数在渐近状态下的特征。提出了一个新的随机搜索渐近定律，能够解释和推断其收敛行为。

Conclusion: 提出的新工具能够分析超参数损失函数，例如提供最佳性能的置信区间，并确定有效超参数的数量，从而为超参数优化提供新的见解和方法。

Abstract: Hyperparameters greatly impact models' capabilities; however, modern models
are too large for extensive search. Instead, researchers design recipes that
train well across scales based on their understanding of the hyperparameters.
Despite this importance, few tools exist for understanding the hyperparameter
loss surface. We discover novel structure in it and propose a new theory
yielding such tools. The loss surface is complex, but as you approach the
optimum simple structure emerges. It becomes characterized by a few basic
features, like its effective dimension and the best possible loss. To uncover
this asymptotic regime, we develop a novel technique based on random search.
Within this regime, the best scores from random search take on a new
distribution we discover. Its parameters are exactly the features defining the
loss surface in the asymptotic regime. From these features, we derive a new
asymptotic law for random search that can explain and extrapolate its
convergence. These new tools enable new analyses, such as confidence intervals
for the best possible performance or determining the effective number of
hyperparameters. We make these tools available at
https://github.com/nicholaslourie/opda .

</details>


### [305] [Accuracy Law for the Future of Deep Time Series Forecasting](https://arxiv.org/abs/2510.02729)
*Yuxuan Wang,Haixu Wu,Yuezhou Ma,Yuchen Fang,Ziyi Zhang,Yong Liu,Shiyu Wang,Zhou Ye,Yang Xiang,Jianmin Wang,Mingsheng Long*

Main category: cs.LG

TL;DR: 时间序列预测的研究目标应为估计性能上限，而非追求在现有基准上的微小改进。


<details>
  <summary>Details</summary>
Motivation: 当前深度时间序列预测领域的研究者因在标准基准上的微小改进而感到困惑，缺乏明确的研究方向，与图像识别追求100%准确率不同，时间序列预测本身存在不可避免的误差下限。本文旨在解决如何估计深度时间序列预测的性能上限这一根本问题。

Method: 通过超越传统的序列预测指标（如ADF检验），利用窗户（window-wise）层面的统计特性来分析预测性能，并基于对超过2800个新训练的深度预测器的严格统计检验，发现了深度模型最小预测误差与窗户模式复杂度之间存在显著的指数关系，并将其命名为“准确率法则”。

Result: “准确率法则”成功地帮助识别了广泛使用的基准测试中的饱和任务，并推导出了一个有效的训练策略，为大型时间序列模型提供了有价值的见解。

Conclusion: “准确率法则”为深度时间序列预测提供了新的研究方向，有助于研究者聚焦于更具挑战性的问题，并指导了模型训练策略的优化。

Abstract: Deep time series forecasting has emerged as a booming direction in recent
years. Despite the exponential growth of community interests, researchers are
sometimes confused about the direction of their efforts due to minor
improvements on standard benchmarks. In this paper, we notice that, unlike
image recognition, whose well-acknowledged and realizable goal is 100%
accuracy, time series forecasting inherently faces a non-zero error lower bound
due to its partially observable and uncertain nature. To pinpoint the research
objective and release researchers from saturated tasks, this paper focuses on a
fundamental question: how to estimate the performance upper bound of deep time
series forecasting? Going beyond classical series-wise predictability metrics,
e.g., ADF test, we realize that the forecasting performance is highly related
to window-wise properties because of the sequence-to-sequence forecasting
paradigm of deep time series models. Based on rigorous statistical tests of
over 2,800 newly trained deep forecasters, we discover a significant
exponential relationship between the minimum forecasting error of deep models
and the complexity of window-wise series patterns, which is termed the accuracy
law. The proposed accuracy law successfully guides us to identify saturated
tasks from widely used benchmarks and derives an effective training strategy
for large time series models, offering valuable insights for future research.

</details>


### [306] [Dale meets Langevin: A Multiplicative Denoising Diffusion Model](https://arxiv.org/abs/2510.02730)
*Nishanth Shetty,Madhava Prasath,Chandra Sekhar Seelamantula*

Main category: cs.LG

TL;DR: 生物启发的指数梯度下降法通过反向时间随机微分方程实现，并提出了一种新的乘性去噪评分匹配方法，在图像生成方面取得了良好效果。


<details>
  <summary>Details</summary>
Motivation: 标准的梯度下降法在机器学习中广泛应用，但与生物学习机制不符。因此，需要开发受生物学启发的学习技术。

Method: 从控制指数梯度下降的 Dale 律出发，将其与几何布朗运动的随机微分方程联系起来。通过对反向时间随机微分方程进行离散化，得到一种乘性更新规则，这与指数梯度下降法的采样等效形式一致。在此基础上，提出了一种新的乘性去噪评分匹配方法，该方法适用于对数正态分布的数据（例如图像数据）。

Result: 所提出的乘性更新方案在 MNIST、Fashion MNIST 和 Kuzushiji 数据集上进行了实验，结果表明该方案具有良好的生成能力，能够生成逼真的图像。

Conclusion: 这项工作首次提出了一个受生物启发的生成模型，该模型采用乘性更新，并基于几何布朗运动。该模型在图像生成方面展现出有前景的结果，并为开发新的生成模型提供了新的方向。

Abstract: Gradient descent has proven to be a powerful and effective technique for
optimization in numerous machine learning applications. Recent advances in
computational neuroscience have shown that learning in standard gradient
descent optimization formulation is not consistent with learning in biological
systems. This has opened up interesting avenues for building biologically
inspired learning techniques. One such approach is inspired by Dale's law,
which states that inhibitory and excitatory synapses do not swap roles during
the course of learning. The resulting exponential gradient descent optimization
scheme leads to log-normally distributed synaptic weights. Interestingly, the
density that satisfies the Fokker-Planck equation corresponding to the
stochastic differential equation (SDE) with geometric Brownian motion (GBM) is
the log-normal density. Leveraging this connection, we start with the SDE
governing geometric Brownian motion, and show that discretizing the
corresponding reverse-time SDE yields a multiplicative update rule, which
surprisingly, coincides with the sampling equivalent of the exponential
gradient descent update founded on Dale's law. Furthermore, we propose a new
formalism for multiplicative denoising score-matching, subsuming the loss
function proposed by Hyvaerinen for non-negative data. Indeed, log-normally
distributed data is positive and the proposed score-matching formalism turns
out to be a natural fit. This allows for training of score-based models for
image data and results in a novel multiplicative update scheme for sample
generation starting from a log-normal density. Experimental results on MNIST,
Fashion MNIST, and Kuzushiji datasets demonstrate generative capability of the
new scheme. To the best of our knowledge, this is the first instance of a
biologically inspired generative model employing multiplicative updates,
founded on geometric Brownian motion.

</details>


### [307] [Hybrid-Collaborative Augmentation and Contrastive Sample Adaptive-Differential Awareness for Robust Attributed Graph Clustering](https://arxiv.org/abs/2510.02731)
*Tianxiang Zhao,Youqing Wang,Jinlu Wang,Jiapu Wang,Mingliang Cui,Junbin Gao,Jipeng Guo*

Main category: cs.LG

TL;DR: 该研究提出了一种名为RAGC的新型图聚类方法，通过结合混合协同增强（HCA）和对比样本自适应差异感知（CSADA）来解决现有对比属性图聚类（CAGC）方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有CAGC方法主要依赖节点级别的信息和增强，忽略了边级别信息的增强以及节点和边级别增强之间的相互作用。此外，它们通常平等对待所有对比样本对，未能区分难易样本对，从而限制了区分能力。

Method: RAGC同时执行节点和边级别的嵌入表示和增强，以实现更全面的相似性度量。然后，利用高置信度的伪标签信息，设计CSADA策略，通过权重调制函数自适应地识别和区分对比样本对。HCA和CSADA模块相互促进，增强表示学习的区分能力。

Result: 在六个基准数据集上的综合图聚类评估表明，RAGC在区分能力方面优于现有的CAGC方法。

Conclusion: RAGC通过混合协同增强和对比样本自适应差异感知，有效提升了图聚类方法的性能，特别是在处理节点和边信息交互以及区分不同难易程度的对比样本方面。

Abstract: Due to its powerful capability of self-supervised representation learning and
clustering, contrastive attributed graph clustering (CAGC) has achieved great
success, which mainly depends on effective data augmentation and contrastive
objective setting. However, most CAGC methods utilize edges as auxiliary
information to obtain node-level embedding representation and only focus on
node-level embedding augmentation. This approach overlooks edge-level embedding
augmentation and the interactions between node-level and edge-level embedding
augmentations across various granularity. Moreover, they often treat all
contrastive sample pairs equally, neglecting the significant differences
between hard and easy positive-negative sample pairs, which ultimately limits
their discriminative capability. To tackle these issues, a novel robust
attributed graph clustering (RAGC), incorporating hybrid-collaborative
augmentation (HCA) and contrastive sample adaptive-differential awareness
(CSADA), is proposed. First, node-level and edge-level embedding
representations and augmentations are simultaneously executed to establish a
more comprehensive similarity measurement criterion for subsequent contrastive
learning. In turn, the discriminative similarity further consciously guides
edge augmentation. Second, by leveraging pseudo-label information with high
confidence, a CSADA strategy is elaborately designed, which adaptively
identifies all contrastive sample pairs and differentially treats them by an
innovative weight modulation function. The HCA and CSADA modules mutually
reinforce each other in a beneficent cycle, thereby enhancing discriminability
in representation learning. Comprehensive graph clustering evaluations over six
benchmark datasets demonstrate the effectiveness of the proposed RAGC against
several state-of-the-art CAGC methods.

</details>


### [308] [TokenFlow: Responsive LLM Text Streaming Serving under Request Burst via Preemptive Scheduling](https://arxiv.org/abs/2510.02758)
*Junyi Chen,Chuheng Du,Renyuan Liu,Shuochao Yao,Dingtian Yan,Jiang Liao,Shengzhong Liu,Fan Wu,Guihai Chen*

Main category: cs.LG

TL;DR: TokenFlow通过抢占式请求调度和主动KV缓存管理来优化LLM服务中的文本流式传输，提高了吞吐量和响应速度。


<details>
  <summary>Details</summary>
Motivation: 标准LLM服务系统在处理实时交互时，由于非抢占式请求调度和被动内存管理，资源利用率低，请求并行处理能力差，尤其是在请求突发时。

Method: TokenFlow采用抢占式请求调度（基于实时令牌缓冲区占用和消耗率动态确定请求优先级）和主动KV缓存管理（在后台进行GPU和CPU内存之间的KV缓存转移），并使I/O与计算重叠，以最小化请求抢占开销。

Result: 在Llama3-8B和Qwen2.5-32B模型上，使用RTX 4090、A6000、H200等多种GPU进行的大量实验表明，TokenFlow的有效吞吐量最高可提高82.5%，P99的首次令牌生成时间（TTFT）最多可降低80.2%，同时不降低整体令牌吞吐量。

Conclusion: TokenFlow通过创新的调度和内存管理策略，显著提升了LLM服务的文本流式传输性能，解决了现有系统在响应速度和生成稳定性方面的挑战。

Abstract: Real-time LLM interactions demand streamed token generations, where text
tokens are progressively generated and delivered to users while balancing two
objectives: responsiveness (i.e., low time-to-first-token) and steady
generation (i.e.,required time-between-tokens). Standard LLM serving systems
suffer from the inflexibility caused by non-preemptive request scheduling and
reactive memory management, leading to poor resource utilization and low
request processing parallelism under request bursts. Therefore, we present
TokenFlow, a novel LLM serving system with enhanced text streaming performance
via preemptive request scheduling and proactive key-value (KV) cache
management. TokenFlow dynamically prioritizes requests based on real-time token
buffer occupancy and token consumption rate, while actively transferring KV
cache between GPU and CPU memory in the background and overlapping I/O with
computation to minimize request preemption overhead. Extensive experiments on
Llama3-8B and Qwen2.5-32B across multiple GPUs (RTX 4090, A6000, H200)
demonstrate that TokenFlow achieves up to 82.5% higher effective throughput
(accounting for actual user consumption) while reducing P99 TTFT by up to
80.2%, without degrading overall token throughput.

</details>


### [309] [Fusing Multi- and Hyperspectral Satellite Data for Harmful Algal Bloom Monitoring with Self-Supervised and Hierarchical Deep Learning](https://arxiv.org/abs/2510.02763)
*Nicholas LaHaye,Kelly M. Luis,Michelle M. Gierach*

Main category: cs.LG

TL;DR: 本研究提出了一种名为SIT-FUSE的自监督机器学习框架，利用多源卫星数据（VIIRS、MODIS、Sentinel-3、TROPOMI）融合太阳诱导荧光（SIF），实现了有害藻华（HAB）严重程度和物种的监测，无需每种仪器都有标记数据集。


<details>
  <summary>Details</summary>
Motivation: 在标签稀缺的环境中，对有害藻华（HAB）进行大规模监测，并实现对全球水生生物地球化学的探索性分析。

Method: 利用自监督表示学习和分层深度聚类，融合来自VIIRS、MODIS、Sentinel-3和TROPOMI的反射率和太阳诱导荧光（SIF）数据，生成HAB严重程度和物种产品。

Result: 该框架生成的HAB严重程度和物种产品与墨西哥湾和南加州的实测数据（2018-2025）高度一致，能够准确识别总浮游植物、凯氏鳍藻、亚历山大藻和海链藻等。

Conclusion: 该研究成功开发了一种在标签稀缺环境下进行HAB监测的可扩展方法，并通过分层嵌入实现了探索性分析，为在全球水生生物地球化学领域运行自监督学习奠定了基础。

Abstract: We present a self-supervised machine learning framework for detecting and
mapping harmful algal bloom (HAB) severity and speciation using multi-sensor
satellite data. By fusing reflectance data from operational instruments (VIIRS,
MODIS, Sentinel-3, PACE) with TROPOMI solar-induced fluorescence (SIF), our
framework, called SIT-FUSE, generates HAB severity and speciation products
without requiring per-instrument labeled datasets. The framework employs
self-supervised representation learning, hierarchical deep clustering to
segment phytoplankton concentrations and speciations into interpretable
classes, validated against in-situ data from the Gulf of Mexico and Southern
California (2018-2025). Results show strong agreement with total phytoplankton,
Karenia brevis, Alexandrium spp., and Pseudo-nitzschia spp. measurements. This
work advances scalable HAB monitoring in label-scarce environments while
enabling exploratory analysis via hierarchical embeddings: a critical step
toward operationalizing self-supervised learning for global aquatic
biogeochemistry.

</details>


### [310] [Curl Descent: Non-Gradient Learning Dynamics with Sign-Diverse Plasticity](https://arxiv.org/abs/2510.02765)
*Hugo Ninou,Jonathan Kadmon,N. Alex Cayco-Gajic*

Main category: cs.LG

TL;DR: 生物神经网络的学习动力学可能包含非梯度“curl”分量，并且仍然可以有效地优化损失函数。


<details>
  <summary>Details</summary>
Motivation: 探索生物神经网络是否以及如何使用梯度下降以外的学习策略，特别是包含“curl”分量的非梯度动力学。

Method: 在分析可行的学生-教师框架内，通过引入具有规则翻转可塑性的神经元来系统地分析前馈网络中的非梯度动力学，以研究“curl”分量的影响。

Result: 少量“curl”分量可以稳定原始解流形，使学习动力学类似于梯度下降；然而，当“curl”分量超过临界值时，会破坏解流形，导致混沌学习动力学或意外地加速学习。

Conclusion: 研究结果确定了能够支持通过各种学习规则进行鲁棒学习的特定网络架构，为神经元网络中基于梯度的学习规范理论提供了重要的对立观点。

Abstract: Gradient-based algorithms are a cornerstone of artificial neural network
training, yet it remains unclear whether biological neural networks use similar
gradient-based strategies during learning. Experiments often discover a
diversity of synaptic plasticity rules, but whether these amount to an
approximation to gradient descent is unclear. Here we investigate a previously
overlooked possibility: that learning dynamics may include fundamentally
non-gradient "curl"-like components while still being able to effectively
optimize a loss function. Curl terms naturally emerge in networks with
inhibitory-excitatory connectivity or Hebbian/anti-Hebbian plasticity,
resulting in learning dynamics that cannot be framed as gradient descent on any
objective. To investigate the impact of these curl terms, we analyze
feedforward networks within an analytically tractable student-teacher
framework, systematically introducing non-gradient dynamics through neurons
exhibiting rule-flipped plasticity. Small curl terms preserve the stability of
the original solution manifold, resulting in learning dynamics similar to
gradient descent. Beyond a critical value, strong curl terms destabilize the
solution manifold. Depending on the network architecture, this loss of
stability can lead to chaotic learning dynamics that destroy performance. In
other cases, the curl terms can counterintuitively speed learning compared to
gradient descent by allowing the weight dynamics to escape saddles by
temporarily ascending the loss. Our results identify specific architectures
capable of supporting robust learning via diverse learning rules, providing an
important counterpoint to normative theories of gradient-based learning in
neural networks.

</details>


### [311] [A Granular Study of Safety Pretraining under Model Abliteration](https://arxiv.org/abs/2510.02768)
*Shashank Agnihotri,Jonas Jakubassa,Priyam Dey,Sachin Goyal,Bernt Schiele,Venkatesh Babu Radhakrishnan,Margret Keuper*

Main category: cs.LG

TL;DR: 研究了推理时修改LLM的激活编辑是否会影响安全干预措施（如拒绝训练或元标签训练）的有效性，并研究了模型消融技术（一种用于去除拒绝敏感方向的轻量级投影技术）的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 评估常见的安全干预措施（如拒绝训练和元标签训练）在面对推理时的模型激活编辑时是否仍然有效。

Method: 使用模型消融技术，在SmolLM2-1.7B的安全预训练检查点和常用开放基线上，对原始模型和消融模型进行了评估。对20个系统，每个系统使用100个包含有害和无害案例的提示进行测试，并使用多个裁判对响应进行分类（拒绝/非拒绝），同时验证裁判的准确性。此外，还探究了模型识别自身输出中拒绝的能力。

Result: 提供了模型检查点级别的安全组件在消融下的鲁棒性表征，量化了裁判选择对评估结果的影响，并提出了将推理时编辑纳入安全评估的实用方案。

Conclusion: 揭示了数据中心的安全组件在推理时模型编辑下的鲁棒性，并为评估这些组件提供了量化方法和实用协议。

Abstract: Open-weight LLMs can be modified at inference time with simple activation
edits, which raises a practical question for safety: do common safety
interventions like refusal training or metatag training survive such edits? We
study model abliteration, a lightweight projection technique designed to remove
refusal-sensitive directions, and conduct a controlled evaluation across a
granular sequence of Safety Pretraining checkpoints for SmolLM2-1.7B, alongside
widely used open baselines. For each of 20 systems, original and abliterated,
we issue 100 prompts with balanced harmful and harmless cases, classify
responses as **Refusal** or **Non-Refusal** using multiple judges, and validate
judge fidelity on a small human-labeled subset. We also probe whether models
can identify refusal in their own outputs. Our study produces a
checkpoint-level characterization of which data-centric safety components
remain robust under abliteration, quantifies how judge selection influences
evaluation outcomes, and outlines a practical protocol for integrating
inference-time edits into safety assessments. Code:
https://github.com/shashankskagnihotri/safety_pretraining.

</details>


### [312] [Optimal Rates for Generalization of Gradient Descent for Deep ReLU Classification](https://arxiv.org/abs/2510.02779)
*Yuanfan Li,Yunwen Lei,Zheng-Chu Guo,Yiming Ying*

Main category: cs.LG

TL;DR: 梯度下降（GD）在深度 ReLU 网络中实现了与 minimax 最优率相当的泛化性能，其深度依赖性为多项式。


<details>
  <summary>Details</summary>
Motivation: 探究 GD 方法在深度神经网络中的泛化性能是否能达到与核方法相当的 minimax 最优率。

Method: 通过精确权衡优化和泛化误差，证明了 GD 在深度 ReLU 网络上可以实现最优泛化率。具体来说，在数据与裕度 $\gamma$ 呈 NTK 可分离的假设下，证明了超额风险率为 $\widetilde{O}(L^4 (1 + \gamma L^2) / (n \gamma^2))$。关键技术贡献在于对参考模型附近激活模式的新颖控制，从而为深度 ReLU 网络提供了更紧的 Rademacher 复杂度界限。

Result: 在 NTK 可分离的假设下，实现了 $\widetilde{O}(L^4 (1 + \gamma L^2) / (n \gamma^2))$ 的超额风险率。

Conclusion: GD 在深度 ReLU 网络中可以达到与 minimax 最优率相当的泛化率，并且对深度的依赖性为多项式。

Abstract: Recent advances have significantly improved our understanding of the
generalization performance of gradient descent (GD) methods in deep neural
networks. A natural and fundamental question is whether GD can achieve
generalization rates comparable to the minimax optimal rates established in the
kernel setting. Existing results either yield suboptimal rates of
$O(1/\sqrt{n})$, or focus on networks with smooth activation functions,
incurring exponential dependence on network depth $L$. In this work, we
establish optimal generalization rates for GD with deep ReLU networks by
carefully trading off optimization and generalization errors, achieving only
polynomial dependence on depth. Specifically, under the assumption that the
data are NTK separable from the margin $\gamma$, we prove an excess risk rate
of $\widetilde{O}(L^4 (1 + \gamma L^2) / (n \gamma^2))$, which aligns with the
optimal SVM-type rate $\widetilde{O}(1 / (n \gamma^2))$ up to depth-dependent
factors. A key technical contribution is our novel control of activation
patterns near a reference model, enabling a sharper Rademacher complexity bound
for deep ReLU networks trained with gradient descent.

</details>


### [313] [OptunaHub: A Platform for Black-Box Optimization](https://arxiv.org/abs/2510.02798)
*Yoshihiko Ozaki,Shuhei Watanabe,Toshihiko Yanase*

Main category: cs.LG

TL;DR: OptunaHub是一个社区平台，旨在整合黑盒优化（BBO）方法和基准测试，以促进跨领域研究。


<details>
  <summary>Details</summary>
Motivation: 黑盒优化（BBO）在 AutoML 和材料信息学等领域推动了进步，但研究工作往往分散在不同领域。

Method: OptunaHub 提供统一的 Python API、贡献者包注册中心和 Web 界面，以促进搜索能力和跨领域研究。

Result: OptunaHub 旨在促进贡献和应用的良性循环。

Conclusion: OptunaHub 是一个社区平台，旨在整合黑盒优化（BBO）方法和基准测试，以促进跨领域研究。

Abstract: Black-box optimization (BBO) drives advances in domains such as AutoML and
Materials Informatics, yet research efforts often remain fragmented across
domains. We introduce OptunaHub (https://hub.optuna.org/), a community platform
that centralizes BBO methods and benchmarks. OptunaHub provides unified Python
APIs, a contributor package registry, and a web interface to promote
searchability and cross-domain research. OptunaHub aims to foster a virtuous
cycle of contributions and applications. The source code is publicly available
in the optunahub, optunahub-registry, and optunahub-web repositories under the
Optuna organization on GitHub (https://github.com/optuna/).

</details>


### [314] [Relevance-Aware Thresholding in Online Conformal Prediction for Time Series](https://arxiv.org/abs/2510.02809)
*Théo Dupuy,Binbin Xu,Stéphane Perrey,Jacky Montmain,Abdelhak Imoussaten*

Main category: cs.LG

TL;DR: 在线共识预测 (OCP) 通过更新阈值来适应数据分布变化，但现有方法在更新阈值时主要关注覆盖有效性，忽略了预测区间与其相关性的量化。


<details>
  <summary>Details</summary>
Motivation: 现有OCP方法在更新阈值时，仅考虑预测区间是否包含真实值（二元评估），而忽略了量化预测区间与真实值的相关性，这可能导致阈值更新过于剧烈，影响区间宽度。

Method: 提出一种增强OCP阈值更新的方法，用能量化预测区间相关性的函数替代原有的二元评估方法，以平滑阈值更新过程。

Result: 实验结果表明，该方法在保持覆盖有效性的同时，能够产生比现有OCP方法更窄的预测区间。

Conclusion: 通过在OCP的阈值更新步骤中引入更广泛的相关性量化函数，可以有效改善预测区间的宽度，同时保证覆盖的有效性。

Abstract: Uncertainty quantification has received considerable interest in recent works
in Machine Learning. In particular, Conformal Prediction (CP) gains ground in
this field. For the case of time series, Online Conformal Prediction (OCP)
becomes an option to address the problem of data distribution shift over time.
Indeed, the idea of OCP is to update a threshold of some quantity (whether the
miscoverage level or the quantile) based on the distribution observation. To
evaluate the performance of OCP methods, two key aspects are typically
considered: the coverage validity and the prediction interval width
minimization. Recently, new OCP methods have emerged, offering long-run
coverage guarantees and producing more informative intervals. However, during
the threshold update step, most of these methods focus solely on the validity
of the prediction intervals~--~that is, whether the ground truth falls inside
or outside the interval~--~without accounting for their relevance. In this
paper, we aim to leverage this overlooked aspect. Specifically, we propose
enhancing the threshold update step by replacing the binary evaluation
(inside/outside) with a broader class of functions that quantify the relevance
of the prediction interval using the ground truth. This approach helps prevent
abrupt threshold changes, potentially resulting in narrower prediction
intervals. Indeed, experimental results on real-world datasets suggest that
these functions can produce tighter intervals compared to existing OCP methods
while maintaining coverage validity.

</details>


### [315] [Dissecting Transformers: A CLEAR Perspective towards Green AI](https://arxiv.org/abs/2510.02810)
*Hemang Jain,Shailender Goyal,Divyansh Pandey,Karthik Vaidhyanathan*

Main category: cs.LG

TL;DR: LLM推理消耗大量能源，现有研究缺乏细粒度测量方法。本文提出CLEAR方法，对Transformer架构的核心组件进行细粒度能耗分析，发现Attention块能耗高，FLOPs不能完全反映能耗。研究为构建节能模型提供了基础。


<details>
  <summary>Details</summary>
Motivation: 现有研究对LLM推理的能耗分析过于粗略，缺乏细粒度测量方法，导致节能研究滞后。

Method: 提出一种名为CLEAR（Component-Level Energy Assessment via Repeated sampling）的新方法，解决微秒级组件执行与毫秒级传感器监控之间的时间不匹配问题，实现对Transformer架构核心组件的细粒度能耗分析。

Result: 对15个不同模型进行了评估，证明CLEAR方法能将组件级能耗方差控制在9.5%以下，并捕捉超过90%的模型总能耗。分析发现Attention块每FLOPs能耗显著更高，表明FLOPs无法完全代表真实能耗。

Conclusion: FLOPs不能完全代表Transformer组件的真实能耗。本文的研究为构建节能模型提供了详细的组件级能耗基线和见解，是组件级优化的初步尝试。

Abstract: The rapid adoption of Large Language Models (LLMs) has raised significant
environmental concerns. Unlike the one-time cost of training, LLM inference
occurs continuously at a global scale and now dominates the AI energy
footprint. Yet, most sustainability studies report only coarse, model-level
metrics due to the lack of fine-grained measurement methods, treating energy
efficiency more as an afterthought than as a primary objective. We present the
first fine-grained empirical analysis of inference energy across core
components of transformer architecture. We propose a novel methodology,
Component-Level Energy Assessment via Repeated sampling (CLEAR), to overcome
temporal mismatch between microsecond scale component execution and monitoring
of millisecond (ms) scale energy sensors. Using CLEAR, we evaluate 15 models
spanning four distinct architecture types and consistently keep component-wise
energy variance below 9.5\% while capturing more than 90\% of the model's total
energy as individual components. Our empirical analysis reveals that Attention
blocks consume significantly more energy per floating-point operation (FLOP),
indicating that energy consumption is not proportionally aligned with FLOP
counts. This shows that FLOPs alone fail to capture the true energy cost at a
component level. Our findings establish detailed component-level energy
baselines and provide insight as an initial step to build energy-efficient
transformer models through component-level optimizations.

</details>


### [316] [Mitigating Spurious Correlation via Distributionally Robust Learning with Hierarchical Ambiguity Sets](https://arxiv.org/abs/2510.02818)
*Sung Ho Jo,Seonghwi Kim,Minwoo Chae*

Main category: cs.LG

TL;DR: 所提出的方法通过分层扩展Group DRO来解决组内和组间分布偏移问题，并在新的基准测试中表现出鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的监督学习方法容易受到分布偏移（尤其是测试数据中的分布偏移）的影响，而现有的Group DRO等鲁棒性方法虽然能应对子群体或组偏移，但对于样本量有限的少数群体中的组内分布偏移则显得脆弱。

Method: 提出了一种Group DRO的分层扩展方法，该方法能够同时处理组内和组间的分布偏移，并在多个层面上实现对分布偏移的鲁棒性。此外，还引入了模拟现实中少数群体分布偏移的新基准测试设置。

Result: 所提出的方法在新基准测试设置中表现出强大的鲁棒性，并且在标准的基准测试中也取得了优越的性能，显著优于现有的鲁棒学习方法。

Conclusion: 研究结果强调了扩大模糊集以更好地捕捉组内和组间分布不确定性的重要性，这对于提高模型的鲁棒性至关重要。

Abstract: Conventional supervised learning methods are often vulnerable to spurious
correlations, particularly under distribution shifts in test data. To address
this issue, several approaches, most notably Group DRO, have been developed.
While these methods are highly robust to subpopulation or group shifts, they
remain vulnerable to intra-group distributional shifts, which frequently occur
in minority groups with limited samples. We propose a hierarchical extension of
Group DRO that addresses both inter-group and intra-group uncertainties,
providing robustness to distribution shifts at multiple levels. We also
introduce new benchmark settings that simulate realistic minority group
distribution shifts-an important yet previously underexplored challenge in
spurious correlation research. Our method demonstrates strong robustness under
these conditions-where existing robust learning methods consistently fail-while
also achieving superior performance on standard benchmarks. These results
highlight the importance of broadening the ambiguity set to better capture both
inter-group and intra-group distributional uncertainties.

</details>


### [317] [FlexiQ: Adaptive Mixed-Precision Quantization for Latency/Accuracy Trade-Offs in Deep Neural Networks](https://arxiv.org/abs/2510.02822)
*Jaemin Kim,Hongjun Um,Sungkyun Kim,Yongjun Park,Jiwon Seo*

Main category: cs.LG

TL;DR: FlexiQ是一种自适应混合精度量化方案，可在不显著牺牲精度的情况下提高计算机视觉模型的推理速度，并能实时适应工作负载变化。


<details>
  <summary>Details</summary>
Motivation: 现有的硬件加速器（如NPU和GPU）成本高昂且难以扩展以应对实时工作负载的波动，而神经网路需要这些加速器来处理其巨大的计算开销。

Method: FlexiQ是一种自适应混合精度量化方案，它能选择性地将低比特计算应用于值范围较小的特征通道，并采用一种高效的比特降低方法来最小化量化误差。此外，FlexiQ还能实时调整其低比特通道的比例，以有效管理工作负载的波动。

Result: 在11个基于卷积和Transformer的视觉模型上进行评估，FlexiQ在4位模型上平均准确率提高了6.6%，优于四种最先进的量化技术。混合精度模型实现了高效的精度-延迟权衡，其中50%的4位模型仅带来0.6%的准确率损失，却实现了100%的4位模型相对于8位模型的40%的加速比。在NPU和GPU上的延迟评估证实FlexiQ的运行时开销极小。

Conclusion: FlexiQ通过其自适应混合精度量化方案，在保持高精度的同时显著提高了推理速度，并能有效应对工作负载的变化，同时在自定义NPU和GPU上展现出硬件效率和整体性能优势。

Abstract: Neural networks commonly execute on hardware accelerators such as NPUs and
GPUs for their size and computation overhead. These accelerators are costly and
it is hard to scale their resources to handle real-time workload fluctuations.
  We present FlexiQ, an adaptive mixed-precision quantization scheme for
computer vision models. FlexiQ selectively applies low-bitwidth computation to
feature channels with small value ranges and employs an efficient bit-lowering
method to minimize quantization errors while maintaining inference accuracy.
Furthermore, FlexiQ adjusts its low-bitwidth channel ratio in real time,
enabling quantized models to effectively manage fluctuating inference workload.
  We implemented FlexiQ prototype, including the mixed-precision inference
runtime on our custom NPU and GPUs. Evaluated on eleven convolution- and
transformer-based vision models, FlexiQ achieves on average 6.6% higher
accuracy for 4-bit models with finetuning and outperforms four state-of-the-art
quantization techniques. Moreover, our mixed-precision models achieved an
efficient accuracy-latency trade-off, with the 50% 4-bit model incurring only
0.6% accuracy loss while achieving 40% of the speedup of the 100% 4-bit model
over 8-bit model. Latency evaluations on our NPU and GPUs confirmed that FlexiQ
introduces minimal runtime overhead, demonstrating its hardware efficiency and
overall performance benefits.

</details>


### [318] [The Curious Case of In-Training Compression of State Space Models](https://arxiv.org/abs/2510.02823)
*Makram Chahine,Philipp Nazari,Daniela Rus,T. Konstantin Rusch*

Main category: cs.LG

TL;DR: 状态空间模型（SSMs）通过在训练过程中进行压缩来提高计算效率和性能。


<details>
  <summary>Details</summary>
Motivation: 平衡SSMs的表达能力和计算负担，同时保留任务关键结构。

Method: 利用状态空间模型（SSMs）的Hankel奇异值分析和特征值稳定性性质，在训练过程中识别并保留高影响力的状态维度，实现系统压缩。

Result: 与直接在较小维度下训练的模型相比，经过压缩的SSMs在优化过程中速度显著加快，同时保留了更多的任务关键信息，从而在计算效率和性能之间取得了更好的平衡。

Conclusion: 在训练过程中进行压缩的SSMs能够以更高的性能实现计算效率。

Abstract: State Space Models (SSMs), developed to tackle long sequence modeling tasks
efficiently, offer both parallelizable training and fast inference. At their
core are recurrent dynamical systems that maintain a hidden state, with update
costs scaling with the state dimension. A key design challenge is striking the
right balance between maximizing expressivity and limiting this computational
burden. Control theory, and more specifically Hankel singular value analysis,
provides a potent framework for the measure of energy for each state, as well
as the balanced truncation of the original system down to a smaller
representation with performance guarantees. Leveraging the eigenvalue stability
properties of Hankel matrices, we apply this lens to SSMs during training,
where only dimensions of high influence are identified and preserved. Our
approach applies to Linear Time-Invariant SSMs such as Linear Recurrent Units,
but is also extendable to selective models. Experiments show that in-training
reduction significantly accelerates optimization while preserving expressivity,
with compressed models retaining task-critical structure lost by models trained
directly at smaller dimension. In other words, SSMs that begin large and shrink
during training achieve computational efficiency while maintaining higher
performance.

</details>


### [319] [Multi-scale Autoregressive Models are Laplacian, Discrete, and Latent Diffusion Models in Disguise](https://arxiv.org/abs/2510.02826)
*Steve Hong,Samuel Belkadi*

Main category: cs.LG

TL;DR: VAR模型可以通过迭代细化框架来理解，它通过前向过程构建潜在的拉普拉斯金字塔，并通过学习到的后向过程进行多步重建。这种视角揭示了VAR的效率和保真度的三个关键设计选择：在学习到的潜在空间中进行细化，将预测视为代码索引上的离散分类，以及按空间频率划分任务。


<details>
  <summary>Details</summary>
Motivation: 将VAR模型视为迭代细化框架，探索其与去噪扩散模型的联系，并解释其效率和保真度的原因。

Method: 将VAR形式化为一个确定性的前向过程，构建拉普拉斯风格的潜在金字塔，并结合一个学习到的后向过程进行重建。

Result: 量化了每个设计选择对保真度和速度的贡献，并展示了该框架在图生成和天气预测方面的扩展性。

Conclusion: VAR模型可以通过迭代细化框架来理解，关键设计选择包括在潜在空间中细化、将预测视为离散分类以及按空间频率划分任务。该框架还可以扩展到其他领域，并与扩散模型生态系统集成。

Abstract: We revisit Visual Autoregressive (VAR) models through the lens of an
iterative-refinement framework. Rather than viewing VAR solely as next-scale
autoregression, we formalise it as a deterministic forward process that
constructs a Laplacian-style latent pyramid, paired with a learned backward
process that reconstructs it in a small number of coarse-to-fine steps. This
view connects VAR to denoising diffusion and isolates three design choices that
help explain its efficiency and fidelity: refining in a learned latent space,
casting prediction as discrete classification over code indices, and
partitioning the task by spatial frequency. We run controlled experiments to
quantify each factor's contribution to fidelity and speed, and we outline how
the same framework extends to permutation-invariant graph generation and to
probabilistic, ensemble-style medium-range weather forecasting. The framework
also suggests practical interfaces for VAR to leverage tools from the diffusion
ecosystem while retaining few-step, scale-parallel generation.

</details>


### [320] [Subject-Adaptive Sparse Linear Models for Interpretable Personalized Health Prediction from Multimodal Lifelog Data](https://arxiv.org/abs/2510.02835)
*Dohyun Bu,Jisoo Han,Soohwa Kwon,Yulim So,Jong-Seok Lee*

Main category: cs.LG

TL;DR: SASL框架通过结合可解释的线性模型和置信度门控的LightGBM模型，在个性化健康预测方面取得了与复杂黑盒模型相当的性能，同时显著减少了参数数量并提高了透明度。


<details>
  <summary>Details</summary>
Motivation: 现有的个性化健康预测模型（如深度神经网络和梯度提升模型）缺乏可解释性，并且未能充分处理个体间差异。

Method: SASL框架结合了普通最小二乘回归和特定于受试者的交互，使用迭代向后特征消除来构建稀疏模型。对于有序目标，采用回归然后阈值的方法来最大化宏平均F1分数。对于难以预测的情况，通过置信度门控选择性地整合LightGBM模型的输出。

Result: SASL-LightGBM混合框架在CH-2025数据集上的表现与复杂的黑盒方法相当，但参数更少，透明度更高。

Conclusion: SASL框架为个性化健康预测提供了一种可解释且有效的解决方案，能够为临床医生和从业者提供清晰、可操作的见解。

Abstract: Improved prediction of personalized health outcomes -- such as sleep quality
and stress -- from multimodal lifelog data could have meaningful clinical and
practical implications. However, state-of-the-art models, primarily deep neural
networks and gradient-boosted ensembles, sacrifice interpretability and fail to
adequately address the significant inter-individual variability inherent in
lifelog data. To overcome these challenges, we propose the Subject-Adaptive
Sparse Linear (SASL) framework, an interpretable modeling approach explicitly
designed for personalized health prediction. SASL integrates ordinary least
squares regression with subject-specific interactions, systematically
distinguishing global from individual-level effects. We employ an iterative
backward feature elimination method based on nested $F$-tests to construct a
sparse and statistically robust model. Additionally, recognizing that health
outcomes often represent discretized versions of continuous processes, we
develop a regression-then-thresholding approach specifically designed to
maximize macro-averaged F1 scores for ordinal targets. For intrinsically
challenging predictions, SASL selectively incorporates outputs from compact
LightGBM models through confidence-based gating, enhancing accuracy without
compromising interpretability. Evaluations conducted on the CH-2025 dataset --
which comprises roughly 450 daily observations from ten subjects -- demonstrate
that the hybrid SASL-LightGBM framework achieves predictive performance
comparable to that of sophisticated black-box methods, but with significantly
fewer parameters and substantially greater transparency, thus providing clear
and actionable insights for clinicians and practitioners.

</details>


### [321] [Knowledge-Aware Modeling with Frequency Adaptive Learning for Battery Health Prognostics](https://arxiv.org/abs/2510.02839)
*Vijay Babu Pamshetti,Wei Zhang,Sumei Sun,Jie Zhang,Yonggang Wen,Qingyu Yan*

Main category: cs.LG

TL;DR: Karma是一个知识感知模型，通过频率自适应学习来预测电池健康状况，在两个主流数据集上分别实现了50.6%和32.6%的误差降低，优于最先进的算法。


<details>
  <summary>Details</summary>
Motivation: 电池健康预测对于确保现代能源系统的安全、效率和可持续性至关重要，但由于电池退化行为复杂，实现准确可靠的预测一直具有挑战性。现有的数据驱动模型虽然能捕捉时间退化特征，但往往缺乏知识指导，导致长期健康预测不可靠。

Method: Karma模型首先进行信号分解以获得不同频带的电池信号。开发了一个双流深度学习架构，一个流捕捉长期低频退化趋势，另一个流模拟高频短期动态。Karma通过知识指导来规范预测，其中电池退化根据经验研究模拟为双指数函数。该双流模型使用粒子滤波器来优化知识参数，以确保物理上一致且可靠的预测和不确定性量化。

Result: Karma模型在两个主流数据集上的电池健康预测方面表现优于最先进的算法，平均误差分别降低了50.6%和32.6%。

Conclusion: Karma模型具有鲁棒性、泛化性，并有潜力应用于各种应用中，实现更安全、更可靠的电池管理。

Abstract: Battery health prognostics are critical for ensuring safety, efficiency, and
sustainability in modern energy systems. However, it has been challenging to
achieve accurate and robust prognostics due to complex battery degradation
behaviors with nonlinearity, noise, capacity regeneration, etc. Existing
data-driven models capture temporal degradation features but often lack
knowledge guidance, which leads to unreliable long-term health prognostics. To
overcome these limitations, we propose Karma, a knowledge-aware model with
frequency-adaptive learning for battery capacity estimation and remaining
useful life prediction. The model first performs signal decomposition to derive
battery signals in different frequency bands. A dual-stream deep learning
architecture is developed, where one stream captures long-term low-frequency
degradation trends and the other models high-frequency short-term dynamics.
Karma regulates the prognostics with knowledge, where battery degradation is
modeled as a double exponential function based on empirical studies. Our
dual-stream model is used to optimize the parameters of the knowledge with
particle filters to ensure physically consistent and reliable prognostics and
uncertainty quantification. Experimental study demonstrates Karma's superior
performance, achieving average error reductions of 50.6% and 32.6% over
state-of-the-art algorithms for battery health prediction on two mainstream
datasets, respectively. These results highlight Karma's robustness,
generalizability, and potential for safer and more reliable battery management
across diverse applications.

</details>


### [322] [RoiRL: Efficient, Self-Supervised Reasoning with Offline Iterative Reinforcement Learning](https://arxiv.org/abs/2510.02892)
*Aleksei Arzhantsev,Otmane Sakhi,Flavian Vasile*

Main category: cs.LG

TL;DR: RoiRL是一种轻量级的离线强化学习方法，可以提高大型语言模型的推理能力，无需标注数据，并且比现有的TTRL方法更快、计算成本更低。


<details>
  <summary>Details</summary>
Motivation: 目前的强化学习（RL）方法在提高大型语言模型（LLM）的推理能力方面至关重要，但通常需要真实奖励。测试时强化学习（TTRL）虽然消除了对真实奖励的依赖，但需要大量的在线RL和高昂的计算成本。

Method: RoiRL（Reasoning with offline iterative Reinforcement Learning）是一种离线学习方法，通过优化加权对数似然目标来实现，无需维护参考模型，从而实现稳定的训练，并显著降低内存和计算要求。

Result: RoiRL的训练速度是TTRL的2.5倍，并且在推理基准测试中始终优于TTRL。

Conclusion: RoiRL为无需标注数据即可实现自我改进的LLM提供了一条可扩展的途径。

Abstract: Reinforcement learning (RL) is central to improving reasoning in large
language models (LLMs) but typically requires ground-truth rewards. Test-Time
Reinforcement Learning (TTRL) removes this need by using majority-vote rewards,
but relies on heavy online RL and incurs substantial computational cost. We
propose RoiRL: Reasoning with offline iterative Reinforcement Learning, a
family of lightweight offline learning alternatives that can target the same
regularized optimal policies. Unlike TTRL, RoiRL eliminates the need to
maintain a reference model and instead optimizes weighted log-likelihood
objectives, enabling stable training with significantly lower memory and
compute requirements. Experimental results show that RoiRL trains to 2.5x
faster and consistently outperforms TTRL on reasoning benchmarks, establishing
a scalable path to self-improving LLMs without labels.

</details>


### [323] [DMark: Order-Agnostic Watermarking for Diffusion Large Language Models](https://arxiv.org/abs/2510.02902)
*Linyu Wu,Linhao Zhong,Wenjie Qu,Yuexin Li,Yue Liu,Shengfang Zhai,Chunhua Shen,Jiaheng Zhang*

Main category: cs.LG

TL;DR: DMark 是首个为扩散大语言模型（dLLM）设计的、可有效检测水印的框架，解决了现有方法在 dLLM 上检测率低的问题。


<details>
  <summary>Details</summary>
Motivation: 现有水印方法在 dLLM 上检测效果不佳，因为 dLLM 的非顺序解码方式破坏了传统水印方法的因果设计。

Method: DMark 提出三种互补策略：预测水印（在上下文不可用时使用模型预测的标记）、双向水印（利用扩散解码特有的前向和后向依赖）以及预测-双向水印（结合两者以最大化检测强度）。

Result: 在多种 dLLM 上进行的实验表明，DMark 在 1% 的误报率下实现了 92.0-99.5% 的检测率，同时保持了文本质量，而现有方法的简单改编仅能达到 49.6-71.2% 的检测率。DMark 还表现出对文本操纵的鲁棒性。

Conclusion: DMark 证明了对非自回归语言模型进行有效水印检测是可行的。

Abstract: Diffusion large language models (dLLMs) offer faster generation than
autoregressive models while maintaining comparable quality, but existing
watermarking methods fail on them due to their non-sequential decoding. Unlike
autoregressive models that generate tokens left-to-right, dLLMs can finalize
tokens in arbitrary order, breaking the causal design underlying traditional
watermarks. We present DMark, the first watermarking framework designed
specifically for dLLMs. DMark introduces three complementary strategies to
restore watermark detectability: predictive watermarking uses model-predicted
tokens when actual context is unavailable; bidirectional watermarking exploits
both forward and backward dependencies unique to diffusion decoding; and
predictive-bidirectional watermarking combines both approaches to maximize
detection strength. Experiments across multiple dLLMs show that DMark achieves
92.0-99.5% detection rates at 1% false positive rate while maintaining text
quality, compared to only 49.6-71.2% for naive adaptations of existing methods.
DMark also demonstrates robustness against text manipulations, establishing
that effective watermarking is feasible for non-autoregressive language models.

</details>


### [324] [Learning Explicit Single-Cell Dynamics Using ODE Representations](https://arxiv.org/abs/2510.02903)
*Jan-Philipp von Bassewitz,Adeel Pervez,Marco Fumero,Matthew Robinson,Theofanis Karaletsos,Francesco Locatello*

Main category: cs.LG

TL;DR: Cell-MNN是一个端到端的神经网络模型，可以对细胞分化动力学进行建模，并学习可解释的基因相互作用。


<details>
  <summary>Details</summary>
Motivation: 现有的细胞分化动力学模型计算成本高，训练过程多阶段，且无法发现明确的基因相互作用。随着单细胞数据集的快速增长，需要更有效、可解释的模型。

Method: 提出了一种名为Cell-MNN的编码器-解码器架构，其潜在表示是控制细胞演化动力学的局部线性ODE。该模型是端到端的，并且明确学习基因相互作用。

Result: Cell-MNN在单细胞基准测试中表现具有竞争力，在扩展到大型数据集和跨数据集联合训练方面优于现有技术，并学习了可解释的基因相互作用，这些相互作用已通过TRRUST数据库进行了验证。

Conclusion: Cell-MNN提供了一种计算效率高且可解释的方法来模拟细胞分化动力学，有望在理解和治疗相关疾病方面取得进展。

Abstract: Modeling the dynamics of cellular differentiation is fundamental to advancing
the understanding and treatment of diseases associated with this process, such
as cancer. With the rapid growth of single-cell datasets, this has also become
a particularly promising and active domain for machine learning. Current
state-of-the-art models, however, rely on computationally expensive optimal
transport preprocessing and multi-stage training, while also not discovering
explicit gene interactions. To address these challenges we propose
Cell-Mechanistic Neural Networks (Cell-MNN), an encoder-decoder architecture
whose latent representation is a locally linearized ODE governing the dynamics
of cellular evolution from stem to tissue cells. Cell-MNN is fully end-to-end
(besides a standard PCA pre-processing) and its ODE representation explicitly
learns biologically consistent and interpretable gene interactions.
Empirically, we show that Cell-MNN achieves competitive performance on
single-cell benchmarks, surpasses state-of-the-art baselines in scaling to
larger datasets and joint training across multiple datasets, while also
learning interpretable gene interactions that we validate against the TRRUST
database of gene interactions.

</details>


### [325] [FeDABoost: Fairness Aware Federated Learning with Adaptive Boosting](https://arxiv.org/abs/2510.02914)
*Tharuka Kasthuri Arachchige,Veselka Boeva,Shahrooz Abghari*

Main category: cs.LG

TL;DR: FeDABoost通过动态提升和自适应梯度聚合来提高非IID设置下的联邦学习性能和公平性。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在解决联邦学习在非独立同分布（non-IID）设置下性能和公平性不佳的问题。

Method: 提出了一种名为FeDABoost的新型联邦学习框架，该框架结合了动态提升机制和自适应梯度聚合策略。聚合方法借鉴了SAMME算法的权重机制，为本地错误率较低的客户端分配更高的权重。同时，FeDABoost通过调整focal loss的聚焦参数来动态提升表现不佳的客户端，从而在本地训练中更关注难分类的样本。

Result: 在MNIST、FEMNIST和CIFAR10三个基准数据集上的实验结果表明，FeDABoost与FedAvg和Ditto相比，在公平性和性能方面均有提升，达到了具有竞争力的水平。

Conclusion: FeDABoost通过其独特的聚合和提升策略，有效改善了非IID设置下的联邦学习性能和公平性。

Abstract: This work focuses on improving the performance and fairness of Federated
Learning (FL) in non IID settings by enhancing model aggregation and boosting
the training of underperforming clients. We propose FeDABoost, a novel FL
framework that integrates a dynamic boosting mechanism and an adaptive gradient
aggregation strategy. Inspired by the weighting mechanism of the Multiclass
AdaBoost (SAMME) algorithm, our aggregation method assigns higher weights to
clients with lower local error rates, thereby promoting more reliable
contributions to the global model. In parallel, FeDABoost dynamically boosts
underperforming clients by adjusting the focal loss focusing parameter,
emphasizing hard to classify examples during local training. We have evaluated
FeDABoost on three benchmark datasets MNIST, FEMNIST, and CIFAR10, and compared
its performance with those of FedAvg and Ditto. The results show that FeDABoost
achieves improved fairness and competitive performance.

</details>


### [326] [RAxSS: Retrieval-Augmented Sparse Sampling for Explainable Variable-Length Medical Time Series Classification](https://arxiv.org/abs/2510.02936)
*Aydin Javadov,Samir Garibov,Tobias Hoesli,Qiyang Sun,Florian von Wangenheim,Joseph Ollier,Björn W. Schuller*

Main category: cs.LG

TL;DR: 本研究将随机稀疏采样框架泛化至检索信息分类，通过结合通道内相似性加权窗口预测并聚合概率空间中的预测，实现可解释的iEEG分类。


<details>
  <summary>Details</summary>
Motivation: 解决医学时间序列分析中数据稀疏、噪声大、记录长度变化大等挑战，并提高模型的可解释性和鲁棒性。

Method: 将随机稀疏采样框架泛化至检索信息分类，通过结合通道内相似性加权窗口预测并聚合概率空间中的预测，实现可解释的iEEG分类。

Result: 在四个医疗中心的iEEG记录中取得了具有竞争力的iEEG分类性能，并提供了明确的证据追踪，增强了可解释性。

Conclusion: 所提出的方法在临床可变长度时间序列分类方面具有可靠性和可解释性潜力。

Abstract: Medical time series analysis is challenging due to data sparsity, noise, and
highly variable recording lengths. Prior work has shown that stochastic sparse
sampling effectively handles variable-length signals, while retrieval-augmented
approaches improve explainability and robustness to noise and weak temporal
correlations. In this study, we generalize the stochastic sparse sampling
framework for retrieval-informed classification. Specifically, we weight window
predictions by within-channel similarity and aggregate them in probability
space, yielding convex series-level scores and an explicit evidence trail for
explainability. Our method achieves competitive iEEG classification performance
and provides practitioners with greater transparency and explainability. We
evaluate our method in iEEG recordings collected in four medical centers,
demonstrating its potential for reliable and explainable clinical
variable-length time series classification.

</details>


### [327] [Ergodic Risk Measures: Towards a Risk-Aware Foundation for Continual Reinforcement Learning](https://arxiv.org/abs/2510.02945)
*Juan Sebastian Rojas,Chi-Guhn Lee*

Main category: cs.LG

TL;DR: Continual RL, traditionally risk-neutral, is now explored through a risk-aware lens using novel ergodic risk measures compatible with continual learning, addressing the limitations of existing risk measure theory.


<details>
  <summary>Details</summary>
Motivation: To extend continual reinforcement learning (RL) beyond risk-neutral decision-making to a risk-aware framework, addressing the limitations of existing theories in the continual setting.

Method: Extend risk measure theory to the continual RL setting by introducing a new class of ergodic risk measures, after showing the incompatibility of current risk measures.

Result: Introduced ergodic risk measures compatible with continual learning and demonstrated their theoretical soundness and intuitive appeal through a case study and empirical results.

Conclusion: Ergodic risk measures provide a theoretically sound and intuitively appealing foundation for risk-aware continual RL, overcoming the limitations of existing risk measures in this setting.

Abstract: Continual reinforcement learning (continual RL) seeks to formalize the
notions of lifelong learning and endless adaptation in RL. In particular, the
aim of continual RL is to develop RL agents that can maintain a careful balance
between retaining useful information and adapting to new situations. To date,
continual RL has been explored almost exclusively through the lens of
risk-neutral decision-making, in which the agent aims to optimize the expected
(or mean) long-run performance. In this work, we present the first formal
theoretical treatment of continual RL through the lens of risk-aware
decision-making, in which the agent aims to optimize a reward-based measure of
long-run performance beyond the mean. In particular, we show that the classical
theory of risk measures, widely used as a theoretical foundation in
non-continual risk-aware RL, is, in its current form, incompatible with the
continual setting. Then, building on this insight, we extend risk measure
theory into the continual setting by introducing a new class of ergodic risk
measures that are compatible with continual learning. Finally, we provide a
case study of risk-aware continual learning, along with empirical results,
which show the intuitive appeal and theoretical soundness of ergodic risk
measures.

</details>


### [328] [ContextFlow: Context-Aware Flow Matching For Trajectory Inference From Spatial Omics Data](https://arxiv.org/abs/2510.02952)
*Santanu Subhash Rathod,Francesco Ceccarelli,Sean B. Holden,Pietro Liò,Xiao Zhang,Jovan Tanevski*

Main category: cs.LG

TL;DR: ContextFlow是一个结合了局部组织结构和配体-受体信号传导的上下文感知流匹配框架，用于从纵向空间解析的组学数据中推断组织动力学，并在多个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 从纵向空间解析的组学数据中推断组织动力学对于理解组织结构和功能的变化至关重要，这在发育、再生、修复、疾病进展和治疗反应等过程中具有根本意义。

Method: ContextFlow是一个上下文感知流匹配框架，它通过整合局部组织组织和配体-受体信号传导模式到一个迁移合理性矩阵中，来指导从空间解析的组学数据中推断组织结构动力学。该矩阵用于规范化最优传输目标，并将这些上下文约束嵌入其中。

Result: 在三个数据集上的评估结果显示，ContextFlow在推断准确性和生物学一致性的多个定量和定性指标上，持续优于最先进的流匹配方法。

Conclusion: ContextFlow是一个通用的框架，能够从纵向、空间解析的组学数据中进行时空动力学的建模，并生成既统计上一致又具有生物学意义的轨迹。

Abstract: Inferring trajectories from longitudinal spatially-resolved omics data is
fundamental to understanding the dynamics of structural and functional tissue
changes in development, regeneration and repair, disease progression, and
response to treatment. We propose ContextFlow, a novel context-aware flow
matching framework that incorporates prior knowledge to guide the inference of
structural tissue dynamics from spatially resolved omics data. Specifically,
ContextFlow integrates local tissue organization and ligand-receptor
communication patterns into a transition plausibility matrix that regularizes
the optimal transport objective. By embedding these contextual constraints,
ContextFlow generates trajectories that are not only statistically consistent
but also biologically meaningful, making it a generalizable framework for
modeling spatiotemporal dynamics from longitudinal, spatially resolved omics
data. Evaluated on three datasets, ContextFlow consistently outperforms
state-of-the-art flow matching methods across multiple quantitative and
qualitative metrics of inference accuracy and biological coherence. Our code is
available at: \href{https://github.com/santanurathod/ContextFlow}{ContextFlow}

</details>


### [329] [Confidence and Dispersity as Signals: Unsupervised Model Evaluation and Ranking](https://arxiv.org/abs/2510.02956)
*Weijian Deng,Weijie Tu,Ibrahim Radwan,Mohammad Abu Alsheikh,Stephen Gould,Liang Zheng*

Main category: cs.LG

TL;DR: 该研究提出了一种统一的框架，用于在无标签测试数据的情况下评估和排名模型。该框架利用模型预测的置信度和离散度这两个内在属性来评估模型的泛化能力。研究结果表明，结合这两种属性的混合指标比单一指标更有效，其中预测矩阵的核范数在各种任务和数据集上表现尤为稳健和准确，即使在中度类别不平衡的情况下也能保持可靠性。


<details>
  <summary>Details</summary>
Motivation: 在缺乏标签测试数据的情况下，评估模型在分布偏移下的泛化能力对于实际部署至关重要。本研究旨在提供一个统一且实用的框架，用于在两种常见的部署场景（数据集中心评估和模型中心评估）下进行无监督模型评估和排名。

Method: 该研究提出利用模型预测的置信度和离散度这两个内在属性来评估模型的泛化能力。研究人员系统地评估了一系列基于置信度、离散度和混合指标的方法，并广泛测试了不同模型架构、数据集和分布偏移类型。

Result: 研究结果表明，混合指标在数据集中心和模型中心评估场景中都持续优于单一指标。具体而言，预测矩阵的核范数在各种任务（包括真实世界数据集）中提供了稳健且准确的性能，并在中度类别不平衡下保持可靠性。

Conclusion: 本研究提出的统一框架，结合模型预测的置信度和离散度，为在部署场景中进行无监督模型评估提供了一个实用且可推广的基础。其中，预测矩阵的核范数是一种特别有效的方法。

Abstract: Assessing model generalization under distribution shift is essential for
real-world deployment, particularly when labeled test data is unavailable. This
paper presents a unified and practical framework for unsupervised model
evaluation and ranking in two common deployment settings: (1) estimating the
accuracy of a fixed model on multiple unlabeled test sets (dataset-centric
evaluation), and (2) ranking a set of candidate models on a single unlabeled
test set (model-centric evaluation). We demonstrate that two intrinsic
properties of model predictions, namely confidence (which reflects prediction
certainty) and dispersity (which captures the diversity of predicted classes),
together provide strong and complementary signals for generalization. We
systematically benchmark a set of confidence-based, dispersity-based, and
hybrid metrics across a wide range of model architectures, datasets, and
distribution shift types. Our results show that hybrid metrics consistently
outperform single-aspect metrics on both dataset-centric and model-centric
evaluation settings. In particular, the nuclear norm of the prediction matrix
provides robust and accurate performance across tasks, including real-world
datasets, and maintains reliability under moderate class imbalance. These
findings offer a practical and generalizable basis for unsupervised model
assessment in deployment scenarios.

</details>


### [330] [From high-frequency sensors to noon reports: Using transfer learning for shaft power prediction in maritime](https://arxiv.org/abs/2510.03003)
*Akriti Sharma,Dogan Altan,Dusica Marijan,Arnbjørn Maressa*

Main category: cs.LG

TL;DR: 该研究提出了一种基于迁移学习的方法来预测船舶轴功率，通过在大量数据上预训练模型，然后在少量数据上进行微调，以提高预测精度并降低数据获取成本。


<details>
  <summary>Details</summary>
Motivation: 随着全球海运量的增长，船舶能耗优化对于降低成本和提高运营效率至关重要。轴功率直接影响燃油消耗，因此准确预测轴功率是优化船舶性能的关键。然而，高质量传感器数据的获取成本高昂且不切实际，因此需要探索替代数据源。

Method: 提出一种基于迁移学习的方法，首先在同一艘船的高频数据上训练模型，然后使用其他船只的低频每日午报数据对模型进行微调。

Result: 与仅使用午报数据训练的模型相比，该方法在姊妹船上的平均绝对百分比误差降低了10.6%，在相似船型上的误差降低了3.6%，在不同船型上的误差降低了5.3%。

Conclusion: 基于迁移学习的方法能够有效地利用不同频率的数据源，提高船舶轴功率预测的准确性，尤其是在数据量有限的情况下，为船舶能耗优化提供了有价值的解决方案。

Abstract: With the growth of global maritime transportation, energy optimization has
become crucial for reducing costs and ensuring operational efficiency. Shaft
power is the mechanical power transmitted from the engine to the shaft and
directly impacts fuel consumption, making its accurate prediction a paramount
step in optimizing vessel performance. Power consumption is highly correlated
with ship parameters such as speed and shaft rotation per minute, as well as
weather and sea conditions. Frequent access to this operational data can
improve prediction accuracy. However, obtaining high-quality sensor data is
often infeasible and costly, making alternative sources such as noon reports a
viable option. In this paper, we propose a transfer learning-based approach for
predicting vessels shaft power, where a model is initially trained on
high-frequency data from a vessel and then fine-tuned with low-frequency daily
noon reports from other vessels. We tested our approach on sister vessels
(identical dimensions and configurations), a similar vessel (slightly larger
with a different engine), and a different vessel (distinct dimensions and
configurations). The experiments showed that the mean absolute percentage error
decreased by 10.6 percent for sister vessels, 3.6 percent for a similar vessel,
and 5.3 percent for a different vessel, compared to the model trained solely on
noon report data.

</details>


### [331] [BrainIB++: Leveraging Graph Neural Networks and Information Bottleneck for Functional Brain Biomarkers in Schizophrenia](https://arxiv.org/abs/2510.03004)
*Tianzheng Hu,Qiang Li,Shu Liu,Vince D. Calhoun,Guido van Wingen,Shujian Yu*

Main category: cs.LG

TL;DR: BrainIB++是一个基于图神经网络和信息瓶颈原理的新型框架，能够自动识别具有高信息量的脑区作为子图，以提高精神分裂症诊断模型的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统的基于机器学习的诊断模型需要大量手动特征工程，容易引入偏差；深度学习模型虽然无需手动干预，但缺乏可解释性，难以获得可靠的脑生物标志物，限制了其临床应用。因此，需要一种能够自动识别信息丰富且可解释的脑生物标志物的诊断模型。

Method: 提出了一种名为BrainIB++的端到端图神经网络框架，该框架应用信息瓶颈（IB）原理，在模型训练过程中自动识别最有信息量的数据驱动脑区作为子图，以实现模型的可解释性。

Result: BrainIB++在三个多队列精神分裂症数据集上与九种已建立的脑网络分类方法进行了性能评估，结果显示该模型具有优越的诊断准确性，并且能够泛化到未见过的数据。此外，该模型识别出的子图与已知的精神分裂症临床生物标志物一致，特别强调了视觉、感觉运动和高级认知功能脑网络中的异常。

Conclusion: BrainIB++通过结合图神经网络和信息瓶颈原理，成功地提高了精神分裂症诊断模型的准确性和可解释性，并且识别出的脑网络异常与临床观察一致，证明了其在现实世界诊断应用中的潜力。

Abstract: The development of diagnostic models is gaining traction in the field of
psychiatric disorders. Recently, machine learning classifiers based on
resting-state functional magnetic resonance imaging (rs-fMRI) have been
developed to identify brain biomarkers that differentiate psychiatric disorders
from healthy controls. However, conventional machine learning-based diagnostic
models often depend on extensive feature engineering, which introduces bias
through manual intervention. While deep learning models are expected to operate
without manual involvement, their lack of interpretability poses significant
challenges in obtaining explainable and reliable brain biomarkers to support
diagnostic decisions, ultimately limiting their clinical applicability. In this
study, we introduce an end-to-end innovative graph neural network framework
named BrainIB++, which applies the information bottleneck (IB) principle to
identify the most informative data-driven brain regions as subgraphs during
model training for interpretation. We evaluate the performance of our model
against nine established brain network classification methods across three
multi-cohort schizophrenia datasets. It consistently demonstrates superior
diagnostic accuracy and exhibits generalizability to unseen data. Furthermore,
the subgraphs identified by our model also correspond with established clinical
biomarkers in schizophrenia, particularly emphasizing abnormalities in the
visual, sensorimotor, and higher cognition brain functional network. This
alignment enhances the model's interpretability and underscores its relevance
for real-world diagnostic applications.

</details>


### [332] [Distributional Inverse Reinforcement Learning](https://arxiv.org/abs/2510.03013)
*Feiyang Wu,Ye Zhao,Anqi Wu*

Main category: cs.LG

TL;DR: 提出一种用于离线逆强化学习（IRL）的分布框架，该框架联合建模回报函数的 umcertainty 和回报的完整分布。该方法通过最小化一阶随机优势（FSD）违反来捕获专家行为中的丰富结构，特别是在学习回报分布方面，并将失真风险度量（DRMs）整合到策略学习中，从而能够恢复回报分布和分布感知的策略。该方法在合成基准、真实世界神经行为数据和 MuJoCo 控制任务上的实证结果表明，该方法恢复了富有表现力的回报表示，并取得了最先进的模仿性能。


<details>
  <summary>Details</summary>
Motivation: 传统的 IRL 方法通常只估计确定性的奖励或匹配预期的回报，无法捕捉专家行为中的丰富结构。本研究旨在提出一种新的 IRL 框架，能够更好地建模回报函数的不确定性以及回报的完整分布，从而实现更精确的行为分析和风险感知模仿学习。

Method: 提出一个分布框架，用于离线逆强化学习（IRL）。该框架联合建模回报函数的不确定性和回报的完整分布。通过最小化一阶随机优势（FSD）违反来整合失真风险度量（DRMs）到策略学习中，从而恢复回报分布和分布感知的策略。

Result: 在合成基准、真实世界神经行为数据和 MuJoCo 控制任务上的实证结果表明，该方法恢复了富有表现力的回报表示，并取得了最先进的模仿性能。

Conclusion: 所提出的分布框架能够有效地进行离线 IRL，不仅可以恢复回报分布，还可以学习分布感知的策略，在行为分析和风险感知模仿学习方面具有优势。实证结果证明了该方法的有效性和优越性。

Abstract: We propose a distributional framework for offline Inverse Reinforcement
Learning (IRL) that jointly models uncertainty over reward functions and full
distributions of returns. Unlike conventional IRL approaches that recover a
deterministic reward estimate or match only expected returns, our method
captures richer structure in expert behavior, particularly in learning the
reward distribution, by minimizing first-order stochastic dominance (FSD)
violations and thus integrating distortion risk measures (DRMs) into policy
learning, enabling the recovery of both reward distributions and
distribution-aware policies. This formulation is well-suited for behavior
analysis and risk-aware imitation learning. Empirical results on synthetic
benchmarks, real-world neurobehavioral data, and MuJoCo control tasks
demonstrate that our method recovers expressive reward representations and
achieves state-of-the-art imitation performance.

</details>


### [333] [Learning Robust Diffusion Models from Imprecise Supervision](https://arxiv.org/abs/2510.03016)
*Dong-Dong Wu,Jiacheng Cui,Wei Wang,Zhiqiang She,Masashi Sugiyama*

Main category: cs.LG

TL;DR: DMIS是一个统一的框架，用于从不精确的监督中训练扩散模型，以解决条件输入中包含不精确信息的问题，从而提高生成质量。


<details>
  <summary>Details</summary>
Motivation: 在条件扩散模型的训练中，由于大规模数据集中存在的标签噪声、歧义或不完整等不精确信息，会导致条件不匹配，从而降低生成质量。本研究旨在解决这一挑战。

Method: DMIS框架源于似然最大化，将目标分解为生成和分类两个组成部分。生成部分对不精确标签分布进行建模，分类部分利用扩散分类器推断类别后验概率，并通过优化的时间步采样策略进一步提高效率。

Result: 在图像生成、弱监督学习和噪声数据集浓缩等多种不精确监督形式的任务上进行的广泛实验表明，DMIS consistently produces high-quality and class-discriminative samples。

Conclusion: DMIS框架能够有效地处理不精确的监督信息，在各种生成任务中生成高质量且具有类别区分度的样本。

Abstract: Conditional diffusion models have achieved remarkable success in various
generative tasks recently, but their training typically relies on large-scale
datasets that inevitably contain imprecise information in conditional inputs.
Such supervision, often stemming from noisy, ambiguous, or incomplete labels,
will cause condition mismatch and degrade generation quality. To address this
challenge, we propose DMIS, a unified framework for training robust Diffusion
Models from Imprecise Supervision, which is the first systematic study within
diffusion models. Our framework is derived from likelihood maximization and
decomposes the objective into generative and classification components: the
generative component models imprecise-label distributions, while the
classification component leverages a diffusion classifier to infer
class-posterior probabilities, with its efficiency further improved by an
optimized timestep sampling strategy. Extensive experiments on diverse forms of
imprecise supervision, covering tasks of image generation, weakly supervised
learning, and noisy dataset condensation demonstrate that DMIS consistently
produces high-quality and class-discriminative samples.

</details>


### [334] [Differentially Private Wasserstein Barycenters](https://arxiv.org/abs/2510.03021)
*Anming Gu,Sasidhar Kunapuli,Mark Bun,Edward Chien,Kristjan Greenewald*

Main category: cs.LG

TL;DR: 本文提出了首个计算 Wasserstein 重心的差分隐私算法，并在各种数据集上进行了实证评估，证明了其在隐私和准确性之间取得了良好的权衡。


<details>
  <summary>Details</summary>
Motivation: 由于 Wasserstein 重心在机器学习、统计学和计算机图形学等领域有广泛应用，而其输入数据（经验分布）通常包含敏感信息，因此需要开发差分隐私（DP）方法来保护数据隐私。

Method: 提出并实现了一种计算差分隐私 Wasserstein 重心的算法。

Result: 在合成数据、MNIST 数据集和大规模美国人口数据集上的实验结果表明，该算法能够生成高质量的私有重心，并实现良好的准确性-隐私权衡。

Conclusion: 本文提出的差分隐私 Wasserstein 重心算法在保证数据隐私的同时，能够生成高质量的重心，并在准确性和隐私性之间取得了有效的权衡。

Abstract: The Wasserstein barycenter is defined as the mean of a set of probability
measures under the optimal transport metric, and has numerous applications
spanning machine learning, statistics, and computer graphics. In practice these
input measures are empirical distributions built from sensitive datasets,
motivating a differentially private (DP) treatment. We present, to our
knowledge, the first algorithms for computing Wasserstein barycenters under
differential privacy. Empirically, on synthetic data, MNIST, and large-scale
U.S. population datasets, our methods produce high-quality private barycenters
with strong accuracy-privacy tradeoffs.

</details>


### [335] [Lightweight Transformer for EEG Classification via Balanced Signed Graph Algorithm Unrolling](https://arxiv.org/abs/2510.03027)
*Junyi Yao,Parham Eftekhar,Gene Cheung,Xujin Chris Liu,Yao Wang,Wei Hu*

Main category: cs.LG

TL;DR: 通过在平衡符号图上进行谱去噪来区分癫痫患者和健康受试者，该方法实现了具有竞争力的分类性能，但参数量大大减少。


<details>
  <summary>Details</summary>
Motivation: 区分癫痫患者和健康受试者，利用EEG信号中的固有反相关性，这些反相关性可以用有限图中的负边来建模。

Method: 构建轻量级、可解释的类Transformer神经网络，通过在平衡符号图上展开谱去重算法。通过Lanczos近似在映射的正图上实现理想低通滤波器，并通过数据学习最优截止频率。通过学习两个不同信号类的后验概率，并评估其重建误差来进行二元分类。

Result: 所提出的方法在EEG信号的二元分类中取得了与代表性深度学习方案相当的分类性能，同时使用的参数量大大减少。

Conclusion: 所提出的基于谱去噪和类Transformer神经网络的方法，能够以更少的参数量有效地从EEG信号中区分癫痫患者和健康受试者。

Abstract: Samples of brain signals collected by EEG sensors have inherent
anti-correlations that are well modeled by negative edges in a finite graph. To
differentiate epilepsy patients from healthy subjects using collected EEG
signals, we build lightweight and interpretable transformer-like neural nets by
unrolling a spectral denoising algorithm for signals on a balanced signed graph
-- graph with no cycles of odd number of negative edges. A balanced signed
graph has well-defined frequencies that map to a corresponding positive graph
via similarity transform of the graph Laplacian matrices. We implement an ideal
low-pass filter efficiently on the mapped positive graph via Lanczos
approximation, where the optimal cutoff frequency is learned from data. Given
that two balanced signed graph denoisers learn posterior probabilities of two
different signal classes during training, we evaluate their reconstruction
errors for binary classification of EEG signals. Experiments show that our
method achieves classification performance comparable to representative deep
learning schemes, while employing dramatically fewer parameters.

</details>


### [336] [CHORD: Customizing Hybrid-precision On-device Model for Sequential Recommendation with Device-cloud Collaboration](https://arxiv.org/abs/2510.03038)
*Tianqi Liu,Kairui Fu,Shengyu Zhang,Wenyan Fan,Zhaocheng Du,Jieming Zhu,Fan Wu,Fei Wu*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: With the advancement of mobile device capabilities, deploying reranking
models directly on devices has become feasible, enabling real-time contextual
recommendations. When migrating models from cloud to devices, resource
heterogeneity inevitably necessitates model compression. Recent quantization
methods show promise for efficient deployment, yet they overlook
device-specific user interests, resulting in compromised recommendation
accuracy. While on-device finetuning captures personalized user preference, it
imposes additional computational burden through local retraining. To address
these challenges, we propose a framework for \underline{\textbf{C}}ustomizing
\underline{\textbf{H}}ybrid-precision \underline{\textbf{O}}n-device model for
sequential \underline{\textbf{R}}ecommendation with
\underline{\textbf{D}}evice-cloud collaboration (\textbf{CHORD}), leveraging
channel-wise mixed-precision quantization to simultaneously achieve
personalization and resource-adaptive deployment. CHORD distributes randomly
initialized models across heterogeneous devices and identifies user-specific
critical parameters through auxiliary hypernetwork modules on the cloud. Our
parameter sensitivity analysis operates across multiple granularities (layer,
filter, and element levels), enabling precise mapping from user profiles to
quantization strategy. Through on-device mixed-precision quantization, CHORD
delivers dynamic model adaptation and accelerated inference without
backpropagation, eliminating costly retraining cycles. We minimize
communication overhead by encoding quantization strategies using only 2 bits
per channel instead of 32-bit weights. Experiments on three real-world datasets
with two popular backbones (SASRec and Caser) demonstrate the accuracy,
efficiency, and adaptivity of CHORD.

</details>


### [337] [Bayesian E(3)-Equivariant Interatomic Potential with Iterative Restratification of Many-body Message Passing](https://arxiv.org/abs/2510.03046)
*Soohaeng Yoo Willow,Tae Hyeon Park,Gi Beom Sim,Sung Wook Moon,Seung Kyu Min,D. ChangMo Yang,Hyun Woo Kim,Juho Lee,Chang Woo Myung*

Main category: cs.LG

TL;DR: 使用贝叶斯E(3)等变机器学习势（MLP）和联合能量-力负对数似然（NLL_JEF）损失函数，解决了当前MLP在不确定性量化方面的不足，在机器学习势不确定性预测、离分布检测、校准和主动学习任务上展现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习势（MLP）在不确定性量化方面存在不足，限制了其在主动学习、校准和离分布检测等方面的应用可靠性。

Method: 开发了具有迭代重分层的贝叶斯E(3)等变MLP，并引入了联合能量-力负对数似然（NLL_JEF）损失函数，对能量和力进行不确定性建模。系统性地评估了多种贝叶斯方法在不确定性预测、离分布检测、校准和主动学习任务上的表现。

Result: NLL_JEF损失函数相比传统损失函数具有更高的准确性。提出的方法在不确定性预测、离分布检测、校准和主动学习任务上表现优于现有方法，特别是通过贝叶斯主动学习（BALD）结合能量和力不确定性量化，优于随机采样和仅基于能量不确定性的采样。

Conclusion: 贝叶斯等变神经网络为开发用于大规模原子模拟的不确定性感知MLP提供了一个强大的框架，能够实现具有竞争力的准确性，并支持不确定性引导下的主动学习、离分布检测和能量/力校准。

Abstract: Machine learning potentials (MLPs) have become essential for large-scale
atomistic simulations, enabling ab initio-level accuracy with computational
efficiency. However, current MLPs struggle with uncertainty quantification,
limiting their reliability for active learning, calibration, and
out-of-distribution (OOD) detection. We address these challenges by developing
Bayesian E(3) equivariant MLPs with iterative restratification of many-body
message passing. Our approach introduces the joint energy-force negative
log-likelihood (NLL$_\text{JEF}$) loss function, which explicitly models
uncertainty in both energies and interatomic forces, yielding superior accuracy
compared to conventional NLL losses. We systematically benchmark multiple
Bayesian approaches, including deep ensembles with mean-variance estimation,
stochastic weight averaging Gaussian, improved variational online Newton, and
laplace approximation by evaluating their performance on uncertainty
prediction, OOD detection, calibration, and active learning tasks. We further
demonstrate that NLL$_\text{JEF}$ facilitates efficient active learning by
quantifying energy and force uncertainties. Using Bayesian active learning by
disagreement (BALD), our framework outperforms random sampling and
energy-uncertainty-based sampling. Our results demonstrate that Bayesian MLPs
achieve competitive accuracy with state-of-the-art models while enabling
uncertainty-guided active learning, OOD detection, and energy/forces
calibration. This work establishes Bayesian equivariant neural networks as a
powerful framework for developing uncertainty-aware MLPs for atomistic
simulations at scale.

</details>


### [338] [ZeroShotOpt: Towards Zero-Shot Pretrained Models for Efficient Black-Box Optimization](https://arxiv.org/abs/2510.03051)
*Jamison Meindl,Yunsheng Tian,Tony Cui,Veronika Thost,Zhang-Wei Hong,Johannes Dürholt,Jie Chen,Wojciech Matusik,Mina Konaković Luković*

Main category: cs.LG

TL;DR: ZeroShotOpt是一个通用的、预训练的连续黑盒优化模型，它利用大规模优化轨迹的离线强化学习，实现了强大的零样本泛化能力，并在样本效率方面与领先的全局优化器相媲美。


<details>
  <summary>Details</summary>
Motivation: 传统的贝叶斯优化（BO）在超参数调整和跨问题泛化方面存在不足，需要更高效、更通用的全局优化方法。

Method: ZeroShotOpt利用离线强化学习，在大规模的优化轨迹上进行预训练。通过生成数百万个具有多样化景观的合成高斯过程函数，模型学会了可转移的优化策略。

Result: ZeroShotOpt在各种未见过的基准测试中实现了强大的零样本泛化能力，其样本效率与包括BO在内的领先全局优化器相当或更优。

Conclusion: ZeroShotOpt提供了一种可重用的基础，能够实现稳健的零样本泛化，并匹配或超越了现有最优方法的样本效率，为未来的优化研究提供了新的方向。

Abstract: Global optimization of expensive, derivative-free black-box functions
requires extreme sample efficiency. While Bayesian optimization (BO) is the
current state-of-the-art, its performance hinges on surrogate and acquisition
function hyper-parameters that are often hand-tuned and fail to generalize
across problem landscapes. We present ZeroShotOpt, a general-purpose,
pretrained model for continuous black-box optimization tasks ranging from 2D to
20D. Our approach leverages offline reinforcement learning on large-scale
optimization trajectories collected from 12 BO variants. To scale pretraining,
we generate millions of synthetic Gaussian process-based functions with diverse
landscapes, enabling the model to learn transferable optimization policies. As
a result, ZeroShotOpt achieves robust zero-shot generalization on a wide array
of unseen benchmarks, matching or surpassing the sample efficiency of leading
global optimizers, including BO, while also offering a reusable foundation for
future extensions and improvements. Our open-source code, dataset, and model
are available at: https://github.com/jamisonmeindl/zeroshotopt

</details>


### [339] [Comparative Analysis of Parameterized Action Actor-Critic Reinforcement Learning Algorithms for Web Search Match Plan Generation](https://arxiv.org/abs/2510.03064)
*Ubayd Bapoo,Clement N Nyirenda*

Main category: cs.LG

TL;DR: PAGAC算法在处理高维参数化动作空间时表现最佳，训练速度更快，回报更高。


<details>
  <summary>Details</summary>
Motivation: 在完全可观察的环境中，针对高维决策任务，评估SAC、GAC和TQC算法在参数化动作空间上的性能。

Method: 通过在Platform-v0和Goal-v0基准测试中实现和比较PAGAC、PASAC和PATQC算法，并使用Microsoft NNI进行超参数优化，来评估不同算法在参数化动作空间上的表现。

Result: PAGAC算法在训练速度和获得的回报方面均优于PASAC和PATQC算法，在Platform-v0和Goal-v0测试中均取得了最好的结果，并具有良好的稳定性和效率。

Conclusion: PAGAC算法是处理高维参数化动作空间任务的理想选择，在效率和性能方面均表现出色。未来研究可探索结合熵正则化和截断方法的混合策略以进一步提升稳定性和泛化能力。

Abstract: This study evaluates the performance of Soft Actor Critic (SAC), Greedy Actor
Critic (GAC), and Truncated Quantile Critics (TQC) in high-dimensional
decision-making tasks using fully observable environments. The focus is on
parametrized action (PA) spaces, eliminating the need for recurrent networks,
with benchmarks Platform-v0 and Goal-v0 testing discrete actions linked to
continuous action-parameter spaces. Hyperparameter optimization was performed
with Microsoft NNI, ensuring reproducibility by modifying the codebase for GAC
and TQC. Results show that Parameterized Action Greedy Actor-Critic (PAGAC)
outperformed other algorithms, achieving the fastest training times and highest
returns across benchmarks, completing 5,000 episodes in 41:24 for the Platform
game and 24:04 for the Robot Soccer Goal game. Its speed and stability provide
clear advantages in complex action spaces. Compared to PASAC and PATQC, PAGAC
demonstrated superior efficiency and reliability, making it ideal for tasks
requiring rapid convergence and robust performance. Future work could explore
hybrid strategies combining entropy-regularization with truncation-based
methods to enhance stability and expand investigations into generalizability.

</details>


### [340] [A Unified Deep Reinforcement Learning Approach for Close Enough Traveling Salesman Problem](https://arxiv.org/abs/2510.03065)
*Mingfeng Fan,Jiaqi Cheng,Yaoxin Wu,Yifeng Zhang,Yibin Yang,Guohua Wu,Guillaume Sartoretti*

Main category: cs.LG

TL;DR: 本篇论文提出了一种名为UD3RL的深度强化学习框架，用于解决近似旅行商问题（CETSP），该问题包含一个基于邻域的访问标准，使得节点在智能体进入其邻域时即被视为已访问。


<details>
  <summary>Details</summary>
Motivation: 现有的深度强化学习方法在处理近似旅行商问题（CETSP）时面临挑战，主要是由于其邻域访问标准。本研究旨在提出一种新的方法来解决这个问题。

Method: 提出了一种新颖的统一双解码器深度强化学习（UD3RL）框架，它将决策过程分为节点选择和路径点确定两个子任务。该框架使用适应性编码器提取特征，然后通过节点解码器和位置解码器处理这两个子任务。引入了k近邻子图交互策略来增强空间推理能力。该框架使用REINFORCE算法进行训练，能够泛化到不同大小和不同邻域半径类型（恒定和随机）的问题。

Result: 实验结果表明，UD3RL在解决方案质量和运行时间方面均优于传统方法。该方法在处理不同问题规模、空间分布和半径范围时表现出强大的泛化能力，并且在动态环境中也表现出鲁棒性。

Conclusion: UD3RL框架能够有效地解决近似旅行商问题（CETSP），并在解决方案质量、运行时间、泛化能力和鲁棒性方面均优于现有方法。

Abstract: In recent years, deep reinforcement learning (DRL) has gained traction for
solving the NP-hard traveling salesman problem (TSP). However, limited
attention has been given to the close-enough TSP (CETSP), primarily due to the
challenge introduced by its neighborhood-based visitation criterion, wherein a
node is considered visited if the agent enters a compact neighborhood around
it. In this work, we formulate a Markov decision process (MDP) for CETSP using
a discretization scheme and propose a novel unified dual-decoder DRL (UD3RL)
framework that separates decision-making into node selection and waypoint
determination. Specifically, an adapted encoder is employed for effective
feature extraction, followed by a node-decoder and a loc-decoder to handle the
two sub-tasks, respectively. A k-nearest neighbors subgraph interaction
strategy is further introduced to enhance spatial reasoning during location
decoding. Furthermore, we customize the REINFORCE algorithm to train UD3RL as a
unified model capable of generalizing across different problem sizes and
varying neighborhood radius types (i.e., constant and random radii).
Experimental results show that UD3RL outperforms conventional methods in both
solution quality and runtime, while exhibiting strong generalization across
problem scales, spatial distributions, and radius ranges, as well as robustness
to dynamic environments.

</details>


### [341] [Bootstrap Learning for Combinatorial Graph Alignment with Sequential GNNs](https://arxiv.org/abs/2510.03086)
*Marc Lelarge*

Main category: cs.LG

TL;DR: 图神经网络在组合优化问题上难以超越传统优化方法，限制了其应用。本文提出了一种新颖的图对齐链式处理方法，通过训练一系列图神经网络来迭代优化节点相似性矩阵，并结合基于节点对的操作来捕捉全局结构信息。实验结果表明，该方法在合成基准测试中准确率提高了3倍以上，并能解决其他方法无法处理的正则图问题，结合传统优化方法后，在图对齐基准测试中性能优于现有最先进的求解器。


<details>
  <summary>Details</summary>
Motivation: 图神经网络在组合优化问题（如图对齐）上的表现不如传统优化方法，因此需要改进图神经网络以解决这类问题。

Method: 本文提出了一种新颖的链式处理方法，用于图对齐问题。该方法训练一系列图神经网络，每个网络在前一个网络的相似性矩阵基础上进行迭代优化。推理时，这种方法利用了节点对齐质量的离散排序信息，产生引导效应。此外，该方法采用基于节点对的操作，而非单个节点，以捕捉图对齐所需的全局结构模式。

Result: 在合成基准测试中，该链式图神经网络的准确率比现有方法高3倍以上，并且能够解决其他所有方法都失败的正则图问题。当与传统优化方法结合使用时，该方法在图对齐基准测试中的表现显著优于最先进的求解器。

Conclusion: 所提出的链式图神经网络方法在图对齐问题上取得了显著的改进，能够处理更具挑战性的实例，并为图神经网络在组合优化问题上的应用开辟了新的途径。

Abstract: Graph neural networks (GNNs) have struggled to outperform traditional
optimization methods on combinatorial problems, limiting their practical
impact. We address this gap by introducing a novel chaining procedure for the
graph alignment problem, a fundamental NP-hard task of finding optimal node
correspondences between unlabeled graphs using only structural information. Our
method trains a sequence of GNNs where each network learns to iteratively
refine similarity matrices produced by previous networks. During inference,
this creates a bootstrap effect: each GNN improves upon partial solutions by
incorporating discrete ranking information about node alignment quality from
prior iterations. We combine this with a powerful architecture that operates on
node pairs rather than individual nodes, capturing global structural patterns
essential for alignment that standard message-passing networks cannot
represent. Extensive experiments on synthetic benchmarks demonstrate
substantial improvements: our chained GNNs achieve over 3x better accuracy than
existing methods on challenging instances, and uniquely solve regular graphs
where all competing approaches fail. When combined with traditional
optimization as post-processing, our method substantially outperforms
state-of-the-art solvers on the graph alignment benchmark.

</details>


### [342] [Distilled Protein Backbone Generation](https://arxiv.org/abs/2510.03095)
*Liyang Xie,Haoran Zhang,Zhendong Wang,Wesley Tansey,Mingyuan Zhou*

Main category: cs.LG

TL;DR: 通过引入分数蒸馏技术，我们开发了一种能够快速生成蛋白质骨架的模型，将采样速度提高了20倍以上，同时保持了与教师模型相当的设计能力、多样性和新颖性。


<details>
  <summary>Details</summary>
Motivation: 当前的基于扩散和流的生成模型在蛋白质骨架生成方面表现出色，但在生成速度方面存在瓶颈，限制了其在蛋白质发现中的应用。现有的分数蒸馏技术在处理蛋白质生成时效果不佳，需要进行改进。

Method: 研究并改编了分数蒸馏（Score identity Distillation, SiD）技术，并结合了多步生成和推理时噪声调制技术，以训练能够进行少步采样的蛋白质骨架生成器。

Result: 经过改进的分数蒸馏模型实现了超过20倍的采样速度提升，同时在设计能力、多样性和新颖性方面与教师模型（Proteina）相当。

Conclusion: 通过分数蒸馏技术和多步生成与推理时噪声调制相结合，我们成功地训练了少步蛋白质骨架生成器，显著降低了推理成本，使得基于扩散的模型能够更好地应用于大规模蛋白质设计和工程领域。

Abstract: Diffusion- and flow-based generative models have recently demonstrated strong
performance in protein backbone generation tasks, offering unprecedented
capabilities for de novo protein design. However, while achieving notable
performance in generation quality, these models are limited by their generating
speed, often requiring hundreds of iterative steps in the reverse-diffusion
process. This computational bottleneck limits their practical utility in
large-scale protein discovery, where thousands to millions of candidate
structures are needed. To address this challenge, we explore the techniques of
score distillation, which has shown great success in reducing the number of
sampling steps in the vision domain while maintaining high generation quality.
However, a straightforward adaptation of these methods results in unacceptably
low designability. Through extensive study, we have identified how to
appropriately adapt Score identity Distillation (SiD), a state-of-the-art score
distillation strategy, to train few-step protein backbone generators which
significantly reduce sampling time, while maintaining comparable performance to
their pretrained teacher model. In particular, multistep generation combined
with inference time noise modulation is key to the success. We demonstrate that
our distilled few-step generators achieve more than a 20-fold improvement in
sampling speed, while achieving similar levels of designability, diversity, and
novelty as the Proteina teacher model. This reduction in inference cost enables
large-scale in silico protein design, thereby bringing diffusion-based models
closer to real-world protein engineering applications.

</details>


### [343] [Adaptive Node Feature Selection For Graph Neural Networks](https://arxiv.org/abs/2510.03096)
*Ali Azizpour,Madeline Navarro,Santiago Segarra*

Main category: cs.LG

TL;DR: 提出一种自适应节点特征选择方法，用于图神经网络（GNN），在训练过程中识别并移除不必要的特征。


<details>
  <summary>Details</summary>
Motivation: 需要一种能够衡量特征对模型输出贡献度的方法，以用于解释决策、降维，甚至通过移除无用变量来提高性能。然而，图结构数据引入的复杂依赖性可能不适用于经典的特征重要性度量。

Method: 提出一种模型无关、任务无关的方法，通过在特征置换时验证性能的变化来在训练期间确定相关特征。该方法基于干预，理论上通过表征GNN性能如何依赖于节点数据和图结构之间的关系来激励。

Result: 不仅在训练结束后返回特征重要性得分，还跟踪特征被逐步删除时相关性的演变。因此，可以监控特征是否被有效删除，并用该技术评估其他指标。

Conclusion: 经验结果验证了该方法对于不同图架构的灵活性，以及其在更具挑战性的图学习环境中的适应性。

Abstract: We propose an adaptive node feature selection approach for graph neural
networks (GNNs) that identifies and removes unnecessary features during
training. The ability to measure how features contribute to model output is key
for interpreting decisions, reducing dimensionality, and even improving
performance by eliminating unhelpful variables. However, graph-structured data
introduces complex dependencies that may not be amenable to classical feature
importance metrics. Inspired by this challenge, we present a model- and
task-agnostic method that determines relevant features during training based on
changes in validation performance upon permuting feature values. We
theoretically motivate our intervention-based approach by characterizing how
GNN performance depends on the relationships between node data and graph
structure. Not only do we return feature importance scores once training
concludes, we also track how relevance evolves as features are successively
dropped. We can therefore monitor if features are eliminated effectively and
also evaluate other metrics with this technique. Our empirical results verify
the flexibility of our approach to different graph architectures as well as its
adaptability to more challenging graph learning settings.

</details>


### [344] [AdaBet: Gradient-free Layer Selection for Efficient Training of Deep Neural Networks](https://arxiv.org/abs/2510.03101)
*Irene Tenison,Soumyajit Chatterjee,Fahim Kawsar,Mohammad Malekzadeh*

Main category: cs.LG

TL;DR: AdaBet通过分析激活空间拓扑特征（Betti数）和仅使用前向传播来选择重要的、可再训练的层，从而在无需标签或梯度的帮助下，在有限的计算和内存资源下实现高效的自适应。


<details>
  <summary>Details</summary>
Motivation: 为了在边缘和移动设备上利用预训练的神经网络，需要在计算和内存资源有限的情况下，有效地适应特定用户的运行时数据分布。然而，在设备上重新训练由于现代神经网络的深度不断增加以及所有层的梯度优化带来的计算开销，仍然不切实际。现有方法通过选择部分层进行再训练来降低成本，但需要标记数据、至少一次全模型反向传播或服务器端元训练，这限制了它们在资源受限设备上的适用性。

Method: AdaBet是一种无梯度的方法，通过分析激活空间的拓扑特征（使用Betti数）并仅进行前向传播来选择重要的层，从而对层进行排名。这使得可以选择具有高学习能力的层进行再训练和自适应，而无需标签或梯度。

Result: 在16个基准模型和数据集对上评估AdaBet，结果显示AdaBet的平均分类准确率比基于梯度的基线方法高出5%，同时平均峰值内存消耗降低了40%。

Conclusion: AdaBet是一种有效的、无梯度的层选择方法，可以在资源受限的设备上进行有效的模型适应，并且优于现有的基于梯度的方法。

Abstract: To utilize pre-trained neural networks on edge and mobile devices, we often
require efficient adaptation to user-specific runtime data distributions while
operating under limited compute and memory resources. On-device retraining with
a target dataset can facilitate such adaptations; however, it remains
impractical due to the increasing depth of modern neural nets, as well as the
computational overhead associated with gradient-based optimization across all
layers. Current approaches reduce training cost by selecting a subset of layers
for retraining, however, they rely on labeled data, at least one full-model
backpropagation, or server-side meta-training; limiting their suitability for
constrained devices. We introduce AdaBet, a gradient-free layer selection
approach to rank important layers by analyzing topological features of their
activation spaces through Betti Numbers and using forward passes alone. AdaBet
allows selecting layers with high learning capacity, which are important for
retraining and adaptation, without requiring labels or gradients. Evaluating
AdaBet on sixteen pairs of benchmark models and datasets, shows AdaBet achieves
an average gain of 5% more classification accuracy over gradient-based
baselines while reducing average peak memory consumption by 40%.

</details>


### [345] [Real Time Headway Predictions in Urban Rail Systems and Implications for Service Control: A Deep Learning Approach](https://arxiv.org/abs/2510.03121)
*Muhammad Usama,Haris Koutsopoulos*

Main category: cs.LG

TL;DR: 本研究提出一种基于ConvLSTM的深度学习框架，用于预测城市地铁线路的列车头班车间距，以优化调度和提高乘客满意度。


<details>
  <summary>Details</summary>
Motivation: Efficient real-time dispatching in urban metro systems is essential for ensuring service reliability, maximizing resource utilization, and improving passenger satisfaction.

Method: This study presents a novel deep learning framework centered on a Convolutional Long Short-Term Memory (ConvLSTM) model designed to predict the complex spatiotemporal propagation of train headways across an entire metro line. By directly incorporating planned terminal headways as a critical input alongside historical headway data, the proposed model accurately forecasts future headway dynamics, effectively capturing both their temporal evolution and spatial dependencies across all stations. We introduce a flexible methodology to simulate diverse dispatcher strategies, ranging from maintaining even headways to implementing custom patterns derived from observed terminal departures.

Result: Evaluated on a large-scale dataset from an urban metro line, the proposed ConvLSTM model demonstrates promising headway predictions, offering actionable insights for real-time decision-making.

Conclusion: This framework provides rail operators with a powerful, computationally efficient tool to optimize dispatching strategies, thereby significantly improving service consistency and passenger satisfaction. In contrast to existing research primarily focused on passenger load predictioning or atypical disruption scenarios, our approach emphasizes proactive operational control.

Abstract: Efficient real-time dispatching in urban metro systems is essential for
ensuring service reliability, maximizing resource utilization, and improving
passenger satisfaction. This study presents a novel deep learning framework
centered on a Convolutional Long Short-Term Memory (ConvLSTM) model designed to
predict the complex spatiotemporal propagation of train headways across an
entire metro line. By directly incorporating planned terminal headways as a
critical input alongside historical headway data, the proposed model accurately
forecasts future headway dynamics, effectively capturing both their temporal
evolution and spatial dependencies across all stations. This capability
empowers dispatchers to evaluate the impact of various terminal headway control
decisions without resorting to computationally intensive simulations. We
introduce a flexible methodology to simulate diverse dispatcher strategies,
ranging from maintaining even headways to implementing custom patterns derived
from observed terminal departures. In contrast to existing research primarily
focused on passenger load predictioning or atypical disruption scenarios, our
approach emphasizes proactive operational control. Evaluated on a large-scale
dataset from an urban metro line, the proposed ConvLSTM model demonstrates
promising headway predictions, offering actionable insights for real-time
decision-making. This framework provides rail operators with a powerful,
computationally efficient tool to optimize dispatching strategies, thereby
significantly improving service consistency and passenger satisfaction.

</details>


### [346] [Signature-Informed Transformer for Asset Allocation](https://arxiv.org/abs/2510.03129)
*Yoontae Hwang,Stefan Zohren*

Main category: cs.LG

TL;DR: SIT框架通过整合路径签名和金融归纳偏置，在资产配置方面优于传统和深度学习方法。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习在资产配置中常遇到的目标不匹配和误差放大问题。

Method: 提出签名感知Transformer（SIT）框架，利用路径签名丰富资产动态的几何表示，并通过签名增强的注意力机制将金融归纳偏置（如超前滞后效应）融入模型，直接优化风险感知金融目标。

Result: 在标普100股指日度数据上，SIT的资产配置表现显著优于基线模型，尤其是在与预测-优化模型相比时。

Conclusion: 在机器学习的风险感知资本配置中，组合感知目标和几何感知归纳偏置至关重要。

Abstract: Robust asset allocation is a key challenge in quantitative finance, where
deep-learning forecasters often fail due to objective mismatch and error
amplification. We introduce the Signature-Informed Transformer (SIT), a novel
framework that learns end-to-end allocation policies by directly optimizing a
risk-aware financial objective. SIT's core innovations include path signatures
for a rich geometric representation of asset dynamics and a signature-augmented
attention mechanism embedding financial inductive biases, like lead-lag
effects, into the model. Evaluated on daily S\&P 100 equity data, SIT
decisively outperforms traditional and deep-learning baselines, especially when
compared to predict-then-optimize models. These results indicate that
portfolio-aware objectives and geometry-aware inductive biases are essential
for risk-aware capital allocation in machine-learning systems. The code is
available at:
https://github.com/Yoontae6719/Signature-Informed-Transformer-For-Asset-Allocation

</details>


### [347] [Enhancing XAI Narratives through Multi-Narrative Refinement and Knowledge Distillation](https://arxiv.org/abs/2510.03134)
*Flavio Giorgi,Matteo Silvestri,Cesare Campagnano,Fabrizio Silvestri,Gabriele Tolomei*

Main category: cs.LG

TL;DR: 本研究提出一个利用语言模型生成反事实解释的叙述性摘要，以提高其可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的反事实解释技术对于非专业人士来说过于复杂和技术化，难以理解。

Method: 提出一个新颖的流水线，利用大型和小型语言模型来组合反事实解释的叙述。采用知识蒸馏技术和改进机制，使小型语言模型能够与大型语言模型相媲美，同时保持强大的推理能力。引入一种评估自然语言叙述的方法，以验证模型响应是否符合事实和反事实的地面真值。

Result: 所提出的流水线提高了学生模型的推理能力和实际性能。

Conclusion: 该研究通过利用语言模型生成反事实解释的叙述性摘要，提高了深度学习模型的可解释性，使其更适用于实际应用。

Abstract: Explainable Artificial Intelligence has become a crucial area of research,
aiming to demystify the decision-making processes of deep learning models.
Among various explainability techniques, counterfactual explanations have been
proven particularly promising, as they offer insights into model behavior by
highlighting minimal changes that would alter a prediction. Despite their
potential, these explanations are often complex and technical, making them
difficult for non-experts to interpret. To address this challenge, we propose a
novel pipeline that leverages Language Models, large and small, to compose
narratives for counterfactual explanations. We employ knowledge distillation
techniques along with a refining mechanism to enable Small Language Models to
perform comparably to their larger counterparts while maintaining robust
reasoning abilities. In addition, we introduce a simple but effective
evaluation method to assess natural language narratives, designed to verify
whether the models' responses are in line with the factual, counterfactual
ground truth. As a result, our proposed pipeline enhances both the reasoning
capabilities and practical performance of student models, making them more
suitable for real-world use cases.

</details>


### [348] [Mixture of Many Zero-Compute Experts: A High-Rate Quantization Theory Perspective](https://arxiv.org/abs/2510.03151)
*Yehuda Dar*

Main category: cs.LG

TL;DR: 本文将高比特量化理论应用于混合专家（MoE）模型以解决回归问题。通过将输入空间划分为多个区域，每个区域由一个简单的专家（常量预测器）负责。当专家数量足够多时，输入空间区域会变得非常小，从而可以分析模型的近似误差。论文分别讨论了一维和多维输入情况下的测试误差及其优化方法，并研究了在给定输入空间划分的情况下，专家参数的学习及其统计学特性。最终，论文从理论和实践上阐述了MoE模型学习中的近似误差与估计误差之间的权衡如何受专家数量的影响。


<details>
  <summary>Details</summary>
Motivation: 本文旨在利用高比特量化理论来分析混合专家（MoE）模型在回归任务中的表现，并提出新的见解。

Method: 本文使用高比特量化理论，将输入空间划分为多个区域，每个区域由一个简单的单参数专家（常量预测器）负责。通过分析当专家数量很大时输入空间区域变小的特性，来研究模型的近似误差。论文分别在一维和多维输入情况下推导了测试误差的表达式，并研究了其最小化方法。此外，还讨论了在给定输入空间划分的情况下，专家参数的学习及其统计学性质。

Result: 研究结果表明，对于一维输入，论文给出了测试误差的精确公式以及最优的划分和专家参数。对于多维输入，论文给出了测试误差的上限，并对其最小化进行了研究。此外，还阐述了专家数量如何影响MoE模型学习中的近似误差和估计误差之间的权衡。

Conclusion: 本文通过结合高比特量化理论和混合专家模型，为回归任务提供了新的理论分析框架。研究结果揭示了专家数量对模型近似误差和估计误差权衡的重要性，并为理解和优化MoE模型提供了理论指导。

Abstract: This paper uses classical high-rate quantization theory to provide new
insights into mixture-of-experts (MoE) models for regression tasks. Our MoE is
defined by a segmentation of the input space to regions, each with a
single-parameter expert that acts as a constant predictor with zero-compute at
inference. Motivated by high-rate quantization theory assumptions, we assume
that the number of experts is sufficiently large to make their input-space
regions very small. This lets us to study the approximation error of our MoE
model class: (i) for one-dimensional inputs, we formulate the test error and
its minimizing segmentation and experts; (ii) for multidimensional inputs, we
formulate an upper bound for the test error and study its minimization.
Moreover, we consider the learning of the expert parameters from a training
dataset, given an input-space segmentation, and formulate their statistical
learning properties. This leads us to theoretically and empirically show how
the tradeoff between approximation and estimation errors in MoE learning
depends on the number of experts.

</details>


### [349] [Calibrated Uncertainty Sampling for Active Learning](https://arxiv.org/abs/2510.03162)
*Ha Manh Bui,Iliana Maifeld-Carucci,Anqi Liu*

Main category: cs.LG

TL;DR: 本文提出了一种新的主动学习（AL）查询策略，通过估计和优先查询校准误差最高的样本，来解决深度神经网络（DNN）中不准确的不确定性估计问题，从而在降低分类器校准误差的同时提高泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于模型不确定性的主动学习方法在面对深度神经网络（DNN）时，由于其不确定性估计通常不够准确（即校准误差高），导致查询效率低下，最终影响分类器的泛化能力和校准误差。

Method: 提出了一种新的查询函数（AF），该函数首先估计校准误差，并优先查询校准误差最高的样本，然后再利用 DNN 的不确定性。具体来说，采用了基于核的校准误差估计方法，并证明了该方法在理论上可以保证未标记数据池和测试数据的校准误差有界。

Result: 在经验评估中，所提出的方法在不同 pool-based AL 场景下，相比于其他查询函数基线，在校准误差和泛化误差方面均表现更优。

Conclusion: 通过估计并优先查询校准误差高的样本，可以有效解决 DNN 中不确定性估计不准确的问题，从而提高主动学习的效率和最终模型的性能，降低校准误差和泛化误差。

Abstract: We study the problem of actively learning a classifier with a low calibration
error. One of the most popular Acquisition Functions (AFs) in pool-based Active
Learning (AL) is querying by the model's uncertainty. However, we recognize
that an uncalibrated uncertainty model on the unlabeled pool may significantly
affect the AF effectiveness, leading to sub-optimal generalization and high
calibration error on unseen data. Deep Neural Networks (DNNs) make it even
worse as the model uncertainty from DNN is usually uncalibrated. Therefore, we
propose a new AF by estimating calibration errors and query samples with the
highest calibration error before leveraging DNN uncertainty. Specifically, we
utilize a kernel calibration error estimator under the covariate shift and
formally show that AL with this AF eventually leads to a bounded calibration
error on the unlabeled pool and unseen test data. Empirically, our proposed
method surpasses other AF baselines by having a lower calibration and
generalization error across pool-based AL settings.

</details>


### [350] [Why Do We Need Warm-up? A Theoretical Perspective](https://arxiv.org/abs/2510.03164)
*Foivos Alimisis,Rustem Islamov,Aurelien Lucchi*

Main category: cs.LG

TL;DR: 学习率预热（在训练开始时增加学习率）在深度学习中很常见，但其理论基础尚不清楚。本研究提出了一个原则性的解释，说明预热为何能改善训练。


<details>
  <summary>Details</summary>
Motivation: 解释学习率预热为何能改善深度学习训练的理论基础。

Method: 使用一种广义的 $(L_0, L_1)$-平滑条件，证明了带有预热的学习率预热策略比固定步长具有更快的收敛速度，并建立了上下复杂度界限。

Result: 证明了在$(L_0, L_1)$-平滑条件下，带有预热的学习率预热策略比固定步长具有更快的收敛速度，并建立了上下复杂度界限。在语言和视觉模型上进行了实验验证。

Conclusion: 证明了学习率预热在$(L_0, L_1)$-平滑条件下可以加速收敛，并通过实验证实了其有效性。

Abstract: Learning rate warm-up - increasing the learning rate at the beginning of
training - has become a ubiquitous heuristic in modern deep learning, yet its
theoretical foundations remain poorly understood. In this work, we provide a
principled explanation for why warm-up improves training. We rely on a
generalization of the $(L_0, L_1)$-smoothness condition, which bounds local
curvature as a linear function of the loss sub-optimality and exhibits
desirable closure properties. We demonstrate both theoretically and empirically
that this condition holds for common neural architectures trained with
mean-squared error and cross-entropy losses. Under this assumption, we prove
that Gradient Descent with a warm-up schedule achieves faster convergence than
with a fixed step-size, establishing upper and lower complexity bounds.
Finally, we validate our theoretical insights through experiments on language
and vision models, confirming the practical benefits of warm-up schedules.

</details>


### [351] [FTTE: Federated Learning on Resource-Constrained Devices](https://arxiv.org/abs/2510.03165)
*Irene Tenison,Anna Murphy,Charles Beauville,Lalana Kagal*

Main category: cs.LG

TL;DR: FTTE是一个新颖的半异步联邦学习框架，通过稀疏参数更新和基于更新年龄和方差的滞后加权聚合，解决了资源受限边缘设备上的联邦学习挑战。


<details>
  <summary>Details</summary>
Motivation: 部署在资源受限的边缘节点上的联邦学习面临内存、能量和通信带宽有限的挑战。传统的同步和异步联邦学习方法在异构、大规模网络中存在拖尾延迟和收敛缓慢的问题。

Method: FTTE采用稀疏参数更新和基于客户端更新的年龄和方差的滞后加权聚合。

Result: 与同步联邦学习（如FedAVG）相比，FTTE的收敛速度提高了81%，设备内存使用量降低了80%，通信负载减少了69%。与半异步联邦学习（如FedBuff）相比，FTTE在挑战性环境下始终能达到相当或更高的目标精度。

Conclusion: FTTE是第一个适用于异构、资源受限的边缘设备上真实世界联邦学习部署的实用且可扩展的解决方案。

Abstract: Federated learning (FL) enables collaborative model training across
distributed devices while preserving data privacy, but deployment on
resource-constrained edge nodes remains challenging due to limited memory,
energy, and communication bandwidth. Traditional synchronous and asynchronous
FL approaches further suffer from straggler induced delays and slow convergence
in heterogeneous, large scale networks. We present FTTE (Federated Tiny
Training Engine),a novel semi-asynchronous FL framework that uniquely employs
sparse parameter updates and a staleness-weighted aggregation based on both age
and variance of client updates. Extensive experiments across diverse models and
data distributions - including up to 500 clients and 90% stragglers -
demonstrate that FTTE not only achieves 81% faster convergence, 80% lower
on-device memory usage, and 69% communication payload reduction than
synchronous FL (eg.FedAVG), but also consistently reaches comparable or higher
target accuracy than semi-asynchronous (eg.FedBuff) in challenging regimes.
These results establish FTTE as the first practical and scalable solution for
real-world FL deployments on heterogeneous and predominantly
resource-constrained edge devices.

</details>


### [352] [Q-Learning with Shift-Aware Upper Confidence Bound in Non-Stationary Reinforcement Learning](https://arxiv.org/abs/2510.03181)
*Ha Manh Bui,Felix Parker,Kimia Ghobadi,Anqi Liu*

Main category: cs.LG

TL;DR: 本研究提出了一种名为密度-QUCB (DQUCB) 的新算法，用于解决非平稳强化学习中的分布偏移问题，该算法通过使用转移密度函数检测分布偏移，并利用其似然性来提高Q学习UCB的探索-利用权衡能力。


<details>
  <summary>Details</summary>
Motivation: 现有的Q学习UCB算法在面对分布偏移时，其策略可能会利用次优奖励，因此需要一种能够感知和适应分布偏移的算法。

Method: 提出密度-QUCB (DQUCB) 算法，利用转移密度函数检测分布偏移，并以此来增强Q学习UCB的探索-利用权衡能力。

Result: 理论上，证明了DQUCB比QUCB具有更好的遗憾界限。实验上，DQUCB在强化学习任务和COVID-19患者医院分配任务中均优于QUCB基线，表现出计算效率和较低的遗憾。

Conclusion: DQUCB是一种有效的感知分布偏移的强化学习算法，通过结合转移密度估计，能够提高学习效率和策略性能。

Abstract: We study the Non-Stationary Reinforcement Learning (RL) under distribution
shifts in both finite-horizon episodic and infinite-horizon discounted Markov
Decision Processes (MDPs). In the finite-horizon case, the transition functions
may suddenly change at a particular episode. In the infinite-horizon setting,
such changes can occur at an arbitrary time step during the agent's interaction
with the environment. While the Q-learning Upper Confidence Bound algorithm
(QUCB) can discover a proper policy during learning, due to the distribution
shifts, this policy can exploit sub-optimal rewards after the shift happens. To
address this issue, we propose Density-QUCB (DQUCB), a shift-aware
Q-learning~UCB algorithm, which uses a transition density function to detect
distribution shifts, then leverages its likelihood to enhance the uncertainty
estimation quality of Q-learning~UCB, resulting in a balance between
exploration and exploitation. Theoretically, we prove that our oracle DQUCB
achieves a better regret guarantee than QUCB. Empirically, our DQUCB enjoys the
computational efficiency of model-free RL and outperforms QUCB baselines by
having a lower regret across RL tasks, as well as a real-world COVID-19 patient
hospital allocation task using a Deep-Q-learning architecture.

</details>


### [353] [PRISM-Physics: Causal DAG-Based Process Evaluation for Physics Reasoning](https://arxiv.org/abs/2510.03185)
*Wanjia Zhao,Qinwei Ma,Jingzhe Shi,Shirley Wu,Jiaqi Han,Yijia Xiao,Si-Yuan Chen,Xiao Luo,Ludwig Schmidt,James Zou*

Main category: cs.LG

TL;DR: PRISM-Physics 是一个用于评估复杂物理推理问题的框架和基准，它使用有向无环图（DAG）来表示解决方案，从而实现精细、可解释和有理论依据的评分。


<details>
  <summary>Details</summary>
Motivation: 现有物理基准主要评估最终答案，无法捕捉推理过程，而基于 LLM 的评分方法存在可靠性和诊断有效性问题。

Method: 引入 PRISM-Physics，一个将解决方案表示为公式有向无环图（DAG）的评估框架，并结合基于规则的符号公式匹配方法进行一致性验证。

Result: PRISM-Physics 的评估结果与人类专家的评分更一致，并揭示了当前 LLM 在物理推理方面存在的不足，同时提供了诊断见解和用于后续训练的丰富信号。

Conclusion: PRISM-Physics 通过结构严谨性、理论保证和符号验证，为推进过程级评估和开发具有更深入科学推理能力的模型奠定了基础。

Abstract: Benchmarks for competition-style reasoning have advanced evaluation in
mathematics and programming, yet physics remains comparatively explored. Most
existing physics benchmarks evaluate only final answers, which fail to capture
reasoning processes, while recent stepwise methods rely on heuristic
LLM-as-judge scoring or restrictive linear assumptions, limiting reliability
and diagnostic validity. We introduce PRISM-Physics, a process-level evaluation
framework and benchmark for complex physics reasoning problems. Solutions are
represented as directed acyclic graphs (DAGs) of formulas, explicitly encoding
causal dependencies among intermediate steps to enable fine-grained,
interpretable, and theoretically grounded scoring. We prove the optimality of
the DAG representation and the corresponding scoring policy. Combining with a
fully rule-based method for symbolic formula equivalence matching that we
developed, we ensure consistent validation across diverse formulations without
heuristic judgments. Results show that our evaluation framework is more aligned
with human experts' scoring. Experiments on state-of-the-art LLMs reveal
persistent reasoning failures in physics, while step-level scoring offers both
diagnostic insight and rich signals for later training. By combining structural
rigor, theoretical guarantees, and symbolic validation, PRISM-Physics provides
a principled foundation for advancing process-level evaluation and guiding the
development of models with deeper scientific reasoning capabilities.

</details>


### [354] [Superposition disentanglement of neural representations reveals hidden alignment](https://arxiv.org/abs/2510.03186)
*André Longon,David Klindt,Meenakshi Khosla*

Main category: cs.LG

TL;DR: 神经科学和人工智能中的表征对齐度量衡会受到神经元叠加的影响，而叠加是指单个神经元参与多个特征的表示。本研究探讨了叠加是否会以不希望的方式与对齐度量衡相互作用，并提出具有不同叠加排列的模型会干扰度量衡，从而导致比预期更低的对齐度。研究人员开发了一种理论来解释严格排列度量衡如何依赖于叠加排列，并通过训练稀疏自编码器（SAEs）来验证这一理论，结果表明当模型的基神经元被稀疏超完备潜在代码替换时，对齐度得分通常会增加。此外，在视觉领域中，DNN到DNN和DNN到大脑的线性回归对齐度也显示出类似的增加。研究结果表明，为了使度量衡能够揭示神经编码之间的真实表征对齐度，必须进行叠加解缠。


<details>
  <summary>Details</summary>
Motivation: 探究神经元叠加是否会以不希望的方式与表征对齐度量衡相互作用，以及这种相互作用如何影响度量衡的准确性。

Method: 1. 提出理论：研究叠加排列如何影响严格排列度量衡。 2. 实验验证：训练稀疏自编码器（SAEs）来解缠玩具模型中的叠加，并观察对齐度得分的变化。 3. 扩展实验：在视觉领域中，研究DNN到DNN和DNN到大脑的线性回归对齐度。

Result: 1. 叠加解缠有助于提高对齐度得分，表明模型的基神经元被稀疏超完备潜在代码替换时，对齐度会增加。 2. 在视觉领域，DNN到DNN和DNN到大脑的线性回归对齐度也显示出类似的增加。 3. 叠加解缠对于度量衡准确揭示神经编码之间的真实表征对齐度是必要的。

Conclusion: 神经元叠加是影响表征对齐度度量衡准确性的一个关键因素。当模型采用不同的叠加排列时，现有的对齐度度量衡可能会低估它们之间的真实相似性。为了准确评估不同模型或大脑之间的表征对齐度，需要进行叠加解缠。

Abstract: The superposition hypothesis states that a single neuron within a population
may participate in the representation of multiple features in order for the
population to represent more features than the number of neurons. In
neuroscience and AI, representational alignment metrics measure the extent to
which different deep neural networks (DNNs) or brains represent similar
information. In this work, we explore a critical question: \textit{does
superposition interact with alignment metrics in any undesirable way?} We
hypothesize that models which represent the same features in \textit{different
superposition arrangements}, i.e., their neurons have different linear
combinations of the features, will interfere with predictive mapping metrics
(semi-matching, soft-matching, linear regression), producing lower alignment
than expected. We first develop a theory for how the strict permutation metrics
are dependent on superposition arrangements. This is tested by training sparse
autoencoders (SAEs) to disentangle superposition in toy models, where alignment
scores are shown to typically increase when a model's base neurons are replaced
with its sparse overcomplete latent codes. We find similar increases for
DNN\(\rightarrow\)DNN and DNN\(\rightarrow\)brain linear regression alignment
in the visual domain. Our results suggest that superposition disentanglement is
necessary for mapping metrics to uncover the true representational alignment
between neural codes.

</details>


### [355] [Estimation of Resistance Training RPE using Inertial Sensors and Electromyography](https://arxiv.org/abs/2510.03197)
*James Thomas,Johan Wahlström*

Main category: cs.LG

TL;DR: 机器学习模型可用于估算阻力训练中的自我感觉劳损度（RPE），其中随机森林分类器表现最佳。


<details>
  <summary>Details</summary>
Motivation: 为了通过个性化反馈和伤害预防来增强阻力训练，需要准确估算自我感觉劳损度（RPE）。

Method: 本研究使用来自可穿戴惯性和肌电图（EMG）传感器的数据，在单臂哑铃弯举期间应用机器学习模型来估算RPE。收集了69组和1000多次重复的定制数据集，并提取统计特征以进行模型训练。

Result: 在评估的模型中，随机森林分类器取得了最高的性能，精确准确率为41.4%，RPE准确率为85.9%±1。EMG数据的使用比单独使用惯性传感器略微提高了模型准确性，但其效用可能受到数据质量和放置敏感性等因素的限制。特征分析显示，离心重复时间是RPE最强的预测因子。

Conclusion: 研究结果表明，基于可穿戴传感器估算RPE是可行的，并确定了提高模型泛化能力的关键挑战。

Abstract: Accurate estimation of rating of perceived exertion (RPE) can enhance
resistance training through personalized feedback and injury prevention. This
study investigates the application of machine learning models to estimate RPE
during single-arm dumbbell bicep curls, using data from wearable inertial and
electromyography (EMG) sensors. A custom dataset of 69 sets and over 1000
repetitions was collected, with statistical features extracted for model
training. Among the models evaluated, a random forest classifier achieved the
highest performance, with 41.4% exact accuracy and 85.9% $\pm1$ RPE accuracy.
While the inclusion of EMG data slightly improved model accuracy over inertial
sensors alone, its utility may have been limited by factors such as data
quality and placement sensitivity. Feature analysis highlighted eccentric
repetition time as the strongest RPE predictor. The results demonstrate the
feasibility of wearable-sensor-based RPE estimation and identify key challenges
for improving model generalizability.

</details>


### [356] [Best-of-Majority: Minimax-Optimal Strategy for Pass@$k$ Inference Scaling](https://arxiv.org/abs/2510.03199)
*Qiwei Di,Kaixuan Ji,Xuheng Li,Heyang Zhao,Quanquan Gu*

Main category: cs.LG

TL;DR: LLM推理中的Pass@k设置下，提出了一种名为Best-of-Majority (BoM)的新推理策略，该策略结合了多数投票和BoN的优点，并证明了其在理论上的最优性和在实践中的优越性。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM推理方法，如多数投票和Best-of-N (BoN)，在处理困难任务时，其单次选择的性能会受到限制，而Pass@k的评估方式表明，多次采样和选择最佳结果的策略更有效。因此，需要研究更通用的Pass@k推理设置下的推理扩展性。

Method: 提出了一种名为Best-of-Majority (BoM)的新推理策略。该策略的关键步骤是限制候选集为N个样本中出现频率较高的响应，然后再从中选择奖励最高的前k个。理论上证明了当采样预算N满足一定条件时，BoM的后悔值为O(ε_opt + sqrt(ε_RM^2*C^*/k))，并建立了匹配的下界，证明了算法的minimax最优性。

Result: BoM策略被证明是minimax最优的，并且与多数投票和BoN不同，其性能不会随着采样预算N的增加而下降。在数学问题推理的实验中，BoM的表现优于多数投票和BoN。

Conclusion: BoM是一种新颖且有效的LLM推理策略，它在Pass@k设置下实现了理论最优，并且在实践中能够超越现有的方法，尤其是在增加采样预算时性能更稳定。

Abstract: LLM inference often generates a batch of candidates for a prompt and selects
one via strategies like majority voting or Best-of- N (BoN). For difficult
tasks, this single-shot selection often underperforms. Consequently,
evaluations commonly report Pass@$k$: the agent may submit up to $k$ responses,
and only the best of them is used when computing regret. Motivated by this, we
study inference scaling in the more general Pass@$k$ inference setting, and
prove that neither majority voting nor BoN exhibits the desirable scaling with
$k$ and the sampling budget $N$. Combining the advantages of majority voting
and BoN, we propose a new inference strategy called Best-of-Majority (BoM),
with a pivotal step that restricts the candidates to the responses with high
frequency in the $N$ samples before selecting the top-$k$ rewards. We prove
that when the sampling budget is $N=\tilde\Omega(C^*)$, the regret of BoM is
$O(\epsilon_{\mathrm{opt}}+\sqrt{\epsilon_{\mathrm{RM}}^2C^*/k})$, where $C^*$
is the coverage coefficient, $\epsilon_{\mathrm{RM}}$ is the estimation error
of the reward model, and $\epsilon_{\mathrm{opt}}$ is the estimation error of
reward at the optimal response. We further establish a matching lower bound,
certifying that our algorithm is minimax optimal. Beyond optimality, BoM has a
key advantage: unlike majority voting and BoN, its performance does not degrade
when increasing $N$. Experimental results of inference on math problems show
BoM outperforming both majority voting and BoN.

</details>


### [357] [To Distill or Decide? Understanding the Algorithmic Trade-off in Partially Observable Reinforcement Learning](https://arxiv.org/abs/2510.03207)
*Yuda Song,Dhruv Rohatgi,Aarti Singh,J. Andrew Bagnell*

Main category: cs.LG

TL;DR: 该论文研究了在强化学习（RL）中，利用特权专家蒸馏（privileged expert distillation）来解决部分可观察性问题，并将其与标准的RL方法进行了比较。


<details>
  <summary>Details</summary>
Motivation: 在强化学习（RL）中，部分可观察性是一个长期存在的挑战，需要学习复杂的、依赖于历史的策略。特权专家蒸馏利用训练期间潜在状态信息（例如来自模拟器）来学习和模仿最优的潜在、马尔可夫策略，从而将“学会观察”与“学会行动”分离开来。

Method: 通过一个名为“受扰块马尔可夫决策过程”（perturbed Block MDP）的简单但有启发性的理论模型，以及在具有挑战性的模拟运动任务上进行的受控实验，研究了特权专家蒸馏与标准RL（无特权信息）之间的算法权衡。

Result: 研究发现，（1）该权衡在经验上取决于潜在动态的随机性，这与受扰块马尔可夫决策过程中近似可解码性与信念收缩的对比在理论上预测一致；（2）最优潜在策略并非总是要蒸馏的最佳潜在策略。

Conclusion: 研究结果为有效利用特权信息提供了新的指导方针，有望提高在许多实际部分可观察领域中策略学习的效率。

Abstract: Partial observability is a notorious challenge in reinforcement learning
(RL), due to the need to learn complex, history-dependent policies. Recent
empirical successes have used privileged expert distillation--which leverages
availability of latent state information during training (e.g., from a
simulator) to learn and imitate the optimal latent, Markovian policy--to
disentangle the task of "learning to see" from "learning to act". While expert
distillation is more computationally efficient than RL without latent state
information, it also has well-documented failure modes. In this paper--through
a simple but instructive theoretical model called the perturbed Block MDP, and
controlled experiments on challenging simulated locomotion tasks--we
investigate the algorithmic trade-off between privileged expert distillation
and standard RL without privileged information. Our main findings are: (1) The
trade-off empirically hinges on the stochasticity of the latent dynamics, as
theoretically predicted by contrasting approximate decodability with belief
contraction in the perturbed Block MDP; and (2) The optimal latent policy is
not always the best latent policy to distill. Our results suggest new
guidelines for effectively exploiting privileged information, potentially
advancing the efficiency of policy learning across many practical partially
observable domains.

</details>


### [358] [Low-probability Tokens Sustain Exploration in Reinforcement Learning with Verifiable Reward](https://arxiv.org/abs/2510.03222)
*Guanhua Huang,Tingqiang Xu,Mingze Wang,Qi Yi,Xue Gong,Siheng Li,Ruibin Xiong,Kejiao Li,Yuhao Jiang,Bo Zhou*

Main category: cs.LG

TL;DR: RLVR的训练瓶颈在于策略熵崩溃导致性能停滞。本文提出低概率正则化（Lp-Reg）通过正则化策略以放大“推理火花”（有价值的低概率探索性token），以解决这一问题，并在数学基准测试中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: RLVR在复杂推理中推动了LLM的发展，但其扩展性受到训练瓶颈的限制，即随着策略熵的崩溃，性能会停滞不前，这表明探索能力的丧失。以往的方法通常通过维持高策略熵来解决这个问题，但对有意义的探索的精确机制仍未得到充分研究。本文旨在解决RLVR中的探索动态问题，特别是低概率探索性token（“推理火花”）在训练过程中被系统性地消除的问题。

Method: 提出低概率正则化（Lp-Reg）方法。该方法通过将策略正则化到一个启发式代理分布来实现。该代理分布通过过滤掉假定的噪声token并重新归一化剩余候选token的分布来构建。这产生了一个噪声较小的代理分布，其中“推理火花”的概率被放大，并作为软正则化目标，通过KL散度来防止这些有价值的token被消除。

Result: Lp-Reg能够实现约1000步的稳定性策略训练，而基线熵控制方法在此过程中会崩溃。这种持续的探索带来了最先进的性能，在五个数学基准测试中平均准确率达到60.17%，比以前的方法提高了2.66%。

Conclusion: Lp-Reg通过放大“推理火花”来解决RLVR中的探索退化问题，实现了更稳定的训练和最先进的性能。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has propelled Large
Language Models in complex reasoning, yet its scalability is often hindered by
a training bottleneck where performance plateaus as policy entropy collapses,
signaling a loss of exploration. Previous methods typically address this by
maintaining high policy entropy, yet the precise mechanisms that govern
meaningful exploration have remained underexplored. Our analysis suggests that
an unselective focus on entropy risks amplifying irrelevant tokens and
destabilizing training. This paper investigates the exploration dynamics within
RLVR and identifies a key issue: the gradual elimination of valuable
low-probability exploratory tokens, which we term \textbf{\textit{reasoning
sparks}}. We find that while abundant in pre-trained models, these sparks are
systematically extinguished during RLVR due to over-penalization, leading to a
degeneracy in exploration. To address this, we introduce Low-probability
Regularization (Lp-Reg). Its core mechanism regularizes the policy towards a
heuristic proxy distribution. This proxy is constructed by filtering out
presumed noise tokens and re-normalizing the distribution over the remaining
candidates. The result is a less-noisy proxy where the probability of
\textit{reasoning sparks} is amplified, which then serves as a soft
regularization target to shield these valuable tokens from elimination via KL
divergence. Experiments show that Lp-Reg enables stable on-policy training for
around 1,000 steps, a regime where baseline entropy-control methods collapse.
This sustained exploration leads to state-of-the-art performance, achieving a
$60.17\%$ average accuracy on five math benchmarks, an improvement of $2.66\%$
over prior methods. Code is available at https://github.com/CarlanLark/Lp-Reg.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [359] [Even Faster Kernel Matrix Linear Algebra via Density Estimation](https://arxiv.org/abs/2510.02540)
*Rikhav Shah,Sandeep Silwal,Haike Xu*

Main category: cs.DS

TL;DR: 本文研究了在核矩阵的线性代数任务中使用核密度估计（KDE）。


<details>
  <summary>Details</summary>
Motivation: 现有算法在计算核矩阵的矩阵向量乘积、矩阵乘积、谱范数和所有条目之和时，对于n的依赖性过高，并且对于ε的依赖性也需要改进。

Method: 使用核密度估计（KDE）来处理核矩阵的线性代数任务。

Result: 在相对误差(1+ε)范围内，改进了矩阵向量乘积、矩阵乘积、谱范数和所有条目之和的计算算法。这些算法的运行时间取决于维度d、点数n和目标误差ε。与读取单个条目相比，通过KDE查询访问核矩阵时，n的依赖性大大降低。

Conclusion: 本文提出的算法在ε上的依赖性得到降低，并且在计算核矩阵所有条目之和时，n的依赖性也得到降低。此外，本文还提供了一些相关问题的下界，这些下界提供了条件二次时间硬度结果，并暗示了基于KDE的方法在所研究问题上的局限性。

Abstract: This paper studies the use of kernel density estimation (KDE) for linear
algebraic tasks involving the kernel matrix of a collection of $n$ data points
in $\mathbb R^d$. In particular, we improve upon existing algorithms for
computing the following up to $(1+\varepsilon)$ relative error: matrix-vector
products, matrix-matrix products, the spectral norm, and sum of all entries.
The runtimes of our algorithms depend on the dimension $d$, the number of
points $n$, and the target error $\varepsilon$. Importantly, the dependence on
$n$ in each case is far lower when accessing the kernel matrix through KDE
queries as opposed to reading individual entries.
  Our improvements over existing best algorithms (particularly those of
Backurs, Indyk, Musco, and Wagner '21) for these tasks reduce the polynomial
dependence on $\varepsilon$, and additionally decreases the dependence on $n$
in the case of computing the sum of all entries of the kernel matrix.
  We complement our upper bounds with several lower bounds for related
problems, which provide (conditional) quadratic time hardness results and
additionally hint at the limits of KDE based approaches for the problems we
study.

</details>


### [360] [Near-Optimal Fault-Tolerant Strong Connectivity Preservers](https://arxiv.org/abs/2510.02562)
*Gary Hoppenworth,Thatchaphol Saranurak,Benyu Wang*

Main category: cs.DS

TL;DR: 该论文研究了有向图的k容错连通性保持器和k连通性保持器问题，并取得了接近最优的界限。


<details>
  <summary>Details</summary>
Motivation: 在有向图中，k容错连通性保持器的最优界限与现有上界之间存在较大差距。

Method: 论文提出了一种新的k容错连通性保持器构造，其边数为O(k4^k n log n)。同时，也提供了一种构造方法，边数为O(8^k n log^{5/2} n)，构造时间为多项式时间。

Result: 论文证明了存在边数为O(k4^k n log n)的有向图k容错连通性保持器，并提出了一种构造方法，边数为O(8^k n log^{5/2} n)。这两种构造都显著优于之前的上界，接近最优界限。

Conclusion: 该论文在有向图的k容错连通性保持器和k连通性保持器问题上取得了重要的进展，将现有上界大幅提升，接近最优界限。此外，论文还展示了k连通性保持器的构造不一定需要指数级的边数。

Abstract: A $k$-fault-tolerant connectivity preserver of a directed $n$-vertex graph
$G$ is a subgraph $H$ such that, for any edge set $F \subseteq E(G)$ of size
$|F| \le k$, the strongly connected components of $G - F$ and $H - F$ are the
same. While some graphs require a preserver with $\Omega(2^{k}n)$ edges
[BCR18], the best-known upper bound is $\tilde{O}(k2^{k}n^{2-1/k})$ edges
[CC20], leaving a significant gap of $\Omega(n^{1-1/k})$. In contrast, there is
no gap in undirected graphs; the optimal bound of $\Theta(kn)$ has been
well-established since the 90s [NI92].
  We nearly close the gap for directed graphs; we prove that there exists a
$k$-fault-tolerant connectivity preserver with $O(k4^{k}n\log n)$ edges, and we
can construct one with $O(8^{k}n\log^{5/2}n)$ edges in $\text{poly}(2^{k}n)$
time.
  Our results also improve the state-of-the-art for a closely related object; a
\textit{$k$-connectivity preserver} of $G$ is a subgraph $H$ where, for all $i
\le k$, the strongly $i$-connected components of $G$ and $H$ agree. By a known
reduction, we obtain a $k$-connectivity preserver with $O(k4^{k}n\log n)$
edges, improving the previous best bound of $\tilde{O}(k2^{k}n^{2-1/(k-1)})$
[CC20]. Therefore, for any constant $k$, our results are optimal to a $\log n$
factor for both problems.
  Lastly, we show that the exponential dependency on $k$ is not inherent for
$k$-connectivity preservers by presenting another construction with $O(n
\sqrt{kn})$ edges.

</details>


### [361] [Congestion bounds via Laplacian eigenvalues and their application to tensor networks with arbitrary geometry](https://arxiv.org/abs/2510.02725)
*Sayan Mukherjee,Shinichiro Akiyama*

Main category: cs.DS

TL;DR: 将任意图的顶点嵌入树中以最小化重叠度是一个重要问题，在计算机科学和物理学中有应用。本文研究将一个n顶点图G的双射嵌入到一个n叶二叉树B的叶子中。这种嵌入的拥塞度由删除B的任意顶点所产生的两个分量诱导的割的最大尺寸给出。拥塞度cng(G)定义为任何嵌入所获得的最小拥塞度。我们证明了λ2(G)·2n/9≤cng(G)≤λn(G)·2n/9，其中0=λ1(G)≤…≤λn(G)是G的拉普拉斯特征值。我们还提供了一个收缩启发式，通过分层谱聚类原始图，我们在数值上发现它在为稀疏图寻找低拥塞嵌入方面是有效的。我们在不同结构的图族（超立方体和格）、随机图以及量子电路的张量网络表示上，对我们的拥塞度界限进行了数值比较。我们的结果暗示了张量网络收缩的内存复杂度的下界和上界，以底层图为参数。


<details>
  <summary>Details</summary>
Motivation: 嵌入图的顶点到树中并最小化重叠度，在计算机科学和物理学中有实际应用。

Method: 研究将n顶点图G的双射嵌入到一个n叶二叉树B的叶子中，并定义了拥塞度。推导了拥塞度的上下界，并提出了一种基于分层谱聚类的收缩启发式算法。

Result: 在数值上，该启发式算法在为稀疏图寻找低拥塞嵌入方面被证明是有效的。在对不同图族（包括超立方体、格、随机图和量子电路的张量网络表示）的拥塞度界限进行数值比较后，得到了对张量网络收缩内存复杂度的下界和上界的推论。

Conclusion: 本文为图嵌入问题提供了理论界限和有效的启发式算法，并在张量网络收缩方面有实际意义。

Abstract: Embedding the vertices of arbitrary graphs into trees while minimizing some
measure of overlap is an important problem with applications in computer
science and physics. In this work, we consider the problem of bijectively
embedding the vertices of an $n$-vertex graph $G$ into the leaves of an
$n$-leaf rooted binary tree $\mathcal{B}$. The congestion of such an embedding
is given by the largest size of the cut induced by the two components obtained
by deleting any vertex of $\mathcal{B}$. The congestion $\mathrm{cng}(G)$ is
defined as the minimum congestion obtained by any embedding. We show that
$\lambda_2(G)\cdot 2n/9\le \mathrm{cng} (G)\le \lambda_n(G)\cdot 2n/9$, where
$0=\lambda_1(G)\le \cdots \le \lambda_n(G)$ are the Laplacian eigenvalues of
$G$. We also provide a contraction heuristic given by hierarchically spectral
clustering the original graph, which we numerically find to be effective in
finding low congestion embeddings for sparse graphs. We numerically compare our
congestion bounds on different families of graphs with regular structure
(hypercubes and lattices), random graphs, and tensor network representations of
quantum circuits. Our results imply lower and upper bounds on the memory
complexity of tensor network contraction in terms of the underlying graph.

</details>


### [362] [On the Enumeration of all Unique Paths of Recombining Trinomial Trees](https://arxiv.org/abs/2510.02727)
*Ethan Torres,Ramavarapu Sreenivas,Richard Sowers*

Main category: cs.DS

TL;DR: Recombining trinomial trees are used in option pricing, logistics, and control. An exponential number of trajectories exist, but symmetries allow for a mass-shifting algorithm that generates one representative per path-equivalence class, reducing the search space exponentially. An upper bound on the counting expression provides a theoretical lower bound on algorithmic cost, and empirical tests confirm this with substantial speedups over classical methods. Connections to Motzkin paths suggest further refinements.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of modeling discrete-event systems with recombining trinomial trees, where the naive approach of enumerating all trajectories is computationally infeasible due to the exponential growth of states with tree depth.

Method: The paper introduces a mass-shifting enumeration algorithm that leverages translational invariance and a bijection between paths and weak compositions in time-homogeneous trinomial trees. This algorithm generates a single representative for each path-equivalence class and implicitly counts weak compositions, significantly reducing the search space. Additionally, an upper bound on the combinatorial counting expression is derived to establish a theoretical lower bound on the algorithmic cost.

Result: The mass-shifting algorithm trims the search space by an exponential factor, enabling deeper trees and tighter numerical approximations. The derived upper bound yields a theoretical lower bound on algorithmic cost of approximately O(D^1/2 * 1.612^D). Empirical tests, with provided pseudo-code, corroborate this bound, demonstrating a small constant overhead and significant speedups compared to classical breadth-first traversal.

Conclusion: The study presents an efficient mass-shifting enumeration algorithm for recombining trinomial trees, significantly improving computational feasibility for deeper trees. The theoretical cost bound is established and empirically validated. Furthermore, the paper highlights structural links to Motzkin paths, suggesting avenues for refined enumerative formulas and new analytical tools for path-dependent functionals.

Abstract: Recombining trinomial trees are a workhorse for modeling discrete-event
systems in option pricing, logistics, and feedback control. Because each node
stores a state-dependent quantity, a depth-$D$ tree naively yields
$\mathcal{O}(3^{D})$ trajectories, making exhaustive enumeration infeasible.
Under time-homogeneous dynamics, however, the graph exhibits two exploitable
symmetries: (i) translational invariance of nodes and (ii) a canonical
bijection between admissible paths and ordered tuples encoding weak
compositions. Leveraging these, we introduce a mass-shifting enumeration
algorithm that slides integer "masses" through a cardinality tuple to generate
exactly one representative per path-equivalence class while implicitly counting
the associated weak compositions. This trims the search space by an exponential
factor, enabling markedly deeper trees -- and therefore tighter numerical
approximations of the underlying evolution -- to be processed in practice. We
further derive an upper bound on the combinatorial counting expression that
induces a theoretical lower bound on the algorithmic cost of approximately
$\mathcal{O}\bigl(D^{1/2}1.612^{D}\bigr)$. This correspondence permits direct
benchmarking while empirical tests, whose pseudo-code we provide, corroborate
the bound, showing only a small constant overhead and substantial speedups over
classical breadth-first traversal. Finally, we highlight structural links
between our algorithmic/combinatorial framework and Motzkin paths with
Narayana-type refinements, suggesting refined enumerative formulas and new
potential analytic tools for path-dependent functionals.

</details>


### [363] [Pareto-optimal Non-uniform Language Generation](https://arxiv.org/abs/2510.02795)
*Moses Charikar,Chirag Pabbaraju*

Main category: cs.DS

TL;DR: 本文研究了语言生成模型中的帕累托最优性问题，提出了一种几乎帕累托最优的生成算法，该算法在噪声和代表性生成等实际场景下也具有适用性。


<details>
  <summary>Details</summary>
Motivation: 研究在语言生成模型中，对于不同语言的生成时间能否同时达到最优，即帕累托最优性。

Method: 提出了一种新的算法框架，该算法在某些语言上的生成时间比现有算法更优，同时保证了在其他语言上的生成时间不会显著变差，实现了（近乎）帕累托最优。

Result: 所提出的算法在理论上实现了（近乎）帕累托最优的语言生成，即无法在所有语言上同时改进生成时间。该算法框架还可以应用于噪声和代表性生成等实际场景。

Conclusion: 本文提出的算法在语言生成模型中实现了（近乎）帕累托最优，为该领域的研究提供了新的方向和理论保障。

Abstract: Kleinberg and Mullainathan (2024) recently proposed an interesting model for
language generation in the limit: Given a countable collection of languages,
and an adversary enumerating the strings of some language $L$ from the
collection, the objective is to generate new strings from the target language,
such that all strings generated beyond some finite time are valid. Li, Raman
and Tewari (2024) and Charikar and Pabbaraju (2024) showed strong non-uniform
generation guarantees in this model, giving algorithms that generate new valid
strings from $L$ after seeing a number of distinct input strings $t(L)$ that
depends only on $L$ (and the collection), but not the enumeration order.
However, for both these works, the language-wise generation times $t(L)$ of the
algorithm can be strictly sub-optimal.
  In this work, we study Pareto-optimality of non-uniform language generation
in the limit. We propose an algorithm, whose generation times $t^\star(L)$ are
(almost) Pareto-optimal: any other algorithm whose generation time for some
language $L$ is strictly smaller than $t^\star(L)$, must satisfy that its
generation time for some other language $L'$ is strictly worse than
$t^\star(L')$. Pareto-optimality is essentially the best that one can achieve
for non-uniform generation. Our algorithmic framework conveniently adapts to
further give Pareto-optimal non-uniform generation algorithms in the
practically motivated settings of noisy as well as representative generation.

</details>


### [364] [Low Recourse Arborescence Forests Under Uniformly Random Arcs](https://arxiv.org/abs/2510.02950)
*J Niklas Dahlmeier,D Ellis Hershkowitz*

Main category: cs.DS

TL;DR: 该论文研究了在插入弧时如何维护最大基数森林，同时最小化所需更改的总弧数。


<details>
  <summary>Details</summary>
Motivation: 在插入弧时维护最大基数森林，同时最小化所需更改的总弧数。

Method: 提出了一种算法，在所有 m 个弧均匀随机到达的情况下，预期追索成本为 O(m·log^2 n)。

Result: 在插入弧时维护最大基数森林，同时最小化所需更改的总弧数。

Conclusion: 在插入弧时维护最大基数森林，同时最小化所需更改的总弧数。

Abstract: In this work, we study how to maintain a forest of arborescences of maximum
arc cardinality under arc insertions while minimizing recourse -- the total
number of arcs changed in the maintained solution. This problem is the
"arborescence version'' of max cardinality matching.
  On the impossibility side, we observe that even in this insertion-only model,
it is possible for $m$ adversarial arc arrivals to necessarily incur $\Omega(m
\cdot n)$ recourse, matching a trivial upper bound of $O(m \cdot n)$. On the
possibility side, we give an algorithm with expected recourse $O(m \cdot \log^2
n)$ if all $m$ arcs arrive uniformly at random.

</details>


### [365] [Oracle-based Uniform Sampling from Convex Bodies](https://arxiv.org/abs/2510.02983)
*Thanh Dang,Jiaming Liang*

Main category: cs.DS

TL;DR: We developed new MCMC algorithms for uniform sampling on convex bodies using the Alternating Sampling Framework and a Restricted Gaussian Oracle (RGO). We efficiently implemented RGO via rejection sampling, assuming access to a projection or separation oracle. We provide non-asymptotic complexities for unbiased sampling accuracy measured in Renyi or chi^2-divergence.


<details>
  <summary>Details</summary>
Motivation: The paper aims to develop efficient Markov chain Monte Carlo (MCMC) algorithms for sampling uniformly from a convex body K.

Method: The proposed algorithms utilize the Alternating Sampling Framework/proximal sampler, which involves Gibbs sampling on an augmented distribution and requires a Restricted Gaussian Oracle (RGO). The RGO is efficiently implemented using rejection sampling and relies on either a projection oracle or a separation oracle for the convex body K.

Result: The paper establishes non-asymptotic complexities for obtaining unbiased samples. The accuracy of these samples is measured using Renyi divergence or chi^2-divergence.

Conclusion: The work provides efficient MCMC algorithms with proven theoretical guarantees for uniform sampling from convex bodies, specifically by addressing the challenge of implementing the Restricted Gaussian Oracle.

Abstract: We propose new Markov chain Monte Carlo algorithms to sample a uniform
distribution on a convex body $K$. Our algorithms are based on the Alternating
Sampling Framework/proximal sampler, which uses Gibbs sampling on an augmented
distribution and assumes access to the so-called restricted Gaussian oracle
(RGO). The key contribution of this work is the efficient implementation of RGO
for uniform sampling on $K$ via rejection sampling and access to either a
projection oracle or a separation oracle on $K$. In both oracle cases, we
establish non-asymptotic complexities to obtain unbiased samples where the
accuracy is measured in R\'enyi divergence or $\chi^2$-divergence.

</details>


### [366] [Smooth Trade-off for Tensor PCA via Sharp Bounds for Kikuchi Matrices](https://arxiv.org/abs/2510.03061)
*Pravesh K. Kothari,Jeff Xu*

Main category: cs.DS

TL;DR: 该论文研究了张量PCA问题，旨在确定在什么条件下可以从随机高斯张量中区分出信号，并恢复未知布尔向量v。研究表明，基于Kikuchi层级的谱算法在信号强度λ满足λ ≥ Θr(1) · n^{-r/4} · ℓ^{1/2-r/4}时可以成功，其中Θr(1)是与n和ℓ无关的绝对常数。这解决了之前关于信号恢复所需最小信号强度以及是否存在平滑权衡的猜想，并且在ℓ ≤ 3r/4的情况下，之前已经通过非渐进技术得到了相似的界限。


<details>
  <summary>Details</summary>
Motivation: 确定张量PCA问题中恢复未知向量v所需的最小信号强度λ，并解决关于信号恢复是否存在平滑权衡的猜想，这对理解量子计算在张量PCA中的应用至关重要。

Method: 使用基于Kikuchi层级的谱算法。

Result: 证明了当信号强度λ ≥ Θr(1) · n^{-r/4} · ℓ^{1/2-r/4}时，该算法能够成功恢复信号，其中Θr(1)是与n和ℓ无关的绝对常数。此结果在ℓ ≤ 3r/4的情况下得到了先前非渐进技术的支持。

Conclusion: 该论文解决了张量PCA问题中信号恢复的最小信号强度问题，并证明了谱算法在满足特定条件时能够有效工作，这为理解量子计算在张量PCA中的潜力提供了关键见解。

Abstract: In this work, we revisit algorithms for Tensor PCA: given an order-$r$ tensor
of the form $T = G+\lambda \cdot v^{\otimes r}$ where $G$ is a random symmetric
Gaussian tensor with unit variance entries and $v$ is an unknown boolean vector
in $\{\pm 1\}^n$, what's the minimum $\lambda$ at which one can distinguish $T$
from a random Gaussian tensor and more generally, recover $v$? As a result of a
long line of work, we know that for any $\ell \in \N$, there is a $n^{O(\ell)}$
time algorithm that succeeds when the signal strength $\lambda \gtrsim
\sqrt{\log n} \cdot n^{-r/4} \cdot \ell^{1/2-r/4}$. The question of whether the
logarithmic factor is necessary turns out to be crucial to understanding
whether larger polynomial time allows recovering the signal at a lower signal
strength. Such a smooth trade-off is necessary for tensor PCA being a candidate
problem for quantum speedups[SOKB25]. It was first conjectured by [WAM19] and
then, more recently, with an eye on smooth trade-offs, reiterated in a blogpost
of Bandeira.
  In this work, we resolve these conjectures and show that spectral algorithms
based on the Kikuchi hierarchy \cite{WAM19} succeed whenever $\lambda \geq
\Theta_r(1) \cdot n^{-r/4} \cdot \ell^{1/2-r/4}$ where $\Theta_r(1)$ only hides
an absolute constant independent of $n$ and $\ell$. A sharp bound such as this
was previously known only for $\ell \leq 3r/4$ via non-asymptotic techniques in
random matrix theory inspired by free probability.

</details>


<div id='cond-mat.mes-hall'></div>

# cond-mat.mes-hall [[Back]](#toc)

### [367] [Symmetry in magnetoelectric electromagnetism and magnetoelectric meta-atoms](https://arxiv.org/abs/2510.02521)
*E. O. Kamenetskii*

Main category: cond-mat.mes-hall

TL;DR: 尽管电磁学具有时空对称性，但磁电（ME）过程的特点是时空对称性破缺。本研究旨在表明，具有时间反转和空间反转对称性破缺（所谓的ME场）的量子真空场可以在电磁（EM）辐射的亚波长区域中被观察到。ME场的来源是ME超原子，即由磁绝缘体组成的亚波长元件，其中动力学ME行为是由于磁化和电极化的调制和拓扑耦合。EM和ME系统之间的相互作用通过ME虚光子等虚结构发生。实验上，ME量子真空态可以在微波腔中被检测到。这些研究的主题涉及ME量子电动力学（MEQED）的基本方面。


<details>
  <summary>Details</summary>
Motivation: 磁电（ME）过程的特点是时空对称性破缺，而电磁学具有时空对称性。本研究的动机是探索在亚波长区域观察到具有时间反转和空间反转对称性破缺的量子真空场（ME场）。

Method: 本研究提出使用由磁绝缘体组成的亚波长元件（ME超原子）作为ME场的来源。通过调制和拓扑耦合磁化和电极化，实现了动力学ME行为。EM和ME系统之间的相互作用通过ME虚光子等虚结构进行。

Result: 实验上，ME量子真空态可以在微波腔中被检测到。

Conclusion: 本研究探索了ME量子电动力学（MEQED）的基本方面，表明ME场可以在亚波长区域被观察到，并且可以通过微波腔进行实验检测。

Abstract: While in electromagnetism we have space-time symmetry, magnetoelectric (ME)
processes are characterized by space-time symmetry breaking. Our goal is to
show that quantum vacuum fields with both time reversal and space inversion
symmetry breaking (so-called ME fields) can be observed in subwavelength
regions of electromagnetic (EM) radiation. The sources of ME fields are ME
meta-atoms, subwavelength elements made of magnetic insulators, where the
dynamical ME behavior is due to modulations and topological coupling of
magnetization and electric polarization. The interaction between EM and ME
systems occurs through virtual structures, ME virtual photons. Experimentally,
ME quantum vacuum states can be detected in a microwave cavity. The topic of
these studies concerns fundamental aspects of ME quantum electrodynamics
(MEQED).

</details>


### [368] [A Physical Unclonable Function Based on Variations of Write Times in STT-MRAM due to Manufacturing Defects](https://arxiv.org/abs/2510.02574)
*Jacob Huber,Supriyo Bandyopadhyay*

Main category: cond-mat.mes-hall

TL;DR: 基于STT-MRAM的PUF


<details>
  <summary>Details</summary>
Motivation: PUF利用设备响应中固有的随机变化来提供独一无二的“生物识别”特征，可用于身份验证。

Method: 通过微磁模拟研究了六种不同缺陷形态下，注入自旋极化电流的磁性隧道结的开关时间对缺陷的敏感性。

Result: 研究表明，磁性隧道结的开关时间对制造过程中引入的结构缺陷具有敏感性，证明了其作为PUF的可行性。

Conclusion: STT-MRAM的开关时间可作为PUF的基础。

Abstract: A physical unclonable function (PUF) utilizes the unclonable random
variations in a device's responses to a set of inputs to produce a unique
"biometric" that can be used for authentication. The variations are caused by
unpredictable, unclonable and random manufacturing defects. Here, we show that
the switching time of a magnetic tunnel junction injected with a spin-polarized
current generating spin transfer torque is sensitive to the nature of
structural defects introduced during manufacturing and hence can be the basis
of a PUF. We use micromagnetic simulations to study the switching times under a
constant current excitation for six different (commonly encountered) defect
morphologies in spin-transfer-torque magnetic random access memory (STT-MRAM)
to establish the viability of a PUF.

</details>


### [369] [Ginzburg-Landau theory of spin pumping through an antiferromagnetic layer near the Néel temperature](https://arxiv.org/abs/2510.02644)
*Yuto Furutani,Hayato Fukushima,Yutaka Yamamoto,Masanori Ichioka,Hiroto Adachi*

Main category: cond-mat.mes-hall

TL;DR: 在铁磁/反铁磁/重金属三层结构中，特别是在反铁磁的Néel温度附近，由于界面交换相互作用，发现泵浦出的自旋流有显著的、与频率强相关的增强现象。


<details>
  <summary>Details</summary>
Motivation: 描述了在铁磁/反铁磁/重金属三层结构中，利用Ginzburg-Landau理论描述自旋流及其在Néel温度附近的动态行为，特别是Néel序参数的动态起伏。

Method: 利用Ginzburg-Landau理论，结合铁磁/反铁磁界面处的界面交换相互作用，推导了泵浦自旋流的表达式，并分析了其在Néel温度附近的频率依赖性。

Result: 发现了当存在铁磁自旋与反铁磁Néel序参数之间的界面交换相互作用时，泵浦自旋流会随着Néel温度的升高而显著增强，并且这种增强具有强烈的频率依赖性，在Néel温度附近达到峰值。

Conclusion: 该理论为解释Y$_3$Fe$_5$O$_{12}$/CoO/Pt体系中观测到的具有强频率依赖性的自旋流增强现象提供了理论依据。

Abstract: Spin pumping is a microwave-driven means for injecting spins from a
ferromagnet into the adjacent target material. The insertion of a thin
antiferromagnetic layer between the ferromagnet and the target material is
known to enhance the spin pumping signal. Here, in view of describing dynamic
fluctuations of the N\'{e}el order parameter, we develop Ginzburg-Landau theory
of the spin pumping in a ferromagnet/antiferromagnet/heavy metal trilayer in
the vicinity of the antiferromagnetic N\'{e}el temperature $T_{\rm N}$. When
there exists an interfacial exchange interaction between the ferromagnetic
spins and the antiferromagnetic N\'{e}el order parameter at the
ferromagnet/antiferromagnet interface, we find a strongly frequency-dependent
enhancement of the pumped spin current that is peaked at $T_{\rm N}$. The
present finding offers an explanation for the enhanced spin pumping with strong
frequency dependence observed in a Y$_3$Fe$_5$O$_{12}$/CoO/Pt system.

</details>


### [370] [Classification of electromagnetic responses by quantum geometry](https://arxiv.org/abs/2510.02661)
*Longjun Xiang,Jinxiong Jia,Fuming Xu,Jian Wang*

Main category: cond-mat.mes-hall

TL;DR: 该论文提出了一个统一的量子几何框架，用于描述由电磁场驱动的 Bloch 电子双线性电荷电流，包括普通霍尔效应（OHE）、磁非线性霍尔效应（MNHE）和平面霍尔效应（PHE）。


<details>
  <summary>Details</summary>
Motivation: 在量子材料中，Bloch 电子的非线性电荷电流可以通过量子几何来表征，例如由 Berry 曲率偶极子和量子度量偶极子引起的本征和非本征非线性霍尔效应。然而，对于由电磁场驱动的双线性电荷电流，包括 OHE、MNHE 和 PHE，仍然缺乏统一的量子几何描述。

Method: 论文利用传统的量子几何和新提出的塞曼量子几何，将双线性电导率（由轨道最小耦合和塞曼耦合引起）进行了分类。具体来说，本征轨道和自旋双线性电流（分别对应轨道和自旋 MNHE）由量子度量四极矩和塞曼量子度量偶极子控制；而非本征轨道和自旋双线性电流（分别对应轨道和自旋 PHE）则由 Berry 曲率四极矩和塞曼 Berry 曲率偶极子控制。此外，论文还发现 OHE 的带间贡献可能来自量子度量四极矩。

Result: 论文提出了一个完整的量子几何分类，并在此基础上研究了三维拓扑绝缘体表面狄拉克锥的自旋 PHE。

Conclusion: 该研究成功地建立了双线性电流的量子几何分类，并揭示了量子几何在理解和预测 OHE、MNHE 和 PHE 等现象中的重要作用。

Abstract: The nonlinear charge current $j_a=\sigma_{abc}E_bE_c$ of Bloch electrons in
quantum materials under an electric field can be well characterized by the
quantum geometry, as most exemplified by the extrinsic and intrinsic nonlinear
Hall effects induced by the Berry curvature dipole and the quantum metric
dipole, respectively. Nevertheless, a unified quantum geometric description for
the bilinear charge current $j_a=\sigma_{ab,c}E_bB_c$ of Bloch electrons driven
by the electromagnetic fields, including the ordinary Hall effect (OHE), the
magnetononlinear Hall effect (MNHE), and the planar Hall effect (PHE), remains
elusive. Herein, we show that this bilinear conductivity, as contributed by the
orbital minimal coupling and the spin Zeeman coupling of the applied magnetic
field, respectively, can be classified by the conventional quantum geometry and
the recently proposed Zeeman quantum geometry, where the symmetry constraint
from the fundamental response equation is encoded. Specifically, we uncover
that the intrinsic orbital and spin bilinear currents--responsible for the
orbital and spin MNHEs--are governed by the quantum metric quadrupole and the
Zeeman quantum metric dipole, respectively. In contrast, the extrinsic orbital
and spin bilinear currents, which are linear in the relaxation time $\tau$ and
lead to the orbital and spin PHEs, are governed by the Berry curvature
quadrupole and the Zeeman Berry curvature dipole, respectively.
Counterintuitively, we find that the OHE due to the Lorentz force can also
include an interband contribution from the quantum metric quadrupole. After
building the quantum geometric classification of this bilinear current, we
study the rarely known spin PHE with the surface Dirac cone of
three-dimensional topological insulators.

</details>


### [371] [Weak localization and antilocalization corrections to nonlinear transport: a semiclassical Boltzmann treatment](https://arxiv.org/abs/2510.02684)
*Dmitry V. Chichinadze*

Main category: cond-mat.mes-hall

TL;DR: 弱(反)局域化效應影響兩維系統的二階非線性傳輸。


<details>
  <summary>Details</summary>
Motivation: 研究弱(反)局域化對二維系統中二階非線性傳輸的影響，以及其與石墨烯器件中觀察到的現象的聯繫。

Method: 使用半經典玻爾茲曼方法，求解準粒子分佈函數到二階，並計算線性與非線性電導率張量。

Result: 發現局域化效應可能導致非線性電導率張量的符號改變，這與單層石墨烯器件中的觀察結果一致。

Conclusion: 弱(反)局域化效應在打破反演對稱性的情況下，會顯著影響二維系統的非線性傳輸特性，並可能導致非線性電導率符號的改變。

Abstract: The nonlinear transport regime is manifested in the nonlinear current-voltage
characteristic of the system. An example of such a nonlinear regime is a setup
in which current is injected into the sample and the measured voltage drop is
quadratic in the injected current. Such a quadratic nonlinear regime requires
inversion symmetry to be broken. This is the same symmetry condition as one
needs to observe weak antilocalization, which can be prominent in
two-dimensional systems. Here, we study the effects of weak (anti)localization
on second-order nonlinear transport in two-dimensional systems using the
semiclassical Boltzmann approach. We solve for quasiparticle distribution
function up to the second order in the applied external electric field and
calculate linear and nonlinear conductivity tensors for a toy model. We find
that localization effects could lead to a sign change of the nonlinear
conductivity tensor -- a phenomenon observed in single-layer graphene devices.

</details>


### [372] [Magnetocaloric effect for the altermagnetic candidate MnTe](https://arxiv.org/abs/2510.02777)
*N. N. Orlova,V. D. Esin,A. V. Timonina,N. N. Kolesnikov,E. V. Deviatov*

Main category: cond-mat.mes-hall

TL;DR: MnTe 晶体在低于 Néel 温度时表现出反常的磁热效应，这证实了自旋轨道耦合诱导的自旋极化。


<details>
  <summary>Details</summary>
Motivation: 研究 MnTe  الال터자성체在向具有自发自旋极化的状态转变时的磁热效应。

Method: 使用麦克斯韦关系从实验磁化曲线上计算等温磁熵变 $\Delta S$。

Result: 在临界温度 $T_c\approx 81$~K 附近观察到清晰的磁热效应，表现为 $\Delta S$ 的窄峰，并伴有急剧的磁化跃变。

Conclusion: 所观察到的行为不同于标准的铁磁转变，证实了 MnTe  الال터자성体状态中预测的自旋轨道耦合诱导的自旋极化。

Abstract: We experimentally investigate magnetocaloric effect for single crystals of
MnTe altermagnet at the transition to the state with spontaneous spin
polarization, i.e. well below the N\'eel temperature of MnTe. The isothermal
magnetic entropy change $\Delta S$ is calculated from the experimental
magnetization curves by using Maxwell relation. We observe well-defined
magnetocaloric effect as a narrow $\Delta S$ peak around the cricital
temperature $T_c\approx 81$~K, which is accompanied by sharp magnetization
jump. This behavior is unusual for standard ferromagnetic transitions, so it
confirms the predicted spin-orbit-induced spin polarization in the MnTe
altermagnetic state.

</details>


### [373] [Using Landau quantization to probe disorder in semiconductor heterostructures](https://arxiv.org/abs/2510.02794)
*Asser Elsayed,Davide Costa,Lucas E. A. Stehouwer,Alberto Tosato,Mario Lodari,Brian Paquelet Wuetz,Davide Degli Esposti,Giordano Scappucci*

Main category: cond-mat.mes-hall

TL;DR: 论文提出了一种通过量子霍尔平台宽度来估算半导体异质结构无序性的新方法。


<details>
  <summary>Details</summary>
Motivation: 为了在大量自旋量子比特阵列中减少无序源并确保高产量和均匀性，理解半导体异质结构中的散射机制至关重要。通常通过与金属-绝缘体转变相关的临界渗流密度来估算二维电子或空穴气体的无序性，但实验测量精度要求高，尤其在低载流子密度下。因此，需要一种更可靠的估算方法。

Method: 将实验测量的渗流密度与量子霍尔平台宽度联系起来，以此作为表征半导体异质结构无序性的一种替代方法。

Result: 通过实验证明了渗流密度与量子霍尔平台宽度的关联性。

Conclusion: 所提出的方法为表征半导体异质结构无序性提供了一种新的、更易于实验操作的途径，有望克服传统方法在低载流子密度下测量精度不足的困难。

Abstract: Understanding scattering mechanisms in semiconductor heterostructures is
crucial to reducing sources of disorder and ensuring high yield and uniformity
in large spin qubit arrays. Disorder of the parent two-dimensional electron or
hole gas is commonly estimated by the critical, percolation-driven density
associated with the metal-insulator transition. However, a reliable estimation
of the critical density within percolation theory is hindered by the need to
measure conductivity with high precision at low carrier densities, where
experiments are most difficult. Here, we connect experimentally percolation
density and quantum Hall plateau width, in line with an earlier heuristic
intuition, and offer an alternative method for characterizing semiconductor
heterostructure disorder.

</details>


### [374] [Dissipation properties of anomalous Hall effect: intrinsic vs. extrinsic magnetic materials](https://arxiv.org/abs/2510.02935)
*V. Desbuis,D. Lacour,M. Hehn,S. Geiskopf,L. Michez,J. Rial,V. Baltz,J. -E. Wegrowe*

Main category: cond-mat.mes-hall

TL;DR: 研究了磁性材料霍尔器件与外部电路的耦合，发现不同磁性材料（锰硅、钴钆、镍铁）在不同霍尔效应下，其功率耗散规律相似，且与材料的内在机制（如拓扑结构）关系不大，而主要受电荷注入和屏幕效应影响。


<details>
  <summary>Details</summary>
Motivation: 研究不同磁性材料的霍尔效应及其在电路中的功率耗散规律，探讨其影响因素。

Method: 通过实验测量不同材料（Mn5Si3, Co75Gd25, Ni80Fe20）在不同载流子注入方式（异常霍尔效应、平面霍尔效应）下，作为负载电阻和霍尔角的函数的电流、电压和功率。

Result: 发现三种材料的功率耗散规律相似，主要与负载电阻和霍尔角有关，与材料的内在磁性机制关系不大，表明电荷注入和屏幕效应是主导因素。

Conclusion: 霍尔效应的耗散特性主要由电荷注入和屏幕效应决定，而非其内在的物理机制。

Abstract: A comparative study of anomalous-Hall current injection and anisotropic
current injection (through planar Hall effect) are studied in Hall devices
contacted to a lateral load circuit. Hall currents are injected into the load
circuit from three different kinds of magnetic Hall bars: Mn5Si3 altermagnet,
Co75Gd25 ferrimagnet, and Ni80Fe20 ferromagnet. The current, the voltage and
the power are measured as a function of the load resistance and the Hall angle.
It is observed that the power dissipated for the three kinds of materials
fellow the same law as a function of load resistance and Hall angle, at the
leading order in the Hall angle. Since the anomalous Hall effect in the
altermagnetic Hall-bar is due to the intrinsic topological structure (i.e. due
to the presence of a Berry phase in the reciprocal space), these observations
suggest that the dissipative properties of anomalous Hall effect are dominated
by the injection of electric charges accumulated at the edges (including
electric screening), instead of the very mechanism responsible for it.

</details>


### [375] [Deconstruction of the anisotropic magnetic interactions from spin-entangled optical excitations in van der Waals antiferromagnets](https://arxiv.org/abs/2510.03010)
*Dipankar Jana,Swagata Acharya,Milan Orlita,Clement Faugeras,Dimitar Pashov,Mark van Schilfgaarde,Marek Potemski,Maciej Koperski*

Main category: cond-mat.mes-hall

TL;DR: The study uses ab initio theory to analyze magneto-optical excitations in MnPS3 and NiPS3, identifying spin-flip transitions on magnetic atoms as the source of spin-entangled excitations and providing a realistic bandgap estimation.


<details>
  <summary>Details</summary>
Motivation: To resolve the mechanisms driving spin-entangled optical transitions and determine the single-particle bandgap in van der Waals antiferromagnetic d systems like MnPS3 and NiPS3, due to the complex interplay between magnetic ordering and optical transitions.

Method: Employed high-fidelity ab initio theory to estimate the bandgap, elucidating atom- and orbital-resolved contributions, and analyzed spin-entangled excitations observed in photoluminescence and absorption resonances.

Result: Demonstrated that spin-entangled excitations originate from on-site spin-flip transitions confined to Mn or Ni atoms, providing a realistic estimation of the bandgap.

Conclusion: Spin-entangled optical excitations in MnPS3 and NiPS3 films stem from on-site spin-flip transitions within the magnetic atoms, and the study successfully estimated the material bandgap using advanced theoretical methods.

Abstract: Magneto-optical excitations in antiferromagnetic d systems can originate from
a multiplicity of light-spin and spin-spin interactions, as the light and spin
degrees of freedom can be entangled. This is exemplified in van der Waals
systems with attendant strong anisotropy between in-plane and out-of-plane
directions, such as MnPS3 and NiPS3 films studied here. The rich interplay
between the magnetic ordering and sub-bandgap optical transitions poses a
challenge to resolve the mechanisms driving spin-entangled optical transitions,
as well as the single-particle bandgap itself. Here we employ a high-fidelity
ab initio theory to find a realistic estimation of the bandgap by elucidating
the atom- and orbital-resolved contributions to the fundamental sub-bands. We
further demonstrate that the spin-entangled excitations, observable as
photoluminescence and absorption resonances, originate from an on-site
spin-flip transition confined to a magnetic atom (Mn or Ni). The evolution of
the spin-flip transition

</details>


### [376] [Polarization dependence of spin-electric transitions in molecular exchange qubits](https://arxiv.org/abs/2510.03099)
*Filippo Troiani,Athanassios K. Boudalis*

Main category: cond-mat.mes-hall

TL;DR: 准光学实验可以用来研究分子自旋系统中的磁性转变，但区分电偶极和磁偶极跃迁是个挑战，因为它们的信号可能同时出现，且自旋-电跃迁的峰值强度不确定。本文利用自旋哈密顿模型计算了电偶极和磁偶极跃迁所需的极化，结果表明极化可以有效区分这两种跃迁，并有助于确定零场分裂的物理起源，这对于理解分子自旋三角形中自旋量子比特的相干性至关重要。


<details>
  <summary>Details</summary>
Motivation: 区分准光学实验中由电偶极和磁偶极共同引起的跃迁信号，以及确定零场分裂的物理起源。

Method: 通过分子自旋三角形的自旋哈密顿模型，计算了电偶极和磁偶极诱导跃迁所需的极化。

Result: 极化可以清晰地区分电偶极和磁偶极诱导跃迁，并且有助于识别零场分裂的物理起源。

Conclusion: 极化分析是区分电偶极和磁偶极诱导跃迁的有效方法，并能揭示零场分裂的物理本质，这对分子自旋量子比特的相干性研究具有重要意义。

Abstract: Quasi-optical experiments are emerging as a powerful technique to probe
magnetic transitions in molecular spin systems. However, the simultaneous
presence of the electric- and magnetic-dipole induced transitions poses the
challenge of discriminating between these two contributions. Besides, the
identification of the spin-electric transitions can hardly rely on the peak
intensity, because of the current uncertainties on the value of the
spin-electric coupling in most molecular compounds. Here, we compute the
polarizations required for electric- and magnetic-dipole induced transitions
through spin-Hamiltonian models of molecular spin triangles. We show that the
polarization allows a clear discrimination between the two kinds of
transitions. In addition, it allows one to identify the physical origin of the
zero-field splitting in the ground multiplet, a debated issue with significant
implications on the coherence properties of the spin qubit implemented in
molecular spin triangles.

</details>


### [377] [Spatial uniformity of g-tensor and spin-orbit interaction in germanium hole spin qubits](https://arxiv.org/abs/2510.03125)
*Inga Seidler,Bence Hetényi,Lisa Sommer,Leonardo Massai,Konstantinos Tsoukalas,Eoin G. Kelly,Alexei Orekhov,Michele Aldeghi,Stephen W. Bedell,Stephan Paredes,Felix J. Schupp,Matthias Mergenthaler,Gian Salis,Andreas Fuhrer,Patrick Harvey-Collard*

Main category: cond-mat.mes-hall

TL;DR: Ge/SiGe异质结构中的量子点是半导体自旋量子比特的领先平台，但量子比特频率的变化限制了大型量子比特阵列的操作。本文研究了Y型几何结构中六个和七个量子点的g-张量，发现g-张量的面外主轴倾斜平均约为1.1度，且相邻量子点倾斜相似。此外，还发现了强烈的面内g-张量各向异性，并且在其中一个器件中，提取的自旋翻转隧穿向量与均匀的Dresselhaus类自旋-轨道场一致。这些结果揭示了以前未预测或观察到的自旋-轨道相互作用和g-张量的长程关联性。


<details>
  <summary>Details</summary>
Motivation: Ge/SiGe异质结构中的量子点是半导体自旋量子比特的领先平台，但量子比特频率的变化限制了大型量子比特阵列的操作。研究g-张量的各向异性对于理解和操作这些量子比特至关重要。

Method: 研究Y型几何结构中六个和七个量子点的g-张量，分析其面外主轴倾斜和面内各向异性，并提取自旋翻转隧穿向量。

Result: g-张量的面外主轴倾斜平均约为1.1度，且相邻量子点倾斜相似。发现了强烈的面内g-张量各向异性，并且在其中一个器件中，提取的自旋翻转隧穿向量与均匀的Dresselhaus类自旋-轨道场一致。排除了静电约束形状或局部应变是g-张量各向异性起源的可能性。

Conclusion: 揭示了以前未预测或观察到的自旋-轨道相互作用和g-张量的长程关联性，这对于可靠地理解锗量子点中的g-张量至关重要。

Abstract: Holes in Ge/SiGe heterostructures are now a leading platform for
semiconductor spin qubits, thanks to the high confinement quality,
two-dimensional arrays, high tunability, and larger gate structure dimensions.
One limiting factor for the operation of large arrays of qubits is the
considerable variation in qubit frequencies or properties resulting from the
strongly anisotropic $g$-tensor. We study the $g$-tensors of six and seven
qubits in an array with a Y geometry across two devices. We report a mean
distribution of the tilts of the $g$-tensor's out-of-plane principal axis of
around $1.1 \deg$, where nearby quantum dots are more likely to have a similar
tilt. Independently of this tilt, and unlike simple theoretical predictions, we
find a strong in-plane $g$-tensor anisotropy with strong correlations between
neighboring quantum dots. Additionally, in one device where the principal axes
of all g-tensors are aligned along the [100] crystal direction, we extract the
spin-flip tunneling vector from adjacent dot pairs and find a pattern that is
consistent with a uniform Dresselhaus-like spin-orbit field. The Y arrangement
of the gate layout and quantum dots allows us to rule out local factors like
electrostatic confinement shape or local strain as the origin of the
preferential direction. Our results reveal long-range correlations in the
spin-orbit interaction and $g$-tensors that were not previously predicted or
observed, and could prove critical to reliably understand $g$-tensors in
germanium quantum dots.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [378] [Delay-Tolerant Augmented-Consensus-based Distributed Directed Optimization](https://arxiv.org/abs/2510.02889)
*Mohammadreza Doostmohammadian,Narahari Kasagatta Ramesh,Alireza Aghasi*

Main category: eess.SY

TL;DR: 该论文提出了一种分布式优化算法，可以处理通信延迟，并证明了其在强连通有向网络上的收敛性。


<details>
  <summary>Details</summary>
Motivation: 在分布式优化中，通信延迟会影响优化协议的收敛性，而现有文献对这种情况的探讨不足。

Method: 提出了一种新的分布式优化算法，该算法能够处理异构的、任意的但有界的固定通信延迟。算法结合了矩阵理论、代数图论和增强共识模型来证明收敛性。

Result: 仿真结果验证了该算法的有效性，并与一些无延迟算法进行了性能比较。

Conclusion: 该算法能够有效地处理通信延迟问题，并在强连通有向网络上实现分布式优化的收敛。

Abstract: Distributed optimization finds applications in large-scale machine learning,
data processing and classification over multi-agent networks. In real-world
scenarios, the communication network of agents may encounter latency that may
affect the convergence of the optimization protocol. This paper addresses the
case where the information exchange among the agents (computing nodes) over
data-transmission channels (links) might be subject to communication
time-delays, which is not well addressed in the existing literature. Our
proposed algorithm improves the state-of-the-art by handling heterogeneous and
arbitrary but bounded and fixed (time-invariant) delays over general
strongly-connected directed networks. Arguments from matrix theory, algebraic
graph theory, and augmented consensus formulation are applied to prove the
convergence to the optimal value. Simulations are provided to verify the
results and compare the performance with some existing delay-free algorithms.

</details>


### [379] [An Encoder-Decoder Network for Beamforming over Sparse Large-Scale MIMO Channels](https://arxiv.org/abs/2510.02355)
*Yubo Zhang,Jeremy Johnston,Xiaodong Wang*

Main category: eess.SY

TL;DR: We propose an end-to-end deep learning framework (EDN) for downlink beamforming in large-scale sparse MIMO channels, utilizing an encoder NN at each user and a beamformer decoder NN at the BS, extended to near-field and far-field scenarios.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient end-to-end deep learning framework for downlink beamforming in large-scale sparse MIMO channels, addressing both far-field and near-field scenarios.

Method: The framework employs a deep EDN architecture with three modules: an encoder NN at each user for channel compression, a beamformer decoder NN at the BS for mapping latent vectors to beamformers, and a channel decoder NN at the BS for channel reconstruction. Training involves semi-amortized learning with analytical gradient ascent and knowledge distillation, transitioning from supervised MMSE to unsupervised sum-rate optimization.

Result: Extensive simulations demonstrate the effectiveness of the proposed EDN beamforming framework under diverse network and channel conditions.

Conclusion: The developed EDN framework offers an effective solution for downlink beamforming in large-scale sparse MIMO channels, adaptable to both near-field and far-field hybrid beamforming.

Abstract: We develop an end-to-end deep learning framework for downlink beamforming in
large-scale sparse MIMO channels. The core is a deep EDN architecture with
three modules: (i) an encoder NN, deployed at each user end, that compresses
estimated downlink channels into low-dimensional latent vectors. The latent
vector from each user is compressed and then fed back to the BS. (ii) a
beamformer decoder NN at the BS that maps recovered latent vectors to
beamformers, and (iii) a channel decoder NN at the BS that reconstructs
downlink channels from recovered latent vectors to further refine the
beamformers. The training of EDN leverages two key strategies: (a)
semi-amortized learning, where the beamformer decoder NN contains an analytical
gradient ascent during both training and inference stages, and (b) knowledge
distillation, where the loss function consists of a supervised term and an
unsupervised term, and starting from supervised training with MMSE beamformers,
over the epochs, the model training gradually shifts toward unsupervised using
the sum-rate objective. The proposed EDN beamforming framework is extended to
both far-field and near-field hybrid beamforming scenarios. Extensive
simulations validate its effectiveness under diverse network and channel
conditions.

</details>


### [380] [Precise HDV Positioning through Safety-Aware Integrated Sensing and Communication in a Value-of-Information-Driven 6G V2X System](https://arxiv.org/abs/2510.02363)
*Mohammad Reza Abedi,Zahra Rashidi,Nader Mokari,Hamid Saeedi,Nizar Zorba*

Main category: eess.SY

TL;DR: ISAC技术可用于6G车联网，以提高高精度定位和通信能力。但无线资源有限和干扰是主要障碍。本研究提出了一种新的信息价值（VoI）度量，以优先传输安全关键数据，并使用MADDPG算法解决了联合感知-通信-控制问题。仿真结果表明，该方法可将碰撞风险（CR）降低33%，将碰撞时间（TTC）提高66%。


<details>
  <summary>Details</summary>
Motivation: 为了解决6G车联网中有限的无线资源和干扰问题，以同时满足高精度定位和通信容量的需求，并提高CAVs（联网自动驾驶汽车）传输安全关键数据以改善TTC和降低CR比率的能力。

Method: 将联合感知-通信-控制问题建模为一个双时间尺度序贯决策过程，并使用多智能体分布式确定性策略梯度（MADDPG）算法求解。通过关注高VoI数据，该框架简化了复杂性并优化了网络和流量资源的使用。

Result: 仿真结果表明，该方法可将碰撞风险（CR）降低至少33%，并将碰撞时间（TTC）提高高达66%。

Conclusion: 提出的基于VoI度量的MADDPG方法在混合自主环境中能够显著提高安全性与效率。

Abstract: Recent advancements in Integrated Sensing and Communications (ISAC) have
unlocked new potential for addressing the dual demands of high-resolution
positioning and reliable communication in 6G Vehicle-to-Everything (V2X)
networks. These capabilities are vital for transmitting safety-critical data
from Connected Autonomous Vehicles (CAVs) to improve metrics such as Time to
Collision (TTC) and reduce the Collision Risk (CR) ratio. However, limited
radio resources and interference remain major obstacles to achieving both
precision and capacity simultaneously. The challenge intensifies in
mixedtraffic scenarios involving Human-Driven Vehicles (HDVs), which lack
connectivity and cannot share their status or positioning. Additionally, CAV
sensors are limited in range and accuracy, making detection of HDVs unreliable.
ISAC plays a pivotal role here by enabling the sensing of HDV positions via
shared communication infrastructure, improving environmental awareness. To
address these challenges, this paper proposes a novel Value of Information
(VoI) metric that prioritizes the transmission of safety-critical data. The
joint sensing-communication-control problem is modeled as a two-time-scale
sequential decision process and solved using a Multi-Agent Distributed
Deterministic Policy Gradient (MADDPG) algorithm. By focusing on high- VoI
data, the framework reduces complexity and optimizes network and traffic
resource usage. Simulations show that the proposed approach significantly
reduces the CR ratio by at least 33% and improves the TTC by up to 66%,
demonstrating its effectiveness in enhancing safety and efficiency in
mixedautonomy environments.

</details>


### [381] [Conceptualizing and Modeling Communication-Based Cyberattacks on Automated Vehicles](https://arxiv.org/abs/2510.02364)
*Tianyi Li,Tianyu Liu,Yicheng Yang*

Main category: eess.SY

TL;DR: 自适应巡航控制（ACC）在电动汽车（EV）和内燃机（ICE）汽车中的应用日益广泛，但同时也增加了通信相关网络攻击的风险。由于两种动力系统对控制输入的响应方式不同，它们的网络弹性未经量化。本文提出了一种新颖的方法，通过形式化六种新的消息级攻击向量，并在环形道路仿真中进行实现，系统地改变了ACC的市场渗透率（MPR）和受损车辆的空间分布。通过一个三层风险分类，将干扰度量转化为实际可行的防御优先级。仿真结果表明，与ICE车辆相比，EV车队在速度标准差、车距振荡和攻击后恢复速度方面表现更佳，显示出固有的稳定性优势。这些发现阐明了控制器到动力系统的耦合如何影响脆弱性，并为混合自动驾驶交通中的攻击检测和缓解提供了量化指导。


<details>
  <summary>Details</summary>
Motivation: 区分电动汽车（EV）和内燃机（ICE）汽车在自适应巡航控制（ACC）系统中的网络弹性，因为它们对控制输入的响应方式不同，目前尚未量化。

Method: 提出六种新颖的消息级攻击向量，并在环形道路仿真中实现，同时改变ACC的市场渗透率（MPR）和受损车辆的空间分布。

Result: 在所有仿真场景中，电动汽车（EV）车队表现出比内燃机（ICE）车队更低的速度标准差、更小的车距振荡和更快的攻击后恢复速度，表明其具有固有的稳定性优势。

Conclusion: 控制器到动力系统的耦合会影响车辆的网络脆弱性。研究结果为在混合自动驾驶交通中检测和缓解此类攻击提供了量化指导。

Abstract: Adaptive Cruise Control (ACC) is rapidly proliferating across electric
vehicles (EVs) and internal combustion engine (ICE) vehicles, enhancing traffic
flow while simultaneously expanding the attack surface for communication-based
cyberattacks. Because the two powertrains translate control inputs into motion
differently, their cyber-resilience remains unquantified. Therefore, we
formalize six novel message-level attack vectors and implement them in a
ring-road simulation that systematically varies the ACC market penetration
rates (MPRs) and the spatial pattern of compromised vehicles. A three-tier risk
taxonomy converts disturbance metrics into actionable defense priorities for
practitioners. Across all simulation scenarios, EV platoons exhibit lower
velocity standard deviation, reduced spacing oscillations, and faster
post-attack recovery compared to ICE counterparts, revealing an inherent
stability advantage. These findings clarify how controller-to-powertrain
coupling influences vulnerability and offer quantitative guidance for the
detection and mitigation of attacks in mixed automated traffic.

</details>


### [382] [Dynamic Modeling and Control System Analysis for Continuous-Disc Filters in Pulp Mill Operations](https://arxiv.org/abs/2510.02385)
*Jose M. Campos-Salazar,Felipe Santander,Sebastian Larrain*

Main category: eess.SY

TL;DR: 真空圆盘过滤系统可通过基于一阶原理的动态多变量模型进行建模，MPC控制器比PI控制器表现更好。


<details>
  <summary>Details</summary>
Motivation: 现有真空圆盘过滤系统中的经验性控制方法缺乏正式的建模和控制，导致其性能未被优化。

Method: 本文基于一阶原理开发了一个连续圆盘过滤（CD-filter）系统的动态多变量模型，并对其进行了简化。然后，设计了一个线性的状态空间模型，用于开发分散式PI控制方案和集中式模型预测控制（MPC）策略。

Result: MPC在跟踪精度、过冲抑制和干扰抑制方面优于PI控制。3D效率图显示了协调入口流量和固体浓度的重要性。

Conclusion: CD-filter的性能优化需要先进的多变量控制。

Abstract: Vacuum disc filtration is critical in pulp mills for white liquor
clarification and pulp washing, involving tightly coupled dynamics between
rotational speed, vacuum pressure, slurry concentration, filtrate flow, and
cake thickness. These nonlinear interactions are often regulated using
empirical methods, lacking formal modeling and control. This article develops a
dynamic, multivariable model of a continuous-disc filter (CD-filter) system
based on first principles, simplified to a single representative disc for
tractability. A linearized state-space model supports the design of two control
strategies: a decentralized PI-based scheme and a centralized model predictive
control (MPC). MATLAB-Simulink simulations reveal that MPC outperforms PI in
tracking accuracy, overshoot reduction, and disturbance rejection. A 3D
efficiency surface illustrates the importance of coordinating inlet flow and
solids concentration. Results highlight the need for advanced multivariable
control in optimizing CD-filter performance.

</details>


### [383] [Data-Driven Stochastic Distribution System Hardening Based on Bayesian Online Learning](https://arxiv.org/abs/2510.02485)
*Wenlong Shi,Hongyi Li,Zhaoyu Wang*

Main category: eess.SY

TL;DR: 本篇论文提出了一种数据驱动的随机配电线路加固策略，利用深度神经网络预测停电场景，并通过决策依赖的分布鲁棒优化和贝叶斯在线学习算法来优化加固决策，以提高配电系统的韧性。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的极端天气频繁导致配电系统大面积停电，因此提高配电系统韧性至关重要。然而，如何有效利用包含天气信息的真实停电数据来指导配电系统的加固决策仍是一个待解决的问题。

Method: 1. 开发了一个深度神经网络（DNN）回归模型，用于预测在不同加固决策下，停电场景的概率演变。 2. 将问题构建为一个决策依赖的分布鲁棒优化（DRO）模型，利用数据驱动的模糊集来处理停电场景分布中的不确定性。 3. 提出了一种贝叶斯在线学习算法来处理决策依赖的不确定性，该算法将原问题分解为内部和外部问题，并通过贝叶斯定理和贝叶斯推断动态更新特定决策的模糊集，逐步优化加固决策。 4. 通过动态遗憾分析证明了算法的收敛性。

Result: 在爱荷华州雷德菲尔德的一个真实配电系统上进行了案例研究，并构建了包含24年（2001-2024）的实用停电记录数据集。仿真结果验证了所提策略的有效性。

Conclusion: 所提出的数据驱动随机配电线路加固策略能够有效利用历史停电数据和天气信息，通过结合深度学习和鲁棒优化技术，在面对不确定性时做出更优的加固决策，从而提高配电系统的韧性。

Abstract: Extreme weather frequently cause widespread outages in distribution systems
(DSs), demonstrating the importance of hardening strategies for resilience
enhancement. However, the well-utilization of real-world outage data with
associated weather conditions to make informed hardening decisions in DSs is
still an open issue. To bridge this research gap, this paper proposes a
data-driven stochastic distribution line (DL) hardening strategy. First, a deep
neural network (DNN) regression model is developed to predict the probabilistic
evolution of outage scenarios under various hardening decisions. Based on the
DNN predictions, the problem is formulated as a decision-dependent
distributionally robust optimization (DRO) model, accounting for uncertainties
in outage scenario distributions using a data-driven ambiguity set. To address
decision-dependent uncertainty, a Bayesian online learning algorithm is
proposed. This algorithm decomposes the original problem into inner and outer
problems. Then, it iteratively refines hardening decisions by sequentially
incorporating outage data and dynamically updating decision-specific ambiguity
sets by using Bayes' theorem and Bayesian Inference. Also, the convergence of
the algorithm is proven through dynamic regret analysis. Finally, case studies
are implemented on a real-world DS in Redfield, Iowa, USA. A dataset spanning
24 years (2001-2024) is constructed based on the utility outage records. The
simulation results validates the effectiveness of the proposed strategy.

</details>


### [384] [Power Distribution System Blackstart Restoration Using Renewable Energy](https://arxiv.org/abs/2510.02495)
*Wenlong Shi,Hongyi Li,Cong Bai,Zhaoyu Wang*

Main category: eess.SY

TL;DR: 本篇综述研究了利用可再生能源对配电系统进行黑启动恢复的最新技术进展。


<details>
  <summary>Details</summary>
Motivation: 集成可再生能源到电网可以减少碳排放，并有助于配电系统（DS）的黑启动恢复。该过程利用可再生能源、逆变器、态势感知和配电自动化，在配电系统层面启动黑启动，实现快速响应和自下而上的恢复。

Method: 首先，介绍了分布式能源（DERs）、网络拓扑和负荷动态的数学模型。然后，讨论了态势感知如何通过实时监控和预测来提高恢复性能。接着，提供了配电系统黑启动恢复问题的目标、约束和现有决策方法。最后，概述了剩余的挑战，并强调了机遇和未来的研究方向。

Result: 本综述涵盖了利用可再生能源进行配电系统黑启动恢复的最新技术进展。

Conclusion: 可再生能源在配电系统黑启动恢复中具有重要作用，但仍存在挑战和机遇，需要进一步的研究。

Abstract: Integrating renewable energy sources into the grid not only reduces global
carbon emissions, but also facilitates distribution system (DS) blackstart
restoration. This process leverages renewable energy, inverters, situational
awareness and distribution automation to initiate blackstart at the DS level,
obtaining a fast response and bottom-up restoration. In this Review, we survey
the latest technological advances for DS blackstart restoration using renewable
energy. We first present mathematical models for distributed energy resources
(DERs), network topology, and load dynamics. We then discuss how the
situational awareness can help improve restoration performance through
real-time monitoring and forecasting. Next, the DS blackstart restoration
problem, including objectives, constraints, and existing methodologies for
decision-making are provided. Lastly, we outline remaining challenges, and
highlight the opportunities and future research directions.

</details>


### [385] [Situationally Aware Rolling Horizon Multi-Tier Load Restoration Considering Behind-The-Meter DER](https://arxiv.org/abs/2510.02502)
*Wenlong Shi,Junyuan Zheng,Zhaoyu Wang*

Main category: eess.SY

TL;DR: 该研究提出了一个多层级负荷恢复框架，以解决电力分配系统中跨馈线恢复的挑战，并结合了情境感知能力和安全的滚动优化策略。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注相邻馈线间的负荷转移，缺乏对跨越多条馈线、并考虑冷负荷拾取和分布式能源（DERs）延迟重连等复杂系统动态的恢复策略。

Method: 提出了一种多层级负荷恢复模型，包括多层级负荷转移以及变电站变压器和馈线保护模型。通过引入二进制动作开关变量和负荷块转移变量来捕捉开关动作和多层级转移过程的动态。该问题被构建为一个混合整数线性规划（MILP）问题，并嵌入滚动优化框架中。为了解决传统滚动优化视野短浅的问题，提出了一系列基于分段恢复奖励界限的安全约束，确保每个时间步的恢复都能达到其最优恢复潜力的（1-ε）倍，从而平衡短期决策和长期恢复目标。

Result: 在修改后的IEEE 123节点测试馈线上进行了案例研究，验证了所提出的多层级恢复框架的有效性。

Conclusion: 所提出的多层级、情境感知的负荷恢复框架能够有效地处理电力分配系统中复杂的恢复场景，并通过安全的滚动优化策略保证了恢复过程的效率和鲁棒性。

Abstract: Restoration in power distribution systems (PDSs) is well studied, however,
most existing research focuses on network partition and microgrid formation,
where load transfer is limited to adjacent feeders. This focus is not
practical, as when adjacent feeders lack sufficient capacity, utilities may
request support from more distant feeders in practice. Such a hirarchical
restoration is complex, especially when involving changing system conditions
due to cold load pickup and delayed reconnection of behind-the-meter DERs. To
fill this research gap, a situationally aware multi-tier load restoration
framework is proposed. Specifically, models are proposed to describe the
multi-tier load restoration, including the multi-tier load transfer and
substation transformer and feeder protection models. By introducing binary
actional switching variables and load block transfer variables, the models
effectively captures the dynamics of switches and multi-tier transfer process.
To integrate situational awareness of evolving system conditions, the problem
is formulated as a mixed-integer linear program (MILP) and then embedded within
a rolling horizon optimization. Particularly, a set of safeguarded constraints
are developed based on segment-level restoration reward bounds to mitigate the
myopia of traditional rolling horizon optimization. The proposed safeguarded
rolling strategy guarantees that each time step is lower bounded by a
$(1-\varepsilon)$-fraction of its optimal restoration potential, thereby
balancing short-term switching decisions with long-term restoration goals.
Finally, cases studies on the modified IEEE 123-node test feeder validate the
proposed multi-tier restoration framework.

</details>


### [386] [A Bilevel Optimization Framework for Adversarial Control of Gas Pipeline Operations](https://arxiv.org/abs/2510.02503)
*Tejaswini Sanjay Katale,Lu Gao,Yunpeng Zhang,Alaa Senouci*

Main category: eess.SY

TL;DR: 网络物理威胁对石油管道网络构成重大风险，本研究提出了一种基于物理信息仿真和优化框架，通过集成液压动力学、SCADA状态估计、模型预测控制（MPC）和双层优化，来分析和防御此类攻击。


<details>
  <summary>Details</summary>
Motivation: 石油管道运行技术系统面临日益增长的网络攻击风险，需要分析和防御针对能源基础设施的网络物理威胁。

Method: 开发了一个包含网络液压动力学、SCADA状态估计、MPC和双层攻击模型（针对隐蔽性虚假数据注入攻击）的仿真与优化框架。该框架使用节点压力演化和基于边的Weymouth关系来模拟管道流体动力学，并考虑了阀门和压缩机等设备。通过扩展卡尔曼滤波器估计网络状态，利用MPC在执行器约束和预测需求下计算安全控制输入。攻击被形式化为一个双层优化问题，并通过KKT条件重构为一个混合整数二次规划问题来求解。

Result: 通过对天然气管道案例的研究，证明了在攻击下可以隐蔽地降低服务输送能力。结果表明，无法检测的攻击可以在瞬时偏差很小的情况下造成持续的输送能力损失。

Conclusion: 研究结果揭示了在网络物理基础设施中集成检测和控制策略的必要性，以应对隐蔽的网络攻击对能源供应的影响。

Abstract: Cyberattacks on pipeline operational technology systems pose growing risks to
energy infrastructure. This study develops a physics-informed simulation and
optimization framework for analyzing cyber-physical threats in petroleum
pipeline networks. The model integrates networked hydraulic dynamics,
SCADA-based state estimation, model predictive control (MPC), and a bi-level
formulation for stealthy false-data injection (FDI) attacks. Pipeline flow and
pressure dynamics are modeled on a directed graph using nodal pressure
evolution and edge-based Weymouth-type relations, including control-aware
equipment such as valves and compressors. An extended Kalman filter estimates
the full network state from partial SCADA telemetry. The controller computes
pressure-safe control inputs via MPC under actuator constraints and forecasted
demands. Adversarial manipulation is formalized as a bi-level optimization
problem where an attacker perturbs sensor data to degrade throughput while
remaining undetected by bad-data detectors. This attack-control interaction is
solved via Karush-Kuhn-Tucker (KKT) reformulation, which results in a tractable
mixed-integer quadratic program. Test gas pipeline case studies demonstrate the
covert reduction of service delivery under attack. Results show that
undetectable attacks can cause sustained throughput loss with minimal
instantaneous deviation. This reveals the need for integrated detection and
control strategies in cyber-physical infrastructure.

</details>


### [387] [Guaranteed Time Control using Linear Matrix Inequalities](https://arxiv.org/abs/2510.02636)
*Víctor Costa da Silva Campos,Mariella Maia Quadros,Luciano Frezzato,Leonardo Mozelli,Anh-Tu Nguyen*

Main category: eess.SY

TL;DR: 本研究提出一种合成方法，旨在确保在考虑不确定性、输入和状态约束的情况下，在特定时间内达到包含原点的非零目标集。


<details>
  <summary>Details</summary>
Motivation: 不确定性和约束条件下，合成控制以确保达到目标集的时间具有最小上界。

Method: 使用Lyapunov函数谐波变换和分段二次函数表示，通过策略迭代和线性矩阵不等式求解。

Result: 该方法能够处理不确定性、输入和状态约束，并已扩展到分段和非线性系统，通过三个示例证明了其有效性。

Conclusion: 所提出的方法有效地解决了在不确定性和约束条件下，在限定时间内达到目标集的问题。

Abstract: This paper presents a synthesis approach aiming to guarantee a minimum
upper-bound for the time taken to reach a target set of non-zero measure that
encompasses the origin, while taking into account uncertainties and input and
state constraints. This approach is based on a harmonic transformation of the
Lyapunov function and a novel piecewise quadratic representation of this
transformed Lyapunov function over a simplicial partition of the state space.
The problem is solved in a policy iteration fashion, whereas the evaluation and
improvement steps are formulated as linear matrix inequalities employing the
structural relaxation approach. Though initially formulated for uncertain
polytopic systems, extensions to piecewise and nonlinear systems are discussed.
Three examples illustrate the effectiveness of the proposed approach in
different scenarios.

</details>


### [388] [A Control-Barrier-Function-Based Algorithm for Policy Adaptation in Reinforcement Learning](https://arxiv.org/abs/2510.02720)
*Wenjian Hao,Zehui Lu,Nicolas Miguel,Shaoshuai Mou*

Main category: eess.SY

TL;DR: 该研究提出了一种基于控制障碍函数（CBF）的策略适应方法，用于在保持原始成本最小化的同时，权衡新的成本函数，并通过数值实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 将预训练策略从最小化原始成本适应到最小化原始成本和附加成本的权衡解，同时明确约束偏离原始成本最优值的程度。

Method: 将问题构建为约束优化问题，并开发一个闭环系统来控制策略参数的演化，利用控制障碍函数的集合不变性来保证约束满足。

Result: 通过在 Cartpole、Lunar Lander 和四足机器人等基准测试上的数值实验，证明了所提方法的有效性。

Conclusion: 所提出的控制障碍函数（CBF）驱动的策略适应方法能够有效地在满足约束的同时，实现原始成本最小化与附加成本权衡的目标，并已在多个仿真和机器人平台上得到验证。

Abstract: This paper considers the problem of adapting a predesigned policy,
represented by a parameterized function class, from a solution that minimizes a
given original cost function to a trade-off solution between minimizing the
original objective and an additional cost function. The problem is formulated
as a constrained optimization problem, where deviations from the optimal value
of the original cost are explicitly constrained. To solve it, we develop a
closed-loop system that governs the evolution of the policy parameters, with a
closed-loop controller designed to adjust the additional cost gradient to
ensure the satisfaction of the constraint. The resulting closed-loop system,
termed control-barrier-function-based policy adaptation, exploits the
set-invariance property of control barrier functions to guarantee constraint
satisfaction. The effectiveness of the proposed method is demonstrated through
numerical experiments on the Cartpole and Lunar Lander benchmarks from OpenAI
Gym, as well as a quadruped robot, thereby illustrating both its practicality
and potential for real-world policy adaptation.

</details>


### [389] [Periodic Event-Triggered Prescribed Time Control of Euler-Lagrange Systems under State and Input Constraints](https://arxiv.org/abs/2510.02769)
*Chidre Shravista Kashyap,Karnan A,Pushpak Jagtap,Jishnu Keshavan*

Main category: eess.SY

TL;DR: 本文提出了一种周期事件触发的自适应障碍控制策略，用于解决具有状态、输入和时间（SIT）约束的扰动欧拉-拉格朗日系统的轨迹跟踪问题。该策略通过事件触发更新来减少控制动作，并确保跟踪误差在规定时间内收敛到规定界限内，同时抵抗外部干扰。


<details>
  <summary>Details</summary>
Motivation: 解决具有SIT约束的扰动欧拉-拉格朗日系统的轨迹跟踪问题，并确保跟踪误差在规定时间内收敛到规定界限内，同时抵抗外部干扰。

Method: 设计了一种无近似的自适应障碍控制架构，通过周期性评估触发条件来生成事件触发更新。通过分析滤波跟踪误差的性能退化来推导监控周期的上限，并考虑了一个时变阈值函数来减少瞬态行为中的触发次数，从而避免了芝诺现象。

Result: 所提出的控制策略能够确保跟踪误差在规定时间内收敛到规定界限内，并且能够有效抵抗外部干扰。通过仿真和实验研究验证了该控制方案的有效性。

Conclusion: 所提出的周期事件触发自适应障碍控制策略能够有效地解决扰动欧拉-拉格朗日系统的轨迹跟踪问题，并满足SIT约束，同时减少控制动作的频率。

Abstract: This article proposes a periodic event-triggered adaptive barrier control
policy for the trajectory tracking problem of perturbed Euler-Lagrangian
systems with state, input, and temporal (SIT) constraints. In particular, an
approximation-free adaptive-barrier control architecture is designed to ensure
prescribed-time convergence of the tracking error to a prescribed bound while
rejecting exogenous disturbances. In contrast to existing approaches that
necessitate continuous real-time control action, the proposed controller
generates event-based updates through periodic evaluation of the triggering
condition. Additionally, we derive an upper bound on the monitoring period by
analysing the performance degradation of the filtered tracking error to
facilitate periodic evaluation of the event-triggered strategy. To this end, a
time-varying threshold function is considered in the triggering mechanism to
reduce the number of triggers during the transient phase of system behaviour.
Notably, the proposed design avoids Zeno behaviour and precludes the need for
continuous monitoring of the triggering condition. A simulation and
experimental study is undertaken to demonstrate the efficacy of the proposed
control scheme.

</details>


### [390] [Life Estimation of HVDC Cable Insulation under Load Cycles: from Macroscopic to Microscopic Charge Conduction Modelling](https://arxiv.org/abs/2510.02866)
*Bassel Diban,Giovanni Mazzanti*

Main category: eess.SY

TL;DR: 本篇论文通过引入双极电荷传输（BCT）模型来估算高压直流（HVDC）电缆绝缘在负载循环下的寿命，该模型考虑了绝缘厚度内的电荷传导和传输。


<details>
  <summary>Details</summary>
Motivation: 为高压直流电缆绝缘在负载循环下的寿命估算提供一种新的微观模型，以更精确地计算绝缘内的电场。

Method: 开发并验证了BCT模型，使用脉冲声发射（PEA）电荷测量优化模型参数，并将优化后的模型集成到500 kV DC-XLPE绝缘电缆的寿命估算中，并与宏观模型进行了比较。

Result: BCT模型在电场反转方面与宏观模型结果相似，但在加热和冷却负载循环下的行为不同；在高温电缆中，两种模型计算出的最大电场幅值和位置均不同。

Conclusion: BCT模型为高压直流电缆绝缘寿命估算提供了一种新的微观视角，其在不同负载循环下的行为与宏观模型存在差异，有待进一步研究。

Abstract: This paper goes one step forward in the life estimation of HVDC cable
insulation under load cycles by introducing for the first time a microscopic
model of charge conduction and transport i.e., Bipolar Charge Transport BCT
model for electric field calculation inside the insulation thickness. The paper
firstly includes the development and the validation of BCT model with that
found in literature. Then, the parameters of the developed BCT model are
optimized using Pulsed Electro-Acoustic PEA space charge measurements. Followed
by the integration of the developed, validated and optimized model into the
electric field calculation for life estimation of a 500 kV DC-XLPE insulated
cable subjected to Type Test load cycles according to Cigre Techical Brochure
852. The developed microscopic model is compared to the macroscopic models
already found in the literature. The microscopic model shows a comparable
electric field inversion similarly to macroscopic models. However, the behavior
of the microscopic model is noticed to be different under heating and cooling
load cycles. In hot cable, the maximum electric field stabilizes at different
amplitude and position inside the insulation thickness in both models. This
investigation has been carried out in the framework of the HEU-NEWGEN research
project.

</details>


### [391] [Global Convergence of Policy Gradient for Entropy Regularized Linear-Quadratic Control with multiplicative noise](https://arxiv.org/abs/2510.02896)
*Gabriel Diaz,Lucky Li,Wenhao Zhang*

Main category: eess.SY

TL;DR: 该论文研究了在未知系统参数下，利用强化学习（RL）解决具有乘性噪声的无限时间熵正则化线性二次控制（LQC）问题。


<details>
  <summary>Details</summary>
Motivation: 在动态且系统参数未知的环境中，探索强化学习在顺序决策制定中的应用，特别是针对具有挑战性的熵正则化线性二次控制（LQC）问题。

Method: 1. 改进了正则化策略梯度（RPG）算法以适应随机最优控制，并在梯度占优和近乎光滑的条件下证明了RPG的全局收敛性。 2. 提出了一种基于无监督优化的新型无模型强化学习算法：样本基础正则化策略梯度（SB-RPG），该算法在无需系统参数知识的情况下，依然能保证全局收敛性。 3. 利用熵正则化来加速收敛并解决探索与利用的权衡问题。

Result: RPG算法在梯度占优和近乎光滑条件下能够全局收敛。SB-RPG算法在无需系统参数的情况下，也具有全局收敛的理论保证，并且通过数值模拟验证了其有效性。

Conclusion: SB-RPG是一种有效的无模型强化学习算法，能够解决具有乘性噪声的熵正则化LQC问题，尤其在系统参数未知的情况下表现出色。

Abstract: Reinforcement Learning (RL) has emerged as a powerful framework for
sequential decision-making in dynamic environments, particularly when system
parameters are unknown. This paper investigates RL-based control for
entropy-regularized Linear Quadratic control (LQC) problems with multiplicative
noises over an infinite time horizon. First, we adapt the Regularized Policy
Gradient (RPG) algorithm to stochastic optimal control settings, proving that
despite the non-convexity of the problem, RPG converges globally under
conditions of gradient domination and near-smoothness. Second, based on
zero-order optimization approach, we introduce a novel model free RL algorithm:
Sample-Based Regularized Policy Gradient (SB-RPG). SB-RPG operates without
knowledge of system parameters yet still retains strong theoretical guarantees
of global convergence. Our model leverages entropy regularization to accelerate
convergence and address the exploration versus exploitation trade-off inherent
in RL. Numerical simulations validate the theoretical results and demonstrate
the efficacy of SB-RPG in unknown-parameters environments.

</details>


### [392] [Incomplete Air Mixing Reduces the Efficiency of Commercial Buildings Behaving as Virtual Batteries](https://arxiv.org/abs/2510.02933)
*Austin J. Lin,Jacques A. de Chalendar,Johanna L. Mathieu*

Main category: eess.SY

TL;DR: HVAC负荷转移的虚拟电池模型需要考虑空气混合不完全导致的效率损失，通过闭环风扇功耗测量可以提高虚拟电池性能。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟电池模型未能充分捕捉HVAC负荷转移的效率损失，尤其是在空气混合不完全的情况下。

Method: 提出一个新的分析建筑模型，考虑了供应空气与空间内空气混合不完全对HVAC风扇功耗和负荷转移效率的影响，并提出通过闭环风扇功耗测量来优化虚拟电池性能。

Result: 新模型能更好地定性匹配实验结果，显示随着空气混合恶化，虚拟电池效率降低。闭环测量可显著提高虚拟电池的效率，例如将单向效率从0.75提高到0.99。

Conclusion: 空气混合不完全是HVAC虚拟电池模型效率损失的关键因素。通过对风扇功耗进行闭环测量，可以在无需测量空气混合参数的情况下，有效提升虚拟电池的性能。

Abstract: Commercial building Heating, Ventilation, and Air Conditioning (HVAC) systems
can provide flexibility to the electricity grid. Some researchers have found it
convenient to model HVAC systems as virtual batteries. These models also better
align with models used by grid planners and operators. However, experiments
have shown that HVAC load shifting can be inefficient, and virtual battery
models do not capture this inefficiency well. While the models typically use
the average room temperature as the system's ``state of charge," they do not
capture other factors that affect HVAC power/energy such as airflow and mixing.
Here, we develop a new analytical building model to explore how incomplete
mixing of supply air into a conditioned space leads to inefficiency in a
virtual battery capturing the dynamics of HVAC fan power load shifting. The
model qualitatively matches experimental results better than previous models,
and shows that, as mixing becomes worse, the virtual battery becomes less
efficient. Unfortunately, air mixing is unmeasured/unmeasurable. However, we
show that, by closing the loop around measurements of fan power, we can improve
the virtual battery's performance without the need for air mixing measurements.
For example, in one case, we show a roundtrip efficiency improvement from 0.75
to 0.99.

</details>


### [393] [Real-Time Peer-to-Peer Energy Trading for Multi-Microgrids: Improved Double Auction Mechanism and Prediction-Free Online Trading Approach](https://arxiv.org/abs/2510.02985)
*Kaidi Huang,Lin Cheng,Yue Zhou,Fashun Shi,Yufei Xi,Yingrui Zhuang,Ning Qi*

Main category: eess.SY

TL;DR: 本论文提出了一种改进的双重拍卖机制和数据驱动的双参考在线优化框架，以解决实时 P2P 能源交易中的计算复杂性和次优决策问题。


<details>
  <summary>Details</summary>
Motivation: 现有的 P2P 能源交易市场面临计算复杂度高和参与者在不确定性下决策不佳的挑战，而现有的预测方法依赖于不可靠的预测，预测无关的方法则会导致短视行为。

Method: 提出了一种改进的双重拍卖机制（简化出价程序以降低计算负担和加速收敛）和数据驱动的双参考在线优化（DDOO）框架（通过引入两个参考信号来减轻短视决策）。

Result: 仿真结果表明，改进的机制将计算时间显著减少，并将本地能源自给自足时间从0.01%提高到29.86%，将反向潮流时间从24.51%减少到3.96%，并将平均运营成本降低19.20%。与传统的 Lyapunov 优化和模型预测控制方法相比，DDOO 框架的运营成本降低了 10%-13%，最优性差距仅为 5.76%。

Conclusion: 所提出的改进机制和 DDOO 框架在解决 P2P 能源交易中的计算和决策挑战方面是有效且可扩展的，并能在实际应用中带来显著的经济和效率效益。

Abstract: Peer-to-peer energy trading offers a promising solution for enhancing
renewable energy utilization and economic benefits within interconnected
microgrids. However, existing real-time P2P markets face two key challenges:
high computational complexity in trading mechanisms, and suboptimal participant
decision-making under diverse uncertainties. Existing prediction-based
decision-making methods rely heavily on accurate forecasts, which are typically
unavailable for microgrids, while prediction-free methods suffer from myopic
behaviors. To address these challenges, this paper proposes an improved double
auction mechanism combined with an adaptive step-size search algorithm to
reduce computational burden, and a data-driven dual-reference online
optimization (DDOO) framework to enhance participant decision-making. The
improved mechanism simplifies bidding procedures, significantly reducing
computational burden and ensuring rapid convergence to the market equilibrium.
Additionally, the prediction-free DDOO framework mitigates myopic
decision-making by introducing two informative reference signals. Case studies
on a 20-microgrid system demonstrate the effectiveness and scalability of the
proposed mechanism and approach. The improved mechanism significantly decreases
the computational time while increasing local energy self-sufficiency periods
from 0.01% to 29.86%, reducing reverse power flow periods from 24.51% to 3.96%,
and lowering average operating costs by 19.20%. Compared with conventional
approaches such as Lyapunov optimization and model predictive control, the DDOO
framework achieves a 10%-13% reduction in operating costs with an optimality
gap of only 5.76%.

</details>


### [394] [Economic zone data-enabled predictive control for connected open water systems](https://arxiv.org/abs/2510.03043)
*Xiaoqiao Chen,Xuewen Zhang,Minghao Han,Adrian Wing-Keung Law,Xunyuan Yin*

Main category: eess.SY

TL;DR: 该研究提出了一种结合经济分区、数据驱动预测控制（DeePC）和贝叶斯优化的混合整数方法，用于实时调控连通开放水系统中的水位，以确保安全并最小化能耗。


<details>
  <summary>Details</summary>
Motivation: 实时调控连通开放水系统的水分布对于确保系统安全和满足运行要求至关重要。

Method: 提出了一种混合整数经济分区数据驱动预测控制（DeePC）方法，利用贝叶斯优化来确定控制目标分区，以平衡分区跟踪和能耗。该方法通过预测系统未来的水位动态并基于输入输出数据生成最优控制动作，无需第一性原理或显式数据驱动模型。

Result: 在模拟中，所提出的 DeePC 方法将水位维持在目标区域内的时间为 97.04%，平均能耗为 33.5 kWh/0.5h。与基线方法相比，该方法将分区跟踪均方误差降低了 98.82%，将能耗降低了 44.08%，并将区域违规频率降低了 86.94%，平均能耗降低了 4.69%。

Conclusion: 所提出的混合整数经济分区 DeePC 方法结合贝叶斯优化，能够有效地实时调控连通开放水系统，实现了水位在期望区域内的稳定，并显著降低了泵的能耗。

Abstract: Real-time regulation of water distribution in connected open water systems is
critical for ensuring system safety and meeting operational requirements. In
this work, we consider a connected open water system that includes linkage
hydraulic structures such as weirs, pumps and sluice gates. We propose a
mixed-integer economic zone data-enabled predictive control (DeePC) approach,
which is used to maintain the water levels of the branches within desired zones
to avoid floods and reduce the energy consumption of the pumps in the
considered water system. The proposed DeePC-based approach predicts the future
dynamics of the system water levels, and generates optimal control actions
based on system input and output data, thereby eliminating the need for both
first-principles modeling and explicit data-driven modeling. To achieve
multiple control objectives in order of priority, we utilize lexicographic
optimization and adapt traditional DeePC cost function for zone tracking and
energy consumption minimization. Additionally, Bayesian optimization is
utilized to determine the control target zone, which effectively balances zone
tracking and energy consumption in the presence of external disturbances.
Comprehensive simulations and comparative analyses demonstrate the
effectiveness of the proposed method. The proposed method maintains water
levels within the desired zone for 97.04% of the operating time, with an
average energy consumption of 33.5 kWh per 0.5 h. Compared to baseline methods,
the proposed approach reduces the zone-tracking mean square error by 98.82%
relative to economic zone DeePC without Bayesian optimization, and lowers
energy consumption by 44.08% relative to economic set-point tracking DeePC. As
compared to passive pump/gate control, the proposed method lowers the frequency
of zone violations by 86.94% and the average energy consumption by 4.69%.

</details>


### [395] [Eigenvalue Tracking of Large-Scale Systems Impacted by Time Delays](https://arxiv.org/abs/2510.03070)
*Andreas Bouterakos,Georgios Tzounas*

Main category: eess.SY

TL;DR: 该论文提出一种基于数值积分和延时微分代数方程（DDAE）模型分析方法，用于追踪含时延电力系统模型中特征值的变化轨迹，并成功应用于IEEE 39总线系统和爱尔兰输电网络模型。


<details>
  <summary>Details</summary>
Motivation: 在含时延的电力系统模型中，追踪特征值的变化轨迹对于理解系统稳定性和动态行为至关重要。

Method: 提出一种基于延拓的数值方法，该方法利用数值积分来追踪参数变化时特征值的轨迹，并能处理含有一个或多个时滞变量的DDAE系统模型，同时保持模型的稀疏性，并将时滞本身作为可变参数进行处理。

Result: 该方法在修改后的IEEE 39总线系统（包含分布式能源）上进行了准确性验证，并在爱尔兰输电网络的真实动态模型上进行了可扩展性讨论。

Conclusion: 所提出的方法能够有效地追踪含时延电力系统模型中的特征值轨迹，并具有良好的准确性和可扩展性，可用于分析复杂的电力系统动力学。

Abstract: The paper focuses on tracking eigenvalue trajectories in power system models
with time delays. We formulate a continuation-based approach that employs
numerical integration to follow eigenvalues as system parameters vary, in the
presence of one or multiple delayed variables. The formulation preserves the
sparsity of the delay differential-algebraic equation (DDAE) system model and
allows treating the delay magnitude itself as a varying parameter with
implementation aspects discussed in detail. Accuracy is demonstrated on a
modified IEEE 39-bus system with distributed energy resources. Scalability is
discussed using a realistic dynamic model of the Irish transmission network.

</details>


### [396] [A Dimension-Decomposed Learning Framework for Online Disturbance Identification in Quadrotor SE(3) Control](https://arxiv.org/abs/2510.03100)
*Tianhua Gao*

Main category: eess.SY

TL;DR: DiD-L通过将高维映射分解为低维子映射来解决四旋翼无人机在复杂动态干扰和模型不确定性下的稳定性问题，SANM方法无需预训练即可实现指数收敛。


<details>
  <summary>Details</summary>
Motivation: 高维特征下的欠拟合问题限制了现有基于学习的方法的识别能力，对四旋翼无人机在复杂动态干扰和模型不确定性下的稳定性构成了挑战。

Method: 提出维度分解学习（DiD-L）方法，并开发了切片自适应神经映射（SANM）用于几何控制。该方法将高维映射分解为多个低维子映射（“切片”），并使用浅层神经网络和自适应律在线更新，无需预训练或持久激励条件。

Result: 证明了全状态闭环系统具有任意接近指数稳定性，能够应对多维时变干扰和模型不确定性。

Conclusion: SANM方法在无需预训练的情况下，能够实现指数收敛，解决了现有方法在处理未知干扰和模型不确定性时的局限性，提高了四旋翼无人机的稳定性和识别能力。

Abstract: Quadrotor stability under complex dynamic disturbances and model
uncertainties poses significant challenges. One of them remains the
underfitting problem in high-dimensional features, which limits the
identification capability of current learning-based methods. To address this,
we introduce a new perspective: Dimension-Decomposed Learning (DiD-L), from
which we develop the Sliced Adaptive-Neuro Mapping (SANM) approach for
geometric control. Specifically, the high-dimensional mapping for
identification is axially ``sliced" into multiple low-dimensional submappings
(``slices"). In this way, the complex high-dimensional problem is decomposed
into a set of simple low-dimensional tasks addressed by shallow neural networks
and adaptive laws. These neural networks and adaptive laws are updated online
via Lyapunov-based adaptation without any pre-training or persistent excitation
(PE) condition. To enhance the interpretability of the proposed approach, we
prove that the full-state closed-loop system exhibits arbitrarily close to
exponential stability despite multi-dimensional time-varying disturbances and
model uncertainties. This result is novel as it demonstrates exponential
convergence without requiring pre-training for unknown disturbances and
specific knowledge of the model.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [397] [Hyperparameters are all you need: Using five-step inference for an original diffusion model to generate images comparable to the latest distillation model](https://arxiv.org/abs/2510.02390)
*Zilai Li*

Main category: cs.GR

TL;DR: 提出了一种无需训练即可生成高质量图像（512x512和1024x1024）的算法，该算法仅需八步即可完成采样，且具有灵活的引导尺度。


<details>
  <summary>Details</summary>
Motivation: 分析了扩散模型ODE和SDE的截断误差，旨在改进生成过程的效率和质量。

Method: 基于扩散模型ODE和SDE的截断误差分析，提出了一种新的训练免费算法。

Result: 生成的1024x1024图像在8步内FID性能可与最新的蒸馏模型相媲美，且无需额外训练。生成的512x512图像在8步内的FID性能优于DPM++ 2m ODE求解器在20步内的结果。在COCO 2014、COCO 2017和LAION数据集上，最佳FID分别为15.7、22.35和17.52，优于DPM++2m和AMED-plugin solver。该算法在五步推理中也能取得具有竞争力的FID性能。在六步内生成1024*1024图像的FID性能与最新的蒸馏算法接近。

Conclusion: 所提出的训练免费算法在图像生成效率和质量方面取得了显著的进步，特别是在高分辨率图像生成方面，并且在多种数据集和不同步数下都表现出优越的性能。

Abstract: The diffusion model is a state-of-the-art generative model that generates an
image by applying a neural network iteratively. Moreover, this generation
process is regarded as an algorithm solving an ordinary differential equation
or a stochastic differential equation. Based on the analysis of the truncation
error of the diffusion ODE and SDE, our study proposes a training-free
algorithm that generates high-quality 512 x 512 and 1024 x 1024 images in eight
steps, with flexible guidance scales. To the best of my knowledge, our
algorithm is the first one that samples a 1024 x 1024 resolution image in 8
steps with an FID performance comparable to that of the latest distillation
model, but without additional training. Meanwhile, our algorithm can also
generate a 512 x 512 image in 8 steps, and its FID performance is better than
the inference result using state-of-the-art ODE solver DPM++ 2m in 20 steps. We
validate our eight-step image generation algorithm using the COCO 2014, COCO
2017, and LAION datasets. And our best FID performance is 15.7, 22.35, and
17.52. While the FID performance of DPM++2m is 17.3, 23.75, and 17.33. Further,
it also outperforms the state-of-the-art AMED-plugin solver, whose FID
performance is 19.07, 25.50, and 18.06. We also apply the algorithm in
five-step inference without additional training, for which the best FID
performance in the datasets mentioned above is 19.18, 23.24, and 19.61,
respectively, and is comparable to the performance of the state-of-the-art AMED
Pulgin solver in eight steps, SDXL-turbo in four steps, and the
state-of-the-art diffusion distillation model Flash Diffusion in five steps. We
also validate our algorithm in synthesizing 1024 * 1024 images within 6 steps,
whose FID performance only has a limited distance to the latest distillation
algorithm. The code is in repo:
https://github.com/TheLovesOfLadyPurple/Hyperparameters-are-all-you-need

</details>


### [398] [Visualizing Spatial Point Clouds: A Task-Oriented Taxonomy](https://arxiv.org/abs/2510.02651)
*Mahsa Partovi,Federico Iuricich*

Main category: cs.GR

TL;DR: 3D点云可视化对自动驾驶、环境监测和灾难响应等领域至关重要，但现有技术在映射可视化技术和分析目标方面存在差距。本研究提出了一个分类法，将40年的可视化设计选择与现代应用中的挑战联系起来，旨在促进更有效、可解释和以用户为中心的可视化技术。


<details>
  <summary>Details</summary>
Motivation: 3D点云数据的可视化对于自动驾驶、环境监测和灾难响应等领域至关重要，但现有技术在映射可视化技术和分析目标方面存在差距。

Method: 本研究分析了空间点云可视化的设计空间，并引入了一个分类法，将40年的可视化设计选择与现代应用中的挑战联系起来。

Result: 该分类法将可视化策略基于数据类型、用户目标和可视化技术进行构建，为推进更有效、可解释和以用户为中心的可视化技术奠定了基础。

Conclusion: 本研究提出的框架为推进更有效、可解释和以用户为中心的可视化技术奠定了基础。

Abstract: The visualization of 3D point cloud data is essential in fields such as
autonomous navigation, environmental monitoring, and disaster response, where
tasks like object recognition, structural analysis, and spatiotemporal
exploration rely on clear and effective visual representation. Despite
advancements in AI-driven processing, visualization remains a critical tool for
interpreting complex spatial datasets. However, designing effective point cloud
visualizations presents significant challenges due to the sparsity, density
variations, and scale of the data. In this work, we analyze the design space of
spatial point cloud visualization, highlighting a gap in systematically mapping
visualization techniques to analytical objectives. We introduce a taxonomy that
categorizes four decades of visualization design choices, linking them to
fundamental challenges in modern applications. By structuring visualization
strategies based on data types, user objectives, and visualization techniques,
our framework provides a foundation for advancing more effective,
interpretable, and user-centered visualization techniques.

</details>


### [399] [GS-Share: Enabling High-fidelity Map Sharing with Incremental Gaussian Splatting](https://arxiv.org/abs/2510.02884)
*Xinran Zhang,Hanqi Zhu,Yifan Duan,Yanyong Zhang*

Main category: cs.GR

TL;DR: GS-Share是一个高保真、更新及时的3D地图共享系统，采用紧凑表示，提高了地图质量并减少了传输开销。


<details>
  <summary>Details</summary>
Motivation: 当前3D地图的构建和共享系统在保真度、更新速度和网络效率方面存在不足，尤其是在自动驾驶和增强现实等应用中。

Method: GS-Share系统包含基于锚点的全局地图构建、基于虚拟图像的地图增强和增量式地图更新。

Result: 与最先进的方法相比，GS-Share在PSNR、LPIPS和Depth L1方面分别提高了11%、22%和74%，尤其是在外推视图方面。此外，地图传输开销减少了36%。

Conclusion: GS-Share成功解决了现有3D地图共享系统的挑战，提供了高保真度、实时更新和网络效率，并且具有紧凑的表示形式。

Abstract: Constructing and sharing 3D maps is essential for many applications,
including autonomous driving and augmented reality. Recently, 3D Gaussian
splatting has emerged as a promising approach for accurate 3D reconstruction.
However, a practical map-sharing system that features high-fidelity, continuous
updates, and network efficiency remains elusive. To address these challenges,
we introduce GS-Share, a photorealistic map-sharing system with a compact
representation. The core of GS-Share includes anchor-based global map
construction, virtual-image-based map enhancement, and incremental map update.
We evaluate GS-Share against state-of-the-art methods, demonstrating that our
system achieves higher fidelity, particularly for extrapolated views, with
improvements of 11%, 22%, and 74% in PSNR, LPIPS, and Depth L1, respectively.
Furthermore, GS-Share is significantly more compact, reducing map transmission
overhead by 36%.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [400] [A many-objective evolutionary algorithm using indicator-driven weight vector optimization](https://arxiv.org/abs/2510.02709)
*Xiaojing Han,Yuanxin Li*

Main category: cs.NE

TL;DR: 当遇到不规则的帕累托前沿（PF）时，传统的基于分解的 MOEA 方法由于其固定的权重向量而表现不佳。本研究提出了一种自适应的许多目标进化算法，该算法使用简化的超体积指标来动态调整权重向量，从而提高优化性能。


<details>
  <summary>Details</summary>
Motivation: 现有的多目标优化方法在处理不规则的帕累托前沿（PF）时，例如退化、断开或反转的PF，由于固定的权重向量，往往会导致解分布不均匀或优化效果不佳。

Method: 提出了一种自适应的许多目标进化算法，该算法结合了指标评估技术和基于分解的方法。具体来说，它在 MOEA/D 框架的基础上，使用简化的超体积指标来评估解的分布，并利用 R2 指标（作为超体积的近似）来动态调整权重向量的更新频率。

Result: 实验结果表明，与六种最先进的算法相比，所提出的算法在效率和有效性方面表现更优。

Conclusion: 所提出的自适应许多目标进化算法能够有效解决在不规则帕累托前沿问题上传统固定权重向量方法的不足，通过动态调整权重向量实现了更优的优化结果。

Abstract: For regular Pareto Fronts (PFs), such as those that are smooth, continuous,
and uniformly distributed, using fixed weight vectors is sufficient for
multi-objective optimization approaches using decomposition. However, when
encountering irregular PFs-including degenerate, disconnected, inverted, etc.
Fixed weight vectors can often cause a non-uniform distribution of the sets or
even poor optimization results. To address this issue, this study proposes an
adaptive many-objective evolutionary algorithm with a simplified hypervolume
indicator. It synthesizes indicator assessment techniques with
decomposition-based methods to facilitate self-adaptive and dynamic adjustment
of the weight vectors in many-objective optimization methods. Specifically,
based on the MOEA/D framework, it uses a simplified hypervolume indicator to
accurately assess solution distribution. Simultaneously, applying the R2
indicator (as an approximation of hypervolume) dynamically regulates the update
frequency of the weight vectors. Experimental results demonstrate that the
proposed algorithm is efficient and effective when compared with six
state-of-the-art algorithms.

</details>
